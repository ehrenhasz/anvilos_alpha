{
  "module_name": "macb_main.c",
  "hash_id": "c92782b27c38d46178facba5bf21352220acd9180fbec2e19815b6364a457078",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/cadence/macb_main.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n#include <linux/clk.h>\n#include <linux/clk-provider.h>\n#include <linux/crc32.h>\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/kernel.h>\n#include <linux/types.h>\n#include <linux/circ_buf.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/io.h>\n#include <linux/gpio.h>\n#include <linux/gpio/consumer.h>\n#include <linux/interrupt.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/dma-mapping.h>\n#include <linux/platform_device.h>\n#include <linux/phylink.h>\n#include <linux/of.h>\n#include <linux/of_gpio.h>\n#include <linux/of_mdio.h>\n#include <linux/of_net.h>\n#include <linux/ip.h>\n#include <linux/udp.h>\n#include <linux/tcp.h>\n#include <linux/iopoll.h>\n#include <linux/phy/phy.h>\n#include <linux/pm_runtime.h>\n#include <linux/ptp_classify.h>\n#include <linux/reset.h>\n#include <linux/firmware/xlnx-zynqmp.h>\n#include \"macb.h\"\n\n \nstruct sifive_fu540_macb_mgmt {\n\tvoid __iomem *reg;\n\tunsigned long rate;\n\tstruct clk_hw hw;\n};\n\n#define MACB_RX_BUFFER_SIZE\t128\n#define RX_BUFFER_MULTIPLE\t64   \n\n#define DEFAULT_RX_RING_SIZE\t512  \n#define MIN_RX_RING_SIZE\t64\n#define MAX_RX_RING_SIZE\t8192\n#define RX_RING_BYTES(bp)\t(macb_dma_desc_get_size(bp)\t\\\n\t\t\t\t * (bp)->rx_ring_size)\n\n#define DEFAULT_TX_RING_SIZE\t512  \n#define MIN_TX_RING_SIZE\t64\n#define MAX_TX_RING_SIZE\t4096\n#define TX_RING_BYTES(bp)\t(macb_dma_desc_get_size(bp)\t\\\n\t\t\t\t * (bp)->tx_ring_size)\n\n \n#define MACB_TX_WAKEUP_THRESH(bp)\t(3 * (bp)->tx_ring_size / 4)\n\n#define MACB_RX_INT_FLAGS\t(MACB_BIT(RCOMP) | MACB_BIT(ISR_ROVR))\n#define MACB_TX_ERR_FLAGS\t(MACB_BIT(ISR_TUND)\t\t\t\\\n\t\t\t\t\t| MACB_BIT(ISR_RLE)\t\t\\\n\t\t\t\t\t| MACB_BIT(TXERR))\n#define MACB_TX_INT_FLAGS\t(MACB_TX_ERR_FLAGS | MACB_BIT(TCOMP)\t\\\n\t\t\t\t\t| MACB_BIT(TXUBR))\n\n \n#define MACB_TX_LEN_ALIGN\t8\n#define MACB_MAX_TX_LEN\t\t((unsigned int)((1 << MACB_TX_FRMLEN_SIZE) - 1) & ~((unsigned int)(MACB_TX_LEN_ALIGN - 1)))\n \n#define GEM_MAX_TX_LEN\t\t(unsigned int)(0x3FC0)\n\n#define GEM_MTU_MIN_SIZE\tETH_MIN_MTU\n#define MACB_NETIF_LSO\t\tNETIF_F_TSO\n\n#define MACB_WOL_HAS_MAGIC_PACKET\t(0x1 << 0)\n#define MACB_WOL_ENABLED\t\t(0x1 << 1)\n\n#define HS_SPEED_10000M\t\t\t4\n#define MACB_SERDES_RATE_10G\t\t1\n\n \n#define MACB_HALT_TIMEOUT\t14000\n#define MACB_PM_TIMEOUT  100  \n\n#define MACB_MDIO_TIMEOUT\t1000000  \n\n \nstatic unsigned int macb_dma_desc_get_size(struct macb *bp)\n{\n#ifdef MACB_EXT_DESC\n\tunsigned int desc_size;\n\n\tswitch (bp->hw_dma_cap) {\n\tcase HW_DMA_CAP_64B:\n\t\tdesc_size = sizeof(struct macb_dma_desc)\n\t\t\t+ sizeof(struct macb_dma_desc_64);\n\t\tbreak;\n\tcase HW_DMA_CAP_PTP:\n\t\tdesc_size = sizeof(struct macb_dma_desc)\n\t\t\t+ sizeof(struct macb_dma_desc_ptp);\n\t\tbreak;\n\tcase HW_DMA_CAP_64B_PTP:\n\t\tdesc_size = sizeof(struct macb_dma_desc)\n\t\t\t+ sizeof(struct macb_dma_desc_64)\n\t\t\t+ sizeof(struct macb_dma_desc_ptp);\n\t\tbreak;\n\tdefault:\n\t\tdesc_size = sizeof(struct macb_dma_desc);\n\t}\n\treturn desc_size;\n#endif\n\treturn sizeof(struct macb_dma_desc);\n}\n\nstatic unsigned int macb_adj_dma_desc_idx(struct macb *bp, unsigned int desc_idx)\n{\n#ifdef MACB_EXT_DESC\n\tswitch (bp->hw_dma_cap) {\n\tcase HW_DMA_CAP_64B:\n\tcase HW_DMA_CAP_PTP:\n\t\tdesc_idx <<= 1;\n\t\tbreak;\n\tcase HW_DMA_CAP_64B_PTP:\n\t\tdesc_idx *= 3;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n#endif\n\treturn desc_idx;\n}\n\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\nstatic struct macb_dma_desc_64 *macb_64b_desc(struct macb *bp, struct macb_dma_desc *desc)\n{\n\treturn (struct macb_dma_desc_64 *)((void *)desc\n\t\t+ sizeof(struct macb_dma_desc));\n}\n#endif\n\n \nstatic unsigned int macb_tx_ring_wrap(struct macb *bp, unsigned int index)\n{\n\treturn index & (bp->tx_ring_size - 1);\n}\n\nstatic struct macb_dma_desc *macb_tx_desc(struct macb_queue *queue,\n\t\t\t\t\t  unsigned int index)\n{\n\tindex = macb_tx_ring_wrap(queue->bp, index);\n\tindex = macb_adj_dma_desc_idx(queue->bp, index);\n\treturn &queue->tx_ring[index];\n}\n\nstatic struct macb_tx_skb *macb_tx_skb(struct macb_queue *queue,\n\t\t\t\t       unsigned int index)\n{\n\treturn &queue->tx_skb[macb_tx_ring_wrap(queue->bp, index)];\n}\n\nstatic dma_addr_t macb_tx_dma(struct macb_queue *queue, unsigned int index)\n{\n\tdma_addr_t offset;\n\n\toffset = macb_tx_ring_wrap(queue->bp, index) *\n\t\t\tmacb_dma_desc_get_size(queue->bp);\n\n\treturn queue->tx_ring_dma + offset;\n}\n\nstatic unsigned int macb_rx_ring_wrap(struct macb *bp, unsigned int index)\n{\n\treturn index & (bp->rx_ring_size - 1);\n}\n\nstatic struct macb_dma_desc *macb_rx_desc(struct macb_queue *queue, unsigned int index)\n{\n\tindex = macb_rx_ring_wrap(queue->bp, index);\n\tindex = macb_adj_dma_desc_idx(queue->bp, index);\n\treturn &queue->rx_ring[index];\n}\n\nstatic void *macb_rx_buffer(struct macb_queue *queue, unsigned int index)\n{\n\treturn queue->rx_buffers + queue->bp->rx_buffer_size *\n\t       macb_rx_ring_wrap(queue->bp, index);\n}\n\n \nstatic u32 hw_readl_native(struct macb *bp, int offset)\n{\n\treturn __raw_readl(bp->regs + offset);\n}\n\nstatic void hw_writel_native(struct macb *bp, int offset, u32 value)\n{\n\t__raw_writel(value, bp->regs + offset);\n}\n\nstatic u32 hw_readl(struct macb *bp, int offset)\n{\n\treturn readl_relaxed(bp->regs + offset);\n}\n\nstatic void hw_writel(struct macb *bp, int offset, u32 value)\n{\n\twritel_relaxed(value, bp->regs + offset);\n}\n\n \nstatic bool hw_is_native_io(void __iomem *addr)\n{\n\tu32 value = MACB_BIT(LLB);\n\n\t__raw_writel(value, addr + MACB_NCR);\n\tvalue = __raw_readl(addr + MACB_NCR);\n\n\t \n\t__raw_writel(0, addr + MACB_NCR);\n\n\treturn value == MACB_BIT(LLB);\n}\n\nstatic bool hw_is_gem(void __iomem *addr, bool native_io)\n{\n\tu32 id;\n\n\tif (native_io)\n\t\tid = __raw_readl(addr + MACB_MID);\n\telse\n\t\tid = readl_relaxed(addr + MACB_MID);\n\n\treturn MACB_BFEXT(IDNUM, id) >= 0x2;\n}\n\nstatic void macb_set_hwaddr(struct macb *bp)\n{\n\tu32 bottom;\n\tu16 top;\n\n\tbottom = cpu_to_le32(*((u32 *)bp->dev->dev_addr));\n\tmacb_or_gem_writel(bp, SA1B, bottom);\n\ttop = cpu_to_le16(*((u16 *)(bp->dev->dev_addr + 4)));\n\tmacb_or_gem_writel(bp, SA1T, top);\n\n\tif (gem_has_ptp(bp)) {\n\t\tgem_writel(bp, RXPTPUNI, bottom);\n\t\tgem_writel(bp, TXPTPUNI, bottom);\n\t}\n\n\t \n\tmacb_or_gem_writel(bp, SA2B, 0);\n\tmacb_or_gem_writel(bp, SA2T, 0);\n\tmacb_or_gem_writel(bp, SA3B, 0);\n\tmacb_or_gem_writel(bp, SA3T, 0);\n\tmacb_or_gem_writel(bp, SA4B, 0);\n\tmacb_or_gem_writel(bp, SA4T, 0);\n}\n\nstatic void macb_get_hwaddr(struct macb *bp)\n{\n\tu32 bottom;\n\tu16 top;\n\tu8 addr[6];\n\tint i;\n\n\t \n\tfor (i = 0; i < 4; i++) {\n\t\tbottom = macb_or_gem_readl(bp, SA1B + i * 8);\n\t\ttop = macb_or_gem_readl(bp, SA1T + i * 8);\n\n\t\taddr[0] = bottom & 0xff;\n\t\taddr[1] = (bottom >> 8) & 0xff;\n\t\taddr[2] = (bottom >> 16) & 0xff;\n\t\taddr[3] = (bottom >> 24) & 0xff;\n\t\taddr[4] = top & 0xff;\n\t\taddr[5] = (top >> 8) & 0xff;\n\n\t\tif (is_valid_ether_addr(addr)) {\n\t\t\teth_hw_addr_set(bp->dev, addr);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tdev_info(&bp->pdev->dev, \"invalid hw address, using random\\n\");\n\teth_hw_addr_random(bp->dev);\n}\n\nstatic int macb_mdio_wait_for_idle(struct macb *bp)\n{\n\tu32 val;\n\n\treturn readx_poll_timeout(MACB_READ_NSR, bp, val, val & MACB_BIT(IDLE),\n\t\t\t\t  1, MACB_MDIO_TIMEOUT);\n}\n\nstatic int macb_mdio_read_c22(struct mii_bus *bus, int mii_id, int regnum)\n{\n\tstruct macb *bp = bus->priv;\n\tint status;\n\n\tstatus = pm_runtime_resume_and_get(&bp->pdev->dev);\n\tif (status < 0)\n\t\tgoto mdio_pm_exit;\n\n\tstatus = macb_mdio_wait_for_idle(bp);\n\tif (status < 0)\n\t\tgoto mdio_read_exit;\n\n\tmacb_writel(bp, MAN, (MACB_BF(SOF, MACB_MAN_C22_SOF)\n\t\t\t      | MACB_BF(RW, MACB_MAN_C22_READ)\n\t\t\t      | MACB_BF(PHYA, mii_id)\n\t\t\t      | MACB_BF(REGA, regnum)\n\t\t\t      | MACB_BF(CODE, MACB_MAN_C22_CODE)));\n\n\tstatus = macb_mdio_wait_for_idle(bp);\n\tif (status < 0)\n\t\tgoto mdio_read_exit;\n\n\tstatus = MACB_BFEXT(DATA, macb_readl(bp, MAN));\n\nmdio_read_exit:\n\tpm_runtime_mark_last_busy(&bp->pdev->dev);\n\tpm_runtime_put_autosuspend(&bp->pdev->dev);\nmdio_pm_exit:\n\treturn status;\n}\n\nstatic int macb_mdio_read_c45(struct mii_bus *bus, int mii_id, int devad,\n\t\t\t      int regnum)\n{\n\tstruct macb *bp = bus->priv;\n\tint status;\n\n\tstatus = pm_runtime_get_sync(&bp->pdev->dev);\n\tif (status < 0) {\n\t\tpm_runtime_put_noidle(&bp->pdev->dev);\n\t\tgoto mdio_pm_exit;\n\t}\n\n\tstatus = macb_mdio_wait_for_idle(bp);\n\tif (status < 0)\n\t\tgoto mdio_read_exit;\n\n\tmacb_writel(bp, MAN, (MACB_BF(SOF, MACB_MAN_C45_SOF)\n\t\t\t      | MACB_BF(RW, MACB_MAN_C45_ADDR)\n\t\t\t      | MACB_BF(PHYA, mii_id)\n\t\t\t      | MACB_BF(REGA, devad & 0x1F)\n\t\t\t      | MACB_BF(DATA, regnum & 0xFFFF)\n\t\t\t      | MACB_BF(CODE, MACB_MAN_C45_CODE)));\n\n\tstatus = macb_mdio_wait_for_idle(bp);\n\tif (status < 0)\n\t\tgoto mdio_read_exit;\n\n\tmacb_writel(bp, MAN, (MACB_BF(SOF, MACB_MAN_C45_SOF)\n\t\t\t      | MACB_BF(RW, MACB_MAN_C45_READ)\n\t\t\t      | MACB_BF(PHYA, mii_id)\n\t\t\t      | MACB_BF(REGA, devad & 0x1F)\n\t\t\t      | MACB_BF(CODE, MACB_MAN_C45_CODE)));\n\n\tstatus = macb_mdio_wait_for_idle(bp);\n\tif (status < 0)\n\t\tgoto mdio_read_exit;\n\n\tstatus = MACB_BFEXT(DATA, macb_readl(bp, MAN));\n\nmdio_read_exit:\n\tpm_runtime_mark_last_busy(&bp->pdev->dev);\n\tpm_runtime_put_autosuspend(&bp->pdev->dev);\nmdio_pm_exit:\n\treturn status;\n}\n\nstatic int macb_mdio_write_c22(struct mii_bus *bus, int mii_id, int regnum,\n\t\t\t       u16 value)\n{\n\tstruct macb *bp = bus->priv;\n\tint status;\n\n\tstatus = pm_runtime_resume_and_get(&bp->pdev->dev);\n\tif (status < 0)\n\t\tgoto mdio_pm_exit;\n\n\tstatus = macb_mdio_wait_for_idle(bp);\n\tif (status < 0)\n\t\tgoto mdio_write_exit;\n\n\tmacb_writel(bp, MAN, (MACB_BF(SOF, MACB_MAN_C22_SOF)\n\t\t\t      | MACB_BF(RW, MACB_MAN_C22_WRITE)\n\t\t\t      | MACB_BF(PHYA, mii_id)\n\t\t\t      | MACB_BF(REGA, regnum)\n\t\t\t      | MACB_BF(CODE, MACB_MAN_C22_CODE)\n\t\t\t      | MACB_BF(DATA, value)));\n\n\tstatus = macb_mdio_wait_for_idle(bp);\n\tif (status < 0)\n\t\tgoto mdio_write_exit;\n\nmdio_write_exit:\n\tpm_runtime_mark_last_busy(&bp->pdev->dev);\n\tpm_runtime_put_autosuspend(&bp->pdev->dev);\nmdio_pm_exit:\n\treturn status;\n}\n\nstatic int macb_mdio_write_c45(struct mii_bus *bus, int mii_id,\n\t\t\t       int devad, int regnum,\n\t\t\t       u16 value)\n{\n\tstruct macb *bp = bus->priv;\n\tint status;\n\n\tstatus = pm_runtime_get_sync(&bp->pdev->dev);\n\tif (status < 0) {\n\t\tpm_runtime_put_noidle(&bp->pdev->dev);\n\t\tgoto mdio_pm_exit;\n\t}\n\n\tstatus = macb_mdio_wait_for_idle(bp);\n\tif (status < 0)\n\t\tgoto mdio_write_exit;\n\n\tmacb_writel(bp, MAN, (MACB_BF(SOF, MACB_MAN_C45_SOF)\n\t\t\t      | MACB_BF(RW, MACB_MAN_C45_ADDR)\n\t\t\t      | MACB_BF(PHYA, mii_id)\n\t\t\t      | MACB_BF(REGA, devad & 0x1F)\n\t\t\t      | MACB_BF(DATA, regnum & 0xFFFF)\n\t\t\t      | MACB_BF(CODE, MACB_MAN_C45_CODE)));\n\n\tstatus = macb_mdio_wait_for_idle(bp);\n\tif (status < 0)\n\t\tgoto mdio_write_exit;\n\n\tmacb_writel(bp, MAN, (MACB_BF(SOF, MACB_MAN_C45_SOF)\n\t\t\t      | MACB_BF(RW, MACB_MAN_C45_WRITE)\n\t\t\t      | MACB_BF(PHYA, mii_id)\n\t\t\t      | MACB_BF(REGA, devad & 0x1F)\n\t\t\t      | MACB_BF(CODE, MACB_MAN_C45_CODE)\n\t\t\t      | MACB_BF(DATA, value)));\n\n\tstatus = macb_mdio_wait_for_idle(bp);\n\tif (status < 0)\n\t\tgoto mdio_write_exit;\n\nmdio_write_exit:\n\tpm_runtime_mark_last_busy(&bp->pdev->dev);\n\tpm_runtime_put_autosuspend(&bp->pdev->dev);\nmdio_pm_exit:\n\treturn status;\n}\n\nstatic void macb_init_buffers(struct macb *bp)\n{\n\tstruct macb_queue *queue;\n\tunsigned int q;\n\n\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue) {\n\t\tqueue_writel(queue, RBQP, lower_32_bits(queue->rx_ring_dma));\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\t\tif (bp->hw_dma_cap & HW_DMA_CAP_64B)\n\t\t\tqueue_writel(queue, RBQPH,\n\t\t\t\t     upper_32_bits(queue->rx_ring_dma));\n#endif\n\t\tqueue_writel(queue, TBQP, lower_32_bits(queue->tx_ring_dma));\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\t\tif (bp->hw_dma_cap & HW_DMA_CAP_64B)\n\t\t\tqueue_writel(queue, TBQPH,\n\t\t\t\t     upper_32_bits(queue->tx_ring_dma));\n#endif\n\t}\n}\n\n \nstatic void macb_set_tx_clk(struct macb *bp, int speed)\n{\n\tlong ferr, rate, rate_rounded;\n\n\tif (!bp->tx_clk || (bp->caps & MACB_CAPS_CLK_HW_CHG))\n\t\treturn;\n\n\t \n\tif (bp->phy_interface == PHY_INTERFACE_MODE_MII)\n\t\treturn;\n\n\tswitch (speed) {\n\tcase SPEED_10:\n\t\trate = 2500000;\n\t\tbreak;\n\tcase SPEED_100:\n\t\trate = 25000000;\n\t\tbreak;\n\tcase SPEED_1000:\n\t\trate = 125000000;\n\t\tbreak;\n\tdefault:\n\t\treturn;\n\t}\n\n\trate_rounded = clk_round_rate(bp->tx_clk, rate);\n\tif (rate_rounded < 0)\n\t\treturn;\n\n\t \n\tferr = abs(rate_rounded - rate);\n\tferr = DIV_ROUND_UP(ferr, rate / 100000);\n\tif (ferr > 5)\n\t\tnetdev_warn(bp->dev,\n\t\t\t    \"unable to generate target frequency: %ld Hz\\n\",\n\t\t\t    rate);\n\n\tif (clk_set_rate(bp->tx_clk, rate_rounded))\n\t\tnetdev_err(bp->dev, \"adjusting tx_clk failed.\\n\");\n}\n\nstatic void macb_usx_pcs_link_up(struct phylink_pcs *pcs, unsigned int neg_mode,\n\t\t\t\t phy_interface_t interface, int speed,\n\t\t\t\t int duplex)\n{\n\tstruct macb *bp = container_of(pcs, struct macb, phylink_usx_pcs);\n\tu32 config;\n\n\tconfig = gem_readl(bp, USX_CONTROL);\n\tconfig = GEM_BFINS(SERDES_RATE, MACB_SERDES_RATE_10G, config);\n\tconfig = GEM_BFINS(USX_CTRL_SPEED, HS_SPEED_10000M, config);\n\tconfig &= ~(GEM_BIT(TX_SCR_BYPASS) | GEM_BIT(RX_SCR_BYPASS));\n\tconfig |= GEM_BIT(TX_EN);\n\tgem_writel(bp, USX_CONTROL, config);\n}\n\nstatic void macb_usx_pcs_get_state(struct phylink_pcs *pcs,\n\t\t\t\t   struct phylink_link_state *state)\n{\n\tstruct macb *bp = container_of(pcs, struct macb, phylink_usx_pcs);\n\tu32 val;\n\n\tstate->speed = SPEED_10000;\n\tstate->duplex = 1;\n\tstate->an_complete = 1;\n\n\tval = gem_readl(bp, USX_STATUS);\n\tstate->link = !!(val & GEM_BIT(USX_BLOCK_LOCK));\n\tval = gem_readl(bp, NCFGR);\n\tif (val & GEM_BIT(PAE))\n\t\tstate->pause = MLO_PAUSE_RX;\n}\n\nstatic int macb_usx_pcs_config(struct phylink_pcs *pcs,\n\t\t\t       unsigned int neg_mode,\n\t\t\t       phy_interface_t interface,\n\t\t\t       const unsigned long *advertising,\n\t\t\t       bool permit_pause_to_mac)\n{\n\tstruct macb *bp = container_of(pcs, struct macb, phylink_usx_pcs);\n\n\tgem_writel(bp, USX_CONTROL, gem_readl(bp, USX_CONTROL) |\n\t\t   GEM_BIT(SIGNAL_OK));\n\n\treturn 0;\n}\n\nstatic void macb_pcs_get_state(struct phylink_pcs *pcs,\n\t\t\t       struct phylink_link_state *state)\n{\n\tstate->link = 0;\n}\n\nstatic void macb_pcs_an_restart(struct phylink_pcs *pcs)\n{\n\t \n}\n\nstatic int macb_pcs_config(struct phylink_pcs *pcs,\n\t\t\t   unsigned int neg_mode,\n\t\t\t   phy_interface_t interface,\n\t\t\t   const unsigned long *advertising,\n\t\t\t   bool permit_pause_to_mac)\n{\n\treturn 0;\n}\n\nstatic const struct phylink_pcs_ops macb_phylink_usx_pcs_ops = {\n\t.pcs_get_state = macb_usx_pcs_get_state,\n\t.pcs_config = macb_usx_pcs_config,\n\t.pcs_link_up = macb_usx_pcs_link_up,\n};\n\nstatic const struct phylink_pcs_ops macb_phylink_pcs_ops = {\n\t.pcs_get_state = macb_pcs_get_state,\n\t.pcs_an_restart = macb_pcs_an_restart,\n\t.pcs_config = macb_pcs_config,\n};\n\nstatic void macb_mac_config(struct phylink_config *config, unsigned int mode,\n\t\t\t    const struct phylink_link_state *state)\n{\n\tstruct net_device *ndev = to_net_dev(config->dev);\n\tstruct macb *bp = netdev_priv(ndev);\n\tunsigned long flags;\n\tu32 old_ctrl, ctrl;\n\tu32 old_ncr, ncr;\n\n\tspin_lock_irqsave(&bp->lock, flags);\n\n\told_ctrl = ctrl = macb_or_gem_readl(bp, NCFGR);\n\told_ncr = ncr = macb_or_gem_readl(bp, NCR);\n\n\tif (bp->caps & MACB_CAPS_MACB_IS_EMAC) {\n\t\tif (state->interface == PHY_INTERFACE_MODE_RMII)\n\t\t\tctrl |= MACB_BIT(RM9200_RMII);\n\t} else if (macb_is_gem(bp)) {\n\t\tctrl &= ~(GEM_BIT(SGMIIEN) | GEM_BIT(PCSSEL));\n\t\tncr &= ~GEM_BIT(ENABLE_HS_MAC);\n\n\t\tif (state->interface == PHY_INTERFACE_MODE_SGMII) {\n\t\t\tctrl |= GEM_BIT(SGMIIEN) | GEM_BIT(PCSSEL);\n\t\t} else if (state->interface == PHY_INTERFACE_MODE_10GBASER) {\n\t\t\tctrl |= GEM_BIT(PCSSEL);\n\t\t\tncr |= GEM_BIT(ENABLE_HS_MAC);\n\t\t} else if (bp->caps & MACB_CAPS_MIIONRGMII &&\n\t\t\t   bp->phy_interface == PHY_INTERFACE_MODE_MII) {\n\t\t\tncr |= MACB_BIT(MIIONRGMII);\n\t\t}\n\t}\n\n\t \n\tif (old_ctrl ^ ctrl)\n\t\tmacb_or_gem_writel(bp, NCFGR, ctrl);\n\n\tif (old_ncr ^ ncr)\n\t\tmacb_or_gem_writel(bp, NCR, ncr);\n\n\t \n\tif (macb_is_gem(bp) && state->interface == PHY_INTERFACE_MODE_SGMII) {\n\t\tu32 pcsctrl, old_pcsctrl;\n\n\t\told_pcsctrl = gem_readl(bp, PCSCNTRL);\n\t\tif (mode == MLO_AN_FIXED)\n\t\t\tpcsctrl = old_pcsctrl & ~GEM_BIT(PCSAUTONEG);\n\t\telse\n\t\t\tpcsctrl = old_pcsctrl | GEM_BIT(PCSAUTONEG);\n\t\tif (old_pcsctrl != pcsctrl)\n\t\t\tgem_writel(bp, PCSCNTRL, pcsctrl);\n\t}\n\n\tspin_unlock_irqrestore(&bp->lock, flags);\n}\n\nstatic void macb_mac_link_down(struct phylink_config *config, unsigned int mode,\n\t\t\t       phy_interface_t interface)\n{\n\tstruct net_device *ndev = to_net_dev(config->dev);\n\tstruct macb *bp = netdev_priv(ndev);\n\tstruct macb_queue *queue;\n\tunsigned int q;\n\tu32 ctrl;\n\n\tif (!(bp->caps & MACB_CAPS_MACB_IS_EMAC))\n\t\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue)\n\t\t\tqueue_writel(queue, IDR,\n\t\t\t\t     bp->rx_intr_mask | MACB_TX_INT_FLAGS | MACB_BIT(HRESP));\n\n\t \n\tctrl = macb_readl(bp, NCR) & ~(MACB_BIT(RE) | MACB_BIT(TE));\n\tmacb_writel(bp, NCR, ctrl);\n\n\tnetif_tx_stop_all_queues(ndev);\n}\n\nstatic void macb_mac_link_up(struct phylink_config *config,\n\t\t\t     struct phy_device *phy,\n\t\t\t     unsigned int mode, phy_interface_t interface,\n\t\t\t     int speed, int duplex,\n\t\t\t     bool tx_pause, bool rx_pause)\n{\n\tstruct net_device *ndev = to_net_dev(config->dev);\n\tstruct macb *bp = netdev_priv(ndev);\n\tstruct macb_queue *queue;\n\tunsigned long flags;\n\tunsigned int q;\n\tu32 ctrl;\n\n\tspin_lock_irqsave(&bp->lock, flags);\n\n\tctrl = macb_or_gem_readl(bp, NCFGR);\n\n\tctrl &= ~(MACB_BIT(SPD) | MACB_BIT(FD));\n\n\tif (speed == SPEED_100)\n\t\tctrl |= MACB_BIT(SPD);\n\n\tif (duplex)\n\t\tctrl |= MACB_BIT(FD);\n\n\tif (!(bp->caps & MACB_CAPS_MACB_IS_EMAC)) {\n\t\tctrl &= ~MACB_BIT(PAE);\n\t\tif (macb_is_gem(bp)) {\n\t\t\tctrl &= ~GEM_BIT(GBE);\n\n\t\t\tif (speed == SPEED_1000)\n\t\t\t\tctrl |= GEM_BIT(GBE);\n\t\t}\n\n\t\tif (rx_pause)\n\t\t\tctrl |= MACB_BIT(PAE);\n\n\t\t \n\t\tbp->macbgem_ops.mog_init_rings(bp);\n\t\tmacb_init_buffers(bp);\n\n\t\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue)\n\t\t\tqueue_writel(queue, IER,\n\t\t\t\t     bp->rx_intr_mask | MACB_TX_INT_FLAGS | MACB_BIT(HRESP));\n\t}\n\n\tmacb_or_gem_writel(bp, NCFGR, ctrl);\n\n\tif (bp->phy_interface == PHY_INTERFACE_MODE_10GBASER)\n\t\tgem_writel(bp, HS_MAC_CONFIG, GEM_BFINS(HS_MAC_SPEED, HS_SPEED_10000M,\n\t\t\t\t\t\t\tgem_readl(bp, HS_MAC_CONFIG)));\n\n\tspin_unlock_irqrestore(&bp->lock, flags);\n\n\tif (!(bp->caps & MACB_CAPS_MACB_IS_EMAC))\n\t\tmacb_set_tx_clk(bp, speed);\n\n\t \n\tctrl = macb_readl(bp, NCR);\n\tif (gem_has_ptp(bp))\n\t\tctrl |= MACB_BIT(PTPUNI);\n\n\tmacb_writel(bp, NCR, ctrl | MACB_BIT(RE) | MACB_BIT(TE));\n\n\tnetif_tx_wake_all_queues(ndev);\n}\n\nstatic struct phylink_pcs *macb_mac_select_pcs(struct phylink_config *config,\n\t\t\t\t\t       phy_interface_t interface)\n{\n\tstruct net_device *ndev = to_net_dev(config->dev);\n\tstruct macb *bp = netdev_priv(ndev);\n\n\tif (interface == PHY_INTERFACE_MODE_10GBASER)\n\t\treturn &bp->phylink_usx_pcs;\n\telse if (interface == PHY_INTERFACE_MODE_SGMII)\n\t\treturn &bp->phylink_sgmii_pcs;\n\telse\n\t\treturn NULL;\n}\n\nstatic const struct phylink_mac_ops macb_phylink_ops = {\n\t.mac_select_pcs = macb_mac_select_pcs,\n\t.mac_config = macb_mac_config,\n\t.mac_link_down = macb_mac_link_down,\n\t.mac_link_up = macb_mac_link_up,\n};\n\nstatic bool macb_phy_handle_exists(struct device_node *dn)\n{\n\tdn = of_parse_phandle(dn, \"phy-handle\", 0);\n\tof_node_put(dn);\n\treturn dn != NULL;\n}\n\nstatic int macb_phylink_connect(struct macb *bp)\n{\n\tstruct device_node *dn = bp->pdev->dev.of_node;\n\tstruct net_device *dev = bp->dev;\n\tstruct phy_device *phydev;\n\tint ret;\n\n\tif (dn)\n\t\tret = phylink_of_phy_connect(bp->phylink, dn, 0);\n\n\tif (!dn || (ret && !macb_phy_handle_exists(dn))) {\n\t\tphydev = phy_find_first(bp->mii_bus);\n\t\tif (!phydev) {\n\t\t\tnetdev_err(dev, \"no PHY found\\n\");\n\t\t\treturn -ENXIO;\n\t\t}\n\n\t\t \n\t\tret = phylink_connect_phy(bp->phylink, phydev);\n\t}\n\n\tif (ret) {\n\t\tnetdev_err(dev, \"Could not attach PHY (%d)\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tphylink_start(bp->phylink);\n\n\treturn 0;\n}\n\nstatic void macb_get_pcs_fixed_state(struct phylink_config *config,\n\t\t\t\t     struct phylink_link_state *state)\n{\n\tstruct net_device *ndev = to_net_dev(config->dev);\n\tstruct macb *bp = netdev_priv(ndev);\n\n\tstate->link = (macb_readl(bp, NSR) & MACB_BIT(NSR_LINK)) != 0;\n}\n\n \nstatic int macb_mii_probe(struct net_device *dev)\n{\n\tstruct macb *bp = netdev_priv(dev);\n\n\tbp->phylink_sgmii_pcs.ops = &macb_phylink_pcs_ops;\n\tbp->phylink_sgmii_pcs.neg_mode = true;\n\tbp->phylink_usx_pcs.ops = &macb_phylink_usx_pcs_ops;\n\tbp->phylink_usx_pcs.neg_mode = true;\n\n\tbp->phylink_config.dev = &dev->dev;\n\tbp->phylink_config.type = PHYLINK_NETDEV;\n\tbp->phylink_config.mac_managed_pm = true;\n\n\tif (bp->phy_interface == PHY_INTERFACE_MODE_SGMII) {\n\t\tbp->phylink_config.poll_fixed_state = true;\n\t\tbp->phylink_config.get_fixed_state = macb_get_pcs_fixed_state;\n\t}\n\n\tbp->phylink_config.mac_capabilities = MAC_ASYM_PAUSE |\n\t\tMAC_10 | MAC_100;\n\n\t__set_bit(PHY_INTERFACE_MODE_MII,\n\t\t  bp->phylink_config.supported_interfaces);\n\t__set_bit(PHY_INTERFACE_MODE_RMII,\n\t\t  bp->phylink_config.supported_interfaces);\n\n\t \n\tif (macb_is_gem(bp) && (bp->caps & MACB_CAPS_GIGABIT_MODE_AVAILABLE)) {\n\t\tbp->phylink_config.mac_capabilities |= MAC_1000FD;\n\t\tif (!(bp->caps & MACB_CAPS_NO_GIGABIT_HALF))\n\t\t\tbp->phylink_config.mac_capabilities |= MAC_1000HD;\n\n\t\t__set_bit(PHY_INTERFACE_MODE_GMII,\n\t\t\t  bp->phylink_config.supported_interfaces);\n\t\tphy_interface_set_rgmii(bp->phylink_config.supported_interfaces);\n\n\t\tif (bp->caps & MACB_CAPS_PCS)\n\t\t\t__set_bit(PHY_INTERFACE_MODE_SGMII,\n\t\t\t\t  bp->phylink_config.supported_interfaces);\n\n\t\tif (bp->caps & MACB_CAPS_HIGH_SPEED) {\n\t\t\t__set_bit(PHY_INTERFACE_MODE_10GBASER,\n\t\t\t\t  bp->phylink_config.supported_interfaces);\n\t\t\tbp->phylink_config.mac_capabilities |= MAC_10000FD;\n\t\t}\n\t}\n\n\tbp->phylink = phylink_create(&bp->phylink_config, bp->pdev->dev.fwnode,\n\t\t\t\t     bp->phy_interface, &macb_phylink_ops);\n\tif (IS_ERR(bp->phylink)) {\n\t\tnetdev_err(dev, \"Could not create a phylink instance (%ld)\\n\",\n\t\t\t   PTR_ERR(bp->phylink));\n\t\treturn PTR_ERR(bp->phylink);\n\t}\n\n\treturn 0;\n}\n\nstatic int macb_mdiobus_register(struct macb *bp)\n{\n\tstruct device_node *child, *np = bp->pdev->dev.of_node;\n\n\t \n\tchild = of_get_child_by_name(np, \"mdio\");\n\tif (child) {\n\t\tint ret = of_mdiobus_register(bp->mii_bus, child);\n\n\t\tof_node_put(child);\n\t\treturn ret;\n\t}\n\n\tif (of_phy_is_fixed_link(np))\n\t\treturn mdiobus_register(bp->mii_bus);\n\n\t \n\tfor_each_available_child_of_node(np, child)\n\t\tif (of_mdiobus_child_is_phy(child)) {\n\t\t\t \n\t\t\tof_node_put(child);\n\n\t\t\treturn of_mdiobus_register(bp->mii_bus, np);\n\t\t}\n\n\treturn mdiobus_register(bp->mii_bus);\n}\n\nstatic int macb_mii_init(struct macb *bp)\n{\n\tint err = -ENXIO;\n\n\t \n\tmacb_writel(bp, NCR, MACB_BIT(MPE));\n\n\tbp->mii_bus = mdiobus_alloc();\n\tif (!bp->mii_bus) {\n\t\terr = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\n\tbp->mii_bus->name = \"MACB_mii_bus\";\n\tbp->mii_bus->read = &macb_mdio_read_c22;\n\tbp->mii_bus->write = &macb_mdio_write_c22;\n\tbp->mii_bus->read_c45 = &macb_mdio_read_c45;\n\tbp->mii_bus->write_c45 = &macb_mdio_write_c45;\n\tsnprintf(bp->mii_bus->id, MII_BUS_ID_SIZE, \"%s-%x\",\n\t\t bp->pdev->name, bp->pdev->id);\n\tbp->mii_bus->priv = bp;\n\tbp->mii_bus->parent = &bp->pdev->dev;\n\n\tdev_set_drvdata(&bp->dev->dev, bp->mii_bus);\n\n\terr = macb_mdiobus_register(bp);\n\tif (err)\n\t\tgoto err_out_free_mdiobus;\n\n\terr = macb_mii_probe(bp->dev);\n\tif (err)\n\t\tgoto err_out_unregister_bus;\n\n\treturn 0;\n\nerr_out_unregister_bus:\n\tmdiobus_unregister(bp->mii_bus);\nerr_out_free_mdiobus:\n\tmdiobus_free(bp->mii_bus);\nerr_out:\n\treturn err;\n}\n\nstatic void macb_update_stats(struct macb *bp)\n{\n\tu32 *p = &bp->hw_stats.macb.rx_pause_frames;\n\tu32 *end = &bp->hw_stats.macb.tx_pause_frames + 1;\n\tint offset = MACB_PFR;\n\n\tWARN_ON((unsigned long)(end - p - 1) != (MACB_TPF - MACB_PFR) / 4);\n\n\tfor (; p < end; p++, offset += 4)\n\t\t*p += bp->macb_reg_readl(bp, offset);\n}\n\nstatic int macb_halt_tx(struct macb *bp)\n{\n\tunsigned long\thalt_time, timeout;\n\tu32\t\tstatus;\n\n\tmacb_writel(bp, NCR, macb_readl(bp, NCR) | MACB_BIT(THALT));\n\n\ttimeout = jiffies + usecs_to_jiffies(MACB_HALT_TIMEOUT);\n\tdo {\n\t\thalt_time = jiffies;\n\t\tstatus = macb_readl(bp, TSR);\n\t\tif (!(status & MACB_BIT(TGO)))\n\t\t\treturn 0;\n\n\t\tudelay(250);\n\t} while (time_before(halt_time, timeout));\n\n\treturn -ETIMEDOUT;\n}\n\nstatic void macb_tx_unmap(struct macb *bp, struct macb_tx_skb *tx_skb, int budget)\n{\n\tif (tx_skb->mapping) {\n\t\tif (tx_skb->mapped_as_page)\n\t\t\tdma_unmap_page(&bp->pdev->dev, tx_skb->mapping,\n\t\t\t\t       tx_skb->size, DMA_TO_DEVICE);\n\t\telse\n\t\t\tdma_unmap_single(&bp->pdev->dev, tx_skb->mapping,\n\t\t\t\t\t tx_skb->size, DMA_TO_DEVICE);\n\t\ttx_skb->mapping = 0;\n\t}\n\n\tif (tx_skb->skb) {\n\t\tnapi_consume_skb(tx_skb->skb, budget);\n\t\ttx_skb->skb = NULL;\n\t}\n}\n\nstatic void macb_set_addr(struct macb *bp, struct macb_dma_desc *desc, dma_addr_t addr)\n{\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\tstruct macb_dma_desc_64 *desc_64;\n\n\tif (bp->hw_dma_cap & HW_DMA_CAP_64B) {\n\t\tdesc_64 = macb_64b_desc(bp, desc);\n\t\tdesc_64->addrh = upper_32_bits(addr);\n\t\t \n\t\tdma_wmb();\n\t}\n#endif\n\tdesc->addr = lower_32_bits(addr);\n}\n\nstatic dma_addr_t macb_get_addr(struct macb *bp, struct macb_dma_desc *desc)\n{\n\tdma_addr_t addr = 0;\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\tstruct macb_dma_desc_64 *desc_64;\n\n\tif (bp->hw_dma_cap & HW_DMA_CAP_64B) {\n\t\tdesc_64 = macb_64b_desc(bp, desc);\n\t\taddr = ((u64)(desc_64->addrh) << 32);\n\t}\n#endif\n\taddr |= MACB_BF(RX_WADDR, MACB_BFEXT(RX_WADDR, desc->addr));\n#ifdef CONFIG_MACB_USE_HWSTAMP\n\tif (bp->hw_dma_cap & HW_DMA_CAP_PTP)\n\t\taddr &= ~GEM_BIT(DMA_RXVALID);\n#endif\n\treturn addr;\n}\n\nstatic void macb_tx_error_task(struct work_struct *work)\n{\n\tstruct macb_queue\t*queue = container_of(work, struct macb_queue,\n\t\t\t\t\t\t      tx_error_task);\n\tbool\t\t\thalt_timeout = false;\n\tstruct macb\t\t*bp = queue->bp;\n\tstruct macb_tx_skb\t*tx_skb;\n\tstruct macb_dma_desc\t*desc;\n\tstruct sk_buff\t\t*skb;\n\tunsigned int\t\ttail;\n\tunsigned long\t\tflags;\n\n\tnetdev_vdbg(bp->dev, \"macb_tx_error_task: q = %u, t = %u, h = %u\\n\",\n\t\t    (unsigned int)(queue - bp->queues),\n\t\t    queue->tx_tail, queue->tx_head);\n\n\t \n\tnapi_disable(&queue->napi_tx);\n\tspin_lock_irqsave(&bp->lock, flags);\n\n\t \n\tnetif_tx_stop_all_queues(bp->dev);\n\n\t \n\tif (macb_halt_tx(bp)) {\n\t\tnetdev_err(bp->dev, \"BUG: halt tx timed out\\n\");\n\t\tmacb_writel(bp, NCR, macb_readl(bp, NCR) & (~MACB_BIT(TE)));\n\t\thalt_timeout = true;\n\t}\n\n\t \n\tfor (tail = queue->tx_tail; tail != queue->tx_head; tail++) {\n\t\tu32\tctrl;\n\n\t\tdesc = macb_tx_desc(queue, tail);\n\t\tctrl = desc->ctrl;\n\t\ttx_skb = macb_tx_skb(queue, tail);\n\t\tskb = tx_skb->skb;\n\n\t\tif (ctrl & MACB_BIT(TX_USED)) {\n\t\t\t \n\t\t\twhile (!skb) {\n\t\t\t\tmacb_tx_unmap(bp, tx_skb, 0);\n\t\t\t\ttail++;\n\t\t\t\ttx_skb = macb_tx_skb(queue, tail);\n\t\t\t\tskb = tx_skb->skb;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (!(ctrl & MACB_BIT(TX_BUF_EXHAUSTED))) {\n\t\t\t\tnetdev_vdbg(bp->dev, \"txerr skb %u (data %p) TX complete\\n\",\n\t\t\t\t\t    macb_tx_ring_wrap(bp, tail),\n\t\t\t\t\t    skb->data);\n\t\t\t\tbp->dev->stats.tx_packets++;\n\t\t\t\tqueue->stats.tx_packets++;\n\t\t\t\tbp->dev->stats.tx_bytes += skb->len;\n\t\t\t\tqueue->stats.tx_bytes += skb->len;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tif (ctrl & MACB_BIT(TX_BUF_EXHAUSTED))\n\t\t\t\tnetdev_err(bp->dev,\n\t\t\t\t\t   \"BUG: TX buffers exhausted mid-frame\\n\");\n\n\t\t\tdesc->ctrl = ctrl | MACB_BIT(TX_USED);\n\t\t}\n\n\t\tmacb_tx_unmap(bp, tx_skb, 0);\n\t}\n\n\t \n\tdesc = macb_tx_desc(queue, 0);\n\tmacb_set_addr(bp, desc, 0);\n\tdesc->ctrl = MACB_BIT(TX_USED);\n\n\t \n\twmb();\n\n\t \n\tqueue_writel(queue, TBQP, lower_32_bits(queue->tx_ring_dma));\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\tif (bp->hw_dma_cap & HW_DMA_CAP_64B)\n\t\tqueue_writel(queue, TBQPH, upper_32_bits(queue->tx_ring_dma));\n#endif\n\t \n\tqueue->tx_head = 0;\n\tqueue->tx_tail = 0;\n\n\t \n\tmacb_writel(bp, TSR, macb_readl(bp, TSR));\n\tqueue_writel(queue, IER, MACB_TX_INT_FLAGS);\n\n\tif (halt_timeout)\n\t\tmacb_writel(bp, NCR, macb_readl(bp, NCR) | MACB_BIT(TE));\n\n\t \n\tnetif_tx_start_all_queues(bp->dev);\n\tmacb_writel(bp, NCR, macb_readl(bp, NCR) | MACB_BIT(TSTART));\n\n\tspin_unlock_irqrestore(&bp->lock, flags);\n\tnapi_enable(&queue->napi_tx);\n}\n\nstatic bool ptp_one_step_sync(struct sk_buff *skb)\n{\n\tstruct ptp_header *hdr;\n\tunsigned int ptp_class;\n\tu8 msgtype;\n\n\t \n\tif (likely(!(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)))\n\t\tgoto not_oss;\n\n\t \n\tptp_class = ptp_classify_raw(skb);\n\tif (ptp_class == PTP_CLASS_NONE)\n\t\tgoto not_oss;\n\n\thdr = ptp_parse_header(skb, ptp_class);\n\tif (!hdr)\n\t\tgoto not_oss;\n\n\tif (hdr->flag_field[0] & PTP_FLAG_TWOSTEP)\n\t\tgoto not_oss;\n\n\tmsgtype = ptp_get_msgtype(hdr, ptp_class);\n\tif (msgtype == PTP_MSGTYPE_SYNC)\n\t\treturn true;\n\nnot_oss:\n\treturn false;\n}\n\nstatic int macb_tx_complete(struct macb_queue *queue, int budget)\n{\n\tstruct macb *bp = queue->bp;\n\tu16 queue_index = queue - bp->queues;\n\tunsigned int tail;\n\tunsigned int head;\n\tint packets = 0;\n\n\tspin_lock(&queue->tx_ptr_lock);\n\thead = queue->tx_head;\n\tfor (tail = queue->tx_tail; tail != head && packets < budget; tail++) {\n\t\tstruct macb_tx_skb\t*tx_skb;\n\t\tstruct sk_buff\t\t*skb;\n\t\tstruct macb_dma_desc\t*desc;\n\t\tu32\t\t\tctrl;\n\n\t\tdesc = macb_tx_desc(queue, tail);\n\n\t\t \n\t\trmb();\n\n\t\tctrl = desc->ctrl;\n\n\t\t \n\t\tif (!(ctrl & MACB_BIT(TX_USED)))\n\t\t\tbreak;\n\n\t\t \n\t\tfor (;; tail++) {\n\t\t\ttx_skb = macb_tx_skb(queue, tail);\n\t\t\tskb = tx_skb->skb;\n\n\t\t\t \n\t\t\tif (skb) {\n\t\t\t\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&\n\t\t\t\t    !ptp_one_step_sync(skb))\n\t\t\t\t\tgem_ptp_do_txstamp(bp, skb, desc);\n\n\t\t\t\tnetdev_vdbg(bp->dev, \"skb %u (data %p) TX complete\\n\",\n\t\t\t\t\t    macb_tx_ring_wrap(bp, tail),\n\t\t\t\t\t    skb->data);\n\t\t\t\tbp->dev->stats.tx_packets++;\n\t\t\t\tqueue->stats.tx_packets++;\n\t\t\t\tbp->dev->stats.tx_bytes += skb->len;\n\t\t\t\tqueue->stats.tx_bytes += skb->len;\n\t\t\t\tpackets++;\n\t\t\t}\n\n\t\t\t \n\t\t\tmacb_tx_unmap(bp, tx_skb, budget);\n\n\t\t\t \n\t\t\tif (skb)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tqueue->tx_tail = tail;\n\tif (__netif_subqueue_stopped(bp->dev, queue_index) &&\n\t    CIRC_CNT(queue->tx_head, queue->tx_tail,\n\t\t     bp->tx_ring_size) <= MACB_TX_WAKEUP_THRESH(bp))\n\t\tnetif_wake_subqueue(bp->dev, queue_index);\n\tspin_unlock(&queue->tx_ptr_lock);\n\n\treturn packets;\n}\n\nstatic void gem_rx_refill(struct macb_queue *queue)\n{\n\tunsigned int\t\tentry;\n\tstruct sk_buff\t\t*skb;\n\tdma_addr_t\t\tpaddr;\n\tstruct macb *bp = queue->bp;\n\tstruct macb_dma_desc *desc;\n\n\twhile (CIRC_SPACE(queue->rx_prepared_head, queue->rx_tail,\n\t\t\tbp->rx_ring_size) > 0) {\n\t\tentry = macb_rx_ring_wrap(bp, queue->rx_prepared_head);\n\n\t\t \n\t\trmb();\n\n\t\tdesc = macb_rx_desc(queue, entry);\n\n\t\tif (!queue->rx_skbuff[entry]) {\n\t\t\t \n\t\t\tskb = netdev_alloc_skb(bp->dev, bp->rx_buffer_size);\n\t\t\tif (unlikely(!skb)) {\n\t\t\t\tnetdev_err(bp->dev,\n\t\t\t\t\t   \"Unable to allocate sk_buff\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tpaddr = dma_map_single(&bp->pdev->dev, skb->data,\n\t\t\t\t\t       bp->rx_buffer_size,\n\t\t\t\t\t       DMA_FROM_DEVICE);\n\t\t\tif (dma_mapping_error(&bp->pdev->dev, paddr)) {\n\t\t\t\tdev_kfree_skb(skb);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tqueue->rx_skbuff[entry] = skb;\n\n\t\t\tif (entry == bp->rx_ring_size - 1)\n\t\t\t\tpaddr |= MACB_BIT(RX_WRAP);\n\t\t\tdesc->ctrl = 0;\n\t\t\t \n\t\t\tdma_wmb();\n\t\t\tmacb_set_addr(bp, desc, paddr);\n\n\t\t\t \n\t\t\tskb_reserve(skb, NET_IP_ALIGN);\n\t\t} else {\n\t\t\tdesc->ctrl = 0;\n\t\t\tdma_wmb();\n\t\t\tdesc->addr &= ~MACB_BIT(RX_USED);\n\t\t}\n\t\tqueue->rx_prepared_head++;\n\t}\n\n\t \n\twmb();\n\n\tnetdev_vdbg(bp->dev, \"rx ring: queue: %p, prepared head %d, tail %d\\n\",\n\t\t\tqueue, queue->rx_prepared_head, queue->rx_tail);\n}\n\n \nstatic void discard_partial_frame(struct macb_queue *queue, unsigned int begin,\n\t\t\t\t  unsigned int end)\n{\n\tunsigned int frag;\n\n\tfor (frag = begin; frag != end; frag++) {\n\t\tstruct macb_dma_desc *desc = macb_rx_desc(queue, frag);\n\n\t\tdesc->addr &= ~MACB_BIT(RX_USED);\n\t}\n\n\t \n\twmb();\n\n\t \n}\n\nstatic int gem_rx(struct macb_queue *queue, struct napi_struct *napi,\n\t\t  int budget)\n{\n\tstruct macb *bp = queue->bp;\n\tunsigned int\t\tlen;\n\tunsigned int\t\tentry;\n\tstruct sk_buff\t\t*skb;\n\tstruct macb_dma_desc\t*desc;\n\tint\t\t\tcount = 0;\n\n\twhile (count < budget) {\n\t\tu32 ctrl;\n\t\tdma_addr_t addr;\n\t\tbool rxused;\n\n\t\tentry = macb_rx_ring_wrap(bp, queue->rx_tail);\n\t\tdesc = macb_rx_desc(queue, entry);\n\n\t\t \n\t\trmb();\n\n\t\trxused = (desc->addr & MACB_BIT(RX_USED)) ? true : false;\n\t\taddr = macb_get_addr(bp, desc);\n\n\t\tif (!rxused)\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\n\t\tctrl = desc->ctrl;\n\n\t\tqueue->rx_tail++;\n\t\tcount++;\n\n\t\tif (!(ctrl & MACB_BIT(RX_SOF) && ctrl & MACB_BIT(RX_EOF))) {\n\t\t\tnetdev_err(bp->dev,\n\t\t\t\t   \"not whole frame pointed by descriptor\\n\");\n\t\t\tbp->dev->stats.rx_dropped++;\n\t\t\tqueue->stats.rx_dropped++;\n\t\t\tbreak;\n\t\t}\n\t\tskb = queue->rx_skbuff[entry];\n\t\tif (unlikely(!skb)) {\n\t\t\tnetdev_err(bp->dev,\n\t\t\t\t   \"inconsistent Rx descriptor chain\\n\");\n\t\t\tbp->dev->stats.rx_dropped++;\n\t\t\tqueue->stats.rx_dropped++;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tqueue->rx_skbuff[entry] = NULL;\n\t\tlen = ctrl & bp->rx_frm_len_mask;\n\n\t\tnetdev_vdbg(bp->dev, \"gem_rx %u (len %u)\\n\", entry, len);\n\n\t\tskb_put(skb, len);\n\t\tdma_unmap_single(&bp->pdev->dev, addr,\n\t\t\t\t bp->rx_buffer_size, DMA_FROM_DEVICE);\n\n\t\tskb->protocol = eth_type_trans(skb, bp->dev);\n\t\tskb_checksum_none_assert(skb);\n\t\tif (bp->dev->features & NETIF_F_RXCSUM &&\n\t\t    !(bp->dev->flags & IFF_PROMISC) &&\n\t\t    GEM_BFEXT(RX_CSUM, ctrl) & GEM_RX_CSUM_CHECKED_MASK)\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\t\tbp->dev->stats.rx_packets++;\n\t\tqueue->stats.rx_packets++;\n\t\tbp->dev->stats.rx_bytes += skb->len;\n\t\tqueue->stats.rx_bytes += skb->len;\n\n\t\tgem_ptp_do_rxstamp(bp, skb, desc);\n\n#if defined(DEBUG) && defined(VERBOSE_DEBUG)\n\t\tnetdev_vdbg(bp->dev, \"received skb of length %u, csum: %08x\\n\",\n\t\t\t    skb->len, skb->csum);\n\t\tprint_hex_dump(KERN_DEBUG, \" mac: \", DUMP_PREFIX_ADDRESS, 16, 1,\n\t\t\t       skb_mac_header(skb), 16, true);\n\t\tprint_hex_dump(KERN_DEBUG, \"data: \", DUMP_PREFIX_ADDRESS, 16, 1,\n\t\t\t       skb->data, 32, true);\n#endif\n\n\t\tnapi_gro_receive(napi, skb);\n\t}\n\n\tgem_rx_refill(queue);\n\n\treturn count;\n}\n\nstatic int macb_rx_frame(struct macb_queue *queue, struct napi_struct *napi,\n\t\t\t unsigned int first_frag, unsigned int last_frag)\n{\n\tunsigned int len;\n\tunsigned int frag;\n\tunsigned int offset;\n\tstruct sk_buff *skb;\n\tstruct macb_dma_desc *desc;\n\tstruct macb *bp = queue->bp;\n\n\tdesc = macb_rx_desc(queue, last_frag);\n\tlen = desc->ctrl & bp->rx_frm_len_mask;\n\n\tnetdev_vdbg(bp->dev, \"macb_rx_frame frags %u - %u (len %u)\\n\",\n\t\tmacb_rx_ring_wrap(bp, first_frag),\n\t\tmacb_rx_ring_wrap(bp, last_frag), len);\n\n\t \n\tskb = netdev_alloc_skb(bp->dev, len + NET_IP_ALIGN);\n\tif (!skb) {\n\t\tbp->dev->stats.rx_dropped++;\n\t\tfor (frag = first_frag; ; frag++) {\n\t\t\tdesc = macb_rx_desc(queue, frag);\n\t\t\tdesc->addr &= ~MACB_BIT(RX_USED);\n\t\t\tif (frag == last_frag)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\twmb();\n\n\t\treturn 1;\n\t}\n\n\toffset = 0;\n\tlen += NET_IP_ALIGN;\n\tskb_checksum_none_assert(skb);\n\tskb_put(skb, len);\n\n\tfor (frag = first_frag; ; frag++) {\n\t\tunsigned int frag_len = bp->rx_buffer_size;\n\n\t\tif (offset + frag_len > len) {\n\t\t\tif (unlikely(frag != last_frag)) {\n\t\t\t\tdev_kfree_skb_any(skb);\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tfrag_len = len - offset;\n\t\t}\n\t\tskb_copy_to_linear_data_offset(skb, offset,\n\t\t\t\t\t       macb_rx_buffer(queue, frag),\n\t\t\t\t\t       frag_len);\n\t\toffset += bp->rx_buffer_size;\n\t\tdesc = macb_rx_desc(queue, frag);\n\t\tdesc->addr &= ~MACB_BIT(RX_USED);\n\n\t\tif (frag == last_frag)\n\t\t\tbreak;\n\t}\n\n\t \n\twmb();\n\n\t__skb_pull(skb, NET_IP_ALIGN);\n\tskb->protocol = eth_type_trans(skb, bp->dev);\n\n\tbp->dev->stats.rx_packets++;\n\tbp->dev->stats.rx_bytes += skb->len;\n\tnetdev_vdbg(bp->dev, \"received skb of length %u, csum: %08x\\n\",\n\t\t    skb->len, skb->csum);\n\tnapi_gro_receive(napi, skb);\n\n\treturn 0;\n}\n\nstatic inline void macb_init_rx_ring(struct macb_queue *queue)\n{\n\tstruct macb *bp = queue->bp;\n\tdma_addr_t addr;\n\tstruct macb_dma_desc *desc = NULL;\n\tint i;\n\n\taddr = queue->rx_buffers_dma;\n\tfor (i = 0; i < bp->rx_ring_size; i++) {\n\t\tdesc = macb_rx_desc(queue, i);\n\t\tmacb_set_addr(bp, desc, addr);\n\t\tdesc->ctrl = 0;\n\t\taddr += bp->rx_buffer_size;\n\t}\n\tdesc->addr |= MACB_BIT(RX_WRAP);\n\tqueue->rx_tail = 0;\n}\n\nstatic int macb_rx(struct macb_queue *queue, struct napi_struct *napi,\n\t\t   int budget)\n{\n\tstruct macb *bp = queue->bp;\n\tbool reset_rx_queue = false;\n\tint received = 0;\n\tunsigned int tail;\n\tint first_frag = -1;\n\n\tfor (tail = queue->rx_tail; budget > 0; tail++) {\n\t\tstruct macb_dma_desc *desc = macb_rx_desc(queue, tail);\n\t\tu32 ctrl;\n\n\t\t \n\t\trmb();\n\n\t\tif (!(desc->addr & MACB_BIT(RX_USED)))\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\n\t\tctrl = desc->ctrl;\n\n\t\tif (ctrl & MACB_BIT(RX_SOF)) {\n\t\t\tif (first_frag != -1)\n\t\t\t\tdiscard_partial_frame(queue, first_frag, tail);\n\t\t\tfirst_frag = tail;\n\t\t}\n\n\t\tif (ctrl & MACB_BIT(RX_EOF)) {\n\t\t\tint dropped;\n\n\t\t\tif (unlikely(first_frag == -1)) {\n\t\t\t\treset_rx_queue = true;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tdropped = macb_rx_frame(queue, napi, first_frag, tail);\n\t\t\tfirst_frag = -1;\n\t\t\tif (unlikely(dropped < 0)) {\n\t\t\t\treset_rx_queue = true;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!dropped) {\n\t\t\t\treceived++;\n\t\t\t\tbudget--;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (unlikely(reset_rx_queue)) {\n\t\tunsigned long flags;\n\t\tu32 ctrl;\n\n\t\tnetdev_err(bp->dev, \"RX queue corruption: reset it\\n\");\n\n\t\tspin_lock_irqsave(&bp->lock, flags);\n\n\t\tctrl = macb_readl(bp, NCR);\n\t\tmacb_writel(bp, NCR, ctrl & ~MACB_BIT(RE));\n\n\t\tmacb_init_rx_ring(queue);\n\t\tqueue_writel(queue, RBQP, queue->rx_ring_dma);\n\n\t\tmacb_writel(bp, NCR, ctrl | MACB_BIT(RE));\n\n\t\tspin_unlock_irqrestore(&bp->lock, flags);\n\t\treturn received;\n\t}\n\n\tif (first_frag != -1)\n\t\tqueue->rx_tail = first_frag;\n\telse\n\t\tqueue->rx_tail = tail;\n\n\treturn received;\n}\n\nstatic bool macb_rx_pending(struct macb_queue *queue)\n{\n\tstruct macb *bp = queue->bp;\n\tunsigned int\t\tentry;\n\tstruct macb_dma_desc\t*desc;\n\n\tentry = macb_rx_ring_wrap(bp, queue->rx_tail);\n\tdesc = macb_rx_desc(queue, entry);\n\n\t \n\trmb();\n\n\treturn (desc->addr & MACB_BIT(RX_USED)) != 0;\n}\n\nstatic int macb_rx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct macb_queue *queue = container_of(napi, struct macb_queue, napi_rx);\n\tstruct macb *bp = queue->bp;\n\tint work_done;\n\n\twork_done = bp->macbgem_ops.mog_rx(queue, napi, budget);\n\n\tnetdev_vdbg(bp->dev, \"RX poll: queue = %u, work_done = %d, budget = %d\\n\",\n\t\t    (unsigned int)(queue - bp->queues), work_done, budget);\n\n\tif (work_done < budget && napi_complete_done(napi, work_done)) {\n\t\tqueue_writel(queue, IER, bp->rx_intr_mask);\n\n\t\t \n\t\tif (macb_rx_pending(queue)) {\n\t\t\tqueue_writel(queue, IDR, bp->rx_intr_mask);\n\t\t\tif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\n\t\t\t\tqueue_writel(queue, ISR, MACB_BIT(RCOMP));\n\t\t\tnetdev_vdbg(bp->dev, \"poll: packets pending, reschedule\\n\");\n\t\t\tnapi_schedule(napi);\n\t\t}\n\t}\n\n\t \n\n\treturn work_done;\n}\n\nstatic void macb_tx_restart(struct macb_queue *queue)\n{\n\tstruct macb *bp = queue->bp;\n\tunsigned int head_idx, tbqp;\n\n\tspin_lock(&queue->tx_ptr_lock);\n\n\tif (queue->tx_head == queue->tx_tail)\n\t\tgoto out_tx_ptr_unlock;\n\n\ttbqp = queue_readl(queue, TBQP) / macb_dma_desc_get_size(bp);\n\ttbqp = macb_adj_dma_desc_idx(bp, macb_tx_ring_wrap(bp, tbqp));\n\thead_idx = macb_adj_dma_desc_idx(bp, macb_tx_ring_wrap(bp, queue->tx_head));\n\n\tif (tbqp == head_idx)\n\t\tgoto out_tx_ptr_unlock;\n\n\tspin_lock_irq(&bp->lock);\n\tmacb_writel(bp, NCR, macb_readl(bp, NCR) | MACB_BIT(TSTART));\n\tspin_unlock_irq(&bp->lock);\n\nout_tx_ptr_unlock:\n\tspin_unlock(&queue->tx_ptr_lock);\n}\n\nstatic bool macb_tx_complete_pending(struct macb_queue *queue)\n{\n\tbool retval = false;\n\n\tspin_lock(&queue->tx_ptr_lock);\n\tif (queue->tx_head != queue->tx_tail) {\n\t\t \n\t\trmb();\n\n\t\tif (macb_tx_desc(queue, queue->tx_tail)->ctrl & MACB_BIT(TX_USED))\n\t\t\tretval = true;\n\t}\n\tspin_unlock(&queue->tx_ptr_lock);\n\treturn retval;\n}\n\nstatic int macb_tx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct macb_queue *queue = container_of(napi, struct macb_queue, napi_tx);\n\tstruct macb *bp = queue->bp;\n\tint work_done;\n\n\twork_done = macb_tx_complete(queue, budget);\n\n\trmb(); \n\tif (queue->txubr_pending) {\n\t\tqueue->txubr_pending = false;\n\t\tnetdev_vdbg(bp->dev, \"poll: tx restart\\n\");\n\t\tmacb_tx_restart(queue);\n\t}\n\n\tnetdev_vdbg(bp->dev, \"TX poll: queue = %u, work_done = %d, budget = %d\\n\",\n\t\t    (unsigned int)(queue - bp->queues), work_done, budget);\n\n\tif (work_done < budget && napi_complete_done(napi, work_done)) {\n\t\tqueue_writel(queue, IER, MACB_BIT(TCOMP));\n\n\t\t \n\t\tif (macb_tx_complete_pending(queue)) {\n\t\t\tqueue_writel(queue, IDR, MACB_BIT(TCOMP));\n\t\t\tif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\n\t\t\t\tqueue_writel(queue, ISR, MACB_BIT(TCOMP));\n\t\t\tnetdev_vdbg(bp->dev, \"TX poll: packets pending, reschedule\\n\");\n\t\t\tnapi_schedule(napi);\n\t\t}\n\t}\n\n\treturn work_done;\n}\n\nstatic void macb_hresp_error_task(struct tasklet_struct *t)\n{\n\tstruct macb *bp = from_tasklet(bp, t, hresp_err_tasklet);\n\tstruct net_device *dev = bp->dev;\n\tstruct macb_queue *queue;\n\tunsigned int q;\n\tu32 ctrl;\n\n\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue) {\n\t\tqueue_writel(queue, IDR, bp->rx_intr_mask |\n\t\t\t\t\t MACB_TX_INT_FLAGS |\n\t\t\t\t\t MACB_BIT(HRESP));\n\t}\n\tctrl = macb_readl(bp, NCR);\n\tctrl &= ~(MACB_BIT(RE) | MACB_BIT(TE));\n\tmacb_writel(bp, NCR, ctrl);\n\n\tnetif_tx_stop_all_queues(dev);\n\tnetif_carrier_off(dev);\n\n\tbp->macbgem_ops.mog_init_rings(bp);\n\n\t \n\tmacb_init_buffers(bp);\n\n\t \n\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue)\n\t\tqueue_writel(queue, IER,\n\t\t\t     bp->rx_intr_mask |\n\t\t\t     MACB_TX_INT_FLAGS |\n\t\t\t     MACB_BIT(HRESP));\n\n\tctrl |= MACB_BIT(RE) | MACB_BIT(TE);\n\tmacb_writel(bp, NCR, ctrl);\n\n\tnetif_carrier_on(dev);\n\tnetif_tx_start_all_queues(dev);\n}\n\nstatic irqreturn_t macb_wol_interrupt(int irq, void *dev_id)\n{\n\tstruct macb_queue *queue = dev_id;\n\tstruct macb *bp = queue->bp;\n\tu32 status;\n\n\tstatus = queue_readl(queue, ISR);\n\n\tif (unlikely(!status))\n\t\treturn IRQ_NONE;\n\n\tspin_lock(&bp->lock);\n\n\tif (status & MACB_BIT(WOL)) {\n\t\tqueue_writel(queue, IDR, MACB_BIT(WOL));\n\t\tmacb_writel(bp, WOL, 0);\n\t\tnetdev_vdbg(bp->dev, \"MACB WoL: queue = %u, isr = 0x%08lx\\n\",\n\t\t\t    (unsigned int)(queue - bp->queues),\n\t\t\t    (unsigned long)status);\n\t\tif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\n\t\t\tqueue_writel(queue, ISR, MACB_BIT(WOL));\n\t\tpm_wakeup_event(&bp->pdev->dev, 0);\n\t}\n\n\tspin_unlock(&bp->lock);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t gem_wol_interrupt(int irq, void *dev_id)\n{\n\tstruct macb_queue *queue = dev_id;\n\tstruct macb *bp = queue->bp;\n\tu32 status;\n\n\tstatus = queue_readl(queue, ISR);\n\n\tif (unlikely(!status))\n\t\treturn IRQ_NONE;\n\n\tspin_lock(&bp->lock);\n\n\tif (status & GEM_BIT(WOL)) {\n\t\tqueue_writel(queue, IDR, GEM_BIT(WOL));\n\t\tgem_writel(bp, WOL, 0);\n\t\tnetdev_vdbg(bp->dev, \"GEM WoL: queue = %u, isr = 0x%08lx\\n\",\n\t\t\t    (unsigned int)(queue - bp->queues),\n\t\t\t    (unsigned long)status);\n\t\tif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\n\t\t\tqueue_writel(queue, ISR, GEM_BIT(WOL));\n\t\tpm_wakeup_event(&bp->pdev->dev, 0);\n\t}\n\n\tspin_unlock(&bp->lock);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t macb_interrupt(int irq, void *dev_id)\n{\n\tstruct macb_queue *queue = dev_id;\n\tstruct macb *bp = queue->bp;\n\tstruct net_device *dev = bp->dev;\n\tu32 status, ctrl;\n\n\tstatus = queue_readl(queue, ISR);\n\n\tif (unlikely(!status))\n\t\treturn IRQ_NONE;\n\n\tspin_lock(&bp->lock);\n\n\twhile (status) {\n\t\t \n\t\tif (unlikely(!netif_running(dev))) {\n\t\t\tqueue_writel(queue, IDR, -1);\n\t\t\tif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\n\t\t\t\tqueue_writel(queue, ISR, -1);\n\t\t\tbreak;\n\t\t}\n\n\t\tnetdev_vdbg(bp->dev, \"queue = %u, isr = 0x%08lx\\n\",\n\t\t\t    (unsigned int)(queue - bp->queues),\n\t\t\t    (unsigned long)status);\n\n\t\tif (status & bp->rx_intr_mask) {\n\t\t\t \n\t\t\tqueue_writel(queue, IDR, bp->rx_intr_mask);\n\t\t\tif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\n\t\t\t\tqueue_writel(queue, ISR, MACB_BIT(RCOMP));\n\n\t\t\tif (napi_schedule_prep(&queue->napi_rx)) {\n\t\t\t\tnetdev_vdbg(bp->dev, \"scheduling RX softirq\\n\");\n\t\t\t\t__napi_schedule(&queue->napi_rx);\n\t\t\t}\n\t\t}\n\n\t\tif (status & (MACB_BIT(TCOMP) |\n\t\t\t      MACB_BIT(TXUBR))) {\n\t\t\tqueue_writel(queue, IDR, MACB_BIT(TCOMP));\n\t\t\tif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\n\t\t\t\tqueue_writel(queue, ISR, MACB_BIT(TCOMP) |\n\t\t\t\t\t\t\t MACB_BIT(TXUBR));\n\n\t\t\tif (status & MACB_BIT(TXUBR)) {\n\t\t\t\tqueue->txubr_pending = true;\n\t\t\t\twmb();  \n\t\t\t}\n\n\t\t\tif (napi_schedule_prep(&queue->napi_tx)) {\n\t\t\t\tnetdev_vdbg(bp->dev, \"scheduling TX softirq\\n\");\n\t\t\t\t__napi_schedule(&queue->napi_tx);\n\t\t\t}\n\t\t}\n\n\t\tif (unlikely(status & (MACB_TX_ERR_FLAGS))) {\n\t\t\tqueue_writel(queue, IDR, MACB_TX_INT_FLAGS);\n\t\t\tschedule_work(&queue->tx_error_task);\n\n\t\t\tif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\n\t\t\t\tqueue_writel(queue, ISR, MACB_TX_ERR_FLAGS);\n\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\n\t\t \n\t\tif (status & MACB_BIT(RXUBR)) {\n\t\t\tctrl = macb_readl(bp, NCR);\n\t\t\tmacb_writel(bp, NCR, ctrl & ~MACB_BIT(RE));\n\t\t\twmb();\n\t\t\tmacb_writel(bp, NCR, ctrl | MACB_BIT(RE));\n\n\t\t\tif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\n\t\t\t\tqueue_writel(queue, ISR, MACB_BIT(RXUBR));\n\t\t}\n\n\t\tif (status & MACB_BIT(ISR_ROVR)) {\n\t\t\t \n\t\t\tif (macb_is_gem(bp))\n\t\t\t\tbp->hw_stats.gem.rx_overruns++;\n\t\t\telse\n\t\t\t\tbp->hw_stats.macb.rx_overruns++;\n\n\t\t\tif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\n\t\t\t\tqueue_writel(queue, ISR, MACB_BIT(ISR_ROVR));\n\t\t}\n\n\t\tif (status & MACB_BIT(HRESP)) {\n\t\t\ttasklet_schedule(&bp->hresp_err_tasklet);\n\t\t\tnetdev_err(dev, \"DMA bus error: HRESP not OK\\n\");\n\n\t\t\tif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\n\t\t\t\tqueue_writel(queue, ISR, MACB_BIT(HRESP));\n\t\t}\n\t\tstatus = queue_readl(queue, ISR);\n\t}\n\n\tspin_unlock(&bp->lock);\n\n\treturn IRQ_HANDLED;\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\n \nstatic void macb_poll_controller(struct net_device *dev)\n{\n\tstruct macb *bp = netdev_priv(dev);\n\tstruct macb_queue *queue;\n\tunsigned long flags;\n\tunsigned int q;\n\n\tlocal_irq_save(flags);\n\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue)\n\t\tmacb_interrupt(dev->irq, queue);\n\tlocal_irq_restore(flags);\n}\n#endif\n\nstatic unsigned int macb_tx_map(struct macb *bp,\n\t\t\t\tstruct macb_queue *queue,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tunsigned int hdrlen)\n{\n\tdma_addr_t mapping;\n\tunsigned int len, entry, i, tx_head = queue->tx_head;\n\tstruct macb_tx_skb *tx_skb = NULL;\n\tstruct macb_dma_desc *desc;\n\tunsigned int offset, size, count = 0;\n\tunsigned int f, nr_frags = skb_shinfo(skb)->nr_frags;\n\tunsigned int eof = 1, mss_mfs = 0;\n\tu32 ctrl, lso_ctrl = 0, seq_ctrl = 0;\n\n\t \n\tif (skb_shinfo(skb)->gso_size != 0) {\n\t\tif (ip_hdr(skb)->protocol == IPPROTO_UDP)\n\t\t\t \n\t\t\tlso_ctrl = MACB_LSO_UFO_ENABLE;\n\t\telse\n\t\t\t \n\t\t\tlso_ctrl = MACB_LSO_TSO_ENABLE;\n\t}\n\n\t \n\tlen = skb_headlen(skb);\n\n\t \n\tsize = hdrlen;\n\n\toffset = 0;\n\twhile (len) {\n\t\tentry = macb_tx_ring_wrap(bp, tx_head);\n\t\ttx_skb = &queue->tx_skb[entry];\n\n\t\tmapping = dma_map_single(&bp->pdev->dev,\n\t\t\t\t\t skb->data + offset,\n\t\t\t\t\t size, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&bp->pdev->dev, mapping))\n\t\t\tgoto dma_error;\n\n\t\t \n\t\ttx_skb->skb = NULL;\n\t\ttx_skb->mapping = mapping;\n\t\ttx_skb->size = size;\n\t\ttx_skb->mapped_as_page = false;\n\n\t\tlen -= size;\n\t\toffset += size;\n\t\tcount++;\n\t\ttx_head++;\n\n\t\tsize = min(len, bp->max_tx_length);\n\t}\n\n\t \n\tfor (f = 0; f < nr_frags; f++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[f];\n\n\t\tlen = skb_frag_size(frag);\n\t\toffset = 0;\n\t\twhile (len) {\n\t\t\tsize = min(len, bp->max_tx_length);\n\t\t\tentry = macb_tx_ring_wrap(bp, tx_head);\n\t\t\ttx_skb = &queue->tx_skb[entry];\n\n\t\t\tmapping = skb_frag_dma_map(&bp->pdev->dev, frag,\n\t\t\t\t\t\t   offset, size, DMA_TO_DEVICE);\n\t\t\tif (dma_mapping_error(&bp->pdev->dev, mapping))\n\t\t\t\tgoto dma_error;\n\n\t\t\t \n\t\t\ttx_skb->skb = NULL;\n\t\t\ttx_skb->mapping = mapping;\n\t\t\ttx_skb->size = size;\n\t\t\ttx_skb->mapped_as_page = true;\n\n\t\t\tlen -= size;\n\t\t\toffset += size;\n\t\t\tcount++;\n\t\t\ttx_head++;\n\t\t}\n\t}\n\n\t \n\tif (unlikely(!tx_skb)) {\n\t\tnetdev_err(bp->dev, \"BUG! empty skb!\\n\");\n\t\treturn 0;\n\t}\n\n\t \n\ttx_skb->skb = skb;\n\n\t \n\n\t \n\ti = tx_head;\n\tentry = macb_tx_ring_wrap(bp, i);\n\tctrl = MACB_BIT(TX_USED);\n\tdesc = macb_tx_desc(queue, entry);\n\tdesc->ctrl = ctrl;\n\n\tif (lso_ctrl) {\n\t\tif (lso_ctrl == MACB_LSO_UFO_ENABLE)\n\t\t\t \n\t\t\tmss_mfs = skb_shinfo(skb)->gso_size +\n\t\t\t\t\tskb_transport_offset(skb) +\n\t\t\t\t\tETH_FCS_LEN;\n\t\telse   {\n\t\t\tmss_mfs = skb_shinfo(skb)->gso_size;\n\t\t\t \n\t\t\tseq_ctrl = 0;\n\t\t}\n\t}\n\n\tdo {\n\t\ti--;\n\t\tentry = macb_tx_ring_wrap(bp, i);\n\t\ttx_skb = &queue->tx_skb[entry];\n\t\tdesc = macb_tx_desc(queue, entry);\n\n\t\tctrl = (u32)tx_skb->size;\n\t\tif (eof) {\n\t\t\tctrl |= MACB_BIT(TX_LAST);\n\t\t\teof = 0;\n\t\t}\n\t\tif (unlikely(entry == (bp->tx_ring_size - 1)))\n\t\t\tctrl |= MACB_BIT(TX_WRAP);\n\n\t\t \n\t\tif (i == queue->tx_head) {\n\t\t\tctrl |= MACB_BF(TX_LSO, lso_ctrl);\n\t\t\tctrl |= MACB_BF(TX_TCP_SEQ_SRC, seq_ctrl);\n\t\t\tif ((bp->dev->features & NETIF_F_HW_CSUM) &&\n\t\t\t    skb->ip_summed != CHECKSUM_PARTIAL && !lso_ctrl &&\n\t\t\t    !ptp_one_step_sync(skb))\n\t\t\t\tctrl |= MACB_BIT(TX_NOCRC);\n\t\t} else\n\t\t\t \n\t\t\tctrl |= MACB_BF(MSS_MFS, mss_mfs);\n\n\t\t \n\t\tmacb_set_addr(bp, desc, tx_skb->mapping);\n\t\t \n\t\twmb();\n\t\tdesc->ctrl = ctrl;\n\t} while (i != queue->tx_head);\n\n\tqueue->tx_head = tx_head;\n\n\treturn count;\n\ndma_error:\n\tnetdev_err(bp->dev, \"TX DMA map failed\\n\");\n\n\tfor (i = queue->tx_head; i != tx_head; i++) {\n\t\ttx_skb = macb_tx_skb(queue, i);\n\n\t\tmacb_tx_unmap(bp, tx_skb, 0);\n\t}\n\n\treturn 0;\n}\n\nstatic netdev_features_t macb_features_check(struct sk_buff *skb,\n\t\t\t\t\t     struct net_device *dev,\n\t\t\t\t\t     netdev_features_t features)\n{\n\tunsigned int nr_frags, f;\n\tunsigned int hdrlen;\n\n\t \n\n\t \n\tif (!skb_is_nonlinear(skb) || (ip_hdr(skb)->protocol != IPPROTO_UDP))\n\t\treturn features;\n\n\t \n\thdrlen = skb_transport_offset(skb);\n\n\t \n\tif (!IS_ALIGNED(skb_headlen(skb) - hdrlen, MACB_TX_LEN_ALIGN))\n\t\treturn features & ~MACB_NETIF_LSO;\n\n\tnr_frags = skb_shinfo(skb)->nr_frags;\n\t \n\tnr_frags--;\n\tfor (f = 0; f < nr_frags; f++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[f];\n\n\t\tif (!IS_ALIGNED(skb_frag_size(frag), MACB_TX_LEN_ALIGN))\n\t\t\treturn features & ~MACB_NETIF_LSO;\n\t}\n\treturn features;\n}\n\nstatic inline int macb_clear_csum(struct sk_buff *skb)\n{\n\t \n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\t \n\tif (unlikely(skb_cow_head(skb, 0)))\n\t\treturn -1;\n\n\t \n\t*(__sum16 *)(skb_checksum_start(skb) + skb->csum_offset) = 0;\n\treturn 0;\n}\n\nstatic int macb_pad_and_fcs(struct sk_buff **skb, struct net_device *ndev)\n{\n\tbool cloned = skb_cloned(*skb) || skb_header_cloned(*skb) ||\n\t\t      skb_is_nonlinear(*skb);\n\tint padlen = ETH_ZLEN - (*skb)->len;\n\tint tailroom = skb_tailroom(*skb);\n\tstruct sk_buff *nskb;\n\tu32 fcs;\n\n\tif (!(ndev->features & NETIF_F_HW_CSUM) ||\n\t    !((*skb)->ip_summed != CHECKSUM_PARTIAL) ||\n\t    skb_shinfo(*skb)->gso_size || ptp_one_step_sync(*skb))\n\t\treturn 0;\n\n\tif (padlen <= 0) {\n\t\t \n\t\tif (tailroom >= ETH_FCS_LEN)\n\t\t\tgoto add_fcs;\n\t\t \n\t\telse\n\t\t\tpadlen = ETH_FCS_LEN;\n\t} else {\n\t\t \n\t\tpadlen += ETH_FCS_LEN;\n\t}\n\n\tif (cloned || tailroom < padlen) {\n\t\tnskb = skb_copy_expand(*skb, 0, padlen, GFP_ATOMIC);\n\t\tif (!nskb)\n\t\t\treturn -ENOMEM;\n\n\t\tdev_consume_skb_any(*skb);\n\t\t*skb = nskb;\n\t}\n\n\tif (padlen > ETH_FCS_LEN)\n\t\tskb_put_zero(*skb, padlen - ETH_FCS_LEN);\n\nadd_fcs:\n\t \n\tfcs = crc32_le(~0, (*skb)->data, (*skb)->len);\n\tfcs = ~fcs;\n\n\tskb_put_u8(*skb, fcs\t\t& 0xff);\n\tskb_put_u8(*skb, (fcs >> 8)\t& 0xff);\n\tskb_put_u8(*skb, (fcs >> 16)\t& 0xff);\n\tskb_put_u8(*skb, (fcs >> 24)\t& 0xff);\n\n\treturn 0;\n}\n\nstatic netdev_tx_t macb_start_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tu16 queue_index = skb_get_queue_mapping(skb);\n\tstruct macb *bp = netdev_priv(dev);\n\tstruct macb_queue *queue = &bp->queues[queue_index];\n\tunsigned int desc_cnt, nr_frags, frag_size, f;\n\tunsigned int hdrlen;\n\tbool is_lso;\n\tnetdev_tx_t ret = NETDEV_TX_OK;\n\n\tif (macb_clear_csum(skb)) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn ret;\n\t}\n\n\tif (macb_pad_and_fcs(&skb, dev)) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn ret;\n\t}\n\n#ifdef CONFIG_MACB_USE_HWSTAMP\n\tif ((skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP) &&\n\t    (bp->hw_dma_cap & HW_DMA_CAP_PTP))\n\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n#endif\n\n\tis_lso = (skb_shinfo(skb)->gso_size != 0);\n\n\tif (is_lso) {\n\t\t \n\t\tif (ip_hdr(skb)->protocol == IPPROTO_UDP)\n\t\t\t \n\t\t\thdrlen = skb_transport_offset(skb);\n\t\telse\n\t\t\thdrlen = skb_tcp_all_headers(skb);\n\t\tif (skb_headlen(skb) < hdrlen) {\n\t\t\tnetdev_err(bp->dev, \"Error - LSO headers fragmented!!!\\n\");\n\t\t\t \n\t\t\treturn NETDEV_TX_BUSY;\n\t\t}\n\t} else\n\t\thdrlen = min(skb_headlen(skb), bp->max_tx_length);\n\n#if defined(DEBUG) && defined(VERBOSE_DEBUG)\n\tnetdev_vdbg(bp->dev,\n\t\t    \"start_xmit: queue %hu len %u head %p data %p tail %p end %p\\n\",\n\t\t    queue_index, skb->len, skb->head, skb->data,\n\t\t    skb_tail_pointer(skb), skb_end_pointer(skb));\n\tprint_hex_dump(KERN_DEBUG, \"data: \", DUMP_PREFIX_OFFSET, 16, 1,\n\t\t       skb->data, 16, true);\n#endif\n\n\t \n\tif (is_lso && (skb_headlen(skb) > hdrlen))\n\t\t \n\t\tdesc_cnt = DIV_ROUND_UP((skb_headlen(skb) - hdrlen), bp->max_tx_length) + 1;\n\telse\n\t\tdesc_cnt = DIV_ROUND_UP(skb_headlen(skb), bp->max_tx_length);\n\tnr_frags = skb_shinfo(skb)->nr_frags;\n\tfor (f = 0; f < nr_frags; f++) {\n\t\tfrag_size = skb_frag_size(&skb_shinfo(skb)->frags[f]);\n\t\tdesc_cnt += DIV_ROUND_UP(frag_size, bp->max_tx_length);\n\t}\n\n\tspin_lock_bh(&queue->tx_ptr_lock);\n\n\t \n\tif (CIRC_SPACE(queue->tx_head, queue->tx_tail,\n\t\t       bp->tx_ring_size) < desc_cnt) {\n\t\tnetif_stop_subqueue(dev, queue_index);\n\t\tnetdev_dbg(bp->dev, \"tx_head = %u, tx_tail = %u\\n\",\n\t\t\t   queue->tx_head, queue->tx_tail);\n\t\tret = NETDEV_TX_BUSY;\n\t\tgoto unlock;\n\t}\n\n\t \n\tif (!macb_tx_map(bp, queue, skb, hdrlen)) {\n\t\tdev_kfree_skb_any(skb);\n\t\tgoto unlock;\n\t}\n\n\t \n\twmb();\n\tskb_tx_timestamp(skb);\n\n\tspin_lock_irq(&bp->lock);\n\tmacb_writel(bp, NCR, macb_readl(bp, NCR) | MACB_BIT(TSTART));\n\tspin_unlock_irq(&bp->lock);\n\n\tif (CIRC_SPACE(queue->tx_head, queue->tx_tail, bp->tx_ring_size) < 1)\n\t\tnetif_stop_subqueue(dev, queue_index);\n\nunlock:\n\tspin_unlock_bh(&queue->tx_ptr_lock);\n\n\treturn ret;\n}\n\nstatic void macb_init_rx_buffer_size(struct macb *bp, size_t size)\n{\n\tif (!macb_is_gem(bp)) {\n\t\tbp->rx_buffer_size = MACB_RX_BUFFER_SIZE;\n\t} else {\n\t\tbp->rx_buffer_size = size;\n\n\t\tif (bp->rx_buffer_size % RX_BUFFER_MULTIPLE) {\n\t\t\tnetdev_dbg(bp->dev,\n\t\t\t\t   \"RX buffer must be multiple of %d bytes, expanding\\n\",\n\t\t\t\t   RX_BUFFER_MULTIPLE);\n\t\t\tbp->rx_buffer_size =\n\t\t\t\troundup(bp->rx_buffer_size, RX_BUFFER_MULTIPLE);\n\t\t}\n\t}\n\n\tnetdev_dbg(bp->dev, \"mtu [%u] rx_buffer_size [%zu]\\n\",\n\t\t   bp->dev->mtu, bp->rx_buffer_size);\n}\n\nstatic void gem_free_rx_buffers(struct macb *bp)\n{\n\tstruct sk_buff\t\t*skb;\n\tstruct macb_dma_desc\t*desc;\n\tstruct macb_queue *queue;\n\tdma_addr_t\t\taddr;\n\tunsigned int q;\n\tint i;\n\n\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue) {\n\t\tif (!queue->rx_skbuff)\n\t\t\tcontinue;\n\n\t\tfor (i = 0; i < bp->rx_ring_size; i++) {\n\t\t\tskb = queue->rx_skbuff[i];\n\n\t\t\tif (!skb)\n\t\t\t\tcontinue;\n\n\t\t\tdesc = macb_rx_desc(queue, i);\n\t\t\taddr = macb_get_addr(bp, desc);\n\n\t\t\tdma_unmap_single(&bp->pdev->dev, addr, bp->rx_buffer_size,\n\t\t\t\t\tDMA_FROM_DEVICE);\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tskb = NULL;\n\t\t}\n\n\t\tkfree(queue->rx_skbuff);\n\t\tqueue->rx_skbuff = NULL;\n\t}\n}\n\nstatic void macb_free_rx_buffers(struct macb *bp)\n{\n\tstruct macb_queue *queue = &bp->queues[0];\n\n\tif (queue->rx_buffers) {\n\t\tdma_free_coherent(&bp->pdev->dev,\n\t\t\t\t  bp->rx_ring_size * bp->rx_buffer_size,\n\t\t\t\t  queue->rx_buffers, queue->rx_buffers_dma);\n\t\tqueue->rx_buffers = NULL;\n\t}\n}\n\nstatic void macb_free_consistent(struct macb *bp)\n{\n\tstruct macb_queue *queue;\n\tunsigned int q;\n\tint size;\n\n\tbp->macbgem_ops.mog_free_rx_buffers(bp);\n\n\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue) {\n\t\tkfree(queue->tx_skb);\n\t\tqueue->tx_skb = NULL;\n\t\tif (queue->tx_ring) {\n\t\t\tsize = TX_RING_BYTES(bp) + bp->tx_bd_rd_prefetch;\n\t\t\tdma_free_coherent(&bp->pdev->dev, size,\n\t\t\t\t\t  queue->tx_ring, queue->tx_ring_dma);\n\t\t\tqueue->tx_ring = NULL;\n\t\t}\n\t\tif (queue->rx_ring) {\n\t\t\tsize = RX_RING_BYTES(bp) + bp->rx_bd_rd_prefetch;\n\t\t\tdma_free_coherent(&bp->pdev->dev, size,\n\t\t\t\t\t  queue->rx_ring, queue->rx_ring_dma);\n\t\t\tqueue->rx_ring = NULL;\n\t\t}\n\t}\n}\n\nstatic int gem_alloc_rx_buffers(struct macb *bp)\n{\n\tstruct macb_queue *queue;\n\tunsigned int q;\n\tint size;\n\n\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue) {\n\t\tsize = bp->rx_ring_size * sizeof(struct sk_buff *);\n\t\tqueue->rx_skbuff = kzalloc(size, GFP_KERNEL);\n\t\tif (!queue->rx_skbuff)\n\t\t\treturn -ENOMEM;\n\t\telse\n\t\t\tnetdev_dbg(bp->dev,\n\t\t\t\t   \"Allocated %d RX struct sk_buff entries at %p\\n\",\n\t\t\t\t   bp->rx_ring_size, queue->rx_skbuff);\n\t}\n\treturn 0;\n}\n\nstatic int macb_alloc_rx_buffers(struct macb *bp)\n{\n\tstruct macb_queue *queue = &bp->queues[0];\n\tint size;\n\n\tsize = bp->rx_ring_size * bp->rx_buffer_size;\n\tqueue->rx_buffers = dma_alloc_coherent(&bp->pdev->dev, size,\n\t\t\t\t\t    &queue->rx_buffers_dma, GFP_KERNEL);\n\tif (!queue->rx_buffers)\n\t\treturn -ENOMEM;\n\n\tnetdev_dbg(bp->dev,\n\t\t   \"Allocated RX buffers of %d bytes at %08lx (mapped %p)\\n\",\n\t\t   size, (unsigned long)queue->rx_buffers_dma, queue->rx_buffers);\n\treturn 0;\n}\n\nstatic int macb_alloc_consistent(struct macb *bp)\n{\n\tstruct macb_queue *queue;\n\tunsigned int q;\n\tint size;\n\n\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue) {\n\t\tsize = TX_RING_BYTES(bp) + bp->tx_bd_rd_prefetch;\n\t\tqueue->tx_ring = dma_alloc_coherent(&bp->pdev->dev, size,\n\t\t\t\t\t\t    &queue->tx_ring_dma,\n\t\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!queue->tx_ring)\n\t\t\tgoto out_err;\n\t\tnetdev_dbg(bp->dev,\n\t\t\t   \"Allocated TX ring for queue %u of %d bytes at %08lx (mapped %p)\\n\",\n\t\t\t   q, size, (unsigned long)queue->tx_ring_dma,\n\t\t\t   queue->tx_ring);\n\n\t\tsize = bp->tx_ring_size * sizeof(struct macb_tx_skb);\n\t\tqueue->tx_skb = kmalloc(size, GFP_KERNEL);\n\t\tif (!queue->tx_skb)\n\t\t\tgoto out_err;\n\n\t\tsize = RX_RING_BYTES(bp) + bp->rx_bd_rd_prefetch;\n\t\tqueue->rx_ring = dma_alloc_coherent(&bp->pdev->dev, size,\n\t\t\t\t\t\t &queue->rx_ring_dma, GFP_KERNEL);\n\t\tif (!queue->rx_ring)\n\t\t\tgoto out_err;\n\t\tnetdev_dbg(bp->dev,\n\t\t\t   \"Allocated RX ring of %d bytes at %08lx (mapped %p)\\n\",\n\t\t\t   size, (unsigned long)queue->rx_ring_dma, queue->rx_ring);\n\t}\n\tif (bp->macbgem_ops.mog_alloc_rx_buffers(bp))\n\t\tgoto out_err;\n\n\treturn 0;\n\nout_err:\n\tmacb_free_consistent(bp);\n\treturn -ENOMEM;\n}\n\nstatic void gem_init_rings(struct macb *bp)\n{\n\tstruct macb_queue *queue;\n\tstruct macb_dma_desc *desc = NULL;\n\tunsigned int q;\n\tint i;\n\n\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue) {\n\t\tfor (i = 0; i < bp->tx_ring_size; i++) {\n\t\t\tdesc = macb_tx_desc(queue, i);\n\t\t\tmacb_set_addr(bp, desc, 0);\n\t\t\tdesc->ctrl = MACB_BIT(TX_USED);\n\t\t}\n\t\tdesc->ctrl |= MACB_BIT(TX_WRAP);\n\t\tqueue->tx_head = 0;\n\t\tqueue->tx_tail = 0;\n\n\t\tqueue->rx_tail = 0;\n\t\tqueue->rx_prepared_head = 0;\n\n\t\tgem_rx_refill(queue);\n\t}\n\n}\n\nstatic void macb_init_rings(struct macb *bp)\n{\n\tint i;\n\tstruct macb_dma_desc *desc = NULL;\n\n\tmacb_init_rx_ring(&bp->queues[0]);\n\n\tfor (i = 0; i < bp->tx_ring_size; i++) {\n\t\tdesc = macb_tx_desc(&bp->queues[0], i);\n\t\tmacb_set_addr(bp, desc, 0);\n\t\tdesc->ctrl = MACB_BIT(TX_USED);\n\t}\n\tbp->queues[0].tx_head = 0;\n\tbp->queues[0].tx_tail = 0;\n\tdesc->ctrl |= MACB_BIT(TX_WRAP);\n}\n\nstatic void macb_reset_hw(struct macb *bp)\n{\n\tstruct macb_queue *queue;\n\tunsigned int q;\n\tu32 ctrl = macb_readl(bp, NCR);\n\n\t \n\tctrl &= ~(MACB_BIT(RE) | MACB_BIT(TE));\n\n\t \n\tctrl |= MACB_BIT(CLRSTAT);\n\n\tmacb_writel(bp, NCR, ctrl);\n\n\t \n\tmacb_writel(bp, TSR, -1);\n\tmacb_writel(bp, RSR, -1);\n\n\t \n\tgem_writel(bp, PBUFRXCUT, 0);\n\n\t \n\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue) {\n\t\tqueue_writel(queue, IDR, -1);\n\t\tqueue_readl(queue, ISR);\n\t\tif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\n\t\t\tqueue_writel(queue, ISR, -1);\n\t}\n}\n\nstatic u32 gem_mdc_clk_div(struct macb *bp)\n{\n\tu32 config;\n\tunsigned long pclk_hz = clk_get_rate(bp->pclk);\n\n\tif (pclk_hz <= 20000000)\n\t\tconfig = GEM_BF(CLK, GEM_CLK_DIV8);\n\telse if (pclk_hz <= 40000000)\n\t\tconfig = GEM_BF(CLK, GEM_CLK_DIV16);\n\telse if (pclk_hz <= 80000000)\n\t\tconfig = GEM_BF(CLK, GEM_CLK_DIV32);\n\telse if (pclk_hz <= 120000000)\n\t\tconfig = GEM_BF(CLK, GEM_CLK_DIV48);\n\telse if (pclk_hz <= 160000000)\n\t\tconfig = GEM_BF(CLK, GEM_CLK_DIV64);\n\telse if (pclk_hz <= 240000000)\n\t\tconfig = GEM_BF(CLK, GEM_CLK_DIV96);\n\telse if (pclk_hz <= 320000000)\n\t\tconfig = GEM_BF(CLK, GEM_CLK_DIV128);\n\telse\n\t\tconfig = GEM_BF(CLK, GEM_CLK_DIV224);\n\n\treturn config;\n}\n\nstatic u32 macb_mdc_clk_div(struct macb *bp)\n{\n\tu32 config;\n\tunsigned long pclk_hz;\n\n\tif (macb_is_gem(bp))\n\t\treturn gem_mdc_clk_div(bp);\n\n\tpclk_hz = clk_get_rate(bp->pclk);\n\tif (pclk_hz <= 20000000)\n\t\tconfig = MACB_BF(CLK, MACB_CLK_DIV8);\n\telse if (pclk_hz <= 40000000)\n\t\tconfig = MACB_BF(CLK, MACB_CLK_DIV16);\n\telse if (pclk_hz <= 80000000)\n\t\tconfig = MACB_BF(CLK, MACB_CLK_DIV32);\n\telse\n\t\tconfig = MACB_BF(CLK, MACB_CLK_DIV64);\n\n\treturn config;\n}\n\n \nstatic u32 macb_dbw(struct macb *bp)\n{\n\tif (!macb_is_gem(bp))\n\t\treturn 0;\n\n\tswitch (GEM_BFEXT(DBWDEF, gem_readl(bp, DCFG1))) {\n\tcase 4:\n\t\treturn GEM_BF(DBW, GEM_DBW128);\n\tcase 2:\n\t\treturn GEM_BF(DBW, GEM_DBW64);\n\tcase 1:\n\tdefault:\n\t\treturn GEM_BF(DBW, GEM_DBW32);\n\t}\n}\n\n \nstatic void macb_configure_dma(struct macb *bp)\n{\n\tstruct macb_queue *queue;\n\tu32 buffer_size;\n\tunsigned int q;\n\tu32 dmacfg;\n\n\tbuffer_size = bp->rx_buffer_size / RX_BUFFER_MULTIPLE;\n\tif (macb_is_gem(bp)) {\n\t\tdmacfg = gem_readl(bp, DMACFG) & ~GEM_BF(RXBS, -1L);\n\t\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue) {\n\t\t\tif (q)\n\t\t\t\tqueue_writel(queue, RBQS, buffer_size);\n\t\t\telse\n\t\t\t\tdmacfg |= GEM_BF(RXBS, buffer_size);\n\t\t}\n\t\tif (bp->dma_burst_length)\n\t\t\tdmacfg = GEM_BFINS(FBLDO, bp->dma_burst_length, dmacfg);\n\t\tdmacfg |= GEM_BIT(TXPBMS) | GEM_BF(RXBMS, -1L);\n\t\tdmacfg &= ~GEM_BIT(ENDIA_PKT);\n\n\t\tif (bp->native_io)\n\t\t\tdmacfg &= ~GEM_BIT(ENDIA_DESC);\n\t\telse\n\t\t\tdmacfg |= GEM_BIT(ENDIA_DESC);  \n\n\t\tif (bp->dev->features & NETIF_F_HW_CSUM)\n\t\t\tdmacfg |= GEM_BIT(TXCOEN);\n\t\telse\n\t\t\tdmacfg &= ~GEM_BIT(TXCOEN);\n\n\t\tdmacfg &= ~GEM_BIT(ADDR64);\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\t\tif (bp->hw_dma_cap & HW_DMA_CAP_64B)\n\t\t\tdmacfg |= GEM_BIT(ADDR64);\n#endif\n#ifdef CONFIG_MACB_USE_HWSTAMP\n\t\tif (bp->hw_dma_cap & HW_DMA_CAP_PTP)\n\t\t\tdmacfg |= GEM_BIT(RXEXT) | GEM_BIT(TXEXT);\n#endif\n\t\tnetdev_dbg(bp->dev, \"Cadence configure DMA with 0x%08x\\n\",\n\t\t\t   dmacfg);\n\t\tgem_writel(bp, DMACFG, dmacfg);\n\t}\n}\n\nstatic void macb_init_hw(struct macb *bp)\n{\n\tu32 config;\n\n\tmacb_reset_hw(bp);\n\tmacb_set_hwaddr(bp);\n\n\tconfig = macb_mdc_clk_div(bp);\n\tconfig |= MACB_BF(RBOF, NET_IP_ALIGN);\t \n\tconfig |= MACB_BIT(DRFCS);\t\t \n\tif (bp->caps & MACB_CAPS_JUMBO)\n\t\tconfig |= MACB_BIT(JFRAME);\t \n\telse\n\t\tconfig |= MACB_BIT(BIG);\t \n\tif (bp->dev->flags & IFF_PROMISC)\n\t\tconfig |= MACB_BIT(CAF);\t \n\telse if (macb_is_gem(bp) && bp->dev->features & NETIF_F_RXCSUM)\n\t\tconfig |= GEM_BIT(RXCOEN);\n\tif (!(bp->dev->flags & IFF_BROADCAST))\n\t\tconfig |= MACB_BIT(NBC);\t \n\tconfig |= macb_dbw(bp);\n\tmacb_writel(bp, NCFGR, config);\n\tif ((bp->caps & MACB_CAPS_JUMBO) && bp->jumbo_max_len)\n\t\tgem_writel(bp, JML, bp->jumbo_max_len);\n\tbp->rx_frm_len_mask = MACB_RX_FRMLEN_MASK;\n\tif (bp->caps & MACB_CAPS_JUMBO)\n\t\tbp->rx_frm_len_mask = MACB_RX_JFRMLEN_MASK;\n\n\tmacb_configure_dma(bp);\n\n\t \n\tif (bp->rx_watermark)\n\t\tgem_writel(bp, PBUFRXCUT, (bp->rx_watermark | GEM_BIT(ENCUTTHRU)));\n}\n\n \n\nstatic inline int hash_bit_value(int bitnr, __u8 *addr)\n{\n\tif (addr[bitnr / 8] & (1 << (bitnr % 8)))\n\t\treturn 1;\n\treturn 0;\n}\n\n \nstatic int hash_get_index(__u8 *addr)\n{\n\tint i, j, bitval;\n\tint hash_index = 0;\n\n\tfor (j = 0; j < 6; j++) {\n\t\tfor (i = 0, bitval = 0; i < 8; i++)\n\t\t\tbitval ^= hash_bit_value(i * 6 + j, addr);\n\n\t\thash_index |= (bitval << j);\n\t}\n\n\treturn hash_index;\n}\n\n \nstatic void macb_sethashtable(struct net_device *dev)\n{\n\tstruct netdev_hw_addr *ha;\n\tunsigned long mc_filter[2];\n\tunsigned int bitnr;\n\tstruct macb *bp = netdev_priv(dev);\n\n\tmc_filter[0] = 0;\n\tmc_filter[1] = 0;\n\n\tnetdev_for_each_mc_addr(ha, dev) {\n\t\tbitnr = hash_get_index(ha->addr);\n\t\tmc_filter[bitnr >> 5] |= 1 << (bitnr & 31);\n\t}\n\n\tmacb_or_gem_writel(bp, HRB, mc_filter[0]);\n\tmacb_or_gem_writel(bp, HRT, mc_filter[1]);\n}\n\n \nstatic void macb_set_rx_mode(struct net_device *dev)\n{\n\tunsigned long cfg;\n\tstruct macb *bp = netdev_priv(dev);\n\n\tcfg = macb_readl(bp, NCFGR);\n\n\tif (dev->flags & IFF_PROMISC) {\n\t\t \n\t\tcfg |= MACB_BIT(CAF);\n\n\t\t \n\t\tif (macb_is_gem(bp))\n\t\t\tcfg &= ~GEM_BIT(RXCOEN);\n\t} else {\n\t\t \n\t\tcfg &= ~MACB_BIT(CAF);\n\n\t\t \n\t\tif (macb_is_gem(bp) && dev->features & NETIF_F_RXCSUM)\n\t\t\tcfg |= GEM_BIT(RXCOEN);\n\t}\n\n\tif (dev->flags & IFF_ALLMULTI) {\n\t\t \n\t\tmacb_or_gem_writel(bp, HRB, -1);\n\t\tmacb_or_gem_writel(bp, HRT, -1);\n\t\tcfg |= MACB_BIT(NCFGR_MTI);\n\t} else if (!netdev_mc_empty(dev)) {\n\t\t \n\t\tmacb_sethashtable(dev);\n\t\tcfg |= MACB_BIT(NCFGR_MTI);\n\t} else if (dev->flags & (~IFF_ALLMULTI)) {\n\t\t \n\t\tmacb_or_gem_writel(bp, HRB, 0);\n\t\tmacb_or_gem_writel(bp, HRT, 0);\n\t\tcfg &= ~MACB_BIT(NCFGR_MTI);\n\t}\n\n\tmacb_writel(bp, NCFGR, cfg);\n}\n\nstatic int macb_open(struct net_device *dev)\n{\n\tsize_t bufsz = dev->mtu + ETH_HLEN + ETH_FCS_LEN + NET_IP_ALIGN;\n\tstruct macb *bp = netdev_priv(dev);\n\tstruct macb_queue *queue;\n\tunsigned int q;\n\tint err;\n\n\tnetdev_dbg(bp->dev, \"open\\n\");\n\n\terr = pm_runtime_resume_and_get(&bp->pdev->dev);\n\tif (err < 0)\n\t\treturn err;\n\n\t \n\tmacb_init_rx_buffer_size(bp, bufsz);\n\n\terr = macb_alloc_consistent(bp);\n\tif (err) {\n\t\tnetdev_err(dev, \"Unable to allocate DMA memory (error %d)\\n\",\n\t\t\t   err);\n\t\tgoto pm_exit;\n\t}\n\n\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue) {\n\t\tnapi_enable(&queue->napi_rx);\n\t\tnapi_enable(&queue->napi_tx);\n\t}\n\n\tmacb_init_hw(bp);\n\n\terr = phy_power_on(bp->sgmii_phy);\n\tif (err)\n\t\tgoto reset_hw;\n\n\terr = macb_phylink_connect(bp);\n\tif (err)\n\t\tgoto phy_off;\n\n\tnetif_tx_start_all_queues(dev);\n\n\tif (bp->ptp_info)\n\t\tbp->ptp_info->ptp_init(dev);\n\n\treturn 0;\n\nphy_off:\n\tphy_power_off(bp->sgmii_phy);\n\nreset_hw:\n\tmacb_reset_hw(bp);\n\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue) {\n\t\tnapi_disable(&queue->napi_rx);\n\t\tnapi_disable(&queue->napi_tx);\n\t}\n\tmacb_free_consistent(bp);\npm_exit:\n\tpm_runtime_put_sync(&bp->pdev->dev);\n\treturn err;\n}\n\nstatic int macb_close(struct net_device *dev)\n{\n\tstruct macb *bp = netdev_priv(dev);\n\tstruct macb_queue *queue;\n\tunsigned long flags;\n\tunsigned int q;\n\n\tnetif_tx_stop_all_queues(dev);\n\n\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue) {\n\t\tnapi_disable(&queue->napi_rx);\n\t\tnapi_disable(&queue->napi_tx);\n\t}\n\n\tphylink_stop(bp->phylink);\n\tphylink_disconnect_phy(bp->phylink);\n\n\tphy_power_off(bp->sgmii_phy);\n\n\tspin_lock_irqsave(&bp->lock, flags);\n\tmacb_reset_hw(bp);\n\tnetif_carrier_off(dev);\n\tspin_unlock_irqrestore(&bp->lock, flags);\n\n\tmacb_free_consistent(bp);\n\n\tif (bp->ptp_info)\n\t\tbp->ptp_info->ptp_remove(dev);\n\n\tpm_runtime_put(&bp->pdev->dev);\n\n\treturn 0;\n}\n\nstatic int macb_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tif (netif_running(dev))\n\t\treturn -EBUSY;\n\n\tdev->mtu = new_mtu;\n\n\treturn 0;\n}\n\nstatic int macb_set_mac_addr(struct net_device *dev, void *addr)\n{\n\tint err;\n\n\terr = eth_mac_addr(dev, addr);\n\tif (err < 0)\n\t\treturn err;\n\n\tmacb_set_hwaddr(netdev_priv(dev));\n\treturn 0;\n}\n\nstatic void gem_update_stats(struct macb *bp)\n{\n\tstruct macb_queue *queue;\n\tunsigned int i, q, idx;\n\tunsigned long *stat;\n\n\tu32 *p = &bp->hw_stats.gem.tx_octets_31_0;\n\n\tfor (i = 0; i < GEM_STATS_LEN; ++i, ++p) {\n\t\tu32 offset = gem_statistics[i].offset;\n\t\tu64 val = bp->macb_reg_readl(bp, offset);\n\n\t\tbp->ethtool_stats[i] += val;\n\t\t*p += val;\n\n\t\tif (offset == GEM_OCTTXL || offset == GEM_OCTRXL) {\n\t\t\t \n\t\t\tval = bp->macb_reg_readl(bp, offset + 4);\n\t\t\tbp->ethtool_stats[i] += ((u64)val) << 32;\n\t\t\t*(++p) += val;\n\t\t}\n\t}\n\n\tidx = GEM_STATS_LEN;\n\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue)\n\t\tfor (i = 0, stat = &queue->stats.first; i < QUEUE_STATS_LEN; ++i, ++stat)\n\t\t\tbp->ethtool_stats[idx++] = *stat;\n}\n\nstatic struct net_device_stats *gem_get_stats(struct macb *bp)\n{\n\tstruct gem_stats *hwstat = &bp->hw_stats.gem;\n\tstruct net_device_stats *nstat = &bp->dev->stats;\n\n\tif (!netif_running(bp->dev))\n\t\treturn nstat;\n\n\tgem_update_stats(bp);\n\n\tnstat->rx_errors = (hwstat->rx_frame_check_sequence_errors +\n\t\t\t    hwstat->rx_alignment_errors +\n\t\t\t    hwstat->rx_resource_errors +\n\t\t\t    hwstat->rx_overruns +\n\t\t\t    hwstat->rx_oversize_frames +\n\t\t\t    hwstat->rx_jabbers +\n\t\t\t    hwstat->rx_undersized_frames +\n\t\t\t    hwstat->rx_length_field_frame_errors);\n\tnstat->tx_errors = (hwstat->tx_late_collisions +\n\t\t\t    hwstat->tx_excessive_collisions +\n\t\t\t    hwstat->tx_underrun +\n\t\t\t    hwstat->tx_carrier_sense_errors);\n\tnstat->multicast = hwstat->rx_multicast_frames;\n\tnstat->collisions = (hwstat->tx_single_collision_frames +\n\t\t\t     hwstat->tx_multiple_collision_frames +\n\t\t\t     hwstat->tx_excessive_collisions);\n\tnstat->rx_length_errors = (hwstat->rx_oversize_frames +\n\t\t\t\t   hwstat->rx_jabbers +\n\t\t\t\t   hwstat->rx_undersized_frames +\n\t\t\t\t   hwstat->rx_length_field_frame_errors);\n\tnstat->rx_over_errors = hwstat->rx_resource_errors;\n\tnstat->rx_crc_errors = hwstat->rx_frame_check_sequence_errors;\n\tnstat->rx_frame_errors = hwstat->rx_alignment_errors;\n\tnstat->rx_fifo_errors = hwstat->rx_overruns;\n\tnstat->tx_aborted_errors = hwstat->tx_excessive_collisions;\n\tnstat->tx_carrier_errors = hwstat->tx_carrier_sense_errors;\n\tnstat->tx_fifo_errors = hwstat->tx_underrun;\n\n\treturn nstat;\n}\n\nstatic void gem_get_ethtool_stats(struct net_device *dev,\n\t\t\t\t  struct ethtool_stats *stats, u64 *data)\n{\n\tstruct macb *bp;\n\n\tbp = netdev_priv(dev);\n\tgem_update_stats(bp);\n\tmemcpy(data, &bp->ethtool_stats, sizeof(u64)\n\t\t\t* (GEM_STATS_LEN + QUEUE_STATS_LEN * MACB_MAX_QUEUES));\n}\n\nstatic int gem_get_sset_count(struct net_device *dev, int sset)\n{\n\tstruct macb *bp = netdev_priv(dev);\n\n\tswitch (sset) {\n\tcase ETH_SS_STATS:\n\t\treturn GEM_STATS_LEN + bp->num_queues * QUEUE_STATS_LEN;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic void gem_get_ethtool_strings(struct net_device *dev, u32 sset, u8 *p)\n{\n\tchar stat_string[ETH_GSTRING_LEN];\n\tstruct macb *bp = netdev_priv(dev);\n\tstruct macb_queue *queue;\n\tunsigned int i;\n\tunsigned int q;\n\n\tswitch (sset) {\n\tcase ETH_SS_STATS:\n\t\tfor (i = 0; i < GEM_STATS_LEN; i++, p += ETH_GSTRING_LEN)\n\t\t\tmemcpy(p, gem_statistics[i].stat_string,\n\t\t\t       ETH_GSTRING_LEN);\n\n\t\tfor (q = 0, queue = bp->queues; q < bp->num_queues; ++q, ++queue) {\n\t\t\tfor (i = 0; i < QUEUE_STATS_LEN; i++, p += ETH_GSTRING_LEN) {\n\t\t\t\tsnprintf(stat_string, ETH_GSTRING_LEN, \"q%d_%s\",\n\t\t\t\t\t\tq, queue_statistics[i].stat_string);\n\t\t\t\tmemcpy(p, stat_string, ETH_GSTRING_LEN);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\t}\n}\n\nstatic struct net_device_stats *macb_get_stats(struct net_device *dev)\n{\n\tstruct macb *bp = netdev_priv(dev);\n\tstruct net_device_stats *nstat = &bp->dev->stats;\n\tstruct macb_stats *hwstat = &bp->hw_stats.macb;\n\n\tif (macb_is_gem(bp))\n\t\treturn gem_get_stats(bp);\n\n\t \n\tmacb_update_stats(bp);\n\n\t \n\tnstat->rx_errors = (hwstat->rx_fcs_errors +\n\t\t\t    hwstat->rx_align_errors +\n\t\t\t    hwstat->rx_resource_errors +\n\t\t\t    hwstat->rx_overruns +\n\t\t\t    hwstat->rx_oversize_pkts +\n\t\t\t    hwstat->rx_jabbers +\n\t\t\t    hwstat->rx_undersize_pkts +\n\t\t\t    hwstat->rx_length_mismatch);\n\tnstat->tx_errors = (hwstat->tx_late_cols +\n\t\t\t    hwstat->tx_excessive_cols +\n\t\t\t    hwstat->tx_underruns +\n\t\t\t    hwstat->tx_carrier_errors +\n\t\t\t    hwstat->sqe_test_errors);\n\tnstat->collisions = (hwstat->tx_single_cols +\n\t\t\t     hwstat->tx_multiple_cols +\n\t\t\t     hwstat->tx_excessive_cols);\n\tnstat->rx_length_errors = (hwstat->rx_oversize_pkts +\n\t\t\t\t   hwstat->rx_jabbers +\n\t\t\t\t   hwstat->rx_undersize_pkts +\n\t\t\t\t   hwstat->rx_length_mismatch);\n\tnstat->rx_over_errors = hwstat->rx_resource_errors +\n\t\t\t\t   hwstat->rx_overruns;\n\tnstat->rx_crc_errors = hwstat->rx_fcs_errors;\n\tnstat->rx_frame_errors = hwstat->rx_align_errors;\n\tnstat->rx_fifo_errors = hwstat->rx_overruns;\n\t \n\tnstat->tx_aborted_errors = hwstat->tx_excessive_cols;\n\tnstat->tx_carrier_errors = hwstat->tx_carrier_errors;\n\tnstat->tx_fifo_errors = hwstat->tx_underruns;\n\t \n\n\treturn nstat;\n}\n\nstatic int macb_get_regs_len(struct net_device *netdev)\n{\n\treturn MACB_GREGS_NBR * sizeof(u32);\n}\n\nstatic void macb_get_regs(struct net_device *dev, struct ethtool_regs *regs,\n\t\t\t  void *p)\n{\n\tstruct macb *bp = netdev_priv(dev);\n\tunsigned int tail, head;\n\tu32 *regs_buff = p;\n\n\tregs->version = (macb_readl(bp, MID) & ((1 << MACB_REV_SIZE) - 1))\n\t\t\t| MACB_GREGS_VERSION;\n\n\ttail = macb_tx_ring_wrap(bp, bp->queues[0].tx_tail);\n\thead = macb_tx_ring_wrap(bp, bp->queues[0].tx_head);\n\n\tregs_buff[0]  = macb_readl(bp, NCR);\n\tregs_buff[1]  = macb_or_gem_readl(bp, NCFGR);\n\tregs_buff[2]  = macb_readl(bp, NSR);\n\tregs_buff[3]  = macb_readl(bp, TSR);\n\tregs_buff[4]  = macb_readl(bp, RBQP);\n\tregs_buff[5]  = macb_readl(bp, TBQP);\n\tregs_buff[6]  = macb_readl(bp, RSR);\n\tregs_buff[7]  = macb_readl(bp, IMR);\n\n\tregs_buff[8]  = tail;\n\tregs_buff[9]  = head;\n\tregs_buff[10] = macb_tx_dma(&bp->queues[0], tail);\n\tregs_buff[11] = macb_tx_dma(&bp->queues[0], head);\n\n\tif (!(bp->caps & MACB_CAPS_USRIO_DISABLED))\n\t\tregs_buff[12] = macb_or_gem_readl(bp, USRIO);\n\tif (macb_is_gem(bp))\n\t\tregs_buff[13] = gem_readl(bp, DMACFG);\n}\n\nstatic void macb_get_wol(struct net_device *netdev, struct ethtool_wolinfo *wol)\n{\n\tstruct macb *bp = netdev_priv(netdev);\n\n\tif (bp->wol & MACB_WOL_HAS_MAGIC_PACKET) {\n\t\tphylink_ethtool_get_wol(bp->phylink, wol);\n\t\twol->supported |= WAKE_MAGIC;\n\n\t\tif (bp->wol & MACB_WOL_ENABLED)\n\t\t\twol->wolopts |= WAKE_MAGIC;\n\t}\n}\n\nstatic int macb_set_wol(struct net_device *netdev, struct ethtool_wolinfo *wol)\n{\n\tstruct macb *bp = netdev_priv(netdev);\n\tint ret;\n\n\t \n\tret = phylink_ethtool_set_wol(bp->phylink, wol);\n\t \n\tif (!ret || ret != -EOPNOTSUPP)\n\t\treturn ret;\n\n\tif (!(bp->wol & MACB_WOL_HAS_MAGIC_PACKET) ||\n\t    (wol->wolopts & ~WAKE_MAGIC))\n\t\treturn -EOPNOTSUPP;\n\n\tif (wol->wolopts & WAKE_MAGIC)\n\t\tbp->wol |= MACB_WOL_ENABLED;\n\telse\n\t\tbp->wol &= ~MACB_WOL_ENABLED;\n\n\tdevice_set_wakeup_enable(&bp->pdev->dev, bp->wol & MACB_WOL_ENABLED);\n\n\treturn 0;\n}\n\nstatic int macb_get_link_ksettings(struct net_device *netdev,\n\t\t\t\t   struct ethtool_link_ksettings *kset)\n{\n\tstruct macb *bp = netdev_priv(netdev);\n\n\treturn phylink_ethtool_ksettings_get(bp->phylink, kset);\n}\n\nstatic int macb_set_link_ksettings(struct net_device *netdev,\n\t\t\t\t   const struct ethtool_link_ksettings *kset)\n{\n\tstruct macb *bp = netdev_priv(netdev);\n\n\treturn phylink_ethtool_ksettings_set(bp->phylink, kset);\n}\n\nstatic void macb_get_ringparam(struct net_device *netdev,\n\t\t\t       struct ethtool_ringparam *ring,\n\t\t\t       struct kernel_ethtool_ringparam *kernel_ring,\n\t\t\t       struct netlink_ext_ack *extack)\n{\n\tstruct macb *bp = netdev_priv(netdev);\n\n\tring->rx_max_pending = MAX_RX_RING_SIZE;\n\tring->tx_max_pending = MAX_TX_RING_SIZE;\n\n\tring->rx_pending = bp->rx_ring_size;\n\tring->tx_pending = bp->tx_ring_size;\n}\n\nstatic int macb_set_ringparam(struct net_device *netdev,\n\t\t\t      struct ethtool_ringparam *ring,\n\t\t\t      struct kernel_ethtool_ringparam *kernel_ring,\n\t\t\t      struct netlink_ext_ack *extack)\n{\n\tstruct macb *bp = netdev_priv(netdev);\n\tu32 new_rx_size, new_tx_size;\n\tunsigned int reset = 0;\n\n\tif ((ring->rx_mini_pending) || (ring->rx_jumbo_pending))\n\t\treturn -EINVAL;\n\n\tnew_rx_size = clamp_t(u32, ring->rx_pending,\n\t\t\t      MIN_RX_RING_SIZE, MAX_RX_RING_SIZE);\n\tnew_rx_size = roundup_pow_of_two(new_rx_size);\n\n\tnew_tx_size = clamp_t(u32, ring->tx_pending,\n\t\t\t      MIN_TX_RING_SIZE, MAX_TX_RING_SIZE);\n\tnew_tx_size = roundup_pow_of_two(new_tx_size);\n\n\tif ((new_tx_size == bp->tx_ring_size) &&\n\t    (new_rx_size == bp->rx_ring_size)) {\n\t\t \n\t\treturn 0;\n\t}\n\n\tif (netif_running(bp->dev)) {\n\t\treset = 1;\n\t\tmacb_close(bp->dev);\n\t}\n\n\tbp->rx_ring_size = new_rx_size;\n\tbp->tx_ring_size = new_tx_size;\n\n\tif (reset)\n\t\tmacb_open(bp->dev);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_MACB_USE_HWSTAMP\nstatic unsigned int gem_get_tsu_rate(struct macb *bp)\n{\n\tstruct clk *tsu_clk;\n\tunsigned int tsu_rate;\n\n\ttsu_clk = devm_clk_get(&bp->pdev->dev, \"tsu_clk\");\n\tif (!IS_ERR(tsu_clk))\n\t\ttsu_rate = clk_get_rate(tsu_clk);\n\t \n\telse if (!IS_ERR(bp->pclk)) {\n\t\ttsu_clk = bp->pclk;\n\t\ttsu_rate = clk_get_rate(tsu_clk);\n\t} else\n\t\treturn -ENOTSUPP;\n\treturn tsu_rate;\n}\n\nstatic s32 gem_get_ptp_max_adj(void)\n{\n\treturn 64000000;\n}\n\nstatic int gem_get_ts_info(struct net_device *dev,\n\t\t\t   struct ethtool_ts_info *info)\n{\n\tstruct macb *bp = netdev_priv(dev);\n\n\tif ((bp->hw_dma_cap & HW_DMA_CAP_PTP) == 0) {\n\t\tethtool_op_get_ts_info(dev, info);\n\t\treturn 0;\n\t}\n\n\tinfo->so_timestamping =\n\t\tSOF_TIMESTAMPING_TX_SOFTWARE |\n\t\tSOF_TIMESTAMPING_RX_SOFTWARE |\n\t\tSOF_TIMESTAMPING_SOFTWARE |\n\t\tSOF_TIMESTAMPING_TX_HARDWARE |\n\t\tSOF_TIMESTAMPING_RX_HARDWARE |\n\t\tSOF_TIMESTAMPING_RAW_HARDWARE;\n\tinfo->tx_types =\n\t\t(1 << HWTSTAMP_TX_ONESTEP_SYNC) |\n\t\t(1 << HWTSTAMP_TX_OFF) |\n\t\t(1 << HWTSTAMP_TX_ON);\n\tinfo->rx_filters =\n\t\t(1 << HWTSTAMP_FILTER_NONE) |\n\t\t(1 << HWTSTAMP_FILTER_ALL);\n\n\tinfo->phc_index = bp->ptp_clock ? ptp_clock_index(bp->ptp_clock) : -1;\n\n\treturn 0;\n}\n\nstatic struct macb_ptp_info gem_ptp_info = {\n\t.ptp_init\t = gem_ptp_init,\n\t.ptp_remove\t = gem_ptp_remove,\n\t.get_ptp_max_adj = gem_get_ptp_max_adj,\n\t.get_tsu_rate\t = gem_get_tsu_rate,\n\t.get_ts_info\t = gem_get_ts_info,\n\t.get_hwtst\t = gem_get_hwtst,\n\t.set_hwtst\t = gem_set_hwtst,\n};\n#endif\n\nstatic int macb_get_ts_info(struct net_device *netdev,\n\t\t\t    struct ethtool_ts_info *info)\n{\n\tstruct macb *bp = netdev_priv(netdev);\n\n\tif (bp->ptp_info)\n\t\treturn bp->ptp_info->get_ts_info(netdev, info);\n\n\treturn ethtool_op_get_ts_info(netdev, info);\n}\n\nstatic void gem_enable_flow_filters(struct macb *bp, bool enable)\n{\n\tstruct net_device *netdev = bp->dev;\n\tstruct ethtool_rx_fs_item *item;\n\tu32 t2_scr;\n\tint num_t2_scr;\n\n\tif (!(netdev->features & NETIF_F_NTUPLE))\n\t\treturn;\n\n\tnum_t2_scr = GEM_BFEXT(T2SCR, gem_readl(bp, DCFG8));\n\n\tlist_for_each_entry(item, &bp->rx_fs_list.list, list) {\n\t\tstruct ethtool_rx_flow_spec *fs = &item->fs;\n\t\tstruct ethtool_tcpip4_spec *tp4sp_m;\n\n\t\tif (fs->location >= num_t2_scr)\n\t\t\tcontinue;\n\n\t\tt2_scr = gem_readl_n(bp, SCRT2, fs->location);\n\n\t\t \n\t\tt2_scr = GEM_BFINS(ETHTEN, enable, t2_scr);\n\n\t\t \n\t\ttp4sp_m = &(fs->m_u.tcp_ip4_spec);\n\n\t\tif (enable && (tp4sp_m->ip4src == 0xFFFFFFFF))\n\t\t\tt2_scr = GEM_BFINS(CMPAEN, 1, t2_scr);\n\t\telse\n\t\t\tt2_scr = GEM_BFINS(CMPAEN, 0, t2_scr);\n\n\t\tif (enable && (tp4sp_m->ip4dst == 0xFFFFFFFF))\n\t\t\tt2_scr = GEM_BFINS(CMPBEN, 1, t2_scr);\n\t\telse\n\t\t\tt2_scr = GEM_BFINS(CMPBEN, 0, t2_scr);\n\n\t\tif (enable && ((tp4sp_m->psrc == 0xFFFF) || (tp4sp_m->pdst == 0xFFFF)))\n\t\t\tt2_scr = GEM_BFINS(CMPCEN, 1, t2_scr);\n\t\telse\n\t\t\tt2_scr = GEM_BFINS(CMPCEN, 0, t2_scr);\n\n\t\tgem_writel_n(bp, SCRT2, fs->location, t2_scr);\n\t}\n}\n\nstatic void gem_prog_cmp_regs(struct macb *bp, struct ethtool_rx_flow_spec *fs)\n{\n\tstruct ethtool_tcpip4_spec *tp4sp_v, *tp4sp_m;\n\tuint16_t index = fs->location;\n\tu32 w0, w1, t2_scr;\n\tbool cmp_a = false;\n\tbool cmp_b = false;\n\tbool cmp_c = false;\n\n\tif (!macb_is_gem(bp))\n\t\treturn;\n\n\ttp4sp_v = &(fs->h_u.tcp_ip4_spec);\n\ttp4sp_m = &(fs->m_u.tcp_ip4_spec);\n\n\t \n\tif (tp4sp_m->ip4src == 0xFFFFFFFF) {\n\t\t \n\t\tw0 = 0;\n\t\tw1 = 0;\n\t\tw0 = tp4sp_v->ip4src;\n\t\tw1 = GEM_BFINS(T2DISMSK, 1, w1);  \n\t\tw1 = GEM_BFINS(T2CMPOFST, GEM_T2COMPOFST_ETYPE, w1);\n\t\tw1 = GEM_BFINS(T2OFST, ETYPE_SRCIP_OFFSET, w1);\n\t\tgem_writel_n(bp, T2CMPW0, T2CMP_OFST(GEM_IP4SRC_CMP(index)), w0);\n\t\tgem_writel_n(bp, T2CMPW1, T2CMP_OFST(GEM_IP4SRC_CMP(index)), w1);\n\t\tcmp_a = true;\n\t}\n\n\t \n\tif (tp4sp_m->ip4dst == 0xFFFFFFFF) {\n\t\t \n\t\tw0 = 0;\n\t\tw1 = 0;\n\t\tw0 = tp4sp_v->ip4dst;\n\t\tw1 = GEM_BFINS(T2DISMSK, 1, w1);  \n\t\tw1 = GEM_BFINS(T2CMPOFST, GEM_T2COMPOFST_ETYPE, w1);\n\t\tw1 = GEM_BFINS(T2OFST, ETYPE_DSTIP_OFFSET, w1);\n\t\tgem_writel_n(bp, T2CMPW0, T2CMP_OFST(GEM_IP4DST_CMP(index)), w0);\n\t\tgem_writel_n(bp, T2CMPW1, T2CMP_OFST(GEM_IP4DST_CMP(index)), w1);\n\t\tcmp_b = true;\n\t}\n\n\t \n\tif ((tp4sp_m->psrc == 0xFFFF) || (tp4sp_m->pdst == 0xFFFF)) {\n\t\t \n\t\tw0 = 0;\n\t\tw1 = 0;\n\t\tw1 = GEM_BFINS(T2CMPOFST, GEM_T2COMPOFST_IPHDR, w1);\n\t\tif (tp4sp_m->psrc == tp4sp_m->pdst) {\n\t\t\tw0 = GEM_BFINS(T2MASK, tp4sp_v->psrc, w0);\n\t\t\tw0 = GEM_BFINS(T2CMP, tp4sp_v->pdst, w0);\n\t\t\tw1 = GEM_BFINS(T2DISMSK, 1, w1);  \n\t\t\tw1 = GEM_BFINS(T2OFST, IPHDR_SRCPORT_OFFSET, w1);\n\t\t} else {\n\t\t\t \n\t\t\tw1 = GEM_BFINS(T2DISMSK, 0, w1);  \n\t\t\tw0 = GEM_BFINS(T2MASK, 0xFFFF, w0);\n\t\t\tif (tp4sp_m->psrc == 0xFFFF) {  \n\t\t\t\tw0 = GEM_BFINS(T2CMP, tp4sp_v->psrc, w0);\n\t\t\t\tw1 = GEM_BFINS(T2OFST, IPHDR_SRCPORT_OFFSET, w1);\n\t\t\t} else {  \n\t\t\t\tw0 = GEM_BFINS(T2CMP, tp4sp_v->pdst, w0);\n\t\t\t\tw1 = GEM_BFINS(T2OFST, IPHDR_DSTPORT_OFFSET, w1);\n\t\t\t}\n\t\t}\n\t\tgem_writel_n(bp, T2CMPW0, T2CMP_OFST(GEM_PORT_CMP(index)), w0);\n\t\tgem_writel_n(bp, T2CMPW1, T2CMP_OFST(GEM_PORT_CMP(index)), w1);\n\t\tcmp_c = true;\n\t}\n\n\tt2_scr = 0;\n\tt2_scr = GEM_BFINS(QUEUE, (fs->ring_cookie) & 0xFF, t2_scr);\n\tt2_scr = GEM_BFINS(ETHT2IDX, SCRT2_ETHT, t2_scr);\n\tif (cmp_a)\n\t\tt2_scr = GEM_BFINS(CMPA, GEM_IP4SRC_CMP(index), t2_scr);\n\tif (cmp_b)\n\t\tt2_scr = GEM_BFINS(CMPB, GEM_IP4DST_CMP(index), t2_scr);\n\tif (cmp_c)\n\t\tt2_scr = GEM_BFINS(CMPC, GEM_PORT_CMP(index), t2_scr);\n\tgem_writel_n(bp, SCRT2, index, t2_scr);\n}\n\nstatic int gem_add_flow_filter(struct net_device *netdev,\n\t\tstruct ethtool_rxnfc *cmd)\n{\n\tstruct macb *bp = netdev_priv(netdev);\n\tstruct ethtool_rx_flow_spec *fs = &cmd->fs;\n\tstruct ethtool_rx_fs_item *item, *newfs;\n\tunsigned long flags;\n\tint ret = -EINVAL;\n\tbool added = false;\n\n\tnewfs = kmalloc(sizeof(*newfs), GFP_KERNEL);\n\tif (newfs == NULL)\n\t\treturn -ENOMEM;\n\tmemcpy(&newfs->fs, fs, sizeof(newfs->fs));\n\n\tnetdev_dbg(netdev,\n\t\t\t\"Adding flow filter entry,type=%u,queue=%u,loc=%u,src=%08X,dst=%08X,ps=%u,pd=%u\\n\",\n\t\t\tfs->flow_type, (int)fs->ring_cookie, fs->location,\n\t\t\thtonl(fs->h_u.tcp_ip4_spec.ip4src),\n\t\t\thtonl(fs->h_u.tcp_ip4_spec.ip4dst),\n\t\t\tbe16_to_cpu(fs->h_u.tcp_ip4_spec.psrc),\n\t\t\tbe16_to_cpu(fs->h_u.tcp_ip4_spec.pdst));\n\n\tspin_lock_irqsave(&bp->rx_fs_lock, flags);\n\n\t \n\tlist_for_each_entry(item, &bp->rx_fs_list.list, list) {\n\t\tif (item->fs.location > newfs->fs.location) {\n\t\t\tlist_add_tail(&newfs->list, &item->list);\n\t\t\tadded = true;\n\t\t\tbreak;\n\t\t} else if (item->fs.location == fs->location) {\n\t\t\tnetdev_err(netdev, \"Rule not added: location %d not free!\\n\",\n\t\t\t\t\tfs->location);\n\t\t\tret = -EBUSY;\n\t\t\tgoto err;\n\t\t}\n\t}\n\tif (!added)\n\t\tlist_add_tail(&newfs->list, &bp->rx_fs_list.list);\n\n\tgem_prog_cmp_regs(bp, fs);\n\tbp->rx_fs_list.count++;\n\t \n\tgem_enable_flow_filters(bp, 1);\n\n\tspin_unlock_irqrestore(&bp->rx_fs_lock, flags);\n\treturn 0;\n\nerr:\n\tspin_unlock_irqrestore(&bp->rx_fs_lock, flags);\n\tkfree(newfs);\n\treturn ret;\n}\n\nstatic int gem_del_flow_filter(struct net_device *netdev,\n\t\tstruct ethtool_rxnfc *cmd)\n{\n\tstruct macb *bp = netdev_priv(netdev);\n\tstruct ethtool_rx_fs_item *item;\n\tstruct ethtool_rx_flow_spec *fs;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bp->rx_fs_lock, flags);\n\n\tlist_for_each_entry(item, &bp->rx_fs_list.list, list) {\n\t\tif (item->fs.location == cmd->fs.location) {\n\t\t\t \n\t\t\tfs = &(item->fs);\n\t\t\tnetdev_dbg(netdev,\n\t\t\t\t\t\"Deleting flow filter entry,type=%u,queue=%u,loc=%u,src=%08X,dst=%08X,ps=%u,pd=%u\\n\",\n\t\t\t\t\tfs->flow_type, (int)fs->ring_cookie, fs->location,\n\t\t\t\t\thtonl(fs->h_u.tcp_ip4_spec.ip4src),\n\t\t\t\t\thtonl(fs->h_u.tcp_ip4_spec.ip4dst),\n\t\t\t\t\tbe16_to_cpu(fs->h_u.tcp_ip4_spec.psrc),\n\t\t\t\t\tbe16_to_cpu(fs->h_u.tcp_ip4_spec.pdst));\n\n\t\t\tgem_writel_n(bp, SCRT2, fs->location, 0);\n\n\t\t\tlist_del(&item->list);\n\t\t\tbp->rx_fs_list.count--;\n\t\t\tspin_unlock_irqrestore(&bp->rx_fs_lock, flags);\n\t\t\tkfree(item);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tspin_unlock_irqrestore(&bp->rx_fs_lock, flags);\n\treturn -EINVAL;\n}\n\nstatic int gem_get_flow_entry(struct net_device *netdev,\n\t\tstruct ethtool_rxnfc *cmd)\n{\n\tstruct macb *bp = netdev_priv(netdev);\n\tstruct ethtool_rx_fs_item *item;\n\n\tlist_for_each_entry(item, &bp->rx_fs_list.list, list) {\n\t\tif (item->fs.location == cmd->fs.location) {\n\t\t\tmemcpy(&cmd->fs, &item->fs, sizeof(cmd->fs));\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nstatic int gem_get_all_flow_entries(struct net_device *netdev,\n\t\tstruct ethtool_rxnfc *cmd, u32 *rule_locs)\n{\n\tstruct macb *bp = netdev_priv(netdev);\n\tstruct ethtool_rx_fs_item *item;\n\tuint32_t cnt = 0;\n\n\tlist_for_each_entry(item, &bp->rx_fs_list.list, list) {\n\t\tif (cnt == cmd->rule_cnt)\n\t\t\treturn -EMSGSIZE;\n\t\trule_locs[cnt] = item->fs.location;\n\t\tcnt++;\n\t}\n\tcmd->data = bp->max_tuples;\n\tcmd->rule_cnt = cnt;\n\n\treturn 0;\n}\n\nstatic int gem_get_rxnfc(struct net_device *netdev, struct ethtool_rxnfc *cmd,\n\t\tu32 *rule_locs)\n{\n\tstruct macb *bp = netdev_priv(netdev);\n\tint ret = 0;\n\n\tswitch (cmd->cmd) {\n\tcase ETHTOOL_GRXRINGS:\n\t\tcmd->data = bp->num_queues;\n\t\tbreak;\n\tcase ETHTOOL_GRXCLSRLCNT:\n\t\tcmd->rule_cnt = bp->rx_fs_list.count;\n\t\tbreak;\n\tcase ETHTOOL_GRXCLSRULE:\n\t\tret = gem_get_flow_entry(netdev, cmd);\n\t\tbreak;\n\tcase ETHTOOL_GRXCLSRLALL:\n\t\tret = gem_get_all_flow_entries(netdev, cmd, rule_locs);\n\t\tbreak;\n\tdefault:\n\t\tnetdev_err(netdev,\n\t\t\t  \"Command parameter %d is not supported\\n\", cmd->cmd);\n\t\tret = -EOPNOTSUPP;\n\t}\n\n\treturn ret;\n}\n\nstatic int gem_set_rxnfc(struct net_device *netdev, struct ethtool_rxnfc *cmd)\n{\n\tstruct macb *bp = netdev_priv(netdev);\n\tint ret;\n\n\tswitch (cmd->cmd) {\n\tcase ETHTOOL_SRXCLSRLINS:\n\t\tif ((cmd->fs.location >= bp->max_tuples)\n\t\t\t\t|| (cmd->fs.ring_cookie >= bp->num_queues)) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tret = gem_add_flow_filter(netdev, cmd);\n\t\tbreak;\n\tcase ETHTOOL_SRXCLSRLDEL:\n\t\tret = gem_del_flow_filter(netdev, cmd);\n\t\tbreak;\n\tdefault:\n\t\tnetdev_err(netdev,\n\t\t\t  \"Command parameter %d is not supported\\n\", cmd->cmd);\n\t\tret = -EOPNOTSUPP;\n\t}\n\n\treturn ret;\n}\n\nstatic const struct ethtool_ops macb_ethtool_ops = {\n\t.get_regs_len\t\t= macb_get_regs_len,\n\t.get_regs\t\t= macb_get_regs,\n\t.get_link\t\t= ethtool_op_get_link,\n\t.get_ts_info\t\t= ethtool_op_get_ts_info,\n\t.get_wol\t\t= macb_get_wol,\n\t.set_wol\t\t= macb_set_wol,\n\t.get_link_ksettings     = macb_get_link_ksettings,\n\t.set_link_ksettings     = macb_set_link_ksettings,\n\t.get_ringparam\t\t= macb_get_ringparam,\n\t.set_ringparam\t\t= macb_set_ringparam,\n};\n\nstatic const struct ethtool_ops gem_ethtool_ops = {\n\t.get_regs_len\t\t= macb_get_regs_len,\n\t.get_regs\t\t= macb_get_regs,\n\t.get_wol\t\t= macb_get_wol,\n\t.set_wol\t\t= macb_set_wol,\n\t.get_link\t\t= ethtool_op_get_link,\n\t.get_ts_info\t\t= macb_get_ts_info,\n\t.get_ethtool_stats\t= gem_get_ethtool_stats,\n\t.get_strings\t\t= gem_get_ethtool_strings,\n\t.get_sset_count\t\t= gem_get_sset_count,\n\t.get_link_ksettings     = macb_get_link_ksettings,\n\t.set_link_ksettings     = macb_set_link_ksettings,\n\t.get_ringparam\t\t= macb_get_ringparam,\n\t.set_ringparam\t\t= macb_set_ringparam,\n\t.get_rxnfc\t\t\t= gem_get_rxnfc,\n\t.set_rxnfc\t\t\t= gem_set_rxnfc,\n};\n\nstatic int macb_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tstruct macb *bp = netdev_priv(dev);\n\n\tif (!netif_running(dev))\n\t\treturn -EINVAL;\n\n\tif (bp->ptp_info) {\n\t\tswitch (cmd) {\n\t\tcase SIOCSHWTSTAMP:\n\t\t\treturn bp->ptp_info->set_hwtst(dev, rq, cmd);\n\t\tcase SIOCGHWTSTAMP:\n\t\t\treturn bp->ptp_info->get_hwtst(dev, rq);\n\t\t}\n\t}\n\n\treturn phylink_mii_ioctl(bp->phylink, rq, cmd);\n}\n\nstatic inline void macb_set_txcsum_feature(struct macb *bp,\n\t\t\t\t\t   netdev_features_t features)\n{\n\tu32 val;\n\n\tif (!macb_is_gem(bp))\n\t\treturn;\n\n\tval = gem_readl(bp, DMACFG);\n\tif (features & NETIF_F_HW_CSUM)\n\t\tval |= GEM_BIT(TXCOEN);\n\telse\n\t\tval &= ~GEM_BIT(TXCOEN);\n\n\tgem_writel(bp, DMACFG, val);\n}\n\nstatic inline void macb_set_rxcsum_feature(struct macb *bp,\n\t\t\t\t\t   netdev_features_t features)\n{\n\tstruct net_device *netdev = bp->dev;\n\tu32 val;\n\n\tif (!macb_is_gem(bp))\n\t\treturn;\n\n\tval = gem_readl(bp, NCFGR);\n\tif ((features & NETIF_F_RXCSUM) && !(netdev->flags & IFF_PROMISC))\n\t\tval |= GEM_BIT(RXCOEN);\n\telse\n\t\tval &= ~GEM_BIT(RXCOEN);\n\n\tgem_writel(bp, NCFGR, val);\n}\n\nstatic inline void macb_set_rxflow_feature(struct macb *bp,\n\t\t\t\t\t   netdev_features_t features)\n{\n\tif (!macb_is_gem(bp))\n\t\treturn;\n\n\tgem_enable_flow_filters(bp, !!(features & NETIF_F_NTUPLE));\n}\n\nstatic int macb_set_features(struct net_device *netdev,\n\t\t\t     netdev_features_t features)\n{\n\tstruct macb *bp = netdev_priv(netdev);\n\tnetdev_features_t changed = features ^ netdev->features;\n\n\t \n\tif (changed & NETIF_F_HW_CSUM)\n\t\tmacb_set_txcsum_feature(bp, features);\n\n\t \n\tif (changed & NETIF_F_RXCSUM)\n\t\tmacb_set_rxcsum_feature(bp, features);\n\n\t \n\tif (changed & NETIF_F_NTUPLE)\n\t\tmacb_set_rxflow_feature(bp, features);\n\n\treturn 0;\n}\n\nstatic void macb_restore_features(struct macb *bp)\n{\n\tstruct net_device *netdev = bp->dev;\n\tnetdev_features_t features = netdev->features;\n\tstruct ethtool_rx_fs_item *item;\n\n\t \n\tmacb_set_txcsum_feature(bp, features);\n\n\t \n\tmacb_set_rxcsum_feature(bp, features);\n\n\t \n\tlist_for_each_entry(item, &bp->rx_fs_list.list, list)\n\t\tgem_prog_cmp_regs(bp, &item->fs);\n\n\tmacb_set_rxflow_feature(bp, features);\n}\n\nstatic const struct net_device_ops macb_netdev_ops = {\n\t.ndo_open\t\t= macb_open,\n\t.ndo_stop\t\t= macb_close,\n\t.ndo_start_xmit\t\t= macb_start_xmit,\n\t.ndo_set_rx_mode\t= macb_set_rx_mode,\n\t.ndo_get_stats\t\t= macb_get_stats,\n\t.ndo_eth_ioctl\t\t= macb_ioctl,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_change_mtu\t\t= macb_change_mtu,\n\t.ndo_set_mac_address\t= macb_set_mac_addr,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= macb_poll_controller,\n#endif\n\t.ndo_set_features\t= macb_set_features,\n\t.ndo_features_check\t= macb_features_check,\n};\n\n \nstatic void macb_configure_caps(struct macb *bp,\n\t\t\t\tconst struct macb_config *dt_conf)\n{\n\tu32 dcfg;\n\n\tif (dt_conf)\n\t\tbp->caps = dt_conf->caps;\n\n\tif (hw_is_gem(bp->regs, bp->native_io)) {\n\t\tbp->caps |= MACB_CAPS_MACB_IS_GEM;\n\n\t\tdcfg = gem_readl(bp, DCFG1);\n\t\tif (GEM_BFEXT(IRQCOR, dcfg) == 0)\n\t\t\tbp->caps |= MACB_CAPS_ISR_CLEAR_ON_WRITE;\n\t\tif (GEM_BFEXT(NO_PCS, dcfg) == 0)\n\t\t\tbp->caps |= MACB_CAPS_PCS;\n\t\tdcfg = gem_readl(bp, DCFG12);\n\t\tif (GEM_BFEXT(HIGH_SPEED, dcfg) == 1)\n\t\t\tbp->caps |= MACB_CAPS_HIGH_SPEED;\n\t\tdcfg = gem_readl(bp, DCFG2);\n\t\tif ((dcfg & (GEM_BIT(RX_PKT_BUFF) | GEM_BIT(TX_PKT_BUFF))) == 0)\n\t\t\tbp->caps |= MACB_CAPS_FIFO_MODE;\n\t\tif (gem_has_ptp(bp)) {\n\t\t\tif (!GEM_BFEXT(TSU, gem_readl(bp, DCFG5)))\n\t\t\t\tdev_err(&bp->pdev->dev,\n\t\t\t\t\t\"GEM doesn't support hardware ptp.\\n\");\n\t\t\telse {\n#ifdef CONFIG_MACB_USE_HWSTAMP\n\t\t\t\tbp->hw_dma_cap |= HW_DMA_CAP_PTP;\n\t\t\t\tbp->ptp_info = &gem_ptp_info;\n#endif\n\t\t\t}\n\t\t}\n\t}\n\n\tdev_dbg(&bp->pdev->dev, \"Cadence caps 0x%08x\\n\", bp->caps);\n}\n\nstatic void macb_probe_queues(void __iomem *mem,\n\t\t\t      bool native_io,\n\t\t\t      unsigned int *queue_mask,\n\t\t\t      unsigned int *num_queues)\n{\n\t*queue_mask = 0x1;\n\t*num_queues = 1;\n\n\t \n\tif (!hw_is_gem(mem, native_io))\n\t\treturn;\n\n\t \n\t*queue_mask |= readl_relaxed(mem + GEM_DCFG6) & 0xff;\n\t*num_queues = hweight32(*queue_mask);\n}\n\nstatic void macb_clks_disable(struct clk *pclk, struct clk *hclk, struct clk *tx_clk,\n\t\t\t      struct clk *rx_clk, struct clk *tsu_clk)\n{\n\tstruct clk_bulk_data clks[] = {\n\t\t{ .clk = tsu_clk, },\n\t\t{ .clk = rx_clk, },\n\t\t{ .clk = pclk, },\n\t\t{ .clk = hclk, },\n\t\t{ .clk = tx_clk },\n\t};\n\n\tclk_bulk_disable_unprepare(ARRAY_SIZE(clks), clks);\n}\n\nstatic int macb_clk_init(struct platform_device *pdev, struct clk **pclk,\n\t\t\t struct clk **hclk, struct clk **tx_clk,\n\t\t\t struct clk **rx_clk, struct clk **tsu_clk)\n{\n\tstruct macb_platform_data *pdata;\n\tint err;\n\n\tpdata = dev_get_platdata(&pdev->dev);\n\tif (pdata) {\n\t\t*pclk = pdata->pclk;\n\t\t*hclk = pdata->hclk;\n\t} else {\n\t\t*pclk = devm_clk_get(&pdev->dev, \"pclk\");\n\t\t*hclk = devm_clk_get(&pdev->dev, \"hclk\");\n\t}\n\n\tif (IS_ERR_OR_NULL(*pclk))\n\t\treturn dev_err_probe(&pdev->dev,\n\t\t\t\t     IS_ERR(*pclk) ? PTR_ERR(*pclk) : -ENODEV,\n\t\t\t\t     \"failed to get pclk\\n\");\n\n\tif (IS_ERR_OR_NULL(*hclk))\n\t\treturn dev_err_probe(&pdev->dev,\n\t\t\t\t     IS_ERR(*hclk) ? PTR_ERR(*hclk) : -ENODEV,\n\t\t\t\t     \"failed to get hclk\\n\");\n\n\t*tx_clk = devm_clk_get_optional(&pdev->dev, \"tx_clk\");\n\tif (IS_ERR(*tx_clk))\n\t\treturn PTR_ERR(*tx_clk);\n\n\t*rx_clk = devm_clk_get_optional(&pdev->dev, \"rx_clk\");\n\tif (IS_ERR(*rx_clk))\n\t\treturn PTR_ERR(*rx_clk);\n\n\t*tsu_clk = devm_clk_get_optional(&pdev->dev, \"tsu_clk\");\n\tif (IS_ERR(*tsu_clk))\n\t\treturn PTR_ERR(*tsu_clk);\n\n\terr = clk_prepare_enable(*pclk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable pclk (%d)\\n\", err);\n\t\treturn err;\n\t}\n\n\terr = clk_prepare_enable(*hclk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable hclk (%d)\\n\", err);\n\t\tgoto err_disable_pclk;\n\t}\n\n\terr = clk_prepare_enable(*tx_clk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable tx_clk (%d)\\n\", err);\n\t\tgoto err_disable_hclk;\n\t}\n\n\terr = clk_prepare_enable(*rx_clk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable rx_clk (%d)\\n\", err);\n\t\tgoto err_disable_txclk;\n\t}\n\n\terr = clk_prepare_enable(*tsu_clk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable tsu_clk (%d)\\n\", err);\n\t\tgoto err_disable_rxclk;\n\t}\n\n\treturn 0;\n\nerr_disable_rxclk:\n\tclk_disable_unprepare(*rx_clk);\n\nerr_disable_txclk:\n\tclk_disable_unprepare(*tx_clk);\n\nerr_disable_hclk:\n\tclk_disable_unprepare(*hclk);\n\nerr_disable_pclk:\n\tclk_disable_unprepare(*pclk);\n\n\treturn err;\n}\n\nstatic int macb_init(struct platform_device *pdev)\n{\n\tstruct net_device *dev = platform_get_drvdata(pdev);\n\tunsigned int hw_q, q;\n\tstruct macb *bp = netdev_priv(dev);\n\tstruct macb_queue *queue;\n\tint err;\n\tu32 val, reg;\n\n\tbp->tx_ring_size = DEFAULT_TX_RING_SIZE;\n\tbp->rx_ring_size = DEFAULT_RX_RING_SIZE;\n\n\t \n\tfor (hw_q = 0, q = 0; hw_q < MACB_MAX_QUEUES; ++hw_q) {\n\t\tif (!(bp->queue_mask & (1 << hw_q)))\n\t\t\tcontinue;\n\n\t\tqueue = &bp->queues[q];\n\t\tqueue->bp = bp;\n\t\tspin_lock_init(&queue->tx_ptr_lock);\n\t\tnetif_napi_add(dev, &queue->napi_rx, macb_rx_poll);\n\t\tnetif_napi_add(dev, &queue->napi_tx, macb_tx_poll);\n\t\tif (hw_q) {\n\t\t\tqueue->ISR  = GEM_ISR(hw_q - 1);\n\t\t\tqueue->IER  = GEM_IER(hw_q - 1);\n\t\t\tqueue->IDR  = GEM_IDR(hw_q - 1);\n\t\t\tqueue->IMR  = GEM_IMR(hw_q - 1);\n\t\t\tqueue->TBQP = GEM_TBQP(hw_q - 1);\n\t\t\tqueue->RBQP = GEM_RBQP(hw_q - 1);\n\t\t\tqueue->RBQS = GEM_RBQS(hw_q - 1);\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\t\t\tif (bp->hw_dma_cap & HW_DMA_CAP_64B) {\n\t\t\t\tqueue->TBQPH = GEM_TBQPH(hw_q - 1);\n\t\t\t\tqueue->RBQPH = GEM_RBQPH(hw_q - 1);\n\t\t\t}\n#endif\n\t\t} else {\n\t\t\t \n\t\t\tqueue->ISR  = MACB_ISR;\n\t\t\tqueue->IER  = MACB_IER;\n\t\t\tqueue->IDR  = MACB_IDR;\n\t\t\tqueue->IMR  = MACB_IMR;\n\t\t\tqueue->TBQP = MACB_TBQP;\n\t\t\tqueue->RBQP = MACB_RBQP;\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\t\t\tif (bp->hw_dma_cap & HW_DMA_CAP_64B) {\n\t\t\t\tqueue->TBQPH = MACB_TBQPH;\n\t\t\t\tqueue->RBQPH = MACB_RBQPH;\n\t\t\t}\n#endif\n\t\t}\n\n\t\t \n\t\tqueue->irq = platform_get_irq(pdev, q);\n\t\terr = devm_request_irq(&pdev->dev, queue->irq, macb_interrupt,\n\t\t\t\t       IRQF_SHARED, dev->name, queue);\n\t\tif (err) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"Unable to request IRQ %d (error %d)\\n\",\n\t\t\t\tqueue->irq, err);\n\t\t\treturn err;\n\t\t}\n\n\t\tINIT_WORK(&queue->tx_error_task, macb_tx_error_task);\n\t\tq++;\n\t}\n\n\tdev->netdev_ops = &macb_netdev_ops;\n\n\t \n\tif (macb_is_gem(bp)) {\n\t\tbp->macbgem_ops.mog_alloc_rx_buffers = gem_alloc_rx_buffers;\n\t\tbp->macbgem_ops.mog_free_rx_buffers = gem_free_rx_buffers;\n\t\tbp->macbgem_ops.mog_init_rings = gem_init_rings;\n\t\tbp->macbgem_ops.mog_rx = gem_rx;\n\t\tdev->ethtool_ops = &gem_ethtool_ops;\n\t} else {\n\t\tbp->macbgem_ops.mog_alloc_rx_buffers = macb_alloc_rx_buffers;\n\t\tbp->macbgem_ops.mog_free_rx_buffers = macb_free_rx_buffers;\n\t\tbp->macbgem_ops.mog_init_rings = macb_init_rings;\n\t\tbp->macbgem_ops.mog_rx = macb_rx;\n\t\tdev->ethtool_ops = &macb_ethtool_ops;\n\t}\n\n\tdev->priv_flags |= IFF_LIVE_ADDR_CHANGE;\n\n\t \n\tdev->hw_features = NETIF_F_SG;\n\n\t \n\tif (GEM_BFEXT(PBUF_LSO, gem_readl(bp, DCFG6)))\n\t\tdev->hw_features |= MACB_NETIF_LSO;\n\n\t \n\tif (macb_is_gem(bp) && !(bp->caps & MACB_CAPS_FIFO_MODE))\n\t\tdev->hw_features |= NETIF_F_HW_CSUM | NETIF_F_RXCSUM;\n\tif (bp->caps & MACB_CAPS_SG_DISABLED)\n\t\tdev->hw_features &= ~NETIF_F_SG;\n\tdev->features = dev->hw_features;\n\n\t \n\treg = gem_readl(bp, DCFG8);\n\tbp->max_tuples = min((GEM_BFEXT(SCR2CMP, reg) / 3),\n\t\t\tGEM_BFEXT(T2SCR, reg));\n\tINIT_LIST_HEAD(&bp->rx_fs_list.list);\n\tif (bp->max_tuples > 0) {\n\t\t \n\t\tif (GEM_BFEXT(SCR2ETH, reg) > 0) {\n\t\t\t \n\t\t\treg = 0;\n\t\t\treg = GEM_BFINS(ETHTCMP, (uint16_t)ETH_P_IP, reg);\n\t\t\tgem_writel_n(bp, ETHT, SCRT2_ETHT, reg);\n\t\t\t \n\t\t\tdev->hw_features |= NETIF_F_NTUPLE;\n\t\t\t \n\t\t\tbp->rx_fs_list.count = 0;\n\t\t\tspin_lock_init(&bp->rx_fs_lock);\n\t\t} else\n\t\t\tbp->max_tuples = 0;\n\t}\n\n\tif (!(bp->caps & MACB_CAPS_USRIO_DISABLED)) {\n\t\tval = 0;\n\t\tif (phy_interface_mode_is_rgmii(bp->phy_interface))\n\t\t\tval = bp->usrio->rgmii;\n\t\telse if (bp->phy_interface == PHY_INTERFACE_MODE_RMII &&\n\t\t\t (bp->caps & MACB_CAPS_USRIO_DEFAULT_IS_MII_GMII))\n\t\t\tval = bp->usrio->rmii;\n\t\telse if (!(bp->caps & MACB_CAPS_USRIO_DEFAULT_IS_MII_GMII))\n\t\t\tval = bp->usrio->mii;\n\n\t\tif (bp->caps & MACB_CAPS_USRIO_HAS_CLKEN)\n\t\t\tval |= bp->usrio->refclk;\n\n\t\tmacb_or_gem_writel(bp, USRIO, val);\n\t}\n\n\t \n\tval = macb_mdc_clk_div(bp);\n\tval |= macb_dbw(bp);\n\tif (bp->phy_interface == PHY_INTERFACE_MODE_SGMII)\n\t\tval |= GEM_BIT(SGMIIEN) | GEM_BIT(PCSSEL);\n\tmacb_writel(bp, NCFGR, val);\n\n\treturn 0;\n}\n\nstatic const struct macb_usrio_config macb_default_usrio = {\n\t.mii = MACB_BIT(MII),\n\t.rmii = MACB_BIT(RMII),\n\t.rgmii = GEM_BIT(RGMII),\n\t.refclk = MACB_BIT(CLKEN),\n};\n\n#if defined(CONFIG_OF)\n \n#define AT91ETHER_MAX_RBUFF_SZ\t0x600\n \n#define AT91ETHER_MAX_RX_DESCR\t9\n\nstatic struct sifive_fu540_macb_mgmt *mgmt;\n\nstatic int at91ether_alloc_coherent(struct macb *lp)\n{\n\tstruct macb_queue *q = &lp->queues[0];\n\n\tq->rx_ring = dma_alloc_coherent(&lp->pdev->dev,\n\t\t\t\t\t (AT91ETHER_MAX_RX_DESCR *\n\t\t\t\t\t  macb_dma_desc_get_size(lp)),\n\t\t\t\t\t &q->rx_ring_dma, GFP_KERNEL);\n\tif (!q->rx_ring)\n\t\treturn -ENOMEM;\n\n\tq->rx_buffers = dma_alloc_coherent(&lp->pdev->dev,\n\t\t\t\t\t    AT91ETHER_MAX_RX_DESCR *\n\t\t\t\t\t    AT91ETHER_MAX_RBUFF_SZ,\n\t\t\t\t\t    &q->rx_buffers_dma, GFP_KERNEL);\n\tif (!q->rx_buffers) {\n\t\tdma_free_coherent(&lp->pdev->dev,\n\t\t\t\t  AT91ETHER_MAX_RX_DESCR *\n\t\t\t\t  macb_dma_desc_get_size(lp),\n\t\t\t\t  q->rx_ring, q->rx_ring_dma);\n\t\tq->rx_ring = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void at91ether_free_coherent(struct macb *lp)\n{\n\tstruct macb_queue *q = &lp->queues[0];\n\n\tif (q->rx_ring) {\n\t\tdma_free_coherent(&lp->pdev->dev,\n\t\t\t\t  AT91ETHER_MAX_RX_DESCR *\n\t\t\t\t  macb_dma_desc_get_size(lp),\n\t\t\t\t  q->rx_ring, q->rx_ring_dma);\n\t\tq->rx_ring = NULL;\n\t}\n\n\tif (q->rx_buffers) {\n\t\tdma_free_coherent(&lp->pdev->dev,\n\t\t\t\t  AT91ETHER_MAX_RX_DESCR *\n\t\t\t\t  AT91ETHER_MAX_RBUFF_SZ,\n\t\t\t\t  q->rx_buffers, q->rx_buffers_dma);\n\t\tq->rx_buffers = NULL;\n\t}\n}\n\n \nstatic int at91ether_start(struct macb *lp)\n{\n\tstruct macb_queue *q = &lp->queues[0];\n\tstruct macb_dma_desc *desc;\n\tdma_addr_t addr;\n\tu32 ctl;\n\tint i, ret;\n\n\tret = at91ether_alloc_coherent(lp);\n\tif (ret)\n\t\treturn ret;\n\n\taddr = q->rx_buffers_dma;\n\tfor (i = 0; i < AT91ETHER_MAX_RX_DESCR; i++) {\n\t\tdesc = macb_rx_desc(q, i);\n\t\tmacb_set_addr(lp, desc, addr);\n\t\tdesc->ctrl = 0;\n\t\taddr += AT91ETHER_MAX_RBUFF_SZ;\n\t}\n\n\t \n\tdesc->addr |= MACB_BIT(RX_WRAP);\n\n\t \n\tq->rx_tail = 0;\n\n\t \n\tmacb_writel(lp, RBQP, q->rx_ring_dma);\n\n\t \n\tctl = macb_readl(lp, NCR);\n\tmacb_writel(lp, NCR, ctl | MACB_BIT(RE) | MACB_BIT(TE));\n\n\t \n\tmacb_writel(lp, IER, MACB_BIT(RCOMP)\t|\n\t\t\t     MACB_BIT(RXUBR)\t|\n\t\t\t     MACB_BIT(ISR_TUND)\t|\n\t\t\t     MACB_BIT(ISR_RLE)\t|\n\t\t\t     MACB_BIT(TCOMP)\t|\n\t\t\t     MACB_BIT(ISR_ROVR)\t|\n\t\t\t     MACB_BIT(HRESP));\n\n\treturn 0;\n}\n\nstatic void at91ether_stop(struct macb *lp)\n{\n\tu32 ctl;\n\n\t \n\tmacb_writel(lp, IDR, MACB_BIT(RCOMP)\t|\n\t\t\t     MACB_BIT(RXUBR)\t|\n\t\t\t     MACB_BIT(ISR_TUND)\t|\n\t\t\t     MACB_BIT(ISR_RLE)\t|\n\t\t\t     MACB_BIT(TCOMP)\t|\n\t\t\t     MACB_BIT(ISR_ROVR) |\n\t\t\t     MACB_BIT(HRESP));\n\n\t \n\tctl = macb_readl(lp, NCR);\n\tmacb_writel(lp, NCR, ctl & ~(MACB_BIT(TE) | MACB_BIT(RE)));\n\n\t \n\tat91ether_free_coherent(lp);\n}\n\n \nstatic int at91ether_open(struct net_device *dev)\n{\n\tstruct macb *lp = netdev_priv(dev);\n\tu32 ctl;\n\tint ret;\n\n\tret = pm_runtime_resume_and_get(&lp->pdev->dev);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tctl = macb_readl(lp, NCR);\n\tmacb_writel(lp, NCR, ctl | MACB_BIT(CLRSTAT));\n\n\tmacb_set_hwaddr(lp);\n\n\tret = at91ether_start(lp);\n\tif (ret)\n\t\tgoto pm_exit;\n\n\tret = macb_phylink_connect(lp);\n\tif (ret)\n\t\tgoto stop;\n\n\tnetif_start_queue(dev);\n\n\treturn 0;\n\nstop:\n\tat91ether_stop(lp);\npm_exit:\n\tpm_runtime_put_sync(&lp->pdev->dev);\n\treturn ret;\n}\n\n \nstatic int at91ether_close(struct net_device *dev)\n{\n\tstruct macb *lp = netdev_priv(dev);\n\n\tnetif_stop_queue(dev);\n\n\tphylink_stop(lp->phylink);\n\tphylink_disconnect_phy(lp->phylink);\n\n\tat91ether_stop(lp);\n\n\treturn pm_runtime_put(&lp->pdev->dev);\n}\n\n \nstatic netdev_tx_t at91ether_start_xmit(struct sk_buff *skb,\n\t\t\t\t\tstruct net_device *dev)\n{\n\tstruct macb *lp = netdev_priv(dev);\n\n\tif (macb_readl(lp, TSR) & MACB_BIT(RM9200_BNQ)) {\n\t\tint desc = 0;\n\n\t\tnetif_stop_queue(dev);\n\n\t\t \n\t\tlp->rm9200_txq[desc].skb = skb;\n\t\tlp->rm9200_txq[desc].size = skb->len;\n\t\tlp->rm9200_txq[desc].mapping = dma_map_single(&lp->pdev->dev, skb->data,\n\t\t\t\t\t\t\t      skb->len, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&lp->pdev->dev, lp->rm9200_txq[desc].mapping)) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tdev->stats.tx_dropped++;\n\t\t\tnetdev_err(dev, \"%s: DMA mapping error\\n\", __func__);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\n\t\t \n\t\tmacb_writel(lp, TAR, lp->rm9200_txq[desc].mapping);\n\t\t \n\t\tmacb_writel(lp, TCR, skb->len);\n\n\t} else {\n\t\tnetdev_err(dev, \"%s called, but device is busy!\\n\", __func__);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\treturn NETDEV_TX_OK;\n}\n\n \nstatic void at91ether_rx(struct net_device *dev)\n{\n\tstruct macb *lp = netdev_priv(dev);\n\tstruct macb_queue *q = &lp->queues[0];\n\tstruct macb_dma_desc *desc;\n\tunsigned char *p_recv;\n\tstruct sk_buff *skb;\n\tunsigned int pktlen;\n\n\tdesc = macb_rx_desc(q, q->rx_tail);\n\twhile (desc->addr & MACB_BIT(RX_USED)) {\n\t\tp_recv = q->rx_buffers + q->rx_tail * AT91ETHER_MAX_RBUFF_SZ;\n\t\tpktlen = MACB_BF(RX_FRMLEN, desc->ctrl);\n\t\tskb = netdev_alloc_skb(dev, pktlen + 2);\n\t\tif (skb) {\n\t\t\tskb_reserve(skb, 2);\n\t\t\tskb_put_data(skb, p_recv, pktlen);\n\n\t\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\t\tdev->stats.rx_packets++;\n\t\t\tdev->stats.rx_bytes += pktlen;\n\t\t\tnetif_rx(skb);\n\t\t} else {\n\t\t\tdev->stats.rx_dropped++;\n\t\t}\n\n\t\tif (desc->ctrl & MACB_BIT(RX_MHASH_MATCH))\n\t\t\tdev->stats.multicast++;\n\n\t\t \n\t\tdesc->addr &= ~MACB_BIT(RX_USED);\n\n\t\t \n\t\tif (q->rx_tail == AT91ETHER_MAX_RX_DESCR - 1)\n\t\t\tq->rx_tail = 0;\n\t\telse\n\t\t\tq->rx_tail++;\n\n\t\tdesc = macb_rx_desc(q, q->rx_tail);\n\t}\n}\n\n \nstatic irqreturn_t at91ether_interrupt(int irq, void *dev_id)\n{\n\tstruct net_device *dev = dev_id;\n\tstruct macb *lp = netdev_priv(dev);\n\tu32 intstatus, ctl;\n\tunsigned int desc;\n\n\t \n\tintstatus = macb_readl(lp, ISR);\n\n\t \n\tif (intstatus & MACB_BIT(RCOMP))\n\t\tat91ether_rx(dev);\n\n\t \n\tif (intstatus & MACB_BIT(TCOMP)) {\n\t\t \n\t\tif (intstatus & (MACB_BIT(ISR_TUND) | MACB_BIT(ISR_RLE)))\n\t\t\tdev->stats.tx_errors++;\n\n\t\tdesc = 0;\n\t\tif (lp->rm9200_txq[desc].skb) {\n\t\t\tdev_consume_skb_irq(lp->rm9200_txq[desc].skb);\n\t\t\tlp->rm9200_txq[desc].skb = NULL;\n\t\t\tdma_unmap_single(&lp->pdev->dev, lp->rm9200_txq[desc].mapping,\n\t\t\t\t\t lp->rm9200_txq[desc].size, DMA_TO_DEVICE);\n\t\t\tdev->stats.tx_packets++;\n\t\t\tdev->stats.tx_bytes += lp->rm9200_txq[desc].size;\n\t\t}\n\t\tnetif_wake_queue(dev);\n\t}\n\n\t \n\tif (intstatus & MACB_BIT(RXUBR)) {\n\t\tctl = macb_readl(lp, NCR);\n\t\tmacb_writel(lp, NCR, ctl & ~MACB_BIT(RE));\n\t\twmb();\n\t\tmacb_writel(lp, NCR, ctl | MACB_BIT(RE));\n\t}\n\n\tif (intstatus & MACB_BIT(ISR_ROVR))\n\t\tnetdev_err(dev, \"ROVR error\\n\");\n\n\treturn IRQ_HANDLED;\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void at91ether_poll_controller(struct net_device *dev)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tat91ether_interrupt(dev->irq, dev);\n\tlocal_irq_restore(flags);\n}\n#endif\n\nstatic const struct net_device_ops at91ether_netdev_ops = {\n\t.ndo_open\t\t= at91ether_open,\n\t.ndo_stop\t\t= at91ether_close,\n\t.ndo_start_xmit\t\t= at91ether_start_xmit,\n\t.ndo_get_stats\t\t= macb_get_stats,\n\t.ndo_set_rx_mode\t= macb_set_rx_mode,\n\t.ndo_set_mac_address\t= eth_mac_addr,\n\t.ndo_eth_ioctl\t\t= macb_ioctl,\n\t.ndo_validate_addr\t= eth_validate_addr,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= at91ether_poll_controller,\n#endif\n};\n\nstatic int at91ether_clk_init(struct platform_device *pdev, struct clk **pclk,\n\t\t\t      struct clk **hclk, struct clk **tx_clk,\n\t\t\t      struct clk **rx_clk, struct clk **tsu_clk)\n{\n\tint err;\n\n\t*hclk = NULL;\n\t*tx_clk = NULL;\n\t*rx_clk = NULL;\n\t*tsu_clk = NULL;\n\n\t*pclk = devm_clk_get(&pdev->dev, \"ether_clk\");\n\tif (IS_ERR(*pclk))\n\t\treturn PTR_ERR(*pclk);\n\n\terr = clk_prepare_enable(*pclk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable pclk (%d)\\n\", err);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int at91ether_init(struct platform_device *pdev)\n{\n\tstruct net_device *dev = platform_get_drvdata(pdev);\n\tstruct macb *bp = netdev_priv(dev);\n\tint err;\n\n\tbp->queues[0].bp = bp;\n\n\tdev->netdev_ops = &at91ether_netdev_ops;\n\tdev->ethtool_ops = &macb_ethtool_ops;\n\n\terr = devm_request_irq(&pdev->dev, dev->irq, at91ether_interrupt,\n\t\t\t       0, dev->name, dev);\n\tif (err)\n\t\treturn err;\n\n\tmacb_writel(bp, NCR, 0);\n\n\tmacb_writel(bp, NCFGR, MACB_BF(CLK, MACB_CLK_DIV32) | MACB_BIT(BIG));\n\n\treturn 0;\n}\n\nstatic unsigned long fu540_macb_tx_recalc_rate(struct clk_hw *hw,\n\t\t\t\t\t       unsigned long parent_rate)\n{\n\treturn mgmt->rate;\n}\n\nstatic long fu540_macb_tx_round_rate(struct clk_hw *hw, unsigned long rate,\n\t\t\t\t     unsigned long *parent_rate)\n{\n\tif (WARN_ON(rate < 2500000))\n\t\treturn 2500000;\n\telse if (rate == 2500000)\n\t\treturn 2500000;\n\telse if (WARN_ON(rate < 13750000))\n\t\treturn 2500000;\n\telse if (WARN_ON(rate < 25000000))\n\t\treturn 25000000;\n\telse if (rate == 25000000)\n\t\treturn 25000000;\n\telse if (WARN_ON(rate < 75000000))\n\t\treturn 25000000;\n\telse if (WARN_ON(rate < 125000000))\n\t\treturn 125000000;\n\telse if (rate == 125000000)\n\t\treturn 125000000;\n\n\tWARN_ON(rate > 125000000);\n\n\treturn 125000000;\n}\n\nstatic int fu540_macb_tx_set_rate(struct clk_hw *hw, unsigned long rate,\n\t\t\t\t  unsigned long parent_rate)\n{\n\trate = fu540_macb_tx_round_rate(hw, rate, &parent_rate);\n\tif (rate != 125000000)\n\t\tiowrite32(1, mgmt->reg);\n\telse\n\t\tiowrite32(0, mgmt->reg);\n\tmgmt->rate = rate;\n\n\treturn 0;\n}\n\nstatic const struct clk_ops fu540_c000_ops = {\n\t.recalc_rate = fu540_macb_tx_recalc_rate,\n\t.round_rate = fu540_macb_tx_round_rate,\n\t.set_rate = fu540_macb_tx_set_rate,\n};\n\nstatic int fu540_c000_clk_init(struct platform_device *pdev, struct clk **pclk,\n\t\t\t       struct clk **hclk, struct clk **tx_clk,\n\t\t\t       struct clk **rx_clk, struct clk **tsu_clk)\n{\n\tstruct clk_init_data init;\n\tint err = 0;\n\n\terr = macb_clk_init(pdev, pclk, hclk, tx_clk, rx_clk, tsu_clk);\n\tif (err)\n\t\treturn err;\n\n\tmgmt = devm_kzalloc(&pdev->dev, sizeof(*mgmt), GFP_KERNEL);\n\tif (!mgmt) {\n\t\terr = -ENOMEM;\n\t\tgoto err_disable_clks;\n\t}\n\n\tinit.name = \"sifive-gemgxl-mgmt\";\n\tinit.ops = &fu540_c000_ops;\n\tinit.flags = 0;\n\tinit.num_parents = 0;\n\n\tmgmt->rate = 0;\n\tmgmt->hw.init = &init;\n\n\t*tx_clk = devm_clk_register(&pdev->dev, &mgmt->hw);\n\tif (IS_ERR(*tx_clk)) {\n\t\terr = PTR_ERR(*tx_clk);\n\t\tgoto err_disable_clks;\n\t}\n\n\terr = clk_prepare_enable(*tx_clk);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"failed to enable tx_clk (%u)\\n\", err);\n\t\t*tx_clk = NULL;\n\t\tgoto err_disable_clks;\n\t} else {\n\t\tdev_info(&pdev->dev, \"Registered clk switch '%s'\\n\", init.name);\n\t}\n\n\treturn 0;\n\nerr_disable_clks:\n\tmacb_clks_disable(*pclk, *hclk, *tx_clk, *rx_clk, *tsu_clk);\n\n\treturn err;\n}\n\nstatic int fu540_c000_init(struct platform_device *pdev)\n{\n\tmgmt->reg = devm_platform_ioremap_resource(pdev, 1);\n\tif (IS_ERR(mgmt->reg))\n\t\treturn PTR_ERR(mgmt->reg);\n\n\treturn macb_init(pdev);\n}\n\nstatic int init_reset_optional(struct platform_device *pdev)\n{\n\tstruct net_device *dev = platform_get_drvdata(pdev);\n\tstruct macb *bp = netdev_priv(dev);\n\tint ret;\n\n\tif (bp->phy_interface == PHY_INTERFACE_MODE_SGMII) {\n\t\t \n\t\tbp->sgmii_phy = devm_phy_optional_get(&pdev->dev, NULL);\n\n\t\tif (IS_ERR(bp->sgmii_phy))\n\t\t\treturn dev_err_probe(&pdev->dev, PTR_ERR(bp->sgmii_phy),\n\t\t\t\t\t     \"failed to get SGMII PHY\\n\");\n\n\t\tret = phy_init(bp->sgmii_phy);\n\t\tif (ret)\n\t\t\treturn dev_err_probe(&pdev->dev, ret,\n\t\t\t\t\t     \"failed to init SGMII PHY\\n\");\n\n\t\tret = zynqmp_pm_is_function_supported(PM_IOCTL, IOCTL_SET_GEM_CONFIG);\n\t\tif (!ret) {\n\t\t\tu32 pm_info[2];\n\n\t\t\tret = of_property_read_u32_array(pdev->dev.of_node, \"power-domains\",\n\t\t\t\t\t\t\t pm_info, ARRAY_SIZE(pm_info));\n\t\t\tif (ret) {\n\t\t\t\tdev_err(&pdev->dev, \"Failed to read power management information\\n\");\n\t\t\t\tgoto err_out_phy_exit;\n\t\t\t}\n\t\t\tret = zynqmp_pm_set_gem_config(pm_info[1], GEM_CONFIG_FIXED, 0);\n\t\t\tif (ret)\n\t\t\t\tgoto err_out_phy_exit;\n\n\t\t\tret = zynqmp_pm_set_gem_config(pm_info[1], GEM_CONFIG_SGMII_MODE, 1);\n\t\t\tif (ret)\n\t\t\t\tgoto err_out_phy_exit;\n\t\t}\n\n\t}\n\n\t \n\tret = device_reset_optional(&pdev->dev);\n\tif (ret) {\n\t\tphy_exit(bp->sgmii_phy);\n\t\treturn dev_err_probe(&pdev->dev, ret, \"failed to reset controller\");\n\t}\n\n\tret = macb_init(pdev);\n\nerr_out_phy_exit:\n\tif (ret)\n\t\tphy_exit(bp->sgmii_phy);\n\n\treturn ret;\n}\n\nstatic const struct macb_usrio_config sama7g5_usrio = {\n\t.mii = 0,\n\t.rmii = 1,\n\t.rgmii = 2,\n\t.refclk = BIT(2),\n\t.hdfctlen = BIT(6),\n};\n\nstatic const struct macb_config fu540_c000_config = {\n\t.caps = MACB_CAPS_GIGABIT_MODE_AVAILABLE | MACB_CAPS_JUMBO |\n\t\tMACB_CAPS_GEM_HAS_PTP,\n\t.dma_burst_length = 16,\n\t.clk_init = fu540_c000_clk_init,\n\t.init = fu540_c000_init,\n\t.jumbo_max_len = 10240,\n\t.usrio = &macb_default_usrio,\n};\n\nstatic const struct macb_config at91sam9260_config = {\n\t.caps = MACB_CAPS_USRIO_HAS_CLKEN | MACB_CAPS_USRIO_DEFAULT_IS_MII_GMII,\n\t.clk_init = macb_clk_init,\n\t.init = macb_init,\n\t.usrio = &macb_default_usrio,\n};\n\nstatic const struct macb_config sama5d3macb_config = {\n\t.caps = MACB_CAPS_SG_DISABLED |\n\t\tMACB_CAPS_USRIO_HAS_CLKEN | MACB_CAPS_USRIO_DEFAULT_IS_MII_GMII,\n\t.clk_init = macb_clk_init,\n\t.init = macb_init,\n\t.usrio = &macb_default_usrio,\n};\n\nstatic const struct macb_config pc302gem_config = {\n\t.caps = MACB_CAPS_SG_DISABLED | MACB_CAPS_GIGABIT_MODE_AVAILABLE,\n\t.dma_burst_length = 16,\n\t.clk_init = macb_clk_init,\n\t.init = macb_init,\n\t.usrio = &macb_default_usrio,\n};\n\nstatic const struct macb_config sama5d2_config = {\n\t.caps = MACB_CAPS_USRIO_DEFAULT_IS_MII_GMII,\n\t.dma_burst_length = 16,\n\t.clk_init = macb_clk_init,\n\t.init = macb_init,\n\t.usrio = &macb_default_usrio,\n};\n\nstatic const struct macb_config sama5d29_config = {\n\t.caps = MACB_CAPS_USRIO_DEFAULT_IS_MII_GMII | MACB_CAPS_GEM_HAS_PTP,\n\t.dma_burst_length = 16,\n\t.clk_init = macb_clk_init,\n\t.init = macb_init,\n\t.usrio = &macb_default_usrio,\n};\n\nstatic const struct macb_config sama5d3_config = {\n\t.caps = MACB_CAPS_SG_DISABLED | MACB_CAPS_GIGABIT_MODE_AVAILABLE |\n\t\tMACB_CAPS_USRIO_DEFAULT_IS_MII_GMII | MACB_CAPS_JUMBO,\n\t.dma_burst_length = 16,\n\t.clk_init = macb_clk_init,\n\t.init = macb_init,\n\t.jumbo_max_len = 10240,\n\t.usrio = &macb_default_usrio,\n};\n\nstatic const struct macb_config sama5d4_config = {\n\t.caps = MACB_CAPS_USRIO_DEFAULT_IS_MII_GMII,\n\t.dma_burst_length = 4,\n\t.clk_init = macb_clk_init,\n\t.init = macb_init,\n\t.usrio = &macb_default_usrio,\n};\n\nstatic const struct macb_config emac_config = {\n\t.caps = MACB_CAPS_NEEDS_RSTONUBR | MACB_CAPS_MACB_IS_EMAC,\n\t.clk_init = at91ether_clk_init,\n\t.init = at91ether_init,\n\t.usrio = &macb_default_usrio,\n};\n\nstatic const struct macb_config np4_config = {\n\t.caps = MACB_CAPS_USRIO_DISABLED,\n\t.clk_init = macb_clk_init,\n\t.init = macb_init,\n\t.usrio = &macb_default_usrio,\n};\n\nstatic const struct macb_config zynqmp_config = {\n\t.caps = MACB_CAPS_GIGABIT_MODE_AVAILABLE |\n\t\tMACB_CAPS_JUMBO |\n\t\tMACB_CAPS_GEM_HAS_PTP | MACB_CAPS_BD_RD_PREFETCH,\n\t.dma_burst_length = 16,\n\t.clk_init = macb_clk_init,\n\t.init = init_reset_optional,\n\t.jumbo_max_len = 10240,\n\t.usrio = &macb_default_usrio,\n};\n\nstatic const struct macb_config zynq_config = {\n\t.caps = MACB_CAPS_GIGABIT_MODE_AVAILABLE | MACB_CAPS_NO_GIGABIT_HALF |\n\t\tMACB_CAPS_NEEDS_RSTONUBR,\n\t.dma_burst_length = 16,\n\t.clk_init = macb_clk_init,\n\t.init = macb_init,\n\t.usrio = &macb_default_usrio,\n};\n\nstatic const struct macb_config mpfs_config = {\n\t.caps = MACB_CAPS_GIGABIT_MODE_AVAILABLE |\n\t\tMACB_CAPS_JUMBO |\n\t\tMACB_CAPS_GEM_HAS_PTP,\n\t.dma_burst_length = 16,\n\t.clk_init = macb_clk_init,\n\t.init = init_reset_optional,\n\t.usrio = &macb_default_usrio,\n\t.max_tx_length = 4040,  \n\t.jumbo_max_len = 4040,\n};\n\nstatic const struct macb_config sama7g5_gem_config = {\n\t.caps = MACB_CAPS_GIGABIT_MODE_AVAILABLE | MACB_CAPS_CLK_HW_CHG |\n\t\tMACB_CAPS_MIIONRGMII | MACB_CAPS_GEM_HAS_PTP,\n\t.dma_burst_length = 16,\n\t.clk_init = macb_clk_init,\n\t.init = macb_init,\n\t.usrio = &sama7g5_usrio,\n};\n\nstatic const struct macb_config sama7g5_emac_config = {\n\t.caps = MACB_CAPS_USRIO_DEFAULT_IS_MII_GMII |\n\t\tMACB_CAPS_USRIO_HAS_CLKEN | MACB_CAPS_MIIONRGMII |\n\t\tMACB_CAPS_GEM_HAS_PTP,\n\t.dma_burst_length = 16,\n\t.clk_init = macb_clk_init,\n\t.init = macb_init,\n\t.usrio = &sama7g5_usrio,\n};\n\nstatic const struct macb_config versal_config = {\n\t.caps = MACB_CAPS_GIGABIT_MODE_AVAILABLE | MACB_CAPS_JUMBO |\n\t\tMACB_CAPS_GEM_HAS_PTP | MACB_CAPS_BD_RD_PREFETCH | MACB_CAPS_NEED_TSUCLK,\n\t.dma_burst_length = 16,\n\t.clk_init = macb_clk_init,\n\t.init = init_reset_optional,\n\t.jumbo_max_len = 10240,\n\t.usrio = &macb_default_usrio,\n};\n\nstatic const struct of_device_id macb_dt_ids[] = {\n\t{ .compatible = \"cdns,at91sam9260-macb\", .data = &at91sam9260_config },\n\t{ .compatible = \"cdns,macb\" },\n\t{ .compatible = \"cdns,np4-macb\", .data = &np4_config },\n\t{ .compatible = \"cdns,pc302-gem\", .data = &pc302gem_config },\n\t{ .compatible = \"cdns,gem\", .data = &pc302gem_config },\n\t{ .compatible = \"cdns,sam9x60-macb\", .data = &at91sam9260_config },\n\t{ .compatible = \"atmel,sama5d2-gem\", .data = &sama5d2_config },\n\t{ .compatible = \"atmel,sama5d29-gem\", .data = &sama5d29_config },\n\t{ .compatible = \"atmel,sama5d3-gem\", .data = &sama5d3_config },\n\t{ .compatible = \"atmel,sama5d3-macb\", .data = &sama5d3macb_config },\n\t{ .compatible = \"atmel,sama5d4-gem\", .data = &sama5d4_config },\n\t{ .compatible = \"cdns,at91rm9200-emac\", .data = &emac_config },\n\t{ .compatible = \"cdns,emac\", .data = &emac_config },\n\t{ .compatible = \"cdns,zynqmp-gem\", .data = &zynqmp_config},  \n\t{ .compatible = \"cdns,zynq-gem\", .data = &zynq_config },  \n\t{ .compatible = \"sifive,fu540-c000-gem\", .data = &fu540_c000_config },\n\t{ .compatible = \"microchip,mpfs-macb\", .data = &mpfs_config },\n\t{ .compatible = \"microchip,sama7g5-gem\", .data = &sama7g5_gem_config },\n\t{ .compatible = \"microchip,sama7g5-emac\", .data = &sama7g5_emac_config },\n\t{ .compatible = \"xlnx,zynqmp-gem\", .data = &zynqmp_config},\n\t{ .compatible = \"xlnx,zynq-gem\", .data = &zynq_config },\n\t{ .compatible = \"xlnx,versal-gem\", .data = &versal_config},\n\t{   }\n};\nMODULE_DEVICE_TABLE(of, macb_dt_ids);\n#endif  \n\nstatic const struct macb_config default_gem_config = {\n\t.caps = MACB_CAPS_GIGABIT_MODE_AVAILABLE |\n\t\tMACB_CAPS_JUMBO |\n\t\tMACB_CAPS_GEM_HAS_PTP,\n\t.dma_burst_length = 16,\n\t.clk_init = macb_clk_init,\n\t.init = macb_init,\n\t.usrio = &macb_default_usrio,\n\t.jumbo_max_len = 10240,\n};\n\nstatic int macb_probe(struct platform_device *pdev)\n{\n\tconst struct macb_config *macb_config = &default_gem_config;\n\tint (*clk_init)(struct platform_device *, struct clk **,\n\t\t\tstruct clk **, struct clk **,  struct clk **,\n\t\t\tstruct clk **) = macb_config->clk_init;\n\tint (*init)(struct platform_device *) = macb_config->init;\n\tstruct device_node *np = pdev->dev.of_node;\n\tstruct clk *pclk, *hclk = NULL, *tx_clk = NULL, *rx_clk = NULL;\n\tstruct clk *tsu_clk = NULL;\n\tunsigned int queue_mask, num_queues;\n\tbool native_io;\n\tphy_interface_t interface;\n\tstruct net_device *dev;\n\tstruct resource *regs;\n\tu32 wtrmrk_rst_val;\n\tvoid __iomem *mem;\n\tstruct macb *bp;\n\tint err, val;\n\n\tmem = devm_platform_get_and_ioremap_resource(pdev, 0, &regs);\n\tif (IS_ERR(mem))\n\t\treturn PTR_ERR(mem);\n\n\tif (np) {\n\t\tconst struct of_device_id *match;\n\n\t\tmatch = of_match_node(macb_dt_ids, np);\n\t\tif (match && match->data) {\n\t\t\tmacb_config = match->data;\n\t\t\tclk_init = macb_config->clk_init;\n\t\t\tinit = macb_config->init;\n\t\t}\n\t}\n\n\terr = clk_init(pdev, &pclk, &hclk, &tx_clk, &rx_clk, &tsu_clk);\n\tif (err)\n\t\treturn err;\n\n\tpm_runtime_set_autosuspend_delay(&pdev->dev, MACB_PM_TIMEOUT);\n\tpm_runtime_use_autosuspend(&pdev->dev);\n\tpm_runtime_get_noresume(&pdev->dev);\n\tpm_runtime_set_active(&pdev->dev);\n\tpm_runtime_enable(&pdev->dev);\n\tnative_io = hw_is_native_io(mem);\n\n\tmacb_probe_queues(mem, native_io, &queue_mask, &num_queues);\n\tdev = alloc_etherdev_mq(sizeof(*bp), num_queues);\n\tif (!dev) {\n\t\terr = -ENOMEM;\n\t\tgoto err_disable_clocks;\n\t}\n\n\tdev->base_addr = regs->start;\n\n\tSET_NETDEV_DEV(dev, &pdev->dev);\n\n\tbp = netdev_priv(dev);\n\tbp->pdev = pdev;\n\tbp->dev = dev;\n\tbp->regs = mem;\n\tbp->native_io = native_io;\n\tif (native_io) {\n\t\tbp->macb_reg_readl = hw_readl_native;\n\t\tbp->macb_reg_writel = hw_writel_native;\n\t} else {\n\t\tbp->macb_reg_readl = hw_readl;\n\t\tbp->macb_reg_writel = hw_writel;\n\t}\n\tbp->num_queues = num_queues;\n\tbp->queue_mask = queue_mask;\n\tif (macb_config)\n\t\tbp->dma_burst_length = macb_config->dma_burst_length;\n\tbp->pclk = pclk;\n\tbp->hclk = hclk;\n\tbp->tx_clk = tx_clk;\n\tbp->rx_clk = rx_clk;\n\tbp->tsu_clk = tsu_clk;\n\tif (macb_config)\n\t\tbp->jumbo_max_len = macb_config->jumbo_max_len;\n\n\tif (!hw_is_gem(bp->regs, bp->native_io))\n\t\tbp->max_tx_length = MACB_MAX_TX_LEN;\n\telse if (macb_config->max_tx_length)\n\t\tbp->max_tx_length = macb_config->max_tx_length;\n\telse\n\t\tbp->max_tx_length = GEM_MAX_TX_LEN;\n\n\tbp->wol = 0;\n\tif (of_property_read_bool(np, \"magic-packet\"))\n\t\tbp->wol |= MACB_WOL_HAS_MAGIC_PACKET;\n\tdevice_set_wakeup_capable(&pdev->dev, bp->wol & MACB_WOL_HAS_MAGIC_PACKET);\n\n\tbp->usrio = macb_config->usrio;\n\n\t \n\tif (GEM_BFEXT(PBUF_CUTTHRU, gem_readl(bp, DCFG6))) {\n\t\terr = of_property_read_u32(bp->pdev->dev.of_node,\n\t\t\t\t\t   \"cdns,rx-watermark\",\n\t\t\t\t\t   &bp->rx_watermark);\n\n\t\tif (!err) {\n\t\t\t \n\t\t\twtrmrk_rst_val = (1 << (GEM_BFEXT(RX_PBUF_ADDR, gem_readl(bp, DCFG2)))) - 1;\n\t\t\tif (bp->rx_watermark > wtrmrk_rst_val || !bp->rx_watermark) {\n\t\t\t\tdev_info(&bp->pdev->dev, \"Invalid watermark value\\n\");\n\t\t\t\tbp->rx_watermark = 0;\n\t\t\t}\n\t\t}\n\t}\n\tspin_lock_init(&bp->lock);\n\n\t \n\tmacb_configure_caps(bp, macb_config);\n\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\tif (GEM_BFEXT(DAW64, gem_readl(bp, DCFG6))) {\n\t\tdma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(44));\n\t\tbp->hw_dma_cap |= HW_DMA_CAP_64B;\n\t}\n#endif\n\tplatform_set_drvdata(pdev, dev);\n\n\tdev->irq = platform_get_irq(pdev, 0);\n\tif (dev->irq < 0) {\n\t\terr = dev->irq;\n\t\tgoto err_out_free_netdev;\n\t}\n\n\t \n\tdev->min_mtu = GEM_MTU_MIN_SIZE;\n\tif ((bp->caps & MACB_CAPS_JUMBO) && bp->jumbo_max_len)\n\t\tdev->max_mtu = bp->jumbo_max_len - ETH_HLEN - ETH_FCS_LEN;\n\telse\n\t\tdev->max_mtu = ETH_DATA_LEN;\n\n\tif (bp->caps & MACB_CAPS_BD_RD_PREFETCH) {\n\t\tval = GEM_BFEXT(RXBD_RDBUFF, gem_readl(bp, DCFG10));\n\t\tif (val)\n\t\t\tbp->rx_bd_rd_prefetch = (2 << (val - 1)) *\n\t\t\t\t\t\tmacb_dma_desc_get_size(bp);\n\n\t\tval = GEM_BFEXT(TXBD_RDBUFF, gem_readl(bp, DCFG10));\n\t\tif (val)\n\t\t\tbp->tx_bd_rd_prefetch = (2 << (val - 1)) *\n\t\t\t\t\t\tmacb_dma_desc_get_size(bp);\n\t}\n\n\tbp->rx_intr_mask = MACB_RX_INT_FLAGS;\n\tif (bp->caps & MACB_CAPS_NEEDS_RSTONUBR)\n\t\tbp->rx_intr_mask |= MACB_BIT(RXUBR);\n\n\terr = of_get_ethdev_address(np, bp->dev);\n\tif (err == -EPROBE_DEFER)\n\t\tgoto err_out_free_netdev;\n\telse if (err)\n\t\tmacb_get_hwaddr(bp);\n\n\terr = of_get_phy_mode(np, &interface);\n\tif (err)\n\t\t \n\t\tbp->phy_interface = PHY_INTERFACE_MODE_MII;\n\telse\n\t\tbp->phy_interface = interface;\n\n\t \n\terr = init(pdev);\n\tif (err)\n\t\tgoto err_out_free_netdev;\n\n\terr = macb_mii_init(bp);\n\tif (err)\n\t\tgoto err_out_phy_exit;\n\n\tnetif_carrier_off(dev);\n\n\terr = register_netdev(dev);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"Cannot register net device, aborting.\\n\");\n\t\tgoto err_out_unregister_mdio;\n\t}\n\n\ttasklet_setup(&bp->hresp_err_tasklet, macb_hresp_error_task);\n\n\tnetdev_info(dev, \"Cadence %s rev 0x%08x at 0x%08lx irq %d (%pM)\\n\",\n\t\t    macb_is_gem(bp) ? \"GEM\" : \"MACB\", macb_readl(bp, MID),\n\t\t    dev->base_addr, dev->irq, dev->dev_addr);\n\n\tpm_runtime_mark_last_busy(&bp->pdev->dev);\n\tpm_runtime_put_autosuspend(&bp->pdev->dev);\n\n\treturn 0;\n\nerr_out_unregister_mdio:\n\tmdiobus_unregister(bp->mii_bus);\n\tmdiobus_free(bp->mii_bus);\n\nerr_out_phy_exit:\n\tphy_exit(bp->sgmii_phy);\n\nerr_out_free_netdev:\n\tfree_netdev(dev);\n\nerr_disable_clocks:\n\tmacb_clks_disable(pclk, hclk, tx_clk, rx_clk, tsu_clk);\n\tpm_runtime_disable(&pdev->dev);\n\tpm_runtime_set_suspended(&pdev->dev);\n\tpm_runtime_dont_use_autosuspend(&pdev->dev);\n\n\treturn err;\n}\n\nstatic int macb_remove(struct platform_device *pdev)\n{\n\tstruct net_device *dev;\n\tstruct macb *bp;\n\n\tdev = platform_get_drvdata(pdev);\n\n\tif (dev) {\n\t\tbp = netdev_priv(dev);\n\t\tphy_exit(bp->sgmii_phy);\n\t\tmdiobus_unregister(bp->mii_bus);\n\t\tmdiobus_free(bp->mii_bus);\n\n\t\tunregister_netdev(dev);\n\t\ttasklet_kill(&bp->hresp_err_tasklet);\n\t\tpm_runtime_disable(&pdev->dev);\n\t\tpm_runtime_dont_use_autosuspend(&pdev->dev);\n\t\tif (!pm_runtime_suspended(&pdev->dev)) {\n\t\t\tmacb_clks_disable(bp->pclk, bp->hclk, bp->tx_clk,\n\t\t\t\t\t  bp->rx_clk, bp->tsu_clk);\n\t\t\tpm_runtime_set_suspended(&pdev->dev);\n\t\t}\n\t\tphylink_destroy(bp->phylink);\n\t\tfree_netdev(dev);\n\t}\n\n\treturn 0;\n}\n\nstatic int __maybe_unused macb_suspend(struct device *dev)\n{\n\tstruct net_device *netdev = dev_get_drvdata(dev);\n\tstruct macb *bp = netdev_priv(netdev);\n\tstruct macb_queue *queue;\n\tunsigned long flags;\n\tunsigned int q;\n\tint err;\n\n\tif (!device_may_wakeup(&bp->dev->dev))\n\t\tphy_exit(bp->sgmii_phy);\n\n\tif (!netif_running(netdev))\n\t\treturn 0;\n\n\tif (bp->wol & MACB_WOL_ENABLED) {\n\t\tspin_lock_irqsave(&bp->lock, flags);\n\t\t \n\t\tmacb_writel(bp, TSR, -1);\n\t\tmacb_writel(bp, RSR, -1);\n\t\tfor (q = 0, queue = bp->queues; q < bp->num_queues;\n\t\t     ++q, ++queue) {\n\t\t\t \n\t\t\tqueue_writel(queue, IDR, -1);\n\t\t\tqueue_readl(queue, ISR);\n\t\t\tif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\n\t\t\t\tqueue_writel(queue, ISR, -1);\n\t\t}\n\t\t \n\t\tdevm_free_irq(dev, bp->queues[0].irq, bp->queues);\n\t\tif (macb_is_gem(bp)) {\n\t\t\terr = devm_request_irq(dev, bp->queues[0].irq, gem_wol_interrupt,\n\t\t\t\t\t       IRQF_SHARED, netdev->name, bp->queues);\n\t\t\tif (err) {\n\t\t\t\tdev_err(dev,\n\t\t\t\t\t\"Unable to request IRQ %d (error %d)\\n\",\n\t\t\t\t\tbp->queues[0].irq, err);\n\t\t\t\tspin_unlock_irqrestore(&bp->lock, flags);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t\tqueue_writel(bp->queues, IER, GEM_BIT(WOL));\n\t\t\tgem_writel(bp, WOL, MACB_BIT(MAG));\n\t\t} else {\n\t\t\terr = devm_request_irq(dev, bp->queues[0].irq, macb_wol_interrupt,\n\t\t\t\t\t       IRQF_SHARED, netdev->name, bp->queues);\n\t\t\tif (err) {\n\t\t\t\tdev_err(dev,\n\t\t\t\t\t\"Unable to request IRQ %d (error %d)\\n\",\n\t\t\t\t\tbp->queues[0].irq, err);\n\t\t\t\tspin_unlock_irqrestore(&bp->lock, flags);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t\tqueue_writel(bp->queues, IER, MACB_BIT(WOL));\n\t\t\tmacb_writel(bp, WOL, MACB_BIT(MAG));\n\t\t}\n\t\tspin_unlock_irqrestore(&bp->lock, flags);\n\n\t\tenable_irq_wake(bp->queues[0].irq);\n\t}\n\n\tnetif_device_detach(netdev);\n\tfor (q = 0, queue = bp->queues; q < bp->num_queues;\n\t     ++q, ++queue) {\n\t\tnapi_disable(&queue->napi_rx);\n\t\tnapi_disable(&queue->napi_tx);\n\t}\n\n\tif (!(bp->wol & MACB_WOL_ENABLED)) {\n\t\trtnl_lock();\n\t\tphylink_stop(bp->phylink);\n\t\trtnl_unlock();\n\t\tspin_lock_irqsave(&bp->lock, flags);\n\t\tmacb_reset_hw(bp);\n\t\tspin_unlock_irqrestore(&bp->lock, flags);\n\t}\n\n\tif (!(bp->caps & MACB_CAPS_USRIO_DISABLED))\n\t\tbp->pm_data.usrio = macb_or_gem_readl(bp, USRIO);\n\n\tif (netdev->hw_features & NETIF_F_NTUPLE)\n\t\tbp->pm_data.scrt2 = gem_readl_n(bp, ETHT, SCRT2_ETHT);\n\n\tif (bp->ptp_info)\n\t\tbp->ptp_info->ptp_remove(netdev);\n\tif (!device_may_wakeup(dev))\n\t\tpm_runtime_force_suspend(dev);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused macb_resume(struct device *dev)\n{\n\tstruct net_device *netdev = dev_get_drvdata(dev);\n\tstruct macb *bp = netdev_priv(netdev);\n\tstruct macb_queue *queue;\n\tunsigned long flags;\n\tunsigned int q;\n\tint err;\n\n\tif (!device_may_wakeup(&bp->dev->dev))\n\t\tphy_init(bp->sgmii_phy);\n\n\tif (!netif_running(netdev))\n\t\treturn 0;\n\n\tif (!device_may_wakeup(dev))\n\t\tpm_runtime_force_resume(dev);\n\n\tif (bp->wol & MACB_WOL_ENABLED) {\n\t\tspin_lock_irqsave(&bp->lock, flags);\n\t\t \n\t\tif (macb_is_gem(bp)) {\n\t\t\tqueue_writel(bp->queues, IDR, GEM_BIT(WOL));\n\t\t\tgem_writel(bp, WOL, 0);\n\t\t} else {\n\t\t\tqueue_writel(bp->queues, IDR, MACB_BIT(WOL));\n\t\t\tmacb_writel(bp, WOL, 0);\n\t\t}\n\t\t \n\t\tqueue_readl(bp->queues, ISR);\n\t\tif (bp->caps & MACB_CAPS_ISR_CLEAR_ON_WRITE)\n\t\t\tqueue_writel(bp->queues, ISR, -1);\n\t\t \n\t\tdevm_free_irq(dev, bp->queues[0].irq, bp->queues);\n\t\terr = devm_request_irq(dev, bp->queues[0].irq, macb_interrupt,\n\t\t\t\t       IRQF_SHARED, netdev->name, bp->queues);\n\t\tif (err) {\n\t\t\tdev_err(dev,\n\t\t\t\t\"Unable to request IRQ %d (error %d)\\n\",\n\t\t\t\tbp->queues[0].irq, err);\n\t\t\tspin_unlock_irqrestore(&bp->lock, flags);\n\t\t\treturn err;\n\t\t}\n\t\tspin_unlock_irqrestore(&bp->lock, flags);\n\n\t\tdisable_irq_wake(bp->queues[0].irq);\n\n\t\t \n\t\trtnl_lock();\n\t\tphylink_stop(bp->phylink);\n\t\trtnl_unlock();\n\t}\n\n\tfor (q = 0, queue = bp->queues; q < bp->num_queues;\n\t     ++q, ++queue) {\n\t\tnapi_enable(&queue->napi_rx);\n\t\tnapi_enable(&queue->napi_tx);\n\t}\n\n\tif (netdev->hw_features & NETIF_F_NTUPLE)\n\t\tgem_writel_n(bp, ETHT, SCRT2_ETHT, bp->pm_data.scrt2);\n\n\tif (!(bp->caps & MACB_CAPS_USRIO_DISABLED))\n\t\tmacb_or_gem_writel(bp, USRIO, bp->pm_data.usrio);\n\n\tmacb_writel(bp, NCR, MACB_BIT(MPE));\n\tmacb_init_hw(bp);\n\tmacb_set_rx_mode(netdev);\n\tmacb_restore_features(bp);\n\trtnl_lock();\n\n\tphylink_start(bp->phylink);\n\trtnl_unlock();\n\n\tnetif_device_attach(netdev);\n\tif (bp->ptp_info)\n\t\tbp->ptp_info->ptp_init(netdev);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused macb_runtime_suspend(struct device *dev)\n{\n\tstruct net_device *netdev = dev_get_drvdata(dev);\n\tstruct macb *bp = netdev_priv(netdev);\n\n\tif (!(device_may_wakeup(dev)))\n\t\tmacb_clks_disable(bp->pclk, bp->hclk, bp->tx_clk, bp->rx_clk, bp->tsu_clk);\n\telse if (!(bp->caps & MACB_CAPS_NEED_TSUCLK))\n\t\tmacb_clks_disable(NULL, NULL, NULL, NULL, bp->tsu_clk);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused macb_runtime_resume(struct device *dev)\n{\n\tstruct net_device *netdev = dev_get_drvdata(dev);\n\tstruct macb *bp = netdev_priv(netdev);\n\n\tif (!(device_may_wakeup(dev))) {\n\t\tclk_prepare_enable(bp->pclk);\n\t\tclk_prepare_enable(bp->hclk);\n\t\tclk_prepare_enable(bp->tx_clk);\n\t\tclk_prepare_enable(bp->rx_clk);\n\t\tclk_prepare_enable(bp->tsu_clk);\n\t} else if (!(bp->caps & MACB_CAPS_NEED_TSUCLK)) {\n\t\tclk_prepare_enable(bp->tsu_clk);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct dev_pm_ops macb_pm_ops = {\n\tSET_SYSTEM_SLEEP_PM_OPS(macb_suspend, macb_resume)\n\tSET_RUNTIME_PM_OPS(macb_runtime_suspend, macb_runtime_resume, NULL)\n};\n\nstatic struct platform_driver macb_driver = {\n\t.probe\t\t= macb_probe,\n\t.remove\t\t= macb_remove,\n\t.driver\t\t= {\n\t\t.name\t\t= \"macb\",\n\t\t.of_match_table\t= of_match_ptr(macb_dt_ids),\n\t\t.pm\t= &macb_pm_ops,\n\t},\n};\n\nmodule_platform_driver(macb_driver);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"Cadence MACB/GEM Ethernet driver\");\nMODULE_AUTHOR(\"Haavard Skinnemoen (Atmel)\");\nMODULE_ALIAS(\"platform:macb\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}