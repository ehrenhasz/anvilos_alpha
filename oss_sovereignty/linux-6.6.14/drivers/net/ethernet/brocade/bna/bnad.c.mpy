{
  "module_name": "bnad.c",
  "hash_id": "746529c45827dc6e30540effd3fd8923acd730d7aaa7e3f8445019e9a5c6bd03",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/brocade/bna/bnad.c",
  "human_readable_source": "\n \n \n#include <linux/bitops.h>\n#include <linux/netdevice.h>\n#include <linux/skbuff.h>\n#include <linux/etherdevice.h>\n#include <linux/in.h>\n#include <linux/ethtool.h>\n#include <linux/if_vlan.h>\n#include <linux/if_ether.h>\n#include <linux/ip.h>\n#include <linux/prefetch.h>\n#include <linux/module.h>\n\n#include \"bnad.h\"\n#include \"bna.h\"\n#include \"cna.h\"\n\nstatic DEFINE_MUTEX(bnad_fwimg_mutex);\n\n \nstatic uint bnad_msix_disable;\nmodule_param(bnad_msix_disable, uint, 0444);\nMODULE_PARM_DESC(bnad_msix_disable, \"Disable MSIX mode\");\n\nstatic uint bnad_ioc_auto_recover = 1;\nmodule_param(bnad_ioc_auto_recover, uint, 0444);\nMODULE_PARM_DESC(bnad_ioc_auto_recover, \"Enable / Disable auto recovery\");\n\nstatic uint bna_debugfs_enable = 1;\nmodule_param(bna_debugfs_enable, uint, 0644);\nMODULE_PARM_DESC(bna_debugfs_enable, \"Enables debugfs feature, default=1,\"\n\t\t \" Range[false:0|true:1]\");\n\n \nstatic u32 bnad_rxqs_per_cq = 2;\nstatic atomic_t bna_id;\nstatic const u8 bnad_bcast_addr[] __aligned(2) =\n\t{ 0xff, 0xff, 0xff, 0xff, 0xff, 0xff };\n\n \n#define BNAD_GET_MBOX_IRQ(_bnad)\t\t\t\t\\\n\t(((_bnad)->cfg_flags & BNAD_CF_MSIX) ?\t\t\t\\\n\t ((_bnad)->msix_table[BNAD_MAILBOX_MSIX_INDEX].vector) : \\\n\t ((_bnad)->pcidev->irq))\n\n#define BNAD_FILL_UNMAPQ_MEM_REQ(_res_info, _num, _size)\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\t(_res_info)->res_type = BNA_RES_T_MEM;\t\t\t\\\n\t(_res_info)->res_u.mem_info.mem_type = BNA_MEM_T_KVA;\t\\\n\t(_res_info)->res_u.mem_info.num = (_num);\t\t\\\n\t(_res_info)->res_u.mem_info.len = (_size);\t\t\\\n} while (0)\n\n \nstatic void\nbnad_cq_cleanup(struct bnad *bnad, struct bna_ccb *ccb)\n{\n\tstruct bna_cq_entry *cmpl;\n\tint i;\n\n\tfor (i = 0; i < ccb->q_depth; i++) {\n\t\tcmpl = &((struct bna_cq_entry *)ccb->sw_q)[i];\n\t\tcmpl->valid = 0;\n\t}\n}\n\n \n\n\n \nstatic u32\nbnad_tx_buff_unmap(struct bnad *bnad,\n\t\t\t      struct bnad_tx_unmap *unmap_q,\n\t\t\t      u32 q_depth, u32 index)\n{\n\tstruct bnad_tx_unmap *unmap;\n\tstruct sk_buff *skb;\n\tint vector, nvecs;\n\n\tunmap = &unmap_q[index];\n\tnvecs = unmap->nvecs;\n\n\tskb = unmap->skb;\n\tunmap->skb = NULL;\n\tunmap->nvecs = 0;\n\tdma_unmap_single(&bnad->pcidev->dev,\n\t\tdma_unmap_addr(&unmap->vectors[0], dma_addr),\n\t\tskb_headlen(skb), DMA_TO_DEVICE);\n\tdma_unmap_addr_set(&unmap->vectors[0], dma_addr, 0);\n\tnvecs--;\n\n\tvector = 0;\n\twhile (nvecs) {\n\t\tvector++;\n\t\tif (vector == BFI_TX_MAX_VECTORS_PER_WI) {\n\t\t\tvector = 0;\n\t\t\tBNA_QE_INDX_INC(index, q_depth);\n\t\t\tunmap = &unmap_q[index];\n\t\t}\n\n\t\tdma_unmap_page(&bnad->pcidev->dev,\n\t\t\tdma_unmap_addr(&unmap->vectors[vector], dma_addr),\n\t\t\tdma_unmap_len(&unmap->vectors[vector], dma_len),\n\t\t\tDMA_TO_DEVICE);\n\t\tdma_unmap_addr_set(&unmap->vectors[vector], dma_addr, 0);\n\t\tnvecs--;\n\t}\n\n\tBNA_QE_INDX_INC(index, q_depth);\n\n\treturn index;\n}\n\n \nstatic void\nbnad_txq_cleanup(struct bnad *bnad, struct bna_tcb *tcb)\n{\n\tstruct bnad_tx_unmap *unmap_q = tcb->unmap_q;\n\tstruct sk_buff *skb;\n\tint i;\n\n\tfor (i = 0; i < tcb->q_depth; i++) {\n\t\tskb = unmap_q[i].skb;\n\t\tif (!skb)\n\t\t\tcontinue;\n\t\tbnad_tx_buff_unmap(bnad, unmap_q, tcb->q_depth, i);\n\n\t\tdev_kfree_skb_any(skb);\n\t}\n}\n\n \nstatic u32\nbnad_txcmpl_process(struct bnad *bnad, struct bna_tcb *tcb)\n{\n\tu32 sent_packets = 0, sent_bytes = 0;\n\tu32 wis, unmap_wis, hw_cons, cons, q_depth;\n\tstruct bnad_tx_unmap *unmap_q = tcb->unmap_q;\n\tstruct bnad_tx_unmap *unmap;\n\tstruct sk_buff *skb;\n\n\t \n\tif (!test_bit(BNAD_TXQ_TX_STARTED, &tcb->flags))\n\t\treturn 0;\n\n\thw_cons = *(tcb->hw_consumer_index);\n\trmb();\n\tcons = tcb->consumer_index;\n\tq_depth = tcb->q_depth;\n\n\twis = BNA_Q_INDEX_CHANGE(cons, hw_cons, q_depth);\n\tBUG_ON(!(wis <= BNA_QE_IN_USE_CNT(tcb, tcb->q_depth)));\n\n\twhile (wis) {\n\t\tunmap = &unmap_q[cons];\n\n\t\tskb = unmap->skb;\n\n\t\tsent_packets++;\n\t\tsent_bytes += skb->len;\n\n\t\tunmap_wis = BNA_TXQ_WI_NEEDED(unmap->nvecs);\n\t\twis -= unmap_wis;\n\n\t\tcons = bnad_tx_buff_unmap(bnad, unmap_q, q_depth, cons);\n\t\tdev_kfree_skb_any(skb);\n\t}\n\n\t \n\ttcb->consumer_index = hw_cons;\n\n\ttcb->txq->tx_packets += sent_packets;\n\ttcb->txq->tx_bytes += sent_bytes;\n\n\treturn sent_packets;\n}\n\nstatic u32\nbnad_tx_complete(struct bnad *bnad, struct bna_tcb *tcb)\n{\n\tstruct net_device *netdev = bnad->netdev;\n\tu32 sent = 0;\n\n\tif (test_and_set_bit(BNAD_TXQ_FREE_SENT, &tcb->flags))\n\t\treturn 0;\n\n\tsent = bnad_txcmpl_process(bnad, tcb);\n\tif (sent) {\n\t\tif (netif_queue_stopped(netdev) &&\n\t\t    netif_carrier_ok(netdev) &&\n\t\t    BNA_QE_FREE_CNT(tcb, tcb->q_depth) >=\n\t\t\t\t    BNAD_NETIF_WAKE_THRESHOLD) {\n\t\t\tif (test_bit(BNAD_TXQ_TX_STARTED, &tcb->flags)) {\n\t\t\t\tnetif_wake_queue(netdev);\n\t\t\t\tBNAD_UPDATE_CTR(bnad, netif_queue_wakeup);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (likely(test_bit(BNAD_TXQ_TX_STARTED, &tcb->flags)))\n\t\tbna_ib_ack(tcb->i_dbell, sent);\n\n\tsmp_mb__before_atomic();\n\tclear_bit(BNAD_TXQ_FREE_SENT, &tcb->flags);\n\n\treturn sent;\n}\n\n \nstatic irqreturn_t\nbnad_msix_tx(int irq, void *data)\n{\n\tstruct bna_tcb *tcb = (struct bna_tcb *)data;\n\tstruct bnad *bnad = tcb->bnad;\n\n\tbnad_tx_complete(bnad, tcb);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic inline void\nbnad_rxq_alloc_uninit(struct bnad *bnad, struct bna_rcb *rcb)\n{\n\tstruct bnad_rx_unmap_q *unmap_q = rcb->unmap_q;\n\n\tunmap_q->reuse_pi = -1;\n\tunmap_q->alloc_order = -1;\n\tunmap_q->map_size = 0;\n\tunmap_q->type = BNAD_RXBUF_NONE;\n}\n\n \nstatic int\nbnad_rxq_alloc_init(struct bnad *bnad, struct bna_rcb *rcb)\n{\n\tstruct bnad_rx_unmap_q *unmap_q = rcb->unmap_q;\n\tint order;\n\n\tbnad_rxq_alloc_uninit(bnad, rcb);\n\n\torder = get_order(rcb->rxq->buffer_size);\n\n\tunmap_q->type = BNAD_RXBUF_PAGE;\n\n\tif (bna_is_small_rxq(rcb->id)) {\n\t\tunmap_q->alloc_order = 0;\n\t\tunmap_q->map_size = rcb->rxq->buffer_size;\n\t} else {\n\t\tif (rcb->rxq->multi_buffer) {\n\t\t\tunmap_q->alloc_order = 0;\n\t\t\tunmap_q->map_size = rcb->rxq->buffer_size;\n\t\t\tunmap_q->type = BNAD_RXBUF_MULTI_BUFF;\n\t\t} else {\n\t\t\tunmap_q->alloc_order = order;\n\t\t\tunmap_q->map_size =\n\t\t\t\t(rcb->rxq->buffer_size > 2048) ?\n\t\t\t\tPAGE_SIZE << order : 2048;\n\t\t}\n\t}\n\n\tBUG_ON((PAGE_SIZE << order) % unmap_q->map_size);\n\n\treturn 0;\n}\n\nstatic inline void\nbnad_rxq_cleanup_page(struct bnad *bnad, struct bnad_rx_unmap *unmap)\n{\n\tif (!unmap->page)\n\t\treturn;\n\n\tdma_unmap_page(&bnad->pcidev->dev,\n\t\t\tdma_unmap_addr(&unmap->vector, dma_addr),\n\t\t\tunmap->vector.len, DMA_FROM_DEVICE);\n\tput_page(unmap->page);\n\tunmap->page = NULL;\n\tdma_unmap_addr_set(&unmap->vector, dma_addr, 0);\n\tunmap->vector.len = 0;\n}\n\nstatic inline void\nbnad_rxq_cleanup_skb(struct bnad *bnad, struct bnad_rx_unmap *unmap)\n{\n\tif (!unmap->skb)\n\t\treturn;\n\n\tdma_unmap_single(&bnad->pcidev->dev,\n\t\t\tdma_unmap_addr(&unmap->vector, dma_addr),\n\t\t\tunmap->vector.len, DMA_FROM_DEVICE);\n\tdev_kfree_skb_any(unmap->skb);\n\tunmap->skb = NULL;\n\tdma_unmap_addr_set(&unmap->vector, dma_addr, 0);\n\tunmap->vector.len = 0;\n}\n\nstatic void\nbnad_rxq_cleanup(struct bnad *bnad, struct bna_rcb *rcb)\n{\n\tstruct bnad_rx_unmap_q *unmap_q = rcb->unmap_q;\n\tint i;\n\n\tfor (i = 0; i < rcb->q_depth; i++) {\n\t\tstruct bnad_rx_unmap *unmap = &unmap_q->unmap[i];\n\n\t\tif (BNAD_RXBUF_IS_SK_BUFF(unmap_q->type))\n\t\t\tbnad_rxq_cleanup_skb(bnad, unmap);\n\t\telse\n\t\t\tbnad_rxq_cleanup_page(bnad, unmap);\n\t}\n\tbnad_rxq_alloc_uninit(bnad, rcb);\n}\n\nstatic u32\nbnad_rxq_refill_page(struct bnad *bnad, struct bna_rcb *rcb, u32 nalloc)\n{\n\tu32 alloced, prod, q_depth;\n\tstruct bnad_rx_unmap_q *unmap_q = rcb->unmap_q;\n\tstruct bnad_rx_unmap *unmap, *prev;\n\tstruct bna_rxq_entry *rxent;\n\tstruct page *page;\n\tu32 page_offset, alloc_size;\n\tdma_addr_t dma_addr;\n\n\tprod = rcb->producer_index;\n\tq_depth = rcb->q_depth;\n\n\talloc_size = PAGE_SIZE << unmap_q->alloc_order;\n\talloced = 0;\n\n\twhile (nalloc--) {\n\t\tunmap = &unmap_q->unmap[prod];\n\n\t\tif (unmap_q->reuse_pi < 0) {\n\t\t\tpage = alloc_pages(GFP_ATOMIC | __GFP_COMP,\n\t\t\t\t\tunmap_q->alloc_order);\n\t\t\tpage_offset = 0;\n\t\t} else {\n\t\t\tprev = &unmap_q->unmap[unmap_q->reuse_pi];\n\t\t\tpage = prev->page;\n\t\t\tpage_offset = prev->page_offset + unmap_q->map_size;\n\t\t\tget_page(page);\n\t\t}\n\n\t\tif (unlikely(!page)) {\n\t\t\tBNAD_UPDATE_CTR(bnad, rxbuf_alloc_failed);\n\t\t\trcb->rxq->rxbuf_alloc_failed++;\n\t\t\tgoto finishing;\n\t\t}\n\n\t\tdma_addr = dma_map_page(&bnad->pcidev->dev, page, page_offset,\n\t\t\t\t\tunmap_q->map_size, DMA_FROM_DEVICE);\n\t\tif (dma_mapping_error(&bnad->pcidev->dev, dma_addr)) {\n\t\t\tput_page(page);\n\t\t\tBNAD_UPDATE_CTR(bnad, rxbuf_map_failed);\n\t\t\trcb->rxq->rxbuf_map_failed++;\n\t\t\tgoto finishing;\n\t\t}\n\n\t\tunmap->page = page;\n\t\tunmap->page_offset = page_offset;\n\t\tdma_unmap_addr_set(&unmap->vector, dma_addr, dma_addr);\n\t\tunmap->vector.len = unmap_q->map_size;\n\t\tpage_offset += unmap_q->map_size;\n\n\t\tif (page_offset < alloc_size)\n\t\t\tunmap_q->reuse_pi = prod;\n\t\telse\n\t\t\tunmap_q->reuse_pi = -1;\n\n\t\trxent = &((struct bna_rxq_entry *)rcb->sw_q)[prod];\n\t\tBNA_SET_DMA_ADDR(dma_addr, &rxent->host_addr);\n\t\tBNA_QE_INDX_INC(prod, q_depth);\n\t\talloced++;\n\t}\n\nfinishing:\n\tif (likely(alloced)) {\n\t\trcb->producer_index = prod;\n\t\tsmp_mb();\n\t\tif (likely(test_bit(BNAD_RXQ_POST_OK, &rcb->flags)))\n\t\t\tbna_rxq_prod_indx_doorbell(rcb);\n\t}\n\n\treturn alloced;\n}\n\nstatic u32\nbnad_rxq_refill_skb(struct bnad *bnad, struct bna_rcb *rcb, u32 nalloc)\n{\n\tu32 alloced, prod, q_depth, buff_sz;\n\tstruct bnad_rx_unmap_q *unmap_q = rcb->unmap_q;\n\tstruct bnad_rx_unmap *unmap;\n\tstruct bna_rxq_entry *rxent;\n\tstruct sk_buff *skb;\n\tdma_addr_t dma_addr;\n\n\tbuff_sz = rcb->rxq->buffer_size;\n\tprod = rcb->producer_index;\n\tq_depth = rcb->q_depth;\n\n\talloced = 0;\n\twhile (nalloc--) {\n\t\tunmap = &unmap_q->unmap[prod];\n\n\t\tskb = netdev_alloc_skb_ip_align(bnad->netdev, buff_sz);\n\n\t\tif (unlikely(!skb)) {\n\t\t\tBNAD_UPDATE_CTR(bnad, rxbuf_alloc_failed);\n\t\t\trcb->rxq->rxbuf_alloc_failed++;\n\t\t\tgoto finishing;\n\t\t}\n\n\t\tdma_addr = dma_map_single(&bnad->pcidev->dev, skb->data,\n\t\t\t\t\t  buff_sz, DMA_FROM_DEVICE);\n\t\tif (dma_mapping_error(&bnad->pcidev->dev, dma_addr)) {\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tBNAD_UPDATE_CTR(bnad, rxbuf_map_failed);\n\t\t\trcb->rxq->rxbuf_map_failed++;\n\t\t\tgoto finishing;\n\t\t}\n\n\t\tunmap->skb = skb;\n\t\tdma_unmap_addr_set(&unmap->vector, dma_addr, dma_addr);\n\t\tunmap->vector.len = buff_sz;\n\n\t\trxent = &((struct bna_rxq_entry *)rcb->sw_q)[prod];\n\t\tBNA_SET_DMA_ADDR(dma_addr, &rxent->host_addr);\n\t\tBNA_QE_INDX_INC(prod, q_depth);\n\t\talloced++;\n\t}\n\nfinishing:\n\tif (likely(alloced)) {\n\t\trcb->producer_index = prod;\n\t\tsmp_mb();\n\t\tif (likely(test_bit(BNAD_RXQ_POST_OK, &rcb->flags)))\n\t\t\tbna_rxq_prod_indx_doorbell(rcb);\n\t}\n\n\treturn alloced;\n}\n\nstatic inline void\nbnad_rxq_post(struct bnad *bnad, struct bna_rcb *rcb)\n{\n\tstruct bnad_rx_unmap_q *unmap_q = rcb->unmap_q;\n\tu32 to_alloc;\n\n\tto_alloc = BNA_QE_FREE_CNT(rcb, rcb->q_depth);\n\tif (!(to_alloc >> BNAD_RXQ_REFILL_THRESHOLD_SHIFT))\n\t\treturn;\n\n\tif (BNAD_RXBUF_IS_SK_BUFF(unmap_q->type))\n\t\tbnad_rxq_refill_skb(bnad, rcb, to_alloc);\n\telse\n\t\tbnad_rxq_refill_page(bnad, rcb, to_alloc);\n}\n\n#define flags_cksum_prot_mask (BNA_CQ_EF_IPV4 | BNA_CQ_EF_L3_CKSUM_OK | \\\n\t\t\t\t\tBNA_CQ_EF_IPV6 | \\\n\t\t\t\t\tBNA_CQ_EF_TCP | BNA_CQ_EF_UDP | \\\n\t\t\t\t\tBNA_CQ_EF_L4_CKSUM_OK)\n\n#define flags_tcp4 (BNA_CQ_EF_IPV4 | BNA_CQ_EF_L3_CKSUM_OK | \\\n\t\t\t\tBNA_CQ_EF_TCP | BNA_CQ_EF_L4_CKSUM_OK)\n#define flags_tcp6 (BNA_CQ_EF_IPV6 | \\\n\t\t\t\tBNA_CQ_EF_TCP | BNA_CQ_EF_L4_CKSUM_OK)\n#define flags_udp4 (BNA_CQ_EF_IPV4 | BNA_CQ_EF_L3_CKSUM_OK | \\\n\t\t\t\tBNA_CQ_EF_UDP | BNA_CQ_EF_L4_CKSUM_OK)\n#define flags_udp6 (BNA_CQ_EF_IPV6 | \\\n\t\t\t\tBNA_CQ_EF_UDP | BNA_CQ_EF_L4_CKSUM_OK)\n\nstatic void\nbnad_cq_drop_packet(struct bnad *bnad, struct bna_rcb *rcb,\n\t\t    u32 sop_ci, u32 nvecs)\n{\n\tstruct bnad_rx_unmap_q *unmap_q;\n\tstruct bnad_rx_unmap *unmap;\n\tu32 ci, vec;\n\n\tunmap_q = rcb->unmap_q;\n\tfor (vec = 0, ci = sop_ci; vec < nvecs; vec++) {\n\t\tunmap = &unmap_q->unmap[ci];\n\t\tBNA_QE_INDX_INC(ci, rcb->q_depth);\n\n\t\tif (BNAD_RXBUF_IS_SK_BUFF(unmap_q->type))\n\t\t\tbnad_rxq_cleanup_skb(bnad, unmap);\n\t\telse\n\t\t\tbnad_rxq_cleanup_page(bnad, unmap);\n\t}\n}\n\nstatic void\nbnad_cq_setup_skb_frags(struct bna_ccb *ccb, struct sk_buff *skb, u32 nvecs)\n{\n\tstruct bna_rcb *rcb;\n\tstruct bnad *bnad;\n\tstruct bnad_rx_unmap_q *unmap_q;\n\tstruct bna_cq_entry *cq, *cmpl;\n\tu32 ci, pi, totlen = 0;\n\n\tcq = ccb->sw_q;\n\tpi = ccb->producer_index;\n\tcmpl = &cq[pi];\n\n\trcb = bna_is_small_rxq(cmpl->rxq_id) ? ccb->rcb[1] : ccb->rcb[0];\n\tunmap_q = rcb->unmap_q;\n\tbnad = rcb->bnad;\n\tci = rcb->consumer_index;\n\n\t \n\tprefetch(page_address(unmap_q->unmap[ci].page) +\n\t\t unmap_q->unmap[ci].page_offset);\n\n\twhile (nvecs--) {\n\t\tstruct bnad_rx_unmap *unmap;\n\t\tu32 len;\n\n\t\tunmap = &unmap_q->unmap[ci];\n\t\tBNA_QE_INDX_INC(ci, rcb->q_depth);\n\n\t\tdma_unmap_page(&bnad->pcidev->dev,\n\t\t\t       dma_unmap_addr(&unmap->vector, dma_addr),\n\t\t\t       unmap->vector.len, DMA_FROM_DEVICE);\n\n\t\tlen = ntohs(cmpl->length);\n\t\tskb->truesize += unmap->vector.len;\n\t\ttotlen += len;\n\n\t\tskb_fill_page_desc(skb, skb_shinfo(skb)->nr_frags,\n\t\t\t\t   unmap->page, unmap->page_offset, len);\n\n\t\tunmap->page = NULL;\n\t\tunmap->vector.len = 0;\n\n\t\tBNA_QE_INDX_INC(pi, ccb->q_depth);\n\t\tcmpl = &cq[pi];\n\t}\n\n\tskb->len += totlen;\n\tskb->data_len += totlen;\n}\n\nstatic inline void\nbnad_cq_setup_skb(struct bnad *bnad, struct sk_buff *skb,\n\t\t  struct bnad_rx_unmap *unmap, u32 len)\n{\n\tprefetch(skb->data);\n\n\tdma_unmap_single(&bnad->pcidev->dev,\n\t\t\tdma_unmap_addr(&unmap->vector, dma_addr),\n\t\t\tunmap->vector.len, DMA_FROM_DEVICE);\n\n\tskb_put(skb, len);\n\tskb->protocol = eth_type_trans(skb, bnad->netdev);\n\n\tunmap->skb = NULL;\n\tunmap->vector.len = 0;\n}\n\nstatic u32\nbnad_cq_process(struct bnad *bnad, struct bna_ccb *ccb, int budget)\n{\n\tstruct bna_cq_entry *cq, *cmpl, *next_cmpl;\n\tstruct bna_rcb *rcb = NULL;\n\tstruct bnad_rx_unmap_q *unmap_q;\n\tstruct bnad_rx_unmap *unmap = NULL;\n\tstruct sk_buff *skb = NULL;\n\tstruct bna_pkt_rate *pkt_rt = &ccb->pkt_rate;\n\tstruct bnad_rx_ctrl *rx_ctrl = ccb->ctrl;\n\tu32 packets = 0, len = 0, totlen = 0;\n\tu32 pi, vec, sop_ci = 0, nvecs = 0;\n\tu32 flags, masked_flags;\n\n\tprefetch(bnad->netdev);\n\n\tcq = ccb->sw_q;\n\n\twhile (packets < budget) {\n\t\tcmpl = &cq[ccb->producer_index];\n\t\tif (!cmpl->valid)\n\t\t\tbreak;\n\t\t \n\t\trmb();\n\n\t\tBNA_UPDATE_PKT_CNT(pkt_rt, ntohs(cmpl->length));\n\n\t\tif (bna_is_small_rxq(cmpl->rxq_id))\n\t\t\trcb = ccb->rcb[1];\n\t\telse\n\t\t\trcb = ccb->rcb[0];\n\n\t\tunmap_q = rcb->unmap_q;\n\n\t\t \n\t\tsop_ci = rcb->consumer_index;\n\n\t\tif (BNAD_RXBUF_IS_SK_BUFF(unmap_q->type)) {\n\t\t\tunmap = &unmap_q->unmap[sop_ci];\n\t\t\tskb = unmap->skb;\n\t\t} else {\n\t\t\tskb = napi_get_frags(&rx_ctrl->napi);\n\t\t\tif (unlikely(!skb))\n\t\t\t\tbreak;\n\t\t}\n\t\tprefetch(skb);\n\n\t\tflags = ntohl(cmpl->flags);\n\t\tlen = ntohs(cmpl->length);\n\t\ttotlen = len;\n\t\tnvecs = 1;\n\n\t\t \n\t\tif (BNAD_RXBUF_IS_MULTI_BUFF(unmap_q->type) &&\n\t\t    (flags & BNA_CQ_EF_EOP) == 0) {\n\t\t\tpi = ccb->producer_index;\n\t\t\tdo {\n\t\t\t\tBNA_QE_INDX_INC(pi, ccb->q_depth);\n\t\t\t\tnext_cmpl = &cq[pi];\n\n\t\t\t\tif (!next_cmpl->valid)\n\t\t\t\t\tbreak;\n\t\t\t\t \n\t\t\t\trmb();\n\n\t\t\t\tlen = ntohs(next_cmpl->length);\n\t\t\t\tflags = ntohl(next_cmpl->flags);\n\n\t\t\t\tnvecs++;\n\t\t\t\ttotlen += len;\n\t\t\t} while ((flags & BNA_CQ_EF_EOP) == 0);\n\n\t\t\tif (!next_cmpl->valid)\n\t\t\t\tbreak;\n\t\t}\n\t\tpackets++;\n\n\t\t \n\t\tif (unlikely(flags & (BNA_CQ_EF_MAC_ERROR |\n\t\t\t\t\t\tBNA_CQ_EF_FCS_ERROR |\n\t\t\t\t\t\tBNA_CQ_EF_TOO_LONG))) {\n\t\t\tbnad_cq_drop_packet(bnad, rcb, sop_ci, nvecs);\n\t\t\trcb->rxq->rx_packets_with_error++;\n\n\t\t\tgoto next;\n\t\t}\n\n\t\tif (BNAD_RXBUF_IS_SK_BUFF(unmap_q->type))\n\t\t\tbnad_cq_setup_skb(bnad, skb, unmap, len);\n\t\telse\n\t\t\tbnad_cq_setup_skb_frags(ccb, skb, nvecs);\n\n\t\trcb->rxq->rx_packets++;\n\t\trcb->rxq->rx_bytes += totlen;\n\t\tccb->bytes_per_intr += totlen;\n\n\t\tmasked_flags = flags & flags_cksum_prot_mask;\n\n\t\tif (likely\n\t\t    ((bnad->netdev->features & NETIF_F_RXCSUM) &&\n\t\t     ((masked_flags == flags_tcp4) ||\n\t\t      (masked_flags == flags_udp4) ||\n\t\t      (masked_flags == flags_tcp6) ||\n\t\t      (masked_flags == flags_udp6))))\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\telse\n\t\t\tskb_checksum_none_assert(skb);\n\n\t\tif ((flags & BNA_CQ_EF_VLAN) &&\n\t\t    (bnad->netdev->features & NETIF_F_HW_VLAN_CTAG_RX))\n\t\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), ntohs(cmpl->vlan_tag));\n\n\t\tif (BNAD_RXBUF_IS_SK_BUFF(unmap_q->type))\n\t\t\tnetif_receive_skb(skb);\n\t\telse\n\t\t\tnapi_gro_frags(&rx_ctrl->napi);\n\nnext:\n\t\tBNA_QE_INDX_ADD(rcb->consumer_index, nvecs, rcb->q_depth);\n\t\tfor (vec = 0; vec < nvecs; vec++) {\n\t\t\tcmpl = &cq[ccb->producer_index];\n\t\t\tcmpl->valid = 0;\n\t\t\tBNA_QE_INDX_INC(ccb->producer_index, ccb->q_depth);\n\t\t}\n\t}\n\n\tnapi_gro_flush(&rx_ctrl->napi, false);\n\tif (likely(test_bit(BNAD_RXQ_STARTED, &ccb->rcb[0]->flags)))\n\t\tbna_ib_ack_disable_irq(ccb->i_dbell, packets);\n\n\tbnad_rxq_post(bnad, ccb->rcb[0]);\n\tif (ccb->rcb[1])\n\t\tbnad_rxq_post(bnad, ccb->rcb[1]);\n\n\treturn packets;\n}\n\nstatic void\nbnad_netif_rx_schedule_poll(struct bnad *bnad, struct bna_ccb *ccb)\n{\n\tstruct bnad_rx_ctrl *rx_ctrl = (struct bnad_rx_ctrl *)(ccb->ctrl);\n\tstruct napi_struct *napi = &rx_ctrl->napi;\n\n\tif (likely(napi_schedule_prep(napi))) {\n\t\t__napi_schedule(napi);\n\t\trx_ctrl->rx_schedule++;\n\t}\n}\n\n \nstatic irqreturn_t\nbnad_msix_rx(int irq, void *data)\n{\n\tstruct bna_ccb *ccb = (struct bna_ccb *)data;\n\n\tif (ccb) {\n\t\t((struct bnad_rx_ctrl *)ccb->ctrl)->rx_intr_ctr++;\n\t\tbnad_netif_rx_schedule_poll(ccb->bnad, ccb);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\n \n\n \nstatic irqreturn_t\nbnad_msix_mbox_handler(int irq, void *data)\n{\n\tu32 intr_status;\n\tunsigned long flags;\n\tstruct bnad *bnad = (struct bnad *)data;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tif (unlikely(test_bit(BNAD_RF_MBOX_IRQ_DISABLED, &bnad->run_flags))) {\n\t\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\t\treturn IRQ_HANDLED;\n\t}\n\n\tbna_intr_status_get(&bnad->bna, intr_status);\n\n\tif (BNA_IS_MBOX_ERR_INTR(&bnad->bna, intr_status))\n\t\tbna_mbox_handler(&bnad->bna, intr_status);\n\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t\nbnad_isr(int irq, void *data)\n{\n\tint i, j;\n\tu32 intr_status;\n\tunsigned long flags;\n\tstruct bnad *bnad = (struct bnad *)data;\n\tstruct bnad_rx_info *rx_info;\n\tstruct bnad_rx_ctrl *rx_ctrl;\n\tstruct bna_tcb *tcb = NULL;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tif (unlikely(test_bit(BNAD_RF_MBOX_IRQ_DISABLED, &bnad->run_flags))) {\n\t\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\t\treturn IRQ_NONE;\n\t}\n\n\tbna_intr_status_get(&bnad->bna, intr_status);\n\n\tif (unlikely(!intr_status)) {\n\t\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\t\treturn IRQ_NONE;\n\t}\n\n\tif (BNA_IS_MBOX_ERR_INTR(&bnad->bna, intr_status))\n\t\tbna_mbox_handler(&bnad->bna, intr_status);\n\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\tif (!BNA_IS_INTX_DATA_INTR(intr_status))\n\t\treturn IRQ_HANDLED;\n\n\t \n\t \n\tfor (i = 0; i < bnad->num_tx; i++) {\n\t\tfor (j = 0; j < bnad->num_txq_per_tx; j++) {\n\t\t\ttcb = bnad->tx_info[i].tcb[j];\n\t\t\tif (tcb && test_bit(BNAD_TXQ_TX_STARTED, &tcb->flags))\n\t\t\t\tbnad_tx_complete(bnad, bnad->tx_info[i].tcb[j]);\n\t\t}\n\t}\n\t \n\tfor (i = 0; i < bnad->num_rx; i++) {\n\t\trx_info = &bnad->rx_info[i];\n\t\tif (!rx_info->rx)\n\t\t\tcontinue;\n\t\tfor (j = 0; j < bnad->num_rxp_per_rx; j++) {\n\t\t\trx_ctrl = &rx_info->rx_ctrl[j];\n\t\t\tif (rx_ctrl->ccb)\n\t\t\t\tbnad_netif_rx_schedule_poll(bnad,\n\t\t\t\t\t\t\t    rx_ctrl->ccb);\n\t\t}\n\t}\n\treturn IRQ_HANDLED;\n}\n\n \nstatic void\nbnad_enable_mbox_irq(struct bnad *bnad)\n{\n\tclear_bit(BNAD_RF_MBOX_IRQ_DISABLED, &bnad->run_flags);\n\n\tBNAD_UPDATE_CTR(bnad, mbox_intr_enabled);\n}\n\n \nstatic void\nbnad_disable_mbox_irq(struct bnad *bnad)\n{\n\tset_bit(BNAD_RF_MBOX_IRQ_DISABLED, &bnad->run_flags);\n\n\tBNAD_UPDATE_CTR(bnad, mbox_intr_disabled);\n}\n\nstatic void\nbnad_set_netdev_perm_addr(struct bnad *bnad)\n{\n\tstruct net_device *netdev = bnad->netdev;\n\n\tether_addr_copy(netdev->perm_addr, bnad->perm_addr);\n\tif (is_zero_ether_addr(netdev->dev_addr))\n\t\teth_hw_addr_set(netdev, bnad->perm_addr);\n}\n\n \n\n \nvoid\nbnad_cb_mbox_intr_enable(struct bnad *bnad)\n{\n\tbnad_enable_mbox_irq(bnad);\n}\n\nvoid\nbnad_cb_mbox_intr_disable(struct bnad *bnad)\n{\n\tbnad_disable_mbox_irq(bnad);\n}\n\nvoid\nbnad_cb_ioceth_ready(struct bnad *bnad)\n{\n\tbnad->bnad_completions.ioc_comp_status = BNA_CB_SUCCESS;\n\tcomplete(&bnad->bnad_completions.ioc_comp);\n}\n\nvoid\nbnad_cb_ioceth_failed(struct bnad *bnad)\n{\n\tbnad->bnad_completions.ioc_comp_status = BNA_CB_FAIL;\n\tcomplete(&bnad->bnad_completions.ioc_comp);\n}\n\nvoid\nbnad_cb_ioceth_disabled(struct bnad *bnad)\n{\n\tbnad->bnad_completions.ioc_comp_status = BNA_CB_SUCCESS;\n\tcomplete(&bnad->bnad_completions.ioc_comp);\n}\n\nstatic void\nbnad_cb_enet_disabled(void *arg)\n{\n\tstruct bnad *bnad = (struct bnad *)arg;\n\n\tnetif_carrier_off(bnad->netdev);\n\tcomplete(&bnad->bnad_completions.enet_comp);\n}\n\nvoid\nbnad_cb_ethport_link_status(struct bnad *bnad,\n\t\t\tenum bna_link_status link_status)\n{\n\tbool link_up = false;\n\n\tlink_up = (link_status == BNA_LINK_UP) || (link_status == BNA_CEE_UP);\n\n\tif (link_status == BNA_CEE_UP) {\n\t\tif (!test_bit(BNAD_RF_CEE_RUNNING, &bnad->run_flags))\n\t\t\tBNAD_UPDATE_CTR(bnad, cee_toggle);\n\t\tset_bit(BNAD_RF_CEE_RUNNING, &bnad->run_flags);\n\t} else {\n\t\tif (test_bit(BNAD_RF_CEE_RUNNING, &bnad->run_flags))\n\t\t\tBNAD_UPDATE_CTR(bnad, cee_toggle);\n\t\tclear_bit(BNAD_RF_CEE_RUNNING, &bnad->run_flags);\n\t}\n\n\tif (link_up) {\n\t\tif (!netif_carrier_ok(bnad->netdev)) {\n\t\t\tuint tx_id, tcb_id;\n\t\t\tnetdev_info(bnad->netdev, \"link up\\n\");\n\t\t\tnetif_carrier_on(bnad->netdev);\n\t\t\tBNAD_UPDATE_CTR(bnad, link_toggle);\n\t\t\tfor (tx_id = 0; tx_id < bnad->num_tx; tx_id++) {\n\t\t\t\tfor (tcb_id = 0; tcb_id < bnad->num_txq_per_tx;\n\t\t\t\t      tcb_id++) {\n\t\t\t\t\tstruct bna_tcb *tcb =\n\t\t\t\t\tbnad->tx_info[tx_id].tcb[tcb_id];\n\t\t\t\t\tu32 txq_id;\n\t\t\t\t\tif (!tcb)\n\t\t\t\t\t\tcontinue;\n\n\t\t\t\t\ttxq_id = tcb->id;\n\n\t\t\t\t\tif (test_bit(BNAD_TXQ_TX_STARTED,\n\t\t\t\t\t\t     &tcb->flags)) {\n\t\t\t\t\t\t \n\t\t\t\t\t\tnetif_wake_subqueue(\n\t\t\t\t\t\t\t\tbnad->netdev,\n\t\t\t\t\t\t\t\ttxq_id);\n\t\t\t\t\t\tBNAD_UPDATE_CTR(bnad,\n\t\t\t\t\t\t\tnetif_queue_wakeup);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tnetif_stop_subqueue(\n\t\t\t\t\t\t\t\tbnad->netdev,\n\t\t\t\t\t\t\t\ttxq_id);\n\t\t\t\t\t\tBNAD_UPDATE_CTR(bnad,\n\t\t\t\t\t\t\tnetif_queue_stop);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif (netif_carrier_ok(bnad->netdev)) {\n\t\t\tnetdev_info(bnad->netdev, \"link down\\n\");\n\t\t\tnetif_carrier_off(bnad->netdev);\n\t\t\tBNAD_UPDATE_CTR(bnad, link_toggle);\n\t\t}\n\t}\n}\n\nstatic void\nbnad_cb_tx_disabled(void *arg, struct bna_tx *tx)\n{\n\tstruct bnad *bnad = (struct bnad *)arg;\n\n\tcomplete(&bnad->bnad_completions.tx_comp);\n}\n\nstatic void\nbnad_cb_tcb_setup(struct bnad *bnad, struct bna_tcb *tcb)\n{\n\tstruct bnad_tx_info *tx_info =\n\t\t\t(struct bnad_tx_info *)tcb->txq->tx->priv;\n\n\ttcb->priv = tcb;\n\ttx_info->tcb[tcb->id] = tcb;\n}\n\nstatic void\nbnad_cb_tcb_destroy(struct bnad *bnad, struct bna_tcb *tcb)\n{\n\tstruct bnad_tx_info *tx_info =\n\t\t\t(struct bnad_tx_info *)tcb->txq->tx->priv;\n\n\ttx_info->tcb[tcb->id] = NULL;\n\ttcb->priv = NULL;\n}\n\nstatic void\nbnad_cb_ccb_setup(struct bnad *bnad, struct bna_ccb *ccb)\n{\n\tstruct bnad_rx_info *rx_info =\n\t\t\t(struct bnad_rx_info *)ccb->cq->rx->priv;\n\n\trx_info->rx_ctrl[ccb->id].ccb = ccb;\n\tccb->ctrl = &rx_info->rx_ctrl[ccb->id];\n}\n\nstatic void\nbnad_cb_ccb_destroy(struct bnad *bnad, struct bna_ccb *ccb)\n{\n\tstruct bnad_rx_info *rx_info =\n\t\t\t(struct bnad_rx_info *)ccb->cq->rx->priv;\n\n\trx_info->rx_ctrl[ccb->id].ccb = NULL;\n}\n\nstatic void\nbnad_cb_tx_stall(struct bnad *bnad, struct bna_tx *tx)\n{\n\tstruct bnad_tx_info *tx_info = tx->priv;\n\tstruct bna_tcb *tcb;\n\tu32 txq_id;\n\tint i;\n\n\tfor (i = 0; i < BNAD_MAX_TXQ_PER_TX; i++) {\n\t\ttcb = tx_info->tcb[i];\n\t\tif (!tcb)\n\t\t\tcontinue;\n\t\ttxq_id = tcb->id;\n\t\tclear_bit(BNAD_TXQ_TX_STARTED, &tcb->flags);\n\t\tnetif_stop_subqueue(bnad->netdev, txq_id);\n\t}\n}\n\nstatic void\nbnad_cb_tx_resume(struct bnad *bnad, struct bna_tx *tx)\n{\n\tstruct bnad_tx_info *tx_info = tx->priv;\n\tstruct bna_tcb *tcb;\n\tu32 txq_id;\n\tint i;\n\n\tfor (i = 0; i < BNAD_MAX_TXQ_PER_TX; i++) {\n\t\ttcb = tx_info->tcb[i];\n\t\tif (!tcb)\n\t\t\tcontinue;\n\t\ttxq_id = tcb->id;\n\n\t\tBUG_ON(test_bit(BNAD_TXQ_TX_STARTED, &tcb->flags));\n\t\tset_bit(BNAD_TXQ_TX_STARTED, &tcb->flags);\n\t\tBUG_ON(*(tcb->hw_consumer_index) != 0);\n\n\t\tif (netif_carrier_ok(bnad->netdev)) {\n\t\t\tnetif_wake_subqueue(bnad->netdev, txq_id);\n\t\t\tBNAD_UPDATE_CTR(bnad, netif_queue_wakeup);\n\t\t}\n\t}\n\n\t \n\tif (is_zero_ether_addr(bnad->perm_addr)) {\n\t\tbna_enet_perm_mac_get(&bnad->bna.enet, bnad->perm_addr);\n\t\tbnad_set_netdev_perm_addr(bnad);\n\t}\n}\n\n \nstatic void\nbnad_tx_cleanup(struct delayed_work *work)\n{\n\tstruct bnad_tx_info *tx_info =\n\t\tcontainer_of(work, struct bnad_tx_info, tx_cleanup_work);\n\tstruct bnad *bnad = NULL;\n\tstruct bna_tcb *tcb;\n\tunsigned long flags;\n\tu32 i, pending = 0;\n\n\tfor (i = 0; i < BNAD_MAX_TXQ_PER_TX; i++) {\n\t\ttcb = tx_info->tcb[i];\n\t\tif (!tcb)\n\t\t\tcontinue;\n\n\t\tbnad = tcb->bnad;\n\n\t\tif (test_and_set_bit(BNAD_TXQ_FREE_SENT, &tcb->flags)) {\n\t\t\tpending++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tbnad_txq_cleanup(bnad, tcb);\n\n\t\tsmp_mb__before_atomic();\n\t\tclear_bit(BNAD_TXQ_FREE_SENT, &tcb->flags);\n\t}\n\n\tif (pending) {\n\t\tqueue_delayed_work(bnad->work_q, &tx_info->tx_cleanup_work,\n\t\t\tmsecs_to_jiffies(1));\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_tx_cleanup_complete(tx_info->tx);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n}\n\nstatic void\nbnad_cb_tx_cleanup(struct bnad *bnad, struct bna_tx *tx)\n{\n\tstruct bnad_tx_info *tx_info = tx->priv;\n\tstruct bna_tcb *tcb;\n\tint i;\n\n\tfor (i = 0; i < BNAD_MAX_TXQ_PER_TX; i++) {\n\t\ttcb = tx_info->tcb[i];\n\t\tif (!tcb)\n\t\t\tcontinue;\n\t}\n\n\tqueue_delayed_work(bnad->work_q, &tx_info->tx_cleanup_work, 0);\n}\n\nstatic void\nbnad_cb_rx_stall(struct bnad *bnad, struct bna_rx *rx)\n{\n\tstruct bnad_rx_info *rx_info = rx->priv;\n\tstruct bna_ccb *ccb;\n\tstruct bnad_rx_ctrl *rx_ctrl;\n\tint i;\n\n\tfor (i = 0; i < BNAD_MAX_RXP_PER_RX; i++) {\n\t\trx_ctrl = &rx_info->rx_ctrl[i];\n\t\tccb = rx_ctrl->ccb;\n\t\tif (!ccb)\n\t\t\tcontinue;\n\n\t\tclear_bit(BNAD_RXQ_POST_OK, &ccb->rcb[0]->flags);\n\n\t\tif (ccb->rcb[1])\n\t\t\tclear_bit(BNAD_RXQ_POST_OK, &ccb->rcb[1]->flags);\n\t}\n}\n\n \nstatic void\nbnad_rx_cleanup(void *work)\n{\n\tstruct bnad_rx_info *rx_info =\n\t\tcontainer_of(work, struct bnad_rx_info, rx_cleanup_work);\n\tstruct bnad_rx_ctrl *rx_ctrl;\n\tstruct bnad *bnad = NULL;\n\tunsigned long flags;\n\tu32 i;\n\n\tfor (i = 0; i < BNAD_MAX_RXP_PER_RX; i++) {\n\t\trx_ctrl = &rx_info->rx_ctrl[i];\n\n\t\tif (!rx_ctrl->ccb)\n\t\t\tcontinue;\n\n\t\tbnad = rx_ctrl->ccb->bnad;\n\n\t\t \n\t\tnapi_disable(&rx_ctrl->napi);\n\n\t\tbnad_cq_cleanup(bnad, rx_ctrl->ccb);\n\t\tbnad_rxq_cleanup(bnad, rx_ctrl->ccb->rcb[0]);\n\t\tif (rx_ctrl->ccb->rcb[1])\n\t\t\tbnad_rxq_cleanup(bnad, rx_ctrl->ccb->rcb[1]);\n\t}\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_rx_cleanup_complete(rx_info->rx);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n}\n\nstatic void\nbnad_cb_rx_cleanup(struct bnad *bnad, struct bna_rx *rx)\n{\n\tstruct bnad_rx_info *rx_info = rx->priv;\n\tstruct bna_ccb *ccb;\n\tstruct bnad_rx_ctrl *rx_ctrl;\n\tint i;\n\n\tfor (i = 0; i < BNAD_MAX_RXP_PER_RX; i++) {\n\t\trx_ctrl = &rx_info->rx_ctrl[i];\n\t\tccb = rx_ctrl->ccb;\n\t\tif (!ccb)\n\t\t\tcontinue;\n\n\t\tclear_bit(BNAD_RXQ_STARTED, &ccb->rcb[0]->flags);\n\n\t\tif (ccb->rcb[1])\n\t\t\tclear_bit(BNAD_RXQ_STARTED, &ccb->rcb[1]->flags);\n\t}\n\n\tqueue_work(bnad->work_q, &rx_info->rx_cleanup_work);\n}\n\nstatic void\nbnad_cb_rx_post(struct bnad *bnad, struct bna_rx *rx)\n{\n\tstruct bnad_rx_info *rx_info = rx->priv;\n\tstruct bna_ccb *ccb;\n\tstruct bna_rcb *rcb;\n\tstruct bnad_rx_ctrl *rx_ctrl;\n\tint i, j;\n\n\tfor (i = 0; i < BNAD_MAX_RXP_PER_RX; i++) {\n\t\trx_ctrl = &rx_info->rx_ctrl[i];\n\t\tccb = rx_ctrl->ccb;\n\t\tif (!ccb)\n\t\t\tcontinue;\n\n\t\tnapi_enable(&rx_ctrl->napi);\n\n\t\tfor (j = 0; j < BNAD_MAX_RXQ_PER_RXP; j++) {\n\t\t\trcb = ccb->rcb[j];\n\t\t\tif (!rcb)\n\t\t\t\tcontinue;\n\n\t\t\tbnad_rxq_alloc_init(bnad, rcb);\n\t\t\tset_bit(BNAD_RXQ_STARTED, &rcb->flags);\n\t\t\tset_bit(BNAD_RXQ_POST_OK, &rcb->flags);\n\t\t\tbnad_rxq_post(bnad, rcb);\n\t\t}\n\t}\n}\n\nstatic void\nbnad_cb_rx_disabled(void *arg, struct bna_rx *rx)\n{\n\tstruct bnad *bnad = (struct bnad *)arg;\n\n\tcomplete(&bnad->bnad_completions.rx_comp);\n}\n\nstatic void\nbnad_cb_rx_mcast_add(struct bnad *bnad, struct bna_rx *rx)\n{\n\tbnad->bnad_completions.mcast_comp_status = BNA_CB_SUCCESS;\n\tcomplete(&bnad->bnad_completions.mcast_comp);\n}\n\nvoid\nbnad_cb_stats_get(struct bnad *bnad, enum bna_cb_status status,\n\t\t       struct bna_stats *stats)\n{\n\tif (status == BNA_CB_SUCCESS)\n\t\tBNAD_UPDATE_CTR(bnad, hw_stats_updates);\n\n\tif (!netif_running(bnad->netdev) ||\n\t\t!test_bit(BNAD_RF_STATS_TIMER_RUNNING, &bnad->run_flags))\n\t\treturn;\n\n\tmod_timer(&bnad->stats_timer,\n\t\t  jiffies + msecs_to_jiffies(BNAD_STATS_TIMER_FREQ));\n}\n\nstatic void\nbnad_cb_enet_mtu_set(struct bnad *bnad)\n{\n\tbnad->bnad_completions.mtu_comp_status = BNA_CB_SUCCESS;\n\tcomplete(&bnad->bnad_completions.mtu_comp);\n}\n\nvoid\nbnad_cb_completion(void *arg, enum bfa_status status)\n{\n\tstruct bnad_iocmd_comp *iocmd_comp =\n\t\t\t(struct bnad_iocmd_comp *)arg;\n\n\tiocmd_comp->comp_status = (u32) status;\n\tcomplete(&iocmd_comp->comp);\n}\n\n \n\nstatic void\nbnad_mem_free(struct bnad *bnad,\n\t      struct bna_mem_info *mem_info)\n{\n\tint i;\n\tdma_addr_t dma_pa;\n\n\tif (mem_info->mdl == NULL)\n\t\treturn;\n\n\tfor (i = 0; i < mem_info->num; i++) {\n\t\tif (mem_info->mdl[i].kva != NULL) {\n\t\t\tif (mem_info->mem_type == BNA_MEM_T_DMA) {\n\t\t\t\tBNA_GET_DMA_ADDR(&(mem_info->mdl[i].dma),\n\t\t\t\t\t\tdma_pa);\n\t\t\t\tdma_free_coherent(&bnad->pcidev->dev,\n\t\t\t\t\t\t  mem_info->mdl[i].len,\n\t\t\t\t\t\t  mem_info->mdl[i].kva, dma_pa);\n\t\t\t} else\n\t\t\t\tkfree(mem_info->mdl[i].kva);\n\t\t}\n\t}\n\tkfree(mem_info->mdl);\n\tmem_info->mdl = NULL;\n}\n\nstatic int\nbnad_mem_alloc(struct bnad *bnad,\n\t       struct bna_mem_info *mem_info)\n{\n\tint i;\n\tdma_addr_t dma_pa;\n\n\tif ((mem_info->num == 0) || (mem_info->len == 0)) {\n\t\tmem_info->mdl = NULL;\n\t\treturn 0;\n\t}\n\n\tmem_info->mdl = kcalloc(mem_info->num, sizeof(struct bna_mem_descr),\n\t\t\t\tGFP_KERNEL);\n\tif (mem_info->mdl == NULL)\n\t\treturn -ENOMEM;\n\n\tif (mem_info->mem_type == BNA_MEM_T_DMA) {\n\t\tfor (i = 0; i < mem_info->num; i++) {\n\t\t\tmem_info->mdl[i].len = mem_info->len;\n\t\t\tmem_info->mdl[i].kva =\n\t\t\t\tdma_alloc_coherent(&bnad->pcidev->dev,\n\t\t\t\t\t\t   mem_info->len, &dma_pa,\n\t\t\t\t\t\t   GFP_KERNEL);\n\t\t\tif (mem_info->mdl[i].kva == NULL)\n\t\t\t\tgoto err_return;\n\n\t\t\tBNA_SET_DMA_ADDR(dma_pa,\n\t\t\t\t\t &(mem_info->mdl[i].dma));\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < mem_info->num; i++) {\n\t\t\tmem_info->mdl[i].len = mem_info->len;\n\t\t\tmem_info->mdl[i].kva = kzalloc(mem_info->len,\n\t\t\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (mem_info->mdl[i].kva == NULL)\n\t\t\t\tgoto err_return;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_return:\n\tbnad_mem_free(bnad, mem_info);\n\treturn -ENOMEM;\n}\n\n \nstatic void\nbnad_mbox_irq_free(struct bnad *bnad)\n{\n\tint irq;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbnad_disable_mbox_irq(bnad);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\tirq = BNAD_GET_MBOX_IRQ(bnad);\n\tfree_irq(irq, bnad);\n}\n\n \nstatic int\nbnad_mbox_irq_alloc(struct bnad *bnad)\n{\n\tint\t\terr = 0;\n\tunsigned long\tirq_flags, flags;\n\tu32\tirq;\n\tirq_handler_t\tirq_handler;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tif (bnad->cfg_flags & BNAD_CF_MSIX) {\n\t\tirq_handler = (irq_handler_t)bnad_msix_mbox_handler;\n\t\tirq = bnad->msix_table[BNAD_MAILBOX_MSIX_INDEX].vector;\n\t\tirq_flags = 0;\n\t} else {\n\t\tirq_handler = (irq_handler_t)bnad_isr;\n\t\tirq = bnad->pcidev->irq;\n\t\tirq_flags = IRQF_SHARED;\n\t}\n\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\tsprintf(bnad->mbox_irq_name, \"%s\", BNAD_NAME);\n\n\t \n\tset_bit(BNAD_RF_MBOX_IRQ_DISABLED, &bnad->run_flags);\n\n\tBNAD_UPDATE_CTR(bnad, mbox_intr_disabled);\n\n\terr = request_irq(irq, irq_handler, irq_flags,\n\t\t\t  bnad->mbox_irq_name, bnad);\n\n\treturn err;\n}\n\nstatic void\nbnad_txrx_irq_free(struct bnad *bnad, struct bna_intr_info *intr_info)\n{\n\tkfree(intr_info->idl);\n\tintr_info->idl = NULL;\n}\n\n \nstatic int\nbnad_txrx_irq_alloc(struct bnad *bnad, enum bnad_intr_source src,\n\t\t    u32 txrx_id, struct bna_intr_info *intr_info)\n{\n\tint i, vector_start = 0;\n\tu32 cfg_flags;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tcfg_flags = bnad->cfg_flags;\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\tif (cfg_flags & BNAD_CF_MSIX) {\n\t\tintr_info->intr_type = BNA_INTR_T_MSIX;\n\t\tintr_info->idl = kcalloc(intr_info->num,\n\t\t\t\t\tsizeof(struct bna_intr_descr),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!intr_info->idl)\n\t\t\treturn -ENOMEM;\n\n\t\tswitch (src) {\n\t\tcase BNAD_INTR_TX:\n\t\t\tvector_start = BNAD_MAILBOX_MSIX_VECTORS + txrx_id;\n\t\t\tbreak;\n\n\t\tcase BNAD_INTR_RX:\n\t\t\tvector_start = BNAD_MAILBOX_MSIX_VECTORS +\n\t\t\t\t\t(bnad->num_tx * bnad->num_txq_per_tx) +\n\t\t\t\t\ttxrx_id;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\n\t\tfor (i = 0; i < intr_info->num; i++)\n\t\t\tintr_info->idl[i].vector = vector_start + i;\n\t} else {\n\t\tintr_info->intr_type = BNA_INTR_T_INTX;\n\t\tintr_info->num = 1;\n\t\tintr_info->idl = kcalloc(intr_info->num,\n\t\t\t\t\tsizeof(struct bna_intr_descr),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!intr_info->idl)\n\t\t\treturn -ENOMEM;\n\n\t\tswitch (src) {\n\t\tcase BNAD_INTR_TX:\n\t\t\tintr_info->idl[0].vector = BNAD_INTX_TX_IB_BITMASK;\n\t\t\tbreak;\n\n\t\tcase BNAD_INTR_RX:\n\t\t\tintr_info->idl[0].vector = BNAD_INTX_RX_IB_BITMASK;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nstatic void\nbnad_tx_msix_unregister(struct bnad *bnad, struct bnad_tx_info *tx_info,\n\t\t\tint num_txqs)\n{\n\tint i;\n\tint vector_num;\n\n\tfor (i = 0; i < num_txqs; i++) {\n\t\tif (tx_info->tcb[i] == NULL)\n\t\t\tcontinue;\n\n\t\tvector_num = tx_info->tcb[i]->intr_vector;\n\t\tfree_irq(bnad->msix_table[vector_num].vector, tx_info->tcb[i]);\n\t}\n}\n\n \nstatic int\nbnad_tx_msix_register(struct bnad *bnad, struct bnad_tx_info *tx_info,\n\t\t\tu32 tx_id, int num_txqs)\n{\n\tint i;\n\tint err;\n\tint vector_num;\n\n\tfor (i = 0; i < num_txqs; i++) {\n\t\tvector_num = tx_info->tcb[i]->intr_vector;\n\t\tsprintf(tx_info->tcb[i]->name, \"%s TXQ %d\", bnad->netdev->name,\n\t\t\t\ttx_id + tx_info->tcb[i]->id);\n\t\terr = request_irq(bnad->msix_table[vector_num].vector,\n\t\t\t\t  (irq_handler_t)bnad_msix_tx, 0,\n\t\t\t\t  tx_info->tcb[i]->name,\n\t\t\t\t  tx_info->tcb[i]);\n\t\tif (err)\n\t\t\tgoto err_return;\n\t}\n\n\treturn 0;\n\nerr_return:\n\tif (i > 0)\n\t\tbnad_tx_msix_unregister(bnad, tx_info, (i - 1));\n\treturn -1;\n}\n\n \nstatic void\nbnad_rx_msix_unregister(struct bnad *bnad, struct bnad_rx_info *rx_info,\n\t\t\tint num_rxps)\n{\n\tint i;\n\tint vector_num;\n\n\tfor (i = 0; i < num_rxps; i++) {\n\t\tif (rx_info->rx_ctrl[i].ccb == NULL)\n\t\t\tcontinue;\n\n\t\tvector_num = rx_info->rx_ctrl[i].ccb->intr_vector;\n\t\tfree_irq(bnad->msix_table[vector_num].vector,\n\t\t\t rx_info->rx_ctrl[i].ccb);\n\t}\n}\n\n \nstatic int\nbnad_rx_msix_register(struct bnad *bnad, struct bnad_rx_info *rx_info,\n\t\t\tu32 rx_id, int num_rxps)\n{\n\tint i;\n\tint err;\n\tint vector_num;\n\n\tfor (i = 0; i < num_rxps; i++) {\n\t\tvector_num = rx_info->rx_ctrl[i].ccb->intr_vector;\n\t\tsprintf(rx_info->rx_ctrl[i].ccb->name, \"%s CQ %d\",\n\t\t\tbnad->netdev->name,\n\t\t\trx_id + rx_info->rx_ctrl[i].ccb->id);\n\t\terr = request_irq(bnad->msix_table[vector_num].vector,\n\t\t\t\t  (irq_handler_t)bnad_msix_rx, 0,\n\t\t\t\t  rx_info->rx_ctrl[i].ccb->name,\n\t\t\t\t  rx_info->rx_ctrl[i].ccb);\n\t\tif (err)\n\t\t\tgoto err_return;\n\t}\n\n\treturn 0;\n\nerr_return:\n\tif (i > 0)\n\t\tbnad_rx_msix_unregister(bnad, rx_info, (i - 1));\n\treturn -1;\n}\n\n \nstatic void\nbnad_tx_res_free(struct bnad *bnad, struct bna_res_info *res_info)\n{\n\tint i;\n\n\tfor (i = 0; i < BNA_TX_RES_T_MAX; i++) {\n\t\tif (res_info[i].res_type == BNA_RES_T_MEM)\n\t\t\tbnad_mem_free(bnad, &res_info[i].res_u.mem_info);\n\t\telse if (res_info[i].res_type == BNA_RES_T_INTR)\n\t\t\tbnad_txrx_irq_free(bnad, &res_info[i].res_u.intr_info);\n\t}\n}\n\n \nstatic int\nbnad_tx_res_alloc(struct bnad *bnad, struct bna_res_info *res_info,\n\t\t  u32 tx_id)\n{\n\tint i, err = 0;\n\n\tfor (i = 0; i < BNA_TX_RES_T_MAX; i++) {\n\t\tif (res_info[i].res_type == BNA_RES_T_MEM)\n\t\t\terr = bnad_mem_alloc(bnad,\n\t\t\t\t\t&res_info[i].res_u.mem_info);\n\t\telse if (res_info[i].res_type == BNA_RES_T_INTR)\n\t\t\terr = bnad_txrx_irq_alloc(bnad, BNAD_INTR_TX, tx_id,\n\t\t\t\t\t&res_info[i].res_u.intr_info);\n\t\tif (err)\n\t\t\tgoto err_return;\n\t}\n\treturn 0;\n\nerr_return:\n\tbnad_tx_res_free(bnad, res_info);\n\treturn err;\n}\n\n \nstatic void\nbnad_rx_res_free(struct bnad *bnad, struct bna_res_info *res_info)\n{\n\tint i;\n\n\tfor (i = 0; i < BNA_RX_RES_T_MAX; i++) {\n\t\tif (res_info[i].res_type == BNA_RES_T_MEM)\n\t\t\tbnad_mem_free(bnad, &res_info[i].res_u.mem_info);\n\t\telse if (res_info[i].res_type == BNA_RES_T_INTR)\n\t\t\tbnad_txrx_irq_free(bnad, &res_info[i].res_u.intr_info);\n\t}\n}\n\n \nstatic int\nbnad_rx_res_alloc(struct bnad *bnad, struct bna_res_info *res_info,\n\t\t  uint rx_id)\n{\n\tint i, err = 0;\n\n\t \n\tfor (i = 0; i < BNA_RX_RES_T_MAX; i++) {\n\t\tif (res_info[i].res_type == BNA_RES_T_MEM)\n\t\t\terr = bnad_mem_alloc(bnad,\n\t\t\t\t\t&res_info[i].res_u.mem_info);\n\t\telse if (res_info[i].res_type == BNA_RES_T_INTR)\n\t\t\terr = bnad_txrx_irq_alloc(bnad, BNAD_INTR_RX, rx_id,\n\t\t\t\t\t&res_info[i].res_u.intr_info);\n\t\tif (err)\n\t\t\tgoto err_return;\n\t}\n\treturn 0;\n\nerr_return:\n\tbnad_rx_res_free(bnad, res_info);\n\treturn err;\n}\n\n \n \nstatic void\nbnad_ioc_timeout(struct timer_list *t)\n{\n\tstruct bnad *bnad = from_timer(bnad, t, bna.ioceth.ioc.ioc_timer);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbfa_nw_ioc_timeout(&bnad->bna.ioceth.ioc);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n}\n\nstatic void\nbnad_ioc_hb_check(struct timer_list *t)\n{\n\tstruct bnad *bnad = from_timer(bnad, t, bna.ioceth.ioc.hb_timer);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbfa_nw_ioc_hb_check(&bnad->bna.ioceth.ioc);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n}\n\nstatic void\nbnad_iocpf_timeout(struct timer_list *t)\n{\n\tstruct bnad *bnad = from_timer(bnad, t, bna.ioceth.ioc.iocpf_timer);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbfa_nw_iocpf_timeout(&bnad->bna.ioceth.ioc);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n}\n\nstatic void\nbnad_iocpf_sem_timeout(struct timer_list *t)\n{\n\tstruct bnad *bnad = from_timer(bnad, t, bna.ioceth.ioc.sem_timer);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbfa_nw_iocpf_sem_timeout(&bnad->bna.ioceth.ioc);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n}\n\n \n\n \nstatic void\nbnad_dim_timeout(struct timer_list *t)\n{\n\tstruct bnad *bnad = from_timer(bnad, t, dim_timer);\n\tstruct bnad_rx_info *rx_info;\n\tstruct bnad_rx_ctrl *rx_ctrl;\n\tint i, j;\n\tunsigned long flags;\n\n\tif (!netif_carrier_ok(bnad->netdev))\n\t\treturn;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tfor (i = 0; i < bnad->num_rx; i++) {\n\t\trx_info = &bnad->rx_info[i];\n\t\tif (!rx_info->rx)\n\t\t\tcontinue;\n\t\tfor (j = 0; j < bnad->num_rxp_per_rx; j++) {\n\t\t\trx_ctrl = &rx_info->rx_ctrl[j];\n\t\t\tif (!rx_ctrl->ccb)\n\t\t\t\tcontinue;\n\t\t\tbna_rx_dim_update(rx_ctrl->ccb);\n\t\t}\n\t}\n\n\t \n\tif (test_bit(BNAD_RF_DIM_TIMER_RUNNING, &bnad->run_flags))\n\t\tmod_timer(&bnad->dim_timer,\n\t\t\t  jiffies + msecs_to_jiffies(BNAD_DIM_TIMER_FREQ));\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n}\n\n \nstatic void\nbnad_stats_timeout(struct timer_list *t)\n{\n\tstruct bnad *bnad = from_timer(bnad, t, stats_timer);\n\tunsigned long flags;\n\n\tif (!netif_running(bnad->netdev) ||\n\t\t!test_bit(BNAD_RF_STATS_TIMER_RUNNING, &bnad->run_flags))\n\t\treturn;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_hw_stats_get(&bnad->bna);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n}\n\n \nvoid\nbnad_dim_timer_start(struct bnad *bnad)\n{\n\tif (bnad->cfg_flags & BNAD_CF_DIM_ENABLED &&\n\t    !test_bit(BNAD_RF_DIM_TIMER_RUNNING, &bnad->run_flags)) {\n\t\ttimer_setup(&bnad->dim_timer, bnad_dim_timeout, 0);\n\t\tset_bit(BNAD_RF_DIM_TIMER_RUNNING, &bnad->run_flags);\n\t\tmod_timer(&bnad->dim_timer,\n\t\t\t  jiffies + msecs_to_jiffies(BNAD_DIM_TIMER_FREQ));\n\t}\n}\n\n \nstatic void\nbnad_stats_timer_start(struct bnad *bnad)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tif (!test_and_set_bit(BNAD_RF_STATS_TIMER_RUNNING, &bnad->run_flags)) {\n\t\ttimer_setup(&bnad->stats_timer, bnad_stats_timeout, 0);\n\t\tmod_timer(&bnad->stats_timer,\n\t\t\t  jiffies + msecs_to_jiffies(BNAD_STATS_TIMER_FREQ));\n\t}\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n}\n\n \nstatic void\nbnad_stats_timer_stop(struct bnad *bnad)\n{\n\tint to_del = 0;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tif (test_and_clear_bit(BNAD_RF_STATS_TIMER_RUNNING, &bnad->run_flags))\n\t\tto_del = 1;\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\tif (to_del)\n\t\tdel_timer_sync(&bnad->stats_timer);\n}\n\n \n\nstatic void\nbnad_netdev_mc_list_get(struct net_device *netdev, u8 *mc_list)\n{\n\tint i = 1;  \n\tstruct netdev_hw_addr *mc_addr;\n\n\tnetdev_for_each_mc_addr(mc_addr, netdev) {\n\t\tether_addr_copy(&mc_list[i * ETH_ALEN], &mc_addr->addr[0]);\n\t\ti++;\n\t}\n}\n\nstatic int\nbnad_napi_poll_rx(struct napi_struct *napi, int budget)\n{\n\tstruct bnad_rx_ctrl *rx_ctrl =\n\t\tcontainer_of(napi, struct bnad_rx_ctrl, napi);\n\tstruct bnad *bnad = rx_ctrl->bnad;\n\tint rcvd = 0;\n\n\trx_ctrl->rx_poll_ctr++;\n\n\tif (!netif_carrier_ok(bnad->netdev))\n\t\tgoto poll_exit;\n\n\trcvd = bnad_cq_process(bnad, rx_ctrl->ccb, budget);\n\tif (rcvd >= budget)\n\t\treturn rcvd;\n\npoll_exit:\n\tnapi_complete_done(napi, rcvd);\n\n\trx_ctrl->rx_complete++;\n\n\tif (rx_ctrl->ccb)\n\t\tbnad_enable_rx_irq_unsafe(rx_ctrl->ccb);\n\n\treturn rcvd;\n}\n\nstatic void\nbnad_napi_add(struct bnad *bnad, u32 rx_id)\n{\n\tstruct bnad_rx_ctrl *rx_ctrl;\n\tint i;\n\n\t \n\tfor (i = 0; i <\tbnad->num_rxp_per_rx; i++) {\n\t\trx_ctrl = &bnad->rx_info[rx_id].rx_ctrl[i];\n\t\tnetif_napi_add(bnad->netdev, &rx_ctrl->napi,\n\t\t\t       bnad_napi_poll_rx);\n\t}\n}\n\nstatic void\nbnad_napi_delete(struct bnad *bnad, u32 rx_id)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < bnad->num_rxp_per_rx; i++)\n\t\tnetif_napi_del(&bnad->rx_info[rx_id].rx_ctrl[i].napi);\n}\n\n \nvoid\nbnad_destroy_tx(struct bnad *bnad, u32 tx_id)\n{\n\tstruct bnad_tx_info *tx_info = &bnad->tx_info[tx_id];\n\tstruct bna_res_info *res_info = &bnad->tx_res_info[tx_id].res_info[0];\n\tunsigned long flags;\n\n\tif (!tx_info->tx)\n\t\treturn;\n\n\tinit_completion(&bnad->bnad_completions.tx_comp);\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_tx_disable(tx_info->tx, BNA_HARD_CLEANUP, bnad_cb_tx_disabled);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\twait_for_completion(&bnad->bnad_completions.tx_comp);\n\n\tif (tx_info->tcb[0]->intr_type == BNA_INTR_T_MSIX)\n\t\tbnad_tx_msix_unregister(bnad, tx_info,\n\t\t\tbnad->num_txq_per_tx);\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_tx_destroy(tx_info->tx);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\ttx_info->tx = NULL;\n\ttx_info->tx_id = 0;\n\n\tbnad_tx_res_free(bnad, res_info);\n}\n\n \nint\nbnad_setup_tx(struct bnad *bnad, u32 tx_id)\n{\n\tint err;\n\tstruct bnad_tx_info *tx_info = &bnad->tx_info[tx_id];\n\tstruct bna_res_info *res_info = &bnad->tx_res_info[tx_id].res_info[0];\n\tstruct bna_intr_info *intr_info =\n\t\t\t&res_info[BNA_TX_RES_INTR_T_TXCMPL].res_u.intr_info;\n\tstruct bna_tx_config *tx_config = &bnad->tx_config[tx_id];\n\tstatic const struct bna_tx_event_cbfn tx_cbfn = {\n\t\t.tcb_setup_cbfn = bnad_cb_tcb_setup,\n\t\t.tcb_destroy_cbfn = bnad_cb_tcb_destroy,\n\t\t.tx_stall_cbfn = bnad_cb_tx_stall,\n\t\t.tx_resume_cbfn = bnad_cb_tx_resume,\n\t\t.tx_cleanup_cbfn = bnad_cb_tx_cleanup,\n\t};\n\n\tstruct bna_tx *tx;\n\tunsigned long flags;\n\n\ttx_info->tx_id = tx_id;\n\n\t \n\ttx_config->num_txq = bnad->num_txq_per_tx;\n\ttx_config->txq_depth = bnad->txq_depth;\n\ttx_config->tx_type = BNA_TX_T_REGULAR;\n\ttx_config->coalescing_timeo = bnad->tx_coalescing_timeo;\n\n\t \n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_tx_res_req(bnad->num_txq_per_tx,\n\t\tbnad->txq_depth, res_info);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\t \n\tBNAD_FILL_UNMAPQ_MEM_REQ(&res_info[BNA_TX_RES_MEM_T_UNMAPQ],\n\t\t\tbnad->num_txq_per_tx, (sizeof(struct bnad_tx_unmap) *\n\t\t\tbnad->txq_depth));\n\n\t \n\terr = bnad_tx_res_alloc(bnad, res_info, tx_id);\n\tif (err)\n\t\treturn err;\n\n\t \n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\ttx = bna_tx_create(&bnad->bna, bnad, tx_config, &tx_cbfn, res_info,\n\t\t\ttx_info);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\tif (!tx) {\n\t\terr = -ENOMEM;\n\t\tgoto err_return;\n\t}\n\ttx_info->tx = tx;\n\n\tINIT_DELAYED_WORK(&tx_info->tx_cleanup_work,\n\t\t\t(work_func_t)bnad_tx_cleanup);\n\n\t \n\tif (intr_info->intr_type == BNA_INTR_T_MSIX) {\n\t\terr = bnad_tx_msix_register(bnad, tx_info,\n\t\t\ttx_id, bnad->num_txq_per_tx);\n\t\tif (err)\n\t\t\tgoto cleanup_tx;\n\t}\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_tx_enable(tx);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\treturn 0;\n\ncleanup_tx:\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_tx_destroy(tx_info->tx);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\ttx_info->tx = NULL;\n\ttx_info->tx_id = 0;\nerr_return:\n\tbnad_tx_res_free(bnad, res_info);\n\treturn err;\n}\n\n \n \nstatic void\nbnad_init_rx_config(struct bnad *bnad, struct bna_rx_config *rx_config)\n{\n\tmemset(rx_config, 0, sizeof(*rx_config));\n\trx_config->rx_type = BNA_RX_T_REGULAR;\n\trx_config->num_paths = bnad->num_rxp_per_rx;\n\trx_config->coalescing_timeo = bnad->rx_coalescing_timeo;\n\n\tif (bnad->num_rxp_per_rx > 1) {\n\t\trx_config->rss_status = BNA_STATUS_T_ENABLED;\n\t\trx_config->rss_config.hash_type =\n\t\t\t\t(BFI_ENET_RSS_IPV6 |\n\t\t\t\t BFI_ENET_RSS_IPV6_TCP |\n\t\t\t\t BFI_ENET_RSS_IPV4 |\n\t\t\t\t BFI_ENET_RSS_IPV4_TCP);\n\t\trx_config->rss_config.hash_mask =\n\t\t\t\tbnad->num_rxp_per_rx - 1;\n\t\tnetdev_rss_key_fill(rx_config->rss_config.toeplitz_hash_key,\n\t\t\tsizeof(rx_config->rss_config.toeplitz_hash_key));\n\t} else {\n\t\trx_config->rss_status = BNA_STATUS_T_DISABLED;\n\t\tmemset(&rx_config->rss_config, 0,\n\t\t       sizeof(rx_config->rss_config));\n\t}\n\n\trx_config->frame_size = BNAD_FRAME_SIZE(bnad->netdev->mtu);\n\trx_config->q0_multi_buf = BNA_STATUS_T_DISABLED;\n\n\t \n\t \n\trx_config->rxp_type = BNA_RXP_SLR;\n\n\tif (BNAD_PCI_DEV_IS_CAT2(bnad) &&\n\t    rx_config->frame_size > 4096) {\n\t\t \n\t\trx_config->q0_buf_size = 2048;\n\t\t \n\t\trx_config->q0_num_vecs = 4;\n\t\trx_config->q0_depth = bnad->rxq_depth * rx_config->q0_num_vecs;\n\t\trx_config->q0_multi_buf = BNA_STATUS_T_ENABLED;\n\t} else {\n\t\trx_config->q0_buf_size = rx_config->frame_size;\n\t\trx_config->q0_num_vecs = 1;\n\t\trx_config->q0_depth = bnad->rxq_depth;\n\t}\n\n\t \n\tif (rx_config->rxp_type == BNA_RXP_SLR) {\n\t\trx_config->q1_depth = bnad->rxq_depth;\n\t\trx_config->q1_buf_size = BFI_SMALL_RXBUF_SIZE;\n\t}\n\n\trx_config->vlan_strip_status =\n\t\t(bnad->netdev->features & NETIF_F_HW_VLAN_CTAG_RX) ?\n\t\tBNA_STATUS_T_ENABLED : BNA_STATUS_T_DISABLED;\n}\n\nstatic void\nbnad_rx_ctrl_init(struct bnad *bnad, u32 rx_id)\n{\n\tstruct bnad_rx_info *rx_info = &bnad->rx_info[rx_id];\n\tint i;\n\n\tfor (i = 0; i < bnad->num_rxp_per_rx; i++)\n\t\trx_info->rx_ctrl[i].bnad = bnad;\n}\n\n \nstatic u32\nbnad_reinit_rx(struct bnad *bnad)\n{\n\tstruct net_device *netdev = bnad->netdev;\n\tu32 err = 0, current_err = 0;\n\tu32 rx_id = 0, count = 0;\n\tunsigned long flags;\n\n\t \n\tfor (rx_id = 0; rx_id < bnad->num_rx; rx_id++) {\n\t\tif (!bnad->rx_info[rx_id].rx)\n\t\t\tcontinue;\n\t\tbnad_destroy_rx(bnad, rx_id);\n\t}\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_enet_mtu_set(&bnad->bna.enet,\n\t\t\t BNAD_FRAME_SIZE(bnad->netdev->mtu), NULL);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\tfor (rx_id = 0; rx_id < bnad->num_rx; rx_id++) {\n\t\tcount++;\n\t\tcurrent_err = bnad_setup_rx(bnad, rx_id);\n\t\tif (current_err && !err) {\n\t\t\terr = current_err;\n\t\t\tnetdev_err(netdev, \"RXQ:%u setup failed\\n\", rx_id);\n\t\t}\n\t}\n\n\t \n\tif (bnad->rx_info[0].rx && !err) {\n\t\tbnad_restore_vlans(bnad, 0);\n\t\tbnad_enable_default_bcast(bnad);\n\t\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\t\tbnad_mac_addr_set_locked(bnad, netdev->dev_addr);\n\t\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\t\tbnad_set_rx_mode(netdev);\n\t}\n\n\treturn count;\n}\n\n \nvoid\nbnad_destroy_rx(struct bnad *bnad, u32 rx_id)\n{\n\tstruct bnad_rx_info *rx_info = &bnad->rx_info[rx_id];\n\tstruct bna_rx_config *rx_config = &bnad->rx_config[rx_id];\n\tstruct bna_res_info *res_info = &bnad->rx_res_info[rx_id].res_info[0];\n\tunsigned long flags;\n\tint to_del = 0;\n\n\tif (!rx_info->rx)\n\t\treturn;\n\n\tif (0 == rx_id) {\n\t\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\t\tif (bnad->cfg_flags & BNAD_CF_DIM_ENABLED &&\n\t\t    test_bit(BNAD_RF_DIM_TIMER_RUNNING, &bnad->run_flags)) {\n\t\t\tclear_bit(BNAD_RF_DIM_TIMER_RUNNING, &bnad->run_flags);\n\t\t\tto_del = 1;\n\t\t}\n\t\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\t\tif (to_del)\n\t\t\tdel_timer_sync(&bnad->dim_timer);\n\t}\n\n\tinit_completion(&bnad->bnad_completions.rx_comp);\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_rx_disable(rx_info->rx, BNA_HARD_CLEANUP, bnad_cb_rx_disabled);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\twait_for_completion(&bnad->bnad_completions.rx_comp);\n\n\tif (rx_info->rx_ctrl[0].ccb->intr_type == BNA_INTR_T_MSIX)\n\t\tbnad_rx_msix_unregister(bnad, rx_info, rx_config->num_paths);\n\n\tbnad_napi_delete(bnad, rx_id);\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_rx_destroy(rx_info->rx);\n\n\trx_info->rx = NULL;\n\trx_info->rx_id = 0;\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\tbnad_rx_res_free(bnad, res_info);\n}\n\n \nint\nbnad_setup_rx(struct bnad *bnad, u32 rx_id)\n{\n\tint err;\n\tstruct bnad_rx_info *rx_info = &bnad->rx_info[rx_id];\n\tstruct bna_res_info *res_info = &bnad->rx_res_info[rx_id].res_info[0];\n\tstruct bna_intr_info *intr_info =\n\t\t\t&res_info[BNA_RX_RES_T_INTR].res_u.intr_info;\n\tstruct bna_rx_config *rx_config = &bnad->rx_config[rx_id];\n\tstatic const struct bna_rx_event_cbfn rx_cbfn = {\n\t\t.rcb_setup_cbfn = NULL,\n\t\t.rcb_destroy_cbfn = NULL,\n\t\t.ccb_setup_cbfn = bnad_cb_ccb_setup,\n\t\t.ccb_destroy_cbfn = bnad_cb_ccb_destroy,\n\t\t.rx_stall_cbfn = bnad_cb_rx_stall,\n\t\t.rx_cleanup_cbfn = bnad_cb_rx_cleanup,\n\t\t.rx_post_cbfn = bnad_cb_rx_post,\n\t};\n\tstruct bna_rx *rx;\n\tunsigned long flags;\n\n\trx_info->rx_id = rx_id;\n\n\t \n\tbnad_init_rx_config(bnad, rx_config);\n\n\t \n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_rx_res_req(rx_config, res_info);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\t \n\tBNAD_FILL_UNMAPQ_MEM_REQ(&res_info[BNA_RX_RES_MEM_T_UNMAPDQ],\n\t\t\t\t rx_config->num_paths,\n\t\t\t(rx_config->q0_depth *\n\t\t\t sizeof(struct bnad_rx_unmap)) +\n\t\t\t sizeof(struct bnad_rx_unmap_q));\n\n\tif (rx_config->rxp_type != BNA_RXP_SINGLE) {\n\t\tBNAD_FILL_UNMAPQ_MEM_REQ(&res_info[BNA_RX_RES_MEM_T_UNMAPHQ],\n\t\t\t\t\t rx_config->num_paths,\n\t\t\t\t(rx_config->q1_depth *\n\t\t\t\t sizeof(struct bnad_rx_unmap) +\n\t\t\t\t sizeof(struct bnad_rx_unmap_q)));\n\t}\n\t \n\terr = bnad_rx_res_alloc(bnad, res_info, rx_id);\n\tif (err)\n\t\treturn err;\n\n\tbnad_rx_ctrl_init(bnad, rx_id);\n\n\t \n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\trx = bna_rx_create(&bnad->bna, bnad, rx_config, &rx_cbfn, res_info,\n\t\t\trx_info);\n\tif (!rx) {\n\t\terr = -ENOMEM;\n\t\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\t\tgoto err_return;\n\t}\n\trx_info->rx = rx;\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\tINIT_WORK(&rx_info->rx_cleanup_work,\n\t\t\t(work_func_t)(bnad_rx_cleanup));\n\n\t \n\tbnad_napi_add(bnad, rx_id);\n\n\t \n\tif (intr_info->intr_type == BNA_INTR_T_MSIX) {\n\t\terr = bnad_rx_msix_register(bnad, rx_info, rx_id,\n\t\t\t\t\t\trx_config->num_paths);\n\t\tif (err)\n\t\t\tgoto err_return;\n\t}\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tif (0 == rx_id) {\n\t\t \n\t\tif (bnad->cfg_flags & BNAD_CF_DIM_ENABLED)\n\t\t\tbna_rx_dim_reconfig(&bnad->bna, bna_napi_dim_vector);\n\n\t\t \n\t\tbna_rx_vlanfilter_enable(rx);\n\n\t\t \n\t\tbnad_dim_timer_start(bnad);\n\t}\n\n\tbna_rx_enable(rx);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\treturn 0;\n\nerr_return:\n\tbnad_destroy_rx(bnad, rx_id);\n\treturn err;\n}\n\n \nvoid\nbnad_tx_coalescing_timeo_set(struct bnad *bnad)\n{\n\tstruct bnad_tx_info *tx_info;\n\n\ttx_info = &bnad->tx_info[0];\n\tif (!tx_info->tx)\n\t\treturn;\n\n\tbna_tx_coalescing_timeo_set(tx_info->tx, bnad->tx_coalescing_timeo);\n}\n\n \nvoid\nbnad_rx_coalescing_timeo_set(struct bnad *bnad)\n{\n\tstruct bnad_rx_info *rx_info;\n\tint\ti;\n\n\tfor (i = 0; i < bnad->num_rx; i++) {\n\t\trx_info = &bnad->rx_info[i];\n\t\tif (!rx_info->rx)\n\t\t\tcontinue;\n\t\tbna_rx_coalescing_timeo_set(rx_info->rx,\n\t\t\t\tbnad->rx_coalescing_timeo);\n\t}\n}\n\n \nint\nbnad_mac_addr_set_locked(struct bnad *bnad, const u8 *mac_addr)\n{\n\tint ret;\n\n\tif (!is_valid_ether_addr(mac_addr))\n\t\treturn -EADDRNOTAVAIL;\n\n\t \n\tif (!bnad->rx_info[0].rx)\n\t\treturn 0;\n\n\tret = bna_rx_ucast_set(bnad->rx_info[0].rx, mac_addr);\n\tif (ret != BNA_CB_SUCCESS)\n\t\treturn -EADDRNOTAVAIL;\n\n\treturn 0;\n}\n\n \nint\nbnad_enable_default_bcast(struct bnad *bnad)\n{\n\tstruct bnad_rx_info *rx_info = &bnad->rx_info[0];\n\tint ret;\n\tunsigned long flags;\n\n\tinit_completion(&bnad->bnad_completions.mcast_comp);\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tret = bna_rx_mcast_add(rx_info->rx, bnad_bcast_addr,\n\t\t\t       bnad_cb_rx_mcast_add);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\tif (ret == BNA_CB_SUCCESS)\n\t\twait_for_completion(&bnad->bnad_completions.mcast_comp);\n\telse\n\t\treturn -ENODEV;\n\n\tif (bnad->bnad_completions.mcast_comp_status != BNA_CB_SUCCESS)\n\t\treturn -ENODEV;\n\n\treturn 0;\n}\n\n \nvoid\nbnad_restore_vlans(struct bnad *bnad, u32 rx_id)\n{\n\tu16 vid;\n\tunsigned long flags;\n\n\tfor_each_set_bit(vid, bnad->active_vlans, VLAN_N_VID) {\n\t\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\t\tbna_rx_vlan_add(bnad->rx_info[rx_id].rx, vid);\n\t\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\t}\n}\n\n \nvoid\nbnad_netdev_qstats_fill(struct bnad *bnad, struct rtnl_link_stats64 *stats)\n{\n\tint i, j;\n\n\tfor (i = 0; i < bnad->num_rx; i++) {\n\t\tfor (j = 0; j < bnad->num_rxp_per_rx; j++) {\n\t\t\tif (bnad->rx_info[i].rx_ctrl[j].ccb) {\n\t\t\t\tstats->rx_packets += bnad->rx_info[i].\n\t\t\t\trx_ctrl[j].ccb->rcb[0]->rxq->rx_packets;\n\t\t\t\tstats->rx_bytes += bnad->rx_info[i].\n\t\t\t\t\trx_ctrl[j].ccb->rcb[0]->rxq->rx_bytes;\n\t\t\t\tif (bnad->rx_info[i].rx_ctrl[j].ccb->rcb[1] &&\n\t\t\t\t\tbnad->rx_info[i].rx_ctrl[j].ccb->\n\t\t\t\t\trcb[1]->rxq) {\n\t\t\t\t\tstats->rx_packets +=\n\t\t\t\t\t\tbnad->rx_info[i].rx_ctrl[j].\n\t\t\t\t\t\tccb->rcb[1]->rxq->rx_packets;\n\t\t\t\t\tstats->rx_bytes +=\n\t\t\t\t\t\tbnad->rx_info[i].rx_ctrl[j].\n\t\t\t\t\t\tccb->rcb[1]->rxq->rx_bytes;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tfor (i = 0; i < bnad->num_tx; i++) {\n\t\tfor (j = 0; j < bnad->num_txq_per_tx; j++) {\n\t\t\tif (bnad->tx_info[i].tcb[j]) {\n\t\t\t\tstats->tx_packets +=\n\t\t\t\tbnad->tx_info[i].tcb[j]->txq->tx_packets;\n\t\t\t\tstats->tx_bytes +=\n\t\t\t\t\tbnad->tx_info[i].tcb[j]->txq->tx_bytes;\n\t\t\t}\n\t\t}\n\t}\n}\n\n \nvoid\nbnad_netdev_hwstats_fill(struct bnad *bnad, struct rtnl_link_stats64 *stats)\n{\n\tstruct bfi_enet_stats_mac *mac_stats;\n\tu32 bmap;\n\tint i;\n\n\tmac_stats = &bnad->stats.bna_stats->hw_stats.mac_stats;\n\tstats->rx_errors =\n\t\tmac_stats->rx_fcs_error + mac_stats->rx_alignment_error +\n\t\tmac_stats->rx_frame_length_error + mac_stats->rx_code_error +\n\t\tmac_stats->rx_undersize;\n\tstats->tx_errors = mac_stats->tx_fcs_error +\n\t\t\t\t\tmac_stats->tx_undersize;\n\tstats->rx_dropped = mac_stats->rx_drop;\n\tstats->tx_dropped = mac_stats->tx_drop;\n\tstats->multicast = mac_stats->rx_multicast;\n\tstats->collisions = mac_stats->tx_total_collision;\n\n\tstats->rx_length_errors = mac_stats->rx_frame_length_error;\n\n\t \n\n\tstats->rx_crc_errors = mac_stats->rx_fcs_error;\n\tstats->rx_frame_errors = mac_stats->rx_alignment_error;\n\t \n\tbmap = bna_rx_rid_mask(&bnad->bna);\n\tfor (i = 0; bmap; i++) {\n\t\tif (bmap & 1) {\n\t\t\tstats->rx_fifo_errors +=\n\t\t\t\tbnad->stats.bna_stats->\n\t\t\t\t\thw_stats.rxf_stats[i].frame_drops;\n\t\t\tbreak;\n\t\t}\n\t\tbmap >>= 1;\n\t}\n}\n\nstatic void\nbnad_mbox_irq_sync(struct bnad *bnad)\n{\n\tu32 irq;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tif (bnad->cfg_flags & BNAD_CF_MSIX)\n\t\tirq = bnad->msix_table[BNAD_MAILBOX_MSIX_INDEX].vector;\n\telse\n\t\tirq = bnad->pcidev->irq;\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\tsynchronize_irq(irq);\n}\n\n \nstatic int\nbnad_tso_prepare(struct bnad *bnad, struct sk_buff *skb)\n{\n\tint err;\n\n\terr = skb_cow_head(skb, 0);\n\tif (err < 0) {\n\t\tBNAD_UPDATE_CTR(bnad, tso_err);\n\t\treturn err;\n\t}\n\n\t \n\tif (vlan_get_protocol(skb) == htons(ETH_P_IP)) {\n\t\tstruct iphdr *iph = ip_hdr(skb);\n\n\t\t \n\t\tiph->tot_len = 0;\n\t\tiph->check = 0;\n\n\t\ttcp_hdr(skb)->check =\n\t\t\t~csum_tcpudp_magic(iph->saddr, iph->daddr, 0,\n\t\t\t\t\t   IPPROTO_TCP, 0);\n\t\tBNAD_UPDATE_CTR(bnad, tso4);\n\t} else {\n\t\ttcp_v6_gso_csum_prep(skb);\n\t\tBNAD_UPDATE_CTR(bnad, tso6);\n\t}\n\n\treturn 0;\n}\n\n \nstatic void\nbnad_q_num_init(struct bnad *bnad)\n{\n\tint rxps;\n\n\trxps = min((uint)num_online_cpus(),\n\t\t\t(uint)(BNAD_MAX_RX * BNAD_MAX_RXP_PER_RX));\n\n\tif (!(bnad->cfg_flags & BNAD_CF_MSIX))\n\t\trxps = 1;\t \n\n\tbnad->num_rx = 1;\n\tbnad->num_tx = 1;\n\tbnad->num_rxp_per_rx = rxps;\n\tbnad->num_txq_per_tx = BNAD_TXQ_NUM;\n}\n\n \nstatic void\nbnad_q_num_adjust(struct bnad *bnad, int msix_vectors, int temp)\n{\n\tbnad->num_txq_per_tx = 1;\n\tif ((msix_vectors >= (bnad->num_tx * bnad->num_txq_per_tx)  +\n\t     bnad_rxqs_per_cq + BNAD_MAILBOX_MSIX_VECTORS) &&\n\t    (bnad->cfg_flags & BNAD_CF_MSIX)) {\n\t\tbnad->num_rxp_per_rx = msix_vectors -\n\t\t\t(bnad->num_tx * bnad->num_txq_per_tx) -\n\t\t\tBNAD_MAILBOX_MSIX_VECTORS;\n\t} else\n\t\tbnad->num_rxp_per_rx = 1;\n}\n\n \nstatic int\nbnad_ioceth_disable(struct bnad *bnad)\n{\n\tunsigned long flags;\n\tint err = 0;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tinit_completion(&bnad->bnad_completions.ioc_comp);\n\tbna_ioceth_disable(&bnad->bna.ioceth, BNA_HARD_CLEANUP);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\twait_for_completion_timeout(&bnad->bnad_completions.ioc_comp,\n\t\tmsecs_to_jiffies(BNAD_IOCETH_TIMEOUT));\n\n\terr = bnad->bnad_completions.ioc_comp_status;\n\treturn err;\n}\n\nstatic int\nbnad_ioceth_enable(struct bnad *bnad)\n{\n\tint err = 0;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tinit_completion(&bnad->bnad_completions.ioc_comp);\n\tbnad->bnad_completions.ioc_comp_status = BNA_CB_WAITING;\n\tbna_ioceth_enable(&bnad->bna.ioceth);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\twait_for_completion_timeout(&bnad->bnad_completions.ioc_comp,\n\t\tmsecs_to_jiffies(BNAD_IOCETH_TIMEOUT));\n\n\terr = bnad->bnad_completions.ioc_comp_status;\n\n\treturn err;\n}\n\n \nstatic void\nbnad_res_free(struct bnad *bnad, struct bna_res_info *res_info,\n\t\tu32 res_val_max)\n{\n\tint i;\n\n\tfor (i = 0; i < res_val_max; i++)\n\t\tbnad_mem_free(bnad, &res_info[i].res_u.mem_info);\n}\n\n \nstatic int\nbnad_res_alloc(struct bnad *bnad, struct bna_res_info *res_info,\n\t\tu32 res_val_max)\n{\n\tint i, err;\n\n\tfor (i = 0; i < res_val_max; i++) {\n\t\terr = bnad_mem_alloc(bnad, &res_info[i].res_u.mem_info);\n\t\tif (err)\n\t\t\tgoto err_return;\n\t}\n\treturn 0;\n\nerr_return:\n\tbnad_res_free(bnad, res_info, res_val_max);\n\treturn err;\n}\n\n \nstatic void\nbnad_enable_msix(struct bnad *bnad)\n{\n\tint i, ret;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tif (!(bnad->cfg_flags & BNAD_CF_MSIX)) {\n\t\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\t\treturn;\n\t}\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\tif (bnad->msix_table)\n\t\treturn;\n\n\tbnad->msix_table =\n\t\tkcalloc(bnad->msix_num, sizeof(struct msix_entry), GFP_KERNEL);\n\n\tif (!bnad->msix_table)\n\t\tgoto intx_mode;\n\n\tfor (i = 0; i < bnad->msix_num; i++)\n\t\tbnad->msix_table[i].entry = i;\n\n\tret = pci_enable_msix_range(bnad->pcidev, bnad->msix_table,\n\t\t\t\t    1, bnad->msix_num);\n\tif (ret < 0) {\n\t\tgoto intx_mode;\n\t} else if (ret < bnad->msix_num) {\n\t\tdev_warn(&bnad->pcidev->dev,\n\t\t\t \"%d MSI-X vectors allocated < %d requested\\n\",\n\t\t\t ret, bnad->msix_num);\n\n\t\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\t\t \n\t\tbnad_q_num_adjust(bnad, (ret - BNAD_MAILBOX_MSIX_VECTORS) / 2,\n\t\t\t(ret - BNAD_MAILBOX_MSIX_VECTORS) / 2);\n\t\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\t\tbnad->msix_num = BNAD_NUM_TXQ + BNAD_NUM_RXP +\n\t\t\t BNAD_MAILBOX_MSIX_VECTORS;\n\n\t\tif (bnad->msix_num > ret) {\n\t\t\tpci_disable_msix(bnad->pcidev);\n\t\t\tgoto intx_mode;\n\t\t}\n\t}\n\n\tpci_intx(bnad->pcidev, 0);\n\n\treturn;\n\nintx_mode:\n\tdev_warn(&bnad->pcidev->dev,\n\t\t \"MSI-X enable failed - operating in INTx mode\\n\");\n\n\tkfree(bnad->msix_table);\n\tbnad->msix_table = NULL;\n\tbnad->msix_num = 0;\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbnad->cfg_flags &= ~BNAD_CF_MSIX;\n\tbnad_q_num_init(bnad);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n}\n\nstatic void\nbnad_disable_msix(struct bnad *bnad)\n{\n\tu32 cfg_flags;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tcfg_flags = bnad->cfg_flags;\n\tif (bnad->cfg_flags & BNAD_CF_MSIX)\n\t\tbnad->cfg_flags &= ~BNAD_CF_MSIX;\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\tif (cfg_flags & BNAD_CF_MSIX) {\n\t\tpci_disable_msix(bnad->pcidev);\n\t\tkfree(bnad->msix_table);\n\t\tbnad->msix_table = NULL;\n\t}\n}\n\n \nstatic int\nbnad_open(struct net_device *netdev)\n{\n\tint err;\n\tstruct bnad *bnad = netdev_priv(netdev);\n\tstruct bna_pause_config pause_config;\n\tunsigned long flags;\n\n\tmutex_lock(&bnad->conf_mutex);\n\n\t \n\terr = bnad_setup_tx(bnad, 0);\n\tif (err)\n\t\tgoto err_return;\n\n\t \n\terr = bnad_setup_rx(bnad, 0);\n\tif (err)\n\t\tgoto cleanup_tx;\n\n\t \n\tpause_config.tx_pause = 0;\n\tpause_config.rx_pause = 0;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_enet_mtu_set(&bnad->bna.enet,\n\t\t\t BNAD_FRAME_SIZE(bnad->netdev->mtu), NULL);\n\tbna_enet_pause_config(&bnad->bna.enet, &pause_config);\n\tbna_enet_enable(&bnad->bna.enet);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\t \n\tbnad_enable_default_bcast(bnad);\n\n\t \n\tbnad_restore_vlans(bnad, 0);\n\n\t \n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbnad_mac_addr_set_locked(bnad, netdev->dev_addr);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\t \n\tbnad_stats_timer_start(bnad);\n\n\tmutex_unlock(&bnad->conf_mutex);\n\n\treturn 0;\n\ncleanup_tx:\n\tbnad_destroy_tx(bnad, 0);\n\nerr_return:\n\tmutex_unlock(&bnad->conf_mutex);\n\treturn err;\n}\n\nstatic int\nbnad_stop(struct net_device *netdev)\n{\n\tstruct bnad *bnad = netdev_priv(netdev);\n\tunsigned long flags;\n\n\tmutex_lock(&bnad->conf_mutex);\n\n\t \n\tbnad_stats_timer_stop(bnad);\n\n\tinit_completion(&bnad->bnad_completions.enet_comp);\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_enet_disable(&bnad->bna.enet, BNA_HARD_CLEANUP,\n\t\t\tbnad_cb_enet_disabled);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\twait_for_completion(&bnad->bnad_completions.enet_comp);\n\n\tbnad_destroy_tx(bnad, 0);\n\tbnad_destroy_rx(bnad, 0);\n\n\t \n\tbnad_mbox_irq_sync(bnad);\n\n\tmutex_unlock(&bnad->conf_mutex);\n\n\treturn 0;\n}\n\n \n \nstatic int\nbnad_txq_wi_prepare(struct bnad *bnad, struct bna_tcb *tcb,\n\t\t    struct sk_buff *skb, struct bna_txq_entry *txqent)\n{\n\tu16 flags = 0;\n\tu32 gso_size;\n\tu16 vlan_tag = 0;\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tvlan_tag = (u16)skb_vlan_tag_get(skb);\n\t\tflags |= (BNA_TXQ_WI_CF_INS_PRIO | BNA_TXQ_WI_CF_INS_VLAN);\n\t}\n\tif (test_bit(BNAD_RF_CEE_RUNNING, &bnad->run_flags)) {\n\t\tvlan_tag = ((tcb->priority & 0x7) << VLAN_PRIO_SHIFT)\n\t\t\t\t| (vlan_tag & 0x1fff);\n\t\tflags |= (BNA_TXQ_WI_CF_INS_PRIO | BNA_TXQ_WI_CF_INS_VLAN);\n\t}\n\ttxqent->hdr.wi.vlan_tag = htons(vlan_tag);\n\n\tif (skb_is_gso(skb)) {\n\t\tgso_size = skb_shinfo(skb)->gso_size;\n\t\tif (unlikely(gso_size > bnad->netdev->mtu)) {\n\t\t\tBNAD_UPDATE_CTR(bnad, tx_skb_mss_too_long);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (unlikely((gso_size + skb_tcp_all_headers(skb)) >= skb->len)) {\n\t\t\ttxqent->hdr.wi.opcode = htons(BNA_TXQ_WI_SEND);\n\t\t\ttxqent->hdr.wi.lso_mss = 0;\n\t\t\tBNAD_UPDATE_CTR(bnad, tx_skb_tso_too_short);\n\t\t} else {\n\t\t\ttxqent->hdr.wi.opcode = htons(BNA_TXQ_WI_SEND_LSO);\n\t\t\ttxqent->hdr.wi.lso_mss = htons(gso_size);\n\t\t}\n\n\t\tif (bnad_tso_prepare(bnad, skb)) {\n\t\t\tBNAD_UPDATE_CTR(bnad, tx_skb_tso_prepare);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tflags |= (BNA_TXQ_WI_CF_IP_CKSUM | BNA_TXQ_WI_CF_TCP_CKSUM);\n\t\ttxqent->hdr.wi.l4_hdr_size_n_offset =\n\t\t\thtons(BNA_TXQ_WI_L4_HDR_N_OFFSET(\n\t\t\ttcp_hdrlen(skb) >> 2, skb_transport_offset(skb)));\n\t} else  {\n\t\ttxqent->hdr.wi.opcode =\thtons(BNA_TXQ_WI_SEND);\n\t\ttxqent->hdr.wi.lso_mss = 0;\n\n\t\tif (unlikely(skb->len > (bnad->netdev->mtu + VLAN_ETH_HLEN))) {\n\t\t\tBNAD_UPDATE_CTR(bnad, tx_skb_non_tso_too_long);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\t\t__be16 net_proto = vlan_get_protocol(skb);\n\t\t\tu8 proto = 0;\n\n\t\t\tif (net_proto == htons(ETH_P_IP))\n\t\t\t\tproto = ip_hdr(skb)->protocol;\n#ifdef NETIF_F_IPV6_CSUM\n\t\t\telse if (net_proto == htons(ETH_P_IPV6)) {\n\t\t\t\t \n\t\t\t\tproto = ipv6_hdr(skb)->nexthdr;\n\t\t\t}\n#endif\n\t\t\tif (proto == IPPROTO_TCP) {\n\t\t\t\tflags |= BNA_TXQ_WI_CF_TCP_CKSUM;\n\t\t\t\ttxqent->hdr.wi.l4_hdr_size_n_offset =\n\t\t\t\t\thtons(BNA_TXQ_WI_L4_HDR_N_OFFSET\n\t\t\t\t\t      (0, skb_transport_offset(skb)));\n\n\t\t\t\tBNAD_UPDATE_CTR(bnad, tcpcsum_offload);\n\n\t\t\t\tif (unlikely(skb_headlen(skb) <\n\t\t\t\t\t    skb_tcp_all_headers(skb))) {\n\t\t\t\t\tBNAD_UPDATE_CTR(bnad, tx_skb_tcp_hdr);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t} else if (proto == IPPROTO_UDP) {\n\t\t\t\tflags |= BNA_TXQ_WI_CF_UDP_CKSUM;\n\t\t\t\ttxqent->hdr.wi.l4_hdr_size_n_offset =\n\t\t\t\t\thtons(BNA_TXQ_WI_L4_HDR_N_OFFSET\n\t\t\t\t\t      (0, skb_transport_offset(skb)));\n\n\t\t\t\tBNAD_UPDATE_CTR(bnad, udpcsum_offload);\n\t\t\t\tif (unlikely(skb_headlen(skb) <\n\t\t\t\t\t    skb_transport_offset(skb) +\n\t\t\t\t    sizeof(struct udphdr))) {\n\t\t\t\t\tBNAD_UPDATE_CTR(bnad, tx_skb_udp_hdr);\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\t\t\t} else {\n\n\t\t\t\tBNAD_UPDATE_CTR(bnad, tx_skb_csum_err);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else\n\t\t\ttxqent->hdr.wi.l4_hdr_size_n_offset = 0;\n\t}\n\n\ttxqent->hdr.wi.flags = htons(flags);\n\ttxqent->hdr.wi.frame_length = htonl(skb->len);\n\n\treturn 0;\n}\n\n \nstatic netdev_tx_t\nbnad_start_xmit(struct sk_buff *skb, struct net_device *netdev)\n{\n\tstruct bnad *bnad = netdev_priv(netdev);\n\tu32 txq_id = 0;\n\tstruct bna_tcb *tcb = NULL;\n\tstruct bnad_tx_unmap *unmap_q, *unmap, *head_unmap;\n\tu32\t\tprod, q_depth, vect_id;\n\tu32\t\twis, vectors, len;\n\tint\t\ti;\n\tdma_addr_t\t\tdma_addr;\n\tstruct bna_txq_entry *txqent;\n\n\tlen = skb_headlen(skb);\n\n\t \n\n\tif (unlikely(skb->len <= ETH_HLEN)) {\n\t\tdev_kfree_skb_any(skb);\n\t\tBNAD_UPDATE_CTR(bnad, tx_skb_too_short);\n\t\treturn NETDEV_TX_OK;\n\t}\n\tif (unlikely(len > BFI_TX_MAX_DATA_PER_VECTOR)) {\n\t\tdev_kfree_skb_any(skb);\n\t\tBNAD_UPDATE_CTR(bnad, tx_skb_headlen_zero);\n\t\treturn NETDEV_TX_OK;\n\t}\n\tif (unlikely(len == 0)) {\n\t\tdev_kfree_skb_any(skb);\n\t\tBNAD_UPDATE_CTR(bnad, tx_skb_headlen_zero);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\ttcb = bnad->tx_info[0].tcb[txq_id];\n\n\t \n\tif (unlikely(!tcb || !test_bit(BNAD_TXQ_TX_STARTED, &tcb->flags))) {\n\t\tdev_kfree_skb_any(skb);\n\t\tBNAD_UPDATE_CTR(bnad, tx_skb_stopping);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\tq_depth = tcb->q_depth;\n\tprod = tcb->producer_index;\n\tunmap_q = tcb->unmap_q;\n\n\tvectors = 1 + skb_shinfo(skb)->nr_frags;\n\twis = BNA_TXQ_WI_NEEDED(vectors);\t \n\n\tif (unlikely(vectors > BFI_TX_MAX_VECTORS_PER_PKT)) {\n\t\tdev_kfree_skb_any(skb);\n\t\tBNAD_UPDATE_CTR(bnad, tx_skb_max_vectors);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\t \n\tif (unlikely(wis > BNA_QE_FREE_CNT(tcb, q_depth))) {\n\t\tif ((*tcb->hw_consumer_index != tcb->consumer_index) &&\n\t\t    !test_and_set_bit(BNAD_TXQ_FREE_SENT, &tcb->flags)) {\n\t\t\tu32 sent;\n\t\t\tsent = bnad_txcmpl_process(bnad, tcb);\n\t\t\tif (likely(test_bit(BNAD_TXQ_TX_STARTED, &tcb->flags)))\n\t\t\t\tbna_ib_ack(tcb->i_dbell, sent);\n\t\t\tsmp_mb__before_atomic();\n\t\t\tclear_bit(BNAD_TXQ_FREE_SENT, &tcb->flags);\n\t\t} else {\n\t\t\tnetif_stop_queue(netdev);\n\t\t\tBNAD_UPDATE_CTR(bnad, netif_queue_stop);\n\t\t}\n\n\t\tsmp_mb();\n\t\t \n\t\tif (likely(wis > BNA_QE_FREE_CNT(tcb, q_depth))) {\n\t\t\tBNAD_UPDATE_CTR(bnad, netif_queue_stop);\n\t\t\treturn NETDEV_TX_BUSY;\n\t\t} else {\n\t\t\tnetif_wake_queue(netdev);\n\t\t\tBNAD_UPDATE_CTR(bnad, netif_queue_wakeup);\n\t\t}\n\t}\n\n\ttxqent = &((struct bna_txq_entry *)tcb->sw_q)[prod];\n\thead_unmap = &unmap_q[prod];\n\n\t \n\tif (bnad_txq_wi_prepare(bnad, tcb, skb, txqent)) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\ttxqent->hdr.wi.reserved = 0;\n\ttxqent->hdr.wi.num_vectors = vectors;\n\n\thead_unmap->skb = skb;\n\thead_unmap->nvecs = 0;\n\n\t \n\tunmap = head_unmap;\n\tdma_addr = dma_map_single(&bnad->pcidev->dev, skb->data,\n\t\t\t\t  len, DMA_TO_DEVICE);\n\tif (dma_mapping_error(&bnad->pcidev->dev, dma_addr)) {\n\t\tdev_kfree_skb_any(skb);\n\t\tBNAD_UPDATE_CTR(bnad, tx_skb_map_failed);\n\t\treturn NETDEV_TX_OK;\n\t}\n\tBNA_SET_DMA_ADDR(dma_addr, &txqent->vector[0].host_addr);\n\ttxqent->vector[0].length = htons(len);\n\tdma_unmap_addr_set(&unmap->vectors[0], dma_addr, dma_addr);\n\thead_unmap->nvecs++;\n\n\tfor (i = 0, vect_id = 0; i < vectors - 1; i++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tu32\t\tsize = skb_frag_size(frag);\n\n\t\tif (unlikely(size == 0)) {\n\t\t\t \n\t\t\tbnad_tx_buff_unmap(bnad, unmap_q, q_depth,\n\t\t\t\ttcb->producer_index);\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tBNAD_UPDATE_CTR(bnad, tx_skb_frag_zero);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\n\t\tlen += size;\n\n\t\tvect_id++;\n\t\tif (vect_id == BFI_TX_MAX_VECTORS_PER_WI) {\n\t\t\tvect_id = 0;\n\t\t\tBNA_QE_INDX_INC(prod, q_depth);\n\t\t\ttxqent = &((struct bna_txq_entry *)tcb->sw_q)[prod];\n\t\t\ttxqent->hdr.wi_ext.opcode = htons(BNA_TXQ_WI_EXTENSION);\n\t\t\tunmap = &unmap_q[prod];\n\t\t}\n\n\t\tdma_addr = skb_frag_dma_map(&bnad->pcidev->dev, frag,\n\t\t\t\t\t    0, size, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&bnad->pcidev->dev, dma_addr)) {\n\t\t\t \n\t\t\tbnad_tx_buff_unmap(bnad, unmap_q, q_depth,\n\t\t\t\t\t   tcb->producer_index);\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tBNAD_UPDATE_CTR(bnad, tx_skb_map_failed);\n\t\t\treturn NETDEV_TX_OK;\n\t\t}\n\n\t\tdma_unmap_len_set(&unmap->vectors[vect_id], dma_len, size);\n\t\tBNA_SET_DMA_ADDR(dma_addr, &txqent->vector[vect_id].host_addr);\n\t\ttxqent->vector[vect_id].length = htons(size);\n\t\tdma_unmap_addr_set(&unmap->vectors[vect_id], dma_addr,\n\t\t\t\t   dma_addr);\n\t\thead_unmap->nvecs++;\n\t}\n\n\tif (unlikely(len != skb->len)) {\n\t\t \n\t\tbnad_tx_buff_unmap(bnad, unmap_q, q_depth, tcb->producer_index);\n\t\tdev_kfree_skb_any(skb);\n\t\tBNAD_UPDATE_CTR(bnad, tx_skb_len_mismatch);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\tBNA_QE_INDX_INC(prod, q_depth);\n\ttcb->producer_index = prod;\n\n\twmb();\n\n\tif (unlikely(!test_bit(BNAD_TXQ_TX_STARTED, &tcb->flags)))\n\t\treturn NETDEV_TX_OK;\n\n\tskb_tx_timestamp(skb);\n\n\tbna_txq_prod_indx_doorbell(tcb);\n\n\treturn NETDEV_TX_OK;\n}\n\n \nstatic void\nbnad_get_stats64(struct net_device *netdev, struct rtnl_link_stats64 *stats)\n{\n\tstruct bnad *bnad = netdev_priv(netdev);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\n\tbnad_netdev_qstats_fill(bnad, stats);\n\tbnad_netdev_hwstats_fill(bnad, stats);\n\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n}\n\nstatic void\nbnad_set_rx_ucast_fltr(struct bnad *bnad)\n{\n\tstruct net_device *netdev = bnad->netdev;\n\tint uc_count = netdev_uc_count(netdev);\n\tenum bna_cb_status ret;\n\tu8 *mac_list;\n\tstruct netdev_hw_addr *ha;\n\tint entry;\n\n\tif (netdev_uc_empty(bnad->netdev)) {\n\t\tbna_rx_ucast_listset(bnad->rx_info[0].rx, 0, NULL);\n\t\treturn;\n\t}\n\n\tif (uc_count > bna_attr(&bnad->bna)->num_ucmac)\n\t\tgoto mode_default;\n\n\tmac_list = kcalloc(ETH_ALEN, uc_count, GFP_ATOMIC);\n\tif (mac_list == NULL)\n\t\tgoto mode_default;\n\n\tentry = 0;\n\tnetdev_for_each_uc_addr(ha, netdev) {\n\t\tether_addr_copy(&mac_list[entry * ETH_ALEN], &ha->addr[0]);\n\t\tentry++;\n\t}\n\n\tret = bna_rx_ucast_listset(bnad->rx_info[0].rx, entry, mac_list);\n\tkfree(mac_list);\n\n\tif (ret != BNA_CB_SUCCESS)\n\t\tgoto mode_default;\n\n\treturn;\n\n\t \nmode_default:\n\tbnad->cfg_flags |= BNAD_CF_DEFAULT;\n\tbna_rx_ucast_listset(bnad->rx_info[0].rx, 0, NULL);\n}\n\nstatic void\nbnad_set_rx_mcast_fltr(struct bnad *bnad)\n{\n\tstruct net_device *netdev = bnad->netdev;\n\tint mc_count = netdev_mc_count(netdev);\n\tenum bna_cb_status ret;\n\tu8 *mac_list;\n\n\tif (netdev->flags & IFF_ALLMULTI)\n\t\tgoto mode_allmulti;\n\n\tif (netdev_mc_empty(netdev))\n\t\treturn;\n\n\tif (mc_count > bna_attr(&bnad->bna)->num_mcmac)\n\t\tgoto mode_allmulti;\n\n\tmac_list = kcalloc(mc_count + 1, ETH_ALEN, GFP_ATOMIC);\n\n\tif (mac_list == NULL)\n\t\tgoto mode_allmulti;\n\n\tether_addr_copy(&mac_list[0], &bnad_bcast_addr[0]);\n\n\t \n\tbnad_netdev_mc_list_get(netdev, mac_list);\n\tret = bna_rx_mcast_listset(bnad->rx_info[0].rx, mc_count + 1, mac_list);\n\tkfree(mac_list);\n\n\tif (ret != BNA_CB_SUCCESS)\n\t\tgoto mode_allmulti;\n\n\treturn;\n\nmode_allmulti:\n\tbnad->cfg_flags |= BNAD_CF_ALLMULTI;\n\tbna_rx_mcast_delall(bnad->rx_info[0].rx);\n}\n\nvoid\nbnad_set_rx_mode(struct net_device *netdev)\n{\n\tstruct bnad *bnad = netdev_priv(netdev);\n\tenum bna_rxmode new_mode, mode_mask;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\n\tif (bnad->rx_info[0].rx == NULL) {\n\t\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\t\treturn;\n\t}\n\n\t \n\tbnad->cfg_flags &= ~(BNAD_CF_PROMISC | BNAD_CF_DEFAULT |\n\t\t\tBNAD_CF_ALLMULTI);\n\n\tnew_mode = 0;\n\tif (netdev->flags & IFF_PROMISC) {\n\t\tnew_mode |= BNAD_RXMODE_PROMISC_DEFAULT;\n\t\tbnad->cfg_flags |= BNAD_CF_PROMISC;\n\t} else {\n\t\tbnad_set_rx_mcast_fltr(bnad);\n\n\t\tif (bnad->cfg_flags & BNAD_CF_ALLMULTI)\n\t\t\tnew_mode |= BNA_RXMODE_ALLMULTI;\n\n\t\tbnad_set_rx_ucast_fltr(bnad);\n\n\t\tif (bnad->cfg_flags & BNAD_CF_DEFAULT)\n\t\t\tnew_mode |= BNA_RXMODE_DEFAULT;\n\t}\n\n\tmode_mask = BNA_RXMODE_PROMISC | BNA_RXMODE_DEFAULT |\n\t\t\tBNA_RXMODE_ALLMULTI;\n\tbna_rx_mode_set(bnad->rx_info[0].rx, new_mode, mode_mask);\n\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n}\n\n \nstatic int\nbnad_set_mac_address(struct net_device *netdev, void *addr)\n{\n\tint err;\n\tstruct bnad *bnad = netdev_priv(netdev);\n\tstruct sockaddr *sa = (struct sockaddr *)addr;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\n\terr = bnad_mac_addr_set_locked(bnad, sa->sa_data);\n\tif (!err)\n\t\teth_hw_addr_set(netdev, sa->sa_data);\n\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\treturn err;\n}\n\nstatic int\nbnad_mtu_set(struct bnad *bnad, int frame_size)\n{\n\tunsigned long flags;\n\n\tinit_completion(&bnad->bnad_completions.mtu_comp);\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_enet_mtu_set(&bnad->bna.enet, frame_size, bnad_cb_enet_mtu_set);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\twait_for_completion(&bnad->bnad_completions.mtu_comp);\n\n\treturn bnad->bnad_completions.mtu_comp_status;\n}\n\nstatic int\nbnad_change_mtu(struct net_device *netdev, int new_mtu)\n{\n\tint err, mtu;\n\tstruct bnad *bnad = netdev_priv(netdev);\n\tu32 frame, new_frame;\n\n\tmutex_lock(&bnad->conf_mutex);\n\n\tmtu = netdev->mtu;\n\tnetdev->mtu = new_mtu;\n\n\tframe = BNAD_FRAME_SIZE(mtu);\n\tnew_frame = BNAD_FRAME_SIZE(new_mtu);\n\n\t \n\tif (BNAD_PCI_DEV_IS_CAT2(bnad) &&\n\t    netif_running(bnad->netdev)) {\n\t\t \n\t\tif ((frame <= 4096 && new_frame > 4096) ||\n\t\t    (frame > 4096 && new_frame <= 4096))\n\t\t\tbnad_reinit_rx(bnad);\n\t}\n\n\terr = bnad_mtu_set(bnad, new_frame);\n\tif (err)\n\t\terr = -EBUSY;\n\n\tmutex_unlock(&bnad->conf_mutex);\n\treturn err;\n}\n\nstatic int\nbnad_vlan_rx_add_vid(struct net_device *netdev, __be16 proto, u16 vid)\n{\n\tstruct bnad *bnad = netdev_priv(netdev);\n\tunsigned long flags;\n\n\tif (!bnad->rx_info[0].rx)\n\t\treturn 0;\n\n\tmutex_lock(&bnad->conf_mutex);\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_rx_vlan_add(bnad->rx_info[0].rx, vid);\n\tset_bit(vid, bnad->active_vlans);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\tmutex_unlock(&bnad->conf_mutex);\n\n\treturn 0;\n}\n\nstatic int\nbnad_vlan_rx_kill_vid(struct net_device *netdev, __be16 proto, u16 vid)\n{\n\tstruct bnad *bnad = netdev_priv(netdev);\n\tunsigned long flags;\n\n\tif (!bnad->rx_info[0].rx)\n\t\treturn 0;\n\n\tmutex_lock(&bnad->conf_mutex);\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tclear_bit(vid, bnad->active_vlans);\n\tbna_rx_vlan_del(bnad->rx_info[0].rx, vid);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\tmutex_unlock(&bnad->conf_mutex);\n\n\treturn 0;\n}\n\nstatic int bnad_set_features(struct net_device *dev, netdev_features_t features)\n{\n\tstruct bnad *bnad = netdev_priv(dev);\n\tnetdev_features_t changed = features ^ dev->features;\n\n\tif ((changed & NETIF_F_HW_VLAN_CTAG_RX) && netif_running(dev)) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\n\t\tif (features & NETIF_F_HW_VLAN_CTAG_RX)\n\t\t\tbna_rx_vlan_strip_enable(bnad->rx_info[0].rx);\n\t\telse\n\t\t\tbna_rx_vlan_strip_disable(bnad->rx_info[0].rx);\n\n\t\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void\nbnad_netpoll(struct net_device *netdev)\n{\n\tstruct bnad *bnad = netdev_priv(netdev);\n\tstruct bnad_rx_info *rx_info;\n\tstruct bnad_rx_ctrl *rx_ctrl;\n\tu32 curr_mask;\n\tint i, j;\n\n\tif (!(bnad->cfg_flags & BNAD_CF_MSIX)) {\n\t\tbna_intx_disable(&bnad->bna, curr_mask);\n\t\tbnad_isr(bnad->pcidev->irq, netdev);\n\t\tbna_intx_enable(&bnad->bna, curr_mask);\n\t} else {\n\t\t \n\n\t\t \n\t\tfor (i = 0; i < bnad->num_rx; i++) {\n\t\t\trx_info = &bnad->rx_info[i];\n\t\t\tif (!rx_info->rx)\n\t\t\t\tcontinue;\n\t\t\tfor (j = 0; j < bnad->num_rxp_per_rx; j++) {\n\t\t\t\trx_ctrl = &rx_info->rx_ctrl[j];\n\t\t\t\tif (rx_ctrl->ccb)\n\t\t\t\t\tbnad_netif_rx_schedule_poll(bnad,\n\t\t\t\t\t\t\t    rx_ctrl->ccb);\n\t\t\t}\n\t\t}\n\t}\n}\n#endif\n\nstatic const struct net_device_ops bnad_netdev_ops = {\n\t.ndo_open\t\t= bnad_open,\n\t.ndo_stop\t\t= bnad_stop,\n\t.ndo_start_xmit\t\t= bnad_start_xmit,\n\t.ndo_get_stats64\t= bnad_get_stats64,\n\t.ndo_set_rx_mode\t= bnad_set_rx_mode,\n\t.ndo_validate_addr      = eth_validate_addr,\n\t.ndo_set_mac_address    = bnad_set_mac_address,\n\t.ndo_change_mtu\t\t= bnad_change_mtu,\n\t.ndo_vlan_rx_add_vid    = bnad_vlan_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid   = bnad_vlan_rx_kill_vid,\n\t.ndo_set_features\t= bnad_set_features,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller    = bnad_netpoll\n#endif\n};\n\nstatic void\nbnad_netdev_init(struct bnad *bnad)\n{\n\tstruct net_device *netdev = bnad->netdev;\n\n\tnetdev->hw_features = NETIF_F_SG | NETIF_F_RXCSUM |\n\t\tNETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\tNETIF_F_TSO | NETIF_F_TSO6 | NETIF_F_HW_VLAN_CTAG_TX |\n\t\tNETIF_F_HW_VLAN_CTAG_RX;\n\n\tnetdev->vlan_features = NETIF_F_SG | NETIF_F_HIGHDMA |\n\t\tNETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\tNETIF_F_TSO | NETIF_F_TSO6;\n\n\tnetdev->features |= netdev->hw_features | NETIF_F_HW_VLAN_CTAG_FILTER |\n\t\t\t    NETIF_F_HIGHDMA;\n\n\tnetdev->mem_start = bnad->mmio_start;\n\tnetdev->mem_end = bnad->mmio_start + bnad->mmio_len - 1;\n\n\t \n\tnetdev->min_mtu = ETH_ZLEN - ETH_HLEN;\n\tnetdev->max_mtu = BNAD_JUMBO_MTU;\n\n\tnetdev->netdev_ops = &bnad_netdev_ops;\n\tbnad_set_ethtool_ops(netdev);\n}\n\n \nstatic int\nbnad_init(struct bnad *bnad,\n\t  struct pci_dev *pdev, struct net_device *netdev)\n{\n\tunsigned long flags;\n\n\tSET_NETDEV_DEV(netdev, &pdev->dev);\n\tpci_set_drvdata(pdev, netdev);\n\n\tbnad->netdev = netdev;\n\tbnad->pcidev = pdev;\n\tbnad->mmio_start = pci_resource_start(pdev, 0);\n\tbnad->mmio_len = pci_resource_len(pdev, 0);\n\tbnad->bar0 = ioremap(bnad->mmio_start, bnad->mmio_len);\n\tif (!bnad->bar0) {\n\t\tdev_err(&pdev->dev, \"ioremap for bar0 failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tdev_info(&pdev->dev, \"bar0 mapped to %p, len %llu\\n\", bnad->bar0,\n\t\t (unsigned long long) bnad->mmio_len);\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tif (!bnad_msix_disable)\n\t\tbnad->cfg_flags = BNAD_CF_MSIX;\n\n\tbnad->cfg_flags |= BNAD_CF_DIM_ENABLED;\n\n\tbnad_q_num_init(bnad);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\tbnad->msix_num = (bnad->num_tx * bnad->num_txq_per_tx) +\n\t\t(bnad->num_rx * bnad->num_rxp_per_rx) +\n\t\t\t BNAD_MAILBOX_MSIX_VECTORS;\n\n\tbnad->txq_depth = BNAD_TXQ_DEPTH;\n\tbnad->rxq_depth = BNAD_RXQ_DEPTH;\n\n\tbnad->tx_coalescing_timeo = BFI_TX_COALESCING_TIMEO;\n\tbnad->rx_coalescing_timeo = BFI_RX_COALESCING_TIMEO;\n\n\tsprintf(bnad->wq_name, \"%s_wq_%d\", BNAD_NAME, bnad->id);\n\tbnad->work_q = create_singlethread_workqueue(bnad->wq_name);\n\tif (!bnad->work_q) {\n\t\tiounmap(bnad->bar0);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void\nbnad_uninit(struct bnad *bnad)\n{\n\tif (bnad->work_q) {\n\t\tdestroy_workqueue(bnad->work_q);\n\t\tbnad->work_q = NULL;\n\t}\n\n\tif (bnad->bar0)\n\t\tiounmap(bnad->bar0);\n}\n\n \nstatic void\nbnad_lock_init(struct bnad *bnad)\n{\n\tspin_lock_init(&bnad->bna_lock);\n\tmutex_init(&bnad->conf_mutex);\n}\n\nstatic void\nbnad_lock_uninit(struct bnad *bnad)\n{\n\tmutex_destroy(&bnad->conf_mutex);\n}\n\n \nstatic int\nbnad_pci_init(struct bnad *bnad, struct pci_dev *pdev)\n{\n\tint err;\n\n\terr = pci_enable_device(pdev);\n\tif (err)\n\t\treturn err;\n\terr = pci_request_regions(pdev, BNAD_NAME);\n\tif (err)\n\t\tgoto disable_device;\n\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (err)\n\t\tgoto release_regions;\n\tpci_set_master(pdev);\n\treturn 0;\n\nrelease_regions:\n\tpci_release_regions(pdev);\ndisable_device:\n\tpci_disable_device(pdev);\n\n\treturn err;\n}\n\nstatic void\nbnad_pci_uninit(struct pci_dev *pdev)\n{\n\tpci_release_regions(pdev);\n\tpci_disable_device(pdev);\n}\n\nstatic int\nbnad_pci_probe(struct pci_dev *pdev,\n\t\tconst struct pci_device_id *pcidev_id)\n{\n\tint\terr;\n\tstruct bnad *bnad;\n\tstruct bna *bna;\n\tstruct net_device *netdev;\n\tstruct bfa_pcidev pcidev_info;\n\tunsigned long flags;\n\n\tmutex_lock(&bnad_fwimg_mutex);\n\tif (!cna_get_firmware_buf(pdev)) {\n\t\tmutex_unlock(&bnad_fwimg_mutex);\n\t\tdev_err(&pdev->dev, \"failed to load firmware image!\\n\");\n\t\treturn -ENODEV;\n\t}\n\tmutex_unlock(&bnad_fwimg_mutex);\n\n\t \n\tnetdev = alloc_etherdev(sizeof(struct bnad));\n\tif (!netdev) {\n\t\terr = -ENOMEM;\n\t\treturn err;\n\t}\n\tbnad = netdev_priv(netdev);\n\tbnad_lock_init(bnad);\n\tbnad->id = atomic_inc_return(&bna_id) - 1;\n\n\tmutex_lock(&bnad->conf_mutex);\n\t \n\terr = bnad_pci_init(bnad, pdev);\n\tif (err)\n\t\tgoto unlock_mutex;\n\n\t \n\terr = bnad_init(bnad, pdev, netdev);\n\tif (err)\n\t\tgoto pci_uninit;\n\n\t \n\tbnad_netdev_init(bnad);\n\n\t \n\tnetif_carrier_off(netdev);\n\n\t \n\tif (bna_debugfs_enable)\n\t\tbnad_debugfs_init(bnad);\n\n\t \n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_res_req(&bnad->res_info[0]);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\t \n\terr = bnad_res_alloc(bnad, &bnad->res_info[0], BNA_RES_T_MAX);\n\tif (err)\n\t\tgoto drv_uninit;\n\n\tbna = &bnad->bna;\n\n\t \n\tpcidev_info.pci_slot = PCI_SLOT(bnad->pcidev->devfn);\n\tpcidev_info.pci_func = PCI_FUNC(bnad->pcidev->devfn);\n\tpcidev_info.device_id = bnad->pcidev->device;\n\tpcidev_info.pci_bar_kva = bnad->bar0;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_init(bna, bnad, &pcidev_info, &bnad->res_info[0]);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\tbnad->stats.bna_stats = &bna->stats;\n\n\tbnad_enable_msix(bnad);\n\terr = bnad_mbox_irq_alloc(bnad);\n\tif (err)\n\t\tgoto res_free;\n\n\t \n\ttimer_setup(&bnad->bna.ioceth.ioc.ioc_timer, bnad_ioc_timeout, 0);\n\ttimer_setup(&bnad->bna.ioceth.ioc.hb_timer, bnad_ioc_hb_check, 0);\n\ttimer_setup(&bnad->bna.ioceth.ioc.iocpf_timer, bnad_iocpf_timeout, 0);\n\ttimer_setup(&bnad->bna.ioceth.ioc.sem_timer, bnad_iocpf_sem_timeout,\n\t\t    0);\n\n\t \n\terr = bnad_ioceth_enable(bnad);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"initialization failed err=%d\\n\", err);\n\t\tgoto probe_success;\n\t}\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tif (bna_num_txq_set(bna, BNAD_NUM_TXQ + 1) ||\n\t\tbna_num_rxp_set(bna, BNAD_NUM_RXP + 1)) {\n\t\tbnad_q_num_adjust(bnad, bna_attr(bna)->num_txq - 1,\n\t\t\tbna_attr(bna)->num_rxp - 1);\n\t\tif (bna_num_txq_set(bna, BNAD_NUM_TXQ + 1) ||\n\t\t\tbna_num_rxp_set(bna, BNAD_NUM_RXP + 1))\n\t\t\terr = -EIO;\n\t}\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\tif (err)\n\t\tgoto disable_ioceth;\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_mod_res_req(&bnad->bna, &bnad->mod_res_info[0]);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\terr = bnad_res_alloc(bnad, &bnad->mod_res_info[0], BNA_MOD_RES_T_MAX);\n\tif (err) {\n\t\terr = -EIO;\n\t\tgoto disable_ioceth;\n\t}\n\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_mod_init(&bnad->bna, &bnad->mod_res_info[0]);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\t \n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_enet_perm_mac_get(&bna->enet, bnad->perm_addr);\n\tbnad_set_netdev_perm_addr(bnad);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\tmutex_unlock(&bnad->conf_mutex);\n\n\t \n\terr = register_netdev(netdev);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"registering net device failed\\n\");\n\t\tgoto probe_uninit;\n\t}\n\tset_bit(BNAD_RF_NETDEV_REGISTERED, &bnad->run_flags);\n\n\treturn 0;\n\nprobe_success:\n\tmutex_unlock(&bnad->conf_mutex);\n\treturn 0;\n\nprobe_uninit:\n\tmutex_lock(&bnad->conf_mutex);\n\tbnad_res_free(bnad, &bnad->mod_res_info[0], BNA_MOD_RES_T_MAX);\ndisable_ioceth:\n\tbnad_ioceth_disable(bnad);\n\tdel_timer_sync(&bnad->bna.ioceth.ioc.ioc_timer);\n\tdel_timer_sync(&bnad->bna.ioceth.ioc.sem_timer);\n\tdel_timer_sync(&bnad->bna.ioceth.ioc.hb_timer);\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_uninit(bna);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\tbnad_mbox_irq_free(bnad);\n\tbnad_disable_msix(bnad);\nres_free:\n\tbnad_res_free(bnad, &bnad->res_info[0], BNA_RES_T_MAX);\ndrv_uninit:\n\t \n\tkfree(bnad->regdata);\n\tbnad_debugfs_uninit(bnad);\n\tbnad_uninit(bnad);\npci_uninit:\n\tbnad_pci_uninit(pdev);\nunlock_mutex:\n\tmutex_unlock(&bnad->conf_mutex);\n\tbnad_lock_uninit(bnad);\n\tfree_netdev(netdev);\n\treturn err;\n}\n\nstatic void\nbnad_pci_remove(struct pci_dev *pdev)\n{\n\tstruct net_device *netdev = pci_get_drvdata(pdev);\n\tstruct bnad *bnad;\n\tstruct bna *bna;\n\tunsigned long flags;\n\n\tif (!netdev)\n\t\treturn;\n\n\tbnad = netdev_priv(netdev);\n\tbna = &bnad->bna;\n\n\tif (test_and_clear_bit(BNAD_RF_NETDEV_REGISTERED, &bnad->run_flags))\n\t\tunregister_netdev(netdev);\n\n\tmutex_lock(&bnad->conf_mutex);\n\tbnad_ioceth_disable(bnad);\n\tdel_timer_sync(&bnad->bna.ioceth.ioc.ioc_timer);\n\tdel_timer_sync(&bnad->bna.ioceth.ioc.sem_timer);\n\tdel_timer_sync(&bnad->bna.ioceth.ioc.hb_timer);\n\tspin_lock_irqsave(&bnad->bna_lock, flags);\n\tbna_uninit(bna);\n\tspin_unlock_irqrestore(&bnad->bna_lock, flags);\n\n\tbnad_res_free(bnad, &bnad->mod_res_info[0], BNA_MOD_RES_T_MAX);\n\tbnad_res_free(bnad, &bnad->res_info[0], BNA_RES_T_MAX);\n\tbnad_mbox_irq_free(bnad);\n\tbnad_disable_msix(bnad);\n\tbnad_pci_uninit(pdev);\n\tmutex_unlock(&bnad->conf_mutex);\n\tbnad_lock_uninit(bnad);\n\t \n\tkfree(bnad->regdata);\n\tbnad_debugfs_uninit(bnad);\n\tbnad_uninit(bnad);\n\tfree_netdev(netdev);\n}\n\nstatic const struct pci_device_id bnad_pci_id_table[] = {\n\t{\n\t\tPCI_DEVICE(PCI_VENDOR_ID_BROCADE,\n\t\t\tPCI_DEVICE_ID_BROCADE_CT),\n\t\t.class = PCI_CLASS_NETWORK_ETHERNET << 8,\n\t\t.class_mask =  0xffff00\n\t},\n\t{\n\t\tPCI_DEVICE(PCI_VENDOR_ID_BROCADE,\n\t\t\tBFA_PCI_DEVICE_ID_CT2),\n\t\t.class = PCI_CLASS_NETWORK_ETHERNET << 8,\n\t\t.class_mask =  0xffff00\n\t},\n\t{0,  },\n};\n\nMODULE_DEVICE_TABLE(pci, bnad_pci_id_table);\n\nstatic struct pci_driver bnad_pci_driver = {\n\t.name = BNAD_NAME,\n\t.id_table = bnad_pci_id_table,\n\t.probe = bnad_pci_probe,\n\t.remove = bnad_pci_remove,\n};\n\nstatic int __init\nbnad_module_init(void)\n{\n\tint err;\n\n\tbfa_nw_ioc_auto_recover(bnad_ioc_auto_recover);\n\n\terr = pci_register_driver(&bnad_pci_driver);\n\tif (err < 0) {\n\t\tpr_err(\"bna: PCI driver registration failed err=%d\\n\", err);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic void __exit\nbnad_module_exit(void)\n{\n\tpci_unregister_driver(&bnad_pci_driver);\n\trelease_firmware(bfi_fw);\n}\n\nmodule_init(bnad_module_init);\nmodule_exit(bnad_module_exit);\n\nMODULE_AUTHOR(\"Brocade\");\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"QLogic BR-series 10G PCIe Ethernet driver\");\nMODULE_FIRMWARE(CNA_FW_FILE_CT);\nMODULE_FIRMWARE(CNA_FW_FILE_CT2);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}