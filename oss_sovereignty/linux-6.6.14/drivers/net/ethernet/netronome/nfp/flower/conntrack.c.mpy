{
  "module_name": "conntrack.c",
  "hash_id": "1357d2f185dd4ddeec50f6b0b7c1d98d61d0e56886ff8521401b75ab8d238728",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/netronome/nfp/flower/conntrack.c",
  "human_readable_source": "\n \n\n#include <net/tc_act/tc_csum.h>\n#include <net/tc_act/tc_ct.h>\n\n#include \"conntrack.h\"\n#include \"../nfp_port.h\"\n\nconst struct rhashtable_params nfp_tc_ct_merge_params = {\n\t.head_offset\t\t= offsetof(struct nfp_fl_ct_tc_merge,\n\t\t\t\t\t   hash_node),\n\t.key_len\t\t= sizeof(unsigned long) * 2,\n\t.key_offset\t\t= offsetof(struct nfp_fl_ct_tc_merge, cookie),\n\t.automatic_shrinking\t= true,\n};\n\nconst struct rhashtable_params nfp_nft_ct_merge_params = {\n\t.head_offset\t\t= offsetof(struct nfp_fl_nft_tc_merge,\n\t\t\t\t\t   hash_node),\n\t.key_len\t\t= sizeof(unsigned long) * 3,\n\t.key_offset\t\t= offsetof(struct nfp_fl_nft_tc_merge, cookie),\n\t.automatic_shrinking\t= true,\n};\n\nstatic struct flow_action_entry *get_flow_act(struct flow_rule *rule,\n\t\t\t\t\t      enum flow_action_id act_id);\n\n \nstatic void *get_hashentry(struct rhashtable *ht, void *key,\n\t\t\t   const struct rhashtable_params params, size_t size)\n{\n\tvoid *result;\n\n\tresult = rhashtable_lookup_fast(ht, key, params);\n\n\tif (result)\n\t\treturn result;\n\n\tresult = kzalloc(size, GFP_KERNEL);\n\tif (!result)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\treturn result;\n}\n\nbool is_pre_ct_flow(struct flow_cls_offload *flow)\n{\n\tstruct flow_rule *rule = flow_cls_offload_flow_rule(flow);\n\tstruct flow_dissector *dissector = rule->match.dissector;\n\tstruct flow_action_entry *act;\n\tstruct flow_match_ct ct;\n\tint i;\n\n\tif (dissector->used_keys & BIT_ULL(FLOW_DISSECTOR_KEY_CT)) {\n\t\tflow_rule_match_ct(rule, &ct);\n\t\tif (ct.key->ct_state)\n\t\t\treturn false;\n\t}\n\n\tif (flow->common.chain_index)\n\t\treturn false;\n\n\tflow_action_for_each(i, act, &flow->rule->action) {\n\t\tif (act->id == FLOW_ACTION_CT) {\n\t\t\t \n\t\t\tif ((!act->ct.action || act->ct.action == TCA_CT_ACT_NAT))\n\t\t\t\treturn true;\n\t\t\telse\n\t\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nbool is_post_ct_flow(struct flow_cls_offload *flow)\n{\n\tstruct flow_rule *rule = flow_cls_offload_flow_rule(flow);\n\tstruct flow_dissector *dissector = rule->match.dissector;\n\tstruct flow_action_entry *act;\n\tbool exist_ct_clear = false;\n\tstruct flow_match_ct ct;\n\tint i;\n\n\tif (dissector->used_keys & BIT_ULL(FLOW_DISSECTOR_KEY_CT)) {\n\t\tflow_rule_match_ct(rule, &ct);\n\t\tif (ct.key->ct_state & TCA_FLOWER_KEY_CT_FLAGS_ESTABLISHED)\n\t\t\treturn true;\n\t} else {\n\t\t \n\t\tflow_action_for_each(i, act, &flow->rule->action) {\n\t\t\tif (act->id == FLOW_ACTION_CT) {\n\t\t\t\t \n\t\t\t\tif (act->ct.action == TCA_CT_ACT_CLEAR) {\n\t\t\t\t\texist_ct_clear = true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\t \n\t\tif (flow->common.chain_index && exist_ct_clear)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic void *get_mangled_key(struct flow_rule *rule, void *buf,\n\t\t\t     u32 offset, size_t key_sz,\n\t\t\t     enum flow_action_mangle_base htype)\n{\n\tstruct flow_action_entry *act;\n\tu32 *val = (u32 *)buf;\n\tu32 off, msk, key;\n\tint i;\n\n\tflow_action_for_each(i, act, &rule->action) {\n\t\tif (act->id == FLOW_ACTION_MANGLE &&\n\t\t    act->mangle.htype == htype) {\n\t\t\toff = act->mangle.offset - offset;\n\t\t\tmsk = act->mangle.mask;\n\t\t\tkey = act->mangle.val;\n\n\t\t\t \n\t\t\tif (off % 4 || off >= key_sz)\n\t\t\t\tcontinue;\n\n\t\t\tval[off >> 2] &= msk;\n\t\t\tval[off >> 2] |= key;\n\t\t}\n\t}\n\n\treturn buf;\n}\n\n \n#define NFP_IPV4_TOS_MASK\tGENMASK(23, 16)\n#define NFP_IPV4_TTL_MASK\tGENMASK(31, 24)\n#define NFP_IPV6_TCLASS_MASK\tGENMASK(27, 20)\n#define NFP_IPV6_HLIMIT_MASK\tGENMASK(7, 0)\nstatic void *get_mangled_tos_ttl(struct flow_rule *rule, void *buf,\n\t\t\t\t bool is_v6)\n{\n\tstruct flow_match_ip match;\n\t \n\t__be32 ip_hdr[3];\n\tu32 tmp, hdr_len;\n\n\tflow_rule_match_ip(rule, &match);\n\n\tif (is_v6) {\n\t\ttmp = FIELD_PREP(NFP_IPV6_TCLASS_MASK, match.key->tos);\n\t\tip_hdr[0] = cpu_to_be32(tmp);\n\t\ttmp = FIELD_PREP(NFP_IPV6_HLIMIT_MASK, match.key->ttl);\n\t\tip_hdr[1] = cpu_to_be32(tmp);\n\t\thdr_len = 2 * sizeof(__be32);\n\t} else {\n\t\ttmp = FIELD_PREP(NFP_IPV4_TOS_MASK, match.key->tos);\n\t\tip_hdr[0] = cpu_to_be32(tmp);\n\t\ttmp = FIELD_PREP(NFP_IPV4_TTL_MASK, match.key->ttl);\n\t\tip_hdr[2] = cpu_to_be32(tmp);\n\t\thdr_len = 3 * sizeof(__be32);\n\t}\n\n\tget_mangled_key(rule, ip_hdr, 0, hdr_len,\n\t\t\tis_v6 ? FLOW_ACT_MANGLE_HDR_TYPE_IP6 :\n\t\t\t\tFLOW_ACT_MANGLE_HDR_TYPE_IP4);\n\n\tmatch.key = buf;\n\n\tif (is_v6) {\n\t\ttmp = be32_to_cpu(ip_hdr[0]);\n\t\tmatch.key->tos = FIELD_GET(NFP_IPV6_TCLASS_MASK, tmp);\n\t\ttmp = be32_to_cpu(ip_hdr[1]);\n\t\tmatch.key->ttl = FIELD_GET(NFP_IPV6_HLIMIT_MASK, tmp);\n\t} else {\n\t\ttmp = be32_to_cpu(ip_hdr[0]);\n\t\tmatch.key->tos = FIELD_GET(NFP_IPV4_TOS_MASK, tmp);\n\t\ttmp = be32_to_cpu(ip_hdr[2]);\n\t\tmatch.key->ttl = FIELD_GET(NFP_IPV4_TTL_MASK, tmp);\n\t}\n\n\treturn buf;\n}\n\n \nstatic bool nfp_ct_merge_check_cannot_skip(struct nfp_fl_ct_flow_entry *entry1,\n\t\t\t\t\t   struct nfp_fl_ct_flow_entry *entry2)\n{\n\t \n\tif ((entry1->flags & NFP_FL_ACTION_DO_NAT) &&\n\t    entry2->type == CT_TYPE_POST_CT)\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic int nfp_ct_merge_check(struct nfp_fl_ct_flow_entry *entry1,\n\t\t\t      struct nfp_fl_ct_flow_entry *entry2)\n{\n\tunsigned long long ovlp_keys;\n\tbool out, is_v6 = false;\n\tu8 ip_proto = 0;\n\tovlp_keys = entry1->rule->match.dissector->used_keys &\n\t\t\tentry2->rule->match.dissector->used_keys;\n\t \n\tchar buf[64];\n\n\tif (entry1->netdev && entry2->netdev &&\n\t    entry1->netdev != entry2->netdev)\n\t\treturn -EINVAL;\n\n\t \n\tif (ovlp_keys & BIT_ULL(FLOW_DISSECTOR_KEY_CONTROL)) {\n\t\tstruct flow_match_control match1, match2;\n\n\t\tflow_rule_match_control(entry1->rule, &match1);\n\t\tflow_rule_match_control(entry2->rule, &match2);\n\t\tCOMPARE_UNMASKED_FIELDS(match1, match2, &out);\n\t\tif (out)\n\t\t\tgoto check_failed;\n\t}\n\n\tif (ovlp_keys & BIT_ULL(FLOW_DISSECTOR_KEY_BASIC)) {\n\t\tstruct flow_match_basic match1, match2;\n\n\t\tflow_rule_match_basic(entry1->rule, &match1);\n\t\tflow_rule_match_basic(entry2->rule, &match2);\n\n\t\t \n\t\tis_v6 = match1.key->n_proto == htons(ETH_P_IPV6);\n\t\t \n\t\tip_proto = match1.key->ip_proto;\n\n\t\tCOMPARE_UNMASKED_FIELDS(match1, match2, &out);\n\t\tif (out)\n\t\t\tgoto check_failed;\n\t}\n\n\t \n\tif ((ovlp_keys & BIT_ULL(FLOW_DISSECTOR_KEY_IPV4_ADDRS)) &&\n\t    nfp_ct_merge_check_cannot_skip(entry1, entry2)) {\n\t\tstruct flow_match_ipv4_addrs match1, match2;\n\n\t\tflow_rule_match_ipv4_addrs(entry1->rule, &match1);\n\t\tflow_rule_match_ipv4_addrs(entry2->rule, &match2);\n\n\t\tmemcpy(buf, match1.key, sizeof(*match1.key));\n\t\tmatch1.key = get_mangled_key(entry1->rule, buf,\n\t\t\t\t\t     offsetof(struct iphdr, saddr),\n\t\t\t\t\t     sizeof(*match1.key),\n\t\t\t\t\t     FLOW_ACT_MANGLE_HDR_TYPE_IP4);\n\n\t\tCOMPARE_UNMASKED_FIELDS(match1, match2, &out);\n\t\tif (out)\n\t\t\tgoto check_failed;\n\t}\n\n\t \n\tif ((ovlp_keys & BIT_ULL(FLOW_DISSECTOR_KEY_IPV6_ADDRS)) &&\n\t    nfp_ct_merge_check_cannot_skip(entry1, entry2)) {\n\t\tstruct flow_match_ipv6_addrs match1, match2;\n\n\t\tflow_rule_match_ipv6_addrs(entry1->rule, &match1);\n\t\tflow_rule_match_ipv6_addrs(entry2->rule, &match2);\n\n\t\tmemcpy(buf, match1.key, sizeof(*match1.key));\n\t\tmatch1.key = get_mangled_key(entry1->rule, buf,\n\t\t\t\t\t     offsetof(struct ipv6hdr, saddr),\n\t\t\t\t\t     sizeof(*match1.key),\n\t\t\t\t\t     FLOW_ACT_MANGLE_HDR_TYPE_IP6);\n\n\t\tCOMPARE_UNMASKED_FIELDS(match1, match2, &out);\n\t\tif (out)\n\t\t\tgoto check_failed;\n\t}\n\n\t \n\tif ((ovlp_keys & BIT_ULL(FLOW_DISSECTOR_KEY_PORTS)) &&\n\t    nfp_ct_merge_check_cannot_skip(entry1, entry2)) {\n\t\tenum flow_action_mangle_base htype = FLOW_ACT_MANGLE_UNSPEC;\n\t\tstruct flow_match_ports match1, match2;\n\n\t\tflow_rule_match_ports(entry1->rule, &match1);\n\t\tflow_rule_match_ports(entry2->rule, &match2);\n\n\t\tif (ip_proto == IPPROTO_UDP)\n\t\t\thtype = FLOW_ACT_MANGLE_HDR_TYPE_UDP;\n\t\telse if (ip_proto == IPPROTO_TCP)\n\t\t\thtype = FLOW_ACT_MANGLE_HDR_TYPE_TCP;\n\n\t\tmemcpy(buf, match1.key, sizeof(*match1.key));\n\t\tmatch1.key = get_mangled_key(entry1->rule, buf, 0,\n\t\t\t\t\t     sizeof(*match1.key), htype);\n\n\t\tCOMPARE_UNMASKED_FIELDS(match1, match2, &out);\n\t\tif (out)\n\t\t\tgoto check_failed;\n\t}\n\n\tif (ovlp_keys & BIT_ULL(FLOW_DISSECTOR_KEY_ETH_ADDRS)) {\n\t\tstruct flow_match_eth_addrs match1, match2;\n\n\t\tflow_rule_match_eth_addrs(entry1->rule, &match1);\n\t\tflow_rule_match_eth_addrs(entry2->rule, &match2);\n\n\t\tmemcpy(buf, match1.key, sizeof(*match1.key));\n\t\tmatch1.key = get_mangled_key(entry1->rule, buf, 0,\n\t\t\t\t\t     sizeof(*match1.key),\n\t\t\t\t\t     FLOW_ACT_MANGLE_HDR_TYPE_ETH);\n\n\t\tCOMPARE_UNMASKED_FIELDS(match1, match2, &out);\n\t\tif (out)\n\t\t\tgoto check_failed;\n\t}\n\n\tif (ovlp_keys & BIT_ULL(FLOW_DISSECTOR_KEY_VLAN)) {\n\t\tstruct flow_match_vlan match1, match2;\n\n\t\tflow_rule_match_vlan(entry1->rule, &match1);\n\t\tflow_rule_match_vlan(entry2->rule, &match2);\n\t\tCOMPARE_UNMASKED_FIELDS(match1, match2, &out);\n\t\tif (out)\n\t\t\tgoto check_failed;\n\t}\n\n\tif (ovlp_keys & BIT_ULL(FLOW_DISSECTOR_KEY_MPLS)) {\n\t\tstruct flow_match_mpls match1, match2;\n\n\t\tflow_rule_match_mpls(entry1->rule, &match1);\n\t\tflow_rule_match_mpls(entry2->rule, &match2);\n\t\tCOMPARE_UNMASKED_FIELDS(match1, match2, &out);\n\t\tif (out)\n\t\t\tgoto check_failed;\n\t}\n\n\tif (ovlp_keys & BIT_ULL(FLOW_DISSECTOR_KEY_TCP)) {\n\t\tstruct flow_match_tcp match1, match2;\n\n\t\tflow_rule_match_tcp(entry1->rule, &match1);\n\t\tflow_rule_match_tcp(entry2->rule, &match2);\n\t\tCOMPARE_UNMASKED_FIELDS(match1, match2, &out);\n\t\tif (out)\n\t\t\tgoto check_failed;\n\t}\n\n\tif (ovlp_keys & BIT_ULL(FLOW_DISSECTOR_KEY_IP)) {\n\t\tstruct flow_match_ip match1, match2;\n\n\t\tflow_rule_match_ip(entry1->rule, &match1);\n\t\tflow_rule_match_ip(entry2->rule, &match2);\n\n\t\tmatch1.key = get_mangled_tos_ttl(entry1->rule, buf, is_v6);\n\t\tCOMPARE_UNMASKED_FIELDS(match1, match2, &out);\n\t\tif (out)\n\t\t\tgoto check_failed;\n\t}\n\n\tif (ovlp_keys & BIT_ULL(FLOW_DISSECTOR_KEY_ENC_KEYID)) {\n\t\tstruct flow_match_enc_keyid match1, match2;\n\n\t\tflow_rule_match_enc_keyid(entry1->rule, &match1);\n\t\tflow_rule_match_enc_keyid(entry2->rule, &match2);\n\t\tCOMPARE_UNMASKED_FIELDS(match1, match2, &out);\n\t\tif (out)\n\t\t\tgoto check_failed;\n\t}\n\n\tif (ovlp_keys & BIT_ULL(FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS)) {\n\t\tstruct flow_match_ipv4_addrs match1, match2;\n\n\t\tflow_rule_match_enc_ipv4_addrs(entry1->rule, &match1);\n\t\tflow_rule_match_enc_ipv4_addrs(entry2->rule, &match2);\n\t\tCOMPARE_UNMASKED_FIELDS(match1, match2, &out);\n\t\tif (out)\n\t\t\tgoto check_failed;\n\t}\n\n\tif (ovlp_keys & BIT_ULL(FLOW_DISSECTOR_KEY_ENC_IPV6_ADDRS)) {\n\t\tstruct flow_match_ipv6_addrs match1, match2;\n\n\t\tflow_rule_match_enc_ipv6_addrs(entry1->rule, &match1);\n\t\tflow_rule_match_enc_ipv6_addrs(entry2->rule, &match2);\n\t\tCOMPARE_UNMASKED_FIELDS(match1, match2, &out);\n\t\tif (out)\n\t\t\tgoto check_failed;\n\t}\n\n\tif (ovlp_keys & BIT_ULL(FLOW_DISSECTOR_KEY_ENC_CONTROL)) {\n\t\tstruct flow_match_control match1, match2;\n\n\t\tflow_rule_match_enc_control(entry1->rule, &match1);\n\t\tflow_rule_match_enc_control(entry2->rule, &match2);\n\t\tCOMPARE_UNMASKED_FIELDS(match1, match2, &out);\n\t\tif (out)\n\t\t\tgoto check_failed;\n\t}\n\n\tif (ovlp_keys & BIT_ULL(FLOW_DISSECTOR_KEY_ENC_IP)) {\n\t\tstruct flow_match_ip match1, match2;\n\n\t\tflow_rule_match_enc_ip(entry1->rule, &match1);\n\t\tflow_rule_match_enc_ip(entry2->rule, &match2);\n\t\tCOMPARE_UNMASKED_FIELDS(match1, match2, &out);\n\t\tif (out)\n\t\t\tgoto check_failed;\n\t}\n\n\tif (ovlp_keys & BIT_ULL(FLOW_DISSECTOR_KEY_ENC_OPTS)) {\n\t\tstruct flow_match_enc_opts match1, match2;\n\n\t\tflow_rule_match_enc_opts(entry1->rule, &match1);\n\t\tflow_rule_match_enc_opts(entry2->rule, &match2);\n\t\tCOMPARE_UNMASKED_FIELDS(match1, match2, &out);\n\t\tif (out)\n\t\t\tgoto check_failed;\n\t}\n\n\treturn 0;\n\ncheck_failed:\n\treturn -EINVAL;\n}\n\nstatic int nfp_ct_check_vlan_merge(struct flow_action_entry *a_in,\n\t\t\t\t   struct flow_rule *rule)\n{\n\tstruct flow_match_vlan match;\n\n\tif (unlikely(flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_CVLAN)))\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif (likely(!flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_VLAN)))\n\t\treturn 0;\n\n\tswitch (a_in->id) {\n\t \n\tcase FLOW_ACTION_VLAN_POP:\n\t\treturn -EOPNOTSUPP;\n\n\tcase FLOW_ACTION_VLAN_PUSH:\n\tcase FLOW_ACTION_VLAN_MANGLE:\n\t\tflow_rule_match_vlan(rule, &match);\n\t\t \n\t\tif ((match.key->vlan_id & match.mask->vlan_id) ^\n\t\t    (a_in->vlan.vid & match.mask->vlan_id))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\t \n\t\tif ((match.key->vlan_tpid & match.mask->vlan_tpid) ^\n\t\t    (a_in->vlan.proto & match.mask->vlan_tpid))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\t \n\t\tif ((match.key->vlan_priority & match.mask->vlan_priority) ^\n\t\t    (a_in->vlan.prio & match.mask->vlan_priority))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int nfp_ct_merge_extra_check(struct nfp_fl_ct_flow_entry *nft_entry,\n\t\t\t\t    struct nfp_fl_ct_tc_merge *tc_m_entry)\n{\n\tstruct nfp_fl_nft_tc_merge *prev_nft_m_entry;\n\tstruct nfp_fl_ct_flow_entry *pre_ct_entry;\n\n\tpre_ct_entry = tc_m_entry->pre_ct_parent;\n\tprev_nft_m_entry = pre_ct_entry->prev_m_entries[pre_ct_entry->num_prev_m_entries - 1];\n\n\treturn nfp_ct_merge_check(prev_nft_m_entry->nft_parent, nft_entry);\n}\n\nstatic int nfp_ct_merge_act_check(struct nfp_fl_ct_flow_entry *pre_ct_entry,\n\t\t\t\t  struct nfp_fl_ct_flow_entry *post_ct_entry,\n\t\t\t\t  struct nfp_fl_ct_flow_entry *nft_entry)\n{\n\tstruct flow_action_entry *act;\n\tint i, err;\n\n\t \n\tflow_action_for_each(i, act, &pre_ct_entry->rule->action) {\n\t\tswitch (act->id) {\n\t\tcase FLOW_ACTION_VLAN_PUSH:\n\t\tcase FLOW_ACTION_VLAN_POP:\n\t\tcase FLOW_ACTION_VLAN_MANGLE:\n\t\t\terr = nfp_ct_check_vlan_merge(act, post_ct_entry->rule);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tbreak;\n\t\tcase FLOW_ACTION_MPLS_PUSH:\n\t\tcase FLOW_ACTION_MPLS_POP:\n\t\tcase FLOW_ACTION_MPLS_MANGLE:\n\t\t\treturn -EOPNOTSUPP;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tflow_action_for_each(i, act, &nft_entry->rule->action) {\n\t\tswitch (act->id) {\n\t\tcase FLOW_ACTION_VLAN_PUSH:\n\t\tcase FLOW_ACTION_VLAN_POP:\n\t\tcase FLOW_ACTION_VLAN_MANGLE:\n\t\tcase FLOW_ACTION_MPLS_PUSH:\n\t\tcase FLOW_ACTION_MPLS_POP:\n\t\tcase FLOW_ACTION_MPLS_MANGLE:\n\t\t\treturn -EOPNOTSUPP;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int nfp_ct_check_meta(struct nfp_fl_ct_flow_entry *post_ct_entry,\n\t\t\t     struct nfp_fl_ct_flow_entry *nft_entry)\n{\n\tstruct flow_dissector *dissector = post_ct_entry->rule->match.dissector;\n\tstruct flow_action_entry *ct_met;\n\tstruct flow_match_ct ct;\n\tint i;\n\n\tct_met = get_flow_act(nft_entry->rule, FLOW_ACTION_CT_METADATA);\n\tif (ct_met && (dissector->used_keys & BIT_ULL(FLOW_DISSECTOR_KEY_CT))) {\n\t\tu32 *act_lbl;\n\n\t\tact_lbl = ct_met->ct_metadata.labels;\n\t\tflow_rule_match_ct(post_ct_entry->rule, &ct);\n\t\tfor (i = 0; i < 4; i++) {\n\t\t\tif ((ct.key->ct_labels[i] & ct.mask->ct_labels[i]) ^\n\t\t\t    (act_lbl[i] & ct.mask->ct_labels[i]))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((ct.key->ct_mark & ct.mask->ct_mark) ^\n\t\t    (ct_met->ct_metadata.mark & ct.mask->ct_mark))\n\t\t\treturn -EINVAL;\n\n\t\treturn 0;\n\t} else {\n\t\t \n\t\tif (nft_entry->flags & NFP_FL_ACTION_DO_MANGLE)\n\t\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int\nnfp_fl_calc_key_layers_sz(struct nfp_fl_key_ls in_key_ls, uint16_t *map)\n{\n\tint key_size;\n\n\t \n\tkey_size = sizeof(struct nfp_flower_meta_tci);\n\tmap[FLOW_PAY_META_TCI] = 0;\n\n\tif (in_key_ls.key_layer & NFP_FLOWER_LAYER_EXT_META) {\n\t\tmap[FLOW_PAY_EXT_META] = key_size;\n\t\tkey_size += sizeof(struct nfp_flower_ext_meta);\n\t}\n\tif (in_key_ls.key_layer & NFP_FLOWER_LAYER_PORT) {\n\t\tmap[FLOW_PAY_INPORT] = key_size;\n\t\tkey_size += sizeof(struct nfp_flower_in_port);\n\t}\n\tif (in_key_ls.key_layer & NFP_FLOWER_LAYER_MAC) {\n\t\tmap[FLOW_PAY_MAC_MPLS] = key_size;\n\t\tkey_size += sizeof(struct nfp_flower_mac_mpls);\n\t}\n\tif (in_key_ls.key_layer & NFP_FLOWER_LAYER_TP) {\n\t\tmap[FLOW_PAY_L4] = key_size;\n\t\tkey_size += sizeof(struct nfp_flower_tp_ports);\n\t}\n\tif (in_key_ls.key_layer & NFP_FLOWER_LAYER_IPV4) {\n\t\tmap[FLOW_PAY_IPV4] = key_size;\n\t\tkey_size += sizeof(struct nfp_flower_ipv4);\n\t}\n\tif (in_key_ls.key_layer & NFP_FLOWER_LAYER_IPV6) {\n\t\tmap[FLOW_PAY_IPV6] = key_size;\n\t\tkey_size += sizeof(struct nfp_flower_ipv6);\n\t}\n\n\tif (in_key_ls.key_layer_two & NFP_FLOWER_LAYER2_QINQ) {\n\t\tmap[FLOW_PAY_QINQ] = key_size;\n\t\tkey_size += sizeof(struct nfp_flower_vlan);\n\t}\n\n\tif (in_key_ls.key_layer_two & NFP_FLOWER_LAYER2_GRE) {\n\t\tmap[FLOW_PAY_GRE] = key_size;\n\t\tif (in_key_ls.key_layer_two & NFP_FLOWER_LAYER2_TUN_IPV6)\n\t\t\tkey_size += sizeof(struct nfp_flower_ipv6_gre_tun);\n\t\telse\n\t\t\tkey_size += sizeof(struct nfp_flower_ipv4_gre_tun);\n\t}\n\n\tif ((in_key_ls.key_layer & NFP_FLOWER_LAYER_VXLAN) ||\n\t    (in_key_ls.key_layer_two & NFP_FLOWER_LAYER2_GENEVE)) {\n\t\tmap[FLOW_PAY_UDP_TUN] = key_size;\n\t\tif (in_key_ls.key_layer_two & NFP_FLOWER_LAYER2_TUN_IPV6)\n\t\t\tkey_size += sizeof(struct nfp_flower_ipv6_udp_tun);\n\t\telse\n\t\t\tkey_size += sizeof(struct nfp_flower_ipv4_udp_tun);\n\t}\n\n\tif (in_key_ls.key_layer_two & NFP_FLOWER_LAYER2_GENEVE_OP) {\n\t\tmap[FLOW_PAY_GENEVE_OPT] = key_size;\n\t\tkey_size += sizeof(struct nfp_flower_geneve_options);\n\t}\n\n\treturn key_size;\n}\n\n \nstatic void nfp_fl_get_csum_flag(struct flow_action_entry *a_in, u8 ip_proto, u32 *csum)\n{\n\tif (a_in->id != FLOW_ACTION_MANGLE)\n\t\treturn;\n\n\tswitch (a_in->mangle.htype) {\n\tcase FLOW_ACT_MANGLE_HDR_TYPE_IP4:\n\t\t*csum |= TCA_CSUM_UPDATE_FLAG_IPV4HDR;\n\t\tif (ip_proto == IPPROTO_TCP)\n\t\t\t*csum |= TCA_CSUM_UPDATE_FLAG_TCP;\n\t\telse if (ip_proto == IPPROTO_UDP)\n\t\t\t*csum |= TCA_CSUM_UPDATE_FLAG_UDP;\n\t\tbreak;\n\tcase FLOW_ACT_MANGLE_HDR_TYPE_TCP:\n\t\t*csum |= TCA_CSUM_UPDATE_FLAG_TCP;\n\t\tbreak;\n\tcase FLOW_ACT_MANGLE_HDR_TYPE_UDP:\n\t\t*csum |= TCA_CSUM_UPDATE_FLAG_UDP;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic int nfp_fl_merge_actions_offload(struct flow_rule **rules,\n\t\t\t\t\tstruct nfp_flower_priv *priv,\n\t\t\t\t\tstruct net_device *netdev,\n\t\t\t\t\tstruct nfp_fl_payload *flow_pay,\n\t\t\t\t\tint num_rules)\n{\n\tenum flow_action_hw_stats tmp_stats = FLOW_ACTION_HW_STATS_DONT_CARE;\n\tstruct flow_action_entry *a_in;\n\tint i, j, id, num_actions = 0;\n\tstruct flow_rule *a_rule;\n\tint err = 0, offset = 0;\n\n\tfor (i = 0; i < num_rules; i++)\n\t\tnum_actions += rules[i]->action.num_entries;\n\n\t \n\ta_rule = flow_rule_alloc(num_actions + (num_rules / 2));\n\tif (!a_rule)\n\t\treturn -ENOMEM;\n\n\t \n\tif (rules[num_rules - 1]->action.num_entries != 0)\n\t\ttmp_stats = rules[num_rules - 1]->action.entries[0].hw_stats;\n\n\t \n\ta_rule->match = rules[0]->match;\n\n\t \n\tfor (j = 0; j < num_rules; j++) {\n\t\tu32 csum_updated = 0;\n\t\tu8 ip_proto = 0;\n\n\t\tif (flow_rule_match_key(rules[j], FLOW_DISSECTOR_KEY_BASIC)) {\n\t\t\tstruct flow_match_basic match;\n\n\t\t\t \n\t\t\tflow_rule_match_basic(rules[j], &match);\n\t\t\tif (match.mask->ip_proto) {\n\t\t\t\ta_rule->match = rules[j]->match;\n\t\t\t\tip_proto = match.key->ip_proto;\n\t\t\t}\n\t\t}\n\n\t\tfor (i = 0; i < rules[j]->action.num_entries; i++) {\n\t\t\ta_in = &rules[j]->action.entries[i];\n\t\t\tid = a_in->id;\n\n\t\t\t \n\t\t\tswitch (id) {\n\t\t\tcase FLOW_ACTION_CT:\n\t\t\tcase FLOW_ACTION_GOTO:\n\t\t\tcase FLOW_ACTION_CT_METADATA:\n\t\t\t\tcontinue;\n\t\t\tdefault:\n\t\t\t\t \n\t\t\t\tif (j & 0x01) {\n\t\t\t\t\tif (a_in->hw_stats == FLOW_ACTION_HW_STATS_DONT_CARE)\n\t\t\t\t\t\ta_in->hw_stats = tmp_stats;\n\t\t\t\t\tnfp_fl_get_csum_flag(a_in, ip_proto, &csum_updated);\n\t\t\t\t}\n\t\t\t\tmemcpy(&a_rule->action.entries[offset++],\n\t\t\t\t       a_in, sizeof(struct flow_action_entry));\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\t \n\t\tif (csum_updated) {\n\t\t\tstruct flow_action_entry *csum_action;\n\n\t\t\tcsum_action = &a_rule->action.entries[offset++];\n\t\t\tcsum_action->id = FLOW_ACTION_CSUM;\n\t\t\tcsum_action->csum_flags = csum_updated;\n\t\t\tcsum_action->hw_stats = tmp_stats;\n\t\t}\n\t}\n\n\t \n\ta_rule->action.num_entries = offset;\n\terr = nfp_flower_compile_action(priv->app, a_rule, netdev, flow_pay, NULL);\n\tkfree(a_rule);\n\n\treturn err;\n}\n\nstatic int nfp_fl_ct_add_offload(struct nfp_fl_nft_tc_merge *m_entry)\n{\n\tenum nfp_flower_tun_type tun_type = NFP_FL_TUNNEL_NONE;\n\tstruct nfp_fl_ct_zone_entry *zt = m_entry->zt;\n\tstruct flow_rule *rules[NFP_MAX_ENTRY_RULES];\n\tstruct nfp_fl_ct_flow_entry *pre_ct_entry;\n\tstruct nfp_fl_key_ls key_layer, tmp_layer;\n\tstruct nfp_flower_priv *priv = zt->priv;\n\tu16 key_map[_FLOW_PAY_LAYERS_MAX];\n\tstruct nfp_fl_payload *flow_pay;\n\tu8 *key, *msk, *kdata, *mdata;\n\tstruct nfp_port *port = NULL;\n\tint num_rules, err, i, j = 0;\n\tstruct net_device *netdev;\n\tbool qinq_sup;\n\tu32 port_id;\n\tu16 offset;\n\n\tnetdev = m_entry->netdev;\n\tqinq_sup = !!(priv->flower_ext_feats & NFP_FL_FEATS_VLAN_QINQ);\n\n\tpre_ct_entry = m_entry->tc_m_parent->pre_ct_parent;\n\tnum_rules = pre_ct_entry->num_prev_m_entries * 2 + _CT_TYPE_MAX;\n\n\tfor (i = 0; i < pre_ct_entry->num_prev_m_entries; i++) {\n\t\trules[j++] = pre_ct_entry->prev_m_entries[i]->tc_m_parent->pre_ct_parent->rule;\n\t\trules[j++] = pre_ct_entry->prev_m_entries[i]->nft_parent->rule;\n\t}\n\n\trules[j++] = m_entry->tc_m_parent->pre_ct_parent->rule;\n\trules[j++] = m_entry->nft_parent->rule;\n\trules[j++] = m_entry->tc_m_parent->post_ct_parent->rule;\n\n\tmemset(&key_layer, 0, sizeof(struct nfp_fl_key_ls));\n\tmemset(&key_map, 0, sizeof(key_map));\n\n\t \n\tfor (i = 0; i < num_rules; i++) {\n\t\terr = nfp_flower_calculate_key_layers(priv->app,\n\t\t\t\t\t\t      m_entry->netdev,\n\t\t\t\t\t\t      &tmp_layer, rules[i],\n\t\t\t\t\t\t      &tun_type, NULL);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tkey_layer.key_layer |= tmp_layer.key_layer;\n\t\tkey_layer.key_layer_two |= tmp_layer.key_layer_two;\n\t}\n\tkey_layer.key_size = nfp_fl_calc_key_layers_sz(key_layer, key_map);\n\n\tflow_pay = nfp_flower_allocate_new(&key_layer);\n\tif (!flow_pay)\n\t\treturn -ENOMEM;\n\n\tmemset(flow_pay->unmasked_data, 0, key_layer.key_size);\n\tmemset(flow_pay->mask_data, 0, key_layer.key_size);\n\n\tkdata = flow_pay->unmasked_data;\n\tmdata = flow_pay->mask_data;\n\n\toffset = key_map[FLOW_PAY_META_TCI];\n\tkey = kdata + offset;\n\tmsk = mdata + offset;\n\tnfp_flower_compile_meta((struct nfp_flower_meta_tci *)key,\n\t\t\t\t(struct nfp_flower_meta_tci *)msk,\n\t\t\t\tkey_layer.key_layer);\n\n\tif (NFP_FLOWER_LAYER_EXT_META & key_layer.key_layer) {\n\t\toffset =  key_map[FLOW_PAY_EXT_META];\n\t\tkey = kdata + offset;\n\t\tmsk = mdata + offset;\n\t\tnfp_flower_compile_ext_meta((struct nfp_flower_ext_meta *)key,\n\t\t\t\t\t    key_layer.key_layer_two);\n\t\tnfp_flower_compile_ext_meta((struct nfp_flower_ext_meta *)msk,\n\t\t\t\t\t    key_layer.key_layer_two);\n\t}\n\n\t \n\tport_id = nfp_flower_get_port_id_from_netdev(priv->app, netdev);\n\toffset = key_map[FLOW_PAY_INPORT];\n\tkey = kdata + offset;\n\tmsk = mdata + offset;\n\terr = nfp_flower_compile_port((struct nfp_flower_in_port *)key,\n\t\t\t\t      port_id, false, tun_type, NULL);\n\tif (err)\n\t\tgoto ct_offload_err;\n\terr = nfp_flower_compile_port((struct nfp_flower_in_port *)msk,\n\t\t\t\t      port_id, true, tun_type, NULL);\n\tif (err)\n\t\tgoto ct_offload_err;\n\n\t \n\tif (!qinq_sup) {\n\t\tfor (i = 0; i < num_rules; i++) {\n\t\t\toffset = key_map[FLOW_PAY_META_TCI];\n\t\t\tkey = kdata + offset;\n\t\t\tmsk = mdata + offset;\n\t\t\tnfp_flower_compile_tci((struct nfp_flower_meta_tci *)key,\n\t\t\t\t\t       (struct nfp_flower_meta_tci *)msk,\n\t\t\t\t\t       rules[i]);\n\t\t}\n\t}\n\n\tif (NFP_FLOWER_LAYER_MAC & key_layer.key_layer) {\n\t\toffset = key_map[FLOW_PAY_MAC_MPLS];\n\t\tkey = kdata + offset;\n\t\tmsk = mdata + offset;\n\t\tfor (i = 0; i < num_rules; i++) {\n\t\t\tnfp_flower_compile_mac((struct nfp_flower_mac_mpls *)key,\n\t\t\t\t\t       (struct nfp_flower_mac_mpls *)msk,\n\t\t\t\t\t       rules[i]);\n\t\t\terr = nfp_flower_compile_mpls((struct nfp_flower_mac_mpls *)key,\n\t\t\t\t\t\t      (struct nfp_flower_mac_mpls *)msk,\n\t\t\t\t\t\t      rules[i], NULL);\n\t\t\tif (err)\n\t\t\t\tgoto ct_offload_err;\n\t\t}\n\t}\n\n\tif (NFP_FLOWER_LAYER_IPV4 & key_layer.key_layer) {\n\t\toffset = key_map[FLOW_PAY_IPV4];\n\t\tkey = kdata + offset;\n\t\tmsk = mdata + offset;\n\t\tfor (i = 0; i < num_rules; i++) {\n\t\t\tnfp_flower_compile_ipv4((struct nfp_flower_ipv4 *)key,\n\t\t\t\t\t\t(struct nfp_flower_ipv4 *)msk,\n\t\t\t\t\t\trules[i]);\n\t\t}\n\t}\n\n\tif (NFP_FLOWER_LAYER_IPV6 & key_layer.key_layer) {\n\t\toffset = key_map[FLOW_PAY_IPV6];\n\t\tkey = kdata + offset;\n\t\tmsk = mdata + offset;\n\t\tfor (i = 0; i < num_rules; i++) {\n\t\t\tnfp_flower_compile_ipv6((struct nfp_flower_ipv6 *)key,\n\t\t\t\t\t\t(struct nfp_flower_ipv6 *)msk,\n\t\t\t\t\t\trules[i]);\n\t\t}\n\t}\n\n\tif (NFP_FLOWER_LAYER_TP & key_layer.key_layer) {\n\t\toffset = key_map[FLOW_PAY_L4];\n\t\tkey = kdata + offset;\n\t\tmsk = mdata + offset;\n\t\tfor (i = 0; i < num_rules; i++) {\n\t\t\tnfp_flower_compile_tport((struct nfp_flower_tp_ports *)key,\n\t\t\t\t\t\t (struct nfp_flower_tp_ports *)msk,\n\t\t\t\t\t\t rules[i]);\n\t\t}\n\t}\n\n\tif (NFP_FLOWER_LAYER2_QINQ & key_layer.key_layer_two) {\n\t\toffset = key_map[FLOW_PAY_QINQ];\n\t\tkey = kdata + offset;\n\t\tmsk = mdata + offset;\n\t\tfor (i = 0; i < num_rules; i++) {\n\t\t\tnfp_flower_compile_vlan((struct nfp_flower_vlan *)key,\n\t\t\t\t\t\t(struct nfp_flower_vlan *)msk,\n\t\t\t\t\t\trules[i]);\n\t\t}\n\t}\n\n\tif (key_layer.key_layer_two & NFP_FLOWER_LAYER2_GRE) {\n\t\toffset = key_map[FLOW_PAY_GRE];\n\t\tkey = kdata + offset;\n\t\tmsk = mdata + offset;\n\t\tif (key_layer.key_layer_two & NFP_FLOWER_LAYER2_TUN_IPV6) {\n\t\t\tstruct nfp_flower_ipv6_gre_tun *gre_match;\n\t\t\tstruct nfp_ipv6_addr_entry *entry;\n\t\t\tstruct in6_addr *dst;\n\n\t\t\tfor (i = 0; i < num_rules; i++) {\n\t\t\t\tnfp_flower_compile_ipv6_gre_tun((void *)key,\n\t\t\t\t\t\t\t\t(void *)msk, rules[i]);\n\t\t\t}\n\t\t\tgre_match = (struct nfp_flower_ipv6_gre_tun *)key;\n\t\t\tdst = &gre_match->ipv6.dst;\n\n\t\t\tentry = nfp_tunnel_add_ipv6_off(priv->app, dst);\n\t\t\tif (!entry) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto ct_offload_err;\n\t\t\t}\n\n\t\t\tflow_pay->nfp_tun_ipv6 = entry;\n\t\t} else {\n\t\t\t__be32 dst;\n\n\t\t\tfor (i = 0; i < num_rules; i++) {\n\t\t\t\tnfp_flower_compile_ipv4_gre_tun((void *)key,\n\t\t\t\t\t\t\t\t(void *)msk, rules[i]);\n\t\t\t}\n\t\t\tdst = ((struct nfp_flower_ipv4_gre_tun *)key)->ipv4.dst;\n\n\t\t\t \n\t\t\tflow_pay->nfp_tun_ipv4_addr = dst;\n\t\t\tnfp_tunnel_add_ipv4_off(priv->app, dst);\n\t\t}\n\t}\n\n\tif (key_layer.key_layer & NFP_FLOWER_LAYER_VXLAN ||\n\t    key_layer.key_layer_two & NFP_FLOWER_LAYER2_GENEVE) {\n\t\toffset = key_map[FLOW_PAY_UDP_TUN];\n\t\tkey = kdata + offset;\n\t\tmsk = mdata + offset;\n\t\tif (key_layer.key_layer_two & NFP_FLOWER_LAYER2_TUN_IPV6) {\n\t\t\tstruct nfp_flower_ipv6_udp_tun *udp_match;\n\t\t\tstruct nfp_ipv6_addr_entry *entry;\n\t\t\tstruct in6_addr *dst;\n\n\t\t\tfor (i = 0; i < num_rules; i++) {\n\t\t\t\tnfp_flower_compile_ipv6_udp_tun((void *)key,\n\t\t\t\t\t\t\t\t(void *)msk, rules[i]);\n\t\t\t}\n\t\t\tudp_match = (struct nfp_flower_ipv6_udp_tun *)key;\n\t\t\tdst = &udp_match->ipv6.dst;\n\n\t\t\tentry = nfp_tunnel_add_ipv6_off(priv->app, dst);\n\t\t\tif (!entry) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto ct_offload_err;\n\t\t\t}\n\n\t\t\tflow_pay->nfp_tun_ipv6 = entry;\n\t\t} else {\n\t\t\t__be32 dst;\n\n\t\t\tfor (i = 0; i < num_rules; i++) {\n\t\t\t\tnfp_flower_compile_ipv4_udp_tun((void *)key,\n\t\t\t\t\t\t\t\t(void *)msk, rules[i]);\n\t\t\t}\n\t\t\tdst = ((struct nfp_flower_ipv4_udp_tun *)key)->ipv4.dst;\n\n\t\t\t \n\t\t\tflow_pay->nfp_tun_ipv4_addr = dst;\n\t\t\tnfp_tunnel_add_ipv4_off(priv->app, dst);\n\t\t}\n\n\t\tif (key_layer.key_layer_two & NFP_FLOWER_LAYER2_GENEVE_OP) {\n\t\t\toffset = key_map[FLOW_PAY_GENEVE_OPT];\n\t\t\tkey = kdata + offset;\n\t\t\tmsk = mdata + offset;\n\t\t\tfor (i = 0; i < num_rules; i++)\n\t\t\t\tnfp_flower_compile_geneve_opt(key, msk, rules[i]);\n\t\t}\n\t}\n\n\t \n\terr = nfp_fl_merge_actions_offload(rules, priv, netdev, flow_pay, num_rules);\n\tif (err)\n\t\tgoto ct_offload_err;\n\n\t \n\tflow_pay->tc_flower_cookie = ((unsigned long)flow_pay) | 0x1;\n\terr = nfp_compile_flow_metadata(priv->app, flow_pay->tc_flower_cookie,\n\t\t\t\t\tflow_pay, netdev, NULL);\n\tif (err)\n\t\tgoto ct_offload_err;\n\n\tif (nfp_netdev_is_nfp_repr(netdev))\n\t\tport = nfp_port_from_netdev(netdev);\n\n\terr = rhashtable_insert_fast(&priv->flow_table, &flow_pay->fl_node,\n\t\t\t\t     nfp_flower_table_params);\n\tif (err)\n\t\tgoto ct_release_offload_meta_err;\n\n\terr = nfp_flower_xmit_flow(priv->app, flow_pay,\n\t\t\t\t   NFP_FLOWER_CMSG_TYPE_FLOW_ADD);\n\tif (err)\n\t\tgoto ct_remove_rhash_err;\n\n\tm_entry->tc_flower_cookie = flow_pay->tc_flower_cookie;\n\tm_entry->flow_pay = flow_pay;\n\n\tif (port)\n\t\tport->tc_offload_cnt++;\n\n\treturn err;\n\nct_remove_rhash_err:\n\tWARN_ON_ONCE(rhashtable_remove_fast(&priv->flow_table,\n\t\t\t\t\t    &flow_pay->fl_node,\n\t\t\t\t\t    nfp_flower_table_params));\nct_release_offload_meta_err:\n\tnfp_modify_flow_metadata(priv->app, flow_pay);\nct_offload_err:\n\tif (flow_pay->nfp_tun_ipv4_addr)\n\t\tnfp_tunnel_del_ipv4_off(priv->app, flow_pay->nfp_tun_ipv4_addr);\n\tif (flow_pay->nfp_tun_ipv6)\n\t\tnfp_tunnel_put_ipv6_off(priv->app, flow_pay->nfp_tun_ipv6);\n\tkfree(flow_pay->action_data);\n\tkfree(flow_pay->mask_data);\n\tkfree(flow_pay->unmasked_data);\n\tkfree(flow_pay);\n\treturn err;\n}\n\nstatic int nfp_fl_ct_del_offload(struct nfp_app *app, unsigned long cookie,\n\t\t\t\t struct net_device *netdev)\n{\n\tstruct nfp_flower_priv *priv = app->priv;\n\tstruct nfp_fl_payload *flow_pay;\n\tstruct nfp_port *port = NULL;\n\tint err = 0;\n\n\tif (nfp_netdev_is_nfp_repr(netdev))\n\t\tport = nfp_port_from_netdev(netdev);\n\n\tflow_pay = nfp_flower_search_fl_table(app, cookie, netdev);\n\tif (!flow_pay)\n\t\treturn -ENOENT;\n\n\terr = nfp_modify_flow_metadata(app, flow_pay);\n\tif (err)\n\t\tgoto err_free_merge_flow;\n\n\tif (flow_pay->nfp_tun_ipv4_addr)\n\t\tnfp_tunnel_del_ipv4_off(app, flow_pay->nfp_tun_ipv4_addr);\n\n\tif (flow_pay->nfp_tun_ipv6)\n\t\tnfp_tunnel_put_ipv6_off(app, flow_pay->nfp_tun_ipv6);\n\n\tif (!flow_pay->in_hw) {\n\t\terr = 0;\n\t\tgoto err_free_merge_flow;\n\t}\n\n\terr = nfp_flower_xmit_flow(app, flow_pay,\n\t\t\t\t   NFP_FLOWER_CMSG_TYPE_FLOW_DEL);\n\nerr_free_merge_flow:\n\tnfp_flower_del_linked_merge_flows(app, flow_pay);\n\tif (port)\n\t\tport->tc_offload_cnt--;\n\tkfree(flow_pay->action_data);\n\tkfree(flow_pay->mask_data);\n\tkfree(flow_pay->unmasked_data);\n\tWARN_ON_ONCE(rhashtable_remove_fast(&priv->flow_table,\n\t\t\t\t\t    &flow_pay->fl_node,\n\t\t\t\t\t    nfp_flower_table_params));\n\tkfree_rcu(flow_pay, rcu);\n\treturn err;\n}\n\nstatic int nfp_ct_do_nft_merge(struct nfp_fl_ct_zone_entry *zt,\n\t\t\t       struct nfp_fl_ct_flow_entry *nft_entry,\n\t\t\t       struct nfp_fl_ct_tc_merge *tc_m_entry)\n{\n\tstruct nfp_fl_ct_flow_entry *post_ct_entry, *pre_ct_entry;\n\tstruct nfp_fl_nft_tc_merge *nft_m_entry;\n\tunsigned long new_cookie[3];\n\tint err;\n\n\tpre_ct_entry = tc_m_entry->pre_ct_parent;\n\tpost_ct_entry = tc_m_entry->post_ct_parent;\n\n\terr = nfp_ct_merge_act_check(pre_ct_entry, post_ct_entry, nft_entry);\n\tif (err)\n\t\treturn err;\n\n\t \n\terr = nfp_ct_merge_check(pre_ct_entry, nft_entry);\n\tif (err)\n\t\treturn err;\n\terr = nfp_ct_merge_check(nft_entry, post_ct_entry);\n\tif (err)\n\t\treturn err;\n\terr = nfp_ct_check_meta(post_ct_entry, nft_entry);\n\tif (err)\n\t\treturn err;\n\n\tif (pre_ct_entry->num_prev_m_entries > 0) {\n\t\terr = nfp_ct_merge_extra_check(nft_entry, tc_m_entry);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t \n\tnew_cookie[0] = tc_m_entry->cookie[0];\n\tnew_cookie[1] = tc_m_entry->cookie[1];\n\tnew_cookie[2] = nft_entry->cookie;\n\tnft_m_entry = get_hashentry(&zt->nft_merge_tb,\n\t\t\t\t    &new_cookie,\n\t\t\t\t    nfp_nft_ct_merge_params,\n\t\t\t\t    sizeof(*nft_m_entry));\n\n\tif (IS_ERR(nft_m_entry))\n\t\treturn PTR_ERR(nft_m_entry);\n\n\t \n\tif (!memcmp(&new_cookie, nft_m_entry->cookie, sizeof(new_cookie)))\n\t\treturn 0;\n\n\tmemcpy(&nft_m_entry->cookie, &new_cookie, sizeof(new_cookie));\n\tnft_m_entry->zt = zt;\n\tnft_m_entry->tc_m_parent = tc_m_entry;\n\tnft_m_entry->nft_parent = nft_entry;\n\tnft_m_entry->tc_flower_cookie = 0;\n\t \n\tnft_m_entry->netdev = pre_ct_entry->netdev;\n\n\t \n\tlist_add(&nft_m_entry->tc_merge_list, &tc_m_entry->children);\n\tlist_add(&nft_m_entry->nft_flow_list, &nft_entry->children);\n\n\terr = rhashtable_insert_fast(&zt->nft_merge_tb, &nft_m_entry->hash_node,\n\t\t\t\t     nfp_nft_ct_merge_params);\n\tif (err)\n\t\tgoto err_nft_ct_merge_insert;\n\n\tzt->nft_merge_count++;\n\n\tif (post_ct_entry->goto_chain_index > 0)\n\t\treturn nfp_fl_create_new_pre_ct(nft_m_entry);\n\n\t \n\terr = nfp_fl_ct_add_offload(nft_m_entry);\n\tif (err)\n\t\tgoto err_nft_ct_offload;\n\n\treturn err;\n\nerr_nft_ct_offload:\n\tnfp_fl_ct_del_offload(zt->priv->app, nft_m_entry->tc_flower_cookie,\n\t\t\t      nft_m_entry->netdev);\nerr_nft_ct_merge_insert:\n\tlist_del(&nft_m_entry->tc_merge_list);\n\tlist_del(&nft_m_entry->nft_flow_list);\n\tkfree(nft_m_entry);\n\treturn err;\n}\n\nstatic int nfp_ct_do_tc_merge(struct nfp_fl_ct_zone_entry *zt,\n\t\t\t      struct nfp_fl_ct_flow_entry *ct_entry1,\n\t\t\t      struct nfp_fl_ct_flow_entry *ct_entry2)\n{\n\tstruct nfp_fl_ct_flow_entry *post_ct_entry, *pre_ct_entry;\n\tstruct nfp_fl_ct_flow_entry *nft_entry, *nft_tmp;\n\tstruct nfp_fl_ct_tc_merge *m_entry;\n\tunsigned long new_cookie[2];\n\tint err;\n\n\tif (ct_entry1->type == CT_TYPE_PRE_CT) {\n\t\tpre_ct_entry = ct_entry1;\n\t\tpost_ct_entry = ct_entry2;\n\t} else {\n\t\tpost_ct_entry = ct_entry1;\n\t\tpre_ct_entry = ct_entry2;\n\t}\n\n\t \n\tif (post_ct_entry->chain_index != pre_ct_entry->goto_chain_index)\n\t\treturn -EINVAL;\n\n\terr = nfp_ct_merge_check(pre_ct_entry, post_ct_entry);\n\tif (err)\n\t\treturn err;\n\n\tnew_cookie[0] = pre_ct_entry->cookie;\n\tnew_cookie[1] = post_ct_entry->cookie;\n\tm_entry = get_hashentry(&zt->tc_merge_tb, &new_cookie,\n\t\t\t\tnfp_tc_ct_merge_params, sizeof(*m_entry));\n\tif (IS_ERR(m_entry))\n\t\treturn PTR_ERR(m_entry);\n\n\t \n\tif (!memcmp(&new_cookie, m_entry->cookie, sizeof(new_cookie)))\n\t\treturn 0;\n\n\tmemcpy(&m_entry->cookie, &new_cookie, sizeof(new_cookie));\n\tm_entry->zt = zt;\n\tm_entry->post_ct_parent = post_ct_entry;\n\tm_entry->pre_ct_parent = pre_ct_entry;\n\n\t \n\tlist_add(&m_entry->post_ct_list, &post_ct_entry->children);\n\tlist_add(&m_entry->pre_ct_list, &pre_ct_entry->children);\n\tINIT_LIST_HEAD(&m_entry->children);\n\n\terr = rhashtable_insert_fast(&zt->tc_merge_tb, &m_entry->hash_node,\n\t\t\t\t     nfp_tc_ct_merge_params);\n\tif (err)\n\t\tgoto err_ct_tc_merge_insert;\n\tzt->tc_merge_count++;\n\n\t \n\tlist_for_each_entry_safe(nft_entry, nft_tmp, &zt->nft_flows_list,\n\t\t\t\t list_node) {\n\t\tnfp_ct_do_nft_merge(zt, nft_entry, m_entry);\n\t}\n\n\treturn 0;\n\nerr_ct_tc_merge_insert:\n\tlist_del(&m_entry->post_ct_list);\n\tlist_del(&m_entry->pre_ct_list);\n\tkfree(m_entry);\n\treturn err;\n}\n\nstatic struct\nnfp_fl_ct_zone_entry *get_nfp_zone_entry(struct nfp_flower_priv *priv,\n\t\t\t\t\t u16 zone, bool wildcarded)\n{\n\tstruct nfp_fl_ct_zone_entry *zt;\n\tint err;\n\n\tif (wildcarded && priv->ct_zone_wc)\n\t\treturn priv->ct_zone_wc;\n\n\tif (!wildcarded) {\n\t\tzt = get_hashentry(&priv->ct_zone_table, &zone,\n\t\t\t\t   nfp_zone_table_params, sizeof(*zt));\n\n\t\t \n\t\tif (IS_ERR(zt) || zt->priv)\n\t\t\treturn zt;\n\t} else {\n\t\tzt = kzalloc(sizeof(*zt), GFP_KERNEL);\n\t\tif (!zt)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tzt->zone = zone;\n\tzt->priv = priv;\n\tzt->nft = NULL;\n\n\t \n\tINIT_LIST_HEAD(&zt->pre_ct_list);\n\tINIT_LIST_HEAD(&zt->post_ct_list);\n\tINIT_LIST_HEAD(&zt->nft_flows_list);\n\n\terr = rhashtable_init(&zt->tc_merge_tb, &nfp_tc_ct_merge_params);\n\tif (err)\n\t\tgoto err_tc_merge_tb_init;\n\n\terr = rhashtable_init(&zt->nft_merge_tb, &nfp_nft_ct_merge_params);\n\tif (err)\n\t\tgoto err_nft_merge_tb_init;\n\n\tif (wildcarded) {\n\t\tpriv->ct_zone_wc = zt;\n\t} else {\n\t\terr = rhashtable_insert_fast(&priv->ct_zone_table,\n\t\t\t\t\t     &zt->hash_node,\n\t\t\t\t\t     nfp_zone_table_params);\n\t\tif (err)\n\t\t\tgoto err_zone_insert;\n\t}\n\n\treturn zt;\n\nerr_zone_insert:\n\trhashtable_destroy(&zt->nft_merge_tb);\nerr_nft_merge_tb_init:\n\trhashtable_destroy(&zt->tc_merge_tb);\nerr_tc_merge_tb_init:\n\tkfree(zt);\n\treturn ERR_PTR(err);\n}\n\nstatic struct net_device *get_netdev_from_rule(struct flow_rule *rule)\n{\n\tif (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_META)) {\n\t\tstruct flow_match_meta match;\n\n\t\tflow_rule_match_meta(rule, &match);\n\t\tif (match.key->ingress_ifindex & match.mask->ingress_ifindex)\n\t\t\treturn __dev_get_by_index(&init_net,\n\t\t\t\t\t\t  match.key->ingress_ifindex);\n\t}\n\n\treturn NULL;\n}\n\nstatic void nfp_nft_ct_translate_mangle_action(struct flow_action_entry *mangle_action)\n{\n\tif (mangle_action->id != FLOW_ACTION_MANGLE)\n\t\treturn;\n\n\tswitch (mangle_action->mangle.htype) {\n\tcase FLOW_ACT_MANGLE_HDR_TYPE_IP4:\n\tcase FLOW_ACT_MANGLE_HDR_TYPE_IP6:\n\t\tmangle_action->mangle.val = (__force u32)cpu_to_be32(mangle_action->mangle.val);\n\t\tmangle_action->mangle.mask = (__force u32)cpu_to_be32(mangle_action->mangle.mask);\n\t\treturn;\n\n\tcase FLOW_ACT_MANGLE_HDR_TYPE_TCP:\n\tcase FLOW_ACT_MANGLE_HDR_TYPE_UDP:\n\t\tmangle_action->mangle.val = (__force u16)cpu_to_be16(mangle_action->mangle.val);\n\t\tmangle_action->mangle.mask = (__force u16)cpu_to_be16(mangle_action->mangle.mask);\n\t\treturn;\n\n\tdefault:\n\t\treturn;\n\t}\n}\n\nstatic int nfp_nft_ct_set_flow_flag(struct flow_action_entry *act,\n\t\t\t\t    struct nfp_fl_ct_flow_entry *entry)\n{\n\tswitch (act->id) {\n\tcase FLOW_ACTION_CT:\n\t\tif (act->ct.action == TCA_CT_ACT_NAT)\n\t\t\tentry->flags |= NFP_FL_ACTION_DO_NAT;\n\t\tbreak;\n\n\tcase FLOW_ACTION_MANGLE:\n\t\tentry->flags |= NFP_FL_ACTION_DO_MANGLE;\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic struct\nnfp_fl_ct_flow_entry *nfp_fl_ct_add_flow(struct nfp_fl_ct_zone_entry *zt,\n\t\t\t\t\t struct net_device *netdev,\n\t\t\t\t\t struct flow_cls_offload *flow,\n\t\t\t\t\t bool is_nft, struct netlink_ext_ack *extack)\n{\n\tstruct nf_flow_match *nft_match = NULL;\n\tstruct nfp_fl_ct_flow_entry *entry;\n\tstruct nfp_fl_ct_map_entry *map;\n\tstruct flow_action_entry *act;\n\tint err, i;\n\n\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tentry->rule = flow_rule_alloc(flow->rule->action.num_entries);\n\tif (!entry->rule) {\n\t\terr = -ENOMEM;\n\t\tgoto err_pre_ct_rule;\n\t}\n\n\t \n\tif (is_nft) {\n\t\tnft_match = kzalloc(sizeof(*nft_match), GFP_KERNEL);\n\t\tif (!nft_match) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_pre_ct_act;\n\t\t}\n\t\tmemcpy(&nft_match->dissector, flow->rule->match.dissector,\n\t\t       sizeof(nft_match->dissector));\n\t\tmemcpy(&nft_match->mask, flow->rule->match.mask,\n\t\t       sizeof(nft_match->mask));\n\t\tmemcpy(&nft_match->key, flow->rule->match.key,\n\t\t       sizeof(nft_match->key));\n\t\tentry->rule->match.dissector = &nft_match->dissector;\n\t\tentry->rule->match.mask = &nft_match->mask;\n\t\tentry->rule->match.key = &nft_match->key;\n\n\t\tif (!netdev)\n\t\t\tnetdev = get_netdev_from_rule(entry->rule);\n\t} else {\n\t\tentry->rule->match.dissector = flow->rule->match.dissector;\n\t\tentry->rule->match.mask = flow->rule->match.mask;\n\t\tentry->rule->match.key = flow->rule->match.key;\n\t}\n\n\tentry->zt = zt;\n\tentry->netdev = netdev;\n\tentry->cookie = flow->cookie > 0 ? flow->cookie : (unsigned long)entry;\n\tentry->chain_index = flow->common.chain_index;\n\tentry->tun_offset = NFP_FL_CT_NO_TUN;\n\n\t \n\tentry->rule->action.num_entries = flow->rule->action.num_entries;\n\tflow_action_for_each(i, act, &flow->rule->action) {\n\t\tstruct flow_action_entry *new_act;\n\n\t\tnew_act = &entry->rule->action.entries[i];\n\t\tmemcpy(new_act, act, sizeof(struct flow_action_entry));\n\t\t \n\t\tif (is_nft)\n\t\t\tnfp_nft_ct_translate_mangle_action(new_act);\n\n\t\tnfp_nft_ct_set_flow_flag(new_act, entry);\n\t\t \n\t\tif (act->id == FLOW_ACTION_TUNNEL_ENCAP) {\n\t\t\tstruct ip_tunnel_info *tun = act->tunnel;\n\t\t\tsize_t tun_size = sizeof(*tun) + tun->options_len;\n\n\t\t\tnew_act->tunnel = kmemdup(tun, tun_size, GFP_ATOMIC);\n\t\t\tif (!new_act->tunnel) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto err_pre_ct_tun_cp;\n\t\t\t}\n\t\t\tentry->tun_offset = i;\n\t\t}\n\t}\n\n\tINIT_LIST_HEAD(&entry->children);\n\n\tif (flow->cookie == 0)\n\t\treturn entry;\n\n\t \n\tmap = get_hashentry(&zt->priv->ct_map_table, &flow->cookie,\n\t\t\t    nfp_ct_map_params, sizeof(*map));\n\tif (IS_ERR(map)) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"offload error: ct map entry creation failed\");\n\t\terr = -ENOMEM;\n\t\tgoto err_ct_flow_insert;\n\t}\n\tmap->cookie = flow->cookie;\n\tmap->ct_entry = entry;\n\terr = rhashtable_insert_fast(&zt->priv->ct_map_table,\n\t\t\t\t     &map->hash_node,\n\t\t\t\t     nfp_ct_map_params);\n\tif (err) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"offload error: ct map entry table add failed\");\n\t\tgoto err_map_insert;\n\t}\n\n\treturn entry;\n\nerr_map_insert:\n\tkfree(map);\nerr_ct_flow_insert:\n\tif (entry->tun_offset != NFP_FL_CT_NO_TUN)\n\t\tkfree(entry->rule->action.entries[entry->tun_offset].tunnel);\nerr_pre_ct_tun_cp:\n\tkfree(nft_match);\nerr_pre_ct_act:\n\tkfree(entry->rule);\nerr_pre_ct_rule:\n\tkfree(entry);\n\treturn ERR_PTR(err);\n}\n\nstatic void cleanup_nft_merge_entry(struct nfp_fl_nft_tc_merge *m_entry)\n{\n\tstruct nfp_fl_ct_zone_entry *zt;\n\tint err;\n\n\tzt = m_entry->zt;\n\n\t \n\tif (m_entry->tc_flower_cookie) {\n\t\terr = nfp_fl_ct_del_offload(zt->priv->app, m_entry->tc_flower_cookie,\n\t\t\t\t\t    m_entry->netdev);\n\t\tif (err)\n\t\t\treturn;\n\t}\n\n\tWARN_ON_ONCE(rhashtable_remove_fast(&zt->nft_merge_tb,\n\t\t\t\t\t    &m_entry->hash_node,\n\t\t\t\t\t    nfp_nft_ct_merge_params));\n\tzt->nft_merge_count--;\n\tlist_del(&m_entry->tc_merge_list);\n\tlist_del(&m_entry->nft_flow_list);\n\n\tif (m_entry->next_pre_ct_entry) {\n\t\tstruct nfp_fl_ct_map_entry pre_ct_map_ent;\n\n\t\tpre_ct_map_ent.ct_entry = m_entry->next_pre_ct_entry;\n\t\tpre_ct_map_ent.cookie = 0;\n\t\tnfp_fl_ct_del_flow(&pre_ct_map_ent);\n\t}\n\n\tkfree(m_entry);\n}\n\nstatic void nfp_free_nft_merge_children(void *entry, bool is_nft_flow)\n{\n\tstruct nfp_fl_nft_tc_merge *m_entry, *tmp;\n\n\t \n\n\tif (is_nft_flow) {\n\t\t \n\t\tstruct nfp_fl_ct_flow_entry *ct_entry = entry;\n\n\t\tlist_for_each_entry_safe(m_entry, tmp, &ct_entry->children,\n\t\t\t\t\t nft_flow_list) {\n\t\t\tcleanup_nft_merge_entry(m_entry);\n\t\t}\n\t} else {\n\t\t \n\t\tstruct nfp_fl_ct_tc_merge *ct_entry = entry;\n\n\t\tlist_for_each_entry_safe(m_entry, tmp, &ct_entry->children,\n\t\t\t\t\t tc_merge_list) {\n\t\t\tcleanup_nft_merge_entry(m_entry);\n\t\t}\n\t}\n}\n\nstatic void nfp_del_tc_merge_entry(struct nfp_fl_ct_tc_merge *m_ent)\n{\n\tstruct nfp_fl_ct_zone_entry *zt;\n\tint err;\n\n\tzt = m_ent->zt;\n\terr = rhashtable_remove_fast(&zt->tc_merge_tb,\n\t\t\t\t     &m_ent->hash_node,\n\t\t\t\t     nfp_tc_ct_merge_params);\n\tif (err)\n\t\tpr_warn(\"WARNING: could not remove merge_entry from hashtable\\n\");\n\tzt->tc_merge_count--;\n\tlist_del(&m_ent->post_ct_list);\n\tlist_del(&m_ent->pre_ct_list);\n\n\tif (!list_empty(&m_ent->children))\n\t\tnfp_free_nft_merge_children(m_ent, false);\n\tkfree(m_ent);\n}\n\nstatic void nfp_free_tc_merge_children(struct nfp_fl_ct_flow_entry *entry)\n{\n\tstruct nfp_fl_ct_tc_merge *m_ent, *tmp;\n\n\tswitch (entry->type) {\n\tcase CT_TYPE_PRE_CT:\n\t\tlist_for_each_entry_safe(m_ent, tmp, &entry->children, pre_ct_list) {\n\t\t\tnfp_del_tc_merge_entry(m_ent);\n\t\t}\n\t\tbreak;\n\tcase CT_TYPE_POST_CT:\n\t\tlist_for_each_entry_safe(m_ent, tmp, &entry->children, post_ct_list) {\n\t\t\tnfp_del_tc_merge_entry(m_ent);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nvoid nfp_fl_ct_clean_flow_entry(struct nfp_fl_ct_flow_entry *entry)\n{\n\tlist_del(&entry->list_node);\n\n\tif (!list_empty(&entry->children)) {\n\t\tif (entry->type == CT_TYPE_NFT)\n\t\t\tnfp_free_nft_merge_children(entry, true);\n\t\telse\n\t\t\tnfp_free_tc_merge_children(entry);\n\t}\n\n\tif (entry->tun_offset != NFP_FL_CT_NO_TUN)\n\t\tkfree(entry->rule->action.entries[entry->tun_offset].tunnel);\n\n\tif (entry->type == CT_TYPE_NFT) {\n\t\tstruct nf_flow_match *nft_match;\n\n\t\tnft_match = container_of(entry->rule->match.dissector,\n\t\t\t\t\t struct nf_flow_match, dissector);\n\t\tkfree(nft_match);\n\t}\n\n\tkfree(entry->rule);\n\tkfree(entry);\n}\n\nstatic struct flow_action_entry *get_flow_act_ct(struct flow_rule *rule)\n{\n\tstruct flow_action_entry *act;\n\tint i;\n\n\t \n\tflow_action_for_each(i, act, &rule->action) {\n\t\tif (act->id == FLOW_ACTION_CT && act->ct.action != TCA_CT_ACT_CLEAR)\n\t\t\treturn act;\n\t}\n\n\treturn NULL;\n}\n\nstatic struct flow_action_entry *get_flow_act(struct flow_rule *rule,\n\t\t\t\t\t      enum flow_action_id act_id)\n{\n\tstruct flow_action_entry *act = NULL;\n\tint i;\n\n\tflow_action_for_each(i, act, &rule->action) {\n\t\tif (act->id == act_id)\n\t\t\treturn act;\n\t}\n\treturn NULL;\n}\n\nstatic void\nnfp_ct_merge_tc_entries(struct nfp_fl_ct_flow_entry *ct_entry1,\n\t\t\tstruct nfp_fl_ct_zone_entry *zt_src,\n\t\t\tstruct nfp_fl_ct_zone_entry *zt_dst)\n{\n\tstruct nfp_fl_ct_flow_entry *ct_entry2, *ct_tmp;\n\tstruct list_head *ct_list;\n\n\tif (ct_entry1->type == CT_TYPE_PRE_CT)\n\t\tct_list = &zt_src->post_ct_list;\n\telse if (ct_entry1->type == CT_TYPE_POST_CT)\n\t\tct_list = &zt_src->pre_ct_list;\n\telse\n\t\treturn;\n\n\tlist_for_each_entry_safe(ct_entry2, ct_tmp, ct_list,\n\t\t\t\t list_node) {\n\t\tnfp_ct_do_tc_merge(zt_dst, ct_entry2, ct_entry1);\n\t}\n}\n\nstatic void\nnfp_ct_merge_nft_with_tc(struct nfp_fl_ct_flow_entry *nft_entry,\n\t\t\t struct nfp_fl_ct_zone_entry *zt)\n{\n\tstruct nfp_fl_ct_tc_merge *tc_merge_entry;\n\tstruct rhashtable_iter iter;\n\n\trhashtable_walk_enter(&zt->tc_merge_tb, &iter);\n\trhashtable_walk_start(&iter);\n\twhile ((tc_merge_entry = rhashtable_walk_next(&iter)) != NULL) {\n\t\tif (IS_ERR(tc_merge_entry))\n\t\t\tcontinue;\n\t\trhashtable_walk_stop(&iter);\n\t\tnfp_ct_do_nft_merge(zt, nft_entry, tc_merge_entry);\n\t\trhashtable_walk_start(&iter);\n\t}\n\trhashtable_walk_stop(&iter);\n\trhashtable_walk_exit(&iter);\n}\n\nint nfp_fl_ct_handle_pre_ct(struct nfp_flower_priv *priv,\n\t\t\t    struct net_device *netdev,\n\t\t\t    struct flow_cls_offload *flow,\n\t\t\t    struct netlink_ext_ack *extack,\n\t\t\t    struct nfp_fl_nft_tc_merge *m_entry)\n{\n\tstruct flow_action_entry *ct_act, *ct_goto;\n\tstruct nfp_fl_ct_flow_entry *ct_entry;\n\tstruct nfp_fl_ct_zone_entry *zt;\n\tint err;\n\n\tct_act = get_flow_act_ct(flow->rule);\n\tif (!ct_act) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"unsupported offload: Conntrack action empty in conntrack offload\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tct_goto = get_flow_act(flow->rule, FLOW_ACTION_GOTO);\n\tif (!ct_goto) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"unsupported offload: Conntrack requires ACTION_GOTO\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tzt = get_nfp_zone_entry(priv, ct_act->ct.zone, false);\n\tif (IS_ERR(zt)) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"offload error: Could not create zone table entry\");\n\t\treturn PTR_ERR(zt);\n\t}\n\n\tif (!zt->nft) {\n\t\tzt->nft = ct_act->ct.flow_table;\n\t\terr = nf_flow_table_offload_add_cb(zt->nft, nfp_fl_ct_handle_nft_flow, zt);\n\t\tif (err) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t\t   \"offload error: Could not register nft_callback\");\n\t\t\treturn err;\n\t\t}\n\t}\n\n\t \n\tct_entry = nfp_fl_ct_add_flow(zt, netdev, flow, false, extack);\n\tif (IS_ERR(ct_entry))\n\t\treturn PTR_ERR(ct_entry);\n\tct_entry->type = CT_TYPE_PRE_CT;\n\tct_entry->chain_index = flow->common.chain_index;\n\tct_entry->goto_chain_index = ct_goto->chain_index;\n\n\tif (m_entry) {\n\t\tstruct nfp_fl_ct_flow_entry *pre_ct_entry;\n\t\tint i;\n\n\t\tpre_ct_entry = m_entry->tc_m_parent->pre_ct_parent;\n\t\tfor (i = 0; i < pre_ct_entry->num_prev_m_entries; i++)\n\t\t\tct_entry->prev_m_entries[i] = pre_ct_entry->prev_m_entries[i];\n\t\tct_entry->prev_m_entries[i++] = m_entry;\n\t\tct_entry->num_prev_m_entries = i;\n\n\t\tm_entry->next_pre_ct_entry = ct_entry;\n\t}\n\n\tlist_add(&ct_entry->list_node, &zt->pre_ct_list);\n\tzt->pre_ct_count++;\n\n\tnfp_ct_merge_tc_entries(ct_entry, zt, zt);\n\n\t \n\tif (priv->ct_zone_wc)\n\t\tnfp_ct_merge_tc_entries(ct_entry, priv->ct_zone_wc, zt);\n\n\treturn 0;\n}\n\nint nfp_fl_ct_handle_post_ct(struct nfp_flower_priv *priv,\n\t\t\t     struct net_device *netdev,\n\t\t\t     struct flow_cls_offload *flow,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct flow_rule *rule = flow_cls_offload_flow_rule(flow);\n\tstruct nfp_fl_ct_flow_entry *ct_entry;\n\tstruct nfp_fl_ct_zone_entry *zt;\n\tbool wildcarded = false;\n\tstruct flow_match_ct ct;\n\tstruct flow_action_entry *ct_goto;\n\n\tflow_rule_match_ct(rule, &ct);\n\tif (!ct.mask->ct_zone) {\n\t\twildcarded = true;\n\t} else if (ct.mask->ct_zone != U16_MAX) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"unsupported offload: partially wildcarded ct_zone is not supported\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tzt = get_nfp_zone_entry(priv, ct.key->ct_zone, wildcarded);\n\tif (IS_ERR(zt)) {\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"offload error: Could not create zone table entry\");\n\t\treturn PTR_ERR(zt);\n\t}\n\n\t \n\tct_entry = nfp_fl_ct_add_flow(zt, netdev, flow, false, extack);\n\tif (IS_ERR(ct_entry))\n\t\treturn PTR_ERR(ct_entry);\n\n\tct_entry->type = CT_TYPE_POST_CT;\n\tct_entry->chain_index = flow->common.chain_index;\n\tct_goto = get_flow_act(flow->rule, FLOW_ACTION_GOTO);\n\tct_entry->goto_chain_index = ct_goto ? ct_goto->chain_index : 0;\n\tlist_add(&ct_entry->list_node, &zt->post_ct_list);\n\tzt->post_ct_count++;\n\n\tif (wildcarded) {\n\t\t \n\t\tstruct rhashtable_iter iter;\n\t\tstruct nfp_fl_ct_zone_entry *zone_table;\n\n\t\trhashtable_walk_enter(&priv->ct_zone_table, &iter);\n\t\trhashtable_walk_start(&iter);\n\t\twhile ((zone_table = rhashtable_walk_next(&iter)) != NULL) {\n\t\t\tif (IS_ERR(zone_table))\n\t\t\t\tcontinue;\n\t\t\trhashtable_walk_stop(&iter);\n\t\t\tnfp_ct_merge_tc_entries(ct_entry, zone_table, zone_table);\n\t\t\trhashtable_walk_start(&iter);\n\t\t}\n\t\trhashtable_walk_stop(&iter);\n\t\trhashtable_walk_exit(&iter);\n\t} else {\n\t\tnfp_ct_merge_tc_entries(ct_entry, zt, zt);\n\t}\n\n\treturn 0;\n}\n\nint nfp_fl_create_new_pre_ct(struct nfp_fl_nft_tc_merge *m_entry)\n{\n\tstruct nfp_fl_ct_flow_entry *pre_ct_entry, *post_ct_entry;\n\tstruct flow_cls_offload new_pre_ct_flow;\n\tint err;\n\n\tpre_ct_entry = m_entry->tc_m_parent->pre_ct_parent;\n\tif (pre_ct_entry->num_prev_m_entries >= NFP_MAX_RECIRC_CT_ZONES - 1)\n\t\treturn -1;\n\n\tpost_ct_entry = m_entry->tc_m_parent->post_ct_parent;\n\tmemset(&new_pre_ct_flow, 0, sizeof(struct flow_cls_offload));\n\tnew_pre_ct_flow.rule = post_ct_entry->rule;\n\tnew_pre_ct_flow.common.chain_index = post_ct_entry->chain_index;\n\n\terr = nfp_fl_ct_handle_pre_ct(pre_ct_entry->zt->priv,\n\t\t\t\t      pre_ct_entry->netdev,\n\t\t\t\t      &new_pre_ct_flow, NULL,\n\t\t\t\t      m_entry);\n\treturn err;\n}\n\nstatic void\nnfp_fl_ct_sub_stats(struct nfp_fl_nft_tc_merge *nft_merge,\n\t\t    enum ct_entry_type type, u64 *m_pkts,\n\t\t    u64 *m_bytes, u64 *m_used)\n{\n\tstruct nfp_flower_priv *priv = nft_merge->zt->priv;\n\tstruct nfp_fl_payload *nfp_flow;\n\tu32 ctx_id;\n\n\tnfp_flow = nft_merge->flow_pay;\n\tif (!nfp_flow)\n\t\treturn;\n\n\tctx_id = be32_to_cpu(nfp_flow->meta.host_ctx_id);\n\t*m_pkts += priv->stats[ctx_id].pkts;\n\t*m_bytes += priv->stats[ctx_id].bytes;\n\t*m_used = max_t(u64, *m_used, priv->stats[ctx_id].used);\n\n\t \n\tif (!list_empty(&nfp_flow->linked_flows))\n\t\tnfp_flower_update_merge_stats(priv->app, nfp_flow);\n\n\tif (type != CT_TYPE_NFT) {\n\t\t \n\t\tflow_stats_update(&nft_merge->nft_parent->stats,\n\t\t\t\t  priv->stats[ctx_id].bytes,\n\t\t\t\t  priv->stats[ctx_id].pkts,\n\t\t\t\t  0, priv->stats[ctx_id].used,\n\t\t\t\t  FLOW_ACTION_HW_STATS_DELAYED);\n\t} else {\n\t\t \n\t\tflow_stats_update(&nft_merge->tc_m_parent->pre_ct_parent->stats,\n\t\t\t\t  priv->stats[ctx_id].bytes,\n\t\t\t\t  priv->stats[ctx_id].pkts,\n\t\t\t\t  0, priv->stats[ctx_id].used,\n\t\t\t\t  FLOW_ACTION_HW_STATS_DELAYED);\n\t\t \n\t\tflow_stats_update(&nft_merge->tc_m_parent->post_ct_parent->stats,\n\t\t\t\t  priv->stats[ctx_id].bytes,\n\t\t\t\t  priv->stats[ctx_id].pkts,\n\t\t\t\t  0, priv->stats[ctx_id].used,\n\t\t\t\t  FLOW_ACTION_HW_STATS_DELAYED);\n\t}\n\n\t \n\tif (nft_merge->tc_m_parent->pre_ct_parent->num_prev_m_entries > 0) {\n\t\tstruct nfp_fl_nft_tc_merge *tmp_nft_merge;\n\t\tint i;\n\n\t\tfor (i = 0; i < nft_merge->tc_m_parent->pre_ct_parent->num_prev_m_entries; i++) {\n\t\t\ttmp_nft_merge = nft_merge->tc_m_parent->pre_ct_parent->prev_m_entries[i];\n\t\t\tflow_stats_update(&tmp_nft_merge->tc_m_parent->pre_ct_parent->stats,\n\t\t\t\t\t  priv->stats[ctx_id].bytes,\n\t\t\t\t\t  priv->stats[ctx_id].pkts,\n\t\t\t\t\t  0, priv->stats[ctx_id].used,\n\t\t\t\t\t  FLOW_ACTION_HW_STATS_DELAYED);\n\t\t\tflow_stats_update(&tmp_nft_merge->tc_m_parent->post_ct_parent->stats,\n\t\t\t\t\t  priv->stats[ctx_id].bytes,\n\t\t\t\t\t  priv->stats[ctx_id].pkts,\n\t\t\t\t\t  0, priv->stats[ctx_id].used,\n\t\t\t\t\t  FLOW_ACTION_HW_STATS_DELAYED);\n\t\t\tflow_stats_update(&tmp_nft_merge->nft_parent->stats,\n\t\t\t\t\t  priv->stats[ctx_id].bytes,\n\t\t\t\t\t  priv->stats[ctx_id].pkts,\n\t\t\t\t\t  0, priv->stats[ctx_id].used,\n\t\t\t\t\t  FLOW_ACTION_HW_STATS_DELAYED);\n\t\t}\n\t}\n\n\t \n\tpriv->stats[ctx_id].pkts = 0;\n\tpriv->stats[ctx_id].bytes = 0;\n}\n\nint nfp_fl_ct_stats(struct flow_cls_offload *flow,\n\t\t    struct nfp_fl_ct_map_entry *ct_map_ent)\n{\n\tstruct nfp_fl_ct_flow_entry *ct_entry = ct_map_ent->ct_entry;\n\tstruct nfp_fl_nft_tc_merge *nft_merge, *nft_m_tmp;\n\tstruct nfp_fl_ct_tc_merge *tc_merge, *tc_m_tmp;\n\n\tu64 pkts = 0, bytes = 0, used = 0;\n\tu64 m_pkts, m_bytes, m_used;\n\n\tspin_lock_bh(&ct_entry->zt->priv->stats_lock);\n\n\tif (ct_entry->type == CT_TYPE_PRE_CT) {\n\t\t \n\t\tlist_for_each_entry_safe(tc_merge, tc_m_tmp, &ct_entry->children,\n\t\t\t\t\t pre_ct_list) {\n\t\t\tm_pkts = 0;\n\t\t\tm_bytes = 0;\n\t\t\tm_used = 0;\n\t\t\t \n\t\t\tlist_for_each_entry_safe(nft_merge, nft_m_tmp, &tc_merge->children,\n\t\t\t\t\t\t tc_merge_list) {\n\t\t\t\tnfp_fl_ct_sub_stats(nft_merge, CT_TYPE_PRE_CT,\n\t\t\t\t\t\t    &m_pkts, &m_bytes, &m_used);\n\t\t\t}\n\t\t\tpkts += m_pkts;\n\t\t\tbytes += m_bytes;\n\t\t\tused = max_t(u64, used, m_used);\n\t\t\t \n\t\t\tflow_stats_update(&tc_merge->post_ct_parent->stats,\n\t\t\t\t\t  m_bytes, m_pkts, 0, m_used,\n\t\t\t\t\t  FLOW_ACTION_HW_STATS_DELAYED);\n\t\t}\n\t} else if (ct_entry->type == CT_TYPE_POST_CT) {\n\t\t \n\t\tlist_for_each_entry_safe(tc_merge, tc_m_tmp, &ct_entry->children,\n\t\t\t\t\t post_ct_list) {\n\t\t\tm_pkts = 0;\n\t\t\tm_bytes = 0;\n\t\t\tm_used = 0;\n\t\t\t \n\t\t\tlist_for_each_entry_safe(nft_merge, nft_m_tmp, &tc_merge->children,\n\t\t\t\t\t\t tc_merge_list) {\n\t\t\t\tnfp_fl_ct_sub_stats(nft_merge, CT_TYPE_POST_CT,\n\t\t\t\t\t\t    &m_pkts, &m_bytes, &m_used);\n\t\t\t}\n\t\t\tpkts += m_pkts;\n\t\t\tbytes += m_bytes;\n\t\t\tused = max_t(u64, used, m_used);\n\t\t\t \n\t\t\tflow_stats_update(&tc_merge->pre_ct_parent->stats,\n\t\t\t\t\t  m_bytes, m_pkts, 0, m_used,\n\t\t\t\t\t  FLOW_ACTION_HW_STATS_DELAYED);\n\t\t}\n\t} else  {\n\t\t \n\t\tlist_for_each_entry_safe(nft_merge, nft_m_tmp, &ct_entry->children,\n\t\t\t\t\t nft_flow_list) {\n\t\t\tnfp_fl_ct_sub_stats(nft_merge, CT_TYPE_NFT,\n\t\t\t\t\t    &pkts, &bytes, &used);\n\t\t}\n\t}\n\n\t \n\tflow_stats_update(&ct_entry->stats, bytes, pkts, 0, used,\n\t\t\t  FLOW_ACTION_HW_STATS_DELAYED);\n\t \n\tflow_stats_update(&flow->stats, ct_entry->stats.bytes,\n\t\t\t  ct_entry->stats.pkts, 0,\n\t\t\t  ct_entry->stats.lastused,\n\t\t\t  FLOW_ACTION_HW_STATS_DELAYED);\n\t \n\tct_entry->stats.pkts = 0;\n\tct_entry->stats.bytes = 0;\n\tspin_unlock_bh(&ct_entry->zt->priv->stats_lock);\n\n\treturn 0;\n}\n\nstatic bool\nnfp_fl_ct_offload_nft_supported(struct flow_cls_offload *flow)\n{\n\tstruct flow_rule *flow_rule = flow->rule;\n\tstruct flow_action *flow_action =\n\t\t&flow_rule->action;\n\tstruct flow_action_entry *act;\n\tint i;\n\n\tflow_action_for_each(i, act, flow_action) {\n\t\tif (act->id == FLOW_ACTION_CT_METADATA) {\n\t\t\tenum ip_conntrack_info ctinfo =\n\t\t\t\tact->ct_metadata.cookie & NFCT_INFOMASK;\n\n\t\t\treturn ctinfo != IP_CT_NEW;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic int\nnfp_fl_ct_offload_nft_flow(struct nfp_fl_ct_zone_entry *zt, struct flow_cls_offload *flow)\n{\n\tstruct nfp_fl_ct_map_entry *ct_map_ent;\n\tstruct nfp_fl_ct_flow_entry *ct_entry;\n\tstruct netlink_ext_ack *extack = NULL;\n\n\textack = flow->common.extack;\n\tswitch (flow->command) {\n\tcase FLOW_CLS_REPLACE:\n\t\tif (!nfp_fl_ct_offload_nft_supported(flow))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\t \n\t\tct_map_ent = rhashtable_lookup_fast(&zt->priv->ct_map_table, &flow->cookie,\n\t\t\t\t\t\t    nfp_ct_map_params);\n\t\tif (!ct_map_ent) {\n\t\t\tct_entry = nfp_fl_ct_add_flow(zt, NULL, flow, true, extack);\n\t\t\tif (IS_ERR(ct_entry))\n\t\t\t\treturn PTR_ERR(ct_entry);\n\t\t\tct_entry->type = CT_TYPE_NFT;\n\t\t\tlist_add(&ct_entry->list_node, &zt->nft_flows_list);\n\t\t\tzt->nft_flows_count++;\n\t\t\tnfp_ct_merge_nft_with_tc(ct_entry, zt);\n\t\t}\n\t\treturn 0;\n\tcase FLOW_CLS_DESTROY:\n\t\tct_map_ent = rhashtable_lookup_fast(&zt->priv->ct_map_table, &flow->cookie,\n\t\t\t\t\t\t    nfp_ct_map_params);\n\t\treturn nfp_fl_ct_del_flow(ct_map_ent);\n\tcase FLOW_CLS_STATS:\n\t\tct_map_ent = rhashtable_lookup_fast(&zt->priv->ct_map_table, &flow->cookie,\n\t\t\t\t\t\t    nfp_ct_map_params);\n\t\tif (ct_map_ent)\n\t\t\treturn nfp_fl_ct_stats(flow, ct_map_ent);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn -EINVAL;\n}\n\nint nfp_fl_ct_handle_nft_flow(enum tc_setup_type type, void *type_data, void *cb_priv)\n{\n\tstruct flow_cls_offload *flow = type_data;\n\tstruct nfp_fl_ct_zone_entry *zt = cb_priv;\n\tint err = -EOPNOTSUPP;\n\n\tswitch (type) {\n\tcase TC_SETUP_CLSFLOWER:\n\t\twhile (!mutex_trylock(&zt->priv->nfp_fl_lock)) {\n\t\t\tif (!zt->nft)  \n\t\t\t\treturn err;\n\t\t\tmsleep(20);\n\t\t}\n\t\terr = nfp_fl_ct_offload_nft_flow(zt, flow);\n\t\tmutex_unlock(&zt->priv->nfp_fl_lock);\n\t\tbreak;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n\treturn err;\n}\n\nstatic void\nnfp_fl_ct_clean_nft_entries(struct nfp_fl_ct_zone_entry *zt)\n{\n\tstruct nfp_fl_ct_flow_entry *nft_entry, *ct_tmp;\n\tstruct nfp_fl_ct_map_entry *ct_map_ent;\n\n\tlist_for_each_entry_safe(nft_entry, ct_tmp, &zt->nft_flows_list,\n\t\t\t\t list_node) {\n\t\tct_map_ent = rhashtable_lookup_fast(&zt->priv->ct_map_table,\n\t\t\t\t\t\t    &nft_entry->cookie,\n\t\t\t\t\t\t    nfp_ct_map_params);\n\t\tnfp_fl_ct_del_flow(ct_map_ent);\n\t}\n}\n\nint nfp_fl_ct_del_flow(struct nfp_fl_ct_map_entry *ct_map_ent)\n{\n\tstruct nfp_fl_ct_flow_entry *ct_entry;\n\tstruct nfp_fl_ct_zone_entry *zt;\n\tstruct rhashtable *m_table;\n\tstruct nf_flowtable *nft;\n\n\tif (!ct_map_ent)\n\t\treturn -ENOENT;\n\n\tzt = ct_map_ent->ct_entry->zt;\n\tct_entry = ct_map_ent->ct_entry;\n\tm_table = &zt->priv->ct_map_table;\n\n\tswitch (ct_entry->type) {\n\tcase CT_TYPE_PRE_CT:\n\t\tzt->pre_ct_count--;\n\t\tif (ct_map_ent->cookie > 0)\n\t\t\trhashtable_remove_fast(m_table, &ct_map_ent->hash_node,\n\t\t\t\t\t       nfp_ct_map_params);\n\t\tnfp_fl_ct_clean_flow_entry(ct_entry);\n\t\tif (ct_map_ent->cookie > 0)\n\t\t\tkfree(ct_map_ent);\n\n\t\tif (!zt->pre_ct_count && zt->nft) {\n\t\t\tnft = zt->nft;\n\t\t\tzt->nft = NULL;  \n\t\t\tnf_flow_table_offload_del_cb(nft,\n\t\t\t\t\t\t     nfp_fl_ct_handle_nft_flow,\n\t\t\t\t\t\t     zt);\n\t\t\tnfp_fl_ct_clean_nft_entries(zt);\n\t\t}\n\t\tbreak;\n\tcase CT_TYPE_POST_CT:\n\t\tzt->post_ct_count--;\n\t\trhashtable_remove_fast(m_table, &ct_map_ent->hash_node,\n\t\t\t\t       nfp_ct_map_params);\n\t\tnfp_fl_ct_clean_flow_entry(ct_entry);\n\t\tkfree(ct_map_ent);\n\t\tbreak;\n\tcase CT_TYPE_NFT:\n\t\tzt->nft_flows_count--;\n\t\trhashtable_remove_fast(m_table, &ct_map_ent->hash_node,\n\t\t\t\t       nfp_ct_map_params);\n\t\tnfp_fl_ct_clean_flow_entry(ct_map_ent->ct_entry);\n\t\tkfree(ct_map_ent);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}