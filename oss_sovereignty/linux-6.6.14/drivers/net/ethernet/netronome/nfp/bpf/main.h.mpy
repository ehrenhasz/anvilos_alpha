{
  "module_name": "main.h",
  "hash_id": "3943dd1d603e8d8ba49e9e060b75cd5b60884dd2e7468b33cf39cbafd4b15e7c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/netronome/nfp/bpf/main.h",
  "human_readable_source": " \n \n\n#ifndef __NFP_BPF_H__\n#define __NFP_BPF_H__ 1\n\n#include <linux/bitfield.h>\n#include <linux/bpf.h>\n#include <linux/bpf_verifier.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/rhashtable.h>\n#include <linux/skbuff.h>\n#include <linux/types.h>\n#include <linux/wait.h>\n\n#include \"../ccm.h\"\n#include \"../nfp_asm.h\"\n#include \"fw.h\"\n\n#define cmsg_warn(bpf, msg...)\tnn_dp_warn(&(bpf)->app->ctrl->dp, msg)\n\n \n#define OP_RELO_TYPE\t0xff00000000000000ULL\n\nenum nfp_relo_type {\n\tRELO_NONE = 0,\n\t \n\tRELO_BR_REL,\n\t \n\tRELO_BR_GO_OUT,\n\tRELO_BR_GO_ABORT,\n\tRELO_BR_GO_CALL_PUSH_REGS,\n\tRELO_BR_GO_CALL_POP_REGS,\n\t \n\tRELO_BR_NEXT_PKT,\n\tRELO_BR_HELPER,\n\t \n\tRELO_IMMED_REL,\n};\n\n \n#define BR_OFF_RELO\t\t15000\n\nenum static_regs {\n\tSTATIC_REG_IMMA\t\t= 20,  \n\tSTATIC_REG_IMM\t\t= 21,  \n\tSTATIC_REG_STACK\t= 22,  \n\tSTATIC_REG_PKT_LEN\t= 22,  \n};\n\nenum pkt_vec {\n\tPKT_VEC_PKT_LEN\t\t= 0,\n\tPKT_VEC_PKT_PTR\t\t= 2,\n\tPKT_VEC_QSEL_SET\t= 4,\n\tPKT_VEC_QSEL_VAL\t= 6,\n};\n\n#define PKT_VEL_QSEL_SET_BIT\t4\n\n#define pv_len(np)\treg_lm(1, PKT_VEC_PKT_LEN)\n#define pv_ctm_ptr(np)\treg_lm(1, PKT_VEC_PKT_PTR)\n#define pv_qsel_set(np)\treg_lm(1, PKT_VEC_QSEL_SET)\n#define pv_qsel_val(np)\treg_lm(1, PKT_VEC_QSEL_VAL)\n\n#define stack_reg(np)\treg_a(STATIC_REG_STACK)\n#define stack_imm(np)\timm_b(np)\n#define plen_reg(np)\treg_b(STATIC_REG_PKT_LEN)\n#define pptr_reg(np)\tpv_ctm_ptr(np)\n#define imm_a(np)\treg_a(STATIC_REG_IMM)\n#define imm_b(np)\treg_b(STATIC_REG_IMM)\n#define imma_a(np)\treg_a(STATIC_REG_IMMA)\n#define imma_b(np)\treg_b(STATIC_REG_IMMA)\n#define imm_both(np)\treg_both(STATIC_REG_IMM)\n#define ret_reg(np)\timm_a(np)\n\n#define NFP_BPF_ABI_FLAGS\treg_imm(0)\n#define   NFP_BPF_ABI_FLAG_MARK\t1\n\n \nstruct nfp_app_bpf {\n\tstruct nfp_app *app;\n\tstruct nfp_ccm ccm;\n\n\tstruct bpf_offload_dev *bpf_dev;\n\n\tunsigned int cmsg_key_sz;\n\tunsigned int cmsg_val_sz;\n\n\tunsigned int cmsg_cache_cnt;\n\n\tstruct list_head map_list;\n\tunsigned int maps_in_use;\n\tunsigned int map_elems_in_use;\n\n\tstruct rhashtable maps_neutral;\n\n\tu32 abi_version;\n\n\tstruct nfp_bpf_cap_adjust_head {\n\t\tu32 flags;\n\t\tint off_min;\n\t\tint off_max;\n\t\tint guaranteed_sub;\n\t\tint guaranteed_add;\n\t} adjust_head;\n\n\tstruct {\n\t\tu32 types;\n\t\tu32 max_maps;\n\t\tu32 max_elems;\n\t\tu32 max_key_sz;\n\t\tu32 max_val_sz;\n\t\tu32 max_elem_sz;\n\t} maps;\n\n\tstruct {\n\t\tu32 map_lookup;\n\t\tu32 map_update;\n\t\tu32 map_delete;\n\t\tu32 perf_event_output;\n\t} helpers;\n\n\tbool pseudo_random;\n\tbool queue_select;\n\tbool adjust_tail;\n\tbool cmsg_multi_ent;\n};\n\nenum nfp_bpf_map_use {\n\tNFP_MAP_UNUSED = 0,\n\tNFP_MAP_USE_READ,\n\tNFP_MAP_USE_WRITE,\n\tNFP_MAP_USE_ATOMIC_CNT,\n};\n\nstruct nfp_bpf_map_word {\n\tunsigned char type\t\t:4;\n\tunsigned char non_zero_update\t:1;\n};\n\n#define NFP_BPF_MAP_CACHE_CNT\t\t4U\n#define NFP_BPF_MAP_CACHE_TIME_NS\t(250 * 1000)\n\n \nstruct nfp_bpf_map {\n\tstruct bpf_offloaded_map *offmap;\n\tstruct nfp_app_bpf *bpf;\n\tu32 tid;\n\n\tspinlock_t cache_lock;\n\tu32 cache_blockers;\n\tu32 cache_gen;\n\tu64 cache_to;\n\tstruct sk_buff *cache;\n\n\tstruct list_head l;\n\tstruct nfp_bpf_map_word use_map[];\n};\n\nstruct nfp_bpf_neutral_map {\n\tstruct rhash_head l;\n\tstruct bpf_map *ptr;\n\tu32 map_id;\n\tu32 count;\n};\n\nextern const struct rhashtable_params nfp_bpf_maps_neutral_params;\n\nstruct nfp_prog;\nstruct nfp_insn_meta;\ntypedef int (*instr_cb_t)(struct nfp_prog *, struct nfp_insn_meta *);\n\n#define nfp_prog_first_meta(nfp_prog)\t\t\t\t\t\\\n\tlist_first_entry(&(nfp_prog)->insns, struct nfp_insn_meta, l)\n#define nfp_prog_last_meta(nfp_prog)\t\t\t\t\t\\\n\tlist_last_entry(&(nfp_prog)->insns, struct nfp_insn_meta, l)\n#define nfp_meta_next(meta)\tlist_next_entry(meta, l)\n#define nfp_meta_prev(meta)\tlist_prev_entry(meta, l)\n\n \nstruct nfp_bpf_reg_state {\n\tstruct bpf_reg_state reg;\n\tbool var_off;\n};\n\n#define FLAG_INSN_IS_JUMP_DST\t\t\tBIT(0)\n#define FLAG_INSN_IS_SUBPROG_START\t\tBIT(1)\n#define FLAG_INSN_PTR_CALLER_STACK_FRAME\tBIT(2)\n \n#define FLAG_INSN_SKIP_NOOP\t\t\tBIT(3)\n \n#define FLAG_INSN_SKIP_PREC_DEPENDENT\t\tBIT(4)\n \n#define FLAG_INSN_SKIP_VERIFIER_OPT\t\tBIT(5)\n \n#define FLAG_INSN_DO_ZEXT\t\t\tBIT(6)\n\n#define FLAG_INSN_SKIP_MASK\t\t(FLAG_INSN_SKIP_NOOP | \\\n\t\t\t\t\t FLAG_INSN_SKIP_PREC_DEPENDENT | \\\n\t\t\t\t\t FLAG_INSN_SKIP_VERIFIER_OPT)\n\n \nstruct nfp_insn_meta {\n\tstruct bpf_insn insn;\n\tunion {\n\t\t \n\t\tstruct {\n\t\t\tstruct bpf_reg_state ptr;\n\t\t\tstruct bpf_insn *paired_st;\n\t\t\ts16 ldst_gather_len;\n\t\t\tbool ptr_not_const;\n\t\t\tstruct {\n\t\t\t\ts16 range_start;\n\t\t\t\ts16 range_end;\n\t\t\t\tbool do_init;\n\t\t\t} pkt_cache;\n\t\t\tbool xadd_over_16bit;\n\t\t\tbool xadd_maybe_16bit;\n\t\t};\n\t\t \n\t\tstruct {\n\t\t\tstruct nfp_insn_meta *jmp_dst;\n\t\t\tbool jump_neg_op;\n\t\t\tu32 num_insns_after_br;  \n\t\t};\n\t\t \n\t\tstruct {\n\t\t\tu32 func_id;\n\t\t\tstruct bpf_reg_state arg1;\n\t\t\tstruct nfp_bpf_reg_state arg2;\n\t\t};\n\t\t \n\t\tstruct {\n\t\t\tu64 umin_src;\n\t\t\tu64 umax_src;\n\t\t\tu64 umin_dst;\n\t\t\tu64 umax_dst;\n\t\t};\n\t};\n\tunsigned int off;\n\tunsigned short n;\n\tunsigned short flags;\n\tunsigned short subprog_idx;\n\tinstr_cb_t double_cb;\n\n\tstruct list_head l;\n};\n\n#define BPF_SIZE_MASK\t0x18\n\nstatic inline u8 mbpf_class(const struct nfp_insn_meta *meta)\n{\n\treturn BPF_CLASS(meta->insn.code);\n}\n\nstatic inline u8 mbpf_src(const struct nfp_insn_meta *meta)\n{\n\treturn BPF_SRC(meta->insn.code);\n}\n\nstatic inline u8 mbpf_op(const struct nfp_insn_meta *meta)\n{\n\treturn BPF_OP(meta->insn.code);\n}\n\nstatic inline u8 mbpf_mode(const struct nfp_insn_meta *meta)\n{\n\treturn BPF_MODE(meta->insn.code);\n}\n\nstatic inline bool is_mbpf_alu(const struct nfp_insn_meta *meta)\n{\n\treturn mbpf_class(meta) == BPF_ALU64 || mbpf_class(meta) == BPF_ALU;\n}\n\nstatic inline bool is_mbpf_load(const struct nfp_insn_meta *meta)\n{\n\treturn (meta->insn.code & ~BPF_SIZE_MASK) == (BPF_LDX | BPF_MEM);\n}\n\nstatic inline bool is_mbpf_jmp32(const struct nfp_insn_meta *meta)\n{\n\treturn mbpf_class(meta) == BPF_JMP32;\n}\n\nstatic inline bool is_mbpf_jmp64(const struct nfp_insn_meta *meta)\n{\n\treturn mbpf_class(meta) == BPF_JMP;\n}\n\nstatic inline bool is_mbpf_jmp(const struct nfp_insn_meta *meta)\n{\n\treturn is_mbpf_jmp32(meta) || is_mbpf_jmp64(meta);\n}\n\nstatic inline bool is_mbpf_store(const struct nfp_insn_meta *meta)\n{\n\treturn (meta->insn.code & ~BPF_SIZE_MASK) == (BPF_STX | BPF_MEM);\n}\n\nstatic inline bool is_mbpf_load_pkt(const struct nfp_insn_meta *meta)\n{\n\treturn is_mbpf_load(meta) && meta->ptr.type == PTR_TO_PACKET;\n}\n\nstatic inline bool is_mbpf_store_pkt(const struct nfp_insn_meta *meta)\n{\n\treturn is_mbpf_store(meta) && meta->ptr.type == PTR_TO_PACKET;\n}\n\nstatic inline bool is_mbpf_classic_load(const struct nfp_insn_meta *meta)\n{\n\tu8 code = meta->insn.code;\n\n\treturn BPF_CLASS(code) == BPF_LD &&\n\t       (BPF_MODE(code) == BPF_ABS || BPF_MODE(code) == BPF_IND);\n}\n\nstatic inline bool is_mbpf_classic_store(const struct nfp_insn_meta *meta)\n{\n\tu8 code = meta->insn.code;\n\n\treturn BPF_CLASS(code) == BPF_ST && BPF_MODE(code) == BPF_MEM;\n}\n\nstatic inline bool is_mbpf_classic_store_pkt(const struct nfp_insn_meta *meta)\n{\n\treturn is_mbpf_classic_store(meta) && meta->ptr.type == PTR_TO_PACKET;\n}\n\nstatic inline bool is_mbpf_atomic(const struct nfp_insn_meta *meta)\n{\n\treturn (meta->insn.code & ~BPF_SIZE_MASK) == (BPF_STX | BPF_ATOMIC);\n}\n\nstatic inline bool is_mbpf_mul(const struct nfp_insn_meta *meta)\n{\n\treturn is_mbpf_alu(meta) && mbpf_op(meta) == BPF_MUL;\n}\n\nstatic inline bool is_mbpf_div(const struct nfp_insn_meta *meta)\n{\n\treturn is_mbpf_alu(meta) && mbpf_op(meta) == BPF_DIV;\n}\n\nstatic inline bool is_mbpf_cond_jump(const struct nfp_insn_meta *meta)\n{\n\tu8 op;\n\n\tif (is_mbpf_jmp32(meta))\n\t\treturn true;\n\n\tif (!is_mbpf_jmp64(meta))\n\t\treturn false;\n\n\top = mbpf_op(meta);\n\treturn op != BPF_JA && op != BPF_EXIT && op != BPF_CALL;\n}\n\nstatic inline bool is_mbpf_helper_call(const struct nfp_insn_meta *meta)\n{\n\tstruct bpf_insn insn = meta->insn;\n\n\treturn insn.code == (BPF_JMP | BPF_CALL) &&\n\t\tinsn.src_reg != BPF_PSEUDO_CALL;\n}\n\nstatic inline bool is_mbpf_pseudo_call(const struct nfp_insn_meta *meta)\n{\n\tstruct bpf_insn insn = meta->insn;\n\n\treturn insn.code == (BPF_JMP | BPF_CALL) &&\n\t\tinsn.src_reg == BPF_PSEUDO_CALL;\n}\n\n#define STACK_FRAME_ALIGN 64\n\n \nstruct nfp_bpf_subprog_info {\n\tu16 stack_depth;\n\tu8 needs_reg_push : 1;\n};\n\n \nstruct nfp_prog {\n\tstruct nfp_app_bpf *bpf;\n\n\tu64 *prog;\n\tunsigned int prog_len;\n\tunsigned int __prog_alloc_len;\n\n\tunsigned int stack_size;\n\n\tstruct nfp_insn_meta *verifier_meta;\n\n\tenum bpf_prog_type type;\n\n\tunsigned int last_bpf_off;\n\tunsigned int tgt_out;\n\tunsigned int tgt_abort;\n\tunsigned int tgt_call_push_regs;\n\tunsigned int tgt_call_pop_regs;\n\n\tunsigned int n_translated;\n\tint error;\n\n\tunsigned int stack_frame_depth;\n\tunsigned int adjust_head_location;\n\n\tunsigned int map_records_cnt;\n\tunsigned int subprog_cnt;\n\tstruct nfp_bpf_neutral_map **map_records;\n\tstruct nfp_bpf_subprog_info *subprog;\n\n\tunsigned int n_insns;\n\tstruct list_head insns;\n};\n\n \nstruct nfp_bpf_vnic {\n\tstruct bpf_prog *tc_prog;\n\tunsigned int start_off;\n\tunsigned int tgt_done;\n};\n\nbool nfp_is_subprog_start(struct nfp_insn_meta *meta);\nvoid nfp_bpf_jit_prepare(struct nfp_prog *nfp_prog);\nint nfp_bpf_jit(struct nfp_prog *prog);\nbool nfp_bpf_supported_opcode(u8 code);\nbool nfp_bpf_offload_check_mtu(struct nfp_net *nn, struct bpf_prog *prog,\n\t\t\t       unsigned int mtu);\n\nint nfp_verify_insn(struct bpf_verifier_env *env, int insn_idx,\n\t\t    int prev_insn_idx);\nint nfp_bpf_finalize(struct bpf_verifier_env *env);\n\nint nfp_bpf_opt_replace_insn(struct bpf_verifier_env *env, u32 off,\n\t\t\t     struct bpf_insn *insn);\nint nfp_bpf_opt_remove_insns(struct bpf_verifier_env *env, u32 off, u32 cnt);\n\nextern const struct bpf_prog_offload_ops nfp_bpf_dev_ops;\n\nstruct netdev_bpf;\nstruct nfp_app;\nstruct nfp_net;\n\nint nfp_ndo_bpf(struct nfp_app *app, struct nfp_net *nn,\n\t\tstruct netdev_bpf *bpf);\nint nfp_net_bpf_offload(struct nfp_net *nn, struct bpf_prog *prog,\n\t\t\tbool old_prog, struct netlink_ext_ack *extack);\n\nstruct nfp_insn_meta *\nnfp_bpf_goto_meta(struct nfp_prog *nfp_prog, struct nfp_insn_meta *meta,\n\t\t  unsigned int insn_idx);\n\nvoid *nfp_bpf_relo_for_vnic(struct nfp_prog *nfp_prog, struct nfp_bpf_vnic *bv);\n\nunsigned int nfp_bpf_ctrl_cmsg_min_mtu(struct nfp_app_bpf *bpf);\nunsigned int nfp_bpf_ctrl_cmsg_mtu(struct nfp_app_bpf *bpf);\nunsigned int nfp_bpf_ctrl_cmsg_cache_cnt(struct nfp_app_bpf *bpf);\nlong long int\nnfp_bpf_ctrl_alloc_map(struct nfp_app_bpf *bpf, struct bpf_map *map);\nvoid\nnfp_bpf_ctrl_free_map(struct nfp_app_bpf *bpf, struct nfp_bpf_map *nfp_map);\nint nfp_bpf_ctrl_getfirst_entry(struct bpf_offloaded_map *offmap,\n\t\t\t\tvoid *next_key);\nint nfp_bpf_ctrl_update_entry(struct bpf_offloaded_map *offmap,\n\t\t\t      void *key, void *value, u64 flags);\nint nfp_bpf_ctrl_del_entry(struct bpf_offloaded_map *offmap, void *key);\nint nfp_bpf_ctrl_lookup_entry(struct bpf_offloaded_map *offmap,\n\t\t\t      void *key, void *value);\nint nfp_bpf_ctrl_getnext_entry(struct bpf_offloaded_map *offmap,\n\t\t\t       void *key, void *next_key);\n\nint nfp_bpf_event_output(struct nfp_app_bpf *bpf, const void *data,\n\t\t\t unsigned int len);\n\nvoid nfp_bpf_ctrl_msg_rx(struct nfp_app *app, struct sk_buff *skb);\nvoid\nnfp_bpf_ctrl_msg_rx_raw(struct nfp_app *app, const void *data,\n\t\t\tunsigned int len);\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}