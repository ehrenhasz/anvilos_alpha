{
  "module_name": "offload.c",
  "hash_id": "9093e76a9c3010b26af4ac482c3c8a08f992a664720a088b297a04ba73f75621",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/netronome/nfp/bpf/offload.c",
  "human_readable_source": "\n \n\n \n\n#define pr_fmt(fmt)\t\"NFP net bpf: \" fmt\n\n#include <linux/bpf.h>\n#include <linux/kernel.h>\n#include <linux/netdevice.h>\n#include <linux/pci.h>\n#include <linux/jiffies.h>\n#include <linux/timer.h>\n#include <linux/list.h>\n#include <linux/mm.h>\n\n#include <net/pkt_cls.h>\n#include <net/tc_act/tc_gact.h>\n#include <net/tc_act/tc_mirred.h>\n\n#include \"main.h\"\n#include \"../ccm.h\"\n#include \"../nfp_app.h\"\n#include \"../nfp_net_ctrl.h\"\n#include \"../nfp_net.h\"\n\nstatic int\nnfp_map_ptr_record(struct nfp_app_bpf *bpf, struct nfp_prog *nfp_prog,\n\t\t   struct bpf_map *map)\n{\n\tstruct nfp_bpf_neutral_map *record;\n\tint err;\n\n\t \n\trecord = rhashtable_lookup_fast(&bpf->maps_neutral, &map->id,\n\t\t\t\t\tnfp_bpf_maps_neutral_params);\n\tif (record) {\n\t\tnfp_prog->map_records[nfp_prog->map_records_cnt++] = record;\n\t\trecord->count++;\n\t\treturn 0;\n\t}\n\n\t \n\tbpf_map_inc(map);\n\n\trecord = kmalloc(sizeof(*record), GFP_KERNEL);\n\tif (!record) {\n\t\terr = -ENOMEM;\n\t\tgoto err_map_put;\n\t}\n\n\trecord->ptr = map;\n\trecord->map_id = map->id;\n\trecord->count = 1;\n\n\terr = rhashtable_insert_fast(&bpf->maps_neutral, &record->l,\n\t\t\t\t     nfp_bpf_maps_neutral_params);\n\tif (err)\n\t\tgoto err_free_rec;\n\n\tnfp_prog->map_records[nfp_prog->map_records_cnt++] = record;\n\n\treturn 0;\n\nerr_free_rec:\n\tkfree(record);\nerr_map_put:\n\tbpf_map_put(map);\n\treturn err;\n}\n\nstatic void\nnfp_map_ptrs_forget(struct nfp_app_bpf *bpf, struct nfp_prog *nfp_prog)\n{\n\tbool freed = false;\n\tint i;\n\n\tfor (i = 0; i < nfp_prog->map_records_cnt; i++) {\n\t\tif (--nfp_prog->map_records[i]->count) {\n\t\t\tnfp_prog->map_records[i] = NULL;\n\t\t\tcontinue;\n\t\t}\n\n\t\tWARN_ON(rhashtable_remove_fast(&bpf->maps_neutral,\n\t\t\t\t\t       &nfp_prog->map_records[i]->l,\n\t\t\t\t\t       nfp_bpf_maps_neutral_params));\n\t\tfreed = true;\n\t}\n\n\tif (freed) {\n\t\tsynchronize_rcu();\n\n\t\tfor (i = 0; i < nfp_prog->map_records_cnt; i++)\n\t\t\tif (nfp_prog->map_records[i]) {\n\t\t\t\tbpf_map_put(nfp_prog->map_records[i]->ptr);\n\t\t\t\tkfree(nfp_prog->map_records[i]);\n\t\t\t}\n\t}\n\n\tkfree(nfp_prog->map_records);\n\tnfp_prog->map_records = NULL;\n\tnfp_prog->map_records_cnt = 0;\n}\n\nstatic int\nnfp_map_ptrs_record(struct nfp_app_bpf *bpf, struct nfp_prog *nfp_prog,\n\t\t    struct bpf_prog *prog)\n{\n\tint i, cnt, err = 0;\n\n\tmutex_lock(&prog->aux->used_maps_mutex);\n\n\t \n\tcnt = 0;\n\tfor (i = 0; i < prog->aux->used_map_cnt; i++)\n\t\tif (bpf_map_offload_neutral(prog->aux->used_maps[i]))\n\t\t\tcnt++;\n\tif (!cnt)\n\t\tgoto out;\n\n\tnfp_prog->map_records = kmalloc_array(cnt,\n\t\t\t\t\t      sizeof(nfp_prog->map_records[0]),\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!nfp_prog->map_records) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < prog->aux->used_map_cnt; i++)\n\t\tif (bpf_map_offload_neutral(prog->aux->used_maps[i])) {\n\t\t\terr = nfp_map_ptr_record(bpf, nfp_prog,\n\t\t\t\t\t\t prog->aux->used_maps[i]);\n\t\t\tif (err) {\n\t\t\t\tnfp_map_ptrs_forget(bpf, nfp_prog);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\tWARN_ON(cnt != nfp_prog->map_records_cnt);\n\nout:\n\tmutex_unlock(&prog->aux->used_maps_mutex);\n\treturn err;\n}\n\nstatic int\nnfp_prog_prepare(struct nfp_prog *nfp_prog, const struct bpf_insn *prog,\n\t\t unsigned int cnt)\n{\n\tstruct nfp_insn_meta *meta;\n\tunsigned int i;\n\n\tfor (i = 0; i < cnt; i++) {\n\t\tmeta = kzalloc(sizeof(*meta), GFP_KERNEL);\n\t\tif (!meta)\n\t\t\treturn -ENOMEM;\n\n\t\tmeta->insn = prog[i];\n\t\tmeta->n = i;\n\t\tif (is_mbpf_alu(meta)) {\n\t\t\tmeta->umin_src = U64_MAX;\n\t\t\tmeta->umin_dst = U64_MAX;\n\t\t}\n\n\t\tlist_add_tail(&meta->l, &nfp_prog->insns);\n\t}\n\tnfp_prog->n_insns = cnt;\n\n\tnfp_bpf_jit_prepare(nfp_prog);\n\n\treturn 0;\n}\n\nstatic void nfp_prog_free(struct nfp_prog *nfp_prog)\n{\n\tstruct nfp_insn_meta *meta, *tmp;\n\n\tkfree(nfp_prog->subprog);\n\n\tlist_for_each_entry_safe(meta, tmp, &nfp_prog->insns, l) {\n\t\tlist_del(&meta->l);\n\t\tkfree(meta);\n\t}\n\tkfree(nfp_prog);\n}\n\nstatic int nfp_bpf_verifier_prep(struct bpf_prog *prog)\n{\n\tstruct nfp_prog *nfp_prog;\n\tint ret;\n\n\tnfp_prog = kzalloc(sizeof(*nfp_prog), GFP_KERNEL);\n\tif (!nfp_prog)\n\t\treturn -ENOMEM;\n\tprog->aux->offload->dev_priv = nfp_prog;\n\n\tINIT_LIST_HEAD(&nfp_prog->insns);\n\tnfp_prog->type = prog->type;\n\tnfp_prog->bpf = bpf_offload_dev_priv(prog->aux->offload->offdev);\n\n\tret = nfp_prog_prepare(nfp_prog, prog->insnsi, prog->len);\n\tif (ret)\n\t\tgoto err_free;\n\n\tnfp_prog->verifier_meta = nfp_prog_first_meta(nfp_prog);\n\n\treturn 0;\n\nerr_free:\n\tnfp_prog_free(nfp_prog);\n\n\treturn ret;\n}\n\nstatic int nfp_bpf_translate(struct bpf_prog *prog)\n{\n\tstruct nfp_net *nn = netdev_priv(prog->aux->offload->netdev);\n\tstruct nfp_prog *nfp_prog = prog->aux->offload->dev_priv;\n\tunsigned int max_instr;\n\tint err;\n\n\t \n\tif (prog->aux->offload->opt_failed)\n\t\treturn -EINVAL;\n\n\tmax_instr = nn_readw(nn, NFP_NET_CFG_BPF_MAX_LEN);\n\tnfp_prog->__prog_alloc_len = max_instr * sizeof(u64);\n\n\tnfp_prog->prog = kvmalloc(nfp_prog->__prog_alloc_len, GFP_KERNEL);\n\tif (!nfp_prog->prog)\n\t\treturn -ENOMEM;\n\n\terr = nfp_bpf_jit(nfp_prog);\n\tif (err)\n\t\treturn err;\n\n\tprog->aux->offload->jited_len = nfp_prog->prog_len * sizeof(u64);\n\tprog->aux->offload->jited_image = nfp_prog->prog;\n\n\treturn nfp_map_ptrs_record(nfp_prog->bpf, nfp_prog, prog);\n}\n\nstatic void nfp_bpf_destroy(struct bpf_prog *prog)\n{\n\tstruct nfp_prog *nfp_prog = prog->aux->offload->dev_priv;\n\n\tkvfree(nfp_prog->prog);\n\tnfp_map_ptrs_forget(nfp_prog->bpf, nfp_prog);\n\tnfp_prog_free(nfp_prog);\n}\n\n \nstatic void nfp_map_bpf_byte_swap(struct nfp_bpf_map *nfp_map, void *value)\n{\n\tu32 *word = value;\n\tunsigned int i;\n\n\tfor (i = 0; i < DIV_ROUND_UP(nfp_map->offmap->map.value_size, 4); i++)\n\t\tif (nfp_map->use_map[i].type == NFP_MAP_USE_ATOMIC_CNT)\n\t\t\tword[i] = (__force u32)cpu_to_be32(word[i]);\n}\n\n \nstatic void\nnfp_map_bpf_byte_swap_record(struct nfp_bpf_map *nfp_map, void *value)\n{\n\tu32 *word = value;\n\tunsigned int i;\n\n\tfor (i = 0; i < DIV_ROUND_UP(nfp_map->offmap->map.value_size, 4); i++)\n\t\tif (nfp_map->use_map[i].type == NFP_MAP_UNUSED &&\n\t\t    word[i] != (__force u32)cpu_to_be32(word[i]))\n\t\t\tnfp_map->use_map[i].non_zero_update = 1;\n}\n\nstatic int\nnfp_bpf_map_lookup_entry(struct bpf_offloaded_map *offmap,\n\t\t\t void *key, void *value)\n{\n\tint err;\n\n\terr = nfp_bpf_ctrl_lookup_entry(offmap, key, value);\n\tif (err)\n\t\treturn err;\n\n\tnfp_map_bpf_byte_swap(offmap->dev_priv, value);\n\treturn 0;\n}\n\nstatic int\nnfp_bpf_map_update_entry(struct bpf_offloaded_map *offmap,\n\t\t\t void *key, void *value, u64 flags)\n{\n\tnfp_map_bpf_byte_swap(offmap->dev_priv, value);\n\tnfp_map_bpf_byte_swap_record(offmap->dev_priv, value);\n\treturn nfp_bpf_ctrl_update_entry(offmap, key, value, flags);\n}\n\nstatic int\nnfp_bpf_map_get_next_key(struct bpf_offloaded_map *offmap,\n\t\t\t void *key, void *next_key)\n{\n\tif (!key)\n\t\treturn nfp_bpf_ctrl_getfirst_entry(offmap, next_key);\n\treturn nfp_bpf_ctrl_getnext_entry(offmap, key, next_key);\n}\n\nstatic int\nnfp_bpf_map_delete_elem(struct bpf_offloaded_map *offmap, void *key)\n{\n\tif (offmap->map.map_type == BPF_MAP_TYPE_ARRAY)\n\t\treturn -EINVAL;\n\treturn nfp_bpf_ctrl_del_entry(offmap, key);\n}\n\nstatic const struct bpf_map_dev_ops nfp_bpf_map_ops = {\n\t.map_get_next_key\t= nfp_bpf_map_get_next_key,\n\t.map_lookup_elem\t= nfp_bpf_map_lookup_entry,\n\t.map_update_elem\t= nfp_bpf_map_update_entry,\n\t.map_delete_elem\t= nfp_bpf_map_delete_elem,\n};\n\nstatic int\nnfp_bpf_map_alloc(struct nfp_app_bpf *bpf, struct bpf_offloaded_map *offmap)\n{\n\tstruct nfp_bpf_map *nfp_map;\n\tunsigned int use_map_size;\n\tlong long int res;\n\n\tif (!bpf->maps.types)\n\t\treturn -EOPNOTSUPP;\n\n\tif (offmap->map.map_flags ||\n\t    offmap->map.numa_node != NUMA_NO_NODE) {\n\t\tpr_info(\"map flags are not supported\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!(bpf->maps.types & 1 << offmap->map.map_type)) {\n\t\tpr_info(\"map type not supported\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\tif (bpf->maps.max_maps == bpf->maps_in_use) {\n\t\tpr_info(\"too many maps for a device\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tif (bpf->maps.max_elems - bpf->map_elems_in_use <\n\t    offmap->map.max_entries) {\n\t\tpr_info(\"map with too many elements: %u, left: %u\\n\",\n\t\t\toffmap->map.max_entries,\n\t\t\tbpf->maps.max_elems - bpf->map_elems_in_use);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (round_up(offmap->map.key_size, 8) +\n\t    round_up(offmap->map.value_size, 8) > bpf->maps.max_elem_sz) {\n\t\tpr_info(\"map elements too large: %u, FW max element size (key+value): %u\\n\",\n\t\t\tround_up(offmap->map.key_size, 8) +\n\t\t\tround_up(offmap->map.value_size, 8),\n\t\t\tbpf->maps.max_elem_sz);\n\t\treturn -ENOMEM;\n\t}\n\tif (offmap->map.key_size > bpf->maps.max_key_sz) {\n\t\tpr_info(\"map key size %u, FW max is %u\\n\",\n\t\t\toffmap->map.key_size, bpf->maps.max_key_sz);\n\t\treturn -ENOMEM;\n\t}\n\tif (offmap->map.value_size > bpf->maps.max_val_sz) {\n\t\tpr_info(\"map value size %u, FW max is %u\\n\",\n\t\t\toffmap->map.value_size, bpf->maps.max_val_sz);\n\t\treturn -ENOMEM;\n\t}\n\n\tuse_map_size = DIV_ROUND_UP(offmap->map.value_size, 4) *\n\t\t       sizeof_field(struct nfp_bpf_map, use_map[0]);\n\n\tnfp_map = kzalloc(sizeof(*nfp_map) + use_map_size, GFP_USER);\n\tif (!nfp_map)\n\t\treturn -ENOMEM;\n\n\toffmap->dev_priv = nfp_map;\n\tnfp_map->offmap = offmap;\n\tnfp_map->bpf = bpf;\n\tspin_lock_init(&nfp_map->cache_lock);\n\n\tres = nfp_bpf_ctrl_alloc_map(bpf, &offmap->map);\n\tif (res < 0) {\n\t\tkfree(nfp_map);\n\t\treturn res;\n\t}\n\n\tnfp_map->tid = res;\n\toffmap->dev_ops = &nfp_bpf_map_ops;\n\tbpf->maps_in_use++;\n\tbpf->map_elems_in_use += offmap->map.max_entries;\n\tlist_add_tail(&nfp_map->l, &bpf->map_list);\n\n\treturn 0;\n}\n\nstatic int\nnfp_bpf_map_free(struct nfp_app_bpf *bpf, struct bpf_offloaded_map *offmap)\n{\n\tstruct nfp_bpf_map *nfp_map = offmap->dev_priv;\n\n\tnfp_bpf_ctrl_free_map(bpf, nfp_map);\n\tdev_consume_skb_any(nfp_map->cache);\n\tWARN_ON_ONCE(nfp_map->cache_blockers);\n\tlist_del_init(&nfp_map->l);\n\tbpf->map_elems_in_use -= offmap->map.max_entries;\n\tbpf->maps_in_use--;\n\tkfree(nfp_map);\n\n\treturn 0;\n}\n\nint nfp_ndo_bpf(struct nfp_app *app, struct nfp_net *nn, struct netdev_bpf *bpf)\n{\n\tswitch (bpf->command) {\n\tcase BPF_OFFLOAD_MAP_ALLOC:\n\t\treturn nfp_bpf_map_alloc(app->priv, bpf->offmap);\n\tcase BPF_OFFLOAD_MAP_FREE:\n\t\treturn nfp_bpf_map_free(app->priv, bpf->offmap);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic unsigned long\nnfp_bpf_perf_event_copy(void *dst, const void *src,\n\t\t\tunsigned long off, unsigned long len)\n{\n\tmemcpy(dst, src + off, len);\n\treturn 0;\n}\n\nint nfp_bpf_event_output(struct nfp_app_bpf *bpf, const void *data,\n\t\t\t unsigned int len)\n{\n\tstruct cmsg_bpf_event *cbe = (void *)data;\n\tstruct nfp_bpf_neutral_map *record;\n\tu32 pkt_size, data_size, map_id;\n\tu64 map_id_full;\n\n\tif (len < sizeof(struct cmsg_bpf_event))\n\t\treturn -EINVAL;\n\n\tpkt_size = be32_to_cpu(cbe->pkt_size);\n\tdata_size = be32_to_cpu(cbe->data_size);\n\tmap_id_full = be64_to_cpu(cbe->map_ptr);\n\tmap_id = map_id_full;\n\n\tif (len < sizeof(struct cmsg_bpf_event) + pkt_size + data_size)\n\t\treturn -EINVAL;\n\tif (cbe->hdr.ver != NFP_CCM_ABI_VERSION)\n\t\treturn -EINVAL;\n\n\trcu_read_lock();\n\trecord = rhashtable_lookup(&bpf->maps_neutral, &map_id,\n\t\t\t\t   nfp_bpf_maps_neutral_params);\n\tif (!record || map_id_full > U32_MAX) {\n\t\trcu_read_unlock();\n\t\tcmsg_warn(bpf, \"perf event: map id %lld (0x%llx) not recognized, dropping event\\n\",\n\t\t\t  map_id_full, map_id_full);\n\t\treturn -EINVAL;\n\t}\n\n\tbpf_event_output(record->ptr, be32_to_cpu(cbe->cpu_id),\n\t\t\t &cbe->data[round_up(pkt_size, 4)], data_size,\n\t\t\t cbe->data, pkt_size, nfp_bpf_perf_event_copy);\n\trcu_read_unlock();\n\n\treturn 0;\n}\n\nbool nfp_bpf_offload_check_mtu(struct nfp_net *nn, struct bpf_prog *prog,\n\t\t\t       unsigned int mtu)\n{\n\tunsigned int fw_mtu, pkt_off;\n\n\tfw_mtu = nn_readb(nn, NFP_NET_CFG_BPF_INL_MTU) * 64 - 32;\n\tpkt_off = min(prog->aux->max_pkt_offset, mtu);\n\n\treturn fw_mtu < pkt_off;\n}\n\nstatic int\nnfp_net_bpf_load(struct nfp_net *nn, struct bpf_prog *prog,\n\t\t struct netlink_ext_ack *extack)\n{\n\tstruct nfp_prog *nfp_prog = prog->aux->offload->dev_priv;\n\tunsigned int max_stack, max_prog_len;\n\tdma_addr_t dma_addr;\n\tvoid *img;\n\tint err;\n\n\tif (nfp_bpf_offload_check_mtu(nn, prog, nn->dp.netdev->mtu)) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"BPF offload not supported with potential packet access beyond HW packet split boundary\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmax_stack = nn_readb(nn, NFP_NET_CFG_BPF_STACK_SZ) * 64;\n\tif (nfp_prog->stack_size > max_stack) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"stack too large\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmax_prog_len = nn_readw(nn, NFP_NET_CFG_BPF_MAX_LEN);\n\tif (nfp_prog->prog_len > max_prog_len) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"program too long\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\timg = nfp_bpf_relo_for_vnic(nfp_prog, nn->app_priv);\n\tif (IS_ERR(img))\n\t\treturn PTR_ERR(img);\n\n\tdma_addr = dma_map_single(nn->dp.dev, img,\n\t\t\t\t  nfp_prog->prog_len * sizeof(u64),\n\t\t\t\t  DMA_TO_DEVICE);\n\tif (dma_mapping_error(nn->dp.dev, dma_addr)) {\n\t\tkfree(img);\n\t\treturn -ENOMEM;\n\t}\n\n\tnn_writew(nn, NFP_NET_CFG_BPF_SIZE, nfp_prog->prog_len);\n\tnn_writeq(nn, NFP_NET_CFG_BPF_ADDR, dma_addr);\n\n\t \n\terr = nfp_net_reconfig(nn, NFP_NET_CFG_UPDATE_BPF);\n\tif (err)\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"FW command error while loading BPF\");\n\n\tdma_unmap_single(nn->dp.dev, dma_addr, nfp_prog->prog_len * sizeof(u64),\n\t\t\t DMA_TO_DEVICE);\n\tkfree(img);\n\n\treturn err;\n}\n\nstatic void\nnfp_net_bpf_start(struct nfp_net *nn, struct netlink_ext_ack *extack)\n{\n\tint err;\n\n\t \n\tnn->dp.ctrl |= NFP_NET_CFG_CTRL_BPF;\n\tnn_writel(nn, NFP_NET_CFG_CTRL, nn->dp.ctrl);\n\terr = nfp_net_reconfig(nn, NFP_NET_CFG_UPDATE_GEN);\n\tif (err)\n\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t   \"FW command error while enabling BPF\");\n}\n\nstatic int nfp_net_bpf_stop(struct nfp_net *nn)\n{\n\tif (!(nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF))\n\t\treturn 0;\n\n\tnn->dp.ctrl &= ~NFP_NET_CFG_CTRL_BPF;\n\tnn_writel(nn, NFP_NET_CFG_CTRL, nn->dp.ctrl);\n\n\treturn nfp_net_reconfig(nn, NFP_NET_CFG_UPDATE_GEN);\n}\n\nint nfp_net_bpf_offload(struct nfp_net *nn, struct bpf_prog *prog,\n\t\t\tbool old_prog, struct netlink_ext_ack *extack)\n{\n\tint err;\n\n\tif (prog && !bpf_offload_dev_match(prog, nn->dp.netdev))\n\t\treturn -EINVAL;\n\n\tif (prog && old_prog) {\n\t\tu8 cap;\n\n\t\tcap = nn_readb(nn, NFP_NET_CFG_BPF_CAP);\n\t\tif (!(cap & NFP_NET_BPF_CAP_RELO)) {\n\t\t\tNL_SET_ERR_MSG_MOD(extack,\n\t\t\t\t\t   \"FW does not support live reload\");\n\t\t\treturn -EBUSY;\n\t\t}\n\t}\n\n\t \n\tif (!old_prog && nn->dp.ctrl & NFP_NET_CFG_CTRL_BPF)\n\t\treturn -EBUSY;\n\n\tif (old_prog && !prog)\n\t\treturn nfp_net_bpf_stop(nn);\n\n\terr = nfp_net_bpf_load(nn, prog, extack);\n\tif (err)\n\t\treturn err;\n\n\tif (!old_prog)\n\t\tnfp_net_bpf_start(nn, extack);\n\n\treturn 0;\n}\n\nconst struct bpf_prog_offload_ops nfp_bpf_dev_ops = {\n\t.insn_hook\t= nfp_verify_insn,\n\t.finalize\t= nfp_bpf_finalize,\n\t.replace_insn\t= nfp_bpf_opt_replace_insn,\n\t.remove_insns\t= nfp_bpf_opt_remove_insns,\n\t.prepare\t= nfp_bpf_verifier_prep,\n\t.translate\t= nfp_bpf_translate,\n\t.destroy\t= nfp_bpf_destroy,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}