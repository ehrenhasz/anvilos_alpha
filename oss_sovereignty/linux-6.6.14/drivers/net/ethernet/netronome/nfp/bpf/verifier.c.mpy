{
  "module_name": "verifier.c",
  "hash_id": "75bda6c6e0c80338ba45cffb401119974275d8b1accac8f0cfe3fd7efefd3d7e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/netronome/nfp/bpf/verifier.c",
  "human_readable_source": "\n \n\n#include <linux/bpf.h>\n#include <linux/bpf_verifier.h>\n#include <linux/kernel.h>\n#include <linux/netdevice.h>\n#include <linux/pkt_cls.h>\n\n#include \"../nfp_app.h\"\n#include \"../nfp_main.h\"\n#include \"../nfp_net.h\"\n#include \"fw.h\"\n#include \"main.h\"\n\n#define pr_vlog(env, fmt, ...)\t\\\n\tbpf_verifier_log_write(env, \"[nfp] \" fmt, ##__VA_ARGS__)\n\nstruct nfp_insn_meta *\nnfp_bpf_goto_meta(struct nfp_prog *nfp_prog, struct nfp_insn_meta *meta,\n\t\t  unsigned int insn_idx)\n{\n\tunsigned int forward, backward, i;\n\n\tbackward = meta->n - insn_idx;\n\tforward = insn_idx - meta->n;\n\n\tif (min(forward, backward) > nfp_prog->n_insns - insn_idx - 1) {\n\t\tbackward = nfp_prog->n_insns - insn_idx - 1;\n\t\tmeta = nfp_prog_last_meta(nfp_prog);\n\t}\n\tif (min(forward, backward) > insn_idx && backward > insn_idx) {\n\t\tforward = insn_idx;\n\t\tmeta = nfp_prog_first_meta(nfp_prog);\n\t}\n\n\tif (forward < backward)\n\t\tfor (i = 0; i < forward; i++)\n\t\t\tmeta = nfp_meta_next(meta);\n\telse\n\t\tfor (i = 0; i < backward; i++)\n\t\t\tmeta = nfp_meta_prev(meta);\n\n\treturn meta;\n}\n\nstatic void\nnfp_record_adjust_head(struct nfp_app_bpf *bpf, struct nfp_prog *nfp_prog,\n\t\t       struct nfp_insn_meta *meta,\n\t\t       const struct bpf_reg_state *reg2)\n{\n\tunsigned int location =\tUINT_MAX;\n\tint imm;\n\n\t \n\tif (reg2->type != SCALAR_VALUE || !tnum_is_const(reg2->var_off))\n\t\tgoto exit_set_location;\n\timm = reg2->var_off.value;\n\t \n\tif (imm > ETH_ZLEN - ETH_HLEN)\n\t\tgoto exit_set_location;\n\tif (imm > (int)bpf->adjust_head.guaranteed_add ||\n\t    imm < -bpf->adjust_head.guaranteed_sub)\n\t\tgoto exit_set_location;\n\n\tif (nfp_prog->adjust_head_location) {\n\t\t \n\t\tif (nfp_prog->adjust_head_location != meta->n)\n\t\t\tgoto exit_set_location;\n\n\t\tif (meta->arg2.reg.var_off.value != imm)\n\t\t\tgoto exit_set_location;\n\t}\n\n\tlocation = meta->n;\nexit_set_location:\n\tnfp_prog->adjust_head_location = location;\n}\n\nstatic bool nfp_bpf_map_update_value_ok(struct bpf_verifier_env *env)\n{\n\tconst struct bpf_reg_state *reg1 = cur_regs(env) + BPF_REG_1;\n\tconst struct bpf_reg_state *reg3 = cur_regs(env) + BPF_REG_3;\n\tstruct bpf_offloaded_map *offmap;\n\tstruct bpf_func_state *state;\n\tstruct nfp_bpf_map *nfp_map;\n\tint off, i;\n\n\tstate = env->cur_state->frame[reg3->frameno];\n\n\t \n\n\toffmap = map_to_offmap(reg1->map_ptr);\n\tnfp_map = offmap->dev_priv;\n\toff = reg3->off + reg3->var_off.value;\n\n\tfor (i = 0; i < offmap->map.value_size; i++) {\n\t\tstruct bpf_stack_state *stack_entry;\n\t\tunsigned int soff;\n\n\t\tsoff = -(off + i) - 1;\n\t\tstack_entry = &state->stack[soff / BPF_REG_SIZE];\n\t\tif (stack_entry->slot_type[soff % BPF_REG_SIZE] == STACK_ZERO)\n\t\t\tcontinue;\n\n\t\tif (nfp_map->use_map[i / 4].type == NFP_MAP_USE_ATOMIC_CNT) {\n\t\t\tpr_vlog(env, \"value at offset %d/%d may be non-zero, bpf_map_update_elem() is required to initialize atomic counters to zero to avoid offload endian issues\\n\",\n\t\t\t\ti, soff);\n\t\t\treturn false;\n\t\t}\n\t\tnfp_map->use_map[i / 4].non_zero_update = 1;\n\t}\n\n\treturn true;\n}\n\nstatic int\nnfp_bpf_stack_arg_ok(const char *fname, struct bpf_verifier_env *env,\n\t\t     const struct bpf_reg_state *reg,\n\t\t     struct nfp_bpf_reg_state *old_arg)\n{\n\ts64 off, old_off;\n\n\tif (reg->type != PTR_TO_STACK) {\n\t\tpr_vlog(env, \"%s: unsupported ptr type %d\\n\",\n\t\t\tfname, reg->type);\n\t\treturn false;\n\t}\n\tif (!tnum_is_const(reg->var_off)) {\n\t\tpr_vlog(env, \"%s: variable pointer\\n\", fname);\n\t\treturn false;\n\t}\n\n\toff = reg->var_off.value + reg->off;\n\tif (-off % 4) {\n\t\tpr_vlog(env, \"%s: unaligned stack pointer %lld\\n\", fname, -off);\n\t\treturn false;\n\t}\n\n\t \n\tif (!old_arg)\n\t\treturn true;\n\n\told_off = old_arg->reg.var_off.value + old_arg->reg.off;\n\told_arg->var_off |= off != old_off;\n\n\treturn true;\n}\n\nstatic bool\nnfp_bpf_map_call_ok(const char *fname, struct bpf_verifier_env *env,\n\t\t    struct nfp_insn_meta *meta,\n\t\t    u32 helper_tgt, const struct bpf_reg_state *reg1)\n{\n\tif (!helper_tgt) {\n\t\tpr_vlog(env, \"%s: not supported by FW\\n\", fname);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int\nnfp_bpf_check_helper_call(struct nfp_prog *nfp_prog,\n\t\t\t  struct bpf_verifier_env *env,\n\t\t\t  struct nfp_insn_meta *meta)\n{\n\tconst struct bpf_reg_state *reg1 = cur_regs(env) + BPF_REG_1;\n\tconst struct bpf_reg_state *reg2 = cur_regs(env) + BPF_REG_2;\n\tconst struct bpf_reg_state *reg3 = cur_regs(env) + BPF_REG_3;\n\tstruct nfp_app_bpf *bpf = nfp_prog->bpf;\n\tu32 func_id = meta->insn.imm;\n\n\tswitch (func_id) {\n\tcase BPF_FUNC_xdp_adjust_head:\n\t\tif (!bpf->adjust_head.off_max) {\n\t\t\tpr_vlog(env, \"adjust_head not supported by FW\\n\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\t\tif (!(bpf->adjust_head.flags & NFP_BPF_ADJUST_HEAD_NO_META)) {\n\t\t\tpr_vlog(env, \"adjust_head: FW requires shifting metadata, not supported by the driver\\n\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tnfp_record_adjust_head(bpf, nfp_prog, meta, reg2);\n\t\tbreak;\n\n\tcase BPF_FUNC_xdp_adjust_tail:\n\t\tif (!bpf->adjust_tail) {\n\t\t\tpr_vlog(env, \"adjust_tail not supported by FW\\n\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\t\tbreak;\n\n\tcase BPF_FUNC_map_lookup_elem:\n\t\tif (!nfp_bpf_map_call_ok(\"map_lookup\", env, meta,\n\t\t\t\t\t bpf->helpers.map_lookup, reg1) ||\n\t\t    !nfp_bpf_stack_arg_ok(\"map_lookup\", env, reg2,\n\t\t\t\t\t  meta->func_id ? &meta->arg2 : NULL))\n\t\t\treturn -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase BPF_FUNC_map_update_elem:\n\t\tif (!nfp_bpf_map_call_ok(\"map_update\", env, meta,\n\t\t\t\t\t bpf->helpers.map_update, reg1) ||\n\t\t    !nfp_bpf_stack_arg_ok(\"map_update\", env, reg2,\n\t\t\t\t\t  meta->func_id ? &meta->arg2 : NULL) ||\n\t\t    !nfp_bpf_stack_arg_ok(\"map_update\", env, reg3, NULL) ||\n\t\t    !nfp_bpf_map_update_value_ok(env))\n\t\t\treturn -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase BPF_FUNC_map_delete_elem:\n\t\tif (!nfp_bpf_map_call_ok(\"map_delete\", env, meta,\n\t\t\t\t\t bpf->helpers.map_delete, reg1) ||\n\t\t    !nfp_bpf_stack_arg_ok(\"map_delete\", env, reg2,\n\t\t\t\t\t  meta->func_id ? &meta->arg2 : NULL))\n\t\t\treturn -EOPNOTSUPP;\n\t\tbreak;\n\n\tcase BPF_FUNC_get_prandom_u32:\n\t\tif (bpf->pseudo_random)\n\t\t\tbreak;\n\t\tpr_vlog(env, \"bpf_get_prandom_u32(): FW doesn't support random number generation\\n\");\n\t\treturn -EOPNOTSUPP;\n\n\tcase BPF_FUNC_perf_event_output:\n\t\tBUILD_BUG_ON(NFP_BPF_SCALAR_VALUE != SCALAR_VALUE ||\n\t\t\t     NFP_BPF_MAP_VALUE != PTR_TO_MAP_VALUE ||\n\t\t\t     NFP_BPF_STACK != PTR_TO_STACK ||\n\t\t\t     NFP_BPF_PACKET_DATA != PTR_TO_PACKET);\n\n\t\tif (!bpf->helpers.perf_event_output) {\n\t\t\tpr_vlog(env, \"event_output: not supported by FW\\n\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\t \n\t\tif (reg3->var_off.mask & BPF_F_INDEX_MASK ||\n\t\t    (reg3->var_off.value & BPF_F_INDEX_MASK) !=\n\t\t    BPF_F_CURRENT_CPU) {\n\t\t\tchar tn_buf[48];\n\n\t\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg3->var_off);\n\t\t\tpr_vlog(env, \"event_output: must use BPF_F_CURRENT_CPU, var_off: %s\\n\",\n\t\t\t\ttn_buf);\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\t \n\t\treg1 = cur_regs(env) + BPF_REG_4;\n\n\t\tif (reg1->type != SCALAR_VALUE   &&\n\t\t    reg1->type != PTR_TO_STACK &&\n\t\t    reg1->type != PTR_TO_MAP_VALUE &&\n\t\t    reg1->type != PTR_TO_PACKET) {\n\t\t\tpr_vlog(env, \"event_output: unsupported ptr type: %d\\n\",\n\t\t\t\treg1->type);\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\n\t\tif (reg1->type == PTR_TO_STACK &&\n\t\t    !nfp_bpf_stack_arg_ok(\"event_output\", env, reg1, NULL))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\t \n\t\tdev_warn_once(&nfp_prog->bpf->app->pf->pdev->dev,\n\t\t\t      \"bpf: note: return codes and behavior of bpf_event_output() helper differs for offloaded programs!\\n\");\n\t\tpr_vlog(env, \"warning: return codes and behavior of event_output helper differ for offload!\\n\");\n\n\t\tif (!meta->func_id)\n\t\t\tbreak;\n\n\t\tif (reg1->type != meta->arg1.type) {\n\t\t\tpr_vlog(env, \"event_output: ptr type changed: %d %d\\n\",\n\t\t\t\tmeta->arg1.type, reg1->type);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tpr_vlog(env, \"unsupported function id: %d\\n\", func_id);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmeta->func_id = func_id;\n\tmeta->arg1 = *reg1;\n\tmeta->arg2.reg = *reg2;\n\n\treturn 0;\n}\n\nstatic int\nnfp_bpf_check_exit(struct nfp_prog *nfp_prog,\n\t\t   struct bpf_verifier_env *env)\n{\n\tconst struct bpf_reg_state *reg0 = cur_regs(env) + BPF_REG_0;\n\tu64 imm;\n\n\tif (nfp_prog->type == BPF_PROG_TYPE_XDP)\n\t\treturn 0;\n\n\tif (!(reg0->type == SCALAR_VALUE && tnum_is_const(reg0->var_off))) {\n\t\tchar tn_buf[48];\n\n\t\ttnum_strn(tn_buf, sizeof(tn_buf), reg0->var_off);\n\t\tpr_vlog(env, \"unsupported exit state: %d, var_off: %s\\n\",\n\t\t\treg0->type, tn_buf);\n\t\treturn -EINVAL;\n\t}\n\n\timm = reg0->var_off.value;\n\tif (nfp_prog->type == BPF_PROG_TYPE_SCHED_CLS &&\n\t    imm <= TC_ACT_REDIRECT &&\n\t    imm != TC_ACT_SHOT && imm != TC_ACT_STOLEN &&\n\t    imm != TC_ACT_QUEUED) {\n\t\tpr_vlog(env, \"unsupported exit state: %d, imm: %llx\\n\",\n\t\t\treg0->type, imm);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nnfp_bpf_check_stack_access(struct nfp_prog *nfp_prog,\n\t\t\t   struct nfp_insn_meta *meta,\n\t\t\t   const struct bpf_reg_state *reg,\n\t\t\t   struct bpf_verifier_env *env)\n{\n\ts32 old_off, new_off;\n\n\tif (reg->frameno != env->cur_state->curframe)\n\t\tmeta->flags |= FLAG_INSN_PTR_CALLER_STACK_FRAME;\n\n\tif (!tnum_is_const(reg->var_off)) {\n\t\tpr_vlog(env, \"variable ptr stack access\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (meta->ptr.type == NOT_INIT)\n\t\treturn 0;\n\n\told_off = meta->ptr.off + meta->ptr.var_off.value;\n\tnew_off = reg->off + reg->var_off.value;\n\n\tmeta->ptr_not_const |= old_off != new_off;\n\n\tif (!meta->ptr_not_const)\n\t\treturn 0;\n\n\tif (old_off % 4 == new_off % 4)\n\t\treturn 0;\n\n\tpr_vlog(env, \"stack access changed location was:%d is:%d\\n\",\n\t\told_off, new_off);\n\treturn -EINVAL;\n}\n\nstatic const char *nfp_bpf_map_use_name(enum nfp_bpf_map_use use)\n{\n\tstatic const char * const names[] = {\n\t\t[NFP_MAP_UNUSED]\t= \"unused\",\n\t\t[NFP_MAP_USE_READ]\t= \"read\",\n\t\t[NFP_MAP_USE_WRITE]\t= \"write\",\n\t\t[NFP_MAP_USE_ATOMIC_CNT] = \"atomic\",\n\t};\n\n\tif (use >= ARRAY_SIZE(names) || !names[use])\n\t\treturn \"unknown\";\n\treturn names[use];\n}\n\nstatic int\nnfp_bpf_map_mark_used_one(struct bpf_verifier_env *env,\n\t\t\t  struct nfp_bpf_map *nfp_map,\n\t\t\t  unsigned int off, enum nfp_bpf_map_use use)\n{\n\tif (nfp_map->use_map[off / 4].type != NFP_MAP_UNUSED &&\n\t    nfp_map->use_map[off / 4].type != use) {\n\t\tpr_vlog(env, \"map value use type conflict %s vs %s off: %u\\n\",\n\t\t\tnfp_bpf_map_use_name(nfp_map->use_map[off / 4].type),\n\t\t\tnfp_bpf_map_use_name(use), off);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (nfp_map->use_map[off / 4].non_zero_update &&\n\t    use == NFP_MAP_USE_ATOMIC_CNT) {\n\t\tpr_vlog(env, \"atomic counter in map value may already be initialized to non-zero value off: %u\\n\",\n\t\t\toff);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tnfp_map->use_map[off / 4].type = use;\n\n\treturn 0;\n}\n\nstatic int\nnfp_bpf_map_mark_used(struct bpf_verifier_env *env, struct nfp_insn_meta *meta,\n\t\t      const struct bpf_reg_state *reg,\n\t\t      enum nfp_bpf_map_use use)\n{\n\tstruct bpf_offloaded_map *offmap;\n\tstruct nfp_bpf_map *nfp_map;\n\tunsigned int size, off;\n\tint i, err;\n\n\tif (!tnum_is_const(reg->var_off)) {\n\t\tpr_vlog(env, \"map value offset is variable\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\toff = reg->var_off.value + meta->insn.off + reg->off;\n\tsize = BPF_LDST_BYTES(&meta->insn);\n\toffmap = map_to_offmap(reg->map_ptr);\n\tnfp_map = offmap->dev_priv;\n\n\tif (off + size > offmap->map.value_size) {\n\t\tpr_vlog(env, \"map value access out-of-bounds\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i < size; i += 4 - (off + i) % 4) {\n\t\terr = nfp_bpf_map_mark_used_one(env, nfp_map, off + i, use);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int\nnfp_bpf_check_ptr(struct nfp_prog *nfp_prog, struct nfp_insn_meta *meta,\n\t\t  struct bpf_verifier_env *env, u8 reg_no)\n{\n\tconst struct bpf_reg_state *reg = cur_regs(env) + reg_no;\n\tint err;\n\n\tif (reg->type != PTR_TO_CTX &&\n\t    reg->type != PTR_TO_STACK &&\n\t    reg->type != PTR_TO_MAP_VALUE &&\n\t    reg->type != PTR_TO_PACKET) {\n\t\tpr_vlog(env, \"unsupported ptr type: %d\\n\", reg->type);\n\t\treturn -EINVAL;\n\t}\n\n\tif (reg->type == PTR_TO_STACK) {\n\t\terr = nfp_bpf_check_stack_access(nfp_prog, meta, reg, env);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (reg->type == PTR_TO_MAP_VALUE) {\n\t\tif (is_mbpf_load(meta)) {\n\t\t\terr = nfp_bpf_map_mark_used(env, meta, reg,\n\t\t\t\t\t\t    NFP_MAP_USE_READ);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\tif (is_mbpf_store(meta)) {\n\t\t\tpr_vlog(env, \"map writes not supported\\n\");\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n\t\tif (is_mbpf_atomic(meta)) {\n\t\t\terr = nfp_bpf_map_mark_used(env, meta, reg,\n\t\t\t\t\t\t    NFP_MAP_USE_ATOMIC_CNT);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (meta->ptr.type != NOT_INIT && meta->ptr.type != reg->type) {\n\t\tpr_vlog(env, \"ptr type changed for instruction %d -> %d\\n\",\n\t\t\tmeta->ptr.type, reg->type);\n\t\treturn -EINVAL;\n\t}\n\n\tmeta->ptr = *reg;\n\n\treturn 0;\n}\n\nstatic int\nnfp_bpf_check_store(struct nfp_prog *nfp_prog, struct nfp_insn_meta *meta,\n\t\t    struct bpf_verifier_env *env)\n{\n\tconst struct bpf_reg_state *reg = cur_regs(env) + meta->insn.dst_reg;\n\n\tif (reg->type == PTR_TO_CTX) {\n\t\tif (nfp_prog->type == BPF_PROG_TYPE_XDP) {\n\t\t\t \n\t\t\tswitch (meta->insn.off) {\n\t\t\tcase offsetof(struct xdp_md, rx_queue_index):\n\t\t\t\tif (nfp_prog->bpf->queue_select)\n\t\t\t\t\tgoto exit_check_ptr;\n\t\t\t\tpr_vlog(env, \"queue selection not supported by FW\\n\");\n\t\t\t\treturn -EOPNOTSUPP;\n\t\t\t}\n\t\t}\n\t\tpr_vlog(env, \"unsupported store to context field\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\nexit_check_ptr:\n\treturn nfp_bpf_check_ptr(nfp_prog, meta, env, meta->insn.dst_reg);\n}\n\nstatic int\nnfp_bpf_check_atomic(struct nfp_prog *nfp_prog, struct nfp_insn_meta *meta,\n\t\t     struct bpf_verifier_env *env)\n{\n\tconst struct bpf_reg_state *sreg = cur_regs(env) + meta->insn.src_reg;\n\tconst struct bpf_reg_state *dreg = cur_regs(env) + meta->insn.dst_reg;\n\n\tif (meta->insn.imm != BPF_ADD) {\n\t\tpr_vlog(env, \"atomic op not implemented: %d\\n\", meta->insn.imm);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (dreg->type != PTR_TO_MAP_VALUE) {\n\t\tpr_vlog(env, \"atomic add not to a map value pointer: %d\\n\",\n\t\t\tdreg->type);\n\t\treturn -EOPNOTSUPP;\n\t}\n\tif (sreg->type != SCALAR_VALUE) {\n\t\tpr_vlog(env, \"atomic add not of a scalar: %d\\n\", sreg->type);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tmeta->xadd_over_16bit |=\n\t\tsreg->var_off.value > 0xffff || sreg->var_off.mask > 0xffff;\n\tmeta->xadd_maybe_16bit |=\n\t\t(sreg->var_off.value & ~sreg->var_off.mask) <= 0xffff;\n\n\treturn nfp_bpf_check_ptr(nfp_prog, meta, env, meta->insn.dst_reg);\n}\n\nstatic int\nnfp_bpf_check_alu(struct nfp_prog *nfp_prog, struct nfp_insn_meta *meta,\n\t\t  struct bpf_verifier_env *env)\n{\n\tconst struct bpf_reg_state *sreg =\n\t\tcur_regs(env) + meta->insn.src_reg;\n\tconst struct bpf_reg_state *dreg =\n\t\tcur_regs(env) + meta->insn.dst_reg;\n\n\tmeta->umin_src = min(meta->umin_src, sreg->umin_value);\n\tmeta->umax_src = max(meta->umax_src, sreg->umax_value);\n\tmeta->umin_dst = min(meta->umin_dst, dreg->umin_value);\n\tmeta->umax_dst = max(meta->umax_dst, dreg->umax_value);\n\n\t \n\tif (is_mbpf_mul(meta)) {\n\t\tif (meta->umax_dst > U32_MAX) {\n\t\t\tpr_vlog(env, \"multiplier is not within u32 value range\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (mbpf_src(meta) == BPF_X && meta->umax_src > U32_MAX) {\n\t\t\tpr_vlog(env, \"multiplicand is not within u32 value range\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (mbpf_class(meta) == BPF_ALU64 &&\n\t\t    mbpf_src(meta) == BPF_K && meta->insn.imm < 0) {\n\t\t\tpr_vlog(env, \"sign extended multiplicand won't be within u32 value range\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\tif (is_mbpf_div(meta)) {\n\t\tif (meta->umax_dst > U32_MAX) {\n\t\t\tpr_vlog(env, \"dividend is not within u32 value range\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (mbpf_src(meta) == BPF_X) {\n\t\t\tif (meta->umin_src != meta->umax_src) {\n\t\t\t\tpr_vlog(env, \"divisor is not constant\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (meta->umax_src > U32_MAX) {\n\t\t\t\tpr_vlog(env, \"divisor is not within u32 value range\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t\tif (mbpf_src(meta) == BPF_K && meta->insn.imm < 0) {\n\t\t\tpr_vlog(env, \"divide by negative constant is not supported\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint nfp_verify_insn(struct bpf_verifier_env *env, int insn_idx,\n\t\t    int prev_insn_idx)\n{\n\tstruct nfp_prog *nfp_prog = env->prog->aux->offload->dev_priv;\n\tstruct nfp_insn_meta *meta = nfp_prog->verifier_meta;\n\n\tmeta = nfp_bpf_goto_meta(nfp_prog, meta, insn_idx);\n\tnfp_prog->verifier_meta = meta;\n\n\tif (!nfp_bpf_supported_opcode(meta->insn.code)) {\n\t\tpr_vlog(env, \"instruction %#02x not supported\\n\",\n\t\t\tmeta->insn.code);\n\t\treturn -EINVAL;\n\t}\n\n\tif (meta->insn.src_reg >= MAX_BPF_REG ||\n\t    meta->insn.dst_reg >= MAX_BPF_REG) {\n\t\tpr_vlog(env, \"program uses extended registers - jit hardening?\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (is_mbpf_helper_call(meta))\n\t\treturn nfp_bpf_check_helper_call(nfp_prog, env, meta);\n\tif (meta->insn.code == (BPF_JMP | BPF_EXIT))\n\t\treturn nfp_bpf_check_exit(nfp_prog, env);\n\n\tif (is_mbpf_load(meta))\n\t\treturn nfp_bpf_check_ptr(nfp_prog, meta, env,\n\t\t\t\t\t meta->insn.src_reg);\n\tif (is_mbpf_store(meta))\n\t\treturn nfp_bpf_check_store(nfp_prog, meta, env);\n\n\tif (is_mbpf_atomic(meta))\n\t\treturn nfp_bpf_check_atomic(nfp_prog, meta, env);\n\n\tif (is_mbpf_alu(meta))\n\t\treturn nfp_bpf_check_alu(nfp_prog, meta, env);\n\n\treturn 0;\n}\n\nstatic int\nnfp_assign_subprog_idx_and_regs(struct bpf_verifier_env *env,\n\t\t\t\tstruct nfp_prog *nfp_prog)\n{\n\tstruct nfp_insn_meta *meta;\n\tint index = 0;\n\n\tlist_for_each_entry(meta, &nfp_prog->insns, l) {\n\t\tif (nfp_is_subprog_start(meta))\n\t\t\tindex++;\n\t\tmeta->subprog_idx = index;\n\n\t\tif (meta->insn.dst_reg >= BPF_REG_6 &&\n\t\t    meta->insn.dst_reg <= BPF_REG_9)\n\t\t\tnfp_prog->subprog[index].needs_reg_push = 1;\n\t}\n\n\tif (index + 1 != nfp_prog->subprog_cnt) {\n\t\tpr_vlog(env, \"BUG: number of processed BPF functions is not consistent (processed %d, expected %d)\\n\",\n\t\t\tindex + 1, nfp_prog->subprog_cnt);\n\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\nstatic unsigned int nfp_bpf_get_stack_usage(struct nfp_prog *nfp_prog)\n{\n\tstruct nfp_insn_meta *meta = nfp_prog_first_meta(nfp_prog);\n\tunsigned int max_depth = 0, depth = 0, frame = 0;\n\tstruct nfp_insn_meta *ret_insn[MAX_CALL_FRAMES];\n\tunsigned short frame_depths[MAX_CALL_FRAMES];\n\tunsigned short ret_prog[MAX_CALL_FRAMES];\n\tunsigned short idx = meta->subprog_idx;\n\n\t \nprocess_subprog:\n\tframe_depths[frame] = nfp_prog->subprog[idx].stack_depth;\n\tframe_depths[frame] = round_up(frame_depths[frame], STACK_FRAME_ALIGN);\n\tdepth += frame_depths[frame];\n\tmax_depth = max(max_depth, depth);\n\ncontinue_subprog:\n\tfor (; meta != nfp_prog_last_meta(nfp_prog) && meta->subprog_idx == idx;\n\t     meta = nfp_meta_next(meta)) {\n\t\tif (!is_mbpf_pseudo_call(meta))\n\t\t\tcontinue;\n\n\t\t \n\t\tret_insn[frame] = nfp_meta_next(meta);\n\t\tret_prog[frame] = idx;\n\n\t\t \n\t\tmeta = nfp_bpf_goto_meta(nfp_prog, meta,\n\t\t\t\t\t meta->n + 1 + meta->insn.imm);\n\t\tidx = meta->subprog_idx;\n\t\tframe++;\n\t\tgoto process_subprog;\n\t}\n\t \n\tif (frame == 0)\n\t\treturn max_depth;\n\n\tdepth -= frame_depths[frame];\n\tframe--;\n\tmeta = ret_insn[frame];\n\tidx = ret_prog[frame];\n\tgoto continue_subprog;\n}\n\nstatic void nfp_bpf_insn_flag_zext(struct nfp_prog *nfp_prog,\n\t\t\t\t   struct bpf_insn_aux_data *aux)\n{\n\tstruct nfp_insn_meta *meta;\n\n\tlist_for_each_entry(meta, &nfp_prog->insns, l) {\n\t\tif (aux[meta->n].zext_dst)\n\t\t\tmeta->flags |= FLAG_INSN_DO_ZEXT;\n\t}\n}\n\nint nfp_bpf_finalize(struct bpf_verifier_env *env)\n{\n\tstruct bpf_subprog_info *info;\n\tstruct nfp_prog *nfp_prog;\n\tunsigned int max_stack;\n\tstruct nfp_net *nn;\n\tint i;\n\n\tnfp_prog = env->prog->aux->offload->dev_priv;\n\tnfp_prog->subprog_cnt = env->subprog_cnt;\n\tnfp_prog->subprog = kcalloc(nfp_prog->subprog_cnt,\n\t\t\t\t    sizeof(nfp_prog->subprog[0]), GFP_KERNEL);\n\tif (!nfp_prog->subprog)\n\t\treturn -ENOMEM;\n\n\tnfp_assign_subprog_idx_and_regs(env, nfp_prog);\n\n\tinfo = env->subprog_info;\n\tfor (i = 0; i < nfp_prog->subprog_cnt; i++) {\n\t\tnfp_prog->subprog[i].stack_depth = info[i].stack_depth;\n\n\t\tif (i == 0)\n\t\t\tcontinue;\n\n\t\t \n\t\tnfp_prog->subprog[i].stack_depth += REG_WIDTH;\n\t\t \n\t\tif (nfp_prog->subprog[i].needs_reg_push)\n\t\t\tnfp_prog->subprog[i].stack_depth += BPF_REG_SIZE * 4;\n\t}\n\n\tnn = netdev_priv(env->prog->aux->offload->netdev);\n\tmax_stack = nn_readb(nn, NFP_NET_CFG_BPF_STACK_SZ) * 64;\n\tnfp_prog->stack_size = nfp_bpf_get_stack_usage(nfp_prog);\n\tif (nfp_prog->stack_size > max_stack) {\n\t\tpr_vlog(env, \"stack too large: program %dB > FW stack %dB\\n\",\n\t\t\tnfp_prog->stack_size, max_stack);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tnfp_bpf_insn_flag_zext(nfp_prog, env->insn_aux_data);\n\treturn 0;\n}\n\nint nfp_bpf_opt_replace_insn(struct bpf_verifier_env *env, u32 off,\n\t\t\t     struct bpf_insn *insn)\n{\n\tstruct nfp_prog *nfp_prog = env->prog->aux->offload->dev_priv;\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tstruct nfp_insn_meta *meta = nfp_prog->verifier_meta;\n\n\tmeta = nfp_bpf_goto_meta(nfp_prog, meta, aux_data[off].orig_idx);\n\tnfp_prog->verifier_meta = meta;\n\n\t \n\tif (is_mbpf_cond_jump(meta) &&\n\t    insn->code == (BPF_JMP | BPF_JA | BPF_K)) {\n\t\tunsigned int tgt_off;\n\n\t\ttgt_off = off + insn->off + 1;\n\n\t\tif (!insn->off) {\n\t\t\tmeta->jmp_dst = list_next_entry(meta, l);\n\t\t\tmeta->jump_neg_op = false;\n\t\t} else if (meta->jmp_dst->n != aux_data[tgt_off].orig_idx) {\n\t\t\tpr_vlog(env, \"branch hard wire at %d changes target %d -> %d\\n\",\n\t\t\t\toff, meta->jmp_dst->n,\n\t\t\t\taux_data[tgt_off].orig_idx);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tpr_vlog(env, \"unsupported instruction replacement %hhx -> %hhx\\n\",\n\t\tmeta->insn.code, insn->code);\n\treturn -EINVAL;\n}\n\nint nfp_bpf_opt_remove_insns(struct bpf_verifier_env *env, u32 off, u32 cnt)\n{\n\tstruct nfp_prog *nfp_prog = env->prog->aux->offload->dev_priv;\n\tstruct bpf_insn_aux_data *aux_data = env->insn_aux_data;\n\tstruct nfp_insn_meta *meta = nfp_prog->verifier_meta;\n\tunsigned int i;\n\n\tmeta = nfp_bpf_goto_meta(nfp_prog, meta, aux_data[off].orig_idx);\n\n\tfor (i = 0; i < cnt; i++) {\n\t\tif (WARN_ON_ONCE(&meta->l == &nfp_prog->insns))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (meta->flags & FLAG_INSN_SKIP_VERIFIER_OPT)\n\t\t\ti--;\n\n\t\tmeta->flags |= FLAG_INSN_SKIP_VERIFIER_OPT;\n\t\tmeta = list_next_entry(meta, l);\n\t}\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}