{
  "module_name": "nfp_net_dp.c",
  "hash_id": "94e3eb69c5355f958c3ee626c9fe82e377510dd37655178dbdf7ac9764fcb40a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/netronome/nfp/nfp_net_dp.c",
  "human_readable_source": "\n \n\n#include \"nfp_app.h\"\n#include \"nfp_net_dp.h\"\n#include \"nfp_net_xsk.h\"\n\n \nvoid *nfp_net_rx_alloc_one(struct nfp_net_dp *dp, dma_addr_t *dma_addr)\n{\n\tvoid *frag;\n\n\tif (!dp->xdp_prog) {\n\t\tfrag = netdev_alloc_frag(dp->fl_bufsz);\n\t} else {\n\t\tstruct page *page;\n\n\t\tpage = alloc_page(GFP_KERNEL);\n\t\tfrag = page ? page_address(page) : NULL;\n\t}\n\tif (!frag) {\n\t\tnn_dp_warn(dp, \"Failed to alloc receive page frag\\n\");\n\t\treturn NULL;\n\t}\n\n\t*dma_addr = nfp_net_dma_map_rx(dp, frag);\n\tif (dma_mapping_error(dp->dev, *dma_addr)) {\n\t\tnfp_net_free_frag(frag, dp->xdp_prog);\n\t\tnn_dp_warn(dp, \"Failed to map DMA RX buffer\\n\");\n\t\treturn NULL;\n\t}\n\n\treturn frag;\n}\n\n \nstatic void\nnfp_net_tx_ring_init(struct nfp_net_tx_ring *tx_ring, struct nfp_net_dp *dp,\n\t\t     struct nfp_net_r_vector *r_vec, unsigned int idx,\n\t\t     bool is_xdp)\n{\n\tstruct nfp_net *nn = r_vec->nfp_net;\n\n\ttx_ring->idx = idx;\n\ttx_ring->r_vec = r_vec;\n\ttx_ring->is_xdp = is_xdp;\n\tu64_stats_init(&tx_ring->r_vec->tx_sync);\n\n\ttx_ring->qcidx = tx_ring->idx * nn->stride_tx;\n\ttx_ring->txrwb = dp->txrwb ? &dp->txrwb[idx] : NULL;\n\ttx_ring->qcp_q = nn->tx_bar + NFP_QCP_QUEUE_OFF(tx_ring->qcidx);\n}\n\n \nstatic void\nnfp_net_rx_ring_init(struct nfp_net_rx_ring *rx_ring,\n\t\t     struct nfp_net_r_vector *r_vec, unsigned int idx)\n{\n\tstruct nfp_net *nn = r_vec->nfp_net;\n\n\trx_ring->idx = idx;\n\trx_ring->r_vec = r_vec;\n\tu64_stats_init(&rx_ring->r_vec->rx_sync);\n\n\trx_ring->fl_qcidx = rx_ring->idx * nn->stride_rx;\n\trx_ring->qcp_fl = nn->rx_bar + NFP_QCP_QUEUE_OFF(rx_ring->fl_qcidx);\n}\n\n \nvoid nfp_net_rx_ring_reset(struct nfp_net_rx_ring *rx_ring)\n{\n\tunsigned int wr_idx, last_idx;\n\n\t \n\tif (rx_ring->wr_p == 0 && rx_ring->rd_p == 0)\n\t\treturn;\n\n\t \n\twr_idx = D_IDX(rx_ring, rx_ring->wr_p);\n\tlast_idx = rx_ring->cnt - 1;\n\tif (rx_ring->r_vec->xsk_pool) {\n\t\trx_ring->xsk_rxbufs[wr_idx] = rx_ring->xsk_rxbufs[last_idx];\n\t\tmemset(&rx_ring->xsk_rxbufs[last_idx], 0,\n\t\t       sizeof(*rx_ring->xsk_rxbufs));\n\t} else {\n\t\trx_ring->rxbufs[wr_idx] = rx_ring->rxbufs[last_idx];\n\t\tmemset(&rx_ring->rxbufs[last_idx], 0, sizeof(*rx_ring->rxbufs));\n\t}\n\n\tmemset(rx_ring->rxds, 0, rx_ring->size);\n\trx_ring->wr_p = 0;\n\trx_ring->rd_p = 0;\n}\n\n \nstatic void\nnfp_net_rx_ring_bufs_free(struct nfp_net_dp *dp,\n\t\t\t  struct nfp_net_rx_ring *rx_ring)\n{\n\tunsigned int i;\n\n\tif (nfp_net_has_xsk_pool_slow(dp, rx_ring->idx))\n\t\treturn;\n\n\tfor (i = 0; i < rx_ring->cnt - 1; i++) {\n\t\t \n\t\tif (!rx_ring->rxbufs[i].frag)\n\t\t\tcontinue;\n\n\t\tnfp_net_dma_unmap_rx(dp, rx_ring->rxbufs[i].dma_addr);\n\t\tnfp_net_free_frag(rx_ring->rxbufs[i].frag, dp->xdp_prog);\n\t\trx_ring->rxbufs[i].dma_addr = 0;\n\t\trx_ring->rxbufs[i].frag = NULL;\n\t}\n}\n\n \nstatic int\nnfp_net_rx_ring_bufs_alloc(struct nfp_net_dp *dp,\n\t\t\t   struct nfp_net_rx_ring *rx_ring)\n{\n\tstruct nfp_net_rx_buf *rxbufs;\n\tunsigned int i;\n\n\tif (nfp_net_has_xsk_pool_slow(dp, rx_ring->idx))\n\t\treturn 0;\n\n\trxbufs = rx_ring->rxbufs;\n\n\tfor (i = 0; i < rx_ring->cnt - 1; i++) {\n\t\trxbufs[i].frag = nfp_net_rx_alloc_one(dp, &rxbufs[i].dma_addr);\n\t\tif (!rxbufs[i].frag) {\n\t\t\tnfp_net_rx_ring_bufs_free(dp, rx_ring);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint nfp_net_tx_rings_prepare(struct nfp_net *nn, struct nfp_net_dp *dp)\n{\n\tunsigned int r;\n\n\tdp->tx_rings = kcalloc(dp->num_tx_rings, sizeof(*dp->tx_rings),\n\t\t\t       GFP_KERNEL);\n\tif (!dp->tx_rings)\n\t\treturn -ENOMEM;\n\n\tif (dp->ctrl & NFP_NET_CFG_CTRL_TXRWB) {\n\t\tdp->txrwb = dma_alloc_coherent(dp->dev,\n\t\t\t\t\t       dp->num_tx_rings * sizeof(u64),\n\t\t\t\t\t       &dp->txrwb_dma, GFP_KERNEL);\n\t\tif (!dp->txrwb)\n\t\t\tgoto err_free_rings;\n\t}\n\n\tfor (r = 0; r < dp->num_tx_rings; r++) {\n\t\tint bias = 0;\n\n\t\tif (r >= dp->num_stack_tx_rings)\n\t\t\tbias = dp->num_stack_tx_rings;\n\n\t\tnfp_net_tx_ring_init(&dp->tx_rings[r], dp,\n\t\t\t\t     &nn->r_vecs[r - bias], r, bias);\n\n\t\tif (nfp_net_tx_ring_alloc(dp, &dp->tx_rings[r]))\n\t\t\tgoto err_free_prev;\n\n\t\tif (nfp_net_tx_ring_bufs_alloc(dp, &dp->tx_rings[r]))\n\t\t\tgoto err_free_ring;\n\t}\n\n\treturn 0;\n\nerr_free_prev:\n\twhile (r--) {\n\t\tnfp_net_tx_ring_bufs_free(dp, &dp->tx_rings[r]);\nerr_free_ring:\n\t\tnfp_net_tx_ring_free(dp, &dp->tx_rings[r]);\n\t}\n\tif (dp->txrwb)\n\t\tdma_free_coherent(dp->dev, dp->num_tx_rings * sizeof(u64),\n\t\t\t\t  dp->txrwb, dp->txrwb_dma);\nerr_free_rings:\n\tkfree(dp->tx_rings);\n\treturn -ENOMEM;\n}\n\nvoid nfp_net_tx_rings_free(struct nfp_net_dp *dp)\n{\n\tunsigned int r;\n\n\tfor (r = 0; r < dp->num_tx_rings; r++) {\n\t\tnfp_net_tx_ring_bufs_free(dp, &dp->tx_rings[r]);\n\t\tnfp_net_tx_ring_free(dp, &dp->tx_rings[r]);\n\t}\n\n\tif (dp->txrwb)\n\t\tdma_free_coherent(dp->dev, dp->num_tx_rings * sizeof(u64),\n\t\t\t\t  dp->txrwb, dp->txrwb_dma);\n\tkfree(dp->tx_rings);\n}\n\n \nstatic void nfp_net_rx_ring_free(struct nfp_net_rx_ring *rx_ring)\n{\n\tstruct nfp_net_r_vector *r_vec = rx_ring->r_vec;\n\tstruct nfp_net_dp *dp = &r_vec->nfp_net->dp;\n\n\tif (dp->netdev)\n\t\txdp_rxq_info_unreg(&rx_ring->xdp_rxq);\n\n\tif (nfp_net_has_xsk_pool_slow(dp, rx_ring->idx))\n\t\tkvfree(rx_ring->xsk_rxbufs);\n\telse\n\t\tkvfree(rx_ring->rxbufs);\n\n\tif (rx_ring->rxds)\n\t\tdma_free_coherent(dp->dev, rx_ring->size,\n\t\t\t\t  rx_ring->rxds, rx_ring->dma);\n\n\trx_ring->cnt = 0;\n\trx_ring->rxbufs = NULL;\n\trx_ring->xsk_rxbufs = NULL;\n\trx_ring->rxds = NULL;\n\trx_ring->dma = 0;\n\trx_ring->size = 0;\n}\n\n \nstatic int\nnfp_net_rx_ring_alloc(struct nfp_net_dp *dp, struct nfp_net_rx_ring *rx_ring)\n{\n\tenum xdp_mem_type mem_type;\n\tsize_t rxbuf_sw_desc_sz;\n\tint err;\n\n\tif (nfp_net_has_xsk_pool_slow(dp, rx_ring->idx)) {\n\t\tmem_type = MEM_TYPE_XSK_BUFF_POOL;\n\t\trxbuf_sw_desc_sz = sizeof(*rx_ring->xsk_rxbufs);\n\t} else {\n\t\tmem_type = MEM_TYPE_PAGE_ORDER0;\n\t\trxbuf_sw_desc_sz = sizeof(*rx_ring->rxbufs);\n\t}\n\n\tif (dp->netdev) {\n\t\terr = xdp_rxq_info_reg(&rx_ring->xdp_rxq, dp->netdev,\n\t\t\t\t       rx_ring->idx, rx_ring->r_vec->napi.napi_id);\n\t\tif (err < 0)\n\t\t\treturn err;\n\n\t\terr = xdp_rxq_info_reg_mem_model(&rx_ring->xdp_rxq, mem_type, NULL);\n\t\tif (err)\n\t\t\tgoto err_alloc;\n\t}\n\n\trx_ring->cnt = dp->rxd_cnt;\n\trx_ring->size = array_size(rx_ring->cnt, sizeof(*rx_ring->rxds));\n\trx_ring->rxds = dma_alloc_coherent(dp->dev, rx_ring->size,\n\t\t\t\t\t   &rx_ring->dma,\n\t\t\t\t\t   GFP_KERNEL | __GFP_NOWARN);\n\tif (!rx_ring->rxds) {\n\t\tnetdev_warn(dp->netdev, \"failed to allocate RX descriptor ring memory, requested descriptor count: %d, consider lowering descriptor count\\n\",\n\t\t\t    rx_ring->cnt);\n\t\tgoto err_alloc;\n\t}\n\n\tif (nfp_net_has_xsk_pool_slow(dp, rx_ring->idx)) {\n\t\trx_ring->xsk_rxbufs = kvcalloc(rx_ring->cnt, rxbuf_sw_desc_sz,\n\t\t\t\t\t       GFP_KERNEL);\n\t\tif (!rx_ring->xsk_rxbufs)\n\t\t\tgoto err_alloc;\n\t} else {\n\t\trx_ring->rxbufs = kvcalloc(rx_ring->cnt, rxbuf_sw_desc_sz,\n\t\t\t\t\t   GFP_KERNEL);\n\t\tif (!rx_ring->rxbufs)\n\t\t\tgoto err_alloc;\n\t}\n\n\treturn 0;\n\nerr_alloc:\n\tnfp_net_rx_ring_free(rx_ring);\n\treturn -ENOMEM;\n}\n\nint nfp_net_rx_rings_prepare(struct nfp_net *nn, struct nfp_net_dp *dp)\n{\n\tunsigned int r;\n\n\tdp->rx_rings = kcalloc(dp->num_rx_rings, sizeof(*dp->rx_rings),\n\t\t\t       GFP_KERNEL);\n\tif (!dp->rx_rings)\n\t\treturn -ENOMEM;\n\n\tfor (r = 0; r < dp->num_rx_rings; r++) {\n\t\tnfp_net_rx_ring_init(&dp->rx_rings[r], &nn->r_vecs[r], r);\n\n\t\tif (nfp_net_rx_ring_alloc(dp, &dp->rx_rings[r]))\n\t\t\tgoto err_free_prev;\n\n\t\tif (nfp_net_rx_ring_bufs_alloc(dp, &dp->rx_rings[r]))\n\t\t\tgoto err_free_ring;\n\t}\n\n\treturn 0;\n\nerr_free_prev:\n\twhile (r--) {\n\t\tnfp_net_rx_ring_bufs_free(dp, &dp->rx_rings[r]);\nerr_free_ring:\n\t\tnfp_net_rx_ring_free(&dp->rx_rings[r]);\n\t}\n\tkfree(dp->rx_rings);\n\treturn -ENOMEM;\n}\n\nvoid nfp_net_rx_rings_free(struct nfp_net_dp *dp)\n{\n\tunsigned int r;\n\n\tfor (r = 0; r < dp->num_rx_rings; r++) {\n\t\tnfp_net_rx_ring_bufs_free(dp, &dp->rx_rings[r]);\n\t\tnfp_net_rx_ring_free(&dp->rx_rings[r]);\n\t}\n\n\tkfree(dp->rx_rings);\n}\n\nvoid\nnfp_net_rx_ring_hw_cfg_write(struct nfp_net *nn,\n\t\t\t     struct nfp_net_rx_ring *rx_ring, unsigned int idx)\n{\n\t \n\tnn_writeq(nn, NFP_NET_CFG_RXR_ADDR(idx), rx_ring->dma);\n\tnn_writeb(nn, NFP_NET_CFG_RXR_SZ(idx), ilog2(rx_ring->cnt));\n\tnn_writeb(nn, NFP_NET_CFG_RXR_VEC(idx), rx_ring->r_vec->irq_entry);\n}\n\nvoid\nnfp_net_tx_ring_hw_cfg_write(struct nfp_net *nn,\n\t\t\t     struct nfp_net_tx_ring *tx_ring, unsigned int idx)\n{\n\tnn_writeq(nn, NFP_NET_CFG_TXR_ADDR(idx), tx_ring->dma);\n\tif (tx_ring->txrwb) {\n\t\t*tx_ring->txrwb = 0;\n\t\tnn_writeq(nn, NFP_NET_CFG_TXR_WB_ADDR(idx),\n\t\t\t  nn->dp.txrwb_dma + idx * sizeof(u64));\n\t}\n\tnn_writeb(nn, NFP_NET_CFG_TXR_SZ(idx), ilog2(tx_ring->cnt));\n\tnn_writeb(nn, NFP_NET_CFG_TXR_VEC(idx), tx_ring->r_vec->irq_entry);\n}\n\nvoid nfp_net_vec_clear_ring_data(struct nfp_net *nn, unsigned int idx)\n{\n\tnn_writeq(nn, NFP_NET_CFG_RXR_ADDR(idx), 0);\n\tnn_writeb(nn, NFP_NET_CFG_RXR_SZ(idx), 0);\n\tnn_writeb(nn, NFP_NET_CFG_RXR_VEC(idx), 0);\n\n\tnn_writeq(nn, NFP_NET_CFG_TXR_ADDR(idx), 0);\n\tnn_writeq(nn, NFP_NET_CFG_TXR_WB_ADDR(idx), 0);\n\tnn_writeb(nn, NFP_NET_CFG_TXR_SZ(idx), 0);\n\tnn_writeb(nn, NFP_NET_CFG_TXR_VEC(idx), 0);\n}\n\nnetdev_tx_t nfp_net_tx(struct sk_buff *skb, struct net_device *netdev)\n{\n\tstruct nfp_net *nn = netdev_priv(netdev);\n\n\treturn nn->dp.ops->xmit(skb, netdev);\n}\n\nbool __nfp_ctrl_tx(struct nfp_net *nn, struct sk_buff *skb)\n{\n\tstruct nfp_net_r_vector *r_vec = &nn->r_vecs[0];\n\n\treturn nn->dp.ops->ctrl_tx_one(nn, r_vec, skb, false);\n}\n\nbool nfp_ctrl_tx(struct nfp_net *nn, struct sk_buff *skb)\n{\n\tstruct nfp_net_r_vector *r_vec = &nn->r_vecs[0];\n\tbool ret;\n\n\tspin_lock_bh(&r_vec->lock);\n\tret = nn->dp.ops->ctrl_tx_one(nn, r_vec, skb, false);\n\tspin_unlock_bh(&r_vec->lock);\n\n\treturn ret;\n}\n\nbool nfp_net_vlan_strip(struct sk_buff *skb, const struct nfp_net_rx_desc *rxd,\n\t\t\tconst struct nfp_meta_parsed *meta)\n{\n\tu16 tpid = 0, tci = 0;\n\n\tif (rxd->rxd.flags & PCIE_DESC_RX_VLAN) {\n\t\ttpid = ETH_P_8021Q;\n\t\ttci = le16_to_cpu(rxd->rxd.vlan);\n\t} else if (meta->vlan.stripped) {\n\t\tif (meta->vlan.tpid == NFP_NET_VLAN_CTAG)\n\t\t\ttpid = ETH_P_8021Q;\n\t\telse if (meta->vlan.tpid == NFP_NET_VLAN_STAG)\n\t\t\ttpid = ETH_P_8021AD;\n\t\telse\n\t\t\treturn false;\n\n\t\ttci = meta->vlan.tci;\n\t}\n\tif (tpid)\n\t\t__vlan_hwaccel_put_tag(skb, htons(tpid), tci);\n\n\treturn true;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}