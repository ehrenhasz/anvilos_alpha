{
  "module_name": "main.c",
  "hash_id": "fec0fe5c433707ac7adbb3cf5f5b85e44d83367578619920b86b49053b8972e8",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/atheros/alx/main.c",
  "human_readable_source": " \n\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/interrupt.h>\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <linux/if_vlan.h>\n#include <linux/mdio.h>\n#include <linux/bitops.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <net/ip6_checksum.h>\n#include <linux/crc32.h>\n#include \"alx.h\"\n#include \"hw.h\"\n#include \"reg.h\"\n\nstatic const char alx_drv_name[] = \"alx\";\n\nstatic void alx_free_txbuf(struct alx_tx_queue *txq, int entry)\n{\n\tstruct alx_buffer *txb = &txq->bufs[entry];\n\n\tif (dma_unmap_len(txb, size)) {\n\t\tdma_unmap_single(txq->dev,\n\t\t\t\t dma_unmap_addr(txb, dma),\n\t\t\t\t dma_unmap_len(txb, size),\n\t\t\t\t DMA_TO_DEVICE);\n\t\tdma_unmap_len_set(txb, size, 0);\n\t}\n\n\tif (txb->skb) {\n\t\tdev_kfree_skb_any(txb->skb);\n\t\ttxb->skb = NULL;\n\t}\n}\n\nstatic int alx_refill_rx_ring(struct alx_priv *alx, gfp_t gfp)\n{\n\tstruct alx_rx_queue *rxq = alx->qnapi[0]->rxq;\n\tstruct sk_buff *skb;\n\tstruct alx_buffer *cur_buf;\n\tdma_addr_t dma;\n\tu16 cur, next, count = 0;\n\n\tnext = cur = rxq->write_idx;\n\tif (++next == alx->rx_ringsz)\n\t\tnext = 0;\n\tcur_buf = &rxq->bufs[cur];\n\n\twhile (!cur_buf->skb && next != rxq->read_idx) {\n\t\tstruct alx_rfd *rfd = &rxq->rfd[cur];\n\n\t\t \n\t\tskb = __netdev_alloc_skb(alx->dev, alx->rxbuf_size + 64, gfp);\n\t\tif (!skb)\n\t\t\tbreak;\n\n\t\tif (((unsigned long)skb->data & 0xfff) == 0xfc0)\n\t\t\tskb_reserve(skb, 64);\n\n\t\tdma = dma_map_single(&alx->hw.pdev->dev,\n\t\t\t\t     skb->data, alx->rxbuf_size,\n\t\t\t\t     DMA_FROM_DEVICE);\n\t\tif (dma_mapping_error(&alx->hw.pdev->dev, dma)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (WARN_ON(dma & 3)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\tbreak;\n\t\t}\n\n\t\tcur_buf->skb = skb;\n\t\tdma_unmap_len_set(cur_buf, size, alx->rxbuf_size);\n\t\tdma_unmap_addr_set(cur_buf, dma, dma);\n\t\trfd->addr = cpu_to_le64(dma);\n\n\t\tcur = next;\n\t\tif (++next == alx->rx_ringsz)\n\t\t\tnext = 0;\n\t\tcur_buf = &rxq->bufs[cur];\n\t\tcount++;\n\t}\n\n\tif (count) {\n\t\t \n\t\twmb();\n\t\trxq->write_idx = cur;\n\t\talx_write_mem16(&alx->hw, ALX_RFD_PIDX, cur);\n\t}\n\n\treturn count;\n}\n\nstatic struct alx_tx_queue *alx_tx_queue_mapping(struct alx_priv *alx,\n\t\t\t\t\t\t struct sk_buff *skb)\n{\n\tunsigned int r_idx = skb->queue_mapping;\n\n\tif (r_idx >= alx->num_txq)\n\t\tr_idx = r_idx % alx->num_txq;\n\n\treturn alx->qnapi[r_idx]->txq;\n}\n\nstatic struct netdev_queue *alx_get_tx_queue(const struct alx_tx_queue *txq)\n{\n\treturn netdev_get_tx_queue(txq->netdev, txq->queue_idx);\n}\n\nstatic inline int alx_tpd_avail(struct alx_tx_queue *txq)\n{\n\tif (txq->write_idx >= txq->read_idx)\n\t\treturn txq->count + txq->read_idx - txq->write_idx - 1;\n\treturn txq->read_idx - txq->write_idx - 1;\n}\n\nstatic bool alx_clean_tx_irq(struct alx_tx_queue *txq)\n{\n\tstruct alx_priv *alx;\n\tstruct netdev_queue *tx_queue;\n\tu16 hw_read_idx, sw_read_idx;\n\tunsigned int total_bytes = 0, total_packets = 0;\n\tint budget = ALX_DEFAULT_TX_WORK;\n\n\talx = netdev_priv(txq->netdev);\n\ttx_queue = alx_get_tx_queue(txq);\n\n\tsw_read_idx = txq->read_idx;\n\thw_read_idx = alx_read_mem16(&alx->hw, txq->c_reg);\n\n\tif (sw_read_idx != hw_read_idx) {\n\t\twhile (sw_read_idx != hw_read_idx && budget > 0) {\n\t\t\tstruct sk_buff *skb;\n\n\t\t\tskb = txq->bufs[sw_read_idx].skb;\n\t\t\tif (skb) {\n\t\t\t\ttotal_bytes += skb->len;\n\t\t\t\ttotal_packets++;\n\t\t\t\tbudget--;\n\t\t\t}\n\n\t\t\talx_free_txbuf(txq, sw_read_idx);\n\n\t\t\tif (++sw_read_idx == txq->count)\n\t\t\t\tsw_read_idx = 0;\n\t\t}\n\t\ttxq->read_idx = sw_read_idx;\n\n\t\tnetdev_tx_completed_queue(tx_queue, total_packets, total_bytes);\n\t}\n\n\tif (netif_tx_queue_stopped(tx_queue) && netif_carrier_ok(alx->dev) &&\n\t    alx_tpd_avail(txq) > txq->count / 4)\n\t\tnetif_tx_wake_queue(tx_queue);\n\n\treturn sw_read_idx == hw_read_idx;\n}\n\nstatic void alx_schedule_link_check(struct alx_priv *alx)\n{\n\tschedule_work(&alx->link_check_wk);\n}\n\nstatic void alx_schedule_reset(struct alx_priv *alx)\n{\n\tschedule_work(&alx->reset_wk);\n}\n\nstatic int alx_clean_rx_irq(struct alx_rx_queue *rxq, int budget)\n{\n\tstruct alx_priv *alx;\n\tstruct alx_rrd *rrd;\n\tstruct alx_buffer *rxb;\n\tstruct sk_buff *skb;\n\tu16 length, rfd_cleaned = 0;\n\tint work = 0;\n\n\talx = netdev_priv(rxq->netdev);\n\n\twhile (work < budget) {\n\t\trrd = &rxq->rrd[rxq->rrd_read_idx];\n\t\tif (!(rrd->word3 & cpu_to_le32(1 << RRD_UPDATED_SHIFT)))\n\t\t\tbreak;\n\t\trrd->word3 &= ~cpu_to_le32(1 << RRD_UPDATED_SHIFT);\n\n\t\tif (ALX_GET_FIELD(le32_to_cpu(rrd->word0),\n\t\t\t\t  RRD_SI) != rxq->read_idx ||\n\t\t    ALX_GET_FIELD(le32_to_cpu(rrd->word0),\n\t\t\t\t  RRD_NOR) != 1) {\n\t\t\talx_schedule_reset(alx);\n\t\t\treturn work;\n\t\t}\n\n\t\trxb = &rxq->bufs[rxq->read_idx];\n\t\tdma_unmap_single(rxq->dev,\n\t\t\t\t dma_unmap_addr(rxb, dma),\n\t\t\t\t dma_unmap_len(rxb, size),\n\t\t\t\t DMA_FROM_DEVICE);\n\t\tdma_unmap_len_set(rxb, size, 0);\n\t\tskb = rxb->skb;\n\t\trxb->skb = NULL;\n\n\t\tif (rrd->word3 & cpu_to_le32(1 << RRD_ERR_RES_SHIFT) ||\n\t\t    rrd->word3 & cpu_to_le32(1 << RRD_ERR_LEN_SHIFT)) {\n\t\t\trrd->word3 = 0;\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tgoto next_pkt;\n\t\t}\n\n\t\tlength = ALX_GET_FIELD(le32_to_cpu(rrd->word3),\n\t\t\t\t       RRD_PKTLEN) - ETH_FCS_LEN;\n\t\tskb_put(skb, length);\n\t\tskb->protocol = eth_type_trans(skb, rxq->netdev);\n\n\t\tskb_checksum_none_assert(skb);\n\t\tif (alx->dev->features & NETIF_F_RXCSUM &&\n\t\t    !(rrd->word3 & (cpu_to_le32(1 << RRD_ERR_L4_SHIFT) |\n\t\t\t\t    cpu_to_le32(1 << RRD_ERR_IPV4_SHIFT)))) {\n\t\t\tswitch (ALX_GET_FIELD(le32_to_cpu(rrd->word2),\n\t\t\t\t\t      RRD_PID)) {\n\t\t\tcase RRD_PID_IPV6UDP:\n\t\t\tcase RRD_PID_IPV4UDP:\n\t\t\tcase RRD_PID_IPV4TCP:\n\t\t\tcase RRD_PID_IPV6TCP:\n\t\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tnapi_gro_receive(&rxq->np->napi, skb);\n\t\twork++;\n\nnext_pkt:\n\t\tif (++rxq->read_idx == rxq->count)\n\t\t\trxq->read_idx = 0;\n\t\tif (++rxq->rrd_read_idx == rxq->count)\n\t\t\trxq->rrd_read_idx = 0;\n\n\t\tif (++rfd_cleaned > ALX_RX_ALLOC_THRESH)\n\t\t\trfd_cleaned -= alx_refill_rx_ring(alx, GFP_ATOMIC);\n\t}\n\n\tif (rfd_cleaned)\n\t\talx_refill_rx_ring(alx, GFP_ATOMIC);\n\n\treturn work;\n}\n\nstatic int alx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct alx_napi *np = container_of(napi, struct alx_napi, napi);\n\tstruct alx_priv *alx = np->alx;\n\tstruct alx_hw *hw = &alx->hw;\n\tunsigned long flags;\n\tbool tx_complete = true;\n\tint work = 0;\n\n\tif (np->txq)\n\t\ttx_complete = alx_clean_tx_irq(np->txq);\n\tif (np->rxq)\n\t\twork = alx_clean_rx_irq(np->rxq, budget);\n\n\tif (!tx_complete || work == budget)\n\t\treturn budget;\n\n\tnapi_complete_done(&np->napi, work);\n\n\t \n\tif (alx->hw.pdev->msix_enabled) {\n\t\talx_mask_msix(hw, np->vec_idx, false);\n\t} else {\n\t\tspin_lock_irqsave(&alx->irq_lock, flags);\n\t\talx->int_mask |= ALX_ISR_TX_Q0 | ALX_ISR_RX_Q0;\n\t\talx_write_mem32(hw, ALX_IMR, alx->int_mask);\n\t\tspin_unlock_irqrestore(&alx->irq_lock, flags);\n\t}\n\n\talx_post_write(hw);\n\n\treturn work;\n}\n\nstatic bool alx_intr_handle_misc(struct alx_priv *alx, u32 intr)\n{\n\tstruct alx_hw *hw = &alx->hw;\n\n\tif (intr & ALX_ISR_FATAL) {\n\t\tnetif_warn(alx, hw, alx->dev,\n\t\t\t   \"fatal interrupt 0x%x, resetting\\n\", intr);\n\t\talx_schedule_reset(alx);\n\t\treturn true;\n\t}\n\n\tif (intr & ALX_ISR_ALERT)\n\t\tnetdev_warn(alx->dev, \"alert interrupt: 0x%x\\n\", intr);\n\n\tif (intr & ALX_ISR_PHY) {\n\t\t \n\t\talx->int_mask &= ~ALX_ISR_PHY;\n\t\talx_write_mem32(hw, ALX_IMR, alx->int_mask);\n\t\talx_schedule_link_check(alx);\n\t}\n\n\treturn false;\n}\n\nstatic irqreturn_t alx_intr_handle(struct alx_priv *alx, u32 intr)\n{\n\tstruct alx_hw *hw = &alx->hw;\n\n\tspin_lock(&alx->irq_lock);\n\n\t \n\talx_write_mem32(hw, ALX_ISR, intr | ALX_ISR_DIS);\n\tintr &= alx->int_mask;\n\n\tif (alx_intr_handle_misc(alx, intr))\n\t\tgoto out;\n\n\tif (intr & (ALX_ISR_TX_Q0 | ALX_ISR_RX_Q0)) {\n\t\tnapi_schedule(&alx->qnapi[0]->napi);\n\t\t \n\t\talx->int_mask &= ~ALX_ISR_ALL_QUEUES;\n\t\talx_write_mem32(hw, ALX_IMR, alx->int_mask);\n\t}\n\n\talx_write_mem32(hw, ALX_ISR, 0);\n\n out:\n\tspin_unlock(&alx->irq_lock);\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t alx_intr_msix_ring(int irq, void *data)\n{\n\tstruct alx_napi *np = data;\n\tstruct alx_hw *hw = &np->alx->hw;\n\n\t \n\talx_mask_msix(hw, np->vec_idx, true);\n\t \n\talx_write_mem32(hw, ALX_ISR, np->vec_mask);\n\n\tnapi_schedule(&np->napi);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t alx_intr_msix_misc(int irq, void *data)\n{\n\tstruct alx_priv *alx = data;\n\tstruct alx_hw *hw = &alx->hw;\n\tu32 intr;\n\n\t \n\talx_mask_msix(hw, 0, true);\n\n\t \n\tintr = alx_read_mem32(hw, ALX_ISR);\n\tintr &= (alx->int_mask & ~ALX_ISR_ALL_QUEUES);\n\n\tif (alx_intr_handle_misc(alx, intr))\n\t\treturn IRQ_HANDLED;\n\n\t \n\talx_write_mem32(hw, ALX_ISR, intr);\n\n\t \n\talx_mask_msix(hw, 0, false);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t alx_intr_msi(int irq, void *data)\n{\n\tstruct alx_priv *alx = data;\n\n\treturn alx_intr_handle(alx, alx_read_mem32(&alx->hw, ALX_ISR));\n}\n\nstatic irqreturn_t alx_intr_legacy(int irq, void *data)\n{\n\tstruct alx_priv *alx = data;\n\tstruct alx_hw *hw = &alx->hw;\n\tu32 intr;\n\n\tintr = alx_read_mem32(hw, ALX_ISR);\n\n\tif (intr & ALX_ISR_DIS || !(intr & alx->int_mask))\n\t\treturn IRQ_NONE;\n\n\treturn alx_intr_handle(alx, intr);\n}\n\nstatic const u16 txring_header_reg[] = {ALX_TPD_PRI0_ADDR_LO,\n\t\t\t\t\tALX_TPD_PRI1_ADDR_LO,\n\t\t\t\t\tALX_TPD_PRI2_ADDR_LO,\n\t\t\t\t\tALX_TPD_PRI3_ADDR_LO};\n\nstatic void alx_init_ring_ptrs(struct alx_priv *alx)\n{\n\tstruct alx_hw *hw = &alx->hw;\n\tu32 addr_hi = ((u64)alx->descmem.dma) >> 32;\n\tstruct alx_napi *np;\n\tint i;\n\n\tfor (i = 0; i < alx->num_napi; i++) {\n\t\tnp = alx->qnapi[i];\n\t\tif (np->txq) {\n\t\t\tnp->txq->read_idx = 0;\n\t\t\tnp->txq->write_idx = 0;\n\t\t\talx_write_mem32(hw,\n\t\t\t\t\ttxring_header_reg[np->txq->queue_idx],\n\t\t\t\t\tnp->txq->tpd_dma);\n\t\t}\n\n\t\tif (np->rxq) {\n\t\t\tnp->rxq->read_idx = 0;\n\t\t\tnp->rxq->write_idx = 0;\n\t\t\tnp->rxq->rrd_read_idx = 0;\n\t\t\talx_write_mem32(hw, ALX_RRD_ADDR_LO, np->rxq->rrd_dma);\n\t\t\talx_write_mem32(hw, ALX_RFD_ADDR_LO, np->rxq->rfd_dma);\n\t\t}\n\t}\n\n\talx_write_mem32(hw, ALX_TX_BASE_ADDR_HI, addr_hi);\n\talx_write_mem32(hw, ALX_TPD_RING_SZ, alx->tx_ringsz);\n\n\talx_write_mem32(hw, ALX_RX_BASE_ADDR_HI, addr_hi);\n\talx_write_mem32(hw, ALX_RRD_RING_SZ, alx->rx_ringsz);\n\talx_write_mem32(hw, ALX_RFD_RING_SZ, alx->rx_ringsz);\n\talx_write_mem32(hw, ALX_RFD_BUF_SZ, alx->rxbuf_size);\n\n\t \n\talx_write_mem32(hw, ALX_SRAM9, ALX_SRAM_LOAD_PTR);\n}\n\nstatic void alx_free_txring_buf(struct alx_tx_queue *txq)\n{\n\tint i;\n\n\tif (!txq->bufs)\n\t\treturn;\n\n\tfor (i = 0; i < txq->count; i++)\n\t\talx_free_txbuf(txq, i);\n\n\tmemset(txq->bufs, 0, txq->count * sizeof(struct alx_buffer));\n\tmemset(txq->tpd, 0, txq->count * sizeof(struct alx_txd));\n\ttxq->write_idx = 0;\n\ttxq->read_idx = 0;\n\n\tnetdev_tx_reset_queue(alx_get_tx_queue(txq));\n}\n\nstatic void alx_free_rxring_buf(struct alx_rx_queue *rxq)\n{\n\tstruct alx_buffer *cur_buf;\n\tu16 i;\n\n\tif (!rxq->bufs)\n\t\treturn;\n\n\tfor (i = 0; i < rxq->count; i++) {\n\t\tcur_buf = rxq->bufs + i;\n\t\tif (cur_buf->skb) {\n\t\t\tdma_unmap_single(rxq->dev,\n\t\t\t\t\t dma_unmap_addr(cur_buf, dma),\n\t\t\t\t\t dma_unmap_len(cur_buf, size),\n\t\t\t\t\t DMA_FROM_DEVICE);\n\t\t\tdev_kfree_skb(cur_buf->skb);\n\t\t\tcur_buf->skb = NULL;\n\t\t\tdma_unmap_len_set(cur_buf, size, 0);\n\t\t\tdma_unmap_addr_set(cur_buf, dma, 0);\n\t\t}\n\t}\n\n\trxq->write_idx = 0;\n\trxq->read_idx = 0;\n\trxq->rrd_read_idx = 0;\n}\n\nstatic void alx_free_buffers(struct alx_priv *alx)\n{\n\tint i;\n\n\tfor (i = 0; i < alx->num_txq; i++)\n\t\tif (alx->qnapi[i] && alx->qnapi[i]->txq)\n\t\t\talx_free_txring_buf(alx->qnapi[i]->txq);\n\n\tif (alx->qnapi[0] && alx->qnapi[0]->rxq)\n\t\talx_free_rxring_buf(alx->qnapi[0]->rxq);\n}\n\nstatic int alx_reinit_rings(struct alx_priv *alx)\n{\n\talx_free_buffers(alx);\n\n\talx_init_ring_ptrs(alx);\n\n\tif (!alx_refill_rx_ring(alx, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void alx_add_mc_addr(struct alx_hw *hw, const u8 *addr, u32 *mc_hash)\n{\n\tu32 crc32, bit, reg;\n\n\tcrc32 = ether_crc(ETH_ALEN, addr);\n\treg = (crc32 >> 31) & 0x1;\n\tbit = (crc32 >> 26) & 0x1F;\n\n\tmc_hash[reg] |= BIT(bit);\n}\n\nstatic void __alx_set_rx_mode(struct net_device *netdev)\n{\n\tstruct alx_priv *alx = netdev_priv(netdev);\n\tstruct alx_hw *hw = &alx->hw;\n\tstruct netdev_hw_addr *ha;\n\tu32 mc_hash[2] = {};\n\n\tif (!(netdev->flags & IFF_ALLMULTI)) {\n\t\tnetdev_for_each_mc_addr(ha, netdev)\n\t\t\talx_add_mc_addr(hw, ha->addr, mc_hash);\n\n\t\talx_write_mem32(hw, ALX_HASH_TBL0, mc_hash[0]);\n\t\talx_write_mem32(hw, ALX_HASH_TBL1, mc_hash[1]);\n\t}\n\n\thw->rx_ctrl &= ~(ALX_MAC_CTRL_MULTIALL_EN | ALX_MAC_CTRL_PROMISC_EN);\n\tif (netdev->flags & IFF_PROMISC)\n\t\thw->rx_ctrl |= ALX_MAC_CTRL_PROMISC_EN;\n\tif (netdev->flags & IFF_ALLMULTI)\n\t\thw->rx_ctrl |= ALX_MAC_CTRL_MULTIALL_EN;\n\n\talx_write_mem32(hw, ALX_MAC_CTRL, hw->rx_ctrl);\n}\n\nstatic void alx_set_rx_mode(struct net_device *netdev)\n{\n\t__alx_set_rx_mode(netdev);\n}\n\nstatic int alx_set_mac_address(struct net_device *netdev, void *data)\n{\n\tstruct alx_priv *alx = netdev_priv(netdev);\n\tstruct alx_hw *hw = &alx->hw;\n\tstruct sockaddr *addr = data;\n\n\tif (!is_valid_ether_addr(addr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\tif (netdev->addr_assign_type & NET_ADDR_RANDOM)\n\t\tnetdev->addr_assign_type ^= NET_ADDR_RANDOM;\n\n\teth_hw_addr_set(netdev, addr->sa_data);\n\tmemcpy(hw->mac_addr, addr->sa_data, netdev->addr_len);\n\talx_set_macaddr(hw, hw->mac_addr);\n\n\treturn 0;\n}\n\nstatic int alx_alloc_tx_ring(struct alx_priv *alx, struct alx_tx_queue *txq,\n\t\t\t     int offset)\n{\n\ttxq->bufs = kcalloc(txq->count, sizeof(struct alx_buffer), GFP_KERNEL);\n\tif (!txq->bufs)\n\t\treturn -ENOMEM;\n\n\ttxq->tpd = alx->descmem.virt + offset;\n\ttxq->tpd_dma = alx->descmem.dma + offset;\n\toffset += sizeof(struct alx_txd) * txq->count;\n\n\treturn offset;\n}\n\nstatic int alx_alloc_rx_ring(struct alx_priv *alx, struct alx_rx_queue *rxq,\n\t\t\t     int offset)\n{\n\trxq->bufs = kcalloc(rxq->count, sizeof(struct alx_buffer), GFP_KERNEL);\n\tif (!rxq->bufs)\n\t\treturn -ENOMEM;\n\n\trxq->rrd = alx->descmem.virt + offset;\n\trxq->rrd_dma = alx->descmem.dma + offset;\n\toffset += sizeof(struct alx_rrd) * rxq->count;\n\n\trxq->rfd = alx->descmem.virt + offset;\n\trxq->rfd_dma = alx->descmem.dma + offset;\n\toffset += sizeof(struct alx_rfd) * rxq->count;\n\n\treturn offset;\n}\n\nstatic int alx_alloc_rings(struct alx_priv *alx)\n{\n\tint i, offset = 0;\n\n\t \n\talx->descmem.size = sizeof(struct alx_txd) * alx->tx_ringsz *\n\t\t\t    alx->num_txq +\n\t\t\t    sizeof(struct alx_rrd) * alx->rx_ringsz +\n\t\t\t    sizeof(struct alx_rfd) * alx->rx_ringsz;\n\talx->descmem.virt = dma_alloc_coherent(&alx->hw.pdev->dev,\n\t\t\t\t\t       alx->descmem.size,\n\t\t\t\t\t       &alx->descmem.dma, GFP_KERNEL);\n\tif (!alx->descmem.virt)\n\t\treturn -ENOMEM;\n\n\t \n\tBUILD_BUG_ON(sizeof(struct alx_txd) % 8);\n\tBUILD_BUG_ON(sizeof(struct alx_rrd) % 8);\n\n\tfor (i = 0; i < alx->num_txq; i++) {\n\t\toffset = alx_alloc_tx_ring(alx, alx->qnapi[i]->txq, offset);\n\t\tif (offset < 0) {\n\t\t\tnetdev_err(alx->dev, \"Allocation of tx buffer failed!\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\toffset = alx_alloc_rx_ring(alx, alx->qnapi[0]->rxq, offset);\n\tif (offset < 0) {\n\t\tnetdev_err(alx->dev, \"Allocation of rx buffer failed!\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void alx_free_rings(struct alx_priv *alx)\n{\n\tint i;\n\n\talx_free_buffers(alx);\n\n\tfor (i = 0; i < alx->num_txq; i++)\n\t\tif (alx->qnapi[i] && alx->qnapi[i]->txq)\n\t\t\tkfree(alx->qnapi[i]->txq->bufs);\n\n\tif (alx->qnapi[0] && alx->qnapi[0]->rxq)\n\t\tkfree(alx->qnapi[0]->rxq->bufs);\n\n\tif (alx->descmem.virt)\n\t\tdma_free_coherent(&alx->hw.pdev->dev,\n\t\t\t\t  alx->descmem.size,\n\t\t\t\t  alx->descmem.virt,\n\t\t\t\t  alx->descmem.dma);\n}\n\nstatic void alx_free_napis(struct alx_priv *alx)\n{\n\tstruct alx_napi *np;\n\tint i;\n\n\tfor (i = 0; i < alx->num_napi; i++) {\n\t\tnp = alx->qnapi[i];\n\t\tif (!np)\n\t\t\tcontinue;\n\n\t\tnetif_napi_del(&np->napi);\n\t\tkfree(np->txq);\n\t\tkfree(np->rxq);\n\t\tkfree(np);\n\t\talx->qnapi[i] = NULL;\n\t}\n}\n\nstatic const u16 tx_pidx_reg[] = {ALX_TPD_PRI0_PIDX, ALX_TPD_PRI1_PIDX,\n\t\t\t\t  ALX_TPD_PRI2_PIDX, ALX_TPD_PRI3_PIDX};\nstatic const u16 tx_cidx_reg[] = {ALX_TPD_PRI0_CIDX, ALX_TPD_PRI1_CIDX,\n\t\t\t\t  ALX_TPD_PRI2_CIDX, ALX_TPD_PRI3_CIDX};\nstatic const u32 tx_vect_mask[] = {ALX_ISR_TX_Q0, ALX_ISR_TX_Q1,\n\t\t\t\t   ALX_ISR_TX_Q2, ALX_ISR_TX_Q3};\nstatic const u32 rx_vect_mask[] = {ALX_ISR_RX_Q0, ALX_ISR_RX_Q1,\n\t\t\t\t   ALX_ISR_RX_Q2, ALX_ISR_RX_Q3,\n\t\t\t\t   ALX_ISR_RX_Q4, ALX_ISR_RX_Q5,\n\t\t\t\t   ALX_ISR_RX_Q6, ALX_ISR_RX_Q7};\n\nstatic int alx_alloc_napis(struct alx_priv *alx)\n{\n\tstruct alx_napi *np;\n\tstruct alx_rx_queue *rxq;\n\tstruct alx_tx_queue *txq;\n\tint i;\n\n\talx->int_mask &= ~ALX_ISR_ALL_QUEUES;\n\n\t \n\tfor (i = 0; i < alx->num_napi; i++) {\n\t\tnp = kzalloc(sizeof(struct alx_napi), GFP_KERNEL);\n\t\tif (!np)\n\t\t\tgoto err_out;\n\n\t\tnp->alx = alx;\n\t\tnetif_napi_add(alx->dev, &np->napi, alx_poll);\n\t\talx->qnapi[i] = np;\n\t}\n\n\t \n\tfor (i = 0; i < alx->num_txq; i++) {\n\t\tnp = alx->qnapi[i];\n\t\ttxq = kzalloc(sizeof(*txq), GFP_KERNEL);\n\t\tif (!txq)\n\t\t\tgoto err_out;\n\n\t\tnp->txq = txq;\n\t\ttxq->p_reg = tx_pidx_reg[i];\n\t\ttxq->c_reg = tx_cidx_reg[i];\n\t\ttxq->queue_idx = i;\n\t\ttxq->count = alx->tx_ringsz;\n\t\ttxq->netdev = alx->dev;\n\t\ttxq->dev = &alx->hw.pdev->dev;\n\t\tnp->vec_mask |= tx_vect_mask[i];\n\t\talx->int_mask |= tx_vect_mask[i];\n\t}\n\n\t \n\tnp = alx->qnapi[0];\n\trxq = kzalloc(sizeof(*rxq), GFP_KERNEL);\n\tif (!rxq)\n\t\tgoto err_out;\n\n\tnp->rxq = rxq;\n\trxq->np = alx->qnapi[0];\n\trxq->queue_idx = 0;\n\trxq->count = alx->rx_ringsz;\n\trxq->netdev = alx->dev;\n\trxq->dev = &alx->hw.pdev->dev;\n\tnp->vec_mask |= rx_vect_mask[0];\n\talx->int_mask |= rx_vect_mask[0];\n\n\treturn 0;\n\nerr_out:\n\tnetdev_err(alx->dev, \"error allocating internal structures\\n\");\n\talx_free_napis(alx);\n\treturn -ENOMEM;\n}\n\nstatic const int txq_vec_mapping_shift[] = {\n\t0, ALX_MSI_MAP_TBL1_TXQ0_SHIFT,\n\t0, ALX_MSI_MAP_TBL1_TXQ1_SHIFT,\n\t1, ALX_MSI_MAP_TBL2_TXQ2_SHIFT,\n\t1, ALX_MSI_MAP_TBL2_TXQ3_SHIFT,\n};\n\nstatic void alx_config_vector_mapping(struct alx_priv *alx)\n{\n\tstruct alx_hw *hw = &alx->hw;\n\tu32 tbl[2] = {0, 0};\n\tint i, vector, idx, shift;\n\n\tif (alx->hw.pdev->msix_enabled) {\n\t\t \n\t\tfor (i = 0, vector = 1; i < alx->num_txq; i++, vector++) {\n\t\t\tidx = txq_vec_mapping_shift[i * 2];\n\t\t\tshift = txq_vec_mapping_shift[i * 2 + 1];\n\t\t\ttbl[idx] |= vector << shift;\n\t\t}\n\n\t\t \n\t\ttbl[0] |= 1 << ALX_MSI_MAP_TBL1_RXQ0_SHIFT;\n\t}\n\n\talx_write_mem32(hw, ALX_MSI_MAP_TBL1, tbl[0]);\n\talx_write_mem32(hw, ALX_MSI_MAP_TBL2, tbl[1]);\n\talx_write_mem32(hw, ALX_MSI_ID_MAP, 0);\n}\n\nstatic int alx_enable_msix(struct alx_priv *alx)\n{\n\tint err, num_vec, num_txq, num_rxq;\n\n\tnum_txq = min_t(int, num_online_cpus(), ALX_MAX_TX_QUEUES);\n\tnum_rxq = 1;\n\tnum_vec = max_t(int, num_txq, num_rxq) + 1;\n\n\terr = pci_alloc_irq_vectors(alx->hw.pdev, num_vec, num_vec,\n\t\t\tPCI_IRQ_MSIX);\n\tif (err < 0) {\n\t\tnetdev_warn(alx->dev, \"Enabling MSI-X interrupts failed!\\n\");\n\t\treturn err;\n\t}\n\n\talx->num_vec = num_vec;\n\talx->num_napi = num_vec - 1;\n\talx->num_txq = num_txq;\n\talx->num_rxq = num_rxq;\n\n\treturn err;\n}\n\nstatic int alx_request_msix(struct alx_priv *alx)\n{\n\tstruct net_device *netdev = alx->dev;\n\tint i, err, vector = 0, free_vector = 0;\n\n\terr = request_irq(pci_irq_vector(alx->hw.pdev, 0), alx_intr_msix_misc,\n\t\t\t  0, netdev->name, alx);\n\tif (err)\n\t\tgoto out_err;\n\n\tfor (i = 0; i < alx->num_napi; i++) {\n\t\tstruct alx_napi *np = alx->qnapi[i];\n\n\t\tvector++;\n\n\t\tif (np->txq && np->rxq)\n\t\t\tsprintf(np->irq_lbl, \"%s-TxRx-%u\", netdev->name,\n\t\t\t\tnp->txq->queue_idx);\n\t\telse if (np->txq)\n\t\t\tsprintf(np->irq_lbl, \"%s-tx-%u\", netdev->name,\n\t\t\t\tnp->txq->queue_idx);\n\t\telse if (np->rxq)\n\t\t\tsprintf(np->irq_lbl, \"%s-rx-%u\", netdev->name,\n\t\t\t\tnp->rxq->queue_idx);\n\t\telse\n\t\t\tsprintf(np->irq_lbl, \"%s-unused\", netdev->name);\n\n\t\tnp->vec_idx = vector;\n\t\terr = request_irq(pci_irq_vector(alx->hw.pdev, vector),\n\t\t\t\t  alx_intr_msix_ring, 0, np->irq_lbl, np);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t}\n\treturn 0;\n\nout_free:\n\tfree_irq(pci_irq_vector(alx->hw.pdev, free_vector++), alx);\n\n\tvector--;\n\tfor (i = 0; i < vector; i++)\n\t\tfree_irq(pci_irq_vector(alx->hw.pdev,free_vector++),\n\t\t\t alx->qnapi[i]);\n\nout_err:\n\treturn err;\n}\n\nstatic int alx_init_intr(struct alx_priv *alx)\n{\n\tint ret;\n\n\tret = pci_alloc_irq_vectors(alx->hw.pdev, 1, 1,\n\t\t\tPCI_IRQ_MSI | PCI_IRQ_LEGACY);\n\tif (ret < 0)\n\t\treturn ret;\n\n\talx->num_vec = 1;\n\talx->num_napi = 1;\n\talx->num_txq = 1;\n\talx->num_rxq = 1;\n\treturn 0;\n}\n\nstatic void alx_irq_enable(struct alx_priv *alx)\n{\n\tstruct alx_hw *hw = &alx->hw;\n\tint i;\n\n\t \n\talx_write_mem32(hw, ALX_ISR, 0);\n\talx_write_mem32(hw, ALX_IMR, alx->int_mask);\n\talx_post_write(hw);\n\n\tif (alx->hw.pdev->msix_enabled) {\n\t\t \n\t\tfor (i = 0; i < alx->num_vec; i++)\n\t\t\talx_mask_msix(hw, i, false);\n\t}\n}\n\nstatic void alx_irq_disable(struct alx_priv *alx)\n{\n\tstruct alx_hw *hw = &alx->hw;\n\tint i;\n\n\talx_write_mem32(hw, ALX_ISR, ALX_ISR_DIS);\n\talx_write_mem32(hw, ALX_IMR, 0);\n\talx_post_write(hw);\n\n\tif (alx->hw.pdev->msix_enabled) {\n\t\tfor (i = 0; i < alx->num_vec; i++) {\n\t\t\talx_mask_msix(hw, i, true);\n\t\t\tsynchronize_irq(pci_irq_vector(alx->hw.pdev, i));\n\t\t}\n\t} else {\n\t\tsynchronize_irq(pci_irq_vector(alx->hw.pdev, 0));\n\t}\n}\n\nstatic int alx_realloc_resources(struct alx_priv *alx)\n{\n\tint err;\n\n\talx_free_rings(alx);\n\talx_free_napis(alx);\n\tpci_free_irq_vectors(alx->hw.pdev);\n\n\terr = alx_init_intr(alx);\n\tif (err)\n\t\treturn err;\n\n\terr = alx_alloc_napis(alx);\n\tif (err)\n\t\treturn err;\n\n\terr = alx_alloc_rings(alx);\n\tif (err)\n\t\treturn err;\n\n\treturn 0;\n}\n\nstatic int alx_request_irq(struct alx_priv *alx)\n{\n\tstruct pci_dev *pdev = alx->hw.pdev;\n\tstruct alx_hw *hw = &alx->hw;\n\tint err;\n\tu32 msi_ctrl;\n\n\tmsi_ctrl = (hw->imt >> 1) << ALX_MSI_RETRANS_TM_SHIFT;\n\n\tif (alx->hw.pdev->msix_enabled) {\n\t\talx_write_mem32(hw, ALX_MSI_RETRANS_TIMER, msi_ctrl);\n\t\terr = alx_request_msix(alx);\n\t\tif (!err)\n\t\t\tgoto out;\n\n\t\t \n\t\terr = alx_realloc_resources(alx);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tif (alx->hw.pdev->msi_enabled) {\n\t\talx_write_mem32(hw, ALX_MSI_RETRANS_TIMER,\n\t\t\t\tmsi_ctrl | ALX_MSI_MASK_SEL_LINE);\n\t\terr = request_irq(pci_irq_vector(pdev, 0), alx_intr_msi, 0,\n\t\t\t\t  alx->dev->name, alx);\n\t\tif (!err)\n\t\t\tgoto out;\n\n\t\t \n\t\tpci_free_irq_vectors(alx->hw.pdev);\n\t}\n\n\talx_write_mem32(hw, ALX_MSI_RETRANS_TIMER, 0);\n\terr = request_irq(pci_irq_vector(pdev, 0), alx_intr_legacy, IRQF_SHARED,\n\t\t\t  alx->dev->name, alx);\nout:\n\tif (!err)\n\t\talx_config_vector_mapping(alx);\n\telse\n\t\tnetdev_err(alx->dev, \"IRQ registration failed!\\n\");\n\treturn err;\n}\n\nstatic void alx_free_irq(struct alx_priv *alx)\n{\n\tstruct pci_dev *pdev = alx->hw.pdev;\n\tint i;\n\n\tfree_irq(pci_irq_vector(pdev, 0), alx);\n\tif (alx->hw.pdev->msix_enabled) {\n\t\tfor (i = 0; i < alx->num_napi; i++)\n\t\t\tfree_irq(pci_irq_vector(pdev, i + 1), alx->qnapi[i]);\n\t}\n\n\tpci_free_irq_vectors(pdev);\n}\n\nstatic int alx_identify_hw(struct alx_priv *alx)\n{\n\tstruct alx_hw *hw = &alx->hw;\n\tint rev = alx_hw_revision(hw);\n\n\tif (rev > ALX_REV_C0)\n\t\treturn -EINVAL;\n\n\thw->max_dma_chnl = rev >= ALX_REV_B0 ? 4 : 2;\n\n\treturn 0;\n}\n\nstatic int alx_init_sw(struct alx_priv *alx)\n{\n\tstruct pci_dev *pdev = alx->hw.pdev;\n\tstruct alx_hw *hw = &alx->hw;\n\tint err;\n\n\terr = alx_identify_hw(alx);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"unrecognized chip, aborting\\n\");\n\t\treturn err;\n\t}\n\n\talx->hw.lnk_patch =\n\t\tpdev->device == ALX_DEV_ID_AR8161 &&\n\t\tpdev->subsystem_vendor == PCI_VENDOR_ID_ATTANSIC &&\n\t\tpdev->subsystem_device == 0x0091 &&\n\t\tpdev->revision == 0;\n\n\thw->smb_timer = 400;\n\thw->mtu = alx->dev->mtu;\n\talx->rxbuf_size = ALX_MAX_FRAME_LEN(hw->mtu);\n\t \n\talx->dev->min_mtu = 34;\n\talx->dev->max_mtu = ALX_MAX_FRAME_LEN(ALX_MAX_FRAME_SIZE);\n\talx->tx_ringsz = 256;\n\talx->rx_ringsz = 512;\n\thw->imt = 200;\n\talx->int_mask = ALX_ISR_MISC;\n\thw->dma_chnl = hw->max_dma_chnl;\n\thw->ith_tpd = alx->tx_ringsz / 3;\n\thw->link_speed = SPEED_UNKNOWN;\n\thw->duplex = DUPLEX_UNKNOWN;\n\thw->adv_cfg = ADVERTISED_Autoneg |\n\t\t      ADVERTISED_10baseT_Half |\n\t\t      ADVERTISED_10baseT_Full |\n\t\t      ADVERTISED_100baseT_Full |\n\t\t      ADVERTISED_100baseT_Half |\n\t\t      ADVERTISED_1000baseT_Full;\n\thw->flowctrl = ALX_FC_ANEG | ALX_FC_RX | ALX_FC_TX;\n\n\thw->rx_ctrl = ALX_MAC_CTRL_WOLSPED_SWEN |\n\t\t      ALX_MAC_CTRL_MHASH_ALG_HI5B |\n\t\t      ALX_MAC_CTRL_BRD_EN |\n\t\t      ALX_MAC_CTRL_PCRCE |\n\t\t      ALX_MAC_CTRL_CRCE |\n\t\t      ALX_MAC_CTRL_RXFC_EN |\n\t\t      ALX_MAC_CTRL_TXFC_EN |\n\t\t      7 << ALX_MAC_CTRL_PRMBLEN_SHIFT;\n\tmutex_init(&alx->mtx);\n\n\treturn 0;\n}\n\n\nstatic netdev_features_t alx_fix_features(struct net_device *netdev,\n\t\t\t\t\t  netdev_features_t features)\n{\n\tif (netdev->mtu > ALX_MAX_TSO_PKT_SIZE)\n\t\tfeatures &= ~(NETIF_F_TSO | NETIF_F_TSO6);\n\n\treturn features;\n}\n\nstatic void alx_netif_stop(struct alx_priv *alx)\n{\n\tint i;\n\n\tnetif_trans_update(alx->dev);\n\tif (netif_carrier_ok(alx->dev)) {\n\t\tnetif_carrier_off(alx->dev);\n\t\tnetif_tx_disable(alx->dev);\n\t\tfor (i = 0; i < alx->num_napi; i++)\n\t\t\tnapi_disable(&alx->qnapi[i]->napi);\n\t}\n}\n\nstatic void alx_halt(struct alx_priv *alx)\n{\n\tstruct alx_hw *hw = &alx->hw;\n\n\tlockdep_assert_held(&alx->mtx);\n\n\talx_netif_stop(alx);\n\thw->link_speed = SPEED_UNKNOWN;\n\thw->duplex = DUPLEX_UNKNOWN;\n\n\talx_reset_mac(hw);\n\n\t \n\talx_enable_aspm(hw, false, false);\n\talx_irq_disable(alx);\n\talx_free_buffers(alx);\n}\n\nstatic void alx_configure(struct alx_priv *alx)\n{\n\tstruct alx_hw *hw = &alx->hw;\n\n\talx_configure_basic(hw);\n\talx_disable_rss(hw);\n\t__alx_set_rx_mode(alx->dev);\n\n\talx_write_mem32(hw, ALX_MAC_CTRL, hw->rx_ctrl);\n}\n\nstatic void alx_activate(struct alx_priv *alx)\n{\n\tlockdep_assert_held(&alx->mtx);\n\n\t \n\talx_reinit_rings(alx);\n\talx_configure(alx);\n\n\t \n\talx_write_mem32(&alx->hw, ALX_ISR, ~(u32)ALX_ISR_DIS);\n\n\talx_irq_enable(alx);\n\n\talx_schedule_link_check(alx);\n}\n\nstatic void alx_reinit(struct alx_priv *alx)\n{\n\tlockdep_assert_held(&alx->mtx);\n\n\talx_halt(alx);\n\talx_activate(alx);\n}\n\nstatic int alx_change_mtu(struct net_device *netdev, int mtu)\n{\n\tstruct alx_priv *alx = netdev_priv(netdev);\n\tint max_frame = ALX_MAX_FRAME_LEN(mtu);\n\n\tnetdev->mtu = mtu;\n\talx->hw.mtu = mtu;\n\talx->rxbuf_size = max(max_frame, ALX_DEF_RXBUF_SIZE);\n\tnetdev_update_features(netdev);\n\tif (netif_running(netdev)) {\n\t\tmutex_lock(&alx->mtx);\n\t\talx_reinit(alx);\n\t\tmutex_unlock(&alx->mtx);\n\t}\n\treturn 0;\n}\n\nstatic void alx_netif_start(struct alx_priv *alx)\n{\n\tint i;\n\n\tnetif_tx_wake_all_queues(alx->dev);\n\tfor (i = 0; i < alx->num_napi; i++)\n\t\tnapi_enable(&alx->qnapi[i]->napi);\n\tnetif_carrier_on(alx->dev);\n}\n\nstatic int __alx_open(struct alx_priv *alx, bool resume)\n{\n\tint err;\n\n\terr = alx_enable_msix(alx);\n\tif (err < 0) {\n\t\terr = alx_init_intr(alx);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (!resume)\n\t\tnetif_carrier_off(alx->dev);\n\n\terr = alx_alloc_napis(alx);\n\tif (err)\n\t\tgoto out_disable_adv_intr;\n\n\terr = alx_alloc_rings(alx);\n\tif (err)\n\t\tgoto out_free_rings;\n\n\talx_configure(alx);\n\n\terr = alx_request_irq(alx);\n\tif (err)\n\t\tgoto out_free_rings;\n\n\t \n\talx_reinit_rings(alx);\n\n\tnetif_set_real_num_tx_queues(alx->dev, alx->num_txq);\n\tnetif_set_real_num_rx_queues(alx->dev, alx->num_rxq);\n\n\t \n\talx_write_mem32(&alx->hw, ALX_ISR, ~(u32)ALX_ISR_DIS);\n\n\talx_irq_enable(alx);\n\n\tif (!resume)\n\t\tnetif_tx_start_all_queues(alx->dev);\n\n\talx_schedule_link_check(alx);\n\treturn 0;\n\nout_free_rings:\n\talx_free_rings(alx);\n\talx_free_napis(alx);\nout_disable_adv_intr:\n\tpci_free_irq_vectors(alx->hw.pdev);\n\treturn err;\n}\n\nstatic void __alx_stop(struct alx_priv *alx)\n{\n\tlockdep_assert_held(&alx->mtx);\n\n\talx_free_irq(alx);\n\n\tcancel_work_sync(&alx->link_check_wk);\n\tcancel_work_sync(&alx->reset_wk);\n\n\talx_halt(alx);\n\talx_free_rings(alx);\n\talx_free_napis(alx);\n}\n\nstatic const char *alx_speed_desc(struct alx_hw *hw)\n{\n\tswitch (alx_speed_to_ethadv(hw->link_speed, hw->duplex)) {\n\tcase ADVERTISED_1000baseT_Full:\n\t\treturn \"1 Gbps Full\";\n\tcase ADVERTISED_100baseT_Full:\n\t\treturn \"100 Mbps Full\";\n\tcase ADVERTISED_100baseT_Half:\n\t\treturn \"100 Mbps Half\";\n\tcase ADVERTISED_10baseT_Full:\n\t\treturn \"10 Mbps Full\";\n\tcase ADVERTISED_10baseT_Half:\n\t\treturn \"10 Mbps Half\";\n\tdefault:\n\t\treturn \"Unknown speed\";\n\t}\n}\n\nstatic void alx_check_link(struct alx_priv *alx)\n{\n\tstruct alx_hw *hw = &alx->hw;\n\tunsigned long flags;\n\tint old_speed;\n\tint err;\n\n\tlockdep_assert_held(&alx->mtx);\n\n\t \n\talx_clear_phy_intr(hw);\n\n\told_speed = hw->link_speed;\n\terr = alx_read_phy_link(hw);\n\tif (err < 0)\n\t\tgoto reset;\n\n\tspin_lock_irqsave(&alx->irq_lock, flags);\n\talx->int_mask |= ALX_ISR_PHY;\n\talx_write_mem32(hw, ALX_IMR, alx->int_mask);\n\tspin_unlock_irqrestore(&alx->irq_lock, flags);\n\n\tif (old_speed == hw->link_speed)\n\t\treturn;\n\n\tif (hw->link_speed != SPEED_UNKNOWN) {\n\t\tnetif_info(alx, link, alx->dev,\n\t\t\t   \"NIC Up: %s\\n\", alx_speed_desc(hw));\n\t\talx_post_phy_link(hw);\n\t\talx_enable_aspm(hw, true, true);\n\t\talx_start_mac(hw);\n\n\t\tif (old_speed == SPEED_UNKNOWN)\n\t\t\talx_netif_start(alx);\n\t} else {\n\t\t \n\t\talx_netif_stop(alx);\n\t\tnetif_info(alx, link, alx->dev, \"Link Down\\n\");\n\t\terr = alx_reset_mac(hw);\n\t\tif (err)\n\t\t\tgoto reset;\n\t\talx_irq_disable(alx);\n\n\t\t \n\t\terr = alx_reinit_rings(alx);\n\t\tif (err)\n\t\t\tgoto reset;\n\t\talx_configure(alx);\n\t\talx_enable_aspm(hw, false, true);\n\t\talx_post_phy_link(hw);\n\t\talx_irq_enable(alx);\n\t}\n\n\treturn;\n\nreset:\n\talx_schedule_reset(alx);\n}\n\nstatic int alx_open(struct net_device *netdev)\n{\n\tstruct alx_priv *alx = netdev_priv(netdev);\n\tint ret;\n\n\tmutex_lock(&alx->mtx);\n\tret = __alx_open(alx, false);\n\tmutex_unlock(&alx->mtx);\n\n\treturn ret;\n}\n\nstatic int alx_stop(struct net_device *netdev)\n{\n\tstruct alx_priv *alx = netdev_priv(netdev);\n\n\tmutex_lock(&alx->mtx);\n\t__alx_stop(alx);\n\tmutex_unlock(&alx->mtx);\n\n\treturn 0;\n}\n\nstatic void alx_link_check(struct work_struct *work)\n{\n\tstruct alx_priv *alx;\n\n\talx = container_of(work, struct alx_priv, link_check_wk);\n\n\tmutex_lock(&alx->mtx);\n\talx_check_link(alx);\n\tmutex_unlock(&alx->mtx);\n}\n\nstatic void alx_reset(struct work_struct *work)\n{\n\tstruct alx_priv *alx = container_of(work, struct alx_priv, reset_wk);\n\n\tmutex_lock(&alx->mtx);\n\talx_reinit(alx);\n\tmutex_unlock(&alx->mtx);\n}\n\nstatic int alx_tpd_req(struct sk_buff *skb)\n{\n\tint num;\n\n\tnum = skb_shinfo(skb)->nr_frags + 1;\n\t \n\tif (skb_is_gso(skb) && skb_shinfo(skb)->gso_type & SKB_GSO_TCPV6)\n\t\tnum++;\n\n\treturn num;\n}\n\nstatic int alx_tx_csum(struct sk_buff *skb, struct alx_txd *first)\n{\n\tu8 cso, css;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tcso = skb_checksum_start_offset(skb);\n\tif (cso & 1)\n\t\treturn -EINVAL;\n\n\tcss = cso + skb->csum_offset;\n\tfirst->word1 |= cpu_to_le32((cso >> 1) << TPD_CXSUMSTART_SHIFT);\n\tfirst->word1 |= cpu_to_le32((css >> 1) << TPD_CXSUMOFFSET_SHIFT);\n\tfirst->word1 |= cpu_to_le32(1 << TPD_CXSUM_EN_SHIFT);\n\n\treturn 0;\n}\n\nstatic int alx_tso(struct sk_buff *skb, struct alx_txd *first)\n{\n\tint err;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tif (!skb_is_gso(skb))\n\t\treturn 0;\n\n\terr = skb_cow_head(skb, 0);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\tstruct iphdr *iph = ip_hdr(skb);\n\n\t\tiph->check = 0;\n\t\ttcp_hdr(skb)->check = ~csum_tcpudp_magic(iph->saddr, iph->daddr,\n\t\t\t\t\t\t\t 0, IPPROTO_TCP, 0);\n\t\tfirst->word1 |= 1 << TPD_IPV4_SHIFT;\n\t} else if (skb_is_gso_v6(skb)) {\n\t\ttcp_v6_gso_csum_prep(skb);\n\t\t \n\t\tfirst->adrl.l.pkt_len = skb->len;\n\t\tfirst->word1 |= 1 << TPD_LSO_V2_SHIFT;\n\t}\n\n\tfirst->word1 |= 1 << TPD_LSO_EN_SHIFT;\n\tfirst->word1 |= (skb_transport_offset(skb) &\n\t\t\t TPD_L4HDROFFSET_MASK) << TPD_L4HDROFFSET_SHIFT;\n\tfirst->word1 |= (skb_shinfo(skb)->gso_size &\n\t\t\t TPD_MSS_MASK) << TPD_MSS_SHIFT;\n\treturn 1;\n}\n\nstatic int alx_map_tx_skb(struct alx_tx_queue *txq, struct sk_buff *skb)\n{\n\tstruct alx_txd *tpd, *first_tpd;\n\tdma_addr_t dma;\n\tint maplen, f, first_idx = txq->write_idx;\n\n\tfirst_tpd = &txq->tpd[txq->write_idx];\n\ttpd = first_tpd;\n\n\tif (tpd->word1 & (1 << TPD_LSO_V2_SHIFT)) {\n\t\tif (++txq->write_idx == txq->count)\n\t\t\ttxq->write_idx = 0;\n\n\t\ttpd = &txq->tpd[txq->write_idx];\n\t\ttpd->len = first_tpd->len;\n\t\ttpd->vlan_tag = first_tpd->vlan_tag;\n\t\ttpd->word1 = first_tpd->word1;\n\t}\n\n\tmaplen = skb_headlen(skb);\n\tdma = dma_map_single(txq->dev, skb->data, maplen,\n\t\t\t     DMA_TO_DEVICE);\n\tif (dma_mapping_error(txq->dev, dma))\n\t\tgoto err_dma;\n\n\tdma_unmap_len_set(&txq->bufs[txq->write_idx], size, maplen);\n\tdma_unmap_addr_set(&txq->bufs[txq->write_idx], dma, dma);\n\n\ttpd->adrl.addr = cpu_to_le64(dma);\n\ttpd->len = cpu_to_le16(maplen);\n\n\tfor (f = 0; f < skb_shinfo(skb)->nr_frags; f++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[f];\n\n\t\tif (++txq->write_idx == txq->count)\n\t\t\ttxq->write_idx = 0;\n\t\ttpd = &txq->tpd[txq->write_idx];\n\n\t\ttpd->word1 = first_tpd->word1;\n\n\t\tmaplen = skb_frag_size(frag);\n\t\tdma = skb_frag_dma_map(txq->dev, frag, 0,\n\t\t\t\t       maplen, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(txq->dev, dma))\n\t\t\tgoto err_dma;\n\t\tdma_unmap_len_set(&txq->bufs[txq->write_idx], size, maplen);\n\t\tdma_unmap_addr_set(&txq->bufs[txq->write_idx], dma, dma);\n\n\t\ttpd->adrl.addr = cpu_to_le64(dma);\n\t\ttpd->len = cpu_to_le16(maplen);\n\t}\n\n\t \n\ttpd->word1 |= cpu_to_le32(1 << TPD_EOP_SHIFT);\n\ttxq->bufs[txq->write_idx].skb = skb;\n\n\tif (++txq->write_idx == txq->count)\n\t\ttxq->write_idx = 0;\n\n\treturn 0;\n\nerr_dma:\n\tf = first_idx;\n\twhile (f != txq->write_idx) {\n\t\talx_free_txbuf(txq, f);\n\t\tif (++f == txq->count)\n\t\t\tf = 0;\n\t}\n\treturn -ENOMEM;\n}\n\nstatic netdev_tx_t alx_start_xmit_ring(struct sk_buff *skb,\n\t\t\t\t       struct alx_tx_queue *txq)\n{\n\tstruct alx_priv *alx;\n\tstruct alx_txd *first;\n\tint tso;\n\n\talx = netdev_priv(txq->netdev);\n\n\tif (alx_tpd_avail(txq) < alx_tpd_req(skb)) {\n\t\tnetif_tx_stop_queue(alx_get_tx_queue(txq));\n\t\tgoto drop;\n\t}\n\n\tfirst = &txq->tpd[txq->write_idx];\n\tmemset(first, 0, sizeof(*first));\n\n\ttso = alx_tso(skb, first);\n\tif (tso < 0)\n\t\tgoto drop;\n\telse if (!tso && alx_tx_csum(skb, first))\n\t\tgoto drop;\n\n\tif (alx_map_tx_skb(txq, skb) < 0)\n\t\tgoto drop;\n\n\tnetdev_tx_sent_queue(alx_get_tx_queue(txq), skb->len);\n\n\t \n\twmb();\n\talx_write_mem16(&alx->hw, txq->p_reg, txq->write_idx);\n\n\tif (alx_tpd_avail(txq) < txq->count / 8)\n\t\tnetif_tx_stop_queue(alx_get_tx_queue(txq));\n\n\treturn NETDEV_TX_OK;\n\ndrop:\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\nstatic netdev_tx_t alx_start_xmit(struct sk_buff *skb,\n\t\t\t\t  struct net_device *netdev)\n{\n\tstruct alx_priv *alx = netdev_priv(netdev);\n\treturn alx_start_xmit_ring(skb, alx_tx_queue_mapping(alx, skb));\n}\n\nstatic void alx_tx_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tstruct alx_priv *alx = netdev_priv(dev);\n\n\talx_schedule_reset(alx);\n}\n\nstatic int alx_mdio_read(struct net_device *netdev,\n\t\t\t int prtad, int devad, u16 addr)\n{\n\tstruct alx_priv *alx = netdev_priv(netdev);\n\tstruct alx_hw *hw = &alx->hw;\n\tu16 val;\n\tint err;\n\n\tif (prtad != hw->mdio.prtad)\n\t\treturn -EINVAL;\n\n\tif (devad == MDIO_DEVAD_NONE)\n\t\terr = alx_read_phy_reg(hw, addr, &val);\n\telse\n\t\terr = alx_read_phy_ext(hw, devad, addr, &val);\n\n\tif (err)\n\t\treturn err;\n\treturn val;\n}\n\nstatic int alx_mdio_write(struct net_device *netdev,\n\t\t\t  int prtad, int devad, u16 addr, u16 val)\n{\n\tstruct alx_priv *alx = netdev_priv(netdev);\n\tstruct alx_hw *hw = &alx->hw;\n\n\tif (prtad != hw->mdio.prtad)\n\t\treturn -EINVAL;\n\n\tif (devad == MDIO_DEVAD_NONE)\n\t\treturn alx_write_phy_reg(hw, addr, val);\n\n\treturn alx_write_phy_ext(hw, devad, addr, val);\n}\n\nstatic int alx_ioctl(struct net_device *netdev, struct ifreq *ifr, int cmd)\n{\n\tstruct alx_priv *alx = netdev_priv(netdev);\n\n\tif (!netif_running(netdev))\n\t\treturn -EAGAIN;\n\n\treturn mdio_mii_ioctl(&alx->hw.mdio, if_mii(ifr), cmd);\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void alx_poll_controller(struct net_device *netdev)\n{\n\tstruct alx_priv *alx = netdev_priv(netdev);\n\tint i;\n\n\tif (alx->hw.pdev->msix_enabled) {\n\t\talx_intr_msix_misc(0, alx);\n\t\tfor (i = 0; i < alx->num_txq; i++)\n\t\t\talx_intr_msix_ring(0, alx->qnapi[i]);\n\t} else if (alx->hw.pdev->msi_enabled)\n\t\talx_intr_msi(0, alx);\n\telse\n\t\talx_intr_legacy(0, alx);\n}\n#endif\n\nstatic void alx_get_stats64(struct net_device *dev,\n\t\t\t    struct rtnl_link_stats64 *net_stats)\n{\n\tstruct alx_priv *alx = netdev_priv(dev);\n\tstruct alx_hw_stats *hw_stats = &alx->hw.stats;\n\n\tspin_lock(&alx->stats_lock);\n\n\talx_update_hw_stats(&alx->hw);\n\n\tnet_stats->tx_bytes   = hw_stats->tx_byte_cnt;\n\tnet_stats->rx_bytes   = hw_stats->rx_byte_cnt;\n\tnet_stats->multicast  = hw_stats->rx_mcast;\n\tnet_stats->collisions = hw_stats->tx_single_col +\n\t\t\t\thw_stats->tx_multi_col +\n\t\t\t\thw_stats->tx_late_col +\n\t\t\t\thw_stats->tx_abort_col;\n\n\tnet_stats->rx_errors  = hw_stats->rx_frag +\n\t\t\t\thw_stats->rx_fcs_err +\n\t\t\t\thw_stats->rx_len_err +\n\t\t\t\thw_stats->rx_ov_sz +\n\t\t\t\thw_stats->rx_ov_rrd +\n\t\t\t\thw_stats->rx_align_err +\n\t\t\t\thw_stats->rx_ov_rxf;\n\n\tnet_stats->rx_fifo_errors   = hw_stats->rx_ov_rxf;\n\tnet_stats->rx_length_errors = hw_stats->rx_len_err;\n\tnet_stats->rx_crc_errors    = hw_stats->rx_fcs_err;\n\tnet_stats->rx_frame_errors  = hw_stats->rx_align_err;\n\tnet_stats->rx_dropped       = hw_stats->rx_ov_rrd;\n\n\tnet_stats->tx_errors = hw_stats->tx_late_col +\n\t\t\t       hw_stats->tx_abort_col +\n\t\t\t       hw_stats->tx_underrun +\n\t\t\t       hw_stats->tx_trunc;\n\n\tnet_stats->tx_aborted_errors = hw_stats->tx_abort_col;\n\tnet_stats->tx_fifo_errors    = hw_stats->tx_underrun;\n\tnet_stats->tx_window_errors  = hw_stats->tx_late_col;\n\n\tnet_stats->tx_packets = hw_stats->tx_ok + net_stats->tx_errors;\n\tnet_stats->rx_packets = hw_stats->rx_ok + net_stats->rx_errors;\n\n\tspin_unlock(&alx->stats_lock);\n}\n\nstatic const struct net_device_ops alx_netdev_ops = {\n\t.ndo_open               = alx_open,\n\t.ndo_stop               = alx_stop,\n\t.ndo_start_xmit         = alx_start_xmit,\n\t.ndo_get_stats64        = alx_get_stats64,\n\t.ndo_set_rx_mode        = alx_set_rx_mode,\n\t.ndo_validate_addr      = eth_validate_addr,\n\t.ndo_set_mac_address    = alx_set_mac_address,\n\t.ndo_change_mtu         = alx_change_mtu,\n\t.ndo_eth_ioctl           = alx_ioctl,\n\t.ndo_tx_timeout         = alx_tx_timeout,\n\t.ndo_fix_features\t= alx_fix_features,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller    = alx_poll_controller,\n#endif\n};\n\nstatic int alx_probe(struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tstruct net_device *netdev;\n\tstruct alx_priv *alx;\n\tstruct alx_hw *hw;\n\tbool phy_configured;\n\tint err;\n\n\terr = pci_enable_device_mem(pdev);\n\tif (err)\n\t\treturn err;\n\n\t \n\tif (!dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64))) {\n\t\tdev_dbg(&pdev->dev, \"DMA to 64-BIT addresses\\n\");\n\t} else {\n\t\terr = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));\n\t\tif (err) {\n\t\t\tdev_err(&pdev->dev, \"No usable DMA config, aborting\\n\");\n\t\t\tgoto out_pci_disable;\n\t\t}\n\t}\n\n\terr = pci_request_mem_regions(pdev, alx_drv_name);\n\tif (err) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"pci_request_mem_regions failed\\n\");\n\t\tgoto out_pci_disable;\n\t}\n\n\tpci_set_master(pdev);\n\n\tif (!pdev->pm_cap) {\n\t\tdev_err(&pdev->dev,\n\t\t\t\"Can't find power management capability, aborting\\n\");\n\t\terr = -EIO;\n\t\tgoto out_pci_release;\n\t}\n\n\tnetdev = alloc_etherdev_mqs(sizeof(*alx),\n\t\t\t\t    ALX_MAX_TX_QUEUES, 1);\n\tif (!netdev) {\n\t\terr = -ENOMEM;\n\t\tgoto out_pci_release;\n\t}\n\n\tSET_NETDEV_DEV(netdev, &pdev->dev);\n\talx = netdev_priv(netdev);\n\tspin_lock_init(&alx->hw.mdio_lock);\n\tspin_lock_init(&alx->irq_lock);\n\tspin_lock_init(&alx->stats_lock);\n\talx->dev = netdev;\n\talx->hw.pdev = pdev;\n\talx->msg_enable = NETIF_MSG_LINK | NETIF_MSG_HW | NETIF_MSG_IFUP |\n\t\t\t  NETIF_MSG_TX_ERR | NETIF_MSG_RX_ERR | NETIF_MSG_WOL;\n\thw = &alx->hw;\n\tpci_set_drvdata(pdev, alx);\n\n\thw->hw_addr = pci_ioremap_bar(pdev, 0);\n\tif (!hw->hw_addr) {\n\t\tdev_err(&pdev->dev, \"cannot map device registers\\n\");\n\t\terr = -EIO;\n\t\tgoto out_free_netdev;\n\t}\n\n\tnetdev->netdev_ops = &alx_netdev_ops;\n\tnetdev->ethtool_ops = &alx_ethtool_ops;\n\tnetdev->irq = pci_irq_vector(pdev, 0);\n\tnetdev->watchdog_timeo = ALX_WATCHDOG_TIME;\n\n\tif (ent->driver_data & ALX_DEV_QUIRK_MSI_INTX_DISABLE_BUG)\n\t\tpdev->dev_flags |= PCI_DEV_FLAGS_MSI_INTX_DISABLE_BUG;\n\n\terr = alx_init_sw(alx);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"net device private data init failed\\n\");\n\t\tgoto out_unmap;\n\t}\n\n\tmutex_lock(&alx->mtx);\n\n\talx_reset_pcie(hw);\n\n\tphy_configured = alx_phy_configured(hw);\n\n\tif (!phy_configured)\n\t\talx_reset_phy(hw);\n\n\terr = alx_reset_mac(hw);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"MAC Reset failed, error = %d\\n\", err);\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tif (!phy_configured) {\n\t\terr = alx_setup_speed_duplex(hw, hw->adv_cfg, hw->flowctrl);\n\t\tif (err) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"failed to configure PHY speed/duplex (err=%d)\\n\",\n\t\t\t\terr);\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\tnetdev->hw_features = NETIF_F_SG |\n\t\t\t      NETIF_F_HW_CSUM |\n\t\t\t      NETIF_F_RXCSUM |\n\t\t\t      NETIF_F_TSO |\n\t\t\t      NETIF_F_TSO6;\n\n\tif (alx_get_perm_macaddr(hw, hw->perm_addr)) {\n\t\tdev_warn(&pdev->dev,\n\t\t\t \"Invalid permanent address programmed, using random one\\n\");\n\t\teth_hw_addr_random(netdev);\n\t\tmemcpy(hw->perm_addr, netdev->dev_addr, netdev->addr_len);\n\t}\n\n\tmemcpy(hw->mac_addr, hw->perm_addr, ETH_ALEN);\n\teth_hw_addr_set(netdev, hw->mac_addr);\n\tmemcpy(netdev->perm_addr, hw->perm_addr, ETH_ALEN);\n\n\thw->mdio.prtad = 0;\n\thw->mdio.mmds = 0;\n\thw->mdio.dev = netdev;\n\thw->mdio.mode_support = MDIO_SUPPORTS_C45 |\n\t\t\t\tMDIO_SUPPORTS_C22 |\n\t\t\t\tMDIO_EMULATE_C22;\n\thw->mdio.mdio_read = alx_mdio_read;\n\thw->mdio.mdio_write = alx_mdio_write;\n\n\tif (!alx_get_phy_info(hw)) {\n\t\tdev_err(&pdev->dev, \"failed to identify PHY\\n\");\n\t\terr = -EIO;\n\t\tgoto out_unlock;\n\t}\n\n\tmutex_unlock(&alx->mtx);\n\n\tINIT_WORK(&alx->link_check_wk, alx_link_check);\n\tINIT_WORK(&alx->reset_wk, alx_reset);\n\tnetif_carrier_off(netdev);\n\n\terr = register_netdev(netdev);\n\tif (err) {\n\t\tdev_err(&pdev->dev, \"register netdevice failed\\n\");\n\t\tgoto out_unmap;\n\t}\n\n\tnetdev_info(netdev,\n\t\t    \"Qualcomm Atheros AR816x/AR817x Ethernet [%pM]\\n\",\n\t\t    netdev->dev_addr);\n\n\treturn 0;\n\nout_unlock:\n\tmutex_unlock(&alx->mtx);\nout_unmap:\n\tiounmap(hw->hw_addr);\nout_free_netdev:\n\tfree_netdev(netdev);\nout_pci_release:\n\tpci_release_mem_regions(pdev);\nout_pci_disable:\n\tpci_disable_device(pdev);\n\treturn err;\n}\n\nstatic void alx_remove(struct pci_dev *pdev)\n{\n\tstruct alx_priv *alx = pci_get_drvdata(pdev);\n\tstruct alx_hw *hw = &alx->hw;\n\n\t \n\talx_set_macaddr(hw, hw->perm_addr);\n\n\tunregister_netdev(alx->dev);\n\tiounmap(hw->hw_addr);\n\tpci_release_mem_regions(pdev);\n\n\tpci_disable_device(pdev);\n\n\tmutex_destroy(&alx->mtx);\n\n\tfree_netdev(alx->dev);\n}\n\nstatic int alx_suspend(struct device *dev)\n{\n\tstruct alx_priv *alx = dev_get_drvdata(dev);\n\n\tif (!netif_running(alx->dev))\n\t\treturn 0;\n\n\trtnl_lock();\n\tnetif_device_detach(alx->dev);\n\n\tmutex_lock(&alx->mtx);\n\t__alx_stop(alx);\n\tmutex_unlock(&alx->mtx);\n\trtnl_unlock();\n\n\treturn 0;\n}\n\nstatic int alx_resume(struct device *dev)\n{\n\tstruct alx_priv *alx = dev_get_drvdata(dev);\n\tstruct alx_hw *hw = &alx->hw;\n\tint err;\n\n\trtnl_lock();\n\tmutex_lock(&alx->mtx);\n\talx_reset_phy(hw);\n\n\tif (!netif_running(alx->dev)) {\n\t\terr = 0;\n\t\tgoto unlock;\n\t}\n\n\terr = __alx_open(alx, true);\n\tif (err)\n\t\tgoto unlock;\n\n\tnetif_device_attach(alx->dev);\n\nunlock:\n\tmutex_unlock(&alx->mtx);\n\trtnl_unlock();\n\treturn err;\n}\n\nstatic DEFINE_SIMPLE_DEV_PM_OPS(alx_pm_ops, alx_suspend, alx_resume);\n\nstatic pci_ers_result_t alx_pci_error_detected(struct pci_dev *pdev,\n\t\t\t\t\t       pci_channel_state_t state)\n{\n\tstruct alx_priv *alx = pci_get_drvdata(pdev);\n\tstruct net_device *netdev = alx->dev;\n\tpci_ers_result_t rc = PCI_ERS_RESULT_NEED_RESET;\n\n\tdev_info(&pdev->dev, \"pci error detected\\n\");\n\n\tmutex_lock(&alx->mtx);\n\n\tif (netif_running(netdev)) {\n\t\tnetif_device_detach(netdev);\n\t\talx_halt(alx);\n\t}\n\n\tif (state == pci_channel_io_perm_failure)\n\t\trc = PCI_ERS_RESULT_DISCONNECT;\n\telse\n\t\tpci_disable_device(pdev);\n\n\tmutex_unlock(&alx->mtx);\n\n\treturn rc;\n}\n\nstatic pci_ers_result_t alx_pci_error_slot_reset(struct pci_dev *pdev)\n{\n\tstruct alx_priv *alx = pci_get_drvdata(pdev);\n\tstruct alx_hw *hw = &alx->hw;\n\tpci_ers_result_t rc = PCI_ERS_RESULT_DISCONNECT;\n\n\tdev_info(&pdev->dev, \"pci error slot reset\\n\");\n\n\tmutex_lock(&alx->mtx);\n\n\tif (pci_enable_device(pdev)) {\n\t\tdev_err(&pdev->dev, \"Failed to re-enable PCI device after reset\\n\");\n\t\tgoto out;\n\t}\n\n\tpci_set_master(pdev);\n\n\talx_reset_pcie(hw);\n\tif (!alx_reset_mac(hw))\n\t\trc = PCI_ERS_RESULT_RECOVERED;\nout:\n\tmutex_unlock(&alx->mtx);\n\n\treturn rc;\n}\n\nstatic void alx_pci_error_resume(struct pci_dev *pdev)\n{\n\tstruct alx_priv *alx = pci_get_drvdata(pdev);\n\tstruct net_device *netdev = alx->dev;\n\n\tdev_info(&pdev->dev, \"pci error resume\\n\");\n\n\tmutex_lock(&alx->mtx);\n\n\tif (netif_running(netdev)) {\n\t\talx_activate(alx);\n\t\tnetif_device_attach(netdev);\n\t}\n\n\tmutex_unlock(&alx->mtx);\n}\n\nstatic const struct pci_error_handlers alx_err_handlers = {\n\t.error_detected = alx_pci_error_detected,\n\t.slot_reset     = alx_pci_error_slot_reset,\n\t.resume         = alx_pci_error_resume,\n};\n\nstatic const struct pci_device_id alx_pci_tbl[] = {\n\t{ PCI_VDEVICE(ATTANSIC, ALX_DEV_ID_AR8161),\n\t  .driver_data = ALX_DEV_QUIRK_MSI_INTX_DISABLE_BUG },\n\t{ PCI_VDEVICE(ATTANSIC, ALX_DEV_ID_E2200),\n\t  .driver_data = ALX_DEV_QUIRK_MSI_INTX_DISABLE_BUG },\n\t{ PCI_VDEVICE(ATTANSIC, ALX_DEV_ID_E2400),\n\t  .driver_data = ALX_DEV_QUIRK_MSI_INTX_DISABLE_BUG },\n\t{ PCI_VDEVICE(ATTANSIC, ALX_DEV_ID_E2500),\n\t  .driver_data = ALX_DEV_QUIRK_MSI_INTX_DISABLE_BUG },\n\t{ PCI_VDEVICE(ATTANSIC, ALX_DEV_ID_AR8162),\n\t  .driver_data = ALX_DEV_QUIRK_MSI_INTX_DISABLE_BUG },\n\t{ PCI_VDEVICE(ATTANSIC, ALX_DEV_ID_AR8171) },\n\t{ PCI_VDEVICE(ATTANSIC, ALX_DEV_ID_AR8172) },\n\t{}\n};\n\nstatic struct pci_driver alx_driver = {\n\t.name        = alx_drv_name,\n\t.id_table    = alx_pci_tbl,\n\t.probe       = alx_probe,\n\t.remove      = alx_remove,\n\t.err_handler = &alx_err_handlers,\n\t.driver.pm   = pm_sleep_ptr(&alx_pm_ops),\n};\n\nmodule_pci_driver(alx_driver);\nMODULE_DEVICE_TABLE(pci, alx_pci_tbl);\nMODULE_AUTHOR(\"Johannes Berg <johannes@sipsolutions.net>\");\nMODULE_AUTHOR(\"Qualcomm Corporation\");\nMODULE_DESCRIPTION(\n\t\"Qualcomm Atheros(R) AR816x/AR817x PCI-E Ethernet Network Driver\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}