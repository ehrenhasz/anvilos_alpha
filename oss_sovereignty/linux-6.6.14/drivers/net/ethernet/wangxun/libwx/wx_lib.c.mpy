{
  "module_name": "wx_lib.c",
  "hash_id": "cd761ec9913dabb9a47ed561de2c9a3e349d26038aa91abf366cfe5b8d61200f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/wangxun/libwx/wx_lib.c",
  "human_readable_source": "\n \n\n#include <linux/etherdevice.h>\n#include <net/ip6_checksum.h>\n#include <net/page_pool/helpers.h>\n#include <net/inet_ecn.h>\n#include <linux/iopoll.h>\n#include <linux/sctp.h>\n#include <linux/pci.h>\n#include <net/tcp.h>\n#include <net/ip.h>\n\n#include \"wx_type.h\"\n#include \"wx_lib.h\"\n#include \"wx_hw.h\"\n\n \nstatic struct wx_dec_ptype wx_ptype_lookup[256] = {\n\t \n\t[0x11] = WX_PTT(L2, NONE, NONE, NONE, NONE, PAY2),\n\t[0x12] = WX_PTT(L2, NONE, NONE, NONE, TS,   PAY2),\n\t[0x13] = WX_PTT(L2, NONE, NONE, NONE, NONE, PAY2),\n\t[0x14] = WX_PTT(L2, NONE, NONE, NONE, NONE, PAY2),\n\t[0x15] = WX_PTT(L2, NONE, NONE, NONE, NONE, NONE),\n\t[0x16] = WX_PTT(L2, NONE, NONE, NONE, NONE, PAY2),\n\t[0x17] = WX_PTT(L2, NONE, NONE, NONE, NONE, NONE),\n\n\t \n\t[0x18 ... 0x1F] = WX_PTT(L2, NONE, NONE, NONE, NONE, NONE),\n\n\t \n\t[0x21] = WX_PTT(IP, FGV4, NONE, NONE, NONE, PAY3),\n\t[0x22] = WX_PTT(IP, IPV4, NONE, NONE, NONE, PAY3),\n\t[0x23] = WX_PTT(IP, IPV4, NONE, NONE, UDP,  PAY4),\n\t[0x24] = WX_PTT(IP, IPV4, NONE, NONE, TCP,  PAY4),\n\t[0x25] = WX_PTT(IP, IPV4, NONE, NONE, SCTP, PAY4),\n\t[0x29] = WX_PTT(IP, FGV6, NONE, NONE, NONE, PAY3),\n\t[0x2A] = WX_PTT(IP, IPV6, NONE, NONE, NONE, PAY3),\n\t[0x2B] = WX_PTT(IP, IPV6, NONE, NONE, UDP,  PAY3),\n\t[0x2C] = WX_PTT(IP, IPV6, NONE, NONE, TCP,  PAY4),\n\t[0x2D] = WX_PTT(IP, IPV6, NONE, NONE, SCTP, PAY4),\n\n\t \n\t[0x30 ... 0x34] = WX_PTT(FCOE, NONE, NONE, NONE, NONE, PAY3),\n\t[0x38 ... 0x3C] = WX_PTT(FCOE, NONE, NONE, NONE, NONE, PAY3),\n\n\t \n\t[0x81] = WX_PTT(IP, IPV4, IPIP, FGV4, NONE, PAY3),\n\t[0x82] = WX_PTT(IP, IPV4, IPIP, IPV4, NONE, PAY3),\n\t[0x83] = WX_PTT(IP, IPV4, IPIP, IPV4, UDP,  PAY4),\n\t[0x84] = WX_PTT(IP, IPV4, IPIP, IPV4, TCP,  PAY4),\n\t[0x85] = WX_PTT(IP, IPV4, IPIP, IPV4, SCTP, PAY4),\n\t[0x89] = WX_PTT(IP, IPV4, IPIP, FGV6, NONE, PAY3),\n\t[0x8A] = WX_PTT(IP, IPV4, IPIP, IPV6, NONE, PAY3),\n\t[0x8B] = WX_PTT(IP, IPV4, IPIP, IPV6, UDP,  PAY4),\n\t[0x8C] = WX_PTT(IP, IPV4, IPIP, IPV6, TCP,  PAY4),\n\t[0x8D] = WX_PTT(IP, IPV4, IPIP, IPV6, SCTP, PAY4),\n\n\t \n\t[0x90] = WX_PTT(IP, IPV4, IG, NONE, NONE, PAY3),\n\t[0x91] = WX_PTT(IP, IPV4, IG, FGV4, NONE, PAY3),\n\t[0x92] = WX_PTT(IP, IPV4, IG, IPV4, NONE, PAY3),\n\t[0x93] = WX_PTT(IP, IPV4, IG, IPV4, UDP,  PAY4),\n\t[0x94] = WX_PTT(IP, IPV4, IG, IPV4, TCP,  PAY4),\n\t[0x95] = WX_PTT(IP, IPV4, IG, IPV4, SCTP, PAY4),\n\t[0x99] = WX_PTT(IP, IPV4, IG, FGV6, NONE, PAY3),\n\t[0x9A] = WX_PTT(IP, IPV4, IG, IPV6, NONE, PAY3),\n\t[0x9B] = WX_PTT(IP, IPV4, IG, IPV6, UDP,  PAY4),\n\t[0x9C] = WX_PTT(IP, IPV4, IG, IPV6, TCP,  PAY4),\n\t[0x9D] = WX_PTT(IP, IPV4, IG, IPV6, SCTP, PAY4),\n\n\t \n\t[0xA0] = WX_PTT(IP, IPV4, IGM, NONE, NONE, PAY3),\n\t[0xA1] = WX_PTT(IP, IPV4, IGM, FGV4, NONE, PAY3),\n\t[0xA2] = WX_PTT(IP, IPV4, IGM, IPV4, NONE, PAY3),\n\t[0xA3] = WX_PTT(IP, IPV4, IGM, IPV4, UDP,  PAY4),\n\t[0xA4] = WX_PTT(IP, IPV4, IGM, IPV4, TCP,  PAY4),\n\t[0xA5] = WX_PTT(IP, IPV4, IGM, IPV4, SCTP, PAY4),\n\t[0xA9] = WX_PTT(IP, IPV4, IGM, FGV6, NONE, PAY3),\n\t[0xAA] = WX_PTT(IP, IPV4, IGM, IPV6, NONE, PAY3),\n\t[0xAB] = WX_PTT(IP, IPV4, IGM, IPV6, UDP,  PAY4),\n\t[0xAC] = WX_PTT(IP, IPV4, IGM, IPV6, TCP,  PAY4),\n\t[0xAD] = WX_PTT(IP, IPV4, IGM, IPV6, SCTP, PAY4),\n\n\t \n\t[0xB0] = WX_PTT(IP, IPV4, IGMV, NONE, NONE, PAY3),\n\t[0xB1] = WX_PTT(IP, IPV4, IGMV, FGV4, NONE, PAY3),\n\t[0xB2] = WX_PTT(IP, IPV4, IGMV, IPV4, NONE, PAY3),\n\t[0xB3] = WX_PTT(IP, IPV4, IGMV, IPV4, UDP,  PAY4),\n\t[0xB4] = WX_PTT(IP, IPV4, IGMV, IPV4, TCP,  PAY4),\n\t[0xB5] = WX_PTT(IP, IPV4, IGMV, IPV4, SCTP, PAY4),\n\t[0xB9] = WX_PTT(IP, IPV4, IGMV, FGV6, NONE, PAY3),\n\t[0xBA] = WX_PTT(IP, IPV4, IGMV, IPV6, NONE, PAY3),\n\t[0xBB] = WX_PTT(IP, IPV4, IGMV, IPV6, UDP,  PAY4),\n\t[0xBC] = WX_PTT(IP, IPV4, IGMV, IPV6, TCP,  PAY4),\n\t[0xBD] = WX_PTT(IP, IPV4, IGMV, IPV6, SCTP, PAY4),\n\n\t \n\t[0xC1] = WX_PTT(IP, IPV6, IPIP, FGV4, NONE, PAY3),\n\t[0xC2] = WX_PTT(IP, IPV6, IPIP, IPV4, NONE, PAY3),\n\t[0xC3] = WX_PTT(IP, IPV6, IPIP, IPV4, UDP,  PAY4),\n\t[0xC4] = WX_PTT(IP, IPV6, IPIP, IPV4, TCP,  PAY4),\n\t[0xC5] = WX_PTT(IP, IPV6, IPIP, IPV4, SCTP, PAY4),\n\t[0xC9] = WX_PTT(IP, IPV6, IPIP, FGV6, NONE, PAY3),\n\t[0xCA] = WX_PTT(IP, IPV6, IPIP, IPV6, NONE, PAY3),\n\t[0xCB] = WX_PTT(IP, IPV6, IPIP, IPV6, UDP,  PAY4),\n\t[0xCC] = WX_PTT(IP, IPV6, IPIP, IPV6, TCP,  PAY4),\n\t[0xCD] = WX_PTT(IP, IPV6, IPIP, IPV6, SCTP, PAY4),\n\n\t \n\t[0xD0] = WX_PTT(IP, IPV6, IG, NONE, NONE, PAY3),\n\t[0xD1] = WX_PTT(IP, IPV6, IG, FGV4, NONE, PAY3),\n\t[0xD2] = WX_PTT(IP, IPV6, IG, IPV4, NONE, PAY3),\n\t[0xD3] = WX_PTT(IP, IPV6, IG, IPV4, UDP,  PAY4),\n\t[0xD4] = WX_PTT(IP, IPV6, IG, IPV4, TCP,  PAY4),\n\t[0xD5] = WX_PTT(IP, IPV6, IG, IPV4, SCTP, PAY4),\n\t[0xD9] = WX_PTT(IP, IPV6, IG, FGV6, NONE, PAY3),\n\t[0xDA] = WX_PTT(IP, IPV6, IG, IPV6, NONE, PAY3),\n\t[0xDB] = WX_PTT(IP, IPV6, IG, IPV6, UDP,  PAY4),\n\t[0xDC] = WX_PTT(IP, IPV6, IG, IPV6, TCP,  PAY4),\n\t[0xDD] = WX_PTT(IP, IPV6, IG, IPV6, SCTP, PAY4),\n\n\t \n\t[0xE0] = WX_PTT(IP, IPV6, IGM, NONE, NONE, PAY3),\n\t[0xE1] = WX_PTT(IP, IPV6, IGM, FGV4, NONE, PAY3),\n\t[0xE2] = WX_PTT(IP, IPV6, IGM, IPV4, NONE, PAY3),\n\t[0xE3] = WX_PTT(IP, IPV6, IGM, IPV4, UDP,  PAY4),\n\t[0xE4] = WX_PTT(IP, IPV6, IGM, IPV4, TCP,  PAY4),\n\t[0xE5] = WX_PTT(IP, IPV6, IGM, IPV4, SCTP, PAY4),\n\t[0xE9] = WX_PTT(IP, IPV6, IGM, FGV6, NONE, PAY3),\n\t[0xEA] = WX_PTT(IP, IPV6, IGM, IPV6, NONE, PAY3),\n\t[0xEB] = WX_PTT(IP, IPV6, IGM, IPV6, UDP,  PAY4),\n\t[0xEC] = WX_PTT(IP, IPV6, IGM, IPV6, TCP,  PAY4),\n\t[0xED] = WX_PTT(IP, IPV6, IGM, IPV6, SCTP, PAY4),\n\n\t \n\t[0xF0] = WX_PTT(IP, IPV6, IGMV, NONE, NONE, PAY3),\n\t[0xF1] = WX_PTT(IP, IPV6, IGMV, FGV4, NONE, PAY3),\n\t[0xF2] = WX_PTT(IP, IPV6, IGMV, IPV4, NONE, PAY3),\n\t[0xF3] = WX_PTT(IP, IPV6, IGMV, IPV4, UDP,  PAY4),\n\t[0xF4] = WX_PTT(IP, IPV6, IGMV, IPV4, TCP,  PAY4),\n\t[0xF5] = WX_PTT(IP, IPV6, IGMV, IPV4, SCTP, PAY4),\n\t[0xF9] = WX_PTT(IP, IPV6, IGMV, FGV6, NONE, PAY3),\n\t[0xFA] = WX_PTT(IP, IPV6, IGMV, IPV6, NONE, PAY3),\n\t[0xFB] = WX_PTT(IP, IPV6, IGMV, IPV6, UDP,  PAY4),\n\t[0xFC] = WX_PTT(IP, IPV6, IGMV, IPV6, TCP,  PAY4),\n\t[0xFD] = WX_PTT(IP, IPV6, IGMV, IPV6, SCTP, PAY4),\n};\n\nstatic struct wx_dec_ptype wx_decode_ptype(const u8 ptype)\n{\n\treturn wx_ptype_lookup[ptype];\n}\n\n \nstatic __le32 wx_test_staterr(union wx_rx_desc *rx_desc,\n\t\t\t      const u32 stat_err_bits)\n{\n\treturn rx_desc->wb.upper.status_error & cpu_to_le32(stat_err_bits);\n}\n\nstatic void wx_dma_sync_frag(struct wx_ring *rx_ring,\n\t\t\t     struct wx_rx_buffer *rx_buffer)\n{\n\tstruct sk_buff *skb = rx_buffer->skb;\n\tskb_frag_t *frag = &skb_shinfo(skb)->frags[0];\n\n\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t      WX_CB(skb)->dma,\n\t\t\t\t      skb_frag_off(frag),\n\t\t\t\t      skb_frag_size(frag),\n\t\t\t\t      DMA_FROM_DEVICE);\n\n\t \n\tif (unlikely(WX_CB(skb)->page_released))\n\t\tpage_pool_put_full_page(rx_ring->page_pool, rx_buffer->page, false);\n}\n\nstatic struct wx_rx_buffer *wx_get_rx_buffer(struct wx_ring *rx_ring,\n\t\t\t\t\t     union wx_rx_desc *rx_desc,\n\t\t\t\t\t     struct sk_buff **skb,\n\t\t\t\t\t     int *rx_buffer_pgcnt)\n{\n\tstruct wx_rx_buffer *rx_buffer;\n\tunsigned int size;\n\n\trx_buffer = &rx_ring->rx_buffer_info[rx_ring->next_to_clean];\n\tsize = le16_to_cpu(rx_desc->wb.upper.length);\n\n#if (PAGE_SIZE < 8192)\n\t*rx_buffer_pgcnt = page_count(rx_buffer->page);\n#else\n\t*rx_buffer_pgcnt = 0;\n#endif\n\n\tprefetchw(rx_buffer->page);\n\t*skb = rx_buffer->skb;\n\n\t \n\tif (!wx_test_staterr(rx_desc, WX_RXD_STAT_EOP)) {\n\t\tif (!*skb)\n\t\t\tgoto skip_sync;\n\t} else {\n\t\tif (*skb)\n\t\t\twx_dma_sync_frag(rx_ring, rx_buffer);\n\t}\n\n\t \n\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t      rx_buffer->dma,\n\t\t\t\t      rx_buffer->page_offset,\n\t\t\t\t      size,\n\t\t\t\t      DMA_FROM_DEVICE);\nskip_sync:\n\treturn rx_buffer;\n}\n\nstatic void wx_put_rx_buffer(struct wx_ring *rx_ring,\n\t\t\t     struct wx_rx_buffer *rx_buffer,\n\t\t\t     struct sk_buff *skb,\n\t\t\t     int rx_buffer_pgcnt)\n{\n\tif (!IS_ERR(skb) && WX_CB(skb)->dma == rx_buffer->dma)\n\t\t \n\t\tWX_CB(skb)->page_released = true;\n\n\t \n\trx_buffer->page = NULL;\n\trx_buffer->skb = NULL;\n}\n\nstatic struct sk_buff *wx_build_skb(struct wx_ring *rx_ring,\n\t\t\t\t    struct wx_rx_buffer *rx_buffer,\n\t\t\t\t    union wx_rx_desc *rx_desc)\n{\n\tunsigned int size = le16_to_cpu(rx_desc->wb.upper.length);\n#if (PAGE_SIZE < 8192)\n\tunsigned int truesize = WX_RX_BUFSZ;\n#else\n\tunsigned int truesize = ALIGN(size, L1_CACHE_BYTES);\n#endif\n\tstruct sk_buff *skb = rx_buffer->skb;\n\n\tif (!skb) {\n\t\tvoid *page_addr = page_address(rx_buffer->page) +\n\t\t\t\t  rx_buffer->page_offset;\n\n\t\t \n\t\tprefetch(page_addr);\n#if L1_CACHE_BYTES < 128\n\t\tprefetch(page_addr + L1_CACHE_BYTES);\n#endif\n\n\t\t \n\t\tskb = napi_alloc_skb(&rx_ring->q_vector->napi, WX_RXBUFFER_256);\n\t\tif (unlikely(!skb))\n\t\t\treturn NULL;\n\n\t\t \n\t\tprefetchw(skb->data);\n\n\t\tif (size <= WX_RXBUFFER_256) {\n\t\t\tmemcpy(__skb_put(skb, size), page_addr,\n\t\t\t       ALIGN(size, sizeof(long)));\n\t\t\tpage_pool_put_full_page(rx_ring->page_pool, rx_buffer->page, true);\n\t\t\treturn skb;\n\t\t}\n\n\t\tskb_mark_for_recycle(skb);\n\n\t\tif (!wx_test_staterr(rx_desc, WX_RXD_STAT_EOP))\n\t\t\tWX_CB(skb)->dma = rx_buffer->dma;\n\n\t\tskb_add_rx_frag(skb, 0, rx_buffer->page,\n\t\t\t\trx_buffer->page_offset,\n\t\t\t\tsize, truesize);\n\t\tgoto out;\n\n\t} else {\n\t\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, rx_buffer->page,\n\t\t\t\trx_buffer->page_offset, size, truesize);\n\t}\n\nout:\n#if (PAGE_SIZE < 8192)\n\t \n\trx_buffer->page_offset ^= truesize;\n#else\n\t \n\trx_buffer->page_offset += truesize;\n#endif\n\n\treturn skb;\n}\n\nstatic bool wx_alloc_mapped_page(struct wx_ring *rx_ring,\n\t\t\t\t struct wx_rx_buffer *bi)\n{\n\tstruct page *page = bi->page;\n\tdma_addr_t dma;\n\n\t \n\tif (likely(page))\n\t\treturn true;\n\n\tpage = page_pool_dev_alloc_pages(rx_ring->page_pool);\n\tWARN_ON(!page);\n\tdma = page_pool_get_dma_addr(page);\n\n\tbi->page_dma = dma;\n\tbi->page = page;\n\tbi->page_offset = 0;\n\n\treturn true;\n}\n\n \nvoid wx_alloc_rx_buffers(struct wx_ring *rx_ring, u16 cleaned_count)\n{\n\tu16 i = rx_ring->next_to_use;\n\tunion wx_rx_desc *rx_desc;\n\tstruct wx_rx_buffer *bi;\n\n\t \n\tif (!cleaned_count)\n\t\treturn;\n\n\trx_desc = WX_RX_DESC(rx_ring, i);\n\tbi = &rx_ring->rx_buffer_info[i];\n\ti -= rx_ring->count;\n\n\tdo {\n\t\tif (!wx_alloc_mapped_page(rx_ring, bi))\n\t\t\tbreak;\n\n\t\t \n\t\tdma_sync_single_range_for_device(rx_ring->dev, bi->dma,\n\t\t\t\t\t\t bi->page_offset,\n\t\t\t\t\t\t WX_RX_BUFSZ,\n\t\t\t\t\t\t DMA_FROM_DEVICE);\n\n\t\trx_desc->read.pkt_addr =\n\t\t\tcpu_to_le64(bi->page_dma + bi->page_offset);\n\n\t\trx_desc++;\n\t\tbi++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\trx_desc = WX_RX_DESC(rx_ring, 0);\n\t\t\tbi = rx_ring->rx_buffer_info;\n\t\t\ti -= rx_ring->count;\n\t\t}\n\n\t\t \n\t\trx_desc->wb.upper.status_error = 0;\n\n\t\tcleaned_count--;\n\t} while (cleaned_count);\n\n\ti += rx_ring->count;\n\n\tif (rx_ring->next_to_use != i) {\n\t\trx_ring->next_to_use = i;\n\t\t \n\t\trx_ring->next_to_alloc = i;\n\n\t\t \n\t\twmb();\n\t\twritel(i, rx_ring->tail);\n\t}\n}\n\nu16 wx_desc_unused(struct wx_ring *ring)\n{\n\tu16 ntc = ring->next_to_clean;\n\tu16 ntu = ring->next_to_use;\n\n\treturn ((ntc > ntu) ? 0 : ring->count) + ntc - ntu - 1;\n}\n\n \nstatic bool wx_is_non_eop(struct wx_ring *rx_ring,\n\t\t\t  union wx_rx_desc *rx_desc,\n\t\t\t  struct sk_buff *skb)\n{\n\tu32 ntc = rx_ring->next_to_clean + 1;\n\n\t \n\tntc = (ntc < rx_ring->count) ? ntc : 0;\n\trx_ring->next_to_clean = ntc;\n\n\tprefetch(WX_RX_DESC(rx_ring, ntc));\n\n\t \n\tif (likely(wx_test_staterr(rx_desc, WX_RXD_STAT_EOP)))\n\t\treturn false;\n\n\trx_ring->rx_buffer_info[ntc].skb = skb;\n\n\treturn true;\n}\n\nstatic void wx_pull_tail(struct sk_buff *skb)\n{\n\tskb_frag_t *frag = &skb_shinfo(skb)->frags[0];\n\tunsigned int pull_len;\n\tunsigned char *va;\n\n\t \n\tva = skb_frag_address(frag);\n\n\t \n\tpull_len = eth_get_headlen(skb->dev, va, WX_RXBUFFER_256);\n\n\t \n\tskb_copy_to_linear_data(skb, va, ALIGN(pull_len, sizeof(long)));\n\n\t \n\tskb_frag_size_sub(frag, pull_len);\n\tskb_frag_off_add(frag, pull_len);\n\tskb->data_len -= pull_len;\n\tskb->tail += pull_len;\n}\n\n \nstatic bool wx_cleanup_headers(struct wx_ring *rx_ring,\n\t\t\t       union wx_rx_desc *rx_desc,\n\t\t\t       struct sk_buff *skb)\n{\n\tstruct net_device *netdev = rx_ring->netdev;\n\n\t \n\tif (!netdev ||\n\t    unlikely(wx_test_staterr(rx_desc, WX_RXD_ERR_RXE) &&\n\t\t     !(netdev->features & NETIF_F_RXALL))) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn true;\n\t}\n\n\t \n\tif (!skb_headlen(skb))\n\t\twx_pull_tail(skb);\n\n\t \n\tif (eth_skb_pad(skb))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic void wx_rx_hash(struct wx_ring *ring,\n\t\t       union wx_rx_desc *rx_desc,\n\t\t       struct sk_buff *skb)\n{\n\tu16 rss_type;\n\n\tif (!(ring->netdev->features & NETIF_F_RXHASH))\n\t\treturn;\n\n\trss_type = le16_to_cpu(rx_desc->wb.lower.lo_dword.hs_rss.pkt_info) &\n\t\t\t       WX_RXD_RSSTYPE_MASK;\n\n\tif (!rss_type)\n\t\treturn;\n\n\tskb_set_hash(skb, le32_to_cpu(rx_desc->wb.lower.hi_dword.rss),\n\t\t     (WX_RSS_L4_TYPES_MASK & (1ul << rss_type)) ?\n\t\t     PKT_HASH_TYPE_L4 : PKT_HASH_TYPE_L3);\n}\n\n \nstatic void wx_rx_checksum(struct wx_ring *ring,\n\t\t\t   union wx_rx_desc *rx_desc,\n\t\t\t   struct sk_buff *skb)\n{\n\tstruct wx_dec_ptype dptype = wx_decode_ptype(WX_RXD_PKTTYPE(rx_desc));\n\n\tskb_checksum_none_assert(skb);\n\t \n\tif (!(ring->netdev->features & NETIF_F_RXCSUM))\n\t\treturn;\n\n\t \n\tif ((wx_test_staterr(rx_desc, WX_RXD_STAT_IPCS) &&\n\t     wx_test_staterr(rx_desc, WX_RXD_ERR_IPE)) ||\n\t    (wx_test_staterr(rx_desc, WX_RXD_STAT_OUTERIPCS) &&\n\t     wx_test_staterr(rx_desc, WX_RXD_ERR_OUTERIPER))) {\n\t\tring->rx_stats.csum_err++;\n\t\treturn;\n\t}\n\n\t \n\tif (!wx_test_staterr(rx_desc, WX_RXD_STAT_L4CS))\n\t\treturn;\n\n\t \n\tif (dptype.prot != WX_DEC_PTYPE_PROT_SCTP && WX_RXD_IPV6EX(rx_desc))\n\t\treturn;\n\n\t \n\tif (wx_test_staterr(rx_desc, WX_RXD_ERR_TCPE)) {\n\t\tring->rx_stats.csum_err++;\n\t\treturn;\n\t}\n\n\t \n\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\t \n\tif (dptype.etype >= WX_DEC_PTYPE_ETYPE_IG)\n\t\t__skb_incr_checksum_unnecessary(skb);\n\tring->rx_stats.csum_good_cnt++;\n}\n\nstatic void wx_rx_vlan(struct wx_ring *ring, union wx_rx_desc *rx_desc,\n\t\t       struct sk_buff *skb)\n{\n\tu16 ethertype;\n\tu8 idx = 0;\n\n\tif ((ring->netdev->features &\n\t     (NETIF_F_HW_VLAN_CTAG_RX | NETIF_F_HW_VLAN_STAG_RX)) &&\n\t    wx_test_staterr(rx_desc, WX_RXD_STAT_VP)) {\n\t\tidx = (le16_to_cpu(rx_desc->wb.lower.lo_dword.hs_rss.pkt_info) &\n\t\t       0x1c0) >> 6;\n\t\tethertype = ring->q_vector->wx->tpid[idx];\n\t\t__vlan_hwaccel_put_tag(skb, htons(ethertype),\n\t\t\t\t       le16_to_cpu(rx_desc->wb.upper.vlan));\n\t}\n}\n\n \nstatic void wx_process_skb_fields(struct wx_ring *rx_ring,\n\t\t\t\t  union wx_rx_desc *rx_desc,\n\t\t\t\t  struct sk_buff *skb)\n{\n\twx_rx_hash(rx_ring, rx_desc, skb);\n\twx_rx_checksum(rx_ring, rx_desc, skb);\n\twx_rx_vlan(rx_ring, rx_desc, skb);\n\tskb_record_rx_queue(skb, rx_ring->queue_index);\n\tskb->protocol = eth_type_trans(skb, rx_ring->netdev);\n}\n\n \nstatic int wx_clean_rx_irq(struct wx_q_vector *q_vector,\n\t\t\t   struct wx_ring *rx_ring,\n\t\t\t   int budget)\n{\n\tunsigned int total_rx_bytes = 0, total_rx_packets = 0;\n\tu16 cleaned_count = wx_desc_unused(rx_ring);\n\n\tdo {\n\t\tstruct wx_rx_buffer *rx_buffer;\n\t\tunion wx_rx_desc *rx_desc;\n\t\tstruct sk_buff *skb;\n\t\tint rx_buffer_pgcnt;\n\n\t\t \n\t\tif (cleaned_count >= WX_RX_BUFFER_WRITE) {\n\t\t\twx_alloc_rx_buffers(rx_ring, cleaned_count);\n\t\t\tcleaned_count = 0;\n\t\t}\n\n\t\trx_desc = WX_RX_DESC(rx_ring, rx_ring->next_to_clean);\n\t\tif (!wx_test_staterr(rx_desc, WX_RXD_STAT_DD))\n\t\t\tbreak;\n\n\t\t \n\t\tdma_rmb();\n\n\t\trx_buffer = wx_get_rx_buffer(rx_ring, rx_desc, &skb, &rx_buffer_pgcnt);\n\n\t\t \n\t\tskb = wx_build_skb(rx_ring, rx_buffer, rx_desc);\n\n\t\t \n\t\tif (!skb) {\n\t\t\tbreak;\n\t\t}\n\n\t\twx_put_rx_buffer(rx_ring, rx_buffer, skb, rx_buffer_pgcnt);\n\t\tcleaned_count++;\n\n\t\t \n\t\tif (wx_is_non_eop(rx_ring, rx_desc, skb))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (wx_cleanup_headers(rx_ring, rx_desc, skb))\n\t\t\tcontinue;\n\n\t\t \n\t\ttotal_rx_bytes += skb->len;\n\n\t\t \n\t\twx_process_skb_fields(rx_ring, rx_desc, skb);\n\t\tnapi_gro_receive(&q_vector->napi, skb);\n\n\t\t \n\t\ttotal_rx_packets++;\n\t} while (likely(total_rx_packets < budget));\n\n\tu64_stats_update_begin(&rx_ring->syncp);\n\trx_ring->stats.packets += total_rx_packets;\n\trx_ring->stats.bytes += total_rx_bytes;\n\tu64_stats_update_end(&rx_ring->syncp);\n\tq_vector->rx.total_packets += total_rx_packets;\n\tq_vector->rx.total_bytes += total_rx_bytes;\n\n\treturn total_rx_packets;\n}\n\nstatic struct netdev_queue *wx_txring_txq(const struct wx_ring *ring)\n{\n\treturn netdev_get_tx_queue(ring->netdev, ring->queue_index);\n}\n\n \nstatic bool wx_clean_tx_irq(struct wx_q_vector *q_vector,\n\t\t\t    struct wx_ring *tx_ring, int napi_budget)\n{\n\tunsigned int budget = q_vector->wx->tx_work_limit;\n\tunsigned int total_bytes = 0, total_packets = 0;\n\tunsigned int i = tx_ring->next_to_clean;\n\tstruct wx_tx_buffer *tx_buffer;\n\tunion wx_tx_desc *tx_desc;\n\n\tif (!netif_carrier_ok(tx_ring->netdev))\n\t\treturn true;\n\n\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\ttx_desc = WX_TX_DESC(tx_ring, i);\n\ti -= tx_ring->count;\n\n\tdo {\n\t\tunion wx_tx_desc *eop_desc = tx_buffer->next_to_watch;\n\n\t\t \n\t\tif (!eop_desc)\n\t\t\tbreak;\n\n\t\t \n\t\tsmp_rmb();\n\n\t\t \n\t\tif (!(eop_desc->wb.status & cpu_to_le32(WX_TXD_STAT_DD)))\n\t\t\tbreak;\n\n\t\t \n\t\ttx_buffer->next_to_watch = NULL;\n\n\t\t \n\t\ttotal_bytes += tx_buffer->bytecount;\n\t\ttotal_packets += tx_buffer->gso_segs;\n\n\t\t \n\t\tnapi_consume_skb(tx_buffer->skb, napi_budget);\n\n\t\t \n\t\tdma_unmap_single(tx_ring->dev,\n\t\t\t\t dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t dma_unmap_len(tx_buffer, len),\n\t\t\t\t DMA_TO_DEVICE);\n\n\t\t \n\t\tdma_unmap_len_set(tx_buffer, len, 0);\n\n\t\t \n\t\twhile (tx_desc != eop_desc) {\n\t\t\ttx_buffer++;\n\t\t\ttx_desc++;\n\t\t\ti++;\n\t\t\tif (unlikely(!i)) {\n\t\t\t\ti -= tx_ring->count;\n\t\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t\t\ttx_desc = WX_TX_DESC(tx_ring, 0);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (dma_unmap_len(tx_buffer, len)) {\n\t\t\t\tdma_unmap_page(tx_ring->dev,\n\t\t\t\t\t       dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t\t       dma_unmap_len(tx_buffer, len),\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\t\t\tdma_unmap_len_set(tx_buffer, len, 0);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\ttx_buffer++;\n\t\ttx_desc++;\n\t\ti++;\n\t\tif (unlikely(!i)) {\n\t\t\ti -= tx_ring->count;\n\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t\ttx_desc = WX_TX_DESC(tx_ring, 0);\n\t\t}\n\n\t\t \n\t\tprefetch(tx_desc);\n\n\t\t \n\t\tbudget--;\n\t} while (likely(budget));\n\n\ti += tx_ring->count;\n\ttx_ring->next_to_clean = i;\n\tu64_stats_update_begin(&tx_ring->syncp);\n\ttx_ring->stats.bytes += total_bytes;\n\ttx_ring->stats.packets += total_packets;\n\tu64_stats_update_end(&tx_ring->syncp);\n\tq_vector->tx.total_bytes += total_bytes;\n\tq_vector->tx.total_packets += total_packets;\n\n\tnetdev_tx_completed_queue(wx_txring_txq(tx_ring),\n\t\t\t\t  total_packets, total_bytes);\n\n#define TX_WAKE_THRESHOLD (DESC_NEEDED * 2)\n\tif (unlikely(total_packets && netif_carrier_ok(tx_ring->netdev) &&\n\t\t     (wx_desc_unused(tx_ring) >= TX_WAKE_THRESHOLD))) {\n\t\t \n\t\tsmp_mb();\n\n\t\tif (__netif_subqueue_stopped(tx_ring->netdev,\n\t\t\t\t\t     tx_ring->queue_index) &&\n\t\t    netif_running(tx_ring->netdev))\n\t\t\tnetif_wake_subqueue(tx_ring->netdev,\n\t\t\t\t\t    tx_ring->queue_index);\n\t}\n\n\treturn !!budget;\n}\n\n \nstatic int wx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct wx_q_vector *q_vector = container_of(napi, struct wx_q_vector, napi);\n\tint per_ring_budget, work_done = 0;\n\tstruct wx *wx = q_vector->wx;\n\tbool clean_complete = true;\n\tstruct wx_ring *ring;\n\n\twx_for_each_ring(ring, q_vector->tx) {\n\t\tif (!wx_clean_tx_irq(q_vector, ring, budget))\n\t\t\tclean_complete = false;\n\t}\n\n\t \n\tif (budget <= 0)\n\t\treturn budget;\n\n\t \n\tif (q_vector->rx.count > 1)\n\t\tper_ring_budget = max(budget / q_vector->rx.count, 1);\n\telse\n\t\tper_ring_budget = budget;\n\n\twx_for_each_ring(ring, q_vector->rx) {\n\t\tint cleaned = wx_clean_rx_irq(q_vector, ring, per_ring_budget);\n\n\t\twork_done += cleaned;\n\t\tif (cleaned >= per_ring_budget)\n\t\t\tclean_complete = false;\n\t}\n\n\t \n\tif (!clean_complete)\n\t\treturn budget;\n\n\t \n\tif (likely(napi_complete_done(napi, work_done))) {\n\t\tif (netif_running(wx->netdev))\n\t\t\twx_intr_enable(wx, WX_INTR_Q(q_vector->v_idx));\n\t}\n\n\treturn min(work_done, budget - 1);\n}\n\nstatic int wx_maybe_stop_tx(struct wx_ring *tx_ring, u16 size)\n{\n\tif (likely(wx_desc_unused(tx_ring) >= size))\n\t\treturn 0;\n\n\tnetif_stop_subqueue(tx_ring->netdev, tx_ring->queue_index);\n\n\t \n\tsmp_mb();\n\n\t \n\tif (likely(wx_desc_unused(tx_ring) < size))\n\t\treturn -EBUSY;\n\n\t \n\tnetif_start_subqueue(tx_ring->netdev, tx_ring->queue_index);\n\n\treturn 0;\n}\n\nstatic u32 wx_tx_cmd_type(u32 tx_flags)\n{\n\t \n\tu32 cmd_type = WX_TXD_DTYP_DATA | WX_TXD_IFCS;\n\n\t \n\tcmd_type |= WX_SET_FLAG(tx_flags, WX_TX_FLAGS_HW_VLAN, WX_TXD_VLE);\n\t \n\tcmd_type |= WX_SET_FLAG(tx_flags, WX_TX_FLAGS_TSO, WX_TXD_TSE);\n\t \n\tcmd_type |= WX_SET_FLAG(tx_flags, WX_TX_FLAGS_TSTAMP, WX_TXD_MAC_TSTAMP);\n\tcmd_type |= WX_SET_FLAG(tx_flags, WX_TX_FLAGS_LINKSEC, WX_TXD_LINKSEC);\n\n\treturn cmd_type;\n}\n\nstatic void wx_tx_olinfo_status(union wx_tx_desc *tx_desc,\n\t\t\t\tu32 tx_flags, unsigned int paylen)\n{\n\tu32 olinfo_status = paylen << WX_TXD_PAYLEN_SHIFT;\n\n\t \n\tolinfo_status |= WX_SET_FLAG(tx_flags, WX_TX_FLAGS_CSUM, WX_TXD_L4CS);\n\t \n\tolinfo_status |= WX_SET_FLAG(tx_flags, WX_TX_FLAGS_IPV4, WX_TXD_IIPCS);\n\t \n\tolinfo_status |= WX_SET_FLAG(tx_flags, WX_TX_FLAGS_OUTER_IPV4,\n\t\t\t\t     WX_TXD_EIPCS);\n\t \n\tolinfo_status |= WX_SET_FLAG(tx_flags, WX_TX_FLAGS_CC, WX_TXD_CC);\n\tolinfo_status |= WX_SET_FLAG(tx_flags, WX_TX_FLAGS_IPSEC,\n\t\t\t\t     WX_TXD_IPSEC);\n\ttx_desc->read.olinfo_status = cpu_to_le32(olinfo_status);\n}\n\nstatic void wx_tx_map(struct wx_ring *tx_ring,\n\t\t      struct wx_tx_buffer *first,\n\t\t      const u8 hdr_len)\n{\n\tstruct sk_buff *skb = first->skb;\n\tstruct wx_tx_buffer *tx_buffer;\n\tu32 tx_flags = first->tx_flags;\n\tu16 i = tx_ring->next_to_use;\n\tunsigned int data_len, size;\n\tunion wx_tx_desc *tx_desc;\n\tskb_frag_t *frag;\n\tdma_addr_t dma;\n\tu32 cmd_type;\n\n\tcmd_type = wx_tx_cmd_type(tx_flags);\n\ttx_desc = WX_TX_DESC(tx_ring, i);\n\twx_tx_olinfo_status(tx_desc, tx_flags, skb->len - hdr_len);\n\n\tsize = skb_headlen(skb);\n\tdata_len = skb->data_len;\n\tdma = dma_map_single(tx_ring->dev, skb->data, size, DMA_TO_DEVICE);\n\n\ttx_buffer = first;\n\n\tfor (frag = &skb_shinfo(skb)->frags[0];; frag++) {\n\t\tif (dma_mapping_error(tx_ring->dev, dma))\n\t\t\tgoto dma_error;\n\n\t\t \n\t\tdma_unmap_len_set(tx_buffer, len, size);\n\t\tdma_unmap_addr_set(tx_buffer, dma, dma);\n\n\t\ttx_desc->read.buffer_addr = cpu_to_le64(dma);\n\n\t\twhile (unlikely(size > WX_MAX_DATA_PER_TXD)) {\n\t\t\ttx_desc->read.cmd_type_len =\n\t\t\t\tcpu_to_le32(cmd_type ^ WX_MAX_DATA_PER_TXD);\n\n\t\t\ti++;\n\t\t\ttx_desc++;\n\t\t\tif (i == tx_ring->count) {\n\t\t\t\ttx_desc = WX_TX_DESC(tx_ring, 0);\n\t\t\t\ti = 0;\n\t\t\t}\n\t\t\ttx_desc->read.olinfo_status = 0;\n\n\t\t\tdma += WX_MAX_DATA_PER_TXD;\n\t\t\tsize -= WX_MAX_DATA_PER_TXD;\n\n\t\t\ttx_desc->read.buffer_addr = cpu_to_le64(dma);\n\t\t}\n\n\t\tif (likely(!data_len))\n\t\t\tbreak;\n\n\t\ttx_desc->read.cmd_type_len = cpu_to_le32(cmd_type ^ size);\n\n\t\ti++;\n\t\ttx_desc++;\n\t\tif (i == tx_ring->count) {\n\t\t\ttx_desc = WX_TX_DESC(tx_ring, 0);\n\t\t\ti = 0;\n\t\t}\n\t\ttx_desc->read.olinfo_status = 0;\n\n\t\tsize = skb_frag_size(frag);\n\n\t\tdata_len -= size;\n\n\t\tdma = skb_frag_dma_map(tx_ring->dev, frag, 0, size,\n\t\t\t\t       DMA_TO_DEVICE);\n\n\t\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\t}\n\n\t \n\tcmd_type |= size | WX_TXD_EOP | WX_TXD_RS;\n\ttx_desc->read.cmd_type_len = cpu_to_le32(cmd_type);\n\n\tnetdev_tx_sent_queue(wx_txring_txq(tx_ring), first->bytecount);\n\n\tskb_tx_timestamp(skb);\n\n\t \n\twmb();\n\n\t \n\tfirst->next_to_watch = tx_desc;\n\n\ti++;\n\tif (i == tx_ring->count)\n\t\ti = 0;\n\n\ttx_ring->next_to_use = i;\n\n\twx_maybe_stop_tx(tx_ring, DESC_NEEDED);\n\n\tif (netif_xmit_stopped(wx_txring_txq(tx_ring)) || !netdev_xmit_more())\n\t\twritel(i, tx_ring->tail);\n\n\treturn;\ndma_error:\n\tdev_err(tx_ring->dev, \"TX DMA map failed\\n\");\n\n\t \n\tfor (;;) {\n\t\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\t\tif (dma_unmap_len(tx_buffer, len))\n\t\t\tdma_unmap_page(tx_ring->dev,\n\t\t\t\t       dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t       dma_unmap_len(tx_buffer, len),\n\t\t\t\t       DMA_TO_DEVICE);\n\t\tdma_unmap_len_set(tx_buffer, len, 0);\n\t\tif (tx_buffer == first)\n\t\t\tbreak;\n\t\tif (i == 0)\n\t\t\ti += tx_ring->count;\n\t\ti--;\n\t}\n\n\tdev_kfree_skb_any(first->skb);\n\tfirst->skb = NULL;\n\n\ttx_ring->next_to_use = i;\n}\n\nstatic void wx_tx_ctxtdesc(struct wx_ring *tx_ring, u32 vlan_macip_lens,\n\t\t\t   u32 fcoe_sof_eof, u32 type_tucmd, u32 mss_l4len_idx)\n{\n\tstruct wx_tx_context_desc *context_desc;\n\tu16 i = tx_ring->next_to_use;\n\n\tcontext_desc = WX_TX_CTXTDESC(tx_ring, i);\n\ti++;\n\ttx_ring->next_to_use = (i < tx_ring->count) ? i : 0;\n\n\t \n\ttype_tucmd |= WX_TXD_DTYP_CTXT;\n\tcontext_desc->vlan_macip_lens   = cpu_to_le32(vlan_macip_lens);\n\tcontext_desc->seqnum_seed       = cpu_to_le32(fcoe_sof_eof);\n\tcontext_desc->type_tucmd_mlhl   = cpu_to_le32(type_tucmd);\n\tcontext_desc->mss_l4len_idx     = cpu_to_le32(mss_l4len_idx);\n}\n\nstatic void wx_get_ipv6_proto(struct sk_buff *skb, int offset, u8 *nexthdr)\n{\n\tstruct ipv6hdr *hdr = (struct ipv6hdr *)(skb->data + offset);\n\n\t*nexthdr = hdr->nexthdr;\n\toffset += sizeof(struct ipv6hdr);\n\twhile (ipv6_ext_hdr(*nexthdr)) {\n\t\tstruct ipv6_opt_hdr _hdr, *hp;\n\n\t\tif (*nexthdr == NEXTHDR_NONE)\n\t\t\treturn;\n\t\thp = skb_header_pointer(skb, offset, sizeof(_hdr), &_hdr);\n\t\tif (!hp)\n\t\t\treturn;\n\t\tif (*nexthdr == NEXTHDR_FRAGMENT)\n\t\t\tbreak;\n\t\t*nexthdr = hp->nexthdr;\n\t}\n}\n\nunion network_header {\n\tstruct iphdr *ipv4;\n\tstruct ipv6hdr *ipv6;\n\tvoid *raw;\n};\n\nstatic u8 wx_encode_tx_desc_ptype(const struct wx_tx_buffer *first)\n{\n\tu8 tun_prot = 0, l4_prot = 0, ptype = 0;\n\tstruct sk_buff *skb = first->skb;\n\n\tif (skb->encapsulation) {\n\t\tunion network_header hdr;\n\n\t\tswitch (first->protocol) {\n\t\tcase htons(ETH_P_IP):\n\t\t\ttun_prot = ip_hdr(skb)->protocol;\n\t\t\tptype = WX_PTYPE_TUN_IPV4;\n\t\t\tbreak;\n\t\tcase htons(ETH_P_IPV6):\n\t\t\twx_get_ipv6_proto(skb, skb_network_offset(skb), &tun_prot);\n\t\t\tptype = WX_PTYPE_TUN_IPV6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn ptype;\n\t\t}\n\n\t\tif (tun_prot == IPPROTO_IPIP) {\n\t\t\thdr.raw = (void *)inner_ip_hdr(skb);\n\t\t\tptype |= WX_PTYPE_PKT_IPIP;\n\t\t} else if (tun_prot == IPPROTO_UDP) {\n\t\t\thdr.raw = (void *)inner_ip_hdr(skb);\n\t\t\tif (skb->inner_protocol_type != ENCAP_TYPE_ETHER ||\n\t\t\t    skb->inner_protocol != htons(ETH_P_TEB)) {\n\t\t\t\tptype |= WX_PTYPE_PKT_IG;\n\t\t\t} else {\n\t\t\t\tif (((struct ethhdr *)skb_inner_mac_header(skb))->h_proto\n\t\t\t\t     == htons(ETH_P_8021Q))\n\t\t\t\t\tptype |= WX_PTYPE_PKT_IGMV;\n\t\t\t\telse\n\t\t\t\t\tptype |= WX_PTYPE_PKT_IGM;\n\t\t\t}\n\n\t\t} else if (tun_prot == IPPROTO_GRE) {\n\t\t\thdr.raw = (void *)inner_ip_hdr(skb);\n\t\t\tif (skb->inner_protocol ==  htons(ETH_P_IP) ||\n\t\t\t    skb->inner_protocol ==  htons(ETH_P_IPV6)) {\n\t\t\t\tptype |= WX_PTYPE_PKT_IG;\n\t\t\t} else {\n\t\t\t\tif (((struct ethhdr *)skb_inner_mac_header(skb))->h_proto\n\t\t\t\t    == htons(ETH_P_8021Q))\n\t\t\t\t\tptype |= WX_PTYPE_PKT_IGMV;\n\t\t\t\telse\n\t\t\t\t\tptype |= WX_PTYPE_PKT_IGM;\n\t\t\t}\n\t\t} else {\n\t\t\treturn ptype;\n\t\t}\n\n\t\tswitch (hdr.ipv4->version) {\n\t\tcase IPVERSION:\n\t\t\tl4_prot = hdr.ipv4->protocol;\n\t\t\tbreak;\n\t\tcase 6:\n\t\t\twx_get_ipv6_proto(skb, skb_inner_network_offset(skb), &l4_prot);\n\t\t\tptype |= WX_PTYPE_PKT_IPV6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn ptype;\n\t\t}\n\t} else {\n\t\tswitch (first->protocol) {\n\t\tcase htons(ETH_P_IP):\n\t\t\tl4_prot = ip_hdr(skb)->protocol;\n\t\t\tptype = WX_PTYPE_PKT_IP;\n\t\t\tbreak;\n\t\tcase htons(ETH_P_IPV6):\n\t\t\twx_get_ipv6_proto(skb, skb_network_offset(skb), &l4_prot);\n\t\t\tptype = WX_PTYPE_PKT_IP | WX_PTYPE_PKT_IPV6;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn WX_PTYPE_PKT_MAC | WX_PTYPE_TYP_MAC;\n\t\t}\n\t}\n\tswitch (l4_prot) {\n\tcase IPPROTO_TCP:\n\t\tptype |= WX_PTYPE_TYP_TCP;\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\tptype |= WX_PTYPE_TYP_UDP;\n\t\tbreak;\n\tcase IPPROTO_SCTP:\n\t\tptype |= WX_PTYPE_TYP_SCTP;\n\t\tbreak;\n\tdefault:\n\t\tptype |= WX_PTYPE_TYP_IP;\n\t\tbreak;\n\t}\n\n\treturn ptype;\n}\n\nstatic int wx_tso(struct wx_ring *tx_ring, struct wx_tx_buffer *first,\n\t\t  u8 *hdr_len, u8 ptype)\n{\n\tu32 vlan_macip_lens, type_tucmd, mss_l4len_idx;\n\tstruct net_device *netdev = tx_ring->netdev;\n\tu32 l4len, tunhdr_eiplen_tunlen = 0;\n\tstruct sk_buff *skb = first->skb;\n\tbool enc = skb->encapsulation;\n\tstruct ipv6hdr *ipv6h;\n\tstruct tcphdr *tcph;\n\tstruct iphdr *iph;\n\tu8 tun_prot = 0;\n\tint err;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\tif (!skb_is_gso(skb))\n\t\treturn 0;\n\n\terr = skb_cow_head(skb, 0);\n\tif (err < 0)\n\t\treturn err;\n\n\t \n\tiph = enc ? inner_ip_hdr(skb) : ip_hdr(skb);\n\tif (iph->version == 4) {\n\t\ttcph = enc ? inner_tcp_hdr(skb) : tcp_hdr(skb);\n\t\tiph->tot_len = 0;\n\t\tiph->check = 0;\n\t\ttcph->check = ~csum_tcpudp_magic(iph->saddr,\n\t\t\t\t\t\t iph->daddr, 0,\n\t\t\t\t\t\t IPPROTO_TCP, 0);\n\t\tfirst->tx_flags |= WX_TX_FLAGS_TSO |\n\t\t\t\t   WX_TX_FLAGS_CSUM |\n\t\t\t\t   WX_TX_FLAGS_IPV4 |\n\t\t\t\t   WX_TX_FLAGS_CC;\n\t} else if (iph->version == 6 && skb_is_gso_v6(skb)) {\n\t\tipv6h = enc ? inner_ipv6_hdr(skb) : ipv6_hdr(skb);\n\t\ttcph = enc ? inner_tcp_hdr(skb) : tcp_hdr(skb);\n\t\tipv6h->payload_len = 0;\n\t\ttcph->check = ~csum_ipv6_magic(&ipv6h->saddr,\n\t\t\t\t\t       &ipv6h->daddr, 0,\n\t\t\t\t\t       IPPROTO_TCP, 0);\n\t\tfirst->tx_flags |= WX_TX_FLAGS_TSO |\n\t\t\t\t   WX_TX_FLAGS_CSUM |\n\t\t\t\t   WX_TX_FLAGS_CC;\n\t}\n\n\t \n\tl4len = enc ? inner_tcp_hdrlen(skb) : tcp_hdrlen(skb);\n\t*hdr_len = enc ? (skb_inner_transport_header(skb) - skb->data) :\n\t\t\t skb_transport_offset(skb);\n\t*hdr_len += l4len;\n\n\t \n\tfirst->gso_segs = skb_shinfo(skb)->gso_segs;\n\tfirst->bytecount += (first->gso_segs - 1) * *hdr_len;\n\n\t \n\tmss_l4len_idx = l4len << WX_TXD_L4LEN_SHIFT;\n\tmss_l4len_idx |= skb_shinfo(skb)->gso_size << WX_TXD_MSS_SHIFT;\n\n\t \n\tif (enc) {\n\t\tswitch (first->protocol) {\n\t\tcase htons(ETH_P_IP):\n\t\t\ttun_prot = ip_hdr(skb)->protocol;\n\t\t\tfirst->tx_flags |= WX_TX_FLAGS_OUTER_IPV4;\n\t\t\tbreak;\n\t\tcase htons(ETH_P_IPV6):\n\t\t\ttun_prot = ipv6_hdr(skb)->nexthdr;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tswitch (tun_prot) {\n\t\tcase IPPROTO_UDP:\n\t\t\ttunhdr_eiplen_tunlen = WX_TXD_TUNNEL_UDP;\n\t\t\ttunhdr_eiplen_tunlen |= ((skb_network_header_len(skb) >> 2) <<\n\t\t\t\t\t\t WX_TXD_OUTER_IPLEN_SHIFT) |\n\t\t\t\t\t\t(((skb_inner_mac_header(skb) -\n\t\t\t\t\t\tskb_transport_header(skb)) >> 1) <<\n\t\t\t\t\t\tWX_TXD_TUNNEL_LEN_SHIFT);\n\t\t\tbreak;\n\t\tcase IPPROTO_GRE:\n\t\t\ttunhdr_eiplen_tunlen = WX_TXD_TUNNEL_GRE;\n\t\t\ttunhdr_eiplen_tunlen |= ((skb_network_header_len(skb) >> 2) <<\n\t\t\t\t\t\t WX_TXD_OUTER_IPLEN_SHIFT) |\n\t\t\t\t\t\t(((skb_inner_mac_header(skb) -\n\t\t\t\t\t\tskb_transport_header(skb)) >> 1) <<\n\t\t\t\t\t\tWX_TXD_TUNNEL_LEN_SHIFT);\n\t\t\tbreak;\n\t\tcase IPPROTO_IPIP:\n\t\t\ttunhdr_eiplen_tunlen = (((char *)inner_ip_hdr(skb) -\n\t\t\t\t\t\t(char *)ip_hdr(skb)) >> 2) <<\n\t\t\t\t\t\tWX_TXD_OUTER_IPLEN_SHIFT;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t\tvlan_macip_lens = skb_inner_network_header_len(skb) >> 1;\n\t} else {\n\t\tvlan_macip_lens = skb_network_header_len(skb) >> 1;\n\t}\n\n\tvlan_macip_lens |= skb_network_offset(skb) << WX_TXD_MACLEN_SHIFT;\n\tvlan_macip_lens |= first->tx_flags & WX_TX_FLAGS_VLAN_MASK;\n\n\ttype_tucmd = ptype << 24;\n\tif (skb->vlan_proto == htons(ETH_P_8021AD) &&\n\t    netdev->features & NETIF_F_HW_VLAN_STAG_TX)\n\t\ttype_tucmd |= WX_SET_FLAG(first->tx_flags,\n\t\t\t\t\t  WX_TX_FLAGS_HW_VLAN,\n\t\t\t\t\t  0x1 << WX_TXD_TAG_TPID_SEL_SHIFT);\n\twx_tx_ctxtdesc(tx_ring, vlan_macip_lens, tunhdr_eiplen_tunlen,\n\t\t       type_tucmd, mss_l4len_idx);\n\n\treturn 1;\n}\n\nstatic void wx_tx_csum(struct wx_ring *tx_ring, struct wx_tx_buffer *first,\n\t\t       u8 ptype)\n{\n\tu32 tunhdr_eiplen_tunlen = 0, vlan_macip_lens = 0;\n\tstruct net_device *netdev = tx_ring->netdev;\n\tu32 mss_l4len_idx = 0, type_tucmd;\n\tstruct sk_buff *skb = first->skb;\n\tu8 tun_prot = 0;\n\n\tif (skb->ip_summed != CHECKSUM_PARTIAL) {\n\t\tif (!(first->tx_flags & WX_TX_FLAGS_HW_VLAN) &&\n\t\t    !(first->tx_flags & WX_TX_FLAGS_CC))\n\t\t\treturn;\n\t\tvlan_macip_lens = skb_network_offset(skb) <<\n\t\t\t\t  WX_TXD_MACLEN_SHIFT;\n\t} else {\n\t\tu8 l4_prot = 0;\n\t\tunion {\n\t\t\tstruct iphdr *ipv4;\n\t\t\tstruct ipv6hdr *ipv6;\n\t\t\tu8 *raw;\n\t\t} network_hdr;\n\t\tunion {\n\t\t\tstruct tcphdr *tcphdr;\n\t\t\tu8 *raw;\n\t\t} transport_hdr;\n\n\t\tif (skb->encapsulation) {\n\t\t\tnetwork_hdr.raw = skb_inner_network_header(skb);\n\t\t\ttransport_hdr.raw = skb_inner_transport_header(skb);\n\t\t\tvlan_macip_lens = skb_network_offset(skb) <<\n\t\t\t\t\t  WX_TXD_MACLEN_SHIFT;\n\t\t\tswitch (first->protocol) {\n\t\t\tcase htons(ETH_P_IP):\n\t\t\t\ttun_prot = ip_hdr(skb)->protocol;\n\t\t\t\tbreak;\n\t\t\tcase htons(ETH_P_IPV6):\n\t\t\t\ttun_prot = ipv6_hdr(skb)->nexthdr;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tswitch (tun_prot) {\n\t\t\tcase IPPROTO_UDP:\n\t\t\t\ttunhdr_eiplen_tunlen = WX_TXD_TUNNEL_UDP;\n\t\t\t\ttunhdr_eiplen_tunlen |=\n\t\t\t\t\t((skb_network_header_len(skb) >> 2) <<\n\t\t\t\t\tWX_TXD_OUTER_IPLEN_SHIFT) |\n\t\t\t\t\t(((skb_inner_mac_header(skb) -\n\t\t\t\t\tskb_transport_header(skb)) >> 1) <<\n\t\t\t\t\tWX_TXD_TUNNEL_LEN_SHIFT);\n\t\t\t\tbreak;\n\t\t\tcase IPPROTO_GRE:\n\t\t\t\ttunhdr_eiplen_tunlen = WX_TXD_TUNNEL_GRE;\n\t\t\t\ttunhdr_eiplen_tunlen |= ((skb_network_header_len(skb) >> 2) <<\n\t\t\t\t\t\t\t WX_TXD_OUTER_IPLEN_SHIFT) |\n\t\t\t\t\t\t\t (((skb_inner_mac_header(skb) -\n\t\t\t\t\t\t\t    skb_transport_header(skb)) >> 1) <<\n\t\t\t\t\t\t\t  WX_TXD_TUNNEL_LEN_SHIFT);\n\t\t\t\tbreak;\n\t\t\tcase IPPROTO_IPIP:\n\t\t\t\ttunhdr_eiplen_tunlen = (((char *)inner_ip_hdr(skb) -\n\t\t\t\t\t\t\t(char *)ip_hdr(skb)) >> 2) <<\n\t\t\t\t\t\t\tWX_TXD_OUTER_IPLEN_SHIFT;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t} else {\n\t\t\tnetwork_hdr.raw = skb_network_header(skb);\n\t\t\ttransport_hdr.raw = skb_transport_header(skb);\n\t\t\tvlan_macip_lens = skb_network_offset(skb) <<\n\t\t\t\t\t  WX_TXD_MACLEN_SHIFT;\n\t\t}\n\n\t\tswitch (network_hdr.ipv4->version) {\n\t\tcase IPVERSION:\n\t\t\tvlan_macip_lens |= (transport_hdr.raw - network_hdr.raw) >> 1;\n\t\t\tl4_prot = network_hdr.ipv4->protocol;\n\t\t\tbreak;\n\t\tcase 6:\n\t\t\tvlan_macip_lens |= (transport_hdr.raw - network_hdr.raw) >> 1;\n\t\t\tl4_prot = network_hdr.ipv6->nexthdr;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tswitch (l4_prot) {\n\t\tcase IPPROTO_TCP:\n\t\tmss_l4len_idx = (transport_hdr.tcphdr->doff * 4) <<\n\t\t\t\tWX_TXD_L4LEN_SHIFT;\n\t\t\tbreak;\n\t\tcase IPPROTO_SCTP:\n\t\t\tmss_l4len_idx = sizeof(struct sctphdr) <<\n\t\t\t\t\tWX_TXD_L4LEN_SHIFT;\n\t\t\tbreak;\n\t\tcase IPPROTO_UDP:\n\t\t\tmss_l4len_idx = sizeof(struct udphdr) <<\n\t\t\t\t\tWX_TXD_L4LEN_SHIFT;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tfirst->tx_flags |= WX_TX_FLAGS_CSUM;\n\t}\n\tfirst->tx_flags |= WX_TX_FLAGS_CC;\n\t \n\tvlan_macip_lens |= first->tx_flags & WX_TX_FLAGS_VLAN_MASK;\n\n\ttype_tucmd = ptype << 24;\n\tif (skb->vlan_proto == htons(ETH_P_8021AD) &&\n\t    netdev->features & NETIF_F_HW_VLAN_STAG_TX)\n\t\ttype_tucmd |= WX_SET_FLAG(first->tx_flags,\n\t\t\t\t\t  WX_TX_FLAGS_HW_VLAN,\n\t\t\t\t\t  0x1 << WX_TXD_TAG_TPID_SEL_SHIFT);\n\twx_tx_ctxtdesc(tx_ring, vlan_macip_lens, tunhdr_eiplen_tunlen,\n\t\t       type_tucmd, mss_l4len_idx);\n}\n\nstatic netdev_tx_t wx_xmit_frame_ring(struct sk_buff *skb,\n\t\t\t\t      struct wx_ring *tx_ring)\n{\n\tu16 count = TXD_USE_COUNT(skb_headlen(skb));\n\tstruct wx_tx_buffer *first;\n\tu8 hdr_len = 0, ptype;\n\tunsigned short f;\n\tu32 tx_flags = 0;\n\tint tso;\n\n\t \n\tfor (f = 0; f < skb_shinfo(skb)->nr_frags; f++)\n\t\tcount += TXD_USE_COUNT(skb_frag_size(&skb_shinfo(skb)->\n\t\t\t\t\t\t     frags[f]));\n\n\tif (wx_maybe_stop_tx(tx_ring, count + 3))\n\t\treturn NETDEV_TX_BUSY;\n\n\t \n\tfirst = &tx_ring->tx_buffer_info[tx_ring->next_to_use];\n\tfirst->skb = skb;\n\tfirst->bytecount = skb->len;\n\tfirst->gso_segs = 1;\n\n\t \n\tif (skb_vlan_tag_present(skb)) {\n\t\ttx_flags |= skb_vlan_tag_get(skb) << WX_TX_FLAGS_VLAN_SHIFT;\n\t\ttx_flags |= WX_TX_FLAGS_HW_VLAN;\n\t}\n\n\t \n\tfirst->tx_flags = tx_flags;\n\tfirst->protocol = vlan_get_protocol(skb);\n\n\tptype = wx_encode_tx_desc_ptype(first);\n\n\ttso = wx_tso(tx_ring, first, &hdr_len, ptype);\n\tif (tso < 0)\n\t\tgoto out_drop;\n\telse if (!tso)\n\t\twx_tx_csum(tx_ring, first, ptype);\n\twx_tx_map(tx_ring, first, hdr_len);\n\n\treturn NETDEV_TX_OK;\nout_drop:\n\tdev_kfree_skb_any(first->skb);\n\tfirst->skb = NULL;\n\n\treturn NETDEV_TX_OK;\n}\n\nnetdev_tx_t wx_xmit_frame(struct sk_buff *skb,\n\t\t\t  struct net_device *netdev)\n{\n\tunsigned int r_idx = skb->queue_mapping;\n\tstruct wx *wx = netdev_priv(netdev);\n\tstruct wx_ring *tx_ring;\n\n\tif (!netif_carrier_ok(netdev)) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\t \n\tif (skb_put_padto(skb, 17))\n\t\treturn NETDEV_TX_OK;\n\n\tif (r_idx >= wx->num_tx_queues)\n\t\tr_idx = r_idx % wx->num_tx_queues;\n\ttx_ring = wx->tx_ring[r_idx];\n\n\treturn wx_xmit_frame_ring(skb, tx_ring);\n}\nEXPORT_SYMBOL(wx_xmit_frame);\n\nvoid wx_napi_enable_all(struct wx *wx)\n{\n\tstruct wx_q_vector *q_vector;\n\tint q_idx;\n\n\tfor (q_idx = 0; q_idx < wx->num_q_vectors; q_idx++) {\n\t\tq_vector = wx->q_vector[q_idx];\n\t\tnapi_enable(&q_vector->napi);\n\t}\n}\nEXPORT_SYMBOL(wx_napi_enable_all);\n\nvoid wx_napi_disable_all(struct wx *wx)\n{\n\tstruct wx_q_vector *q_vector;\n\tint q_idx;\n\n\tfor (q_idx = 0; q_idx < wx->num_q_vectors; q_idx++) {\n\t\tq_vector = wx->q_vector[q_idx];\n\t\tnapi_disable(&q_vector->napi);\n\t}\n}\nEXPORT_SYMBOL(wx_napi_disable_all);\n\n \nstatic void wx_set_rss_queues(struct wx *wx)\n{\n\twx->num_rx_queues = wx->mac.max_rx_queues;\n\twx->num_tx_queues = wx->mac.max_tx_queues;\n}\n\nstatic void wx_set_num_queues(struct wx *wx)\n{\n\t \n\twx->num_rx_queues = 1;\n\twx->num_tx_queues = 1;\n\twx->queues_per_pool = 1;\n\n\twx_set_rss_queues(wx);\n}\n\n \nstatic int wx_acquire_msix_vectors(struct wx *wx)\n{\n\tstruct irq_affinity affd = {0, };\n\tint nvecs, i;\n\n\tnvecs = min_t(int, num_online_cpus(), wx->mac.max_msix_vectors);\n\n\twx->msix_entries = kcalloc(nvecs,\n\t\t\t\t   sizeof(struct msix_entry),\n\t\t\t\t   GFP_KERNEL);\n\tif (!wx->msix_entries)\n\t\treturn -ENOMEM;\n\n\tnvecs = pci_alloc_irq_vectors_affinity(wx->pdev, nvecs,\n\t\t\t\t\t       nvecs,\n\t\t\t\t\t       PCI_IRQ_MSIX | PCI_IRQ_AFFINITY,\n\t\t\t\t\t       &affd);\n\tif (nvecs < 0) {\n\t\twx_err(wx, \"Failed to allocate MSI-X interrupts. Err: %d\\n\", nvecs);\n\t\tkfree(wx->msix_entries);\n\t\twx->msix_entries = NULL;\n\t\treturn nvecs;\n\t}\n\n\tfor (i = 0; i < nvecs; i++) {\n\t\twx->msix_entries[i].entry = i;\n\t\twx->msix_entries[i].vector = pci_irq_vector(wx->pdev, i);\n\t}\n\n\t \n\tnvecs -= 1;\n\twx->num_q_vectors = nvecs;\n\twx->num_rx_queues = nvecs;\n\twx->num_tx_queues = nvecs;\n\n\treturn 0;\n}\n\n \nstatic int wx_set_interrupt_capability(struct wx *wx)\n{\n\tstruct pci_dev *pdev = wx->pdev;\n\tint nvecs, ret;\n\n\t \n\tret = wx_acquire_msix_vectors(wx);\n\tif (ret == 0 || (ret == -ENOMEM))\n\t\treturn ret;\n\n\twx->num_rx_queues = 1;\n\twx->num_tx_queues = 1;\n\twx->num_q_vectors = 1;\n\n\t \n\tnvecs = 1;\n\tnvecs = pci_alloc_irq_vectors(pdev, nvecs,\n\t\t\t\t      nvecs, PCI_IRQ_MSI | PCI_IRQ_LEGACY);\n\tif (nvecs == 1) {\n\t\tif (pdev->msi_enabled)\n\t\t\twx_err(wx, \"Fallback to MSI.\\n\");\n\t\telse\n\t\t\twx_err(wx, \"Fallback to LEGACY.\\n\");\n\t} else {\n\t\twx_err(wx, \"Failed to allocate MSI/LEGACY interrupts. Error: %d\\n\", nvecs);\n\t\treturn nvecs;\n\t}\n\n\tpdev->irq = pci_irq_vector(pdev, 0);\n\n\treturn 0;\n}\n\n \nstatic void wx_cache_ring_rss(struct wx *wx)\n{\n\tu16 i;\n\n\tfor (i = 0; i < wx->num_rx_queues; i++)\n\t\twx->rx_ring[i]->reg_idx = i;\n\n\tfor (i = 0; i < wx->num_tx_queues; i++)\n\t\twx->tx_ring[i]->reg_idx = i;\n}\n\nstatic void wx_add_ring(struct wx_ring *ring, struct wx_ring_container *head)\n{\n\tring->next = head->ring;\n\thead->ring = ring;\n\thead->count++;\n}\n\n \nstatic int wx_alloc_q_vector(struct wx *wx,\n\t\t\t     unsigned int v_count, unsigned int v_idx,\n\t\t\t     unsigned int txr_count, unsigned int txr_idx,\n\t\t\t     unsigned int rxr_count, unsigned int rxr_idx)\n{\n\tstruct wx_q_vector *q_vector;\n\tint ring_count, default_itr;\n\tstruct wx_ring *ring;\n\n\t \n\tring_count = txr_count + rxr_count;\n\n\tq_vector = kzalloc(struct_size(q_vector, ring, ring_count),\n\t\t\t   GFP_KERNEL);\n\tif (!q_vector)\n\t\treturn -ENOMEM;\n\n\t \n\tnetif_napi_add(wx->netdev, &q_vector->napi,\n\t\t       wx_poll);\n\n\t \n\twx->q_vector[v_idx] = q_vector;\n\tq_vector->wx = wx;\n\tq_vector->v_idx = v_idx;\n\tif (cpu_online(v_idx))\n\t\tq_vector->numa_node = cpu_to_node(v_idx);\n\n\t \n\tring = q_vector->ring;\n\n\tif (wx->mac.type == wx_mac_sp)\n\t\tdefault_itr = WX_12K_ITR;\n\telse\n\t\tdefault_itr = WX_7K_ITR;\n\t \n\tif (txr_count && !rxr_count)\n\t\t \n\t\tq_vector->itr = wx->tx_itr_setting ?\n\t\t\t\tdefault_itr : wx->tx_itr_setting;\n\telse\n\t\t \n\t\tq_vector->itr = wx->rx_itr_setting ?\n\t\t\t\tdefault_itr : wx->rx_itr_setting;\n\n\twhile (txr_count) {\n\t\t \n\t\tring->dev = &wx->pdev->dev;\n\t\tring->netdev = wx->netdev;\n\n\t\t \n\t\tring->q_vector = q_vector;\n\n\t\t \n\t\twx_add_ring(ring, &q_vector->tx);\n\n\t\t \n\t\tring->count = wx->tx_ring_count;\n\n\t\tring->queue_index = txr_idx;\n\n\t\t \n\t\twx->tx_ring[txr_idx] = ring;\n\n\t\t \n\t\ttxr_count--;\n\t\ttxr_idx += v_count;\n\n\t\t \n\t\tring++;\n\t}\n\n\twhile (rxr_count) {\n\t\t \n\t\tring->dev = &wx->pdev->dev;\n\t\tring->netdev = wx->netdev;\n\n\t\t \n\t\tring->q_vector = q_vector;\n\n\t\t \n\t\twx_add_ring(ring, &q_vector->rx);\n\n\t\t \n\t\tring->count = wx->rx_ring_count;\n\t\tring->queue_index = rxr_idx;\n\n\t\t \n\t\twx->rx_ring[rxr_idx] = ring;\n\n\t\t \n\t\trxr_count--;\n\t\trxr_idx += v_count;\n\n\t\t \n\t\tring++;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void wx_free_q_vector(struct wx *wx, int v_idx)\n{\n\tstruct wx_q_vector *q_vector = wx->q_vector[v_idx];\n\tstruct wx_ring *ring;\n\n\twx_for_each_ring(ring, q_vector->tx)\n\t\twx->tx_ring[ring->queue_index] = NULL;\n\n\twx_for_each_ring(ring, q_vector->rx)\n\t\twx->rx_ring[ring->queue_index] = NULL;\n\n\twx->q_vector[v_idx] = NULL;\n\tnetif_napi_del(&q_vector->napi);\n\tkfree_rcu(q_vector, rcu);\n}\n\n \nstatic int wx_alloc_q_vectors(struct wx *wx)\n{\n\tunsigned int rxr_idx = 0, txr_idx = 0, v_idx = 0;\n\tunsigned int rxr_remaining = wx->num_rx_queues;\n\tunsigned int txr_remaining = wx->num_tx_queues;\n\tunsigned int q_vectors = wx->num_q_vectors;\n\tint rqpv, tqpv;\n\tint err;\n\n\tfor (; v_idx < q_vectors; v_idx++) {\n\t\trqpv = DIV_ROUND_UP(rxr_remaining, q_vectors - v_idx);\n\t\ttqpv = DIV_ROUND_UP(txr_remaining, q_vectors - v_idx);\n\t\terr = wx_alloc_q_vector(wx, q_vectors, v_idx,\n\t\t\t\t\ttqpv, txr_idx,\n\t\t\t\t\trqpv, rxr_idx);\n\n\t\tif (err)\n\t\t\tgoto err_out;\n\n\t\t \n\t\trxr_remaining -= rqpv;\n\t\ttxr_remaining -= tqpv;\n\t\trxr_idx++;\n\t\ttxr_idx++;\n\t}\n\n\treturn 0;\n\nerr_out:\n\twx->num_tx_queues = 0;\n\twx->num_rx_queues = 0;\n\twx->num_q_vectors = 0;\n\n\twhile (v_idx--)\n\t\twx_free_q_vector(wx, v_idx);\n\n\treturn -ENOMEM;\n}\n\n \nstatic void wx_free_q_vectors(struct wx *wx)\n{\n\tint v_idx = wx->num_q_vectors;\n\n\twx->num_tx_queues = 0;\n\twx->num_rx_queues = 0;\n\twx->num_q_vectors = 0;\n\n\twhile (v_idx--)\n\t\twx_free_q_vector(wx, v_idx);\n}\n\nvoid wx_reset_interrupt_capability(struct wx *wx)\n{\n\tstruct pci_dev *pdev = wx->pdev;\n\n\tif (!pdev->msi_enabled && !pdev->msix_enabled)\n\t\treturn;\n\n\tif (pdev->msix_enabled) {\n\t\tkfree(wx->msix_entries);\n\t\twx->msix_entries = NULL;\n\t}\n\tpci_free_irq_vectors(wx->pdev);\n}\nEXPORT_SYMBOL(wx_reset_interrupt_capability);\n\n \nvoid wx_clear_interrupt_scheme(struct wx *wx)\n{\n\twx_free_q_vectors(wx);\n\twx_reset_interrupt_capability(wx);\n}\nEXPORT_SYMBOL(wx_clear_interrupt_scheme);\n\nint wx_init_interrupt_scheme(struct wx *wx)\n{\n\tint ret;\n\n\t \n\twx_set_num_queues(wx);\n\n\t \n\tret = wx_set_interrupt_capability(wx);\n\tif (ret) {\n\t\twx_err(wx, \"Allocate irq vectors for failed.\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tret = wx_alloc_q_vectors(wx);\n\tif (ret) {\n\t\twx_err(wx, \"Unable to allocate memory for queue vectors.\\n\");\n\t\twx_reset_interrupt_capability(wx);\n\t\treturn ret;\n\t}\n\n\twx_cache_ring_rss(wx);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(wx_init_interrupt_scheme);\n\nirqreturn_t wx_msix_clean_rings(int __always_unused irq, void *data)\n{\n\tstruct wx_q_vector *q_vector = data;\n\n\t \n\tif (q_vector->rx.ring || q_vector->tx.ring)\n\t\tnapi_schedule_irqoff(&q_vector->napi);\n\n\treturn IRQ_HANDLED;\n}\nEXPORT_SYMBOL(wx_msix_clean_rings);\n\nvoid wx_free_irq(struct wx *wx)\n{\n\tstruct pci_dev *pdev = wx->pdev;\n\tint vector;\n\n\tif (!(pdev->msix_enabled)) {\n\t\tfree_irq(pdev->irq, wx);\n\t\treturn;\n\t}\n\n\tfor (vector = 0; vector < wx->num_q_vectors; vector++) {\n\t\tstruct wx_q_vector *q_vector = wx->q_vector[vector];\n\t\tstruct msix_entry *entry = &wx->msix_entries[vector];\n\n\t\t \n\t\tif (!q_vector->rx.ring && !q_vector->tx.ring)\n\t\t\tcontinue;\n\n\t\tfree_irq(entry->vector, q_vector);\n\t}\n\n\tif (wx->mac.type == wx_mac_em)\n\t\tfree_irq(wx->msix_entries[vector].vector, wx);\n}\nEXPORT_SYMBOL(wx_free_irq);\n\n \nint wx_setup_isb_resources(struct wx *wx)\n{\n\tstruct pci_dev *pdev = wx->pdev;\n\n\twx->isb_mem = dma_alloc_coherent(&pdev->dev,\n\t\t\t\t\t sizeof(u32) * 4,\n\t\t\t\t\t &wx->isb_dma,\n\t\t\t\t\t GFP_KERNEL);\n\tif (!wx->isb_mem) {\n\t\twx_err(wx, \"Alloc isb_mem failed\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(wx_setup_isb_resources);\n\n \nvoid wx_free_isb_resources(struct wx *wx)\n{\n\tstruct pci_dev *pdev = wx->pdev;\n\n\tdma_free_coherent(&pdev->dev, sizeof(u32) * 4,\n\t\t\t  wx->isb_mem, wx->isb_dma);\n\twx->isb_mem = NULL;\n}\nEXPORT_SYMBOL(wx_free_isb_resources);\n\nu32 wx_misc_isb(struct wx *wx, enum wx_isb_idx idx)\n{\n\tu32 cur_tag = 0;\n\n\tcur_tag = wx->isb_mem[WX_ISB_HEADER];\n\twx->isb_tag[idx] = cur_tag;\n\n\treturn (__force u32)cpu_to_le32(wx->isb_mem[idx]);\n}\nEXPORT_SYMBOL(wx_misc_isb);\n\n \nstatic void wx_set_ivar(struct wx *wx, s8 direction,\n\t\t\tu16 queue, u16 msix_vector)\n{\n\tu32 ivar, index;\n\n\tif (direction == -1) {\n\t\t \n\t\tmsix_vector |= WX_PX_IVAR_ALLOC_VAL;\n\t\tindex = 0;\n\t\tivar = rd32(wx, WX_PX_MISC_IVAR);\n\t\tivar &= ~(0xFF << index);\n\t\tivar |= (msix_vector << index);\n\t\twr32(wx, WX_PX_MISC_IVAR, ivar);\n\t} else {\n\t\t \n\t\tmsix_vector |= WX_PX_IVAR_ALLOC_VAL;\n\t\tindex = ((16 * (queue & 1)) + (8 * direction));\n\t\tivar = rd32(wx, WX_PX_IVAR(queue >> 1));\n\t\tivar &= ~(0xFF << index);\n\t\tivar |= (msix_vector << index);\n\t\twr32(wx, WX_PX_IVAR(queue >> 1), ivar);\n\t}\n}\n\n \nstatic void wx_write_eitr(struct wx_q_vector *q_vector)\n{\n\tstruct wx *wx = q_vector->wx;\n\tint v_idx = q_vector->v_idx;\n\tu32 itr_reg;\n\n\tif (wx->mac.type == wx_mac_sp)\n\t\titr_reg = q_vector->itr & WX_SP_MAX_EITR;\n\telse\n\t\titr_reg = q_vector->itr & WX_EM_MAX_EITR;\n\n\titr_reg |= WX_PX_ITR_CNT_WDIS;\n\n\twr32(wx, WX_PX_ITR(v_idx), itr_reg);\n}\n\n \nvoid wx_configure_vectors(struct wx *wx)\n{\n\tstruct pci_dev *pdev = wx->pdev;\n\tu32 eitrsel = 0;\n\tu16 v_idx;\n\n\tif (pdev->msix_enabled) {\n\t\t \n\t\twr32(wx, WX_PX_ITRSEL, eitrsel);\n\t\t \n\t\twr32(wx, WX_PX_GPIE, WX_PX_GPIE_MODEL);\n\t} else {\n\t\t \n\t\twr32(wx, WX_PX_GPIE, 0);\n\t}\n\n\t \n\tfor (v_idx = 0; v_idx < wx->num_q_vectors; v_idx++) {\n\t\tstruct wx_q_vector *q_vector = wx->q_vector[v_idx];\n\t\tstruct wx_ring *ring;\n\n\t\twx_for_each_ring(ring, q_vector->rx)\n\t\t\twx_set_ivar(wx, 0, ring->reg_idx, v_idx);\n\n\t\twx_for_each_ring(ring, q_vector->tx)\n\t\t\twx_set_ivar(wx, 1, ring->reg_idx, v_idx);\n\n\t\twx_write_eitr(q_vector);\n\t}\n\n\twx_set_ivar(wx, -1, 0, v_idx);\n\tif (pdev->msix_enabled)\n\t\twr32(wx, WX_PX_ITR(v_idx), 1950);\n}\nEXPORT_SYMBOL(wx_configure_vectors);\n\n \nstatic void wx_clean_rx_ring(struct wx_ring *rx_ring)\n{\n\tstruct wx_rx_buffer *rx_buffer;\n\tu16 i = rx_ring->next_to_clean;\n\n\trx_buffer = &rx_ring->rx_buffer_info[i];\n\n\t \n\twhile (i != rx_ring->next_to_alloc) {\n\t\tif (rx_buffer->skb) {\n\t\t\tstruct sk_buff *skb = rx_buffer->skb;\n\n\t\t\tif (WX_CB(skb)->page_released)\n\t\t\t\tpage_pool_put_full_page(rx_ring->page_pool, rx_buffer->page, false);\n\n\t\t\tdev_kfree_skb(skb);\n\t\t}\n\n\t\t \n\t\tdma_sync_single_range_for_cpu(rx_ring->dev,\n\t\t\t\t\t      rx_buffer->dma,\n\t\t\t\t\t      rx_buffer->page_offset,\n\t\t\t\t\t      WX_RX_BUFSZ,\n\t\t\t\t\t      DMA_FROM_DEVICE);\n\n\t\t \n\t\tpage_pool_put_full_page(rx_ring->page_pool, rx_buffer->page, false);\n\n\t\ti++;\n\t\trx_buffer++;\n\t\tif (i == rx_ring->count) {\n\t\t\ti = 0;\n\t\t\trx_buffer = rx_ring->rx_buffer_info;\n\t\t}\n\t}\n\n\trx_ring->next_to_alloc = 0;\n\trx_ring->next_to_clean = 0;\n\trx_ring->next_to_use = 0;\n}\n\n \nvoid wx_clean_all_rx_rings(struct wx *wx)\n{\n\tint i;\n\n\tfor (i = 0; i < wx->num_rx_queues; i++)\n\t\twx_clean_rx_ring(wx->rx_ring[i]);\n}\nEXPORT_SYMBOL(wx_clean_all_rx_rings);\n\n \nstatic void wx_free_rx_resources(struct wx_ring *rx_ring)\n{\n\twx_clean_rx_ring(rx_ring);\n\tkvfree(rx_ring->rx_buffer_info);\n\trx_ring->rx_buffer_info = NULL;\n\n\t \n\tif (!rx_ring->desc)\n\t\treturn;\n\n\tdma_free_coherent(rx_ring->dev, rx_ring->size,\n\t\t\t  rx_ring->desc, rx_ring->dma);\n\n\trx_ring->desc = NULL;\n\n\tif (rx_ring->page_pool) {\n\t\tpage_pool_destroy(rx_ring->page_pool);\n\t\trx_ring->page_pool = NULL;\n\t}\n}\n\n \nstatic void wx_free_all_rx_resources(struct wx *wx)\n{\n\tint i;\n\n\tfor (i = 0; i < wx->num_rx_queues; i++)\n\t\twx_free_rx_resources(wx->rx_ring[i]);\n}\n\n \nstatic void wx_clean_tx_ring(struct wx_ring *tx_ring)\n{\n\tstruct wx_tx_buffer *tx_buffer;\n\tu16 i = tx_ring->next_to_clean;\n\n\ttx_buffer = &tx_ring->tx_buffer_info[i];\n\n\twhile (i != tx_ring->next_to_use) {\n\t\tunion wx_tx_desc *eop_desc, *tx_desc;\n\n\t\t \n\t\tdev_kfree_skb_any(tx_buffer->skb);\n\n\t\t \n\t\tdma_unmap_single(tx_ring->dev,\n\t\t\t\t dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t dma_unmap_len(tx_buffer, len),\n\t\t\t\t DMA_TO_DEVICE);\n\n\t\t \n\t\teop_desc = tx_buffer->next_to_watch;\n\t\ttx_desc = WX_TX_DESC(tx_ring, i);\n\n\t\t \n\t\twhile (tx_desc != eop_desc) {\n\t\t\ttx_buffer++;\n\t\t\ttx_desc++;\n\t\t\ti++;\n\t\t\tif (unlikely(i == tx_ring->count)) {\n\t\t\t\ti = 0;\n\t\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t\t\ttx_desc = WX_TX_DESC(tx_ring, 0);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (dma_unmap_len(tx_buffer, len))\n\t\t\t\tdma_unmap_page(tx_ring->dev,\n\t\t\t\t\t       dma_unmap_addr(tx_buffer, dma),\n\t\t\t\t\t       dma_unmap_len(tx_buffer, len),\n\t\t\t\t\t       DMA_TO_DEVICE);\n\t\t}\n\n\t\t \n\t\ttx_buffer++;\n\t\ti++;\n\t\tif (unlikely(i == tx_ring->count)) {\n\t\t\ti = 0;\n\t\t\ttx_buffer = tx_ring->tx_buffer_info;\n\t\t}\n\t}\n\n\tnetdev_tx_reset_queue(wx_txring_txq(tx_ring));\n\n\t \n\ttx_ring->next_to_use = 0;\n\ttx_ring->next_to_clean = 0;\n}\n\n \nvoid wx_clean_all_tx_rings(struct wx *wx)\n{\n\tint i;\n\n\tfor (i = 0; i < wx->num_tx_queues; i++)\n\t\twx_clean_tx_ring(wx->tx_ring[i]);\n}\nEXPORT_SYMBOL(wx_clean_all_tx_rings);\n\n \nstatic void wx_free_tx_resources(struct wx_ring *tx_ring)\n{\n\twx_clean_tx_ring(tx_ring);\n\tkvfree(tx_ring->tx_buffer_info);\n\ttx_ring->tx_buffer_info = NULL;\n\n\t \n\tif (!tx_ring->desc)\n\t\treturn;\n\n\tdma_free_coherent(tx_ring->dev, tx_ring->size,\n\t\t\t  tx_ring->desc, tx_ring->dma);\n\ttx_ring->desc = NULL;\n}\n\n \nstatic void wx_free_all_tx_resources(struct wx *wx)\n{\n\tint i;\n\n\tfor (i = 0; i < wx->num_tx_queues; i++)\n\t\twx_free_tx_resources(wx->tx_ring[i]);\n}\n\nvoid wx_free_resources(struct wx *wx)\n{\n\twx_free_isb_resources(wx);\n\twx_free_all_rx_resources(wx);\n\twx_free_all_tx_resources(wx);\n}\nEXPORT_SYMBOL(wx_free_resources);\n\nstatic int wx_alloc_page_pool(struct wx_ring *rx_ring)\n{\n\tint ret = 0;\n\n\tstruct page_pool_params pp_params = {\n\t\t.flags = PP_FLAG_DMA_MAP | PP_FLAG_DMA_SYNC_DEV,\n\t\t.order = 0,\n\t\t.pool_size = rx_ring->size,\n\t\t.nid = dev_to_node(rx_ring->dev),\n\t\t.dev = rx_ring->dev,\n\t\t.dma_dir = DMA_FROM_DEVICE,\n\t\t.offset = 0,\n\t\t.max_len = PAGE_SIZE,\n\t};\n\n\trx_ring->page_pool = page_pool_create(&pp_params);\n\tif (IS_ERR(rx_ring->page_pool)) {\n\t\tret = PTR_ERR(rx_ring->page_pool);\n\t\trx_ring->page_pool = NULL;\n\t}\n\n\treturn ret;\n}\n\n \nstatic int wx_setup_rx_resources(struct wx_ring *rx_ring)\n{\n\tstruct device *dev = rx_ring->dev;\n\tint orig_node = dev_to_node(dev);\n\tint numa_node = NUMA_NO_NODE;\n\tint size, ret;\n\n\tsize = sizeof(struct wx_rx_buffer) * rx_ring->count;\n\n\tif (rx_ring->q_vector)\n\t\tnuma_node = rx_ring->q_vector->numa_node;\n\n\trx_ring->rx_buffer_info = kvmalloc_node(size, GFP_KERNEL, numa_node);\n\tif (!rx_ring->rx_buffer_info)\n\t\trx_ring->rx_buffer_info = kvmalloc(size, GFP_KERNEL);\n\tif (!rx_ring->rx_buffer_info)\n\t\tgoto err;\n\n\t \n\trx_ring->size = rx_ring->count * sizeof(union wx_rx_desc);\n\trx_ring->size = ALIGN(rx_ring->size, 4096);\n\n\tset_dev_node(dev, numa_node);\n\trx_ring->desc = dma_alloc_coherent(dev, rx_ring->size,\n\t\t\t\t\t   &rx_ring->dma, GFP_KERNEL);\n\tif (!rx_ring->desc) {\n\t\tset_dev_node(dev, orig_node);\n\t\trx_ring->desc = dma_alloc_coherent(dev, rx_ring->size,\n\t\t\t\t\t\t   &rx_ring->dma, GFP_KERNEL);\n\t}\n\n\tif (!rx_ring->desc)\n\t\tgoto err;\n\n\trx_ring->next_to_clean = 0;\n\trx_ring->next_to_use = 0;\n\n\tret = wx_alloc_page_pool(rx_ring);\n\tif (ret < 0) {\n\t\tdev_err(rx_ring->dev, \"Page pool creation failed: %d\\n\", ret);\n\t\tgoto err_desc;\n\t}\n\n\treturn 0;\n\nerr_desc:\n\tdma_free_coherent(dev, rx_ring->size, rx_ring->desc, rx_ring->dma);\nerr:\n\tkvfree(rx_ring->rx_buffer_info);\n\trx_ring->rx_buffer_info = NULL;\n\tdev_err(dev, \"Unable to allocate memory for the Rx descriptor ring\\n\");\n\treturn -ENOMEM;\n}\n\n \nstatic int wx_setup_all_rx_resources(struct wx *wx)\n{\n\tint i, err = 0;\n\n\tfor (i = 0; i < wx->num_rx_queues; i++) {\n\t\terr = wx_setup_rx_resources(wx->rx_ring[i]);\n\t\tif (!err)\n\t\t\tcontinue;\n\n\t\twx_err(wx, \"Allocation for Rx Queue %u failed\\n\", i);\n\t\tgoto err_setup_rx;\n\t}\n\n\treturn 0;\nerr_setup_rx:\n\t \n\twhile (i--)\n\t\twx_free_rx_resources(wx->rx_ring[i]);\n\treturn err;\n}\n\n \nstatic int wx_setup_tx_resources(struct wx_ring *tx_ring)\n{\n\tstruct device *dev = tx_ring->dev;\n\tint orig_node = dev_to_node(dev);\n\tint numa_node = NUMA_NO_NODE;\n\tint size;\n\n\tsize = sizeof(struct wx_tx_buffer) * tx_ring->count;\n\n\tif (tx_ring->q_vector)\n\t\tnuma_node = tx_ring->q_vector->numa_node;\n\n\ttx_ring->tx_buffer_info = kvmalloc_node(size, GFP_KERNEL, numa_node);\n\tif (!tx_ring->tx_buffer_info)\n\t\ttx_ring->tx_buffer_info = kvmalloc(size, GFP_KERNEL);\n\tif (!tx_ring->tx_buffer_info)\n\t\tgoto err;\n\n\t \n\ttx_ring->size = tx_ring->count * sizeof(union wx_tx_desc);\n\ttx_ring->size = ALIGN(tx_ring->size, 4096);\n\n\tset_dev_node(dev, numa_node);\n\ttx_ring->desc = dma_alloc_coherent(dev, tx_ring->size,\n\t\t\t\t\t   &tx_ring->dma, GFP_KERNEL);\n\tif (!tx_ring->desc) {\n\t\tset_dev_node(dev, orig_node);\n\t\ttx_ring->desc = dma_alloc_coherent(dev, tx_ring->size,\n\t\t\t\t\t\t   &tx_ring->dma, GFP_KERNEL);\n\t}\n\n\tif (!tx_ring->desc)\n\t\tgoto err;\n\n\ttx_ring->next_to_use = 0;\n\ttx_ring->next_to_clean = 0;\n\n\treturn 0;\n\nerr:\n\tkvfree(tx_ring->tx_buffer_info);\n\ttx_ring->tx_buffer_info = NULL;\n\tdev_err(dev, \"Unable to allocate memory for the Tx descriptor ring\\n\");\n\treturn -ENOMEM;\n}\n\n \nstatic int wx_setup_all_tx_resources(struct wx *wx)\n{\n\tint i, err = 0;\n\n\tfor (i = 0; i < wx->num_tx_queues; i++) {\n\t\terr = wx_setup_tx_resources(wx->tx_ring[i]);\n\t\tif (!err)\n\t\t\tcontinue;\n\n\t\twx_err(wx, \"Allocation for Tx Queue %u failed\\n\", i);\n\t\tgoto err_setup_tx;\n\t}\n\n\treturn 0;\nerr_setup_tx:\n\t \n\twhile (i--)\n\t\twx_free_tx_resources(wx->tx_ring[i]);\n\treturn err;\n}\n\nint wx_setup_resources(struct wx *wx)\n{\n\tint err;\n\n\t \n\terr = wx_setup_all_tx_resources(wx);\n\tif (err)\n\t\treturn err;\n\n\t \n\terr = wx_setup_all_rx_resources(wx);\n\tif (err)\n\t\tgoto err_free_tx;\n\n\terr = wx_setup_isb_resources(wx);\n\tif (err)\n\t\tgoto err_free_rx;\n\n\treturn 0;\n\nerr_free_rx:\n\twx_free_all_rx_resources(wx);\nerr_free_tx:\n\twx_free_all_tx_resources(wx);\n\n\treturn err;\n}\nEXPORT_SYMBOL(wx_setup_resources);\n\n \nvoid wx_get_stats64(struct net_device *netdev,\n\t\t    struct rtnl_link_stats64 *stats)\n{\n\tstruct wx *wx = netdev_priv(netdev);\n\tint i;\n\n\trcu_read_lock();\n\tfor (i = 0; i < wx->num_rx_queues; i++) {\n\t\tstruct wx_ring *ring = READ_ONCE(wx->rx_ring[i]);\n\t\tu64 bytes, packets;\n\t\tunsigned int start;\n\n\t\tif (ring) {\n\t\t\tdo {\n\t\t\t\tstart = u64_stats_fetch_begin(&ring->syncp);\n\t\t\t\tpackets = ring->stats.packets;\n\t\t\t\tbytes   = ring->stats.bytes;\n\t\t\t} while (u64_stats_fetch_retry(&ring->syncp, start));\n\t\t\tstats->rx_packets += packets;\n\t\t\tstats->rx_bytes   += bytes;\n\t\t}\n\t}\n\n\tfor (i = 0; i < wx->num_tx_queues; i++) {\n\t\tstruct wx_ring *ring = READ_ONCE(wx->tx_ring[i]);\n\t\tu64 bytes, packets;\n\t\tunsigned int start;\n\n\t\tif (ring) {\n\t\t\tdo {\n\t\t\t\tstart = u64_stats_fetch_begin(&ring->syncp);\n\t\t\t\tpackets = ring->stats.packets;\n\t\t\t\tbytes   = ring->stats.bytes;\n\t\t\t} while (u64_stats_fetch_retry(&ring->syncp,\n\t\t\t\t\t\t\t   start));\n\t\t\tstats->tx_packets += packets;\n\t\t\tstats->tx_bytes   += bytes;\n\t\t}\n\t}\n\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(wx_get_stats64);\n\nint wx_set_features(struct net_device *netdev, netdev_features_t features)\n{\n\tnetdev_features_t changed = netdev->features ^ features;\n\tstruct wx *wx = netdev_priv(netdev);\n\n\tif (changed & NETIF_F_RXHASH)\n\t\twr32m(wx, WX_RDB_RA_CTL, WX_RDB_RA_CTL_RSS_EN,\n\t\t      WX_RDB_RA_CTL_RSS_EN);\n\telse\n\t\twr32m(wx, WX_RDB_RA_CTL, WX_RDB_RA_CTL_RSS_EN, 0);\n\n\tif (changed &\n\t    (NETIF_F_HW_VLAN_CTAG_RX |\n\t     NETIF_F_HW_VLAN_STAG_RX))\n\t\twx_set_rx_mode(netdev);\n\n\treturn 1;\n}\nEXPORT_SYMBOL(wx_set_features);\n\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}