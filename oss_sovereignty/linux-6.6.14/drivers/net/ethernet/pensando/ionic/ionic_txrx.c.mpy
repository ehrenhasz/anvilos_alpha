{
  "module_name": "ionic_txrx.c",
  "hash_id": "0f4ee1492e8d07862c22b99885959485e238fd8baff883e02c5426ccc18400f5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/pensando/ionic/ionic_txrx.c",
  "human_readable_source": "\n \n\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <linux/if_vlan.h>\n#include <net/ip6_checksum.h>\n\n#include \"ionic.h\"\n#include \"ionic_lif.h\"\n#include \"ionic_txrx.h\"\n\nstatic inline void ionic_txq_post(struct ionic_queue *q, bool ring_dbell,\n\t\t\t\t  ionic_desc_cb cb_func, void *cb_arg)\n{\n\tionic_q_post(q, ring_dbell, cb_func, cb_arg);\n}\n\nstatic inline void ionic_rxq_post(struct ionic_queue *q, bool ring_dbell,\n\t\t\t\t  ionic_desc_cb cb_func, void *cb_arg)\n{\n\tionic_q_post(q, ring_dbell, cb_func, cb_arg);\n}\n\nbool ionic_txq_poke_doorbell(struct ionic_queue *q)\n{\n\tunsigned long now, then, dif;\n\tstruct netdev_queue *netdev_txq;\n\tstruct net_device *netdev;\n\n\tnetdev = q->lif->netdev;\n\tnetdev_txq = netdev_get_tx_queue(netdev, q->index);\n\n\tHARD_TX_LOCK(netdev, netdev_txq, smp_processor_id());\n\n\tif (q->tail_idx == q->head_idx) {\n\t\tHARD_TX_UNLOCK(netdev, netdev_txq);\n\t\treturn false;\n\t}\n\n\tnow = READ_ONCE(jiffies);\n\tthen = q->dbell_jiffies;\n\tdif = now - then;\n\n\tif (dif > q->dbell_deadline) {\n\t\tionic_dbell_ring(q->lif->kern_dbpage, q->hw_type,\n\t\t\t\t q->dbval | q->head_idx);\n\n\t\tq->dbell_jiffies = now;\n\t}\n\n\tHARD_TX_UNLOCK(netdev, netdev_txq);\n\n\treturn true;\n}\n\nbool ionic_rxq_poke_doorbell(struct ionic_queue *q)\n{\n\tunsigned long now, then, dif;\n\n\t \n\n\tif (q->tail_idx == q->head_idx)\n\t\treturn false;\n\n\tnow = READ_ONCE(jiffies);\n\tthen = q->dbell_jiffies;\n\tdif = now - then;\n\n\tif (dif > q->dbell_deadline) {\n\t\tionic_dbell_ring(q->lif->kern_dbpage, q->hw_type,\n\t\t\t\t q->dbval | q->head_idx);\n\n\t\tq->dbell_jiffies = now;\n\n\t\tdif = 2 * q->dbell_deadline;\n\t\tif (dif > IONIC_RX_MAX_DOORBELL_DEADLINE)\n\t\t\tdif = IONIC_RX_MAX_DOORBELL_DEADLINE;\n\n\t\tq->dbell_deadline = dif;\n\t}\n\n\treturn true;\n}\n\nstatic inline struct netdev_queue *q_to_ndq(struct ionic_queue *q)\n{\n\treturn netdev_get_tx_queue(q->lif->netdev, q->index);\n}\n\nstatic int ionic_rx_page_alloc(struct ionic_queue *q,\n\t\t\t       struct ionic_buf_info *buf_info)\n{\n\tstruct net_device *netdev = q->lif->netdev;\n\tstruct ionic_rx_stats *stats;\n\tstruct device *dev;\n\tstruct page *page;\n\n\tdev = q->dev;\n\tstats = q_to_rx_stats(q);\n\n\tif (unlikely(!buf_info)) {\n\t\tnet_err_ratelimited(\"%s: %s invalid buf_info in alloc\\n\",\n\t\t\t\t    netdev->name, q->name);\n\t\treturn -EINVAL;\n\t}\n\n\tpage = alloc_pages(IONIC_PAGE_GFP_MASK, 0);\n\tif (unlikely(!page)) {\n\t\tnet_err_ratelimited(\"%s: %s page alloc failed\\n\",\n\t\t\t\t    netdev->name, q->name);\n\t\tstats->alloc_err++;\n\t\treturn -ENOMEM;\n\t}\n\n\tbuf_info->dma_addr = dma_map_page(dev, page, 0,\n\t\t\t\t\t  IONIC_PAGE_SIZE, DMA_FROM_DEVICE);\n\tif (unlikely(dma_mapping_error(dev, buf_info->dma_addr))) {\n\t\t__free_pages(page, 0);\n\t\tnet_err_ratelimited(\"%s: %s dma map failed\\n\",\n\t\t\t\t    netdev->name, q->name);\n\t\tstats->dma_map_err++;\n\t\treturn -EIO;\n\t}\n\n\tbuf_info->page = page;\n\tbuf_info->page_offset = 0;\n\n\treturn 0;\n}\n\nstatic void ionic_rx_page_free(struct ionic_queue *q,\n\t\t\t       struct ionic_buf_info *buf_info)\n{\n\tstruct net_device *netdev = q->lif->netdev;\n\tstruct device *dev = q->dev;\n\n\tif (unlikely(!buf_info)) {\n\t\tnet_err_ratelimited(\"%s: %s invalid buf_info in free\\n\",\n\t\t\t\t    netdev->name, q->name);\n\t\treturn;\n\t}\n\n\tif (!buf_info->page)\n\t\treturn;\n\n\tdma_unmap_page(dev, buf_info->dma_addr, IONIC_PAGE_SIZE, DMA_FROM_DEVICE);\n\t__free_pages(buf_info->page, 0);\n\tbuf_info->page = NULL;\n}\n\nstatic bool ionic_rx_buf_recycle(struct ionic_queue *q,\n\t\t\t\t struct ionic_buf_info *buf_info, u32 used)\n{\n\tu32 size;\n\n\t \n\tif (page_is_pfmemalloc(buf_info->page))\n\t\treturn false;\n\n\t \n\tif (page_to_nid(buf_info->page) != numa_mem_id())\n\t\treturn false;\n\n\tsize = ALIGN(used, IONIC_PAGE_SPLIT_SZ);\n\tbuf_info->page_offset += size;\n\tif (buf_info->page_offset >= IONIC_PAGE_SIZE)\n\t\treturn false;\n\n\tget_page(buf_info->page);\n\n\treturn true;\n}\n\nstatic struct sk_buff *ionic_rx_frags(struct ionic_queue *q,\n\t\t\t\t      struct ionic_desc_info *desc_info,\n\t\t\t\t      struct ionic_rxq_comp *comp)\n{\n\tstruct net_device *netdev = q->lif->netdev;\n\tstruct ionic_buf_info *buf_info;\n\tstruct ionic_rx_stats *stats;\n\tstruct device *dev = q->dev;\n\tstruct sk_buff *skb;\n\tunsigned int i;\n\tu16 frag_len;\n\tu16 len;\n\n\tstats = q_to_rx_stats(q);\n\n\tbuf_info = &desc_info->bufs[0];\n\tlen = le16_to_cpu(comp->len);\n\n\tprefetchw(buf_info->page);\n\n\tskb = napi_get_frags(&q_to_qcq(q)->napi);\n\tif (unlikely(!skb)) {\n\t\tnet_warn_ratelimited(\"%s: SKB alloc failed on %s!\\n\",\n\t\t\t\t     netdev->name, q->name);\n\t\tstats->alloc_err++;\n\t\treturn NULL;\n\t}\n\n\ti = comp->num_sg_elems + 1;\n\tdo {\n\t\tif (unlikely(!buf_info->page)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\treturn NULL;\n\t\t}\n\n\t\tfrag_len = min_t(u16, len, min_t(u32, IONIC_MAX_BUF_LEN,\n\t\t\t\t\t\t IONIC_PAGE_SIZE - buf_info->page_offset));\n\t\tlen -= frag_len;\n\n\t\tdma_sync_single_for_cpu(dev,\n\t\t\t\t\tbuf_info->dma_addr + buf_info->page_offset,\n\t\t\t\t\tfrag_len, DMA_FROM_DEVICE);\n\n\t\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,\n\t\t\t\tbuf_info->page, buf_info->page_offset, frag_len,\n\t\t\t\tIONIC_PAGE_SIZE);\n\n\t\tif (!ionic_rx_buf_recycle(q, buf_info, frag_len)) {\n\t\t\tdma_unmap_page(dev, buf_info->dma_addr,\n\t\t\t\t       IONIC_PAGE_SIZE, DMA_FROM_DEVICE);\n\t\t\tbuf_info->page = NULL;\n\t\t}\n\n\t\tbuf_info++;\n\n\t\ti--;\n\t} while (i > 0);\n\n\treturn skb;\n}\n\nstatic struct sk_buff *ionic_rx_copybreak(struct ionic_queue *q,\n\t\t\t\t\t  struct ionic_desc_info *desc_info,\n\t\t\t\t\t  struct ionic_rxq_comp *comp)\n{\n\tstruct net_device *netdev = q->lif->netdev;\n\tstruct ionic_buf_info *buf_info;\n\tstruct ionic_rx_stats *stats;\n\tstruct device *dev = q->dev;\n\tstruct sk_buff *skb;\n\tu16 len;\n\n\tstats = q_to_rx_stats(q);\n\n\tbuf_info = &desc_info->bufs[0];\n\tlen = le16_to_cpu(comp->len);\n\n\tskb = napi_alloc_skb(&q_to_qcq(q)->napi, len);\n\tif (unlikely(!skb)) {\n\t\tnet_warn_ratelimited(\"%s: SKB alloc failed on %s!\\n\",\n\t\t\t\t     netdev->name, q->name);\n\t\tstats->alloc_err++;\n\t\treturn NULL;\n\t}\n\n\tif (unlikely(!buf_info->page)) {\n\t\tdev_kfree_skb(skb);\n\t\treturn NULL;\n\t}\n\n\tdma_sync_single_for_cpu(dev, buf_info->dma_addr + buf_info->page_offset,\n\t\t\t\tlen, DMA_FROM_DEVICE);\n\tskb_copy_to_linear_data(skb, page_address(buf_info->page) + buf_info->page_offset, len);\n\tdma_sync_single_for_device(dev, buf_info->dma_addr + buf_info->page_offset,\n\t\t\t\t   len, DMA_FROM_DEVICE);\n\n\tskb_put(skb, len);\n\tskb->protocol = eth_type_trans(skb, q->lif->netdev);\n\n\treturn skb;\n}\n\nstatic void ionic_rx_clean(struct ionic_queue *q,\n\t\t\t   struct ionic_desc_info *desc_info,\n\t\t\t   struct ionic_cq_info *cq_info,\n\t\t\t   void *cb_arg)\n{\n\tstruct net_device *netdev = q->lif->netdev;\n\tstruct ionic_qcq *qcq = q_to_qcq(q);\n\tstruct ionic_rx_stats *stats;\n\tstruct ionic_rxq_comp *comp;\n\tstruct sk_buff *skb;\n\n\tcomp = cq_info->cq_desc + qcq->cq.desc_size - sizeof(*comp);\n\n\tstats = q_to_rx_stats(q);\n\n\tif (comp->status) {\n\t\tstats->dropped++;\n\t\treturn;\n\t}\n\n\tstats->pkts++;\n\tstats->bytes += le16_to_cpu(comp->len);\n\n\tif (le16_to_cpu(comp->len) <= q->lif->rx_copybreak)\n\t\tskb = ionic_rx_copybreak(q, desc_info, comp);\n\telse\n\t\tskb = ionic_rx_frags(q, desc_info, comp);\n\n\tif (unlikely(!skb)) {\n\t\tstats->dropped++;\n\t\treturn;\n\t}\n\n\tskb_record_rx_queue(skb, q->index);\n\n\tif (likely(netdev->features & NETIF_F_RXHASH)) {\n\t\tswitch (comp->pkt_type_color & IONIC_RXQ_COMP_PKT_TYPE_MASK) {\n\t\tcase IONIC_PKT_TYPE_IPV4:\n\t\tcase IONIC_PKT_TYPE_IPV6:\n\t\t\tskb_set_hash(skb, le32_to_cpu(comp->rss_hash),\n\t\t\t\t     PKT_HASH_TYPE_L3);\n\t\t\tbreak;\n\t\tcase IONIC_PKT_TYPE_IPV4_TCP:\n\t\tcase IONIC_PKT_TYPE_IPV6_TCP:\n\t\tcase IONIC_PKT_TYPE_IPV4_UDP:\n\t\tcase IONIC_PKT_TYPE_IPV6_UDP:\n\t\t\tskb_set_hash(skb, le32_to_cpu(comp->rss_hash),\n\t\t\t\t     PKT_HASH_TYPE_L4);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (likely(netdev->features & NETIF_F_RXCSUM) &&\n\t    (comp->csum_flags & IONIC_RXQ_COMP_CSUM_F_CALC)) {\n\t\tskb->ip_summed = CHECKSUM_COMPLETE;\n\t\tskb->csum = (__force __wsum)le16_to_cpu(comp->csum);\n\t\tstats->csum_complete++;\n\t} else {\n\t\tstats->csum_none++;\n\t}\n\n\tif (unlikely((comp->csum_flags & IONIC_RXQ_COMP_CSUM_F_TCP_BAD) ||\n\t\t     (comp->csum_flags & IONIC_RXQ_COMP_CSUM_F_UDP_BAD) ||\n\t\t     (comp->csum_flags & IONIC_RXQ_COMP_CSUM_F_IP_BAD)))\n\t\tstats->csum_error++;\n\n\tif (likely(netdev->features & NETIF_F_HW_VLAN_CTAG_RX) &&\n\t    (comp->csum_flags & IONIC_RXQ_COMP_CSUM_F_VLAN)) {\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q),\n\t\t\t\t       le16_to_cpu(comp->vlan_tci));\n\t\tstats->vlan_stripped++;\n\t}\n\n\tif (unlikely(q->features & IONIC_RXQ_F_HWSTAMP)) {\n\t\t__le64 *cq_desc_hwstamp;\n\t\tu64 hwstamp;\n\n\t\tcq_desc_hwstamp =\n\t\t\tcq_info->cq_desc +\n\t\t\tqcq->cq.desc_size -\n\t\t\tsizeof(struct ionic_rxq_comp) -\n\t\t\tIONIC_HWSTAMP_CQ_NEGOFFSET;\n\n\t\thwstamp = le64_to_cpu(*cq_desc_hwstamp);\n\n\t\tif (hwstamp != IONIC_HWSTAMP_INVALID) {\n\t\t\tskb_hwtstamps(skb)->hwtstamp = ionic_lif_phc_ktime(q->lif, hwstamp);\n\t\t\tstats->hwstamp_valid++;\n\t\t} else {\n\t\t\tstats->hwstamp_invalid++;\n\t\t}\n\t}\n\n\tif (le16_to_cpu(comp->len) <= q->lif->rx_copybreak)\n\t\tnapi_gro_receive(&qcq->napi, skb);\n\telse\n\t\tnapi_gro_frags(&qcq->napi);\n}\n\nbool ionic_rx_service(struct ionic_cq *cq, struct ionic_cq_info *cq_info)\n{\n\tstruct ionic_queue *q = cq->bound_q;\n\tstruct ionic_desc_info *desc_info;\n\tstruct ionic_rxq_comp *comp;\n\n\tcomp = cq_info->cq_desc + cq->desc_size - sizeof(*comp);\n\n\tif (!color_match(comp->pkt_type_color, cq->done_color))\n\t\treturn false;\n\n\t \n\tif (q->tail_idx == q->head_idx)\n\t\treturn false;\n\n\tif (q->tail_idx != le16_to_cpu(comp->comp_index))\n\t\treturn false;\n\n\tdesc_info = &q->info[q->tail_idx];\n\tq->tail_idx = (q->tail_idx + 1) & (q->num_descs - 1);\n\n\t \n\tionic_rx_clean(q, desc_info, cq_info, desc_info->cb_arg);\n\n\tdesc_info->cb = NULL;\n\tdesc_info->cb_arg = NULL;\n\n\treturn true;\n}\n\nstatic inline void ionic_write_cmb_desc(struct ionic_queue *q,\n\t\t\t\t\tvoid __iomem *cmb_desc,\n\t\t\t\t\tvoid *desc)\n{\n\tif (q_to_qcq(q)->flags & IONIC_QCQ_F_CMB_RINGS)\n\t\tmemcpy_toio(cmb_desc, desc, q->desc_size);\n}\n\nvoid ionic_rx_fill(struct ionic_queue *q)\n{\n\tstruct net_device *netdev = q->lif->netdev;\n\tstruct ionic_desc_info *desc_info;\n\tstruct ionic_rxq_sg_desc *sg_desc;\n\tstruct ionic_rxq_sg_elem *sg_elem;\n\tstruct ionic_buf_info *buf_info;\n\tunsigned int fill_threshold;\n\tstruct ionic_rxq_desc *desc;\n\tunsigned int remain_len;\n\tunsigned int frag_len;\n\tunsigned int nfrags;\n\tunsigned int n_fill;\n\tunsigned int i, j;\n\tunsigned int len;\n\n\tn_fill = ionic_q_space_avail(q);\n\n\tfill_threshold = min_t(unsigned int, IONIC_RX_FILL_THRESHOLD,\n\t\t\t       q->num_descs / IONIC_RX_FILL_DIV);\n\tif (n_fill < fill_threshold)\n\t\treturn;\n\n\tlen = netdev->mtu + ETH_HLEN + VLAN_HLEN;\n\n\tfor (i = n_fill; i; i--) {\n\t\tnfrags = 0;\n\t\tremain_len = len;\n\t\tdesc_info = &q->info[q->head_idx];\n\t\tdesc = desc_info->desc;\n\t\tbuf_info = &desc_info->bufs[0];\n\n\t\tif (!buf_info->page) {  \n\t\t\tif (unlikely(ionic_rx_page_alloc(q, buf_info))) {\n\t\t\t\tdesc->addr = 0;\n\t\t\t\tdesc->len = 0;\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tdesc->addr = cpu_to_le64(buf_info->dma_addr + buf_info->page_offset);\n\t\tfrag_len = min_t(u16, len, min_t(u32, IONIC_MAX_BUF_LEN,\n\t\t\t\t\t\t IONIC_PAGE_SIZE - buf_info->page_offset));\n\t\tdesc->len = cpu_to_le16(frag_len);\n\t\tremain_len -= frag_len;\n\t\tbuf_info++;\n\t\tnfrags++;\n\n\t\t \n\t\tsg_desc = desc_info->sg_desc;\n\t\tfor (j = 0; remain_len > 0 && j < q->max_sg_elems; j++) {\n\t\t\tsg_elem = &sg_desc->elems[j];\n\t\t\tif (!buf_info->page) {  \n\t\t\t\tif (unlikely(ionic_rx_page_alloc(q, buf_info))) {\n\t\t\t\t\tsg_elem->addr = 0;\n\t\t\t\t\tsg_elem->len = 0;\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tsg_elem->addr = cpu_to_le64(buf_info->dma_addr + buf_info->page_offset);\n\t\t\tfrag_len = min_t(u16, remain_len, min_t(u32, IONIC_MAX_BUF_LEN,\n\t\t\t\t\t\t\t\tIONIC_PAGE_SIZE -\n\t\t\t\t\t\t\t\tbuf_info->page_offset));\n\t\t\tsg_elem->len = cpu_to_le16(frag_len);\n\t\t\tremain_len -= frag_len;\n\t\t\tbuf_info++;\n\t\t\tnfrags++;\n\t\t}\n\n\t\t \n\t\tif (j < q->max_sg_elems) {\n\t\t\tsg_elem = &sg_desc->elems[j];\n\t\t\tmemset(sg_elem, 0, sizeof(*sg_elem));\n\t\t}\n\n\t\tdesc->opcode = (nfrags > 1) ? IONIC_RXQ_DESC_OPCODE_SG :\n\t\t\t\t\t      IONIC_RXQ_DESC_OPCODE_SIMPLE;\n\t\tdesc_info->nbufs = nfrags;\n\n\t\tionic_write_cmb_desc(q, desc_info->cmb_desc, desc);\n\n\t\tionic_rxq_post(q, false, ionic_rx_clean, NULL);\n\t}\n\n\tionic_dbell_ring(q->lif->kern_dbpage, q->hw_type,\n\t\t\t q->dbval | q->head_idx);\n\n\tq->dbell_deadline = IONIC_RX_MIN_DOORBELL_DEADLINE;\n\tq->dbell_jiffies = jiffies;\n\n\tmod_timer(&q_to_qcq(q)->napi_qcq->napi_deadline,\n\t\t  jiffies + IONIC_NAPI_DEADLINE);\n}\n\nvoid ionic_rx_empty(struct ionic_queue *q)\n{\n\tstruct ionic_desc_info *desc_info;\n\tstruct ionic_buf_info *buf_info;\n\tunsigned int i, j;\n\n\tfor (i = 0; i < q->num_descs; i++) {\n\t\tdesc_info = &q->info[i];\n\t\tfor (j = 0; j < IONIC_RX_MAX_SG_ELEMS + 1; j++) {\n\t\t\tbuf_info = &desc_info->bufs[j];\n\t\t\tif (buf_info->page)\n\t\t\t\tionic_rx_page_free(q, buf_info);\n\t\t}\n\n\t\tdesc_info->nbufs = 0;\n\t\tdesc_info->cb = NULL;\n\t\tdesc_info->cb_arg = NULL;\n\t}\n\n\tq->head_idx = 0;\n\tq->tail_idx = 0;\n}\n\nstatic void ionic_dim_update(struct ionic_qcq *qcq, int napi_mode)\n{\n\tstruct dim_sample dim_sample;\n\tstruct ionic_lif *lif;\n\tunsigned int qi;\n\tu64 pkts, bytes;\n\n\tif (!qcq->intr.dim_coal_hw)\n\t\treturn;\n\n\tlif = qcq->q.lif;\n\tqi = qcq->cq.bound_q->index;\n\n\tswitch (napi_mode) {\n\tcase IONIC_LIF_F_TX_DIM_INTR:\n\t\tpkts = lif->txqstats[qi].pkts;\n\t\tbytes = lif->txqstats[qi].bytes;\n\t\tbreak;\n\tcase IONIC_LIF_F_RX_DIM_INTR:\n\t\tpkts = lif->rxqstats[qi].pkts;\n\t\tbytes = lif->rxqstats[qi].bytes;\n\t\tbreak;\n\tdefault:\n\t\tpkts = lif->txqstats[qi].pkts + lif->rxqstats[qi].pkts;\n\t\tbytes = lif->txqstats[qi].bytes + lif->rxqstats[qi].bytes;\n\t\tbreak;\n\t}\n\n\tdim_update_sample(qcq->cq.bound_intr->rearm_count,\n\t\t\t  pkts, bytes, &dim_sample);\n\n\tnet_dim(&qcq->dim, dim_sample);\n}\n\nint ionic_tx_napi(struct napi_struct *napi, int budget)\n{\n\tstruct ionic_qcq *qcq = napi_to_qcq(napi);\n\tstruct ionic_cq *cq = napi_to_cq(napi);\n\tstruct ionic_dev *idev;\n\tstruct ionic_lif *lif;\n\tu32 work_done = 0;\n\tu32 flags = 0;\n\n\tlif = cq->bound_q->lif;\n\tidev = &lif->ionic->idev;\n\n\twork_done = ionic_cq_service(cq, budget,\n\t\t\t\t     ionic_tx_service, NULL, NULL);\n\n\tif (work_done < budget && napi_complete_done(napi, work_done)) {\n\t\tionic_dim_update(qcq, IONIC_LIF_F_TX_DIM_INTR);\n\t\tflags |= IONIC_INTR_CRED_UNMASK;\n\t\tcq->bound_intr->rearm_count++;\n\t}\n\n\tif (work_done || flags) {\n\t\tflags |= IONIC_INTR_CRED_RESET_COALESCE;\n\t\tionic_intr_credits(idev->intr_ctrl,\n\t\t\t\t   cq->bound_intr->index,\n\t\t\t\t   work_done, flags);\n\t}\n\n\tif (!work_done && ionic_txq_poke_doorbell(&qcq->q))\n\t\tmod_timer(&qcq->napi_deadline, jiffies + IONIC_NAPI_DEADLINE);\n\n\treturn work_done;\n}\n\nint ionic_rx_napi(struct napi_struct *napi, int budget)\n{\n\tstruct ionic_qcq *qcq = napi_to_qcq(napi);\n\tstruct ionic_cq *cq = napi_to_cq(napi);\n\tstruct ionic_dev *idev;\n\tstruct ionic_lif *lif;\n\tu32 work_done = 0;\n\tu32 flags = 0;\n\n\tlif = cq->bound_q->lif;\n\tidev = &lif->ionic->idev;\n\n\twork_done = ionic_cq_service(cq, budget,\n\t\t\t\t     ionic_rx_service, NULL, NULL);\n\n\tionic_rx_fill(cq->bound_q);\n\n\tif (work_done < budget && napi_complete_done(napi, work_done)) {\n\t\tionic_dim_update(qcq, IONIC_LIF_F_RX_DIM_INTR);\n\t\tflags |= IONIC_INTR_CRED_UNMASK;\n\t\tcq->bound_intr->rearm_count++;\n\t}\n\n\tif (work_done || flags) {\n\t\tflags |= IONIC_INTR_CRED_RESET_COALESCE;\n\t\tionic_intr_credits(idev->intr_ctrl,\n\t\t\t\t   cq->bound_intr->index,\n\t\t\t\t   work_done, flags);\n\t}\n\n\tif (!work_done && ionic_rxq_poke_doorbell(&qcq->q))\n\t\tmod_timer(&qcq->napi_deadline, jiffies + IONIC_NAPI_DEADLINE);\n\n\treturn work_done;\n}\n\nint ionic_txrx_napi(struct napi_struct *napi, int budget)\n{\n\tstruct ionic_qcq *rxqcq = napi_to_qcq(napi);\n\tstruct ionic_cq *rxcq = napi_to_cq(napi);\n\tunsigned int qi = rxcq->bound_q->index;\n\tstruct ionic_qcq *txqcq;\n\tstruct ionic_dev *idev;\n\tstruct ionic_lif *lif;\n\tstruct ionic_cq *txcq;\n\tbool resched = false;\n\tu32 rx_work_done = 0;\n\tu32 tx_work_done = 0;\n\tu32 flags = 0;\n\n\tlif = rxcq->bound_q->lif;\n\tidev = &lif->ionic->idev;\n\ttxqcq = lif->txqcqs[qi];\n\ttxcq = &lif->txqcqs[qi]->cq;\n\n\ttx_work_done = ionic_cq_service(txcq, IONIC_TX_BUDGET_DEFAULT,\n\t\t\t\t\tionic_tx_service, NULL, NULL);\n\n\trx_work_done = ionic_cq_service(rxcq, budget,\n\t\t\t\t\tionic_rx_service, NULL, NULL);\n\n\tionic_rx_fill(rxcq->bound_q);\n\n\tif (rx_work_done < budget && napi_complete_done(napi, rx_work_done)) {\n\t\tionic_dim_update(rxqcq, 0);\n\t\tflags |= IONIC_INTR_CRED_UNMASK;\n\t\trxcq->bound_intr->rearm_count++;\n\t}\n\n\tif (rx_work_done || flags) {\n\t\tflags |= IONIC_INTR_CRED_RESET_COALESCE;\n\t\tionic_intr_credits(idev->intr_ctrl, rxcq->bound_intr->index,\n\t\t\t\t   tx_work_done + rx_work_done, flags);\n\t}\n\n\tif (!rx_work_done && ionic_rxq_poke_doorbell(&rxqcq->q))\n\t\tresched = true;\n\tif (!tx_work_done && ionic_txq_poke_doorbell(&txqcq->q))\n\t\tresched = true;\n\tif (resched)\n\t\tmod_timer(&rxqcq->napi_deadline, jiffies + IONIC_NAPI_DEADLINE);\n\n\treturn rx_work_done;\n}\n\nstatic dma_addr_t ionic_tx_map_single(struct ionic_queue *q,\n\t\t\t\t      void *data, size_t len)\n{\n\tstruct ionic_tx_stats *stats = q_to_tx_stats(q);\n\tstruct device *dev = q->dev;\n\tdma_addr_t dma_addr;\n\n\tdma_addr = dma_map_single(dev, data, len, DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, dma_addr)) {\n\t\tnet_warn_ratelimited(\"%s: DMA single map failed on %s!\\n\",\n\t\t\t\t     q->lif->netdev->name, q->name);\n\t\tstats->dma_map_err++;\n\t\treturn 0;\n\t}\n\treturn dma_addr;\n}\n\nstatic dma_addr_t ionic_tx_map_frag(struct ionic_queue *q,\n\t\t\t\t    const skb_frag_t *frag,\n\t\t\t\t    size_t offset, size_t len)\n{\n\tstruct ionic_tx_stats *stats = q_to_tx_stats(q);\n\tstruct device *dev = q->dev;\n\tdma_addr_t dma_addr;\n\n\tdma_addr = skb_frag_dma_map(dev, frag, offset, len, DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, dma_addr)) {\n\t\tnet_warn_ratelimited(\"%s: DMA frag map failed on %s!\\n\",\n\t\t\t\t     q->lif->netdev->name, q->name);\n\t\tstats->dma_map_err++;\n\t}\n\treturn dma_addr;\n}\n\nstatic int ionic_tx_map_skb(struct ionic_queue *q, struct sk_buff *skb,\n\t\t\t    struct ionic_desc_info *desc_info)\n{\n\tstruct ionic_buf_info *buf_info = desc_info->bufs;\n\tstruct ionic_tx_stats *stats = q_to_tx_stats(q);\n\tstruct device *dev = q->dev;\n\tdma_addr_t dma_addr;\n\tunsigned int nfrags;\n\tskb_frag_t *frag;\n\tint frag_idx;\n\n\tdma_addr = ionic_tx_map_single(q, skb->data, skb_headlen(skb));\n\tif (dma_mapping_error(dev, dma_addr)) {\n\t\tstats->dma_map_err++;\n\t\treturn -EIO;\n\t}\n\tbuf_info->dma_addr = dma_addr;\n\tbuf_info->len = skb_headlen(skb);\n\tbuf_info++;\n\n\tfrag = skb_shinfo(skb)->frags;\n\tnfrags = skb_shinfo(skb)->nr_frags;\n\tfor (frag_idx = 0; frag_idx < nfrags; frag_idx++, frag++) {\n\t\tdma_addr = ionic_tx_map_frag(q, frag, 0, skb_frag_size(frag));\n\t\tif (dma_mapping_error(dev, dma_addr)) {\n\t\t\tstats->dma_map_err++;\n\t\t\tgoto dma_fail;\n\t\t}\n\t\tbuf_info->dma_addr = dma_addr;\n\t\tbuf_info->len = skb_frag_size(frag);\n\t\tbuf_info++;\n\t}\n\n\tdesc_info->nbufs = 1 + nfrags;\n\n\treturn 0;\n\ndma_fail:\n\t \n\twhile (frag_idx > 0) {\n\t\tfrag_idx--;\n\t\tbuf_info--;\n\t\tdma_unmap_page(dev, buf_info->dma_addr,\n\t\t\t       buf_info->len, DMA_TO_DEVICE);\n\t}\n\tdma_unmap_single(dev, buf_info->dma_addr, buf_info->len, DMA_TO_DEVICE);\n\treturn -EIO;\n}\n\nstatic void ionic_tx_desc_unmap_bufs(struct ionic_queue *q,\n\t\t\t\t     struct ionic_desc_info *desc_info)\n{\n\tstruct ionic_buf_info *buf_info = desc_info->bufs;\n\tstruct device *dev = q->dev;\n\tunsigned int i;\n\n\tif (!desc_info->nbufs)\n\t\treturn;\n\n\tdma_unmap_single(dev, (dma_addr_t)buf_info->dma_addr,\n\t\t\t buf_info->len, DMA_TO_DEVICE);\n\tbuf_info++;\n\tfor (i = 1; i < desc_info->nbufs; i++, buf_info++)\n\t\tdma_unmap_page(dev, (dma_addr_t)buf_info->dma_addr,\n\t\t\t       buf_info->len, DMA_TO_DEVICE);\n\n\tdesc_info->nbufs = 0;\n}\n\nstatic void ionic_tx_clean(struct ionic_queue *q,\n\t\t\t   struct ionic_desc_info *desc_info,\n\t\t\t   struct ionic_cq_info *cq_info,\n\t\t\t   void *cb_arg)\n{\n\tstruct ionic_tx_stats *stats = q_to_tx_stats(q);\n\tstruct ionic_qcq *qcq = q_to_qcq(q);\n\tstruct sk_buff *skb = cb_arg;\n\tu16 qi;\n\n\tionic_tx_desc_unmap_bufs(q, desc_info);\n\n\tif (!skb)\n\t\treturn;\n\n\tqi = skb_get_queue_mapping(skb);\n\n\tif (unlikely(q->features & IONIC_TXQ_F_HWSTAMP)) {\n\t\tif (cq_info) {\n\t\t\tstruct skb_shared_hwtstamps hwts = {};\n\t\t\t__le64 *cq_desc_hwstamp;\n\t\t\tu64 hwstamp;\n\n\t\t\tcq_desc_hwstamp =\n\t\t\t\tcq_info->cq_desc +\n\t\t\t\tqcq->cq.desc_size -\n\t\t\t\tsizeof(struct ionic_txq_comp) -\n\t\t\t\tIONIC_HWSTAMP_CQ_NEGOFFSET;\n\n\t\t\thwstamp = le64_to_cpu(*cq_desc_hwstamp);\n\n\t\t\tif (hwstamp != IONIC_HWSTAMP_INVALID) {\n\t\t\t\thwts.hwtstamp = ionic_lif_phc_ktime(q->lif, hwstamp);\n\n\t\t\t\tskb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;\n\t\t\t\tskb_tstamp_tx(skb, &hwts);\n\n\t\t\t\tstats->hwstamp_valid++;\n\t\t\t} else {\n\t\t\t\tstats->hwstamp_invalid++;\n\t\t\t}\n\t\t}\n\n\t} else if (unlikely(__netif_subqueue_stopped(q->lif->netdev, qi))) {\n\t\tnetif_wake_subqueue(q->lif->netdev, qi);\n\t}\n\n\tdesc_info->bytes = skb->len;\n\tstats->clean++;\n\n\tdev_consume_skb_any(skb);\n}\n\nbool ionic_tx_service(struct ionic_cq *cq, struct ionic_cq_info *cq_info)\n{\n\tstruct ionic_queue *q = cq->bound_q;\n\tstruct ionic_desc_info *desc_info;\n\tstruct ionic_txq_comp *comp;\n\tint bytes = 0;\n\tint pkts = 0;\n\tu16 index;\n\n\tcomp = cq_info->cq_desc + cq->desc_size - sizeof(*comp);\n\n\tif (!color_match(comp->color, cq->done_color))\n\t\treturn false;\n\n\t \n\tdo {\n\t\tdesc_info = &q->info[q->tail_idx];\n\t\tdesc_info->bytes = 0;\n\t\tindex = q->tail_idx;\n\t\tq->tail_idx = (q->tail_idx + 1) & (q->num_descs - 1);\n\t\tionic_tx_clean(q, desc_info, cq_info, desc_info->cb_arg);\n\t\tif (desc_info->cb_arg) {\n\t\t\tpkts++;\n\t\t\tbytes += desc_info->bytes;\n\t\t}\n\t\tdesc_info->cb = NULL;\n\t\tdesc_info->cb_arg = NULL;\n\t} while (index != le16_to_cpu(comp->comp_index));\n\n\tif (pkts && bytes && !unlikely(q->features & IONIC_TXQ_F_HWSTAMP))\n\t\tnetdev_tx_completed_queue(q_to_ndq(q), pkts, bytes);\n\n\treturn true;\n}\n\nvoid ionic_tx_flush(struct ionic_cq *cq)\n{\n\tstruct ionic_dev *idev = &cq->lif->ionic->idev;\n\tu32 work_done;\n\n\twork_done = ionic_cq_service(cq, cq->num_descs,\n\t\t\t\t     ionic_tx_service, NULL, NULL);\n\tif (work_done)\n\t\tionic_intr_credits(idev->intr_ctrl, cq->bound_intr->index,\n\t\t\t\t   work_done, IONIC_INTR_CRED_RESET_COALESCE);\n}\n\nvoid ionic_tx_empty(struct ionic_queue *q)\n{\n\tstruct ionic_desc_info *desc_info;\n\tint bytes = 0;\n\tint pkts = 0;\n\n\t \n\twhile (q->head_idx != q->tail_idx) {\n\t\tdesc_info = &q->info[q->tail_idx];\n\t\tdesc_info->bytes = 0;\n\t\tq->tail_idx = (q->tail_idx + 1) & (q->num_descs - 1);\n\t\tionic_tx_clean(q, desc_info, NULL, desc_info->cb_arg);\n\t\tif (desc_info->cb_arg) {\n\t\t\tpkts++;\n\t\t\tbytes += desc_info->bytes;\n\t\t}\n\t\tdesc_info->cb = NULL;\n\t\tdesc_info->cb_arg = NULL;\n\t}\n\n\tif (pkts && bytes && !unlikely(q->features & IONIC_TXQ_F_HWSTAMP))\n\t\tnetdev_tx_completed_queue(q_to_ndq(q), pkts, bytes);\n}\n\nstatic int ionic_tx_tcp_inner_pseudo_csum(struct sk_buff *skb)\n{\n\tint err;\n\n\terr = skb_cow_head(skb, 0);\n\tif (err)\n\t\treturn err;\n\n\tif (skb->protocol == cpu_to_be16(ETH_P_IP)) {\n\t\tinner_ip_hdr(skb)->check = 0;\n\t\tinner_tcp_hdr(skb)->check =\n\t\t\t~csum_tcpudp_magic(inner_ip_hdr(skb)->saddr,\n\t\t\t\t\t   inner_ip_hdr(skb)->daddr,\n\t\t\t\t\t   0, IPPROTO_TCP, 0);\n\t} else if (skb->protocol == cpu_to_be16(ETH_P_IPV6)) {\n\t\tinner_tcp_hdr(skb)->check =\n\t\t\t~csum_ipv6_magic(&inner_ipv6_hdr(skb)->saddr,\n\t\t\t\t\t &inner_ipv6_hdr(skb)->daddr,\n\t\t\t\t\t 0, IPPROTO_TCP, 0);\n\t}\n\n\treturn 0;\n}\n\nstatic int ionic_tx_tcp_pseudo_csum(struct sk_buff *skb)\n{\n\tint err;\n\n\terr = skb_cow_head(skb, 0);\n\tif (err)\n\t\treturn err;\n\n\tif (skb->protocol == cpu_to_be16(ETH_P_IP)) {\n\t\tip_hdr(skb)->check = 0;\n\t\ttcp_hdr(skb)->check =\n\t\t\t~csum_tcpudp_magic(ip_hdr(skb)->saddr,\n\t\t\t\t\t   ip_hdr(skb)->daddr,\n\t\t\t\t\t   0, IPPROTO_TCP, 0);\n\t} else if (skb->protocol == cpu_to_be16(ETH_P_IPV6)) {\n\t\ttcp_v6_gso_csum_prep(skb);\n\t}\n\n\treturn 0;\n}\n\nstatic void ionic_tx_tso_post(struct ionic_queue *q,\n\t\t\t      struct ionic_desc_info *desc_info,\n\t\t\t      struct sk_buff *skb,\n\t\t\t      dma_addr_t addr, u8 nsge, u16 len,\n\t\t\t      unsigned int hdrlen, unsigned int mss,\n\t\t\t      bool outer_csum,\n\t\t\t      u16 vlan_tci, bool has_vlan,\n\t\t\t      bool start, bool done)\n{\n\tstruct ionic_txq_desc *desc = desc_info->desc;\n\tu8 flags = 0;\n\tu64 cmd;\n\n\tflags |= has_vlan ? IONIC_TXQ_DESC_FLAG_VLAN : 0;\n\tflags |= outer_csum ? IONIC_TXQ_DESC_FLAG_ENCAP : 0;\n\tflags |= start ? IONIC_TXQ_DESC_FLAG_TSO_SOT : 0;\n\tflags |= done ? IONIC_TXQ_DESC_FLAG_TSO_EOT : 0;\n\n\tcmd = encode_txq_desc_cmd(IONIC_TXQ_DESC_OPCODE_TSO, flags, nsge, addr);\n\tdesc->cmd = cpu_to_le64(cmd);\n\tdesc->len = cpu_to_le16(len);\n\tdesc->vlan_tci = cpu_to_le16(vlan_tci);\n\tdesc->hdr_len = cpu_to_le16(hdrlen);\n\tdesc->mss = cpu_to_le16(mss);\n\n\tionic_write_cmb_desc(q, desc_info->cmb_desc, desc);\n\n\tif (start) {\n\t\tskb_tx_timestamp(skb);\n\t\tif (!unlikely(q->features & IONIC_TXQ_F_HWSTAMP))\n\t\t\tnetdev_tx_sent_queue(q_to_ndq(q), skb->len);\n\t\tionic_txq_post(q, false, ionic_tx_clean, skb);\n\t} else {\n\t\tionic_txq_post(q, done, NULL, NULL);\n\t}\n}\n\nstatic int ionic_tx_tso(struct ionic_queue *q, struct sk_buff *skb)\n{\n\tstruct ionic_tx_stats *stats = q_to_tx_stats(q);\n\tstruct ionic_desc_info *desc_info;\n\tstruct ionic_buf_info *buf_info;\n\tstruct ionic_txq_sg_elem *elem;\n\tstruct ionic_txq_desc *desc;\n\tunsigned int chunk_len;\n\tunsigned int frag_rem;\n\tunsigned int tso_rem;\n\tunsigned int seg_rem;\n\tdma_addr_t desc_addr;\n\tdma_addr_t frag_addr;\n\tunsigned int hdrlen;\n\tunsigned int len;\n\tunsigned int mss;\n\tbool start, done;\n\tbool outer_csum;\n\tbool has_vlan;\n\tu16 desc_len;\n\tu8 desc_nsge;\n\tu16 vlan_tci;\n\tbool encap;\n\tint err;\n\n\tdesc_info = &q->info[q->head_idx];\n\tbuf_info = desc_info->bufs;\n\n\tif (unlikely(ionic_tx_map_skb(q, skb, desc_info)))\n\t\treturn -EIO;\n\n\tlen = skb->len;\n\tmss = skb_shinfo(skb)->gso_size;\n\touter_csum = (skb_shinfo(skb)->gso_type & (SKB_GSO_GRE |\n\t\t\t\t\t\t   SKB_GSO_GRE_CSUM |\n\t\t\t\t\t\t   SKB_GSO_IPXIP4 |\n\t\t\t\t\t\t   SKB_GSO_IPXIP6 |\n\t\t\t\t\t\t   SKB_GSO_UDP_TUNNEL |\n\t\t\t\t\t\t   SKB_GSO_UDP_TUNNEL_CSUM));\n\thas_vlan = !!skb_vlan_tag_present(skb);\n\tvlan_tci = skb_vlan_tag_get(skb);\n\tencap = skb->encapsulation;\n\n\t \n\n\tif (encap)\n\t\terr = ionic_tx_tcp_inner_pseudo_csum(skb);\n\telse\n\t\terr = ionic_tx_tcp_pseudo_csum(skb);\n\tif (err) {\n\t\t \n\t\tionic_tx_desc_unmap_bufs(q, desc_info);\n\t\treturn err;\n\t}\n\n\tif (encap)\n\t\thdrlen = skb_inner_tcp_all_headers(skb);\n\telse\n\t\thdrlen = skb_tcp_all_headers(skb);\n\n\ttso_rem = len;\n\tseg_rem = min(tso_rem, hdrlen + mss);\n\n\tfrag_addr = 0;\n\tfrag_rem = 0;\n\n\tstart = true;\n\n\twhile (tso_rem > 0) {\n\t\tdesc = NULL;\n\t\telem = NULL;\n\t\tdesc_addr = 0;\n\t\tdesc_len = 0;\n\t\tdesc_nsge = 0;\n\t\t \n\t\twhile (seg_rem > 0) {\n\t\t\t \n\t\t\tif (frag_rem == 0) {\n\t\t\t\t \n\t\t\t\tfrag_addr = buf_info->dma_addr;\n\t\t\t\tfrag_rem = buf_info->len;\n\t\t\t\tbuf_info++;\n\t\t\t}\n\t\t\tchunk_len = min(frag_rem, seg_rem);\n\t\t\tif (!desc) {\n\t\t\t\t \n\t\t\t\tdesc = desc_info->txq_desc;\n\t\t\t\telem = desc_info->txq_sg_desc->elems;\n\t\t\t\tdesc_addr = frag_addr;\n\t\t\t\tdesc_len = chunk_len;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\telem->addr = cpu_to_le64(frag_addr);\n\t\t\t\telem->len = cpu_to_le16(chunk_len);\n\t\t\t\telem++;\n\t\t\t\tdesc_nsge++;\n\t\t\t}\n\t\t\tfrag_addr += chunk_len;\n\t\t\tfrag_rem -= chunk_len;\n\t\t\ttso_rem -= chunk_len;\n\t\t\tseg_rem -= chunk_len;\n\t\t}\n\t\tseg_rem = min(tso_rem, mss);\n\t\tdone = (tso_rem == 0);\n\t\t \n\t\tionic_tx_tso_post(q, desc_info, skb,\n\t\t\t\t  desc_addr, desc_nsge, desc_len,\n\t\t\t\t  hdrlen, mss, outer_csum, vlan_tci, has_vlan,\n\t\t\t\t  start, done);\n\t\tstart = false;\n\t\t \n\t\tdesc_info = &q->info[q->head_idx];\n\t\tdesc_info->nbufs = 0;\n\t}\n\n\tstats->pkts += DIV_ROUND_UP(len - hdrlen, mss);\n\tstats->bytes += len;\n\tstats->tso++;\n\tstats->tso_bytes = len;\n\n\treturn 0;\n}\n\nstatic void ionic_tx_calc_csum(struct ionic_queue *q, struct sk_buff *skb,\n\t\t\t       struct ionic_desc_info *desc_info)\n{\n\tstruct ionic_txq_desc *desc = desc_info->txq_desc;\n\tstruct ionic_buf_info *buf_info = desc_info->bufs;\n\tstruct ionic_tx_stats *stats = q_to_tx_stats(q);\n\tbool has_vlan;\n\tu8 flags = 0;\n\tbool encap;\n\tu64 cmd;\n\n\thas_vlan = !!skb_vlan_tag_present(skb);\n\tencap = skb->encapsulation;\n\n\tflags |= has_vlan ? IONIC_TXQ_DESC_FLAG_VLAN : 0;\n\tflags |= encap ? IONIC_TXQ_DESC_FLAG_ENCAP : 0;\n\n\tcmd = encode_txq_desc_cmd(IONIC_TXQ_DESC_OPCODE_CSUM_PARTIAL,\n\t\t\t\t  flags, skb_shinfo(skb)->nr_frags,\n\t\t\t\t  buf_info->dma_addr);\n\tdesc->cmd = cpu_to_le64(cmd);\n\tdesc->len = cpu_to_le16(buf_info->len);\n\tif (has_vlan) {\n\t\tdesc->vlan_tci = cpu_to_le16(skb_vlan_tag_get(skb));\n\t\tstats->vlan_inserted++;\n\t} else {\n\t\tdesc->vlan_tci = 0;\n\t}\n\tdesc->csum_start = cpu_to_le16(skb_checksum_start_offset(skb));\n\tdesc->csum_offset = cpu_to_le16(skb->csum_offset);\n\n\tionic_write_cmb_desc(q, desc_info->cmb_desc, desc);\n\n\tif (skb_csum_is_sctp(skb))\n\t\tstats->crc32_csum++;\n\telse\n\t\tstats->csum++;\n}\n\nstatic void ionic_tx_calc_no_csum(struct ionic_queue *q, struct sk_buff *skb,\n\t\t\t\t  struct ionic_desc_info *desc_info)\n{\n\tstruct ionic_txq_desc *desc = desc_info->txq_desc;\n\tstruct ionic_buf_info *buf_info = desc_info->bufs;\n\tstruct ionic_tx_stats *stats = q_to_tx_stats(q);\n\tbool has_vlan;\n\tu8 flags = 0;\n\tbool encap;\n\tu64 cmd;\n\n\thas_vlan = !!skb_vlan_tag_present(skb);\n\tencap = skb->encapsulation;\n\n\tflags |= has_vlan ? IONIC_TXQ_DESC_FLAG_VLAN : 0;\n\tflags |= encap ? IONIC_TXQ_DESC_FLAG_ENCAP : 0;\n\n\tcmd = encode_txq_desc_cmd(IONIC_TXQ_DESC_OPCODE_CSUM_NONE,\n\t\t\t\t  flags, skb_shinfo(skb)->nr_frags,\n\t\t\t\t  buf_info->dma_addr);\n\tdesc->cmd = cpu_to_le64(cmd);\n\tdesc->len = cpu_to_le16(buf_info->len);\n\tif (has_vlan) {\n\t\tdesc->vlan_tci = cpu_to_le16(skb_vlan_tag_get(skb));\n\t\tstats->vlan_inserted++;\n\t} else {\n\t\tdesc->vlan_tci = 0;\n\t}\n\tdesc->csum_start = 0;\n\tdesc->csum_offset = 0;\n\n\tionic_write_cmb_desc(q, desc_info->cmb_desc, desc);\n\n\tstats->csum_none++;\n}\n\nstatic void ionic_tx_skb_frags(struct ionic_queue *q, struct sk_buff *skb,\n\t\t\t       struct ionic_desc_info *desc_info)\n{\n\tstruct ionic_txq_sg_desc *sg_desc = desc_info->txq_sg_desc;\n\tstruct ionic_buf_info *buf_info = &desc_info->bufs[1];\n\tstruct ionic_txq_sg_elem *elem = sg_desc->elems;\n\tstruct ionic_tx_stats *stats = q_to_tx_stats(q);\n\tunsigned int i;\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++, buf_info++, elem++) {\n\t\telem->addr = cpu_to_le64(buf_info->dma_addr);\n\t\telem->len = cpu_to_le16(buf_info->len);\n\t}\n\n\tstats->frags += skb_shinfo(skb)->nr_frags;\n}\n\nstatic int ionic_tx(struct ionic_queue *q, struct sk_buff *skb)\n{\n\tstruct ionic_desc_info *desc_info = &q->info[q->head_idx];\n\tstruct ionic_tx_stats *stats = q_to_tx_stats(q);\n\n\tif (unlikely(ionic_tx_map_skb(q, skb, desc_info)))\n\t\treturn -EIO;\n\n\t \n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tionic_tx_calc_csum(q, skb, desc_info);\n\telse\n\t\tionic_tx_calc_no_csum(q, skb, desc_info);\n\n\t \n\tionic_tx_skb_frags(q, skb, desc_info);\n\n\tskb_tx_timestamp(skb);\n\tstats->pkts++;\n\tstats->bytes += skb->len;\n\n\tif (!unlikely(q->features & IONIC_TXQ_F_HWSTAMP))\n\t\tnetdev_tx_sent_queue(q_to_ndq(q), skb->len);\n\tionic_txq_post(q, !netdev_xmit_more(), ionic_tx_clean, skb);\n\n\treturn 0;\n}\n\nstatic int ionic_tx_descs_needed(struct ionic_queue *q, struct sk_buff *skb)\n{\n\tstruct ionic_tx_stats *stats = q_to_tx_stats(q);\n\tint ndescs;\n\tint err;\n\n\t \n\tif (skb_is_gso(skb))\n\t\tndescs = skb_shinfo(skb)->gso_segs;\n\telse\n\t\tndescs = 1;\n\n\t \n\tif (skb_shinfo(skb)->nr_frags <= q->max_sg_elems)\n\t\treturn ndescs;\n\n\t \n\terr = skb_linearize(skb);\n\tif (err)\n\t\treturn err;\n\n\tstats->linearize++;\n\n\treturn ndescs;\n}\n\nstatic int ionic_maybe_stop_tx(struct ionic_queue *q, int ndescs)\n{\n\tint stopped = 0;\n\n\tif (unlikely(!ionic_q_has_space(q, ndescs))) {\n\t\tnetif_stop_subqueue(q->lif->netdev, q->index);\n\t\tstopped = 1;\n\n\t\t \n\t\tsmp_rmb();\n\t\tif (ionic_q_has_space(q, ndescs)) {\n\t\t\tnetif_wake_subqueue(q->lif->netdev, q->index);\n\t\t\tstopped = 0;\n\t\t}\n\t}\n\n\treturn stopped;\n}\n\nstatic netdev_tx_t ionic_start_hwstamp_xmit(struct sk_buff *skb,\n\t\t\t\t\t    struct net_device *netdev)\n{\n\tstruct ionic_lif *lif = netdev_priv(netdev);\n\tstruct ionic_queue *q = &lif->hwstamp_txq->q;\n\tint err, ndescs;\n\n\t \n\n\tndescs = ionic_tx_descs_needed(q, skb);\n\tif (unlikely(ndescs < 0))\n\t\tgoto err_out_drop;\n\n\tif (unlikely(!ionic_q_has_space(q, ndescs)))\n\t\tgoto err_out_drop;\n\n\tskb_shinfo(skb)->tx_flags |= SKBTX_HW_TSTAMP;\n\tif (skb_is_gso(skb))\n\t\terr = ionic_tx_tso(q, skb);\n\telse\n\t\terr = ionic_tx(q, skb);\n\n\tif (err)\n\t\tgoto err_out_drop;\n\n\treturn NETDEV_TX_OK;\n\nerr_out_drop:\n\tq->drop++;\n\tdev_kfree_skb(skb);\n\treturn NETDEV_TX_OK;\n}\n\nnetdev_tx_t ionic_start_xmit(struct sk_buff *skb, struct net_device *netdev)\n{\n\tu16 queue_index = skb_get_queue_mapping(skb);\n\tstruct ionic_lif *lif = netdev_priv(netdev);\n\tstruct ionic_queue *q;\n\tint ndescs;\n\tint err;\n\n\tif (unlikely(!test_bit(IONIC_LIF_F_UP, lif->state))) {\n\t\tdev_kfree_skb(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\tif (unlikely(skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP))\n\t\tif (lif->hwstamp_txq && lif->phc->ts_config_tx_mode)\n\t\t\treturn ionic_start_hwstamp_xmit(skb, netdev);\n\n\tif (unlikely(queue_index >= lif->nxqs))\n\t\tqueue_index = 0;\n\tq = &lif->txqcqs[queue_index]->q;\n\n\tndescs = ionic_tx_descs_needed(q, skb);\n\tif (ndescs < 0)\n\t\tgoto err_out_drop;\n\n\tif (unlikely(ionic_maybe_stop_tx(q, ndescs)))\n\t\treturn NETDEV_TX_BUSY;\n\n\tif (skb_is_gso(skb))\n\t\terr = ionic_tx_tso(q, skb);\n\telse\n\t\terr = ionic_tx(q, skb);\n\n\tif (err)\n\t\tgoto err_out_drop;\n\n\t \n\tionic_maybe_stop_tx(q, 4);\n\n\treturn NETDEV_TX_OK;\n\nerr_out_drop:\n\tq->drop++;\n\tdev_kfree_skb(skb);\n\treturn NETDEV_TX_OK;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}