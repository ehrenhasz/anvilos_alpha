{
  "module_name": "hw_channel.c",
  "hash_id": "a272d94a622a3857a4fc7759fe604b86ff5d02b40d66e4a3e69be4ac01ed9be1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/microsoft/mana/hw_channel.c",
  "human_readable_source": "\n \n\n#include <net/mana/gdma.h>\n#include <net/mana/hw_channel.h>\n\nstatic int mana_hwc_get_msg_index(struct hw_channel_context *hwc, u16 *msg_id)\n{\n\tstruct gdma_resource *r = &hwc->inflight_msg_res;\n\tunsigned long flags;\n\tu32 index;\n\n\tdown(&hwc->sema);\n\n\tspin_lock_irqsave(&r->lock, flags);\n\n\tindex = find_first_zero_bit(hwc->inflight_msg_res.map,\n\t\t\t\t    hwc->inflight_msg_res.size);\n\n\tbitmap_set(hwc->inflight_msg_res.map, index, 1);\n\n\tspin_unlock_irqrestore(&r->lock, flags);\n\n\t*msg_id = index;\n\n\treturn 0;\n}\n\nstatic void mana_hwc_put_msg_index(struct hw_channel_context *hwc, u16 msg_id)\n{\n\tstruct gdma_resource *r = &hwc->inflight_msg_res;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&r->lock, flags);\n\tbitmap_clear(hwc->inflight_msg_res.map, msg_id, 1);\n\tspin_unlock_irqrestore(&r->lock, flags);\n\n\tup(&hwc->sema);\n}\n\nstatic int mana_hwc_verify_resp_msg(const struct hwc_caller_ctx *caller_ctx,\n\t\t\t\t    const struct gdma_resp_hdr *resp_msg,\n\t\t\t\t    u32 resp_len)\n{\n\tif (resp_len < sizeof(*resp_msg))\n\t\treturn -EPROTO;\n\n\tif (resp_len > caller_ctx->output_buflen)\n\t\treturn -EPROTO;\n\n\treturn 0;\n}\n\nstatic void mana_hwc_handle_resp(struct hw_channel_context *hwc, u32 resp_len,\n\t\t\t\t const struct gdma_resp_hdr *resp_msg)\n{\n\tstruct hwc_caller_ctx *ctx;\n\tint err;\n\n\tif (!test_bit(resp_msg->response.hwc_msg_id,\n\t\t      hwc->inflight_msg_res.map)) {\n\t\tdev_err(hwc->dev, \"hwc_rx: invalid msg_id = %u\\n\",\n\t\t\tresp_msg->response.hwc_msg_id);\n\t\treturn;\n\t}\n\n\tctx = hwc->caller_ctx + resp_msg->response.hwc_msg_id;\n\terr = mana_hwc_verify_resp_msg(ctx, resp_msg, resp_len);\n\tif (err)\n\t\tgoto out;\n\n\tctx->status_code = resp_msg->status;\n\n\tmemcpy(ctx->output_buf, resp_msg, resp_len);\nout:\n\tctx->error = err;\n\tcomplete(&ctx->comp_event);\n}\n\nstatic int mana_hwc_post_rx_wqe(const struct hwc_wq *hwc_rxq,\n\t\t\t\tstruct hwc_work_request *req)\n{\n\tstruct device *dev = hwc_rxq->hwc->dev;\n\tstruct gdma_sge *sge;\n\tint err;\n\n\tsge = &req->sge;\n\tsge->address = (u64)req->buf_sge_addr;\n\tsge->mem_key = hwc_rxq->msg_buf->gpa_mkey;\n\tsge->size = req->buf_len;\n\n\tmemset(&req->wqe_req, 0, sizeof(struct gdma_wqe_request));\n\treq->wqe_req.sgl = sge;\n\treq->wqe_req.num_sge = 1;\n\treq->wqe_req.client_data_unit = 0;\n\n\terr = mana_gd_post_and_ring(hwc_rxq->gdma_wq, &req->wqe_req, NULL);\n\tif (err)\n\t\tdev_err(dev, \"Failed to post WQE on HWC RQ: %d\\n\", err);\n\treturn err;\n}\n\nstatic void mana_hwc_init_event_handler(void *ctx, struct gdma_queue *q_self,\n\t\t\t\t\tstruct gdma_event *event)\n{\n\tstruct hw_channel_context *hwc = ctx;\n\tstruct gdma_dev *gd = hwc->gdma_dev;\n\tunion hwc_init_type_data type_data;\n\tunion hwc_init_eq_id_db eq_db;\n\tu32 type, val;\n\n\tswitch (event->type) {\n\tcase GDMA_EQE_HWC_INIT_EQ_ID_DB:\n\t\teq_db.as_uint32 = event->details[0];\n\t\thwc->cq->gdma_eq->id = eq_db.eq_id;\n\t\tgd->doorbell = eq_db.doorbell;\n\t\tbreak;\n\n\tcase GDMA_EQE_HWC_INIT_DATA:\n\t\ttype_data.as_uint32 = event->details[0];\n\t\ttype = type_data.type;\n\t\tval = type_data.value;\n\n\t\tswitch (type) {\n\t\tcase HWC_INIT_DATA_CQID:\n\t\t\thwc->cq->gdma_cq->id = val;\n\t\t\tbreak;\n\n\t\tcase HWC_INIT_DATA_RQID:\n\t\t\thwc->rxq->gdma_wq->id = val;\n\t\t\tbreak;\n\n\t\tcase HWC_INIT_DATA_SQID:\n\t\t\thwc->txq->gdma_wq->id = val;\n\t\t\tbreak;\n\n\t\tcase HWC_INIT_DATA_QUEUE_DEPTH:\n\t\t\thwc->hwc_init_q_depth_max = (u16)val;\n\t\t\tbreak;\n\n\t\tcase HWC_INIT_DATA_MAX_REQUEST:\n\t\t\thwc->hwc_init_max_req_msg_size = val;\n\t\t\tbreak;\n\n\t\tcase HWC_INIT_DATA_MAX_RESPONSE:\n\t\t\thwc->hwc_init_max_resp_msg_size = val;\n\t\t\tbreak;\n\n\t\tcase HWC_INIT_DATA_MAX_NUM_CQS:\n\t\t\tgd->gdma_context->max_num_cqs = val;\n\t\t\tbreak;\n\n\t\tcase HWC_INIT_DATA_PDID:\n\t\t\thwc->gdma_dev->pdid = val;\n\t\t\tbreak;\n\n\t\tcase HWC_INIT_DATA_GPA_MKEY:\n\t\t\thwc->rxq->msg_buf->gpa_mkey = val;\n\t\t\thwc->txq->msg_buf->gpa_mkey = val;\n\t\t\tbreak;\n\n\t\tcase HWC_INIT_DATA_PF_DEST_RQ_ID:\n\t\t\thwc->pf_dest_vrq_id = val;\n\t\t\tbreak;\n\n\t\tcase HWC_INIT_DATA_PF_DEST_CQ_ID:\n\t\t\thwc->pf_dest_vrcq_id = val;\n\t\t\tbreak;\n\t\t}\n\n\t\tbreak;\n\n\tcase GDMA_EQE_HWC_INIT_DONE:\n\t\tcomplete(&hwc->hwc_init_eqe_comp);\n\t\tbreak;\n\n\tcase GDMA_EQE_HWC_SOC_RECONFIG_DATA:\n\t\ttype_data.as_uint32 = event->details[0];\n\t\ttype = type_data.type;\n\t\tval = type_data.value;\n\n\t\tswitch (type) {\n\t\tcase HWC_DATA_CFG_HWC_TIMEOUT:\n\t\t\thwc->hwc_timeout = val;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tdev_warn(hwc->dev, \"Received unknown reconfig type %u\\n\", type);\n\t\t\tbreak;\n\t\t}\n\n\t\tbreak;\n\n\tdefault:\n\t\tdev_warn(hwc->dev, \"Received unknown gdma event %u\\n\", event->type);\n\t\t \n\t\tbreak;\n\t}\n}\n\nstatic void mana_hwc_rx_event_handler(void *ctx, u32 gdma_rxq_id,\n\t\t\t\t      const struct hwc_rx_oob *rx_oob)\n{\n\tstruct hw_channel_context *hwc = ctx;\n\tstruct hwc_wq *hwc_rxq = hwc->rxq;\n\tstruct hwc_work_request *rx_req;\n\tstruct gdma_resp_hdr *resp;\n\tstruct gdma_wqe *dma_oob;\n\tstruct gdma_queue *rq;\n\tstruct gdma_sge *sge;\n\tu64 rq_base_addr;\n\tu64 rx_req_idx;\n\tu8 *wqe;\n\n\tif (WARN_ON_ONCE(hwc_rxq->gdma_wq->id != gdma_rxq_id))\n\t\treturn;\n\n\trq = hwc_rxq->gdma_wq;\n\twqe = mana_gd_get_wqe_ptr(rq, rx_oob->wqe_offset / GDMA_WQE_BU_SIZE);\n\tdma_oob = (struct gdma_wqe *)wqe;\n\n\tsge = (struct gdma_sge *)(wqe + 8 + dma_oob->inline_oob_size_div4 * 4);\n\n\t \n\trq_base_addr = hwc_rxq->msg_buf->mem_info.dma_handle;\n\trx_req_idx = (sge->address - rq_base_addr) / hwc->max_req_msg_size;\n\n\trx_req = &hwc_rxq->msg_buf->reqs[rx_req_idx];\n\tresp = (struct gdma_resp_hdr *)rx_req->buf_va;\n\n\tif (resp->response.hwc_msg_id >= hwc->num_inflight_msg) {\n\t\tdev_err(hwc->dev, \"HWC RX: wrong msg_id=%u\\n\",\n\t\t\tresp->response.hwc_msg_id);\n\t\treturn;\n\t}\n\n\tmana_hwc_handle_resp(hwc, rx_oob->tx_oob_data_size, resp);\n\n\t \n\tresp = NULL;\n\n\tmana_hwc_post_rx_wqe(hwc_rxq, rx_req);\n}\n\nstatic void mana_hwc_tx_event_handler(void *ctx, u32 gdma_txq_id,\n\t\t\t\t      const struct hwc_rx_oob *rx_oob)\n{\n\tstruct hw_channel_context *hwc = ctx;\n\tstruct hwc_wq *hwc_txq = hwc->txq;\n\n\tWARN_ON_ONCE(!hwc_txq || hwc_txq->gdma_wq->id != gdma_txq_id);\n}\n\nstatic int mana_hwc_create_gdma_wq(struct hw_channel_context *hwc,\n\t\t\t\t   enum gdma_queue_type type, u64 queue_size,\n\t\t\t\t   struct gdma_queue **queue)\n{\n\tstruct gdma_queue_spec spec = {};\n\n\tif (type != GDMA_SQ && type != GDMA_RQ)\n\t\treturn -EINVAL;\n\n\tspec.type = type;\n\tspec.monitor_avl_buf = false;\n\tspec.queue_size = queue_size;\n\n\treturn mana_gd_create_hwc_queue(hwc->gdma_dev, &spec, queue);\n}\n\nstatic int mana_hwc_create_gdma_cq(struct hw_channel_context *hwc,\n\t\t\t\t   u64 queue_size,\n\t\t\t\t   void *ctx, gdma_cq_callback *cb,\n\t\t\t\t   struct gdma_queue *parent_eq,\n\t\t\t\t   struct gdma_queue **queue)\n{\n\tstruct gdma_queue_spec spec = {};\n\n\tspec.type = GDMA_CQ;\n\tspec.monitor_avl_buf = false;\n\tspec.queue_size = queue_size;\n\tspec.cq.context = ctx;\n\tspec.cq.callback = cb;\n\tspec.cq.parent_eq = parent_eq;\n\n\treturn mana_gd_create_hwc_queue(hwc->gdma_dev, &spec, queue);\n}\n\nstatic int mana_hwc_create_gdma_eq(struct hw_channel_context *hwc,\n\t\t\t\t   u64 queue_size,\n\t\t\t\t   void *ctx, gdma_eq_callback *cb,\n\t\t\t\t   struct gdma_queue **queue)\n{\n\tstruct gdma_queue_spec spec = {};\n\n\tspec.type = GDMA_EQ;\n\tspec.monitor_avl_buf = false;\n\tspec.queue_size = queue_size;\n\tspec.eq.context = ctx;\n\tspec.eq.callback = cb;\n\tspec.eq.log2_throttle_limit = DEFAULT_LOG2_THROTTLING_FOR_ERROR_EQ;\n\n\treturn mana_gd_create_hwc_queue(hwc->gdma_dev, &spec, queue);\n}\n\nstatic void mana_hwc_comp_event(void *ctx, struct gdma_queue *q_self)\n{\n\tstruct hwc_rx_oob comp_data = {};\n\tstruct gdma_comp *completions;\n\tstruct hwc_cq *hwc_cq = ctx;\n\tint comp_read, i;\n\n\tWARN_ON_ONCE(hwc_cq->gdma_cq != q_self);\n\n\tcompletions = hwc_cq->comp_buf;\n\tcomp_read = mana_gd_poll_cq(q_self, completions, hwc_cq->queue_depth);\n\tWARN_ON_ONCE(comp_read <= 0 || comp_read > hwc_cq->queue_depth);\n\n\tfor (i = 0; i < comp_read; ++i) {\n\t\tcomp_data = *(struct hwc_rx_oob *)completions[i].cqe_data;\n\n\t\tif (completions[i].is_sq)\n\t\t\thwc_cq->tx_event_handler(hwc_cq->tx_event_ctx,\n\t\t\t\t\t\tcompletions[i].wq_num,\n\t\t\t\t\t\t&comp_data);\n\t\telse\n\t\t\thwc_cq->rx_event_handler(hwc_cq->rx_event_ctx,\n\t\t\t\t\t\tcompletions[i].wq_num,\n\t\t\t\t\t\t&comp_data);\n\t}\n\n\tmana_gd_ring_cq(q_self, SET_ARM_BIT);\n}\n\nstatic void mana_hwc_destroy_cq(struct gdma_context *gc, struct hwc_cq *hwc_cq)\n{\n\tkfree(hwc_cq->comp_buf);\n\n\tif (hwc_cq->gdma_cq)\n\t\tmana_gd_destroy_queue(gc, hwc_cq->gdma_cq);\n\n\tif (hwc_cq->gdma_eq)\n\t\tmana_gd_destroy_queue(gc, hwc_cq->gdma_eq);\n\n\tkfree(hwc_cq);\n}\n\nstatic int mana_hwc_create_cq(struct hw_channel_context *hwc, u16 q_depth,\n\t\t\t      gdma_eq_callback *callback, void *ctx,\n\t\t\t      hwc_rx_event_handler_t *rx_ev_hdlr,\n\t\t\t      void *rx_ev_ctx,\n\t\t\t      hwc_tx_event_handler_t *tx_ev_hdlr,\n\t\t\t      void *tx_ev_ctx, struct hwc_cq **hwc_cq_ptr)\n{\n\tstruct gdma_queue *eq, *cq;\n\tstruct gdma_comp *comp_buf;\n\tstruct hwc_cq *hwc_cq;\n\tu32 eq_size, cq_size;\n\tint err;\n\n\teq_size = roundup_pow_of_two(GDMA_EQE_SIZE * q_depth);\n\tif (eq_size < MINIMUM_SUPPORTED_PAGE_SIZE)\n\t\teq_size = MINIMUM_SUPPORTED_PAGE_SIZE;\n\n\tcq_size = roundup_pow_of_two(GDMA_CQE_SIZE * q_depth);\n\tif (cq_size < MINIMUM_SUPPORTED_PAGE_SIZE)\n\t\tcq_size = MINIMUM_SUPPORTED_PAGE_SIZE;\n\n\thwc_cq = kzalloc(sizeof(*hwc_cq), GFP_KERNEL);\n\tif (!hwc_cq)\n\t\treturn -ENOMEM;\n\n\terr = mana_hwc_create_gdma_eq(hwc, eq_size, ctx, callback, &eq);\n\tif (err) {\n\t\tdev_err(hwc->dev, \"Failed to create HWC EQ for RQ: %d\\n\", err);\n\t\tgoto out;\n\t}\n\thwc_cq->gdma_eq = eq;\n\n\terr = mana_hwc_create_gdma_cq(hwc, cq_size, hwc_cq, mana_hwc_comp_event,\n\t\t\t\t      eq, &cq);\n\tif (err) {\n\t\tdev_err(hwc->dev, \"Failed to create HWC CQ for RQ: %d\\n\", err);\n\t\tgoto out;\n\t}\n\thwc_cq->gdma_cq = cq;\n\n\tcomp_buf = kcalloc(q_depth, sizeof(*comp_buf), GFP_KERNEL);\n\tif (!comp_buf) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\thwc_cq->hwc = hwc;\n\thwc_cq->comp_buf = comp_buf;\n\thwc_cq->queue_depth = q_depth;\n\thwc_cq->rx_event_handler = rx_ev_hdlr;\n\thwc_cq->rx_event_ctx = rx_ev_ctx;\n\thwc_cq->tx_event_handler = tx_ev_hdlr;\n\thwc_cq->tx_event_ctx = tx_ev_ctx;\n\n\t*hwc_cq_ptr = hwc_cq;\n\treturn 0;\nout:\n\tmana_hwc_destroy_cq(hwc->gdma_dev->gdma_context, hwc_cq);\n\treturn err;\n}\n\nstatic int mana_hwc_alloc_dma_buf(struct hw_channel_context *hwc, u16 q_depth,\n\t\t\t\t  u32 max_msg_size,\n\t\t\t\t  struct hwc_dma_buf **dma_buf_ptr)\n{\n\tstruct gdma_context *gc = hwc->gdma_dev->gdma_context;\n\tstruct hwc_work_request *hwc_wr;\n\tstruct hwc_dma_buf *dma_buf;\n\tstruct gdma_mem_info *gmi;\n\tvoid *virt_addr;\n\tu32 buf_size;\n\tu8 *base_pa;\n\tint err;\n\tu16 i;\n\n\tdma_buf = kzalloc(struct_size(dma_buf, reqs, q_depth), GFP_KERNEL);\n\tif (!dma_buf)\n\t\treturn -ENOMEM;\n\n\tdma_buf->num_reqs = q_depth;\n\n\tbuf_size = PAGE_ALIGN(q_depth * max_msg_size);\n\n\tgmi = &dma_buf->mem_info;\n\terr = mana_gd_alloc_memory(gc, buf_size, gmi);\n\tif (err) {\n\t\tdev_err(hwc->dev, \"Failed to allocate DMA buffer: %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\tvirt_addr = dma_buf->mem_info.virt_addr;\n\tbase_pa = (u8 *)dma_buf->mem_info.dma_handle;\n\n\tfor (i = 0; i < q_depth; i++) {\n\t\thwc_wr = &dma_buf->reqs[i];\n\n\t\thwc_wr->buf_va = virt_addr + i * max_msg_size;\n\t\thwc_wr->buf_sge_addr = base_pa + i * max_msg_size;\n\n\t\thwc_wr->buf_len = max_msg_size;\n\t}\n\n\t*dma_buf_ptr = dma_buf;\n\treturn 0;\nout:\n\tkfree(dma_buf);\n\treturn err;\n}\n\nstatic void mana_hwc_dealloc_dma_buf(struct hw_channel_context *hwc,\n\t\t\t\t     struct hwc_dma_buf *dma_buf)\n{\n\tif (!dma_buf)\n\t\treturn;\n\n\tmana_gd_free_memory(&dma_buf->mem_info);\n\n\tkfree(dma_buf);\n}\n\nstatic void mana_hwc_destroy_wq(struct hw_channel_context *hwc,\n\t\t\t\tstruct hwc_wq *hwc_wq)\n{\n\tmana_hwc_dealloc_dma_buf(hwc, hwc_wq->msg_buf);\n\n\tif (hwc_wq->gdma_wq)\n\t\tmana_gd_destroy_queue(hwc->gdma_dev->gdma_context,\n\t\t\t\t      hwc_wq->gdma_wq);\n\n\tkfree(hwc_wq);\n}\n\nstatic int mana_hwc_create_wq(struct hw_channel_context *hwc,\n\t\t\t      enum gdma_queue_type q_type, u16 q_depth,\n\t\t\t      u32 max_msg_size, struct hwc_cq *hwc_cq,\n\t\t\t      struct hwc_wq **hwc_wq_ptr)\n{\n\tstruct gdma_queue *queue;\n\tstruct hwc_wq *hwc_wq;\n\tu32 queue_size;\n\tint err;\n\n\tWARN_ON(q_type != GDMA_SQ && q_type != GDMA_RQ);\n\n\tif (q_type == GDMA_RQ)\n\t\tqueue_size = roundup_pow_of_two(GDMA_MAX_RQE_SIZE * q_depth);\n\telse\n\t\tqueue_size = roundup_pow_of_two(GDMA_MAX_SQE_SIZE * q_depth);\n\n\tif (queue_size < MINIMUM_SUPPORTED_PAGE_SIZE)\n\t\tqueue_size = MINIMUM_SUPPORTED_PAGE_SIZE;\n\n\thwc_wq = kzalloc(sizeof(*hwc_wq), GFP_KERNEL);\n\tif (!hwc_wq)\n\t\treturn -ENOMEM;\n\n\terr = mana_hwc_create_gdma_wq(hwc, q_type, queue_size, &queue);\n\tif (err)\n\t\tgoto out;\n\n\thwc_wq->hwc = hwc;\n\thwc_wq->gdma_wq = queue;\n\thwc_wq->queue_depth = q_depth;\n\thwc_wq->hwc_cq = hwc_cq;\n\n\terr = mana_hwc_alloc_dma_buf(hwc, q_depth, max_msg_size,\n\t\t\t\t     &hwc_wq->msg_buf);\n\tif (err)\n\t\tgoto out;\n\n\t*hwc_wq_ptr = hwc_wq;\n\treturn 0;\nout:\n\tif (err)\n\t\tmana_hwc_destroy_wq(hwc, hwc_wq);\n\treturn err;\n}\n\nstatic int mana_hwc_post_tx_wqe(const struct hwc_wq *hwc_txq,\n\t\t\t\tstruct hwc_work_request *req,\n\t\t\t\tu32 dest_virt_rq_id, u32 dest_virt_rcq_id,\n\t\t\t\tbool dest_pf)\n{\n\tstruct device *dev = hwc_txq->hwc->dev;\n\tstruct hwc_tx_oob *tx_oob;\n\tstruct gdma_sge *sge;\n\tint err;\n\n\tif (req->msg_size == 0 || req->msg_size > req->buf_len) {\n\t\tdev_err(dev, \"wrong msg_size: %u, buf_len: %u\\n\",\n\t\t\treq->msg_size, req->buf_len);\n\t\treturn -EINVAL;\n\t}\n\n\ttx_oob = &req->tx_oob;\n\n\ttx_oob->vrq_id = dest_virt_rq_id;\n\ttx_oob->dest_vfid = 0;\n\ttx_oob->vrcq_id = dest_virt_rcq_id;\n\ttx_oob->vscq_id = hwc_txq->hwc_cq->gdma_cq->id;\n\ttx_oob->loopback = false;\n\ttx_oob->lso_override = false;\n\ttx_oob->dest_pf = dest_pf;\n\ttx_oob->vsq_id = hwc_txq->gdma_wq->id;\n\n\tsge = &req->sge;\n\tsge->address = (u64)req->buf_sge_addr;\n\tsge->mem_key = hwc_txq->msg_buf->gpa_mkey;\n\tsge->size = req->msg_size;\n\n\tmemset(&req->wqe_req, 0, sizeof(struct gdma_wqe_request));\n\treq->wqe_req.sgl = sge;\n\treq->wqe_req.num_sge = 1;\n\treq->wqe_req.inline_oob_size = sizeof(struct hwc_tx_oob);\n\treq->wqe_req.inline_oob_data = tx_oob;\n\treq->wqe_req.client_data_unit = 0;\n\n\terr = mana_gd_post_and_ring(hwc_txq->gdma_wq, &req->wqe_req, NULL);\n\tif (err)\n\t\tdev_err(dev, \"Failed to post WQE on HWC SQ: %d\\n\", err);\n\treturn err;\n}\n\nstatic int mana_hwc_init_inflight_msg(struct hw_channel_context *hwc,\n\t\t\t\t      u16 num_msg)\n{\n\tint err;\n\n\tsema_init(&hwc->sema, num_msg);\n\n\terr = mana_gd_alloc_res_map(num_msg, &hwc->inflight_msg_res);\n\tif (err)\n\t\tdev_err(hwc->dev, \"Failed to init inflight_msg_res: %d\\n\", err);\n\treturn err;\n}\n\nstatic int mana_hwc_test_channel(struct hw_channel_context *hwc, u16 q_depth,\n\t\t\t\t u32 max_req_msg_size, u32 max_resp_msg_size)\n{\n\tstruct gdma_context *gc = hwc->gdma_dev->gdma_context;\n\tstruct hwc_wq *hwc_rxq = hwc->rxq;\n\tstruct hwc_work_request *req;\n\tstruct hwc_caller_ctx *ctx;\n\tint err;\n\tint i;\n\n\t \n\tfor (i = 0; i < q_depth; i++) {\n\t\treq = &hwc_rxq->msg_buf->reqs[i];\n\t\terr = mana_hwc_post_rx_wqe(hwc_rxq, req);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tctx = kcalloc(q_depth, sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < q_depth; ++i)\n\t\tinit_completion(&ctx[i].comp_event);\n\n\thwc->caller_ctx = ctx;\n\n\treturn mana_gd_test_eq(gc, hwc->cq->gdma_eq);\n}\n\nstatic int mana_hwc_establish_channel(struct gdma_context *gc, u16 *q_depth,\n\t\t\t\t      u32 *max_req_msg_size,\n\t\t\t\t      u32 *max_resp_msg_size)\n{\n\tstruct hw_channel_context *hwc = gc->hwc.driver_data;\n\tstruct gdma_queue *rq = hwc->rxq->gdma_wq;\n\tstruct gdma_queue *sq = hwc->txq->gdma_wq;\n\tstruct gdma_queue *eq = hwc->cq->gdma_eq;\n\tstruct gdma_queue *cq = hwc->cq->gdma_cq;\n\tint err;\n\n\tinit_completion(&hwc->hwc_init_eqe_comp);\n\n\terr = mana_smc_setup_hwc(&gc->shm_channel, false,\n\t\t\t\t eq->mem_info.dma_handle,\n\t\t\t\t cq->mem_info.dma_handle,\n\t\t\t\t rq->mem_info.dma_handle,\n\t\t\t\t sq->mem_info.dma_handle,\n\t\t\t\t eq->eq.msix_index);\n\tif (err)\n\t\treturn err;\n\n\tif (!wait_for_completion_timeout(&hwc->hwc_init_eqe_comp, 60 * HZ))\n\t\treturn -ETIMEDOUT;\n\n\t*q_depth = hwc->hwc_init_q_depth_max;\n\t*max_req_msg_size = hwc->hwc_init_max_req_msg_size;\n\t*max_resp_msg_size = hwc->hwc_init_max_resp_msg_size;\n\n\t \n\tif (WARN_ON(cq->id >= gc->max_num_cqs))\n\t\treturn -EPROTO;\n\n\tgc->cq_table = vcalloc(gc->max_num_cqs, sizeof(struct gdma_queue *));\n\tif (!gc->cq_table)\n\t\treturn -ENOMEM;\n\n\tgc->cq_table[cq->id] = cq;\n\n\treturn 0;\n}\n\nstatic int mana_hwc_init_queues(struct hw_channel_context *hwc, u16 q_depth,\n\t\t\t\tu32 max_req_msg_size, u32 max_resp_msg_size)\n{\n\tint err;\n\n\terr = mana_hwc_init_inflight_msg(hwc, q_depth);\n\tif (err)\n\t\treturn err;\n\n\t \n\terr = mana_hwc_create_cq(hwc, q_depth * 2,\n\t\t\t\t mana_hwc_init_event_handler, hwc,\n\t\t\t\t mana_hwc_rx_event_handler, hwc,\n\t\t\t\t mana_hwc_tx_event_handler, hwc, &hwc->cq);\n\tif (err) {\n\t\tdev_err(hwc->dev, \"Failed to create HWC CQ: %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\terr = mana_hwc_create_wq(hwc, GDMA_RQ, q_depth, max_req_msg_size,\n\t\t\t\t hwc->cq, &hwc->rxq);\n\tif (err) {\n\t\tdev_err(hwc->dev, \"Failed to create HWC RQ: %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\terr = mana_hwc_create_wq(hwc, GDMA_SQ, q_depth, max_resp_msg_size,\n\t\t\t\t hwc->cq, &hwc->txq);\n\tif (err) {\n\t\tdev_err(hwc->dev, \"Failed to create HWC SQ: %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\thwc->num_inflight_msg = q_depth;\n\thwc->max_req_msg_size = max_req_msg_size;\n\n\treturn 0;\nout:\n\t \n\treturn err;\n}\n\nint mana_hwc_create_channel(struct gdma_context *gc)\n{\n\tu32 max_req_msg_size, max_resp_msg_size;\n\tstruct gdma_dev *gd = &gc->hwc;\n\tstruct hw_channel_context *hwc;\n\tu16 q_depth_max;\n\tint err;\n\n\thwc = kzalloc(sizeof(*hwc), GFP_KERNEL);\n\tif (!hwc)\n\t\treturn -ENOMEM;\n\n\tgd->gdma_context = gc;\n\tgd->driver_data = hwc;\n\thwc->gdma_dev = gd;\n\thwc->dev = gc->dev;\n\thwc->hwc_timeout = HW_CHANNEL_WAIT_RESOURCE_TIMEOUT_MS;\n\n\t \n\tgd->dev_id.as_uint32 = 0;\n\tgd->dev_id.type = GDMA_DEVICE_HWC;\n\n\tgd->pdid = INVALID_PDID;\n\tgd->doorbell = INVALID_DOORBELL;\n\n\t \n\terr = mana_hwc_init_queues(hwc, HW_CHANNEL_VF_BOOTSTRAP_QUEUE_DEPTH,\n\t\t\t\t   HW_CHANNEL_MAX_REQUEST_SIZE,\n\t\t\t\t   HW_CHANNEL_MAX_RESPONSE_SIZE);\n\tif (err) {\n\t\tdev_err(hwc->dev, \"Failed to initialize HWC: %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\terr = mana_hwc_establish_channel(gc, &q_depth_max, &max_req_msg_size,\n\t\t\t\t\t &max_resp_msg_size);\n\tif (err) {\n\t\tdev_err(hwc->dev, \"Failed to establish HWC: %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\terr = mana_hwc_test_channel(gc->hwc.driver_data,\n\t\t\t\t    HW_CHANNEL_VF_BOOTSTRAP_QUEUE_DEPTH,\n\t\t\t\t    max_req_msg_size, max_resp_msg_size);\n\tif (err) {\n\t\tdev_err(hwc->dev, \"Failed to test HWC: %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\treturn 0;\nout:\n\tmana_hwc_destroy_channel(gc);\n\treturn err;\n}\n\nvoid mana_hwc_destroy_channel(struct gdma_context *gc)\n{\n\tstruct hw_channel_context *hwc = gc->hwc.driver_data;\n\n\tif (!hwc)\n\t\treturn;\n\n\t \n\tif (gc->max_num_cqs > 0) {\n\t\tmana_smc_teardown_hwc(&gc->shm_channel, false);\n\t\tgc->max_num_cqs = 0;\n\t}\n\n\tkfree(hwc->caller_ctx);\n\thwc->caller_ctx = NULL;\n\n\tif (hwc->txq)\n\t\tmana_hwc_destroy_wq(hwc, hwc->txq);\n\n\tif (hwc->rxq)\n\t\tmana_hwc_destroy_wq(hwc, hwc->rxq);\n\n\tif (hwc->cq)\n\t\tmana_hwc_destroy_cq(hwc->gdma_dev->gdma_context, hwc->cq);\n\n\tmana_gd_free_res_map(&hwc->inflight_msg_res);\n\n\thwc->num_inflight_msg = 0;\n\n\thwc->gdma_dev->doorbell = INVALID_DOORBELL;\n\thwc->gdma_dev->pdid = INVALID_PDID;\n\n\thwc->hwc_timeout = 0;\n\n\tkfree(hwc);\n\tgc->hwc.driver_data = NULL;\n\tgc->hwc.gdma_context = NULL;\n\n\tvfree(gc->cq_table);\n\tgc->cq_table = NULL;\n}\n\nint mana_hwc_send_request(struct hw_channel_context *hwc, u32 req_len,\n\t\t\t  const void *req, u32 resp_len, void *resp)\n{\n\tstruct gdma_context *gc = hwc->gdma_dev->gdma_context;\n\tstruct hwc_work_request *tx_wr;\n\tstruct hwc_wq *txq = hwc->txq;\n\tstruct gdma_req_hdr *req_msg;\n\tstruct hwc_caller_ctx *ctx;\n\tu32 dest_vrcq = 0;\n\tu32 dest_vrq = 0;\n\tu16 msg_id;\n\tint err;\n\n\tmana_hwc_get_msg_index(hwc, &msg_id);\n\n\ttx_wr = &txq->msg_buf->reqs[msg_id];\n\n\tif (req_len > tx_wr->buf_len) {\n\t\tdev_err(hwc->dev, \"HWC: req msg size: %d > %d\\n\", req_len,\n\t\t\ttx_wr->buf_len);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tctx = hwc->caller_ctx + msg_id;\n\tctx->output_buf = resp;\n\tctx->output_buflen = resp_len;\n\n\treq_msg = (struct gdma_req_hdr *)tx_wr->buf_va;\n\tif (req)\n\t\tmemcpy(req_msg, req, req_len);\n\n\treq_msg->req.hwc_msg_id = msg_id;\n\n\ttx_wr->msg_size = req_len;\n\n\tif (gc->is_pf) {\n\t\tdest_vrq = hwc->pf_dest_vrq_id;\n\t\tdest_vrcq = hwc->pf_dest_vrcq_id;\n\t}\n\n\terr = mana_hwc_post_tx_wqe(txq, tx_wr, dest_vrq, dest_vrcq, false);\n\tif (err) {\n\t\tdev_err(hwc->dev, \"HWC: Failed to post send WQE: %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\tif (!wait_for_completion_timeout(&ctx->comp_event,\n\t\t\t\t\t (msecs_to_jiffies(hwc->hwc_timeout) * HZ))) {\n\t\tdev_err(hwc->dev, \"HWC: Request timed out!\\n\");\n\t\terr = -ETIMEDOUT;\n\t\tgoto out;\n\t}\n\n\tif (ctx->error) {\n\t\terr = ctx->error;\n\t\tgoto out;\n\t}\n\n\tif (ctx->status_code && ctx->status_code != GDMA_STATUS_MORE_ENTRIES) {\n\t\tdev_err(hwc->dev, \"HWC: Failed hw_channel req: 0x%x\\n\",\n\t\t\tctx->status_code);\n\t\terr = -EPROTO;\n\t\tgoto out;\n\t}\nout:\n\tmana_hwc_put_msg_index(hwc, msg_id);\n\treturn err;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}