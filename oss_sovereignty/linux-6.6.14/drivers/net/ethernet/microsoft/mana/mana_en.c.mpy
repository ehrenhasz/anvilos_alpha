{
  "module_name": "mana_en.c",
  "hash_id": "2fc9987d74e94b542e7929995f75759d621ee6ed844776b2696c3712d412003a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/microsoft/mana/mana_en.c",
  "human_readable_source": "\n \n\n#include <uapi/linux/bpf.h>\n\n#include <linux/inetdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/filter.h>\n#include <linux/mm.h>\n#include <linux/pci.h>\n\n#include <net/checksum.h>\n#include <net/ip6_checksum.h>\n#include <net/page_pool/helpers.h>\n#include <net/xdp.h>\n\n#include <net/mana/mana.h>\n#include <net/mana/mana_auxiliary.h>\n\nstatic DEFINE_IDA(mana_adev_ida);\n\nstatic int mana_adev_idx_alloc(void)\n{\n\treturn ida_alloc(&mana_adev_ida, GFP_KERNEL);\n}\n\nstatic void mana_adev_idx_free(int idx)\n{\n\tida_free(&mana_adev_ida, idx);\n}\n\n \n\nstatic int mana_open(struct net_device *ndev)\n{\n\tstruct mana_port_context *apc = netdev_priv(ndev);\n\tint err;\n\n\terr = mana_alloc_queues(ndev);\n\tif (err)\n\t\treturn err;\n\n\tapc->port_is_up = true;\n\n\t \n\tsmp_wmb();\n\n\tnetif_carrier_on(ndev);\n\tnetif_tx_wake_all_queues(ndev);\n\n\treturn 0;\n}\n\nstatic int mana_close(struct net_device *ndev)\n{\n\tstruct mana_port_context *apc = netdev_priv(ndev);\n\n\tif (!apc->port_is_up)\n\t\treturn 0;\n\n\treturn mana_detach(ndev, true);\n}\n\nstatic bool mana_can_tx(struct gdma_queue *wq)\n{\n\treturn mana_gd_wq_avail_space(wq) >= MAX_TX_WQE_SIZE;\n}\n\nstatic unsigned int mana_checksum_info(struct sk_buff *skb)\n{\n\tif (skb->protocol == htons(ETH_P_IP)) {\n\t\tstruct iphdr *ip = ip_hdr(skb);\n\n\t\tif (ip->protocol == IPPROTO_TCP)\n\t\t\treturn IPPROTO_TCP;\n\n\t\tif (ip->protocol == IPPROTO_UDP)\n\t\t\treturn IPPROTO_UDP;\n\t} else if (skb->protocol == htons(ETH_P_IPV6)) {\n\t\tstruct ipv6hdr *ip6 = ipv6_hdr(skb);\n\n\t\tif (ip6->nexthdr == IPPROTO_TCP)\n\t\t\treturn IPPROTO_TCP;\n\n\t\tif (ip6->nexthdr == IPPROTO_UDP)\n\t\t\treturn IPPROTO_UDP;\n\t}\n\n\t \n\treturn 0;\n}\n\nstatic void mana_add_sge(struct mana_tx_package *tp, struct mana_skb_head *ash,\n\t\t\t int sg_i, dma_addr_t da, int sge_len, u32 gpa_mkey)\n{\n\tash->dma_handle[sg_i] = da;\n\tash->size[sg_i] = sge_len;\n\n\ttp->wqe_req.sgl[sg_i].address = da;\n\ttp->wqe_req.sgl[sg_i].mem_key = gpa_mkey;\n\ttp->wqe_req.sgl[sg_i].size = sge_len;\n}\n\nstatic int mana_map_skb(struct sk_buff *skb, struct mana_port_context *apc,\n\t\t\tstruct mana_tx_package *tp, int gso_hs)\n{\n\tstruct mana_skb_head *ash = (struct mana_skb_head *)skb->head;\n\tint hsg = 1;  \n\tstruct gdma_dev *gd = apc->ac->gdma_dev;\n\tint skb_hlen = skb_headlen(skb);\n\tint sge0_len, sge1_len = 0;\n\tstruct gdma_context *gc;\n\tstruct device *dev;\n\tskb_frag_t *frag;\n\tdma_addr_t da;\n\tint sg_i;\n\tint i;\n\n\tgc = gd->gdma_context;\n\tdev = gc->dev;\n\n\tif (gso_hs && gso_hs < skb_hlen) {\n\t\tsge0_len = gso_hs;\n\t\tsge1_len = skb_hlen - gso_hs;\n\t} else {\n\t\tsge0_len = skb_hlen;\n\t}\n\n\tda = dma_map_single(dev, skb->data, sge0_len, DMA_TO_DEVICE);\n\tif (dma_mapping_error(dev, da))\n\t\treturn -ENOMEM;\n\n\tmana_add_sge(tp, ash, 0, da, sge0_len, gd->gpa_mkey);\n\n\tif (sge1_len) {\n\t\tsg_i = 1;\n\t\tda = dma_map_single(dev, skb->data + sge0_len, sge1_len,\n\t\t\t\t    DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dev, da))\n\t\t\tgoto frag_err;\n\n\t\tmana_add_sge(tp, ash, sg_i, da, sge1_len, gd->gpa_mkey);\n\t\thsg = 2;\n\t}\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tsg_i = hsg + i;\n\n\t\tfrag = &skb_shinfo(skb)->frags[i];\n\t\tda = skb_frag_dma_map(dev, frag, 0, skb_frag_size(frag),\n\t\t\t\t      DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(dev, da))\n\t\t\tgoto frag_err;\n\n\t\tmana_add_sge(tp, ash, sg_i, da, skb_frag_size(frag),\n\t\t\t     gd->gpa_mkey);\n\t}\n\n\treturn 0;\n\nfrag_err:\n\tfor (i = sg_i - 1; i >= hsg; i--)\n\t\tdma_unmap_page(dev, ash->dma_handle[i], ash->size[i],\n\t\t\t       DMA_TO_DEVICE);\n\n\tfor (i = hsg - 1; i >= 0; i--)\n\t\tdma_unmap_single(dev, ash->dma_handle[i], ash->size[i],\n\t\t\t\t DMA_TO_DEVICE);\n\n\treturn -ENOMEM;\n}\n\n \nstatic int mana_fix_skb_head(struct net_device *ndev, struct sk_buff *skb,\n\t\t\t     int gso_hs)\n{\n\tint num_sge = 1 + skb_shinfo(skb)->nr_frags;\n\tint skb_hlen = skb_headlen(skb);\n\n\tif (gso_hs < skb_hlen) {\n\t\tnum_sge++;\n\t} else if (gso_hs > skb_hlen) {\n\t\tif (net_ratelimit())\n\t\t\tnetdev_err(ndev,\n\t\t\t\t   \"TX nonlinear head: hs:%d, skb_hlen:%d\\n\",\n\t\t\t\t   gso_hs, skb_hlen);\n\n\t\treturn -EINVAL;\n\t}\n\n\treturn num_sge;\n}\n\n \nstatic int mana_get_gso_hs(struct sk_buff *skb)\n{\n\tint gso_hs;\n\n\tif (skb->encapsulation) {\n\t\tgso_hs = skb_inner_tcp_all_headers(skb);\n\t} else {\n\t\tif (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) {\n\t\t\tgso_hs = skb_transport_offset(skb) +\n\t\t\t\t sizeof(struct udphdr);\n\t\t} else {\n\t\t\tgso_hs = skb_tcp_all_headers(skb);\n\t\t}\n\t}\n\n\treturn gso_hs;\n}\n\nnetdev_tx_t mana_start_xmit(struct sk_buff *skb, struct net_device *ndev)\n{\n\tenum mana_tx_pkt_format pkt_fmt = MANA_SHORT_PKT_FMT;\n\tstruct mana_port_context *apc = netdev_priv(ndev);\n\tint gso_hs = 0;  \n\tu16 txq_idx = skb_get_queue_mapping(skb);\n\tstruct gdma_dev *gd = apc->ac->gdma_dev;\n\tbool ipv4 = false, ipv6 = false;\n\tstruct mana_tx_package pkg = {};\n\tstruct netdev_queue *net_txq;\n\tstruct mana_stats_tx *tx_stats;\n\tstruct gdma_queue *gdma_sq;\n\tunsigned int csum_type;\n\tstruct mana_txq *txq;\n\tstruct mana_cq *cq;\n\tint err, len;\n\n\tif (unlikely(!apc->port_is_up))\n\t\tgoto tx_drop;\n\n\tif (skb_cow_head(skb, MANA_HEADROOM))\n\t\tgoto tx_drop_count;\n\n\ttxq = &apc->tx_qp[txq_idx].txq;\n\tgdma_sq = txq->gdma_sq;\n\tcq = &apc->tx_qp[txq_idx].tx_cq;\n\ttx_stats = &txq->stats;\n\n\tpkg.tx_oob.s_oob.vcq_num = cq->gdma_id;\n\tpkg.tx_oob.s_oob.vsq_frame = txq->vsq_frame;\n\n\tif (txq->vp_offset > MANA_SHORT_VPORT_OFFSET_MAX) {\n\t\tpkg.tx_oob.l_oob.long_vp_offset = txq->vp_offset;\n\t\tpkt_fmt = MANA_LONG_PKT_FMT;\n\t} else {\n\t\tpkg.tx_oob.s_oob.short_vp_offset = txq->vp_offset;\n\t}\n\n\tif (skb_vlan_tag_present(skb)) {\n\t\tpkt_fmt = MANA_LONG_PKT_FMT;\n\t\tpkg.tx_oob.l_oob.inject_vlan_pri_tag = 1;\n\t\tpkg.tx_oob.l_oob.pcp = skb_vlan_tag_get_prio(skb);\n\t\tpkg.tx_oob.l_oob.dei = skb_vlan_tag_get_cfi(skb);\n\t\tpkg.tx_oob.l_oob.vlan_id = skb_vlan_tag_get_id(skb);\n\t}\n\n\tpkg.tx_oob.s_oob.pkt_fmt = pkt_fmt;\n\n\tif (pkt_fmt == MANA_SHORT_PKT_FMT) {\n\t\tpkg.wqe_req.inline_oob_size = sizeof(struct mana_tx_short_oob);\n\t\tu64_stats_update_begin(&tx_stats->syncp);\n\t\ttx_stats->short_pkt_fmt++;\n\t\tu64_stats_update_end(&tx_stats->syncp);\n\t} else {\n\t\tpkg.wqe_req.inline_oob_size = sizeof(struct mana_tx_oob);\n\t\tu64_stats_update_begin(&tx_stats->syncp);\n\t\ttx_stats->long_pkt_fmt++;\n\t\tu64_stats_update_end(&tx_stats->syncp);\n\t}\n\n\tpkg.wqe_req.inline_oob_data = &pkg.tx_oob;\n\tpkg.wqe_req.flags = 0;\n\tpkg.wqe_req.client_data_unit = 0;\n\n\tpkg.wqe_req.num_sge = 1 + skb_shinfo(skb)->nr_frags;\n\n\tif (skb->protocol == htons(ETH_P_IP))\n\t\tipv4 = true;\n\telse if (skb->protocol == htons(ETH_P_IPV6))\n\t\tipv6 = true;\n\n\tif (skb_is_gso(skb)) {\n\t\tint num_sge;\n\n\t\tgso_hs = mana_get_gso_hs(skb);\n\n\t\tnum_sge = mana_fix_skb_head(ndev, skb, gso_hs);\n\t\tif (num_sge > 0)\n\t\t\tpkg.wqe_req.num_sge = num_sge;\n\t\telse\n\t\t\tgoto tx_drop_count;\n\n\t\tu64_stats_update_begin(&tx_stats->syncp);\n\t\tif (skb->encapsulation) {\n\t\t\ttx_stats->tso_inner_packets++;\n\t\t\ttx_stats->tso_inner_bytes += skb->len - gso_hs;\n\t\t} else {\n\t\t\ttx_stats->tso_packets++;\n\t\t\ttx_stats->tso_bytes += skb->len - gso_hs;\n\t\t}\n\t\tu64_stats_update_end(&tx_stats->syncp);\n\n\t\tpkg.tx_oob.s_oob.is_outer_ipv4 = ipv4;\n\t\tpkg.tx_oob.s_oob.is_outer_ipv6 = ipv6;\n\n\t\tpkg.tx_oob.s_oob.comp_iphdr_csum = 1;\n\t\tpkg.tx_oob.s_oob.comp_tcp_csum = 1;\n\t\tpkg.tx_oob.s_oob.trans_off = skb_transport_offset(skb);\n\n\t\tpkg.wqe_req.client_data_unit = skb_shinfo(skb)->gso_size;\n\t\tpkg.wqe_req.flags = GDMA_WR_OOB_IN_SGL | GDMA_WR_PAD_BY_SGE0;\n\t\tif (ipv4) {\n\t\t\tip_hdr(skb)->tot_len = 0;\n\t\t\tip_hdr(skb)->check = 0;\n\t\t\ttcp_hdr(skb)->check =\n\t\t\t\t~csum_tcpudp_magic(ip_hdr(skb)->saddr,\n\t\t\t\t\t\t   ip_hdr(skb)->daddr, 0,\n\t\t\t\t\t\t   IPPROTO_TCP, 0);\n\t\t} else {\n\t\t\tipv6_hdr(skb)->payload_len = 0;\n\t\t\ttcp_hdr(skb)->check =\n\t\t\t\t~csum_ipv6_magic(&ipv6_hdr(skb)->saddr,\n\t\t\t\t\t\t &ipv6_hdr(skb)->daddr, 0,\n\t\t\t\t\t\t IPPROTO_TCP, 0);\n\t\t}\n\t} else if (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tcsum_type = mana_checksum_info(skb);\n\n\t\tu64_stats_update_begin(&tx_stats->syncp);\n\t\ttx_stats->csum_partial++;\n\t\tu64_stats_update_end(&tx_stats->syncp);\n\n\t\tif (csum_type == IPPROTO_TCP) {\n\t\t\tpkg.tx_oob.s_oob.is_outer_ipv4 = ipv4;\n\t\t\tpkg.tx_oob.s_oob.is_outer_ipv6 = ipv6;\n\n\t\t\tpkg.tx_oob.s_oob.comp_tcp_csum = 1;\n\t\t\tpkg.tx_oob.s_oob.trans_off = skb_transport_offset(skb);\n\n\t\t} else if (csum_type == IPPROTO_UDP) {\n\t\t\tpkg.tx_oob.s_oob.is_outer_ipv4 = ipv4;\n\t\t\tpkg.tx_oob.s_oob.is_outer_ipv6 = ipv6;\n\n\t\t\tpkg.tx_oob.s_oob.comp_udp_csum = 1;\n\t\t} else {\n\t\t\t \n\t\t\tif (skb_checksum_help(skb))\n\t\t\t\tgoto tx_drop_count;\n\t\t}\n\t}\n\n\tWARN_ON_ONCE(pkg.wqe_req.num_sge > MAX_TX_WQE_SGL_ENTRIES);\n\n\tif (pkg.wqe_req.num_sge <= ARRAY_SIZE(pkg.sgl_array)) {\n\t\tpkg.wqe_req.sgl = pkg.sgl_array;\n\t} else {\n\t\tpkg.sgl_ptr = kmalloc_array(pkg.wqe_req.num_sge,\n\t\t\t\t\t    sizeof(struct gdma_sge),\n\t\t\t\t\t    GFP_ATOMIC);\n\t\tif (!pkg.sgl_ptr)\n\t\t\tgoto tx_drop_count;\n\n\t\tpkg.wqe_req.sgl = pkg.sgl_ptr;\n\t}\n\n\tif (mana_map_skb(skb, apc, &pkg, gso_hs)) {\n\t\tu64_stats_update_begin(&tx_stats->syncp);\n\t\ttx_stats->mana_map_err++;\n\t\tu64_stats_update_end(&tx_stats->syncp);\n\t\tgoto free_sgl_ptr;\n\t}\n\n\tskb_queue_tail(&txq->pending_skbs, skb);\n\n\tlen = skb->len;\n\tnet_txq = netdev_get_tx_queue(ndev, txq_idx);\n\n\terr = mana_gd_post_work_request(gdma_sq, &pkg.wqe_req,\n\t\t\t\t\t(struct gdma_posted_wqe_info *)skb->cb);\n\tif (!mana_can_tx(gdma_sq)) {\n\t\tnetif_tx_stop_queue(net_txq);\n\t\tapc->eth_stats.stop_queue++;\n\t}\n\n\tif (err) {\n\t\t(void)skb_dequeue_tail(&txq->pending_skbs);\n\t\tnetdev_warn(ndev, \"Failed to post TX OOB: %d\\n\", err);\n\t\terr = NETDEV_TX_BUSY;\n\t\tgoto tx_busy;\n\t}\n\n\terr = NETDEV_TX_OK;\n\tatomic_inc(&txq->pending_sends);\n\n\tmana_gd_wq_ring_doorbell(gd->gdma_context, gdma_sq);\n\n\t \n\tskb = NULL;\n\n\ttx_stats = &txq->stats;\n\tu64_stats_update_begin(&tx_stats->syncp);\n\ttx_stats->packets++;\n\ttx_stats->bytes += len;\n\tu64_stats_update_end(&tx_stats->syncp);\n\ntx_busy:\n\tif (netif_tx_queue_stopped(net_txq) && mana_can_tx(gdma_sq)) {\n\t\tnetif_tx_wake_queue(net_txq);\n\t\tapc->eth_stats.wake_queue++;\n\t}\n\n\tkfree(pkg.sgl_ptr);\n\treturn err;\n\nfree_sgl_ptr:\n\tkfree(pkg.sgl_ptr);\ntx_drop_count:\n\tndev->stats.tx_dropped++;\ntx_drop:\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\nstatic void mana_get_stats64(struct net_device *ndev,\n\t\t\t     struct rtnl_link_stats64 *st)\n{\n\tstruct mana_port_context *apc = netdev_priv(ndev);\n\tunsigned int num_queues = apc->num_queues;\n\tstruct mana_stats_rx *rx_stats;\n\tstruct mana_stats_tx *tx_stats;\n\tunsigned int start;\n\tu64 packets, bytes;\n\tint q;\n\n\tif (!apc->port_is_up)\n\t\treturn;\n\n\tnetdev_stats_to_stats64(st, &ndev->stats);\n\n\tfor (q = 0; q < num_queues; q++) {\n\t\trx_stats = &apc->rxqs[q]->stats;\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&rx_stats->syncp);\n\t\t\tpackets = rx_stats->packets;\n\t\t\tbytes = rx_stats->bytes;\n\t\t} while (u64_stats_fetch_retry(&rx_stats->syncp, start));\n\n\t\tst->rx_packets += packets;\n\t\tst->rx_bytes += bytes;\n\t}\n\n\tfor (q = 0; q < num_queues; q++) {\n\t\ttx_stats = &apc->tx_qp[q].txq.stats;\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&tx_stats->syncp);\n\t\t\tpackets = tx_stats->packets;\n\t\t\tbytes = tx_stats->bytes;\n\t\t} while (u64_stats_fetch_retry(&tx_stats->syncp, start));\n\n\t\tst->tx_packets += packets;\n\t\tst->tx_bytes += bytes;\n\t}\n}\n\nstatic int mana_get_tx_queue(struct net_device *ndev, struct sk_buff *skb,\n\t\t\t     int old_q)\n{\n\tstruct mana_port_context *apc = netdev_priv(ndev);\n\tu32 hash = skb_get_hash(skb);\n\tstruct sock *sk = skb->sk;\n\tint txq;\n\n\ttxq = apc->indir_table[hash & MANA_INDIRECT_TABLE_MASK];\n\n\tif (txq != old_q && sk && sk_fullsock(sk) &&\n\t    rcu_access_pointer(sk->sk_dst_cache))\n\t\tsk_tx_queue_set(sk, txq);\n\n\treturn txq;\n}\n\nstatic u16 mana_select_queue(struct net_device *ndev, struct sk_buff *skb,\n\t\t\t     struct net_device *sb_dev)\n{\n\tint txq;\n\n\tif (ndev->real_num_tx_queues == 1)\n\t\treturn 0;\n\n\ttxq = sk_tx_queue_get(skb->sk);\n\n\tif (txq < 0 || skb->ooo_okay || txq >= ndev->real_num_tx_queues) {\n\t\tif (skb_rx_queue_recorded(skb))\n\t\t\ttxq = skb_get_rx_queue(skb);\n\t\telse\n\t\t\ttxq = mana_get_tx_queue(ndev, skb, txq);\n\t}\n\n\treturn txq;\n}\n\n \nstatic void mana_pre_dealloc_rxbufs(struct mana_port_context *mpc)\n{\n\tstruct device *dev;\n\tint i;\n\n\tdev = mpc->ac->gdma_dev->gdma_context->dev;\n\n\tif (!mpc->rxbufs_pre)\n\t\tgoto out1;\n\n\tif (!mpc->das_pre)\n\t\tgoto out2;\n\n\twhile (mpc->rxbpre_total) {\n\t\ti = --mpc->rxbpre_total;\n\t\tdma_unmap_single(dev, mpc->das_pre[i], mpc->rxbpre_datasize,\n\t\t\t\t DMA_FROM_DEVICE);\n\t\tput_page(virt_to_head_page(mpc->rxbufs_pre[i]));\n\t}\n\n\tkfree(mpc->das_pre);\n\tmpc->das_pre = NULL;\n\nout2:\n\tkfree(mpc->rxbufs_pre);\n\tmpc->rxbufs_pre = NULL;\n\nout1:\n\tmpc->rxbpre_datasize = 0;\n\tmpc->rxbpre_alloc_size = 0;\n\tmpc->rxbpre_headroom = 0;\n}\n\n \nstatic void *mana_get_rxbuf_pre(struct mana_rxq *rxq, dma_addr_t *da)\n{\n\tstruct net_device *ndev = rxq->ndev;\n\tstruct mana_port_context *mpc;\n\tvoid *va;\n\n\tmpc = netdev_priv(ndev);\n\n\tif (!mpc->rxbufs_pre || !mpc->das_pre || !mpc->rxbpre_total) {\n\t\tnetdev_err(ndev, \"No RX pre-allocated bufs\\n\");\n\t\treturn NULL;\n\t}\n\n\t \n\tif (mpc->rxbpre_datasize != rxq->datasize) {\n\t\tnetdev_err(ndev, \"rxbpre_datasize mismatch: %u: %u\\n\",\n\t\t\t   mpc->rxbpre_datasize, rxq->datasize);\n\t\treturn NULL;\n\t}\n\n\tif (mpc->rxbpre_alloc_size != rxq->alloc_size) {\n\t\tnetdev_err(ndev, \"rxbpre_alloc_size mismatch: %u: %u\\n\",\n\t\t\t   mpc->rxbpre_alloc_size, rxq->alloc_size);\n\t\treturn NULL;\n\t}\n\n\tif (mpc->rxbpre_headroom != rxq->headroom) {\n\t\tnetdev_err(ndev, \"rxbpre_headroom mismatch: %u: %u\\n\",\n\t\t\t   mpc->rxbpre_headroom, rxq->headroom);\n\t\treturn NULL;\n\t}\n\n\tmpc->rxbpre_total--;\n\n\t*da = mpc->das_pre[mpc->rxbpre_total];\n\tva = mpc->rxbufs_pre[mpc->rxbpre_total];\n\tmpc->rxbufs_pre[mpc->rxbpre_total] = NULL;\n\n\t \n\tif (!mpc->rxbpre_total)\n\t\tmana_pre_dealloc_rxbufs(mpc);\n\n\treturn va;\n}\n\n \nstatic void mana_get_rxbuf_cfg(int mtu, u32 *datasize, u32 *alloc_size,\n\t\t\t       u32 *headroom)\n{\n\tif (mtu > MANA_XDP_MTU_MAX)\n\t\t*headroom = 0;  \n\telse\n\t\t*headroom = XDP_PACKET_HEADROOM;\n\n\t*alloc_size = mtu + MANA_RXBUF_PAD + *headroom;\n\n\t*datasize = ALIGN(mtu + ETH_HLEN, MANA_RX_DATA_ALIGN);\n}\n\nstatic int mana_pre_alloc_rxbufs(struct mana_port_context *mpc, int new_mtu)\n{\n\tstruct device *dev;\n\tstruct page *page;\n\tdma_addr_t da;\n\tint num_rxb;\n\tvoid *va;\n\tint i;\n\n\tmana_get_rxbuf_cfg(new_mtu, &mpc->rxbpre_datasize,\n\t\t\t   &mpc->rxbpre_alloc_size, &mpc->rxbpre_headroom);\n\n\tdev = mpc->ac->gdma_dev->gdma_context->dev;\n\n\tnum_rxb = mpc->num_queues * RX_BUFFERS_PER_QUEUE;\n\n\tWARN(mpc->rxbufs_pre, \"mana rxbufs_pre exists\\n\");\n\tmpc->rxbufs_pre = kmalloc_array(num_rxb, sizeof(void *), GFP_KERNEL);\n\tif (!mpc->rxbufs_pre)\n\t\tgoto error;\n\n\tmpc->das_pre = kmalloc_array(num_rxb, sizeof(dma_addr_t), GFP_KERNEL);\n\tif (!mpc->das_pre)\n\t\tgoto error;\n\n\tmpc->rxbpre_total = 0;\n\n\tfor (i = 0; i < num_rxb; i++) {\n\t\tif (mpc->rxbpre_alloc_size > PAGE_SIZE) {\n\t\t\tva = netdev_alloc_frag(mpc->rxbpre_alloc_size);\n\t\t\tif (!va)\n\t\t\t\tgoto error;\n\n\t\t\tpage = virt_to_head_page(va);\n\t\t\t \n\t\t\tif (compound_order(page) <\n\t\t\t    get_order(mpc->rxbpre_alloc_size)) {\n\t\t\t\tput_page(page);\n\t\t\t\tgoto error;\n\t\t\t}\n\t\t} else {\n\t\t\tpage = dev_alloc_page();\n\t\t\tif (!page)\n\t\t\t\tgoto error;\n\n\t\t\tva = page_to_virt(page);\n\t\t}\n\n\t\tda = dma_map_single(dev, va + mpc->rxbpre_headroom,\n\t\t\t\t    mpc->rxbpre_datasize, DMA_FROM_DEVICE);\n\t\tif (dma_mapping_error(dev, da)) {\n\t\t\tput_page(virt_to_head_page(va));\n\t\t\tgoto error;\n\t\t}\n\n\t\tmpc->rxbufs_pre[i] = va;\n\t\tmpc->das_pre[i] = da;\n\t\tmpc->rxbpre_total = i + 1;\n\t}\n\n\treturn 0;\n\nerror:\n\tmana_pre_dealloc_rxbufs(mpc);\n\treturn -ENOMEM;\n}\n\nstatic int mana_change_mtu(struct net_device *ndev, int new_mtu)\n{\n\tstruct mana_port_context *mpc = netdev_priv(ndev);\n\tunsigned int old_mtu = ndev->mtu;\n\tint err;\n\n\t \n\terr = mana_pre_alloc_rxbufs(mpc, new_mtu);\n\tif (err) {\n\t\tnetdev_err(ndev, \"Insufficient memory for new MTU\\n\");\n\t\treturn err;\n\t}\n\n\terr = mana_detach(ndev, false);\n\tif (err) {\n\t\tnetdev_err(ndev, \"mana_detach failed: %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\tndev->mtu = new_mtu;\n\n\terr = mana_attach(ndev);\n\tif (err) {\n\t\tnetdev_err(ndev, \"mana_attach failed: %d\\n\", err);\n\t\tndev->mtu = old_mtu;\n\t}\n\nout:\n\tmana_pre_dealloc_rxbufs(mpc);\n\treturn err;\n}\n\nstatic const struct net_device_ops mana_devops = {\n\t.ndo_open\t\t= mana_open,\n\t.ndo_stop\t\t= mana_close,\n\t.ndo_select_queue\t= mana_select_queue,\n\t.ndo_start_xmit\t\t= mana_start_xmit,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_get_stats64\t= mana_get_stats64,\n\t.ndo_bpf\t\t= mana_bpf,\n\t.ndo_xdp_xmit\t\t= mana_xdp_xmit,\n\t.ndo_change_mtu\t\t= mana_change_mtu,\n};\n\nstatic void mana_cleanup_port_context(struct mana_port_context *apc)\n{\n\tkfree(apc->rxqs);\n\tapc->rxqs = NULL;\n}\n\nstatic int mana_init_port_context(struct mana_port_context *apc)\n{\n\tapc->rxqs = kcalloc(apc->num_queues, sizeof(struct mana_rxq *),\n\t\t\t    GFP_KERNEL);\n\n\treturn !apc->rxqs ? -ENOMEM : 0;\n}\n\nstatic int mana_send_request(struct mana_context *ac, void *in_buf,\n\t\t\t     u32 in_len, void *out_buf, u32 out_len)\n{\n\tstruct gdma_context *gc = ac->gdma_dev->gdma_context;\n\tstruct gdma_resp_hdr *resp = out_buf;\n\tstruct gdma_req_hdr *req = in_buf;\n\tstruct device *dev = gc->dev;\n\tstatic atomic_t activity_id;\n\tint err;\n\n\treq->dev_id = gc->mana.dev_id;\n\treq->activity_id = atomic_inc_return(&activity_id);\n\n\terr = mana_gd_send_request(gc, in_len, in_buf, out_len,\n\t\t\t\t   out_buf);\n\tif (err || resp->status) {\n\t\tdev_err(dev, \"Failed to send mana message: %d, 0x%x\\n\",\n\t\t\terr, resp->status);\n\t\treturn err ? err : -EPROTO;\n\t}\n\n\tif (req->dev_id.as_uint32 != resp->dev_id.as_uint32 ||\n\t    req->activity_id != resp->activity_id) {\n\t\tdev_err(dev, \"Unexpected mana message response: %x,%x,%x,%x\\n\",\n\t\t\treq->dev_id.as_uint32, resp->dev_id.as_uint32,\n\t\t\treq->activity_id, resp->activity_id);\n\t\treturn -EPROTO;\n\t}\n\n\treturn 0;\n}\n\nstatic int mana_verify_resp_hdr(const struct gdma_resp_hdr *resp_hdr,\n\t\t\t\tconst enum mana_command_code expected_code,\n\t\t\t\tconst u32 min_size)\n{\n\tif (resp_hdr->response.msg_type != expected_code)\n\t\treturn -EPROTO;\n\n\tif (resp_hdr->response.msg_version < GDMA_MESSAGE_V1)\n\t\treturn -EPROTO;\n\n\tif (resp_hdr->response.msg_size < min_size)\n\t\treturn -EPROTO;\n\n\treturn 0;\n}\n\nstatic int mana_pf_register_hw_vport(struct mana_port_context *apc)\n{\n\tstruct mana_register_hw_vport_resp resp = {};\n\tstruct mana_register_hw_vport_req req = {};\n\tint err;\n\n\tmana_gd_init_req_hdr(&req.hdr, MANA_REGISTER_HW_PORT,\n\t\t\t     sizeof(req), sizeof(resp));\n\treq.attached_gfid = 1;\n\treq.is_pf_default_vport = 1;\n\treq.allow_all_ether_types = 1;\n\n\terr = mana_send_request(apc->ac, &req, sizeof(req), &resp,\n\t\t\t\tsizeof(resp));\n\tif (err) {\n\t\tnetdev_err(apc->ndev, \"Failed to register hw vPort: %d\\n\", err);\n\t\treturn err;\n\t}\n\n\terr = mana_verify_resp_hdr(&resp.hdr, MANA_REGISTER_HW_PORT,\n\t\t\t\t   sizeof(resp));\n\tif (err || resp.hdr.status) {\n\t\tnetdev_err(apc->ndev, \"Failed to register hw vPort: %d, 0x%x\\n\",\n\t\t\t   err, resp.hdr.status);\n\t\treturn err ? err : -EPROTO;\n\t}\n\n\tapc->port_handle = resp.hw_vport_handle;\n\treturn 0;\n}\n\nstatic void mana_pf_deregister_hw_vport(struct mana_port_context *apc)\n{\n\tstruct mana_deregister_hw_vport_resp resp = {};\n\tstruct mana_deregister_hw_vport_req req = {};\n\tint err;\n\n\tmana_gd_init_req_hdr(&req.hdr, MANA_DEREGISTER_HW_PORT,\n\t\t\t     sizeof(req), sizeof(resp));\n\treq.hw_vport_handle = apc->port_handle;\n\n\terr = mana_send_request(apc->ac, &req, sizeof(req), &resp,\n\t\t\t\tsizeof(resp));\n\tif (err) {\n\t\tnetdev_err(apc->ndev, \"Failed to unregister hw vPort: %d\\n\",\n\t\t\t   err);\n\t\treturn;\n\t}\n\n\terr = mana_verify_resp_hdr(&resp.hdr, MANA_DEREGISTER_HW_PORT,\n\t\t\t\t   sizeof(resp));\n\tif (err || resp.hdr.status)\n\t\tnetdev_err(apc->ndev,\n\t\t\t   \"Failed to deregister hw vPort: %d, 0x%x\\n\",\n\t\t\t   err, resp.hdr.status);\n}\n\nstatic int mana_pf_register_filter(struct mana_port_context *apc)\n{\n\tstruct mana_register_filter_resp resp = {};\n\tstruct mana_register_filter_req req = {};\n\tint err;\n\n\tmana_gd_init_req_hdr(&req.hdr, MANA_REGISTER_FILTER,\n\t\t\t     sizeof(req), sizeof(resp));\n\treq.vport = apc->port_handle;\n\tmemcpy(req.mac_addr, apc->mac_addr, ETH_ALEN);\n\n\terr = mana_send_request(apc->ac, &req, sizeof(req), &resp,\n\t\t\t\tsizeof(resp));\n\tif (err) {\n\t\tnetdev_err(apc->ndev, \"Failed to register filter: %d\\n\", err);\n\t\treturn err;\n\t}\n\n\terr = mana_verify_resp_hdr(&resp.hdr, MANA_REGISTER_FILTER,\n\t\t\t\t   sizeof(resp));\n\tif (err || resp.hdr.status) {\n\t\tnetdev_err(apc->ndev, \"Failed to register filter: %d, 0x%x\\n\",\n\t\t\t   err, resp.hdr.status);\n\t\treturn err ? err : -EPROTO;\n\t}\n\n\tapc->pf_filter_handle = resp.filter_handle;\n\treturn 0;\n}\n\nstatic void mana_pf_deregister_filter(struct mana_port_context *apc)\n{\n\tstruct mana_deregister_filter_resp resp = {};\n\tstruct mana_deregister_filter_req req = {};\n\tint err;\n\n\tmana_gd_init_req_hdr(&req.hdr, MANA_DEREGISTER_FILTER,\n\t\t\t     sizeof(req), sizeof(resp));\n\treq.filter_handle = apc->pf_filter_handle;\n\n\terr = mana_send_request(apc->ac, &req, sizeof(req), &resp,\n\t\t\t\tsizeof(resp));\n\tif (err) {\n\t\tnetdev_err(apc->ndev, \"Failed to unregister filter: %d\\n\",\n\t\t\t   err);\n\t\treturn;\n\t}\n\n\terr = mana_verify_resp_hdr(&resp.hdr, MANA_DEREGISTER_FILTER,\n\t\t\t\t   sizeof(resp));\n\tif (err || resp.hdr.status)\n\t\tnetdev_err(apc->ndev,\n\t\t\t   \"Failed to deregister filter: %d, 0x%x\\n\",\n\t\t\t   err, resp.hdr.status);\n}\n\nstatic int mana_query_device_cfg(struct mana_context *ac, u32 proto_major_ver,\n\t\t\t\t u32 proto_minor_ver, u32 proto_micro_ver,\n\t\t\t\t u16 *max_num_vports)\n{\n\tstruct gdma_context *gc = ac->gdma_dev->gdma_context;\n\tstruct mana_query_device_cfg_resp resp = {};\n\tstruct mana_query_device_cfg_req req = {};\n\tstruct device *dev = gc->dev;\n\tint err = 0;\n\n\tmana_gd_init_req_hdr(&req.hdr, MANA_QUERY_DEV_CONFIG,\n\t\t\t     sizeof(req), sizeof(resp));\n\n\treq.hdr.resp.msg_version = GDMA_MESSAGE_V2;\n\n\treq.proto_major_ver = proto_major_ver;\n\treq.proto_minor_ver = proto_minor_ver;\n\treq.proto_micro_ver = proto_micro_ver;\n\n\terr = mana_send_request(ac, &req, sizeof(req), &resp, sizeof(resp));\n\tif (err) {\n\t\tdev_err(dev, \"Failed to query config: %d\", err);\n\t\treturn err;\n\t}\n\n\terr = mana_verify_resp_hdr(&resp.hdr, MANA_QUERY_DEV_CONFIG,\n\t\t\t\t   sizeof(resp));\n\tif (err || resp.hdr.status) {\n\t\tdev_err(dev, \"Invalid query result: %d, 0x%x\\n\", err,\n\t\t\tresp.hdr.status);\n\t\tif (!err)\n\t\t\terr = -EPROTO;\n\t\treturn err;\n\t}\n\n\t*max_num_vports = resp.max_num_vports;\n\n\tif (resp.hdr.response.msg_version == GDMA_MESSAGE_V2)\n\t\tgc->adapter_mtu = resp.adapter_mtu;\n\telse\n\t\tgc->adapter_mtu = ETH_FRAME_LEN;\n\n\treturn 0;\n}\n\nstatic int mana_query_vport_cfg(struct mana_port_context *apc, u32 vport_index,\n\t\t\t\tu32 *max_sq, u32 *max_rq, u32 *num_indir_entry)\n{\n\tstruct mana_query_vport_cfg_resp resp = {};\n\tstruct mana_query_vport_cfg_req req = {};\n\tint err;\n\n\tmana_gd_init_req_hdr(&req.hdr, MANA_QUERY_VPORT_CONFIG,\n\t\t\t     sizeof(req), sizeof(resp));\n\n\treq.vport_index = vport_index;\n\n\terr = mana_send_request(apc->ac, &req, sizeof(req), &resp,\n\t\t\t\tsizeof(resp));\n\tif (err)\n\t\treturn err;\n\n\terr = mana_verify_resp_hdr(&resp.hdr, MANA_QUERY_VPORT_CONFIG,\n\t\t\t\t   sizeof(resp));\n\tif (err)\n\t\treturn err;\n\n\tif (resp.hdr.status)\n\t\treturn -EPROTO;\n\n\t*max_sq = resp.max_num_sq;\n\t*max_rq = resp.max_num_rq;\n\t*num_indir_entry = resp.num_indirection_ent;\n\n\tapc->port_handle = resp.vport;\n\tether_addr_copy(apc->mac_addr, resp.mac_addr);\n\n\treturn 0;\n}\n\nvoid mana_uncfg_vport(struct mana_port_context *apc)\n{\n\tmutex_lock(&apc->vport_mutex);\n\tapc->vport_use_count--;\n\tWARN_ON(apc->vport_use_count < 0);\n\tmutex_unlock(&apc->vport_mutex);\n}\nEXPORT_SYMBOL_NS(mana_uncfg_vport, NET_MANA);\n\nint mana_cfg_vport(struct mana_port_context *apc, u32 protection_dom_id,\n\t\t   u32 doorbell_pg_id)\n{\n\tstruct mana_config_vport_resp resp = {};\n\tstruct mana_config_vport_req req = {};\n\tint err;\n\n\t \n\tmutex_lock(&apc->vport_mutex);\n\tif (apc->vport_use_count > 0) {\n\t\tmutex_unlock(&apc->vport_mutex);\n\t\treturn -EBUSY;\n\t}\n\tapc->vport_use_count++;\n\tmutex_unlock(&apc->vport_mutex);\n\n\tmana_gd_init_req_hdr(&req.hdr, MANA_CONFIG_VPORT_TX,\n\t\t\t     sizeof(req), sizeof(resp));\n\treq.vport = apc->port_handle;\n\treq.pdid = protection_dom_id;\n\treq.doorbell_pageid = doorbell_pg_id;\n\n\terr = mana_send_request(apc->ac, &req, sizeof(req), &resp,\n\t\t\t\tsizeof(resp));\n\tif (err) {\n\t\tnetdev_err(apc->ndev, \"Failed to configure vPort: %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\terr = mana_verify_resp_hdr(&resp.hdr, MANA_CONFIG_VPORT_TX,\n\t\t\t\t   sizeof(resp));\n\tif (err || resp.hdr.status) {\n\t\tnetdev_err(apc->ndev, \"Failed to configure vPort: %d, 0x%x\\n\",\n\t\t\t   err, resp.hdr.status);\n\t\tif (!err)\n\t\t\terr = -EPROTO;\n\n\t\tgoto out;\n\t}\n\n\tapc->tx_shortform_allowed = resp.short_form_allowed;\n\tapc->tx_vp_offset = resp.tx_vport_offset;\n\n\tnetdev_info(apc->ndev, \"Configured vPort %llu PD %u DB %u\\n\",\n\t\t    apc->port_handle, protection_dom_id, doorbell_pg_id);\nout:\n\tif (err)\n\t\tmana_uncfg_vport(apc);\n\n\treturn err;\n}\nEXPORT_SYMBOL_NS(mana_cfg_vport, NET_MANA);\n\nstatic int mana_cfg_vport_steering(struct mana_port_context *apc,\n\t\t\t\t   enum TRI_STATE rx,\n\t\t\t\t   bool update_default_rxobj, bool update_key,\n\t\t\t\t   bool update_tab)\n{\n\tu16 num_entries = MANA_INDIRECT_TABLE_SIZE;\n\tstruct mana_cfg_rx_steer_req_v2 *req;\n\tstruct mana_cfg_rx_steer_resp resp = {};\n\tstruct net_device *ndev = apc->ndev;\n\tmana_handle_t *req_indir_tab;\n\tu32 req_buf_size;\n\tint err;\n\n\treq_buf_size = sizeof(*req) + sizeof(mana_handle_t) * num_entries;\n\treq = kzalloc(req_buf_size, GFP_KERNEL);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\tmana_gd_init_req_hdr(&req->hdr, MANA_CONFIG_VPORT_RX, req_buf_size,\n\t\t\t     sizeof(resp));\n\n\treq->hdr.req.msg_version = GDMA_MESSAGE_V2;\n\n\treq->vport = apc->port_handle;\n\treq->num_indir_entries = num_entries;\n\treq->indir_tab_offset = sizeof(*req);\n\treq->rx_enable = rx;\n\treq->rss_enable = apc->rss_state;\n\treq->update_default_rxobj = update_default_rxobj;\n\treq->update_hashkey = update_key;\n\treq->update_indir_tab = update_tab;\n\treq->default_rxobj = apc->default_rxobj;\n\treq->cqe_coalescing_enable = 0;\n\n\tif (update_key)\n\t\tmemcpy(&req->hashkey, apc->hashkey, MANA_HASH_KEY_SIZE);\n\n\tif (update_tab) {\n\t\treq_indir_tab = (mana_handle_t *)(req + 1);\n\t\tmemcpy(req_indir_tab, apc->rxobj_table,\n\t\t       req->num_indir_entries * sizeof(mana_handle_t));\n\t}\n\n\terr = mana_send_request(apc->ac, req, req_buf_size, &resp,\n\t\t\t\tsizeof(resp));\n\tif (err) {\n\t\tnetdev_err(ndev, \"Failed to configure vPort RX: %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\terr = mana_verify_resp_hdr(&resp.hdr, MANA_CONFIG_VPORT_RX,\n\t\t\t\t   sizeof(resp));\n\tif (err) {\n\t\tnetdev_err(ndev, \"vPort RX configuration failed: %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\tif (resp.hdr.status) {\n\t\tnetdev_err(ndev, \"vPort RX configuration failed: 0x%x\\n\",\n\t\t\t   resp.hdr.status);\n\t\terr = -EPROTO;\n\t}\n\n\tnetdev_info(ndev, \"Configured steering vPort %llu entries %u\\n\",\n\t\t    apc->port_handle, num_entries);\nout:\n\tkfree(req);\n\treturn err;\n}\n\nint mana_create_wq_obj(struct mana_port_context *apc,\n\t\t       mana_handle_t vport,\n\t\t       u32 wq_type, struct mana_obj_spec *wq_spec,\n\t\t       struct mana_obj_spec *cq_spec,\n\t\t       mana_handle_t *wq_obj)\n{\n\tstruct mana_create_wqobj_resp resp = {};\n\tstruct mana_create_wqobj_req req = {};\n\tstruct net_device *ndev = apc->ndev;\n\tint err;\n\n\tmana_gd_init_req_hdr(&req.hdr, MANA_CREATE_WQ_OBJ,\n\t\t\t     sizeof(req), sizeof(resp));\n\treq.vport = vport;\n\treq.wq_type = wq_type;\n\treq.wq_gdma_region = wq_spec->gdma_region;\n\treq.cq_gdma_region = cq_spec->gdma_region;\n\treq.wq_size = wq_spec->queue_size;\n\treq.cq_size = cq_spec->queue_size;\n\treq.cq_moderation_ctx_id = cq_spec->modr_ctx_id;\n\treq.cq_parent_qid = cq_spec->attached_eq;\n\n\terr = mana_send_request(apc->ac, &req, sizeof(req), &resp,\n\t\t\t\tsizeof(resp));\n\tif (err) {\n\t\tnetdev_err(ndev, \"Failed to create WQ object: %d\\n\", err);\n\t\tgoto out;\n\t}\n\n\terr = mana_verify_resp_hdr(&resp.hdr, MANA_CREATE_WQ_OBJ,\n\t\t\t\t   sizeof(resp));\n\tif (err || resp.hdr.status) {\n\t\tnetdev_err(ndev, \"Failed to create WQ object: %d, 0x%x\\n\", err,\n\t\t\t   resp.hdr.status);\n\t\tif (!err)\n\t\t\terr = -EPROTO;\n\t\tgoto out;\n\t}\n\n\tif (resp.wq_obj == INVALID_MANA_HANDLE) {\n\t\tnetdev_err(ndev, \"Got an invalid WQ object handle\\n\");\n\t\terr = -EPROTO;\n\t\tgoto out;\n\t}\n\n\t*wq_obj = resp.wq_obj;\n\twq_spec->queue_index = resp.wq_id;\n\tcq_spec->queue_index = resp.cq_id;\n\n\treturn 0;\nout:\n\treturn err;\n}\nEXPORT_SYMBOL_NS(mana_create_wq_obj, NET_MANA);\n\nvoid mana_destroy_wq_obj(struct mana_port_context *apc, u32 wq_type,\n\t\t\t mana_handle_t wq_obj)\n{\n\tstruct mana_destroy_wqobj_resp resp = {};\n\tstruct mana_destroy_wqobj_req req = {};\n\tstruct net_device *ndev = apc->ndev;\n\tint err;\n\n\tmana_gd_init_req_hdr(&req.hdr, MANA_DESTROY_WQ_OBJ,\n\t\t\t     sizeof(req), sizeof(resp));\n\treq.wq_type = wq_type;\n\treq.wq_obj_handle = wq_obj;\n\n\terr = mana_send_request(apc->ac, &req, sizeof(req), &resp,\n\t\t\t\tsizeof(resp));\n\tif (err) {\n\t\tnetdev_err(ndev, \"Failed to destroy WQ object: %d\\n\", err);\n\t\treturn;\n\t}\n\n\terr = mana_verify_resp_hdr(&resp.hdr, MANA_DESTROY_WQ_OBJ,\n\t\t\t\t   sizeof(resp));\n\tif (err || resp.hdr.status)\n\t\tnetdev_err(ndev, \"Failed to destroy WQ object: %d, 0x%x\\n\", err,\n\t\t\t   resp.hdr.status);\n}\nEXPORT_SYMBOL_NS(mana_destroy_wq_obj, NET_MANA);\n\nstatic void mana_destroy_eq(struct mana_context *ac)\n{\n\tstruct gdma_context *gc = ac->gdma_dev->gdma_context;\n\tstruct gdma_queue *eq;\n\tint i;\n\n\tif (!ac->eqs)\n\t\treturn;\n\n\tfor (i = 0; i < gc->max_num_queues; i++) {\n\t\teq = ac->eqs[i].eq;\n\t\tif (!eq)\n\t\t\tcontinue;\n\n\t\tmana_gd_destroy_queue(gc, eq);\n\t}\n\n\tkfree(ac->eqs);\n\tac->eqs = NULL;\n}\n\nstatic int mana_create_eq(struct mana_context *ac)\n{\n\tstruct gdma_dev *gd = ac->gdma_dev;\n\tstruct gdma_context *gc = gd->gdma_context;\n\tstruct gdma_queue_spec spec = {};\n\tint err;\n\tint i;\n\n\tac->eqs = kcalloc(gc->max_num_queues, sizeof(struct mana_eq),\n\t\t\t  GFP_KERNEL);\n\tif (!ac->eqs)\n\t\treturn -ENOMEM;\n\n\tspec.type = GDMA_EQ;\n\tspec.monitor_avl_buf = false;\n\tspec.queue_size = EQ_SIZE;\n\tspec.eq.callback = NULL;\n\tspec.eq.context = ac->eqs;\n\tspec.eq.log2_throttle_limit = LOG2_EQ_THROTTLE;\n\n\tfor (i = 0; i < gc->max_num_queues; i++) {\n\t\terr = mana_gd_create_mana_eq(gd, &spec, &ac->eqs[i].eq);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\treturn 0;\nout:\n\tmana_destroy_eq(ac);\n\treturn err;\n}\n\nstatic int mana_fence_rq(struct mana_port_context *apc, struct mana_rxq *rxq)\n{\n\tstruct mana_fence_rq_resp resp = {};\n\tstruct mana_fence_rq_req req = {};\n\tint err;\n\n\tinit_completion(&rxq->fence_event);\n\n\tmana_gd_init_req_hdr(&req.hdr, MANA_FENCE_RQ,\n\t\t\t     sizeof(req), sizeof(resp));\n\treq.wq_obj_handle =  rxq->rxobj;\n\n\terr = mana_send_request(apc->ac, &req, sizeof(req), &resp,\n\t\t\t\tsizeof(resp));\n\tif (err) {\n\t\tnetdev_err(apc->ndev, \"Failed to fence RQ %u: %d\\n\",\n\t\t\t   rxq->rxq_idx, err);\n\t\treturn err;\n\t}\n\n\terr = mana_verify_resp_hdr(&resp.hdr, MANA_FENCE_RQ, sizeof(resp));\n\tif (err || resp.hdr.status) {\n\t\tnetdev_err(apc->ndev, \"Failed to fence RQ %u: %d, 0x%x\\n\",\n\t\t\t   rxq->rxq_idx, err, resp.hdr.status);\n\t\tif (!err)\n\t\t\terr = -EPROTO;\n\n\t\treturn err;\n\t}\n\n\tif (wait_for_completion_timeout(&rxq->fence_event, 10 * HZ) == 0) {\n\t\tnetdev_err(apc->ndev, \"Failed to fence RQ %u: timed out\\n\",\n\t\t\t   rxq->rxq_idx);\n\t\treturn -ETIMEDOUT;\n\t}\n\n\treturn 0;\n}\n\nstatic void mana_fence_rqs(struct mana_port_context *apc)\n{\n\tunsigned int rxq_idx;\n\tstruct mana_rxq *rxq;\n\tint err;\n\n\tfor (rxq_idx = 0; rxq_idx < apc->num_queues; rxq_idx++) {\n\t\trxq = apc->rxqs[rxq_idx];\n\t\terr = mana_fence_rq(apc, rxq);\n\n\t\t \n\t\tif (err)\n\t\t\tmsleep(100);\n\t}\n}\n\nstatic int mana_move_wq_tail(struct gdma_queue *wq, u32 num_units)\n{\n\tu32 used_space_old;\n\tu32 used_space_new;\n\n\tused_space_old = wq->head - wq->tail;\n\tused_space_new = wq->head - (wq->tail + num_units);\n\n\tif (WARN_ON_ONCE(used_space_new > used_space_old))\n\t\treturn -ERANGE;\n\n\twq->tail += num_units;\n\treturn 0;\n}\n\nstatic void mana_unmap_skb(struct sk_buff *skb, struct mana_port_context *apc)\n{\n\tstruct mana_skb_head *ash = (struct mana_skb_head *)skb->head;\n\tstruct gdma_context *gc = apc->ac->gdma_dev->gdma_context;\n\tstruct device *dev = gc->dev;\n\tint hsg, i;\n\n\t \n\thsg = (skb_is_gso(skb) && skb_headlen(skb) > ash->size[0]) ? 2 : 1;\n\n\tfor (i = 0; i < hsg; i++)\n\t\tdma_unmap_single(dev, ash->dma_handle[i], ash->size[i],\n\t\t\t\t DMA_TO_DEVICE);\n\n\tfor (i = hsg; i < skb_shinfo(skb)->nr_frags + hsg; i++)\n\t\tdma_unmap_page(dev, ash->dma_handle[i], ash->size[i],\n\t\t\t       DMA_TO_DEVICE);\n}\n\nstatic void mana_poll_tx_cq(struct mana_cq *cq)\n{\n\tstruct gdma_comp *completions = cq->gdma_comp_buf;\n\tstruct gdma_posted_wqe_info *wqe_info;\n\tunsigned int pkt_transmitted = 0;\n\tunsigned int wqe_unit_cnt = 0;\n\tstruct mana_txq *txq = cq->txq;\n\tstruct mana_port_context *apc;\n\tstruct netdev_queue *net_txq;\n\tstruct gdma_queue *gdma_wq;\n\tunsigned int avail_space;\n\tstruct net_device *ndev;\n\tstruct sk_buff *skb;\n\tbool txq_stopped;\n\tint comp_read;\n\tint i;\n\n\tndev = txq->ndev;\n\tapc = netdev_priv(ndev);\n\n\tcomp_read = mana_gd_poll_cq(cq->gdma_cq, completions,\n\t\t\t\t    CQE_POLLING_BUFFER);\n\n\tif (comp_read < 1)\n\t\treturn;\n\n\tfor (i = 0; i < comp_read; i++) {\n\t\tstruct mana_tx_comp_oob *cqe_oob;\n\n\t\tif (WARN_ON_ONCE(!completions[i].is_sq))\n\t\t\treturn;\n\n\t\tcqe_oob = (struct mana_tx_comp_oob *)completions[i].cqe_data;\n\t\tif (WARN_ON_ONCE(cqe_oob->cqe_hdr.client_type !=\n\t\t\t\t MANA_CQE_COMPLETION))\n\t\t\treturn;\n\n\t\tswitch (cqe_oob->cqe_hdr.cqe_type) {\n\t\tcase CQE_TX_OKAY:\n\t\t\tbreak;\n\n\t\tcase CQE_TX_SA_DROP:\n\t\tcase CQE_TX_MTU_DROP:\n\t\tcase CQE_TX_INVALID_OOB:\n\t\tcase CQE_TX_INVALID_ETH_TYPE:\n\t\tcase CQE_TX_HDR_PROCESSING_ERROR:\n\t\tcase CQE_TX_VF_DISABLED:\n\t\tcase CQE_TX_VPORT_IDX_OUT_OF_RANGE:\n\t\tcase CQE_TX_VPORT_DISABLED:\n\t\tcase CQE_TX_VLAN_TAGGING_VIOLATION:\n\t\t\tif (net_ratelimit())\n\t\t\t\tnetdev_err(ndev, \"TX: CQE error %d\\n\",\n\t\t\t\t\t   cqe_oob->cqe_hdr.cqe_type);\n\n\t\t\tapc->eth_stats.tx_cqe_err++;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\t \n\t\t\tif (net_ratelimit())\n\t\t\t\tnetdev_err(ndev, \"TX: unknown CQE type %d\\n\",\n\t\t\t\t\t   cqe_oob->cqe_hdr.cqe_type);\n\n\t\t\tapc->eth_stats.tx_cqe_unknown_type++;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (WARN_ON_ONCE(txq->gdma_txq_id != completions[i].wq_num))\n\t\t\treturn;\n\n\t\tskb = skb_dequeue(&txq->pending_skbs);\n\t\tif (WARN_ON_ONCE(!skb))\n\t\t\treturn;\n\n\t\twqe_info = (struct gdma_posted_wqe_info *)skb->cb;\n\t\twqe_unit_cnt += wqe_info->wqe_size_in_bu;\n\n\t\tmana_unmap_skb(skb, apc);\n\n\t\tnapi_consume_skb(skb, cq->budget);\n\n\t\tpkt_transmitted++;\n\t}\n\n\tif (WARN_ON_ONCE(wqe_unit_cnt == 0))\n\t\treturn;\n\n\tmana_move_wq_tail(txq->gdma_sq, wqe_unit_cnt);\n\n\tgdma_wq = txq->gdma_sq;\n\tavail_space = mana_gd_wq_avail_space(gdma_wq);\n\n\t \n\tsmp_mb();\n\n\tnet_txq = txq->net_txq;\n\ttxq_stopped = netif_tx_queue_stopped(net_txq);\n\n\t \n\tsmp_rmb();\n\n\tif (txq_stopped && apc->port_is_up && avail_space >= MAX_TX_WQE_SIZE) {\n\t\tnetif_tx_wake_queue(net_txq);\n\t\tapc->eth_stats.wake_queue++;\n\t}\n\n\tif (atomic_sub_return(pkt_transmitted, &txq->pending_sends) < 0)\n\t\tWARN_ON_ONCE(1);\n\n\tcq->work_done = pkt_transmitted;\n}\n\nstatic void mana_post_pkt_rxq(struct mana_rxq *rxq)\n{\n\tstruct mana_recv_buf_oob *recv_buf_oob;\n\tu32 curr_index;\n\tint err;\n\n\tcurr_index = rxq->buf_index++;\n\tif (rxq->buf_index == rxq->num_rx_buf)\n\t\trxq->buf_index = 0;\n\n\trecv_buf_oob = &rxq->rx_oobs[curr_index];\n\n\terr = mana_gd_post_work_request(rxq->gdma_rq, &recv_buf_oob->wqe_req,\n\t\t\t\t\t&recv_buf_oob->wqe_inf);\n\tif (WARN_ON_ONCE(err))\n\t\treturn;\n\n\tWARN_ON_ONCE(recv_buf_oob->wqe_inf.wqe_size_in_bu != 1);\n}\n\nstatic struct sk_buff *mana_build_skb(struct mana_rxq *rxq, void *buf_va,\n\t\t\t\t      uint pkt_len, struct xdp_buff *xdp)\n{\n\tstruct sk_buff *skb = napi_build_skb(buf_va, rxq->alloc_size);\n\n\tif (!skb)\n\t\treturn NULL;\n\n\tif (xdp->data_hard_start) {\n\t\tskb_reserve(skb, xdp->data - xdp->data_hard_start);\n\t\tskb_put(skb, xdp->data_end - xdp->data);\n\t\treturn skb;\n\t}\n\n\tskb_reserve(skb, rxq->headroom);\n\tskb_put(skb, pkt_len);\n\n\treturn skb;\n}\n\nstatic void mana_rx_skb(void *buf_va, bool from_pool,\n\t\t\tstruct mana_rxcomp_oob *cqe, struct mana_rxq *rxq)\n{\n\tstruct mana_stats_rx *rx_stats = &rxq->stats;\n\tstruct net_device *ndev = rxq->ndev;\n\tuint pkt_len = cqe->ppi[0].pkt_len;\n\tu16 rxq_idx = rxq->rxq_idx;\n\tstruct napi_struct *napi;\n\tstruct xdp_buff xdp = {};\n\tstruct sk_buff *skb;\n\tu32 hash_value;\n\tu32 act;\n\n\trxq->rx_cq.work_done++;\n\tnapi = &rxq->rx_cq.napi;\n\n\tif (!buf_va) {\n\t\t++ndev->stats.rx_dropped;\n\t\treturn;\n\t}\n\n\tact = mana_run_xdp(ndev, rxq, &xdp, buf_va, pkt_len);\n\n\tif (act == XDP_REDIRECT && !rxq->xdp_rc)\n\t\treturn;\n\n\tif (act != XDP_PASS && act != XDP_TX)\n\t\tgoto drop_xdp;\n\n\tskb = mana_build_skb(rxq, buf_va, pkt_len, &xdp);\n\n\tif (!skb)\n\t\tgoto drop;\n\n\tif (from_pool)\n\t\tskb_mark_for_recycle(skb);\n\n\tskb->dev = napi->dev;\n\n\tskb->protocol = eth_type_trans(skb, ndev);\n\tskb_checksum_none_assert(skb);\n\tskb_record_rx_queue(skb, rxq_idx);\n\n\tif ((ndev->features & NETIF_F_RXCSUM) && cqe->rx_iphdr_csum_succeed) {\n\t\tif (cqe->rx_tcp_csum_succeed || cqe->rx_udp_csum_succeed)\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t}\n\n\tif (cqe->rx_hashtype != 0 && (ndev->features & NETIF_F_RXHASH)) {\n\t\thash_value = cqe->ppi[0].pkt_hash;\n\n\t\tif (cqe->rx_hashtype & MANA_HASH_L4)\n\t\t\tskb_set_hash(skb, hash_value, PKT_HASH_TYPE_L4);\n\t\telse\n\t\t\tskb_set_hash(skb, hash_value, PKT_HASH_TYPE_L3);\n\t}\n\n\tif (cqe->rx_vlantag_present) {\n\t\tu16 vlan_tci = cqe->rx_vlan_id;\n\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), vlan_tci);\n\t}\n\n\tu64_stats_update_begin(&rx_stats->syncp);\n\trx_stats->packets++;\n\trx_stats->bytes += pkt_len;\n\n\tif (act == XDP_TX)\n\t\trx_stats->xdp_tx++;\n\tu64_stats_update_end(&rx_stats->syncp);\n\n\tif (act == XDP_TX) {\n\t\tskb_set_queue_mapping(skb, rxq_idx);\n\t\tmana_xdp_tx(skb, ndev);\n\t\treturn;\n\t}\n\n\tnapi_gro_receive(napi, skb);\n\n\treturn;\n\ndrop_xdp:\n\tu64_stats_update_begin(&rx_stats->syncp);\n\trx_stats->xdp_drop++;\n\tu64_stats_update_end(&rx_stats->syncp);\n\ndrop:\n\tif (from_pool) {\n\t\tpage_pool_recycle_direct(rxq->page_pool,\n\t\t\t\t\t virt_to_head_page(buf_va));\n\t} else {\n\t\tWARN_ON_ONCE(rxq->xdp_save_va);\n\t\t \n\t\trxq->xdp_save_va = buf_va;\n\t}\n\n\t++ndev->stats.rx_dropped;\n\n\treturn;\n}\n\nstatic void *mana_get_rxfrag(struct mana_rxq *rxq, struct device *dev,\n\t\t\t     dma_addr_t *da, bool *from_pool, bool is_napi)\n{\n\tstruct page *page;\n\tvoid *va;\n\n\t*from_pool = false;\n\n\t \n\tif (rxq->xdp_save_va) {\n\t\tva = rxq->xdp_save_va;\n\t\trxq->xdp_save_va = NULL;\n\t} else if (rxq->alloc_size > PAGE_SIZE) {\n\t\tif (is_napi)\n\t\t\tva = napi_alloc_frag(rxq->alloc_size);\n\t\telse\n\t\t\tva = netdev_alloc_frag(rxq->alloc_size);\n\n\t\tif (!va)\n\t\t\treturn NULL;\n\n\t\tpage = virt_to_head_page(va);\n\t\t \n\t\tif (compound_order(page) < get_order(rxq->alloc_size)) {\n\t\t\tput_page(page);\n\t\t\treturn NULL;\n\t\t}\n\t} else {\n\t\tpage = page_pool_dev_alloc_pages(rxq->page_pool);\n\t\tif (!page)\n\t\t\treturn NULL;\n\n\t\t*from_pool = true;\n\t\tva = page_to_virt(page);\n\t}\n\n\t*da = dma_map_single(dev, va + rxq->headroom, rxq->datasize,\n\t\t\t     DMA_FROM_DEVICE);\n\tif (dma_mapping_error(dev, *da)) {\n\t\tif (*from_pool)\n\t\t\tpage_pool_put_full_page(rxq->page_pool, page, false);\n\t\telse\n\t\t\tput_page(virt_to_head_page(va));\n\n\t\treturn NULL;\n\t}\n\n\treturn va;\n}\n\n \nstatic void mana_refill_rx_oob(struct device *dev, struct mana_rxq *rxq,\n\t\t\t       struct mana_recv_buf_oob *rxoob, void **old_buf,\n\t\t\t       bool *old_fp)\n{\n\tbool from_pool;\n\tdma_addr_t da;\n\tvoid *va;\n\n\tva = mana_get_rxfrag(rxq, dev, &da, &from_pool, true);\n\tif (!va)\n\t\treturn;\n\n\tdma_unmap_single(dev, rxoob->sgl[0].address, rxq->datasize,\n\t\t\t DMA_FROM_DEVICE);\n\t*old_buf = rxoob->buf_va;\n\t*old_fp = rxoob->from_pool;\n\n\trxoob->buf_va = va;\n\trxoob->sgl[0].address = da;\n\trxoob->from_pool = from_pool;\n}\n\nstatic void mana_process_rx_cqe(struct mana_rxq *rxq, struct mana_cq *cq,\n\t\t\t\tstruct gdma_comp *cqe)\n{\n\tstruct mana_rxcomp_oob *oob = (struct mana_rxcomp_oob *)cqe->cqe_data;\n\tstruct gdma_context *gc = rxq->gdma_rq->gdma_dev->gdma_context;\n\tstruct net_device *ndev = rxq->ndev;\n\tstruct mana_recv_buf_oob *rxbuf_oob;\n\tstruct mana_port_context *apc;\n\tstruct device *dev = gc->dev;\n\tvoid *old_buf = NULL;\n\tu32 curr, pktlen;\n\tbool old_fp;\n\n\tapc = netdev_priv(ndev);\n\n\tswitch (oob->cqe_hdr.cqe_type) {\n\tcase CQE_RX_OKAY:\n\t\tbreak;\n\n\tcase CQE_RX_TRUNCATED:\n\t\t++ndev->stats.rx_dropped;\n\t\trxbuf_oob = &rxq->rx_oobs[rxq->buf_index];\n\t\tnetdev_warn_once(ndev, \"Dropped a truncated packet\\n\");\n\t\tgoto drop;\n\n\tcase CQE_RX_COALESCED_4:\n\t\tnetdev_err(ndev, \"RX coalescing is unsupported\\n\");\n\t\tapc->eth_stats.rx_coalesced_err++;\n\t\treturn;\n\n\tcase CQE_RX_OBJECT_FENCE:\n\t\tcomplete(&rxq->fence_event);\n\t\treturn;\n\n\tdefault:\n\t\tnetdev_err(ndev, \"Unknown RX CQE type = %d\\n\",\n\t\t\t   oob->cqe_hdr.cqe_type);\n\t\tapc->eth_stats.rx_cqe_unknown_type++;\n\t\treturn;\n\t}\n\n\tpktlen = oob->ppi[0].pkt_len;\n\n\tif (pktlen == 0) {\n\t\t \n\t\tnetdev_err(ndev, \"RX pkt len=0, rq=%u, cq=%u, rxobj=0x%llx\\n\",\n\t\t\t   rxq->gdma_id, cq->gdma_id, rxq->rxobj);\n\t\treturn;\n\t}\n\n\tcurr = rxq->buf_index;\n\trxbuf_oob = &rxq->rx_oobs[curr];\n\tWARN_ON_ONCE(rxbuf_oob->wqe_inf.wqe_size_in_bu != 1);\n\n\tmana_refill_rx_oob(dev, rxq, rxbuf_oob, &old_buf, &old_fp);\n\n\t \n\tmana_rx_skb(old_buf, old_fp, oob, rxq);\n\ndrop:\n\tmana_move_wq_tail(rxq->gdma_rq, rxbuf_oob->wqe_inf.wqe_size_in_bu);\n\n\tmana_post_pkt_rxq(rxq);\n}\n\nstatic void mana_poll_rx_cq(struct mana_cq *cq)\n{\n\tstruct gdma_comp *comp = cq->gdma_comp_buf;\n\tstruct mana_rxq *rxq = cq->rxq;\n\tint comp_read, i;\n\n\tcomp_read = mana_gd_poll_cq(cq->gdma_cq, comp, CQE_POLLING_BUFFER);\n\tWARN_ON_ONCE(comp_read > CQE_POLLING_BUFFER);\n\n\trxq->xdp_flush = false;\n\n\tfor (i = 0; i < comp_read; i++) {\n\t\tif (WARN_ON_ONCE(comp[i].is_sq))\n\t\t\treturn;\n\n\t\t \n\t\tif (WARN_ON_ONCE(comp[i].wq_num != cq->rxq->gdma_id))\n\t\t\treturn;\n\n\t\tmana_process_rx_cqe(rxq, cq, &comp[i]);\n\t}\n\n\tif (comp_read > 0) {\n\t\tstruct gdma_context *gc = rxq->gdma_rq->gdma_dev->gdma_context;\n\n\t\tmana_gd_wq_ring_doorbell(gc, rxq->gdma_rq);\n\t}\n\n\tif (rxq->xdp_flush)\n\t\txdp_do_flush();\n}\n\nstatic int mana_cq_handler(void *context, struct gdma_queue *gdma_queue)\n{\n\tstruct mana_cq *cq = context;\n\tu8 arm_bit;\n\tint w;\n\n\tWARN_ON_ONCE(cq->gdma_cq != gdma_queue);\n\n\tif (cq->type == MANA_CQ_TYPE_RX)\n\t\tmana_poll_rx_cq(cq);\n\telse\n\t\tmana_poll_tx_cq(cq);\n\n\tw = cq->work_done;\n\n\tif (w < cq->budget &&\n\t    napi_complete_done(&cq->napi, w)) {\n\t\tarm_bit = SET_ARM_BIT;\n\t} else {\n\t\tarm_bit = 0;\n\t}\n\n\tmana_gd_ring_cq(gdma_queue, arm_bit);\n\n\treturn w;\n}\n\nstatic int mana_poll(struct napi_struct *napi, int budget)\n{\n\tstruct mana_cq *cq = container_of(napi, struct mana_cq, napi);\n\tint w;\n\n\tcq->work_done = 0;\n\tcq->budget = budget;\n\n\tw = mana_cq_handler(cq, cq->gdma_cq);\n\n\treturn min(w, budget);\n}\n\nstatic void mana_schedule_napi(void *context, struct gdma_queue *gdma_queue)\n{\n\tstruct mana_cq *cq = context;\n\n\tnapi_schedule_irqoff(&cq->napi);\n}\n\nstatic void mana_deinit_cq(struct mana_port_context *apc, struct mana_cq *cq)\n{\n\tstruct gdma_dev *gd = apc->ac->gdma_dev;\n\n\tif (!cq->gdma_cq)\n\t\treturn;\n\n\tmana_gd_destroy_queue(gd->gdma_context, cq->gdma_cq);\n}\n\nstatic void mana_deinit_txq(struct mana_port_context *apc, struct mana_txq *txq)\n{\n\tstruct gdma_dev *gd = apc->ac->gdma_dev;\n\n\tif (!txq->gdma_sq)\n\t\treturn;\n\n\tmana_gd_destroy_queue(gd->gdma_context, txq->gdma_sq);\n}\n\nstatic void mana_destroy_txq(struct mana_port_context *apc)\n{\n\tstruct napi_struct *napi;\n\tint i;\n\n\tif (!apc->tx_qp)\n\t\treturn;\n\n\tfor (i = 0; i < apc->num_queues; i++) {\n\t\tnapi = &apc->tx_qp[i].tx_cq.napi;\n\t\tnapi_synchronize(napi);\n\t\tnapi_disable(napi);\n\t\tnetif_napi_del(napi);\n\n\t\tmana_destroy_wq_obj(apc, GDMA_SQ, apc->tx_qp[i].tx_object);\n\n\t\tmana_deinit_cq(apc, &apc->tx_qp[i].tx_cq);\n\n\t\tmana_deinit_txq(apc, &apc->tx_qp[i].txq);\n\t}\n\n\tkfree(apc->tx_qp);\n\tapc->tx_qp = NULL;\n}\n\nstatic int mana_create_txq(struct mana_port_context *apc,\n\t\t\t   struct net_device *net)\n{\n\tstruct mana_context *ac = apc->ac;\n\tstruct gdma_dev *gd = ac->gdma_dev;\n\tstruct mana_obj_spec wq_spec;\n\tstruct mana_obj_spec cq_spec;\n\tstruct gdma_queue_spec spec;\n\tstruct gdma_context *gc;\n\tstruct mana_txq *txq;\n\tstruct mana_cq *cq;\n\tu32 txq_size;\n\tu32 cq_size;\n\tint err;\n\tint i;\n\n\tapc->tx_qp = kcalloc(apc->num_queues, sizeof(struct mana_tx_qp),\n\t\t\t     GFP_KERNEL);\n\tif (!apc->tx_qp)\n\t\treturn -ENOMEM;\n\n\t \n\ttxq_size = MAX_SEND_BUFFERS_PER_QUEUE * 32;\n\tBUILD_BUG_ON(!PAGE_ALIGNED(txq_size));\n\n\tcq_size = MAX_SEND_BUFFERS_PER_QUEUE * COMP_ENTRY_SIZE;\n\tcq_size = PAGE_ALIGN(cq_size);\n\n\tgc = gd->gdma_context;\n\n\tfor (i = 0; i < apc->num_queues; i++) {\n\t\tapc->tx_qp[i].tx_object = INVALID_MANA_HANDLE;\n\n\t\t \n\t\ttxq = &apc->tx_qp[i].txq;\n\n\t\tu64_stats_init(&txq->stats.syncp);\n\t\ttxq->ndev = net;\n\t\ttxq->net_txq = netdev_get_tx_queue(net, i);\n\t\ttxq->vp_offset = apc->tx_vp_offset;\n\t\tskb_queue_head_init(&txq->pending_skbs);\n\n\t\tmemset(&spec, 0, sizeof(spec));\n\t\tspec.type = GDMA_SQ;\n\t\tspec.monitor_avl_buf = true;\n\t\tspec.queue_size = txq_size;\n\t\terr = mana_gd_create_mana_wq_cq(gd, &spec, &txq->gdma_sq);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t \n\t\tcq = &apc->tx_qp[i].tx_cq;\n\t\tcq->type = MANA_CQ_TYPE_TX;\n\n\t\tcq->txq = txq;\n\n\t\tmemset(&spec, 0, sizeof(spec));\n\t\tspec.type = GDMA_CQ;\n\t\tspec.monitor_avl_buf = false;\n\t\tspec.queue_size = cq_size;\n\t\tspec.cq.callback = mana_schedule_napi;\n\t\tspec.cq.parent_eq = ac->eqs[i].eq;\n\t\tspec.cq.context = cq;\n\t\terr = mana_gd_create_mana_wq_cq(gd, &spec, &cq->gdma_cq);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tmemset(&wq_spec, 0, sizeof(wq_spec));\n\t\tmemset(&cq_spec, 0, sizeof(cq_spec));\n\n\t\twq_spec.gdma_region = txq->gdma_sq->mem_info.dma_region_handle;\n\t\twq_spec.queue_size = txq->gdma_sq->queue_size;\n\n\t\tcq_spec.gdma_region = cq->gdma_cq->mem_info.dma_region_handle;\n\t\tcq_spec.queue_size = cq->gdma_cq->queue_size;\n\t\tcq_spec.modr_ctx_id = 0;\n\t\tcq_spec.attached_eq = cq->gdma_cq->cq.parent->id;\n\n\t\terr = mana_create_wq_obj(apc, apc->port_handle, GDMA_SQ,\n\t\t\t\t\t &wq_spec, &cq_spec,\n\t\t\t\t\t &apc->tx_qp[i].tx_object);\n\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\ttxq->gdma_sq->id = wq_spec.queue_index;\n\t\tcq->gdma_cq->id = cq_spec.queue_index;\n\n\t\ttxq->gdma_sq->mem_info.dma_region_handle =\n\t\t\tGDMA_INVALID_DMA_REGION;\n\t\tcq->gdma_cq->mem_info.dma_region_handle =\n\t\t\tGDMA_INVALID_DMA_REGION;\n\n\t\ttxq->gdma_txq_id = txq->gdma_sq->id;\n\n\t\tcq->gdma_id = cq->gdma_cq->id;\n\n\t\tif (WARN_ON(cq->gdma_id >= gc->max_num_cqs)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tgc->cq_table[cq->gdma_id] = cq->gdma_cq;\n\n\t\tnetif_napi_add_tx(net, &cq->napi, mana_poll);\n\t\tnapi_enable(&cq->napi);\n\n\t\tmana_gd_ring_cq(cq->gdma_cq, SET_ARM_BIT);\n\t}\n\n\treturn 0;\nout:\n\tmana_destroy_txq(apc);\n\treturn err;\n}\n\nstatic void mana_destroy_rxq(struct mana_port_context *apc,\n\t\t\t     struct mana_rxq *rxq, bool validate_state)\n\n{\n\tstruct gdma_context *gc = apc->ac->gdma_dev->gdma_context;\n\tstruct mana_recv_buf_oob *rx_oob;\n\tstruct device *dev = gc->dev;\n\tstruct napi_struct *napi;\n\tstruct page *page;\n\tint i;\n\n\tif (!rxq)\n\t\treturn;\n\n\tnapi = &rxq->rx_cq.napi;\n\n\tif (validate_state)\n\t\tnapi_synchronize(napi);\n\n\tnapi_disable(napi);\n\n\txdp_rxq_info_unreg(&rxq->xdp_rxq);\n\n\tnetif_napi_del(napi);\n\n\tmana_destroy_wq_obj(apc, GDMA_RQ, rxq->rxobj);\n\n\tmana_deinit_cq(apc, &rxq->rx_cq);\n\n\tif (rxq->xdp_save_va)\n\t\tput_page(virt_to_head_page(rxq->xdp_save_va));\n\n\tfor (i = 0; i < rxq->num_rx_buf; i++) {\n\t\trx_oob = &rxq->rx_oobs[i];\n\n\t\tif (!rx_oob->buf_va)\n\t\t\tcontinue;\n\n\t\tdma_unmap_single(dev, rx_oob->sgl[0].address,\n\t\t\t\t rx_oob->sgl[0].size, DMA_FROM_DEVICE);\n\n\t\tpage = virt_to_head_page(rx_oob->buf_va);\n\n\t\tif (rx_oob->from_pool)\n\t\t\tpage_pool_put_full_page(rxq->page_pool, page, false);\n\t\telse\n\t\t\tput_page(page);\n\n\t\trx_oob->buf_va = NULL;\n\t}\n\n\tpage_pool_destroy(rxq->page_pool);\n\n\tif (rxq->gdma_rq)\n\t\tmana_gd_destroy_queue(gc, rxq->gdma_rq);\n\n\tkfree(rxq);\n}\n\nstatic int mana_fill_rx_oob(struct mana_recv_buf_oob *rx_oob, u32 mem_key,\n\t\t\t    struct mana_rxq *rxq, struct device *dev)\n{\n\tstruct mana_port_context *mpc = netdev_priv(rxq->ndev);\n\tbool from_pool = false;\n\tdma_addr_t da;\n\tvoid *va;\n\n\tif (mpc->rxbufs_pre)\n\t\tva = mana_get_rxbuf_pre(rxq, &da);\n\telse\n\t\tva = mana_get_rxfrag(rxq, dev, &da, &from_pool, false);\n\n\tif (!va)\n\t\treturn -ENOMEM;\n\n\trx_oob->buf_va = va;\n\trx_oob->from_pool = from_pool;\n\n\trx_oob->sgl[0].address = da;\n\trx_oob->sgl[0].size = rxq->datasize;\n\trx_oob->sgl[0].mem_key = mem_key;\n\n\treturn 0;\n}\n\n#define MANA_WQE_HEADER_SIZE 16\n#define MANA_WQE_SGE_SIZE 16\n\nstatic int mana_alloc_rx_wqe(struct mana_port_context *apc,\n\t\t\t     struct mana_rxq *rxq, u32 *rxq_size, u32 *cq_size)\n{\n\tstruct gdma_context *gc = apc->ac->gdma_dev->gdma_context;\n\tstruct mana_recv_buf_oob *rx_oob;\n\tstruct device *dev = gc->dev;\n\tu32 buf_idx;\n\tint ret;\n\n\tWARN_ON(rxq->datasize == 0);\n\n\t*rxq_size = 0;\n\t*cq_size = 0;\n\n\tfor (buf_idx = 0; buf_idx < rxq->num_rx_buf; buf_idx++) {\n\t\trx_oob = &rxq->rx_oobs[buf_idx];\n\t\tmemset(rx_oob, 0, sizeof(*rx_oob));\n\n\t\trx_oob->num_sge = 1;\n\n\t\tret = mana_fill_rx_oob(rx_oob, apc->ac->gdma_dev->gpa_mkey, rxq,\n\t\t\t\t       dev);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\trx_oob->wqe_req.sgl = rx_oob->sgl;\n\t\trx_oob->wqe_req.num_sge = rx_oob->num_sge;\n\t\trx_oob->wqe_req.inline_oob_size = 0;\n\t\trx_oob->wqe_req.inline_oob_data = NULL;\n\t\trx_oob->wqe_req.flags = 0;\n\t\trx_oob->wqe_req.client_data_unit = 0;\n\n\t\t*rxq_size += ALIGN(MANA_WQE_HEADER_SIZE +\n\t\t\t\t   MANA_WQE_SGE_SIZE * rx_oob->num_sge, 32);\n\t\t*cq_size += COMP_ENTRY_SIZE;\n\t}\n\n\treturn 0;\n}\n\nstatic int mana_push_wqe(struct mana_rxq *rxq)\n{\n\tstruct mana_recv_buf_oob *rx_oob;\n\tu32 buf_idx;\n\tint err;\n\n\tfor (buf_idx = 0; buf_idx < rxq->num_rx_buf; buf_idx++) {\n\t\trx_oob = &rxq->rx_oobs[buf_idx];\n\n\t\terr = mana_gd_post_and_ring(rxq->gdma_rq, &rx_oob->wqe_req,\n\t\t\t\t\t    &rx_oob->wqe_inf);\n\t\tif (err)\n\t\t\treturn -ENOSPC;\n\t}\n\n\treturn 0;\n}\n\nstatic int mana_create_page_pool(struct mana_rxq *rxq, struct gdma_context *gc)\n{\n\tstruct page_pool_params pprm = {};\n\tint ret;\n\n\tpprm.pool_size = RX_BUFFERS_PER_QUEUE;\n\tpprm.nid = gc->numa_node;\n\tpprm.napi = &rxq->rx_cq.napi;\n\n\trxq->page_pool = page_pool_create(&pprm);\n\n\tif (IS_ERR(rxq->page_pool)) {\n\t\tret = PTR_ERR(rxq->page_pool);\n\t\trxq->page_pool = NULL;\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic struct mana_rxq *mana_create_rxq(struct mana_port_context *apc,\n\t\t\t\t\tu32 rxq_idx, struct mana_eq *eq,\n\t\t\t\t\tstruct net_device *ndev)\n{\n\tstruct gdma_dev *gd = apc->ac->gdma_dev;\n\tstruct mana_obj_spec wq_spec;\n\tstruct mana_obj_spec cq_spec;\n\tstruct gdma_queue_spec spec;\n\tstruct mana_cq *cq = NULL;\n\tstruct gdma_context *gc;\n\tu32 cq_size, rq_size;\n\tstruct mana_rxq *rxq;\n\tint err;\n\n\tgc = gd->gdma_context;\n\n\trxq = kzalloc(struct_size(rxq, rx_oobs, RX_BUFFERS_PER_QUEUE),\n\t\t      GFP_KERNEL);\n\tif (!rxq)\n\t\treturn NULL;\n\n\trxq->ndev = ndev;\n\trxq->num_rx_buf = RX_BUFFERS_PER_QUEUE;\n\trxq->rxq_idx = rxq_idx;\n\trxq->rxobj = INVALID_MANA_HANDLE;\n\n\tmana_get_rxbuf_cfg(ndev->mtu, &rxq->datasize, &rxq->alloc_size,\n\t\t\t   &rxq->headroom);\n\n\t \n\terr = mana_create_page_pool(rxq, gc);\n\tif (err) {\n\t\tnetdev_err(ndev, \"Create page pool err:%d\\n\", err);\n\t\tgoto out;\n\t}\n\n\terr = mana_alloc_rx_wqe(apc, rxq, &rq_size, &cq_size);\n\tif (err)\n\t\tgoto out;\n\n\trq_size = PAGE_ALIGN(rq_size);\n\tcq_size = PAGE_ALIGN(cq_size);\n\n\t \n\tmemset(&spec, 0, sizeof(spec));\n\tspec.type = GDMA_RQ;\n\tspec.monitor_avl_buf = true;\n\tspec.queue_size = rq_size;\n\terr = mana_gd_create_mana_wq_cq(gd, &spec, &rxq->gdma_rq);\n\tif (err)\n\t\tgoto out;\n\n\t \n\tcq = &rxq->rx_cq;\n\tcq->type = MANA_CQ_TYPE_RX;\n\tcq->rxq = rxq;\n\n\tmemset(&spec, 0, sizeof(spec));\n\tspec.type = GDMA_CQ;\n\tspec.monitor_avl_buf = false;\n\tspec.queue_size = cq_size;\n\tspec.cq.callback = mana_schedule_napi;\n\tspec.cq.parent_eq = eq->eq;\n\tspec.cq.context = cq;\n\terr = mana_gd_create_mana_wq_cq(gd, &spec, &cq->gdma_cq);\n\tif (err)\n\t\tgoto out;\n\n\tmemset(&wq_spec, 0, sizeof(wq_spec));\n\tmemset(&cq_spec, 0, sizeof(cq_spec));\n\twq_spec.gdma_region = rxq->gdma_rq->mem_info.dma_region_handle;\n\twq_spec.queue_size = rxq->gdma_rq->queue_size;\n\n\tcq_spec.gdma_region = cq->gdma_cq->mem_info.dma_region_handle;\n\tcq_spec.queue_size = cq->gdma_cq->queue_size;\n\tcq_spec.modr_ctx_id = 0;\n\tcq_spec.attached_eq = cq->gdma_cq->cq.parent->id;\n\n\terr = mana_create_wq_obj(apc, apc->port_handle, GDMA_RQ,\n\t\t\t\t &wq_spec, &cq_spec, &rxq->rxobj);\n\tif (err)\n\t\tgoto out;\n\n\trxq->gdma_rq->id = wq_spec.queue_index;\n\tcq->gdma_cq->id = cq_spec.queue_index;\n\n\trxq->gdma_rq->mem_info.dma_region_handle = GDMA_INVALID_DMA_REGION;\n\tcq->gdma_cq->mem_info.dma_region_handle = GDMA_INVALID_DMA_REGION;\n\n\trxq->gdma_id = rxq->gdma_rq->id;\n\tcq->gdma_id = cq->gdma_cq->id;\n\n\terr = mana_push_wqe(rxq);\n\tif (err)\n\t\tgoto out;\n\n\tif (WARN_ON(cq->gdma_id >= gc->max_num_cqs)) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tgc->cq_table[cq->gdma_id] = cq->gdma_cq;\n\n\tnetif_napi_add_weight(ndev, &cq->napi, mana_poll, 1);\n\n\tWARN_ON(xdp_rxq_info_reg(&rxq->xdp_rxq, ndev, rxq_idx,\n\t\t\t\t cq->napi.napi_id));\n\tWARN_ON(xdp_rxq_info_reg_mem_model(&rxq->xdp_rxq, MEM_TYPE_PAGE_POOL,\n\t\t\t\t\t   rxq->page_pool));\n\n\tnapi_enable(&cq->napi);\n\n\tmana_gd_ring_cq(cq->gdma_cq, SET_ARM_BIT);\nout:\n\tif (!err)\n\t\treturn rxq;\n\n\tnetdev_err(ndev, \"Failed to create RXQ: err = %d\\n\", err);\n\n\tmana_destroy_rxq(apc, rxq, false);\n\n\tif (cq)\n\t\tmana_deinit_cq(apc, cq);\n\n\treturn NULL;\n}\n\nstatic int mana_add_rx_queues(struct mana_port_context *apc,\n\t\t\t      struct net_device *ndev)\n{\n\tstruct mana_context *ac = apc->ac;\n\tstruct mana_rxq *rxq;\n\tint err = 0;\n\tint i;\n\n\tfor (i = 0; i < apc->num_queues; i++) {\n\t\trxq = mana_create_rxq(apc, i, &ac->eqs[i], ndev);\n\t\tif (!rxq) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tu64_stats_init(&rxq->stats.syncp);\n\n\t\tapc->rxqs[i] = rxq;\n\t}\n\n\tapc->default_rxobj = apc->rxqs[0]->rxobj;\nout:\n\treturn err;\n}\n\nstatic void mana_destroy_vport(struct mana_port_context *apc)\n{\n\tstruct gdma_dev *gd = apc->ac->gdma_dev;\n\tstruct mana_rxq *rxq;\n\tu32 rxq_idx;\n\n\tfor (rxq_idx = 0; rxq_idx < apc->num_queues; rxq_idx++) {\n\t\trxq = apc->rxqs[rxq_idx];\n\t\tif (!rxq)\n\t\t\tcontinue;\n\n\t\tmana_destroy_rxq(apc, rxq, true);\n\t\tapc->rxqs[rxq_idx] = NULL;\n\t}\n\n\tmana_destroy_txq(apc);\n\tmana_uncfg_vport(apc);\n\n\tif (gd->gdma_context->is_pf)\n\t\tmana_pf_deregister_hw_vport(apc);\n}\n\nstatic int mana_create_vport(struct mana_port_context *apc,\n\t\t\t     struct net_device *net)\n{\n\tstruct gdma_dev *gd = apc->ac->gdma_dev;\n\tint err;\n\n\tapc->default_rxobj = INVALID_MANA_HANDLE;\n\n\tif (gd->gdma_context->is_pf) {\n\t\terr = mana_pf_register_hw_vport(apc);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\terr = mana_cfg_vport(apc, gd->pdid, gd->doorbell);\n\tif (err)\n\t\treturn err;\n\n\treturn mana_create_txq(apc, net);\n}\n\nstatic void mana_rss_table_init(struct mana_port_context *apc)\n{\n\tint i;\n\n\tfor (i = 0; i < MANA_INDIRECT_TABLE_SIZE; i++)\n\t\tapc->indir_table[i] =\n\t\t\tethtool_rxfh_indir_default(i, apc->num_queues);\n}\n\nint mana_config_rss(struct mana_port_context *apc, enum TRI_STATE rx,\n\t\t    bool update_hash, bool update_tab)\n{\n\tu32 queue_idx;\n\tint err;\n\tint i;\n\n\tif (update_tab) {\n\t\tfor (i = 0; i < MANA_INDIRECT_TABLE_SIZE; i++) {\n\t\t\tqueue_idx = apc->indir_table[i];\n\t\t\tapc->rxobj_table[i] = apc->rxqs[queue_idx]->rxobj;\n\t\t}\n\t}\n\n\terr = mana_cfg_vport_steering(apc, rx, true, update_hash, update_tab);\n\tif (err)\n\t\treturn err;\n\n\tmana_fence_rqs(apc);\n\n\treturn 0;\n}\n\nvoid mana_query_gf_stats(struct mana_port_context *apc)\n{\n\tstruct mana_query_gf_stat_resp resp = {};\n\tstruct mana_query_gf_stat_req req = {};\n\tstruct net_device *ndev = apc->ndev;\n\tint err;\n\n\tmana_gd_init_req_hdr(&req.hdr, MANA_QUERY_GF_STAT,\n\t\t\t     sizeof(req), sizeof(resp));\n\treq.req_stats = STATISTICS_FLAGS_HC_TX_BYTES |\n\t\t\tSTATISTICS_FLAGS_HC_TX_UCAST_PACKETS |\n\t\t\tSTATISTICS_FLAGS_HC_TX_UCAST_BYTES |\n\t\t\tSTATISTICS_FLAGS_HC_TX_MCAST_PACKETS |\n\t\t\tSTATISTICS_FLAGS_HC_TX_MCAST_BYTES |\n\t\t\tSTATISTICS_FLAGS_HC_TX_BCAST_PACKETS |\n\t\t\tSTATISTICS_FLAGS_HC_TX_BCAST_BYTES;\n\n\terr = mana_send_request(apc->ac, &req, sizeof(req), &resp,\n\t\t\t\tsizeof(resp));\n\tif (err) {\n\t\tnetdev_err(ndev, \"Failed to query GF stats: %d\\n\", err);\n\t\treturn;\n\t}\n\terr = mana_verify_resp_hdr(&resp.hdr, MANA_QUERY_GF_STAT,\n\t\t\t\t   sizeof(resp));\n\tif (err || resp.hdr.status) {\n\t\tnetdev_err(ndev, \"Failed to query GF stats: %d, 0x%x\\n\", err,\n\t\t\t   resp.hdr.status);\n\t\treturn;\n\t}\n\n\tapc->eth_stats.hc_tx_bytes = resp.hc_tx_bytes;\n\tapc->eth_stats.hc_tx_ucast_pkts = resp.hc_tx_ucast_pkts;\n\tapc->eth_stats.hc_tx_ucast_bytes = resp.hc_tx_ucast_bytes;\n\tapc->eth_stats.hc_tx_bcast_pkts = resp.hc_tx_bcast_pkts;\n\tapc->eth_stats.hc_tx_bcast_bytes = resp.hc_tx_bcast_bytes;\n\tapc->eth_stats.hc_tx_mcast_pkts = resp.hc_tx_mcast_pkts;\n\tapc->eth_stats.hc_tx_mcast_bytes = resp.hc_tx_mcast_bytes;\n}\n\nstatic int mana_init_port(struct net_device *ndev)\n{\n\tstruct mana_port_context *apc = netdev_priv(ndev);\n\tu32 max_txq, max_rxq, max_queues;\n\tint port_idx = apc->port_idx;\n\tu32 num_indirect_entries;\n\tint err;\n\n\terr = mana_init_port_context(apc);\n\tif (err)\n\t\treturn err;\n\n\terr = mana_query_vport_cfg(apc, port_idx, &max_txq, &max_rxq,\n\t\t\t\t   &num_indirect_entries);\n\tif (err) {\n\t\tnetdev_err(ndev, \"Failed to query info for vPort %d\\n\",\n\t\t\t   port_idx);\n\t\tgoto reset_apc;\n\t}\n\n\tmax_queues = min_t(u32, max_txq, max_rxq);\n\tif (apc->max_queues > max_queues)\n\t\tapc->max_queues = max_queues;\n\n\tif (apc->num_queues > apc->max_queues)\n\t\tapc->num_queues = apc->max_queues;\n\n\teth_hw_addr_set(ndev, apc->mac_addr);\n\n\treturn 0;\n\nreset_apc:\n\tkfree(apc->rxqs);\n\tapc->rxqs = NULL;\n\treturn err;\n}\n\nint mana_alloc_queues(struct net_device *ndev)\n{\n\tstruct mana_port_context *apc = netdev_priv(ndev);\n\tstruct gdma_dev *gd = apc->ac->gdma_dev;\n\tint err;\n\n\terr = mana_create_vport(apc, ndev);\n\tif (err)\n\t\treturn err;\n\n\terr = netif_set_real_num_tx_queues(ndev, apc->num_queues);\n\tif (err)\n\t\tgoto destroy_vport;\n\n\terr = mana_add_rx_queues(apc, ndev);\n\tif (err)\n\t\tgoto destroy_vport;\n\n\tapc->rss_state = apc->num_queues > 1 ? TRI_STATE_TRUE : TRI_STATE_FALSE;\n\n\terr = netif_set_real_num_rx_queues(ndev, apc->num_queues);\n\tif (err)\n\t\tgoto destroy_vport;\n\n\tmana_rss_table_init(apc);\n\n\terr = mana_config_rss(apc, TRI_STATE_TRUE, true, true);\n\tif (err)\n\t\tgoto destroy_vport;\n\n\tif (gd->gdma_context->is_pf) {\n\t\terr = mana_pf_register_filter(apc);\n\t\tif (err)\n\t\t\tgoto destroy_vport;\n\t}\n\n\tmana_chn_setxdp(apc, mana_xdp_get(apc));\n\n\treturn 0;\n\ndestroy_vport:\n\tmana_destroy_vport(apc);\n\treturn err;\n}\n\nint mana_attach(struct net_device *ndev)\n{\n\tstruct mana_port_context *apc = netdev_priv(ndev);\n\tint err;\n\n\tASSERT_RTNL();\n\n\terr = mana_init_port(ndev);\n\tif (err)\n\t\treturn err;\n\n\tif (apc->port_st_save) {\n\t\terr = mana_alloc_queues(ndev);\n\t\tif (err) {\n\t\t\tmana_cleanup_port_context(apc);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tapc->port_is_up = apc->port_st_save;\n\n\t \n\tsmp_wmb();\n\n\tif (apc->port_is_up)\n\t\tnetif_carrier_on(ndev);\n\n\tnetif_device_attach(ndev);\n\n\treturn 0;\n}\n\nstatic int mana_dealloc_queues(struct net_device *ndev)\n{\n\tstruct mana_port_context *apc = netdev_priv(ndev);\n\tunsigned long timeout = jiffies + 120 * HZ;\n\tstruct gdma_dev *gd = apc->ac->gdma_dev;\n\tstruct mana_txq *txq;\n\tstruct sk_buff *skb;\n\tint i, err;\n\tu32 tsleep;\n\n\tif (apc->port_is_up)\n\t\treturn -EINVAL;\n\n\tmana_chn_setxdp(apc, NULL);\n\n\tif (gd->gdma_context->is_pf)\n\t\tmana_pf_deregister_filter(apc);\n\n\t \n\n\tfor (i = 0; i < apc->num_queues; i++) {\n\t\ttxq = &apc->tx_qp[i].txq;\n\t\ttsleep = 1000;\n\t\twhile (atomic_read(&txq->pending_sends) > 0 &&\n\t\t       time_before(jiffies, timeout)) {\n\t\t\tusleep_range(tsleep, tsleep + 1000);\n\t\t\ttsleep <<= 1;\n\t\t}\n\t\tif (atomic_read(&txq->pending_sends)) {\n\t\t\terr = pcie_flr(to_pci_dev(gd->gdma_context->dev));\n\t\t\tif (err) {\n\t\t\t\tnetdev_err(ndev, \"flr failed %d with %d pkts pending in txq %u\\n\",\n\t\t\t\t\t   err, atomic_read(&txq->pending_sends),\n\t\t\t\t\t   txq->gdma_txq_id);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfor (i = 0; i < apc->num_queues; i++) {\n\t\ttxq = &apc->tx_qp[i].txq;\n\t\twhile ((skb = skb_dequeue(&txq->pending_skbs))) {\n\t\t\tmana_unmap_skb(skb, apc);\n\t\t\tdev_kfree_skb_any(skb);\n\t\t}\n\t\tatomic_set(&txq->pending_sends, 0);\n\t}\n\t \n\n\tapc->rss_state = TRI_STATE_FALSE;\n\terr = mana_config_rss(apc, TRI_STATE_FALSE, false, false);\n\tif (err) {\n\t\tnetdev_err(ndev, \"Failed to disable vPort: %d\\n\", err);\n\t\treturn err;\n\t}\n\n\tmana_destroy_vport(apc);\n\n\treturn 0;\n}\n\nint mana_detach(struct net_device *ndev, bool from_close)\n{\n\tstruct mana_port_context *apc = netdev_priv(ndev);\n\tint err;\n\n\tASSERT_RTNL();\n\n\tapc->port_st_save = apc->port_is_up;\n\tapc->port_is_up = false;\n\n\t \n\tsmp_wmb();\n\n\tnetif_tx_disable(ndev);\n\tnetif_carrier_off(ndev);\n\n\tif (apc->port_st_save) {\n\t\terr = mana_dealloc_queues(ndev);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (!from_close) {\n\t\tnetif_device_detach(ndev);\n\t\tmana_cleanup_port_context(apc);\n\t}\n\n\treturn 0;\n}\n\nstatic int mana_probe_port(struct mana_context *ac, int port_idx,\n\t\t\t   struct net_device **ndev_storage)\n{\n\tstruct gdma_context *gc = ac->gdma_dev->gdma_context;\n\tstruct mana_port_context *apc;\n\tstruct net_device *ndev;\n\tint err;\n\n\tndev = alloc_etherdev_mq(sizeof(struct mana_port_context),\n\t\t\t\t gc->max_num_queues);\n\tif (!ndev)\n\t\treturn -ENOMEM;\n\n\t*ndev_storage = ndev;\n\n\tapc = netdev_priv(ndev);\n\tapc->ac = ac;\n\tapc->ndev = ndev;\n\tapc->max_queues = gc->max_num_queues;\n\tapc->num_queues = gc->max_num_queues;\n\tapc->port_handle = INVALID_MANA_HANDLE;\n\tapc->pf_filter_handle = INVALID_MANA_HANDLE;\n\tapc->port_idx = port_idx;\n\n\tmutex_init(&apc->vport_mutex);\n\tapc->vport_use_count = 0;\n\n\tndev->netdev_ops = &mana_devops;\n\tndev->ethtool_ops = &mana_ethtool_ops;\n\tndev->mtu = ETH_DATA_LEN;\n\tndev->max_mtu = gc->adapter_mtu - ETH_HLEN;\n\tndev->min_mtu = ETH_MIN_MTU;\n\tndev->needed_headroom = MANA_HEADROOM;\n\tndev->dev_port = port_idx;\n\tSET_NETDEV_DEV(ndev, gc->dev);\n\n\tnetif_carrier_off(ndev);\n\n\tnetdev_rss_key_fill(apc->hashkey, MANA_HASH_KEY_SIZE);\n\n\terr = mana_init_port(ndev);\n\tif (err)\n\t\tgoto free_net;\n\n\tnetdev_lockdep_set_classes(ndev);\n\n\tndev->hw_features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM;\n\tndev->hw_features |= NETIF_F_RXCSUM;\n\tndev->hw_features |= NETIF_F_TSO | NETIF_F_TSO6;\n\tndev->hw_features |= NETIF_F_RXHASH;\n\tndev->features = ndev->hw_features | NETIF_F_HW_VLAN_CTAG_TX |\n\t\t\t NETIF_F_HW_VLAN_CTAG_RX;\n\tndev->vlan_features = ndev->features;\n\tndev->xdp_features = NETDEV_XDP_ACT_BASIC | NETDEV_XDP_ACT_REDIRECT |\n\t\t\t     NETDEV_XDP_ACT_NDO_XMIT;\n\n\terr = register_netdev(ndev);\n\tif (err) {\n\t\tnetdev_err(ndev, \"Unable to register netdev.\\n\");\n\t\tgoto reset_apc;\n\t}\n\n\treturn 0;\n\nreset_apc:\n\tkfree(apc->rxqs);\n\tapc->rxqs = NULL;\nfree_net:\n\t*ndev_storage = NULL;\n\tnetdev_err(ndev, \"Failed to probe vPort %d: %d\\n\", port_idx, err);\n\tfree_netdev(ndev);\n\treturn err;\n}\n\nstatic void adev_release(struct device *dev)\n{\n\tstruct mana_adev *madev = container_of(dev, struct mana_adev, adev.dev);\n\n\tkfree(madev);\n}\n\nstatic void remove_adev(struct gdma_dev *gd)\n{\n\tstruct auxiliary_device *adev = gd->adev;\n\tint id = adev->id;\n\n\tauxiliary_device_delete(adev);\n\tauxiliary_device_uninit(adev);\n\n\tmana_adev_idx_free(id);\n\tgd->adev = NULL;\n}\n\nstatic int add_adev(struct gdma_dev *gd)\n{\n\tstruct auxiliary_device *adev;\n\tstruct mana_adev *madev;\n\tint ret;\n\n\tmadev = kzalloc(sizeof(*madev), GFP_KERNEL);\n\tif (!madev)\n\t\treturn -ENOMEM;\n\n\tadev = &madev->adev;\n\tret = mana_adev_idx_alloc();\n\tif (ret < 0)\n\t\tgoto idx_fail;\n\tadev->id = ret;\n\n\tadev->name = \"rdma\";\n\tadev->dev.parent = gd->gdma_context->dev;\n\tadev->dev.release = adev_release;\n\tmadev->mdev = gd;\n\n\tret = auxiliary_device_init(adev);\n\tif (ret)\n\t\tgoto init_fail;\n\n\tret = auxiliary_device_add(adev);\n\tif (ret)\n\t\tgoto add_fail;\n\n\tgd->adev = adev;\n\treturn 0;\n\nadd_fail:\n\tauxiliary_device_uninit(adev);\n\ninit_fail:\n\tmana_adev_idx_free(adev->id);\n\nidx_fail:\n\tkfree(madev);\n\n\treturn ret;\n}\n\nint mana_probe(struct gdma_dev *gd, bool resuming)\n{\n\tstruct gdma_context *gc = gd->gdma_context;\n\tstruct mana_context *ac = gd->driver_data;\n\tstruct device *dev = gc->dev;\n\tu16 num_ports = 0;\n\tint err;\n\tint i;\n\n\tdev_info(dev,\n\t\t \"Microsoft Azure Network Adapter protocol version: %d.%d.%d\\n\",\n\t\t MANA_MAJOR_VERSION, MANA_MINOR_VERSION, MANA_MICRO_VERSION);\n\n\terr = mana_gd_register_device(gd);\n\tif (err)\n\t\treturn err;\n\n\tif (!resuming) {\n\t\tac = kzalloc(sizeof(*ac), GFP_KERNEL);\n\t\tif (!ac)\n\t\t\treturn -ENOMEM;\n\n\t\tac->gdma_dev = gd;\n\t\tgd->driver_data = ac;\n\t}\n\n\terr = mana_create_eq(ac);\n\tif (err)\n\t\tgoto out;\n\n\terr = mana_query_device_cfg(ac, MANA_MAJOR_VERSION, MANA_MINOR_VERSION,\n\t\t\t\t    MANA_MICRO_VERSION, &num_ports);\n\tif (err)\n\t\tgoto out;\n\n\tif (!resuming) {\n\t\tac->num_ports = num_ports;\n\t} else {\n\t\tif (ac->num_ports != num_ports) {\n\t\t\tdev_err(dev, \"The number of vPorts changed: %d->%d\\n\",\n\t\t\t\tac->num_ports, num_ports);\n\t\t\terr = -EPROTO;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (ac->num_ports == 0)\n\t\tdev_err(dev, \"Failed to detect any vPort\\n\");\n\n\tif (ac->num_ports > MAX_PORTS_IN_MANA_DEV)\n\t\tac->num_ports = MAX_PORTS_IN_MANA_DEV;\n\n\tif (!resuming) {\n\t\tfor (i = 0; i < ac->num_ports; i++) {\n\t\t\terr = mana_probe_port(ac, i, &ac->ports[i]);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < ac->num_ports; i++) {\n\t\t\trtnl_lock();\n\t\t\terr = mana_attach(ac->ports[i]);\n\t\t\trtnl_unlock();\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\terr = add_adev(gd);\nout:\n\tif (err)\n\t\tmana_remove(gd, false);\n\n\treturn err;\n}\n\nvoid mana_remove(struct gdma_dev *gd, bool suspending)\n{\n\tstruct gdma_context *gc = gd->gdma_context;\n\tstruct mana_context *ac = gd->driver_data;\n\tstruct device *dev = gc->dev;\n\tstruct net_device *ndev;\n\tint err;\n\tint i;\n\n\t \n\tif (gd->adev)\n\t\tremove_adev(gd);\n\n\tfor (i = 0; i < ac->num_ports; i++) {\n\t\tndev = ac->ports[i];\n\t\tif (!ndev) {\n\t\t\tif (i == 0)\n\t\t\t\tdev_err(dev, \"No net device to remove\\n\");\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\trtnl_lock();\n\n\t\terr = mana_detach(ndev, false);\n\t\tif (err)\n\t\t\tnetdev_err(ndev, \"Failed to detach vPort %d: %d\\n\",\n\t\t\t\t   i, err);\n\n\t\tif (suspending) {\n\t\t\t \n\t\t\trtnl_unlock();\n\t\t\tcontinue;\n\t\t}\n\n\t\tunregister_netdevice(ndev);\n\n\t\trtnl_unlock();\n\n\t\tfree_netdev(ndev);\n\t}\n\n\tmana_destroy_eq(ac);\nout:\n\tmana_gd_deregister_device(gd);\n\n\tif (suspending)\n\t\treturn;\n\n\tgd->driver_data = NULL;\n\tgd->gdma_context = NULL;\n\tkfree(ac);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}