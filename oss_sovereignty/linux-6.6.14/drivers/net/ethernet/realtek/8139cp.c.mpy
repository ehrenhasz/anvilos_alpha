{
  "module_name": "8139cp.c",
  "hash_id": "ffbbda0fa20096353aae4847a4326bf2ca304c46442ea826c49685d6643842d1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/realtek/8139cp.c",
  "human_readable_source": " \n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#define DRV_NAME\t\t\"8139cp\"\n#define DRV_VERSION\t\t\"1.3\"\n#define DRV_RELDATE\t\t\"Mar 22, 2004\"\n\n\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/kernel.h>\n#include <linux/compiler.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/pci.h>\n#include <linux/dma-mapping.h>\n#include <linux/delay.h>\n#include <linux/ethtool.h>\n#include <linux/gfp.h>\n#include <linux/mii.h>\n#include <linux/if_vlan.h>\n#include <linux/crc32.h>\n#include <linux/in.h>\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <linux/udp.h>\n#include <linux/cache.h>\n#include <asm/io.h>\n#include <asm/irq.h>\n#include <linux/uaccess.h>\n\n \nstatic char version[] =\nDRV_NAME \": 10/100 PCI Ethernet driver v\" DRV_VERSION \" (\" DRV_RELDATE \")\\n\";\n\nMODULE_AUTHOR(\"Jeff Garzik <jgarzik@pobox.com>\");\nMODULE_DESCRIPTION(\"RealTek RTL-8139C+ series 10/100 PCI Ethernet driver\");\nMODULE_VERSION(DRV_VERSION);\nMODULE_LICENSE(\"GPL\");\n\nstatic int debug = -1;\nmodule_param(debug, int, 0);\nMODULE_PARM_DESC (debug, \"8139cp: bitmapped message enable number\");\n\n \nstatic int multicast_filter_limit = 32;\nmodule_param(multicast_filter_limit, int, 0);\nMODULE_PARM_DESC (multicast_filter_limit, \"8139cp: maximum number of filtered multicast addresses\");\n\n#define CP_DEF_MSG_ENABLE\t(NETIF_MSG_DRV\t\t| \\\n\t\t\t\t NETIF_MSG_PROBE \t| \\\n\t\t\t\t NETIF_MSG_LINK)\n#define CP_NUM_STATS\t\t14\t \n#define CP_STATS_SIZE\t\t64\t \n#define CP_REGS_SIZE\t\t(0xff + 1)\n#define CP_REGS_VER\t\t1\t\t \n#define CP_RX_RING_SIZE\t\t64\n#define CP_TX_RING_SIZE\t\t64\n#define CP_RING_BYTES\t\t\\\n\t\t((sizeof(struct cp_desc) * CP_RX_RING_SIZE) +\t\\\n\t\t (sizeof(struct cp_desc) * CP_TX_RING_SIZE) +\t\\\n\t\t CP_STATS_SIZE)\n#define NEXT_TX(N)\t\t(((N) + 1) & (CP_TX_RING_SIZE - 1))\n#define NEXT_RX(N)\t\t(((N) + 1) & (CP_RX_RING_SIZE - 1))\n#define TX_BUFFS_AVAIL(CP)\t\t\t\t\t\\\n\t(((CP)->tx_tail <= (CP)->tx_head) ?\t\t\t\\\n\t  (CP)->tx_tail + (CP_TX_RING_SIZE - 1) - (CP)->tx_head :\t\\\n\t  (CP)->tx_tail - (CP)->tx_head - 1)\n\n#define PKT_BUF_SZ\t\t1536\t \n#define CP_INTERNAL_PHY\t\t32\n\n \n#define RX_FIFO_THRESH\t\t5\t \n#define RX_DMA_BURST\t\t4\t \n#define TX_DMA_BURST\t\t6\t \n#define TX_EARLY_THRESH\t\t256\t \n\n \n#define TX_TIMEOUT\t\t(6*HZ)\n\n \n#define CP_MIN_MTU\t\t60\t \n#define CP_MAX_MTU\t\t4096\n\nenum {\n\t \n\tMAC0\t\t= 0x00,\t \n\tMAR0\t\t= 0x08,\t \n\tStatsAddr\t= 0x10,\t \n\tTxRingAddr\t= 0x20,  \n\tHiTxRingAddr\t= 0x28,  \n\tCmd\t\t= 0x37,  \n\tIntrMask\t= 0x3C,  \n\tIntrStatus\t= 0x3E,  \n\tTxConfig\t= 0x40,  \n\tChipVersion\t= 0x43,  \n\tRxConfig\t= 0x44,  \n\tRxMissed\t= 0x4C,\t \n\tCfg9346\t\t= 0x50,  \n\tConfig1\t\t= 0x52,  \n\tConfig3\t\t= 0x59,  \n\tConfig4\t\t= 0x5A,  \n\tMultiIntr\t= 0x5C,  \n\tBasicModeCtrl\t= 0x62,\t \n\tBasicModeStatus\t= 0x64,  \n\tNWayAdvert\t= 0x66,  \n\tNWayLPAR\t= 0x68,  \n\tNWayExpansion\t= 0x6A,  \n\tTxDmaOkLowDesc  = 0x82,  \n\tConfig5\t\t= 0xD8,\t \n\tTxPoll\t\t= 0xD9,\t \n\tRxMaxSize\t= 0xDA,  \n\tCpCmd\t\t= 0xE0,  \n\tIntrMitigate\t= 0xE2,\t \n\tRxRingAddr\t= 0xE4,  \n\tTxThresh\t= 0xEC,  \n\tOldRxBufAddr\t= 0x30,  \n\tOldTSD0\t\t= 0x10,  \n\n\t \n\tDescOwn\t\t= (1 << 31),  \n\tRingEnd\t\t= (1 << 30),  \n\tFirstFrag\t= (1 << 29),  \n\tLastFrag\t= (1 << 28),  \n\tLargeSend\t= (1 << 27),  \n\tMSSShift\t= 16,\t      \n\tMSSMask\t\t= 0x7ff,      \n\tTxError\t\t= (1 << 23),  \n\tRxError\t\t= (1 << 20),  \n\tIPCS\t\t= (1 << 18),  \n\tUDPCS\t\t= (1 << 17),  \n\tTCPCS\t\t= (1 << 16),  \n\tTxVlanTag\t= (1 << 17),  \n\tRxVlanTagged\t= (1 << 16),  \n\tIPFail\t\t= (1 << 15),  \n\tUDPFail\t\t= (1 << 14),  \n\tTCPFail\t\t= (1 << 13),  \n\tNormalTxPoll\t= (1 << 6),   \n\tPID1\t\t= (1 << 17),  \n\tPID0\t\t= (1 << 16),  \n\tRxProtoTCP\t= 1,\n\tRxProtoUDP\t= 2,\n\tRxProtoIP\t= 3,\n\tTxFIFOUnder\t= (1 << 25),  \n\tTxOWC\t\t= (1 << 22),  \n\tTxLinkFail\t= (1 << 21),  \n\tTxMaxCol\t= (1 << 20),  \n\tTxColCntShift\t= 16,\t      \n\tTxColCntMask\t= 0x01 | 0x02 | 0x04 | 0x08,  \n\tRxErrFrame\t= (1 << 27),  \n\tRxMcast\t\t= (1 << 26),  \n\tRxErrCRC\t= (1 << 18),  \n\tRxErrRunt\t= (1 << 19),  \n\tRxErrLong\t= (1 << 21),  \n\tRxErrFIFO\t= (1 << 22),  \n\n\t \n\tDumpStats\t= (1 << 3),   \n\n\t \n\tRxCfgFIFOShift\t= 13,\t      \n\tRxCfgDMAShift\t= 8,\t      \n\tAcceptErr\t= 0x20,\t      \n\tAcceptRunt\t= 0x10,\t      \n\tAcceptBroadcast\t= 0x08,\t      \n\tAcceptMulticast\t= 0x04,\t      \n\tAcceptMyPhys\t= 0x02,\t      \n\tAcceptAllPhys\t= 0x01,\t      \n\n\t \n\tPciErr\t\t= (1 << 15),  \n\tTimerIntr\t= (1 << 14),  \n\tLenChg\t\t= (1 << 13),  \n\tSWInt\t\t= (1 << 8),   \n\tTxEmpty\t\t= (1 << 7),   \n\tRxFIFOOvr\t= (1 << 6),   \n\tLinkChg\t\t= (1 << 5),   \n\tRxEmpty\t\t= (1 << 4),   \n\tTxErr\t\t= (1 << 3),   \n\tTxOK\t\t= (1 << 2),   \n\tRxErr\t\t= (1 << 1),   \n\tRxOK\t\t= (1 << 0),   \n\tIntrResvd\t= (1 << 10),  \n\n\tIntrAll\t\t= PciErr | TimerIntr | LenChg | SWInt | TxEmpty |\n\t\t\t  RxFIFOOvr | LinkChg | RxEmpty | TxErr | TxOK |\n\t\t\t  RxErr | RxOK | IntrResvd,\n\n\t \n\tCmdReset\t= (1 << 4),   \n\tRxOn\t\t= (1 << 3),   \n\tTxOn\t\t= (1 << 2),   \n\n\t \n\tRxVlanOn\t= (1 << 6),   \n\tRxChkSum\t= (1 << 5),   \n\tPCIDAC\t\t= (1 << 4),   \n\tPCIMulRW\t= (1 << 3),   \n\tCpRxOn\t\t= (1 << 1),   \n\tCpTxOn\t\t= (1 << 0),   \n\n\t \n\tCfg9346_Lock\t= 0x00,\t      \n\tCfg9346_Unlock\t= 0xC0,\t      \n\n\t \n\tIFG\t\t= (1 << 25) | (1 << 24),  \n\tTxDMAShift\t= 8,\t      \n\n\t \n\tTxThreshMask\t= 0x3f,\t      \n\tTxThreshMax\t= 2048,\t      \n\n\t \n\tDriverLoaded\t= (1 << 5),   \n\tLWACT           = (1 << 4),   \n\tPMEnable\t= (1 << 0),   \n\n\t \n\tPARMEnable\t= (1 << 6),   \n\tMagicPacket     = (1 << 5),   \n\tLinkUp          = (1 << 4),   \n\n\t \n\tLWPTN           = (1 << 1),   \n\tLWPME           = (1 << 4),   \n\n\t \n\tBWF             = (1 << 6),   \n\tMWF             = (1 << 5),   \n\tUWF             = (1 << 4),   \n\tLANWake         = (1 << 1),   \n\tPMEStatus\t= (1 << 0),   \n\n\tcp_norx_intr_mask = PciErr | LinkChg | TxOK | TxErr | TxEmpty,\n\tcp_rx_intr_mask = RxOK | RxErr | RxEmpty | RxFIFOOvr,\n\tcp_intr_mask = cp_rx_intr_mask | cp_norx_intr_mask,\n};\n\nstatic const unsigned int cp_rx_config =\n\t  (RX_FIFO_THRESH << RxCfgFIFOShift) |\n\t  (RX_DMA_BURST << RxCfgDMAShift);\n\nstruct cp_desc {\n\t__le32\t\topts1;\n\t__le32\t\topts2;\n\t__le64\t\taddr;\n};\n\nstruct cp_dma_stats {\n\t__le64\t\t\ttx_ok;\n\t__le64\t\t\trx_ok;\n\t__le64\t\t\ttx_err;\n\t__le32\t\t\trx_err;\n\t__le16\t\t\trx_fifo;\n\t__le16\t\t\tframe_align;\n\t__le32\t\t\ttx_ok_1col;\n\t__le32\t\t\ttx_ok_mcol;\n\t__le64\t\t\trx_ok_phys;\n\t__le64\t\t\trx_ok_bcast;\n\t__le32\t\t\trx_ok_mcast;\n\t__le16\t\t\ttx_abort;\n\t__le16\t\t\ttx_underrun;\n} __packed;\n\nstruct cp_extra_stats {\n\tunsigned long\t\trx_frags;\n};\n\nstruct cp_private {\n\tvoid\t\t\t__iomem *regs;\n\tstruct net_device\t*dev;\n\tspinlock_t\t\tlock;\n\tu32\t\t\tmsg_enable;\n\n\tstruct napi_struct\tnapi;\n\n\tstruct pci_dev\t\t*pdev;\n\tu32\t\t\trx_config;\n\tu16\t\t\tcpcmd;\n\n\tstruct cp_extra_stats\tcp_stats;\n\n\tunsigned\t\trx_head\t\t____cacheline_aligned;\n\tunsigned\t\trx_tail;\n\tstruct cp_desc\t\t*rx_ring;\n\tstruct sk_buff\t\t*rx_skb[CP_RX_RING_SIZE];\n\n\tunsigned\t\ttx_head\t\t____cacheline_aligned;\n\tunsigned\t\ttx_tail;\n\tstruct cp_desc\t\t*tx_ring;\n\tstruct sk_buff\t\t*tx_skb[CP_TX_RING_SIZE];\n\tu32\t\t\ttx_opts[CP_TX_RING_SIZE];\n\n\tunsigned\t\trx_buf_sz;\n\tunsigned\t\twol_enabled : 1;  \n\n\tdma_addr_t\t\tring_dma;\n\n\tstruct mii_if_info\tmii_if;\n};\n\n#define cpr8(reg)\treadb(cp->regs + (reg))\n#define cpr16(reg)\treadw(cp->regs + (reg))\n#define cpr32(reg)\treadl(cp->regs + (reg))\n#define cpw8(reg,val)\twriteb((val), cp->regs + (reg))\n#define cpw16(reg,val)\twritew((val), cp->regs + (reg))\n#define cpw32(reg,val)\twritel((val), cp->regs + (reg))\n#define cpw8_f(reg,val) do {\t\t\t\\\n\twriteb((val), cp->regs + (reg));\t\\\n\treadb(cp->regs + (reg));\t\t\\\n\t} while (0)\n#define cpw16_f(reg,val) do {\t\t\t\\\n\twritew((val), cp->regs + (reg));\t\\\n\treadw(cp->regs + (reg));\t\t\\\n\t} while (0)\n#define cpw32_f(reg,val) do {\t\t\t\\\n\twritel((val), cp->regs + (reg));\t\\\n\treadl(cp->regs + (reg));\t\t\\\n\t} while (0)\n\n\nstatic void __cp_set_rx_mode (struct net_device *dev);\nstatic void cp_tx (struct cp_private *cp);\nstatic void cp_clean_rings (struct cp_private *cp);\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void cp_poll_controller(struct net_device *dev);\n#endif\nstatic int cp_get_eeprom_len(struct net_device *dev);\nstatic int cp_get_eeprom(struct net_device *dev,\n\t\t\t struct ethtool_eeprom *eeprom, u8 *data);\nstatic int cp_set_eeprom(struct net_device *dev,\n\t\t\t struct ethtool_eeprom *eeprom, u8 *data);\n\nstatic struct {\n\tconst char str[ETH_GSTRING_LEN];\n} ethtool_stats_keys[] = {\n\t{ \"tx_ok\" },\n\t{ \"rx_ok\" },\n\t{ \"tx_err\" },\n\t{ \"rx_err\" },\n\t{ \"rx_fifo\" },\n\t{ \"frame_align\" },\n\t{ \"tx_ok_1col\" },\n\t{ \"tx_ok_mcol\" },\n\t{ \"rx_ok_phys\" },\n\t{ \"rx_ok_bcast\" },\n\t{ \"rx_ok_mcast\" },\n\t{ \"tx_abort\" },\n\t{ \"tx_underrun\" },\n\t{ \"rx_frags\" },\n};\n\n\nstatic inline void cp_set_rxbufsize (struct cp_private *cp)\n{\n\tunsigned int mtu = cp->dev->mtu;\n\n\tif (mtu > ETH_DATA_LEN)\n\t\t \n\t\tcp->rx_buf_sz = mtu + ETH_HLEN + 8;\n\telse\n\t\tcp->rx_buf_sz = PKT_BUF_SZ;\n}\n\nstatic inline void cp_rx_skb (struct cp_private *cp, struct sk_buff *skb,\n\t\t\t      struct cp_desc *desc)\n{\n\tu32 opts2 = le32_to_cpu(desc->opts2);\n\n\tskb->protocol = eth_type_trans (skb, cp->dev);\n\n\tcp->dev->stats.rx_packets++;\n\tcp->dev->stats.rx_bytes += skb->len;\n\n\tif (opts2 & RxVlanTagged)\n\t\t__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), swab16(opts2 & 0xffff));\n\n\tnapi_gro_receive(&cp->napi, skb);\n}\n\nstatic void cp_rx_err_acct (struct cp_private *cp, unsigned rx_tail,\n\t\t\t    u32 status, u32 len)\n{\n\tnetif_dbg(cp, rx_err, cp->dev, \"rx err, slot %d status 0x%x len %d\\n\",\n\t\t  rx_tail, status, len);\n\tcp->dev->stats.rx_errors++;\n\tif (status & RxErrFrame)\n\t\tcp->dev->stats.rx_frame_errors++;\n\tif (status & RxErrCRC)\n\t\tcp->dev->stats.rx_crc_errors++;\n\tif ((status & RxErrRunt) || (status & RxErrLong))\n\t\tcp->dev->stats.rx_length_errors++;\n\tif ((status & (FirstFrag | LastFrag)) != (FirstFrag | LastFrag))\n\t\tcp->dev->stats.rx_length_errors++;\n\tif (status & RxErrFIFO)\n\t\tcp->dev->stats.rx_fifo_errors++;\n}\n\nstatic inline unsigned int cp_rx_csum_ok (u32 status)\n{\n\tunsigned int protocol = (status >> 16) & 0x3;\n\n\tif (((protocol == RxProtoTCP) && !(status & TCPFail)) ||\n\t    ((protocol == RxProtoUDP) && !(status & UDPFail)))\n\t\treturn 1;\n\telse\n\t\treturn 0;\n}\n\nstatic int cp_rx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct cp_private *cp = container_of(napi, struct cp_private, napi);\n\tstruct net_device *dev = cp->dev;\n\tunsigned int rx_tail = cp->rx_tail;\n\tint rx = 0;\n\n\tcpw16(IntrStatus, cp_rx_intr_mask);\n\n\twhile (rx < budget) {\n\t\tu32 status, len;\n\t\tdma_addr_t mapping, new_mapping;\n\t\tstruct sk_buff *skb, *new_skb;\n\t\tstruct cp_desc *desc;\n\t\tconst unsigned buflen = cp->rx_buf_sz;\n\n\t\tskb = cp->rx_skb[rx_tail];\n\t\tBUG_ON(!skb);\n\n\t\tdesc = &cp->rx_ring[rx_tail];\n\t\tstatus = le32_to_cpu(desc->opts1);\n\t\tif (status & DescOwn)\n\t\t\tbreak;\n\n\t\tlen = (status & 0x1fff) - 4;\n\t\tmapping = le64_to_cpu(desc->addr);\n\n\t\tif ((status & (FirstFrag | LastFrag)) != (FirstFrag | LastFrag)) {\n\t\t\t \n\t\t\tcp_rx_err_acct(cp, rx_tail, status, len);\n\t\t\tdev->stats.rx_dropped++;\n\t\t\tcp->cp_stats.rx_frags++;\n\t\t\tgoto rx_next;\n\t\t}\n\n\t\tif (status & (RxError | RxErrFIFO)) {\n\t\t\tcp_rx_err_acct(cp, rx_tail, status, len);\n\t\t\tgoto rx_next;\n\t\t}\n\n\t\tnetif_dbg(cp, rx_status, dev, \"rx slot %d status 0x%x len %d\\n\",\n\t\t\t  rx_tail, status, len);\n\n\t\tnew_skb = napi_alloc_skb(napi, buflen);\n\t\tif (!new_skb) {\n\t\t\tdev->stats.rx_dropped++;\n\t\t\tgoto rx_next;\n\t\t}\n\n\t\tnew_mapping = dma_map_single(&cp->pdev->dev, new_skb->data, buflen,\n\t\t\t\t\t DMA_FROM_DEVICE);\n\t\tif (dma_mapping_error(&cp->pdev->dev, new_mapping)) {\n\t\t\tdev->stats.rx_dropped++;\n\t\t\tkfree_skb(new_skb);\n\t\t\tgoto rx_next;\n\t\t}\n\n\t\tdma_unmap_single(&cp->pdev->dev, mapping,\n\t\t\t\t buflen, DMA_FROM_DEVICE);\n\n\t\t \n\t\tif (cp_rx_csum_ok(status))\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\t\telse\n\t\t\tskb_checksum_none_assert(skb);\n\n\t\tskb_put(skb, len);\n\n\t\tcp->rx_skb[rx_tail] = new_skb;\n\n\t\tcp_rx_skb(cp, skb, desc);\n\t\trx++;\n\t\tmapping = new_mapping;\n\nrx_next:\n\t\tcp->rx_ring[rx_tail].opts2 = 0;\n\t\tcp->rx_ring[rx_tail].addr = cpu_to_le64(mapping);\n\t\tif (rx_tail == (CP_RX_RING_SIZE - 1))\n\t\t\tdesc->opts1 = cpu_to_le32(DescOwn | RingEnd |\n\t\t\t\t\t\t  cp->rx_buf_sz);\n\t\telse\n\t\t\tdesc->opts1 = cpu_to_le32(DescOwn | cp->rx_buf_sz);\n\t\trx_tail = NEXT_RX(rx_tail);\n\t}\n\n\tcp->rx_tail = rx_tail;\n\n\t \n\tif (rx < budget && napi_complete_done(napi, rx)) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&cp->lock, flags);\n\t\tcpw16_f(IntrMask, cp_intr_mask);\n\t\tspin_unlock_irqrestore(&cp->lock, flags);\n\t}\n\n\treturn rx;\n}\n\nstatic irqreturn_t cp_interrupt (int irq, void *dev_instance)\n{\n\tstruct net_device *dev = dev_instance;\n\tstruct cp_private *cp;\n\tint handled = 0;\n\tu16 status;\n\tu16 mask;\n\n\tif (unlikely(dev == NULL))\n\t\treturn IRQ_NONE;\n\tcp = netdev_priv(dev);\n\n\tspin_lock(&cp->lock);\n\n\tmask = cpr16(IntrMask);\n\tif (!mask)\n\t\tgoto out_unlock;\n\n\tstatus = cpr16(IntrStatus);\n\tif (!status || (status == 0xFFFF))\n\t\tgoto out_unlock;\n\n\thandled = 1;\n\n\tnetif_dbg(cp, intr, dev, \"intr, status %04x cmd %02x cpcmd %04x\\n\",\n\t\t  status, cpr8(Cmd), cpr16(CpCmd));\n\n\tcpw16(IntrStatus, status & ~cp_rx_intr_mask);\n\n\t \n\tif (unlikely(!netif_running(dev))) {\n\t\tcpw16(IntrMask, 0);\n\t\tgoto out_unlock;\n\t}\n\n\tif (status & (RxOK | RxErr | RxEmpty | RxFIFOOvr))\n\t\tif (napi_schedule_prep(&cp->napi)) {\n\t\t\tcpw16_f(IntrMask, cp_norx_intr_mask);\n\t\t\t__napi_schedule(&cp->napi);\n\t\t}\n\n\tif (status & (TxOK | TxErr | TxEmpty | SWInt))\n\t\tcp_tx(cp);\n\tif (status & LinkChg)\n\t\tmii_check_media(&cp->mii_if, netif_msg_link(cp), false);\n\n\n\tif (status & PciErr) {\n\t\tu16 pci_status;\n\n\t\tpci_read_config_word(cp->pdev, PCI_STATUS, &pci_status);\n\t\tpci_write_config_word(cp->pdev, PCI_STATUS, pci_status);\n\t\tnetdev_err(dev, \"PCI bus error, status=%04x, PCI status=%04x\\n\",\n\t\t\t   status, pci_status);\n\n\t\t \n\t}\n\nout_unlock:\n\tspin_unlock(&cp->lock);\n\n\treturn IRQ_RETVAL(handled);\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\n \nstatic void cp_poll_controller(struct net_device *dev)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tconst int irq = cp->pdev->irq;\n\n\tdisable_irq(irq);\n\tcp_interrupt(irq, dev);\n\tenable_irq(irq);\n}\n#endif\n\nstatic void cp_tx (struct cp_private *cp)\n{\n\tunsigned tx_head = cp->tx_head;\n\tunsigned tx_tail = cp->tx_tail;\n\tunsigned bytes_compl = 0, pkts_compl = 0;\n\n\twhile (tx_tail != tx_head) {\n\t\tstruct cp_desc *txd = cp->tx_ring + tx_tail;\n\t\tstruct sk_buff *skb;\n\t\tu32 status;\n\n\t\trmb();\n\t\tstatus = le32_to_cpu(txd->opts1);\n\t\tif (status & DescOwn)\n\t\t\tbreak;\n\n\t\tskb = cp->tx_skb[tx_tail];\n\t\tBUG_ON(!skb);\n\n\t\tdma_unmap_single(&cp->pdev->dev, le64_to_cpu(txd->addr),\n\t\t\t\t cp->tx_opts[tx_tail] & 0xffff,\n\t\t\t\t DMA_TO_DEVICE);\n\n\t\tif (status & LastFrag) {\n\t\t\tif (status & (TxError | TxFIFOUnder)) {\n\t\t\t\tnetif_dbg(cp, tx_err, cp->dev,\n\t\t\t\t\t  \"tx err, status 0x%x\\n\", status);\n\t\t\t\tcp->dev->stats.tx_errors++;\n\t\t\t\tif (status & TxOWC)\n\t\t\t\t\tcp->dev->stats.tx_window_errors++;\n\t\t\t\tif (status & TxMaxCol)\n\t\t\t\t\tcp->dev->stats.tx_aborted_errors++;\n\t\t\t\tif (status & TxLinkFail)\n\t\t\t\t\tcp->dev->stats.tx_carrier_errors++;\n\t\t\t\tif (status & TxFIFOUnder)\n\t\t\t\t\tcp->dev->stats.tx_fifo_errors++;\n\t\t\t} else {\n\t\t\t\tcp->dev->stats.collisions +=\n\t\t\t\t\t((status >> TxColCntShift) & TxColCntMask);\n\t\t\t\tcp->dev->stats.tx_packets++;\n\t\t\t\tcp->dev->stats.tx_bytes += skb->len;\n\t\t\t\tnetif_dbg(cp, tx_done, cp->dev,\n\t\t\t\t\t  \"tx done, slot %d\\n\", tx_tail);\n\t\t\t}\n\t\t\tbytes_compl += skb->len;\n\t\t\tpkts_compl++;\n\t\t\tdev_consume_skb_irq(skb);\n\t\t}\n\n\t\tcp->tx_skb[tx_tail] = NULL;\n\n\t\ttx_tail = NEXT_TX(tx_tail);\n\t}\n\n\tcp->tx_tail = tx_tail;\n\n\tnetdev_completed_queue(cp->dev, pkts_compl, bytes_compl);\n\tif (TX_BUFFS_AVAIL(cp) > (MAX_SKB_FRAGS + 1))\n\t\tnetif_wake_queue(cp->dev);\n}\n\nstatic inline u32 cp_tx_vlan_tag(struct sk_buff *skb)\n{\n\treturn skb_vlan_tag_present(skb) ?\n\t\tTxVlanTag | swab16(skb_vlan_tag_get(skb)) : 0x00;\n}\n\nstatic void unwind_tx_frag_mapping(struct cp_private *cp, struct sk_buff *skb,\n\t\t\t\t   int first, int entry_last)\n{\n\tint frag, index;\n\tstruct cp_desc *txd;\n\tskb_frag_t *this_frag;\n\tfor (frag = 0; frag+first < entry_last; frag++) {\n\t\tindex = first+frag;\n\t\tcp->tx_skb[index] = NULL;\n\t\ttxd = &cp->tx_ring[index];\n\t\tthis_frag = &skb_shinfo(skb)->frags[frag];\n\t\tdma_unmap_single(&cp->pdev->dev, le64_to_cpu(txd->addr),\n\t\t\t\t skb_frag_size(this_frag), DMA_TO_DEVICE);\n\t}\n}\n\nstatic netdev_tx_t cp_start_xmit (struct sk_buff *skb,\n\t\t\t\t\tstruct net_device *dev)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tunsigned entry;\n\tu32 eor, opts1;\n\tunsigned long intr_flags;\n\t__le32 opts2;\n\tint mss = 0;\n\n\tspin_lock_irqsave(&cp->lock, intr_flags);\n\n\t \n\tif (TX_BUFFS_AVAIL(cp) <= (skb_shinfo(skb)->nr_frags + 1)) {\n\t\tnetif_stop_queue(dev);\n\t\tspin_unlock_irqrestore(&cp->lock, intr_flags);\n\t\tnetdev_err(dev, \"BUG! Tx Ring full when queue awake!\\n\");\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\tentry = cp->tx_head;\n\teor = (entry == (CP_TX_RING_SIZE - 1)) ? RingEnd : 0;\n\tmss = skb_shinfo(skb)->gso_size;\n\n\tif (mss > MSSMask) {\n\t\tnetdev_WARN_ONCE(dev, \"Net bug: GSO size %d too large for 8139CP\\n\",\n\t\t\t\t mss);\n\t\tgoto out_dma_error;\n\t}\n\n\topts2 = cpu_to_le32(cp_tx_vlan_tag(skb));\n\topts1 = DescOwn;\n\tif (mss)\n\t\topts1 |= LargeSend | (mss << MSSShift);\n\telse if (skb->ip_summed == CHECKSUM_PARTIAL) {\n\t\tconst struct iphdr *ip = ip_hdr(skb);\n\t\tif (ip->protocol == IPPROTO_TCP)\n\t\t\topts1 |= IPCS | TCPCS;\n\t\telse if (ip->protocol == IPPROTO_UDP)\n\t\t\topts1 |= IPCS | UDPCS;\n\t\telse {\n\t\t\tWARN_ONCE(1,\n\t\t\t\t  \"Net bug: asked to checksum invalid Legacy IP packet\\n\");\n\t\t\tgoto out_dma_error;\n\t\t}\n\t}\n\n\tif (skb_shinfo(skb)->nr_frags == 0) {\n\t\tstruct cp_desc *txd = &cp->tx_ring[entry];\n\t\tu32 len;\n\t\tdma_addr_t mapping;\n\n\t\tlen = skb->len;\n\t\tmapping = dma_map_single(&cp->pdev->dev, skb->data, len, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&cp->pdev->dev, mapping))\n\t\t\tgoto out_dma_error;\n\n\t\ttxd->opts2 = opts2;\n\t\ttxd->addr = cpu_to_le64(mapping);\n\t\twmb();\n\n\t\topts1 |= eor | len | FirstFrag | LastFrag;\n\n\t\ttxd->opts1 = cpu_to_le32(opts1);\n\t\twmb();\n\n\t\tcp->tx_skb[entry] = skb;\n\t\tcp->tx_opts[entry] = opts1;\n\t\tnetif_dbg(cp, tx_queued, cp->dev, \"tx queued, slot %d, skblen %d\\n\",\n\t\t\t  entry, skb->len);\n\t} else {\n\t\tstruct cp_desc *txd;\n\t\tu32 first_len, first_eor, ctrl;\n\t\tdma_addr_t first_mapping;\n\t\tint frag, first_entry = entry;\n\n\t\t \n\t\tfirst_eor = eor;\n\t\tfirst_len = skb_headlen(skb);\n\t\tfirst_mapping = dma_map_single(&cp->pdev->dev, skb->data,\n\t\t\t\t\t       first_len, DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(&cp->pdev->dev, first_mapping))\n\t\t\tgoto out_dma_error;\n\n\t\tcp->tx_skb[entry] = skb;\n\n\t\tfor (frag = 0; frag < skb_shinfo(skb)->nr_frags; frag++) {\n\t\t\tconst skb_frag_t *this_frag = &skb_shinfo(skb)->frags[frag];\n\t\t\tu32 len;\n\t\t\tdma_addr_t mapping;\n\n\t\t\tentry = NEXT_TX(entry);\n\n\t\t\tlen = skb_frag_size(this_frag);\n\t\t\tmapping = dma_map_single(&cp->pdev->dev,\n\t\t\t\t\t\t skb_frag_address(this_frag),\n\t\t\t\t\t\t len, DMA_TO_DEVICE);\n\t\t\tif (dma_mapping_error(&cp->pdev->dev, mapping)) {\n\t\t\t\tunwind_tx_frag_mapping(cp, skb, first_entry, entry);\n\t\t\t\tgoto out_dma_error;\n\t\t\t}\n\n\t\t\teor = (entry == (CP_TX_RING_SIZE - 1)) ? RingEnd : 0;\n\n\t\t\tctrl = opts1 | eor | len;\n\n\t\t\tif (frag == skb_shinfo(skb)->nr_frags - 1)\n\t\t\t\tctrl |= LastFrag;\n\n\t\t\ttxd = &cp->tx_ring[entry];\n\t\t\ttxd->opts2 = opts2;\n\t\t\ttxd->addr = cpu_to_le64(mapping);\n\t\t\twmb();\n\n\t\t\ttxd->opts1 = cpu_to_le32(ctrl);\n\t\t\twmb();\n\n\t\t\tcp->tx_opts[entry] = ctrl;\n\t\t\tcp->tx_skb[entry] = skb;\n\t\t}\n\n\t\ttxd = &cp->tx_ring[first_entry];\n\t\ttxd->opts2 = opts2;\n\t\ttxd->addr = cpu_to_le64(first_mapping);\n\t\twmb();\n\n\t\tctrl = opts1 | first_eor | first_len | FirstFrag;\n\t\ttxd->opts1 = cpu_to_le32(ctrl);\n\t\twmb();\n\n\t\tcp->tx_opts[first_entry] = ctrl;\n\t\tnetif_dbg(cp, tx_queued, cp->dev, \"tx queued, slots %d-%d, skblen %d\\n\",\n\t\t\t  first_entry, entry, skb->len);\n\t}\n\tcp->tx_head = NEXT_TX(entry);\n\n\tnetdev_sent_queue(dev, skb->len);\n\tif (TX_BUFFS_AVAIL(cp) <= (MAX_SKB_FRAGS + 1))\n\t\tnetif_stop_queue(dev);\n\nout_unlock:\n\tspin_unlock_irqrestore(&cp->lock, intr_flags);\n\n\tcpw8(TxPoll, NormalTxPoll);\n\n\treturn NETDEV_TX_OK;\nout_dma_error:\n\tdev_kfree_skb_any(skb);\n\tcp->dev->stats.tx_dropped++;\n\tgoto out_unlock;\n}\n\n \n\nstatic void __cp_set_rx_mode (struct net_device *dev)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tu32 mc_filter[2];\t \n\tint rx_mode;\n\n\t \n\tif (dev->flags & IFF_PROMISC) {\n\t\t \n\t\trx_mode =\n\t\t    AcceptBroadcast | AcceptMulticast | AcceptMyPhys |\n\t\t    AcceptAllPhys;\n\t\tmc_filter[1] = mc_filter[0] = 0xffffffff;\n\t} else if ((netdev_mc_count(dev) > multicast_filter_limit) ||\n\t\t   (dev->flags & IFF_ALLMULTI)) {\n\t\t \n\t\trx_mode = AcceptBroadcast | AcceptMulticast | AcceptMyPhys;\n\t\tmc_filter[1] = mc_filter[0] = 0xffffffff;\n\t} else {\n\t\tstruct netdev_hw_addr *ha;\n\t\trx_mode = AcceptBroadcast | AcceptMyPhys;\n\t\tmc_filter[1] = mc_filter[0] = 0;\n\t\tnetdev_for_each_mc_addr(ha, dev) {\n\t\t\tint bit_nr = ether_crc(ETH_ALEN, ha->addr) >> 26;\n\n\t\t\tmc_filter[bit_nr >> 5] |= 1 << (bit_nr & 31);\n\t\t\trx_mode |= AcceptMulticast;\n\t\t}\n\t}\n\n\t \n\tcp->rx_config = cp_rx_config | rx_mode;\n\tcpw32_f(RxConfig, cp->rx_config);\n\n\tcpw32_f (MAR0 + 0, mc_filter[0]);\n\tcpw32_f (MAR0 + 4, mc_filter[1]);\n}\n\nstatic void cp_set_rx_mode (struct net_device *dev)\n{\n\tunsigned long flags;\n\tstruct cp_private *cp = netdev_priv(dev);\n\n\tspin_lock_irqsave (&cp->lock, flags);\n\t__cp_set_rx_mode(dev);\n\tspin_unlock_irqrestore (&cp->lock, flags);\n}\n\nstatic void __cp_get_stats(struct cp_private *cp)\n{\n\t \n\tcp->dev->stats.rx_missed_errors += (cpr32 (RxMissed) & 0xffffff);\n\tcpw32 (RxMissed, 0);\n}\n\nstatic struct net_device_stats *cp_get_stats(struct net_device *dev)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&cp->lock, flags);\n\tif (netif_running(dev) && netif_device_present(dev))\n\t\t__cp_get_stats(cp);\n\tspin_unlock_irqrestore(&cp->lock, flags);\n\n\treturn &dev->stats;\n}\n\nstatic void cp_stop_hw (struct cp_private *cp)\n{\n\tcpw16(IntrStatus, ~(cpr16(IntrStatus)));\n\tcpw16_f(IntrMask, 0);\n\tcpw8(Cmd, 0);\n\tcpw16_f(CpCmd, 0);\n\tcpw16_f(IntrStatus, ~(cpr16(IntrStatus)));\n\n\tcp->rx_tail = 0;\n\tcp->tx_head = cp->tx_tail = 0;\n\n\tnetdev_reset_queue(cp->dev);\n}\n\nstatic void cp_reset_hw (struct cp_private *cp)\n{\n\tunsigned work = 1000;\n\n\tcpw8(Cmd, CmdReset);\n\n\twhile (work--) {\n\t\tif (!(cpr8(Cmd) & CmdReset))\n\t\t\treturn;\n\n\t\tschedule_timeout_uninterruptible(10);\n\t}\n\n\tnetdev_err(cp->dev, \"hardware reset timeout\\n\");\n}\n\nstatic inline void cp_start_hw (struct cp_private *cp)\n{\n\tdma_addr_t ring_dma;\n\n\tcpw16(CpCmd, cp->cpcmd);\n\n\t \n\tcpw32_f(HiTxRingAddr, 0);\n\tcpw32_f(HiTxRingAddr + 4, 0);\n\n\tring_dma = cp->ring_dma;\n\tcpw32_f(RxRingAddr, ring_dma & 0xffffffff);\n\tcpw32_f(RxRingAddr + 4, (ring_dma >> 16) >> 16);\n\n\tring_dma += sizeof(struct cp_desc) * CP_RX_RING_SIZE;\n\tcpw32_f(TxRingAddr, ring_dma & 0xffffffff);\n\tcpw32_f(TxRingAddr + 4, (ring_dma >> 16) >> 16);\n\n\t \n\tcpw8(Cmd, RxOn | TxOn);\n\n\tnetdev_reset_queue(cp->dev);\n}\n\nstatic void cp_enable_irq(struct cp_private *cp)\n{\n\tcpw16_f(IntrMask, cp_intr_mask);\n}\n\nstatic void cp_init_hw (struct cp_private *cp)\n{\n\tstruct net_device *dev = cp->dev;\n\n\tcp_reset_hw(cp);\n\n\tcpw8_f (Cfg9346, Cfg9346_Unlock);\n\n\t \n\tcpw32_f (MAC0 + 0, le32_to_cpu (*(__le32 *) (dev->dev_addr + 0)));\n\tcpw32_f (MAC0 + 4, le32_to_cpu (*(__le32 *) (dev->dev_addr + 4)));\n\n\tcp_start_hw(cp);\n\tcpw8(TxThresh, 0x06);  \n\n\t__cp_set_rx_mode(dev);\n\tcpw32_f (TxConfig, IFG | (TX_DMA_BURST << TxDMAShift));\n\n\tcpw8(Config1, cpr8(Config1) | DriverLoaded | PMEnable);\n\t \n\tcpw8(Config3, PARMEnable);\n\tcp->wol_enabled = 0;\n\n\tcpw8(Config5, cpr8(Config5) & PMEStatus);\n\n\tcpw16(MultiIntr, 0);\n\n\tcpw8_f(Cfg9346, Cfg9346_Lock);\n}\n\nstatic int cp_refill_rx(struct cp_private *cp)\n{\n\tstruct net_device *dev = cp->dev;\n\tunsigned i;\n\n\tfor (i = 0; i < CP_RX_RING_SIZE; i++) {\n\t\tstruct sk_buff *skb;\n\t\tdma_addr_t mapping;\n\n\t\tskb = netdev_alloc_skb_ip_align(dev, cp->rx_buf_sz);\n\t\tif (!skb)\n\t\t\tgoto err_out;\n\n\t\tmapping = dma_map_single(&cp->pdev->dev, skb->data,\n\t\t\t\t\t cp->rx_buf_sz, DMA_FROM_DEVICE);\n\t\tif (dma_mapping_error(&cp->pdev->dev, mapping)) {\n\t\t\tkfree_skb(skb);\n\t\t\tgoto err_out;\n\t\t}\n\t\tcp->rx_skb[i] = skb;\n\n\t\tcp->rx_ring[i].opts2 = 0;\n\t\tcp->rx_ring[i].addr = cpu_to_le64(mapping);\n\t\tif (i == (CP_RX_RING_SIZE - 1))\n\t\t\tcp->rx_ring[i].opts1 =\n\t\t\t\tcpu_to_le32(DescOwn | RingEnd | cp->rx_buf_sz);\n\t\telse\n\t\t\tcp->rx_ring[i].opts1 =\n\t\t\t\tcpu_to_le32(DescOwn | cp->rx_buf_sz);\n\t}\n\n\treturn 0;\n\nerr_out:\n\tcp_clean_rings(cp);\n\treturn -ENOMEM;\n}\n\nstatic void cp_init_rings_index (struct cp_private *cp)\n{\n\tcp->rx_tail = 0;\n\tcp->tx_head = cp->tx_tail = 0;\n}\n\nstatic int cp_init_rings (struct cp_private *cp)\n{\n\tmemset(cp->tx_ring, 0, sizeof(struct cp_desc) * CP_TX_RING_SIZE);\n\tcp->tx_ring[CP_TX_RING_SIZE - 1].opts1 = cpu_to_le32(RingEnd);\n\tmemset(cp->tx_opts, 0, sizeof(cp->tx_opts));\n\n\tcp_init_rings_index(cp);\n\n\treturn cp_refill_rx (cp);\n}\n\nstatic int cp_alloc_rings (struct cp_private *cp)\n{\n\tstruct device *d = &cp->pdev->dev;\n\tvoid *mem;\n\tint rc;\n\n\tmem = dma_alloc_coherent(d, CP_RING_BYTES, &cp->ring_dma, GFP_KERNEL);\n\tif (!mem)\n\t\treturn -ENOMEM;\n\n\tcp->rx_ring = mem;\n\tcp->tx_ring = &cp->rx_ring[CP_RX_RING_SIZE];\n\n\trc = cp_init_rings(cp);\n\tif (rc < 0)\n\t\tdma_free_coherent(d, CP_RING_BYTES, cp->rx_ring, cp->ring_dma);\n\n\treturn rc;\n}\n\nstatic void cp_clean_rings (struct cp_private *cp)\n{\n\tstruct cp_desc *desc;\n\tunsigned i;\n\n\tfor (i = 0; i < CP_RX_RING_SIZE; i++) {\n\t\tif (cp->rx_skb[i]) {\n\t\t\tdesc = cp->rx_ring + i;\n\t\t\tdma_unmap_single(&cp->pdev->dev,le64_to_cpu(desc->addr),\n\t\t\t\t\t cp->rx_buf_sz, DMA_FROM_DEVICE);\n\t\t\tdev_kfree_skb_any(cp->rx_skb[i]);\n\t\t}\n\t}\n\n\tfor (i = 0; i < CP_TX_RING_SIZE; i++) {\n\t\tif (cp->tx_skb[i]) {\n\t\t\tstruct sk_buff *skb = cp->tx_skb[i];\n\n\t\t\tdesc = cp->tx_ring + i;\n\t\t\tdma_unmap_single(&cp->pdev->dev,le64_to_cpu(desc->addr),\n\t\t\t\t\t le32_to_cpu(desc->opts1) & 0xffff,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\t\tif (le32_to_cpu(desc->opts1) & LastFrag)\n\t\t\t\tdev_kfree_skb_any(skb);\n\t\t\tcp->dev->stats.tx_dropped++;\n\t\t}\n\t}\n\tnetdev_reset_queue(cp->dev);\n\n\tmemset(cp->rx_ring, 0, sizeof(struct cp_desc) * CP_RX_RING_SIZE);\n\tmemset(cp->tx_ring, 0, sizeof(struct cp_desc) * CP_TX_RING_SIZE);\n\tmemset(cp->tx_opts, 0, sizeof(cp->tx_opts));\n\n\tmemset(cp->rx_skb, 0, sizeof(struct sk_buff *) * CP_RX_RING_SIZE);\n\tmemset(cp->tx_skb, 0, sizeof(struct sk_buff *) * CP_TX_RING_SIZE);\n}\n\nstatic void cp_free_rings (struct cp_private *cp)\n{\n\tcp_clean_rings(cp);\n\tdma_free_coherent(&cp->pdev->dev, CP_RING_BYTES, cp->rx_ring,\n\t\t\t  cp->ring_dma);\n\tcp->rx_ring = NULL;\n\tcp->tx_ring = NULL;\n}\n\nstatic int cp_open (struct net_device *dev)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tconst int irq = cp->pdev->irq;\n\tint rc;\n\n\tnetif_dbg(cp, ifup, dev, \"enabling interface\\n\");\n\n\trc = cp_alloc_rings(cp);\n\tif (rc)\n\t\treturn rc;\n\n\tnapi_enable(&cp->napi);\n\n\tcp_init_hw(cp);\n\n\trc = request_irq(irq, cp_interrupt, IRQF_SHARED, dev->name, dev);\n\tif (rc)\n\t\tgoto err_out_hw;\n\n\tcp_enable_irq(cp);\n\n\tnetif_carrier_off(dev);\n\tmii_check_media(&cp->mii_if, netif_msg_link(cp), true);\n\tnetif_start_queue(dev);\n\n\treturn 0;\n\nerr_out_hw:\n\tnapi_disable(&cp->napi);\n\tcp_stop_hw(cp);\n\tcp_free_rings(cp);\n\treturn rc;\n}\n\nstatic int cp_close (struct net_device *dev)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tunsigned long flags;\n\n\tnapi_disable(&cp->napi);\n\n\tnetif_dbg(cp, ifdown, dev, \"disabling interface\\n\");\n\n\tspin_lock_irqsave(&cp->lock, flags);\n\n\tnetif_stop_queue(dev);\n\tnetif_carrier_off(dev);\n\n\tcp_stop_hw(cp);\n\n\tspin_unlock_irqrestore(&cp->lock, flags);\n\n\tfree_irq(cp->pdev->irq, dev);\n\n\tcp_free_rings(cp);\n\treturn 0;\n}\n\nstatic void cp_tx_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tunsigned long flags;\n\tint i;\n\n\tnetdev_warn(dev, \"Transmit timeout, status %2x %4x %4x %4x\\n\",\n\t\t    cpr8(Cmd), cpr16(CpCmd),\n\t\t    cpr16(IntrStatus), cpr16(IntrMask));\n\n\tspin_lock_irqsave(&cp->lock, flags);\n\n\tnetif_dbg(cp, tx_err, cp->dev, \"TX ring head %d tail %d desc %x\\n\",\n\t\t  cp->tx_head, cp->tx_tail, cpr16(TxDmaOkLowDesc));\n\tfor (i = 0; i < CP_TX_RING_SIZE; i++) {\n\t\tnetif_dbg(cp, tx_err, cp->dev,\n\t\t\t  \"TX slot %d @%p: %08x (%08x) %08x %llx %p\\n\",\n\t\t\t  i, &cp->tx_ring[i], le32_to_cpu(cp->tx_ring[i].opts1),\n\t\t\t  cp->tx_opts[i], le32_to_cpu(cp->tx_ring[i].opts2),\n\t\t\t  le64_to_cpu(cp->tx_ring[i].addr),\n\t\t\t  cp->tx_skb[i]);\n\t}\n\n\tcp_stop_hw(cp);\n\tcp_clean_rings(cp);\n\tcp_init_rings(cp);\n\tcp_start_hw(cp);\n\t__cp_set_rx_mode(dev);\n\tcpw16_f(IntrMask, cp_norx_intr_mask);\n\n\tnetif_wake_queue(dev);\n\tnapi_schedule_irqoff(&cp->napi);\n\n\tspin_unlock_irqrestore(&cp->lock, flags);\n}\n\nstatic int cp_change_mtu(struct net_device *dev, int new_mtu)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\n\t \n\tif (!netif_running(dev)) {\n\t\tdev->mtu = new_mtu;\n\t\tcp_set_rxbufsize(cp);\t \n\t\treturn 0;\n\t}\n\n\t \n\tcp_close(dev);\n\tdev->mtu = new_mtu;\n\tcp_set_rxbufsize(cp);\n\treturn cp_open(dev);\n}\n\nstatic const char mii_2_8139_map[8] = {\n\tBasicModeCtrl,\n\tBasicModeStatus,\n\t0,\n\t0,\n\tNWayAdvert,\n\tNWayLPAR,\n\tNWayExpansion,\n\t0\n};\n\nstatic int mdio_read(struct net_device *dev, int phy_id, int location)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\n\treturn location < 8 && mii_2_8139_map[location] ?\n\t       readw(cp->regs + mii_2_8139_map[location]) : 0;\n}\n\n\nstatic void mdio_write(struct net_device *dev, int phy_id, int location,\n\t\t       int value)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\n\tif (location == 0) {\n\t\tcpw8(Cfg9346, Cfg9346_Unlock);\n\t\tcpw16(BasicModeCtrl, value);\n\t\tcpw8(Cfg9346, Cfg9346_Lock);\n\t} else if (location < 8 && mii_2_8139_map[location])\n\t\tcpw16(mii_2_8139_map[location], value);\n}\n\n \nstatic int netdev_set_wol (struct cp_private *cp,\n\t\t\t   const struct ethtool_wolinfo *wol)\n{\n\tu8 options;\n\n\toptions = cpr8 (Config3) & ~(LinkUp | MagicPacket);\n\t \n\tif (wol->wolopts) {\n\t\tif (wol->wolopts & WAKE_PHY)\toptions |= LinkUp;\n\t\tif (wol->wolopts & WAKE_MAGIC)\toptions |= MagicPacket;\n\t}\n\n\tcpw8 (Cfg9346, Cfg9346_Unlock);\n\tcpw8 (Config3, options);\n\tcpw8 (Cfg9346, Cfg9346_Lock);\n\n\toptions = 0;  \n\toptions = cpr8 (Config5) & ~(UWF | MWF | BWF);\n\t \n\tif (wol->wolopts) {\n\t\tif (wol->wolopts & WAKE_UCAST)  options |= UWF;\n\t\tif (wol->wolopts & WAKE_BCAST)\toptions |= BWF;\n\t\tif (wol->wolopts & WAKE_MCAST)\toptions |= MWF;\n\t}\n\n\tcpw8 (Config5, options);\n\n\tcp->wol_enabled = (wol->wolopts) ? 1 : 0;\n\n\treturn 0;\n}\n\n \nstatic void netdev_get_wol (struct cp_private *cp,\n\t             struct ethtool_wolinfo *wol)\n{\n\tu8 options;\n\n\twol->wolopts   = 0;  \n\twol->supported = WAKE_PHY   | WAKE_BCAST | WAKE_MAGIC |\n\t\t         WAKE_MCAST | WAKE_UCAST;\n\t \n\tif (!cp->wol_enabled) return;\n\n\toptions        = cpr8 (Config3);\n\tif (options & LinkUp)        wol->wolopts |= WAKE_PHY;\n\tif (options & MagicPacket)   wol->wolopts |= WAKE_MAGIC;\n\n\toptions        = 0;  \n\toptions        = cpr8 (Config5);\n\tif (options & UWF)           wol->wolopts |= WAKE_UCAST;\n\tif (options & BWF)           wol->wolopts |= WAKE_BCAST;\n\tif (options & MWF)           wol->wolopts |= WAKE_MCAST;\n}\n\nstatic void cp_get_drvinfo (struct net_device *dev, struct ethtool_drvinfo *info)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\n\tstrscpy(info->driver, DRV_NAME, sizeof(info->driver));\n\tstrscpy(info->version, DRV_VERSION, sizeof(info->version));\n\tstrscpy(info->bus_info, pci_name(cp->pdev), sizeof(info->bus_info));\n}\n\nstatic void cp_get_ringparam(struct net_device *dev,\n\t\t\t     struct ethtool_ringparam *ring,\n\t\t\t     struct kernel_ethtool_ringparam *kernel_ring,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tring->rx_max_pending = CP_RX_RING_SIZE;\n\tring->tx_max_pending = CP_TX_RING_SIZE;\n\tring->rx_pending = CP_RX_RING_SIZE;\n\tring->tx_pending = CP_TX_RING_SIZE;\n}\n\nstatic int cp_get_regs_len(struct net_device *dev)\n{\n\treturn CP_REGS_SIZE;\n}\n\nstatic int cp_get_sset_count (struct net_device *dev, int sset)\n{\n\tswitch (sset) {\n\tcase ETH_SS_STATS:\n\t\treturn CP_NUM_STATS;\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic int cp_get_link_ksettings(struct net_device *dev,\n\t\t\t\t struct ethtool_link_ksettings *cmd)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cp->lock, flags);\n\tmii_ethtool_get_link_ksettings(&cp->mii_if, cmd);\n\tspin_unlock_irqrestore(&cp->lock, flags);\n\n\treturn 0;\n}\n\nstatic int cp_set_link_ksettings(struct net_device *dev,\n\t\t\t\t const struct ethtool_link_ksettings *cmd)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tint rc;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&cp->lock, flags);\n\trc = mii_ethtool_set_link_ksettings(&cp->mii_if, cmd);\n\tspin_unlock_irqrestore(&cp->lock, flags);\n\n\treturn rc;\n}\n\nstatic int cp_nway_reset(struct net_device *dev)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\treturn mii_nway_restart(&cp->mii_if);\n}\n\nstatic u32 cp_get_msglevel(struct net_device *dev)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\treturn cp->msg_enable;\n}\n\nstatic void cp_set_msglevel(struct net_device *dev, u32 value)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tcp->msg_enable = value;\n}\n\nstatic int cp_set_features(struct net_device *dev, netdev_features_t features)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tunsigned long flags;\n\n\tif (!((dev->features ^ features) & NETIF_F_RXCSUM))\n\t\treturn 0;\n\n\tspin_lock_irqsave(&cp->lock, flags);\n\n\tif (features & NETIF_F_RXCSUM)\n\t\tcp->cpcmd |= RxChkSum;\n\telse\n\t\tcp->cpcmd &= ~RxChkSum;\n\n\tif (features & NETIF_F_HW_VLAN_CTAG_RX)\n\t\tcp->cpcmd |= RxVlanOn;\n\telse\n\t\tcp->cpcmd &= ~RxVlanOn;\n\n\tcpw16_f(CpCmd, cp->cpcmd);\n\tspin_unlock_irqrestore(&cp->lock, flags);\n\n\treturn 0;\n}\n\nstatic void cp_get_regs(struct net_device *dev, struct ethtool_regs *regs,\n\t\t        void *p)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tunsigned long flags;\n\n\tif (regs->len < CP_REGS_SIZE)\n\t\treturn  ;\n\n\tregs->version = CP_REGS_VER;\n\n\tspin_lock_irqsave(&cp->lock, flags);\n\tmemcpy_fromio(p, cp->regs, CP_REGS_SIZE);\n\tspin_unlock_irqrestore(&cp->lock, flags);\n}\n\nstatic void cp_get_wol (struct net_device *dev, struct ethtool_wolinfo *wol)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tunsigned long flags;\n\n\tspin_lock_irqsave (&cp->lock, flags);\n\tnetdev_get_wol (cp, wol);\n\tspin_unlock_irqrestore (&cp->lock, flags);\n}\n\nstatic int cp_set_wol (struct net_device *dev, struct ethtool_wolinfo *wol)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tunsigned long flags;\n\tint rc;\n\n\tspin_lock_irqsave (&cp->lock, flags);\n\trc = netdev_set_wol (cp, wol);\n\tspin_unlock_irqrestore (&cp->lock, flags);\n\n\treturn rc;\n}\n\nstatic void cp_get_strings (struct net_device *dev, u32 stringset, u8 *buf)\n{\n\tswitch (stringset) {\n\tcase ETH_SS_STATS:\n\t\tmemcpy(buf, &ethtool_stats_keys, sizeof(ethtool_stats_keys));\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t\tbreak;\n\t}\n}\n\nstatic void cp_get_ethtool_stats (struct net_device *dev,\n\t\t\t\t  struct ethtool_stats *estats, u64 *tmp_stats)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tstruct cp_dma_stats *nic_stats;\n\tdma_addr_t dma;\n\tint i;\n\n\tnic_stats = dma_alloc_coherent(&cp->pdev->dev, sizeof(*nic_stats),\n\t\t\t\t       &dma, GFP_KERNEL);\n\tif (!nic_stats)\n\t\treturn;\n\n\t \n\tcpw32(StatsAddr + 4, (u64)dma >> 32);\n\tcpw32(StatsAddr, ((u64)dma & DMA_BIT_MASK(32)) | DumpStats);\n\tcpr32(StatsAddr);\n\n\tfor (i = 0; i < 1000; i++) {\n\t\tif ((cpr32(StatsAddr) & DumpStats) == 0)\n\t\t\tbreak;\n\t\tudelay(10);\n\t}\n\tcpw32(StatsAddr, 0);\n\tcpw32(StatsAddr + 4, 0);\n\tcpr32(StatsAddr);\n\n\ti = 0;\n\ttmp_stats[i++] = le64_to_cpu(nic_stats->tx_ok);\n\ttmp_stats[i++] = le64_to_cpu(nic_stats->rx_ok);\n\ttmp_stats[i++] = le64_to_cpu(nic_stats->tx_err);\n\ttmp_stats[i++] = le32_to_cpu(nic_stats->rx_err);\n\ttmp_stats[i++] = le16_to_cpu(nic_stats->rx_fifo);\n\ttmp_stats[i++] = le16_to_cpu(nic_stats->frame_align);\n\ttmp_stats[i++] = le32_to_cpu(nic_stats->tx_ok_1col);\n\ttmp_stats[i++] = le32_to_cpu(nic_stats->tx_ok_mcol);\n\ttmp_stats[i++] = le64_to_cpu(nic_stats->rx_ok_phys);\n\ttmp_stats[i++] = le64_to_cpu(nic_stats->rx_ok_bcast);\n\ttmp_stats[i++] = le32_to_cpu(nic_stats->rx_ok_mcast);\n\ttmp_stats[i++] = le16_to_cpu(nic_stats->tx_abort);\n\ttmp_stats[i++] = le16_to_cpu(nic_stats->tx_underrun);\n\ttmp_stats[i++] = cp->cp_stats.rx_frags;\n\tBUG_ON(i != CP_NUM_STATS);\n\n\tdma_free_coherent(&cp->pdev->dev, sizeof(*nic_stats), nic_stats, dma);\n}\n\nstatic const struct ethtool_ops cp_ethtool_ops = {\n\t.get_drvinfo\t\t= cp_get_drvinfo,\n\t.get_regs_len\t\t= cp_get_regs_len,\n\t.get_sset_count\t\t= cp_get_sset_count,\n\t.nway_reset\t\t= cp_nway_reset,\n\t.get_link\t\t= ethtool_op_get_link,\n\t.get_msglevel\t\t= cp_get_msglevel,\n\t.set_msglevel\t\t= cp_set_msglevel,\n\t.get_regs\t\t= cp_get_regs,\n\t.get_wol\t\t= cp_get_wol,\n\t.set_wol\t\t= cp_set_wol,\n\t.get_strings\t\t= cp_get_strings,\n\t.get_ethtool_stats\t= cp_get_ethtool_stats,\n\t.get_eeprom_len\t\t= cp_get_eeprom_len,\n\t.get_eeprom\t\t= cp_get_eeprom,\n\t.set_eeprom\t\t= cp_set_eeprom,\n\t.get_ringparam\t\t= cp_get_ringparam,\n\t.get_link_ksettings\t= cp_get_link_ksettings,\n\t.set_link_ksettings\t= cp_set_link_ksettings,\n};\n\nstatic int cp_ioctl (struct net_device *dev, struct ifreq *rq, int cmd)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tint rc;\n\tunsigned long flags;\n\n\tif (!netif_running(dev))\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&cp->lock, flags);\n\trc = generic_mii_ioctl(&cp->mii_if, if_mii(rq), cmd, NULL);\n\tspin_unlock_irqrestore(&cp->lock, flags);\n\treturn rc;\n}\n\nstatic int cp_set_mac_address(struct net_device *dev, void *p)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tstruct sockaddr *addr = p;\n\n\tif (!is_valid_ether_addr(addr->sa_data))\n\t\treturn -EADDRNOTAVAIL;\n\n\teth_hw_addr_set(dev, addr->sa_data);\n\n\tspin_lock_irq(&cp->lock);\n\n\tcpw8_f(Cfg9346, Cfg9346_Unlock);\n\tcpw32_f(MAC0 + 0, le32_to_cpu (*(__le32 *) (dev->dev_addr + 0)));\n\tcpw32_f(MAC0 + 4, le32_to_cpu (*(__le32 *) (dev->dev_addr + 4)));\n\tcpw8_f(Cfg9346, Cfg9346_Lock);\n\n\tspin_unlock_irq(&cp->lock);\n\n\treturn 0;\n}\n\n \n\n \n#define EE_SHIFT_CLK\t0x04\t \n#define EE_CS\t\t\t0x08\t \n#define EE_DATA_WRITE\t0x02\t \n#define EE_WRITE_0\t\t0x00\n#define EE_WRITE_1\t\t0x02\n#define EE_DATA_READ\t0x01\t \n#define EE_ENB\t\t\t(0x80 | EE_CS)\n\n \n\n#define eeprom_delay()\treadb(ee_addr)\n\n \n#define EE_EXTEND_CMD\t(4)\n#define EE_WRITE_CMD\t(5)\n#define EE_READ_CMD\t\t(6)\n#define EE_ERASE_CMD\t(7)\n\n#define EE_EWDS_ADDR\t(0)\n#define EE_WRAL_ADDR\t(1)\n#define EE_ERAL_ADDR\t(2)\n#define EE_EWEN_ADDR\t(3)\n\n#define CP_EEPROM_MAGIC PCI_DEVICE_ID_REALTEK_8139\n\nstatic void eeprom_cmd_start(void __iomem *ee_addr)\n{\n\twriteb (EE_ENB & ~EE_CS, ee_addr);\n\twriteb (EE_ENB, ee_addr);\n\teeprom_delay ();\n}\n\nstatic void eeprom_cmd(void __iomem *ee_addr, int cmd, int cmd_len)\n{\n\tint i;\n\n\t \n\tfor (i = cmd_len - 1; i >= 0; i--) {\n\t\tint dataval = (cmd & (1 << i)) ? EE_DATA_WRITE : 0;\n\t\twriteb (EE_ENB | dataval, ee_addr);\n\t\teeprom_delay ();\n\t\twriteb (EE_ENB | dataval | EE_SHIFT_CLK, ee_addr);\n\t\teeprom_delay ();\n\t}\n\twriteb (EE_ENB, ee_addr);\n\teeprom_delay ();\n}\n\nstatic void eeprom_cmd_end(void __iomem *ee_addr)\n{\n\twriteb(0, ee_addr);\n\teeprom_delay ();\n}\n\nstatic void eeprom_extend_cmd(void __iomem *ee_addr, int extend_cmd,\n\t\t\t      int addr_len)\n{\n\tint cmd = (EE_EXTEND_CMD << addr_len) | (extend_cmd << (addr_len - 2));\n\n\teeprom_cmd_start(ee_addr);\n\teeprom_cmd(ee_addr, cmd, 3 + addr_len);\n\teeprom_cmd_end(ee_addr);\n}\n\nstatic u16 read_eeprom (void __iomem *ioaddr, int location, int addr_len)\n{\n\tint i;\n\tu16 retval = 0;\n\tvoid __iomem *ee_addr = ioaddr + Cfg9346;\n\tint read_cmd = location | (EE_READ_CMD << addr_len);\n\n\teeprom_cmd_start(ee_addr);\n\teeprom_cmd(ee_addr, read_cmd, 3 + addr_len);\n\n\tfor (i = 16; i > 0; i--) {\n\t\twriteb (EE_ENB | EE_SHIFT_CLK, ee_addr);\n\t\teeprom_delay ();\n\t\tretval =\n\t\t    (retval << 1) | ((readb (ee_addr) & EE_DATA_READ) ? 1 :\n\t\t\t\t     0);\n\t\twriteb (EE_ENB, ee_addr);\n\t\teeprom_delay ();\n\t}\n\n\teeprom_cmd_end(ee_addr);\n\n\treturn retval;\n}\n\nstatic void write_eeprom(void __iomem *ioaddr, int location, u16 val,\n\t\t\t int addr_len)\n{\n\tint i;\n\tvoid __iomem *ee_addr = ioaddr + Cfg9346;\n\tint write_cmd = location | (EE_WRITE_CMD << addr_len);\n\n\teeprom_extend_cmd(ee_addr, EE_EWEN_ADDR, addr_len);\n\n\teeprom_cmd_start(ee_addr);\n\teeprom_cmd(ee_addr, write_cmd, 3 + addr_len);\n\teeprom_cmd(ee_addr, val, 16);\n\teeprom_cmd_end(ee_addr);\n\n\teeprom_cmd_start(ee_addr);\n\tfor (i = 0; i < 20000; i++)\n\t\tif (readb(ee_addr) & EE_DATA_READ)\n\t\t\tbreak;\n\teeprom_cmd_end(ee_addr);\n\n\teeprom_extend_cmd(ee_addr, EE_EWDS_ADDR, addr_len);\n}\n\nstatic int cp_get_eeprom_len(struct net_device *dev)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tint size;\n\n\tspin_lock_irq(&cp->lock);\n\tsize = read_eeprom(cp->regs, 0, 8) == 0x8129 ? 256 : 128;\n\tspin_unlock_irq(&cp->lock);\n\n\treturn size;\n}\n\nstatic int cp_get_eeprom(struct net_device *dev,\n\t\t\t struct ethtool_eeprom *eeprom, u8 *data)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tunsigned int addr_len;\n\tu16 val;\n\tu32 offset = eeprom->offset >> 1;\n\tu32 len = eeprom->len;\n\tu32 i = 0;\n\n\teeprom->magic = CP_EEPROM_MAGIC;\n\n\tspin_lock_irq(&cp->lock);\n\n\taddr_len = read_eeprom(cp->regs, 0, 8) == 0x8129 ? 8 : 6;\n\n\tif (eeprom->offset & 1) {\n\t\tval = read_eeprom(cp->regs, offset, addr_len);\n\t\tdata[i++] = (u8)(val >> 8);\n\t\toffset++;\n\t}\n\n\twhile (i < len - 1) {\n\t\tval = read_eeprom(cp->regs, offset, addr_len);\n\t\tdata[i++] = (u8)val;\n\t\tdata[i++] = (u8)(val >> 8);\n\t\toffset++;\n\t}\n\n\tif (i < len) {\n\t\tval = read_eeprom(cp->regs, offset, addr_len);\n\t\tdata[i] = (u8)val;\n\t}\n\n\tspin_unlock_irq(&cp->lock);\n\treturn 0;\n}\n\nstatic int cp_set_eeprom(struct net_device *dev,\n\t\t\t struct ethtool_eeprom *eeprom, u8 *data)\n{\n\tstruct cp_private *cp = netdev_priv(dev);\n\tunsigned int addr_len;\n\tu16 val;\n\tu32 offset = eeprom->offset >> 1;\n\tu32 len = eeprom->len;\n\tu32 i = 0;\n\n\tif (eeprom->magic != CP_EEPROM_MAGIC)\n\t\treturn -EINVAL;\n\n\tspin_lock_irq(&cp->lock);\n\n\taddr_len = read_eeprom(cp->regs, 0, 8) == 0x8129 ? 8 : 6;\n\n\tif (eeprom->offset & 1) {\n\t\tval = read_eeprom(cp->regs, offset, addr_len) & 0xff;\n\t\tval |= (u16)data[i++] << 8;\n\t\twrite_eeprom(cp->regs, offset, val, addr_len);\n\t\toffset++;\n\t}\n\n\twhile (i < len - 1) {\n\t\tval = (u16)data[i++];\n\t\tval |= (u16)data[i++] << 8;\n\t\twrite_eeprom(cp->regs, offset, val, addr_len);\n\t\toffset++;\n\t}\n\n\tif (i < len) {\n\t\tval = read_eeprom(cp->regs, offset, addr_len) & 0xff00;\n\t\tval |= (u16)data[i];\n\t\twrite_eeprom(cp->regs, offset, val, addr_len);\n\t}\n\n\tspin_unlock_irq(&cp->lock);\n\treturn 0;\n}\n\n \nstatic void cp_set_d3_state (struct cp_private *cp)\n{\n\tpci_enable_wake(cp->pdev, PCI_D0, 1);  \n\tpci_set_power_state (cp->pdev, PCI_D3hot);\n}\n\nstatic netdev_features_t cp_features_check(struct sk_buff *skb,\n\t\t\t\t\t   struct net_device *dev,\n\t\t\t\t\t   netdev_features_t features)\n{\n\tif (skb_shinfo(skb)->gso_size > MSSMask)\n\t\tfeatures &= ~NETIF_F_TSO;\n\n\treturn vlan_features_check(skb, features);\n}\nstatic const struct net_device_ops cp_netdev_ops = {\n\t.ndo_open\t\t= cp_open,\n\t.ndo_stop\t\t= cp_close,\n\t.ndo_validate_addr\t= eth_validate_addr,\n\t.ndo_set_mac_address \t= cp_set_mac_address,\n\t.ndo_set_rx_mode\t= cp_set_rx_mode,\n\t.ndo_get_stats\t\t= cp_get_stats,\n\t.ndo_eth_ioctl\t\t= cp_ioctl,\n\t.ndo_start_xmit\t\t= cp_start_xmit,\n\t.ndo_tx_timeout\t\t= cp_tx_timeout,\n\t.ndo_set_features\t= cp_set_features,\n\t.ndo_change_mtu\t\t= cp_change_mtu,\n\t.ndo_features_check\t= cp_features_check,\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller\t= cp_poll_controller,\n#endif\n};\n\nstatic int cp_init_one (struct pci_dev *pdev, const struct pci_device_id *ent)\n{\n\tstruct net_device *dev;\n\tstruct cp_private *cp;\n\tint rc;\n\tvoid __iomem *regs;\n\tresource_size_t pciaddr;\n\tunsigned int addr_len, i, pci_using_dac;\n\t__le16 addr[ETH_ALEN / 2];\n\n\tpr_info_once(\"%s\", version);\n\n\tif (pdev->vendor == PCI_VENDOR_ID_REALTEK &&\n\t    pdev->device == PCI_DEVICE_ID_REALTEK_8139 && pdev->revision < 0x20) {\n\t\tdev_info(&pdev->dev,\n\t\t\t \"This (id %04x:%04x rev %02x) is not an 8139C+ compatible chip, use 8139too\\n\",\n\t\t\t pdev->vendor, pdev->device, pdev->revision);\n\t\treturn -ENODEV;\n\t}\n\n\tdev = alloc_etherdev(sizeof(struct cp_private));\n\tif (!dev)\n\t\treturn -ENOMEM;\n\tSET_NETDEV_DEV(dev, &pdev->dev);\n\n\tcp = netdev_priv(dev);\n\tcp->pdev = pdev;\n\tcp->dev = dev;\n\tcp->msg_enable = (debug < 0 ? CP_DEF_MSG_ENABLE : debug);\n\tspin_lock_init (&cp->lock);\n\tcp->mii_if.dev = dev;\n\tcp->mii_if.mdio_read = mdio_read;\n\tcp->mii_if.mdio_write = mdio_write;\n\tcp->mii_if.phy_id = CP_INTERNAL_PHY;\n\tcp->mii_if.phy_id_mask = 0x1f;\n\tcp->mii_if.reg_num_mask = 0x1f;\n\tcp_set_rxbufsize(cp);\n\n\trc = pci_enable_device(pdev);\n\tif (rc)\n\t\tgoto err_out_free;\n\n\trc = pci_set_mwi(pdev);\n\tif (rc)\n\t\tgoto err_out_disable;\n\n\trc = pci_request_regions(pdev, DRV_NAME);\n\tif (rc)\n\t\tgoto err_out_mwi;\n\n\tpciaddr = pci_resource_start(pdev, 1);\n\tif (!pciaddr) {\n\t\trc = -EIO;\n\t\tdev_err(&pdev->dev, \"no MMIO resource\\n\");\n\t\tgoto err_out_res;\n\t}\n\tif (pci_resource_len(pdev, 1) < CP_REGS_SIZE) {\n\t\trc = -EIO;\n\t\tdev_err(&pdev->dev, \"MMIO resource (%llx) too small\\n\",\n\t\t       (unsigned long long)pci_resource_len(pdev, 1));\n\t\tgoto err_out_res;\n\t}\n\n\t \n\tif ((sizeof(dma_addr_t) > 4) &&\n\t    !dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64))) {\n\t\tpci_using_dac = 1;\n\t} else {\n\t\tpci_using_dac = 0;\n\n\t\trc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));\n\t\tif (rc) {\n\t\t\tdev_err(&pdev->dev,\n\t\t\t\t\"No usable DMA configuration, aborting\\n\");\n\t\t\tgoto err_out_res;\n\t\t}\n\t}\n\n\tcp->cpcmd = (pci_using_dac ? PCIDAC : 0) |\n\t\t    PCIMulRW | RxChkSum | CpRxOn | CpTxOn;\n\n\tdev->features |= NETIF_F_RXCSUM;\n\tdev->hw_features |= NETIF_F_RXCSUM;\n\n\tregs = ioremap(pciaddr, CP_REGS_SIZE);\n\tif (!regs) {\n\t\trc = -EIO;\n\t\tdev_err(&pdev->dev, \"Cannot map PCI MMIO (%Lx@%Lx)\\n\",\n\t\t\t(unsigned long long)pci_resource_len(pdev, 1),\n\t\t       (unsigned long long)pciaddr);\n\t\tgoto err_out_res;\n\t}\n\tcp->regs = regs;\n\n\tcp_stop_hw(cp);\n\n\t \n\taddr_len = read_eeprom (regs, 0, 8) == 0x8129 ? 8 : 6;\n\tfor (i = 0; i < 3; i++)\n\t\taddr[i] = cpu_to_le16(read_eeprom (regs, i + 7, addr_len));\n\teth_hw_addr_set(dev, (u8 *)addr);\n\n\tdev->netdev_ops = &cp_netdev_ops;\n\tnetif_napi_add_weight(dev, &cp->napi, cp_rx_poll, 16);\n\tdev->ethtool_ops = &cp_ethtool_ops;\n\tdev->watchdog_timeo = TX_TIMEOUT;\n\n\tdev->features |= NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_TSO |\n\t\tNETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX;\n\n\tif (pci_using_dac)\n\t\tdev->features |= NETIF_F_HIGHDMA;\n\n\tdev->hw_features |= NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_TSO |\n\t\tNETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX;\n\tdev->vlan_features = NETIF_F_SG | NETIF_F_IP_CSUM | NETIF_F_TSO |\n\t\tNETIF_F_HIGHDMA;\n\n\t \n\tdev->min_mtu = CP_MIN_MTU;\n\tdev->max_mtu = CP_MAX_MTU;\n\n\trc = register_netdev(dev);\n\tif (rc)\n\t\tgoto err_out_iomap;\n\n\tnetdev_info(dev, \"RTL-8139C+ at 0x%p, %pM, IRQ %d\\n\",\n\t\t    regs, dev->dev_addr, pdev->irq);\n\n\tpci_set_drvdata(pdev, dev);\n\n\t \n\tpci_set_master(pdev);\n\n\tif (cp->wol_enabled)\n\t\tcp_set_d3_state (cp);\n\n\treturn 0;\n\nerr_out_iomap:\n\tiounmap(regs);\nerr_out_res:\n\tpci_release_regions(pdev);\nerr_out_mwi:\n\tpci_clear_mwi(pdev);\nerr_out_disable:\n\tpci_disable_device(pdev);\nerr_out_free:\n\tfree_netdev(dev);\n\treturn rc;\n}\n\nstatic void cp_remove_one (struct pci_dev *pdev)\n{\n\tstruct net_device *dev = pci_get_drvdata(pdev);\n\tstruct cp_private *cp = netdev_priv(dev);\n\n\tunregister_netdev(dev);\n\tiounmap(cp->regs);\n\tif (cp->wol_enabled)\n\t\tpci_set_power_state (pdev, PCI_D0);\n\tpci_release_regions(pdev);\n\tpci_clear_mwi(pdev);\n\tpci_disable_device(pdev);\n\tfree_netdev(dev);\n}\n\nstatic int __maybe_unused cp_suspend(struct device *device)\n{\n\tstruct net_device *dev = dev_get_drvdata(device);\n\tstruct cp_private *cp = netdev_priv(dev);\n\tunsigned long flags;\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\tnetif_device_detach (dev);\n\tnetif_stop_queue (dev);\n\n\tspin_lock_irqsave (&cp->lock, flags);\n\n\t \n\tcpw16 (IntrMask, 0);\n\tcpw8  (Cmd, cpr8 (Cmd) & (~RxOn | ~TxOn));\n\n\tspin_unlock_irqrestore (&cp->lock, flags);\n\n\tdevice_set_wakeup_enable(device, cp->wol_enabled);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused cp_resume(struct device *device)\n{\n\tstruct net_device *dev = dev_get_drvdata(device);\n\tstruct cp_private *cp = netdev_priv(dev);\n\tunsigned long flags;\n\n\tif (!netif_running(dev))\n\t\treturn 0;\n\n\tnetif_device_attach (dev);\n\n\t \n\tcp_init_rings_index (cp);\n\tcp_init_hw (cp);\n\tcp_enable_irq(cp);\n\tnetif_start_queue (dev);\n\n\tspin_lock_irqsave (&cp->lock, flags);\n\n\tmii_check_media(&cp->mii_if, netif_msg_link(cp), false);\n\n\tspin_unlock_irqrestore (&cp->lock, flags);\n\n\treturn 0;\n}\n\nstatic const struct pci_device_id cp_pci_tbl[] = {\n        { PCI_DEVICE(PCI_VENDOR_ID_REALTEK,     PCI_DEVICE_ID_REALTEK_8139), },\n        { PCI_DEVICE(PCI_VENDOR_ID_TTTECH,      PCI_DEVICE_ID_TTTECH_MC322), },\n        { },\n};\nMODULE_DEVICE_TABLE(pci, cp_pci_tbl);\n\nstatic SIMPLE_DEV_PM_OPS(cp_pm_ops, cp_suspend, cp_resume);\n\nstatic struct pci_driver cp_driver = {\n\t.name         = DRV_NAME,\n\t.id_table     = cp_pci_tbl,\n\t.probe        =\tcp_init_one,\n\t.remove       = cp_remove_one,\n\t.driver.pm    = &cp_pm_ops,\n};\n\nmodule_pci_driver(cp_driver);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}