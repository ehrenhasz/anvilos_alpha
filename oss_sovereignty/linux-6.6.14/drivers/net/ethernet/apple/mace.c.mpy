{
  "module_name": "mace.c",
  "hash_id": "b79e9e2949794c39907ec97fb88be5e2ea9fcdaac43421f4280e21ea962e10d2",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/apple/mace.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/delay.h>\n#include <linux/string.h>\n#include <linux/timer.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/crc32.h>\n#include <linux/spinlock.h>\n#include <linux/bitrev.h>\n#include <linux/slab.h>\n#include <linux/pgtable.h>\n#include <asm/dbdma.h>\n#include <asm/io.h>\n#include <asm/macio.h>\n\n#include \"mace.h\"\n\nstatic int port_aaui = -1;\n\n#define N_RX_RING\t8\n#define N_TX_RING\t6\n#define MAX_TX_ACTIVE\t1\n#define NCMDS_TX\t1\t \n#define RX_BUFLEN\t(ETH_FRAME_LEN + 8)\n#define TX_TIMEOUT\tHZ\t \n\n \n#define BROKEN_ADDRCHG_REV\t0x0941\n\n \n#define TX_DMA_ERR\t0x80\n\nstruct mace_data {\n    volatile struct mace __iomem *mace;\n    volatile struct dbdma_regs __iomem *tx_dma;\n    int tx_dma_intr;\n    volatile struct dbdma_regs __iomem *rx_dma;\n    int rx_dma_intr;\n    volatile struct dbdma_cmd *tx_cmds;\t \n    volatile struct dbdma_cmd *rx_cmds;\t \n    struct sk_buff *rx_bufs[N_RX_RING];\n    int rx_fill;\n    int rx_empty;\n    struct sk_buff *tx_bufs[N_TX_RING];\n    int tx_fill;\n    int tx_empty;\n    unsigned char maccc;\n    unsigned char tx_fullup;\n    unsigned char tx_active;\n    unsigned char tx_bad_runt;\n    struct timer_list tx_timeout;\n    int timeout_active;\n    int port_aaui;\n    int chipid;\n    struct macio_dev *mdev;\n    spinlock_t lock;\n};\n\n \n#define PRIV_BYTES\t(sizeof(struct mace_data) \\\n\t+ (N_RX_RING + NCMDS_TX * N_TX_RING + 3) * sizeof(struct dbdma_cmd))\n\nstatic int mace_open(struct net_device *dev);\nstatic int mace_close(struct net_device *dev);\nstatic netdev_tx_t mace_xmit_start(struct sk_buff *skb, struct net_device *dev);\nstatic void mace_set_multicast(struct net_device *dev);\nstatic void mace_reset(struct net_device *dev);\nstatic int mace_set_address(struct net_device *dev, void *addr);\nstatic irqreturn_t mace_interrupt(int irq, void *dev_id);\nstatic irqreturn_t mace_txdma_intr(int irq, void *dev_id);\nstatic irqreturn_t mace_rxdma_intr(int irq, void *dev_id);\nstatic void mace_set_timeout(struct net_device *dev);\nstatic void mace_tx_timeout(struct timer_list *t);\nstatic inline void dbdma_reset(volatile struct dbdma_regs __iomem *dma);\nstatic inline void mace_clean_rings(struct mace_data *mp);\nstatic void __mace_set_address(struct net_device *dev, const void *addr);\n\n \nstatic unsigned char *dummy_buf;\n\nstatic const struct net_device_ops mace_netdev_ops = {\n\t.ndo_open\t\t= mace_open,\n\t.ndo_stop\t\t= mace_close,\n\t.ndo_start_xmit\t\t= mace_xmit_start,\n\t.ndo_set_rx_mode\t= mace_set_multicast,\n\t.ndo_set_mac_address\t= mace_set_address,\n\t.ndo_validate_addr\t= eth_validate_addr,\n};\n\nstatic int mace_probe(struct macio_dev *mdev, const struct of_device_id *match)\n{\n\tstruct device_node *mace = macio_get_of_node(mdev);\n\tstruct net_device *dev;\n\tstruct mace_data *mp;\n\tconst unsigned char *addr;\n\tu8 macaddr[ETH_ALEN];\n\tint j, rev, rc = -EBUSY;\n\n\tif (macio_resource_count(mdev) != 3 || macio_irq_count(mdev) != 3) {\n\t\tprintk(KERN_ERR \"can't use MACE %pOF: need 3 addrs and 3 irqs\\n\",\n\t\t       mace);\n\t\treturn -ENODEV;\n\t}\n\n\taddr = of_get_property(mace, \"mac-address\", NULL);\n\tif (addr == NULL) {\n\t\taddr = of_get_property(mace, \"local-mac-address\", NULL);\n\t\tif (addr == NULL) {\n\t\t\tprintk(KERN_ERR \"Can't get mac-address for MACE %pOF\\n\",\n\t\t\t       mace);\n\t\t\treturn -ENODEV;\n\t\t}\n\t}\n\n\t \n\tif (dummy_buf == NULL) {\n\t\tdummy_buf = kmalloc(RX_BUFLEN+2, GFP_KERNEL);\n\t\tif (dummy_buf == NULL)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (macio_request_resources(mdev, \"mace\")) {\n\t\tprintk(KERN_ERR \"MACE: can't request IO resources !\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\tdev = alloc_etherdev(PRIV_BYTES);\n\tif (!dev) {\n\t\trc = -ENOMEM;\n\t\tgoto err_release;\n\t}\n\tSET_NETDEV_DEV(dev, &mdev->ofdev.dev);\n\n\tmp = netdev_priv(dev);\n\tmp->mdev = mdev;\n\tmacio_set_drvdata(mdev, dev);\n\n\tdev->base_addr = macio_resource_start(mdev, 0);\n\tmp->mace = ioremap(dev->base_addr, 0x1000);\n\tif (mp->mace == NULL) {\n\t\tprintk(KERN_ERR \"MACE: can't map IO resources !\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto err_free;\n\t}\n\tdev->irq = macio_irq(mdev, 0);\n\n\trev = addr[0] == 0 && addr[1] == 0xA0;\n\tfor (j = 0; j < 6; ++j) {\n\t\tmacaddr[j] = rev ? bitrev8(addr[j]): addr[j];\n\t}\n\teth_hw_addr_set(dev, macaddr);\n\tmp->chipid = (in_8(&mp->mace->chipid_hi) << 8) |\n\t\t\tin_8(&mp->mace->chipid_lo);\n\n\n\tmp = netdev_priv(dev);\n\tmp->maccc = ENXMT | ENRCV;\n\n\tmp->tx_dma = ioremap(macio_resource_start(mdev, 1), 0x1000);\n\tif (mp->tx_dma == NULL) {\n\t\tprintk(KERN_ERR \"MACE: can't map TX DMA resources !\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto err_unmap_io;\n\t}\n\tmp->tx_dma_intr = macio_irq(mdev, 1);\n\n\tmp->rx_dma = ioremap(macio_resource_start(mdev, 2), 0x1000);\n\tif (mp->rx_dma == NULL) {\n\t\tprintk(KERN_ERR \"MACE: can't map RX DMA resources !\\n\");\n\t\trc = -ENOMEM;\n\t\tgoto err_unmap_tx_dma;\n\t}\n\tmp->rx_dma_intr = macio_irq(mdev, 2);\n\n\tmp->tx_cmds = (volatile struct dbdma_cmd *) DBDMA_ALIGN(mp + 1);\n\tmp->rx_cmds = mp->tx_cmds + NCMDS_TX * N_TX_RING + 1;\n\n\tmemset((char *) mp->tx_cmds, 0,\n\t       (NCMDS_TX*N_TX_RING + N_RX_RING + 2) * sizeof(struct dbdma_cmd));\n\ttimer_setup(&mp->tx_timeout, mace_tx_timeout, 0);\n\tspin_lock_init(&mp->lock);\n\tmp->timeout_active = 0;\n\n\tif (port_aaui >= 0)\n\t\tmp->port_aaui = port_aaui;\n\telse {\n\t\t \n\t\tif (of_machine_is_compatible(\"AAPL,ShinerESB\"))\n\t\t\tmp->port_aaui = 1;\n\t\telse {\n#ifdef CONFIG_MACE_AAUI_PORT\n\t\t\tmp->port_aaui = 1;\n#else\n\t\t\tmp->port_aaui = 0;\n#endif\n\t\t}\n\t}\n\n\tdev->netdev_ops = &mace_netdev_ops;\n\n\t \n\tmace_reset(dev);\n\n\trc = request_irq(dev->irq, mace_interrupt, 0, \"MACE\", dev);\n\tif (rc) {\n\t\tprintk(KERN_ERR \"MACE: can't get irq %d\\n\", dev->irq);\n\t\tgoto err_unmap_rx_dma;\n\t}\n\trc = request_irq(mp->tx_dma_intr, mace_txdma_intr, 0, \"MACE-txdma\", dev);\n\tif (rc) {\n\t\tprintk(KERN_ERR \"MACE: can't get irq %d\\n\", mp->tx_dma_intr);\n\t\tgoto err_free_irq;\n\t}\n\trc = request_irq(mp->rx_dma_intr, mace_rxdma_intr, 0, \"MACE-rxdma\", dev);\n\tif (rc) {\n\t\tprintk(KERN_ERR \"MACE: can't get irq %d\\n\", mp->rx_dma_intr);\n\t\tgoto err_free_tx_irq;\n\t}\n\n\trc = register_netdev(dev);\n\tif (rc) {\n\t\tprintk(KERN_ERR \"MACE: Cannot register net device, aborting.\\n\");\n\t\tgoto err_free_rx_irq;\n\t}\n\n\tprintk(KERN_INFO \"%s: MACE at %pM, chip revision %d.%d\\n\",\n\t       dev->name, dev->dev_addr,\n\t       mp->chipid >> 8, mp->chipid & 0xff);\n\n\treturn 0;\n\n err_free_rx_irq:\n\tfree_irq(macio_irq(mdev, 2), dev);\n err_free_tx_irq:\n\tfree_irq(macio_irq(mdev, 1), dev);\n err_free_irq:\n\tfree_irq(macio_irq(mdev, 0), dev);\n err_unmap_rx_dma:\n\tiounmap(mp->rx_dma);\n err_unmap_tx_dma:\n\tiounmap(mp->tx_dma);\n err_unmap_io:\n\tiounmap(mp->mace);\n err_free:\n\tfree_netdev(dev);\n err_release:\n\tmacio_release_resources(mdev);\n\n\treturn rc;\n}\n\nstatic int mace_remove(struct macio_dev *mdev)\n{\n\tstruct net_device *dev = macio_get_drvdata(mdev);\n\tstruct mace_data *mp;\n\n\tBUG_ON(dev == NULL);\n\n\tmacio_set_drvdata(mdev, NULL);\n\n\tmp = netdev_priv(dev);\n\n\tunregister_netdev(dev);\n\n\tfree_irq(dev->irq, dev);\n\tfree_irq(mp->tx_dma_intr, dev);\n\tfree_irq(mp->rx_dma_intr, dev);\n\n\tiounmap(mp->rx_dma);\n\tiounmap(mp->tx_dma);\n\tiounmap(mp->mace);\n\n\tfree_netdev(dev);\n\n\tmacio_release_resources(mdev);\n\n\treturn 0;\n}\n\nstatic void dbdma_reset(volatile struct dbdma_regs __iomem *dma)\n{\n    int i;\n\n    out_le32(&dma->control, (WAKE|FLUSH|PAUSE|RUN) << 16);\n\n     \n    for (i = 200; i > 0; --i)\n\tif (le32_to_cpu(dma->control) & RUN)\n\t    udelay(1);\n}\n\nstatic void mace_reset(struct net_device *dev)\n{\n    struct mace_data *mp = netdev_priv(dev);\n    volatile struct mace __iomem *mb = mp->mace;\n    int i;\n\n     \n    i = 200;\n    while (--i) {\n\tout_8(&mb->biucc, SWRST);\n\tif (in_8(&mb->biucc) & SWRST) {\n\t    udelay(10);\n\t    continue;\n\t}\n\tbreak;\n    }\n    if (!i) {\n\tprintk(KERN_ERR \"mace: cannot reset chip!\\n\");\n\treturn;\n    }\n\n    out_8(&mb->imr, 0xff);\t \n    i = in_8(&mb->ir);\n    out_8(&mb->maccc, 0);\t \n\n    out_8(&mb->biucc, XMTSP_64);\n    out_8(&mb->utr, RTRD);\n    out_8(&mb->fifocc, RCVFW_32 | XMTFW_16 | XMTFWU | RCVFWU | XMTBRST);\n    out_8(&mb->xmtfc, AUTO_PAD_XMIT);  \n    out_8(&mb->rcvfc, 0);\n\n     \n    __mace_set_address(dev, dev->dev_addr);\n\n     \n    if (mp->chipid == BROKEN_ADDRCHG_REV)\n\tout_8(&mb->iac, LOGADDR);\n    else {\n\tout_8(&mb->iac, ADDRCHG | LOGADDR);\n\twhile ((in_8(&mb->iac) & ADDRCHG) != 0)\n\t\t;\n    }\n    for (i = 0; i < 8; ++i)\n\tout_8(&mb->ladrf, 0);\n\n     \n    if (mp->chipid != BROKEN_ADDRCHG_REV)\n\tout_8(&mb->iac, 0);\n\n    if (mp->port_aaui)\n\tout_8(&mb->plscc, PORTSEL_AUI + ENPLSIO);\n    else\n\tout_8(&mb->plscc, PORTSEL_GPSI + ENPLSIO);\n}\n\nstatic void __mace_set_address(struct net_device *dev, const void *addr)\n{\n    struct mace_data *mp = netdev_priv(dev);\n    volatile struct mace __iomem *mb = mp->mace;\n    const unsigned char *p = addr;\n    u8 macaddr[ETH_ALEN];\n    int i;\n\n     \n    if (mp->chipid == BROKEN_ADDRCHG_REV)\n\tout_8(&mb->iac, PHYADDR);\n    else {\n\tout_8(&mb->iac, ADDRCHG | PHYADDR);\n\twhile ((in_8(&mb->iac) & ADDRCHG) != 0)\n\t    ;\n    }\n    for (i = 0; i < 6; ++i)\n        out_8(&mb->padr, macaddr[i] = p[i]);\n\n    eth_hw_addr_set(dev, macaddr);\n\n    if (mp->chipid != BROKEN_ADDRCHG_REV)\n        out_8(&mb->iac, 0);\n}\n\nstatic int mace_set_address(struct net_device *dev, void *addr)\n{\n    struct mace_data *mp = netdev_priv(dev);\n    volatile struct mace __iomem *mb = mp->mace;\n    unsigned long flags;\n\n    spin_lock_irqsave(&mp->lock, flags);\n\n    __mace_set_address(dev, addr);\n\n     \n    out_8(&mb->maccc, mp->maccc);\n\n    spin_unlock_irqrestore(&mp->lock, flags);\n    return 0;\n}\n\nstatic inline void mace_clean_rings(struct mace_data *mp)\n{\n    int i;\n\n     \n    for (i = 0; i < N_RX_RING; ++i) {\n\tif (mp->rx_bufs[i] != NULL) {\n\t    dev_kfree_skb(mp->rx_bufs[i]);\n\t    mp->rx_bufs[i] = NULL;\n\t}\n    }\n    for (i = mp->tx_empty; i != mp->tx_fill; ) {\n\tdev_kfree_skb(mp->tx_bufs[i]);\n\tif (++i >= N_TX_RING)\n\t    i = 0;\n    }\n}\n\nstatic int mace_open(struct net_device *dev)\n{\n    struct mace_data *mp = netdev_priv(dev);\n    volatile struct mace __iomem *mb = mp->mace;\n    volatile struct dbdma_regs __iomem *rd = mp->rx_dma;\n    volatile struct dbdma_regs __iomem *td = mp->tx_dma;\n    volatile struct dbdma_cmd *cp;\n    int i;\n    struct sk_buff *skb;\n    unsigned char *data;\n\n     \n    mace_reset(dev);\n\n     \n    mace_clean_rings(mp);\n    memset((char *)mp->rx_cmds, 0, N_RX_RING * sizeof(struct dbdma_cmd));\n    cp = mp->rx_cmds;\n    for (i = 0; i < N_RX_RING - 1; ++i) {\n\tskb = netdev_alloc_skb(dev, RX_BUFLEN + 2);\n\tif (!skb) {\n\t    data = dummy_buf;\n\t} else {\n\t    skb_reserve(skb, 2);\t \n\t    data = skb->data;\n\t}\n\tmp->rx_bufs[i] = skb;\n\tcp->req_count = cpu_to_le16(RX_BUFLEN);\n\tcp->command = cpu_to_le16(INPUT_LAST + INTR_ALWAYS);\n\tcp->phy_addr = cpu_to_le32(virt_to_bus(data));\n\tcp->xfer_status = 0;\n\t++cp;\n    }\n    mp->rx_bufs[i] = NULL;\n    cp->command = cpu_to_le16(DBDMA_STOP);\n    mp->rx_fill = i;\n    mp->rx_empty = 0;\n\n     \n    ++cp;\n    cp->command = cpu_to_le16(DBDMA_NOP + BR_ALWAYS);\n    cp->cmd_dep = cpu_to_le32(virt_to_bus(mp->rx_cmds));\n\n     \n    out_le32(&rd->control, (RUN|PAUSE|FLUSH|WAKE) << 16);  \n    out_le32(&rd->cmdptr, virt_to_bus(mp->rx_cmds));\n    out_le32(&rd->control, (RUN << 16) | RUN);\n\n     \n    cp = mp->tx_cmds + NCMDS_TX * N_TX_RING;\n    cp->command = cpu_to_le16(DBDMA_NOP + BR_ALWAYS);\n    cp->cmd_dep = cpu_to_le32(virt_to_bus(mp->tx_cmds));\n\n     \n    out_le32(&td->control, (RUN|PAUSE|FLUSH|WAKE) << 16);\n    out_le32(&td->cmdptr, virt_to_bus(mp->tx_cmds));\n    mp->tx_fill = 0;\n    mp->tx_empty = 0;\n    mp->tx_fullup = 0;\n    mp->tx_active = 0;\n    mp->tx_bad_runt = 0;\n\n     \n    out_8(&mb->maccc, mp->maccc);\n     \n    out_8(&mb->imr, RCVINT);\n\n    return 0;\n}\n\nstatic int mace_close(struct net_device *dev)\n{\n    struct mace_data *mp = netdev_priv(dev);\n    volatile struct mace __iomem *mb = mp->mace;\n    volatile struct dbdma_regs __iomem *rd = mp->rx_dma;\n    volatile struct dbdma_regs __iomem *td = mp->tx_dma;\n\n     \n    out_8(&mb->maccc, 0);\n    out_8(&mb->imr, 0xff);\t\t \n\n     \n    rd->control = cpu_to_le32((RUN|PAUSE|FLUSH|WAKE) << 16);  \n    td->control = cpu_to_le32((RUN|PAUSE|FLUSH|WAKE) << 16);  \n\n    mace_clean_rings(mp);\n\n    return 0;\n}\n\nstatic inline void mace_set_timeout(struct net_device *dev)\n{\n    struct mace_data *mp = netdev_priv(dev);\n\n    if (mp->timeout_active)\n\tdel_timer(&mp->tx_timeout);\n    mp->tx_timeout.expires = jiffies + TX_TIMEOUT;\n    add_timer(&mp->tx_timeout);\n    mp->timeout_active = 1;\n}\n\nstatic netdev_tx_t mace_xmit_start(struct sk_buff *skb, struct net_device *dev)\n{\n    struct mace_data *mp = netdev_priv(dev);\n    volatile struct dbdma_regs __iomem *td = mp->tx_dma;\n    volatile struct dbdma_cmd *cp, *np;\n    unsigned long flags;\n    int fill, next, len;\n\n     \n    spin_lock_irqsave(&mp->lock, flags);\n    fill = mp->tx_fill;\n    next = fill + 1;\n    if (next >= N_TX_RING)\n\tnext = 0;\n    if (next == mp->tx_empty) {\n\tnetif_stop_queue(dev);\n\tmp->tx_fullup = 1;\n\tspin_unlock_irqrestore(&mp->lock, flags);\n\treturn NETDEV_TX_BUSY;\t\t \n    }\n    spin_unlock_irqrestore(&mp->lock, flags);\n\n     \n    len = skb->len;\n    if (len > ETH_FRAME_LEN) {\n\tprintk(KERN_DEBUG \"mace: xmit frame too long (%d)\\n\", len);\n\tlen = ETH_FRAME_LEN;\n    }\n    mp->tx_bufs[fill] = skb;\n    cp = mp->tx_cmds + NCMDS_TX * fill;\n    cp->req_count = cpu_to_le16(len);\n    cp->phy_addr = cpu_to_le32(virt_to_bus(skb->data));\n\n    np = mp->tx_cmds + NCMDS_TX * next;\n    out_le16(&np->command, DBDMA_STOP);\n\n     \n    spin_lock_irqsave(&mp->lock, flags);\n    mp->tx_fill = next;\n    if (!mp->tx_bad_runt && mp->tx_active < MAX_TX_ACTIVE) {\n\tout_le16(&cp->xfer_status, 0);\n\tout_le16(&cp->command, OUTPUT_LAST);\n\tout_le32(&td->control, ((RUN|WAKE) << 16) + (RUN|WAKE));\n\t++mp->tx_active;\n\tmace_set_timeout(dev);\n    }\n    if (++next >= N_TX_RING)\n\tnext = 0;\n    if (next == mp->tx_empty)\n\tnetif_stop_queue(dev);\n    spin_unlock_irqrestore(&mp->lock, flags);\n\n    return NETDEV_TX_OK;\n}\n\nstatic void mace_set_multicast(struct net_device *dev)\n{\n    struct mace_data *mp = netdev_priv(dev);\n    volatile struct mace __iomem *mb = mp->mace;\n    int i;\n    u32 crc;\n    unsigned long flags;\n\n    spin_lock_irqsave(&mp->lock, flags);\n    mp->maccc &= ~PROM;\n    if (dev->flags & IFF_PROMISC) {\n\tmp->maccc |= PROM;\n    } else {\n\tunsigned char multicast_filter[8];\n\tstruct netdev_hw_addr *ha;\n\n\tif (dev->flags & IFF_ALLMULTI) {\n\t    for (i = 0; i < 8; i++)\n\t\tmulticast_filter[i] = 0xff;\n\t} else {\n\t    for (i = 0; i < 8; i++)\n\t\tmulticast_filter[i] = 0;\n\t    netdev_for_each_mc_addr(ha, dev) {\n\t        crc = ether_crc_le(6, ha->addr);\n\t\ti = crc >> 26;\t \n\t\tmulticast_filter[i >> 3] |= 1 << (i & 7);\n\t    }\n\t}\n#if 0\n\tprintk(\"Multicast filter :\");\n\tfor (i = 0; i < 8; i++)\n\t    printk(\"%02x \", multicast_filter[i]);\n\tprintk(\"\\n\");\n#endif\n\n\tif (mp->chipid == BROKEN_ADDRCHG_REV)\n\t    out_8(&mb->iac, LOGADDR);\n\telse {\n\t    out_8(&mb->iac, ADDRCHG | LOGADDR);\n\t    while ((in_8(&mb->iac) & ADDRCHG) != 0)\n\t\t;\n\t}\n\tfor (i = 0; i < 8; ++i)\n\t    out_8(&mb->ladrf, multicast_filter[i]);\n\tif (mp->chipid != BROKEN_ADDRCHG_REV)\n\t    out_8(&mb->iac, 0);\n    }\n     \n    out_8(&mb->maccc, mp->maccc);\n    spin_unlock_irqrestore(&mp->lock, flags);\n}\n\nstatic void mace_handle_misc_intrs(struct mace_data *mp, int intr, struct net_device *dev)\n{\n    volatile struct mace __iomem *mb = mp->mace;\n    static int mace_babbles, mace_jabbers;\n\n    if (intr & MPCO)\n\tdev->stats.rx_missed_errors += 256;\n    dev->stats.rx_missed_errors += in_8(&mb->mpc);    \n    if (intr & RNTPCO)\n\tdev->stats.rx_length_errors += 256;\n    dev->stats.rx_length_errors += in_8(&mb->rntpc);  \n    if (intr & CERR)\n\t++dev->stats.tx_heartbeat_errors;\n    if (intr & BABBLE)\n\tif (mace_babbles++ < 4)\n\t    printk(KERN_DEBUG \"mace: babbling transmitter\\n\");\n    if (intr & JABBER)\n\tif (mace_jabbers++ < 4)\n\t    printk(KERN_DEBUG \"mace: jabbering transceiver\\n\");\n}\n\nstatic irqreturn_t mace_interrupt(int irq, void *dev_id)\n{\n    struct net_device *dev = (struct net_device *) dev_id;\n    struct mace_data *mp = netdev_priv(dev);\n    volatile struct mace __iomem *mb = mp->mace;\n    volatile struct dbdma_regs __iomem *td = mp->tx_dma;\n    volatile struct dbdma_cmd *cp;\n    int intr, fs, i, stat, x;\n    int xcount, dstat;\n    unsigned long flags;\n     \n\n    spin_lock_irqsave(&mp->lock, flags);\n    intr = in_8(&mb->ir);\t\t \n    in_8(&mb->xmtrc);\t\t\t \n    mace_handle_misc_intrs(mp, intr, dev);\n\n    i = mp->tx_empty;\n    while (in_8(&mb->pr) & XMTSV) {\n\tdel_timer(&mp->tx_timeout);\n\tmp->timeout_active = 0;\n\t \n\tintr = in_8(&mb->ir);\n\tif (intr != 0)\n\t    mace_handle_misc_intrs(mp, intr, dev);\n\tif (mp->tx_bad_runt) {\n\t    fs = in_8(&mb->xmtfs);\n\t    mp->tx_bad_runt = 0;\n\t    out_8(&mb->xmtfc, AUTO_PAD_XMIT);\n\t    continue;\n\t}\n\tdstat = le32_to_cpu(td->status);\n\t \n\tout_le32(&td->control, RUN << 16);\n\t \n\txcount = (in_8(&mb->fifofc) >> XMTFC_SH) & XMTFC_MASK;\n\tif (xcount == 0 || (dstat & DEAD)) {\n\t     \n\t    out_8(&mb->xmtfc, DXMTFCS);\n\t}\n\tfs = in_8(&mb->xmtfs);\n\tif ((fs & XMTSV) == 0) {\n\t    printk(KERN_ERR \"mace: xmtfs not valid! (fs=%x xc=%d ds=%x)\\n\",\n\t\t   fs, xcount, dstat);\n\t    mace_reset(dev);\n\t\t \n\t}\n\tcp = mp->tx_cmds + NCMDS_TX * i;\n\tstat = le16_to_cpu(cp->xfer_status);\n\tif ((fs & (UFLO|LCOL|LCAR|RTRY)) || (dstat & DEAD) || xcount == 0) {\n\t     \n\t    udelay(1);\n\t    x = (in_8(&mb->fifofc) >> XMTFC_SH) & XMTFC_MASK;\n\t    if (x != 0) {\n\t\t \n\t\tmp->tx_bad_runt = 1;\n\t\tmace_set_timeout(dev);\n\t    } else {\n\t\t \n\t\tout_8(&mb->maccc, in_8(&mb->maccc) & ~ENXMT);\n\t\tout_8(&mb->fifocc, in_8(&mb->fifocc) | XMTFWU);\n\t\tudelay(1);\n\t\tout_8(&mb->maccc, in_8(&mb->maccc) | ENXMT);\n\t\tout_8(&mb->xmtfc, AUTO_PAD_XMIT);\n\t    }\n\t}\n\t \n\tif (i == mp->tx_fill) {\n\t    printk(KERN_DEBUG \"mace: tx ring ran out? (fs=%x xc=%d ds=%x)\\n\",\n\t\t   fs, xcount, dstat);\n\t    continue;\n\t}\n\t \n\tif (fs & (UFLO|LCOL|LCAR|RTRY)) {\n\t    ++dev->stats.tx_errors;\n\t    if (fs & LCAR)\n\t\t++dev->stats.tx_carrier_errors;\n\t    if (fs & (UFLO|LCOL|RTRY))\n\t\t++dev->stats.tx_aborted_errors;\n\t} else {\n\t    dev->stats.tx_bytes += mp->tx_bufs[i]->len;\n\t    ++dev->stats.tx_packets;\n\t}\n\tdev_consume_skb_irq(mp->tx_bufs[i]);\n\t--mp->tx_active;\n\tif (++i >= N_TX_RING)\n\t    i = 0;\n#if 0\n\tmace_last_fs = fs;\n\tmace_last_xcount = xcount;\n#endif\n    }\n\n    if (i != mp->tx_empty) {\n\tmp->tx_fullup = 0;\n\tnetif_wake_queue(dev);\n    }\n    mp->tx_empty = i;\n    i += mp->tx_active;\n    if (i >= N_TX_RING)\n\ti -= N_TX_RING;\n    if (!mp->tx_bad_runt && i != mp->tx_fill && mp->tx_active < MAX_TX_ACTIVE) {\n\tdo {\n\t     \n\t    cp = mp->tx_cmds + NCMDS_TX * i;\n\t    out_le16(&cp->xfer_status, 0);\n\t    out_le16(&cp->command, OUTPUT_LAST);\n\t    ++mp->tx_active;\n\t    if (++i >= N_TX_RING)\n\t\ti = 0;\n\t} while (i != mp->tx_fill && mp->tx_active < MAX_TX_ACTIVE);\n\tout_le32(&td->control, ((RUN|WAKE) << 16) + (RUN|WAKE));\n\tmace_set_timeout(dev);\n    }\n    spin_unlock_irqrestore(&mp->lock, flags);\n    return IRQ_HANDLED;\n}\n\nstatic void mace_tx_timeout(struct timer_list *t)\n{\n    struct mace_data *mp = from_timer(mp, t, tx_timeout);\n    struct net_device *dev = macio_get_drvdata(mp->mdev);\n    volatile struct mace __iomem *mb = mp->mace;\n    volatile struct dbdma_regs __iomem *td = mp->tx_dma;\n    volatile struct dbdma_regs __iomem *rd = mp->rx_dma;\n    volatile struct dbdma_cmd *cp;\n    unsigned long flags;\n    int i;\n\n    spin_lock_irqsave(&mp->lock, flags);\n    mp->timeout_active = 0;\n    if (mp->tx_active == 0 && !mp->tx_bad_runt)\n\tgoto out;\n\n     \n    mace_handle_misc_intrs(mp, in_8(&mb->ir), dev);\n\n    cp = mp->tx_cmds + NCMDS_TX * mp->tx_empty;\n\n     \n    out_8(&mb->maccc, 0);\n    printk(KERN_ERR \"mace: transmit timeout - resetting\\n\");\n    dbdma_reset(td);\n    mace_reset(dev);\n\n     \n    cp = bus_to_virt(le32_to_cpu(rd->cmdptr));\n    dbdma_reset(rd);\n    out_le16(&cp->xfer_status, 0);\n    out_le32(&rd->cmdptr, virt_to_bus(cp));\n    out_le32(&rd->control, (RUN << 16) | RUN);\n\n     \n    i = mp->tx_empty;\n    mp->tx_active = 0;\n    ++dev->stats.tx_errors;\n    if (mp->tx_bad_runt) {\n\tmp->tx_bad_runt = 0;\n    } else if (i != mp->tx_fill) {\n\tdev_kfree_skb_irq(mp->tx_bufs[i]);\n\tif (++i >= N_TX_RING)\n\t    i = 0;\n\tmp->tx_empty = i;\n    }\n    mp->tx_fullup = 0;\n    netif_wake_queue(dev);\n    if (i != mp->tx_fill) {\n\tcp = mp->tx_cmds + NCMDS_TX * i;\n\tout_le16(&cp->xfer_status, 0);\n\tout_le16(&cp->command, OUTPUT_LAST);\n\tout_le32(&td->cmdptr, virt_to_bus(cp));\n\tout_le32(&td->control, (RUN << 16) | RUN);\n\t++mp->tx_active;\n\tmace_set_timeout(dev);\n    }\n\n     \n    out_8(&mb->imr, RCVINT);\n    out_8(&mb->maccc, mp->maccc);\n\nout:\n    spin_unlock_irqrestore(&mp->lock, flags);\n}\n\nstatic irqreturn_t mace_txdma_intr(int irq, void *dev_id)\n{\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t mace_rxdma_intr(int irq, void *dev_id)\n{\n    struct net_device *dev = (struct net_device *) dev_id;\n    struct mace_data *mp = netdev_priv(dev);\n    volatile struct dbdma_regs __iomem *rd = mp->rx_dma;\n    volatile struct dbdma_cmd *cp, *np;\n    int i, nb, stat, next;\n    struct sk_buff *skb;\n    unsigned frame_status;\n    static int mace_lost_status;\n    unsigned char *data;\n    unsigned long flags;\n\n    spin_lock_irqsave(&mp->lock, flags);\n    for (i = mp->rx_empty; i != mp->rx_fill; ) {\n\tcp = mp->rx_cmds + i;\n\tstat = le16_to_cpu(cp->xfer_status);\n\tif ((stat & ACTIVE) == 0) {\n\t    next = i + 1;\n\t    if (next >= N_RX_RING)\n\t\tnext = 0;\n\t    np = mp->rx_cmds + next;\n\t    if (next != mp->rx_fill &&\n\t\t(le16_to_cpu(np->xfer_status) & ACTIVE) != 0) {\n\t\tprintk(KERN_DEBUG \"mace: lost a status word\\n\");\n\t\t++mace_lost_status;\n\t    } else\n\t\tbreak;\n\t}\n\tnb = le16_to_cpu(cp->req_count) - le16_to_cpu(cp->res_count);\n\tout_le16(&cp->command, DBDMA_STOP);\n\t \n\tskb = mp->rx_bufs[i];\n\tif (!skb) {\n\t    ++dev->stats.rx_dropped;\n\t} else if (nb > 8) {\n\t    data = skb->data;\n\t    frame_status = (data[nb-3] << 8) + data[nb-4];\n\t    if (frame_status & (RS_OFLO|RS_CLSN|RS_FRAMERR|RS_FCSERR)) {\n\t\t++dev->stats.rx_errors;\n\t\tif (frame_status & RS_OFLO)\n\t\t    ++dev->stats.rx_over_errors;\n\t\tif (frame_status & RS_FRAMERR)\n\t\t    ++dev->stats.rx_frame_errors;\n\t\tif (frame_status & RS_FCSERR)\n\t\t    ++dev->stats.rx_crc_errors;\n\t    } else {\n\t\t \n\t\tif (*(unsigned short *)(data+12) < 1536)  \n\t\t    nb -= 4;\n\t\telse\t \n\t\t    nb -= 8;\n\t\tskb_put(skb, nb);\n\t\tskb->protocol = eth_type_trans(skb, dev);\n\t\tdev->stats.rx_bytes += skb->len;\n\t\tnetif_rx(skb);\n\t\tmp->rx_bufs[i] = NULL;\n\t\t++dev->stats.rx_packets;\n\t    }\n\t} else {\n\t    ++dev->stats.rx_errors;\n\t    ++dev->stats.rx_length_errors;\n\t}\n\n\t \n\tif (++i >= N_RX_RING)\n\t    i = 0;\n    }\n    mp->rx_empty = i;\n\n    i = mp->rx_fill;\n    for (;;) {\n\tnext = i + 1;\n\tif (next >= N_RX_RING)\n\t    next = 0;\n\tif (next == mp->rx_empty)\n\t    break;\n\tcp = mp->rx_cmds + i;\n\tskb = mp->rx_bufs[i];\n\tif (!skb) {\n\t    skb = netdev_alloc_skb(dev, RX_BUFLEN + 2);\n\t    if (skb) {\n\t\tskb_reserve(skb, 2);\n\t\tmp->rx_bufs[i] = skb;\n\t    }\n\t}\n\tcp->req_count = cpu_to_le16(RX_BUFLEN);\n\tdata = skb? skb->data: dummy_buf;\n\tcp->phy_addr = cpu_to_le32(virt_to_bus(data));\n\tout_le16(&cp->xfer_status, 0);\n\tout_le16(&cp->command, INPUT_LAST + INTR_ALWAYS);\n#if 0\n\tif ((le32_to_cpu(rd->status) & ACTIVE) != 0) {\n\t    out_le32(&rd->control, (PAUSE << 16) | PAUSE);\n\t    while ((in_le32(&rd->status) & ACTIVE) != 0)\n\t\t;\n\t}\n#endif\n\ti = next;\n    }\n    if (i != mp->rx_fill) {\n\tout_le32(&rd->control, ((RUN|WAKE) << 16) | (RUN|WAKE));\n\tmp->rx_fill = i;\n    }\n    spin_unlock_irqrestore(&mp->lock, flags);\n    return IRQ_HANDLED;\n}\n\nstatic const struct of_device_id mace_match[] =\n{\n\t{\n\t.name \t\t= \"mace\",\n\t},\n\t{},\n};\nMODULE_DEVICE_TABLE (of, mace_match);\n\nstatic struct macio_driver mace_driver =\n{\n\t.driver = {\n\t\t.name \t\t= \"mace\",\n\t\t.owner\t\t= THIS_MODULE,\n\t\t.of_match_table\t= mace_match,\n\t},\n\t.probe\t\t= mace_probe,\n\t.remove\t\t= mace_remove,\n};\n\n\nstatic int __init mace_init(void)\n{\n\treturn macio_register_driver(&mace_driver);\n}\n\nstatic void __exit mace_cleanup(void)\n{\n\tmacio_unregister_driver(&mace_driver);\n\n\tkfree(dummy_buf);\n\tdummy_buf = NULL;\n}\n\nMODULE_AUTHOR(\"Paul Mackerras\");\nMODULE_DESCRIPTION(\"PowerMac MACE driver.\");\nmodule_param(port_aaui, int, 0);\nMODULE_PARM_DESC(port_aaui, \"MACE uses AAUI port (0-1)\");\nMODULE_LICENSE(\"GPL\");\n\nmodule_init(mace_init);\nmodule_exit(mace_cleanup);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}