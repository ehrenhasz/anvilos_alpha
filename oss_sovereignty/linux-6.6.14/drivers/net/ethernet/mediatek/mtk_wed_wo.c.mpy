{
  "module_name": "mtk_wed_wo.c",
  "hash_id": "e6728abec94f22c93f78a1f187c10da90651aa65fbe95182b3484a2e62b4d0d6",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mediatek/mtk_wed_wo.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/dma-mapping.h>\n#include <linux/interrupt.h>\n#include <linux/mfd/syscon.h>\n#include <linux/of.h>\n#include <linux/of_irq.h>\n#include <linux/bitfield.h>\n\n#include \"mtk_wed.h\"\n#include \"mtk_wed_regs.h\"\n#include \"mtk_wed_wo.h\"\n\nstatic u32\nmtk_wed_mmio_r32(struct mtk_wed_wo *wo, u32 reg)\n{\n\tu32 val;\n\n\tif (regmap_read(wo->mmio.regs, reg, &val))\n\t\tval = ~0;\n\n\treturn val;\n}\n\nstatic void\nmtk_wed_mmio_w32(struct mtk_wed_wo *wo, u32 reg, u32 val)\n{\n\tregmap_write(wo->mmio.regs, reg, val);\n}\n\nstatic u32\nmtk_wed_wo_get_isr(struct mtk_wed_wo *wo)\n{\n\tu32 val = mtk_wed_mmio_r32(wo, MTK_WED_WO_CCIF_RCHNUM);\n\n\treturn val & MTK_WED_WO_CCIF_RCHNUM_MASK;\n}\n\nstatic void\nmtk_wed_wo_set_isr(struct mtk_wed_wo *wo, u32 mask)\n{\n\tmtk_wed_mmio_w32(wo, MTK_WED_WO_CCIF_IRQ0_MASK, mask);\n}\n\nstatic void\nmtk_wed_wo_set_ack(struct mtk_wed_wo *wo, u32 mask)\n{\n\tmtk_wed_mmio_w32(wo, MTK_WED_WO_CCIF_ACK, mask);\n}\n\nstatic void\nmtk_wed_wo_set_isr_mask(struct mtk_wed_wo *wo, u32 mask, u32 val, bool set)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&wo->mmio.lock, flags);\n\two->mmio.irq_mask &= ~mask;\n\two->mmio.irq_mask |= val;\n\tif (set)\n\t\tmtk_wed_wo_set_isr(wo, wo->mmio.irq_mask);\n\tspin_unlock_irqrestore(&wo->mmio.lock, flags);\n}\n\nstatic void\nmtk_wed_wo_irq_enable(struct mtk_wed_wo *wo, u32 mask)\n{\n\tmtk_wed_wo_set_isr_mask(wo, 0, mask, false);\n\ttasklet_schedule(&wo->mmio.irq_tasklet);\n}\n\nstatic void\nmtk_wed_wo_irq_disable(struct mtk_wed_wo *wo, u32 mask)\n{\n\tmtk_wed_wo_set_isr_mask(wo, mask, 0, true);\n}\n\nstatic void\nmtk_wed_wo_kickout(struct mtk_wed_wo *wo)\n{\n\tmtk_wed_mmio_w32(wo, MTK_WED_WO_CCIF_BUSY, 1 << MTK_WED_WO_TXCH_NUM);\n\tmtk_wed_mmio_w32(wo, MTK_WED_WO_CCIF_TCHNUM, MTK_WED_WO_TXCH_NUM);\n}\n\nstatic void\nmtk_wed_wo_queue_kick(struct mtk_wed_wo *wo, struct mtk_wed_wo_queue *q,\n\t\t      u32 val)\n{\n\twmb();\n\tmtk_wed_mmio_w32(wo, q->regs.cpu_idx, val);\n}\n\nstatic void *\nmtk_wed_wo_dequeue(struct mtk_wed_wo *wo, struct mtk_wed_wo_queue *q, u32 *len,\n\t\t   bool flush)\n{\n\tint buf_len = SKB_WITH_OVERHEAD(q->buf_size);\n\tint index = (q->tail + 1) % q->n_desc;\n\tstruct mtk_wed_wo_queue_entry *entry;\n\tstruct mtk_wed_wo_queue_desc *desc;\n\tvoid *buf;\n\n\tif (!q->queued)\n\t\treturn NULL;\n\n\tif (flush)\n\t\tq->desc[index].ctrl |= cpu_to_le32(MTK_WED_WO_CTL_DMA_DONE);\n\telse if (!(q->desc[index].ctrl & cpu_to_le32(MTK_WED_WO_CTL_DMA_DONE)))\n\t\treturn NULL;\n\n\tq->tail = index;\n\tq->queued--;\n\n\tdesc = &q->desc[index];\n\tentry = &q->entry[index];\n\tbuf = entry->buf;\n\tif (len)\n\t\t*len = FIELD_GET(MTK_WED_WO_CTL_SD_LEN0,\n\t\t\t\t le32_to_cpu(READ_ONCE(desc->ctrl)));\n\tif (buf)\n\t\tdma_unmap_single(wo->hw->dev, entry->addr, buf_len,\n\t\t\t\t DMA_FROM_DEVICE);\n\tentry->buf = NULL;\n\n\treturn buf;\n}\n\nstatic int\nmtk_wed_wo_queue_refill(struct mtk_wed_wo *wo, struct mtk_wed_wo_queue *q,\n\t\t\tbool rx)\n{\n\tenum dma_data_direction dir = rx ? DMA_FROM_DEVICE : DMA_TO_DEVICE;\n\tint n_buf = 0;\n\n\twhile (q->queued < q->n_desc) {\n\t\tstruct mtk_wed_wo_queue_entry *entry;\n\t\tdma_addr_t addr;\n\t\tvoid *buf;\n\n\t\tbuf = page_frag_alloc(&q->cache, q->buf_size, GFP_ATOMIC);\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\taddr = dma_map_single(wo->hw->dev, buf, q->buf_size, dir);\n\t\tif (unlikely(dma_mapping_error(wo->hw->dev, addr))) {\n\t\t\tskb_free_frag(buf);\n\t\t\tbreak;\n\t\t}\n\n\t\tq->head = (q->head + 1) % q->n_desc;\n\t\tentry = &q->entry[q->head];\n\t\tentry->addr = addr;\n\t\tentry->len = q->buf_size;\n\t\tq->entry[q->head].buf = buf;\n\n\t\tif (rx) {\n\t\t\tstruct mtk_wed_wo_queue_desc *desc = &q->desc[q->head];\n\t\t\tu32 ctrl = MTK_WED_WO_CTL_LAST_SEC0 |\n\t\t\t\t   FIELD_PREP(MTK_WED_WO_CTL_SD_LEN0,\n\t\t\t\t\t      entry->len);\n\n\t\t\tWRITE_ONCE(desc->buf0, cpu_to_le32(addr));\n\t\t\tWRITE_ONCE(desc->ctrl, cpu_to_le32(ctrl));\n\t\t}\n\t\tq->queued++;\n\t\tn_buf++;\n\t}\n\n\treturn n_buf;\n}\n\nstatic void\nmtk_wed_wo_rx_complete(struct mtk_wed_wo *wo)\n{\n\tmtk_wed_wo_set_ack(wo, MTK_WED_WO_RXCH_INT_MASK);\n\tmtk_wed_wo_irq_enable(wo, MTK_WED_WO_RXCH_INT_MASK);\n}\n\nstatic void\nmtk_wed_wo_rx_run_queue(struct mtk_wed_wo *wo, struct mtk_wed_wo_queue *q)\n{\n\tfor (;;) {\n\t\tstruct mtk_wed_mcu_hdr *hdr;\n\t\tstruct sk_buff *skb;\n\t\tvoid *data;\n\t\tu32 len;\n\n\t\tdata = mtk_wed_wo_dequeue(wo, q, &len, false);\n\t\tif (!data)\n\t\t\tbreak;\n\n\t\tskb = build_skb(data, q->buf_size);\n\t\tif (!skb) {\n\t\t\tskb_free_frag(data);\n\t\t\tcontinue;\n\t\t}\n\n\t\t__skb_put(skb, len);\n\t\tif (mtk_wed_mcu_check_msg(wo, skb)) {\n\t\t\tdev_kfree_skb(skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\thdr = (struct mtk_wed_mcu_hdr *)skb->data;\n\t\tif (hdr->flag & cpu_to_le16(MTK_WED_WARP_CMD_FLAG_RSP))\n\t\t\tmtk_wed_mcu_rx_event(wo, skb);\n\t\telse\n\t\t\tmtk_wed_mcu_rx_unsolicited_event(wo, skb);\n\t}\n\n\tif (mtk_wed_wo_queue_refill(wo, q, true)) {\n\t\tu32 index = (q->head - 1) % q->n_desc;\n\n\t\tmtk_wed_wo_queue_kick(wo, q, index);\n\t}\n}\n\nstatic irqreturn_t\nmtk_wed_wo_irq_handler(int irq, void *data)\n{\n\tstruct mtk_wed_wo *wo = data;\n\n\tmtk_wed_wo_set_isr(wo, 0);\n\ttasklet_schedule(&wo->mmio.irq_tasklet);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void mtk_wed_wo_irq_tasklet(struct tasklet_struct *t)\n{\n\tstruct mtk_wed_wo *wo = from_tasklet(wo, t, mmio.irq_tasklet);\n\tu32 intr, mask;\n\n\t \n\tmtk_wed_wo_set_isr(wo, 0);\n\n\tintr = mtk_wed_wo_get_isr(wo);\n\tintr &= wo->mmio.irq_mask;\n\tmask = intr & (MTK_WED_WO_RXCH_INT_MASK | MTK_WED_WO_EXCEPTION_INT_MASK);\n\tmtk_wed_wo_irq_disable(wo, mask);\n\n\tif (intr & MTK_WED_WO_RXCH_INT_MASK) {\n\t\tmtk_wed_wo_rx_run_queue(wo, &wo->q_rx);\n\t\tmtk_wed_wo_rx_complete(wo);\n\t}\n}\n\n \n\nstatic int\nmtk_wed_wo_queue_alloc(struct mtk_wed_wo *wo, struct mtk_wed_wo_queue *q,\n\t\t       int n_desc, int buf_size, int index,\n\t\t       struct mtk_wed_wo_queue_regs *regs)\n{\n\tq->regs = *regs;\n\tq->n_desc = n_desc;\n\tq->buf_size = buf_size;\n\n\tq->desc = dmam_alloc_coherent(wo->hw->dev, n_desc * sizeof(*q->desc),\n\t\t\t\t      &q->desc_dma, GFP_KERNEL);\n\tif (!q->desc)\n\t\treturn -ENOMEM;\n\n\tq->entry = devm_kzalloc(wo->hw->dev, n_desc * sizeof(*q->entry),\n\t\t\t\tGFP_KERNEL);\n\tif (!q->entry)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void\nmtk_wed_wo_queue_free(struct mtk_wed_wo *wo, struct mtk_wed_wo_queue *q)\n{\n\tmtk_wed_mmio_w32(wo, q->regs.cpu_idx, 0);\n\tdma_free_coherent(wo->hw->dev, q->n_desc * sizeof(*q->desc), q->desc,\n\t\t\t  q->desc_dma);\n}\n\nstatic void\nmtk_wed_wo_queue_tx_clean(struct mtk_wed_wo *wo, struct mtk_wed_wo_queue *q)\n{\n\tstruct page *page;\n\tint i;\n\n\tfor (i = 0; i < q->n_desc; i++) {\n\t\tstruct mtk_wed_wo_queue_entry *entry = &q->entry[i];\n\n\t\tif (!entry->buf)\n\t\t\tcontinue;\n\n\t\tdma_unmap_single(wo->hw->dev, entry->addr, entry->len,\n\t\t\t\t DMA_TO_DEVICE);\n\t\tskb_free_frag(entry->buf);\n\t\tentry->buf = NULL;\n\t}\n\n\tif (!q->cache.va)\n\t\treturn;\n\n\tpage = virt_to_page(q->cache.va);\n\t__page_frag_cache_drain(page, q->cache.pagecnt_bias);\n\tmemset(&q->cache, 0, sizeof(q->cache));\n}\n\nstatic void\nmtk_wed_wo_queue_rx_clean(struct mtk_wed_wo *wo, struct mtk_wed_wo_queue *q)\n{\n\tstruct page *page;\n\n\tfor (;;) {\n\t\tvoid *buf = mtk_wed_wo_dequeue(wo, q, NULL, true);\n\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\tskb_free_frag(buf);\n\t}\n\n\tif (!q->cache.va)\n\t\treturn;\n\n\tpage = virt_to_page(q->cache.va);\n\t__page_frag_cache_drain(page, q->cache.pagecnt_bias);\n\tmemset(&q->cache, 0, sizeof(q->cache));\n}\n\nstatic void\nmtk_wed_wo_queue_reset(struct mtk_wed_wo *wo, struct mtk_wed_wo_queue *q)\n{\n\tmtk_wed_mmio_w32(wo, q->regs.cpu_idx, 0);\n\tmtk_wed_mmio_w32(wo, q->regs.desc_base, q->desc_dma);\n\tmtk_wed_mmio_w32(wo, q->regs.ring_size, q->n_desc);\n}\n\nint mtk_wed_wo_queue_tx_skb(struct mtk_wed_wo *wo, struct mtk_wed_wo_queue *q,\n\t\t\t    struct sk_buff *skb)\n{\n\tstruct mtk_wed_wo_queue_entry *entry;\n\tstruct mtk_wed_wo_queue_desc *desc;\n\tint ret = 0, index;\n\tu32 ctrl;\n\n\tq->tail = mtk_wed_mmio_r32(wo, q->regs.dma_idx);\n\tindex = (q->head + 1) % q->n_desc;\n\tif (q->tail == index) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tentry = &q->entry[index];\n\tif (skb->len > entry->len) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tdesc = &q->desc[index];\n\tq->head = index;\n\n\tdma_sync_single_for_cpu(wo->hw->dev, entry->addr, skb->len,\n\t\t\t\tDMA_TO_DEVICE);\n\tmemcpy(entry->buf, skb->data, skb->len);\n\tdma_sync_single_for_device(wo->hw->dev, entry->addr, skb->len,\n\t\t\t\t   DMA_TO_DEVICE);\n\n\tctrl = FIELD_PREP(MTK_WED_WO_CTL_SD_LEN0, skb->len) |\n\t       MTK_WED_WO_CTL_LAST_SEC0 | MTK_WED_WO_CTL_DMA_DONE;\n\tWRITE_ONCE(desc->buf0, cpu_to_le32(entry->addr));\n\tWRITE_ONCE(desc->ctrl, cpu_to_le32(ctrl));\n\n\tmtk_wed_wo_queue_kick(wo, q, q->head);\n\tmtk_wed_wo_kickout(wo);\nout:\n\tdev_kfree_skb(skb);\n\n\treturn ret;\n}\n\nstatic int\nmtk_wed_wo_exception_init(struct mtk_wed_wo *wo)\n{\n\treturn 0;\n}\n\nstatic int\nmtk_wed_wo_hardware_init(struct mtk_wed_wo *wo)\n{\n\tstruct mtk_wed_wo_queue_regs regs;\n\tstruct device_node *np;\n\tint ret;\n\n\tnp = of_parse_phandle(wo->hw->node, \"mediatek,wo-ccif\", 0);\n\tif (!np)\n\t\treturn -ENODEV;\n\n\two->mmio.regs = syscon_regmap_lookup_by_phandle(np, NULL);\n\tif (IS_ERR(wo->mmio.regs)) {\n\t\tret = PTR_ERR(wo->mmio.regs);\n\t\tgoto error_put;\n\t}\n\n\two->mmio.irq = irq_of_parse_and_map(np, 0);\n\two->mmio.irq_mask = MTK_WED_WO_ALL_INT_MASK;\n\tspin_lock_init(&wo->mmio.lock);\n\ttasklet_setup(&wo->mmio.irq_tasklet, mtk_wed_wo_irq_tasklet);\n\n\tret = devm_request_irq(wo->hw->dev, wo->mmio.irq,\n\t\t\t       mtk_wed_wo_irq_handler, IRQF_TRIGGER_HIGH,\n\t\t\t       KBUILD_MODNAME, wo);\n\tif (ret)\n\t\tgoto error;\n\n\tregs.desc_base = MTK_WED_WO_CCIF_DUMMY1;\n\tregs.ring_size = MTK_WED_WO_CCIF_DUMMY2;\n\tregs.dma_idx = MTK_WED_WO_CCIF_SHADOW4;\n\tregs.cpu_idx = MTK_WED_WO_CCIF_DUMMY3;\n\n\tret = mtk_wed_wo_queue_alloc(wo, &wo->q_tx, MTK_WED_WO_RING_SIZE,\n\t\t\t\t     MTK_WED_WO_CMD_LEN, MTK_WED_WO_TXCH_NUM,\n\t\t\t\t     &regs);\n\tif (ret)\n\t\tgoto error;\n\n\tmtk_wed_wo_queue_refill(wo, &wo->q_tx, false);\n\tmtk_wed_wo_queue_reset(wo, &wo->q_tx);\n\n\tregs.desc_base = MTK_WED_WO_CCIF_DUMMY5;\n\tregs.ring_size = MTK_WED_WO_CCIF_DUMMY6;\n\tregs.dma_idx = MTK_WED_WO_CCIF_SHADOW8;\n\tregs.cpu_idx = MTK_WED_WO_CCIF_DUMMY7;\n\n\tret = mtk_wed_wo_queue_alloc(wo, &wo->q_rx, MTK_WED_WO_RING_SIZE,\n\t\t\t\t     MTK_WED_WO_CMD_LEN, MTK_WED_WO_RXCH_NUM,\n\t\t\t\t     &regs);\n\tif (ret)\n\t\tgoto error;\n\n\tmtk_wed_wo_queue_refill(wo, &wo->q_rx, true);\n\tmtk_wed_wo_queue_reset(wo, &wo->q_rx);\n\n\t \n\tmtk_wed_wo_set_isr(wo, wo->mmio.irq_mask);\n\n\treturn 0;\n\nerror:\n\tdevm_free_irq(wo->hw->dev, wo->mmio.irq, wo);\nerror_put:\n\tof_node_put(np);\n\treturn ret;\n}\n\nstatic void\nmtk_wed_wo_hw_deinit(struct mtk_wed_wo *wo)\n{\n\t \n\tmtk_wed_wo_set_isr(wo, 0);\n\n\ttasklet_disable(&wo->mmio.irq_tasklet);\n\n\tdisable_irq(wo->mmio.irq);\n\tdevm_free_irq(wo->hw->dev, wo->mmio.irq, wo);\n\n\tmtk_wed_wo_queue_tx_clean(wo, &wo->q_tx);\n\tmtk_wed_wo_queue_rx_clean(wo, &wo->q_rx);\n\tmtk_wed_wo_queue_free(wo, &wo->q_tx);\n\tmtk_wed_wo_queue_free(wo, &wo->q_rx);\n}\n\nint mtk_wed_wo_init(struct mtk_wed_hw *hw)\n{\n\tstruct mtk_wed_wo *wo;\n\tint ret;\n\n\two = devm_kzalloc(hw->dev, sizeof(*wo), GFP_KERNEL);\n\tif (!wo)\n\t\treturn -ENOMEM;\n\n\thw->wed_wo = wo;\n\two->hw = hw;\n\n\tret = mtk_wed_wo_hardware_init(wo);\n\tif (ret)\n\t\treturn ret;\n\n\tret = mtk_wed_mcu_init(wo);\n\tif (ret)\n\t\treturn ret;\n\n\treturn mtk_wed_wo_exception_init(wo);\n}\n\nvoid mtk_wed_wo_deinit(struct mtk_wed_hw *hw)\n{\n\tstruct mtk_wed_wo *wo = hw->wed_wo;\n\n\tmtk_wed_wo_hw_deinit(wo);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}