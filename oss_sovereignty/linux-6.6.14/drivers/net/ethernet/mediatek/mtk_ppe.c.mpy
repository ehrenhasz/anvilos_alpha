{
  "module_name": "mtk_ppe.c",
  "hash_id": "ff1d1486ebe03a2ce23738cb743ba38b0c849594394aacf529043cbcf22ac946",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/ethernet/mediatek/mtk_ppe.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/io.h>\n#include <linux/iopoll.h>\n#include <linux/etherdevice.h>\n#include <linux/platform_device.h>\n#include <linux/if_ether.h>\n#include <linux/if_vlan.h>\n#include <net/dst_metadata.h>\n#include <net/dsa.h>\n#include \"mtk_eth_soc.h\"\n#include \"mtk_ppe.h\"\n#include \"mtk_ppe_regs.h\"\n\nstatic DEFINE_SPINLOCK(ppe_lock);\n\nstatic const struct rhashtable_params mtk_flow_l2_ht_params = {\n\t.head_offset = offsetof(struct mtk_flow_entry, l2_node),\n\t.key_offset = offsetof(struct mtk_flow_entry, data.bridge),\n\t.key_len = offsetof(struct mtk_foe_bridge, key_end),\n\t.automatic_shrinking = true,\n};\n\nstatic void ppe_w32(struct mtk_ppe *ppe, u32 reg, u32 val)\n{\n\twritel(val, ppe->base + reg);\n}\n\nstatic u32 ppe_r32(struct mtk_ppe *ppe, u32 reg)\n{\n\treturn readl(ppe->base + reg);\n}\n\nstatic u32 ppe_m32(struct mtk_ppe *ppe, u32 reg, u32 mask, u32 set)\n{\n\tu32 val;\n\n\tval = ppe_r32(ppe, reg);\n\tval &= ~mask;\n\tval |= set;\n\tppe_w32(ppe, reg, val);\n\n\treturn val;\n}\n\nstatic u32 ppe_set(struct mtk_ppe *ppe, u32 reg, u32 val)\n{\n\treturn ppe_m32(ppe, reg, 0, val);\n}\n\nstatic u32 ppe_clear(struct mtk_ppe *ppe, u32 reg, u32 val)\n{\n\treturn ppe_m32(ppe, reg, val, 0);\n}\n\nstatic u32 mtk_eth_timestamp(struct mtk_eth *eth)\n{\n\treturn mtk_r32(eth, 0x0010) & mtk_get_ib1_ts_mask(eth);\n}\n\nstatic int mtk_ppe_wait_busy(struct mtk_ppe *ppe)\n{\n\tint ret;\n\tu32 val;\n\n\tret = readl_poll_timeout(ppe->base + MTK_PPE_GLO_CFG, val,\n\t\t\t\t !(val & MTK_PPE_GLO_CFG_BUSY),\n\t\t\t\t 20, MTK_PPE_WAIT_TIMEOUT_US);\n\n\tif (ret)\n\t\tdev_err(ppe->dev, \"PPE table busy\");\n\n\treturn ret;\n}\n\nstatic int mtk_ppe_mib_wait_busy(struct mtk_ppe *ppe)\n{\n\tint ret;\n\tu32 val;\n\n\tret = readl_poll_timeout(ppe->base + MTK_PPE_MIB_SER_CR, val,\n\t\t\t\t !(val & MTK_PPE_MIB_SER_CR_ST),\n\t\t\t\t 20, MTK_PPE_WAIT_TIMEOUT_US);\n\n\tif (ret)\n\t\tdev_err(ppe->dev, \"MIB table busy\");\n\n\treturn ret;\n}\n\nstatic int mtk_mib_entry_read(struct mtk_ppe *ppe, u16 index, u64 *bytes, u64 *packets)\n{\n\tu32 val, cnt_r0, cnt_r1, cnt_r2;\n\tint ret;\n\n\tval = FIELD_PREP(MTK_PPE_MIB_SER_CR_ADDR, index) | MTK_PPE_MIB_SER_CR_ST;\n\tppe_w32(ppe, MTK_PPE_MIB_SER_CR, val);\n\n\tret = mtk_ppe_mib_wait_busy(ppe);\n\tif (ret)\n\t\treturn ret;\n\n\tcnt_r0 = readl(ppe->base + MTK_PPE_MIB_SER_R0);\n\tcnt_r1 = readl(ppe->base + MTK_PPE_MIB_SER_R1);\n\tcnt_r2 = readl(ppe->base + MTK_PPE_MIB_SER_R2);\n\n\tif (mtk_is_netsys_v3_or_greater(ppe->eth)) {\n\t\t \n\t\tu32 cnt_r3 = readl(ppe->base + MTK_PPE_MIB_SER_R3);\n\t\t*bytes = ((u64)cnt_r1 << 32) | cnt_r0;\n\t\t*packets = ((u64)cnt_r3 << 32) | cnt_r2;\n\t} else {\n\t\t \n\t\tu32 byte_cnt_low = FIELD_GET(MTK_PPE_MIB_SER_R0_BYTE_CNT_LOW, cnt_r0);\n\t\tu32 byte_cnt_high = FIELD_GET(MTK_PPE_MIB_SER_R1_BYTE_CNT_HIGH, cnt_r1);\n\t\tu32 pkt_cnt_low = FIELD_GET(MTK_PPE_MIB_SER_R1_PKT_CNT_LOW, cnt_r1);\n\t\tu32 pkt_cnt_high = FIELD_GET(MTK_PPE_MIB_SER_R2_PKT_CNT_HIGH, cnt_r2);\n\t\t*bytes = ((u64)byte_cnt_high << 32) | byte_cnt_low;\n\t\t*packets = ((u64)pkt_cnt_high << 16) | pkt_cnt_low;\n\t}\n\n\treturn 0;\n}\n\nstatic void mtk_ppe_cache_clear(struct mtk_ppe *ppe)\n{\n\tppe_set(ppe, MTK_PPE_CACHE_CTL, MTK_PPE_CACHE_CTL_CLEAR);\n\tppe_clear(ppe, MTK_PPE_CACHE_CTL, MTK_PPE_CACHE_CTL_CLEAR);\n}\n\nstatic void mtk_ppe_cache_enable(struct mtk_ppe *ppe, bool enable)\n{\n\tmtk_ppe_cache_clear(ppe);\n\n\tppe_m32(ppe, MTK_PPE_CACHE_CTL, MTK_PPE_CACHE_CTL_EN,\n\t\tenable * MTK_PPE_CACHE_CTL_EN);\n}\n\nstatic u32 mtk_ppe_hash_entry(struct mtk_eth *eth, struct mtk_foe_entry *e)\n{\n\tu32 hv1, hv2, hv3;\n\tu32 hash;\n\n\tswitch (mtk_get_ib1_pkt_type(eth, e->ib1)) {\n\t\tcase MTK_PPE_PKT_TYPE_IPV4_ROUTE:\n\t\tcase MTK_PPE_PKT_TYPE_IPV4_HNAPT:\n\t\t\thv1 = e->ipv4.orig.ports;\n\t\t\thv2 = e->ipv4.orig.dest_ip;\n\t\t\thv3 = e->ipv4.orig.src_ip;\n\t\t\tbreak;\n\t\tcase MTK_PPE_PKT_TYPE_IPV6_ROUTE_3T:\n\t\tcase MTK_PPE_PKT_TYPE_IPV6_ROUTE_5T:\n\t\t\thv1 = e->ipv6.src_ip[3] ^ e->ipv6.dest_ip[3];\n\t\t\thv1 ^= e->ipv6.ports;\n\n\t\t\thv2 = e->ipv6.src_ip[2] ^ e->ipv6.dest_ip[2];\n\t\t\thv2 ^= e->ipv6.dest_ip[0];\n\n\t\t\thv3 = e->ipv6.src_ip[1] ^ e->ipv6.dest_ip[1];\n\t\t\thv3 ^= e->ipv6.src_ip[0];\n\t\t\tbreak;\n\t\tcase MTK_PPE_PKT_TYPE_IPV4_DSLITE:\n\t\tcase MTK_PPE_PKT_TYPE_IPV6_6RD:\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn MTK_PPE_HASH_MASK;\n\t}\n\n\thash = (hv1 & hv2) | ((~hv1) & hv3);\n\thash = (hash >> 24) | ((hash & 0xffffff) << 8);\n\thash ^= hv1 ^ hv2 ^ hv3;\n\thash ^= hash >> 16;\n\thash <<= (ffs(eth->soc->hash_offset) - 1);\n\thash &= MTK_PPE_ENTRIES - 1;\n\n\treturn hash;\n}\n\nstatic inline struct mtk_foe_mac_info *\nmtk_foe_entry_l2(struct mtk_eth *eth, struct mtk_foe_entry *entry)\n{\n\tint type = mtk_get_ib1_pkt_type(eth, entry->ib1);\n\n\tif (type == MTK_PPE_PKT_TYPE_BRIDGE)\n\t\treturn &entry->bridge.l2;\n\n\tif (type >= MTK_PPE_PKT_TYPE_IPV4_DSLITE)\n\t\treturn &entry->ipv6.l2;\n\n\treturn &entry->ipv4.l2;\n}\n\nstatic inline u32 *\nmtk_foe_entry_ib2(struct mtk_eth *eth, struct mtk_foe_entry *entry)\n{\n\tint type = mtk_get_ib1_pkt_type(eth, entry->ib1);\n\n\tif (type == MTK_PPE_PKT_TYPE_BRIDGE)\n\t\treturn &entry->bridge.ib2;\n\n\tif (type >= MTK_PPE_PKT_TYPE_IPV4_DSLITE)\n\t\treturn &entry->ipv6.ib2;\n\n\treturn &entry->ipv4.ib2;\n}\n\nint mtk_foe_entry_prepare(struct mtk_eth *eth, struct mtk_foe_entry *entry,\n\t\t\t  int type, int l4proto, u8 pse_port, u8 *src_mac,\n\t\t\t  u8 *dest_mac)\n{\n\tstruct mtk_foe_mac_info *l2;\n\tu32 ports_pad, val;\n\n\tmemset(entry, 0, sizeof(*entry));\n\n\tif (mtk_is_netsys_v2_or_greater(eth)) {\n\t\tval = FIELD_PREP(MTK_FOE_IB1_STATE, MTK_FOE_STATE_BIND) |\n\t\t      FIELD_PREP(MTK_FOE_IB1_PACKET_TYPE_V2, type) |\n\t\t      FIELD_PREP(MTK_FOE_IB1_UDP, l4proto == IPPROTO_UDP) |\n\t\t      MTK_FOE_IB1_BIND_CACHE_V2 | MTK_FOE_IB1_BIND_TTL_V2;\n\t\tentry->ib1 = val;\n\n\t\tval = FIELD_PREP(MTK_FOE_IB2_DEST_PORT_V2, pse_port) |\n\t\t      FIELD_PREP(MTK_FOE_IB2_PORT_AG_V2, 0xf);\n\t} else {\n\t\tint port_mg = eth->soc->offload_version > 1 ? 0 : 0x3f;\n\n\t\tval = FIELD_PREP(MTK_FOE_IB1_STATE, MTK_FOE_STATE_BIND) |\n\t\t      FIELD_PREP(MTK_FOE_IB1_PACKET_TYPE, type) |\n\t\t      FIELD_PREP(MTK_FOE_IB1_UDP, l4proto == IPPROTO_UDP) |\n\t\t      MTK_FOE_IB1_BIND_CACHE | MTK_FOE_IB1_BIND_TTL;\n\t\tentry->ib1 = val;\n\n\t\tval = FIELD_PREP(MTK_FOE_IB2_DEST_PORT, pse_port) |\n\t\t      FIELD_PREP(MTK_FOE_IB2_PORT_MG, port_mg) |\n\t\t      FIELD_PREP(MTK_FOE_IB2_PORT_AG, 0x1f);\n\t}\n\n\tif (is_multicast_ether_addr(dest_mac))\n\t\tval |= mtk_get_ib2_multicast_mask(eth);\n\n\tports_pad = 0xa5a5a500 | (l4proto & 0xff);\n\tif (type == MTK_PPE_PKT_TYPE_IPV4_ROUTE)\n\t\tentry->ipv4.orig.ports = ports_pad;\n\tif (type == MTK_PPE_PKT_TYPE_IPV6_ROUTE_3T)\n\t\tentry->ipv6.ports = ports_pad;\n\n\tif (type == MTK_PPE_PKT_TYPE_BRIDGE) {\n\t\tether_addr_copy(entry->bridge.src_mac, src_mac);\n\t\tether_addr_copy(entry->bridge.dest_mac, dest_mac);\n\t\tentry->bridge.ib2 = val;\n\t\tl2 = &entry->bridge.l2;\n\t} else if (type >= MTK_PPE_PKT_TYPE_IPV4_DSLITE) {\n\t\tentry->ipv6.ib2 = val;\n\t\tl2 = &entry->ipv6.l2;\n\t} else {\n\t\tentry->ipv4.ib2 = val;\n\t\tl2 = &entry->ipv4.l2;\n\t}\n\n\tl2->dest_mac_hi = get_unaligned_be32(dest_mac);\n\tl2->dest_mac_lo = get_unaligned_be16(dest_mac + 4);\n\tl2->src_mac_hi = get_unaligned_be32(src_mac);\n\tl2->src_mac_lo = get_unaligned_be16(src_mac + 4);\n\n\tif (type >= MTK_PPE_PKT_TYPE_IPV6_ROUTE_3T)\n\t\tl2->etype = ETH_P_IPV6;\n\telse\n\t\tl2->etype = ETH_P_IP;\n\n\treturn 0;\n}\n\nint mtk_foe_entry_set_pse_port(struct mtk_eth *eth,\n\t\t\t       struct mtk_foe_entry *entry, u8 port)\n{\n\tu32 *ib2 = mtk_foe_entry_ib2(eth, entry);\n\tu32 val = *ib2;\n\n\tif (mtk_is_netsys_v2_or_greater(eth)) {\n\t\tval &= ~MTK_FOE_IB2_DEST_PORT_V2;\n\t\tval |= FIELD_PREP(MTK_FOE_IB2_DEST_PORT_V2, port);\n\t} else {\n\t\tval &= ~MTK_FOE_IB2_DEST_PORT;\n\t\tval |= FIELD_PREP(MTK_FOE_IB2_DEST_PORT, port);\n\t}\n\t*ib2 = val;\n\n\treturn 0;\n}\n\nint mtk_foe_entry_set_ipv4_tuple(struct mtk_eth *eth,\n\t\t\t\t struct mtk_foe_entry *entry, bool egress,\n\t\t\t\t __be32 src_addr, __be16 src_port,\n\t\t\t\t __be32 dest_addr, __be16 dest_port)\n{\n\tint type = mtk_get_ib1_pkt_type(eth, entry->ib1);\n\tstruct mtk_ipv4_tuple *t;\n\n\tswitch (type) {\n\tcase MTK_PPE_PKT_TYPE_IPV4_HNAPT:\n\t\tif (egress) {\n\t\t\tt = &entry->ipv4.new;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tcase MTK_PPE_PKT_TYPE_IPV4_DSLITE:\n\tcase MTK_PPE_PKT_TYPE_IPV4_ROUTE:\n\t\tt = &entry->ipv4.orig;\n\t\tbreak;\n\tcase MTK_PPE_PKT_TYPE_IPV6_6RD:\n\t\tentry->ipv6_6rd.tunnel_src_ip = be32_to_cpu(src_addr);\n\t\tentry->ipv6_6rd.tunnel_dest_ip = be32_to_cpu(dest_addr);\n\t\treturn 0;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn -EINVAL;\n\t}\n\n\tt->src_ip = be32_to_cpu(src_addr);\n\tt->dest_ip = be32_to_cpu(dest_addr);\n\n\tif (type == MTK_PPE_PKT_TYPE_IPV4_ROUTE)\n\t\treturn 0;\n\n\tt->src_port = be16_to_cpu(src_port);\n\tt->dest_port = be16_to_cpu(dest_port);\n\n\treturn 0;\n}\n\nint mtk_foe_entry_set_ipv6_tuple(struct mtk_eth *eth,\n\t\t\t\t struct mtk_foe_entry *entry,\n\t\t\t\t __be32 *src_addr, __be16 src_port,\n\t\t\t\t __be32 *dest_addr, __be16 dest_port)\n{\n\tint type = mtk_get_ib1_pkt_type(eth, entry->ib1);\n\tu32 *src, *dest;\n\tint i;\n\n\tswitch (type) {\n\tcase MTK_PPE_PKT_TYPE_IPV4_DSLITE:\n\t\tsrc = entry->dslite.tunnel_src_ip;\n\t\tdest = entry->dslite.tunnel_dest_ip;\n\t\tbreak;\n\tcase MTK_PPE_PKT_TYPE_IPV6_ROUTE_5T:\n\tcase MTK_PPE_PKT_TYPE_IPV6_6RD:\n\t\tentry->ipv6.src_port = be16_to_cpu(src_port);\n\t\tentry->ipv6.dest_port = be16_to_cpu(dest_port);\n\t\tfallthrough;\n\tcase MTK_PPE_PKT_TYPE_IPV6_ROUTE_3T:\n\t\tsrc = entry->ipv6.src_ip;\n\t\tdest = entry->ipv6.dest_ip;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i < 4; i++)\n\t\tsrc[i] = be32_to_cpu(src_addr[i]);\n\tfor (i = 0; i < 4; i++)\n\t\tdest[i] = be32_to_cpu(dest_addr[i]);\n\n\treturn 0;\n}\n\nint mtk_foe_entry_set_dsa(struct mtk_eth *eth, struct mtk_foe_entry *entry,\n\t\t\t  int port)\n{\n\tstruct mtk_foe_mac_info *l2 = mtk_foe_entry_l2(eth, entry);\n\n\tl2->etype = BIT(port);\n\n\tif (!(entry->ib1 & mtk_get_ib1_vlan_layer_mask(eth)))\n\t\tentry->ib1 |= mtk_prep_ib1_vlan_layer(eth, 1);\n\telse\n\t\tl2->etype |= BIT(8);\n\n\tentry->ib1 &= ~mtk_get_ib1_vlan_tag_mask(eth);\n\n\treturn 0;\n}\n\nint mtk_foe_entry_set_vlan(struct mtk_eth *eth, struct mtk_foe_entry *entry,\n\t\t\t   int vid)\n{\n\tstruct mtk_foe_mac_info *l2 = mtk_foe_entry_l2(eth, entry);\n\n\tswitch (mtk_get_ib1_vlan_layer(eth, entry->ib1)) {\n\tcase 0:\n\t\tentry->ib1 |= mtk_get_ib1_vlan_tag_mask(eth) |\n\t\t\t      mtk_prep_ib1_vlan_layer(eth, 1);\n\t\tl2->vlan1 = vid;\n\t\treturn 0;\n\tcase 1:\n\t\tif (!(entry->ib1 & mtk_get_ib1_vlan_tag_mask(eth))) {\n\t\t\tl2->vlan1 = vid;\n\t\t\tl2->etype |= BIT(8);\n\t\t} else {\n\t\t\tl2->vlan2 = vid;\n\t\t\tentry->ib1 += mtk_prep_ib1_vlan_layer(eth, 1);\n\t\t}\n\t\treturn 0;\n\tdefault:\n\t\treturn -ENOSPC;\n\t}\n}\n\nint mtk_foe_entry_set_pppoe(struct mtk_eth *eth, struct mtk_foe_entry *entry,\n\t\t\t    int sid)\n{\n\tstruct mtk_foe_mac_info *l2 = mtk_foe_entry_l2(eth, entry);\n\n\tif (!(entry->ib1 & mtk_get_ib1_vlan_layer_mask(eth)) ||\n\t    (entry->ib1 & mtk_get_ib1_vlan_tag_mask(eth)))\n\t\tl2->etype = ETH_P_PPP_SES;\n\n\tentry->ib1 |= mtk_get_ib1_ppoe_mask(eth);\n\tl2->pppoe_id = sid;\n\n\treturn 0;\n}\n\nint mtk_foe_entry_set_wdma(struct mtk_eth *eth, struct mtk_foe_entry *entry,\n\t\t\t   int wdma_idx, int txq, int bss, int wcid)\n{\n\tstruct mtk_foe_mac_info *l2 = mtk_foe_entry_l2(eth, entry);\n\tu32 *ib2 = mtk_foe_entry_ib2(eth, entry);\n\n\tswitch (eth->soc->version) {\n\tcase 3:\n\t\t*ib2 &= ~MTK_FOE_IB2_PORT_MG_V2;\n\t\t*ib2 |=  FIELD_PREP(MTK_FOE_IB2_RX_IDX, txq) |\n\t\t\t MTK_FOE_IB2_WDMA_WINFO_V2;\n\t\tl2->w3info = FIELD_PREP(MTK_FOE_WINFO_WCID_V3, wcid) |\n\t\t\t     FIELD_PREP(MTK_FOE_WINFO_BSS_V3, bss);\n\t\tbreak;\n\tcase 2:\n\t\t*ib2 &= ~MTK_FOE_IB2_PORT_MG_V2;\n\t\t*ib2 |=  FIELD_PREP(MTK_FOE_IB2_RX_IDX, txq) |\n\t\t\t MTK_FOE_IB2_WDMA_WINFO_V2;\n\t\tl2->winfo = FIELD_PREP(MTK_FOE_WINFO_WCID, wcid) |\n\t\t\t    FIELD_PREP(MTK_FOE_WINFO_BSS, bss);\n\t\tbreak;\n\tdefault:\n\t\t*ib2 &= ~MTK_FOE_IB2_PORT_MG;\n\t\t*ib2 |= MTK_FOE_IB2_WDMA_WINFO;\n\t\tif (wdma_idx)\n\t\t\t*ib2 |= MTK_FOE_IB2_WDMA_DEVIDX;\n\t\tl2->vlan2 = FIELD_PREP(MTK_FOE_VLAN2_WINFO_BSS, bss) |\n\t\t\t    FIELD_PREP(MTK_FOE_VLAN2_WINFO_WCID, wcid) |\n\t\t\t    FIELD_PREP(MTK_FOE_VLAN2_WINFO_RING, txq);\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nint mtk_foe_entry_set_queue(struct mtk_eth *eth, struct mtk_foe_entry *entry,\n\t\t\t    unsigned int queue)\n{\n\tu32 *ib2 = mtk_foe_entry_ib2(eth, entry);\n\n\tif (mtk_is_netsys_v2_or_greater(eth)) {\n\t\t*ib2 &= ~MTK_FOE_IB2_QID_V2;\n\t\t*ib2 |= FIELD_PREP(MTK_FOE_IB2_QID_V2, queue);\n\t\t*ib2 |= MTK_FOE_IB2_PSE_QOS_V2;\n\t} else {\n\t\t*ib2 &= ~MTK_FOE_IB2_QID;\n\t\t*ib2 |= FIELD_PREP(MTK_FOE_IB2_QID, queue);\n\t\t*ib2 |= MTK_FOE_IB2_PSE_QOS;\n\t}\n\n\treturn 0;\n}\n\nstatic bool\nmtk_flow_entry_match(struct mtk_eth *eth, struct mtk_flow_entry *entry,\n\t\t     struct mtk_foe_entry *data)\n{\n\tint type, len;\n\n\tif ((data->ib1 ^ entry->data.ib1) & MTK_FOE_IB1_UDP)\n\t\treturn false;\n\n\ttype = mtk_get_ib1_pkt_type(eth, entry->data.ib1);\n\tif (type > MTK_PPE_PKT_TYPE_IPV4_DSLITE)\n\t\tlen = offsetof(struct mtk_foe_entry, ipv6._rsv);\n\telse\n\t\tlen = offsetof(struct mtk_foe_entry, ipv4.ib2);\n\n\treturn !memcmp(&entry->data.data, &data->data, len - 4);\n}\n\nstatic void\n__mtk_foe_entry_clear(struct mtk_ppe *ppe, struct mtk_flow_entry *entry)\n{\n\tstruct hlist_head *head;\n\tstruct hlist_node *tmp;\n\n\tif (entry->type == MTK_FLOW_TYPE_L2) {\n\t\trhashtable_remove_fast(&ppe->l2_flows, &entry->l2_node,\n\t\t\t\t       mtk_flow_l2_ht_params);\n\n\t\thead = &entry->l2_flows;\n\t\thlist_for_each_entry_safe(entry, tmp, head, l2_data.list)\n\t\t\t__mtk_foe_entry_clear(ppe, entry);\n\t\treturn;\n\t}\n\n\thlist_del_init(&entry->list);\n\tif (entry->hash != 0xffff) {\n\t\tstruct mtk_foe_entry *hwe = mtk_foe_get_entry(ppe, entry->hash);\n\n\t\thwe->ib1 &= ~MTK_FOE_IB1_STATE;\n\t\thwe->ib1 |= FIELD_PREP(MTK_FOE_IB1_STATE, MTK_FOE_STATE_INVALID);\n\t\tdma_wmb();\n\t\tmtk_ppe_cache_clear(ppe);\n\n\t\tif (ppe->accounting) {\n\t\t\tstruct mtk_foe_accounting *acct;\n\n\t\t\tacct = ppe->acct_table + entry->hash * sizeof(*acct);\n\t\t\tacct->packets = 0;\n\t\t\tacct->bytes = 0;\n\t\t}\n\t}\n\tentry->hash = 0xffff;\n\n\tif (entry->type != MTK_FLOW_TYPE_L2_SUBFLOW)\n\t\treturn;\n\n\thlist_del_init(&entry->l2_data.list);\n\tkfree(entry);\n}\n\nstatic int __mtk_foe_entry_idle_time(struct mtk_ppe *ppe, u32 ib1)\n{\n\tu32 ib1_ts_mask = mtk_get_ib1_ts_mask(ppe->eth);\n\tu16 now = mtk_eth_timestamp(ppe->eth);\n\tu16 timestamp = ib1 & ib1_ts_mask;\n\n\tif (timestamp > now)\n\t\treturn ib1_ts_mask + 1 - timestamp + now;\n\telse\n\t\treturn now - timestamp;\n}\n\nstatic void\nmtk_flow_entry_update_l2(struct mtk_ppe *ppe, struct mtk_flow_entry *entry)\n{\n\tu32 ib1_ts_mask = mtk_get_ib1_ts_mask(ppe->eth);\n\tstruct mtk_flow_entry *cur;\n\tstruct mtk_foe_entry *hwe;\n\tstruct hlist_node *tmp;\n\tint idle;\n\n\tidle = __mtk_foe_entry_idle_time(ppe, entry->data.ib1);\n\thlist_for_each_entry_safe(cur, tmp, &entry->l2_flows, l2_data.list) {\n\t\tint cur_idle;\n\t\tu32 ib1;\n\n\t\thwe = mtk_foe_get_entry(ppe, cur->hash);\n\t\tib1 = READ_ONCE(hwe->ib1);\n\n\t\tif (FIELD_GET(MTK_FOE_IB1_STATE, ib1) != MTK_FOE_STATE_BIND) {\n\t\t\tcur->hash = 0xffff;\n\t\t\t__mtk_foe_entry_clear(ppe, cur);\n\t\t\tcontinue;\n\t\t}\n\n\t\tcur_idle = __mtk_foe_entry_idle_time(ppe, ib1);\n\t\tif (cur_idle >= idle)\n\t\t\tcontinue;\n\n\t\tidle = cur_idle;\n\t\tentry->data.ib1 &= ~ib1_ts_mask;\n\t\tentry->data.ib1 |= hwe->ib1 & ib1_ts_mask;\n\t}\n}\n\nstatic void\nmtk_flow_entry_update(struct mtk_ppe *ppe, struct mtk_flow_entry *entry)\n{\n\tstruct mtk_foe_entry foe = {};\n\tstruct mtk_foe_entry *hwe;\n\n\tspin_lock_bh(&ppe_lock);\n\n\tif (entry->type == MTK_FLOW_TYPE_L2) {\n\t\tmtk_flow_entry_update_l2(ppe, entry);\n\t\tgoto out;\n\t}\n\n\tif (entry->hash == 0xffff)\n\t\tgoto out;\n\n\thwe = mtk_foe_get_entry(ppe, entry->hash);\n\tmemcpy(&foe, hwe, ppe->eth->soc->foe_entry_size);\n\tif (!mtk_flow_entry_match(ppe->eth, entry, &foe)) {\n\t\tentry->hash = 0xffff;\n\t\tgoto out;\n\t}\n\n\tentry->data.ib1 = foe.ib1;\n\nout:\n\tspin_unlock_bh(&ppe_lock);\n}\n\nstatic void\n__mtk_foe_entry_commit(struct mtk_ppe *ppe, struct mtk_foe_entry *entry,\n\t\t       u16 hash)\n{\n\tstruct mtk_eth *eth = ppe->eth;\n\tu16 timestamp = mtk_eth_timestamp(eth);\n\tstruct mtk_foe_entry *hwe;\n\tu32 val;\n\n\tif (mtk_is_netsys_v2_or_greater(eth)) {\n\t\tentry->ib1 &= ~MTK_FOE_IB1_BIND_TIMESTAMP_V2;\n\t\tentry->ib1 |= FIELD_PREP(MTK_FOE_IB1_BIND_TIMESTAMP_V2,\n\t\t\t\t\t timestamp);\n\t} else {\n\t\tentry->ib1 &= ~MTK_FOE_IB1_BIND_TIMESTAMP;\n\t\tentry->ib1 |= FIELD_PREP(MTK_FOE_IB1_BIND_TIMESTAMP,\n\t\t\t\t\t timestamp);\n\t}\n\n\thwe = mtk_foe_get_entry(ppe, hash);\n\tmemcpy(&hwe->data, &entry->data, eth->soc->foe_entry_size - sizeof(hwe->ib1));\n\twmb();\n\thwe->ib1 = entry->ib1;\n\n\tif (ppe->accounting) {\n\t\tif (mtk_is_netsys_v2_or_greater(eth))\n\t\t\tval = MTK_FOE_IB2_MIB_CNT_V2;\n\t\telse\n\t\t\tval = MTK_FOE_IB2_MIB_CNT;\n\t\t*mtk_foe_entry_ib2(eth, hwe) |= val;\n\t}\n\n\tdma_wmb();\n\n\tmtk_ppe_cache_clear(ppe);\n}\n\nvoid mtk_foe_entry_clear(struct mtk_ppe *ppe, struct mtk_flow_entry *entry)\n{\n\tspin_lock_bh(&ppe_lock);\n\t__mtk_foe_entry_clear(ppe, entry);\n\tspin_unlock_bh(&ppe_lock);\n}\n\nstatic int\nmtk_foe_entry_commit_l2(struct mtk_ppe *ppe, struct mtk_flow_entry *entry)\n{\n\tstruct mtk_flow_entry *prev;\n\n\tentry->type = MTK_FLOW_TYPE_L2;\n\n\tprev = rhashtable_lookup_get_insert_fast(&ppe->l2_flows, &entry->l2_node,\n\t\t\t\t\t\t mtk_flow_l2_ht_params);\n\tif (likely(!prev))\n\t\treturn 0;\n\n\tif (IS_ERR(prev))\n\t\treturn PTR_ERR(prev);\n\n\treturn rhashtable_replace_fast(&ppe->l2_flows, &prev->l2_node,\n\t\t\t\t       &entry->l2_node, mtk_flow_l2_ht_params);\n}\n\nint mtk_foe_entry_commit(struct mtk_ppe *ppe, struct mtk_flow_entry *entry)\n{\n\tconst struct mtk_soc_data *soc = ppe->eth->soc;\n\tint type = mtk_get_ib1_pkt_type(ppe->eth, entry->data.ib1);\n\tu32 hash;\n\n\tif (type == MTK_PPE_PKT_TYPE_BRIDGE)\n\t\treturn mtk_foe_entry_commit_l2(ppe, entry);\n\n\thash = mtk_ppe_hash_entry(ppe->eth, &entry->data);\n\tentry->hash = 0xffff;\n\tspin_lock_bh(&ppe_lock);\n\thlist_add_head(&entry->list, &ppe->foe_flow[hash / soc->hash_offset]);\n\tspin_unlock_bh(&ppe_lock);\n\n\treturn 0;\n}\n\nstatic void\nmtk_foe_entry_commit_subflow(struct mtk_ppe *ppe, struct mtk_flow_entry *entry,\n\t\t\t     u16 hash)\n{\n\tconst struct mtk_soc_data *soc = ppe->eth->soc;\n\tstruct mtk_flow_entry *flow_info;\n\tstruct mtk_foe_entry foe = {}, *hwe;\n\tstruct mtk_foe_mac_info *l2;\n\tu32 ib1_mask = mtk_get_ib1_pkt_type_mask(ppe->eth) | MTK_FOE_IB1_UDP;\n\tint type;\n\n\tflow_info = kzalloc(sizeof(*flow_info), GFP_ATOMIC);\n\tif (!flow_info)\n\t\treturn;\n\n\tflow_info->l2_data.base_flow = entry;\n\tflow_info->type = MTK_FLOW_TYPE_L2_SUBFLOW;\n\tflow_info->hash = hash;\n\thlist_add_head(&flow_info->list,\n\t\t       &ppe->foe_flow[hash / soc->hash_offset]);\n\thlist_add_head(&flow_info->l2_data.list, &entry->l2_flows);\n\n\thwe = mtk_foe_get_entry(ppe, hash);\n\tmemcpy(&foe, hwe, soc->foe_entry_size);\n\tfoe.ib1 &= ib1_mask;\n\tfoe.ib1 |= entry->data.ib1 & ~ib1_mask;\n\n\tl2 = mtk_foe_entry_l2(ppe->eth, &foe);\n\tmemcpy(l2, &entry->data.bridge.l2, sizeof(*l2));\n\n\ttype = mtk_get_ib1_pkt_type(ppe->eth, foe.ib1);\n\tif (type == MTK_PPE_PKT_TYPE_IPV4_HNAPT)\n\t\tmemcpy(&foe.ipv4.new, &foe.ipv4.orig, sizeof(foe.ipv4.new));\n\telse if (type >= MTK_PPE_PKT_TYPE_IPV6_ROUTE_3T && l2->etype == ETH_P_IP)\n\t\tl2->etype = ETH_P_IPV6;\n\n\t*mtk_foe_entry_ib2(ppe->eth, &foe) = entry->data.bridge.ib2;\n\n\t__mtk_foe_entry_commit(ppe, &foe, hash);\n}\n\nvoid __mtk_ppe_check_skb(struct mtk_ppe *ppe, struct sk_buff *skb, u16 hash)\n{\n\tconst struct mtk_soc_data *soc = ppe->eth->soc;\n\tstruct hlist_head *head = &ppe->foe_flow[hash / soc->hash_offset];\n\tstruct mtk_foe_entry *hwe = mtk_foe_get_entry(ppe, hash);\n\tstruct mtk_flow_entry *entry;\n\tstruct mtk_foe_bridge key = {};\n\tstruct hlist_node *n;\n\tstruct ethhdr *eh;\n\tbool found = false;\n\tu8 *tag;\n\n\tspin_lock_bh(&ppe_lock);\n\n\tif (FIELD_GET(MTK_FOE_IB1_STATE, hwe->ib1) == MTK_FOE_STATE_BIND)\n\t\tgoto out;\n\n\thlist_for_each_entry_safe(entry, n, head, list) {\n\t\tif (entry->type == MTK_FLOW_TYPE_L2_SUBFLOW) {\n\t\t\tif (unlikely(FIELD_GET(MTK_FOE_IB1_STATE, hwe->ib1) ==\n\t\t\t\t     MTK_FOE_STATE_BIND))\n\t\t\t\tcontinue;\n\n\t\t\tentry->hash = 0xffff;\n\t\t\t__mtk_foe_entry_clear(ppe, entry);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (found || !mtk_flow_entry_match(ppe->eth, entry, hwe)) {\n\t\t\tif (entry->hash != 0xffff)\n\t\t\t\tentry->hash = 0xffff;\n\t\t\tcontinue;\n\t\t}\n\n\t\tentry->hash = hash;\n\t\t__mtk_foe_entry_commit(ppe, &entry->data, hash);\n\t\tfound = true;\n\t}\n\n\tif (found)\n\t\tgoto out;\n\n\teh = eth_hdr(skb);\n\tether_addr_copy(key.dest_mac, eh->h_dest);\n\tether_addr_copy(key.src_mac, eh->h_source);\n\ttag = skb->data - 2;\n\tkey.vlan = 0;\n\tswitch (skb->protocol) {\n#if IS_ENABLED(CONFIG_NET_DSA)\n\tcase htons(ETH_P_XDSA):\n\t\tif (!netdev_uses_dsa(skb->dev) ||\n\t\t    skb->dev->dsa_ptr->tag_ops->proto != DSA_TAG_PROTO_MTK)\n\t\t\tgoto out;\n\n\t\tif (!skb_metadata_dst(skb))\n\t\t\ttag += 4;\n\n\t\tif (get_unaligned_be16(tag) != ETH_P_8021Q)\n\t\t\tbreak;\n\n\t\tfallthrough;\n#endif\n\tcase htons(ETH_P_8021Q):\n\t\tkey.vlan = get_unaligned_be16(tag + 2) & VLAN_VID_MASK;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tentry = rhashtable_lookup_fast(&ppe->l2_flows, &key, mtk_flow_l2_ht_params);\n\tif (!entry)\n\t\tgoto out;\n\n\tmtk_foe_entry_commit_subflow(ppe, entry, hash);\n\nout:\n\tspin_unlock_bh(&ppe_lock);\n}\n\nint mtk_foe_entry_idle_time(struct mtk_ppe *ppe, struct mtk_flow_entry *entry)\n{\n\tmtk_flow_entry_update(ppe, entry);\n\n\treturn __mtk_foe_entry_idle_time(ppe, entry->data.ib1);\n}\n\nint mtk_ppe_prepare_reset(struct mtk_ppe *ppe)\n{\n\tif (!ppe)\n\t\treturn -EINVAL;\n\n\t \n\tppe_clear(ppe, MTK_PPE_TB_CFG, MTK_PPE_TB_CFG_KEEPALIVE);\n\tppe_clear(ppe, MTK_PPE_BIND_LMT1, MTK_PPE_NTU_KEEPALIVE);\n\tppe_w32(ppe, MTK_PPE_KEEPALIVE, 0);\n\tusleep_range(10000, 11000);\n\n\t \n\tppe_set(ppe, MTK_PPE_BIND_LMT1, MTK_PPE_NTU_KEEPALIVE);\n\tppe_w32(ppe, MTK_PPE_KEEPALIVE, 0xffffffff);\n\n\t \n\tppe_set(ppe, MTK_PPE_TB_CFG, MTK_PPE_TB_TICK_SEL);\n\tppe_set(ppe, MTK_PPE_TB_CFG, MTK_PPE_TB_CFG_KEEPALIVE);\n\tusleep_range(10000, 11000);\n\n\t \n\tppe_clear(ppe, MTK_PPE_TB_CFG, MTK_PPE_TB_CFG_SCAN_MODE);\n\tusleep_range(10000, 11000);\n\n\treturn mtk_ppe_wait_busy(ppe);\n}\n\nstruct mtk_foe_accounting *mtk_foe_entry_get_mib(struct mtk_ppe *ppe, u32 index,\n\t\t\t\t\t\t struct mtk_foe_accounting *diff)\n{\n\tstruct mtk_foe_accounting *acct;\n\tint size = sizeof(struct mtk_foe_accounting);\n\tu64 bytes, packets;\n\n\tif (!ppe->accounting)\n\t\treturn NULL;\n\n\tif (mtk_mib_entry_read(ppe, index, &bytes, &packets))\n\t\treturn NULL;\n\n\tacct = ppe->acct_table + index * size;\n\n\tacct->bytes += bytes;\n\tacct->packets += packets;\n\n\tif (diff) {\n\t\tdiff->bytes = bytes;\n\t\tdiff->packets = packets;\n\t}\n\n\treturn acct;\n}\n\nstruct mtk_ppe *mtk_ppe_init(struct mtk_eth *eth, void __iomem *base, int index)\n{\n\tbool accounting = eth->soc->has_accounting;\n\tconst struct mtk_soc_data *soc = eth->soc;\n\tstruct mtk_foe_accounting *acct;\n\tstruct device *dev = eth->dev;\n\tstruct mtk_mib_entry *mib;\n\tstruct mtk_ppe *ppe;\n\tu32 foe_flow_size;\n\tvoid *foe;\n\n\tppe = devm_kzalloc(dev, sizeof(*ppe), GFP_KERNEL);\n\tif (!ppe)\n\t\treturn NULL;\n\n\trhashtable_init(&ppe->l2_flows, &mtk_flow_l2_ht_params);\n\n\t \n\tppe->base = base;\n\tppe->eth = eth;\n\tppe->dev = dev;\n\tppe->version = eth->soc->offload_version;\n\tppe->accounting = accounting;\n\n\tfoe = dmam_alloc_coherent(ppe->dev,\n\t\t\t\t  MTK_PPE_ENTRIES * soc->foe_entry_size,\n\t\t\t\t  &ppe->foe_phys, GFP_KERNEL);\n\tif (!foe)\n\t\tgoto err_free_l2_flows;\n\n\tppe->foe_table = foe;\n\n\tfoe_flow_size = (MTK_PPE_ENTRIES / soc->hash_offset) *\n\t\t\tsizeof(*ppe->foe_flow);\n\tppe->foe_flow = devm_kzalloc(dev, foe_flow_size, GFP_KERNEL);\n\tif (!ppe->foe_flow)\n\t\tgoto err_free_l2_flows;\n\n\tif (accounting) {\n\t\tmib = dmam_alloc_coherent(ppe->dev, MTK_PPE_ENTRIES * sizeof(*mib),\n\t\t\t\t\t  &ppe->mib_phys, GFP_KERNEL);\n\t\tif (!mib)\n\t\t\treturn NULL;\n\n\t\tppe->mib_table = mib;\n\n\t\tacct = devm_kzalloc(dev, MTK_PPE_ENTRIES * sizeof(*acct),\n\t\t\t\t    GFP_KERNEL);\n\n\t\tif (!acct)\n\t\t\treturn NULL;\n\n\t\tppe->acct_table = acct;\n\t}\n\n\tmtk_ppe_debugfs_init(ppe, index);\n\n\treturn ppe;\n\nerr_free_l2_flows:\n\trhashtable_destroy(&ppe->l2_flows);\n\treturn NULL;\n}\n\nvoid mtk_ppe_deinit(struct mtk_eth *eth)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(eth->ppe); i++) {\n\t\tif (!eth->ppe[i])\n\t\t\treturn;\n\t\trhashtable_destroy(&eth->ppe[i]->l2_flows);\n\t}\n}\n\nstatic void mtk_ppe_init_foe_table(struct mtk_ppe *ppe)\n{\n\tstatic const u8 skip[] = { 12, 25, 38, 51, 76, 89, 102 };\n\tint i, k;\n\n\tmemset(ppe->foe_table, 0,\n\t       MTK_PPE_ENTRIES * ppe->eth->soc->foe_entry_size);\n\n\tif (!IS_ENABLED(CONFIG_SOC_MT7621))\n\t\treturn;\n\n\t \n\tfor (i = 0; i < MTK_PPE_ENTRIES; i += 128) {\n\t\tfor (k = 0; k < ARRAY_SIZE(skip); k++) {\n\t\t\tstruct mtk_foe_entry *hwe;\n\n\t\t\thwe = mtk_foe_get_entry(ppe, i + skip[k]);\n\t\t\thwe->ib1 |= MTK_FOE_IB1_STATIC;\n\t\t}\n\t}\n}\n\nvoid mtk_ppe_start(struct mtk_ppe *ppe)\n{\n\tu32 val;\n\n\tif (!ppe)\n\t\treturn;\n\n\tmtk_ppe_init_foe_table(ppe);\n\tppe_w32(ppe, MTK_PPE_TB_BASE, ppe->foe_phys);\n\n\tval = MTK_PPE_TB_CFG_AGE_NON_L4 |\n\t      MTK_PPE_TB_CFG_AGE_UNBIND |\n\t      MTK_PPE_TB_CFG_AGE_TCP |\n\t      MTK_PPE_TB_CFG_AGE_UDP |\n\t      MTK_PPE_TB_CFG_AGE_TCP_FIN |\n\t      FIELD_PREP(MTK_PPE_TB_CFG_SEARCH_MISS,\n\t\t\t MTK_PPE_SEARCH_MISS_ACTION_FORWARD_BUILD) |\n\t      FIELD_PREP(MTK_PPE_TB_CFG_KEEPALIVE,\n\t\t\t MTK_PPE_KEEPALIVE_DISABLE) |\n\t      FIELD_PREP(MTK_PPE_TB_CFG_HASH_MODE, 1) |\n\t      FIELD_PREP(MTK_PPE_TB_CFG_SCAN_MODE,\n\t\t\t MTK_PPE_SCAN_MODE_KEEPALIVE_AGE) |\n\t      FIELD_PREP(MTK_PPE_TB_CFG_ENTRY_NUM,\n\t\t\t MTK_PPE_ENTRIES_SHIFT);\n\tif (mtk_is_netsys_v2_or_greater(ppe->eth))\n\t\tval |= MTK_PPE_TB_CFG_INFO_SEL;\n\tif (!mtk_is_netsys_v3_or_greater(ppe->eth))\n\t\tval |= MTK_PPE_TB_CFG_ENTRY_80B;\n\tppe_w32(ppe, MTK_PPE_TB_CFG, val);\n\n\tppe_w32(ppe, MTK_PPE_IP_PROTO_CHK,\n\t\tMTK_PPE_IP_PROTO_CHK_IPV4 | MTK_PPE_IP_PROTO_CHK_IPV6);\n\n\tmtk_ppe_cache_enable(ppe, true);\n\n\tval = MTK_PPE_FLOW_CFG_IP6_3T_ROUTE |\n\t      MTK_PPE_FLOW_CFG_IP6_5T_ROUTE |\n\t      MTK_PPE_FLOW_CFG_IP6_6RD |\n\t      MTK_PPE_FLOW_CFG_IP4_NAT |\n\t      MTK_PPE_FLOW_CFG_IP4_NAPT |\n\t      MTK_PPE_FLOW_CFG_IP4_DSLITE |\n\t      MTK_PPE_FLOW_CFG_IP4_NAT_FRAG;\n\tif (mtk_is_netsys_v2_or_greater(ppe->eth))\n\t\tval |= MTK_PPE_MD_TOAP_BYP_CRSN0 |\n\t\t       MTK_PPE_MD_TOAP_BYP_CRSN1 |\n\t\t       MTK_PPE_MD_TOAP_BYP_CRSN2 |\n\t\t       MTK_PPE_FLOW_CFG_IP4_HASH_GRE_KEY;\n\telse\n\t\tval |= MTK_PPE_FLOW_CFG_IP4_TCP_FRAG |\n\t\t       MTK_PPE_FLOW_CFG_IP4_UDP_FRAG;\n\tppe_w32(ppe, MTK_PPE_FLOW_CFG, val);\n\n\tval = FIELD_PREP(MTK_PPE_UNBIND_AGE_MIN_PACKETS, 1000) |\n\t      FIELD_PREP(MTK_PPE_UNBIND_AGE_DELTA, 3);\n\tppe_w32(ppe, MTK_PPE_UNBIND_AGE, val);\n\n\tval = FIELD_PREP(MTK_PPE_BIND_AGE0_DELTA_UDP, 12) |\n\t      FIELD_PREP(MTK_PPE_BIND_AGE0_DELTA_NON_L4, 1);\n\tppe_w32(ppe, MTK_PPE_BIND_AGE0, val);\n\n\tval = FIELD_PREP(MTK_PPE_BIND_AGE1_DELTA_TCP_FIN, 1) |\n\t      FIELD_PREP(MTK_PPE_BIND_AGE1_DELTA_TCP, 7);\n\tppe_w32(ppe, MTK_PPE_BIND_AGE1, val);\n\n\tval = MTK_PPE_BIND_LIMIT0_QUARTER | MTK_PPE_BIND_LIMIT0_HALF;\n\tppe_w32(ppe, MTK_PPE_BIND_LIMIT0, val);\n\n\tval = MTK_PPE_BIND_LIMIT1_FULL |\n\t      FIELD_PREP(MTK_PPE_BIND_LIMIT1_NON_L4, 1);\n\tppe_w32(ppe, MTK_PPE_BIND_LIMIT1, val);\n\n\tval = FIELD_PREP(MTK_PPE_BIND_RATE_BIND, 30) |\n\t      FIELD_PREP(MTK_PPE_BIND_RATE_PREBIND, 1);\n\tppe_w32(ppe, MTK_PPE_BIND_RATE, val);\n\n\t \n\tval = MTK_PPE_GLO_CFG_EN |\n\t      MTK_PPE_GLO_CFG_IP4_L4_CS_DROP |\n\t      MTK_PPE_GLO_CFG_IP4_CS_DROP |\n\t      MTK_PPE_GLO_CFG_FLOW_DROP_UPDATE;\n\tppe_w32(ppe, MTK_PPE_GLO_CFG, val);\n\n\tppe_w32(ppe, MTK_PPE_DEFAULT_CPU_PORT, 0);\n\n\tif (mtk_is_netsys_v2_or_greater(ppe->eth)) {\n\t\tppe_w32(ppe, MTK_PPE_DEFAULT_CPU_PORT1, 0xcb777);\n\t\tppe_w32(ppe, MTK_PPE_SBW_CTRL, 0x7f);\n\t}\n\n\tif (ppe->accounting && ppe->mib_phys) {\n\t\tppe_w32(ppe, MTK_PPE_MIB_TB_BASE, ppe->mib_phys);\n\t\tppe_m32(ppe, MTK_PPE_MIB_CFG, MTK_PPE_MIB_CFG_EN,\n\t\t\tMTK_PPE_MIB_CFG_EN);\n\t\tppe_m32(ppe, MTK_PPE_MIB_CFG, MTK_PPE_MIB_CFG_RD_CLR,\n\t\t\tMTK_PPE_MIB_CFG_RD_CLR);\n\t\tppe_m32(ppe, MTK_PPE_MIB_CACHE_CTL, MTK_PPE_MIB_CACHE_CTL_EN,\n\t\t\tMTK_PPE_MIB_CFG_RD_CLR);\n\t}\n}\n\nint mtk_ppe_stop(struct mtk_ppe *ppe)\n{\n\tu32 val;\n\tint i;\n\n\tif (!ppe)\n\t\treturn 0;\n\n\tfor (i = 0; i < MTK_PPE_ENTRIES; i++) {\n\t\tstruct mtk_foe_entry *hwe = mtk_foe_get_entry(ppe, i);\n\n\t\thwe->ib1 = FIELD_PREP(MTK_FOE_IB1_STATE,\n\t\t\t\t      MTK_FOE_STATE_INVALID);\n\t}\n\n\tmtk_ppe_cache_enable(ppe, false);\n\n\t \n\tppe_clear(ppe, MTK_PPE_GLO_CFG, MTK_PPE_GLO_CFG_EN);\n\tppe_w32(ppe, MTK_PPE_FLOW_CFG, 0);\n\n\t \n\tval = MTK_PPE_TB_CFG_AGE_NON_L4 |\n\t      MTK_PPE_TB_CFG_AGE_UNBIND |\n\t      MTK_PPE_TB_CFG_AGE_TCP |\n\t      MTK_PPE_TB_CFG_AGE_UDP |\n\t      MTK_PPE_TB_CFG_AGE_TCP_FIN;\n\tppe_clear(ppe, MTK_PPE_TB_CFG, val);\n\n\treturn mtk_ppe_wait_busy(ppe);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}