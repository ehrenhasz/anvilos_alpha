{
  "module_name": "xen-netfront.c",
  "hash_id": "0e81cd155a2ba9de3999f88882ca197727cb2b879c7699b4bcadd810257dc015",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/xen-netfront.c",
  "human_readable_source": " \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/skbuff.h>\n#include <linux/ethtool.h>\n#include <linux/if_ether.h>\n#include <net/tcp.h>\n#include <linux/udp.h>\n#include <linux/moduleparam.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <net/ip.h>\n#include <linux/bpf.h>\n#include <net/page_pool/types.h>\n#include <linux/bpf_trace.h>\n\n#include <xen/xen.h>\n#include <xen/xenbus.h>\n#include <xen/events.h>\n#include <xen/page.h>\n#include <xen/platform_pci.h>\n#include <xen/grant_table.h>\n\n#include <xen/interface/io/netif.h>\n#include <xen/interface/memory.h>\n#include <xen/interface/grant_table.h>\n\n \n#define MAX_QUEUES_DEFAULT 8\nstatic unsigned int xennet_max_queues;\nmodule_param_named(max_queues, xennet_max_queues, uint, 0644);\nMODULE_PARM_DESC(max_queues,\n\t\t \"Maximum number of queues per virtual interface\");\n\nstatic bool __read_mostly xennet_trusted = true;\nmodule_param_named(trusted, xennet_trusted, bool, 0644);\nMODULE_PARM_DESC(trusted, \"Is the backend trusted\");\n\n#define XENNET_TIMEOUT  (5 * HZ)\n\nstatic const struct ethtool_ops xennet_ethtool_ops;\n\nstruct netfront_cb {\n\tint pull_to;\n};\n\n#define NETFRONT_SKB_CB(skb)\t((struct netfront_cb *)((skb)->cb))\n\n#define RX_COPY_THRESHOLD 256\n\n#define NET_TX_RING_SIZE __CONST_RING_SIZE(xen_netif_tx, XEN_PAGE_SIZE)\n#define NET_RX_RING_SIZE __CONST_RING_SIZE(xen_netif_rx, XEN_PAGE_SIZE)\n\n \n#define NET_RX_SLOTS_MIN (XEN_NETIF_NR_SLOTS_MIN + 1)\n\n \n#define QUEUE_NAME_SIZE (IFNAMSIZ + 6)\n\n \n#define IRQ_NAME_SIZE (QUEUE_NAME_SIZE + 3)\n\nstatic DECLARE_WAIT_QUEUE_HEAD(module_wq);\n\nstruct netfront_stats {\n\tu64\t\t\tpackets;\n\tu64\t\t\tbytes;\n\tstruct u64_stats_sync\tsyncp;\n};\n\nstruct netfront_info;\n\nstruct netfront_queue {\n\tunsigned int id;  \n\tchar name[QUEUE_NAME_SIZE];  \n\tstruct netfront_info *info;\n\n\tstruct bpf_prog __rcu *xdp_prog;\n\n\tstruct napi_struct napi;\n\n\t \n\tunsigned int tx_evtchn, rx_evtchn;\n\tunsigned int tx_irq, rx_irq;\n\t \n\tchar tx_irq_name[IRQ_NAME_SIZE];  \n\tchar rx_irq_name[IRQ_NAME_SIZE];  \n\n\tspinlock_t   tx_lock;\n\tstruct xen_netif_tx_front_ring tx;\n\tint tx_ring_ref;\n\n\t \n\tstruct sk_buff *tx_skbs[NET_TX_RING_SIZE];\n\tunsigned short tx_link[NET_TX_RING_SIZE];\n#define TX_LINK_NONE 0xffff\n#define TX_PENDING   0xfffe\n\tgrant_ref_t gref_tx_head;\n\tgrant_ref_t grant_tx_ref[NET_TX_RING_SIZE];\n\tstruct page *grant_tx_page[NET_TX_RING_SIZE];\n\tunsigned tx_skb_freelist;\n\tunsigned int tx_pend_queue;\n\n\tspinlock_t   rx_lock ____cacheline_aligned_in_smp;\n\tstruct xen_netif_rx_front_ring rx;\n\tint rx_ring_ref;\n\n\tstruct timer_list rx_refill_timer;\n\n\tstruct sk_buff *rx_skbs[NET_RX_RING_SIZE];\n\tgrant_ref_t gref_rx_head;\n\tgrant_ref_t grant_rx_ref[NET_RX_RING_SIZE];\n\n\tunsigned int rx_rsp_unconsumed;\n\tspinlock_t rx_cons_lock;\n\n\tstruct page_pool *page_pool;\n\tstruct xdp_rxq_info xdp_rxq;\n};\n\nstruct netfront_info {\n\tstruct list_head list;\n\tstruct net_device *netdev;\n\n\tstruct xenbus_device *xbdev;\n\n\t \n\tstruct netfront_queue *queues;\n\n\t \n\tstruct netfront_stats __percpu *rx_stats;\n\tstruct netfront_stats __percpu *tx_stats;\n\n\t \n\tbool netback_has_xdp_headroom;\n\tbool netfront_xdp_enabled;\n\n\t \n\tbool broken;\n\n\t \n\tbool bounce;\n\n\tatomic_t rx_gso_checksum_fixup;\n};\n\nstruct netfront_rx_info {\n\tstruct xen_netif_rx_response rx;\n\tstruct xen_netif_extra_info extras[XEN_NETIF_EXTRA_TYPE_MAX - 1];\n};\n\n \n\nstatic void add_id_to_list(unsigned *head, unsigned short *list,\n\t\t\t   unsigned short id)\n{\n\tlist[id] = *head;\n\t*head = id;\n}\n\nstatic unsigned short get_id_from_list(unsigned *head, unsigned short *list)\n{\n\tunsigned int id = *head;\n\n\tif (id != TX_LINK_NONE) {\n\t\t*head = list[id];\n\t\tlist[id] = TX_LINK_NONE;\n\t}\n\treturn id;\n}\n\nstatic int xennet_rxidx(RING_IDX idx)\n{\n\treturn idx & (NET_RX_RING_SIZE - 1);\n}\n\nstatic struct sk_buff *xennet_get_rx_skb(struct netfront_queue *queue,\n\t\t\t\t\t RING_IDX ri)\n{\n\tint i = xennet_rxidx(ri);\n\tstruct sk_buff *skb = queue->rx_skbs[i];\n\tqueue->rx_skbs[i] = NULL;\n\treturn skb;\n}\n\nstatic grant_ref_t xennet_get_rx_ref(struct netfront_queue *queue,\n\t\t\t\t\t    RING_IDX ri)\n{\n\tint i = xennet_rxidx(ri);\n\tgrant_ref_t ref = queue->grant_rx_ref[i];\n\tqueue->grant_rx_ref[i] = INVALID_GRANT_REF;\n\treturn ref;\n}\n\n#ifdef CONFIG_SYSFS\nstatic const struct attribute_group xennet_dev_group;\n#endif\n\nstatic bool xennet_can_sg(struct net_device *dev)\n{\n\treturn dev->features & NETIF_F_SG;\n}\n\n\nstatic void rx_refill_timeout(struct timer_list *t)\n{\n\tstruct netfront_queue *queue = from_timer(queue, t, rx_refill_timer);\n\tnapi_schedule(&queue->napi);\n}\n\nstatic int netfront_tx_slot_available(struct netfront_queue *queue)\n{\n\treturn (queue->tx.req_prod_pvt - queue->tx.rsp_cons) <\n\t\t(NET_TX_RING_SIZE - XEN_NETIF_NR_SLOTS_MIN - 1);\n}\n\nstatic void xennet_maybe_wake_tx(struct netfront_queue *queue)\n{\n\tstruct net_device *dev = queue->info->netdev;\n\tstruct netdev_queue *dev_queue = netdev_get_tx_queue(dev, queue->id);\n\n\tif (unlikely(netif_tx_queue_stopped(dev_queue)) &&\n\t    netfront_tx_slot_available(queue) &&\n\t    likely(netif_running(dev)))\n\t\tnetif_tx_wake_queue(netdev_get_tx_queue(dev, queue->id));\n}\n\n\nstatic struct sk_buff *xennet_alloc_one_rx_buffer(struct netfront_queue *queue)\n{\n\tstruct sk_buff *skb;\n\tstruct page *page;\n\n\tskb = __netdev_alloc_skb(queue->info->netdev,\n\t\t\t\t RX_COPY_THRESHOLD + NET_IP_ALIGN,\n\t\t\t\t GFP_ATOMIC | __GFP_NOWARN);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tpage = page_pool_alloc_pages(queue->page_pool,\n\t\t\t\t     GFP_ATOMIC | __GFP_NOWARN | __GFP_ZERO);\n\tif (unlikely(!page)) {\n\t\tkfree_skb(skb);\n\t\treturn NULL;\n\t}\n\tskb_add_rx_frag(skb, 0, page, 0, 0, PAGE_SIZE);\n\n\t \n\tskb_reserve(skb, NET_IP_ALIGN);\n\tskb->dev = queue->info->netdev;\n\n\treturn skb;\n}\n\n\nstatic void xennet_alloc_rx_buffers(struct netfront_queue *queue)\n{\n\tRING_IDX req_prod = queue->rx.req_prod_pvt;\n\tint notify;\n\tint err = 0;\n\n\tif (unlikely(!netif_carrier_ok(queue->info->netdev)))\n\t\treturn;\n\n\tfor (req_prod = queue->rx.req_prod_pvt;\n\t     req_prod - queue->rx.rsp_cons < NET_RX_RING_SIZE;\n\t     req_prod++) {\n\t\tstruct sk_buff *skb;\n\t\tunsigned short id;\n\t\tgrant_ref_t ref;\n\t\tstruct page *page;\n\t\tstruct xen_netif_rx_request *req;\n\n\t\tskb = xennet_alloc_one_rx_buffer(queue);\n\t\tif (!skb) {\n\t\t\terr = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\n\t\tid = xennet_rxidx(req_prod);\n\n\t\tBUG_ON(queue->rx_skbs[id]);\n\t\tqueue->rx_skbs[id] = skb;\n\n\t\tref = gnttab_claim_grant_reference(&queue->gref_rx_head);\n\t\tWARN_ON_ONCE(IS_ERR_VALUE((unsigned long)(int)ref));\n\t\tqueue->grant_rx_ref[id] = ref;\n\n\t\tpage = skb_frag_page(&skb_shinfo(skb)->frags[0]);\n\n\t\treq = RING_GET_REQUEST(&queue->rx, req_prod);\n\t\tgnttab_page_grant_foreign_access_ref_one(ref,\n\t\t\t\t\t\t\t queue->info->xbdev->otherend_id,\n\t\t\t\t\t\t\t page,\n\t\t\t\t\t\t\t 0);\n\t\treq->id = id;\n\t\treq->gref = ref;\n\t}\n\n\tqueue->rx.req_prod_pvt = req_prod;\n\n\t \n\tif (req_prod - queue->rx.rsp_cons < NET_RX_SLOTS_MIN ||\n\t    unlikely(err)) {\n\t\tmod_timer(&queue->rx_refill_timer, jiffies + (HZ/10));\n\t\treturn;\n\t}\n\n\tRING_PUSH_REQUESTS_AND_CHECK_NOTIFY(&queue->rx, notify);\n\tif (notify)\n\t\tnotify_remote_via_irq(queue->rx_irq);\n}\n\nstatic int xennet_open(struct net_device *dev)\n{\n\tstruct netfront_info *np = netdev_priv(dev);\n\tunsigned int num_queues = dev->real_num_tx_queues;\n\tunsigned int i = 0;\n\tstruct netfront_queue *queue = NULL;\n\n\tif (!np->queues || np->broken)\n\t\treturn -ENODEV;\n\n\tfor (i = 0; i < num_queues; ++i) {\n\t\tqueue = &np->queues[i];\n\t\tnapi_enable(&queue->napi);\n\n\t\tspin_lock_bh(&queue->rx_lock);\n\t\tif (netif_carrier_ok(dev)) {\n\t\t\txennet_alloc_rx_buffers(queue);\n\t\t\tqueue->rx.sring->rsp_event = queue->rx.rsp_cons + 1;\n\t\t\tif (RING_HAS_UNCONSUMED_RESPONSES(&queue->rx))\n\t\t\t\tnapi_schedule(&queue->napi);\n\t\t}\n\t\tspin_unlock_bh(&queue->rx_lock);\n\t}\n\n\tnetif_tx_start_all_queues(dev);\n\n\treturn 0;\n}\n\nstatic bool xennet_tx_buf_gc(struct netfront_queue *queue)\n{\n\tRING_IDX cons, prod;\n\tunsigned short id;\n\tstruct sk_buff *skb;\n\tbool more_to_do;\n\tbool work_done = false;\n\tconst struct device *dev = &queue->info->netdev->dev;\n\n\tBUG_ON(!netif_carrier_ok(queue->info->netdev));\n\n\tdo {\n\t\tprod = queue->tx.sring->rsp_prod;\n\t\tif (RING_RESPONSE_PROD_OVERFLOW(&queue->tx, prod)) {\n\t\t\tdev_alert(dev, \"Illegal number of responses %u\\n\",\n\t\t\t\t  prod - queue->tx.rsp_cons);\n\t\t\tgoto err;\n\t\t}\n\t\trmb();  \n\n\t\tfor (cons = queue->tx.rsp_cons; cons != prod; cons++) {\n\t\t\tstruct xen_netif_tx_response txrsp;\n\n\t\t\twork_done = true;\n\n\t\t\tRING_COPY_RESPONSE(&queue->tx, cons, &txrsp);\n\t\t\tif (txrsp.status == XEN_NETIF_RSP_NULL)\n\t\t\t\tcontinue;\n\n\t\t\tid = txrsp.id;\n\t\t\tif (id >= RING_SIZE(&queue->tx)) {\n\t\t\t\tdev_alert(dev,\n\t\t\t\t\t  \"Response has incorrect id (%u)\\n\",\n\t\t\t\t\t  id);\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tif (queue->tx_link[id] != TX_PENDING) {\n\t\t\t\tdev_alert(dev,\n\t\t\t\t\t  \"Response for inactive request\\n\");\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tqueue->tx_link[id] = TX_LINK_NONE;\n\t\t\tskb = queue->tx_skbs[id];\n\t\t\tqueue->tx_skbs[id] = NULL;\n\t\t\tif (unlikely(!gnttab_end_foreign_access_ref(\n\t\t\t\tqueue->grant_tx_ref[id]))) {\n\t\t\t\tdev_alert(dev,\n\t\t\t\t\t  \"Grant still in use by backend domain\\n\");\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tgnttab_release_grant_reference(\n\t\t\t\t&queue->gref_tx_head, queue->grant_tx_ref[id]);\n\t\t\tqueue->grant_tx_ref[id] = INVALID_GRANT_REF;\n\t\t\tqueue->grant_tx_page[id] = NULL;\n\t\t\tadd_id_to_list(&queue->tx_skb_freelist, queue->tx_link, id);\n\t\t\tdev_kfree_skb_irq(skb);\n\t\t}\n\n\t\tqueue->tx.rsp_cons = prod;\n\n\t\tRING_FINAL_CHECK_FOR_RESPONSES(&queue->tx, more_to_do);\n\t} while (more_to_do);\n\n\txennet_maybe_wake_tx(queue);\n\n\treturn work_done;\n\n err:\n\tqueue->info->broken = true;\n\tdev_alert(dev, \"Disabled for further use\\n\");\n\n\treturn work_done;\n}\n\nstruct xennet_gnttab_make_txreq {\n\tstruct netfront_queue *queue;\n\tstruct sk_buff *skb;\n\tstruct page *page;\n\tstruct xen_netif_tx_request *tx;       \n\tstruct xen_netif_tx_request tx_local;  \n\tunsigned int size;\n};\n\nstatic void xennet_tx_setup_grant(unsigned long gfn, unsigned int offset,\n\t\t\t\t  unsigned int len, void *data)\n{\n\tstruct xennet_gnttab_make_txreq *info = data;\n\tunsigned int id;\n\tstruct xen_netif_tx_request *tx;\n\tgrant_ref_t ref;\n\t \n\tstruct page *page = info->page;\n\tstruct netfront_queue *queue = info->queue;\n\tstruct sk_buff *skb = info->skb;\n\n\tid = get_id_from_list(&queue->tx_skb_freelist, queue->tx_link);\n\ttx = RING_GET_REQUEST(&queue->tx, queue->tx.req_prod_pvt++);\n\tref = gnttab_claim_grant_reference(&queue->gref_tx_head);\n\tWARN_ON_ONCE(IS_ERR_VALUE((unsigned long)(int)ref));\n\n\tgnttab_grant_foreign_access_ref(ref, queue->info->xbdev->otherend_id,\n\t\t\t\t\tgfn, GNTMAP_readonly);\n\n\tqueue->tx_skbs[id] = skb;\n\tqueue->grant_tx_page[id] = page;\n\tqueue->grant_tx_ref[id] = ref;\n\n\tinfo->tx_local.id = id;\n\tinfo->tx_local.gref = ref;\n\tinfo->tx_local.offset = offset;\n\tinfo->tx_local.size = len;\n\tinfo->tx_local.flags = 0;\n\n\t*tx = info->tx_local;\n\n\t \n\tadd_id_to_list(&queue->tx_pend_queue, queue->tx_link, id);\n\n\tinfo->tx = tx;\n\tinfo->size += info->tx_local.size;\n}\n\nstatic struct xen_netif_tx_request *xennet_make_first_txreq(\n\tstruct xennet_gnttab_make_txreq *info,\n\tunsigned int offset, unsigned int len)\n{\n\tinfo->size = 0;\n\n\tgnttab_for_one_grant(info->page, offset, len, xennet_tx_setup_grant, info);\n\n\treturn info->tx;\n}\n\nstatic void xennet_make_one_txreq(unsigned long gfn, unsigned int offset,\n\t\t\t\t  unsigned int len, void *data)\n{\n\tstruct xennet_gnttab_make_txreq *info = data;\n\n\tinfo->tx->flags |= XEN_NETTXF_more_data;\n\tskb_get(info->skb);\n\txennet_tx_setup_grant(gfn, offset, len, data);\n}\n\nstatic void xennet_make_txreqs(\n\tstruct xennet_gnttab_make_txreq *info,\n\tstruct page *page,\n\tunsigned int offset, unsigned int len)\n{\n\t \n\tpage += offset >> PAGE_SHIFT;\n\toffset &= ~PAGE_MASK;\n\n\twhile (len) {\n\t\tinfo->page = page;\n\t\tinfo->size = 0;\n\n\t\tgnttab_foreach_grant_in_range(page, offset, len,\n\t\t\t\t\t      xennet_make_one_txreq,\n\t\t\t\t\t      info);\n\n\t\tpage++;\n\t\toffset = 0;\n\t\tlen -= info->size;\n\t}\n}\n\n \nstatic int xennet_count_skb_slots(struct sk_buff *skb)\n{\n\tint i, frags = skb_shinfo(skb)->nr_frags;\n\tint slots;\n\n\tslots = gnttab_count_grant(offset_in_page(skb->data),\n\t\t\t\t   skb_headlen(skb));\n\n\tfor (i = 0; i < frags; i++) {\n\t\tskb_frag_t *frag = skb_shinfo(skb)->frags + i;\n\t\tunsigned long size = skb_frag_size(frag);\n\t\tunsigned long offset = skb_frag_off(frag);\n\n\t\t \n\t\toffset &= ~PAGE_MASK;\n\n\t\tslots += gnttab_count_grant(offset, size);\n\t}\n\n\treturn slots;\n}\n\nstatic u16 xennet_select_queue(struct net_device *dev, struct sk_buff *skb,\n\t\t\t       struct net_device *sb_dev)\n{\n\tunsigned int num_queues = dev->real_num_tx_queues;\n\tu32 hash;\n\tu16 queue_idx;\n\n\t \n\tif (num_queues == 1) {\n\t\tqueue_idx = 0;\n\t} else {\n\t\thash = skb_get_hash(skb);\n\t\tqueue_idx = hash % num_queues;\n\t}\n\n\treturn queue_idx;\n}\n\nstatic void xennet_mark_tx_pending(struct netfront_queue *queue)\n{\n\tunsigned int i;\n\n\twhile ((i = get_id_from_list(&queue->tx_pend_queue, queue->tx_link)) !=\n\t       TX_LINK_NONE)\n\t\tqueue->tx_link[i] = TX_PENDING;\n}\n\nstatic int xennet_xdp_xmit_one(struct net_device *dev,\n\t\t\t       struct netfront_queue *queue,\n\t\t\t       struct xdp_frame *xdpf)\n{\n\tstruct netfront_info *np = netdev_priv(dev);\n\tstruct netfront_stats *tx_stats = this_cpu_ptr(np->tx_stats);\n\tstruct xennet_gnttab_make_txreq info = {\n\t\t.queue = queue,\n\t\t.skb = NULL,\n\t\t.page = virt_to_page(xdpf->data),\n\t};\n\tint notify;\n\n\txennet_make_first_txreq(&info,\n\t\t\t\toffset_in_page(xdpf->data),\n\t\t\t\txdpf->len);\n\n\txennet_mark_tx_pending(queue);\n\n\tRING_PUSH_REQUESTS_AND_CHECK_NOTIFY(&queue->tx, notify);\n\tif (notify)\n\t\tnotify_remote_via_irq(queue->tx_irq);\n\n\tu64_stats_update_begin(&tx_stats->syncp);\n\ttx_stats->bytes += xdpf->len;\n\ttx_stats->packets++;\n\tu64_stats_update_end(&tx_stats->syncp);\n\n\txennet_tx_buf_gc(queue);\n\n\treturn 0;\n}\n\nstatic int xennet_xdp_xmit(struct net_device *dev, int n,\n\t\t\t   struct xdp_frame **frames, u32 flags)\n{\n\tunsigned int num_queues = dev->real_num_tx_queues;\n\tstruct netfront_info *np = netdev_priv(dev);\n\tstruct netfront_queue *queue = NULL;\n\tunsigned long irq_flags;\n\tint nxmit = 0;\n\tint i;\n\n\tif (unlikely(np->broken))\n\t\treturn -ENODEV;\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK))\n\t\treturn -EINVAL;\n\n\tqueue = &np->queues[smp_processor_id() % num_queues];\n\n\tspin_lock_irqsave(&queue->tx_lock, irq_flags);\n\tfor (i = 0; i < n; i++) {\n\t\tstruct xdp_frame *xdpf = frames[i];\n\n\t\tif (!xdpf)\n\t\t\tcontinue;\n\t\tif (xennet_xdp_xmit_one(dev, queue, xdpf))\n\t\t\tbreak;\n\t\tnxmit++;\n\t}\n\tspin_unlock_irqrestore(&queue->tx_lock, irq_flags);\n\n\treturn nxmit;\n}\n\nstatic struct sk_buff *bounce_skb(const struct sk_buff *skb)\n{\n\tunsigned int headerlen = skb_headroom(skb);\n\t \n\tunsigned int size = ALIGN(skb_end_offset(skb) + skb->data_len,\n\t\t\t\t  XEN_PAGE_SIZE);\n\tstruct sk_buff *n = alloc_skb(size, GFP_ATOMIC | __GFP_ZERO);\n\n\tif (!n)\n\t\treturn NULL;\n\n\tif (!IS_ALIGNED((uintptr_t)n->head, XEN_PAGE_SIZE)) {\n\t\tWARN_ONCE(1, \"misaligned skb allocated\\n\");\n\t\tkfree_skb(n);\n\t\treturn NULL;\n\t}\n\n\t \n\tskb_reserve(n, headerlen);\n\t \n\tskb_put(n, skb->len);\n\n\tBUG_ON(skb_copy_bits(skb, -headerlen, n->head, headerlen + skb->len));\n\n\tskb_copy_header(n, skb);\n\treturn n;\n}\n\n#define MAX_XEN_SKB_FRAGS (65536 / XEN_PAGE_SIZE + 1)\n\nstatic netdev_tx_t xennet_start_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct netfront_info *np = netdev_priv(dev);\n\tstruct netfront_stats *tx_stats = this_cpu_ptr(np->tx_stats);\n\tstruct xen_netif_tx_request *first_tx;\n\tunsigned int i;\n\tint notify;\n\tint slots;\n\tstruct page *page;\n\tunsigned int offset;\n\tunsigned int len;\n\tunsigned long flags;\n\tstruct netfront_queue *queue = NULL;\n\tstruct xennet_gnttab_make_txreq info = { };\n\tunsigned int num_queues = dev->real_num_tx_queues;\n\tu16 queue_index;\n\tstruct sk_buff *nskb;\n\n\t \n\tif (num_queues < 1)\n\t\tgoto drop;\n\tif (unlikely(np->broken))\n\t\tgoto drop;\n\t \n\tqueue_index = skb_get_queue_mapping(skb);\n\tqueue = &np->queues[queue_index];\n\n\t \n\tif (unlikely(skb->len > XEN_NETIF_MAX_TX_SIZE)) {\n\t\tnet_alert_ratelimited(\n\t\t\t\"xennet: skb->len = %u, too big for wire format\\n\",\n\t\t\tskb->len);\n\t\tgoto drop;\n\t}\n\n\tslots = xennet_count_skb_slots(skb);\n\tif (unlikely(slots > MAX_XEN_SKB_FRAGS + 1)) {\n\t\tnet_dbg_ratelimited(\"xennet: skb rides the rocket: %d slots, %d bytes\\n\",\n\t\t\t\t    slots, skb->len);\n\t\tif (skb_linearize(skb))\n\t\t\tgoto drop;\n\t}\n\n\tpage = virt_to_page(skb->data);\n\toffset = offset_in_page(skb->data);\n\n\t \n\tif (np->bounce || unlikely(PAGE_SIZE - offset < ETH_HLEN)) {\n\t\tnskb = bounce_skb(skb);\n\t\tif (!nskb)\n\t\t\tgoto drop;\n\t\tdev_consume_skb_any(skb);\n\t\tskb = nskb;\n\t\tpage = virt_to_page(skb->data);\n\t\toffset = offset_in_page(skb->data);\n\t}\n\n\tlen = skb_headlen(skb);\n\n\tspin_lock_irqsave(&queue->tx_lock, flags);\n\n\tif (unlikely(!netif_carrier_ok(dev) ||\n\t\t     (slots > 1 && !xennet_can_sg(dev)) ||\n\t\t     netif_needs_gso(skb, netif_skb_features(skb)))) {\n\t\tspin_unlock_irqrestore(&queue->tx_lock, flags);\n\t\tgoto drop;\n\t}\n\n\t \n\tinfo.queue = queue;\n\tinfo.skb = skb;\n\tinfo.page = page;\n\tfirst_tx = xennet_make_first_txreq(&info, offset, len);\n\toffset += info.tx_local.size;\n\tif (offset == PAGE_SIZE) {\n\t\tpage++;\n\t\toffset = 0;\n\t}\n\tlen -= info.tx_local.size;\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t \n\t\tfirst_tx->flags |= XEN_NETTXF_csum_blank |\n\t\t\t\t   XEN_NETTXF_data_validated;\n\telse if (skb->ip_summed == CHECKSUM_UNNECESSARY)\n\t\t \n\t\tfirst_tx->flags |= XEN_NETTXF_data_validated;\n\n\t \n\tif (skb_shinfo(skb)->gso_size) {\n\t\tstruct xen_netif_extra_info *gso;\n\n\t\tgso = (struct xen_netif_extra_info *)\n\t\t\tRING_GET_REQUEST(&queue->tx, queue->tx.req_prod_pvt++);\n\n\t\tfirst_tx->flags |= XEN_NETTXF_extra_info;\n\n\t\tgso->u.gso.size = skb_shinfo(skb)->gso_size;\n\t\tgso->u.gso.type = (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV6) ?\n\t\t\tXEN_NETIF_GSO_TYPE_TCPV6 :\n\t\t\tXEN_NETIF_GSO_TYPE_TCPV4;\n\t\tgso->u.gso.pad = 0;\n\t\tgso->u.gso.features = 0;\n\n\t\tgso->type = XEN_NETIF_EXTRA_TYPE_GSO;\n\t\tgso->flags = 0;\n\t}\n\n\t \n\txennet_make_txreqs(&info, page, offset, len);\n\n\t \n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tskb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\txennet_make_txreqs(&info, skb_frag_page(frag),\n\t\t\t\t\tskb_frag_off(frag),\n\t\t\t\t\tskb_frag_size(frag));\n\t}\n\n\t \n\tfirst_tx->size = skb->len;\n\n\t \n\tskb_tx_timestamp(skb);\n\n\txennet_mark_tx_pending(queue);\n\n\tRING_PUSH_REQUESTS_AND_CHECK_NOTIFY(&queue->tx, notify);\n\tif (notify)\n\t\tnotify_remote_via_irq(queue->tx_irq);\n\n\tu64_stats_update_begin(&tx_stats->syncp);\n\ttx_stats->bytes += skb->len;\n\ttx_stats->packets++;\n\tu64_stats_update_end(&tx_stats->syncp);\n\n\t \n\txennet_tx_buf_gc(queue);\n\n\tif (!netfront_tx_slot_available(queue))\n\t\tnetif_tx_stop_queue(netdev_get_tx_queue(dev, queue->id));\n\n\tspin_unlock_irqrestore(&queue->tx_lock, flags);\n\n\treturn NETDEV_TX_OK;\n\n drop:\n\tdev->stats.tx_dropped++;\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\nstatic int xennet_close(struct net_device *dev)\n{\n\tstruct netfront_info *np = netdev_priv(dev);\n\tunsigned int num_queues = dev->real_num_tx_queues;\n\tunsigned int i;\n\tstruct netfront_queue *queue;\n\tnetif_tx_stop_all_queues(np->netdev);\n\tfor (i = 0; i < num_queues; ++i) {\n\t\tqueue = &np->queues[i];\n\t\tnapi_disable(&queue->napi);\n\t}\n\treturn 0;\n}\n\nstatic void xennet_destroy_queues(struct netfront_info *info)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < info->netdev->real_num_tx_queues; i++) {\n\t\tstruct netfront_queue *queue = &info->queues[i];\n\n\t\tif (netif_running(info->netdev))\n\t\t\tnapi_disable(&queue->napi);\n\t\tnetif_napi_del(&queue->napi);\n\t}\n\n\tkfree(info->queues);\n\tinfo->queues = NULL;\n}\n\nstatic void xennet_uninit(struct net_device *dev)\n{\n\tstruct netfront_info *np = netdev_priv(dev);\n\txennet_destroy_queues(np);\n}\n\nstatic void xennet_set_rx_rsp_cons(struct netfront_queue *queue, RING_IDX val)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&queue->rx_cons_lock, flags);\n\tqueue->rx.rsp_cons = val;\n\tqueue->rx_rsp_unconsumed = XEN_RING_NR_UNCONSUMED_RESPONSES(&queue->rx);\n\tspin_unlock_irqrestore(&queue->rx_cons_lock, flags);\n}\n\nstatic void xennet_move_rx_slot(struct netfront_queue *queue, struct sk_buff *skb,\n\t\t\t\tgrant_ref_t ref)\n{\n\tint new = xennet_rxidx(queue->rx.req_prod_pvt);\n\n\tBUG_ON(queue->rx_skbs[new]);\n\tqueue->rx_skbs[new] = skb;\n\tqueue->grant_rx_ref[new] = ref;\n\tRING_GET_REQUEST(&queue->rx, queue->rx.req_prod_pvt)->id = new;\n\tRING_GET_REQUEST(&queue->rx, queue->rx.req_prod_pvt)->gref = ref;\n\tqueue->rx.req_prod_pvt++;\n}\n\nstatic int xennet_get_extras(struct netfront_queue *queue,\n\t\t\t     struct xen_netif_extra_info *extras,\n\t\t\t     RING_IDX rp)\n\n{\n\tstruct xen_netif_extra_info extra;\n\tstruct device *dev = &queue->info->netdev->dev;\n\tRING_IDX cons = queue->rx.rsp_cons;\n\tint err = 0;\n\n\tdo {\n\t\tstruct sk_buff *skb;\n\t\tgrant_ref_t ref;\n\n\t\tif (unlikely(cons + 1 == rp)) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"Missing extra info\\n\");\n\t\t\terr = -EBADR;\n\t\t\tbreak;\n\t\t}\n\n\t\tRING_COPY_RESPONSE(&queue->rx, ++cons, &extra);\n\n\t\tif (unlikely(!extra.type ||\n\t\t\t     extra.type >= XEN_NETIF_EXTRA_TYPE_MAX)) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"Invalid extra type: %d\\n\",\n\t\t\t\t\t extra.type);\n\t\t\terr = -EINVAL;\n\t\t} else {\n\t\t\textras[extra.type - 1] = extra;\n\t\t}\n\n\t\tskb = xennet_get_rx_skb(queue, cons);\n\t\tref = xennet_get_rx_ref(queue, cons);\n\t\txennet_move_rx_slot(queue, skb, ref);\n\t} while (extra.flags & XEN_NETIF_EXTRA_FLAG_MORE);\n\n\txennet_set_rx_rsp_cons(queue, cons);\n\treturn err;\n}\n\nstatic u32 xennet_run_xdp(struct netfront_queue *queue, struct page *pdata,\n\t\t   struct xen_netif_rx_response *rx, struct bpf_prog *prog,\n\t\t   struct xdp_buff *xdp, bool *need_xdp_flush)\n{\n\tstruct xdp_frame *xdpf;\n\tu32 len = rx->status;\n\tu32 act;\n\tint err;\n\n\txdp_init_buff(xdp, XEN_PAGE_SIZE - XDP_PACKET_HEADROOM,\n\t\t      &queue->xdp_rxq);\n\txdp_prepare_buff(xdp, page_address(pdata), XDP_PACKET_HEADROOM,\n\t\t\t len, false);\n\n\tact = bpf_prog_run_xdp(prog, xdp);\n\tswitch (act) {\n\tcase XDP_TX:\n\t\tget_page(pdata);\n\t\txdpf = xdp_convert_buff_to_frame(xdp);\n\t\terr = xennet_xdp_xmit(queue->info->netdev, 1, &xdpf, 0);\n\t\tif (unlikely(!err))\n\t\t\txdp_return_frame_rx_napi(xdpf);\n\t\telse if (unlikely(err < 0))\n\t\t\ttrace_xdp_exception(queue->info->netdev, prog, act);\n\t\tbreak;\n\tcase XDP_REDIRECT:\n\t\tget_page(pdata);\n\t\terr = xdp_do_redirect(queue->info->netdev, xdp, prog);\n\t\t*need_xdp_flush = true;\n\t\tif (unlikely(err))\n\t\t\ttrace_xdp_exception(queue->info->netdev, prog, act);\n\t\tbreak;\n\tcase XDP_PASS:\n\tcase XDP_DROP:\n\t\tbreak;\n\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(queue->info->netdev, prog, act);\n\t\tbreak;\n\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(queue->info->netdev, prog, act);\n\t}\n\n\treturn act;\n}\n\nstatic int xennet_get_responses(struct netfront_queue *queue,\n\t\t\t\tstruct netfront_rx_info *rinfo, RING_IDX rp,\n\t\t\t\tstruct sk_buff_head *list,\n\t\t\t\tbool *need_xdp_flush)\n{\n\tstruct xen_netif_rx_response *rx = &rinfo->rx, rx_local;\n\tint max = XEN_NETIF_NR_SLOTS_MIN + (rx->status <= RX_COPY_THRESHOLD);\n\tRING_IDX cons = queue->rx.rsp_cons;\n\tstruct sk_buff *skb = xennet_get_rx_skb(queue, cons);\n\tstruct xen_netif_extra_info *extras = rinfo->extras;\n\tgrant_ref_t ref = xennet_get_rx_ref(queue, cons);\n\tstruct device *dev = &queue->info->netdev->dev;\n\tstruct bpf_prog *xdp_prog;\n\tstruct xdp_buff xdp;\n\tint slots = 1;\n\tint err = 0;\n\tu32 verdict;\n\n\tif (rx->flags & XEN_NETRXF_extra_info) {\n\t\terr = xennet_get_extras(queue, extras, rp);\n\t\tif (!err) {\n\t\t\tif (extras[XEN_NETIF_EXTRA_TYPE_XDP - 1].type) {\n\t\t\t\tstruct xen_netif_extra_info *xdp;\n\n\t\t\t\txdp = &extras[XEN_NETIF_EXTRA_TYPE_XDP - 1];\n\t\t\t\trx->offset = xdp->u.xdp.headroom;\n\t\t\t}\n\t\t}\n\t\tcons = queue->rx.rsp_cons;\n\t}\n\n\tfor (;;) {\n\t\t \n\t\tif (ref == INVALID_GRANT_REF) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"Bad rx response id %d.\\n\",\n\t\t\t\t\t rx->id);\n\t\t\terr = -EINVAL;\n\t\t\tgoto next;\n\t\t}\n\n\t\tif (unlikely(rx->status < 0 ||\n\t\t\t     rx->offset + rx->status > XEN_PAGE_SIZE)) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"rx->offset: %u, size: %d\\n\",\n\t\t\t\t\t rx->offset, rx->status);\n\t\t\txennet_move_rx_slot(queue, skb, ref);\n\t\t\terr = -EINVAL;\n\t\t\tgoto next;\n\t\t}\n\n\t\tif (!gnttab_end_foreign_access_ref(ref)) {\n\t\t\tdev_alert(dev,\n\t\t\t\t  \"Grant still in use by backend domain\\n\");\n\t\t\tqueue->info->broken = true;\n\t\t\tdev_alert(dev, \"Disabled for further use\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tgnttab_release_grant_reference(&queue->gref_rx_head, ref);\n\n\t\trcu_read_lock();\n\t\txdp_prog = rcu_dereference(queue->xdp_prog);\n\t\tif (xdp_prog) {\n\t\t\tif (!(rx->flags & XEN_NETRXF_more_data)) {\n\t\t\t\t \n\t\t\t\tverdict = xennet_run_xdp(queue,\n\t\t\t\t\t\t\t skb_frag_page(&skb_shinfo(skb)->frags[0]),\n\t\t\t\t\t\t\t rx, xdp_prog, &xdp, need_xdp_flush);\n\t\t\t\tif (verdict != XDP_PASS)\n\t\t\t\t\terr = -EINVAL;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\terr = -EINVAL;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\n\t\t__skb_queue_tail(list, skb);\n\nnext:\n\t\tif (!(rx->flags & XEN_NETRXF_more_data))\n\t\t\tbreak;\n\n\t\tif (cons + slots == rp) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tdev_warn(dev, \"Need more slots\\n\");\n\t\t\terr = -ENOENT;\n\t\t\tbreak;\n\t\t}\n\n\t\tRING_COPY_RESPONSE(&queue->rx, cons + slots, &rx_local);\n\t\trx = &rx_local;\n\t\tskb = xennet_get_rx_skb(queue, cons + slots);\n\t\tref = xennet_get_rx_ref(queue, cons + slots);\n\t\tslots++;\n\t}\n\n\tif (unlikely(slots > max)) {\n\t\tif (net_ratelimit())\n\t\t\tdev_warn(dev, \"Too many slots\\n\");\n\t\terr = -E2BIG;\n\t}\n\n\tif (unlikely(err))\n\t\txennet_set_rx_rsp_cons(queue, cons + slots);\n\n\treturn err;\n}\n\nstatic int xennet_set_skb_gso(struct sk_buff *skb,\n\t\t\t      struct xen_netif_extra_info *gso)\n{\n\tif (!gso->u.gso.size) {\n\t\tif (net_ratelimit())\n\t\t\tpr_warn(\"GSO size must not be zero\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (gso->u.gso.type != XEN_NETIF_GSO_TYPE_TCPV4 &&\n\t    gso->u.gso.type != XEN_NETIF_GSO_TYPE_TCPV6) {\n\t\tif (net_ratelimit())\n\t\t\tpr_warn(\"Bad GSO type %d\\n\", gso->u.gso.type);\n\t\treturn -EINVAL;\n\t}\n\n\tskb_shinfo(skb)->gso_size = gso->u.gso.size;\n\tskb_shinfo(skb)->gso_type =\n\t\t(gso->u.gso.type == XEN_NETIF_GSO_TYPE_TCPV4) ?\n\t\tSKB_GSO_TCPV4 :\n\t\tSKB_GSO_TCPV6;\n\n\t \n\tskb_shinfo(skb)->gso_type |= SKB_GSO_DODGY;\n\tskb_shinfo(skb)->gso_segs = 0;\n\n\treturn 0;\n}\n\nstatic int xennet_fill_frags(struct netfront_queue *queue,\n\t\t\t     struct sk_buff *skb,\n\t\t\t     struct sk_buff_head *list)\n{\n\tRING_IDX cons = queue->rx.rsp_cons;\n\tstruct sk_buff *nskb;\n\n\twhile ((nskb = __skb_dequeue(list))) {\n\t\tstruct xen_netif_rx_response rx;\n\t\tskb_frag_t *nfrag = &skb_shinfo(nskb)->frags[0];\n\n\t\tRING_COPY_RESPONSE(&queue->rx, ++cons, &rx);\n\n\t\tif (skb_shinfo(skb)->nr_frags == MAX_SKB_FRAGS) {\n\t\t\tunsigned int pull_to = NETFRONT_SKB_CB(skb)->pull_to;\n\n\t\t\tBUG_ON(pull_to < skb_headlen(skb));\n\t\t\t__pskb_pull_tail(skb, pull_to - skb_headlen(skb));\n\t\t}\n\t\tif (unlikely(skb_shinfo(skb)->nr_frags >= MAX_SKB_FRAGS)) {\n\t\t\txennet_set_rx_rsp_cons(queue,\n\t\t\t\t\t       ++cons + skb_queue_len(list));\n\t\t\tkfree_skb(nskb);\n\t\t\treturn -ENOENT;\n\t\t}\n\n\t\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,\n\t\t\t\tskb_frag_page(nfrag),\n\t\t\t\trx.offset, rx.status, PAGE_SIZE);\n\n\t\tskb_shinfo(nskb)->nr_frags = 0;\n\t\tkfree_skb(nskb);\n\t}\n\n\txennet_set_rx_rsp_cons(queue, cons);\n\n\treturn 0;\n}\n\nstatic int checksum_setup(struct net_device *dev, struct sk_buff *skb)\n{\n\tbool recalculate_partial_csum = false;\n\n\t \n\tif (skb->ip_summed != CHECKSUM_PARTIAL && skb_is_gso(skb)) {\n\t\tstruct netfront_info *np = netdev_priv(dev);\n\t\tatomic_inc(&np->rx_gso_checksum_fixup);\n\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\trecalculate_partial_csum = true;\n\t}\n\n\t \n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\treturn skb_checksum_setup(skb, recalculate_partial_csum);\n}\n\nstatic int handle_incoming_queue(struct netfront_queue *queue,\n\t\t\t\t struct sk_buff_head *rxq)\n{\n\tstruct netfront_stats *rx_stats = this_cpu_ptr(queue->info->rx_stats);\n\tint packets_dropped = 0;\n\tstruct sk_buff *skb;\n\n\twhile ((skb = __skb_dequeue(rxq)) != NULL) {\n\t\tint pull_to = NETFRONT_SKB_CB(skb)->pull_to;\n\n\t\tif (pull_to > skb_headlen(skb))\n\t\t\t__pskb_pull_tail(skb, pull_to - skb_headlen(skb));\n\n\t\t \n\t\tskb->protocol = eth_type_trans(skb, queue->info->netdev);\n\t\tskb_reset_network_header(skb);\n\n\t\tif (checksum_setup(queue->info->netdev, skb)) {\n\t\t\tkfree_skb(skb);\n\t\t\tpackets_dropped++;\n\t\t\tqueue->info->netdev->stats.rx_errors++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tu64_stats_update_begin(&rx_stats->syncp);\n\t\trx_stats->packets++;\n\t\trx_stats->bytes += skb->len;\n\t\tu64_stats_update_end(&rx_stats->syncp);\n\n\t\t \n\t\tnapi_gro_receive(&queue->napi, skb);\n\t}\n\n\treturn packets_dropped;\n}\n\nstatic int xennet_poll(struct napi_struct *napi, int budget)\n{\n\tstruct netfront_queue *queue = container_of(napi, struct netfront_queue, napi);\n\tstruct net_device *dev = queue->info->netdev;\n\tstruct sk_buff *skb;\n\tstruct netfront_rx_info rinfo;\n\tstruct xen_netif_rx_response *rx = &rinfo.rx;\n\tstruct xen_netif_extra_info *extras = rinfo.extras;\n\tRING_IDX i, rp;\n\tint work_done;\n\tstruct sk_buff_head rxq;\n\tstruct sk_buff_head errq;\n\tstruct sk_buff_head tmpq;\n\tint err;\n\tbool need_xdp_flush = false;\n\n\tspin_lock(&queue->rx_lock);\n\n\tskb_queue_head_init(&rxq);\n\tskb_queue_head_init(&errq);\n\tskb_queue_head_init(&tmpq);\n\n\trp = queue->rx.sring->rsp_prod;\n\tif (RING_RESPONSE_PROD_OVERFLOW(&queue->rx, rp)) {\n\t\tdev_alert(&dev->dev, \"Illegal number of responses %u\\n\",\n\t\t\t  rp - queue->rx.rsp_cons);\n\t\tqueue->info->broken = true;\n\t\tspin_unlock(&queue->rx_lock);\n\t\treturn 0;\n\t}\n\trmb();  \n\n\ti = queue->rx.rsp_cons;\n\twork_done = 0;\n\twhile ((i != rp) && (work_done < budget)) {\n\t\tRING_COPY_RESPONSE(&queue->rx, i, rx);\n\t\tmemset(extras, 0, sizeof(rinfo.extras));\n\n\t\terr = xennet_get_responses(queue, &rinfo, rp, &tmpq,\n\t\t\t\t\t   &need_xdp_flush);\n\n\t\tif (unlikely(err)) {\n\t\t\tif (queue->info->broken) {\n\t\t\t\tspin_unlock(&queue->rx_lock);\n\t\t\t\treturn 0;\n\t\t\t}\nerr:\n\t\t\twhile ((skb = __skb_dequeue(&tmpq)))\n\t\t\t\t__skb_queue_tail(&errq, skb);\n\t\t\tdev->stats.rx_errors++;\n\t\t\ti = queue->rx.rsp_cons;\n\t\t\tcontinue;\n\t\t}\n\n\t\tskb = __skb_dequeue(&tmpq);\n\n\t\tif (extras[XEN_NETIF_EXTRA_TYPE_GSO - 1].type) {\n\t\t\tstruct xen_netif_extra_info *gso;\n\t\t\tgso = &extras[XEN_NETIF_EXTRA_TYPE_GSO - 1];\n\n\t\t\tif (unlikely(xennet_set_skb_gso(skb, gso))) {\n\t\t\t\t__skb_queue_head(&tmpq, skb);\n\t\t\t\txennet_set_rx_rsp_cons(queue,\n\t\t\t\t\t\t       queue->rx.rsp_cons +\n\t\t\t\t\t\t       skb_queue_len(&tmpq));\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\n\t\tNETFRONT_SKB_CB(skb)->pull_to = rx->status;\n\t\tif (NETFRONT_SKB_CB(skb)->pull_to > RX_COPY_THRESHOLD)\n\t\t\tNETFRONT_SKB_CB(skb)->pull_to = RX_COPY_THRESHOLD;\n\n\t\tskb_frag_off_set(&skb_shinfo(skb)->frags[0], rx->offset);\n\t\tskb_frag_size_set(&skb_shinfo(skb)->frags[0], rx->status);\n\t\tskb->data_len = rx->status;\n\t\tskb->len += rx->status;\n\n\t\tif (unlikely(xennet_fill_frags(queue, skb, &tmpq)))\n\t\t\tgoto err;\n\n\t\tif (rx->flags & XEN_NETRXF_csum_blank)\n\t\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\telse if (rx->flags & XEN_NETRXF_data_validated)\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\t\t__skb_queue_tail(&rxq, skb);\n\n\t\ti = queue->rx.rsp_cons + 1;\n\t\txennet_set_rx_rsp_cons(queue, i);\n\t\twork_done++;\n\t}\n\tif (need_xdp_flush)\n\t\txdp_do_flush();\n\n\t__skb_queue_purge(&errq);\n\n\twork_done -= handle_incoming_queue(queue, &rxq);\n\n\txennet_alloc_rx_buffers(queue);\n\n\tif (work_done < budget) {\n\t\tint more_to_do = 0;\n\n\t\tnapi_complete_done(napi, work_done);\n\n\t\tRING_FINAL_CHECK_FOR_RESPONSES(&queue->rx, more_to_do);\n\t\tif (more_to_do)\n\t\t\tnapi_schedule(napi);\n\t}\n\n\tspin_unlock(&queue->rx_lock);\n\n\treturn work_done;\n}\n\nstatic int xennet_change_mtu(struct net_device *dev, int mtu)\n{\n\tint max = xennet_can_sg(dev) ? XEN_NETIF_MAX_TX_SIZE : ETH_DATA_LEN;\n\n\tif (mtu > max)\n\t\treturn -EINVAL;\n\tdev->mtu = mtu;\n\treturn 0;\n}\n\nstatic void xennet_get_stats64(struct net_device *dev,\n\t\t\t       struct rtnl_link_stats64 *tot)\n{\n\tstruct netfront_info *np = netdev_priv(dev);\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct netfront_stats *rx_stats = per_cpu_ptr(np->rx_stats, cpu);\n\t\tstruct netfront_stats *tx_stats = per_cpu_ptr(np->tx_stats, cpu);\n\t\tu64 rx_packets, rx_bytes, tx_packets, tx_bytes;\n\t\tunsigned int start;\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&tx_stats->syncp);\n\t\t\ttx_packets = tx_stats->packets;\n\t\t\ttx_bytes = tx_stats->bytes;\n\t\t} while (u64_stats_fetch_retry(&tx_stats->syncp, start));\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&rx_stats->syncp);\n\t\t\trx_packets = rx_stats->packets;\n\t\t\trx_bytes = rx_stats->bytes;\n\t\t} while (u64_stats_fetch_retry(&rx_stats->syncp, start));\n\n\t\ttot->rx_packets += rx_packets;\n\t\ttot->tx_packets += tx_packets;\n\t\ttot->rx_bytes   += rx_bytes;\n\t\ttot->tx_bytes   += tx_bytes;\n\t}\n\n\ttot->rx_errors  = dev->stats.rx_errors;\n\ttot->tx_dropped = dev->stats.tx_dropped;\n}\n\nstatic void xennet_release_tx_bufs(struct netfront_queue *queue)\n{\n\tstruct sk_buff *skb;\n\tint i;\n\n\tfor (i = 0; i < NET_TX_RING_SIZE; i++) {\n\t\t \n\t\tif (!queue->tx_skbs[i])\n\t\t\tcontinue;\n\n\t\tskb = queue->tx_skbs[i];\n\t\tqueue->tx_skbs[i] = NULL;\n\t\tget_page(queue->grant_tx_page[i]);\n\t\tgnttab_end_foreign_access(queue->grant_tx_ref[i],\n\t\t\t\t\t  queue->grant_tx_page[i]);\n\t\tqueue->grant_tx_page[i] = NULL;\n\t\tqueue->grant_tx_ref[i] = INVALID_GRANT_REF;\n\t\tadd_id_to_list(&queue->tx_skb_freelist, queue->tx_link, i);\n\t\tdev_kfree_skb_irq(skb);\n\t}\n}\n\nstatic void xennet_release_rx_bufs(struct netfront_queue *queue)\n{\n\tint id, ref;\n\n\tspin_lock_bh(&queue->rx_lock);\n\n\tfor (id = 0; id < NET_RX_RING_SIZE; id++) {\n\t\tstruct sk_buff *skb;\n\t\tstruct page *page;\n\n\t\tskb = queue->rx_skbs[id];\n\t\tif (!skb)\n\t\t\tcontinue;\n\n\t\tref = queue->grant_rx_ref[id];\n\t\tif (ref == INVALID_GRANT_REF)\n\t\t\tcontinue;\n\n\t\tpage = skb_frag_page(&skb_shinfo(skb)->frags[0]);\n\n\t\t \n\t\tget_page(page);\n\t\tgnttab_end_foreign_access(ref, page);\n\t\tqueue->grant_rx_ref[id] = INVALID_GRANT_REF;\n\n\t\tkfree_skb(skb);\n\t}\n\n\tspin_unlock_bh(&queue->rx_lock);\n}\n\nstatic netdev_features_t xennet_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\tstruct netfront_info *np = netdev_priv(dev);\n\n\tif (features & NETIF_F_SG &&\n\t    !xenbus_read_unsigned(np->xbdev->otherend, \"feature-sg\", 0))\n\t\tfeatures &= ~NETIF_F_SG;\n\n\tif (features & NETIF_F_IPV6_CSUM &&\n\t    !xenbus_read_unsigned(np->xbdev->otherend,\n\t\t\t\t  \"feature-ipv6-csum-offload\", 0))\n\t\tfeatures &= ~NETIF_F_IPV6_CSUM;\n\n\tif (features & NETIF_F_TSO &&\n\t    !xenbus_read_unsigned(np->xbdev->otherend, \"feature-gso-tcpv4\", 0))\n\t\tfeatures &= ~NETIF_F_TSO;\n\n\tif (features & NETIF_F_TSO6 &&\n\t    !xenbus_read_unsigned(np->xbdev->otherend, \"feature-gso-tcpv6\", 0))\n\t\tfeatures &= ~NETIF_F_TSO6;\n\n\treturn features;\n}\n\nstatic int xennet_set_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\tif (!(features & NETIF_F_SG) && dev->mtu > ETH_DATA_LEN) {\n\t\tnetdev_info(dev, \"Reducing MTU because no SG offload\");\n\t\tdev->mtu = ETH_DATA_LEN;\n\t}\n\n\treturn 0;\n}\n\nstatic bool xennet_handle_tx(struct netfront_queue *queue, unsigned int *eoi)\n{\n\tunsigned long flags;\n\n\tif (unlikely(queue->info->broken))\n\t\treturn false;\n\n\tspin_lock_irqsave(&queue->tx_lock, flags);\n\tif (xennet_tx_buf_gc(queue))\n\t\t*eoi = 0;\n\tspin_unlock_irqrestore(&queue->tx_lock, flags);\n\n\treturn true;\n}\n\nstatic irqreturn_t xennet_tx_interrupt(int irq, void *dev_id)\n{\n\tunsigned int eoiflag = XEN_EOI_FLAG_SPURIOUS;\n\n\tif (likely(xennet_handle_tx(dev_id, &eoiflag)))\n\t\txen_irq_lateeoi(irq, eoiflag);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic bool xennet_handle_rx(struct netfront_queue *queue, unsigned int *eoi)\n{\n\tunsigned int work_queued;\n\tunsigned long flags;\n\n\tif (unlikely(queue->info->broken))\n\t\treturn false;\n\n\tspin_lock_irqsave(&queue->rx_cons_lock, flags);\n\twork_queued = XEN_RING_NR_UNCONSUMED_RESPONSES(&queue->rx);\n\tif (work_queued > queue->rx_rsp_unconsumed) {\n\t\tqueue->rx_rsp_unconsumed = work_queued;\n\t\t*eoi = 0;\n\t} else if (unlikely(work_queued < queue->rx_rsp_unconsumed)) {\n\t\tconst struct device *dev = &queue->info->netdev->dev;\n\n\t\tspin_unlock_irqrestore(&queue->rx_cons_lock, flags);\n\t\tdev_alert(dev, \"RX producer index going backwards\\n\");\n\t\tdev_alert(dev, \"Disabled for further use\\n\");\n\t\tqueue->info->broken = true;\n\t\treturn false;\n\t}\n\tspin_unlock_irqrestore(&queue->rx_cons_lock, flags);\n\n\tif (likely(netif_carrier_ok(queue->info->netdev) && work_queued))\n\t\tnapi_schedule(&queue->napi);\n\n\treturn true;\n}\n\nstatic irqreturn_t xennet_rx_interrupt(int irq, void *dev_id)\n{\n\tunsigned int eoiflag = XEN_EOI_FLAG_SPURIOUS;\n\n\tif (likely(xennet_handle_rx(dev_id, &eoiflag)))\n\t\txen_irq_lateeoi(irq, eoiflag);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t xennet_interrupt(int irq, void *dev_id)\n{\n\tunsigned int eoiflag = XEN_EOI_FLAG_SPURIOUS;\n\n\tif (xennet_handle_tx(dev_id, &eoiflag) &&\n\t    xennet_handle_rx(dev_id, &eoiflag))\n\t\txen_irq_lateeoi(irq, eoiflag);\n\n\treturn IRQ_HANDLED;\n}\n\n#ifdef CONFIG_NET_POLL_CONTROLLER\nstatic void xennet_poll_controller(struct net_device *dev)\n{\n\t \n\tstruct netfront_info *info = netdev_priv(dev);\n\tunsigned int num_queues = dev->real_num_tx_queues;\n\tunsigned int i;\n\n\tif (info->broken)\n\t\treturn;\n\n\tfor (i = 0; i < num_queues; ++i)\n\t\txennet_interrupt(0, &info->queues[i]);\n}\n#endif\n\n#define NETBACK_XDP_HEADROOM_DISABLE\t0\n#define NETBACK_XDP_HEADROOM_ENABLE\t1\n\nstatic int talk_to_netback_xdp(struct netfront_info *np, int xdp)\n{\n\tint err;\n\tunsigned short headroom;\n\n\theadroom = xdp ? XDP_PACKET_HEADROOM : 0;\n\terr = xenbus_printf(XBT_NIL, np->xbdev->nodename,\n\t\t\t    \"xdp-headroom\", \"%hu\",\n\t\t\t    headroom);\n\tif (err)\n\t\tpr_warn(\"Error writing xdp-headroom\\n\");\n\n\treturn err;\n}\n\nstatic int xennet_xdp_set(struct net_device *dev, struct bpf_prog *prog,\n\t\t\t  struct netlink_ext_ack *extack)\n{\n\tunsigned long max_mtu = XEN_PAGE_SIZE - XDP_PACKET_HEADROOM;\n\tstruct netfront_info *np = netdev_priv(dev);\n\tstruct bpf_prog *old_prog;\n\tunsigned int i, err;\n\n\tif (dev->mtu > max_mtu) {\n\t\tnetdev_warn(dev, \"XDP requires MTU less than %lu\\n\", max_mtu);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!np->netback_has_xdp_headroom)\n\t\treturn 0;\n\n\txenbus_switch_state(np->xbdev, XenbusStateReconfiguring);\n\n\terr = talk_to_netback_xdp(np, prog ? NETBACK_XDP_HEADROOM_ENABLE :\n\t\t\t\t  NETBACK_XDP_HEADROOM_DISABLE);\n\tif (err)\n\t\treturn err;\n\n\t \n\twait_event(module_wq,\n\t\t   xenbus_read_driver_state(np->xbdev->otherend) ==\n\t\t   XenbusStateReconfigured);\n\tnp->netfront_xdp_enabled = true;\n\n\told_prog = rtnl_dereference(np->queues[0].xdp_prog);\n\n\tif (prog)\n\t\tbpf_prog_add(prog, dev->real_num_tx_queues);\n\n\tfor (i = 0; i < dev->real_num_tx_queues; ++i)\n\t\trcu_assign_pointer(np->queues[i].xdp_prog, prog);\n\n\tif (old_prog)\n\t\tfor (i = 0; i < dev->real_num_tx_queues; ++i)\n\t\t\tbpf_prog_put(old_prog);\n\n\txenbus_switch_state(np->xbdev, XenbusStateConnected);\n\n\treturn 0;\n}\n\nstatic int xennet_xdp(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tstruct netfront_info *np = netdev_priv(dev);\n\n\tif (np->broken)\n\t\treturn -ENODEV;\n\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn xennet_xdp_set(dev, xdp->prog, xdp->extack);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic const struct net_device_ops xennet_netdev_ops = {\n\t.ndo_uninit          = xennet_uninit,\n\t.ndo_open            = xennet_open,\n\t.ndo_stop            = xennet_close,\n\t.ndo_start_xmit      = xennet_start_xmit,\n\t.ndo_change_mtu\t     = xennet_change_mtu,\n\t.ndo_get_stats64     = xennet_get_stats64,\n\t.ndo_set_mac_address = eth_mac_addr,\n\t.ndo_validate_addr   = eth_validate_addr,\n\t.ndo_fix_features    = xennet_fix_features,\n\t.ndo_set_features    = xennet_set_features,\n\t.ndo_select_queue    = xennet_select_queue,\n\t.ndo_bpf            = xennet_xdp,\n\t.ndo_xdp_xmit\t    = xennet_xdp_xmit,\n#ifdef CONFIG_NET_POLL_CONTROLLER\n\t.ndo_poll_controller = xennet_poll_controller,\n#endif\n};\n\nstatic void xennet_free_netdev(struct net_device *netdev)\n{\n\tstruct netfront_info *np = netdev_priv(netdev);\n\n\tfree_percpu(np->rx_stats);\n\tfree_percpu(np->tx_stats);\n\tfree_netdev(netdev);\n}\n\nstatic struct net_device *xennet_create_dev(struct xenbus_device *dev)\n{\n\tint err;\n\tstruct net_device *netdev;\n\tstruct netfront_info *np;\n\n\tnetdev = alloc_etherdev_mq(sizeof(struct netfront_info), xennet_max_queues);\n\tif (!netdev)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tnp                   = netdev_priv(netdev);\n\tnp->xbdev            = dev;\n\n\tnp->queues = NULL;\n\n\terr = -ENOMEM;\n\tnp->rx_stats = netdev_alloc_pcpu_stats(struct netfront_stats);\n\tif (np->rx_stats == NULL)\n\t\tgoto exit;\n\tnp->tx_stats = netdev_alloc_pcpu_stats(struct netfront_stats);\n\tif (np->tx_stats == NULL)\n\t\tgoto exit;\n\n\tnetdev->netdev_ops\t= &xennet_netdev_ops;\n\n\tnetdev->features        = NETIF_F_IP_CSUM | NETIF_F_RXCSUM |\n\t\t\t\t  NETIF_F_GSO_ROBUST;\n\tnetdev->hw_features\t= NETIF_F_SG |\n\t\t\t\t  NETIF_F_IPV6_CSUM |\n\t\t\t\t  NETIF_F_TSO | NETIF_F_TSO6;\n\n\t \n\tnetdev->features |= netdev->hw_features;\n\tnetdev->xdp_features = NETDEV_XDP_ACT_BASIC | NETDEV_XDP_ACT_REDIRECT |\n\t\t\t       NETDEV_XDP_ACT_NDO_XMIT;\n\n\tnetdev->ethtool_ops = &xennet_ethtool_ops;\n\tnetdev->min_mtu = ETH_MIN_MTU;\n\tnetdev->max_mtu = XEN_NETIF_MAX_TX_SIZE;\n\tSET_NETDEV_DEV(netdev, &dev->dev);\n\n\tnp->netdev = netdev;\n\tnp->netfront_xdp_enabled = false;\n\n\tnetif_carrier_off(netdev);\n\n\tdo {\n\t\txenbus_switch_state(dev, XenbusStateInitialising);\n\t\terr = wait_event_timeout(module_wq,\n\t\t\t\t xenbus_read_driver_state(dev->otherend) !=\n\t\t\t\t XenbusStateClosed &&\n\t\t\t\t xenbus_read_driver_state(dev->otherend) !=\n\t\t\t\t XenbusStateUnknown, XENNET_TIMEOUT);\n\t} while (!err);\n\n\treturn netdev;\n\n exit:\n\txennet_free_netdev(netdev);\n\treturn ERR_PTR(err);\n}\n\n \nstatic int netfront_probe(struct xenbus_device *dev,\n\t\t\t  const struct xenbus_device_id *id)\n{\n\tint err;\n\tstruct net_device *netdev;\n\tstruct netfront_info *info;\n\n\tnetdev = xennet_create_dev(dev);\n\tif (IS_ERR(netdev)) {\n\t\terr = PTR_ERR(netdev);\n\t\txenbus_dev_fatal(dev, err, \"creating netdev\");\n\t\treturn err;\n\t}\n\n\tinfo = netdev_priv(netdev);\n\tdev_set_drvdata(&dev->dev, info);\n#ifdef CONFIG_SYSFS\n\tinfo->netdev->sysfs_groups[0] = &xennet_dev_group;\n#endif\n\n\treturn 0;\n}\n\nstatic void xennet_end_access(int ref, void *page)\n{\n\t \n\tif (ref != INVALID_GRANT_REF)\n\t\tgnttab_end_foreign_access(ref, virt_to_page(page));\n}\n\nstatic void xennet_disconnect_backend(struct netfront_info *info)\n{\n\tunsigned int i = 0;\n\tunsigned int num_queues = info->netdev->real_num_tx_queues;\n\n\tnetif_carrier_off(info->netdev);\n\n\tfor (i = 0; i < num_queues && info->queues; ++i) {\n\t\tstruct netfront_queue *queue = &info->queues[i];\n\n\t\tdel_timer_sync(&queue->rx_refill_timer);\n\n\t\tif (queue->tx_irq && (queue->tx_irq == queue->rx_irq))\n\t\t\tunbind_from_irqhandler(queue->tx_irq, queue);\n\t\tif (queue->tx_irq && (queue->tx_irq != queue->rx_irq)) {\n\t\t\tunbind_from_irqhandler(queue->tx_irq, queue);\n\t\t\tunbind_from_irqhandler(queue->rx_irq, queue);\n\t\t}\n\t\tqueue->tx_evtchn = queue->rx_evtchn = 0;\n\t\tqueue->tx_irq = queue->rx_irq = 0;\n\n\t\tif (netif_running(info->netdev))\n\t\t\tnapi_synchronize(&queue->napi);\n\n\t\txennet_release_tx_bufs(queue);\n\t\txennet_release_rx_bufs(queue);\n\t\tgnttab_free_grant_references(queue->gref_tx_head);\n\t\tgnttab_free_grant_references(queue->gref_rx_head);\n\n\t\t \n\t\txennet_end_access(queue->tx_ring_ref, queue->tx.sring);\n\t\txennet_end_access(queue->rx_ring_ref, queue->rx.sring);\n\n\t\tqueue->tx_ring_ref = INVALID_GRANT_REF;\n\t\tqueue->rx_ring_ref = INVALID_GRANT_REF;\n\t\tqueue->tx.sring = NULL;\n\t\tqueue->rx.sring = NULL;\n\n\t\tpage_pool_destroy(queue->page_pool);\n\t}\n}\n\n \nstatic int netfront_resume(struct xenbus_device *dev)\n{\n\tstruct netfront_info *info = dev_get_drvdata(&dev->dev);\n\n\tdev_dbg(&dev->dev, \"%s\\n\", dev->nodename);\n\n\tnetif_tx_lock_bh(info->netdev);\n\tnetif_device_detach(info->netdev);\n\tnetif_tx_unlock_bh(info->netdev);\n\n\txennet_disconnect_backend(info);\n\n\trtnl_lock();\n\tif (info->queues)\n\t\txennet_destroy_queues(info);\n\trtnl_unlock();\n\n\treturn 0;\n}\n\nstatic int xen_net_read_mac(struct xenbus_device *dev, u8 mac[])\n{\n\tchar *s, *e, *macstr;\n\tint i;\n\n\tmacstr = s = xenbus_read(XBT_NIL, dev->nodename, \"mac\", NULL);\n\tif (IS_ERR(macstr))\n\t\treturn PTR_ERR(macstr);\n\n\tfor (i = 0; i < ETH_ALEN; i++) {\n\t\tmac[i] = simple_strtoul(s, &e, 16);\n\t\tif ((s == e) || (*e != ((i == ETH_ALEN-1) ? '\\0' : ':'))) {\n\t\t\tkfree(macstr);\n\t\t\treturn -ENOENT;\n\t\t}\n\t\ts = e+1;\n\t}\n\n\tkfree(macstr);\n\treturn 0;\n}\n\nstatic int setup_netfront_single(struct netfront_queue *queue)\n{\n\tint err;\n\n\terr = xenbus_alloc_evtchn(queue->info->xbdev, &queue->tx_evtchn);\n\tif (err < 0)\n\t\tgoto fail;\n\n\terr = bind_evtchn_to_irqhandler_lateeoi(queue->tx_evtchn,\n\t\t\t\t\t\txennet_interrupt, 0,\n\t\t\t\t\t\tqueue->info->netdev->name,\n\t\t\t\t\t\tqueue);\n\tif (err < 0)\n\t\tgoto bind_fail;\n\tqueue->rx_evtchn = queue->tx_evtchn;\n\tqueue->rx_irq = queue->tx_irq = err;\n\n\treturn 0;\n\nbind_fail:\n\txenbus_free_evtchn(queue->info->xbdev, queue->tx_evtchn);\n\tqueue->tx_evtchn = 0;\nfail:\n\treturn err;\n}\n\nstatic int setup_netfront_split(struct netfront_queue *queue)\n{\n\tint err;\n\n\terr = xenbus_alloc_evtchn(queue->info->xbdev, &queue->tx_evtchn);\n\tif (err < 0)\n\t\tgoto fail;\n\terr = xenbus_alloc_evtchn(queue->info->xbdev, &queue->rx_evtchn);\n\tif (err < 0)\n\t\tgoto alloc_rx_evtchn_fail;\n\n\tsnprintf(queue->tx_irq_name, sizeof(queue->tx_irq_name),\n\t\t \"%s-tx\", queue->name);\n\terr = bind_evtchn_to_irqhandler_lateeoi(queue->tx_evtchn,\n\t\t\t\t\t\txennet_tx_interrupt, 0,\n\t\t\t\t\t\tqueue->tx_irq_name, queue);\n\tif (err < 0)\n\t\tgoto bind_tx_fail;\n\tqueue->tx_irq = err;\n\n\tsnprintf(queue->rx_irq_name, sizeof(queue->rx_irq_name),\n\t\t \"%s-rx\", queue->name);\n\terr = bind_evtchn_to_irqhandler_lateeoi(queue->rx_evtchn,\n\t\t\t\t\t\txennet_rx_interrupt, 0,\n\t\t\t\t\t\tqueue->rx_irq_name, queue);\n\tif (err < 0)\n\t\tgoto bind_rx_fail;\n\tqueue->rx_irq = err;\n\n\treturn 0;\n\nbind_rx_fail:\n\tunbind_from_irqhandler(queue->tx_irq, queue);\n\tqueue->tx_irq = 0;\nbind_tx_fail:\n\txenbus_free_evtchn(queue->info->xbdev, queue->rx_evtchn);\n\tqueue->rx_evtchn = 0;\nalloc_rx_evtchn_fail:\n\txenbus_free_evtchn(queue->info->xbdev, queue->tx_evtchn);\n\tqueue->tx_evtchn = 0;\nfail:\n\treturn err;\n}\n\nstatic int setup_netfront(struct xenbus_device *dev,\n\t\t\tstruct netfront_queue *queue, unsigned int feature_split_evtchn)\n{\n\tstruct xen_netif_tx_sring *txs;\n\tstruct xen_netif_rx_sring *rxs;\n\tint err;\n\n\tqueue->tx_ring_ref = INVALID_GRANT_REF;\n\tqueue->rx_ring_ref = INVALID_GRANT_REF;\n\tqueue->rx.sring = NULL;\n\tqueue->tx.sring = NULL;\n\n\terr = xenbus_setup_ring(dev, GFP_NOIO | __GFP_HIGH, (void **)&txs,\n\t\t\t\t1, &queue->tx_ring_ref);\n\tif (err)\n\t\tgoto fail;\n\n\tXEN_FRONT_RING_INIT(&queue->tx, txs, XEN_PAGE_SIZE);\n\n\terr = xenbus_setup_ring(dev, GFP_NOIO | __GFP_HIGH, (void **)&rxs,\n\t\t\t\t1, &queue->rx_ring_ref);\n\tif (err)\n\t\tgoto fail;\n\n\tXEN_FRONT_RING_INIT(&queue->rx, rxs, XEN_PAGE_SIZE);\n\n\tif (feature_split_evtchn)\n\t\terr = setup_netfront_split(queue);\n\t \n\tif (!feature_split_evtchn || err)\n\t\terr = setup_netfront_single(queue);\n\n\tif (err)\n\t\tgoto fail;\n\n\treturn 0;\n\n fail:\n\txenbus_teardown_ring((void **)&queue->rx.sring, 1, &queue->rx_ring_ref);\n\txenbus_teardown_ring((void **)&queue->tx.sring, 1, &queue->tx_ring_ref);\n\n\treturn err;\n}\n\n \nstatic int xennet_init_queue(struct netfront_queue *queue)\n{\n\tunsigned short i;\n\tint err = 0;\n\tchar *devid;\n\n\tspin_lock_init(&queue->tx_lock);\n\tspin_lock_init(&queue->rx_lock);\n\tspin_lock_init(&queue->rx_cons_lock);\n\n\ttimer_setup(&queue->rx_refill_timer, rx_refill_timeout, 0);\n\n\tdevid = strrchr(queue->info->xbdev->nodename, '/') + 1;\n\tsnprintf(queue->name, sizeof(queue->name), \"vif%s-q%u\",\n\t\t devid, queue->id);\n\n\t \n\tqueue->tx_skb_freelist = 0;\n\tqueue->tx_pend_queue = TX_LINK_NONE;\n\tfor (i = 0; i < NET_TX_RING_SIZE; i++) {\n\t\tqueue->tx_link[i] = i + 1;\n\t\tqueue->grant_tx_ref[i] = INVALID_GRANT_REF;\n\t\tqueue->grant_tx_page[i] = NULL;\n\t}\n\tqueue->tx_link[NET_TX_RING_SIZE - 1] = TX_LINK_NONE;\n\n\t \n\tfor (i = 0; i < NET_RX_RING_SIZE; i++) {\n\t\tqueue->rx_skbs[i] = NULL;\n\t\tqueue->grant_rx_ref[i] = INVALID_GRANT_REF;\n\t}\n\n\t \n\tif (gnttab_alloc_grant_references(NET_TX_RING_SIZE,\n\t\t\t\t\t  &queue->gref_tx_head) < 0) {\n\t\tpr_alert(\"can't alloc tx grant refs\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto exit;\n\t}\n\n\t \n\tif (gnttab_alloc_grant_references(NET_RX_RING_SIZE,\n\t\t\t\t\t  &queue->gref_rx_head) < 0) {\n\t\tpr_alert(\"can't alloc rx grant refs\\n\");\n\t\terr = -ENOMEM;\n\t\tgoto exit_free_tx;\n\t}\n\n\treturn 0;\n\n exit_free_tx:\n\tgnttab_free_grant_references(queue->gref_tx_head);\n exit:\n\treturn err;\n}\n\nstatic int write_queue_xenstore_keys(struct netfront_queue *queue,\n\t\t\t   struct xenbus_transaction *xbt, int write_hierarchical)\n{\n\t \n\tstruct xenbus_device *dev = queue->info->xbdev;\n\tint err;\n\tconst char *message;\n\tchar *path;\n\tsize_t pathsize;\n\n\t \n\tif (write_hierarchical) {\n\t\tpathsize = strlen(dev->nodename) + 10;\n\t\tpath = kzalloc(pathsize, GFP_KERNEL);\n\t\tif (!path) {\n\t\t\terr = -ENOMEM;\n\t\t\tmessage = \"out of memory while writing ring references\";\n\t\t\tgoto error;\n\t\t}\n\t\tsnprintf(path, pathsize, \"%s/queue-%u\",\n\t\t\t\tdev->nodename, queue->id);\n\t} else {\n\t\tpath = (char *)dev->nodename;\n\t}\n\n\t \n\terr = xenbus_printf(*xbt, path, \"tx-ring-ref\", \"%u\",\n\t\t\tqueue->tx_ring_ref);\n\tif (err) {\n\t\tmessage = \"writing tx-ring-ref\";\n\t\tgoto error;\n\t}\n\n\terr = xenbus_printf(*xbt, path, \"rx-ring-ref\", \"%u\",\n\t\t\tqueue->rx_ring_ref);\n\tif (err) {\n\t\tmessage = \"writing rx-ring-ref\";\n\t\tgoto error;\n\t}\n\n\t \n\tif (queue->tx_evtchn == queue->rx_evtchn) {\n\t\t \n\t\terr = xenbus_printf(*xbt, path,\n\t\t\t\t\"event-channel\", \"%u\", queue->tx_evtchn);\n\t\tif (err) {\n\t\t\tmessage = \"writing event-channel\";\n\t\t\tgoto error;\n\t\t}\n\t} else {\n\t\t \n\t\terr = xenbus_printf(*xbt, path,\n\t\t\t\t\"event-channel-tx\", \"%u\", queue->tx_evtchn);\n\t\tif (err) {\n\t\t\tmessage = \"writing event-channel-tx\";\n\t\t\tgoto error;\n\t\t}\n\n\t\terr = xenbus_printf(*xbt, path,\n\t\t\t\t\"event-channel-rx\", \"%u\", queue->rx_evtchn);\n\t\tif (err) {\n\t\t\tmessage = \"writing event-channel-rx\";\n\t\t\tgoto error;\n\t\t}\n\t}\n\n\tif (write_hierarchical)\n\t\tkfree(path);\n\treturn 0;\n\nerror:\n\tif (write_hierarchical)\n\t\tkfree(path);\n\txenbus_dev_fatal(dev, err, \"%s\", message);\n\treturn err;\n}\n\n\n\nstatic int xennet_create_page_pool(struct netfront_queue *queue)\n{\n\tint err;\n\tstruct page_pool_params pp_params = {\n\t\t.order = 0,\n\t\t.flags = 0,\n\t\t.pool_size = NET_RX_RING_SIZE,\n\t\t.nid = NUMA_NO_NODE,\n\t\t.dev = &queue->info->netdev->dev,\n\t\t.offset = XDP_PACKET_HEADROOM,\n\t\t.max_len = XEN_PAGE_SIZE - XDP_PACKET_HEADROOM,\n\t};\n\n\tqueue->page_pool = page_pool_create(&pp_params);\n\tif (IS_ERR(queue->page_pool)) {\n\t\terr = PTR_ERR(queue->page_pool);\n\t\tqueue->page_pool = NULL;\n\t\treturn err;\n\t}\n\n\terr = xdp_rxq_info_reg(&queue->xdp_rxq, queue->info->netdev,\n\t\t\t       queue->id, 0);\n\tif (err) {\n\t\tnetdev_err(queue->info->netdev, \"xdp_rxq_info_reg failed\\n\");\n\t\tgoto err_free_pp;\n\t}\n\n\terr = xdp_rxq_info_reg_mem_model(&queue->xdp_rxq,\n\t\t\t\t\t MEM_TYPE_PAGE_POOL, queue->page_pool);\n\tif (err) {\n\t\tnetdev_err(queue->info->netdev, \"xdp_rxq_info_reg_mem_model failed\\n\");\n\t\tgoto err_unregister_rxq;\n\t}\n\treturn 0;\n\nerr_unregister_rxq:\n\txdp_rxq_info_unreg(&queue->xdp_rxq);\nerr_free_pp:\n\tpage_pool_destroy(queue->page_pool);\n\tqueue->page_pool = NULL;\n\treturn err;\n}\n\nstatic int xennet_create_queues(struct netfront_info *info,\n\t\t\t\tunsigned int *num_queues)\n{\n\tunsigned int i;\n\tint ret;\n\n\tinfo->queues = kcalloc(*num_queues, sizeof(struct netfront_queue),\n\t\t\t       GFP_KERNEL);\n\tif (!info->queues)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < *num_queues; i++) {\n\t\tstruct netfront_queue *queue = &info->queues[i];\n\n\t\tqueue->id = i;\n\t\tqueue->info = info;\n\n\t\tret = xennet_init_queue(queue);\n\t\tif (ret < 0) {\n\t\t\tdev_warn(&info->xbdev->dev,\n\t\t\t\t \"only created %d queues\\n\", i);\n\t\t\t*num_queues = i;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tret = xennet_create_page_pool(queue);\n\t\tif (ret < 0) {\n\t\t\tdev_err(&info->xbdev->dev, \"can't allocate page pool\\n\");\n\t\t\t*num_queues = i;\n\t\t\treturn ret;\n\t\t}\n\n\t\tnetif_napi_add(queue->info->netdev, &queue->napi, xennet_poll);\n\t\tif (netif_running(info->netdev))\n\t\t\tnapi_enable(&queue->napi);\n\t}\n\n\tnetif_set_real_num_tx_queues(info->netdev, *num_queues);\n\n\tif (*num_queues == 0) {\n\t\tdev_err(&info->xbdev->dev, \"no queues\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n \nstatic int talk_to_netback(struct xenbus_device *dev,\n\t\t\t   struct netfront_info *info)\n{\n\tconst char *message;\n\tstruct xenbus_transaction xbt;\n\tint err;\n\tunsigned int feature_split_evtchn;\n\tunsigned int i = 0;\n\tunsigned int max_queues = 0;\n\tstruct netfront_queue *queue = NULL;\n\tunsigned int num_queues = 1;\n\tu8 addr[ETH_ALEN];\n\n\tinfo->netdev->irq = 0;\n\n\t \n\tinfo->bounce = !xennet_trusted ||\n\t\t       !xenbus_read_unsigned(dev->nodename, \"trusted\", 1);\n\n\t \n\tmax_queues = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t  \"multi-queue-max-queues\", 1);\n\tnum_queues = min(max_queues, xennet_max_queues);\n\n\t \n\tfeature_split_evtchn = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\"feature-split-event-channels\", 0);\n\n\t \n\terr = xen_net_read_mac(dev, addr);\n\tif (err) {\n\t\txenbus_dev_fatal(dev, err, \"parsing %s/mac\", dev->nodename);\n\t\tgoto out_unlocked;\n\t}\n\teth_hw_addr_set(info->netdev, addr);\n\n\tinfo->netback_has_xdp_headroom = xenbus_read_unsigned(info->xbdev->otherend,\n\t\t\t\t\t\t\t      \"feature-xdp-headroom\", 0);\n\tif (info->netback_has_xdp_headroom) {\n\t\t \n\t\terr = talk_to_netback_xdp(info, info->netfront_xdp_enabled ?\n\t\t\t\t\t  NETBACK_XDP_HEADROOM_ENABLE :\n\t\t\t\t\t  NETBACK_XDP_HEADROOM_DISABLE);\n\t\tif (err)\n\t\t\tgoto out_unlocked;\n\t}\n\n\trtnl_lock();\n\tif (info->queues)\n\t\txennet_destroy_queues(info);\n\n\t \n\tinfo->broken = false;\n\n\terr = xennet_create_queues(info, &num_queues);\n\tif (err < 0) {\n\t\txenbus_dev_fatal(dev, err, \"creating queues\");\n\t\tkfree(info->queues);\n\t\tinfo->queues = NULL;\n\t\tgoto out;\n\t}\n\trtnl_unlock();\n\n\t \n\tfor (i = 0; i < num_queues; ++i) {\n\t\tqueue = &info->queues[i];\n\t\terr = setup_netfront(dev, queue, feature_split_evtchn);\n\t\tif (err)\n\t\t\tgoto destroy_ring;\n\t}\n\nagain:\n\terr = xenbus_transaction_start(&xbt);\n\tif (err) {\n\t\txenbus_dev_fatal(dev, err, \"starting transaction\");\n\t\tgoto destroy_ring;\n\t}\n\n\tif (xenbus_exists(XBT_NIL,\n\t\t\t  info->xbdev->otherend, \"multi-queue-max-queues\")) {\n\t\t \n\t\terr = xenbus_printf(xbt, dev->nodename,\n\t\t\t\t    \"multi-queue-num-queues\", \"%u\", num_queues);\n\t\tif (err) {\n\t\t\tmessage = \"writing multi-queue-num-queues\";\n\t\t\tgoto abort_transaction_no_dev_fatal;\n\t\t}\n\t}\n\n\tif (num_queues == 1) {\n\t\terr = write_queue_xenstore_keys(&info->queues[0], &xbt, 0);  \n\t\tif (err)\n\t\t\tgoto abort_transaction_no_dev_fatal;\n\t} else {\n\t\t \n\t\tfor (i = 0; i < num_queues; ++i) {\n\t\t\tqueue = &info->queues[i];\n\t\t\terr = write_queue_xenstore_keys(queue, &xbt, 1);  \n\t\t\tif (err)\n\t\t\t\tgoto abort_transaction_no_dev_fatal;\n\t\t}\n\t}\n\n\t \n\terr = xenbus_printf(xbt, dev->nodename, \"request-rx-copy\", \"%u\",\n\t\t\t    1);\n\tif (err) {\n\t\tmessage = \"writing request-rx-copy\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-rx-notify\", \"%d\", 1);\n\tif (err) {\n\t\tmessage = \"writing feature-rx-notify\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-sg\", \"%d\", 1);\n\tif (err) {\n\t\tmessage = \"writing feature-sg\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_printf(xbt, dev->nodename, \"feature-gso-tcpv4\", \"%d\", 1);\n\tif (err) {\n\t\tmessage = \"writing feature-gso-tcpv4\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_write(xbt, dev->nodename, \"feature-gso-tcpv6\", \"1\");\n\tif (err) {\n\t\tmessage = \"writing feature-gso-tcpv6\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_write(xbt, dev->nodename, \"feature-ipv6-csum-offload\",\n\t\t\t   \"1\");\n\tif (err) {\n\t\tmessage = \"writing feature-ipv6-csum-offload\";\n\t\tgoto abort_transaction;\n\t}\n\n\terr = xenbus_transaction_end(xbt, 0);\n\tif (err) {\n\t\tif (err == -EAGAIN)\n\t\t\tgoto again;\n\t\txenbus_dev_fatal(dev, err, \"completing transaction\");\n\t\tgoto destroy_ring;\n\t}\n\n\treturn 0;\n\n abort_transaction:\n\txenbus_dev_fatal(dev, err, \"%s\", message);\nabort_transaction_no_dev_fatal:\n\txenbus_transaction_end(xbt, 1);\n destroy_ring:\n\txennet_disconnect_backend(info);\n\trtnl_lock();\n\txennet_destroy_queues(info);\n out:\n\trtnl_unlock();\nout_unlocked:\n\tdevice_unregister(&dev->dev);\n\treturn err;\n}\n\nstatic int xennet_connect(struct net_device *dev)\n{\n\tstruct netfront_info *np = netdev_priv(dev);\n\tunsigned int num_queues = 0;\n\tint err;\n\tunsigned int j = 0;\n\tstruct netfront_queue *queue = NULL;\n\n\tif (!xenbus_read_unsigned(np->xbdev->otherend, \"feature-rx-copy\", 0)) {\n\t\tdev_info(&dev->dev,\n\t\t\t \"backend does not support copying receive path\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\terr = talk_to_netback(np->xbdev, np);\n\tif (err)\n\t\treturn err;\n\tif (np->netback_has_xdp_headroom)\n\t\tpr_info(\"backend supports XDP headroom\\n\");\n\tif (np->bounce)\n\t\tdev_info(&np->xbdev->dev,\n\t\t\t \"bouncing transmitted data to zeroed pages\\n\");\n\n\t \n\tnum_queues = dev->real_num_tx_queues;\n\n\tif (dev->reg_state == NETREG_UNINITIALIZED) {\n\t\terr = register_netdev(dev);\n\t\tif (err) {\n\t\t\tpr_warn(\"%s: register_netdev err=%d\\n\", __func__, err);\n\t\t\tdevice_unregister(&np->xbdev->dev);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\trtnl_lock();\n\tnetdev_update_features(dev);\n\trtnl_unlock();\n\n\t \n\tnetif_tx_lock_bh(np->netdev);\n\tnetif_device_attach(np->netdev);\n\tnetif_tx_unlock_bh(np->netdev);\n\n\tnetif_carrier_on(np->netdev);\n\tfor (j = 0; j < num_queues; ++j) {\n\t\tqueue = &np->queues[j];\n\n\t\tnotify_remote_via_irq(queue->tx_irq);\n\t\tif (queue->tx_irq != queue->rx_irq)\n\t\t\tnotify_remote_via_irq(queue->rx_irq);\n\n\t\tspin_lock_bh(&queue->rx_lock);\n\t\txennet_alloc_rx_buffers(queue);\n\t\tspin_unlock_bh(&queue->rx_lock);\n\t}\n\n\treturn 0;\n}\n\n \nstatic void netback_changed(struct xenbus_device *dev,\n\t\t\t    enum xenbus_state backend_state)\n{\n\tstruct netfront_info *np = dev_get_drvdata(&dev->dev);\n\tstruct net_device *netdev = np->netdev;\n\n\tdev_dbg(&dev->dev, \"%s\\n\", xenbus_strstate(backend_state));\n\n\twake_up_all(&module_wq);\n\n\tswitch (backend_state) {\n\tcase XenbusStateInitialising:\n\tcase XenbusStateInitialised:\n\tcase XenbusStateReconfiguring:\n\tcase XenbusStateReconfigured:\n\tcase XenbusStateUnknown:\n\t\tbreak;\n\n\tcase XenbusStateInitWait:\n\t\tif (dev->state != XenbusStateInitialising)\n\t\t\tbreak;\n\t\tif (xennet_connect(netdev) != 0)\n\t\t\tbreak;\n\t\txenbus_switch_state(dev, XenbusStateConnected);\n\t\tbreak;\n\n\tcase XenbusStateConnected:\n\t\tnetdev_notify_peers(netdev);\n\t\tbreak;\n\n\tcase XenbusStateClosed:\n\t\tif (dev->state == XenbusStateClosed)\n\t\t\tbreak;\n\t\tfallthrough;\t \n\tcase XenbusStateClosing:\n\t\txenbus_frontend_closed(dev);\n\t\tbreak;\n\t}\n}\n\nstatic const struct xennet_stat {\n\tchar name[ETH_GSTRING_LEN];\n\tu16 offset;\n} xennet_stats[] = {\n\t{\n\t\t\"rx_gso_checksum_fixup\",\n\t\toffsetof(struct netfront_info, rx_gso_checksum_fixup)\n\t},\n};\n\nstatic int xennet_get_sset_count(struct net_device *dev, int string_set)\n{\n\tswitch (string_set) {\n\tcase ETH_SS_STATS:\n\t\treturn ARRAY_SIZE(xennet_stats);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic void xennet_get_ethtool_stats(struct net_device *dev,\n\t\t\t\t     struct ethtool_stats *stats, u64 * data)\n{\n\tvoid *np = netdev_priv(dev);\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(xennet_stats); i++)\n\t\tdata[i] = atomic_read((atomic_t *)(np + xennet_stats[i].offset));\n}\n\nstatic void xennet_get_strings(struct net_device *dev, u32 stringset, u8 * data)\n{\n\tint i;\n\n\tswitch (stringset) {\n\tcase ETH_SS_STATS:\n\t\tfor (i = 0; i < ARRAY_SIZE(xennet_stats); i++)\n\t\t\tmemcpy(data + i * ETH_GSTRING_LEN,\n\t\t\t       xennet_stats[i].name, ETH_GSTRING_LEN);\n\t\tbreak;\n\t}\n}\n\nstatic const struct ethtool_ops xennet_ethtool_ops =\n{\n\t.get_link = ethtool_op_get_link,\n\n\t.get_sset_count = xennet_get_sset_count,\n\t.get_ethtool_stats = xennet_get_ethtool_stats,\n\t.get_strings = xennet_get_strings,\n\t.get_ts_info = ethtool_op_get_ts_info,\n};\n\n#ifdef CONFIG_SYSFS\nstatic ssize_t show_rxbuf(struct device *dev,\n\t\t\t  struct device_attribute *attr, char *buf)\n{\n\treturn sprintf(buf, \"%lu\\n\", NET_RX_RING_SIZE);\n}\n\nstatic ssize_t store_rxbuf(struct device *dev,\n\t\t\t   struct device_attribute *attr,\n\t\t\t   const char *buf, size_t len)\n{\n\tchar *endp;\n\n\tif (!capable(CAP_NET_ADMIN))\n\t\treturn -EPERM;\n\n\tsimple_strtoul(buf, &endp, 0);\n\tif (endp == buf)\n\t\treturn -EBADMSG;\n\n\t \n\n\treturn len;\n}\n\nstatic DEVICE_ATTR(rxbuf_min, 0644, show_rxbuf, store_rxbuf);\nstatic DEVICE_ATTR(rxbuf_max, 0644, show_rxbuf, store_rxbuf);\nstatic DEVICE_ATTR(rxbuf_cur, 0444, show_rxbuf, NULL);\n\nstatic struct attribute *xennet_dev_attrs[] = {\n\t&dev_attr_rxbuf_min.attr,\n\t&dev_attr_rxbuf_max.attr,\n\t&dev_attr_rxbuf_cur.attr,\n\tNULL\n};\n\nstatic const struct attribute_group xennet_dev_group = {\n\t.attrs = xennet_dev_attrs\n};\n#endif  \n\nstatic void xennet_bus_close(struct xenbus_device *dev)\n{\n\tint ret;\n\n\tif (xenbus_read_driver_state(dev->otherend) == XenbusStateClosed)\n\t\treturn;\n\tdo {\n\t\txenbus_switch_state(dev, XenbusStateClosing);\n\t\tret = wait_event_timeout(module_wq,\n\t\t\t\t   xenbus_read_driver_state(dev->otherend) ==\n\t\t\t\t   XenbusStateClosing ||\n\t\t\t\t   xenbus_read_driver_state(dev->otherend) ==\n\t\t\t\t   XenbusStateClosed ||\n\t\t\t\t   xenbus_read_driver_state(dev->otherend) ==\n\t\t\t\t   XenbusStateUnknown,\n\t\t\t\t   XENNET_TIMEOUT);\n\t} while (!ret);\n\n\tif (xenbus_read_driver_state(dev->otherend) == XenbusStateClosed)\n\t\treturn;\n\n\tdo {\n\t\txenbus_switch_state(dev, XenbusStateClosed);\n\t\tret = wait_event_timeout(module_wq,\n\t\t\t\t   xenbus_read_driver_state(dev->otherend) ==\n\t\t\t\t   XenbusStateClosed ||\n\t\t\t\t   xenbus_read_driver_state(dev->otherend) ==\n\t\t\t\t   XenbusStateUnknown,\n\t\t\t\t   XENNET_TIMEOUT);\n\t} while (!ret);\n}\n\nstatic void xennet_remove(struct xenbus_device *dev)\n{\n\tstruct netfront_info *info = dev_get_drvdata(&dev->dev);\n\n\txennet_bus_close(dev);\n\txennet_disconnect_backend(info);\n\n\tif (info->netdev->reg_state == NETREG_REGISTERED)\n\t\tunregister_netdev(info->netdev);\n\n\tif (info->queues) {\n\t\trtnl_lock();\n\t\txennet_destroy_queues(info);\n\t\trtnl_unlock();\n\t}\n\txennet_free_netdev(info->netdev);\n}\n\nstatic const struct xenbus_device_id netfront_ids[] = {\n\t{ \"vif\" },\n\t{ \"\" }\n};\n\nstatic struct xenbus_driver netfront_driver = {\n\t.ids = netfront_ids,\n\t.probe = netfront_probe,\n\t.remove = xennet_remove,\n\t.resume = netfront_resume,\n\t.otherend_changed = netback_changed,\n};\n\nstatic int __init netif_init(void)\n{\n\tif (!xen_domain())\n\t\treturn -ENODEV;\n\n\tif (!xen_has_pv_nic_devices())\n\t\treturn -ENODEV;\n\n\tpr_info(\"Initialising Xen virtual ethernet driver\\n\");\n\n\t \n\tif (xennet_max_queues == 0)\n\t\txennet_max_queues = min_t(unsigned int, MAX_QUEUES_DEFAULT,\n\t\t\t\t\t  num_online_cpus());\n\n\treturn xenbus_register_frontend(&netfront_driver);\n}\nmodule_init(netif_init);\n\n\nstatic void __exit netif_exit(void)\n{\n\txenbus_unregister_driver(&netfront_driver);\n}\nmodule_exit(netif_exit);\n\nMODULE_DESCRIPTION(\"Xen virtual network device frontend\");\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS(\"xen:vif\");\nMODULE_ALIAS(\"xennet\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}