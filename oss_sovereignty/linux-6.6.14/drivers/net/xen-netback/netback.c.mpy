{
  "module_name": "netback.c",
  "hash_id": "9c8d75455777ed3c8887aa9c94affa64dcd46e0da3c7e188a7361f8515ee0168",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/xen-netback/netback.c",
  "human_readable_source": " \n\n#include \"common.h\"\n\n#include <linux/kthread.h>\n#include <linux/if_vlan.h>\n#include <linux/udp.h>\n#include <linux/highmem.h>\n\n#include <net/tcp.h>\n\n#include <xen/xen.h>\n#include <xen/events.h>\n#include <xen/interface/memory.h>\n#include <xen/page.h>\n\n#include <asm/xen/hypercall.h>\n\n \nbool separate_tx_rx_irq = true;\nmodule_param(separate_tx_rx_irq, bool, 0644);\n\n \nunsigned int rx_drain_timeout_msecs = 10000;\nmodule_param(rx_drain_timeout_msecs, uint, 0444);\n\n \nunsigned int rx_stall_timeout_msecs = 60000;\nmodule_param(rx_stall_timeout_msecs, uint, 0444);\n\n#define MAX_QUEUES_DEFAULT 8\nunsigned int xenvif_max_queues;\nmodule_param_named(max_queues, xenvif_max_queues, uint, 0644);\nMODULE_PARM_DESC(max_queues,\n\t\t \"Maximum number of queues per virtual interface\");\n\n \n#define FATAL_SKB_SLOTS_DEFAULT 20\nstatic unsigned int fatal_skb_slots = FATAL_SKB_SLOTS_DEFAULT;\nmodule_param(fatal_skb_slots, uint, 0444);\n\n \n#define XEN_NETBACK_TX_COPY_LEN 128\n\n \n#define XENVIF_HASH_CACHE_SIZE_DEFAULT 64\nunsigned int xenvif_hash_cache_size = XENVIF_HASH_CACHE_SIZE_DEFAULT;\nmodule_param_named(hash_cache_size, xenvif_hash_cache_size, uint, 0644);\nMODULE_PARM_DESC(hash_cache_size, \"Number of flows in the hash cache\");\n\n \nbool provides_xdp_headroom = true;\nmodule_param(provides_xdp_headroom, bool, 0644);\n\nstatic void xenvif_idx_release(struct xenvif_queue *queue, u16 pending_idx,\n\t\t\t       u8 status);\n\nstatic void make_tx_response(struct xenvif_queue *queue,\n\t\t\t     struct xen_netif_tx_request *txp,\n\t\t\t     unsigned int extra_count,\n\t\t\t     s8       st);\nstatic void push_tx_responses(struct xenvif_queue *queue);\n\nstatic void xenvif_idx_unmap(struct xenvif_queue *queue, u16 pending_idx);\n\nstatic inline int tx_work_todo(struct xenvif_queue *queue);\n\nstatic inline unsigned long idx_to_pfn(struct xenvif_queue *queue,\n\t\t\t\t       u16 idx)\n{\n\treturn page_to_pfn(queue->mmap_pages[idx]);\n}\n\nstatic inline unsigned long idx_to_kaddr(struct xenvif_queue *queue,\n\t\t\t\t\t u16 idx)\n{\n\treturn (unsigned long)pfn_to_kaddr(idx_to_pfn(queue, idx));\n}\n\n#define callback_param(vif, pending_idx) \\\n\t(vif->pending_tx_info[pending_idx].callback_struct)\n\n \nstatic inline struct xenvif_queue *ubuf_to_queue(const struct ubuf_info_msgzc *ubuf)\n{\n\tu16 pending_idx = ubuf->desc;\n\tstruct pending_tx_info *temp =\n\t\tcontainer_of(ubuf, struct pending_tx_info, callback_struct);\n\treturn container_of(temp - pending_idx,\n\t\t\t    struct xenvif_queue,\n\t\t\t    pending_tx_info[0]);\n}\n\nstatic u16 frag_get_pending_idx(skb_frag_t *frag)\n{\n\treturn (u16)skb_frag_off(frag);\n}\n\nstatic void frag_set_pending_idx(skb_frag_t *frag, u16 pending_idx)\n{\n\tskb_frag_off_set(frag, pending_idx);\n}\n\nstatic inline pending_ring_idx_t pending_index(unsigned i)\n{\n\treturn i & (MAX_PENDING_REQS-1);\n}\n\nvoid xenvif_kick_thread(struct xenvif_queue *queue)\n{\n\twake_up(&queue->wq);\n}\n\nvoid xenvif_napi_schedule_or_enable_events(struct xenvif_queue *queue)\n{\n\tint more_to_do;\n\n\tRING_FINAL_CHECK_FOR_REQUESTS(&queue->tx, more_to_do);\n\n\tif (more_to_do)\n\t\tnapi_schedule(&queue->napi);\n\telse if (atomic_fetch_andnot(NETBK_TX_EOI | NETBK_COMMON_EOI,\n\t\t\t\t     &queue->eoi_pending) &\n\t\t (NETBK_TX_EOI | NETBK_COMMON_EOI))\n\t\txen_irq_lateeoi(queue->tx_irq, 0);\n}\n\nstatic void tx_add_credit(struct xenvif_queue *queue)\n{\n\tunsigned long max_burst, max_credit;\n\n\t \n\tmax_burst = max(131072UL, queue->credit_bytes);\n\n\t \n\tmax_credit = queue->remaining_credit + queue->credit_bytes;\n\tif (max_credit < queue->remaining_credit)\n\t\tmax_credit = ULONG_MAX;  \n\n\tqueue->remaining_credit = min(max_credit, max_burst);\n\tqueue->rate_limited = false;\n}\n\nvoid xenvif_tx_credit_callback(struct timer_list *t)\n{\n\tstruct xenvif_queue *queue = from_timer(queue, t, credit_timeout);\n\ttx_add_credit(queue);\n\txenvif_napi_schedule_or_enable_events(queue);\n}\n\nstatic void xenvif_tx_err(struct xenvif_queue *queue,\n\t\t\t  struct xen_netif_tx_request *txp,\n\t\t\t  unsigned int extra_count, RING_IDX end)\n{\n\tRING_IDX cons = queue->tx.req_cons;\n\tunsigned long flags;\n\n\tdo {\n\t\tspin_lock_irqsave(&queue->response_lock, flags);\n\t\tmake_tx_response(queue, txp, extra_count, XEN_NETIF_RSP_ERROR);\n\t\tpush_tx_responses(queue);\n\t\tspin_unlock_irqrestore(&queue->response_lock, flags);\n\t\tif (cons == end)\n\t\t\tbreak;\n\t\tRING_COPY_REQUEST(&queue->tx, cons++, txp);\n\t\textra_count = 0;  \n\t} while (1);\n\tqueue->tx.req_cons = cons;\n}\n\nstatic void xenvif_fatal_tx_err(struct xenvif *vif)\n{\n\tnetdev_err(vif->dev, \"fatal error; disabling device\\n\");\n\tvif->disabled = true;\n\t \n\tif (vif->num_queues)\n\t\txenvif_kick_thread(&vif->queues[0]);\n}\n\nstatic int xenvif_count_requests(struct xenvif_queue *queue,\n\t\t\t\t struct xen_netif_tx_request *first,\n\t\t\t\t unsigned int extra_count,\n\t\t\t\t struct xen_netif_tx_request *txp,\n\t\t\t\t int work_to_do)\n{\n\tRING_IDX cons = queue->tx.req_cons;\n\tint slots = 0;\n\tint drop_err = 0;\n\tint more_data;\n\n\tif (!(first->flags & XEN_NETTXF_more_data))\n\t\treturn 0;\n\n\tdo {\n\t\tstruct xen_netif_tx_request dropped_tx = { 0 };\n\n\t\tif (slots >= work_to_do) {\n\t\t\tnetdev_err(queue->vif->dev,\n\t\t\t\t   \"Asked for %d slots but exceeds this limit\\n\",\n\t\t\t\t   work_to_do);\n\t\t\txenvif_fatal_tx_err(queue->vif);\n\t\t\treturn -ENODATA;\n\t\t}\n\n\t\t \n\t\tif (unlikely(slots >= fatal_skb_slots)) {\n\t\t\tnetdev_err(queue->vif->dev,\n\t\t\t\t   \"Malicious frontend using %d slots, threshold %u\\n\",\n\t\t\t\t   slots, fatal_skb_slots);\n\t\t\txenvif_fatal_tx_err(queue->vif);\n\t\t\treturn -E2BIG;\n\t\t}\n\n\t\t \n\t\tif (!drop_err && slots >= XEN_NETBK_LEGACY_SLOTS_MAX) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tnetdev_dbg(queue->vif->dev,\n\t\t\t\t\t   \"Too many slots (%d) exceeding limit (%d), dropping packet\\n\",\n\t\t\t\t\t   slots, XEN_NETBK_LEGACY_SLOTS_MAX);\n\t\t\tdrop_err = -E2BIG;\n\t\t}\n\n\t\tif (drop_err)\n\t\t\ttxp = &dropped_tx;\n\n\t\tRING_COPY_REQUEST(&queue->tx, cons + slots, txp);\n\n\t\t \n\t\tif (!drop_err && txp->size > first->size) {\n\t\t\tif (net_ratelimit())\n\t\t\t\tnetdev_dbg(queue->vif->dev,\n\t\t\t\t\t   \"Invalid tx request, slot size %u > remaining size %u\\n\",\n\t\t\t\t\t   txp->size, first->size);\n\t\t\tdrop_err = -EIO;\n\t\t}\n\n\t\tfirst->size -= txp->size;\n\t\tslots++;\n\n\t\tif (unlikely((txp->offset + txp->size) > XEN_PAGE_SIZE)) {\n\t\t\tnetdev_err(queue->vif->dev, \"Cross page boundary, txp->offset: %u, size: %u\\n\",\n\t\t\t\t txp->offset, txp->size);\n\t\t\txenvif_fatal_tx_err(queue->vif);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tmore_data = txp->flags & XEN_NETTXF_more_data;\n\n\t\tif (!drop_err)\n\t\t\ttxp++;\n\n\t} while (more_data);\n\n\tif (drop_err) {\n\t\txenvif_tx_err(queue, first, extra_count, cons + slots);\n\t\treturn drop_err;\n\t}\n\n\treturn slots;\n}\n\n\nstruct xenvif_tx_cb {\n\tu16 copy_pending_idx[XEN_NETBK_LEGACY_SLOTS_MAX + 1];\n\tu8 copy_count;\n\tu32 split_mask;\n};\n\n#define XENVIF_TX_CB(skb) ((struct xenvif_tx_cb *)(skb)->cb)\n#define copy_pending_idx(skb, i) (XENVIF_TX_CB(skb)->copy_pending_idx[i])\n#define copy_count(skb) (XENVIF_TX_CB(skb)->copy_count)\n\nstatic inline void xenvif_tx_create_map_op(struct xenvif_queue *queue,\n\t\t\t\t\t   u16 pending_idx,\n\t\t\t\t\t   struct xen_netif_tx_request *txp,\n\t\t\t\t\t   unsigned int extra_count,\n\t\t\t\t\t   struct gnttab_map_grant_ref *mop)\n{\n\tqueue->pages_to_map[mop-queue->tx_map_ops] = queue->mmap_pages[pending_idx];\n\tgnttab_set_map_op(mop, idx_to_kaddr(queue, pending_idx),\n\t\t\t  GNTMAP_host_map | GNTMAP_readonly,\n\t\t\t  txp->gref, queue->vif->domid);\n\n\tmemcpy(&queue->pending_tx_info[pending_idx].req, txp,\n\t       sizeof(*txp));\n\tqueue->pending_tx_info[pending_idx].extra_count = extra_count;\n}\n\nstatic inline struct sk_buff *xenvif_alloc_skb(unsigned int size)\n{\n\tstruct sk_buff *skb =\n\t\talloc_skb(size + NET_SKB_PAD + NET_IP_ALIGN,\n\t\t\t  GFP_ATOMIC | __GFP_NOWARN);\n\n\tBUILD_BUG_ON(sizeof(*XENVIF_TX_CB(skb)) > sizeof(skb->cb));\n\tif (unlikely(skb == NULL))\n\t\treturn NULL;\n\n\t \n\tskb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN);\n\n\t \n\tskb_shinfo(skb)->destructor_arg = NULL;\n\n\treturn skb;\n}\n\nstatic void xenvif_get_requests(struct xenvif_queue *queue,\n\t\t\t\tstruct sk_buff *skb,\n\t\t\t\tstruct xen_netif_tx_request *first,\n\t\t\t\tstruct xen_netif_tx_request *txfrags,\n\t\t\t        unsigned *copy_ops,\n\t\t\t        unsigned *map_ops,\n\t\t\t\tunsigned int frag_overflow,\n\t\t\t\tstruct sk_buff *nskb,\n\t\t\t\tunsigned int extra_count,\n\t\t\t\tunsigned int data_len)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\tskb_frag_t *frags = shinfo->frags;\n\tu16 pending_idx;\n\tpending_ring_idx_t index;\n\tunsigned int nr_slots;\n\tstruct gnttab_copy *cop = queue->tx_copy_ops + *copy_ops;\n\tstruct gnttab_map_grant_ref *gop = queue->tx_map_ops + *map_ops;\n\tstruct xen_netif_tx_request *txp = first;\n\n\tnr_slots = shinfo->nr_frags + frag_overflow + 1;\n\n\tcopy_count(skb) = 0;\n\tXENVIF_TX_CB(skb)->split_mask = 0;\n\n\t \n\t__skb_put(skb, data_len);\n\twhile (data_len > 0) {\n\t\tint amount = data_len > txp->size ? txp->size : data_len;\n\t\tbool split = false;\n\n\t\tcop->source.u.ref = txp->gref;\n\t\tcop->source.domid = queue->vif->domid;\n\t\tcop->source.offset = txp->offset;\n\n\t\tcop->dest.domid = DOMID_SELF;\n\t\tcop->dest.offset = (offset_in_page(skb->data +\n\t\t\t\t\t\t   skb_headlen(skb) -\n\t\t\t\t\t\t   data_len)) & ~XEN_PAGE_MASK;\n\t\tcop->dest.u.gmfn = virt_to_gfn(skb->data + skb_headlen(skb)\n\t\t\t\t               - data_len);\n\n\t\t \n\t\tif (cop->dest.offset + amount > XEN_PAGE_SIZE) {\n\t\t\tamount = XEN_PAGE_SIZE - cop->dest.offset;\n\t\t\tXENVIF_TX_CB(skb)->split_mask |= 1U << copy_count(skb);\n\t\t\tsplit = true;\n\t\t}\n\n\t\tcop->len = amount;\n\t\tcop->flags = GNTCOPY_source_gref;\n\n\t\tindex = pending_index(queue->pending_cons);\n\t\tpending_idx = queue->pending_ring[index];\n\t\tcallback_param(queue, pending_idx).ctx = NULL;\n\t\tcopy_pending_idx(skb, copy_count(skb)) = pending_idx;\n\t\tif (!split)\n\t\t\tcopy_count(skb)++;\n\n\t\tcop++;\n\t\tdata_len -= amount;\n\n\t\tif (amount == txp->size) {\n\t\t\t \n\n\t\t\tmemcpy(&queue->pending_tx_info[pending_idx].req,\n\t\t\t       txp, sizeof(*txp));\n\t\t\tqueue->pending_tx_info[pending_idx].extra_count =\n\t\t\t\t(txp == first) ? extra_count : 0;\n\n\t\t\tif (txp == first)\n\t\t\t\ttxp = txfrags;\n\t\t\telse\n\t\t\t\ttxp++;\n\t\t\tqueue->pending_cons++;\n\t\t\tnr_slots--;\n\t\t} else {\n\t\t\t \n\t\t\ttxp->offset += amount;\n\t\t\ttxp->size -= amount;\n\t\t}\n\t}\n\n\tfor (shinfo->nr_frags = 0; nr_slots > 0 && shinfo->nr_frags < MAX_SKB_FRAGS;\n\t     nr_slots--) {\n\t\tif (unlikely(!txp->size)) {\n\t\t\tunsigned long flags;\n\n\t\t\tspin_lock_irqsave(&queue->response_lock, flags);\n\t\t\tmake_tx_response(queue, txp, 0, XEN_NETIF_RSP_OKAY);\n\t\t\tpush_tx_responses(queue);\n\t\t\tspin_unlock_irqrestore(&queue->response_lock, flags);\n\t\t\t++txp;\n\t\t\tcontinue;\n\t\t}\n\n\t\tindex = pending_index(queue->pending_cons++);\n\t\tpending_idx = queue->pending_ring[index];\n\t\txenvif_tx_create_map_op(queue, pending_idx, txp,\n\t\t\t\t        txp == first ? extra_count : 0, gop);\n\t\tfrag_set_pending_idx(&frags[shinfo->nr_frags], pending_idx);\n\t\t++shinfo->nr_frags;\n\t\t++gop;\n\n\t\tif (txp == first)\n\t\t\ttxp = txfrags;\n\t\telse\n\t\t\ttxp++;\n\t}\n\n\tif (nr_slots > 0) {\n\n\t\tshinfo = skb_shinfo(nskb);\n\t\tfrags = shinfo->frags;\n\n\t\tfor (shinfo->nr_frags = 0; shinfo->nr_frags < nr_slots; ++txp) {\n\t\t\tif (unlikely(!txp->size)) {\n\t\t\t\tunsigned long flags;\n\n\t\t\t\tspin_lock_irqsave(&queue->response_lock, flags);\n\t\t\t\tmake_tx_response(queue, txp, 0,\n\t\t\t\t\t\t XEN_NETIF_RSP_OKAY);\n\t\t\t\tpush_tx_responses(queue);\n\t\t\t\tspin_unlock_irqrestore(&queue->response_lock,\n\t\t\t\t\t\t       flags);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tindex = pending_index(queue->pending_cons++);\n\t\t\tpending_idx = queue->pending_ring[index];\n\t\t\txenvif_tx_create_map_op(queue, pending_idx, txp, 0,\n\t\t\t\t\t\tgop);\n\t\t\tfrag_set_pending_idx(&frags[shinfo->nr_frags],\n\t\t\t\t\t     pending_idx);\n\t\t\t++shinfo->nr_frags;\n\t\t\t++gop;\n\t\t}\n\n\t\tif (shinfo->nr_frags) {\n\t\t\tskb_shinfo(skb)->frag_list = nskb;\n\t\t\tnskb = NULL;\n\t\t}\n\t}\n\n\tif (nskb) {\n\t\t \n\t\tkfree_skb(nskb);\n\t}\n\n\t(*copy_ops) = cop - queue->tx_copy_ops;\n\t(*map_ops) = gop - queue->tx_map_ops;\n}\n\nstatic inline void xenvif_grant_handle_set(struct xenvif_queue *queue,\n\t\t\t\t\t   u16 pending_idx,\n\t\t\t\t\t   grant_handle_t handle)\n{\n\tif (unlikely(queue->grant_tx_handle[pending_idx] !=\n\t\t     NETBACK_INVALID_HANDLE)) {\n\t\tnetdev_err(queue->vif->dev,\n\t\t\t   \"Trying to overwrite active handle! pending_idx: 0x%x\\n\",\n\t\t\t   pending_idx);\n\t\tBUG();\n\t}\n\tqueue->grant_tx_handle[pending_idx] = handle;\n}\n\nstatic inline void xenvif_grant_handle_reset(struct xenvif_queue *queue,\n\t\t\t\t\t     u16 pending_idx)\n{\n\tif (unlikely(queue->grant_tx_handle[pending_idx] ==\n\t\t     NETBACK_INVALID_HANDLE)) {\n\t\tnetdev_err(queue->vif->dev,\n\t\t\t   \"Trying to unmap invalid handle! pending_idx: 0x%x\\n\",\n\t\t\t   pending_idx);\n\t\tBUG();\n\t}\n\tqueue->grant_tx_handle[pending_idx] = NETBACK_INVALID_HANDLE;\n}\n\nstatic int xenvif_tx_check_gop(struct xenvif_queue *queue,\n\t\t\t       struct sk_buff *skb,\n\t\t\t       struct gnttab_map_grant_ref **gopp_map,\n\t\t\t       struct gnttab_copy **gopp_copy)\n{\n\tstruct gnttab_map_grant_ref *gop_map = *gopp_map;\n\tu16 pending_idx;\n\t \n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\t \n\tstruct skb_shared_info *first_shinfo = NULL;\n\tint nr_frags = shinfo->nr_frags;\n\tconst bool sharedslot = nr_frags &&\n\t\t\t\tfrag_get_pending_idx(&shinfo->frags[0]) ==\n\t\t\t\t    copy_pending_idx(skb, copy_count(skb) - 1);\n\tint i, err = 0;\n\n\tfor (i = 0; i < copy_count(skb); i++) {\n\t\tint newerr;\n\n\t\t \n\t\tpending_idx = copy_pending_idx(skb, i);\n\n\t\tnewerr = (*gopp_copy)->status;\n\n\t\t \n\t\tif (XENVIF_TX_CB(skb)->split_mask & (1U << i)) {\n\t\t\t(*gopp_copy)++;\n\t\t\tif (!newerr)\n\t\t\t\tnewerr = (*gopp_copy)->status;\n\t\t}\n\t\tif (likely(!newerr)) {\n\t\t\t \n\t\t\tif (i < copy_count(skb) - 1 || !sharedslot)\n\t\t\t\txenvif_idx_release(queue, pending_idx,\n\t\t\t\t\t\t   XEN_NETIF_RSP_OKAY);\n\t\t} else {\n\t\t\terr = newerr;\n\t\t\tif (net_ratelimit())\n\t\t\t\tnetdev_dbg(queue->vif->dev,\n\t\t\t\t\t   \"Grant copy of header failed! status: %d pending_idx: %u ref: %u\\n\",\n\t\t\t\t\t   (*gopp_copy)->status,\n\t\t\t\t\t   pending_idx,\n\t\t\t\t\t   (*gopp_copy)->source.u.ref);\n\t\t\t \n\t\t\tif (i < copy_count(skb) - 1 || !sharedslot)\n\t\t\t\txenvif_idx_release(queue, pending_idx,\n\t\t\t\t\t\t   XEN_NETIF_RSP_ERROR);\n\t\t}\n\t\t(*gopp_copy)++;\n\t}\n\ncheck_frags:\n\tfor (i = 0; i < nr_frags; i++, gop_map++) {\n\t\tint j, newerr;\n\n\t\tpending_idx = frag_get_pending_idx(&shinfo->frags[i]);\n\n\t\t \n\t\tnewerr = gop_map->status;\n\n\t\tif (likely(!newerr)) {\n\t\t\txenvif_grant_handle_set(queue,\n\t\t\t\t\t\tpending_idx,\n\t\t\t\t\t\tgop_map->handle);\n\t\t\t \n\t\t\tif (unlikely(err)) {\n\t\t\t\txenvif_idx_unmap(queue, pending_idx);\n\t\t\t\t \n\t\t\t\tif (i == 0 && !first_shinfo && sharedslot)\n\t\t\t\t\txenvif_idx_release(queue, pending_idx,\n\t\t\t\t\t\t\t   XEN_NETIF_RSP_ERROR);\n\t\t\t\telse\n\t\t\t\t\txenvif_idx_release(queue, pending_idx,\n\t\t\t\t\t\t\t   XEN_NETIF_RSP_OKAY);\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (net_ratelimit())\n\t\t\tnetdev_dbg(queue->vif->dev,\n\t\t\t\t   \"Grant map of %d. frag failed! status: %d pending_idx: %u ref: %u\\n\",\n\t\t\t\t   i,\n\t\t\t\t   gop_map->status,\n\t\t\t\t   pending_idx,\n\t\t\t\t   gop_map->ref);\n\n\t\txenvif_idx_release(queue, pending_idx, XEN_NETIF_RSP_ERROR);\n\n\t\t \n\t\tif (err)\n\t\t\tcontinue;\n\n\t\t \n\t\tfor (j = 0; j < i; j++) {\n\t\t\tpending_idx = frag_get_pending_idx(&shinfo->frags[j]);\n\t\t\txenvif_idx_unmap(queue, pending_idx);\n\t\t\txenvif_idx_release(queue, pending_idx,\n\t\t\t\t\t   XEN_NETIF_RSP_OKAY);\n\t\t}\n\n\t\t \n\t\tif (first_shinfo) {\n\t\t\tfor (j = 0; j < first_shinfo->nr_frags; j++) {\n\t\t\t\tpending_idx = frag_get_pending_idx(&first_shinfo->frags[j]);\n\t\t\t\txenvif_idx_unmap(queue, pending_idx);\n\t\t\t\txenvif_idx_release(queue, pending_idx,\n\t\t\t\t\t\t   XEN_NETIF_RSP_OKAY);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\terr = newerr;\n\t}\n\n\tif (skb_has_frag_list(skb) && !first_shinfo) {\n\t\tfirst_shinfo = shinfo;\n\t\tshinfo = skb_shinfo(shinfo->frag_list);\n\t\tnr_frags = shinfo->nr_frags;\n\n\t\tgoto check_frags;\n\t}\n\n\t*gopp_map = gop_map;\n\treturn err;\n}\n\nstatic void xenvif_fill_frags(struct xenvif_queue *queue, struct sk_buff *skb)\n{\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\tint nr_frags = shinfo->nr_frags;\n\tint i;\n\tu16 prev_pending_idx = INVALID_PENDING_IDX;\n\n\tfor (i = 0; i < nr_frags; i++) {\n\t\tskb_frag_t *frag = shinfo->frags + i;\n\t\tstruct xen_netif_tx_request *txp;\n\t\tstruct page *page;\n\t\tu16 pending_idx;\n\n\t\tpending_idx = frag_get_pending_idx(frag);\n\n\t\t \n\t\tif (prev_pending_idx == INVALID_PENDING_IDX)\n\t\t\tskb_shinfo(skb)->destructor_arg =\n\t\t\t\t&callback_param(queue, pending_idx);\n\t\telse\n\t\t\tcallback_param(queue, prev_pending_idx).ctx =\n\t\t\t\t&callback_param(queue, pending_idx);\n\n\t\tcallback_param(queue, pending_idx).ctx = NULL;\n\t\tprev_pending_idx = pending_idx;\n\n\t\ttxp = &queue->pending_tx_info[pending_idx].req;\n\t\tpage = virt_to_page((void *)idx_to_kaddr(queue, pending_idx));\n\t\t__skb_fill_page_desc(skb, i, page, txp->offset, txp->size);\n\t\tskb->len += txp->size;\n\t\tskb->data_len += txp->size;\n\t\tskb->truesize += txp->size;\n\n\t\t \n\t\tget_page(queue->mmap_pages[pending_idx]);\n\t}\n}\n\nstatic int xenvif_get_extras(struct xenvif_queue *queue,\n\t\t\t     struct xen_netif_extra_info *extras,\n\t\t\t     unsigned int *extra_count,\n\t\t\t     int work_to_do)\n{\n\tstruct xen_netif_extra_info extra;\n\tRING_IDX cons = queue->tx.req_cons;\n\n\tdo {\n\t\tif (unlikely(work_to_do-- <= 0)) {\n\t\t\tnetdev_err(queue->vif->dev, \"Missing extra info\\n\");\n\t\t\txenvif_fatal_tx_err(queue->vif);\n\t\t\treturn -EBADR;\n\t\t}\n\n\t\tRING_COPY_REQUEST(&queue->tx, cons, &extra);\n\n\t\tqueue->tx.req_cons = ++cons;\n\t\t(*extra_count)++;\n\n\t\tif (unlikely(!extra.type ||\n\t\t\t     extra.type >= XEN_NETIF_EXTRA_TYPE_MAX)) {\n\t\t\tnetdev_err(queue->vif->dev,\n\t\t\t\t   \"Invalid extra type: %d\\n\", extra.type);\n\t\t\txenvif_fatal_tx_err(queue->vif);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tmemcpy(&extras[extra.type - 1], &extra, sizeof(extra));\n\t} while (extra.flags & XEN_NETIF_EXTRA_FLAG_MORE);\n\n\treturn work_to_do;\n}\n\nstatic int xenvif_set_skb_gso(struct xenvif *vif,\n\t\t\t      struct sk_buff *skb,\n\t\t\t      struct xen_netif_extra_info *gso)\n{\n\tif (!gso->u.gso.size) {\n\t\tnetdev_err(vif->dev, \"GSO size must not be zero.\\n\");\n\t\txenvif_fatal_tx_err(vif);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (gso->u.gso.type) {\n\tcase XEN_NETIF_GSO_TYPE_TCPV4:\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_TCPV4;\n\t\tbreak;\n\tcase XEN_NETIF_GSO_TYPE_TCPV6:\n\t\tskb_shinfo(skb)->gso_type = SKB_GSO_TCPV6;\n\t\tbreak;\n\tdefault:\n\t\tnetdev_err(vif->dev, \"Bad GSO type %d.\\n\", gso->u.gso.type);\n\t\txenvif_fatal_tx_err(vif);\n\t\treturn -EINVAL;\n\t}\n\n\tskb_shinfo(skb)->gso_size = gso->u.gso.size;\n\t \n\n\treturn 0;\n}\n\nstatic int checksum_setup(struct xenvif_queue *queue, struct sk_buff *skb)\n{\n\tbool recalculate_partial_csum = false;\n\n\t \n\tif (skb->ip_summed != CHECKSUM_PARTIAL && skb_is_gso(skb)) {\n\t\tqueue->stats.rx_gso_checksum_fixup++;\n\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\trecalculate_partial_csum = true;\n\t}\n\n\t \n\tif (skb->ip_summed != CHECKSUM_PARTIAL)\n\t\treturn 0;\n\n\treturn skb_checksum_setup(skb, recalculate_partial_csum);\n}\n\nstatic bool tx_credit_exceeded(struct xenvif_queue *queue, unsigned size)\n{\n\tu64 now = get_jiffies_64();\n\tu64 next_credit = queue->credit_window_start +\n\t\tmsecs_to_jiffies(queue->credit_usec / 1000);\n\n\t \n\tif (timer_pending(&queue->credit_timeout)) {\n\t\tqueue->rate_limited = true;\n\t\treturn true;\n\t}\n\n\t \n\tif (time_after_eq64(now, next_credit)) {\n\t\tqueue->credit_window_start = now;\n\t\ttx_add_credit(queue);\n\t}\n\n\t \n\tif (size > queue->remaining_credit) {\n\t\tmod_timer(&queue->credit_timeout,\n\t\t\t  next_credit);\n\t\tqueue->credit_window_start = next_credit;\n\t\tqueue->rate_limited = true;\n\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \n\nstatic int xenvif_mcast_add(struct xenvif *vif, const u8 *addr)\n{\n\tstruct xenvif_mcast_addr *mcast;\n\n\tif (vif->fe_mcast_count == XEN_NETBK_MCAST_MAX) {\n\t\tif (net_ratelimit())\n\t\t\tnetdev_err(vif->dev,\n\t\t\t\t   \"Too many multicast addresses\\n\");\n\t\treturn -ENOSPC;\n\t}\n\n\tmcast = kzalloc(sizeof(*mcast), GFP_ATOMIC);\n\tif (!mcast)\n\t\treturn -ENOMEM;\n\n\tether_addr_copy(mcast->addr, addr);\n\tlist_add_tail_rcu(&mcast->entry, &vif->fe_mcast_addr);\n\tvif->fe_mcast_count++;\n\n\treturn 0;\n}\n\nstatic void xenvif_mcast_del(struct xenvif *vif, const u8 *addr)\n{\n\tstruct xenvif_mcast_addr *mcast;\n\n\tlist_for_each_entry_rcu(mcast, &vif->fe_mcast_addr, entry) {\n\t\tif (ether_addr_equal(addr, mcast->addr)) {\n\t\t\t--vif->fe_mcast_count;\n\t\t\tlist_del_rcu(&mcast->entry);\n\t\t\tkfree_rcu(mcast, rcu);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nbool xenvif_mcast_match(struct xenvif *vif, const u8 *addr)\n{\n\tstruct xenvif_mcast_addr *mcast;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(mcast, &vif->fe_mcast_addr, entry) {\n\t\tif (ether_addr_equal(addr, mcast->addr)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn true;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn false;\n}\n\nvoid xenvif_mcast_addr_list_free(struct xenvif *vif)\n{\n\t \n\twhile (!list_empty(&vif->fe_mcast_addr)) {\n\t\tstruct xenvif_mcast_addr *mcast;\n\n\t\tmcast = list_first_entry(&vif->fe_mcast_addr,\n\t\t\t\t\t struct xenvif_mcast_addr,\n\t\t\t\t\t entry);\n\t\t--vif->fe_mcast_count;\n\t\tlist_del(&mcast->entry);\n\t\tkfree(mcast);\n\t}\n}\n\nstatic void xenvif_tx_build_gops(struct xenvif_queue *queue,\n\t\t\t\t     int budget,\n\t\t\t\t     unsigned *copy_ops,\n\t\t\t\t     unsigned *map_ops)\n{\n\tstruct sk_buff *skb, *nskb;\n\tint ret;\n\tunsigned int frag_overflow;\n\n\twhile (skb_queue_len(&queue->tx_queue) < budget) {\n\t\tstruct xen_netif_tx_request txreq;\n\t\tstruct xen_netif_tx_request txfrags[XEN_NETBK_LEGACY_SLOTS_MAX];\n\t\tstruct xen_netif_extra_info extras[XEN_NETIF_EXTRA_TYPE_MAX-1];\n\t\tunsigned int extra_count;\n\t\tRING_IDX idx;\n\t\tint work_to_do;\n\t\tunsigned int data_len;\n\n\t\tif (queue->tx.sring->req_prod - queue->tx.req_cons >\n\t\t    XEN_NETIF_TX_RING_SIZE) {\n\t\t\tnetdev_err(queue->vif->dev,\n\t\t\t\t   \"Impossible number of requests. \"\n\t\t\t\t   \"req_prod %d, req_cons %d, size %ld\\n\",\n\t\t\t\t   queue->tx.sring->req_prod, queue->tx.req_cons,\n\t\t\t\t   XEN_NETIF_TX_RING_SIZE);\n\t\t\txenvif_fatal_tx_err(queue->vif);\n\t\t\tbreak;\n\t\t}\n\n\t\twork_to_do = XEN_RING_NR_UNCONSUMED_REQUESTS(&queue->tx);\n\t\tif (!work_to_do)\n\t\t\tbreak;\n\n\t\tidx = queue->tx.req_cons;\n\t\trmb();  \n\t\tRING_COPY_REQUEST(&queue->tx, idx, &txreq);\n\n\t\t \n\t\tif (txreq.size > queue->remaining_credit &&\n\t\t    tx_credit_exceeded(queue, txreq.size))\n\t\t\tbreak;\n\n\t\tqueue->remaining_credit -= txreq.size;\n\n\t\twork_to_do--;\n\t\tqueue->tx.req_cons = ++idx;\n\n\t\tmemset(extras, 0, sizeof(extras));\n\t\textra_count = 0;\n\t\tif (txreq.flags & XEN_NETTXF_extra_info) {\n\t\t\twork_to_do = xenvif_get_extras(queue, extras,\n\t\t\t\t\t\t       &extra_count,\n\t\t\t\t\t\t       work_to_do);\n\t\t\tidx = queue->tx.req_cons;\n\t\t\tif (unlikely(work_to_do < 0))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (extras[XEN_NETIF_EXTRA_TYPE_MCAST_ADD - 1].type) {\n\t\t\tstruct xen_netif_extra_info *extra;\n\n\t\t\textra = &extras[XEN_NETIF_EXTRA_TYPE_MCAST_ADD - 1];\n\t\t\tret = xenvif_mcast_add(queue->vif, extra->u.mcast.addr);\n\n\t\t\tmake_tx_response(queue, &txreq, extra_count,\n\t\t\t\t\t (ret == 0) ?\n\t\t\t\t\t XEN_NETIF_RSP_OKAY :\n\t\t\t\t\t XEN_NETIF_RSP_ERROR);\n\t\t\tpush_tx_responses(queue);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (extras[XEN_NETIF_EXTRA_TYPE_MCAST_DEL - 1].type) {\n\t\t\tstruct xen_netif_extra_info *extra;\n\n\t\t\textra = &extras[XEN_NETIF_EXTRA_TYPE_MCAST_DEL - 1];\n\t\t\txenvif_mcast_del(queue->vif, extra->u.mcast.addr);\n\n\t\t\tmake_tx_response(queue, &txreq, extra_count,\n\t\t\t\t\t XEN_NETIF_RSP_OKAY);\n\t\t\tpush_tx_responses(queue);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdata_len = (txreq.size > XEN_NETBACK_TX_COPY_LEN) ?\n\t\t\tXEN_NETBACK_TX_COPY_LEN : txreq.size;\n\n\t\tret = xenvif_count_requests(queue, &txreq, extra_count,\n\t\t\t\t\t    txfrags, work_to_do);\n\n\t\tif (unlikely(ret < 0))\n\t\t\tbreak;\n\n\t\tidx += ret;\n\n\t\tif (unlikely(txreq.size < ETH_HLEN)) {\n\t\t\tnetdev_dbg(queue->vif->dev,\n\t\t\t\t   \"Bad packet size: %d\\n\", txreq.size);\n\t\t\txenvif_tx_err(queue, &txreq, extra_count, idx);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (unlikely((txreq.offset + txreq.size) > XEN_PAGE_SIZE)) {\n\t\t\tnetdev_err(queue->vif->dev, \"Cross page boundary, txreq.offset: %u, size: %u\\n\",\n\t\t\t\t   txreq.offset, txreq.size);\n\t\t\txenvif_fatal_tx_err(queue->vif);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (ret >= XEN_NETBK_LEGACY_SLOTS_MAX - 1 && data_len < txreq.size)\n\t\t\tdata_len = txreq.size;\n\n\t\tskb = xenvif_alloc_skb(data_len);\n\t\tif (unlikely(skb == NULL)) {\n\t\t\tnetdev_dbg(queue->vif->dev,\n\t\t\t\t   \"Can't allocate a skb in start_xmit.\\n\");\n\t\t\txenvif_tx_err(queue, &txreq, extra_count, idx);\n\t\t\tbreak;\n\t\t}\n\n\t\tskb_shinfo(skb)->nr_frags = ret;\n\t\t \n\t\tfrag_overflow = 0;\n\t\tnskb = NULL;\n\t\tif (skb_shinfo(skb)->nr_frags > MAX_SKB_FRAGS) {\n\t\t\tfrag_overflow = skb_shinfo(skb)->nr_frags - MAX_SKB_FRAGS;\n\t\t\tBUG_ON(frag_overflow > MAX_SKB_FRAGS);\n\t\t\tskb_shinfo(skb)->nr_frags = MAX_SKB_FRAGS;\n\t\t\tnskb = xenvif_alloc_skb(0);\n\t\t\tif (unlikely(nskb == NULL)) {\n\t\t\t\tskb_shinfo(skb)->nr_frags = 0;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\txenvif_tx_err(queue, &txreq, extra_count, idx);\n\t\t\t\tif (net_ratelimit())\n\t\t\t\t\tnetdev_err(queue->vif->dev,\n\t\t\t\t\t\t   \"Can't allocate the frag_list skb.\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (extras[XEN_NETIF_EXTRA_TYPE_GSO - 1].type) {\n\t\t\tstruct xen_netif_extra_info *gso;\n\t\t\tgso = &extras[XEN_NETIF_EXTRA_TYPE_GSO - 1];\n\n\t\t\tif (xenvif_set_skb_gso(queue->vif, skb, gso)) {\n\t\t\t\t \n\t\t\t\tskb_shinfo(skb)->nr_frags = 0;\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tkfree_skb(nskb);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (extras[XEN_NETIF_EXTRA_TYPE_HASH - 1].type) {\n\t\t\tstruct xen_netif_extra_info *extra;\n\t\t\tenum pkt_hash_types type = PKT_HASH_TYPE_NONE;\n\n\t\t\textra = &extras[XEN_NETIF_EXTRA_TYPE_HASH - 1];\n\n\t\t\tswitch (extra->u.hash.type) {\n\t\t\tcase _XEN_NETIF_CTRL_HASH_TYPE_IPV4:\n\t\t\tcase _XEN_NETIF_CTRL_HASH_TYPE_IPV6:\n\t\t\t\ttype = PKT_HASH_TYPE_L3;\n\t\t\t\tbreak;\n\n\t\t\tcase _XEN_NETIF_CTRL_HASH_TYPE_IPV4_TCP:\n\t\t\tcase _XEN_NETIF_CTRL_HASH_TYPE_IPV6_TCP:\n\t\t\t\ttype = PKT_HASH_TYPE_L4;\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (type != PKT_HASH_TYPE_NONE)\n\t\t\t\tskb_set_hash(skb,\n\t\t\t\t\t     *(u32 *)extra->u.hash.value,\n\t\t\t\t\t     type);\n\t\t}\n\n\t\txenvif_get_requests(queue, skb, &txreq, txfrags, copy_ops,\n\t\t\t\t    map_ops, frag_overflow, nskb, extra_count,\n\t\t\t\t    data_len);\n\n\t\t__skb_queue_tail(&queue->tx_queue, skb);\n\n\t\tqueue->tx.req_cons = idx;\n\t}\n\n\treturn;\n}\n\n \nstatic int xenvif_handle_frag_list(struct xenvif_queue *queue, struct sk_buff *skb)\n{\n\tunsigned int offset = skb_headlen(skb);\n\tskb_frag_t frags[MAX_SKB_FRAGS];\n\tint i, f;\n\tstruct ubuf_info *uarg;\n\tstruct sk_buff *nskb = skb_shinfo(skb)->frag_list;\n\n\tqueue->stats.tx_zerocopy_sent += 2;\n\tqueue->stats.tx_frag_overflow++;\n\n\txenvif_fill_frags(queue, nskb);\n\t \n\tskb->truesize -= skb->data_len;\n\tskb->len += nskb->len;\n\tskb->data_len += nskb->len;\n\n\t \n\tfor (i = 0; offset < skb->len; i++) {\n\t\tstruct page *page;\n\t\tunsigned int len;\n\n\t\tBUG_ON(i >= MAX_SKB_FRAGS);\n\t\tpage = alloc_page(GFP_ATOMIC);\n\t\tif (!page) {\n\t\t\tint j;\n\t\t\tskb->truesize += skb->data_len;\n\t\t\tfor (j = 0; j < i; j++)\n\t\t\t\tput_page(skb_frag_page(&frags[j]));\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tif (offset + PAGE_SIZE < skb->len)\n\t\t\tlen = PAGE_SIZE;\n\t\telse\n\t\t\tlen = skb->len - offset;\n\t\tif (skb_copy_bits(skb, offset, page_address(page), len))\n\t\t\tBUG();\n\n\t\toffset += len;\n\t\tskb_frag_fill_page_desc(&frags[i], page, 0, len);\n\t}\n\n\t \n\tfor (f = 0; f < skb_shinfo(skb)->nr_frags; f++)\n\t\tskb_frag_unref(skb, f);\n\tuarg = skb_shinfo(skb)->destructor_arg;\n\t \n\tatomic_inc(&queue->inflight_packets);\n\tuarg->callback(NULL, uarg, true);\n\tskb_shinfo(skb)->destructor_arg = NULL;\n\n\t \n\tmemcpy(skb_shinfo(skb)->frags, frags, i * sizeof(skb_frag_t));\n\tskb_shinfo(skb)->nr_frags = i;\n\tskb->truesize += i * PAGE_SIZE;\n\n\treturn 0;\n}\n\nstatic int xenvif_tx_submit(struct xenvif_queue *queue)\n{\n\tstruct gnttab_map_grant_ref *gop_map = queue->tx_map_ops;\n\tstruct gnttab_copy *gop_copy = queue->tx_copy_ops;\n\tstruct sk_buff *skb;\n\tint work_done = 0;\n\n\twhile ((skb = __skb_dequeue(&queue->tx_queue)) != NULL) {\n\t\tstruct xen_netif_tx_request *txp;\n\t\tu16 pending_idx;\n\n\t\tpending_idx = copy_pending_idx(skb, 0);\n\t\ttxp = &queue->pending_tx_info[pending_idx].req;\n\n\t\t \n\t\tif (unlikely(xenvif_tx_check_gop(queue, skb, &gop_map, &gop_copy))) {\n\t\t\t \n\t\t\tskb_shinfo(skb)->nr_frags = 0;\n\t\t\tif (skb_has_frag_list(skb)) {\n\t\t\t\tstruct sk_buff *nskb =\n\t\t\t\t\t\tskb_shinfo(skb)->frag_list;\n\t\t\t\tskb_shinfo(nskb)->nr_frags = 0;\n\t\t\t}\n\t\t\tkfree_skb(skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (txp->flags & XEN_NETTXF_csum_blank)\n\t\t\tskb->ip_summed = CHECKSUM_PARTIAL;\n\t\telse if (txp->flags & XEN_NETTXF_data_validated)\n\t\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\t\txenvif_fill_frags(queue, skb);\n\n\t\tif (unlikely(skb_has_frag_list(skb))) {\n\t\t\tstruct sk_buff *nskb = skb_shinfo(skb)->frag_list;\n\t\t\txenvif_skb_zerocopy_prepare(queue, nskb);\n\t\t\tif (xenvif_handle_frag_list(queue, skb)) {\n\t\t\t\tif (net_ratelimit())\n\t\t\t\t\tnetdev_err(queue->vif->dev,\n\t\t\t\t\t\t   \"Not enough memory to consolidate frag_list!\\n\");\n\t\t\t\txenvif_skb_zerocopy_prepare(queue, skb);\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t \n\t\t\tskb_frag_list_init(skb);\n\t\t\tkfree_skb(nskb);\n\t\t}\n\n\t\tskb->dev      = queue->vif->dev;\n\t\tskb->protocol = eth_type_trans(skb, skb->dev);\n\t\tskb_reset_network_header(skb);\n\n\t\tif (checksum_setup(queue, skb)) {\n\t\t\tnetdev_dbg(queue->vif->dev,\n\t\t\t\t   \"Can't setup checksum in net_tx_action\\n\");\n\t\t\t \n\t\t\tif (skb_shinfo(skb)->destructor_arg)\n\t\t\t\txenvif_skb_zerocopy_prepare(queue, skb);\n\t\t\tkfree_skb(skb);\n\t\t\tcontinue;\n\t\t}\n\n\t\tskb_probe_transport_header(skb);\n\n\t\t \n\t\tif (skb_is_gso(skb)) {\n\t\t\tint mss, hdrlen;\n\n\t\t\t \n\t\t\tWARN_ON_ONCE(!skb_transport_header_was_set(skb));\n\t\t\tif (unlikely(!skb_transport_header_was_set(skb))) {\n\t\t\t\tkfree_skb(skb);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tmss = skb_shinfo(skb)->gso_size;\n\t\t\thdrlen = skb_tcp_all_headers(skb);\n\n\t\t\tskb_shinfo(skb)->gso_segs =\n\t\t\t\tDIV_ROUND_UP(skb->len - hdrlen, mss);\n\t\t}\n\n\t\tqueue->stats.rx_bytes += skb->len;\n\t\tqueue->stats.rx_packets++;\n\n\t\twork_done++;\n\n\t\t \n\t\tif (skb_shinfo(skb)->destructor_arg) {\n\t\t\txenvif_skb_zerocopy_prepare(queue, skb);\n\t\t\tqueue->stats.tx_zerocopy_sent++;\n\t\t}\n\n\t\tnetif_receive_skb(skb);\n\t}\n\n\treturn work_done;\n}\n\nvoid xenvif_zerocopy_callback(struct sk_buff *skb, struct ubuf_info *ubuf_base,\n\t\t\t      bool zerocopy_success)\n{\n\tunsigned long flags;\n\tpending_ring_idx_t index;\n\tstruct ubuf_info_msgzc *ubuf = uarg_to_msgzc(ubuf_base);\n\tstruct xenvif_queue *queue = ubuf_to_queue(ubuf);\n\n\t \n\tspin_lock_irqsave(&queue->callback_lock, flags);\n\tdo {\n\t\tu16 pending_idx = ubuf->desc;\n\t\tubuf = (struct ubuf_info_msgzc *) ubuf->ctx;\n\t\tBUG_ON(queue->dealloc_prod - queue->dealloc_cons >=\n\t\t\tMAX_PENDING_REQS);\n\t\tindex = pending_index(queue->dealloc_prod);\n\t\tqueue->dealloc_ring[index] = pending_idx;\n\t\t \n\t\tsmp_wmb();\n\t\tqueue->dealloc_prod++;\n\t} while (ubuf);\n\tspin_unlock_irqrestore(&queue->callback_lock, flags);\n\n\tif (likely(zerocopy_success))\n\t\tqueue->stats.tx_zerocopy_success++;\n\telse\n\t\tqueue->stats.tx_zerocopy_fail++;\n\txenvif_skb_zerocopy_complete(queue);\n}\n\nstatic inline void xenvif_tx_dealloc_action(struct xenvif_queue *queue)\n{\n\tstruct gnttab_unmap_grant_ref *gop;\n\tpending_ring_idx_t dc, dp;\n\tu16 pending_idx, pending_idx_release[MAX_PENDING_REQS];\n\tunsigned int i = 0;\n\n\tdc = queue->dealloc_cons;\n\tgop = queue->tx_unmap_ops;\n\n\t \n\tdo {\n\t\tdp = queue->dealloc_prod;\n\n\t\t \n\t\tsmp_rmb();\n\n\t\twhile (dc != dp) {\n\t\t\tBUG_ON(gop - queue->tx_unmap_ops >= MAX_PENDING_REQS);\n\t\t\tpending_idx =\n\t\t\t\tqueue->dealloc_ring[pending_index(dc++)];\n\n\t\t\tpending_idx_release[gop - queue->tx_unmap_ops] =\n\t\t\t\tpending_idx;\n\t\t\tqueue->pages_to_unmap[gop - queue->tx_unmap_ops] =\n\t\t\t\tqueue->mmap_pages[pending_idx];\n\t\t\tgnttab_set_unmap_op(gop,\n\t\t\t\t\t    idx_to_kaddr(queue, pending_idx),\n\t\t\t\t\t    GNTMAP_host_map,\n\t\t\t\t\t    queue->grant_tx_handle[pending_idx]);\n\t\t\txenvif_grant_handle_reset(queue, pending_idx);\n\t\t\t++gop;\n\t\t}\n\n\t} while (dp != queue->dealloc_prod);\n\n\tqueue->dealloc_cons = dc;\n\n\tif (gop - queue->tx_unmap_ops > 0) {\n\t\tint ret;\n\t\tret = gnttab_unmap_refs(queue->tx_unmap_ops,\n\t\t\t\t\tNULL,\n\t\t\t\t\tqueue->pages_to_unmap,\n\t\t\t\t\tgop - queue->tx_unmap_ops);\n\t\tif (ret) {\n\t\t\tnetdev_err(queue->vif->dev, \"Unmap fail: nr_ops %tu ret %d\\n\",\n\t\t\t\t   gop - queue->tx_unmap_ops, ret);\n\t\t\tfor (i = 0; i < gop - queue->tx_unmap_ops; ++i) {\n\t\t\t\tif (gop[i].status != GNTST_okay)\n\t\t\t\t\tnetdev_err(queue->vif->dev,\n\t\t\t\t\t\t   \" host_addr: 0x%llx handle: 0x%x status: %d\\n\",\n\t\t\t\t\t\t   gop[i].host_addr,\n\t\t\t\t\t\t   gop[i].handle,\n\t\t\t\t\t\t   gop[i].status);\n\t\t\t}\n\t\t\tBUG();\n\t\t}\n\t}\n\n\tfor (i = 0; i < gop - queue->tx_unmap_ops; ++i)\n\t\txenvif_idx_release(queue, pending_idx_release[i],\n\t\t\t\t   XEN_NETIF_RSP_OKAY);\n}\n\n\n \nint xenvif_tx_action(struct xenvif_queue *queue, int budget)\n{\n\tunsigned nr_mops = 0, nr_cops = 0;\n\tint work_done, ret;\n\n\tif (unlikely(!tx_work_todo(queue)))\n\t\treturn 0;\n\n\txenvif_tx_build_gops(queue, budget, &nr_cops, &nr_mops);\n\n\tif (nr_cops == 0)\n\t\treturn 0;\n\n\tgnttab_batch_copy(queue->tx_copy_ops, nr_cops);\n\tif (nr_mops != 0) {\n\t\tret = gnttab_map_refs(queue->tx_map_ops,\n\t\t\t\t      NULL,\n\t\t\t\t      queue->pages_to_map,\n\t\t\t\t      nr_mops);\n\t\tif (ret) {\n\t\t\tunsigned int i;\n\n\t\t\tnetdev_err(queue->vif->dev, \"Map fail: nr %u ret %d\\n\",\n\t\t\t\t   nr_mops, ret);\n\t\t\tfor (i = 0; i < nr_mops; ++i)\n\t\t\t\tWARN_ON_ONCE(queue->tx_map_ops[i].status ==\n\t\t\t\t             GNTST_okay);\n\t\t}\n\t}\n\n\twork_done = xenvif_tx_submit(queue);\n\n\treturn work_done;\n}\n\nstatic void xenvif_idx_release(struct xenvif_queue *queue, u16 pending_idx,\n\t\t\t       u8 status)\n{\n\tstruct pending_tx_info *pending_tx_info;\n\tpending_ring_idx_t index;\n\tunsigned long flags;\n\n\tpending_tx_info = &queue->pending_tx_info[pending_idx];\n\n\tspin_lock_irqsave(&queue->response_lock, flags);\n\n\tmake_tx_response(queue, &pending_tx_info->req,\n\t\t\t pending_tx_info->extra_count, status);\n\n\t \n\tindex = pending_index(queue->pending_prod++);\n\tqueue->pending_ring[index] = pending_idx;\n\n\tpush_tx_responses(queue);\n\n\tspin_unlock_irqrestore(&queue->response_lock, flags);\n}\n\n\nstatic void make_tx_response(struct xenvif_queue *queue,\n\t\t\t     struct xen_netif_tx_request *txp,\n\t\t\t     unsigned int extra_count,\n\t\t\t     s8       st)\n{\n\tRING_IDX i = queue->tx.rsp_prod_pvt;\n\tstruct xen_netif_tx_response *resp;\n\n\tresp = RING_GET_RESPONSE(&queue->tx, i);\n\tresp->id     = txp->id;\n\tresp->status = st;\n\n\twhile (extra_count-- != 0)\n\t\tRING_GET_RESPONSE(&queue->tx, ++i)->status = XEN_NETIF_RSP_NULL;\n\n\tqueue->tx.rsp_prod_pvt = ++i;\n}\n\nstatic void push_tx_responses(struct xenvif_queue *queue)\n{\n\tint notify;\n\n\tRING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&queue->tx, notify);\n\tif (notify)\n\t\tnotify_remote_via_irq(queue->tx_irq);\n}\n\nstatic void xenvif_idx_unmap(struct xenvif_queue *queue, u16 pending_idx)\n{\n\tint ret;\n\tstruct gnttab_unmap_grant_ref tx_unmap_op;\n\n\tgnttab_set_unmap_op(&tx_unmap_op,\n\t\t\t    idx_to_kaddr(queue, pending_idx),\n\t\t\t    GNTMAP_host_map,\n\t\t\t    queue->grant_tx_handle[pending_idx]);\n\txenvif_grant_handle_reset(queue, pending_idx);\n\n\tret = gnttab_unmap_refs(&tx_unmap_op, NULL,\n\t\t\t\t&queue->mmap_pages[pending_idx], 1);\n\tif (ret) {\n\t\tnetdev_err(queue->vif->dev,\n\t\t\t   \"Unmap fail: ret: %d pending_idx: %d host_addr: %llx handle: 0x%x status: %d\\n\",\n\t\t\t   ret,\n\t\t\t   pending_idx,\n\t\t\t   tx_unmap_op.host_addr,\n\t\t\t   tx_unmap_op.handle,\n\t\t\t   tx_unmap_op.status);\n\t\tBUG();\n\t}\n}\n\nstatic inline int tx_work_todo(struct xenvif_queue *queue)\n{\n\tif (likely(RING_HAS_UNCONSUMED_REQUESTS(&queue->tx)))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic inline bool tx_dealloc_work_todo(struct xenvif_queue *queue)\n{\n\treturn queue->dealloc_cons != queue->dealloc_prod;\n}\n\nvoid xenvif_unmap_frontend_data_rings(struct xenvif_queue *queue)\n{\n\tif (queue->tx.sring)\n\t\txenbus_unmap_ring_vfree(xenvif_to_xenbus_device(queue->vif),\n\t\t\t\t\tqueue->tx.sring);\n\tif (queue->rx.sring)\n\t\txenbus_unmap_ring_vfree(xenvif_to_xenbus_device(queue->vif),\n\t\t\t\t\tqueue->rx.sring);\n}\n\nint xenvif_map_frontend_data_rings(struct xenvif_queue *queue,\n\t\t\t\t   grant_ref_t tx_ring_ref,\n\t\t\t\t   grant_ref_t rx_ring_ref)\n{\n\tvoid *addr;\n\tstruct xen_netif_tx_sring *txs;\n\tstruct xen_netif_rx_sring *rxs;\n\tRING_IDX rsp_prod, req_prod;\n\tint err;\n\n\terr = xenbus_map_ring_valloc(xenvif_to_xenbus_device(queue->vif),\n\t\t\t\t     &tx_ring_ref, 1, &addr);\n\tif (err)\n\t\tgoto err;\n\n\ttxs = (struct xen_netif_tx_sring *)addr;\n\trsp_prod = READ_ONCE(txs->rsp_prod);\n\treq_prod = READ_ONCE(txs->req_prod);\n\n\tBACK_RING_ATTACH(&queue->tx, txs, rsp_prod, XEN_PAGE_SIZE);\n\n\terr = -EIO;\n\tif (req_prod - rsp_prod > RING_SIZE(&queue->tx))\n\t\tgoto err;\n\n\terr = xenbus_map_ring_valloc(xenvif_to_xenbus_device(queue->vif),\n\t\t\t\t     &rx_ring_ref, 1, &addr);\n\tif (err)\n\t\tgoto err;\n\n\trxs = (struct xen_netif_rx_sring *)addr;\n\trsp_prod = READ_ONCE(rxs->rsp_prod);\n\treq_prod = READ_ONCE(rxs->req_prod);\n\n\tBACK_RING_ATTACH(&queue->rx, rxs, rsp_prod, XEN_PAGE_SIZE);\n\n\terr = -EIO;\n\tif (req_prod - rsp_prod > RING_SIZE(&queue->rx))\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\txenvif_unmap_frontend_data_rings(queue);\n\treturn err;\n}\n\nstatic bool xenvif_dealloc_kthread_should_stop(struct xenvif_queue *queue)\n{\n\t \n\treturn kthread_should_stop() &&\n\t\t!atomic_read(&queue->inflight_packets);\n}\n\nint xenvif_dealloc_kthread(void *data)\n{\n\tstruct xenvif_queue *queue = data;\n\n\tfor (;;) {\n\t\twait_event_interruptible(queue->dealloc_wq,\n\t\t\t\t\t tx_dealloc_work_todo(queue) ||\n\t\t\t\t\t xenvif_dealloc_kthread_should_stop(queue));\n\t\tif (xenvif_dealloc_kthread_should_stop(queue))\n\t\t\tbreak;\n\n\t\txenvif_tx_dealloc_action(queue);\n\t\tcond_resched();\n\t}\n\n\t \n\tif (tx_dealloc_work_todo(queue))\n\t\txenvif_tx_dealloc_action(queue);\n\n\treturn 0;\n}\n\nstatic void make_ctrl_response(struct xenvif *vif,\n\t\t\t       const struct xen_netif_ctrl_request *req,\n\t\t\t       u32 status, u32 data)\n{\n\tRING_IDX idx = vif->ctrl.rsp_prod_pvt;\n\tstruct xen_netif_ctrl_response rsp = {\n\t\t.id = req->id,\n\t\t.type = req->type,\n\t\t.status = status,\n\t\t.data = data,\n\t};\n\n\t*RING_GET_RESPONSE(&vif->ctrl, idx) = rsp;\n\tvif->ctrl.rsp_prod_pvt = ++idx;\n}\n\nstatic void push_ctrl_response(struct xenvif *vif)\n{\n\tint notify;\n\n\tRING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&vif->ctrl, notify);\n\tif (notify)\n\t\tnotify_remote_via_irq(vif->ctrl_irq);\n}\n\nstatic void process_ctrl_request(struct xenvif *vif,\n\t\t\t\t const struct xen_netif_ctrl_request *req)\n{\n\tu32 status = XEN_NETIF_CTRL_STATUS_NOT_SUPPORTED;\n\tu32 data = 0;\n\n\tswitch (req->type) {\n\tcase XEN_NETIF_CTRL_TYPE_SET_HASH_ALGORITHM:\n\t\tstatus = xenvif_set_hash_alg(vif, req->data[0]);\n\t\tbreak;\n\n\tcase XEN_NETIF_CTRL_TYPE_GET_HASH_FLAGS:\n\t\tstatus = xenvif_get_hash_flags(vif, &data);\n\t\tbreak;\n\n\tcase XEN_NETIF_CTRL_TYPE_SET_HASH_FLAGS:\n\t\tstatus = xenvif_set_hash_flags(vif, req->data[0]);\n\t\tbreak;\n\n\tcase XEN_NETIF_CTRL_TYPE_SET_HASH_KEY:\n\t\tstatus = xenvif_set_hash_key(vif, req->data[0],\n\t\t\t\t\t     req->data[1]);\n\t\tbreak;\n\n\tcase XEN_NETIF_CTRL_TYPE_GET_HASH_MAPPING_SIZE:\n\t\tstatus = XEN_NETIF_CTRL_STATUS_SUCCESS;\n\t\tdata = XEN_NETBK_MAX_HASH_MAPPING_SIZE;\n\t\tbreak;\n\n\tcase XEN_NETIF_CTRL_TYPE_SET_HASH_MAPPING_SIZE:\n\t\tstatus = xenvif_set_hash_mapping_size(vif,\n\t\t\t\t\t\t      req->data[0]);\n\t\tbreak;\n\n\tcase XEN_NETIF_CTRL_TYPE_SET_HASH_MAPPING:\n\t\tstatus = xenvif_set_hash_mapping(vif, req->data[0],\n\t\t\t\t\t\t req->data[1],\n\t\t\t\t\t\t req->data[2]);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tmake_ctrl_response(vif, req, status, data);\n\tpush_ctrl_response(vif);\n}\n\nstatic void xenvif_ctrl_action(struct xenvif *vif)\n{\n\tfor (;;) {\n\t\tRING_IDX req_prod, req_cons;\n\n\t\treq_prod = vif->ctrl.sring->req_prod;\n\t\treq_cons = vif->ctrl.req_cons;\n\n\t\t \n\t\trmb();\n\n\t\tif (req_cons == req_prod)\n\t\t\tbreak;\n\n\t\twhile (req_cons != req_prod) {\n\t\t\tstruct xen_netif_ctrl_request req;\n\n\t\t\tRING_COPY_REQUEST(&vif->ctrl, req_cons, &req);\n\t\t\treq_cons++;\n\n\t\t\tprocess_ctrl_request(vif, &req);\n\t\t}\n\n\t\tvif->ctrl.req_cons = req_cons;\n\t\tvif->ctrl.sring->req_event = req_cons + 1;\n\t}\n}\n\nstatic bool xenvif_ctrl_work_todo(struct xenvif *vif)\n{\n\tif (likely(RING_HAS_UNCONSUMED_REQUESTS(&vif->ctrl)))\n\t\treturn true;\n\n\treturn false;\n}\n\nirqreturn_t xenvif_ctrl_irq_fn(int irq, void *data)\n{\n\tstruct xenvif *vif = data;\n\tunsigned int eoi_flag = XEN_EOI_FLAG_SPURIOUS;\n\n\twhile (xenvif_ctrl_work_todo(vif)) {\n\t\txenvif_ctrl_action(vif);\n\t\teoi_flag = 0;\n\t}\n\n\txen_irq_lateeoi(irq, eoi_flag);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int __init netback_init(void)\n{\n\tint rc = 0;\n\n\tif (!xen_domain())\n\t\treturn -ENODEV;\n\n\t \n\tif (xenvif_max_queues == 0)\n\t\txenvif_max_queues = min_t(unsigned int, MAX_QUEUES_DEFAULT,\n\t\t\t\t\t  num_online_cpus());\n\n\tif (fatal_skb_slots < XEN_NETBK_LEGACY_SLOTS_MAX) {\n\t\tpr_info(\"fatal_skb_slots too small (%d), bump it to XEN_NETBK_LEGACY_SLOTS_MAX (%d)\\n\",\n\t\t\tfatal_skb_slots, XEN_NETBK_LEGACY_SLOTS_MAX);\n\t\tfatal_skb_slots = XEN_NETBK_LEGACY_SLOTS_MAX;\n\t}\n\n\trc = xenvif_xenbus_init();\n\tif (rc)\n\t\tgoto failed_init;\n\n#ifdef CONFIG_DEBUG_FS\n\txen_netback_dbg_root = debugfs_create_dir(\"xen-netback\", NULL);\n#endif  \n\n\treturn 0;\n\nfailed_init:\n\treturn rc;\n}\n\nmodule_init(netback_init);\n\nstatic void __exit netback_fini(void)\n{\n#ifdef CONFIG_DEBUG_FS\n\tdebugfs_remove_recursive(xen_netback_dbg_root);\n#endif  \n\txenvif_xenbus_fini();\n}\nmodule_exit(netback_fini);\n\nMODULE_LICENSE(\"Dual BSD/GPL\");\nMODULE_ALIAS(\"xen-backend:vif\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}