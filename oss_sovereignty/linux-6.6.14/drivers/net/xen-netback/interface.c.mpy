{
  "module_name": "interface.c",
  "hash_id": "f3a085d04ba98d67ed7fbc6de7c149132f8db0dd48fe6c0c12c7ac43839a5567",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/xen-netback/interface.c",
  "human_readable_source": " \n\n#include \"common.h\"\n\n#include <linux/kthread.h>\n#include <linux/sched/task.h>\n#include <linux/ethtool.h>\n#include <linux/rtnetlink.h>\n#include <linux/if_vlan.h>\n#include <linux/vmalloc.h>\n\n#include <xen/events.h>\n#include <asm/xen/hypercall.h>\n#include <xen/balloon.h>\n\n \n#define XENVIF_RX_QUEUE_BYTES (XEN_NETIF_RX_RING_SIZE/2 * PAGE_SIZE)\n\n \nvoid xenvif_skb_zerocopy_prepare(struct xenvif_queue *queue,\n\t\t\t\t struct sk_buff *skb)\n{\n\tskb_shinfo(skb)->flags |= SKBFL_ZEROCOPY_ENABLE;\n\tatomic_inc(&queue->inflight_packets);\n}\n\nvoid xenvif_skb_zerocopy_complete(struct xenvif_queue *queue)\n{\n\tatomic_dec(&queue->inflight_packets);\n\n\t \n\twake_up(&queue->dealloc_wq);\n}\n\nstatic int xenvif_schedulable(struct xenvif *vif)\n{\n\treturn netif_running(vif->dev) &&\n\t\ttest_bit(VIF_STATUS_CONNECTED, &vif->status) &&\n\t\t!vif->disabled;\n}\n\nstatic bool xenvif_handle_tx_interrupt(struct xenvif_queue *queue)\n{\n\tbool rc;\n\n\trc = RING_HAS_UNCONSUMED_REQUESTS(&queue->tx);\n\tif (rc)\n\t\tnapi_schedule(&queue->napi);\n\treturn rc;\n}\n\nstatic irqreturn_t xenvif_tx_interrupt(int irq, void *dev_id)\n{\n\tstruct xenvif_queue *queue = dev_id;\n\tint old;\n\n\told = atomic_fetch_or(NETBK_TX_EOI, &queue->eoi_pending);\n\tWARN(old & NETBK_TX_EOI, \"Interrupt while EOI pending\\n\");\n\n\tif (!xenvif_handle_tx_interrupt(queue)) {\n\t\tatomic_andnot(NETBK_TX_EOI, &queue->eoi_pending);\n\t\txen_irq_lateeoi(irq, XEN_EOI_FLAG_SPURIOUS);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int xenvif_poll(struct napi_struct *napi, int budget)\n{\n\tstruct xenvif_queue *queue =\n\t\tcontainer_of(napi, struct xenvif_queue, napi);\n\tint work_done;\n\n\t \n\tif (unlikely(queue->vif->disabled)) {\n\t\tnapi_complete(napi);\n\t\treturn 0;\n\t}\n\n\twork_done = xenvif_tx_action(queue, budget);\n\n\tif (work_done < budget) {\n\t\tnapi_complete_done(napi, work_done);\n\t\t \n\t\tif (likely(!queue->rate_limited))\n\t\t\txenvif_napi_schedule_or_enable_events(queue);\n\t}\n\n\treturn work_done;\n}\n\nstatic bool xenvif_handle_rx_interrupt(struct xenvif_queue *queue)\n{\n\tbool rc;\n\n\trc = xenvif_have_rx_work(queue, false);\n\tif (rc)\n\t\txenvif_kick_thread(queue);\n\treturn rc;\n}\n\nstatic irqreturn_t xenvif_rx_interrupt(int irq, void *dev_id)\n{\n\tstruct xenvif_queue *queue = dev_id;\n\tint old;\n\n\told = atomic_fetch_or(NETBK_RX_EOI, &queue->eoi_pending);\n\tWARN(old & NETBK_RX_EOI, \"Interrupt while EOI pending\\n\");\n\n\tif (!xenvif_handle_rx_interrupt(queue)) {\n\t\tatomic_andnot(NETBK_RX_EOI, &queue->eoi_pending);\n\t\txen_irq_lateeoi(irq, XEN_EOI_FLAG_SPURIOUS);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nirqreturn_t xenvif_interrupt(int irq, void *dev_id)\n{\n\tstruct xenvif_queue *queue = dev_id;\n\tint old;\n\tbool has_rx, has_tx;\n\n\told = atomic_fetch_or(NETBK_COMMON_EOI, &queue->eoi_pending);\n\tWARN(old, \"Interrupt while EOI pending\\n\");\n\n\thas_tx = xenvif_handle_tx_interrupt(queue);\n\thas_rx = xenvif_handle_rx_interrupt(queue);\n\n\tif (!has_rx && !has_tx) {\n\t\tatomic_andnot(NETBK_COMMON_EOI, &queue->eoi_pending);\n\t\txen_irq_lateeoi(irq, XEN_EOI_FLAG_SPURIOUS);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\nstatic u16 xenvif_select_queue(struct net_device *dev, struct sk_buff *skb,\n\t\t\t       struct net_device *sb_dev)\n{\n\tstruct xenvif *vif = netdev_priv(dev);\n\tunsigned int size = vif->hash.size;\n\tunsigned int num_queues;\n\n\t \n\tnum_queues = READ_ONCE(vif->num_queues);\n\tif (num_queues < 1)\n\t\treturn 0;\n\n\tif (vif->hash.alg == XEN_NETIF_CTRL_HASH_ALGORITHM_NONE)\n\t\treturn netdev_pick_tx(dev, skb, NULL) %\n\t\t       dev->real_num_tx_queues;\n\n\txenvif_set_skb_hash(vif, skb);\n\n\tif (size == 0)\n\t\treturn skb_get_hash_raw(skb) % dev->real_num_tx_queues;\n\n\treturn vif->hash.mapping[vif->hash.mapping_sel]\n\t\t\t\t[skb_get_hash_raw(skb) % size];\n}\n\nstatic netdev_tx_t\nxenvif_start_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct xenvif *vif = netdev_priv(dev);\n\tstruct xenvif_queue *queue = NULL;\n\tunsigned int num_queues;\n\tu16 index;\n\tstruct xenvif_rx_cb *cb;\n\n\tBUG_ON(skb->dev != dev);\n\n\t \n\tnum_queues = READ_ONCE(vif->num_queues);\n\tif (num_queues < 1)\n\t\tgoto drop;\n\n\t \n\tindex = skb_get_queue_mapping(skb);\n\tif (index >= num_queues) {\n\t\tpr_warn_ratelimited(\"Invalid queue %hu for packet on interface %s\\n\",\n\t\t\t\t    index, vif->dev->name);\n\t\tindex %= num_queues;\n\t}\n\tqueue = &vif->queues[index];\n\n\t \n\tif (queue->task == NULL ||\n\t    queue->dealloc_task == NULL ||\n\t    !xenvif_schedulable(vif))\n\t\tgoto drop;\n\n\tif (vif->multicast_control && skb->pkt_type == PACKET_MULTICAST) {\n\t\tstruct ethhdr *eth = (struct ethhdr *)skb->data;\n\n\t\tif (!xenvif_mcast_match(vif, eth->h_dest))\n\t\t\tgoto drop;\n\t}\n\n\tcb = XENVIF_RX_CB(skb);\n\tcb->expires = jiffies + vif->drain_timeout;\n\n\t \n\tif (vif->hash.alg == XEN_NETIF_CTRL_HASH_ALGORITHM_NONE)\n\t\tskb_clear_hash(skb);\n\n\tif (!xenvif_rx_queue_tail(queue, skb))\n\t\tgoto drop;\n\n\txenvif_kick_thread(queue);\n\n\treturn NETDEV_TX_OK;\n\n drop:\n\tvif->dev->stats.tx_dropped++;\n\tdev_kfree_skb_any(skb);\n\treturn NETDEV_TX_OK;\n}\n\nstatic struct net_device_stats *xenvif_get_stats(struct net_device *dev)\n{\n\tstruct xenvif *vif = netdev_priv(dev);\n\tstruct xenvif_queue *queue = NULL;\n\tunsigned int num_queues;\n\tu64 rx_bytes = 0;\n\tu64 rx_packets = 0;\n\tu64 tx_bytes = 0;\n\tu64 tx_packets = 0;\n\tunsigned int index;\n\n\trcu_read_lock();\n\tnum_queues = READ_ONCE(vif->num_queues);\n\n\t \n\tfor (index = 0; index < num_queues; ++index) {\n\t\tqueue = &vif->queues[index];\n\t\trx_bytes += queue->stats.rx_bytes;\n\t\trx_packets += queue->stats.rx_packets;\n\t\ttx_bytes += queue->stats.tx_bytes;\n\t\ttx_packets += queue->stats.tx_packets;\n\t}\n\n\trcu_read_unlock();\n\n\tvif->dev->stats.rx_bytes = rx_bytes;\n\tvif->dev->stats.rx_packets = rx_packets;\n\tvif->dev->stats.tx_bytes = tx_bytes;\n\tvif->dev->stats.tx_packets = tx_packets;\n\n\treturn &vif->dev->stats;\n}\n\nstatic void xenvif_up(struct xenvif *vif)\n{\n\tstruct xenvif_queue *queue = NULL;\n\tunsigned int num_queues = vif->num_queues;\n\tunsigned int queue_index;\n\n\tfor (queue_index = 0; queue_index < num_queues; ++queue_index) {\n\t\tqueue = &vif->queues[queue_index];\n\t\tnapi_enable(&queue->napi);\n\t\tenable_irq(queue->tx_irq);\n\t\tif (queue->tx_irq != queue->rx_irq)\n\t\t\tenable_irq(queue->rx_irq);\n\t\txenvif_napi_schedule_or_enable_events(queue);\n\t}\n}\n\nstatic void xenvif_down(struct xenvif *vif)\n{\n\tstruct xenvif_queue *queue = NULL;\n\tunsigned int num_queues = vif->num_queues;\n\tunsigned int queue_index;\n\n\tfor (queue_index = 0; queue_index < num_queues; ++queue_index) {\n\t\tqueue = &vif->queues[queue_index];\n\t\tdisable_irq(queue->tx_irq);\n\t\tif (queue->tx_irq != queue->rx_irq)\n\t\t\tdisable_irq(queue->rx_irq);\n\t\tnapi_disable(&queue->napi);\n\t\tdel_timer_sync(&queue->credit_timeout);\n\t}\n}\n\nstatic int xenvif_open(struct net_device *dev)\n{\n\tstruct xenvif *vif = netdev_priv(dev);\n\tif (test_bit(VIF_STATUS_CONNECTED, &vif->status))\n\t\txenvif_up(vif);\n\tnetif_tx_start_all_queues(dev);\n\treturn 0;\n}\n\nstatic int xenvif_close(struct net_device *dev)\n{\n\tstruct xenvif *vif = netdev_priv(dev);\n\tif (test_bit(VIF_STATUS_CONNECTED, &vif->status))\n\t\txenvif_down(vif);\n\tnetif_tx_stop_all_queues(dev);\n\treturn 0;\n}\n\nstatic int xenvif_change_mtu(struct net_device *dev, int mtu)\n{\n\tstruct xenvif *vif = netdev_priv(dev);\n\tint max = vif->can_sg ? ETH_MAX_MTU - VLAN_ETH_HLEN : ETH_DATA_LEN;\n\n\tif (mtu > max)\n\t\treturn -EINVAL;\n\tdev->mtu = mtu;\n\treturn 0;\n}\n\nstatic netdev_features_t xenvif_fix_features(struct net_device *dev,\n\tnetdev_features_t features)\n{\n\tstruct xenvif *vif = netdev_priv(dev);\n\n\tif (!vif->can_sg)\n\t\tfeatures &= ~NETIF_F_SG;\n\tif (~(vif->gso_mask) & GSO_BIT(TCPV4))\n\t\tfeatures &= ~NETIF_F_TSO;\n\tif (~(vif->gso_mask) & GSO_BIT(TCPV6))\n\t\tfeatures &= ~NETIF_F_TSO6;\n\tif (!vif->ip_csum)\n\t\tfeatures &= ~NETIF_F_IP_CSUM;\n\tif (!vif->ipv6_csum)\n\t\tfeatures &= ~NETIF_F_IPV6_CSUM;\n\n\treturn features;\n}\n\nstatic const struct xenvif_stat {\n\tchar name[ETH_GSTRING_LEN];\n\tu16 offset;\n} xenvif_stats[] = {\n\t{\n\t\t\"rx_gso_checksum_fixup\",\n\t\toffsetof(struct xenvif_stats, rx_gso_checksum_fixup)\n\t},\n\t \n\t{\n\t\t\"tx_zerocopy_sent\",\n\t\toffsetof(struct xenvif_stats, tx_zerocopy_sent),\n\t},\n\t{\n\t\t\"tx_zerocopy_success\",\n\t\toffsetof(struct xenvif_stats, tx_zerocopy_success),\n\t},\n\t{\n\t\t\"tx_zerocopy_fail\",\n\t\toffsetof(struct xenvif_stats, tx_zerocopy_fail)\n\t},\n\t \n\t{\n\t\t\"tx_frag_overflow\",\n\t\toffsetof(struct xenvif_stats, tx_frag_overflow)\n\t},\n};\n\nstatic int xenvif_get_sset_count(struct net_device *dev, int string_set)\n{\n\tswitch (string_set) {\n\tcase ETH_SS_STATS:\n\t\treturn ARRAY_SIZE(xenvif_stats);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic void xenvif_get_ethtool_stats(struct net_device *dev,\n\t\t\t\t     struct ethtool_stats *stats, u64 * data)\n{\n\tstruct xenvif *vif = netdev_priv(dev);\n\tunsigned int num_queues;\n\tint i;\n\tunsigned int queue_index;\n\n\trcu_read_lock();\n\tnum_queues = READ_ONCE(vif->num_queues);\n\n\tfor (i = 0; i < ARRAY_SIZE(xenvif_stats); i++) {\n\t\tunsigned long accum = 0;\n\t\tfor (queue_index = 0; queue_index < num_queues; ++queue_index) {\n\t\t\tvoid *vif_stats = &vif->queues[queue_index].stats;\n\t\t\taccum += *(unsigned long *)(vif_stats + xenvif_stats[i].offset);\n\t\t}\n\t\tdata[i] = accum;\n\t}\n\n\trcu_read_unlock();\n}\n\nstatic void xenvif_get_strings(struct net_device *dev, u32 stringset, u8 * data)\n{\n\tint i;\n\n\tswitch (stringset) {\n\tcase ETH_SS_STATS:\n\t\tfor (i = 0; i < ARRAY_SIZE(xenvif_stats); i++)\n\t\t\tmemcpy(data + i * ETH_GSTRING_LEN,\n\t\t\t       xenvif_stats[i].name, ETH_GSTRING_LEN);\n\t\tbreak;\n\t}\n}\n\nstatic const struct ethtool_ops xenvif_ethtool_ops = {\n\t.get_link\t= ethtool_op_get_link,\n\n\t.get_sset_count = xenvif_get_sset_count,\n\t.get_ethtool_stats = xenvif_get_ethtool_stats,\n\t.get_strings = xenvif_get_strings,\n};\n\nstatic const struct net_device_ops xenvif_netdev_ops = {\n\t.ndo_select_queue = xenvif_select_queue,\n\t.ndo_start_xmit\t= xenvif_start_xmit,\n\t.ndo_get_stats\t= xenvif_get_stats,\n\t.ndo_open\t= xenvif_open,\n\t.ndo_stop\t= xenvif_close,\n\t.ndo_change_mtu\t= xenvif_change_mtu,\n\t.ndo_fix_features = xenvif_fix_features,\n\t.ndo_set_mac_address = eth_mac_addr,\n\t.ndo_validate_addr   = eth_validate_addr,\n};\n\nstruct xenvif *xenvif_alloc(struct device *parent, domid_t domid,\n\t\t\t    unsigned int handle)\n{\n\tstatic const u8 dummy_addr[ETH_ALEN] = {\n\t\t0xfe, 0xff, 0xff, 0xff, 0xff, 0xff,\n\t};\n\tint err;\n\tstruct net_device *dev;\n\tstruct xenvif *vif;\n\tchar name[IFNAMSIZ] = {};\n\n\tsnprintf(name, IFNAMSIZ - 1, \"vif%u.%u\", domid, handle);\n\t \n\tdev = alloc_netdev_mq(sizeof(struct xenvif), name, NET_NAME_UNKNOWN,\n\t\t\t      ether_setup, xenvif_max_queues);\n\tif (dev == NULL) {\n\t\tpr_warn(\"Could not allocate netdev for %s\\n\", name);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tSET_NETDEV_DEV(dev, parent);\n\n\tvif = netdev_priv(dev);\n\n\tvif->domid  = domid;\n\tvif->handle = handle;\n\tvif->can_sg = 1;\n\tvif->ip_csum = 1;\n\tvif->dev = dev;\n\tvif->disabled = false;\n\tvif->drain_timeout = msecs_to_jiffies(rx_drain_timeout_msecs);\n\tvif->stall_timeout = msecs_to_jiffies(rx_stall_timeout_msecs);\n\n\t \n\tvif->queues = NULL;\n\tvif->num_queues = 0;\n\n\tvif->xdp_headroom = 0;\n\n\tspin_lock_init(&vif->lock);\n\tINIT_LIST_HEAD(&vif->fe_mcast_addr);\n\n\tdev->netdev_ops\t= &xenvif_netdev_ops;\n\tdev->hw_features = NETIF_F_SG |\n\t\tNETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |\n\t\tNETIF_F_TSO | NETIF_F_TSO6 | NETIF_F_FRAGLIST;\n\tdev->features = dev->hw_features | NETIF_F_RXCSUM;\n\tdev->ethtool_ops = &xenvif_ethtool_ops;\n\n\tdev->min_mtu = ETH_MIN_MTU;\n\tdev->max_mtu = ETH_MAX_MTU - VLAN_ETH_HLEN;\n\n\t \n\teth_hw_addr_set(dev, dummy_addr);\n\n\tnetif_carrier_off(dev);\n\n\terr = register_netdev(dev);\n\tif (err) {\n\t\tnetdev_warn(dev, \"Could not register device: err=%d\\n\", err);\n\t\tfree_netdev(dev);\n\t\treturn ERR_PTR(err);\n\t}\n\n\tnetdev_dbg(dev, \"Successfully created xenvif\\n\");\n\n\t__module_get(THIS_MODULE);\n\n\treturn vif;\n}\n\nint xenvif_init_queue(struct xenvif_queue *queue)\n{\n\tint err, i;\n\n\tqueue->credit_bytes = queue->remaining_credit = ~0UL;\n\tqueue->credit_usec  = 0UL;\n\ttimer_setup(&queue->credit_timeout, xenvif_tx_credit_callback, 0);\n\tqueue->credit_window_start = get_jiffies_64();\n\n\tqueue->rx_queue_max = XENVIF_RX_QUEUE_BYTES;\n\n\tskb_queue_head_init(&queue->rx_queue);\n\tskb_queue_head_init(&queue->tx_queue);\n\n\tqueue->pending_cons = 0;\n\tqueue->pending_prod = MAX_PENDING_REQS;\n\tfor (i = 0; i < MAX_PENDING_REQS; ++i)\n\t\tqueue->pending_ring[i] = i;\n\n\tspin_lock_init(&queue->callback_lock);\n\tspin_lock_init(&queue->response_lock);\n\n\t \n\terr = gnttab_alloc_pages(MAX_PENDING_REQS,\n\t\t\t\t queue->mmap_pages);\n\tif (err) {\n\t\tnetdev_err(queue->vif->dev, \"Could not reserve mmap_pages\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < MAX_PENDING_REQS; i++) {\n\t\tqueue->pending_tx_info[i].callback_struct = (struct ubuf_info_msgzc)\n\t\t\t{ { .callback = xenvif_zerocopy_callback },\n\t\t\t  { { .ctx = NULL,\n\t\t\t      .desc = i } } };\n\t\tqueue->grant_tx_handle[i] = NETBACK_INVALID_HANDLE;\n\t}\n\n\treturn 0;\n}\n\nvoid xenvif_carrier_on(struct xenvif *vif)\n{\n\trtnl_lock();\n\tif (!vif->can_sg && vif->dev->mtu > ETH_DATA_LEN)\n\t\tdev_set_mtu(vif->dev, ETH_DATA_LEN);\n\tnetdev_update_features(vif->dev);\n\tset_bit(VIF_STATUS_CONNECTED, &vif->status);\n\tif (netif_running(vif->dev))\n\t\txenvif_up(vif);\n\trtnl_unlock();\n}\n\nint xenvif_connect_ctrl(struct xenvif *vif, grant_ref_t ring_ref,\n\t\t\tunsigned int evtchn)\n{\n\tstruct net_device *dev = vif->dev;\n\tstruct xenbus_device *xendev = xenvif_to_xenbus_device(vif);\n\tvoid *addr;\n\tstruct xen_netif_ctrl_sring *shared;\n\tRING_IDX rsp_prod, req_prod;\n\tint err;\n\n\terr = xenbus_map_ring_valloc(xendev, &ring_ref, 1, &addr);\n\tif (err)\n\t\tgoto err;\n\n\tshared = (struct xen_netif_ctrl_sring *)addr;\n\trsp_prod = READ_ONCE(shared->rsp_prod);\n\treq_prod = READ_ONCE(shared->req_prod);\n\n\tBACK_RING_ATTACH(&vif->ctrl, shared, rsp_prod, XEN_PAGE_SIZE);\n\n\terr = -EIO;\n\tif (req_prod - rsp_prod > RING_SIZE(&vif->ctrl))\n\t\tgoto err_unmap;\n\n\terr = bind_interdomain_evtchn_to_irq_lateeoi(xendev, evtchn);\n\tif (err < 0)\n\t\tgoto err_unmap;\n\n\tvif->ctrl_irq = err;\n\n\txenvif_init_hash(vif);\n\n\terr = request_threaded_irq(vif->ctrl_irq, NULL, xenvif_ctrl_irq_fn,\n\t\t\t\t   IRQF_ONESHOT, \"xen-netback-ctrl\", vif);\n\tif (err) {\n\t\tpr_warn(\"Could not setup irq handler for %s\\n\", dev->name);\n\t\tgoto err_deinit;\n\t}\n\n\treturn 0;\n\nerr_deinit:\n\txenvif_deinit_hash(vif);\n\tunbind_from_irqhandler(vif->ctrl_irq, vif);\n\tvif->ctrl_irq = 0;\n\nerr_unmap:\n\txenbus_unmap_ring_vfree(xendev, vif->ctrl.sring);\n\tvif->ctrl.sring = NULL;\n\nerr:\n\treturn err;\n}\n\nstatic void xenvif_disconnect_queue(struct xenvif_queue *queue)\n{\n\tif (queue->task) {\n\t\tkthread_stop(queue->task);\n\t\tput_task_struct(queue->task);\n\t\tqueue->task = NULL;\n\t}\n\n\tif (queue->dealloc_task) {\n\t\tkthread_stop(queue->dealloc_task);\n\t\tqueue->dealloc_task = NULL;\n\t}\n\n\tif (queue->napi.poll) {\n\t\tnetif_napi_del(&queue->napi);\n\t\tqueue->napi.poll = NULL;\n\t}\n\n\tif (queue->tx_irq) {\n\t\tunbind_from_irqhandler(queue->tx_irq, queue);\n\t\tif (queue->tx_irq == queue->rx_irq)\n\t\t\tqueue->rx_irq = 0;\n\t\tqueue->tx_irq = 0;\n\t}\n\n\tif (queue->rx_irq) {\n\t\tunbind_from_irqhandler(queue->rx_irq, queue);\n\t\tqueue->rx_irq = 0;\n\t}\n\n\txenvif_unmap_frontend_data_rings(queue);\n}\n\nint xenvif_connect_data(struct xenvif_queue *queue,\n\t\t\tunsigned long tx_ring_ref,\n\t\t\tunsigned long rx_ring_ref,\n\t\t\tunsigned int tx_evtchn,\n\t\t\tunsigned int rx_evtchn)\n{\n\tstruct xenbus_device *dev = xenvif_to_xenbus_device(queue->vif);\n\tstruct task_struct *task;\n\tint err;\n\n\tBUG_ON(queue->tx_irq);\n\tBUG_ON(queue->task);\n\tBUG_ON(queue->dealloc_task);\n\n\terr = xenvif_map_frontend_data_rings(queue, tx_ring_ref,\n\t\t\t\t\t     rx_ring_ref);\n\tif (err < 0)\n\t\tgoto err;\n\n\tinit_waitqueue_head(&queue->wq);\n\tinit_waitqueue_head(&queue->dealloc_wq);\n\tatomic_set(&queue->inflight_packets, 0);\n\n\tnetif_napi_add(queue->vif->dev, &queue->napi, xenvif_poll);\n\n\tqueue->stalled = true;\n\n\ttask = kthread_run(xenvif_kthread_guest_rx, queue,\n\t\t\t   \"%s-guest-rx\", queue->name);\n\tif (IS_ERR(task))\n\t\tgoto kthread_err;\n\tqueue->task = task;\n\t \n\tget_task_struct(task);\n\n\ttask = kthread_run(xenvif_dealloc_kthread, queue,\n\t\t\t   \"%s-dealloc\", queue->name);\n\tif (IS_ERR(task))\n\t\tgoto kthread_err;\n\tqueue->dealloc_task = task;\n\n\tif (tx_evtchn == rx_evtchn) {\n\t\t \n\t\terr = bind_interdomain_evtchn_to_irqhandler_lateeoi(\n\t\t\tdev, tx_evtchn, xenvif_interrupt, 0,\n\t\t\tqueue->name, queue);\n\t\tif (err < 0)\n\t\t\tgoto err;\n\t\tqueue->tx_irq = queue->rx_irq = err;\n\t\tdisable_irq(queue->tx_irq);\n\t} else {\n\t\t \n\t\tsnprintf(queue->tx_irq_name, sizeof(queue->tx_irq_name),\n\t\t\t \"%s-tx\", queue->name);\n\t\terr = bind_interdomain_evtchn_to_irqhandler_lateeoi(\n\t\t\tdev, tx_evtchn, xenvif_tx_interrupt, 0,\n\t\t\tqueue->tx_irq_name, queue);\n\t\tif (err < 0)\n\t\t\tgoto err;\n\t\tqueue->tx_irq = err;\n\t\tdisable_irq(queue->tx_irq);\n\n\t\tsnprintf(queue->rx_irq_name, sizeof(queue->rx_irq_name),\n\t\t\t \"%s-rx\", queue->name);\n\t\terr = bind_interdomain_evtchn_to_irqhandler_lateeoi(\n\t\t\tdev, rx_evtchn, xenvif_rx_interrupt, 0,\n\t\t\tqueue->rx_irq_name, queue);\n\t\tif (err < 0)\n\t\t\tgoto err;\n\t\tqueue->rx_irq = err;\n\t\tdisable_irq(queue->rx_irq);\n\t}\n\n\treturn 0;\n\nkthread_err:\n\tpr_warn(\"Could not allocate kthread for %s\\n\", queue->name);\n\terr = PTR_ERR(task);\nerr:\n\txenvif_disconnect_queue(queue);\n\treturn err;\n}\n\nvoid xenvif_carrier_off(struct xenvif *vif)\n{\n\tstruct net_device *dev = vif->dev;\n\n\trtnl_lock();\n\tif (test_and_clear_bit(VIF_STATUS_CONNECTED, &vif->status)) {\n\t\tnetif_carrier_off(dev);  \n\t\tif (netif_running(dev))\n\t\t\txenvif_down(vif);\n\t}\n\trtnl_unlock();\n}\n\nvoid xenvif_disconnect_data(struct xenvif *vif)\n{\n\tstruct xenvif_queue *queue = NULL;\n\tunsigned int num_queues = vif->num_queues;\n\tunsigned int queue_index;\n\n\txenvif_carrier_off(vif);\n\n\tfor (queue_index = 0; queue_index < num_queues; ++queue_index) {\n\t\tqueue = &vif->queues[queue_index];\n\n\t\txenvif_disconnect_queue(queue);\n\t}\n\n\txenvif_mcast_addr_list_free(vif);\n}\n\nvoid xenvif_disconnect_ctrl(struct xenvif *vif)\n{\n\tif (vif->ctrl_irq) {\n\t\txenvif_deinit_hash(vif);\n\t\tunbind_from_irqhandler(vif->ctrl_irq, vif);\n\t\tvif->ctrl_irq = 0;\n\t}\n\n\tif (vif->ctrl.sring) {\n\t\txenbus_unmap_ring_vfree(xenvif_to_xenbus_device(vif),\n\t\t\t\t\tvif->ctrl.sring);\n\t\tvif->ctrl.sring = NULL;\n\t}\n}\n\n \nvoid xenvif_deinit_queue(struct xenvif_queue *queue)\n{\n\tgnttab_free_pages(MAX_PENDING_REQS, queue->mmap_pages);\n}\n\nvoid xenvif_free(struct xenvif *vif)\n{\n\tstruct xenvif_queue *queues = vif->queues;\n\tunsigned int num_queues = vif->num_queues;\n\tunsigned int queue_index;\n\n\tunregister_netdev(vif->dev);\n\tfree_netdev(vif->dev);\n\n\tfor (queue_index = 0; queue_index < num_queues; ++queue_index)\n\t\txenvif_deinit_queue(&queues[queue_index]);\n\tvfree(queues);\n\n\tmodule_put(THIS_MODULE);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}