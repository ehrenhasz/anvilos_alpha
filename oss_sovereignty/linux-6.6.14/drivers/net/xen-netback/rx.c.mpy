{
  "module_name": "rx.c",
  "hash_id": "9ec337a283bd100eba09e97015f4c2b4451de56f3648e0c335d1be42f854820c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/xen-netback/rx.c",
  "human_readable_source": " \n#include \"common.h\"\n\n#include <linux/kthread.h>\n\n#include <xen/xen.h>\n#include <xen/events.h>\n\n \nstatic void xenvif_update_needed_slots(struct xenvif_queue *queue,\n\t\t\t\t       const struct sk_buff *skb)\n{\n\tunsigned int needed = 0;\n\n\tif (skb) {\n\t\tneeded = DIV_ROUND_UP(skb->len, XEN_PAGE_SIZE);\n\t\tif (skb_is_gso(skb))\n\t\t\tneeded++;\n\t\tif (skb->sw_hash)\n\t\t\tneeded++;\n\t}\n\n\tWRITE_ONCE(queue->rx_slots_needed, needed);\n}\n\nstatic bool xenvif_rx_ring_slots_available(struct xenvif_queue *queue)\n{\n\tRING_IDX prod, cons;\n\tunsigned int needed;\n\n\tneeded = READ_ONCE(queue->rx_slots_needed);\n\tif (!needed)\n\t\treturn false;\n\n\tdo {\n\t\tprod = queue->rx.sring->req_prod;\n\t\tcons = queue->rx.req_cons;\n\n\t\tif (prod - cons >= needed)\n\t\t\treturn true;\n\n\t\tqueue->rx.sring->req_event = prod + 1;\n\n\t\t \n\t\tmb();\n\t} while (queue->rx.sring->req_prod != prod);\n\n\treturn false;\n}\n\nbool xenvif_rx_queue_tail(struct xenvif_queue *queue, struct sk_buff *skb)\n{\n\tunsigned long flags;\n\tbool ret = true;\n\n\tspin_lock_irqsave(&queue->rx_queue.lock, flags);\n\n\tif (queue->rx_queue_len >= queue->rx_queue_max) {\n\t\tstruct net_device *dev = queue->vif->dev;\n\n\t\tnetif_tx_stop_queue(netdev_get_tx_queue(dev, queue->id));\n\t\tret = false;\n\t} else {\n\t\tif (skb_queue_empty(&queue->rx_queue))\n\t\t\txenvif_update_needed_slots(queue, skb);\n\n\t\t__skb_queue_tail(&queue->rx_queue, skb);\n\n\t\tqueue->rx_queue_len += skb->len;\n\t}\n\n\tspin_unlock_irqrestore(&queue->rx_queue.lock, flags);\n\n\treturn ret;\n}\n\nstatic struct sk_buff *xenvif_rx_dequeue(struct xenvif_queue *queue)\n{\n\tstruct sk_buff *skb;\n\n\tspin_lock_irq(&queue->rx_queue.lock);\n\n\tskb = __skb_dequeue(&queue->rx_queue);\n\tif (skb) {\n\t\txenvif_update_needed_slots(queue, skb_peek(&queue->rx_queue));\n\n\t\tqueue->rx_queue_len -= skb->len;\n\t\tif (queue->rx_queue_len < queue->rx_queue_max) {\n\t\t\tstruct netdev_queue *txq;\n\n\t\t\ttxq = netdev_get_tx_queue(queue->vif->dev, queue->id);\n\t\t\tnetif_tx_wake_queue(txq);\n\t\t}\n\t}\n\n\tspin_unlock_irq(&queue->rx_queue.lock);\n\n\treturn skb;\n}\n\nstatic void xenvif_rx_queue_purge(struct xenvif_queue *queue)\n{\n\tstruct sk_buff *skb;\n\n\twhile ((skb = xenvif_rx_dequeue(queue)) != NULL)\n\t\tkfree_skb(skb);\n}\n\nstatic void xenvif_rx_queue_drop_expired(struct xenvif_queue *queue)\n{\n\tstruct sk_buff *skb;\n\n\tfor (;;) {\n\t\tskb = skb_peek(&queue->rx_queue);\n\t\tif (!skb)\n\t\t\tbreak;\n\t\tif (time_before(jiffies, XENVIF_RX_CB(skb)->expires))\n\t\t\tbreak;\n\t\txenvif_rx_dequeue(queue);\n\t\tkfree_skb(skb);\n\t\tqueue->vif->dev->stats.rx_dropped++;\n\t}\n}\n\nstatic void xenvif_rx_copy_flush(struct xenvif_queue *queue)\n{\n\tunsigned int i;\n\tint notify;\n\n\tgnttab_batch_copy(queue->rx_copy.op, queue->rx_copy.num);\n\n\tfor (i = 0; i < queue->rx_copy.num; i++) {\n\t\tstruct gnttab_copy *op;\n\n\t\top = &queue->rx_copy.op[i];\n\n\t\t \n\t\tif (unlikely(op->status != GNTST_okay)) {\n\t\t\tstruct xen_netif_rx_response *rsp;\n\n\t\t\trsp = RING_GET_RESPONSE(&queue->rx,\n\t\t\t\t\t\tqueue->rx_copy.idx[i]);\n\t\t\trsp->status = op->status;\n\t\t}\n\t}\n\n\tqueue->rx_copy.num = 0;\n\n\t \n\tRING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&queue->rx, notify);\n\tif (notify)\n\t\tnotify_remote_via_irq(queue->rx_irq);\n\n\t__skb_queue_purge(queue->rx_copy.completed);\n}\n\nstatic void xenvif_rx_copy_add(struct xenvif_queue *queue,\n\t\t\t       struct xen_netif_rx_request *req,\n\t\t\t       unsigned int offset, void *data, size_t len)\n{\n\tstruct gnttab_copy *op;\n\tstruct page *page;\n\tstruct xen_page_foreign *foreign;\n\n\tif (queue->rx_copy.num == COPY_BATCH_SIZE)\n\t\txenvif_rx_copy_flush(queue);\n\n\top = &queue->rx_copy.op[queue->rx_copy.num];\n\n\tpage = virt_to_page(data);\n\n\top->flags = GNTCOPY_dest_gref;\n\n\tforeign = xen_page_foreign(page);\n\tif (foreign) {\n\t\top->source.domid = foreign->domid;\n\t\top->source.u.ref = foreign->gref;\n\t\top->flags |= GNTCOPY_source_gref;\n\t} else {\n\t\top->source.u.gmfn = virt_to_gfn(data);\n\t\top->source.domid  = DOMID_SELF;\n\t}\n\n\top->source.offset = xen_offset_in_page(data);\n\top->dest.u.ref    = req->gref;\n\top->dest.domid    = queue->vif->domid;\n\top->dest.offset   = offset;\n\top->len           = len;\n\n\tqueue->rx_copy.idx[queue->rx_copy.num] = queue->rx.req_cons;\n\tqueue->rx_copy.num++;\n}\n\nstatic unsigned int xenvif_gso_type(struct sk_buff *skb)\n{\n\tif (skb_is_gso(skb)) {\n\t\tif (skb_shinfo(skb)->gso_type & SKB_GSO_TCPV4)\n\t\t\treturn XEN_NETIF_GSO_TYPE_TCPV4;\n\t\telse\n\t\t\treturn XEN_NETIF_GSO_TYPE_TCPV6;\n\t}\n\treturn XEN_NETIF_GSO_TYPE_NONE;\n}\n\nstruct xenvif_pkt_state {\n\tstruct sk_buff *skb;\n\tsize_t remaining_len;\n\tstruct sk_buff *frag_iter;\n\tint frag;  \n\tunsigned int frag_offset;\n\tstruct xen_netif_extra_info extras[XEN_NETIF_EXTRA_TYPE_MAX - 1];\n\tunsigned int extra_count;\n\tunsigned int slot;\n};\n\nstatic void xenvif_rx_next_skb(struct xenvif_queue *queue,\n\t\t\t       struct xenvif_pkt_state *pkt)\n{\n\tstruct sk_buff *skb;\n\tunsigned int gso_type;\n\n\tskb = xenvif_rx_dequeue(queue);\n\n\tqueue->stats.tx_bytes += skb->len;\n\tqueue->stats.tx_packets++;\n\n\t \n\tmemset(pkt, 0, sizeof(struct xenvif_pkt_state));\n\n\tpkt->skb = skb;\n\tpkt->frag_iter = skb;\n\tpkt->remaining_len = skb->len;\n\tpkt->frag = -1;\n\n\tgso_type = xenvif_gso_type(skb);\n\tif ((1 << gso_type) & queue->vif->gso_mask) {\n\t\tstruct xen_netif_extra_info *extra;\n\n\t\textra = &pkt->extras[XEN_NETIF_EXTRA_TYPE_GSO - 1];\n\n\t\textra->u.gso.type = gso_type;\n\t\textra->u.gso.size = skb_shinfo(skb)->gso_size;\n\t\textra->u.gso.pad = 0;\n\t\textra->u.gso.features = 0;\n\t\textra->type = XEN_NETIF_EXTRA_TYPE_GSO;\n\t\textra->flags = 0;\n\n\t\tpkt->extra_count++;\n\t}\n\n\tif (queue->vif->xdp_headroom) {\n\t\tstruct xen_netif_extra_info *extra;\n\n\t\textra = &pkt->extras[XEN_NETIF_EXTRA_TYPE_XDP - 1];\n\n\t\tmemset(extra, 0, sizeof(struct xen_netif_extra_info));\n\t\textra->u.xdp.headroom = queue->vif->xdp_headroom;\n\t\textra->type = XEN_NETIF_EXTRA_TYPE_XDP;\n\t\textra->flags = 0;\n\n\t\tpkt->extra_count++;\n\t}\n\n\tif (skb->sw_hash) {\n\t\tstruct xen_netif_extra_info *extra;\n\n\t\textra = &pkt->extras[XEN_NETIF_EXTRA_TYPE_HASH - 1];\n\n\t\textra->u.hash.algorithm =\n\t\t\tXEN_NETIF_CTRL_HASH_ALGORITHM_TOEPLITZ;\n\n\t\tif (skb->l4_hash)\n\t\t\textra->u.hash.type =\n\t\t\t\tskb->protocol == htons(ETH_P_IP) ?\n\t\t\t\t_XEN_NETIF_CTRL_HASH_TYPE_IPV4_TCP :\n\t\t\t\t_XEN_NETIF_CTRL_HASH_TYPE_IPV6_TCP;\n\t\telse\n\t\t\textra->u.hash.type =\n\t\t\t\tskb->protocol == htons(ETH_P_IP) ?\n\t\t\t\t_XEN_NETIF_CTRL_HASH_TYPE_IPV4 :\n\t\t\t\t_XEN_NETIF_CTRL_HASH_TYPE_IPV6;\n\n\t\t*(uint32_t *)extra->u.hash.value = skb_get_hash_raw(skb);\n\n\t\textra->type = XEN_NETIF_EXTRA_TYPE_HASH;\n\t\textra->flags = 0;\n\n\t\tpkt->extra_count++;\n\t}\n}\n\nstatic void xenvif_rx_complete(struct xenvif_queue *queue,\n\t\t\t       struct xenvif_pkt_state *pkt)\n{\n\t \n\tqueue->rx.rsp_prod_pvt = queue->rx.req_cons;\n\n\t__skb_queue_tail(queue->rx_copy.completed, pkt->skb);\n}\n\nstatic void xenvif_rx_next_frag(struct xenvif_pkt_state *pkt)\n{\n\tstruct sk_buff *frag_iter = pkt->frag_iter;\n\tunsigned int nr_frags = skb_shinfo(frag_iter)->nr_frags;\n\n\tpkt->frag++;\n\tpkt->frag_offset = 0;\n\n\tif (pkt->frag >= nr_frags) {\n\t\tif (frag_iter == pkt->skb)\n\t\t\tpkt->frag_iter = skb_shinfo(frag_iter)->frag_list;\n\t\telse\n\t\t\tpkt->frag_iter = frag_iter->next;\n\n\t\tpkt->frag = -1;\n\t}\n}\n\nstatic void xenvif_rx_next_chunk(struct xenvif_queue *queue,\n\t\t\t\t struct xenvif_pkt_state *pkt,\n\t\t\t\t unsigned int offset, void **data,\n\t\t\t\t size_t *len)\n{\n\tstruct sk_buff *frag_iter = pkt->frag_iter;\n\tvoid *frag_data;\n\tsize_t frag_len, chunk_len;\n\n\tBUG_ON(!frag_iter);\n\n\tif (pkt->frag == -1) {\n\t\tfrag_data = frag_iter->data;\n\t\tfrag_len = skb_headlen(frag_iter);\n\t} else {\n\t\tskb_frag_t *frag = &skb_shinfo(frag_iter)->frags[pkt->frag];\n\n\t\tfrag_data = skb_frag_address(frag);\n\t\tfrag_len = skb_frag_size(frag);\n\t}\n\n\tfrag_data += pkt->frag_offset;\n\tfrag_len -= pkt->frag_offset;\n\n\tchunk_len = min_t(size_t, frag_len, XEN_PAGE_SIZE - offset);\n\tchunk_len = min_t(size_t, chunk_len, XEN_PAGE_SIZE -\n\t\t\t\t\t     xen_offset_in_page(frag_data));\n\n\tpkt->frag_offset += chunk_len;\n\n\t \n\tif (frag_len == chunk_len)\n\t\txenvif_rx_next_frag(pkt);\n\n\t*data = frag_data;\n\t*len = chunk_len;\n}\n\nstatic void xenvif_rx_data_slot(struct xenvif_queue *queue,\n\t\t\t\tstruct xenvif_pkt_state *pkt,\n\t\t\t\tstruct xen_netif_rx_request *req,\n\t\t\t\tstruct xen_netif_rx_response *rsp)\n{\n\tunsigned int offset = queue->vif->xdp_headroom;\n\tunsigned int flags;\n\n\tdo {\n\t\tsize_t len;\n\t\tvoid *data;\n\n\t\txenvif_rx_next_chunk(queue, pkt, offset, &data, &len);\n\t\txenvif_rx_copy_add(queue, req, offset, data, len);\n\n\t\toffset += len;\n\t\tpkt->remaining_len -= len;\n\n\t} while (offset < XEN_PAGE_SIZE && pkt->remaining_len > 0);\n\n\tif (pkt->remaining_len > 0)\n\t\tflags = XEN_NETRXF_more_data;\n\telse\n\t\tflags = 0;\n\n\tif (pkt->slot == 0) {\n\t\tstruct sk_buff *skb = pkt->skb;\n\n\t\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\t\tflags |= XEN_NETRXF_csum_blank |\n\t\t\t\t XEN_NETRXF_data_validated;\n\t\telse if (skb->ip_summed == CHECKSUM_UNNECESSARY)\n\t\t\tflags |= XEN_NETRXF_data_validated;\n\n\t\tif (pkt->extra_count != 0)\n\t\t\tflags |= XEN_NETRXF_extra_info;\n\t}\n\n\trsp->offset = 0;\n\trsp->flags = flags;\n\trsp->id = req->id;\n\trsp->status = (s16)offset;\n}\n\nstatic void xenvif_rx_extra_slot(struct xenvif_queue *queue,\n\t\t\t\t struct xenvif_pkt_state *pkt,\n\t\t\t\t struct xen_netif_rx_request *req,\n\t\t\t\t struct xen_netif_rx_response *rsp)\n{\n\tstruct xen_netif_extra_info *extra = (void *)rsp;\n\tunsigned int i;\n\n\tpkt->extra_count--;\n\n\tfor (i = 0; i < ARRAY_SIZE(pkt->extras); i++) {\n\t\tif (pkt->extras[i].type) {\n\t\t\t*extra = pkt->extras[i];\n\n\t\t\tif (pkt->extra_count != 0)\n\t\t\t\textra->flags |= XEN_NETIF_EXTRA_FLAG_MORE;\n\n\t\t\tpkt->extras[i].type = 0;\n\t\t\treturn;\n\t\t}\n\t}\n\tBUG();\n}\n\nstatic void xenvif_rx_skb(struct xenvif_queue *queue)\n{\n\tstruct xenvif_pkt_state pkt;\n\n\txenvif_rx_next_skb(queue, &pkt);\n\n\tqueue->last_rx_time = jiffies;\n\n\tdo {\n\t\tstruct xen_netif_rx_request *req;\n\t\tstruct xen_netif_rx_response *rsp;\n\n\t\treq = RING_GET_REQUEST(&queue->rx, queue->rx.req_cons);\n\t\trsp = RING_GET_RESPONSE(&queue->rx, queue->rx.req_cons);\n\n\t\t \n\t\tif (pkt.slot != 0 && pkt.extra_count != 0)\n\t\t\txenvif_rx_extra_slot(queue, &pkt, req, rsp);\n\t\telse\n\t\t\txenvif_rx_data_slot(queue, &pkt, req, rsp);\n\n\t\tqueue->rx.req_cons++;\n\t\tpkt.slot++;\n\t} while (pkt.remaining_len > 0 || pkt.extra_count != 0);\n\n\txenvif_rx_complete(queue, &pkt);\n}\n\n#define RX_BATCH_SIZE 64\n\nstatic void xenvif_rx_action(struct xenvif_queue *queue)\n{\n\tstruct sk_buff_head completed_skbs;\n\tunsigned int work_done = 0;\n\n\t__skb_queue_head_init(&completed_skbs);\n\tqueue->rx_copy.completed = &completed_skbs;\n\n\twhile (xenvif_rx_ring_slots_available(queue) &&\n\t       !skb_queue_empty(&queue->rx_queue) &&\n\t       work_done < RX_BATCH_SIZE) {\n\t\txenvif_rx_skb(queue);\n\t\twork_done++;\n\t}\n\n\t \n\txenvif_rx_copy_flush(queue);\n}\n\nstatic RING_IDX xenvif_rx_queue_slots(const struct xenvif_queue *queue)\n{\n\tRING_IDX prod, cons;\n\n\tprod = queue->rx.sring->req_prod;\n\tcons = queue->rx.req_cons;\n\n\treturn prod - cons;\n}\n\nstatic bool xenvif_rx_queue_stalled(const struct xenvif_queue *queue)\n{\n\tunsigned int needed = READ_ONCE(queue->rx_slots_needed);\n\n\treturn !queue->stalled &&\n\t\txenvif_rx_queue_slots(queue) < needed &&\n\t\ttime_after(jiffies,\n\t\t\t   queue->last_rx_time + queue->vif->stall_timeout);\n}\n\nstatic bool xenvif_rx_queue_ready(struct xenvif_queue *queue)\n{\n\tunsigned int needed = READ_ONCE(queue->rx_slots_needed);\n\n\treturn queue->stalled && xenvif_rx_queue_slots(queue) >= needed;\n}\n\nbool xenvif_have_rx_work(struct xenvif_queue *queue, bool test_kthread)\n{\n\treturn xenvif_rx_ring_slots_available(queue) ||\n\t\t(queue->vif->stall_timeout &&\n\t\t (xenvif_rx_queue_stalled(queue) ||\n\t\t  xenvif_rx_queue_ready(queue))) ||\n\t\t(test_kthread && kthread_should_stop()) ||\n\t\tqueue->vif->disabled;\n}\n\nstatic long xenvif_rx_queue_timeout(struct xenvif_queue *queue)\n{\n\tstruct sk_buff *skb;\n\tlong timeout;\n\n\tskb = skb_peek(&queue->rx_queue);\n\tif (!skb)\n\t\treturn MAX_SCHEDULE_TIMEOUT;\n\n\ttimeout = XENVIF_RX_CB(skb)->expires - jiffies;\n\treturn timeout < 0 ? 0 : timeout;\n}\n\n \nstatic void xenvif_wait_for_rx_work(struct xenvif_queue *queue)\n{\n\tDEFINE_WAIT(wait);\n\n\tif (xenvif_have_rx_work(queue, true))\n\t\treturn;\n\n\tfor (;;) {\n\t\tlong ret;\n\n\t\tprepare_to_wait(&queue->wq, &wait, TASK_INTERRUPTIBLE);\n\t\tif (xenvif_have_rx_work(queue, true))\n\t\t\tbreak;\n\t\tif (atomic_fetch_andnot(NETBK_RX_EOI | NETBK_COMMON_EOI,\n\t\t\t\t\t&queue->eoi_pending) &\n\t\t    (NETBK_RX_EOI | NETBK_COMMON_EOI))\n\t\t\txen_irq_lateeoi(queue->rx_irq, 0);\n\n\t\tret = schedule_timeout(xenvif_rx_queue_timeout(queue));\n\t\tif (!ret)\n\t\t\tbreak;\n\t}\n\tfinish_wait(&queue->wq, &wait);\n}\n\nstatic void xenvif_queue_carrier_off(struct xenvif_queue *queue)\n{\n\tstruct xenvif *vif = queue->vif;\n\n\tqueue->stalled = true;\n\n\t \n\tspin_lock(&vif->lock);\n\tif (vif->stalled_queues++ == 0) {\n\t\tnetdev_info(vif->dev, \"Guest Rx stalled\");\n\t\tnetif_carrier_off(vif->dev);\n\t}\n\tspin_unlock(&vif->lock);\n}\n\nstatic void xenvif_queue_carrier_on(struct xenvif_queue *queue)\n{\n\tstruct xenvif *vif = queue->vif;\n\n\tqueue->last_rx_time = jiffies;  \n\tqueue->stalled = false;\n\n\t \n\tspin_lock(&vif->lock);\n\tif (--vif->stalled_queues == 0) {\n\t\tnetdev_info(vif->dev, \"Guest Rx ready\");\n\t\tnetif_carrier_on(vif->dev);\n\t}\n\tspin_unlock(&vif->lock);\n}\n\nint xenvif_kthread_guest_rx(void *data)\n{\n\tstruct xenvif_queue *queue = data;\n\tstruct xenvif *vif = queue->vif;\n\n\tif (!vif->stall_timeout)\n\t\txenvif_queue_carrier_on(queue);\n\n\tfor (;;) {\n\t\txenvif_wait_for_rx_work(queue);\n\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\t \n\t\tif (unlikely(vif->disabled && queue->id == 0)) {\n\t\t\txenvif_carrier_off(vif);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!skb_queue_empty(&queue->rx_queue))\n\t\t\txenvif_rx_action(queue);\n\n\t\t \n\t\tif (vif->stall_timeout) {\n\t\t\tif (xenvif_rx_queue_stalled(queue))\n\t\t\t\txenvif_queue_carrier_off(queue);\n\t\t\telse if (xenvif_rx_queue_ready(queue))\n\t\t\t\txenvif_queue_carrier_on(queue);\n\t\t}\n\n\t\t \n\t\txenvif_rx_queue_drop_expired(queue);\n\n\t\tcond_resched();\n\t}\n\n\t \n\txenvif_rx_queue_purge(queue);\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}