{
  "module_name": "internal.h",
  "hash_id": "ecb3bb50b4389623dd4d97cadc6d519292b2495560a7cf41066893cdb2430fec",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wireless/intel/iwlwifi/pcie/internal.h",
  "human_readable_source": " \n \n#ifndef __iwl_trans_int_pcie_h__\n#define __iwl_trans_int_pcie_h__\n\n#include <linux/spinlock.h>\n#include <linux/interrupt.h>\n#include <linux/skbuff.h>\n#include <linux/wait.h>\n#include <linux/pci.h>\n#include <linux/timer.h>\n#include <linux/cpu.h>\n\n#include \"iwl-fh.h\"\n#include \"iwl-csr.h\"\n#include \"iwl-trans.h\"\n#include \"iwl-debug.h\"\n#include \"iwl-io.h\"\n#include \"iwl-op-mode.h\"\n#include \"iwl-drv.h\"\n#include \"queue/tx.h\"\n#include \"iwl-context-info.h\"\n\n \n#define RX_NUM_QUEUES 1\n#define RX_POST_REQ_ALLOC 2\n#define RX_CLAIM_REQ_ALLOC 8\n#define RX_PENDING_WATERMARK 16\n#define FIRST_RX_QUEUE 512\n\nstruct iwl_host_cmd;\n\n \n\n \nstruct iwl_rx_mem_buffer {\n\tdma_addr_t page_dma;\n\tstruct page *page;\n\tstruct list_head list;\n\tu32 offset;\n\tu16 vid;\n\tbool invalid;\n};\n\n \nstruct isr_statistics {\n\tu32 hw;\n\tu32 sw;\n\tu32 err_code;\n\tu32 sch;\n\tu32 alive;\n\tu32 rfkill;\n\tu32 ctkill;\n\tu32 wakeup;\n\tu32 rx;\n\tu32 tx;\n\tu32 unhandled;\n};\n\n \nstruct iwl_rx_transfer_desc {\n\t__le16 rbid;\n\t__le16 reserved[3];\n\t__le64 addr;\n} __packed;\n\n#define IWL_RX_CD_FLAGS_FRAGMENTED\tBIT(0)\n\n \nstruct iwl_rx_completion_desc {\n\t__le32 reserved1;\n\t__le16 rbid;\n\tu8 flags;\n\tu8 reserved2[25];\n} __packed;\n\n \nstruct iwl_rx_completion_desc_bz {\n\t__le16 rbid;\n\tu8 flags;\n\tu8 reserved[1];\n} __packed;\n\n \nstruct iwl_rxq {\n\tint id;\n\tvoid *bd;\n\tdma_addr_t bd_dma;\n\tvoid *used_bd;\n\tdma_addr_t used_bd_dma;\n\tu32 read;\n\tu32 write;\n\tu32 free_count;\n\tu32 used_count;\n\tu32 write_actual;\n\tu32 queue_size;\n\tstruct list_head rx_free;\n\tstruct list_head rx_used;\n\tbool need_update, next_rb_is_fragment;\n\tvoid *rb_stts;\n\tdma_addr_t rb_stts_dma;\n\tspinlock_t lock;\n\tstruct napi_struct napi;\n\tstruct iwl_rx_mem_buffer *queue[RX_QUEUE_SIZE];\n};\n\n \nstruct iwl_rb_allocator {\n\tatomic_t req_pending;\n\tatomic_t req_ready;\n\tstruct list_head rbd_allocated;\n\tstruct list_head rbd_empty;\n\tspinlock_t lock;\n\tstruct workqueue_struct *alloc_wq;\n\tstruct work_struct rx_alloc;\n};\n\n \nstatic inline __le16 iwl_get_closed_rb_stts(struct iwl_trans *trans,\n\t\t\t\t\t    struct iwl_rxq *rxq)\n{\n\tif (trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_AX210) {\n\t\t__le16 *rb_stts = rxq->rb_stts;\n\n\t\treturn READ_ONCE(*rb_stts);\n\t} else {\n\t\tstruct iwl_rb_status *rb_stts = rxq->rb_stts;\n\n\t\treturn READ_ONCE(rb_stts->closed_rb_num);\n\t}\n}\n\n#ifdef CONFIG_IWLWIFI_DEBUGFS\n \nenum iwl_fw_mon_dbgfs_state {\n\tIWL_FW_MON_DBGFS_STATE_CLOSED,\n\tIWL_FW_MON_DBGFS_STATE_OPEN,\n\tIWL_FW_MON_DBGFS_STATE_DISABLED,\n};\n#endif\n\n \nenum iwl_shared_irq_flags {\n\tIWL_SHARED_IRQ_NON_RX\t\t= BIT(0),\n\tIWL_SHARED_IRQ_FIRST_RSS\t= BIT(1),\n};\n\n \nenum iwl_image_response_code {\n\tIWL_IMAGE_RESP_DEF\t\t= 0,\n\tIWL_IMAGE_RESP_SUCCESS\t\t= 1,\n\tIWL_IMAGE_RESP_FAIL\t\t= 2,\n};\n\n \n#ifdef CONFIG_IWLWIFI_DEBUGFS\nstruct cont_rec {\n\tu32 prev_wr_ptr;\n\tu32 prev_wrap_cnt;\n\tu8  state;\n\t \n\tstruct mutex mutex;\n};\n#endif\n\nenum iwl_pcie_fw_reset_state {\n\tFW_RESET_IDLE,\n\tFW_RESET_REQUESTED,\n\tFW_RESET_OK,\n\tFW_RESET_ERROR,\n};\n\n \nenum iwl_pcie_imr_status {\n\tIMR_D2S_IDLE,\n\tIMR_D2S_REQUESTED,\n\tIMR_D2S_COMPLETED,\n\tIMR_D2S_ERROR,\n};\n\n \nstruct iwl_trans_pcie {\n\tstruct iwl_rxq *rxq;\n\tstruct iwl_rx_mem_buffer *rx_pool;\n\tstruct iwl_rx_mem_buffer **global_table;\n\tstruct iwl_rb_allocator rba;\n\tunion {\n\t\tstruct iwl_context_info *ctxt_info;\n\t\tstruct iwl_context_info_gen3 *ctxt_info_gen3;\n\t};\n\tstruct iwl_prph_info *prph_info;\n\tstruct iwl_prph_scratch *prph_scratch;\n\tvoid *iml;\n\tdma_addr_t ctxt_info_dma_addr;\n\tdma_addr_t prph_info_dma_addr;\n\tdma_addr_t prph_scratch_dma_addr;\n\tdma_addr_t iml_dma_addr;\n\tstruct iwl_trans *trans;\n\n\tstruct net_device napi_dev;\n\n\t \n\t__le32 *ict_tbl;\n\tdma_addr_t ict_tbl_dma;\n\tint ict_index;\n\tbool use_ict;\n\tbool is_down, opmode_down;\n\ts8 debug_rfkill;\n\tstruct isr_statistics isr_stats;\n\n\tspinlock_t irq_lock;\n\tstruct mutex mutex;\n\tu32 inta_mask;\n\tu32 scd_base_addr;\n\tstruct iwl_dma_ptr kw;\n\n\t \n\tstruct iwl_dram_regions pnvm_data;\n\tstruct iwl_dram_regions reduced_tables_data;\n\n\tstruct iwl_txq *txq_memory;\n\n\t \n\tstruct pci_dev *pci_dev;\n\tu8 __iomem *hw_base;\n\n\tbool ucode_write_complete;\n\tbool sx_complete;\n\twait_queue_head_t ucode_write_waitq;\n\twait_queue_head_t sx_waitq;\n\n\tu8 n_no_reclaim_cmds;\n\tu8 no_reclaim_cmds[MAX_NO_RECLAIM_CMDS];\n\tu16 num_rx_bufs;\n\n\tenum iwl_amsdu_size rx_buf_size;\n\tbool scd_set_active;\n\tbool pcie_dbg_dumped_once;\n\tu32 rx_page_order;\n\tu32 rx_buf_bytes;\n\tu32 supported_dma_mask;\n\n\t \n\tspinlock_t alloc_page_lock;\n\tstruct page *alloc_page;\n\tu32 alloc_page_used;\n\n\t \n\tspinlock_t reg_lock;\n\tbool cmd_hold_nic_awake;\n\n#ifdef CONFIG_IWLWIFI_DEBUGFS\n\tstruct cont_rec fw_mon_data;\n#endif\n\n\tstruct msix_entry msix_entries[IWL_MAX_RX_HW_QUEUES];\n\tbool msix_enabled;\n\tu8 shared_vec_mask;\n\tu32 alloc_vecs;\n\tu32 def_irq;\n\tu32 fh_init_mask;\n\tu32 hw_init_mask;\n\tu32 fh_mask;\n\tu32 hw_mask;\n\tcpumask_t affinity_mask[IWL_MAX_RX_HW_QUEUES];\n\tu16 tx_cmd_queue_size;\n\tbool in_rescan;\n\n\tvoid *base_rb_stts;\n\tdma_addr_t base_rb_stts_dma;\n\n\tbool fw_reset_handshake;\n\tenum iwl_pcie_fw_reset_state fw_reset_state;\n\twait_queue_head_t fw_reset_waitq;\n\tenum iwl_pcie_imr_status imr_status;\n\twait_queue_head_t imr_waitq;\n\tchar rf_name[32];\n};\n\nstatic inline struct iwl_trans_pcie *\nIWL_TRANS_GET_PCIE_TRANS(struct iwl_trans *trans)\n{\n\treturn (void *)trans->trans_specific;\n}\n\nstatic inline void iwl_pcie_clear_irq(struct iwl_trans *trans, int queue)\n{\n\t \n\tiwl_write32(trans, CSR_MSIX_AUTOMASK_ST_AD, BIT(queue));\n}\n\nstatic inline struct iwl_trans *\niwl_trans_pcie_get_trans(struct iwl_trans_pcie *trans_pcie)\n{\n\treturn container_of((void *)trans_pcie, struct iwl_trans,\n\t\t\t    trans_specific);\n}\n\n \nstruct iwl_trans\n*iwl_trans_pcie_alloc(struct pci_dev *pdev,\n\t\t      const struct pci_device_id *ent,\n\t\t      const struct iwl_cfg_trans_params *cfg_trans);\nvoid iwl_trans_pcie_free(struct iwl_trans *trans);\nvoid iwl_trans_pcie_free_pnvm_dram_regions(struct iwl_dram_regions *dram_regions,\n\t\t\t\t\t   struct device *dev);\n\nbool __iwl_trans_pcie_grab_nic_access(struct iwl_trans *trans);\n#define _iwl_trans_pcie_grab_nic_access(trans)\t\t\t\\\n\t__cond_lock(nic_access_nobh,\t\t\t\t\\\n\t\t    likely(__iwl_trans_pcie_grab_nic_access(trans)))\n\n \nint iwl_pcie_rx_init(struct iwl_trans *trans);\nint iwl_pcie_gen2_rx_init(struct iwl_trans *trans);\nirqreturn_t iwl_pcie_msix_isr(int irq, void *data);\nirqreturn_t iwl_pcie_irq_handler(int irq, void *dev_id);\nirqreturn_t iwl_pcie_irq_msix_handler(int irq, void *dev_id);\nirqreturn_t iwl_pcie_irq_rx_msix_handler(int irq, void *dev_id);\nint iwl_pcie_rx_stop(struct iwl_trans *trans);\nvoid iwl_pcie_rx_free(struct iwl_trans *trans);\nvoid iwl_pcie_free_rbs_pool(struct iwl_trans *trans);\nvoid iwl_pcie_rx_init_rxb_lists(struct iwl_rxq *rxq);\nvoid iwl_pcie_rx_napi_sync(struct iwl_trans *trans);\nvoid iwl_pcie_rxq_alloc_rbs(struct iwl_trans *trans, gfp_t priority,\n\t\t\t    struct iwl_rxq *rxq);\n\n \nirqreturn_t iwl_pcie_isr(int irq, void *data);\nint iwl_pcie_alloc_ict(struct iwl_trans *trans);\nvoid iwl_pcie_free_ict(struct iwl_trans *trans);\nvoid iwl_pcie_reset_ict(struct iwl_trans *trans);\nvoid iwl_pcie_disable_ict(struct iwl_trans *trans);\n\n \nint iwl_pcie_tx_init(struct iwl_trans *trans);\nvoid iwl_pcie_tx_start(struct iwl_trans *trans, u32 scd_base_addr);\nint iwl_pcie_tx_stop(struct iwl_trans *trans);\nvoid iwl_pcie_tx_free(struct iwl_trans *trans);\nbool iwl_trans_pcie_txq_enable(struct iwl_trans *trans, int queue, u16 ssn,\n\t\t\t       const struct iwl_trans_txq_scd_cfg *cfg,\n\t\t\t       unsigned int wdg_timeout);\nvoid iwl_trans_pcie_txq_disable(struct iwl_trans *trans, int queue,\n\t\t\t\tbool configure_scd);\nvoid iwl_trans_pcie_txq_set_shared_mode(struct iwl_trans *trans, u32 txq_id,\n\t\t\t\t\tbool shared_mode);\nint iwl_trans_pcie_tx(struct iwl_trans *trans, struct sk_buff *skb,\n\t\t      struct iwl_device_tx_cmd *dev_cmd, int txq_id);\nvoid iwl_pcie_txq_check_wrptrs(struct iwl_trans *trans);\nint iwl_trans_pcie_send_hcmd(struct iwl_trans *trans, struct iwl_host_cmd *cmd);\nvoid iwl_pcie_hcmd_complete(struct iwl_trans *trans,\n\t\t\t    struct iwl_rx_cmd_buffer *rxb);\nvoid iwl_trans_pcie_tx_reset(struct iwl_trans *trans);\n\n \nvoid iwl_pcie_dump_csr(struct iwl_trans *trans);\n\n \nstatic inline void _iwl_disable_interrupts(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\tclear_bit(STATUS_INT_ENABLED, &trans->status);\n\tif (!trans_pcie->msix_enabled) {\n\t\t \n\t\tiwl_write32(trans, CSR_INT_MASK, 0x00000000);\n\n\t\t \n\t\tiwl_write32(trans, CSR_INT, 0xffffffff);\n\t\tiwl_write32(trans, CSR_FH_INT_STATUS, 0xffffffff);\n\t} else {\n\t\t \n\t\tiwl_write32(trans, CSR_MSIX_FH_INT_MASK_AD,\n\t\t\t    trans_pcie->fh_init_mask);\n\t\tiwl_write32(trans, CSR_MSIX_HW_INT_MASK_AD,\n\t\t\t    trans_pcie->hw_init_mask);\n\t}\n\tIWL_DEBUG_ISR(trans, \"Disabled interrupts\\n\");\n}\n\nstatic inline int iwl_pcie_get_num_sections(const struct fw_img *fw,\n\t\t\t\t\t    int start)\n{\n\tint i = 0;\n\n\twhile (start < fw->num_sec &&\n\t       fw->sec[start].offset != CPU1_CPU2_SEPARATOR_SECTION &&\n\t       fw->sec[start].offset != PAGING_SEPARATOR_SECTION) {\n\t\tstart++;\n\t\ti++;\n\t}\n\n\treturn i;\n}\n\nstatic inline void iwl_pcie_ctxt_info_free_fw_img(struct iwl_trans *trans)\n{\n\tstruct iwl_self_init_dram *dram = &trans->init_dram;\n\tint i;\n\n\tif (!dram->fw) {\n\t\tWARN_ON(dram->fw_cnt);\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < dram->fw_cnt; i++)\n\t\tdma_free_coherent(trans->dev, dram->fw[i].size,\n\t\t\t\t  dram->fw[i].block, dram->fw[i].physical);\n\n\tkfree(dram->fw);\n\tdram->fw_cnt = 0;\n\tdram->fw = NULL;\n}\n\nstatic inline void iwl_disable_interrupts(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\tspin_lock_bh(&trans_pcie->irq_lock);\n\t_iwl_disable_interrupts(trans);\n\tspin_unlock_bh(&trans_pcie->irq_lock);\n}\n\nstatic inline void _iwl_enable_interrupts(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\tIWL_DEBUG_ISR(trans, \"Enabling interrupts\\n\");\n\tset_bit(STATUS_INT_ENABLED, &trans->status);\n\tif (!trans_pcie->msix_enabled) {\n\t\ttrans_pcie->inta_mask = CSR_INI_SET_MASK;\n\t\tiwl_write32(trans, CSR_INT_MASK, trans_pcie->inta_mask);\n\t} else {\n\t\t \n\t\ttrans_pcie->hw_mask = trans_pcie->hw_init_mask;\n\t\ttrans_pcie->fh_mask = trans_pcie->fh_init_mask;\n\t\tiwl_write32(trans, CSR_MSIX_FH_INT_MASK_AD,\n\t\t\t    ~trans_pcie->fh_mask);\n\t\tiwl_write32(trans, CSR_MSIX_HW_INT_MASK_AD,\n\t\t\t    ~trans_pcie->hw_mask);\n\t}\n}\n\nstatic inline void iwl_enable_interrupts(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\tspin_lock_bh(&trans_pcie->irq_lock);\n\t_iwl_enable_interrupts(trans);\n\tspin_unlock_bh(&trans_pcie->irq_lock);\n}\nstatic inline void iwl_enable_hw_int_msk_msix(struct iwl_trans *trans, u32 msk)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\tiwl_write32(trans, CSR_MSIX_HW_INT_MASK_AD, ~msk);\n\ttrans_pcie->hw_mask = msk;\n}\n\nstatic inline void iwl_enable_fh_int_msk_msix(struct iwl_trans *trans, u32 msk)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\tiwl_write32(trans, CSR_MSIX_FH_INT_MASK_AD, ~msk);\n\ttrans_pcie->fh_mask = msk;\n}\n\nstatic inline void iwl_enable_fw_load_int(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\tIWL_DEBUG_ISR(trans, \"Enabling FW load interrupt\\n\");\n\tif (!trans_pcie->msix_enabled) {\n\t\ttrans_pcie->inta_mask = CSR_INT_BIT_FH_TX;\n\t\tiwl_write32(trans, CSR_INT_MASK, trans_pcie->inta_mask);\n\t} else {\n\t\tiwl_write32(trans, CSR_MSIX_HW_INT_MASK_AD,\n\t\t\t    trans_pcie->hw_init_mask);\n\t\tiwl_enable_fh_int_msk_msix(trans,\n\t\t\t\t\t   MSIX_FH_INT_CAUSES_D2S_CH0_NUM);\n\t}\n}\n\nstatic inline void iwl_enable_fw_load_int_ctx_info(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\tIWL_DEBUG_ISR(trans, \"Enabling ALIVE interrupt only\\n\");\n\n\tif (!trans_pcie->msix_enabled) {\n\t\t \n\t\ttrans_pcie->inta_mask =  CSR_INT_BIT_ALIVE | CSR_INT_BIT_FH_RX;\n\t\tiwl_write32(trans, CSR_INT_MASK, trans_pcie->inta_mask);\n\t} else {\n\t\tiwl_enable_hw_int_msk_msix(trans,\n\t\t\t\t\t   MSIX_HW_INT_CAUSES_REG_ALIVE);\n\t\t \n\t\tiwl_enable_fh_int_msk_msix(trans, trans_pcie->fh_init_mask);\n\t}\n}\n\nstatic inline const char *queue_name(struct device *dev,\n\t\t\t\t     struct iwl_trans_pcie *trans_p, int i)\n{\n\tif (trans_p->shared_vec_mask) {\n\t\tint vec = trans_p->shared_vec_mask &\n\t\t\t  IWL_SHARED_IRQ_FIRST_RSS ? 1 : 0;\n\n\t\tif (i == 0)\n\t\t\treturn DRV_NAME \":shared_IRQ\";\n\n\t\treturn devm_kasprintf(dev, GFP_KERNEL,\n\t\t\t\t      DRV_NAME \":queue_%d\", i + vec);\n\t}\n\tif (i == 0)\n\t\treturn DRV_NAME \":default_queue\";\n\n\tif (i == trans_p->alloc_vecs - 1)\n\t\treturn DRV_NAME \":exception\";\n\n\treturn devm_kasprintf(dev, GFP_KERNEL,\n\t\t\t      DRV_NAME  \":queue_%d\", i);\n}\n\nstatic inline void iwl_enable_rfkill_int(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\tIWL_DEBUG_ISR(trans, \"Enabling rfkill interrupt\\n\");\n\tif (!trans_pcie->msix_enabled) {\n\t\ttrans_pcie->inta_mask = CSR_INT_BIT_RF_KILL;\n\t\tiwl_write32(trans, CSR_INT_MASK, trans_pcie->inta_mask);\n\t} else {\n\t\tiwl_write32(trans, CSR_MSIX_FH_INT_MASK_AD,\n\t\t\t    trans_pcie->fh_init_mask);\n\t\tiwl_enable_hw_int_msk_msix(trans,\n\t\t\t\t\t   MSIX_HW_INT_CAUSES_REG_RF_KILL);\n\t}\n\n\tif (trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_9000) {\n\t\t \n\t\tiwl_set_bit(trans, CSR_GP_CNTRL,\n\t\t\t    CSR_GP_CNTRL_REG_FLAG_RFKILL_WAKE_L1A_EN);\n\t}\n}\n\nvoid iwl_pcie_handle_rfkill_irq(struct iwl_trans *trans, bool from_irq);\n\nstatic inline bool iwl_is_rfkill_set(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\tlockdep_assert_held(&trans_pcie->mutex);\n\n\tif (trans_pcie->debug_rfkill == 1)\n\t\treturn true;\n\n\treturn !(iwl_read32(trans, CSR_GP_CNTRL) &\n\t\tCSR_GP_CNTRL_REG_FLAG_HW_RF_KILL_SW);\n}\n\nstatic inline void __iwl_trans_pcie_set_bits_mask(struct iwl_trans *trans,\n\t\t\t\t\t\t  u32 reg, u32 mask, u32 value)\n{\n\tu32 v;\n\n#ifdef CONFIG_IWLWIFI_DEBUG\n\tWARN_ON_ONCE(value & ~mask);\n#endif\n\n\tv = iwl_read32(trans, reg);\n\tv &= ~mask;\n\tv |= value;\n\tiwl_write32(trans, reg, v);\n}\n\nstatic inline void __iwl_trans_pcie_clear_bit(struct iwl_trans *trans,\n\t\t\t\t\t      u32 reg, u32 mask)\n{\n\t__iwl_trans_pcie_set_bits_mask(trans, reg, mask, 0);\n}\n\nstatic inline void __iwl_trans_pcie_set_bit(struct iwl_trans *trans,\n\t\t\t\t\t    u32 reg, u32 mask)\n{\n\t__iwl_trans_pcie_set_bits_mask(trans, reg, mask, mask);\n}\n\nstatic inline bool iwl_pcie_dbg_on(struct iwl_trans *trans)\n{\n\treturn (trans->dbg.dest_tlv || iwl_trans_dbg_ini_valid(trans));\n}\n\nvoid iwl_trans_pcie_rf_kill(struct iwl_trans *trans, bool state, bool from_irq);\nvoid iwl_trans_pcie_dump_regs(struct iwl_trans *trans);\n\n#ifdef CONFIG_IWLWIFI_DEBUGFS\nvoid iwl_trans_pcie_dbgfs_register(struct iwl_trans *trans);\n#else\nstatic inline void iwl_trans_pcie_dbgfs_register(struct iwl_trans *trans) { }\n#endif\n\nvoid iwl_pcie_rx_allocator_work(struct work_struct *data);\n\n \nint iwl_pcie_gen2_apm_init(struct iwl_trans *trans);\nvoid iwl_pcie_apm_config(struct iwl_trans *trans);\nint iwl_pcie_prepare_card_hw(struct iwl_trans *trans);\nvoid iwl_pcie_synchronize_irqs(struct iwl_trans *trans);\nbool iwl_pcie_check_hw_rf_kill(struct iwl_trans *trans);\nvoid iwl_trans_pcie_handle_stop_rfkill(struct iwl_trans *trans,\n\t\t\t\t       bool was_in_rfkill);\nvoid iwl_pcie_apm_stop_master(struct iwl_trans *trans);\nvoid iwl_pcie_conf_msix_hw(struct iwl_trans_pcie *trans_pcie);\nint iwl_pcie_alloc_dma_ptr(struct iwl_trans *trans,\n\t\t\t   struct iwl_dma_ptr *ptr, size_t size);\nvoid iwl_pcie_free_dma_ptr(struct iwl_trans *trans, struct iwl_dma_ptr *ptr);\nvoid iwl_pcie_apply_destination(struct iwl_trans *trans);\n\n \nvoid iwl_pcie_alloc_fw_monitor(struct iwl_trans *trans, u8 max_power);\n\n \nint iwl_trans_pcie_gen2_start_fw(struct iwl_trans *trans,\n\t\t\t\t const struct fw_img *fw, bool run_in_rfkill);\nvoid iwl_trans_pcie_gen2_fw_alive(struct iwl_trans *trans, u32 scd_addr);\nint iwl_trans_pcie_gen2_send_hcmd(struct iwl_trans *trans,\n\t\t\t\t  struct iwl_host_cmd *cmd);\nvoid iwl_trans_pcie_gen2_stop_device(struct iwl_trans *trans);\nvoid _iwl_trans_pcie_gen2_stop_device(struct iwl_trans *trans);\nvoid iwl_pcie_d3_complete_suspend(struct iwl_trans *trans,\n\t\t\t\t  bool test, bool reset);\nint iwl_pcie_gen2_enqueue_hcmd(struct iwl_trans *trans,\n\t\t\t       struct iwl_host_cmd *cmd);\nint iwl_pcie_enqueue_hcmd(struct iwl_trans *trans,\n\t\t\t  struct iwl_host_cmd *cmd);\nvoid iwl_trans_pcie_copy_imr_fh(struct iwl_trans *trans,\n\t\t\t\tu32 dst_addr, u64 src_addr, u32 byte_cnt);\nint iwl_trans_pcie_copy_imr(struct iwl_trans *trans,\n\t\t\t    u32 dst_addr, u64 src_addr, u32 byte_cnt);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}