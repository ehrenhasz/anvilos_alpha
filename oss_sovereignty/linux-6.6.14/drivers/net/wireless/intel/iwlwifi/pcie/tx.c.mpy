{
  "module_name": "tx.c",
  "hash_id": "74f58146b84fd9567f8dc7978eb4de741e6e8ed9ac403dfec48d766213632bbc",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wireless/intel/iwlwifi/pcie/tx.c",
  "human_readable_source": "\n \n#include <linux/etherdevice.h>\n#include <linux/ieee80211.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <net/ip6_checksum.h>\n#include <net/tso.h>\n\n#include \"iwl-debug.h\"\n#include \"iwl-csr.h\"\n#include \"iwl-prph.h\"\n#include \"iwl-io.h\"\n#include \"iwl-scd.h\"\n#include \"iwl-op-mode.h\"\n#include \"internal.h\"\n#include \"fw/api/tx.h\"\n\n \n\n\nint iwl_pcie_alloc_dma_ptr(struct iwl_trans *trans,\n\t\t\t   struct iwl_dma_ptr *ptr, size_t size)\n{\n\tif (WARN_ON(ptr->addr))\n\t\treturn -EINVAL;\n\n\tptr->addr = dma_alloc_coherent(trans->dev, size,\n\t\t\t\t       &ptr->dma, GFP_KERNEL);\n\tif (!ptr->addr)\n\t\treturn -ENOMEM;\n\tptr->size = size;\n\treturn 0;\n}\n\nvoid iwl_pcie_free_dma_ptr(struct iwl_trans *trans, struct iwl_dma_ptr *ptr)\n{\n\tif (unlikely(!ptr->addr))\n\t\treturn;\n\n\tdma_free_coherent(trans->dev, ptr->size, ptr->addr, ptr->dma);\n\tmemset(ptr, 0, sizeof(*ptr));\n}\n\n \nstatic void iwl_pcie_txq_inc_wr_ptr(struct iwl_trans *trans,\n\t\t\t\t    struct iwl_txq *txq)\n{\n\tu32 reg = 0;\n\tint txq_id = txq->id;\n\n\tlockdep_assert_held(&txq->lock);\n\n\t \n\tif (!trans->trans_cfg->base_params->shadow_reg_enable &&\n\t    txq_id != trans->txqs.cmd.q_id &&\n\t    test_bit(STATUS_TPOWER_PMI, &trans->status)) {\n\t\t \n\t\treg = iwl_read32(trans, CSR_UCODE_DRV_GP1);\n\n\t\tif (reg & CSR_UCODE_DRV_GP1_BIT_MAC_SLEEP) {\n\t\t\tIWL_DEBUG_INFO(trans, \"Tx queue %d requesting wakeup, GP1 = 0x%x\\n\",\n\t\t\t\t       txq_id, reg);\n\t\t\tiwl_set_bit(trans, CSR_GP_CNTRL,\n\t\t\t\t    CSR_GP_CNTRL_REG_FLAG_MAC_ACCESS_REQ);\n\t\t\ttxq->need_update = true;\n\t\t\treturn;\n\t\t}\n\t}\n\n\t \n\tIWL_DEBUG_TX(trans, \"Q:%d WR: 0x%x\\n\", txq_id, txq->write_ptr);\n\tif (!txq->block)\n\t\tiwl_write32(trans, HBUS_TARG_WRPTR,\n\t\t\t    txq->write_ptr | (txq_id << 8));\n}\n\nvoid iwl_pcie_txq_check_wrptrs(struct iwl_trans *trans)\n{\n\tint i;\n\n\tfor (i = 0; i < trans->trans_cfg->base_params->num_of_queues; i++) {\n\t\tstruct iwl_txq *txq = trans->txqs.txq[i];\n\n\t\tif (!test_bit(i, trans->txqs.queue_used))\n\t\t\tcontinue;\n\n\t\tspin_lock_bh(&txq->lock);\n\t\tif (txq->need_update) {\n\t\t\tiwl_pcie_txq_inc_wr_ptr(trans, txq);\n\t\t\ttxq->need_update = false;\n\t\t}\n\t\tspin_unlock_bh(&txq->lock);\n\t}\n}\n\nstatic int iwl_pcie_txq_build_tfd(struct iwl_trans *trans, struct iwl_txq *txq,\n\t\t\t\t  dma_addr_t addr, u16 len, bool reset)\n{\n\tvoid *tfd;\n\tu32 num_tbs;\n\n\ttfd = (u8 *)txq->tfds + trans->txqs.tfd.size * txq->write_ptr;\n\n\tif (reset)\n\t\tmemset(tfd, 0, trans->txqs.tfd.size);\n\n\tnum_tbs = iwl_txq_gen1_tfd_get_num_tbs(trans, tfd);\n\n\t \n\tif (num_tbs >= trans->txqs.tfd.max_tbs) {\n\t\tIWL_ERR(trans, \"Error can not send more than %d chunks\\n\",\n\t\t\ttrans->txqs.tfd.max_tbs);\n\t\treturn -EINVAL;\n\t}\n\n\tif (WARN(addr & ~IWL_TX_DMA_MASK,\n\t\t \"Unaligned address = %llx\\n\", (unsigned long long)addr))\n\t\treturn -EINVAL;\n\n\tiwl_pcie_gen1_tfd_set_tb(trans, tfd, num_tbs, addr, len);\n\n\treturn num_tbs;\n}\n\nstatic void iwl_pcie_clear_cmd_in_flight(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\tif (!trans->trans_cfg->base_params->apmg_wake_up_wa)\n\t\treturn;\n\n\tspin_lock(&trans_pcie->reg_lock);\n\n\tif (WARN_ON(!trans_pcie->cmd_hold_nic_awake)) {\n\t\tspin_unlock(&trans_pcie->reg_lock);\n\t\treturn;\n\t}\n\n\ttrans_pcie->cmd_hold_nic_awake = false;\n\t__iwl_trans_pcie_clear_bit(trans, CSR_GP_CNTRL,\n\t\t\t\t   CSR_GP_CNTRL_REG_FLAG_MAC_ACCESS_REQ);\n\tspin_unlock(&trans_pcie->reg_lock);\n}\n\n \nstatic void iwl_pcie_txq_unmap(struct iwl_trans *trans, int txq_id)\n{\n\tstruct iwl_txq *txq = trans->txqs.txq[txq_id];\n\n\tif (!txq) {\n\t\tIWL_ERR(trans, \"Trying to free a queue that wasn't allocated?\\n\");\n\t\treturn;\n\t}\n\n\tspin_lock_bh(&txq->lock);\n\twhile (txq->write_ptr != txq->read_ptr) {\n\t\tIWL_DEBUG_TX_REPLY(trans, \"Q %d Free %d\\n\",\n\t\t\t\t   txq_id, txq->read_ptr);\n\n\t\tif (txq_id != trans->txqs.cmd.q_id) {\n\t\t\tstruct sk_buff *skb = txq->entries[txq->read_ptr].skb;\n\n\t\t\tif (WARN_ON_ONCE(!skb))\n\t\t\t\tcontinue;\n\n\t\t\tiwl_txq_free_tso_page(trans, skb);\n\t\t}\n\t\tiwl_txq_free_tfd(trans, txq);\n\t\ttxq->read_ptr = iwl_txq_inc_wrap(trans, txq->read_ptr);\n\n\t\tif (txq->read_ptr == txq->write_ptr &&\n\t\t    txq_id == trans->txqs.cmd.q_id)\n\t\t\tiwl_pcie_clear_cmd_in_flight(trans);\n\t}\n\n\twhile (!skb_queue_empty(&txq->overflow_q)) {\n\t\tstruct sk_buff *skb = __skb_dequeue(&txq->overflow_q);\n\n\t\tiwl_op_mode_free_skb(trans->op_mode, skb);\n\t}\n\n\tspin_unlock_bh(&txq->lock);\n\n\t \n\tiwl_wake_queue(trans, txq);\n}\n\n \nstatic void iwl_pcie_txq_free(struct iwl_trans *trans, int txq_id)\n{\n\tstruct iwl_txq *txq = trans->txqs.txq[txq_id];\n\tstruct device *dev = trans->dev;\n\tint i;\n\n\tif (WARN_ON(!txq))\n\t\treturn;\n\n\tiwl_pcie_txq_unmap(trans, txq_id);\n\n\t \n\tif (txq_id == trans->txqs.cmd.q_id)\n\t\tfor (i = 0; i < txq->n_window; i++) {\n\t\t\tkfree_sensitive(txq->entries[i].cmd);\n\t\t\tkfree_sensitive(txq->entries[i].free_buf);\n\t\t}\n\n\t \n\tif (txq->tfds) {\n\t\tdma_free_coherent(dev,\n\t\t\t\t  trans->txqs.tfd.size *\n\t\t\t\t  trans->trans_cfg->base_params->max_tfd_queue_size,\n\t\t\t\t  txq->tfds, txq->dma_addr);\n\t\ttxq->dma_addr = 0;\n\t\ttxq->tfds = NULL;\n\n\t\tdma_free_coherent(dev,\n\t\t\t\t  sizeof(*txq->first_tb_bufs) * txq->n_window,\n\t\t\t\t  txq->first_tb_bufs, txq->first_tb_dma);\n\t}\n\n\tkfree(txq->entries);\n\ttxq->entries = NULL;\n\n\tdel_timer_sync(&txq->stuck_timer);\n\n\t \n\tmemset(txq, 0, sizeof(*txq));\n}\n\nvoid iwl_pcie_tx_start(struct iwl_trans *trans, u32 scd_base_addr)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tint nq = trans->trans_cfg->base_params->num_of_queues;\n\tint chan;\n\tu32 reg_val;\n\tint clear_dwords = (SCD_TRANS_TBL_OFFSET_QUEUE(nq) -\n\t\t\t\tSCD_CONTEXT_MEM_LOWER_BOUND) / sizeof(u32);\n\n\t \n\tmemset(trans->txqs.queue_stopped, 0,\n\t       sizeof(trans->txqs.queue_stopped));\n\tmemset(trans->txqs.queue_used, 0, sizeof(trans->txqs.queue_used));\n\n\ttrans_pcie->scd_base_addr =\n\t\tiwl_read_prph(trans, SCD_SRAM_BASE_ADDR);\n\n\tWARN_ON(scd_base_addr != 0 &&\n\t\tscd_base_addr != trans_pcie->scd_base_addr);\n\n\t \n\tiwl_trans_write_mem(trans, trans_pcie->scd_base_addr +\n\t\t\t\t   SCD_CONTEXT_MEM_LOWER_BOUND,\n\t\t\t    NULL, clear_dwords);\n\n\tiwl_write_prph(trans, SCD_DRAM_BASE_ADDR,\n\t\t       trans->txqs.scd_bc_tbls.dma >> 10);\n\n\t \n\tif (trans->trans_cfg->base_params->scd_chain_ext_wa)\n\t\tiwl_write_prph(trans, SCD_CHAINEXT_EN, 0);\n\n\tiwl_trans_ac_txq_enable(trans, trans->txqs.cmd.q_id,\n\t\t\t\ttrans->txqs.cmd.fifo,\n\t\t\t\ttrans->txqs.cmd.wdg_timeout);\n\n\t \n\tiwl_scd_activate_fifos(trans);\n\n\t \n\tfor (chan = 0; chan < FH_TCSR_CHNL_NUM; chan++)\n\t\tiwl_write_direct32(trans, FH_TCSR_CHNL_TX_CONFIG_REG(chan),\n\t\t\t\t   FH_TCSR_TX_CONFIG_REG_VAL_DMA_CHNL_ENABLE |\n\t\t\t\t   FH_TCSR_TX_CONFIG_REG_VAL_DMA_CREDIT_ENABLE);\n\n\t \n\treg_val = iwl_read_direct32(trans, FH_TX_CHICKEN_BITS_REG);\n\tiwl_write_direct32(trans, FH_TX_CHICKEN_BITS_REG,\n\t\t\t   reg_val | FH_TX_CHICKEN_BITS_SCD_AUTO_RETRY_EN);\n\n\t \n\tif (trans->trans_cfg->device_family < IWL_DEVICE_FAMILY_8000)\n\t\tiwl_clear_bits_prph(trans, APMG_PCIDEV_STT_REG,\n\t\t\t\t    APMG_PCIDEV_STT_VAL_L1_ACT_DIS);\n}\n\nvoid iwl_trans_pcie_tx_reset(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tint txq_id;\n\n\t \n\tif (WARN_ON_ONCE(trans->trans_cfg->gen2))\n\t\treturn;\n\n\tfor (txq_id = 0; txq_id < trans->trans_cfg->base_params->num_of_queues;\n\t     txq_id++) {\n\t\tstruct iwl_txq *txq = trans->txqs.txq[txq_id];\n\t\tif (trans->trans_cfg->gen2)\n\t\t\tiwl_write_direct64(trans,\n\t\t\t\t\t   FH_MEM_CBBC_QUEUE(trans, txq_id),\n\t\t\t\t\t   txq->dma_addr);\n\t\telse\n\t\t\tiwl_write_direct32(trans,\n\t\t\t\t\t   FH_MEM_CBBC_QUEUE(trans, txq_id),\n\t\t\t\t\t   txq->dma_addr >> 8);\n\t\tiwl_pcie_txq_unmap(trans, txq_id);\n\t\ttxq->read_ptr = 0;\n\t\ttxq->write_ptr = 0;\n\t}\n\n\t \n\tiwl_write_direct32(trans, FH_KW_MEM_ADDR_REG,\n\t\t\t   trans_pcie->kw.dma >> 4);\n\n\t \n\tiwl_pcie_tx_start(trans, 0);\n}\n\nstatic void iwl_pcie_tx_stop_fh(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tint ch, ret;\n\tu32 mask = 0;\n\n\tspin_lock_bh(&trans_pcie->irq_lock);\n\n\tif (!iwl_trans_grab_nic_access(trans))\n\t\tgoto out;\n\n\t \n\tfor (ch = 0; ch < FH_TCSR_CHNL_NUM; ch++) {\n\t\tiwl_write32(trans, FH_TCSR_CHNL_TX_CONFIG_REG(ch), 0x0);\n\t\tmask |= FH_TSSR_TX_STATUS_REG_MSK_CHNL_IDLE(ch);\n\t}\n\n\t \n\tret = iwl_poll_bit(trans, FH_TSSR_TX_STATUS_REG, mask, mask, 5000);\n\tif (ret < 0)\n\t\tIWL_ERR(trans,\n\t\t\t\"Failing on timeout while stopping DMA channel %d [0x%08x]\\n\",\n\t\t\tch, iwl_read32(trans, FH_TSSR_TX_STATUS_REG));\n\n\tiwl_trans_release_nic_access(trans);\n\nout:\n\tspin_unlock_bh(&trans_pcie->irq_lock);\n}\n\n \nint iwl_pcie_tx_stop(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tint txq_id;\n\n\t \n\tiwl_scd_deactivate_fifos(trans);\n\n\t \n\tiwl_pcie_tx_stop_fh(trans);\n\n\t \n\tmemset(trans->txqs.queue_stopped, 0,\n\t       sizeof(trans->txqs.queue_stopped));\n\tmemset(trans->txqs.queue_used, 0, sizeof(trans->txqs.queue_used));\n\n\t \n\tif (!trans_pcie->txq_memory)\n\t\treturn 0;\n\n\t \n\tfor (txq_id = 0; txq_id < trans->trans_cfg->base_params->num_of_queues;\n\t     txq_id++)\n\t\tiwl_pcie_txq_unmap(trans, txq_id);\n\n\treturn 0;\n}\n\n \nvoid iwl_pcie_tx_free(struct iwl_trans *trans)\n{\n\tint txq_id;\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\tmemset(trans->txqs.queue_used, 0, sizeof(trans->txqs.queue_used));\n\n\t \n\tif (trans_pcie->txq_memory) {\n\t\tfor (txq_id = 0;\n\t\t     txq_id < trans->trans_cfg->base_params->num_of_queues;\n\t\t     txq_id++) {\n\t\t\tiwl_pcie_txq_free(trans, txq_id);\n\t\t\ttrans->txqs.txq[txq_id] = NULL;\n\t\t}\n\t}\n\n\tkfree(trans_pcie->txq_memory);\n\ttrans_pcie->txq_memory = NULL;\n\n\tiwl_pcie_free_dma_ptr(trans, &trans_pcie->kw);\n\n\tiwl_pcie_free_dma_ptr(trans, &trans->txqs.scd_bc_tbls);\n}\n\n \nstatic int iwl_pcie_tx_alloc(struct iwl_trans *trans)\n{\n\tint ret;\n\tint txq_id, slots_num;\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tu16 bc_tbls_size = trans->trans_cfg->base_params->num_of_queues;\n\n\tif (WARN_ON(trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_AX210))\n\t\treturn -EINVAL;\n\n\tbc_tbls_size *= sizeof(struct iwlagn_scd_bc_tbl);\n\n\t \n\tif (WARN_ON(trans_pcie->txq_memory)) {\n\t\tret = -EINVAL;\n\t\tgoto error;\n\t}\n\n\tret = iwl_pcie_alloc_dma_ptr(trans, &trans->txqs.scd_bc_tbls,\n\t\t\t\t     bc_tbls_size);\n\tif (ret) {\n\t\tIWL_ERR(trans, \"Scheduler BC Table allocation failed\\n\");\n\t\tgoto error;\n\t}\n\n\t \n\tret = iwl_pcie_alloc_dma_ptr(trans, &trans_pcie->kw, IWL_KW_SIZE);\n\tif (ret) {\n\t\tIWL_ERR(trans, \"Keep Warm allocation failed\\n\");\n\t\tgoto error;\n\t}\n\n\ttrans_pcie->txq_memory =\n\t\tkcalloc(trans->trans_cfg->base_params->num_of_queues,\n\t\t\tsizeof(struct iwl_txq), GFP_KERNEL);\n\tif (!trans_pcie->txq_memory) {\n\t\tIWL_ERR(trans, \"Not enough memory for txq\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\n\t \n\tfor (txq_id = 0; txq_id < trans->trans_cfg->base_params->num_of_queues;\n\t     txq_id++) {\n\t\tbool cmd_queue = (txq_id == trans->txqs.cmd.q_id);\n\n\t\tif (cmd_queue)\n\t\t\tslots_num = max_t(u32, IWL_CMD_QUEUE_SIZE,\n\t\t\t\t\t  trans->cfg->min_txq_size);\n\t\telse\n\t\t\tslots_num = max_t(u32, IWL_DEFAULT_QUEUE_SIZE,\n\t\t\t\t\t  trans->cfg->min_ba_txq_size);\n\t\ttrans->txqs.txq[txq_id] = &trans_pcie->txq_memory[txq_id];\n\t\tret = iwl_txq_alloc(trans, trans->txqs.txq[txq_id], slots_num,\n\t\t\t\t    cmd_queue);\n\t\tif (ret) {\n\t\t\tIWL_ERR(trans, \"Tx %d queue alloc failed\\n\", txq_id);\n\t\t\tgoto error;\n\t\t}\n\t\ttrans->txqs.txq[txq_id]->id = txq_id;\n\t}\n\n\treturn 0;\n\nerror:\n\tiwl_pcie_tx_free(trans);\n\n\treturn ret;\n}\n\nint iwl_pcie_tx_init(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tint ret;\n\tint txq_id, slots_num;\n\tbool alloc = false;\n\n\tif (!trans_pcie->txq_memory) {\n\t\tret = iwl_pcie_tx_alloc(trans);\n\t\tif (ret)\n\t\t\tgoto error;\n\t\talloc = true;\n\t}\n\n\tspin_lock_bh(&trans_pcie->irq_lock);\n\n\t \n\tiwl_scd_deactivate_fifos(trans);\n\n\t \n\tiwl_write_direct32(trans, FH_KW_MEM_ADDR_REG,\n\t\t\t   trans_pcie->kw.dma >> 4);\n\n\tspin_unlock_bh(&trans_pcie->irq_lock);\n\n\t \n\tfor (txq_id = 0; txq_id < trans->trans_cfg->base_params->num_of_queues;\n\t     txq_id++) {\n\t\tbool cmd_queue = (txq_id == trans->txqs.cmd.q_id);\n\n\t\tif (cmd_queue)\n\t\t\tslots_num = max_t(u32, IWL_CMD_QUEUE_SIZE,\n\t\t\t\t\t  trans->cfg->min_txq_size);\n\t\telse\n\t\t\tslots_num = max_t(u32, IWL_DEFAULT_QUEUE_SIZE,\n\t\t\t\t\t  trans->cfg->min_ba_txq_size);\n\t\tret = iwl_txq_init(trans, trans->txqs.txq[txq_id], slots_num,\n\t\t\t\t   cmd_queue);\n\t\tif (ret) {\n\t\t\tIWL_ERR(trans, \"Tx %d queue init failed\\n\", txq_id);\n\t\t\tgoto error;\n\t\t}\n\n\t\t \n\t\tiwl_write_direct32(trans, FH_MEM_CBBC_QUEUE(trans, txq_id),\n\t\t\t\t   trans->txqs.txq[txq_id]->dma_addr >> 8);\n\t}\n\n\tiwl_set_bits_prph(trans, SCD_GP_CTRL, SCD_GP_CTRL_AUTO_ACTIVE_MODE);\n\tif (trans->trans_cfg->base_params->num_of_queues > 20)\n\t\tiwl_set_bits_prph(trans, SCD_GP_CTRL,\n\t\t\t\t  SCD_GP_CTRL_ENABLE_31_QUEUES);\n\n\treturn 0;\nerror:\n\t \n\tif (alloc)\n\t\tiwl_pcie_tx_free(trans);\n\treturn ret;\n}\n\nstatic int iwl_pcie_set_cmd_in_flight(struct iwl_trans *trans,\n\t\t\t\t      const struct iwl_host_cmd *cmd)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\t \n\tif (test_bit(STATUS_TRANS_DEAD, &trans->status))\n\t\treturn -ENODEV;\n\n\tif (!trans->trans_cfg->base_params->apmg_wake_up_wa)\n\t\treturn 0;\n\n\t \n\tif (!_iwl_trans_pcie_grab_nic_access(trans))\n\t\treturn -EIO;\n\n\t \n\ttrans_pcie->cmd_hold_nic_awake = true;\n\tspin_unlock(&trans_pcie->reg_lock);\n\n\treturn 0;\n}\n\n \nstatic void iwl_pcie_cmdq_reclaim(struct iwl_trans *trans, int txq_id, int idx)\n{\n\tstruct iwl_txq *txq = trans->txqs.txq[txq_id];\n\tint nfreed = 0;\n\tu16 r;\n\n\tlockdep_assert_held(&txq->lock);\n\n\tidx = iwl_txq_get_cmd_index(txq, idx);\n\tr = iwl_txq_get_cmd_index(txq, txq->read_ptr);\n\n\tif (idx >= trans->trans_cfg->base_params->max_tfd_queue_size ||\n\t    (!iwl_txq_used(txq, idx))) {\n\t\tWARN_ONCE(test_bit(txq_id, trans->txqs.queue_used),\n\t\t\t  \"%s: Read index for DMA queue txq id (%d), index %d is out of range [0-%d] %d %d.\\n\",\n\t\t\t  __func__, txq_id, idx,\n\t\t\t  trans->trans_cfg->base_params->max_tfd_queue_size,\n\t\t\t  txq->write_ptr, txq->read_ptr);\n\t\treturn;\n\t}\n\n\tfor (idx = iwl_txq_inc_wrap(trans, idx); r != idx;\n\t     r = iwl_txq_inc_wrap(trans, r)) {\n\t\ttxq->read_ptr = iwl_txq_inc_wrap(trans, txq->read_ptr);\n\n\t\tif (nfreed++ > 0) {\n\t\t\tIWL_ERR(trans, \"HCMD skipped: index (%d) %d %d\\n\",\n\t\t\t\tidx, txq->write_ptr, r);\n\t\t\tiwl_force_nmi(trans);\n\t\t}\n\t}\n\n\tif (txq->read_ptr == txq->write_ptr)\n\t\tiwl_pcie_clear_cmd_in_flight(trans);\n\n\tiwl_txq_progress(txq);\n}\n\nstatic int iwl_pcie_txq_set_ratid_map(struct iwl_trans *trans, u16 ra_tid,\n\t\t\t\t u16 txq_id)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tu32 tbl_dw_addr;\n\tu32 tbl_dw;\n\tu16 scd_q2ratid;\n\n\tscd_q2ratid = ra_tid & SCD_QUEUE_RA_TID_MAP_RATID_MSK;\n\n\ttbl_dw_addr = trans_pcie->scd_base_addr +\n\t\t\tSCD_TRANS_TBL_OFFSET_QUEUE(txq_id);\n\n\ttbl_dw = iwl_trans_read_mem32(trans, tbl_dw_addr);\n\n\tif (txq_id & 0x1)\n\t\ttbl_dw = (scd_q2ratid << 16) | (tbl_dw & 0x0000FFFF);\n\telse\n\t\ttbl_dw = scd_q2ratid | (tbl_dw & 0xFFFF0000);\n\n\tiwl_trans_write_mem32(trans, tbl_dw_addr, tbl_dw);\n\n\treturn 0;\n}\n\n \n#define BUILD_RAxTID(sta_id, tid)\t(((sta_id) << 4) + (tid))\n\nbool iwl_trans_pcie_txq_enable(struct iwl_trans *trans, int txq_id, u16 ssn,\n\t\t\t       const struct iwl_trans_txq_scd_cfg *cfg,\n\t\t\t       unsigned int wdg_timeout)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_txq *txq = trans->txqs.txq[txq_id];\n\tint fifo = -1;\n\tbool scd_bug = false;\n\n\tif (test_and_set_bit(txq_id, trans->txqs.queue_used))\n\t\tWARN_ONCE(1, \"queue %d already used - expect issues\", txq_id);\n\n\ttxq->wd_timeout = msecs_to_jiffies(wdg_timeout);\n\n\tif (cfg) {\n\t\tfifo = cfg->fifo;\n\n\t\t \n\t\tif (txq_id == trans->txqs.cmd.q_id &&\n\t\t    trans_pcie->scd_set_active)\n\t\t\tiwl_scd_enable_set_active(trans, 0);\n\n\t\t \n\t\tiwl_scd_txq_set_inactive(trans, txq_id);\n\n\t\t \n\t\tif (txq_id != trans->txqs.cmd.q_id)\n\t\t\tiwl_scd_txq_set_chain(trans, txq_id);\n\n\t\tif (cfg->aggregate) {\n\t\t\tu16 ra_tid = BUILD_RAxTID(cfg->sta_id, cfg->tid);\n\n\t\t\t \n\t\t\tiwl_pcie_txq_set_ratid_map(trans, ra_tid, txq_id);\n\n\t\t\t \n\t\t\tiwl_scd_txq_enable_agg(trans, txq_id);\n\t\t\ttxq->ampdu = true;\n\t\t} else {\n\t\t\t \n\t\t\tiwl_scd_txq_disable_agg(trans, txq_id);\n\n\t\t\tssn = txq->read_ptr;\n\t\t}\n\t} else {\n\t\t \n\t\tscd_bug = !trans->trans_cfg->mq_rx_supported &&\n\t\t\t!((ssn - txq->write_ptr) & 0x3f) &&\n\t\t\t(ssn != txq->write_ptr);\n\t\tif (scd_bug)\n\t\t\tssn++;\n\t}\n\n\t \n\ttxq->read_ptr = (ssn & 0xff);\n\ttxq->write_ptr = (ssn & 0xff);\n\tiwl_write_direct32(trans, HBUS_TARG_WRPTR,\n\t\t\t   (ssn & 0xff) | (txq_id << 8));\n\n\tif (cfg) {\n\t\tu8 frame_limit = cfg->frame_limit;\n\n\t\tiwl_write_prph(trans, SCD_QUEUE_RDPTR(txq_id), ssn);\n\n\t\t \n\t\tiwl_trans_write_mem32(trans, trans_pcie->scd_base_addr +\n\t\t\t\tSCD_CONTEXT_QUEUE_OFFSET(txq_id), 0);\n\t\tiwl_trans_write_mem32(trans,\n\t\t\ttrans_pcie->scd_base_addr +\n\t\t\tSCD_CONTEXT_QUEUE_OFFSET(txq_id) + sizeof(u32),\n\t\t\tSCD_QUEUE_CTX_REG2_VAL(WIN_SIZE, frame_limit) |\n\t\t\tSCD_QUEUE_CTX_REG2_VAL(FRAME_LIMIT, frame_limit));\n\n\t\t \n\t\tiwl_write_prph(trans, SCD_QUEUE_STATUS_BITS(txq_id),\n\t\t\t       (1 << SCD_QUEUE_STTS_REG_POS_ACTIVE) |\n\t\t\t       (cfg->fifo << SCD_QUEUE_STTS_REG_POS_TXF) |\n\t\t\t       (1 << SCD_QUEUE_STTS_REG_POS_WSL) |\n\t\t\t       SCD_QUEUE_STTS_REG_MSK);\n\n\t\t \n\t\tif (txq_id == trans->txqs.cmd.q_id &&\n\t\t    trans_pcie->scd_set_active)\n\t\t\tiwl_scd_enable_set_active(trans, BIT(txq_id));\n\n\t\tIWL_DEBUG_TX_QUEUES(trans,\n\t\t\t\t    \"Activate queue %d on FIFO %d WrPtr: %d\\n\",\n\t\t\t\t    txq_id, fifo, ssn & 0xff);\n\t} else {\n\t\tIWL_DEBUG_TX_QUEUES(trans,\n\t\t\t\t    \"Activate queue %d WrPtr: %d\\n\",\n\t\t\t\t    txq_id, ssn & 0xff);\n\t}\n\n\treturn scd_bug;\n}\n\nvoid iwl_trans_pcie_txq_set_shared_mode(struct iwl_trans *trans, u32 txq_id,\n\t\t\t\t\tbool shared_mode)\n{\n\tstruct iwl_txq *txq = trans->txqs.txq[txq_id];\n\n\ttxq->ampdu = !shared_mode;\n}\n\nvoid iwl_trans_pcie_txq_disable(struct iwl_trans *trans, int txq_id,\n\t\t\t\tbool configure_scd)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tu32 stts_addr = trans_pcie->scd_base_addr +\n\t\t\tSCD_TX_STTS_QUEUE_OFFSET(txq_id);\n\tstatic const u32 zero_val[4] = {};\n\n\ttrans->txqs.txq[txq_id]->frozen_expiry_remainder = 0;\n\ttrans->txqs.txq[txq_id]->frozen = false;\n\n\t \n\tif (!test_and_clear_bit(txq_id, trans->txqs.queue_used)) {\n\t\tWARN_ONCE(test_bit(STATUS_DEVICE_ENABLED, &trans->status),\n\t\t\t  \"queue %d not used\", txq_id);\n\t\treturn;\n\t}\n\n\tif (configure_scd) {\n\t\tiwl_scd_txq_set_inactive(trans, txq_id);\n\n\t\tiwl_trans_write_mem(trans, stts_addr, (const void *)zero_val,\n\t\t\t\t    ARRAY_SIZE(zero_val));\n\t}\n\n\tiwl_pcie_txq_unmap(trans, txq_id);\n\ttrans->txqs.txq[txq_id]->ampdu = false;\n\n\tIWL_DEBUG_TX_QUEUES(trans, \"Deactivate queue %d\\n\", txq_id);\n}\n\n \n\n \nint iwl_pcie_enqueue_hcmd(struct iwl_trans *trans,\n\t\t\t  struct iwl_host_cmd *cmd)\n{\n\tstruct iwl_txq *txq = trans->txqs.txq[trans->txqs.cmd.q_id];\n\tstruct iwl_device_cmd *out_cmd;\n\tstruct iwl_cmd_meta *out_meta;\n\tvoid *dup_buf = NULL;\n\tdma_addr_t phys_addr;\n\tint idx;\n\tu16 copy_size, cmd_size, tb0_size;\n\tbool had_nocopy = false;\n\tu8 group_id = iwl_cmd_groupid(cmd->id);\n\tint i, ret;\n\tu32 cmd_pos;\n\tconst u8 *cmddata[IWL_MAX_CMD_TBS_PER_TFD];\n\tu16 cmdlen[IWL_MAX_CMD_TBS_PER_TFD];\n\tunsigned long flags;\n\n\tif (WARN(!trans->wide_cmd_header &&\n\t\t group_id > IWL_ALWAYS_LONG_GROUP,\n\t\t \"unsupported wide command %#x\\n\", cmd->id))\n\t\treturn -EINVAL;\n\n\tif (group_id != 0) {\n\t\tcopy_size = sizeof(struct iwl_cmd_header_wide);\n\t\tcmd_size = sizeof(struct iwl_cmd_header_wide);\n\t} else {\n\t\tcopy_size = sizeof(struct iwl_cmd_header);\n\t\tcmd_size = sizeof(struct iwl_cmd_header);\n\t}\n\n\t \n\tBUILD_BUG_ON(IWL_MAX_CMD_TBS_PER_TFD > IWL_NUM_OF_TBS - 1);\n\n\tfor (i = 0; i < IWL_MAX_CMD_TBS_PER_TFD; i++) {\n\t\tcmddata[i] = cmd->data[i];\n\t\tcmdlen[i] = cmd->len[i];\n\n\t\tif (!cmd->len[i])\n\t\t\tcontinue;\n\n\t\t \n\t\tif (copy_size < IWL_FIRST_TB_SIZE) {\n\t\t\tint copy = IWL_FIRST_TB_SIZE - copy_size;\n\n\t\t\tif (copy > cmdlen[i])\n\t\t\t\tcopy = cmdlen[i];\n\t\t\tcmdlen[i] -= copy;\n\t\t\tcmddata[i] += copy;\n\t\t\tcopy_size += copy;\n\t\t}\n\n\t\tif (cmd->dataflags[i] & IWL_HCMD_DFL_NOCOPY) {\n\t\t\thad_nocopy = true;\n\t\t\tif (WARN_ON(cmd->dataflags[i] & IWL_HCMD_DFL_DUP)) {\n\t\t\t\tidx = -EINVAL;\n\t\t\t\tgoto free_dup_buf;\n\t\t\t}\n\t\t} else if (cmd->dataflags[i] & IWL_HCMD_DFL_DUP) {\n\t\t\t \n\t\t\thad_nocopy = true;\n\n\t\t\t \n\t\t\tif (WARN_ON(dup_buf)) {\n\t\t\t\tidx = -EINVAL;\n\t\t\t\tgoto free_dup_buf;\n\t\t\t}\n\n\t\t\tdup_buf = kmemdup(cmddata[i], cmdlen[i],\n\t\t\t\t\t  GFP_ATOMIC);\n\t\t\tif (!dup_buf)\n\t\t\t\treturn -ENOMEM;\n\t\t} else {\n\t\t\t \n\t\t\tif (WARN_ON(had_nocopy)) {\n\t\t\t\tidx = -EINVAL;\n\t\t\t\tgoto free_dup_buf;\n\t\t\t}\n\t\t\tcopy_size += cmdlen[i];\n\t\t}\n\t\tcmd_size += cmd->len[i];\n\t}\n\n\t \n\tif (WARN(copy_size > TFD_MAX_PAYLOAD_SIZE,\n\t\t \"Command %s (%#x) is too large (%d bytes)\\n\",\n\t\t iwl_get_cmd_string(trans, cmd->id),\n\t\t cmd->id, copy_size)) {\n\t\tidx = -EINVAL;\n\t\tgoto free_dup_buf;\n\t}\n\n\tspin_lock_irqsave(&txq->lock, flags);\n\n\tif (iwl_txq_space(trans, txq) < ((cmd->flags & CMD_ASYNC) ? 2 : 1)) {\n\t\tspin_unlock_irqrestore(&txq->lock, flags);\n\n\t\tIWL_ERR(trans, \"No space in command queue\\n\");\n\t\tiwl_op_mode_cmd_queue_full(trans->op_mode);\n\t\tidx = -ENOSPC;\n\t\tgoto free_dup_buf;\n\t}\n\n\tidx = iwl_txq_get_cmd_index(txq, txq->write_ptr);\n\tout_cmd = txq->entries[idx].cmd;\n\tout_meta = &txq->entries[idx].meta;\n\n\tmemset(out_meta, 0, sizeof(*out_meta));\t \n\tif (cmd->flags & CMD_WANT_SKB)\n\t\tout_meta->source = cmd;\n\n\t \n\tif (group_id != 0) {\n\t\tout_cmd->hdr_wide.cmd = iwl_cmd_opcode(cmd->id);\n\t\tout_cmd->hdr_wide.group_id = group_id;\n\t\tout_cmd->hdr_wide.version = iwl_cmd_version(cmd->id);\n\t\tout_cmd->hdr_wide.length =\n\t\t\tcpu_to_le16(cmd_size -\n\t\t\t\t    sizeof(struct iwl_cmd_header_wide));\n\t\tout_cmd->hdr_wide.reserved = 0;\n\t\tout_cmd->hdr_wide.sequence =\n\t\t\tcpu_to_le16(QUEUE_TO_SEQ(trans->txqs.cmd.q_id) |\n\t\t\t\t\t\t INDEX_TO_SEQ(txq->write_ptr));\n\n\t\tcmd_pos = sizeof(struct iwl_cmd_header_wide);\n\t\tcopy_size = sizeof(struct iwl_cmd_header_wide);\n\t} else {\n\t\tout_cmd->hdr.cmd = iwl_cmd_opcode(cmd->id);\n\t\tout_cmd->hdr.sequence =\n\t\t\tcpu_to_le16(QUEUE_TO_SEQ(trans->txqs.cmd.q_id) |\n\t\t\t\t\t\t INDEX_TO_SEQ(txq->write_ptr));\n\t\tout_cmd->hdr.group_id = 0;\n\n\t\tcmd_pos = sizeof(struct iwl_cmd_header);\n\t\tcopy_size = sizeof(struct iwl_cmd_header);\n\t}\n\n\t \n\tfor (i = 0; i < IWL_MAX_CMD_TBS_PER_TFD; i++) {\n\t\tint copy;\n\n\t\tif (!cmd->len[i])\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!(cmd->dataflags[i] & (IWL_HCMD_DFL_NOCOPY |\n\t\t\t\t\t   IWL_HCMD_DFL_DUP))) {\n\t\t\tcopy = cmd->len[i];\n\n\t\t\tmemcpy((u8 *)out_cmd + cmd_pos, cmd->data[i], copy);\n\t\t\tcmd_pos += copy;\n\t\t\tcopy_size += copy;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tcopy = min_t(int, TFD_MAX_PAYLOAD_SIZE - cmd_pos, cmd->len[i]);\n\n\t\tmemcpy((u8 *)out_cmd + cmd_pos, cmd->data[i], copy);\n\t\tcmd_pos += copy;\n\n\t\t \n\t\tif (copy_size < IWL_FIRST_TB_SIZE) {\n\t\t\tcopy = IWL_FIRST_TB_SIZE - copy_size;\n\n\t\t\tif (copy > cmd->len[i])\n\t\t\t\tcopy = cmd->len[i];\n\t\t\tcopy_size += copy;\n\t\t}\n\t}\n\n\tIWL_DEBUG_HC(trans,\n\t\t     \"Sending command %s (%.2x.%.2x), seq: 0x%04X, %d bytes at %d[%d]:%d\\n\",\n\t\t     iwl_get_cmd_string(trans, cmd->id),\n\t\t     group_id, out_cmd->hdr.cmd,\n\t\t     le16_to_cpu(out_cmd->hdr.sequence),\n\t\t     cmd_size, txq->write_ptr, idx, trans->txqs.cmd.q_id);\n\n\t \n\ttb0_size = min_t(int, copy_size, IWL_FIRST_TB_SIZE);\n\tmemcpy(&txq->first_tb_bufs[idx], &out_cmd->hdr, tb0_size);\n\tiwl_pcie_txq_build_tfd(trans, txq,\n\t\t\t       iwl_txq_get_first_tb_dma(txq, idx),\n\t\t\t       tb0_size, true);\n\n\t \n\tif (copy_size > tb0_size) {\n\t\tphys_addr = dma_map_single(trans->dev,\n\t\t\t\t\t   ((u8 *)&out_cmd->hdr) + tb0_size,\n\t\t\t\t\t   copy_size - tb0_size,\n\t\t\t\t\t   DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(trans->dev, phys_addr)) {\n\t\t\tiwl_txq_gen1_tfd_unmap(trans, out_meta, txq,\n\t\t\t\t\t       txq->write_ptr);\n\t\t\tidx = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tiwl_pcie_txq_build_tfd(trans, txq, phys_addr,\n\t\t\t\t       copy_size - tb0_size, false);\n\t}\n\n\t \n\tfor (i = 0; i < IWL_MAX_CMD_TBS_PER_TFD; i++) {\n\t\tvoid *data = (void *)(uintptr_t)cmddata[i];\n\n\t\tif (!cmdlen[i])\n\t\t\tcontinue;\n\t\tif (!(cmd->dataflags[i] & (IWL_HCMD_DFL_NOCOPY |\n\t\t\t\t\t   IWL_HCMD_DFL_DUP)))\n\t\t\tcontinue;\n\t\tif (cmd->dataflags[i] & IWL_HCMD_DFL_DUP)\n\t\t\tdata = dup_buf;\n\t\tphys_addr = dma_map_single(trans->dev, data,\n\t\t\t\t\t   cmdlen[i], DMA_TO_DEVICE);\n\t\tif (dma_mapping_error(trans->dev, phys_addr)) {\n\t\t\tiwl_txq_gen1_tfd_unmap(trans, out_meta, txq,\n\t\t\t\t\t       txq->write_ptr);\n\t\t\tidx = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tiwl_pcie_txq_build_tfd(trans, txq, phys_addr, cmdlen[i], false);\n\t}\n\n\tBUILD_BUG_ON(IWL_TFH_NUM_TBS > sizeof(out_meta->tbs) * BITS_PER_BYTE);\n\tout_meta->flags = cmd->flags;\n\tif (WARN_ON_ONCE(txq->entries[idx].free_buf))\n\t\tkfree_sensitive(txq->entries[idx].free_buf);\n\ttxq->entries[idx].free_buf = dup_buf;\n\n\ttrace_iwlwifi_dev_hcmd(trans->dev, cmd, cmd_size, &out_cmd->hdr_wide);\n\n\t \n\tif (txq->read_ptr == txq->write_ptr && txq->wd_timeout)\n\t\tmod_timer(&txq->stuck_timer, jiffies + txq->wd_timeout);\n\n\tret = iwl_pcie_set_cmd_in_flight(trans, cmd);\n\tif (ret < 0) {\n\t\tidx = ret;\n\t\tgoto out;\n\t}\n\n\t \n\ttxq->write_ptr = iwl_txq_inc_wrap(trans, txq->write_ptr);\n\tiwl_pcie_txq_inc_wr_ptr(trans, txq);\n\n out:\n\tspin_unlock_irqrestore(&txq->lock, flags);\n free_dup_buf:\n\tif (idx < 0)\n\t\tkfree(dup_buf);\n\treturn idx;\n}\n\n \nvoid iwl_pcie_hcmd_complete(struct iwl_trans *trans,\n\t\t\t    struct iwl_rx_cmd_buffer *rxb)\n{\n\tstruct iwl_rx_packet *pkt = rxb_addr(rxb);\n\tu16 sequence = le16_to_cpu(pkt->hdr.sequence);\n\tu8 group_id;\n\tu32 cmd_id;\n\tint txq_id = SEQ_TO_QUEUE(sequence);\n\tint index = SEQ_TO_INDEX(sequence);\n\tint cmd_index;\n\tstruct iwl_device_cmd *cmd;\n\tstruct iwl_cmd_meta *meta;\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_txq *txq = trans->txqs.txq[trans->txqs.cmd.q_id];\n\n\t \n\tif (WARN(txq_id != trans->txqs.cmd.q_id,\n\t\t \"wrong command queue %d (should be %d), sequence 0x%X readp=%d writep=%d\\n\",\n\t\t txq_id, trans->txqs.cmd.q_id, sequence, txq->read_ptr,\n\t\t txq->write_ptr)) {\n\t\tiwl_print_hex_error(trans, pkt, 32);\n\t\treturn;\n\t}\n\n\tspin_lock_bh(&txq->lock);\n\n\tcmd_index = iwl_txq_get_cmd_index(txq, index);\n\tcmd = txq->entries[cmd_index].cmd;\n\tmeta = &txq->entries[cmd_index].meta;\n\tgroup_id = cmd->hdr.group_id;\n\tcmd_id = WIDE_ID(group_id, cmd->hdr.cmd);\n\n\tif (trans->trans_cfg->gen2)\n\t\tiwl_txq_gen2_tfd_unmap(trans, meta,\n\t\t\t\t       iwl_txq_get_tfd(trans, txq, index));\n\telse\n\t\tiwl_txq_gen1_tfd_unmap(trans, meta, txq, index);\n\n\t \n\tif (meta->flags & CMD_WANT_SKB) {\n\t\tstruct page *p = rxb_steal_page(rxb);\n\n\t\tmeta->source->resp_pkt = pkt;\n\t\tmeta->source->_rx_page_addr = (unsigned long)page_address(p);\n\t\tmeta->source->_rx_page_order = trans_pcie->rx_page_order;\n\t}\n\n\tif (meta->flags & CMD_WANT_ASYNC_CALLBACK)\n\t\tiwl_op_mode_async_cb(trans->op_mode, cmd);\n\n\tiwl_pcie_cmdq_reclaim(trans, txq_id, index);\n\n\tif (!(meta->flags & CMD_ASYNC)) {\n\t\tif (!test_bit(STATUS_SYNC_HCMD_ACTIVE, &trans->status)) {\n\t\t\tIWL_WARN(trans,\n\t\t\t\t \"HCMD_ACTIVE already clear for command %s\\n\",\n\t\t\t\t iwl_get_cmd_string(trans, cmd_id));\n\t\t}\n\t\tclear_bit(STATUS_SYNC_HCMD_ACTIVE, &trans->status);\n\t\tIWL_DEBUG_INFO(trans, \"Clearing HCMD_ACTIVE for command %s\\n\",\n\t\t\t       iwl_get_cmd_string(trans, cmd_id));\n\t\twake_up(&trans->wait_command_queue);\n\t}\n\n\tmeta->flags = 0;\n\n\tspin_unlock_bh(&txq->lock);\n}\n\nstatic int iwl_fill_data_tbs(struct iwl_trans *trans, struct sk_buff *skb,\n\t\t\t     struct iwl_txq *txq, u8 hdr_len,\n\t\t\t     struct iwl_cmd_meta *out_meta)\n{\n\tu16 head_tb_len;\n\tint i;\n\n\t \n\thead_tb_len = skb_headlen(skb) - hdr_len;\n\n\tif (head_tb_len > 0) {\n\t\tdma_addr_t tb_phys = dma_map_single(trans->dev,\n\t\t\t\t\t\t    skb->data + hdr_len,\n\t\t\t\t\t\t    head_tb_len, DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(trans->dev, tb_phys)))\n\t\t\treturn -EINVAL;\n\t\ttrace_iwlwifi_dev_tx_tb(trans->dev, skb, skb->data + hdr_len,\n\t\t\t\t\ttb_phys, head_tb_len);\n\t\tiwl_pcie_txq_build_tfd(trans, txq, tb_phys, head_tb_len, false);\n\t}\n\n\t \n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tdma_addr_t tb_phys;\n\t\tint tb_idx;\n\n\t\tif (!skb_frag_size(frag))\n\t\t\tcontinue;\n\n\t\ttb_phys = skb_frag_dma_map(trans->dev, frag, 0,\n\t\t\t\t\t   skb_frag_size(frag), DMA_TO_DEVICE);\n\n\t\tif (unlikely(dma_mapping_error(trans->dev, tb_phys)))\n\t\t\treturn -EINVAL;\n\t\ttrace_iwlwifi_dev_tx_tb(trans->dev, skb, skb_frag_address(frag),\n\t\t\t\t\ttb_phys, skb_frag_size(frag));\n\t\ttb_idx = iwl_pcie_txq_build_tfd(trans, txq, tb_phys,\n\t\t\t\t\t\tskb_frag_size(frag), false);\n\t\tif (tb_idx < 0)\n\t\t\treturn tb_idx;\n\n\t\tout_meta->tbs |= BIT(tb_idx);\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_INET\nstatic int iwl_fill_data_tbs_amsdu(struct iwl_trans *trans, struct sk_buff *skb,\n\t\t\t\t   struct iwl_txq *txq, u8 hdr_len,\n\t\t\t\t   struct iwl_cmd_meta *out_meta,\n\t\t\t\t   struct iwl_device_tx_cmd *dev_cmd,\n\t\t\t\t   u16 tb1_len)\n{\n\tstruct iwl_tx_cmd *tx_cmd = (void *)dev_cmd->payload;\n\tstruct ieee80211_hdr *hdr = (void *)skb->data;\n\tunsigned int snap_ip_tcp_hdrlen, ip_hdrlen, total_len, hdr_room;\n\tunsigned int mss = skb_shinfo(skb)->gso_size;\n\tu16 length, iv_len, amsdu_pad;\n\tu8 *start_hdr;\n\tstruct iwl_tso_hdr_page *hdr_page;\n\tstruct tso_t tso;\n\n\t \n\tBUILD_BUG_ON(IEEE80211_CCMP_HDR_LEN != IEEE80211_GCMP_HDR_LEN);\n\tiv_len = ieee80211_has_protected(hdr->frame_control) ?\n\t\tIEEE80211_CCMP_HDR_LEN : 0;\n\n\ttrace_iwlwifi_dev_tx(trans->dev, skb,\n\t\t\t     iwl_txq_get_tfd(trans, txq, txq->write_ptr),\n\t\t\t     trans->txqs.tfd.size,\n\t\t\t     &dev_cmd->hdr, IWL_FIRST_TB_SIZE + tb1_len, 0);\n\n\tip_hdrlen = skb_transport_header(skb) - skb_network_header(skb);\n\tsnap_ip_tcp_hdrlen = 8 + ip_hdrlen + tcp_hdrlen(skb);\n\ttotal_len = skb->len - snap_ip_tcp_hdrlen - hdr_len - iv_len;\n\tamsdu_pad = 0;\n\n\t \n\thdr_room = DIV_ROUND_UP(total_len, mss) *\n\t\t(3 + snap_ip_tcp_hdrlen + sizeof(struct ethhdr)) + iv_len;\n\n\t \n\thdr_page = get_page_hdr(trans, hdr_room, skb);\n\tif (!hdr_page)\n\t\treturn -ENOMEM;\n\n\tstart_hdr = hdr_page->pos;\n\tmemcpy(hdr_page->pos, skb->data + hdr_len, iv_len);\n\thdr_page->pos += iv_len;\n\n\t \n\tskb_pull(skb, hdr_len + iv_len);\n\n\t \n\tle16_add_cpu(&tx_cmd->len, -snap_ip_tcp_hdrlen);\n\n\ttso_start(skb, &tso);\n\n\twhile (total_len) {\n\t\t \n\t\tunsigned int data_left =\n\t\t\tmin_t(unsigned int, mss, total_len);\n\t\tunsigned int hdr_tb_len;\n\t\tdma_addr_t hdr_tb_phys;\n\t\tu8 *subf_hdrs_start = hdr_page->pos;\n\n\t\ttotal_len -= data_left;\n\n\t\tmemset(hdr_page->pos, 0, amsdu_pad);\n\t\thdr_page->pos += amsdu_pad;\n\t\tamsdu_pad = (4 - (sizeof(struct ethhdr) + snap_ip_tcp_hdrlen +\n\t\t\t\t  data_left)) & 0x3;\n\t\tether_addr_copy(hdr_page->pos, ieee80211_get_DA(hdr));\n\t\thdr_page->pos += ETH_ALEN;\n\t\tether_addr_copy(hdr_page->pos, ieee80211_get_SA(hdr));\n\t\thdr_page->pos += ETH_ALEN;\n\n\t\tlength = snap_ip_tcp_hdrlen + data_left;\n\t\t*((__be16 *)hdr_page->pos) = cpu_to_be16(length);\n\t\thdr_page->pos += sizeof(length);\n\n\t\t \n\t\ttso_build_hdr(skb, hdr_page->pos, &tso, data_left, !total_len);\n\n\t\thdr_page->pos += snap_ip_tcp_hdrlen;\n\n\t\thdr_tb_len = hdr_page->pos - start_hdr;\n\t\thdr_tb_phys = dma_map_single(trans->dev, start_hdr,\n\t\t\t\t\t     hdr_tb_len, DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(trans->dev, hdr_tb_phys)))\n\t\t\treturn -EINVAL;\n\t\tiwl_pcie_txq_build_tfd(trans, txq, hdr_tb_phys,\n\t\t\t\t       hdr_tb_len, false);\n\t\ttrace_iwlwifi_dev_tx_tb(trans->dev, skb, start_hdr,\n\t\t\t\t\thdr_tb_phys, hdr_tb_len);\n\t\t \n\t\tle16_add_cpu(&tx_cmd->len, hdr_page->pos - subf_hdrs_start);\n\n\t\t \n\t\tstart_hdr = hdr_page->pos;\n\n\t\t \n\t\twhile (data_left) {\n\t\t\tunsigned int size = min_t(unsigned int, tso.size,\n\t\t\t\t\t\t  data_left);\n\t\t\tdma_addr_t tb_phys;\n\n\t\t\ttb_phys = dma_map_single(trans->dev, tso.data,\n\t\t\t\t\t\t size, DMA_TO_DEVICE);\n\t\t\tif (unlikely(dma_mapping_error(trans->dev, tb_phys)))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tiwl_pcie_txq_build_tfd(trans, txq, tb_phys,\n\t\t\t\t\t       size, false);\n\t\t\ttrace_iwlwifi_dev_tx_tb(trans->dev, skb, tso.data,\n\t\t\t\t\t\ttb_phys, size);\n\n\t\t\tdata_left -= size;\n\t\t\ttso_build_data(skb, &tso, size);\n\t\t}\n\t}\n\n\t \n\tskb_push(skb, hdr_len + iv_len);\n\n\treturn 0;\n}\n#else  \nstatic int iwl_fill_data_tbs_amsdu(struct iwl_trans *trans, struct sk_buff *skb,\n\t\t\t\t   struct iwl_txq *txq, u8 hdr_len,\n\t\t\t\t   struct iwl_cmd_meta *out_meta,\n\t\t\t\t   struct iwl_device_tx_cmd *dev_cmd,\n\t\t\t\t   u16 tb1_len)\n{\n\t \n\tWARN_ON(1);\n\n\treturn -1;\n}\n#endif  \n\nint iwl_trans_pcie_tx(struct iwl_trans *trans, struct sk_buff *skb,\n\t\t      struct iwl_device_tx_cmd *dev_cmd, int txq_id)\n{\n\tstruct ieee80211_hdr *hdr;\n\tstruct iwl_tx_cmd *tx_cmd = (struct iwl_tx_cmd *)dev_cmd->payload;\n\tstruct iwl_cmd_meta *out_meta;\n\tstruct iwl_txq *txq;\n\tdma_addr_t tb0_phys, tb1_phys, scratch_phys;\n\tvoid *tb1_addr;\n\tvoid *tfd;\n\tu16 len, tb1_len;\n\tbool wait_write_ptr;\n\t__le16 fc;\n\tu8 hdr_len;\n\tu16 wifi_seq;\n\tbool amsdu;\n\n\ttxq = trans->txqs.txq[txq_id];\n\n\tif (WARN_ONCE(!test_bit(txq_id, trans->txqs.queue_used),\n\t\t      \"TX on unused queue %d\\n\", txq_id))\n\t\treturn -EINVAL;\n\n\tif (skb_is_nonlinear(skb) &&\n\t    skb_shinfo(skb)->nr_frags > IWL_TRANS_MAX_FRAGS(trans) &&\n\t    __skb_linearize(skb))\n\t\treturn -ENOMEM;\n\n\t \n\thdr = (struct ieee80211_hdr *)skb->data;\n\tfc = hdr->frame_control;\n\thdr_len = ieee80211_hdrlen(fc);\n\n\tspin_lock(&txq->lock);\n\n\tif (iwl_txq_space(trans, txq) < txq->high_mark) {\n\t\tiwl_txq_stop(trans, txq);\n\n\t\t \n\t\tif (unlikely(iwl_txq_space(trans, txq) < 3)) {\n\t\t\tstruct iwl_device_tx_cmd **dev_cmd_ptr;\n\n\t\t\tdev_cmd_ptr = (void *)((u8 *)skb->cb +\n\t\t\t\t\t       trans->txqs.dev_cmd_offs);\n\n\t\t\t*dev_cmd_ptr = dev_cmd;\n\t\t\t__skb_queue_tail(&txq->overflow_q, skb);\n\n\t\t\tspin_unlock(&txq->lock);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\twifi_seq = IEEE80211_SEQ_TO_SN(le16_to_cpu(hdr->seq_ctrl));\n\tWARN_ONCE(txq->ampdu &&\n\t\t  (wifi_seq & 0xff) != txq->write_ptr,\n\t\t  \"Q: %d WiFi Seq %d tfdNum %d\",\n\t\t  txq_id, wifi_seq, txq->write_ptr);\n\n\t \n\ttxq->entries[txq->write_ptr].skb = skb;\n\ttxq->entries[txq->write_ptr].cmd = dev_cmd;\n\n\tdev_cmd->hdr.sequence =\n\t\tcpu_to_le16((u16)(QUEUE_TO_SEQ(txq_id) |\n\t\t\t    INDEX_TO_SEQ(txq->write_ptr)));\n\n\ttb0_phys = iwl_txq_get_first_tb_dma(txq, txq->write_ptr);\n\tscratch_phys = tb0_phys + sizeof(struct iwl_cmd_header) +\n\t\t       offsetof(struct iwl_tx_cmd, scratch);\n\n\ttx_cmd->dram_lsb_ptr = cpu_to_le32(scratch_phys);\n\ttx_cmd->dram_msb_ptr = iwl_get_dma_hi_addr(scratch_phys);\n\n\t \n\tout_meta = &txq->entries[txq->write_ptr].meta;\n\tout_meta->flags = 0;\n\n\t \n\tlen = sizeof(struct iwl_tx_cmd) + sizeof(struct iwl_cmd_header) +\n\t      hdr_len - IWL_FIRST_TB_SIZE;\n\t \n\tamsdu = ieee80211_is_data_qos(fc) &&\n\t\t(*ieee80211_get_qos_ctl(hdr) &\n\t\t IEEE80211_QOS_CTL_A_MSDU_PRESENT);\n\tif (!amsdu) {\n\t\ttb1_len = ALIGN(len, 4);\n\t\t \n\t\tif (tb1_len != len)\n\t\t\ttx_cmd->tx_flags |= cpu_to_le32(TX_CMD_FLG_MH_PAD);\n\t} else {\n\t\ttb1_len = len;\n\t}\n\n\t \n\tiwl_pcie_txq_build_tfd(trans, txq, tb0_phys,\n\t\t\t       IWL_FIRST_TB_SIZE, true);\n\n\t \n\tBUILD_BUG_ON(sizeof(struct iwl_tx_cmd) < IWL_FIRST_TB_SIZE);\n\tBUILD_BUG_ON(sizeof(struct iwl_cmd_header) +\n\t\t     offsetofend(struct iwl_tx_cmd, scratch) >\n\t\t     IWL_FIRST_TB_SIZE);\n\n\t \n\ttb1_addr = ((u8 *)&dev_cmd->hdr) + IWL_FIRST_TB_SIZE;\n\ttb1_phys = dma_map_single(trans->dev, tb1_addr, tb1_len, DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(trans->dev, tb1_phys)))\n\t\tgoto out_err;\n\tiwl_pcie_txq_build_tfd(trans, txq, tb1_phys, tb1_len, false);\n\n\ttrace_iwlwifi_dev_tx(trans->dev, skb,\n\t\t\t     iwl_txq_get_tfd(trans, txq, txq->write_ptr),\n\t\t\t     trans->txqs.tfd.size,\n\t\t\t     &dev_cmd->hdr, IWL_FIRST_TB_SIZE + tb1_len,\n\t\t\t     hdr_len);\n\n\t \n\tif (amsdu && skb_shinfo(skb)->gso_size) {\n\t\tif (unlikely(iwl_fill_data_tbs_amsdu(trans, skb, txq, hdr_len,\n\t\t\t\t\t\t     out_meta, dev_cmd,\n\t\t\t\t\t\t     tb1_len)))\n\t\t\tgoto out_err;\n\t} else {\n\t\tstruct sk_buff *frag;\n\n\t\tif (unlikely(iwl_fill_data_tbs(trans, skb, txq, hdr_len,\n\t\t\t\t\t       out_meta)))\n\t\t\tgoto out_err;\n\n\t\tskb_walk_frags(skb, frag) {\n\t\t\tif (unlikely(iwl_fill_data_tbs(trans, frag, txq, 0,\n\t\t\t\t\t\t       out_meta)))\n\t\t\t\tgoto out_err;\n\t\t}\n\t}\n\n\t \n\tmemcpy(&txq->first_tb_bufs[txq->write_ptr], dev_cmd, IWL_FIRST_TB_SIZE);\n\n\ttfd = iwl_txq_get_tfd(trans, txq, txq->write_ptr);\n\t \n\tiwl_txq_gen1_update_byte_cnt_tbl(trans, txq, le16_to_cpu(tx_cmd->len),\n\t\t\t\t\t iwl_txq_gen1_tfd_get_num_tbs(trans,\n\t\t\t\t\t\t\t\t      tfd));\n\n\twait_write_ptr = ieee80211_has_morefrags(fc);\n\n\t \n\tif (txq->read_ptr == txq->write_ptr && txq->wd_timeout) {\n\t\t \n\t\tif (!txq->frozen)\n\t\t\tmod_timer(&txq->stuck_timer,\n\t\t\t\t  jiffies + txq->wd_timeout);\n\t\telse\n\t\t\ttxq->frozen_expiry_remainder = txq->wd_timeout;\n\t}\n\n\t \n\ttxq->write_ptr = iwl_txq_inc_wrap(trans, txq->write_ptr);\n\tif (!wait_write_ptr)\n\t\tiwl_pcie_txq_inc_wr_ptr(trans, txq);\n\n\t \n\tspin_unlock(&txq->lock);\n\treturn 0;\nout_err:\n\tiwl_txq_gen1_tfd_unmap(trans, out_meta, txq, txq->write_ptr);\n\tspin_unlock(&txq->lock);\n\treturn -1;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}