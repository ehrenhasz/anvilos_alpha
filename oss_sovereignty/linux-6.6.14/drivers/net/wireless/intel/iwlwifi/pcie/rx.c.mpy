{
  "module_name": "rx.c",
  "hash_id": "1f0941f526fda5f85de6b34d61b1c2bd9eadd6d696124ab05caa75e9dd1f7bd1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wireless/intel/iwlwifi/pcie/rx.c",
  "human_readable_source": "\n \n#include <linux/sched.h>\n#include <linux/wait.h>\n#include <linux/gfp.h>\n\n#include \"iwl-prph.h\"\n#include \"iwl-io.h\"\n#include \"internal.h\"\n#include \"iwl-op-mode.h\"\n#include \"iwl-context-info-gen3.h\"\n\n \n\n \n\n \nstatic int iwl_rxq_space(const struct iwl_rxq *rxq)\n{\n\t \n\tWARN_ON(rxq->queue_size & (rxq->queue_size - 1));\n\n\t \n\treturn (rxq->read - rxq->write - 1) & (rxq->queue_size - 1);\n}\n\n \nstatic inline __le32 iwl_pcie_dma_addr2rbd_ptr(dma_addr_t dma_addr)\n{\n\treturn cpu_to_le32((u32)(dma_addr >> 8));\n}\n\n \nint iwl_pcie_rx_stop(struct iwl_trans *trans)\n{\n\tif (trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_AX210) {\n\t\t \n\t\tiwl_write_umac_prph(trans, RFH_RXF_DMA_CFG_GEN3, 0);\n\t\treturn iwl_poll_umac_prph_bit(trans, RFH_GEN_STATUS_GEN3,\n\t\t\t\t\t      RXF_DMA_IDLE, RXF_DMA_IDLE, 1000);\n\t} else if (trans->trans_cfg->mq_rx_supported) {\n\t\tiwl_write_prph(trans, RFH_RXF_DMA_CFG, 0);\n\t\treturn iwl_poll_prph_bit(trans, RFH_GEN_STATUS,\n\t\t\t\t\t   RXF_DMA_IDLE, RXF_DMA_IDLE, 1000);\n\t} else {\n\t\tiwl_write_direct32(trans, FH_MEM_RCSR_CHNL0_CONFIG_REG, 0);\n\t\treturn iwl_poll_direct_bit(trans, FH_MEM_RSSR_RX_STATUS_REG,\n\t\t\t\t\t   FH_RSSR_CHNL0_RX_STATUS_CHNL_IDLE,\n\t\t\t\t\t   1000);\n\t}\n}\n\n \nstatic void iwl_pcie_rxq_inc_wr_ptr(struct iwl_trans *trans,\n\t\t\t\t    struct iwl_rxq *rxq)\n{\n\tu32 reg;\n\n\tlockdep_assert_held(&rxq->lock);\n\n\t \n\tif (!trans->trans_cfg->base_params->shadow_reg_enable &&\n\t    test_bit(STATUS_TPOWER_PMI, &trans->status)) {\n\t\treg = iwl_read32(trans, CSR_UCODE_DRV_GP1);\n\n\t\tif (reg & CSR_UCODE_DRV_GP1_BIT_MAC_SLEEP) {\n\t\t\tIWL_DEBUG_INFO(trans, \"Rx queue requesting wakeup, GP1 = 0x%x\\n\",\n\t\t\t\t       reg);\n\t\t\tiwl_set_bit(trans, CSR_GP_CNTRL,\n\t\t\t\t    CSR_GP_CNTRL_REG_FLAG_MAC_ACCESS_REQ);\n\t\t\trxq->need_update = true;\n\t\t\treturn;\n\t\t}\n\t}\n\n\trxq->write_actual = round_down(rxq->write, 8);\n\tif (!trans->trans_cfg->mq_rx_supported)\n\t\tiwl_write32(trans, FH_RSCSR_CHNL0_WPTR, rxq->write_actual);\n\telse if (trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_BZ)\n\t\tiwl_write32(trans, HBUS_TARG_WRPTR, rxq->write_actual |\n\t\t\t    HBUS_TARG_WRPTR_RX_Q(rxq->id));\n\telse\n\t\tiwl_write32(trans, RFH_Q_FRBDCB_WIDX_TRG(rxq->id),\n\t\t\t    rxq->write_actual);\n}\n\nstatic void iwl_pcie_rxq_check_wrptr(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tint i;\n\n\tfor (i = 0; i < trans->num_rx_queues; i++) {\n\t\tstruct iwl_rxq *rxq = &trans_pcie->rxq[i];\n\n\t\tif (!rxq->need_update)\n\t\t\tcontinue;\n\t\tspin_lock_bh(&rxq->lock);\n\t\tiwl_pcie_rxq_inc_wr_ptr(trans, rxq);\n\t\trxq->need_update = false;\n\t\tspin_unlock_bh(&rxq->lock);\n\t}\n}\n\nstatic void iwl_pcie_restock_bd(struct iwl_trans *trans,\n\t\t\t\tstruct iwl_rxq *rxq,\n\t\t\t\tstruct iwl_rx_mem_buffer *rxb)\n{\n\tif (trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_AX210) {\n\t\tstruct iwl_rx_transfer_desc *bd = rxq->bd;\n\n\t\tBUILD_BUG_ON(sizeof(*bd) != 2 * sizeof(u64));\n\n\t\tbd[rxq->write].addr = cpu_to_le64(rxb->page_dma);\n\t\tbd[rxq->write].rbid = cpu_to_le16(rxb->vid);\n\t} else {\n\t\t__le64 *bd = rxq->bd;\n\n\t\tbd[rxq->write] = cpu_to_le64(rxb->page_dma | rxb->vid);\n\t}\n\n\tIWL_DEBUG_RX(trans, \"Assigned virtual RB ID %u to queue %d index %d\\n\",\n\t\t     (u32)rxb->vid, rxq->id, rxq->write);\n}\n\n \nstatic void iwl_pcie_rxmq_restock(struct iwl_trans *trans,\n\t\t\t\t  struct iwl_rxq *rxq)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_rx_mem_buffer *rxb;\n\n\t \n\tif (!test_bit(STATUS_DEVICE_ENABLED, &trans->status))\n\t\treturn;\n\n\tspin_lock_bh(&rxq->lock);\n\twhile (rxq->free_count) {\n\t\t \n\t\trxb = list_first_entry(&rxq->rx_free, struct iwl_rx_mem_buffer,\n\t\t\t\t       list);\n\t\tlist_del(&rxb->list);\n\t\trxb->invalid = false;\n\t\t \n\t\tWARN_ON(rxb->page_dma & trans_pcie->supported_dma_mask);\n\t\t \n\t\tiwl_pcie_restock_bd(trans, rxq, rxb);\n\t\trxq->write = (rxq->write + 1) & (rxq->queue_size - 1);\n\t\trxq->free_count--;\n\t}\n\tspin_unlock_bh(&rxq->lock);\n\n\t \n\tif (rxq->write_actual != (rxq->write & ~0x7)) {\n\t\tspin_lock_bh(&rxq->lock);\n\t\tiwl_pcie_rxq_inc_wr_ptr(trans, rxq);\n\t\tspin_unlock_bh(&rxq->lock);\n\t}\n}\n\n \nstatic void iwl_pcie_rxsq_restock(struct iwl_trans *trans,\n\t\t\t\t  struct iwl_rxq *rxq)\n{\n\tstruct iwl_rx_mem_buffer *rxb;\n\n\t \n\tif (!test_bit(STATUS_DEVICE_ENABLED, &trans->status))\n\t\treturn;\n\n\tspin_lock_bh(&rxq->lock);\n\twhile ((iwl_rxq_space(rxq) > 0) && (rxq->free_count)) {\n\t\t__le32 *bd = (__le32 *)rxq->bd;\n\t\t \n\t\trxb = rxq->queue[rxq->write];\n\t\tBUG_ON(rxb && rxb->page);\n\n\t\t \n\t\trxb = list_first_entry(&rxq->rx_free, struct iwl_rx_mem_buffer,\n\t\t\t\t       list);\n\t\tlist_del(&rxb->list);\n\t\trxb->invalid = false;\n\n\t\t \n\t\tbd[rxq->write] = iwl_pcie_dma_addr2rbd_ptr(rxb->page_dma);\n\t\trxq->queue[rxq->write] = rxb;\n\t\trxq->write = (rxq->write + 1) & RX_QUEUE_MASK;\n\t\trxq->free_count--;\n\t}\n\tspin_unlock_bh(&rxq->lock);\n\n\t \n\tif (rxq->write_actual != (rxq->write & ~0x7)) {\n\t\tspin_lock_bh(&rxq->lock);\n\t\tiwl_pcie_rxq_inc_wr_ptr(trans, rxq);\n\t\tspin_unlock_bh(&rxq->lock);\n\t}\n}\n\n \nstatic\nvoid iwl_pcie_rxq_restock(struct iwl_trans *trans, struct iwl_rxq *rxq)\n{\n\tif (trans->trans_cfg->mq_rx_supported)\n\t\tiwl_pcie_rxmq_restock(trans, rxq);\n\telse\n\t\tiwl_pcie_rxsq_restock(trans, rxq);\n}\n\n \nstatic struct page *iwl_pcie_rx_alloc_page(struct iwl_trans *trans,\n\t\t\t\t\t   u32 *offset, gfp_t priority)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tunsigned int rbsize = iwl_trans_get_rb_size(trans_pcie->rx_buf_size);\n\tunsigned int allocsize = PAGE_SIZE << trans_pcie->rx_page_order;\n\tstruct page *page;\n\tgfp_t gfp_mask = priority;\n\n\tif (trans_pcie->rx_page_order > 0)\n\t\tgfp_mask |= __GFP_COMP;\n\n\tif (trans_pcie->alloc_page) {\n\t\tspin_lock_bh(&trans_pcie->alloc_page_lock);\n\t\t \n\t\tif (trans_pcie->alloc_page) {\n\t\t\t*offset = trans_pcie->alloc_page_used;\n\t\t\tpage = trans_pcie->alloc_page;\n\t\t\ttrans_pcie->alloc_page_used += rbsize;\n\t\t\tif (trans_pcie->alloc_page_used >= allocsize)\n\t\t\t\ttrans_pcie->alloc_page = NULL;\n\t\t\telse\n\t\t\t\tget_page(page);\n\t\t\tspin_unlock_bh(&trans_pcie->alloc_page_lock);\n\t\t\treturn page;\n\t\t}\n\t\tspin_unlock_bh(&trans_pcie->alloc_page_lock);\n\t}\n\n\t \n\tpage = alloc_pages(gfp_mask, trans_pcie->rx_page_order);\n\tif (!page) {\n\t\tif (net_ratelimit())\n\t\t\tIWL_DEBUG_INFO(trans, \"alloc_pages failed, order: %d\\n\",\n\t\t\t\t       trans_pcie->rx_page_order);\n\t\t \n\t\tif (!(gfp_mask & __GFP_NOWARN) && net_ratelimit())\n\t\t\tIWL_CRIT(trans,\n\t\t\t\t \"Failed to alloc_pages\\n\");\n\t\treturn NULL;\n\t}\n\n\tif (2 * rbsize <= allocsize) {\n\t\tspin_lock_bh(&trans_pcie->alloc_page_lock);\n\t\tif (!trans_pcie->alloc_page) {\n\t\t\tget_page(page);\n\t\t\ttrans_pcie->alloc_page = page;\n\t\t\ttrans_pcie->alloc_page_used = rbsize;\n\t\t}\n\t\tspin_unlock_bh(&trans_pcie->alloc_page_lock);\n\t}\n\n\t*offset = 0;\n\treturn page;\n}\n\n \nvoid iwl_pcie_rxq_alloc_rbs(struct iwl_trans *trans, gfp_t priority,\n\t\t\t    struct iwl_rxq *rxq)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_rx_mem_buffer *rxb;\n\tstruct page *page;\n\n\twhile (1) {\n\t\tunsigned int offset;\n\n\t\tspin_lock_bh(&rxq->lock);\n\t\tif (list_empty(&rxq->rx_used)) {\n\t\t\tspin_unlock_bh(&rxq->lock);\n\t\t\treturn;\n\t\t}\n\t\tspin_unlock_bh(&rxq->lock);\n\n\t\tpage = iwl_pcie_rx_alloc_page(trans, &offset, priority);\n\t\tif (!page)\n\t\t\treturn;\n\n\t\tspin_lock_bh(&rxq->lock);\n\n\t\tif (list_empty(&rxq->rx_used)) {\n\t\t\tspin_unlock_bh(&rxq->lock);\n\t\t\t__free_pages(page, trans_pcie->rx_page_order);\n\t\t\treturn;\n\t\t}\n\t\trxb = list_first_entry(&rxq->rx_used, struct iwl_rx_mem_buffer,\n\t\t\t\t       list);\n\t\tlist_del(&rxb->list);\n\t\tspin_unlock_bh(&rxq->lock);\n\n\t\tBUG_ON(rxb->page);\n\t\trxb->page = page;\n\t\trxb->offset = offset;\n\t\t \n\t\trxb->page_dma =\n\t\t\tdma_map_page(trans->dev, page, rxb->offset,\n\t\t\t\t     trans_pcie->rx_buf_bytes,\n\t\t\t\t     DMA_FROM_DEVICE);\n\t\tif (dma_mapping_error(trans->dev, rxb->page_dma)) {\n\t\t\trxb->page = NULL;\n\t\t\tspin_lock_bh(&rxq->lock);\n\t\t\tlist_add(&rxb->list, &rxq->rx_used);\n\t\t\tspin_unlock_bh(&rxq->lock);\n\t\t\t__free_pages(page, trans_pcie->rx_page_order);\n\t\t\treturn;\n\t\t}\n\n\t\tspin_lock_bh(&rxq->lock);\n\n\t\tlist_add_tail(&rxb->list, &rxq->rx_free);\n\t\trxq->free_count++;\n\n\t\tspin_unlock_bh(&rxq->lock);\n\t}\n}\n\nvoid iwl_pcie_free_rbs_pool(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tint i;\n\n\tif (!trans_pcie->rx_pool)\n\t\treturn;\n\n\tfor (i = 0; i < RX_POOL_SIZE(trans_pcie->num_rx_bufs); i++) {\n\t\tif (!trans_pcie->rx_pool[i].page)\n\t\t\tcontinue;\n\t\tdma_unmap_page(trans->dev, trans_pcie->rx_pool[i].page_dma,\n\t\t\t       trans_pcie->rx_buf_bytes, DMA_FROM_DEVICE);\n\t\t__free_pages(trans_pcie->rx_pool[i].page,\n\t\t\t     trans_pcie->rx_page_order);\n\t\ttrans_pcie->rx_pool[i].page = NULL;\n\t}\n}\n\n \nstatic void iwl_pcie_rx_allocator(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_rb_allocator *rba = &trans_pcie->rba;\n\tstruct list_head local_empty;\n\tint pending = atomic_read(&rba->req_pending);\n\n\tIWL_DEBUG_TPT(trans, \"Pending allocation requests = %d\\n\", pending);\n\n\t \n\tspin_lock_bh(&rba->lock);\n\t \n\tlist_replace_init(&rba->rbd_empty, &local_empty);\n\tspin_unlock_bh(&rba->lock);\n\n\twhile (pending) {\n\t\tint i;\n\t\tLIST_HEAD(local_allocated);\n\t\tgfp_t gfp_mask = GFP_KERNEL;\n\n\t\t \n\t\tif (pending < RX_PENDING_WATERMARK)\n\t\t\tgfp_mask |= __GFP_NOWARN;\n\n\t\tfor (i = 0; i < RX_CLAIM_REQ_ALLOC;) {\n\t\t\tstruct iwl_rx_mem_buffer *rxb;\n\t\t\tstruct page *page;\n\n\t\t\t \n\t\t\tBUG_ON(list_empty(&local_empty));\n\t\t\t \n\t\t\trxb = list_first_entry(&local_empty,\n\t\t\t\t\t       struct iwl_rx_mem_buffer, list);\n\t\t\tBUG_ON(rxb->page);\n\n\t\t\t \n\t\t\tpage = iwl_pcie_rx_alloc_page(trans, &rxb->offset,\n\t\t\t\t\t\t      gfp_mask);\n\t\t\tif (!page)\n\t\t\t\tcontinue;\n\t\t\trxb->page = page;\n\n\t\t\t \n\t\t\trxb->page_dma = dma_map_page(trans->dev, page,\n\t\t\t\t\t\t     rxb->offset,\n\t\t\t\t\t\t     trans_pcie->rx_buf_bytes,\n\t\t\t\t\t\t     DMA_FROM_DEVICE);\n\t\t\tif (dma_mapping_error(trans->dev, rxb->page_dma)) {\n\t\t\t\trxb->page = NULL;\n\t\t\t\t__free_pages(page, trans_pcie->rx_page_order);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tlist_move(&rxb->list, &local_allocated);\n\t\t\ti++;\n\t\t}\n\n\t\tatomic_dec(&rba->req_pending);\n\t\tpending--;\n\n\t\tif (!pending) {\n\t\t\tpending = atomic_read(&rba->req_pending);\n\t\t\tif (pending)\n\t\t\t\tIWL_DEBUG_TPT(trans,\n\t\t\t\t\t      \"Got more pending allocation requests = %d\\n\",\n\t\t\t\t\t      pending);\n\t\t}\n\n\t\tspin_lock_bh(&rba->lock);\n\t\t \n\t\tlist_splice_tail(&local_allocated, &rba->rbd_allocated);\n\t\t \n\t\tlist_splice_tail_init(&rba->rbd_empty, &local_empty);\n\t\tspin_unlock_bh(&rba->lock);\n\n\t\tatomic_inc(&rba->req_ready);\n\n\t}\n\n\tspin_lock_bh(&rba->lock);\n\t \n\tlist_splice_tail(&local_empty, &rba->rbd_empty);\n\tspin_unlock_bh(&rba->lock);\n\n\tIWL_DEBUG_TPT(trans, \"%s, exit.\\n\", __func__);\n}\n\n \nstatic void iwl_pcie_rx_allocator_get(struct iwl_trans *trans,\n\t\t\t\t      struct iwl_rxq *rxq)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_rb_allocator *rba = &trans_pcie->rba;\n\tint i;\n\n\tlockdep_assert_held(&rxq->lock);\n\n\t \n\tif (atomic_dec_if_positive(&rba->req_ready) < 0)\n\t\treturn;\n\n\tspin_lock(&rba->lock);\n\tfor (i = 0; i < RX_CLAIM_REQ_ALLOC; i++) {\n\t\t \n\t\tstruct iwl_rx_mem_buffer *rxb =\n\t\t\tlist_first_entry(&rba->rbd_allocated,\n\t\t\t\t\t struct iwl_rx_mem_buffer, list);\n\n\t\tlist_move(&rxb->list, &rxq->rx_free);\n\t}\n\tspin_unlock(&rba->lock);\n\n\trxq->used_count -= RX_CLAIM_REQ_ALLOC;\n\trxq->free_count += RX_CLAIM_REQ_ALLOC;\n}\n\nvoid iwl_pcie_rx_allocator_work(struct work_struct *data)\n{\n\tstruct iwl_rb_allocator *rba_p =\n\t\tcontainer_of(data, struct iwl_rb_allocator, rx_alloc);\n\tstruct iwl_trans_pcie *trans_pcie =\n\t\tcontainer_of(rba_p, struct iwl_trans_pcie, rba);\n\n\tiwl_pcie_rx_allocator(trans_pcie->trans);\n}\n\nstatic int iwl_pcie_free_bd_size(struct iwl_trans *trans)\n{\n\tif (trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_AX210)\n\t\treturn sizeof(struct iwl_rx_transfer_desc);\n\n\treturn trans->trans_cfg->mq_rx_supported ?\n\t\t\tsizeof(__le64) : sizeof(__le32);\n}\n\nstatic int iwl_pcie_used_bd_size(struct iwl_trans *trans)\n{\n\tif (trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_BZ)\n\t\treturn sizeof(struct iwl_rx_completion_desc_bz);\n\n\tif (trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_AX210)\n\t\treturn sizeof(struct iwl_rx_completion_desc);\n\n\treturn sizeof(__le32);\n}\n\nstatic void iwl_pcie_free_rxq_dma(struct iwl_trans *trans,\n\t\t\t\t  struct iwl_rxq *rxq)\n{\n\tint free_size = iwl_pcie_free_bd_size(trans);\n\n\tif (rxq->bd)\n\t\tdma_free_coherent(trans->dev,\n\t\t\t\t  free_size * rxq->queue_size,\n\t\t\t\t  rxq->bd, rxq->bd_dma);\n\trxq->bd_dma = 0;\n\trxq->bd = NULL;\n\n\trxq->rb_stts_dma = 0;\n\trxq->rb_stts = NULL;\n\n\tif (rxq->used_bd)\n\t\tdma_free_coherent(trans->dev,\n\t\t\t\t  iwl_pcie_used_bd_size(trans) *\n\t\t\t\t\trxq->queue_size,\n\t\t\t\t  rxq->used_bd, rxq->used_bd_dma);\n\trxq->used_bd_dma = 0;\n\trxq->used_bd = NULL;\n}\n\nstatic size_t iwl_pcie_rb_stts_size(struct iwl_trans *trans)\n{\n\tbool use_rx_td = (trans->trans_cfg->device_family >=\n\t\t\t  IWL_DEVICE_FAMILY_AX210);\n\n\tif (use_rx_td)\n\t\treturn sizeof(__le16);\n\n\treturn sizeof(struct iwl_rb_status);\n}\n\nstatic int iwl_pcie_alloc_rxq_dma(struct iwl_trans *trans,\n\t\t\t\t  struct iwl_rxq *rxq)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tsize_t rb_stts_size = iwl_pcie_rb_stts_size(trans);\n\tstruct device *dev = trans->dev;\n\tint i;\n\tint free_size;\n\n\tspin_lock_init(&rxq->lock);\n\tif (trans->trans_cfg->mq_rx_supported)\n\t\trxq->queue_size = trans->cfg->num_rbds;\n\telse\n\t\trxq->queue_size = RX_QUEUE_SIZE;\n\n\tfree_size = iwl_pcie_free_bd_size(trans);\n\n\t \n\trxq->bd = dma_alloc_coherent(dev, free_size * rxq->queue_size,\n\t\t\t\t     &rxq->bd_dma, GFP_KERNEL);\n\tif (!rxq->bd)\n\t\tgoto err;\n\n\tif (trans->trans_cfg->mq_rx_supported) {\n\t\trxq->used_bd = dma_alloc_coherent(dev,\n\t\t\t\t\t\t  iwl_pcie_used_bd_size(trans) *\n\t\t\t\t\t\t\trxq->queue_size,\n\t\t\t\t\t\t  &rxq->used_bd_dma,\n\t\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!rxq->used_bd)\n\t\t\tgoto err;\n\t}\n\n\trxq->rb_stts = (u8 *)trans_pcie->base_rb_stts + rxq->id * rb_stts_size;\n\trxq->rb_stts_dma =\n\t\ttrans_pcie->base_rb_stts_dma + rxq->id * rb_stts_size;\n\n\treturn 0;\n\nerr:\n\tfor (i = 0; i < trans->num_rx_queues; i++) {\n\t\tstruct iwl_rxq *rxq = &trans_pcie->rxq[i];\n\n\t\tiwl_pcie_free_rxq_dma(trans, rxq);\n\t}\n\n\treturn -ENOMEM;\n}\n\nstatic int iwl_pcie_rx_alloc(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tsize_t rb_stts_size = iwl_pcie_rb_stts_size(trans);\n\tstruct iwl_rb_allocator *rba = &trans_pcie->rba;\n\tint i, ret;\n\n\tif (WARN_ON(trans_pcie->rxq))\n\t\treturn -EINVAL;\n\n\ttrans_pcie->rxq = kcalloc(trans->num_rx_queues, sizeof(struct iwl_rxq),\n\t\t\t\t  GFP_KERNEL);\n\ttrans_pcie->rx_pool = kcalloc(RX_POOL_SIZE(trans_pcie->num_rx_bufs),\n\t\t\t\t      sizeof(trans_pcie->rx_pool[0]),\n\t\t\t\t      GFP_KERNEL);\n\ttrans_pcie->global_table =\n\t\tkcalloc(RX_POOL_SIZE(trans_pcie->num_rx_bufs),\n\t\t\tsizeof(trans_pcie->global_table[0]),\n\t\t\tGFP_KERNEL);\n\tif (!trans_pcie->rxq || !trans_pcie->rx_pool ||\n\t    !trans_pcie->global_table) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tspin_lock_init(&rba->lock);\n\n\t \n\ttrans_pcie->base_rb_stts =\n\t\t\tdma_alloc_coherent(trans->dev,\n\t\t\t\t\t   rb_stts_size * trans->num_rx_queues,\n\t\t\t\t\t   &trans_pcie->base_rb_stts_dma,\n\t\t\t\t\t   GFP_KERNEL);\n\tif (!trans_pcie->base_rb_stts) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tfor (i = 0; i < trans->num_rx_queues; i++) {\n\t\tstruct iwl_rxq *rxq = &trans_pcie->rxq[i];\n\n\t\trxq->id = i;\n\t\tret = iwl_pcie_alloc_rxq_dma(trans, rxq);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\treturn 0;\n\nerr:\n\tif (trans_pcie->base_rb_stts) {\n\t\tdma_free_coherent(trans->dev,\n\t\t\t\t  rb_stts_size * trans->num_rx_queues,\n\t\t\t\t  trans_pcie->base_rb_stts,\n\t\t\t\t  trans_pcie->base_rb_stts_dma);\n\t\ttrans_pcie->base_rb_stts = NULL;\n\t\ttrans_pcie->base_rb_stts_dma = 0;\n\t}\n\tkfree(trans_pcie->rx_pool);\n\ttrans_pcie->rx_pool = NULL;\n\tkfree(trans_pcie->global_table);\n\ttrans_pcie->global_table = NULL;\n\tkfree(trans_pcie->rxq);\n\ttrans_pcie->rxq = NULL;\n\n\treturn ret;\n}\n\nstatic void iwl_pcie_rx_hw_init(struct iwl_trans *trans, struct iwl_rxq *rxq)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tu32 rb_size;\n\tconst u32 rfdnlog = RX_QUEUE_SIZE_LOG;  \n\n\tswitch (trans_pcie->rx_buf_size) {\n\tcase IWL_AMSDU_4K:\n\t\trb_size = FH_RCSR_RX_CONFIG_REG_VAL_RB_SIZE_4K;\n\t\tbreak;\n\tcase IWL_AMSDU_8K:\n\t\trb_size = FH_RCSR_RX_CONFIG_REG_VAL_RB_SIZE_8K;\n\t\tbreak;\n\tcase IWL_AMSDU_12K:\n\t\trb_size = FH_RCSR_RX_CONFIG_REG_VAL_RB_SIZE_12K;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\trb_size = FH_RCSR_RX_CONFIG_REG_VAL_RB_SIZE_4K;\n\t}\n\n\tif (!iwl_trans_grab_nic_access(trans))\n\t\treturn;\n\n\t \n\tiwl_write32(trans, FH_MEM_RCSR_CHNL0_CONFIG_REG, 0);\n\t \n\tiwl_write32(trans, FH_MEM_RCSR_CHNL0_RBDCB_WPTR, 0);\n\tiwl_write32(trans, FH_MEM_RCSR_CHNL0_FLUSH_RB_REQ, 0);\n\tiwl_write32(trans, FH_RSCSR_CHNL0_RDPTR, 0);\n\n\t \n\tiwl_write32(trans, FH_RSCSR_CHNL0_RBDCB_WPTR_REG, 0);\n\n\t \n\tiwl_write32(trans, FH_RSCSR_CHNL0_RBDCB_BASE_REG,\n\t\t    (u32)(rxq->bd_dma >> 8));\n\n\t \n\tiwl_write32(trans, FH_RSCSR_CHNL0_STTS_WPTR_REG,\n\t\t    rxq->rb_stts_dma >> 4);\n\n\t \n\tiwl_write32(trans, FH_MEM_RCSR_CHNL0_CONFIG_REG,\n\t\t    FH_RCSR_RX_CONFIG_CHNL_EN_ENABLE_VAL |\n\t\t    FH_RCSR_CHNL0_RX_IGNORE_RXF_EMPTY |\n\t\t    FH_RCSR_CHNL0_RX_CONFIG_IRQ_DEST_INT_HOST_VAL |\n\t\t    rb_size |\n\t\t    (RX_RB_TIMEOUT << FH_RCSR_RX_CONFIG_REG_IRQ_RBTH_POS) |\n\t\t    (rfdnlog << FH_RCSR_RX_CONFIG_RBDCB_SIZE_POS));\n\n\tiwl_trans_release_nic_access(trans);\n\n\t \n\tiwl_write8(trans, CSR_INT_COALESCING, IWL_HOST_INT_TIMEOUT_DEF);\n\n\t \n\tif (trans->cfg->host_interrupt_operation_mode)\n\t\tiwl_set_bit(trans, CSR_INT_COALESCING, IWL_HOST_INT_OPER_MODE);\n}\n\nstatic void iwl_pcie_rx_mq_hw_init(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tu32 rb_size, enabled = 0;\n\tint i;\n\n\tswitch (trans_pcie->rx_buf_size) {\n\tcase IWL_AMSDU_2K:\n\t\trb_size = RFH_RXF_DMA_RB_SIZE_2K;\n\t\tbreak;\n\tcase IWL_AMSDU_4K:\n\t\trb_size = RFH_RXF_DMA_RB_SIZE_4K;\n\t\tbreak;\n\tcase IWL_AMSDU_8K:\n\t\trb_size = RFH_RXF_DMA_RB_SIZE_8K;\n\t\tbreak;\n\tcase IWL_AMSDU_12K:\n\t\trb_size = RFH_RXF_DMA_RB_SIZE_12K;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON(1);\n\t\trb_size = RFH_RXF_DMA_RB_SIZE_4K;\n\t}\n\n\tif (!iwl_trans_grab_nic_access(trans))\n\t\treturn;\n\n\t \n\tiwl_write_prph_no_grab(trans, RFH_RXF_DMA_CFG, 0);\n\t \n\tiwl_write_prph_no_grab(trans, RFH_RXF_RXQ_ACTIVE, 0);\n\n\tfor (i = 0; i < trans->num_rx_queues; i++) {\n\t\t \n\t\tiwl_write_prph64_no_grab(trans,\n\t\t\t\t\t RFH_Q_FRBDCB_BA_LSB(i),\n\t\t\t\t\t trans_pcie->rxq[i].bd_dma);\n\t\t \n\t\tiwl_write_prph64_no_grab(trans,\n\t\t\t\t\t RFH_Q_URBDCB_BA_LSB(i),\n\t\t\t\t\t trans_pcie->rxq[i].used_bd_dma);\n\t\t \n\t\tiwl_write_prph64_no_grab(trans,\n\t\t\t\t\t RFH_Q_URBD_STTS_WPTR_LSB(i),\n\t\t\t\t\t trans_pcie->rxq[i].rb_stts_dma);\n\t\t \n\t\tiwl_write_prph_no_grab(trans, RFH_Q_FRBDCB_WIDX(i), 0);\n\t\tiwl_write_prph_no_grab(trans, RFH_Q_FRBDCB_RIDX(i), 0);\n\t\tiwl_write_prph_no_grab(trans, RFH_Q_URBDCB_WIDX(i), 0);\n\n\t\tenabled |= BIT(i) | BIT(i + 16);\n\t}\n\n\t \n\tiwl_write_prph_no_grab(trans, RFH_RXF_DMA_CFG,\n\t\t\t       RFH_DMA_EN_ENABLE_VAL | rb_size |\n\t\t\t       RFH_RXF_DMA_MIN_RB_4_8 |\n\t\t\t       RFH_RXF_DMA_DROP_TOO_LARGE_MASK |\n\t\t\t       RFH_RXF_DMA_RBDCB_SIZE_512);\n\n\t \n\tiwl_write_prph_no_grab(trans, RFH_GEN_CFG,\n\t\t\t       RFH_GEN_CFG_RFH_DMA_SNOOP |\n\t\t\t       RFH_GEN_CFG_VAL(DEFAULT_RXQ_NUM, 0) |\n\t\t\t       RFH_GEN_CFG_SERVICE_DMA_SNOOP |\n\t\t\t       RFH_GEN_CFG_VAL(RB_CHUNK_SIZE,\n\t\t\t\t\t       trans->trans_cfg->integrated ?\n\t\t\t\t\t       RFH_GEN_CFG_RB_CHUNK_SIZE_64 :\n\t\t\t\t\t       RFH_GEN_CFG_RB_CHUNK_SIZE_128));\n\t \n\tiwl_write_prph_no_grab(trans, RFH_RXF_RXQ_ACTIVE, enabled);\n\n\tiwl_trans_release_nic_access(trans);\n\n\t \n\tiwl_write8(trans, CSR_INT_COALESCING, IWL_HOST_INT_TIMEOUT_DEF);\n}\n\nvoid iwl_pcie_rx_init_rxb_lists(struct iwl_rxq *rxq)\n{\n\tlockdep_assert_held(&rxq->lock);\n\n\tINIT_LIST_HEAD(&rxq->rx_free);\n\tINIT_LIST_HEAD(&rxq->rx_used);\n\trxq->free_count = 0;\n\trxq->used_count = 0;\n}\n\nstatic int iwl_pcie_rx_handle(struct iwl_trans *trans, int queue, int budget);\n\nstatic int iwl_pcie_napi_poll(struct napi_struct *napi, int budget)\n{\n\tstruct iwl_rxq *rxq = container_of(napi, struct iwl_rxq, napi);\n\tstruct iwl_trans_pcie *trans_pcie;\n\tstruct iwl_trans *trans;\n\tint ret;\n\n\ttrans_pcie = container_of(napi->dev, struct iwl_trans_pcie, napi_dev);\n\ttrans = trans_pcie->trans;\n\n\tret = iwl_pcie_rx_handle(trans, rxq->id, budget);\n\n\tIWL_DEBUG_ISR(trans, \"[%d] handled %d, budget %d\\n\",\n\t\t      rxq->id, ret, budget);\n\n\tif (ret < budget) {\n\t\tspin_lock(&trans_pcie->irq_lock);\n\t\tif (test_bit(STATUS_INT_ENABLED, &trans->status))\n\t\t\t_iwl_enable_interrupts(trans);\n\t\tspin_unlock(&trans_pcie->irq_lock);\n\n\t\tnapi_complete_done(&rxq->napi, ret);\n\t}\n\n\treturn ret;\n}\n\nstatic int iwl_pcie_napi_poll_msix(struct napi_struct *napi, int budget)\n{\n\tstruct iwl_rxq *rxq = container_of(napi, struct iwl_rxq, napi);\n\tstruct iwl_trans_pcie *trans_pcie;\n\tstruct iwl_trans *trans;\n\tint ret;\n\n\ttrans_pcie = container_of(napi->dev, struct iwl_trans_pcie, napi_dev);\n\ttrans = trans_pcie->trans;\n\n\tret = iwl_pcie_rx_handle(trans, rxq->id, budget);\n\tIWL_DEBUG_ISR(trans, \"[%d] handled %d, budget %d\\n\", rxq->id, ret,\n\t\t      budget);\n\n\tif (ret < budget) {\n\t\tint irq_line = rxq->id;\n\n\t\t \n\t\tif (trans_pcie->shared_vec_mask & IWL_SHARED_IRQ_FIRST_RSS &&\n\t\t    rxq->id == 1)\n\t\t\tirq_line = 0;\n\n\t\tspin_lock(&trans_pcie->irq_lock);\n\t\tiwl_pcie_clear_irq(trans, irq_line);\n\t\tspin_unlock(&trans_pcie->irq_lock);\n\n\t\tnapi_complete_done(&rxq->napi, ret);\n\t}\n\n\treturn ret;\n}\n\nvoid iwl_pcie_rx_napi_sync(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tint i;\n\n\tif (unlikely(!trans_pcie->rxq))\n\t\treturn;\n\n\tfor (i = 0; i < trans->num_rx_queues; i++) {\n\t\tstruct iwl_rxq *rxq = &trans_pcie->rxq[i];\n\n\t\tif (rxq && rxq->napi.poll)\n\t\t\tnapi_synchronize(&rxq->napi);\n\t}\n}\n\nstatic int _iwl_pcie_rx_init(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_rxq *def_rxq;\n\tstruct iwl_rb_allocator *rba = &trans_pcie->rba;\n\tint i, err, queue_size, allocator_pool_size, num_alloc;\n\n\tif (!trans_pcie->rxq) {\n\t\terr = iwl_pcie_rx_alloc(trans);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\tdef_rxq = trans_pcie->rxq;\n\n\tcancel_work_sync(&rba->rx_alloc);\n\n\tspin_lock_bh(&rba->lock);\n\tatomic_set(&rba->req_pending, 0);\n\tatomic_set(&rba->req_ready, 0);\n\tINIT_LIST_HEAD(&rba->rbd_allocated);\n\tINIT_LIST_HEAD(&rba->rbd_empty);\n\tspin_unlock_bh(&rba->lock);\n\n\t \n\tiwl_pcie_free_rbs_pool(trans);\n\n\tfor (i = 0; i < RX_QUEUE_SIZE; i++)\n\t\tdef_rxq->queue[i] = NULL;\n\n\tfor (i = 0; i < trans->num_rx_queues; i++) {\n\t\tstruct iwl_rxq *rxq = &trans_pcie->rxq[i];\n\n\t\tspin_lock_bh(&rxq->lock);\n\t\t \n\t\trxq->read = 0;\n\t\trxq->write = 0;\n\t\trxq->write_actual = 0;\n\t\tmemset(rxq->rb_stts, 0,\n\t\t       (trans->trans_cfg->device_family >=\n\t\t\tIWL_DEVICE_FAMILY_AX210) ?\n\t\t       sizeof(__le16) : sizeof(struct iwl_rb_status));\n\n\t\tiwl_pcie_rx_init_rxb_lists(rxq);\n\n\t\tspin_unlock_bh(&rxq->lock);\n\n\t\tif (!rxq->napi.poll) {\n\t\t\tint (*poll)(struct napi_struct *, int) = iwl_pcie_napi_poll;\n\n\t\t\tif (trans_pcie->msix_enabled)\n\t\t\t\tpoll = iwl_pcie_napi_poll_msix;\n\n\t\t\tnetif_napi_add(&trans_pcie->napi_dev, &rxq->napi,\n\t\t\t\t       poll);\n\t\t\tnapi_enable(&rxq->napi);\n\t\t}\n\n\t}\n\n\t \n\tqueue_size = trans->trans_cfg->mq_rx_supported ?\n\t\t\ttrans_pcie->num_rx_bufs - 1 : RX_QUEUE_SIZE;\n\tallocator_pool_size = trans->num_rx_queues *\n\t\t(RX_CLAIM_REQ_ALLOC - RX_POST_REQ_ALLOC);\n\tnum_alloc = queue_size + allocator_pool_size;\n\n\tfor (i = 0; i < num_alloc; i++) {\n\t\tstruct iwl_rx_mem_buffer *rxb = &trans_pcie->rx_pool[i];\n\n\t\tif (i < allocator_pool_size)\n\t\t\tlist_add(&rxb->list, &rba->rbd_empty);\n\t\telse\n\t\t\tlist_add(&rxb->list, &def_rxq->rx_used);\n\t\ttrans_pcie->global_table[i] = rxb;\n\t\trxb->vid = (u16)(i + 1);\n\t\trxb->invalid = true;\n\t}\n\n\tiwl_pcie_rxq_alloc_rbs(trans, GFP_KERNEL, def_rxq);\n\n\treturn 0;\n}\n\nint iwl_pcie_rx_init(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tint ret = _iwl_pcie_rx_init(trans);\n\n\tif (ret)\n\t\treturn ret;\n\n\tif (trans->trans_cfg->mq_rx_supported)\n\t\tiwl_pcie_rx_mq_hw_init(trans);\n\telse\n\t\tiwl_pcie_rx_hw_init(trans, trans_pcie->rxq);\n\n\tiwl_pcie_rxq_restock(trans, trans_pcie->rxq);\n\n\tspin_lock_bh(&trans_pcie->rxq->lock);\n\tiwl_pcie_rxq_inc_wr_ptr(trans, trans_pcie->rxq);\n\tspin_unlock_bh(&trans_pcie->rxq->lock);\n\n\treturn 0;\n}\n\nint iwl_pcie_gen2_rx_init(struct iwl_trans *trans)\n{\n\t \n\tiwl_write8(trans, CSR_INT_COALESCING, IWL_HOST_INT_TIMEOUT_DEF);\n\n\t \n\treturn _iwl_pcie_rx_init(trans);\n}\n\nvoid iwl_pcie_rx_free(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tsize_t rb_stts_size = iwl_pcie_rb_stts_size(trans);\n\tstruct iwl_rb_allocator *rba = &trans_pcie->rba;\n\tint i;\n\n\t \n\tif (!trans_pcie->rxq) {\n\t\tIWL_DEBUG_INFO(trans, \"Free NULL rx context\\n\");\n\t\treturn;\n\t}\n\n\tcancel_work_sync(&rba->rx_alloc);\n\n\tiwl_pcie_free_rbs_pool(trans);\n\n\tif (trans_pcie->base_rb_stts) {\n\t\tdma_free_coherent(trans->dev,\n\t\t\t\t  rb_stts_size * trans->num_rx_queues,\n\t\t\t\t  trans_pcie->base_rb_stts,\n\t\t\t\t  trans_pcie->base_rb_stts_dma);\n\t\ttrans_pcie->base_rb_stts = NULL;\n\t\ttrans_pcie->base_rb_stts_dma = 0;\n\t}\n\n\tfor (i = 0; i < trans->num_rx_queues; i++) {\n\t\tstruct iwl_rxq *rxq = &trans_pcie->rxq[i];\n\n\t\tiwl_pcie_free_rxq_dma(trans, rxq);\n\n\t\tif (rxq->napi.poll) {\n\t\t\tnapi_disable(&rxq->napi);\n\t\t\tnetif_napi_del(&rxq->napi);\n\t\t}\n\t}\n\tkfree(trans_pcie->rx_pool);\n\tkfree(trans_pcie->global_table);\n\tkfree(trans_pcie->rxq);\n\n\tif (trans_pcie->alloc_page)\n\t\t__free_pages(trans_pcie->alloc_page, trans_pcie->rx_page_order);\n}\n\nstatic void iwl_pcie_rx_move_to_allocator(struct iwl_rxq *rxq,\n\t\t\t\t\t  struct iwl_rb_allocator *rba)\n{\n\tspin_lock(&rba->lock);\n\tlist_splice_tail_init(&rxq->rx_used, &rba->rbd_empty);\n\tspin_unlock(&rba->lock);\n}\n\n \nstatic void iwl_pcie_rx_reuse_rbd(struct iwl_trans *trans,\n\t\t\t\t  struct iwl_rx_mem_buffer *rxb,\n\t\t\t\t  struct iwl_rxq *rxq, bool emergency)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_rb_allocator *rba = &trans_pcie->rba;\n\n\t \n\tlist_add_tail(&rxb->list, &rxq->rx_used);\n\n\tif (unlikely(emergency))\n\t\treturn;\n\n\t \n\trxq->used_count++;\n\n\t \n\tif ((rxq->used_count % RX_CLAIM_REQ_ALLOC) == RX_POST_REQ_ALLOC) {\n\t\t \n\t\tiwl_pcie_rx_move_to_allocator(rxq, rba);\n\n\t\tatomic_inc(&rba->req_pending);\n\t\tqueue_work(rba->alloc_wq, &rba->rx_alloc);\n\t}\n}\n\nstatic void iwl_pcie_rx_handle_rb(struct iwl_trans *trans,\n\t\t\t\tstruct iwl_rxq *rxq,\n\t\t\t\tstruct iwl_rx_mem_buffer *rxb,\n\t\t\t\tbool emergency,\n\t\t\t\tint i)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_txq *txq = trans->txqs.txq[trans->txqs.cmd.q_id];\n\tbool page_stolen = false;\n\tint max_len = trans_pcie->rx_buf_bytes;\n\tu32 offset = 0;\n\n\tif (WARN_ON(!rxb))\n\t\treturn;\n\n\tdma_unmap_page(trans->dev, rxb->page_dma, max_len, DMA_FROM_DEVICE);\n\n\twhile (offset + sizeof(u32) + sizeof(struct iwl_cmd_header) < max_len) {\n\t\tstruct iwl_rx_packet *pkt;\n\t\tbool reclaim;\n\t\tint len;\n\t\tstruct iwl_rx_cmd_buffer rxcb = {\n\t\t\t._offset = rxb->offset + offset,\n\t\t\t._rx_page_order = trans_pcie->rx_page_order,\n\t\t\t._page = rxb->page,\n\t\t\t._page_stolen = false,\n\t\t\t.truesize = max_len,\n\t\t};\n\n\t\tpkt = rxb_addr(&rxcb);\n\n\t\tif (pkt->len_n_flags == cpu_to_le32(FH_RSCSR_FRAME_INVALID)) {\n\t\t\tIWL_DEBUG_RX(trans,\n\t\t\t\t     \"Q %d: RB end marker at offset %d\\n\",\n\t\t\t\t     rxq->id, offset);\n\t\t\tbreak;\n\t\t}\n\n\t\tWARN((le32_to_cpu(pkt->len_n_flags) & FH_RSCSR_RXQ_MASK) >>\n\t\t\tFH_RSCSR_RXQ_POS != rxq->id,\n\t\t     \"frame on invalid queue - is on %d and indicates %d\\n\",\n\t\t     rxq->id,\n\t\t     (le32_to_cpu(pkt->len_n_flags) & FH_RSCSR_RXQ_MASK) >>\n\t\t\tFH_RSCSR_RXQ_POS);\n\n\t\tIWL_DEBUG_RX(trans,\n\t\t\t     \"Q %d: cmd at offset %d: %s (%.2x.%2x, seq 0x%x)\\n\",\n\t\t\t     rxq->id, offset,\n\t\t\t     iwl_get_cmd_string(trans,\n\t\t\t\t\t\tWIDE_ID(pkt->hdr.group_id, pkt->hdr.cmd)),\n\t\t\t     pkt->hdr.group_id, pkt->hdr.cmd,\n\t\t\t     le16_to_cpu(pkt->hdr.sequence));\n\n\t\tlen = iwl_rx_packet_len(pkt);\n\t\tlen += sizeof(u32);  \n\n\t\toffset += ALIGN(len, FH_RSCSR_FRAME_ALIGN);\n\n\t\t \n\t\tif (len < sizeof(*pkt) || offset > max_len)\n\t\t\tbreak;\n\n\t\ttrace_iwlwifi_dev_rx(trans->dev, trans, pkt, len);\n\t\ttrace_iwlwifi_dev_rx_data(trans->dev, trans, pkt, len);\n\n\t\t \n\t\treclaim = !(pkt->hdr.sequence & SEQ_RX_FRAME);\n\t\tif (reclaim && !pkt->hdr.group_id) {\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < trans_pcie->n_no_reclaim_cmds; i++) {\n\t\t\t\tif (trans_pcie->no_reclaim_cmds[i] ==\n\t\t\t\t\t\t\tpkt->hdr.cmd) {\n\t\t\t\t\treclaim = false;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (rxq->id == IWL_DEFAULT_RX_QUEUE)\n\t\t\tiwl_op_mode_rx(trans->op_mode, &rxq->napi,\n\t\t\t\t       &rxcb);\n\t\telse\n\t\t\tiwl_op_mode_rx_rss(trans->op_mode, &rxq->napi,\n\t\t\t\t\t   &rxcb, rxq->id);\n\n\t\t \n\n\t\tif (reclaim && txq) {\n\t\t\tu16 sequence = le16_to_cpu(pkt->hdr.sequence);\n\t\t\tint index = SEQ_TO_INDEX(sequence);\n\t\t\tint cmd_index = iwl_txq_get_cmd_index(txq, index);\n\n\t\t\tkfree_sensitive(txq->entries[cmd_index].free_buf);\n\t\t\ttxq->entries[cmd_index].free_buf = NULL;\n\n\t\t\t \n\t\t\tif (!rxcb._page_stolen)\n\t\t\t\tiwl_pcie_hcmd_complete(trans, &rxcb);\n\t\t\telse\n\t\t\t\tIWL_WARN(trans, \"Claim null rxb?\\n\");\n\t\t}\n\n\t\tpage_stolen |= rxcb._page_stolen;\n\t\tif (trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_AX210)\n\t\t\tbreak;\n\t}\n\n\t \n\tif (page_stolen) {\n\t\t__free_pages(rxb->page, trans_pcie->rx_page_order);\n\t\trxb->page = NULL;\n\t}\n\n\t \n\tif (rxb->page != NULL) {\n\t\trxb->page_dma =\n\t\t\tdma_map_page(trans->dev, rxb->page, rxb->offset,\n\t\t\t\t     trans_pcie->rx_buf_bytes,\n\t\t\t\t     DMA_FROM_DEVICE);\n\t\tif (dma_mapping_error(trans->dev, rxb->page_dma)) {\n\t\t\t \n\t\t\t__free_pages(rxb->page, trans_pcie->rx_page_order);\n\t\t\trxb->page = NULL;\n\t\t\tiwl_pcie_rx_reuse_rbd(trans, rxb, rxq, emergency);\n\t\t} else {\n\t\t\tlist_add_tail(&rxb->list, &rxq->rx_free);\n\t\t\trxq->free_count++;\n\t\t}\n\t} else\n\t\tiwl_pcie_rx_reuse_rbd(trans, rxb, rxq, emergency);\n}\n\nstatic struct iwl_rx_mem_buffer *iwl_pcie_get_rxb(struct iwl_trans *trans,\n\t\t\t\t\t\t  struct iwl_rxq *rxq, int i,\n\t\t\t\t\t\t  bool *join)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_rx_mem_buffer *rxb;\n\tu16 vid;\n\n\tBUILD_BUG_ON(sizeof(struct iwl_rx_completion_desc) != 32);\n\tBUILD_BUG_ON(sizeof(struct iwl_rx_completion_desc_bz) != 4);\n\n\tif (!trans->trans_cfg->mq_rx_supported) {\n\t\trxb = rxq->queue[i];\n\t\trxq->queue[i] = NULL;\n\t\treturn rxb;\n\t}\n\n\tif (trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_BZ) {\n\t\tstruct iwl_rx_completion_desc_bz *cd = rxq->used_bd;\n\n\t\tvid = le16_to_cpu(cd[i].rbid);\n\t\t*join = cd[i].flags & IWL_RX_CD_FLAGS_FRAGMENTED;\n\t} else if (trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_AX210) {\n\t\tstruct iwl_rx_completion_desc *cd = rxq->used_bd;\n\n\t\tvid = le16_to_cpu(cd[i].rbid);\n\t\t*join = cd[i].flags & IWL_RX_CD_FLAGS_FRAGMENTED;\n\t} else {\n\t\t__le32 *cd = rxq->used_bd;\n\n\t\tvid = le32_to_cpu(cd[i]) & 0x0FFF;  \n\t}\n\n\tif (!vid || vid > RX_POOL_SIZE(trans_pcie->num_rx_bufs))\n\t\tgoto out_err;\n\n\trxb = trans_pcie->global_table[vid - 1];\n\tif (rxb->invalid)\n\t\tgoto out_err;\n\n\tIWL_DEBUG_RX(trans, \"Got virtual RB ID %u\\n\", (u32)rxb->vid);\n\n\trxb->invalid = true;\n\n\treturn rxb;\n\nout_err:\n\tWARN(1, \"Invalid rxb from HW %u\\n\", (u32)vid);\n\tiwl_force_nmi(trans);\n\treturn NULL;\n}\n\n \nstatic int iwl_pcie_rx_handle(struct iwl_trans *trans, int queue, int budget)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_rxq *rxq;\n\tu32 r, i, count = 0, handled = 0;\n\tbool emergency = false;\n\n\tif (WARN_ON_ONCE(!trans_pcie->rxq || !trans_pcie->rxq[queue].bd))\n\t\treturn budget;\n\n\trxq = &trans_pcie->rxq[queue];\n\nrestart:\n\tspin_lock(&rxq->lock);\n\t \n\tr = le16_to_cpu(iwl_get_closed_rb_stts(trans, rxq)) & 0x0FFF;\n\ti = rxq->read;\n\n\t \n\tr &= (rxq->queue_size - 1);\n\n\t \n\tif (i == r)\n\t\tIWL_DEBUG_RX(trans, \"Q %d: HW = SW = %d\\n\", rxq->id, r);\n\n\twhile (i != r && ++handled < budget) {\n\t\tstruct iwl_rb_allocator *rba = &trans_pcie->rba;\n\t\tstruct iwl_rx_mem_buffer *rxb;\n\t\t \n\t\tu32 rb_pending_alloc =\n\t\t\tatomic_read(&trans_pcie->rba.req_pending) *\n\t\t\tRX_CLAIM_REQ_ALLOC;\n\t\tbool join = false;\n\n\t\tif (unlikely(rb_pending_alloc >= rxq->queue_size / 2 &&\n\t\t\t     !emergency)) {\n\t\t\tiwl_pcie_rx_move_to_allocator(rxq, rba);\n\t\t\temergency = true;\n\t\t\tIWL_DEBUG_TPT(trans,\n\t\t\t\t      \"RX path is in emergency. Pending allocations %d\\n\",\n\t\t\t\t      rb_pending_alloc);\n\t\t}\n\n\t\tIWL_DEBUG_RX(trans, \"Q %d: HW = %d, SW = %d\\n\", rxq->id, r, i);\n\n\t\trxb = iwl_pcie_get_rxb(trans, rxq, i, &join);\n\t\tif (!rxb)\n\t\t\tgoto out;\n\n\t\tif (unlikely(join || rxq->next_rb_is_fragment)) {\n\t\t\trxq->next_rb_is_fragment = join;\n\t\t\t \n\t\t\tlist_add_tail(&rxb->list, &rxq->rx_free);\n\t\t\trxq->free_count++;\n\t\t} else {\n\t\t\tiwl_pcie_rx_handle_rb(trans, rxq, rxb, emergency, i);\n\t\t}\n\n\t\ti = (i + 1) & (rxq->queue_size - 1);\n\n\t\t \n\t\tif (rxq->used_count >= RX_CLAIM_REQ_ALLOC)\n\t\t\tiwl_pcie_rx_allocator_get(trans, rxq);\n\n\t\tif (rxq->used_count % RX_CLAIM_REQ_ALLOC == 0 && !emergency) {\n\t\t\t \n\t\t\tiwl_pcie_rx_move_to_allocator(rxq, rba);\n\t\t} else if (emergency) {\n\t\t\tcount++;\n\t\t\tif (count == 8) {\n\t\t\t\tcount = 0;\n\t\t\t\tif (rb_pending_alloc < rxq->queue_size / 3) {\n\t\t\t\t\tIWL_DEBUG_TPT(trans,\n\t\t\t\t\t\t      \"RX path exited emergency. Pending allocations %d\\n\",\n\t\t\t\t\t\t      rb_pending_alloc);\n\t\t\t\t\temergency = false;\n\t\t\t\t}\n\n\t\t\t\trxq->read = i;\n\t\t\t\tspin_unlock(&rxq->lock);\n\t\t\t\tiwl_pcie_rxq_alloc_rbs(trans, GFP_ATOMIC, rxq);\n\t\t\t\tiwl_pcie_rxq_restock(trans, rxq);\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t}\n\t}\nout:\n\t \n\trxq->read = i;\n\tspin_unlock(&rxq->lock);\n\n\t \n\tif (unlikely(emergency && count))\n\t\tiwl_pcie_rxq_alloc_rbs(trans, GFP_ATOMIC, rxq);\n\n\tiwl_pcie_rxq_restock(trans, rxq);\n\n\treturn handled;\n}\n\nstatic struct iwl_trans_pcie *iwl_pcie_get_trans_pcie(struct msix_entry *entry)\n{\n\tu8 queue = entry->entry;\n\tstruct msix_entry *entries = entry - queue;\n\n\treturn container_of(entries, struct iwl_trans_pcie, msix_entries[0]);\n}\n\n \nirqreturn_t iwl_pcie_irq_rx_msix_handler(int irq, void *dev_id)\n{\n\tstruct msix_entry *entry = dev_id;\n\tstruct iwl_trans_pcie *trans_pcie = iwl_pcie_get_trans_pcie(entry);\n\tstruct iwl_trans *trans = trans_pcie->trans;\n\tstruct iwl_rxq *rxq;\n\n\ttrace_iwlwifi_dev_irq_msix(trans->dev, entry, false, 0, 0);\n\n\tif (WARN_ON(entry->entry >= trans->num_rx_queues))\n\t\treturn IRQ_NONE;\n\n\tif (!trans_pcie->rxq) {\n\t\tif (net_ratelimit())\n\t\t\tIWL_ERR(trans,\n\t\t\t\t\"[%d] Got MSI-X interrupt before we have Rx queues\\n\",\n\t\t\t\tentry->entry);\n\t\treturn IRQ_NONE;\n\t}\n\n\trxq = &trans_pcie->rxq[entry->entry];\n\tlock_map_acquire(&trans->sync_cmd_lockdep_map);\n\tIWL_DEBUG_ISR(trans, \"[%d] Got interrupt\\n\", entry->entry);\n\n\tlocal_bh_disable();\n\tif (napi_schedule_prep(&rxq->napi))\n\t\t__napi_schedule(&rxq->napi);\n\telse\n\t\tiwl_pcie_clear_irq(trans, entry->entry);\n\tlocal_bh_enable();\n\n\tlock_map_release(&trans->sync_cmd_lockdep_map);\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic void iwl_pcie_irq_handle_error(struct iwl_trans *trans)\n{\n\tint i;\n\n\t \n\tif (trans->cfg->internal_wimax_coex &&\n\t    !trans->cfg->apmg_not_supported &&\n\t    (!(iwl_read_prph(trans, APMG_CLK_CTRL_REG) &\n\t\t\t     APMS_CLK_VAL_MRB_FUNC_MODE) ||\n\t     (iwl_read_prph(trans, APMG_PS_CTRL_REG) &\n\t\t\t    APMG_PS_CTRL_VAL_RESET_REQ))) {\n\t\tclear_bit(STATUS_SYNC_HCMD_ACTIVE, &trans->status);\n\t\tiwl_op_mode_wimax_active(trans->op_mode);\n\t\twake_up(&trans->wait_command_queue);\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < trans->trans_cfg->base_params->num_of_queues; i++) {\n\t\tif (!trans->txqs.txq[i])\n\t\t\tcontinue;\n\t\tdel_timer(&trans->txqs.txq[i]->stuck_timer);\n\t}\n\n\t \n\tiwl_trans_fw_error(trans, false);\n\n\tclear_bit(STATUS_SYNC_HCMD_ACTIVE, &trans->status);\n\twake_up(&trans->wait_command_queue);\n}\n\nstatic u32 iwl_pcie_int_cause_non_ict(struct iwl_trans *trans)\n{\n\tu32 inta;\n\n\tlockdep_assert_held(&IWL_TRANS_GET_PCIE_TRANS(trans)->irq_lock);\n\n\ttrace_iwlwifi_dev_irq(trans->dev);\n\n\t \n\tinta = iwl_read32(trans, CSR_INT);\n\n\t \n\treturn inta;\n}\n\n \n#define ICT_SHIFT\t12\n#define ICT_SIZE\t(1 << ICT_SHIFT)\n#define ICT_COUNT\t(ICT_SIZE / sizeof(u32))\n\n \nstatic u32 iwl_pcie_int_cause_ict(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tu32 inta;\n\tu32 val = 0;\n\tu32 read;\n\n\ttrace_iwlwifi_dev_irq(trans->dev);\n\n\t \n\tread = le32_to_cpu(trans_pcie->ict_tbl[trans_pcie->ict_index]);\n\ttrace_iwlwifi_dev_ict_read(trans->dev, trans_pcie->ict_index, read);\n\tif (!read)\n\t\treturn 0;\n\n\t \n\tdo {\n\t\tval |= read;\n\t\tIWL_DEBUG_ISR(trans, \"ICT index %d value 0x%08X\\n\",\n\t\t\t\ttrans_pcie->ict_index, read);\n\t\ttrans_pcie->ict_tbl[trans_pcie->ict_index] = 0;\n\t\ttrans_pcie->ict_index =\n\t\t\t((trans_pcie->ict_index + 1) & (ICT_COUNT - 1));\n\n\t\tread = le32_to_cpu(trans_pcie->ict_tbl[trans_pcie->ict_index]);\n\t\ttrace_iwlwifi_dev_ict_read(trans->dev, trans_pcie->ict_index,\n\t\t\t\t\t   read);\n\t} while (read);\n\n\t \n\tif (val == 0xffffffff)\n\t\tval = 0;\n\n\t \n\tif (val & 0xC0000)\n\t\tval |= 0x8000;\n\n\tinta = (0xff & val) | ((0xff00 & val) << 16);\n\treturn inta;\n}\n\nvoid iwl_pcie_handle_rfkill_irq(struct iwl_trans *trans, bool from_irq)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct isr_statistics *isr_stats = &trans_pcie->isr_stats;\n\tbool hw_rfkill, prev, report;\n\n\tmutex_lock(&trans_pcie->mutex);\n\tprev = test_bit(STATUS_RFKILL_OPMODE, &trans->status);\n\thw_rfkill = iwl_is_rfkill_set(trans);\n\tif (hw_rfkill) {\n\t\tset_bit(STATUS_RFKILL_OPMODE, &trans->status);\n\t\tset_bit(STATUS_RFKILL_HW, &trans->status);\n\t}\n\tif (trans_pcie->opmode_down)\n\t\treport = hw_rfkill;\n\telse\n\t\treport = test_bit(STATUS_RFKILL_OPMODE, &trans->status);\n\n\tIWL_WARN(trans, \"RF_KILL bit toggled to %s.\\n\",\n\t\t hw_rfkill ? \"disable radio\" : \"enable radio\");\n\n\tisr_stats->rfkill++;\n\n\tif (prev != report)\n\t\tiwl_trans_pcie_rf_kill(trans, report, from_irq);\n\tmutex_unlock(&trans_pcie->mutex);\n\n\tif (hw_rfkill) {\n\t\tif (test_and_clear_bit(STATUS_SYNC_HCMD_ACTIVE,\n\t\t\t\t       &trans->status))\n\t\t\tIWL_DEBUG_RF_KILL(trans,\n\t\t\t\t\t  \"Rfkill while SYNC HCMD in flight\\n\");\n\t\twake_up(&trans->wait_command_queue);\n\t} else {\n\t\tclear_bit(STATUS_RFKILL_HW, &trans->status);\n\t\tif (trans_pcie->opmode_down)\n\t\t\tclear_bit(STATUS_RFKILL_OPMODE, &trans->status);\n\t}\n}\n\nirqreturn_t iwl_pcie_irq_handler(int irq, void *dev_id)\n{\n\tstruct iwl_trans *trans = dev_id;\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct isr_statistics *isr_stats = &trans_pcie->isr_stats;\n\tu32 inta = 0;\n\tu32 handled = 0;\n\tbool polling = false;\n\n\tlock_map_acquire(&trans->sync_cmd_lockdep_map);\n\n\tspin_lock_bh(&trans_pcie->irq_lock);\n\n\t \n\tif (likely(trans_pcie->use_ict))\n\t\tinta = iwl_pcie_int_cause_ict(trans);\n\telse\n\t\tinta = iwl_pcie_int_cause_non_ict(trans);\n\n\tif (iwl_have_debug_level(IWL_DL_ISR)) {\n\t\tIWL_DEBUG_ISR(trans,\n\t\t\t      \"ISR inta 0x%08x, enabled 0x%08x(sw), enabled(hw) 0x%08x, fh 0x%08x\\n\",\n\t\t\t      inta, trans_pcie->inta_mask,\n\t\t\t      iwl_read32(trans, CSR_INT_MASK),\n\t\t\t      iwl_read32(trans, CSR_FH_INT_STATUS));\n\t\tif (inta & (~trans_pcie->inta_mask))\n\t\t\tIWL_DEBUG_ISR(trans,\n\t\t\t\t      \"We got a masked interrupt (0x%08x)\\n\",\n\t\t\t\t      inta & (~trans_pcie->inta_mask));\n\t}\n\n\tinta &= trans_pcie->inta_mask;\n\n\t \n\tif (unlikely(!inta)) {\n\t\tIWL_DEBUG_ISR(trans, \"Ignore interrupt, inta == 0\\n\");\n\t\t \n\t\tif (test_bit(STATUS_INT_ENABLED, &trans->status))\n\t\t\t_iwl_enable_interrupts(trans);\n\t\tspin_unlock_bh(&trans_pcie->irq_lock);\n\t\tlock_map_release(&trans->sync_cmd_lockdep_map);\n\t\treturn IRQ_NONE;\n\t}\n\n\tif (unlikely(inta == 0xFFFFFFFF || iwl_trans_is_hw_error_value(inta))) {\n\t\t \n\t\tIWL_WARN(trans, \"HARDWARE GONE?? INTA == 0x%08x\\n\", inta);\n\t\tspin_unlock_bh(&trans_pcie->irq_lock);\n\t\tgoto out;\n\t}\n\n\t \n\t \n\tiwl_write32(trans, CSR_INT, inta | ~trans_pcie->inta_mask);\n\n\tif (iwl_have_debug_level(IWL_DL_ISR))\n\t\tIWL_DEBUG_ISR(trans, \"inta 0x%08x, enabled 0x%08x\\n\",\n\t\t\t      inta, iwl_read32(trans, CSR_INT_MASK));\n\n\tspin_unlock_bh(&trans_pcie->irq_lock);\n\n\t \n\tif (inta & CSR_INT_BIT_HW_ERR) {\n\t\tIWL_ERR(trans, \"Hardware error detected.  Restarting.\\n\");\n\n\t\t \n\t\tiwl_disable_interrupts(trans);\n\n\t\tisr_stats->hw++;\n\t\tiwl_pcie_irq_handle_error(trans);\n\n\t\thandled |= CSR_INT_BIT_HW_ERR;\n\n\t\tgoto out;\n\t}\n\n\t \n\tif (inta & CSR_INT_BIT_SCD) {\n\t\tIWL_DEBUG_ISR(trans,\n\t\t\t      \"Scheduler finished to transmit the frame/frames.\\n\");\n\t\tisr_stats->sch++;\n\t}\n\n\t \n\tif (inta & CSR_INT_BIT_ALIVE) {\n\t\tIWL_DEBUG_ISR(trans, \"Alive interrupt\\n\");\n\t\tisr_stats->alive++;\n\t\tif (trans->trans_cfg->gen2) {\n\t\t\t \n\t\t\tiwl_pcie_rxmq_restock(trans, trans_pcie->rxq);\n\t\t}\n\n\t\thandled |= CSR_INT_BIT_ALIVE;\n\t}\n\n\t \n\tinta &= ~(CSR_INT_BIT_SCD | CSR_INT_BIT_ALIVE);\n\n\t \n\tif (inta & CSR_INT_BIT_RF_KILL) {\n\t\tiwl_pcie_handle_rfkill_irq(trans, true);\n\t\thandled |= CSR_INT_BIT_RF_KILL;\n\t}\n\n\t \n\tif (inta & CSR_INT_BIT_CT_KILL) {\n\t\tIWL_ERR(trans, \"Microcode CT kill error detected.\\n\");\n\t\tisr_stats->ctkill++;\n\t\thandled |= CSR_INT_BIT_CT_KILL;\n\t}\n\n\t \n\tif (inta & CSR_INT_BIT_SW_ERR) {\n\t\tIWL_ERR(trans, \"Microcode SW error detected. \"\n\t\t\t\" Restarting 0x%X.\\n\", inta);\n\t\tisr_stats->sw++;\n\t\tiwl_pcie_irq_handle_error(trans);\n\t\thandled |= CSR_INT_BIT_SW_ERR;\n\t}\n\n\t \n\tif (inta & CSR_INT_BIT_WAKEUP) {\n\t\tIWL_DEBUG_ISR(trans, \"Wakeup interrupt\\n\");\n\t\tiwl_pcie_rxq_check_wrptr(trans);\n\t\tiwl_pcie_txq_check_wrptrs(trans);\n\n\t\tisr_stats->wakeup++;\n\n\t\thandled |= CSR_INT_BIT_WAKEUP;\n\t}\n\n\t \n\tif (inta & (CSR_INT_BIT_FH_RX | CSR_INT_BIT_SW_RX |\n\t\t    CSR_INT_BIT_RX_PERIODIC)) {\n\t\tIWL_DEBUG_ISR(trans, \"Rx interrupt\\n\");\n\t\tif (inta & (CSR_INT_BIT_FH_RX | CSR_INT_BIT_SW_RX)) {\n\t\t\thandled |= (CSR_INT_BIT_FH_RX | CSR_INT_BIT_SW_RX);\n\t\t\tiwl_write32(trans, CSR_FH_INT_STATUS,\n\t\t\t\t\tCSR_FH_INT_RX_MASK);\n\t\t}\n\t\tif (inta & CSR_INT_BIT_RX_PERIODIC) {\n\t\t\thandled |= CSR_INT_BIT_RX_PERIODIC;\n\t\t\tiwl_write32(trans,\n\t\t\t\tCSR_INT, CSR_INT_BIT_RX_PERIODIC);\n\t\t}\n\t\t \n\n\t\t \n\t\tiwl_write8(trans, CSR_INT_PERIODIC_REG,\n\t\t\t    CSR_INT_PERIODIC_DIS);\n\n\t\t \n\t\tif (inta & (CSR_INT_BIT_FH_RX | CSR_INT_BIT_SW_RX))\n\t\t\tiwl_write8(trans, CSR_INT_PERIODIC_REG,\n\t\t\t\t   CSR_INT_PERIODIC_ENA);\n\n\t\tisr_stats->rx++;\n\n\t\tlocal_bh_disable();\n\t\tif (napi_schedule_prep(&trans_pcie->rxq[0].napi)) {\n\t\t\tpolling = true;\n\t\t\t__napi_schedule(&trans_pcie->rxq[0].napi);\n\t\t}\n\t\tlocal_bh_enable();\n\t}\n\n\t \n\tif (inta & CSR_INT_BIT_FH_TX) {\n\t\tiwl_write32(trans, CSR_FH_INT_STATUS, CSR_FH_INT_TX_MASK);\n\t\tIWL_DEBUG_ISR(trans, \"uCode load interrupt\\n\");\n\t\tisr_stats->tx++;\n\t\thandled |= CSR_INT_BIT_FH_TX;\n\t\t \n\t\ttrans_pcie->ucode_write_complete = true;\n\t\twake_up(&trans_pcie->ucode_write_waitq);\n\t\t \n\t\tif (trans_pcie->imr_status == IMR_D2S_REQUESTED) {\n\t\t\ttrans_pcie->imr_status = IMR_D2S_COMPLETED;\n\t\t\twake_up(&trans_pcie->ucode_write_waitq);\n\t\t}\n\t}\n\n\tif (inta & ~handled) {\n\t\tIWL_ERR(trans, \"Unhandled INTA bits 0x%08x\\n\", inta & ~handled);\n\t\tisr_stats->unhandled++;\n\t}\n\n\tif (inta & ~(trans_pcie->inta_mask)) {\n\t\tIWL_WARN(trans, \"Disabled INTA bits 0x%08x were pending\\n\",\n\t\t\t inta & ~trans_pcie->inta_mask);\n\t}\n\n\tif (!polling) {\n\t\tspin_lock_bh(&trans_pcie->irq_lock);\n\t\t \n\t\tif (test_bit(STATUS_INT_ENABLED, &trans->status))\n\t\t\t_iwl_enable_interrupts(trans);\n\t\t \n\t\telse if (handled & CSR_INT_BIT_FH_TX)\n\t\t\tiwl_enable_fw_load_int(trans);\n\t\t \n\t\telse if (handled & CSR_INT_BIT_RF_KILL)\n\t\t\tiwl_enable_rfkill_int(trans);\n\t\t \n\t\telse if (handled & (CSR_INT_BIT_ALIVE | CSR_INT_BIT_FH_RX))\n\t\t\tiwl_enable_fw_load_int_ctx_info(trans);\n\t\tspin_unlock_bh(&trans_pcie->irq_lock);\n\t}\n\nout:\n\tlock_map_release(&trans->sync_cmd_lockdep_map);\n\treturn IRQ_HANDLED;\n}\n\n \n\n \nvoid iwl_pcie_free_ict(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\tif (trans_pcie->ict_tbl) {\n\t\tdma_free_coherent(trans->dev, ICT_SIZE,\n\t\t\t\t  trans_pcie->ict_tbl,\n\t\t\t\t  trans_pcie->ict_tbl_dma);\n\t\ttrans_pcie->ict_tbl = NULL;\n\t\ttrans_pcie->ict_tbl_dma = 0;\n\t}\n}\n\n \nint iwl_pcie_alloc_ict(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\ttrans_pcie->ict_tbl =\n\t\tdma_alloc_coherent(trans->dev, ICT_SIZE,\n\t\t\t\t   &trans_pcie->ict_tbl_dma, GFP_KERNEL);\n\tif (!trans_pcie->ict_tbl)\n\t\treturn -ENOMEM;\n\n\t \n\tif (WARN_ON(trans_pcie->ict_tbl_dma & (ICT_SIZE - 1))) {\n\t\tiwl_pcie_free_ict(trans);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nvoid iwl_pcie_reset_ict(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tu32 val;\n\n\tif (!trans_pcie->ict_tbl)\n\t\treturn;\n\n\tspin_lock_bh(&trans_pcie->irq_lock);\n\t_iwl_disable_interrupts(trans);\n\n\tmemset(trans_pcie->ict_tbl, 0, ICT_SIZE);\n\n\tval = trans_pcie->ict_tbl_dma >> ICT_SHIFT;\n\n\tval |= CSR_DRAM_INT_TBL_ENABLE |\n\t       CSR_DRAM_INIT_TBL_WRAP_CHECK |\n\t       CSR_DRAM_INIT_TBL_WRITE_POINTER;\n\n\tIWL_DEBUG_ISR(trans, \"CSR_DRAM_INT_TBL_REG =0x%x\\n\", val);\n\n\tiwl_write32(trans, CSR_DRAM_INT_TBL_REG, val);\n\ttrans_pcie->use_ict = true;\n\ttrans_pcie->ict_index = 0;\n\tiwl_write32(trans, CSR_INT, trans_pcie->inta_mask);\n\t_iwl_enable_interrupts(trans);\n\tspin_unlock_bh(&trans_pcie->irq_lock);\n}\n\n \nvoid iwl_pcie_disable_ict(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\tspin_lock_bh(&trans_pcie->irq_lock);\n\ttrans_pcie->use_ict = false;\n\tspin_unlock_bh(&trans_pcie->irq_lock);\n}\n\nirqreturn_t iwl_pcie_isr(int irq, void *data)\n{\n\tstruct iwl_trans *trans = data;\n\n\tif (!trans)\n\t\treturn IRQ_NONE;\n\n\t \n\tiwl_write32(trans, CSR_INT_MASK, 0x00000000);\n\n\treturn IRQ_WAKE_THREAD;\n}\n\nirqreturn_t iwl_pcie_msix_isr(int irq, void *data)\n{\n\treturn IRQ_WAKE_THREAD;\n}\n\nirqreturn_t iwl_pcie_irq_msix_handler(int irq, void *dev_id)\n{\n\tstruct msix_entry *entry = dev_id;\n\tstruct iwl_trans_pcie *trans_pcie = iwl_pcie_get_trans_pcie(entry);\n\tstruct iwl_trans *trans = trans_pcie->trans;\n\tstruct isr_statistics *isr_stats = &trans_pcie->isr_stats;\n\tu32 inta_fh_msk = ~MSIX_FH_INT_CAUSES_DATA_QUEUE;\n\tu32 inta_fh, inta_hw;\n\tbool polling = false;\n\tbool sw_err;\n\n\tif (trans_pcie->shared_vec_mask & IWL_SHARED_IRQ_NON_RX)\n\t\tinta_fh_msk |= MSIX_FH_INT_CAUSES_Q0;\n\n\tif (trans_pcie->shared_vec_mask & IWL_SHARED_IRQ_FIRST_RSS)\n\t\tinta_fh_msk |= MSIX_FH_INT_CAUSES_Q1;\n\n\tlock_map_acquire(&trans->sync_cmd_lockdep_map);\n\n\tspin_lock_bh(&trans_pcie->irq_lock);\n\tinta_fh = iwl_read32(trans, CSR_MSIX_FH_INT_CAUSES_AD);\n\tinta_hw = iwl_read32(trans, CSR_MSIX_HW_INT_CAUSES_AD);\n\t \n\tiwl_write32(trans, CSR_MSIX_FH_INT_CAUSES_AD, inta_fh & inta_fh_msk);\n\tiwl_write32(trans, CSR_MSIX_HW_INT_CAUSES_AD, inta_hw);\n\tspin_unlock_bh(&trans_pcie->irq_lock);\n\n\ttrace_iwlwifi_dev_irq_msix(trans->dev, entry, true, inta_fh, inta_hw);\n\n\tif (unlikely(!(inta_fh | inta_hw))) {\n\t\tIWL_DEBUG_ISR(trans, \"Ignore interrupt, inta == 0\\n\");\n\t\tlock_map_release(&trans->sync_cmd_lockdep_map);\n\t\treturn IRQ_NONE;\n\t}\n\n\tif (iwl_have_debug_level(IWL_DL_ISR)) {\n\t\tIWL_DEBUG_ISR(trans,\n\t\t\t      \"ISR[%d] inta_fh 0x%08x, enabled (sw) 0x%08x (hw) 0x%08x\\n\",\n\t\t\t      entry->entry, inta_fh, trans_pcie->fh_mask,\n\t\t\t      iwl_read32(trans, CSR_MSIX_FH_INT_MASK_AD));\n\t\tif (inta_fh & ~trans_pcie->fh_mask)\n\t\t\tIWL_DEBUG_ISR(trans,\n\t\t\t\t      \"We got a masked interrupt (0x%08x)\\n\",\n\t\t\t\t      inta_fh & ~trans_pcie->fh_mask);\n\t}\n\n\tinta_fh &= trans_pcie->fh_mask;\n\n\tif ((trans_pcie->shared_vec_mask & IWL_SHARED_IRQ_NON_RX) &&\n\t    inta_fh & MSIX_FH_INT_CAUSES_Q0) {\n\t\tlocal_bh_disable();\n\t\tif (napi_schedule_prep(&trans_pcie->rxq[0].napi)) {\n\t\t\tpolling = true;\n\t\t\t__napi_schedule(&trans_pcie->rxq[0].napi);\n\t\t}\n\t\tlocal_bh_enable();\n\t}\n\n\tif ((trans_pcie->shared_vec_mask & IWL_SHARED_IRQ_FIRST_RSS) &&\n\t    inta_fh & MSIX_FH_INT_CAUSES_Q1) {\n\t\tlocal_bh_disable();\n\t\tif (napi_schedule_prep(&trans_pcie->rxq[1].napi)) {\n\t\t\tpolling = true;\n\t\t\t__napi_schedule(&trans_pcie->rxq[1].napi);\n\t\t}\n\t\tlocal_bh_enable();\n\t}\n\n\t \n\tif (inta_fh & MSIX_FH_INT_CAUSES_D2S_CH0_NUM &&\n\t    trans_pcie->imr_status == IMR_D2S_REQUESTED) {\n\t\tIWL_DEBUG_ISR(trans, \"IMR Complete interrupt\\n\");\n\t\tisr_stats->tx++;\n\n\t\t \n\t\tif (trans_pcie->imr_status == IMR_D2S_REQUESTED) {\n\t\t\ttrans_pcie->imr_status = IMR_D2S_COMPLETED;\n\t\t\twake_up(&trans_pcie->ucode_write_waitq);\n\t\t}\n\t} else if (inta_fh & MSIX_FH_INT_CAUSES_D2S_CH0_NUM) {\n\t\tIWL_DEBUG_ISR(trans, \"uCode load interrupt\\n\");\n\t\tisr_stats->tx++;\n\t\t \n\t\ttrans_pcie->ucode_write_complete = true;\n\t\twake_up(&trans_pcie->ucode_write_waitq);\n\n\t\t \n\t\tif (trans_pcie->imr_status == IMR_D2S_REQUESTED) {\n\t\t\ttrans_pcie->imr_status = IMR_D2S_COMPLETED;\n\t\t\twake_up(&trans_pcie->ucode_write_waitq);\n\t\t}\n\t}\n\n\tif (trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_BZ)\n\t\tsw_err = inta_hw & MSIX_HW_INT_CAUSES_REG_SW_ERR_BZ;\n\telse\n\t\tsw_err = inta_hw & MSIX_HW_INT_CAUSES_REG_SW_ERR;\n\n\t \n\tif ((inta_fh & MSIX_FH_INT_CAUSES_FH_ERR) || sw_err) {\n\t\tIWL_ERR(trans,\n\t\t\t\"Microcode SW error detected. Restarting 0x%X.\\n\",\n\t\t\tinta_fh);\n\t\tisr_stats->sw++;\n\t\t \n\t\tif (trans_pcie->imr_status == IMR_D2S_REQUESTED) {\n\t\t\ttrans_pcie->imr_status = IMR_D2S_ERROR;\n\t\t\twake_up(&trans_pcie->imr_waitq);\n\t\t} else if (trans_pcie->fw_reset_state == FW_RESET_REQUESTED) {\n\t\t\ttrans_pcie->fw_reset_state = FW_RESET_ERROR;\n\t\t\twake_up(&trans_pcie->fw_reset_waitq);\n\t\t} else {\n\t\t\tiwl_pcie_irq_handle_error(trans);\n\t\t}\n\t}\n\n\t \n\tif (iwl_have_debug_level(IWL_DL_ISR)) {\n\t\tIWL_DEBUG_ISR(trans,\n\t\t\t      \"ISR[%d] inta_hw 0x%08x, enabled (sw) 0x%08x (hw) 0x%08x\\n\",\n\t\t\t      entry->entry, inta_hw, trans_pcie->hw_mask,\n\t\t\t      iwl_read32(trans, CSR_MSIX_HW_INT_MASK_AD));\n\t\tif (inta_hw & ~trans_pcie->hw_mask)\n\t\t\tIWL_DEBUG_ISR(trans,\n\t\t\t\t      \"We got a masked interrupt 0x%08x\\n\",\n\t\t\t\t      inta_hw & ~trans_pcie->hw_mask);\n\t}\n\n\tinta_hw &= trans_pcie->hw_mask;\n\n\t \n\tif (inta_hw & MSIX_HW_INT_CAUSES_REG_ALIVE) {\n\t\tIWL_DEBUG_ISR(trans, \"Alive interrupt\\n\");\n\t\tisr_stats->alive++;\n\t\tif (trans->trans_cfg->gen2) {\n\t\t\t \n\t\t\tiwl_pcie_rxmq_restock(trans, trans_pcie->rxq);\n\t\t}\n\t}\n\n\t \n\tif (inta_hw & MSIX_HW_INT_CAUSES_REG_WAKEUP && trans_pcie->prph_info) {\n\t\tu32 sleep_notif =\n\t\t\tle32_to_cpu(trans_pcie->prph_info->sleep_notif);\n\t\tif (sleep_notif == IWL_D3_SLEEP_STATUS_SUSPEND ||\n\t\t    sleep_notif == IWL_D3_SLEEP_STATUS_RESUME) {\n\t\t\tIWL_DEBUG_ISR(trans,\n\t\t\t\t      \"Sx interrupt: sleep notification = 0x%x\\n\",\n\t\t\t\t      sleep_notif);\n\t\t\ttrans_pcie->sx_complete = true;\n\t\t\twake_up(&trans_pcie->sx_waitq);\n\t\t} else {\n\t\t\t \n\t\t\tIWL_DEBUG_ISR(trans, \"Wakeup interrupt\\n\");\n\t\t\tiwl_pcie_rxq_check_wrptr(trans);\n\t\t\tiwl_pcie_txq_check_wrptrs(trans);\n\n\t\t\tisr_stats->wakeup++;\n\t\t}\n\t}\n\n\t \n\tif (inta_hw & MSIX_HW_INT_CAUSES_REG_CT_KILL) {\n\t\tIWL_ERR(trans, \"Microcode CT kill error detected.\\n\");\n\t\tisr_stats->ctkill++;\n\t}\n\n\t \n\tif (inta_hw & MSIX_HW_INT_CAUSES_REG_RF_KILL)\n\t\tiwl_pcie_handle_rfkill_irq(trans, true);\n\n\tif (inta_hw & MSIX_HW_INT_CAUSES_REG_HW_ERR) {\n\t\tIWL_ERR(trans,\n\t\t\t\"Hardware error detected. Restarting.\\n\");\n\n\t\tisr_stats->hw++;\n\t\ttrans->dbg.hw_error = true;\n\t\tiwl_pcie_irq_handle_error(trans);\n\t}\n\n\tif (inta_hw & MSIX_HW_INT_CAUSES_REG_RESET_DONE) {\n\t\tIWL_DEBUG_ISR(trans, \"Reset flow completed\\n\");\n\t\ttrans_pcie->fw_reset_state = FW_RESET_OK;\n\t\twake_up(&trans_pcie->fw_reset_waitq);\n\t}\n\n\tif (!polling)\n\t\tiwl_pcie_clear_irq(trans, entry->entry);\n\n\tlock_map_release(&trans->sync_cmd_lockdep_map);\n\n\treturn IRQ_HANDLED;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}