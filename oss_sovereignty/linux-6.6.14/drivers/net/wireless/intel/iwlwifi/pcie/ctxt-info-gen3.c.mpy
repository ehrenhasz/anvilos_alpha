{
  "module_name": "ctxt-info-gen3.c",
  "hash_id": "90795e86f878fd15eda2353c933562175131defe518fb4198761bab89fb87883",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wireless/intel/iwlwifi/pcie/ctxt-info-gen3.c",
  "human_readable_source": "\n \n#include \"iwl-trans.h\"\n#include \"iwl-fh.h\"\n#include \"iwl-context-info-gen3.h\"\n#include \"internal.h\"\n#include \"iwl-prph.h\"\n\nstatic void\niwl_pcie_ctxt_info_dbg_enable(struct iwl_trans *trans,\n\t\t\t      struct iwl_prph_scratch_hwm_cfg *dbg_cfg,\n\t\t\t      u32 *control_flags)\n{\n\tenum iwl_fw_ini_allocation_id alloc_id = IWL_FW_INI_ALLOCATION_ID_DBGC1;\n\tstruct iwl_fw_ini_allocation_tlv *fw_mon_cfg;\n\tu32 dbg_flags = 0;\n\n\tif (!iwl_trans_dbg_ini_valid(trans)) {\n\t\tstruct iwl_dram_data *fw_mon = &trans->dbg.fw_mon;\n\n\t\tiwl_pcie_alloc_fw_monitor(trans, 0);\n\n\t\tif (fw_mon->size) {\n\t\t\tdbg_flags |= IWL_PRPH_SCRATCH_EDBG_DEST_DRAM;\n\n\t\t\tIWL_DEBUG_FW(trans,\n\t\t\t\t     \"WRT: Applying DRAM buffer destination\\n\");\n\n\t\t\tdbg_cfg->hwm_base_addr = cpu_to_le64(fw_mon->physical);\n\t\t\tdbg_cfg->hwm_size = cpu_to_le32(fw_mon->size);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tfw_mon_cfg = &trans->dbg.fw_mon_cfg[alloc_id];\n\n\tswitch (le32_to_cpu(fw_mon_cfg->buf_location)) {\n\tcase IWL_FW_INI_LOCATION_SRAM_PATH:\n\t\tdbg_flags |= IWL_PRPH_SCRATCH_EDBG_DEST_INTERNAL;\n\t\tIWL_DEBUG_FW(trans,\n\t\t\t\t\"WRT: Applying SMEM buffer destination\\n\");\n\t\tbreak;\n\n\tcase IWL_FW_INI_LOCATION_NPK_PATH:\n\t\tdbg_flags |= IWL_PRPH_SCRATCH_EDBG_DEST_TB22DTF;\n\t\tIWL_DEBUG_FW(trans,\n\t\t\t     \"WRT: Applying NPK buffer destination\\n\");\n\t\tbreak;\n\n\tcase IWL_FW_INI_LOCATION_DRAM_PATH:\n\t\tif (trans->dbg.fw_mon_ini[alloc_id].num_frags) {\n\t\t\tstruct iwl_dram_data *frag =\n\t\t\t\t&trans->dbg.fw_mon_ini[alloc_id].frags[0];\n\t\t\tdbg_flags |= IWL_PRPH_SCRATCH_EDBG_DEST_DRAM;\n\t\t\tdbg_cfg->hwm_base_addr = cpu_to_le64(frag->physical);\n\t\t\tdbg_cfg->hwm_size = cpu_to_le32(frag->size);\n\t\t\tdbg_cfg->debug_token_config = cpu_to_le32(trans->dbg.ucode_preset);\n\t\t\tIWL_DEBUG_FW(trans,\n\t\t\t\t     \"WRT: Applying DRAM destination (debug_token_config=%u)\\n\",\n\t\t\t\t     dbg_cfg->debug_token_config);\n\t\t\tIWL_DEBUG_FW(trans,\n\t\t\t\t     \"WRT: Applying DRAM destination (alloc_id=%u, num_frags=%u)\\n\",\n\t\t\t\t     alloc_id,\n\t\t\t\t     trans->dbg.fw_mon_ini[alloc_id].num_frags);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tIWL_ERR(trans, \"WRT: Invalid buffer destination\\n\");\n\t}\nout:\n\tif (dbg_flags)\n\t\t*control_flags |= IWL_PRPH_SCRATCH_EARLY_DEBUG_EN | dbg_flags;\n}\n\nint iwl_pcie_ctxt_info_gen3_init(struct iwl_trans *trans,\n\t\t\t\t const struct fw_img *fw)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_context_info_gen3 *ctxt_info_gen3;\n\tstruct iwl_prph_scratch *prph_scratch;\n\tstruct iwl_prph_scratch_ctrl_cfg *prph_sc_ctrl;\n\tstruct iwl_prph_info *prph_info;\n\tu32 control_flags = 0;\n\tint ret;\n\tint cmdq_size = max_t(u32, IWL_CMD_QUEUE_SIZE,\n\t\t\t      trans->cfg->min_txq_size);\n\n\tswitch (trans_pcie->rx_buf_size) {\n\tcase IWL_AMSDU_DEF:\n\t\treturn -EINVAL;\n\tcase IWL_AMSDU_2K:\n\t\tbreak;\n\tcase IWL_AMSDU_4K:\n\t\tcontrol_flags |= IWL_PRPH_SCRATCH_RB_SIZE_4K;\n\t\tbreak;\n\tcase IWL_AMSDU_8K:\n\t\tcontrol_flags |= IWL_PRPH_SCRATCH_RB_SIZE_4K;\n\t\t \n\t\tcontrol_flags |= IWL_PRPH_SCRATCH_RB_SIZE_EXT_8K;\n\t\tbreak;\n\tcase IWL_AMSDU_12K:\n\t\tcontrol_flags |= IWL_PRPH_SCRATCH_RB_SIZE_4K;\n\t\t \n\t\tcontrol_flags |= IWL_PRPH_SCRATCH_RB_SIZE_EXT_16K;\n\t\tbreak;\n\t}\n\n\t \n\tprph_scratch = dma_alloc_coherent(trans->dev, sizeof(*prph_scratch),\n\t\t\t\t\t  &trans_pcie->prph_scratch_dma_addr,\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!prph_scratch)\n\t\treturn -ENOMEM;\n\n\tprph_sc_ctrl = &prph_scratch->ctrl_cfg;\n\n\tprph_sc_ctrl->version.version = 0;\n\tprph_sc_ctrl->version.mac_id =\n\t\tcpu_to_le16((u16)iwl_read32(trans, CSR_HW_REV));\n\tprph_sc_ctrl->version.size = cpu_to_le16(sizeof(*prph_scratch) / 4);\n\n\tcontrol_flags |= IWL_PRPH_SCRATCH_MTR_MODE;\n\tcontrol_flags |= IWL_PRPH_MTR_FORMAT_256B & IWL_PRPH_SCRATCH_MTR_FORMAT;\n\n\tif (trans->trans_cfg->imr_enabled)\n\t\tcontrol_flags |= IWL_PRPH_SCRATCH_IMR_DEBUG_EN;\n\n\t \n\tprph_sc_ctrl->rbd_cfg.free_rbd_addr =\n\t\tcpu_to_le64(trans_pcie->rxq->bd_dma);\n\n\tiwl_pcie_ctxt_info_dbg_enable(trans, &prph_sc_ctrl->hwm_cfg,\n\t\t\t\t      &control_flags);\n\tprph_sc_ctrl->control.control_flags = cpu_to_le32(control_flags);\n\n\t \n\tprph_sc_ctrl->step_cfg.mbx_addr_0 = cpu_to_le32(trans->mbx_addr_0_step);\n\tprph_sc_ctrl->step_cfg.mbx_addr_1 = cpu_to_le32(trans->mbx_addr_1_step);\n\n\t \n\tret = iwl_pcie_init_fw_sec(trans, fw, &prph_scratch->dram);\n\tif (ret)\n\t\tgoto err_free_prph_scratch;\n\n\n\t \n\tBUILD_BUG_ON(sizeof(*prph_info) > PAGE_SIZE / 2);\n\tprph_info = dma_alloc_coherent(trans->dev, PAGE_SIZE,\n\t\t\t\t       &trans_pcie->prph_info_dma_addr,\n\t\t\t\t       GFP_KERNEL);\n\tif (!prph_info) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_prph_scratch;\n\t}\n\n\t \n\tctxt_info_gen3 = dma_alloc_coherent(trans->dev,\n\t\t\t\t\t    sizeof(*ctxt_info_gen3),\n\t\t\t\t\t    &trans_pcie->ctxt_info_dma_addr,\n\t\t\t\t\t    GFP_KERNEL);\n\tif (!ctxt_info_gen3) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_prph_info;\n\t}\n\n\tctxt_info_gen3->prph_info_base_addr =\n\t\tcpu_to_le64(trans_pcie->prph_info_dma_addr);\n\tctxt_info_gen3->prph_scratch_base_addr =\n\t\tcpu_to_le64(trans_pcie->prph_scratch_dma_addr);\n\tctxt_info_gen3->prph_scratch_size =\n\t\tcpu_to_le32(sizeof(*prph_scratch));\n\tctxt_info_gen3->cr_head_idx_arr_base_addr =\n\t\tcpu_to_le64(trans_pcie->rxq->rb_stts_dma);\n\tctxt_info_gen3->tr_tail_idx_arr_base_addr =\n\t\tcpu_to_le64(trans_pcie->prph_info_dma_addr + PAGE_SIZE / 2);\n\tctxt_info_gen3->cr_tail_idx_arr_base_addr =\n\t\tcpu_to_le64(trans_pcie->prph_info_dma_addr + 3 * PAGE_SIZE / 4);\n\tctxt_info_gen3->mtr_base_addr =\n\t\tcpu_to_le64(trans->txqs.txq[trans->txqs.cmd.q_id]->dma_addr);\n\tctxt_info_gen3->mcr_base_addr =\n\t\tcpu_to_le64(trans_pcie->rxq->used_bd_dma);\n\tctxt_info_gen3->mtr_size =\n\t\tcpu_to_le16(TFD_QUEUE_CB_SIZE(cmdq_size));\n\tctxt_info_gen3->mcr_size =\n\t\tcpu_to_le16(RX_QUEUE_CB_SIZE(trans->cfg->num_rbds));\n\n\ttrans_pcie->ctxt_info_gen3 = ctxt_info_gen3;\n\ttrans_pcie->prph_info = prph_info;\n\ttrans_pcie->prph_scratch = prph_scratch;\n\n\t \n\ttrans_pcie->iml = dma_alloc_coherent(trans->dev, trans->iml_len,\n\t\t\t\t\t     &trans_pcie->iml_dma_addr,\n\t\t\t\t\t     GFP_KERNEL);\n\tif (!trans_pcie->iml) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_ctxt_info;\n\t}\n\n\tmemcpy(trans_pcie->iml, trans->iml, trans->iml_len);\n\n\tiwl_enable_fw_load_int_ctx_info(trans);\n\n\t \n\tiwl_write64(trans, CSR_CTXT_INFO_ADDR,\n\t\t    trans_pcie->ctxt_info_dma_addr);\n\tiwl_write64(trans, CSR_IML_DATA_ADDR,\n\t\t    trans_pcie->iml_dma_addr);\n\tiwl_write32(trans, CSR_IML_SIZE_ADDR, trans->iml_len);\n\n\tiwl_set_bit(trans, CSR_CTXT_INFO_BOOT_CTRL,\n\t\t    CSR_AUTO_FUNC_BOOT_ENA);\n\n\treturn 0;\n\nerr_free_ctxt_info:\n\tdma_free_coherent(trans->dev, sizeof(*trans_pcie->ctxt_info_gen3),\n\t\t\t  trans_pcie->ctxt_info_gen3,\n\t\t\t  trans_pcie->ctxt_info_dma_addr);\n\ttrans_pcie->ctxt_info_gen3 = NULL;\nerr_free_prph_info:\n\tdma_free_coherent(trans->dev, PAGE_SIZE, prph_info,\n\t\t\t  trans_pcie->prph_info_dma_addr);\n\nerr_free_prph_scratch:\n\tdma_free_coherent(trans->dev,\n\t\t\t  sizeof(*prph_scratch),\n\t\t\tprph_scratch,\n\t\t\ttrans_pcie->prph_scratch_dma_addr);\n\treturn ret;\n\n}\n\nvoid iwl_pcie_ctxt_info_gen3_free(struct iwl_trans *trans, bool alive)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\n\tif (trans_pcie->iml) {\n\t\tdma_free_coherent(trans->dev, trans->iml_len, trans_pcie->iml,\n\t\t\t\t  trans_pcie->iml_dma_addr);\n\t\ttrans_pcie->iml_dma_addr = 0;\n\t\ttrans_pcie->iml = NULL;\n\t}\n\n\tiwl_pcie_ctxt_info_free_fw_img(trans);\n\n\tif (alive)\n\t\treturn;\n\n\tif (!trans_pcie->ctxt_info_gen3)\n\t\treturn;\n\n\t \n\tdma_free_coherent(trans->dev, sizeof(*trans_pcie->ctxt_info_gen3),\n\t\t\t  trans_pcie->ctxt_info_gen3,\n\t\t\t  trans_pcie->ctxt_info_dma_addr);\n\ttrans_pcie->ctxt_info_dma_addr = 0;\n\ttrans_pcie->ctxt_info_gen3 = NULL;\n\n\tdma_free_coherent(trans->dev, sizeof(*trans_pcie->prph_scratch),\n\t\t\t  trans_pcie->prph_scratch,\n\t\t\t  trans_pcie->prph_scratch_dma_addr);\n\ttrans_pcie->prph_scratch_dma_addr = 0;\n\ttrans_pcie->prph_scratch = NULL;\n\n\t \n\tdma_free_coherent(trans->dev, PAGE_SIZE, trans_pcie->prph_info,\n\t\t\t  trans_pcie->prph_info_dma_addr);\n\ttrans_pcie->prph_info_dma_addr = 0;\n\ttrans_pcie->prph_info = NULL;\n}\n\nstatic int iwl_pcie_load_payloads_continuously(struct iwl_trans *trans,\n\t\t\t\t\t       const struct iwl_pnvm_image *pnvm_data,\n\t\t\t\t\t       struct iwl_dram_data *dram)\n{\n\tu32 len, len0, len1;\n\n\tif (pnvm_data->n_chunks != UNFRAGMENTED_PNVM_PAYLOADS_NUMBER) {\n\t\tIWL_DEBUG_FW(trans, \"expected 2 payloads, got %d.\\n\",\n\t\t\t     pnvm_data->n_chunks);\n\t\treturn -EINVAL;\n\t}\n\n\tlen0 = pnvm_data->chunks[0].len;\n\tlen1 = pnvm_data->chunks[1].len;\n\tif (len1 > 0xFFFFFFFF - len0) {\n\t\tIWL_DEBUG_FW(trans, \"sizes of payloads overflow.\\n\");\n\t\treturn -EINVAL;\n\t}\n\tlen = len0 + len1;\n\n\tdram->block = iwl_pcie_ctxt_info_dma_alloc_coherent(trans, len,\n\t\t\t\t\t\t\t    &dram->physical);\n\tif (!dram->block) {\n\t\tIWL_DEBUG_FW(trans, \"Failed to allocate PNVM DMA.\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tdram->size = len;\n\tmemcpy(dram->block, pnvm_data->chunks[0].data, len0);\n\tmemcpy((u8 *)dram->block + len0, pnvm_data->chunks[1].data, len1);\n\n\treturn 0;\n}\n\nstatic int iwl_pcie_load_payloads_segments\n\t\t\t\t(struct iwl_trans *trans,\n\t\t\t\t struct iwl_dram_regions *dram_regions,\n\t\t\t\t const struct iwl_pnvm_image *pnvm_data)\n{\n\tstruct iwl_dram_data *cur_payload_dram = &dram_regions->drams[0];\n\tstruct iwl_dram_data *desc_dram = &dram_regions->prph_scratch_mem_desc;\n\tstruct iwl_prph_scrath_mem_desc_addr_array *addresses;\n\tconst void *data;\n\tu32 len;\n\tint i;\n\n\t \n\tlen = sizeof(struct iwl_prph_scrath_mem_desc_addr_array);\n\tdesc_dram->block = iwl_pcie_ctxt_info_dma_alloc_coherent\n\t\t\t\t\t\t(trans,\n\t\t\t\t\t\t len,\n\t\t\t\t\t\t &desc_dram->physical);\n\tif (!desc_dram->block) {\n\t\tIWL_DEBUG_FW(trans, \"Failed to allocate PNVM DMA.\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tdesc_dram->size = len;\n\tmemset(desc_dram->block, 0, len);\n\n\t \n\tdram_regions->n_regions = 0;\n\tfor (i = 0; i < pnvm_data->n_chunks; i++) {\n\t\tlen = pnvm_data->chunks[i].len;\n\t\tdata = pnvm_data->chunks[i].data;\n\n\t\tif (iwl_pcie_ctxt_info_alloc_dma(trans,\n\t\t\t\t\t\t data,\n\t\t\t\t\t\t len,\n\t\t\t\t\t\t cur_payload_dram)) {\n\t\t\tiwl_trans_pcie_free_pnvm_dram_regions(dram_regions,\n\t\t\t\t\t\t\t      trans->dev);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tdram_regions->n_regions++;\n\t\tcur_payload_dram++;\n\t}\n\n\t \n\taddresses = desc_dram->block;\n\tfor (i = 0; i < pnvm_data->n_chunks; i++) {\n\t\taddresses->mem_descs[i] =\n\t\t\tcpu_to_le64(dram_regions->drams[i].physical);\n\t}\n\n\treturn 0;\n\n}\n\nint iwl_trans_pcie_ctx_info_gen3_load_pnvm(struct iwl_trans *trans,\n\t\t\t\t\t   const struct iwl_pnvm_image *pnvm_payloads,\n\t\t\t\t\t   const struct iwl_ucode_capabilities *capa)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_prph_scratch_ctrl_cfg *prph_sc_ctrl =\n\t\t&trans_pcie->prph_scratch->ctrl_cfg;\n\tstruct iwl_dram_regions *dram_regions = &trans_pcie->pnvm_data;\n\tint ret = 0;\n\n\t \n\tif (trans->pnvm_loaded)\n\t\treturn 0;\n\n\tif (WARN_ON(prph_sc_ctrl->pnvm_cfg.pnvm_size))\n\t\treturn -EBUSY;\n\n\tif (trans->trans_cfg->device_family < IWL_DEVICE_FAMILY_AX210)\n\t\treturn 0;\n\n\tif (!pnvm_payloads->n_chunks) {\n\t\tIWL_DEBUG_FW(trans, \"no payloads\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (fw_has_capa(capa, IWL_UCODE_TLV_CAPA_FRAGMENTED_PNVM_IMG)) {\n\t\tret = iwl_pcie_load_payloads_segments(trans,\n\t\t\t\t\t\t      dram_regions,\n\t\t\t\t\t\t      pnvm_payloads);\n\t\tif (!ret)\n\t\t\ttrans->pnvm_loaded = true;\n\t} else {\n\t\t \n\t\tret = iwl_pcie_load_payloads_continuously\n\t\t\t\t\t\t(trans,\n\t\t\t\t\t\t pnvm_payloads,\n\t\t\t\t\t\t &dram_regions->drams[0]);\n\t\tif (!ret) {\n\t\t\tdram_regions->n_regions = 1;\n\t\t\ttrans->pnvm_loaded = true;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic inline size_t\niwl_dram_regions_size(const struct iwl_dram_regions *dram_regions)\n{\n\tsize_t total_size = 0;\n\tint i;\n\n\tfor (i = 0; i < dram_regions->n_regions; i++)\n\t\ttotal_size += dram_regions->drams[i].size;\n\n\treturn total_size;\n}\n\nstatic void iwl_pcie_set_pnvm_segments(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_prph_scratch_ctrl_cfg *prph_sc_ctrl =\n\t\t&trans_pcie->prph_scratch->ctrl_cfg;\n\tstruct iwl_dram_regions *dram_regions = &trans_pcie->pnvm_data;\n\n\tprph_sc_ctrl->pnvm_cfg.pnvm_base_addr =\n\t\tcpu_to_le64(dram_regions->prph_scratch_mem_desc.physical);\n\tprph_sc_ctrl->pnvm_cfg.pnvm_size =\n\t\tcpu_to_le32(iwl_dram_regions_size(dram_regions));\n}\n\nstatic void iwl_pcie_set_continuous_pnvm(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_prph_scratch_ctrl_cfg *prph_sc_ctrl =\n\t\t&trans_pcie->prph_scratch->ctrl_cfg;\n\n\tprph_sc_ctrl->pnvm_cfg.pnvm_base_addr =\n\t\tcpu_to_le64(trans_pcie->pnvm_data.drams[0].physical);\n\tprph_sc_ctrl->pnvm_cfg.pnvm_size =\n\t\tcpu_to_le32(trans_pcie->pnvm_data.drams[0].size);\n}\n\nvoid iwl_trans_pcie_ctx_info_gen3_set_pnvm(struct iwl_trans *trans,\n\t\t\t\t\t   const struct iwl_ucode_capabilities *capa)\n{\n\tif (trans->trans_cfg->device_family < IWL_DEVICE_FAMILY_AX210)\n\t\treturn;\n\n\tif (fw_has_capa(capa, IWL_UCODE_TLV_CAPA_FRAGMENTED_PNVM_IMG))\n\t\tiwl_pcie_set_pnvm_segments(trans);\n\telse\n\t\tiwl_pcie_set_continuous_pnvm(trans);\n}\n\nint iwl_trans_pcie_ctx_info_gen3_load_reduce_power(struct iwl_trans *trans,\n\t\t\t\t\t\t   const struct iwl_pnvm_image *payloads,\n\t\t\t\t\t\t   const struct iwl_ucode_capabilities *capa)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_prph_scratch_ctrl_cfg *prph_sc_ctrl =\n\t\t&trans_pcie->prph_scratch->ctrl_cfg;\n\tstruct iwl_dram_regions *dram_regions = &trans_pcie->reduced_tables_data;\n\tint ret = 0;\n\n\t \n\tif (trans->reduce_power_loaded)\n\t\treturn 0;\n\n\tif (trans->trans_cfg->device_family < IWL_DEVICE_FAMILY_AX210)\n\t\treturn 0;\n\n\tif (WARN_ON(prph_sc_ctrl->reduce_power_cfg.size))\n\t\treturn -EBUSY;\n\n\tif (!payloads->n_chunks) {\n\t\tIWL_DEBUG_FW(trans, \"no payloads\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (fw_has_capa(capa, IWL_UCODE_TLV_CAPA_FRAGMENTED_PNVM_IMG)) {\n\t\tret = iwl_pcie_load_payloads_segments(trans,\n\t\t\t\t\t\t      dram_regions,\n\t\t\t\t\t\t      payloads);\n\t\tif (!ret)\n\t\t\ttrans->reduce_power_loaded = true;\n\t} else {\n\t\t \n\t\tret = iwl_pcie_load_payloads_continuously\n\t\t\t\t\t\t(trans,\n\t\t\t\t\t\t payloads,\n\t\t\t\t\t\t &dram_regions->drams[0]);\n\t\tif (!ret) {\n\t\t\tdram_regions->n_regions = 1;\n\t\t\ttrans->reduce_power_loaded = true;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic void iwl_pcie_set_reduce_power_segments(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_prph_scratch_ctrl_cfg *prph_sc_ctrl =\n\t\t&trans_pcie->prph_scratch->ctrl_cfg;\n\tstruct iwl_dram_regions *dram_regions = &trans_pcie->reduced_tables_data;\n\n\tprph_sc_ctrl->reduce_power_cfg.base_addr =\n\t\tcpu_to_le64(dram_regions->prph_scratch_mem_desc.physical);\n\tprph_sc_ctrl->reduce_power_cfg.size =\n\t\tcpu_to_le32(iwl_dram_regions_size(dram_regions));\n}\n\nstatic void iwl_pcie_set_continuous_reduce_power(struct iwl_trans *trans)\n{\n\tstruct iwl_trans_pcie *trans_pcie = IWL_TRANS_GET_PCIE_TRANS(trans);\n\tstruct iwl_prph_scratch_ctrl_cfg *prph_sc_ctrl =\n\t\t&trans_pcie->prph_scratch->ctrl_cfg;\n\n\tprph_sc_ctrl->reduce_power_cfg.base_addr =\n\t\tcpu_to_le64(trans_pcie->reduced_tables_data.drams[0].physical);\n\tprph_sc_ctrl->reduce_power_cfg.size =\n\t\tcpu_to_le32(trans_pcie->reduced_tables_data.drams[0].size);\n}\n\nvoid\niwl_trans_pcie_ctx_info_gen3_set_reduce_power(struct iwl_trans *trans,\n\t\t\t\t\t      const struct iwl_ucode_capabilities *capa)\n{\n\tif (trans->trans_cfg->device_family < IWL_DEVICE_FAMILY_AX210)\n\t\treturn;\n\n\tif (fw_has_capa(capa, IWL_UCODE_TLV_CAPA_FRAGMENTED_PNVM_IMG))\n\t\tiwl_pcie_set_reduce_power_segments(trans);\n\telse\n\t\tiwl_pcie_set_continuous_reduce_power(trans);\n}\n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}