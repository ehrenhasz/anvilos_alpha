{
  "module_name": "tx.h",
  "hash_id": "a397ed0ac12912c3753218ff163b927fdae614999dc367fcae1c84015b75ceba",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wireless/intel/iwlwifi/queue/tx.h",
  "human_readable_source": " \n \n#ifndef __iwl_trans_queue_tx_h__\n#define __iwl_trans_queue_tx_h__\n#include \"iwl-fh.h\"\n#include \"fw/api/tx.h\"\n\nstruct iwl_tso_hdr_page {\n\tstruct page *page;\n\tu8 *pos;\n};\n\nstatic inline dma_addr_t\niwl_txq_get_first_tb_dma(struct iwl_txq *txq, int idx)\n{\n\treturn txq->first_tb_dma +\n\t       sizeof(struct iwl_pcie_first_tb_buf) * idx;\n}\n\nstatic inline u16 iwl_txq_get_cmd_index(const struct iwl_txq *q, u32 index)\n{\n\treturn index & (q->n_window - 1);\n}\n\nvoid iwl_txq_gen2_unmap(struct iwl_trans *trans, int txq_id);\n\nstatic inline void iwl_wake_queue(struct iwl_trans *trans,\n\t\t\t\t  struct iwl_txq *txq)\n{\n\tif (test_and_clear_bit(txq->id, trans->txqs.queue_stopped)) {\n\t\tIWL_DEBUG_TX_QUEUES(trans, \"Wake hwq %d\\n\", txq->id);\n\t\tiwl_op_mode_queue_not_full(trans->op_mode, txq->id);\n\t}\n}\n\nstatic inline void *iwl_txq_get_tfd(struct iwl_trans *trans,\n\t\t\t\t    struct iwl_txq *txq, int idx)\n{\n\tif (trans->trans_cfg->gen2)\n\t\tidx = iwl_txq_get_cmd_index(txq, idx);\n\n\treturn (u8 *)txq->tfds + trans->txqs.tfd.size * idx;\n}\n\nint iwl_txq_alloc(struct iwl_trans *trans, struct iwl_txq *txq, int slots_num,\n\t\t  bool cmd_queue);\n \nstatic inline bool iwl_txq_crosses_4g_boundary(u64 phys, u16 len)\n{\n\treturn upper_32_bits(phys) != upper_32_bits(phys + len);\n}\n\nint iwl_txq_space(struct iwl_trans *trans, const struct iwl_txq *q);\n\nstatic inline void iwl_txq_stop(struct iwl_trans *trans, struct iwl_txq *txq)\n{\n\tif (!test_and_set_bit(txq->id, trans->txqs.queue_stopped)) {\n\t\tiwl_op_mode_queue_full(trans->op_mode, txq->id);\n\t\tIWL_DEBUG_TX_QUEUES(trans, \"Stop hwq %d\\n\", txq->id);\n\t} else {\n\t\tIWL_DEBUG_TX_QUEUES(trans, \"hwq %d already stopped\\n\",\n\t\t\t\t    txq->id);\n\t}\n}\n\n \nstatic inline int iwl_txq_inc_wrap(struct iwl_trans *trans, int index)\n{\n\treturn ++index &\n\t\t(trans->trans_cfg->base_params->max_tfd_queue_size - 1);\n}\n\n \nstatic inline int iwl_txq_dec_wrap(struct iwl_trans *trans, int index)\n{\n\treturn --index &\n\t\t(trans->trans_cfg->base_params->max_tfd_queue_size - 1);\n}\n\nstatic inline bool iwl_txq_used(const struct iwl_txq *q, int i)\n{\n\tint index = iwl_txq_get_cmd_index(q, i);\n\tint r = iwl_txq_get_cmd_index(q, q->read_ptr);\n\tint w = iwl_txq_get_cmd_index(q, q->write_ptr);\n\n\treturn w >= r ?\n\t\t(index >= r && index < w) :\n\t\t!(index < r && index >= w);\n}\n\nvoid iwl_txq_free_tso_page(struct iwl_trans *trans, struct sk_buff *skb);\n\nvoid iwl_txq_log_scd_error(struct iwl_trans *trans, struct iwl_txq *txq);\n\nint iwl_txq_gen2_set_tb(struct iwl_trans *trans,\n\t\t\tstruct iwl_tfh_tfd *tfd, dma_addr_t addr,\n\t\t\tu16 len);\n\nvoid iwl_txq_gen2_tfd_unmap(struct iwl_trans *trans,\n\t\t\t    struct iwl_cmd_meta *meta,\n\t\t\t    struct iwl_tfh_tfd *tfd);\n\nint iwl_txq_dyn_alloc(struct iwl_trans *trans, u32 flags,\n\t\t      u32 sta_mask, u8 tid,\n\t\t      int size, unsigned int timeout);\n\nint iwl_txq_gen2_tx(struct iwl_trans *trans, struct sk_buff *skb,\n\t\t    struct iwl_device_tx_cmd *dev_cmd, int txq_id);\n\nvoid iwl_txq_dyn_free(struct iwl_trans *trans, int queue);\nvoid iwl_txq_gen2_free_tfd(struct iwl_trans *trans, struct iwl_txq *txq);\nvoid iwl_txq_inc_wr_ptr(struct iwl_trans *trans, struct iwl_txq *txq);\nvoid iwl_txq_gen2_tx_free(struct iwl_trans *trans);\nint iwl_txq_init(struct iwl_trans *trans, struct iwl_txq *txq, int slots_num,\n\t\t bool cmd_queue);\nint iwl_txq_gen2_init(struct iwl_trans *trans, int txq_id, int queue_size);\n#ifdef CONFIG_INET\nstruct iwl_tso_hdr_page *get_page_hdr(struct iwl_trans *trans, size_t len,\n\t\t\t\t      struct sk_buff *skb);\n#endif\nstatic inline u8 iwl_txq_gen1_tfd_get_num_tbs(struct iwl_trans *trans,\n\t\t\t\t\t      struct iwl_tfd *tfd)\n{\n\treturn tfd->num_tbs & 0x1f;\n}\n\nstatic inline u16 iwl_txq_gen1_tfd_tb_get_len(struct iwl_trans *trans,\n\t\t\t\t\t      void *_tfd, u8 idx)\n{\n\tstruct iwl_tfd *tfd;\n\tstruct iwl_tfd_tb *tb;\n\n\tif (trans->trans_cfg->gen2) {\n\t\tstruct iwl_tfh_tfd *tfh_tfd = _tfd;\n\t\tstruct iwl_tfh_tb *tfh_tb = &tfh_tfd->tbs[idx];\n\n\t\treturn le16_to_cpu(tfh_tb->tb_len);\n\t}\n\n\ttfd = (struct iwl_tfd *)_tfd;\n\ttb = &tfd->tbs[idx];\n\n\treturn le16_to_cpu(tb->hi_n_len) >> 4;\n}\n\nstatic inline void iwl_pcie_gen1_tfd_set_tb(struct iwl_trans *trans,\n\t\t\t\t\t    struct iwl_tfd *tfd,\n\t\t\t\t\t    u8 idx, dma_addr_t addr, u16 len)\n{\n\tstruct iwl_tfd_tb *tb = &tfd->tbs[idx];\n\tu16 hi_n_len = len << 4;\n\n\tput_unaligned_le32(addr, &tb->lo);\n\thi_n_len |= iwl_get_dma_hi_addr(addr);\n\n\ttb->hi_n_len = cpu_to_le16(hi_n_len);\n\n\ttfd->num_tbs = idx + 1;\n}\n\nvoid iwl_txq_gen1_tfd_unmap(struct iwl_trans *trans,\n\t\t\t    struct iwl_cmd_meta *meta,\n\t\t\t    struct iwl_txq *txq, int index);\nvoid iwl_txq_gen1_inval_byte_cnt_tbl(struct iwl_trans *trans,\n\t\t\t\t     struct iwl_txq *txq);\nvoid iwl_txq_gen1_update_byte_cnt_tbl(struct iwl_trans *trans,\n\t\t\t\t      struct iwl_txq *txq, u16 byte_cnt,\n\t\t\t\t      int num_tbs);\nvoid iwl_txq_reclaim(struct iwl_trans *trans, int txq_id, int ssn,\n\t\t     struct sk_buff_head *skbs, bool is_flush);\nvoid iwl_txq_set_q_ptrs(struct iwl_trans *trans, int txq_id, int ptr);\nvoid iwl_trans_txq_freeze_timer(struct iwl_trans *trans, unsigned long txqs,\n\t\t\t\tbool freeze);\nvoid iwl_txq_progress(struct iwl_txq *txq);\nvoid iwl_txq_free_tfd(struct iwl_trans *trans, struct iwl_txq *txq);\nint iwl_trans_txq_send_hcmd(struct iwl_trans *trans, struct iwl_host_cmd *cmd);\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}