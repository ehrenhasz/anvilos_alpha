{
  "module_name": "tx.c",
  "hash_id": "d97041fbbdb8224f0f572eaa3c23b96c9c18213bb3bd6e8f228a3dca163fb620",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wireless/intel/iwlwifi/queue/tx.c",
  "human_readable_source": "\n \n#include <net/tso.h>\n#include <linux/tcp.h>\n\n#include \"iwl-debug.h\"\n#include \"iwl-io.h\"\n#include \"fw/api/commands.h\"\n#include \"fw/api/tx.h\"\n#include \"fw/api/datapath.h\"\n#include \"fw/api/debug.h\"\n#include \"queue/tx.h\"\n#include \"iwl-fh.h\"\n#include \"iwl-scd.h\"\n#include <linux/dmapool.h>\n\n \nstatic void iwl_pcie_gen2_update_byte_tbl(struct iwl_trans *trans,\n\t\t\t\t\t  struct iwl_txq *txq, u16 byte_cnt,\n\t\t\t\t\t  int num_tbs)\n{\n\tint idx = iwl_txq_get_cmd_index(txq, txq->write_ptr);\n\tu8 filled_tfd_size, num_fetch_chunks;\n\tu16 len = byte_cnt;\n\t__le16 bc_ent;\n\n\tif (WARN(idx >= txq->n_window, \"%d >= %d\\n\", idx, txq->n_window))\n\t\treturn;\n\n\tfilled_tfd_size = offsetof(struct iwl_tfh_tfd, tbs) +\n\t\t\t  num_tbs * sizeof(struct iwl_tfh_tb);\n\t \n\tnum_fetch_chunks = DIV_ROUND_UP(filled_tfd_size, 64) - 1;\n\n\tif (trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_AX210) {\n\t\tstruct iwl_gen3_bc_tbl_entry *scd_bc_tbl_gen3 = txq->bc_tbl.addr;\n\n\t\t \n\t\tWARN_ON(trans->txqs.bc_table_dword);\n\t\tWARN_ON(len > 0x3FFF);\n\t\tbc_ent = cpu_to_le16(len | (num_fetch_chunks << 14));\n\t\tscd_bc_tbl_gen3[idx].tfd_offset = bc_ent;\n\t} else {\n\t\tstruct iwlagn_scd_bc_tbl *scd_bc_tbl = txq->bc_tbl.addr;\n\n\t\t \n\t\tWARN_ON(!trans->txqs.bc_table_dword);\n\t\tlen = DIV_ROUND_UP(len, 4);\n\t\tWARN_ON(len > 0xFFF);\n\t\tbc_ent = cpu_to_le16(len | (num_fetch_chunks << 12));\n\t\tscd_bc_tbl->tfd_offset[idx] = bc_ent;\n\t}\n}\n\n \nvoid iwl_txq_inc_wr_ptr(struct iwl_trans *trans, struct iwl_txq *txq)\n{\n\tlockdep_assert_held(&txq->lock);\n\n\tIWL_DEBUG_TX(trans, \"Q:%d WR: 0x%x\\n\", txq->id, txq->write_ptr);\n\n\t \n\tiwl_write32(trans, HBUS_TARG_WRPTR, txq->write_ptr | (txq->id << 16));\n}\n\nstatic u8 iwl_txq_gen2_get_num_tbs(struct iwl_trans *trans,\n\t\t\t\t   struct iwl_tfh_tfd *tfd)\n{\n\treturn le16_to_cpu(tfd->num_tbs) & 0x1f;\n}\n\nint iwl_txq_gen2_set_tb(struct iwl_trans *trans, struct iwl_tfh_tfd *tfd,\n\t\t\tdma_addr_t addr, u16 len)\n{\n\tint idx = iwl_txq_gen2_get_num_tbs(trans, tfd);\n\tstruct iwl_tfh_tb *tb;\n\n\t \n\tWARN(iwl_txq_crosses_4g_boundary(addr, len),\n\t     \"possible DMA problem with iova:0x%llx, len:%d\\n\",\n\t     (unsigned long long)addr, len);\n\n\tif (WARN_ON(idx >= IWL_TFH_NUM_TBS))\n\t\treturn -EINVAL;\n\ttb = &tfd->tbs[idx];\n\n\t \n\tif (le16_to_cpu(tfd->num_tbs) >= trans->txqs.tfd.max_tbs) {\n\t\tIWL_ERR(trans, \"Error can not send more than %d chunks\\n\",\n\t\t\ttrans->txqs.tfd.max_tbs);\n\t\treturn -EINVAL;\n\t}\n\n\tput_unaligned_le64(addr, &tb->addr);\n\ttb->tb_len = cpu_to_le16(len);\n\n\ttfd->num_tbs = cpu_to_le16(idx + 1);\n\n\treturn idx;\n}\n\nstatic void iwl_txq_set_tfd_invalid_gen2(struct iwl_trans *trans,\n\t\t\t\t\t struct iwl_tfh_tfd *tfd)\n{\n\ttfd->num_tbs = 0;\n\n\tiwl_txq_gen2_set_tb(trans, tfd, trans->invalid_tx_cmd.dma,\n\t\t\t    trans->invalid_tx_cmd.size);\n}\n\nvoid iwl_txq_gen2_tfd_unmap(struct iwl_trans *trans, struct iwl_cmd_meta *meta,\n\t\t\t    struct iwl_tfh_tfd *tfd)\n{\n\tint i, num_tbs;\n\n\t \n\tnum_tbs = iwl_txq_gen2_get_num_tbs(trans, tfd);\n\n\tif (num_tbs > trans->txqs.tfd.max_tbs) {\n\t\tIWL_ERR(trans, \"Too many chunks: %i\\n\", num_tbs);\n\t\treturn;\n\t}\n\n\t \n\tfor (i = 1; i < num_tbs; i++) {\n\t\tif (meta->tbs & BIT(i))\n\t\t\tdma_unmap_page(trans->dev,\n\t\t\t\t       le64_to_cpu(tfd->tbs[i].addr),\n\t\t\t\t       le16_to_cpu(tfd->tbs[i].tb_len),\n\t\t\t\t       DMA_TO_DEVICE);\n\t\telse\n\t\t\tdma_unmap_single(trans->dev,\n\t\t\t\t\t le64_to_cpu(tfd->tbs[i].addr),\n\t\t\t\t\t le16_to_cpu(tfd->tbs[i].tb_len),\n\t\t\t\t\t DMA_TO_DEVICE);\n\t}\n\n\tiwl_txq_set_tfd_invalid_gen2(trans, tfd);\n}\n\nvoid iwl_txq_gen2_free_tfd(struct iwl_trans *trans, struct iwl_txq *txq)\n{\n\t \n\tint idx = iwl_txq_get_cmd_index(txq, txq->read_ptr);\n\tstruct sk_buff *skb;\n\n\tlockdep_assert_held(&txq->lock);\n\n\tif (!txq->entries)\n\t\treturn;\n\n\tiwl_txq_gen2_tfd_unmap(trans, &txq->entries[idx].meta,\n\t\t\t       iwl_txq_get_tfd(trans, txq, idx));\n\n\tskb = txq->entries[idx].skb;\n\n\t \n\tif (skb) {\n\t\tiwl_op_mode_free_skb(trans->op_mode, skb);\n\t\ttxq->entries[idx].skb = NULL;\n\t}\n}\n\nstatic struct page *get_workaround_page(struct iwl_trans *trans,\n\t\t\t\t\tstruct sk_buff *skb)\n{\n\tstruct page **page_ptr;\n\tstruct page *ret;\n\n\tpage_ptr = (void *)((u8 *)skb->cb + trans->txqs.page_offs);\n\n\tret = alloc_page(GFP_ATOMIC);\n\tif (!ret)\n\t\treturn NULL;\n\n\t \n\t*(void **)((u8 *)page_address(ret) + PAGE_SIZE - sizeof(void *)) = *page_ptr;\n\t*page_ptr = ret;\n\n\treturn ret;\n}\n\n \nstatic int iwl_txq_gen2_set_tb_with_wa(struct iwl_trans *trans,\n\t\t\t\t       struct sk_buff *skb,\n\t\t\t\t       struct iwl_tfh_tfd *tfd,\n\t\t\t\t       dma_addr_t phys, void *virt,\n\t\t\t\t       u16 len, struct iwl_cmd_meta *meta)\n{\n\tdma_addr_t oldphys = phys;\n\tstruct page *page;\n\tint ret;\n\n\tif (unlikely(dma_mapping_error(trans->dev, phys)))\n\t\treturn -ENOMEM;\n\n\tif (likely(!iwl_txq_crosses_4g_boundary(phys, len))) {\n\t\tret = iwl_txq_gen2_set_tb(trans, tfd, phys, len);\n\n\t\tif (ret < 0)\n\t\t\tgoto unmap;\n\n\t\tif (meta)\n\t\t\tmeta->tbs |= BIT(ret);\n\n\t\tret = 0;\n\t\tgoto trace;\n\t}\n\n\t \n\n\tif (WARN_ON(len > PAGE_SIZE - sizeof(void *))) {\n\t\tret = -ENOBUFS;\n\t\tgoto unmap;\n\t}\n\n\tpage = get_workaround_page(trans, skb);\n\tif (!page) {\n\t\tret = -ENOMEM;\n\t\tgoto unmap;\n\t}\n\n\tmemcpy(page_address(page), virt, len);\n\n\tphys = dma_map_single(trans->dev, page_address(page), len,\n\t\t\t      DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(trans->dev, phys)))\n\t\treturn -ENOMEM;\n\tret = iwl_txq_gen2_set_tb(trans, tfd, phys, len);\n\tif (ret < 0) {\n\t\t \n\t\toldphys = phys;\n\t\tmeta = NULL;\n\t\tgoto unmap;\n\t}\n\tIWL_WARN(trans,\n\t\t \"TB bug workaround: copied %d bytes from 0x%llx to 0x%llx\\n\",\n\t\t len, (unsigned long long)oldphys, (unsigned long long)phys);\n\n\tret = 0;\nunmap:\n\tif (meta)\n\t\tdma_unmap_page(trans->dev, oldphys, len, DMA_TO_DEVICE);\n\telse\n\t\tdma_unmap_single(trans->dev, oldphys, len, DMA_TO_DEVICE);\ntrace:\n\ttrace_iwlwifi_dev_tx_tb(trans->dev, skb, virt, phys, len);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_INET\nstruct iwl_tso_hdr_page *get_page_hdr(struct iwl_trans *trans, size_t len,\n\t\t\t\t      struct sk_buff *skb)\n{\n\tstruct iwl_tso_hdr_page *p = this_cpu_ptr(trans->txqs.tso_hdr_page);\n\tstruct page **page_ptr;\n\n\tpage_ptr = (void *)((u8 *)skb->cb + trans->txqs.page_offs);\n\n\tif (WARN_ON(*page_ptr))\n\t\treturn NULL;\n\n\tif (!p->page)\n\t\tgoto alloc;\n\n\t \n\tif (p->pos + len < (u8 *)page_address(p->page) + PAGE_SIZE -\n\t\t\t   sizeof(void *))\n\t\tgoto out;\n\n\t \n\t__free_page(p->page);\n\nalloc:\n\tp->page = alloc_page(GFP_ATOMIC);\n\tif (!p->page)\n\t\treturn NULL;\n\tp->pos = page_address(p->page);\n\t \n\t*(void **)((u8 *)page_address(p->page) + PAGE_SIZE - sizeof(void *)) = NULL;\nout:\n\t*page_ptr = p->page;\n\tget_page(p->page);\n\treturn p;\n}\n#endif\n\nstatic int iwl_txq_gen2_build_amsdu(struct iwl_trans *trans,\n\t\t\t\t    struct sk_buff *skb,\n\t\t\t\t    struct iwl_tfh_tfd *tfd, int start_len,\n\t\t\t\t    u8 hdr_len,\n\t\t\t\t    struct iwl_device_tx_cmd *dev_cmd)\n{\n#ifdef CONFIG_INET\n\tstruct iwl_tx_cmd_gen2 *tx_cmd = (void *)dev_cmd->payload;\n\tstruct ieee80211_hdr *hdr = (void *)skb->data;\n\tunsigned int snap_ip_tcp_hdrlen, ip_hdrlen, total_len, hdr_room;\n\tunsigned int mss = skb_shinfo(skb)->gso_size;\n\tu16 length, amsdu_pad;\n\tu8 *start_hdr;\n\tstruct iwl_tso_hdr_page *hdr_page;\n\tstruct tso_t tso;\n\n\ttrace_iwlwifi_dev_tx(trans->dev, skb, tfd, sizeof(*tfd),\n\t\t\t     &dev_cmd->hdr, start_len, 0);\n\n\tip_hdrlen = skb_transport_header(skb) - skb_network_header(skb);\n\tsnap_ip_tcp_hdrlen = 8 + ip_hdrlen + tcp_hdrlen(skb);\n\ttotal_len = skb->len - snap_ip_tcp_hdrlen - hdr_len;\n\tamsdu_pad = 0;\n\n\t \n\thdr_room = DIV_ROUND_UP(total_len, mss) *\n\t\t(3 + snap_ip_tcp_hdrlen + sizeof(struct ethhdr));\n\n\t \n\thdr_page = get_page_hdr(trans, hdr_room, skb);\n\tif (!hdr_page)\n\t\treturn -ENOMEM;\n\n\tstart_hdr = hdr_page->pos;\n\n\t \n\tskb_pull(skb, hdr_len);\n\n\t \n\tle16_add_cpu(&tx_cmd->len, -snap_ip_tcp_hdrlen);\n\n\ttso_start(skb, &tso);\n\n\twhile (total_len) {\n\t\t \n\t\tunsigned int data_left = min_t(unsigned int, mss, total_len);\n\t\tunsigned int tb_len;\n\t\tdma_addr_t tb_phys;\n\t\tu8 *subf_hdrs_start = hdr_page->pos;\n\n\t\ttotal_len -= data_left;\n\n\t\tmemset(hdr_page->pos, 0, amsdu_pad);\n\t\thdr_page->pos += amsdu_pad;\n\t\tamsdu_pad = (4 - (sizeof(struct ethhdr) + snap_ip_tcp_hdrlen +\n\t\t\t\t  data_left)) & 0x3;\n\t\tether_addr_copy(hdr_page->pos, ieee80211_get_DA(hdr));\n\t\thdr_page->pos += ETH_ALEN;\n\t\tether_addr_copy(hdr_page->pos, ieee80211_get_SA(hdr));\n\t\thdr_page->pos += ETH_ALEN;\n\n\t\tlength = snap_ip_tcp_hdrlen + data_left;\n\t\t*((__be16 *)hdr_page->pos) = cpu_to_be16(length);\n\t\thdr_page->pos += sizeof(length);\n\n\t\t \n\t\ttso_build_hdr(skb, hdr_page->pos, &tso, data_left, !total_len);\n\n\t\thdr_page->pos += snap_ip_tcp_hdrlen;\n\n\t\ttb_len = hdr_page->pos - start_hdr;\n\t\ttb_phys = dma_map_single(trans->dev, start_hdr,\n\t\t\t\t\t tb_len, DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(trans->dev, tb_phys)))\n\t\t\tgoto out_err;\n\t\t \n\t\tiwl_txq_gen2_set_tb(trans, tfd, tb_phys, tb_len);\n\t\ttrace_iwlwifi_dev_tx_tb(trans->dev, skb, start_hdr,\n\t\t\t\t\ttb_phys, tb_len);\n\t\t \n\t\tle16_add_cpu(&tx_cmd->len, hdr_page->pos - subf_hdrs_start);\n\n\t\t \n\t\tstart_hdr = hdr_page->pos;\n\n\t\t \n\t\twhile (data_left) {\n\t\t\tint ret;\n\n\t\t\ttb_len = min_t(unsigned int, tso.size, data_left);\n\t\t\ttb_phys = dma_map_single(trans->dev, tso.data,\n\t\t\t\t\t\t tb_len, DMA_TO_DEVICE);\n\t\t\tret = iwl_txq_gen2_set_tb_with_wa(trans, skb, tfd,\n\t\t\t\t\t\t\t  tb_phys, tso.data,\n\t\t\t\t\t\t\t  tb_len, NULL);\n\t\t\tif (ret)\n\t\t\t\tgoto out_err;\n\n\t\t\tdata_left -= tb_len;\n\t\t\ttso_build_data(skb, &tso, tb_len);\n\t\t}\n\t}\n\n\t \n\tskb_push(skb, hdr_len);\n\n\treturn 0;\n\nout_err:\n#endif\n\treturn -EINVAL;\n}\n\nstatic struct\niwl_tfh_tfd *iwl_txq_gen2_build_tx_amsdu(struct iwl_trans *trans,\n\t\t\t\t\t struct iwl_txq *txq,\n\t\t\t\t\t struct iwl_device_tx_cmd *dev_cmd,\n\t\t\t\t\t struct sk_buff *skb,\n\t\t\t\t\t struct iwl_cmd_meta *out_meta,\n\t\t\t\t\t int hdr_len,\n\t\t\t\t\t int tx_cmd_len)\n{\n\tint idx = iwl_txq_get_cmd_index(txq, txq->write_ptr);\n\tstruct iwl_tfh_tfd *tfd = iwl_txq_get_tfd(trans, txq, idx);\n\tdma_addr_t tb_phys;\n\tint len;\n\tvoid *tb1_addr;\n\n\ttb_phys = iwl_txq_get_first_tb_dma(txq, idx);\n\n\t \n\tiwl_txq_gen2_set_tb(trans, tfd, tb_phys, IWL_FIRST_TB_SIZE);\n\n\t \n\tlen = tx_cmd_len + sizeof(struct iwl_cmd_header) + hdr_len -\n\t      IWL_FIRST_TB_SIZE;\n\n\t \n\n\t \n\ttb1_addr = ((u8 *)&dev_cmd->hdr) + IWL_FIRST_TB_SIZE;\n\ttb_phys = dma_map_single(trans->dev, tb1_addr, len, DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(trans->dev, tb_phys)))\n\t\tgoto out_err;\n\t \n\tiwl_txq_gen2_set_tb(trans, tfd, tb_phys, len);\n\n\tif (iwl_txq_gen2_build_amsdu(trans, skb, tfd, len + IWL_FIRST_TB_SIZE,\n\t\t\t\t     hdr_len, dev_cmd))\n\t\tgoto out_err;\n\n\t \n\tmemcpy(&txq->first_tb_bufs[idx], dev_cmd, IWL_FIRST_TB_SIZE);\n\treturn tfd;\n\nout_err:\n\tiwl_txq_gen2_tfd_unmap(trans, out_meta, tfd);\n\treturn NULL;\n}\n\nstatic int iwl_txq_gen2_tx_add_frags(struct iwl_trans *trans,\n\t\t\t\t     struct sk_buff *skb,\n\t\t\t\t     struct iwl_tfh_tfd *tfd,\n\t\t\t\t     struct iwl_cmd_meta *out_meta)\n{\n\tint i;\n\n\tfor (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {\n\t\tconst skb_frag_t *frag = &skb_shinfo(skb)->frags[i];\n\t\tdma_addr_t tb_phys;\n\t\tunsigned int fragsz = skb_frag_size(frag);\n\t\tint ret;\n\n\t\tif (!fragsz)\n\t\t\tcontinue;\n\n\t\ttb_phys = skb_frag_dma_map(trans->dev, frag, 0,\n\t\t\t\t\t   fragsz, DMA_TO_DEVICE);\n\t\tret = iwl_txq_gen2_set_tb_with_wa(trans, skb, tfd, tb_phys,\n\t\t\t\t\t\t  skb_frag_address(frag),\n\t\t\t\t\t\t  fragsz, out_meta);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic struct\niwl_tfh_tfd *iwl_txq_gen2_build_tx(struct iwl_trans *trans,\n\t\t\t\t   struct iwl_txq *txq,\n\t\t\t\t   struct iwl_device_tx_cmd *dev_cmd,\n\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t   struct iwl_cmd_meta *out_meta,\n\t\t\t\t   int hdr_len,\n\t\t\t\t   int tx_cmd_len,\n\t\t\t\t   bool pad)\n{\n\tint idx = iwl_txq_get_cmd_index(txq, txq->write_ptr);\n\tstruct iwl_tfh_tfd *tfd = iwl_txq_get_tfd(trans, txq, idx);\n\tdma_addr_t tb_phys;\n\tint len, tb1_len, tb2_len;\n\tvoid *tb1_addr;\n\tstruct sk_buff *frag;\n\n\ttb_phys = iwl_txq_get_first_tb_dma(txq, idx);\n\n\t \n\tmemcpy(&txq->first_tb_bufs[idx], dev_cmd, IWL_FIRST_TB_SIZE);\n\n\t \n\tiwl_txq_gen2_set_tb(trans, tfd, tb_phys, IWL_FIRST_TB_SIZE);\n\n\t \n\tlen = tx_cmd_len + sizeof(struct iwl_cmd_header) + hdr_len -\n\t      IWL_FIRST_TB_SIZE;\n\n\tif (pad)\n\t\ttb1_len = ALIGN(len, 4);\n\telse\n\t\ttb1_len = len;\n\n\t \n\ttb1_addr = ((u8 *)&dev_cmd->hdr) + IWL_FIRST_TB_SIZE;\n\ttb_phys = dma_map_single(trans->dev, tb1_addr, tb1_len, DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(trans->dev, tb_phys)))\n\t\tgoto out_err;\n\t \n\tiwl_txq_gen2_set_tb(trans, tfd, tb_phys, tb1_len);\n\ttrace_iwlwifi_dev_tx(trans->dev, skb, tfd, sizeof(*tfd), &dev_cmd->hdr,\n\t\t\t     IWL_FIRST_TB_SIZE + tb1_len, hdr_len);\n\n\t \n\ttb2_len = skb_headlen(skb) - hdr_len;\n\n\tif (tb2_len > 0) {\n\t\tint ret;\n\n\t\ttb_phys = dma_map_single(trans->dev, skb->data + hdr_len,\n\t\t\t\t\t tb2_len, DMA_TO_DEVICE);\n\t\tret = iwl_txq_gen2_set_tb_with_wa(trans, skb, tfd, tb_phys,\n\t\t\t\t\t\t  skb->data + hdr_len, tb2_len,\n\t\t\t\t\t\t  NULL);\n\t\tif (ret)\n\t\t\tgoto out_err;\n\t}\n\n\tif (iwl_txq_gen2_tx_add_frags(trans, skb, tfd, out_meta))\n\t\tgoto out_err;\n\n\tskb_walk_frags(skb, frag) {\n\t\tint ret;\n\n\t\ttb_phys = dma_map_single(trans->dev, frag->data,\n\t\t\t\t\t skb_headlen(frag), DMA_TO_DEVICE);\n\t\tret = iwl_txq_gen2_set_tb_with_wa(trans, skb, tfd, tb_phys,\n\t\t\t\t\t\t  frag->data,\n\t\t\t\t\t\t  skb_headlen(frag), NULL);\n\t\tif (ret)\n\t\t\tgoto out_err;\n\t\tif (iwl_txq_gen2_tx_add_frags(trans, frag, tfd, out_meta))\n\t\t\tgoto out_err;\n\t}\n\n\treturn tfd;\n\nout_err:\n\tiwl_txq_gen2_tfd_unmap(trans, out_meta, tfd);\n\treturn NULL;\n}\n\nstatic\nstruct iwl_tfh_tfd *iwl_txq_gen2_build_tfd(struct iwl_trans *trans,\n\t\t\t\t\t   struct iwl_txq *txq,\n\t\t\t\t\t   struct iwl_device_tx_cmd *dev_cmd,\n\t\t\t\t\t   struct sk_buff *skb,\n\t\t\t\t\t   struct iwl_cmd_meta *out_meta)\n{\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data;\n\tint idx = iwl_txq_get_cmd_index(txq, txq->write_ptr);\n\tstruct iwl_tfh_tfd *tfd = iwl_txq_get_tfd(trans, txq, idx);\n\tint len, hdr_len;\n\tbool amsdu;\n\n\t \n\tBUILD_BUG_ON(sizeof(struct iwl_tx_cmd_gen2) < IWL_FIRST_TB_SIZE);\n\tBUILD_BUG_ON(sizeof(struct iwl_cmd_header) +\n\t\t     offsetofend(struct iwl_tx_cmd_gen2, dram_info) >\n\t\t     IWL_FIRST_TB_SIZE);\n\tBUILD_BUG_ON(sizeof(struct iwl_tx_cmd_gen3) < IWL_FIRST_TB_SIZE);\n\tBUILD_BUG_ON(sizeof(struct iwl_cmd_header) +\n\t\t     offsetofend(struct iwl_tx_cmd_gen3, dram_info) >\n\t\t     IWL_FIRST_TB_SIZE);\n\n\tmemset(tfd, 0, sizeof(*tfd));\n\n\tif (trans->trans_cfg->device_family < IWL_DEVICE_FAMILY_AX210)\n\t\tlen = sizeof(struct iwl_tx_cmd_gen2);\n\telse\n\t\tlen = sizeof(struct iwl_tx_cmd_gen3);\n\n\tamsdu = ieee80211_is_data_qos(hdr->frame_control) &&\n\t\t\t(*ieee80211_get_qos_ctl(hdr) &\n\t\t\t IEEE80211_QOS_CTL_A_MSDU_PRESENT);\n\n\thdr_len = ieee80211_hdrlen(hdr->frame_control);\n\n\t \n\tif (amsdu && skb_shinfo(skb)->gso_size)\n\t\treturn iwl_txq_gen2_build_tx_amsdu(trans, txq, dev_cmd, skb,\n\t\t\t\t\t\t    out_meta, hdr_len, len);\n\treturn iwl_txq_gen2_build_tx(trans, txq, dev_cmd, skb, out_meta,\n\t\t\t\t      hdr_len, len, !amsdu);\n}\n\nint iwl_txq_space(struct iwl_trans *trans, const struct iwl_txq *q)\n{\n\tunsigned int max;\n\tunsigned int used;\n\n\t \n\tif (q->n_window < trans->trans_cfg->base_params->max_tfd_queue_size)\n\t\tmax = q->n_window;\n\telse\n\t\tmax = trans->trans_cfg->base_params->max_tfd_queue_size - 1;\n\n\t \n\tused = (q->write_ptr - q->read_ptr) &\n\t\t(trans->trans_cfg->base_params->max_tfd_queue_size - 1);\n\n\tif (WARN_ON(used > max))\n\t\treturn 0;\n\n\treturn max - used;\n}\n\nint iwl_txq_gen2_tx(struct iwl_trans *trans, struct sk_buff *skb,\n\t\t    struct iwl_device_tx_cmd *dev_cmd, int txq_id)\n{\n\tstruct iwl_cmd_meta *out_meta;\n\tstruct iwl_txq *txq = trans->txqs.txq[txq_id];\n\tu16 cmd_len;\n\tint idx;\n\tvoid *tfd;\n\n\tif (WARN_ONCE(txq_id >= IWL_MAX_TVQM_QUEUES,\n\t\t      \"queue %d out of range\", txq_id))\n\t\treturn -EINVAL;\n\n\tif (WARN_ONCE(!test_bit(txq_id, trans->txqs.queue_used),\n\t\t      \"TX on unused queue %d\\n\", txq_id))\n\t\treturn -EINVAL;\n\n\tif (skb_is_nonlinear(skb) &&\n\t    skb_shinfo(skb)->nr_frags > IWL_TRANS_MAX_FRAGS(trans) &&\n\t    __skb_linearize(skb))\n\t\treturn -ENOMEM;\n\n\tspin_lock(&txq->lock);\n\n\tif (iwl_txq_space(trans, txq) < txq->high_mark) {\n\t\tiwl_txq_stop(trans, txq);\n\n\t\t \n\t\tif (unlikely(iwl_txq_space(trans, txq) < 3)) {\n\t\t\tstruct iwl_device_tx_cmd **dev_cmd_ptr;\n\n\t\t\tdev_cmd_ptr = (void *)((u8 *)skb->cb +\n\t\t\t\t\t       trans->txqs.dev_cmd_offs);\n\n\t\t\t*dev_cmd_ptr = dev_cmd;\n\t\t\t__skb_queue_tail(&txq->overflow_q, skb);\n\t\t\tspin_unlock(&txq->lock);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tidx = iwl_txq_get_cmd_index(txq, txq->write_ptr);\n\n\t \n\ttxq->entries[idx].skb = skb;\n\ttxq->entries[idx].cmd = dev_cmd;\n\n\tdev_cmd->hdr.sequence =\n\t\tcpu_to_le16((u16)(QUEUE_TO_SEQ(txq_id) |\n\t\t\t    INDEX_TO_SEQ(idx)));\n\n\t \n\tout_meta = &txq->entries[idx].meta;\n\tout_meta->flags = 0;\n\n\ttfd = iwl_txq_gen2_build_tfd(trans, txq, dev_cmd, skb, out_meta);\n\tif (!tfd) {\n\t\tspin_unlock(&txq->lock);\n\t\treturn -1;\n\t}\n\n\tif (trans->trans_cfg->device_family >= IWL_DEVICE_FAMILY_AX210) {\n\t\tstruct iwl_tx_cmd_gen3 *tx_cmd_gen3 =\n\t\t\t(void *)dev_cmd->payload;\n\n\t\tcmd_len = le16_to_cpu(tx_cmd_gen3->len);\n\t} else {\n\t\tstruct iwl_tx_cmd_gen2 *tx_cmd_gen2 =\n\t\t\t(void *)dev_cmd->payload;\n\n\t\tcmd_len = le16_to_cpu(tx_cmd_gen2->len);\n\t}\n\n\t \n\tiwl_pcie_gen2_update_byte_tbl(trans, txq, cmd_len,\n\t\t\t\t      iwl_txq_gen2_get_num_tbs(trans, tfd));\n\n\t \n\tif (txq->read_ptr == txq->write_ptr && txq->wd_timeout)\n\t\tmod_timer(&txq->stuck_timer, jiffies + txq->wd_timeout);\n\n\t \n\ttxq->write_ptr = iwl_txq_inc_wrap(trans, txq->write_ptr);\n\tiwl_txq_inc_wr_ptr(trans, txq);\n\t \n\tspin_unlock(&txq->lock);\n\treturn 0;\n}\n\n \n\n \nvoid iwl_txq_gen2_unmap(struct iwl_trans *trans, int txq_id)\n{\n\tstruct iwl_txq *txq = trans->txqs.txq[txq_id];\n\n\tspin_lock_bh(&txq->lock);\n\twhile (txq->write_ptr != txq->read_ptr) {\n\t\tIWL_DEBUG_TX_REPLY(trans, \"Q %d Free %d\\n\",\n\t\t\t\t   txq_id, txq->read_ptr);\n\n\t\tif (txq_id != trans->txqs.cmd.q_id) {\n\t\t\tint idx = iwl_txq_get_cmd_index(txq, txq->read_ptr);\n\t\t\tstruct sk_buff *skb = txq->entries[idx].skb;\n\n\t\t\tif (!WARN_ON_ONCE(!skb))\n\t\t\t\tiwl_txq_free_tso_page(trans, skb);\n\t\t}\n\t\tiwl_txq_gen2_free_tfd(trans, txq);\n\t\ttxq->read_ptr = iwl_txq_inc_wrap(trans, txq->read_ptr);\n\t}\n\n\twhile (!skb_queue_empty(&txq->overflow_q)) {\n\t\tstruct sk_buff *skb = __skb_dequeue(&txq->overflow_q);\n\n\t\tiwl_op_mode_free_skb(trans->op_mode, skb);\n\t}\n\n\tspin_unlock_bh(&txq->lock);\n\n\t \n\tiwl_wake_queue(trans, txq);\n}\n\nstatic void iwl_txq_gen2_free_memory(struct iwl_trans *trans,\n\t\t\t\t     struct iwl_txq *txq)\n{\n\tstruct device *dev = trans->dev;\n\n\t \n\tif (txq->tfds) {\n\t\tdma_free_coherent(dev,\n\t\t\t\t  trans->txqs.tfd.size * txq->n_window,\n\t\t\t\t  txq->tfds, txq->dma_addr);\n\t\tdma_free_coherent(dev,\n\t\t\t\t  sizeof(*txq->first_tb_bufs) * txq->n_window,\n\t\t\t\t  txq->first_tb_bufs, txq->first_tb_dma);\n\t}\n\n\tkfree(txq->entries);\n\tif (txq->bc_tbl.addr)\n\t\tdma_pool_free(trans->txqs.bc_pool,\n\t\t\t      txq->bc_tbl.addr, txq->bc_tbl.dma);\n\tkfree(txq);\n}\n\n \nstatic void iwl_txq_gen2_free(struct iwl_trans *trans, int txq_id)\n{\n\tstruct iwl_txq *txq;\n\tint i;\n\n\tif (WARN_ONCE(txq_id >= IWL_MAX_TVQM_QUEUES,\n\t\t      \"queue %d out of range\", txq_id))\n\t\treturn;\n\n\ttxq = trans->txqs.txq[txq_id];\n\n\tif (WARN_ON(!txq))\n\t\treturn;\n\n\tiwl_txq_gen2_unmap(trans, txq_id);\n\n\t \n\tif (txq_id == trans->txqs.cmd.q_id)\n\t\tfor (i = 0; i < txq->n_window; i++) {\n\t\t\tkfree_sensitive(txq->entries[i].cmd);\n\t\t\tkfree_sensitive(txq->entries[i].free_buf);\n\t\t}\n\tdel_timer_sync(&txq->stuck_timer);\n\n\tiwl_txq_gen2_free_memory(trans, txq);\n\n\ttrans->txqs.txq[txq_id] = NULL;\n\n\tclear_bit(txq_id, trans->txqs.queue_used);\n}\n\n \nstatic int iwl_queue_init(struct iwl_txq *q, int slots_num)\n{\n\tq->n_window = slots_num;\n\n\t \n\tif (WARN_ON(!is_power_of_2(slots_num)))\n\t\treturn -EINVAL;\n\n\tq->low_mark = q->n_window / 4;\n\tif (q->low_mark < 4)\n\t\tq->low_mark = 4;\n\n\tq->high_mark = q->n_window / 8;\n\tif (q->high_mark < 2)\n\t\tq->high_mark = 2;\n\n\tq->write_ptr = 0;\n\tq->read_ptr = 0;\n\n\treturn 0;\n}\n\nint iwl_txq_init(struct iwl_trans *trans, struct iwl_txq *txq, int slots_num,\n\t\t bool cmd_queue)\n{\n\tint ret;\n\tu32 tfd_queue_max_size =\n\t\ttrans->trans_cfg->base_params->max_tfd_queue_size;\n\n\ttxq->need_update = false;\n\n\t \n\tif (WARN_ONCE(tfd_queue_max_size & (tfd_queue_max_size - 1),\n\t\t      \"Max tfd queue size must be a power of two, but is %d\",\n\t\t      tfd_queue_max_size))\n\t\treturn -EINVAL;\n\n\t \n\tret = iwl_queue_init(txq, slots_num);\n\tif (ret)\n\t\treturn ret;\n\n\tspin_lock_init(&txq->lock);\n\n\tif (cmd_queue) {\n\t\tstatic struct lock_class_key iwl_txq_cmd_queue_lock_class;\n\n\t\tlockdep_set_class(&txq->lock, &iwl_txq_cmd_queue_lock_class);\n\t}\n\n\t__skb_queue_head_init(&txq->overflow_q);\n\n\treturn 0;\n}\n\nvoid iwl_txq_free_tso_page(struct iwl_trans *trans, struct sk_buff *skb)\n{\n\tstruct page **page_ptr;\n\tstruct page *next;\n\n\tpage_ptr = (void *)((u8 *)skb->cb + trans->txqs.page_offs);\n\tnext = *page_ptr;\n\t*page_ptr = NULL;\n\n\twhile (next) {\n\t\tstruct page *tmp = next;\n\n\t\tnext = *(void **)((u8 *)page_address(next) + PAGE_SIZE -\n\t\t\t\t  sizeof(void *));\n\t\t__free_page(tmp);\n\t}\n}\n\nvoid iwl_txq_log_scd_error(struct iwl_trans *trans, struct iwl_txq *txq)\n{\n\tu32 txq_id = txq->id;\n\tu32 status;\n\tbool active;\n\tu8 fifo;\n\n\tif (trans->trans_cfg->gen2) {\n\t\tIWL_ERR(trans, \"Queue %d is stuck %d %d\\n\", txq_id,\n\t\t\ttxq->read_ptr, txq->write_ptr);\n\t\t \n\t\treturn;\n\t}\n\n\tstatus = iwl_read_prph(trans, SCD_QUEUE_STATUS_BITS(txq_id));\n\tfifo = (status >> SCD_QUEUE_STTS_REG_POS_TXF) & 0x7;\n\tactive = !!(status & BIT(SCD_QUEUE_STTS_REG_POS_ACTIVE));\n\n\tIWL_ERR(trans,\n\t\t\"Queue %d is %sactive on fifo %d and stuck for %u ms. SW [%d, %d] HW [%d, %d] FH TRB=0x0%x\\n\",\n\t\ttxq_id, active ? \"\" : \"in\", fifo,\n\t\tjiffies_to_msecs(txq->wd_timeout),\n\t\ttxq->read_ptr, txq->write_ptr,\n\t\tiwl_read_prph(trans, SCD_QUEUE_RDPTR(txq_id)) &\n\t\t\t(trans->trans_cfg->base_params->max_tfd_queue_size - 1),\n\t\t\tiwl_read_prph(trans, SCD_QUEUE_WRPTR(txq_id)) &\n\t\t\t(trans->trans_cfg->base_params->max_tfd_queue_size - 1),\n\t\t\tiwl_read_direct32(trans, FH_TX_TRB_REG(fifo)));\n}\n\nstatic void iwl_txq_stuck_timer(struct timer_list *t)\n{\n\tstruct iwl_txq *txq = from_timer(txq, t, stuck_timer);\n\tstruct iwl_trans *trans = txq->trans;\n\n\tspin_lock(&txq->lock);\n\t \n\tif (txq->read_ptr == txq->write_ptr) {\n\t\tspin_unlock(&txq->lock);\n\t\treturn;\n\t}\n\tspin_unlock(&txq->lock);\n\n\tiwl_txq_log_scd_error(trans, txq);\n\n\tiwl_force_nmi(trans);\n}\n\nstatic void iwl_txq_set_tfd_invalid_gen1(struct iwl_trans *trans,\n\t\t\t\t\t struct iwl_tfd *tfd)\n{\n\ttfd->num_tbs = 0;\n\n\tiwl_pcie_gen1_tfd_set_tb(trans, tfd, 0, trans->invalid_tx_cmd.dma,\n\t\t\t\t trans->invalid_tx_cmd.size);\n}\n\nint iwl_txq_alloc(struct iwl_trans *trans, struct iwl_txq *txq, int slots_num,\n\t\t  bool cmd_queue)\n{\n\tsize_t num_entries = trans->trans_cfg->gen2 ?\n\t\tslots_num : trans->trans_cfg->base_params->max_tfd_queue_size;\n\tsize_t tfd_sz;\n\tsize_t tb0_buf_sz;\n\tint i;\n\n\tif (WARN_ONCE(slots_num <= 0, \"Invalid slots num:%d\\n\", slots_num))\n\t\treturn -EINVAL;\n\n\tif (WARN_ON(txq->entries || txq->tfds))\n\t\treturn -EINVAL;\n\n\ttfd_sz = trans->txqs.tfd.size * num_entries;\n\n\ttimer_setup(&txq->stuck_timer, iwl_txq_stuck_timer, 0);\n\ttxq->trans = trans;\n\n\ttxq->n_window = slots_num;\n\n\ttxq->entries = kcalloc(slots_num,\n\t\t\t       sizeof(struct iwl_pcie_txq_entry),\n\t\t\t       GFP_KERNEL);\n\n\tif (!txq->entries)\n\t\tgoto error;\n\n\tif (cmd_queue)\n\t\tfor (i = 0; i < slots_num; i++) {\n\t\t\ttxq->entries[i].cmd =\n\t\t\t\tkmalloc(sizeof(struct iwl_device_cmd),\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!txq->entries[i].cmd)\n\t\t\t\tgoto error;\n\t\t}\n\n\t \n\ttxq->tfds = dma_alloc_coherent(trans->dev, tfd_sz,\n\t\t\t\t       &txq->dma_addr, GFP_KERNEL);\n\tif (!txq->tfds)\n\t\tgoto error;\n\n\tBUILD_BUG_ON(sizeof(*txq->first_tb_bufs) != IWL_FIRST_TB_SIZE_ALIGN);\n\n\ttb0_buf_sz = sizeof(*txq->first_tb_bufs) * slots_num;\n\n\ttxq->first_tb_bufs = dma_alloc_coherent(trans->dev, tb0_buf_sz,\n\t\t\t\t\t\t&txq->first_tb_dma,\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (!txq->first_tb_bufs)\n\t\tgoto err_free_tfds;\n\n\tfor (i = 0; i < num_entries; i++) {\n\t\tvoid *tfd = iwl_txq_get_tfd(trans, txq, i);\n\n\t\tif (trans->trans_cfg->gen2)\n\t\t\tiwl_txq_set_tfd_invalid_gen2(trans, tfd);\n\t\telse\n\t\t\tiwl_txq_set_tfd_invalid_gen1(trans, tfd);\n\t}\n\n\treturn 0;\nerr_free_tfds:\n\tdma_free_coherent(trans->dev, tfd_sz, txq->tfds, txq->dma_addr);\n\ttxq->tfds = NULL;\nerror:\n\tif (txq->entries && cmd_queue)\n\t\tfor (i = 0; i < slots_num; i++)\n\t\t\tkfree(txq->entries[i].cmd);\n\tkfree(txq->entries);\n\ttxq->entries = NULL;\n\n\treturn -ENOMEM;\n}\n\nstatic struct iwl_txq *\niwl_txq_dyn_alloc_dma(struct iwl_trans *trans, int size, unsigned int timeout)\n{\n\tsize_t bc_tbl_size, bc_tbl_entries;\n\tstruct iwl_txq *txq;\n\tint ret;\n\n\tWARN_ON(!trans->txqs.bc_tbl_size);\n\n\tbc_tbl_size = trans->txqs.bc_tbl_size;\n\tbc_tbl_entries = bc_tbl_size / sizeof(u16);\n\n\tif (WARN_ON(size > bc_tbl_entries))\n\t\treturn ERR_PTR(-EINVAL);\n\n\ttxq = kzalloc(sizeof(*txq), GFP_KERNEL);\n\tif (!txq)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\ttxq->bc_tbl.addr = dma_pool_alloc(trans->txqs.bc_pool, GFP_KERNEL,\n\t\t\t\t\t  &txq->bc_tbl.dma);\n\tif (!txq->bc_tbl.addr) {\n\t\tIWL_ERR(trans, \"Scheduler BC Table allocation failed\\n\");\n\t\tkfree(txq);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tret = iwl_txq_alloc(trans, txq, size, false);\n\tif (ret) {\n\t\tIWL_ERR(trans, \"Tx queue alloc failed\\n\");\n\t\tgoto error;\n\t}\n\tret = iwl_txq_init(trans, txq, size, false);\n\tif (ret) {\n\t\tIWL_ERR(trans, \"Tx queue init failed\\n\");\n\t\tgoto error;\n\t}\n\n\ttxq->wd_timeout = msecs_to_jiffies(timeout);\n\n\treturn txq;\n\nerror:\n\tiwl_txq_gen2_free_memory(trans, txq);\n\treturn ERR_PTR(ret);\n}\n\nstatic int iwl_txq_alloc_response(struct iwl_trans *trans, struct iwl_txq *txq,\n\t\t\t\t  struct iwl_host_cmd *hcmd)\n{\n\tstruct iwl_tx_queue_cfg_rsp *rsp;\n\tint ret, qid;\n\tu32 wr_ptr;\n\n\tif (WARN_ON(iwl_rx_packet_payload_len(hcmd->resp_pkt) !=\n\t\t    sizeof(*rsp))) {\n\t\tret = -EINVAL;\n\t\tgoto error_free_resp;\n\t}\n\n\trsp = (void *)hcmd->resp_pkt->data;\n\tqid = le16_to_cpu(rsp->queue_number);\n\twr_ptr = le16_to_cpu(rsp->write_pointer);\n\n\tif (qid >= ARRAY_SIZE(trans->txqs.txq)) {\n\t\tWARN_ONCE(1, \"queue index %d unsupported\", qid);\n\t\tret = -EIO;\n\t\tgoto error_free_resp;\n\t}\n\n\tif (test_and_set_bit(qid, trans->txqs.queue_used)) {\n\t\tWARN_ONCE(1, \"queue %d already used\", qid);\n\t\tret = -EIO;\n\t\tgoto error_free_resp;\n\t}\n\n\tif (WARN_ONCE(trans->txqs.txq[qid],\n\t\t      \"queue %d already allocated\\n\", qid)) {\n\t\tret = -EIO;\n\t\tgoto error_free_resp;\n\t}\n\n\ttxq->id = qid;\n\ttrans->txqs.txq[qid] = txq;\n\twr_ptr &= (trans->trans_cfg->base_params->max_tfd_queue_size - 1);\n\n\t \n\ttxq->read_ptr = wr_ptr;\n\ttxq->write_ptr = wr_ptr;\n\n\tIWL_DEBUG_TX_QUEUES(trans, \"Activate queue %d\\n\", qid);\n\n\tiwl_free_resp(hcmd);\n\treturn qid;\n\nerror_free_resp:\n\tiwl_free_resp(hcmd);\n\tiwl_txq_gen2_free_memory(trans, txq);\n\treturn ret;\n}\n\nint iwl_txq_dyn_alloc(struct iwl_trans *trans, u32 flags, u32 sta_mask,\n\t\t      u8 tid, int size, unsigned int timeout)\n{\n\tstruct iwl_txq *txq;\n\tunion {\n\t\tstruct iwl_tx_queue_cfg_cmd old;\n\t\tstruct iwl_scd_queue_cfg_cmd new;\n\t} cmd;\n\tstruct iwl_host_cmd hcmd = {\n\t\t.flags = CMD_WANT_SKB,\n\t};\n\tint ret;\n\n\tif (trans->trans_cfg->device_family == IWL_DEVICE_FAMILY_BZ &&\n\t    trans->hw_rev_step == SILICON_A_STEP)\n\t\tsize = 4096;\n\n\ttxq = iwl_txq_dyn_alloc_dma(trans, size, timeout);\n\tif (IS_ERR(txq))\n\t\treturn PTR_ERR(txq);\n\n\tif (trans->txqs.queue_alloc_cmd_ver == 0) {\n\t\tmemset(&cmd.old, 0, sizeof(cmd.old));\n\t\tcmd.old.tfdq_addr = cpu_to_le64(txq->dma_addr);\n\t\tcmd.old.byte_cnt_addr = cpu_to_le64(txq->bc_tbl.dma);\n\t\tcmd.old.cb_size = cpu_to_le32(TFD_QUEUE_CB_SIZE(size));\n\t\tcmd.old.flags = cpu_to_le16(flags | TX_QUEUE_CFG_ENABLE_QUEUE);\n\t\tcmd.old.tid = tid;\n\n\t\tif (hweight32(sta_mask) != 1) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto error;\n\t\t}\n\t\tcmd.old.sta_id = ffs(sta_mask) - 1;\n\n\t\thcmd.id = SCD_QUEUE_CFG;\n\t\thcmd.len[0] = sizeof(cmd.old);\n\t\thcmd.data[0] = &cmd.old;\n\t} else if (trans->txqs.queue_alloc_cmd_ver == 3) {\n\t\tmemset(&cmd.new, 0, sizeof(cmd.new));\n\t\tcmd.new.operation = cpu_to_le32(IWL_SCD_QUEUE_ADD);\n\t\tcmd.new.u.add.tfdq_dram_addr = cpu_to_le64(txq->dma_addr);\n\t\tcmd.new.u.add.bc_dram_addr = cpu_to_le64(txq->bc_tbl.dma);\n\t\tcmd.new.u.add.cb_size = cpu_to_le32(TFD_QUEUE_CB_SIZE(size));\n\t\tcmd.new.u.add.flags = cpu_to_le32(flags);\n\t\tcmd.new.u.add.sta_mask = cpu_to_le32(sta_mask);\n\t\tcmd.new.u.add.tid = tid;\n\n\t\thcmd.id = WIDE_ID(DATA_PATH_GROUP, SCD_QUEUE_CONFIG_CMD);\n\t\thcmd.len[0] = sizeof(cmd.new);\n\t\thcmd.data[0] = &cmd.new;\n\t} else {\n\t\tret = -EOPNOTSUPP;\n\t\tgoto error;\n\t}\n\n\tret = iwl_trans_send_cmd(trans, &hcmd);\n\tif (ret)\n\t\tgoto error;\n\n\treturn iwl_txq_alloc_response(trans, txq, &hcmd);\n\nerror:\n\tiwl_txq_gen2_free_memory(trans, txq);\n\treturn ret;\n}\n\nvoid iwl_txq_dyn_free(struct iwl_trans *trans, int queue)\n{\n\tif (WARN(queue >= IWL_MAX_TVQM_QUEUES,\n\t\t \"queue %d out of range\", queue))\n\t\treturn;\n\n\t \n\tif (!test_and_clear_bit(queue, trans->txqs.queue_used)) {\n\t\tWARN_ONCE(test_bit(STATUS_DEVICE_ENABLED, &trans->status),\n\t\t\t  \"queue %d not used\", queue);\n\t\treturn;\n\t}\n\n\tiwl_txq_gen2_free(trans, queue);\n\n\tIWL_DEBUG_TX_QUEUES(trans, \"Deactivate queue %d\\n\", queue);\n}\n\nvoid iwl_txq_gen2_tx_free(struct iwl_trans *trans)\n{\n\tint i;\n\n\tmemset(trans->txqs.queue_used, 0, sizeof(trans->txqs.queue_used));\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(trans->txqs.txq); i++) {\n\t\tif (!trans->txqs.txq[i])\n\t\t\tcontinue;\n\n\t\tiwl_txq_gen2_free(trans, i);\n\t}\n}\n\nint iwl_txq_gen2_init(struct iwl_trans *trans, int txq_id, int queue_size)\n{\n\tstruct iwl_txq *queue;\n\tint ret;\n\n\t \n\tif (!trans->txqs.txq[txq_id]) {\n\t\tqueue = kzalloc(sizeof(*queue), GFP_KERNEL);\n\t\tif (!queue) {\n\t\t\tIWL_ERR(trans, \"Not enough memory for tx queue\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\ttrans->txqs.txq[txq_id] = queue;\n\t\tret = iwl_txq_alloc(trans, queue, queue_size, true);\n\t\tif (ret) {\n\t\t\tIWL_ERR(trans, \"Tx %d queue init failed\\n\", txq_id);\n\t\t\tgoto error;\n\t\t}\n\t} else {\n\t\tqueue = trans->txqs.txq[txq_id];\n\t}\n\n\tret = iwl_txq_init(trans, queue, queue_size,\n\t\t\t   (txq_id == trans->txqs.cmd.q_id));\n\tif (ret) {\n\t\tIWL_ERR(trans, \"Tx %d queue alloc failed\\n\", txq_id);\n\t\tgoto error;\n\t}\n\ttrans->txqs.txq[txq_id]->id = txq_id;\n\tset_bit(txq_id, trans->txqs.queue_used);\n\n\treturn 0;\n\nerror:\n\tiwl_txq_gen2_tx_free(trans);\n\treturn ret;\n}\n\nstatic inline dma_addr_t iwl_txq_gen1_tfd_tb_get_addr(struct iwl_trans *trans,\n\t\t\t\t\t\t      struct iwl_tfd *tfd, u8 idx)\n{\n\tstruct iwl_tfd_tb *tb = &tfd->tbs[idx];\n\tdma_addr_t addr;\n\tdma_addr_t hi_len;\n\n\taddr = get_unaligned_le32(&tb->lo);\n\n\tif (sizeof(dma_addr_t) <= sizeof(u32))\n\t\treturn addr;\n\n\thi_len = le16_to_cpu(tb->hi_n_len) & 0xF;\n\n\t \n\treturn addr | ((hi_len << 16) << 16);\n}\n\nvoid iwl_txq_gen1_tfd_unmap(struct iwl_trans *trans,\n\t\t\t    struct iwl_cmd_meta *meta,\n\t\t\t    struct iwl_txq *txq, int index)\n{\n\tint i, num_tbs;\n\tstruct iwl_tfd *tfd = iwl_txq_get_tfd(trans, txq, index);\n\n\t \n\tnum_tbs = iwl_txq_gen1_tfd_get_num_tbs(trans, tfd);\n\n\tif (num_tbs > trans->txqs.tfd.max_tbs) {\n\t\tIWL_ERR(trans, \"Too many chunks: %i\\n\", num_tbs);\n\t\t \n\t\treturn;\n\t}\n\n\t \n\n\tfor (i = 1; i < num_tbs; i++) {\n\t\tif (meta->tbs & BIT(i))\n\t\t\tdma_unmap_page(trans->dev,\n\t\t\t\t       iwl_txq_gen1_tfd_tb_get_addr(trans,\n\t\t\t\t\t\t\t\t    tfd, i),\n\t\t\t\t       iwl_txq_gen1_tfd_tb_get_len(trans,\n\t\t\t\t\t\t\t\t   tfd, i),\n\t\t\t\t       DMA_TO_DEVICE);\n\t\telse\n\t\t\tdma_unmap_single(trans->dev,\n\t\t\t\t\t iwl_txq_gen1_tfd_tb_get_addr(trans,\n\t\t\t\t\t\t\t\t      tfd, i),\n\t\t\t\t\t iwl_txq_gen1_tfd_tb_get_len(trans,\n\t\t\t\t\t\t\t\t     tfd, i),\n\t\t\t\t\t DMA_TO_DEVICE);\n\t}\n\n\tmeta->tbs = 0;\n\n\tiwl_txq_set_tfd_invalid_gen1(trans, tfd);\n}\n\n#define IWL_TX_CRC_SIZE 4\n#define IWL_TX_DELIMITER_SIZE 4\n\n \nvoid iwl_txq_gen1_update_byte_cnt_tbl(struct iwl_trans *trans,\n\t\t\t\t      struct iwl_txq *txq, u16 byte_cnt,\n\t\t\t\t      int num_tbs)\n{\n\tstruct iwlagn_scd_bc_tbl *scd_bc_tbl;\n\tint write_ptr = txq->write_ptr;\n\tint txq_id = txq->id;\n\tu8 sec_ctl = 0;\n\tu16 len = byte_cnt + IWL_TX_CRC_SIZE + IWL_TX_DELIMITER_SIZE;\n\t__le16 bc_ent;\n\tstruct iwl_device_tx_cmd *dev_cmd = txq->entries[txq->write_ptr].cmd;\n\tstruct iwl_tx_cmd *tx_cmd = (void *)dev_cmd->payload;\n\tu8 sta_id = tx_cmd->sta_id;\n\n\tscd_bc_tbl = trans->txqs.scd_bc_tbls.addr;\n\n\tsec_ctl = tx_cmd->sec_ctl;\n\n\tswitch (sec_ctl & TX_CMD_SEC_MSK) {\n\tcase TX_CMD_SEC_CCM:\n\t\tlen += IEEE80211_CCMP_MIC_LEN;\n\t\tbreak;\n\tcase TX_CMD_SEC_TKIP:\n\t\tlen += IEEE80211_TKIP_ICV_LEN;\n\t\tbreak;\n\tcase TX_CMD_SEC_WEP:\n\t\tlen += IEEE80211_WEP_IV_LEN + IEEE80211_WEP_ICV_LEN;\n\t\tbreak;\n\t}\n\tif (trans->txqs.bc_table_dword)\n\t\tlen = DIV_ROUND_UP(len, 4);\n\n\tif (WARN_ON(len > 0xFFF || write_ptr >= TFD_QUEUE_SIZE_MAX))\n\t\treturn;\n\n\tbc_ent = cpu_to_le16(len | (sta_id << 12));\n\n\tscd_bc_tbl[txq_id].tfd_offset[write_ptr] = bc_ent;\n\n\tif (write_ptr < TFD_QUEUE_SIZE_BC_DUP)\n\t\tscd_bc_tbl[txq_id].tfd_offset[TFD_QUEUE_SIZE_MAX + write_ptr] =\n\t\t\tbc_ent;\n}\n\nvoid iwl_txq_gen1_inval_byte_cnt_tbl(struct iwl_trans *trans,\n\t\t\t\t     struct iwl_txq *txq)\n{\n\tstruct iwlagn_scd_bc_tbl *scd_bc_tbl = trans->txqs.scd_bc_tbls.addr;\n\tint txq_id = txq->id;\n\tint read_ptr = txq->read_ptr;\n\tu8 sta_id = 0;\n\t__le16 bc_ent;\n\tstruct iwl_device_tx_cmd *dev_cmd = txq->entries[read_ptr].cmd;\n\tstruct iwl_tx_cmd *tx_cmd = (void *)dev_cmd->payload;\n\n\tWARN_ON(read_ptr >= TFD_QUEUE_SIZE_MAX);\n\n\tif (txq_id != trans->txqs.cmd.q_id)\n\t\tsta_id = tx_cmd->sta_id;\n\n\tbc_ent = cpu_to_le16(1 | (sta_id << 12));\n\n\tscd_bc_tbl[txq_id].tfd_offset[read_ptr] = bc_ent;\n\n\tif (read_ptr < TFD_QUEUE_SIZE_BC_DUP)\n\t\tscd_bc_tbl[txq_id].tfd_offset[TFD_QUEUE_SIZE_MAX + read_ptr] =\n\t\t\tbc_ent;\n}\n\n \nvoid iwl_txq_free_tfd(struct iwl_trans *trans, struct iwl_txq *txq)\n{\n\t \n\tint rd_ptr = txq->read_ptr;\n\tint idx = iwl_txq_get_cmd_index(txq, rd_ptr);\n\tstruct sk_buff *skb;\n\n\tlockdep_assert_held(&txq->lock);\n\n\tif (!txq->entries)\n\t\treturn;\n\n\t \n\tif (trans->trans_cfg->gen2)\n\t\tiwl_txq_gen2_tfd_unmap(trans, &txq->entries[idx].meta,\n\t\t\t\t       iwl_txq_get_tfd(trans, txq, rd_ptr));\n\telse\n\t\tiwl_txq_gen1_tfd_unmap(trans, &txq->entries[idx].meta,\n\t\t\t\t       txq, rd_ptr);\n\n\t \n\tskb = txq->entries[idx].skb;\n\n\t \n\tif (skb) {\n\t\tiwl_op_mode_free_skb(trans->op_mode, skb);\n\t\ttxq->entries[idx].skb = NULL;\n\t}\n}\n\nvoid iwl_txq_progress(struct iwl_txq *txq)\n{\n\tlockdep_assert_held(&txq->lock);\n\n\tif (!txq->wd_timeout)\n\t\treturn;\n\n\t \n\tif (txq->frozen)\n\t\treturn;\n\n\t \n\tif (txq->read_ptr == txq->write_ptr)\n\t\tdel_timer(&txq->stuck_timer);\n\telse\n\t\tmod_timer(&txq->stuck_timer, jiffies + txq->wd_timeout);\n}\n\n \nvoid iwl_txq_reclaim(struct iwl_trans *trans, int txq_id, int ssn,\n\t\t     struct sk_buff_head *skbs, bool is_flush)\n{\n\tstruct iwl_txq *txq = trans->txqs.txq[txq_id];\n\tint tfd_num, read_ptr, last_to_free;\n\n\t \n\tif (WARN_ON(txq_id == trans->txqs.cmd.q_id))\n\t\treturn;\n\n\tif (WARN_ON(!txq))\n\t\treturn;\n\n\ttfd_num = iwl_txq_get_cmd_index(txq, ssn);\n\tread_ptr = iwl_txq_get_cmd_index(txq, txq->read_ptr);\n\n\tspin_lock_bh(&txq->lock);\n\n\tif (!test_bit(txq_id, trans->txqs.queue_used)) {\n\t\tIWL_DEBUG_TX_QUEUES(trans, \"Q %d inactive - ignoring idx %d\\n\",\n\t\t\t\t    txq_id, ssn);\n\t\tgoto out;\n\t}\n\n\tif (read_ptr == tfd_num)\n\t\tgoto out;\n\n\tIWL_DEBUG_TX_REPLY(trans, \"[Q %d] %d -> %d (%d)\\n\",\n\t\t\t   txq_id, txq->read_ptr, tfd_num, ssn);\n\n\t \n\tlast_to_free = iwl_txq_dec_wrap(trans, tfd_num);\n\n\tif (!iwl_txq_used(txq, last_to_free)) {\n\t\tIWL_ERR(trans,\n\t\t\t\"%s: Read index for txq id (%d), last_to_free %d is out of range [0-%d] %d %d.\\n\",\n\t\t\t__func__, txq_id, last_to_free,\n\t\t\ttrans->trans_cfg->base_params->max_tfd_queue_size,\n\t\t\ttxq->write_ptr, txq->read_ptr);\n\n\t\tiwl_op_mode_time_point(trans->op_mode,\n\t\t\t\t       IWL_FW_INI_TIME_POINT_FAKE_TX,\n\t\t\t\t       NULL);\n\t\tgoto out;\n\t}\n\n\tif (WARN_ON(!skb_queue_empty(skbs)))\n\t\tgoto out;\n\n\tfor (;\n\t     read_ptr != tfd_num;\n\t     txq->read_ptr = iwl_txq_inc_wrap(trans, txq->read_ptr),\n\t     read_ptr = iwl_txq_get_cmd_index(txq, txq->read_ptr)) {\n\t\tstruct sk_buff *skb = txq->entries[read_ptr].skb;\n\n\t\tif (WARN_ON_ONCE(!skb))\n\t\t\tcontinue;\n\n\t\tiwl_txq_free_tso_page(trans, skb);\n\n\t\t__skb_queue_tail(skbs, skb);\n\n\t\ttxq->entries[read_ptr].skb = NULL;\n\n\t\tif (!trans->trans_cfg->gen2)\n\t\t\tiwl_txq_gen1_inval_byte_cnt_tbl(trans, txq);\n\n\t\tiwl_txq_free_tfd(trans, txq);\n\t}\n\n\tiwl_txq_progress(txq);\n\n\tif (iwl_txq_space(trans, txq) > txq->low_mark &&\n\t    test_bit(txq_id, trans->txqs.queue_stopped)) {\n\t\tstruct sk_buff_head overflow_skbs;\n\t\tstruct sk_buff *skb;\n\n\t\t__skb_queue_head_init(&overflow_skbs);\n\t\tskb_queue_splice_init(&txq->overflow_q,\n\t\t\t\t      is_flush ? skbs : &overflow_skbs);\n\n\t\t \n\t\ttxq->overflow_tx = true;\n\n\t\t \n\t\tspin_unlock_bh(&txq->lock);\n\n\t\twhile ((skb = __skb_dequeue(&overflow_skbs))) {\n\t\t\tstruct iwl_device_tx_cmd *dev_cmd_ptr;\n\n\t\t\tdev_cmd_ptr = *(void **)((u8 *)skb->cb +\n\t\t\t\t\t\t trans->txqs.dev_cmd_offs);\n\n\t\t\t \n\t\t\tiwl_trans_tx(trans, skb, dev_cmd_ptr, txq_id);\n\t\t}\n\n\t\tif (iwl_txq_space(trans, txq) > txq->low_mark)\n\t\t\tiwl_wake_queue(trans, txq);\n\n\t\tspin_lock_bh(&txq->lock);\n\t\ttxq->overflow_tx = false;\n\t}\n\nout:\n\tspin_unlock_bh(&txq->lock);\n}\n\n \nvoid iwl_txq_set_q_ptrs(struct iwl_trans *trans, int txq_id, int ptr)\n{\n\tstruct iwl_txq *txq = trans->txqs.txq[txq_id];\n\n\tspin_lock_bh(&txq->lock);\n\n\ttxq->write_ptr = ptr;\n\ttxq->read_ptr = txq->write_ptr;\n\n\tspin_unlock_bh(&txq->lock);\n}\n\nvoid iwl_trans_txq_freeze_timer(struct iwl_trans *trans, unsigned long txqs,\n\t\t\t\tbool freeze)\n{\n\tint queue;\n\n\tfor_each_set_bit(queue, &txqs, BITS_PER_LONG) {\n\t\tstruct iwl_txq *txq = trans->txqs.txq[queue];\n\t\tunsigned long now;\n\n\t\tspin_lock_bh(&txq->lock);\n\n\t\tnow = jiffies;\n\n\t\tif (txq->frozen == freeze)\n\t\t\tgoto next_queue;\n\n\t\tIWL_DEBUG_TX_QUEUES(trans, \"%s TXQ %d\\n\",\n\t\t\t\t    freeze ? \"Freezing\" : \"Waking\", queue);\n\n\t\ttxq->frozen = freeze;\n\n\t\tif (txq->read_ptr == txq->write_ptr)\n\t\t\tgoto next_queue;\n\n\t\tif (freeze) {\n\t\t\tif (unlikely(time_after(now,\n\t\t\t\t\t\ttxq->stuck_timer.expires))) {\n\t\t\t\t \n\t\t\t\tgoto next_queue;\n\t\t\t}\n\t\t\t \n\t\t\ttxq->frozen_expiry_remainder =\n\t\t\t\ttxq->stuck_timer.expires - now;\n\t\t\tdel_timer(&txq->stuck_timer);\n\t\t\tgoto next_queue;\n\t\t}\n\n\t\t \n\t\tmod_timer(&txq->stuck_timer,\n\t\t\t  now + txq->frozen_expiry_remainder);\n\nnext_queue:\n\t\tspin_unlock_bh(&txq->lock);\n\t}\n}\n\n#define HOST_COMPLETE_TIMEOUT\t(2 * HZ)\n\nstatic int iwl_trans_txq_send_hcmd_sync(struct iwl_trans *trans,\n\t\t\t\t\tstruct iwl_host_cmd *cmd)\n{\n\tconst char *cmd_str = iwl_get_cmd_string(trans, cmd->id);\n\tstruct iwl_txq *txq = trans->txqs.txq[trans->txqs.cmd.q_id];\n\tint cmd_idx;\n\tint ret;\n\n\tIWL_DEBUG_INFO(trans, \"Attempting to send sync command %s\\n\", cmd_str);\n\n\tif (WARN(test_and_set_bit(STATUS_SYNC_HCMD_ACTIVE,\n\t\t\t\t  &trans->status),\n\t\t \"Command %s: a command is already active!\\n\", cmd_str))\n\t\treturn -EIO;\n\n\tIWL_DEBUG_INFO(trans, \"Setting HCMD_ACTIVE for command %s\\n\", cmd_str);\n\n\tcmd_idx = trans->ops->send_cmd(trans, cmd);\n\tif (cmd_idx < 0) {\n\t\tret = cmd_idx;\n\t\tclear_bit(STATUS_SYNC_HCMD_ACTIVE, &trans->status);\n\t\tIWL_ERR(trans, \"Error sending %s: enqueue_hcmd failed: %d\\n\",\n\t\t\tcmd_str, ret);\n\t\treturn ret;\n\t}\n\n\tret = wait_event_timeout(trans->wait_command_queue,\n\t\t\t\t !test_bit(STATUS_SYNC_HCMD_ACTIVE,\n\t\t\t\t\t   &trans->status),\n\t\t\t\t HOST_COMPLETE_TIMEOUT);\n\tif (!ret) {\n\t\tIWL_ERR(trans, \"Error sending %s: time out after %dms.\\n\",\n\t\t\tcmd_str, jiffies_to_msecs(HOST_COMPLETE_TIMEOUT));\n\n\t\tIWL_ERR(trans, \"Current CMD queue read_ptr %d write_ptr %d\\n\",\n\t\t\ttxq->read_ptr, txq->write_ptr);\n\n\t\tclear_bit(STATUS_SYNC_HCMD_ACTIVE, &trans->status);\n\t\tIWL_DEBUG_INFO(trans, \"Clearing HCMD_ACTIVE for command %s\\n\",\n\t\t\t       cmd_str);\n\t\tret = -ETIMEDOUT;\n\n\t\tiwl_trans_sync_nmi(trans);\n\t\tgoto cancel;\n\t}\n\n\tif (test_bit(STATUS_FW_ERROR, &trans->status)) {\n\t\tif (!test_and_clear_bit(STATUS_SUPPRESS_CMD_ERROR_ONCE,\n\t\t\t\t\t&trans->status)) {\n\t\t\tIWL_ERR(trans, \"FW error in SYNC CMD %s\\n\", cmd_str);\n\t\t\tdump_stack();\n\t\t}\n\t\tret = -EIO;\n\t\tgoto cancel;\n\t}\n\n\tif (!(cmd->flags & CMD_SEND_IN_RFKILL) &&\n\t    test_bit(STATUS_RFKILL_OPMODE, &trans->status)) {\n\t\tIWL_DEBUG_RF_KILL(trans, \"RFKILL in SYNC CMD... no rsp\\n\");\n\t\tret = -ERFKILL;\n\t\tgoto cancel;\n\t}\n\n\tif ((cmd->flags & CMD_WANT_SKB) && !cmd->resp_pkt) {\n\t\tIWL_ERR(trans, \"Error: Response NULL in '%s'\\n\", cmd_str);\n\t\tret = -EIO;\n\t\tgoto cancel;\n\t}\n\n\treturn 0;\n\ncancel:\n\tif (cmd->flags & CMD_WANT_SKB) {\n\t\t \n\t\ttxq->entries[cmd_idx].meta.flags &= ~CMD_WANT_SKB;\n\t}\n\n\tif (cmd->resp_pkt) {\n\t\tiwl_free_resp(cmd);\n\t\tcmd->resp_pkt = NULL;\n\t}\n\n\treturn ret;\n}\n\nint iwl_trans_txq_send_hcmd(struct iwl_trans *trans,\n\t\t\t    struct iwl_host_cmd *cmd)\n{\n\t \n\tif (test_bit(STATUS_TRANS_DEAD, &trans->status))\n\t\treturn -ENODEV;\n\n\tif (!(cmd->flags & CMD_SEND_IN_RFKILL) &&\n\t    test_bit(STATUS_RFKILL_OPMODE, &trans->status)) {\n\t\tIWL_DEBUG_RF_KILL(trans, \"Dropping CMD 0x%x: RF KILL\\n\",\n\t\t\t\t  cmd->id);\n\t\treturn -ERFKILL;\n\t}\n\n\tif (unlikely(trans->system_pm_mode == IWL_PLAT_PM_MODE_D3 &&\n\t\t     !(cmd->flags & CMD_SEND_IN_D3))) {\n\t\tIWL_DEBUG_WOWLAN(trans, \"Dropping CMD 0x%x: D3\\n\", cmd->id);\n\t\treturn -EHOSTDOWN;\n\t}\n\n\tif (cmd->flags & CMD_ASYNC) {\n\t\tint ret;\n\n\t\t \n\t\tif (WARN_ON(cmd->flags & CMD_WANT_SKB))\n\t\t\treturn -EINVAL;\n\n\t\tret = trans->ops->send_cmd(trans, cmd);\n\t\tif (ret < 0) {\n\t\t\tIWL_ERR(trans,\n\t\t\t\t\"Error sending %s: enqueue_hcmd failed: %d\\n\",\n\t\t\t\tiwl_get_cmd_string(trans, cmd->id), ret);\n\t\t\treturn ret;\n\t\t}\n\t\treturn 0;\n\t}\n\n\treturn iwl_trans_txq_send_hcmd_sync(trans, cmd);\n}\n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}