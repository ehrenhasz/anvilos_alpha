{
  "module_name": "iwl-trans.h",
  "hash_id": "8f10c7776f3447036e2601433875df506facc63e69bcdaaca5c6847452f7b35b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wireless/intel/iwlwifi/iwl-trans.h",
  "human_readable_source": " \n \n#ifndef __iwl_trans_h__\n#define __iwl_trans_h__\n\n#include <linux/ieee80211.h>\n#include <linux/mm.h>  \n#include <linux/lockdep.h>\n#include <linux/kernel.h>\n\n#include \"iwl-debug.h\"\n#include \"iwl-config.h\"\n#include \"fw/img.h\"\n#include \"iwl-op-mode.h\"\n#include <linux/firmware.h>\n#include \"fw/api/cmdhdr.h\"\n#include \"fw/api/txq.h\"\n#include \"fw/api/dbg-tlv.h\"\n#include \"iwl-dbg-tlv.h\"\n\n \n\n \n\n \n#define IWL_FW_DBG_DOMAIN_POS\t16\n#define IWL_FW_DBG_DOMAIN\tBIT(IWL_FW_DBG_DOMAIN_POS)\n\n#define IWL_TRANS_FW_DBG_DOMAIN(trans)\tIWL_FW_INI_DOMAIN_ALWAYS_ON\n\n#define FH_RSCSR_FRAME_SIZE_MSK\t\t0x00003FFF\t \n#define FH_RSCSR_FRAME_INVALID\t\t0x55550000\n#define FH_RSCSR_FRAME_ALIGN\t\t0x40\n#define FH_RSCSR_RPA_EN\t\t\tBIT(25)\n#define FH_RSCSR_RADA_EN\t\tBIT(26)\n#define FH_RSCSR_RXQ_POS\t\t16\n#define FH_RSCSR_RXQ_MASK\t\t0x3F0000\n\nstruct iwl_rx_packet {\n\t \n\t__le32 len_n_flags;\n\tstruct iwl_cmd_header hdr;\n\tu8 data[];\n} __packed;\n\nstatic inline u32 iwl_rx_packet_len(const struct iwl_rx_packet *pkt)\n{\n\treturn le32_to_cpu(pkt->len_n_flags) & FH_RSCSR_FRAME_SIZE_MSK;\n}\n\nstatic inline u32 iwl_rx_packet_payload_len(const struct iwl_rx_packet *pkt)\n{\n\treturn iwl_rx_packet_len(pkt) - sizeof(pkt->hdr);\n}\n\n \nenum CMD_MODE {\n\tCMD_ASYNC\t\t= BIT(0),\n\tCMD_WANT_SKB\t\t= BIT(1),\n\tCMD_SEND_IN_RFKILL\t= BIT(2),\n\tCMD_WANT_ASYNC_CALLBACK\t= BIT(3),\n\tCMD_SEND_IN_D3          = BIT(4),\n};\n\n#define DEF_CMD_PAYLOAD_SIZE 320\n\n \nstruct iwl_device_cmd {\n\tunion {\n\t\tstruct {\n\t\t\tstruct iwl_cmd_header hdr;\t \n\t\t\tu8 payload[DEF_CMD_PAYLOAD_SIZE];\n\t\t};\n\t\tstruct {\n\t\t\tstruct iwl_cmd_header_wide hdr_wide;\n\t\t\tu8 payload_wide[DEF_CMD_PAYLOAD_SIZE -\n\t\t\t\t\tsizeof(struct iwl_cmd_header_wide) +\n\t\t\t\t\tsizeof(struct iwl_cmd_header)];\n\t\t};\n\t};\n} __packed;\n\n \nstruct iwl_device_tx_cmd {\n\tstruct iwl_cmd_header hdr;\n\tu8 payload[];\n} __packed;\n\n#define TFD_MAX_PAYLOAD_SIZE (sizeof(struct iwl_device_cmd))\n\n \n#define IWL_MAX_CMD_TBS_PER_TFD\t2\n\n \n#define IWL_TRANS_MAX_FRAGS(trans) ((trans)->txqs.tfd.max_tbs - 3)\n\n \nenum iwl_hcmd_dataflag {\n\tIWL_HCMD_DFL_NOCOPY\t= BIT(0),\n\tIWL_HCMD_DFL_DUP\t= BIT(1),\n};\n\nenum iwl_error_event_table_status {\n\tIWL_ERROR_EVENT_TABLE_LMAC1 = BIT(0),\n\tIWL_ERROR_EVENT_TABLE_LMAC2 = BIT(1),\n\tIWL_ERROR_EVENT_TABLE_UMAC = BIT(2),\n\tIWL_ERROR_EVENT_TABLE_TCM1 = BIT(3),\n\tIWL_ERROR_EVENT_TABLE_TCM2 = BIT(4),\n\tIWL_ERROR_EVENT_TABLE_RCM1 = BIT(5),\n\tIWL_ERROR_EVENT_TABLE_RCM2 = BIT(6),\n};\n\n \nstruct iwl_host_cmd {\n\tconst void *data[IWL_MAX_CMD_TBS_PER_TFD];\n\tstruct iwl_rx_packet *resp_pkt;\n\tunsigned long _rx_page_addr;\n\tu32 _rx_page_order;\n\n\tu32 flags;\n\tu32 id;\n\tu16 len[IWL_MAX_CMD_TBS_PER_TFD];\n\tu8 dataflags[IWL_MAX_CMD_TBS_PER_TFD];\n};\n\nstatic inline void iwl_free_resp(struct iwl_host_cmd *cmd)\n{\n\tfree_pages(cmd->_rx_page_addr, cmd->_rx_page_order);\n}\n\nstruct iwl_rx_cmd_buffer {\n\tstruct page *_page;\n\tint _offset;\n\tbool _page_stolen;\n\tu32 _rx_page_order;\n\tunsigned int truesize;\n};\n\nstatic inline void *rxb_addr(struct iwl_rx_cmd_buffer *r)\n{\n\treturn (void *)((unsigned long)page_address(r->_page) + r->_offset);\n}\n\nstatic inline int rxb_offset(struct iwl_rx_cmd_buffer *r)\n{\n\treturn r->_offset;\n}\n\nstatic inline struct page *rxb_steal_page(struct iwl_rx_cmd_buffer *r)\n{\n\tr->_page_stolen = true;\n\tget_page(r->_page);\n\treturn r->_page;\n}\n\nstatic inline void iwl_free_rxb(struct iwl_rx_cmd_buffer *r)\n{\n\t__free_pages(r->_page, r->_rx_page_order);\n}\n\n#define MAX_NO_RECLAIM_CMDS\t6\n\n#define IWL_MASK(lo, hi) ((1 << (hi)) | ((1 << (hi)) - (1 << (lo))))\n\n \n#define IWL_MAX_HW_QUEUES\t\t32\n#define IWL_MAX_TVQM_QUEUES\t\t512\n\n#define IWL_MAX_TID_COUNT\t8\n#define IWL_MGMT_TID\t\t15\n#define IWL_FRAME_LIMIT\t64\n#define IWL_MAX_RX_HW_QUEUES\t16\n#define IWL_9000_MAX_RX_HW_QUEUES\t6\n\n \nenum iwl_d3_status {\n\tIWL_D3_STATUS_ALIVE,\n\tIWL_D3_STATUS_RESET,\n};\n\n \nenum iwl_trans_status {\n\tSTATUS_SYNC_HCMD_ACTIVE,\n\tSTATUS_DEVICE_ENABLED,\n\tSTATUS_TPOWER_PMI,\n\tSTATUS_INT_ENABLED,\n\tSTATUS_RFKILL_HW,\n\tSTATUS_RFKILL_OPMODE,\n\tSTATUS_FW_ERROR,\n\tSTATUS_TRANS_GOING_IDLE,\n\tSTATUS_TRANS_IDLE,\n\tSTATUS_TRANS_DEAD,\n\tSTATUS_SUPPRESS_CMD_ERROR_ONCE,\n};\n\nstatic inline int\niwl_trans_get_rb_size_order(enum iwl_amsdu_size rb_size)\n{\n\tswitch (rb_size) {\n\tcase IWL_AMSDU_2K:\n\t\treturn get_order(2 * 1024);\n\tcase IWL_AMSDU_4K:\n\t\treturn get_order(4 * 1024);\n\tcase IWL_AMSDU_8K:\n\t\treturn get_order(8 * 1024);\n\tcase IWL_AMSDU_12K:\n\t\treturn get_order(16 * 1024);\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn -1;\n\t}\n}\n\nstatic inline int\niwl_trans_get_rb_size(enum iwl_amsdu_size rb_size)\n{\n\tswitch (rb_size) {\n\tcase IWL_AMSDU_2K:\n\t\treturn 2 * 1024;\n\tcase IWL_AMSDU_4K:\n\t\treturn 4 * 1024;\n\tcase IWL_AMSDU_8K:\n\t\treturn 8 * 1024;\n\tcase IWL_AMSDU_12K:\n\t\treturn 16 * 1024;\n\tdefault:\n\t\tWARN_ON(1);\n\t\treturn 0;\n\t}\n}\n\nstruct iwl_hcmd_names {\n\tu8 cmd_id;\n\tconst char *const cmd_name;\n};\n\n#define HCMD_NAME(x)\t\\\n\t{ .cmd_id = x, .cmd_name = #x }\n\nstruct iwl_hcmd_arr {\n\tconst struct iwl_hcmd_names *arr;\n\tint size;\n};\n\n#define HCMD_ARR(x)\t\\\n\t{ .arr = x, .size = ARRAY_SIZE(x) }\n\n \nstruct iwl_dump_sanitize_ops {\n\tvoid (*frob_txf)(void *ctx, void *buf, size_t buflen);\n\tvoid (*frob_hcmd)(void *ctx, void *hcmd, size_t buflen);\n\tvoid (*frob_mem)(void *ctx, u32 mem_addr, void *mem, size_t buflen);\n};\n\n \nstruct iwl_trans_config {\n\tstruct iwl_op_mode *op_mode;\n\n\tu8 cmd_queue;\n\tu8 cmd_fifo;\n\tunsigned int cmd_q_wdg_timeout;\n\tconst u8 *no_reclaim_cmds;\n\tunsigned int n_no_reclaim_cmds;\n\n\tenum iwl_amsdu_size rx_buf_size;\n\tbool bc_table_dword;\n\tbool scd_set_active;\n\tconst struct iwl_hcmd_arr *command_groups;\n\tint command_groups_size;\n\n\tu8 cb_data_offs;\n\tbool fw_reset_handshake;\n\tu8 queue_alloc_cmd_ver;\n};\n\nstruct iwl_trans_dump_data {\n\tu32 len;\n\tu8 data[];\n};\n\nstruct iwl_trans;\n\nstruct iwl_trans_txq_scd_cfg {\n\tu8 fifo;\n\tu8 sta_id;\n\tu8 tid;\n\tbool aggregate;\n\tint frame_limit;\n};\n\n \nstruct iwl_trans_rxq_dma_data {\n\tu64 fr_bd_cb;\n\tu32 fr_bd_wid;\n\tu64 urbd_stts_wrptr;\n\tu64 ur_bd_cb;\n};\n\n \n#define IPC_DRAM_MAP_ENTRY_NUM_MAX 64\n\n \nstruct iwl_pnvm_image {\n\tstruct {\n\t\tconst void *data;\n\t\tu32 len;\n\t} chunks[IPC_DRAM_MAP_ENTRY_NUM_MAX];\n\tu32 n_chunks;\n\tu32 version;\n};\n\n \nstruct iwl_trans_ops {\n\n\tint (*start_hw)(struct iwl_trans *iwl_trans);\n\tvoid (*op_mode_leave)(struct iwl_trans *iwl_trans);\n\tint (*start_fw)(struct iwl_trans *trans, const struct fw_img *fw,\n\t\t\tbool run_in_rfkill);\n\tvoid (*fw_alive)(struct iwl_trans *trans, u32 scd_addr);\n\tvoid (*stop_device)(struct iwl_trans *trans);\n\n\tint (*d3_suspend)(struct iwl_trans *trans, bool test, bool reset);\n\tint (*d3_resume)(struct iwl_trans *trans, enum iwl_d3_status *status,\n\t\t\t bool test, bool reset);\n\n\tint (*send_cmd)(struct iwl_trans *trans, struct iwl_host_cmd *cmd);\n\n\tint (*tx)(struct iwl_trans *trans, struct sk_buff *skb,\n\t\t  struct iwl_device_tx_cmd *dev_cmd, int queue);\n\tvoid (*reclaim)(struct iwl_trans *trans, int queue, int ssn,\n\t\t\tstruct sk_buff_head *skbs, bool is_flush);\n\n\tvoid (*set_q_ptrs)(struct iwl_trans *trans, int queue, int ptr);\n\n\tbool (*txq_enable)(struct iwl_trans *trans, int queue, u16 ssn,\n\t\t\t   const struct iwl_trans_txq_scd_cfg *cfg,\n\t\t\t   unsigned int queue_wdg_timeout);\n\tvoid (*txq_disable)(struct iwl_trans *trans, int queue,\n\t\t\t    bool configure_scd);\n\t \n\tint (*txq_alloc)(struct iwl_trans *trans, u32 flags,\n\t\t\t u32 sta_mask, u8 tid,\n\t\t\t int size, unsigned int queue_wdg_timeout);\n\tvoid (*txq_free)(struct iwl_trans *trans, int queue);\n\tint (*rxq_dma_data)(struct iwl_trans *trans, int queue,\n\t\t\t    struct iwl_trans_rxq_dma_data *data);\n\n\tvoid (*txq_set_shared_mode)(struct iwl_trans *trans, u32 txq_id,\n\t\t\t\t    bool shared);\n\n\tint (*wait_tx_queues_empty)(struct iwl_trans *trans, u32 txq_bm);\n\tint (*wait_txq_empty)(struct iwl_trans *trans, int queue);\n\tvoid (*freeze_txq_timer)(struct iwl_trans *trans, unsigned long txqs,\n\t\t\t\t bool freeze);\n\tvoid (*block_txq_ptrs)(struct iwl_trans *trans, bool block);\n\n\tvoid (*write8)(struct iwl_trans *trans, u32 ofs, u8 val);\n\tvoid (*write32)(struct iwl_trans *trans, u32 ofs, u32 val);\n\tu32 (*read32)(struct iwl_trans *trans, u32 ofs);\n\tu32 (*read_prph)(struct iwl_trans *trans, u32 ofs);\n\tvoid (*write_prph)(struct iwl_trans *trans, u32 ofs, u32 val);\n\tint (*read_mem)(struct iwl_trans *trans, u32 addr,\n\t\t\tvoid *buf, int dwords);\n\tint (*write_mem)(struct iwl_trans *trans, u32 addr,\n\t\t\t const void *buf, int dwords);\n\tint (*read_config32)(struct iwl_trans *trans, u32 ofs, u32 *val);\n\tvoid (*configure)(struct iwl_trans *trans,\n\t\t\t  const struct iwl_trans_config *trans_cfg);\n\tvoid (*set_pmi)(struct iwl_trans *trans, bool state);\n\tint (*sw_reset)(struct iwl_trans *trans, bool retake_ownership);\n\tbool (*grab_nic_access)(struct iwl_trans *trans);\n\tvoid (*release_nic_access)(struct iwl_trans *trans);\n\tvoid (*set_bits_mask)(struct iwl_trans *trans, u32 reg, u32 mask,\n\t\t\t      u32 value);\n\n\tstruct iwl_trans_dump_data *(*dump_data)(struct iwl_trans *trans,\n\t\t\t\t\t\t u32 dump_mask,\n\t\t\t\t\t\t const struct iwl_dump_sanitize_ops *sanitize_ops,\n\t\t\t\t\t\t void *sanitize_ctx);\n\tvoid (*debugfs_cleanup)(struct iwl_trans *trans);\n\tvoid (*sync_nmi)(struct iwl_trans *trans);\n\tint (*load_pnvm)(struct iwl_trans *trans,\n\t\t\t const struct iwl_pnvm_image *pnvm_payloads,\n\t\t\t const struct iwl_ucode_capabilities *capa);\n\tvoid (*set_pnvm)(struct iwl_trans *trans,\n\t\t\t const struct iwl_ucode_capabilities *capa);\n\tint (*load_reduce_power)(struct iwl_trans *trans,\n\t\t\t\t const struct iwl_pnvm_image *payloads,\n\t\t\t\t const struct iwl_ucode_capabilities *capa);\n\tvoid (*set_reduce_power)(struct iwl_trans *trans,\n\t\t\t\t const struct iwl_ucode_capabilities *capa);\n\n\tvoid (*interrupts)(struct iwl_trans *trans, bool enable);\n\tint (*imr_dma_data)(struct iwl_trans *trans,\n\t\t\t    u32 dst_addr, u64 src_addr,\n\t\t\t    u32 byte_cnt);\n\n};\n\n \nenum iwl_trans_state {\n\tIWL_TRANS_NO_FW,\n\tIWL_TRANS_FW_STARTED,\n\tIWL_TRANS_FW_ALIVE,\n};\n\n \n\n \nenum iwl_plat_pm_mode {\n\tIWL_PLAT_PM_MODE_DISABLED,\n\tIWL_PLAT_PM_MODE_D3,\n};\n\n \nenum iwl_ini_cfg_state {\n\tIWL_INI_CFG_STATE_NOT_LOADED,\n\tIWL_INI_CFG_STATE_LOADED,\n\tIWL_INI_CFG_STATE_CORRUPTED,\n};\n\n \n#define IWL_TRANS_NMI_TIMEOUT (HZ / 4)\n\n \nstruct iwl_dram_data {\n\tdma_addr_t physical;\n\tvoid *block;\n\tint size;\n};\n\n \nstruct iwl_dram_regions {\n\tstruct iwl_dram_data drams[IPC_DRAM_MAP_ENTRY_NUM_MAX];\n\tstruct iwl_dram_data prph_scratch_mem_desc;\n\tu8 n_regions;\n};\n\n \nstruct iwl_fw_mon {\n\tu32 num_frags;\n\tstruct iwl_dram_data *frags;\n};\n\n \nstruct iwl_self_init_dram {\n\tstruct iwl_dram_data *fw;\n\tint fw_cnt;\n\tstruct iwl_dram_data *paging;\n\tint paging_cnt;\n};\n\n \nstruct iwl_imr_data {\n\tu32 imr_enable;\n\tu32 imr_size;\n\tu32 sram_addr;\n\tu32 sram_size;\n\tu32 imr2sram_remainbyte;\n\tu64 imr_curr_addr;\n\t__le64 imr_base_addr;\n};\n\n#define IWL_TRANS_CURRENT_PC_NAME_MAX_BYTES      32\n\n \nstruct iwl_pc_data {\n\tu8  pc_name[IWL_TRANS_CURRENT_PC_NAME_MAX_BYTES];\n\tu32 pc_address;\n};\n\n \nstruct iwl_trans_debug {\n\tu8 n_dest_reg;\n\tbool rec_on;\n\n\tconst struct iwl_fw_dbg_dest_tlv_v1 *dest_tlv;\n\tconst struct iwl_fw_dbg_conf_tlv *conf_tlv[FW_DBG_CONF_MAX];\n\tstruct iwl_fw_dbg_trigger_tlv * const *trigger_tlv;\n\n\tu32 lmac_error_event_table[2];\n\tu32 umac_error_event_table;\n\tu32 tcm_error_event_table[2];\n\tu32 rcm_error_event_table[2];\n\tunsigned int error_event_table_tlv_status;\n\n\tenum iwl_ini_cfg_state internal_ini_cfg;\n\tenum iwl_ini_cfg_state external_ini_cfg;\n\n\tstruct iwl_fw_ini_allocation_tlv fw_mon_cfg[IWL_FW_INI_ALLOCATION_NUM];\n\tstruct iwl_fw_mon fw_mon_ini[IWL_FW_INI_ALLOCATION_NUM];\n\n\tstruct iwl_dram_data fw_mon;\n\n\tbool hw_error;\n\tenum iwl_fw_ini_buffer_location ini_dest;\n\n\tu64 unsupported_region_msk;\n\tstruct iwl_ucode_tlv *active_regions[IWL_FW_INI_MAX_REGION_ID];\n\tstruct list_head debug_info_tlv_list;\n\tstruct iwl_dbg_tlv_time_point_data\n\t\ttime_point[IWL_FW_INI_TIME_POINT_NUM];\n\tstruct list_head periodic_trig_list;\n\n\tu32 domains_bitmap;\n\tu32 ucode_preset;\n\tbool restart_required;\n\tu32 last_tp_resetfw;\n\tstruct iwl_imr_data imr_data;\n\tu8 dump_file_name_ext[IWL_FW_INI_MAX_NAME];\n\tbool dump_file_name_ext_valid;\n\tu32 num_pc;\n\tstruct iwl_pc_data *pc_data;\n};\n\nstruct iwl_dma_ptr {\n\tdma_addr_t dma;\n\tvoid *addr;\n\tsize_t size;\n};\n\nstruct iwl_cmd_meta {\n\t \n\tstruct iwl_host_cmd *source;\n\tu32 flags;\n\tu32 tbs;\n};\n\n \n#define IWL_FIRST_TB_SIZE\t20\n#define IWL_FIRST_TB_SIZE_ALIGN ALIGN(IWL_FIRST_TB_SIZE, 64)\n\nstruct iwl_pcie_txq_entry {\n\tvoid *cmd;\n\tstruct sk_buff *skb;\n\t \n\tconst void *free_buf;\n\tstruct iwl_cmd_meta meta;\n};\n\nstruct iwl_pcie_first_tb_buf {\n\tu8 buf[IWL_FIRST_TB_SIZE_ALIGN];\n};\n\n \nstruct iwl_txq {\n\tvoid *tfds;\n\tstruct iwl_pcie_first_tb_buf *first_tb_bufs;\n\tdma_addr_t first_tb_dma;\n\tstruct iwl_pcie_txq_entry *entries;\n\t \n\tspinlock_t lock;\n\tunsigned long frozen_expiry_remainder;\n\tstruct timer_list stuck_timer;\n\tstruct iwl_trans *trans;\n\tbool need_update;\n\tbool frozen;\n\tbool ampdu;\n\tint block;\n\tunsigned long wd_timeout;\n\tstruct sk_buff_head overflow_q;\n\tstruct iwl_dma_ptr bc_tbl;\n\n\tint write_ptr;\n\tint read_ptr;\n\tdma_addr_t dma_addr;\n\tint n_window;\n\tu32 id;\n\tint low_mark;\n\tint high_mark;\n\n\tbool overflow_tx;\n};\n\n \nstruct iwl_trans_txqs {\n\tunsigned long queue_used[BITS_TO_LONGS(IWL_MAX_TVQM_QUEUES)];\n\tunsigned long queue_stopped[BITS_TO_LONGS(IWL_MAX_TVQM_QUEUES)];\n\tstruct iwl_txq *txq[IWL_MAX_TVQM_QUEUES];\n\tstruct dma_pool *bc_pool;\n\tsize_t bc_tbl_size;\n\tbool bc_table_dword;\n\tu8 page_offs;\n\tu8 dev_cmd_offs;\n\tstruct iwl_tso_hdr_page __percpu *tso_hdr_page;\n\n\tstruct {\n\t\tu8 fifo;\n\t\tu8 q_id;\n\t\tunsigned int wdg_timeout;\n\t} cmd;\n\n\tstruct {\n\t\tu8 max_tbs;\n\t\tu16 size;\n\t\tu8 addr_size;\n\t} tfd;\n\n\tstruct iwl_dma_ptr scd_bc_tbls;\n\n\tu8 queue_alloc_cmd_ver;\n};\n\n \nstruct iwl_trans {\n\tbool csme_own;\n\tconst struct iwl_trans_ops *ops;\n\tstruct iwl_op_mode *op_mode;\n\tconst struct iwl_cfg_trans_params *trans_cfg;\n\tconst struct iwl_cfg *cfg;\n\tstruct iwl_drv *drv;\n\tenum iwl_trans_state state;\n\tunsigned long status;\n\n\tstruct device *dev;\n\tu32 max_skb_frags;\n\tu32 hw_rev;\n\tu32 hw_rev_step;\n\tu32 hw_rf_id;\n\tu32 hw_crf_id;\n\tu32 hw_cnv_id;\n\tu32 hw_wfpm_id;\n\tu32 hw_id;\n\tchar hw_id_str[52];\n\tu32 sku_id[3];\n\n\tu8 rx_mpdu_cmd, rx_mpdu_cmd_hdr_size;\n\n\tbool pm_support;\n\tbool ltr_enabled;\n\tu8 pnvm_loaded:1;\n\tu8 fail_to_parse_pnvm_image:1;\n\tu8 reduce_power_loaded:1;\n\tu8 failed_to_load_reduce_power_image:1;\n\n\tconst struct iwl_hcmd_arr *command_groups;\n\tint command_groups_size;\n\tbool wide_cmd_header;\n\n\twait_queue_head_t wait_command_queue;\n\tu8 num_rx_queues;\n\n\tsize_t iml_len;\n\tu8 *iml;\n\n\t \n\tstruct kmem_cache *dev_cmd_pool;\n\tchar dev_cmd_pool_name[50];\n\n\tstruct dentry *dbgfs_dir;\n\n#ifdef CONFIG_LOCKDEP\n\tstruct lockdep_map sync_cmd_lockdep_map;\n#endif\n\n\tstruct iwl_trans_debug dbg;\n\tstruct iwl_self_init_dram init_dram;\n\n\tenum iwl_plat_pm_mode system_pm_mode;\n\n\tconst char *name;\n\tstruct iwl_trans_txqs txqs;\n\tu32 mbx_addr_0_step;\n\tu32 mbx_addr_1_step;\n\n\tu8 pcie_link_speed;\n\n\tstruct iwl_dma_ptr invalid_tx_cmd;\n\n\t \n\t \n\tchar trans_specific[] __aligned(sizeof(void *));\n};\n\nconst char *iwl_get_cmd_string(struct iwl_trans *trans, u32 id);\nint iwl_cmd_groups_verify_sorted(const struct iwl_trans_config *trans);\n\nstatic inline void iwl_trans_configure(struct iwl_trans *trans,\n\t\t\t\t       const struct iwl_trans_config *trans_cfg)\n{\n\ttrans->op_mode = trans_cfg->op_mode;\n\n\ttrans->ops->configure(trans, trans_cfg);\n\tWARN_ON(iwl_cmd_groups_verify_sorted(trans_cfg));\n}\n\nstatic inline int iwl_trans_start_hw(struct iwl_trans *trans)\n{\n\tmight_sleep();\n\n\treturn trans->ops->start_hw(trans);\n}\n\nstatic inline void iwl_trans_op_mode_leave(struct iwl_trans *trans)\n{\n\tmight_sleep();\n\n\tif (trans->ops->op_mode_leave)\n\t\ttrans->ops->op_mode_leave(trans);\n\n\ttrans->op_mode = NULL;\n\n\ttrans->state = IWL_TRANS_NO_FW;\n}\n\nstatic inline void iwl_trans_fw_alive(struct iwl_trans *trans, u32 scd_addr)\n{\n\tmight_sleep();\n\n\ttrans->state = IWL_TRANS_FW_ALIVE;\n\n\ttrans->ops->fw_alive(trans, scd_addr);\n}\n\nstatic inline int iwl_trans_start_fw(struct iwl_trans *trans,\n\t\t\t\t     const struct fw_img *fw,\n\t\t\t\t     bool run_in_rfkill)\n{\n\tint ret;\n\n\tmight_sleep();\n\n\tWARN_ON_ONCE(!trans->rx_mpdu_cmd);\n\n\tclear_bit(STATUS_FW_ERROR, &trans->status);\n\tret = trans->ops->start_fw(trans, fw, run_in_rfkill);\n\tif (ret == 0)\n\t\ttrans->state = IWL_TRANS_FW_STARTED;\n\n\treturn ret;\n}\n\nstatic inline void iwl_trans_stop_device(struct iwl_trans *trans)\n{\n\tmight_sleep();\n\n\ttrans->ops->stop_device(trans);\n\n\ttrans->state = IWL_TRANS_NO_FW;\n}\n\nstatic inline int iwl_trans_d3_suspend(struct iwl_trans *trans, bool test,\n\t\t\t\t       bool reset)\n{\n\tmight_sleep();\n\tif (!trans->ops->d3_suspend)\n\t\treturn -EOPNOTSUPP;\n\n\treturn trans->ops->d3_suspend(trans, test, reset);\n}\n\nstatic inline int iwl_trans_d3_resume(struct iwl_trans *trans,\n\t\t\t\t      enum iwl_d3_status *status,\n\t\t\t\t      bool test, bool reset)\n{\n\tmight_sleep();\n\tif (!trans->ops->d3_resume)\n\t\treturn -EOPNOTSUPP;\n\n\treturn trans->ops->d3_resume(trans, status, test, reset);\n}\n\nstatic inline struct iwl_trans_dump_data *\niwl_trans_dump_data(struct iwl_trans *trans, u32 dump_mask,\n\t\t    const struct iwl_dump_sanitize_ops *sanitize_ops,\n\t\t    void *sanitize_ctx)\n{\n\tif (!trans->ops->dump_data)\n\t\treturn NULL;\n\treturn trans->ops->dump_data(trans, dump_mask,\n\t\t\t\t     sanitize_ops, sanitize_ctx);\n}\n\nstatic inline struct iwl_device_tx_cmd *\niwl_trans_alloc_tx_cmd(struct iwl_trans *trans)\n{\n\treturn kmem_cache_zalloc(trans->dev_cmd_pool, GFP_ATOMIC);\n}\n\nint iwl_trans_send_cmd(struct iwl_trans *trans, struct iwl_host_cmd *cmd);\n\nstatic inline void iwl_trans_free_tx_cmd(struct iwl_trans *trans,\n\t\t\t\t\t struct iwl_device_tx_cmd *dev_cmd)\n{\n\tkmem_cache_free(trans->dev_cmd_pool, dev_cmd);\n}\n\nstatic inline int iwl_trans_tx(struct iwl_trans *trans, struct sk_buff *skb,\n\t\t\t       struct iwl_device_tx_cmd *dev_cmd, int queue)\n{\n\tif (unlikely(test_bit(STATUS_FW_ERROR, &trans->status)))\n\t\treturn -EIO;\n\n\tif (WARN_ON_ONCE(trans->state != IWL_TRANS_FW_ALIVE)) {\n\t\tIWL_ERR(trans, \"%s bad state = %d\\n\", __func__, trans->state);\n\t\treturn -EIO;\n\t}\n\n\treturn trans->ops->tx(trans, skb, dev_cmd, queue);\n}\n\nstatic inline void iwl_trans_reclaim(struct iwl_trans *trans, int queue,\n\t\t\t\t     int ssn, struct sk_buff_head *skbs,\n\t\t\t\t     bool is_flush)\n{\n\tif (WARN_ON_ONCE(trans->state != IWL_TRANS_FW_ALIVE)) {\n\t\tIWL_ERR(trans, \"%s bad state = %d\\n\", __func__, trans->state);\n\t\treturn;\n\t}\n\n\ttrans->ops->reclaim(trans, queue, ssn, skbs, is_flush);\n}\n\nstatic inline void iwl_trans_set_q_ptrs(struct iwl_trans *trans, int queue,\n\t\t\t\t\tint ptr)\n{\n\tif (WARN_ON_ONCE(trans->state != IWL_TRANS_FW_ALIVE)) {\n\t\tIWL_ERR(trans, \"%s bad state = %d\\n\", __func__, trans->state);\n\t\treturn;\n\t}\n\n\ttrans->ops->set_q_ptrs(trans, queue, ptr);\n}\n\nstatic inline void iwl_trans_txq_disable(struct iwl_trans *trans, int queue,\n\t\t\t\t\t bool configure_scd)\n{\n\ttrans->ops->txq_disable(trans, queue, configure_scd);\n}\n\nstatic inline bool\niwl_trans_txq_enable_cfg(struct iwl_trans *trans, int queue, u16 ssn,\n\t\t\t const struct iwl_trans_txq_scd_cfg *cfg,\n\t\t\t unsigned int queue_wdg_timeout)\n{\n\tmight_sleep();\n\n\tif (WARN_ON_ONCE(trans->state != IWL_TRANS_FW_ALIVE)) {\n\t\tIWL_ERR(trans, \"%s bad state = %d\\n\", __func__, trans->state);\n\t\treturn false;\n\t}\n\n\treturn trans->ops->txq_enable(trans, queue, ssn,\n\t\t\t\t      cfg, queue_wdg_timeout);\n}\n\nstatic inline int\niwl_trans_get_rxq_dma_data(struct iwl_trans *trans, int queue,\n\t\t\t   struct iwl_trans_rxq_dma_data *data)\n{\n\tif (WARN_ON_ONCE(!trans->ops->rxq_dma_data))\n\t\treturn -ENOTSUPP;\n\n\treturn trans->ops->rxq_dma_data(trans, queue, data);\n}\n\nstatic inline void\niwl_trans_txq_free(struct iwl_trans *trans, int queue)\n{\n\tif (WARN_ON_ONCE(!trans->ops->txq_free))\n\t\treturn;\n\n\ttrans->ops->txq_free(trans, queue);\n}\n\nstatic inline int\niwl_trans_txq_alloc(struct iwl_trans *trans,\n\t\t    u32 flags, u32 sta_mask, u8 tid,\n\t\t    int size, unsigned int wdg_timeout)\n{\n\tmight_sleep();\n\n\tif (WARN_ON_ONCE(!trans->ops->txq_alloc))\n\t\treturn -ENOTSUPP;\n\n\tif (WARN_ON_ONCE(trans->state != IWL_TRANS_FW_ALIVE)) {\n\t\tIWL_ERR(trans, \"%s bad state = %d\\n\", __func__, trans->state);\n\t\treturn -EIO;\n\t}\n\n\treturn trans->ops->txq_alloc(trans, flags, sta_mask, tid,\n\t\t\t\t     size, wdg_timeout);\n}\n\nstatic inline void iwl_trans_txq_set_shared_mode(struct iwl_trans *trans,\n\t\t\t\t\t\t int queue, bool shared_mode)\n{\n\tif (trans->ops->txq_set_shared_mode)\n\t\ttrans->ops->txq_set_shared_mode(trans, queue, shared_mode);\n}\n\nstatic inline void iwl_trans_txq_enable(struct iwl_trans *trans, int queue,\n\t\t\t\t\tint fifo, int sta_id, int tid,\n\t\t\t\t\tint frame_limit, u16 ssn,\n\t\t\t\t\tunsigned int queue_wdg_timeout)\n{\n\tstruct iwl_trans_txq_scd_cfg cfg = {\n\t\t.fifo = fifo,\n\t\t.sta_id = sta_id,\n\t\t.tid = tid,\n\t\t.frame_limit = frame_limit,\n\t\t.aggregate = sta_id >= 0,\n\t};\n\n\tiwl_trans_txq_enable_cfg(trans, queue, ssn, &cfg, queue_wdg_timeout);\n}\n\nstatic inline\nvoid iwl_trans_ac_txq_enable(struct iwl_trans *trans, int queue, int fifo,\n\t\t\t     unsigned int queue_wdg_timeout)\n{\n\tstruct iwl_trans_txq_scd_cfg cfg = {\n\t\t.fifo = fifo,\n\t\t.sta_id = -1,\n\t\t.tid = IWL_MAX_TID_COUNT,\n\t\t.frame_limit = IWL_FRAME_LIMIT,\n\t\t.aggregate = false,\n\t};\n\n\tiwl_trans_txq_enable_cfg(trans, queue, 0, &cfg, queue_wdg_timeout);\n}\n\nstatic inline void iwl_trans_freeze_txq_timer(struct iwl_trans *trans,\n\t\t\t\t\t      unsigned long txqs,\n\t\t\t\t\t      bool freeze)\n{\n\tif (WARN_ON_ONCE(trans->state != IWL_TRANS_FW_ALIVE)) {\n\t\tIWL_ERR(trans, \"%s bad state = %d\\n\", __func__, trans->state);\n\t\treturn;\n\t}\n\n\tif (trans->ops->freeze_txq_timer)\n\t\ttrans->ops->freeze_txq_timer(trans, txqs, freeze);\n}\n\nstatic inline void iwl_trans_block_txq_ptrs(struct iwl_trans *trans,\n\t\t\t\t\t    bool block)\n{\n\tif (WARN_ON_ONCE(trans->state != IWL_TRANS_FW_ALIVE)) {\n\t\tIWL_ERR(trans, \"%s bad state = %d\\n\", __func__, trans->state);\n\t\treturn;\n\t}\n\n\tif (trans->ops->block_txq_ptrs)\n\t\ttrans->ops->block_txq_ptrs(trans, block);\n}\n\nstatic inline int iwl_trans_wait_tx_queues_empty(struct iwl_trans *trans,\n\t\t\t\t\t\t u32 txqs)\n{\n\tif (WARN_ON_ONCE(!trans->ops->wait_tx_queues_empty))\n\t\treturn -ENOTSUPP;\n\n\t \n\tif (trans->state != IWL_TRANS_FW_ALIVE) {\n\t\tIWL_ERR(trans, \"%s bad state = %d\\n\", __func__, trans->state);\n\t\treturn -EIO;\n\t}\n\n\treturn trans->ops->wait_tx_queues_empty(trans, txqs);\n}\n\nstatic inline int iwl_trans_wait_txq_empty(struct iwl_trans *trans, int queue)\n{\n\tif (WARN_ON_ONCE(!trans->ops->wait_txq_empty))\n\t\treturn -ENOTSUPP;\n\n\tif (WARN_ON_ONCE(trans->state != IWL_TRANS_FW_ALIVE)) {\n\t\tIWL_ERR(trans, \"%s bad state = %d\\n\", __func__, trans->state);\n\t\treturn -EIO;\n\t}\n\n\treturn trans->ops->wait_txq_empty(trans, queue);\n}\n\nstatic inline void iwl_trans_write8(struct iwl_trans *trans, u32 ofs, u8 val)\n{\n\ttrans->ops->write8(trans, ofs, val);\n}\n\nstatic inline void iwl_trans_write32(struct iwl_trans *trans, u32 ofs, u32 val)\n{\n\ttrans->ops->write32(trans, ofs, val);\n}\n\nstatic inline u32 iwl_trans_read32(struct iwl_trans *trans, u32 ofs)\n{\n\treturn trans->ops->read32(trans, ofs);\n}\n\nstatic inline u32 iwl_trans_read_prph(struct iwl_trans *trans, u32 ofs)\n{\n\treturn trans->ops->read_prph(trans, ofs);\n}\n\nstatic inline void iwl_trans_write_prph(struct iwl_trans *trans, u32 ofs,\n\t\t\t\t\tu32 val)\n{\n\treturn trans->ops->write_prph(trans, ofs, val);\n}\n\nstatic inline int iwl_trans_read_mem(struct iwl_trans *trans, u32 addr,\n\t\t\t\t     void *buf, int dwords)\n{\n\treturn trans->ops->read_mem(trans, addr, buf, dwords);\n}\n\n#define iwl_trans_read_mem_bytes(trans, addr, buf, bufsize)\t\t      \\\n\tdo {\t\t\t\t\t\t\t\t      \\\n\t\tif (__builtin_constant_p(bufsize))\t\t\t      \\\n\t\t\tBUILD_BUG_ON((bufsize) % sizeof(u32));\t\t      \\\n\t\tiwl_trans_read_mem(trans, addr, buf, (bufsize) / sizeof(u32));\\\n\t} while (0)\n\nstatic inline int iwl_trans_write_imr_mem(struct iwl_trans *trans,\n\t\t\t\t\t  u32 dst_addr, u64 src_addr,\n\t\t\t\t\t  u32 byte_cnt)\n{\n\tif (trans->ops->imr_dma_data)\n\t\treturn trans->ops->imr_dma_data(trans, dst_addr, src_addr, byte_cnt);\n\treturn 0;\n}\n\nstatic inline u32 iwl_trans_read_mem32(struct iwl_trans *trans, u32 addr)\n{\n\tu32 value;\n\n\tif (iwl_trans_read_mem(trans, addr, &value, 1))\n\t\treturn 0xa5a5a5a5;\n\n\treturn value;\n}\n\nstatic inline int iwl_trans_write_mem(struct iwl_trans *trans, u32 addr,\n\t\t\t\t      const void *buf, int dwords)\n{\n\treturn trans->ops->write_mem(trans, addr, buf, dwords);\n}\n\nstatic inline u32 iwl_trans_write_mem32(struct iwl_trans *trans, u32 addr,\n\t\t\t\t\tu32 val)\n{\n\treturn iwl_trans_write_mem(trans, addr, &val, 1);\n}\n\nstatic inline void iwl_trans_set_pmi(struct iwl_trans *trans, bool state)\n{\n\tif (trans->ops->set_pmi)\n\t\ttrans->ops->set_pmi(trans, state);\n}\n\nstatic inline int iwl_trans_sw_reset(struct iwl_trans *trans,\n\t\t\t\t     bool retake_ownership)\n{\n\tif (trans->ops->sw_reset)\n\t\treturn trans->ops->sw_reset(trans, retake_ownership);\n\treturn 0;\n}\n\nstatic inline void\niwl_trans_set_bits_mask(struct iwl_trans *trans, u32 reg, u32 mask, u32 value)\n{\n\ttrans->ops->set_bits_mask(trans, reg, mask, value);\n}\n\n#define iwl_trans_grab_nic_access(trans)\t\t\\\n\t__cond_lock(nic_access,\t\t\t\t\\\n\t\t    likely((trans)->ops->grab_nic_access(trans)))\n\nstatic inline void __releases(nic_access)\niwl_trans_release_nic_access(struct iwl_trans *trans)\n{\n\ttrans->ops->release_nic_access(trans);\n\t__release(nic_access);\n}\n\nstatic inline void iwl_trans_fw_error(struct iwl_trans *trans, bool sync)\n{\n\tif (WARN_ON_ONCE(!trans->op_mode))\n\t\treturn;\n\n\t \n\tif (!test_and_set_bit(STATUS_FW_ERROR, &trans->status)) {\n\t\tiwl_op_mode_nic_error(trans->op_mode, sync);\n\t\ttrans->state = IWL_TRANS_NO_FW;\n\t}\n}\n\nstatic inline bool iwl_trans_fw_running(struct iwl_trans *trans)\n{\n\treturn trans->state == IWL_TRANS_FW_ALIVE;\n}\n\nstatic inline void iwl_trans_sync_nmi(struct iwl_trans *trans)\n{\n\tif (trans->ops->sync_nmi)\n\t\ttrans->ops->sync_nmi(trans);\n}\n\nvoid iwl_trans_sync_nmi_with_addr(struct iwl_trans *trans, u32 inta_addr,\n\t\t\t\t  u32 sw_err_bit);\n\nstatic inline int iwl_trans_load_pnvm(struct iwl_trans *trans,\n\t\t\t\t      const struct iwl_pnvm_image *pnvm_data,\n\t\t\t\t      const struct iwl_ucode_capabilities *capa)\n{\n\treturn trans->ops->load_pnvm(trans, pnvm_data, capa);\n}\n\nstatic inline void iwl_trans_set_pnvm(struct iwl_trans *trans,\n\t\t\t\t      const struct iwl_ucode_capabilities *capa)\n{\n\tif (trans->ops->set_pnvm)\n\t\ttrans->ops->set_pnvm(trans, capa);\n}\n\nstatic inline int iwl_trans_load_reduce_power\n\t\t\t\t(struct iwl_trans *trans,\n\t\t\t\t const struct iwl_pnvm_image *payloads,\n\t\t\t\t const struct iwl_ucode_capabilities *capa)\n{\n\treturn trans->ops->load_reduce_power(trans, payloads, capa);\n}\n\nstatic inline void\niwl_trans_set_reduce_power(struct iwl_trans *trans,\n\t\t\t   const struct iwl_ucode_capabilities *capa)\n{\n\tif (trans->ops->set_reduce_power)\n\t\ttrans->ops->set_reduce_power(trans, capa);\n}\n\nstatic inline bool iwl_trans_dbg_ini_valid(struct iwl_trans *trans)\n{\n\treturn trans->dbg.internal_ini_cfg != IWL_INI_CFG_STATE_NOT_LOADED ||\n\t\ttrans->dbg.external_ini_cfg != IWL_INI_CFG_STATE_NOT_LOADED;\n}\n\nstatic inline void iwl_trans_interrupts(struct iwl_trans *trans, bool enable)\n{\n\tif (trans->ops->interrupts)\n\t\ttrans->ops->interrupts(trans, enable);\n}\n\n \nstruct iwl_trans *iwl_trans_alloc(unsigned int priv_size,\n\t\t\t  struct device *dev,\n\t\t\t  const struct iwl_trans_ops *ops,\n\t\t\t  const struct iwl_cfg_trans_params *cfg_trans);\nint iwl_trans_init(struct iwl_trans *trans);\nvoid iwl_trans_free(struct iwl_trans *trans);\n\nstatic inline bool iwl_trans_is_hw_error_value(u32 val)\n{\n\treturn ((val & ~0xf) == 0xa5a5a5a0) || ((val & ~0xf) == 0x5a5a5a50);\n}\n\n \nint __must_check iwl_pci_register_driver(void);\nvoid iwl_pci_unregister_driver(void);\nvoid iwl_trans_pcie_remove(struct iwl_trans *trans, bool rescan);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}