{
  "module_name": "dma.c",
  "hash_id": "2dce614fa7fedddd3b7144a959992e597b93f6b423327eed2be40e74e839f298",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wireless/broadcom/b43legacy/dma.c",
  "human_readable_source": "\n \n\n#include \"b43legacy.h\"\n#include \"dma.h\"\n#include \"main.h\"\n#include \"debugfs.h\"\n#include \"xmit.h\"\n\n#include <linux/dma-mapping.h>\n#include <linux/pci.h>\n#include <linux/delay.h>\n#include <linux/skbuff.h>\n#include <linux/slab.h>\n#include <net/dst.h>\n\n \nstatic\nstruct b43legacy_dmadesc32 *op32_idx2desc(struct b43legacy_dmaring *ring,\n\t\t\t\t\t  int slot,\n\t\t\t\t\t  struct b43legacy_dmadesc_meta **meta)\n{\n\tstruct b43legacy_dmadesc32 *desc;\n\n\t*meta = &(ring->meta[slot]);\n\tdesc = ring->descbase;\n\tdesc = &(desc[slot]);\n\n\treturn desc;\n}\n\nstatic void op32_fill_descriptor(struct b43legacy_dmaring *ring,\n\t\t\t\t struct b43legacy_dmadesc32 *desc,\n\t\t\t\t dma_addr_t dmaaddr, u16 bufsize,\n\t\t\t\t int start, int end, int irq)\n{\n\tstruct b43legacy_dmadesc32 *descbase = ring->descbase;\n\tint slot;\n\tu32 ctl;\n\tu32 addr;\n\tu32 addrext;\n\n\tslot = (int)(desc - descbase);\n\tB43legacy_WARN_ON(!(slot >= 0 && slot < ring->nr_slots));\n\n\taddr = (u32)(dmaaddr & ~SSB_DMA_TRANSLATION_MASK);\n\taddrext = (u32)(dmaaddr & SSB_DMA_TRANSLATION_MASK)\n\t\t   >> SSB_DMA_TRANSLATION_SHIFT;\n\taddr |= ring->dev->dma.translation;\n\tctl = (bufsize - ring->frameoffset)\n\t      & B43legacy_DMA32_DCTL_BYTECNT;\n\tif (slot == ring->nr_slots - 1)\n\t\tctl |= B43legacy_DMA32_DCTL_DTABLEEND;\n\tif (start)\n\t\tctl |= B43legacy_DMA32_DCTL_FRAMESTART;\n\tif (end)\n\t\tctl |= B43legacy_DMA32_DCTL_FRAMEEND;\n\tif (irq)\n\t\tctl |= B43legacy_DMA32_DCTL_IRQ;\n\tctl |= (addrext << B43legacy_DMA32_DCTL_ADDREXT_SHIFT)\n\t       & B43legacy_DMA32_DCTL_ADDREXT_MASK;\n\n\tdesc->control = cpu_to_le32(ctl);\n\tdesc->address = cpu_to_le32(addr);\n}\n\nstatic void op32_poke_tx(struct b43legacy_dmaring *ring, int slot)\n{\n\tb43legacy_dma_write(ring, B43legacy_DMA32_TXINDEX,\n\t\t\t    (u32)(slot * sizeof(struct b43legacy_dmadesc32)));\n}\n\nstatic void op32_tx_suspend(struct b43legacy_dmaring *ring)\n{\n\tb43legacy_dma_write(ring, B43legacy_DMA32_TXCTL,\n\t\t\t    b43legacy_dma_read(ring, B43legacy_DMA32_TXCTL)\n\t\t\t    | B43legacy_DMA32_TXSUSPEND);\n}\n\nstatic void op32_tx_resume(struct b43legacy_dmaring *ring)\n{\n\tb43legacy_dma_write(ring, B43legacy_DMA32_TXCTL,\n\t\t\t    b43legacy_dma_read(ring, B43legacy_DMA32_TXCTL)\n\t\t\t    & ~B43legacy_DMA32_TXSUSPEND);\n}\n\nstatic int op32_get_current_rxslot(struct b43legacy_dmaring *ring)\n{\n\tu32 val;\n\n\tval = b43legacy_dma_read(ring, B43legacy_DMA32_RXSTATUS);\n\tval &= B43legacy_DMA32_RXDPTR;\n\n\treturn (val / sizeof(struct b43legacy_dmadesc32));\n}\n\nstatic void op32_set_current_rxslot(struct b43legacy_dmaring *ring,\n\t\t\t\t    int slot)\n{\n\tb43legacy_dma_write(ring, B43legacy_DMA32_RXINDEX,\n\t\t\t    (u32)(slot * sizeof(struct b43legacy_dmadesc32)));\n}\n\nstatic inline int free_slots(struct b43legacy_dmaring *ring)\n{\n\treturn (ring->nr_slots - ring->used_slots);\n}\n\nstatic inline int next_slot(struct b43legacy_dmaring *ring, int slot)\n{\n\tB43legacy_WARN_ON(!(slot >= -1 && slot <= ring->nr_slots - 1));\n\tif (slot == ring->nr_slots - 1)\n\t\treturn 0;\n\treturn slot + 1;\n}\n\n#ifdef CONFIG_B43LEGACY_DEBUG\nstatic void update_max_used_slots(struct b43legacy_dmaring *ring,\n\t\t\t\t  int current_used_slots)\n{\n\tif (current_used_slots <= ring->max_used_slots)\n\t\treturn;\n\tring->max_used_slots = current_used_slots;\n\tif (b43legacy_debug(ring->dev, B43legacy_DBG_DMAVERBOSE))\n\t\tb43legacydbg(ring->dev->wl,\n\t\t       \"max_used_slots increased to %d on %s ring %d\\n\",\n\t\t       ring->max_used_slots,\n\t\t       ring->tx ? \"TX\" : \"RX\",\n\t\t       ring->index);\n}\n#else\nstatic inline\nvoid update_max_used_slots(struct b43legacy_dmaring *ring,\n\t\t\t   int current_used_slots)\n{ }\n#endif  \n\n \nstatic inline\nint request_slot(struct b43legacy_dmaring *ring)\n{\n\tint slot;\n\n\tB43legacy_WARN_ON(!ring->tx);\n\tB43legacy_WARN_ON(ring->stopped);\n\tB43legacy_WARN_ON(free_slots(ring) == 0);\n\n\tslot = next_slot(ring, ring->current_slot);\n\tring->current_slot = slot;\n\tring->used_slots++;\n\n\tupdate_max_used_slots(ring, ring->used_slots);\n\n\treturn slot;\n}\n\n \nstatic struct b43legacy_dmaring *priority_to_txring(\n\t\t\t\t\t\tstruct b43legacy_wldev *dev,\n\t\t\t\t\t\tint queue_priority)\n{\n\tstruct b43legacy_dmaring *ring;\n\n \nreturn dev->dma.tx_ring1;\n\n\t \n\tswitch (queue_priority) {\n\tdefault:\n\t\tB43legacy_WARN_ON(1);\n\t\tfallthrough;\n\tcase 0:\n\t\tring = dev->dma.tx_ring3;\n\t\tbreak;\n\tcase 1:\n\t\tring = dev->dma.tx_ring2;\n\t\tbreak;\n\tcase 2:\n\t\tring = dev->dma.tx_ring1;\n\t\tbreak;\n\tcase 3:\n\t\tring = dev->dma.tx_ring0;\n\t\tbreak;\n\tcase 4:\n\t\tring = dev->dma.tx_ring4;\n\t\tbreak;\n\tcase 5:\n\t\tring = dev->dma.tx_ring5;\n\t\tbreak;\n\t}\n\n\treturn ring;\n}\n\nstatic u16 b43legacy_dmacontroller_base(enum b43legacy_dmatype type,\n\t\t\t\t\tint controller_idx)\n{\n\tstatic const u16 map32[] = {\n\t\tB43legacy_MMIO_DMA32_BASE0,\n\t\tB43legacy_MMIO_DMA32_BASE1,\n\t\tB43legacy_MMIO_DMA32_BASE2,\n\t\tB43legacy_MMIO_DMA32_BASE3,\n\t\tB43legacy_MMIO_DMA32_BASE4,\n\t\tB43legacy_MMIO_DMA32_BASE5,\n\t};\n\n\tB43legacy_WARN_ON(!(controller_idx >= 0 &&\n\t\t\t  controller_idx < ARRAY_SIZE(map32)));\n\treturn map32[controller_idx];\n}\n\nstatic inline\ndma_addr_t map_descbuffer(struct b43legacy_dmaring *ring,\n\t\t\t  unsigned char *buf,\n\t\t\t  size_t len,\n\t\t\t  int tx)\n{\n\tdma_addr_t dmaaddr;\n\n\tif (tx)\n\t\tdmaaddr = dma_map_single(ring->dev->dev->dma_dev,\n\t\t\t\t\t     buf, len,\n\t\t\t\t\t     DMA_TO_DEVICE);\n\telse\n\t\tdmaaddr = dma_map_single(ring->dev->dev->dma_dev,\n\t\t\t\t\t     buf, len,\n\t\t\t\t\t     DMA_FROM_DEVICE);\n\n\treturn dmaaddr;\n}\n\nstatic inline\nvoid unmap_descbuffer(struct b43legacy_dmaring *ring,\n\t\t      dma_addr_t addr,\n\t\t      size_t len,\n\t\t      int tx)\n{\n\tif (tx)\n\t\tdma_unmap_single(ring->dev->dev->dma_dev,\n\t\t\t\t     addr, len,\n\t\t\t\t     DMA_TO_DEVICE);\n\telse\n\t\tdma_unmap_single(ring->dev->dev->dma_dev,\n\t\t\t\t     addr, len,\n\t\t\t\t     DMA_FROM_DEVICE);\n}\n\nstatic inline\nvoid sync_descbuffer_for_cpu(struct b43legacy_dmaring *ring,\n\t\t\t     dma_addr_t addr,\n\t\t\t     size_t len)\n{\n\tB43legacy_WARN_ON(ring->tx);\n\n\tdma_sync_single_for_cpu(ring->dev->dev->dma_dev,\n\t\t\t\taddr, len, DMA_FROM_DEVICE);\n}\n\nstatic inline\nvoid sync_descbuffer_for_device(struct b43legacy_dmaring *ring,\n\t\t\t\tdma_addr_t addr,\n\t\t\t\tsize_t len)\n{\n\tB43legacy_WARN_ON(ring->tx);\n\n\tdma_sync_single_for_device(ring->dev->dev->dma_dev,\n\t\t\t\t   addr, len, DMA_FROM_DEVICE);\n}\n\nstatic inline\nvoid free_descriptor_buffer(struct b43legacy_dmaring *ring,\n\t\t\t    struct b43legacy_dmadesc_meta *meta,\n\t\t\t    int irq_context)\n{\n\tif (meta->skb) {\n\t\tif (irq_context)\n\t\t\tdev_kfree_skb_irq(meta->skb);\n\t\telse\n\t\t\tdev_kfree_skb(meta->skb);\n\t\tmeta->skb = NULL;\n\t}\n}\n\nstatic int alloc_ringmemory(struct b43legacy_dmaring *ring)\n{\n\t \n\tring->descbase = dma_alloc_coherent(ring->dev->dev->dma_dev,\n\t\t\t\t\t    B43legacy_DMA_RINGMEMSIZE,\n\t\t\t\t\t    &(ring->dmabase), GFP_KERNEL);\n\tif (!ring->descbase)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void free_ringmemory(struct b43legacy_dmaring *ring)\n{\n\tdma_free_coherent(ring->dev->dev->dma_dev, B43legacy_DMA_RINGMEMSIZE,\n\t\t\t  ring->descbase, ring->dmabase);\n}\n\n \nstatic int b43legacy_dmacontroller_rx_reset(struct b43legacy_wldev *dev,\n\t\t\t\t\t    u16 mmio_base,\n\t\t\t\t\t    enum b43legacy_dmatype type)\n{\n\tint i;\n\tu32 value;\n\tu16 offset;\n\n\tmight_sleep();\n\n\toffset = B43legacy_DMA32_RXCTL;\n\tb43legacy_write32(dev, mmio_base + offset, 0);\n\tfor (i = 0; i < 10; i++) {\n\t\toffset = B43legacy_DMA32_RXSTATUS;\n\t\tvalue = b43legacy_read32(dev, mmio_base + offset);\n\t\tvalue &= B43legacy_DMA32_RXSTATE;\n\t\tif (value == B43legacy_DMA32_RXSTAT_DISABLED) {\n\t\t\ti = -1;\n\t\t\tbreak;\n\t\t}\n\t\tmsleep(1);\n\t}\n\tif (i != -1) {\n\t\tb43legacyerr(dev->wl, \"DMA RX reset timed out\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int b43legacy_dmacontroller_tx_reset(struct b43legacy_wldev *dev,\n\t\t\t\t\t    u16 mmio_base,\n\t\t\t\t\t    enum b43legacy_dmatype type)\n{\n\tint i;\n\tu32 value;\n\tu16 offset;\n\n\tmight_sleep();\n\n\tfor (i = 0; i < 10; i++) {\n\t\toffset = B43legacy_DMA32_TXSTATUS;\n\t\tvalue = b43legacy_read32(dev, mmio_base + offset);\n\t\tvalue &= B43legacy_DMA32_TXSTATE;\n\t\tif (value == B43legacy_DMA32_TXSTAT_DISABLED ||\n\t\t    value == B43legacy_DMA32_TXSTAT_IDLEWAIT ||\n\t\t    value == B43legacy_DMA32_TXSTAT_STOPPED)\n\t\t\tbreak;\n\t\tmsleep(1);\n\t}\n\toffset = B43legacy_DMA32_TXCTL;\n\tb43legacy_write32(dev, mmio_base + offset, 0);\n\tfor (i = 0; i < 10; i++) {\n\t\toffset = B43legacy_DMA32_TXSTATUS;\n\t\tvalue = b43legacy_read32(dev, mmio_base + offset);\n\t\tvalue &= B43legacy_DMA32_TXSTATE;\n\t\tif (value == B43legacy_DMA32_TXSTAT_DISABLED) {\n\t\t\ti = -1;\n\t\t\tbreak;\n\t\t}\n\t\tmsleep(1);\n\t}\n\tif (i != -1) {\n\t\tb43legacyerr(dev->wl, \"DMA TX reset timed out\\n\");\n\t\treturn -ENODEV;\n\t}\n\t \n\tmsleep(1);\n\n\treturn 0;\n}\n\n \nstatic bool b43legacy_dma_mapping_error(struct b43legacy_dmaring *ring,\n\t\t\t\t\t dma_addr_t addr,\n\t\t\t\t\t size_t buffersize,\n\t\t\t\t\t bool dma_to_device)\n{\n\tif (unlikely(dma_mapping_error(ring->dev->dev->dma_dev, addr)))\n\t\treturn true;\n\n\tswitch (ring->type) {\n\tcase B43legacy_DMA_30BIT:\n\t\tif ((u64)addr + buffersize > (1ULL << 30))\n\t\t\tgoto address_error;\n\t\tbreak;\n\tcase B43legacy_DMA_32BIT:\n\t\tif ((u64)addr + buffersize > (1ULL << 32))\n\t\t\tgoto address_error;\n\t\tbreak;\n\t}\n\n\t \n\treturn false;\n\naddress_error:\n\t \n\tunmap_descbuffer(ring, addr, buffersize, dma_to_device);\n\n\treturn true;\n}\n\nstatic int setup_rx_descbuffer(struct b43legacy_dmaring *ring,\n\t\t\t       struct b43legacy_dmadesc32 *desc,\n\t\t\t       struct b43legacy_dmadesc_meta *meta,\n\t\t\t       gfp_t gfp_flags)\n{\n\tstruct b43legacy_rxhdr_fw3 *rxhdr;\n\tstruct b43legacy_hwtxstatus *txstat;\n\tdma_addr_t dmaaddr;\n\tstruct sk_buff *skb;\n\n\tB43legacy_WARN_ON(ring->tx);\n\n\tskb = __dev_alloc_skb(ring->rx_buffersize, gfp_flags);\n\tif (unlikely(!skb))\n\t\treturn -ENOMEM;\n\tdmaaddr = map_descbuffer(ring, skb->data,\n\t\t\t\t ring->rx_buffersize, 0);\n\tif (b43legacy_dma_mapping_error(ring, dmaaddr, ring->rx_buffersize, 0)) {\n\t\t \n\t\tgfp_flags |= GFP_DMA;\n\n\t\tdev_kfree_skb_any(skb);\n\n\t\tskb = __dev_alloc_skb(ring->rx_buffersize, gfp_flags);\n\t\tif (unlikely(!skb))\n\t\t\treturn -ENOMEM;\n\t\tdmaaddr = map_descbuffer(ring, skb->data,\n\t\t\t\t\t ring->rx_buffersize, 0);\n\t}\n\n\tif (b43legacy_dma_mapping_error(ring, dmaaddr, ring->rx_buffersize, 0)) {\n\t\tdev_kfree_skb_any(skb);\n\t\treturn -EIO;\n\t}\n\n\tmeta->skb = skb;\n\tmeta->dmaaddr = dmaaddr;\n\top32_fill_descriptor(ring, desc, dmaaddr, ring->rx_buffersize, 0, 0, 0);\n\n\trxhdr = (struct b43legacy_rxhdr_fw3 *)(skb->data);\n\trxhdr->frame_len = 0;\n\ttxstat = (struct b43legacy_hwtxstatus *)(skb->data);\n\ttxstat->cookie = 0;\n\n\treturn 0;\n}\n\n \nstatic int alloc_initial_descbuffers(struct b43legacy_dmaring *ring)\n{\n\tint i;\n\tint err = -ENOMEM;\n\tstruct b43legacy_dmadesc32 *desc;\n\tstruct b43legacy_dmadesc_meta *meta;\n\n\tfor (i = 0; i < ring->nr_slots; i++) {\n\t\tdesc = op32_idx2desc(ring, i, &meta);\n\n\t\terr = setup_rx_descbuffer(ring, desc, meta, GFP_KERNEL);\n\t\tif (err) {\n\t\t\tb43legacyerr(ring->dev->wl,\n\t\t\t       \"Failed to allocate initial descbuffers\\n\");\n\t\t\tgoto err_unwind;\n\t\t}\n\t}\n\tmb();  \n\tring->used_slots = ring->nr_slots;\n\terr = 0;\nout:\n\treturn err;\n\nerr_unwind:\n\tfor (i--; i >= 0; i--) {\n\t\tdesc = op32_idx2desc(ring, i, &meta);\n\n\t\tunmap_descbuffer(ring, meta->dmaaddr, ring->rx_buffersize, 0);\n\t\tdev_kfree_skb(meta->skb);\n\t}\n\tgoto out;\n}\n\n \nstatic int dmacontroller_setup(struct b43legacy_dmaring *ring)\n{\n\tint err = 0;\n\tu32 value;\n\tu32 addrext;\n\tu32 trans = ring->dev->dma.translation;\n\tu32 ringbase = (u32)(ring->dmabase);\n\n\tif (ring->tx) {\n\t\taddrext = (ringbase & SSB_DMA_TRANSLATION_MASK)\n\t\t\t  >> SSB_DMA_TRANSLATION_SHIFT;\n\t\tvalue = B43legacy_DMA32_TXENABLE;\n\t\tvalue |= (addrext << B43legacy_DMA32_TXADDREXT_SHIFT)\n\t\t\t& B43legacy_DMA32_TXADDREXT_MASK;\n\t\tb43legacy_dma_write(ring, B43legacy_DMA32_TXCTL, value);\n\t\tb43legacy_dma_write(ring, B43legacy_DMA32_TXRING,\n\t\t\t\t    (ringbase & ~SSB_DMA_TRANSLATION_MASK)\n\t\t\t\t    | trans);\n\t} else {\n\t\terr = alloc_initial_descbuffers(ring);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\taddrext = (ringbase & SSB_DMA_TRANSLATION_MASK)\n\t\t\t  >> SSB_DMA_TRANSLATION_SHIFT;\n\t\tvalue = (ring->frameoffset <<\n\t\t\t B43legacy_DMA32_RXFROFF_SHIFT);\n\t\tvalue |= B43legacy_DMA32_RXENABLE;\n\t\tvalue |= (addrext << B43legacy_DMA32_RXADDREXT_SHIFT)\n\t\t\t & B43legacy_DMA32_RXADDREXT_MASK;\n\t\tb43legacy_dma_write(ring, B43legacy_DMA32_RXCTL, value);\n\t\tb43legacy_dma_write(ring, B43legacy_DMA32_RXRING,\n\t\t\t\t    (ringbase & ~SSB_DMA_TRANSLATION_MASK)\n\t\t\t\t    | trans);\n\t\tb43legacy_dma_write(ring, B43legacy_DMA32_RXINDEX, 200);\n\t}\n\nout:\n\treturn err;\n}\n\n \nstatic void dmacontroller_cleanup(struct b43legacy_dmaring *ring)\n{\n\tif (ring->tx) {\n\t\tb43legacy_dmacontroller_tx_reset(ring->dev, ring->mmio_base,\n\t\t\t\t\t\t ring->type);\n\t\tb43legacy_dma_write(ring, B43legacy_DMA32_TXRING, 0);\n\t} else {\n\t\tb43legacy_dmacontroller_rx_reset(ring->dev, ring->mmio_base,\n\t\t\t\t\t\t ring->type);\n\t\tb43legacy_dma_write(ring, B43legacy_DMA32_RXRING, 0);\n\t}\n}\n\nstatic void free_all_descbuffers(struct b43legacy_dmaring *ring)\n{\n\tstruct b43legacy_dmadesc_meta *meta;\n\tint i;\n\n\tif (!ring->used_slots)\n\t\treturn;\n\tfor (i = 0; i < ring->nr_slots; i++) {\n\t\top32_idx2desc(ring, i, &meta);\n\n\t\tif (!meta->skb) {\n\t\t\tB43legacy_WARN_ON(!ring->tx);\n\t\t\tcontinue;\n\t\t}\n\t\tif (ring->tx)\n\t\t\tunmap_descbuffer(ring, meta->dmaaddr,\n\t\t\t\t\t meta->skb->len, 1);\n\t\telse\n\t\t\tunmap_descbuffer(ring, meta->dmaaddr,\n\t\t\t\t\t ring->rx_buffersize, 0);\n\t\tfree_descriptor_buffer(ring, meta, 0);\n\t}\n}\n\nstatic enum b43legacy_dmatype b43legacy_engine_type(struct b43legacy_wldev *dev)\n{\n\tu32 tmp;\n\tu16 mmio_base;\n\n\tmmio_base = b43legacy_dmacontroller_base(0, 0);\n\tb43legacy_write32(dev,\n\t\t\tmmio_base + B43legacy_DMA32_TXCTL,\n\t\t\tB43legacy_DMA32_TXADDREXT_MASK);\n\ttmp = b43legacy_read32(dev, mmio_base +\n\t\t\t       B43legacy_DMA32_TXCTL);\n\tif (tmp & B43legacy_DMA32_TXADDREXT_MASK)\n\t\treturn B43legacy_DMA_32BIT;\n\treturn B43legacy_DMA_30BIT;\n}\n\n \nstatic\nstruct b43legacy_dmaring *b43legacy_setup_dmaring(struct b43legacy_wldev *dev,\n\t\t\t\t\t\t  int controller_index,\n\t\t\t\t\t\t  int for_tx,\n\t\t\t\t\t\t  enum b43legacy_dmatype type)\n{\n\tstruct b43legacy_dmaring *ring;\n\tint err;\n\tint nr_slots;\n\tdma_addr_t dma_test;\n\n\tring = kzalloc(sizeof(*ring), GFP_KERNEL);\n\tif (!ring)\n\t\tgoto out;\n\tring->type = type;\n\tring->dev = dev;\n\n\tnr_slots = B43legacy_RXRING_SLOTS;\n\tif (for_tx)\n\t\tnr_slots = B43legacy_TXRING_SLOTS;\n\n\tring->meta = kcalloc(nr_slots, sizeof(struct b43legacy_dmadesc_meta),\n\t\t\t     GFP_KERNEL);\n\tif (!ring->meta)\n\t\tgoto err_kfree_ring;\n\tif (for_tx) {\n\t\tring->txhdr_cache = kcalloc(nr_slots,\n\t\t\t\t\tsizeof(struct b43legacy_txhdr_fw3),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!ring->txhdr_cache)\n\t\t\tgoto err_kfree_meta;\n\n\t\t \n\t\tdma_test = dma_map_single(dev->dev->dma_dev, ring->txhdr_cache,\n\t\t\t\t\t      sizeof(struct b43legacy_txhdr_fw3),\n\t\t\t\t\t      DMA_TO_DEVICE);\n\n\t\tif (b43legacy_dma_mapping_error(ring, dma_test,\n\t\t\t\t\tsizeof(struct b43legacy_txhdr_fw3), 1)) {\n\t\t\t \n\t\t\tkfree(ring->txhdr_cache);\n\t\t\tring->txhdr_cache = kcalloc(nr_slots,\n\t\t\t\t\tsizeof(struct b43legacy_txhdr_fw3),\n\t\t\t\t\tGFP_KERNEL | GFP_DMA);\n\t\t\tif (!ring->txhdr_cache)\n\t\t\t\tgoto err_kfree_meta;\n\n\t\t\tdma_test = dma_map_single(dev->dev->dma_dev,\n\t\t\t\t\tring->txhdr_cache,\n\t\t\t\t\tsizeof(struct b43legacy_txhdr_fw3),\n\t\t\t\t\tDMA_TO_DEVICE);\n\n\t\t\tif (b43legacy_dma_mapping_error(ring, dma_test,\n\t\t\t\t\tsizeof(struct b43legacy_txhdr_fw3), 1))\n\t\t\t\tgoto err_kfree_txhdr_cache;\n\t\t}\n\n\t\tdma_unmap_single(dev->dev->dma_dev, dma_test,\n\t\t\t\t sizeof(struct b43legacy_txhdr_fw3),\n\t\t\t\t DMA_TO_DEVICE);\n\t}\n\n\tring->nr_slots = nr_slots;\n\tring->mmio_base = b43legacy_dmacontroller_base(type, controller_index);\n\tring->index = controller_index;\n\tif (for_tx) {\n\t\tring->tx = true;\n\t\tring->current_slot = -1;\n\t} else {\n\t\tif (ring->index == 0) {\n\t\t\tring->rx_buffersize = B43legacy_DMA0_RX_BUFFERSIZE;\n\t\t\tring->frameoffset = B43legacy_DMA0_RX_FRAMEOFFSET;\n\t\t} else if (ring->index == 3) {\n\t\t\tring->rx_buffersize = B43legacy_DMA3_RX_BUFFERSIZE;\n\t\t\tring->frameoffset = B43legacy_DMA3_RX_FRAMEOFFSET;\n\t\t} else\n\t\t\tB43legacy_WARN_ON(1);\n\t}\n#ifdef CONFIG_B43LEGACY_DEBUG\n\tring->last_injected_overflow = jiffies;\n#endif\n\n\terr = alloc_ringmemory(ring);\n\tif (err)\n\t\tgoto err_kfree_txhdr_cache;\n\terr = dmacontroller_setup(ring);\n\tif (err)\n\t\tgoto err_free_ringmemory;\n\nout:\n\treturn ring;\n\nerr_free_ringmemory:\n\tfree_ringmemory(ring);\nerr_kfree_txhdr_cache:\n\tkfree(ring->txhdr_cache);\nerr_kfree_meta:\n\tkfree(ring->meta);\nerr_kfree_ring:\n\tkfree(ring);\n\tring = NULL;\n\tgoto out;\n}\n\n \nstatic void b43legacy_destroy_dmaring(struct b43legacy_dmaring *ring)\n{\n\tif (!ring)\n\t\treturn;\n\n\tb43legacydbg(ring->dev->wl, \"DMA-%u 0x%04X (%s) max used slots:\"\n\t\t     \" %d/%d\\n\", (unsigned int)(ring->type), ring->mmio_base,\n\t\t     (ring->tx) ? \"TX\" : \"RX\", ring->max_used_slots,\n\t\t     ring->nr_slots);\n\t \n\tdmacontroller_cleanup(ring);\n\tfree_all_descbuffers(ring);\n\tfree_ringmemory(ring);\n\n\tkfree(ring->txhdr_cache);\n\tkfree(ring->meta);\n\tkfree(ring);\n}\n\nvoid b43legacy_dma_free(struct b43legacy_wldev *dev)\n{\n\tstruct b43legacy_dma *dma;\n\n\tif (b43legacy_using_pio(dev))\n\t\treturn;\n\tdma = &dev->dma;\n\n\tb43legacy_destroy_dmaring(dma->rx_ring3);\n\tdma->rx_ring3 = NULL;\n\tb43legacy_destroy_dmaring(dma->rx_ring0);\n\tdma->rx_ring0 = NULL;\n\n\tb43legacy_destroy_dmaring(dma->tx_ring5);\n\tdma->tx_ring5 = NULL;\n\tb43legacy_destroy_dmaring(dma->tx_ring4);\n\tdma->tx_ring4 = NULL;\n\tb43legacy_destroy_dmaring(dma->tx_ring3);\n\tdma->tx_ring3 = NULL;\n\tb43legacy_destroy_dmaring(dma->tx_ring2);\n\tdma->tx_ring2 = NULL;\n\tb43legacy_destroy_dmaring(dma->tx_ring1);\n\tdma->tx_ring1 = NULL;\n\tb43legacy_destroy_dmaring(dma->tx_ring0);\n\tdma->tx_ring0 = NULL;\n}\n\nint b43legacy_dma_init(struct b43legacy_wldev *dev)\n{\n\tstruct b43legacy_dma *dma = &dev->dma;\n\tstruct b43legacy_dmaring *ring;\n\tenum b43legacy_dmatype type = b43legacy_engine_type(dev);\n\tint err;\n\n\terr = dma_set_mask_and_coherent(dev->dev->dma_dev, DMA_BIT_MASK(type));\n\tif (err) {\n#ifdef CONFIG_B43LEGACY_PIO\n\t\tb43legacywarn(dev->wl, \"DMA for this device not supported. \"\n\t\t\t\"Falling back to PIO\\n\");\n\t\tdev->__using_pio = true;\n\t\treturn -EAGAIN;\n#else\n\t\tb43legacyerr(dev->wl, \"DMA for this device not supported and \"\n\t\t       \"no PIO support compiled in\\n\");\n\t\treturn -EOPNOTSUPP;\n#endif\n\t}\n\tdma->translation = ssb_dma_translation(dev->dev);\n\n\terr = -ENOMEM;\n\t \n\tring = b43legacy_setup_dmaring(dev, 0, 1, type);\n\tif (!ring)\n\t\tgoto out;\n\tdma->tx_ring0 = ring;\n\n\tring = b43legacy_setup_dmaring(dev, 1, 1, type);\n\tif (!ring)\n\t\tgoto err_destroy_tx0;\n\tdma->tx_ring1 = ring;\n\n\tring = b43legacy_setup_dmaring(dev, 2, 1, type);\n\tif (!ring)\n\t\tgoto err_destroy_tx1;\n\tdma->tx_ring2 = ring;\n\n\tring = b43legacy_setup_dmaring(dev, 3, 1, type);\n\tif (!ring)\n\t\tgoto err_destroy_tx2;\n\tdma->tx_ring3 = ring;\n\n\tring = b43legacy_setup_dmaring(dev, 4, 1, type);\n\tif (!ring)\n\t\tgoto err_destroy_tx3;\n\tdma->tx_ring4 = ring;\n\n\tring = b43legacy_setup_dmaring(dev, 5, 1, type);\n\tif (!ring)\n\t\tgoto err_destroy_tx4;\n\tdma->tx_ring5 = ring;\n\n\t \n\tring = b43legacy_setup_dmaring(dev, 0, 0, type);\n\tif (!ring)\n\t\tgoto err_destroy_tx5;\n\tdma->rx_ring0 = ring;\n\n\tif (dev->dev->id.revision < 5) {\n\t\tring = b43legacy_setup_dmaring(dev, 3, 0, type);\n\t\tif (!ring)\n\t\t\tgoto err_destroy_rx0;\n\t\tdma->rx_ring3 = ring;\n\t}\n\n\tb43legacydbg(dev->wl, \"%u-bit DMA initialized\\n\", (unsigned int)type);\n\terr = 0;\nout:\n\treturn err;\n\nerr_destroy_rx0:\n\tb43legacy_destroy_dmaring(dma->rx_ring0);\n\tdma->rx_ring0 = NULL;\nerr_destroy_tx5:\n\tb43legacy_destroy_dmaring(dma->tx_ring5);\n\tdma->tx_ring5 = NULL;\nerr_destroy_tx4:\n\tb43legacy_destroy_dmaring(dma->tx_ring4);\n\tdma->tx_ring4 = NULL;\nerr_destroy_tx3:\n\tb43legacy_destroy_dmaring(dma->tx_ring3);\n\tdma->tx_ring3 = NULL;\nerr_destroy_tx2:\n\tb43legacy_destroy_dmaring(dma->tx_ring2);\n\tdma->tx_ring2 = NULL;\nerr_destroy_tx1:\n\tb43legacy_destroy_dmaring(dma->tx_ring1);\n\tdma->tx_ring1 = NULL;\nerr_destroy_tx0:\n\tb43legacy_destroy_dmaring(dma->tx_ring0);\n\tdma->tx_ring0 = NULL;\n\tgoto out;\n}\n\n \nstatic u16 generate_cookie(struct b43legacy_dmaring *ring,\n\t\t\t   int slot)\n{\n\tu16 cookie = 0x1000;\n\n\t \n\tswitch (ring->index) {\n\tcase 0:\n\t\tcookie = 0xA000;\n\t\tbreak;\n\tcase 1:\n\t\tcookie = 0xB000;\n\t\tbreak;\n\tcase 2:\n\t\tcookie = 0xC000;\n\t\tbreak;\n\tcase 3:\n\t\tcookie = 0xD000;\n\t\tbreak;\n\tcase 4:\n\t\tcookie = 0xE000;\n\t\tbreak;\n\tcase 5:\n\t\tcookie = 0xF000;\n\t\tbreak;\n\t}\n\tB43legacy_WARN_ON(!(((u16)slot & 0xF000) == 0x0000));\n\tcookie |= (u16)slot;\n\n\treturn cookie;\n}\n\n \nstatic\nstruct b43legacy_dmaring *parse_cookie(struct b43legacy_wldev *dev,\n\t\t\t\t      u16 cookie, int *slot)\n{\n\tstruct b43legacy_dma *dma = &dev->dma;\n\tstruct b43legacy_dmaring *ring = NULL;\n\n\tswitch (cookie & 0xF000) {\n\tcase 0xA000:\n\t\tring = dma->tx_ring0;\n\t\tbreak;\n\tcase 0xB000:\n\t\tring = dma->tx_ring1;\n\t\tbreak;\n\tcase 0xC000:\n\t\tring = dma->tx_ring2;\n\t\tbreak;\n\tcase 0xD000:\n\t\tring = dma->tx_ring3;\n\t\tbreak;\n\tcase 0xE000:\n\t\tring = dma->tx_ring4;\n\t\tbreak;\n\tcase 0xF000:\n\t\tring = dma->tx_ring5;\n\t\tbreak;\n\tdefault:\n\t\tB43legacy_WARN_ON(1);\n\t}\n\t*slot = (cookie & 0x0FFF);\n\tB43legacy_WARN_ON(!(ring && *slot >= 0 && *slot < ring->nr_slots));\n\n\treturn ring;\n}\n\nstatic int dma_tx_fragment(struct b43legacy_dmaring *ring,\n\t\t\t    struct sk_buff **in_skb)\n{\n\tstruct sk_buff *skb = *in_skb;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tu8 *header;\n\tint slot, old_top_slot, old_used_slots;\n\tint err;\n\tstruct b43legacy_dmadesc32 *desc;\n\tstruct b43legacy_dmadesc_meta *meta;\n\tstruct b43legacy_dmadesc_meta *meta_hdr;\n\tstruct sk_buff *bounce_skb;\n\n#define SLOTS_PER_PACKET  2\n\tB43legacy_WARN_ON(skb_shinfo(skb)->nr_frags != 0);\n\n\told_top_slot = ring->current_slot;\n\told_used_slots = ring->used_slots;\n\n\t \n\tslot = request_slot(ring);\n\tdesc = op32_idx2desc(ring, slot, &meta_hdr);\n\tmemset(meta_hdr, 0, sizeof(*meta_hdr));\n\n\theader = &(ring->txhdr_cache[slot * sizeof(\n\t\t\t       struct b43legacy_txhdr_fw3)]);\n\terr = b43legacy_generate_txhdr(ring->dev, header,\n\t\t\t\t skb->data, skb->len, info,\n\t\t\t\t generate_cookie(ring, slot));\n\tif (unlikely(err)) {\n\t\tring->current_slot = old_top_slot;\n\t\tring->used_slots = old_used_slots;\n\t\treturn err;\n\t}\n\n\tmeta_hdr->dmaaddr = map_descbuffer(ring, (unsigned char *)header,\n\t\t\t\t\t   sizeof(struct b43legacy_txhdr_fw3), 1);\n\tif (b43legacy_dma_mapping_error(ring, meta_hdr->dmaaddr,\n\t\t\t\t\tsizeof(struct b43legacy_txhdr_fw3), 1)) {\n\t\tring->current_slot = old_top_slot;\n\t\tring->used_slots = old_used_slots;\n\t\treturn -EIO;\n\t}\n\top32_fill_descriptor(ring, desc, meta_hdr->dmaaddr,\n\t\t\t     sizeof(struct b43legacy_txhdr_fw3), 1, 0, 0);\n\n\t \n\tslot = request_slot(ring);\n\tdesc = op32_idx2desc(ring, slot, &meta);\n\tmemset(meta, 0, sizeof(*meta));\n\n\tmeta->skb = skb;\n\tmeta->is_last_fragment = true;\n\n\tmeta->dmaaddr = map_descbuffer(ring, skb->data, skb->len, 1);\n\t \n\tif (b43legacy_dma_mapping_error(ring, meta->dmaaddr, skb->len, 1)) {\n\t\tbounce_skb = alloc_skb(skb->len, GFP_KERNEL | GFP_DMA);\n\t\tif (!bounce_skb) {\n\t\t\tring->current_slot = old_top_slot;\n\t\t\tring->used_slots = old_used_slots;\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_unmap_hdr;\n\t\t}\n\n\t\tskb_put_data(bounce_skb, skb->data, skb->len);\n\t\tmemcpy(bounce_skb->cb, skb->cb, sizeof(skb->cb));\n\t\tbounce_skb->dev = skb->dev;\n\t\tskb_set_queue_mapping(bounce_skb, skb_get_queue_mapping(skb));\n\t\tinfo = IEEE80211_SKB_CB(bounce_skb);\n\n\t\tdev_kfree_skb_any(skb);\n\t\tskb = bounce_skb;\n\t\t*in_skb = bounce_skb;\n\t\tmeta->skb = skb;\n\t\tmeta->dmaaddr = map_descbuffer(ring, skb->data, skb->len, 1);\n\t\tif (b43legacy_dma_mapping_error(ring, meta->dmaaddr, skb->len, 1)) {\n\t\t\tring->current_slot = old_top_slot;\n\t\t\tring->used_slots = old_used_slots;\n\t\t\terr = -EIO;\n\t\t\tgoto out_free_bounce;\n\t\t}\n\t}\n\n\top32_fill_descriptor(ring, desc, meta->dmaaddr,\n\t\t\t     skb->len, 0, 1, 1);\n\n\twmb();\t \n\t \n\top32_poke_tx(ring, next_slot(ring, slot));\n\treturn 0;\n\nout_free_bounce:\n\tdev_kfree_skb_any(skb);\nout_unmap_hdr:\n\tunmap_descbuffer(ring, meta_hdr->dmaaddr,\n\t\t\t sizeof(struct b43legacy_txhdr_fw3), 1);\n\treturn err;\n}\n\nstatic inline\nint should_inject_overflow(struct b43legacy_dmaring *ring)\n{\n#ifdef CONFIG_B43LEGACY_DEBUG\n\tif (unlikely(b43legacy_debug(ring->dev,\n\t\t\t\t     B43legacy_DBG_DMAOVERFLOW))) {\n\t\t \n\t\tunsigned long next_overflow;\n\n\t\tnext_overflow = ring->last_injected_overflow + HZ;\n\t\tif (time_after(jiffies, next_overflow)) {\n\t\t\tring->last_injected_overflow = jiffies;\n\t\t\tb43legacydbg(ring->dev->wl,\n\t\t\t       \"Injecting TX ring overflow on \"\n\t\t\t       \"DMA controller %d\\n\", ring->index);\n\t\t\treturn 1;\n\t\t}\n\t}\n#endif  \n\treturn 0;\n}\n\nint b43legacy_dma_tx(struct b43legacy_wldev *dev,\n\t\t     struct sk_buff *skb)\n{\n\tstruct b43legacy_dmaring *ring;\n\tint err = 0;\n\n\tring = priority_to_txring(dev, skb_get_queue_mapping(skb));\n\tB43legacy_WARN_ON(!ring->tx);\n\n\tif (unlikely(ring->stopped)) {\n\t\t \n\t\tif (b43legacy_debug(dev, B43legacy_DBG_DMAVERBOSE))\n\t\t\tb43legacyerr(dev->wl, \"Packet after queue stopped\\n\");\n\t\treturn -ENOSPC;\n\t}\n\n\tif (WARN_ON(free_slots(ring) < SLOTS_PER_PACKET)) {\n\t\t \n\t\tb43legacyerr(dev->wl, \"DMA queue overflow\\n\");\n\t\treturn -ENOSPC;\n\t}\n\n\t \n\terr = dma_tx_fragment(ring, &skb);\n\tif (unlikely(err == -ENOKEY)) {\n\t\t \n\t\tdev_kfree_skb_any(skb);\n\t\treturn 0;\n\t}\n\tif (unlikely(err)) {\n\t\tb43legacyerr(dev->wl, \"DMA tx mapping failure\\n\");\n\t\treturn err;\n\t}\n\tif ((free_slots(ring) < SLOTS_PER_PACKET) ||\n\t    should_inject_overflow(ring)) {\n\t\t \n\t\tunsigned int skb_mapping = skb_get_queue_mapping(skb);\n\t\tieee80211_stop_queue(dev->wl->hw, skb_mapping);\n\t\tdev->wl->tx_queue_stopped[skb_mapping] = 1;\n\t\tring->stopped = true;\n\t\tif (b43legacy_debug(dev, B43legacy_DBG_DMAVERBOSE))\n\t\t\tb43legacydbg(dev->wl, \"Stopped TX ring %d\\n\",\n\t\t\t       ring->index);\n\t}\n\treturn err;\n}\n\nvoid b43legacy_dma_handle_txstatus(struct b43legacy_wldev *dev,\n\t\t\t\t const struct b43legacy_txstatus *status)\n{\n\tstruct b43legacy_dmaring *ring;\n\tstruct b43legacy_dmadesc_meta *meta;\n\tint retry_limit;\n\tint slot;\n\tint firstused;\n\n\tring = parse_cookie(dev, status->cookie, &slot);\n\tif (unlikely(!ring))\n\t\treturn;\n\tB43legacy_WARN_ON(!ring->tx);\n\n\t \n\tfirstused = ring->current_slot - ring->used_slots + 1;\n\tif (firstused < 0)\n\t\tfirstused = ring->nr_slots + firstused;\n\tif (unlikely(slot != firstused)) {\n\t\t \n\t\tb43legacydbg(dev->wl, \"Out of order TX status report on DMA \"\n\t\t\t     \"ring %d. Expected %d, but got %d\\n\",\n\t\t\t     ring->index, firstused, slot);\n\t\treturn;\n\t}\n\n\twhile (1) {\n\t\tB43legacy_WARN_ON(!(slot >= 0 && slot < ring->nr_slots));\n\t\top32_idx2desc(ring, slot, &meta);\n\n\t\tif (meta->skb)\n\t\t\tunmap_descbuffer(ring, meta->dmaaddr,\n\t\t\t\t\t meta->skb->len, 1);\n\t\telse\n\t\t\tunmap_descbuffer(ring, meta->dmaaddr,\n\t\t\t\t\t sizeof(struct b43legacy_txhdr_fw3),\n\t\t\t\t\t 1);\n\n\t\tif (meta->is_last_fragment) {\n\t\t\tstruct ieee80211_tx_info *info;\n\t\t\tBUG_ON(!meta->skb);\n\t\t\tinfo = IEEE80211_SKB_CB(meta->skb);\n\n\t\t\t \n\t\t\tretry_limit = info->status.rates[0].count;\n\t\t\tieee80211_tx_info_clear_status(info);\n\n\t\t\tif (status->acked)\n\t\t\t\tinfo->flags |= IEEE80211_TX_STAT_ACK;\n\n\t\t\tif (status->rts_count > dev->wl->hw->conf.short_frame_max_tx_count) {\n\t\t\t\t \n\t\t\t\tinfo->status.rates[0].count = 0;\n\t\t\t\tinfo->status.rates[1].count = status->frame_count;\n\t\t\t} else {\n\t\t\t\tif (status->frame_count > retry_limit) {\n\t\t\t\t\tinfo->status.rates[0].count = retry_limit;\n\t\t\t\t\tinfo->status.rates[1].count = status->frame_count -\n\t\t\t\t\t\t\tretry_limit;\n\n\t\t\t\t} else {\n\t\t\t\t\tinfo->status.rates[0].count = status->frame_count;\n\t\t\t\t\tinfo->status.rates[1].idx = -1;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t \n\t\t\tieee80211_tx_status_irqsafe(dev->wl->hw, meta->skb);\n\t\t\t \n\t\t\tmeta->skb = NULL;\n\t\t} else {\n\t\t\t \n\t\t\tB43legacy_WARN_ON(meta->skb != NULL);\n\t\t}\n\n\t\t \n\t\tring->used_slots--;\n\n\t\tif (meta->is_last_fragment)\n\t\t\tbreak;\n\t\tslot = next_slot(ring, slot);\n\t}\n\tdev->stats.last_tx = jiffies;\n\tif (ring->stopped) {\n\t\tB43legacy_WARN_ON(free_slots(ring) < SLOTS_PER_PACKET);\n\t\tring->stopped = false;\n\t}\n\n\tif (dev->wl->tx_queue_stopped[ring->queue_prio]) {\n\t\tdev->wl->tx_queue_stopped[ring->queue_prio] = 0;\n\t} else {\n\t\t \n\t\tieee80211_wake_queue(dev->wl->hw, ring->queue_prio);\n\t\tif (b43legacy_debug(dev, B43legacy_DBG_DMAVERBOSE))\n\t\t\tb43legacydbg(dev->wl, \"Woke up TX ring %d\\n\",\n\t\t\t\t     ring->index);\n\t}\n\t \n\tieee80211_queue_work(dev->wl->hw, &dev->wl->tx_work);\n}\n\nstatic void dma_rx(struct b43legacy_dmaring *ring,\n\t\t   int *slot)\n{\n\tstruct b43legacy_dmadesc32 *desc;\n\tstruct b43legacy_dmadesc_meta *meta;\n\tstruct b43legacy_rxhdr_fw3 *rxhdr;\n\tstruct sk_buff *skb;\n\tu16 len;\n\tint err;\n\tdma_addr_t dmaaddr;\n\n\tdesc = op32_idx2desc(ring, *slot, &meta);\n\n\tsync_descbuffer_for_cpu(ring, meta->dmaaddr, ring->rx_buffersize);\n\tskb = meta->skb;\n\n\tif (ring->index == 3) {\n\t\t \n\t\tstruct b43legacy_hwtxstatus *hw =\n\t\t\t\t(struct b43legacy_hwtxstatus *)skb->data;\n\t\tint i = 0;\n\n\t\twhile (hw->cookie == 0) {\n\t\t\tif (i > 100)\n\t\t\t\tbreak;\n\t\t\ti++;\n\t\t\tudelay(2);\n\t\t\tbarrier();\n\t\t}\n\t\tb43legacy_handle_hwtxstatus(ring->dev, hw);\n\t\t \n\t\tsync_descbuffer_for_device(ring, meta->dmaaddr,\n\t\t\t\t\t   ring->rx_buffersize);\n\n\t\treturn;\n\t}\n\trxhdr = (struct b43legacy_rxhdr_fw3 *)skb->data;\n\tlen = le16_to_cpu(rxhdr->frame_len);\n\tif (len == 0) {\n\t\tint i = 0;\n\n\t\tdo {\n\t\t\tudelay(2);\n\t\t\tbarrier();\n\t\t\tlen = le16_to_cpu(rxhdr->frame_len);\n\t\t} while (len == 0 && i++ < 5);\n\t\tif (unlikely(len == 0)) {\n\t\t\t \n\t\t\tsync_descbuffer_for_device(ring, meta->dmaaddr,\n\t\t\t\t\t\t   ring->rx_buffersize);\n\t\t\tgoto drop;\n\t\t}\n\t}\n\tif (unlikely(len > ring->rx_buffersize)) {\n\t\t \n\t\tint cnt = 0;\n\t\ts32 tmp = len;\n\n\t\twhile (1) {\n\t\t\tdesc = op32_idx2desc(ring, *slot, &meta);\n\t\t\t \n\t\t\tsync_descbuffer_for_device(ring, meta->dmaaddr,\n\t\t\t\t\t\t   ring->rx_buffersize);\n\t\t\t*slot = next_slot(ring, *slot);\n\t\t\tcnt++;\n\t\t\ttmp -= ring->rx_buffersize;\n\t\t\tif (tmp <= 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tb43legacyerr(ring->dev->wl, \"DMA RX buffer too small \"\n\t\t       \"(len: %u, buffer: %u, nr-dropped: %d)\\n\",\n\t\t       len, ring->rx_buffersize, cnt);\n\t\tgoto drop;\n\t}\n\n\tdmaaddr = meta->dmaaddr;\n\terr = setup_rx_descbuffer(ring, desc, meta, GFP_ATOMIC);\n\tif (unlikely(err)) {\n\t\tb43legacydbg(ring->dev->wl, \"DMA RX: setup_rx_descbuffer()\"\n\t\t\t     \" failed\\n\");\n\t\tsync_descbuffer_for_device(ring, dmaaddr,\n\t\t\t\t\t   ring->rx_buffersize);\n\t\tgoto drop;\n\t}\n\n\tunmap_descbuffer(ring, dmaaddr, ring->rx_buffersize, 0);\n\tskb_put(skb, len + ring->frameoffset);\n\tskb_pull(skb, ring->frameoffset);\n\n\tb43legacy_rx(ring->dev, skb, rxhdr);\ndrop:\n\treturn;\n}\n\nvoid b43legacy_dma_rx(struct b43legacy_dmaring *ring)\n{\n\tint slot;\n\tint current_slot;\n\tint used_slots = 0;\n\n\tB43legacy_WARN_ON(ring->tx);\n\tcurrent_slot = op32_get_current_rxslot(ring);\n\tB43legacy_WARN_ON(!(current_slot >= 0 && current_slot <\n\t\t\t   ring->nr_slots));\n\n\tslot = ring->current_slot;\n\tfor (; slot != current_slot; slot = next_slot(ring, slot)) {\n\t\tdma_rx(ring, &slot);\n\t\tupdate_max_used_slots(ring, ++used_slots);\n\t}\n\top32_set_current_rxslot(ring, slot);\n\tring->current_slot = slot;\n}\n\nstatic void b43legacy_dma_tx_suspend_ring(struct b43legacy_dmaring *ring)\n{\n\tB43legacy_WARN_ON(!ring->tx);\n\top32_tx_suspend(ring);\n}\n\nstatic void b43legacy_dma_tx_resume_ring(struct b43legacy_dmaring *ring)\n{\n\tB43legacy_WARN_ON(!ring->tx);\n\top32_tx_resume(ring);\n}\n\nvoid b43legacy_dma_tx_suspend(struct b43legacy_wldev *dev)\n{\n\tb43legacy_power_saving_ctl_bits(dev, -1, 1);\n\tb43legacy_dma_tx_suspend_ring(dev->dma.tx_ring0);\n\tb43legacy_dma_tx_suspend_ring(dev->dma.tx_ring1);\n\tb43legacy_dma_tx_suspend_ring(dev->dma.tx_ring2);\n\tb43legacy_dma_tx_suspend_ring(dev->dma.tx_ring3);\n\tb43legacy_dma_tx_suspend_ring(dev->dma.tx_ring4);\n\tb43legacy_dma_tx_suspend_ring(dev->dma.tx_ring5);\n}\n\nvoid b43legacy_dma_tx_resume(struct b43legacy_wldev *dev)\n{\n\tb43legacy_dma_tx_resume_ring(dev->dma.tx_ring5);\n\tb43legacy_dma_tx_resume_ring(dev->dma.tx_ring4);\n\tb43legacy_dma_tx_resume_ring(dev->dma.tx_ring3);\n\tb43legacy_dma_tx_resume_ring(dev->dma.tx_ring2);\n\tb43legacy_dma_tx_resume_ring(dev->dma.tx_ring1);\n\tb43legacy_dma_tx_resume_ring(dev->dma.tx_ring0);\n\tb43legacy_power_saving_ctl_bits(dev, -1, -1);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}