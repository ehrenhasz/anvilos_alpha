{
  "module_name": "pearl_pcie.c",
  "hash_id": "bdee1bd97f62eedc75cbbf12267e268a6112cd075342c0bc5e9aeebd1aef6ac3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wireless/quantenna/qtnfmac/pcie/pearl_pcie.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/firmware.h>\n#include <linux/pci.h>\n#include <linux/vmalloc.h>\n#include <linux/delay.h>\n#include <linux/interrupt.h>\n#include <linux/sched.h>\n#include <linux/completion.h>\n#include <linux/crc32.h>\n#include <linux/spinlock.h>\n#include <linux/circ_buf.h>\n#include <linux/log2.h>\n\n#include \"pcie_priv.h\"\n#include \"pearl_pcie_regs.h\"\n#include \"pearl_pcie_ipc.h\"\n#include \"qtn_hw_ids.h\"\n#include \"core.h\"\n#include \"bus.h\"\n#include \"shm_ipc.h\"\n#include \"debug.h\"\n\n#define PEARL_TX_BD_SIZE_DEFAULT\t32\n#define PEARL_RX_BD_SIZE_DEFAULT\t256\n\nstruct qtnf_pearl_bda {\n\t__le16 bda_len;\n\t__le16 bda_version;\n\t__le32 bda_pci_endian;\n\t__le32 bda_ep_state;\n\t__le32 bda_rc_state;\n\t__le32 bda_dma_mask;\n\t__le32 bda_msi_addr;\n\t__le32 bda_flashsz;\n\tu8 bda_boardname[PCIE_BDA_NAMELEN];\n\t__le32 bda_rc_msi_enabled;\n\tu8 bda_hhbm_list[PCIE_HHBM_MAX_SIZE];\n\t__le32 bda_dsbw_start_index;\n\t__le32 bda_dsbw_end_index;\n\t__le32 bda_dsbw_total_bytes;\n\t__le32 bda_rc_tx_bd_base;\n\t__le32 bda_rc_tx_bd_num;\n\tu8 bda_pcie_mac[QTN_ENET_ADDR_LENGTH];\n\tstruct qtnf_shm_ipc_region bda_shm_reg1 __aligned(4096);  \n\tstruct qtnf_shm_ipc_region bda_shm_reg2 __aligned(4096);  \n} __packed;\n\nstruct qtnf_pearl_tx_bd {\n\t__le32 addr;\n\t__le32 addr_h;\n\t__le32 info;\n\t__le32 info_h;\n} __packed;\n\nstruct qtnf_pearl_rx_bd {\n\t__le32 addr;\n\t__le32 addr_h;\n\t__le32 info;\n\t__le32 info_h;\n\t__le32 next_ptr;\n\t__le32 next_ptr_h;\n} __packed;\n\nstruct qtnf_pearl_fw_hdr {\n\tu8 boardflg[8];\n\t__le32 fwsize;\n\t__le32 seqnum;\n\t__le32 type;\n\t__le32 pktlen;\n\t__le32 crc;\n} __packed;\n\nstruct qtnf_pcie_pearl_state {\n\tstruct qtnf_pcie_bus_priv base;\n\n\t \n\tspinlock_t irq_lock;\n\n\tstruct qtnf_pearl_bda __iomem *bda;\n\tvoid __iomem *pcie_reg_base;\n\n\tstruct qtnf_pearl_tx_bd *tx_bd_vbase;\n\tdma_addr_t tx_bd_pbase;\n\n\tstruct qtnf_pearl_rx_bd *rx_bd_vbase;\n\tdma_addr_t rx_bd_pbase;\n\n\tdma_addr_t bd_table_paddr;\n\tvoid *bd_table_vaddr;\n\tu32 bd_table_len;\n\tu32 pcie_irq_mask;\n\tu32 pcie_irq_rx_count;\n\tu32 pcie_irq_tx_count;\n\tu32 pcie_irq_uf_count;\n};\n\nstatic inline void qtnf_init_hdp_irqs(struct qtnf_pcie_pearl_state *ps)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ps->irq_lock, flags);\n\tps->pcie_irq_mask = (PCIE_HDP_INT_RX_BITS | PCIE_HDP_INT_TX_BITS);\n\tspin_unlock_irqrestore(&ps->irq_lock, flags);\n}\n\nstatic inline void qtnf_enable_hdp_irqs(struct qtnf_pcie_pearl_state *ps)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ps->irq_lock, flags);\n\twritel(ps->pcie_irq_mask, PCIE_HDP_INT_EN(ps->pcie_reg_base));\n\tspin_unlock_irqrestore(&ps->irq_lock, flags);\n}\n\nstatic inline void qtnf_disable_hdp_irqs(struct qtnf_pcie_pearl_state *ps)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ps->irq_lock, flags);\n\twritel(0x0, PCIE_HDP_INT_EN(ps->pcie_reg_base));\n\tspin_unlock_irqrestore(&ps->irq_lock, flags);\n}\n\nstatic inline void qtnf_en_rxdone_irq(struct qtnf_pcie_pearl_state *ps)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ps->irq_lock, flags);\n\tps->pcie_irq_mask |= PCIE_HDP_INT_RX_BITS;\n\twritel(ps->pcie_irq_mask, PCIE_HDP_INT_EN(ps->pcie_reg_base));\n\tspin_unlock_irqrestore(&ps->irq_lock, flags);\n}\n\nstatic inline void qtnf_dis_rxdone_irq(struct qtnf_pcie_pearl_state *ps)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ps->irq_lock, flags);\n\tps->pcie_irq_mask &= ~PCIE_HDP_INT_RX_BITS;\n\twritel(ps->pcie_irq_mask, PCIE_HDP_INT_EN(ps->pcie_reg_base));\n\tspin_unlock_irqrestore(&ps->irq_lock, flags);\n}\n\nstatic inline void qtnf_en_txdone_irq(struct qtnf_pcie_pearl_state *ps)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ps->irq_lock, flags);\n\tps->pcie_irq_mask |= PCIE_HDP_INT_TX_BITS;\n\twritel(ps->pcie_irq_mask, PCIE_HDP_INT_EN(ps->pcie_reg_base));\n\tspin_unlock_irqrestore(&ps->irq_lock, flags);\n}\n\nstatic inline void qtnf_dis_txdone_irq(struct qtnf_pcie_pearl_state *ps)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ps->irq_lock, flags);\n\tps->pcie_irq_mask &= ~PCIE_HDP_INT_TX_BITS;\n\twritel(ps->pcie_irq_mask, PCIE_HDP_INT_EN(ps->pcie_reg_base));\n\tspin_unlock_irqrestore(&ps->irq_lock, flags);\n}\n\nstatic void qtnf_deassert_intx(struct qtnf_pcie_pearl_state *ps)\n{\n\tvoid __iomem *reg = ps->base.sysctl_bar + PEARL_PCIE_CFG0_OFFSET;\n\tu32 cfg;\n\n\tcfg = readl(reg);\n\tcfg &= ~PEARL_ASSERT_INTX;\n\tqtnf_non_posted_write(cfg, reg);\n}\n\nstatic void qtnf_pearl_reset_ep(struct qtnf_pcie_pearl_state *ps)\n{\n\tconst u32 data = QTN_PEARL_IPC_IRQ_WORD(QTN_PEARL_LHOST_EP_RESET);\n\tvoid __iomem *reg = ps->base.sysctl_bar +\n\t\t\t    QTN_PEARL_SYSCTL_LHOST_IRQ_OFFSET;\n\n\tqtnf_non_posted_write(data, reg);\n\tmsleep(QTN_EP_RESET_WAIT_MS);\n\tpci_restore_state(ps->base.pdev);\n}\n\nstatic void qtnf_pcie_pearl_ipc_gen_ep_int(void *arg)\n{\n\tconst struct qtnf_pcie_pearl_state *ps = arg;\n\tconst u32 data = QTN_PEARL_IPC_IRQ_WORD(QTN_PEARL_LHOST_IPC_IRQ);\n\tvoid __iomem *reg = ps->base.sysctl_bar +\n\t\t\t    QTN_PEARL_SYSCTL_LHOST_IRQ_OFFSET;\n\n\tqtnf_non_posted_write(data, reg);\n}\n\nstatic int qtnf_is_state(__le32 __iomem *reg, u32 state)\n{\n\tu32 s = readl(reg);\n\n\treturn s & state;\n}\n\nstatic void qtnf_set_state(__le32 __iomem *reg, u32 state)\n{\n\tu32 s = readl(reg);\n\n\tqtnf_non_posted_write(state | s, reg);\n}\n\nstatic void qtnf_clear_state(__le32 __iomem *reg, u32 state)\n{\n\tu32 s = readl(reg);\n\n\tqtnf_non_posted_write(s & ~state, reg);\n}\n\nstatic int qtnf_poll_state(__le32 __iomem *reg, u32 state, u32 delay_in_ms)\n{\n\tu32 timeout = 0;\n\n\twhile ((qtnf_is_state(reg, state) == 0)) {\n\t\tusleep_range(1000, 1200);\n\t\tif (++timeout > delay_in_ms)\n\t\t\treturn -1;\n\t}\n\n\treturn 0;\n}\n\nstatic int pearl_alloc_bd_table(struct qtnf_pcie_pearl_state *ps)\n{\n\tstruct qtnf_pcie_bus_priv *priv = &ps->base;\n\tdma_addr_t paddr;\n\tvoid *vaddr;\n\tint len;\n\n\tlen = priv->tx_bd_num * sizeof(struct qtnf_pearl_tx_bd) +\n\t\tpriv->rx_bd_num * sizeof(struct qtnf_pearl_rx_bd);\n\n\tvaddr = dmam_alloc_coherent(&priv->pdev->dev, len, &paddr, GFP_KERNEL);\n\tif (!vaddr)\n\t\treturn -ENOMEM;\n\n\t \n\n\tps->bd_table_vaddr = vaddr;\n\tps->bd_table_paddr = paddr;\n\tps->bd_table_len = len;\n\n\tps->tx_bd_vbase = vaddr;\n\tps->tx_bd_pbase = paddr;\n\n\tpr_debug(\"TX descriptor table: vaddr=0x%p paddr=%pad\\n\", vaddr, &paddr);\n\n\tpriv->tx_bd_r_index = 0;\n\tpriv->tx_bd_w_index = 0;\n\n\t \n\n\tvaddr = ((struct qtnf_pearl_tx_bd *)vaddr) + priv->tx_bd_num;\n\tpaddr += priv->tx_bd_num * sizeof(struct qtnf_pearl_tx_bd);\n\n\tps->rx_bd_vbase = vaddr;\n\tps->rx_bd_pbase = paddr;\n\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\twritel(QTN_HOST_HI32(paddr),\n\t       PCIE_HDP_TX_HOST_Q_BASE_H(ps->pcie_reg_base));\n#endif\n\twritel(QTN_HOST_LO32(paddr),\n\t       PCIE_HDP_TX_HOST_Q_BASE_L(ps->pcie_reg_base));\n\twritel(priv->rx_bd_num | (sizeof(struct qtnf_pearl_rx_bd)) << 16,\n\t       PCIE_HDP_TX_HOST_Q_SZ_CTRL(ps->pcie_reg_base));\n\n\tpr_debug(\"RX descriptor table: vaddr=0x%p paddr=%pad\\n\", vaddr, &paddr);\n\n\treturn 0;\n}\n\nstatic int pearl_skb2rbd_attach(struct qtnf_pcie_pearl_state *ps, u16 index)\n{\n\tstruct qtnf_pcie_bus_priv *priv = &ps->base;\n\tstruct qtnf_pearl_rx_bd *rxbd;\n\tstruct sk_buff *skb;\n\tdma_addr_t paddr;\n\n\tskb = netdev_alloc_skb_ip_align(NULL, SKB_BUF_SIZE);\n\tif (!skb) {\n\t\tpriv->rx_skb[index] = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\tpriv->rx_skb[index] = skb;\n\trxbd = &ps->rx_bd_vbase[index];\n\n\tpaddr = dma_map_single(&priv->pdev->dev, skb->data, SKB_BUF_SIZE,\n\t\t\t       DMA_FROM_DEVICE);\n\tif (dma_mapping_error(&priv->pdev->dev, paddr)) {\n\t\tpr_err(\"skb DMA mapping error: %pad\\n\", &paddr);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\trxbd->addr = cpu_to_le32(QTN_HOST_LO32(paddr));\n\trxbd->addr_h = cpu_to_le32(QTN_HOST_HI32(paddr));\n\trxbd->info = 0x0;\n\n\tpriv->rx_bd_w_index = index;\n\n\t \n\twmb();\n\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\twritel(QTN_HOST_HI32(paddr),\n\t       PCIE_HDP_HHBM_BUF_PTR_H(ps->pcie_reg_base));\n#endif\n\twritel(QTN_HOST_LO32(paddr),\n\t       PCIE_HDP_HHBM_BUF_PTR(ps->pcie_reg_base));\n\n\twritel(index, PCIE_HDP_TX_HOST_Q_WR_PTR(ps->pcie_reg_base));\n\treturn 0;\n}\n\nstatic int pearl_alloc_rx_buffers(struct qtnf_pcie_pearl_state *ps)\n{\n\tu16 i;\n\tint ret = 0;\n\n\tmemset(ps->rx_bd_vbase, 0x0,\n\t       ps->base.rx_bd_num * sizeof(struct qtnf_pearl_rx_bd));\n\n\tfor (i = 0; i < ps->base.rx_bd_num; i++) {\n\t\tret = pearl_skb2rbd_attach(ps, i);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n \nstatic void qtnf_pearl_free_xfer_buffers(struct qtnf_pcie_pearl_state *ps)\n{\n\tstruct qtnf_pcie_bus_priv *priv = &ps->base;\n\tstruct qtnf_pearl_tx_bd *txbd;\n\tstruct qtnf_pearl_rx_bd *rxbd;\n\tstruct sk_buff *skb;\n\tdma_addr_t paddr;\n\tint i;\n\n\t \n\tfor (i = 0; i < priv->rx_bd_num; i++) {\n\t\tif (priv->rx_skb && priv->rx_skb[i]) {\n\t\t\trxbd = &ps->rx_bd_vbase[i];\n\t\t\tskb = priv->rx_skb[i];\n\t\t\tpaddr = QTN_HOST_ADDR(le32_to_cpu(rxbd->addr_h),\n\t\t\t\t\t      le32_to_cpu(rxbd->addr));\n\t\t\tdma_unmap_single(&priv->pdev->dev, paddr,\n\t\t\t\t\t SKB_BUF_SIZE, DMA_FROM_DEVICE);\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tpriv->rx_skb[i] = NULL;\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < priv->tx_bd_num; i++) {\n\t\tif (priv->tx_skb && priv->tx_skb[i]) {\n\t\t\ttxbd = &ps->tx_bd_vbase[i];\n\t\t\tskb = priv->tx_skb[i];\n\t\t\tpaddr = QTN_HOST_ADDR(le32_to_cpu(txbd->addr_h),\n\t\t\t\t\t      le32_to_cpu(txbd->addr));\n\t\t\tdma_unmap_single(&priv->pdev->dev, paddr, skb->len,\n\t\t\t\t\t DMA_TO_DEVICE);\n\t\t\tdev_kfree_skb_any(skb);\n\t\t\tpriv->tx_skb[i] = NULL;\n\t\t}\n\t}\n}\n\nstatic int pearl_hhbm_init(struct qtnf_pcie_pearl_state *ps)\n{\n\tu32 val;\n\n\tval = readl(PCIE_HHBM_CONFIG(ps->pcie_reg_base));\n\tval |= HHBM_CONFIG_SOFT_RESET;\n\twritel(val, PCIE_HHBM_CONFIG(ps->pcie_reg_base));\n\tusleep_range(50, 100);\n\tval &= ~HHBM_CONFIG_SOFT_RESET;\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\tval |= HHBM_64BIT;\n#endif\n\twritel(val, PCIE_HHBM_CONFIG(ps->pcie_reg_base));\n\twritel(ps->base.rx_bd_num, PCIE_HHBM_Q_LIMIT_REG(ps->pcie_reg_base));\n\n\treturn 0;\n}\n\nstatic int qtnf_pcie_pearl_init_xfer(struct qtnf_pcie_pearl_state *ps,\n\t\t\t\t     unsigned int tx_bd_size,\n\t\t\t\t     unsigned int rx_bd_size)\n{\n\tstruct qtnf_pcie_bus_priv *priv = &ps->base;\n\tint ret;\n\tu32 val;\n\n\tif (tx_bd_size == 0)\n\t\ttx_bd_size = PEARL_TX_BD_SIZE_DEFAULT;\n\n\tval = tx_bd_size * sizeof(struct qtnf_pearl_tx_bd);\n\n\tif (!is_power_of_2(tx_bd_size) || val > PCIE_HHBM_MAX_SIZE) {\n\t\tpr_warn(\"invalid tx_bd_size value %u, use default %u\\n\",\n\t\t\ttx_bd_size, PEARL_TX_BD_SIZE_DEFAULT);\n\t\tpriv->tx_bd_num = PEARL_TX_BD_SIZE_DEFAULT;\n\t} else {\n\t\tpriv->tx_bd_num = tx_bd_size;\n\t}\n\n\tif (rx_bd_size == 0)\n\t\trx_bd_size = PEARL_RX_BD_SIZE_DEFAULT;\n\n\tval = rx_bd_size * sizeof(dma_addr_t);\n\n\tif (!is_power_of_2(rx_bd_size) || val > PCIE_HHBM_MAX_SIZE) {\n\t\tpr_warn(\"invalid rx_bd_size value %u, use default %u\\n\",\n\t\t\trx_bd_size, PEARL_RX_BD_SIZE_DEFAULT);\n\t\tpriv->rx_bd_num = PEARL_RX_BD_SIZE_DEFAULT;\n\t} else {\n\t\tpriv->rx_bd_num = rx_bd_size;\n\t}\n\n\tpriv->rx_bd_w_index = 0;\n\tpriv->rx_bd_r_index = 0;\n\n\tret = pearl_hhbm_init(ps);\n\tif (ret) {\n\t\tpr_err(\"failed to init h/w queues\\n\");\n\t\treturn ret;\n\t}\n\n\tret = qtnf_pcie_alloc_skb_array(priv);\n\tif (ret) {\n\t\tpr_err(\"failed to allocate skb array\\n\");\n\t\treturn ret;\n\t}\n\n\tret = pearl_alloc_bd_table(ps);\n\tif (ret) {\n\t\tpr_err(\"failed to allocate bd table\\n\");\n\t\treturn ret;\n\t}\n\n\tret = pearl_alloc_rx_buffers(ps);\n\tif (ret) {\n\t\tpr_err(\"failed to allocate rx buffers\\n\");\n\t\treturn ret;\n\t}\n\n\treturn ret;\n}\n\nstatic void qtnf_pearl_data_tx_reclaim(struct qtnf_pcie_pearl_state *ps)\n{\n\tstruct qtnf_pcie_bus_priv *priv = &ps->base;\n\tstruct qtnf_pearl_tx_bd *txbd;\n\tstruct sk_buff *skb;\n\tunsigned long flags;\n\tdma_addr_t paddr;\n\tu32 tx_done_index;\n\tint count = 0;\n\tint i;\n\n\tspin_lock_irqsave(&priv->tx_reclaim_lock, flags);\n\n\ttx_done_index = readl(PCIE_HDP_RX0DMA_CNT(ps->pcie_reg_base))\n\t\t\t& (priv->tx_bd_num - 1);\n\n\ti = priv->tx_bd_r_index;\n\n\twhile (CIRC_CNT(tx_done_index, i, priv->tx_bd_num)) {\n\t\tskb = priv->tx_skb[i];\n\t\tif (likely(skb)) {\n\t\t\ttxbd = &ps->tx_bd_vbase[i];\n\t\t\tpaddr = QTN_HOST_ADDR(le32_to_cpu(txbd->addr_h),\n\t\t\t\t\t      le32_to_cpu(txbd->addr));\n\t\t\tdma_unmap_single(&priv->pdev->dev, paddr, skb->len,\n\t\t\t\t\t DMA_TO_DEVICE);\n\n\t\t\tif (skb->dev) {\n\t\t\t\tdev_sw_netstats_tx_add(skb->dev, 1, skb->len);\n\t\t\t\tif (unlikely(priv->tx_stopped)) {\n\t\t\t\t\tqtnf_wake_all_queues(skb->dev);\n\t\t\t\t\tpriv->tx_stopped = 0;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tdev_kfree_skb_any(skb);\n\t\t}\n\n\t\tpriv->tx_skb[i] = NULL;\n\t\tcount++;\n\n\t\tif (++i >= priv->tx_bd_num)\n\t\t\ti = 0;\n\t}\n\n\tpriv->tx_reclaim_done += count;\n\tpriv->tx_reclaim_req++;\n\tpriv->tx_bd_r_index = i;\n\n\tspin_unlock_irqrestore(&priv->tx_reclaim_lock, flags);\n}\n\nstatic int qtnf_tx_queue_ready(struct qtnf_pcie_pearl_state *ps)\n{\n\tstruct qtnf_pcie_bus_priv *priv = &ps->base;\n\n\tif (!CIRC_SPACE(priv->tx_bd_w_index, priv->tx_bd_r_index,\n\t\t\tpriv->tx_bd_num)) {\n\t\tqtnf_pearl_data_tx_reclaim(ps);\n\n\t\tif (!CIRC_SPACE(priv->tx_bd_w_index, priv->tx_bd_r_index,\n\t\t\t\tpriv->tx_bd_num)) {\n\t\t\tpr_warn_ratelimited(\"reclaim full Tx queue\\n\");\n\t\t\tpriv->tx_full_count++;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 1;\n}\n\nstatic int qtnf_pcie_skb_send(struct qtnf_bus *bus, struct sk_buff *skb)\n{\n\tstruct qtnf_pcie_pearl_state *ps = get_bus_priv(bus);\n\tstruct qtnf_pcie_bus_priv *priv = &ps->base;\n\tdma_addr_t txbd_paddr, skb_paddr;\n\tstruct qtnf_pearl_tx_bd *txbd;\n\tunsigned long flags;\n\tint len, i;\n\tu32 info;\n\tint ret = 0;\n\n\tspin_lock_irqsave(&priv->tx_lock, flags);\n\n\tif (!qtnf_tx_queue_ready(ps)) {\n\t\tif (skb->dev) {\n\t\t\tnetif_tx_stop_all_queues(skb->dev);\n\t\t\tpriv->tx_stopped = 1;\n\t\t}\n\n\t\tspin_unlock_irqrestore(&priv->tx_lock, flags);\n\t\treturn NETDEV_TX_BUSY;\n\t}\n\n\ti = priv->tx_bd_w_index;\n\tpriv->tx_skb[i] = skb;\n\tlen = skb->len;\n\n\tskb_paddr = dma_map_single(&priv->pdev->dev, skb->data, skb->len,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (dma_mapping_error(&priv->pdev->dev, skb_paddr)) {\n\t\tpr_err(\"skb DMA mapping error: %pad\\n\", &skb_paddr);\n\t\tret = -ENOMEM;\n\t\tgoto tx_done;\n\t}\n\n\ttxbd = &ps->tx_bd_vbase[i];\n\ttxbd->addr = cpu_to_le32(QTN_HOST_LO32(skb_paddr));\n\ttxbd->addr_h = cpu_to_le32(QTN_HOST_HI32(skb_paddr));\n\n\tinfo = (len & QTN_PCIE_TX_DESC_LEN_MASK) << QTN_PCIE_TX_DESC_LEN_SHIFT;\n\ttxbd->info = cpu_to_le32(info);\n\n\t \n\tdma_wmb();\n\n\t \n\ttxbd_paddr = ps->tx_bd_pbase + i * sizeof(struct qtnf_pearl_tx_bd);\n\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\twritel(QTN_HOST_HI32(txbd_paddr),\n\t       PCIE_HDP_HOST_WR_DESC0_H(ps->pcie_reg_base));\n#endif\n\twritel(QTN_HOST_LO32(txbd_paddr),\n\t       PCIE_HDP_HOST_WR_DESC0(ps->pcie_reg_base));\n\n\tif (++i >= priv->tx_bd_num)\n\t\ti = 0;\n\n\tpriv->tx_bd_w_index = i;\n\ntx_done:\n\tif (ret) {\n\t\tpr_err_ratelimited(\"drop skb\\n\");\n\t\tif (skb->dev)\n\t\t\tskb->dev->stats.tx_dropped++;\n\t\tdev_kfree_skb_any(skb);\n\t}\n\n\tpriv->tx_done_count++;\n\tspin_unlock_irqrestore(&priv->tx_lock, flags);\n\n\tqtnf_pearl_data_tx_reclaim(ps);\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic int qtnf_pcie_data_tx(struct qtnf_bus *bus, struct sk_buff *skb,\n\t\t\t     unsigned int macid, unsigned int vifid)\n{\n\treturn qtnf_pcie_skb_send(bus, skb);\n}\n\nstatic int qtnf_pcie_data_tx_meta(struct qtnf_bus *bus, struct sk_buff *skb,\n\t\t\t\t  unsigned int macid, unsigned int vifid)\n{\n\tstruct qtnf_frame_meta_info *meta;\n\tint tail_need = sizeof(*meta) - skb_tailroom(skb);\n\tint ret;\n\n\tif (tail_need > 0 && pskb_expand_head(skb, 0, tail_need, GFP_ATOMIC)) {\n\t\tskb->dev->stats.tx_dropped++;\n\t\tdev_kfree_skb_any(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\tmeta = skb_put(skb, sizeof(*meta));\n\tmeta->magic_s = HBM_FRAME_META_MAGIC_PATTERN_S;\n\tmeta->magic_e = HBM_FRAME_META_MAGIC_PATTERN_E;\n\tmeta->macid = macid;\n\tmeta->ifidx = vifid;\n\n\tret = qtnf_pcie_skb_send(bus, skb);\n\tif (unlikely(ret == NETDEV_TX_BUSY))\n\t\t__skb_trim(skb, skb->len - sizeof(*meta));\n\n\treturn ret;\n}\n\nstatic irqreturn_t qtnf_pcie_pearl_interrupt(int irq, void *data)\n{\n\tstruct qtnf_bus *bus = (struct qtnf_bus *)data;\n\tstruct qtnf_pcie_pearl_state *ps = get_bus_priv(bus);\n\tstruct qtnf_pcie_bus_priv *priv = &ps->base;\n\tu32 status;\n\n\tpriv->pcie_irq_count++;\n\tstatus = readl(PCIE_HDP_INT_STATUS(ps->pcie_reg_base));\n\n\tqtnf_shm_ipc_irq_handler(&priv->shm_ipc_ep_in);\n\tqtnf_shm_ipc_irq_handler(&priv->shm_ipc_ep_out);\n\n\tif (!(status & ps->pcie_irq_mask))\n\t\tgoto irq_done;\n\n\tif (status & PCIE_HDP_INT_RX_BITS)\n\t\tps->pcie_irq_rx_count++;\n\n\tif (status & PCIE_HDP_INT_TX_BITS)\n\t\tps->pcie_irq_tx_count++;\n\n\tif (status & PCIE_HDP_INT_HHBM_UF)\n\t\tps->pcie_irq_uf_count++;\n\n\tif (status & PCIE_HDP_INT_RX_BITS) {\n\t\tqtnf_dis_rxdone_irq(ps);\n\t\tnapi_schedule(&bus->mux_napi);\n\t}\n\n\tif (status & PCIE_HDP_INT_TX_BITS) {\n\t\tqtnf_dis_txdone_irq(ps);\n\t\ttasklet_hi_schedule(&priv->reclaim_tq);\n\t}\n\nirq_done:\n\t \n\tqtnf_non_posted_write(~0U, PCIE_HDP_INT_STATUS(ps->pcie_reg_base));\n\n\tif (!priv->msi_enabled)\n\t\tqtnf_deassert_intx(ps);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int qtnf_rx_data_ready(struct qtnf_pcie_pearl_state *ps)\n{\n\tu16 index = ps->base.rx_bd_r_index;\n\tstruct qtnf_pearl_rx_bd *rxbd;\n\tu32 descw;\n\n\trxbd = &ps->rx_bd_vbase[index];\n\tdescw = le32_to_cpu(rxbd->info);\n\n\tif (descw & QTN_TXDONE_MASK)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int qtnf_pcie_pearl_rx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct qtnf_bus *bus = container_of(napi, struct qtnf_bus, mux_napi);\n\tstruct qtnf_pcie_pearl_state *ps = get_bus_priv(bus);\n\tstruct qtnf_pcie_bus_priv *priv = &ps->base;\n\tstruct net_device *ndev = NULL;\n\tstruct sk_buff *skb = NULL;\n\tint processed = 0;\n\tstruct qtnf_pearl_rx_bd *rxbd;\n\tdma_addr_t skb_paddr;\n\tint consume;\n\tu32 descw;\n\tu32 psize;\n\tu16 r_idx;\n\tu16 w_idx;\n\tint ret;\n\n\twhile (processed < budget) {\n\t\tif (!qtnf_rx_data_ready(ps))\n\t\t\tgoto rx_out;\n\n\t\tr_idx = priv->rx_bd_r_index;\n\t\trxbd = &ps->rx_bd_vbase[r_idx];\n\t\tdescw = le32_to_cpu(rxbd->info);\n\n\t\tskb = priv->rx_skb[r_idx];\n\t\tpsize = QTN_GET_LEN(descw);\n\t\tconsume = 1;\n\n\t\tif (!(descw & QTN_TXDONE_MASK)) {\n\t\t\tpr_warn(\"skip invalid rxbd[%d]\\n\", r_idx);\n\t\t\tconsume = 0;\n\t\t}\n\n\t\tif (!skb) {\n\t\t\tpr_warn(\"skip missing rx_skb[%d]\\n\", r_idx);\n\t\t\tconsume = 0;\n\t\t}\n\n\t\tif (skb && (skb_tailroom(skb) <  psize)) {\n\t\t\tpr_err(\"skip packet with invalid length: %u > %u\\n\",\n\t\t\t       psize, skb_tailroom(skb));\n\t\t\tconsume = 0;\n\t\t}\n\n\t\tif (skb) {\n\t\t\tskb_paddr = QTN_HOST_ADDR(le32_to_cpu(rxbd->addr_h),\n\t\t\t\t\t\t  le32_to_cpu(rxbd->addr));\n\t\t\tdma_unmap_single(&priv->pdev->dev, skb_paddr,\n\t\t\t\t\t SKB_BUF_SIZE, DMA_FROM_DEVICE);\n\t\t}\n\n\t\tif (consume) {\n\t\t\tskb_put(skb, psize);\n\t\t\tndev = qtnf_classify_skb(bus, skb);\n\t\t\tif (likely(ndev)) {\n\t\t\t\tdev_sw_netstats_rx_add(ndev, skb->len);\n\t\t\t\tskb->protocol = eth_type_trans(skb, ndev);\n\t\t\t\tnapi_gro_receive(napi, skb);\n\t\t\t} else {\n\t\t\t\tpr_debug(\"drop untagged skb\\n\");\n\t\t\t\tbus->mux_dev.stats.rx_dropped++;\n\t\t\t\tdev_kfree_skb_any(skb);\n\t\t\t}\n\t\t} else {\n\t\t\tif (skb) {\n\t\t\t\tbus->mux_dev.stats.rx_dropped++;\n\t\t\t\tdev_kfree_skb_any(skb);\n\t\t\t}\n\t\t}\n\n\t\tpriv->rx_skb[r_idx] = NULL;\n\t\tif (++r_idx >= priv->rx_bd_num)\n\t\t\tr_idx = 0;\n\n\t\tpriv->rx_bd_r_index = r_idx;\n\n\t\t \n\t\tw_idx = priv->rx_bd_w_index;\n\t\twhile (CIRC_SPACE(priv->rx_bd_w_index, priv->rx_bd_r_index,\n\t\t\t\t  priv->rx_bd_num) > 0) {\n\t\t\tif (++w_idx >= priv->rx_bd_num)\n\t\t\t\tw_idx = 0;\n\n\t\t\tret = pearl_skb2rbd_attach(ps, w_idx);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"failed to allocate new rx_skb[%d]\\n\",\n\t\t\t\t       w_idx);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tprocessed++;\n\t}\n\nrx_out:\n\tif (processed < budget) {\n\t\tnapi_complete(napi);\n\t\tqtnf_en_rxdone_irq(ps);\n\t}\n\n\treturn processed;\n}\n\nstatic void\nqtnf_pcie_data_tx_timeout(struct qtnf_bus *bus, struct net_device *ndev)\n{\n\tstruct qtnf_pcie_pearl_state *ps = (void *)get_bus_priv(bus);\n\n\ttasklet_hi_schedule(&ps->base.reclaim_tq);\n}\n\nstatic void qtnf_pcie_data_rx_start(struct qtnf_bus *bus)\n{\n\tstruct qtnf_pcie_pearl_state *ps = (void *)get_bus_priv(bus);\n\n\tqtnf_enable_hdp_irqs(ps);\n\tnapi_enable(&bus->mux_napi);\n}\n\nstatic void qtnf_pcie_data_rx_stop(struct qtnf_bus *bus)\n{\n\tstruct qtnf_pcie_pearl_state *ps = (void *)get_bus_priv(bus);\n\n\tnapi_disable(&bus->mux_napi);\n\tqtnf_disable_hdp_irqs(ps);\n}\n\nstatic void qtnf_pearl_tx_use_meta_info_set(struct qtnf_bus *bus, bool use_meta)\n{\n\tif (use_meta)\n\t\tbus->bus_ops->data_tx = qtnf_pcie_data_tx_meta;\n\telse\n\t\tbus->bus_ops->data_tx = qtnf_pcie_data_tx;\n}\n\nstatic struct qtnf_bus_ops qtnf_pcie_pearl_bus_ops = {\n\t \n\t.control_tx\t= qtnf_pcie_control_tx,\n\n\t \n\t.data_tx\t\t= qtnf_pcie_data_tx,\n\t.data_tx_timeout\t= qtnf_pcie_data_tx_timeout,\n\t.data_tx_use_meta_set\t= qtnf_pearl_tx_use_meta_info_set,\n\t.data_rx_start\t\t= qtnf_pcie_data_rx_start,\n\t.data_rx_stop\t\t= qtnf_pcie_data_rx_stop,\n};\n\nstatic int qtnf_dbg_irq_stats(struct seq_file *s, void *data)\n{\n\tstruct qtnf_bus *bus = dev_get_drvdata(s->private);\n\tstruct qtnf_pcie_pearl_state *ps = get_bus_priv(bus);\n\tu32 reg = readl(PCIE_HDP_INT_EN(ps->pcie_reg_base));\n\tu32 status;\n\n\tseq_printf(s, \"pcie_irq_count(%u)\\n\", ps->base.pcie_irq_count);\n\tseq_printf(s, \"pcie_irq_tx_count(%u)\\n\", ps->pcie_irq_tx_count);\n\tstatus = reg &  PCIE_HDP_INT_TX_BITS;\n\tseq_printf(s, \"pcie_irq_tx_status(%s)\\n\",\n\t\t   (status == PCIE_HDP_INT_TX_BITS) ? \"EN\" : \"DIS\");\n\tseq_printf(s, \"pcie_irq_rx_count(%u)\\n\", ps->pcie_irq_rx_count);\n\tstatus = reg &  PCIE_HDP_INT_RX_BITS;\n\tseq_printf(s, \"pcie_irq_rx_status(%s)\\n\",\n\t\t   (status == PCIE_HDP_INT_RX_BITS) ? \"EN\" : \"DIS\");\n\tseq_printf(s, \"pcie_irq_uf_count(%u)\\n\", ps->pcie_irq_uf_count);\n\tstatus = reg &  PCIE_HDP_INT_HHBM_UF;\n\tseq_printf(s, \"pcie_irq_hhbm_uf_status(%s)\\n\",\n\t\t   (status == PCIE_HDP_INT_HHBM_UF) ? \"EN\" : \"DIS\");\n\n\treturn 0;\n}\n\nstatic int qtnf_dbg_hdp_stats(struct seq_file *s, void *data)\n{\n\tstruct qtnf_bus *bus = dev_get_drvdata(s->private);\n\tstruct qtnf_pcie_pearl_state *ps = get_bus_priv(bus);\n\tstruct qtnf_pcie_bus_priv *priv = &ps->base;\n\n\tseq_printf(s, \"tx_full_count(%u)\\n\", priv->tx_full_count);\n\tseq_printf(s, \"tx_done_count(%u)\\n\", priv->tx_done_count);\n\tseq_printf(s, \"tx_reclaim_done(%u)\\n\", priv->tx_reclaim_done);\n\tseq_printf(s, \"tx_reclaim_req(%u)\\n\", priv->tx_reclaim_req);\n\n\tseq_printf(s, \"tx_bd_r_index(%u)\\n\", priv->tx_bd_r_index);\n\tseq_printf(s, \"tx_bd_p_index(%u)\\n\",\n\t\t   readl(PCIE_HDP_RX0DMA_CNT(ps->pcie_reg_base))\n\t\t\t& (priv->tx_bd_num - 1));\n\tseq_printf(s, \"tx_bd_w_index(%u)\\n\", priv->tx_bd_w_index);\n\tseq_printf(s, \"tx queue len(%u)\\n\",\n\t\t   CIRC_CNT(priv->tx_bd_w_index, priv->tx_bd_r_index,\n\t\t\t    priv->tx_bd_num));\n\n\tseq_printf(s, \"rx_bd_r_index(%u)\\n\", priv->rx_bd_r_index);\n\tseq_printf(s, \"rx_bd_p_index(%u)\\n\",\n\t\t   readl(PCIE_HDP_TX0DMA_CNT(ps->pcie_reg_base))\n\t\t\t& (priv->rx_bd_num - 1));\n\tseq_printf(s, \"rx_bd_w_index(%u)\\n\", priv->rx_bd_w_index);\n\tseq_printf(s, \"rx alloc queue len(%u)\\n\",\n\t\t   CIRC_SPACE(priv->rx_bd_w_index, priv->rx_bd_r_index,\n\t\t\t      priv->rx_bd_num));\n\n\treturn 0;\n}\n\nstatic int qtnf_ep_fw_send(struct pci_dev *pdev, uint32_t size,\n\t\t\t   int blk, const u8 *pblk, const u8 *fw)\n{\n\tstruct qtnf_bus *bus = pci_get_drvdata(pdev);\n\n\tstruct qtnf_pearl_fw_hdr *hdr;\n\tu8 *pdata;\n\n\tint hds = sizeof(*hdr);\n\tstruct sk_buff *skb = NULL;\n\tint len = 0;\n\tint ret;\n\n\tskb = __dev_alloc_skb(QTN_PCIE_FW_BUFSZ, GFP_KERNEL);\n\tif (!skb)\n\t\treturn -ENOMEM;\n\n\tskb->len = QTN_PCIE_FW_BUFSZ;\n\tskb->dev = NULL;\n\n\thdr = (struct qtnf_pearl_fw_hdr *)skb->data;\n\tmemcpy(hdr->boardflg, QTN_PCIE_BOARDFLG, strlen(QTN_PCIE_BOARDFLG));\n\thdr->fwsize = cpu_to_le32(size);\n\thdr->seqnum = cpu_to_le32(blk);\n\n\tif (blk)\n\t\thdr->type = cpu_to_le32(QTN_FW_DSUB);\n\telse\n\t\thdr->type = cpu_to_le32(QTN_FW_DBEGIN);\n\n\tpdata = skb->data + hds;\n\n\tlen = QTN_PCIE_FW_BUFSZ - hds;\n\tif (pblk >= (fw + size - len)) {\n\t\tlen = fw + size - pblk;\n\t\thdr->type = cpu_to_le32(QTN_FW_DEND);\n\t}\n\n\thdr->pktlen = cpu_to_le32(len);\n\tmemcpy(pdata, pblk, len);\n\thdr->crc = cpu_to_le32(~crc32(0, pdata, len));\n\n\tret = qtnf_pcie_skb_send(bus, skb);\n\n\treturn (ret == NETDEV_TX_OK) ? len : 0;\n}\n\nstatic int\nqtnf_ep_fw_load(struct qtnf_pcie_pearl_state *ps, const u8 *fw, u32 fw_size)\n{\n\tint blk_size = QTN_PCIE_FW_BUFSZ - sizeof(struct qtnf_pearl_fw_hdr);\n\tint blk_count = fw_size / blk_size + ((fw_size % blk_size) ? 1 : 0);\n\tconst u8 *pblk = fw;\n\tint threshold = 0;\n\tint blk = 0;\n\tint len;\n\n\tpr_debug(\"FW upload started: fw_addr=0x%p size=%d\\n\", fw, fw_size);\n\n\twhile (blk < blk_count) {\n\t\tif (++threshold > 10000) {\n\t\t\tpr_err(\"FW upload failed: too many retries\\n\");\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\n\t\tlen = qtnf_ep_fw_send(ps->base.pdev, fw_size, blk, pblk, fw);\n\t\tif (len <= 0)\n\t\t\tcontinue;\n\n\t\tif (!((blk + 1) & QTN_PCIE_FW_DLMASK) ||\n\t\t    (blk == (blk_count - 1))) {\n\t\t\tqtnf_set_state(&ps->bda->bda_rc_state,\n\t\t\t\t       QTN_RC_FW_SYNC);\n\t\t\tif (qtnf_poll_state(&ps->bda->bda_ep_state,\n\t\t\t\t\t    QTN_EP_FW_SYNC,\n\t\t\t\t\t    QTN_FW_DL_TIMEOUT_MS)) {\n\t\t\t\tpr_err(\"FW upload failed: SYNC timed out\\n\");\n\t\t\t\treturn -ETIMEDOUT;\n\t\t\t}\n\n\t\t\tqtnf_clear_state(&ps->bda->bda_ep_state,\n\t\t\t\t\t QTN_EP_FW_SYNC);\n\n\t\t\tif (qtnf_is_state(&ps->bda->bda_ep_state,\n\t\t\t\t\t  QTN_EP_FW_RETRY)) {\n\t\t\t\tif (blk == (blk_count - 1)) {\n\t\t\t\t\tint last_round =\n\t\t\t\t\t\tblk_count & QTN_PCIE_FW_DLMASK;\n\t\t\t\t\tblk -= last_round;\n\t\t\t\t\tpblk -= ((last_round - 1) *\n\t\t\t\t\t\tblk_size + len);\n\t\t\t\t} else {\n\t\t\t\t\tblk -= QTN_PCIE_FW_DLMASK;\n\t\t\t\t\tpblk -= QTN_PCIE_FW_DLMASK * blk_size;\n\t\t\t\t}\n\n\t\t\t\tqtnf_clear_state(&ps->bda->bda_ep_state,\n\t\t\t\t\t\t QTN_EP_FW_RETRY);\n\n\t\t\t\tpr_warn(\"FW upload retry: block #%d\\n\", blk);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tqtnf_pearl_data_tx_reclaim(ps);\n\t\t}\n\n\t\tpblk += len;\n\t\tblk++;\n\t}\n\n\tpr_debug(\"FW upload completed: totally sent %d blocks\\n\", blk);\n\treturn 0;\n}\n\nstatic void qtnf_pearl_fw_work_handler(struct work_struct *work)\n{\n\tstruct qtnf_bus *bus = container_of(work, struct qtnf_bus, fw_work);\n\tstruct qtnf_pcie_pearl_state *ps = (void *)get_bus_priv(bus);\n\tu32 state = QTN_RC_FW_LOADRDY | QTN_RC_FW_QLINK;\n\tconst char *fwname = QTN_PCI_PEARL_FW_NAME;\n\tstruct pci_dev *pdev = ps->base.pdev;\n\tconst struct firmware *fw;\n\tint ret;\n\n\tif (ps->base.flashboot) {\n\t\tstate |= QTN_RC_FW_FLASHBOOT;\n\t} else {\n\t\tret = request_firmware(&fw, fwname, &pdev->dev);\n\t\tif (ret < 0) {\n\t\t\tpr_err(\"failed to get firmware %s\\n\", fwname);\n\t\t\tgoto fw_load_exit;\n\t\t}\n\t}\n\n\tqtnf_set_state(&ps->bda->bda_rc_state, state);\n\n\tif (qtnf_poll_state(&ps->bda->bda_ep_state, QTN_EP_FW_LOADRDY,\n\t\t\t    QTN_FW_DL_TIMEOUT_MS)) {\n\t\tpr_err(\"card is not ready\\n\");\n\n\t\tif (!ps->base.flashboot)\n\t\t\trelease_firmware(fw);\n\n\t\tgoto fw_load_exit;\n\t}\n\n\tqtnf_clear_state(&ps->bda->bda_ep_state, QTN_EP_FW_LOADRDY);\n\n\tif (ps->base.flashboot) {\n\t\tpr_info(\"booting firmware from flash\\n\");\n\n\t} else {\n\t\tpr_info(\"starting firmware upload: %s\\n\", fwname);\n\n\t\tret = qtnf_ep_fw_load(ps, fw->data, fw->size);\n\t\trelease_firmware(fw);\n\t\tif (ret) {\n\t\t\tpr_err(\"firmware upload error\\n\");\n\t\t\tgoto fw_load_exit;\n\t\t}\n\t}\n\n\tif (qtnf_poll_state(&ps->bda->bda_ep_state, QTN_EP_FW_DONE,\n\t\t\t    QTN_FW_DL_TIMEOUT_MS)) {\n\t\tpr_err(\"firmware bringup timed out\\n\");\n\t\tgoto fw_load_exit;\n\t}\n\n\tif (qtnf_poll_state(&ps->bda->bda_ep_state,\n\t\t\t    QTN_EP_FW_QLINK_DONE, QTN_FW_QLINK_TIMEOUT_MS)) {\n\t\tpr_err(\"firmware runtime failure\\n\");\n\t\tgoto fw_load_exit;\n\t}\n\n\tpr_info(\"firmware is up and running\\n\");\n\n\tret = qtnf_pcie_fw_boot_done(bus);\n\tif (ret)\n\t\tgoto fw_load_exit;\n\n\tqtnf_debugfs_add_entry(bus, \"hdp_stats\", qtnf_dbg_hdp_stats);\n\tqtnf_debugfs_add_entry(bus, \"irq_stats\", qtnf_dbg_irq_stats);\n\nfw_load_exit:\n\tput_device(&pdev->dev);\n}\n\nstatic void qtnf_pearl_reclaim_tasklet_fn(struct tasklet_struct *t)\n{\n\tstruct qtnf_pcie_pearl_state *ps = from_tasklet(ps, t, base.reclaim_tq);\n\n\tqtnf_pearl_data_tx_reclaim(ps);\n\tqtnf_en_txdone_irq(ps);\n}\n\nstatic u64 qtnf_pearl_dma_mask_get(void)\n{\n#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT\n\treturn DMA_BIT_MASK(64);\n#else\n\treturn DMA_BIT_MASK(32);\n#endif\n}\n\nstatic int qtnf_pcie_pearl_probe(struct qtnf_bus *bus, unsigned int tx_bd_size,\n\t\t\t\t unsigned int rx_bd_size)\n{\n\tstruct qtnf_shm_ipc_int ipc_int;\n\tstruct qtnf_pcie_pearl_state *ps = get_bus_priv(bus);\n\tstruct pci_dev *pdev = ps->base.pdev;\n\tint ret;\n\n\tbus->bus_ops = &qtnf_pcie_pearl_bus_ops;\n\tspin_lock_init(&ps->irq_lock);\n\tINIT_WORK(&bus->fw_work, qtnf_pearl_fw_work_handler);\n\n\tps->pcie_reg_base = ps->base.dmareg_bar;\n\tps->bda = ps->base.epmem_bar;\n\twritel(ps->base.msi_enabled, &ps->bda->bda_rc_msi_enabled);\n\n\tret = qtnf_pcie_pearl_init_xfer(ps, tx_bd_size, rx_bd_size);\n\tif (ret) {\n\t\tpr_err(\"PCIE xfer init failed\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tqtnf_init_hdp_irqs(ps);\n\n\t \n\tqtnf_disable_hdp_irqs(ps);\n\n\tret = devm_request_irq(&pdev->dev, pdev->irq,\n\t\t\t       &qtnf_pcie_pearl_interrupt, 0,\n\t\t\t       \"qtnf_pearl_irq\", (void *)bus);\n\tif (ret) {\n\t\tpr_err(\"failed to request pcie irq %d\\n\", pdev->irq);\n\t\tqtnf_pearl_free_xfer_buffers(ps);\n\t\treturn ret;\n\t}\n\n\ttasklet_setup(&ps->base.reclaim_tq, qtnf_pearl_reclaim_tasklet_fn);\n\tnetif_napi_add_weight(&bus->mux_dev, &bus->mux_napi,\n\t\t\t      qtnf_pcie_pearl_rx_poll, 10);\n\n\tipc_int.fn = qtnf_pcie_pearl_ipc_gen_ep_int;\n\tipc_int.arg = ps;\n\tqtnf_pcie_init_shm_ipc(&ps->base, &ps->bda->bda_shm_reg1,\n\t\t\t       &ps->bda->bda_shm_reg2, &ipc_int);\n\n\treturn 0;\n}\n\nstatic void qtnf_pcie_pearl_remove(struct qtnf_bus *bus)\n{\n\tstruct qtnf_pcie_pearl_state *ps = get_bus_priv(bus);\n\n\tqtnf_pearl_reset_ep(ps);\n\tqtnf_pearl_free_xfer_buffers(ps);\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int qtnf_pcie_pearl_suspend(struct qtnf_bus *bus)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic int qtnf_pcie_pearl_resume(struct qtnf_bus *bus)\n{\n\treturn 0;\n}\n#endif\n\nstruct qtnf_bus *qtnf_pcie_pearl_alloc(struct pci_dev *pdev)\n{\n\tstruct qtnf_bus *bus;\n\tstruct qtnf_pcie_pearl_state *ps;\n\n\tbus = devm_kzalloc(&pdev->dev, sizeof(*bus) + sizeof(*ps), GFP_KERNEL);\n\tif (!bus)\n\t\treturn NULL;\n\n\tps = get_bus_priv(bus);\n\tps->base.probe_cb = qtnf_pcie_pearl_probe;\n\tps->base.remove_cb = qtnf_pcie_pearl_remove;\n\tps->base.dma_mask_get_cb = qtnf_pearl_dma_mask_get;\n#ifdef CONFIG_PM_SLEEP\n\tps->base.resume_cb = qtnf_pcie_pearl_resume;\n\tps->base.suspend_cb = qtnf_pcie_pearl_suspend;\n#endif\n\n\treturn bus;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}