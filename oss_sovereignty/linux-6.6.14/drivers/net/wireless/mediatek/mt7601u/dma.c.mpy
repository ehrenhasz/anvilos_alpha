{
  "module_name": "dma.c",
  "hash_id": "996dd77783c31da8300a23f50a503df725903dc6e453a4d7aeb01fd7cb3da15b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wireless/mediatek/mt7601u/dma.c",
  "human_readable_source": "\n \n\n#include \"mt7601u.h\"\n#include \"dma.h\"\n#include \"usb.h\"\n#include \"trace.h\"\n\nstatic int mt7601u_submit_rx_buf(struct mt7601u_dev *dev,\n\t\t\t\t struct mt7601u_dma_buf_rx *e, gfp_t gfp);\n\nstatic unsigned int ieee80211_get_hdrlen_from_buf(const u8 *data, unsigned len)\n{\n\tconst struct ieee80211_hdr *hdr = (const struct ieee80211_hdr *)data;\n\tunsigned int hdrlen;\n\n\tif (unlikely(len < 10))\n\t\treturn 0;\n\thdrlen = ieee80211_hdrlen(hdr->frame_control);\n\tif (unlikely(hdrlen > len))\n\t\treturn 0;\n\treturn hdrlen;\n}\n\nstatic struct sk_buff *\nmt7601u_rx_skb_from_seg(struct mt7601u_dev *dev, struct mt7601u_rxwi *rxwi,\n\t\t\tvoid *data, u32 seg_len, u32 truesize, struct page *p)\n{\n\tstruct sk_buff *skb;\n\tu32 true_len, hdr_len = 0, copy, frag;\n\n\tskb = alloc_skb(p ? 128 : seg_len, GFP_ATOMIC);\n\tif (!skb)\n\t\treturn NULL;\n\n\ttrue_len = mt76_mac_process_rx(dev, skb, data, rxwi);\n\tif (!true_len || true_len > seg_len)\n\t\tgoto bad_frame;\n\n\thdr_len = ieee80211_get_hdrlen_from_buf(data, true_len);\n\tif (!hdr_len)\n\t\tgoto bad_frame;\n\n\tif (rxwi->rxinfo & cpu_to_le32(MT_RXINFO_L2PAD)) {\n\t\tskb_put_data(skb, data, hdr_len);\n\n\t\tdata += hdr_len + 2;\n\t\ttrue_len -= hdr_len;\n\t\thdr_len = 0;\n\t}\n\n\t \n\tcopy = (true_len <= skb_tailroom(skb)) ? true_len : hdr_len + 8;\n\tfrag = true_len - copy;\n\n\tskb_put_data(skb, data, copy);\n\tdata += copy;\n\n\tif (frag) {\n\t\tskb_add_rx_frag(skb, 0, p, data - page_address(p),\n\t\t\t\tfrag, truesize);\n\t\tget_page(p);\n\t}\n\n\treturn skb;\n\nbad_frame:\n\tdev_err_ratelimited(dev->dev, \"Error: incorrect frame len:%u hdr:%u\\n\",\n\t\t\t    true_len, hdr_len);\n\tdev_kfree_skb(skb);\n\treturn NULL;\n}\n\nstatic void mt7601u_rx_process_seg(struct mt7601u_dev *dev, u8 *data,\n\t\t\t\t   u32 seg_len, struct page *p,\n\t\t\t\t   struct list_head *list)\n{\n\tstruct sk_buff *skb;\n\tstruct mt7601u_rxwi *rxwi;\n\tu32 fce_info, truesize = seg_len;\n\n\t \n\tfce_info = get_unaligned_le32(data + seg_len - MT_FCE_INFO_LEN);\n\tseg_len -= MT_FCE_INFO_LEN;\n\n\tdata += MT_DMA_HDR_LEN;\n\tseg_len -= MT_DMA_HDR_LEN;\n\n\trxwi = (struct mt7601u_rxwi *) data;\n\tdata += sizeof(struct mt7601u_rxwi);\n\tseg_len -= sizeof(struct mt7601u_rxwi);\n\n\tif (unlikely(rxwi->zero[0] || rxwi->zero[1] || rxwi->zero[2]))\n\t\tdev_err_once(dev->dev, \"Error: RXWI zero fields are set\\n\");\n\tif (unlikely(FIELD_GET(MT_RXD_INFO_TYPE, fce_info)))\n\t\tdev_err_once(dev->dev, \"Error: RX path seen a non-pkt urb\\n\");\n\n\ttrace_mt_rx(dev, rxwi, fce_info);\n\n\tskb = mt7601u_rx_skb_from_seg(dev, rxwi, data, seg_len, truesize, p);\n\tif (!skb)\n\t\treturn;\n\n\tlocal_bh_disable();\n\trcu_read_lock();\n\n\tieee80211_rx_list(dev->hw, NULL, skb, list);\n\n\trcu_read_unlock();\n\tlocal_bh_enable();\n}\n\nstatic u16 mt7601u_rx_next_seg_len(u8 *data, u32 data_len)\n{\n\tu32 min_seg_len = MT_DMA_HDR_LEN + MT_RX_INFO_LEN +\n\t\tsizeof(struct mt7601u_rxwi) + MT_FCE_INFO_LEN;\n\tu16 dma_len = get_unaligned_le16(data);\n\n\tif (data_len < min_seg_len ||\n\t    WARN_ON_ONCE(!dma_len) ||\n\t    WARN_ON_ONCE(dma_len + MT_DMA_HDRS > data_len) ||\n\t    WARN_ON_ONCE(dma_len & 0x3) ||\n\t    WARN_ON_ONCE(dma_len < min_seg_len))\n\t\treturn 0;\n\n\treturn MT_DMA_HDRS + dma_len;\n}\n\nstatic void\nmt7601u_rx_process_entry(struct mt7601u_dev *dev, struct mt7601u_dma_buf_rx *e)\n{\n\tu32 seg_len, data_len = e->urb->actual_length;\n\tu8 *data = page_address(e->p);\n\tstruct page *new_p = NULL;\n\tLIST_HEAD(list);\n\tint cnt = 0;\n\n\tif (!test_bit(MT7601U_STATE_INITIALIZED, &dev->state))\n\t\treturn;\n\n\t \n\tif (data_len > 512)\n\t\tnew_p = dev_alloc_pages(MT_RX_ORDER);\n\n\twhile ((seg_len = mt7601u_rx_next_seg_len(data, data_len))) {\n\t\tmt7601u_rx_process_seg(dev, data, seg_len,\n\t\t\t\t       new_p ? e->p : NULL, &list);\n\n\t\tdata_len -= seg_len;\n\t\tdata += seg_len;\n\t\tcnt++;\n\t}\n\n\tif (cnt > 1)\n\t\ttrace_mt_rx_dma_aggr(dev, cnt, !!new_p);\n\n\tnetif_receive_skb_list(&list);\n\n\tif (new_p) {\n\t\t \n\t\tput_page(e->p);\n\t\te->p = new_p;\n\t}\n}\n\nstatic struct mt7601u_dma_buf_rx *\nmt7601u_rx_get_pending_entry(struct mt7601u_dev *dev)\n{\n\tstruct mt7601u_rx_queue *q = &dev->rx_q;\n\tstruct mt7601u_dma_buf_rx *buf = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dev->rx_lock, flags);\n\n\tif (!q->pending)\n\t\tgoto out;\n\n\tbuf = &q->e[q->start];\n\tq->pending--;\n\tq->start = (q->start + 1) % q->entries;\nout:\n\tspin_unlock_irqrestore(&dev->rx_lock, flags);\n\n\treturn buf;\n}\n\nstatic void mt7601u_complete_rx(struct urb *urb)\n{\n\tstruct mt7601u_dev *dev = urb->context;\n\tstruct mt7601u_rx_queue *q = &dev->rx_q;\n\tunsigned long flags;\n\n\t \n\tswitch (urb->status) {\n\tcase -ECONNRESET:\n\tcase -ESHUTDOWN:\n\tcase -ENOENT:\n\tcase -EPROTO:\n\t\treturn;\n\tdefault:\n\t\tdev_err_ratelimited(dev->dev, \"rx urb failed: %d\\n\",\n\t\t\t\t    urb->status);\n\t\tfallthrough;\n\tcase 0:\n\t\tbreak;\n\t}\n\n\tspin_lock_irqsave(&dev->rx_lock, flags);\n\tif (WARN_ONCE(q->e[q->end].urb != urb, \"RX urb mismatch\"))\n\t\tgoto out;\n\n\tq->end = (q->end + 1) % q->entries;\n\tq->pending++;\n\ttasklet_schedule(&dev->rx_tasklet);\nout:\n\tspin_unlock_irqrestore(&dev->rx_lock, flags);\n}\n\nstatic void mt7601u_rx_tasklet(struct tasklet_struct *t)\n{\n\tstruct mt7601u_dev *dev = from_tasklet(dev, t, rx_tasklet);\n\tstruct mt7601u_dma_buf_rx *e;\n\n\twhile ((e = mt7601u_rx_get_pending_entry(dev))) {\n\t\tif (e->urb->status)\n\t\t\tcontinue;\n\n\t\tmt7601u_rx_process_entry(dev, e);\n\t\tmt7601u_submit_rx_buf(dev, e, GFP_ATOMIC);\n\t}\n}\n\nstatic void mt7601u_complete_tx(struct urb *urb)\n{\n\tstruct mt7601u_tx_queue *q = urb->context;\n\tstruct mt7601u_dev *dev = q->dev;\n\tstruct sk_buff *skb;\n\tunsigned long flags;\n\n\tswitch (urb->status) {\n\tcase -ECONNRESET:\n\tcase -ESHUTDOWN:\n\tcase -ENOENT:\n\tcase -EPROTO:\n\t\treturn;\n\tdefault:\n\t\tdev_err_ratelimited(dev->dev, \"tx urb failed: %d\\n\",\n\t\t\t\t    urb->status);\n\t\tfallthrough;\n\tcase 0:\n\t\tbreak;\n\t}\n\n\tspin_lock_irqsave(&dev->tx_lock, flags);\n\tif (WARN_ONCE(q->e[q->start].urb != urb, \"TX urb mismatch\"))\n\t\tgoto out;\n\n\tskb = q->e[q->start].skb;\n\tq->e[q->start].skb = NULL;\n\ttrace_mt_tx_dma_done(dev, skb);\n\n\t__skb_queue_tail(&dev->tx_skb_done, skb);\n\ttasklet_schedule(&dev->tx_tasklet);\n\n\tif (q->used == q->entries - q->entries / 8)\n\t\tieee80211_wake_queue(dev->hw, skb_get_queue_mapping(skb));\n\n\tq->start = (q->start + 1) % q->entries;\n\tq->used--;\nout:\n\tspin_unlock_irqrestore(&dev->tx_lock, flags);\n}\n\nstatic void mt7601u_tx_tasklet(struct tasklet_struct *t)\n{\n\tstruct mt7601u_dev *dev = from_tasklet(dev, t, tx_tasklet);\n\tstruct sk_buff_head skbs;\n\tunsigned long flags;\n\n\t__skb_queue_head_init(&skbs);\n\n\tspin_lock_irqsave(&dev->tx_lock, flags);\n\n\tset_bit(MT7601U_STATE_MORE_STATS, &dev->state);\n\tif (!test_and_set_bit(MT7601U_STATE_READING_STATS, &dev->state))\n\t\tqueue_delayed_work(dev->stat_wq, &dev->stat_work,\n\t\t\t\t   msecs_to_jiffies(10));\n\n\tskb_queue_splice_init(&dev->tx_skb_done, &skbs);\n\n\tspin_unlock_irqrestore(&dev->tx_lock, flags);\n\n\twhile (!skb_queue_empty(&skbs)) {\n\t\tstruct sk_buff *skb = __skb_dequeue(&skbs);\n\n\t\tmt7601u_tx_status(dev, skb);\n\t}\n}\n\nstatic int mt7601u_dma_submit_tx(struct mt7601u_dev *dev,\n\t\t\t\t struct sk_buff *skb, u8 ep)\n{\n\tstruct usb_device *usb_dev = mt7601u_to_usb_dev(dev);\n\tunsigned snd_pipe = usb_sndbulkpipe(usb_dev, dev->out_eps[ep]);\n\tstruct mt7601u_dma_buf_tx *e;\n\tstruct mt7601u_tx_queue *q = &dev->tx_q[ep];\n\tunsigned long flags;\n\tint ret;\n\n\tspin_lock_irqsave(&dev->tx_lock, flags);\n\n\tif (WARN_ON(q->entries <= q->used)) {\n\t\tret = -ENOSPC;\n\t\tgoto out;\n\t}\n\n\te = &q->e[q->end];\n\tusb_fill_bulk_urb(e->urb, usb_dev, snd_pipe, skb->data, skb->len,\n\t\t\t  mt7601u_complete_tx, q);\n\tret = usb_submit_urb(e->urb, GFP_ATOMIC);\n\tif (ret) {\n\t\t \n\t\tif (ret == -ENODEV)\n\t\t\tset_bit(MT7601U_STATE_REMOVED, &dev->state);\n\t\telse\n\t\t\tdev_err(dev->dev, \"Error: TX urb submit failed:%d\\n\",\n\t\t\t\tret);\n\t\tgoto out;\n\t}\n\n\tq->end = (q->end + 1) % q->entries;\n\tq->used++;\n\te->skb = skb;\n\n\tif (q->used >= q->entries)\n\t\tieee80211_stop_queue(dev->hw, skb_get_queue_mapping(skb));\nout:\n\tspin_unlock_irqrestore(&dev->tx_lock, flags);\n\n\treturn ret;\n}\n\n \nstatic u8 q2ep(u8 qid)\n{\n\t \n\treturn qid + 1;\n}\n\n \nstatic enum mt76_qsel ep2dmaq(u8 ep)\n{\n\tif (ep == 5)\n\t\treturn MT_QSEL_MGMT;\n\treturn MT_QSEL_EDCA;\n}\n\nint mt7601u_dma_enqueue_tx(struct mt7601u_dev *dev, struct sk_buff *skb,\n\t\t\t   struct mt76_wcid *wcid, int hw_q)\n{\n\tu8 ep = q2ep(hw_q);\n\tu32 dma_flags;\n\tint ret;\n\n\tdma_flags = MT_TXD_PKT_INFO_80211;\n\tif (wcid->hw_key_idx == 0xff)\n\t\tdma_flags |= MT_TXD_PKT_INFO_WIV;\n\n\tret = mt7601u_dma_skb_wrap_pkt(skb, ep2dmaq(ep), dma_flags);\n\tif (ret)\n\t\treturn ret;\n\n\tret = mt7601u_dma_submit_tx(dev, skb, ep);\n\tif (ret) {\n\t\tieee80211_free_txskb(dev->hw, skb);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void mt7601u_kill_rx(struct mt7601u_dev *dev)\n{\n\tint i;\n\n\tfor (i = 0; i < dev->rx_q.entries; i++)\n\t\tusb_poison_urb(dev->rx_q.e[i].urb);\n}\n\nstatic int mt7601u_submit_rx_buf(struct mt7601u_dev *dev,\n\t\t\t\t struct mt7601u_dma_buf_rx *e, gfp_t gfp)\n{\n\tstruct usb_device *usb_dev = mt7601u_to_usb_dev(dev);\n\tu8 *buf = page_address(e->p);\n\tunsigned pipe;\n\tint ret;\n\n\tpipe = usb_rcvbulkpipe(usb_dev, dev->in_eps[MT_EP_IN_PKT_RX]);\n\n\tusb_fill_bulk_urb(e->urb, usb_dev, pipe, buf, MT_RX_URB_SIZE,\n\t\t\t  mt7601u_complete_rx, dev);\n\n\ttrace_mt_submit_urb(dev, e->urb);\n\tret = usb_submit_urb(e->urb, gfp);\n\tif (ret)\n\t\tdev_err(dev->dev, \"Error: submit RX URB failed:%d\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int mt7601u_submit_rx(struct mt7601u_dev *dev)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < dev->rx_q.entries; i++) {\n\t\tret = mt7601u_submit_rx_buf(dev, &dev->rx_q.e[i], GFP_KERNEL);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void mt7601u_free_rx(struct mt7601u_dev *dev)\n{\n\tint i;\n\n\tfor (i = 0; i < dev->rx_q.entries; i++) {\n\t\t__free_pages(dev->rx_q.e[i].p, MT_RX_ORDER);\n\t\tusb_free_urb(dev->rx_q.e[i].urb);\n\t}\n}\n\nstatic int mt7601u_alloc_rx(struct mt7601u_dev *dev)\n{\n\tint i;\n\n\tmemset(&dev->rx_q, 0, sizeof(dev->rx_q));\n\tdev->rx_q.dev = dev;\n\tdev->rx_q.entries = N_RX_ENTRIES;\n\n\tfor (i = 0; i < N_RX_ENTRIES; i++) {\n\t\tdev->rx_q.e[i].urb = usb_alloc_urb(0, GFP_KERNEL);\n\t\tdev->rx_q.e[i].p = dev_alloc_pages(MT_RX_ORDER);\n\n\t\tif (!dev->rx_q.e[i].urb || !dev->rx_q.e[i].p)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void mt7601u_free_tx_queue(struct mt7601u_tx_queue *q)\n{\n\tint i;\n\n\tfor (i = 0; i < q->entries; i++)  {\n\t\tusb_poison_urb(q->e[i].urb);\n\t\tif (q->e[i].skb)\n\t\t\tmt7601u_tx_status(q->dev, q->e[i].skb);\n\t\tusb_free_urb(q->e[i].urb);\n\t}\n}\n\nstatic void mt7601u_free_tx(struct mt7601u_dev *dev)\n{\n\tint i;\n\n\tif (!dev->tx_q)\n\t\treturn;\n\n\tfor (i = 0; i < __MT_EP_OUT_MAX; i++)\n\t\tmt7601u_free_tx_queue(&dev->tx_q[i]);\n}\n\nstatic int mt7601u_alloc_tx_queue(struct mt7601u_dev *dev,\n\t\t\t\t  struct mt7601u_tx_queue *q)\n{\n\tint i;\n\n\tq->dev = dev;\n\tq->entries = N_TX_ENTRIES;\n\n\tfor (i = 0; i < N_TX_ENTRIES; i++) {\n\t\tq->e[i].urb = usb_alloc_urb(0, GFP_KERNEL);\n\t\tif (!q->e[i].urb)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic int mt7601u_alloc_tx(struct mt7601u_dev *dev)\n{\n\tint i;\n\n\tdev->tx_q = devm_kcalloc(dev->dev, __MT_EP_OUT_MAX,\n\t\t\t\t sizeof(*dev->tx_q), GFP_KERNEL);\n\tif (!dev->tx_q)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < __MT_EP_OUT_MAX; i++)\n\t\tif (mt7601u_alloc_tx_queue(dev, &dev->tx_q[i]))\n\t\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nint mt7601u_dma_init(struct mt7601u_dev *dev)\n{\n\tint ret;\n\n\ttasklet_setup(&dev->tx_tasklet, mt7601u_tx_tasklet);\n\ttasklet_setup(&dev->rx_tasklet, mt7601u_rx_tasklet);\n\n\tret = mt7601u_alloc_tx(dev);\n\tif (ret)\n\t\tgoto err;\n\tret = mt7601u_alloc_rx(dev);\n\tif (ret)\n\t\tgoto err;\n\n\tret = mt7601u_submit_rx(dev);\n\tif (ret)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tmt7601u_dma_cleanup(dev);\n\treturn ret;\n}\n\nvoid mt7601u_dma_cleanup(struct mt7601u_dev *dev)\n{\n\tmt7601u_kill_rx(dev);\n\n\ttasklet_kill(&dev->rx_tasklet);\n\n\tmt7601u_free_rx(dev);\n\tmt7601u_free_tx(dev);\n\n\ttasklet_kill(&dev->tx_tasklet);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}