{
  "module_name": "tx.c",
  "hash_id": "61e85cebfa4c9086537230245a5b411b307b7f2823e1398c00406af104cb690a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wireless/mediatek/mt76/tx.c",
  "human_readable_source": "\n \n\n#include \"mt76.h\"\n\nstatic int\nmt76_txq_get_qid(struct ieee80211_txq *txq)\n{\n\tif (!txq->sta)\n\t\treturn MT_TXQ_BE;\n\n\treturn txq->ac;\n}\n\nvoid\nmt76_tx_check_agg_ssn(struct ieee80211_sta *sta, struct sk_buff *skb)\n{\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data;\n\tstruct ieee80211_txq *txq;\n\tstruct mt76_txq *mtxq;\n\tu8 tid;\n\n\tif (!sta || !ieee80211_is_data_qos(hdr->frame_control) ||\n\t    !ieee80211_is_data_present(hdr->frame_control))\n\t\treturn;\n\n\ttid = skb->priority & IEEE80211_QOS_CTL_TAG1D_MASK;\n\ttxq = sta->txq[tid];\n\tmtxq = (struct mt76_txq *)txq->drv_priv;\n\tif (!mtxq->aggr)\n\t\treturn;\n\n\tmtxq->agg_ssn = le16_to_cpu(hdr->seq_ctrl) + 0x10;\n}\nEXPORT_SYMBOL_GPL(mt76_tx_check_agg_ssn);\n\nvoid\nmt76_tx_status_lock(struct mt76_dev *dev, struct sk_buff_head *list)\n\t\t   __acquires(&dev->status_lock)\n{\n\t__skb_queue_head_init(list);\n\tspin_lock_bh(&dev->status_lock);\n}\nEXPORT_SYMBOL_GPL(mt76_tx_status_lock);\n\nvoid\nmt76_tx_status_unlock(struct mt76_dev *dev, struct sk_buff_head *list)\n\t\t      __releases(&dev->status_lock)\n{\n\tstruct ieee80211_hw *hw;\n\tstruct sk_buff *skb;\n\n\tspin_unlock_bh(&dev->status_lock);\n\n\trcu_read_lock();\n\twhile ((skb = __skb_dequeue(list)) != NULL) {\n\t\tstruct ieee80211_tx_status status = {\n\t\t\t.skb = skb,\n\t\t\t.info = IEEE80211_SKB_CB(skb),\n\t\t};\n\t\tstruct ieee80211_rate_status rs = {};\n\t\tstruct mt76_tx_cb *cb = mt76_tx_skb_cb(skb);\n\t\tstruct mt76_wcid *wcid;\n\n\t\twcid = rcu_dereference(dev->wcid[cb->wcid]);\n\t\tif (wcid) {\n\t\t\tstatus.sta = wcid_to_sta(wcid);\n\t\t\tif (status.sta && (wcid->rate.flags || wcid->rate.legacy)) {\n\t\t\t\trs.rate_idx = wcid->rate;\n\t\t\t\tstatus.rates = &rs;\n\t\t\t\tstatus.n_rates = 1;\n\t\t\t} else {\n\t\t\t\tstatus.n_rates = 0;\n\t\t\t}\n\t\t}\n\n\t\thw = mt76_tx_status_get_hw(dev, skb);\n\t\tspin_lock_bh(&dev->rx_lock);\n\t\tieee80211_tx_status_ext(hw, &status);\n\t\tspin_unlock_bh(&dev->rx_lock);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(mt76_tx_status_unlock);\n\nstatic void\n__mt76_tx_status_skb_done(struct mt76_dev *dev, struct sk_buff *skb, u8 flags,\n\t\t\t  struct sk_buff_head *list)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct mt76_tx_cb *cb = mt76_tx_skb_cb(skb);\n\tu8 done = MT_TX_CB_DMA_DONE | MT_TX_CB_TXS_DONE;\n\n\tflags |= cb->flags;\n\tcb->flags = flags;\n\n\tif ((flags & done) != done)\n\t\treturn;\n\n\t \n\tif (flags & MT_TX_CB_TXS_FAILED) {\n\t\tinfo->status.rates[0].count = 0;\n\t\tinfo->status.rates[0].idx = -1;\n\t\tinfo->flags |= IEEE80211_TX_STAT_ACK;\n\t}\n\n\t__skb_queue_tail(list, skb);\n}\n\nvoid\nmt76_tx_status_skb_done(struct mt76_dev *dev, struct sk_buff *skb,\n\t\t\tstruct sk_buff_head *list)\n{\n\t__mt76_tx_status_skb_done(dev, skb, MT_TX_CB_TXS_DONE, list);\n}\nEXPORT_SYMBOL_GPL(mt76_tx_status_skb_done);\n\nint\nmt76_tx_status_skb_add(struct mt76_dev *dev, struct mt76_wcid *wcid,\n\t\t       struct sk_buff *skb)\n{\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct mt76_tx_cb *cb = mt76_tx_skb_cb(skb);\n\tint pid;\n\n\tmemset(cb, 0, sizeof(*cb));\n\n\tif (!wcid || !rcu_access_pointer(dev->wcid[wcid->idx]))\n\t\treturn MT_PACKET_ID_NO_ACK;\n\n\tif (info->flags & IEEE80211_TX_CTL_NO_ACK)\n\t\treturn MT_PACKET_ID_NO_ACK;\n\n\tif (!(info->flags & (IEEE80211_TX_CTL_REQ_TX_STATUS |\n\t\t\t     IEEE80211_TX_CTL_RATE_CTRL_PROBE))) {\n\t\tif (mtk_wed_device_active(&dev->mmio.wed) &&\n\t\t    ((info->flags & IEEE80211_TX_CTL_HW_80211_ENCAP) ||\n\t\t     ieee80211_is_data(hdr->frame_control)))\n\t\t\treturn MT_PACKET_ID_WED;\n\n\t\treturn MT_PACKET_ID_NO_SKB;\n\t}\n\n\tspin_lock_bh(&dev->status_lock);\n\n\tpid = idr_alloc(&wcid->pktid, skb, MT_PACKET_ID_FIRST,\n\t\t\tMT_PACKET_ID_MASK, GFP_ATOMIC);\n\tif (pid < 0) {\n\t\tpid = MT_PACKET_ID_NO_SKB;\n\t\tgoto out;\n\t}\n\n\tcb->wcid = wcid->idx;\n\tcb->pktid = pid;\n\n\tif (list_empty(&wcid->list))\n\t\tlist_add_tail(&wcid->list, &dev->wcid_list);\n\nout:\n\tspin_unlock_bh(&dev->status_lock);\n\n\treturn pid;\n}\nEXPORT_SYMBOL_GPL(mt76_tx_status_skb_add);\n\nstruct sk_buff *\nmt76_tx_status_skb_get(struct mt76_dev *dev, struct mt76_wcid *wcid, int pktid,\n\t\t       struct sk_buff_head *list)\n{\n\tstruct sk_buff *skb;\n\tint id;\n\n\tlockdep_assert_held(&dev->status_lock);\n\n\tskb = idr_remove(&wcid->pktid, pktid);\n\tif (skb)\n\t\tgoto out;\n\n\t \n\tidr_for_each_entry(&wcid->pktid, skb, id) {\n\t\tstruct mt76_tx_cb *cb = mt76_tx_skb_cb(skb);\n\n\t\tif (pktid >= 0) {\n\t\t\tif (!(cb->flags & MT_TX_CB_DMA_DONE))\n\t\t\t\tcontinue;\n\n\t\t\tif (time_is_after_jiffies(cb->jiffies +\n\t\t\t\t\t\t   MT_TX_STATUS_SKB_TIMEOUT))\n\t\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tidr_remove(&wcid->pktid, cb->pktid);\n\t\t__mt76_tx_status_skb_done(dev, skb, MT_TX_CB_TXS_FAILED |\n\t\t\t\t\t\t    MT_TX_CB_TXS_DONE, list);\n\t}\n\nout:\n\tif (idr_is_empty(&wcid->pktid))\n\t\tlist_del_init(&wcid->list);\n\n\treturn skb;\n}\nEXPORT_SYMBOL_GPL(mt76_tx_status_skb_get);\n\nvoid\nmt76_tx_status_check(struct mt76_dev *dev, bool flush)\n{\n\tstruct mt76_wcid *wcid, *tmp;\n\tstruct sk_buff_head list;\n\n\tmt76_tx_status_lock(dev, &list);\n\tlist_for_each_entry_safe(wcid, tmp, &dev->wcid_list, list)\n\t\tmt76_tx_status_skb_get(dev, wcid, flush ? -1 : 0, &list);\n\tmt76_tx_status_unlock(dev, &list);\n}\nEXPORT_SYMBOL_GPL(mt76_tx_status_check);\n\nstatic void\nmt76_tx_check_non_aql(struct mt76_dev *dev, struct mt76_wcid *wcid,\n\t\t      struct sk_buff *skb)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tint pending;\n\n\tif (!wcid || info->tx_time_est)\n\t\treturn;\n\n\tpending = atomic_dec_return(&wcid->non_aql_packets);\n\tif (pending < 0)\n\t\tatomic_cmpxchg(&wcid->non_aql_packets, pending, 0);\n}\n\nvoid __mt76_tx_complete_skb(struct mt76_dev *dev, u16 wcid_idx, struct sk_buff *skb,\n\t\t\t    struct list_head *free_list)\n{\n\tstruct mt76_tx_cb *cb = mt76_tx_skb_cb(skb);\n\tstruct ieee80211_tx_status status = {\n\t\t.skb = skb,\n\t\t.free_list = free_list,\n\t};\n\tstruct mt76_wcid *wcid = NULL;\n\tstruct ieee80211_hw *hw;\n\tstruct sk_buff_head list;\n\n\trcu_read_lock();\n\n\tif (wcid_idx < ARRAY_SIZE(dev->wcid))\n\t\twcid = rcu_dereference(dev->wcid[wcid_idx]);\n\n\tmt76_tx_check_non_aql(dev, wcid, skb);\n\n#ifdef CONFIG_NL80211_TESTMODE\n\tif (mt76_is_testmode_skb(dev, skb, &hw)) {\n\t\tstruct mt76_phy *phy = hw->priv;\n\n\t\tif (skb == phy->test.tx_skb)\n\t\t\tphy->test.tx_done++;\n\t\tif (phy->test.tx_queued == phy->test.tx_done)\n\t\t\twake_up(&dev->tx_wait);\n\n\t\tdev_kfree_skb_any(skb);\n\t\tgoto out;\n\t}\n#endif\n\n\tif (cb->pktid < MT_PACKET_ID_FIRST) {\n\t\tstruct ieee80211_rate_status rs = {};\n\n\t\thw = mt76_tx_status_get_hw(dev, skb);\n\t\tstatus.sta = wcid_to_sta(wcid);\n\t\tif (status.sta && (wcid->rate.flags || wcid->rate.legacy)) {\n\t\t\trs.rate_idx = wcid->rate;\n\t\t\tstatus.rates = &rs;\n\t\t\tstatus.n_rates = 1;\n\t\t}\n\t\tspin_lock_bh(&dev->rx_lock);\n\t\tieee80211_tx_status_ext(hw, &status);\n\t\tspin_unlock_bh(&dev->rx_lock);\n\t\tgoto out;\n\t}\n\n\tmt76_tx_status_lock(dev, &list);\n\tcb->jiffies = jiffies;\n\t__mt76_tx_status_skb_done(dev, skb, MT_TX_CB_DMA_DONE, &list);\n\tmt76_tx_status_unlock(dev, &list);\n\nout:\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL_GPL(__mt76_tx_complete_skb);\n\nstatic int\n__mt76_tx_queue_skb(struct mt76_phy *phy, int qid, struct sk_buff *skb,\n\t\t    struct mt76_wcid *wcid, struct ieee80211_sta *sta,\n\t\t    bool *stop)\n{\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct mt76_queue *q = phy->q_tx[qid];\n\tstruct mt76_dev *dev = phy->dev;\n\tbool non_aql;\n\tint pending;\n\tint idx;\n\n\tnon_aql = !info->tx_time_est;\n\tidx = dev->queue_ops->tx_queue_skb(dev, q, qid, skb, wcid, sta);\n\tif (idx < 0 || !sta)\n\t\treturn idx;\n\n\twcid = (struct mt76_wcid *)sta->drv_priv;\n\tq->entry[idx].wcid = wcid->idx;\n\n\tif (!non_aql)\n\t\treturn idx;\n\n\tpending = atomic_inc_return(&wcid->non_aql_packets);\n\tif (stop && pending >= MT_MAX_NON_AQL_PKT)\n\t\t*stop = true;\n\n\treturn idx;\n}\n\nvoid\nmt76_tx(struct mt76_phy *phy, struct ieee80211_sta *sta,\n\tstruct mt76_wcid *wcid, struct sk_buff *skb)\n{\n\tstruct mt76_dev *dev = phy->dev;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\tstruct ieee80211_hdr *hdr = (struct ieee80211_hdr *)skb->data;\n\tstruct mt76_queue *q;\n\tint qid = skb_get_queue_mapping(skb);\n\n\tif (mt76_testmode_enabled(phy)) {\n\t\tieee80211_free_txskb(phy->hw, skb);\n\t\treturn;\n\t}\n\n\tif (WARN_ON(qid >= MT_TXQ_PSD)) {\n\t\tqid = MT_TXQ_BE;\n\t\tskb_set_queue_mapping(skb, qid);\n\t}\n\n\tif ((dev->drv->drv_flags & MT_DRV_HW_MGMT_TXQ) &&\n\t    !(info->flags & IEEE80211_TX_CTL_HW_80211_ENCAP) &&\n\t    !ieee80211_is_data(hdr->frame_control) &&\n\t    !ieee80211_is_bufferable_mmpdu(skb)) {\n\t\tqid = MT_TXQ_PSD;\n\t}\n\n\tif (wcid && !(wcid->tx_info & MT_WCID_TX_INFO_SET))\n\t\tieee80211_get_tx_rates(info->control.vif, sta, skb,\n\t\t\t\t       info->control.rates, 1);\n\n\tinfo->hw_queue |= FIELD_PREP(MT_TX_HW_QUEUE_PHY, phy->band_idx);\n\tq = phy->q_tx[qid];\n\n\tspin_lock_bh(&q->lock);\n\t__mt76_tx_queue_skb(phy, qid, skb, wcid, sta, NULL);\n\tdev->queue_ops->kick(dev, q);\n\tspin_unlock_bh(&q->lock);\n}\nEXPORT_SYMBOL_GPL(mt76_tx);\n\nstatic struct sk_buff *\nmt76_txq_dequeue(struct mt76_phy *phy, struct mt76_txq *mtxq)\n{\n\tstruct ieee80211_txq *txq = mtxq_to_txq(mtxq);\n\tstruct ieee80211_tx_info *info;\n\tstruct sk_buff *skb;\n\n\tskb = ieee80211_tx_dequeue(phy->hw, txq);\n\tif (!skb)\n\t\treturn NULL;\n\n\tinfo = IEEE80211_SKB_CB(skb);\n\tinfo->hw_queue |= FIELD_PREP(MT_TX_HW_QUEUE_PHY, phy->band_idx);\n\n\treturn skb;\n}\n\nstatic void\nmt76_queue_ps_skb(struct mt76_phy *phy, struct ieee80211_sta *sta,\n\t\t  struct sk_buff *skb, bool last)\n{\n\tstruct mt76_wcid *wcid = (struct mt76_wcid *)sta->drv_priv;\n\tstruct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);\n\n\tinfo->control.flags |= IEEE80211_TX_CTRL_PS_RESPONSE;\n\tif (last)\n\t\tinfo->flags |= IEEE80211_TX_STATUS_EOSP |\n\t\t\t       IEEE80211_TX_CTL_REQ_TX_STATUS;\n\n\tmt76_skb_set_moredata(skb, !last);\n\t__mt76_tx_queue_skb(phy, MT_TXQ_PSD, skb, wcid, sta, NULL);\n}\n\nvoid\nmt76_release_buffered_frames(struct ieee80211_hw *hw, struct ieee80211_sta *sta,\n\t\t\t     u16 tids, int nframes,\n\t\t\t     enum ieee80211_frame_release_type reason,\n\t\t\t     bool more_data)\n{\n\tstruct mt76_phy *phy = hw->priv;\n\tstruct mt76_dev *dev = phy->dev;\n\tstruct sk_buff *last_skb = NULL;\n\tstruct mt76_queue *hwq = phy->q_tx[MT_TXQ_PSD];\n\tint i;\n\n\tspin_lock_bh(&hwq->lock);\n\tfor (i = 0; tids && nframes; i++, tids >>= 1) {\n\t\tstruct ieee80211_txq *txq = sta->txq[i];\n\t\tstruct mt76_txq *mtxq = (struct mt76_txq *)txq->drv_priv;\n\t\tstruct sk_buff *skb;\n\n\t\tif (!(tids & 1))\n\t\t\tcontinue;\n\n\t\tdo {\n\t\t\tskb = mt76_txq_dequeue(phy, mtxq);\n\t\t\tif (!skb)\n\t\t\t\tbreak;\n\n\t\t\tnframes--;\n\t\t\tif (last_skb)\n\t\t\t\tmt76_queue_ps_skb(phy, sta, last_skb, false);\n\n\t\t\tlast_skb = skb;\n\t\t} while (nframes);\n\t}\n\n\tif (last_skb) {\n\t\tmt76_queue_ps_skb(phy, sta, last_skb, true);\n\t\tdev->queue_ops->kick(dev, hwq);\n\t} else {\n\t\tieee80211_sta_eosp(sta);\n\t}\n\n\tspin_unlock_bh(&hwq->lock);\n}\nEXPORT_SYMBOL_GPL(mt76_release_buffered_frames);\n\nstatic bool\nmt76_txq_stopped(struct mt76_queue *q)\n{\n\treturn q->stopped || q->blocked ||\n\t       q->queued + MT_TXQ_FREE_THR >= q->ndesc;\n}\n\nstatic int\nmt76_txq_send_burst(struct mt76_phy *phy, struct mt76_queue *q,\n\t\t    struct mt76_txq *mtxq, struct mt76_wcid *wcid)\n{\n\tstruct mt76_dev *dev = phy->dev;\n\tstruct ieee80211_txq *txq = mtxq_to_txq(mtxq);\n\tenum mt76_txq_id qid = mt76_txq_get_qid(txq);\n\tstruct ieee80211_tx_info *info;\n\tstruct sk_buff *skb;\n\tint n_frames = 1;\n\tbool stop = false;\n\tint idx;\n\n\tif (test_bit(MT_WCID_FLAG_PS, &wcid->flags))\n\t\treturn 0;\n\n\tif (atomic_read(&wcid->non_aql_packets) >= MT_MAX_NON_AQL_PKT)\n\t\treturn 0;\n\n\tskb = mt76_txq_dequeue(phy, mtxq);\n\tif (!skb)\n\t\treturn 0;\n\n\tinfo = IEEE80211_SKB_CB(skb);\n\tif (!(wcid->tx_info & MT_WCID_TX_INFO_SET))\n\t\tieee80211_get_tx_rates(txq->vif, txq->sta, skb,\n\t\t\t\t       info->control.rates, 1);\n\n\tspin_lock(&q->lock);\n\tidx = __mt76_tx_queue_skb(phy, qid, skb, wcid, txq->sta, &stop);\n\tspin_unlock(&q->lock);\n\tif (idx < 0)\n\t\treturn idx;\n\n\tdo {\n\t\tif (test_bit(MT76_RESET, &phy->state))\n\t\t\treturn -EBUSY;\n\n\t\tif (stop || mt76_txq_stopped(q))\n\t\t\tbreak;\n\n\t\tskb = mt76_txq_dequeue(phy, mtxq);\n\t\tif (!skb)\n\t\t\tbreak;\n\n\t\tinfo = IEEE80211_SKB_CB(skb);\n\t\tif (!(wcid->tx_info & MT_WCID_TX_INFO_SET))\n\t\t\tieee80211_get_tx_rates(txq->vif, txq->sta, skb,\n\t\t\t\t\t       info->control.rates, 1);\n\n\t\tspin_lock(&q->lock);\n\t\tidx = __mt76_tx_queue_skb(phy, qid, skb, wcid, txq->sta, &stop);\n\t\tspin_unlock(&q->lock);\n\t\tif (idx < 0)\n\t\t\tbreak;\n\n\t\tn_frames++;\n\t} while (1);\n\n\tspin_lock(&q->lock);\n\tdev->queue_ops->kick(dev, q);\n\tspin_unlock(&q->lock);\n\n\treturn n_frames;\n}\n\nstatic int\nmt76_txq_schedule_list(struct mt76_phy *phy, enum mt76_txq_id qid)\n{\n\tstruct mt76_queue *q = phy->q_tx[qid];\n\tstruct mt76_dev *dev = phy->dev;\n\tstruct ieee80211_txq *txq;\n\tstruct mt76_txq *mtxq;\n\tstruct mt76_wcid *wcid;\n\tint ret = 0;\n\n\twhile (1) {\n\t\tint n_frames = 0;\n\n\t\tif (test_bit(MT76_RESET, &phy->state))\n\t\t\treturn -EBUSY;\n\n\t\tif (dev->queue_ops->tx_cleanup &&\n\t\t    q->queued + 2 * MT_TXQ_FREE_THR >= q->ndesc) {\n\t\t\tdev->queue_ops->tx_cleanup(dev, q, false);\n\t\t}\n\n\t\ttxq = ieee80211_next_txq(phy->hw, qid);\n\t\tif (!txq)\n\t\t\tbreak;\n\n\t\tmtxq = (struct mt76_txq *)txq->drv_priv;\n\t\twcid = rcu_dereference(dev->wcid[mtxq->wcid]);\n\t\tif (!wcid || test_bit(MT_WCID_FLAG_PS, &wcid->flags))\n\t\t\tcontinue;\n\n\t\tif (mtxq->send_bar && mtxq->aggr) {\n\t\t\tstruct ieee80211_txq *txq = mtxq_to_txq(mtxq);\n\t\t\tstruct ieee80211_sta *sta = txq->sta;\n\t\t\tstruct ieee80211_vif *vif = txq->vif;\n\t\t\tu16 agg_ssn = mtxq->agg_ssn;\n\t\t\tu8 tid = txq->tid;\n\n\t\t\tmtxq->send_bar = false;\n\t\t\tieee80211_send_bar(vif, sta->addr, tid, agg_ssn);\n\t\t}\n\n\t\tif (!mt76_txq_stopped(q))\n\t\t\tn_frames = mt76_txq_send_burst(phy, q, mtxq, wcid);\n\n\t\tieee80211_return_txq(phy->hw, txq, false);\n\n\t\tif (unlikely(n_frames < 0))\n\t\t\treturn n_frames;\n\n\t\tret += n_frames;\n\t}\n\n\treturn ret;\n}\n\nvoid mt76_txq_schedule(struct mt76_phy *phy, enum mt76_txq_id qid)\n{\n\tint len;\n\n\tif (qid >= 4)\n\t\treturn;\n\n\tlocal_bh_disable();\n\trcu_read_lock();\n\n\tdo {\n\t\tieee80211_txq_schedule_start(phy->hw, qid);\n\t\tlen = mt76_txq_schedule_list(phy, qid);\n\t\tieee80211_txq_schedule_end(phy->hw, qid);\n\t} while (len > 0);\n\n\trcu_read_unlock();\n\tlocal_bh_enable();\n}\nEXPORT_SYMBOL_GPL(mt76_txq_schedule);\n\nvoid mt76_txq_schedule_all(struct mt76_phy *phy)\n{\n\tint i;\n\n\tfor (i = 0; i <= MT_TXQ_BK; i++)\n\t\tmt76_txq_schedule(phy, i);\n}\nEXPORT_SYMBOL_GPL(mt76_txq_schedule_all);\n\nvoid mt76_tx_worker_run(struct mt76_dev *dev)\n{\n\tstruct mt76_phy *phy;\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(dev->phys); i++) {\n\t\tphy = dev->phys[i];\n\t\tif (!phy)\n\t\t\tcontinue;\n\n\t\tmt76_txq_schedule_all(phy);\n\t}\n\n#ifdef CONFIG_NL80211_TESTMODE\n\tfor (i = 0; i < ARRAY_SIZE(dev->phys); i++) {\n\t\tphy = dev->phys[i];\n\t\tif (!phy || !phy->test.tx_pending)\n\t\t\tcontinue;\n\n\t\tmt76_testmode_tx_pending(phy);\n\t}\n#endif\n}\nEXPORT_SYMBOL_GPL(mt76_tx_worker_run);\n\nvoid mt76_tx_worker(struct mt76_worker *w)\n{\n\tstruct mt76_dev *dev = container_of(w, struct mt76_dev, tx_worker);\n\n\tmt76_tx_worker_run(dev);\n}\n\nvoid mt76_stop_tx_queues(struct mt76_phy *phy, struct ieee80211_sta *sta,\n\t\t\t bool send_bar)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(sta->txq); i++) {\n\t\tstruct ieee80211_txq *txq = sta->txq[i];\n\t\tstruct mt76_queue *hwq;\n\t\tstruct mt76_txq *mtxq;\n\n\t\tif (!txq)\n\t\t\tcontinue;\n\n\t\thwq = phy->q_tx[mt76_txq_get_qid(txq)];\n\t\tmtxq = (struct mt76_txq *)txq->drv_priv;\n\n\t\tspin_lock_bh(&hwq->lock);\n\t\tmtxq->send_bar = mtxq->aggr && send_bar;\n\t\tspin_unlock_bh(&hwq->lock);\n\t}\n}\nEXPORT_SYMBOL_GPL(mt76_stop_tx_queues);\n\nvoid mt76_wake_tx_queue(struct ieee80211_hw *hw, struct ieee80211_txq *txq)\n{\n\tstruct mt76_phy *phy = hw->priv;\n\tstruct mt76_dev *dev = phy->dev;\n\n\tif (!test_bit(MT76_STATE_RUNNING, &phy->state))\n\t\treturn;\n\n\tmt76_worker_schedule(&dev->tx_worker);\n}\nEXPORT_SYMBOL_GPL(mt76_wake_tx_queue);\n\nu8 mt76_ac_to_hwq(u8 ac)\n{\n\tstatic const u8 wmm_queue_map[] = {\n\t\t[IEEE80211_AC_BE] = 0,\n\t\t[IEEE80211_AC_BK] = 1,\n\t\t[IEEE80211_AC_VI] = 2,\n\t\t[IEEE80211_AC_VO] = 3,\n\t};\n\n\tif (WARN_ON(ac >= IEEE80211_NUM_ACS))\n\t\treturn 0;\n\n\treturn wmm_queue_map[ac];\n}\nEXPORT_SYMBOL_GPL(mt76_ac_to_hwq);\n\nint mt76_skb_adjust_pad(struct sk_buff *skb, int pad)\n{\n\tstruct sk_buff *iter, *last = skb;\n\n\t \n\tskb_walk_frags(skb, iter) {\n\t\tlast = iter;\n\t\tif (!iter->next) {\n\t\t\tskb->data_len += pad;\n\t\t\tskb->len += pad;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (skb_pad(last, pad))\n\t\treturn -ENOMEM;\n\n\t__skb_put(last, pad);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(mt76_skb_adjust_pad);\n\nvoid mt76_queue_tx_complete(struct mt76_dev *dev, struct mt76_queue *q,\n\t\t\t    struct mt76_queue_entry *e)\n{\n\tif (e->skb)\n\t\tdev->drv->tx_complete_skb(dev, e);\n\n\tspin_lock_bh(&q->lock);\n\tq->tail = (q->tail + 1) % q->ndesc;\n\tq->queued--;\n\tspin_unlock_bh(&q->lock);\n}\nEXPORT_SYMBOL_GPL(mt76_queue_tx_complete);\n\nvoid __mt76_set_tx_blocked(struct mt76_dev *dev, bool blocked)\n{\n\tstruct mt76_phy *phy = &dev->phy;\n\tstruct mt76_queue *q = phy->q_tx[0];\n\n\tif (blocked == q->blocked)\n\t\treturn;\n\n\tq->blocked = blocked;\n\n\tphy = dev->phys[MT_BAND1];\n\tif (phy) {\n\t\tq = phy->q_tx[0];\n\t\tq->blocked = blocked;\n\t}\n\tphy = dev->phys[MT_BAND2];\n\tif (phy) {\n\t\tq = phy->q_tx[0];\n\t\tq->blocked = blocked;\n\t}\n\n\tif (!blocked)\n\t\tmt76_worker_schedule(&dev->tx_worker);\n}\nEXPORT_SYMBOL_GPL(__mt76_set_tx_blocked);\n\nint mt76_token_consume(struct mt76_dev *dev, struct mt76_txwi_cache **ptxwi)\n{\n\tint token;\n\n\tspin_lock_bh(&dev->token_lock);\n\n\ttoken = idr_alloc(&dev->token, *ptxwi, 0, dev->token_size, GFP_ATOMIC);\n\tif (token >= 0)\n\t\tdev->token_count++;\n\n#ifdef CONFIG_NET_MEDIATEK_SOC_WED\n\tif (mtk_wed_device_active(&dev->mmio.wed) &&\n\t    token >= dev->mmio.wed.wlan.token_start)\n\t\tdev->wed_token_count++;\n#endif\n\n\tif (dev->token_count >= dev->token_size - MT76_TOKEN_FREE_THR)\n\t\t__mt76_set_tx_blocked(dev, true);\n\n\tspin_unlock_bh(&dev->token_lock);\n\n\treturn token;\n}\nEXPORT_SYMBOL_GPL(mt76_token_consume);\n\nint mt76_rx_token_consume(struct mt76_dev *dev, void *ptr,\n\t\t\t  struct mt76_txwi_cache *t, dma_addr_t phys)\n{\n\tint token;\n\n\tspin_lock_bh(&dev->rx_token_lock);\n\ttoken = idr_alloc(&dev->rx_token, t, 0, dev->rx_token_size,\n\t\t\t  GFP_ATOMIC);\n\tif (token >= 0) {\n\t\tt->ptr = ptr;\n\t\tt->dma_addr = phys;\n\t}\n\tspin_unlock_bh(&dev->rx_token_lock);\n\n\treturn token;\n}\nEXPORT_SYMBOL_GPL(mt76_rx_token_consume);\n\nstruct mt76_txwi_cache *\nmt76_token_release(struct mt76_dev *dev, int token, bool *wake)\n{\n\tstruct mt76_txwi_cache *txwi;\n\n\tspin_lock_bh(&dev->token_lock);\n\n\ttxwi = idr_remove(&dev->token, token);\n\tif (txwi) {\n\t\tdev->token_count--;\n\n#ifdef CONFIG_NET_MEDIATEK_SOC_WED\n\t\tif (mtk_wed_device_active(&dev->mmio.wed) &&\n\t\t    token >= dev->mmio.wed.wlan.token_start &&\n\t\t    --dev->wed_token_count == 0)\n\t\t\twake_up(&dev->tx_wait);\n#endif\n\t}\n\n\tif (dev->token_count < dev->token_size - MT76_TOKEN_FREE_THR &&\n\t    dev->phy.q_tx[0]->blocked)\n\t\t*wake = true;\n\n\tspin_unlock_bh(&dev->token_lock);\n\n\treturn txwi;\n}\nEXPORT_SYMBOL_GPL(mt76_token_release);\n\nstruct mt76_txwi_cache *\nmt76_rx_token_release(struct mt76_dev *dev, int token)\n{\n\tstruct mt76_txwi_cache *t;\n\n\tspin_lock_bh(&dev->rx_token_lock);\n\tt = idr_remove(&dev->rx_token, token);\n\tspin_unlock_bh(&dev->rx_token_lock);\n\n\treturn t;\n}\nEXPORT_SYMBOL_GPL(mt76_rx_token_release);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}