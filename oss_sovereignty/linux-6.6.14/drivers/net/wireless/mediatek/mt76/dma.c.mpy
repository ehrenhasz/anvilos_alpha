{
  "module_name": "dma.c",
  "hash_id": "d0cf1bcc6edeae9d84fd353f6458d4461681a53668fcc59a8969b2523c120ad5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wireless/mediatek/mt76/dma.c",
  "human_readable_source": "\n \n\n#include <linux/dma-mapping.h>\n#include \"mt76.h\"\n#include \"dma.h\"\n\n#if IS_ENABLED(CONFIG_NET_MEDIATEK_SOC_WED)\n\n#define Q_READ(_dev, _q, _field) ({\t\t\t\t\t\\\n\tu32 _offset = offsetof(struct mt76_queue_regs, _field);\t\t\\\n\tu32 _val;\t\t\t\t\t\t\t\\\n\tif ((_q)->flags & MT_QFLAG_WED)\t\t\t\t\t\\\n\t\t_val = mtk_wed_device_reg_read(&(_dev)->mmio.wed,\t\\\n\t\t\t\t\t       ((_q)->wed_regs +\t\\\n\t\t\t\t\t        _offset));\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\t_val = readl(&(_q)->regs->_field);\t\t\t\\\n\t_val;\t\t\t\t\t\t\t\t\\\n})\n\n#define Q_WRITE(_dev, _q, _field, _val)\tdo {\t\t\t\t\\\n\tu32 _offset = offsetof(struct mt76_queue_regs, _field);\t\t\\\n\tif ((_q)->flags & MT_QFLAG_WED)\t\t\t\t\t\\\n\t\tmtk_wed_device_reg_write(&(_dev)->mmio.wed,\t\t\\\n\t\t\t\t\t ((_q)->wed_regs + _offset),\t\\\n\t\t\t\t\t _val);\t\t\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\twritel(_val, &(_q)->regs->_field);\t\t\t\\\n} while (0)\n\n#else\n\n#define Q_READ(_dev, _q, _field)\treadl(&(_q)->regs->_field)\n#define Q_WRITE(_dev, _q, _field, _val)\twritel(_val, &(_q)->regs->_field)\n\n#endif\n\nstatic struct mt76_txwi_cache *\nmt76_alloc_txwi(struct mt76_dev *dev)\n{\n\tstruct mt76_txwi_cache *t;\n\tdma_addr_t addr;\n\tu8 *txwi;\n\tint size;\n\n\tsize = L1_CACHE_ALIGN(dev->drv->txwi_size + sizeof(*t));\n\ttxwi = kzalloc(size, GFP_ATOMIC);\n\tif (!txwi)\n\t\treturn NULL;\n\n\taddr = dma_map_single(dev->dma_dev, txwi, dev->drv->txwi_size,\n\t\t\t      DMA_TO_DEVICE);\n\tt = (struct mt76_txwi_cache *)(txwi + dev->drv->txwi_size);\n\tt->dma_addr = addr;\n\n\treturn t;\n}\n\nstatic struct mt76_txwi_cache *\nmt76_alloc_rxwi(struct mt76_dev *dev)\n{\n\tstruct mt76_txwi_cache *t;\n\n\tt = kzalloc(L1_CACHE_ALIGN(sizeof(*t)), GFP_ATOMIC);\n\tif (!t)\n\t\treturn NULL;\n\n\tt->ptr = NULL;\n\treturn t;\n}\n\nstatic struct mt76_txwi_cache *\n__mt76_get_txwi(struct mt76_dev *dev)\n{\n\tstruct mt76_txwi_cache *t = NULL;\n\n\tspin_lock(&dev->lock);\n\tif (!list_empty(&dev->txwi_cache)) {\n\t\tt = list_first_entry(&dev->txwi_cache, struct mt76_txwi_cache,\n\t\t\t\t     list);\n\t\tlist_del(&t->list);\n\t}\n\tspin_unlock(&dev->lock);\n\n\treturn t;\n}\n\nstatic struct mt76_txwi_cache *\n__mt76_get_rxwi(struct mt76_dev *dev)\n{\n\tstruct mt76_txwi_cache *t = NULL;\n\n\tspin_lock_bh(&dev->wed_lock);\n\tif (!list_empty(&dev->rxwi_cache)) {\n\t\tt = list_first_entry(&dev->rxwi_cache, struct mt76_txwi_cache,\n\t\t\t\t     list);\n\t\tlist_del(&t->list);\n\t}\n\tspin_unlock_bh(&dev->wed_lock);\n\n\treturn t;\n}\n\nstatic struct mt76_txwi_cache *\nmt76_get_txwi(struct mt76_dev *dev)\n{\n\tstruct mt76_txwi_cache *t = __mt76_get_txwi(dev);\n\n\tif (t)\n\t\treturn t;\n\n\treturn mt76_alloc_txwi(dev);\n}\n\nstruct mt76_txwi_cache *\nmt76_get_rxwi(struct mt76_dev *dev)\n{\n\tstruct mt76_txwi_cache *t = __mt76_get_rxwi(dev);\n\n\tif (t)\n\t\treturn t;\n\n\treturn mt76_alloc_rxwi(dev);\n}\nEXPORT_SYMBOL_GPL(mt76_get_rxwi);\n\nvoid\nmt76_put_txwi(struct mt76_dev *dev, struct mt76_txwi_cache *t)\n{\n\tif (!t)\n\t\treturn;\n\n\tspin_lock(&dev->lock);\n\tlist_add(&t->list, &dev->txwi_cache);\n\tspin_unlock(&dev->lock);\n}\nEXPORT_SYMBOL_GPL(mt76_put_txwi);\n\nvoid\nmt76_put_rxwi(struct mt76_dev *dev, struct mt76_txwi_cache *t)\n{\n\tif (!t)\n\t\treturn;\n\n\tspin_lock_bh(&dev->wed_lock);\n\tlist_add(&t->list, &dev->rxwi_cache);\n\tspin_unlock_bh(&dev->wed_lock);\n}\nEXPORT_SYMBOL_GPL(mt76_put_rxwi);\n\nstatic void\nmt76_free_pending_txwi(struct mt76_dev *dev)\n{\n\tstruct mt76_txwi_cache *t;\n\n\tlocal_bh_disable();\n\twhile ((t = __mt76_get_txwi(dev)) != NULL) {\n\t\tdma_unmap_single(dev->dma_dev, t->dma_addr, dev->drv->txwi_size,\n\t\t\t\t DMA_TO_DEVICE);\n\t\tkfree(mt76_get_txwi_ptr(dev, t));\n\t}\n\tlocal_bh_enable();\n}\n\nvoid\nmt76_free_pending_rxwi(struct mt76_dev *dev)\n{\n\tstruct mt76_txwi_cache *t;\n\n\tlocal_bh_disable();\n\twhile ((t = __mt76_get_rxwi(dev)) != NULL) {\n\t\tif (t->ptr)\n\t\t\tmt76_put_page_pool_buf(t->ptr, false);\n\t\tkfree(t);\n\t}\n\tlocal_bh_enable();\n}\nEXPORT_SYMBOL_GPL(mt76_free_pending_rxwi);\n\nstatic void\nmt76_dma_sync_idx(struct mt76_dev *dev, struct mt76_queue *q)\n{\n\tQ_WRITE(dev, q, desc_base, q->desc_dma);\n\tQ_WRITE(dev, q, ring_size, q->ndesc);\n\tq->head = Q_READ(dev, q, dma_idx);\n\tq->tail = q->head;\n}\n\nstatic void\nmt76_dma_queue_reset(struct mt76_dev *dev, struct mt76_queue *q)\n{\n\tint i;\n\n\tif (!q || !q->ndesc)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < q->ndesc; i++)\n\t\tq->desc[i].ctrl = cpu_to_le32(MT_DMA_CTL_DMA_DONE);\n\n\tQ_WRITE(dev, q, cpu_idx, 0);\n\tQ_WRITE(dev, q, dma_idx, 0);\n\tmt76_dma_sync_idx(dev, q);\n}\n\nstatic int\nmt76_dma_add_rx_buf(struct mt76_dev *dev, struct mt76_queue *q,\n\t\t    struct mt76_queue_buf *buf, void *data)\n{\n\tstruct mt76_desc *desc = &q->desc[q->head];\n\tstruct mt76_queue_entry *entry = &q->entry[q->head];\n\tstruct mt76_txwi_cache *txwi = NULL;\n\tu32 buf1 = 0, ctrl;\n\tint idx = q->head;\n\tint rx_token;\n\n\tctrl = FIELD_PREP(MT_DMA_CTL_SD_LEN0, buf[0].len);\n\n\tif (mt76_queue_is_wed_rx(q)) {\n\t\ttxwi = mt76_get_rxwi(dev);\n\t\tif (!txwi)\n\t\t\treturn -ENOMEM;\n\n\t\trx_token = mt76_rx_token_consume(dev, data, txwi, buf->addr);\n\t\tif (rx_token < 0) {\n\t\t\tmt76_put_rxwi(dev, txwi);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tbuf1 |= FIELD_PREP(MT_DMA_CTL_TOKEN, rx_token);\n\t\tctrl |= MT_DMA_CTL_TO_HOST;\n\t}\n\n\tWRITE_ONCE(desc->buf0, cpu_to_le32(buf->addr));\n\tWRITE_ONCE(desc->buf1, cpu_to_le32(buf1));\n\tWRITE_ONCE(desc->ctrl, cpu_to_le32(ctrl));\n\tWRITE_ONCE(desc->info, 0);\n\n\tentry->dma_addr[0] = buf->addr;\n\tentry->dma_len[0] = buf->len;\n\tentry->txwi = txwi;\n\tentry->buf = data;\n\tentry->wcid = 0xffff;\n\tentry->skip_buf1 = true;\n\tq->head = (q->head + 1) % q->ndesc;\n\tq->queued++;\n\n\treturn idx;\n}\n\nstatic int\nmt76_dma_add_buf(struct mt76_dev *dev, struct mt76_queue *q,\n\t\t struct mt76_queue_buf *buf, int nbufs, u32 info,\n\t\t struct sk_buff *skb, void *txwi)\n{\n\tstruct mt76_queue_entry *entry;\n\tstruct mt76_desc *desc;\n\tint i, idx = -1;\n\tu32 ctrl, next;\n\n\tif (txwi) {\n\t\tq->entry[q->head].txwi = DMA_DUMMY_DATA;\n\t\tq->entry[q->head].skip_buf0 = true;\n\t}\n\n\tfor (i = 0; i < nbufs; i += 2, buf += 2) {\n\t\tu32 buf0 = buf[0].addr, buf1 = 0;\n\n\t\tidx = q->head;\n\t\tnext = (q->head + 1) % q->ndesc;\n\n\t\tdesc = &q->desc[idx];\n\t\tentry = &q->entry[idx];\n\n\t\tif (buf[0].skip_unmap)\n\t\t\tentry->skip_buf0 = true;\n\t\tentry->skip_buf1 = i == nbufs - 1;\n\n\t\tentry->dma_addr[0] = buf[0].addr;\n\t\tentry->dma_len[0] = buf[0].len;\n\n\t\tctrl = FIELD_PREP(MT_DMA_CTL_SD_LEN0, buf[0].len);\n\t\tif (i < nbufs - 1) {\n\t\t\tentry->dma_addr[1] = buf[1].addr;\n\t\t\tentry->dma_len[1] = buf[1].len;\n\t\t\tbuf1 = buf[1].addr;\n\t\t\tctrl |= FIELD_PREP(MT_DMA_CTL_SD_LEN1, buf[1].len);\n\t\t\tif (buf[1].skip_unmap)\n\t\t\t\tentry->skip_buf1 = true;\n\t\t}\n\n\t\tif (i == nbufs - 1)\n\t\t\tctrl |= MT_DMA_CTL_LAST_SEC0;\n\t\telse if (i == nbufs - 2)\n\t\t\tctrl |= MT_DMA_CTL_LAST_SEC1;\n\n\t\tWRITE_ONCE(desc->buf0, cpu_to_le32(buf0));\n\t\tWRITE_ONCE(desc->buf1, cpu_to_le32(buf1));\n\t\tWRITE_ONCE(desc->info, cpu_to_le32(info));\n\t\tWRITE_ONCE(desc->ctrl, cpu_to_le32(ctrl));\n\n\t\tq->head = next;\n\t\tq->queued++;\n\t}\n\n\tq->entry[idx].txwi = txwi;\n\tq->entry[idx].skb = skb;\n\tq->entry[idx].wcid = 0xffff;\n\n\treturn idx;\n}\n\nstatic void\nmt76_dma_tx_cleanup_idx(struct mt76_dev *dev, struct mt76_queue *q, int idx,\n\t\t\tstruct mt76_queue_entry *prev_e)\n{\n\tstruct mt76_queue_entry *e = &q->entry[idx];\n\n\tif (!e->skip_buf0)\n\t\tdma_unmap_single(dev->dma_dev, e->dma_addr[0], e->dma_len[0],\n\t\t\t\t DMA_TO_DEVICE);\n\n\tif (!e->skip_buf1)\n\t\tdma_unmap_single(dev->dma_dev, e->dma_addr[1], e->dma_len[1],\n\t\t\t\t DMA_TO_DEVICE);\n\n\tif (e->txwi == DMA_DUMMY_DATA)\n\t\te->txwi = NULL;\n\n\t*prev_e = *e;\n\tmemset(e, 0, sizeof(*e));\n}\n\nstatic void\nmt76_dma_kick_queue(struct mt76_dev *dev, struct mt76_queue *q)\n{\n\twmb();\n\tQ_WRITE(dev, q, cpu_idx, q->head);\n}\n\nstatic void\nmt76_dma_tx_cleanup(struct mt76_dev *dev, struct mt76_queue *q, bool flush)\n{\n\tstruct mt76_queue_entry entry;\n\tint last;\n\n\tif (!q || !q->ndesc)\n\t\treturn;\n\n\tspin_lock_bh(&q->cleanup_lock);\n\tif (flush)\n\t\tlast = -1;\n\telse\n\t\tlast = Q_READ(dev, q, dma_idx);\n\n\twhile (q->queued > 0 && q->tail != last) {\n\t\tmt76_dma_tx_cleanup_idx(dev, q, q->tail, &entry);\n\t\tmt76_queue_tx_complete(dev, q, &entry);\n\n\t\tif (entry.txwi) {\n\t\t\tif (!(dev->drv->drv_flags & MT_DRV_TXWI_NO_FREE))\n\t\t\t\tmt76_put_txwi(dev, entry.txwi);\n\t\t}\n\n\t\tif (!flush && q->tail == last)\n\t\t\tlast = Q_READ(dev, q, dma_idx);\n\t}\n\tspin_unlock_bh(&q->cleanup_lock);\n\n\tif (flush) {\n\t\tspin_lock_bh(&q->lock);\n\t\tmt76_dma_sync_idx(dev, q);\n\t\tmt76_dma_kick_queue(dev, q);\n\t\tspin_unlock_bh(&q->lock);\n\t}\n\n\tif (!q->queued)\n\t\twake_up(&dev->tx_wait);\n}\n\nstatic void *\nmt76_dma_get_buf(struct mt76_dev *dev, struct mt76_queue *q, int idx,\n\t\t int *len, u32 *info, bool *more, bool *drop)\n{\n\tstruct mt76_queue_entry *e = &q->entry[idx];\n\tstruct mt76_desc *desc = &q->desc[idx];\n\tvoid *buf;\n\n\tif (len) {\n\t\tu32 ctrl = le32_to_cpu(READ_ONCE(desc->ctrl));\n\t\t*len = FIELD_GET(MT_DMA_CTL_SD_LEN0, ctrl);\n\t\t*more = !(ctrl & MT_DMA_CTL_LAST_SEC0);\n\t}\n\n\tif (info)\n\t\t*info = le32_to_cpu(desc->info);\n\n\tif (mt76_queue_is_wed_rx(q)) {\n\t\tu32 buf1 = le32_to_cpu(desc->buf1);\n\t\tu32 token = FIELD_GET(MT_DMA_CTL_TOKEN, buf1);\n\t\tstruct mt76_txwi_cache *t = mt76_rx_token_release(dev, token);\n\n\t\tif (!t)\n\t\t\treturn NULL;\n\n\t\tdma_sync_single_for_cpu(dev->dma_dev, t->dma_addr,\n\t\t\t\tSKB_WITH_OVERHEAD(q->buf_size),\n\t\t\t\tpage_pool_get_dma_dir(q->page_pool));\n\n\t\tbuf = t->ptr;\n\t\tt->dma_addr = 0;\n\t\tt->ptr = NULL;\n\n\t\tmt76_put_rxwi(dev, t);\n\n\t\tif (drop) {\n\t\t\tu32 ctrl = le32_to_cpu(READ_ONCE(desc->ctrl));\n\n\t\t\t*drop = !!(ctrl & (MT_DMA_CTL_TO_HOST_A |\n\t\t\t\t\t   MT_DMA_CTL_DROP));\n\n\t\t\t*drop |= !!(buf1 & MT_DMA_CTL_WO_DROP);\n\t\t}\n\t} else {\n\t\tbuf = e->buf;\n\t\te->buf = NULL;\n\t\tdma_sync_single_for_cpu(dev->dma_dev, e->dma_addr[0],\n\t\t\t\tSKB_WITH_OVERHEAD(q->buf_size),\n\t\t\t\tpage_pool_get_dma_dir(q->page_pool));\n\t}\n\n\treturn buf;\n}\n\nstatic void *\nmt76_dma_dequeue(struct mt76_dev *dev, struct mt76_queue *q, bool flush,\n\t\t int *len, u32 *info, bool *more, bool *drop)\n{\n\tint idx = q->tail;\n\n\t*more = false;\n\tif (!q->queued)\n\t\treturn NULL;\n\n\tif (flush)\n\t\tq->desc[idx].ctrl |= cpu_to_le32(MT_DMA_CTL_DMA_DONE);\n\telse if (!(q->desc[idx].ctrl & cpu_to_le32(MT_DMA_CTL_DMA_DONE)))\n\t\treturn NULL;\n\n\tq->tail = (q->tail + 1) % q->ndesc;\n\tq->queued--;\n\n\treturn mt76_dma_get_buf(dev, q, idx, len, info, more, drop);\n}\n\nstatic int\nmt76_dma_tx_queue_skb_raw(struct mt76_dev *dev, struct mt76_queue *q,\n\t\t\t  struct sk_buff *skb, u32 tx_info)\n{\n\tstruct mt76_queue_buf buf = {};\n\tdma_addr_t addr;\n\n\tif (test_bit(MT76_MCU_RESET, &dev->phy.state))\n\t\tgoto error;\n\n\tif (q->queued + 1 >= q->ndesc - 1)\n\t\tgoto error;\n\n\taddr = dma_map_single(dev->dma_dev, skb->data, skb->len,\n\t\t\t      DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev->dma_dev, addr)))\n\t\tgoto error;\n\n\tbuf.addr = addr;\n\tbuf.len = skb->len;\n\n\tspin_lock_bh(&q->lock);\n\tmt76_dma_add_buf(dev, q, &buf, 1, tx_info, skb, NULL);\n\tmt76_dma_kick_queue(dev, q);\n\tspin_unlock_bh(&q->lock);\n\n\treturn 0;\n\nerror:\n\tdev_kfree_skb(skb);\n\treturn -ENOMEM;\n}\n\nstatic int\nmt76_dma_tx_queue_skb(struct mt76_dev *dev, struct mt76_queue *q,\n\t\t      enum mt76_txq_id qid, struct sk_buff *skb,\n\t\t      struct mt76_wcid *wcid, struct ieee80211_sta *sta)\n{\n\tstruct ieee80211_tx_status status = {\n\t\t.sta = sta,\n\t};\n\tstruct mt76_tx_info tx_info = {\n\t\t.skb = skb,\n\t};\n\tstruct ieee80211_hw *hw;\n\tint len, n = 0, ret = -ENOMEM;\n\tstruct mt76_txwi_cache *t;\n\tstruct sk_buff *iter;\n\tdma_addr_t addr;\n\tu8 *txwi;\n\n\tif (test_bit(MT76_RESET, &dev->phy.state))\n\t\tgoto free_skb;\n\n\tt = mt76_get_txwi(dev);\n\tif (!t)\n\t\tgoto free_skb;\n\n\ttxwi = mt76_get_txwi_ptr(dev, t);\n\n\tskb->prev = skb->next = NULL;\n\tif (dev->drv->drv_flags & MT_DRV_TX_ALIGNED4_SKBS)\n\t\tmt76_insert_hdr_pad(skb);\n\n\tlen = skb_headlen(skb);\n\taddr = dma_map_single(dev->dma_dev, skb->data, len, DMA_TO_DEVICE);\n\tif (unlikely(dma_mapping_error(dev->dma_dev, addr)))\n\t\tgoto free;\n\n\ttx_info.buf[n].addr = t->dma_addr;\n\ttx_info.buf[n++].len = dev->drv->txwi_size;\n\ttx_info.buf[n].addr = addr;\n\ttx_info.buf[n++].len = len;\n\n\tskb_walk_frags(skb, iter) {\n\t\tif (n == ARRAY_SIZE(tx_info.buf))\n\t\t\tgoto unmap;\n\n\t\taddr = dma_map_single(dev->dma_dev, iter->data, iter->len,\n\t\t\t\t      DMA_TO_DEVICE);\n\t\tif (unlikely(dma_mapping_error(dev->dma_dev, addr)))\n\t\t\tgoto unmap;\n\n\t\ttx_info.buf[n].addr = addr;\n\t\ttx_info.buf[n++].len = iter->len;\n\t}\n\ttx_info.nbuf = n;\n\n\tif (q->queued + (tx_info.nbuf + 1) / 2 >= q->ndesc - 1) {\n\t\tret = -ENOMEM;\n\t\tgoto unmap;\n\t}\n\n\tdma_sync_single_for_cpu(dev->dma_dev, t->dma_addr, dev->drv->txwi_size,\n\t\t\t\tDMA_TO_DEVICE);\n\tret = dev->drv->tx_prepare_skb(dev, txwi, qid, wcid, sta, &tx_info);\n\tdma_sync_single_for_device(dev->dma_dev, t->dma_addr, dev->drv->txwi_size,\n\t\t\t\t   DMA_TO_DEVICE);\n\tif (ret < 0)\n\t\tgoto unmap;\n\n\treturn mt76_dma_add_buf(dev, q, tx_info.buf, tx_info.nbuf,\n\t\t\t\ttx_info.info, tx_info.skb, t);\n\nunmap:\n\tfor (n--; n > 0; n--)\n\t\tdma_unmap_single(dev->dma_dev, tx_info.buf[n].addr,\n\t\t\t\t tx_info.buf[n].len, DMA_TO_DEVICE);\n\nfree:\n#ifdef CONFIG_NL80211_TESTMODE\n\t \n\tif (mt76_is_testmode_skb(dev, skb, &hw)) {\n\t\tstruct mt76_phy *phy = hw->priv;\n\n\t\tif (tx_info.skb == phy->test.tx_skb)\n\t\t\tphy->test.tx_done--;\n\t}\n#endif\n\n\tmt76_put_txwi(dev, t);\n\nfree_skb:\n\tstatus.skb = tx_info.skb;\n\thw = mt76_tx_status_get_hw(dev, tx_info.skb);\n\tspin_lock_bh(&dev->rx_lock);\n\tieee80211_tx_status_ext(hw, &status);\n\tspin_unlock_bh(&dev->rx_lock);\n\n\treturn ret;\n}\n\nstatic int\nmt76_dma_rx_fill(struct mt76_dev *dev, struct mt76_queue *q,\n\t\t bool allow_direct)\n{\n\tint len = SKB_WITH_OVERHEAD(q->buf_size);\n\tint frames = 0;\n\n\tif (!q->ndesc)\n\t\treturn 0;\n\n\tspin_lock_bh(&q->lock);\n\n\twhile (q->queued < q->ndesc - 1) {\n\t\tenum dma_data_direction dir;\n\t\tstruct mt76_queue_buf qbuf;\n\t\tdma_addr_t addr;\n\t\tint offset;\n\t\tvoid *buf;\n\n\t\tbuf = mt76_get_page_pool_buf(q, &offset, q->buf_size);\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\taddr = page_pool_get_dma_addr(virt_to_head_page(buf)) + offset;\n\t\tdir = page_pool_get_dma_dir(q->page_pool);\n\t\tdma_sync_single_for_device(dev->dma_dev, addr, len, dir);\n\n\t\tqbuf.addr = addr + q->buf_offset;\n\t\tqbuf.len = len - q->buf_offset;\n\t\tqbuf.skip_unmap = false;\n\t\tif (mt76_dma_add_rx_buf(dev, q, &qbuf, buf) < 0) {\n\t\t\tmt76_put_page_pool_buf(buf, allow_direct);\n\t\t\tbreak;\n\t\t}\n\t\tframes++;\n\t}\n\n\tif (frames)\n\t\tmt76_dma_kick_queue(dev, q);\n\n\tspin_unlock_bh(&q->lock);\n\n\treturn frames;\n}\n\nint mt76_dma_wed_setup(struct mt76_dev *dev, struct mt76_queue *q, bool reset)\n{\n#ifdef CONFIG_NET_MEDIATEK_SOC_WED\n\tstruct mtk_wed_device *wed = &dev->mmio.wed;\n\tint ret, type, ring;\n\tu8 flags;\n\n\tif (!q || !q->ndesc)\n\t\treturn -EINVAL;\n\n\tflags = q->flags;\n\tif (!mtk_wed_device_active(wed))\n\t\tq->flags &= ~MT_QFLAG_WED;\n\n\tif (!(q->flags & MT_QFLAG_WED))\n\t\treturn 0;\n\n\ttype = FIELD_GET(MT_QFLAG_WED_TYPE, q->flags);\n\tring = FIELD_GET(MT_QFLAG_WED_RING, q->flags);\n\n\tswitch (type) {\n\tcase MT76_WED_Q_TX:\n\t\tret = mtk_wed_device_tx_ring_setup(wed, ring, q->regs, reset);\n\t\tif (!ret)\n\t\t\tq->wed_regs = wed->tx_ring[ring].reg_base;\n\t\tbreak;\n\tcase MT76_WED_Q_TXFREE:\n\t\t \n\t\tq->flags = 0;\n\t\tmt76_dma_queue_reset(dev, q);\n\t\tmt76_dma_rx_fill(dev, q, false);\n\t\tq->flags = flags;\n\n\t\tret = mtk_wed_device_txfree_ring_setup(wed, q->regs);\n\t\tif (!ret)\n\t\t\tq->wed_regs = wed->txfree_ring.reg_base;\n\t\tbreak;\n\tcase MT76_WED_Q_RX:\n\t\tret = mtk_wed_device_rx_ring_setup(wed, ring, q->regs, reset);\n\t\tif (!ret)\n\t\t\tq->wed_regs = wed->rx_ring[ring].reg_base;\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t}\n\n\treturn ret;\n#else\n\treturn 0;\n#endif\n}\nEXPORT_SYMBOL_GPL(mt76_dma_wed_setup);\n\nstatic int\nmt76_dma_alloc_queue(struct mt76_dev *dev, struct mt76_queue *q,\n\t\t     int idx, int n_desc, int bufsize,\n\t\t     u32 ring_base)\n{\n\tint ret, size;\n\n\tspin_lock_init(&q->lock);\n\tspin_lock_init(&q->cleanup_lock);\n\n\tq->regs = dev->mmio.regs + ring_base + idx * MT_RING_SIZE;\n\tq->ndesc = n_desc;\n\tq->buf_size = bufsize;\n\tq->hw_idx = idx;\n\n\tsize = q->ndesc * sizeof(struct mt76_desc);\n\tq->desc = dmam_alloc_coherent(dev->dma_dev, size, &q->desc_dma, GFP_KERNEL);\n\tif (!q->desc)\n\t\treturn -ENOMEM;\n\n\tsize = q->ndesc * sizeof(*q->entry);\n\tq->entry = devm_kzalloc(dev->dev, size, GFP_KERNEL);\n\tif (!q->entry)\n\t\treturn -ENOMEM;\n\n\tret = mt76_create_page_pool(dev, q);\n\tif (ret)\n\t\treturn ret;\n\n\tret = mt76_dma_wed_setup(dev, q, false);\n\tif (ret)\n\t\treturn ret;\n\n\tif (q->flags != MT_WED_Q_TXFREE)\n\t\tmt76_dma_queue_reset(dev, q);\n\n\treturn 0;\n}\n\nstatic void\nmt76_dma_rx_cleanup(struct mt76_dev *dev, struct mt76_queue *q)\n{\n\tvoid *buf;\n\tbool more;\n\n\tif (!q->ndesc)\n\t\treturn;\n\n\tspin_lock_bh(&q->lock);\n\n\tdo {\n\t\tbuf = mt76_dma_dequeue(dev, q, true, NULL, NULL, &more, NULL);\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\tmt76_put_page_pool_buf(buf, false);\n\t} while (1);\n\n\tif (q->rx_head) {\n\t\tdev_kfree_skb(q->rx_head);\n\t\tq->rx_head = NULL;\n\t}\n\n\tspin_unlock_bh(&q->lock);\n}\n\nstatic void\nmt76_dma_rx_reset(struct mt76_dev *dev, enum mt76_rxq_id qid)\n{\n\tstruct mt76_queue *q = &dev->q_rx[qid];\n\tint i;\n\n\tif (!q->ndesc)\n\t\treturn;\n\n\tfor (i = 0; i < q->ndesc; i++)\n\t\tq->desc[i].ctrl = cpu_to_le32(MT_DMA_CTL_DMA_DONE);\n\n\tmt76_dma_rx_cleanup(dev, q);\n\n\t \n\tmt76_dma_wed_setup(dev, q, true);\n\tif (q->flags != MT_WED_Q_TXFREE) {\n\t\tmt76_dma_sync_idx(dev, q);\n\t\tmt76_dma_rx_fill(dev, q, false);\n\t}\n}\n\nstatic void\nmt76_add_fragment(struct mt76_dev *dev, struct mt76_queue *q, void *data,\n\t\t  int len, bool more, u32 info, bool allow_direct)\n{\n\tstruct sk_buff *skb = q->rx_head;\n\tstruct skb_shared_info *shinfo = skb_shinfo(skb);\n\tint nr_frags = shinfo->nr_frags;\n\n\tif (nr_frags < ARRAY_SIZE(shinfo->frags)) {\n\t\tstruct page *page = virt_to_head_page(data);\n\t\tint offset = data - page_address(page) + q->buf_offset;\n\n\t\tskb_add_rx_frag(skb, nr_frags, page, offset, len, q->buf_size);\n\t} else {\n\t\tmt76_put_page_pool_buf(data, allow_direct);\n\t}\n\n\tif (more)\n\t\treturn;\n\n\tq->rx_head = NULL;\n\tif (nr_frags < ARRAY_SIZE(shinfo->frags))\n\t\tdev->drv->rx_skb(dev, q - dev->q_rx, skb, &info);\n\telse\n\t\tdev_kfree_skb(skb);\n}\n\nstatic int\nmt76_dma_rx_process(struct mt76_dev *dev, struct mt76_queue *q, int budget)\n{\n\tint len, data_len, done = 0, dma_idx;\n\tstruct sk_buff *skb;\n\tunsigned char *data;\n\tbool check_ddone = false;\n\tbool allow_direct = !mt76_queue_is_wed_rx(q);\n\tbool more;\n\n\tif (IS_ENABLED(CONFIG_NET_MEDIATEK_SOC_WED) &&\n\t    q->flags == MT_WED_Q_TXFREE) {\n\t\tdma_idx = Q_READ(dev, q, dma_idx);\n\t\tcheck_ddone = true;\n\t}\n\n\twhile (done < budget) {\n\t\tbool drop = false;\n\t\tu32 info;\n\n\t\tif (check_ddone) {\n\t\t\tif (q->tail == dma_idx)\n\t\t\t\tdma_idx = Q_READ(dev, q, dma_idx);\n\n\t\t\tif (q->tail == dma_idx)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tdata = mt76_dma_dequeue(dev, q, false, &len, &info, &more,\n\t\t\t\t\t&drop);\n\t\tif (!data)\n\t\t\tbreak;\n\n\t\tif (drop)\n\t\t\tgoto free_frag;\n\n\t\tif (q->rx_head)\n\t\t\tdata_len = q->buf_size;\n\t\telse\n\t\t\tdata_len = SKB_WITH_OVERHEAD(q->buf_size);\n\n\t\tif (data_len < len + q->buf_offset) {\n\t\t\tdev_kfree_skb(q->rx_head);\n\t\t\tq->rx_head = NULL;\n\t\t\tgoto free_frag;\n\t\t}\n\n\t\tif (q->rx_head) {\n\t\t\tmt76_add_fragment(dev, q, data, len, more, info,\n\t\t\t\t\t  allow_direct);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!more && dev->drv->rx_check &&\n\t\t    !(dev->drv->rx_check(dev, data, len)))\n\t\t\tgoto free_frag;\n\n\t\tskb = napi_build_skb(data, q->buf_size);\n\t\tif (!skb)\n\t\t\tgoto free_frag;\n\n\t\tskb_reserve(skb, q->buf_offset);\n\t\tskb_mark_for_recycle(skb);\n\n\t\t*(u32 *)skb->cb = info;\n\n\t\t__skb_put(skb, len);\n\t\tdone++;\n\n\t\tif (more) {\n\t\t\tq->rx_head = skb;\n\t\t\tcontinue;\n\t\t}\n\n\t\tdev->drv->rx_skb(dev, q - dev->q_rx, skb, &info);\n\t\tcontinue;\n\nfree_frag:\n\t\tmt76_put_page_pool_buf(data, allow_direct);\n\t}\n\n\tmt76_dma_rx_fill(dev, q, true);\n\treturn done;\n}\n\nint mt76_dma_rx_poll(struct napi_struct *napi, int budget)\n{\n\tstruct mt76_dev *dev;\n\tint qid, done = 0, cur;\n\n\tdev = container_of(napi->dev, struct mt76_dev, napi_dev);\n\tqid = napi - dev->napi;\n\n\trcu_read_lock();\n\n\tdo {\n\t\tcur = mt76_dma_rx_process(dev, &dev->q_rx[qid], budget - done);\n\t\tmt76_rx_poll_complete(dev, qid, napi);\n\t\tdone += cur;\n\t} while (cur && done < budget);\n\n\trcu_read_unlock();\n\n\tif (done < budget && napi_complete(napi))\n\t\tdev->drv->rx_poll_complete(dev, qid);\n\n\treturn done;\n}\nEXPORT_SYMBOL_GPL(mt76_dma_rx_poll);\n\nstatic int\nmt76_dma_init(struct mt76_dev *dev,\n\t      int (*poll)(struct napi_struct *napi, int budget))\n{\n\tint i;\n\n\tinit_dummy_netdev(&dev->napi_dev);\n\tinit_dummy_netdev(&dev->tx_napi_dev);\n\tsnprintf(dev->napi_dev.name, sizeof(dev->napi_dev.name), \"%s\",\n\t\t wiphy_name(dev->hw->wiphy));\n\tdev->napi_dev.threaded = 1;\n\tinit_completion(&dev->mmio.wed_reset);\n\tinit_completion(&dev->mmio.wed_reset_complete);\n\n\tmt76_for_each_q_rx(dev, i) {\n\t\tnetif_napi_add(&dev->napi_dev, &dev->napi[i], poll);\n\t\tmt76_dma_rx_fill(dev, &dev->q_rx[i], false);\n\t\tnapi_enable(&dev->napi[i]);\n\t}\n\n\treturn 0;\n}\n\nstatic const struct mt76_queue_ops mt76_dma_ops = {\n\t.init = mt76_dma_init,\n\t.alloc = mt76_dma_alloc_queue,\n\t.reset_q = mt76_dma_queue_reset,\n\t.tx_queue_skb_raw = mt76_dma_tx_queue_skb_raw,\n\t.tx_queue_skb = mt76_dma_tx_queue_skb,\n\t.tx_cleanup = mt76_dma_tx_cleanup,\n\t.rx_cleanup = mt76_dma_rx_cleanup,\n\t.rx_reset = mt76_dma_rx_reset,\n\t.kick = mt76_dma_kick_queue,\n};\n\nvoid mt76_dma_attach(struct mt76_dev *dev)\n{\n\tdev->queue_ops = &mt76_dma_ops;\n}\nEXPORT_SYMBOL_GPL(mt76_dma_attach);\n\nvoid mt76_dma_cleanup(struct mt76_dev *dev)\n{\n\tint i;\n\n\tmt76_worker_disable(&dev->tx_worker);\n\tnetif_napi_del(&dev->tx_napi);\n\n\tfor (i = 0; i < ARRAY_SIZE(dev->phys); i++) {\n\t\tstruct mt76_phy *phy = dev->phys[i];\n\t\tint j;\n\n\t\tif (!phy)\n\t\t\tcontinue;\n\n\t\tfor (j = 0; j < ARRAY_SIZE(phy->q_tx); j++)\n\t\t\tmt76_dma_tx_cleanup(dev, phy->q_tx[j], true);\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(dev->q_mcu); i++)\n\t\tmt76_dma_tx_cleanup(dev, dev->q_mcu[i], true);\n\n\tmt76_for_each_q_rx(dev, i) {\n\t\tstruct mt76_queue *q = &dev->q_rx[i];\n\n\t\tnetif_napi_del(&dev->napi[i]);\n\t\tmt76_dma_rx_cleanup(dev, q);\n\n\t\tpage_pool_destroy(q->page_pool);\n\t}\n\n\tmt76_free_pending_txwi(dev);\n\tmt76_free_pending_rxwi(dev);\n\n\tif (mtk_wed_device_active(&dev->mmio.wed))\n\t\tmtk_wed_device_detach(&dev->mmio.wed);\n}\nEXPORT_SYMBOL_GPL(mt76_dma_cleanup);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}