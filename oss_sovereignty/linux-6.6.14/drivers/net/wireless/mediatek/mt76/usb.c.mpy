{
  "module_name": "usb.c",
  "hash_id": "0b8165ba0aa0f897a003753d42fc34b9677cc02595908cbe6673abddfdb66024",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wireless/mediatek/mt76/usb.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include \"mt76.h\"\n#include \"usb_trace.h\"\n#include \"dma.h\"\n\n#define MT_VEND_REQ_MAX_RETRY\t10\n#define MT_VEND_REQ_TOUT_MS\t300\n\nstatic bool disable_usb_sg;\nmodule_param_named(disable_usb_sg, disable_usb_sg, bool, 0644);\nMODULE_PARM_DESC(disable_usb_sg, \"Disable usb scatter-gather support\");\n\nint __mt76u_vendor_request(struct mt76_dev *dev, u8 req, u8 req_type,\n\t\t\t   u16 val, u16 offset, void *buf, size_t len)\n{\n\tstruct usb_interface *uintf = to_usb_interface(dev->dev);\n\tstruct usb_device *udev = interface_to_usbdev(uintf);\n\tunsigned int pipe;\n\tint i, ret;\n\n\tlockdep_assert_held(&dev->usb.usb_ctrl_mtx);\n\n\tpipe = (req_type & USB_DIR_IN) ? usb_rcvctrlpipe(udev, 0)\n\t\t\t\t       : usb_sndctrlpipe(udev, 0);\n\tfor (i = 0; i < MT_VEND_REQ_MAX_RETRY; i++) {\n\t\tif (test_bit(MT76_REMOVED, &dev->phy.state))\n\t\t\treturn -EIO;\n\n\t\tret = usb_control_msg(udev, pipe, req, req_type, val,\n\t\t\t\t      offset, buf, len, MT_VEND_REQ_TOUT_MS);\n\t\tif (ret == -ENODEV)\n\t\t\tset_bit(MT76_REMOVED, &dev->phy.state);\n\t\tif (ret >= 0 || ret == -ENODEV)\n\t\t\treturn ret;\n\t\tusleep_range(5000, 10000);\n\t}\n\n\tdev_err(dev->dev, \"vendor request req:%02x off:%04x failed:%d\\n\",\n\t\treq, offset, ret);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(__mt76u_vendor_request);\n\nint mt76u_vendor_request(struct mt76_dev *dev, u8 req,\n\t\t\t u8 req_type, u16 val, u16 offset,\n\t\t\t void *buf, size_t len)\n{\n\tint ret;\n\n\tmutex_lock(&dev->usb.usb_ctrl_mtx);\n\tret = __mt76u_vendor_request(dev, req, req_type,\n\t\t\t\t     val, offset, buf, len);\n\ttrace_usb_reg_wr(dev, offset, val);\n\tmutex_unlock(&dev->usb.usb_ctrl_mtx);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(mt76u_vendor_request);\n\nu32 ___mt76u_rr(struct mt76_dev *dev, u8 req, u8 req_type, u32 addr)\n{\n\tstruct mt76_usb *usb = &dev->usb;\n\tu32 data = ~0;\n\tint ret;\n\n\tret = __mt76u_vendor_request(dev, req, req_type, addr >> 16,\n\t\t\t\t     addr, usb->data, sizeof(__le32));\n\tif (ret == sizeof(__le32))\n\t\tdata = get_unaligned_le32(usb->data);\n\ttrace_usb_reg_rr(dev, addr, data);\n\n\treturn data;\n}\nEXPORT_SYMBOL_GPL(___mt76u_rr);\n\nstatic u32 __mt76u_rr(struct mt76_dev *dev, u32 addr)\n{\n\tu8 req;\n\n\tswitch (addr & MT_VEND_TYPE_MASK) {\n\tcase MT_VEND_TYPE_EEPROM:\n\t\treq = MT_VEND_READ_EEPROM;\n\t\tbreak;\n\tcase MT_VEND_TYPE_CFG:\n\t\treq = MT_VEND_READ_CFG;\n\t\tbreak;\n\tdefault:\n\t\treq = MT_VEND_MULTI_READ;\n\t\tbreak;\n\t}\n\n\treturn ___mt76u_rr(dev, req, USB_DIR_IN | USB_TYPE_VENDOR,\n\t\t\t   addr & ~MT_VEND_TYPE_MASK);\n}\n\nstatic u32 mt76u_rr(struct mt76_dev *dev, u32 addr)\n{\n\tu32 ret;\n\n\tmutex_lock(&dev->usb.usb_ctrl_mtx);\n\tret = __mt76u_rr(dev, addr);\n\tmutex_unlock(&dev->usb.usb_ctrl_mtx);\n\n\treturn ret;\n}\n\nvoid ___mt76u_wr(struct mt76_dev *dev, u8 req, u8 req_type,\n\t\t u32 addr, u32 val)\n{\n\tstruct mt76_usb *usb = &dev->usb;\n\n\tput_unaligned_le32(val, usb->data);\n\t__mt76u_vendor_request(dev, req, req_type, addr >> 16,\n\t\t\t       addr, usb->data, sizeof(__le32));\n\ttrace_usb_reg_wr(dev, addr, val);\n}\nEXPORT_SYMBOL_GPL(___mt76u_wr);\n\nstatic void __mt76u_wr(struct mt76_dev *dev, u32 addr, u32 val)\n{\n\tu8 req;\n\n\tswitch (addr & MT_VEND_TYPE_MASK) {\n\tcase MT_VEND_TYPE_CFG:\n\t\treq = MT_VEND_WRITE_CFG;\n\t\tbreak;\n\tdefault:\n\t\treq = MT_VEND_MULTI_WRITE;\n\t\tbreak;\n\t}\n\t___mt76u_wr(dev, req, USB_DIR_OUT | USB_TYPE_VENDOR,\n\t\t    addr & ~MT_VEND_TYPE_MASK, val);\n}\n\nstatic void mt76u_wr(struct mt76_dev *dev, u32 addr, u32 val)\n{\n\tmutex_lock(&dev->usb.usb_ctrl_mtx);\n\t__mt76u_wr(dev, addr, val);\n\tmutex_unlock(&dev->usb.usb_ctrl_mtx);\n}\n\nstatic u32 mt76u_rmw(struct mt76_dev *dev, u32 addr,\n\t\t     u32 mask, u32 val)\n{\n\tmutex_lock(&dev->usb.usb_ctrl_mtx);\n\tval |= __mt76u_rr(dev, addr) & ~mask;\n\t__mt76u_wr(dev, addr, val);\n\tmutex_unlock(&dev->usb.usb_ctrl_mtx);\n\n\treturn val;\n}\n\nstatic void mt76u_copy(struct mt76_dev *dev, u32 offset,\n\t\t       const void *data, int len)\n{\n\tstruct mt76_usb *usb = &dev->usb;\n\tconst u8 *val = data;\n\tint ret;\n\tint current_batch_size;\n\tint i = 0;\n\n\t \n\tlen = round_up(len, 4);\n\n\tmutex_lock(&usb->usb_ctrl_mtx);\n\twhile (i < len) {\n\t\tcurrent_batch_size = min_t(int, usb->data_len, len - i);\n\t\tmemcpy(usb->data, val + i, current_batch_size);\n\t\tret = __mt76u_vendor_request(dev, MT_VEND_MULTI_WRITE,\n\t\t\t\t\t     USB_DIR_OUT | USB_TYPE_VENDOR,\n\t\t\t\t\t     0, offset + i, usb->data,\n\t\t\t\t\t     current_batch_size);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\ti += current_batch_size;\n\t}\n\tmutex_unlock(&usb->usb_ctrl_mtx);\n}\n\nvoid mt76u_read_copy(struct mt76_dev *dev, u32 offset,\n\t\t     void *data, int len)\n{\n\tstruct mt76_usb *usb = &dev->usb;\n\tint i = 0, batch_len, ret;\n\tu8 *val = data;\n\n\tlen = round_up(len, 4);\n\tmutex_lock(&usb->usb_ctrl_mtx);\n\twhile (i < len) {\n\t\tbatch_len = min_t(int, usb->data_len, len - i);\n\t\tret = __mt76u_vendor_request(dev, MT_VEND_READ_EXT,\n\t\t\t\t\t     USB_DIR_IN | USB_TYPE_VENDOR,\n\t\t\t\t\t     (offset + i) >> 16, offset + i,\n\t\t\t\t\t     usb->data, batch_len);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\tmemcpy(val + i, usb->data, batch_len);\n\t\ti += batch_len;\n\t}\n\tmutex_unlock(&usb->usb_ctrl_mtx);\n}\nEXPORT_SYMBOL_GPL(mt76u_read_copy);\n\nvoid mt76u_single_wr(struct mt76_dev *dev, const u8 req,\n\t\t     const u16 offset, const u32 val)\n{\n\tmutex_lock(&dev->usb.usb_ctrl_mtx);\n\t__mt76u_vendor_request(dev, req,\n\t\t\t       USB_DIR_OUT | USB_TYPE_VENDOR,\n\t\t\t       val & 0xffff, offset, NULL, 0);\n\t__mt76u_vendor_request(dev, req,\n\t\t\t       USB_DIR_OUT | USB_TYPE_VENDOR,\n\t\t\t       val >> 16, offset + 2, NULL, 0);\n\tmutex_unlock(&dev->usb.usb_ctrl_mtx);\n}\nEXPORT_SYMBOL_GPL(mt76u_single_wr);\n\nstatic int\nmt76u_req_wr_rp(struct mt76_dev *dev, u32 base,\n\t\tconst struct mt76_reg_pair *data, int len)\n{\n\tstruct mt76_usb *usb = &dev->usb;\n\n\tmutex_lock(&usb->usb_ctrl_mtx);\n\twhile (len > 0) {\n\t\t__mt76u_wr(dev, base + data->reg, data->value);\n\t\tlen--;\n\t\tdata++;\n\t}\n\tmutex_unlock(&usb->usb_ctrl_mtx);\n\n\treturn 0;\n}\n\nstatic int\nmt76u_wr_rp(struct mt76_dev *dev, u32 base,\n\t    const struct mt76_reg_pair *data, int n)\n{\n\tif (test_bit(MT76_STATE_MCU_RUNNING, &dev->phy.state))\n\t\treturn dev->mcu_ops->mcu_wr_rp(dev, base, data, n);\n\telse\n\t\treturn mt76u_req_wr_rp(dev, base, data, n);\n}\n\nstatic int\nmt76u_req_rd_rp(struct mt76_dev *dev, u32 base, struct mt76_reg_pair *data,\n\t\tint len)\n{\n\tstruct mt76_usb *usb = &dev->usb;\n\n\tmutex_lock(&usb->usb_ctrl_mtx);\n\twhile (len > 0) {\n\t\tdata->value = __mt76u_rr(dev, base + data->reg);\n\t\tlen--;\n\t\tdata++;\n\t}\n\tmutex_unlock(&usb->usb_ctrl_mtx);\n\n\treturn 0;\n}\n\nstatic int\nmt76u_rd_rp(struct mt76_dev *dev, u32 base,\n\t    struct mt76_reg_pair *data, int n)\n{\n\tif (test_bit(MT76_STATE_MCU_RUNNING, &dev->phy.state))\n\t\treturn dev->mcu_ops->mcu_rd_rp(dev, base, data, n);\n\telse\n\t\treturn mt76u_req_rd_rp(dev, base, data, n);\n}\n\nstatic bool mt76u_check_sg(struct mt76_dev *dev)\n{\n\tstruct usb_interface *uintf = to_usb_interface(dev->dev);\n\tstruct usb_device *udev = interface_to_usbdev(uintf);\n\n\treturn (!disable_usb_sg && udev->bus->sg_tablesize > 0 &&\n\t\tudev->bus->no_sg_constraint);\n}\n\nstatic int\nmt76u_set_endpoints(struct usb_interface *intf,\n\t\t    struct mt76_usb *usb)\n{\n\tstruct usb_host_interface *intf_desc = intf->cur_altsetting;\n\tstruct usb_endpoint_descriptor *ep_desc;\n\tint i, in_ep = 0, out_ep = 0;\n\n\tfor (i = 0; i < intf_desc->desc.bNumEndpoints; i++) {\n\t\tep_desc = &intf_desc->endpoint[i].desc;\n\n\t\tif (usb_endpoint_is_bulk_in(ep_desc) &&\n\t\t    in_ep < __MT_EP_IN_MAX) {\n\t\t\tusb->in_ep[in_ep] = usb_endpoint_num(ep_desc);\n\t\t\tin_ep++;\n\t\t} else if (usb_endpoint_is_bulk_out(ep_desc) &&\n\t\t\t   out_ep < __MT_EP_OUT_MAX) {\n\t\t\tusb->out_ep[out_ep] = usb_endpoint_num(ep_desc);\n\t\t\tout_ep++;\n\t\t}\n\t}\n\n\tif (in_ep != __MT_EP_IN_MAX || out_ep != __MT_EP_OUT_MAX)\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic int\nmt76u_fill_rx_sg(struct mt76_dev *dev, struct mt76_queue *q, struct urb *urb,\n\t\t int nsgs)\n{\n\tint i;\n\n\tfor (i = 0; i < nsgs; i++) {\n\t\tvoid *data;\n\t\tint offset;\n\n\t\tdata = mt76_get_page_pool_buf(q, &offset, q->buf_size);\n\t\tif (!data)\n\t\t\tbreak;\n\n\t\tsg_set_page(&urb->sg[i], virt_to_head_page(data), q->buf_size,\n\t\t\t    offset);\n\t}\n\n\tif (i < nsgs) {\n\t\tint j;\n\n\t\tfor (j = nsgs; j < urb->num_sgs; j++)\n\t\t\tmt76_put_page_pool_buf(sg_virt(&urb->sg[j]), false);\n\t\turb->num_sgs = i;\n\t}\n\n\turb->num_sgs = max_t(int, i, urb->num_sgs);\n\turb->transfer_buffer_length = urb->num_sgs * q->buf_size;\n\tsg_init_marker(urb->sg, urb->num_sgs);\n\n\treturn i ? : -ENOMEM;\n}\n\nstatic int\nmt76u_refill_rx(struct mt76_dev *dev, struct mt76_queue *q,\n\t\tstruct urb *urb, int nsgs)\n{\n\tenum mt76_rxq_id qid = q - &dev->q_rx[MT_RXQ_MAIN];\n\tint offset;\n\n\tif (qid == MT_RXQ_MAIN && dev->usb.sg_en)\n\t\treturn mt76u_fill_rx_sg(dev, q, urb, nsgs);\n\n\turb->transfer_buffer_length = q->buf_size;\n\turb->transfer_buffer = mt76_get_page_pool_buf(q, &offset, q->buf_size);\n\n\treturn urb->transfer_buffer ? 0 : -ENOMEM;\n}\n\nstatic int\nmt76u_urb_alloc(struct mt76_dev *dev, struct mt76_queue_entry *e,\n\t\tint sg_max_size)\n{\n\tunsigned int size = sizeof(struct urb);\n\n\tif (dev->usb.sg_en)\n\t\tsize += sg_max_size * sizeof(struct scatterlist);\n\n\te->urb = kzalloc(size, GFP_KERNEL);\n\tif (!e->urb)\n\t\treturn -ENOMEM;\n\n\tusb_init_urb(e->urb);\n\n\tif (dev->usb.sg_en && sg_max_size > 0)\n\t\te->urb->sg = (struct scatterlist *)(e->urb + 1);\n\n\treturn 0;\n}\n\nstatic int\nmt76u_rx_urb_alloc(struct mt76_dev *dev, struct mt76_queue *q,\n\t\t   struct mt76_queue_entry *e)\n{\n\tenum mt76_rxq_id qid = q - &dev->q_rx[MT_RXQ_MAIN];\n\tint err, sg_size;\n\n\tsg_size = qid == MT_RXQ_MAIN ? MT_RX_SG_MAX_SIZE : 0;\n\terr = mt76u_urb_alloc(dev, e, sg_size);\n\tif (err)\n\t\treturn err;\n\n\treturn mt76u_refill_rx(dev, q, e->urb, sg_size);\n}\n\nstatic void mt76u_urb_free(struct urb *urb)\n{\n\tint i;\n\n\tfor (i = 0; i < urb->num_sgs; i++)\n\t\tmt76_put_page_pool_buf(sg_virt(&urb->sg[i]), false);\n\n\tif (urb->transfer_buffer)\n\t\tmt76_put_page_pool_buf(urb->transfer_buffer, false);\n\n\tusb_free_urb(urb);\n}\n\nstatic void\nmt76u_fill_bulk_urb(struct mt76_dev *dev, int dir, int index,\n\t\t    struct urb *urb, usb_complete_t complete_fn,\n\t\t    void *context)\n{\n\tstruct usb_interface *uintf = to_usb_interface(dev->dev);\n\tstruct usb_device *udev = interface_to_usbdev(uintf);\n\tunsigned int pipe;\n\n\tif (dir == USB_DIR_IN)\n\t\tpipe = usb_rcvbulkpipe(udev, dev->usb.in_ep[index]);\n\telse\n\t\tpipe = usb_sndbulkpipe(udev, dev->usb.out_ep[index]);\n\n\turb->dev = udev;\n\turb->pipe = pipe;\n\turb->complete = complete_fn;\n\turb->context = context;\n}\n\nstatic struct urb *\nmt76u_get_next_rx_entry(struct mt76_queue *q)\n{\n\tstruct urb *urb = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\tif (q->queued > 0) {\n\t\turb = q->entry[q->tail].urb;\n\t\tq->tail = (q->tail + 1) % q->ndesc;\n\t\tq->queued--;\n\t}\n\tspin_unlock_irqrestore(&q->lock, flags);\n\n\treturn urb;\n}\n\nstatic int\nmt76u_get_rx_entry_len(struct mt76_dev *dev, u8 *data,\n\t\t       u32 data_len)\n{\n\tu16 dma_len, min_len;\n\n\tdma_len = get_unaligned_le16(data);\n\tif (dev->drv->drv_flags & MT_DRV_RX_DMA_HDR)\n\t\treturn dma_len;\n\n\tmin_len = MT_DMA_HDR_LEN + MT_RX_RXWI_LEN + MT_FCE_INFO_LEN;\n\tif (data_len < min_len || !dma_len ||\n\t    dma_len + MT_DMA_HDR_LEN > data_len ||\n\t    (dma_len & 0x3))\n\t\treturn -EINVAL;\n\treturn dma_len;\n}\n\nstatic struct sk_buff *\nmt76u_build_rx_skb(struct mt76_dev *dev, void *data,\n\t\t   int len, int buf_size)\n{\n\tint head_room, drv_flags = dev->drv->drv_flags;\n\tstruct sk_buff *skb;\n\n\thead_room = drv_flags & MT_DRV_RX_DMA_HDR ? 0 : MT_DMA_HDR_LEN;\n\tif (SKB_WITH_OVERHEAD(buf_size) < head_room + len) {\n\t\tstruct page *page;\n\n\t\t \n\t\tskb = alloc_skb(MT_SKB_HEAD_LEN, GFP_ATOMIC);\n\t\tif (!skb)\n\t\t\treturn NULL;\n\n\t\tskb_put_data(skb, data + head_room, MT_SKB_HEAD_LEN);\n\t\tdata += head_room + MT_SKB_HEAD_LEN;\n\t\tpage = virt_to_head_page(data);\n\t\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,\n\t\t\t\tpage, data - page_address(page),\n\t\t\t\tlen - MT_SKB_HEAD_LEN, buf_size);\n\n\t\treturn skb;\n\t}\n\n\t \n\tskb = build_skb(data, buf_size);\n\tif (!skb)\n\t\treturn NULL;\n\n\tskb_reserve(skb, head_room);\n\t__skb_put(skb, len);\n\n\treturn skb;\n}\n\nstatic int\nmt76u_process_rx_entry(struct mt76_dev *dev, struct urb *urb,\n\t\t       int buf_size)\n{\n\tu8 *data = urb->num_sgs ? sg_virt(&urb->sg[0]) : urb->transfer_buffer;\n\tint data_len = urb->num_sgs ? urb->sg[0].length : urb->actual_length;\n\tint len, nsgs = 1, head_room, drv_flags = dev->drv->drv_flags;\n\tstruct sk_buff *skb;\n\n\tif (!test_bit(MT76_STATE_INITIALIZED, &dev->phy.state))\n\t\treturn 0;\n\n\tlen = mt76u_get_rx_entry_len(dev, data, urb->actual_length);\n\tif (len < 0)\n\t\treturn 0;\n\n\thead_room = drv_flags & MT_DRV_RX_DMA_HDR ? 0 : MT_DMA_HDR_LEN;\n\tdata_len = min_t(int, len, data_len - head_room);\n\n\tif (len == data_len &&\n\t    dev->drv->rx_check && !dev->drv->rx_check(dev, data, data_len))\n\t\treturn 0;\n\n\tskb = mt76u_build_rx_skb(dev, data, data_len, buf_size);\n\tif (!skb)\n\t\treturn 0;\n\n\tlen -= data_len;\n\twhile (len > 0 && nsgs < urb->num_sgs) {\n\t\tdata_len = min_t(int, len, urb->sg[nsgs].length);\n\t\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,\n\t\t\t\tsg_page(&urb->sg[nsgs]),\n\t\t\t\turb->sg[nsgs].offset, data_len,\n\t\t\t\tbuf_size);\n\t\tlen -= data_len;\n\t\tnsgs++;\n\t}\n\n\tskb_mark_for_recycle(skb);\n\tdev->drv->rx_skb(dev, MT_RXQ_MAIN, skb, NULL);\n\n\treturn nsgs;\n}\n\nstatic void mt76u_complete_rx(struct urb *urb)\n{\n\tstruct mt76_dev *dev = dev_get_drvdata(&urb->dev->dev);\n\tstruct mt76_queue *q = urb->context;\n\tunsigned long flags;\n\n\ttrace_rx_urb(dev, urb);\n\n\tswitch (urb->status) {\n\tcase -ECONNRESET:\n\tcase -ESHUTDOWN:\n\tcase -ENOENT:\n\tcase -EPROTO:\n\t\treturn;\n\tdefault:\n\t\tdev_err_ratelimited(dev->dev, \"rx urb failed: %d\\n\",\n\t\t\t\t    urb->status);\n\t\tfallthrough;\n\tcase 0:\n\t\tbreak;\n\t}\n\n\tspin_lock_irqsave(&q->lock, flags);\n\tif (WARN_ONCE(q->entry[q->head].urb != urb, \"rx urb mismatch\"))\n\t\tgoto out;\n\n\tq->head = (q->head + 1) % q->ndesc;\n\tq->queued++;\n\tmt76_worker_schedule(&dev->usb.rx_worker);\nout:\n\tspin_unlock_irqrestore(&q->lock, flags);\n}\n\nstatic int\nmt76u_submit_rx_buf(struct mt76_dev *dev, enum mt76_rxq_id qid,\n\t\t    struct urb *urb)\n{\n\tint ep = qid == MT_RXQ_MAIN ? MT_EP_IN_PKT_RX : MT_EP_IN_CMD_RESP;\n\n\tmt76u_fill_bulk_urb(dev, USB_DIR_IN, ep, urb,\n\t\t\t    mt76u_complete_rx, &dev->q_rx[qid]);\n\ttrace_submit_urb(dev, urb);\n\n\treturn usb_submit_urb(urb, GFP_ATOMIC);\n}\n\nstatic void\nmt76u_process_rx_queue(struct mt76_dev *dev, struct mt76_queue *q)\n{\n\tint qid = q - &dev->q_rx[MT_RXQ_MAIN];\n\tstruct urb *urb;\n\tint err, count;\n\n\twhile (true) {\n\t\turb = mt76u_get_next_rx_entry(q);\n\t\tif (!urb)\n\t\t\tbreak;\n\n\t\tcount = mt76u_process_rx_entry(dev, urb, q->buf_size);\n\t\tif (count > 0) {\n\t\t\terr = mt76u_refill_rx(dev, q, urb, count);\n\t\t\tif (err < 0)\n\t\t\t\tbreak;\n\t\t}\n\t\tmt76u_submit_rx_buf(dev, qid, urb);\n\t}\n\tif (qid == MT_RXQ_MAIN) {\n\t\tlocal_bh_disable();\n\t\tmt76_rx_poll_complete(dev, MT_RXQ_MAIN, NULL);\n\t\tlocal_bh_enable();\n\t}\n}\n\nstatic void mt76u_rx_worker(struct mt76_worker *w)\n{\n\tstruct mt76_usb *usb = container_of(w, struct mt76_usb, rx_worker);\n\tstruct mt76_dev *dev = container_of(usb, struct mt76_dev, usb);\n\tint i;\n\n\trcu_read_lock();\n\tmt76_for_each_q_rx(dev, i)\n\t\tmt76u_process_rx_queue(dev, &dev->q_rx[i]);\n\trcu_read_unlock();\n}\n\nstatic int\nmt76u_submit_rx_buffers(struct mt76_dev *dev, enum mt76_rxq_id qid)\n{\n\tstruct mt76_queue *q = &dev->q_rx[qid];\n\tunsigned long flags;\n\tint i, err = 0;\n\n\tspin_lock_irqsave(&q->lock, flags);\n\tfor (i = 0; i < q->ndesc; i++) {\n\t\terr = mt76u_submit_rx_buf(dev, qid, q->entry[i].urb);\n\t\tif (err < 0)\n\t\t\tbreak;\n\t}\n\tq->head = q->tail = 0;\n\tq->queued = 0;\n\tspin_unlock_irqrestore(&q->lock, flags);\n\n\treturn err;\n}\n\nstatic int\nmt76u_alloc_rx_queue(struct mt76_dev *dev, enum mt76_rxq_id qid)\n{\n\tstruct mt76_queue *q = &dev->q_rx[qid];\n\tint i, err;\n\n\terr = mt76_create_page_pool(dev, q);\n\tif (err)\n\t\treturn err;\n\n\tspin_lock_init(&q->lock);\n\tq->entry = devm_kcalloc(dev->dev,\n\t\t\t\tMT_NUM_RX_ENTRIES, sizeof(*q->entry),\n\t\t\t\tGFP_KERNEL);\n\tif (!q->entry)\n\t\treturn -ENOMEM;\n\n\tq->ndesc = MT_NUM_RX_ENTRIES;\n\tq->buf_size = PAGE_SIZE;\n\n\tfor (i = 0; i < q->ndesc; i++) {\n\t\terr = mt76u_rx_urb_alloc(dev, q, &q->entry[i]);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\treturn mt76u_submit_rx_buffers(dev, qid);\n}\n\nint mt76u_alloc_mcu_queue(struct mt76_dev *dev)\n{\n\treturn mt76u_alloc_rx_queue(dev, MT_RXQ_MCU);\n}\nEXPORT_SYMBOL_GPL(mt76u_alloc_mcu_queue);\n\nstatic void\nmt76u_free_rx_queue(struct mt76_dev *dev, struct mt76_queue *q)\n{\n\tint i;\n\n\tfor (i = 0; i < q->ndesc; i++) {\n\t\tif (!q->entry[i].urb)\n\t\t\tcontinue;\n\n\t\tmt76u_urb_free(q->entry[i].urb);\n\t\tq->entry[i].urb = NULL;\n\t}\n\tpage_pool_destroy(q->page_pool);\n\tq->page_pool = NULL;\n}\n\nstatic void mt76u_free_rx(struct mt76_dev *dev)\n{\n\tint i;\n\n\tmt76_worker_teardown(&dev->usb.rx_worker);\n\n\tmt76_for_each_q_rx(dev, i)\n\t\tmt76u_free_rx_queue(dev, &dev->q_rx[i]);\n}\n\nvoid mt76u_stop_rx(struct mt76_dev *dev)\n{\n\tint i;\n\n\tmt76_worker_disable(&dev->usb.rx_worker);\n\n\tmt76_for_each_q_rx(dev, i) {\n\t\tstruct mt76_queue *q = &dev->q_rx[i];\n\t\tint j;\n\n\t\tfor (j = 0; j < q->ndesc; j++)\n\t\t\tusb_poison_urb(q->entry[j].urb);\n\t}\n}\nEXPORT_SYMBOL_GPL(mt76u_stop_rx);\n\nint mt76u_resume_rx(struct mt76_dev *dev)\n{\n\tint i;\n\n\tmt76_for_each_q_rx(dev, i) {\n\t\tstruct mt76_queue *q = &dev->q_rx[i];\n\t\tint err, j;\n\n\t\tfor (j = 0; j < q->ndesc; j++)\n\t\t\tusb_unpoison_urb(q->entry[j].urb);\n\n\t\terr = mt76u_submit_rx_buffers(dev, i);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t}\n\n\tmt76_worker_enable(&dev->usb.rx_worker);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(mt76u_resume_rx);\n\nstatic void mt76u_status_worker(struct mt76_worker *w)\n{\n\tstruct mt76_usb *usb = container_of(w, struct mt76_usb, status_worker);\n\tstruct mt76_dev *dev = container_of(usb, struct mt76_dev, usb);\n\tstruct mt76_queue_entry entry;\n\tstruct mt76_queue *q;\n\tint i;\n\n\tif (!test_bit(MT76_STATE_RUNNING, &dev->phy.state))\n\t\treturn;\n\n\tfor (i = 0; i < IEEE80211_NUM_ACS; i++) {\n\t\tq = dev->phy.q_tx[i];\n\t\tif (!q)\n\t\t\tcontinue;\n\n\t\twhile (q->queued > 0) {\n\t\t\tif (!q->entry[q->tail].done)\n\t\t\t\tbreak;\n\n\t\t\tentry = q->entry[q->tail];\n\t\t\tq->entry[q->tail].done = false;\n\n\t\t\tmt76_queue_tx_complete(dev, q, &entry);\n\t\t}\n\n\t\tif (!q->queued)\n\t\t\twake_up(&dev->tx_wait);\n\n\t\tmt76_worker_schedule(&dev->tx_worker);\n\t}\n\n\tif (dev->drv->tx_status_data &&\n\t    !test_and_set_bit(MT76_READING_STATS, &dev->phy.state))\n\t\tqueue_work(dev->wq, &dev->usb.stat_work);\n}\n\nstatic void mt76u_tx_status_data(struct work_struct *work)\n{\n\tstruct mt76_usb *usb;\n\tstruct mt76_dev *dev;\n\tu8 update = 1;\n\tu16 count = 0;\n\n\tusb = container_of(work, struct mt76_usb, stat_work);\n\tdev = container_of(usb, struct mt76_dev, usb);\n\n\twhile (true) {\n\t\tif (test_bit(MT76_REMOVED, &dev->phy.state))\n\t\t\tbreak;\n\n\t\tif (!dev->drv->tx_status_data(dev, &update))\n\t\t\tbreak;\n\t\tcount++;\n\t}\n\n\tif (count && test_bit(MT76_STATE_RUNNING, &dev->phy.state))\n\t\tqueue_work(dev->wq, &usb->stat_work);\n\telse\n\t\tclear_bit(MT76_READING_STATS, &dev->phy.state);\n}\n\nstatic void mt76u_complete_tx(struct urb *urb)\n{\n\tstruct mt76_dev *dev = dev_get_drvdata(&urb->dev->dev);\n\tstruct mt76_queue_entry *e = urb->context;\n\n\tif (mt76u_urb_error(urb))\n\t\tdev_err(dev->dev, \"tx urb failed: %d\\n\", urb->status);\n\te->done = true;\n\n\tmt76_worker_schedule(&dev->usb.status_worker);\n}\n\nstatic int\nmt76u_tx_setup_buffers(struct mt76_dev *dev, struct sk_buff *skb,\n\t\t       struct urb *urb)\n{\n\turb->transfer_buffer_length = skb->len;\n\n\tif (!dev->usb.sg_en) {\n\t\turb->transfer_buffer = skb->data;\n\t\treturn 0;\n\t}\n\n\tsg_init_table(urb->sg, MT_TX_SG_MAX_SIZE);\n\turb->num_sgs = skb_to_sgvec(skb, urb->sg, 0, skb->len);\n\tif (!urb->num_sgs)\n\t\treturn -ENOMEM;\n\n\treturn urb->num_sgs;\n}\n\nstatic int\nmt76u_tx_queue_skb(struct mt76_dev *dev, struct mt76_queue *q,\n\t\t   enum mt76_txq_id qid, struct sk_buff *skb,\n\t\t   struct mt76_wcid *wcid, struct ieee80211_sta *sta)\n{\n\tstruct mt76_tx_info tx_info = {\n\t\t.skb = skb,\n\t};\n\tu16 idx = q->head;\n\tint err;\n\n\tif (q->queued == q->ndesc)\n\t\treturn -ENOSPC;\n\n\tskb->prev = skb->next = NULL;\n\terr = dev->drv->tx_prepare_skb(dev, NULL, qid, wcid, sta, &tx_info);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = mt76u_tx_setup_buffers(dev, tx_info.skb, q->entry[idx].urb);\n\tif (err < 0)\n\t\treturn err;\n\n\tmt76u_fill_bulk_urb(dev, USB_DIR_OUT, q2ep(q->hw_idx),\n\t\t\t    q->entry[idx].urb, mt76u_complete_tx,\n\t\t\t    &q->entry[idx]);\n\n\tq->head = (q->head + 1) % q->ndesc;\n\tq->entry[idx].skb = tx_info.skb;\n\tq->entry[idx].wcid = 0xffff;\n\tq->queued++;\n\n\treturn idx;\n}\n\nstatic void mt76u_tx_kick(struct mt76_dev *dev, struct mt76_queue *q)\n{\n\tstruct urb *urb;\n\tint err;\n\n\twhile (q->first != q->head) {\n\t\turb = q->entry[q->first].urb;\n\n\t\ttrace_submit_urb(dev, urb);\n\t\terr = usb_submit_urb(urb, GFP_ATOMIC);\n\t\tif (err < 0) {\n\t\t\tif (err == -ENODEV)\n\t\t\t\tset_bit(MT76_REMOVED, &dev->phy.state);\n\t\t\telse\n\t\t\t\tdev_err(dev->dev, \"tx urb submit failed:%d\\n\",\n\t\t\t\t\terr);\n\t\t\tbreak;\n\t\t}\n\t\tq->first = (q->first + 1) % q->ndesc;\n\t}\n}\n\nstatic u8 mt76u_ac_to_hwq(struct mt76_dev *dev, u8 ac)\n{\n\tif (mt76_chip(dev) == 0x7663) {\n\t\tstatic const u8 lmac_queue_map[] = {\n\t\t\t \n\t\t\t[IEEE80211_AC_BK] = 0,\n\t\t\t[IEEE80211_AC_BE] = 1,\n\t\t\t[IEEE80211_AC_VI] = 2,\n\t\t\t[IEEE80211_AC_VO] = 4,\n\t\t};\n\n\t\tif (WARN_ON(ac >= ARRAY_SIZE(lmac_queue_map)))\n\t\t\treturn 1;  \n\n\t\treturn lmac_queue_map[ac];\n\t}\n\n\treturn mt76_ac_to_hwq(ac);\n}\n\nstatic int mt76u_alloc_tx(struct mt76_dev *dev)\n{\n\tstruct mt76_queue *q;\n\tint i, j, err;\n\n\tfor (i = 0; i <= MT_TXQ_PSD; i++) {\n\t\tif (i >= IEEE80211_NUM_ACS) {\n\t\t\tdev->phy.q_tx[i] = dev->phy.q_tx[0];\n\t\t\tcontinue;\n\t\t}\n\n\t\tq = devm_kzalloc(dev->dev, sizeof(*q), GFP_KERNEL);\n\t\tif (!q)\n\t\t\treturn -ENOMEM;\n\n\t\tspin_lock_init(&q->lock);\n\t\tq->hw_idx = mt76u_ac_to_hwq(dev, i);\n\n\t\tdev->phy.q_tx[i] = q;\n\n\t\tq->entry = devm_kcalloc(dev->dev,\n\t\t\t\t\tMT_NUM_TX_ENTRIES, sizeof(*q->entry),\n\t\t\t\t\tGFP_KERNEL);\n\t\tif (!q->entry)\n\t\t\treturn -ENOMEM;\n\n\t\tq->ndesc = MT_NUM_TX_ENTRIES;\n\t\tfor (j = 0; j < q->ndesc; j++) {\n\t\t\terr = mt76u_urb_alloc(dev, &q->entry[j],\n\t\t\t\t\t      MT_TX_SG_MAX_SIZE);\n\t\t\tif (err < 0)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic void mt76u_free_tx(struct mt76_dev *dev)\n{\n\tint i;\n\n\tmt76_worker_teardown(&dev->usb.status_worker);\n\n\tfor (i = 0; i < IEEE80211_NUM_ACS; i++) {\n\t\tstruct mt76_queue *q;\n\t\tint j;\n\n\t\tq = dev->phy.q_tx[i];\n\t\tif (!q)\n\t\t\tcontinue;\n\n\t\tfor (j = 0; j < q->ndesc; j++) {\n\t\t\tusb_free_urb(q->entry[j].urb);\n\t\t\tq->entry[j].urb = NULL;\n\t\t}\n\t}\n}\n\nvoid mt76u_stop_tx(struct mt76_dev *dev)\n{\n\tint ret;\n\n\tmt76_worker_disable(&dev->usb.status_worker);\n\n\tret = wait_event_timeout(dev->tx_wait, !mt76_has_tx_pending(&dev->phy),\n\t\t\t\t HZ / 5);\n\tif (!ret) {\n\t\tstruct mt76_queue_entry entry;\n\t\tstruct mt76_queue *q;\n\t\tint i, j;\n\n\t\tdev_err(dev->dev, \"timed out waiting for pending tx\\n\");\n\n\t\tfor (i = 0; i < IEEE80211_NUM_ACS; i++) {\n\t\t\tq = dev->phy.q_tx[i];\n\t\t\tif (!q)\n\t\t\t\tcontinue;\n\n\t\t\tfor (j = 0; j < q->ndesc; j++)\n\t\t\t\tusb_kill_urb(q->entry[j].urb);\n\t\t}\n\n\t\tmt76_worker_disable(&dev->tx_worker);\n\n\t\t \n\t\tfor (i = 0; i < IEEE80211_NUM_ACS; i++) {\n\t\t\tq = dev->phy.q_tx[i];\n\t\t\tif (!q)\n\t\t\t\tcontinue;\n\n\t\t\twhile (q->queued > 0) {\n\t\t\t\tentry = q->entry[q->tail];\n\t\t\t\tq->entry[q->tail].done = false;\n\t\t\t\tmt76_queue_tx_complete(dev, q, &entry);\n\t\t\t}\n\t\t}\n\n\t\tmt76_worker_enable(&dev->tx_worker);\n\t}\n\n\tcancel_work_sync(&dev->usb.stat_work);\n\tclear_bit(MT76_READING_STATS, &dev->phy.state);\n\n\tmt76_worker_enable(&dev->usb.status_worker);\n\n\tmt76_tx_status_check(dev, true);\n}\nEXPORT_SYMBOL_GPL(mt76u_stop_tx);\n\nvoid mt76u_queues_deinit(struct mt76_dev *dev)\n{\n\tmt76u_stop_rx(dev);\n\tmt76u_stop_tx(dev);\n\n\tmt76u_free_rx(dev);\n\tmt76u_free_tx(dev);\n}\nEXPORT_SYMBOL_GPL(mt76u_queues_deinit);\n\nint mt76u_alloc_queues(struct mt76_dev *dev)\n{\n\tint err;\n\n\terr = mt76u_alloc_rx_queue(dev, MT_RXQ_MAIN);\n\tif (err < 0)\n\t\treturn err;\n\n\treturn mt76u_alloc_tx(dev);\n}\nEXPORT_SYMBOL_GPL(mt76u_alloc_queues);\n\nstatic const struct mt76_queue_ops usb_queue_ops = {\n\t.tx_queue_skb = mt76u_tx_queue_skb,\n\t.kick = mt76u_tx_kick,\n};\n\nint __mt76u_init(struct mt76_dev *dev, struct usb_interface *intf,\n\t\t struct mt76_bus_ops *ops)\n{\n\tstruct usb_device *udev = interface_to_usbdev(intf);\n\tstruct mt76_usb *usb = &dev->usb;\n\tint err;\n\n\tINIT_WORK(&usb->stat_work, mt76u_tx_status_data);\n\n\tusb->data_len = usb_maxpacket(udev, usb_sndctrlpipe(udev, 0));\n\tif (usb->data_len < 32)\n\t\tusb->data_len = 32;\n\n\tusb->data = devm_kmalloc(dev->dev, usb->data_len, GFP_KERNEL);\n\tif (!usb->data)\n\t\treturn -ENOMEM;\n\n\tmutex_init(&usb->usb_ctrl_mtx);\n\tdev->bus = ops;\n\tdev->queue_ops = &usb_queue_ops;\n\n\tdev_set_drvdata(&udev->dev, dev);\n\n\tusb->sg_en = mt76u_check_sg(dev);\n\n\terr = mt76u_set_endpoints(intf, usb);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = mt76_worker_setup(dev->hw, &usb->rx_worker, mt76u_rx_worker,\n\t\t\t\t\"usb-rx\");\n\tif (err)\n\t\treturn err;\n\n\terr = mt76_worker_setup(dev->hw, &usb->status_worker,\n\t\t\t\tmt76u_status_worker, \"usb-status\");\n\tif (err)\n\t\treturn err;\n\n\tsched_set_fifo_low(usb->rx_worker.task);\n\tsched_set_fifo_low(usb->status_worker.task);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(__mt76u_init);\n\nint mt76u_init(struct mt76_dev *dev, struct usb_interface *intf)\n{\n\tstatic struct mt76_bus_ops bus_ops = {\n\t\t.rr = mt76u_rr,\n\t\t.wr = mt76u_wr,\n\t\t.rmw = mt76u_rmw,\n\t\t.read_copy = mt76u_read_copy,\n\t\t.write_copy = mt76u_copy,\n\t\t.wr_rp = mt76u_wr_rp,\n\t\t.rd_rp = mt76u_rd_rp,\n\t\t.type = MT76_BUS_USB,\n\t};\n\n\treturn __mt76u_init(dev, intf, &bus_ops);\n}\nEXPORT_SYMBOL_GPL(mt76u_init);\n\nMODULE_AUTHOR(\"Lorenzo Bianconi <lorenzo.bianconi83@gmail.com>\");\nMODULE_LICENSE(\"Dual BSD/GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}