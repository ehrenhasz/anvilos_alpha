{
  "module_name": "virtio_net.c",
  "hash_id": "aa57d3c4f7fe0b7307fa48451f116a2d2e8b05620e43909dac44e222be7b243a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/virtio_net.c",
  "human_readable_source": "\n \n\n#include <linux/netdevice.h>\n#include <linux/etherdevice.h>\n#include <linux/ethtool.h>\n#include <linux/module.h>\n#include <linux/virtio.h>\n#include <linux/virtio_net.h>\n#include <linux/bpf.h>\n#include <linux/bpf_trace.h>\n#include <linux/scatterlist.h>\n#include <linux/if_vlan.h>\n#include <linux/slab.h>\n#include <linux/cpu.h>\n#include <linux/average.h>\n#include <linux/filter.h>\n#include <linux/kernel.h>\n#include <net/route.h>\n#include <net/xdp.h>\n#include <net/net_failover.h>\n#include <net/netdev_rx_queue.h>\n\nstatic int napi_weight = NAPI_POLL_WEIGHT;\nmodule_param(napi_weight, int, 0444);\n\nstatic bool csum = true, gso = true, napi_tx = true;\nmodule_param(csum, bool, 0444);\nmodule_param(gso, bool, 0444);\nmodule_param(napi_tx, bool, 0644);\n\n \n#define GOOD_PACKET_LEN (ETH_HLEN + VLAN_HLEN + ETH_DATA_LEN)\n#define GOOD_COPY_LEN\t128\n\n#define VIRTNET_RX_PAD (NET_IP_ALIGN + NET_SKB_PAD)\n\n \n#define VIRTIO_XDP_HEADROOM 256\n\n \n#define VIRTIO_XDP_TX\t\tBIT(0)\n#define VIRTIO_XDP_REDIR\tBIT(1)\n\n#define VIRTIO_XDP_FLAG\tBIT(0)\n\n \nDECLARE_EWMA(pkt_len, 0, 64)\n\n#define VIRTNET_DRIVER_VERSION \"1.0.0\"\n\nstatic const unsigned long guest_offloads[] = {\n\tVIRTIO_NET_F_GUEST_TSO4,\n\tVIRTIO_NET_F_GUEST_TSO6,\n\tVIRTIO_NET_F_GUEST_ECN,\n\tVIRTIO_NET_F_GUEST_UFO,\n\tVIRTIO_NET_F_GUEST_CSUM,\n\tVIRTIO_NET_F_GUEST_USO4,\n\tVIRTIO_NET_F_GUEST_USO6,\n\tVIRTIO_NET_F_GUEST_HDRLEN\n};\n\n#define GUEST_OFFLOAD_GRO_HW_MASK ((1ULL << VIRTIO_NET_F_GUEST_TSO4) | \\\n\t\t\t\t(1ULL << VIRTIO_NET_F_GUEST_TSO6) | \\\n\t\t\t\t(1ULL << VIRTIO_NET_F_GUEST_ECN)  | \\\n\t\t\t\t(1ULL << VIRTIO_NET_F_GUEST_UFO)  | \\\n\t\t\t\t(1ULL << VIRTIO_NET_F_GUEST_USO4) | \\\n\t\t\t\t(1ULL << VIRTIO_NET_F_GUEST_USO6))\n\nstruct virtnet_stat_desc {\n\tchar desc[ETH_GSTRING_LEN];\n\tsize_t offset;\n};\n\nstruct virtnet_sq_stats {\n\tstruct u64_stats_sync syncp;\n\tu64_stats_t packets;\n\tu64_stats_t bytes;\n\tu64_stats_t xdp_tx;\n\tu64_stats_t xdp_tx_drops;\n\tu64_stats_t kicks;\n\tu64_stats_t tx_timeouts;\n};\n\nstruct virtnet_rq_stats {\n\tstruct u64_stats_sync syncp;\n\tu64_stats_t packets;\n\tu64_stats_t bytes;\n\tu64_stats_t drops;\n\tu64_stats_t xdp_packets;\n\tu64_stats_t xdp_tx;\n\tu64_stats_t xdp_redirects;\n\tu64_stats_t xdp_drops;\n\tu64_stats_t kicks;\n};\n\n#define VIRTNET_SQ_STAT(m)\toffsetof(struct virtnet_sq_stats, m)\n#define VIRTNET_RQ_STAT(m)\toffsetof(struct virtnet_rq_stats, m)\n\nstatic const struct virtnet_stat_desc virtnet_sq_stats_desc[] = {\n\t{ \"packets\",\t\tVIRTNET_SQ_STAT(packets) },\n\t{ \"bytes\",\t\tVIRTNET_SQ_STAT(bytes) },\n\t{ \"xdp_tx\",\t\tVIRTNET_SQ_STAT(xdp_tx) },\n\t{ \"xdp_tx_drops\",\tVIRTNET_SQ_STAT(xdp_tx_drops) },\n\t{ \"kicks\",\t\tVIRTNET_SQ_STAT(kicks) },\n\t{ \"tx_timeouts\",\tVIRTNET_SQ_STAT(tx_timeouts) },\n};\n\nstatic const struct virtnet_stat_desc virtnet_rq_stats_desc[] = {\n\t{ \"packets\",\t\tVIRTNET_RQ_STAT(packets) },\n\t{ \"bytes\",\t\tVIRTNET_RQ_STAT(bytes) },\n\t{ \"drops\",\t\tVIRTNET_RQ_STAT(drops) },\n\t{ \"xdp_packets\",\tVIRTNET_RQ_STAT(xdp_packets) },\n\t{ \"xdp_tx\",\t\tVIRTNET_RQ_STAT(xdp_tx) },\n\t{ \"xdp_redirects\",\tVIRTNET_RQ_STAT(xdp_redirects) },\n\t{ \"xdp_drops\",\t\tVIRTNET_RQ_STAT(xdp_drops) },\n\t{ \"kicks\",\t\tVIRTNET_RQ_STAT(kicks) },\n};\n\n#define VIRTNET_SQ_STATS_LEN\tARRAY_SIZE(virtnet_sq_stats_desc)\n#define VIRTNET_RQ_STATS_LEN\tARRAY_SIZE(virtnet_rq_stats_desc)\n\nstruct virtnet_interrupt_coalesce {\n\tu32 max_packets;\n\tu32 max_usecs;\n};\n\n \nstruct virtnet_rq_dma {\n\tdma_addr_t addr;\n\tu32 ref;\n\tu16 len;\n\tu16 need_sync;\n};\n\n \nstruct send_queue {\n\t \n\tstruct virtqueue *vq;\n\n\t \n\tstruct scatterlist sg[MAX_SKB_FRAGS + 2];\n\n\t \n\tchar name[16];\n\n\tstruct virtnet_sq_stats stats;\n\n\tstruct virtnet_interrupt_coalesce intr_coal;\n\n\tstruct napi_struct napi;\n\n\t \n\tbool reset;\n};\n\n \nstruct receive_queue {\n\t \n\tstruct virtqueue *vq;\n\n\tstruct napi_struct napi;\n\n\tstruct bpf_prog __rcu *xdp_prog;\n\n\tstruct virtnet_rq_stats stats;\n\n\tstruct virtnet_interrupt_coalesce intr_coal;\n\n\t \n\tstruct page *pages;\n\n\t \n\tstruct ewma_pkt_len mrg_avg_pkt_len;\n\n\t \n\tstruct page_frag alloc_frag;\n\n\t \n\tstruct scatterlist sg[MAX_SKB_FRAGS + 2];\n\n\t \n\tunsigned int min_buf_len;\n\n\t \n\tchar name[16];\n\n\tstruct xdp_rxq_info xdp_rxq;\n\n\t \n\tstruct virtnet_rq_dma *last_dma;\n\n\t \n\tbool do_dma;\n};\n\n \n#define VIRTIO_NET_RSS_MAX_KEY_SIZE     40\n#define VIRTIO_NET_RSS_MAX_TABLE_LEN    128\nstruct virtio_net_ctrl_rss {\n\tu32 hash_types;\n\tu16 indirection_table_mask;\n\tu16 unclassified_queue;\n\tu16 indirection_table[VIRTIO_NET_RSS_MAX_TABLE_LEN];\n\tu16 max_tx_vq;\n\tu8 hash_key_length;\n\tu8 key[VIRTIO_NET_RSS_MAX_KEY_SIZE];\n};\n\n \nstruct control_buf {\n\tstruct virtio_net_ctrl_hdr hdr;\n\tvirtio_net_ctrl_ack status;\n\tstruct virtio_net_ctrl_mq mq;\n\tu8 promisc;\n\tu8 allmulti;\n\t__virtio16 vid;\n\t__virtio64 offloads;\n\tstruct virtio_net_ctrl_rss rss;\n\tstruct virtio_net_ctrl_coal_tx coal_tx;\n\tstruct virtio_net_ctrl_coal_rx coal_rx;\n\tstruct virtio_net_ctrl_coal_vq coal_vq;\n};\n\nstruct virtnet_info {\n\tstruct virtio_device *vdev;\n\tstruct virtqueue *cvq;\n\tstruct net_device *dev;\n\tstruct send_queue *sq;\n\tstruct receive_queue *rq;\n\tunsigned int status;\n\n\t \n\tu16 max_queue_pairs;\n\n\t \n\tu16 curr_queue_pairs;\n\n\t \n\tu16 xdp_queue_pairs;\n\n\t \n\tbool xdp_enabled;\n\n\t \n\tbool big_packets;\n\n\t \n\tunsigned int big_packets_num_skbfrags;\n\n\t \n\tbool mergeable_rx_bufs;\n\n\t \n\tbool has_rss;\n\tbool has_rss_hash_report;\n\tu8 rss_key_size;\n\tu16 rss_indir_table_size;\n\tu32 rss_hash_types_supported;\n\tu32 rss_hash_types_saved;\n\n\t \n\tbool has_cvq;\n\n\t \n\tbool any_header_sg;\n\n\t \n\tu8 hdr_len;\n\n\t \n\tstruct delayed_work refill;\n\n\t \n\tbool refill_enabled;\n\n\t \n\tspinlock_t refill_lock;\n\n\t \n\tstruct work_struct config_work;\n\n\t \n\tbool affinity_hint_set;\n\n\t \n\tstruct hlist_node node;\n\tstruct hlist_node node_dead;\n\n\tstruct control_buf *ctrl;\n\n\t \n\tu8 duplex;\n\tu32 speed;\n\n\t \n\tstruct virtnet_interrupt_coalesce intr_coal_tx;\n\tstruct virtnet_interrupt_coalesce intr_coal_rx;\n\n\tunsigned long guest_offloads;\n\tunsigned long guest_offloads_capable;\n\n\t \n\tstruct failover *failover;\n};\n\nstruct padded_vnet_hdr {\n\tstruct virtio_net_hdr_v1_hash hdr;\n\t \n\tchar padding[12];\n};\n\nstruct virtio_net_common_hdr {\n\tunion {\n\t\tstruct virtio_net_hdr hdr;\n\t\tstruct virtio_net_hdr_mrg_rxbuf\tmrg_hdr;\n\t\tstruct virtio_net_hdr_v1_hash hash_v1_hdr;\n\t};\n};\n\nstatic void virtnet_sq_free_unused_buf(struct virtqueue *vq, void *buf);\n\nstatic bool is_xdp_frame(void *ptr)\n{\n\treturn (unsigned long)ptr & VIRTIO_XDP_FLAG;\n}\n\nstatic void *xdp_to_ptr(struct xdp_frame *ptr)\n{\n\treturn (void *)((unsigned long)ptr | VIRTIO_XDP_FLAG);\n}\n\nstatic struct xdp_frame *ptr_to_xdp(void *ptr)\n{\n\treturn (struct xdp_frame *)((unsigned long)ptr & ~VIRTIO_XDP_FLAG);\n}\n\n \nstatic int vq2txq(struct virtqueue *vq)\n{\n\treturn (vq->index - 1) / 2;\n}\n\nstatic int txq2vq(int txq)\n{\n\treturn txq * 2 + 1;\n}\n\nstatic int vq2rxq(struct virtqueue *vq)\n{\n\treturn vq->index / 2;\n}\n\nstatic int rxq2vq(int rxq)\n{\n\treturn rxq * 2;\n}\n\nstatic inline struct virtio_net_common_hdr *\nskb_vnet_common_hdr(struct sk_buff *skb)\n{\n\treturn (struct virtio_net_common_hdr *)skb->cb;\n}\n\n \nstatic void give_pages(struct receive_queue *rq, struct page *page)\n{\n\tstruct page *end;\n\n\t \n\tfor (end = page; end->private; end = (struct page *)end->private);\n\tend->private = (unsigned long)rq->pages;\n\trq->pages = page;\n}\n\nstatic struct page *get_a_page(struct receive_queue *rq, gfp_t gfp_mask)\n{\n\tstruct page *p = rq->pages;\n\n\tif (p) {\n\t\trq->pages = (struct page *)p->private;\n\t\t \n\t\tp->private = 0;\n\t} else\n\t\tp = alloc_page(gfp_mask);\n\treturn p;\n}\n\nstatic void virtnet_rq_free_buf(struct virtnet_info *vi,\n\t\t\t\tstruct receive_queue *rq, void *buf)\n{\n\tif (vi->mergeable_rx_bufs)\n\t\tput_page(virt_to_head_page(buf));\n\telse if (vi->big_packets)\n\t\tgive_pages(rq, buf);\n\telse\n\t\tput_page(virt_to_head_page(buf));\n}\n\nstatic void enable_delayed_refill(struct virtnet_info *vi)\n{\n\tspin_lock_bh(&vi->refill_lock);\n\tvi->refill_enabled = true;\n\tspin_unlock_bh(&vi->refill_lock);\n}\n\nstatic void disable_delayed_refill(struct virtnet_info *vi)\n{\n\tspin_lock_bh(&vi->refill_lock);\n\tvi->refill_enabled = false;\n\tspin_unlock_bh(&vi->refill_lock);\n}\n\nstatic void virtqueue_napi_schedule(struct napi_struct *napi,\n\t\t\t\t    struct virtqueue *vq)\n{\n\tif (napi_schedule_prep(napi)) {\n\t\tvirtqueue_disable_cb(vq);\n\t\t__napi_schedule(napi);\n\t}\n}\n\nstatic void virtqueue_napi_complete(struct napi_struct *napi,\n\t\t\t\t    struct virtqueue *vq, int processed)\n{\n\tint opaque;\n\n\topaque = virtqueue_enable_cb_prepare(vq);\n\tif (napi_complete_done(napi, processed)) {\n\t\tif (unlikely(virtqueue_poll(vq, opaque)))\n\t\t\tvirtqueue_napi_schedule(napi, vq);\n\t} else {\n\t\tvirtqueue_disable_cb(vq);\n\t}\n}\n\nstatic void skb_xmit_done(struct virtqueue *vq)\n{\n\tstruct virtnet_info *vi = vq->vdev->priv;\n\tstruct napi_struct *napi = &vi->sq[vq2txq(vq)].napi;\n\n\t \n\tvirtqueue_disable_cb(vq);\n\n\tif (napi->weight)\n\t\tvirtqueue_napi_schedule(napi, vq);\n\telse\n\t\t \n\t\tnetif_wake_subqueue(vi->dev, vq2txq(vq));\n}\n\n#define MRG_CTX_HEADER_SHIFT 22\nstatic void *mergeable_len_to_ctx(unsigned int truesize,\n\t\t\t\t  unsigned int headroom)\n{\n\treturn (void *)(unsigned long)((headroom << MRG_CTX_HEADER_SHIFT) | truesize);\n}\n\nstatic unsigned int mergeable_ctx_to_headroom(void *mrg_ctx)\n{\n\treturn (unsigned long)mrg_ctx >> MRG_CTX_HEADER_SHIFT;\n}\n\nstatic unsigned int mergeable_ctx_to_truesize(void *mrg_ctx)\n{\n\treturn (unsigned long)mrg_ctx & ((1 << MRG_CTX_HEADER_SHIFT) - 1);\n}\n\nstatic struct sk_buff *virtnet_build_skb(void *buf, unsigned int buflen,\n\t\t\t\t\t unsigned int headroom,\n\t\t\t\t\t unsigned int len)\n{\n\tstruct sk_buff *skb;\n\n\tskb = build_skb(buf, buflen);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tskb_reserve(skb, headroom);\n\tskb_put(skb, len);\n\n\treturn skb;\n}\n\n \nstatic struct sk_buff *page_to_skb(struct virtnet_info *vi,\n\t\t\t\t   struct receive_queue *rq,\n\t\t\t\t   struct page *page, unsigned int offset,\n\t\t\t\t   unsigned int len, unsigned int truesize,\n\t\t\t\t   unsigned int headroom)\n{\n\tstruct sk_buff *skb;\n\tstruct virtio_net_common_hdr *hdr;\n\tunsigned int copy, hdr_len, hdr_padded_len;\n\tstruct page *page_to_free = NULL;\n\tint tailroom, shinfo_size;\n\tchar *p, *hdr_p, *buf;\n\n\tp = page_address(page) + offset;\n\thdr_p = p;\n\n\thdr_len = vi->hdr_len;\n\tif (vi->mergeable_rx_bufs)\n\t\thdr_padded_len = hdr_len;\n\telse\n\t\thdr_padded_len = sizeof(struct padded_vnet_hdr);\n\n\tbuf = p - headroom;\n\tlen -= hdr_len;\n\toffset += hdr_padded_len;\n\tp += hdr_padded_len;\n\ttailroom = truesize - headroom  - hdr_padded_len - len;\n\n\tshinfo_size = SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\t \n\tif (!NET_IP_ALIGN && len > GOOD_COPY_LEN && tailroom >= shinfo_size) {\n\t\tskb = virtnet_build_skb(buf, truesize, p - buf, len);\n\t\tif (unlikely(!skb))\n\t\t\treturn NULL;\n\n\t\tpage = (struct page *)page->private;\n\t\tif (page)\n\t\t\tgive_pages(rq, page);\n\t\tgoto ok;\n\t}\n\n\t \n\tskb = napi_alloc_skb(&rq->napi, GOOD_COPY_LEN);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\t \n\tif (len <= skb_tailroom(skb))\n\t\tcopy = len;\n\telse\n\t\tcopy = ETH_HLEN;\n\tskb_put_data(skb, p, copy);\n\n\tlen -= copy;\n\toffset += copy;\n\n\tif (vi->mergeable_rx_bufs) {\n\t\tif (len)\n\t\t\tskb_add_rx_frag(skb, 0, page, offset, len, truesize);\n\t\telse\n\t\t\tpage_to_free = page;\n\t\tgoto ok;\n\t}\n\n\t \n\tif (unlikely(len > MAX_SKB_FRAGS * PAGE_SIZE)) {\n\t\tnet_dbg_ratelimited(\"%s: too much data\\n\", skb->dev->name);\n\t\tdev_kfree_skb(skb);\n\t\treturn NULL;\n\t}\n\tBUG_ON(offset >= PAGE_SIZE);\n\twhile (len) {\n\t\tunsigned int frag_size = min((unsigned)PAGE_SIZE - offset, len);\n\t\tskb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page, offset,\n\t\t\t\tfrag_size, truesize);\n\t\tlen -= frag_size;\n\t\tpage = (struct page *)page->private;\n\t\toffset = 0;\n\t}\n\n\tif (page)\n\t\tgive_pages(rq, page);\n\nok:\n\thdr = skb_vnet_common_hdr(skb);\n\tmemcpy(hdr, hdr_p, hdr_len);\n\tif (page_to_free)\n\t\tput_page(page_to_free);\n\n\treturn skb;\n}\n\nstatic void virtnet_rq_unmap(struct receive_queue *rq, void *buf, u32 len)\n{\n\tstruct page *page = virt_to_head_page(buf);\n\tstruct virtnet_rq_dma *dma;\n\tvoid *head;\n\tint offset;\n\n\thead = page_address(page);\n\n\tdma = head;\n\n\t--dma->ref;\n\n\tif (dma->need_sync && len) {\n\t\toffset = buf - (head + sizeof(*dma));\n\n\t\tvirtqueue_dma_sync_single_range_for_cpu(rq->vq, dma->addr,\n\t\t\t\t\t\t\toffset, len,\n\t\t\t\t\t\t\tDMA_FROM_DEVICE);\n\t}\n\n\tif (dma->ref)\n\t\treturn;\n\n\tvirtqueue_dma_unmap_single_attrs(rq->vq, dma->addr, dma->len,\n\t\t\t\t\t DMA_FROM_DEVICE, DMA_ATTR_SKIP_CPU_SYNC);\n\tput_page(page);\n}\n\nstatic void *virtnet_rq_get_buf(struct receive_queue *rq, u32 *len, void **ctx)\n{\n\tvoid *buf;\n\n\tbuf = virtqueue_get_buf_ctx(rq->vq, len, ctx);\n\tif (buf && rq->do_dma)\n\t\tvirtnet_rq_unmap(rq, buf, *len);\n\n\treturn buf;\n}\n\nstatic void virtnet_rq_init_one_sg(struct receive_queue *rq, void *buf, u32 len)\n{\n\tstruct virtnet_rq_dma *dma;\n\tdma_addr_t addr;\n\tu32 offset;\n\tvoid *head;\n\n\tif (!rq->do_dma) {\n\t\tsg_init_one(rq->sg, buf, len);\n\t\treturn;\n\t}\n\n\thead = page_address(rq->alloc_frag.page);\n\n\toffset = buf - head;\n\n\tdma = head;\n\n\taddr = dma->addr - sizeof(*dma) + offset;\n\n\tsg_init_table(rq->sg, 1);\n\trq->sg[0].dma_address = addr;\n\trq->sg[0].length = len;\n}\n\nstatic void *virtnet_rq_alloc(struct receive_queue *rq, u32 size, gfp_t gfp)\n{\n\tstruct page_frag *alloc_frag = &rq->alloc_frag;\n\tstruct virtnet_rq_dma *dma;\n\tvoid *buf, *head;\n\tdma_addr_t addr;\n\n\tif (unlikely(!skb_page_frag_refill(size, alloc_frag, gfp)))\n\t\treturn NULL;\n\n\thead = page_address(alloc_frag->page);\n\n\tif (rq->do_dma) {\n\t\tdma = head;\n\n\t\t \n\t\tif (!alloc_frag->offset) {\n\t\t\tif (rq->last_dma) {\n\t\t\t\t \n\t\t\t\tvirtnet_rq_unmap(rq, rq->last_dma, 0);\n\t\t\t\trq->last_dma = NULL;\n\t\t\t}\n\n\t\t\tdma->len = alloc_frag->size - sizeof(*dma);\n\n\t\t\taddr = virtqueue_dma_map_single_attrs(rq->vq, dma + 1,\n\t\t\t\t\t\t\t      dma->len, DMA_FROM_DEVICE, 0);\n\t\t\tif (virtqueue_dma_mapping_error(rq->vq, addr))\n\t\t\t\treturn NULL;\n\n\t\t\tdma->addr = addr;\n\t\t\tdma->need_sync = virtqueue_dma_need_sync(rq->vq, addr);\n\n\t\t\t \n\t\t\tget_page(alloc_frag->page);\n\t\t\tdma->ref = 1;\n\t\t\talloc_frag->offset = sizeof(*dma);\n\n\t\t\trq->last_dma = dma;\n\t\t}\n\n\t\t++dma->ref;\n\t}\n\n\tbuf = head + alloc_frag->offset;\n\n\tget_page(alloc_frag->page);\n\talloc_frag->offset += size;\n\n\treturn buf;\n}\n\nstatic void virtnet_rq_set_premapped(struct virtnet_info *vi)\n{\n\tint i;\n\n\t \n\tif (!vi->mergeable_rx_bufs && vi->big_packets)\n\t\treturn;\n\n\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\tif (virtqueue_set_dma_premapped(vi->rq[i].vq))\n\t\t\tcontinue;\n\n\t\tvi->rq[i].do_dma = true;\n\t}\n}\n\nstatic void virtnet_rq_unmap_free_buf(struct virtqueue *vq, void *buf)\n{\n\tstruct virtnet_info *vi = vq->vdev->priv;\n\tstruct receive_queue *rq;\n\tint i = vq2rxq(vq);\n\n\trq = &vi->rq[i];\n\n\tif (rq->do_dma)\n\t\tvirtnet_rq_unmap(rq, buf, 0);\n\n\tvirtnet_rq_free_buf(vi, rq, buf);\n}\n\nstatic void free_old_xmit_skbs(struct send_queue *sq, bool in_napi)\n{\n\tunsigned int len;\n\tunsigned int packets = 0;\n\tunsigned int bytes = 0;\n\tvoid *ptr;\n\n\twhile ((ptr = virtqueue_get_buf(sq->vq, &len)) != NULL) {\n\t\tif (likely(!is_xdp_frame(ptr))) {\n\t\t\tstruct sk_buff *skb = ptr;\n\n\t\t\tpr_debug(\"Sent skb %p\\n\", skb);\n\n\t\t\tbytes += skb->len;\n\t\t\tnapi_consume_skb(skb, in_napi);\n\t\t} else {\n\t\t\tstruct xdp_frame *frame = ptr_to_xdp(ptr);\n\n\t\t\tbytes += xdp_get_frame_len(frame);\n\t\t\txdp_return_frame(frame);\n\t\t}\n\t\tpackets++;\n\t}\n\n\t \n\tif (!packets)\n\t\treturn;\n\n\tu64_stats_update_begin(&sq->stats.syncp);\n\tu64_stats_add(&sq->stats.bytes, bytes);\n\tu64_stats_add(&sq->stats.packets, packets);\n\tu64_stats_update_end(&sq->stats.syncp);\n}\n\nstatic bool is_xdp_raw_buffer_queue(struct virtnet_info *vi, int q)\n{\n\tif (q < (vi->curr_queue_pairs - vi->xdp_queue_pairs))\n\t\treturn false;\n\telse if (q < vi->curr_queue_pairs)\n\t\treturn true;\n\telse\n\t\treturn false;\n}\n\nstatic void check_sq_full_and_disable(struct virtnet_info *vi,\n\t\t\t\t      struct net_device *dev,\n\t\t\t\t      struct send_queue *sq)\n{\n\tbool use_napi = sq->napi.weight;\n\tint qnum;\n\n\tqnum = sq - vi->sq;\n\n\t \n\tif (sq->vq->num_free < 2+MAX_SKB_FRAGS) {\n\t\tnetif_stop_subqueue(dev, qnum);\n\t\tif (use_napi) {\n\t\t\tif (unlikely(!virtqueue_enable_cb_delayed(sq->vq)))\n\t\t\t\tvirtqueue_napi_schedule(&sq->napi, sq->vq);\n\t\t} else if (unlikely(!virtqueue_enable_cb_delayed(sq->vq))) {\n\t\t\t \n\t\t\tfree_old_xmit_skbs(sq, false);\n\t\t\tif (sq->vq->num_free >= 2+MAX_SKB_FRAGS) {\n\t\t\t\tnetif_start_subqueue(dev, qnum);\n\t\t\t\tvirtqueue_disable_cb(sq->vq);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic int __virtnet_xdp_xmit_one(struct virtnet_info *vi,\n\t\t\t\t   struct send_queue *sq,\n\t\t\t\t   struct xdp_frame *xdpf)\n{\n\tstruct virtio_net_hdr_mrg_rxbuf *hdr;\n\tstruct skb_shared_info *shinfo;\n\tu8 nr_frags = 0;\n\tint err, i;\n\n\tif (unlikely(xdpf->headroom < vi->hdr_len))\n\t\treturn -EOVERFLOW;\n\n\tif (unlikely(xdp_frame_has_frags(xdpf))) {\n\t\tshinfo = xdp_get_shared_info_from_frame(xdpf);\n\t\tnr_frags = shinfo->nr_frags;\n\t}\n\n\t \n\txdpf->headroom -= vi->hdr_len;\n\txdpf->data -= vi->hdr_len;\n\t \n\thdr = xdpf->data;\n\tmemset(hdr, 0, vi->hdr_len);\n\txdpf->len   += vi->hdr_len;\n\n\tsg_init_table(sq->sg, nr_frags + 1);\n\tsg_set_buf(sq->sg, xdpf->data, xdpf->len);\n\tfor (i = 0; i < nr_frags; i++) {\n\t\tskb_frag_t *frag = &shinfo->frags[i];\n\n\t\tsg_set_page(&sq->sg[i + 1], skb_frag_page(frag),\n\t\t\t    skb_frag_size(frag), skb_frag_off(frag));\n\t}\n\n\terr = virtqueue_add_outbuf(sq->vq, sq->sg, nr_frags + 1,\n\t\t\t\t   xdp_to_ptr(xdpf), GFP_ATOMIC);\n\tif (unlikely(err))\n\t\treturn -ENOSPC;  \n\n\treturn 0;\n}\n\n \n#define virtnet_xdp_get_sq(vi) ({                                       \\\n\tint cpu = smp_processor_id();                                   \\\n\tstruct netdev_queue *txq;                                       \\\n\ttypeof(vi) v = (vi);                                            \\\n\tunsigned int qp;                                                \\\n\t\t\t\t\t\t\t\t\t\\\n\tif (v->curr_queue_pairs > nr_cpu_ids) {                         \\\n\t\tqp = v->curr_queue_pairs - v->xdp_queue_pairs;          \\\n\t\tqp += cpu;                                              \\\n\t\ttxq = netdev_get_tx_queue(v->dev, qp);                  \\\n\t\t__netif_tx_acquire(txq);                                \\\n\t} else {                                                        \\\n\t\tqp = cpu % v->curr_queue_pairs;                         \\\n\t\ttxq = netdev_get_tx_queue(v->dev, qp);                  \\\n\t\t__netif_tx_lock(txq, cpu);                              \\\n\t}                                                               \\\n\tv->sq + qp;                                                     \\\n})\n\n#define virtnet_xdp_put_sq(vi, q) {                                     \\\n\tstruct netdev_queue *txq;                                       \\\n\ttypeof(vi) v = (vi);                                            \\\n\t\t\t\t\t\t\t\t\t\\\n\ttxq = netdev_get_tx_queue(v->dev, (q) - v->sq);                 \\\n\tif (v->curr_queue_pairs > nr_cpu_ids)                           \\\n\t\t__netif_tx_release(txq);                                \\\n\telse                                                            \\\n\t\t__netif_tx_unlock(txq);                                 \\\n}\n\nstatic int virtnet_xdp_xmit(struct net_device *dev,\n\t\t\t    int n, struct xdp_frame **frames, u32 flags)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tstruct receive_queue *rq = vi->rq;\n\tstruct bpf_prog *xdp_prog;\n\tstruct send_queue *sq;\n\tunsigned int len;\n\tint packets = 0;\n\tint bytes = 0;\n\tint nxmit = 0;\n\tint kicks = 0;\n\tvoid *ptr;\n\tint ret;\n\tint i;\n\n\t \n\txdp_prog = rcu_access_pointer(rq->xdp_prog);\n\tif (!xdp_prog)\n\t\treturn -ENXIO;\n\n\tsq = virtnet_xdp_get_sq(vi);\n\n\tif (unlikely(flags & ~XDP_XMIT_FLAGS_MASK)) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\twhile ((ptr = virtqueue_get_buf(sq->vq, &len)) != NULL) {\n\t\tif (likely(is_xdp_frame(ptr))) {\n\t\t\tstruct xdp_frame *frame = ptr_to_xdp(ptr);\n\n\t\t\tbytes += xdp_get_frame_len(frame);\n\t\t\txdp_return_frame(frame);\n\t\t} else {\n\t\t\tstruct sk_buff *skb = ptr;\n\n\t\t\tbytes += skb->len;\n\t\t\tnapi_consume_skb(skb, false);\n\t\t}\n\t\tpackets++;\n\t}\n\n\tfor (i = 0; i < n; i++) {\n\t\tstruct xdp_frame *xdpf = frames[i];\n\n\t\tif (__virtnet_xdp_xmit_one(vi, sq, xdpf))\n\t\t\tbreak;\n\t\tnxmit++;\n\t}\n\tret = nxmit;\n\n\tif (!is_xdp_raw_buffer_queue(vi, sq - vi->sq))\n\t\tcheck_sq_full_and_disable(vi, dev, sq);\n\n\tif (flags & XDP_XMIT_FLUSH) {\n\t\tif (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq))\n\t\t\tkicks = 1;\n\t}\nout:\n\tu64_stats_update_begin(&sq->stats.syncp);\n\tu64_stats_add(&sq->stats.bytes, bytes);\n\tu64_stats_add(&sq->stats.packets, packets);\n\tu64_stats_add(&sq->stats.xdp_tx, n);\n\tu64_stats_add(&sq->stats.xdp_tx_drops, n - nxmit);\n\tu64_stats_add(&sq->stats.kicks, kicks);\n\tu64_stats_update_end(&sq->stats.syncp);\n\n\tvirtnet_xdp_put_sq(vi, sq);\n\treturn ret;\n}\n\nstatic void put_xdp_frags(struct xdp_buff *xdp)\n{\n\tstruct skb_shared_info *shinfo;\n\tstruct page *xdp_page;\n\tint i;\n\n\tif (xdp_buff_has_frags(xdp)) {\n\t\tshinfo = xdp_get_shared_info_from_buff(xdp);\n\t\tfor (i = 0; i < shinfo->nr_frags; i++) {\n\t\t\txdp_page = skb_frag_page(&shinfo->frags[i]);\n\t\t\tput_page(xdp_page);\n\t\t}\n\t}\n}\n\nstatic int virtnet_xdp_handler(struct bpf_prog *xdp_prog, struct xdp_buff *xdp,\n\t\t\t       struct net_device *dev,\n\t\t\t       unsigned int *xdp_xmit,\n\t\t\t       struct virtnet_rq_stats *stats)\n{\n\tstruct xdp_frame *xdpf;\n\tint err;\n\tu32 act;\n\n\tact = bpf_prog_run_xdp(xdp_prog, xdp);\n\tu64_stats_inc(&stats->xdp_packets);\n\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\treturn act;\n\n\tcase XDP_TX:\n\t\tu64_stats_inc(&stats->xdp_tx);\n\t\txdpf = xdp_convert_buff_to_frame(xdp);\n\t\tif (unlikely(!xdpf)) {\n\t\t\tnetdev_dbg(dev, \"convert buff to frame failed for xdp\\n\");\n\t\t\treturn XDP_DROP;\n\t\t}\n\n\t\terr = virtnet_xdp_xmit(dev, 1, &xdpf, 0);\n\t\tif (unlikely(!err)) {\n\t\t\txdp_return_frame_rx_napi(xdpf);\n\t\t} else if (unlikely(err < 0)) {\n\t\t\ttrace_xdp_exception(dev, xdp_prog, act);\n\t\t\treturn XDP_DROP;\n\t\t}\n\t\t*xdp_xmit |= VIRTIO_XDP_TX;\n\t\treturn act;\n\n\tcase XDP_REDIRECT:\n\t\tu64_stats_inc(&stats->xdp_redirects);\n\t\terr = xdp_do_redirect(dev, xdp, xdp_prog);\n\t\tif (err)\n\t\t\treturn XDP_DROP;\n\n\t\t*xdp_xmit |= VIRTIO_XDP_REDIR;\n\t\treturn act;\n\n\tdefault:\n\t\tbpf_warn_invalid_xdp_action(dev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_ABORTED:\n\t\ttrace_xdp_exception(dev, xdp_prog, act);\n\t\tfallthrough;\n\tcase XDP_DROP:\n\t\treturn XDP_DROP;\n\t}\n}\n\nstatic unsigned int virtnet_get_headroom(struct virtnet_info *vi)\n{\n\treturn vi->xdp_enabled ? VIRTIO_XDP_HEADROOM : 0;\n}\n\n \nstatic struct page *xdp_linearize_page(struct receive_queue *rq,\n\t\t\t\t       int *num_buf,\n\t\t\t\t       struct page *p,\n\t\t\t\t       int offset,\n\t\t\t\t       int page_off,\n\t\t\t\t       unsigned int *len)\n{\n\tint tailroom = SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tstruct page *page;\n\n\tif (page_off + *len + tailroom > PAGE_SIZE)\n\t\treturn NULL;\n\n\tpage = alloc_page(GFP_ATOMIC);\n\tif (!page)\n\t\treturn NULL;\n\n\tmemcpy(page_address(page) + page_off, page_address(p) + offset, *len);\n\tpage_off += *len;\n\n\twhile (--*num_buf) {\n\t\tunsigned int buflen;\n\t\tvoid *buf;\n\t\tint off;\n\n\t\tbuf = virtnet_rq_get_buf(rq, &buflen, NULL);\n\t\tif (unlikely(!buf))\n\t\t\tgoto err_buf;\n\n\t\tp = virt_to_head_page(buf);\n\t\toff = buf - page_address(p);\n\n\t\t \n\t\tif ((page_off + buflen + tailroom) > PAGE_SIZE) {\n\t\t\tput_page(p);\n\t\t\tgoto err_buf;\n\t\t}\n\n\t\tmemcpy(page_address(page) + page_off,\n\t\t       page_address(p) + off, buflen);\n\t\tpage_off += buflen;\n\t\tput_page(p);\n\t}\n\n\t \n\t*len = page_off - VIRTIO_XDP_HEADROOM;\n\treturn page;\nerr_buf:\n\t__free_pages(page, 0);\n\treturn NULL;\n}\n\nstatic struct sk_buff *receive_small_build_skb(struct virtnet_info *vi,\n\t\t\t\t\t       unsigned int xdp_headroom,\n\t\t\t\t\t       void *buf,\n\t\t\t\t\t       unsigned int len)\n{\n\tunsigned int header_offset;\n\tunsigned int headroom;\n\tunsigned int buflen;\n\tstruct sk_buff *skb;\n\n\theader_offset = VIRTNET_RX_PAD + xdp_headroom;\n\theadroom = vi->hdr_len + header_offset;\n\tbuflen = SKB_DATA_ALIGN(GOOD_PACKET_LEN + headroom) +\n\t\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tskb = virtnet_build_skb(buf, buflen, headroom, len);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\tbuf += header_offset;\n\tmemcpy(skb_vnet_common_hdr(skb), buf, vi->hdr_len);\n\n\treturn skb;\n}\n\nstatic struct sk_buff *receive_small_xdp(struct net_device *dev,\n\t\t\t\t\t struct virtnet_info *vi,\n\t\t\t\t\t struct receive_queue *rq,\n\t\t\t\t\t struct bpf_prog *xdp_prog,\n\t\t\t\t\t void *buf,\n\t\t\t\t\t unsigned int xdp_headroom,\n\t\t\t\t\t unsigned int len,\n\t\t\t\t\t unsigned int *xdp_xmit,\n\t\t\t\t\t struct virtnet_rq_stats *stats)\n{\n\tunsigned int header_offset = VIRTNET_RX_PAD + xdp_headroom;\n\tunsigned int headroom = vi->hdr_len + header_offset;\n\tstruct virtio_net_hdr_mrg_rxbuf *hdr = buf + header_offset;\n\tstruct page *page = virt_to_head_page(buf);\n\tstruct page *xdp_page;\n\tunsigned int buflen;\n\tstruct xdp_buff xdp;\n\tstruct sk_buff *skb;\n\tunsigned int metasize = 0;\n\tu32 act;\n\n\tif (unlikely(hdr->hdr.gso_type))\n\t\tgoto err_xdp;\n\n\tbuflen = SKB_DATA_ALIGN(GOOD_PACKET_LEN + headroom) +\n\t\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tif (unlikely(xdp_headroom < virtnet_get_headroom(vi))) {\n\t\tint offset = buf - page_address(page) + header_offset;\n\t\tunsigned int tlen = len + vi->hdr_len;\n\t\tint num_buf = 1;\n\n\t\txdp_headroom = virtnet_get_headroom(vi);\n\t\theader_offset = VIRTNET_RX_PAD + xdp_headroom;\n\t\theadroom = vi->hdr_len + header_offset;\n\t\tbuflen = SKB_DATA_ALIGN(GOOD_PACKET_LEN + headroom) +\n\t\t\tSKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\t\txdp_page = xdp_linearize_page(rq, &num_buf, page,\n\t\t\t\t\t      offset, header_offset,\n\t\t\t\t\t      &tlen);\n\t\tif (!xdp_page)\n\t\t\tgoto err_xdp;\n\n\t\tbuf = page_address(xdp_page);\n\t\tput_page(page);\n\t\tpage = xdp_page;\n\t}\n\n\txdp_init_buff(&xdp, buflen, &rq->xdp_rxq);\n\txdp_prepare_buff(&xdp, buf + VIRTNET_RX_PAD + vi->hdr_len,\n\t\t\t xdp_headroom, len, true);\n\n\tact = virtnet_xdp_handler(xdp_prog, &xdp, dev, xdp_xmit, stats);\n\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\t \n\t\tlen = xdp.data_end - xdp.data;\n\t\tmetasize = xdp.data - xdp.data_meta;\n\t\tbreak;\n\n\tcase XDP_TX:\n\tcase XDP_REDIRECT:\n\t\tgoto xdp_xmit;\n\n\tdefault:\n\t\tgoto err_xdp;\n\t}\n\n\tskb = virtnet_build_skb(buf, buflen, xdp.data - buf, len);\n\tif (unlikely(!skb))\n\t\tgoto err;\n\n\tif (metasize)\n\t\tskb_metadata_set(skb, metasize);\n\n\treturn skb;\n\nerr_xdp:\n\tu64_stats_inc(&stats->xdp_drops);\nerr:\n\tu64_stats_inc(&stats->drops);\n\tput_page(page);\nxdp_xmit:\n\treturn NULL;\n}\n\nstatic struct sk_buff *receive_small(struct net_device *dev,\n\t\t\t\t     struct virtnet_info *vi,\n\t\t\t\t     struct receive_queue *rq,\n\t\t\t\t     void *buf, void *ctx,\n\t\t\t\t     unsigned int len,\n\t\t\t\t     unsigned int *xdp_xmit,\n\t\t\t\t     struct virtnet_rq_stats *stats)\n{\n\tunsigned int xdp_headroom = (unsigned long)ctx;\n\tstruct page *page = virt_to_head_page(buf);\n\tstruct sk_buff *skb;\n\n\tlen -= vi->hdr_len;\n\tu64_stats_add(&stats->bytes, len);\n\n\tif (unlikely(len > GOOD_PACKET_LEN)) {\n\t\tpr_debug(\"%s: rx error: len %u exceeds max size %d\\n\",\n\t\t\t dev->name, len, GOOD_PACKET_LEN);\n\t\tDEV_STATS_INC(dev, rx_length_errors);\n\t\tgoto err;\n\t}\n\n\tif (unlikely(vi->xdp_enabled)) {\n\t\tstruct bpf_prog *xdp_prog;\n\n\t\trcu_read_lock();\n\t\txdp_prog = rcu_dereference(rq->xdp_prog);\n\t\tif (xdp_prog) {\n\t\t\tskb = receive_small_xdp(dev, vi, rq, xdp_prog, buf,\n\t\t\t\t\t\txdp_headroom, len, xdp_xmit,\n\t\t\t\t\t\tstats);\n\t\t\trcu_read_unlock();\n\t\t\treturn skb;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tskb = receive_small_build_skb(vi, xdp_headroom, buf, len);\n\tif (likely(skb))\n\t\treturn skb;\n\nerr:\n\tu64_stats_inc(&stats->drops);\n\tput_page(page);\n\treturn NULL;\n}\n\nstatic struct sk_buff *receive_big(struct net_device *dev,\n\t\t\t\t   struct virtnet_info *vi,\n\t\t\t\t   struct receive_queue *rq,\n\t\t\t\t   void *buf,\n\t\t\t\t   unsigned int len,\n\t\t\t\t   struct virtnet_rq_stats *stats)\n{\n\tstruct page *page = buf;\n\tstruct sk_buff *skb =\n\t\tpage_to_skb(vi, rq, page, 0, len, PAGE_SIZE, 0);\n\n\tu64_stats_add(&stats->bytes, len - vi->hdr_len);\n\tif (unlikely(!skb))\n\t\tgoto err;\n\n\treturn skb;\n\nerr:\n\tu64_stats_inc(&stats->drops);\n\tgive_pages(rq, page);\n\treturn NULL;\n}\n\nstatic void mergeable_buf_free(struct receive_queue *rq, int num_buf,\n\t\t\t       struct net_device *dev,\n\t\t\t       struct virtnet_rq_stats *stats)\n{\n\tstruct page *page;\n\tvoid *buf;\n\tint len;\n\n\twhile (num_buf-- > 1) {\n\t\tbuf = virtnet_rq_get_buf(rq, &len, NULL);\n\t\tif (unlikely(!buf)) {\n\t\t\tpr_debug(\"%s: rx error: %d buffers missing\\n\",\n\t\t\t\t dev->name, num_buf);\n\t\t\tDEV_STATS_INC(dev, rx_length_errors);\n\t\t\tbreak;\n\t\t}\n\t\tu64_stats_add(&stats->bytes, len);\n\t\tpage = virt_to_head_page(buf);\n\t\tput_page(page);\n\t}\n}\n\n \nstatic struct sk_buff *build_skb_from_xdp_buff(struct net_device *dev,\n\t\t\t\t\t       struct virtnet_info *vi,\n\t\t\t\t\t       struct xdp_buff *xdp,\n\t\t\t\t\t       unsigned int xdp_frags_truesz)\n{\n\tstruct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(xdp);\n\tunsigned int headroom, data_len;\n\tstruct sk_buff *skb;\n\tint metasize;\n\tu8 nr_frags;\n\n\tif (unlikely(xdp->data_end > xdp_data_hard_end(xdp))) {\n\t\tpr_debug(\"Error building skb as missing reserved tailroom for xdp\");\n\t\treturn NULL;\n\t}\n\n\tif (unlikely(xdp_buff_has_frags(xdp)))\n\t\tnr_frags = sinfo->nr_frags;\n\n\tskb = build_skb(xdp->data_hard_start, xdp->frame_sz);\n\tif (unlikely(!skb))\n\t\treturn NULL;\n\n\theadroom = xdp->data - xdp->data_hard_start;\n\tdata_len = xdp->data_end - xdp->data;\n\tskb_reserve(skb, headroom);\n\t__skb_put(skb, data_len);\n\n\tmetasize = xdp->data - xdp->data_meta;\n\tmetasize = metasize > 0 ? metasize : 0;\n\tif (metasize)\n\t\tskb_metadata_set(skb, metasize);\n\n\tif (unlikely(xdp_buff_has_frags(xdp)))\n\t\txdp_update_skb_shared_info(skb, nr_frags,\n\t\t\t\t\t   sinfo->xdp_frags_size,\n\t\t\t\t\t   xdp_frags_truesz,\n\t\t\t\t\t   xdp_buff_is_frag_pfmemalloc(xdp));\n\n\treturn skb;\n}\n\n \nstatic int virtnet_build_xdp_buff_mrg(struct net_device *dev,\n\t\t\t\t      struct virtnet_info *vi,\n\t\t\t\t      struct receive_queue *rq,\n\t\t\t\t      struct xdp_buff *xdp,\n\t\t\t\t      void *buf,\n\t\t\t\t      unsigned int len,\n\t\t\t\t      unsigned int frame_sz,\n\t\t\t\t      int *num_buf,\n\t\t\t\t      unsigned int *xdp_frags_truesize,\n\t\t\t\t      struct virtnet_rq_stats *stats)\n{\n\tstruct virtio_net_hdr_mrg_rxbuf *hdr = buf;\n\tunsigned int headroom, tailroom, room;\n\tunsigned int truesize, cur_frag_size;\n\tstruct skb_shared_info *shinfo;\n\tunsigned int xdp_frags_truesz = 0;\n\tstruct page *page;\n\tskb_frag_t *frag;\n\tint offset;\n\tvoid *ctx;\n\n\txdp_init_buff(xdp, frame_sz, &rq->xdp_rxq);\n\txdp_prepare_buff(xdp, buf - VIRTIO_XDP_HEADROOM,\n\t\t\t VIRTIO_XDP_HEADROOM + vi->hdr_len, len - vi->hdr_len, true);\n\n\tif (!*num_buf)\n\t\treturn 0;\n\n\tif (*num_buf > 1) {\n\t\t \n\t\tif (!xdp_buff_has_frags(xdp))\n\t\t\txdp_buff_set_frags_flag(xdp);\n\n\t\tshinfo = xdp_get_shared_info_from_buff(xdp);\n\t\tshinfo->nr_frags = 0;\n\t\tshinfo->xdp_frags_size = 0;\n\t}\n\n\tif (*num_buf > MAX_SKB_FRAGS + 1)\n\t\treturn -EINVAL;\n\n\twhile (--*num_buf > 0) {\n\t\tbuf = virtnet_rq_get_buf(rq, &len, &ctx);\n\t\tif (unlikely(!buf)) {\n\t\t\tpr_debug(\"%s: rx error: %d buffers out of %d missing\\n\",\n\t\t\t\t dev->name, *num_buf,\n\t\t\t\t virtio16_to_cpu(vi->vdev, hdr->num_buffers));\n\t\t\tDEV_STATS_INC(dev, rx_length_errors);\n\t\t\tgoto err;\n\t\t}\n\n\t\tu64_stats_add(&stats->bytes, len);\n\t\tpage = virt_to_head_page(buf);\n\t\toffset = buf - page_address(page);\n\n\t\ttruesize = mergeable_ctx_to_truesize(ctx);\n\t\theadroom = mergeable_ctx_to_headroom(ctx);\n\t\ttailroom = headroom ? sizeof(struct skb_shared_info) : 0;\n\t\troom = SKB_DATA_ALIGN(headroom + tailroom);\n\n\t\tcur_frag_size = truesize;\n\t\txdp_frags_truesz += cur_frag_size;\n\t\tif (unlikely(len > truesize - room || cur_frag_size > PAGE_SIZE)) {\n\t\t\tput_page(page);\n\t\t\tpr_debug(\"%s: rx error: len %u exceeds truesize %lu\\n\",\n\t\t\t\t dev->name, len, (unsigned long)(truesize - room));\n\t\t\tDEV_STATS_INC(dev, rx_length_errors);\n\t\t\tgoto err;\n\t\t}\n\n\t\tfrag = &shinfo->frags[shinfo->nr_frags++];\n\t\tskb_frag_fill_page_desc(frag, page, offset, len);\n\t\tif (page_is_pfmemalloc(page))\n\t\t\txdp_buff_set_frag_pfmemalloc(xdp);\n\n\t\tshinfo->xdp_frags_size += len;\n\t}\n\n\t*xdp_frags_truesize = xdp_frags_truesz;\n\treturn 0;\n\nerr:\n\tput_xdp_frags(xdp);\n\treturn -EINVAL;\n}\n\nstatic void *mergeable_xdp_get_buf(struct virtnet_info *vi,\n\t\t\t\t   struct receive_queue *rq,\n\t\t\t\t   struct bpf_prog *xdp_prog,\n\t\t\t\t   void *ctx,\n\t\t\t\t   unsigned int *frame_sz,\n\t\t\t\t   int *num_buf,\n\t\t\t\t   struct page **page,\n\t\t\t\t   int offset,\n\t\t\t\t   unsigned int *len,\n\t\t\t\t   struct virtio_net_hdr_mrg_rxbuf *hdr)\n{\n\tunsigned int truesize = mergeable_ctx_to_truesize(ctx);\n\tunsigned int headroom = mergeable_ctx_to_headroom(ctx);\n\tstruct page *xdp_page;\n\tunsigned int xdp_room;\n\n\t \n\tif (unlikely(hdr->hdr.gso_type))\n\t\treturn NULL;\n\n\t \n\t*frame_sz = truesize;\n\n\tif (likely(headroom >= virtnet_get_headroom(vi) &&\n\t\t   (*num_buf == 1 || xdp_prog->aux->xdp_has_frags))) {\n\t\treturn page_address(*page) + offset;\n\t}\n\n\t \n\tif (!xdp_prog->aux->xdp_has_frags) {\n\t\t \n\t\txdp_page = xdp_linearize_page(rq, num_buf,\n\t\t\t\t\t      *page, offset,\n\t\t\t\t\t      VIRTIO_XDP_HEADROOM,\n\t\t\t\t\t      len);\n\t\tif (!xdp_page)\n\t\t\treturn NULL;\n\t} else {\n\t\txdp_room = SKB_DATA_ALIGN(VIRTIO_XDP_HEADROOM +\n\t\t\t\t\t  sizeof(struct skb_shared_info));\n\t\tif (*len + xdp_room > PAGE_SIZE)\n\t\t\treturn NULL;\n\n\t\txdp_page = alloc_page(GFP_ATOMIC);\n\t\tif (!xdp_page)\n\t\t\treturn NULL;\n\n\t\tmemcpy(page_address(xdp_page) + VIRTIO_XDP_HEADROOM,\n\t\t       page_address(*page) + offset, *len);\n\t}\n\n\t*frame_sz = PAGE_SIZE;\n\n\tput_page(*page);\n\n\t*page = xdp_page;\n\n\treturn page_address(*page) + VIRTIO_XDP_HEADROOM;\n}\n\nstatic struct sk_buff *receive_mergeable_xdp(struct net_device *dev,\n\t\t\t\t\t     struct virtnet_info *vi,\n\t\t\t\t\t     struct receive_queue *rq,\n\t\t\t\t\t     struct bpf_prog *xdp_prog,\n\t\t\t\t\t     void *buf,\n\t\t\t\t\t     void *ctx,\n\t\t\t\t\t     unsigned int len,\n\t\t\t\t\t     unsigned int *xdp_xmit,\n\t\t\t\t\t     struct virtnet_rq_stats *stats)\n{\n\tstruct virtio_net_hdr_mrg_rxbuf *hdr = buf;\n\tint num_buf = virtio16_to_cpu(vi->vdev, hdr->num_buffers);\n\tstruct page *page = virt_to_head_page(buf);\n\tint offset = buf - page_address(page);\n\tunsigned int xdp_frags_truesz = 0;\n\tstruct sk_buff *head_skb;\n\tunsigned int frame_sz;\n\tstruct xdp_buff xdp;\n\tvoid *data;\n\tu32 act;\n\tint err;\n\n\tdata = mergeable_xdp_get_buf(vi, rq, xdp_prog, ctx, &frame_sz, &num_buf, &page,\n\t\t\t\t     offset, &len, hdr);\n\tif (unlikely(!data))\n\t\tgoto err_xdp;\n\n\terr = virtnet_build_xdp_buff_mrg(dev, vi, rq, &xdp, data, len, frame_sz,\n\t\t\t\t\t &num_buf, &xdp_frags_truesz, stats);\n\tif (unlikely(err))\n\t\tgoto err_xdp;\n\n\tact = virtnet_xdp_handler(xdp_prog, &xdp, dev, xdp_xmit, stats);\n\n\tswitch (act) {\n\tcase XDP_PASS:\n\t\thead_skb = build_skb_from_xdp_buff(dev, vi, &xdp, xdp_frags_truesz);\n\t\tif (unlikely(!head_skb))\n\t\t\tbreak;\n\t\treturn head_skb;\n\n\tcase XDP_TX:\n\tcase XDP_REDIRECT:\n\t\treturn NULL;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tput_xdp_frags(&xdp);\n\nerr_xdp:\n\tput_page(page);\n\tmergeable_buf_free(rq, num_buf, dev, stats);\n\n\tu64_stats_inc(&stats->xdp_drops);\n\tu64_stats_inc(&stats->drops);\n\treturn NULL;\n}\n\nstatic struct sk_buff *receive_mergeable(struct net_device *dev,\n\t\t\t\t\t struct virtnet_info *vi,\n\t\t\t\t\t struct receive_queue *rq,\n\t\t\t\t\t void *buf,\n\t\t\t\t\t void *ctx,\n\t\t\t\t\t unsigned int len,\n\t\t\t\t\t unsigned int *xdp_xmit,\n\t\t\t\t\t struct virtnet_rq_stats *stats)\n{\n\tstruct virtio_net_hdr_mrg_rxbuf *hdr = buf;\n\tint num_buf = virtio16_to_cpu(vi->vdev, hdr->num_buffers);\n\tstruct page *page = virt_to_head_page(buf);\n\tint offset = buf - page_address(page);\n\tstruct sk_buff *head_skb, *curr_skb;\n\tunsigned int truesize = mergeable_ctx_to_truesize(ctx);\n\tunsigned int headroom = mergeable_ctx_to_headroom(ctx);\n\tunsigned int tailroom = headroom ? sizeof(struct skb_shared_info) : 0;\n\tunsigned int room = SKB_DATA_ALIGN(headroom + tailroom);\n\n\thead_skb = NULL;\n\tu64_stats_add(&stats->bytes, len - vi->hdr_len);\n\n\tif (unlikely(len > truesize - room)) {\n\t\tpr_debug(\"%s: rx error: len %u exceeds truesize %lu\\n\",\n\t\t\t dev->name, len, (unsigned long)(truesize - room));\n\t\tDEV_STATS_INC(dev, rx_length_errors);\n\t\tgoto err_skb;\n\t}\n\n\tif (unlikely(vi->xdp_enabled)) {\n\t\tstruct bpf_prog *xdp_prog;\n\n\t\trcu_read_lock();\n\t\txdp_prog = rcu_dereference(rq->xdp_prog);\n\t\tif (xdp_prog) {\n\t\t\thead_skb = receive_mergeable_xdp(dev, vi, rq, xdp_prog, buf, ctx,\n\t\t\t\t\t\t\t len, xdp_xmit, stats);\n\t\t\trcu_read_unlock();\n\t\t\treturn head_skb;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\thead_skb = page_to_skb(vi, rq, page, offset, len, truesize, headroom);\n\tcurr_skb = head_skb;\n\n\tif (unlikely(!curr_skb))\n\t\tgoto err_skb;\n\twhile (--num_buf) {\n\t\tint num_skb_frags;\n\n\t\tbuf = virtnet_rq_get_buf(rq, &len, &ctx);\n\t\tif (unlikely(!buf)) {\n\t\t\tpr_debug(\"%s: rx error: %d buffers out of %d missing\\n\",\n\t\t\t\t dev->name, num_buf,\n\t\t\t\t virtio16_to_cpu(vi->vdev,\n\t\t\t\t\t\t hdr->num_buffers));\n\t\t\tDEV_STATS_INC(dev, rx_length_errors);\n\t\t\tgoto err_buf;\n\t\t}\n\n\t\tu64_stats_add(&stats->bytes, len);\n\t\tpage = virt_to_head_page(buf);\n\n\t\ttruesize = mergeable_ctx_to_truesize(ctx);\n\t\theadroom = mergeable_ctx_to_headroom(ctx);\n\t\ttailroom = headroom ? sizeof(struct skb_shared_info) : 0;\n\t\troom = SKB_DATA_ALIGN(headroom + tailroom);\n\t\tif (unlikely(len > truesize - room)) {\n\t\t\tpr_debug(\"%s: rx error: len %u exceeds truesize %lu\\n\",\n\t\t\t\t dev->name, len, (unsigned long)(truesize - room));\n\t\t\tDEV_STATS_INC(dev, rx_length_errors);\n\t\t\tgoto err_skb;\n\t\t}\n\n\t\tnum_skb_frags = skb_shinfo(curr_skb)->nr_frags;\n\t\tif (unlikely(num_skb_frags == MAX_SKB_FRAGS)) {\n\t\t\tstruct sk_buff *nskb = alloc_skb(0, GFP_ATOMIC);\n\n\t\t\tif (unlikely(!nskb))\n\t\t\t\tgoto err_skb;\n\t\t\tif (curr_skb == head_skb)\n\t\t\t\tskb_shinfo(curr_skb)->frag_list = nskb;\n\t\t\telse\n\t\t\t\tcurr_skb->next = nskb;\n\t\t\tcurr_skb = nskb;\n\t\t\thead_skb->truesize += nskb->truesize;\n\t\t\tnum_skb_frags = 0;\n\t\t}\n\t\tif (curr_skb != head_skb) {\n\t\t\thead_skb->data_len += len;\n\t\t\thead_skb->len += len;\n\t\t\thead_skb->truesize += truesize;\n\t\t}\n\t\toffset = buf - page_address(page);\n\t\tif (skb_can_coalesce(curr_skb, num_skb_frags, page, offset)) {\n\t\t\tput_page(page);\n\t\t\tskb_coalesce_rx_frag(curr_skb, num_skb_frags - 1,\n\t\t\t\t\t     len, truesize);\n\t\t} else {\n\t\t\tskb_add_rx_frag(curr_skb, num_skb_frags, page,\n\t\t\t\t\toffset, len, truesize);\n\t\t}\n\t}\n\n\tewma_pkt_len_add(&rq->mrg_avg_pkt_len, head_skb->len);\n\treturn head_skb;\n\nerr_skb:\n\tput_page(page);\n\tmergeable_buf_free(rq, num_buf, dev, stats);\n\nerr_buf:\n\tu64_stats_inc(&stats->drops);\n\tdev_kfree_skb(head_skb);\n\treturn NULL;\n}\n\nstatic void virtio_skb_set_hash(const struct virtio_net_hdr_v1_hash *hdr_hash,\n\t\t\t\tstruct sk_buff *skb)\n{\n\tenum pkt_hash_types rss_hash_type;\n\n\tif (!hdr_hash || !skb)\n\t\treturn;\n\n\tswitch (__le16_to_cpu(hdr_hash->hash_report)) {\n\tcase VIRTIO_NET_HASH_REPORT_TCPv4:\n\tcase VIRTIO_NET_HASH_REPORT_UDPv4:\n\tcase VIRTIO_NET_HASH_REPORT_TCPv6:\n\tcase VIRTIO_NET_HASH_REPORT_UDPv6:\n\tcase VIRTIO_NET_HASH_REPORT_TCPv6_EX:\n\tcase VIRTIO_NET_HASH_REPORT_UDPv6_EX:\n\t\trss_hash_type = PKT_HASH_TYPE_L4;\n\t\tbreak;\n\tcase VIRTIO_NET_HASH_REPORT_IPv4:\n\tcase VIRTIO_NET_HASH_REPORT_IPv6:\n\tcase VIRTIO_NET_HASH_REPORT_IPv6_EX:\n\t\trss_hash_type = PKT_HASH_TYPE_L3;\n\t\tbreak;\n\tcase VIRTIO_NET_HASH_REPORT_NONE:\n\tdefault:\n\t\trss_hash_type = PKT_HASH_TYPE_NONE;\n\t}\n\tskb_set_hash(skb, __le32_to_cpu(hdr_hash->hash_value), rss_hash_type);\n}\n\nstatic void receive_buf(struct virtnet_info *vi, struct receive_queue *rq,\n\t\t\tvoid *buf, unsigned int len, void **ctx,\n\t\t\tunsigned int *xdp_xmit,\n\t\t\tstruct virtnet_rq_stats *stats)\n{\n\tstruct net_device *dev = vi->dev;\n\tstruct sk_buff *skb;\n\tstruct virtio_net_common_hdr *hdr;\n\n\tif (unlikely(len < vi->hdr_len + ETH_HLEN)) {\n\t\tpr_debug(\"%s: short packet %i\\n\", dev->name, len);\n\t\tDEV_STATS_INC(dev, rx_length_errors);\n\t\tvirtnet_rq_free_buf(vi, rq, buf);\n\t\treturn;\n\t}\n\n\tif (vi->mergeable_rx_bufs)\n\t\tskb = receive_mergeable(dev, vi, rq, buf, ctx, len, xdp_xmit,\n\t\t\t\t\tstats);\n\telse if (vi->big_packets)\n\t\tskb = receive_big(dev, vi, rq, buf, len, stats);\n\telse\n\t\tskb = receive_small(dev, vi, rq, buf, ctx, len, xdp_xmit, stats);\n\n\tif (unlikely(!skb))\n\t\treturn;\n\n\thdr = skb_vnet_common_hdr(skb);\n\tif (dev->features & NETIF_F_RXHASH && vi->has_rss_hash_report)\n\t\tvirtio_skb_set_hash(&hdr->hash_v1_hdr, skb);\n\n\tif (hdr->hdr.flags & VIRTIO_NET_HDR_F_DATA_VALID)\n\t\tskb->ip_summed = CHECKSUM_UNNECESSARY;\n\n\tif (virtio_net_hdr_to_skb(skb, &hdr->hdr,\n\t\t\t\t  virtio_is_little_endian(vi->vdev))) {\n\t\tnet_warn_ratelimited(\"%s: bad gso: type: %u, size: %u\\n\",\n\t\t\t\t     dev->name, hdr->hdr.gso_type,\n\t\t\t\t     hdr->hdr.gso_size);\n\t\tgoto frame_err;\n\t}\n\n\tskb_record_rx_queue(skb, vq2rxq(rq->vq));\n\tskb->protocol = eth_type_trans(skb, dev);\n\tpr_debug(\"Receiving skb proto 0x%04x len %i type %i\\n\",\n\t\t ntohs(skb->protocol), skb->len, skb->pkt_type);\n\n\tnapi_gro_receive(&rq->napi, skb);\n\treturn;\n\nframe_err:\n\tDEV_STATS_INC(dev, rx_frame_errors);\n\tdev_kfree_skb(skb);\n}\n\n \nstatic int add_recvbuf_small(struct virtnet_info *vi, struct receive_queue *rq,\n\t\t\t     gfp_t gfp)\n{\n\tchar *buf;\n\tunsigned int xdp_headroom = virtnet_get_headroom(vi);\n\tvoid *ctx = (void *)(unsigned long)xdp_headroom;\n\tint len = vi->hdr_len + VIRTNET_RX_PAD + GOOD_PACKET_LEN + xdp_headroom;\n\tint err;\n\n\tlen = SKB_DATA_ALIGN(len) +\n\t      SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\n\tbuf = virtnet_rq_alloc(rq, len, gfp);\n\tif (unlikely(!buf))\n\t\treturn -ENOMEM;\n\n\tvirtnet_rq_init_one_sg(rq, buf + VIRTNET_RX_PAD + xdp_headroom,\n\t\t\t       vi->hdr_len + GOOD_PACKET_LEN);\n\n\terr = virtqueue_add_inbuf_ctx(rq->vq, rq->sg, 1, buf, ctx, gfp);\n\tif (err < 0) {\n\t\tif (rq->do_dma)\n\t\t\tvirtnet_rq_unmap(rq, buf, 0);\n\t\tput_page(virt_to_head_page(buf));\n\t}\n\n\treturn err;\n}\n\nstatic int add_recvbuf_big(struct virtnet_info *vi, struct receive_queue *rq,\n\t\t\t   gfp_t gfp)\n{\n\tstruct page *first, *list = NULL;\n\tchar *p;\n\tint i, err, offset;\n\n\tsg_init_table(rq->sg, vi->big_packets_num_skbfrags + 2);\n\n\t \n\tfor (i = vi->big_packets_num_skbfrags + 1; i > 1; --i) {\n\t\tfirst = get_a_page(rq, gfp);\n\t\tif (!first) {\n\t\t\tif (list)\n\t\t\t\tgive_pages(rq, list);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tsg_set_buf(&rq->sg[i], page_address(first), PAGE_SIZE);\n\n\t\t \n\t\tfirst->private = (unsigned long)list;\n\t\tlist = first;\n\t}\n\n\tfirst = get_a_page(rq, gfp);\n\tif (!first) {\n\t\tgive_pages(rq, list);\n\t\treturn -ENOMEM;\n\t}\n\tp = page_address(first);\n\n\t \n\t \n\tsg_set_buf(&rq->sg[0], p, vi->hdr_len);\n\n\t \n\toffset = sizeof(struct padded_vnet_hdr);\n\tsg_set_buf(&rq->sg[1], p + offset, PAGE_SIZE - offset);\n\n\t \n\tfirst->private = (unsigned long)list;\n\terr = virtqueue_add_inbuf(rq->vq, rq->sg, vi->big_packets_num_skbfrags + 2,\n\t\t\t\t  first, gfp);\n\tif (err < 0)\n\t\tgive_pages(rq, first);\n\n\treturn err;\n}\n\nstatic unsigned int get_mergeable_buf_len(struct receive_queue *rq,\n\t\t\t\t\t  struct ewma_pkt_len *avg_pkt_len,\n\t\t\t\t\t  unsigned int room)\n{\n\tstruct virtnet_info *vi = rq->vq->vdev->priv;\n\tconst size_t hdr_len = vi->hdr_len;\n\tunsigned int len;\n\n\tif (room)\n\t\treturn PAGE_SIZE - room;\n\n\tlen = hdr_len +\tclamp_t(unsigned int, ewma_pkt_len_read(avg_pkt_len),\n\t\t\t\trq->min_buf_len, PAGE_SIZE - hdr_len);\n\n\treturn ALIGN(len, L1_CACHE_BYTES);\n}\n\nstatic int add_recvbuf_mergeable(struct virtnet_info *vi,\n\t\t\t\t struct receive_queue *rq, gfp_t gfp)\n{\n\tstruct page_frag *alloc_frag = &rq->alloc_frag;\n\tunsigned int headroom = virtnet_get_headroom(vi);\n\tunsigned int tailroom = headroom ? sizeof(struct skb_shared_info) : 0;\n\tunsigned int room = SKB_DATA_ALIGN(headroom + tailroom);\n\tunsigned int len, hole;\n\tvoid *ctx;\n\tchar *buf;\n\tint err;\n\n\t \n\tlen = get_mergeable_buf_len(rq, &rq->mrg_avg_pkt_len, room);\n\n\tbuf = virtnet_rq_alloc(rq, len + room, gfp);\n\tif (unlikely(!buf))\n\t\treturn -ENOMEM;\n\n\tbuf += headroom;  \n\thole = alloc_frag->size - alloc_frag->offset;\n\tif (hole < len + room) {\n\t\t \n\t\tif (!headroom)\n\t\t\tlen += hole;\n\t\talloc_frag->offset += hole;\n\t}\n\n\tvirtnet_rq_init_one_sg(rq, buf, len);\n\n\tctx = mergeable_len_to_ctx(len + room, headroom);\n\terr = virtqueue_add_inbuf_ctx(rq->vq, rq->sg, 1, buf, ctx, gfp);\n\tif (err < 0) {\n\t\tif (rq->do_dma)\n\t\t\tvirtnet_rq_unmap(rq, buf, 0);\n\t\tput_page(virt_to_head_page(buf));\n\t}\n\n\treturn err;\n}\n\n \nstatic bool try_fill_recv(struct virtnet_info *vi, struct receive_queue *rq,\n\t\t\t  gfp_t gfp)\n{\n\tint err;\n\tbool oom;\n\n\tdo {\n\t\tif (vi->mergeable_rx_bufs)\n\t\t\terr = add_recvbuf_mergeable(vi, rq, gfp);\n\t\telse if (vi->big_packets)\n\t\t\terr = add_recvbuf_big(vi, rq, gfp);\n\t\telse\n\t\t\terr = add_recvbuf_small(vi, rq, gfp);\n\n\t\toom = err == -ENOMEM;\n\t\tif (err)\n\t\t\tbreak;\n\t} while (rq->vq->num_free);\n\tif (virtqueue_kick_prepare(rq->vq) && virtqueue_notify(rq->vq)) {\n\t\tunsigned long flags;\n\n\t\tflags = u64_stats_update_begin_irqsave(&rq->stats.syncp);\n\t\tu64_stats_inc(&rq->stats.kicks);\n\t\tu64_stats_update_end_irqrestore(&rq->stats.syncp, flags);\n\t}\n\n\treturn !oom;\n}\n\nstatic void skb_recv_done(struct virtqueue *rvq)\n{\n\tstruct virtnet_info *vi = rvq->vdev->priv;\n\tstruct receive_queue *rq = &vi->rq[vq2rxq(rvq)];\n\n\tvirtqueue_napi_schedule(&rq->napi, rvq);\n}\n\nstatic void virtnet_napi_enable(struct virtqueue *vq, struct napi_struct *napi)\n{\n\tnapi_enable(napi);\n\n\t \n\tlocal_bh_disable();\n\tvirtqueue_napi_schedule(napi, vq);\n\tlocal_bh_enable();\n}\n\nstatic void virtnet_napi_tx_enable(struct virtnet_info *vi,\n\t\t\t\t   struct virtqueue *vq,\n\t\t\t\t   struct napi_struct *napi)\n{\n\tif (!napi->weight)\n\t\treturn;\n\n\t \n\tif (!vi->affinity_hint_set) {\n\t\tnapi->weight = 0;\n\t\treturn;\n\t}\n\n\treturn virtnet_napi_enable(vq, napi);\n}\n\nstatic void virtnet_napi_tx_disable(struct napi_struct *napi)\n{\n\tif (napi->weight)\n\t\tnapi_disable(napi);\n}\n\nstatic void refill_work(struct work_struct *work)\n{\n\tstruct virtnet_info *vi =\n\t\tcontainer_of(work, struct virtnet_info, refill.work);\n\tbool still_empty;\n\tint i;\n\n\tfor (i = 0; i < vi->curr_queue_pairs; i++) {\n\t\tstruct receive_queue *rq = &vi->rq[i];\n\n\t\tnapi_disable(&rq->napi);\n\t\tstill_empty = !try_fill_recv(vi, rq, GFP_KERNEL);\n\t\tvirtnet_napi_enable(rq->vq, &rq->napi);\n\n\t\t \n\t\tif (still_empty)\n\t\t\tschedule_delayed_work(&vi->refill, HZ/2);\n\t}\n}\n\nstatic int virtnet_receive(struct receive_queue *rq, int budget,\n\t\t\t   unsigned int *xdp_xmit)\n{\n\tstruct virtnet_info *vi = rq->vq->vdev->priv;\n\tstruct virtnet_rq_stats stats = {};\n\tunsigned int len;\n\tint packets = 0;\n\tvoid *buf;\n\tint i;\n\n\tif (!vi->big_packets || vi->mergeable_rx_bufs) {\n\t\tvoid *ctx;\n\n\t\twhile (packets < budget &&\n\t\t       (buf = virtnet_rq_get_buf(rq, &len, &ctx))) {\n\t\t\treceive_buf(vi, rq, buf, len, ctx, xdp_xmit, &stats);\n\t\t\tpackets++;\n\t\t}\n\t} else {\n\t\twhile (packets < budget &&\n\t\t       (buf = virtnet_rq_get_buf(rq, &len, NULL)) != NULL) {\n\t\t\treceive_buf(vi, rq, buf, len, NULL, xdp_xmit, &stats);\n\t\t\tpackets++;\n\t\t}\n\t}\n\n\tif (rq->vq->num_free > min((unsigned int)budget, virtqueue_get_vring_size(rq->vq)) / 2) {\n\t\tif (!try_fill_recv(vi, rq, GFP_ATOMIC)) {\n\t\t\tspin_lock(&vi->refill_lock);\n\t\t\tif (vi->refill_enabled)\n\t\t\t\tschedule_delayed_work(&vi->refill, 0);\n\t\t\tspin_unlock(&vi->refill_lock);\n\t\t}\n\t}\n\n\tu64_stats_set(&stats.packets, packets);\n\tu64_stats_update_begin(&rq->stats.syncp);\n\tfor (i = 0; i < VIRTNET_RQ_STATS_LEN; i++) {\n\t\tsize_t offset = virtnet_rq_stats_desc[i].offset;\n\t\tu64_stats_t *item, *src;\n\n\t\titem = (u64_stats_t *)((u8 *)&rq->stats + offset);\n\t\tsrc = (u64_stats_t *)((u8 *)&stats + offset);\n\t\tu64_stats_add(item, u64_stats_read(src));\n\t}\n\tu64_stats_update_end(&rq->stats.syncp);\n\n\treturn packets;\n}\n\nstatic void virtnet_poll_cleantx(struct receive_queue *rq)\n{\n\tstruct virtnet_info *vi = rq->vq->vdev->priv;\n\tunsigned int index = vq2rxq(rq->vq);\n\tstruct send_queue *sq = &vi->sq[index];\n\tstruct netdev_queue *txq = netdev_get_tx_queue(vi->dev, index);\n\n\tif (!sq->napi.weight || is_xdp_raw_buffer_queue(vi, index))\n\t\treturn;\n\n\tif (__netif_tx_trylock(txq)) {\n\t\tif (sq->reset) {\n\t\t\t__netif_tx_unlock(txq);\n\t\t\treturn;\n\t\t}\n\n\t\tdo {\n\t\t\tvirtqueue_disable_cb(sq->vq);\n\t\t\tfree_old_xmit_skbs(sq, true);\n\t\t} while (unlikely(!virtqueue_enable_cb_delayed(sq->vq)));\n\n\t\tif (sq->vq->num_free >= 2 + MAX_SKB_FRAGS)\n\t\t\tnetif_tx_wake_queue(txq);\n\n\t\t__netif_tx_unlock(txq);\n\t}\n}\n\nstatic int virtnet_poll(struct napi_struct *napi, int budget)\n{\n\tstruct receive_queue *rq =\n\t\tcontainer_of(napi, struct receive_queue, napi);\n\tstruct virtnet_info *vi = rq->vq->vdev->priv;\n\tstruct send_queue *sq;\n\tunsigned int received;\n\tunsigned int xdp_xmit = 0;\n\n\tvirtnet_poll_cleantx(rq);\n\n\treceived = virtnet_receive(rq, budget, &xdp_xmit);\n\n\tif (xdp_xmit & VIRTIO_XDP_REDIR)\n\t\txdp_do_flush();\n\n\t \n\tif (received < budget)\n\t\tvirtqueue_napi_complete(napi, rq->vq, received);\n\n\tif (xdp_xmit & VIRTIO_XDP_TX) {\n\t\tsq = virtnet_xdp_get_sq(vi);\n\t\tif (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)) {\n\t\t\tu64_stats_update_begin(&sq->stats.syncp);\n\t\t\tu64_stats_inc(&sq->stats.kicks);\n\t\t\tu64_stats_update_end(&sq->stats.syncp);\n\t\t}\n\t\tvirtnet_xdp_put_sq(vi, sq);\n\t}\n\n\treturn received;\n}\n\nstatic void virtnet_disable_queue_pair(struct virtnet_info *vi, int qp_index)\n{\n\tvirtnet_napi_tx_disable(&vi->sq[qp_index].napi);\n\tnapi_disable(&vi->rq[qp_index].napi);\n\txdp_rxq_info_unreg(&vi->rq[qp_index].xdp_rxq);\n}\n\nstatic int virtnet_enable_queue_pair(struct virtnet_info *vi, int qp_index)\n{\n\tstruct net_device *dev = vi->dev;\n\tint err;\n\n\terr = xdp_rxq_info_reg(&vi->rq[qp_index].xdp_rxq, dev, qp_index,\n\t\t\t       vi->rq[qp_index].napi.napi_id);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = xdp_rxq_info_reg_mem_model(&vi->rq[qp_index].xdp_rxq,\n\t\t\t\t\t MEM_TYPE_PAGE_SHARED, NULL);\n\tif (err < 0)\n\t\tgoto err_xdp_reg_mem_model;\n\n\tvirtnet_napi_enable(vi->rq[qp_index].vq, &vi->rq[qp_index].napi);\n\tvirtnet_napi_tx_enable(vi, vi->sq[qp_index].vq, &vi->sq[qp_index].napi);\n\n\treturn 0;\n\nerr_xdp_reg_mem_model:\n\txdp_rxq_info_unreg(&vi->rq[qp_index].xdp_rxq);\n\treturn err;\n}\n\nstatic int virtnet_open(struct net_device *dev)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tint i, err;\n\n\tenable_delayed_refill(vi);\n\n\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\tif (i < vi->curr_queue_pairs)\n\t\t\t \n\t\t\tif (!try_fill_recv(vi, &vi->rq[i], GFP_KERNEL))\n\t\t\t\tschedule_delayed_work(&vi->refill, 0);\n\n\t\terr = virtnet_enable_queue_pair(vi, i);\n\t\tif (err < 0)\n\t\t\tgoto err_enable_qp;\n\t}\n\n\treturn 0;\n\nerr_enable_qp:\n\tdisable_delayed_refill(vi);\n\tcancel_delayed_work_sync(&vi->refill);\n\n\tfor (i--; i >= 0; i--)\n\t\tvirtnet_disable_queue_pair(vi, i);\n\treturn err;\n}\n\nstatic int virtnet_poll_tx(struct napi_struct *napi, int budget)\n{\n\tstruct send_queue *sq = container_of(napi, struct send_queue, napi);\n\tstruct virtnet_info *vi = sq->vq->vdev->priv;\n\tunsigned int index = vq2txq(sq->vq);\n\tstruct netdev_queue *txq;\n\tint opaque;\n\tbool done;\n\n\tif (unlikely(is_xdp_raw_buffer_queue(vi, index))) {\n\t\t \n\t\tnapi_complete_done(napi, 0);\n\t\treturn 0;\n\t}\n\n\ttxq = netdev_get_tx_queue(vi->dev, index);\n\t__netif_tx_lock(txq, raw_smp_processor_id());\n\tvirtqueue_disable_cb(sq->vq);\n\tfree_old_xmit_skbs(sq, true);\n\n\tif (sq->vq->num_free >= 2 + MAX_SKB_FRAGS)\n\t\tnetif_tx_wake_queue(txq);\n\n\topaque = virtqueue_enable_cb_prepare(sq->vq);\n\n\tdone = napi_complete_done(napi, 0);\n\n\tif (!done)\n\t\tvirtqueue_disable_cb(sq->vq);\n\n\t__netif_tx_unlock(txq);\n\n\tif (done) {\n\t\tif (unlikely(virtqueue_poll(sq->vq, opaque))) {\n\t\t\tif (napi_schedule_prep(napi)) {\n\t\t\t\t__netif_tx_lock(txq, raw_smp_processor_id());\n\t\t\t\tvirtqueue_disable_cb(sq->vq);\n\t\t\t\t__netif_tx_unlock(txq);\n\t\t\t\t__napi_schedule(napi);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int xmit_skb(struct send_queue *sq, struct sk_buff *skb)\n{\n\tstruct virtio_net_hdr_mrg_rxbuf *hdr;\n\tconst unsigned char *dest = ((struct ethhdr *)skb->data)->h_dest;\n\tstruct virtnet_info *vi = sq->vq->vdev->priv;\n\tint num_sg;\n\tunsigned hdr_len = vi->hdr_len;\n\tbool can_push;\n\n\tpr_debug(\"%s: xmit %p %pM\\n\", vi->dev->name, skb, dest);\n\n\tcan_push = vi->any_header_sg &&\n\t\t!((unsigned long)skb->data & (__alignof__(*hdr) - 1)) &&\n\t\t!skb_header_cloned(skb) && skb_headroom(skb) >= hdr_len;\n\t \n\tif (can_push)\n\t\thdr = (struct virtio_net_hdr_mrg_rxbuf *)(skb->data - hdr_len);\n\telse\n\t\thdr = &skb_vnet_common_hdr(skb)->mrg_hdr;\n\n\tif (virtio_net_hdr_from_skb(skb, &hdr->hdr,\n\t\t\t\t    virtio_is_little_endian(vi->vdev), false,\n\t\t\t\t    0))\n\t\treturn -EPROTO;\n\n\tif (vi->mergeable_rx_bufs)\n\t\thdr->num_buffers = 0;\n\n\tsg_init_table(sq->sg, skb_shinfo(skb)->nr_frags + (can_push ? 1 : 2));\n\tif (can_push) {\n\t\t__skb_push(skb, hdr_len);\n\t\tnum_sg = skb_to_sgvec(skb, sq->sg, 0, skb->len);\n\t\tif (unlikely(num_sg < 0))\n\t\t\treturn num_sg;\n\t\t \n\t\t__skb_pull(skb, hdr_len);\n\t} else {\n\t\tsg_set_buf(sq->sg, hdr, hdr_len);\n\t\tnum_sg = skb_to_sgvec(skb, sq->sg + 1, 0, skb->len);\n\t\tif (unlikely(num_sg < 0))\n\t\t\treturn num_sg;\n\t\tnum_sg++;\n\t}\n\treturn virtqueue_add_outbuf(sq->vq, sq->sg, num_sg, skb, GFP_ATOMIC);\n}\n\nstatic netdev_tx_t start_xmit(struct sk_buff *skb, struct net_device *dev)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tint qnum = skb_get_queue_mapping(skb);\n\tstruct send_queue *sq = &vi->sq[qnum];\n\tint err;\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, qnum);\n\tbool kick = !netdev_xmit_more();\n\tbool use_napi = sq->napi.weight;\n\n\t \n\tdo {\n\t\tif (use_napi)\n\t\t\tvirtqueue_disable_cb(sq->vq);\n\n\t\tfree_old_xmit_skbs(sq, false);\n\n\t} while (use_napi && kick &&\n\t       unlikely(!virtqueue_enable_cb_delayed(sq->vq)));\n\n\t \n\tskb_tx_timestamp(skb);\n\n\t \n\terr = xmit_skb(sq, skb);\n\n\t \n\tif (unlikely(err)) {\n\t\tDEV_STATS_INC(dev, tx_fifo_errors);\n\t\tif (net_ratelimit())\n\t\t\tdev_warn(&dev->dev,\n\t\t\t\t \"Unexpected TXQ (%d) queue failure: %d\\n\",\n\t\t\t\t qnum, err);\n\t\tDEV_STATS_INC(dev, tx_dropped);\n\t\tdev_kfree_skb_any(skb);\n\t\treturn NETDEV_TX_OK;\n\t}\n\n\t \n\tif (!use_napi) {\n\t\tskb_orphan(skb);\n\t\tnf_reset_ct(skb);\n\t}\n\n\tcheck_sq_full_and_disable(vi, dev, sq);\n\n\tif (kick || netif_xmit_stopped(txq)) {\n\t\tif (virtqueue_kick_prepare(sq->vq) && virtqueue_notify(sq->vq)) {\n\t\t\tu64_stats_update_begin(&sq->stats.syncp);\n\t\t\tu64_stats_inc(&sq->stats.kicks);\n\t\t\tu64_stats_update_end(&sq->stats.syncp);\n\t\t}\n\t}\n\n\treturn NETDEV_TX_OK;\n}\n\nstatic int virtnet_rx_resize(struct virtnet_info *vi,\n\t\t\t     struct receive_queue *rq, u32 ring_num)\n{\n\tbool running = netif_running(vi->dev);\n\tint err, qindex;\n\n\tqindex = rq - vi->rq;\n\n\tif (running)\n\t\tnapi_disable(&rq->napi);\n\n\terr = virtqueue_resize(rq->vq, ring_num, virtnet_rq_unmap_free_buf);\n\tif (err)\n\t\tnetdev_err(vi->dev, \"resize rx fail: rx queue index: %d err: %d\\n\", qindex, err);\n\n\tif (!try_fill_recv(vi, rq, GFP_KERNEL))\n\t\tschedule_delayed_work(&vi->refill, 0);\n\n\tif (running)\n\t\tvirtnet_napi_enable(rq->vq, &rq->napi);\n\treturn err;\n}\n\nstatic int virtnet_tx_resize(struct virtnet_info *vi,\n\t\t\t     struct send_queue *sq, u32 ring_num)\n{\n\tbool running = netif_running(vi->dev);\n\tstruct netdev_queue *txq;\n\tint err, qindex;\n\n\tqindex = sq - vi->sq;\n\n\tif (running)\n\t\tvirtnet_napi_tx_disable(&sq->napi);\n\n\ttxq = netdev_get_tx_queue(vi->dev, qindex);\n\n\t \n\t__netif_tx_lock_bh(txq);\n\n\t \n\tsq->reset = true;\n\n\t \n\tnetif_stop_subqueue(vi->dev, qindex);\n\n\t__netif_tx_unlock_bh(txq);\n\n\terr = virtqueue_resize(sq->vq, ring_num, virtnet_sq_free_unused_buf);\n\tif (err)\n\t\tnetdev_err(vi->dev, \"resize tx fail: tx queue index: %d err: %d\\n\", qindex, err);\n\n\t__netif_tx_lock_bh(txq);\n\tsq->reset = false;\n\tnetif_tx_wake_queue(txq);\n\t__netif_tx_unlock_bh(txq);\n\n\tif (running)\n\t\tvirtnet_napi_tx_enable(vi, sq->vq, &sq->napi);\n\treturn err;\n}\n\n \nstatic bool virtnet_send_command(struct virtnet_info *vi, u8 class, u8 cmd,\n\t\t\t\t struct scatterlist *out)\n{\n\tstruct scatterlist *sgs[4], hdr, stat;\n\tunsigned out_num = 0, tmp;\n\tint ret;\n\n\t \n\tBUG_ON(!virtio_has_feature(vi->vdev, VIRTIO_NET_F_CTRL_VQ));\n\n\tvi->ctrl->status = ~0;\n\tvi->ctrl->hdr.class = class;\n\tvi->ctrl->hdr.cmd = cmd;\n\t \n\tsg_init_one(&hdr, &vi->ctrl->hdr, sizeof(vi->ctrl->hdr));\n\tsgs[out_num++] = &hdr;\n\n\tif (out)\n\t\tsgs[out_num++] = out;\n\n\t \n\tsg_init_one(&stat, &vi->ctrl->status, sizeof(vi->ctrl->status));\n\tsgs[out_num] = &stat;\n\n\tBUG_ON(out_num + 1 > ARRAY_SIZE(sgs));\n\tret = virtqueue_add_sgs(vi->cvq, sgs, out_num, 1, vi, GFP_ATOMIC);\n\tif (ret < 0) {\n\t\tdev_warn(&vi->vdev->dev,\n\t\t\t \"Failed to add sgs for command vq: %d\\n.\", ret);\n\t\treturn false;\n\t}\n\n\tif (unlikely(!virtqueue_kick(vi->cvq)))\n\t\treturn vi->ctrl->status == VIRTIO_NET_OK;\n\n\t \n\twhile (!virtqueue_get_buf(vi->cvq, &tmp) &&\n\t       !virtqueue_is_broken(vi->cvq))\n\t\tcpu_relax();\n\n\treturn vi->ctrl->status == VIRTIO_NET_OK;\n}\n\nstatic int virtnet_set_mac_address(struct net_device *dev, void *p)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tstruct virtio_device *vdev = vi->vdev;\n\tint ret;\n\tstruct sockaddr *addr;\n\tstruct scatterlist sg;\n\n\tif (virtio_has_feature(vi->vdev, VIRTIO_NET_F_STANDBY))\n\t\treturn -EOPNOTSUPP;\n\n\taddr = kmemdup(p, sizeof(*addr), GFP_KERNEL);\n\tif (!addr)\n\t\treturn -ENOMEM;\n\n\tret = eth_prepare_mac_addr_change(dev, addr);\n\tif (ret)\n\t\tgoto out;\n\n\tif (virtio_has_feature(vdev, VIRTIO_NET_F_CTRL_MAC_ADDR)) {\n\t\tsg_init_one(&sg, addr->sa_data, dev->addr_len);\n\t\tif (!virtnet_send_command(vi, VIRTIO_NET_CTRL_MAC,\n\t\t\t\t\t  VIRTIO_NET_CTRL_MAC_ADDR_SET, &sg)) {\n\t\t\tdev_warn(&vdev->dev,\n\t\t\t\t \"Failed to set mac address by vq command.\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t} else if (virtio_has_feature(vdev, VIRTIO_NET_F_MAC) &&\n\t\t   !virtio_has_feature(vdev, VIRTIO_F_VERSION_1)) {\n\t\tunsigned int i;\n\n\t\t \n\t\tfor (i = 0; i < dev->addr_len; i++)\n\t\t\tvirtio_cwrite8(vdev,\n\t\t\t\t       offsetof(struct virtio_net_config, mac) +\n\t\t\t\t       i, addr->sa_data[i]);\n\t}\n\n\teth_commit_mac_addr_change(dev, p);\n\tret = 0;\n\nout:\n\tkfree(addr);\n\treturn ret;\n}\n\nstatic void virtnet_stats(struct net_device *dev,\n\t\t\t  struct rtnl_link_stats64 *tot)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tunsigned int start;\n\tint i;\n\n\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\tu64 tpackets, tbytes, terrors, rpackets, rbytes, rdrops;\n\t\tstruct receive_queue *rq = &vi->rq[i];\n\t\tstruct send_queue *sq = &vi->sq[i];\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&sq->stats.syncp);\n\t\t\ttpackets = u64_stats_read(&sq->stats.packets);\n\t\t\ttbytes   = u64_stats_read(&sq->stats.bytes);\n\t\t\tterrors  = u64_stats_read(&sq->stats.tx_timeouts);\n\t\t} while (u64_stats_fetch_retry(&sq->stats.syncp, start));\n\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&rq->stats.syncp);\n\t\t\trpackets = u64_stats_read(&rq->stats.packets);\n\t\t\trbytes   = u64_stats_read(&rq->stats.bytes);\n\t\t\trdrops   = u64_stats_read(&rq->stats.drops);\n\t\t} while (u64_stats_fetch_retry(&rq->stats.syncp, start));\n\n\t\ttot->rx_packets += rpackets;\n\t\ttot->tx_packets += tpackets;\n\t\ttot->rx_bytes   += rbytes;\n\t\ttot->tx_bytes   += tbytes;\n\t\ttot->rx_dropped += rdrops;\n\t\ttot->tx_errors  += terrors;\n\t}\n\n\ttot->tx_dropped = DEV_STATS_READ(dev, tx_dropped);\n\ttot->tx_fifo_errors = DEV_STATS_READ(dev, tx_fifo_errors);\n\ttot->rx_length_errors = DEV_STATS_READ(dev, rx_length_errors);\n\ttot->rx_frame_errors = DEV_STATS_READ(dev, rx_frame_errors);\n}\n\nstatic void virtnet_ack_link_announce(struct virtnet_info *vi)\n{\n\trtnl_lock();\n\tif (!virtnet_send_command(vi, VIRTIO_NET_CTRL_ANNOUNCE,\n\t\t\t\t  VIRTIO_NET_CTRL_ANNOUNCE_ACK, NULL))\n\t\tdev_warn(&vi->dev->dev, \"Failed to ack link announce.\\n\");\n\trtnl_unlock();\n}\n\nstatic int _virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)\n{\n\tstruct scatterlist sg;\n\tstruct net_device *dev = vi->dev;\n\n\tif (!vi->has_cvq || !virtio_has_feature(vi->vdev, VIRTIO_NET_F_MQ))\n\t\treturn 0;\n\n\tvi->ctrl->mq.virtqueue_pairs = cpu_to_virtio16(vi->vdev, queue_pairs);\n\tsg_init_one(&sg, &vi->ctrl->mq, sizeof(vi->ctrl->mq));\n\n\tif (!virtnet_send_command(vi, VIRTIO_NET_CTRL_MQ,\n\t\t\t\t  VIRTIO_NET_CTRL_MQ_VQ_PAIRS_SET, &sg)) {\n\t\tdev_warn(&dev->dev, \"Fail to set num of queue pairs to %d\\n\",\n\t\t\t queue_pairs);\n\t\treturn -EINVAL;\n\t} else {\n\t\tvi->curr_queue_pairs = queue_pairs;\n\t\t \n\t\tif (dev->flags & IFF_UP)\n\t\t\tschedule_delayed_work(&vi->refill, 0);\n\t}\n\n\treturn 0;\n}\n\nstatic int virtnet_set_queues(struct virtnet_info *vi, u16 queue_pairs)\n{\n\tint err;\n\n\trtnl_lock();\n\terr = _virtnet_set_queues(vi, queue_pairs);\n\trtnl_unlock();\n\treturn err;\n}\n\nstatic int virtnet_close(struct net_device *dev)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tint i;\n\n\t \n\tdisable_delayed_refill(vi);\n\t \n\tcancel_delayed_work_sync(&vi->refill);\n\n\tfor (i = 0; i < vi->max_queue_pairs; i++)\n\t\tvirtnet_disable_queue_pair(vi, i);\n\n\treturn 0;\n}\n\nstatic void virtnet_set_rx_mode(struct net_device *dev)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tstruct scatterlist sg[2];\n\tstruct virtio_net_ctrl_mac *mac_data;\n\tstruct netdev_hw_addr *ha;\n\tint uc_count;\n\tint mc_count;\n\tvoid *buf;\n\tint i;\n\n\t \n\tif (!virtio_has_feature(vi->vdev, VIRTIO_NET_F_CTRL_RX))\n\t\treturn;\n\n\tvi->ctrl->promisc = ((dev->flags & IFF_PROMISC) != 0);\n\tvi->ctrl->allmulti = ((dev->flags & IFF_ALLMULTI) != 0);\n\n\tsg_init_one(sg, &vi->ctrl->promisc, sizeof(vi->ctrl->promisc));\n\n\tif (!virtnet_send_command(vi, VIRTIO_NET_CTRL_RX,\n\t\t\t\t  VIRTIO_NET_CTRL_RX_PROMISC, sg))\n\t\tdev_warn(&dev->dev, \"Failed to %sable promisc mode.\\n\",\n\t\t\t vi->ctrl->promisc ? \"en\" : \"dis\");\n\n\tsg_init_one(sg, &vi->ctrl->allmulti, sizeof(vi->ctrl->allmulti));\n\n\tif (!virtnet_send_command(vi, VIRTIO_NET_CTRL_RX,\n\t\t\t\t  VIRTIO_NET_CTRL_RX_ALLMULTI, sg))\n\t\tdev_warn(&dev->dev, \"Failed to %sable allmulti mode.\\n\",\n\t\t\t vi->ctrl->allmulti ? \"en\" : \"dis\");\n\n\tuc_count = netdev_uc_count(dev);\n\tmc_count = netdev_mc_count(dev);\n\t \n\tbuf = kzalloc(((uc_count + mc_count) * ETH_ALEN) +\n\t\t      (2 * sizeof(mac_data->entries)), GFP_ATOMIC);\n\tmac_data = buf;\n\tif (!buf)\n\t\treturn;\n\n\tsg_init_table(sg, 2);\n\n\t \n\tmac_data->entries = cpu_to_virtio32(vi->vdev, uc_count);\n\ti = 0;\n\tnetdev_for_each_uc_addr(ha, dev)\n\t\tmemcpy(&mac_data->macs[i++][0], ha->addr, ETH_ALEN);\n\n\tsg_set_buf(&sg[0], mac_data,\n\t\t   sizeof(mac_data->entries) + (uc_count * ETH_ALEN));\n\n\t \n\tmac_data = (void *)&mac_data->macs[uc_count][0];\n\n\tmac_data->entries = cpu_to_virtio32(vi->vdev, mc_count);\n\ti = 0;\n\tnetdev_for_each_mc_addr(ha, dev)\n\t\tmemcpy(&mac_data->macs[i++][0], ha->addr, ETH_ALEN);\n\n\tsg_set_buf(&sg[1], mac_data,\n\t\t   sizeof(mac_data->entries) + (mc_count * ETH_ALEN));\n\n\tif (!virtnet_send_command(vi, VIRTIO_NET_CTRL_MAC,\n\t\t\t\t  VIRTIO_NET_CTRL_MAC_TABLE_SET, sg))\n\t\tdev_warn(&dev->dev, \"Failed to set MAC filter table.\\n\");\n\n\tkfree(buf);\n}\n\nstatic int virtnet_vlan_rx_add_vid(struct net_device *dev,\n\t\t\t\t   __be16 proto, u16 vid)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tstruct scatterlist sg;\n\n\tvi->ctrl->vid = cpu_to_virtio16(vi->vdev, vid);\n\tsg_init_one(&sg, &vi->ctrl->vid, sizeof(vi->ctrl->vid));\n\n\tif (!virtnet_send_command(vi, VIRTIO_NET_CTRL_VLAN,\n\t\t\t\t  VIRTIO_NET_CTRL_VLAN_ADD, &sg))\n\t\tdev_warn(&dev->dev, \"Failed to add VLAN ID %d.\\n\", vid);\n\treturn 0;\n}\n\nstatic int virtnet_vlan_rx_kill_vid(struct net_device *dev,\n\t\t\t\t    __be16 proto, u16 vid)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tstruct scatterlist sg;\n\n\tvi->ctrl->vid = cpu_to_virtio16(vi->vdev, vid);\n\tsg_init_one(&sg, &vi->ctrl->vid, sizeof(vi->ctrl->vid));\n\n\tif (!virtnet_send_command(vi, VIRTIO_NET_CTRL_VLAN,\n\t\t\t\t  VIRTIO_NET_CTRL_VLAN_DEL, &sg))\n\t\tdev_warn(&dev->dev, \"Failed to kill VLAN ID %d.\\n\", vid);\n\treturn 0;\n}\n\nstatic void virtnet_clean_affinity(struct virtnet_info *vi)\n{\n\tint i;\n\n\tif (vi->affinity_hint_set) {\n\t\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\t\tvirtqueue_set_affinity(vi->rq[i].vq, NULL);\n\t\t\tvirtqueue_set_affinity(vi->sq[i].vq, NULL);\n\t\t}\n\n\t\tvi->affinity_hint_set = false;\n\t}\n}\n\nstatic void virtnet_set_affinity(struct virtnet_info *vi)\n{\n\tcpumask_var_t mask;\n\tint stragglers;\n\tint group_size;\n\tint i, j, cpu;\n\tint num_cpu;\n\tint stride;\n\n\tif (!zalloc_cpumask_var(&mask, GFP_KERNEL)) {\n\t\tvirtnet_clean_affinity(vi);\n\t\treturn;\n\t}\n\n\tnum_cpu = num_online_cpus();\n\tstride = max_t(int, num_cpu / vi->curr_queue_pairs, 1);\n\tstragglers = num_cpu >= vi->curr_queue_pairs ?\n\t\t\tnum_cpu % vi->curr_queue_pairs :\n\t\t\t0;\n\tcpu = cpumask_first(cpu_online_mask);\n\n\tfor (i = 0; i < vi->curr_queue_pairs; i++) {\n\t\tgroup_size = stride + (i < stragglers ? 1 : 0);\n\n\t\tfor (j = 0; j < group_size; j++) {\n\t\t\tcpumask_set_cpu(cpu, mask);\n\t\t\tcpu = cpumask_next_wrap(cpu, cpu_online_mask,\n\t\t\t\t\t\tnr_cpu_ids, false);\n\t\t}\n\t\tvirtqueue_set_affinity(vi->rq[i].vq, mask);\n\t\tvirtqueue_set_affinity(vi->sq[i].vq, mask);\n\t\t__netif_set_xps_queue(vi->dev, cpumask_bits(mask), i, XPS_CPUS);\n\t\tcpumask_clear(mask);\n\t}\n\n\tvi->affinity_hint_set = true;\n\tfree_cpumask_var(mask);\n}\n\nstatic int virtnet_cpu_online(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct virtnet_info *vi = hlist_entry_safe(node, struct virtnet_info,\n\t\t\t\t\t\t   node);\n\tvirtnet_set_affinity(vi);\n\treturn 0;\n}\n\nstatic int virtnet_cpu_dead(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct virtnet_info *vi = hlist_entry_safe(node, struct virtnet_info,\n\t\t\t\t\t\t   node_dead);\n\tvirtnet_set_affinity(vi);\n\treturn 0;\n}\n\nstatic int virtnet_cpu_down_prep(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct virtnet_info *vi = hlist_entry_safe(node, struct virtnet_info,\n\t\t\t\t\t\t   node);\n\n\tvirtnet_clean_affinity(vi);\n\treturn 0;\n}\n\nstatic enum cpuhp_state virtionet_online;\n\nstatic int virtnet_cpu_notif_add(struct virtnet_info *vi)\n{\n\tint ret;\n\n\tret = cpuhp_state_add_instance_nocalls(virtionet_online, &vi->node);\n\tif (ret)\n\t\treturn ret;\n\tret = cpuhp_state_add_instance_nocalls(CPUHP_VIRT_NET_DEAD,\n\t\t\t\t\t       &vi->node_dead);\n\tif (!ret)\n\t\treturn ret;\n\tcpuhp_state_remove_instance_nocalls(virtionet_online, &vi->node);\n\treturn ret;\n}\n\nstatic void virtnet_cpu_notif_remove(struct virtnet_info *vi)\n{\n\tcpuhp_state_remove_instance_nocalls(virtionet_online, &vi->node);\n\tcpuhp_state_remove_instance_nocalls(CPUHP_VIRT_NET_DEAD,\n\t\t\t\t\t    &vi->node_dead);\n}\n\nstatic void virtnet_get_ringparam(struct net_device *dev,\n\t\t\t\t  struct ethtool_ringparam *ring,\n\t\t\t\t  struct kernel_ethtool_ringparam *kernel_ring,\n\t\t\t\t  struct netlink_ext_ack *extack)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\n\tring->rx_max_pending = vi->rq[0].vq->num_max;\n\tring->tx_max_pending = vi->sq[0].vq->num_max;\n\tring->rx_pending = virtqueue_get_vring_size(vi->rq[0].vq);\n\tring->tx_pending = virtqueue_get_vring_size(vi->sq[0].vq);\n}\n\nstatic int virtnet_send_ctrl_coal_vq_cmd(struct virtnet_info *vi,\n\t\t\t\t\t u16 vqn, u32 max_usecs, u32 max_packets);\n\nstatic int virtnet_set_ringparam(struct net_device *dev,\n\t\t\t\t struct ethtool_ringparam *ring,\n\t\t\t\t struct kernel_ethtool_ringparam *kernel_ring,\n\t\t\t\t struct netlink_ext_ack *extack)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tu32 rx_pending, tx_pending;\n\tstruct receive_queue *rq;\n\tstruct send_queue *sq;\n\tint i, err;\n\n\tif (ring->rx_mini_pending || ring->rx_jumbo_pending)\n\t\treturn -EINVAL;\n\n\trx_pending = virtqueue_get_vring_size(vi->rq[0].vq);\n\ttx_pending = virtqueue_get_vring_size(vi->sq[0].vq);\n\n\tif (ring->rx_pending == rx_pending &&\n\t    ring->tx_pending == tx_pending)\n\t\treturn 0;\n\n\tif (ring->rx_pending > vi->rq[0].vq->num_max)\n\t\treturn -EINVAL;\n\n\tif (ring->tx_pending > vi->sq[0].vq->num_max)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\trq = vi->rq + i;\n\t\tsq = vi->sq + i;\n\n\t\tif (ring->tx_pending != tx_pending) {\n\t\t\terr = virtnet_tx_resize(vi, sq, ring->tx_pending);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\t \n\t\t\terr = virtnet_send_ctrl_coal_vq_cmd(vi, txq2vq(i),\n\t\t\t\t\t\t\t    vi->intr_coal_tx.max_usecs,\n\t\t\t\t\t\t\t    vi->intr_coal_tx.max_packets);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tvi->sq[i].intr_coal.max_usecs = vi->intr_coal_tx.max_usecs;\n\t\t\tvi->sq[i].intr_coal.max_packets = vi->intr_coal_tx.max_packets;\n\t\t}\n\n\t\tif (ring->rx_pending != rx_pending) {\n\t\t\terr = virtnet_rx_resize(vi, rq, ring->rx_pending);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\t \n\t\t\terr = virtnet_send_ctrl_coal_vq_cmd(vi, rxq2vq(i),\n\t\t\t\t\t\t\t    vi->intr_coal_rx.max_usecs,\n\t\t\t\t\t\t\t    vi->intr_coal_rx.max_packets);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\n\t\t\tvi->rq[i].intr_coal.max_usecs = vi->intr_coal_rx.max_usecs;\n\t\t\tvi->rq[i].intr_coal.max_packets = vi->intr_coal_rx.max_packets;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic bool virtnet_commit_rss_command(struct virtnet_info *vi)\n{\n\tstruct net_device *dev = vi->dev;\n\tstruct scatterlist sgs[4];\n\tunsigned int sg_buf_size;\n\n\t \n\tsg_init_table(sgs, 4);\n\n\tsg_buf_size = offsetof(struct virtio_net_ctrl_rss, indirection_table);\n\tsg_set_buf(&sgs[0], &vi->ctrl->rss, sg_buf_size);\n\n\tsg_buf_size = sizeof(uint16_t) * (vi->ctrl->rss.indirection_table_mask + 1);\n\tsg_set_buf(&sgs[1], vi->ctrl->rss.indirection_table, sg_buf_size);\n\n\tsg_buf_size = offsetof(struct virtio_net_ctrl_rss, key)\n\t\t\t- offsetof(struct virtio_net_ctrl_rss, max_tx_vq);\n\tsg_set_buf(&sgs[2], &vi->ctrl->rss.max_tx_vq, sg_buf_size);\n\n\tsg_buf_size = vi->rss_key_size;\n\tsg_set_buf(&sgs[3], vi->ctrl->rss.key, sg_buf_size);\n\n\tif (!virtnet_send_command(vi, VIRTIO_NET_CTRL_MQ,\n\t\t\t\t  vi->has_rss ? VIRTIO_NET_CTRL_MQ_RSS_CONFIG\n\t\t\t\t  : VIRTIO_NET_CTRL_MQ_HASH_CONFIG, sgs)) {\n\t\tdev_warn(&dev->dev, \"VIRTIONET issue with committing RSS sgs\\n\");\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic void virtnet_init_default_rss(struct virtnet_info *vi)\n{\n\tu32 indir_val = 0;\n\tint i = 0;\n\n\tvi->ctrl->rss.hash_types = vi->rss_hash_types_supported;\n\tvi->rss_hash_types_saved = vi->rss_hash_types_supported;\n\tvi->ctrl->rss.indirection_table_mask = vi->rss_indir_table_size\n\t\t\t\t\t\t? vi->rss_indir_table_size - 1 : 0;\n\tvi->ctrl->rss.unclassified_queue = 0;\n\n\tfor (; i < vi->rss_indir_table_size; ++i) {\n\t\tindir_val = ethtool_rxfh_indir_default(i, vi->curr_queue_pairs);\n\t\tvi->ctrl->rss.indirection_table[i] = indir_val;\n\t}\n\n\tvi->ctrl->rss.max_tx_vq = vi->has_rss ? vi->curr_queue_pairs : 0;\n\tvi->ctrl->rss.hash_key_length = vi->rss_key_size;\n\n\tnetdev_rss_key_fill(vi->ctrl->rss.key, vi->rss_key_size);\n}\n\nstatic void virtnet_get_hashflow(const struct virtnet_info *vi, struct ethtool_rxnfc *info)\n{\n\tinfo->data = 0;\n\tswitch (info->flow_type) {\n\tcase TCP_V4_FLOW:\n\t\tif (vi->rss_hash_types_saved & VIRTIO_NET_RSS_HASH_TYPE_TCPv4) {\n\t\t\tinfo->data = RXH_IP_SRC | RXH_IP_DST |\n\t\t\t\t\t\t RXH_L4_B_0_1 | RXH_L4_B_2_3;\n\t\t} else if (vi->rss_hash_types_saved & VIRTIO_NET_RSS_HASH_TYPE_IPv4) {\n\t\t\tinfo->data = RXH_IP_SRC | RXH_IP_DST;\n\t\t}\n\t\tbreak;\n\tcase TCP_V6_FLOW:\n\t\tif (vi->rss_hash_types_saved & VIRTIO_NET_RSS_HASH_TYPE_TCPv6) {\n\t\t\tinfo->data = RXH_IP_SRC | RXH_IP_DST |\n\t\t\t\t\t\t RXH_L4_B_0_1 | RXH_L4_B_2_3;\n\t\t} else if (vi->rss_hash_types_saved & VIRTIO_NET_RSS_HASH_TYPE_IPv6) {\n\t\t\tinfo->data = RXH_IP_SRC | RXH_IP_DST;\n\t\t}\n\t\tbreak;\n\tcase UDP_V4_FLOW:\n\t\tif (vi->rss_hash_types_saved & VIRTIO_NET_RSS_HASH_TYPE_UDPv4) {\n\t\t\tinfo->data = RXH_IP_SRC | RXH_IP_DST |\n\t\t\t\t\t\t RXH_L4_B_0_1 | RXH_L4_B_2_3;\n\t\t} else if (vi->rss_hash_types_saved & VIRTIO_NET_RSS_HASH_TYPE_IPv4) {\n\t\t\tinfo->data = RXH_IP_SRC | RXH_IP_DST;\n\t\t}\n\t\tbreak;\n\tcase UDP_V6_FLOW:\n\t\tif (vi->rss_hash_types_saved & VIRTIO_NET_RSS_HASH_TYPE_UDPv6) {\n\t\t\tinfo->data = RXH_IP_SRC | RXH_IP_DST |\n\t\t\t\t\t\t RXH_L4_B_0_1 | RXH_L4_B_2_3;\n\t\t} else if (vi->rss_hash_types_saved & VIRTIO_NET_RSS_HASH_TYPE_IPv6) {\n\t\t\tinfo->data = RXH_IP_SRC | RXH_IP_DST;\n\t\t}\n\t\tbreak;\n\tcase IPV4_FLOW:\n\t\tif (vi->rss_hash_types_saved & VIRTIO_NET_RSS_HASH_TYPE_IPv4)\n\t\t\tinfo->data = RXH_IP_SRC | RXH_IP_DST;\n\n\t\tbreak;\n\tcase IPV6_FLOW:\n\t\tif (vi->rss_hash_types_saved & VIRTIO_NET_RSS_HASH_TYPE_IPv6)\n\t\t\tinfo->data = RXH_IP_SRC | RXH_IP_DST;\n\n\t\tbreak;\n\tdefault:\n\t\tinfo->data = 0;\n\t\tbreak;\n\t}\n}\n\nstatic bool virtnet_set_hashflow(struct virtnet_info *vi, struct ethtool_rxnfc *info)\n{\n\tu32 new_hashtypes = vi->rss_hash_types_saved;\n\tbool is_disable = info->data & RXH_DISCARD;\n\tbool is_l4 = info->data == (RXH_IP_SRC | RXH_IP_DST | RXH_L4_B_0_1 | RXH_L4_B_2_3);\n\n\t \n\tif (!((info->data == (RXH_IP_SRC | RXH_IP_DST)) | is_l4 | is_disable))\n\t\treturn false;\n\n\tswitch (info->flow_type) {\n\tcase TCP_V4_FLOW:\n\t\tnew_hashtypes &= ~(VIRTIO_NET_RSS_HASH_TYPE_IPv4 | VIRTIO_NET_RSS_HASH_TYPE_TCPv4);\n\t\tif (!is_disable)\n\t\t\tnew_hashtypes |= VIRTIO_NET_RSS_HASH_TYPE_IPv4\n\t\t\t\t| (is_l4 ? VIRTIO_NET_RSS_HASH_TYPE_TCPv4 : 0);\n\t\tbreak;\n\tcase UDP_V4_FLOW:\n\t\tnew_hashtypes &= ~(VIRTIO_NET_RSS_HASH_TYPE_IPv4 | VIRTIO_NET_RSS_HASH_TYPE_UDPv4);\n\t\tif (!is_disable)\n\t\t\tnew_hashtypes |= VIRTIO_NET_RSS_HASH_TYPE_IPv4\n\t\t\t\t| (is_l4 ? VIRTIO_NET_RSS_HASH_TYPE_UDPv4 : 0);\n\t\tbreak;\n\tcase IPV4_FLOW:\n\t\tnew_hashtypes &= ~VIRTIO_NET_RSS_HASH_TYPE_IPv4;\n\t\tif (!is_disable)\n\t\t\tnew_hashtypes = VIRTIO_NET_RSS_HASH_TYPE_IPv4;\n\t\tbreak;\n\tcase TCP_V6_FLOW:\n\t\tnew_hashtypes &= ~(VIRTIO_NET_RSS_HASH_TYPE_IPv6 | VIRTIO_NET_RSS_HASH_TYPE_TCPv6);\n\t\tif (!is_disable)\n\t\t\tnew_hashtypes |= VIRTIO_NET_RSS_HASH_TYPE_IPv6\n\t\t\t\t| (is_l4 ? VIRTIO_NET_RSS_HASH_TYPE_TCPv6 : 0);\n\t\tbreak;\n\tcase UDP_V6_FLOW:\n\t\tnew_hashtypes &= ~(VIRTIO_NET_RSS_HASH_TYPE_IPv6 | VIRTIO_NET_RSS_HASH_TYPE_UDPv6);\n\t\tif (!is_disable)\n\t\t\tnew_hashtypes |= VIRTIO_NET_RSS_HASH_TYPE_IPv6\n\t\t\t\t| (is_l4 ? VIRTIO_NET_RSS_HASH_TYPE_UDPv6 : 0);\n\t\tbreak;\n\tcase IPV6_FLOW:\n\t\tnew_hashtypes &= ~VIRTIO_NET_RSS_HASH_TYPE_IPv6;\n\t\tif (!is_disable)\n\t\t\tnew_hashtypes = VIRTIO_NET_RSS_HASH_TYPE_IPv6;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\treturn false;\n\t}\n\n\t \n\tif (new_hashtypes != (new_hashtypes & vi->rss_hash_types_supported))\n\t\treturn false;\n\n\tif (new_hashtypes != vi->rss_hash_types_saved) {\n\t\tvi->rss_hash_types_saved = new_hashtypes;\n\t\tvi->ctrl->rss.hash_types = vi->rss_hash_types_saved;\n\t\tif (vi->dev->features & NETIF_F_RXHASH)\n\t\t\treturn virtnet_commit_rss_command(vi);\n\t}\n\n\treturn true;\n}\n\nstatic void virtnet_get_drvinfo(struct net_device *dev,\n\t\t\t\tstruct ethtool_drvinfo *info)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tstruct virtio_device *vdev = vi->vdev;\n\n\tstrscpy(info->driver, KBUILD_MODNAME, sizeof(info->driver));\n\tstrscpy(info->version, VIRTNET_DRIVER_VERSION, sizeof(info->version));\n\tstrscpy(info->bus_info, virtio_bus_name(vdev), sizeof(info->bus_info));\n\n}\n\n \nstatic int virtnet_set_channels(struct net_device *dev,\n\t\t\t\tstruct ethtool_channels *channels)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tu16 queue_pairs = channels->combined_count;\n\tint err;\n\n\t \n\tif (channels->rx_count || channels->tx_count || channels->other_count)\n\t\treturn -EINVAL;\n\n\tif (queue_pairs > vi->max_queue_pairs || queue_pairs == 0)\n\t\treturn -EINVAL;\n\n\t \n\tif (vi->rq[0].xdp_prog)\n\t\treturn -EINVAL;\n\n\tcpus_read_lock();\n\terr = _virtnet_set_queues(vi, queue_pairs);\n\tif (err) {\n\t\tcpus_read_unlock();\n\t\tgoto err;\n\t}\n\tvirtnet_set_affinity(vi);\n\tcpus_read_unlock();\n\n\tnetif_set_real_num_tx_queues(dev, queue_pairs);\n\tnetif_set_real_num_rx_queues(dev, queue_pairs);\n err:\n\treturn err;\n}\n\nstatic void virtnet_get_strings(struct net_device *dev, u32 stringset, u8 *data)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tunsigned int i, j;\n\tu8 *p = data;\n\n\tswitch (stringset) {\n\tcase ETH_SS_STATS:\n\t\tfor (i = 0; i < vi->curr_queue_pairs; i++) {\n\t\t\tfor (j = 0; j < VIRTNET_RQ_STATS_LEN; j++)\n\t\t\t\tethtool_sprintf(&p, \"rx_queue_%u_%s\", i,\n\t\t\t\t\t\tvirtnet_rq_stats_desc[j].desc);\n\t\t}\n\n\t\tfor (i = 0; i < vi->curr_queue_pairs; i++) {\n\t\t\tfor (j = 0; j < VIRTNET_SQ_STATS_LEN; j++)\n\t\t\t\tethtool_sprintf(&p, \"tx_queue_%u_%s\", i,\n\t\t\t\t\t\tvirtnet_sq_stats_desc[j].desc);\n\t\t}\n\t\tbreak;\n\t}\n}\n\nstatic int virtnet_get_sset_count(struct net_device *dev, int sset)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\n\tswitch (sset) {\n\tcase ETH_SS_STATS:\n\t\treturn vi->curr_queue_pairs * (VIRTNET_RQ_STATS_LEN +\n\t\t\t\t\t       VIRTNET_SQ_STATS_LEN);\n\tdefault:\n\t\treturn -EOPNOTSUPP;\n\t}\n}\n\nstatic void virtnet_get_ethtool_stats(struct net_device *dev,\n\t\t\t\t      struct ethtool_stats *stats, u64 *data)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tunsigned int idx = 0, start, i, j;\n\tconst u8 *stats_base;\n\tconst u64_stats_t *p;\n\tsize_t offset;\n\n\tfor (i = 0; i < vi->curr_queue_pairs; i++) {\n\t\tstruct receive_queue *rq = &vi->rq[i];\n\n\t\tstats_base = (const u8 *)&rq->stats;\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&rq->stats.syncp);\n\t\t\tfor (j = 0; j < VIRTNET_RQ_STATS_LEN; j++) {\n\t\t\t\toffset = virtnet_rq_stats_desc[j].offset;\n\t\t\t\tp = (const u64_stats_t *)(stats_base + offset);\n\t\t\t\tdata[idx + j] = u64_stats_read(p);\n\t\t\t}\n\t\t} while (u64_stats_fetch_retry(&rq->stats.syncp, start));\n\t\tidx += VIRTNET_RQ_STATS_LEN;\n\t}\n\n\tfor (i = 0; i < vi->curr_queue_pairs; i++) {\n\t\tstruct send_queue *sq = &vi->sq[i];\n\n\t\tstats_base = (const u8 *)&sq->stats;\n\t\tdo {\n\t\t\tstart = u64_stats_fetch_begin(&sq->stats.syncp);\n\t\t\tfor (j = 0; j < VIRTNET_SQ_STATS_LEN; j++) {\n\t\t\t\toffset = virtnet_sq_stats_desc[j].offset;\n\t\t\t\tp = (const u64_stats_t *)(stats_base + offset);\n\t\t\t\tdata[idx + j] = u64_stats_read(p);\n\t\t\t}\n\t\t} while (u64_stats_fetch_retry(&sq->stats.syncp, start));\n\t\tidx += VIRTNET_SQ_STATS_LEN;\n\t}\n}\n\nstatic void virtnet_get_channels(struct net_device *dev,\n\t\t\t\t struct ethtool_channels *channels)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\n\tchannels->combined_count = vi->curr_queue_pairs;\n\tchannels->max_combined = vi->max_queue_pairs;\n\tchannels->max_other = 0;\n\tchannels->rx_count = 0;\n\tchannels->tx_count = 0;\n\tchannels->other_count = 0;\n}\n\nstatic int virtnet_set_link_ksettings(struct net_device *dev,\n\t\t\t\t      const struct ethtool_link_ksettings *cmd)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\n\treturn ethtool_virtdev_set_link_ksettings(dev, cmd,\n\t\t\t\t\t\t  &vi->speed, &vi->duplex);\n}\n\nstatic int virtnet_get_link_ksettings(struct net_device *dev,\n\t\t\t\t      struct ethtool_link_ksettings *cmd)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\n\tcmd->base.speed = vi->speed;\n\tcmd->base.duplex = vi->duplex;\n\tcmd->base.port = PORT_OTHER;\n\n\treturn 0;\n}\n\nstatic int virtnet_send_notf_coal_cmds(struct virtnet_info *vi,\n\t\t\t\t       struct ethtool_coalesce *ec)\n{\n\tstruct scatterlist sgs_tx, sgs_rx;\n\tint i;\n\n\tvi->ctrl->coal_tx.tx_usecs = cpu_to_le32(ec->tx_coalesce_usecs);\n\tvi->ctrl->coal_tx.tx_max_packets = cpu_to_le32(ec->tx_max_coalesced_frames);\n\tsg_init_one(&sgs_tx, &vi->ctrl->coal_tx, sizeof(vi->ctrl->coal_tx));\n\n\tif (!virtnet_send_command(vi, VIRTIO_NET_CTRL_NOTF_COAL,\n\t\t\t\t  VIRTIO_NET_CTRL_NOTF_COAL_TX_SET,\n\t\t\t\t  &sgs_tx))\n\t\treturn -EINVAL;\n\n\t \n\tvi->intr_coal_tx.max_usecs = ec->tx_coalesce_usecs;\n\tvi->intr_coal_tx.max_packets = ec->tx_max_coalesced_frames;\n\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\tvi->sq[i].intr_coal.max_usecs = ec->tx_coalesce_usecs;\n\t\tvi->sq[i].intr_coal.max_packets = ec->tx_max_coalesced_frames;\n\t}\n\n\tvi->ctrl->coal_rx.rx_usecs = cpu_to_le32(ec->rx_coalesce_usecs);\n\tvi->ctrl->coal_rx.rx_max_packets = cpu_to_le32(ec->rx_max_coalesced_frames);\n\tsg_init_one(&sgs_rx, &vi->ctrl->coal_rx, sizeof(vi->ctrl->coal_rx));\n\n\tif (!virtnet_send_command(vi, VIRTIO_NET_CTRL_NOTF_COAL,\n\t\t\t\t  VIRTIO_NET_CTRL_NOTF_COAL_RX_SET,\n\t\t\t\t  &sgs_rx))\n\t\treturn -EINVAL;\n\n\t \n\tvi->intr_coal_rx.max_usecs = ec->rx_coalesce_usecs;\n\tvi->intr_coal_rx.max_packets = ec->rx_max_coalesced_frames;\n\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\tvi->rq[i].intr_coal.max_usecs = ec->rx_coalesce_usecs;\n\t\tvi->rq[i].intr_coal.max_packets = ec->rx_max_coalesced_frames;\n\t}\n\n\treturn 0;\n}\n\nstatic int virtnet_send_ctrl_coal_vq_cmd(struct virtnet_info *vi,\n\t\t\t\t\t u16 vqn, u32 max_usecs, u32 max_packets)\n{\n\tstruct scatterlist sgs;\n\n\tvi->ctrl->coal_vq.vqn = cpu_to_le16(vqn);\n\tvi->ctrl->coal_vq.coal.max_usecs = cpu_to_le32(max_usecs);\n\tvi->ctrl->coal_vq.coal.max_packets = cpu_to_le32(max_packets);\n\tsg_init_one(&sgs, &vi->ctrl->coal_vq, sizeof(vi->ctrl->coal_vq));\n\n\tif (!virtnet_send_command(vi, VIRTIO_NET_CTRL_NOTF_COAL,\n\t\t\t\t  VIRTIO_NET_CTRL_NOTF_COAL_VQ_SET,\n\t\t\t\t  &sgs))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int virtnet_send_notf_coal_vq_cmds(struct virtnet_info *vi,\n\t\t\t\t\t  struct ethtool_coalesce *ec,\n\t\t\t\t\t  u16 queue)\n{\n\tint err;\n\n\terr = virtnet_send_ctrl_coal_vq_cmd(vi, rxq2vq(queue),\n\t\t\t\t\t    ec->rx_coalesce_usecs,\n\t\t\t\t\t    ec->rx_max_coalesced_frames);\n\tif (err)\n\t\treturn err;\n\n\tvi->rq[queue].intr_coal.max_usecs = ec->rx_coalesce_usecs;\n\tvi->rq[queue].intr_coal.max_packets = ec->rx_max_coalesced_frames;\n\n\terr = virtnet_send_ctrl_coal_vq_cmd(vi, txq2vq(queue),\n\t\t\t\t\t    ec->tx_coalesce_usecs,\n\t\t\t\t\t    ec->tx_max_coalesced_frames);\n\tif (err)\n\t\treturn err;\n\n\tvi->sq[queue].intr_coal.max_usecs = ec->tx_coalesce_usecs;\n\tvi->sq[queue].intr_coal.max_packets = ec->tx_max_coalesced_frames;\n\n\treturn 0;\n}\n\nstatic int virtnet_coal_params_supported(struct ethtool_coalesce *ec)\n{\n\t \n\tif (ec->rx_coalesce_usecs || ec->tx_coalesce_usecs)\n\t\treturn -EOPNOTSUPP;\n\n\tif (ec->tx_max_coalesced_frames > 1 ||\n\t    ec->rx_max_coalesced_frames != 1)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int virtnet_should_update_vq_weight(int dev_flags, int weight,\n\t\t\t\t\t   int vq_weight, bool *should_update)\n{\n\tif (weight ^ vq_weight) {\n\t\tif (dev_flags & IFF_UP)\n\t\t\treturn -EBUSY;\n\t\t*should_update = true;\n\t}\n\n\treturn 0;\n}\n\nstatic int virtnet_set_coalesce(struct net_device *dev,\n\t\t\t\tstruct ethtool_coalesce *ec,\n\t\t\t\tstruct kernel_ethtool_coalesce *kernel_coal,\n\t\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tint ret, queue_number, napi_weight;\n\tbool update_napi = false;\n\n\t \n\tnapi_weight = ec->tx_max_coalesced_frames ? NAPI_POLL_WEIGHT : 0;\n\tfor (queue_number = 0; queue_number < vi->max_queue_pairs; queue_number++) {\n\t\tret = virtnet_should_update_vq_weight(dev->flags, napi_weight,\n\t\t\t\t\t\t      vi->sq[queue_number].napi.weight,\n\t\t\t\t\t\t      &update_napi);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tif (update_napi) {\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (virtio_has_feature(vi->vdev, VIRTIO_NET_F_NOTF_COAL))\n\t\tret = virtnet_send_notf_coal_cmds(vi, ec);\n\telse\n\t\tret = virtnet_coal_params_supported(ec);\n\n\tif (ret)\n\t\treturn ret;\n\n\tif (update_napi) {\n\t\tfor (; queue_number < vi->max_queue_pairs; queue_number++)\n\t\t\tvi->sq[queue_number].napi.weight = napi_weight;\n\t}\n\n\treturn ret;\n}\n\nstatic int virtnet_get_coalesce(struct net_device *dev,\n\t\t\t\tstruct ethtool_coalesce *ec,\n\t\t\t\tstruct kernel_ethtool_coalesce *kernel_coal,\n\t\t\t\tstruct netlink_ext_ack *extack)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\n\tif (virtio_has_feature(vi->vdev, VIRTIO_NET_F_NOTF_COAL)) {\n\t\tec->rx_coalesce_usecs = vi->intr_coal_rx.max_usecs;\n\t\tec->tx_coalesce_usecs = vi->intr_coal_tx.max_usecs;\n\t\tec->tx_max_coalesced_frames = vi->intr_coal_tx.max_packets;\n\t\tec->rx_max_coalesced_frames = vi->intr_coal_rx.max_packets;\n\t} else {\n\t\tec->rx_max_coalesced_frames = 1;\n\n\t\tif (vi->sq[0].napi.weight)\n\t\t\tec->tx_max_coalesced_frames = 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int virtnet_set_per_queue_coalesce(struct net_device *dev,\n\t\t\t\t\t  u32 queue,\n\t\t\t\t\t  struct ethtool_coalesce *ec)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tint ret, napi_weight;\n\tbool update_napi = false;\n\n\tif (queue >= vi->max_queue_pairs)\n\t\treturn -EINVAL;\n\n\t \n\tnapi_weight = ec->tx_max_coalesced_frames ? NAPI_POLL_WEIGHT : 0;\n\tret = virtnet_should_update_vq_weight(dev->flags, napi_weight,\n\t\t\t\t\t      vi->sq[queue].napi.weight,\n\t\t\t\t\t      &update_napi);\n\tif (ret)\n\t\treturn ret;\n\n\tif (virtio_has_feature(vi->vdev, VIRTIO_NET_F_VQ_NOTF_COAL))\n\t\tret = virtnet_send_notf_coal_vq_cmds(vi, ec, queue);\n\telse\n\t\tret = virtnet_coal_params_supported(ec);\n\n\tif (ret)\n\t\treturn ret;\n\n\tif (update_napi)\n\t\tvi->sq[queue].napi.weight = napi_weight;\n\n\treturn 0;\n}\n\nstatic int virtnet_get_per_queue_coalesce(struct net_device *dev,\n\t\t\t\t\t  u32 queue,\n\t\t\t\t\t  struct ethtool_coalesce *ec)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\n\tif (queue >= vi->max_queue_pairs)\n\t\treturn -EINVAL;\n\n\tif (virtio_has_feature(vi->vdev, VIRTIO_NET_F_VQ_NOTF_COAL)) {\n\t\tec->rx_coalesce_usecs = vi->rq[queue].intr_coal.max_usecs;\n\t\tec->tx_coalesce_usecs = vi->sq[queue].intr_coal.max_usecs;\n\t\tec->tx_max_coalesced_frames = vi->sq[queue].intr_coal.max_packets;\n\t\tec->rx_max_coalesced_frames = vi->rq[queue].intr_coal.max_packets;\n\t} else {\n\t\tec->rx_max_coalesced_frames = 1;\n\n\t\tif (vi->sq[queue].napi.weight)\n\t\t\tec->tx_max_coalesced_frames = 1;\n\t}\n\n\treturn 0;\n}\n\nstatic void virtnet_init_settings(struct net_device *dev)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\n\tvi->speed = SPEED_UNKNOWN;\n\tvi->duplex = DUPLEX_UNKNOWN;\n}\n\nstatic void virtnet_update_settings(struct virtnet_info *vi)\n{\n\tu32 speed;\n\tu8 duplex;\n\n\tif (!virtio_has_feature(vi->vdev, VIRTIO_NET_F_SPEED_DUPLEX))\n\t\treturn;\n\n\tvirtio_cread_le(vi->vdev, struct virtio_net_config, speed, &speed);\n\n\tif (ethtool_validate_speed(speed))\n\t\tvi->speed = speed;\n\n\tvirtio_cread_le(vi->vdev, struct virtio_net_config, duplex, &duplex);\n\n\tif (ethtool_validate_duplex(duplex))\n\t\tvi->duplex = duplex;\n}\n\nstatic u32 virtnet_get_rxfh_key_size(struct net_device *dev)\n{\n\treturn ((struct virtnet_info *)netdev_priv(dev))->rss_key_size;\n}\n\nstatic u32 virtnet_get_rxfh_indir_size(struct net_device *dev)\n{\n\treturn ((struct virtnet_info *)netdev_priv(dev))->rss_indir_table_size;\n}\n\nstatic int virtnet_get_rxfh(struct net_device *dev, u32 *indir, u8 *key, u8 *hfunc)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tint i;\n\n\tif (indir) {\n\t\tfor (i = 0; i < vi->rss_indir_table_size; ++i)\n\t\t\tindir[i] = vi->ctrl->rss.indirection_table[i];\n\t}\n\n\tif (key)\n\t\tmemcpy(key, vi->ctrl->rss.key, vi->rss_key_size);\n\n\tif (hfunc)\n\t\t*hfunc = ETH_RSS_HASH_TOP;\n\n\treturn 0;\n}\n\nstatic int virtnet_set_rxfh(struct net_device *dev, const u32 *indir, const u8 *key, const u8 hfunc)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tint i;\n\n\tif (hfunc != ETH_RSS_HASH_NO_CHANGE && hfunc != ETH_RSS_HASH_TOP)\n\t\treturn -EOPNOTSUPP;\n\n\tif (indir) {\n\t\tfor (i = 0; i < vi->rss_indir_table_size; ++i)\n\t\t\tvi->ctrl->rss.indirection_table[i] = indir[i];\n\t}\n\tif (key)\n\t\tmemcpy(vi->ctrl->rss.key, key, vi->rss_key_size);\n\n\tvirtnet_commit_rss_command(vi);\n\n\treturn 0;\n}\n\nstatic int virtnet_get_rxnfc(struct net_device *dev, struct ethtool_rxnfc *info, u32 *rule_locs)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tint rc = 0;\n\n\tswitch (info->cmd) {\n\tcase ETHTOOL_GRXRINGS:\n\t\tinfo->data = vi->curr_queue_pairs;\n\t\tbreak;\n\tcase ETHTOOL_GRXFH:\n\t\tvirtnet_get_hashflow(vi, info);\n\t\tbreak;\n\tdefault:\n\t\trc = -EOPNOTSUPP;\n\t}\n\n\treturn rc;\n}\n\nstatic int virtnet_set_rxnfc(struct net_device *dev, struct ethtool_rxnfc *info)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tint rc = 0;\n\n\tswitch (info->cmd) {\n\tcase ETHTOOL_SRXFH:\n\t\tif (!virtnet_set_hashflow(vi, info))\n\t\t\trc = -EINVAL;\n\n\t\tbreak;\n\tdefault:\n\t\trc = -EOPNOTSUPP;\n\t}\n\n\treturn rc;\n}\n\nstatic const struct ethtool_ops virtnet_ethtool_ops = {\n\t.supported_coalesce_params = ETHTOOL_COALESCE_MAX_FRAMES |\n\t\tETHTOOL_COALESCE_USECS,\n\t.get_drvinfo = virtnet_get_drvinfo,\n\t.get_link = ethtool_op_get_link,\n\t.get_ringparam = virtnet_get_ringparam,\n\t.set_ringparam = virtnet_set_ringparam,\n\t.get_strings = virtnet_get_strings,\n\t.get_sset_count = virtnet_get_sset_count,\n\t.get_ethtool_stats = virtnet_get_ethtool_stats,\n\t.set_channels = virtnet_set_channels,\n\t.get_channels = virtnet_get_channels,\n\t.get_ts_info = ethtool_op_get_ts_info,\n\t.get_link_ksettings = virtnet_get_link_ksettings,\n\t.set_link_ksettings = virtnet_set_link_ksettings,\n\t.set_coalesce = virtnet_set_coalesce,\n\t.get_coalesce = virtnet_get_coalesce,\n\t.set_per_queue_coalesce = virtnet_set_per_queue_coalesce,\n\t.get_per_queue_coalesce = virtnet_get_per_queue_coalesce,\n\t.get_rxfh_key_size = virtnet_get_rxfh_key_size,\n\t.get_rxfh_indir_size = virtnet_get_rxfh_indir_size,\n\t.get_rxfh = virtnet_get_rxfh,\n\t.set_rxfh = virtnet_set_rxfh,\n\t.get_rxnfc = virtnet_get_rxnfc,\n\t.set_rxnfc = virtnet_set_rxnfc,\n};\n\nstatic void virtnet_freeze_down(struct virtio_device *vdev)\n{\n\tstruct virtnet_info *vi = vdev->priv;\n\n\t \n\tflush_work(&vi->config_work);\n\n\tnetif_tx_lock_bh(vi->dev);\n\tnetif_device_detach(vi->dev);\n\tnetif_tx_unlock_bh(vi->dev);\n\tif (netif_running(vi->dev))\n\t\tvirtnet_close(vi->dev);\n}\n\nstatic int init_vqs(struct virtnet_info *vi);\n\nstatic int virtnet_restore_up(struct virtio_device *vdev)\n{\n\tstruct virtnet_info *vi = vdev->priv;\n\tint err;\n\n\terr = init_vqs(vi);\n\tif (err)\n\t\treturn err;\n\n\tvirtio_device_ready(vdev);\n\n\tenable_delayed_refill(vi);\n\n\tif (netif_running(vi->dev)) {\n\t\terr = virtnet_open(vi->dev);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tnetif_tx_lock_bh(vi->dev);\n\tnetif_device_attach(vi->dev);\n\tnetif_tx_unlock_bh(vi->dev);\n\treturn err;\n}\n\nstatic int virtnet_set_guest_offloads(struct virtnet_info *vi, u64 offloads)\n{\n\tstruct scatterlist sg;\n\tvi->ctrl->offloads = cpu_to_virtio64(vi->vdev, offloads);\n\n\tsg_init_one(&sg, &vi->ctrl->offloads, sizeof(vi->ctrl->offloads));\n\n\tif (!virtnet_send_command(vi, VIRTIO_NET_CTRL_GUEST_OFFLOADS,\n\t\t\t\t  VIRTIO_NET_CTRL_GUEST_OFFLOADS_SET, &sg)) {\n\t\tdev_warn(&vi->dev->dev, \"Fail to set guest offload.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int virtnet_clear_guest_offloads(struct virtnet_info *vi)\n{\n\tu64 offloads = 0;\n\n\tif (!vi->guest_offloads)\n\t\treturn 0;\n\n\treturn virtnet_set_guest_offloads(vi, offloads);\n}\n\nstatic int virtnet_restore_guest_offloads(struct virtnet_info *vi)\n{\n\tu64 offloads = vi->guest_offloads;\n\n\tif (!vi->guest_offloads)\n\t\treturn 0;\n\n\treturn virtnet_set_guest_offloads(vi, offloads);\n}\n\nstatic int virtnet_xdp_set(struct net_device *dev, struct bpf_prog *prog,\n\t\t\t   struct netlink_ext_ack *extack)\n{\n\tunsigned int room = SKB_DATA_ALIGN(VIRTIO_XDP_HEADROOM +\n\t\t\t\t\t   sizeof(struct skb_shared_info));\n\tunsigned int max_sz = PAGE_SIZE - room - ETH_HLEN;\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tstruct bpf_prog *old_prog;\n\tu16 xdp_qp = 0, curr_qp;\n\tint i, err;\n\n\tif (!virtio_has_feature(vi->vdev, VIRTIO_NET_F_CTRL_GUEST_OFFLOADS)\n\t    && (virtio_has_feature(vi->vdev, VIRTIO_NET_F_GUEST_TSO4) ||\n\t        virtio_has_feature(vi->vdev, VIRTIO_NET_F_GUEST_TSO6) ||\n\t        virtio_has_feature(vi->vdev, VIRTIO_NET_F_GUEST_ECN) ||\n\t\tvirtio_has_feature(vi->vdev, VIRTIO_NET_F_GUEST_UFO) ||\n\t\tvirtio_has_feature(vi->vdev, VIRTIO_NET_F_GUEST_CSUM) ||\n\t\tvirtio_has_feature(vi->vdev, VIRTIO_NET_F_GUEST_USO4) ||\n\t\tvirtio_has_feature(vi->vdev, VIRTIO_NET_F_GUEST_USO6))) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"Can't set XDP while host is implementing GRO_HW/CSUM, disable GRO_HW/CSUM first\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (vi->mergeable_rx_bufs && !vi->any_header_sg) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"XDP expects header/data in single page, any_header_sg required\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (prog && !prog->aux->xdp_has_frags && dev->mtu > max_sz) {\n\t\tNL_SET_ERR_MSG_MOD(extack, \"MTU too large to enable XDP without frags\");\n\t\tnetdev_warn(dev, \"single-buffer XDP requires MTU less than %u\\n\", max_sz);\n\t\treturn -EINVAL;\n\t}\n\n\tcurr_qp = vi->curr_queue_pairs - vi->xdp_queue_pairs;\n\tif (prog)\n\t\txdp_qp = nr_cpu_ids;\n\n\t \n\tif (curr_qp + xdp_qp > vi->max_queue_pairs) {\n\t\tnetdev_warn_once(dev, \"XDP request %i queues but max is %i. XDP_TX and XDP_REDIRECT will operate in a slower locked tx mode.\\n\",\n\t\t\t\t curr_qp + xdp_qp, vi->max_queue_pairs);\n\t\txdp_qp = 0;\n\t}\n\n\told_prog = rtnl_dereference(vi->rq[0].xdp_prog);\n\tif (!prog && !old_prog)\n\t\treturn 0;\n\n\tif (prog)\n\t\tbpf_prog_add(prog, vi->max_queue_pairs - 1);\n\n\t \n\tif (netif_running(dev)) {\n\t\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\t\tnapi_disable(&vi->rq[i].napi);\n\t\t\tvirtnet_napi_tx_disable(&vi->sq[i].napi);\n\t\t}\n\t}\n\n\tif (!prog) {\n\t\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\t\trcu_assign_pointer(vi->rq[i].xdp_prog, prog);\n\t\t\tif (i == 0)\n\t\t\t\tvirtnet_restore_guest_offloads(vi);\n\t\t}\n\t\tsynchronize_net();\n\t}\n\n\terr = _virtnet_set_queues(vi, curr_qp + xdp_qp);\n\tif (err)\n\t\tgoto err;\n\tnetif_set_real_num_rx_queues(dev, curr_qp + xdp_qp);\n\tvi->xdp_queue_pairs = xdp_qp;\n\n\tif (prog) {\n\t\tvi->xdp_enabled = true;\n\t\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\t\trcu_assign_pointer(vi->rq[i].xdp_prog, prog);\n\t\t\tif (i == 0 && !old_prog)\n\t\t\t\tvirtnet_clear_guest_offloads(vi);\n\t\t}\n\t\tif (!old_prog)\n\t\t\txdp_features_set_redirect_target(dev, true);\n\t} else {\n\t\txdp_features_clear_redirect_target(dev);\n\t\tvi->xdp_enabled = false;\n\t}\n\n\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\tif (old_prog)\n\t\t\tbpf_prog_put(old_prog);\n\t\tif (netif_running(dev)) {\n\t\t\tvirtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);\n\t\t\tvirtnet_napi_tx_enable(vi, vi->sq[i].vq,\n\t\t\t\t\t       &vi->sq[i].napi);\n\t\t}\n\t}\n\n\treturn 0;\n\nerr:\n\tif (!prog) {\n\t\tvirtnet_clear_guest_offloads(vi);\n\t\tfor (i = 0; i < vi->max_queue_pairs; i++)\n\t\t\trcu_assign_pointer(vi->rq[i].xdp_prog, old_prog);\n\t}\n\n\tif (netif_running(dev)) {\n\t\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\t\tvirtnet_napi_enable(vi->rq[i].vq, &vi->rq[i].napi);\n\t\t\tvirtnet_napi_tx_enable(vi, vi->sq[i].vq,\n\t\t\t\t\t       &vi->sq[i].napi);\n\t\t}\n\t}\n\tif (prog)\n\t\tbpf_prog_sub(prog, vi->max_queue_pairs - 1);\n\treturn err;\n}\n\nstatic int virtnet_xdp(struct net_device *dev, struct netdev_bpf *xdp)\n{\n\tswitch (xdp->command) {\n\tcase XDP_SETUP_PROG:\n\t\treturn virtnet_xdp_set(dev, xdp->prog, xdp->extack);\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int virtnet_get_phys_port_name(struct net_device *dev, char *buf,\n\t\t\t\t      size_t len)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tint ret;\n\n\tif (!virtio_has_feature(vi->vdev, VIRTIO_NET_F_STANDBY))\n\t\treturn -EOPNOTSUPP;\n\n\tret = snprintf(buf, len, \"sby\");\n\tif (ret >= len)\n\t\treturn -EOPNOTSUPP;\n\n\treturn 0;\n}\n\nstatic int virtnet_set_features(struct net_device *dev,\n\t\t\t\tnetdev_features_t features)\n{\n\tstruct virtnet_info *vi = netdev_priv(dev);\n\tu64 offloads;\n\tint err;\n\n\tif ((dev->features ^ features) & NETIF_F_GRO_HW) {\n\t\tif (vi->xdp_enabled)\n\t\t\treturn -EBUSY;\n\n\t\tif (features & NETIF_F_GRO_HW)\n\t\t\toffloads = vi->guest_offloads_capable;\n\t\telse\n\t\t\toffloads = vi->guest_offloads_capable &\n\t\t\t\t   ~GUEST_OFFLOAD_GRO_HW_MASK;\n\n\t\terr = virtnet_set_guest_offloads(vi, offloads);\n\t\tif (err)\n\t\t\treturn err;\n\t\tvi->guest_offloads = offloads;\n\t}\n\n\tif ((dev->features ^ features) & NETIF_F_RXHASH) {\n\t\tif (features & NETIF_F_RXHASH)\n\t\t\tvi->ctrl->rss.hash_types = vi->rss_hash_types_saved;\n\t\telse\n\t\t\tvi->ctrl->rss.hash_types = VIRTIO_NET_HASH_REPORT_NONE;\n\n\t\tif (!virtnet_commit_rss_command(vi))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void virtnet_tx_timeout(struct net_device *dev, unsigned int txqueue)\n{\n\tstruct virtnet_info *priv = netdev_priv(dev);\n\tstruct send_queue *sq = &priv->sq[txqueue];\n\tstruct netdev_queue *txq = netdev_get_tx_queue(dev, txqueue);\n\n\tu64_stats_update_begin(&sq->stats.syncp);\n\tu64_stats_inc(&sq->stats.tx_timeouts);\n\tu64_stats_update_end(&sq->stats.syncp);\n\n\tnetdev_err(dev, \"TX timeout on queue: %u, sq: %s, vq: 0x%x, name: %s, %u usecs ago\\n\",\n\t\t   txqueue, sq->name, sq->vq->index, sq->vq->name,\n\t\t   jiffies_to_usecs(jiffies - READ_ONCE(txq->trans_start)));\n}\n\nstatic const struct net_device_ops virtnet_netdev = {\n\t.ndo_open            = virtnet_open,\n\t.ndo_stop   \t     = virtnet_close,\n\t.ndo_start_xmit      = start_xmit,\n\t.ndo_validate_addr   = eth_validate_addr,\n\t.ndo_set_mac_address = virtnet_set_mac_address,\n\t.ndo_set_rx_mode     = virtnet_set_rx_mode,\n\t.ndo_get_stats64     = virtnet_stats,\n\t.ndo_vlan_rx_add_vid = virtnet_vlan_rx_add_vid,\n\t.ndo_vlan_rx_kill_vid = virtnet_vlan_rx_kill_vid,\n\t.ndo_bpf\t\t= virtnet_xdp,\n\t.ndo_xdp_xmit\t\t= virtnet_xdp_xmit,\n\t.ndo_features_check\t= passthru_features_check,\n\t.ndo_get_phys_port_name\t= virtnet_get_phys_port_name,\n\t.ndo_set_features\t= virtnet_set_features,\n\t.ndo_tx_timeout\t\t= virtnet_tx_timeout,\n};\n\nstatic void virtnet_config_changed_work(struct work_struct *work)\n{\n\tstruct virtnet_info *vi =\n\t\tcontainer_of(work, struct virtnet_info, config_work);\n\tu16 v;\n\n\tif (virtio_cread_feature(vi->vdev, VIRTIO_NET_F_STATUS,\n\t\t\t\t struct virtio_net_config, status, &v) < 0)\n\t\treturn;\n\n\tif (v & VIRTIO_NET_S_ANNOUNCE) {\n\t\tnetdev_notify_peers(vi->dev);\n\t\tvirtnet_ack_link_announce(vi);\n\t}\n\n\t \n\tv &= VIRTIO_NET_S_LINK_UP;\n\n\tif (vi->status == v)\n\t\treturn;\n\n\tvi->status = v;\n\n\tif (vi->status & VIRTIO_NET_S_LINK_UP) {\n\t\tvirtnet_update_settings(vi);\n\t\tnetif_carrier_on(vi->dev);\n\t\tnetif_tx_wake_all_queues(vi->dev);\n\t} else {\n\t\tnetif_carrier_off(vi->dev);\n\t\tnetif_tx_stop_all_queues(vi->dev);\n\t}\n}\n\nstatic void virtnet_config_changed(struct virtio_device *vdev)\n{\n\tstruct virtnet_info *vi = vdev->priv;\n\n\tschedule_work(&vi->config_work);\n}\n\nstatic void virtnet_free_queues(struct virtnet_info *vi)\n{\n\tint i;\n\n\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\t__netif_napi_del(&vi->rq[i].napi);\n\t\t__netif_napi_del(&vi->sq[i].napi);\n\t}\n\n\t \n\tsynchronize_net();\n\n\tkfree(vi->rq);\n\tkfree(vi->sq);\n\tkfree(vi->ctrl);\n}\n\nstatic void _free_receive_bufs(struct virtnet_info *vi)\n{\n\tstruct bpf_prog *old_prog;\n\tint i;\n\n\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\twhile (vi->rq[i].pages)\n\t\t\t__free_pages(get_a_page(&vi->rq[i], GFP_KERNEL), 0);\n\n\t\told_prog = rtnl_dereference(vi->rq[i].xdp_prog);\n\t\tRCU_INIT_POINTER(vi->rq[i].xdp_prog, NULL);\n\t\tif (old_prog)\n\t\t\tbpf_prog_put(old_prog);\n\t}\n}\n\nstatic void free_receive_bufs(struct virtnet_info *vi)\n{\n\trtnl_lock();\n\t_free_receive_bufs(vi);\n\trtnl_unlock();\n}\n\nstatic void free_receive_page_frags(struct virtnet_info *vi)\n{\n\tint i;\n\tfor (i = 0; i < vi->max_queue_pairs; i++)\n\t\tif (vi->rq[i].alloc_frag.page) {\n\t\t\tif (vi->rq[i].do_dma && vi->rq[i].last_dma)\n\t\t\t\tvirtnet_rq_unmap(&vi->rq[i], vi->rq[i].last_dma, 0);\n\t\t\tput_page(vi->rq[i].alloc_frag.page);\n\t\t}\n}\n\nstatic void virtnet_sq_free_unused_buf(struct virtqueue *vq, void *buf)\n{\n\tif (!is_xdp_frame(buf))\n\t\tdev_kfree_skb(buf);\n\telse\n\t\txdp_return_frame(ptr_to_xdp(buf));\n}\n\nstatic void free_unused_bufs(struct virtnet_info *vi)\n{\n\tvoid *buf;\n\tint i;\n\n\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\tstruct virtqueue *vq = vi->sq[i].vq;\n\t\twhile ((buf = virtqueue_detach_unused_buf(vq)) != NULL)\n\t\t\tvirtnet_sq_free_unused_buf(vq, buf);\n\t\tcond_resched();\n\t}\n\n\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\tstruct virtqueue *vq = vi->rq[i].vq;\n\n\t\twhile ((buf = virtqueue_detach_unused_buf(vq)) != NULL)\n\t\t\tvirtnet_rq_unmap_free_buf(vq, buf);\n\t\tcond_resched();\n\t}\n}\n\nstatic void virtnet_del_vqs(struct virtnet_info *vi)\n{\n\tstruct virtio_device *vdev = vi->vdev;\n\n\tvirtnet_clean_affinity(vi);\n\n\tvdev->config->del_vqs(vdev);\n\n\tvirtnet_free_queues(vi);\n}\n\n \nstatic unsigned int mergeable_min_buf_len(struct virtnet_info *vi, struct virtqueue *vq)\n{\n\tconst unsigned int hdr_len = vi->hdr_len;\n\tunsigned int rq_size = virtqueue_get_vring_size(vq);\n\tunsigned int packet_len = vi->big_packets ? IP_MAX_MTU : vi->dev->max_mtu;\n\tunsigned int buf_len = hdr_len + ETH_HLEN + VLAN_HLEN + packet_len;\n\tunsigned int min_buf_len = DIV_ROUND_UP(buf_len, rq_size);\n\n\treturn max(max(min_buf_len, hdr_len) - hdr_len,\n\t\t   (unsigned int)GOOD_PACKET_LEN);\n}\n\nstatic int virtnet_find_vqs(struct virtnet_info *vi)\n{\n\tvq_callback_t **callbacks;\n\tstruct virtqueue **vqs;\n\tint ret = -ENOMEM;\n\tint i, total_vqs;\n\tconst char **names;\n\tbool *ctx;\n\n\t \n\ttotal_vqs = vi->max_queue_pairs * 2 +\n\t\t    virtio_has_feature(vi->vdev, VIRTIO_NET_F_CTRL_VQ);\n\n\t \n\tvqs = kcalloc(total_vqs, sizeof(*vqs), GFP_KERNEL);\n\tif (!vqs)\n\t\tgoto err_vq;\n\tcallbacks = kmalloc_array(total_vqs, sizeof(*callbacks), GFP_KERNEL);\n\tif (!callbacks)\n\t\tgoto err_callback;\n\tnames = kmalloc_array(total_vqs, sizeof(*names), GFP_KERNEL);\n\tif (!names)\n\t\tgoto err_names;\n\tif (!vi->big_packets || vi->mergeable_rx_bufs) {\n\t\tctx = kcalloc(total_vqs, sizeof(*ctx), GFP_KERNEL);\n\t\tif (!ctx)\n\t\t\tgoto err_ctx;\n\t} else {\n\t\tctx = NULL;\n\t}\n\n\t \n\tif (vi->has_cvq) {\n\t\tcallbacks[total_vqs - 1] = NULL;\n\t\tnames[total_vqs - 1] = \"control\";\n\t}\n\n\t \n\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\tcallbacks[rxq2vq(i)] = skb_recv_done;\n\t\tcallbacks[txq2vq(i)] = skb_xmit_done;\n\t\tsprintf(vi->rq[i].name, \"input.%d\", i);\n\t\tsprintf(vi->sq[i].name, \"output.%d\", i);\n\t\tnames[rxq2vq(i)] = vi->rq[i].name;\n\t\tnames[txq2vq(i)] = vi->sq[i].name;\n\t\tif (ctx)\n\t\t\tctx[rxq2vq(i)] = true;\n\t}\n\n\tret = virtio_find_vqs_ctx(vi->vdev, total_vqs, vqs, callbacks,\n\t\t\t\t  names, ctx, NULL);\n\tif (ret)\n\t\tgoto err_find;\n\n\tif (vi->has_cvq) {\n\t\tvi->cvq = vqs[total_vqs - 1];\n\t\tif (virtio_has_feature(vi->vdev, VIRTIO_NET_F_CTRL_VLAN))\n\t\t\tvi->dev->features |= NETIF_F_HW_VLAN_CTAG_FILTER;\n\t}\n\n\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\tvi->rq[i].vq = vqs[rxq2vq(i)];\n\t\tvi->rq[i].min_buf_len = mergeable_min_buf_len(vi, vi->rq[i].vq);\n\t\tvi->sq[i].vq = vqs[txq2vq(i)];\n\t}\n\n\t \n\n\nerr_find:\n\tkfree(ctx);\nerr_ctx:\n\tkfree(names);\nerr_names:\n\tkfree(callbacks);\nerr_callback:\n\tkfree(vqs);\nerr_vq:\n\treturn ret;\n}\n\nstatic int virtnet_alloc_queues(struct virtnet_info *vi)\n{\n\tint i;\n\n\tif (vi->has_cvq) {\n\t\tvi->ctrl = kzalloc(sizeof(*vi->ctrl), GFP_KERNEL);\n\t\tif (!vi->ctrl)\n\t\t\tgoto err_ctrl;\n\t} else {\n\t\tvi->ctrl = NULL;\n\t}\n\tvi->sq = kcalloc(vi->max_queue_pairs, sizeof(*vi->sq), GFP_KERNEL);\n\tif (!vi->sq)\n\t\tgoto err_sq;\n\tvi->rq = kcalloc(vi->max_queue_pairs, sizeof(*vi->rq), GFP_KERNEL);\n\tif (!vi->rq)\n\t\tgoto err_rq;\n\n\tINIT_DELAYED_WORK(&vi->refill, refill_work);\n\tfor (i = 0; i < vi->max_queue_pairs; i++) {\n\t\tvi->rq[i].pages = NULL;\n\t\tnetif_napi_add_weight(vi->dev, &vi->rq[i].napi, virtnet_poll,\n\t\t\t\t      napi_weight);\n\t\tnetif_napi_add_tx_weight(vi->dev, &vi->sq[i].napi,\n\t\t\t\t\t virtnet_poll_tx,\n\t\t\t\t\t napi_tx ? napi_weight : 0);\n\n\t\tsg_init_table(vi->rq[i].sg, ARRAY_SIZE(vi->rq[i].sg));\n\t\tewma_pkt_len_init(&vi->rq[i].mrg_avg_pkt_len);\n\t\tsg_init_table(vi->sq[i].sg, ARRAY_SIZE(vi->sq[i].sg));\n\n\t\tu64_stats_init(&vi->rq[i].stats.syncp);\n\t\tu64_stats_init(&vi->sq[i].stats.syncp);\n\t}\n\n\treturn 0;\n\nerr_rq:\n\tkfree(vi->sq);\nerr_sq:\n\tkfree(vi->ctrl);\nerr_ctrl:\n\treturn -ENOMEM;\n}\n\nstatic int init_vqs(struct virtnet_info *vi)\n{\n\tint ret;\n\n\t \n\tret = virtnet_alloc_queues(vi);\n\tif (ret)\n\t\tgoto err;\n\n\tret = virtnet_find_vqs(vi);\n\tif (ret)\n\t\tgoto err_free;\n\n\tvirtnet_rq_set_premapped(vi);\n\n\tcpus_read_lock();\n\tvirtnet_set_affinity(vi);\n\tcpus_read_unlock();\n\n\treturn 0;\n\nerr_free:\n\tvirtnet_free_queues(vi);\nerr:\n\treturn ret;\n}\n\n#ifdef CONFIG_SYSFS\nstatic ssize_t mergeable_rx_buffer_size_show(struct netdev_rx_queue *queue,\n\t\tchar *buf)\n{\n\tstruct virtnet_info *vi = netdev_priv(queue->dev);\n\tunsigned int queue_index = get_netdev_rx_queue_index(queue);\n\tunsigned int headroom = virtnet_get_headroom(vi);\n\tunsigned int tailroom = headroom ? sizeof(struct skb_shared_info) : 0;\n\tstruct ewma_pkt_len *avg;\n\n\tBUG_ON(queue_index >= vi->max_queue_pairs);\n\tavg = &vi->rq[queue_index].mrg_avg_pkt_len;\n\treturn sprintf(buf, \"%u\\n\",\n\t\t       get_mergeable_buf_len(&vi->rq[queue_index], avg,\n\t\t\t\t       SKB_DATA_ALIGN(headroom + tailroom)));\n}\n\nstatic struct rx_queue_attribute mergeable_rx_buffer_size_attribute =\n\t__ATTR_RO(mergeable_rx_buffer_size);\n\nstatic struct attribute *virtio_net_mrg_rx_attrs[] = {\n\t&mergeable_rx_buffer_size_attribute.attr,\n\tNULL\n};\n\nstatic const struct attribute_group virtio_net_mrg_rx_group = {\n\t.name = \"virtio_net\",\n\t.attrs = virtio_net_mrg_rx_attrs\n};\n#endif\n\nstatic bool virtnet_fail_on_feature(struct virtio_device *vdev,\n\t\t\t\t    unsigned int fbit,\n\t\t\t\t    const char *fname, const char *dname)\n{\n\tif (!virtio_has_feature(vdev, fbit))\n\t\treturn false;\n\n\tdev_err(&vdev->dev, \"device advertises feature %s but not %s\",\n\t\tfname, dname);\n\n\treturn true;\n}\n\n#define VIRTNET_FAIL_ON(vdev, fbit, dbit)\t\t\t\\\n\tvirtnet_fail_on_feature(vdev, fbit, #fbit, dbit)\n\nstatic bool virtnet_validate_features(struct virtio_device *vdev)\n{\n\tif (!virtio_has_feature(vdev, VIRTIO_NET_F_CTRL_VQ) &&\n\t    (VIRTNET_FAIL_ON(vdev, VIRTIO_NET_F_CTRL_RX,\n\t\t\t     \"VIRTIO_NET_F_CTRL_VQ\") ||\n\t     VIRTNET_FAIL_ON(vdev, VIRTIO_NET_F_CTRL_VLAN,\n\t\t\t     \"VIRTIO_NET_F_CTRL_VQ\") ||\n\t     VIRTNET_FAIL_ON(vdev, VIRTIO_NET_F_GUEST_ANNOUNCE,\n\t\t\t     \"VIRTIO_NET_F_CTRL_VQ\") ||\n\t     VIRTNET_FAIL_ON(vdev, VIRTIO_NET_F_MQ, \"VIRTIO_NET_F_CTRL_VQ\") ||\n\t     VIRTNET_FAIL_ON(vdev, VIRTIO_NET_F_CTRL_MAC_ADDR,\n\t\t\t     \"VIRTIO_NET_F_CTRL_VQ\") ||\n\t     VIRTNET_FAIL_ON(vdev, VIRTIO_NET_F_RSS,\n\t\t\t     \"VIRTIO_NET_F_CTRL_VQ\") ||\n\t     VIRTNET_FAIL_ON(vdev, VIRTIO_NET_F_HASH_REPORT,\n\t\t\t     \"VIRTIO_NET_F_CTRL_VQ\") ||\n\t     VIRTNET_FAIL_ON(vdev, VIRTIO_NET_F_NOTF_COAL,\n\t\t\t     \"VIRTIO_NET_F_CTRL_VQ\") ||\n\t     VIRTNET_FAIL_ON(vdev, VIRTIO_NET_F_VQ_NOTF_COAL,\n\t\t\t     \"VIRTIO_NET_F_CTRL_VQ\"))) {\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n#define MIN_MTU ETH_MIN_MTU\n#define MAX_MTU ETH_MAX_MTU\n\nstatic int virtnet_validate(struct virtio_device *vdev)\n{\n\tif (!vdev->config->get) {\n\t\tdev_err(&vdev->dev, \"%s failure: config access disabled\\n\",\n\t\t\t__func__);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!virtnet_validate_features(vdev))\n\t\treturn -EINVAL;\n\n\tif (virtio_has_feature(vdev, VIRTIO_NET_F_MTU)) {\n\t\tint mtu = virtio_cread16(vdev,\n\t\t\t\t\t offsetof(struct virtio_net_config,\n\t\t\t\t\t\t  mtu));\n\t\tif (mtu < MIN_MTU)\n\t\t\t__virtio_clear_bit(vdev, VIRTIO_NET_F_MTU);\n\t}\n\n\tif (virtio_has_feature(vdev, VIRTIO_NET_F_STANDBY) &&\n\t    !virtio_has_feature(vdev, VIRTIO_NET_F_MAC)) {\n\t\tdev_warn(&vdev->dev, \"device advertises feature VIRTIO_NET_F_STANDBY but not VIRTIO_NET_F_MAC, disabling standby\");\n\t\t__virtio_clear_bit(vdev, VIRTIO_NET_F_STANDBY);\n\t}\n\n\treturn 0;\n}\n\nstatic bool virtnet_check_guest_gso(const struct virtnet_info *vi)\n{\n\treturn virtio_has_feature(vi->vdev, VIRTIO_NET_F_GUEST_TSO4) ||\n\t\tvirtio_has_feature(vi->vdev, VIRTIO_NET_F_GUEST_TSO6) ||\n\t\tvirtio_has_feature(vi->vdev, VIRTIO_NET_F_GUEST_ECN) ||\n\t\tvirtio_has_feature(vi->vdev, VIRTIO_NET_F_GUEST_UFO) ||\n\t\t(virtio_has_feature(vi->vdev, VIRTIO_NET_F_GUEST_USO4) &&\n\t\tvirtio_has_feature(vi->vdev, VIRTIO_NET_F_GUEST_USO6));\n}\n\nstatic void virtnet_set_big_packets(struct virtnet_info *vi, const int mtu)\n{\n\tbool guest_gso = virtnet_check_guest_gso(vi);\n\n\t \n\tif (mtu > ETH_DATA_LEN || guest_gso) {\n\t\tvi->big_packets = true;\n\t\tvi->big_packets_num_skbfrags = guest_gso ? MAX_SKB_FRAGS : DIV_ROUND_UP(mtu, PAGE_SIZE);\n\t}\n}\n\nstatic int virtnet_probe(struct virtio_device *vdev)\n{\n\tint i, err = -ENOMEM;\n\tstruct net_device *dev;\n\tstruct virtnet_info *vi;\n\tu16 max_queue_pairs;\n\tint mtu = 0;\n\n\t \n\tmax_queue_pairs = 1;\n\tif (virtio_has_feature(vdev, VIRTIO_NET_F_MQ) || virtio_has_feature(vdev, VIRTIO_NET_F_RSS))\n\t\tmax_queue_pairs =\n\t\t     virtio_cread16(vdev, offsetof(struct virtio_net_config, max_virtqueue_pairs));\n\n\t \n\tif (max_queue_pairs < VIRTIO_NET_CTRL_MQ_VQ_PAIRS_MIN ||\n\t    max_queue_pairs > VIRTIO_NET_CTRL_MQ_VQ_PAIRS_MAX ||\n\t    !virtio_has_feature(vdev, VIRTIO_NET_F_CTRL_VQ))\n\t\tmax_queue_pairs = 1;\n\n\t \n\tdev = alloc_etherdev_mq(sizeof(struct virtnet_info), max_queue_pairs);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\n\t \n\tdev->priv_flags |= IFF_UNICAST_FLT | IFF_LIVE_ADDR_CHANGE |\n\t\t\t   IFF_TX_SKB_NO_LINEAR;\n\tdev->netdev_ops = &virtnet_netdev;\n\tdev->features = NETIF_F_HIGHDMA;\n\n\tdev->ethtool_ops = &virtnet_ethtool_ops;\n\tSET_NETDEV_DEV(dev, &vdev->dev);\n\n\t \n\tif (virtio_has_feature(vdev, VIRTIO_NET_F_CSUM)) {\n\t\t \n\t\tdev->hw_features |= NETIF_F_HW_CSUM | NETIF_F_SG;\n\t\tif (csum)\n\t\t\tdev->features |= NETIF_F_HW_CSUM | NETIF_F_SG;\n\n\t\tif (virtio_has_feature(vdev, VIRTIO_NET_F_GSO)) {\n\t\t\tdev->hw_features |= NETIF_F_TSO\n\t\t\t\t| NETIF_F_TSO_ECN | NETIF_F_TSO6;\n\t\t}\n\t\t \n\t\tif (virtio_has_feature(vdev, VIRTIO_NET_F_HOST_TSO4))\n\t\t\tdev->hw_features |= NETIF_F_TSO;\n\t\tif (virtio_has_feature(vdev, VIRTIO_NET_F_HOST_TSO6))\n\t\t\tdev->hw_features |= NETIF_F_TSO6;\n\t\tif (virtio_has_feature(vdev, VIRTIO_NET_F_HOST_ECN))\n\t\t\tdev->hw_features |= NETIF_F_TSO_ECN;\n\t\tif (virtio_has_feature(vdev, VIRTIO_NET_F_HOST_USO))\n\t\t\tdev->hw_features |= NETIF_F_GSO_UDP_L4;\n\n\t\tdev->features |= NETIF_F_GSO_ROBUST;\n\n\t\tif (gso)\n\t\t\tdev->features |= dev->hw_features & NETIF_F_ALL_TSO;\n\t\t \n\t}\n\tif (virtio_has_feature(vdev, VIRTIO_NET_F_GUEST_CSUM))\n\t\tdev->features |= NETIF_F_RXCSUM;\n\tif (virtio_has_feature(vdev, VIRTIO_NET_F_GUEST_TSO4) ||\n\t    virtio_has_feature(vdev, VIRTIO_NET_F_GUEST_TSO6))\n\t\tdev->features |= NETIF_F_GRO_HW;\n\tif (virtio_has_feature(vdev, VIRTIO_NET_F_CTRL_GUEST_OFFLOADS))\n\t\tdev->hw_features |= NETIF_F_GRO_HW;\n\n\tdev->vlan_features = dev->features;\n\tdev->xdp_features = NETDEV_XDP_ACT_BASIC | NETDEV_XDP_ACT_REDIRECT;\n\n\t \n\tdev->min_mtu = MIN_MTU;\n\tdev->max_mtu = MAX_MTU;\n\n\t \n\tif (virtio_has_feature(vdev, VIRTIO_NET_F_MAC)) {\n\t\tu8 addr[ETH_ALEN];\n\n\t\tvirtio_cread_bytes(vdev,\n\t\t\t\t   offsetof(struct virtio_net_config, mac),\n\t\t\t\t   addr, ETH_ALEN);\n\t\teth_hw_addr_set(dev, addr);\n\t} else {\n\t\teth_hw_addr_random(dev);\n\t\tdev_info(&vdev->dev, \"Assigned random MAC address %pM\\n\",\n\t\t\t dev->dev_addr);\n\t}\n\n\t \n\tvi = netdev_priv(dev);\n\tvi->dev = dev;\n\tvi->vdev = vdev;\n\tvdev->priv = vi;\n\n\tINIT_WORK(&vi->config_work, virtnet_config_changed_work);\n\tspin_lock_init(&vi->refill_lock);\n\n\tif (virtio_has_feature(vdev, VIRTIO_NET_F_MRG_RXBUF)) {\n\t\tvi->mergeable_rx_bufs = true;\n\t\tdev->xdp_features |= NETDEV_XDP_ACT_RX_SG;\n\t}\n\n\tif (virtio_has_feature(vi->vdev, VIRTIO_NET_F_NOTF_COAL)) {\n\t\tvi->intr_coal_rx.max_usecs = 0;\n\t\tvi->intr_coal_tx.max_usecs = 0;\n\t\tvi->intr_coal_tx.max_packets = 0;\n\t\tvi->intr_coal_rx.max_packets = 0;\n\t}\n\n\tif (virtio_has_feature(vdev, VIRTIO_NET_F_HASH_REPORT))\n\t\tvi->has_rss_hash_report = true;\n\n\tif (virtio_has_feature(vdev, VIRTIO_NET_F_RSS))\n\t\tvi->has_rss = true;\n\n\tif (vi->has_rss || vi->has_rss_hash_report) {\n\t\tvi->rss_indir_table_size =\n\t\t\tvirtio_cread16(vdev, offsetof(struct virtio_net_config,\n\t\t\t\trss_max_indirection_table_length));\n\t\tvi->rss_key_size =\n\t\t\tvirtio_cread8(vdev, offsetof(struct virtio_net_config, rss_max_key_size));\n\n\t\tvi->rss_hash_types_supported =\n\t\t    virtio_cread32(vdev, offsetof(struct virtio_net_config, supported_hash_types));\n\t\tvi->rss_hash_types_supported &=\n\t\t\t\t~(VIRTIO_NET_RSS_HASH_TYPE_IP_EX |\n\t\t\t\t  VIRTIO_NET_RSS_HASH_TYPE_TCP_EX |\n\t\t\t\t  VIRTIO_NET_RSS_HASH_TYPE_UDP_EX);\n\n\t\tdev->hw_features |= NETIF_F_RXHASH;\n\t}\n\n\tif (vi->has_rss_hash_report)\n\t\tvi->hdr_len = sizeof(struct virtio_net_hdr_v1_hash);\n\telse if (virtio_has_feature(vdev, VIRTIO_NET_F_MRG_RXBUF) ||\n\t\t virtio_has_feature(vdev, VIRTIO_F_VERSION_1))\n\t\tvi->hdr_len = sizeof(struct virtio_net_hdr_mrg_rxbuf);\n\telse\n\t\tvi->hdr_len = sizeof(struct virtio_net_hdr);\n\n\tif (virtio_has_feature(vdev, VIRTIO_F_ANY_LAYOUT) ||\n\t    virtio_has_feature(vdev, VIRTIO_F_VERSION_1))\n\t\tvi->any_header_sg = true;\n\n\tif (virtio_has_feature(vdev, VIRTIO_NET_F_CTRL_VQ))\n\t\tvi->has_cvq = true;\n\n\tif (virtio_has_feature(vdev, VIRTIO_NET_F_MTU)) {\n\t\tmtu = virtio_cread16(vdev,\n\t\t\t\t     offsetof(struct virtio_net_config,\n\t\t\t\t\t      mtu));\n\t\tif (mtu < dev->min_mtu) {\n\t\t\t \n\t\t\tdev_err(&vdev->dev,\n\t\t\t\t\"device MTU appears to have changed it is now %d < %d\",\n\t\t\t\tmtu, dev->min_mtu);\n\t\t\terr = -EINVAL;\n\t\t\tgoto free;\n\t\t}\n\n\t\tdev->mtu = mtu;\n\t\tdev->max_mtu = mtu;\n\t}\n\n\tvirtnet_set_big_packets(vi, mtu);\n\n\tif (vi->any_header_sg)\n\t\tdev->needed_headroom = vi->hdr_len;\n\n\t \n\tif (num_online_cpus() >= max_queue_pairs)\n\t\tvi->curr_queue_pairs = max_queue_pairs;\n\telse\n\t\tvi->curr_queue_pairs = num_online_cpus();\n\tvi->max_queue_pairs = max_queue_pairs;\n\n\t \n\terr = init_vqs(vi);\n\tif (err)\n\t\tgoto free;\n\n#ifdef CONFIG_SYSFS\n\tif (vi->mergeable_rx_bufs)\n\t\tdev->sysfs_rx_queue_group = &virtio_net_mrg_rx_group;\n#endif\n\tnetif_set_real_num_tx_queues(dev, vi->curr_queue_pairs);\n\tnetif_set_real_num_rx_queues(dev, vi->curr_queue_pairs);\n\n\tvirtnet_init_settings(dev);\n\n\tif (virtio_has_feature(vdev, VIRTIO_NET_F_STANDBY)) {\n\t\tvi->failover = net_failover_create(vi->dev);\n\t\tif (IS_ERR(vi->failover)) {\n\t\t\terr = PTR_ERR(vi->failover);\n\t\t\tgoto free_vqs;\n\t\t}\n\t}\n\n\tif (vi->has_rss || vi->has_rss_hash_report)\n\t\tvirtnet_init_default_rss(vi);\n\n\t \n\trtnl_lock();\n\n\terr = register_netdevice(dev);\n\tif (err) {\n\t\tpr_debug(\"virtio_net: registering device failed\\n\");\n\t\trtnl_unlock();\n\t\tgoto free_failover;\n\t}\n\n\tvirtio_device_ready(vdev);\n\n\t_virtnet_set_queues(vi, vi->curr_queue_pairs);\n\n\t \n\tif (!virtio_has_feature(vdev, VIRTIO_NET_F_MAC) &&\n\t    virtio_has_feature(vi->vdev, VIRTIO_NET_F_CTRL_MAC_ADDR)) {\n\t\tstruct scatterlist sg;\n\n\t\tsg_init_one(&sg, dev->dev_addr, dev->addr_len);\n\t\tif (!virtnet_send_command(vi, VIRTIO_NET_CTRL_MAC,\n\t\t\t\t\t  VIRTIO_NET_CTRL_MAC_ADDR_SET, &sg)) {\n\t\t\tpr_debug(\"virtio_net: setting MAC address failed\\n\");\n\t\t\trtnl_unlock();\n\t\t\terr = -EINVAL;\n\t\t\tgoto free_unregister_netdev;\n\t\t}\n\t}\n\n\trtnl_unlock();\n\n\terr = virtnet_cpu_notif_add(vi);\n\tif (err) {\n\t\tpr_debug(\"virtio_net: registering cpu notifier failed\\n\");\n\t\tgoto free_unregister_netdev;\n\t}\n\n\t \n\tnetif_carrier_off(dev);\n\tif (virtio_has_feature(vi->vdev, VIRTIO_NET_F_STATUS)) {\n\t\tschedule_work(&vi->config_work);\n\t} else {\n\t\tvi->status = VIRTIO_NET_S_LINK_UP;\n\t\tvirtnet_update_settings(vi);\n\t\tnetif_carrier_on(dev);\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(guest_offloads); i++)\n\t\tif (virtio_has_feature(vi->vdev, guest_offloads[i]))\n\t\t\tset_bit(guest_offloads[i], &vi->guest_offloads);\n\tvi->guest_offloads_capable = vi->guest_offloads;\n\n\tpr_debug(\"virtnet: registered device %s with %d RX and TX vq's\\n\",\n\t\t dev->name, max_queue_pairs);\n\n\treturn 0;\n\nfree_unregister_netdev:\n\tunregister_netdev(dev);\nfree_failover:\n\tnet_failover_destroy(vi->failover);\nfree_vqs:\n\tvirtio_reset_device(vdev);\n\tcancel_delayed_work_sync(&vi->refill);\n\tfree_receive_page_frags(vi);\n\tvirtnet_del_vqs(vi);\nfree:\n\tfree_netdev(dev);\n\treturn err;\n}\n\nstatic void remove_vq_common(struct virtnet_info *vi)\n{\n\tvirtio_reset_device(vi->vdev);\n\n\t \n\tfree_unused_bufs(vi);\n\n\tfree_receive_bufs(vi);\n\n\tfree_receive_page_frags(vi);\n\n\tvirtnet_del_vqs(vi);\n}\n\nstatic void virtnet_remove(struct virtio_device *vdev)\n{\n\tstruct virtnet_info *vi = vdev->priv;\n\n\tvirtnet_cpu_notif_remove(vi);\n\n\t \n\tflush_work(&vi->config_work);\n\n\tunregister_netdev(vi->dev);\n\n\tnet_failover_destroy(vi->failover);\n\n\tremove_vq_common(vi);\n\n\tfree_netdev(vi->dev);\n}\n\nstatic __maybe_unused int virtnet_freeze(struct virtio_device *vdev)\n{\n\tstruct virtnet_info *vi = vdev->priv;\n\n\tvirtnet_cpu_notif_remove(vi);\n\tvirtnet_freeze_down(vdev);\n\tremove_vq_common(vi);\n\n\treturn 0;\n}\n\nstatic __maybe_unused int virtnet_restore(struct virtio_device *vdev)\n{\n\tstruct virtnet_info *vi = vdev->priv;\n\tint err;\n\n\terr = virtnet_restore_up(vdev);\n\tif (err)\n\t\treturn err;\n\tvirtnet_set_queues(vi, vi->curr_queue_pairs);\n\n\terr = virtnet_cpu_notif_add(vi);\n\tif (err) {\n\t\tvirtnet_freeze_down(vdev);\n\t\tremove_vq_common(vi);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic struct virtio_device_id id_table[] = {\n\t{ VIRTIO_ID_NET, VIRTIO_DEV_ANY_ID },\n\t{ 0 },\n};\n\n#define VIRTNET_FEATURES \\\n\tVIRTIO_NET_F_CSUM, VIRTIO_NET_F_GUEST_CSUM, \\\n\tVIRTIO_NET_F_MAC, \\\n\tVIRTIO_NET_F_HOST_TSO4, VIRTIO_NET_F_HOST_UFO, VIRTIO_NET_F_HOST_TSO6, \\\n\tVIRTIO_NET_F_HOST_ECN, VIRTIO_NET_F_GUEST_TSO4, VIRTIO_NET_F_GUEST_TSO6, \\\n\tVIRTIO_NET_F_GUEST_ECN, VIRTIO_NET_F_GUEST_UFO, \\\n\tVIRTIO_NET_F_HOST_USO, VIRTIO_NET_F_GUEST_USO4, VIRTIO_NET_F_GUEST_USO6, \\\n\tVIRTIO_NET_F_MRG_RXBUF, VIRTIO_NET_F_STATUS, VIRTIO_NET_F_CTRL_VQ, \\\n\tVIRTIO_NET_F_CTRL_RX, VIRTIO_NET_F_CTRL_VLAN, \\\n\tVIRTIO_NET_F_GUEST_ANNOUNCE, VIRTIO_NET_F_MQ, \\\n\tVIRTIO_NET_F_CTRL_MAC_ADDR, \\\n\tVIRTIO_NET_F_MTU, VIRTIO_NET_F_CTRL_GUEST_OFFLOADS, \\\n\tVIRTIO_NET_F_SPEED_DUPLEX, VIRTIO_NET_F_STANDBY, \\\n\tVIRTIO_NET_F_RSS, VIRTIO_NET_F_HASH_REPORT, VIRTIO_NET_F_NOTF_COAL, \\\n\tVIRTIO_NET_F_VQ_NOTF_COAL, \\\n\tVIRTIO_NET_F_GUEST_HDRLEN\n\nstatic unsigned int features[] = {\n\tVIRTNET_FEATURES,\n};\n\nstatic unsigned int features_legacy[] = {\n\tVIRTNET_FEATURES,\n\tVIRTIO_NET_F_GSO,\n\tVIRTIO_F_ANY_LAYOUT,\n};\n\nstatic struct virtio_driver virtio_net_driver = {\n\t.feature_table = features,\n\t.feature_table_size = ARRAY_SIZE(features),\n\t.feature_table_legacy = features_legacy,\n\t.feature_table_size_legacy = ARRAY_SIZE(features_legacy),\n\t.driver.name =\tKBUILD_MODNAME,\n\t.driver.owner =\tTHIS_MODULE,\n\t.id_table =\tid_table,\n\t.validate =\tvirtnet_validate,\n\t.probe =\tvirtnet_probe,\n\t.remove =\tvirtnet_remove,\n\t.config_changed = virtnet_config_changed,\n#ifdef CONFIG_PM_SLEEP\n\t.freeze =\tvirtnet_freeze,\n\t.restore =\tvirtnet_restore,\n#endif\n};\n\nstatic __init int virtio_net_driver_init(void)\n{\n\tint ret;\n\n\tret = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN, \"virtio/net:online\",\n\t\t\t\t      virtnet_cpu_online,\n\t\t\t\t      virtnet_cpu_down_prep);\n\tif (ret < 0)\n\t\tgoto out;\n\tvirtionet_online = ret;\n\tret = cpuhp_setup_state_multi(CPUHP_VIRT_NET_DEAD, \"virtio/net:dead\",\n\t\t\t\t      NULL, virtnet_cpu_dead);\n\tif (ret)\n\t\tgoto err_dead;\n\tret = register_virtio_driver(&virtio_net_driver);\n\tif (ret)\n\t\tgoto err_virtio;\n\treturn 0;\nerr_virtio:\n\tcpuhp_remove_multi_state(CPUHP_VIRT_NET_DEAD);\nerr_dead:\n\tcpuhp_remove_multi_state(virtionet_online);\nout:\n\treturn ret;\n}\nmodule_init(virtio_net_driver_init);\n\nstatic __exit void virtio_net_driver_exit(void)\n{\n\tunregister_virtio_driver(&virtio_net_driver);\n\tcpuhp_remove_multi_state(CPUHP_VIRT_NET_DEAD);\n\tcpuhp_remove_multi_state(virtionet_online);\n}\nmodule_exit(virtio_net_driver_exit);\n\nMODULE_DEVICE_TABLE(virtio, id_table);\nMODULE_DESCRIPTION(\"Virtio network driver\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}