{
  "module_name": "queueing.h",
  "hash_id": "f3ac5f872427a9a4265a60707d135b9089c77f7f8c5478a0ccd7b859295b66f5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/net/wireguard/queueing.h",
  "human_readable_source": " \n \n\n#ifndef _WG_QUEUEING_H\n#define _WG_QUEUEING_H\n\n#include \"peer.h\"\n#include <linux/types.h>\n#include <linux/skbuff.h>\n#include <linux/ip.h>\n#include <linux/ipv6.h>\n#include <net/ip_tunnels.h>\n\nstruct wg_device;\nstruct wg_peer;\nstruct multicore_worker;\nstruct crypt_queue;\nstruct prev_queue;\nstruct sk_buff;\n\n \nint wg_packet_queue_init(struct crypt_queue *queue, work_func_t function,\n\t\t\t unsigned int len);\nvoid wg_packet_queue_free(struct crypt_queue *queue, bool purge);\nstruct multicore_worker __percpu *\nwg_packet_percpu_multicore_worker_alloc(work_func_t function, void *ptr);\n\n \nvoid wg_packet_receive(struct wg_device *wg, struct sk_buff *skb);\nvoid wg_packet_handshake_receive_worker(struct work_struct *work);\n \nint wg_packet_rx_poll(struct napi_struct *napi, int budget);\n \nvoid wg_packet_decrypt_worker(struct work_struct *work);\n\n \nvoid wg_packet_send_queued_handshake_initiation(struct wg_peer *peer,\n\t\t\t\t\t\tbool is_retry);\nvoid wg_packet_send_handshake_response(struct wg_peer *peer);\nvoid wg_packet_send_handshake_cookie(struct wg_device *wg,\n\t\t\t\t     struct sk_buff *initiating_skb,\n\t\t\t\t     __le32 sender_index);\nvoid wg_packet_send_keepalive(struct wg_peer *peer);\nvoid wg_packet_purge_staged_packets(struct wg_peer *peer);\nvoid wg_packet_send_staged_packets(struct wg_peer *peer);\n \nvoid wg_packet_handshake_send_worker(struct work_struct *work);\nvoid wg_packet_tx_worker(struct work_struct *work);\nvoid wg_packet_encrypt_worker(struct work_struct *work);\n\nenum packet_state {\n\tPACKET_STATE_UNCRYPTED,\n\tPACKET_STATE_CRYPTED,\n\tPACKET_STATE_DEAD\n};\n\nstruct packet_cb {\n\tu64 nonce;\n\tstruct noise_keypair *keypair;\n\tatomic_t state;\n\tu32 mtu;\n\tu8 ds;\n};\n\n#define PACKET_CB(skb) ((struct packet_cb *)((skb)->cb))\n#define PACKET_PEER(skb) (PACKET_CB(skb)->keypair->entry.peer)\n\nstatic inline bool wg_check_packet_protocol(struct sk_buff *skb)\n{\n\t__be16 real_protocol = ip_tunnel_parse_protocol(skb);\n\treturn real_protocol && skb->protocol == real_protocol;\n}\n\nstatic inline void wg_reset_packet(struct sk_buff *skb, bool encapsulating)\n{\n\tu8 l4_hash = skb->l4_hash;\n\tu8 sw_hash = skb->sw_hash;\n\tu32 hash = skb->hash;\n\tskb_scrub_packet(skb, true);\n\tmemset(&skb->headers, 0, sizeof(skb->headers));\n\tif (encapsulating) {\n\t\tskb->l4_hash = l4_hash;\n\t\tskb->sw_hash = sw_hash;\n\t\tskb->hash = hash;\n\t}\n\tskb->queue_mapping = 0;\n\tskb->nohdr = 0;\n\tskb->peeked = 0;\n\tskb->mac_len = 0;\n\tskb->dev = NULL;\n#ifdef CONFIG_NET_SCHED\n\tskb->tc_index = 0;\n#endif\n\tskb_reset_redirect(skb);\n\tskb->hdr_len = skb_headroom(skb);\n\tskb_reset_mac_header(skb);\n\tskb_reset_network_header(skb);\n\tskb_reset_transport_header(skb);\n\tskb_probe_transport_header(skb);\n\tskb_reset_inner_headers(skb);\n}\n\nstatic inline int wg_cpumask_choose_online(int *stored_cpu, unsigned int id)\n{\n\tunsigned int cpu = *stored_cpu, cpu_index, i;\n\n\tif (unlikely(cpu >= nr_cpu_ids ||\n\t\t     !cpumask_test_cpu(cpu, cpu_online_mask))) {\n\t\tcpu_index = id % cpumask_weight(cpu_online_mask);\n\t\tcpu = cpumask_first(cpu_online_mask);\n\t\tfor (i = 0; i < cpu_index; ++i)\n\t\t\tcpu = cpumask_next(cpu, cpu_online_mask);\n\t\t*stored_cpu = cpu;\n\t}\n\treturn cpu;\n}\n\n \nstatic inline int wg_cpumask_next_online(int *last_cpu)\n{\n\tint cpu = cpumask_next(*last_cpu, cpu_online_mask);\n\tif (cpu >= nr_cpu_ids)\n\t\tcpu = cpumask_first(cpu_online_mask);\n\t*last_cpu = cpu;\n\treturn cpu;\n}\n\nvoid wg_prev_queue_init(struct prev_queue *queue);\n\n \nbool wg_prev_queue_enqueue(struct prev_queue *queue, struct sk_buff *skb);\n\n \nstruct sk_buff *wg_prev_queue_dequeue(struct prev_queue *queue);\n\n \nstatic inline struct sk_buff *wg_prev_queue_peek(struct prev_queue *queue)\n{\n\tif (queue->peeked)\n\t\treturn queue->peeked;\n\tqueue->peeked = wg_prev_queue_dequeue(queue);\n\treturn queue->peeked;\n}\n\n \nstatic inline void wg_prev_queue_drop_peeked(struct prev_queue *queue)\n{\n\tqueue->peeked = NULL;\n}\n\nstatic inline int wg_queue_enqueue_per_device_and_peer(\n\tstruct crypt_queue *device_queue, struct prev_queue *peer_queue,\n\tstruct sk_buff *skb, struct workqueue_struct *wq)\n{\n\tint cpu;\n\n\tatomic_set_release(&PACKET_CB(skb)->state, PACKET_STATE_UNCRYPTED);\n\t \n\tif (unlikely(!wg_prev_queue_enqueue(peer_queue, skb)))\n\t\treturn -ENOSPC;\n\n\t \n\tcpu = wg_cpumask_next_online(&device_queue->last_cpu);\n\tif (unlikely(ptr_ring_produce_bh(&device_queue->ring, skb)))\n\t\treturn -EPIPE;\n\tqueue_work_on(cpu, wq, &per_cpu_ptr(device_queue->worker, cpu)->work);\n\treturn 0;\n}\n\nstatic inline void wg_queue_enqueue_per_peer_tx(struct sk_buff *skb, enum packet_state state)\n{\n\t \n\tstruct wg_peer *peer = wg_peer_get(PACKET_PEER(skb));\n\n\tatomic_set_release(&PACKET_CB(skb)->state, state);\n\tqueue_work_on(wg_cpumask_choose_online(&peer->serial_work_cpu, peer->internal_id),\n\t\t      peer->device->packet_crypt_wq, &peer->transmit_packet_work);\n\twg_peer_put(peer);\n}\n\nstatic inline void wg_queue_enqueue_per_peer_rx(struct sk_buff *skb, enum packet_state state)\n{\n\t \n\tstruct wg_peer *peer = wg_peer_get(PACKET_PEER(skb));\n\n\tatomic_set_release(&PACKET_CB(skb)->state, state);\n\tnapi_schedule(&peer->napi);\n\twg_peer_put(peer);\n}\n\n#ifdef DEBUG\nbool wg_packet_counter_selftest(void);\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}