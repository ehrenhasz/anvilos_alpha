{
  "module_name": "intel-nand-controller.c",
  "hash_id": "82283990c85e15a9c4394c57163e9e2e135e127d1c445aaa02e2f7ef0a87b371",
  "original_prompt": "Ingested from linux-6.6.14/drivers/mtd/nand/raw/intel-nand-controller.c",
  "human_readable_source": "\n \n\n#include <linux/clk.h>\n#include <linux/completion.h>\n#include <linux/dmaengine.h>\n#include <linux/dma-direction.h>\n#include <linux/dma-mapping.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/iopoll.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n\n#include <linux/mtd/mtd.h>\n#include <linux/mtd/rawnand.h>\n#include <linux/mtd/nand.h>\n\n#include <linux/of.h>\n#include <linux/platform_device.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/units.h>\n#include <asm/unaligned.h>\n\n#define EBU_CLC\t\t\t0x000\n#define EBU_CLC_RST\t\t0x00000000u\n\n#define EBU_ADDR_SEL(n)\t\t(0x020 + (n) * 4)\n \n#define EBU_ADDR_MASK(x)\t((x) << 4)\n#define EBU_ADDR_SEL_REGEN\t0x1\n\n#define EBU_BUSCON(n)\t\t(0x060 + (n) * 4)\n#define EBU_BUSCON_CMULT_V4\t0x1\n#define EBU_BUSCON_RECOVC(n)\t((n) << 2)\n#define EBU_BUSCON_HOLDC(n)\t((n) << 4)\n#define EBU_BUSCON_WAITRDC(n)\t((n) << 6)\n#define EBU_BUSCON_WAITWRC(n)\t((n) << 8)\n#define EBU_BUSCON_BCGEN_CS\t0x0\n#define EBU_BUSCON_SETUP_EN\tBIT(22)\n#define EBU_BUSCON_ALEC\t\t0xC000\n\n#define EBU_CON\t\t\t0x0B0\n#define EBU_CON_NANDM_EN\tBIT(0)\n#define EBU_CON_NANDM_DIS\t0x0\n#define EBU_CON_CSMUX_E_EN\tBIT(1)\n#define EBU_CON_ALE_P_LOW\tBIT(2)\n#define EBU_CON_CLE_P_LOW\tBIT(3)\n#define EBU_CON_CS_P_LOW\tBIT(4)\n#define EBU_CON_SE_P_LOW\tBIT(5)\n#define EBU_CON_WP_P_LOW\tBIT(6)\n#define EBU_CON_PRE_P_LOW\tBIT(7)\n#define EBU_CON_IN_CS_S(n)\t((n) << 8)\n#define EBU_CON_OUT_CS_S(n)\t((n) << 10)\n#define EBU_CON_LAT_EN_CS_P\t((0x3D) << 18)\n\n#define EBU_WAIT\t\t0x0B4\n#define EBU_WAIT_RDBY\t\tBIT(0)\n#define EBU_WAIT_WR_C\t\tBIT(3)\n\n#define HSNAND_CTL1\t\t0x110\n#define HSNAND_CTL1_ADDR_SHIFT\t24\n\n#define HSNAND_CTL2\t\t0x114\n#define HSNAND_CTL2_ADDR_SHIFT\t8\n#define HSNAND_CTL2_CYC_N_V5\t(0x2 << 16)\n\n#define HSNAND_INT_MSK_CTL\t0x124\n#define HSNAND_INT_MSK_CTL_WR_C\tBIT(4)\n\n#define HSNAND_INT_STA\t\t0x128\n#define HSNAND_INT_STA_WR_C\tBIT(4)\n\n#define HSNAND_CTL\t\t0x130\n#define HSNAND_CTL_ENABLE_ECC\tBIT(0)\n#define HSNAND_CTL_GO\t\tBIT(2)\n#define HSNAND_CTL_CE_SEL_CS(n)\tBIT(3 + (n))\n#define HSNAND_CTL_RW_READ\t0x0\n#define HSNAND_CTL_RW_WRITE\tBIT(10)\n#define HSNAND_CTL_ECC_OFF_V8TH\tBIT(11)\n#define HSNAND_CTL_CKFF_EN\t0x0\n#define HSNAND_CTL_MSG_EN\tBIT(17)\n\n#define HSNAND_PARA0\t\t0x13c\n#define HSNAND_PARA0_PAGE_V8192\t0x3\n#define HSNAND_PARA0_PIB_V256\t(0x3 << 4)\n#define HSNAND_PARA0_BYP_EN_NP\t0x0\n#define HSNAND_PARA0_BYP_DEC_NP\t0x0\n#define HSNAND_PARA0_TYPE_ONFI\tBIT(18)\n#define HSNAND_PARA0_ADEP_EN\tBIT(21)\n\n#define HSNAND_CMSG_0\t\t0x150\n#define HSNAND_CMSG_1\t\t0x154\n\n#define HSNAND_ALE_OFFS\t\tBIT(2)\n#define HSNAND_CLE_OFFS\t\tBIT(3)\n#define HSNAND_CS_OFFS\t\tBIT(4)\n\n#define HSNAND_ECC_OFFSET\t0x008\n\n#define MAX_CS\t2\n\n#define USEC_PER_SEC\t1000000L\n\nstruct ebu_nand_cs {\n\tvoid __iomem *chipaddr;\n\tu32 addr_sel;\n};\n\nstruct ebu_nand_controller {\n\tstruct nand_controller controller;\n\tstruct nand_chip chip;\n\tstruct device *dev;\n\tvoid __iomem *ebu;\n\tvoid __iomem *hsnand;\n\tstruct dma_chan *dma_tx;\n\tstruct dma_chan *dma_rx;\n\tstruct completion dma_access_complete;\n\tstruct clk *clk;\n\tu32 nd_para0;\n\tu8 cs_num;\n\tstruct ebu_nand_cs cs[MAX_CS];\n};\n\nstatic inline struct ebu_nand_controller *nand_to_ebu(struct nand_chip *chip)\n{\n\treturn container_of(chip, struct ebu_nand_controller, chip);\n}\n\nstatic int ebu_nand_waitrdy(struct nand_chip *chip, int timeout_ms)\n{\n\tstruct ebu_nand_controller *ctrl = nand_to_ebu(chip);\n\tu32 status;\n\n\treturn readl_poll_timeout(ctrl->ebu + EBU_WAIT, status,\n\t\t\t\t  (status & EBU_WAIT_RDBY) ||\n\t\t\t\t  (status & EBU_WAIT_WR_C), 20, timeout_ms);\n}\n\nstatic u8 ebu_nand_readb(struct nand_chip *chip)\n{\n\tstruct ebu_nand_controller *ebu_host = nand_get_controller_data(chip);\n\tu8 cs_num = ebu_host->cs_num;\n\tu8 val;\n\n\tval = readb(ebu_host->cs[cs_num].chipaddr + HSNAND_CS_OFFS);\n\tebu_nand_waitrdy(chip, 1000);\n\treturn val;\n}\n\nstatic void ebu_nand_writeb(struct nand_chip *chip, u32 offset, u8 value)\n{\n\tstruct ebu_nand_controller *ebu_host = nand_get_controller_data(chip);\n\tu8 cs_num = ebu_host->cs_num;\n\n\twriteb(value, ebu_host->cs[cs_num].chipaddr + offset);\n\tebu_nand_waitrdy(chip, 1000);\n}\n\nstatic void ebu_read_buf(struct nand_chip *chip, u_char *buf, unsigned int len)\n{\n\tint i;\n\n\tfor (i = 0; i < len; i++)\n\t\tbuf[i] = ebu_nand_readb(chip);\n}\n\nstatic void ebu_write_buf(struct nand_chip *chip, const u_char *buf, int len)\n{\n\tint i;\n\n\tfor (i = 0; i < len; i++)\n\t\tebu_nand_writeb(chip, HSNAND_CS_OFFS, buf[i]);\n}\n\nstatic void ebu_nand_disable(struct nand_chip *chip)\n{\n\tstruct ebu_nand_controller *ebu_host = nand_get_controller_data(chip);\n\n\twritel(0, ebu_host->ebu + EBU_CON);\n}\n\nstatic void ebu_select_chip(struct nand_chip *chip)\n{\n\tstruct ebu_nand_controller *ebu_host = nand_get_controller_data(chip);\n\tvoid __iomem *nand_con = ebu_host->ebu + EBU_CON;\n\tu32 cs = ebu_host->cs_num;\n\n\twritel(EBU_CON_NANDM_EN | EBU_CON_CSMUX_E_EN | EBU_CON_CS_P_LOW |\n\t       EBU_CON_SE_P_LOW | EBU_CON_WP_P_LOW | EBU_CON_PRE_P_LOW |\n\t       EBU_CON_IN_CS_S(cs) | EBU_CON_OUT_CS_S(cs) |\n\t       EBU_CON_LAT_EN_CS_P, nand_con);\n}\n\nstatic int ebu_nand_set_timings(struct nand_chip *chip, int csline,\n\t\t\t\tconst struct nand_interface_config *conf)\n{\n\tstruct ebu_nand_controller *ctrl = nand_to_ebu(chip);\n\tunsigned int rate = clk_get_rate(ctrl->clk) / HZ_PER_MHZ;\n\tunsigned int period = DIV_ROUND_UP(USEC_PER_SEC, rate);\n\tconst struct nand_sdr_timings *timings;\n\tu32 trecov, thold, twrwait, trdwait;\n\tu32 reg = 0;\n\n\ttimings = nand_get_sdr_timings(conf);\n\tif (IS_ERR(timings))\n\t\treturn PTR_ERR(timings);\n\n\tif (csline == NAND_DATA_IFACE_CHECK_ONLY)\n\t\treturn 0;\n\n\ttrecov = DIV_ROUND_UP(max(timings->tREA_max, timings->tREH_min),\n\t\t\t      period);\n\treg |= EBU_BUSCON_RECOVC(trecov);\n\n\tthold = DIV_ROUND_UP(max(timings->tDH_min, timings->tDS_min), period);\n\treg |= EBU_BUSCON_HOLDC(thold);\n\n\ttrdwait = DIV_ROUND_UP(max(timings->tRC_min, timings->tREH_min),\n\t\t\t       period);\n\treg |= EBU_BUSCON_WAITRDC(trdwait);\n\n\ttwrwait = DIV_ROUND_UP(max(timings->tWC_min, timings->tWH_min), period);\n\treg |= EBU_BUSCON_WAITWRC(twrwait);\n\n\treg |= EBU_BUSCON_CMULT_V4 | EBU_BUSCON_BCGEN_CS | EBU_BUSCON_ALEC |\n\t\tEBU_BUSCON_SETUP_EN;\n\n\twritel(reg, ctrl->ebu + EBU_BUSCON(ctrl->cs_num));\n\n\treturn 0;\n}\n\nstatic int ebu_nand_ooblayout_ecc(struct mtd_info *mtd, int section,\n\t\t\t\t  struct mtd_oob_region *oobregion)\n{\n\tstruct nand_chip *chip = mtd_to_nand(mtd);\n\n\tif (section)\n\t\treturn -ERANGE;\n\n\toobregion->offset = HSNAND_ECC_OFFSET;\n\toobregion->length = chip->ecc.total;\n\n\treturn 0;\n}\n\nstatic int ebu_nand_ooblayout_free(struct mtd_info *mtd, int section,\n\t\t\t\t   struct mtd_oob_region *oobregion)\n{\n\tstruct nand_chip *chip = mtd_to_nand(mtd);\n\n\tif (section)\n\t\treturn -ERANGE;\n\n\toobregion->offset = chip->ecc.total + HSNAND_ECC_OFFSET;\n\toobregion->length = mtd->oobsize - oobregion->offset;\n\n\treturn 0;\n}\n\nstatic const struct mtd_ooblayout_ops ebu_nand_ooblayout_ops = {\n\t.ecc = ebu_nand_ooblayout_ecc,\n\t.free = ebu_nand_ooblayout_free,\n};\n\nstatic void ebu_dma_rx_callback(void *cookie)\n{\n\tstruct ebu_nand_controller *ebu_host = cookie;\n\n\tdmaengine_terminate_async(ebu_host->dma_rx);\n\n\tcomplete(&ebu_host->dma_access_complete);\n}\n\nstatic void ebu_dma_tx_callback(void *cookie)\n{\n\tstruct ebu_nand_controller *ebu_host = cookie;\n\n\tdmaengine_terminate_async(ebu_host->dma_tx);\n\n\tcomplete(&ebu_host->dma_access_complete);\n}\n\nstatic int ebu_dma_start(struct ebu_nand_controller *ebu_host, u32 dir,\n\t\t\t const u8 *buf, u32 len)\n{\n\tstruct dma_async_tx_descriptor *tx;\n\tstruct completion *dma_completion;\n\tdma_async_tx_callback callback;\n\tstruct dma_chan *chan;\n\tdma_cookie_t cookie;\n\tunsigned long flags = DMA_CTRL_ACK | DMA_PREP_INTERRUPT;\n\tdma_addr_t buf_dma;\n\tint ret;\n\tu32 timeout;\n\n\tif (dir == DMA_DEV_TO_MEM) {\n\t\tchan = ebu_host->dma_rx;\n\t\tdma_completion = &ebu_host->dma_access_complete;\n\t\tcallback = ebu_dma_rx_callback;\n\t} else {\n\t\tchan = ebu_host->dma_tx;\n\t\tdma_completion = &ebu_host->dma_access_complete;\n\t\tcallback = ebu_dma_tx_callback;\n\t}\n\n\tbuf_dma = dma_map_single(chan->device->dev, (void *)buf, len, dir);\n\tif (dma_mapping_error(chan->device->dev, buf_dma)) {\n\t\tdev_err(ebu_host->dev, \"Failed to map DMA buffer\\n\");\n\t\tret = -EIO;\n\t\tgoto err_unmap;\n\t}\n\n\ttx = dmaengine_prep_slave_single(chan, buf_dma, len, dir, flags);\n\tif (!tx) {\n\t\tret = -ENXIO;\n\t\tgoto err_unmap;\n\t}\n\n\ttx->callback = callback;\n\ttx->callback_param = ebu_host;\n\tcookie = tx->tx_submit(tx);\n\n\tret = dma_submit_error(cookie);\n\tif (ret) {\n\t\tdev_err(ebu_host->dev, \"dma_submit_error %d\\n\", cookie);\n\t\tret = -EIO;\n\t\tgoto err_unmap;\n\t}\n\n\tinit_completion(dma_completion);\n\tdma_async_issue_pending(chan);\n\n\t \n\ttimeout = wait_for_completion_timeout(dma_completion, msecs_to_jiffies(1000));\n\tif (!timeout) {\n\t\tdev_err(ebu_host->dev, \"I/O Error in DMA RX (status %d)\\n\",\n\t\t\tdmaengine_tx_status(chan, cookie, NULL));\n\t\tdmaengine_terminate_sync(chan);\n\t\tret = -ETIMEDOUT;\n\t\tgoto err_unmap;\n\t}\n\n\treturn 0;\n\nerr_unmap:\n\tdma_unmap_single(ebu_host->dev, buf_dma, len, dir);\n\n\treturn ret;\n}\n\nstatic void ebu_nand_trigger(struct ebu_nand_controller *ebu_host,\n\t\t\t     int page, u32 cmd)\n{\n\tunsigned int val;\n\n\tval = cmd | (page & 0xFF) << HSNAND_CTL1_ADDR_SHIFT;\n\twritel(val, ebu_host->hsnand + HSNAND_CTL1);\n\tval = (page & 0xFFFF00) >> 8 | HSNAND_CTL2_CYC_N_V5;\n\twritel(val, ebu_host->hsnand + HSNAND_CTL2);\n\n\twritel(ebu_host->nd_para0, ebu_host->hsnand + HSNAND_PARA0);\n\n\t \n\twritel(0xFFFFFFFF, ebu_host->hsnand + HSNAND_CMSG_0);\n\twritel(0xFFFFFFFF, ebu_host->hsnand + HSNAND_CMSG_1);\n\n\twritel(HSNAND_INT_MSK_CTL_WR_C,\n\t       ebu_host->hsnand + HSNAND_INT_MSK_CTL);\n\n\tif (!cmd)\n\t\tval = HSNAND_CTL_RW_READ;\n\telse\n\t\tval = HSNAND_CTL_RW_WRITE;\n\n\twritel(HSNAND_CTL_MSG_EN | HSNAND_CTL_CKFF_EN |\n\t       HSNAND_CTL_ECC_OFF_V8TH | HSNAND_CTL_CE_SEL_CS(ebu_host->cs_num) |\n\t       HSNAND_CTL_ENABLE_ECC | HSNAND_CTL_GO | val,\n\t       ebu_host->hsnand + HSNAND_CTL);\n}\n\nstatic int ebu_nand_read_page_hwecc(struct nand_chip *chip, u8 *buf,\n\t\t\t\t    int oob_required, int page)\n{\n\tstruct mtd_info *mtd = nand_to_mtd(chip);\n\tstruct ebu_nand_controller *ebu_host = nand_get_controller_data(chip);\n\tint ret, reg_data;\n\n\tebu_nand_trigger(ebu_host, page, NAND_CMD_READ0);\n\n\tret = ebu_dma_start(ebu_host, DMA_DEV_TO_MEM, buf, mtd->writesize);\n\tif (ret)\n\t\treturn ret;\n\n\tif (oob_required)\n\t\tchip->ecc.read_oob(chip, page);\n\n\treg_data = readl(ebu_host->hsnand + HSNAND_CTL);\n\treg_data &= ~HSNAND_CTL_GO;\n\twritel(reg_data, ebu_host->hsnand + HSNAND_CTL);\n\n\treturn 0;\n}\n\nstatic int ebu_nand_write_page_hwecc(struct nand_chip *chip, const u8 *buf,\n\t\t\t\t     int oob_required, int page)\n{\n\tstruct mtd_info *mtd = nand_to_mtd(chip);\n\tstruct ebu_nand_controller *ebu_host = nand_get_controller_data(chip);\n\tvoid __iomem *int_sta = ebu_host->hsnand + HSNAND_INT_STA;\n\tint reg_data, ret, val;\n\tu32 reg;\n\n\tebu_nand_trigger(ebu_host, page, NAND_CMD_SEQIN);\n\n\tret = ebu_dma_start(ebu_host, DMA_MEM_TO_DEV, buf, mtd->writesize);\n\tif (ret)\n\t\treturn ret;\n\n\tif (oob_required) {\n\t\treg = get_unaligned_le32(chip->oob_poi);\n\t\twritel(reg, ebu_host->hsnand + HSNAND_CMSG_0);\n\n\t\treg = get_unaligned_le32(chip->oob_poi + 4);\n\t\twritel(reg, ebu_host->hsnand + HSNAND_CMSG_1);\n\t}\n\n\tret = readl_poll_timeout_atomic(int_sta, val, !(val & HSNAND_INT_STA_WR_C),\n\t\t\t\t\t10, 1000);\n\tif (ret)\n\t\treturn ret;\n\n\treg_data = readl(ebu_host->hsnand + HSNAND_CTL);\n\treg_data &= ~HSNAND_CTL_GO;\n\twritel(reg_data, ebu_host->hsnand + HSNAND_CTL);\n\n\treturn 0;\n}\n\nstatic const u8 ecc_strength[] = { 1, 1, 4, 8, 24, 32, 40, 60, };\n\nstatic int ebu_nand_attach_chip(struct nand_chip *chip)\n{\n\tstruct mtd_info *mtd = nand_to_mtd(chip);\n\tstruct ebu_nand_controller *ebu_host = nand_get_controller_data(chip);\n\tu32 ecc_steps, ecc_bytes, ecc_total, pagesize, pg_per_blk;\n\tu32 ecc_strength_ds = chip->ecc.strength;\n\tu32 ecc_size = chip->ecc.size;\n\tu32 writesize = mtd->writesize;\n\tu32 blocksize = mtd->erasesize;\n\tint bch_algo, start, val;\n\n\t \n\tif (!chip->ecc.size)\n\t\tchip->ecc.size = 512;\n\n\tswitch (ecc_size) {\n\tcase 512:\n\t\tstart = 1;\n\t\tif (!ecc_strength_ds)\n\t\t\tecc_strength_ds = 4;\n\t\tbreak;\n\tcase 1024:\n\t\tstart = 4;\n\t\tif (!ecc_strength_ds)\n\t\t\tecc_strength_ds = 32;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tbch_algo = round_up(start + 1, 4);\n\tfor (val = start; val < bch_algo; val++) {\n\t\tif (ecc_strength_ds == ecc_strength[val])\n\t\t\tbreak;\n\t}\n\tif (val == bch_algo)\n\t\treturn -EINVAL;\n\n\tif (ecc_strength_ds == 8)\n\t\tecc_bytes = 14;\n\telse\n\t\tecc_bytes = DIV_ROUND_UP(ecc_strength_ds * fls(8 * ecc_size), 8);\n\n\tecc_steps = writesize / ecc_size;\n\tecc_total = ecc_steps * ecc_bytes;\n\tif ((ecc_total + 8) > mtd->oobsize)\n\t\treturn -ERANGE;\n\n\tchip->ecc.total = ecc_total;\n\tpagesize = fls(writesize >> 11);\n\tif (pagesize > HSNAND_PARA0_PAGE_V8192)\n\t\treturn -ERANGE;\n\n\tpg_per_blk = fls((blocksize / writesize) >> 6) / 8;\n\tif (pg_per_blk > HSNAND_PARA0_PIB_V256)\n\t\treturn -ERANGE;\n\n\tebu_host->nd_para0 = pagesize | pg_per_blk | HSNAND_PARA0_BYP_EN_NP |\n\t\t\t     HSNAND_PARA0_BYP_DEC_NP | HSNAND_PARA0_ADEP_EN |\n\t\t\t     HSNAND_PARA0_TYPE_ONFI | (val << 29);\n\n\tmtd_set_ooblayout(mtd, &ebu_nand_ooblayout_ops);\n\tchip->ecc.read_page = ebu_nand_read_page_hwecc;\n\tchip->ecc.write_page = ebu_nand_write_page_hwecc;\n\n\treturn 0;\n}\n\nstatic int ebu_nand_exec_op(struct nand_chip *chip,\n\t\t\t    const struct nand_operation *op, bool check_only)\n{\n\tconst struct nand_op_instr *instr = NULL;\n\tunsigned int op_id;\n\tint i, timeout_ms, ret = 0;\n\n\tif (check_only)\n\t\treturn 0;\n\n\tebu_select_chip(chip);\n\tfor (op_id = 0; op_id < op->ninstrs; op_id++) {\n\t\tinstr = &op->instrs[op_id];\n\n\t\tswitch (instr->type) {\n\t\tcase NAND_OP_CMD_INSTR:\n\t\t\tebu_nand_writeb(chip, HSNAND_CLE_OFFS | HSNAND_CS_OFFS,\n\t\t\t\t\tinstr->ctx.cmd.opcode);\n\t\t\tbreak;\n\n\t\tcase NAND_OP_ADDR_INSTR:\n\t\t\tfor (i = 0; i < instr->ctx.addr.naddrs; i++)\n\t\t\t\tebu_nand_writeb(chip,\n\t\t\t\t\t\tHSNAND_ALE_OFFS | HSNAND_CS_OFFS,\n\t\t\t\t\t\tinstr->ctx.addr.addrs[i]);\n\t\t\tbreak;\n\n\t\tcase NAND_OP_DATA_IN_INSTR:\n\t\t\tebu_read_buf(chip, instr->ctx.data.buf.in,\n\t\t\t\t     instr->ctx.data.len);\n\t\t\tbreak;\n\n\t\tcase NAND_OP_DATA_OUT_INSTR:\n\t\t\tebu_write_buf(chip, instr->ctx.data.buf.out,\n\t\t\t\t      instr->ctx.data.len);\n\t\t\tbreak;\n\n\t\tcase NAND_OP_WAITRDY_INSTR:\n\t\t\ttimeout_ms = instr->ctx.waitrdy.timeout_ms * 1000;\n\t\t\tret = ebu_nand_waitrdy(chip, timeout_ms);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic const struct nand_controller_ops ebu_nand_controller_ops = {\n\t.attach_chip = ebu_nand_attach_chip,\n\t.setup_interface = ebu_nand_set_timings,\n\t.exec_op = ebu_nand_exec_op,\n};\n\nstatic void ebu_dma_cleanup(struct ebu_nand_controller *ebu_host)\n{\n\tif (ebu_host->dma_rx)\n\t\tdma_release_channel(ebu_host->dma_rx);\n\n\tif (ebu_host->dma_tx)\n\t\tdma_release_channel(ebu_host->dma_tx);\n}\n\nstatic int ebu_nand_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct ebu_nand_controller *ebu_host;\n\tstruct device_node *chip_np;\n\tstruct nand_chip *nand;\n\tstruct mtd_info *mtd;\n\tstruct resource *res;\n\tchar *resname;\n\tint ret;\n\tu32 cs;\n\n\tebu_host = devm_kzalloc(dev, sizeof(*ebu_host), GFP_KERNEL);\n\tif (!ebu_host)\n\t\treturn -ENOMEM;\n\n\tebu_host->dev = dev;\n\tnand_controller_init(&ebu_host->controller);\n\n\tebu_host->ebu = devm_platform_ioremap_resource_byname(pdev, \"ebunand\");\n\tif (IS_ERR(ebu_host->ebu))\n\t\treturn PTR_ERR(ebu_host->ebu);\n\n\tebu_host->hsnand = devm_platform_ioremap_resource_byname(pdev, \"hsnand\");\n\tif (IS_ERR(ebu_host->hsnand))\n\t\treturn PTR_ERR(ebu_host->hsnand);\n\n\tchip_np = of_get_next_child(dev->of_node, NULL);\n\tif (!chip_np)\n\t\treturn dev_err_probe(dev, -EINVAL,\n\t\t\t\t     \"Could not find child node for the NAND chip\\n\");\n\n\tret = of_property_read_u32(chip_np, \"reg\", &cs);\n\tif (ret) {\n\t\tdev_err(dev, \"failed to get chip select: %d\\n\", ret);\n\t\tgoto err_of_node_put;\n\t}\n\tif (cs >= MAX_CS) {\n\t\tdev_err(dev, \"got invalid chip select: %d\\n\", cs);\n\t\tret = -EINVAL;\n\t\tgoto err_of_node_put;\n\t}\n\n\tebu_host->cs_num = cs;\n\n\tresname = devm_kasprintf(dev, GFP_KERNEL, \"nand_cs%d\", cs);\n\tif (!resname) {\n\t\tret = -ENOMEM;\n\t\tgoto err_of_node_put;\n\t}\n\n\tebu_host->cs[cs].chipaddr = devm_platform_ioremap_resource_byname(pdev,\n\t\t\t\t\t\t\t\t\t  resname);\n\tif (IS_ERR(ebu_host->cs[cs].chipaddr)) {\n\t\tret = PTR_ERR(ebu_host->cs[cs].chipaddr);\n\t\tgoto err_of_node_put;\n\t}\n\n\tebu_host->clk = devm_clk_get_enabled(dev, NULL);\n\tif (IS_ERR(ebu_host->clk)) {\n\t\tret = dev_err_probe(dev, PTR_ERR(ebu_host->clk),\n\t\t\t\t    \"failed to get and enable clock\\n\");\n\t\tgoto err_of_node_put;\n\t}\n\n\tebu_host->dma_tx = dma_request_chan(dev, \"tx\");\n\tif (IS_ERR(ebu_host->dma_tx)) {\n\t\tret = dev_err_probe(dev, PTR_ERR(ebu_host->dma_tx),\n\t\t\t\t    \"failed to request DMA tx chan!.\\n\");\n\t\tgoto err_of_node_put;\n\t}\n\n\tebu_host->dma_rx = dma_request_chan(dev, \"rx\");\n\tif (IS_ERR(ebu_host->dma_rx)) {\n\t\tret = dev_err_probe(dev, PTR_ERR(ebu_host->dma_rx),\n\t\t\t\t    \"failed to request DMA rx chan!.\\n\");\n\t\tebu_host->dma_rx = NULL;\n\t\tgoto err_cleanup_dma;\n\t}\n\n\tresname = devm_kasprintf(dev, GFP_KERNEL, \"addr_sel%d\", cs);\n\tif (!resname) {\n\t\tret = -ENOMEM;\n\t\tgoto err_cleanup_dma;\n\t}\n\n\tres = platform_get_resource_byname(pdev, IORESOURCE_MEM, resname);\n\tif (!res) {\n\t\tret = -EINVAL;\n\t\tgoto err_cleanup_dma;\n\t}\n\tebu_host->cs[cs].addr_sel = res->start;\n\twritel(ebu_host->cs[cs].addr_sel | EBU_ADDR_MASK(5) | EBU_ADDR_SEL_REGEN,\n\t       ebu_host->ebu + EBU_ADDR_SEL(cs));\n\n\tnand_set_flash_node(&ebu_host->chip, chip_np);\n\n\tmtd = nand_to_mtd(&ebu_host->chip);\n\tif (!mtd->name) {\n\t\tdev_err(ebu_host->dev, \"NAND label property is mandatory\\n\");\n\t\tret = -EINVAL;\n\t\tgoto err_cleanup_dma;\n\t}\n\n\tmtd->dev.parent = dev;\n\tebu_host->dev = dev;\n\n\tplatform_set_drvdata(pdev, ebu_host);\n\tnand_set_controller_data(&ebu_host->chip, ebu_host);\n\n\tnand = &ebu_host->chip;\n\tnand->controller = &ebu_host->controller;\n\tnand->controller->ops = &ebu_nand_controller_ops;\n\n\t \n\tret = nand_scan(&ebu_host->chip, 1);\n\tif (ret)\n\t\tgoto err_cleanup_dma;\n\n\tret = mtd_device_register(mtd, NULL, 0);\n\tif (ret)\n\t\tgoto err_clean_nand;\n\n\treturn 0;\n\nerr_clean_nand:\n\tnand_cleanup(&ebu_host->chip);\nerr_cleanup_dma:\n\tebu_dma_cleanup(ebu_host);\nerr_of_node_put:\n\tof_node_put(chip_np);\n\n\treturn ret;\n}\n\nstatic void ebu_nand_remove(struct platform_device *pdev)\n{\n\tstruct ebu_nand_controller *ebu_host = platform_get_drvdata(pdev);\n\tint ret;\n\n\tret = mtd_device_unregister(nand_to_mtd(&ebu_host->chip));\n\tWARN_ON(ret);\n\tnand_cleanup(&ebu_host->chip);\n\tebu_nand_disable(&ebu_host->chip);\n\tebu_dma_cleanup(ebu_host);\n}\n\nstatic const struct of_device_id ebu_nand_match[] = {\n\t{ .compatible = \"intel,lgm-ebunand\" },\n\t{}\n};\nMODULE_DEVICE_TABLE(of, ebu_nand_match);\n\nstatic struct platform_driver ebu_nand_driver = {\n\t.probe = ebu_nand_probe,\n\t.remove_new = ebu_nand_remove,\n\t.driver = {\n\t\t.name = \"intel-nand-controller\",\n\t\t.of_match_table = ebu_nand_match,\n\t},\n\n};\nmodule_platform_driver(ebu_nand_driver);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Vadivel Murugan R <vadivel.muruganx.ramuthevar@intel.com>\");\nMODULE_DESCRIPTION(\"Intel's LGM External Bus NAND Controller driver\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}