{
  "module_name": "core.c",
  "hash_id": "92deb04d40ba2966aeb4585fcacd34ae5e166abd60942e69dcf32414055bab6c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/mtd/nand/spi/core.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)\t\"spi-nand: \" fmt\n\n#include <linux/device.h>\n#include <linux/jiffies.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/mtd/spinand.h>\n#include <linux/of.h>\n#include <linux/slab.h>\n#include <linux/string.h>\n#include <linux/spi/spi.h>\n#include <linux/spi/spi-mem.h>\n\nstatic int spinand_read_reg_op(struct spinand_device *spinand, u8 reg, u8 *val)\n{\n\tstruct spi_mem_op op = SPINAND_GET_FEATURE_OP(reg,\n\t\t\t\t\t\t      spinand->scratchbuf);\n\tint ret;\n\n\tret = spi_mem_exec_op(spinand->spimem, &op);\n\tif (ret)\n\t\treturn ret;\n\n\t*val = *spinand->scratchbuf;\n\treturn 0;\n}\n\nstatic int spinand_write_reg_op(struct spinand_device *spinand, u8 reg, u8 val)\n{\n\tstruct spi_mem_op op = SPINAND_SET_FEATURE_OP(reg,\n\t\t\t\t\t\t      spinand->scratchbuf);\n\n\t*spinand->scratchbuf = val;\n\treturn spi_mem_exec_op(spinand->spimem, &op);\n}\n\nstatic int spinand_read_status(struct spinand_device *spinand, u8 *status)\n{\n\treturn spinand_read_reg_op(spinand, REG_STATUS, status);\n}\n\nstatic int spinand_get_cfg(struct spinand_device *spinand, u8 *cfg)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\n\tif (WARN_ON(spinand->cur_target < 0 ||\n\t\t    spinand->cur_target >= nand->memorg.ntargets))\n\t\treturn -EINVAL;\n\n\t*cfg = spinand->cfg_cache[spinand->cur_target];\n\treturn 0;\n}\n\nstatic int spinand_set_cfg(struct spinand_device *spinand, u8 cfg)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tint ret;\n\n\tif (WARN_ON(spinand->cur_target < 0 ||\n\t\t    spinand->cur_target >= nand->memorg.ntargets))\n\t\treturn -EINVAL;\n\n\tif (spinand->cfg_cache[spinand->cur_target] == cfg)\n\t\treturn 0;\n\n\tret = spinand_write_reg_op(spinand, REG_CFG, cfg);\n\tif (ret)\n\t\treturn ret;\n\n\tspinand->cfg_cache[spinand->cur_target] = cfg;\n\treturn 0;\n}\n\n \nint spinand_upd_cfg(struct spinand_device *spinand, u8 mask, u8 val)\n{\n\tint ret;\n\tu8 cfg;\n\n\tret = spinand_get_cfg(spinand, &cfg);\n\tif (ret)\n\t\treturn ret;\n\n\tcfg &= ~mask;\n\tcfg |= val;\n\n\treturn spinand_set_cfg(spinand, cfg);\n}\n\n \nint spinand_select_target(struct spinand_device *spinand, unsigned int target)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tint ret;\n\n\tif (WARN_ON(target >= nand->memorg.ntargets))\n\t\treturn -EINVAL;\n\n\tif (spinand->cur_target == target)\n\t\treturn 0;\n\n\tif (nand->memorg.ntargets == 1) {\n\t\tspinand->cur_target = target;\n\t\treturn 0;\n\t}\n\n\tret = spinand->select_target(spinand, target);\n\tif (ret)\n\t\treturn ret;\n\n\tspinand->cur_target = target;\n\treturn 0;\n}\n\nstatic int spinand_read_cfg(struct spinand_device *spinand)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tunsigned int target;\n\tint ret;\n\n\tfor (target = 0; target < nand->memorg.ntargets; target++) {\n\t\tret = spinand_select_target(spinand, target);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\t \n\t\tret = spinand_read_reg_op(spinand, REG_CFG,\n\t\t\t\t\t  &spinand->cfg_cache[target]);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int spinand_init_cfg_cache(struct spinand_device *spinand)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tstruct device *dev = &spinand->spimem->spi->dev;\n\n\tspinand->cfg_cache = devm_kcalloc(dev,\n\t\t\t\t\t  nand->memorg.ntargets,\n\t\t\t\t\t  sizeof(*spinand->cfg_cache),\n\t\t\t\t\t  GFP_KERNEL);\n\tif (!spinand->cfg_cache)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int spinand_init_quad_enable(struct spinand_device *spinand)\n{\n\tbool enable = false;\n\n\tif (!(spinand->flags & SPINAND_HAS_QE_BIT))\n\t\treturn 0;\n\n\tif (spinand->op_templates.read_cache->data.buswidth == 4 ||\n\t    spinand->op_templates.write_cache->data.buswidth == 4 ||\n\t    spinand->op_templates.update_cache->data.buswidth == 4)\n\t\tenable = true;\n\n\treturn spinand_upd_cfg(spinand, CFG_QUAD_ENABLE,\n\t\t\t       enable ? CFG_QUAD_ENABLE : 0);\n}\n\nstatic int spinand_ecc_enable(struct spinand_device *spinand,\n\t\t\t      bool enable)\n{\n\treturn spinand_upd_cfg(spinand, CFG_ECC_ENABLE,\n\t\t\t       enable ? CFG_ECC_ENABLE : 0);\n}\n\nstatic int spinand_check_ecc_status(struct spinand_device *spinand, u8 status)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\n\tif (spinand->eccinfo.get_status)\n\t\treturn spinand->eccinfo.get_status(spinand, status);\n\n\tswitch (status & STATUS_ECC_MASK) {\n\tcase STATUS_ECC_NO_BITFLIPS:\n\t\treturn 0;\n\n\tcase STATUS_ECC_HAS_BITFLIPS:\n\t\t \n\t\treturn nanddev_get_ecc_conf(nand)->strength;\n\n\tcase STATUS_ECC_UNCOR_ERROR:\n\t\treturn -EBADMSG;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int spinand_noecc_ooblayout_ecc(struct mtd_info *mtd, int section,\n\t\t\t\t       struct mtd_oob_region *region)\n{\n\treturn -ERANGE;\n}\n\nstatic int spinand_noecc_ooblayout_free(struct mtd_info *mtd, int section,\n\t\t\t\t\tstruct mtd_oob_region *region)\n{\n\tif (section)\n\t\treturn -ERANGE;\n\n\t \n\tregion->offset = 2;\n\tregion->length = 62;\n\n\treturn 0;\n}\n\nstatic const struct mtd_ooblayout_ops spinand_noecc_ooblayout = {\n\t.ecc = spinand_noecc_ooblayout_ecc,\n\t.free = spinand_noecc_ooblayout_free,\n};\n\nstatic int spinand_ondie_ecc_init_ctx(struct nand_device *nand)\n{\n\tstruct spinand_device *spinand = nand_to_spinand(nand);\n\tstruct mtd_info *mtd = nanddev_to_mtd(nand);\n\tstruct spinand_ondie_ecc_conf *engine_conf;\n\n\tnand->ecc.ctx.conf.engine_type = NAND_ECC_ENGINE_TYPE_ON_DIE;\n\tnand->ecc.ctx.conf.step_size = nand->ecc.requirements.step_size;\n\tnand->ecc.ctx.conf.strength = nand->ecc.requirements.strength;\n\n\tengine_conf = kzalloc(sizeof(*engine_conf), GFP_KERNEL);\n\tif (!engine_conf)\n\t\treturn -ENOMEM;\n\n\tnand->ecc.ctx.priv = engine_conf;\n\n\tif (spinand->eccinfo.ooblayout)\n\t\tmtd_set_ooblayout(mtd, spinand->eccinfo.ooblayout);\n\telse\n\t\tmtd_set_ooblayout(mtd, &spinand_noecc_ooblayout);\n\n\treturn 0;\n}\n\nstatic void spinand_ondie_ecc_cleanup_ctx(struct nand_device *nand)\n{\n\tkfree(nand->ecc.ctx.priv);\n}\n\nstatic int spinand_ondie_ecc_prepare_io_req(struct nand_device *nand,\n\t\t\t\t\t    struct nand_page_io_req *req)\n{\n\tstruct spinand_device *spinand = nand_to_spinand(nand);\n\tbool enable = (req->mode != MTD_OPS_RAW);\n\n\tmemset(spinand->oobbuf, 0xff, nanddev_per_page_oobsize(nand));\n\n\t \n\treturn spinand_ecc_enable(spinand, enable);\n}\n\nstatic int spinand_ondie_ecc_finish_io_req(struct nand_device *nand,\n\t\t\t\t\t   struct nand_page_io_req *req)\n{\n\tstruct spinand_ondie_ecc_conf *engine_conf = nand->ecc.ctx.priv;\n\tstruct spinand_device *spinand = nand_to_spinand(nand);\n\tstruct mtd_info *mtd = spinand_to_mtd(spinand);\n\tint ret;\n\n\tif (req->mode == MTD_OPS_RAW)\n\t\treturn 0;\n\n\t \n\tif (req->type == NAND_PAGE_WRITE)\n\t\treturn 0;\n\n\t \n\tret = spinand_check_ecc_status(spinand, engine_conf->status);\n\tif (ret == -EBADMSG)\n\t\tmtd->ecc_stats.failed++;\n\telse if (ret > 0)\n\t\tmtd->ecc_stats.corrected += ret;\n\n\treturn ret;\n}\n\nstatic struct nand_ecc_engine_ops spinand_ondie_ecc_engine_ops = {\n\t.init_ctx = spinand_ondie_ecc_init_ctx,\n\t.cleanup_ctx = spinand_ondie_ecc_cleanup_ctx,\n\t.prepare_io_req = spinand_ondie_ecc_prepare_io_req,\n\t.finish_io_req = spinand_ondie_ecc_finish_io_req,\n};\n\nstatic struct nand_ecc_engine spinand_ondie_ecc_engine = {\n\t.ops = &spinand_ondie_ecc_engine_ops,\n};\n\nstatic void spinand_ondie_ecc_save_status(struct nand_device *nand, u8 status)\n{\n\tstruct spinand_ondie_ecc_conf *engine_conf = nand->ecc.ctx.priv;\n\n\tif (nand->ecc.ctx.conf.engine_type == NAND_ECC_ENGINE_TYPE_ON_DIE &&\n\t    engine_conf)\n\t\tengine_conf->status = status;\n}\n\nstatic int spinand_write_enable_op(struct spinand_device *spinand)\n{\n\tstruct spi_mem_op op = SPINAND_WR_EN_DIS_OP(true);\n\n\treturn spi_mem_exec_op(spinand->spimem, &op);\n}\n\nstatic int spinand_load_page_op(struct spinand_device *spinand,\n\t\t\t\tconst struct nand_page_io_req *req)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tunsigned int row = nanddev_pos_to_row(nand, &req->pos);\n\tstruct spi_mem_op op = SPINAND_PAGE_READ_OP(row);\n\n\treturn spi_mem_exec_op(spinand->spimem, &op);\n}\n\nstatic int spinand_read_from_cache_op(struct spinand_device *spinand,\n\t\t\t\t      const struct nand_page_io_req *req)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tstruct mtd_info *mtd = spinand_to_mtd(spinand);\n\tstruct spi_mem_dirmap_desc *rdesc;\n\tunsigned int nbytes = 0;\n\tvoid *buf = NULL;\n\tu16 column = 0;\n\tssize_t ret;\n\n\tif (req->datalen) {\n\t\tbuf = spinand->databuf;\n\t\tnbytes = nanddev_page_size(nand);\n\t\tcolumn = 0;\n\t}\n\n\tif (req->ooblen) {\n\t\tnbytes += nanddev_per_page_oobsize(nand);\n\t\tif (!buf) {\n\t\t\tbuf = spinand->oobbuf;\n\t\t\tcolumn = nanddev_page_size(nand);\n\t\t}\n\t}\n\n\tif (req->mode == MTD_OPS_RAW)\n\t\trdesc = spinand->dirmaps[req->pos.plane].rdesc;\n\telse\n\t\trdesc = spinand->dirmaps[req->pos.plane].rdesc_ecc;\n\n\twhile (nbytes) {\n\t\tret = spi_mem_dirmap_read(rdesc, column, nbytes, buf);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tif (!ret || ret > nbytes)\n\t\t\treturn -EIO;\n\n\t\tnbytes -= ret;\n\t\tcolumn += ret;\n\t\tbuf += ret;\n\t}\n\n\tif (req->datalen)\n\t\tmemcpy(req->databuf.in, spinand->databuf + req->dataoffs,\n\t\t       req->datalen);\n\n\tif (req->ooblen) {\n\t\tif (req->mode == MTD_OPS_AUTO_OOB)\n\t\t\tmtd_ooblayout_get_databytes(mtd, req->oobbuf.in,\n\t\t\t\t\t\t    spinand->oobbuf,\n\t\t\t\t\t\t    req->ooboffs,\n\t\t\t\t\t\t    req->ooblen);\n\t\telse\n\t\t\tmemcpy(req->oobbuf.in, spinand->oobbuf + req->ooboffs,\n\t\t\t       req->ooblen);\n\t}\n\n\treturn 0;\n}\n\nstatic int spinand_write_to_cache_op(struct spinand_device *spinand,\n\t\t\t\t     const struct nand_page_io_req *req)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tstruct mtd_info *mtd = spinand_to_mtd(spinand);\n\tstruct spi_mem_dirmap_desc *wdesc;\n\tunsigned int nbytes, column = 0;\n\tvoid *buf = spinand->databuf;\n\tssize_t ret;\n\n\t \n\tnbytes = nanddev_page_size(nand) + nanddev_per_page_oobsize(nand);\n\tmemset(spinand->databuf, 0xff, nanddev_page_size(nand));\n\n\tif (req->datalen)\n\t\tmemcpy(spinand->databuf + req->dataoffs, req->databuf.out,\n\t\t       req->datalen);\n\n\tif (req->ooblen) {\n\t\tif (req->mode == MTD_OPS_AUTO_OOB)\n\t\t\tmtd_ooblayout_set_databytes(mtd, req->oobbuf.out,\n\t\t\t\t\t\t    spinand->oobbuf,\n\t\t\t\t\t\t    req->ooboffs,\n\t\t\t\t\t\t    req->ooblen);\n\t\telse\n\t\t\tmemcpy(spinand->oobbuf + req->ooboffs, req->oobbuf.out,\n\t\t\t       req->ooblen);\n\t}\n\n\tif (req->mode == MTD_OPS_RAW)\n\t\twdesc = spinand->dirmaps[req->pos.plane].wdesc;\n\telse\n\t\twdesc = spinand->dirmaps[req->pos.plane].wdesc_ecc;\n\n\twhile (nbytes) {\n\t\tret = spi_mem_dirmap_write(wdesc, column, nbytes, buf);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tif (!ret || ret > nbytes)\n\t\t\treturn -EIO;\n\n\t\tnbytes -= ret;\n\t\tcolumn += ret;\n\t\tbuf += ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int spinand_program_op(struct spinand_device *spinand,\n\t\t\t      const struct nand_page_io_req *req)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tunsigned int row = nanddev_pos_to_row(nand, &req->pos);\n\tstruct spi_mem_op op = SPINAND_PROG_EXEC_OP(row);\n\n\treturn spi_mem_exec_op(spinand->spimem, &op);\n}\n\nstatic int spinand_erase_op(struct spinand_device *spinand,\n\t\t\t    const struct nand_pos *pos)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tunsigned int row = nanddev_pos_to_row(nand, pos);\n\tstruct spi_mem_op op = SPINAND_BLK_ERASE_OP(row);\n\n\treturn spi_mem_exec_op(spinand->spimem, &op);\n}\n\nstatic int spinand_wait(struct spinand_device *spinand,\n\t\t\tunsigned long initial_delay_us,\n\t\t\tunsigned long poll_delay_us,\n\t\t\tu8 *s)\n{\n\tstruct spi_mem_op op = SPINAND_GET_FEATURE_OP(REG_STATUS,\n\t\t\t\t\t\t      spinand->scratchbuf);\n\tu8 status;\n\tint ret;\n\n\tret = spi_mem_poll_status(spinand->spimem, &op, STATUS_BUSY, 0,\n\t\t\t\t  initial_delay_us,\n\t\t\t\t  poll_delay_us,\n\t\t\t\t  SPINAND_WAITRDY_TIMEOUT_MS);\n\tif (ret)\n\t\treturn ret;\n\n\tstatus = *spinand->scratchbuf;\n\tif (!(status & STATUS_BUSY))\n\t\tgoto out;\n\n\t \n\tret = spinand_read_status(spinand, &status);\n\tif (ret)\n\t\treturn ret;\n\nout:\n\tif (s)\n\t\t*s = status;\n\n\treturn status & STATUS_BUSY ? -ETIMEDOUT : 0;\n}\n\nstatic int spinand_read_id_op(struct spinand_device *spinand, u8 naddr,\n\t\t\t      u8 ndummy, u8 *buf)\n{\n\tstruct spi_mem_op op = SPINAND_READID_OP(\n\t\tnaddr, ndummy, spinand->scratchbuf, SPINAND_MAX_ID_LEN);\n\tint ret;\n\n\tret = spi_mem_exec_op(spinand->spimem, &op);\n\tif (!ret)\n\t\tmemcpy(buf, spinand->scratchbuf, SPINAND_MAX_ID_LEN);\n\n\treturn ret;\n}\n\nstatic int spinand_reset_op(struct spinand_device *spinand)\n{\n\tstruct spi_mem_op op = SPINAND_RESET_OP;\n\tint ret;\n\n\tret = spi_mem_exec_op(spinand->spimem, &op);\n\tif (ret)\n\t\treturn ret;\n\n\treturn spinand_wait(spinand,\n\t\t\t    SPINAND_RESET_INITIAL_DELAY_US,\n\t\t\t    SPINAND_RESET_POLL_DELAY_US,\n\t\t\t    NULL);\n}\n\nstatic int spinand_lock_block(struct spinand_device *spinand, u8 lock)\n{\n\treturn spinand_write_reg_op(spinand, REG_BLOCK_LOCK, lock);\n}\n\nstatic int spinand_read_page(struct spinand_device *spinand,\n\t\t\t     const struct nand_page_io_req *req)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tu8 status;\n\tint ret;\n\n\tret = nand_ecc_prepare_io_req(nand, (struct nand_page_io_req *)req);\n\tif (ret)\n\t\treturn ret;\n\n\tret = spinand_load_page_op(spinand, req);\n\tif (ret)\n\t\treturn ret;\n\n\tret = spinand_wait(spinand,\n\t\t\t   SPINAND_READ_INITIAL_DELAY_US,\n\t\t\t   SPINAND_READ_POLL_DELAY_US,\n\t\t\t   &status);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tspinand_ondie_ecc_save_status(nand, status);\n\n\tret = spinand_read_from_cache_op(spinand, req);\n\tif (ret)\n\t\treturn ret;\n\n\treturn nand_ecc_finish_io_req(nand, (struct nand_page_io_req *)req);\n}\n\nstatic int spinand_write_page(struct spinand_device *spinand,\n\t\t\t      const struct nand_page_io_req *req)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tu8 status;\n\tint ret;\n\n\tret = nand_ecc_prepare_io_req(nand, (struct nand_page_io_req *)req);\n\tif (ret)\n\t\treturn ret;\n\n\tret = spinand_write_enable_op(spinand);\n\tif (ret)\n\t\treturn ret;\n\n\tret = spinand_write_to_cache_op(spinand, req);\n\tif (ret)\n\t\treturn ret;\n\n\tret = spinand_program_op(spinand, req);\n\tif (ret)\n\t\treturn ret;\n\n\tret = spinand_wait(spinand,\n\t\t\t   SPINAND_WRITE_INITIAL_DELAY_US,\n\t\t\t   SPINAND_WRITE_POLL_DELAY_US,\n\t\t\t   &status);\n\tif (!ret && (status & STATUS_PROG_FAILED))\n\t\treturn -EIO;\n\n\treturn nand_ecc_finish_io_req(nand, (struct nand_page_io_req *)req);\n}\n\nstatic int spinand_mtd_read(struct mtd_info *mtd, loff_t from,\n\t\t\t    struct mtd_oob_ops *ops)\n{\n\tstruct spinand_device *spinand = mtd_to_spinand(mtd);\n\tstruct nand_device *nand = mtd_to_nanddev(mtd);\n\tstruct mtd_ecc_stats old_stats;\n\tunsigned int max_bitflips = 0;\n\tstruct nand_io_iter iter;\n\tbool disable_ecc = false;\n\tbool ecc_failed = false;\n\tint ret = 0;\n\n\tif (ops->mode == MTD_OPS_RAW || !spinand->eccinfo.ooblayout)\n\t\tdisable_ecc = true;\n\n\tmutex_lock(&spinand->lock);\n\n\told_stats = mtd->ecc_stats;\n\n\tnanddev_io_for_each_page(nand, NAND_PAGE_READ, from, ops, &iter) {\n\t\tif (disable_ecc)\n\t\t\titer.req.mode = MTD_OPS_RAW;\n\n\t\tret = spinand_select_target(spinand, iter.req.pos.target);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tret = spinand_read_page(spinand, &iter.req);\n\t\tif (ret < 0 && ret != -EBADMSG)\n\t\t\tbreak;\n\n\t\tif (ret == -EBADMSG)\n\t\t\tecc_failed = true;\n\t\telse\n\t\t\tmax_bitflips = max_t(unsigned int, max_bitflips, ret);\n\n\t\tret = 0;\n\t\tops->retlen += iter.req.datalen;\n\t\tops->oobretlen += iter.req.ooblen;\n\t}\n\n\tif (ops->stats) {\n\t\tops->stats->uncorrectable_errors +=\n\t\t\tmtd->ecc_stats.failed - old_stats.failed;\n\t\tops->stats->corrected_bitflips +=\n\t\t\tmtd->ecc_stats.corrected - old_stats.corrected;\n\t}\n\n\tmutex_unlock(&spinand->lock);\n\n\tif (ecc_failed && !ret)\n\t\tret = -EBADMSG;\n\n\treturn ret ? ret : max_bitflips;\n}\n\nstatic int spinand_mtd_write(struct mtd_info *mtd, loff_t to,\n\t\t\t     struct mtd_oob_ops *ops)\n{\n\tstruct spinand_device *spinand = mtd_to_spinand(mtd);\n\tstruct nand_device *nand = mtd_to_nanddev(mtd);\n\tstruct nand_io_iter iter;\n\tbool disable_ecc = false;\n\tint ret = 0;\n\n\tif (ops->mode == MTD_OPS_RAW || !mtd->ooblayout)\n\t\tdisable_ecc = true;\n\n\tmutex_lock(&spinand->lock);\n\n\tnanddev_io_for_each_page(nand, NAND_PAGE_WRITE, to, ops, &iter) {\n\t\tif (disable_ecc)\n\t\t\titer.req.mode = MTD_OPS_RAW;\n\n\t\tret = spinand_select_target(spinand, iter.req.pos.target);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tret = spinand_write_page(spinand, &iter.req);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tops->retlen += iter.req.datalen;\n\t\tops->oobretlen += iter.req.ooblen;\n\t}\n\n\tmutex_unlock(&spinand->lock);\n\n\treturn ret;\n}\n\nstatic bool spinand_isbad(struct nand_device *nand, const struct nand_pos *pos)\n{\n\tstruct spinand_device *spinand = nand_to_spinand(nand);\n\tu8 marker[2] = { };\n\tstruct nand_page_io_req req = {\n\t\t.pos = *pos,\n\t\t.ooblen = sizeof(marker),\n\t\t.ooboffs = 0,\n\t\t.oobbuf.in = marker,\n\t\t.mode = MTD_OPS_RAW,\n\t};\n\n\tspinand_select_target(spinand, pos->target);\n\tspinand_read_page(spinand, &req);\n\tif (marker[0] != 0xff || marker[1] != 0xff)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int spinand_mtd_block_isbad(struct mtd_info *mtd, loff_t offs)\n{\n\tstruct nand_device *nand = mtd_to_nanddev(mtd);\n\tstruct spinand_device *spinand = nand_to_spinand(nand);\n\tstruct nand_pos pos;\n\tint ret;\n\n\tnanddev_offs_to_pos(nand, offs, &pos);\n\tmutex_lock(&spinand->lock);\n\tret = nanddev_isbad(nand, &pos);\n\tmutex_unlock(&spinand->lock);\n\n\treturn ret;\n}\n\nstatic int spinand_markbad(struct nand_device *nand, const struct nand_pos *pos)\n{\n\tstruct spinand_device *spinand = nand_to_spinand(nand);\n\tu8 marker[2] = { };\n\tstruct nand_page_io_req req = {\n\t\t.pos = *pos,\n\t\t.ooboffs = 0,\n\t\t.ooblen = sizeof(marker),\n\t\t.oobbuf.out = marker,\n\t\t.mode = MTD_OPS_RAW,\n\t};\n\tint ret;\n\n\tret = spinand_select_target(spinand, pos->target);\n\tif (ret)\n\t\treturn ret;\n\n\tret = spinand_write_enable_op(spinand);\n\tif (ret)\n\t\treturn ret;\n\n\treturn spinand_write_page(spinand, &req);\n}\n\nstatic int spinand_mtd_block_markbad(struct mtd_info *mtd, loff_t offs)\n{\n\tstruct nand_device *nand = mtd_to_nanddev(mtd);\n\tstruct spinand_device *spinand = nand_to_spinand(nand);\n\tstruct nand_pos pos;\n\tint ret;\n\n\tnanddev_offs_to_pos(nand, offs, &pos);\n\tmutex_lock(&spinand->lock);\n\tret = nanddev_markbad(nand, &pos);\n\tmutex_unlock(&spinand->lock);\n\n\treturn ret;\n}\n\nstatic int spinand_erase(struct nand_device *nand, const struct nand_pos *pos)\n{\n\tstruct spinand_device *spinand = nand_to_spinand(nand);\n\tu8 status;\n\tint ret;\n\n\tret = spinand_select_target(spinand, pos->target);\n\tif (ret)\n\t\treturn ret;\n\n\tret = spinand_write_enable_op(spinand);\n\tif (ret)\n\t\treturn ret;\n\n\tret = spinand_erase_op(spinand, pos);\n\tif (ret)\n\t\treturn ret;\n\n\tret = spinand_wait(spinand,\n\t\t\t   SPINAND_ERASE_INITIAL_DELAY_US,\n\t\t\t   SPINAND_ERASE_POLL_DELAY_US,\n\t\t\t   &status);\n\n\tif (!ret && (status & STATUS_ERASE_FAILED))\n\t\tret = -EIO;\n\n\treturn ret;\n}\n\nstatic int spinand_mtd_erase(struct mtd_info *mtd,\n\t\t\t     struct erase_info *einfo)\n{\n\tstruct spinand_device *spinand = mtd_to_spinand(mtd);\n\tint ret;\n\n\tmutex_lock(&spinand->lock);\n\tret = nanddev_mtd_erase(mtd, einfo);\n\tmutex_unlock(&spinand->lock);\n\n\treturn ret;\n}\n\nstatic int spinand_mtd_block_isreserved(struct mtd_info *mtd, loff_t offs)\n{\n\tstruct spinand_device *spinand = mtd_to_spinand(mtd);\n\tstruct nand_device *nand = mtd_to_nanddev(mtd);\n\tstruct nand_pos pos;\n\tint ret;\n\n\tnanddev_offs_to_pos(nand, offs, &pos);\n\tmutex_lock(&spinand->lock);\n\tret = nanddev_isreserved(nand, &pos);\n\tmutex_unlock(&spinand->lock);\n\n\treturn ret;\n}\n\nstatic int spinand_create_dirmap(struct spinand_device *spinand,\n\t\t\t\t unsigned int plane)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tstruct spi_mem_dirmap_info info = {\n\t\t.length = nanddev_page_size(nand) +\n\t\t\t  nanddev_per_page_oobsize(nand),\n\t};\n\tstruct spi_mem_dirmap_desc *desc;\n\n\t \n\tinfo.offset = plane << fls(nand->memorg.pagesize);\n\n\tinfo.op_tmpl = *spinand->op_templates.update_cache;\n\tdesc = devm_spi_mem_dirmap_create(&spinand->spimem->spi->dev,\n\t\t\t\t\t  spinand->spimem, &info);\n\tif (IS_ERR(desc))\n\t\treturn PTR_ERR(desc);\n\n\tspinand->dirmaps[plane].wdesc = desc;\n\n\tinfo.op_tmpl = *spinand->op_templates.read_cache;\n\tdesc = devm_spi_mem_dirmap_create(&spinand->spimem->spi->dev,\n\t\t\t\t\t  spinand->spimem, &info);\n\tif (IS_ERR(desc))\n\t\treturn PTR_ERR(desc);\n\n\tspinand->dirmaps[plane].rdesc = desc;\n\n\tif (nand->ecc.engine->integration != NAND_ECC_ENGINE_INTEGRATION_PIPELINED) {\n\t\tspinand->dirmaps[plane].wdesc_ecc = spinand->dirmaps[plane].wdesc;\n\t\tspinand->dirmaps[plane].rdesc_ecc = spinand->dirmaps[plane].rdesc;\n\n\t\treturn 0;\n\t}\n\n\tinfo.op_tmpl = *spinand->op_templates.update_cache;\n\tinfo.op_tmpl.data.ecc = true;\n\tdesc = devm_spi_mem_dirmap_create(&spinand->spimem->spi->dev,\n\t\t\t\t\t  spinand->spimem, &info);\n\tif (IS_ERR(desc))\n\t\treturn PTR_ERR(desc);\n\n\tspinand->dirmaps[plane].wdesc_ecc = desc;\n\n\tinfo.op_tmpl = *spinand->op_templates.read_cache;\n\tinfo.op_tmpl.data.ecc = true;\n\tdesc = devm_spi_mem_dirmap_create(&spinand->spimem->spi->dev,\n\t\t\t\t\t  spinand->spimem, &info);\n\tif (IS_ERR(desc))\n\t\treturn PTR_ERR(desc);\n\n\tspinand->dirmaps[plane].rdesc_ecc = desc;\n\n\treturn 0;\n}\n\nstatic int spinand_create_dirmaps(struct spinand_device *spinand)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tint i, ret;\n\n\tspinand->dirmaps = devm_kzalloc(&spinand->spimem->spi->dev,\n\t\t\t\t\tsizeof(*spinand->dirmaps) *\n\t\t\t\t\tnand->memorg.planes_per_lun,\n\t\t\t\t\tGFP_KERNEL);\n\tif (!spinand->dirmaps)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nand->memorg.planes_per_lun; i++) {\n\t\tret = spinand_create_dirmap(spinand, i);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic const struct nand_ops spinand_ops = {\n\t.erase = spinand_erase,\n\t.markbad = spinand_markbad,\n\t.isbad = spinand_isbad,\n};\n\nstatic const struct spinand_manufacturer *spinand_manufacturers[] = {\n\t&alliancememory_spinand_manufacturer,\n\t&ato_spinand_manufacturer,\n\t&esmt_c8_spinand_manufacturer,\n\t&gigadevice_spinand_manufacturer,\n\t&macronix_spinand_manufacturer,\n\t&micron_spinand_manufacturer,\n\t&paragon_spinand_manufacturer,\n\t&toshiba_spinand_manufacturer,\n\t&winbond_spinand_manufacturer,\n\t&xtx_spinand_manufacturer,\n};\n\nstatic int spinand_manufacturer_match(struct spinand_device *spinand,\n\t\t\t\t      enum spinand_readid_method rdid_method)\n{\n\tu8 *id = spinand->id.data;\n\tunsigned int i;\n\tint ret;\n\n\tfor (i = 0; i < ARRAY_SIZE(spinand_manufacturers); i++) {\n\t\tconst struct spinand_manufacturer *manufacturer =\n\t\t\tspinand_manufacturers[i];\n\n\t\tif (id[0] != manufacturer->id)\n\t\t\tcontinue;\n\n\t\tret = spinand_match_and_init(spinand,\n\t\t\t\t\t     manufacturer->chips,\n\t\t\t\t\t     manufacturer->nchips,\n\t\t\t\t\t     rdid_method);\n\t\tif (ret < 0)\n\t\t\tcontinue;\n\n\t\tspinand->manufacturer = manufacturer;\n\t\treturn 0;\n\t}\n\treturn -ENOTSUPP;\n}\n\nstatic int spinand_id_detect(struct spinand_device *spinand)\n{\n\tu8 *id = spinand->id.data;\n\tint ret;\n\n\tret = spinand_read_id_op(spinand, 0, 0, id);\n\tif (ret)\n\t\treturn ret;\n\tret = spinand_manufacturer_match(spinand, SPINAND_READID_METHOD_OPCODE);\n\tif (!ret)\n\t\treturn 0;\n\n\tret = spinand_read_id_op(spinand, 1, 0, id);\n\tif (ret)\n\t\treturn ret;\n\tret = spinand_manufacturer_match(spinand,\n\t\t\t\t\t SPINAND_READID_METHOD_OPCODE_ADDR);\n\tif (!ret)\n\t\treturn 0;\n\n\tret = spinand_read_id_op(spinand, 0, 1, id);\n\tif (ret)\n\t\treturn ret;\n\tret = spinand_manufacturer_match(spinand,\n\t\t\t\t\t SPINAND_READID_METHOD_OPCODE_DUMMY);\n\n\treturn ret;\n}\n\nstatic int spinand_manufacturer_init(struct spinand_device *spinand)\n{\n\tif (spinand->manufacturer->ops->init)\n\t\treturn spinand->manufacturer->ops->init(spinand);\n\n\treturn 0;\n}\n\nstatic void spinand_manufacturer_cleanup(struct spinand_device *spinand)\n{\n\t \n\tif (spinand->manufacturer->ops->cleanup)\n\t\treturn spinand->manufacturer->ops->cleanup(spinand);\n}\n\nstatic const struct spi_mem_op *\nspinand_select_op_variant(struct spinand_device *spinand,\n\t\t\t  const struct spinand_op_variants *variants)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tunsigned int i;\n\n\tfor (i = 0; i < variants->nops; i++) {\n\t\tstruct spi_mem_op op = variants->ops[i];\n\t\tunsigned int nbytes;\n\t\tint ret;\n\n\t\tnbytes = nanddev_per_page_oobsize(nand) +\n\t\t\t nanddev_page_size(nand);\n\n\t\twhile (nbytes) {\n\t\t\top.data.nbytes = nbytes;\n\t\t\tret = spi_mem_adjust_op_size(spinand->spimem, &op);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\n\t\t\tif (!spi_mem_supports_op(spinand->spimem, &op))\n\t\t\t\tbreak;\n\n\t\t\tnbytes -= op.data.nbytes;\n\t\t}\n\n\t\tif (!nbytes)\n\t\t\treturn &variants->ops[i];\n\t}\n\n\treturn NULL;\n}\n\n \nint spinand_match_and_init(struct spinand_device *spinand,\n\t\t\t   const struct spinand_info *table,\n\t\t\t   unsigned int table_size,\n\t\t\t   enum spinand_readid_method rdid_method)\n{\n\tu8 *id = spinand->id.data;\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tunsigned int i;\n\n\tfor (i = 0; i < table_size; i++) {\n\t\tconst struct spinand_info *info = &table[i];\n\t\tconst struct spi_mem_op *op;\n\n\t\tif (rdid_method != info->devid.method)\n\t\t\tcontinue;\n\n\t\tif (memcmp(id + 1, info->devid.id, info->devid.len))\n\t\t\tcontinue;\n\n\t\tnand->memorg = table[i].memorg;\n\t\tnanddev_set_ecc_requirements(nand, &table[i].eccreq);\n\t\tspinand->eccinfo = table[i].eccinfo;\n\t\tspinand->flags = table[i].flags;\n\t\tspinand->id.len = 1 + table[i].devid.len;\n\t\tspinand->select_target = table[i].select_target;\n\n\t\top = spinand_select_op_variant(spinand,\n\t\t\t\t\t       info->op_variants.read_cache);\n\t\tif (!op)\n\t\t\treturn -ENOTSUPP;\n\n\t\tspinand->op_templates.read_cache = op;\n\n\t\top = spinand_select_op_variant(spinand,\n\t\t\t\t\t       info->op_variants.write_cache);\n\t\tif (!op)\n\t\t\treturn -ENOTSUPP;\n\n\t\tspinand->op_templates.write_cache = op;\n\n\t\top = spinand_select_op_variant(spinand,\n\t\t\t\t\t       info->op_variants.update_cache);\n\t\tspinand->op_templates.update_cache = op;\n\n\t\treturn 0;\n\t}\n\n\treturn -ENOTSUPP;\n}\n\nstatic int spinand_detect(struct spinand_device *spinand)\n{\n\tstruct device *dev = &spinand->spimem->spi->dev;\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tint ret;\n\n\tret = spinand_reset_op(spinand);\n\tif (ret)\n\t\treturn ret;\n\n\tret = spinand_id_detect(spinand);\n\tif (ret) {\n\t\tdev_err(dev, \"unknown raw ID %*phN\\n\", SPINAND_MAX_ID_LEN,\n\t\t\tspinand->id.data);\n\t\treturn ret;\n\t}\n\n\tif (nand->memorg.ntargets > 1 && !spinand->select_target) {\n\t\tdev_err(dev,\n\t\t\t\"SPI NANDs with more than one die must implement ->select_target()\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tdev_info(&spinand->spimem->spi->dev,\n\t\t \"%s SPI NAND was found.\\n\", spinand->manufacturer->name);\n\tdev_info(&spinand->spimem->spi->dev,\n\t\t \"%llu MiB, block size: %zu KiB, page size: %zu, OOB size: %u\\n\",\n\t\t nanddev_size(nand) >> 20, nanddev_eraseblock_size(nand) >> 10,\n\t\t nanddev_page_size(nand), nanddev_per_page_oobsize(nand));\n\n\treturn 0;\n}\n\nstatic int spinand_init_flash(struct spinand_device *spinand)\n{\n\tstruct device *dev = &spinand->spimem->spi->dev;\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\tint ret, i;\n\n\tret = spinand_read_cfg(spinand);\n\tif (ret)\n\t\treturn ret;\n\n\tret = spinand_init_quad_enable(spinand);\n\tif (ret)\n\t\treturn ret;\n\n\tret = spinand_upd_cfg(spinand, CFG_OTP_ENABLE, 0);\n\tif (ret)\n\t\treturn ret;\n\n\tret = spinand_manufacturer_init(spinand);\n\tif (ret) {\n\t\tdev_err(dev,\n\t\t\"Failed to initialize the SPI NAND chip (err = %d)\\n\",\n\t\tret);\n\t\treturn ret;\n\t}\n\n\t \n\tfor (i = 0; i < nand->memorg.ntargets; i++) {\n\t\tret = spinand_select_target(spinand, i);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tret = spinand_lock_block(spinand, BL_ALL_UNLOCKED);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\tif (ret)\n\t\tspinand_manufacturer_cleanup(spinand);\n\n\treturn ret;\n}\n\nstatic void spinand_mtd_resume(struct mtd_info *mtd)\n{\n\tstruct spinand_device *spinand = mtd_to_spinand(mtd);\n\tint ret;\n\n\tret = spinand_reset_op(spinand);\n\tif (ret)\n\t\treturn;\n\n\tret = spinand_init_flash(spinand);\n\tif (ret)\n\t\treturn;\n\n\tspinand_ecc_enable(spinand, false);\n}\n\nstatic int spinand_init(struct spinand_device *spinand)\n{\n\tstruct device *dev = &spinand->spimem->spi->dev;\n\tstruct mtd_info *mtd = spinand_to_mtd(spinand);\n\tstruct nand_device *nand = mtd_to_nanddev(mtd);\n\tint ret;\n\n\t \n\tspinand->scratchbuf = kzalloc(SPINAND_MAX_ID_LEN, GFP_KERNEL);\n\tif (!spinand->scratchbuf)\n\t\treturn -ENOMEM;\n\n\tret = spinand_detect(spinand);\n\tif (ret)\n\t\tgoto err_free_bufs;\n\n\t \n\tspinand->databuf = kzalloc(nanddev_page_size(nand) +\n\t\t\t       nanddev_per_page_oobsize(nand),\n\t\t\t       GFP_KERNEL);\n\tif (!spinand->databuf) {\n\t\tret = -ENOMEM;\n\t\tgoto err_free_bufs;\n\t}\n\n\tspinand->oobbuf = spinand->databuf + nanddev_page_size(nand);\n\n\tret = spinand_init_cfg_cache(spinand);\n\tif (ret)\n\t\tgoto err_free_bufs;\n\n\tret = spinand_init_flash(spinand);\n\tif (ret)\n\t\tgoto err_free_bufs;\n\n\tret = nanddev_init(nand, &spinand_ops, THIS_MODULE);\n\tif (ret)\n\t\tgoto err_manuf_cleanup;\n\n\t \n\tnand->ecc.defaults.engine_type = NAND_ECC_ENGINE_TYPE_ON_DIE;\n\tnand->ecc.ondie_engine = &spinand_ondie_ecc_engine;\n\n\tspinand_ecc_enable(spinand, false);\n\tret = nanddev_ecc_engine_init(nand);\n\tif (ret)\n\t\tgoto err_cleanup_nanddev;\n\n\tmtd->_read_oob = spinand_mtd_read;\n\tmtd->_write_oob = spinand_mtd_write;\n\tmtd->_block_isbad = spinand_mtd_block_isbad;\n\tmtd->_block_markbad = spinand_mtd_block_markbad;\n\tmtd->_block_isreserved = spinand_mtd_block_isreserved;\n\tmtd->_erase = spinand_mtd_erase;\n\tmtd->_max_bad_blocks = nanddev_mtd_max_bad_blocks;\n\tmtd->_resume = spinand_mtd_resume;\n\n\tif (nand->ecc.engine) {\n\t\tret = mtd_ooblayout_count_freebytes(mtd);\n\t\tif (ret < 0)\n\t\t\tgoto err_cleanup_ecc_engine;\n\t}\n\n\tmtd->oobavail = ret;\n\n\t \n\tmtd->ecc_strength = nanddev_get_ecc_conf(nand)->strength;\n\tmtd->ecc_step_size = nanddev_get_ecc_conf(nand)->step_size;\n\n\tret = spinand_create_dirmaps(spinand);\n\tif (ret) {\n\t\tdev_err(dev,\n\t\t\t\"Failed to create direct mappings for read/write operations (err = %d)\\n\",\n\t\t\tret);\n\t\tgoto err_cleanup_ecc_engine;\n\t}\n\n\treturn 0;\n\nerr_cleanup_ecc_engine:\n\tnanddev_ecc_engine_cleanup(nand);\n\nerr_cleanup_nanddev:\n\tnanddev_cleanup(nand);\n\nerr_manuf_cleanup:\n\tspinand_manufacturer_cleanup(spinand);\n\nerr_free_bufs:\n\tkfree(spinand->databuf);\n\tkfree(spinand->scratchbuf);\n\treturn ret;\n}\n\nstatic void spinand_cleanup(struct spinand_device *spinand)\n{\n\tstruct nand_device *nand = spinand_to_nand(spinand);\n\n\tnanddev_cleanup(nand);\n\tspinand_manufacturer_cleanup(spinand);\n\tkfree(spinand->databuf);\n\tkfree(spinand->scratchbuf);\n}\n\nstatic int spinand_probe(struct spi_mem *mem)\n{\n\tstruct spinand_device *spinand;\n\tstruct mtd_info *mtd;\n\tint ret;\n\n\tspinand = devm_kzalloc(&mem->spi->dev, sizeof(*spinand),\n\t\t\t       GFP_KERNEL);\n\tif (!spinand)\n\t\treturn -ENOMEM;\n\n\tspinand->spimem = mem;\n\tspi_mem_set_drvdata(mem, spinand);\n\tspinand_set_of_node(spinand, mem->spi->dev.of_node);\n\tmutex_init(&spinand->lock);\n\tmtd = spinand_to_mtd(spinand);\n\tmtd->dev.parent = &mem->spi->dev;\n\n\tret = spinand_init(spinand);\n\tif (ret)\n\t\treturn ret;\n\n\tret = mtd_device_register(mtd, NULL, 0);\n\tif (ret)\n\t\tgoto err_spinand_cleanup;\n\n\treturn 0;\n\nerr_spinand_cleanup:\n\tspinand_cleanup(spinand);\n\n\treturn ret;\n}\n\nstatic int spinand_remove(struct spi_mem *mem)\n{\n\tstruct spinand_device *spinand;\n\tstruct mtd_info *mtd;\n\tint ret;\n\n\tspinand = spi_mem_get_drvdata(mem);\n\tmtd = spinand_to_mtd(spinand);\n\n\tret = mtd_device_unregister(mtd);\n\tif (ret)\n\t\treturn ret;\n\n\tspinand_cleanup(spinand);\n\n\treturn 0;\n}\n\nstatic const struct spi_device_id spinand_ids[] = {\n\t{ .name = \"spi-nand\" },\n\t{   },\n};\nMODULE_DEVICE_TABLE(spi, spinand_ids);\n\n#ifdef CONFIG_OF\nstatic const struct of_device_id spinand_of_ids[] = {\n\t{ .compatible = \"spi-nand\" },\n\t{   },\n};\nMODULE_DEVICE_TABLE(of, spinand_of_ids);\n#endif\n\nstatic struct spi_mem_driver spinand_drv = {\n\t.spidrv = {\n\t\t.id_table = spinand_ids,\n\t\t.driver = {\n\t\t\t.name = \"spi-nand\",\n\t\t\t.of_match_table = of_match_ptr(spinand_of_ids),\n\t\t},\n\t},\n\t.probe = spinand_probe,\n\t.remove = spinand_remove,\n};\nmodule_spi_mem_driver(spinand_drv);\n\nMODULE_DESCRIPTION(\"SPI NAND framework\");\nMODULE_AUTHOR(\"Peter Pan<peterpandong@micron.com>\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}