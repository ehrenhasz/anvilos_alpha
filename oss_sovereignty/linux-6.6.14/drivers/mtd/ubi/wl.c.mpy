{
  "module_name": "wl.c",
  "hash_id": "b8b530fb82c3aaaa480964187c341d46b2e8b78cfc40a9006d8352d9292f20dc",
  "original_prompt": "Ingested from linux-6.6.14/drivers/mtd/ubi/wl.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/slab.h>\n#include <linux/crc32.h>\n#include <linux/freezer.h>\n#include <linux/kthread.h>\n#include \"ubi.h\"\n#include \"wl.h\"\n\n \n#define WL_RESERVED_PEBS 1\n\n \n#define UBI_WL_THRESHOLD CONFIG_MTD_UBI_WL_THRESHOLD\n\n \n#define WL_FREE_MAX_DIFF (2*UBI_WL_THRESHOLD)\n\n \n#define WL_MAX_FAILURES 32\n\nstatic int self_check_ec(struct ubi_device *ubi, int pnum, int ec);\nstatic int self_check_in_wl_tree(const struct ubi_device *ubi,\n\t\t\t\t struct ubi_wl_entry *e, struct rb_root *root);\nstatic int self_check_in_pq(const struct ubi_device *ubi,\n\t\t\t    struct ubi_wl_entry *e);\n\n \nstatic void wl_tree_add(struct ubi_wl_entry *e, struct rb_root *root)\n{\n\tstruct rb_node **p, *parent = NULL;\n\n\tp = &root->rb_node;\n\twhile (*p) {\n\t\tstruct ubi_wl_entry *e1;\n\n\t\tparent = *p;\n\t\te1 = rb_entry(parent, struct ubi_wl_entry, u.rb);\n\n\t\tif (e->ec < e1->ec)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (e->ec > e1->ec)\n\t\t\tp = &(*p)->rb_right;\n\t\telse {\n\t\t\tubi_assert(e->pnum != e1->pnum);\n\t\t\tif (e->pnum < e1->pnum)\n\t\t\t\tp = &(*p)->rb_left;\n\t\t\telse\n\t\t\t\tp = &(*p)->rb_right;\n\t\t}\n\t}\n\n\trb_link_node(&e->u.rb, parent, p);\n\trb_insert_color(&e->u.rb, root);\n}\n\n \nstatic void wl_entry_destroy(struct ubi_device *ubi, struct ubi_wl_entry *e)\n{\n\tubi->lookuptbl[e->pnum] = NULL;\n\tkmem_cache_free(ubi_wl_entry_slab, e);\n}\n\n \nstatic int do_work(struct ubi_device *ubi)\n{\n\tint err;\n\tstruct ubi_work *wrk;\n\n\tcond_resched();\n\n\t \n\tdown_read(&ubi->work_sem);\n\tspin_lock(&ubi->wl_lock);\n\tif (list_empty(&ubi->works)) {\n\t\tspin_unlock(&ubi->wl_lock);\n\t\tup_read(&ubi->work_sem);\n\t\treturn 0;\n\t}\n\n\twrk = list_entry(ubi->works.next, struct ubi_work, list);\n\tlist_del(&wrk->list);\n\tubi->works_count -= 1;\n\tubi_assert(ubi->works_count >= 0);\n\tspin_unlock(&ubi->wl_lock);\n\n\t \n\terr = wrk->func(ubi, wrk, 0);\n\tif (err)\n\t\tubi_err(ubi, \"work failed with error code %d\", err);\n\tup_read(&ubi->work_sem);\n\n\treturn err;\n}\n\n \nstatic int in_wl_tree(struct ubi_wl_entry *e, struct rb_root *root)\n{\n\tstruct rb_node *p;\n\n\tp = root->rb_node;\n\twhile (p) {\n\t\tstruct ubi_wl_entry *e1;\n\n\t\te1 = rb_entry(p, struct ubi_wl_entry, u.rb);\n\n\t\tif (e->pnum == e1->pnum) {\n\t\t\tubi_assert(e == e1);\n\t\t\treturn 1;\n\t\t}\n\n\t\tif (e->ec < e1->ec)\n\t\t\tp = p->rb_left;\n\t\telse if (e->ec > e1->ec)\n\t\t\tp = p->rb_right;\n\t\telse {\n\t\t\tubi_assert(e->pnum != e1->pnum);\n\t\t\tif (e->pnum < e1->pnum)\n\t\t\t\tp = p->rb_left;\n\t\t\telse\n\t\t\t\tp = p->rb_right;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic inline int in_pq(const struct ubi_device *ubi, struct ubi_wl_entry *e)\n{\n\tstruct ubi_wl_entry *p;\n\tint i;\n\n\tfor (i = 0; i < UBI_PROT_QUEUE_LEN; ++i)\n\t\tlist_for_each_entry(p, &ubi->pq[i], u.list)\n\t\t\tif (p == e)\n\t\t\t\treturn 1;\n\n\treturn 0;\n}\n\n \nstatic void prot_queue_add(struct ubi_device *ubi, struct ubi_wl_entry *e)\n{\n\tint pq_tail = ubi->pq_head - 1;\n\n\tif (pq_tail < 0)\n\t\tpq_tail = UBI_PROT_QUEUE_LEN - 1;\n\tubi_assert(pq_tail >= 0 && pq_tail < UBI_PROT_QUEUE_LEN);\n\tlist_add_tail(&e->u.list, &ubi->pq[pq_tail]);\n\tdbg_wl(\"added PEB %d EC %d to the protection queue\", e->pnum, e->ec);\n}\n\n \nstatic struct ubi_wl_entry *find_wl_entry(struct ubi_device *ubi,\n\t\t\t\t\t  struct rb_root *root, int diff)\n{\n\tstruct rb_node *p;\n\tstruct ubi_wl_entry *e;\n\tint max;\n\n\te = rb_entry(rb_first(root), struct ubi_wl_entry, u.rb);\n\tmax = e->ec + diff;\n\n\tp = root->rb_node;\n\twhile (p) {\n\t\tstruct ubi_wl_entry *e1;\n\n\t\te1 = rb_entry(p, struct ubi_wl_entry, u.rb);\n\t\tif (e1->ec >= max)\n\t\t\tp = p->rb_left;\n\t\telse {\n\t\t\tp = p->rb_right;\n\t\t\te = e1;\n\t\t}\n\t}\n\n\treturn e;\n}\n\n \nstatic struct ubi_wl_entry *find_mean_wl_entry(struct ubi_device *ubi,\n\t\t\t\t\t       struct rb_root *root)\n{\n\tstruct ubi_wl_entry *e, *first, *last;\n\n\tfirst = rb_entry(rb_first(root), struct ubi_wl_entry, u.rb);\n\tlast = rb_entry(rb_last(root), struct ubi_wl_entry, u.rb);\n\n\tif (last->ec - first->ec < WL_FREE_MAX_DIFF) {\n\t\te = rb_entry(root->rb_node, struct ubi_wl_entry, u.rb);\n\n\t\t \n\t\te = may_reserve_for_fm(ubi, e, root);\n\t} else\n\t\te = find_wl_entry(ubi, root, WL_FREE_MAX_DIFF/2);\n\n\treturn e;\n}\n\n \nstatic struct ubi_wl_entry *wl_get_wle(struct ubi_device *ubi)\n{\n\tstruct ubi_wl_entry *e;\n\n\te = find_mean_wl_entry(ubi, &ubi->free);\n\tif (!e) {\n\t\tubi_err(ubi, \"no free eraseblocks\");\n\t\treturn NULL;\n\t}\n\n\tself_check_in_wl_tree(ubi, e, &ubi->free);\n\n\t \n\trb_erase(&e->u.rb, &ubi->free);\n\tubi->free_count--;\n\tdbg_wl(\"PEB %d EC %d\", e->pnum, e->ec);\n\n\treturn e;\n}\n\n \nstatic int prot_queue_del(struct ubi_device *ubi, int pnum)\n{\n\tstruct ubi_wl_entry *e;\n\n\te = ubi->lookuptbl[pnum];\n\tif (!e)\n\t\treturn -ENODEV;\n\n\tif (self_check_in_pq(ubi, e))\n\t\treturn -ENODEV;\n\n\tlist_del(&e->u.list);\n\tdbg_wl(\"deleted PEB %d from the protection queue\", e->pnum);\n\treturn 0;\n}\n\n \nstatic int sync_erase(struct ubi_device *ubi, struct ubi_wl_entry *e,\n\t\t      int torture)\n{\n\tint err;\n\tstruct ubi_ec_hdr *ec_hdr;\n\tunsigned long long ec = e->ec;\n\n\tdbg_wl(\"erase PEB %d, old EC %llu\", e->pnum, ec);\n\n\terr = self_check_ec(ubi, e->pnum, e->ec);\n\tif (err)\n\t\treturn -EINVAL;\n\n\tec_hdr = kzalloc(ubi->ec_hdr_alsize, GFP_NOFS);\n\tif (!ec_hdr)\n\t\treturn -ENOMEM;\n\n\terr = ubi_io_sync_erase(ubi, e->pnum, torture);\n\tif (err < 0)\n\t\tgoto out_free;\n\n\tec += err;\n\tif (ec > UBI_MAX_ERASECOUNTER) {\n\t\t \n\t\tubi_err(ubi, \"erase counter overflow at PEB %d, EC %llu\",\n\t\t\te->pnum, ec);\n\t\terr = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\tdbg_wl(\"erased PEB %d, new EC %llu\", e->pnum, ec);\n\n\tec_hdr->ec = cpu_to_be64(ec);\n\n\terr = ubi_io_write_ec_hdr(ubi, e->pnum, ec_hdr);\n\tif (err)\n\t\tgoto out_free;\n\n\te->ec = ec;\n\tspin_lock(&ubi->wl_lock);\n\tif (e->ec > ubi->max_ec)\n\t\tubi->max_ec = e->ec;\n\tspin_unlock(&ubi->wl_lock);\n\nout_free:\n\tkfree(ec_hdr);\n\treturn err;\n}\n\n \nstatic void serve_prot_queue(struct ubi_device *ubi)\n{\n\tstruct ubi_wl_entry *e, *tmp;\n\tint count;\n\n\t \nrepeat:\n\tcount = 0;\n\tspin_lock(&ubi->wl_lock);\n\tlist_for_each_entry_safe(e, tmp, &ubi->pq[ubi->pq_head], u.list) {\n\t\tdbg_wl(\"PEB %d EC %d protection over, move to used tree\",\n\t\t\te->pnum, e->ec);\n\n\t\tlist_del(&e->u.list);\n\t\twl_tree_add(e, &ubi->used);\n\t\tif (count++ > 32) {\n\t\t\t \n\t\t\tspin_unlock(&ubi->wl_lock);\n\t\t\tcond_resched();\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\n\tubi->pq_head += 1;\n\tif (ubi->pq_head == UBI_PROT_QUEUE_LEN)\n\t\tubi->pq_head = 0;\n\tubi_assert(ubi->pq_head >= 0 && ubi->pq_head < UBI_PROT_QUEUE_LEN);\n\tspin_unlock(&ubi->wl_lock);\n}\n\n \nstatic void __schedule_ubi_work(struct ubi_device *ubi, struct ubi_work *wrk)\n{\n\tspin_lock(&ubi->wl_lock);\n\tlist_add_tail(&wrk->list, &ubi->works);\n\tubi_assert(ubi->works_count >= 0);\n\tubi->works_count += 1;\n\tif (ubi->thread_enabled && !ubi_dbg_is_bgt_disabled(ubi))\n\t\twake_up_process(ubi->bgt_thread);\n\tspin_unlock(&ubi->wl_lock);\n}\n\n \nstatic void schedule_ubi_work(struct ubi_device *ubi, struct ubi_work *wrk)\n{\n\tdown_read(&ubi->work_sem);\n\t__schedule_ubi_work(ubi, wrk);\n\tup_read(&ubi->work_sem);\n}\n\nstatic int erase_worker(struct ubi_device *ubi, struct ubi_work *wl_wrk,\n\t\t\tint shutdown);\n\n \nstatic int schedule_erase(struct ubi_device *ubi, struct ubi_wl_entry *e,\n\t\t\t  int vol_id, int lnum, int torture, bool nested)\n{\n\tstruct ubi_work *wl_wrk;\n\n\tubi_assert(e);\n\n\tdbg_wl(\"schedule erasure of PEB %d, EC %d, torture %d\",\n\t       e->pnum, e->ec, torture);\n\n\twl_wrk = kmalloc(sizeof(struct ubi_work), GFP_NOFS);\n\tif (!wl_wrk)\n\t\treturn -ENOMEM;\n\n\twl_wrk->func = &erase_worker;\n\twl_wrk->e = e;\n\twl_wrk->vol_id = vol_id;\n\twl_wrk->lnum = lnum;\n\twl_wrk->torture = torture;\n\n\tif (nested)\n\t\t__schedule_ubi_work(ubi, wl_wrk);\n\telse\n\t\tschedule_ubi_work(ubi, wl_wrk);\n\treturn 0;\n}\n\nstatic int __erase_worker(struct ubi_device *ubi, struct ubi_work *wl_wrk);\n \nstatic int do_sync_erase(struct ubi_device *ubi, struct ubi_wl_entry *e,\n\t\t\t int vol_id, int lnum, int torture)\n{\n\tstruct ubi_work wl_wrk;\n\n\tdbg_wl(\"sync erase of PEB %i\", e->pnum);\n\n\twl_wrk.e = e;\n\twl_wrk.vol_id = vol_id;\n\twl_wrk.lnum = lnum;\n\twl_wrk.torture = torture;\n\n\treturn __erase_worker(ubi, &wl_wrk);\n}\n\nstatic int ensure_wear_leveling(struct ubi_device *ubi, int nested);\n \nstatic int wear_leveling_worker(struct ubi_device *ubi, struct ubi_work *wrk,\n\t\t\t\tint shutdown)\n{\n\tint err, scrubbing = 0, torture = 0, protect = 0, erroneous = 0;\n\tint erase = 0, keep = 0, vol_id = -1, lnum = -1;\n\tstruct ubi_wl_entry *e1, *e2;\n\tstruct ubi_vid_io_buf *vidb;\n\tstruct ubi_vid_hdr *vid_hdr;\n\tint dst_leb_clean = 0;\n\n\tkfree(wrk);\n\tif (shutdown)\n\t\treturn 0;\n\n\tvidb = ubi_alloc_vid_buf(ubi, GFP_NOFS);\n\tif (!vidb)\n\t\treturn -ENOMEM;\n\n\tvid_hdr = ubi_get_vid_hdr(vidb);\n\n\tdown_read(&ubi->fm_eba_sem);\n\tmutex_lock(&ubi->move_mutex);\n\tspin_lock(&ubi->wl_lock);\n\tubi_assert(!ubi->move_from && !ubi->move_to);\n\tubi_assert(!ubi->move_to_put);\n\n#ifdef CONFIG_MTD_UBI_FASTMAP\n\tif (!next_peb_for_wl(ubi) ||\n#else\n\tif (!ubi->free.rb_node ||\n#endif\n\t    (!ubi->used.rb_node && !ubi->scrub.rb_node)) {\n\t\t \n\t\tdbg_wl(\"cancel WL, a list is empty: free %d, used %d\",\n\t\t       !ubi->free.rb_node, !ubi->used.rb_node);\n\t\tgoto out_cancel;\n\t}\n\n#ifdef CONFIG_MTD_UBI_FASTMAP\n\te1 = find_anchor_wl_entry(&ubi->used);\n\tif (e1 && ubi->fm_anchor &&\n\t    (ubi->fm_anchor->ec - e1->ec >= UBI_WL_THRESHOLD)) {\n\t\tubi->fm_do_produce_anchor = 1;\n\t\t \n\t\twl_tree_add(ubi->fm_anchor, &ubi->free);\n\t\tubi->fm_anchor = NULL;\n\t\tubi->free_count++;\n\t}\n\n\tif (ubi->fm_do_produce_anchor) {\n\t\tif (!e1)\n\t\t\tgoto out_cancel;\n\t\te2 = get_peb_for_wl(ubi);\n\t\tif (!e2)\n\t\t\tgoto out_cancel;\n\n\t\tself_check_in_wl_tree(ubi, e1, &ubi->used);\n\t\trb_erase(&e1->u.rb, &ubi->used);\n\t\tdbg_wl(\"anchor-move PEB %d to PEB %d\", e1->pnum, e2->pnum);\n\t\tubi->fm_do_produce_anchor = 0;\n\t} else if (!ubi->scrub.rb_node) {\n#else\n\tif (!ubi->scrub.rb_node) {\n#endif\n\t\t \n\t\te1 = rb_entry(rb_first(&ubi->used), struct ubi_wl_entry, u.rb);\n\t\te2 = get_peb_for_wl(ubi);\n\t\tif (!e2)\n\t\t\tgoto out_cancel;\n\n\t\tif (!(e2->ec - e1->ec >= UBI_WL_THRESHOLD)) {\n\t\t\tdbg_wl(\"no WL needed: min used EC %d, max free EC %d\",\n\t\t\t       e1->ec, e2->ec);\n\n\t\t\t \n\t\t\twl_tree_add(e2, &ubi->free);\n\t\t\tubi->free_count++;\n\t\t\tgoto out_cancel;\n\t\t}\n\t\tself_check_in_wl_tree(ubi, e1, &ubi->used);\n\t\trb_erase(&e1->u.rb, &ubi->used);\n\t\tdbg_wl(\"move PEB %d EC %d to PEB %d EC %d\",\n\t\t       e1->pnum, e1->ec, e2->pnum, e2->ec);\n\t} else {\n\t\t \n\t\tscrubbing = 1;\n\t\te1 = rb_entry(rb_first(&ubi->scrub), struct ubi_wl_entry, u.rb);\n\t\te2 = get_peb_for_wl(ubi);\n\t\tif (!e2)\n\t\t\tgoto out_cancel;\n\n\t\tself_check_in_wl_tree(ubi, e1, &ubi->scrub);\n\t\trb_erase(&e1->u.rb, &ubi->scrub);\n\t\tdbg_wl(\"scrub PEB %d to PEB %d\", e1->pnum, e2->pnum);\n\t}\n\n\tubi->move_from = e1;\n\tubi->move_to = e2;\n\tspin_unlock(&ubi->wl_lock);\n\n\t \n\n\terr = ubi_io_read_vid_hdr(ubi, e1->pnum, vidb, 0);\n\tif (err && err != UBI_IO_BITFLIPS) {\n\t\tdst_leb_clean = 1;\n\t\tif (err == UBI_IO_FF) {\n\t\t\t \n\t\t\tdbg_wl(\"PEB %d has no VID header\", e1->pnum);\n\t\t\tprotect = 1;\n\t\t\tgoto out_not_moved;\n\t\t} else if (err == UBI_IO_FF_BITFLIPS) {\n\t\t\t \n\t\t\tdbg_wl(\"PEB %d has no VID header but has bit-flips\",\n\t\t\t       e1->pnum);\n\t\t\tscrubbing = 1;\n\t\t\tgoto out_not_moved;\n\t\t} else if (ubi->fast_attach && err == UBI_IO_BAD_HDR_EBADMSG) {\n\t\t\t \n\t\t\tdbg_wl(\"PEB %d has ECC errors, maybe from an interrupted erasure\",\n\t\t\t       e1->pnum);\n\t\t\terase = 1;\n\t\t\tgoto out_not_moved;\n\t\t}\n\n\t\tubi_err(ubi, \"error %d while reading VID header from PEB %d\",\n\t\t\terr, e1->pnum);\n\t\tgoto out_error;\n\t}\n\n\tvol_id = be32_to_cpu(vid_hdr->vol_id);\n\tlnum = be32_to_cpu(vid_hdr->lnum);\n\n\terr = ubi_eba_copy_leb(ubi, e1->pnum, e2->pnum, vidb);\n\tif (err) {\n\t\tif (err == MOVE_CANCEL_RACE) {\n\t\t\t \n\t\t\tprotect = 1;\n\t\t\tdst_leb_clean = 1;\n\t\t\tgoto out_not_moved;\n\t\t}\n\t\tif (err == MOVE_RETRY) {\n\t\t\tscrubbing = 1;\n\t\t\tdst_leb_clean = 1;\n\t\t\tgoto out_not_moved;\n\t\t}\n\t\tif (err == MOVE_TARGET_BITFLIPS || err == MOVE_TARGET_WR_ERR ||\n\t\t    err == MOVE_TARGET_RD_ERR) {\n\t\t\t \n\t\t\ttorture = 1;\n\t\t\tkeep = 1;\n\t\t\tgoto out_not_moved;\n\t\t}\n\n\t\tif (err == MOVE_SOURCE_RD_ERR) {\n\t\t\t \n\t\t\tif (ubi->erroneous_peb_count > ubi->max_erroneous) {\n\t\t\t\tubi_err(ubi, \"too many erroneous eraseblocks (%d)\",\n\t\t\t\t\tubi->erroneous_peb_count);\n\t\t\t\tgoto out_error;\n\t\t\t}\n\t\t\tdst_leb_clean = 1;\n\t\t\terroneous = 1;\n\t\t\tgoto out_not_moved;\n\t\t}\n\n\t\tif (err < 0)\n\t\t\tgoto out_error;\n\n\t\tubi_assert(0);\n\t}\n\n\t \n\tif (scrubbing)\n\t\tubi_msg(ubi, \"scrubbed PEB %d (LEB %d:%d), data moved to PEB %d\",\n\t\t\te1->pnum, vol_id, lnum, e2->pnum);\n\tubi_free_vid_buf(vidb);\n\n\tspin_lock(&ubi->wl_lock);\n\tif (!ubi->move_to_put) {\n\t\twl_tree_add(e2, &ubi->used);\n\t\te2 = NULL;\n\t}\n\tubi->move_from = ubi->move_to = NULL;\n\tubi->move_to_put = ubi->wl_scheduled = 0;\n\tspin_unlock(&ubi->wl_lock);\n\n\terr = do_sync_erase(ubi, e1, vol_id, lnum, 0);\n\tif (err) {\n\t\tif (e2) {\n\t\t\tspin_lock(&ubi->wl_lock);\n\t\t\twl_entry_destroy(ubi, e2);\n\t\t\tspin_unlock(&ubi->wl_lock);\n\t\t}\n\t\tgoto out_ro;\n\t}\n\n\tif (e2) {\n\t\t \n\t\tdbg_wl(\"PEB %d (LEB %d:%d) was put meanwhile, erase\",\n\t\t       e2->pnum, vol_id, lnum);\n\t\terr = do_sync_erase(ubi, e2, vol_id, lnum, 0);\n\t\tif (err)\n\t\t\tgoto out_ro;\n\t}\n\n\tdbg_wl(\"done\");\n\tmutex_unlock(&ubi->move_mutex);\n\tup_read(&ubi->fm_eba_sem);\n\treturn 0;\n\n\t \nout_not_moved:\n\tif (vol_id != -1)\n\t\tdbg_wl(\"cancel moving PEB %d (LEB %d:%d) to PEB %d (%d)\",\n\t\t       e1->pnum, vol_id, lnum, e2->pnum, err);\n\telse\n\t\tdbg_wl(\"cancel moving PEB %d to PEB %d (%d)\",\n\t\t       e1->pnum, e2->pnum, err);\n\tspin_lock(&ubi->wl_lock);\n\tif (protect)\n\t\tprot_queue_add(ubi, e1);\n\telse if (erroneous) {\n\t\twl_tree_add(e1, &ubi->erroneous);\n\t\tubi->erroneous_peb_count += 1;\n\t} else if (scrubbing)\n\t\twl_tree_add(e1, &ubi->scrub);\n\telse if (keep)\n\t\twl_tree_add(e1, &ubi->used);\n\tif (dst_leb_clean) {\n\t\twl_tree_add(e2, &ubi->free);\n\t\tubi->free_count++;\n\t}\n\n\tubi_assert(!ubi->move_to_put);\n\tubi->move_from = ubi->move_to = NULL;\n\tubi->wl_scheduled = 0;\n\tspin_unlock(&ubi->wl_lock);\n\n\tubi_free_vid_buf(vidb);\n\tif (dst_leb_clean) {\n\t\tensure_wear_leveling(ubi, 1);\n\t} else {\n\t\terr = do_sync_erase(ubi, e2, vol_id, lnum, torture);\n\t\tif (err)\n\t\t\tgoto out_ro;\n\t}\n\n\tif (erase) {\n\t\terr = do_sync_erase(ubi, e1, vol_id, lnum, 1);\n\t\tif (err)\n\t\t\tgoto out_ro;\n\t}\n\n\tmutex_unlock(&ubi->move_mutex);\n\tup_read(&ubi->fm_eba_sem);\n\treturn 0;\n\nout_error:\n\tif (vol_id != -1)\n\t\tubi_err(ubi, \"error %d while moving PEB %d to PEB %d\",\n\t\t\terr, e1->pnum, e2->pnum);\n\telse\n\t\tubi_err(ubi, \"error %d while moving PEB %d (LEB %d:%d) to PEB %d\",\n\t\t\terr, e1->pnum, vol_id, lnum, e2->pnum);\n\tspin_lock(&ubi->wl_lock);\n\tubi->move_from = ubi->move_to = NULL;\n\tubi->move_to_put = ubi->wl_scheduled = 0;\n\twl_entry_destroy(ubi, e1);\n\twl_entry_destroy(ubi, e2);\n\tspin_unlock(&ubi->wl_lock);\n\n\tubi_free_vid_buf(vidb);\n\nout_ro:\n\tubi_ro_mode(ubi);\n\tmutex_unlock(&ubi->move_mutex);\n\tup_read(&ubi->fm_eba_sem);\n\tubi_assert(err != 0);\n\treturn err < 0 ? err : -EIO;\n\nout_cancel:\n\tubi->wl_scheduled = 0;\n\tspin_unlock(&ubi->wl_lock);\n\tmutex_unlock(&ubi->move_mutex);\n\tup_read(&ubi->fm_eba_sem);\n\tubi_free_vid_buf(vidb);\n\treturn 0;\n}\n\n \nstatic int ensure_wear_leveling(struct ubi_device *ubi, int nested)\n{\n\tint err = 0;\n\tstruct ubi_work *wrk;\n\n\tspin_lock(&ubi->wl_lock);\n\tif (ubi->wl_scheduled)\n\t\t \n\t\tgoto out_unlock;\n\n\t \n\tif (!ubi->scrub.rb_node) {\n#ifdef CONFIG_MTD_UBI_FASTMAP\n\t\tif (!need_wear_leveling(ubi))\n\t\t\tgoto out_unlock;\n#else\n\t\tstruct ubi_wl_entry *e1;\n\t\tstruct ubi_wl_entry *e2;\n\n\t\tif (!ubi->used.rb_node || !ubi->free.rb_node)\n\t\t\t \n\t\t\tgoto out_unlock;\n\n\t\t \n\t\te1 = rb_entry(rb_first(&ubi->used), struct ubi_wl_entry, u.rb);\n\t\te2 = find_wl_entry(ubi, &ubi->free, WL_FREE_MAX_DIFF);\n\n\t\tif (!(e2->ec - e1->ec >= UBI_WL_THRESHOLD))\n\t\t\tgoto out_unlock;\n#endif\n\t\tdbg_wl(\"schedule wear-leveling\");\n\t} else\n\t\tdbg_wl(\"schedule scrubbing\");\n\n\tubi->wl_scheduled = 1;\n\tspin_unlock(&ubi->wl_lock);\n\n\twrk = kmalloc(sizeof(struct ubi_work), GFP_NOFS);\n\tif (!wrk) {\n\t\terr = -ENOMEM;\n\t\tgoto out_cancel;\n\t}\n\n\twrk->func = &wear_leveling_worker;\n\tif (nested)\n\t\t__schedule_ubi_work(ubi, wrk);\n\telse\n\t\tschedule_ubi_work(ubi, wrk);\n\treturn err;\n\nout_cancel:\n\tspin_lock(&ubi->wl_lock);\n\tubi->wl_scheduled = 0;\nout_unlock:\n\tspin_unlock(&ubi->wl_lock);\n\treturn err;\n}\n\n \nstatic int __erase_worker(struct ubi_device *ubi, struct ubi_work *wl_wrk)\n{\n\tstruct ubi_wl_entry *e = wl_wrk->e;\n\tint pnum = e->pnum;\n\tint vol_id = wl_wrk->vol_id;\n\tint lnum = wl_wrk->lnum;\n\tint err, available_consumed = 0;\n\n\tdbg_wl(\"erase PEB %d EC %d LEB %d:%d\",\n\t       pnum, e->ec, wl_wrk->vol_id, wl_wrk->lnum);\n\n\terr = sync_erase(ubi, e, wl_wrk->torture);\n\tif (!err) {\n\t\tspin_lock(&ubi->wl_lock);\n\n\t\tif (!ubi->fm_disabled && !ubi->fm_anchor &&\n\t\t    e->pnum < UBI_FM_MAX_START) {\n\t\t\t \n\t\t\tubi->fm_anchor = e;\n\t\t\tubi->fm_do_produce_anchor = 0;\n\t\t} else {\n\t\t\twl_tree_add(e, &ubi->free);\n\t\t\tubi->free_count++;\n\t\t}\n\n\t\tspin_unlock(&ubi->wl_lock);\n\n\t\t \n\t\tserve_prot_queue(ubi);\n\n\t\t \n\t\terr = ensure_wear_leveling(ubi, 1);\n\t\treturn err;\n\t}\n\n\tubi_err(ubi, \"failed to erase PEB %d, error %d\", pnum, err);\n\n\tif (err == -EINTR || err == -ENOMEM || err == -EAGAIN ||\n\t    err == -EBUSY) {\n\t\tint err1;\n\n\t\t \n\t\terr1 = schedule_erase(ubi, e, vol_id, lnum, 0, true);\n\t\tif (err1) {\n\t\t\tspin_lock(&ubi->wl_lock);\n\t\t\twl_entry_destroy(ubi, e);\n\t\t\tspin_unlock(&ubi->wl_lock);\n\t\t\terr = err1;\n\t\t\tgoto out_ro;\n\t\t}\n\t\treturn err;\n\t}\n\n\tspin_lock(&ubi->wl_lock);\n\twl_entry_destroy(ubi, e);\n\tspin_unlock(&ubi->wl_lock);\n\tif (err != -EIO)\n\t\t \n\t\tgoto out_ro;\n\n\t \n\n\tif (!ubi->bad_allowed) {\n\t\tubi_err(ubi, \"bad physical eraseblock %d detected\", pnum);\n\t\tgoto out_ro;\n\t}\n\n\tspin_lock(&ubi->volumes_lock);\n\tif (ubi->beb_rsvd_pebs == 0) {\n\t\tif (ubi->avail_pebs == 0) {\n\t\t\tspin_unlock(&ubi->volumes_lock);\n\t\t\tubi_err(ubi, \"no reserved/available physical eraseblocks\");\n\t\t\tgoto out_ro;\n\t\t}\n\t\tubi->avail_pebs -= 1;\n\t\tavailable_consumed = 1;\n\t}\n\tspin_unlock(&ubi->volumes_lock);\n\n\tubi_msg(ubi, \"mark PEB %d as bad\", pnum);\n\terr = ubi_io_mark_bad(ubi, pnum);\n\tif (err)\n\t\tgoto out_ro;\n\n\tspin_lock(&ubi->volumes_lock);\n\tif (ubi->beb_rsvd_pebs > 0) {\n\t\tif (available_consumed) {\n\t\t\t \n\t\t\tubi->avail_pebs += 1;\n\t\t\tavailable_consumed = 0;\n\t\t}\n\t\tubi->beb_rsvd_pebs -= 1;\n\t}\n\tubi->bad_peb_count += 1;\n\tubi->good_peb_count -= 1;\n\tubi_calculate_reserved(ubi);\n\tif (available_consumed)\n\t\tubi_warn(ubi, \"no PEBs in the reserved pool, used an available PEB\");\n\telse if (ubi->beb_rsvd_pebs)\n\t\tubi_msg(ubi, \"%d PEBs left in the reserve\",\n\t\t\tubi->beb_rsvd_pebs);\n\telse\n\t\tubi_warn(ubi, \"last PEB from the reserve was used\");\n\tspin_unlock(&ubi->volumes_lock);\n\n\treturn err;\n\nout_ro:\n\tif (available_consumed) {\n\t\tspin_lock(&ubi->volumes_lock);\n\t\tubi->avail_pebs += 1;\n\t\tspin_unlock(&ubi->volumes_lock);\n\t}\n\tubi_ro_mode(ubi);\n\treturn err;\n}\n\nstatic int erase_worker(struct ubi_device *ubi, struct ubi_work *wl_wrk,\n\t\t\t  int shutdown)\n{\n\tint ret;\n\n\tif (shutdown) {\n\t\tstruct ubi_wl_entry *e = wl_wrk->e;\n\n\t\tdbg_wl(\"cancel erasure of PEB %d EC %d\", e->pnum, e->ec);\n\t\tkfree(wl_wrk);\n\t\twl_entry_destroy(ubi, e);\n\t\treturn 0;\n\t}\n\n\tret = __erase_worker(ubi, wl_wrk);\n\tkfree(wl_wrk);\n\treturn ret;\n}\n\n \nint ubi_wl_put_peb(struct ubi_device *ubi, int vol_id, int lnum,\n\t\t   int pnum, int torture)\n{\n\tint err;\n\tstruct ubi_wl_entry *e;\n\n\tdbg_wl(\"PEB %d\", pnum);\n\tubi_assert(pnum >= 0);\n\tubi_assert(pnum < ubi->peb_count);\n\n\tdown_read(&ubi->fm_protect);\n\nretry:\n\tspin_lock(&ubi->wl_lock);\n\te = ubi->lookuptbl[pnum];\n\tif (!e) {\n\t\t \n\t\tspin_unlock(&ubi->wl_lock);\n\t\tup_read(&ubi->fm_protect);\n\t\treturn 0;\n\t}\n\tif (e == ubi->move_from) {\n\t\t \n\t\tdbg_wl(\"PEB %d is being moved, wait\", pnum);\n\t\tspin_unlock(&ubi->wl_lock);\n\n\t\t \n\t\tmutex_lock(&ubi->move_mutex);\n\t\tmutex_unlock(&ubi->move_mutex);\n\t\tgoto retry;\n\t} else if (e == ubi->move_to) {\n\t\t \n\t\tdbg_wl(\"PEB %d is the target of data moving\", pnum);\n\t\tubi_assert(!ubi->move_to_put);\n\t\tubi->move_to_put = 1;\n\t\tspin_unlock(&ubi->wl_lock);\n\t\tup_read(&ubi->fm_protect);\n\t\treturn 0;\n\t} else {\n\t\tif (in_wl_tree(e, &ubi->used)) {\n\t\t\tself_check_in_wl_tree(ubi, e, &ubi->used);\n\t\t\trb_erase(&e->u.rb, &ubi->used);\n\t\t} else if (in_wl_tree(e, &ubi->scrub)) {\n\t\t\tself_check_in_wl_tree(ubi, e, &ubi->scrub);\n\t\t\trb_erase(&e->u.rb, &ubi->scrub);\n\t\t} else if (in_wl_tree(e, &ubi->erroneous)) {\n\t\t\tself_check_in_wl_tree(ubi, e, &ubi->erroneous);\n\t\t\trb_erase(&e->u.rb, &ubi->erroneous);\n\t\t\tubi->erroneous_peb_count -= 1;\n\t\t\tubi_assert(ubi->erroneous_peb_count >= 0);\n\t\t\t \n\t\t\ttorture = 1;\n\t\t} else {\n\t\t\terr = prot_queue_del(ubi, e->pnum);\n\t\t\tif (err) {\n\t\t\t\tubi_err(ubi, \"PEB %d not found\", pnum);\n\t\t\t\tubi_ro_mode(ubi);\n\t\t\t\tspin_unlock(&ubi->wl_lock);\n\t\t\t\tup_read(&ubi->fm_protect);\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock(&ubi->wl_lock);\n\n\terr = schedule_erase(ubi, e, vol_id, lnum, torture, false);\n\tif (err) {\n\t\tspin_lock(&ubi->wl_lock);\n\t\twl_tree_add(e, &ubi->used);\n\t\tspin_unlock(&ubi->wl_lock);\n\t}\n\n\tup_read(&ubi->fm_protect);\n\treturn err;\n}\n\n \nint ubi_wl_scrub_peb(struct ubi_device *ubi, int pnum)\n{\n\tstruct ubi_wl_entry *e;\n\n\tubi_msg(ubi, \"schedule PEB %d for scrubbing\", pnum);\n\nretry:\n\tspin_lock(&ubi->wl_lock);\n\te = ubi->lookuptbl[pnum];\n\tif (e == ubi->move_from || in_wl_tree(e, &ubi->scrub) ||\n\t\t\t\t   in_wl_tree(e, &ubi->erroneous)) {\n\t\tspin_unlock(&ubi->wl_lock);\n\t\treturn 0;\n\t}\n\n\tif (e == ubi->move_to) {\n\t\t \n\t\tspin_unlock(&ubi->wl_lock);\n\t\tdbg_wl(\"the PEB %d is not in proper tree, retry\", pnum);\n\t\tyield();\n\t\tgoto retry;\n\t}\n\n\tif (in_wl_tree(e, &ubi->used)) {\n\t\tself_check_in_wl_tree(ubi, e, &ubi->used);\n\t\trb_erase(&e->u.rb, &ubi->used);\n\t} else {\n\t\tint err;\n\n\t\terr = prot_queue_del(ubi, e->pnum);\n\t\tif (err) {\n\t\t\tubi_err(ubi, \"PEB %d not found\", pnum);\n\t\t\tubi_ro_mode(ubi);\n\t\t\tspin_unlock(&ubi->wl_lock);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\twl_tree_add(e, &ubi->scrub);\n\tspin_unlock(&ubi->wl_lock);\n\n\t \n\treturn ensure_wear_leveling(ubi, 0);\n}\n\n \nint ubi_wl_flush(struct ubi_device *ubi, int vol_id, int lnum)\n{\n\tint err = 0;\n\tint found = 1;\n\n\t \n\tdbg_wl(\"flush pending work for LEB %d:%d (%d pending works)\",\n\t       vol_id, lnum, ubi->works_count);\n\n\twhile (found) {\n\t\tstruct ubi_work *wrk, *tmp;\n\t\tfound = 0;\n\n\t\tdown_read(&ubi->work_sem);\n\t\tspin_lock(&ubi->wl_lock);\n\t\tlist_for_each_entry_safe(wrk, tmp, &ubi->works, list) {\n\t\t\tif ((vol_id == UBI_ALL || wrk->vol_id == vol_id) &&\n\t\t\t    (lnum == UBI_ALL || wrk->lnum == lnum)) {\n\t\t\t\tlist_del(&wrk->list);\n\t\t\t\tubi->works_count -= 1;\n\t\t\t\tubi_assert(ubi->works_count >= 0);\n\t\t\t\tspin_unlock(&ubi->wl_lock);\n\n\t\t\t\terr = wrk->func(ubi, wrk, 0);\n\t\t\t\tif (err) {\n\t\t\t\t\tup_read(&ubi->work_sem);\n\t\t\t\t\treturn err;\n\t\t\t\t}\n\n\t\t\t\tspin_lock(&ubi->wl_lock);\n\t\t\t\tfound = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&ubi->wl_lock);\n\t\tup_read(&ubi->work_sem);\n\t}\n\n\t \n\tdown_write(&ubi->work_sem);\n\tup_write(&ubi->work_sem);\n\n\treturn err;\n}\n\nstatic bool scrub_possible(struct ubi_device *ubi, struct ubi_wl_entry *e)\n{\n\tif (in_wl_tree(e, &ubi->scrub))\n\t\treturn false;\n\telse if (in_wl_tree(e, &ubi->erroneous))\n\t\treturn false;\n\telse if (ubi->move_from == e)\n\t\treturn false;\n\telse if (ubi->move_to == e)\n\t\treturn false;\n\n\treturn true;\n}\n\n \nint ubi_bitflip_check(struct ubi_device *ubi, int pnum, int force)\n{\n\tint err = 0;\n\tstruct ubi_wl_entry *e;\n\n\tif (pnum < 0 || pnum >= ubi->peb_count) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tdown_write(&ubi->work_sem);\n\n\t \n\tspin_lock(&ubi->wl_lock);\n\te = ubi->lookuptbl[pnum];\n\tif (!e) {\n\t\tspin_unlock(&ubi->wl_lock);\n\t\terr = -ENOENT;\n\t\tgoto out_resume;\n\t}\n\n\t \n\tif (!scrub_possible(ubi, e)) {\n\t\tspin_unlock(&ubi->wl_lock);\n\t\terr = -EBUSY;\n\t\tgoto out_resume;\n\t}\n\tspin_unlock(&ubi->wl_lock);\n\n\tif (!force) {\n\t\tmutex_lock(&ubi->buf_mutex);\n\t\terr = ubi_io_read(ubi, ubi->peb_buf, pnum, 0, ubi->peb_size);\n\t\tmutex_unlock(&ubi->buf_mutex);\n\t}\n\n\tif (force || err == UBI_IO_BITFLIPS) {\n\t\t \n\t\tspin_lock(&ubi->wl_lock);\n\n\t\t \n\t\te = ubi->lookuptbl[pnum];\n\t\tif (!e) {\n\t\t\tspin_unlock(&ubi->wl_lock);\n\t\t\terr = -ENOENT;\n\t\t\tgoto out_resume;\n\t\t}\n\n\t\t \n\t\tif (!scrub_possible(ubi, e)) {\n\t\t\tspin_unlock(&ubi->wl_lock);\n\t\t\terr = -EBUSY;\n\t\t\tgoto out_resume;\n\t\t}\n\n\t\tif (in_pq(ubi, e)) {\n\t\t\tprot_queue_del(ubi, e->pnum);\n\t\t\twl_tree_add(e, &ubi->scrub);\n\t\t\tspin_unlock(&ubi->wl_lock);\n\n\t\t\terr = ensure_wear_leveling(ubi, 1);\n\t\t} else if (in_wl_tree(e, &ubi->used)) {\n\t\t\trb_erase(&e->u.rb, &ubi->used);\n\t\t\twl_tree_add(e, &ubi->scrub);\n\t\t\tspin_unlock(&ubi->wl_lock);\n\n\t\t\terr = ensure_wear_leveling(ubi, 1);\n\t\t} else if (in_wl_tree(e, &ubi->free)) {\n\t\t\trb_erase(&e->u.rb, &ubi->free);\n\t\t\tubi->free_count--;\n\t\t\tspin_unlock(&ubi->wl_lock);\n\n\t\t\t \n\t\t\terr = schedule_erase(ubi, e, UBI_UNKNOWN, UBI_UNKNOWN,\n\t\t\t\t\t     force ? 0 : 1, true);\n\t\t} else {\n\t\t\tspin_unlock(&ubi->wl_lock);\n\t\t\terr = -EAGAIN;\n\t\t}\n\n\t\tif (!err && !force)\n\t\t\terr = -EUCLEAN;\n\t} else {\n\t\terr = 0;\n\t}\n\nout_resume:\n\tup_write(&ubi->work_sem);\nout:\n\n\treturn err;\n}\n\n \nstatic void tree_destroy(struct ubi_device *ubi, struct rb_root *root)\n{\n\tstruct rb_node *rb;\n\tstruct ubi_wl_entry *e;\n\n\trb = root->rb_node;\n\twhile (rb) {\n\t\tif (rb->rb_left)\n\t\t\trb = rb->rb_left;\n\t\telse if (rb->rb_right)\n\t\t\trb = rb->rb_right;\n\t\telse {\n\t\t\te = rb_entry(rb, struct ubi_wl_entry, u.rb);\n\n\t\t\trb = rb_parent(rb);\n\t\t\tif (rb) {\n\t\t\t\tif (rb->rb_left == &e->u.rb)\n\t\t\t\t\trb->rb_left = NULL;\n\t\t\t\telse\n\t\t\t\t\trb->rb_right = NULL;\n\t\t\t}\n\n\t\t\twl_entry_destroy(ubi, e);\n\t\t}\n\t}\n}\n\n \nint ubi_thread(void *u)\n{\n\tint failures = 0;\n\tstruct ubi_device *ubi = u;\n\n\tubi_msg(ubi, \"background thread \\\"%s\\\" started, PID %d\",\n\t\tubi->bgt_name, task_pid_nr(current));\n\n\tset_freezable();\n\tfor (;;) {\n\t\tint err;\n\n\t\tif (kthread_should_stop())\n\t\t\tbreak;\n\n\t\tif (try_to_freeze())\n\t\t\tcontinue;\n\n\t\tspin_lock(&ubi->wl_lock);\n\t\tif (list_empty(&ubi->works) || ubi->ro_mode ||\n\t\t    !ubi->thread_enabled || ubi_dbg_is_bgt_disabled(ubi)) {\n\t\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t\tspin_unlock(&ubi->wl_lock);\n\n\t\t\t \n\t\t\tif (kthread_should_stop()) {\n\t\t\t\tset_current_state(TASK_RUNNING);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tschedule();\n\t\t\tcontinue;\n\t\t}\n\t\tspin_unlock(&ubi->wl_lock);\n\n\t\terr = do_work(ubi);\n\t\tif (err) {\n\t\t\tubi_err(ubi, \"%s: work failed with error code %d\",\n\t\t\t\tubi->bgt_name, err);\n\t\t\tif (failures++ > WL_MAX_FAILURES) {\n\t\t\t\t \n\t\t\t\tubi_msg(ubi, \"%s: %d consecutive failures\",\n\t\t\t\t\tubi->bgt_name, WL_MAX_FAILURES);\n\t\t\t\tubi_ro_mode(ubi);\n\t\t\t\tubi->thread_enabled = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t} else\n\t\t\tfailures = 0;\n\n\t\tcond_resched();\n\t}\n\n\tdbg_wl(\"background thread \\\"%s\\\" is killed\", ubi->bgt_name);\n\tubi->thread_enabled = 0;\n\treturn 0;\n}\n\n \nstatic void shutdown_work(struct ubi_device *ubi)\n{\n\twhile (!list_empty(&ubi->works)) {\n\t\tstruct ubi_work *wrk;\n\n\t\twrk = list_entry(ubi->works.next, struct ubi_work, list);\n\t\tlist_del(&wrk->list);\n\t\twrk->func(ubi, wrk, 1);\n\t\tubi->works_count -= 1;\n\t\tubi_assert(ubi->works_count >= 0);\n\t}\n}\n\n \nstatic int erase_aeb(struct ubi_device *ubi, struct ubi_ainf_peb *aeb, bool sync)\n{\n\tstruct ubi_wl_entry *e;\n\tint err;\n\n\te = kmem_cache_alloc(ubi_wl_entry_slab, GFP_KERNEL);\n\tif (!e)\n\t\treturn -ENOMEM;\n\n\te->pnum = aeb->pnum;\n\te->ec = aeb->ec;\n\tubi->lookuptbl[e->pnum] = e;\n\n\tif (sync) {\n\t\terr = sync_erase(ubi, e, false);\n\t\tif (err)\n\t\t\tgoto out_free;\n\n\t\twl_tree_add(e, &ubi->free);\n\t\tubi->free_count++;\n\t} else {\n\t\terr = schedule_erase(ubi, e, aeb->vol_id, aeb->lnum, 0, false);\n\t\tif (err)\n\t\t\tgoto out_free;\n\t}\n\n\treturn 0;\n\nout_free:\n\twl_entry_destroy(ubi, e);\n\n\treturn err;\n}\n\n \nint ubi_wl_init(struct ubi_device *ubi, struct ubi_attach_info *ai)\n{\n\tint err, i, reserved_pebs, found_pebs = 0;\n\tstruct rb_node *rb1, *rb2;\n\tstruct ubi_ainf_volume *av;\n\tstruct ubi_ainf_peb *aeb, *tmp;\n\tstruct ubi_wl_entry *e;\n\n\tubi->used = ubi->erroneous = ubi->free = ubi->scrub = RB_ROOT;\n\tspin_lock_init(&ubi->wl_lock);\n\tmutex_init(&ubi->move_mutex);\n\tinit_rwsem(&ubi->work_sem);\n\tubi->max_ec = ai->max_ec;\n\tINIT_LIST_HEAD(&ubi->works);\n\n\tsprintf(ubi->bgt_name, UBI_BGT_NAME_PATTERN, ubi->ubi_num);\n\n\terr = -ENOMEM;\n\tubi->lookuptbl = kcalloc(ubi->peb_count, sizeof(void *), GFP_KERNEL);\n\tif (!ubi->lookuptbl)\n\t\treturn err;\n\n\tfor (i = 0; i < UBI_PROT_QUEUE_LEN; i++)\n\t\tINIT_LIST_HEAD(&ubi->pq[i]);\n\tubi->pq_head = 0;\n\n\tubi->free_count = 0;\n\tlist_for_each_entry_safe(aeb, tmp, &ai->erase, u.list) {\n\t\tcond_resched();\n\n\t\terr = erase_aeb(ubi, aeb, false);\n\t\tif (err)\n\t\t\tgoto out_free;\n\n\t\tfound_pebs++;\n\t}\n\n\tlist_for_each_entry(aeb, &ai->free, u.list) {\n\t\tcond_resched();\n\n\t\te = kmem_cache_alloc(ubi_wl_entry_slab, GFP_KERNEL);\n\t\tif (!e) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\te->pnum = aeb->pnum;\n\t\te->ec = aeb->ec;\n\t\tubi_assert(e->ec >= 0);\n\n\t\twl_tree_add(e, &ubi->free);\n\t\tubi->free_count++;\n\n\t\tubi->lookuptbl[e->pnum] = e;\n\n\t\tfound_pebs++;\n\t}\n\n\tubi_rb_for_each_entry(rb1, av, &ai->volumes, rb) {\n\t\tubi_rb_for_each_entry(rb2, aeb, &av->root, u.rb) {\n\t\t\tcond_resched();\n\n\t\t\te = kmem_cache_alloc(ubi_wl_entry_slab, GFP_KERNEL);\n\t\t\tif (!e) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out_free;\n\t\t\t}\n\n\t\t\te->pnum = aeb->pnum;\n\t\t\te->ec = aeb->ec;\n\t\t\tubi->lookuptbl[e->pnum] = e;\n\n\t\t\tif (!aeb->scrub) {\n\t\t\t\tdbg_wl(\"add PEB %d EC %d to the used tree\",\n\t\t\t\t       e->pnum, e->ec);\n\t\t\t\twl_tree_add(e, &ubi->used);\n\t\t\t} else {\n\t\t\t\tdbg_wl(\"add PEB %d EC %d to the scrub tree\",\n\t\t\t\t       e->pnum, e->ec);\n\t\t\t\twl_tree_add(e, &ubi->scrub);\n\t\t\t}\n\n\t\t\tfound_pebs++;\n\t\t}\n\t}\n\n\tlist_for_each_entry(aeb, &ai->fastmap, u.list) {\n\t\tcond_resched();\n\n\t\te = ubi_find_fm_block(ubi, aeb->pnum);\n\n\t\tif (e) {\n\t\t\tubi_assert(!ubi->lookuptbl[e->pnum]);\n\t\t\tubi->lookuptbl[e->pnum] = e;\n\t\t} else {\n\t\t\tbool sync = false;\n\n\t\t\t \n\t\t\tif (ubi->lookuptbl[aeb->pnum])\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (aeb->vol_id == UBI_FM_SB_VOLUME_ID)\n\t\t\t\tsync = true;\n\n\t\t\terr = erase_aeb(ubi, aeb, sync);\n\t\t\tif (err)\n\t\t\t\tgoto out_free;\n\t\t}\n\n\t\tfound_pebs++;\n\t}\n\n\tdbg_wl(\"found %i PEBs\", found_pebs);\n\n\tubi_assert(ubi->good_peb_count == found_pebs);\n\n\treserved_pebs = WL_RESERVED_PEBS;\n\tubi_fastmap_init(ubi, &reserved_pebs);\n\n\tif (ubi->avail_pebs < reserved_pebs) {\n\t\tubi_err(ubi, \"no enough physical eraseblocks (%d, need %d)\",\n\t\t\tubi->avail_pebs, reserved_pebs);\n\t\tif (ubi->corr_peb_count)\n\t\t\tubi_err(ubi, \"%d PEBs are corrupted and not used\",\n\t\t\t\tubi->corr_peb_count);\n\t\terr = -ENOSPC;\n\t\tgoto out_free;\n\t}\n\tubi->avail_pebs -= reserved_pebs;\n\tubi->rsvd_pebs += reserved_pebs;\n\n\t \n\terr = ensure_wear_leveling(ubi, 0);\n\tif (err)\n\t\tgoto out_free;\n\n#ifdef CONFIG_MTD_UBI_FASTMAP\n\tif (!ubi->ro_mode && !ubi->fm_disabled)\n\t\tubi_ensure_anchor_pebs(ubi);\n#endif\n\treturn 0;\n\nout_free:\n\tshutdown_work(ubi);\n\ttree_destroy(ubi, &ubi->used);\n\ttree_destroy(ubi, &ubi->free);\n\ttree_destroy(ubi, &ubi->scrub);\n\tkfree(ubi->lookuptbl);\n\treturn err;\n}\n\n \nstatic void protection_queue_destroy(struct ubi_device *ubi)\n{\n\tint i;\n\tstruct ubi_wl_entry *e, *tmp;\n\n\tfor (i = 0; i < UBI_PROT_QUEUE_LEN; ++i) {\n\t\tlist_for_each_entry_safe(e, tmp, &ubi->pq[i], u.list) {\n\t\t\tlist_del(&e->u.list);\n\t\t\twl_entry_destroy(ubi, e);\n\t\t}\n\t}\n}\n\n \nvoid ubi_wl_close(struct ubi_device *ubi)\n{\n\tdbg_wl(\"close the WL sub-system\");\n\tubi_fastmap_close(ubi);\n\tshutdown_work(ubi);\n\tprotection_queue_destroy(ubi);\n\ttree_destroy(ubi, &ubi->used);\n\ttree_destroy(ubi, &ubi->erroneous);\n\ttree_destroy(ubi, &ubi->free);\n\ttree_destroy(ubi, &ubi->scrub);\n\tkfree(ubi->lookuptbl);\n}\n\n \nstatic int self_check_ec(struct ubi_device *ubi, int pnum, int ec)\n{\n\tint err;\n\tlong long read_ec;\n\tstruct ubi_ec_hdr *ec_hdr;\n\n\tif (!ubi_dbg_chk_gen(ubi))\n\t\treturn 0;\n\n\tec_hdr = kzalloc(ubi->ec_hdr_alsize, GFP_NOFS);\n\tif (!ec_hdr)\n\t\treturn -ENOMEM;\n\n\terr = ubi_io_read_ec_hdr(ubi, pnum, ec_hdr, 0);\n\tif (err && err != UBI_IO_BITFLIPS) {\n\t\t \n\t\terr = 0;\n\t\tgoto out_free;\n\t}\n\n\tread_ec = be64_to_cpu(ec_hdr->ec);\n\tif (ec != read_ec && read_ec - ec > 1) {\n\t\tubi_err(ubi, \"self-check failed for PEB %d\", pnum);\n\t\tubi_err(ubi, \"read EC is %lld, should be %d\", read_ec, ec);\n\t\tdump_stack();\n\t\terr = 1;\n\t} else\n\t\terr = 0;\n\nout_free:\n\tkfree(ec_hdr);\n\treturn err;\n}\n\n \nstatic int self_check_in_wl_tree(const struct ubi_device *ubi,\n\t\t\t\t struct ubi_wl_entry *e, struct rb_root *root)\n{\n\tif (!ubi_dbg_chk_gen(ubi))\n\t\treturn 0;\n\n\tif (in_wl_tree(e, root))\n\t\treturn 0;\n\n\tubi_err(ubi, \"self-check failed for PEB %d, EC %d, RB-tree %p \",\n\t\te->pnum, e->ec, root);\n\tdump_stack();\n\treturn -EINVAL;\n}\n\n \nstatic int self_check_in_pq(const struct ubi_device *ubi,\n\t\t\t    struct ubi_wl_entry *e)\n{\n\tif (!ubi_dbg_chk_gen(ubi))\n\t\treturn 0;\n\n\tif (in_pq(ubi, e))\n\t\treturn 0;\n\n\tubi_err(ubi, \"self-check failed for PEB %d, EC %d, Protect queue\",\n\t\te->pnum, e->ec);\n\tdump_stack();\n\treturn -EINVAL;\n}\n#ifndef CONFIG_MTD_UBI_FASTMAP\nstatic struct ubi_wl_entry *get_peb_for_wl(struct ubi_device *ubi)\n{\n\tstruct ubi_wl_entry *e;\n\n\te = find_wl_entry(ubi, &ubi->free, WL_FREE_MAX_DIFF);\n\tself_check_in_wl_tree(ubi, e, &ubi->free);\n\tubi->free_count--;\n\tubi_assert(ubi->free_count >= 0);\n\trb_erase(&e->u.rb, &ubi->free);\n\n\treturn e;\n}\n\n \nstatic int produce_free_peb(struct ubi_device *ubi)\n{\n\tint err;\n\n\twhile (!ubi->free.rb_node && ubi->works_count) {\n\t\tspin_unlock(&ubi->wl_lock);\n\n\t\tdbg_wl(\"do one work synchronously\");\n\t\terr = do_work(ubi);\n\n\t\tspin_lock(&ubi->wl_lock);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n \nint ubi_wl_get_peb(struct ubi_device *ubi)\n{\n\tint err;\n\tstruct ubi_wl_entry *e;\n\nretry:\n\tdown_read(&ubi->fm_eba_sem);\n\tspin_lock(&ubi->wl_lock);\n\tif (!ubi->free.rb_node) {\n\t\tif (ubi->works_count == 0) {\n\t\t\tubi_err(ubi, \"no free eraseblocks\");\n\t\t\tubi_assert(list_empty(&ubi->works));\n\t\t\tspin_unlock(&ubi->wl_lock);\n\t\t\treturn -ENOSPC;\n\t\t}\n\n\t\terr = produce_free_peb(ubi);\n\t\tif (err < 0) {\n\t\t\tspin_unlock(&ubi->wl_lock);\n\t\t\treturn err;\n\t\t}\n\t\tspin_unlock(&ubi->wl_lock);\n\t\tup_read(&ubi->fm_eba_sem);\n\t\tgoto retry;\n\n\t}\n\te = wl_get_wle(ubi);\n\tprot_queue_add(ubi, e);\n\tspin_unlock(&ubi->wl_lock);\n\n\terr = ubi_self_check_all_ff(ubi, e->pnum, ubi->vid_hdr_aloffset,\n\t\t\t\t    ubi->peb_size - ubi->vid_hdr_aloffset);\n\tif (err) {\n\t\tubi_err(ubi, \"new PEB %d does not contain all 0xFF bytes\", e->pnum);\n\t\treturn err;\n\t}\n\n\treturn e->pnum;\n}\n#else\n#include \"fastmap-wl.c\"\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}