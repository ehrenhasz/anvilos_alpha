{
  "module_name": "fastmap-wl.c",
  "hash_id": "4639a0e595cbdb6933f68747d9bcf62555ba899e4bc3e4829b3fa342bef657f3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/mtd/ubi/fastmap-wl.c",
  "human_readable_source": "\n \n\n \nstatic void update_fastmap_work_fn(struct work_struct *wrk)\n{\n\tstruct ubi_device *ubi = container_of(wrk, struct ubi_device, fm_work);\n\n\tubi_update_fastmap(ubi);\n\tspin_lock(&ubi->wl_lock);\n\tubi->fm_work_scheduled = 0;\n\tspin_unlock(&ubi->wl_lock);\n}\n\n \nstatic struct ubi_wl_entry *find_anchor_wl_entry(struct rb_root *root)\n{\n\tstruct rb_node *p;\n\tstruct ubi_wl_entry *e, *victim = NULL;\n\tint max_ec = UBI_MAX_ERASECOUNTER;\n\n\tubi_rb_for_each_entry(p, e, root, u.rb) {\n\t\tif (e->pnum < UBI_FM_MAX_START && e->ec < max_ec) {\n\t\t\tvictim = e;\n\t\t\tmax_ec = e->ec;\n\t\t}\n\t}\n\n\treturn victim;\n}\n\nstatic inline void return_unused_peb(struct ubi_device *ubi,\n\t\t\t\t     struct ubi_wl_entry *e)\n{\n\twl_tree_add(e, &ubi->free);\n\tubi->free_count++;\n}\n\n \nstatic void return_unused_pool_pebs(struct ubi_device *ubi,\n\t\t\t\t    struct ubi_fm_pool *pool)\n{\n\tint i;\n\tstruct ubi_wl_entry *e;\n\n\tfor (i = pool->used; i < pool->size; i++) {\n\t\te = ubi->lookuptbl[pool->pebs[i]];\n\t\treturn_unused_peb(ubi, e);\n\t}\n}\n\n \nstruct ubi_wl_entry *ubi_wl_get_fm_peb(struct ubi_device *ubi, int anchor)\n{\n\tstruct ubi_wl_entry *e = NULL;\n\n\tif (!ubi->free.rb_node || (ubi->free_count - ubi->beb_rsvd_pebs < 1))\n\t\tgoto out;\n\n\tif (anchor)\n\t\te = find_anchor_wl_entry(&ubi->free);\n\telse\n\t\te = find_mean_wl_entry(ubi, &ubi->free);\n\n\tif (!e)\n\t\tgoto out;\n\n\tself_check_in_wl_tree(ubi, e, &ubi->free);\n\n\t \n\trb_erase(&e->u.rb, &ubi->free);\n\tubi->free_count--;\nout:\n\treturn e;\n}\n\n \nstatic bool has_enough_free_count(struct ubi_device *ubi, bool is_wl_pool)\n{\n\tint fm_used = 0;\t\n\tint beb_rsvd_pebs;\n\n\tif (!ubi->free.rb_node)\n\t\treturn false;\n\n\tbeb_rsvd_pebs = is_wl_pool ? ubi->beb_rsvd_pebs : 0;\n\tif (ubi->fm_wl_pool.size > 0 && !(ubi->ro_mode || ubi->fm_disabled))\n\t\tfm_used = ubi->fm_size / ubi->leb_size - 1;\n\n\treturn ubi->free_count - beb_rsvd_pebs > fm_used;\n}\n\n \nvoid ubi_refill_pools(struct ubi_device *ubi)\n{\n\tstruct ubi_fm_pool *wl_pool = &ubi->fm_wl_pool;\n\tstruct ubi_fm_pool *pool = &ubi->fm_pool;\n\tstruct ubi_wl_entry *e;\n\tint enough;\n\n\tspin_lock(&ubi->wl_lock);\n\n\treturn_unused_pool_pebs(ubi, wl_pool);\n\treturn_unused_pool_pebs(ubi, pool);\n\n\twl_pool->size = 0;\n\tpool->size = 0;\n\n\tif (ubi->fm_anchor) {\n\t\twl_tree_add(ubi->fm_anchor, &ubi->free);\n\t\tubi->free_count++;\n\t\tubi->fm_anchor = NULL;\n\t}\n\n\tif (!ubi->fm_disabled)\n\t\t \n\t\tubi->fm_anchor = ubi_wl_get_fm_peb(ubi, 1);\n\n\tfor (;;) {\n\t\tenough = 0;\n\t\tif (pool->size < pool->max_size) {\n\t\t\tif (!has_enough_free_count(ubi, false))\n\t\t\t\tbreak;\n\n\t\t\te = wl_get_wle(ubi);\n\t\t\tif (!e)\n\t\t\t\tbreak;\n\n\t\t\tpool->pebs[pool->size] = e->pnum;\n\t\t\tpool->size++;\n\t\t} else\n\t\t\tenough++;\n\n\t\tif (wl_pool->size < wl_pool->max_size) {\n\t\t\tif (!has_enough_free_count(ubi, true))\n\t\t\t\tbreak;\n\n\t\t\te = find_wl_entry(ubi, &ubi->free, WL_FREE_MAX_DIFF);\n\t\t\tself_check_in_wl_tree(ubi, e, &ubi->free);\n\t\t\trb_erase(&e->u.rb, &ubi->free);\n\t\t\tubi->free_count--;\n\n\t\t\twl_pool->pebs[wl_pool->size] = e->pnum;\n\t\t\twl_pool->size++;\n\t\t} else\n\t\t\tenough++;\n\n\t\tif (enough == 2)\n\t\t\tbreak;\n\t}\n\n\twl_pool->used = 0;\n\tpool->used = 0;\n\n\tspin_unlock(&ubi->wl_lock);\n}\n\n \nstatic int produce_free_peb(struct ubi_device *ubi)\n{\n\tint err;\n\n\twhile (!ubi->free.rb_node && ubi->works_count) {\n\t\tdbg_wl(\"do one work synchronously\");\n\t\terr = do_work(ubi);\n\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\n \nint ubi_wl_get_peb(struct ubi_device *ubi)\n{\n\tint ret, attempts = 0;\n\tstruct ubi_fm_pool *pool = &ubi->fm_pool;\n\tstruct ubi_fm_pool *wl_pool = &ubi->fm_wl_pool;\n\nagain:\n\tdown_read(&ubi->fm_eba_sem);\n\tspin_lock(&ubi->wl_lock);\n\n\t \n\tif (pool->used == pool->size || wl_pool->used == wl_pool->size) {\n\t\tspin_unlock(&ubi->wl_lock);\n\t\tup_read(&ubi->fm_eba_sem);\n\t\tret = ubi_update_fastmap(ubi);\n\t\tif (ret) {\n\t\t\tubi_msg(ubi, \"Unable to write a new fastmap: %i\", ret);\n\t\t\tdown_read(&ubi->fm_eba_sem);\n\t\t\treturn -ENOSPC;\n\t\t}\n\t\tdown_read(&ubi->fm_eba_sem);\n\t\tspin_lock(&ubi->wl_lock);\n\t}\n\n\tif (pool->used == pool->size) {\n\t\tspin_unlock(&ubi->wl_lock);\n\t\tattempts++;\n\t\tif (attempts == 10) {\n\t\t\tubi_err(ubi, \"Unable to get a free PEB from user WL pool\");\n\t\t\tret = -ENOSPC;\n\t\t\tgoto out;\n\t\t}\n\t\tup_read(&ubi->fm_eba_sem);\n\t\tret = produce_free_peb(ubi);\n\t\tif (ret < 0) {\n\t\t\tdown_read(&ubi->fm_eba_sem);\n\t\t\tgoto out;\n\t\t}\n\t\tgoto again;\n\t}\n\n\tubi_assert(pool->used < pool->size);\n\tret = pool->pebs[pool->used++];\n\tprot_queue_add(ubi, ubi->lookuptbl[ret]);\n\tspin_unlock(&ubi->wl_lock);\nout:\n\treturn ret;\n}\n\n \nstatic struct ubi_wl_entry *next_peb_for_wl(struct ubi_device *ubi)\n{\n\tstruct ubi_fm_pool *pool = &ubi->fm_wl_pool;\n\tint pnum;\n\n\tif (pool->used == pool->size)\n\t\treturn NULL;\n\n\tpnum = pool->pebs[pool->used];\n\treturn ubi->lookuptbl[pnum];\n}\n\n \nstatic bool need_wear_leveling(struct ubi_device *ubi)\n{\n\tint ec;\n\tstruct ubi_wl_entry *e;\n\n\tif (!ubi->used.rb_node)\n\t\treturn false;\n\n\te = next_peb_for_wl(ubi);\n\tif (!e) {\n\t\tif (!ubi->free.rb_node)\n\t\t\treturn false;\n\t\te = find_wl_entry(ubi, &ubi->free, WL_FREE_MAX_DIFF);\n\t\tec = e->ec;\n\t} else {\n\t\tec = e->ec;\n\t\tif (ubi->free.rb_node) {\n\t\t\te = find_wl_entry(ubi, &ubi->free, WL_FREE_MAX_DIFF);\n\t\t\tec = max(ec, e->ec);\n\t\t}\n\t}\n\te = rb_entry(rb_first(&ubi->used), struct ubi_wl_entry, u.rb);\n\n\treturn ec - e->ec >= UBI_WL_THRESHOLD;\n}\n\n \nstatic struct ubi_wl_entry *get_peb_for_wl(struct ubi_device *ubi)\n{\n\tstruct ubi_fm_pool *pool = &ubi->fm_wl_pool;\n\tint pnum;\n\n\tubi_assert(rwsem_is_locked(&ubi->fm_eba_sem));\n\n\tif (pool->used == pool->size) {\n\t\t \n\t\tif (!ubi->fm_work_scheduled) {\n\t\t\tubi->fm_work_scheduled = 1;\n\t\t\tschedule_work(&ubi->fm_work);\n\t\t}\n\t\treturn NULL;\n\t}\n\n\tpnum = pool->pebs[pool->used++];\n\treturn ubi->lookuptbl[pnum];\n}\n\n \nint ubi_ensure_anchor_pebs(struct ubi_device *ubi)\n{\n\tstruct ubi_work *wrk;\n\tstruct ubi_wl_entry *anchor;\n\n\tspin_lock(&ubi->wl_lock);\n\n\t \n\tif (ubi->fm_anchor) {\n\t\tspin_unlock(&ubi->wl_lock);\n\t\treturn 0;\n\t}\n\n\t \n\tanchor = ubi_wl_get_fm_peb(ubi, 1);\n\tif (anchor) {\n\t\tubi->fm_anchor = anchor;\n\t\tspin_unlock(&ubi->wl_lock);\n\t\treturn 0;\n\t}\n\n\tubi->fm_do_produce_anchor = 1;\n\t \n\tif (ubi->wl_scheduled) {\n\t\tspin_unlock(&ubi->wl_lock);\n\t\treturn 0;\n\t}\n\tubi->wl_scheduled = 1;\n\tspin_unlock(&ubi->wl_lock);\n\n\twrk = kmalloc(sizeof(struct ubi_work), GFP_NOFS);\n\tif (!wrk) {\n\t\tspin_lock(&ubi->wl_lock);\n\t\tubi->wl_scheduled = 0;\n\t\tspin_unlock(&ubi->wl_lock);\n\t\treturn -ENOMEM;\n\t}\n\n\twrk->func = &wear_leveling_worker;\n\t__schedule_ubi_work(ubi, wrk);\n\treturn 0;\n}\n\n \nint ubi_wl_put_fm_peb(struct ubi_device *ubi, struct ubi_wl_entry *fm_e,\n\t\t      int lnum, int torture)\n{\n\tstruct ubi_wl_entry *e;\n\tint vol_id, pnum = fm_e->pnum;\n\n\tdbg_wl(\"PEB %d\", pnum);\n\n\tubi_assert(pnum >= 0);\n\tubi_assert(pnum < ubi->peb_count);\n\n\tspin_lock(&ubi->wl_lock);\n\te = ubi->lookuptbl[pnum];\n\n\t \n\tif (!e) {\n\t\te = fm_e;\n\t\tubi_assert(e->ec >= 0);\n\t\tubi->lookuptbl[pnum] = e;\n\t}\n\n\tspin_unlock(&ubi->wl_lock);\n\n\tvol_id = lnum ? UBI_FM_DATA_VOLUME_ID : UBI_FM_SB_VOLUME_ID;\n\treturn schedule_erase(ubi, e, vol_id, lnum, torture, true);\n}\n\n \nint ubi_is_erase_work(struct ubi_work *wrk)\n{\n\treturn wrk->func == erase_worker;\n}\n\nstatic void ubi_fastmap_close(struct ubi_device *ubi)\n{\n\tint i;\n\n\treturn_unused_pool_pebs(ubi, &ubi->fm_pool);\n\treturn_unused_pool_pebs(ubi, &ubi->fm_wl_pool);\n\n\tif (ubi->fm_anchor) {\n\t\treturn_unused_peb(ubi, ubi->fm_anchor);\n\t\tubi->fm_anchor = NULL;\n\t}\n\n\tif (ubi->fm) {\n\t\tfor (i = 0; i < ubi->fm->used_blocks; i++)\n\t\t\tkfree(ubi->fm->e[i]);\n\t}\n\tkfree(ubi->fm);\n}\n\n \nstatic struct ubi_wl_entry *may_reserve_for_fm(struct ubi_device *ubi,\n\t\t\t\t\t   struct ubi_wl_entry *e,\n\t\t\t\t\t   struct rb_root *root) {\n\tif (e && !ubi->fm_disabled && !ubi->fm &&\n\t    e->pnum < UBI_FM_MAX_START)\n\t\te = rb_entry(rb_next(root->rb_node),\n\t\t\t     struct ubi_wl_entry, u.rb);\n\n\treturn e;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}