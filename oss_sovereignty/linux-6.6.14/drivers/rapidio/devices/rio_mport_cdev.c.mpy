{
  "module_name": "rio_mport_cdev.c",
  "hash_id": "407b85f6c21d6bb56f6c3febe2660b8e2a8d0214046e042c8c746733f50e176a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/rapidio/devices/rio_mport_cdev.c",
  "human_readable_source": "\n \n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/cdev.h>\n#include <linux/ioctl.h>\n#include <linux/uaccess.h>\n#include <linux/list.h>\n#include <linux/fs.h>\n#include <linux/err.h>\n#include <linux/net.h>\n#include <linux/poll.h>\n#include <linux/spinlock.h>\n#include <linux/sched.h>\n#include <linux/kfifo.h>\n\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/mman.h>\n\n#include <linux/dma-mapping.h>\n#ifdef CONFIG_RAPIDIO_DMA_ENGINE\n#include <linux/dmaengine.h>\n#endif\n\n#include <linux/rio.h>\n#include <linux/rio_ids.h>\n#include <linux/rio_drv.h>\n#include <linux/rio_mport_cdev.h>\n\n#include \"../rio.h\"\n\n#define DRV_NAME\t\"rio_mport\"\n#define DRV_PREFIX\tDRV_NAME \": \"\n#define DEV_NAME\t\"rio_mport\"\n#define DRV_VERSION     \"1.0.0\"\n\n \nenum {\n\tDBG_NONE\t= 0,\n\tDBG_INIT\t= BIT(0),  \n\tDBG_EXIT\t= BIT(1),  \n\tDBG_MPORT\t= BIT(2),  \n\tDBG_RDEV\t= BIT(3),  \n\tDBG_DMA\t\t= BIT(4),  \n\tDBG_MMAP\t= BIT(5),  \n\tDBG_IBW\t\t= BIT(6),  \n\tDBG_EVENT\t= BIT(7),  \n\tDBG_OBW\t\t= BIT(8),  \n\tDBG_DBELL\t= BIT(9),  \n\tDBG_ALL\t\t= ~0,\n};\n\n#ifdef DEBUG\n#define rmcd_debug(level, fmt, arg...)\t\t\\\n\tdo {\t\t\t\t\t\\\n\t\tif (DBG_##level & dbg_level)\t\\\n\t\t\tpr_debug(DRV_PREFIX \"%s: \" fmt \"\\n\", __func__, ##arg); \\\n\t} while (0)\n#else\n#define rmcd_debug(level, fmt, arg...) \\\n\t\tno_printk(KERN_DEBUG pr_fmt(DRV_PREFIX fmt \"\\n\"), ##arg)\n#endif\n\n#define rmcd_warn(fmt, arg...) \\\n\tpr_warn(DRV_PREFIX \"%s WARNING \" fmt \"\\n\", __func__, ##arg)\n\n#define rmcd_error(fmt, arg...) \\\n\tpr_err(DRV_PREFIX \"%s ERROR \" fmt \"\\n\", __func__, ##arg)\n\nMODULE_AUTHOR(\"Jerry Jacobs <jerry.jacobs@prodrive-technologies.com>\");\nMODULE_AUTHOR(\"Aurelien Jacquiot <a-jacquiot@ti.com>\");\nMODULE_AUTHOR(\"Alexandre Bounine <alexandre.bounine@idt.com>\");\nMODULE_AUTHOR(\"Andre van Herk <andre.van.herk@prodrive-technologies.com>\");\nMODULE_DESCRIPTION(\"RapidIO mport character device driver\");\nMODULE_LICENSE(\"GPL\");\nMODULE_VERSION(DRV_VERSION);\n\nstatic int dma_timeout = 3000;  \nmodule_param(dma_timeout, int, S_IRUGO);\nMODULE_PARM_DESC(dma_timeout, \"DMA Transfer Timeout in msec (default: 3000)\");\n\n#ifdef DEBUG\nstatic u32 dbg_level = DBG_NONE;\nmodule_param(dbg_level, uint, S_IWUSR | S_IWGRP | S_IRUGO);\nMODULE_PARM_DESC(dbg_level, \"Debugging output level (default 0 = none)\");\n#endif\n\n \nstruct mport_dma_buf {\n\tvoid\t\t*ib_base;\n\tdma_addr_t\tib_phys;\n\tu32\t\tib_size;\n\tu64\t\tib_rio_base;\n\tbool\t\tib_map;\n\tstruct file\t*filp;\n};\n\n \nenum rio_mport_map_dir {\n\tMAP_INBOUND,\n\tMAP_OUTBOUND,\n\tMAP_DMA,\n};\n\nstruct rio_mport_mapping {\n\tstruct list_head node;\n\tstruct mport_dev *md;\n\tenum rio_mport_map_dir dir;\n\tu16 rioid;\n\tu64 rio_addr;\n\tdma_addr_t phys_addr;  \n\tvoid *virt_addr;  \n\tu64 size;\n\tstruct kref ref;  \n\tstruct file *filp;\n};\n\nstruct rio_mport_dma_map {\n\tint valid;\n\tu64 length;\n\tvoid *vaddr;\n\tdma_addr_t paddr;\n};\n\n#define MPORT_MAX_DMA_BUFS\t16\n#define MPORT_EVENT_DEPTH\t10\n\n \nstruct mport_dev {\n\tatomic_t\t\tactive;\n\tstruct list_head\tnode;\n\tstruct cdev\t\tcdev;\n\tstruct device\t\tdev;\n\tstruct rio_mport\t*mport;\n\tstruct mutex\t\tbuf_mutex;\n\tstruct mutex\t\tfile_mutex;\n\tstruct list_head\tfile_list;\n\tstruct rio_mport_properties\tproperties;\n\tstruct list_head\t\tdoorbells;\n\tspinlock_t\t\t\tdb_lock;\n\tstruct list_head\t\tportwrites;\n\tspinlock_t\t\t\tpw_lock;\n\tstruct list_head\tmappings;\n#ifdef CONFIG_RAPIDIO_DMA_ENGINE\n\tstruct dma_chan *dma_chan;\n\tstruct kref\tdma_ref;\n\tstruct completion comp;\n#endif\n};\n\n \nstruct mport_cdev_priv {\n\tstruct mport_dev\t*md;\n\tstruct fasync_struct\t*async_queue;\n\tstruct list_head\tlist;\n\tstruct list_head\tdb_filters;\n\tstruct list_head        pw_filters;\n\tstruct kfifo            event_fifo;\n\twait_queue_head_t       event_rx_wait;\n\tspinlock_t              fifo_lock;\n\tu32\t\t\tevent_mask;  \n#ifdef CONFIG_RAPIDIO_DMA_ENGINE\n\tstruct dma_chan\t\t*dmach;\n\tstruct list_head\tasync_list;\n\tspinlock_t              req_lock;\n\tstruct mutex\t\tdma_lock;\n\tstruct kref\t\tdma_ref;\n\tstruct completion\tcomp;\n#endif\n};\n\n \nstruct rio_mport_pw_filter {\n\tstruct list_head md_node;\n\tstruct list_head priv_node;\n\tstruct mport_cdev_priv *priv;\n\tstruct rio_pw_filter filter;\n};\n\n \nstruct rio_mport_db_filter {\n\tstruct list_head data_node;\n\tstruct list_head priv_node;\n\tstruct mport_cdev_priv *priv;\n\tstruct rio_doorbell_filter filter;\n};\n\nstatic LIST_HEAD(mport_devs);\nstatic DEFINE_MUTEX(mport_devs_lock);\n\n#if (0)  \nstatic DECLARE_WAIT_QUEUE_HEAD(mport_cdev_wait);\n#endif\n\nstatic struct class *dev_class;\nstatic dev_t dev_number;\n\nstatic void mport_release_mapping(struct kref *ref);\n\nstatic int rio_mport_maint_rd(struct mport_cdev_priv *priv, void __user *arg,\n\t\t\t      int local)\n{\n\tstruct rio_mport *mport = priv->md->mport;\n\tstruct rio_mport_maint_io maint_io;\n\tu32 *buffer;\n\tu32 offset;\n\tsize_t length;\n\tint ret, i;\n\n\tif (unlikely(copy_from_user(&maint_io, arg, sizeof(maint_io))))\n\t\treturn -EFAULT;\n\n\tif ((maint_io.offset % 4) ||\n\t    (maint_io.length == 0) || (maint_io.length % 4) ||\n\t    (maint_io.length + maint_io.offset) > RIO_MAINT_SPACE_SZ)\n\t\treturn -EINVAL;\n\n\tbuffer = vmalloc(maint_io.length);\n\tif (buffer == NULL)\n\t\treturn -ENOMEM;\n\tlength = maint_io.length/sizeof(u32);\n\toffset = maint_io.offset;\n\n\tfor (i = 0; i < length; i++) {\n\t\tif (local)\n\t\t\tret = __rio_local_read_config_32(mport,\n\t\t\t\toffset, &buffer[i]);\n\t\telse\n\t\t\tret = rio_mport_read_config_32(mport, maint_io.rioid,\n\t\t\t\tmaint_io.hopcount, offset, &buffer[i]);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\toffset += 4;\n\t}\n\n\tif (unlikely(copy_to_user((void __user *)(uintptr_t)maint_io.buffer,\n\t\t\t\t   buffer, maint_io.length)))\n\t\tret = -EFAULT;\nout:\n\tvfree(buffer);\n\treturn ret;\n}\n\nstatic int rio_mport_maint_wr(struct mport_cdev_priv *priv, void __user *arg,\n\t\t\t      int local)\n{\n\tstruct rio_mport *mport = priv->md->mport;\n\tstruct rio_mport_maint_io maint_io;\n\tu32 *buffer;\n\tu32 offset;\n\tsize_t length;\n\tint ret = -EINVAL, i;\n\n\tif (unlikely(copy_from_user(&maint_io, arg, sizeof(maint_io))))\n\t\treturn -EFAULT;\n\n\tif ((maint_io.offset % 4) ||\n\t    (maint_io.length == 0) || (maint_io.length % 4) ||\n\t    (maint_io.length + maint_io.offset) > RIO_MAINT_SPACE_SZ)\n\t\treturn -EINVAL;\n\n\tbuffer = vmalloc(maint_io.length);\n\tif (buffer == NULL)\n\t\treturn -ENOMEM;\n\tlength = maint_io.length;\n\n\tif (unlikely(copy_from_user(buffer,\n\t\t\t(void __user *)(uintptr_t)maint_io.buffer, length))) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\toffset = maint_io.offset;\n\tlength /= sizeof(u32);\n\n\tfor (i = 0; i < length; i++) {\n\t\tif (local)\n\t\t\tret = __rio_local_write_config_32(mport,\n\t\t\t\t\t\t\t  offset, buffer[i]);\n\t\telse\n\t\t\tret = rio_mport_write_config_32(mport, maint_io.rioid,\n\t\t\t\t\t\t\tmaint_io.hopcount,\n\t\t\t\t\t\t\toffset, buffer[i]);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\toffset += 4;\n\t}\n\nout:\n\tvfree(buffer);\n\treturn ret;\n}\n\n\n \nstatic int\nrio_mport_create_outbound_mapping(struct mport_dev *md, struct file *filp,\n\t\t\t\t  u16 rioid, u64 raddr, u32 size,\n\t\t\t\t  dma_addr_t *paddr)\n{\n\tstruct rio_mport *mport = md->mport;\n\tstruct rio_mport_mapping *map;\n\tint ret;\n\n\trmcd_debug(OBW, \"did=%d ra=0x%llx sz=0x%x\", rioid, raddr, size);\n\n\tmap = kzalloc(sizeof(*map), GFP_KERNEL);\n\tif (map == NULL)\n\t\treturn -ENOMEM;\n\n\tret = rio_map_outb_region(mport, rioid, raddr, size, 0, paddr);\n\tif (ret < 0)\n\t\tgoto err_map_outb;\n\n\tmap->dir = MAP_OUTBOUND;\n\tmap->rioid = rioid;\n\tmap->rio_addr = raddr;\n\tmap->size = size;\n\tmap->phys_addr = *paddr;\n\tmap->filp = filp;\n\tmap->md = md;\n\tkref_init(&map->ref);\n\tlist_add_tail(&map->node, &md->mappings);\n\treturn 0;\nerr_map_outb:\n\tkfree(map);\n\treturn ret;\n}\n\nstatic int\nrio_mport_get_outbound_mapping(struct mport_dev *md, struct file *filp,\n\t\t\t       u16 rioid, u64 raddr, u32 size,\n\t\t\t       dma_addr_t *paddr)\n{\n\tstruct rio_mport_mapping *map;\n\tint err = -ENOMEM;\n\n\tmutex_lock(&md->buf_mutex);\n\tlist_for_each_entry(map, &md->mappings, node) {\n\t\tif (map->dir != MAP_OUTBOUND)\n\t\t\tcontinue;\n\t\tif (rioid == map->rioid &&\n\t\t    raddr == map->rio_addr && size == map->size) {\n\t\t\t*paddr = map->phys_addr;\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t} else if (rioid == map->rioid &&\n\t\t\t   raddr < (map->rio_addr + map->size - 1) &&\n\t\t\t   (raddr + size) > map->rio_addr) {\n\t\t\terr = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (err == -ENOMEM)\n\t\terr = rio_mport_create_outbound_mapping(md, filp, rioid, raddr,\n\t\t\t\t\t\tsize, paddr);\n\tmutex_unlock(&md->buf_mutex);\n\treturn err;\n}\n\nstatic int rio_mport_obw_map(struct file *filp, void __user *arg)\n{\n\tstruct mport_cdev_priv *priv = filp->private_data;\n\tstruct mport_dev *data = priv->md;\n\tstruct rio_mmap map;\n\tdma_addr_t paddr;\n\tint ret;\n\n\tif (unlikely(copy_from_user(&map, arg, sizeof(map))))\n\t\treturn -EFAULT;\n\n\trmcd_debug(OBW, \"did=%d ra=0x%llx sz=0x%llx\",\n\t\t   map.rioid, map.rio_addr, map.length);\n\n\tret = rio_mport_get_outbound_mapping(data, filp, map.rioid,\n\t\t\t\t\t     map.rio_addr, map.length, &paddr);\n\tif (ret < 0) {\n\t\trmcd_error(\"Failed to set OBW err= %d\", ret);\n\t\treturn ret;\n\t}\n\n\tmap.handle = paddr;\n\n\tif (unlikely(copy_to_user(arg, &map, sizeof(map))))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\n \nstatic int rio_mport_obw_free(struct file *filp, void __user *arg)\n{\n\tstruct mport_cdev_priv *priv = filp->private_data;\n\tstruct mport_dev *md = priv->md;\n\tu64 handle;\n\tstruct rio_mport_mapping *map, *_map;\n\n\tif (!md->mport->ops->unmap_outb)\n\t\treturn -EPROTONOSUPPORT;\n\n\tif (copy_from_user(&handle, arg, sizeof(handle)))\n\t\treturn -EFAULT;\n\n\trmcd_debug(OBW, \"h=0x%llx\", handle);\n\n\tmutex_lock(&md->buf_mutex);\n\tlist_for_each_entry_safe(map, _map, &md->mappings, node) {\n\t\tif (map->dir == MAP_OUTBOUND && map->phys_addr == handle) {\n\t\t\tif (map->filp == filp) {\n\t\t\t\trmcd_debug(OBW, \"kref_put h=0x%llx\", handle);\n\t\t\t\tmap->filp = NULL;\n\t\t\t\tkref_put(&map->ref, mport_release_mapping);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&md->buf_mutex);\n\n\treturn 0;\n}\n\n \nstatic int maint_hdid_set(struct mport_cdev_priv *priv, void __user *arg)\n{\n\tstruct mport_dev *md = priv->md;\n\tu16 hdid;\n\n\tif (copy_from_user(&hdid, arg, sizeof(hdid)))\n\t\treturn -EFAULT;\n\n\tmd->mport->host_deviceid = hdid;\n\tmd->properties.hdid = hdid;\n\trio_local_set_device_id(md->mport, hdid);\n\n\trmcd_debug(MPORT, \"Set host device Id to %d\", hdid);\n\n\treturn 0;\n}\n\n \nstatic int maint_comptag_set(struct mport_cdev_priv *priv, void __user *arg)\n{\n\tstruct mport_dev *md = priv->md;\n\tu32 comptag;\n\n\tif (copy_from_user(&comptag, arg, sizeof(comptag)))\n\t\treturn -EFAULT;\n\n\trio_local_write_config_32(md->mport, RIO_COMPONENT_TAG_CSR, comptag);\n\n\trmcd_debug(MPORT, \"Set host Component Tag to %d\", comptag);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_RAPIDIO_DMA_ENGINE\n\nstruct mport_dma_req {\n\tstruct kref refcount;\n\tstruct list_head node;\n\tstruct file *filp;\n\tstruct mport_cdev_priv *priv;\n\tenum rio_transfer_sync sync;\n\tstruct sg_table sgt;\n\tstruct page **page_list;\n\tunsigned int nr_pages;\n\tstruct rio_mport_mapping *map;\n\tstruct dma_chan *dmach;\n\tenum dma_data_direction dir;\n\tdma_cookie_t cookie;\n\tenum dma_status\tstatus;\n\tstruct completion req_comp;\n};\n\nstatic void mport_release_def_dma(struct kref *dma_ref)\n{\n\tstruct mport_dev *md =\n\t\t\tcontainer_of(dma_ref, struct mport_dev, dma_ref);\n\n\trmcd_debug(EXIT, \"DMA_%d\", md->dma_chan->chan_id);\n\trio_release_dma(md->dma_chan);\n\tmd->dma_chan = NULL;\n}\n\nstatic void mport_release_dma(struct kref *dma_ref)\n{\n\tstruct mport_cdev_priv *priv =\n\t\t\tcontainer_of(dma_ref, struct mport_cdev_priv, dma_ref);\n\n\trmcd_debug(EXIT, \"DMA_%d\", priv->dmach->chan_id);\n\tcomplete(&priv->comp);\n}\n\nstatic void dma_req_free(struct kref *ref)\n{\n\tstruct mport_dma_req *req = container_of(ref, struct mport_dma_req,\n\t\t\trefcount);\n\tstruct mport_cdev_priv *priv = req->priv;\n\n\tdma_unmap_sg(req->dmach->device->dev,\n\t\t     req->sgt.sgl, req->sgt.nents, req->dir);\n\tsg_free_table(&req->sgt);\n\tif (req->page_list) {\n\t\tunpin_user_pages(req->page_list, req->nr_pages);\n\t\tkfree(req->page_list);\n\t}\n\n\tif (req->map) {\n\t\tmutex_lock(&req->map->md->buf_mutex);\n\t\tkref_put(&req->map->ref, mport_release_mapping);\n\t\tmutex_unlock(&req->map->md->buf_mutex);\n\t}\n\n\tkref_put(&priv->dma_ref, mport_release_dma);\n\n\tkfree(req);\n}\n\nstatic void dma_xfer_callback(void *param)\n{\n\tstruct mport_dma_req *req = (struct mport_dma_req *)param;\n\tstruct mport_cdev_priv *priv = req->priv;\n\n\treq->status = dma_async_is_tx_complete(priv->dmach, req->cookie,\n\t\t\t\t\t       NULL, NULL);\n\tcomplete(&req->req_comp);\n\tkref_put(&req->refcount, dma_req_free);\n}\n\n \nstatic struct dma_async_tx_descriptor\n*prep_dma_xfer(struct dma_chan *chan, struct rio_transfer_io *transfer,\n\tstruct sg_table *sgt, int nents, enum dma_transfer_direction dir,\n\tenum dma_ctrl_flags flags)\n{\n\tstruct rio_dma_data tx_data;\n\n\ttx_data.sg = sgt->sgl;\n\ttx_data.sg_len = nents;\n\ttx_data.rio_addr_u = 0;\n\ttx_data.rio_addr = transfer->rio_addr;\n\tif (dir == DMA_MEM_TO_DEV) {\n\t\tswitch (transfer->method) {\n\t\tcase RIO_EXCHANGE_NWRITE:\n\t\t\ttx_data.wr_type = RDW_ALL_NWRITE;\n\t\t\tbreak;\n\t\tcase RIO_EXCHANGE_NWRITE_R_ALL:\n\t\t\ttx_data.wr_type = RDW_ALL_NWRITE_R;\n\t\t\tbreak;\n\t\tcase RIO_EXCHANGE_NWRITE_R:\n\t\t\ttx_data.wr_type = RDW_LAST_NWRITE_R;\n\t\t\tbreak;\n\t\tcase RIO_EXCHANGE_DEFAULT:\n\t\t\ttx_data.wr_type = RDW_DEFAULT;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t}\n\n\treturn rio_dma_prep_xfer(chan, transfer->rioid, &tx_data, dir, flags);\n}\n\n \nstatic int get_dma_channel(struct mport_cdev_priv *priv)\n{\n\tmutex_lock(&priv->dma_lock);\n\tif (!priv->dmach) {\n\t\tpriv->dmach = rio_request_mport_dma(priv->md->mport);\n\t\tif (!priv->dmach) {\n\t\t\t \n\t\t\tif (priv->md->dma_chan) {\n\t\t\t\tpriv->dmach = priv->md->dma_chan;\n\t\t\t\tkref_get(&priv->md->dma_ref);\n\t\t\t} else {\n\t\t\t\trmcd_error(\"Failed to get DMA channel\");\n\t\t\t\tmutex_unlock(&priv->dma_lock);\n\t\t\t\treturn -ENODEV;\n\t\t\t}\n\t\t} else if (!priv->md->dma_chan) {\n\t\t\t \n\t\t\tpriv->md->dma_chan = priv->dmach;\n\t\t\tkref_init(&priv->md->dma_ref);\n\t\t\trmcd_debug(DMA, \"Register DMA_chan %d as default\",\n\t\t\t\t   priv->dmach->chan_id);\n\t\t}\n\n\t\tkref_init(&priv->dma_ref);\n\t\tinit_completion(&priv->comp);\n\t}\n\n\tkref_get(&priv->dma_ref);\n\tmutex_unlock(&priv->dma_lock);\n\treturn 0;\n}\n\nstatic void put_dma_channel(struct mport_cdev_priv *priv)\n{\n\tkref_put(&priv->dma_ref, mport_release_dma);\n}\n\n \nstatic int do_dma_request(struct mport_dma_req *req,\n\t\t\t  struct rio_transfer_io *xfer,\n\t\t\t  enum rio_transfer_sync sync, int nents)\n{\n\tstruct mport_cdev_priv *priv;\n\tstruct sg_table *sgt;\n\tstruct dma_chan *chan;\n\tstruct dma_async_tx_descriptor *tx;\n\tdma_cookie_t cookie;\n\tunsigned long tmo = msecs_to_jiffies(dma_timeout);\n\tenum dma_transfer_direction dir;\n\tlong wret;\n\tint ret = 0;\n\n\tpriv = req->priv;\n\tsgt = &req->sgt;\n\n\tchan = priv->dmach;\n\tdir = (req->dir == DMA_FROM_DEVICE) ? DMA_DEV_TO_MEM : DMA_MEM_TO_DEV;\n\n\trmcd_debug(DMA, \"%s(%d) uses %s for DMA_%s\",\n\t\t   current->comm, task_pid_nr(current),\n\t\t   dev_name(&chan->dev->device),\n\t\t   (dir == DMA_DEV_TO_MEM)?\"READ\":\"WRITE\");\n\n\t \n\ttx = prep_dma_xfer(chan, xfer, sgt, nents, dir,\n\t\t\t   DMA_CTRL_ACK | DMA_PREP_INTERRUPT);\n\n\tif (!tx) {\n\t\trmcd_debug(DMA, \"prep error for %s A:0x%llx L:0x%llx\",\n\t\t\t(dir == DMA_DEV_TO_MEM)?\"READ\":\"WRITE\",\n\t\t\txfer->rio_addr, xfer->length);\n\t\tret = -EIO;\n\t\tgoto err_out;\n\t} else if (IS_ERR(tx)) {\n\t\tret = PTR_ERR(tx);\n\t\trmcd_debug(DMA, \"prep error %d for %s A:0x%llx L:0x%llx\", ret,\n\t\t\t(dir == DMA_DEV_TO_MEM)?\"READ\":\"WRITE\",\n\t\t\txfer->rio_addr, xfer->length);\n\t\tgoto err_out;\n\t}\n\n\ttx->callback = dma_xfer_callback;\n\ttx->callback_param = req;\n\n\treq->status = DMA_IN_PROGRESS;\n\tkref_get(&req->refcount);\n\n\tcookie = dmaengine_submit(tx);\n\treq->cookie = cookie;\n\n\trmcd_debug(DMA, \"pid=%d DMA_%s tx_cookie = %d\", task_pid_nr(current),\n\t\t   (dir == DMA_DEV_TO_MEM)?\"READ\":\"WRITE\", cookie);\n\n\tif (dma_submit_error(cookie)) {\n\t\trmcd_error(\"submit err=%d (addr:0x%llx len:0x%llx)\",\n\t\t\t   cookie, xfer->rio_addr, xfer->length);\n\t\tkref_put(&req->refcount, dma_req_free);\n\t\tret = -EIO;\n\t\tgoto err_out;\n\t}\n\n\tdma_async_issue_pending(chan);\n\n\tif (sync == RIO_TRANSFER_ASYNC) {\n\t\tspin_lock(&priv->req_lock);\n\t\tlist_add_tail(&req->node, &priv->async_list);\n\t\tspin_unlock(&priv->req_lock);\n\t\treturn cookie;\n\t} else if (sync == RIO_TRANSFER_FAF)\n\t\treturn 0;\n\n\twret = wait_for_completion_interruptible_timeout(&req->req_comp, tmo);\n\n\tif (wret == 0) {\n\t\t \n\t\trmcd_error(\"%s(%d) timed out waiting for DMA_%s %d\",\n\t\t       current->comm, task_pid_nr(current),\n\t\t       (dir == DMA_DEV_TO_MEM)?\"READ\":\"WRITE\", cookie);\n\t\treturn -ETIMEDOUT;\n\t} else if (wret == -ERESTARTSYS) {\n\t\t \n\t\trmcd_error(\"%s(%d) wait for DMA_%s %d was interrupted\",\n\t\t\tcurrent->comm, task_pid_nr(current),\n\t\t\t(dir == DMA_DEV_TO_MEM)?\"READ\":\"WRITE\", cookie);\n\t\treturn -EINTR;\n\t}\n\n\tif (req->status != DMA_COMPLETE) {\n\t\t \n\t\trmcd_error(\"%s(%d) DMA_%s %d completed with status %d (ret=%d)\",\n\t\t\tcurrent->comm, task_pid_nr(current),\n\t\t\t(dir == DMA_DEV_TO_MEM)?\"READ\":\"WRITE\",\n\t\t\tcookie, req->status, ret);\n\t\tret = -EIO;\n\t}\n\nerr_out:\n\treturn ret;\n}\n\n \nstatic int\nrio_dma_transfer(struct file *filp, u32 transfer_mode,\n\t\t enum rio_transfer_sync sync, enum dma_data_direction dir,\n\t\t struct rio_transfer_io *xfer)\n{\n\tstruct mport_cdev_priv *priv = filp->private_data;\n\tunsigned long nr_pages = 0;\n\tstruct page **page_list = NULL;\n\tstruct mport_dma_req *req;\n\tstruct mport_dev *md = priv->md;\n\tstruct dma_chan *chan;\n\tint ret;\n\tint nents;\n\n\tif (xfer->length == 0)\n\t\treturn -EINVAL;\n\treq = kzalloc(sizeof(*req), GFP_KERNEL);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\tret = get_dma_channel(priv);\n\tif (ret) {\n\t\tkfree(req);\n\t\treturn ret;\n\t}\n\tchan = priv->dmach;\n\n\tkref_init(&req->refcount);\n\tinit_completion(&req->req_comp);\n\treq->dir = dir;\n\treq->filp = filp;\n\treq->priv = priv;\n\treq->dmach = chan;\n\treq->sync = sync;\n\n\t \n\tif (xfer->loc_addr) {\n\t\tunsigned int offset;\n\t\tlong pinned;\n\n\t\toffset = lower_32_bits(offset_in_page(xfer->loc_addr));\n\t\tnr_pages = PAGE_ALIGN(xfer->length + offset) >> PAGE_SHIFT;\n\n\t\tpage_list = kmalloc_array(nr_pages,\n\t\t\t\t\t  sizeof(*page_list), GFP_KERNEL);\n\t\tif (page_list == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_req;\n\t\t}\n\n\t\tpinned = pin_user_pages_fast(\n\t\t\t\t(unsigned long)xfer->loc_addr & PAGE_MASK,\n\t\t\t\tnr_pages,\n\t\t\t\tdir == DMA_FROM_DEVICE ? FOLL_WRITE : 0,\n\t\t\t\tpage_list);\n\n\t\tif (pinned != nr_pages) {\n\t\t\tif (pinned < 0) {\n\t\t\t\trmcd_error(\"pin_user_pages_fast err=%ld\",\n\t\t\t\t\t   pinned);\n\t\t\t\tnr_pages = 0;\n\t\t\t} else {\n\t\t\t\trmcd_error(\"pinned %ld out of %ld pages\",\n\t\t\t\t\t   pinned, nr_pages);\n\t\t\t\t \n\t\t\t\tnr_pages = pinned;\n\t\t\t}\n\t\t\tret = -EFAULT;\n\t\t\tgoto err_pg;\n\t\t}\n\n\t\tret = sg_alloc_table_from_pages(&req->sgt, page_list, nr_pages,\n\t\t\t\t\toffset, xfer->length, GFP_KERNEL);\n\t\tif (ret) {\n\t\t\trmcd_error(\"sg_alloc_table failed with err=%d\", ret);\n\t\t\tgoto err_pg;\n\t\t}\n\n\t\treq->page_list = page_list;\n\t\treq->nr_pages = nr_pages;\n\t} else {\n\t\tdma_addr_t baddr;\n\t\tstruct rio_mport_mapping *map;\n\n\t\tbaddr = (dma_addr_t)xfer->handle;\n\n\t\tmutex_lock(&md->buf_mutex);\n\t\tlist_for_each_entry(map, &md->mappings, node) {\n\t\t\tif (baddr >= map->phys_addr &&\n\t\t\t    baddr < (map->phys_addr + map->size)) {\n\t\t\t\tkref_get(&map->ref);\n\t\t\t\treq->map = map;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tmutex_unlock(&md->buf_mutex);\n\n\t\tif (req->map == NULL) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_req;\n\t\t}\n\n\t\tif (xfer->length + xfer->offset > req->map->size) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto err_req;\n\t\t}\n\n\t\tret = sg_alloc_table(&req->sgt, 1, GFP_KERNEL);\n\t\tif (unlikely(ret)) {\n\t\t\trmcd_error(\"sg_alloc_table failed for internal buf\");\n\t\t\tgoto err_req;\n\t\t}\n\n\t\tsg_set_buf(req->sgt.sgl,\n\t\t\t   req->map->virt_addr + (baddr - req->map->phys_addr) +\n\t\t\t\txfer->offset, xfer->length);\n\t}\n\n\tnents = dma_map_sg(chan->device->dev,\n\t\t\t   req->sgt.sgl, req->sgt.nents, dir);\n\tif (nents == 0) {\n\t\trmcd_error(\"Failed to map SG list\");\n\t\tret = -EFAULT;\n\t\tgoto err_pg;\n\t}\n\n\tret = do_dma_request(req, xfer, sync, nents);\n\n\tif (ret >= 0) {\n\t\tif (sync == RIO_TRANSFER_ASYNC)\n\t\t\treturn ret;  \n\t} else {\n\t\trmcd_debug(DMA, \"do_dma_request failed with err=%d\", ret);\n\t}\n\nerr_pg:\n\tif (!req->page_list) {\n\t\tunpin_user_pages(page_list, nr_pages);\n\t\tkfree(page_list);\n\t}\nerr_req:\n\tkref_put(&req->refcount, dma_req_free);\n\treturn ret;\n}\n\nstatic int rio_mport_transfer_ioctl(struct file *filp, void __user *arg)\n{\n\tstruct mport_cdev_priv *priv = filp->private_data;\n\tstruct rio_transaction transaction;\n\tstruct rio_transfer_io *transfer;\n\tenum dma_data_direction dir;\n\tint i, ret = 0;\n\tsize_t size;\n\n\tif (unlikely(copy_from_user(&transaction, arg, sizeof(transaction))))\n\t\treturn -EFAULT;\n\n\tif (transaction.count != 1)  \n\t\treturn -EINVAL;\n\n\tif ((transaction.transfer_mode &\n\t     priv->md->properties.transfer_mode) == 0)\n\t\treturn -ENODEV;\n\n\tsize = array_size(sizeof(*transfer), transaction.count);\n\ttransfer = vmalloc(size);\n\tif (!transfer)\n\t\treturn -ENOMEM;\n\n\tif (unlikely(copy_from_user(transfer,\n\t\t\t\t    (void __user *)(uintptr_t)transaction.block,\n\t\t\t\t    size))) {\n\t\tret = -EFAULT;\n\t\tgoto out_free;\n\t}\n\n\tdir = (transaction.dir == RIO_TRANSFER_DIR_READ) ?\n\t\t\t\t\tDMA_FROM_DEVICE : DMA_TO_DEVICE;\n\tfor (i = 0; i < transaction.count && ret == 0; i++)\n\t\tret = rio_dma_transfer(filp, transaction.transfer_mode,\n\t\t\ttransaction.sync, dir, &transfer[i]);\n\n\tif (unlikely(copy_to_user((void __user *)(uintptr_t)transaction.block,\n\t\t\t\t  transfer, size)))\n\t\tret = -EFAULT;\n\nout_free:\n\tvfree(transfer);\n\n\treturn ret;\n}\n\nstatic int rio_mport_wait_for_async_dma(struct file *filp, void __user *arg)\n{\n\tstruct mport_cdev_priv *priv;\n\tstruct rio_async_tx_wait w_param;\n\tstruct mport_dma_req *req;\n\tdma_cookie_t cookie;\n\tunsigned long tmo;\n\tlong wret;\n\tint found = 0;\n\tint ret;\n\n\tpriv = (struct mport_cdev_priv *)filp->private_data;\n\n\tif (unlikely(copy_from_user(&w_param, arg, sizeof(w_param))))\n\t\treturn -EFAULT;\n\n\tcookie = w_param.token;\n\tif (w_param.timeout)\n\t\ttmo = msecs_to_jiffies(w_param.timeout);\n\telse  \n\t\ttmo = msecs_to_jiffies(dma_timeout);\n\n\tspin_lock(&priv->req_lock);\n\tlist_for_each_entry(req, &priv->async_list, node) {\n\t\tif (req->cookie == cookie) {\n\t\t\tlist_del(&req->node);\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&priv->req_lock);\n\n\tif (!found)\n\t\treturn -EAGAIN;\n\n\twret = wait_for_completion_interruptible_timeout(&req->req_comp, tmo);\n\n\tif (wret == 0) {\n\t\t \n\t\trmcd_error(\"%s(%d) timed out waiting for ASYNC DMA_%s\",\n\t\t       current->comm, task_pid_nr(current),\n\t\t       (req->dir == DMA_FROM_DEVICE)?\"READ\":\"WRITE\");\n\t\tret = -ETIMEDOUT;\n\t\tgoto err_tmo;\n\t} else if (wret == -ERESTARTSYS) {\n\t\t \n\t\trmcd_error(\"%s(%d) wait for ASYNC DMA_%s was interrupted\",\n\t\t\tcurrent->comm, task_pid_nr(current),\n\t\t\t(req->dir == DMA_FROM_DEVICE)?\"READ\":\"WRITE\");\n\t\tret = -EINTR;\n\t\tgoto err_tmo;\n\t}\n\n\tif (req->status != DMA_COMPLETE) {\n\t\t \n\t\trmcd_error(\"%s(%d) ASYNC DMA_%s completion with status %d\",\n\t\t\tcurrent->comm, task_pid_nr(current),\n\t\t\t(req->dir == DMA_FROM_DEVICE)?\"READ\":\"WRITE\",\n\t\t\treq->status);\n\t\tret = -EIO;\n\t} else\n\t\tret = 0;\n\n\tif (req->status != DMA_IN_PROGRESS && req->status != DMA_PAUSED)\n\t\tkref_put(&req->refcount, dma_req_free);\n\n\treturn ret;\n\nerr_tmo:\n\t \n\tspin_lock(&priv->req_lock);\n\tlist_add_tail(&req->node, &priv->async_list);\n\tspin_unlock(&priv->req_lock);\n\treturn ret;\n}\n\nstatic int rio_mport_create_dma_mapping(struct mport_dev *md, struct file *filp,\n\t\t\tu64 size, struct rio_mport_mapping **mapping)\n{\n\tstruct rio_mport_mapping *map;\n\n\tmap = kzalloc(sizeof(*map), GFP_KERNEL);\n\tif (map == NULL)\n\t\treturn -ENOMEM;\n\n\tmap->virt_addr = dma_alloc_coherent(md->mport->dev.parent, size,\n\t\t\t\t\t    &map->phys_addr, GFP_KERNEL);\n\tif (map->virt_addr == NULL) {\n\t\tkfree(map);\n\t\treturn -ENOMEM;\n\t}\n\n\tmap->dir = MAP_DMA;\n\tmap->size = size;\n\tmap->filp = filp;\n\tmap->md = md;\n\tkref_init(&map->ref);\n\tmutex_lock(&md->buf_mutex);\n\tlist_add_tail(&map->node, &md->mappings);\n\tmutex_unlock(&md->buf_mutex);\n\t*mapping = map;\n\n\treturn 0;\n}\n\nstatic int rio_mport_alloc_dma(struct file *filp, void __user *arg)\n{\n\tstruct mport_cdev_priv *priv = filp->private_data;\n\tstruct mport_dev *md = priv->md;\n\tstruct rio_dma_mem map;\n\tstruct rio_mport_mapping *mapping = NULL;\n\tint ret;\n\n\tif (unlikely(copy_from_user(&map, arg, sizeof(map))))\n\t\treturn -EFAULT;\n\n\tret = rio_mport_create_dma_mapping(md, filp, map.length, &mapping);\n\tif (ret)\n\t\treturn ret;\n\n\tmap.dma_handle = mapping->phys_addr;\n\n\tif (unlikely(copy_to_user(arg, &map, sizeof(map)))) {\n\t\tmutex_lock(&md->buf_mutex);\n\t\tkref_put(&mapping->ref, mport_release_mapping);\n\t\tmutex_unlock(&md->buf_mutex);\n\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\nstatic int rio_mport_free_dma(struct file *filp, void __user *arg)\n{\n\tstruct mport_cdev_priv *priv = filp->private_data;\n\tstruct mport_dev *md = priv->md;\n\tu64 handle;\n\tint ret = -EFAULT;\n\tstruct rio_mport_mapping *map, *_map;\n\n\tif (copy_from_user(&handle, arg, sizeof(handle)))\n\t\treturn -EFAULT;\n\trmcd_debug(EXIT, \"filp=%p\", filp);\n\n\tmutex_lock(&md->buf_mutex);\n\tlist_for_each_entry_safe(map, _map, &md->mappings, node) {\n\t\tif (map->dir == MAP_DMA && map->phys_addr == handle &&\n\t\t    map->filp == filp) {\n\t\t\tkref_put(&map->ref, mport_release_mapping);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&md->buf_mutex);\n\n\tif (ret == -EFAULT) {\n\t\trmcd_debug(DMA, \"ERR no matching mapping\");\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n#else\nstatic int rio_mport_transfer_ioctl(struct file *filp, void *arg)\n{\n\treturn -ENODEV;\n}\n\nstatic int rio_mport_wait_for_async_dma(struct file *filp, void __user *arg)\n{\n\treturn -ENODEV;\n}\n\nstatic int rio_mport_alloc_dma(struct file *filp, void __user *arg)\n{\n\treturn -ENODEV;\n}\n\nstatic int rio_mport_free_dma(struct file *filp, void __user *arg)\n{\n\treturn -ENODEV;\n}\n#endif  \n\n \n\nstatic int\nrio_mport_create_inbound_mapping(struct mport_dev *md, struct file *filp,\n\t\t\t\tu64 raddr, u64 size,\n\t\t\t\tstruct rio_mport_mapping **mapping)\n{\n\tstruct rio_mport *mport = md->mport;\n\tstruct rio_mport_mapping *map;\n\tint ret;\n\n\t \n\tif (size > 0xffffffff)\n\t\treturn -EINVAL;\n\n\tmap = kzalloc(sizeof(*map), GFP_KERNEL);\n\tif (map == NULL)\n\t\treturn -ENOMEM;\n\n\tmap->virt_addr = dma_alloc_coherent(mport->dev.parent, size,\n\t\t\t\t\t    &map->phys_addr, GFP_KERNEL);\n\tif (map->virt_addr == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto err_dma_alloc;\n\t}\n\n\tif (raddr == RIO_MAP_ANY_ADDR)\n\t\traddr = map->phys_addr;\n\tret = rio_map_inb_region(mport, map->phys_addr, raddr, (u32)size, 0);\n\tif (ret < 0)\n\t\tgoto err_map_inb;\n\n\tmap->dir = MAP_INBOUND;\n\tmap->rio_addr = raddr;\n\tmap->size = size;\n\tmap->filp = filp;\n\tmap->md = md;\n\tkref_init(&map->ref);\n\tmutex_lock(&md->buf_mutex);\n\tlist_add_tail(&map->node, &md->mappings);\n\tmutex_unlock(&md->buf_mutex);\n\t*mapping = map;\n\treturn 0;\n\nerr_map_inb:\n\tdma_free_coherent(mport->dev.parent, size,\n\t\t\t  map->virt_addr, map->phys_addr);\nerr_dma_alloc:\n\tkfree(map);\n\treturn ret;\n}\n\nstatic int\nrio_mport_get_inbound_mapping(struct mport_dev *md, struct file *filp,\n\t\t\t      u64 raddr, u64 size,\n\t\t\t      struct rio_mport_mapping **mapping)\n{\n\tstruct rio_mport_mapping *map;\n\tint err = -ENOMEM;\n\n\tif (raddr == RIO_MAP_ANY_ADDR)\n\t\tgoto get_new;\n\n\tmutex_lock(&md->buf_mutex);\n\tlist_for_each_entry(map, &md->mappings, node) {\n\t\tif (map->dir != MAP_INBOUND)\n\t\t\tcontinue;\n\t\tif (raddr == map->rio_addr && size == map->size) {\n\t\t\t \n\t\t\t*mapping = map;\n\t\t\terr = 0;\n\t\t\tbreak;\n\t\t} else if (raddr < (map->rio_addr + map->size - 1) &&\n\t\t\t   (raddr + size) > map->rio_addr) {\n\t\t\terr = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&md->buf_mutex);\n\n\tif (err != -ENOMEM)\n\t\treturn err;\nget_new:\n\t \n\treturn rio_mport_create_inbound_mapping(md, filp, raddr, size, mapping);\n}\n\nstatic int rio_mport_map_inbound(struct file *filp, void __user *arg)\n{\n\tstruct mport_cdev_priv *priv = filp->private_data;\n\tstruct mport_dev *md = priv->md;\n\tstruct rio_mmap map;\n\tstruct rio_mport_mapping *mapping = NULL;\n\tint ret;\n\n\tif (!md->mport->ops->map_inb)\n\t\treturn -EPROTONOSUPPORT;\n\tif (unlikely(copy_from_user(&map, arg, sizeof(map))))\n\t\treturn -EFAULT;\n\n\trmcd_debug(IBW, \"%s filp=%p\", dev_name(&priv->md->dev), filp);\n\n\tret = rio_mport_get_inbound_mapping(md, filp, map.rio_addr,\n\t\t\t\t\t    map.length, &mapping);\n\tif (ret)\n\t\treturn ret;\n\n\tmap.handle = mapping->phys_addr;\n\tmap.rio_addr = mapping->rio_addr;\n\n\tif (unlikely(copy_to_user(arg, &map, sizeof(map)))) {\n\t\t \n\t\tif (ret == 0 && mapping->filp == filp) {\n\t\t\tmutex_lock(&md->buf_mutex);\n\t\t\tkref_put(&mapping->ref, mport_release_mapping);\n\t\t\tmutex_unlock(&md->buf_mutex);\n\t\t}\n\t\treturn -EFAULT;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int rio_mport_inbound_free(struct file *filp, void __user *arg)\n{\n\tstruct mport_cdev_priv *priv = filp->private_data;\n\tstruct mport_dev *md = priv->md;\n\tu64 handle;\n\tstruct rio_mport_mapping *map, *_map;\n\n\trmcd_debug(IBW, \"%s filp=%p\", dev_name(&priv->md->dev), filp);\n\n\tif (!md->mport->ops->unmap_inb)\n\t\treturn -EPROTONOSUPPORT;\n\n\tif (copy_from_user(&handle, arg, sizeof(handle)))\n\t\treturn -EFAULT;\n\n\tmutex_lock(&md->buf_mutex);\n\tlist_for_each_entry_safe(map, _map, &md->mappings, node) {\n\t\tif (map->dir == MAP_INBOUND && map->phys_addr == handle) {\n\t\t\tif (map->filp == filp) {\n\t\t\t\tmap->filp = NULL;\n\t\t\t\tkref_put(&map->ref, mport_release_mapping);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&md->buf_mutex);\n\n\treturn 0;\n}\n\n \nstatic int maint_port_idx_get(struct mport_cdev_priv *priv, void __user *arg)\n{\n\tstruct mport_dev *md = priv->md;\n\tu32 port_idx = md->mport->index;\n\n\trmcd_debug(MPORT, \"port_index=%d\", port_idx);\n\n\tif (copy_to_user(arg, &port_idx, sizeof(port_idx)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int rio_mport_add_event(struct mport_cdev_priv *priv,\n\t\t\t       struct rio_event *event)\n{\n\tint overflow;\n\n\tif (!(priv->event_mask & event->header))\n\t\treturn -EACCES;\n\n\tspin_lock(&priv->fifo_lock);\n\toverflow = kfifo_avail(&priv->event_fifo) < sizeof(*event)\n\t\t|| kfifo_in(&priv->event_fifo, (unsigned char *)event,\n\t\t\tsizeof(*event)) != sizeof(*event);\n\tspin_unlock(&priv->fifo_lock);\n\n\twake_up_interruptible(&priv->event_rx_wait);\n\n\tif (overflow) {\n\t\tdev_warn(&priv->md->dev, DRV_NAME \": event fifo overflow\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\treturn 0;\n}\n\nstatic void rio_mport_doorbell_handler(struct rio_mport *mport, void *dev_id,\n\t\t\t\t       u16 src, u16 dst, u16 info)\n{\n\tstruct mport_dev *data = dev_id;\n\tstruct mport_cdev_priv *priv;\n\tstruct rio_mport_db_filter *db_filter;\n\tstruct rio_event event;\n\tint handled;\n\n\tevent.header = RIO_DOORBELL;\n\tevent.u.doorbell.rioid = src;\n\tevent.u.doorbell.payload = info;\n\n\thandled = 0;\n\tspin_lock(&data->db_lock);\n\tlist_for_each_entry(db_filter, &data->doorbells, data_node) {\n\t\tif (((db_filter->filter.rioid == RIO_INVALID_DESTID ||\n\t\t      db_filter->filter.rioid == src)) &&\n\t\t      info >= db_filter->filter.low &&\n\t\t      info <= db_filter->filter.high) {\n\t\t\tpriv = db_filter->priv;\n\t\t\trio_mport_add_event(priv, &event);\n\t\t\thandled = 1;\n\t\t}\n\t}\n\tspin_unlock(&data->db_lock);\n\n\tif (!handled)\n\t\tdev_warn(&data->dev,\n\t\t\t\"%s: spurious DB received from 0x%x, info=0x%04x\\n\",\n\t\t\t__func__, src, info);\n}\n\nstatic int rio_mport_add_db_filter(struct mport_cdev_priv *priv,\n\t\t\t\t   void __user *arg)\n{\n\tstruct mport_dev *md = priv->md;\n\tstruct rio_mport_db_filter *db_filter;\n\tstruct rio_doorbell_filter filter;\n\tunsigned long flags;\n\tint ret;\n\n\tif (copy_from_user(&filter, arg, sizeof(filter)))\n\t\treturn -EFAULT;\n\n\tif (filter.low > filter.high)\n\t\treturn -EINVAL;\n\n\tret = rio_request_inb_dbell(md->mport, md, filter.low, filter.high,\n\t\t\t\t    rio_mport_doorbell_handler);\n\tif (ret) {\n\t\trmcd_error(\"%s failed to register IBDB, err=%d\",\n\t\t\t   dev_name(&md->dev), ret);\n\t\treturn ret;\n\t}\n\n\tdb_filter = kzalloc(sizeof(*db_filter), GFP_KERNEL);\n\tif (db_filter == NULL) {\n\t\trio_release_inb_dbell(md->mport, filter.low, filter.high);\n\t\treturn -ENOMEM;\n\t}\n\n\tdb_filter->filter = filter;\n\tdb_filter->priv = priv;\n\tspin_lock_irqsave(&md->db_lock, flags);\n\tlist_add_tail(&db_filter->priv_node, &priv->db_filters);\n\tlist_add_tail(&db_filter->data_node, &md->doorbells);\n\tspin_unlock_irqrestore(&md->db_lock, flags);\n\n\treturn 0;\n}\n\nstatic void rio_mport_delete_db_filter(struct rio_mport_db_filter *db_filter)\n{\n\tlist_del(&db_filter->data_node);\n\tlist_del(&db_filter->priv_node);\n\tkfree(db_filter);\n}\n\nstatic int rio_mport_remove_db_filter(struct mport_cdev_priv *priv,\n\t\t\t\t      void __user *arg)\n{\n\tstruct rio_mport_db_filter *db_filter;\n\tstruct rio_doorbell_filter filter;\n\tunsigned long flags;\n\tint ret = -EINVAL;\n\n\tif (copy_from_user(&filter, arg, sizeof(filter)))\n\t\treturn -EFAULT;\n\n\tif (filter.low > filter.high)\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&priv->md->db_lock, flags);\n\tlist_for_each_entry(db_filter, &priv->db_filters, priv_node) {\n\t\tif (db_filter->filter.rioid == filter.rioid &&\n\t\t    db_filter->filter.low == filter.low &&\n\t\t    db_filter->filter.high == filter.high) {\n\t\t\trio_mport_delete_db_filter(db_filter);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&priv->md->db_lock, flags);\n\n\tif (!ret)\n\t\trio_release_inb_dbell(priv->md->mport, filter.low, filter.high);\n\n\treturn ret;\n}\n\nstatic int rio_mport_match_pw(union rio_pw_msg *msg,\n\t\t\t      struct rio_pw_filter *filter)\n{\n\tif ((msg->em.comptag & filter->mask) < filter->low ||\n\t\t(msg->em.comptag & filter->mask) > filter->high)\n\t\treturn 0;\n\treturn 1;\n}\n\nstatic int rio_mport_pw_handler(struct rio_mport *mport, void *context,\n\t\t\t\tunion rio_pw_msg *msg, int step)\n{\n\tstruct mport_dev *md = context;\n\tstruct mport_cdev_priv *priv;\n\tstruct rio_mport_pw_filter *pw_filter;\n\tstruct rio_event event;\n\tint handled;\n\n\tevent.header = RIO_PORTWRITE;\n\tmemcpy(event.u.portwrite.payload, msg->raw, RIO_PW_MSG_SIZE);\n\n\thandled = 0;\n\tspin_lock(&md->pw_lock);\n\tlist_for_each_entry(pw_filter, &md->portwrites, md_node) {\n\t\tif (rio_mport_match_pw(msg, &pw_filter->filter)) {\n\t\t\tpriv = pw_filter->priv;\n\t\t\trio_mport_add_event(priv, &event);\n\t\t\thandled = 1;\n\t\t}\n\t}\n\tspin_unlock(&md->pw_lock);\n\n\tif (!handled) {\n\t\tprintk_ratelimited(KERN_WARNING DRV_NAME\n\t\t\t\": mport%d received spurious PW from 0x%08x\\n\",\n\t\t\tmport->id, msg->em.comptag);\n\t}\n\n\treturn 0;\n}\n\nstatic int rio_mport_add_pw_filter(struct mport_cdev_priv *priv,\n\t\t\t\t   void __user *arg)\n{\n\tstruct mport_dev *md = priv->md;\n\tstruct rio_mport_pw_filter *pw_filter;\n\tstruct rio_pw_filter filter;\n\tunsigned long flags;\n\tint hadd = 0;\n\n\tif (copy_from_user(&filter, arg, sizeof(filter)))\n\t\treturn -EFAULT;\n\n\tpw_filter = kzalloc(sizeof(*pw_filter), GFP_KERNEL);\n\tif (pw_filter == NULL)\n\t\treturn -ENOMEM;\n\n\tpw_filter->filter = filter;\n\tpw_filter->priv = priv;\n\tspin_lock_irqsave(&md->pw_lock, flags);\n\tif (list_empty(&md->portwrites))\n\t\thadd = 1;\n\tlist_add_tail(&pw_filter->priv_node, &priv->pw_filters);\n\tlist_add_tail(&pw_filter->md_node, &md->portwrites);\n\tspin_unlock_irqrestore(&md->pw_lock, flags);\n\n\tif (hadd) {\n\t\tint ret;\n\n\t\tret = rio_add_mport_pw_handler(md->mport, md,\n\t\t\t\t\t       rio_mport_pw_handler);\n\t\tif (ret) {\n\t\t\tdev_err(&md->dev,\n\t\t\t\t\"%s: failed to add IB_PW handler, err=%d\\n\",\n\t\t\t\t__func__, ret);\n\t\t\treturn ret;\n\t\t}\n\t\trio_pw_enable(md->mport, 1);\n\t}\n\n\treturn 0;\n}\n\nstatic void rio_mport_delete_pw_filter(struct rio_mport_pw_filter *pw_filter)\n{\n\tlist_del(&pw_filter->md_node);\n\tlist_del(&pw_filter->priv_node);\n\tkfree(pw_filter);\n}\n\nstatic int rio_mport_match_pw_filter(struct rio_pw_filter *a,\n\t\t\t\t     struct rio_pw_filter *b)\n{\n\tif ((a->mask == b->mask) && (a->low == b->low) && (a->high == b->high))\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic int rio_mport_remove_pw_filter(struct mport_cdev_priv *priv,\n\t\t\t\t      void __user *arg)\n{\n\tstruct mport_dev *md = priv->md;\n\tstruct rio_mport_pw_filter *pw_filter;\n\tstruct rio_pw_filter filter;\n\tunsigned long flags;\n\tint ret = -EINVAL;\n\tint hdel = 0;\n\n\tif (copy_from_user(&filter, arg, sizeof(filter)))\n\t\treturn -EFAULT;\n\n\tspin_lock_irqsave(&md->pw_lock, flags);\n\tlist_for_each_entry(pw_filter, &priv->pw_filters, priv_node) {\n\t\tif (rio_mport_match_pw_filter(&pw_filter->filter, &filter)) {\n\t\t\trio_mport_delete_pw_filter(pw_filter);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (list_empty(&md->portwrites))\n\t\thdel = 1;\n\tspin_unlock_irqrestore(&md->pw_lock, flags);\n\n\tif (hdel) {\n\t\trio_del_mport_pw_handler(md->mport, priv->md,\n\t\t\t\t\t rio_mport_pw_handler);\n\t\trio_pw_enable(md->mport, 0);\n\t}\n\n\treturn ret;\n}\n\n \nstatic void rio_release_dev(struct device *dev)\n{\n\tstruct rio_dev *rdev;\n\n\trdev = to_rio_dev(dev);\n\tpr_info(DRV_PREFIX \"%s: %s\\n\", __func__, rio_name(rdev));\n\tkfree(rdev);\n}\n\n\nstatic void rio_release_net(struct device *dev)\n{\n\tstruct rio_net *net;\n\n\tnet = to_rio_net(dev);\n\trmcd_debug(RDEV, \"net_%d\", net->id);\n\tkfree(net);\n}\n\n\n \nstatic int rio_mport_add_riodev(struct mport_cdev_priv *priv,\n\t\t\t\t   void __user *arg)\n{\n\tstruct mport_dev *md = priv->md;\n\tstruct rio_rdev_info dev_info;\n\tstruct rio_dev *rdev;\n\tstruct rio_switch *rswitch = NULL;\n\tstruct rio_mport *mport;\n\tstruct device *dev;\n\tsize_t size;\n\tu32 rval;\n\tu32 swpinfo = 0;\n\tu16 destid;\n\tu8 hopcount;\n\tint err;\n\n\tif (copy_from_user(&dev_info, arg, sizeof(dev_info)))\n\t\treturn -EFAULT;\n\tdev_info.name[sizeof(dev_info.name) - 1] = '\\0';\n\n\trmcd_debug(RDEV, \"name:%s ct:0x%x did:0x%x hc:0x%x\", dev_info.name,\n\t\t   dev_info.comptag, dev_info.destid, dev_info.hopcount);\n\n\tdev = bus_find_device_by_name(&rio_bus_type, NULL, dev_info.name);\n\tif (dev) {\n\t\trmcd_debug(RDEV, \"device %s already exists\", dev_info.name);\n\t\tput_device(dev);\n\t\treturn -EEXIST;\n\t}\n\n\tsize = sizeof(*rdev);\n\tmport = md->mport;\n\tdestid = dev_info.destid;\n\thopcount = dev_info.hopcount;\n\n\tif (rio_mport_read_config_32(mport, destid, hopcount,\n\t\t\t\t     RIO_PEF_CAR, &rval))\n\t\treturn -EIO;\n\n\tif (rval & RIO_PEF_SWITCH) {\n\t\trio_mport_read_config_32(mport, destid, hopcount,\n\t\t\t\t\t RIO_SWP_INFO_CAR, &swpinfo);\n\t\tsize += struct_size(rswitch, nextdev, RIO_GET_TOTAL_PORTS(swpinfo));\n\t}\n\n\trdev = kzalloc(size, GFP_KERNEL);\n\tif (rdev == NULL)\n\t\treturn -ENOMEM;\n\n\tif (mport->net == NULL) {\n\t\tstruct rio_net *net;\n\n\t\tnet = rio_alloc_net(mport);\n\t\tif (!net) {\n\t\t\terr = -ENOMEM;\n\t\t\trmcd_debug(RDEV, \"failed to allocate net object\");\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\tnet->id = mport->id;\n\t\tnet->hport = mport;\n\t\tdev_set_name(&net->dev, \"rnet_%d\", net->id);\n\t\tnet->dev.parent = &mport->dev;\n\t\tnet->dev.release = rio_release_net;\n\t\terr = rio_add_net(net);\n\t\tif (err) {\n\t\t\trmcd_debug(RDEV, \"failed to register net, err=%d\", err);\n\t\t\tkfree(net);\n\t\t\tgoto cleanup;\n\t\t}\n\t}\n\n\trdev->net = mport->net;\n\trdev->pef = rval;\n\trdev->swpinfo = swpinfo;\n\trio_mport_read_config_32(mport, destid, hopcount,\n\t\t\t\t RIO_DEV_ID_CAR, &rval);\n\trdev->did = rval >> 16;\n\trdev->vid = rval & 0xffff;\n\trio_mport_read_config_32(mport, destid, hopcount, RIO_DEV_INFO_CAR,\n\t\t\t\t &rdev->device_rev);\n\trio_mport_read_config_32(mport, destid, hopcount, RIO_ASM_ID_CAR,\n\t\t\t\t &rval);\n\trdev->asm_did = rval >> 16;\n\trdev->asm_vid = rval & 0xffff;\n\trio_mport_read_config_32(mport, destid, hopcount, RIO_ASM_INFO_CAR,\n\t\t\t\t &rval);\n\trdev->asm_rev = rval >> 16;\n\n\tif (rdev->pef & RIO_PEF_EXT_FEATURES) {\n\t\trdev->efptr = rval & 0xffff;\n\t\trdev->phys_efptr = rio_mport_get_physefb(mport, 0, destid,\n\t\t\t\t\t\thopcount, &rdev->phys_rmap);\n\n\t\trdev->em_efptr = rio_mport_get_feature(mport, 0, destid,\n\t\t\t\t\t\thopcount, RIO_EFB_ERR_MGMNT);\n\t}\n\n\trio_mport_read_config_32(mport, destid, hopcount, RIO_SRC_OPS_CAR,\n\t\t\t\t &rdev->src_ops);\n\trio_mport_read_config_32(mport, destid, hopcount, RIO_DST_OPS_CAR,\n\t\t\t\t &rdev->dst_ops);\n\n\trdev->comp_tag = dev_info.comptag;\n\trdev->destid = destid;\n\t \n\trdev->hopcount = hopcount;\n\n\tif (rdev->pef & RIO_PEF_SWITCH) {\n\t\trswitch = rdev->rswitch;\n\t\trswitch->route_table = NULL;\n\t}\n\n\tif (strlen(dev_info.name))\n\t\tdev_set_name(&rdev->dev, \"%s\", dev_info.name);\n\telse if (rdev->pef & RIO_PEF_SWITCH)\n\t\tdev_set_name(&rdev->dev, \"%02x:s:%04x\", mport->id,\n\t\t\t     rdev->comp_tag & RIO_CTAG_UDEVID);\n\telse\n\t\tdev_set_name(&rdev->dev, \"%02x:e:%04x\", mport->id,\n\t\t\t     rdev->comp_tag & RIO_CTAG_UDEVID);\n\n\tINIT_LIST_HEAD(&rdev->net_list);\n\trdev->dev.parent = &mport->net->dev;\n\trio_attach_device(rdev);\n\trdev->dev.release = rio_release_dev;\n\n\tif (rdev->dst_ops & RIO_DST_OPS_DOORBELL)\n\t\trio_init_dbell_res(&rdev->riores[RIO_DOORBELL_RESOURCE],\n\t\t\t\t   0, 0xffff);\n\terr = rio_add_device(rdev);\n\tif (err) {\n\t\tput_device(&rdev->dev);\n\t\treturn err;\n\t}\n\n\trio_dev_get(rdev);\n\n\treturn 0;\ncleanup:\n\tkfree(rdev);\n\treturn err;\n}\n\nstatic int rio_mport_del_riodev(struct mport_cdev_priv *priv, void __user *arg)\n{\n\tstruct rio_rdev_info dev_info;\n\tstruct rio_dev *rdev = NULL;\n\tstruct device  *dev;\n\tstruct rio_mport *mport;\n\tstruct rio_net *net;\n\n\tif (copy_from_user(&dev_info, arg, sizeof(dev_info)))\n\t\treturn -EFAULT;\n\tdev_info.name[sizeof(dev_info.name) - 1] = '\\0';\n\n\tmport = priv->md->mport;\n\n\t \n\tif (strlen(dev_info.name)) {\n\t\tdev = bus_find_device_by_name(&rio_bus_type, NULL,\n\t\t\t\t\t      dev_info.name);\n\t\tif (dev)\n\t\t\trdev = to_rio_dev(dev);\n\t} else {\n\t\tdo {\n\t\t\trdev = rio_get_comptag(dev_info.comptag, rdev);\n\t\t\tif (rdev && rdev->dev.parent == &mport->net->dev &&\n\t\t\t    rdev->destid == dev_info.destid &&\n\t\t\t    rdev->hopcount == dev_info.hopcount)\n\t\t\t\tbreak;\n\t\t} while (rdev);\n\t}\n\n\tif (!rdev) {\n\t\trmcd_debug(RDEV,\n\t\t\t\"device name:%s ct:0x%x did:0x%x hc:0x%x not found\",\n\t\t\tdev_info.name, dev_info.comptag, dev_info.destid,\n\t\t\tdev_info.hopcount);\n\t\treturn -ENODEV;\n\t}\n\n\tnet = rdev->net;\n\trio_dev_put(rdev);\n\trio_del_device(rdev, RIO_DEVICE_SHUTDOWN);\n\n\tif (list_empty(&net->devices)) {\n\t\trio_free_net(net);\n\t\tmport->net = NULL;\n\t}\n\n\treturn 0;\n}\n\n \n\n \nstatic int mport_cdev_open(struct inode *inode, struct file *filp)\n{\n\tint ret;\n\tint minor = iminor(inode);\n\tstruct mport_dev *chdev;\n\tstruct mport_cdev_priv *priv;\n\n\t \n\tif (minor >= RIO_MAX_MPORTS) {\n\t\trmcd_error(\"Invalid minor device number\");\n\t\treturn -EINVAL;\n\t}\n\n\tchdev = container_of(inode->i_cdev, struct mport_dev, cdev);\n\n\trmcd_debug(INIT, \"%s filp=%p\", dev_name(&chdev->dev), filp);\n\n\tif (atomic_read(&chdev->active) == 0)\n\t\treturn -ENODEV;\n\n\tget_device(&chdev->dev);\n\n\tpriv = kzalloc(sizeof(*priv), GFP_KERNEL);\n\tif (!priv) {\n\t\tput_device(&chdev->dev);\n\t\treturn -ENOMEM;\n\t}\n\n\tpriv->md = chdev;\n\n\tINIT_LIST_HEAD(&priv->db_filters);\n\tINIT_LIST_HEAD(&priv->pw_filters);\n\tspin_lock_init(&priv->fifo_lock);\n\tinit_waitqueue_head(&priv->event_rx_wait);\n\tret = kfifo_alloc(&priv->event_fifo,\n\t\t\t  sizeof(struct rio_event) * MPORT_EVENT_DEPTH,\n\t\t\t  GFP_KERNEL);\n\tif (ret < 0) {\n\t\tput_device(&chdev->dev);\n\t\tdev_err(&chdev->dev, DRV_NAME \": kfifo_alloc failed\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto err_fifo;\n\t}\n\n#ifdef CONFIG_RAPIDIO_DMA_ENGINE\n\tINIT_LIST_HEAD(&priv->async_list);\n\tspin_lock_init(&priv->req_lock);\n\tmutex_init(&priv->dma_lock);\n#endif\n\tmutex_lock(&chdev->file_mutex);\n\tlist_add_tail(&priv->list, &chdev->file_list);\n\tmutex_unlock(&chdev->file_mutex);\n\n\tfilp->private_data = priv;\n\tgoto out;\nerr_fifo:\n\tkfree(priv);\nout:\n\treturn ret;\n}\n\nstatic int mport_cdev_fasync(int fd, struct file *filp, int mode)\n{\n\tstruct mport_cdev_priv *priv = filp->private_data;\n\n\treturn fasync_helper(fd, filp, mode, &priv->async_queue);\n}\n\n#ifdef CONFIG_RAPIDIO_DMA_ENGINE\nstatic void mport_cdev_release_dma(struct file *filp)\n{\n\tstruct mport_cdev_priv *priv = filp->private_data;\n\tstruct mport_dev *md;\n\tstruct mport_dma_req *req, *req_next;\n\tunsigned long tmo = msecs_to_jiffies(dma_timeout);\n\tlong wret;\n\tLIST_HEAD(list);\n\n\trmcd_debug(EXIT, \"from filp=%p %s(%d)\",\n\t\t   filp, current->comm, task_pid_nr(current));\n\n\tif (!priv->dmach) {\n\t\trmcd_debug(EXIT, \"No DMA channel for filp=%p\", filp);\n\t\treturn;\n\t}\n\n\tmd = priv->md;\n\n\tspin_lock(&priv->req_lock);\n\tif (!list_empty(&priv->async_list)) {\n\t\trmcd_debug(EXIT, \"async list not empty filp=%p %s(%d)\",\n\t\t\t   filp, current->comm, task_pid_nr(current));\n\t\tlist_splice_init(&priv->async_list, &list);\n\t}\n\tspin_unlock(&priv->req_lock);\n\n\tif (!list_empty(&list)) {\n\t\trmcd_debug(EXIT, \"temp list not empty\");\n\t\tlist_for_each_entry_safe(req, req_next, &list, node) {\n\t\t\trmcd_debug(EXIT, \"free req->filp=%p cookie=%d compl=%s\",\n\t\t\t\t   req->filp, req->cookie,\n\t\t\t\t   completion_done(&req->req_comp)?\"yes\":\"no\");\n\t\t\tlist_del(&req->node);\n\t\t\tkref_put(&req->refcount, dma_req_free);\n\t\t}\n\t}\n\n\tput_dma_channel(priv);\n\twret = wait_for_completion_interruptible_timeout(&priv->comp, tmo);\n\n\tif (wret <= 0) {\n\t\trmcd_error(\"%s(%d) failed waiting for DMA release err=%ld\",\n\t\t\tcurrent->comm, task_pid_nr(current), wret);\n\t}\n\n\tif (priv->dmach != priv->md->dma_chan) {\n\t\trmcd_debug(EXIT, \"Release DMA channel for filp=%p %s(%d)\",\n\t\t\t   filp, current->comm, task_pid_nr(current));\n\t\trio_release_dma(priv->dmach);\n\t} else {\n\t\trmcd_debug(EXIT, \"Adjust default DMA channel refcount\");\n\t\tkref_put(&md->dma_ref, mport_release_def_dma);\n\t}\n\n\tpriv->dmach = NULL;\n}\n#else\n#define mport_cdev_release_dma(priv) do {} while (0)\n#endif\n\n \nstatic int mport_cdev_release(struct inode *inode, struct file *filp)\n{\n\tstruct mport_cdev_priv *priv = filp->private_data;\n\tstruct mport_dev *chdev;\n\tstruct rio_mport_pw_filter *pw_filter, *pw_filter_next;\n\tstruct rio_mport_db_filter *db_filter, *db_filter_next;\n\tstruct rio_mport_mapping *map, *_map;\n\tunsigned long flags;\n\n\trmcd_debug(EXIT, \"%s filp=%p\", dev_name(&priv->md->dev), filp);\n\n\tchdev = priv->md;\n\tmport_cdev_release_dma(filp);\n\n\tpriv->event_mask = 0;\n\n\tspin_lock_irqsave(&chdev->pw_lock, flags);\n\tif (!list_empty(&priv->pw_filters)) {\n\t\tlist_for_each_entry_safe(pw_filter, pw_filter_next,\n\t\t\t\t\t &priv->pw_filters, priv_node)\n\t\t\trio_mport_delete_pw_filter(pw_filter);\n\t}\n\tspin_unlock_irqrestore(&chdev->pw_lock, flags);\n\n\tspin_lock_irqsave(&chdev->db_lock, flags);\n\tlist_for_each_entry_safe(db_filter, db_filter_next,\n\t\t\t\t &priv->db_filters, priv_node) {\n\t\trio_mport_delete_db_filter(db_filter);\n\t}\n\tspin_unlock_irqrestore(&chdev->db_lock, flags);\n\n\tkfifo_free(&priv->event_fifo);\n\n\tmutex_lock(&chdev->buf_mutex);\n\tlist_for_each_entry_safe(map, _map, &chdev->mappings, node) {\n\t\tif (map->filp == filp) {\n\t\t\trmcd_debug(EXIT, \"release mapping %p filp=%p\",\n\t\t\t\t   map->virt_addr, filp);\n\t\t\tkref_put(&map->ref, mport_release_mapping);\n\t\t}\n\t}\n\tmutex_unlock(&chdev->buf_mutex);\n\n\tmport_cdev_fasync(-1, filp, 0);\n\tfilp->private_data = NULL;\n\tmutex_lock(&chdev->file_mutex);\n\tlist_del(&priv->list);\n\tmutex_unlock(&chdev->file_mutex);\n\tput_device(&chdev->dev);\n\tkfree(priv);\n\treturn 0;\n}\n\n \nstatic long mport_cdev_ioctl(struct file *filp,\n\t\tunsigned int cmd, unsigned long arg)\n{\n\tint err = -EINVAL;\n\tstruct mport_cdev_priv *data = filp->private_data;\n\tstruct mport_dev *md = data->md;\n\n\tif (atomic_read(&md->active) == 0)\n\t\treturn -ENODEV;\n\n\tswitch (cmd) {\n\tcase RIO_MPORT_MAINT_READ_LOCAL:\n\t\treturn rio_mport_maint_rd(data, (void __user *)arg, 1);\n\tcase RIO_MPORT_MAINT_WRITE_LOCAL:\n\t\treturn rio_mport_maint_wr(data, (void __user *)arg, 1);\n\tcase RIO_MPORT_MAINT_READ_REMOTE:\n\t\treturn rio_mport_maint_rd(data, (void __user *)arg, 0);\n\tcase RIO_MPORT_MAINT_WRITE_REMOTE:\n\t\treturn rio_mport_maint_wr(data, (void __user *)arg, 0);\n\tcase RIO_MPORT_MAINT_HDID_SET:\n\t\treturn maint_hdid_set(data, (void __user *)arg);\n\tcase RIO_MPORT_MAINT_COMPTAG_SET:\n\t\treturn maint_comptag_set(data, (void __user *)arg);\n\tcase RIO_MPORT_MAINT_PORT_IDX_GET:\n\t\treturn maint_port_idx_get(data, (void __user *)arg);\n\tcase RIO_MPORT_GET_PROPERTIES:\n\t\tmd->properties.hdid = md->mport->host_deviceid;\n\t\tif (copy_to_user((void __user *)arg, &(md->properties),\n\t\t\t\t sizeof(md->properties)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tcase RIO_ENABLE_DOORBELL_RANGE:\n\t\treturn rio_mport_add_db_filter(data, (void __user *)arg);\n\tcase RIO_DISABLE_DOORBELL_RANGE:\n\t\treturn rio_mport_remove_db_filter(data, (void __user *)arg);\n\tcase RIO_ENABLE_PORTWRITE_RANGE:\n\t\treturn rio_mport_add_pw_filter(data, (void __user *)arg);\n\tcase RIO_DISABLE_PORTWRITE_RANGE:\n\t\treturn rio_mport_remove_pw_filter(data, (void __user *)arg);\n\tcase RIO_SET_EVENT_MASK:\n\t\tdata->event_mask = (u32)arg;\n\t\treturn 0;\n\tcase RIO_GET_EVENT_MASK:\n\t\tif (copy_to_user((void __user *)arg, &data->event_mask,\n\t\t\t\t    sizeof(u32)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tcase RIO_MAP_OUTBOUND:\n\t\treturn rio_mport_obw_map(filp, (void __user *)arg);\n\tcase RIO_MAP_INBOUND:\n\t\treturn rio_mport_map_inbound(filp, (void __user *)arg);\n\tcase RIO_UNMAP_OUTBOUND:\n\t\treturn rio_mport_obw_free(filp, (void __user *)arg);\n\tcase RIO_UNMAP_INBOUND:\n\t\treturn rio_mport_inbound_free(filp, (void __user *)arg);\n\tcase RIO_ALLOC_DMA:\n\t\treturn rio_mport_alloc_dma(filp, (void __user *)arg);\n\tcase RIO_FREE_DMA:\n\t\treturn rio_mport_free_dma(filp, (void __user *)arg);\n\tcase RIO_WAIT_FOR_ASYNC:\n\t\treturn rio_mport_wait_for_async_dma(filp, (void __user *)arg);\n\tcase RIO_TRANSFER:\n\t\treturn rio_mport_transfer_ioctl(filp, (void __user *)arg);\n\tcase RIO_DEV_ADD:\n\t\treturn rio_mport_add_riodev(data, (void __user *)arg);\n\tcase RIO_DEV_DEL:\n\t\treturn rio_mport_del_riodev(data, (void __user *)arg);\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\n \nstatic void mport_release_mapping(struct kref *ref)\n{\n\tstruct rio_mport_mapping *map =\n\t\t\tcontainer_of(ref, struct rio_mport_mapping, ref);\n\tstruct rio_mport *mport = map->md->mport;\n\n\trmcd_debug(MMAP, \"type %d mapping @ %p (phys = %pad) for %s\",\n\t\t   map->dir, map->virt_addr,\n\t\t   &map->phys_addr, mport->name);\n\n\tlist_del(&map->node);\n\n\tswitch (map->dir) {\n\tcase MAP_INBOUND:\n\t\trio_unmap_inb_region(mport, map->phys_addr);\n\t\tfallthrough;\n\tcase MAP_DMA:\n\t\tdma_free_coherent(mport->dev.parent, map->size,\n\t\t\t\t  map->virt_addr, map->phys_addr);\n\t\tbreak;\n\tcase MAP_OUTBOUND:\n\t\trio_unmap_outb_region(mport, map->rioid, map->rio_addr);\n\t\tbreak;\n\t}\n\tkfree(map);\n}\n\nstatic void mport_mm_open(struct vm_area_struct *vma)\n{\n\tstruct rio_mport_mapping *map = vma->vm_private_data;\n\n\trmcd_debug(MMAP, \"%pad\", &map->phys_addr);\n\tkref_get(&map->ref);\n}\n\nstatic void mport_mm_close(struct vm_area_struct *vma)\n{\n\tstruct rio_mport_mapping *map = vma->vm_private_data;\n\n\trmcd_debug(MMAP, \"%pad\", &map->phys_addr);\n\tmutex_lock(&map->md->buf_mutex);\n\tkref_put(&map->ref, mport_release_mapping);\n\tmutex_unlock(&map->md->buf_mutex);\n}\n\nstatic const struct vm_operations_struct vm_ops = {\n\t.open =\tmport_mm_open,\n\t.close = mport_mm_close,\n};\n\nstatic int mport_cdev_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tstruct mport_cdev_priv *priv = filp->private_data;\n\tstruct mport_dev *md;\n\tsize_t size = vma->vm_end - vma->vm_start;\n\tdma_addr_t baddr;\n\tunsigned long offset;\n\tint found = 0, ret;\n\tstruct rio_mport_mapping *map;\n\n\trmcd_debug(MMAP, \"0x%x bytes at offset 0x%lx\",\n\t\t   (unsigned int)size, vma->vm_pgoff);\n\n\tmd = priv->md;\n\tbaddr = ((dma_addr_t)vma->vm_pgoff << PAGE_SHIFT);\n\n\tmutex_lock(&md->buf_mutex);\n\tlist_for_each_entry(map, &md->mappings, node) {\n\t\tif (baddr >= map->phys_addr &&\n\t\t    baddr < (map->phys_addr + map->size)) {\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&md->buf_mutex);\n\n\tif (!found)\n\t\treturn -ENOMEM;\n\n\toffset = baddr - map->phys_addr;\n\n\tif (size + offset > map->size)\n\t\treturn -EINVAL;\n\n\tvma->vm_pgoff = offset >> PAGE_SHIFT;\n\trmcd_debug(MMAP, \"MMAP adjusted offset = 0x%lx\", vma->vm_pgoff);\n\n\tif (map->dir == MAP_INBOUND || map->dir == MAP_DMA)\n\t\tret = dma_mmap_coherent(md->mport->dev.parent, vma,\n\t\t\t\tmap->virt_addr, map->phys_addr, map->size);\n\telse if (map->dir == MAP_OUTBOUND) {\n\t\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\t\tret = vm_iomap_memory(vma, map->phys_addr, map->size);\n\t} else {\n\t\trmcd_error(\"Attempt to mmap unsupported mapping type\");\n\t\tret = -EIO;\n\t}\n\n\tif (!ret) {\n\t\tvma->vm_private_data = map;\n\t\tvma->vm_ops = &vm_ops;\n\t\tmport_mm_open(vma);\n\t} else {\n\t\trmcd_error(\"MMAP exit with err=%d\", ret);\n\t}\n\n\treturn ret;\n}\n\nstatic __poll_t mport_cdev_poll(struct file *filp, poll_table *wait)\n{\n\tstruct mport_cdev_priv *priv = filp->private_data;\n\n\tpoll_wait(filp, &priv->event_rx_wait, wait);\n\tif (kfifo_len(&priv->event_fifo))\n\t\treturn EPOLLIN | EPOLLRDNORM;\n\n\treturn 0;\n}\n\nstatic ssize_t mport_read(struct file *filp, char __user *buf, size_t count,\n\t\t\tloff_t *ppos)\n{\n\tstruct mport_cdev_priv *priv = filp->private_data;\n\tint copied;\n\tssize_t ret;\n\n\tif (!count)\n\t\treturn 0;\n\n\tif (kfifo_is_empty(&priv->event_fifo) &&\n\t    (filp->f_flags & O_NONBLOCK))\n\t\treturn -EAGAIN;\n\n\tif (count % sizeof(struct rio_event))\n\t\treturn -EINVAL;\n\n\tret = wait_event_interruptible(priv->event_rx_wait,\n\t\t\t\t\tkfifo_len(&priv->event_fifo) != 0);\n\tif (ret)\n\t\treturn ret;\n\n\twhile (ret < count) {\n\t\tif (kfifo_to_user(&priv->event_fifo, buf,\n\t\t      sizeof(struct rio_event), &copied))\n\t\t\treturn -EFAULT;\n\t\tret += copied;\n\t\tbuf += copied;\n\t}\n\n\treturn ret;\n}\n\nstatic ssize_t mport_write(struct file *filp, const char __user *buf,\n\t\t\t size_t count, loff_t *ppos)\n{\n\tstruct mport_cdev_priv *priv = filp->private_data;\n\tstruct rio_mport *mport = priv->md->mport;\n\tstruct rio_event event;\n\tint len, ret;\n\n\tif (!count)\n\t\treturn 0;\n\n\tif (count % sizeof(event))\n\t\treturn -EINVAL;\n\n\tlen = 0;\n\twhile ((count - len) >= (int)sizeof(event)) {\n\t\tif (copy_from_user(&event, buf, sizeof(event)))\n\t\t\treturn -EFAULT;\n\n\t\tif (event.header != RIO_DOORBELL)\n\t\t\treturn -EINVAL;\n\n\t\tret = rio_mport_send_doorbell(mport,\n\t\t\t\t\t      event.u.doorbell.rioid,\n\t\t\t\t\t      event.u.doorbell.payload);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tlen += sizeof(event);\n\t\tbuf += sizeof(event);\n\t}\n\n\treturn len;\n}\n\nstatic const struct file_operations mport_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= mport_cdev_open,\n\t.release\t= mport_cdev_release,\n\t.poll\t\t= mport_cdev_poll,\n\t.read\t\t= mport_read,\n\t.write\t\t= mport_write,\n\t.mmap\t\t= mport_cdev_mmap,\n\t.fasync\t\t= mport_cdev_fasync,\n\t.unlocked_ioctl = mport_cdev_ioctl\n};\n\n \n\nstatic void mport_device_release(struct device *dev)\n{\n\tstruct mport_dev *md;\n\n\trmcd_debug(EXIT, \"%s\", dev_name(dev));\n\tmd = container_of(dev, struct mport_dev, dev);\n\tkfree(md);\n}\n\n \nstatic struct mport_dev *mport_cdev_add(struct rio_mport *mport)\n{\n\tint ret = 0;\n\tstruct mport_dev *md;\n\tstruct rio_mport_attr attr;\n\n\tmd = kzalloc(sizeof(*md), GFP_KERNEL);\n\tif (!md) {\n\t\trmcd_error(\"Unable allocate a device object\");\n\t\treturn NULL;\n\t}\n\n\tmd->mport = mport;\n\tmutex_init(&md->buf_mutex);\n\tmutex_init(&md->file_mutex);\n\tINIT_LIST_HEAD(&md->file_list);\n\n\tdevice_initialize(&md->dev);\n\tmd->dev.devt = MKDEV(MAJOR(dev_number), mport->id);\n\tmd->dev.class = dev_class;\n\tmd->dev.parent = &mport->dev;\n\tmd->dev.release = mport_device_release;\n\tdev_set_name(&md->dev, DEV_NAME \"%d\", mport->id);\n\tatomic_set(&md->active, 1);\n\n\tcdev_init(&md->cdev, &mport_fops);\n\tmd->cdev.owner = THIS_MODULE;\n\n\tINIT_LIST_HEAD(&md->doorbells);\n\tspin_lock_init(&md->db_lock);\n\tINIT_LIST_HEAD(&md->portwrites);\n\tspin_lock_init(&md->pw_lock);\n\tINIT_LIST_HEAD(&md->mappings);\n\n\tmd->properties.id = mport->id;\n\tmd->properties.sys_size = mport->sys_size;\n\tmd->properties.hdid = mport->host_deviceid;\n\tmd->properties.index = mport->index;\n\n\t \n#ifdef CONFIG_FSL_RIO  \n\tmd->properties.transfer_mode |= RIO_TRANSFER_MODE_MAPPED;\n#else\n\tmd->properties.transfer_mode |= RIO_TRANSFER_MODE_TRANSFER;\n#endif\n\n\tret = cdev_device_add(&md->cdev, &md->dev);\n\tif (ret) {\n\t\trmcd_error(\"Failed to register mport %d (err=%d)\",\n\t\t       mport->id, ret);\n\t\tgoto err_cdev;\n\t}\n\tret = rio_query_mport(mport, &attr);\n\tif (!ret) {\n\t\tmd->properties.flags = attr.flags;\n\t\tmd->properties.link_speed = attr.link_speed;\n\t\tmd->properties.link_width = attr.link_width;\n\t\tmd->properties.dma_max_sge = attr.dma_max_sge;\n\t\tmd->properties.dma_max_size = attr.dma_max_size;\n\t\tmd->properties.dma_align = attr.dma_align;\n\t\tmd->properties.cap_sys_size = 0;\n\t\tmd->properties.cap_transfer_mode = 0;\n\t\tmd->properties.cap_addr_size = 0;\n\t} else\n\t\tpr_info(DRV_PREFIX \"Failed to obtain info for %s cdev(%d:%d)\\n\",\n\t\t\tmport->name, MAJOR(dev_number), mport->id);\n\n\tmutex_lock(&mport_devs_lock);\n\tlist_add_tail(&md->node, &mport_devs);\n\tmutex_unlock(&mport_devs_lock);\n\n\tpr_info(DRV_PREFIX \"Added %s cdev(%d:%d)\\n\",\n\t\tmport->name, MAJOR(dev_number), mport->id);\n\n\treturn md;\n\nerr_cdev:\n\tput_device(&md->dev);\n\treturn NULL;\n}\n\n \nstatic void mport_cdev_terminate_dma(struct mport_dev *md)\n{\n#ifdef CONFIG_RAPIDIO_DMA_ENGINE\n\tstruct mport_cdev_priv *client;\n\n\trmcd_debug(DMA, \"%s\", dev_name(&md->dev));\n\n\tmutex_lock(&md->file_mutex);\n\tlist_for_each_entry(client, &md->file_list, list) {\n\t\tif (client->dmach) {\n\t\t\tdmaengine_terminate_all(client->dmach);\n\t\t\trio_release_dma(client->dmach);\n\t\t}\n\t}\n\tmutex_unlock(&md->file_mutex);\n\n\tif (md->dma_chan) {\n\t\tdmaengine_terminate_all(md->dma_chan);\n\t\trio_release_dma(md->dma_chan);\n\t\tmd->dma_chan = NULL;\n\t}\n#endif\n}\n\n\n \nstatic int mport_cdev_kill_fasync(struct mport_dev *md)\n{\n\tunsigned int files = 0;\n\tstruct mport_cdev_priv *client;\n\n\tmutex_lock(&md->file_mutex);\n\tlist_for_each_entry(client, &md->file_list, list) {\n\t\tif (client->async_queue)\n\t\t\tkill_fasync(&client->async_queue, SIGIO, POLL_HUP);\n\t\tfiles++;\n\t}\n\tmutex_unlock(&md->file_mutex);\n\treturn files;\n}\n\n \nstatic void mport_cdev_remove(struct mport_dev *md)\n{\n\tstruct rio_mport_mapping *map, *_map;\n\n\trmcd_debug(EXIT, \"Remove %s cdev\", md->mport->name);\n\tatomic_set(&md->active, 0);\n\tmport_cdev_terminate_dma(md);\n\trio_del_mport_pw_handler(md->mport, md, rio_mport_pw_handler);\n\tcdev_device_del(&md->cdev, &md->dev);\n\tmport_cdev_kill_fasync(md);\n\n\t \n\n\t \n\tmutex_lock(&md->buf_mutex);\n\tlist_for_each_entry_safe(map, _map, &md->mappings, node) {\n\t\tkref_put(&map->ref, mport_release_mapping);\n\t}\n\tmutex_unlock(&md->buf_mutex);\n\n\tif (!list_empty(&md->mappings))\n\t\trmcd_warn(\"WARNING: %s pending mappings on removal\",\n\t\t\t  md->mport->name);\n\n\trio_release_inb_dbell(md->mport, 0, 0x0fff);\n\n\tput_device(&md->dev);\n}\n\n \n\n \nstatic int mport_add_mport(struct device *dev)\n{\n\tstruct rio_mport *mport = NULL;\n\tstruct mport_dev *chdev = NULL;\n\n\tmport = to_rio_mport(dev);\n\tif (!mport)\n\t\treturn -ENODEV;\n\n\tchdev = mport_cdev_add(mport);\n\tif (!chdev)\n\t\treturn -ENODEV;\n\n\treturn 0;\n}\n\n \nstatic void mport_remove_mport(struct device *dev)\n{\n\tstruct rio_mport *mport = NULL;\n\tstruct mport_dev *chdev;\n\tint found = 0;\n\n\tmport = to_rio_mport(dev);\n\trmcd_debug(EXIT, \"Remove %s\", mport->name);\n\n\tmutex_lock(&mport_devs_lock);\n\tlist_for_each_entry(chdev, &mport_devs, node) {\n\t\tif (chdev->mport->id == mport->id) {\n\t\t\tatomic_set(&chdev->active, 0);\n\t\t\tlist_del(&chdev->node);\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&mport_devs_lock);\n\n\tif (found)\n\t\tmport_cdev_remove(chdev);\n}\n\n \nstatic struct class_interface rio_mport_interface __refdata = {\n\t.class\t\t= &rio_mport_class,\n\t.add_dev\t= mport_add_mport,\n\t.remove_dev\t= mport_remove_mport,\n};\n\n \n\n \nstatic int __init mport_init(void)\n{\n\tint ret;\n\n\t \n\tdev_class = class_create(DRV_NAME);\n\tif (IS_ERR(dev_class)) {\n\t\trmcd_error(\"Unable to create \" DRV_NAME \" class\");\n\t\treturn PTR_ERR(dev_class);\n\t}\n\n\tret = alloc_chrdev_region(&dev_number, 0, RIO_MAX_MPORTS, DRV_NAME);\n\tif (ret < 0)\n\t\tgoto err_chr;\n\n\trmcd_debug(INIT, \"Registered class with major=%d\", MAJOR(dev_number));\n\n\t \n\tret = class_interface_register(&rio_mport_interface);\n\tif (ret) {\n\t\trmcd_error(\"class_interface_register() failed, err=%d\", ret);\n\t\tgoto err_cli;\n\t}\n\n\treturn 0;\n\nerr_cli:\n\tunregister_chrdev_region(dev_number, RIO_MAX_MPORTS);\nerr_chr:\n\tclass_destroy(dev_class);\n\treturn ret;\n}\n\n \nstatic void __exit mport_exit(void)\n{\n\tclass_interface_unregister(&rio_mport_interface);\n\tclass_destroy(dev_class);\n\tunregister_chrdev_region(dev_number, RIO_MAX_MPORTS);\n}\n\nmodule_init(mport_init);\nmodule_exit(mport_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}