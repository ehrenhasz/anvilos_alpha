{
  "module_name": "dm-crypt.c",
  "hash_id": "87c06b248c5a360602a9787f06e14d05211427f7fa920e01d48f54f476c48dce",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-crypt.c",
  "human_readable_source": "\n \n\n#include <linux/completion.h>\n#include <linux/err.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/key.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/blk-integrity.h>\n#include <linux/mempool.h>\n#include <linux/slab.h>\n#include <linux/crypto.h>\n#include <linux/workqueue.h>\n#include <linux/kthread.h>\n#include <linux/backing-dev.h>\n#include <linux/atomic.h>\n#include <linux/scatterlist.h>\n#include <linux/rbtree.h>\n#include <linux/ctype.h>\n#include <asm/page.h>\n#include <asm/unaligned.h>\n#include <crypto/hash.h>\n#include <crypto/md5.h>\n#include <crypto/skcipher.h>\n#include <crypto/aead.h>\n#include <crypto/authenc.h>\n#include <crypto/utils.h>\n#include <linux/rtnetlink.h>  \n#include <linux/key-type.h>\n#include <keys/user-type.h>\n#include <keys/encrypted-type.h>\n#include <keys/trusted-type.h>\n\n#include <linux/device-mapper.h>\n\n#include \"dm-audit.h\"\n\n#define DM_MSG_PREFIX \"crypt\"\n\n \nstruct convert_context {\n\tstruct completion restart;\n\tstruct bio *bio_in;\n\tstruct bio *bio_out;\n\tstruct bvec_iter iter_in;\n\tstruct bvec_iter iter_out;\n\tu64 cc_sector;\n\tatomic_t cc_pending;\n\tunion {\n\t\tstruct skcipher_request *req;\n\t\tstruct aead_request *req_aead;\n\t} r;\n\n};\n\n \nstruct dm_crypt_io {\n\tstruct crypt_config *cc;\n\tstruct bio *base_bio;\n\tu8 *integrity_metadata;\n\tbool integrity_metadata_from_pool:1;\n\tbool in_tasklet:1;\n\n\tstruct work_struct work;\n\tstruct tasklet_struct tasklet;\n\n\tstruct convert_context ctx;\n\n\tatomic_t io_pending;\n\tblk_status_t error;\n\tsector_t sector;\n\n\tstruct rb_node rb_node;\n} CRYPTO_MINALIGN_ATTR;\n\nstruct dm_crypt_request {\n\tstruct convert_context *ctx;\n\tstruct scatterlist sg_in[4];\n\tstruct scatterlist sg_out[4];\n\tu64 iv_sector;\n};\n\nstruct crypt_config;\n\nstruct crypt_iv_operations {\n\tint (*ctr)(struct crypt_config *cc, struct dm_target *ti,\n\t\t   const char *opts);\n\tvoid (*dtr)(struct crypt_config *cc);\n\tint (*init)(struct crypt_config *cc);\n\tint (*wipe)(struct crypt_config *cc);\n\tint (*generator)(struct crypt_config *cc, u8 *iv,\n\t\t\t struct dm_crypt_request *dmreq);\n\tint (*post)(struct crypt_config *cc, u8 *iv,\n\t\t    struct dm_crypt_request *dmreq);\n};\n\nstruct iv_benbi_private {\n\tint shift;\n};\n\n#define LMK_SEED_SIZE 64  \nstruct iv_lmk_private {\n\tstruct crypto_shash *hash_tfm;\n\tu8 *seed;\n};\n\n#define TCW_WHITENING_SIZE 16\nstruct iv_tcw_private {\n\tstruct crypto_shash *crc32_tfm;\n\tu8 *iv_seed;\n\tu8 *whitening;\n};\n\n#define ELEPHANT_MAX_KEY_SIZE 32\nstruct iv_elephant_private {\n\tstruct crypto_skcipher *tfm;\n};\n\n \nenum flags { DM_CRYPT_SUSPENDED, DM_CRYPT_KEY_VALID,\n\t     DM_CRYPT_SAME_CPU, DM_CRYPT_NO_OFFLOAD,\n\t     DM_CRYPT_NO_READ_WORKQUEUE, DM_CRYPT_NO_WRITE_WORKQUEUE,\n\t     DM_CRYPT_WRITE_INLINE };\n\nenum cipher_flags {\n\tCRYPT_MODE_INTEGRITY_AEAD,\t \n\tCRYPT_IV_LARGE_SECTORS,\t\t \n\tCRYPT_ENCRYPT_PREPROCESS,\t \n};\n\n \nstruct crypt_config {\n\tstruct dm_dev *dev;\n\tsector_t start;\n\n\tstruct percpu_counter n_allocated_pages;\n\n\tstruct workqueue_struct *io_queue;\n\tstruct workqueue_struct *crypt_queue;\n\n\tspinlock_t write_thread_lock;\n\tstruct task_struct *write_thread;\n\tstruct rb_root write_tree;\n\n\tchar *cipher_string;\n\tchar *cipher_auth;\n\tchar *key_string;\n\n\tconst struct crypt_iv_operations *iv_gen_ops;\n\tunion {\n\t\tstruct iv_benbi_private benbi;\n\t\tstruct iv_lmk_private lmk;\n\t\tstruct iv_tcw_private tcw;\n\t\tstruct iv_elephant_private elephant;\n\t} iv_gen_private;\n\tu64 iv_offset;\n\tunsigned int iv_size;\n\tunsigned short sector_size;\n\tunsigned char sector_shift;\n\n\tunion {\n\t\tstruct crypto_skcipher **tfms;\n\t\tstruct crypto_aead **tfms_aead;\n\t} cipher_tfm;\n\tunsigned int tfms_count;\n\tunsigned long cipher_flags;\n\n\t \n\tunsigned int dmreq_start;\n\n\tunsigned int per_bio_data_size;\n\n\tunsigned long flags;\n\tunsigned int key_size;\n\tunsigned int key_parts;       \n\tunsigned int key_extra_size;  \n\tunsigned int key_mac_size;    \n\n\tunsigned int integrity_tag_size;\n\tunsigned int integrity_iv_size;\n\tunsigned int on_disk_tag_size;\n\n\t \n\tunsigned int tag_pool_max_sectors;\n\tmempool_t tag_pool;\n\tmempool_t req_pool;\n\tmempool_t page_pool;\n\n\tstruct bio_set bs;\n\tstruct mutex bio_alloc_lock;\n\n\tu8 *authenc_key;  \n\tu8 key[];\n};\n\n#define MIN_IOS\t\t64\n#define MAX_TAG_SIZE\t480\n#define POOL_ENTRY_SIZE\t512\n\nstatic DEFINE_SPINLOCK(dm_crypt_clients_lock);\nstatic unsigned int dm_crypt_clients_n;\nstatic volatile unsigned long dm_crypt_pages_per_client;\n#define DM_CRYPT_MEMORY_PERCENT\t\t\t2\n#define DM_CRYPT_MIN_PAGES_PER_CLIENT\t\t(BIO_MAX_VECS * 16)\n\nstatic void crypt_endio(struct bio *clone);\nstatic void kcryptd_queue_crypt(struct dm_crypt_io *io);\nstatic struct scatterlist *crypt_get_sg_data(struct crypt_config *cc,\n\t\t\t\t\t     struct scatterlist *sg);\n\nstatic bool crypt_integrity_aead(struct crypt_config *cc);\n\n \nstatic struct crypto_skcipher *any_tfm(struct crypt_config *cc)\n{\n\treturn cc->cipher_tfm.tfms[0];\n}\n\nstatic struct crypto_aead *any_tfm_aead(struct crypt_config *cc)\n{\n\treturn cc->cipher_tfm.tfms_aead[0];\n}\n\n \n\nstatic int crypt_iv_plain_gen(struct crypt_config *cc, u8 *iv,\n\t\t\t      struct dm_crypt_request *dmreq)\n{\n\tmemset(iv, 0, cc->iv_size);\n\t*(__le32 *)iv = cpu_to_le32(dmreq->iv_sector & 0xffffffff);\n\n\treturn 0;\n}\n\nstatic int crypt_iv_plain64_gen(struct crypt_config *cc, u8 *iv,\n\t\t\t\tstruct dm_crypt_request *dmreq)\n{\n\tmemset(iv, 0, cc->iv_size);\n\t*(__le64 *)iv = cpu_to_le64(dmreq->iv_sector);\n\n\treturn 0;\n}\n\nstatic int crypt_iv_plain64be_gen(struct crypt_config *cc, u8 *iv,\n\t\t\t\t  struct dm_crypt_request *dmreq)\n{\n\tmemset(iv, 0, cc->iv_size);\n\t \n\t*(__be64 *)&iv[cc->iv_size - sizeof(u64)] = cpu_to_be64(dmreq->iv_sector);\n\n\treturn 0;\n}\n\nstatic int crypt_iv_essiv_gen(struct crypt_config *cc, u8 *iv,\n\t\t\t      struct dm_crypt_request *dmreq)\n{\n\t \n\tmemset(iv, 0, cc->iv_size);\n\t*(__le64 *)iv = cpu_to_le64(dmreq->iv_sector);\n\n\treturn 0;\n}\n\nstatic int crypt_iv_benbi_ctr(struct crypt_config *cc, struct dm_target *ti,\n\t\t\t      const char *opts)\n{\n\tunsigned int bs;\n\tint log;\n\n\tif (crypt_integrity_aead(cc))\n\t\tbs = crypto_aead_blocksize(any_tfm_aead(cc));\n\telse\n\t\tbs = crypto_skcipher_blocksize(any_tfm(cc));\n\tlog = ilog2(bs);\n\n\t \n\tif (1 << log != bs) {\n\t\tti->error = \"cypher blocksize is not a power of 2\";\n\t\treturn -EINVAL;\n\t}\n\n\tif (log > 9) {\n\t\tti->error = \"cypher blocksize is > 512\";\n\t\treturn -EINVAL;\n\t}\n\n\tcc->iv_gen_private.benbi.shift = 9 - log;\n\n\treturn 0;\n}\n\nstatic void crypt_iv_benbi_dtr(struct crypt_config *cc)\n{\n}\n\nstatic int crypt_iv_benbi_gen(struct crypt_config *cc, u8 *iv,\n\t\t\t      struct dm_crypt_request *dmreq)\n{\n\t__be64 val;\n\n\tmemset(iv, 0, cc->iv_size - sizeof(u64));  \n\n\tval = cpu_to_be64(((u64)dmreq->iv_sector << cc->iv_gen_private.benbi.shift) + 1);\n\tput_unaligned(val, (__be64 *)(iv + cc->iv_size - sizeof(u64)));\n\n\treturn 0;\n}\n\nstatic int crypt_iv_null_gen(struct crypt_config *cc, u8 *iv,\n\t\t\t     struct dm_crypt_request *dmreq)\n{\n\tmemset(iv, 0, cc->iv_size);\n\n\treturn 0;\n}\n\nstatic void crypt_iv_lmk_dtr(struct crypt_config *cc)\n{\n\tstruct iv_lmk_private *lmk = &cc->iv_gen_private.lmk;\n\n\tif (lmk->hash_tfm && !IS_ERR(lmk->hash_tfm))\n\t\tcrypto_free_shash(lmk->hash_tfm);\n\tlmk->hash_tfm = NULL;\n\n\tkfree_sensitive(lmk->seed);\n\tlmk->seed = NULL;\n}\n\nstatic int crypt_iv_lmk_ctr(struct crypt_config *cc, struct dm_target *ti,\n\t\t\t    const char *opts)\n{\n\tstruct iv_lmk_private *lmk = &cc->iv_gen_private.lmk;\n\n\tif (cc->sector_size != (1 << SECTOR_SHIFT)) {\n\t\tti->error = \"Unsupported sector size for LMK\";\n\t\treturn -EINVAL;\n\t}\n\n\tlmk->hash_tfm = crypto_alloc_shash(\"md5\", 0,\n\t\t\t\t\t   CRYPTO_ALG_ALLOCATES_MEMORY);\n\tif (IS_ERR(lmk->hash_tfm)) {\n\t\tti->error = \"Error initializing LMK hash\";\n\t\treturn PTR_ERR(lmk->hash_tfm);\n\t}\n\n\t \n\tif (cc->key_parts == cc->tfms_count) {\n\t\tlmk->seed = NULL;\n\t\treturn 0;\n\t}\n\n\tlmk->seed = kzalloc(LMK_SEED_SIZE, GFP_KERNEL);\n\tif (!lmk->seed) {\n\t\tcrypt_iv_lmk_dtr(cc);\n\t\tti->error = \"Error kmallocing seed storage in LMK\";\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic int crypt_iv_lmk_init(struct crypt_config *cc)\n{\n\tstruct iv_lmk_private *lmk = &cc->iv_gen_private.lmk;\n\tint subkey_size = cc->key_size / cc->key_parts;\n\n\t \n\tif (lmk->seed)\n\t\tmemcpy(lmk->seed, cc->key + (cc->tfms_count * subkey_size),\n\t\t       crypto_shash_digestsize(lmk->hash_tfm));\n\n\treturn 0;\n}\n\nstatic int crypt_iv_lmk_wipe(struct crypt_config *cc)\n{\n\tstruct iv_lmk_private *lmk = &cc->iv_gen_private.lmk;\n\n\tif (lmk->seed)\n\t\tmemset(lmk->seed, 0, LMK_SEED_SIZE);\n\n\treturn 0;\n}\n\nstatic int crypt_iv_lmk_one(struct crypt_config *cc, u8 *iv,\n\t\t\t    struct dm_crypt_request *dmreq,\n\t\t\t    u8 *data)\n{\n\tstruct iv_lmk_private *lmk = &cc->iv_gen_private.lmk;\n\tSHASH_DESC_ON_STACK(desc, lmk->hash_tfm);\n\tstruct md5_state md5state;\n\t__le32 buf[4];\n\tint i, r;\n\n\tdesc->tfm = lmk->hash_tfm;\n\n\tr = crypto_shash_init(desc);\n\tif (r)\n\t\treturn r;\n\n\tif (lmk->seed) {\n\t\tr = crypto_shash_update(desc, lmk->seed, LMK_SEED_SIZE);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\t \n\tr = crypto_shash_update(desc, data + 16, 16 * 31);\n\tif (r)\n\t\treturn r;\n\n\t \n\tbuf[0] = cpu_to_le32(dmreq->iv_sector & 0xFFFFFFFF);\n\tbuf[1] = cpu_to_le32((((u64)dmreq->iv_sector >> 32) & 0x00FFFFFF) | 0x80000000);\n\tbuf[2] = cpu_to_le32(4024);\n\tbuf[3] = 0;\n\tr = crypto_shash_update(desc, (u8 *)buf, sizeof(buf));\n\tif (r)\n\t\treturn r;\n\n\t \n\tr = crypto_shash_export(desc, &md5state);\n\tif (r)\n\t\treturn r;\n\n\tfor (i = 0; i < MD5_HASH_WORDS; i++)\n\t\t__cpu_to_le32s(&md5state.hash[i]);\n\tmemcpy(iv, &md5state.hash, cc->iv_size);\n\n\treturn 0;\n}\n\nstatic int crypt_iv_lmk_gen(struct crypt_config *cc, u8 *iv,\n\t\t\t    struct dm_crypt_request *dmreq)\n{\n\tstruct scatterlist *sg;\n\tu8 *src;\n\tint r = 0;\n\n\tif (bio_data_dir(dmreq->ctx->bio_in) == WRITE) {\n\t\tsg = crypt_get_sg_data(cc, dmreq->sg_in);\n\t\tsrc = kmap_local_page(sg_page(sg));\n\t\tr = crypt_iv_lmk_one(cc, iv, dmreq, src + sg->offset);\n\t\tkunmap_local(src);\n\t} else\n\t\tmemset(iv, 0, cc->iv_size);\n\n\treturn r;\n}\n\nstatic int crypt_iv_lmk_post(struct crypt_config *cc, u8 *iv,\n\t\t\t     struct dm_crypt_request *dmreq)\n{\n\tstruct scatterlist *sg;\n\tu8 *dst;\n\tint r;\n\n\tif (bio_data_dir(dmreq->ctx->bio_in) == WRITE)\n\t\treturn 0;\n\n\tsg = crypt_get_sg_data(cc, dmreq->sg_out);\n\tdst = kmap_local_page(sg_page(sg));\n\tr = crypt_iv_lmk_one(cc, iv, dmreq, dst + sg->offset);\n\n\t \n\tif (!r)\n\t\tcrypto_xor(dst + sg->offset, iv, cc->iv_size);\n\n\tkunmap_local(dst);\n\treturn r;\n}\n\nstatic void crypt_iv_tcw_dtr(struct crypt_config *cc)\n{\n\tstruct iv_tcw_private *tcw = &cc->iv_gen_private.tcw;\n\n\tkfree_sensitive(tcw->iv_seed);\n\ttcw->iv_seed = NULL;\n\tkfree_sensitive(tcw->whitening);\n\ttcw->whitening = NULL;\n\n\tif (tcw->crc32_tfm && !IS_ERR(tcw->crc32_tfm))\n\t\tcrypto_free_shash(tcw->crc32_tfm);\n\ttcw->crc32_tfm = NULL;\n}\n\nstatic int crypt_iv_tcw_ctr(struct crypt_config *cc, struct dm_target *ti,\n\t\t\t    const char *opts)\n{\n\tstruct iv_tcw_private *tcw = &cc->iv_gen_private.tcw;\n\n\tif (cc->sector_size != (1 << SECTOR_SHIFT)) {\n\t\tti->error = \"Unsupported sector size for TCW\";\n\t\treturn -EINVAL;\n\t}\n\n\tif (cc->key_size <= (cc->iv_size + TCW_WHITENING_SIZE)) {\n\t\tti->error = \"Wrong key size for TCW\";\n\t\treturn -EINVAL;\n\t}\n\n\ttcw->crc32_tfm = crypto_alloc_shash(\"crc32\", 0,\n\t\t\t\t\t    CRYPTO_ALG_ALLOCATES_MEMORY);\n\tif (IS_ERR(tcw->crc32_tfm)) {\n\t\tti->error = \"Error initializing CRC32 in TCW\";\n\t\treturn PTR_ERR(tcw->crc32_tfm);\n\t}\n\n\ttcw->iv_seed = kzalloc(cc->iv_size, GFP_KERNEL);\n\ttcw->whitening = kzalloc(TCW_WHITENING_SIZE, GFP_KERNEL);\n\tif (!tcw->iv_seed || !tcw->whitening) {\n\t\tcrypt_iv_tcw_dtr(cc);\n\t\tti->error = \"Error allocating seed storage in TCW\";\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic int crypt_iv_tcw_init(struct crypt_config *cc)\n{\n\tstruct iv_tcw_private *tcw = &cc->iv_gen_private.tcw;\n\tint key_offset = cc->key_size - cc->iv_size - TCW_WHITENING_SIZE;\n\n\tmemcpy(tcw->iv_seed, &cc->key[key_offset], cc->iv_size);\n\tmemcpy(tcw->whitening, &cc->key[key_offset + cc->iv_size],\n\t       TCW_WHITENING_SIZE);\n\n\treturn 0;\n}\n\nstatic int crypt_iv_tcw_wipe(struct crypt_config *cc)\n{\n\tstruct iv_tcw_private *tcw = &cc->iv_gen_private.tcw;\n\n\tmemset(tcw->iv_seed, 0, cc->iv_size);\n\tmemset(tcw->whitening, 0, TCW_WHITENING_SIZE);\n\n\treturn 0;\n}\n\nstatic int crypt_iv_tcw_whitening(struct crypt_config *cc,\n\t\t\t\t  struct dm_crypt_request *dmreq,\n\t\t\t\t  u8 *data)\n{\n\tstruct iv_tcw_private *tcw = &cc->iv_gen_private.tcw;\n\t__le64 sector = cpu_to_le64(dmreq->iv_sector);\n\tu8 buf[TCW_WHITENING_SIZE];\n\tSHASH_DESC_ON_STACK(desc, tcw->crc32_tfm);\n\tint i, r;\n\n\t \n\tcrypto_xor_cpy(buf, tcw->whitening, (u8 *)&sector, 8);\n\tcrypto_xor_cpy(&buf[8], tcw->whitening + 8, (u8 *)&sector, 8);\n\n\t \n\tdesc->tfm = tcw->crc32_tfm;\n\tfor (i = 0; i < 4; i++) {\n\t\tr = crypto_shash_init(desc);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = crypto_shash_update(desc, &buf[i * 4], 4);\n\t\tif (r)\n\t\t\tgoto out;\n\t\tr = crypto_shash_final(desc, &buf[i * 4]);\n\t\tif (r)\n\t\t\tgoto out;\n\t}\n\tcrypto_xor(&buf[0], &buf[12], 4);\n\tcrypto_xor(&buf[4], &buf[8], 4);\n\n\t \n\tfor (i = 0; i < ((1 << SECTOR_SHIFT) / 8); i++)\n\t\tcrypto_xor(data + i * 8, buf, 8);\nout:\n\tmemzero_explicit(buf, sizeof(buf));\n\treturn r;\n}\n\nstatic int crypt_iv_tcw_gen(struct crypt_config *cc, u8 *iv,\n\t\t\t    struct dm_crypt_request *dmreq)\n{\n\tstruct scatterlist *sg;\n\tstruct iv_tcw_private *tcw = &cc->iv_gen_private.tcw;\n\t__le64 sector = cpu_to_le64(dmreq->iv_sector);\n\tu8 *src;\n\tint r = 0;\n\n\t \n\tif (bio_data_dir(dmreq->ctx->bio_in) != WRITE) {\n\t\tsg = crypt_get_sg_data(cc, dmreq->sg_in);\n\t\tsrc = kmap_local_page(sg_page(sg));\n\t\tr = crypt_iv_tcw_whitening(cc, dmreq, src + sg->offset);\n\t\tkunmap_local(src);\n\t}\n\n\t \n\tcrypto_xor_cpy(iv, tcw->iv_seed, (u8 *)&sector, 8);\n\tif (cc->iv_size > 8)\n\t\tcrypto_xor_cpy(&iv[8], tcw->iv_seed + 8, (u8 *)&sector,\n\t\t\t       cc->iv_size - 8);\n\n\treturn r;\n}\n\nstatic int crypt_iv_tcw_post(struct crypt_config *cc, u8 *iv,\n\t\t\t     struct dm_crypt_request *dmreq)\n{\n\tstruct scatterlist *sg;\n\tu8 *dst;\n\tint r;\n\n\tif (bio_data_dir(dmreq->ctx->bio_in) != WRITE)\n\t\treturn 0;\n\n\t \n\tsg = crypt_get_sg_data(cc, dmreq->sg_out);\n\tdst = kmap_local_page(sg_page(sg));\n\tr = crypt_iv_tcw_whitening(cc, dmreq, dst + sg->offset);\n\tkunmap_local(dst);\n\n\treturn r;\n}\n\nstatic int crypt_iv_random_gen(struct crypt_config *cc, u8 *iv,\n\t\t\t\tstruct dm_crypt_request *dmreq)\n{\n\t \n\tget_random_bytes(iv, cc->iv_size);\n\treturn 0;\n}\n\nstatic int crypt_iv_eboiv_ctr(struct crypt_config *cc, struct dm_target *ti,\n\t\t\t    const char *opts)\n{\n\tif (crypt_integrity_aead(cc)) {\n\t\tti->error = \"AEAD transforms not supported for EBOIV\";\n\t\treturn -EINVAL;\n\t}\n\n\tif (crypto_skcipher_blocksize(any_tfm(cc)) != cc->iv_size) {\n\t\tti->error = \"Block size of EBOIV cipher does not match IV size of block cipher\";\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int crypt_iv_eboiv_gen(struct crypt_config *cc, u8 *iv,\n\t\t\t    struct dm_crypt_request *dmreq)\n{\n\tstruct crypto_skcipher *tfm = any_tfm(cc);\n\tstruct skcipher_request *req;\n\tstruct scatterlist src, dst;\n\tDECLARE_CRYPTO_WAIT(wait);\n\tunsigned int reqsize;\n\tint err;\n\tu8 *buf;\n\n\treqsize = sizeof(*req) + crypto_skcipher_reqsize(tfm);\n\treqsize = ALIGN(reqsize, __alignof__(__le64));\n\n\treq = kmalloc(reqsize + cc->iv_size, GFP_NOIO);\n\tif (!req)\n\t\treturn -ENOMEM;\n\n\tskcipher_request_set_tfm(req, tfm);\n\n\tbuf = (u8 *)req + reqsize;\n\tmemset(buf, 0, cc->iv_size);\n\t*(__le64 *)buf = cpu_to_le64(dmreq->iv_sector * cc->sector_size);\n\n\tsg_init_one(&src, page_address(ZERO_PAGE(0)), cc->iv_size);\n\tsg_init_one(&dst, iv, cc->iv_size);\n\tskcipher_request_set_crypt(req, &src, &dst, cc->iv_size, buf);\n\tskcipher_request_set_callback(req, 0, crypto_req_done, &wait);\n\terr = crypto_wait_req(crypto_skcipher_encrypt(req), &wait);\n\tkfree_sensitive(req);\n\n\treturn err;\n}\n\nstatic void crypt_iv_elephant_dtr(struct crypt_config *cc)\n{\n\tstruct iv_elephant_private *elephant = &cc->iv_gen_private.elephant;\n\n\tcrypto_free_skcipher(elephant->tfm);\n\telephant->tfm = NULL;\n}\n\nstatic int crypt_iv_elephant_ctr(struct crypt_config *cc, struct dm_target *ti,\n\t\t\t    const char *opts)\n{\n\tstruct iv_elephant_private *elephant = &cc->iv_gen_private.elephant;\n\tint r;\n\n\telephant->tfm = crypto_alloc_skcipher(\"ecb(aes)\", 0,\n\t\t\t\t\t      CRYPTO_ALG_ALLOCATES_MEMORY);\n\tif (IS_ERR(elephant->tfm)) {\n\t\tr = PTR_ERR(elephant->tfm);\n\t\telephant->tfm = NULL;\n\t\treturn r;\n\t}\n\n\tr = crypt_iv_eboiv_ctr(cc, ti, NULL);\n\tif (r)\n\t\tcrypt_iv_elephant_dtr(cc);\n\treturn r;\n}\n\nstatic void diffuser_disk_to_cpu(u32 *d, size_t n)\n{\n#ifndef __LITTLE_ENDIAN\n\tint i;\n\n\tfor (i = 0; i < n; i++)\n\t\td[i] = le32_to_cpu((__le32)d[i]);\n#endif\n}\n\nstatic void diffuser_cpu_to_disk(__le32 *d, size_t n)\n{\n#ifndef __LITTLE_ENDIAN\n\tint i;\n\n\tfor (i = 0; i < n; i++)\n\t\td[i] = cpu_to_le32((u32)d[i]);\n#endif\n}\n\nstatic void diffuser_a_decrypt(u32 *d, size_t n)\n{\n\tint i, i1, i2, i3;\n\n\tfor (i = 0; i < 5; i++) {\n\t\ti1 = 0;\n\t\ti2 = n - 2;\n\t\ti3 = n - 5;\n\n\t\twhile (i1 < (n - 1)) {\n\t\t\td[i1] += d[i2] ^ (d[i3] << 9 | d[i3] >> 23);\n\t\t\ti1++; i2++; i3++;\n\n\t\t\tif (i3 >= n)\n\t\t\t\ti3 -= n;\n\n\t\t\td[i1] += d[i2] ^ d[i3];\n\t\t\ti1++; i2++; i3++;\n\n\t\t\tif (i2 >= n)\n\t\t\t\ti2 -= n;\n\n\t\t\td[i1] += d[i2] ^ (d[i3] << 13 | d[i3] >> 19);\n\t\t\ti1++; i2++; i3++;\n\n\t\t\td[i1] += d[i2] ^ d[i3];\n\t\t\ti1++; i2++; i3++;\n\t\t}\n\t}\n}\n\nstatic void diffuser_a_encrypt(u32 *d, size_t n)\n{\n\tint i, i1, i2, i3;\n\n\tfor (i = 0; i < 5; i++) {\n\t\ti1 = n - 1;\n\t\ti2 = n - 2 - 1;\n\t\ti3 = n - 5 - 1;\n\n\t\twhile (i1 > 0) {\n\t\t\td[i1] -= d[i2] ^ d[i3];\n\t\t\ti1--; i2--; i3--;\n\n\t\t\td[i1] -= d[i2] ^ (d[i3] << 13 | d[i3] >> 19);\n\t\t\ti1--; i2--; i3--;\n\n\t\t\tif (i2 < 0)\n\t\t\t\ti2 += n;\n\n\t\t\td[i1] -= d[i2] ^ d[i3];\n\t\t\ti1--; i2--; i3--;\n\n\t\t\tif (i3 < 0)\n\t\t\t\ti3 += n;\n\n\t\t\td[i1] -= d[i2] ^ (d[i3] << 9 | d[i3] >> 23);\n\t\t\ti1--; i2--; i3--;\n\t\t}\n\t}\n}\n\nstatic void diffuser_b_decrypt(u32 *d, size_t n)\n{\n\tint i, i1, i2, i3;\n\n\tfor (i = 0; i < 3; i++) {\n\t\ti1 = 0;\n\t\ti2 = 2;\n\t\ti3 = 5;\n\n\t\twhile (i1 < (n - 1)) {\n\t\t\td[i1] += d[i2] ^ d[i3];\n\t\t\ti1++; i2++; i3++;\n\n\t\t\td[i1] += d[i2] ^ (d[i3] << 10 | d[i3] >> 22);\n\t\t\ti1++; i2++; i3++;\n\n\t\t\tif (i2 >= n)\n\t\t\t\ti2 -= n;\n\n\t\t\td[i1] += d[i2] ^ d[i3];\n\t\t\ti1++; i2++; i3++;\n\n\t\t\tif (i3 >= n)\n\t\t\t\ti3 -= n;\n\n\t\t\td[i1] += d[i2] ^ (d[i3] << 25 | d[i3] >> 7);\n\t\t\ti1++; i2++; i3++;\n\t\t}\n\t}\n}\n\nstatic void diffuser_b_encrypt(u32 *d, size_t n)\n{\n\tint i, i1, i2, i3;\n\n\tfor (i = 0; i < 3; i++) {\n\t\ti1 = n - 1;\n\t\ti2 = 2 - 1;\n\t\ti3 = 5 - 1;\n\n\t\twhile (i1 > 0) {\n\t\t\td[i1] -= d[i2] ^ (d[i3] << 25 | d[i3] >> 7);\n\t\t\ti1--; i2--; i3--;\n\n\t\t\tif (i3 < 0)\n\t\t\t\ti3 += n;\n\n\t\t\td[i1] -= d[i2] ^ d[i3];\n\t\t\ti1--; i2--; i3--;\n\n\t\t\tif (i2 < 0)\n\t\t\t\ti2 += n;\n\n\t\t\td[i1] -= d[i2] ^ (d[i3] << 10 | d[i3] >> 22);\n\t\t\ti1--; i2--; i3--;\n\n\t\t\td[i1] -= d[i2] ^ d[i3];\n\t\t\ti1--; i2--; i3--;\n\t\t}\n\t}\n}\n\nstatic int crypt_iv_elephant(struct crypt_config *cc, struct dm_crypt_request *dmreq)\n{\n\tstruct iv_elephant_private *elephant = &cc->iv_gen_private.elephant;\n\tu8 *es, *ks, *data, *data2, *data_offset;\n\tstruct skcipher_request *req;\n\tstruct scatterlist *sg, *sg2, src, dst;\n\tDECLARE_CRYPTO_WAIT(wait);\n\tint i, r;\n\n\treq = skcipher_request_alloc(elephant->tfm, GFP_NOIO);\n\tes = kzalloc(16, GFP_NOIO);  \n\tks = kzalloc(32, GFP_NOIO);  \n\n\tif (!req || !es || !ks) {\n\t\tr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t*(__le64 *)es = cpu_to_le64(dmreq->iv_sector * cc->sector_size);\n\n\t \n\tsg_init_one(&src, es, 16);\n\tsg_init_one(&dst, ks, 16);\n\tskcipher_request_set_crypt(req, &src, &dst, 16, NULL);\n\tskcipher_request_set_callback(req, 0, crypto_req_done, &wait);\n\tr = crypto_wait_req(crypto_skcipher_encrypt(req), &wait);\n\tif (r)\n\t\tgoto out;\n\n\t \n\tes[15] = 0x80;\n\tsg_init_one(&dst, &ks[16], 16);\n\tr = crypto_wait_req(crypto_skcipher_encrypt(req), &wait);\n\tif (r)\n\t\tgoto out;\n\n\tsg = crypt_get_sg_data(cc, dmreq->sg_out);\n\tdata = kmap_local_page(sg_page(sg));\n\tdata_offset = data + sg->offset;\n\n\t \n\tif (bio_data_dir(dmreq->ctx->bio_in) == WRITE) {\n\t\tsg2 = crypt_get_sg_data(cc, dmreq->sg_in);\n\t\tdata2 = kmap_local_page(sg_page(sg2));\n\t\tmemcpy(data_offset, data2 + sg2->offset, cc->sector_size);\n\t\tkunmap_local(data2);\n\t}\n\n\tif (bio_data_dir(dmreq->ctx->bio_in) != WRITE) {\n\t\tdiffuser_disk_to_cpu((u32 *)data_offset, cc->sector_size / sizeof(u32));\n\t\tdiffuser_b_decrypt((u32 *)data_offset, cc->sector_size / sizeof(u32));\n\t\tdiffuser_a_decrypt((u32 *)data_offset, cc->sector_size / sizeof(u32));\n\t\tdiffuser_cpu_to_disk((__le32 *)data_offset, cc->sector_size / sizeof(u32));\n\t}\n\n\tfor (i = 0; i < (cc->sector_size / 32); i++)\n\t\tcrypto_xor(data_offset + i * 32, ks, 32);\n\n\tif (bio_data_dir(dmreq->ctx->bio_in) == WRITE) {\n\t\tdiffuser_disk_to_cpu((u32 *)data_offset, cc->sector_size / sizeof(u32));\n\t\tdiffuser_a_encrypt((u32 *)data_offset, cc->sector_size / sizeof(u32));\n\t\tdiffuser_b_encrypt((u32 *)data_offset, cc->sector_size / sizeof(u32));\n\t\tdiffuser_cpu_to_disk((__le32 *)data_offset, cc->sector_size / sizeof(u32));\n\t}\n\n\tkunmap_local(data);\nout:\n\tkfree_sensitive(ks);\n\tkfree_sensitive(es);\n\tskcipher_request_free(req);\n\treturn r;\n}\n\nstatic int crypt_iv_elephant_gen(struct crypt_config *cc, u8 *iv,\n\t\t\t    struct dm_crypt_request *dmreq)\n{\n\tint r;\n\n\tif (bio_data_dir(dmreq->ctx->bio_in) == WRITE) {\n\t\tr = crypt_iv_elephant(cc, dmreq);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn crypt_iv_eboiv_gen(cc, iv, dmreq);\n}\n\nstatic int crypt_iv_elephant_post(struct crypt_config *cc, u8 *iv,\n\t\t\t\t  struct dm_crypt_request *dmreq)\n{\n\tif (bio_data_dir(dmreq->ctx->bio_in) != WRITE)\n\t\treturn crypt_iv_elephant(cc, dmreq);\n\n\treturn 0;\n}\n\nstatic int crypt_iv_elephant_init(struct crypt_config *cc)\n{\n\tstruct iv_elephant_private *elephant = &cc->iv_gen_private.elephant;\n\tint key_offset = cc->key_size - cc->key_extra_size;\n\n\treturn crypto_skcipher_setkey(elephant->tfm, &cc->key[key_offset], cc->key_extra_size);\n}\n\nstatic int crypt_iv_elephant_wipe(struct crypt_config *cc)\n{\n\tstruct iv_elephant_private *elephant = &cc->iv_gen_private.elephant;\n\tu8 key[ELEPHANT_MAX_KEY_SIZE];\n\n\tmemset(key, 0, cc->key_extra_size);\n\treturn crypto_skcipher_setkey(elephant->tfm, key, cc->key_extra_size);\n}\n\nstatic const struct crypt_iv_operations crypt_iv_plain_ops = {\n\t.generator = crypt_iv_plain_gen\n};\n\nstatic const struct crypt_iv_operations crypt_iv_plain64_ops = {\n\t.generator = crypt_iv_plain64_gen\n};\n\nstatic const struct crypt_iv_operations crypt_iv_plain64be_ops = {\n\t.generator = crypt_iv_plain64be_gen\n};\n\nstatic const struct crypt_iv_operations crypt_iv_essiv_ops = {\n\t.generator = crypt_iv_essiv_gen\n};\n\nstatic const struct crypt_iv_operations crypt_iv_benbi_ops = {\n\t.ctr\t   = crypt_iv_benbi_ctr,\n\t.dtr\t   = crypt_iv_benbi_dtr,\n\t.generator = crypt_iv_benbi_gen\n};\n\nstatic const struct crypt_iv_operations crypt_iv_null_ops = {\n\t.generator = crypt_iv_null_gen\n};\n\nstatic const struct crypt_iv_operations crypt_iv_lmk_ops = {\n\t.ctr\t   = crypt_iv_lmk_ctr,\n\t.dtr\t   = crypt_iv_lmk_dtr,\n\t.init\t   = crypt_iv_lmk_init,\n\t.wipe\t   = crypt_iv_lmk_wipe,\n\t.generator = crypt_iv_lmk_gen,\n\t.post\t   = crypt_iv_lmk_post\n};\n\nstatic const struct crypt_iv_operations crypt_iv_tcw_ops = {\n\t.ctr\t   = crypt_iv_tcw_ctr,\n\t.dtr\t   = crypt_iv_tcw_dtr,\n\t.init\t   = crypt_iv_tcw_init,\n\t.wipe\t   = crypt_iv_tcw_wipe,\n\t.generator = crypt_iv_tcw_gen,\n\t.post\t   = crypt_iv_tcw_post\n};\n\nstatic const struct crypt_iv_operations crypt_iv_random_ops = {\n\t.generator = crypt_iv_random_gen\n};\n\nstatic const struct crypt_iv_operations crypt_iv_eboiv_ops = {\n\t.ctr\t   = crypt_iv_eboiv_ctr,\n\t.generator = crypt_iv_eboiv_gen\n};\n\nstatic const struct crypt_iv_operations crypt_iv_elephant_ops = {\n\t.ctr\t   = crypt_iv_elephant_ctr,\n\t.dtr\t   = crypt_iv_elephant_dtr,\n\t.init\t   = crypt_iv_elephant_init,\n\t.wipe\t   = crypt_iv_elephant_wipe,\n\t.generator = crypt_iv_elephant_gen,\n\t.post\t   = crypt_iv_elephant_post\n};\n\n \nstatic bool crypt_integrity_aead(struct crypt_config *cc)\n{\n\treturn test_bit(CRYPT_MODE_INTEGRITY_AEAD, &cc->cipher_flags);\n}\n\nstatic bool crypt_integrity_hmac(struct crypt_config *cc)\n{\n\treturn crypt_integrity_aead(cc) && cc->key_mac_size;\n}\n\n \nstatic struct scatterlist *crypt_get_sg_data(struct crypt_config *cc,\n\t\t\t\t\t     struct scatterlist *sg)\n{\n\tif (unlikely(crypt_integrity_aead(cc)))\n\t\treturn &sg[2];\n\n\treturn sg;\n}\n\nstatic int dm_crypt_integrity_io_alloc(struct dm_crypt_io *io, struct bio *bio)\n{\n\tstruct bio_integrity_payload *bip;\n\tunsigned int tag_len;\n\tint ret;\n\n\tif (!bio_sectors(bio) || !io->cc->on_disk_tag_size)\n\t\treturn 0;\n\n\tbip = bio_integrity_alloc(bio, GFP_NOIO, 1);\n\tif (IS_ERR(bip))\n\t\treturn PTR_ERR(bip);\n\n\ttag_len = io->cc->on_disk_tag_size * (bio_sectors(bio) >> io->cc->sector_shift);\n\n\tbip->bip_iter.bi_sector = io->cc->start + io->sector;\n\n\tret = bio_integrity_add_page(bio, virt_to_page(io->integrity_metadata),\n\t\t\t\t     tag_len, offset_in_page(io->integrity_metadata));\n\tif (unlikely(ret != tag_len))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int crypt_integrity_ctr(struct crypt_config *cc, struct dm_target *ti)\n{\n#ifdef CONFIG_BLK_DEV_INTEGRITY\n\tstruct blk_integrity *bi = blk_get_integrity(cc->dev->bdev->bd_disk);\n\tstruct mapped_device *md = dm_table_get_md(ti->table);\n\n\t \n\tif (!bi || strcasecmp(bi->profile->name, \"DM-DIF-EXT-TAG\")) {\n\t\tti->error = \"Integrity profile not supported.\";\n\t\treturn -EINVAL;\n\t}\n\n\tif (bi->tag_size != cc->on_disk_tag_size ||\n\t    bi->tuple_size != cc->on_disk_tag_size) {\n\t\tti->error = \"Integrity profile tag size mismatch.\";\n\t\treturn -EINVAL;\n\t}\n\tif (1 << bi->interval_exp != cc->sector_size) {\n\t\tti->error = \"Integrity profile sector size mismatch.\";\n\t\treturn -EINVAL;\n\t}\n\n\tif (crypt_integrity_aead(cc)) {\n\t\tcc->integrity_tag_size = cc->on_disk_tag_size - cc->integrity_iv_size;\n\t\tDMDEBUG(\"%s: Integrity AEAD, tag size %u, IV size %u.\", dm_device_name(md),\n\t\t       cc->integrity_tag_size, cc->integrity_iv_size);\n\n\t\tif (crypto_aead_setauthsize(any_tfm_aead(cc), cc->integrity_tag_size)) {\n\t\t\tti->error = \"Integrity AEAD auth tag size is not supported.\";\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (cc->integrity_iv_size)\n\t\tDMDEBUG(\"%s: Additional per-sector space %u bytes for IV.\", dm_device_name(md),\n\t\t       cc->integrity_iv_size);\n\n\tif ((cc->integrity_tag_size + cc->integrity_iv_size) != bi->tag_size) {\n\t\tti->error = \"Not enough space for integrity tag in the profile.\";\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n#else\n\tti->error = \"Integrity profile not supported.\";\n\treturn -EINVAL;\n#endif\n}\n\nstatic void crypt_convert_init(struct crypt_config *cc,\n\t\t\t       struct convert_context *ctx,\n\t\t\t       struct bio *bio_out, struct bio *bio_in,\n\t\t\t       sector_t sector)\n{\n\tctx->bio_in = bio_in;\n\tctx->bio_out = bio_out;\n\tif (bio_in)\n\t\tctx->iter_in = bio_in->bi_iter;\n\tif (bio_out)\n\t\tctx->iter_out = bio_out->bi_iter;\n\tctx->cc_sector = sector + cc->iv_offset;\n\tinit_completion(&ctx->restart);\n}\n\nstatic struct dm_crypt_request *dmreq_of_req(struct crypt_config *cc,\n\t\t\t\t\t     void *req)\n{\n\treturn (struct dm_crypt_request *)((char *)req + cc->dmreq_start);\n}\n\nstatic void *req_of_dmreq(struct crypt_config *cc, struct dm_crypt_request *dmreq)\n{\n\treturn (void *)((char *)dmreq - cc->dmreq_start);\n}\n\nstatic u8 *iv_of_dmreq(struct crypt_config *cc,\n\t\t       struct dm_crypt_request *dmreq)\n{\n\tif (crypt_integrity_aead(cc))\n\t\treturn (u8 *)ALIGN((unsigned long)(dmreq + 1),\n\t\t\tcrypto_aead_alignmask(any_tfm_aead(cc)) + 1);\n\telse\n\t\treturn (u8 *)ALIGN((unsigned long)(dmreq + 1),\n\t\t\tcrypto_skcipher_alignmask(any_tfm(cc)) + 1);\n}\n\nstatic u8 *org_iv_of_dmreq(struct crypt_config *cc,\n\t\t       struct dm_crypt_request *dmreq)\n{\n\treturn iv_of_dmreq(cc, dmreq) + cc->iv_size;\n}\n\nstatic __le64 *org_sector_of_dmreq(struct crypt_config *cc,\n\t\t       struct dm_crypt_request *dmreq)\n{\n\tu8 *ptr = iv_of_dmreq(cc, dmreq) + cc->iv_size + cc->iv_size;\n\n\treturn (__le64 *) ptr;\n}\n\nstatic unsigned int *org_tag_of_dmreq(struct crypt_config *cc,\n\t\t       struct dm_crypt_request *dmreq)\n{\n\tu8 *ptr = iv_of_dmreq(cc, dmreq) + cc->iv_size +\n\t\t  cc->iv_size + sizeof(uint64_t);\n\n\treturn (unsigned int *)ptr;\n}\n\nstatic void *tag_from_dmreq(struct crypt_config *cc,\n\t\t\t\tstruct dm_crypt_request *dmreq)\n{\n\tstruct convert_context *ctx = dmreq->ctx;\n\tstruct dm_crypt_io *io = container_of(ctx, struct dm_crypt_io, ctx);\n\n\treturn &io->integrity_metadata[*org_tag_of_dmreq(cc, dmreq) *\n\t\tcc->on_disk_tag_size];\n}\n\nstatic void *iv_tag_from_dmreq(struct crypt_config *cc,\n\t\t\t       struct dm_crypt_request *dmreq)\n{\n\treturn tag_from_dmreq(cc, dmreq) + cc->integrity_tag_size;\n}\n\nstatic int crypt_convert_block_aead(struct crypt_config *cc,\n\t\t\t\t     struct convert_context *ctx,\n\t\t\t\t     struct aead_request *req,\n\t\t\t\t     unsigned int tag_offset)\n{\n\tstruct bio_vec bv_in = bio_iter_iovec(ctx->bio_in, ctx->iter_in);\n\tstruct bio_vec bv_out = bio_iter_iovec(ctx->bio_out, ctx->iter_out);\n\tstruct dm_crypt_request *dmreq;\n\tu8 *iv, *org_iv, *tag_iv, *tag;\n\t__le64 *sector;\n\tint r = 0;\n\n\tBUG_ON(cc->integrity_iv_size && cc->integrity_iv_size != cc->iv_size);\n\n\t \n\tif (unlikely(bv_in.bv_len & (cc->sector_size - 1)))\n\t\treturn -EIO;\n\n\tdmreq = dmreq_of_req(cc, req);\n\tdmreq->iv_sector = ctx->cc_sector;\n\tif (test_bit(CRYPT_IV_LARGE_SECTORS, &cc->cipher_flags))\n\t\tdmreq->iv_sector >>= cc->sector_shift;\n\tdmreq->ctx = ctx;\n\n\t*org_tag_of_dmreq(cc, dmreq) = tag_offset;\n\n\tsector = org_sector_of_dmreq(cc, dmreq);\n\t*sector = cpu_to_le64(ctx->cc_sector - cc->iv_offset);\n\n\tiv = iv_of_dmreq(cc, dmreq);\n\torg_iv = org_iv_of_dmreq(cc, dmreq);\n\ttag = tag_from_dmreq(cc, dmreq);\n\ttag_iv = iv_tag_from_dmreq(cc, dmreq);\n\n\t \n\tsg_init_table(dmreq->sg_in, 4);\n\tsg_set_buf(&dmreq->sg_in[0], sector, sizeof(uint64_t));\n\tsg_set_buf(&dmreq->sg_in[1], org_iv, cc->iv_size);\n\tsg_set_page(&dmreq->sg_in[2], bv_in.bv_page, cc->sector_size, bv_in.bv_offset);\n\tsg_set_buf(&dmreq->sg_in[3], tag, cc->integrity_tag_size);\n\n\tsg_init_table(dmreq->sg_out, 4);\n\tsg_set_buf(&dmreq->sg_out[0], sector, sizeof(uint64_t));\n\tsg_set_buf(&dmreq->sg_out[1], org_iv, cc->iv_size);\n\tsg_set_page(&dmreq->sg_out[2], bv_out.bv_page, cc->sector_size, bv_out.bv_offset);\n\tsg_set_buf(&dmreq->sg_out[3], tag, cc->integrity_tag_size);\n\n\tif (cc->iv_gen_ops) {\n\t\t \n\t\tif (cc->integrity_iv_size && bio_data_dir(ctx->bio_in) != WRITE) {\n\t\t\tmemcpy(org_iv, tag_iv, cc->iv_size);\n\t\t} else {\n\t\t\tr = cc->iv_gen_ops->generator(cc, org_iv, dmreq);\n\t\t\tif (r < 0)\n\t\t\t\treturn r;\n\t\t\t \n\t\t\tif (cc->integrity_iv_size)\n\t\t\t\tmemcpy(tag_iv, org_iv, cc->iv_size);\n\t\t}\n\t\t \n\t\tmemcpy(iv, org_iv, cc->iv_size);\n\t}\n\n\taead_request_set_ad(req, sizeof(uint64_t) + cc->iv_size);\n\tif (bio_data_dir(ctx->bio_in) == WRITE) {\n\t\taead_request_set_crypt(req, dmreq->sg_in, dmreq->sg_out,\n\t\t\t\t       cc->sector_size, iv);\n\t\tr = crypto_aead_encrypt(req);\n\t\tif (cc->integrity_tag_size + cc->integrity_iv_size != cc->on_disk_tag_size)\n\t\t\tmemset(tag + cc->integrity_tag_size + cc->integrity_iv_size, 0,\n\t\t\t       cc->on_disk_tag_size - (cc->integrity_tag_size + cc->integrity_iv_size));\n\t} else {\n\t\taead_request_set_crypt(req, dmreq->sg_in, dmreq->sg_out,\n\t\t\t\t       cc->sector_size + cc->integrity_tag_size, iv);\n\t\tr = crypto_aead_decrypt(req);\n\t}\n\n\tif (r == -EBADMSG) {\n\t\tsector_t s = le64_to_cpu(*sector);\n\n\t\tDMERR_LIMIT(\"%pg: INTEGRITY AEAD ERROR, sector %llu\",\n\t\t\t    ctx->bio_in->bi_bdev, s);\n\t\tdm_audit_log_bio(DM_MSG_PREFIX, \"integrity-aead\",\n\t\t\t\t ctx->bio_in, s, 0);\n\t}\n\n\tif (!r && cc->iv_gen_ops && cc->iv_gen_ops->post)\n\t\tr = cc->iv_gen_ops->post(cc, org_iv, dmreq);\n\n\tbio_advance_iter(ctx->bio_in, &ctx->iter_in, cc->sector_size);\n\tbio_advance_iter(ctx->bio_out, &ctx->iter_out, cc->sector_size);\n\n\treturn r;\n}\n\nstatic int crypt_convert_block_skcipher(struct crypt_config *cc,\n\t\t\t\t\tstruct convert_context *ctx,\n\t\t\t\t\tstruct skcipher_request *req,\n\t\t\t\t\tunsigned int tag_offset)\n{\n\tstruct bio_vec bv_in = bio_iter_iovec(ctx->bio_in, ctx->iter_in);\n\tstruct bio_vec bv_out = bio_iter_iovec(ctx->bio_out, ctx->iter_out);\n\tstruct scatterlist *sg_in, *sg_out;\n\tstruct dm_crypt_request *dmreq;\n\tu8 *iv, *org_iv, *tag_iv;\n\t__le64 *sector;\n\tint r = 0;\n\n\t \n\tif (unlikely(bv_in.bv_len & (cc->sector_size - 1)))\n\t\treturn -EIO;\n\n\tdmreq = dmreq_of_req(cc, req);\n\tdmreq->iv_sector = ctx->cc_sector;\n\tif (test_bit(CRYPT_IV_LARGE_SECTORS, &cc->cipher_flags))\n\t\tdmreq->iv_sector >>= cc->sector_shift;\n\tdmreq->ctx = ctx;\n\n\t*org_tag_of_dmreq(cc, dmreq) = tag_offset;\n\n\tiv = iv_of_dmreq(cc, dmreq);\n\torg_iv = org_iv_of_dmreq(cc, dmreq);\n\ttag_iv = iv_tag_from_dmreq(cc, dmreq);\n\n\tsector = org_sector_of_dmreq(cc, dmreq);\n\t*sector = cpu_to_le64(ctx->cc_sector - cc->iv_offset);\n\n\t \n\tsg_in  = &dmreq->sg_in[0];\n\tsg_out = &dmreq->sg_out[0];\n\n\tsg_init_table(sg_in, 1);\n\tsg_set_page(sg_in, bv_in.bv_page, cc->sector_size, bv_in.bv_offset);\n\n\tsg_init_table(sg_out, 1);\n\tsg_set_page(sg_out, bv_out.bv_page, cc->sector_size, bv_out.bv_offset);\n\n\tif (cc->iv_gen_ops) {\n\t\t \n\t\tif (cc->integrity_iv_size && bio_data_dir(ctx->bio_in) != WRITE) {\n\t\t\tmemcpy(org_iv, tag_iv, cc->integrity_iv_size);\n\t\t} else {\n\t\t\tr = cc->iv_gen_ops->generator(cc, org_iv, dmreq);\n\t\t\tif (r < 0)\n\t\t\t\treturn r;\n\t\t\t \n\t\t\tif (test_bit(CRYPT_ENCRYPT_PREPROCESS, &cc->cipher_flags))\n\t\t\t\tsg_in = sg_out;\n\t\t\t \n\t\t\tif (cc->integrity_iv_size)\n\t\t\t\tmemcpy(tag_iv, org_iv, cc->integrity_iv_size);\n\t\t}\n\t\t \n\t\tmemcpy(iv, org_iv, cc->iv_size);\n\t}\n\n\tskcipher_request_set_crypt(req, sg_in, sg_out, cc->sector_size, iv);\n\n\tif (bio_data_dir(ctx->bio_in) == WRITE)\n\t\tr = crypto_skcipher_encrypt(req);\n\telse\n\t\tr = crypto_skcipher_decrypt(req);\n\n\tif (!r && cc->iv_gen_ops && cc->iv_gen_ops->post)\n\t\tr = cc->iv_gen_ops->post(cc, org_iv, dmreq);\n\n\tbio_advance_iter(ctx->bio_in, &ctx->iter_in, cc->sector_size);\n\tbio_advance_iter(ctx->bio_out, &ctx->iter_out, cc->sector_size);\n\n\treturn r;\n}\n\nstatic void kcryptd_async_done(void *async_req, int error);\n\nstatic int crypt_alloc_req_skcipher(struct crypt_config *cc,\n\t\t\t\t     struct convert_context *ctx)\n{\n\tunsigned int key_index = ctx->cc_sector & (cc->tfms_count - 1);\n\n\tif (!ctx->r.req) {\n\t\tctx->r.req = mempool_alloc(&cc->req_pool, in_interrupt() ? GFP_ATOMIC : GFP_NOIO);\n\t\tif (!ctx->r.req)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tskcipher_request_set_tfm(ctx->r.req, cc->cipher_tfm.tfms[key_index]);\n\n\t \n\tskcipher_request_set_callback(ctx->r.req,\n\t    CRYPTO_TFM_REQ_MAY_BACKLOG,\n\t    kcryptd_async_done, dmreq_of_req(cc, ctx->r.req));\n\n\treturn 0;\n}\n\nstatic int crypt_alloc_req_aead(struct crypt_config *cc,\n\t\t\t\t struct convert_context *ctx)\n{\n\tif (!ctx->r.req_aead) {\n\t\tctx->r.req_aead = mempool_alloc(&cc->req_pool, in_interrupt() ? GFP_ATOMIC : GFP_NOIO);\n\t\tif (!ctx->r.req_aead)\n\t\t\treturn -ENOMEM;\n\t}\n\n\taead_request_set_tfm(ctx->r.req_aead, cc->cipher_tfm.tfms_aead[0]);\n\n\t \n\taead_request_set_callback(ctx->r.req_aead,\n\t    CRYPTO_TFM_REQ_MAY_BACKLOG,\n\t    kcryptd_async_done, dmreq_of_req(cc, ctx->r.req_aead));\n\n\treturn 0;\n}\n\nstatic int crypt_alloc_req(struct crypt_config *cc,\n\t\t\t    struct convert_context *ctx)\n{\n\tif (crypt_integrity_aead(cc))\n\t\treturn crypt_alloc_req_aead(cc, ctx);\n\telse\n\t\treturn crypt_alloc_req_skcipher(cc, ctx);\n}\n\nstatic void crypt_free_req_skcipher(struct crypt_config *cc,\n\t\t\t\t    struct skcipher_request *req, struct bio *base_bio)\n{\n\tstruct dm_crypt_io *io = dm_per_bio_data(base_bio, cc->per_bio_data_size);\n\n\tif ((struct skcipher_request *)(io + 1) != req)\n\t\tmempool_free(req, &cc->req_pool);\n}\n\nstatic void crypt_free_req_aead(struct crypt_config *cc,\n\t\t\t\tstruct aead_request *req, struct bio *base_bio)\n{\n\tstruct dm_crypt_io *io = dm_per_bio_data(base_bio, cc->per_bio_data_size);\n\n\tif ((struct aead_request *)(io + 1) != req)\n\t\tmempool_free(req, &cc->req_pool);\n}\n\nstatic void crypt_free_req(struct crypt_config *cc, void *req, struct bio *base_bio)\n{\n\tif (crypt_integrity_aead(cc))\n\t\tcrypt_free_req_aead(cc, req, base_bio);\n\telse\n\t\tcrypt_free_req_skcipher(cc, req, base_bio);\n}\n\n \nstatic blk_status_t crypt_convert(struct crypt_config *cc,\n\t\t\t struct convert_context *ctx, bool atomic, bool reset_pending)\n{\n\tunsigned int tag_offset = 0;\n\tunsigned int sector_step = cc->sector_size >> SECTOR_SHIFT;\n\tint r;\n\n\t \n\tif (reset_pending)\n\t\tatomic_set(&ctx->cc_pending, 1);\n\n\twhile (ctx->iter_in.bi_size && ctx->iter_out.bi_size) {\n\n\t\tr = crypt_alloc_req(cc, ctx);\n\t\tif (r) {\n\t\t\tcomplete(&ctx->restart);\n\t\t\treturn BLK_STS_DEV_RESOURCE;\n\t\t}\n\n\t\tatomic_inc(&ctx->cc_pending);\n\n\t\tif (crypt_integrity_aead(cc))\n\t\t\tr = crypt_convert_block_aead(cc, ctx, ctx->r.req_aead, tag_offset);\n\t\telse\n\t\t\tr = crypt_convert_block_skcipher(cc, ctx, ctx->r.req, tag_offset);\n\n\t\tswitch (r) {\n\t\t \n\t\tcase -EBUSY:\n\t\t\tif (in_interrupt()) {\n\t\t\t\tif (try_wait_for_completion(&ctx->restart)) {\n\t\t\t\t\t \n\t\t\t\t} else {\n\t\t\t\t\t \n\t\t\t\t\tctx->r.req = NULL;\n\t\t\t\t\tctx->cc_sector += sector_step;\n\t\t\t\t\ttag_offset++;\n\t\t\t\t\treturn BLK_STS_DEV_RESOURCE;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\twait_for_completion(&ctx->restart);\n\t\t\t}\n\t\t\treinit_completion(&ctx->restart);\n\t\t\tfallthrough;\n\t\t \n\t\tcase -EINPROGRESS:\n\t\t\tctx->r.req = NULL;\n\t\t\tctx->cc_sector += sector_step;\n\t\t\ttag_offset++;\n\t\t\tcontinue;\n\t\t \n\t\tcase 0:\n\t\t\tatomic_dec(&ctx->cc_pending);\n\t\t\tctx->cc_sector += sector_step;\n\t\t\ttag_offset++;\n\t\t\tif (!atomic)\n\t\t\t\tcond_resched();\n\t\t\tcontinue;\n\t\t \n\t\tcase -EBADMSG:\n\t\t\tatomic_dec(&ctx->cc_pending);\n\t\t\treturn BLK_STS_PROTECTION;\n\t\t \n\t\tdefault:\n\t\t\tatomic_dec(&ctx->cc_pending);\n\t\t\treturn BLK_STS_IOERR;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void crypt_free_buffer_pages(struct crypt_config *cc, struct bio *clone);\n\n \nstatic struct bio *crypt_alloc_buffer(struct dm_crypt_io *io, unsigned int size)\n{\n\tstruct crypt_config *cc = io->cc;\n\tstruct bio *clone;\n\tunsigned int nr_iovecs = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tgfp_t gfp_mask = GFP_NOWAIT | __GFP_HIGHMEM;\n\tunsigned int remaining_size;\n\tunsigned int order = MAX_ORDER;\n\nretry:\n\tif (unlikely(gfp_mask & __GFP_DIRECT_RECLAIM))\n\t\tmutex_lock(&cc->bio_alloc_lock);\n\n\tclone = bio_alloc_bioset(cc->dev->bdev, nr_iovecs, io->base_bio->bi_opf,\n\t\t\t\t GFP_NOIO, &cc->bs);\n\tclone->bi_private = io;\n\tclone->bi_end_io = crypt_endio;\n\n\tremaining_size = size;\n\n\twhile (remaining_size) {\n\t\tstruct page *pages;\n\t\tunsigned size_to_add;\n\t\tunsigned remaining_order = __fls((remaining_size + PAGE_SIZE - 1) >> PAGE_SHIFT);\n\t\torder = min(order, remaining_order);\n\n\t\twhile (order > 0) {\n\t\t\tif (unlikely(percpu_counter_read_positive(&cc->n_allocated_pages) +\n\t\t\t\t\t(1 << order) > dm_crypt_pages_per_client))\n\t\t\t\tgoto decrease_order;\n\t\t\tpages = alloc_pages(gfp_mask\n\t\t\t\t| __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN | __GFP_COMP,\n\t\t\t\torder);\n\t\t\tif (likely(pages != NULL)) {\n\t\t\t\tpercpu_counter_add(&cc->n_allocated_pages, 1 << order);\n\t\t\t\tgoto have_pages;\n\t\t\t}\ndecrease_order:\n\t\t\torder--;\n\t\t}\n\n\t\tpages = mempool_alloc(&cc->page_pool, gfp_mask);\n\t\tif (!pages) {\n\t\t\tcrypt_free_buffer_pages(cc, clone);\n\t\t\tbio_put(clone);\n\t\t\tgfp_mask |= __GFP_DIRECT_RECLAIM;\n\t\t\torder = 0;\n\t\t\tgoto retry;\n\t\t}\n\nhave_pages:\n\t\tsize_to_add = min((unsigned)PAGE_SIZE << order, remaining_size);\n\t\t__bio_add_page(clone, pages, size_to_add, 0);\n\t\tremaining_size -= size_to_add;\n\t}\n\n\t \n\tif (dm_crypt_integrity_io_alloc(io, clone)) {\n\t\tcrypt_free_buffer_pages(cc, clone);\n\t\tbio_put(clone);\n\t\tclone = NULL;\n\t}\n\n\tif (unlikely(gfp_mask & __GFP_DIRECT_RECLAIM))\n\t\tmutex_unlock(&cc->bio_alloc_lock);\n\n\treturn clone;\n}\n\nstatic void crypt_free_buffer_pages(struct crypt_config *cc, struct bio *clone)\n{\n\tstruct folio_iter fi;\n\n\tif (clone->bi_vcnt > 0) {  \n\t\tbio_for_each_folio_all(fi, clone) {\n\t\t\tif (folio_test_large(fi.folio)) {\n\t\t\t\tpercpu_counter_sub(&cc->n_allocated_pages,\n\t\t\t\t\t\t1 << folio_order(fi.folio));\n\t\t\t\tfolio_put(fi.folio);\n\t\t\t} else {\n\t\t\t\tmempool_free(&fi.folio->page, &cc->page_pool);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void crypt_io_init(struct dm_crypt_io *io, struct crypt_config *cc,\n\t\t\t  struct bio *bio, sector_t sector)\n{\n\tio->cc = cc;\n\tio->base_bio = bio;\n\tio->sector = sector;\n\tio->error = 0;\n\tio->ctx.r.req = NULL;\n\tio->integrity_metadata = NULL;\n\tio->integrity_metadata_from_pool = false;\n\tio->in_tasklet = false;\n\tatomic_set(&io->io_pending, 0);\n}\n\nstatic void crypt_inc_pending(struct dm_crypt_io *io)\n{\n\tatomic_inc(&io->io_pending);\n}\n\nstatic void kcryptd_io_bio_endio(struct work_struct *work)\n{\n\tstruct dm_crypt_io *io = container_of(work, struct dm_crypt_io, work);\n\n\tbio_endio(io->base_bio);\n}\n\n \nstatic void crypt_dec_pending(struct dm_crypt_io *io)\n{\n\tstruct crypt_config *cc = io->cc;\n\tstruct bio *base_bio = io->base_bio;\n\tblk_status_t error = io->error;\n\n\tif (!atomic_dec_and_test(&io->io_pending))\n\t\treturn;\n\n\tif (io->ctx.r.req)\n\t\tcrypt_free_req(cc, io->ctx.r.req, base_bio);\n\n\tif (unlikely(io->integrity_metadata_from_pool))\n\t\tmempool_free(io->integrity_metadata, &io->cc->tag_pool);\n\telse\n\t\tkfree(io->integrity_metadata);\n\n\tbase_bio->bi_status = error;\n\n\t \n\tif (io->in_tasklet) {\n\t\tINIT_WORK(&io->work, kcryptd_io_bio_endio);\n\t\tqueue_work(cc->io_queue, &io->work);\n\t\treturn;\n\t}\n\n\tbio_endio(base_bio);\n}\n\n \nstatic void crypt_endio(struct bio *clone)\n{\n\tstruct dm_crypt_io *io = clone->bi_private;\n\tstruct crypt_config *cc = io->cc;\n\tunsigned int rw = bio_data_dir(clone);\n\tblk_status_t error;\n\n\t \n\tif (rw == WRITE)\n\t\tcrypt_free_buffer_pages(cc, clone);\n\n\terror = clone->bi_status;\n\tbio_put(clone);\n\n\tif (rw == READ && !error) {\n\t\tkcryptd_queue_crypt(io);\n\t\treturn;\n\t}\n\n\tif (unlikely(error))\n\t\tio->error = error;\n\n\tcrypt_dec_pending(io);\n}\n\n#define CRYPT_MAP_READ_GFP GFP_NOWAIT\n\nstatic int kcryptd_io_read(struct dm_crypt_io *io, gfp_t gfp)\n{\n\tstruct crypt_config *cc = io->cc;\n\tstruct bio *clone;\n\n\t \n\tclone = bio_alloc_clone(cc->dev->bdev, io->base_bio, gfp, &cc->bs);\n\tif (!clone)\n\t\treturn 1;\n\tclone->bi_private = io;\n\tclone->bi_end_io = crypt_endio;\n\n\tcrypt_inc_pending(io);\n\n\tclone->bi_iter.bi_sector = cc->start + io->sector;\n\n\tif (dm_crypt_integrity_io_alloc(io, clone)) {\n\t\tcrypt_dec_pending(io);\n\t\tbio_put(clone);\n\t\treturn 1;\n\t}\n\n\tdm_submit_bio_remap(io->base_bio, clone);\n\treturn 0;\n}\n\nstatic void kcryptd_io_read_work(struct work_struct *work)\n{\n\tstruct dm_crypt_io *io = container_of(work, struct dm_crypt_io, work);\n\n\tcrypt_inc_pending(io);\n\tif (kcryptd_io_read(io, GFP_NOIO))\n\t\tio->error = BLK_STS_RESOURCE;\n\tcrypt_dec_pending(io);\n}\n\nstatic void kcryptd_queue_read(struct dm_crypt_io *io)\n{\n\tstruct crypt_config *cc = io->cc;\n\n\tINIT_WORK(&io->work, kcryptd_io_read_work);\n\tqueue_work(cc->io_queue, &io->work);\n}\n\nstatic void kcryptd_io_write(struct dm_crypt_io *io)\n{\n\tstruct bio *clone = io->ctx.bio_out;\n\n\tdm_submit_bio_remap(io->base_bio, clone);\n}\n\n#define crypt_io_from_node(node) rb_entry((node), struct dm_crypt_io, rb_node)\n\nstatic int dmcrypt_write(void *data)\n{\n\tstruct crypt_config *cc = data;\n\tstruct dm_crypt_io *io;\n\n\twhile (1) {\n\t\tstruct rb_root write_tree;\n\t\tstruct blk_plug plug;\n\n\t\tspin_lock_irq(&cc->write_thread_lock);\ncontinue_locked:\n\n\t\tif (!RB_EMPTY_ROOT(&cc->write_tree))\n\t\t\tgoto pop_from_list;\n\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\n\t\tspin_unlock_irq(&cc->write_thread_lock);\n\n\t\tif (unlikely(kthread_should_stop())) {\n\t\t\tset_current_state(TASK_RUNNING);\n\t\t\tbreak;\n\t\t}\n\n\t\tschedule();\n\n\t\tset_current_state(TASK_RUNNING);\n\t\tspin_lock_irq(&cc->write_thread_lock);\n\t\tgoto continue_locked;\n\npop_from_list:\n\t\twrite_tree = cc->write_tree;\n\t\tcc->write_tree = RB_ROOT;\n\t\tspin_unlock_irq(&cc->write_thread_lock);\n\n\t\tBUG_ON(rb_parent(write_tree.rb_node));\n\n\t\t \n\t\tblk_start_plug(&plug);\n\t\tdo {\n\t\t\tio = crypt_io_from_node(rb_first(&write_tree));\n\t\t\trb_erase(&io->rb_node, &write_tree);\n\t\t\tkcryptd_io_write(io);\n\t\t\tcond_resched();\n\t\t} while (!RB_EMPTY_ROOT(&write_tree));\n\t\tblk_finish_plug(&plug);\n\t}\n\treturn 0;\n}\n\nstatic void kcryptd_crypt_write_io_submit(struct dm_crypt_io *io, int async)\n{\n\tstruct bio *clone = io->ctx.bio_out;\n\tstruct crypt_config *cc = io->cc;\n\tunsigned long flags;\n\tsector_t sector;\n\tstruct rb_node **rbp, *parent;\n\n\tif (unlikely(io->error)) {\n\t\tcrypt_free_buffer_pages(cc, clone);\n\t\tbio_put(clone);\n\t\tcrypt_dec_pending(io);\n\t\treturn;\n\t}\n\n\t \n\tBUG_ON(io->ctx.iter_out.bi_size);\n\n\tclone->bi_iter.bi_sector = cc->start + io->sector;\n\n\tif ((likely(!async) && test_bit(DM_CRYPT_NO_OFFLOAD, &cc->flags)) ||\n\t    test_bit(DM_CRYPT_NO_WRITE_WORKQUEUE, &cc->flags)) {\n\t\tdm_submit_bio_remap(io->base_bio, clone);\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&cc->write_thread_lock, flags);\n\tif (RB_EMPTY_ROOT(&cc->write_tree))\n\t\twake_up_process(cc->write_thread);\n\trbp = &cc->write_tree.rb_node;\n\tparent = NULL;\n\tsector = io->sector;\n\twhile (*rbp) {\n\t\tparent = *rbp;\n\t\tif (sector < crypt_io_from_node(parent)->sector)\n\t\t\trbp = &(*rbp)->rb_left;\n\t\telse\n\t\t\trbp = &(*rbp)->rb_right;\n\t}\n\trb_link_node(&io->rb_node, parent, rbp);\n\trb_insert_color(&io->rb_node, &cc->write_tree);\n\tspin_unlock_irqrestore(&cc->write_thread_lock, flags);\n}\n\nstatic bool kcryptd_crypt_write_inline(struct crypt_config *cc,\n\t\t\t\t       struct convert_context *ctx)\n\n{\n\tif (!test_bit(DM_CRYPT_WRITE_INLINE, &cc->flags))\n\t\treturn false;\n\n\t \n\tswitch (bio_op(ctx->bio_in)) {\n\tcase REQ_OP_WRITE:\n\tcase REQ_OP_WRITE_ZEROES:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic void kcryptd_crypt_write_continue(struct work_struct *work)\n{\n\tstruct dm_crypt_io *io = container_of(work, struct dm_crypt_io, work);\n\tstruct crypt_config *cc = io->cc;\n\tstruct convert_context *ctx = &io->ctx;\n\tint crypt_finished;\n\tsector_t sector = io->sector;\n\tblk_status_t r;\n\n\twait_for_completion(&ctx->restart);\n\treinit_completion(&ctx->restart);\n\n\tr = crypt_convert(cc, &io->ctx, true, false);\n\tif (r)\n\t\tio->error = r;\n\tcrypt_finished = atomic_dec_and_test(&ctx->cc_pending);\n\tif (!crypt_finished && kcryptd_crypt_write_inline(cc, ctx)) {\n\t\t \n\t\twait_for_completion(&ctx->restart);\n\t\tcrypt_finished = 1;\n\t}\n\n\t \n\tif (crypt_finished) {\n\t\tkcryptd_crypt_write_io_submit(io, 0);\n\t\tio->sector = sector;\n\t}\n\n\tcrypt_dec_pending(io);\n}\n\nstatic void kcryptd_crypt_write_convert(struct dm_crypt_io *io)\n{\n\tstruct crypt_config *cc = io->cc;\n\tstruct convert_context *ctx = &io->ctx;\n\tstruct bio *clone;\n\tint crypt_finished;\n\tsector_t sector = io->sector;\n\tblk_status_t r;\n\n\t \n\tcrypt_inc_pending(io);\n\tcrypt_convert_init(cc, ctx, NULL, io->base_bio, sector);\n\n\tclone = crypt_alloc_buffer(io, io->base_bio->bi_iter.bi_size);\n\tif (unlikely(!clone)) {\n\t\tio->error = BLK_STS_IOERR;\n\t\tgoto dec;\n\t}\n\n\tio->ctx.bio_out = clone;\n\tio->ctx.iter_out = clone->bi_iter;\n\n\tsector += bio_sectors(clone);\n\n\tcrypt_inc_pending(io);\n\tr = crypt_convert(cc, ctx,\n\t\t\t  test_bit(DM_CRYPT_NO_WRITE_WORKQUEUE, &cc->flags), true);\n\t \n\tif (r == BLK_STS_DEV_RESOURCE) {\n\t\tINIT_WORK(&io->work, kcryptd_crypt_write_continue);\n\t\tqueue_work(cc->crypt_queue, &io->work);\n\t\treturn;\n\t}\n\tif (r)\n\t\tio->error = r;\n\tcrypt_finished = atomic_dec_and_test(&ctx->cc_pending);\n\tif (!crypt_finished && kcryptd_crypt_write_inline(cc, ctx)) {\n\t\t \n\t\twait_for_completion(&ctx->restart);\n\t\tcrypt_finished = 1;\n\t}\n\n\t \n\tif (crypt_finished) {\n\t\tkcryptd_crypt_write_io_submit(io, 0);\n\t\tio->sector = sector;\n\t}\n\ndec:\n\tcrypt_dec_pending(io);\n}\n\nstatic void kcryptd_crypt_read_done(struct dm_crypt_io *io)\n{\n\tcrypt_dec_pending(io);\n}\n\nstatic void kcryptd_crypt_read_continue(struct work_struct *work)\n{\n\tstruct dm_crypt_io *io = container_of(work, struct dm_crypt_io, work);\n\tstruct crypt_config *cc = io->cc;\n\tblk_status_t r;\n\n\twait_for_completion(&io->ctx.restart);\n\treinit_completion(&io->ctx.restart);\n\n\tr = crypt_convert(cc, &io->ctx, true, false);\n\tif (r)\n\t\tio->error = r;\n\n\tif (atomic_dec_and_test(&io->ctx.cc_pending))\n\t\tkcryptd_crypt_read_done(io);\n\n\tcrypt_dec_pending(io);\n}\n\nstatic void kcryptd_crypt_read_convert(struct dm_crypt_io *io)\n{\n\tstruct crypt_config *cc = io->cc;\n\tblk_status_t r;\n\n\tcrypt_inc_pending(io);\n\n\tcrypt_convert_init(cc, &io->ctx, io->base_bio, io->base_bio,\n\t\t\t   io->sector);\n\n\tr = crypt_convert(cc, &io->ctx,\n\t\t\t  test_bit(DM_CRYPT_NO_READ_WORKQUEUE, &cc->flags), true);\n\t \n\tif (r == BLK_STS_DEV_RESOURCE) {\n\t\tINIT_WORK(&io->work, kcryptd_crypt_read_continue);\n\t\tqueue_work(cc->crypt_queue, &io->work);\n\t\treturn;\n\t}\n\tif (r)\n\t\tio->error = r;\n\n\tif (atomic_dec_and_test(&io->ctx.cc_pending))\n\t\tkcryptd_crypt_read_done(io);\n\n\tcrypt_dec_pending(io);\n}\n\nstatic void kcryptd_async_done(void *data, int error)\n{\n\tstruct dm_crypt_request *dmreq = data;\n\tstruct convert_context *ctx = dmreq->ctx;\n\tstruct dm_crypt_io *io = container_of(ctx, struct dm_crypt_io, ctx);\n\tstruct crypt_config *cc = io->cc;\n\n\t \n\tif (error == -EINPROGRESS) {\n\t\tcomplete(&ctx->restart);\n\t\treturn;\n\t}\n\n\tif (!error && cc->iv_gen_ops && cc->iv_gen_ops->post)\n\t\terror = cc->iv_gen_ops->post(cc, org_iv_of_dmreq(cc, dmreq), dmreq);\n\n\tif (error == -EBADMSG) {\n\t\tsector_t s = le64_to_cpu(*org_sector_of_dmreq(cc, dmreq));\n\n\t\tDMERR_LIMIT(\"%pg: INTEGRITY AEAD ERROR, sector %llu\",\n\t\t\t    ctx->bio_in->bi_bdev, s);\n\t\tdm_audit_log_bio(DM_MSG_PREFIX, \"integrity-aead\",\n\t\t\t\t ctx->bio_in, s, 0);\n\t\tio->error = BLK_STS_PROTECTION;\n\t} else if (error < 0)\n\t\tio->error = BLK_STS_IOERR;\n\n\tcrypt_free_req(cc, req_of_dmreq(cc, dmreq), io->base_bio);\n\n\tif (!atomic_dec_and_test(&ctx->cc_pending))\n\t\treturn;\n\n\t \n\tif (bio_data_dir(io->base_bio) == READ) {\n\t\tkcryptd_crypt_read_done(io);\n\t\treturn;\n\t}\n\n\tif (kcryptd_crypt_write_inline(cc, ctx)) {\n\t\tcomplete(&ctx->restart);\n\t\treturn;\n\t}\n\n\tkcryptd_crypt_write_io_submit(io, 1);\n}\n\nstatic void kcryptd_crypt(struct work_struct *work)\n{\n\tstruct dm_crypt_io *io = container_of(work, struct dm_crypt_io, work);\n\n\tif (bio_data_dir(io->base_bio) == READ)\n\t\tkcryptd_crypt_read_convert(io);\n\telse\n\t\tkcryptd_crypt_write_convert(io);\n}\n\nstatic void kcryptd_crypt_tasklet(unsigned long work)\n{\n\tkcryptd_crypt((struct work_struct *)work);\n}\n\nstatic void kcryptd_queue_crypt(struct dm_crypt_io *io)\n{\n\tstruct crypt_config *cc = io->cc;\n\n\tif ((bio_data_dir(io->base_bio) == READ && test_bit(DM_CRYPT_NO_READ_WORKQUEUE, &cc->flags)) ||\n\t    (bio_data_dir(io->base_bio) == WRITE && test_bit(DM_CRYPT_NO_WRITE_WORKQUEUE, &cc->flags))) {\n\t\t \n\t\tif (in_hardirq() || irqs_disabled()) {\n\t\t\tio->in_tasklet = true;\n\t\t\ttasklet_init(&io->tasklet, kcryptd_crypt_tasklet, (unsigned long)&io->work);\n\t\t\ttasklet_schedule(&io->tasklet);\n\t\t\treturn;\n\t\t}\n\n\t\tkcryptd_crypt(&io->work);\n\t\treturn;\n\t}\n\n\tINIT_WORK(&io->work, kcryptd_crypt);\n\tqueue_work(cc->crypt_queue, &io->work);\n}\n\nstatic void crypt_free_tfms_aead(struct crypt_config *cc)\n{\n\tif (!cc->cipher_tfm.tfms_aead)\n\t\treturn;\n\n\tif (cc->cipher_tfm.tfms_aead[0] && !IS_ERR(cc->cipher_tfm.tfms_aead[0])) {\n\t\tcrypto_free_aead(cc->cipher_tfm.tfms_aead[0]);\n\t\tcc->cipher_tfm.tfms_aead[0] = NULL;\n\t}\n\n\tkfree(cc->cipher_tfm.tfms_aead);\n\tcc->cipher_tfm.tfms_aead = NULL;\n}\n\nstatic void crypt_free_tfms_skcipher(struct crypt_config *cc)\n{\n\tunsigned int i;\n\n\tif (!cc->cipher_tfm.tfms)\n\t\treturn;\n\n\tfor (i = 0; i < cc->tfms_count; i++)\n\t\tif (cc->cipher_tfm.tfms[i] && !IS_ERR(cc->cipher_tfm.tfms[i])) {\n\t\t\tcrypto_free_skcipher(cc->cipher_tfm.tfms[i]);\n\t\t\tcc->cipher_tfm.tfms[i] = NULL;\n\t\t}\n\n\tkfree(cc->cipher_tfm.tfms);\n\tcc->cipher_tfm.tfms = NULL;\n}\n\nstatic void crypt_free_tfms(struct crypt_config *cc)\n{\n\tif (crypt_integrity_aead(cc))\n\t\tcrypt_free_tfms_aead(cc);\n\telse\n\t\tcrypt_free_tfms_skcipher(cc);\n}\n\nstatic int crypt_alloc_tfms_skcipher(struct crypt_config *cc, char *ciphermode)\n{\n\tunsigned int i;\n\tint err;\n\n\tcc->cipher_tfm.tfms = kcalloc(cc->tfms_count,\n\t\t\t\t      sizeof(struct crypto_skcipher *),\n\t\t\t\t      GFP_KERNEL);\n\tif (!cc->cipher_tfm.tfms)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < cc->tfms_count; i++) {\n\t\tcc->cipher_tfm.tfms[i] = crypto_alloc_skcipher(ciphermode, 0,\n\t\t\t\t\t\tCRYPTO_ALG_ALLOCATES_MEMORY);\n\t\tif (IS_ERR(cc->cipher_tfm.tfms[i])) {\n\t\t\terr = PTR_ERR(cc->cipher_tfm.tfms[i]);\n\t\t\tcrypt_free_tfms(cc);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\t \n\tDMDEBUG_LIMIT(\"%s using implementation \\\"%s\\\"\", ciphermode,\n\t       crypto_skcipher_alg(any_tfm(cc))->base.cra_driver_name);\n\treturn 0;\n}\n\nstatic int crypt_alloc_tfms_aead(struct crypt_config *cc, char *ciphermode)\n{\n\tint err;\n\n\tcc->cipher_tfm.tfms = kmalloc(sizeof(struct crypto_aead *), GFP_KERNEL);\n\tif (!cc->cipher_tfm.tfms)\n\t\treturn -ENOMEM;\n\n\tcc->cipher_tfm.tfms_aead[0] = crypto_alloc_aead(ciphermode, 0,\n\t\t\t\t\t\tCRYPTO_ALG_ALLOCATES_MEMORY);\n\tif (IS_ERR(cc->cipher_tfm.tfms_aead[0])) {\n\t\terr = PTR_ERR(cc->cipher_tfm.tfms_aead[0]);\n\t\tcrypt_free_tfms(cc);\n\t\treturn err;\n\t}\n\n\tDMDEBUG_LIMIT(\"%s using implementation \\\"%s\\\"\", ciphermode,\n\t       crypto_aead_alg(any_tfm_aead(cc))->base.cra_driver_name);\n\treturn 0;\n}\n\nstatic int crypt_alloc_tfms(struct crypt_config *cc, char *ciphermode)\n{\n\tif (crypt_integrity_aead(cc))\n\t\treturn crypt_alloc_tfms_aead(cc, ciphermode);\n\telse\n\t\treturn crypt_alloc_tfms_skcipher(cc, ciphermode);\n}\n\nstatic unsigned int crypt_subkey_size(struct crypt_config *cc)\n{\n\treturn (cc->key_size - cc->key_extra_size) >> ilog2(cc->tfms_count);\n}\n\nstatic unsigned int crypt_authenckey_size(struct crypt_config *cc)\n{\n\treturn crypt_subkey_size(cc) + RTA_SPACE(sizeof(struct crypto_authenc_key_param));\n}\n\n \nstatic void crypt_copy_authenckey(char *p, const void *key,\n\t\t\t\t  unsigned int enckeylen, unsigned int authkeylen)\n{\n\tstruct crypto_authenc_key_param *param;\n\tstruct rtattr *rta;\n\n\trta = (struct rtattr *)p;\n\tparam = RTA_DATA(rta);\n\tparam->enckeylen = cpu_to_be32(enckeylen);\n\trta->rta_len = RTA_LENGTH(sizeof(*param));\n\trta->rta_type = CRYPTO_AUTHENC_KEYA_PARAM;\n\tp += RTA_SPACE(sizeof(*param));\n\tmemcpy(p, key + enckeylen, authkeylen);\n\tp += authkeylen;\n\tmemcpy(p, key, enckeylen);\n}\n\nstatic int crypt_setkey(struct crypt_config *cc)\n{\n\tunsigned int subkey_size;\n\tint err = 0, i, r;\n\n\t \n\tsubkey_size = crypt_subkey_size(cc);\n\n\tif (crypt_integrity_hmac(cc)) {\n\t\tif (subkey_size < cc->key_mac_size)\n\t\t\treturn -EINVAL;\n\n\t\tcrypt_copy_authenckey(cc->authenc_key, cc->key,\n\t\t\t\t      subkey_size - cc->key_mac_size,\n\t\t\t\t      cc->key_mac_size);\n\t}\n\n\tfor (i = 0; i < cc->tfms_count; i++) {\n\t\tif (crypt_integrity_hmac(cc))\n\t\t\tr = crypto_aead_setkey(cc->cipher_tfm.tfms_aead[i],\n\t\t\t\tcc->authenc_key, crypt_authenckey_size(cc));\n\t\telse if (crypt_integrity_aead(cc))\n\t\t\tr = crypto_aead_setkey(cc->cipher_tfm.tfms_aead[i],\n\t\t\t\t\t       cc->key + (i * subkey_size),\n\t\t\t\t\t       subkey_size);\n\t\telse\n\t\t\tr = crypto_skcipher_setkey(cc->cipher_tfm.tfms[i],\n\t\t\t\t\t\t   cc->key + (i * subkey_size),\n\t\t\t\t\t\t   subkey_size);\n\t\tif (r)\n\t\t\terr = r;\n\t}\n\n\tif (crypt_integrity_hmac(cc))\n\t\tmemzero_explicit(cc->authenc_key, crypt_authenckey_size(cc));\n\n\treturn err;\n}\n\n#ifdef CONFIG_KEYS\n\nstatic bool contains_whitespace(const char *str)\n{\n\twhile (*str)\n\t\tif (isspace(*str++))\n\t\t\treturn true;\n\treturn false;\n}\n\nstatic int set_key_user(struct crypt_config *cc, struct key *key)\n{\n\tconst struct user_key_payload *ukp;\n\n\tukp = user_key_payload_locked(key);\n\tif (!ukp)\n\t\treturn -EKEYREVOKED;\n\n\tif (cc->key_size != ukp->datalen)\n\t\treturn -EINVAL;\n\n\tmemcpy(cc->key, ukp->data, cc->key_size);\n\n\treturn 0;\n}\n\nstatic int set_key_encrypted(struct crypt_config *cc, struct key *key)\n{\n\tconst struct encrypted_key_payload *ekp;\n\n\tekp = key->payload.data[0];\n\tif (!ekp)\n\t\treturn -EKEYREVOKED;\n\n\tif (cc->key_size != ekp->decrypted_datalen)\n\t\treturn -EINVAL;\n\n\tmemcpy(cc->key, ekp->decrypted_data, cc->key_size);\n\n\treturn 0;\n}\n\nstatic int set_key_trusted(struct crypt_config *cc, struct key *key)\n{\n\tconst struct trusted_key_payload *tkp;\n\n\ttkp = key->payload.data[0];\n\tif (!tkp)\n\t\treturn -EKEYREVOKED;\n\n\tif (cc->key_size != tkp->key_len)\n\t\treturn -EINVAL;\n\n\tmemcpy(cc->key, tkp->key, cc->key_size);\n\n\treturn 0;\n}\n\nstatic int crypt_set_keyring_key(struct crypt_config *cc, const char *key_string)\n{\n\tchar *new_key_string, *key_desc;\n\tint ret;\n\tstruct key_type *type;\n\tstruct key *key;\n\tint (*set_key)(struct crypt_config *cc, struct key *key);\n\n\t \n\tif (contains_whitespace(key_string)) {\n\t\tDMERR(\"whitespace chars not allowed in key string\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tkey_desc = strchr(key_string, ':');\n\tif (!key_desc || key_desc == key_string || !strlen(key_desc + 1))\n\t\treturn -EINVAL;\n\n\tif (!strncmp(key_string, \"logon:\", key_desc - key_string + 1)) {\n\t\ttype = &key_type_logon;\n\t\tset_key = set_key_user;\n\t} else if (!strncmp(key_string, \"user:\", key_desc - key_string + 1)) {\n\t\ttype = &key_type_user;\n\t\tset_key = set_key_user;\n\t} else if (IS_ENABLED(CONFIG_ENCRYPTED_KEYS) &&\n\t\t   !strncmp(key_string, \"encrypted:\", key_desc - key_string + 1)) {\n\t\ttype = &key_type_encrypted;\n\t\tset_key = set_key_encrypted;\n\t} else if (IS_ENABLED(CONFIG_TRUSTED_KEYS) &&\n\t\t   !strncmp(key_string, \"trusted:\", key_desc - key_string + 1)) {\n\t\ttype = &key_type_trusted;\n\t\tset_key = set_key_trusted;\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\tnew_key_string = kstrdup(key_string, GFP_KERNEL);\n\tif (!new_key_string)\n\t\treturn -ENOMEM;\n\n\tkey = request_key(type, key_desc + 1, NULL);\n\tif (IS_ERR(key)) {\n\t\tkfree_sensitive(new_key_string);\n\t\treturn PTR_ERR(key);\n\t}\n\n\tdown_read(&key->sem);\n\n\tret = set_key(cc, key);\n\tif (ret < 0) {\n\t\tup_read(&key->sem);\n\t\tkey_put(key);\n\t\tkfree_sensitive(new_key_string);\n\t\treturn ret;\n\t}\n\n\tup_read(&key->sem);\n\tkey_put(key);\n\n\t \n\tclear_bit(DM_CRYPT_KEY_VALID, &cc->flags);\n\n\tret = crypt_setkey(cc);\n\n\tif (!ret) {\n\t\tset_bit(DM_CRYPT_KEY_VALID, &cc->flags);\n\t\tkfree_sensitive(cc->key_string);\n\t\tcc->key_string = new_key_string;\n\t} else\n\t\tkfree_sensitive(new_key_string);\n\n\treturn ret;\n}\n\nstatic int get_key_size(char **key_string)\n{\n\tchar *colon, dummy;\n\tint ret;\n\n\tif (*key_string[0] != ':')\n\t\treturn strlen(*key_string) >> 1;\n\n\t \n\tcolon = strpbrk(*key_string + 1, \":\");\n\tif (!colon)\n\t\treturn -EINVAL;\n\n\tif (sscanf(*key_string + 1, \"%u%c\", &ret, &dummy) != 2 || dummy != ':')\n\t\treturn -EINVAL;\n\n\t*key_string = colon;\n\n\t \n\n\treturn ret;\n}\n\n#else\n\nstatic int crypt_set_keyring_key(struct crypt_config *cc, const char *key_string)\n{\n\treturn -EINVAL;\n}\n\nstatic int get_key_size(char **key_string)\n{\n\treturn (*key_string[0] == ':') ? -EINVAL : (int)(strlen(*key_string) >> 1);\n}\n\n#endif  \n\nstatic int crypt_set_key(struct crypt_config *cc, char *key)\n{\n\tint r = -EINVAL;\n\tint key_string_len = strlen(key);\n\n\t \n\tif (!cc->key_size && strcmp(key, \"-\"))\n\t\tgoto out;\n\n\t \n\tif (key[0] == ':') {\n\t\tr = crypt_set_keyring_key(cc, key + 1);\n\t\tgoto out;\n\t}\n\n\t \n\tclear_bit(DM_CRYPT_KEY_VALID, &cc->flags);\n\n\t \n\tkfree_sensitive(cc->key_string);\n\tcc->key_string = NULL;\n\n\t \n\tif (cc->key_size && hex2bin(cc->key, key, cc->key_size) < 0)\n\t\tgoto out;\n\n\tr = crypt_setkey(cc);\n\tif (!r)\n\t\tset_bit(DM_CRYPT_KEY_VALID, &cc->flags);\n\nout:\n\t \n\tmemset(key, '0', key_string_len);\n\n\treturn r;\n}\n\nstatic int crypt_wipe_key(struct crypt_config *cc)\n{\n\tint r;\n\n\tclear_bit(DM_CRYPT_KEY_VALID, &cc->flags);\n\tget_random_bytes(&cc->key, cc->key_size);\n\n\t \n\tif (cc->iv_gen_ops && cc->iv_gen_ops->wipe) {\n\t\tr = cc->iv_gen_ops->wipe(cc);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tkfree_sensitive(cc->key_string);\n\tcc->key_string = NULL;\n\tr = crypt_setkey(cc);\n\tmemset(&cc->key, 0, cc->key_size * sizeof(u8));\n\n\treturn r;\n}\n\nstatic void crypt_calculate_pages_per_client(void)\n{\n\tunsigned long pages = (totalram_pages() - totalhigh_pages()) * DM_CRYPT_MEMORY_PERCENT / 100;\n\n\tif (!dm_crypt_clients_n)\n\t\treturn;\n\n\tpages /= dm_crypt_clients_n;\n\tif (pages < DM_CRYPT_MIN_PAGES_PER_CLIENT)\n\t\tpages = DM_CRYPT_MIN_PAGES_PER_CLIENT;\n\tdm_crypt_pages_per_client = pages;\n}\n\nstatic void *crypt_page_alloc(gfp_t gfp_mask, void *pool_data)\n{\n\tstruct crypt_config *cc = pool_data;\n\tstruct page *page;\n\n\t \n\tif (unlikely(percpu_counter_read_positive(&cc->n_allocated_pages) >= dm_crypt_pages_per_client) &&\n\t    likely(gfp_mask & __GFP_NORETRY))\n\t\treturn NULL;\n\n\tpage = alloc_page(gfp_mask);\n\tif (likely(page != NULL))\n\t\tpercpu_counter_add(&cc->n_allocated_pages, 1);\n\n\treturn page;\n}\n\nstatic void crypt_page_free(void *page, void *pool_data)\n{\n\tstruct crypt_config *cc = pool_data;\n\n\t__free_page(page);\n\tpercpu_counter_sub(&cc->n_allocated_pages, 1);\n}\n\nstatic void crypt_dtr(struct dm_target *ti)\n{\n\tstruct crypt_config *cc = ti->private;\n\n\tti->private = NULL;\n\n\tif (!cc)\n\t\treturn;\n\n\tif (cc->write_thread)\n\t\tkthread_stop(cc->write_thread);\n\n\tif (cc->io_queue)\n\t\tdestroy_workqueue(cc->io_queue);\n\tif (cc->crypt_queue)\n\t\tdestroy_workqueue(cc->crypt_queue);\n\n\tcrypt_free_tfms(cc);\n\n\tbioset_exit(&cc->bs);\n\n\tmempool_exit(&cc->page_pool);\n\tmempool_exit(&cc->req_pool);\n\tmempool_exit(&cc->tag_pool);\n\n\tWARN_ON(percpu_counter_sum(&cc->n_allocated_pages) != 0);\n\tpercpu_counter_destroy(&cc->n_allocated_pages);\n\n\tif (cc->iv_gen_ops && cc->iv_gen_ops->dtr)\n\t\tcc->iv_gen_ops->dtr(cc);\n\n\tif (cc->dev)\n\t\tdm_put_device(ti, cc->dev);\n\n\tkfree_sensitive(cc->cipher_string);\n\tkfree_sensitive(cc->key_string);\n\tkfree_sensitive(cc->cipher_auth);\n\tkfree_sensitive(cc->authenc_key);\n\n\tmutex_destroy(&cc->bio_alloc_lock);\n\n\t \n\tkfree_sensitive(cc);\n\n\tspin_lock(&dm_crypt_clients_lock);\n\tWARN_ON(!dm_crypt_clients_n);\n\tdm_crypt_clients_n--;\n\tcrypt_calculate_pages_per_client();\n\tspin_unlock(&dm_crypt_clients_lock);\n\n\tdm_audit_log_dtr(DM_MSG_PREFIX, ti, 1);\n}\n\nstatic int crypt_ctr_ivmode(struct dm_target *ti, const char *ivmode)\n{\n\tstruct crypt_config *cc = ti->private;\n\n\tif (crypt_integrity_aead(cc))\n\t\tcc->iv_size = crypto_aead_ivsize(any_tfm_aead(cc));\n\telse\n\t\tcc->iv_size = crypto_skcipher_ivsize(any_tfm(cc));\n\n\tif (cc->iv_size)\n\t\t \n\t\tcc->iv_size = max(cc->iv_size,\n\t\t\t\t  (unsigned int)(sizeof(u64) / sizeof(u8)));\n\telse if (ivmode) {\n\t\tDMWARN(\"Selected cipher does not support IVs\");\n\t\tivmode = NULL;\n\t}\n\n\t \n\tif (ivmode == NULL)\n\t\tcc->iv_gen_ops = NULL;\n\telse if (strcmp(ivmode, \"plain\") == 0)\n\t\tcc->iv_gen_ops = &crypt_iv_plain_ops;\n\telse if (strcmp(ivmode, \"plain64\") == 0)\n\t\tcc->iv_gen_ops = &crypt_iv_plain64_ops;\n\telse if (strcmp(ivmode, \"plain64be\") == 0)\n\t\tcc->iv_gen_ops = &crypt_iv_plain64be_ops;\n\telse if (strcmp(ivmode, \"essiv\") == 0)\n\t\tcc->iv_gen_ops = &crypt_iv_essiv_ops;\n\telse if (strcmp(ivmode, \"benbi\") == 0)\n\t\tcc->iv_gen_ops = &crypt_iv_benbi_ops;\n\telse if (strcmp(ivmode, \"null\") == 0)\n\t\tcc->iv_gen_ops = &crypt_iv_null_ops;\n\telse if (strcmp(ivmode, \"eboiv\") == 0)\n\t\tcc->iv_gen_ops = &crypt_iv_eboiv_ops;\n\telse if (strcmp(ivmode, \"elephant\") == 0) {\n\t\tcc->iv_gen_ops = &crypt_iv_elephant_ops;\n\t\tcc->key_parts = 2;\n\t\tcc->key_extra_size = cc->key_size / 2;\n\t\tif (cc->key_extra_size > ELEPHANT_MAX_KEY_SIZE)\n\t\t\treturn -EINVAL;\n\t\tset_bit(CRYPT_ENCRYPT_PREPROCESS, &cc->cipher_flags);\n\t} else if (strcmp(ivmode, \"lmk\") == 0) {\n\t\tcc->iv_gen_ops = &crypt_iv_lmk_ops;\n\t\t \n\t\tif (cc->key_size % cc->key_parts) {\n\t\t\tcc->key_parts++;\n\t\t\tcc->key_extra_size = cc->key_size / cc->key_parts;\n\t\t}\n\t} else if (strcmp(ivmode, \"tcw\") == 0) {\n\t\tcc->iv_gen_ops = &crypt_iv_tcw_ops;\n\t\tcc->key_parts += 2;  \n\t\tcc->key_extra_size = cc->iv_size + TCW_WHITENING_SIZE;\n\t} else if (strcmp(ivmode, \"random\") == 0) {\n\t\tcc->iv_gen_ops = &crypt_iv_random_ops;\n\t\t \n\t\tcc->integrity_iv_size = cc->iv_size;\n\t} else {\n\t\tti->error = \"Invalid IV mode\";\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int crypt_ctr_auth_cipher(struct crypt_config *cc, char *cipher_api)\n{\n\tchar *start, *end, *mac_alg = NULL;\n\tstruct crypto_ahash *mac;\n\n\tif (!strstarts(cipher_api, \"authenc(\"))\n\t\treturn 0;\n\n\tstart = strchr(cipher_api, '(');\n\tend = strchr(cipher_api, ',');\n\tif (!start || !end || ++start > end)\n\t\treturn -EINVAL;\n\n\tmac_alg = kzalloc(end - start + 1, GFP_KERNEL);\n\tif (!mac_alg)\n\t\treturn -ENOMEM;\n\tstrncpy(mac_alg, start, end - start);\n\n\tmac = crypto_alloc_ahash(mac_alg, 0, CRYPTO_ALG_ALLOCATES_MEMORY);\n\tkfree(mac_alg);\n\n\tif (IS_ERR(mac))\n\t\treturn PTR_ERR(mac);\n\n\tcc->key_mac_size = crypto_ahash_digestsize(mac);\n\tcrypto_free_ahash(mac);\n\n\tcc->authenc_key = kmalloc(crypt_authenckey_size(cc), GFP_KERNEL);\n\tif (!cc->authenc_key)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic int crypt_ctr_cipher_new(struct dm_target *ti, char *cipher_in, char *key,\n\t\t\t\tchar **ivmode, char **ivopts)\n{\n\tstruct crypt_config *cc = ti->private;\n\tchar *tmp, *cipher_api, buf[CRYPTO_MAX_ALG_NAME];\n\tint ret = -EINVAL;\n\n\tcc->tfms_count = 1;\n\n\t \n\ttmp = &cipher_in[strlen(\"capi:\")];\n\n\t \n\t*ivopts = strrchr(tmp, ':');\n\tif (*ivopts) {\n\t\t**ivopts = '\\0';\n\t\t(*ivopts)++;\n\t}\n\t \n\t*ivmode = strrchr(tmp, '-');\n\tif (*ivmode) {\n\t\t**ivmode = '\\0';\n\t\t(*ivmode)++;\n\t}\n\t \n\tcipher_api = tmp;\n\n\t \n\tif (crypt_integrity_aead(cc)) {\n\t\tret = crypt_ctr_auth_cipher(cc, cipher_api);\n\t\tif (ret < 0) {\n\t\t\tti->error = \"Invalid AEAD cipher spec\";\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (*ivmode && !strcmp(*ivmode, \"lmk\"))\n\t\tcc->tfms_count = 64;\n\n\tif (*ivmode && !strcmp(*ivmode, \"essiv\")) {\n\t\tif (!*ivopts) {\n\t\t\tti->error = \"Digest algorithm missing for ESSIV mode\";\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tret = snprintf(buf, CRYPTO_MAX_ALG_NAME, \"essiv(%s,%s)\",\n\t\t\t       cipher_api, *ivopts);\n\t\tif (ret < 0 || ret >= CRYPTO_MAX_ALG_NAME) {\n\t\t\tti->error = \"Cannot allocate cipher string\";\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tcipher_api = buf;\n\t}\n\n\tcc->key_parts = cc->tfms_count;\n\n\t \n\tret = crypt_alloc_tfms(cc, cipher_api);\n\tif (ret < 0) {\n\t\tti->error = \"Error allocating crypto tfm\";\n\t\treturn ret;\n\t}\n\n\tif (crypt_integrity_aead(cc))\n\t\tcc->iv_size = crypto_aead_ivsize(any_tfm_aead(cc));\n\telse\n\t\tcc->iv_size = crypto_skcipher_ivsize(any_tfm(cc));\n\n\treturn 0;\n}\n\nstatic int crypt_ctr_cipher_old(struct dm_target *ti, char *cipher_in, char *key,\n\t\t\t\tchar **ivmode, char **ivopts)\n{\n\tstruct crypt_config *cc = ti->private;\n\tchar *tmp, *cipher, *chainmode, *keycount;\n\tchar *cipher_api = NULL;\n\tint ret = -EINVAL;\n\tchar dummy;\n\n\tif (strchr(cipher_in, '(') || crypt_integrity_aead(cc)) {\n\t\tti->error = \"Bad cipher specification\";\n\t\treturn -EINVAL;\n\t}\n\n\t \n\ttmp = cipher_in;\n\tkeycount = strsep(&tmp, \"-\");\n\tcipher = strsep(&keycount, \":\");\n\n\tif (!keycount)\n\t\tcc->tfms_count = 1;\n\telse if (sscanf(keycount, \"%u%c\", &cc->tfms_count, &dummy) != 1 ||\n\t\t !is_power_of_2(cc->tfms_count)) {\n\t\tti->error = \"Bad cipher key count specification\";\n\t\treturn -EINVAL;\n\t}\n\tcc->key_parts = cc->tfms_count;\n\n\tchainmode = strsep(&tmp, \"-\");\n\t*ivmode = strsep(&tmp, \":\");\n\t*ivopts = tmp;\n\n\t \n\tif (!chainmode || (!strcmp(chainmode, \"plain\") && !*ivmode)) {\n\t\tchainmode = \"cbc\";\n\t\t*ivmode = \"plain\";\n\t}\n\n\tif (strcmp(chainmode, \"ecb\") && !*ivmode) {\n\t\tti->error = \"IV mechanism required\";\n\t\treturn -EINVAL;\n\t}\n\n\tcipher_api = kmalloc(CRYPTO_MAX_ALG_NAME, GFP_KERNEL);\n\tif (!cipher_api)\n\t\tgoto bad_mem;\n\n\tif (*ivmode && !strcmp(*ivmode, \"essiv\")) {\n\t\tif (!*ivopts) {\n\t\t\tti->error = \"Digest algorithm missing for ESSIV mode\";\n\t\t\tkfree(cipher_api);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tret = snprintf(cipher_api, CRYPTO_MAX_ALG_NAME,\n\t\t\t       \"essiv(%s(%s),%s)\", chainmode, cipher, *ivopts);\n\t} else {\n\t\tret = snprintf(cipher_api, CRYPTO_MAX_ALG_NAME,\n\t\t\t       \"%s(%s)\", chainmode, cipher);\n\t}\n\tif (ret < 0 || ret >= CRYPTO_MAX_ALG_NAME) {\n\t\tkfree(cipher_api);\n\t\tgoto bad_mem;\n\t}\n\n\t \n\tret = crypt_alloc_tfms(cc, cipher_api);\n\tif (ret < 0) {\n\t\tti->error = \"Error allocating crypto tfm\";\n\t\tkfree(cipher_api);\n\t\treturn ret;\n\t}\n\tkfree(cipher_api);\n\n\treturn 0;\nbad_mem:\n\tti->error = \"Cannot allocate cipher strings\";\n\treturn -ENOMEM;\n}\n\nstatic int crypt_ctr_cipher(struct dm_target *ti, char *cipher_in, char *key)\n{\n\tstruct crypt_config *cc = ti->private;\n\tchar *ivmode = NULL, *ivopts = NULL;\n\tint ret;\n\n\tcc->cipher_string = kstrdup(cipher_in, GFP_KERNEL);\n\tif (!cc->cipher_string) {\n\t\tti->error = \"Cannot allocate cipher strings\";\n\t\treturn -ENOMEM;\n\t}\n\n\tif (strstarts(cipher_in, \"capi:\"))\n\t\tret = crypt_ctr_cipher_new(ti, cipher_in, key, &ivmode, &ivopts);\n\telse\n\t\tret = crypt_ctr_cipher_old(ti, cipher_in, key, &ivmode, &ivopts);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = crypt_ctr_ivmode(ti, ivmode);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tret = crypt_set_key(cc, key);\n\tif (ret < 0) {\n\t\tti->error = \"Error decoding and setting key\";\n\t\treturn ret;\n\t}\n\n\t \n\tif (cc->iv_gen_ops && cc->iv_gen_ops->ctr) {\n\t\tret = cc->iv_gen_ops->ctr(cc, ti, ivopts);\n\t\tif (ret < 0) {\n\t\t\tti->error = \"Error creating IV\";\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t \n\tif (cc->iv_gen_ops && cc->iv_gen_ops->init) {\n\t\tret = cc->iv_gen_ops->init(cc);\n\t\tif (ret < 0) {\n\t\t\tti->error = \"Error initialising IV\";\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t \n\tif (cc->key_string)\n\t\tmemset(cc->key, 0, cc->key_size * sizeof(u8));\n\n\treturn ret;\n}\n\nstatic int crypt_ctr_optional(struct dm_target *ti, unsigned int argc, char **argv)\n{\n\tstruct crypt_config *cc = ti->private;\n\tstruct dm_arg_set as;\n\tstatic const struct dm_arg _args[] = {\n\t\t{0, 8, \"Invalid number of feature args\"},\n\t};\n\tunsigned int opt_params, val;\n\tconst char *opt_string, *sval;\n\tchar dummy;\n\tint ret;\n\n\t \n\tas.argc = argc;\n\tas.argv = argv;\n\n\tret = dm_read_arg_group(_args, &as, &opt_params, &ti->error);\n\tif (ret)\n\t\treturn ret;\n\n\twhile (opt_params--) {\n\t\topt_string = dm_shift_arg(&as);\n\t\tif (!opt_string) {\n\t\t\tti->error = \"Not enough feature arguments\";\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!strcasecmp(opt_string, \"allow_discards\"))\n\t\t\tti->num_discard_bios = 1;\n\n\t\telse if (!strcasecmp(opt_string, \"same_cpu_crypt\"))\n\t\t\tset_bit(DM_CRYPT_SAME_CPU, &cc->flags);\n\n\t\telse if (!strcasecmp(opt_string, \"submit_from_crypt_cpus\"))\n\t\t\tset_bit(DM_CRYPT_NO_OFFLOAD, &cc->flags);\n\t\telse if (!strcasecmp(opt_string, \"no_read_workqueue\"))\n\t\t\tset_bit(DM_CRYPT_NO_READ_WORKQUEUE, &cc->flags);\n\t\telse if (!strcasecmp(opt_string, \"no_write_workqueue\"))\n\t\t\tset_bit(DM_CRYPT_NO_WRITE_WORKQUEUE, &cc->flags);\n\t\telse if (sscanf(opt_string, \"integrity:%u:\", &val) == 1) {\n\t\t\tif (val == 0 || val > MAX_TAG_SIZE) {\n\t\t\t\tti->error = \"Invalid integrity arguments\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tcc->on_disk_tag_size = val;\n\t\t\tsval = strchr(opt_string + strlen(\"integrity:\"), ':') + 1;\n\t\t\tif (!strcasecmp(sval, \"aead\")) {\n\t\t\t\tset_bit(CRYPT_MODE_INTEGRITY_AEAD, &cc->cipher_flags);\n\t\t\t} else  if (strcasecmp(sval, \"none\")) {\n\t\t\t\tti->error = \"Unknown integrity profile\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tcc->cipher_auth = kstrdup(sval, GFP_KERNEL);\n\t\t\tif (!cc->cipher_auth)\n\t\t\t\treturn -ENOMEM;\n\t\t} else if (sscanf(opt_string, \"sector_size:%hu%c\", &cc->sector_size, &dummy) == 1) {\n\t\t\tif (cc->sector_size < (1 << SECTOR_SHIFT) ||\n\t\t\t    cc->sector_size > 4096 ||\n\t\t\t    (cc->sector_size & (cc->sector_size - 1))) {\n\t\t\t\tti->error = \"Invalid feature value for sector_size\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (ti->len & ((cc->sector_size >> SECTOR_SHIFT) - 1)) {\n\t\t\t\tti->error = \"Device size is not multiple of sector_size feature\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tcc->sector_shift = __ffs(cc->sector_size) - SECTOR_SHIFT;\n\t\t} else if (!strcasecmp(opt_string, \"iv_large_sectors\"))\n\t\t\tset_bit(CRYPT_IV_LARGE_SECTORS, &cc->cipher_flags);\n\t\telse {\n\t\t\tti->error = \"Invalid feature arguments\";\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic int crypt_report_zones(struct dm_target *ti,\n\t\tstruct dm_report_zones_args *args, unsigned int nr_zones)\n{\n\tstruct crypt_config *cc = ti->private;\n\n\treturn dm_report_zones(cc->dev->bdev, cc->start,\n\t\t\tcc->start + dm_target_offset(ti, args->next_sector),\n\t\t\targs, nr_zones);\n}\n#else\n#define crypt_report_zones NULL\n#endif\n\n \nstatic int crypt_ctr(struct dm_target *ti, unsigned int argc, char **argv)\n{\n\tstruct crypt_config *cc;\n\tconst char *devname = dm_table_device_name(ti->table);\n\tint key_size;\n\tunsigned int align_mask;\n\tunsigned long long tmpll;\n\tint ret;\n\tsize_t iv_size_padding, additional_req_size;\n\tchar dummy;\n\n\tif (argc < 5) {\n\t\tti->error = \"Not enough arguments\";\n\t\treturn -EINVAL;\n\t}\n\n\tkey_size = get_key_size(&argv[1]);\n\tif (key_size < 0) {\n\t\tti->error = \"Cannot parse key size\";\n\t\treturn -EINVAL;\n\t}\n\n\tcc = kzalloc(struct_size(cc, key, key_size), GFP_KERNEL);\n\tif (!cc) {\n\t\tti->error = \"Cannot allocate encryption context\";\n\t\treturn -ENOMEM;\n\t}\n\tcc->key_size = key_size;\n\tcc->sector_size = (1 << SECTOR_SHIFT);\n\tcc->sector_shift = 0;\n\n\tti->private = cc;\n\n\tspin_lock(&dm_crypt_clients_lock);\n\tdm_crypt_clients_n++;\n\tcrypt_calculate_pages_per_client();\n\tspin_unlock(&dm_crypt_clients_lock);\n\n\tret = percpu_counter_init(&cc->n_allocated_pages, 0, GFP_KERNEL);\n\tif (ret < 0)\n\t\tgoto bad;\n\n\t \n\tif (argc > 5) {\n\t\tret = crypt_ctr_optional(ti, argc - 5, &argv[5]);\n\t\tif (ret)\n\t\t\tgoto bad;\n\t}\n\n\tret = crypt_ctr_cipher(ti, argv[0], argv[1]);\n\tif (ret < 0)\n\t\tgoto bad;\n\n\tif (crypt_integrity_aead(cc)) {\n\t\tcc->dmreq_start = sizeof(struct aead_request);\n\t\tcc->dmreq_start += crypto_aead_reqsize(any_tfm_aead(cc));\n\t\talign_mask = crypto_aead_alignmask(any_tfm_aead(cc));\n\t} else {\n\t\tcc->dmreq_start = sizeof(struct skcipher_request);\n\t\tcc->dmreq_start += crypto_skcipher_reqsize(any_tfm(cc));\n\t\talign_mask = crypto_skcipher_alignmask(any_tfm(cc));\n\t}\n\tcc->dmreq_start = ALIGN(cc->dmreq_start, __alignof__(struct dm_crypt_request));\n\n\tif (align_mask < CRYPTO_MINALIGN) {\n\t\t \n\t\tiv_size_padding = -(cc->dmreq_start + sizeof(struct dm_crypt_request))\n\t\t\t\t& align_mask;\n\t} else {\n\t\t \n\t\tiv_size_padding = align_mask;\n\t}\n\n\t \n\tadditional_req_size = sizeof(struct dm_crypt_request) +\n\t\tiv_size_padding + cc->iv_size +\n\t\tcc->iv_size +\n\t\tsizeof(uint64_t) +\n\t\tsizeof(unsigned int);\n\n\tret = mempool_init_kmalloc_pool(&cc->req_pool, MIN_IOS, cc->dmreq_start + additional_req_size);\n\tif (ret) {\n\t\tti->error = \"Cannot allocate crypt request mempool\";\n\t\tgoto bad;\n\t}\n\n\tcc->per_bio_data_size = ti->per_io_data_size =\n\t\tALIGN(sizeof(struct dm_crypt_io) + cc->dmreq_start + additional_req_size,\n\t\t      ARCH_DMA_MINALIGN);\n\n\tret = mempool_init(&cc->page_pool, BIO_MAX_VECS, crypt_page_alloc, crypt_page_free, cc);\n\tif (ret) {\n\t\tti->error = \"Cannot allocate page mempool\";\n\t\tgoto bad;\n\t}\n\n\tret = bioset_init(&cc->bs, MIN_IOS, 0, BIOSET_NEED_BVECS);\n\tif (ret) {\n\t\tti->error = \"Cannot allocate crypt bioset\";\n\t\tgoto bad;\n\t}\n\n\tmutex_init(&cc->bio_alloc_lock);\n\n\tret = -EINVAL;\n\tif ((sscanf(argv[2], \"%llu%c\", &tmpll, &dummy) != 1) ||\n\t    (tmpll & ((cc->sector_size >> SECTOR_SHIFT) - 1))) {\n\t\tti->error = \"Invalid iv_offset sector\";\n\t\tgoto bad;\n\t}\n\tcc->iv_offset = tmpll;\n\n\tret = dm_get_device(ti, argv[3], dm_table_get_mode(ti->table), &cc->dev);\n\tif (ret) {\n\t\tti->error = \"Device lookup failed\";\n\t\tgoto bad;\n\t}\n\n\tret = -EINVAL;\n\tif (sscanf(argv[4], \"%llu%c\", &tmpll, &dummy) != 1 || tmpll != (sector_t)tmpll) {\n\t\tti->error = \"Invalid device sector\";\n\t\tgoto bad;\n\t}\n\tcc->start = tmpll;\n\n\tif (bdev_is_zoned(cc->dev->bdev)) {\n\t\t \n\t\tset_bit(DM_CRYPT_NO_WRITE_WORKQUEUE, &cc->flags);\n\t\tset_bit(DM_CRYPT_WRITE_INLINE, &cc->flags);\n\n\t\t \n\t\tDMDEBUG(\"Zone append operations will be emulated\");\n\t\tti->emulate_zone_append = true;\n\t}\n\n\tif (crypt_integrity_aead(cc) || cc->integrity_iv_size) {\n\t\tret = crypt_integrity_ctr(cc, ti);\n\t\tif (ret)\n\t\t\tgoto bad;\n\n\t\tcc->tag_pool_max_sectors = POOL_ENTRY_SIZE / cc->on_disk_tag_size;\n\t\tif (!cc->tag_pool_max_sectors)\n\t\t\tcc->tag_pool_max_sectors = 1;\n\n\t\tret = mempool_init_kmalloc_pool(&cc->tag_pool, MIN_IOS,\n\t\t\tcc->tag_pool_max_sectors * cc->on_disk_tag_size);\n\t\tif (ret) {\n\t\t\tti->error = \"Cannot allocate integrity tags mempool\";\n\t\t\tgoto bad;\n\t\t}\n\n\t\tcc->tag_pool_max_sectors <<= cc->sector_shift;\n\t}\n\n\tret = -ENOMEM;\n\tcc->io_queue = alloc_workqueue(\"kcryptd_io/%s\", WQ_MEM_RECLAIM, 1, devname);\n\tif (!cc->io_queue) {\n\t\tti->error = \"Couldn't create kcryptd io queue\";\n\t\tgoto bad;\n\t}\n\n\tif (test_bit(DM_CRYPT_SAME_CPU, &cc->flags))\n\t\tcc->crypt_queue = alloc_workqueue(\"kcryptd/%s\", WQ_CPU_INTENSIVE | WQ_MEM_RECLAIM,\n\t\t\t\t\t\t  1, devname);\n\telse\n\t\tcc->crypt_queue = alloc_workqueue(\"kcryptd/%s\",\n\t\t\t\t\t\t  WQ_CPU_INTENSIVE | WQ_MEM_RECLAIM | WQ_UNBOUND,\n\t\t\t\t\t\t  num_online_cpus(), devname);\n\tif (!cc->crypt_queue) {\n\t\tti->error = \"Couldn't create kcryptd queue\";\n\t\tgoto bad;\n\t}\n\n\tspin_lock_init(&cc->write_thread_lock);\n\tcc->write_tree = RB_ROOT;\n\n\tcc->write_thread = kthread_run(dmcrypt_write, cc, \"dmcrypt_write/%s\", devname);\n\tif (IS_ERR(cc->write_thread)) {\n\t\tret = PTR_ERR(cc->write_thread);\n\t\tcc->write_thread = NULL;\n\t\tti->error = \"Couldn't spawn write thread\";\n\t\tgoto bad;\n\t}\n\n\tti->num_flush_bios = 1;\n\tti->limit_swap_bios = true;\n\tti->accounts_remapped_io = true;\n\n\tdm_audit_log_ctr(DM_MSG_PREFIX, ti, 1);\n\treturn 0;\n\nbad:\n\tdm_audit_log_ctr(DM_MSG_PREFIX, ti, 0);\n\tcrypt_dtr(ti);\n\treturn ret;\n}\n\nstatic int crypt_map(struct dm_target *ti, struct bio *bio)\n{\n\tstruct dm_crypt_io *io;\n\tstruct crypt_config *cc = ti->private;\n\n\t \n\tif (unlikely(bio->bi_opf & REQ_PREFLUSH ||\n\t    bio_op(bio) == REQ_OP_DISCARD)) {\n\t\tbio_set_dev(bio, cc->dev->bdev);\n\t\tif (bio_sectors(bio))\n\t\t\tbio->bi_iter.bi_sector = cc->start +\n\t\t\t\tdm_target_offset(ti, bio->bi_iter.bi_sector);\n\t\treturn DM_MAPIO_REMAPPED;\n\t}\n\n\t \n\tif (unlikely(bio->bi_iter.bi_size > (BIO_MAX_VECS << PAGE_SHIFT)) &&\n\t    (bio_data_dir(bio) == WRITE || cc->on_disk_tag_size))\n\t\tdm_accept_partial_bio(bio, ((BIO_MAX_VECS << PAGE_SHIFT) >> SECTOR_SHIFT));\n\n\t \n\tif (unlikely((bio->bi_iter.bi_sector & ((cc->sector_size >> SECTOR_SHIFT) - 1)) != 0))\n\t\treturn DM_MAPIO_KILL;\n\n\tif (unlikely(bio->bi_iter.bi_size & (cc->sector_size - 1)))\n\t\treturn DM_MAPIO_KILL;\n\n\tio = dm_per_bio_data(bio, cc->per_bio_data_size);\n\tcrypt_io_init(io, cc, bio, dm_target_offset(ti, bio->bi_iter.bi_sector));\n\n\tif (cc->on_disk_tag_size) {\n\t\tunsigned int tag_len = cc->on_disk_tag_size * (bio_sectors(bio) >> cc->sector_shift);\n\n\t\tif (unlikely(tag_len > KMALLOC_MAX_SIZE))\n\t\t\tio->integrity_metadata = NULL;\n\t\telse\n\t\t\tio->integrity_metadata = kmalloc(tag_len, GFP_NOIO | __GFP_NORETRY | __GFP_NOMEMALLOC | __GFP_NOWARN);\n\n\t\tif (unlikely(!io->integrity_metadata)) {\n\t\t\tif (bio_sectors(bio) > cc->tag_pool_max_sectors)\n\t\t\t\tdm_accept_partial_bio(bio, cc->tag_pool_max_sectors);\n\t\t\tio->integrity_metadata = mempool_alloc(&cc->tag_pool, GFP_NOIO);\n\t\t\tio->integrity_metadata_from_pool = true;\n\t\t}\n\t}\n\n\tif (crypt_integrity_aead(cc))\n\t\tio->ctx.r.req_aead = (struct aead_request *)(io + 1);\n\telse\n\t\tio->ctx.r.req = (struct skcipher_request *)(io + 1);\n\n\tif (bio_data_dir(io->base_bio) == READ) {\n\t\tif (kcryptd_io_read(io, CRYPT_MAP_READ_GFP))\n\t\t\tkcryptd_queue_read(io);\n\t} else\n\t\tkcryptd_queue_crypt(io);\n\n\treturn DM_MAPIO_SUBMITTED;\n}\n\nstatic char hex2asc(unsigned char c)\n{\n\treturn c + '0' + ((unsigned int)(9 - c) >> 4 & 0x27);\n}\n\nstatic void crypt_status(struct dm_target *ti, status_type_t type,\n\t\t\t unsigned int status_flags, char *result, unsigned int maxlen)\n{\n\tstruct crypt_config *cc = ti->private;\n\tunsigned int i, sz = 0;\n\tint num_feature_args = 0;\n\n\tswitch (type) {\n\tcase STATUSTYPE_INFO:\n\t\tresult[0] = '\\0';\n\t\tbreak;\n\n\tcase STATUSTYPE_TABLE:\n\t\tDMEMIT(\"%s \", cc->cipher_string);\n\n\t\tif (cc->key_size > 0) {\n\t\t\tif (cc->key_string)\n\t\t\t\tDMEMIT(\":%u:%s\", cc->key_size, cc->key_string);\n\t\t\telse {\n\t\t\t\tfor (i = 0; i < cc->key_size; i++) {\n\t\t\t\t\tDMEMIT(\"%c%c\", hex2asc(cc->key[i] >> 4),\n\t\t\t\t\t       hex2asc(cc->key[i] & 0xf));\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\tDMEMIT(\"-\");\n\n\t\tDMEMIT(\" %llu %s %llu\", (unsigned long long)cc->iv_offset,\n\t\t\t\tcc->dev->name, (unsigned long long)cc->start);\n\n\t\tnum_feature_args += !!ti->num_discard_bios;\n\t\tnum_feature_args += test_bit(DM_CRYPT_SAME_CPU, &cc->flags);\n\t\tnum_feature_args += test_bit(DM_CRYPT_NO_OFFLOAD, &cc->flags);\n\t\tnum_feature_args += test_bit(DM_CRYPT_NO_READ_WORKQUEUE, &cc->flags);\n\t\tnum_feature_args += test_bit(DM_CRYPT_NO_WRITE_WORKQUEUE, &cc->flags);\n\t\tnum_feature_args += cc->sector_size != (1 << SECTOR_SHIFT);\n\t\tnum_feature_args += test_bit(CRYPT_IV_LARGE_SECTORS, &cc->cipher_flags);\n\t\tif (cc->on_disk_tag_size)\n\t\t\tnum_feature_args++;\n\t\tif (num_feature_args) {\n\t\t\tDMEMIT(\" %d\", num_feature_args);\n\t\t\tif (ti->num_discard_bios)\n\t\t\t\tDMEMIT(\" allow_discards\");\n\t\t\tif (test_bit(DM_CRYPT_SAME_CPU, &cc->flags))\n\t\t\t\tDMEMIT(\" same_cpu_crypt\");\n\t\t\tif (test_bit(DM_CRYPT_NO_OFFLOAD, &cc->flags))\n\t\t\t\tDMEMIT(\" submit_from_crypt_cpus\");\n\t\t\tif (test_bit(DM_CRYPT_NO_READ_WORKQUEUE, &cc->flags))\n\t\t\t\tDMEMIT(\" no_read_workqueue\");\n\t\t\tif (test_bit(DM_CRYPT_NO_WRITE_WORKQUEUE, &cc->flags))\n\t\t\t\tDMEMIT(\" no_write_workqueue\");\n\t\t\tif (cc->on_disk_tag_size)\n\t\t\t\tDMEMIT(\" integrity:%u:%s\", cc->on_disk_tag_size, cc->cipher_auth);\n\t\t\tif (cc->sector_size != (1 << SECTOR_SHIFT))\n\t\t\t\tDMEMIT(\" sector_size:%d\", cc->sector_size);\n\t\t\tif (test_bit(CRYPT_IV_LARGE_SECTORS, &cc->cipher_flags))\n\t\t\t\tDMEMIT(\" iv_large_sectors\");\n\t\t}\n\t\tbreak;\n\n\tcase STATUSTYPE_IMA:\n\t\tDMEMIT_TARGET_NAME_VERSION(ti->type);\n\t\tDMEMIT(\",allow_discards=%c\", ti->num_discard_bios ? 'y' : 'n');\n\t\tDMEMIT(\",same_cpu_crypt=%c\", test_bit(DM_CRYPT_SAME_CPU, &cc->flags) ? 'y' : 'n');\n\t\tDMEMIT(\",submit_from_crypt_cpus=%c\", test_bit(DM_CRYPT_NO_OFFLOAD, &cc->flags) ?\n\t\t       'y' : 'n');\n\t\tDMEMIT(\",no_read_workqueue=%c\", test_bit(DM_CRYPT_NO_READ_WORKQUEUE, &cc->flags) ?\n\t\t       'y' : 'n');\n\t\tDMEMIT(\",no_write_workqueue=%c\", test_bit(DM_CRYPT_NO_WRITE_WORKQUEUE, &cc->flags) ?\n\t\t       'y' : 'n');\n\t\tDMEMIT(\",iv_large_sectors=%c\", test_bit(CRYPT_IV_LARGE_SECTORS, &cc->cipher_flags) ?\n\t\t       'y' : 'n');\n\n\t\tif (cc->on_disk_tag_size)\n\t\t\tDMEMIT(\",integrity_tag_size=%u,cipher_auth=%s\",\n\t\t\t       cc->on_disk_tag_size, cc->cipher_auth);\n\t\tif (cc->sector_size != (1 << SECTOR_SHIFT))\n\t\t\tDMEMIT(\",sector_size=%d\", cc->sector_size);\n\t\tif (cc->cipher_string)\n\t\t\tDMEMIT(\",cipher_string=%s\", cc->cipher_string);\n\n\t\tDMEMIT(\",key_size=%u\", cc->key_size);\n\t\tDMEMIT(\",key_parts=%u\", cc->key_parts);\n\t\tDMEMIT(\",key_extra_size=%u\", cc->key_extra_size);\n\t\tDMEMIT(\",key_mac_size=%u\", cc->key_mac_size);\n\t\tDMEMIT(\";\");\n\t\tbreak;\n\t}\n}\n\nstatic void crypt_postsuspend(struct dm_target *ti)\n{\n\tstruct crypt_config *cc = ti->private;\n\n\tset_bit(DM_CRYPT_SUSPENDED, &cc->flags);\n}\n\nstatic int crypt_preresume(struct dm_target *ti)\n{\n\tstruct crypt_config *cc = ti->private;\n\n\tif (!test_bit(DM_CRYPT_KEY_VALID, &cc->flags)) {\n\t\tDMERR(\"aborting resume - crypt key is not set.\");\n\t\treturn -EAGAIN;\n\t}\n\n\treturn 0;\n}\n\nstatic void crypt_resume(struct dm_target *ti)\n{\n\tstruct crypt_config *cc = ti->private;\n\n\tclear_bit(DM_CRYPT_SUSPENDED, &cc->flags);\n}\n\n \nstatic int crypt_message(struct dm_target *ti, unsigned int argc, char **argv,\n\t\t\t char *result, unsigned int maxlen)\n{\n\tstruct crypt_config *cc = ti->private;\n\tint key_size, ret = -EINVAL;\n\n\tif (argc < 2)\n\t\tgoto error;\n\n\tif (!strcasecmp(argv[0], \"key\")) {\n\t\tif (!test_bit(DM_CRYPT_SUSPENDED, &cc->flags)) {\n\t\t\tDMWARN(\"not suspended during key manipulation.\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (argc == 3 && !strcasecmp(argv[1], \"set\")) {\n\t\t\t \n\t\t\tkey_size = get_key_size(&argv[2]);\n\t\t\tif (key_size < 0 || cc->key_size != key_size) {\n\t\t\t\tmemset(argv[2], '0', strlen(argv[2]));\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tret = crypt_set_key(cc, argv[2]);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tif (cc->iv_gen_ops && cc->iv_gen_ops->init)\n\t\t\t\tret = cc->iv_gen_ops->init(cc);\n\t\t\t \n\t\t\tif (cc->key_string)\n\t\t\t\tmemset(cc->key, 0, cc->key_size * sizeof(u8));\n\t\t\treturn ret;\n\t\t}\n\t\tif (argc == 2 && !strcasecmp(argv[1], \"wipe\"))\n\t\t\treturn crypt_wipe_key(cc);\n\t}\n\nerror:\n\tDMWARN(\"unrecognised message received.\");\n\treturn -EINVAL;\n}\n\nstatic int crypt_iterate_devices(struct dm_target *ti,\n\t\t\t\t iterate_devices_callout_fn fn, void *data)\n{\n\tstruct crypt_config *cc = ti->private;\n\n\treturn fn(ti, cc->dev, cc->start, ti->len, data);\n}\n\nstatic void crypt_io_hints(struct dm_target *ti, struct queue_limits *limits)\n{\n\tstruct crypt_config *cc = ti->private;\n\n\t \n\tlimits->max_segment_size = PAGE_SIZE;\n\n\tlimits->logical_block_size =\n\t\tmax_t(unsigned int, limits->logical_block_size, cc->sector_size);\n\tlimits->physical_block_size =\n\t\tmax_t(unsigned int, limits->physical_block_size, cc->sector_size);\n\tlimits->io_min = max_t(unsigned int, limits->io_min, cc->sector_size);\n\tlimits->dma_alignment = limits->logical_block_size - 1;\n}\n\nstatic struct target_type crypt_target = {\n\t.name   = \"crypt\",\n\t.version = {1, 24, 0},\n\t.module = THIS_MODULE,\n\t.ctr    = crypt_ctr,\n\t.dtr    = crypt_dtr,\n\t.features = DM_TARGET_ZONED_HM,\n\t.report_zones = crypt_report_zones,\n\t.map    = crypt_map,\n\t.status = crypt_status,\n\t.postsuspend = crypt_postsuspend,\n\t.preresume = crypt_preresume,\n\t.resume = crypt_resume,\n\t.message = crypt_message,\n\t.iterate_devices = crypt_iterate_devices,\n\t.io_hints = crypt_io_hints,\n};\nmodule_dm(crypt);\n\nMODULE_AUTHOR(\"Jana Saout <jana@saout.de>\");\nMODULE_DESCRIPTION(DM_NAME \" target for transparent encryption / decryption\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}