{
  "module_name": "dm.c",
  "hash_id": "3bf27190e7e07a6caf0d1f958f8f4ecdeb864fcc29aaf0fbe2cb742e09d13ac8",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm.c",
  "human_readable_source": "\n \n\n#include \"dm-core.h\"\n#include \"dm-rq.h\"\n#include \"dm-uevent.h\"\n#include \"dm-ima.h\"\n\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/signal.h>\n#include <linux/blkpg.h>\n#include <linux/bio.h>\n#include <linux/mempool.h>\n#include <linux/dax.h>\n#include <linux/slab.h>\n#include <linux/idr.h>\n#include <linux/uio.h>\n#include <linux/hdreg.h>\n#include <linux/delay.h>\n#include <linux/wait.h>\n#include <linux/pr.h>\n#include <linux/refcount.h>\n#include <linux/part_stat.h>\n#include <linux/blk-crypto.h>\n#include <linux/blk-crypto-profile.h>\n\n#define DM_MSG_PREFIX \"core\"\n\n \n#define DM_COOKIE_ENV_VAR_NAME \"DM_COOKIE\"\n#define DM_COOKIE_LENGTH 24\n\n \n#define REQ_DM_POLL_LIST\tREQ_DRV\n\nstatic const char *_name = DM_NAME;\n\nstatic unsigned int major;\nstatic unsigned int _major;\n\nstatic DEFINE_IDR(_minor_idr);\n\nstatic DEFINE_SPINLOCK(_minor_lock);\n\nstatic void do_deferred_remove(struct work_struct *w);\n\nstatic DECLARE_WORK(deferred_remove_work, do_deferred_remove);\n\nstatic struct workqueue_struct *deferred_remove_workqueue;\n\natomic_t dm_global_event_nr = ATOMIC_INIT(0);\nDECLARE_WAIT_QUEUE_HEAD(dm_global_eventq);\n\nvoid dm_issue_global_event(void)\n{\n\tatomic_inc(&dm_global_event_nr);\n\twake_up(&dm_global_eventq);\n}\n\nDEFINE_STATIC_KEY_FALSE(stats_enabled);\nDEFINE_STATIC_KEY_FALSE(swap_bios_enabled);\nDEFINE_STATIC_KEY_FALSE(zoned_enabled);\n\n \nstruct clone_info {\n\tstruct dm_table *map;\n\tstruct bio *bio;\n\tstruct dm_io *io;\n\tsector_t sector;\n\tunsigned int sector_count;\n\tbool is_abnormal_io:1;\n\tbool submit_as_polled:1;\n};\n\nstatic inline struct dm_target_io *clone_to_tio(struct bio *clone)\n{\n\treturn container_of(clone, struct dm_target_io, clone);\n}\n\nvoid *dm_per_bio_data(struct bio *bio, size_t data_size)\n{\n\tif (!dm_tio_flagged(clone_to_tio(bio), DM_TIO_INSIDE_DM_IO))\n\t\treturn (char *)bio - DM_TARGET_IO_BIO_OFFSET - data_size;\n\treturn (char *)bio - DM_IO_BIO_OFFSET - data_size;\n}\nEXPORT_SYMBOL_GPL(dm_per_bio_data);\n\nstruct bio *dm_bio_from_per_bio_data(void *data, size_t data_size)\n{\n\tstruct dm_io *io = (struct dm_io *)((char *)data + data_size);\n\n\tif (io->magic == DM_IO_MAGIC)\n\t\treturn (struct bio *)((char *)io + DM_IO_BIO_OFFSET);\n\tBUG_ON(io->magic != DM_TIO_MAGIC);\n\treturn (struct bio *)((char *)io + DM_TARGET_IO_BIO_OFFSET);\n}\nEXPORT_SYMBOL_GPL(dm_bio_from_per_bio_data);\n\nunsigned int dm_bio_get_target_bio_nr(const struct bio *bio)\n{\n\treturn container_of(bio, struct dm_target_io, clone)->target_bio_nr;\n}\nEXPORT_SYMBOL_GPL(dm_bio_get_target_bio_nr);\n\n#define MINOR_ALLOCED ((void *)-1)\n\n#define DM_NUMA_NODE NUMA_NO_NODE\nstatic int dm_numa_node = DM_NUMA_NODE;\n\n#define DEFAULT_SWAP_BIOS\t(8 * 1048576 / PAGE_SIZE)\nstatic int swap_bios = DEFAULT_SWAP_BIOS;\nstatic int get_swap_bios(void)\n{\n\tint latch = READ_ONCE(swap_bios);\n\n\tif (unlikely(latch <= 0))\n\t\tlatch = DEFAULT_SWAP_BIOS;\n\treturn latch;\n}\n\nstruct table_device {\n\tstruct list_head list;\n\trefcount_t count;\n\tstruct dm_dev dm_dev;\n};\n\n \n#define RESERVED_BIO_BASED_IOS\t\t16\nstatic unsigned int reserved_bio_based_ios = RESERVED_BIO_BASED_IOS;\n\nstatic int __dm_get_module_param_int(int *module_param, int min, int max)\n{\n\tint param = READ_ONCE(*module_param);\n\tint modified_param = 0;\n\tbool modified = true;\n\n\tif (param < min)\n\t\tmodified_param = min;\n\telse if (param > max)\n\t\tmodified_param = max;\n\telse\n\t\tmodified = false;\n\n\tif (modified) {\n\t\t(void)cmpxchg(module_param, param, modified_param);\n\t\tparam = modified_param;\n\t}\n\n\treturn param;\n}\n\nunsigned int __dm_get_module_param(unsigned int *module_param, unsigned int def, unsigned int max)\n{\n\tunsigned int param = READ_ONCE(*module_param);\n\tunsigned int modified_param = 0;\n\n\tif (!param)\n\t\tmodified_param = def;\n\telse if (param > max)\n\t\tmodified_param = max;\n\n\tif (modified_param) {\n\t\t(void)cmpxchg(module_param, param, modified_param);\n\t\tparam = modified_param;\n\t}\n\n\treturn param;\n}\n\nunsigned int dm_get_reserved_bio_based_ios(void)\n{\n\treturn __dm_get_module_param(&reserved_bio_based_ios,\n\t\t\t\t     RESERVED_BIO_BASED_IOS, DM_RESERVED_MAX_IOS);\n}\nEXPORT_SYMBOL_GPL(dm_get_reserved_bio_based_ios);\n\nstatic unsigned int dm_get_numa_node(void)\n{\n\treturn __dm_get_module_param_int(&dm_numa_node,\n\t\t\t\t\t DM_NUMA_NODE, num_online_nodes() - 1);\n}\n\nstatic int __init local_init(void)\n{\n\tint r;\n\n\tr = dm_uevent_init();\n\tif (r)\n\t\treturn r;\n\n\tdeferred_remove_workqueue = alloc_ordered_workqueue(\"kdmremove\", 0);\n\tif (!deferred_remove_workqueue) {\n\t\tr = -ENOMEM;\n\t\tgoto out_uevent_exit;\n\t}\n\n\t_major = major;\n\tr = register_blkdev(_major, _name);\n\tif (r < 0)\n\t\tgoto out_free_workqueue;\n\n\tif (!_major)\n\t\t_major = r;\n\n\treturn 0;\n\nout_free_workqueue:\n\tdestroy_workqueue(deferred_remove_workqueue);\nout_uevent_exit:\n\tdm_uevent_exit();\n\n\treturn r;\n}\n\nstatic void local_exit(void)\n{\n\tdestroy_workqueue(deferred_remove_workqueue);\n\n\tunregister_blkdev(_major, _name);\n\tdm_uevent_exit();\n\n\t_major = 0;\n\n\tDMINFO(\"cleaned up\");\n}\n\nstatic int (*_inits[])(void) __initdata = {\n\tlocal_init,\n\tdm_target_init,\n\tdm_linear_init,\n\tdm_stripe_init,\n\tdm_io_init,\n\tdm_kcopyd_init,\n\tdm_interface_init,\n\tdm_statistics_init,\n};\n\nstatic void (*_exits[])(void) = {\n\tlocal_exit,\n\tdm_target_exit,\n\tdm_linear_exit,\n\tdm_stripe_exit,\n\tdm_io_exit,\n\tdm_kcopyd_exit,\n\tdm_interface_exit,\n\tdm_statistics_exit,\n};\n\nstatic int __init dm_init(void)\n{\n\tconst int count = ARRAY_SIZE(_inits);\n\tint r, i;\n\n#if (IS_ENABLED(CONFIG_IMA) && !IS_ENABLED(CONFIG_IMA_DISABLE_HTABLE))\n\tDMWARN(\"CONFIG_IMA_DISABLE_HTABLE is disabled.\"\n\t       \" Duplicate IMA measurements will not be recorded in the IMA log.\");\n#endif\n\n\tfor (i = 0; i < count; i++) {\n\t\tr = _inits[i]();\n\t\tif (r)\n\t\t\tgoto bad;\n\t}\n\n\treturn 0;\nbad:\n\twhile (i--)\n\t\t_exits[i]();\n\n\treturn r;\n}\n\nstatic void __exit dm_exit(void)\n{\n\tint i = ARRAY_SIZE(_exits);\n\n\twhile (i--)\n\t\t_exits[i]();\n\n\t \n\tidr_destroy(&_minor_idr);\n}\n\n \nint dm_deleting_md(struct mapped_device *md)\n{\n\treturn test_bit(DMF_DELETING, &md->flags);\n}\n\nstatic int dm_blk_open(struct gendisk *disk, blk_mode_t mode)\n{\n\tstruct mapped_device *md;\n\n\tspin_lock(&_minor_lock);\n\n\tmd = disk->private_data;\n\tif (!md)\n\t\tgoto out;\n\n\tif (test_bit(DMF_FREEING, &md->flags) ||\n\t    dm_deleting_md(md)) {\n\t\tmd = NULL;\n\t\tgoto out;\n\t}\n\n\tdm_get(md);\n\tatomic_inc(&md->open_count);\nout:\n\tspin_unlock(&_minor_lock);\n\n\treturn md ? 0 : -ENXIO;\n}\n\nstatic void dm_blk_close(struct gendisk *disk)\n{\n\tstruct mapped_device *md;\n\n\tspin_lock(&_minor_lock);\n\n\tmd = disk->private_data;\n\tif (WARN_ON(!md))\n\t\tgoto out;\n\n\tif (atomic_dec_and_test(&md->open_count) &&\n\t    (test_bit(DMF_DEFERRED_REMOVE, &md->flags)))\n\t\tqueue_work(deferred_remove_workqueue, &deferred_remove_work);\n\n\tdm_put(md);\nout:\n\tspin_unlock(&_minor_lock);\n}\n\nint dm_open_count(struct mapped_device *md)\n{\n\treturn atomic_read(&md->open_count);\n}\n\n \nint dm_lock_for_deletion(struct mapped_device *md, bool mark_deferred, bool only_deferred)\n{\n\tint r = 0;\n\n\tspin_lock(&_minor_lock);\n\n\tif (dm_open_count(md)) {\n\t\tr = -EBUSY;\n\t\tif (mark_deferred)\n\t\t\tset_bit(DMF_DEFERRED_REMOVE, &md->flags);\n\t} else if (only_deferred && !test_bit(DMF_DEFERRED_REMOVE, &md->flags))\n\t\tr = -EEXIST;\n\telse\n\t\tset_bit(DMF_DELETING, &md->flags);\n\n\tspin_unlock(&_minor_lock);\n\n\treturn r;\n}\n\nint dm_cancel_deferred_remove(struct mapped_device *md)\n{\n\tint r = 0;\n\n\tspin_lock(&_minor_lock);\n\n\tif (test_bit(DMF_DELETING, &md->flags))\n\t\tr = -EBUSY;\n\telse\n\t\tclear_bit(DMF_DEFERRED_REMOVE, &md->flags);\n\n\tspin_unlock(&_minor_lock);\n\n\treturn r;\n}\n\nstatic void do_deferred_remove(struct work_struct *w)\n{\n\tdm_deferred_remove();\n}\n\nstatic int dm_blk_getgeo(struct block_device *bdev, struct hd_geometry *geo)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\n\treturn dm_get_geometry(md, geo);\n}\n\nstatic int dm_prepare_ioctl(struct mapped_device *md, int *srcu_idx,\n\t\t\t    struct block_device **bdev)\n{\n\tstruct dm_target *ti;\n\tstruct dm_table *map;\n\tint r;\n\nretry:\n\tr = -ENOTTY;\n\tmap = dm_get_live_table(md, srcu_idx);\n\tif (!map || !dm_table_get_size(map))\n\t\treturn r;\n\n\t \n\tif (map->num_targets != 1)\n\t\treturn r;\n\n\tti = dm_table_get_target(map, 0);\n\tif (!ti->type->prepare_ioctl)\n\t\treturn r;\n\n\tif (dm_suspended_md(md))\n\t\treturn -EAGAIN;\n\n\tr = ti->type->prepare_ioctl(ti, bdev);\n\tif (r == -ENOTCONN && !fatal_signal_pending(current)) {\n\t\tdm_put_live_table(md, *srcu_idx);\n\t\tfsleep(10000);\n\t\tgoto retry;\n\t}\n\n\treturn r;\n}\n\nstatic void dm_unprepare_ioctl(struct mapped_device *md, int srcu_idx)\n{\n\tdm_put_live_table(md, srcu_idx);\n}\n\nstatic int dm_blk_ioctl(struct block_device *bdev, blk_mode_t mode,\n\t\t\tunsigned int cmd, unsigned long arg)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\tint r, srcu_idx;\n\n\tr = dm_prepare_ioctl(md, &srcu_idx, &bdev);\n\tif (r < 0)\n\t\tgoto out;\n\n\tif (r > 0) {\n\t\t \n\t\tif (!capable(CAP_SYS_RAWIO)) {\n\t\t\tDMDEBUG_LIMIT(\n\t\"%s: sending ioctl %x to DM device without required privilege.\",\n\t\t\t\tcurrent->comm, cmd);\n\t\t\tr = -ENOIOCTLCMD;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (!bdev->bd_disk->fops->ioctl)\n\t\tr = -ENOTTY;\n\telse\n\t\tr = bdev->bd_disk->fops->ioctl(bdev, mode, cmd, arg);\nout:\n\tdm_unprepare_ioctl(md, srcu_idx);\n\treturn r;\n}\n\nu64 dm_start_time_ns_from_clone(struct bio *bio)\n{\n\treturn jiffies_to_nsecs(clone_to_tio(bio)->io->start_time);\n}\nEXPORT_SYMBOL_GPL(dm_start_time_ns_from_clone);\n\nstatic inline bool bio_is_flush_with_data(struct bio *bio)\n{\n\treturn ((bio->bi_opf & REQ_PREFLUSH) && bio->bi_iter.bi_size);\n}\n\nstatic inline unsigned int dm_io_sectors(struct dm_io *io, struct bio *bio)\n{\n\t \n\tif (bio_is_flush_with_data(bio))\n\t\treturn 0;\n\tif (unlikely(dm_io_flagged(io, DM_IO_WAS_SPLIT)))\n\t\treturn io->sectors;\n\treturn bio_sectors(bio);\n}\n\nstatic void dm_io_acct(struct dm_io *io, bool end)\n{\n\tstruct bio *bio = io->orig_bio;\n\n\tif (dm_io_flagged(io, DM_IO_BLK_STAT)) {\n\t\tif (!end)\n\t\t\tbdev_start_io_acct(bio->bi_bdev, bio_op(bio),\n\t\t\t\t\t   io->start_time);\n\t\telse\n\t\t\tbdev_end_io_acct(bio->bi_bdev, bio_op(bio),\n\t\t\t\t\t dm_io_sectors(io, bio),\n\t\t\t\t\t io->start_time);\n\t}\n\n\tif (static_branch_unlikely(&stats_enabled) &&\n\t    unlikely(dm_stats_used(&io->md->stats))) {\n\t\tsector_t sector;\n\n\t\tif (unlikely(dm_io_flagged(io, DM_IO_WAS_SPLIT)))\n\t\t\tsector = bio_end_sector(bio) - io->sector_offset;\n\t\telse\n\t\t\tsector = bio->bi_iter.bi_sector;\n\n\t\tdm_stats_account_io(&io->md->stats, bio_data_dir(bio),\n\t\t\t\t    sector, dm_io_sectors(io, bio),\n\t\t\t\t    end, io->start_time, &io->stats_aux);\n\t}\n}\n\nstatic void __dm_start_io_acct(struct dm_io *io)\n{\n\tdm_io_acct(io, false);\n}\n\nstatic void dm_start_io_acct(struct dm_io *io, struct bio *clone)\n{\n\t \n\tif (dm_io_flagged(io, DM_IO_ACCOUNTED))\n\t\treturn;\n\n\t \n\tif (!clone || likely(dm_tio_is_normal(clone_to_tio(clone)))) {\n\t\tdm_io_set_flag(io, DM_IO_ACCOUNTED);\n\t} else {\n\t\tunsigned long flags;\n\t\t \n\t\tspin_lock_irqsave(&io->lock, flags);\n\t\tif (dm_io_flagged(io, DM_IO_ACCOUNTED)) {\n\t\t\tspin_unlock_irqrestore(&io->lock, flags);\n\t\t\treturn;\n\t\t}\n\t\tdm_io_set_flag(io, DM_IO_ACCOUNTED);\n\t\tspin_unlock_irqrestore(&io->lock, flags);\n\t}\n\n\t__dm_start_io_acct(io);\n}\n\nstatic void dm_end_io_acct(struct dm_io *io)\n{\n\tdm_io_acct(io, true);\n}\n\nstatic struct dm_io *alloc_io(struct mapped_device *md, struct bio *bio)\n{\n\tstruct dm_io *io;\n\tstruct dm_target_io *tio;\n\tstruct bio *clone;\n\n\tclone = bio_alloc_clone(NULL, bio, GFP_NOIO, &md->mempools->io_bs);\n\ttio = clone_to_tio(clone);\n\ttio->flags = 0;\n\tdm_tio_set_flag(tio, DM_TIO_INSIDE_DM_IO);\n\ttio->io = NULL;\n\n\tio = container_of(tio, struct dm_io, tio);\n\tio->magic = DM_IO_MAGIC;\n\tio->status = BLK_STS_OK;\n\n\t \n\tatomic_set(&io->io_count, 2);\n\tthis_cpu_inc(*md->pending_io);\n\tio->orig_bio = bio;\n\tio->md = md;\n\tspin_lock_init(&io->lock);\n\tio->start_time = jiffies;\n\tio->flags = 0;\n\tif (blk_queue_io_stat(md->queue))\n\t\tdm_io_set_flag(io, DM_IO_BLK_STAT);\n\n\tif (static_branch_unlikely(&stats_enabled) &&\n\t    unlikely(dm_stats_used(&md->stats)))\n\t\tdm_stats_record_start(&md->stats, &io->stats_aux);\n\n\treturn io;\n}\n\nstatic void free_io(struct dm_io *io)\n{\n\tbio_put(&io->tio.clone);\n}\n\nstatic struct bio *alloc_tio(struct clone_info *ci, struct dm_target *ti,\n\t\t\t     unsigned int target_bio_nr, unsigned int *len, gfp_t gfp_mask)\n{\n\tstruct mapped_device *md = ci->io->md;\n\tstruct dm_target_io *tio;\n\tstruct bio *clone;\n\n\tif (!ci->io->tio.io) {\n\t\t \n\t\ttio = &ci->io->tio;\n\t\t \n\t\tclone = &tio->clone;\n\t} else {\n\t\tclone = bio_alloc_clone(NULL, ci->bio, gfp_mask,\n\t\t\t\t\t&md->mempools->bs);\n\t\tif (!clone)\n\t\t\treturn NULL;\n\n\t\t \n\t\tclone->bi_opf &= ~REQ_DM_POLL_LIST;\n\n\t\ttio = clone_to_tio(clone);\n\t\ttio->flags = 0;  \n\t}\n\n\ttio->magic = DM_TIO_MAGIC;\n\ttio->io = ci->io;\n\ttio->ti = ti;\n\ttio->target_bio_nr = target_bio_nr;\n\ttio->len_ptr = len;\n\ttio->old_sector = 0;\n\n\t \n\tclone->bi_bdev = md->disk->part0;\n\tif (unlikely(ti->needs_bio_set_dev))\n\t\tbio_set_dev(clone, md->disk->part0);\n\n\tif (len) {\n\t\tclone->bi_iter.bi_size = to_bytes(*len);\n\t\tif (bio_integrity(clone))\n\t\t\tbio_integrity_trim(clone);\n\t}\n\n\treturn clone;\n}\n\nstatic void free_tio(struct bio *clone)\n{\n\tif (dm_tio_flagged(clone_to_tio(clone), DM_TIO_INSIDE_DM_IO))\n\t\treturn;\n\tbio_put(clone);\n}\n\n \nstatic void queue_io(struct mapped_device *md, struct bio *bio)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&md->deferred_lock, flags);\n\tbio_list_add(&md->deferred, bio);\n\tspin_unlock_irqrestore(&md->deferred_lock, flags);\n\tqueue_work(md->wq, &md->work);\n}\n\n \nstruct dm_table *dm_get_live_table(struct mapped_device *md,\n\t\t\t\t   int *srcu_idx) __acquires(md->io_barrier)\n{\n\t*srcu_idx = srcu_read_lock(&md->io_barrier);\n\n\treturn srcu_dereference(md->map, &md->io_barrier);\n}\n\nvoid dm_put_live_table(struct mapped_device *md,\n\t\t       int srcu_idx) __releases(md->io_barrier)\n{\n\tsrcu_read_unlock(&md->io_barrier, srcu_idx);\n}\n\nvoid dm_sync_table(struct mapped_device *md)\n{\n\tsynchronize_srcu(&md->io_barrier);\n\tsynchronize_rcu_expedited();\n}\n\n \nstatic struct dm_table *dm_get_live_table_fast(struct mapped_device *md) __acquires(RCU)\n{\n\trcu_read_lock();\n\treturn rcu_dereference(md->map);\n}\n\nstatic void dm_put_live_table_fast(struct mapped_device *md) __releases(RCU)\n{\n\trcu_read_unlock();\n}\n\nstatic char *_dm_claim_ptr = \"I belong to device-mapper\";\n\n \nstatic struct table_device *open_table_device(struct mapped_device *md,\n\t\tdev_t dev, blk_mode_t mode)\n{\n\tstruct table_device *td;\n\tstruct block_device *bdev;\n\tu64 part_off;\n\tint r;\n\n\ttd = kmalloc_node(sizeof(*td), GFP_KERNEL, md->numa_node_id);\n\tif (!td)\n\t\treturn ERR_PTR(-ENOMEM);\n\trefcount_set(&td->count, 1);\n\n\tbdev = blkdev_get_by_dev(dev, mode, _dm_claim_ptr, NULL);\n\tif (IS_ERR(bdev)) {\n\t\tr = PTR_ERR(bdev);\n\t\tgoto out_free_td;\n\t}\n\n\t \n\tif (md->disk->slave_dir) {\n\t\tr = bd_link_disk_holder(bdev, md->disk);\n\t\tif (r)\n\t\t\tgoto out_blkdev_put;\n\t}\n\n\ttd->dm_dev.mode = mode;\n\ttd->dm_dev.bdev = bdev;\n\ttd->dm_dev.dax_dev = fs_dax_get_by_bdev(bdev, &part_off, NULL, NULL);\n\tformat_dev_t(td->dm_dev.name, dev);\n\tlist_add(&td->list, &md->table_devices);\n\treturn td;\n\nout_blkdev_put:\n\tblkdev_put(bdev, _dm_claim_ptr);\nout_free_td:\n\tkfree(td);\n\treturn ERR_PTR(r);\n}\n\n \nstatic void close_table_device(struct table_device *td, struct mapped_device *md)\n{\n\tif (md->disk->slave_dir)\n\t\tbd_unlink_disk_holder(td->dm_dev.bdev, md->disk);\n\tblkdev_put(td->dm_dev.bdev, _dm_claim_ptr);\n\tput_dax(td->dm_dev.dax_dev);\n\tlist_del(&td->list);\n\tkfree(td);\n}\n\nstatic struct table_device *find_table_device(struct list_head *l, dev_t dev,\n\t\t\t\t\t      blk_mode_t mode)\n{\n\tstruct table_device *td;\n\n\tlist_for_each_entry(td, l, list)\n\t\tif (td->dm_dev.bdev->bd_dev == dev && td->dm_dev.mode == mode)\n\t\t\treturn td;\n\n\treturn NULL;\n}\n\nint dm_get_table_device(struct mapped_device *md, dev_t dev, blk_mode_t mode,\n\t\t\tstruct dm_dev **result)\n{\n\tstruct table_device *td;\n\n\tmutex_lock(&md->table_devices_lock);\n\ttd = find_table_device(&md->table_devices, dev, mode);\n\tif (!td) {\n\t\ttd = open_table_device(md, dev, mode);\n\t\tif (IS_ERR(td)) {\n\t\t\tmutex_unlock(&md->table_devices_lock);\n\t\t\treturn PTR_ERR(td);\n\t\t}\n\t} else {\n\t\trefcount_inc(&td->count);\n\t}\n\tmutex_unlock(&md->table_devices_lock);\n\n\t*result = &td->dm_dev;\n\treturn 0;\n}\n\nvoid dm_put_table_device(struct mapped_device *md, struct dm_dev *d)\n{\n\tstruct table_device *td = container_of(d, struct table_device, dm_dev);\n\n\tmutex_lock(&md->table_devices_lock);\n\tif (refcount_dec_and_test(&td->count))\n\t\tclose_table_device(td, md);\n\tmutex_unlock(&md->table_devices_lock);\n}\n\n \nint dm_get_geometry(struct mapped_device *md, struct hd_geometry *geo)\n{\n\t*geo = md->geometry;\n\n\treturn 0;\n}\n\n \nint dm_set_geometry(struct mapped_device *md, struct hd_geometry *geo)\n{\n\tsector_t sz = (sector_t)geo->cylinders * geo->heads * geo->sectors;\n\n\tif (geo->start > sz) {\n\t\tDMERR(\"Start sector is beyond the geometry limits.\");\n\t\treturn -EINVAL;\n\t}\n\n\tmd->geometry = *geo;\n\n\treturn 0;\n}\n\nstatic int __noflush_suspending(struct mapped_device *md)\n{\n\treturn test_bit(DMF_NOFLUSH_SUSPENDING, &md->flags);\n}\n\nstatic void dm_requeue_add_io(struct dm_io *io, bool first_stage)\n{\n\tstruct mapped_device *md = io->md;\n\n\tif (first_stage) {\n\t\tstruct dm_io *next = md->requeue_list;\n\n\t\tmd->requeue_list = io;\n\t\tio->next = next;\n\t} else {\n\t\tbio_list_add_head(&md->deferred, io->orig_bio);\n\t}\n}\n\nstatic void dm_kick_requeue(struct mapped_device *md, bool first_stage)\n{\n\tif (first_stage)\n\t\tqueue_work(md->wq, &md->requeue_work);\n\telse\n\t\tqueue_work(md->wq, &md->work);\n}\n\n \nstatic bool dm_handle_requeue(struct dm_io *io, bool first_stage)\n{\n\tstruct bio *bio = io->orig_bio;\n\tbool handle_requeue = (io->status == BLK_STS_DM_REQUEUE);\n\tbool handle_polled_eagain = ((io->status == BLK_STS_AGAIN) &&\n\t\t\t\t     (bio->bi_opf & REQ_POLLED));\n\tstruct mapped_device *md = io->md;\n\tbool requeued = false;\n\n\tif (handle_requeue || handle_polled_eagain) {\n\t\tunsigned long flags;\n\n\t\tif (bio->bi_opf & REQ_POLLED) {\n\t\t\t \n\t\t\tbio_clear_polled(bio);\n\t\t}\n\n\t\t \n\t\tspin_lock_irqsave(&md->deferred_lock, flags);\n\t\tif ((__noflush_suspending(md) &&\n\t\t     !WARN_ON_ONCE(dm_is_zone_write(md, bio))) ||\n\t\t    handle_polled_eagain || first_stage) {\n\t\t\tdm_requeue_add_io(io, first_stage);\n\t\t\trequeued = true;\n\t\t} else {\n\t\t\t \n\t\t\tio->status = BLK_STS_IOERR;\n\t\t}\n\t\tspin_unlock_irqrestore(&md->deferred_lock, flags);\n\t}\n\n\tif (requeued)\n\t\tdm_kick_requeue(md, first_stage);\n\n\treturn requeued;\n}\n\nstatic void __dm_io_complete(struct dm_io *io, bool first_stage)\n{\n\tstruct bio *bio = io->orig_bio;\n\tstruct mapped_device *md = io->md;\n\tblk_status_t io_error;\n\tbool requeued;\n\n\trequeued = dm_handle_requeue(io, first_stage);\n\tif (requeued && first_stage)\n\t\treturn;\n\n\tio_error = io->status;\n\tif (dm_io_flagged(io, DM_IO_ACCOUNTED))\n\t\tdm_end_io_acct(io);\n\telse if (!io_error) {\n\t\t \n\t\t__dm_start_io_acct(io);\n\t\tdm_end_io_acct(io);\n\t}\n\tfree_io(io);\n\tsmp_wmb();\n\tthis_cpu_dec(*md->pending_io);\n\n\t \n\tif (unlikely(wq_has_sleeper(&md->wait)))\n\t\twake_up(&md->wait);\n\n\t \n\tif (requeued)\n\t\treturn;\n\n\tif (bio_is_flush_with_data(bio)) {\n\t\t \n\t\tbio->bi_opf &= ~REQ_PREFLUSH;\n\t\tqueue_io(md, bio);\n\t} else {\n\t\t \n\t\tif (io_error)\n\t\t\tbio->bi_status = io_error;\n\t\tbio_endio(bio);\n\t}\n}\n\nstatic void dm_wq_requeue_work(struct work_struct *work)\n{\n\tstruct mapped_device *md = container_of(work, struct mapped_device,\n\t\t\t\t\t\trequeue_work);\n\tunsigned long flags;\n\tstruct dm_io *io;\n\n\t \n\tspin_lock_irqsave(&md->deferred_lock, flags);\n\tio = md->requeue_list;\n\tmd->requeue_list = NULL;\n\tspin_unlock_irqrestore(&md->deferred_lock, flags);\n\n\twhile (io) {\n\t\tstruct dm_io *next = io->next;\n\n\t\tdm_io_rewind(io, &md->disk->bio_split);\n\n\t\tio->next = NULL;\n\t\t__dm_io_complete(io, false);\n\t\tio = next;\n\t\tcond_resched();\n\t}\n}\n\n \nstatic void dm_io_complete(struct dm_io *io)\n{\n\tbool first_requeue;\n\n\t \n\tif (dm_io_flagged(io, DM_IO_WAS_SPLIT))\n\t\tfirst_requeue = true;\n\telse\n\t\tfirst_requeue = false;\n\n\t__dm_io_complete(io, first_requeue);\n}\n\n \nstatic inline void __dm_io_dec_pending(struct dm_io *io)\n{\n\tif (atomic_dec_and_test(&io->io_count))\n\t\tdm_io_complete(io);\n}\n\nstatic void dm_io_set_error(struct dm_io *io, blk_status_t error)\n{\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&io->lock, flags);\n\tif (!(io->status == BLK_STS_DM_REQUEUE &&\n\t      __noflush_suspending(io->md))) {\n\t\tio->status = error;\n\t}\n\tspin_unlock_irqrestore(&io->lock, flags);\n}\n\nstatic void dm_io_dec_pending(struct dm_io *io, blk_status_t error)\n{\n\tif (unlikely(error))\n\t\tdm_io_set_error(io, error);\n\n\t__dm_io_dec_pending(io);\n}\n\n \nstatic inline struct queue_limits *dm_get_queue_limits(struct mapped_device *md)\n{\n\treturn &md->queue->limits;\n}\n\nvoid disable_discard(struct mapped_device *md)\n{\n\tstruct queue_limits *limits = dm_get_queue_limits(md);\n\n\t \n\tlimits->max_discard_sectors = 0;\n}\n\nvoid disable_write_zeroes(struct mapped_device *md)\n{\n\tstruct queue_limits *limits = dm_get_queue_limits(md);\n\n\t \n\tlimits->max_write_zeroes_sectors = 0;\n}\n\nstatic bool swap_bios_limit(struct dm_target *ti, struct bio *bio)\n{\n\treturn unlikely((bio->bi_opf & REQ_SWAP) != 0) && unlikely(ti->limit_swap_bios);\n}\n\nstatic void clone_endio(struct bio *bio)\n{\n\tblk_status_t error = bio->bi_status;\n\tstruct dm_target_io *tio = clone_to_tio(bio);\n\tstruct dm_target *ti = tio->ti;\n\tdm_endio_fn endio = ti->type->end_io;\n\tstruct dm_io *io = tio->io;\n\tstruct mapped_device *md = io->md;\n\n\tif (unlikely(error == BLK_STS_TARGET)) {\n\t\tif (bio_op(bio) == REQ_OP_DISCARD &&\n\t\t    !bdev_max_discard_sectors(bio->bi_bdev))\n\t\t\tdisable_discard(md);\n\t\telse if (bio_op(bio) == REQ_OP_WRITE_ZEROES &&\n\t\t\t !bdev_write_zeroes_sectors(bio->bi_bdev))\n\t\t\tdisable_write_zeroes(md);\n\t}\n\n\tif (static_branch_unlikely(&zoned_enabled) &&\n\t    unlikely(bdev_is_zoned(bio->bi_bdev)))\n\t\tdm_zone_endio(io, bio);\n\n\tif (endio) {\n\t\tint r = endio(ti, bio, &error);\n\n\t\tswitch (r) {\n\t\tcase DM_ENDIO_REQUEUE:\n\t\t\tif (static_branch_unlikely(&zoned_enabled)) {\n\t\t\t\t \n\t\t\t\tif (WARN_ON_ONCE(dm_is_zone_write(md, bio)))\n\t\t\t\t\terror = BLK_STS_IOERR;\n\t\t\t\telse\n\t\t\t\t\terror = BLK_STS_DM_REQUEUE;\n\t\t\t} else\n\t\t\t\terror = BLK_STS_DM_REQUEUE;\n\t\t\tfallthrough;\n\t\tcase DM_ENDIO_DONE:\n\t\t\tbreak;\n\t\tcase DM_ENDIO_INCOMPLETE:\n\t\t\t \n\t\t\treturn;\n\t\tdefault:\n\t\t\tDMCRIT(\"unimplemented target endio return value: %d\", r);\n\t\t\tBUG();\n\t\t}\n\t}\n\n\tif (static_branch_unlikely(&swap_bios_enabled) &&\n\t    unlikely(swap_bios_limit(ti, bio)))\n\t\tup(&md->swap_bios_semaphore);\n\n\tfree_tio(bio);\n\tdm_io_dec_pending(io, error);\n}\n\n \nstatic inline sector_t max_io_len_target_boundary(struct dm_target *ti,\n\t\t\t\t\t\t  sector_t target_offset)\n{\n\treturn ti->len - target_offset;\n}\n\nstatic sector_t __max_io_len(struct dm_target *ti, sector_t sector,\n\t\t\t     unsigned int max_granularity,\n\t\t\t     unsigned int max_sectors)\n{\n\tsector_t target_offset = dm_target_offset(ti, sector);\n\tsector_t len = max_io_len_target_boundary(ti, target_offset);\n\n\t \n\tif (!max_granularity)\n\t\treturn len;\n\treturn min_t(sector_t, len,\n\t\tmin(max_sectors ? : queue_max_sectors(ti->table->md->queue),\n\t\t    blk_chunk_sectors_left(target_offset, max_granularity)));\n}\n\nstatic inline sector_t max_io_len(struct dm_target *ti, sector_t sector)\n{\n\treturn __max_io_len(ti, sector, ti->max_io_len, 0);\n}\n\nint dm_set_target_max_io_len(struct dm_target *ti, sector_t len)\n{\n\tif (len > UINT_MAX) {\n\t\tDMERR(\"Specified maximum size of target IO (%llu) exceeds limit (%u)\",\n\t\t      (unsigned long long)len, UINT_MAX);\n\t\tti->error = \"Maximum size of target IO is too large\";\n\t\treturn -EINVAL;\n\t}\n\n\tti->max_io_len = (uint32_t) len;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(dm_set_target_max_io_len);\n\nstatic struct dm_target *dm_dax_get_live_target(struct mapped_device *md,\n\t\t\t\t\t\tsector_t sector, int *srcu_idx)\n\t__acquires(md->io_barrier)\n{\n\tstruct dm_table *map;\n\tstruct dm_target *ti;\n\n\tmap = dm_get_live_table(md, srcu_idx);\n\tif (!map)\n\t\treturn NULL;\n\n\tti = dm_table_find_target(map, sector);\n\tif (!ti)\n\t\treturn NULL;\n\n\treturn ti;\n}\n\nstatic long dm_dax_direct_access(struct dax_device *dax_dev, pgoff_t pgoff,\n\t\tlong nr_pages, enum dax_access_mode mode, void **kaddr,\n\t\tpfn_t *pfn)\n{\n\tstruct mapped_device *md = dax_get_private(dax_dev);\n\tsector_t sector = pgoff * PAGE_SECTORS;\n\tstruct dm_target *ti;\n\tlong len, ret = -EIO;\n\tint srcu_idx;\n\n\tti = dm_dax_get_live_target(md, sector, &srcu_idx);\n\n\tif (!ti)\n\t\tgoto out;\n\tif (!ti->type->direct_access)\n\t\tgoto out;\n\tlen = max_io_len(ti, sector) / PAGE_SECTORS;\n\tif (len < 1)\n\t\tgoto out;\n\tnr_pages = min(len, nr_pages);\n\tret = ti->type->direct_access(ti, pgoff, nr_pages, mode, kaddr, pfn);\n\n out:\n\tdm_put_live_table(md, srcu_idx);\n\n\treturn ret;\n}\n\nstatic int dm_dax_zero_page_range(struct dax_device *dax_dev, pgoff_t pgoff,\n\t\t\t\t  size_t nr_pages)\n{\n\tstruct mapped_device *md = dax_get_private(dax_dev);\n\tsector_t sector = pgoff * PAGE_SECTORS;\n\tstruct dm_target *ti;\n\tint ret = -EIO;\n\tint srcu_idx;\n\n\tti = dm_dax_get_live_target(md, sector, &srcu_idx);\n\n\tif (!ti)\n\t\tgoto out;\n\tif (WARN_ON(!ti->type->dax_zero_page_range)) {\n\t\t \n\t\tgoto out;\n\t}\n\tret = ti->type->dax_zero_page_range(ti, pgoff, nr_pages);\n out:\n\tdm_put_live_table(md, srcu_idx);\n\n\treturn ret;\n}\n\nstatic size_t dm_dax_recovery_write(struct dax_device *dax_dev, pgoff_t pgoff,\n\t\tvoid *addr, size_t bytes, struct iov_iter *i)\n{\n\tstruct mapped_device *md = dax_get_private(dax_dev);\n\tsector_t sector = pgoff * PAGE_SECTORS;\n\tstruct dm_target *ti;\n\tint srcu_idx;\n\tlong ret = 0;\n\n\tti = dm_dax_get_live_target(md, sector, &srcu_idx);\n\tif (!ti || !ti->type->dax_recovery_write)\n\t\tgoto out;\n\n\tret = ti->type->dax_recovery_write(ti, pgoff, addr, bytes, i);\nout:\n\tdm_put_live_table(md, srcu_idx);\n\treturn ret;\n}\n\n \nvoid dm_accept_partial_bio(struct bio *bio, unsigned int n_sectors)\n{\n\tstruct dm_target_io *tio = clone_to_tio(bio);\n\tstruct dm_io *io = tio->io;\n\tunsigned int bio_sectors = bio_sectors(bio);\n\n\tBUG_ON(dm_tio_flagged(tio, DM_TIO_IS_DUPLICATE_BIO));\n\tBUG_ON(op_is_zone_mgmt(bio_op(bio)));\n\tBUG_ON(bio_op(bio) == REQ_OP_ZONE_APPEND);\n\tBUG_ON(bio_sectors > *tio->len_ptr);\n\tBUG_ON(n_sectors > bio_sectors);\n\n\t*tio->len_ptr -= bio_sectors - n_sectors;\n\tbio->bi_iter.bi_size = n_sectors << SECTOR_SHIFT;\n\n\t \n\tdm_io_set_flag(io, DM_IO_WAS_SPLIT);\n\tio->sectors = n_sectors;\n\tio->sector_offset = bio_sectors(io->orig_bio);\n}\nEXPORT_SYMBOL_GPL(dm_accept_partial_bio);\n\n \nvoid dm_submit_bio_remap(struct bio *clone, struct bio *tgt_clone)\n{\n\tstruct dm_target_io *tio = clone_to_tio(clone);\n\tstruct dm_io *io = tio->io;\n\n\t \n\tif (!tgt_clone)\n\t\ttgt_clone = clone;\n\n\t \n\tdm_start_io_acct(io, clone);\n\n\ttrace_block_bio_remap(tgt_clone, disk_devt(io->md->disk),\n\t\t\t      tio->old_sector);\n\tsubmit_bio_noacct(tgt_clone);\n}\nEXPORT_SYMBOL_GPL(dm_submit_bio_remap);\n\nstatic noinline void __set_swap_bios_limit(struct mapped_device *md, int latch)\n{\n\tmutex_lock(&md->swap_bios_lock);\n\twhile (latch < md->swap_bios) {\n\t\tcond_resched();\n\t\tdown(&md->swap_bios_semaphore);\n\t\tmd->swap_bios--;\n\t}\n\twhile (latch > md->swap_bios) {\n\t\tcond_resched();\n\t\tup(&md->swap_bios_semaphore);\n\t\tmd->swap_bios++;\n\t}\n\tmutex_unlock(&md->swap_bios_lock);\n}\n\nstatic void __map_bio(struct bio *clone)\n{\n\tstruct dm_target_io *tio = clone_to_tio(clone);\n\tstruct dm_target *ti = tio->ti;\n\tstruct dm_io *io = tio->io;\n\tstruct mapped_device *md = io->md;\n\tint r;\n\n\tclone->bi_end_io = clone_endio;\n\n\t \n\ttio->old_sector = clone->bi_iter.bi_sector;\n\n\tif (static_branch_unlikely(&swap_bios_enabled) &&\n\t    unlikely(swap_bios_limit(ti, clone))) {\n\t\tint latch = get_swap_bios();\n\n\t\tif (unlikely(latch != md->swap_bios))\n\t\t\t__set_swap_bios_limit(md, latch);\n\t\tdown(&md->swap_bios_semaphore);\n\t}\n\n\tif (static_branch_unlikely(&zoned_enabled)) {\n\t\t \n\t\tif (unlikely(dm_emulate_zone_append(md)))\n\t\t\tr = dm_zone_map_bio(tio);\n\t\telse\n\t\t\tr = ti->type->map(ti, clone);\n\t} else\n\t\tr = ti->type->map(ti, clone);\n\n\tswitch (r) {\n\tcase DM_MAPIO_SUBMITTED:\n\t\t \n\t\tif (!ti->accounts_remapped_io)\n\t\t\tdm_start_io_acct(io, clone);\n\t\tbreak;\n\tcase DM_MAPIO_REMAPPED:\n\t\tdm_submit_bio_remap(clone, NULL);\n\t\tbreak;\n\tcase DM_MAPIO_KILL:\n\tcase DM_MAPIO_REQUEUE:\n\t\tif (static_branch_unlikely(&swap_bios_enabled) &&\n\t\t    unlikely(swap_bios_limit(ti, clone)))\n\t\t\tup(&md->swap_bios_semaphore);\n\t\tfree_tio(clone);\n\t\tif (r == DM_MAPIO_KILL)\n\t\t\tdm_io_dec_pending(io, BLK_STS_IOERR);\n\t\telse\n\t\t\tdm_io_dec_pending(io, BLK_STS_DM_REQUEUE);\n\t\tbreak;\n\tdefault:\n\t\tDMCRIT(\"unimplemented target map return value: %d\", r);\n\t\tBUG();\n\t}\n}\n\nstatic void setup_split_accounting(struct clone_info *ci, unsigned int len)\n{\n\tstruct dm_io *io = ci->io;\n\n\tif (ci->sector_count > len) {\n\t\t \n\t\tdm_io_set_flag(io, DM_IO_WAS_SPLIT);\n\t\tio->sectors = len;\n\t\tio->sector_offset = bio_sectors(ci->bio);\n\t}\n}\n\nstatic void alloc_multiple_bios(struct bio_list *blist, struct clone_info *ci,\n\t\t\t\tstruct dm_target *ti, unsigned int num_bios,\n\t\t\t\tunsigned *len)\n{\n\tstruct bio *bio;\n\tint try;\n\n\tfor (try = 0; try < 2; try++) {\n\t\tint bio_nr;\n\n\t\tif (try)\n\t\t\tmutex_lock(&ci->io->md->table_devices_lock);\n\t\tfor (bio_nr = 0; bio_nr < num_bios; bio_nr++) {\n\t\t\tbio = alloc_tio(ci, ti, bio_nr, len,\n\t\t\t\t\ttry ? GFP_NOIO : GFP_NOWAIT);\n\t\t\tif (!bio)\n\t\t\t\tbreak;\n\n\t\t\tbio_list_add(blist, bio);\n\t\t}\n\t\tif (try)\n\t\t\tmutex_unlock(&ci->io->md->table_devices_lock);\n\t\tif (bio_nr == num_bios)\n\t\t\treturn;\n\n\t\twhile ((bio = bio_list_pop(blist)))\n\t\t\tfree_tio(bio);\n\t}\n}\n\nstatic int __send_duplicate_bios(struct clone_info *ci, struct dm_target *ti,\n\t\t\t\t unsigned int num_bios, unsigned int *len)\n{\n\tstruct bio_list blist = BIO_EMPTY_LIST;\n\tstruct bio *clone;\n\tunsigned int ret = 0;\n\n\tswitch (num_bios) {\n\tcase 0:\n\t\tbreak;\n\tcase 1:\n\t\tif (len)\n\t\t\tsetup_split_accounting(ci, *len);\n\t\tclone = alloc_tio(ci, ti, 0, len, GFP_NOIO);\n\t\t__map_bio(clone);\n\t\tret = 1;\n\t\tbreak;\n\tdefault:\n\t\tif (len)\n\t\t\tsetup_split_accounting(ci, *len);\n\t\t \n\t\talloc_multiple_bios(&blist, ci, ti, num_bios, len);\n\t\twhile ((clone = bio_list_pop(&blist))) {\n\t\t\tdm_tio_set_flag(clone_to_tio(clone), DM_TIO_IS_DUPLICATE_BIO);\n\t\t\t__map_bio(clone);\n\t\t\tret += 1;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void __send_empty_flush(struct clone_info *ci)\n{\n\tstruct dm_table *t = ci->map;\n\tstruct bio flush_bio;\n\n\t \n\tbio_init(&flush_bio, ci->io->md->disk->part0, NULL, 0,\n\t\t REQ_OP_WRITE | REQ_PREFLUSH | REQ_SYNC);\n\n\tci->bio = &flush_bio;\n\tci->sector_count = 0;\n\tci->io->tio.clone.bi_iter.bi_size = 0;\n\n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tunsigned int bios;\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tatomic_add(ti->num_flush_bios, &ci->io->io_count);\n\t\tbios = __send_duplicate_bios(ci, ti, ti->num_flush_bios, NULL);\n\t\tatomic_sub(ti->num_flush_bios - bios, &ci->io->io_count);\n\t}\n\n\t \n\tatomic_sub(1, &ci->io->io_count);\n\n\tbio_uninit(ci->bio);\n}\n\nstatic void __send_changing_extent_only(struct clone_info *ci, struct dm_target *ti,\n\t\t\t\t\tunsigned int num_bios,\n\t\t\t\t\tunsigned int max_granularity,\n\t\t\t\t\tunsigned int max_sectors)\n{\n\tunsigned int len, bios;\n\n\tlen = min_t(sector_t, ci->sector_count,\n\t\t    __max_io_len(ti, ci->sector, max_granularity, max_sectors));\n\n\tatomic_add(num_bios, &ci->io->io_count);\n\tbios = __send_duplicate_bios(ci, ti, num_bios, &len);\n\t \n\tatomic_sub(num_bios - bios + 1, &ci->io->io_count);\n\n\tci->sector += len;\n\tci->sector_count -= len;\n}\n\nstatic bool is_abnormal_io(struct bio *bio)\n{\n\tenum req_op op = bio_op(bio);\n\n\tif (op != REQ_OP_READ && op != REQ_OP_WRITE && op != REQ_OP_FLUSH) {\n\t\tswitch (op) {\n\t\tcase REQ_OP_DISCARD:\n\t\tcase REQ_OP_SECURE_ERASE:\n\t\tcase REQ_OP_WRITE_ZEROES:\n\t\t\treturn true;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic blk_status_t __process_abnormal_io(struct clone_info *ci,\n\t\t\t\t\t  struct dm_target *ti)\n{\n\tunsigned int num_bios = 0;\n\tunsigned int max_granularity = 0;\n\tunsigned int max_sectors = 0;\n\tstruct queue_limits *limits = dm_get_queue_limits(ti->table->md);\n\n\tswitch (bio_op(ci->bio)) {\n\tcase REQ_OP_DISCARD:\n\t\tnum_bios = ti->num_discard_bios;\n\t\tmax_sectors = limits->max_discard_sectors;\n\t\tif (ti->max_discard_granularity)\n\t\t\tmax_granularity = max_sectors;\n\t\tbreak;\n\tcase REQ_OP_SECURE_ERASE:\n\t\tnum_bios = ti->num_secure_erase_bios;\n\t\tmax_sectors = limits->max_secure_erase_sectors;\n\t\tif (ti->max_secure_erase_granularity)\n\t\t\tmax_granularity = max_sectors;\n\t\tbreak;\n\tcase REQ_OP_WRITE_ZEROES:\n\t\tnum_bios = ti->num_write_zeroes_bios;\n\t\tmax_sectors = limits->max_write_zeroes_sectors;\n\t\tif (ti->max_write_zeroes_granularity)\n\t\t\tmax_granularity = max_sectors;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tif (unlikely(!num_bios))\n\t\treturn BLK_STS_NOTSUPP;\n\n\t__send_changing_extent_only(ci, ti, num_bios,\n\t\t\t\t    max_granularity, max_sectors);\n\treturn BLK_STS_OK;\n}\n\n \nstatic inline struct dm_io **dm_poll_list_head(struct bio *bio)\n{\n\treturn (struct dm_io **)&bio->bi_private;\n}\n\nstatic void dm_queue_poll_io(struct bio *bio, struct dm_io *io)\n{\n\tstruct dm_io **head = dm_poll_list_head(bio);\n\n\tif (!(bio->bi_opf & REQ_DM_POLL_LIST)) {\n\t\tbio->bi_opf |= REQ_DM_POLL_LIST;\n\t\t \n\t\tio->data = bio->bi_private;\n\n\t\t \n\t\tbio->bi_cookie = ~BLK_QC_T_NONE;\n\n\t\tio->next = NULL;\n\t} else {\n\t\t \n\t\tio->data = (*head)->data;\n\t\tio->next = *head;\n\t}\n\n\t*head = io;\n}\n\n \nstatic blk_status_t __split_and_process_bio(struct clone_info *ci)\n{\n\tstruct bio *clone;\n\tstruct dm_target *ti;\n\tunsigned int len;\n\n\tti = dm_table_find_target(ci->map, ci->sector);\n\tif (unlikely(!ti))\n\t\treturn BLK_STS_IOERR;\n\n\tif (unlikely((ci->bio->bi_opf & REQ_NOWAIT) != 0) &&\n\t    unlikely(!dm_target_supports_nowait(ti->type)))\n\t\treturn BLK_STS_NOTSUPP;\n\n\tif (unlikely(ci->is_abnormal_io))\n\t\treturn __process_abnormal_io(ci, ti);\n\n\t \n\tci->submit_as_polled = !!(ci->bio->bi_opf & REQ_POLLED);\n\n\tlen = min_t(sector_t, max_io_len(ti, ci->sector), ci->sector_count);\n\tsetup_split_accounting(ci, len);\n\tclone = alloc_tio(ci, ti, 0, &len, GFP_NOIO);\n\t__map_bio(clone);\n\n\tci->sector += len;\n\tci->sector_count -= len;\n\n\treturn BLK_STS_OK;\n}\n\nstatic void init_clone_info(struct clone_info *ci, struct mapped_device *md,\n\t\t\t    struct dm_table *map, struct bio *bio, bool is_abnormal)\n{\n\tci->map = map;\n\tci->io = alloc_io(md, bio);\n\tci->bio = bio;\n\tci->is_abnormal_io = is_abnormal;\n\tci->submit_as_polled = false;\n\tci->sector = bio->bi_iter.bi_sector;\n\tci->sector_count = bio_sectors(bio);\n\n\t \n\tif (static_branch_unlikely(&zoned_enabled) &&\n\t    WARN_ON_ONCE(op_is_zone_mgmt(bio_op(bio)) && ci->sector_count))\n\t\tci->sector_count = 0;\n}\n\n \nstatic void dm_split_and_process_bio(struct mapped_device *md,\n\t\t\t\t     struct dm_table *map, struct bio *bio)\n{\n\tstruct clone_info ci;\n\tstruct dm_io *io;\n\tblk_status_t error = BLK_STS_OK;\n\tbool is_abnormal;\n\n\tis_abnormal = is_abnormal_io(bio);\n\tif (unlikely(is_abnormal)) {\n\t\t \n\t\tbio = bio_split_to_limits(bio);\n\t\tif (!bio)\n\t\t\treturn;\n\t}\n\n\tinit_clone_info(&ci, md, map, bio, is_abnormal);\n\tio = ci.io;\n\n\tif (bio->bi_opf & REQ_PREFLUSH) {\n\t\t__send_empty_flush(&ci);\n\t\t \n\t\tgoto out;\n\t}\n\n\terror = __split_and_process_bio(&ci);\n\tif (error || !ci.sector_count)\n\t\tgoto out;\n\t \n\tbio_trim(bio, io->sectors, ci.sector_count);\n\ttrace_block_split(bio, bio->bi_iter.bi_sector);\n\tbio_inc_remaining(bio);\n\tsubmit_bio_noacct(bio);\nout:\n\t \n\tif (error || !ci.submit_as_polled) {\n\t\t \n\t\tif (error)\n\t\t\tatomic_dec(&io->io_count);\n\t\tdm_io_dec_pending(io, error);\n\t} else\n\t\tdm_queue_poll_io(bio, io);\n}\n\nstatic void dm_submit_bio(struct bio *bio)\n{\n\tstruct mapped_device *md = bio->bi_bdev->bd_disk->private_data;\n\tint srcu_idx;\n\tstruct dm_table *map;\n\n\tmap = dm_get_live_table(md, &srcu_idx);\n\n\t \n\tif (unlikely(test_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags)) ||\n\t    unlikely(!map)) {\n\t\tif (bio->bi_opf & REQ_NOWAIT)\n\t\t\tbio_wouldblock_error(bio);\n\t\telse if (bio->bi_opf & REQ_RAHEAD)\n\t\t\tbio_io_error(bio);\n\t\telse\n\t\t\tqueue_io(md, bio);\n\t\tgoto out;\n\t}\n\n\tdm_split_and_process_bio(md, map, bio);\nout:\n\tdm_put_live_table(md, srcu_idx);\n}\n\nstatic bool dm_poll_dm_io(struct dm_io *io, struct io_comp_batch *iob,\n\t\t\t  unsigned int flags)\n{\n\tWARN_ON_ONCE(!dm_tio_is_normal(&io->tio));\n\n\t \n\tif (atomic_read(&io->io_count) > 1)\n\t\tbio_poll(&io->tio.clone, iob, flags);\n\n\t \n\treturn atomic_read(&io->io_count) == 1;\n}\n\nstatic int dm_poll_bio(struct bio *bio, struct io_comp_batch *iob,\n\t\t       unsigned int flags)\n{\n\tstruct dm_io **head = dm_poll_list_head(bio);\n\tstruct dm_io *list = *head;\n\tstruct dm_io *tmp = NULL;\n\tstruct dm_io *curr, *next;\n\n\t \n\tif (!(bio->bi_opf & REQ_DM_POLL_LIST))\n\t\treturn 0;\n\n\tWARN_ON_ONCE(!list);\n\n\t \n\tbio->bi_opf &= ~REQ_DM_POLL_LIST;\n\tbio->bi_private = list->data;\n\n\tfor (curr = list, next = curr->next; curr; curr = next, next =\n\t\t\tcurr ? curr->next : NULL) {\n\t\tif (dm_poll_dm_io(curr, iob, flags)) {\n\t\t\t \n\t\t\t__dm_io_dec_pending(curr);\n\t\t} else {\n\t\t\tcurr->next = tmp;\n\t\t\ttmp = curr;\n\t\t}\n\t}\n\n\t \n\tif (tmp) {\n\t\tbio->bi_opf |= REQ_DM_POLL_LIST;\n\t\t \n\t\t*head = tmp;\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\n \nstatic void free_minor(int minor)\n{\n\tspin_lock(&_minor_lock);\n\tidr_remove(&_minor_idr, minor);\n\tspin_unlock(&_minor_lock);\n}\n\n \nstatic int specific_minor(int minor)\n{\n\tint r;\n\n\tif (minor >= (1 << MINORBITS))\n\t\treturn -EINVAL;\n\n\tidr_preload(GFP_KERNEL);\n\tspin_lock(&_minor_lock);\n\n\tr = idr_alloc(&_minor_idr, MINOR_ALLOCED, minor, minor + 1, GFP_NOWAIT);\n\n\tspin_unlock(&_minor_lock);\n\tidr_preload_end();\n\tif (r < 0)\n\t\treturn r == -ENOSPC ? -EBUSY : r;\n\treturn 0;\n}\n\nstatic int next_free_minor(int *minor)\n{\n\tint r;\n\n\tidr_preload(GFP_KERNEL);\n\tspin_lock(&_minor_lock);\n\n\tr = idr_alloc(&_minor_idr, MINOR_ALLOCED, 0, 1 << MINORBITS, GFP_NOWAIT);\n\n\tspin_unlock(&_minor_lock);\n\tidr_preload_end();\n\tif (r < 0)\n\t\treturn r;\n\t*minor = r;\n\treturn 0;\n}\n\nstatic const struct block_device_operations dm_blk_dops;\nstatic const struct block_device_operations dm_rq_blk_dops;\nstatic const struct dax_operations dm_dax_ops;\n\nstatic void dm_wq_work(struct work_struct *work);\n\n#ifdef CONFIG_BLK_INLINE_ENCRYPTION\nstatic void dm_queue_destroy_crypto_profile(struct request_queue *q)\n{\n\tdm_destroy_crypto_profile(q->crypto_profile);\n}\n\n#else  \n\nstatic inline void dm_queue_destroy_crypto_profile(struct request_queue *q)\n{\n}\n#endif  \n\nstatic void cleanup_mapped_device(struct mapped_device *md)\n{\n\tif (md->wq)\n\t\tdestroy_workqueue(md->wq);\n\tdm_free_md_mempools(md->mempools);\n\n\tif (md->dax_dev) {\n\t\tdax_remove_host(md->disk);\n\t\tkill_dax(md->dax_dev);\n\t\tput_dax(md->dax_dev);\n\t\tmd->dax_dev = NULL;\n\t}\n\n\tdm_cleanup_zoned_dev(md);\n\tif (md->disk) {\n\t\tspin_lock(&_minor_lock);\n\t\tmd->disk->private_data = NULL;\n\t\tspin_unlock(&_minor_lock);\n\t\tif (dm_get_md_type(md) != DM_TYPE_NONE) {\n\t\t\tstruct table_device *td;\n\n\t\t\tdm_sysfs_exit(md);\n\t\t\tlist_for_each_entry(td, &md->table_devices, list) {\n\t\t\t\tbd_unlink_disk_holder(td->dm_dev.bdev,\n\t\t\t\t\t\t      md->disk);\n\t\t\t}\n\n\t\t\t \n\t\t\tmutex_lock(&md->table_devices_lock);\n\t\t\tdel_gendisk(md->disk);\n\t\t\tmutex_unlock(&md->table_devices_lock);\n\t\t}\n\t\tdm_queue_destroy_crypto_profile(md->queue);\n\t\tput_disk(md->disk);\n\t}\n\n\tif (md->pending_io) {\n\t\tfree_percpu(md->pending_io);\n\t\tmd->pending_io = NULL;\n\t}\n\n\tcleanup_srcu_struct(&md->io_barrier);\n\n\tmutex_destroy(&md->suspend_lock);\n\tmutex_destroy(&md->type_lock);\n\tmutex_destroy(&md->table_devices_lock);\n\tmutex_destroy(&md->swap_bios_lock);\n\n\tdm_mq_cleanup_mapped_device(md);\n}\n\n \nstatic struct mapped_device *alloc_dev(int minor)\n{\n\tint r, numa_node_id = dm_get_numa_node();\n\tstruct mapped_device *md;\n\tvoid *old_md;\n\n\tmd = kvzalloc_node(sizeof(*md), GFP_KERNEL, numa_node_id);\n\tif (!md) {\n\t\tDMERR(\"unable to allocate device, out of memory.\");\n\t\treturn NULL;\n\t}\n\n\tif (!try_module_get(THIS_MODULE))\n\t\tgoto bad_module_get;\n\n\t \n\tif (minor == DM_ANY_MINOR)\n\t\tr = next_free_minor(&minor);\n\telse\n\t\tr = specific_minor(minor);\n\tif (r < 0)\n\t\tgoto bad_minor;\n\n\tr = init_srcu_struct(&md->io_barrier);\n\tif (r < 0)\n\t\tgoto bad_io_barrier;\n\n\tmd->numa_node_id = numa_node_id;\n\tmd->init_tio_pdu = false;\n\tmd->type = DM_TYPE_NONE;\n\tmutex_init(&md->suspend_lock);\n\tmutex_init(&md->type_lock);\n\tmutex_init(&md->table_devices_lock);\n\tspin_lock_init(&md->deferred_lock);\n\tatomic_set(&md->holders, 1);\n\tatomic_set(&md->open_count, 0);\n\tatomic_set(&md->event_nr, 0);\n\tatomic_set(&md->uevent_seq, 0);\n\tINIT_LIST_HEAD(&md->uevent_list);\n\tINIT_LIST_HEAD(&md->table_devices);\n\tspin_lock_init(&md->uevent_lock);\n\n\t \n\tmd->disk = blk_alloc_disk(md->numa_node_id);\n\tif (!md->disk)\n\t\tgoto bad;\n\tmd->queue = md->disk->queue;\n\n\tinit_waitqueue_head(&md->wait);\n\tINIT_WORK(&md->work, dm_wq_work);\n\tINIT_WORK(&md->requeue_work, dm_wq_requeue_work);\n\tinit_waitqueue_head(&md->eventq);\n\tinit_completion(&md->kobj_holder.completion);\n\n\tmd->requeue_list = NULL;\n\tmd->swap_bios = get_swap_bios();\n\tsema_init(&md->swap_bios_semaphore, md->swap_bios);\n\tmutex_init(&md->swap_bios_lock);\n\n\tmd->disk->major = _major;\n\tmd->disk->first_minor = minor;\n\tmd->disk->minors = 1;\n\tmd->disk->flags |= GENHD_FL_NO_PART;\n\tmd->disk->fops = &dm_blk_dops;\n\tmd->disk->private_data = md;\n\tsprintf(md->disk->disk_name, \"dm-%d\", minor);\n\n\tif (IS_ENABLED(CONFIG_FS_DAX)) {\n\t\tmd->dax_dev = alloc_dax(md, &dm_dax_ops);\n\t\tif (IS_ERR(md->dax_dev)) {\n\t\t\tmd->dax_dev = NULL;\n\t\t\tgoto bad;\n\t\t}\n\t\tset_dax_nocache(md->dax_dev);\n\t\tset_dax_nomc(md->dax_dev);\n\t\tif (dax_add_host(md->dax_dev, md->disk))\n\t\t\tgoto bad;\n\t}\n\n\tformat_dev_t(md->name, MKDEV(_major, minor));\n\n\tmd->wq = alloc_workqueue(\"kdmflush/%s\", WQ_MEM_RECLAIM, 0, md->name);\n\tif (!md->wq)\n\t\tgoto bad;\n\n\tmd->pending_io = alloc_percpu(unsigned long);\n\tif (!md->pending_io)\n\t\tgoto bad;\n\n\tr = dm_stats_init(&md->stats);\n\tif (r < 0)\n\t\tgoto bad;\n\n\t \n\tspin_lock(&_minor_lock);\n\told_md = idr_replace(&_minor_idr, md, minor);\n\tspin_unlock(&_minor_lock);\n\n\tBUG_ON(old_md != MINOR_ALLOCED);\n\n\treturn md;\n\nbad:\n\tcleanup_mapped_device(md);\nbad_io_barrier:\n\tfree_minor(minor);\nbad_minor:\n\tmodule_put(THIS_MODULE);\nbad_module_get:\n\tkvfree(md);\n\treturn NULL;\n}\n\nstatic void unlock_fs(struct mapped_device *md);\n\nstatic void free_dev(struct mapped_device *md)\n{\n\tint minor = MINOR(disk_devt(md->disk));\n\n\tunlock_fs(md);\n\n\tcleanup_mapped_device(md);\n\n\tWARN_ON_ONCE(!list_empty(&md->table_devices));\n\tdm_stats_cleanup(&md->stats);\n\tfree_minor(minor);\n\n\tmodule_put(THIS_MODULE);\n\tkvfree(md);\n}\n\n \nstatic void event_callback(void *context)\n{\n\tunsigned long flags;\n\tLIST_HEAD(uevents);\n\tstruct mapped_device *md = context;\n\n\tspin_lock_irqsave(&md->uevent_lock, flags);\n\tlist_splice_init(&md->uevent_list, &uevents);\n\tspin_unlock_irqrestore(&md->uevent_lock, flags);\n\n\tdm_send_uevents(&uevents, &disk_to_dev(md->disk)->kobj);\n\n\tatomic_inc(&md->event_nr);\n\twake_up(&md->eventq);\n\tdm_issue_global_event();\n}\n\n \nstatic struct dm_table *__bind(struct mapped_device *md, struct dm_table *t,\n\t\t\t       struct queue_limits *limits)\n{\n\tstruct dm_table *old_map;\n\tsector_t size;\n\tint ret;\n\n\tlockdep_assert_held(&md->suspend_lock);\n\n\tsize = dm_table_get_size(t);\n\n\t \n\tif (size != dm_get_size(md))\n\t\tmemset(&md->geometry, 0, sizeof(md->geometry));\n\n\tset_capacity(md->disk, size);\n\n\tdm_table_event_callback(t, event_callback, md);\n\n\tif (dm_table_request_based(t)) {\n\t\t \n\t\tmd->immutable_target = dm_table_get_immutable_target(t);\n\n\t\t \n\t\tif (!md->mempools) {\n\t\t\tmd->mempools = t->mempools;\n\t\t\tt->mempools = NULL;\n\t\t}\n\t} else {\n\t\t \n\t\tdm_free_md_mempools(md->mempools);\n\t\tmd->mempools = t->mempools;\n\t\tt->mempools = NULL;\n\t}\n\n\tret = dm_table_set_restrictions(t, md->queue, limits);\n\tif (ret) {\n\t\told_map = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\told_map = rcu_dereference_protected(md->map, lockdep_is_held(&md->suspend_lock));\n\trcu_assign_pointer(md->map, (void *)t);\n\tmd->immutable_target_type = dm_table_get_immutable_target_type(t);\n\n\tif (old_map)\n\t\tdm_sync_table(md);\nout:\n\treturn old_map;\n}\n\n \nstatic struct dm_table *__unbind(struct mapped_device *md)\n{\n\tstruct dm_table *map = rcu_dereference_protected(md->map, 1);\n\n\tif (!map)\n\t\treturn NULL;\n\n\tdm_table_event_callback(map, NULL, NULL);\n\tRCU_INIT_POINTER(md->map, NULL);\n\tdm_sync_table(md);\n\n\treturn map;\n}\n\n \nint dm_create(int minor, struct mapped_device **result)\n{\n\tstruct mapped_device *md;\n\n\tmd = alloc_dev(minor);\n\tif (!md)\n\t\treturn -ENXIO;\n\n\tdm_ima_reset_data(md);\n\n\t*result = md;\n\treturn 0;\n}\n\n \nvoid dm_lock_md_type(struct mapped_device *md)\n{\n\tmutex_lock(&md->type_lock);\n}\n\nvoid dm_unlock_md_type(struct mapped_device *md)\n{\n\tmutex_unlock(&md->type_lock);\n}\n\nvoid dm_set_md_type(struct mapped_device *md, enum dm_queue_mode type)\n{\n\tBUG_ON(!mutex_is_locked(&md->type_lock));\n\tmd->type = type;\n}\n\nenum dm_queue_mode dm_get_md_type(struct mapped_device *md)\n{\n\treturn md->type;\n}\n\nstruct target_type *dm_get_immutable_target_type(struct mapped_device *md)\n{\n\treturn md->immutable_target_type;\n}\n\n \nint dm_setup_md_queue(struct mapped_device *md, struct dm_table *t)\n{\n\tenum dm_queue_mode type = dm_table_get_type(t);\n\tstruct queue_limits limits;\n\tstruct table_device *td;\n\tint r;\n\n\tswitch (type) {\n\tcase DM_TYPE_REQUEST_BASED:\n\t\tmd->disk->fops = &dm_rq_blk_dops;\n\t\tr = dm_mq_init_request_queue(md, t);\n\t\tif (r) {\n\t\t\tDMERR(\"Cannot initialize queue for request-based dm mapped device\");\n\t\t\treturn r;\n\t\t}\n\t\tbreak;\n\tcase DM_TYPE_BIO_BASED:\n\tcase DM_TYPE_DAX_BIO_BASED:\n\t\tblk_queue_flag_set(QUEUE_FLAG_IO_STAT, md->queue);\n\t\tbreak;\n\tcase DM_TYPE_NONE:\n\t\tWARN_ON_ONCE(true);\n\t\tbreak;\n\t}\n\n\tr = dm_calculate_queue_limits(t, &limits);\n\tif (r) {\n\t\tDMERR(\"Cannot calculate initial queue limits\");\n\t\treturn r;\n\t}\n\tr = dm_table_set_restrictions(t, md->queue, &limits);\n\tif (r)\n\t\treturn r;\n\n\t \n\tmutex_lock(&md->table_devices_lock);\n\tr = add_disk(md->disk);\n\tmutex_unlock(&md->table_devices_lock);\n\tif (r)\n\t\treturn r;\n\n\t \n\tlist_for_each_entry(td, &md->table_devices, list) {\n\t\tr = bd_link_disk_holder(td->dm_dev.bdev, md->disk);\n\t\tif (r)\n\t\t\tgoto out_undo_holders;\n\t}\n\n\tr = dm_sysfs_init(md);\n\tif (r)\n\t\tgoto out_undo_holders;\n\n\tmd->type = type;\n\treturn 0;\n\nout_undo_holders:\n\tlist_for_each_entry_continue_reverse(td, &md->table_devices, list)\n\t\tbd_unlink_disk_holder(td->dm_dev.bdev, md->disk);\n\tmutex_lock(&md->table_devices_lock);\n\tdel_gendisk(md->disk);\n\tmutex_unlock(&md->table_devices_lock);\n\treturn r;\n}\n\nstruct mapped_device *dm_get_md(dev_t dev)\n{\n\tstruct mapped_device *md;\n\tunsigned int minor = MINOR(dev);\n\n\tif (MAJOR(dev) != _major || minor >= (1 << MINORBITS))\n\t\treturn NULL;\n\n\tspin_lock(&_minor_lock);\n\n\tmd = idr_find(&_minor_idr, minor);\n\tif (!md || md == MINOR_ALLOCED || (MINOR(disk_devt(dm_disk(md))) != minor) ||\n\t    test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {\n\t\tmd = NULL;\n\t\tgoto out;\n\t}\n\tdm_get(md);\nout:\n\tspin_unlock(&_minor_lock);\n\n\treturn md;\n}\nEXPORT_SYMBOL_GPL(dm_get_md);\n\nvoid *dm_get_mdptr(struct mapped_device *md)\n{\n\treturn md->interface_ptr;\n}\n\nvoid dm_set_mdptr(struct mapped_device *md, void *ptr)\n{\n\tmd->interface_ptr = ptr;\n}\n\nvoid dm_get(struct mapped_device *md)\n{\n\tatomic_inc(&md->holders);\n\tBUG_ON(test_bit(DMF_FREEING, &md->flags));\n}\n\nint dm_hold(struct mapped_device *md)\n{\n\tspin_lock(&_minor_lock);\n\tif (test_bit(DMF_FREEING, &md->flags)) {\n\t\tspin_unlock(&_minor_lock);\n\t\treturn -EBUSY;\n\t}\n\tdm_get(md);\n\tspin_unlock(&_minor_lock);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(dm_hold);\n\nconst char *dm_device_name(struct mapped_device *md)\n{\n\treturn md->name;\n}\nEXPORT_SYMBOL_GPL(dm_device_name);\n\nstatic void __dm_destroy(struct mapped_device *md, bool wait)\n{\n\tstruct dm_table *map;\n\tint srcu_idx;\n\n\tmight_sleep();\n\n\tspin_lock(&_minor_lock);\n\tidr_replace(&_minor_idr, MINOR_ALLOCED, MINOR(disk_devt(dm_disk(md))));\n\tset_bit(DMF_FREEING, &md->flags);\n\tspin_unlock(&_minor_lock);\n\n\tblk_mark_disk_dead(md->disk);\n\n\t \n\tmutex_lock(&md->suspend_lock);\n\tmap = dm_get_live_table(md, &srcu_idx);\n\tif (!dm_suspended_md(md)) {\n\t\tdm_table_presuspend_targets(map);\n\t\tset_bit(DMF_SUSPENDED, &md->flags);\n\t\tset_bit(DMF_POST_SUSPENDING, &md->flags);\n\t\tdm_table_postsuspend_targets(map);\n\t}\n\t \n\tdm_put_live_table(md, srcu_idx);\n\tmutex_unlock(&md->suspend_lock);\n\n\t \n\tif (wait)\n\t\twhile (atomic_read(&md->holders))\n\t\t\tfsleep(1000);\n\telse if (atomic_read(&md->holders))\n\t\tDMWARN(\"%s: Forcibly removing mapped_device still in use! (%d users)\",\n\t\t       dm_device_name(md), atomic_read(&md->holders));\n\n\tdm_table_destroy(__unbind(md));\n\tfree_dev(md);\n}\n\nvoid dm_destroy(struct mapped_device *md)\n{\n\t__dm_destroy(md, true);\n}\n\nvoid dm_destroy_immediate(struct mapped_device *md)\n{\n\t__dm_destroy(md, false);\n}\n\nvoid dm_put(struct mapped_device *md)\n{\n\tatomic_dec(&md->holders);\n}\nEXPORT_SYMBOL_GPL(dm_put);\n\nstatic bool dm_in_flight_bios(struct mapped_device *md)\n{\n\tint cpu;\n\tunsigned long sum = 0;\n\n\tfor_each_possible_cpu(cpu)\n\t\tsum += *per_cpu_ptr(md->pending_io, cpu);\n\n\treturn sum != 0;\n}\n\nstatic int dm_wait_for_bios_completion(struct mapped_device *md, unsigned int task_state)\n{\n\tint r = 0;\n\tDEFINE_WAIT(wait);\n\n\twhile (true) {\n\t\tprepare_to_wait(&md->wait, &wait, task_state);\n\n\t\tif (!dm_in_flight_bios(md))\n\t\t\tbreak;\n\n\t\tif (signal_pending_state(task_state, current)) {\n\t\t\tr = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tio_schedule();\n\t}\n\tfinish_wait(&md->wait, &wait);\n\n\tsmp_rmb();\n\n\treturn r;\n}\n\nstatic int dm_wait_for_completion(struct mapped_device *md, unsigned int task_state)\n{\n\tint r = 0;\n\n\tif (!queue_is_mq(md->queue))\n\t\treturn dm_wait_for_bios_completion(md, task_state);\n\n\twhile (true) {\n\t\tif (!blk_mq_queue_inflight(md->queue))\n\t\t\tbreak;\n\n\t\tif (signal_pending_state(task_state, current)) {\n\t\t\tr = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tfsleep(5000);\n\t}\n\n\treturn r;\n}\n\n \nstatic void dm_wq_work(struct work_struct *work)\n{\n\tstruct mapped_device *md = container_of(work, struct mapped_device, work);\n\tstruct bio *bio;\n\n\twhile (!test_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags)) {\n\t\tspin_lock_irq(&md->deferred_lock);\n\t\tbio = bio_list_pop(&md->deferred);\n\t\tspin_unlock_irq(&md->deferred_lock);\n\n\t\tif (!bio)\n\t\t\tbreak;\n\n\t\tsubmit_bio_noacct(bio);\n\t\tcond_resched();\n\t}\n}\n\nstatic void dm_queue_flush(struct mapped_device *md)\n{\n\tclear_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags);\n\tsmp_mb__after_atomic();\n\tqueue_work(md->wq, &md->work);\n}\n\n \nstruct dm_table *dm_swap_table(struct mapped_device *md, struct dm_table *table)\n{\n\tstruct dm_table *live_map = NULL, *map = ERR_PTR(-EINVAL);\n\tstruct queue_limits limits;\n\tint r;\n\n\tmutex_lock(&md->suspend_lock);\n\n\t \n\tif (!dm_suspended_md(md))\n\t\tgoto out;\n\n\t \n\tif (dm_table_has_no_data_devices(table)) {\n\t\tlive_map = dm_get_live_table_fast(md);\n\t\tif (live_map)\n\t\t\tlimits = md->queue->limits;\n\t\tdm_put_live_table_fast(md);\n\t}\n\n\tif (!live_map) {\n\t\tr = dm_calculate_queue_limits(table, &limits);\n\t\tif (r) {\n\t\t\tmap = ERR_PTR(r);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tmap = __bind(md, table, &limits);\n\tdm_issue_global_event();\n\nout:\n\tmutex_unlock(&md->suspend_lock);\n\treturn map;\n}\n\n \nstatic int lock_fs(struct mapped_device *md)\n{\n\tint r;\n\n\tWARN_ON(test_bit(DMF_FROZEN, &md->flags));\n\n\tr = freeze_bdev(md->disk->part0);\n\tif (!r)\n\t\tset_bit(DMF_FROZEN, &md->flags);\n\treturn r;\n}\n\nstatic void unlock_fs(struct mapped_device *md)\n{\n\tif (!test_bit(DMF_FROZEN, &md->flags))\n\t\treturn;\n\tthaw_bdev(md->disk->part0);\n\tclear_bit(DMF_FROZEN, &md->flags);\n}\n\n \nstatic int __dm_suspend(struct mapped_device *md, struct dm_table *map,\n\t\t\tunsigned int suspend_flags, unsigned int task_state,\n\t\t\tint dmf_suspended_flag)\n{\n\tbool do_lockfs = suspend_flags & DM_SUSPEND_LOCKFS_FLAG;\n\tbool noflush = suspend_flags & DM_SUSPEND_NOFLUSH_FLAG;\n\tint r;\n\n\tlockdep_assert_held(&md->suspend_lock);\n\n\t \n\tif (noflush)\n\t\tset_bit(DMF_NOFLUSH_SUSPENDING, &md->flags);\n\telse\n\t\tDMDEBUG(\"%s: suspending with flush\", dm_device_name(md));\n\n\t \n\tdm_table_presuspend_targets(map);\n\n\t \n\tif (!noflush && do_lockfs) {\n\t\tr = lock_fs(md);\n\t\tif (r) {\n\t\t\tdm_table_presuspend_undo_targets(map);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\t \n\tset_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags);\n\tif (map)\n\t\tsynchronize_srcu(&md->io_barrier);\n\n\t \n\tif (dm_request_based(md))\n\t\tdm_stop_queue(md->queue);\n\n\tflush_workqueue(md->wq);\n\n\t \n\tr = dm_wait_for_completion(md, task_state);\n\tif (!r)\n\t\tset_bit(dmf_suspended_flag, &md->flags);\n\n\tif (noflush)\n\t\tclear_bit(DMF_NOFLUSH_SUSPENDING, &md->flags);\n\tif (map)\n\t\tsynchronize_srcu(&md->io_barrier);\n\n\t \n\tif (r < 0) {\n\t\tdm_queue_flush(md);\n\n\t\tif (dm_request_based(md))\n\t\t\tdm_start_queue(md->queue);\n\n\t\tunlock_fs(md);\n\t\tdm_table_presuspend_undo_targets(map);\n\t\t \n\t}\n\n\treturn r;\n}\n\n \n \nint dm_suspend(struct mapped_device *md, unsigned int suspend_flags)\n{\n\tstruct dm_table *map = NULL;\n\tint r = 0;\n\nretry:\n\tmutex_lock_nested(&md->suspend_lock, SINGLE_DEPTH_NESTING);\n\n\tif (dm_suspended_md(md)) {\n\t\tr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (dm_suspended_internally_md(md)) {\n\t\t \n\t\tmutex_unlock(&md->suspend_lock);\n\t\tr = wait_on_bit(&md->flags, DMF_SUSPENDED_INTERNALLY, TASK_INTERRUPTIBLE);\n\t\tif (r)\n\t\t\treturn r;\n\t\tgoto retry;\n\t}\n\n\tmap = rcu_dereference_protected(md->map, lockdep_is_held(&md->suspend_lock));\n\tif (!map) {\n\t\t \n\t\tsuspend_flags &= ~DM_SUSPEND_LOCKFS_FLAG;\n\t}\n\n\tr = __dm_suspend(md, map, suspend_flags, TASK_INTERRUPTIBLE, DMF_SUSPENDED);\n\tif (r)\n\t\tgoto out_unlock;\n\n\tset_bit(DMF_POST_SUSPENDING, &md->flags);\n\tdm_table_postsuspend_targets(map);\n\tclear_bit(DMF_POST_SUSPENDING, &md->flags);\n\nout_unlock:\n\tmutex_unlock(&md->suspend_lock);\n\treturn r;\n}\n\nstatic int __dm_resume(struct mapped_device *md, struct dm_table *map)\n{\n\tif (map) {\n\t\tint r = dm_table_resume_targets(map);\n\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tdm_queue_flush(md);\n\n\t \n\tif (dm_request_based(md))\n\t\tdm_start_queue(md->queue);\n\n\tunlock_fs(md);\n\n\treturn 0;\n}\n\nint dm_resume(struct mapped_device *md)\n{\n\tint r;\n\tstruct dm_table *map = NULL;\n\nretry:\n\tr = -EINVAL;\n\tmutex_lock_nested(&md->suspend_lock, SINGLE_DEPTH_NESTING);\n\n\tif (!dm_suspended_md(md))\n\t\tgoto out;\n\n\tif (dm_suspended_internally_md(md)) {\n\t\t \n\t\tmutex_unlock(&md->suspend_lock);\n\t\tr = wait_on_bit(&md->flags, DMF_SUSPENDED_INTERNALLY, TASK_INTERRUPTIBLE);\n\t\tif (r)\n\t\t\treturn r;\n\t\tgoto retry;\n\t}\n\n\tmap = rcu_dereference_protected(md->map, lockdep_is_held(&md->suspend_lock));\n\tif (!map || !dm_table_get_size(map))\n\t\tgoto out;\n\n\tr = __dm_resume(md, map);\n\tif (r)\n\t\tgoto out;\n\n\tclear_bit(DMF_SUSPENDED, &md->flags);\nout:\n\tmutex_unlock(&md->suspend_lock);\n\n\treturn r;\n}\n\n \n\nstatic void __dm_internal_suspend(struct mapped_device *md, unsigned int suspend_flags)\n{\n\tstruct dm_table *map = NULL;\n\n\tlockdep_assert_held(&md->suspend_lock);\n\n\tif (md->internal_suspend_count++)\n\t\treturn;  \n\n\tif (dm_suspended_md(md)) {\n\t\tset_bit(DMF_SUSPENDED_INTERNALLY, &md->flags);\n\t\treturn;  \n\t}\n\n\tmap = rcu_dereference_protected(md->map, lockdep_is_held(&md->suspend_lock));\n\n\t \n\t(void) __dm_suspend(md, map, suspend_flags, TASK_UNINTERRUPTIBLE,\n\t\t\t    DMF_SUSPENDED_INTERNALLY);\n\n\tset_bit(DMF_POST_SUSPENDING, &md->flags);\n\tdm_table_postsuspend_targets(map);\n\tclear_bit(DMF_POST_SUSPENDING, &md->flags);\n}\n\nstatic void __dm_internal_resume(struct mapped_device *md)\n{\n\tBUG_ON(!md->internal_suspend_count);\n\n\tif (--md->internal_suspend_count)\n\t\treturn;  \n\n\tif (dm_suspended_md(md))\n\t\tgoto done;  \n\n\t \n\t(void) __dm_resume(md, NULL);\n\ndone:\n\tclear_bit(DMF_SUSPENDED_INTERNALLY, &md->flags);\n\tsmp_mb__after_atomic();\n\twake_up_bit(&md->flags, DMF_SUSPENDED_INTERNALLY);\n}\n\nvoid dm_internal_suspend_noflush(struct mapped_device *md)\n{\n\tmutex_lock(&md->suspend_lock);\n\t__dm_internal_suspend(md, DM_SUSPEND_NOFLUSH_FLAG);\n\tmutex_unlock(&md->suspend_lock);\n}\nEXPORT_SYMBOL_GPL(dm_internal_suspend_noflush);\n\nvoid dm_internal_resume(struct mapped_device *md)\n{\n\tmutex_lock(&md->suspend_lock);\n\t__dm_internal_resume(md);\n\tmutex_unlock(&md->suspend_lock);\n}\nEXPORT_SYMBOL_GPL(dm_internal_resume);\n\n \n\nvoid dm_internal_suspend_fast(struct mapped_device *md)\n{\n\tmutex_lock(&md->suspend_lock);\n\tif (dm_suspended_md(md) || dm_suspended_internally_md(md))\n\t\treturn;\n\n\tset_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags);\n\tsynchronize_srcu(&md->io_barrier);\n\tflush_workqueue(md->wq);\n\tdm_wait_for_completion(md, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL_GPL(dm_internal_suspend_fast);\n\nvoid dm_internal_resume_fast(struct mapped_device *md)\n{\n\tif (dm_suspended_md(md) || dm_suspended_internally_md(md))\n\t\tgoto done;\n\n\tdm_queue_flush(md);\n\ndone:\n\tmutex_unlock(&md->suspend_lock);\n}\nEXPORT_SYMBOL_GPL(dm_internal_resume_fast);\n\n \nint dm_kobject_uevent(struct mapped_device *md, enum kobject_action action,\n\t\t      unsigned int cookie, bool need_resize_uevent)\n{\n\tint r;\n\tunsigned int noio_flag;\n\tchar udev_cookie[DM_COOKIE_LENGTH];\n\tchar *envp[3] = { NULL, NULL, NULL };\n\tchar **envpp = envp;\n\tif (cookie) {\n\t\tsnprintf(udev_cookie, DM_COOKIE_LENGTH, \"%s=%u\",\n\t\t\t DM_COOKIE_ENV_VAR_NAME, cookie);\n\t\t*envpp++ = udev_cookie;\n\t}\n\tif (need_resize_uevent) {\n\t\t*envpp++ = \"RESIZE=1\";\n\t}\n\n\tnoio_flag = memalloc_noio_save();\n\n\tr = kobject_uevent_env(&disk_to_dev(md->disk)->kobj, action, envp);\n\n\tmemalloc_noio_restore(noio_flag);\n\n\treturn r;\n}\n\nuint32_t dm_next_uevent_seq(struct mapped_device *md)\n{\n\treturn atomic_add_return(1, &md->uevent_seq);\n}\n\nuint32_t dm_get_event_nr(struct mapped_device *md)\n{\n\treturn atomic_read(&md->event_nr);\n}\n\nint dm_wait_event(struct mapped_device *md, int event_nr)\n{\n\treturn wait_event_interruptible(md->eventq,\n\t\t\t(event_nr != atomic_read(&md->event_nr)));\n}\n\nvoid dm_uevent_add(struct mapped_device *md, struct list_head *elist)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&md->uevent_lock, flags);\n\tlist_add(elist, &md->uevent_list);\n\tspin_unlock_irqrestore(&md->uevent_lock, flags);\n}\n\n \nstruct gendisk *dm_disk(struct mapped_device *md)\n{\n\treturn md->disk;\n}\nEXPORT_SYMBOL_GPL(dm_disk);\n\nstruct kobject *dm_kobject(struct mapped_device *md)\n{\n\treturn &md->kobj_holder.kobj;\n}\n\nstruct mapped_device *dm_get_from_kobject(struct kobject *kobj)\n{\n\tstruct mapped_device *md;\n\n\tmd = container_of(kobj, struct mapped_device, kobj_holder.kobj);\n\n\tspin_lock(&_minor_lock);\n\tif (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {\n\t\tmd = NULL;\n\t\tgoto out;\n\t}\n\tdm_get(md);\nout:\n\tspin_unlock(&_minor_lock);\n\n\treturn md;\n}\n\nint dm_suspended_md(struct mapped_device *md)\n{\n\treturn test_bit(DMF_SUSPENDED, &md->flags);\n}\n\nstatic int dm_post_suspending_md(struct mapped_device *md)\n{\n\treturn test_bit(DMF_POST_SUSPENDING, &md->flags);\n}\n\nint dm_suspended_internally_md(struct mapped_device *md)\n{\n\treturn test_bit(DMF_SUSPENDED_INTERNALLY, &md->flags);\n}\n\nint dm_test_deferred_remove_flag(struct mapped_device *md)\n{\n\treturn test_bit(DMF_DEFERRED_REMOVE, &md->flags);\n}\n\nint dm_suspended(struct dm_target *ti)\n{\n\treturn dm_suspended_md(ti->table->md);\n}\nEXPORT_SYMBOL_GPL(dm_suspended);\n\nint dm_post_suspending(struct dm_target *ti)\n{\n\treturn dm_post_suspending_md(ti->table->md);\n}\nEXPORT_SYMBOL_GPL(dm_post_suspending);\n\nint dm_noflush_suspending(struct dm_target *ti)\n{\n\treturn __noflush_suspending(ti->table->md);\n}\nEXPORT_SYMBOL_GPL(dm_noflush_suspending);\n\nvoid dm_free_md_mempools(struct dm_md_mempools *pools)\n{\n\tif (!pools)\n\t\treturn;\n\n\tbioset_exit(&pools->bs);\n\tbioset_exit(&pools->io_bs);\n\n\tkfree(pools);\n}\n\nstruct dm_pr {\n\tu64\told_key;\n\tu64\tnew_key;\n\tu32\tflags;\n\tbool\tabort;\n\tbool\tfail_early;\n\tint\tret;\n\tenum pr_type type;\n\tstruct pr_keys *read_keys;\n\tstruct pr_held_reservation *rsv;\n};\n\nstatic int dm_call_pr(struct block_device *bdev, iterate_devices_callout_fn fn,\n\t\t      struct dm_pr *pr)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\tstruct dm_table *table;\n\tstruct dm_target *ti;\n\tint ret = -ENOTTY, srcu_idx;\n\n\ttable = dm_get_live_table(md, &srcu_idx);\n\tif (!table || !dm_table_get_size(table))\n\t\tgoto out;\n\n\t \n\tif (table->num_targets != 1)\n\t\tgoto out;\n\tti = dm_table_get_target(table, 0);\n\n\tif (dm_suspended_md(md)) {\n\t\tret = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tret = -EINVAL;\n\tif (!ti->type->iterate_devices)\n\t\tgoto out;\n\n\tti->type->iterate_devices(ti, fn, pr);\n\tret = 0;\nout:\n\tdm_put_live_table(md, srcu_idx);\n\treturn ret;\n}\n\n \nstatic int __dm_pr_register(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t    sector_t start, sector_t len, void *data)\n{\n\tstruct dm_pr *pr = data;\n\tconst struct pr_ops *ops = dev->bdev->bd_disk->fops->pr_ops;\n\tint ret;\n\n\tif (!ops || !ops->pr_register) {\n\t\tpr->ret = -EOPNOTSUPP;\n\t\treturn -1;\n\t}\n\n\tret = ops->pr_register(dev->bdev, pr->old_key, pr->new_key, pr->flags);\n\tif (!ret)\n\t\treturn 0;\n\n\tif (!pr->ret)\n\t\tpr->ret = ret;\n\n\tif (pr->fail_early)\n\t\treturn -1;\n\n\treturn 0;\n}\n\nstatic int dm_pr_register(struct block_device *bdev, u64 old_key, u64 new_key,\n\t\t\t  u32 flags)\n{\n\tstruct dm_pr pr = {\n\t\t.old_key\t= old_key,\n\t\t.new_key\t= new_key,\n\t\t.flags\t\t= flags,\n\t\t.fail_early\t= true,\n\t\t.ret\t\t= 0,\n\t};\n\tint ret;\n\n\tret = dm_call_pr(bdev, __dm_pr_register, &pr);\n\tif (ret) {\n\t\t \n\t\treturn ret;\n\t}\n\n\tif (!pr.ret)\n\t\treturn 0;\n\tret = pr.ret;\n\n\tif (!new_key)\n\t\treturn ret;\n\n\t \n\tpr.old_key = new_key;\n\tpr.new_key = 0;\n\tpr.flags = 0;\n\tpr.fail_early = false;\n\t(void) dm_call_pr(bdev, __dm_pr_register, &pr);\n\treturn ret;\n}\n\n\nstatic int __dm_pr_reserve(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t   sector_t start, sector_t len, void *data)\n{\n\tstruct dm_pr *pr = data;\n\tconst struct pr_ops *ops = dev->bdev->bd_disk->fops->pr_ops;\n\n\tif (!ops || !ops->pr_reserve) {\n\t\tpr->ret = -EOPNOTSUPP;\n\t\treturn -1;\n\t}\n\n\tpr->ret = ops->pr_reserve(dev->bdev, pr->old_key, pr->type, pr->flags);\n\tif (!pr->ret)\n\t\treturn -1;\n\n\treturn 0;\n}\n\nstatic int dm_pr_reserve(struct block_device *bdev, u64 key, enum pr_type type,\n\t\t\t u32 flags)\n{\n\tstruct dm_pr pr = {\n\t\t.old_key\t= key,\n\t\t.flags\t\t= flags,\n\t\t.type\t\t= type,\n\t\t.fail_early\t= false,\n\t\t.ret\t\t= 0,\n\t};\n\tint ret;\n\n\tret = dm_call_pr(bdev, __dm_pr_reserve, &pr);\n\tif (ret)\n\t\treturn ret;\n\n\treturn pr.ret;\n}\n\n \nstatic int __dm_pr_release(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t   sector_t start, sector_t len, void *data)\n{\n\tstruct dm_pr *pr = data;\n\tconst struct pr_ops *ops = dev->bdev->bd_disk->fops->pr_ops;\n\n\tif (!ops || !ops->pr_release) {\n\t\tpr->ret = -EOPNOTSUPP;\n\t\treturn -1;\n\t}\n\n\tpr->ret = ops->pr_release(dev->bdev, pr->old_key, pr->type);\n\tif (pr->ret)\n\t\treturn -1;\n\n\treturn 0;\n}\n\nstatic int dm_pr_release(struct block_device *bdev, u64 key, enum pr_type type)\n{\n\tstruct dm_pr pr = {\n\t\t.old_key\t= key,\n\t\t.type\t\t= type,\n\t\t.fail_early\t= false,\n\t};\n\tint ret;\n\n\tret = dm_call_pr(bdev, __dm_pr_release, &pr);\n\tif (ret)\n\t\treturn ret;\n\n\treturn pr.ret;\n}\n\nstatic int __dm_pr_preempt(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t   sector_t start, sector_t len, void *data)\n{\n\tstruct dm_pr *pr = data;\n\tconst struct pr_ops *ops = dev->bdev->bd_disk->fops->pr_ops;\n\n\tif (!ops || !ops->pr_preempt) {\n\t\tpr->ret = -EOPNOTSUPP;\n\t\treturn -1;\n\t}\n\n\tpr->ret = ops->pr_preempt(dev->bdev, pr->old_key, pr->new_key, pr->type,\n\t\t\t\t  pr->abort);\n\tif (!pr->ret)\n\t\treturn -1;\n\n\treturn 0;\n}\n\nstatic int dm_pr_preempt(struct block_device *bdev, u64 old_key, u64 new_key,\n\t\t\t enum pr_type type, bool abort)\n{\n\tstruct dm_pr pr = {\n\t\t.new_key\t= new_key,\n\t\t.old_key\t= old_key,\n\t\t.type\t\t= type,\n\t\t.fail_early\t= false,\n\t};\n\tint ret;\n\n\tret = dm_call_pr(bdev, __dm_pr_preempt, &pr);\n\tif (ret)\n\t\treturn ret;\n\n\treturn pr.ret;\n}\n\nstatic int dm_pr_clear(struct block_device *bdev, u64 key)\n{\n\tstruct mapped_device *md = bdev->bd_disk->private_data;\n\tconst struct pr_ops *ops;\n\tint r, srcu_idx;\n\n\tr = dm_prepare_ioctl(md, &srcu_idx, &bdev);\n\tif (r < 0)\n\t\tgoto out;\n\n\tops = bdev->bd_disk->fops->pr_ops;\n\tif (ops && ops->pr_clear)\n\t\tr = ops->pr_clear(bdev, key);\n\telse\n\t\tr = -EOPNOTSUPP;\nout:\n\tdm_unprepare_ioctl(md, srcu_idx);\n\treturn r;\n}\n\nstatic int __dm_pr_read_keys(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t     sector_t start, sector_t len, void *data)\n{\n\tstruct dm_pr *pr = data;\n\tconst struct pr_ops *ops = dev->bdev->bd_disk->fops->pr_ops;\n\n\tif (!ops || !ops->pr_read_keys) {\n\t\tpr->ret = -EOPNOTSUPP;\n\t\treturn -1;\n\t}\n\n\tpr->ret = ops->pr_read_keys(dev->bdev, pr->read_keys);\n\tif (!pr->ret)\n\t\treturn -1;\n\n\treturn 0;\n}\n\nstatic int dm_pr_read_keys(struct block_device *bdev, struct pr_keys *keys)\n{\n\tstruct dm_pr pr = {\n\t\t.read_keys = keys,\n\t};\n\tint ret;\n\n\tret = dm_call_pr(bdev, __dm_pr_read_keys, &pr);\n\tif (ret)\n\t\treturn ret;\n\n\treturn pr.ret;\n}\n\nstatic int __dm_pr_read_reservation(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t\t    sector_t start, sector_t len, void *data)\n{\n\tstruct dm_pr *pr = data;\n\tconst struct pr_ops *ops = dev->bdev->bd_disk->fops->pr_ops;\n\n\tif (!ops || !ops->pr_read_reservation) {\n\t\tpr->ret = -EOPNOTSUPP;\n\t\treturn -1;\n\t}\n\n\tpr->ret = ops->pr_read_reservation(dev->bdev, pr->rsv);\n\tif (!pr->ret)\n\t\treturn -1;\n\n\treturn 0;\n}\n\nstatic int dm_pr_read_reservation(struct block_device *bdev,\n\t\t\t\t  struct pr_held_reservation *rsv)\n{\n\tstruct dm_pr pr = {\n\t\t.rsv = rsv,\n\t};\n\tint ret;\n\n\tret = dm_call_pr(bdev, __dm_pr_read_reservation, &pr);\n\tif (ret)\n\t\treturn ret;\n\n\treturn pr.ret;\n}\n\nstatic const struct pr_ops dm_pr_ops = {\n\t.pr_register\t= dm_pr_register,\n\t.pr_reserve\t= dm_pr_reserve,\n\t.pr_release\t= dm_pr_release,\n\t.pr_preempt\t= dm_pr_preempt,\n\t.pr_clear\t= dm_pr_clear,\n\t.pr_read_keys\t= dm_pr_read_keys,\n\t.pr_read_reservation = dm_pr_read_reservation,\n};\n\nstatic const struct block_device_operations dm_blk_dops = {\n\t.submit_bio = dm_submit_bio,\n\t.poll_bio = dm_poll_bio,\n\t.open = dm_blk_open,\n\t.release = dm_blk_close,\n\t.ioctl = dm_blk_ioctl,\n\t.getgeo = dm_blk_getgeo,\n\t.report_zones = dm_blk_report_zones,\n\t.pr_ops = &dm_pr_ops,\n\t.owner = THIS_MODULE\n};\n\nstatic const struct block_device_operations dm_rq_blk_dops = {\n\t.open = dm_blk_open,\n\t.release = dm_blk_close,\n\t.ioctl = dm_blk_ioctl,\n\t.getgeo = dm_blk_getgeo,\n\t.pr_ops = &dm_pr_ops,\n\t.owner = THIS_MODULE\n};\n\nstatic const struct dax_operations dm_dax_ops = {\n\t.direct_access = dm_dax_direct_access,\n\t.zero_page_range = dm_dax_zero_page_range,\n\t.recovery_write = dm_dax_recovery_write,\n};\n\n \nmodule_init(dm_init);\nmodule_exit(dm_exit);\n\nmodule_param(major, uint, 0);\nMODULE_PARM_DESC(major, \"The major number of the device mapper\");\n\nmodule_param(reserved_bio_based_ios, uint, 0644);\nMODULE_PARM_DESC(reserved_bio_based_ios, \"Reserved IOs in bio-based mempools\");\n\nmodule_param(dm_numa_node, int, 0644);\nMODULE_PARM_DESC(dm_numa_node, \"NUMA node for DM device memory allocations\");\n\nmodule_param(swap_bios, int, 0644);\nMODULE_PARM_DESC(swap_bios, \"Maximum allowed inflight swap IOs\");\n\nMODULE_DESCRIPTION(DM_NAME \" driver\");\nMODULE_AUTHOR(\"Joe Thornber <dm-devel@redhat.com>\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}