{
  "module_name": "dm-raid1.c",
  "hash_id": "f198f93645a01da3c41d0147047be0d13e8b3e6a461753b8fab3e76a7ed06379",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-raid1.c",
  "human_readable_source": "\n \n\n#include \"dm-bio-record.h\"\n\n#include <linux/init.h>\n#include <linux/mempool.h>\n#include <linux/module.h>\n#include <linux/pagemap.h>\n#include <linux/slab.h>\n#include <linux/workqueue.h>\n#include <linux/device-mapper.h>\n#include <linux/dm-io.h>\n#include <linux/dm-dirty-log.h>\n#include <linux/dm-kcopyd.h>\n#include <linux/dm-region-hash.h>\n\nstatic struct workqueue_struct *dm_raid1_wq;\n\n#define DM_MSG_PREFIX \"raid1\"\n\n#define MAX_RECOVERY 1\t \n\n#define MAX_NR_MIRRORS\t(DM_KCOPYD_MAX_REGIONS + 1)\n\n#define DM_RAID1_HANDLE_ERRORS\t0x01\n#define DM_RAID1_KEEP_LOG\t0x02\n#define errors_handled(p)\t((p)->features & DM_RAID1_HANDLE_ERRORS)\n#define keep_log(p)\t\t((p)->features & DM_RAID1_KEEP_LOG)\n\nstatic DECLARE_WAIT_QUEUE_HEAD(_kmirrord_recovery_stopped);\n\n \nenum dm_raid1_error {\n\tDM_RAID1_WRITE_ERROR,\n\tDM_RAID1_FLUSH_ERROR,\n\tDM_RAID1_SYNC_ERROR,\n\tDM_RAID1_READ_ERROR\n};\n\nstruct mirror {\n\tstruct mirror_set *ms;\n\tatomic_t error_count;\n\tunsigned long error_type;\n\tstruct dm_dev *dev;\n\tsector_t offset;\n};\n\nstruct mirror_set {\n\tstruct dm_target *ti;\n\tstruct list_head list;\n\n\tuint64_t features;\n\n\tspinlock_t lock;\t \n\tstruct bio_list reads;\n\tstruct bio_list writes;\n\tstruct bio_list failures;\n\tstruct bio_list holds;\t \n\n\tstruct dm_region_hash *rh;\n\tstruct dm_kcopyd_client *kcopyd_client;\n\tstruct dm_io_client *io_client;\n\n\t \n\tregion_t nr_regions;\n\tint in_sync;\n\tint log_failure;\n\tint leg_failure;\n\tatomic_t suspend;\n\n\tatomic_t default_mirror;\t \n\n\tstruct workqueue_struct *kmirrord_wq;\n\tstruct work_struct kmirrord_work;\n\tstruct timer_list timer;\n\tunsigned long timer_pending;\n\n\tstruct work_struct trigger_event;\n\n\tunsigned int nr_mirrors;\n\tstruct mirror mirror[];\n};\n\nDECLARE_DM_KCOPYD_THROTTLE_WITH_MODULE_PARM(raid1_resync_throttle,\n\t\t\"A percentage of time allocated for raid resynchronization\");\n\nstatic void wakeup_mirrord(void *context)\n{\n\tstruct mirror_set *ms = context;\n\n\tqueue_work(ms->kmirrord_wq, &ms->kmirrord_work);\n}\n\nstatic void delayed_wake_fn(struct timer_list *t)\n{\n\tstruct mirror_set *ms = from_timer(ms, t, timer);\n\n\tclear_bit(0, &ms->timer_pending);\n\twakeup_mirrord(ms);\n}\n\nstatic void delayed_wake(struct mirror_set *ms)\n{\n\tif (test_and_set_bit(0, &ms->timer_pending))\n\t\treturn;\n\n\tms->timer.expires = jiffies + HZ / 5;\n\tadd_timer(&ms->timer);\n}\n\nstatic void wakeup_all_recovery_waiters(void *context)\n{\n\twake_up_all(&_kmirrord_recovery_stopped);\n}\n\nstatic void queue_bio(struct mirror_set *ms, struct bio *bio, int rw)\n{\n\tunsigned long flags;\n\tint should_wake = 0;\n\tstruct bio_list *bl;\n\n\tbl = (rw == WRITE) ? &ms->writes : &ms->reads;\n\tspin_lock_irqsave(&ms->lock, flags);\n\tshould_wake = !(bl->head);\n\tbio_list_add(bl, bio);\n\tspin_unlock_irqrestore(&ms->lock, flags);\n\n\tif (should_wake)\n\t\twakeup_mirrord(ms);\n}\n\nstatic void dispatch_bios(void *context, struct bio_list *bio_list)\n{\n\tstruct mirror_set *ms = context;\n\tstruct bio *bio;\n\n\twhile ((bio = bio_list_pop(bio_list)))\n\t\tqueue_bio(ms, bio, WRITE);\n}\n\nstruct dm_raid1_bio_record {\n\tstruct mirror *m;\n\t \n\tstruct dm_bio_details details;\n\tregion_t write_region;\n};\n\n \n#define DEFAULT_MIRROR 0\n\n \nstatic struct mirror *bio_get_m(struct bio *bio)\n{\n\treturn (struct mirror *) bio->bi_next;\n}\n\nstatic void bio_set_m(struct bio *bio, struct mirror *m)\n{\n\tbio->bi_next = (struct bio *) m;\n}\n\nstatic struct mirror *get_default_mirror(struct mirror_set *ms)\n{\n\treturn &ms->mirror[atomic_read(&ms->default_mirror)];\n}\n\nstatic void set_default_mirror(struct mirror *m)\n{\n\tstruct mirror_set *ms = m->ms;\n\tstruct mirror *m0 = &(ms->mirror[0]);\n\n\tatomic_set(&ms->default_mirror, m - m0);\n}\n\nstatic struct mirror *get_valid_mirror(struct mirror_set *ms)\n{\n\tstruct mirror *m;\n\n\tfor (m = ms->mirror; m < ms->mirror + ms->nr_mirrors; m++)\n\t\tif (!atomic_read(&m->error_count))\n\t\t\treturn m;\n\n\treturn NULL;\n}\n\n \nstatic void fail_mirror(struct mirror *m, enum dm_raid1_error error_type)\n{\n\tstruct mirror_set *ms = m->ms;\n\tstruct mirror *new;\n\n\tms->leg_failure = 1;\n\n\t \n\tatomic_inc(&m->error_count);\n\n\tif (test_and_set_bit(error_type, &m->error_type))\n\t\treturn;\n\n\tif (!errors_handled(ms))\n\t\treturn;\n\n\tif (m != get_default_mirror(ms))\n\t\tgoto out;\n\n\tif (!ms->in_sync && !keep_log(ms)) {\n\t\t \n\t\tDMERR(\"Primary mirror (%s) failed while out-of-sync: Reads may fail.\",\n\t\t      m->dev->name);\n\t\tgoto out;\n\t}\n\n\tnew = get_valid_mirror(ms);\n\tif (new)\n\t\tset_default_mirror(new);\n\telse\n\t\tDMWARN(\"All sides of mirror have failed.\");\n\nout:\n\tqueue_work(dm_raid1_wq, &ms->trigger_event);\n}\n\nstatic int mirror_flush(struct dm_target *ti)\n{\n\tstruct mirror_set *ms = ti->private;\n\tunsigned long error_bits;\n\n\tunsigned int i;\n\tstruct dm_io_region io[MAX_NR_MIRRORS];\n\tstruct mirror *m;\n\tstruct dm_io_request io_req = {\n\t\t.bi_opf = REQ_OP_WRITE | REQ_PREFLUSH | REQ_SYNC,\n\t\t.mem.type = DM_IO_KMEM,\n\t\t.mem.ptr.addr = NULL,\n\t\t.client = ms->io_client,\n\t};\n\n\tfor (i = 0, m = ms->mirror; i < ms->nr_mirrors; i++, m++) {\n\t\tio[i].bdev = m->dev->bdev;\n\t\tio[i].sector = 0;\n\t\tio[i].count = 0;\n\t}\n\n\terror_bits = -1;\n\tdm_io(&io_req, ms->nr_mirrors, io, &error_bits);\n\tif (unlikely(error_bits != 0)) {\n\t\tfor (i = 0; i < ms->nr_mirrors; i++)\n\t\t\tif (test_bit(i, &error_bits))\n\t\t\t\tfail_mirror(ms->mirror + i,\n\t\t\t\t\t    DM_RAID1_FLUSH_ERROR);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void recovery_complete(int read_err, unsigned long write_err,\n\t\t\t      void *context)\n{\n\tstruct dm_region *reg = context;\n\tstruct mirror_set *ms = dm_rh_region_context(reg);\n\tint m, bit = 0;\n\n\tif (read_err) {\n\t\t \n\t\tDMERR_LIMIT(\"Unable to read primary mirror during recovery\");\n\t\tfail_mirror(get_default_mirror(ms), DM_RAID1_SYNC_ERROR);\n\t}\n\n\tif (write_err) {\n\t\tDMERR_LIMIT(\"Write error during recovery (error = 0x%lx)\",\n\t\t\t    write_err);\n\t\t \n\t\tfor (m = 0; m < ms->nr_mirrors; m++) {\n\t\t\tif (&ms->mirror[m] == get_default_mirror(ms))\n\t\t\t\tcontinue;\n\t\t\tif (test_bit(bit, &write_err))\n\t\t\t\tfail_mirror(ms->mirror + m,\n\t\t\t\t\t    DM_RAID1_SYNC_ERROR);\n\t\t\tbit++;\n\t\t}\n\t}\n\n\tdm_rh_recovery_end(reg, !(read_err || write_err));\n}\n\nstatic void recover(struct mirror_set *ms, struct dm_region *reg)\n{\n\tunsigned int i;\n\tstruct dm_io_region from, to[DM_KCOPYD_MAX_REGIONS], *dest;\n\tstruct mirror *m;\n\tunsigned long flags = 0;\n\tregion_t key = dm_rh_get_region_key(reg);\n\tsector_t region_size = dm_rh_get_region_size(ms->rh);\n\n\t \n\tm = get_default_mirror(ms);\n\tfrom.bdev = m->dev->bdev;\n\tfrom.sector = m->offset + dm_rh_region_to_sector(ms->rh, key);\n\tif (key == (ms->nr_regions - 1)) {\n\t\t \n\t\tfrom.count = ms->ti->len & (region_size - 1);\n\t\tif (!from.count)\n\t\t\tfrom.count = region_size;\n\t} else\n\t\tfrom.count = region_size;\n\n\t \n\tfor (i = 0, dest = to; i < ms->nr_mirrors; i++) {\n\t\tif (&ms->mirror[i] == get_default_mirror(ms))\n\t\t\tcontinue;\n\n\t\tm = ms->mirror + i;\n\t\tdest->bdev = m->dev->bdev;\n\t\tdest->sector = m->offset + dm_rh_region_to_sector(ms->rh, key);\n\t\tdest->count = from.count;\n\t\tdest++;\n\t}\n\n\t \n\tif (!errors_handled(ms))\n\t\tflags |= BIT(DM_KCOPYD_IGNORE_ERROR);\n\n\tdm_kcopyd_copy(ms->kcopyd_client, &from, ms->nr_mirrors - 1, to,\n\t\t       flags, recovery_complete, reg);\n}\n\nstatic void reset_ms_flags(struct mirror_set *ms)\n{\n\tunsigned int m;\n\n\tms->leg_failure = 0;\n\tfor (m = 0; m < ms->nr_mirrors; m++) {\n\t\tatomic_set(&(ms->mirror[m].error_count), 0);\n\t\tms->mirror[m].error_type = 0;\n\t}\n}\n\nstatic void do_recovery(struct mirror_set *ms)\n{\n\tstruct dm_region *reg;\n\tstruct dm_dirty_log *log = dm_rh_dirty_log(ms->rh);\n\n\t \n\tdm_rh_recovery_prepare(ms->rh);\n\n\t \n\twhile ((reg = dm_rh_recovery_start(ms->rh)))\n\t\trecover(ms, reg);\n\n\t \n\tif (!ms->in_sync &&\n\t    (log->type->get_sync_count(log) == ms->nr_regions)) {\n\t\t \n\t\tdm_table_event(ms->ti->table);\n\t\tms->in_sync = 1;\n\t\treset_ms_flags(ms);\n\t}\n}\n\n \nstatic struct mirror *choose_mirror(struct mirror_set *ms, sector_t sector)\n{\n\tstruct mirror *m = get_default_mirror(ms);\n\n\tdo {\n\t\tif (likely(!atomic_read(&m->error_count)))\n\t\t\treturn m;\n\n\t\tif (m-- == ms->mirror)\n\t\t\tm += ms->nr_mirrors;\n\t} while (m != get_default_mirror(ms));\n\n\treturn NULL;\n}\n\nstatic int default_ok(struct mirror *m)\n{\n\tstruct mirror *default_mirror = get_default_mirror(m->ms);\n\n\treturn !atomic_read(&default_mirror->error_count);\n}\n\nstatic int mirror_available(struct mirror_set *ms, struct bio *bio)\n{\n\tstruct dm_dirty_log *log = dm_rh_dirty_log(ms->rh);\n\tregion_t region = dm_rh_bio_to_region(ms->rh, bio);\n\n\tif (log->type->in_sync(log, region, 0))\n\t\treturn choose_mirror(ms,  bio->bi_iter.bi_sector) ? 1 : 0;\n\n\treturn 0;\n}\n\n \nstatic sector_t map_sector(struct mirror *m, struct bio *bio)\n{\n\tif (unlikely(!bio->bi_iter.bi_size))\n\t\treturn 0;\n\treturn m->offset + dm_target_offset(m->ms->ti, bio->bi_iter.bi_sector);\n}\n\nstatic void map_bio(struct mirror *m, struct bio *bio)\n{\n\tbio_set_dev(bio, m->dev->bdev);\n\tbio->bi_iter.bi_sector = map_sector(m, bio);\n}\n\nstatic void map_region(struct dm_io_region *io, struct mirror *m,\n\t\t       struct bio *bio)\n{\n\tio->bdev = m->dev->bdev;\n\tio->sector = map_sector(m, bio);\n\tio->count = bio_sectors(bio);\n}\n\nstatic void hold_bio(struct mirror_set *ms, struct bio *bio)\n{\n\t \n\tspin_lock_irq(&ms->lock);\n\n\tif (atomic_read(&ms->suspend)) {\n\t\tspin_unlock_irq(&ms->lock);\n\n\t\t \n\t\tif (dm_noflush_suspending(ms->ti))\n\t\t\tbio->bi_status = BLK_STS_DM_REQUEUE;\n\t\telse\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\n\t\tbio_endio(bio);\n\t\treturn;\n\t}\n\n\t \n\tbio_list_add(&ms->holds, bio);\n\tspin_unlock_irq(&ms->lock);\n}\n\n \nstatic void read_callback(unsigned long error, void *context)\n{\n\tstruct bio *bio = context;\n\tstruct mirror *m;\n\n\tm = bio_get_m(bio);\n\tbio_set_m(bio, NULL);\n\n\tif (likely(!error)) {\n\t\tbio_endio(bio);\n\t\treturn;\n\t}\n\n\tfail_mirror(m, DM_RAID1_READ_ERROR);\n\n\tif (likely(default_ok(m)) || mirror_available(m->ms, bio)) {\n\t\tDMWARN_LIMIT(\"Read failure on mirror device %s. Trying alternative device.\",\n\t\t\t     m->dev->name);\n\t\tqueue_bio(m->ms, bio, bio_data_dir(bio));\n\t\treturn;\n\t}\n\n\tDMERR_LIMIT(\"Read failure on mirror device %s.  Failing I/O.\",\n\t\t    m->dev->name);\n\tbio_io_error(bio);\n}\n\n \nstatic void read_async_bio(struct mirror *m, struct bio *bio)\n{\n\tstruct dm_io_region io;\n\tstruct dm_io_request io_req = {\n\t\t.bi_opf = REQ_OP_READ,\n\t\t.mem.type = DM_IO_BIO,\n\t\t.mem.ptr.bio = bio,\n\t\t.notify.fn = read_callback,\n\t\t.notify.context = bio,\n\t\t.client = m->ms->io_client,\n\t};\n\n\tmap_region(&io, m, bio);\n\tbio_set_m(bio, m);\n\tBUG_ON(dm_io(&io_req, 1, &io, NULL));\n}\n\nstatic inline int region_in_sync(struct mirror_set *ms, region_t region,\n\t\t\t\t int may_block)\n{\n\tint state = dm_rh_get_state(ms->rh, region, may_block);\n\treturn state == DM_RH_CLEAN || state == DM_RH_DIRTY;\n}\n\nstatic void do_reads(struct mirror_set *ms, struct bio_list *reads)\n{\n\tregion_t region;\n\tstruct bio *bio;\n\tstruct mirror *m;\n\n\twhile ((bio = bio_list_pop(reads))) {\n\t\tregion = dm_rh_bio_to_region(ms->rh, bio);\n\t\tm = get_default_mirror(ms);\n\n\t\t \n\t\tif (likely(region_in_sync(ms, region, 1)))\n\t\t\tm = choose_mirror(ms, bio->bi_iter.bi_sector);\n\t\telse if (m && atomic_read(&m->error_count))\n\t\t\tm = NULL;\n\n\t\tif (likely(m))\n\t\t\tread_async_bio(m, bio);\n\t\telse\n\t\t\tbio_io_error(bio);\n\t}\n}\n\n \nstatic void write_callback(unsigned long error, void *context)\n{\n\tunsigned int i;\n\tstruct bio *bio = context;\n\tstruct mirror_set *ms;\n\tint should_wake = 0;\n\tunsigned long flags;\n\n\tms = bio_get_m(bio)->ms;\n\tbio_set_m(bio, NULL);\n\n\t \n\tif (likely(!error)) {\n\t\tbio_endio(bio);\n\t\treturn;\n\t}\n\n\t \n\tif (bio_op(bio) == REQ_OP_DISCARD) {\n\t\tbio->bi_status = BLK_STS_NOTSUPP;\n\t\tbio_endio(bio);\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < ms->nr_mirrors; i++)\n\t\tif (test_bit(i, &error))\n\t\t\tfail_mirror(ms->mirror + i, DM_RAID1_WRITE_ERROR);\n\n\t \n\tspin_lock_irqsave(&ms->lock, flags);\n\tif (!ms->failures.head)\n\t\tshould_wake = 1;\n\tbio_list_add(&ms->failures, bio);\n\tspin_unlock_irqrestore(&ms->lock, flags);\n\tif (should_wake)\n\t\twakeup_mirrord(ms);\n}\n\nstatic void do_write(struct mirror_set *ms, struct bio *bio)\n{\n\tunsigned int i;\n\tstruct dm_io_region io[MAX_NR_MIRRORS], *dest = io;\n\tstruct mirror *m;\n\tblk_opf_t op_flags = bio->bi_opf & (REQ_FUA | REQ_PREFLUSH);\n\tstruct dm_io_request io_req = {\n\t\t.bi_opf = REQ_OP_WRITE | op_flags,\n\t\t.mem.type = DM_IO_BIO,\n\t\t.mem.ptr.bio = bio,\n\t\t.notify.fn = write_callback,\n\t\t.notify.context = bio,\n\t\t.client = ms->io_client,\n\t};\n\n\tif (bio_op(bio) == REQ_OP_DISCARD) {\n\t\tio_req.bi_opf = REQ_OP_DISCARD | op_flags;\n\t\tio_req.mem.type = DM_IO_KMEM;\n\t\tio_req.mem.ptr.addr = NULL;\n\t}\n\n\tfor (i = 0, m = ms->mirror; i < ms->nr_mirrors; i++, m++)\n\t\tmap_region(dest++, m, bio);\n\n\t \n\tbio_set_m(bio, get_default_mirror(ms));\n\n\tBUG_ON(dm_io(&io_req, ms->nr_mirrors, io, NULL));\n}\n\nstatic void do_writes(struct mirror_set *ms, struct bio_list *writes)\n{\n\tint state;\n\tstruct bio *bio;\n\tstruct bio_list sync, nosync, recover, *this_list = NULL;\n\tstruct bio_list requeue;\n\tstruct dm_dirty_log *log = dm_rh_dirty_log(ms->rh);\n\tregion_t region;\n\n\tif (!writes->head)\n\t\treturn;\n\n\t \n\tbio_list_init(&sync);\n\tbio_list_init(&nosync);\n\tbio_list_init(&recover);\n\tbio_list_init(&requeue);\n\n\twhile ((bio = bio_list_pop(writes))) {\n\t\tif ((bio->bi_opf & REQ_PREFLUSH) ||\n\t\t    (bio_op(bio) == REQ_OP_DISCARD)) {\n\t\t\tbio_list_add(&sync, bio);\n\t\t\tcontinue;\n\t\t}\n\n\t\tregion = dm_rh_bio_to_region(ms->rh, bio);\n\n\t\tif (log->type->is_remote_recovering &&\n\t\t    log->type->is_remote_recovering(log, region)) {\n\t\t\tbio_list_add(&requeue, bio);\n\t\t\tcontinue;\n\t\t}\n\n\t\tstate = dm_rh_get_state(ms->rh, region, 1);\n\t\tswitch (state) {\n\t\tcase DM_RH_CLEAN:\n\t\tcase DM_RH_DIRTY:\n\t\t\tthis_list = &sync;\n\t\t\tbreak;\n\n\t\tcase DM_RH_NOSYNC:\n\t\t\tthis_list = &nosync;\n\t\t\tbreak;\n\n\t\tcase DM_RH_RECOVERING:\n\t\t\tthis_list = &recover;\n\t\t\tbreak;\n\t\t}\n\n\t\tbio_list_add(this_list, bio);\n\t}\n\n\t \n\tif (unlikely(requeue.head)) {\n\t\tspin_lock_irq(&ms->lock);\n\t\tbio_list_merge(&ms->writes, &requeue);\n\t\tspin_unlock_irq(&ms->lock);\n\t\tdelayed_wake(ms);\n\t}\n\n\t \n\tdm_rh_inc_pending(ms->rh, &sync);\n\tdm_rh_inc_pending(ms->rh, &nosync);\n\n\t \n\tms->log_failure = dm_rh_flush(ms->rh) ? 1 : ms->log_failure;\n\n\t \n\tif (unlikely(ms->log_failure) && errors_handled(ms)) {\n\t\tspin_lock_irq(&ms->lock);\n\t\tbio_list_merge(&ms->failures, &sync);\n\t\tspin_unlock_irq(&ms->lock);\n\t\twakeup_mirrord(ms);\n\t} else\n\t\twhile ((bio = bio_list_pop(&sync)))\n\t\t\tdo_write(ms, bio);\n\n\twhile ((bio = bio_list_pop(&recover)))\n\t\tdm_rh_delay(ms->rh, bio);\n\n\twhile ((bio = bio_list_pop(&nosync))) {\n\t\tif (unlikely(ms->leg_failure) && errors_handled(ms) && !keep_log(ms)) {\n\t\t\tspin_lock_irq(&ms->lock);\n\t\t\tbio_list_add(&ms->failures, bio);\n\t\t\tspin_unlock_irq(&ms->lock);\n\t\t\twakeup_mirrord(ms);\n\t\t} else {\n\t\t\tmap_bio(get_default_mirror(ms), bio);\n\t\t\tsubmit_bio_noacct(bio);\n\t\t}\n\t}\n}\n\nstatic void do_failures(struct mirror_set *ms, struct bio_list *failures)\n{\n\tstruct bio *bio;\n\n\tif (likely(!failures->head))\n\t\treturn;\n\n\t \n\twhile ((bio = bio_list_pop(failures))) {\n\t\tif (!ms->log_failure) {\n\t\t\tms->in_sync = 0;\n\t\t\tdm_rh_mark_nosync(ms->rh, bio);\n\t\t}\n\n\t\t \n\t\tif (unlikely(!get_valid_mirror(ms) || (keep_log(ms) && ms->log_failure)))\n\t\t\tbio_io_error(bio);\n\t\telse if (errors_handled(ms) && !keep_log(ms))\n\t\t\thold_bio(ms, bio);\n\t\telse\n\t\t\tbio_endio(bio);\n\t}\n}\n\nstatic void trigger_event(struct work_struct *work)\n{\n\tstruct mirror_set *ms =\n\t\tcontainer_of(work, struct mirror_set, trigger_event);\n\n\tdm_table_event(ms->ti->table);\n}\n\n \nstatic void do_mirror(struct work_struct *work)\n{\n\tstruct mirror_set *ms = container_of(work, struct mirror_set,\n\t\t\t\t\t     kmirrord_work);\n\tstruct bio_list reads, writes, failures;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ms->lock, flags);\n\treads = ms->reads;\n\twrites = ms->writes;\n\tfailures = ms->failures;\n\tbio_list_init(&ms->reads);\n\tbio_list_init(&ms->writes);\n\tbio_list_init(&ms->failures);\n\tspin_unlock_irqrestore(&ms->lock, flags);\n\n\tdm_rh_update_states(ms->rh, errors_handled(ms));\n\tdo_recovery(ms);\n\tdo_reads(ms, &reads);\n\tdo_writes(ms, &writes);\n\tdo_failures(ms, &failures);\n}\n\n \nstatic struct mirror_set *alloc_context(unsigned int nr_mirrors,\n\t\t\t\t\tuint32_t region_size,\n\t\t\t\t\tstruct dm_target *ti,\n\t\t\t\t\tstruct dm_dirty_log *dl)\n{\n\tstruct mirror_set *ms =\n\t\tkzalloc(struct_size(ms, mirror, nr_mirrors), GFP_KERNEL);\n\n\tif (!ms) {\n\t\tti->error = \"Cannot allocate mirror context\";\n\t\treturn NULL;\n\t}\n\n\tspin_lock_init(&ms->lock);\n\tbio_list_init(&ms->reads);\n\tbio_list_init(&ms->writes);\n\tbio_list_init(&ms->failures);\n\tbio_list_init(&ms->holds);\n\n\tms->ti = ti;\n\tms->nr_mirrors = nr_mirrors;\n\tms->nr_regions = dm_sector_div_up(ti->len, region_size);\n\tms->in_sync = 0;\n\tms->log_failure = 0;\n\tms->leg_failure = 0;\n\tatomic_set(&ms->suspend, 0);\n\tatomic_set(&ms->default_mirror, DEFAULT_MIRROR);\n\n\tms->io_client = dm_io_client_create();\n\tif (IS_ERR(ms->io_client)) {\n\t\tti->error = \"Error creating dm_io client\";\n\t\tkfree(ms);\n\t\treturn NULL;\n\t}\n\n\tms->rh = dm_region_hash_create(ms, dispatch_bios, wakeup_mirrord,\n\t\t\t\t       wakeup_all_recovery_waiters,\n\t\t\t\t       ms->ti->begin, MAX_RECOVERY,\n\t\t\t\t       dl, region_size, ms->nr_regions);\n\tif (IS_ERR(ms->rh)) {\n\t\tti->error = \"Error creating dirty region hash\";\n\t\tdm_io_client_destroy(ms->io_client);\n\t\tkfree(ms);\n\t\treturn NULL;\n\t}\n\n\treturn ms;\n}\n\nstatic void free_context(struct mirror_set *ms, struct dm_target *ti,\n\t\t\t unsigned int m)\n{\n\twhile (m--)\n\t\tdm_put_device(ti, ms->mirror[m].dev);\n\n\tdm_io_client_destroy(ms->io_client);\n\tdm_region_hash_destroy(ms->rh);\n\tkfree(ms);\n}\n\nstatic int get_mirror(struct mirror_set *ms, struct dm_target *ti,\n\t\t      unsigned int mirror, char **argv)\n{\n\tunsigned long long offset;\n\tchar dummy;\n\tint ret;\n\n\tif (sscanf(argv[1], \"%llu%c\", &offset, &dummy) != 1 ||\n\t    offset != (sector_t)offset) {\n\t\tti->error = \"Invalid offset\";\n\t\treturn -EINVAL;\n\t}\n\n\tret = dm_get_device(ti, argv[0], dm_table_get_mode(ti->table),\n\t\t\t    &ms->mirror[mirror].dev);\n\tif (ret) {\n\t\tti->error = \"Device lookup failure\";\n\t\treturn ret;\n\t}\n\n\tms->mirror[mirror].ms = ms;\n\tatomic_set(&(ms->mirror[mirror].error_count), 0);\n\tms->mirror[mirror].error_type = 0;\n\tms->mirror[mirror].offset = offset;\n\n\treturn 0;\n}\n\n \nstatic struct dm_dirty_log *create_dirty_log(struct dm_target *ti,\n\t\t\t\t\t     unsigned int argc, char **argv,\n\t\t\t\t\t     unsigned int *args_used)\n{\n\tunsigned int param_count;\n\tstruct dm_dirty_log *dl;\n\tchar dummy;\n\n\tif (argc < 2) {\n\t\tti->error = \"Insufficient mirror log arguments\";\n\t\treturn NULL;\n\t}\n\n\tif (sscanf(argv[1], \"%u%c\", &param_count, &dummy) != 1) {\n\t\tti->error = \"Invalid mirror log argument count\";\n\t\treturn NULL;\n\t}\n\n\t*args_used = 2 + param_count;\n\n\tif (argc < *args_used) {\n\t\tti->error = \"Insufficient mirror log arguments\";\n\t\treturn NULL;\n\t}\n\n\tdl = dm_dirty_log_create(argv[0], ti, mirror_flush, param_count,\n\t\t\t\t argv + 2);\n\tif (!dl) {\n\t\tti->error = \"Error creating mirror dirty log\";\n\t\treturn NULL;\n\t}\n\n\treturn dl;\n}\n\nstatic int parse_features(struct mirror_set *ms, unsigned int argc, char **argv,\n\t\t\t  unsigned int *args_used)\n{\n\tunsigned int num_features;\n\tstruct dm_target *ti = ms->ti;\n\tchar dummy;\n\tint i;\n\n\t*args_used = 0;\n\n\tif (!argc)\n\t\treturn 0;\n\n\tif (sscanf(argv[0], \"%u%c\", &num_features, &dummy) != 1) {\n\t\tti->error = \"Invalid number of features\";\n\t\treturn -EINVAL;\n\t}\n\n\targc--;\n\targv++;\n\t(*args_used)++;\n\n\tif (num_features > argc) {\n\t\tti->error = \"Not enough arguments to support feature count\";\n\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0; i < num_features; i++) {\n\t\tif (!strcmp(\"handle_errors\", argv[0]))\n\t\t\tms->features |= DM_RAID1_HANDLE_ERRORS;\n\t\telse if (!strcmp(\"keep_log\", argv[0]))\n\t\t\tms->features |= DM_RAID1_KEEP_LOG;\n\t\telse {\n\t\t\tti->error = \"Unrecognised feature requested\";\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\targc--;\n\t\targv++;\n\t\t(*args_used)++;\n\t}\n\tif (!errors_handled(ms) && keep_log(ms)) {\n\t\tti->error = \"keep_log feature requires the handle_errors feature\";\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int mirror_ctr(struct dm_target *ti, unsigned int argc, char **argv)\n{\n\tint r;\n\tunsigned int nr_mirrors, m, args_used;\n\tstruct mirror_set *ms;\n\tstruct dm_dirty_log *dl;\n\tchar dummy;\n\n\tdl = create_dirty_log(ti, argc, argv, &args_used);\n\tif (!dl)\n\t\treturn -EINVAL;\n\n\targv += args_used;\n\targc -= args_used;\n\n\tif (!argc || sscanf(argv[0], \"%u%c\", &nr_mirrors, &dummy) != 1 ||\n\t    nr_mirrors < 2 || nr_mirrors > MAX_NR_MIRRORS) {\n\t\tti->error = \"Invalid number of mirrors\";\n\t\tdm_dirty_log_destroy(dl);\n\t\treturn -EINVAL;\n\t}\n\n\targv++, argc--;\n\n\tif (argc < nr_mirrors * 2) {\n\t\tti->error = \"Too few mirror arguments\";\n\t\tdm_dirty_log_destroy(dl);\n\t\treturn -EINVAL;\n\t}\n\n\tms = alloc_context(nr_mirrors, dl->type->get_region_size(dl), ti, dl);\n\tif (!ms) {\n\t\tdm_dirty_log_destroy(dl);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tfor (m = 0; m < nr_mirrors; m++) {\n\t\tr = get_mirror(ms, ti, m, argv);\n\t\tif (r) {\n\t\t\tfree_context(ms, ti, m);\n\t\t\treturn r;\n\t\t}\n\t\targv += 2;\n\t\targc -= 2;\n\t}\n\n\tti->private = ms;\n\n\tr = dm_set_target_max_io_len(ti, dm_rh_get_region_size(ms->rh));\n\tif (r)\n\t\tgoto err_free_context;\n\n\tti->num_flush_bios = 1;\n\tti->num_discard_bios = 1;\n\tti->per_io_data_size = sizeof(struct dm_raid1_bio_record);\n\n\tms->kmirrord_wq = alloc_workqueue(\"kmirrord\", WQ_MEM_RECLAIM, 0);\n\tif (!ms->kmirrord_wq) {\n\t\tDMERR(\"couldn't start kmirrord\");\n\t\tr = -ENOMEM;\n\t\tgoto err_free_context;\n\t}\n\tINIT_WORK(&ms->kmirrord_work, do_mirror);\n\ttimer_setup(&ms->timer, delayed_wake_fn, 0);\n\tms->timer_pending = 0;\n\tINIT_WORK(&ms->trigger_event, trigger_event);\n\n\tr = parse_features(ms, argc, argv, &args_used);\n\tif (r)\n\t\tgoto err_destroy_wq;\n\n\targv += args_used;\n\targc -= args_used;\n\n\t \n\n\tif (argc) {\n\t\tti->error = \"Too many mirror arguments\";\n\t\tr = -EINVAL;\n\t\tgoto err_destroy_wq;\n\t}\n\n\tms->kcopyd_client = dm_kcopyd_client_create(&dm_kcopyd_throttle);\n\tif (IS_ERR(ms->kcopyd_client)) {\n\t\tr = PTR_ERR(ms->kcopyd_client);\n\t\tgoto err_destroy_wq;\n\t}\n\n\twakeup_mirrord(ms);\n\treturn 0;\n\nerr_destroy_wq:\n\tdestroy_workqueue(ms->kmirrord_wq);\nerr_free_context:\n\tfree_context(ms, ti, ms->nr_mirrors);\n\treturn r;\n}\n\nstatic void mirror_dtr(struct dm_target *ti)\n{\n\tstruct mirror_set *ms = ti->private;\n\n\tdel_timer_sync(&ms->timer);\n\tflush_workqueue(ms->kmirrord_wq);\n\tflush_work(&ms->trigger_event);\n\tdm_kcopyd_client_destroy(ms->kcopyd_client);\n\tdestroy_workqueue(ms->kmirrord_wq);\n\tfree_context(ms, ti, ms->nr_mirrors);\n}\n\n \nstatic int mirror_map(struct dm_target *ti, struct bio *bio)\n{\n\tint r, rw = bio_data_dir(bio);\n\tstruct mirror *m;\n\tstruct mirror_set *ms = ti->private;\n\tstruct dm_dirty_log *log = dm_rh_dirty_log(ms->rh);\n\tstruct dm_raid1_bio_record *bio_record =\n\t  dm_per_bio_data(bio, sizeof(struct dm_raid1_bio_record));\n\n\tbio_record->details.bi_bdev = NULL;\n\n\tif (rw == WRITE) {\n\t\t \n\t\tbio_record->write_region = dm_rh_bio_to_region(ms->rh, bio);\n\t\tqueue_bio(ms, bio, rw);\n\t\treturn DM_MAPIO_SUBMITTED;\n\t}\n\n\tr = log->type->in_sync(log, dm_rh_bio_to_region(ms->rh, bio), 0);\n\tif (r < 0 && r != -EWOULDBLOCK)\n\t\treturn DM_MAPIO_KILL;\n\n\t \n\tif (!r || (r == -EWOULDBLOCK)) {\n\t\tif (bio->bi_opf & REQ_RAHEAD)\n\t\t\treturn DM_MAPIO_KILL;\n\n\t\tqueue_bio(ms, bio, rw);\n\t\treturn DM_MAPIO_SUBMITTED;\n\t}\n\n\t \n\tm = choose_mirror(ms, bio->bi_iter.bi_sector);\n\tif (unlikely(!m))\n\t\treturn DM_MAPIO_KILL;\n\n\tdm_bio_record(&bio_record->details, bio);\n\tbio_record->m = m;\n\n\tmap_bio(m, bio);\n\n\treturn DM_MAPIO_REMAPPED;\n}\n\nstatic int mirror_end_io(struct dm_target *ti, struct bio *bio,\n\t\tblk_status_t *error)\n{\n\tint rw = bio_data_dir(bio);\n\tstruct mirror_set *ms = ti->private;\n\tstruct mirror *m = NULL;\n\tstruct dm_bio_details *bd = NULL;\n\tstruct dm_raid1_bio_record *bio_record =\n\t  dm_per_bio_data(bio, sizeof(struct dm_raid1_bio_record));\n\n\t \n\tif (rw == WRITE) {\n\t\tif (!(bio->bi_opf & REQ_PREFLUSH) &&\n\t\t    bio_op(bio) != REQ_OP_DISCARD)\n\t\t\tdm_rh_dec(ms->rh, bio_record->write_region);\n\t\treturn DM_ENDIO_DONE;\n\t}\n\n\tif (*error == BLK_STS_NOTSUPP)\n\t\tgoto out;\n\n\tif (bio->bi_opf & REQ_RAHEAD)\n\t\tgoto out;\n\n\tif (unlikely(*error)) {\n\t\tif (!bio_record->details.bi_bdev) {\n\t\t\t \n\t\t\tDMERR_LIMIT(\"Mirror read failed.\");\n\t\t\treturn DM_ENDIO_DONE;\n\t\t}\n\n\t\tm = bio_record->m;\n\n\t\tDMERR(\"Mirror read failed from %s. Trying alternative device.\",\n\t\t      m->dev->name);\n\n\t\tfail_mirror(m, DM_RAID1_READ_ERROR);\n\n\t\t \n\t\tif (default_ok(m) || mirror_available(ms, bio)) {\n\t\t\tbd = &bio_record->details;\n\n\t\t\tdm_bio_restore(bd, bio);\n\t\t\tbio_record->details.bi_bdev = NULL;\n\t\t\tbio->bi_status = 0;\n\n\t\t\tqueue_bio(ms, bio, rw);\n\t\t\treturn DM_ENDIO_INCOMPLETE;\n\t\t}\n\t\tDMERR(\"All replicated volumes dead, failing I/O\");\n\t}\n\nout:\n\tbio_record->details.bi_bdev = NULL;\n\n\treturn DM_ENDIO_DONE;\n}\n\nstatic void mirror_presuspend(struct dm_target *ti)\n{\n\tstruct mirror_set *ms = ti->private;\n\tstruct dm_dirty_log *log = dm_rh_dirty_log(ms->rh);\n\n\tstruct bio_list holds;\n\tstruct bio *bio;\n\n\tatomic_set(&ms->suspend, 1);\n\n\t \n\tspin_lock_irq(&ms->lock);\n\tholds = ms->holds;\n\tbio_list_init(&ms->holds);\n\tspin_unlock_irq(&ms->lock);\n\n\twhile ((bio = bio_list_pop(&holds)))\n\t\thold_bio(ms, bio);\n\n\t \n\tdm_rh_stop_recovery(ms->rh);\n\n\twait_event(_kmirrord_recovery_stopped,\n\t\t   !dm_rh_recovery_in_flight(ms->rh));\n\n\tif (log->type->presuspend && log->type->presuspend(log))\n\t\t \n\t\tDMWARN(\"log presuspend failed\");\n\n\t \n\tflush_workqueue(ms->kmirrord_wq);\n}\n\nstatic void mirror_postsuspend(struct dm_target *ti)\n{\n\tstruct mirror_set *ms = ti->private;\n\tstruct dm_dirty_log *log = dm_rh_dirty_log(ms->rh);\n\n\tif (log->type->postsuspend && log->type->postsuspend(log))\n\t\t \n\t\tDMWARN(\"log postsuspend failed\");\n}\n\nstatic void mirror_resume(struct dm_target *ti)\n{\n\tstruct mirror_set *ms = ti->private;\n\tstruct dm_dirty_log *log = dm_rh_dirty_log(ms->rh);\n\n\tatomic_set(&ms->suspend, 0);\n\tif (log->type->resume && log->type->resume(log))\n\t\t \n\t\tDMWARN(\"log resume failed\");\n\tdm_rh_start_recovery(ms->rh);\n}\n\n \nstatic char device_status_char(struct mirror *m)\n{\n\tif (!atomic_read(&(m->error_count)))\n\t\treturn 'A';\n\n\treturn (test_bit(DM_RAID1_FLUSH_ERROR, &(m->error_type))) ? 'F' :\n\t\t(test_bit(DM_RAID1_WRITE_ERROR, &(m->error_type))) ? 'D' :\n\t\t(test_bit(DM_RAID1_SYNC_ERROR, &(m->error_type))) ? 'S' :\n\t\t(test_bit(DM_RAID1_READ_ERROR, &(m->error_type))) ? 'R' : 'U';\n}\n\n\nstatic void mirror_status(struct dm_target *ti, status_type_t type,\n\t\t\t  unsigned int status_flags, char *result, unsigned int maxlen)\n{\n\tunsigned int m, sz = 0;\n\tint num_feature_args = 0;\n\tstruct mirror_set *ms = ti->private;\n\tstruct dm_dirty_log *log = dm_rh_dirty_log(ms->rh);\n\tchar buffer[MAX_NR_MIRRORS + 1];\n\n\tswitch (type) {\n\tcase STATUSTYPE_INFO:\n\t\tDMEMIT(\"%d \", ms->nr_mirrors);\n\t\tfor (m = 0; m < ms->nr_mirrors; m++) {\n\t\t\tDMEMIT(\"%s \", ms->mirror[m].dev->name);\n\t\t\tbuffer[m] = device_status_char(&(ms->mirror[m]));\n\t\t}\n\t\tbuffer[m] = '\\0';\n\n\t\tDMEMIT(\"%llu/%llu 1 %s \",\n\t\t      (unsigned long long)log->type->get_sync_count(log),\n\t\t      (unsigned long long)ms->nr_regions, buffer);\n\n\t\tsz += log->type->status(log, type, result+sz, maxlen-sz);\n\n\t\tbreak;\n\n\tcase STATUSTYPE_TABLE:\n\t\tsz = log->type->status(log, type, result, maxlen);\n\n\t\tDMEMIT(\"%d\", ms->nr_mirrors);\n\t\tfor (m = 0; m < ms->nr_mirrors; m++)\n\t\t\tDMEMIT(\" %s %llu\", ms->mirror[m].dev->name,\n\t\t\t       (unsigned long long)ms->mirror[m].offset);\n\n\t\tnum_feature_args += !!errors_handled(ms);\n\t\tnum_feature_args += !!keep_log(ms);\n\t\tif (num_feature_args) {\n\t\t\tDMEMIT(\" %d\", num_feature_args);\n\t\t\tif (errors_handled(ms))\n\t\t\t\tDMEMIT(\" handle_errors\");\n\t\t\tif (keep_log(ms))\n\t\t\t\tDMEMIT(\" keep_log\");\n\t\t}\n\n\t\tbreak;\n\n\tcase STATUSTYPE_IMA:\n\t\tDMEMIT_TARGET_NAME_VERSION(ti->type);\n\t\tDMEMIT(\",nr_mirrors=%d\", ms->nr_mirrors);\n\t\tfor (m = 0; m < ms->nr_mirrors; m++) {\n\t\t\tDMEMIT(\",mirror_device_%d=%s\", m, ms->mirror[m].dev->name);\n\t\t\tDMEMIT(\",mirror_device_%d_status=%c\",\n\t\t\t       m, device_status_char(&(ms->mirror[m])));\n\t\t}\n\n\t\tDMEMIT(\",handle_errors=%c\", errors_handled(ms) ? 'y' : 'n');\n\t\tDMEMIT(\",keep_log=%c\", keep_log(ms) ? 'y' : 'n');\n\n\t\tDMEMIT(\",log_type_status=\");\n\t\tsz += log->type->status(log, type, result+sz, maxlen-sz);\n\t\tDMEMIT(\";\");\n\t\tbreak;\n\t}\n}\n\nstatic int mirror_iterate_devices(struct dm_target *ti,\n\t\t\t\t  iterate_devices_callout_fn fn, void *data)\n{\n\tstruct mirror_set *ms = ti->private;\n\tint ret = 0;\n\tunsigned int i;\n\n\tfor (i = 0; !ret && i < ms->nr_mirrors; i++)\n\t\tret = fn(ti, ms->mirror[i].dev,\n\t\t\t ms->mirror[i].offset, ti->len, data);\n\n\treturn ret;\n}\n\nstatic struct target_type mirror_target = {\n\t.name\t = \"mirror\",\n\t.version = {1, 14, 0},\n\t.module\t = THIS_MODULE,\n\t.ctr\t = mirror_ctr,\n\t.dtr\t = mirror_dtr,\n\t.map\t = mirror_map,\n\t.end_io\t = mirror_end_io,\n\t.presuspend = mirror_presuspend,\n\t.postsuspend = mirror_postsuspend,\n\t.resume\t = mirror_resume,\n\t.status\t = mirror_status,\n\t.iterate_devices = mirror_iterate_devices,\n};\n\nstatic int __init dm_mirror_init(void)\n{\n\tint r;\n\n\tdm_raid1_wq = alloc_workqueue(\"dm_raid1_wq\", 0, 0);\n\tif (!dm_raid1_wq) {\n\t\tDMERR(\"Failed to alloc workqueue\");\n\t\treturn -ENOMEM;\n\t}\n\n\tr = dm_register_target(&mirror_target);\n\tif (r < 0) {\n\t\tdestroy_workqueue(dm_raid1_wq);\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic void __exit dm_mirror_exit(void)\n{\n\tdestroy_workqueue(dm_raid1_wq);\n\tdm_unregister_target(&mirror_target);\n}\n\n \nmodule_init(dm_mirror_init);\nmodule_exit(dm_mirror_exit);\n\nMODULE_DESCRIPTION(DM_NAME \" mirror target\");\nMODULE_AUTHOR(\"Joe Thornber\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}