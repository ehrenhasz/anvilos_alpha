{
  "module_name": "raid1.c",
  "hash_id": "d1e4dd98a31e2e3ccdb9f7f19729bde840a25079fbb0a743292d0c8b4cc07f2d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/raid1.c",
  "human_readable_source": "\n \n\n#include <linux/slab.h>\n#include <linux/delay.h>\n#include <linux/blkdev.h>\n#include <linux/module.h>\n#include <linux/seq_file.h>\n#include <linux/ratelimit.h>\n#include <linux/interval_tree_generic.h>\n\n#include <trace/events/block.h>\n\n#include \"md.h\"\n#include \"raid1.h\"\n#include \"md-bitmap.h\"\n\n#define UNSUPPORTED_MDDEV_FLAGS\t\t\\\n\t((1L << MD_HAS_JOURNAL) |\t\\\n\t (1L << MD_JOURNAL_CLEAN) |\t\\\n\t (1L << MD_HAS_PPL) |\t\t\\\n\t (1L << MD_HAS_MULTIPLE_PPLS))\n\nstatic void allow_barrier(struct r1conf *conf, sector_t sector_nr);\nstatic void lower_barrier(struct r1conf *conf, sector_t sector_nr);\n\n#define raid1_log(md, fmt, args...)\t\t\t\t\\\n\tdo { if ((md)->queue) blk_add_trace_msg((md)->queue, \"raid1 \" fmt, ##args); } while (0)\n\n#include \"raid1-10.c\"\n\n#define START(node) ((node)->start)\n#define LAST(node) ((node)->last)\nINTERVAL_TREE_DEFINE(struct serial_info, node, sector_t, _subtree_last,\n\t\t     START, LAST, static inline, raid1_rb);\n\nstatic int check_and_add_serial(struct md_rdev *rdev, struct r1bio *r1_bio,\n\t\t\t\tstruct serial_info *si, int idx)\n{\n\tunsigned long flags;\n\tint ret = 0;\n\tsector_t lo = r1_bio->sector;\n\tsector_t hi = lo + r1_bio->sectors;\n\tstruct serial_in_rdev *serial = &rdev->serial[idx];\n\n\tspin_lock_irqsave(&serial->serial_lock, flags);\n\t \n\tif (raid1_rb_iter_first(&serial->serial_rb, lo, hi))\n\t\tret = -EBUSY;\n\telse {\n\t\tsi->start = lo;\n\t\tsi->last = hi;\n\t\traid1_rb_insert(si, &serial->serial_rb);\n\t}\n\tspin_unlock_irqrestore(&serial->serial_lock, flags);\n\n\treturn ret;\n}\n\nstatic void wait_for_serialization(struct md_rdev *rdev, struct r1bio *r1_bio)\n{\n\tstruct mddev *mddev = rdev->mddev;\n\tstruct serial_info *si;\n\tint idx = sector_to_idx(r1_bio->sector);\n\tstruct serial_in_rdev *serial = &rdev->serial[idx];\n\n\tif (WARN_ON(!mddev->serial_info_pool))\n\t\treturn;\n\tsi = mempool_alloc(mddev->serial_info_pool, GFP_NOIO);\n\twait_event(serial->serial_io_wait,\n\t\t   check_and_add_serial(rdev, r1_bio, si, idx) == 0);\n}\n\nstatic void remove_serial(struct md_rdev *rdev, sector_t lo, sector_t hi)\n{\n\tstruct serial_info *si;\n\tunsigned long flags;\n\tint found = 0;\n\tstruct mddev *mddev = rdev->mddev;\n\tint idx = sector_to_idx(lo);\n\tstruct serial_in_rdev *serial = &rdev->serial[idx];\n\n\tspin_lock_irqsave(&serial->serial_lock, flags);\n\tfor (si = raid1_rb_iter_first(&serial->serial_rb, lo, hi);\n\t     si; si = raid1_rb_iter_next(si, lo, hi)) {\n\t\tif (si->start == lo && si->last == hi) {\n\t\t\traid1_rb_remove(si, &serial->serial_rb);\n\t\t\tmempool_free(si, mddev->serial_info_pool);\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found)\n\t\tWARN(1, \"The write IO is not recorded for serialization\\n\");\n\tspin_unlock_irqrestore(&serial->serial_lock, flags);\n\twake_up(&serial->serial_io_wait);\n}\n\n \nstatic inline struct r1bio *get_resync_r1bio(struct bio *bio)\n{\n\treturn get_resync_pages(bio)->raid_bio;\n}\n\nstatic void * r1bio_pool_alloc(gfp_t gfp_flags, void *data)\n{\n\tstruct pool_info *pi = data;\n\tint size = offsetof(struct r1bio, bios[pi->raid_disks]);\n\n\t \n\treturn kzalloc(size, gfp_flags);\n}\n\n#define RESYNC_DEPTH 32\n#define RESYNC_SECTORS (RESYNC_BLOCK_SIZE >> 9)\n#define RESYNC_WINDOW (RESYNC_BLOCK_SIZE * RESYNC_DEPTH)\n#define RESYNC_WINDOW_SECTORS (RESYNC_WINDOW >> 9)\n#define CLUSTER_RESYNC_WINDOW (16 * RESYNC_WINDOW)\n#define CLUSTER_RESYNC_WINDOW_SECTORS (CLUSTER_RESYNC_WINDOW >> 9)\n\nstatic void * r1buf_pool_alloc(gfp_t gfp_flags, void *data)\n{\n\tstruct pool_info *pi = data;\n\tstruct r1bio *r1_bio;\n\tstruct bio *bio;\n\tint need_pages;\n\tint j;\n\tstruct resync_pages *rps;\n\n\tr1_bio = r1bio_pool_alloc(gfp_flags, pi);\n\tif (!r1_bio)\n\t\treturn NULL;\n\n\trps = kmalloc_array(pi->raid_disks, sizeof(struct resync_pages),\n\t\t\t    gfp_flags);\n\tif (!rps)\n\t\tgoto out_free_r1bio;\n\n\t \n\tfor (j = pi->raid_disks ; j-- ; ) {\n\t\tbio = bio_kmalloc(RESYNC_PAGES, gfp_flags);\n\t\tif (!bio)\n\t\t\tgoto out_free_bio;\n\t\tbio_init(bio, NULL, bio->bi_inline_vecs, RESYNC_PAGES, 0);\n\t\tr1_bio->bios[j] = bio;\n\t}\n\t \n\tif (test_bit(MD_RECOVERY_REQUESTED, &pi->mddev->recovery))\n\t\tneed_pages = pi->raid_disks;\n\telse\n\t\tneed_pages = 1;\n\tfor (j = 0; j < pi->raid_disks; j++) {\n\t\tstruct resync_pages *rp = &rps[j];\n\n\t\tbio = r1_bio->bios[j];\n\n\t\tif (j < need_pages) {\n\t\t\tif (resync_alloc_pages(rp, gfp_flags))\n\t\t\t\tgoto out_free_pages;\n\t\t} else {\n\t\t\tmemcpy(rp, &rps[0], sizeof(*rp));\n\t\t\tresync_get_all_pages(rp);\n\t\t}\n\n\t\trp->raid_bio = r1_bio;\n\t\tbio->bi_private = rp;\n\t}\n\n\tr1_bio->master_bio = NULL;\n\n\treturn r1_bio;\n\nout_free_pages:\n\twhile (--j >= 0)\n\t\tresync_free_pages(&rps[j]);\n\nout_free_bio:\n\twhile (++j < pi->raid_disks) {\n\t\tbio_uninit(r1_bio->bios[j]);\n\t\tkfree(r1_bio->bios[j]);\n\t}\n\tkfree(rps);\n\nout_free_r1bio:\n\trbio_pool_free(r1_bio, data);\n\treturn NULL;\n}\n\nstatic void r1buf_pool_free(void *__r1_bio, void *data)\n{\n\tstruct pool_info *pi = data;\n\tint i;\n\tstruct r1bio *r1bio = __r1_bio;\n\tstruct resync_pages *rp = NULL;\n\n\tfor (i = pi->raid_disks; i--; ) {\n\t\trp = get_resync_pages(r1bio->bios[i]);\n\t\tresync_free_pages(rp);\n\t\tbio_uninit(r1bio->bios[i]);\n\t\tkfree(r1bio->bios[i]);\n\t}\n\n\t \n\tkfree(rp);\n\n\trbio_pool_free(r1bio, data);\n}\n\nstatic void put_all_bios(struct r1conf *conf, struct r1bio *r1_bio)\n{\n\tint i;\n\n\tfor (i = 0; i < conf->raid_disks * 2; i++) {\n\t\tstruct bio **bio = r1_bio->bios + i;\n\t\tif (!BIO_SPECIAL(*bio))\n\t\t\tbio_put(*bio);\n\t\t*bio = NULL;\n\t}\n}\n\nstatic void free_r1bio(struct r1bio *r1_bio)\n{\n\tstruct r1conf *conf = r1_bio->mddev->private;\n\n\tput_all_bios(conf, r1_bio);\n\tmempool_free(r1_bio, &conf->r1bio_pool);\n}\n\nstatic void put_buf(struct r1bio *r1_bio)\n{\n\tstruct r1conf *conf = r1_bio->mddev->private;\n\tsector_t sect = r1_bio->sector;\n\tint i;\n\n\tfor (i = 0; i < conf->raid_disks * 2; i++) {\n\t\tstruct bio *bio = r1_bio->bios[i];\n\t\tif (bio->bi_end_io)\n\t\t\trdev_dec_pending(conf->mirrors[i].rdev, r1_bio->mddev);\n\t}\n\n\tmempool_free(r1_bio, &conf->r1buf_pool);\n\n\tlower_barrier(conf, sect);\n}\n\nstatic void reschedule_retry(struct r1bio *r1_bio)\n{\n\tunsigned long flags;\n\tstruct mddev *mddev = r1_bio->mddev;\n\tstruct r1conf *conf = mddev->private;\n\tint idx;\n\n\tidx = sector_to_idx(r1_bio->sector);\n\tspin_lock_irqsave(&conf->device_lock, flags);\n\tlist_add(&r1_bio->retry_list, &conf->retry_list);\n\tatomic_inc(&conf->nr_queued[idx]);\n\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\n\twake_up(&conf->wait_barrier);\n\tmd_wakeup_thread(mddev->thread);\n}\n\n \nstatic void call_bio_endio(struct r1bio *r1_bio)\n{\n\tstruct bio *bio = r1_bio->master_bio;\n\n\tif (!test_bit(R1BIO_Uptodate, &r1_bio->state))\n\t\tbio->bi_status = BLK_STS_IOERR;\n\n\tbio_endio(bio);\n}\n\nstatic void raid_end_bio_io(struct r1bio *r1_bio)\n{\n\tstruct bio *bio = r1_bio->master_bio;\n\tstruct r1conf *conf = r1_bio->mddev->private;\n\tsector_t sector = r1_bio->sector;\n\n\t \n\tif (!test_and_set_bit(R1BIO_Returned, &r1_bio->state)) {\n\t\tpr_debug(\"raid1: sync end %s on sectors %llu-%llu\\n\",\n\t\t\t (bio_data_dir(bio) == WRITE) ? \"write\" : \"read\",\n\t\t\t (unsigned long long) bio->bi_iter.bi_sector,\n\t\t\t (unsigned long long) bio_end_sector(bio) - 1);\n\n\t\tcall_bio_endio(r1_bio);\n\t}\n\n\tfree_r1bio(r1_bio);\n\t \n\tallow_barrier(conf, sector);\n}\n\n \nstatic inline void update_head_pos(int disk, struct r1bio *r1_bio)\n{\n\tstruct r1conf *conf = r1_bio->mddev->private;\n\n\tconf->mirrors[disk].head_position =\n\t\tr1_bio->sector + (r1_bio->sectors);\n}\n\n \nstatic int find_bio_disk(struct r1bio *r1_bio, struct bio *bio)\n{\n\tint mirror;\n\tstruct r1conf *conf = r1_bio->mddev->private;\n\tint raid_disks = conf->raid_disks;\n\n\tfor (mirror = 0; mirror < raid_disks * 2; mirror++)\n\t\tif (r1_bio->bios[mirror] == bio)\n\t\t\tbreak;\n\n\tBUG_ON(mirror == raid_disks * 2);\n\tupdate_head_pos(mirror, r1_bio);\n\n\treturn mirror;\n}\n\nstatic void raid1_end_read_request(struct bio *bio)\n{\n\tint uptodate = !bio->bi_status;\n\tstruct r1bio *r1_bio = bio->bi_private;\n\tstruct r1conf *conf = r1_bio->mddev->private;\n\tstruct md_rdev *rdev = conf->mirrors[r1_bio->read_disk].rdev;\n\n\t \n\tupdate_head_pos(r1_bio->read_disk, r1_bio);\n\n\tif (uptodate)\n\t\tset_bit(R1BIO_Uptodate, &r1_bio->state);\n\telse if (test_bit(FailFast, &rdev->flags) &&\n\t\t test_bit(R1BIO_FailFast, &r1_bio->state))\n\t\t \n\t\t;\n\telse {\n\t\t \n\t\tunsigned long flags;\n\t\tspin_lock_irqsave(&conf->device_lock, flags);\n\t\tif (r1_bio->mddev->degraded == conf->raid_disks ||\n\t\t    (r1_bio->mddev->degraded == conf->raid_disks-1 &&\n\t\t     test_bit(In_sync, &rdev->flags)))\n\t\t\tuptodate = 1;\n\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\t}\n\n\tif (uptodate) {\n\t\traid_end_bio_io(r1_bio);\n\t\trdev_dec_pending(rdev, conf->mddev);\n\t} else {\n\t\t \n\t\tpr_err_ratelimited(\"md/raid1:%s: %pg: rescheduling sector %llu\\n\",\n\t\t\t\t   mdname(conf->mddev),\n\t\t\t\t   rdev->bdev,\n\t\t\t\t   (unsigned long long)r1_bio->sector);\n\t\tset_bit(R1BIO_ReadError, &r1_bio->state);\n\t\treschedule_retry(r1_bio);\n\t\t \n\t}\n}\n\nstatic void close_write(struct r1bio *r1_bio)\n{\n\t \n\tif (test_bit(R1BIO_BehindIO, &r1_bio->state)) {\n\t\tbio_free_pages(r1_bio->behind_master_bio);\n\t\tbio_put(r1_bio->behind_master_bio);\n\t\tr1_bio->behind_master_bio = NULL;\n\t}\n\t \n\tmd_bitmap_endwrite(r1_bio->mddev->bitmap, r1_bio->sector,\n\t\t\t   r1_bio->sectors,\n\t\t\t   !test_bit(R1BIO_Degraded, &r1_bio->state),\n\t\t\t   test_bit(R1BIO_BehindIO, &r1_bio->state));\n\tmd_write_end(r1_bio->mddev);\n}\n\nstatic void r1_bio_write_done(struct r1bio *r1_bio)\n{\n\tif (!atomic_dec_and_test(&r1_bio->remaining))\n\t\treturn;\n\n\tif (test_bit(R1BIO_WriteError, &r1_bio->state))\n\t\treschedule_retry(r1_bio);\n\telse {\n\t\tclose_write(r1_bio);\n\t\tif (test_bit(R1BIO_MadeGood, &r1_bio->state))\n\t\t\treschedule_retry(r1_bio);\n\t\telse\n\t\t\traid_end_bio_io(r1_bio);\n\t}\n}\n\nstatic void raid1_end_write_request(struct bio *bio)\n{\n\tstruct r1bio *r1_bio = bio->bi_private;\n\tint behind = test_bit(R1BIO_BehindIO, &r1_bio->state);\n\tstruct r1conf *conf = r1_bio->mddev->private;\n\tstruct bio *to_put = NULL;\n\tint mirror = find_bio_disk(r1_bio, bio);\n\tstruct md_rdev *rdev = conf->mirrors[mirror].rdev;\n\tbool discard_error;\n\tsector_t lo = r1_bio->sector;\n\tsector_t hi = r1_bio->sector + r1_bio->sectors;\n\n\tdiscard_error = bio->bi_status && bio_op(bio) == REQ_OP_DISCARD;\n\n\t \n\tif (bio->bi_status && !discard_error) {\n\t\tset_bit(WriteErrorSeen,\t&rdev->flags);\n\t\tif (!test_and_set_bit(WantReplacement, &rdev->flags))\n\t\t\tset_bit(MD_RECOVERY_NEEDED, &\n\t\t\t\tconf->mddev->recovery);\n\n\t\tif (test_bit(FailFast, &rdev->flags) &&\n\t\t    (bio->bi_opf & MD_FAILFAST) &&\n\t\t     \n\t\t    !test_bit(WriteMostly, &rdev->flags)) {\n\t\t\tmd_error(r1_bio->mddev, rdev);\n\t\t}\n\n\t\t \n\t\tif (!test_bit(Faulty, &rdev->flags))\n\t\t\tset_bit(R1BIO_WriteError, &r1_bio->state);\n\t\telse {\n\t\t\t \n\t\t\tset_bit(R1BIO_Degraded, &r1_bio->state);\n\t\t\t \n\t\t\tr1_bio->bios[mirror] = NULL;\n\t\t\tto_put = bio;\n\t\t}\n\t} else {\n\t\t \n\t\tsector_t first_bad;\n\t\tint bad_sectors;\n\n\t\tr1_bio->bios[mirror] = NULL;\n\t\tto_put = bio;\n\t\t \n\t\tif (test_bit(In_sync, &rdev->flags) &&\n\t\t    !test_bit(Faulty, &rdev->flags))\n\t\t\tset_bit(R1BIO_Uptodate, &r1_bio->state);\n\n\t\t \n\t\tif (is_badblock(rdev, r1_bio->sector, r1_bio->sectors,\n\t\t\t\t&first_bad, &bad_sectors) && !discard_error) {\n\t\t\tr1_bio->bios[mirror] = IO_MADE_GOOD;\n\t\t\tset_bit(R1BIO_MadeGood, &r1_bio->state);\n\t\t}\n\t}\n\n\tif (behind) {\n\t\tif (test_bit(CollisionCheck, &rdev->flags))\n\t\t\tremove_serial(rdev, lo, hi);\n\t\tif (test_bit(WriteMostly, &rdev->flags))\n\t\t\tatomic_dec(&r1_bio->behind_remaining);\n\n\t\t \n\t\tif (atomic_read(&r1_bio->behind_remaining) >= (atomic_read(&r1_bio->remaining)-1) &&\n\t\t    test_bit(R1BIO_Uptodate, &r1_bio->state)) {\n\t\t\t \n\t\t\tif (!test_and_set_bit(R1BIO_Returned, &r1_bio->state)) {\n\t\t\t\tstruct bio *mbio = r1_bio->master_bio;\n\t\t\t\tpr_debug(\"raid1: behind end write sectors\"\n\t\t\t\t\t \" %llu-%llu\\n\",\n\t\t\t\t\t (unsigned long long) mbio->bi_iter.bi_sector,\n\t\t\t\t\t (unsigned long long) bio_end_sector(mbio) - 1);\n\t\t\t\tcall_bio_endio(r1_bio);\n\t\t\t}\n\t\t}\n\t} else if (rdev->mddev->serialize_policy)\n\t\tremove_serial(rdev, lo, hi);\n\tif (r1_bio->bios[mirror] == NULL)\n\t\trdev_dec_pending(rdev, conf->mddev);\n\n\t \n\tr1_bio_write_done(r1_bio);\n\n\tif (to_put)\n\t\tbio_put(to_put);\n}\n\nstatic sector_t align_to_barrier_unit_end(sector_t start_sector,\n\t\t\t\t\t  sector_t sectors)\n{\n\tsector_t len;\n\n\tWARN_ON(sectors == 0);\n\t \n\tlen = round_up(start_sector + 1, BARRIER_UNIT_SECTOR_SIZE) -\n\t      start_sector;\n\n\tif (len > sectors)\n\t\tlen = sectors;\n\n\treturn len;\n}\n\n \nstatic int read_balance(struct r1conf *conf, struct r1bio *r1_bio, int *max_sectors)\n{\n\tconst sector_t this_sector = r1_bio->sector;\n\tint sectors;\n\tint best_good_sectors;\n\tint best_disk, best_dist_disk, best_pending_disk;\n\tint has_nonrot_disk;\n\tint disk;\n\tsector_t best_dist;\n\tunsigned int min_pending;\n\tstruct md_rdev *rdev;\n\tint choose_first;\n\tint choose_next_idle;\n\n\trcu_read_lock();\n\t \n retry:\n\tsectors = r1_bio->sectors;\n\tbest_disk = -1;\n\tbest_dist_disk = -1;\n\tbest_dist = MaxSector;\n\tbest_pending_disk = -1;\n\tmin_pending = UINT_MAX;\n\tbest_good_sectors = 0;\n\thas_nonrot_disk = 0;\n\tchoose_next_idle = 0;\n\tclear_bit(R1BIO_FailFast, &r1_bio->state);\n\n\tif ((conf->mddev->recovery_cp < this_sector + sectors) ||\n\t    (mddev_is_clustered(conf->mddev) &&\n\t    md_cluster_ops->area_resyncing(conf->mddev, READ, this_sector,\n\t\t    this_sector + sectors)))\n\t\tchoose_first = 1;\n\telse\n\t\tchoose_first = 0;\n\n\tfor (disk = 0 ; disk < conf->raid_disks * 2 ; disk++) {\n\t\tsector_t dist;\n\t\tsector_t first_bad;\n\t\tint bad_sectors;\n\t\tunsigned int pending;\n\t\tbool nonrot;\n\n\t\trdev = rcu_dereference(conf->mirrors[disk].rdev);\n\t\tif (r1_bio->bios[disk] == IO_BLOCKED\n\t\t    || rdev == NULL\n\t\t    || test_bit(Faulty, &rdev->flags))\n\t\t\tcontinue;\n\t\tif (!test_bit(In_sync, &rdev->flags) &&\n\t\t    rdev->recovery_offset < this_sector + sectors)\n\t\t\tcontinue;\n\t\tif (test_bit(WriteMostly, &rdev->flags)) {\n\t\t\t \n\t\t\tif (best_dist_disk < 0) {\n\t\t\t\tif (is_badblock(rdev, this_sector, sectors,\n\t\t\t\t\t\t&first_bad, &bad_sectors)) {\n\t\t\t\t\tif (first_bad <= this_sector)\n\t\t\t\t\t\t \n\t\t\t\t\t\tcontinue;\n\t\t\t\t\tbest_good_sectors = first_bad - this_sector;\n\t\t\t\t} else\n\t\t\t\t\tbest_good_sectors = sectors;\n\t\t\t\tbest_dist_disk = disk;\n\t\t\t\tbest_pending_disk = disk;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tif (is_badblock(rdev, this_sector, sectors,\n\t\t\t\t&first_bad, &bad_sectors)) {\n\t\t\tif (best_dist < MaxSector)\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\tif (first_bad <= this_sector) {\n\t\t\t\t \n\t\t\t\tbad_sectors -= (this_sector - first_bad);\n\t\t\t\tif (choose_first && sectors > bad_sectors)\n\t\t\t\t\tsectors = bad_sectors;\n\t\t\t\tif (best_good_sectors > sectors)\n\t\t\t\t\tbest_good_sectors = sectors;\n\n\t\t\t} else {\n\t\t\t\tsector_t good_sectors = first_bad - this_sector;\n\t\t\t\tif (good_sectors > best_good_sectors) {\n\t\t\t\t\tbest_good_sectors = good_sectors;\n\t\t\t\t\tbest_disk = disk;\n\t\t\t\t}\n\t\t\t\tif (choose_first)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t} else {\n\t\t\tif ((sectors > best_good_sectors) && (best_disk >= 0))\n\t\t\t\tbest_disk = -1;\n\t\t\tbest_good_sectors = sectors;\n\t\t}\n\n\t\tif (best_disk >= 0)\n\t\t\t \n\t\t\tset_bit(R1BIO_FailFast, &r1_bio->state);\n\n\t\tnonrot = bdev_nonrot(rdev->bdev);\n\t\thas_nonrot_disk |= nonrot;\n\t\tpending = atomic_read(&rdev->nr_pending);\n\t\tdist = abs(this_sector - conf->mirrors[disk].head_position);\n\t\tif (choose_first) {\n\t\t\tbest_disk = disk;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (conf->mirrors[disk].next_seq_sect == this_sector\n\t\t    || dist == 0) {\n\t\t\tint opt_iosize = bdev_io_opt(rdev->bdev) >> 9;\n\t\t\tstruct raid1_info *mirror = &conf->mirrors[disk];\n\n\t\t\tbest_disk = disk;\n\t\t\t \n\t\t\tif (nonrot && opt_iosize > 0 &&\n\t\t\t    mirror->seq_start != MaxSector &&\n\t\t\t    mirror->next_seq_sect > opt_iosize &&\n\t\t\t    mirror->next_seq_sect - opt_iosize >=\n\t\t\t    mirror->seq_start) {\n\t\t\t\tchoose_next_idle = 1;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tif (choose_next_idle)\n\t\t\tcontinue;\n\n\t\tif (min_pending > pending) {\n\t\t\tmin_pending = pending;\n\t\t\tbest_pending_disk = disk;\n\t\t}\n\n\t\tif (dist < best_dist) {\n\t\t\tbest_dist = dist;\n\t\t\tbest_dist_disk = disk;\n\t\t}\n\t}\n\n\t \n\tif (best_disk == -1) {\n\t\tif (has_nonrot_disk || min_pending == 0)\n\t\t\tbest_disk = best_pending_disk;\n\t\telse\n\t\t\tbest_disk = best_dist_disk;\n\t}\n\n\tif (best_disk >= 0) {\n\t\trdev = rcu_dereference(conf->mirrors[best_disk].rdev);\n\t\tif (!rdev)\n\t\t\tgoto retry;\n\t\tatomic_inc(&rdev->nr_pending);\n\t\tsectors = best_good_sectors;\n\n\t\tif (conf->mirrors[best_disk].next_seq_sect != this_sector)\n\t\t\tconf->mirrors[best_disk].seq_start = this_sector;\n\n\t\tconf->mirrors[best_disk].next_seq_sect = this_sector + sectors;\n\t}\n\trcu_read_unlock();\n\t*max_sectors = sectors;\n\n\treturn best_disk;\n}\n\nstatic void wake_up_barrier(struct r1conf *conf)\n{\n\tif (wq_has_sleeper(&conf->wait_barrier))\n\t\twake_up(&conf->wait_barrier);\n}\n\nstatic void flush_bio_list(struct r1conf *conf, struct bio *bio)\n{\n\t \n\traid1_prepare_flush_writes(conf->mddev->bitmap);\n\twake_up_barrier(conf);\n\n\twhile (bio) {  \n\t\tstruct bio *next = bio->bi_next;\n\n\t\traid1_submit_write(bio);\n\t\tbio = next;\n\t\tcond_resched();\n\t}\n}\n\nstatic void flush_pending_writes(struct r1conf *conf)\n{\n\t \n\tspin_lock_irq(&conf->device_lock);\n\n\tif (conf->pending_bio_list.head) {\n\t\tstruct blk_plug plug;\n\t\tstruct bio *bio;\n\n\t\tbio = bio_list_get(&conf->pending_bio_list);\n\t\tspin_unlock_irq(&conf->device_lock);\n\n\t\t \n\t\t__set_current_state(TASK_RUNNING);\n\t\tblk_start_plug(&plug);\n\t\tflush_bio_list(conf, bio);\n\t\tblk_finish_plug(&plug);\n\t} else\n\t\tspin_unlock_irq(&conf->device_lock);\n}\n\n \nstatic int raise_barrier(struct r1conf *conf, sector_t sector_nr)\n{\n\tint idx = sector_to_idx(sector_nr);\n\n\tspin_lock_irq(&conf->resync_lock);\n\n\t \n\twait_event_lock_irq(conf->wait_barrier,\n\t\t\t    !atomic_read(&conf->nr_waiting[idx]),\n\t\t\t    conf->resync_lock);\n\n\t \n\tatomic_inc(&conf->barrier[idx]);\n\t \n\tsmp_mb__after_atomic();\n\n\t \n\twait_event_lock_irq(conf->wait_barrier,\n\t\t\t    (!conf->array_frozen &&\n\t\t\t     !atomic_read(&conf->nr_pending[idx]) &&\n\t\t\t     atomic_read(&conf->barrier[idx]) < RESYNC_DEPTH) ||\n\t\t\t\ttest_bit(MD_RECOVERY_INTR, &conf->mddev->recovery),\n\t\t\t    conf->resync_lock);\n\n\tif (test_bit(MD_RECOVERY_INTR, &conf->mddev->recovery)) {\n\t\tatomic_dec(&conf->barrier[idx]);\n\t\tspin_unlock_irq(&conf->resync_lock);\n\t\twake_up(&conf->wait_barrier);\n\t\treturn -EINTR;\n\t}\n\n\tatomic_inc(&conf->nr_sync_pending);\n\tspin_unlock_irq(&conf->resync_lock);\n\n\treturn 0;\n}\n\nstatic void lower_barrier(struct r1conf *conf, sector_t sector_nr)\n{\n\tint idx = sector_to_idx(sector_nr);\n\n\tBUG_ON(atomic_read(&conf->barrier[idx]) <= 0);\n\n\tatomic_dec(&conf->barrier[idx]);\n\tatomic_dec(&conf->nr_sync_pending);\n\twake_up(&conf->wait_barrier);\n}\n\nstatic bool _wait_barrier(struct r1conf *conf, int idx, bool nowait)\n{\n\tbool ret = true;\n\n\t \n\tatomic_inc(&conf->nr_pending[idx]);\n\t \n\tsmp_mb__after_atomic();\n\n\t \n\tif (!READ_ONCE(conf->array_frozen) &&\n\t    !atomic_read(&conf->barrier[idx]))\n\t\treturn ret;\n\n\t \n\tspin_lock_irq(&conf->resync_lock);\n\tatomic_inc(&conf->nr_waiting[idx]);\n\tatomic_dec(&conf->nr_pending[idx]);\n\t \n\twake_up_barrier(conf);\n\t \n\n\t \n\tif (nowait) {\n\t\tret = false;\n\t} else {\n\t\twait_event_lock_irq(conf->wait_barrier,\n\t\t\t\t!conf->array_frozen &&\n\t\t\t\t!atomic_read(&conf->barrier[idx]),\n\t\t\t\tconf->resync_lock);\n\t\tatomic_inc(&conf->nr_pending[idx]);\n\t}\n\n\tatomic_dec(&conf->nr_waiting[idx]);\n\tspin_unlock_irq(&conf->resync_lock);\n\treturn ret;\n}\n\nstatic bool wait_read_barrier(struct r1conf *conf, sector_t sector_nr, bool nowait)\n{\n\tint idx = sector_to_idx(sector_nr);\n\tbool ret = true;\n\n\t \n\tatomic_inc(&conf->nr_pending[idx]);\n\n\tif (!READ_ONCE(conf->array_frozen))\n\t\treturn ret;\n\n\tspin_lock_irq(&conf->resync_lock);\n\tatomic_inc(&conf->nr_waiting[idx]);\n\tatomic_dec(&conf->nr_pending[idx]);\n\t \n\twake_up_barrier(conf);\n\t \n\n\t \n\tif (nowait) {\n\t\t \n\t\tret = false;\n\t} else {\n\t\twait_event_lock_irq(conf->wait_barrier,\n\t\t\t\t!conf->array_frozen,\n\t\t\t\tconf->resync_lock);\n\t\tatomic_inc(&conf->nr_pending[idx]);\n\t}\n\n\tatomic_dec(&conf->nr_waiting[idx]);\n\tspin_unlock_irq(&conf->resync_lock);\n\treturn ret;\n}\n\nstatic bool wait_barrier(struct r1conf *conf, sector_t sector_nr, bool nowait)\n{\n\tint idx = sector_to_idx(sector_nr);\n\n\treturn _wait_barrier(conf, idx, nowait);\n}\n\nstatic void _allow_barrier(struct r1conf *conf, int idx)\n{\n\tatomic_dec(&conf->nr_pending[idx]);\n\twake_up_barrier(conf);\n}\n\nstatic void allow_barrier(struct r1conf *conf, sector_t sector_nr)\n{\n\tint idx = sector_to_idx(sector_nr);\n\n\t_allow_barrier(conf, idx);\n}\n\n \nstatic int get_unqueued_pending(struct r1conf *conf)\n{\n\tint idx, ret;\n\n\tret = atomic_read(&conf->nr_sync_pending);\n\tfor (idx = 0; idx < BARRIER_BUCKETS_NR; idx++)\n\t\tret += atomic_read(&conf->nr_pending[idx]) -\n\t\t\tatomic_read(&conf->nr_queued[idx]);\n\n\treturn ret;\n}\n\nstatic void freeze_array(struct r1conf *conf, int extra)\n{\n\t \n\tspin_lock_irq(&conf->resync_lock);\n\tconf->array_frozen = 1;\n\traid1_log(conf->mddev, \"wait freeze\");\n\twait_event_lock_irq_cmd(\n\t\tconf->wait_barrier,\n\t\tget_unqueued_pending(conf) == extra,\n\t\tconf->resync_lock,\n\t\tflush_pending_writes(conf));\n\tspin_unlock_irq(&conf->resync_lock);\n}\nstatic void unfreeze_array(struct r1conf *conf)\n{\n\t \n\tspin_lock_irq(&conf->resync_lock);\n\tconf->array_frozen = 0;\n\tspin_unlock_irq(&conf->resync_lock);\n\twake_up(&conf->wait_barrier);\n}\n\nstatic void alloc_behind_master_bio(struct r1bio *r1_bio,\n\t\t\t\t\t   struct bio *bio)\n{\n\tint size = bio->bi_iter.bi_size;\n\tunsigned vcnt = (size + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tint i = 0;\n\tstruct bio *behind_bio = NULL;\n\n\tbehind_bio = bio_alloc_bioset(NULL, vcnt, 0, GFP_NOIO,\n\t\t\t\t      &r1_bio->mddev->bio_set);\n\tif (!behind_bio)\n\t\treturn;\n\n\t \n\tif (!bio_has_data(bio)) {\n\t\tbehind_bio->bi_iter.bi_size = size;\n\t\tgoto skip_copy;\n\t}\n\n\twhile (i < vcnt && size) {\n\t\tstruct page *page;\n\t\tint len = min_t(int, PAGE_SIZE, size);\n\n\t\tpage = alloc_page(GFP_NOIO);\n\t\tif (unlikely(!page))\n\t\t\tgoto free_pages;\n\n\t\tif (!bio_add_page(behind_bio, page, len, 0)) {\n\t\t\tput_page(page);\n\t\t\tgoto free_pages;\n\t\t}\n\n\t\tsize -= len;\n\t\ti++;\n\t}\n\n\tbio_copy_data(behind_bio, bio);\nskip_copy:\n\tr1_bio->behind_master_bio = behind_bio;\n\tset_bit(R1BIO_BehindIO, &r1_bio->state);\n\n\treturn;\n\nfree_pages:\n\tpr_debug(\"%dB behind alloc failed, doing sync I/O\\n\",\n\t\t bio->bi_iter.bi_size);\n\tbio_free_pages(behind_bio);\n\tbio_put(behind_bio);\n}\n\nstatic void raid1_unplug(struct blk_plug_cb *cb, bool from_schedule)\n{\n\tstruct raid1_plug_cb *plug = container_of(cb, struct raid1_plug_cb,\n\t\t\t\t\t\t  cb);\n\tstruct mddev *mddev = plug->cb.data;\n\tstruct r1conf *conf = mddev->private;\n\tstruct bio *bio;\n\n\tif (from_schedule) {\n\t\tspin_lock_irq(&conf->device_lock);\n\t\tbio_list_merge(&conf->pending_bio_list, &plug->pending);\n\t\tspin_unlock_irq(&conf->device_lock);\n\t\twake_up_barrier(conf);\n\t\tmd_wakeup_thread(mddev->thread);\n\t\tkfree(plug);\n\t\treturn;\n\t}\n\n\t \n\tbio = bio_list_get(&plug->pending);\n\tflush_bio_list(conf, bio);\n\tkfree(plug);\n}\n\nstatic void init_r1bio(struct r1bio *r1_bio, struct mddev *mddev, struct bio *bio)\n{\n\tr1_bio->master_bio = bio;\n\tr1_bio->sectors = bio_sectors(bio);\n\tr1_bio->state = 0;\n\tr1_bio->mddev = mddev;\n\tr1_bio->sector = bio->bi_iter.bi_sector;\n}\n\nstatic inline struct r1bio *\nalloc_r1bio(struct mddev *mddev, struct bio *bio)\n{\n\tstruct r1conf *conf = mddev->private;\n\tstruct r1bio *r1_bio;\n\n\tr1_bio = mempool_alloc(&conf->r1bio_pool, GFP_NOIO);\n\t \n\tmemset(r1_bio->bios, 0, conf->raid_disks * sizeof(r1_bio->bios[0]));\n\tinit_r1bio(r1_bio, mddev, bio);\n\treturn r1_bio;\n}\n\nstatic void raid1_read_request(struct mddev *mddev, struct bio *bio,\n\t\t\t       int max_read_sectors, struct r1bio *r1_bio)\n{\n\tstruct r1conf *conf = mddev->private;\n\tstruct raid1_info *mirror;\n\tstruct bio *read_bio;\n\tstruct bitmap *bitmap = mddev->bitmap;\n\tconst enum req_op op = bio_op(bio);\n\tconst blk_opf_t do_sync = bio->bi_opf & REQ_SYNC;\n\tint max_sectors;\n\tint rdisk;\n\tbool r1bio_existed = !!r1_bio;\n\tchar b[BDEVNAME_SIZE];\n\n\t \n\tgfp_t gfp = r1_bio ? (GFP_NOIO | __GFP_HIGH) : GFP_NOIO;\n\n\tif (r1bio_existed) {\n\t\t \n\t\tstruct md_rdev *rdev;\n\t\trcu_read_lock();\n\t\trdev = rcu_dereference(conf->mirrors[r1_bio->read_disk].rdev);\n\t\tif (rdev)\n\t\t\tsnprintf(b, sizeof(b), \"%pg\", rdev->bdev);\n\t\telse\n\t\t\tstrcpy(b, \"???\");\n\t\trcu_read_unlock();\n\t}\n\n\t \n\tif (!wait_read_barrier(conf, bio->bi_iter.bi_sector,\n\t\t\t\tbio->bi_opf & REQ_NOWAIT)) {\n\t\tbio_wouldblock_error(bio);\n\t\treturn;\n\t}\n\n\tif (!r1_bio)\n\t\tr1_bio = alloc_r1bio(mddev, bio);\n\telse\n\t\tinit_r1bio(r1_bio, mddev, bio);\n\tr1_bio->sectors = max_read_sectors;\n\n\t \n\trdisk = read_balance(conf, r1_bio, &max_sectors);\n\n\tif (rdisk < 0) {\n\t\t \n\t\tif (r1bio_existed) {\n\t\t\tpr_crit_ratelimited(\"md/raid1:%s: %s: unrecoverable I/O read error for block %llu\\n\",\n\t\t\t\t\t    mdname(mddev),\n\t\t\t\t\t    b,\n\t\t\t\t\t    (unsigned long long)r1_bio->sector);\n\t\t}\n\t\traid_end_bio_io(r1_bio);\n\t\treturn;\n\t}\n\tmirror = conf->mirrors + rdisk;\n\n\tif (r1bio_existed)\n\t\tpr_info_ratelimited(\"md/raid1:%s: redirecting sector %llu to other mirror: %pg\\n\",\n\t\t\t\t    mdname(mddev),\n\t\t\t\t    (unsigned long long)r1_bio->sector,\n\t\t\t\t    mirror->rdev->bdev);\n\n\tif (test_bit(WriteMostly, &mirror->rdev->flags) &&\n\t    bitmap) {\n\t\t \n\t\traid1_log(mddev, \"wait behind writes\");\n\t\twait_event(bitmap->behind_wait,\n\t\t\t   atomic_read(&bitmap->behind_writes) == 0);\n\t}\n\n\tif (max_sectors < bio_sectors(bio)) {\n\t\tstruct bio *split = bio_split(bio, max_sectors,\n\t\t\t\t\t      gfp, &conf->bio_split);\n\t\tbio_chain(split, bio);\n\t\tsubmit_bio_noacct(bio);\n\t\tbio = split;\n\t\tr1_bio->master_bio = bio;\n\t\tr1_bio->sectors = max_sectors;\n\t}\n\n\tr1_bio->read_disk = rdisk;\n\tif (!r1bio_existed) {\n\t\tmd_account_bio(mddev, &bio);\n\t\tr1_bio->master_bio = bio;\n\t}\n\tread_bio = bio_alloc_clone(mirror->rdev->bdev, bio, gfp,\n\t\t\t\t   &mddev->bio_set);\n\n\tr1_bio->bios[rdisk] = read_bio;\n\n\tread_bio->bi_iter.bi_sector = r1_bio->sector +\n\t\tmirror->rdev->data_offset;\n\tread_bio->bi_end_io = raid1_end_read_request;\n\tread_bio->bi_opf = op | do_sync;\n\tif (test_bit(FailFast, &mirror->rdev->flags) &&\n\t    test_bit(R1BIO_FailFast, &r1_bio->state))\n\t        read_bio->bi_opf |= MD_FAILFAST;\n\tread_bio->bi_private = r1_bio;\n\n\tif (mddev->gendisk)\n\t        trace_block_bio_remap(read_bio, disk_devt(mddev->gendisk),\n\t\t\t\t      r1_bio->sector);\n\n\tsubmit_bio_noacct(read_bio);\n}\n\nstatic void raid1_write_request(struct mddev *mddev, struct bio *bio,\n\t\t\t\tint max_write_sectors)\n{\n\tstruct r1conf *conf = mddev->private;\n\tstruct r1bio *r1_bio;\n\tint i, disks;\n\tstruct bitmap *bitmap = mddev->bitmap;\n\tunsigned long flags;\n\tstruct md_rdev *blocked_rdev;\n\tint first_clone;\n\tint max_sectors;\n\tbool write_behind = false;\n\n\tif (mddev_is_clustered(mddev) &&\n\t     md_cluster_ops->area_resyncing(mddev, WRITE,\n\t\t     bio->bi_iter.bi_sector, bio_end_sector(bio))) {\n\n\t\tDEFINE_WAIT(w);\n\t\tif (bio->bi_opf & REQ_NOWAIT) {\n\t\t\tbio_wouldblock_error(bio);\n\t\t\treturn;\n\t\t}\n\t\tfor (;;) {\n\t\t\tprepare_to_wait(&conf->wait_barrier,\n\t\t\t\t\t&w, TASK_IDLE);\n\t\t\tif (!md_cluster_ops->area_resyncing(mddev, WRITE,\n\t\t\t\t\t\t\tbio->bi_iter.bi_sector,\n\t\t\t\t\t\t\tbio_end_sector(bio)))\n\t\t\t\tbreak;\n\t\t\tschedule();\n\t\t}\n\t\tfinish_wait(&conf->wait_barrier, &w);\n\t}\n\n\t \n\tif (!wait_barrier(conf, bio->bi_iter.bi_sector,\n\t\t\t\tbio->bi_opf & REQ_NOWAIT)) {\n\t\tbio_wouldblock_error(bio);\n\t\treturn;\n\t}\n\n retry_write:\n\tr1_bio = alloc_r1bio(mddev, bio);\n\tr1_bio->sectors = max_write_sectors;\n\n\t \n\n\tdisks = conf->raid_disks * 2;\n\tblocked_rdev = NULL;\n\trcu_read_lock();\n\tmax_sectors = r1_bio->sectors;\n\tfor (i = 0;  i < disks; i++) {\n\t\tstruct md_rdev *rdev = rcu_dereference(conf->mirrors[i].rdev);\n\n\t\t \n\t\tif (rdev && test_bit(WriteMostly, &rdev->flags))\n\t\t\twrite_behind = true;\n\n\t\tif (rdev && unlikely(test_bit(Blocked, &rdev->flags))) {\n\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\tblocked_rdev = rdev;\n\t\t\tbreak;\n\t\t}\n\t\tr1_bio->bios[i] = NULL;\n\t\tif (!rdev || test_bit(Faulty, &rdev->flags)) {\n\t\t\tif (i < conf->raid_disks)\n\t\t\t\tset_bit(R1BIO_Degraded, &r1_bio->state);\n\t\t\tcontinue;\n\t\t}\n\n\t\tatomic_inc(&rdev->nr_pending);\n\t\tif (test_bit(WriteErrorSeen, &rdev->flags)) {\n\t\t\tsector_t first_bad;\n\t\t\tint bad_sectors;\n\t\t\tint is_bad;\n\n\t\t\tis_bad = is_badblock(rdev, r1_bio->sector, max_sectors,\n\t\t\t\t\t     &first_bad, &bad_sectors);\n\t\t\tif (is_bad < 0) {\n\t\t\t\t \n\t\t\t\tset_bit(BlockedBadBlocks, &rdev->flags);\n\t\t\t\tblocked_rdev = rdev;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (is_bad && first_bad <= r1_bio->sector) {\n\t\t\t\t \n\t\t\t\tbad_sectors -= (r1_bio->sector - first_bad);\n\t\t\t\tif (bad_sectors < max_sectors)\n\t\t\t\t\t \n\t\t\t\t\tmax_sectors = bad_sectors;\n\t\t\t\trdev_dec_pending(rdev, mddev);\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (is_bad) {\n\t\t\t\tint good_sectors = first_bad - r1_bio->sector;\n\t\t\t\tif (good_sectors < max_sectors)\n\t\t\t\t\tmax_sectors = good_sectors;\n\t\t\t}\n\t\t}\n\t\tr1_bio->bios[i] = bio;\n\t}\n\trcu_read_unlock();\n\n\tif (unlikely(blocked_rdev)) {\n\t\t \n\t\tint j;\n\n\t\tfor (j = 0; j < i; j++)\n\t\t\tif (r1_bio->bios[j])\n\t\t\t\trdev_dec_pending(conf->mirrors[j].rdev, mddev);\n\t\tfree_r1bio(r1_bio);\n\t\tallow_barrier(conf, bio->bi_iter.bi_sector);\n\n\t\tif (bio->bi_opf & REQ_NOWAIT) {\n\t\t\tbio_wouldblock_error(bio);\n\t\t\treturn;\n\t\t}\n\t\traid1_log(mddev, \"wait rdev %d blocked\", blocked_rdev->raid_disk);\n\t\tmd_wait_for_blocked_rdev(blocked_rdev, mddev);\n\t\twait_barrier(conf, bio->bi_iter.bi_sector, false);\n\t\tgoto retry_write;\n\t}\n\n\t \n\tif (write_behind && bitmap)\n\t\tmax_sectors = min_t(int, max_sectors,\n\t\t\t\t    BIO_MAX_VECS * (PAGE_SIZE >> 9));\n\tif (max_sectors < bio_sectors(bio)) {\n\t\tstruct bio *split = bio_split(bio, max_sectors,\n\t\t\t\t\t      GFP_NOIO, &conf->bio_split);\n\t\tbio_chain(split, bio);\n\t\tsubmit_bio_noacct(bio);\n\t\tbio = split;\n\t\tr1_bio->master_bio = bio;\n\t\tr1_bio->sectors = max_sectors;\n\t}\n\n\tmd_account_bio(mddev, &bio);\n\tr1_bio->master_bio = bio;\n\tatomic_set(&r1_bio->remaining, 1);\n\tatomic_set(&r1_bio->behind_remaining, 0);\n\n\tfirst_clone = 1;\n\n\tfor (i = 0; i < disks; i++) {\n\t\tstruct bio *mbio = NULL;\n\t\tstruct md_rdev *rdev = conf->mirrors[i].rdev;\n\t\tif (!r1_bio->bios[i])\n\t\t\tcontinue;\n\n\t\tif (first_clone) {\n\t\t\t \n\t\t\tif (bitmap && write_behind &&\n\t\t\t    (atomic_read(&bitmap->behind_writes)\n\t\t\t     < mddev->bitmap_info.max_write_behind) &&\n\t\t\t    !waitqueue_active(&bitmap->behind_wait)) {\n\t\t\t\talloc_behind_master_bio(r1_bio, bio);\n\t\t\t}\n\n\t\t\tmd_bitmap_startwrite(bitmap, r1_bio->sector, r1_bio->sectors,\n\t\t\t\t\t     test_bit(R1BIO_BehindIO, &r1_bio->state));\n\t\t\tfirst_clone = 0;\n\t\t}\n\n\t\tif (r1_bio->behind_master_bio) {\n\t\t\tmbio = bio_alloc_clone(rdev->bdev,\n\t\t\t\t\t       r1_bio->behind_master_bio,\n\t\t\t\t\t       GFP_NOIO, &mddev->bio_set);\n\t\t\tif (test_bit(CollisionCheck, &rdev->flags))\n\t\t\t\twait_for_serialization(rdev, r1_bio);\n\t\t\tif (test_bit(WriteMostly, &rdev->flags))\n\t\t\t\tatomic_inc(&r1_bio->behind_remaining);\n\t\t} else {\n\t\t\tmbio = bio_alloc_clone(rdev->bdev, bio, GFP_NOIO,\n\t\t\t\t\t       &mddev->bio_set);\n\n\t\t\tif (mddev->serialize_policy)\n\t\t\t\twait_for_serialization(rdev, r1_bio);\n\t\t}\n\n\t\tr1_bio->bios[i] = mbio;\n\n\t\tmbio->bi_iter.bi_sector\t= (r1_bio->sector + rdev->data_offset);\n\t\tmbio->bi_end_io\t= raid1_end_write_request;\n\t\tmbio->bi_opf = bio_op(bio) | (bio->bi_opf & (REQ_SYNC | REQ_FUA));\n\t\tif (test_bit(FailFast, &rdev->flags) &&\n\t\t    !test_bit(WriteMostly, &rdev->flags) &&\n\t\t    conf->raid_disks - mddev->degraded > 1)\n\t\t\tmbio->bi_opf |= MD_FAILFAST;\n\t\tmbio->bi_private = r1_bio;\n\n\t\tatomic_inc(&r1_bio->remaining);\n\n\t\tif (mddev->gendisk)\n\t\t\ttrace_block_bio_remap(mbio, disk_devt(mddev->gendisk),\n\t\t\t\t\t      r1_bio->sector);\n\t\t \n\t\tmbio->bi_bdev = (void *)rdev;\n\t\tif (!raid1_add_bio_to_plug(mddev, mbio, raid1_unplug, disks)) {\n\t\t\tspin_lock_irqsave(&conf->device_lock, flags);\n\t\t\tbio_list_add(&conf->pending_bio_list, mbio);\n\t\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\t\t\tmd_wakeup_thread(mddev->thread);\n\t\t}\n\t}\n\n\tr1_bio_write_done(r1_bio);\n\n\t \n\twake_up_barrier(conf);\n}\n\nstatic bool raid1_make_request(struct mddev *mddev, struct bio *bio)\n{\n\tsector_t sectors;\n\n\tif (unlikely(bio->bi_opf & REQ_PREFLUSH)\n\t    && md_flush_request(mddev, bio))\n\t\treturn true;\n\n\t \n\tsectors = align_to_barrier_unit_end(\n\t\tbio->bi_iter.bi_sector, bio_sectors(bio));\n\n\tif (bio_data_dir(bio) == READ)\n\t\traid1_read_request(mddev, bio, sectors, NULL);\n\telse {\n\t\tif (!md_write_start(mddev,bio))\n\t\t\treturn false;\n\t\traid1_write_request(mddev, bio, sectors);\n\t}\n\treturn true;\n}\n\nstatic void raid1_status(struct seq_file *seq, struct mddev *mddev)\n{\n\tstruct r1conf *conf = mddev->private;\n\tint i;\n\n\tseq_printf(seq, \" [%d/%d] [\", conf->raid_disks,\n\t\t   conf->raid_disks - mddev->degraded);\n\trcu_read_lock();\n\tfor (i = 0; i < conf->raid_disks; i++) {\n\t\tstruct md_rdev *rdev = rcu_dereference(conf->mirrors[i].rdev);\n\t\tseq_printf(seq, \"%s\",\n\t\t\t   rdev && test_bit(In_sync, &rdev->flags) ? \"U\" : \"_\");\n\t}\n\trcu_read_unlock();\n\tseq_printf(seq, \"]\");\n}\n\n \nstatic void raid1_error(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tstruct r1conf *conf = mddev->private;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&conf->device_lock, flags);\n\n\tif (test_bit(In_sync, &rdev->flags) &&\n\t    (conf->raid_disks - mddev->degraded) == 1) {\n\t\tset_bit(MD_BROKEN, &mddev->flags);\n\n\t\tif (!mddev->fail_last_dev) {\n\t\t\tconf->recovery_disabled = mddev->recovery_disabled;\n\t\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\t\t\treturn;\n\t\t}\n\t}\n\tset_bit(Blocked, &rdev->flags);\n\tif (test_and_clear_bit(In_sync, &rdev->flags))\n\t\tmddev->degraded++;\n\tset_bit(Faulty, &rdev->flags);\n\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\t \n\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\tset_mask_bits(&mddev->sb_flags, 0,\n\t\t      BIT(MD_SB_CHANGE_DEVS) | BIT(MD_SB_CHANGE_PENDING));\n\tpr_crit(\"md/raid1:%s: Disk failure on %pg, disabling device.\\n\"\n\t\t\"md/raid1:%s: Operation continuing on %d devices.\\n\",\n\t\tmdname(mddev), rdev->bdev,\n\t\tmdname(mddev), conf->raid_disks - mddev->degraded);\n}\n\nstatic void print_conf(struct r1conf *conf)\n{\n\tint i;\n\n\tpr_debug(\"RAID1 conf printout:\\n\");\n\tif (!conf) {\n\t\tpr_debug(\"(!conf)\\n\");\n\t\treturn;\n\t}\n\tpr_debug(\" --- wd:%d rd:%d\\n\", conf->raid_disks - conf->mddev->degraded,\n\t\t conf->raid_disks);\n\n\trcu_read_lock();\n\tfor (i = 0; i < conf->raid_disks; i++) {\n\t\tstruct md_rdev *rdev = rcu_dereference(conf->mirrors[i].rdev);\n\t\tif (rdev)\n\t\t\tpr_debug(\" disk %d, wo:%d, o:%d, dev:%pg\\n\",\n\t\t\t\t i, !test_bit(In_sync, &rdev->flags),\n\t\t\t\t !test_bit(Faulty, &rdev->flags),\n\t\t\t\t rdev->bdev);\n\t}\n\trcu_read_unlock();\n}\n\nstatic void close_sync(struct r1conf *conf)\n{\n\tint idx;\n\n\tfor (idx = 0; idx < BARRIER_BUCKETS_NR; idx++) {\n\t\t_wait_barrier(conf, idx, false);\n\t\t_allow_barrier(conf, idx);\n\t}\n\n\tmempool_exit(&conf->r1buf_pool);\n}\n\nstatic int raid1_spare_active(struct mddev *mddev)\n{\n\tint i;\n\tstruct r1conf *conf = mddev->private;\n\tint count = 0;\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&conf->device_lock, flags);\n\tfor (i = 0; i < conf->raid_disks; i++) {\n\t\tstruct md_rdev *rdev = conf->mirrors[i].rdev;\n\t\tstruct md_rdev *repl = conf->mirrors[conf->raid_disks + i].rdev;\n\t\tif (repl\n\t\t    && !test_bit(Candidate, &repl->flags)\n\t\t    && repl->recovery_offset == MaxSector\n\t\t    && !test_bit(Faulty, &repl->flags)\n\t\t    && !test_and_set_bit(In_sync, &repl->flags)) {\n\t\t\t \n\t\t\tif (!rdev ||\n\t\t\t    !test_and_clear_bit(In_sync, &rdev->flags))\n\t\t\t\tcount++;\n\t\t\tif (rdev) {\n\t\t\t\t \n\t\t\t\tset_bit(Faulty, &rdev->flags);\n\t\t\t\tsysfs_notify_dirent_safe(\n\t\t\t\t\trdev->sysfs_state);\n\t\t\t}\n\t\t}\n\t\tif (rdev\n\t\t    && rdev->recovery_offset == MaxSector\n\t\t    && !test_bit(Faulty, &rdev->flags)\n\t\t    && !test_and_set_bit(In_sync, &rdev->flags)) {\n\t\t\tcount++;\n\t\t\tsysfs_notify_dirent_safe(rdev->sysfs_state);\n\t\t}\n\t}\n\tmddev->degraded -= count;\n\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\n\tprint_conf(conf);\n\treturn count;\n}\n\nstatic int raid1_add_disk(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tstruct r1conf *conf = mddev->private;\n\tint err = -EEXIST;\n\tint mirror = 0, repl_slot = -1;\n\tstruct raid1_info *p;\n\tint first = 0;\n\tint last = conf->raid_disks - 1;\n\n\tif (mddev->recovery_disabled == conf->recovery_disabled)\n\t\treturn -EBUSY;\n\n\tif (md_integrity_add_rdev(rdev, mddev))\n\t\treturn -ENXIO;\n\n\tif (rdev->raid_disk >= 0)\n\t\tfirst = last = rdev->raid_disk;\n\n\t \n\tif (rdev->saved_raid_disk >= 0 &&\n\t    rdev->saved_raid_disk >= first &&\n\t    rdev->saved_raid_disk < conf->raid_disks &&\n\t    conf->mirrors[rdev->saved_raid_disk].rdev == NULL)\n\t\tfirst = last = rdev->saved_raid_disk;\n\n\tfor (mirror = first; mirror <= last; mirror++) {\n\t\tp = conf->mirrors + mirror;\n\t\tif (!p->rdev) {\n\t\t\tif (mddev->gendisk)\n\t\t\t\tdisk_stack_limits(mddev->gendisk, rdev->bdev,\n\t\t\t\t\t\t  rdev->data_offset << 9);\n\n\t\t\tp->head_position = 0;\n\t\t\trdev->raid_disk = mirror;\n\t\t\terr = 0;\n\t\t\t \n\t\t\tif (rdev->saved_raid_disk < 0)\n\t\t\t\tconf->fullsync = 1;\n\t\t\trcu_assign_pointer(p->rdev, rdev);\n\t\t\tbreak;\n\t\t}\n\t\tif (test_bit(WantReplacement, &p->rdev->flags) &&\n\t\t    p[conf->raid_disks].rdev == NULL && repl_slot < 0)\n\t\t\trepl_slot = mirror;\n\t}\n\n\tif (err && repl_slot >= 0) {\n\t\t \n\t\tp = conf->mirrors + repl_slot;\n\t\tclear_bit(In_sync, &rdev->flags);\n\t\tset_bit(Replacement, &rdev->flags);\n\t\trdev->raid_disk = repl_slot;\n\t\terr = 0;\n\t\tconf->fullsync = 1;\n\t\trcu_assign_pointer(p[conf->raid_disks].rdev, rdev);\n\t}\n\n\tprint_conf(conf);\n\treturn err;\n}\n\nstatic int raid1_remove_disk(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tstruct r1conf *conf = mddev->private;\n\tint err = 0;\n\tint number = rdev->raid_disk;\n\tstruct raid1_info *p = conf->mirrors + number;\n\n\tif (unlikely(number >= conf->raid_disks))\n\t\tgoto abort;\n\n\tif (rdev != p->rdev)\n\t\tp = conf->mirrors + conf->raid_disks + number;\n\n\tprint_conf(conf);\n\tif (rdev == p->rdev) {\n\t\tif (test_bit(In_sync, &rdev->flags) ||\n\t\t    atomic_read(&rdev->nr_pending)) {\n\t\t\terr = -EBUSY;\n\t\t\tgoto abort;\n\t\t}\n\t\t \n\t\tif (!test_bit(Faulty, &rdev->flags) &&\n\t\t    mddev->recovery_disabled != conf->recovery_disabled &&\n\t\t    mddev->degraded < conf->raid_disks) {\n\t\t\terr = -EBUSY;\n\t\t\tgoto abort;\n\t\t}\n\t\tp->rdev = NULL;\n\t\tif (!test_bit(RemoveSynchronized, &rdev->flags)) {\n\t\t\tsynchronize_rcu();\n\t\t\tif (atomic_read(&rdev->nr_pending)) {\n\t\t\t\t \n\t\t\t\terr = -EBUSY;\n\t\t\t\tp->rdev = rdev;\n\t\t\t\tgoto abort;\n\t\t\t}\n\t\t}\n\t\tif (conf->mirrors[conf->raid_disks + number].rdev) {\n\t\t\t \n\t\t\tstruct md_rdev *repl =\n\t\t\t\tconf->mirrors[conf->raid_disks + number].rdev;\n\t\t\tfreeze_array(conf, 0);\n\t\t\tif (atomic_read(&repl->nr_pending)) {\n\t\t\t\t \n\t\t\t\terr = -EBUSY;\n\t\t\t\tunfreeze_array(conf);\n\t\t\t\tgoto abort;\n\t\t\t}\n\t\t\tclear_bit(Replacement, &repl->flags);\n\t\t\tp->rdev = repl;\n\t\t\tconf->mirrors[conf->raid_disks + number].rdev = NULL;\n\t\t\tunfreeze_array(conf);\n\t\t}\n\n\t\tclear_bit(WantReplacement, &rdev->flags);\n\t\terr = md_integrity_register(mddev);\n\t}\nabort:\n\n\tprint_conf(conf);\n\treturn err;\n}\n\nstatic void end_sync_read(struct bio *bio)\n{\n\tstruct r1bio *r1_bio = get_resync_r1bio(bio);\n\n\tupdate_head_pos(r1_bio->read_disk, r1_bio);\n\n\t \n\tif (!bio->bi_status)\n\t\tset_bit(R1BIO_Uptodate, &r1_bio->state);\n\n\tif (atomic_dec_and_test(&r1_bio->remaining))\n\t\treschedule_retry(r1_bio);\n}\n\nstatic void abort_sync_write(struct mddev *mddev, struct r1bio *r1_bio)\n{\n\tsector_t sync_blocks = 0;\n\tsector_t s = r1_bio->sector;\n\tlong sectors_to_go = r1_bio->sectors;\n\n\t \n\tdo {\n\t\tmd_bitmap_end_sync(mddev->bitmap, s, &sync_blocks, 1);\n\t\ts += sync_blocks;\n\t\tsectors_to_go -= sync_blocks;\n\t} while (sectors_to_go > 0);\n}\n\nstatic void put_sync_write_buf(struct r1bio *r1_bio, int uptodate)\n{\n\tif (atomic_dec_and_test(&r1_bio->remaining)) {\n\t\tstruct mddev *mddev = r1_bio->mddev;\n\t\tint s = r1_bio->sectors;\n\n\t\tif (test_bit(R1BIO_MadeGood, &r1_bio->state) ||\n\t\t    test_bit(R1BIO_WriteError, &r1_bio->state))\n\t\t\treschedule_retry(r1_bio);\n\t\telse {\n\t\t\tput_buf(r1_bio);\n\t\t\tmd_done_sync(mddev, s, uptodate);\n\t\t}\n\t}\n}\n\nstatic void end_sync_write(struct bio *bio)\n{\n\tint uptodate = !bio->bi_status;\n\tstruct r1bio *r1_bio = get_resync_r1bio(bio);\n\tstruct mddev *mddev = r1_bio->mddev;\n\tstruct r1conf *conf = mddev->private;\n\tsector_t first_bad;\n\tint bad_sectors;\n\tstruct md_rdev *rdev = conf->mirrors[find_bio_disk(r1_bio, bio)].rdev;\n\n\tif (!uptodate) {\n\t\tabort_sync_write(mddev, r1_bio);\n\t\tset_bit(WriteErrorSeen, &rdev->flags);\n\t\tif (!test_and_set_bit(WantReplacement, &rdev->flags))\n\t\t\tset_bit(MD_RECOVERY_NEEDED, &\n\t\t\t\tmddev->recovery);\n\t\tset_bit(R1BIO_WriteError, &r1_bio->state);\n\t} else if (is_badblock(rdev, r1_bio->sector, r1_bio->sectors,\n\t\t\t       &first_bad, &bad_sectors) &&\n\t\t   !is_badblock(conf->mirrors[r1_bio->read_disk].rdev,\n\t\t\t\tr1_bio->sector,\n\t\t\t\tr1_bio->sectors,\n\t\t\t\t&first_bad, &bad_sectors)\n\t\t)\n\t\tset_bit(R1BIO_MadeGood, &r1_bio->state);\n\n\tput_sync_write_buf(r1_bio, uptodate);\n}\n\nstatic int r1_sync_page_io(struct md_rdev *rdev, sector_t sector,\n\t\t\t   int sectors, struct page *page, blk_opf_t rw)\n{\n\tif (sync_page_io(rdev, sector, sectors << 9, page, rw, false))\n\t\t \n\t\treturn 1;\n\tif (rw == REQ_OP_WRITE) {\n\t\tset_bit(WriteErrorSeen, &rdev->flags);\n\t\tif (!test_and_set_bit(WantReplacement,\n\t\t\t\t      &rdev->flags))\n\t\t\tset_bit(MD_RECOVERY_NEEDED, &\n\t\t\t\trdev->mddev->recovery);\n\t}\n\t \n\tif (!rdev_set_badblocks(rdev, sector, sectors, 0))\n\t\tmd_error(rdev->mddev, rdev);\n\treturn 0;\n}\n\nstatic int fix_sync_read_error(struct r1bio *r1_bio)\n{\n\t \n\tstruct mddev *mddev = r1_bio->mddev;\n\tstruct r1conf *conf = mddev->private;\n\tstruct bio *bio = r1_bio->bios[r1_bio->read_disk];\n\tstruct page **pages = get_resync_pages(bio)->pages;\n\tsector_t sect = r1_bio->sector;\n\tint sectors = r1_bio->sectors;\n\tint idx = 0;\n\tstruct md_rdev *rdev;\n\n\trdev = conf->mirrors[r1_bio->read_disk].rdev;\n\tif (test_bit(FailFast, &rdev->flags)) {\n\t\t \n\t\tmd_error(mddev, rdev);\n\t\tif (test_bit(Faulty, &rdev->flags))\n\t\t\t \n\t\t\tbio->bi_end_io = end_sync_write;\n\t}\n\n\twhile(sectors) {\n\t\tint s = sectors;\n\t\tint d = r1_bio->read_disk;\n\t\tint success = 0;\n\t\tint start;\n\n\t\tif (s > (PAGE_SIZE>>9))\n\t\t\ts = PAGE_SIZE >> 9;\n\t\tdo {\n\t\t\tif (r1_bio->bios[d]->bi_end_io == end_sync_read) {\n\t\t\t\t \n\t\t\t\trdev = conf->mirrors[d].rdev;\n\t\t\t\tif (sync_page_io(rdev, sect, s<<9,\n\t\t\t\t\t\t pages[idx],\n\t\t\t\t\t\t REQ_OP_READ, false)) {\n\t\t\t\t\tsuccess = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\td++;\n\t\t\tif (d == conf->raid_disks * 2)\n\t\t\t\td = 0;\n\t\t} while (!success && d != r1_bio->read_disk);\n\n\t\tif (!success) {\n\t\t\tint abort = 0;\n\t\t\t \n\t\t\tpr_crit_ratelimited(\"md/raid1:%s: %pg: unrecoverable I/O read error for block %llu\\n\",\n\t\t\t\t\t    mdname(mddev), bio->bi_bdev,\n\t\t\t\t\t    (unsigned long long)r1_bio->sector);\n\t\t\tfor (d = 0; d < conf->raid_disks * 2; d++) {\n\t\t\t\trdev = conf->mirrors[d].rdev;\n\t\t\t\tif (!rdev || test_bit(Faulty, &rdev->flags))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (!rdev_set_badblocks(rdev, sect, s, 0))\n\t\t\t\t\tabort = 1;\n\t\t\t}\n\t\t\tif (abort) {\n\t\t\t\tconf->recovery_disabled =\n\t\t\t\t\tmddev->recovery_disabled;\n\t\t\t\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\t\t\t\tmd_done_sync(mddev, r1_bio->sectors, 0);\n\t\t\t\tput_buf(r1_bio);\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\t \n\t\t\tsectors -= s;\n\t\t\tsect += s;\n\t\t\tidx++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tstart = d;\n\t\t \n\t\twhile (d != r1_bio->read_disk) {\n\t\t\tif (d == 0)\n\t\t\t\td = conf->raid_disks * 2;\n\t\t\td--;\n\t\t\tif (r1_bio->bios[d]->bi_end_io != end_sync_read)\n\t\t\t\tcontinue;\n\t\t\trdev = conf->mirrors[d].rdev;\n\t\t\tif (r1_sync_page_io(rdev, sect, s,\n\t\t\t\t\t    pages[idx],\n\t\t\t\t\t    REQ_OP_WRITE) == 0) {\n\t\t\t\tr1_bio->bios[d]->bi_end_io = NULL;\n\t\t\t\trdev_dec_pending(rdev, mddev);\n\t\t\t}\n\t\t}\n\t\td = start;\n\t\twhile (d != r1_bio->read_disk) {\n\t\t\tif (d == 0)\n\t\t\t\td = conf->raid_disks * 2;\n\t\t\td--;\n\t\t\tif (r1_bio->bios[d]->bi_end_io != end_sync_read)\n\t\t\t\tcontinue;\n\t\t\trdev = conf->mirrors[d].rdev;\n\t\t\tif (r1_sync_page_io(rdev, sect, s,\n\t\t\t\t\t    pages[idx],\n\t\t\t\t\t    REQ_OP_READ) != 0)\n\t\t\t\tatomic_add(s, &rdev->corrected_errors);\n\t\t}\n\t\tsectors -= s;\n\t\tsect += s;\n\t\tidx ++;\n\t}\n\tset_bit(R1BIO_Uptodate, &r1_bio->state);\n\tbio->bi_status = 0;\n\treturn 1;\n}\n\nstatic void process_checks(struct r1bio *r1_bio)\n{\n\t \n\tstruct mddev *mddev = r1_bio->mddev;\n\tstruct r1conf *conf = mddev->private;\n\tint primary;\n\tint i;\n\tint vcnt;\n\n\t \n\tvcnt = (r1_bio->sectors + PAGE_SIZE / 512 - 1) >> (PAGE_SHIFT - 9);\n\tfor (i = 0; i < conf->raid_disks * 2; i++) {\n\t\tblk_status_t status;\n\t\tstruct bio *b = r1_bio->bios[i];\n\t\tstruct resync_pages *rp = get_resync_pages(b);\n\t\tif (b->bi_end_io != end_sync_read)\n\t\t\tcontinue;\n\t\t \n\t\tstatus = b->bi_status;\n\t\tbio_reset(b, conf->mirrors[i].rdev->bdev, REQ_OP_READ);\n\t\tb->bi_status = status;\n\t\tb->bi_iter.bi_sector = r1_bio->sector +\n\t\t\tconf->mirrors[i].rdev->data_offset;\n\t\tb->bi_end_io = end_sync_read;\n\t\trp->raid_bio = r1_bio;\n\t\tb->bi_private = rp;\n\n\t\t \n\t\tmd_bio_reset_resync_pages(b, rp, r1_bio->sectors << 9);\n\t}\n\tfor (primary = 0; primary < conf->raid_disks * 2; primary++)\n\t\tif (r1_bio->bios[primary]->bi_end_io == end_sync_read &&\n\t\t    !r1_bio->bios[primary]->bi_status) {\n\t\t\tr1_bio->bios[primary]->bi_end_io = NULL;\n\t\t\trdev_dec_pending(conf->mirrors[primary].rdev, mddev);\n\t\t\tbreak;\n\t\t}\n\tr1_bio->read_disk = primary;\n\tfor (i = 0; i < conf->raid_disks * 2; i++) {\n\t\tint j = 0;\n\t\tstruct bio *pbio = r1_bio->bios[primary];\n\t\tstruct bio *sbio = r1_bio->bios[i];\n\t\tblk_status_t status = sbio->bi_status;\n\t\tstruct page **ppages = get_resync_pages(pbio)->pages;\n\t\tstruct page **spages = get_resync_pages(sbio)->pages;\n\t\tstruct bio_vec *bi;\n\t\tint page_len[RESYNC_PAGES] = { 0 };\n\t\tstruct bvec_iter_all iter_all;\n\n\t\tif (sbio->bi_end_io != end_sync_read)\n\t\t\tcontinue;\n\t\t \n\t\tsbio->bi_status = 0;\n\n\t\tbio_for_each_segment_all(bi, sbio, iter_all)\n\t\t\tpage_len[j++] = bi->bv_len;\n\n\t\tif (!status) {\n\t\t\tfor (j = vcnt; j-- ; ) {\n\t\t\t\tif (memcmp(page_address(ppages[j]),\n\t\t\t\t\t   page_address(spages[j]),\n\t\t\t\t\t   page_len[j]))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t} else\n\t\t\tj = 0;\n\t\tif (j >= 0)\n\t\t\tatomic64_add(r1_bio->sectors, &mddev->resync_mismatches);\n\t\tif (j < 0 || (test_bit(MD_RECOVERY_CHECK, &mddev->recovery)\n\t\t\t      && !status)) {\n\t\t\t \n\t\t\tsbio->bi_end_io = NULL;\n\t\t\trdev_dec_pending(conf->mirrors[i].rdev, mddev);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbio_copy_data(sbio, pbio);\n\t}\n}\n\nstatic void sync_request_write(struct mddev *mddev, struct r1bio *r1_bio)\n{\n\tstruct r1conf *conf = mddev->private;\n\tint i;\n\tint disks = conf->raid_disks * 2;\n\tstruct bio *wbio;\n\n\tif (!test_bit(R1BIO_Uptodate, &r1_bio->state))\n\t\t \n\t\tif (!fix_sync_read_error(r1_bio))\n\t\t\treturn;\n\n\tif (test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery))\n\t\tprocess_checks(r1_bio);\n\n\t \n\tatomic_set(&r1_bio->remaining, 1);\n\tfor (i = 0; i < disks ; i++) {\n\t\twbio = r1_bio->bios[i];\n\t\tif (wbio->bi_end_io == NULL ||\n\t\t    (wbio->bi_end_io == end_sync_read &&\n\t\t     (i == r1_bio->read_disk ||\n\t\t      !test_bit(MD_RECOVERY_SYNC, &mddev->recovery))))\n\t\t\tcontinue;\n\t\tif (test_bit(Faulty, &conf->mirrors[i].rdev->flags)) {\n\t\t\tabort_sync_write(mddev, r1_bio);\n\t\t\tcontinue;\n\t\t}\n\n\t\twbio->bi_opf = REQ_OP_WRITE;\n\t\tif (test_bit(FailFast, &conf->mirrors[i].rdev->flags))\n\t\t\twbio->bi_opf |= MD_FAILFAST;\n\n\t\twbio->bi_end_io = end_sync_write;\n\t\tatomic_inc(&r1_bio->remaining);\n\t\tmd_sync_acct(conf->mirrors[i].rdev->bdev, bio_sectors(wbio));\n\n\t\tsubmit_bio_noacct(wbio);\n\t}\n\n\tput_sync_write_buf(r1_bio, 1);\n}\n\n \n\nstatic void fix_read_error(struct r1conf *conf, int read_disk,\n\t\t\t   sector_t sect, int sectors)\n{\n\tstruct mddev *mddev = conf->mddev;\n\twhile(sectors) {\n\t\tint s = sectors;\n\t\tint d = read_disk;\n\t\tint success = 0;\n\t\tint start;\n\t\tstruct md_rdev *rdev;\n\n\t\tif (s > (PAGE_SIZE>>9))\n\t\t\ts = PAGE_SIZE >> 9;\n\n\t\tdo {\n\t\t\tsector_t first_bad;\n\t\t\tint bad_sectors;\n\n\t\t\trcu_read_lock();\n\t\t\trdev = rcu_dereference(conf->mirrors[d].rdev);\n\t\t\tif (rdev &&\n\t\t\t    (test_bit(In_sync, &rdev->flags) ||\n\t\t\t     (!test_bit(Faulty, &rdev->flags) &&\n\t\t\t      rdev->recovery_offset >= sect + s)) &&\n\t\t\t    is_badblock(rdev, sect, s,\n\t\t\t\t\t&first_bad, &bad_sectors) == 0) {\n\t\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\t\trcu_read_unlock();\n\t\t\t\tif (sync_page_io(rdev, sect, s<<9,\n\t\t\t\t\t conf->tmppage, REQ_OP_READ, false))\n\t\t\t\t\tsuccess = 1;\n\t\t\t\trdev_dec_pending(rdev, mddev);\n\t\t\t\tif (success)\n\t\t\t\t\tbreak;\n\t\t\t} else\n\t\t\t\trcu_read_unlock();\n\t\t\td++;\n\t\t\tif (d == conf->raid_disks * 2)\n\t\t\t\td = 0;\n\t\t} while (d != read_disk);\n\n\t\tif (!success) {\n\t\t\t \n\t\t\tstruct md_rdev *rdev = conf->mirrors[read_disk].rdev;\n\t\t\tif (!rdev_set_badblocks(rdev, sect, s, 0))\n\t\t\t\tmd_error(mddev, rdev);\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tstart = d;\n\t\twhile (d != read_disk) {\n\t\t\tif (d==0)\n\t\t\t\td = conf->raid_disks * 2;\n\t\t\td--;\n\t\t\trcu_read_lock();\n\t\t\trdev = rcu_dereference(conf->mirrors[d].rdev);\n\t\t\tif (rdev &&\n\t\t\t    !test_bit(Faulty, &rdev->flags)) {\n\t\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\t\trcu_read_unlock();\n\t\t\t\tr1_sync_page_io(rdev, sect, s,\n\t\t\t\t\t\tconf->tmppage, REQ_OP_WRITE);\n\t\t\t\trdev_dec_pending(rdev, mddev);\n\t\t\t} else\n\t\t\t\trcu_read_unlock();\n\t\t}\n\t\td = start;\n\t\twhile (d != read_disk) {\n\t\t\tif (d==0)\n\t\t\t\td = conf->raid_disks * 2;\n\t\t\td--;\n\t\t\trcu_read_lock();\n\t\t\trdev = rcu_dereference(conf->mirrors[d].rdev);\n\t\t\tif (rdev &&\n\t\t\t    !test_bit(Faulty, &rdev->flags)) {\n\t\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\t\trcu_read_unlock();\n\t\t\t\tif (r1_sync_page_io(rdev, sect, s,\n\t\t\t\t\t\tconf->tmppage, REQ_OP_READ)) {\n\t\t\t\t\tatomic_add(s, &rdev->corrected_errors);\n\t\t\t\t\tpr_info(\"md/raid1:%s: read error corrected (%d sectors at %llu on %pg)\\n\",\n\t\t\t\t\t\tmdname(mddev), s,\n\t\t\t\t\t\t(unsigned long long)(sect +\n\t\t\t\t\t\t\t\t     rdev->data_offset),\n\t\t\t\t\t\trdev->bdev);\n\t\t\t\t}\n\t\t\t\trdev_dec_pending(rdev, mddev);\n\t\t\t} else\n\t\t\t\trcu_read_unlock();\n\t\t}\n\t\tsectors -= s;\n\t\tsect += s;\n\t}\n}\n\nstatic int narrow_write_error(struct r1bio *r1_bio, int i)\n{\n\tstruct mddev *mddev = r1_bio->mddev;\n\tstruct r1conf *conf = mddev->private;\n\tstruct md_rdev *rdev = conf->mirrors[i].rdev;\n\n\t \n\n\tint block_sectors;\n\tsector_t sector;\n\tint sectors;\n\tint sect_to_write = r1_bio->sectors;\n\tint ok = 1;\n\n\tif (rdev->badblocks.shift < 0)\n\t\treturn 0;\n\n\tblock_sectors = roundup(1 << rdev->badblocks.shift,\n\t\t\t\tbdev_logical_block_size(rdev->bdev) >> 9);\n\tsector = r1_bio->sector;\n\tsectors = ((sector + block_sectors)\n\t\t   & ~(sector_t)(block_sectors - 1))\n\t\t- sector;\n\n\twhile (sect_to_write) {\n\t\tstruct bio *wbio;\n\t\tif (sectors > sect_to_write)\n\t\t\tsectors = sect_to_write;\n\t\t \n\n\t\tif (test_bit(R1BIO_BehindIO, &r1_bio->state)) {\n\t\t\twbio = bio_alloc_clone(rdev->bdev,\n\t\t\t\t\t       r1_bio->behind_master_bio,\n\t\t\t\t\t       GFP_NOIO, &mddev->bio_set);\n\t\t} else {\n\t\t\twbio = bio_alloc_clone(rdev->bdev, r1_bio->master_bio,\n\t\t\t\t\t       GFP_NOIO, &mddev->bio_set);\n\t\t}\n\n\t\twbio->bi_opf = REQ_OP_WRITE;\n\t\twbio->bi_iter.bi_sector = r1_bio->sector;\n\t\twbio->bi_iter.bi_size = r1_bio->sectors << 9;\n\n\t\tbio_trim(wbio, sector - r1_bio->sector, sectors);\n\t\twbio->bi_iter.bi_sector += rdev->data_offset;\n\n\t\tif (submit_bio_wait(wbio) < 0)\n\t\t\t \n\t\t\tok = rdev_set_badblocks(rdev, sector,\n\t\t\t\t\t\tsectors, 0)\n\t\t\t\t&& ok;\n\n\t\tbio_put(wbio);\n\t\tsect_to_write -= sectors;\n\t\tsector += sectors;\n\t\tsectors = block_sectors;\n\t}\n\treturn ok;\n}\n\nstatic void handle_sync_write_finished(struct r1conf *conf, struct r1bio *r1_bio)\n{\n\tint m;\n\tint s = r1_bio->sectors;\n\tfor (m = 0; m < conf->raid_disks * 2 ; m++) {\n\t\tstruct md_rdev *rdev = conf->mirrors[m].rdev;\n\t\tstruct bio *bio = r1_bio->bios[m];\n\t\tif (bio->bi_end_io == NULL)\n\t\t\tcontinue;\n\t\tif (!bio->bi_status &&\n\t\t    test_bit(R1BIO_MadeGood, &r1_bio->state)) {\n\t\t\trdev_clear_badblocks(rdev, r1_bio->sector, s, 0);\n\t\t}\n\t\tif (bio->bi_status &&\n\t\t    test_bit(R1BIO_WriteError, &r1_bio->state)) {\n\t\t\tif (!rdev_set_badblocks(rdev, r1_bio->sector, s, 0))\n\t\t\t\tmd_error(conf->mddev, rdev);\n\t\t}\n\t}\n\tput_buf(r1_bio);\n\tmd_done_sync(conf->mddev, s, 1);\n}\n\nstatic void handle_write_finished(struct r1conf *conf, struct r1bio *r1_bio)\n{\n\tint m, idx;\n\tbool fail = false;\n\n\tfor (m = 0; m < conf->raid_disks * 2 ; m++)\n\t\tif (r1_bio->bios[m] == IO_MADE_GOOD) {\n\t\t\tstruct md_rdev *rdev = conf->mirrors[m].rdev;\n\t\t\trdev_clear_badblocks(rdev,\n\t\t\t\t\t     r1_bio->sector,\n\t\t\t\t\t     r1_bio->sectors, 0);\n\t\t\trdev_dec_pending(rdev, conf->mddev);\n\t\t} else if (r1_bio->bios[m] != NULL) {\n\t\t\t \n\t\t\tfail = true;\n\t\t\tif (!narrow_write_error(r1_bio, m)) {\n\t\t\t\tmd_error(conf->mddev,\n\t\t\t\t\t conf->mirrors[m].rdev);\n\t\t\t\t \n\t\t\t\tset_bit(R1BIO_Degraded, &r1_bio->state);\n\t\t\t}\n\t\t\trdev_dec_pending(conf->mirrors[m].rdev,\n\t\t\t\t\t conf->mddev);\n\t\t}\n\tif (fail) {\n\t\tspin_lock_irq(&conf->device_lock);\n\t\tlist_add(&r1_bio->retry_list, &conf->bio_end_io_list);\n\t\tidx = sector_to_idx(r1_bio->sector);\n\t\tatomic_inc(&conf->nr_queued[idx]);\n\t\tspin_unlock_irq(&conf->device_lock);\n\t\t \n\t\twake_up(&conf->wait_barrier);\n\t\tmd_wakeup_thread(conf->mddev->thread);\n\t} else {\n\t\tif (test_bit(R1BIO_WriteError, &r1_bio->state))\n\t\t\tclose_write(r1_bio);\n\t\traid_end_bio_io(r1_bio);\n\t}\n}\n\nstatic void handle_read_error(struct r1conf *conf, struct r1bio *r1_bio)\n{\n\tstruct mddev *mddev = conf->mddev;\n\tstruct bio *bio;\n\tstruct md_rdev *rdev;\n\tsector_t sector;\n\n\tclear_bit(R1BIO_ReadError, &r1_bio->state);\n\t \n\n\tbio = r1_bio->bios[r1_bio->read_disk];\n\tbio_put(bio);\n\tr1_bio->bios[r1_bio->read_disk] = NULL;\n\n\trdev = conf->mirrors[r1_bio->read_disk].rdev;\n\tif (mddev->ro == 0\n\t    && !test_bit(FailFast, &rdev->flags)) {\n\t\tfreeze_array(conf, 1);\n\t\tfix_read_error(conf, r1_bio->read_disk,\n\t\t\t       r1_bio->sector, r1_bio->sectors);\n\t\tunfreeze_array(conf);\n\t} else if (mddev->ro == 0 && test_bit(FailFast, &rdev->flags)) {\n\t\tmd_error(mddev, rdev);\n\t} else {\n\t\tr1_bio->bios[r1_bio->read_disk] = IO_BLOCKED;\n\t}\n\n\trdev_dec_pending(rdev, conf->mddev);\n\tsector = r1_bio->sector;\n\tbio = r1_bio->master_bio;\n\n\t \n\tr1_bio->state = 0;\n\traid1_read_request(mddev, bio, r1_bio->sectors, r1_bio);\n\tallow_barrier(conf, sector);\n}\n\nstatic void raid1d(struct md_thread *thread)\n{\n\tstruct mddev *mddev = thread->mddev;\n\tstruct r1bio *r1_bio;\n\tunsigned long flags;\n\tstruct r1conf *conf = mddev->private;\n\tstruct list_head *head = &conf->retry_list;\n\tstruct blk_plug plug;\n\tint idx;\n\n\tmd_check_recovery(mddev);\n\n\tif (!list_empty_careful(&conf->bio_end_io_list) &&\n\t    !test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags)) {\n\t\tLIST_HEAD(tmp);\n\t\tspin_lock_irqsave(&conf->device_lock, flags);\n\t\tif (!test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags))\n\t\t\tlist_splice_init(&conf->bio_end_io_list, &tmp);\n\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\t\twhile (!list_empty(&tmp)) {\n\t\t\tr1_bio = list_first_entry(&tmp, struct r1bio,\n\t\t\t\t\t\t  retry_list);\n\t\t\tlist_del(&r1_bio->retry_list);\n\t\t\tidx = sector_to_idx(r1_bio->sector);\n\t\t\tatomic_dec(&conf->nr_queued[idx]);\n\t\t\tif (mddev->degraded)\n\t\t\t\tset_bit(R1BIO_Degraded, &r1_bio->state);\n\t\t\tif (test_bit(R1BIO_WriteError, &r1_bio->state))\n\t\t\t\tclose_write(r1_bio);\n\t\t\traid_end_bio_io(r1_bio);\n\t\t}\n\t}\n\n\tblk_start_plug(&plug);\n\tfor (;;) {\n\n\t\tflush_pending_writes(conf);\n\n\t\tspin_lock_irqsave(&conf->device_lock, flags);\n\t\tif (list_empty(head)) {\n\t\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\t\t\tbreak;\n\t\t}\n\t\tr1_bio = list_entry(head->prev, struct r1bio, retry_list);\n\t\tlist_del(head->prev);\n\t\tidx = sector_to_idx(r1_bio->sector);\n\t\tatomic_dec(&conf->nr_queued[idx]);\n\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\n\t\tmddev = r1_bio->mddev;\n\t\tconf = mddev->private;\n\t\tif (test_bit(R1BIO_IsSync, &r1_bio->state)) {\n\t\t\tif (test_bit(R1BIO_MadeGood, &r1_bio->state) ||\n\t\t\t    test_bit(R1BIO_WriteError, &r1_bio->state))\n\t\t\t\thandle_sync_write_finished(conf, r1_bio);\n\t\t\telse\n\t\t\t\tsync_request_write(mddev, r1_bio);\n\t\t} else if (test_bit(R1BIO_MadeGood, &r1_bio->state) ||\n\t\t\t   test_bit(R1BIO_WriteError, &r1_bio->state))\n\t\t\thandle_write_finished(conf, r1_bio);\n\t\telse if (test_bit(R1BIO_ReadError, &r1_bio->state))\n\t\t\thandle_read_error(conf, r1_bio);\n\t\telse\n\t\t\tWARN_ON_ONCE(1);\n\n\t\tcond_resched();\n\t\tif (mddev->sb_flags & ~(1<<MD_SB_CHANGE_PENDING))\n\t\t\tmd_check_recovery(mddev);\n\t}\n\tblk_finish_plug(&plug);\n}\n\nstatic int init_resync(struct r1conf *conf)\n{\n\tint buffs;\n\n\tbuffs = RESYNC_WINDOW / RESYNC_BLOCK_SIZE;\n\tBUG_ON(mempool_initialized(&conf->r1buf_pool));\n\n\treturn mempool_init(&conf->r1buf_pool, buffs, r1buf_pool_alloc,\n\t\t\t    r1buf_pool_free, conf->poolinfo);\n}\n\nstatic struct r1bio *raid1_alloc_init_r1buf(struct r1conf *conf)\n{\n\tstruct r1bio *r1bio = mempool_alloc(&conf->r1buf_pool, GFP_NOIO);\n\tstruct resync_pages *rps;\n\tstruct bio *bio;\n\tint i;\n\n\tfor (i = conf->poolinfo->raid_disks; i--; ) {\n\t\tbio = r1bio->bios[i];\n\t\trps = bio->bi_private;\n\t\tbio_reset(bio, NULL, 0);\n\t\tbio->bi_private = rps;\n\t}\n\tr1bio->master_bio = NULL;\n\treturn r1bio;\n}\n\n \n\nstatic sector_t raid1_sync_request(struct mddev *mddev, sector_t sector_nr,\n\t\t\t\t   int *skipped)\n{\n\tstruct r1conf *conf = mddev->private;\n\tstruct r1bio *r1_bio;\n\tstruct bio *bio;\n\tsector_t max_sector, nr_sectors;\n\tint disk = -1;\n\tint i;\n\tint wonly = -1;\n\tint write_targets = 0, read_targets = 0;\n\tsector_t sync_blocks;\n\tint still_degraded = 0;\n\tint good_sectors = RESYNC_SECTORS;\n\tint min_bad = 0;  \n\tint idx = sector_to_idx(sector_nr);\n\tint page_idx = 0;\n\n\tif (!mempool_initialized(&conf->r1buf_pool))\n\t\tif (init_resync(conf))\n\t\t\treturn 0;\n\n\tmax_sector = mddev->dev_sectors;\n\tif (sector_nr >= max_sector) {\n\t\t \n\t\tif (mddev->curr_resync < max_sector)  \n\t\t\tmd_bitmap_end_sync(mddev->bitmap, mddev->curr_resync,\n\t\t\t\t\t   &sync_blocks, 1);\n\t\telse  \n\t\t\tconf->fullsync = 0;\n\n\t\tmd_bitmap_close_sync(mddev->bitmap);\n\t\tclose_sync(conf);\n\n\t\tif (mddev_is_clustered(mddev)) {\n\t\t\tconf->cluster_sync_low = 0;\n\t\t\tconf->cluster_sync_high = 0;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tif (mddev->bitmap == NULL &&\n\t    mddev->recovery_cp == MaxSector &&\n\t    !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery) &&\n\t    conf->fullsync == 0) {\n\t\t*skipped = 1;\n\t\treturn max_sector - sector_nr;\n\t}\n\t \n\tif (!md_bitmap_start_sync(mddev->bitmap, sector_nr, &sync_blocks, 1) &&\n\t    !conf->fullsync && !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery)) {\n\t\t \n\t\t*skipped = 1;\n\t\treturn sync_blocks;\n\t}\n\n\t \n\tif (atomic_read(&conf->nr_waiting[idx]))\n\t\tschedule_timeout_uninterruptible(1);\n\n\t \n\n\tmd_bitmap_cond_end_sync(mddev->bitmap, sector_nr,\n\t\tmddev_is_clustered(mddev) && (sector_nr + 2 * RESYNC_SECTORS > conf->cluster_sync_high));\n\n\n\tif (raise_barrier(conf, sector_nr))\n\t\treturn 0;\n\n\tr1_bio = raid1_alloc_init_r1buf(conf);\n\n\trcu_read_lock();\n\t \n\n\tr1_bio->mddev = mddev;\n\tr1_bio->sector = sector_nr;\n\tr1_bio->state = 0;\n\tset_bit(R1BIO_IsSync, &r1_bio->state);\n\t \n\tgood_sectors = align_to_barrier_unit_end(sector_nr, good_sectors);\n\n\tfor (i = 0; i < conf->raid_disks * 2; i++) {\n\t\tstruct md_rdev *rdev;\n\t\tbio = r1_bio->bios[i];\n\n\t\trdev = rcu_dereference(conf->mirrors[i].rdev);\n\t\tif (rdev == NULL ||\n\t\t    test_bit(Faulty, &rdev->flags)) {\n\t\t\tif (i < conf->raid_disks)\n\t\t\t\tstill_degraded = 1;\n\t\t} else if (!test_bit(In_sync, &rdev->flags)) {\n\t\t\tbio->bi_opf = REQ_OP_WRITE;\n\t\t\tbio->bi_end_io = end_sync_write;\n\t\t\twrite_targets ++;\n\t\t} else {\n\t\t\t \n\t\t\tsector_t first_bad = MaxSector;\n\t\t\tint bad_sectors;\n\n\t\t\tif (is_badblock(rdev, sector_nr, good_sectors,\n\t\t\t\t\t&first_bad, &bad_sectors)) {\n\t\t\t\tif (first_bad > sector_nr)\n\t\t\t\t\tgood_sectors = first_bad - sector_nr;\n\t\t\t\telse {\n\t\t\t\t\tbad_sectors -= (sector_nr - first_bad);\n\t\t\t\t\tif (min_bad == 0 ||\n\t\t\t\t\t    min_bad > bad_sectors)\n\t\t\t\t\t\tmin_bad = bad_sectors;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (sector_nr < first_bad) {\n\t\t\t\tif (test_bit(WriteMostly, &rdev->flags)) {\n\t\t\t\t\tif (wonly < 0)\n\t\t\t\t\t\twonly = i;\n\t\t\t\t} else {\n\t\t\t\t\tif (disk < 0)\n\t\t\t\t\t\tdisk = i;\n\t\t\t\t}\n\t\t\t\tbio->bi_opf = REQ_OP_READ;\n\t\t\t\tbio->bi_end_io = end_sync_read;\n\t\t\t\tread_targets++;\n\t\t\t} else if (!test_bit(WriteErrorSeen, &rdev->flags) &&\n\t\t\t\ttest_bit(MD_RECOVERY_SYNC, &mddev->recovery) &&\n\t\t\t\t!test_bit(MD_RECOVERY_CHECK, &mddev->recovery)) {\n\t\t\t\t \n\t\t\t\tbio->bi_opf = REQ_OP_WRITE;\n\t\t\t\tbio->bi_end_io = end_sync_write;\n\t\t\t\twrite_targets++;\n\t\t\t}\n\t\t}\n\t\tif (rdev && bio->bi_end_io) {\n\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\tbio->bi_iter.bi_sector = sector_nr + rdev->data_offset;\n\t\t\tbio_set_dev(bio, rdev->bdev);\n\t\t\tif (test_bit(FailFast, &rdev->flags))\n\t\t\t\tbio->bi_opf |= MD_FAILFAST;\n\t\t}\n\t}\n\trcu_read_unlock();\n\tif (disk < 0)\n\t\tdisk = wonly;\n\tr1_bio->read_disk = disk;\n\n\tif (read_targets == 0 && min_bad > 0) {\n\t\t \n\t\tint ok = 1;\n\t\tfor (i = 0 ; i < conf->raid_disks * 2 ; i++)\n\t\t\tif (r1_bio->bios[i]->bi_end_io == end_sync_write) {\n\t\t\t\tstruct md_rdev *rdev = conf->mirrors[i].rdev;\n\t\t\t\tok = rdev_set_badblocks(rdev, sector_nr,\n\t\t\t\t\t\t\tmin_bad, 0\n\t\t\t\t\t) && ok;\n\t\t\t}\n\t\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\t\t*skipped = 1;\n\t\tput_buf(r1_bio);\n\n\t\tif (!ok) {\n\t\t\t \n\t\t\tconf->recovery_disabled = mddev->recovery_disabled;\n\t\t\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\t\t\treturn 0;\n\t\t} else\n\t\t\treturn min_bad;\n\n\t}\n\tif (min_bad > 0 && min_bad < good_sectors) {\n\t\t \n\t\tgood_sectors = min_bad;\n\t}\n\n\tif (test_bit(MD_RECOVERY_SYNC, &mddev->recovery) && read_targets > 0)\n\t\t \n\t\twrite_targets += read_targets-1;\n\n\tif (write_targets == 0 || read_targets == 0) {\n\t\t \n\t\tsector_t rv;\n\t\tif (min_bad > 0)\n\t\t\tmax_sector = sector_nr + min_bad;\n\t\trv = max_sector - sector_nr;\n\t\t*skipped = 1;\n\t\tput_buf(r1_bio);\n\t\treturn rv;\n\t}\n\n\tif (max_sector > mddev->resync_max)\n\t\tmax_sector = mddev->resync_max;  \n\tif (max_sector > sector_nr + good_sectors)\n\t\tmax_sector = sector_nr + good_sectors;\n\tnr_sectors = 0;\n\tsync_blocks = 0;\n\tdo {\n\t\tstruct page *page;\n\t\tint len = PAGE_SIZE;\n\t\tif (sector_nr + (len>>9) > max_sector)\n\t\t\tlen = (max_sector - sector_nr) << 9;\n\t\tif (len == 0)\n\t\t\tbreak;\n\t\tif (sync_blocks == 0) {\n\t\t\tif (!md_bitmap_start_sync(mddev->bitmap, sector_nr,\n\t\t\t\t\t\t  &sync_blocks, still_degraded) &&\n\t\t\t    !conf->fullsync &&\n\t\t\t    !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery))\n\t\t\t\tbreak;\n\t\t\tif ((len >> 9) > sync_blocks)\n\t\t\t\tlen = sync_blocks<<9;\n\t\t}\n\n\t\tfor (i = 0 ; i < conf->raid_disks * 2; i++) {\n\t\t\tstruct resync_pages *rp;\n\n\t\t\tbio = r1_bio->bios[i];\n\t\t\trp = get_resync_pages(bio);\n\t\t\tif (bio->bi_end_io) {\n\t\t\t\tpage = resync_fetch_page(rp, page_idx);\n\n\t\t\t\t \n\t\t\t\t__bio_add_page(bio, page, len, 0);\n\t\t\t}\n\t\t}\n\t\tnr_sectors += len>>9;\n\t\tsector_nr += len>>9;\n\t\tsync_blocks -= (len>>9);\n\t} while (++page_idx < RESYNC_PAGES);\n\n\tr1_bio->sectors = nr_sectors;\n\n\tif (mddev_is_clustered(mddev) &&\n\t\t\tconf->cluster_sync_high < sector_nr + nr_sectors) {\n\t\tconf->cluster_sync_low = mddev->curr_resync_completed;\n\t\tconf->cluster_sync_high = conf->cluster_sync_low + CLUSTER_RESYNC_WINDOW_SECTORS;\n\t\t \n\t\tmd_cluster_ops->resync_info_update(mddev,\n\t\t\t\tconf->cluster_sync_low,\n\t\t\t\tconf->cluster_sync_high);\n\t}\n\n\t \n\tif (test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery)) {\n\t\tatomic_set(&r1_bio->remaining, read_targets);\n\t\tfor (i = 0; i < conf->raid_disks * 2 && read_targets; i++) {\n\t\t\tbio = r1_bio->bios[i];\n\t\t\tif (bio->bi_end_io == end_sync_read) {\n\t\t\t\tread_targets--;\n\t\t\t\tmd_sync_acct_bio(bio, nr_sectors);\n\t\t\t\tif (read_targets == 1)\n\t\t\t\t\tbio->bi_opf &= ~MD_FAILFAST;\n\t\t\t\tsubmit_bio_noacct(bio);\n\t\t\t}\n\t\t}\n\t} else {\n\t\tatomic_set(&r1_bio->remaining, 1);\n\t\tbio = r1_bio->bios[r1_bio->read_disk];\n\t\tmd_sync_acct_bio(bio, nr_sectors);\n\t\tif (read_targets == 1)\n\t\t\tbio->bi_opf &= ~MD_FAILFAST;\n\t\tsubmit_bio_noacct(bio);\n\t}\n\treturn nr_sectors;\n}\n\nstatic sector_t raid1_size(struct mddev *mddev, sector_t sectors, int raid_disks)\n{\n\tif (sectors)\n\t\treturn sectors;\n\n\treturn mddev->dev_sectors;\n}\n\nstatic struct r1conf *setup_conf(struct mddev *mddev)\n{\n\tstruct r1conf *conf;\n\tint i;\n\tstruct raid1_info *disk;\n\tstruct md_rdev *rdev;\n\tint err = -ENOMEM;\n\n\tconf = kzalloc(sizeof(struct r1conf), GFP_KERNEL);\n\tif (!conf)\n\t\tgoto abort;\n\n\tconf->nr_pending = kcalloc(BARRIER_BUCKETS_NR,\n\t\t\t\t   sizeof(atomic_t), GFP_KERNEL);\n\tif (!conf->nr_pending)\n\t\tgoto abort;\n\n\tconf->nr_waiting = kcalloc(BARRIER_BUCKETS_NR,\n\t\t\t\t   sizeof(atomic_t), GFP_KERNEL);\n\tif (!conf->nr_waiting)\n\t\tgoto abort;\n\n\tconf->nr_queued = kcalloc(BARRIER_BUCKETS_NR,\n\t\t\t\t  sizeof(atomic_t), GFP_KERNEL);\n\tif (!conf->nr_queued)\n\t\tgoto abort;\n\n\tconf->barrier = kcalloc(BARRIER_BUCKETS_NR,\n\t\t\t\tsizeof(atomic_t), GFP_KERNEL);\n\tif (!conf->barrier)\n\t\tgoto abort;\n\n\tconf->mirrors = kzalloc(array3_size(sizeof(struct raid1_info),\n\t\t\t\t\t    mddev->raid_disks, 2),\n\t\t\t\tGFP_KERNEL);\n\tif (!conf->mirrors)\n\t\tgoto abort;\n\n\tconf->tmppage = alloc_page(GFP_KERNEL);\n\tif (!conf->tmppage)\n\t\tgoto abort;\n\n\tconf->poolinfo = kzalloc(sizeof(*conf->poolinfo), GFP_KERNEL);\n\tif (!conf->poolinfo)\n\t\tgoto abort;\n\tconf->poolinfo->raid_disks = mddev->raid_disks * 2;\n\terr = mempool_init(&conf->r1bio_pool, NR_RAID_BIOS, r1bio_pool_alloc,\n\t\t\t   rbio_pool_free, conf->poolinfo);\n\tif (err)\n\t\tgoto abort;\n\n\terr = bioset_init(&conf->bio_split, BIO_POOL_SIZE, 0, 0);\n\tif (err)\n\t\tgoto abort;\n\n\tconf->poolinfo->mddev = mddev;\n\n\terr = -EINVAL;\n\tspin_lock_init(&conf->device_lock);\n\trdev_for_each(rdev, mddev) {\n\t\tint disk_idx = rdev->raid_disk;\n\t\tif (disk_idx >= mddev->raid_disks\n\t\t    || disk_idx < 0)\n\t\t\tcontinue;\n\t\tif (test_bit(Replacement, &rdev->flags))\n\t\t\tdisk = conf->mirrors + mddev->raid_disks + disk_idx;\n\t\telse\n\t\t\tdisk = conf->mirrors + disk_idx;\n\n\t\tif (disk->rdev)\n\t\t\tgoto abort;\n\t\tdisk->rdev = rdev;\n\t\tdisk->head_position = 0;\n\t\tdisk->seq_start = MaxSector;\n\t}\n\tconf->raid_disks = mddev->raid_disks;\n\tconf->mddev = mddev;\n\tINIT_LIST_HEAD(&conf->retry_list);\n\tINIT_LIST_HEAD(&conf->bio_end_io_list);\n\n\tspin_lock_init(&conf->resync_lock);\n\tinit_waitqueue_head(&conf->wait_barrier);\n\n\tbio_list_init(&conf->pending_bio_list);\n\tconf->recovery_disabled = mddev->recovery_disabled - 1;\n\n\terr = -EIO;\n\tfor (i = 0; i < conf->raid_disks * 2; i++) {\n\n\t\tdisk = conf->mirrors + i;\n\n\t\tif (i < conf->raid_disks &&\n\t\t    disk[conf->raid_disks].rdev) {\n\t\t\t \n\t\t\tif (!disk->rdev) {\n\t\t\t\t \n\t\t\t\tdisk->rdev =\n\t\t\t\t\tdisk[conf->raid_disks].rdev;\n\t\t\t\tdisk[conf->raid_disks].rdev = NULL;\n\t\t\t} else if (!test_bit(In_sync, &disk->rdev->flags))\n\t\t\t\t \n\t\t\t\tgoto abort;\n\t\t}\n\n\t\tif (!disk->rdev ||\n\t\t    !test_bit(In_sync, &disk->rdev->flags)) {\n\t\t\tdisk->head_position = 0;\n\t\t\tif (disk->rdev &&\n\t\t\t    (disk->rdev->saved_raid_disk < 0))\n\t\t\t\tconf->fullsync = 1;\n\t\t}\n\t}\n\n\terr = -ENOMEM;\n\trcu_assign_pointer(conf->thread,\n\t\t\t   md_register_thread(raid1d, mddev, \"raid1\"));\n\tif (!conf->thread)\n\t\tgoto abort;\n\n\treturn conf;\n\n abort:\n\tif (conf) {\n\t\tmempool_exit(&conf->r1bio_pool);\n\t\tkfree(conf->mirrors);\n\t\tsafe_put_page(conf->tmppage);\n\t\tkfree(conf->poolinfo);\n\t\tkfree(conf->nr_pending);\n\t\tkfree(conf->nr_waiting);\n\t\tkfree(conf->nr_queued);\n\t\tkfree(conf->barrier);\n\t\tbioset_exit(&conf->bio_split);\n\t\tkfree(conf);\n\t}\n\treturn ERR_PTR(err);\n}\n\nstatic void raid1_free(struct mddev *mddev, void *priv);\nstatic int raid1_run(struct mddev *mddev)\n{\n\tstruct r1conf *conf;\n\tint i;\n\tstruct md_rdev *rdev;\n\tint ret;\n\n\tif (mddev->level != 1) {\n\t\tpr_warn(\"md/raid1:%s: raid level not set to mirroring (%d)\\n\",\n\t\t\tmdname(mddev), mddev->level);\n\t\treturn -EIO;\n\t}\n\tif (mddev->reshape_position != MaxSector) {\n\t\tpr_warn(\"md/raid1:%s: reshape_position set but not supported\\n\",\n\t\t\tmdname(mddev));\n\t\treturn -EIO;\n\t}\n\tif (mddev_init_writes_pending(mddev) < 0)\n\t\treturn -ENOMEM;\n\t \n\tif (mddev->private == NULL)\n\t\tconf = setup_conf(mddev);\n\telse\n\t\tconf = mddev->private;\n\n\tif (IS_ERR(conf))\n\t\treturn PTR_ERR(conf);\n\n\tif (mddev->queue)\n\t\tblk_queue_max_write_zeroes_sectors(mddev->queue, 0);\n\n\trdev_for_each(rdev, mddev) {\n\t\tif (!mddev->gendisk)\n\t\t\tcontinue;\n\t\tdisk_stack_limits(mddev->gendisk, rdev->bdev,\n\t\t\t\t  rdev->data_offset << 9);\n\t}\n\n\tmddev->degraded = 0;\n\tfor (i = 0; i < conf->raid_disks; i++)\n\t\tif (conf->mirrors[i].rdev == NULL ||\n\t\t    !test_bit(In_sync, &conf->mirrors[i].rdev->flags) ||\n\t\t    test_bit(Faulty, &conf->mirrors[i].rdev->flags))\n\t\t\tmddev->degraded++;\n\t \n\tif (conf->raid_disks - mddev->degraded < 1) {\n\t\tmd_unregister_thread(mddev, &conf->thread);\n\t\tret = -EINVAL;\n\t\tgoto abort;\n\t}\n\n\tif (conf->raid_disks - mddev->degraded == 1)\n\t\tmddev->recovery_cp = MaxSector;\n\n\tif (mddev->recovery_cp != MaxSector)\n\t\tpr_info(\"md/raid1:%s: not clean -- starting background reconstruction\\n\",\n\t\t\tmdname(mddev));\n\tpr_info(\"md/raid1:%s: active with %d out of %d mirrors\\n\",\n\t\tmdname(mddev), mddev->raid_disks - mddev->degraded,\n\t\tmddev->raid_disks);\n\n\t \n\trcu_assign_pointer(mddev->thread, conf->thread);\n\trcu_assign_pointer(conf->thread, NULL);\n\tmddev->private = conf;\n\tset_bit(MD_FAILFAST_SUPPORTED, &mddev->flags);\n\n\tmd_set_array_sectors(mddev, raid1_size(mddev, 0, 0));\n\n\tret = md_integrity_register(mddev);\n\tif (ret) {\n\t\tmd_unregister_thread(mddev, &mddev->thread);\n\t\tgoto abort;\n\t}\n\treturn 0;\n\nabort:\n\traid1_free(mddev, conf);\n\treturn ret;\n}\n\nstatic void raid1_free(struct mddev *mddev, void *priv)\n{\n\tstruct r1conf *conf = priv;\n\n\tmempool_exit(&conf->r1bio_pool);\n\tkfree(conf->mirrors);\n\tsafe_put_page(conf->tmppage);\n\tkfree(conf->poolinfo);\n\tkfree(conf->nr_pending);\n\tkfree(conf->nr_waiting);\n\tkfree(conf->nr_queued);\n\tkfree(conf->barrier);\n\tbioset_exit(&conf->bio_split);\n\tkfree(conf);\n}\n\nstatic int raid1_resize(struct mddev *mddev, sector_t sectors)\n{\n\t \n\tsector_t newsize = raid1_size(mddev, sectors, 0);\n\tif (mddev->external_size &&\n\t    mddev->array_sectors > newsize)\n\t\treturn -EINVAL;\n\tif (mddev->bitmap) {\n\t\tint ret = md_bitmap_resize(mddev->bitmap, newsize, 0, 0);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\tmd_set_array_sectors(mddev, newsize);\n\tif (sectors > mddev->dev_sectors &&\n\t    mddev->recovery_cp > mddev->dev_sectors) {\n\t\tmddev->recovery_cp = mddev->dev_sectors;\n\t\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\t}\n\tmddev->dev_sectors = sectors;\n\tmddev->resync_max_sectors = sectors;\n\treturn 0;\n}\n\nstatic int raid1_reshape(struct mddev *mddev)\n{\n\t \n\tmempool_t newpool, oldpool;\n\tstruct pool_info *newpoolinfo;\n\tstruct raid1_info *newmirrors;\n\tstruct r1conf *conf = mddev->private;\n\tint cnt, raid_disks;\n\tunsigned long flags;\n\tint d, d2;\n\tint ret;\n\n\tmemset(&newpool, 0, sizeof(newpool));\n\tmemset(&oldpool, 0, sizeof(oldpool));\n\n\t \n\tif (mddev->chunk_sectors != mddev->new_chunk_sectors ||\n\t    mddev->layout != mddev->new_layout ||\n\t    mddev->level != mddev->new_level) {\n\t\tmddev->new_chunk_sectors = mddev->chunk_sectors;\n\t\tmddev->new_layout = mddev->layout;\n\t\tmddev->new_level = mddev->level;\n\t\treturn -EINVAL;\n\t}\n\n\tif (!mddev_is_clustered(mddev))\n\t\tmd_allow_write(mddev);\n\n\traid_disks = mddev->raid_disks + mddev->delta_disks;\n\n\tif (raid_disks < conf->raid_disks) {\n\t\tcnt=0;\n\t\tfor (d= 0; d < conf->raid_disks; d++)\n\t\t\tif (conf->mirrors[d].rdev)\n\t\t\t\tcnt++;\n\t\tif (cnt > raid_disks)\n\t\t\treturn -EBUSY;\n\t}\n\n\tnewpoolinfo = kmalloc(sizeof(*newpoolinfo), GFP_KERNEL);\n\tif (!newpoolinfo)\n\t\treturn -ENOMEM;\n\tnewpoolinfo->mddev = mddev;\n\tnewpoolinfo->raid_disks = raid_disks * 2;\n\n\tret = mempool_init(&newpool, NR_RAID_BIOS, r1bio_pool_alloc,\n\t\t\t   rbio_pool_free, newpoolinfo);\n\tif (ret) {\n\t\tkfree(newpoolinfo);\n\t\treturn ret;\n\t}\n\tnewmirrors = kzalloc(array3_size(sizeof(struct raid1_info),\n\t\t\t\t\t raid_disks, 2),\n\t\t\t     GFP_KERNEL);\n\tif (!newmirrors) {\n\t\tkfree(newpoolinfo);\n\t\tmempool_exit(&newpool);\n\t\treturn -ENOMEM;\n\t}\n\n\tfreeze_array(conf, 0);\n\n\t \n\toldpool = conf->r1bio_pool;\n\tconf->r1bio_pool = newpool;\n\n\tfor (d = d2 = 0; d < conf->raid_disks; d++) {\n\t\tstruct md_rdev *rdev = conf->mirrors[d].rdev;\n\t\tif (rdev && rdev->raid_disk != d2) {\n\t\t\tsysfs_unlink_rdev(mddev, rdev);\n\t\t\trdev->raid_disk = d2;\n\t\t\tsysfs_unlink_rdev(mddev, rdev);\n\t\t\tif (sysfs_link_rdev(mddev, rdev))\n\t\t\t\tpr_warn(\"md/raid1:%s: cannot register rd%d\\n\",\n\t\t\t\t\tmdname(mddev), rdev->raid_disk);\n\t\t}\n\t\tif (rdev)\n\t\t\tnewmirrors[d2++].rdev = rdev;\n\t}\n\tkfree(conf->mirrors);\n\tconf->mirrors = newmirrors;\n\tkfree(conf->poolinfo);\n\tconf->poolinfo = newpoolinfo;\n\n\tspin_lock_irqsave(&conf->device_lock, flags);\n\tmddev->degraded += (raid_disks - conf->raid_disks);\n\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\tconf->raid_disks = mddev->raid_disks = raid_disks;\n\tmddev->delta_disks = 0;\n\n\tunfreeze_array(conf);\n\n\tset_bit(MD_RECOVERY_RECOVER, &mddev->recovery);\n\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\tmd_wakeup_thread(mddev->thread);\n\n\tmempool_exit(&oldpool);\n\treturn 0;\n}\n\nstatic void raid1_quiesce(struct mddev *mddev, int quiesce)\n{\n\tstruct r1conf *conf = mddev->private;\n\n\tif (quiesce)\n\t\tfreeze_array(conf, 0);\n\telse\n\t\tunfreeze_array(conf);\n}\n\nstatic void *raid1_takeover(struct mddev *mddev)\n{\n\t \n\tif (mddev->level == 5 && mddev->raid_disks == 2) {\n\t\tstruct r1conf *conf;\n\t\tmddev->new_level = 1;\n\t\tmddev->new_layout = 0;\n\t\tmddev->new_chunk_sectors = 0;\n\t\tconf = setup_conf(mddev);\n\t\tif (!IS_ERR(conf)) {\n\t\t\t \n\t\t\tconf->array_frozen = 1;\n\t\t\tmddev_clear_unsupported_flags(mddev,\n\t\t\t\tUNSUPPORTED_MDDEV_FLAGS);\n\t\t}\n\t\treturn conf;\n\t}\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic struct md_personality raid1_personality =\n{\n\t.name\t\t= \"raid1\",\n\t.level\t\t= 1,\n\t.owner\t\t= THIS_MODULE,\n\t.make_request\t= raid1_make_request,\n\t.run\t\t= raid1_run,\n\t.free\t\t= raid1_free,\n\t.status\t\t= raid1_status,\n\t.error_handler\t= raid1_error,\n\t.hot_add_disk\t= raid1_add_disk,\n\t.hot_remove_disk= raid1_remove_disk,\n\t.spare_active\t= raid1_spare_active,\n\t.sync_request\t= raid1_sync_request,\n\t.resize\t\t= raid1_resize,\n\t.size\t\t= raid1_size,\n\t.check_reshape\t= raid1_reshape,\n\t.quiesce\t= raid1_quiesce,\n\t.takeover\t= raid1_takeover,\n};\n\nstatic int __init raid_init(void)\n{\n\treturn register_md_personality(&raid1_personality);\n}\n\nstatic void raid_exit(void)\n{\n\tunregister_md_personality(&raid1_personality);\n}\n\nmodule_init(raid_init);\nmodule_exit(raid_exit);\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"RAID1 (mirroring) personality for MD\");\nMODULE_ALIAS(\"md-personality-3\");  \nMODULE_ALIAS(\"md-raid1\");\nMODULE_ALIAS(\"md-level-1\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}