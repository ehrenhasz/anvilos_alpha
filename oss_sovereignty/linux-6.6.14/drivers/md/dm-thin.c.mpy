{
  "module_name": "dm-thin.c",
  "hash_id": "498c85c8ed41714c283f9b31378e27ccdb27be9e97ac39a1060716227fbd81b3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-thin.c",
  "human_readable_source": "\n \n\n#include \"dm-thin-metadata.h\"\n#include \"dm-bio-prison-v1.h\"\n#include \"dm.h\"\n\n#include <linux/device-mapper.h>\n#include <linux/dm-io.h>\n#include <linux/dm-kcopyd.h>\n#include <linux/jiffies.h>\n#include <linux/log2.h>\n#include <linux/list.h>\n#include <linux/rculist.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/sort.h>\n#include <linux/rbtree.h>\n\n#define\tDM_MSG_PREFIX\t\"thin\"\n\n \n#define ENDIO_HOOK_POOL_SIZE 1024\n#define MAPPING_POOL_SIZE 1024\n#define COMMIT_PERIOD HZ\n#define NO_SPACE_TIMEOUT_SECS 60\n\nstatic unsigned int no_space_timeout_secs = NO_SPACE_TIMEOUT_SECS;\n\nDECLARE_DM_KCOPYD_THROTTLE_WITH_MODULE_PARM(snapshot_copy_throttle,\n\t\t\"A percentage of time allocated for copy on write\");\n\n \n#define DATA_DEV_BLOCK_SIZE_MIN_SECTORS (64 * 1024 >> SECTOR_SHIFT)\n#define DATA_DEV_BLOCK_SIZE_MAX_SECTORS (1024 * 1024 * 1024 >> SECTOR_SHIFT)\n\n \n#define MAX_DEV_ID ((1 << 24) - 1)\n\n \n\n \n\n \nenum lock_space {\n\tVIRTUAL,\n\tPHYSICAL\n};\n\nstatic bool build_key(struct dm_thin_device *td, enum lock_space ls,\n\t\t      dm_block_t b, dm_block_t e, struct dm_cell_key *key)\n{\n\tkey->virtual = (ls == VIRTUAL);\n\tkey->dev = dm_thin_dev_id(td);\n\tkey->block_begin = b;\n\tkey->block_end = e;\n\n\treturn dm_cell_key_has_valid_range(key);\n}\n\nstatic void build_data_key(struct dm_thin_device *td, dm_block_t b,\n\t\t\t   struct dm_cell_key *key)\n{\n\t(void) build_key(td, PHYSICAL, b, b + 1llu, key);\n}\n\nstatic void build_virtual_key(struct dm_thin_device *td, dm_block_t b,\n\t\t\t      struct dm_cell_key *key)\n{\n\t(void) build_key(td, VIRTUAL, b, b + 1llu, key);\n}\n\n \n\n#define THROTTLE_THRESHOLD (1 * HZ)\n\nstruct throttle {\n\tstruct rw_semaphore lock;\n\tunsigned long threshold;\n\tbool throttle_applied;\n};\n\nstatic void throttle_init(struct throttle *t)\n{\n\tinit_rwsem(&t->lock);\n\tt->throttle_applied = false;\n}\n\nstatic void throttle_work_start(struct throttle *t)\n{\n\tt->threshold = jiffies + THROTTLE_THRESHOLD;\n}\n\nstatic void throttle_work_update(struct throttle *t)\n{\n\tif (!t->throttle_applied && time_is_before_jiffies(t->threshold)) {\n\t\tdown_write(&t->lock);\n\t\tt->throttle_applied = true;\n\t}\n}\n\nstatic void throttle_work_complete(struct throttle *t)\n{\n\tif (t->throttle_applied) {\n\t\tt->throttle_applied = false;\n\t\tup_write(&t->lock);\n\t}\n}\n\nstatic void throttle_lock(struct throttle *t)\n{\n\tdown_read(&t->lock);\n}\n\nstatic void throttle_unlock(struct throttle *t)\n{\n\tup_read(&t->lock);\n}\n\n \n\n \nstruct dm_thin_new_mapping;\n\n \nenum pool_mode {\n\tPM_WRITE,\t\t \n\tPM_OUT_OF_DATA_SPACE,\t \n\n\t \n\tPM_OUT_OF_METADATA_SPACE,\n\tPM_READ_ONLY,\t\t \n\n\tPM_FAIL,\t\t \n};\n\nstruct pool_features {\n\tenum pool_mode mode;\n\n\tbool zero_new_blocks:1;\n\tbool discard_enabled:1;\n\tbool discard_passdown:1;\n\tbool error_if_no_space:1;\n};\n\nstruct thin_c;\ntypedef void (*process_bio_fn)(struct thin_c *tc, struct bio *bio);\ntypedef void (*process_cell_fn)(struct thin_c *tc, struct dm_bio_prison_cell *cell);\ntypedef void (*process_mapping_fn)(struct dm_thin_new_mapping *m);\n\n#define CELL_SORT_ARRAY_SIZE 8192\n\nstruct pool {\n\tstruct list_head list;\n\tstruct dm_target *ti;\t \n\n\tstruct mapped_device *pool_md;\n\tstruct block_device *data_dev;\n\tstruct block_device *md_dev;\n\tstruct dm_pool_metadata *pmd;\n\n\tdm_block_t low_water_blocks;\n\tuint32_t sectors_per_block;\n\tint sectors_per_block_shift;\n\n\tstruct pool_features pf;\n\tbool low_water_triggered:1;\t \n\tbool suspended:1;\n\tbool out_of_data_space:1;\n\n\tstruct dm_bio_prison *prison;\n\tstruct dm_kcopyd_client *copier;\n\n\tstruct work_struct worker;\n\tstruct workqueue_struct *wq;\n\tstruct throttle throttle;\n\tstruct delayed_work waker;\n\tstruct delayed_work no_space_timeout;\n\n\tunsigned long last_commit_jiffies;\n\tunsigned int ref_count;\n\n\tspinlock_t lock;\n\tstruct bio_list deferred_flush_bios;\n\tstruct bio_list deferred_flush_completions;\n\tstruct list_head prepared_mappings;\n\tstruct list_head prepared_discards;\n\tstruct list_head prepared_discards_pt2;\n\tstruct list_head active_thins;\n\n\tstruct dm_deferred_set *shared_read_ds;\n\tstruct dm_deferred_set *all_io_ds;\n\n\tstruct dm_thin_new_mapping *next_mapping;\n\n\tprocess_bio_fn process_bio;\n\tprocess_bio_fn process_discard;\n\n\tprocess_cell_fn process_cell;\n\tprocess_cell_fn process_discard_cell;\n\n\tprocess_mapping_fn process_prepared_mapping;\n\tprocess_mapping_fn process_prepared_discard;\n\tprocess_mapping_fn process_prepared_discard_pt2;\n\n\tstruct dm_bio_prison_cell **cell_sort_array;\n\n\tmempool_t mapping_pool;\n};\n\nstatic void metadata_operation_failed(struct pool *pool, const char *op, int r);\n\nstatic enum pool_mode get_pool_mode(struct pool *pool)\n{\n\treturn pool->pf.mode;\n}\n\nstatic void notify_of_pool_mode_change(struct pool *pool)\n{\n\tstatic const char *descs[] = {\n\t\t\"write\",\n\t\t\"out-of-data-space\",\n\t\t\"read-only\",\n\t\t\"read-only\",\n\t\t\"fail\"\n\t};\n\tconst char *extra_desc = NULL;\n\tenum pool_mode mode = get_pool_mode(pool);\n\n\tif (mode == PM_OUT_OF_DATA_SPACE) {\n\t\tif (!pool->pf.error_if_no_space)\n\t\t\textra_desc = \" (queue IO)\";\n\t\telse\n\t\t\textra_desc = \" (error IO)\";\n\t}\n\n\tdm_table_event(pool->ti->table);\n\tDMINFO(\"%s: switching pool to %s%s mode\",\n\t       dm_device_name(pool->pool_md),\n\t       descs[(int)mode], extra_desc ? : \"\");\n}\n\n \nstruct pool_c {\n\tstruct dm_target *ti;\n\tstruct pool *pool;\n\tstruct dm_dev *data_dev;\n\tstruct dm_dev *metadata_dev;\n\n\tdm_block_t low_water_blocks;\n\tstruct pool_features requested_pf;  \n\tstruct pool_features adjusted_pf;   \n};\n\n \nstruct thin_c {\n\tstruct list_head list;\n\tstruct dm_dev *pool_dev;\n\tstruct dm_dev *origin_dev;\n\tsector_t origin_size;\n\tdm_thin_id dev_id;\n\n\tstruct pool *pool;\n\tstruct dm_thin_device *td;\n\tstruct mapped_device *thin_md;\n\n\tbool requeue_mode:1;\n\tspinlock_t lock;\n\tstruct list_head deferred_cells;\n\tstruct bio_list deferred_bio_list;\n\tstruct bio_list retry_on_resume_list;\n\tstruct rb_root sort_bio_list;  \n\n\t \n\trefcount_t refcount;\n\tstruct completion can_destroy;\n};\n\n \n\nstatic bool block_size_is_power_of_two(struct pool *pool)\n{\n\treturn pool->sectors_per_block_shift >= 0;\n}\n\nstatic sector_t block_to_sectors(struct pool *pool, dm_block_t b)\n{\n\treturn block_size_is_power_of_two(pool) ?\n\t\t(b << pool->sectors_per_block_shift) :\n\t\t(b * pool->sectors_per_block);\n}\n\n \n\nstruct discard_op {\n\tstruct thin_c *tc;\n\tstruct blk_plug plug;\n\tstruct bio *parent_bio;\n\tstruct bio *bio;\n};\n\nstatic void begin_discard(struct discard_op *op, struct thin_c *tc, struct bio *parent)\n{\n\tBUG_ON(!parent);\n\n\top->tc = tc;\n\tblk_start_plug(&op->plug);\n\top->parent_bio = parent;\n\top->bio = NULL;\n}\n\nstatic int issue_discard(struct discard_op *op, dm_block_t data_b, dm_block_t data_e)\n{\n\tstruct thin_c *tc = op->tc;\n\tsector_t s = block_to_sectors(tc->pool, data_b);\n\tsector_t len = block_to_sectors(tc->pool, data_e - data_b);\n\n\treturn __blkdev_issue_discard(tc->pool_dev->bdev, s, len, GFP_NOIO, &op->bio);\n}\n\nstatic void end_discard(struct discard_op *op, int r)\n{\n\tif (op->bio) {\n\t\t \n\t\tbio_chain(op->bio, op->parent_bio);\n\t\top->bio->bi_opf = REQ_OP_DISCARD;\n\t\tsubmit_bio(op->bio);\n\t}\n\n\tblk_finish_plug(&op->plug);\n\n\t \n\tif (r && !op->parent_bio->bi_status)\n\t\top->parent_bio->bi_status = errno_to_blk_status(r);\n\tbio_endio(op->parent_bio);\n}\n\n \n\n \nstatic void wake_worker(struct pool *pool)\n{\n\tqueue_work(pool->wq, &pool->worker);\n}\n\n \n\nstatic int bio_detain(struct pool *pool, struct dm_cell_key *key, struct bio *bio,\n\t\t      struct dm_bio_prison_cell **cell_result)\n{\n\tint r;\n\tstruct dm_bio_prison_cell *cell_prealloc;\n\n\t \n\tcell_prealloc = dm_bio_prison_alloc_cell(pool->prison, GFP_NOIO);\n\n\tr = dm_bio_detain(pool->prison, key, bio, cell_prealloc, cell_result);\n\tif (r)\n\t\t \n\t\tdm_bio_prison_free_cell(pool->prison, cell_prealloc);\n\n\treturn r;\n}\n\nstatic void cell_release(struct pool *pool,\n\t\t\t struct dm_bio_prison_cell *cell,\n\t\t\t struct bio_list *bios)\n{\n\tdm_cell_release(pool->prison, cell, bios);\n\tdm_bio_prison_free_cell(pool->prison, cell);\n}\n\nstatic void cell_visit_release(struct pool *pool,\n\t\t\t       void (*fn)(void *, struct dm_bio_prison_cell *),\n\t\t\t       void *context,\n\t\t\t       struct dm_bio_prison_cell *cell)\n{\n\tdm_cell_visit_release(pool->prison, fn, context, cell);\n\tdm_bio_prison_free_cell(pool->prison, cell);\n}\n\nstatic void cell_release_no_holder(struct pool *pool,\n\t\t\t\t   struct dm_bio_prison_cell *cell,\n\t\t\t\t   struct bio_list *bios)\n{\n\tdm_cell_release_no_holder(pool->prison, cell, bios);\n\tdm_bio_prison_free_cell(pool->prison, cell);\n}\n\nstatic void cell_error_with_code(struct pool *pool,\n\t\tstruct dm_bio_prison_cell *cell, blk_status_t error_code)\n{\n\tdm_cell_error(pool->prison, cell, error_code);\n\tdm_bio_prison_free_cell(pool->prison, cell);\n}\n\nstatic blk_status_t get_pool_io_error_code(struct pool *pool)\n{\n\treturn pool->out_of_data_space ? BLK_STS_NOSPC : BLK_STS_IOERR;\n}\n\nstatic void cell_error(struct pool *pool, struct dm_bio_prison_cell *cell)\n{\n\tcell_error_with_code(pool, cell, get_pool_io_error_code(pool));\n}\n\nstatic void cell_success(struct pool *pool, struct dm_bio_prison_cell *cell)\n{\n\tcell_error_with_code(pool, cell, 0);\n}\n\nstatic void cell_requeue(struct pool *pool, struct dm_bio_prison_cell *cell)\n{\n\tcell_error_with_code(pool, cell, BLK_STS_DM_REQUEUE);\n}\n\n \n\n \nstatic struct dm_thin_pool_table {\n\tstruct mutex mutex;\n\tstruct list_head pools;\n} dm_thin_pool_table;\n\nstatic void pool_table_init(void)\n{\n\tmutex_init(&dm_thin_pool_table.mutex);\n\tINIT_LIST_HEAD(&dm_thin_pool_table.pools);\n}\n\nstatic void pool_table_exit(void)\n{\n\tmutex_destroy(&dm_thin_pool_table.mutex);\n}\n\nstatic void __pool_table_insert(struct pool *pool)\n{\n\tBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\n\tlist_add(&pool->list, &dm_thin_pool_table.pools);\n}\n\nstatic void __pool_table_remove(struct pool *pool)\n{\n\tBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\n\tlist_del(&pool->list);\n}\n\nstatic struct pool *__pool_table_lookup(struct mapped_device *md)\n{\n\tstruct pool *pool = NULL, *tmp;\n\n\tBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\n\n\tlist_for_each_entry(tmp, &dm_thin_pool_table.pools, list) {\n\t\tif (tmp->pool_md == md) {\n\t\t\tpool = tmp;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn pool;\n}\n\nstatic struct pool *__pool_table_lookup_metadata_dev(struct block_device *md_dev)\n{\n\tstruct pool *pool = NULL, *tmp;\n\n\tBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\n\n\tlist_for_each_entry(tmp, &dm_thin_pool_table.pools, list) {\n\t\tif (tmp->md_dev == md_dev) {\n\t\t\tpool = tmp;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn pool;\n}\n\n \n\nstruct dm_thin_endio_hook {\n\tstruct thin_c *tc;\n\tstruct dm_deferred_entry *shared_read_entry;\n\tstruct dm_deferred_entry *all_io_entry;\n\tstruct dm_thin_new_mapping *overwrite_mapping;\n\tstruct rb_node rb_node;\n\tstruct dm_bio_prison_cell *cell;\n};\n\nstatic void __merge_bio_list(struct bio_list *bios, struct bio_list *master)\n{\n\tbio_list_merge(bios, master);\n\tbio_list_init(master);\n}\n\nstatic void error_bio_list(struct bio_list *bios, blk_status_t error)\n{\n\tstruct bio *bio;\n\n\twhile ((bio = bio_list_pop(bios))) {\n\t\tbio->bi_status = error;\n\t\tbio_endio(bio);\n\t}\n}\n\nstatic void error_thin_bio_list(struct thin_c *tc, struct bio_list *master,\n\t\tblk_status_t error)\n{\n\tstruct bio_list bios;\n\n\tbio_list_init(&bios);\n\n\tspin_lock_irq(&tc->lock);\n\t__merge_bio_list(&bios, master);\n\tspin_unlock_irq(&tc->lock);\n\n\terror_bio_list(&bios, error);\n}\n\nstatic void requeue_deferred_cells(struct thin_c *tc)\n{\n\tstruct pool *pool = tc->pool;\n\tstruct list_head cells;\n\tstruct dm_bio_prison_cell *cell, *tmp;\n\n\tINIT_LIST_HEAD(&cells);\n\n\tspin_lock_irq(&tc->lock);\n\tlist_splice_init(&tc->deferred_cells, &cells);\n\tspin_unlock_irq(&tc->lock);\n\n\tlist_for_each_entry_safe(cell, tmp, &cells, user_list)\n\t\tcell_requeue(pool, cell);\n}\n\nstatic void requeue_io(struct thin_c *tc)\n{\n\tstruct bio_list bios;\n\n\tbio_list_init(&bios);\n\n\tspin_lock_irq(&tc->lock);\n\t__merge_bio_list(&bios, &tc->deferred_bio_list);\n\t__merge_bio_list(&bios, &tc->retry_on_resume_list);\n\tspin_unlock_irq(&tc->lock);\n\n\terror_bio_list(&bios, BLK_STS_DM_REQUEUE);\n\trequeue_deferred_cells(tc);\n}\n\nstatic void error_retry_list_with_code(struct pool *pool, blk_status_t error)\n{\n\tstruct thin_c *tc;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(tc, &pool->active_thins, list)\n\t\terror_thin_bio_list(tc, &tc->retry_on_resume_list, error);\n\trcu_read_unlock();\n}\n\nstatic void error_retry_list(struct pool *pool)\n{\n\terror_retry_list_with_code(pool, get_pool_io_error_code(pool));\n}\n\n \n\nstatic dm_block_t get_bio_block(struct thin_c *tc, struct bio *bio)\n{\n\tstruct pool *pool = tc->pool;\n\tsector_t block_nr = bio->bi_iter.bi_sector;\n\n\tif (block_size_is_power_of_two(pool))\n\t\tblock_nr >>= pool->sectors_per_block_shift;\n\telse\n\t\t(void) sector_div(block_nr, pool->sectors_per_block);\n\n\treturn block_nr;\n}\n\n \nstatic void get_bio_block_range(struct thin_c *tc, struct bio *bio,\n\t\t\t\tdm_block_t *begin, dm_block_t *end)\n{\n\tstruct pool *pool = tc->pool;\n\tsector_t b = bio->bi_iter.bi_sector;\n\tsector_t e = b + (bio->bi_iter.bi_size >> SECTOR_SHIFT);\n\n\tb += pool->sectors_per_block - 1ull;  \n\n\tif (block_size_is_power_of_two(pool)) {\n\t\tb >>= pool->sectors_per_block_shift;\n\t\te >>= pool->sectors_per_block_shift;\n\t} else {\n\t\t(void) sector_div(b, pool->sectors_per_block);\n\t\t(void) sector_div(e, pool->sectors_per_block);\n\t}\n\n\tif (e < b)\n\t\t \n\t\te = b;\n\n\t*begin = b;\n\t*end = e;\n}\n\nstatic void remap(struct thin_c *tc, struct bio *bio, dm_block_t block)\n{\n\tstruct pool *pool = tc->pool;\n\tsector_t bi_sector = bio->bi_iter.bi_sector;\n\n\tbio_set_dev(bio, tc->pool_dev->bdev);\n\tif (block_size_is_power_of_two(pool))\n\t\tbio->bi_iter.bi_sector =\n\t\t\t(block << pool->sectors_per_block_shift) |\n\t\t\t(bi_sector & (pool->sectors_per_block - 1));\n\telse\n\t\tbio->bi_iter.bi_sector = (block * pool->sectors_per_block) +\n\t\t\t\t sector_div(bi_sector, pool->sectors_per_block);\n}\n\nstatic void remap_to_origin(struct thin_c *tc, struct bio *bio)\n{\n\tbio_set_dev(bio, tc->origin_dev->bdev);\n}\n\nstatic int bio_triggers_commit(struct thin_c *tc, struct bio *bio)\n{\n\treturn op_is_flush(bio->bi_opf) &&\n\t\tdm_thin_changed_this_transaction(tc->td);\n}\n\nstatic void inc_all_io_entry(struct pool *pool, struct bio *bio)\n{\n\tstruct dm_thin_endio_hook *h;\n\n\tif (bio_op(bio) == REQ_OP_DISCARD)\n\t\treturn;\n\n\th = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\n\th->all_io_entry = dm_deferred_entry_inc(pool->all_io_ds);\n}\n\nstatic void issue(struct thin_c *tc, struct bio *bio)\n{\n\tstruct pool *pool = tc->pool;\n\n\tif (!bio_triggers_commit(tc, bio)) {\n\t\tdm_submit_bio_remap(bio, NULL);\n\t\treturn;\n\t}\n\n\t \n\tif (dm_thin_aborted_changes(tc->td)) {\n\t\tbio_io_error(bio);\n\t\treturn;\n\t}\n\n\t \n\tspin_lock_irq(&pool->lock);\n\tbio_list_add(&pool->deferred_flush_bios, bio);\n\tspin_unlock_irq(&pool->lock);\n}\n\nstatic void remap_to_origin_and_issue(struct thin_c *tc, struct bio *bio)\n{\n\tremap_to_origin(tc, bio);\n\tissue(tc, bio);\n}\n\nstatic void remap_and_issue(struct thin_c *tc, struct bio *bio,\n\t\t\t    dm_block_t block)\n{\n\tremap(tc, bio, block);\n\tissue(tc, bio);\n}\n\n \n\n \nstruct dm_thin_new_mapping {\n\tstruct list_head list;\n\n\tbool pass_discard:1;\n\tbool maybe_shared:1;\n\n\t \n\tatomic_t prepare_actions;\n\n\tblk_status_t status;\n\tstruct thin_c *tc;\n\tdm_block_t virt_begin, virt_end;\n\tdm_block_t data_block;\n\tstruct dm_bio_prison_cell *cell;\n\n\t \n\tstruct bio *bio;\n\tbio_end_io_t *saved_bi_end_io;\n};\n\nstatic void __complete_mapping_preparation(struct dm_thin_new_mapping *m)\n{\n\tstruct pool *pool = m->tc->pool;\n\n\tif (atomic_dec_and_test(&m->prepare_actions)) {\n\t\tlist_add_tail(&m->list, &pool->prepared_mappings);\n\t\twake_worker(pool);\n\t}\n}\n\nstatic void complete_mapping_preparation(struct dm_thin_new_mapping *m)\n{\n\tunsigned long flags;\n\tstruct pool *pool = m->tc->pool;\n\n\tspin_lock_irqsave(&pool->lock, flags);\n\t__complete_mapping_preparation(m);\n\tspin_unlock_irqrestore(&pool->lock, flags);\n}\n\nstatic void copy_complete(int read_err, unsigned long write_err, void *context)\n{\n\tstruct dm_thin_new_mapping *m = context;\n\n\tm->status = read_err || write_err ? BLK_STS_IOERR : 0;\n\tcomplete_mapping_preparation(m);\n}\n\nstatic void overwrite_endio(struct bio *bio)\n{\n\tstruct dm_thin_endio_hook *h = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\n\tstruct dm_thin_new_mapping *m = h->overwrite_mapping;\n\n\tbio->bi_end_io = m->saved_bi_end_io;\n\n\tm->status = bio->bi_status;\n\tcomplete_mapping_preparation(m);\n}\n\n \n\n \n\n \n\n \nstatic void cell_defer_no_holder(struct thin_c *tc, struct dm_bio_prison_cell *cell)\n{\n\tstruct pool *pool = tc->pool;\n\tunsigned long flags;\n\tstruct bio_list bios;\n\n\tbio_list_init(&bios);\n\tcell_release_no_holder(pool, cell, &bios);\n\n\tif (!bio_list_empty(&bios)) {\n\t\tspin_lock_irqsave(&tc->lock, flags);\n\t\tbio_list_merge(&tc->deferred_bio_list, &bios);\n\t\tspin_unlock_irqrestore(&tc->lock, flags);\n\t\twake_worker(pool);\n\t}\n}\n\nstatic void thin_defer_bio(struct thin_c *tc, struct bio *bio);\n\nstruct remap_info {\n\tstruct thin_c *tc;\n\tstruct bio_list defer_bios;\n\tstruct bio_list issue_bios;\n};\n\nstatic void __inc_remap_and_issue_cell(void *context,\n\t\t\t\t       struct dm_bio_prison_cell *cell)\n{\n\tstruct remap_info *info = context;\n\tstruct bio *bio;\n\n\twhile ((bio = bio_list_pop(&cell->bios))) {\n\t\tif (op_is_flush(bio->bi_opf) || bio_op(bio) == REQ_OP_DISCARD)\n\t\t\tbio_list_add(&info->defer_bios, bio);\n\t\telse {\n\t\t\tinc_all_io_entry(info->tc->pool, bio);\n\n\t\t\t \n\t\t\tbio_list_add(&info->issue_bios, bio);\n\t\t}\n\t}\n}\n\nstatic void inc_remap_and_issue_cell(struct thin_c *tc,\n\t\t\t\t     struct dm_bio_prison_cell *cell,\n\t\t\t\t     dm_block_t block)\n{\n\tstruct bio *bio;\n\tstruct remap_info info;\n\n\tinfo.tc = tc;\n\tbio_list_init(&info.defer_bios);\n\tbio_list_init(&info.issue_bios);\n\n\t \n\tcell_visit_release(tc->pool, __inc_remap_and_issue_cell,\n\t\t\t   &info, cell);\n\n\twhile ((bio = bio_list_pop(&info.defer_bios)))\n\t\tthin_defer_bio(tc, bio);\n\n\twhile ((bio = bio_list_pop(&info.issue_bios)))\n\t\tremap_and_issue(info.tc, bio, block);\n}\n\nstatic void process_prepared_mapping_fail(struct dm_thin_new_mapping *m)\n{\n\tcell_error(m->tc->pool, m->cell);\n\tlist_del(&m->list);\n\tmempool_free(m, &m->tc->pool->mapping_pool);\n}\n\nstatic void complete_overwrite_bio(struct thin_c *tc, struct bio *bio)\n{\n\tstruct pool *pool = tc->pool;\n\n\t \n\tif (!bio_triggers_commit(tc, bio)) {\n\t\tbio_endio(bio);\n\t\treturn;\n\t}\n\n\t \n\tif (dm_thin_aborted_changes(tc->td)) {\n\t\tbio_io_error(bio);\n\t\treturn;\n\t}\n\n\t \n\tspin_lock_irq(&pool->lock);\n\tbio_list_add(&pool->deferred_flush_completions, bio);\n\tspin_unlock_irq(&pool->lock);\n}\n\nstatic void process_prepared_mapping(struct dm_thin_new_mapping *m)\n{\n\tstruct thin_c *tc = m->tc;\n\tstruct pool *pool = tc->pool;\n\tstruct bio *bio = m->bio;\n\tint r;\n\n\tif (m->status) {\n\t\tcell_error(pool, m->cell);\n\t\tgoto out;\n\t}\n\n\t \n\tr = dm_thin_insert_block(tc->td, m->virt_begin, m->data_block);\n\tif (r) {\n\t\tmetadata_operation_failed(pool, \"dm_thin_insert_block\", r);\n\t\tcell_error(pool, m->cell);\n\t\tgoto out;\n\t}\n\n\t \n\tif (bio) {\n\t\tinc_remap_and_issue_cell(tc, m->cell, m->data_block);\n\t\tcomplete_overwrite_bio(tc, bio);\n\t} else {\n\t\tinc_all_io_entry(tc->pool, m->cell->holder);\n\t\tremap_and_issue(tc, m->cell->holder, m->data_block);\n\t\tinc_remap_and_issue_cell(tc, m->cell, m->data_block);\n\t}\n\nout:\n\tlist_del(&m->list);\n\tmempool_free(m, &pool->mapping_pool);\n}\n\n \n\nstatic void free_discard_mapping(struct dm_thin_new_mapping *m)\n{\n\tstruct thin_c *tc = m->tc;\n\n\tif (m->cell)\n\t\tcell_defer_no_holder(tc, m->cell);\n\tmempool_free(m, &tc->pool->mapping_pool);\n}\n\nstatic void process_prepared_discard_fail(struct dm_thin_new_mapping *m)\n{\n\tbio_io_error(m->bio);\n\tfree_discard_mapping(m);\n}\n\nstatic void process_prepared_discard_success(struct dm_thin_new_mapping *m)\n{\n\tbio_endio(m->bio);\n\tfree_discard_mapping(m);\n}\n\nstatic void process_prepared_discard_no_passdown(struct dm_thin_new_mapping *m)\n{\n\tint r;\n\tstruct thin_c *tc = m->tc;\n\n\tr = dm_thin_remove_range(tc->td, m->cell->key.block_begin, m->cell->key.block_end);\n\tif (r) {\n\t\tmetadata_operation_failed(tc->pool, \"dm_thin_remove_range\", r);\n\t\tbio_io_error(m->bio);\n\t} else\n\t\tbio_endio(m->bio);\n\n\tcell_defer_no_holder(tc, m->cell);\n\tmempool_free(m, &tc->pool->mapping_pool);\n}\n\n \n\nstatic void passdown_double_checking_shared_status(struct dm_thin_new_mapping *m,\n\t\t\t\t\t\t   struct bio *discard_parent)\n{\n\t \n\tint r = 0;\n\tbool shared = true;\n\tstruct thin_c *tc = m->tc;\n\tstruct pool *pool = tc->pool;\n\tdm_block_t b = m->data_block, e, end = m->data_block + m->virt_end - m->virt_begin;\n\tstruct discard_op op;\n\n\tbegin_discard(&op, tc, discard_parent);\n\twhile (b != end) {\n\t\t \n\t\tfor (; b < end; b++) {\n\t\t\tr = dm_pool_block_is_shared(pool->pmd, b, &shared);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\n\t\t\tif (!shared)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (b == end)\n\t\t\tbreak;\n\n\t\t \n\t\tfor (e = b + 1; e != end; e++) {\n\t\t\tr = dm_pool_block_is_shared(pool->pmd, e, &shared);\n\t\t\tif (r)\n\t\t\t\tgoto out;\n\n\t\t\tif (shared)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tr = issue_discard(&op, b, e);\n\t\tif (r)\n\t\t\tgoto out;\n\n\t\tb = e;\n\t}\nout:\n\tend_discard(&op, r);\n}\n\nstatic void queue_passdown_pt2(struct dm_thin_new_mapping *m)\n{\n\tunsigned long flags;\n\tstruct pool *pool = m->tc->pool;\n\n\tspin_lock_irqsave(&pool->lock, flags);\n\tlist_add_tail(&m->list, &pool->prepared_discards_pt2);\n\tspin_unlock_irqrestore(&pool->lock, flags);\n\twake_worker(pool);\n}\n\nstatic void passdown_endio(struct bio *bio)\n{\n\t \n\tqueue_passdown_pt2(bio->bi_private);\n\tbio_put(bio);\n}\n\nstatic void process_prepared_discard_passdown_pt1(struct dm_thin_new_mapping *m)\n{\n\tint r;\n\tstruct thin_c *tc = m->tc;\n\tstruct pool *pool = tc->pool;\n\tstruct bio *discard_parent;\n\tdm_block_t data_end = m->data_block + (m->virt_end - m->virt_begin);\n\n\t \n\tr = dm_thin_remove_range(tc->td, m->virt_begin, m->virt_end);\n\tif (r) {\n\t\tmetadata_operation_failed(pool, \"dm_thin_remove_range\", r);\n\t\tbio_io_error(m->bio);\n\t\tcell_defer_no_holder(tc, m->cell);\n\t\tmempool_free(m, &pool->mapping_pool);\n\t\treturn;\n\t}\n\n\t \n\tr = dm_pool_inc_data_range(pool->pmd, m->data_block, data_end);\n\tif (r) {\n\t\tmetadata_operation_failed(pool, \"dm_pool_inc_data_range\", r);\n\t\tbio_io_error(m->bio);\n\t\tcell_defer_no_holder(tc, m->cell);\n\t\tmempool_free(m, &pool->mapping_pool);\n\t\treturn;\n\t}\n\n\tdiscard_parent = bio_alloc(NULL, 1, 0, GFP_NOIO);\n\tdiscard_parent->bi_end_io = passdown_endio;\n\tdiscard_parent->bi_private = m;\n\tif (m->maybe_shared)\n\t\tpassdown_double_checking_shared_status(m, discard_parent);\n\telse {\n\t\tstruct discard_op op;\n\n\t\tbegin_discard(&op, tc, discard_parent);\n\t\tr = issue_discard(&op, m->data_block, data_end);\n\t\tend_discard(&op, r);\n\t}\n}\n\nstatic void process_prepared_discard_passdown_pt2(struct dm_thin_new_mapping *m)\n{\n\tint r;\n\tstruct thin_c *tc = m->tc;\n\tstruct pool *pool = tc->pool;\n\n\t \n\tr = dm_pool_dec_data_range(pool->pmd, m->data_block,\n\t\t\t\t   m->data_block + (m->virt_end - m->virt_begin));\n\tif (r) {\n\t\tmetadata_operation_failed(pool, \"dm_pool_dec_data_range\", r);\n\t\tbio_io_error(m->bio);\n\t} else\n\t\tbio_endio(m->bio);\n\n\tcell_defer_no_holder(tc, m->cell);\n\tmempool_free(m, &pool->mapping_pool);\n}\n\nstatic void process_prepared(struct pool *pool, struct list_head *head,\n\t\t\t     process_mapping_fn *fn)\n{\n\tstruct list_head maps;\n\tstruct dm_thin_new_mapping *m, *tmp;\n\n\tINIT_LIST_HEAD(&maps);\n\tspin_lock_irq(&pool->lock);\n\tlist_splice_init(head, &maps);\n\tspin_unlock_irq(&pool->lock);\n\n\tlist_for_each_entry_safe(m, tmp, &maps, list)\n\t\t(*fn)(m);\n}\n\n \nstatic int io_overlaps_block(struct pool *pool, struct bio *bio)\n{\n\treturn bio->bi_iter.bi_size ==\n\t\t(pool->sectors_per_block << SECTOR_SHIFT);\n}\n\nstatic int io_overwrites_block(struct pool *pool, struct bio *bio)\n{\n\treturn (bio_data_dir(bio) == WRITE) &&\n\t\tio_overlaps_block(pool, bio);\n}\n\nstatic void save_and_set_endio(struct bio *bio, bio_end_io_t **save,\n\t\t\t       bio_end_io_t *fn)\n{\n\t*save = bio->bi_end_io;\n\tbio->bi_end_io = fn;\n}\n\nstatic int ensure_next_mapping(struct pool *pool)\n{\n\tif (pool->next_mapping)\n\t\treturn 0;\n\n\tpool->next_mapping = mempool_alloc(&pool->mapping_pool, GFP_ATOMIC);\n\n\treturn pool->next_mapping ? 0 : -ENOMEM;\n}\n\nstatic struct dm_thin_new_mapping *get_next_mapping(struct pool *pool)\n{\n\tstruct dm_thin_new_mapping *m = pool->next_mapping;\n\n\tBUG_ON(!pool->next_mapping);\n\n\tmemset(m, 0, sizeof(struct dm_thin_new_mapping));\n\tINIT_LIST_HEAD(&m->list);\n\tm->bio = NULL;\n\n\tpool->next_mapping = NULL;\n\n\treturn m;\n}\n\nstatic void ll_zero(struct thin_c *tc, struct dm_thin_new_mapping *m,\n\t\t    sector_t begin, sector_t end)\n{\n\tstruct dm_io_region to;\n\n\tto.bdev = tc->pool_dev->bdev;\n\tto.sector = begin;\n\tto.count = end - begin;\n\n\tdm_kcopyd_zero(tc->pool->copier, 1, &to, 0, copy_complete, m);\n}\n\nstatic void remap_and_issue_overwrite(struct thin_c *tc, struct bio *bio,\n\t\t\t\t      dm_block_t data_begin,\n\t\t\t\t      struct dm_thin_new_mapping *m)\n{\n\tstruct pool *pool = tc->pool;\n\tstruct dm_thin_endio_hook *h = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\n\n\th->overwrite_mapping = m;\n\tm->bio = bio;\n\tsave_and_set_endio(bio, &m->saved_bi_end_io, overwrite_endio);\n\tinc_all_io_entry(pool, bio);\n\tremap_and_issue(tc, bio, data_begin);\n}\n\n \nstatic void schedule_copy(struct thin_c *tc, dm_block_t virt_block,\n\t\t\t  struct dm_dev *origin, dm_block_t data_origin,\n\t\t\t  dm_block_t data_dest,\n\t\t\t  struct dm_bio_prison_cell *cell, struct bio *bio,\n\t\t\t  sector_t len)\n{\n\tstruct pool *pool = tc->pool;\n\tstruct dm_thin_new_mapping *m = get_next_mapping(pool);\n\n\tm->tc = tc;\n\tm->virt_begin = virt_block;\n\tm->virt_end = virt_block + 1u;\n\tm->data_block = data_dest;\n\tm->cell = cell;\n\n\t \n\tatomic_set(&m->prepare_actions, 3);\n\n\tif (!dm_deferred_set_add_work(pool->shared_read_ds, &m->list))\n\t\tcomplete_mapping_preparation(m);  \n\n\t \n\tif (io_overwrites_block(pool, bio))\n\t\tremap_and_issue_overwrite(tc, bio, data_dest, m);\n\telse {\n\t\tstruct dm_io_region from, to;\n\n\t\tfrom.bdev = origin->bdev;\n\t\tfrom.sector = data_origin * pool->sectors_per_block;\n\t\tfrom.count = len;\n\n\t\tto.bdev = tc->pool_dev->bdev;\n\t\tto.sector = data_dest * pool->sectors_per_block;\n\t\tto.count = len;\n\n\t\tdm_kcopyd_copy(pool->copier, &from, 1, &to,\n\t\t\t       0, copy_complete, m);\n\n\t\t \n\t\tif (len < pool->sectors_per_block && pool->pf.zero_new_blocks) {\n\t\t\tatomic_inc(&m->prepare_actions);\n\t\t\tll_zero(tc, m,\n\t\t\t\tdata_dest * pool->sectors_per_block + len,\n\t\t\t\t(data_dest + 1) * pool->sectors_per_block);\n\t\t}\n\t}\n\n\tcomplete_mapping_preparation(m);  \n}\n\nstatic void schedule_internal_copy(struct thin_c *tc, dm_block_t virt_block,\n\t\t\t\t   dm_block_t data_origin, dm_block_t data_dest,\n\t\t\t\t   struct dm_bio_prison_cell *cell, struct bio *bio)\n{\n\tschedule_copy(tc, virt_block, tc->pool_dev,\n\t\t      data_origin, data_dest, cell, bio,\n\t\t      tc->pool->sectors_per_block);\n}\n\nstatic void schedule_zero(struct thin_c *tc, dm_block_t virt_block,\n\t\t\t  dm_block_t data_block, struct dm_bio_prison_cell *cell,\n\t\t\t  struct bio *bio)\n{\n\tstruct pool *pool = tc->pool;\n\tstruct dm_thin_new_mapping *m = get_next_mapping(pool);\n\n\tatomic_set(&m->prepare_actions, 1);  \n\tm->tc = tc;\n\tm->virt_begin = virt_block;\n\tm->virt_end = virt_block + 1u;\n\tm->data_block = data_block;\n\tm->cell = cell;\n\n\t \n\tif (pool->pf.zero_new_blocks) {\n\t\tif (io_overwrites_block(pool, bio))\n\t\t\tremap_and_issue_overwrite(tc, bio, data_block, m);\n\t\telse\n\t\t\tll_zero(tc, m, data_block * pool->sectors_per_block,\n\t\t\t\t(data_block + 1) * pool->sectors_per_block);\n\t} else\n\t\tprocess_prepared_mapping(m);\n}\n\nstatic void schedule_external_copy(struct thin_c *tc, dm_block_t virt_block,\n\t\t\t\t   dm_block_t data_dest,\n\t\t\t\t   struct dm_bio_prison_cell *cell, struct bio *bio)\n{\n\tstruct pool *pool = tc->pool;\n\tsector_t virt_block_begin = virt_block * pool->sectors_per_block;\n\tsector_t virt_block_end = (virt_block + 1) * pool->sectors_per_block;\n\n\tif (virt_block_end <= tc->origin_size)\n\t\tschedule_copy(tc, virt_block, tc->origin_dev,\n\t\t\t      virt_block, data_dest, cell, bio,\n\t\t\t      pool->sectors_per_block);\n\n\telse if (virt_block_begin < tc->origin_size)\n\t\tschedule_copy(tc, virt_block, tc->origin_dev,\n\t\t\t      virt_block, data_dest, cell, bio,\n\t\t\t      tc->origin_size - virt_block_begin);\n\n\telse\n\t\tschedule_zero(tc, virt_block, data_dest, cell, bio);\n}\n\nstatic void set_pool_mode(struct pool *pool, enum pool_mode new_mode);\n\nstatic void requeue_bios(struct pool *pool);\n\nstatic bool is_read_only_pool_mode(enum pool_mode mode)\n{\n\treturn (mode == PM_OUT_OF_METADATA_SPACE || mode == PM_READ_ONLY);\n}\n\nstatic bool is_read_only(struct pool *pool)\n{\n\treturn is_read_only_pool_mode(get_pool_mode(pool));\n}\n\nstatic void check_for_metadata_space(struct pool *pool)\n{\n\tint r;\n\tconst char *ooms_reason = NULL;\n\tdm_block_t nr_free;\n\n\tr = dm_pool_get_free_metadata_block_count(pool->pmd, &nr_free);\n\tif (r)\n\t\tooms_reason = \"Could not get free metadata blocks\";\n\telse if (!nr_free)\n\t\tooms_reason = \"No free metadata blocks\";\n\n\tif (ooms_reason && !is_read_only(pool)) {\n\t\tDMERR(\"%s\", ooms_reason);\n\t\tset_pool_mode(pool, PM_OUT_OF_METADATA_SPACE);\n\t}\n}\n\nstatic void check_for_data_space(struct pool *pool)\n{\n\tint r;\n\tdm_block_t nr_free;\n\n\tif (get_pool_mode(pool) != PM_OUT_OF_DATA_SPACE)\n\t\treturn;\n\n\tr = dm_pool_get_free_block_count(pool->pmd, &nr_free);\n\tif (r)\n\t\treturn;\n\n\tif (nr_free) {\n\t\tset_pool_mode(pool, PM_WRITE);\n\t\trequeue_bios(pool);\n\t}\n}\n\n \nstatic int commit(struct pool *pool)\n{\n\tint r;\n\n\tif (get_pool_mode(pool) >= PM_OUT_OF_METADATA_SPACE)\n\t\treturn -EINVAL;\n\n\tr = dm_pool_commit_metadata(pool->pmd);\n\tif (r)\n\t\tmetadata_operation_failed(pool, \"dm_pool_commit_metadata\", r);\n\telse {\n\t\tcheck_for_metadata_space(pool);\n\t\tcheck_for_data_space(pool);\n\t}\n\n\treturn r;\n}\n\nstatic void check_low_water_mark(struct pool *pool, dm_block_t free_blocks)\n{\n\tif (free_blocks <= pool->low_water_blocks && !pool->low_water_triggered) {\n\t\tDMWARN(\"%s: reached low water mark for data device: sending event.\",\n\t\t       dm_device_name(pool->pool_md));\n\t\tspin_lock_irq(&pool->lock);\n\t\tpool->low_water_triggered = true;\n\t\tspin_unlock_irq(&pool->lock);\n\t\tdm_table_event(pool->ti->table);\n\t}\n}\n\nstatic int alloc_data_block(struct thin_c *tc, dm_block_t *result)\n{\n\tint r;\n\tdm_block_t free_blocks;\n\tstruct pool *pool = tc->pool;\n\n\tif (WARN_ON(get_pool_mode(pool) != PM_WRITE))\n\t\treturn -EINVAL;\n\n\tr = dm_pool_get_free_block_count(pool->pmd, &free_blocks);\n\tif (r) {\n\t\tmetadata_operation_failed(pool, \"dm_pool_get_free_block_count\", r);\n\t\treturn r;\n\t}\n\n\tcheck_low_water_mark(pool, free_blocks);\n\n\tif (!free_blocks) {\n\t\t \n\t\tr = commit(pool);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tr = dm_pool_get_free_block_count(pool->pmd, &free_blocks);\n\t\tif (r) {\n\t\t\tmetadata_operation_failed(pool, \"dm_pool_get_free_block_count\", r);\n\t\t\treturn r;\n\t\t}\n\n\t\tif (!free_blocks) {\n\t\t\tset_pool_mode(pool, PM_OUT_OF_DATA_SPACE);\n\t\t\treturn -ENOSPC;\n\t\t}\n\t}\n\n\tr = dm_pool_alloc_data_block(pool->pmd, result);\n\tif (r) {\n\t\tif (r == -ENOSPC)\n\t\t\tset_pool_mode(pool, PM_OUT_OF_DATA_SPACE);\n\t\telse\n\t\t\tmetadata_operation_failed(pool, \"dm_pool_alloc_data_block\", r);\n\t\treturn r;\n\t}\n\n\tr = dm_pool_get_free_metadata_block_count(pool->pmd, &free_blocks);\n\tif (r) {\n\t\tmetadata_operation_failed(pool, \"dm_pool_get_free_metadata_block_count\", r);\n\t\treturn r;\n\t}\n\n\tif (!free_blocks) {\n\t\t \n\t\tr = commit(pool);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void retry_on_resume(struct bio *bio)\n{\n\tstruct dm_thin_endio_hook *h = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\n\tstruct thin_c *tc = h->tc;\n\n\tspin_lock_irq(&tc->lock);\n\tbio_list_add(&tc->retry_on_resume_list, bio);\n\tspin_unlock_irq(&tc->lock);\n}\n\nstatic blk_status_t should_error_unserviceable_bio(struct pool *pool)\n{\n\tenum pool_mode m = get_pool_mode(pool);\n\n\tswitch (m) {\n\tcase PM_WRITE:\n\t\t \n\t\tDMERR_LIMIT(\"bio unserviceable, yet pool is in PM_WRITE mode\");\n\t\treturn BLK_STS_IOERR;\n\n\tcase PM_OUT_OF_DATA_SPACE:\n\t\treturn pool->pf.error_if_no_space ? BLK_STS_NOSPC : 0;\n\n\tcase PM_OUT_OF_METADATA_SPACE:\n\tcase PM_READ_ONLY:\n\tcase PM_FAIL:\n\t\treturn BLK_STS_IOERR;\n\tdefault:\n\t\t \n\t\tDMERR_LIMIT(\"bio unserviceable, yet pool has an unknown mode\");\n\t\treturn BLK_STS_IOERR;\n\t}\n}\n\nstatic void handle_unserviceable_bio(struct pool *pool, struct bio *bio)\n{\n\tblk_status_t error = should_error_unserviceable_bio(pool);\n\n\tif (error) {\n\t\tbio->bi_status = error;\n\t\tbio_endio(bio);\n\t} else\n\t\tretry_on_resume(bio);\n}\n\nstatic void retry_bios_on_resume(struct pool *pool, struct dm_bio_prison_cell *cell)\n{\n\tstruct bio *bio;\n\tstruct bio_list bios;\n\tblk_status_t error;\n\n\terror = should_error_unserviceable_bio(pool);\n\tif (error) {\n\t\tcell_error_with_code(pool, cell, error);\n\t\treturn;\n\t}\n\n\tbio_list_init(&bios);\n\tcell_release(pool, cell, &bios);\n\n\twhile ((bio = bio_list_pop(&bios)))\n\t\tretry_on_resume(bio);\n}\n\nstatic void process_discard_cell_no_passdown(struct thin_c *tc,\n\t\t\t\t\t     struct dm_bio_prison_cell *virt_cell)\n{\n\tstruct pool *pool = tc->pool;\n\tstruct dm_thin_new_mapping *m = get_next_mapping(pool);\n\n\t \n\tm->tc = tc;\n\tm->virt_begin = virt_cell->key.block_begin;\n\tm->virt_end = virt_cell->key.block_end;\n\tm->cell = virt_cell;\n\tm->bio = virt_cell->holder;\n\n\tif (!dm_deferred_set_add_work(pool->all_io_ds, &m->list))\n\t\tpool->process_prepared_discard(m);\n}\n\nstatic void break_up_discard_bio(struct thin_c *tc, dm_block_t begin, dm_block_t end,\n\t\t\t\t struct bio *bio)\n{\n\tstruct pool *pool = tc->pool;\n\n\tint r;\n\tbool maybe_shared;\n\tstruct dm_cell_key data_key;\n\tstruct dm_bio_prison_cell *data_cell;\n\tstruct dm_thin_new_mapping *m;\n\tdm_block_t virt_begin, virt_end, data_begin, data_end;\n\tdm_block_t len, next_boundary;\n\n\twhile (begin != end) {\n\t\tr = dm_thin_find_mapped_range(tc->td, begin, end, &virt_begin, &virt_end,\n\t\t\t\t\t      &data_begin, &maybe_shared);\n\t\tif (r) {\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\n\t\tdata_end = data_begin + (virt_end - virt_begin);\n\n\t\t \n\t\twhile (data_begin < data_end) {\n\t\t\tr = ensure_next_mapping(pool);\n\t\t\tif (r)\n\t\t\t\treturn;  \n\n\t\t\tnext_boundary = ((data_begin >> BIO_PRISON_MAX_RANGE_SHIFT) + 1)\n\t\t\t\t<< BIO_PRISON_MAX_RANGE_SHIFT;\n\t\t\tlen = min_t(sector_t, data_end - data_begin, next_boundary - data_begin);\n\n\t\t\t \n\t\t\t(void) build_key(tc->td, PHYSICAL, data_begin, data_begin + len, &data_key);\n\t\t\tif (bio_detain(tc->pool, &data_key, NULL, &data_cell)) {\n\t\t\t\t \n\t\t\t\tdata_begin += len;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tm = get_next_mapping(pool);\n\t\t\tm->tc = tc;\n\t\t\tm->maybe_shared = maybe_shared;\n\t\t\tm->virt_begin = virt_begin;\n\t\t\tm->virt_end = virt_begin + len;\n\t\t\tm->data_block = data_begin;\n\t\t\tm->cell = data_cell;\n\t\t\tm->bio = bio;\n\n\t\t\t \n\t\t\tbio_inc_remaining(bio);\n\t\t\tif (!dm_deferred_set_add_work(pool->all_io_ds, &m->list))\n\t\t\t\tpool->process_prepared_discard(m);\n\n\t\t\tvirt_begin += len;\n\t\t\tdata_begin += len;\n\t\t}\n\n\t\tbegin = virt_end;\n\t}\n}\n\nstatic void process_discard_cell_passdown(struct thin_c *tc, struct dm_bio_prison_cell *virt_cell)\n{\n\tstruct bio *bio = virt_cell->holder;\n\tstruct dm_thin_endio_hook *h = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\n\n\t \n\th->cell = virt_cell;\n\tbreak_up_discard_bio(tc, virt_cell->key.block_begin, virt_cell->key.block_end, bio);\n\n\t \n\tbio_endio(bio);\n}\n\nstatic void process_discard_bio(struct thin_c *tc, struct bio *bio)\n{\n\tdm_block_t begin, end;\n\tstruct dm_cell_key virt_key;\n\tstruct dm_bio_prison_cell *virt_cell;\n\n\tget_bio_block_range(tc, bio, &begin, &end);\n\tif (begin == end) {\n\t\t \n\t\tbio_endio(bio);\n\t\treturn;\n\t}\n\n\tif (unlikely(!build_key(tc->td, VIRTUAL, begin, end, &virt_key))) {\n\t\tDMERR_LIMIT(\"Discard doesn't respect bio prison limits\");\n\t\tbio_endio(bio);\n\t\treturn;\n\t}\n\n\tif (bio_detain(tc->pool, &virt_key, bio, &virt_cell)) {\n\t\t \n\t\treturn;\n\t}\n\n\ttc->pool->process_discard_cell(tc, virt_cell);\n}\n\nstatic void break_sharing(struct thin_c *tc, struct bio *bio, dm_block_t block,\n\t\t\t  struct dm_cell_key *key,\n\t\t\t  struct dm_thin_lookup_result *lookup_result,\n\t\t\t  struct dm_bio_prison_cell *cell)\n{\n\tint r;\n\tdm_block_t data_block;\n\tstruct pool *pool = tc->pool;\n\n\tr = alloc_data_block(tc, &data_block);\n\tswitch (r) {\n\tcase 0:\n\t\tschedule_internal_copy(tc, block, lookup_result->block,\n\t\t\t\t       data_block, cell, bio);\n\t\tbreak;\n\n\tcase -ENOSPC:\n\t\tretry_bios_on_resume(pool, cell);\n\t\tbreak;\n\n\tdefault:\n\t\tDMERR_LIMIT(\"%s: alloc_data_block() failed: error = %d\",\n\t\t\t    __func__, r);\n\t\tcell_error(pool, cell);\n\t\tbreak;\n\t}\n}\n\nstatic void __remap_and_issue_shared_cell(void *context,\n\t\t\t\t\t  struct dm_bio_prison_cell *cell)\n{\n\tstruct remap_info *info = context;\n\tstruct bio *bio;\n\n\twhile ((bio = bio_list_pop(&cell->bios))) {\n\t\tif (bio_data_dir(bio) == WRITE || op_is_flush(bio->bi_opf) ||\n\t\t    bio_op(bio) == REQ_OP_DISCARD)\n\t\t\tbio_list_add(&info->defer_bios, bio);\n\t\telse {\n\t\t\tstruct dm_thin_endio_hook *h = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\n\n\t\t\th->shared_read_entry = dm_deferred_entry_inc(info->tc->pool->shared_read_ds);\n\t\t\tinc_all_io_entry(info->tc->pool, bio);\n\t\t\tbio_list_add(&info->issue_bios, bio);\n\t\t}\n\t}\n}\n\nstatic void remap_and_issue_shared_cell(struct thin_c *tc,\n\t\t\t\t\tstruct dm_bio_prison_cell *cell,\n\t\t\t\t\tdm_block_t block)\n{\n\tstruct bio *bio;\n\tstruct remap_info info;\n\n\tinfo.tc = tc;\n\tbio_list_init(&info.defer_bios);\n\tbio_list_init(&info.issue_bios);\n\n\tcell_visit_release(tc->pool, __remap_and_issue_shared_cell,\n\t\t\t   &info, cell);\n\n\twhile ((bio = bio_list_pop(&info.defer_bios)))\n\t\tthin_defer_bio(tc, bio);\n\n\twhile ((bio = bio_list_pop(&info.issue_bios)))\n\t\tremap_and_issue(tc, bio, block);\n}\n\nstatic void process_shared_bio(struct thin_c *tc, struct bio *bio,\n\t\t\t       dm_block_t block,\n\t\t\t       struct dm_thin_lookup_result *lookup_result,\n\t\t\t       struct dm_bio_prison_cell *virt_cell)\n{\n\tstruct dm_bio_prison_cell *data_cell;\n\tstruct pool *pool = tc->pool;\n\tstruct dm_cell_key key;\n\n\t \n\tbuild_data_key(tc->td, lookup_result->block, &key);\n\tif (bio_detain(pool, &key, bio, &data_cell)) {\n\t\tcell_defer_no_holder(tc, virt_cell);\n\t\treturn;\n\t}\n\n\tif (bio_data_dir(bio) == WRITE && bio->bi_iter.bi_size) {\n\t\tbreak_sharing(tc, bio, block, &key, lookup_result, data_cell);\n\t\tcell_defer_no_holder(tc, virt_cell);\n\t} else {\n\t\tstruct dm_thin_endio_hook *h = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\n\n\t\th->shared_read_entry = dm_deferred_entry_inc(pool->shared_read_ds);\n\t\tinc_all_io_entry(pool, bio);\n\t\tremap_and_issue(tc, bio, lookup_result->block);\n\n\t\tremap_and_issue_shared_cell(tc, data_cell, lookup_result->block);\n\t\tremap_and_issue_shared_cell(tc, virt_cell, lookup_result->block);\n\t}\n}\n\nstatic void provision_block(struct thin_c *tc, struct bio *bio, dm_block_t block,\n\t\t\t    struct dm_bio_prison_cell *cell)\n{\n\tint r;\n\tdm_block_t data_block;\n\tstruct pool *pool = tc->pool;\n\n\t \n\tif (!bio->bi_iter.bi_size) {\n\t\tinc_all_io_entry(pool, bio);\n\t\tcell_defer_no_holder(tc, cell);\n\n\t\tremap_and_issue(tc, bio, 0);\n\t\treturn;\n\t}\n\n\t \n\tif (bio_data_dir(bio) == READ) {\n\t\tzero_fill_bio(bio);\n\t\tcell_defer_no_holder(tc, cell);\n\t\tbio_endio(bio);\n\t\treturn;\n\t}\n\n\tr = alloc_data_block(tc, &data_block);\n\tswitch (r) {\n\tcase 0:\n\t\tif (tc->origin_dev)\n\t\t\tschedule_external_copy(tc, block, data_block, cell, bio);\n\t\telse\n\t\t\tschedule_zero(tc, block, data_block, cell, bio);\n\t\tbreak;\n\n\tcase -ENOSPC:\n\t\tretry_bios_on_resume(pool, cell);\n\t\tbreak;\n\n\tdefault:\n\t\tDMERR_LIMIT(\"%s: alloc_data_block() failed: error = %d\",\n\t\t\t    __func__, r);\n\t\tcell_error(pool, cell);\n\t\tbreak;\n\t}\n}\n\nstatic void process_cell(struct thin_c *tc, struct dm_bio_prison_cell *cell)\n{\n\tint r;\n\tstruct pool *pool = tc->pool;\n\tstruct bio *bio = cell->holder;\n\tdm_block_t block = get_bio_block(tc, bio);\n\tstruct dm_thin_lookup_result lookup_result;\n\n\tif (tc->requeue_mode) {\n\t\tcell_requeue(pool, cell);\n\t\treturn;\n\t}\n\n\tr = dm_thin_find_block(tc->td, block, 1, &lookup_result);\n\tswitch (r) {\n\tcase 0:\n\t\tif (lookup_result.shared)\n\t\t\tprocess_shared_bio(tc, bio, block, &lookup_result, cell);\n\t\telse {\n\t\t\tinc_all_io_entry(pool, bio);\n\t\t\tremap_and_issue(tc, bio, lookup_result.block);\n\t\t\tinc_remap_and_issue_cell(tc, cell, lookup_result.block);\n\t\t}\n\t\tbreak;\n\n\tcase -ENODATA:\n\t\tif (bio_data_dir(bio) == READ && tc->origin_dev) {\n\t\t\tinc_all_io_entry(pool, bio);\n\t\t\tcell_defer_no_holder(tc, cell);\n\n\t\t\tif (bio_end_sector(bio) <= tc->origin_size)\n\t\t\t\tremap_to_origin_and_issue(tc, bio);\n\n\t\t\telse if (bio->bi_iter.bi_sector < tc->origin_size) {\n\t\t\t\tzero_fill_bio(bio);\n\t\t\t\tbio->bi_iter.bi_size = (tc->origin_size - bio->bi_iter.bi_sector) << SECTOR_SHIFT;\n\t\t\t\tremap_to_origin_and_issue(tc, bio);\n\n\t\t\t} else {\n\t\t\t\tzero_fill_bio(bio);\n\t\t\t\tbio_endio(bio);\n\t\t\t}\n\t\t} else\n\t\t\tprovision_block(tc, bio, block, cell);\n\t\tbreak;\n\n\tdefault:\n\t\tDMERR_LIMIT(\"%s: dm_thin_find_block() failed: error = %d\",\n\t\t\t    __func__, r);\n\t\tcell_defer_no_holder(tc, cell);\n\t\tbio_io_error(bio);\n\t\tbreak;\n\t}\n}\n\nstatic void process_bio(struct thin_c *tc, struct bio *bio)\n{\n\tstruct pool *pool = tc->pool;\n\tdm_block_t block = get_bio_block(tc, bio);\n\tstruct dm_bio_prison_cell *cell;\n\tstruct dm_cell_key key;\n\n\t \n\tbuild_virtual_key(tc->td, block, &key);\n\tif (bio_detain(pool, &key, bio, &cell))\n\t\treturn;\n\n\tprocess_cell(tc, cell);\n}\n\nstatic void __process_bio_read_only(struct thin_c *tc, struct bio *bio,\n\t\t\t\t    struct dm_bio_prison_cell *cell)\n{\n\tint r;\n\tint rw = bio_data_dir(bio);\n\tdm_block_t block = get_bio_block(tc, bio);\n\tstruct dm_thin_lookup_result lookup_result;\n\n\tr = dm_thin_find_block(tc->td, block, 1, &lookup_result);\n\tswitch (r) {\n\tcase 0:\n\t\tif (lookup_result.shared && (rw == WRITE) && bio->bi_iter.bi_size) {\n\t\t\thandle_unserviceable_bio(tc->pool, bio);\n\t\t\tif (cell)\n\t\t\t\tcell_defer_no_holder(tc, cell);\n\t\t} else {\n\t\t\tinc_all_io_entry(tc->pool, bio);\n\t\t\tremap_and_issue(tc, bio, lookup_result.block);\n\t\t\tif (cell)\n\t\t\t\tinc_remap_and_issue_cell(tc, cell, lookup_result.block);\n\t\t}\n\t\tbreak;\n\n\tcase -ENODATA:\n\t\tif (cell)\n\t\t\tcell_defer_no_holder(tc, cell);\n\t\tif (rw != READ) {\n\t\t\thandle_unserviceable_bio(tc->pool, bio);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (tc->origin_dev) {\n\t\t\tinc_all_io_entry(tc->pool, bio);\n\t\t\tremap_to_origin_and_issue(tc, bio);\n\t\t\tbreak;\n\t\t}\n\n\t\tzero_fill_bio(bio);\n\t\tbio_endio(bio);\n\t\tbreak;\n\n\tdefault:\n\t\tDMERR_LIMIT(\"%s: dm_thin_find_block() failed: error = %d\",\n\t\t\t    __func__, r);\n\t\tif (cell)\n\t\t\tcell_defer_no_holder(tc, cell);\n\t\tbio_io_error(bio);\n\t\tbreak;\n\t}\n}\n\nstatic void process_bio_read_only(struct thin_c *tc, struct bio *bio)\n{\n\t__process_bio_read_only(tc, bio, NULL);\n}\n\nstatic void process_cell_read_only(struct thin_c *tc, struct dm_bio_prison_cell *cell)\n{\n\t__process_bio_read_only(tc, cell->holder, cell);\n}\n\nstatic void process_bio_success(struct thin_c *tc, struct bio *bio)\n{\n\tbio_endio(bio);\n}\n\nstatic void process_bio_fail(struct thin_c *tc, struct bio *bio)\n{\n\tbio_io_error(bio);\n}\n\nstatic void process_cell_success(struct thin_c *tc, struct dm_bio_prison_cell *cell)\n{\n\tcell_success(tc->pool, cell);\n}\n\nstatic void process_cell_fail(struct thin_c *tc, struct dm_bio_prison_cell *cell)\n{\n\tcell_error(tc->pool, cell);\n}\n\n \nstatic int need_commit_due_to_time(struct pool *pool)\n{\n\treturn !time_in_range(jiffies, pool->last_commit_jiffies,\n\t\t\t      pool->last_commit_jiffies + COMMIT_PERIOD);\n}\n\n#define thin_pbd(node) rb_entry((node), struct dm_thin_endio_hook, rb_node)\n#define thin_bio(pbd) dm_bio_from_per_bio_data((pbd), sizeof(struct dm_thin_endio_hook))\n\nstatic void __thin_bio_rb_add(struct thin_c *tc, struct bio *bio)\n{\n\tstruct rb_node **rbp, *parent;\n\tstruct dm_thin_endio_hook *pbd;\n\tsector_t bi_sector = bio->bi_iter.bi_sector;\n\n\trbp = &tc->sort_bio_list.rb_node;\n\tparent = NULL;\n\twhile (*rbp) {\n\t\tparent = *rbp;\n\t\tpbd = thin_pbd(parent);\n\n\t\tif (bi_sector < thin_bio(pbd)->bi_iter.bi_sector)\n\t\t\trbp = &(*rbp)->rb_left;\n\t\telse\n\t\t\trbp = &(*rbp)->rb_right;\n\t}\n\n\tpbd = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\n\trb_link_node(&pbd->rb_node, parent, rbp);\n\trb_insert_color(&pbd->rb_node, &tc->sort_bio_list);\n}\n\nstatic void __extract_sorted_bios(struct thin_c *tc)\n{\n\tstruct rb_node *node;\n\tstruct dm_thin_endio_hook *pbd;\n\tstruct bio *bio;\n\n\tfor (node = rb_first(&tc->sort_bio_list); node; node = rb_next(node)) {\n\t\tpbd = thin_pbd(node);\n\t\tbio = thin_bio(pbd);\n\n\t\tbio_list_add(&tc->deferred_bio_list, bio);\n\t\trb_erase(&pbd->rb_node, &tc->sort_bio_list);\n\t}\n\n\tWARN_ON(!RB_EMPTY_ROOT(&tc->sort_bio_list));\n}\n\nstatic void __sort_thin_deferred_bios(struct thin_c *tc)\n{\n\tstruct bio *bio;\n\tstruct bio_list bios;\n\n\tbio_list_init(&bios);\n\tbio_list_merge(&bios, &tc->deferred_bio_list);\n\tbio_list_init(&tc->deferred_bio_list);\n\n\t \n\twhile ((bio = bio_list_pop(&bios)))\n\t\t__thin_bio_rb_add(tc, bio);\n\n\t \n\t__extract_sorted_bios(tc);\n}\n\nstatic void process_thin_deferred_bios(struct thin_c *tc)\n{\n\tstruct pool *pool = tc->pool;\n\tstruct bio *bio;\n\tstruct bio_list bios;\n\tstruct blk_plug plug;\n\tunsigned int count = 0;\n\n\tif (tc->requeue_mode) {\n\t\terror_thin_bio_list(tc, &tc->deferred_bio_list,\n\t\t\t\tBLK_STS_DM_REQUEUE);\n\t\treturn;\n\t}\n\n\tbio_list_init(&bios);\n\n\tspin_lock_irq(&tc->lock);\n\n\tif (bio_list_empty(&tc->deferred_bio_list)) {\n\t\tspin_unlock_irq(&tc->lock);\n\t\treturn;\n\t}\n\n\t__sort_thin_deferred_bios(tc);\n\n\tbio_list_merge(&bios, &tc->deferred_bio_list);\n\tbio_list_init(&tc->deferred_bio_list);\n\n\tspin_unlock_irq(&tc->lock);\n\n\tblk_start_plug(&plug);\n\twhile ((bio = bio_list_pop(&bios))) {\n\t\t \n\t\tif (ensure_next_mapping(pool)) {\n\t\t\tspin_lock_irq(&tc->lock);\n\t\t\tbio_list_add(&tc->deferred_bio_list, bio);\n\t\t\tbio_list_merge(&tc->deferred_bio_list, &bios);\n\t\t\tspin_unlock_irq(&tc->lock);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (bio_op(bio) == REQ_OP_DISCARD)\n\t\t\tpool->process_discard(tc, bio);\n\t\telse\n\t\t\tpool->process_bio(tc, bio);\n\n\t\tif ((count++ & 127) == 0) {\n\t\t\tthrottle_work_update(&pool->throttle);\n\t\t\tdm_pool_issue_prefetches(pool->pmd);\n\t\t}\n\t\tcond_resched();\n\t}\n\tblk_finish_plug(&plug);\n}\n\nstatic int cmp_cells(const void *lhs, const void *rhs)\n{\n\tstruct dm_bio_prison_cell *lhs_cell = *((struct dm_bio_prison_cell **) lhs);\n\tstruct dm_bio_prison_cell *rhs_cell = *((struct dm_bio_prison_cell **) rhs);\n\n\tBUG_ON(!lhs_cell->holder);\n\tBUG_ON(!rhs_cell->holder);\n\n\tif (lhs_cell->holder->bi_iter.bi_sector < rhs_cell->holder->bi_iter.bi_sector)\n\t\treturn -1;\n\n\tif (lhs_cell->holder->bi_iter.bi_sector > rhs_cell->holder->bi_iter.bi_sector)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic unsigned int sort_cells(struct pool *pool, struct list_head *cells)\n{\n\tunsigned int count = 0;\n\tstruct dm_bio_prison_cell *cell, *tmp;\n\n\tlist_for_each_entry_safe(cell, tmp, cells, user_list) {\n\t\tif (count >= CELL_SORT_ARRAY_SIZE)\n\t\t\tbreak;\n\n\t\tpool->cell_sort_array[count++] = cell;\n\t\tlist_del(&cell->user_list);\n\t}\n\n\tsort(pool->cell_sort_array, count, sizeof(cell), cmp_cells, NULL);\n\n\treturn count;\n}\n\nstatic void process_thin_deferred_cells(struct thin_c *tc)\n{\n\tstruct pool *pool = tc->pool;\n\tstruct list_head cells;\n\tstruct dm_bio_prison_cell *cell;\n\tunsigned int i, j, count;\n\n\tINIT_LIST_HEAD(&cells);\n\n\tspin_lock_irq(&tc->lock);\n\tlist_splice_init(&tc->deferred_cells, &cells);\n\tspin_unlock_irq(&tc->lock);\n\n\tif (list_empty(&cells))\n\t\treturn;\n\n\tdo {\n\t\tcount = sort_cells(tc->pool, &cells);\n\n\t\tfor (i = 0; i < count; i++) {\n\t\t\tcell = pool->cell_sort_array[i];\n\t\t\tBUG_ON(!cell->holder);\n\n\t\t\t \n\t\t\tif (ensure_next_mapping(pool)) {\n\t\t\t\tfor (j = i; j < count; j++)\n\t\t\t\t\tlist_add(&pool->cell_sort_array[j]->user_list, &cells);\n\n\t\t\t\tspin_lock_irq(&tc->lock);\n\t\t\t\tlist_splice(&cells, &tc->deferred_cells);\n\t\t\t\tspin_unlock_irq(&tc->lock);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tif (bio_op(cell->holder) == REQ_OP_DISCARD)\n\t\t\t\tpool->process_discard_cell(tc, cell);\n\t\t\telse\n\t\t\t\tpool->process_cell(tc, cell);\n\t\t}\n\t\tcond_resched();\n\t} while (!list_empty(&cells));\n}\n\nstatic void thin_get(struct thin_c *tc);\nstatic void thin_put(struct thin_c *tc);\n\n \nstatic struct thin_c *get_first_thin(struct pool *pool)\n{\n\tstruct thin_c *tc = NULL;\n\n\trcu_read_lock();\n\tif (!list_empty(&pool->active_thins)) {\n\t\ttc = list_entry_rcu(pool->active_thins.next, struct thin_c, list);\n\t\tthin_get(tc);\n\t}\n\trcu_read_unlock();\n\n\treturn tc;\n}\n\nstatic struct thin_c *get_next_thin(struct pool *pool, struct thin_c *tc)\n{\n\tstruct thin_c *old_tc = tc;\n\n\trcu_read_lock();\n\tlist_for_each_entry_continue_rcu(tc, &pool->active_thins, list) {\n\t\tthin_get(tc);\n\t\tthin_put(old_tc);\n\t\trcu_read_unlock();\n\t\treturn tc;\n\t}\n\tthin_put(old_tc);\n\trcu_read_unlock();\n\n\treturn NULL;\n}\n\nstatic void process_deferred_bios(struct pool *pool)\n{\n\tstruct bio *bio;\n\tstruct bio_list bios, bio_completions;\n\tstruct thin_c *tc;\n\n\ttc = get_first_thin(pool);\n\twhile (tc) {\n\t\tprocess_thin_deferred_cells(tc);\n\t\tprocess_thin_deferred_bios(tc);\n\t\ttc = get_next_thin(pool, tc);\n\t}\n\n\t \n\tbio_list_init(&bios);\n\tbio_list_init(&bio_completions);\n\n\tspin_lock_irq(&pool->lock);\n\tbio_list_merge(&bios, &pool->deferred_flush_bios);\n\tbio_list_init(&pool->deferred_flush_bios);\n\n\tbio_list_merge(&bio_completions, &pool->deferred_flush_completions);\n\tbio_list_init(&pool->deferred_flush_completions);\n\tspin_unlock_irq(&pool->lock);\n\n\tif (bio_list_empty(&bios) && bio_list_empty(&bio_completions) &&\n\t    !(dm_pool_changed_this_transaction(pool->pmd) && need_commit_due_to_time(pool)))\n\t\treturn;\n\n\tif (commit(pool)) {\n\t\tbio_list_merge(&bios, &bio_completions);\n\n\t\twhile ((bio = bio_list_pop(&bios)))\n\t\t\tbio_io_error(bio);\n\t\treturn;\n\t}\n\tpool->last_commit_jiffies = jiffies;\n\n\twhile ((bio = bio_list_pop(&bio_completions)))\n\t\tbio_endio(bio);\n\n\twhile ((bio = bio_list_pop(&bios))) {\n\t\t \n\t\tif (bio->bi_opf & REQ_PREFLUSH)\n\t\t\tbio_endio(bio);\n\t\telse\n\t\t\tdm_submit_bio_remap(bio, NULL);\n\t}\n}\n\nstatic void do_worker(struct work_struct *ws)\n{\n\tstruct pool *pool = container_of(ws, struct pool, worker);\n\n\tthrottle_work_start(&pool->throttle);\n\tdm_pool_issue_prefetches(pool->pmd);\n\tthrottle_work_update(&pool->throttle);\n\tprocess_prepared(pool, &pool->prepared_mappings, &pool->process_prepared_mapping);\n\tthrottle_work_update(&pool->throttle);\n\tprocess_prepared(pool, &pool->prepared_discards, &pool->process_prepared_discard);\n\tthrottle_work_update(&pool->throttle);\n\tprocess_prepared(pool, &pool->prepared_discards_pt2, &pool->process_prepared_discard_pt2);\n\tthrottle_work_update(&pool->throttle);\n\tprocess_deferred_bios(pool);\n\tthrottle_work_complete(&pool->throttle);\n}\n\n \nstatic void do_waker(struct work_struct *ws)\n{\n\tstruct pool *pool = container_of(to_delayed_work(ws), struct pool, waker);\n\n\twake_worker(pool);\n\tqueue_delayed_work(pool->wq, &pool->waker, COMMIT_PERIOD);\n}\n\n \nstatic void do_no_space_timeout(struct work_struct *ws)\n{\n\tstruct pool *pool = container_of(to_delayed_work(ws), struct pool,\n\t\t\t\t\t no_space_timeout);\n\n\tif (get_pool_mode(pool) == PM_OUT_OF_DATA_SPACE && !pool->pf.error_if_no_space) {\n\t\tpool->pf.error_if_no_space = true;\n\t\tnotify_of_pool_mode_change(pool);\n\t\terror_retry_list_with_code(pool, BLK_STS_NOSPC);\n\t}\n}\n\n \n\nstruct pool_work {\n\tstruct work_struct worker;\n\tstruct completion complete;\n};\n\nstatic struct pool_work *to_pool_work(struct work_struct *ws)\n{\n\treturn container_of(ws, struct pool_work, worker);\n}\n\nstatic void pool_work_complete(struct pool_work *pw)\n{\n\tcomplete(&pw->complete);\n}\n\nstatic void pool_work_wait(struct pool_work *pw, struct pool *pool,\n\t\t\t   void (*fn)(struct work_struct *))\n{\n\tINIT_WORK_ONSTACK(&pw->worker, fn);\n\tinit_completion(&pw->complete);\n\tqueue_work(pool->wq, &pw->worker);\n\twait_for_completion(&pw->complete);\n}\n\n \n\nstruct noflush_work {\n\tstruct pool_work pw;\n\tstruct thin_c *tc;\n};\n\nstatic struct noflush_work *to_noflush(struct work_struct *ws)\n{\n\treturn container_of(to_pool_work(ws), struct noflush_work, pw);\n}\n\nstatic void do_noflush_start(struct work_struct *ws)\n{\n\tstruct noflush_work *w = to_noflush(ws);\n\n\tw->tc->requeue_mode = true;\n\trequeue_io(w->tc);\n\tpool_work_complete(&w->pw);\n}\n\nstatic void do_noflush_stop(struct work_struct *ws)\n{\n\tstruct noflush_work *w = to_noflush(ws);\n\n\tw->tc->requeue_mode = false;\n\tpool_work_complete(&w->pw);\n}\n\nstatic void noflush_work(struct thin_c *tc, void (*fn)(struct work_struct *))\n{\n\tstruct noflush_work w;\n\n\tw.tc = tc;\n\tpool_work_wait(&w.pw, tc->pool, fn);\n}\n\n \n\nstatic void set_discard_callbacks(struct pool *pool)\n{\n\tstruct pool_c *pt = pool->ti->private;\n\n\tif (pt->adjusted_pf.discard_passdown) {\n\t\tpool->process_discard_cell = process_discard_cell_passdown;\n\t\tpool->process_prepared_discard = process_prepared_discard_passdown_pt1;\n\t\tpool->process_prepared_discard_pt2 = process_prepared_discard_passdown_pt2;\n\t} else {\n\t\tpool->process_discard_cell = process_discard_cell_no_passdown;\n\t\tpool->process_prepared_discard = process_prepared_discard_no_passdown;\n\t}\n}\n\nstatic void set_pool_mode(struct pool *pool, enum pool_mode new_mode)\n{\n\tstruct pool_c *pt = pool->ti->private;\n\tbool needs_check = dm_pool_metadata_needs_check(pool->pmd);\n\tenum pool_mode old_mode = get_pool_mode(pool);\n\tunsigned long no_space_timeout = READ_ONCE(no_space_timeout_secs) * HZ;\n\n\t \n\tif (new_mode == PM_WRITE && needs_check) {\n\t\tDMERR(\"%s: unable to switch pool to write mode until repaired.\",\n\t\t      dm_device_name(pool->pool_md));\n\t\tif (old_mode != new_mode)\n\t\t\tnew_mode = old_mode;\n\t\telse\n\t\t\tnew_mode = PM_READ_ONLY;\n\t}\n\t \n\tif (old_mode == PM_FAIL)\n\t\tnew_mode = old_mode;\n\n\tswitch (new_mode) {\n\tcase PM_FAIL:\n\t\tdm_pool_metadata_read_only(pool->pmd);\n\t\tpool->process_bio = process_bio_fail;\n\t\tpool->process_discard = process_bio_fail;\n\t\tpool->process_cell = process_cell_fail;\n\t\tpool->process_discard_cell = process_cell_fail;\n\t\tpool->process_prepared_mapping = process_prepared_mapping_fail;\n\t\tpool->process_prepared_discard = process_prepared_discard_fail;\n\n\t\terror_retry_list(pool);\n\t\tbreak;\n\n\tcase PM_OUT_OF_METADATA_SPACE:\n\tcase PM_READ_ONLY:\n\t\tdm_pool_metadata_read_only(pool->pmd);\n\t\tpool->process_bio = process_bio_read_only;\n\t\tpool->process_discard = process_bio_success;\n\t\tpool->process_cell = process_cell_read_only;\n\t\tpool->process_discard_cell = process_cell_success;\n\t\tpool->process_prepared_mapping = process_prepared_mapping_fail;\n\t\tpool->process_prepared_discard = process_prepared_discard_success;\n\n\t\terror_retry_list(pool);\n\t\tbreak;\n\n\tcase PM_OUT_OF_DATA_SPACE:\n\t\t \n\t\tpool->out_of_data_space = true;\n\t\tpool->process_bio = process_bio_read_only;\n\t\tpool->process_discard = process_discard_bio;\n\t\tpool->process_cell = process_cell_read_only;\n\t\tpool->process_prepared_mapping = process_prepared_mapping;\n\t\tset_discard_callbacks(pool);\n\n\t\tif (!pool->pf.error_if_no_space && no_space_timeout)\n\t\t\tqueue_delayed_work(pool->wq, &pool->no_space_timeout, no_space_timeout);\n\t\tbreak;\n\n\tcase PM_WRITE:\n\t\tif (old_mode == PM_OUT_OF_DATA_SPACE)\n\t\t\tcancel_delayed_work_sync(&pool->no_space_timeout);\n\t\tpool->out_of_data_space = false;\n\t\tpool->pf.error_if_no_space = pt->requested_pf.error_if_no_space;\n\t\tdm_pool_metadata_read_write(pool->pmd);\n\t\tpool->process_bio = process_bio;\n\t\tpool->process_discard = process_discard_bio;\n\t\tpool->process_cell = process_cell;\n\t\tpool->process_prepared_mapping = process_prepared_mapping;\n\t\tset_discard_callbacks(pool);\n\t\tbreak;\n\t}\n\n\tpool->pf.mode = new_mode;\n\t \n\tpt->adjusted_pf.mode = new_mode;\n\n\tif (old_mode != new_mode)\n\t\tnotify_of_pool_mode_change(pool);\n}\n\nstatic void abort_transaction(struct pool *pool)\n{\n\tconst char *dev_name = dm_device_name(pool->pool_md);\n\n\tDMERR_LIMIT(\"%s: aborting current metadata transaction\", dev_name);\n\tif (dm_pool_abort_metadata(pool->pmd)) {\n\t\tDMERR(\"%s: failed to abort metadata transaction\", dev_name);\n\t\tset_pool_mode(pool, PM_FAIL);\n\t}\n\n\tif (dm_pool_metadata_set_needs_check(pool->pmd)) {\n\t\tDMERR(\"%s: failed to set 'needs_check' flag in metadata\", dev_name);\n\t\tset_pool_mode(pool, PM_FAIL);\n\t}\n}\n\nstatic void metadata_operation_failed(struct pool *pool, const char *op, int r)\n{\n\tDMERR_LIMIT(\"%s: metadata operation '%s' failed: error = %d\",\n\t\t    dm_device_name(pool->pool_md), op, r);\n\n\tabort_transaction(pool);\n\tset_pool_mode(pool, PM_READ_ONLY);\n}\n\n \n\n \n\n \nstatic void thin_defer_bio(struct thin_c *tc, struct bio *bio)\n{\n\tstruct pool *pool = tc->pool;\n\n\tspin_lock_irq(&tc->lock);\n\tbio_list_add(&tc->deferred_bio_list, bio);\n\tspin_unlock_irq(&tc->lock);\n\n\twake_worker(pool);\n}\n\nstatic void thin_defer_bio_with_throttle(struct thin_c *tc, struct bio *bio)\n{\n\tstruct pool *pool = tc->pool;\n\n\tthrottle_lock(&pool->throttle);\n\tthin_defer_bio(tc, bio);\n\tthrottle_unlock(&pool->throttle);\n}\n\nstatic void thin_defer_cell(struct thin_c *tc, struct dm_bio_prison_cell *cell)\n{\n\tstruct pool *pool = tc->pool;\n\n\tthrottle_lock(&pool->throttle);\n\tspin_lock_irq(&tc->lock);\n\tlist_add_tail(&cell->user_list, &tc->deferred_cells);\n\tspin_unlock_irq(&tc->lock);\n\tthrottle_unlock(&pool->throttle);\n\n\twake_worker(pool);\n}\n\nstatic void thin_hook_bio(struct thin_c *tc, struct bio *bio)\n{\n\tstruct dm_thin_endio_hook *h = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\n\n\th->tc = tc;\n\th->shared_read_entry = NULL;\n\th->all_io_entry = NULL;\n\th->overwrite_mapping = NULL;\n\th->cell = NULL;\n}\n\n \nstatic int thin_bio_map(struct dm_target *ti, struct bio *bio)\n{\n\tint r;\n\tstruct thin_c *tc = ti->private;\n\tdm_block_t block = get_bio_block(tc, bio);\n\tstruct dm_thin_device *td = tc->td;\n\tstruct dm_thin_lookup_result result;\n\tstruct dm_bio_prison_cell *virt_cell, *data_cell;\n\tstruct dm_cell_key key;\n\n\tthin_hook_bio(tc, bio);\n\n\tif (tc->requeue_mode) {\n\t\tbio->bi_status = BLK_STS_DM_REQUEUE;\n\t\tbio_endio(bio);\n\t\treturn DM_MAPIO_SUBMITTED;\n\t}\n\n\tif (get_pool_mode(tc->pool) == PM_FAIL) {\n\t\tbio_io_error(bio);\n\t\treturn DM_MAPIO_SUBMITTED;\n\t}\n\n\tif (op_is_flush(bio->bi_opf) || bio_op(bio) == REQ_OP_DISCARD) {\n\t\tthin_defer_bio_with_throttle(tc, bio);\n\t\treturn DM_MAPIO_SUBMITTED;\n\t}\n\n\t \n\tbuild_virtual_key(tc->td, block, &key);\n\tif (bio_detain(tc->pool, &key, bio, &virt_cell))\n\t\treturn DM_MAPIO_SUBMITTED;\n\n\tr = dm_thin_find_block(td, block, 0, &result);\n\n\t \n\tswitch (r) {\n\tcase 0:\n\t\tif (unlikely(result.shared)) {\n\t\t\t \n\t\t\tthin_defer_cell(tc, virt_cell);\n\t\t\treturn DM_MAPIO_SUBMITTED;\n\t\t}\n\n\t\tbuild_data_key(tc->td, result.block, &key);\n\t\tif (bio_detain(tc->pool, &key, bio, &data_cell)) {\n\t\t\tcell_defer_no_holder(tc, virt_cell);\n\t\t\treturn DM_MAPIO_SUBMITTED;\n\t\t}\n\n\t\tinc_all_io_entry(tc->pool, bio);\n\t\tcell_defer_no_holder(tc, data_cell);\n\t\tcell_defer_no_holder(tc, virt_cell);\n\n\t\tremap(tc, bio, result.block);\n\t\treturn DM_MAPIO_REMAPPED;\n\n\tcase -ENODATA:\n\tcase -EWOULDBLOCK:\n\t\tthin_defer_cell(tc, virt_cell);\n\t\treturn DM_MAPIO_SUBMITTED;\n\n\tdefault:\n\t\t \n\t\tbio_io_error(bio);\n\t\tcell_defer_no_holder(tc, virt_cell);\n\t\treturn DM_MAPIO_SUBMITTED;\n\t}\n}\n\nstatic void requeue_bios(struct pool *pool)\n{\n\tstruct thin_c *tc;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(tc, &pool->active_thins, list) {\n\t\tspin_lock_irq(&tc->lock);\n\t\tbio_list_merge(&tc->deferred_bio_list, &tc->retry_on_resume_list);\n\t\tbio_list_init(&tc->retry_on_resume_list);\n\t\tspin_unlock_irq(&tc->lock);\n\t}\n\trcu_read_unlock();\n}\n\n \nstatic bool is_factor(sector_t block_size, uint32_t n)\n{\n\treturn !sector_div(block_size, n);\n}\n\n \nstatic void disable_discard_passdown_if_not_supported(struct pool_c *pt)\n{\n\tstruct pool *pool = pt->pool;\n\tstruct block_device *data_bdev = pt->data_dev->bdev;\n\tstruct queue_limits *data_limits = &bdev_get_queue(data_bdev)->limits;\n\tconst char *reason = NULL;\n\n\tif (!pt->adjusted_pf.discard_passdown)\n\t\treturn;\n\n\tif (!bdev_max_discard_sectors(pt->data_dev->bdev))\n\t\treason = \"discard unsupported\";\n\n\telse if (data_limits->max_discard_sectors < pool->sectors_per_block)\n\t\treason = \"max discard sectors smaller than a block\";\n\n\tif (reason) {\n\t\tDMWARN(\"Data device (%pg) %s: Disabling discard passdown.\", data_bdev, reason);\n\t\tpt->adjusted_pf.discard_passdown = false;\n\t}\n}\n\nstatic int bind_control_target(struct pool *pool, struct dm_target *ti)\n{\n\tstruct pool_c *pt = ti->private;\n\n\t \n\tenum pool_mode old_mode = get_pool_mode(pool);\n\tenum pool_mode new_mode = pt->adjusted_pf.mode;\n\n\t \n\tpt->adjusted_pf.mode = old_mode;\n\n\tpool->ti = ti;\n\tpool->pf = pt->adjusted_pf;\n\tpool->low_water_blocks = pt->low_water_blocks;\n\n\tset_pool_mode(pool, new_mode);\n\n\treturn 0;\n}\n\nstatic void unbind_control_target(struct pool *pool, struct dm_target *ti)\n{\n\tif (pool->ti == ti)\n\t\tpool->ti = NULL;\n}\n\n \n \nstatic void pool_features_init(struct pool_features *pf)\n{\n\tpf->mode = PM_WRITE;\n\tpf->zero_new_blocks = true;\n\tpf->discard_enabled = true;\n\tpf->discard_passdown = true;\n\tpf->error_if_no_space = false;\n}\n\nstatic void __pool_destroy(struct pool *pool)\n{\n\t__pool_table_remove(pool);\n\n\tvfree(pool->cell_sort_array);\n\tif (dm_pool_metadata_close(pool->pmd) < 0)\n\t\tDMWARN(\"%s: dm_pool_metadata_close() failed.\", __func__);\n\n\tdm_bio_prison_destroy(pool->prison);\n\tdm_kcopyd_client_destroy(pool->copier);\n\n\tcancel_delayed_work_sync(&pool->waker);\n\tcancel_delayed_work_sync(&pool->no_space_timeout);\n\tif (pool->wq)\n\t\tdestroy_workqueue(pool->wq);\n\n\tif (pool->next_mapping)\n\t\tmempool_free(pool->next_mapping, &pool->mapping_pool);\n\tmempool_exit(&pool->mapping_pool);\n\tdm_deferred_set_destroy(pool->shared_read_ds);\n\tdm_deferred_set_destroy(pool->all_io_ds);\n\tkfree(pool);\n}\n\nstatic struct kmem_cache *_new_mapping_cache;\n\nstatic struct pool *pool_create(struct mapped_device *pool_md,\n\t\t\t\tstruct block_device *metadata_dev,\n\t\t\t\tstruct block_device *data_dev,\n\t\t\t\tunsigned long block_size,\n\t\t\t\tint read_only, char **error)\n{\n\tint r;\n\tvoid *err_p;\n\tstruct pool *pool;\n\tstruct dm_pool_metadata *pmd;\n\tbool format_device = read_only ? false : true;\n\n\tpmd = dm_pool_metadata_open(metadata_dev, block_size, format_device);\n\tif (IS_ERR(pmd)) {\n\t\t*error = \"Error creating metadata object\";\n\t\treturn (struct pool *)pmd;\n\t}\n\n\tpool = kzalloc(sizeof(*pool), GFP_KERNEL);\n\tif (!pool) {\n\t\t*error = \"Error allocating memory for pool\";\n\t\terr_p = ERR_PTR(-ENOMEM);\n\t\tgoto bad_pool;\n\t}\n\n\tpool->pmd = pmd;\n\tpool->sectors_per_block = block_size;\n\tif (block_size & (block_size - 1))\n\t\tpool->sectors_per_block_shift = -1;\n\telse\n\t\tpool->sectors_per_block_shift = __ffs(block_size);\n\tpool->low_water_blocks = 0;\n\tpool_features_init(&pool->pf);\n\tpool->prison = dm_bio_prison_create();\n\tif (!pool->prison) {\n\t\t*error = \"Error creating pool's bio prison\";\n\t\terr_p = ERR_PTR(-ENOMEM);\n\t\tgoto bad_prison;\n\t}\n\n\tpool->copier = dm_kcopyd_client_create(&dm_kcopyd_throttle);\n\tif (IS_ERR(pool->copier)) {\n\t\tr = PTR_ERR(pool->copier);\n\t\t*error = \"Error creating pool's kcopyd client\";\n\t\terr_p = ERR_PTR(r);\n\t\tgoto bad_kcopyd_client;\n\t}\n\n\t \n\tpool->wq = alloc_ordered_workqueue(\"dm-\" DM_MSG_PREFIX, WQ_MEM_RECLAIM);\n\tif (!pool->wq) {\n\t\t*error = \"Error creating pool's workqueue\";\n\t\terr_p = ERR_PTR(-ENOMEM);\n\t\tgoto bad_wq;\n\t}\n\n\tthrottle_init(&pool->throttle);\n\tINIT_WORK(&pool->worker, do_worker);\n\tINIT_DELAYED_WORK(&pool->waker, do_waker);\n\tINIT_DELAYED_WORK(&pool->no_space_timeout, do_no_space_timeout);\n\tspin_lock_init(&pool->lock);\n\tbio_list_init(&pool->deferred_flush_bios);\n\tbio_list_init(&pool->deferred_flush_completions);\n\tINIT_LIST_HEAD(&pool->prepared_mappings);\n\tINIT_LIST_HEAD(&pool->prepared_discards);\n\tINIT_LIST_HEAD(&pool->prepared_discards_pt2);\n\tINIT_LIST_HEAD(&pool->active_thins);\n\tpool->low_water_triggered = false;\n\tpool->suspended = true;\n\tpool->out_of_data_space = false;\n\n\tpool->shared_read_ds = dm_deferred_set_create();\n\tif (!pool->shared_read_ds) {\n\t\t*error = \"Error creating pool's shared read deferred set\";\n\t\terr_p = ERR_PTR(-ENOMEM);\n\t\tgoto bad_shared_read_ds;\n\t}\n\n\tpool->all_io_ds = dm_deferred_set_create();\n\tif (!pool->all_io_ds) {\n\t\t*error = \"Error creating pool's all io deferred set\";\n\t\terr_p = ERR_PTR(-ENOMEM);\n\t\tgoto bad_all_io_ds;\n\t}\n\n\tpool->next_mapping = NULL;\n\tr = mempool_init_slab_pool(&pool->mapping_pool, MAPPING_POOL_SIZE,\n\t\t\t\t   _new_mapping_cache);\n\tif (r) {\n\t\t*error = \"Error creating pool's mapping mempool\";\n\t\terr_p = ERR_PTR(r);\n\t\tgoto bad_mapping_pool;\n\t}\n\n\tpool->cell_sort_array =\n\t\tvmalloc(array_size(CELL_SORT_ARRAY_SIZE,\n\t\t\t\t   sizeof(*pool->cell_sort_array)));\n\tif (!pool->cell_sort_array) {\n\t\t*error = \"Error allocating cell sort array\";\n\t\terr_p = ERR_PTR(-ENOMEM);\n\t\tgoto bad_sort_array;\n\t}\n\n\tpool->ref_count = 1;\n\tpool->last_commit_jiffies = jiffies;\n\tpool->pool_md = pool_md;\n\tpool->md_dev = metadata_dev;\n\tpool->data_dev = data_dev;\n\t__pool_table_insert(pool);\n\n\treturn pool;\n\nbad_sort_array:\n\tmempool_exit(&pool->mapping_pool);\nbad_mapping_pool:\n\tdm_deferred_set_destroy(pool->all_io_ds);\nbad_all_io_ds:\n\tdm_deferred_set_destroy(pool->shared_read_ds);\nbad_shared_read_ds:\n\tdestroy_workqueue(pool->wq);\nbad_wq:\n\tdm_kcopyd_client_destroy(pool->copier);\nbad_kcopyd_client:\n\tdm_bio_prison_destroy(pool->prison);\nbad_prison:\n\tkfree(pool);\nbad_pool:\n\tif (dm_pool_metadata_close(pmd))\n\t\tDMWARN(\"%s: dm_pool_metadata_close() failed.\", __func__);\n\n\treturn err_p;\n}\n\nstatic void __pool_inc(struct pool *pool)\n{\n\tBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\n\tpool->ref_count++;\n}\n\nstatic void __pool_dec(struct pool *pool)\n{\n\tBUG_ON(!mutex_is_locked(&dm_thin_pool_table.mutex));\n\tBUG_ON(!pool->ref_count);\n\tif (!--pool->ref_count)\n\t\t__pool_destroy(pool);\n}\n\nstatic struct pool *__pool_find(struct mapped_device *pool_md,\n\t\t\t\tstruct block_device *metadata_dev,\n\t\t\t\tstruct block_device *data_dev,\n\t\t\t\tunsigned long block_size, int read_only,\n\t\t\t\tchar **error, int *created)\n{\n\tstruct pool *pool = __pool_table_lookup_metadata_dev(metadata_dev);\n\n\tif (pool) {\n\t\tif (pool->pool_md != pool_md) {\n\t\t\t*error = \"metadata device already in use by a pool\";\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\t\tif (pool->data_dev != data_dev) {\n\t\t\t*error = \"data device already in use by a pool\";\n\t\t\treturn ERR_PTR(-EBUSY);\n\t\t}\n\t\t__pool_inc(pool);\n\n\t} else {\n\t\tpool = __pool_table_lookup(pool_md);\n\t\tif (pool) {\n\t\t\tif (pool->md_dev != metadata_dev || pool->data_dev != data_dev) {\n\t\t\t\t*error = \"different pool cannot replace a pool\";\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\t\t\t}\n\t\t\t__pool_inc(pool);\n\n\t\t} else {\n\t\t\tpool = pool_create(pool_md, metadata_dev, data_dev, block_size, read_only, error);\n\t\t\t*created = 1;\n\t\t}\n\t}\n\n\treturn pool;\n}\n\n \nstatic void pool_dtr(struct dm_target *ti)\n{\n\tstruct pool_c *pt = ti->private;\n\n\tmutex_lock(&dm_thin_pool_table.mutex);\n\n\tunbind_control_target(pt->pool, ti);\n\t__pool_dec(pt->pool);\n\tdm_put_device(ti, pt->metadata_dev);\n\tdm_put_device(ti, pt->data_dev);\n\tkfree(pt);\n\n\tmutex_unlock(&dm_thin_pool_table.mutex);\n}\n\nstatic int parse_pool_features(struct dm_arg_set *as, struct pool_features *pf,\n\t\t\t       struct dm_target *ti)\n{\n\tint r;\n\tunsigned int argc;\n\tconst char *arg_name;\n\n\tstatic const struct dm_arg _args[] = {\n\t\t{0, 4, \"Invalid number of pool feature arguments\"},\n\t};\n\n\t \n\tif (!as->argc)\n\t\treturn 0;\n\n\tr = dm_read_arg_group(_args, as, &argc, &ti->error);\n\tif (r)\n\t\treturn -EINVAL;\n\n\twhile (argc && !r) {\n\t\targ_name = dm_shift_arg(as);\n\t\targc--;\n\n\t\tif (!strcasecmp(arg_name, \"skip_block_zeroing\"))\n\t\t\tpf->zero_new_blocks = false;\n\n\t\telse if (!strcasecmp(arg_name, \"ignore_discard\"))\n\t\t\tpf->discard_enabled = false;\n\n\t\telse if (!strcasecmp(arg_name, \"no_discard_passdown\"))\n\t\t\tpf->discard_passdown = false;\n\n\t\telse if (!strcasecmp(arg_name, \"read_only\"))\n\t\t\tpf->mode = PM_READ_ONLY;\n\n\t\telse if (!strcasecmp(arg_name, \"error_if_no_space\"))\n\t\t\tpf->error_if_no_space = true;\n\n\t\telse {\n\t\t\tti->error = \"Unrecognised pool feature requested\";\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn r;\n}\n\nstatic void metadata_low_callback(void *context)\n{\n\tstruct pool *pool = context;\n\n\tDMWARN(\"%s: reached low water mark for metadata device: sending event.\",\n\t       dm_device_name(pool->pool_md));\n\n\tdm_table_event(pool->ti->table);\n}\n\n \nstatic int metadata_pre_commit_callback(void *context)\n{\n\tstruct pool *pool = context;\n\n\treturn blkdev_issue_flush(pool->data_dev);\n}\n\nstatic sector_t get_dev_size(struct block_device *bdev)\n{\n\treturn bdev_nr_sectors(bdev);\n}\n\nstatic void warn_if_metadata_device_too_big(struct block_device *bdev)\n{\n\tsector_t metadata_dev_size = get_dev_size(bdev);\n\n\tif (metadata_dev_size > THIN_METADATA_MAX_SECTORS_WARNING)\n\t\tDMWARN(\"Metadata device %pg is larger than %u sectors: excess space will not be used.\",\n\t\t       bdev, THIN_METADATA_MAX_SECTORS);\n}\n\nstatic sector_t get_metadata_dev_size(struct block_device *bdev)\n{\n\tsector_t metadata_dev_size = get_dev_size(bdev);\n\n\tif (metadata_dev_size > THIN_METADATA_MAX_SECTORS)\n\t\tmetadata_dev_size = THIN_METADATA_MAX_SECTORS;\n\n\treturn metadata_dev_size;\n}\n\nstatic dm_block_t get_metadata_dev_size_in_blocks(struct block_device *bdev)\n{\n\tsector_t metadata_dev_size = get_metadata_dev_size(bdev);\n\n\tsector_div(metadata_dev_size, THIN_METADATA_BLOCK_SIZE);\n\n\treturn metadata_dev_size;\n}\n\n \nstatic dm_block_t calc_metadata_threshold(struct pool_c *pt)\n{\n\t \n\tdm_block_t quarter = get_metadata_dev_size_in_blocks(pt->metadata_dev->bdev) / 4;\n\n\treturn min((dm_block_t)1024ULL  , quarter);\n}\n\n \nstatic int pool_ctr(struct dm_target *ti, unsigned int argc, char **argv)\n{\n\tint r, pool_created = 0;\n\tstruct pool_c *pt;\n\tstruct pool *pool;\n\tstruct pool_features pf;\n\tstruct dm_arg_set as;\n\tstruct dm_dev *data_dev;\n\tunsigned long block_size;\n\tdm_block_t low_water_blocks;\n\tstruct dm_dev *metadata_dev;\n\tblk_mode_t metadata_mode;\n\n\t \n\tmutex_lock(&dm_thin_pool_table.mutex);\n\n\tif (argc < 4) {\n\t\tti->error = \"Invalid argument count\";\n\t\tr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tas.argc = argc;\n\tas.argv = argv;\n\n\t \n\tif (!strcmp(argv[0], argv[1])) {\n\t\tti->error = \"Error setting metadata or data device\";\n\t\tr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tpool_features_init(&pf);\n\n\tdm_consume_args(&as, 4);\n\tr = parse_pool_features(&as, &pf, ti);\n\tif (r)\n\t\tgoto out_unlock;\n\n\tmetadata_mode = BLK_OPEN_READ |\n\t\t((pf.mode == PM_READ_ONLY) ? 0 : BLK_OPEN_WRITE);\n\tr = dm_get_device(ti, argv[0], metadata_mode, &metadata_dev);\n\tif (r) {\n\t\tti->error = \"Error opening metadata block device\";\n\t\tgoto out_unlock;\n\t}\n\twarn_if_metadata_device_too_big(metadata_dev->bdev);\n\n\tr = dm_get_device(ti, argv[1], BLK_OPEN_READ | BLK_OPEN_WRITE, &data_dev);\n\tif (r) {\n\t\tti->error = \"Error getting data device\";\n\t\tgoto out_metadata;\n\t}\n\n\tif (kstrtoul(argv[2], 10, &block_size) || !block_size ||\n\t    block_size < DATA_DEV_BLOCK_SIZE_MIN_SECTORS ||\n\t    block_size > DATA_DEV_BLOCK_SIZE_MAX_SECTORS ||\n\t    block_size & (DATA_DEV_BLOCK_SIZE_MIN_SECTORS - 1)) {\n\t\tti->error = \"Invalid block size\";\n\t\tr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (kstrtoull(argv[3], 10, (unsigned long long *)&low_water_blocks)) {\n\t\tti->error = \"Invalid low water mark\";\n\t\tr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tpt = kzalloc(sizeof(*pt), GFP_KERNEL);\n\tif (!pt) {\n\t\tr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tpool = __pool_find(dm_table_get_md(ti->table), metadata_dev->bdev, data_dev->bdev,\n\t\t\t   block_size, pf.mode == PM_READ_ONLY, &ti->error, &pool_created);\n\tif (IS_ERR(pool)) {\n\t\tr = PTR_ERR(pool);\n\t\tgoto out_free_pt;\n\t}\n\n\t \n\tif (!pool_created && pf.discard_enabled != pool->pf.discard_enabled) {\n\t\tti->error = \"Discard support cannot be disabled once enabled\";\n\t\tr = -EINVAL;\n\t\tgoto out_flags_changed;\n\t}\n\n\tpt->pool = pool;\n\tpt->ti = ti;\n\tpt->metadata_dev = metadata_dev;\n\tpt->data_dev = data_dev;\n\tpt->low_water_blocks = low_water_blocks;\n\tpt->adjusted_pf = pt->requested_pf = pf;\n\tti->num_flush_bios = 1;\n\tti->limit_swap_bios = true;\n\n\t \n\tif (pf.discard_enabled && pf.discard_passdown) {\n\t\tti->num_discard_bios = 1;\n\t\t \n\t\tti->discards_supported = true;\n\t\tti->max_discard_granularity = true;\n\t}\n\tti->private = pt;\n\n\tr = dm_pool_register_metadata_threshold(pt->pool->pmd,\n\t\t\t\t\t\tcalc_metadata_threshold(pt),\n\t\t\t\t\t\tmetadata_low_callback,\n\t\t\t\t\t\tpool);\n\tif (r) {\n\t\tti->error = \"Error registering metadata threshold\";\n\t\tgoto out_flags_changed;\n\t}\n\n\tdm_pool_register_pre_commit_callback(pool->pmd,\n\t\t\t\t\t     metadata_pre_commit_callback, pool);\n\n\tmutex_unlock(&dm_thin_pool_table.mutex);\n\n\treturn 0;\n\nout_flags_changed:\n\t__pool_dec(pool);\nout_free_pt:\n\tkfree(pt);\nout:\n\tdm_put_device(ti, data_dev);\nout_metadata:\n\tdm_put_device(ti, metadata_dev);\nout_unlock:\n\tmutex_unlock(&dm_thin_pool_table.mutex);\n\n\treturn r;\n}\n\nstatic int pool_map(struct dm_target *ti, struct bio *bio)\n{\n\tstruct pool_c *pt = ti->private;\n\tstruct pool *pool = pt->pool;\n\n\t \n\tspin_lock_irq(&pool->lock);\n\tbio_set_dev(bio, pt->data_dev->bdev);\n\tspin_unlock_irq(&pool->lock);\n\n\treturn DM_MAPIO_REMAPPED;\n}\n\nstatic int maybe_resize_data_dev(struct dm_target *ti, bool *need_commit)\n{\n\tint r;\n\tstruct pool_c *pt = ti->private;\n\tstruct pool *pool = pt->pool;\n\tsector_t data_size = ti->len;\n\tdm_block_t sb_data_size;\n\n\t*need_commit = false;\n\n\t(void) sector_div(data_size, pool->sectors_per_block);\n\n\tr = dm_pool_get_data_dev_size(pool->pmd, &sb_data_size);\n\tif (r) {\n\t\tDMERR(\"%s: failed to retrieve data device size\",\n\t\t      dm_device_name(pool->pool_md));\n\t\treturn r;\n\t}\n\n\tif (data_size < sb_data_size) {\n\t\tDMERR(\"%s: pool target (%llu blocks) too small: expected %llu\",\n\t\t      dm_device_name(pool->pool_md),\n\t\t      (unsigned long long)data_size, sb_data_size);\n\t\treturn -EINVAL;\n\n\t} else if (data_size > sb_data_size) {\n\t\tif (dm_pool_metadata_needs_check(pool->pmd)) {\n\t\t\tDMERR(\"%s: unable to grow the data device until repaired.\",\n\t\t\t      dm_device_name(pool->pool_md));\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (sb_data_size)\n\t\t\tDMINFO(\"%s: growing the data device from %llu to %llu blocks\",\n\t\t\t       dm_device_name(pool->pool_md),\n\t\t\t       sb_data_size, (unsigned long long)data_size);\n\t\tr = dm_pool_resize_data_dev(pool->pmd, data_size);\n\t\tif (r) {\n\t\t\tmetadata_operation_failed(pool, \"dm_pool_resize_data_dev\", r);\n\t\t\treturn r;\n\t\t}\n\n\t\t*need_commit = true;\n\t}\n\n\treturn 0;\n}\n\nstatic int maybe_resize_metadata_dev(struct dm_target *ti, bool *need_commit)\n{\n\tint r;\n\tstruct pool_c *pt = ti->private;\n\tstruct pool *pool = pt->pool;\n\tdm_block_t metadata_dev_size, sb_metadata_dev_size;\n\n\t*need_commit = false;\n\n\tmetadata_dev_size = get_metadata_dev_size_in_blocks(pool->md_dev);\n\n\tr = dm_pool_get_metadata_dev_size(pool->pmd, &sb_metadata_dev_size);\n\tif (r) {\n\t\tDMERR(\"%s: failed to retrieve metadata device size\",\n\t\t      dm_device_name(pool->pool_md));\n\t\treturn r;\n\t}\n\n\tif (metadata_dev_size < sb_metadata_dev_size) {\n\t\tDMERR(\"%s: metadata device (%llu blocks) too small: expected %llu\",\n\t\t      dm_device_name(pool->pool_md),\n\t\t      metadata_dev_size, sb_metadata_dev_size);\n\t\treturn -EINVAL;\n\n\t} else if (metadata_dev_size > sb_metadata_dev_size) {\n\t\tif (dm_pool_metadata_needs_check(pool->pmd)) {\n\t\t\tDMERR(\"%s: unable to grow the metadata device until repaired.\",\n\t\t\t      dm_device_name(pool->pool_md));\n\t\t\treturn 0;\n\t\t}\n\n\t\twarn_if_metadata_device_too_big(pool->md_dev);\n\t\tDMINFO(\"%s: growing the metadata device from %llu to %llu blocks\",\n\t\t       dm_device_name(pool->pool_md),\n\t\t       sb_metadata_dev_size, metadata_dev_size);\n\n\t\tif (get_pool_mode(pool) == PM_OUT_OF_METADATA_SPACE)\n\t\t\tset_pool_mode(pool, PM_WRITE);\n\n\t\tr = dm_pool_resize_metadata_dev(pool->pmd, metadata_dev_size);\n\t\tif (r) {\n\t\t\tmetadata_operation_failed(pool, \"dm_pool_resize_metadata_dev\", r);\n\t\t\treturn r;\n\t\t}\n\n\t\t*need_commit = true;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int pool_preresume(struct dm_target *ti)\n{\n\tint r;\n\tbool need_commit1, need_commit2;\n\tstruct pool_c *pt = ti->private;\n\tstruct pool *pool = pt->pool;\n\n\t \n\tr = bind_control_target(pool, ti);\n\tif (r)\n\t\tgoto out;\n\n\tr = maybe_resize_data_dev(ti, &need_commit1);\n\tif (r)\n\t\tgoto out;\n\n\tr = maybe_resize_metadata_dev(ti, &need_commit2);\n\tif (r)\n\t\tgoto out;\n\n\tif (need_commit1 || need_commit2)\n\t\t(void) commit(pool);\nout:\n\t \n\tif (r && get_pool_mode(pool) == PM_FAIL)\n\t\tr = 0;\n\n\treturn r;\n}\n\nstatic void pool_suspend_active_thins(struct pool *pool)\n{\n\tstruct thin_c *tc;\n\n\t \n\ttc = get_first_thin(pool);\n\twhile (tc) {\n\t\tdm_internal_suspend_noflush(tc->thin_md);\n\t\ttc = get_next_thin(pool, tc);\n\t}\n}\n\nstatic void pool_resume_active_thins(struct pool *pool)\n{\n\tstruct thin_c *tc;\n\n\t \n\ttc = get_first_thin(pool);\n\twhile (tc) {\n\t\tdm_internal_resume(tc->thin_md);\n\t\ttc = get_next_thin(pool, tc);\n\t}\n}\n\nstatic void pool_resume(struct dm_target *ti)\n{\n\tstruct pool_c *pt = ti->private;\n\tstruct pool *pool = pt->pool;\n\n\t \n\trequeue_bios(pool);\n\tpool_resume_active_thins(pool);\n\n\tspin_lock_irq(&pool->lock);\n\tpool->low_water_triggered = false;\n\tpool->suspended = false;\n\tspin_unlock_irq(&pool->lock);\n\n\tdo_waker(&pool->waker.work);\n}\n\nstatic void pool_presuspend(struct dm_target *ti)\n{\n\tstruct pool_c *pt = ti->private;\n\tstruct pool *pool = pt->pool;\n\n\tspin_lock_irq(&pool->lock);\n\tpool->suspended = true;\n\tspin_unlock_irq(&pool->lock);\n\n\tpool_suspend_active_thins(pool);\n}\n\nstatic void pool_presuspend_undo(struct dm_target *ti)\n{\n\tstruct pool_c *pt = ti->private;\n\tstruct pool *pool = pt->pool;\n\n\tpool_resume_active_thins(pool);\n\n\tspin_lock_irq(&pool->lock);\n\tpool->suspended = false;\n\tspin_unlock_irq(&pool->lock);\n}\n\nstatic void pool_postsuspend(struct dm_target *ti)\n{\n\tstruct pool_c *pt = ti->private;\n\tstruct pool *pool = pt->pool;\n\n\tcancel_delayed_work_sync(&pool->waker);\n\tcancel_delayed_work_sync(&pool->no_space_timeout);\n\tflush_workqueue(pool->wq);\n\t(void) commit(pool);\n}\n\nstatic int check_arg_count(unsigned int argc, unsigned int args_required)\n{\n\tif (argc != args_required) {\n\t\tDMWARN(\"Message received with %u arguments instead of %u.\",\n\t\t       argc, args_required);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int read_dev_id(char *arg, dm_thin_id *dev_id, int warning)\n{\n\tif (!kstrtoull(arg, 10, (unsigned long long *)dev_id) &&\n\t    *dev_id <= MAX_DEV_ID)\n\t\treturn 0;\n\n\tif (warning)\n\t\tDMWARN(\"Message received with invalid device id: %s\", arg);\n\n\treturn -EINVAL;\n}\n\nstatic int process_create_thin_mesg(unsigned int argc, char **argv, struct pool *pool)\n{\n\tdm_thin_id dev_id;\n\tint r;\n\n\tr = check_arg_count(argc, 2);\n\tif (r)\n\t\treturn r;\n\n\tr = read_dev_id(argv[1], &dev_id, 1);\n\tif (r)\n\t\treturn r;\n\n\tr = dm_pool_create_thin(pool->pmd, dev_id);\n\tif (r) {\n\t\tDMWARN(\"Creation of new thinly-provisioned device with id %s failed.\",\n\t\t       argv[1]);\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int process_create_snap_mesg(unsigned int argc, char **argv, struct pool *pool)\n{\n\tdm_thin_id dev_id;\n\tdm_thin_id origin_dev_id;\n\tint r;\n\n\tr = check_arg_count(argc, 3);\n\tif (r)\n\t\treturn r;\n\n\tr = read_dev_id(argv[1], &dev_id, 1);\n\tif (r)\n\t\treturn r;\n\n\tr = read_dev_id(argv[2], &origin_dev_id, 1);\n\tif (r)\n\t\treturn r;\n\n\tr = dm_pool_create_snap(pool->pmd, dev_id, origin_dev_id);\n\tif (r) {\n\t\tDMWARN(\"Creation of new snapshot %s of device %s failed.\",\n\t\t       argv[1], argv[2]);\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int process_delete_mesg(unsigned int argc, char **argv, struct pool *pool)\n{\n\tdm_thin_id dev_id;\n\tint r;\n\n\tr = check_arg_count(argc, 2);\n\tif (r)\n\t\treturn r;\n\n\tr = read_dev_id(argv[1], &dev_id, 1);\n\tif (r)\n\t\treturn r;\n\n\tr = dm_pool_delete_thin_device(pool->pmd, dev_id);\n\tif (r)\n\t\tDMWARN(\"Deletion of thin device %s failed.\", argv[1]);\n\n\treturn r;\n}\n\nstatic int process_set_transaction_id_mesg(unsigned int argc, char **argv, struct pool *pool)\n{\n\tdm_thin_id old_id, new_id;\n\tint r;\n\n\tr = check_arg_count(argc, 3);\n\tif (r)\n\t\treturn r;\n\n\tif (kstrtoull(argv[1], 10, (unsigned long long *)&old_id)) {\n\t\tDMWARN(\"set_transaction_id message: Unrecognised id %s.\", argv[1]);\n\t\treturn -EINVAL;\n\t}\n\n\tif (kstrtoull(argv[2], 10, (unsigned long long *)&new_id)) {\n\t\tDMWARN(\"set_transaction_id message: Unrecognised new id %s.\", argv[2]);\n\t\treturn -EINVAL;\n\t}\n\n\tr = dm_pool_set_metadata_transaction_id(pool->pmd, old_id, new_id);\n\tif (r) {\n\t\tDMWARN(\"Failed to change transaction id from %s to %s.\",\n\t\t       argv[1], argv[2]);\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic int process_reserve_metadata_snap_mesg(unsigned int argc, char **argv, struct pool *pool)\n{\n\tint r;\n\n\tr = check_arg_count(argc, 1);\n\tif (r)\n\t\treturn r;\n\n\t(void) commit(pool);\n\n\tr = dm_pool_reserve_metadata_snap(pool->pmd);\n\tif (r)\n\t\tDMWARN(\"reserve_metadata_snap message failed.\");\n\n\treturn r;\n}\n\nstatic int process_release_metadata_snap_mesg(unsigned int argc, char **argv, struct pool *pool)\n{\n\tint r;\n\n\tr = check_arg_count(argc, 1);\n\tif (r)\n\t\treturn r;\n\n\tr = dm_pool_release_metadata_snap(pool->pmd);\n\tif (r)\n\t\tDMWARN(\"release_metadata_snap message failed.\");\n\n\treturn r;\n}\n\n \nstatic int pool_message(struct dm_target *ti, unsigned int argc, char **argv,\n\t\t\tchar *result, unsigned int maxlen)\n{\n\tint r = -EINVAL;\n\tstruct pool_c *pt = ti->private;\n\tstruct pool *pool = pt->pool;\n\n\tif (get_pool_mode(pool) >= PM_OUT_OF_METADATA_SPACE) {\n\t\tDMERR(\"%s: unable to service pool target messages in READ_ONLY or FAIL mode\",\n\t\t      dm_device_name(pool->pool_md));\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!strcasecmp(argv[0], \"create_thin\"))\n\t\tr = process_create_thin_mesg(argc, argv, pool);\n\n\telse if (!strcasecmp(argv[0], \"create_snap\"))\n\t\tr = process_create_snap_mesg(argc, argv, pool);\n\n\telse if (!strcasecmp(argv[0], \"delete\"))\n\t\tr = process_delete_mesg(argc, argv, pool);\n\n\telse if (!strcasecmp(argv[0], \"set_transaction_id\"))\n\t\tr = process_set_transaction_id_mesg(argc, argv, pool);\n\n\telse if (!strcasecmp(argv[0], \"reserve_metadata_snap\"))\n\t\tr = process_reserve_metadata_snap_mesg(argc, argv, pool);\n\n\telse if (!strcasecmp(argv[0], \"release_metadata_snap\"))\n\t\tr = process_release_metadata_snap_mesg(argc, argv, pool);\n\n\telse\n\t\tDMWARN(\"Unrecognised thin pool target message received: %s\", argv[0]);\n\n\tif (!r)\n\t\t(void) commit(pool);\n\n\treturn r;\n}\n\nstatic void emit_flags(struct pool_features *pf, char *result,\n\t\t       unsigned int sz, unsigned int maxlen)\n{\n\tunsigned int count = !pf->zero_new_blocks + !pf->discard_enabled +\n\t\t!pf->discard_passdown + (pf->mode == PM_READ_ONLY) +\n\t\tpf->error_if_no_space;\n\tDMEMIT(\"%u \", count);\n\n\tif (!pf->zero_new_blocks)\n\t\tDMEMIT(\"skip_block_zeroing \");\n\n\tif (!pf->discard_enabled)\n\t\tDMEMIT(\"ignore_discard \");\n\n\tif (!pf->discard_passdown)\n\t\tDMEMIT(\"no_discard_passdown \");\n\n\tif (pf->mode == PM_READ_ONLY)\n\t\tDMEMIT(\"read_only \");\n\n\tif (pf->error_if_no_space)\n\t\tDMEMIT(\"error_if_no_space \");\n}\n\n \nstatic void pool_status(struct dm_target *ti, status_type_t type,\n\t\t\tunsigned int status_flags, char *result, unsigned int maxlen)\n{\n\tint r;\n\tunsigned int sz = 0;\n\tuint64_t transaction_id;\n\tdm_block_t nr_free_blocks_data;\n\tdm_block_t nr_free_blocks_metadata;\n\tdm_block_t nr_blocks_data;\n\tdm_block_t nr_blocks_metadata;\n\tdm_block_t held_root;\n\tenum pool_mode mode;\n\tchar buf[BDEVNAME_SIZE];\n\tchar buf2[BDEVNAME_SIZE];\n\tstruct pool_c *pt = ti->private;\n\tstruct pool *pool = pt->pool;\n\n\tswitch (type) {\n\tcase STATUSTYPE_INFO:\n\t\tif (get_pool_mode(pool) == PM_FAIL) {\n\t\t\tDMEMIT(\"Fail\");\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (!(status_flags & DM_STATUS_NOFLUSH_FLAG) && !dm_suspended(ti))\n\t\t\t(void) commit(pool);\n\n\t\tr = dm_pool_get_metadata_transaction_id(pool->pmd, &transaction_id);\n\t\tif (r) {\n\t\t\tDMERR(\"%s: dm_pool_get_metadata_transaction_id returned %d\",\n\t\t\t      dm_device_name(pool->pool_md), r);\n\t\t\tgoto err;\n\t\t}\n\n\t\tr = dm_pool_get_free_metadata_block_count(pool->pmd, &nr_free_blocks_metadata);\n\t\tif (r) {\n\t\t\tDMERR(\"%s: dm_pool_get_free_metadata_block_count returned %d\",\n\t\t\t      dm_device_name(pool->pool_md), r);\n\t\t\tgoto err;\n\t\t}\n\n\t\tr = dm_pool_get_metadata_dev_size(pool->pmd, &nr_blocks_metadata);\n\t\tif (r) {\n\t\t\tDMERR(\"%s: dm_pool_get_metadata_dev_size returned %d\",\n\t\t\t      dm_device_name(pool->pool_md), r);\n\t\t\tgoto err;\n\t\t}\n\n\t\tr = dm_pool_get_free_block_count(pool->pmd, &nr_free_blocks_data);\n\t\tif (r) {\n\t\t\tDMERR(\"%s: dm_pool_get_free_block_count returned %d\",\n\t\t\t      dm_device_name(pool->pool_md), r);\n\t\t\tgoto err;\n\t\t}\n\n\t\tr = dm_pool_get_data_dev_size(pool->pmd, &nr_blocks_data);\n\t\tif (r) {\n\t\t\tDMERR(\"%s: dm_pool_get_data_dev_size returned %d\",\n\t\t\t      dm_device_name(pool->pool_md), r);\n\t\t\tgoto err;\n\t\t}\n\n\t\tr = dm_pool_get_metadata_snap(pool->pmd, &held_root);\n\t\tif (r) {\n\t\t\tDMERR(\"%s: dm_pool_get_metadata_snap returned %d\",\n\t\t\t      dm_device_name(pool->pool_md), r);\n\t\t\tgoto err;\n\t\t}\n\n\t\tDMEMIT(\"%llu %llu/%llu %llu/%llu \",\n\t\t       (unsigned long long)transaction_id,\n\t\t       (unsigned long long)(nr_blocks_metadata - nr_free_blocks_metadata),\n\t\t       (unsigned long long)nr_blocks_metadata,\n\t\t       (unsigned long long)(nr_blocks_data - nr_free_blocks_data),\n\t\t       (unsigned long long)nr_blocks_data);\n\n\t\tif (held_root)\n\t\t\tDMEMIT(\"%llu \", held_root);\n\t\telse\n\t\t\tDMEMIT(\"- \");\n\n\t\tmode = get_pool_mode(pool);\n\t\tif (mode == PM_OUT_OF_DATA_SPACE)\n\t\t\tDMEMIT(\"out_of_data_space \");\n\t\telse if (is_read_only_pool_mode(mode))\n\t\t\tDMEMIT(\"ro \");\n\t\telse\n\t\t\tDMEMIT(\"rw \");\n\n\t\tif (!pool->pf.discard_enabled)\n\t\t\tDMEMIT(\"ignore_discard \");\n\t\telse if (pool->pf.discard_passdown)\n\t\t\tDMEMIT(\"discard_passdown \");\n\t\telse\n\t\t\tDMEMIT(\"no_discard_passdown \");\n\n\t\tif (pool->pf.error_if_no_space)\n\t\t\tDMEMIT(\"error_if_no_space \");\n\t\telse\n\t\t\tDMEMIT(\"queue_if_no_space \");\n\n\t\tif (dm_pool_metadata_needs_check(pool->pmd))\n\t\t\tDMEMIT(\"needs_check \");\n\t\telse\n\t\t\tDMEMIT(\"- \");\n\n\t\tDMEMIT(\"%llu \", (unsigned long long)calc_metadata_threshold(pt));\n\n\t\tbreak;\n\n\tcase STATUSTYPE_TABLE:\n\t\tDMEMIT(\"%s %s %lu %llu \",\n\t\t       format_dev_t(buf, pt->metadata_dev->bdev->bd_dev),\n\t\t       format_dev_t(buf2, pt->data_dev->bdev->bd_dev),\n\t\t       (unsigned long)pool->sectors_per_block,\n\t\t       (unsigned long long)pt->low_water_blocks);\n\t\temit_flags(&pt->requested_pf, result, sz, maxlen);\n\t\tbreak;\n\n\tcase STATUSTYPE_IMA:\n\t\t*result = '\\0';\n\t\tbreak;\n\t}\n\treturn;\n\nerr:\n\tDMEMIT(\"Error\");\n}\n\nstatic int pool_iterate_devices(struct dm_target *ti,\n\t\t\t\titerate_devices_callout_fn fn, void *data)\n{\n\tstruct pool_c *pt = ti->private;\n\n\treturn fn(ti, pt->data_dev, 0, ti->len, data);\n}\n\nstatic void pool_io_hints(struct dm_target *ti, struct queue_limits *limits)\n{\n\tstruct pool_c *pt = ti->private;\n\tstruct pool *pool = pt->pool;\n\tsector_t io_opt_sectors = limits->io_opt >> SECTOR_SHIFT;\n\n\t \n\tif (limits->max_sectors < pool->sectors_per_block) {\n\t\twhile (!is_factor(pool->sectors_per_block, limits->max_sectors)) {\n\t\t\tif ((limits->max_sectors & (limits->max_sectors - 1)) == 0)\n\t\t\t\tlimits->max_sectors--;\n\t\t\tlimits->max_sectors = rounddown_pow_of_two(limits->max_sectors);\n\t\t}\n\t}\n\n\t \n\tif (io_opt_sectors < pool->sectors_per_block ||\n\t    !is_factor(io_opt_sectors, pool->sectors_per_block)) {\n\t\tif (is_factor(pool->sectors_per_block, limits->max_sectors))\n\t\t\tblk_limits_io_min(limits, limits->max_sectors << SECTOR_SHIFT);\n\t\telse\n\t\t\tblk_limits_io_min(limits, pool->sectors_per_block << SECTOR_SHIFT);\n\t\tblk_limits_io_opt(limits, pool->sectors_per_block << SECTOR_SHIFT);\n\t}\n\n\t \n\n\tif (pt->adjusted_pf.discard_enabled) {\n\t\tdisable_discard_passdown_if_not_supported(pt);\n\t\tif (!pt->adjusted_pf.discard_passdown)\n\t\t\tlimits->max_discard_sectors = 0;\n\t\t \n\t} else {\n\t\t \n\t\tlimits->discard_granularity = 0;\n\t}\n}\n\nstatic struct target_type pool_target = {\n\t.name = \"thin-pool\",\n\t.features = DM_TARGET_SINGLETON | DM_TARGET_ALWAYS_WRITEABLE |\n\t\t    DM_TARGET_IMMUTABLE,\n\t.version = {1, 23, 0},\n\t.module = THIS_MODULE,\n\t.ctr = pool_ctr,\n\t.dtr = pool_dtr,\n\t.map = pool_map,\n\t.presuspend = pool_presuspend,\n\t.presuspend_undo = pool_presuspend_undo,\n\t.postsuspend = pool_postsuspend,\n\t.preresume = pool_preresume,\n\t.resume = pool_resume,\n\t.message = pool_message,\n\t.status = pool_status,\n\t.iterate_devices = pool_iterate_devices,\n\t.io_hints = pool_io_hints,\n};\n\n \nstatic void thin_get(struct thin_c *tc)\n{\n\trefcount_inc(&tc->refcount);\n}\n\nstatic void thin_put(struct thin_c *tc)\n{\n\tif (refcount_dec_and_test(&tc->refcount))\n\t\tcomplete(&tc->can_destroy);\n}\n\nstatic void thin_dtr(struct dm_target *ti)\n{\n\tstruct thin_c *tc = ti->private;\n\n\tspin_lock_irq(&tc->pool->lock);\n\tlist_del_rcu(&tc->list);\n\tspin_unlock_irq(&tc->pool->lock);\n\tsynchronize_rcu();\n\n\tthin_put(tc);\n\twait_for_completion(&tc->can_destroy);\n\n\tmutex_lock(&dm_thin_pool_table.mutex);\n\n\t__pool_dec(tc->pool);\n\tdm_pool_close_thin_device(tc->td);\n\tdm_put_device(ti, tc->pool_dev);\n\tif (tc->origin_dev)\n\t\tdm_put_device(ti, tc->origin_dev);\n\tkfree(tc);\n\n\tmutex_unlock(&dm_thin_pool_table.mutex);\n}\n\n \nstatic int thin_ctr(struct dm_target *ti, unsigned int argc, char **argv)\n{\n\tint r;\n\tstruct thin_c *tc;\n\tstruct dm_dev *pool_dev, *origin_dev;\n\tstruct mapped_device *pool_md;\n\n\tmutex_lock(&dm_thin_pool_table.mutex);\n\n\tif (argc != 2 && argc != 3) {\n\t\tti->error = \"Invalid argument count\";\n\t\tr = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\ttc = ti->private = kzalloc(sizeof(*tc), GFP_KERNEL);\n\tif (!tc) {\n\t\tti->error = \"Out of memory\";\n\t\tr = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\ttc->thin_md = dm_table_get_md(ti->table);\n\tspin_lock_init(&tc->lock);\n\tINIT_LIST_HEAD(&tc->deferred_cells);\n\tbio_list_init(&tc->deferred_bio_list);\n\tbio_list_init(&tc->retry_on_resume_list);\n\ttc->sort_bio_list = RB_ROOT;\n\n\tif (argc == 3) {\n\t\tif (!strcmp(argv[0], argv[2])) {\n\t\t\tti->error = \"Error setting origin device\";\n\t\t\tr = -EINVAL;\n\t\t\tgoto bad_origin_dev;\n\t\t}\n\n\t\tr = dm_get_device(ti, argv[2], BLK_OPEN_READ, &origin_dev);\n\t\tif (r) {\n\t\t\tti->error = \"Error opening origin device\";\n\t\t\tgoto bad_origin_dev;\n\t\t}\n\t\ttc->origin_dev = origin_dev;\n\t}\n\n\tr = dm_get_device(ti, argv[0], dm_table_get_mode(ti->table), &pool_dev);\n\tif (r) {\n\t\tti->error = \"Error opening pool device\";\n\t\tgoto bad_pool_dev;\n\t}\n\ttc->pool_dev = pool_dev;\n\n\tif (read_dev_id(argv[1], (unsigned long long *)&tc->dev_id, 0)) {\n\t\tti->error = \"Invalid device id\";\n\t\tr = -EINVAL;\n\t\tgoto bad_common;\n\t}\n\n\tpool_md = dm_get_md(tc->pool_dev->bdev->bd_dev);\n\tif (!pool_md) {\n\t\tti->error = \"Couldn't get pool mapped device\";\n\t\tr = -EINVAL;\n\t\tgoto bad_common;\n\t}\n\n\ttc->pool = __pool_table_lookup(pool_md);\n\tif (!tc->pool) {\n\t\tti->error = \"Couldn't find pool object\";\n\t\tr = -EINVAL;\n\t\tgoto bad_pool_lookup;\n\t}\n\t__pool_inc(tc->pool);\n\n\tif (get_pool_mode(tc->pool) == PM_FAIL) {\n\t\tti->error = \"Couldn't open thin device, Pool is in fail mode\";\n\t\tr = -EINVAL;\n\t\tgoto bad_pool;\n\t}\n\n\tr = dm_pool_open_thin_device(tc->pool->pmd, tc->dev_id, &tc->td);\n\tif (r) {\n\t\tti->error = \"Couldn't open thin internal device\";\n\t\tgoto bad_pool;\n\t}\n\n\tr = dm_set_target_max_io_len(ti, tc->pool->sectors_per_block);\n\tif (r)\n\t\tgoto bad;\n\n\tti->num_flush_bios = 1;\n\tti->limit_swap_bios = true;\n\tti->flush_supported = true;\n\tti->accounts_remapped_io = true;\n\tti->per_io_data_size = sizeof(struct dm_thin_endio_hook);\n\n\t \n\tif (tc->pool->pf.discard_enabled) {\n\t\tti->discards_supported = true;\n\t\tti->num_discard_bios = 1;\n\t\tti->max_discard_granularity = true;\n\t}\n\n\tmutex_unlock(&dm_thin_pool_table.mutex);\n\n\tspin_lock_irq(&tc->pool->lock);\n\tif (tc->pool->suspended) {\n\t\tspin_unlock_irq(&tc->pool->lock);\n\t\tmutex_lock(&dm_thin_pool_table.mutex);  \n\t\tti->error = \"Unable to activate thin device while pool is suspended\";\n\t\tr = -EINVAL;\n\t\tgoto bad;\n\t}\n\trefcount_set(&tc->refcount, 1);\n\tinit_completion(&tc->can_destroy);\n\tlist_add_tail_rcu(&tc->list, &tc->pool->active_thins);\n\tspin_unlock_irq(&tc->pool->lock);\n\t \n\tsynchronize_rcu();\n\n\tdm_put(pool_md);\n\n\treturn 0;\n\nbad:\n\tdm_pool_close_thin_device(tc->td);\nbad_pool:\n\t__pool_dec(tc->pool);\nbad_pool_lookup:\n\tdm_put(pool_md);\nbad_common:\n\tdm_put_device(ti, tc->pool_dev);\nbad_pool_dev:\n\tif (tc->origin_dev)\n\t\tdm_put_device(ti, tc->origin_dev);\nbad_origin_dev:\n\tkfree(tc);\nout_unlock:\n\tmutex_unlock(&dm_thin_pool_table.mutex);\n\n\treturn r;\n}\n\nstatic int thin_map(struct dm_target *ti, struct bio *bio)\n{\n\tbio->bi_iter.bi_sector = dm_target_offset(ti, bio->bi_iter.bi_sector);\n\n\treturn thin_bio_map(ti, bio);\n}\n\nstatic int thin_endio(struct dm_target *ti, struct bio *bio,\n\t\tblk_status_t *err)\n{\n\tunsigned long flags;\n\tstruct dm_thin_endio_hook *h = dm_per_bio_data(bio, sizeof(struct dm_thin_endio_hook));\n\tstruct list_head work;\n\tstruct dm_thin_new_mapping *m, *tmp;\n\tstruct pool *pool = h->tc->pool;\n\n\tif (h->shared_read_entry) {\n\t\tINIT_LIST_HEAD(&work);\n\t\tdm_deferred_entry_dec(h->shared_read_entry, &work);\n\n\t\tspin_lock_irqsave(&pool->lock, flags);\n\t\tlist_for_each_entry_safe(m, tmp, &work, list) {\n\t\t\tlist_del(&m->list);\n\t\t\t__complete_mapping_preparation(m);\n\t\t}\n\t\tspin_unlock_irqrestore(&pool->lock, flags);\n\t}\n\n\tif (h->all_io_entry) {\n\t\tINIT_LIST_HEAD(&work);\n\t\tdm_deferred_entry_dec(h->all_io_entry, &work);\n\t\tif (!list_empty(&work)) {\n\t\t\tspin_lock_irqsave(&pool->lock, flags);\n\t\t\tlist_for_each_entry_safe(m, tmp, &work, list)\n\t\t\t\tlist_add_tail(&m->list, &pool->prepared_discards);\n\t\t\tspin_unlock_irqrestore(&pool->lock, flags);\n\t\t\twake_worker(pool);\n\t\t}\n\t}\n\n\tif (h->cell)\n\t\tcell_defer_no_holder(h->tc, h->cell);\n\n\treturn DM_ENDIO_DONE;\n}\n\nstatic void thin_presuspend(struct dm_target *ti)\n{\n\tstruct thin_c *tc = ti->private;\n\n\tif (dm_noflush_suspending(ti))\n\t\tnoflush_work(tc, do_noflush_start);\n}\n\nstatic void thin_postsuspend(struct dm_target *ti)\n{\n\tstruct thin_c *tc = ti->private;\n\n\t \n\tnoflush_work(tc, do_noflush_stop);\n}\n\nstatic int thin_preresume(struct dm_target *ti)\n{\n\tstruct thin_c *tc = ti->private;\n\n\tif (tc->origin_dev)\n\t\ttc->origin_size = get_dev_size(tc->origin_dev->bdev);\n\n\treturn 0;\n}\n\n \nstatic void thin_status(struct dm_target *ti, status_type_t type,\n\t\t\tunsigned int status_flags, char *result, unsigned int maxlen)\n{\n\tint r;\n\tssize_t sz = 0;\n\tdm_block_t mapped, highest;\n\tchar buf[BDEVNAME_SIZE];\n\tstruct thin_c *tc = ti->private;\n\n\tif (get_pool_mode(tc->pool) == PM_FAIL) {\n\t\tDMEMIT(\"Fail\");\n\t\treturn;\n\t}\n\n\tif (!tc->td)\n\t\tDMEMIT(\"-\");\n\telse {\n\t\tswitch (type) {\n\t\tcase STATUSTYPE_INFO:\n\t\t\tr = dm_thin_get_mapped_count(tc->td, &mapped);\n\t\t\tif (r) {\n\t\t\t\tDMERR(\"dm_thin_get_mapped_count returned %d\", r);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tr = dm_thin_get_highest_mapped_block(tc->td, &highest);\n\t\t\tif (r < 0) {\n\t\t\t\tDMERR(\"dm_thin_get_highest_mapped_block returned %d\", r);\n\t\t\t\tgoto err;\n\t\t\t}\n\n\t\t\tDMEMIT(\"%llu \", mapped * tc->pool->sectors_per_block);\n\t\t\tif (r)\n\t\t\t\tDMEMIT(\"%llu\", ((highest + 1) *\n\t\t\t\t\t\ttc->pool->sectors_per_block) - 1);\n\t\t\telse\n\t\t\t\tDMEMIT(\"-\");\n\t\t\tbreak;\n\n\t\tcase STATUSTYPE_TABLE:\n\t\t\tDMEMIT(\"%s %lu\",\n\t\t\t       format_dev_t(buf, tc->pool_dev->bdev->bd_dev),\n\t\t\t       (unsigned long) tc->dev_id);\n\t\t\tif (tc->origin_dev)\n\t\t\t\tDMEMIT(\" %s\", format_dev_t(buf, tc->origin_dev->bdev->bd_dev));\n\t\t\tbreak;\n\n\t\tcase STATUSTYPE_IMA:\n\t\t\t*result = '\\0';\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn;\n\nerr:\n\tDMEMIT(\"Error\");\n}\n\nstatic int thin_iterate_devices(struct dm_target *ti,\n\t\t\t\titerate_devices_callout_fn fn, void *data)\n{\n\tsector_t blocks;\n\tstruct thin_c *tc = ti->private;\n\tstruct pool *pool = tc->pool;\n\n\t \n\tif (!pool->ti)\n\t\treturn 0;\t \n\n\tblocks = pool->ti->len;\n\t(void) sector_div(blocks, pool->sectors_per_block);\n\tif (blocks)\n\t\treturn fn(ti, tc->pool_dev, 0, pool->sectors_per_block * blocks, data);\n\n\treturn 0;\n}\n\nstatic void thin_io_hints(struct dm_target *ti, struct queue_limits *limits)\n{\n\tstruct thin_c *tc = ti->private;\n\tstruct pool *pool = tc->pool;\n\n\tif (pool->pf.discard_enabled) {\n\t\tlimits->discard_granularity = pool->sectors_per_block << SECTOR_SHIFT;\n\t\tlimits->max_discard_sectors = pool->sectors_per_block * BIO_PRISON_MAX_RANGE;\n\t}\n}\n\nstatic struct target_type thin_target = {\n\t.name = \"thin\",\n\t.version = {1, 23, 0},\n\t.module\t= THIS_MODULE,\n\t.ctr = thin_ctr,\n\t.dtr = thin_dtr,\n\t.map = thin_map,\n\t.end_io = thin_endio,\n\t.preresume = thin_preresume,\n\t.presuspend = thin_presuspend,\n\t.postsuspend = thin_postsuspend,\n\t.status = thin_status,\n\t.iterate_devices = thin_iterate_devices,\n\t.io_hints = thin_io_hints,\n};\n\n \n\nstatic int __init dm_thin_init(void)\n{\n\tint r = -ENOMEM;\n\n\tpool_table_init();\n\n\t_new_mapping_cache = KMEM_CACHE(dm_thin_new_mapping, 0);\n\tif (!_new_mapping_cache)\n\t\treturn r;\n\n\tr = dm_register_target(&thin_target);\n\tif (r)\n\t\tgoto bad_new_mapping_cache;\n\n\tr = dm_register_target(&pool_target);\n\tif (r)\n\t\tgoto bad_thin_target;\n\n\treturn 0;\n\nbad_thin_target:\n\tdm_unregister_target(&thin_target);\nbad_new_mapping_cache:\n\tkmem_cache_destroy(_new_mapping_cache);\n\n\treturn r;\n}\n\nstatic void dm_thin_exit(void)\n{\n\tdm_unregister_target(&thin_target);\n\tdm_unregister_target(&pool_target);\n\n\tkmem_cache_destroy(_new_mapping_cache);\n\n\tpool_table_exit();\n}\n\nmodule_init(dm_thin_init);\nmodule_exit(dm_thin_exit);\n\nmodule_param_named(no_space_timeout, no_space_timeout_secs, uint, 0644);\nMODULE_PARM_DESC(no_space_timeout, \"Out of data space queue IO timeout in seconds\");\n\nMODULE_DESCRIPTION(DM_NAME \" thin provisioning target\");\nMODULE_AUTHOR(\"Joe Thornber <dm-devel@redhat.com>\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}