{
  "module_name": "raid5-ppl.c",
  "hash_id": "d95a9bdffd87a0fa8630da30e03ee6831166604facf576e5a0f964dc2ce0fe95",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/raid5-ppl.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/blkdev.h>\n#include <linux/slab.h>\n#include <linux/crc32c.h>\n#include <linux/async_tx.h>\n#include <linux/raid/md_p.h>\n#include \"md.h\"\n#include \"raid5.h\"\n#include \"raid5-log.h\"\n\n \n\n#define PPL_SPACE_SIZE (128 * 1024)\n\nstruct ppl_conf {\n\tstruct mddev *mddev;\n\n\t \n\tstruct ppl_log *child_logs;\n\tint count;\n\n\tint block_size;\t\t \n\tu32 signature;\t\t \n\tatomic64_t seq;\t\t \n\n\tstruct kmem_cache *io_kc;\n\tmempool_t io_pool;\n\tstruct bio_set bs;\n\tstruct bio_set flush_bs;\n\n\t \n\tint recovered_entries;\n\tint mismatch_count;\n\n\t \n\tstruct list_head no_mem_stripes;\n\tspinlock_t no_mem_stripes_lock;\n\n\tunsigned short write_hint;\n};\n\nstruct ppl_log {\n\tstruct ppl_conf *ppl_conf;\t \n\n\tstruct md_rdev *rdev;\t\t \n\tstruct mutex io_mutex;\n\tstruct ppl_io_unit *current_io;\t \n\tspinlock_t io_list_lock;\n\tstruct list_head io_list;\t \n\n\tsector_t next_io_sector;\n\tunsigned int entry_space;\n\tbool use_multippl;\n\tbool wb_cache_on;\n\tunsigned long disk_flush_bitmap;\n};\n\n#define PPL_IO_INLINE_BVECS 32\n\nstruct ppl_io_unit {\n\tstruct ppl_log *log;\n\n\tstruct page *header_page;\t \n\n\tunsigned int entries_count;\t \n\tunsigned int pp_size;\t\t \n\n\tu64 seq;\t\t\t \n\tstruct list_head log_sibling;\t \n\n\tstruct list_head stripe_list;\t \n\tatomic_t pending_stripes;\t \n\tatomic_t pending_flushes;\t \n\n\tbool submitted;\t\t\t \n\n\t \n\tstruct bio bio;\n\tstruct bio_vec biovec[PPL_IO_INLINE_BVECS];\n};\n\nstruct dma_async_tx_descriptor *\nops_run_partial_parity(struct stripe_head *sh, struct raid5_percpu *percpu,\n\t\t       struct dma_async_tx_descriptor *tx)\n{\n\tint disks = sh->disks;\n\tstruct page **srcs = percpu->scribble;\n\tint count = 0, pd_idx = sh->pd_idx, i;\n\tstruct async_submit_ctl submit;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__, (unsigned long long)sh->sector);\n\n\t \n\tif (sh->reconstruct_state == reconstruct_state_prexor_drain_run) {\n\t\t \n\t\tsrcs[count++] = sh->dev[pd_idx].page;\n\t} else if (sh->reconstruct_state == reconstruct_state_drain_run) {\n\t\t \n\t\tfor (i = disks; i--;) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (test_bit(R5_UPTODATE, &dev->flags))\n\t\t\t\tsrcs[count++] = dev->page;\n\t\t}\n\t} else {\n\t\treturn tx;\n\t}\n\n\tinit_async_submit(&submit, ASYNC_TX_FENCE|ASYNC_TX_XOR_ZERO_DST, tx,\n\t\t\t  NULL, sh, (void *) (srcs + sh->disks + 2));\n\n\tif (count == 1)\n\t\ttx = async_memcpy(sh->ppl_page, srcs[0], 0, 0, PAGE_SIZE,\n\t\t\t\t  &submit);\n\telse\n\t\ttx = async_xor(sh->ppl_page, srcs, 0, count, PAGE_SIZE,\n\t\t\t       &submit);\n\n\treturn tx;\n}\n\nstatic void *ppl_io_pool_alloc(gfp_t gfp_mask, void *pool_data)\n{\n\tstruct kmem_cache *kc = pool_data;\n\tstruct ppl_io_unit *io;\n\n\tio = kmem_cache_alloc(kc, gfp_mask);\n\tif (!io)\n\t\treturn NULL;\n\n\tio->header_page = alloc_page(gfp_mask);\n\tif (!io->header_page) {\n\t\tkmem_cache_free(kc, io);\n\t\treturn NULL;\n\t}\n\n\treturn io;\n}\n\nstatic void ppl_io_pool_free(void *element, void *pool_data)\n{\n\tstruct kmem_cache *kc = pool_data;\n\tstruct ppl_io_unit *io = element;\n\n\t__free_page(io->header_page);\n\tkmem_cache_free(kc, io);\n}\n\nstatic struct ppl_io_unit *ppl_new_iounit(struct ppl_log *log,\n\t\t\t\t\t  struct stripe_head *sh)\n{\n\tstruct ppl_conf *ppl_conf = log->ppl_conf;\n\tstruct ppl_io_unit *io;\n\tstruct ppl_header *pplhdr;\n\tstruct page *header_page;\n\n\tio = mempool_alloc(&ppl_conf->io_pool, GFP_NOWAIT);\n\tif (!io)\n\t\treturn NULL;\n\n\theader_page = io->header_page;\n\tmemset(io, 0, sizeof(*io));\n\tio->header_page = header_page;\n\n\tio->log = log;\n\tINIT_LIST_HEAD(&io->log_sibling);\n\tINIT_LIST_HEAD(&io->stripe_list);\n\tatomic_set(&io->pending_stripes, 0);\n\tatomic_set(&io->pending_flushes, 0);\n\tbio_init(&io->bio, log->rdev->bdev, io->biovec, PPL_IO_INLINE_BVECS,\n\t\t REQ_OP_WRITE | REQ_FUA);\n\n\tpplhdr = page_address(io->header_page);\n\tclear_page(pplhdr);\n\tmemset(pplhdr->reserved, 0xff, PPL_HDR_RESERVED);\n\tpplhdr->signature = cpu_to_le32(ppl_conf->signature);\n\n\tio->seq = atomic64_add_return(1, &ppl_conf->seq);\n\tpplhdr->generation = cpu_to_le64(io->seq);\n\n\treturn io;\n}\n\nstatic int ppl_log_stripe(struct ppl_log *log, struct stripe_head *sh)\n{\n\tstruct ppl_io_unit *io = log->current_io;\n\tstruct ppl_header_entry *e = NULL;\n\tstruct ppl_header *pplhdr;\n\tint i;\n\tsector_t data_sector = 0;\n\tint data_disks = 0;\n\tstruct r5conf *conf = sh->raid_conf;\n\n\tpr_debug(\"%s: stripe: %llu\\n\", __func__, (unsigned long long)sh->sector);\n\n\t \n\tif (io && (io->pp_size == log->entry_space ||\n\t\t   io->entries_count == PPL_HDR_MAX_ENTRIES)) {\n\t\tpr_debug(\"%s: add io_unit blocked by seq: %llu\\n\",\n\t\t\t __func__, io->seq);\n\t\tio = NULL;\n\t}\n\n\t \n\tif (!io) {\n\t\tio = ppl_new_iounit(log, sh);\n\t\tif (!io)\n\t\t\treturn -ENOMEM;\n\t\tspin_lock_irq(&log->io_list_lock);\n\t\tlist_add_tail(&io->log_sibling, &log->io_list);\n\t\tspin_unlock_irq(&log->io_list_lock);\n\n\t\tlog->current_io = io;\n\t}\n\n\tfor (i = 0; i < sh->disks; i++) {\n\t\tstruct r5dev *dev = &sh->dev[i];\n\n\t\tif (i != sh->pd_idx && test_bit(R5_Wantwrite, &dev->flags)) {\n\t\t\tif (!data_disks || dev->sector < data_sector)\n\t\t\t\tdata_sector = dev->sector;\n\t\t\tdata_disks++;\n\t\t}\n\t}\n\tBUG_ON(!data_disks);\n\n\tpr_debug(\"%s: seq: %llu data_sector: %llu data_disks: %d\\n\", __func__,\n\t\t io->seq, (unsigned long long)data_sector, data_disks);\n\n\tpplhdr = page_address(io->header_page);\n\n\tif (io->entries_count > 0) {\n\t\tstruct ppl_header_entry *last =\n\t\t\t\t&pplhdr->entries[io->entries_count - 1];\n\t\tstruct stripe_head *sh_last = list_last_entry(\n\t\t\t\t&io->stripe_list, struct stripe_head, log_list);\n\t\tu64 data_sector_last = le64_to_cpu(last->data_sector);\n\t\tu32 data_size_last = le32_to_cpu(last->data_size);\n\n\t\t \n\t\tif ((sh->sector == sh_last->sector + RAID5_STRIPE_SECTORS(conf)) &&\n\t\t    (data_sector >> ilog2(conf->chunk_sectors) ==\n\t\t     data_sector_last >> ilog2(conf->chunk_sectors)) &&\n\t\t    ((data_sector - data_sector_last) * data_disks ==\n\t\t     data_size_last >> 9))\n\t\t\te = last;\n\t}\n\n\tif (!e) {\n\t\te = &pplhdr->entries[io->entries_count++];\n\t\te->data_sector = cpu_to_le64(data_sector);\n\t\te->parity_disk = cpu_to_le32(sh->pd_idx);\n\t\te->checksum = cpu_to_le32(~0);\n\t}\n\n\tle32_add_cpu(&e->data_size, data_disks << PAGE_SHIFT);\n\n\t \n\tif (!test_bit(STRIPE_FULL_WRITE, &sh->state)) {\n\t\tle32_add_cpu(&e->pp_size, PAGE_SIZE);\n\t\tio->pp_size += PAGE_SIZE;\n\t\te->checksum = cpu_to_le32(crc32c_le(le32_to_cpu(e->checksum),\n\t\t\t\t\t\t    page_address(sh->ppl_page),\n\t\t\t\t\t\t    PAGE_SIZE));\n\t}\n\n\tlist_add_tail(&sh->log_list, &io->stripe_list);\n\tatomic_inc(&io->pending_stripes);\n\tsh->ppl_io = io;\n\n\treturn 0;\n}\n\nint ppl_write_stripe(struct r5conf *conf, struct stripe_head *sh)\n{\n\tstruct ppl_conf *ppl_conf = conf->log_private;\n\tstruct ppl_io_unit *io = sh->ppl_io;\n\tstruct ppl_log *log;\n\n\tif (io || test_bit(STRIPE_SYNCING, &sh->state) || !sh->ppl_page ||\n\t    !test_bit(R5_Wantwrite, &sh->dev[sh->pd_idx].flags) ||\n\t    !test_bit(R5_Insync, &sh->dev[sh->pd_idx].flags)) {\n\t\tclear_bit(STRIPE_LOG_TRAPPED, &sh->state);\n\t\treturn -EAGAIN;\n\t}\n\n\tlog = &ppl_conf->child_logs[sh->pd_idx];\n\n\tmutex_lock(&log->io_mutex);\n\n\tif (!log->rdev || test_bit(Faulty, &log->rdev->flags)) {\n\t\tmutex_unlock(&log->io_mutex);\n\t\treturn -EAGAIN;\n\t}\n\n\tset_bit(STRIPE_LOG_TRAPPED, &sh->state);\n\tclear_bit(STRIPE_DELAYED, &sh->state);\n\tatomic_inc(&sh->count);\n\n\tif (ppl_log_stripe(log, sh)) {\n\t\tspin_lock_irq(&ppl_conf->no_mem_stripes_lock);\n\t\tlist_add_tail(&sh->log_list, &ppl_conf->no_mem_stripes);\n\t\tspin_unlock_irq(&ppl_conf->no_mem_stripes_lock);\n\t}\n\n\tmutex_unlock(&log->io_mutex);\n\n\treturn 0;\n}\n\nstatic void ppl_log_endio(struct bio *bio)\n{\n\tstruct ppl_io_unit *io = bio->bi_private;\n\tstruct ppl_log *log = io->log;\n\tstruct ppl_conf *ppl_conf = log->ppl_conf;\n\tstruct stripe_head *sh, *next;\n\n\tpr_debug(\"%s: seq: %llu\\n\", __func__, io->seq);\n\n\tif (bio->bi_status)\n\t\tmd_error(ppl_conf->mddev, log->rdev);\n\n\tlist_for_each_entry_safe(sh, next, &io->stripe_list, log_list) {\n\t\tlist_del_init(&sh->log_list);\n\n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\traid5_release_stripe(sh);\n\t}\n}\n\nstatic void ppl_submit_iounit_bio(struct ppl_io_unit *io, struct bio *bio)\n{\n\tpr_debug(\"%s: seq: %llu size: %u sector: %llu dev: %pg\\n\",\n\t\t __func__, io->seq, bio->bi_iter.bi_size,\n\t\t (unsigned long long)bio->bi_iter.bi_sector,\n\t\t bio->bi_bdev);\n\n\tsubmit_bio(bio);\n}\n\nstatic void ppl_submit_iounit(struct ppl_io_unit *io)\n{\n\tstruct ppl_log *log = io->log;\n\tstruct ppl_conf *ppl_conf = log->ppl_conf;\n\tstruct ppl_header *pplhdr = page_address(io->header_page);\n\tstruct bio *bio = &io->bio;\n\tstruct stripe_head *sh;\n\tint i;\n\n\tbio->bi_private = io;\n\n\tif (!log->rdev || test_bit(Faulty, &log->rdev->flags)) {\n\t\tppl_log_endio(bio);\n\t\treturn;\n\t}\n\n\tfor (i = 0; i < io->entries_count; i++) {\n\t\tstruct ppl_header_entry *e = &pplhdr->entries[i];\n\n\t\tpr_debug(\"%s: seq: %llu entry: %d data_sector: %llu pp_size: %u data_size: %u\\n\",\n\t\t\t __func__, io->seq, i, le64_to_cpu(e->data_sector),\n\t\t\t le32_to_cpu(e->pp_size), le32_to_cpu(e->data_size));\n\n\t\te->data_sector = cpu_to_le64(le64_to_cpu(e->data_sector) >>\n\t\t\t\t\t     ilog2(ppl_conf->block_size >> 9));\n\t\te->checksum = cpu_to_le32(~le32_to_cpu(e->checksum));\n\t}\n\n\tpplhdr->entries_count = cpu_to_le32(io->entries_count);\n\tpplhdr->checksum = cpu_to_le32(~crc32c_le(~0, pplhdr, PPL_HEADER_SIZE));\n\n\t \n\tif (log->use_multippl &&\n\t    log->rdev->ppl.sector + log->rdev->ppl.size - log->next_io_sector <\n\t    (PPL_HEADER_SIZE + io->pp_size) >> 9)\n\t\tlog->next_io_sector = log->rdev->ppl.sector;\n\n\n\tbio->bi_end_io = ppl_log_endio;\n\tbio->bi_iter.bi_sector = log->next_io_sector;\n\t__bio_add_page(bio, io->header_page, PAGE_SIZE, 0);\n\n\tpr_debug(\"%s: log->current_io_sector: %llu\\n\", __func__,\n\t    (unsigned long long)log->next_io_sector);\n\n\tif (log->use_multippl)\n\t\tlog->next_io_sector += (PPL_HEADER_SIZE + io->pp_size) >> 9;\n\n\tWARN_ON(log->disk_flush_bitmap != 0);\n\n\tlist_for_each_entry(sh, &io->stripe_list, log_list) {\n\t\tfor (i = 0; i < sh->disks; i++) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\n\t\t\tif ((ppl_conf->child_logs[i].wb_cache_on) &&\n\t\t\t    (test_bit(R5_Wantwrite, &dev->flags))) {\n\t\t\t\tset_bit(i, &log->disk_flush_bitmap);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (test_bit(STRIPE_FULL_WRITE, &sh->state))\n\t\t\tcontinue;\n\n\t\tif (!bio_add_page(bio, sh->ppl_page, PAGE_SIZE, 0)) {\n\t\t\tstruct bio *prev = bio;\n\n\t\t\tbio = bio_alloc_bioset(prev->bi_bdev, BIO_MAX_VECS,\n\t\t\t\t\t       prev->bi_opf, GFP_NOIO,\n\t\t\t\t\t       &ppl_conf->bs);\n\t\t\tbio->bi_iter.bi_sector = bio_end_sector(prev);\n\t\t\t__bio_add_page(bio, sh->ppl_page, PAGE_SIZE, 0);\n\n\t\t\tbio_chain(bio, prev);\n\t\t\tppl_submit_iounit_bio(io, prev);\n\t\t}\n\t}\n\n\tppl_submit_iounit_bio(io, bio);\n}\n\nstatic void ppl_submit_current_io(struct ppl_log *log)\n{\n\tstruct ppl_io_unit *io;\n\n\tspin_lock_irq(&log->io_list_lock);\n\n\tio = list_first_entry_or_null(&log->io_list, struct ppl_io_unit,\n\t\t\t\t      log_sibling);\n\tif (io && io->submitted)\n\t\tio = NULL;\n\n\tspin_unlock_irq(&log->io_list_lock);\n\n\tif (io) {\n\t\tio->submitted = true;\n\n\t\tif (io == log->current_io)\n\t\t\tlog->current_io = NULL;\n\n\t\tppl_submit_iounit(io);\n\t}\n}\n\nvoid ppl_write_stripe_run(struct r5conf *conf)\n{\n\tstruct ppl_conf *ppl_conf = conf->log_private;\n\tstruct ppl_log *log;\n\tint i;\n\n\tfor (i = 0; i < ppl_conf->count; i++) {\n\t\tlog = &ppl_conf->child_logs[i];\n\n\t\tmutex_lock(&log->io_mutex);\n\t\tppl_submit_current_io(log);\n\t\tmutex_unlock(&log->io_mutex);\n\t}\n}\n\nstatic void ppl_io_unit_finished(struct ppl_io_unit *io)\n{\n\tstruct ppl_log *log = io->log;\n\tstruct ppl_conf *ppl_conf = log->ppl_conf;\n\tstruct r5conf *conf = ppl_conf->mddev->private;\n\tunsigned long flags;\n\n\tpr_debug(\"%s: seq: %llu\\n\", __func__, io->seq);\n\n\tlocal_irq_save(flags);\n\n\tspin_lock(&log->io_list_lock);\n\tlist_del(&io->log_sibling);\n\tspin_unlock(&log->io_list_lock);\n\n\tmempool_free(io, &ppl_conf->io_pool);\n\n\tspin_lock(&ppl_conf->no_mem_stripes_lock);\n\tif (!list_empty(&ppl_conf->no_mem_stripes)) {\n\t\tstruct stripe_head *sh;\n\n\t\tsh = list_first_entry(&ppl_conf->no_mem_stripes,\n\t\t\t\t      struct stripe_head, log_list);\n\t\tlist_del_init(&sh->log_list);\n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\traid5_release_stripe(sh);\n\t}\n\tspin_unlock(&ppl_conf->no_mem_stripes_lock);\n\n\tlocal_irq_restore(flags);\n\n\twake_up(&conf->wait_for_quiescent);\n}\n\nstatic void ppl_flush_endio(struct bio *bio)\n{\n\tstruct ppl_io_unit *io = bio->bi_private;\n\tstruct ppl_log *log = io->log;\n\tstruct ppl_conf *ppl_conf = log->ppl_conf;\n\tstruct r5conf *conf = ppl_conf->mddev->private;\n\n\tpr_debug(\"%s: dev: %pg\\n\", __func__, bio->bi_bdev);\n\n\tif (bio->bi_status) {\n\t\tstruct md_rdev *rdev;\n\n\t\trcu_read_lock();\n\t\trdev = md_find_rdev_rcu(conf->mddev, bio_dev(bio));\n\t\tif (rdev)\n\t\t\tmd_error(rdev->mddev, rdev);\n\t\trcu_read_unlock();\n\t}\n\n\tbio_put(bio);\n\n\tif (atomic_dec_and_test(&io->pending_flushes)) {\n\t\tppl_io_unit_finished(io);\n\t\tmd_wakeup_thread(conf->mddev->thread);\n\t}\n}\n\nstatic void ppl_do_flush(struct ppl_io_unit *io)\n{\n\tstruct ppl_log *log = io->log;\n\tstruct ppl_conf *ppl_conf = log->ppl_conf;\n\tstruct r5conf *conf = ppl_conf->mddev->private;\n\tint raid_disks = conf->raid_disks;\n\tint flushed_disks = 0;\n\tint i;\n\n\tatomic_set(&io->pending_flushes, raid_disks);\n\n\tfor_each_set_bit(i, &log->disk_flush_bitmap, raid_disks) {\n\t\tstruct md_rdev *rdev;\n\t\tstruct block_device *bdev = NULL;\n\n\t\trcu_read_lock();\n\t\trdev = rcu_dereference(conf->disks[i].rdev);\n\t\tif (rdev && !test_bit(Faulty, &rdev->flags))\n\t\t\tbdev = rdev->bdev;\n\t\trcu_read_unlock();\n\n\t\tif (bdev) {\n\t\t\tstruct bio *bio;\n\n\t\t\tbio = bio_alloc_bioset(bdev, 0,\n\t\t\t\t\t       REQ_OP_WRITE | REQ_PREFLUSH,\n\t\t\t\t\t       GFP_NOIO, &ppl_conf->flush_bs);\n\t\t\tbio->bi_private = io;\n\t\t\tbio->bi_end_io = ppl_flush_endio;\n\n\t\t\tpr_debug(\"%s: dev: %ps\\n\", __func__, bio->bi_bdev);\n\n\t\t\tsubmit_bio(bio);\n\t\t\tflushed_disks++;\n\t\t}\n\t}\n\n\tlog->disk_flush_bitmap = 0;\n\n\tfor (i = flushed_disks ; i < raid_disks; i++) {\n\t\tif (atomic_dec_and_test(&io->pending_flushes))\n\t\t\tppl_io_unit_finished(io);\n\t}\n}\n\nstatic inline bool ppl_no_io_unit_submitted(struct r5conf *conf,\n\t\t\t\t\t    struct ppl_log *log)\n{\n\tstruct ppl_io_unit *io;\n\n\tio = list_first_entry_or_null(&log->io_list, struct ppl_io_unit,\n\t\t\t\t      log_sibling);\n\n\treturn !io || !io->submitted;\n}\n\nvoid ppl_quiesce(struct r5conf *conf, int quiesce)\n{\n\tstruct ppl_conf *ppl_conf = conf->log_private;\n\tint i;\n\n\tif (quiesce) {\n\t\tfor (i = 0; i < ppl_conf->count; i++) {\n\t\t\tstruct ppl_log *log = &ppl_conf->child_logs[i];\n\n\t\t\tspin_lock_irq(&log->io_list_lock);\n\t\t\twait_event_lock_irq(conf->wait_for_quiescent,\n\t\t\t\t\t    ppl_no_io_unit_submitted(conf, log),\n\t\t\t\t\t    log->io_list_lock);\n\t\t\tspin_unlock_irq(&log->io_list_lock);\n\t\t}\n\t}\n}\n\nint ppl_handle_flush_request(struct bio *bio)\n{\n\tif (bio->bi_iter.bi_size == 0) {\n\t\tbio_endio(bio);\n\t\treturn 0;\n\t}\n\tbio->bi_opf &= ~REQ_PREFLUSH;\n\treturn -EAGAIN;\n}\n\nvoid ppl_stripe_write_finished(struct stripe_head *sh)\n{\n\tstruct ppl_io_unit *io;\n\n\tio = sh->ppl_io;\n\tsh->ppl_io = NULL;\n\n\tif (io && atomic_dec_and_test(&io->pending_stripes)) {\n\t\tif (io->log->disk_flush_bitmap)\n\t\t\tppl_do_flush(io);\n\t\telse\n\t\t\tppl_io_unit_finished(io);\n\t}\n}\n\nstatic void ppl_xor(int size, struct page *page1, struct page *page2)\n{\n\tstruct async_submit_ctl submit;\n\tstruct dma_async_tx_descriptor *tx;\n\tstruct page *xor_srcs[] = { page1, page2 };\n\n\tinit_async_submit(&submit, ASYNC_TX_ACK|ASYNC_TX_XOR_DROP_DST,\n\t\t\t  NULL, NULL, NULL, NULL);\n\ttx = async_xor(page1, xor_srcs, 0, 2, size, &submit);\n\n\tasync_tx_quiesce(&tx);\n}\n\n \nstatic int ppl_recover_entry(struct ppl_log *log, struct ppl_header_entry *e,\n\t\t\t     sector_t ppl_sector)\n{\n\tstruct ppl_conf *ppl_conf = log->ppl_conf;\n\tstruct mddev *mddev = ppl_conf->mddev;\n\tstruct r5conf *conf = mddev->private;\n\tint block_size = ppl_conf->block_size;\n\tstruct page *page1;\n\tstruct page *page2;\n\tsector_t r_sector_first;\n\tsector_t r_sector_last;\n\tint strip_sectors;\n\tint data_disks;\n\tint i;\n\tint ret = 0;\n\tunsigned int pp_size = le32_to_cpu(e->pp_size);\n\tunsigned int data_size = le32_to_cpu(e->data_size);\n\n\tpage1 = alloc_page(GFP_KERNEL);\n\tpage2 = alloc_page(GFP_KERNEL);\n\n\tif (!page1 || !page2) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tr_sector_first = le64_to_cpu(e->data_sector) * (block_size >> 9);\n\n\tif ((pp_size >> 9) < conf->chunk_sectors) {\n\t\tif (pp_size > 0) {\n\t\t\tdata_disks = data_size / pp_size;\n\t\t\tstrip_sectors = pp_size >> 9;\n\t\t} else {\n\t\t\tdata_disks = conf->raid_disks - conf->max_degraded;\n\t\t\tstrip_sectors = (data_size >> 9) / data_disks;\n\t\t}\n\t\tr_sector_last = r_sector_first +\n\t\t\t\t(data_disks - 1) * conf->chunk_sectors +\n\t\t\t\tstrip_sectors;\n\t} else {\n\t\tdata_disks = conf->raid_disks - conf->max_degraded;\n\t\tstrip_sectors = conf->chunk_sectors;\n\t\tr_sector_last = r_sector_first + (data_size >> 9);\n\t}\n\n\tpr_debug(\"%s: array sector first: %llu last: %llu\\n\", __func__,\n\t\t (unsigned long long)r_sector_first,\n\t\t (unsigned long long)r_sector_last);\n\n\t \n\tif (block_size == 512 &&\n\t    (r_sector_first & (RAID5_STRIPE_SECTORS(conf) - 1)) == 0 &&\n\t    (r_sector_last & (RAID5_STRIPE_SECTORS(conf) - 1)) == 0)\n\t\tblock_size = RAID5_STRIPE_SIZE(conf);\n\n\t \n\tfor (i = 0; i < strip_sectors; i += (block_size >> 9)) {\n\t\tbool update_parity = false;\n\t\tsector_t parity_sector;\n\t\tstruct md_rdev *parity_rdev;\n\t\tstruct stripe_head sh;\n\t\tint disk;\n\t\tint indent = 0;\n\n\t\tpr_debug(\"%s:%*s iter %d start\\n\", __func__, indent, \"\", i);\n\t\tindent += 2;\n\n\t\tmemset(page_address(page1), 0, PAGE_SIZE);\n\n\t\t \n\t\tfor (disk = 0; disk < data_disks; disk++) {\n\t\t\tint dd_idx;\n\t\t\tstruct md_rdev *rdev;\n\t\t\tsector_t sector;\n\t\t\tsector_t r_sector = r_sector_first + i +\n\t\t\t\t\t    (disk * conf->chunk_sectors);\n\n\t\t\tpr_debug(\"%s:%*s data member disk %d start\\n\",\n\t\t\t\t __func__, indent, \"\", disk);\n\t\t\tindent += 2;\n\n\t\t\tif (r_sector >= r_sector_last) {\n\t\t\t\tpr_debug(\"%s:%*s array sector %llu doesn't need parity update\\n\",\n\t\t\t\t\t __func__, indent, \"\",\n\t\t\t\t\t (unsigned long long)r_sector);\n\t\t\t\tindent -= 2;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tupdate_parity = true;\n\n\t\t\t \n\t\t\tsector = raid5_compute_sector(conf, r_sector, 0,\n\t\t\t\t\t\t      &dd_idx, NULL);\n\t\t\tpr_debug(\"%s:%*s processing array sector %llu => data member disk %d, sector %llu\\n\",\n\t\t\t\t __func__, indent, \"\",\n\t\t\t\t (unsigned long long)r_sector, dd_idx,\n\t\t\t\t (unsigned long long)sector);\n\n\t\t\t \n\t\t\trdev = rcu_dereference_protected(\n\t\t\t\t\tconf->disks[dd_idx].rdev, 1);\n\t\t\tif (!rdev || (!test_bit(In_sync, &rdev->flags) &&\n\t\t\t\t      sector >= rdev->recovery_offset)) {\n\t\t\t\tpr_debug(\"%s:%*s data member disk %d missing\\n\",\n\t\t\t\t\t __func__, indent, \"\", dd_idx);\n\t\t\t\tupdate_parity = false;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tpr_debug(\"%s:%*s reading data member disk %pg sector %llu\\n\",\n\t\t\t\t __func__, indent, \"\", rdev->bdev,\n\t\t\t\t (unsigned long long)sector);\n\t\t\tif (!sync_page_io(rdev, sector, block_size, page2,\n\t\t\t\t\tREQ_OP_READ, false)) {\n\t\t\t\tmd_error(mddev, rdev);\n\t\t\t\tpr_debug(\"%s:%*s read failed!\\n\", __func__,\n\t\t\t\t\t indent, \"\");\n\t\t\t\tret = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tppl_xor(block_size, page1, page2);\n\n\t\t\tindent -= 2;\n\t\t}\n\n\t\tif (!update_parity)\n\t\t\tcontinue;\n\n\t\tif (pp_size > 0) {\n\t\t\tpr_debug(\"%s:%*s reading pp disk sector %llu\\n\",\n\t\t\t\t __func__, indent, \"\",\n\t\t\t\t (unsigned long long)(ppl_sector + i));\n\t\t\tif (!sync_page_io(log->rdev,\n\t\t\t\t\tppl_sector - log->rdev->data_offset + i,\n\t\t\t\t\tblock_size, page2, REQ_OP_READ,\n\t\t\t\t\tfalse)) {\n\t\t\t\tpr_debug(\"%s:%*s read failed!\\n\", __func__,\n\t\t\t\t\t indent, \"\");\n\t\t\t\tmd_error(mddev, log->rdev);\n\t\t\t\tret = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tppl_xor(block_size, page1, page2);\n\t\t}\n\n\t\t \n\t\tparity_sector = raid5_compute_sector(conf, r_sector_first + i,\n\t\t\t\t0, &disk, &sh);\n\t\tBUG_ON(sh.pd_idx != le32_to_cpu(e->parity_disk));\n\n\t\t \n\t\tparity_rdev = rcu_dereference_protected(\n\t\t\t\t\tconf->disks[sh.pd_idx].rdev, 1);\n\n\t\tBUG_ON(parity_rdev->bdev->bd_dev != log->rdev->bdev->bd_dev);\n\t\tpr_debug(\"%s:%*s write parity at sector %llu, disk %pg\\n\",\n\t\t\t __func__, indent, \"\",\n\t\t\t (unsigned long long)parity_sector,\n\t\t\t parity_rdev->bdev);\n\t\tif (!sync_page_io(parity_rdev, parity_sector, block_size,\n\t\t\t\t  page1, REQ_OP_WRITE, false)) {\n\t\t\tpr_debug(\"%s:%*s parity write error!\\n\", __func__,\n\t\t\t\t indent, \"\");\n\t\t\tmd_error(mddev, parity_rdev);\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\tif (page1)\n\t\t__free_page(page1);\n\tif (page2)\n\t\t__free_page(page2);\n\treturn ret;\n}\n\nstatic int ppl_recover(struct ppl_log *log, struct ppl_header *pplhdr,\n\t\t       sector_t offset)\n{\n\tstruct ppl_conf *ppl_conf = log->ppl_conf;\n\tstruct md_rdev *rdev = log->rdev;\n\tstruct mddev *mddev = rdev->mddev;\n\tsector_t ppl_sector = rdev->ppl.sector + offset +\n\t\t\t      (PPL_HEADER_SIZE >> 9);\n\tstruct page *page;\n\tint i;\n\tint ret = 0;\n\n\tpage = alloc_page(GFP_KERNEL);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (i = 0; i < le32_to_cpu(pplhdr->entries_count); i++) {\n\t\tstruct ppl_header_entry *e = &pplhdr->entries[i];\n\t\tu32 pp_size = le32_to_cpu(e->pp_size);\n\t\tsector_t sector = ppl_sector;\n\t\tint ppl_entry_sectors = pp_size >> 9;\n\t\tu32 crc, crc_stored;\n\n\t\tpr_debug(\"%s: disk: %d entry: %d ppl_sector: %llu pp_size: %u\\n\",\n\t\t\t __func__, rdev->raid_disk, i,\n\t\t\t (unsigned long long)ppl_sector, pp_size);\n\n\t\tcrc = ~0;\n\t\tcrc_stored = le32_to_cpu(e->checksum);\n\n\t\t \n\t\twhile (pp_size) {\n\t\t\tint s = pp_size > PAGE_SIZE ? PAGE_SIZE : pp_size;\n\n\t\t\tif (!sync_page_io(rdev, sector - rdev->data_offset,\n\t\t\t\t\ts, page, REQ_OP_READ, false)) {\n\t\t\t\tmd_error(mddev, rdev);\n\t\t\t\tret = -EIO;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tcrc = crc32c_le(crc, page_address(page), s);\n\n\t\t\tpp_size -= s;\n\t\t\tsector += s >> 9;\n\t\t}\n\n\t\tcrc = ~crc;\n\n\t\tif (crc != crc_stored) {\n\t\t\t \n\t\t\tpr_debug(\"%s: ppl entry crc does not match: stored: 0x%x calculated: 0x%x\\n\",\n\t\t\t\t __func__, crc_stored, crc);\n\t\t\tppl_conf->mismatch_count++;\n\t\t} else {\n\t\t\tret = ppl_recover_entry(log, e, ppl_sector);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tppl_conf->recovered_entries++;\n\t\t}\n\n\t\tppl_sector += ppl_entry_sectors;\n\t}\n\n\t \n\tret = blkdev_issue_flush(rdev->bdev);\nout:\n\t__free_page(page);\n\treturn ret;\n}\n\nstatic int ppl_write_empty_header(struct ppl_log *log)\n{\n\tstruct page *page;\n\tstruct ppl_header *pplhdr;\n\tstruct md_rdev *rdev = log->rdev;\n\tint ret = 0;\n\n\tpr_debug(\"%s: disk: %d ppl_sector: %llu\\n\", __func__,\n\t\t rdev->raid_disk, (unsigned long long)rdev->ppl.sector);\n\n\tpage = alloc_page(GFP_NOIO | __GFP_ZERO);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tpplhdr = page_address(page);\n\t \n\tblkdev_issue_zeroout(rdev->bdev, rdev->ppl.sector,\n\t\t\t    log->rdev->ppl.size, GFP_NOIO, 0);\n\tmemset(pplhdr->reserved, 0xff, PPL_HDR_RESERVED);\n\tpplhdr->signature = cpu_to_le32(log->ppl_conf->signature);\n\tpplhdr->checksum = cpu_to_le32(~crc32c_le(~0, pplhdr, PAGE_SIZE));\n\n\tif (!sync_page_io(rdev, rdev->ppl.sector - rdev->data_offset,\n\t\t\t  PPL_HEADER_SIZE, page, REQ_OP_WRITE | REQ_SYNC |\n\t\t\t  REQ_FUA, false)) {\n\t\tmd_error(rdev->mddev, rdev);\n\t\tret = -EIO;\n\t}\n\n\t__free_page(page);\n\treturn ret;\n}\n\nstatic int ppl_load_distributed(struct ppl_log *log)\n{\n\tstruct ppl_conf *ppl_conf = log->ppl_conf;\n\tstruct md_rdev *rdev = log->rdev;\n\tstruct mddev *mddev = rdev->mddev;\n\tstruct page *page, *page2;\n\tstruct ppl_header *pplhdr = NULL, *prev_pplhdr = NULL;\n\tu32 crc, crc_stored;\n\tu32 signature;\n\tint ret = 0, i;\n\tsector_t pplhdr_offset = 0, prev_pplhdr_offset = 0;\n\n\tpr_debug(\"%s: disk: %d\\n\", __func__, rdev->raid_disk);\n\t \n\tpage = alloc_page(GFP_KERNEL);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tpage2 = alloc_page(GFP_KERNEL);\n\tif (!page2) {\n\t\t__free_page(page);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\twhile (pplhdr_offset < rdev->ppl.size - (PPL_HEADER_SIZE >> 9)) {\n\t\tif (!sync_page_io(rdev,\n\t\t\t\t  rdev->ppl.sector - rdev->data_offset +\n\t\t\t\t  pplhdr_offset, PAGE_SIZE, page, REQ_OP_READ,\n\t\t\t\t  false)) {\n\t\t\tmd_error(mddev, rdev);\n\t\t\tret = -EIO;\n\t\t\t \n\t\t\tpplhdr = NULL;\n\t\t\tbreak;\n\t\t}\n\t\tpplhdr = page_address(page);\n\n\t\t \n\t\tcrc_stored = le32_to_cpu(pplhdr->checksum);\n\t\tpplhdr->checksum = 0;\n\t\tcrc = ~crc32c_le(~0, pplhdr, PAGE_SIZE);\n\n\t\tif (crc_stored != crc) {\n\t\t\tpr_debug(\"%s: ppl header crc does not match: stored: 0x%x calculated: 0x%x (offset: %llu)\\n\",\n\t\t\t\t __func__, crc_stored, crc,\n\t\t\t\t (unsigned long long)pplhdr_offset);\n\t\t\tpplhdr = prev_pplhdr;\n\t\t\tpplhdr_offset = prev_pplhdr_offset;\n\t\t\tbreak;\n\t\t}\n\n\t\tsignature = le32_to_cpu(pplhdr->signature);\n\n\t\tif (mddev->external) {\n\t\t\t \n\t\t\tppl_conf->signature = signature;\n\t\t} else if (ppl_conf->signature != signature) {\n\t\t\tpr_debug(\"%s: ppl header signature does not match: stored: 0x%x configured: 0x%x (offset: %llu)\\n\",\n\t\t\t\t __func__, signature, ppl_conf->signature,\n\t\t\t\t (unsigned long long)pplhdr_offset);\n\t\t\tpplhdr = prev_pplhdr;\n\t\t\tpplhdr_offset = prev_pplhdr_offset;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (prev_pplhdr && le64_to_cpu(prev_pplhdr->generation) >\n\t\t    le64_to_cpu(pplhdr->generation)) {\n\t\t\t \n\t\t\tpplhdr = prev_pplhdr;\n\t\t\tpplhdr_offset = prev_pplhdr_offset;\n\t\t\tbreak;\n\t\t}\n\n\t\tprev_pplhdr_offset = pplhdr_offset;\n\t\tprev_pplhdr = pplhdr;\n\n\t\tswap(page, page2);\n\n\t\t \n\t\tfor (i = 0; i < le32_to_cpu(pplhdr->entries_count); i++)\n\t\t\tpplhdr_offset +=\n\t\t\t    le32_to_cpu(pplhdr->entries[i].pp_size) >> 9;\n\t\tpplhdr_offset += PPL_HEADER_SIZE >> 9;\n\t}\n\n\t \n\tif (!pplhdr)\n\t\tppl_conf->mismatch_count++;\n\telse\n\t\tpr_debug(\"%s: latest PPL found at offset: %llu, with generation: %llu\\n\",\n\t\t    __func__, (unsigned long long)pplhdr_offset,\n\t\t    le64_to_cpu(pplhdr->generation));\n\n\t \n\tif (pplhdr && !mddev->pers && mddev->recovery_cp != MaxSector)\n\t\tret = ppl_recover(log, pplhdr, pplhdr_offset);\n\n\t \n\tif (!ret && !mddev->pers)\n\t\tret = ppl_write_empty_header(log);\n\n\t__free_page(page);\n\t__free_page(page2);\n\n\tpr_debug(\"%s: return: %d mismatch_count: %d recovered_entries: %d\\n\",\n\t\t __func__, ret, ppl_conf->mismatch_count,\n\t\t ppl_conf->recovered_entries);\n\treturn ret;\n}\n\nstatic int ppl_load(struct ppl_conf *ppl_conf)\n{\n\tint ret = 0;\n\tu32 signature = 0;\n\tbool signature_set = false;\n\tint i;\n\n\tfor (i = 0; i < ppl_conf->count; i++) {\n\t\tstruct ppl_log *log = &ppl_conf->child_logs[i];\n\n\t\t \n\t\tif (!log->rdev)\n\t\t\tcontinue;\n\n\t\tret = ppl_load_distributed(log);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\t \n\t\tif (ppl_conf->mddev->external) {\n\t\t\tif (!signature_set) {\n\t\t\t\tsignature = ppl_conf->signature;\n\t\t\t\tsignature_set = true;\n\t\t\t} else if (signature != ppl_conf->signature) {\n\t\t\t\tpr_warn(\"md/raid:%s: PPL header signature does not match on all member drives\\n\",\n\t\t\t\t\tmdname(ppl_conf->mddev));\n\t\t\t\tret = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tpr_debug(\"%s: return: %d mismatch_count: %d recovered_entries: %d\\n\",\n\t\t __func__, ret, ppl_conf->mismatch_count,\n\t\t ppl_conf->recovered_entries);\n\treturn ret;\n}\n\nstatic void __ppl_exit_log(struct ppl_conf *ppl_conf)\n{\n\tclear_bit(MD_HAS_PPL, &ppl_conf->mddev->flags);\n\tclear_bit(MD_HAS_MULTIPLE_PPLS, &ppl_conf->mddev->flags);\n\n\tkfree(ppl_conf->child_logs);\n\n\tbioset_exit(&ppl_conf->bs);\n\tbioset_exit(&ppl_conf->flush_bs);\n\tmempool_exit(&ppl_conf->io_pool);\n\tkmem_cache_destroy(ppl_conf->io_kc);\n\n\tkfree(ppl_conf);\n}\n\nvoid ppl_exit_log(struct r5conf *conf)\n{\n\tstruct ppl_conf *ppl_conf = conf->log_private;\n\n\tif (ppl_conf) {\n\t\t__ppl_exit_log(ppl_conf);\n\t\tconf->log_private = NULL;\n\t}\n}\n\nstatic int ppl_validate_rdev(struct md_rdev *rdev)\n{\n\tint ppl_data_sectors;\n\tint ppl_size_new;\n\n\t \n\tppl_data_sectors = rdev->ppl.size - (PPL_HEADER_SIZE >> 9);\n\n\tif (ppl_data_sectors > 0)\n\t\tppl_data_sectors = rounddown(ppl_data_sectors,\n\t\t\t\tRAID5_STRIPE_SECTORS((struct r5conf *)rdev->mddev->private));\n\n\tif (ppl_data_sectors <= 0) {\n\t\tpr_warn(\"md/raid:%s: PPL space too small on %pg\\n\",\n\t\t\tmdname(rdev->mddev), rdev->bdev);\n\t\treturn -ENOSPC;\n\t}\n\n\tppl_size_new = ppl_data_sectors + (PPL_HEADER_SIZE >> 9);\n\n\tif ((rdev->ppl.sector < rdev->data_offset &&\n\t     rdev->ppl.sector + ppl_size_new > rdev->data_offset) ||\n\t    (rdev->ppl.sector >= rdev->data_offset &&\n\t     rdev->data_offset + rdev->sectors > rdev->ppl.sector)) {\n\t\tpr_warn(\"md/raid:%s: PPL space overlaps with data on %pg\\n\",\n\t\t\tmdname(rdev->mddev), rdev->bdev);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!rdev->mddev->external &&\n\t    ((rdev->ppl.offset > 0 && rdev->ppl.offset < (rdev->sb_size >> 9)) ||\n\t     (rdev->ppl.offset <= 0 && rdev->ppl.offset + ppl_size_new > 0))) {\n\t\tpr_warn(\"md/raid:%s: PPL space overlaps with superblock on %pg\\n\",\n\t\t\tmdname(rdev->mddev), rdev->bdev);\n\t\treturn -EINVAL;\n\t}\n\n\trdev->ppl.size = ppl_size_new;\n\n\treturn 0;\n}\n\nstatic void ppl_init_child_log(struct ppl_log *log, struct md_rdev *rdev)\n{\n\tif ((rdev->ppl.size << 9) >= (PPL_SPACE_SIZE +\n\t\t\t\t      PPL_HEADER_SIZE) * 2) {\n\t\tlog->use_multippl = true;\n\t\tset_bit(MD_HAS_MULTIPLE_PPLS,\n\t\t\t&log->ppl_conf->mddev->flags);\n\t\tlog->entry_space = PPL_SPACE_SIZE;\n\t} else {\n\t\tlog->use_multippl = false;\n\t\tlog->entry_space = (log->rdev->ppl.size << 9) -\n\t\t\t\t   PPL_HEADER_SIZE;\n\t}\n\tlog->next_io_sector = rdev->ppl.sector;\n\n\tif (bdev_write_cache(rdev->bdev))\n\t\tlog->wb_cache_on = true;\n}\n\nint ppl_init_log(struct r5conf *conf)\n{\n\tstruct ppl_conf *ppl_conf;\n\tstruct mddev *mddev = conf->mddev;\n\tint ret = 0;\n\tint max_disks;\n\tint i;\n\n\tpr_debug(\"md/raid:%s: enabling distributed Partial Parity Log\\n\",\n\t\t mdname(conf->mddev));\n\n\tif (PAGE_SIZE != 4096)\n\t\treturn -EINVAL;\n\n\tif (mddev->level != 5) {\n\t\tpr_warn(\"md/raid:%s PPL is not compatible with raid level %d\\n\",\n\t\t\tmdname(mddev), mddev->level);\n\t\treturn -EINVAL;\n\t}\n\n\tif (mddev->bitmap_info.file || mddev->bitmap_info.offset) {\n\t\tpr_warn(\"md/raid:%s PPL is not compatible with bitmap\\n\",\n\t\t\tmdname(mddev));\n\t\treturn -EINVAL;\n\t}\n\n\tif (test_bit(MD_HAS_JOURNAL, &mddev->flags)) {\n\t\tpr_warn(\"md/raid:%s PPL is not compatible with journal\\n\",\n\t\t\tmdname(mddev));\n\t\treturn -EINVAL;\n\t}\n\n\tmax_disks = sizeof_field(struct ppl_log, disk_flush_bitmap) *\n\t\tBITS_PER_BYTE;\n\tif (conf->raid_disks > max_disks) {\n\t\tpr_warn(\"md/raid:%s PPL doesn't support over %d disks in the array\\n\",\n\t\t\tmdname(mddev), max_disks);\n\t\treturn -EINVAL;\n\t}\n\n\tppl_conf = kzalloc(sizeof(struct ppl_conf), GFP_KERNEL);\n\tif (!ppl_conf)\n\t\treturn -ENOMEM;\n\n\tppl_conf->mddev = mddev;\n\n\tppl_conf->io_kc = KMEM_CACHE(ppl_io_unit, 0);\n\tif (!ppl_conf->io_kc) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tret = mempool_init(&ppl_conf->io_pool, conf->raid_disks, ppl_io_pool_alloc,\n\t\t\t   ppl_io_pool_free, ppl_conf->io_kc);\n\tif (ret)\n\t\tgoto err;\n\n\tret = bioset_init(&ppl_conf->bs, conf->raid_disks, 0, BIOSET_NEED_BVECS);\n\tif (ret)\n\t\tgoto err;\n\n\tret = bioset_init(&ppl_conf->flush_bs, conf->raid_disks, 0, 0);\n\tif (ret)\n\t\tgoto err;\n\n\tppl_conf->count = conf->raid_disks;\n\tppl_conf->child_logs = kcalloc(ppl_conf->count, sizeof(struct ppl_log),\n\t\t\t\t       GFP_KERNEL);\n\tif (!ppl_conf->child_logs) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tatomic64_set(&ppl_conf->seq, 0);\n\tINIT_LIST_HEAD(&ppl_conf->no_mem_stripes);\n\tspin_lock_init(&ppl_conf->no_mem_stripes_lock);\n\n\tif (!mddev->external) {\n\t\tppl_conf->signature = ~crc32c_le(~0, mddev->uuid, sizeof(mddev->uuid));\n\t\tppl_conf->block_size = 512;\n\t} else {\n\t\tppl_conf->block_size = queue_logical_block_size(mddev->queue);\n\t}\n\n\tfor (i = 0; i < ppl_conf->count; i++) {\n\t\tstruct ppl_log *log = &ppl_conf->child_logs[i];\n\t\t \n\t\tstruct md_rdev *rdev =\n\t\t\trcu_dereference_protected(conf->disks[i].rdev, 1);\n\n\t\tmutex_init(&log->io_mutex);\n\t\tspin_lock_init(&log->io_list_lock);\n\t\tINIT_LIST_HEAD(&log->io_list);\n\n\t\tlog->ppl_conf = ppl_conf;\n\t\tlog->rdev = rdev;\n\n\t\tif (rdev) {\n\t\t\tret = ppl_validate_rdev(rdev);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\t\tppl_init_child_log(log, rdev);\n\t\t}\n\t}\n\n\t \n\tret = ppl_load(ppl_conf);\n\n\tif (ret) {\n\t\tgoto err;\n\t} else if (!mddev->pers && mddev->recovery_cp == 0 &&\n\t\t   ppl_conf->recovered_entries > 0 &&\n\t\t   ppl_conf->mismatch_count == 0) {\n\t\t \n\t\tmddev->recovery_cp = MaxSector;\n\t\tset_bit(MD_SB_CHANGE_CLEAN, &mddev->sb_flags);\n\t} else if (mddev->pers && ppl_conf->mismatch_count > 0) {\n\t\t \n\t\tret = -EINVAL;\n\t\tgoto err;\n\t}\n\n\tconf->log_private = ppl_conf;\n\tset_bit(MD_HAS_PPL, &ppl_conf->mddev->flags);\n\n\treturn 0;\nerr:\n\t__ppl_exit_log(ppl_conf);\n\treturn ret;\n}\n\nint ppl_modify_log(struct r5conf *conf, struct md_rdev *rdev, bool add)\n{\n\tstruct ppl_conf *ppl_conf = conf->log_private;\n\tstruct ppl_log *log;\n\tint ret = 0;\n\n\tif (!rdev)\n\t\treturn -EINVAL;\n\n\tpr_debug(\"%s: disk: %d operation: %s dev: %pg\\n\",\n\t\t __func__, rdev->raid_disk, add ? \"add\" : \"remove\",\n\t\t rdev->bdev);\n\n\tif (rdev->raid_disk < 0)\n\t\treturn 0;\n\n\tif (rdev->raid_disk >= ppl_conf->count)\n\t\treturn -ENODEV;\n\n\tlog = &ppl_conf->child_logs[rdev->raid_disk];\n\n\tmutex_lock(&log->io_mutex);\n\tif (add) {\n\t\tret = ppl_validate_rdev(rdev);\n\t\tif (!ret) {\n\t\t\tlog->rdev = rdev;\n\t\t\tret = ppl_write_empty_header(log);\n\t\t\tppl_init_child_log(log, rdev);\n\t\t}\n\t} else {\n\t\tlog->rdev = NULL;\n\t}\n\tmutex_unlock(&log->io_mutex);\n\n\treturn ret;\n}\n\nstatic ssize_t\nppl_write_hint_show(struct mddev *mddev, char *buf)\n{\n\treturn sprintf(buf, \"%d\\n\", 0);\n}\n\nstatic ssize_t\nppl_write_hint_store(struct mddev *mddev, const char *page, size_t len)\n{\n\tstruct r5conf *conf;\n\tint err = 0;\n\tunsigned short new;\n\n\tif (len >= PAGE_SIZE)\n\t\treturn -EINVAL;\n\tif (kstrtou16(page, 10, &new))\n\t\treturn -EINVAL;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\n\tconf = mddev->private;\n\tif (!conf)\n\t\terr = -ENODEV;\n\telse if (!raid5_has_ppl(conf) || !conf->log_private)\n\t\terr = -EINVAL;\n\n\tmddev_unlock(mddev);\n\n\treturn err ?: len;\n}\n\nstruct md_sysfs_entry\nppl_write_hint = __ATTR(ppl_write_hint, S_IRUGO | S_IWUSR,\n\t\t\tppl_write_hint_show,\n\t\t\tppl_write_hint_store);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}