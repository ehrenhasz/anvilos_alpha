{
  "module_name": "dm-snap.c",
  "hash_id": "493e3b3f4c1cf0833b08ad38337c1467f9f37c0de27425b0ec1f67a7970fd48d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-snap.c",
  "human_readable_source": "\n \n\n#include <linux/blkdev.h>\n#include <linux/device-mapper.h>\n#include <linux/delay.h>\n#include <linux/fs.h>\n#include <linux/init.h>\n#include <linux/kdev_t.h>\n#include <linux/list.h>\n#include <linux/list_bl.h>\n#include <linux/mempool.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/log2.h>\n#include <linux/dm-kcopyd.h>\n\n#include \"dm.h\"\n\n#include \"dm-exception-store.h\"\n\n#define DM_MSG_PREFIX \"snapshots\"\n\nstatic const char dm_snapshot_merge_target_name[] = \"snapshot-merge\";\n\n#define dm_target_is_snapshot_merge(ti) \\\n\t((ti)->type->name == dm_snapshot_merge_target_name)\n\n \n#define MIN_IOS 256\n\n#define DM_TRACKED_CHUNK_HASH_SIZE\t16\n#define DM_TRACKED_CHUNK_HASH(x)\t((unsigned long)(x) & \\\n\t\t\t\t\t (DM_TRACKED_CHUNK_HASH_SIZE - 1))\n\nstruct dm_exception_table {\n\tuint32_t hash_mask;\n\tunsigned int hash_shift;\n\tstruct hlist_bl_head *table;\n};\n\nstruct dm_snapshot {\n\tstruct rw_semaphore lock;\n\n\tstruct dm_dev *origin;\n\tstruct dm_dev *cow;\n\n\tstruct dm_target *ti;\n\n\t \n\tstruct list_head list;\n\n\t \n\tint valid;\n\n\t \n\tint snapshot_overflowed;\n\n\t \n\tint active;\n\n\tatomic_t pending_exceptions_count;\n\n\tspinlock_t pe_allocation_lock;\n\n\t \n\tsector_t exception_start_sequence;\n\n\t \n\tsector_t exception_complete_sequence;\n\n\t \n\tstruct rb_root out_of_order_tree;\n\n\tmempool_t pending_pool;\n\n\tstruct dm_exception_table pending;\n\tstruct dm_exception_table complete;\n\n\t \n\tspinlock_t pe_lock;\n\n\t \n\tspinlock_t tracked_chunk_lock;\n\tstruct hlist_head tracked_chunk_hash[DM_TRACKED_CHUNK_HASH_SIZE];\n\n\t \n\tstruct dm_exception_store *store;\n\n\tunsigned int in_progress;\n\tstruct wait_queue_head in_progress_wait;\n\n\tstruct dm_kcopyd_client *kcopyd_client;\n\n\t \n\tunsigned long state_bits;\n\n\t \n\tchunk_t first_merging_chunk;\n\tint num_merging_chunks;\n\n\t \n\tbool merge_failed:1;\n\n\tbool discard_zeroes_cow:1;\n\tbool discard_passdown_origin:1;\n\n\t \n\tstruct bio_list bios_queued_during_merge;\n};\n\n \n#define RUNNING_MERGE          0\n#define SHUTDOWN_MERGE         1\n\n \n#define DEFAULT_COW_THRESHOLD 2048\n\nstatic unsigned int cow_threshold = DEFAULT_COW_THRESHOLD;\nmodule_param_named(snapshot_cow_threshold, cow_threshold, uint, 0644);\nMODULE_PARM_DESC(snapshot_cow_threshold, \"Maximum number of chunks being copied on write\");\n\nDECLARE_DM_KCOPYD_THROTTLE_WITH_MODULE_PARM(snapshot_copy_throttle,\n\t\t\"A percentage of time allocated for copy on write\");\n\nstruct dm_dev *dm_snap_origin(struct dm_snapshot *s)\n{\n\treturn s->origin;\n}\nEXPORT_SYMBOL(dm_snap_origin);\n\nstruct dm_dev *dm_snap_cow(struct dm_snapshot *s)\n{\n\treturn s->cow;\n}\nEXPORT_SYMBOL(dm_snap_cow);\n\nstatic sector_t chunk_to_sector(struct dm_exception_store *store,\n\t\t\t\tchunk_t chunk)\n{\n\treturn chunk << store->chunk_shift;\n}\n\nstatic int bdev_equal(struct block_device *lhs, struct block_device *rhs)\n{\n\t \n\treturn lhs == rhs;\n}\n\nstruct dm_snap_pending_exception {\n\tstruct dm_exception e;\n\n\t \n\tstruct bio_list origin_bios;\n\tstruct bio_list snapshot_bios;\n\n\t \n\tstruct dm_snapshot *snap;\n\n\t \n\tint started;\n\n\t \n\tint copy_error;\n\n\t \n\tsector_t exception_sequence;\n\n\tstruct rb_node out_of_order_node;\n\n\t \n\tstruct bio *full_bio;\n\tbio_end_io_t *full_bio_end_io;\n};\n\n \nstatic struct kmem_cache *exception_cache;\nstatic struct kmem_cache *pending_cache;\n\nstruct dm_snap_tracked_chunk {\n\tstruct hlist_node node;\n\tchunk_t chunk;\n};\n\nstatic void init_tracked_chunk(struct bio *bio)\n{\n\tstruct dm_snap_tracked_chunk *c = dm_per_bio_data(bio, sizeof(struct dm_snap_tracked_chunk));\n\n\tINIT_HLIST_NODE(&c->node);\n}\n\nstatic bool is_bio_tracked(struct bio *bio)\n{\n\tstruct dm_snap_tracked_chunk *c = dm_per_bio_data(bio, sizeof(struct dm_snap_tracked_chunk));\n\n\treturn !hlist_unhashed(&c->node);\n}\n\nstatic void track_chunk(struct dm_snapshot *s, struct bio *bio, chunk_t chunk)\n{\n\tstruct dm_snap_tracked_chunk *c = dm_per_bio_data(bio, sizeof(struct dm_snap_tracked_chunk));\n\n\tc->chunk = chunk;\n\n\tspin_lock_irq(&s->tracked_chunk_lock);\n\thlist_add_head(&c->node,\n\t\t       &s->tracked_chunk_hash[DM_TRACKED_CHUNK_HASH(chunk)]);\n\tspin_unlock_irq(&s->tracked_chunk_lock);\n}\n\nstatic void stop_tracking_chunk(struct dm_snapshot *s, struct bio *bio)\n{\n\tstruct dm_snap_tracked_chunk *c = dm_per_bio_data(bio, sizeof(struct dm_snap_tracked_chunk));\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&s->tracked_chunk_lock, flags);\n\thlist_del(&c->node);\n\tspin_unlock_irqrestore(&s->tracked_chunk_lock, flags);\n}\n\nstatic int __chunk_is_tracked(struct dm_snapshot *s, chunk_t chunk)\n{\n\tstruct dm_snap_tracked_chunk *c;\n\tint found = 0;\n\n\tspin_lock_irq(&s->tracked_chunk_lock);\n\n\thlist_for_each_entry(c,\n\t    &s->tracked_chunk_hash[DM_TRACKED_CHUNK_HASH(chunk)], node) {\n\t\tif (c->chunk == chunk) {\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tspin_unlock_irq(&s->tracked_chunk_lock);\n\n\treturn found;\n}\n\n \nstatic void __check_for_conflicting_io(struct dm_snapshot *s, chunk_t chunk)\n{\n\twhile (__chunk_is_tracked(s, chunk))\n\t\tfsleep(1000);\n}\n\n \nstruct origin {\n\t \n\tstruct block_device *bdev;\n\n\tstruct list_head hash_list;\n\n\t \n\tstruct list_head snapshots;\n};\n\n \nstruct dm_origin {\n\tstruct dm_dev *dev;\n\tstruct dm_target *ti;\n\tunsigned int split_boundary;\n\tstruct list_head hash_list;\n};\n\n \n#define ORIGIN_HASH_SIZE 256\n#define ORIGIN_MASK      0xFF\nstatic struct list_head *_origins;\nstatic struct list_head *_dm_origins;\nstatic struct rw_semaphore _origins_lock;\n\nstatic DECLARE_WAIT_QUEUE_HEAD(_pending_exceptions_done);\nstatic DEFINE_SPINLOCK(_pending_exceptions_done_spinlock);\nstatic uint64_t _pending_exceptions_done_count;\n\nstatic int init_origin_hash(void)\n{\n\tint i;\n\n\t_origins = kmalloc_array(ORIGIN_HASH_SIZE, sizeof(struct list_head),\n\t\t\t\t GFP_KERNEL);\n\tif (!_origins) {\n\t\tDMERR(\"unable to allocate memory for _origins\");\n\t\treturn -ENOMEM;\n\t}\n\tfor (i = 0; i < ORIGIN_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(_origins + i);\n\n\t_dm_origins = kmalloc_array(ORIGIN_HASH_SIZE,\n\t\t\t\t    sizeof(struct list_head),\n\t\t\t\t    GFP_KERNEL);\n\tif (!_dm_origins) {\n\t\tDMERR(\"unable to allocate memory for _dm_origins\");\n\t\tkfree(_origins);\n\t\treturn -ENOMEM;\n\t}\n\tfor (i = 0; i < ORIGIN_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(_dm_origins + i);\n\n\tinit_rwsem(&_origins_lock);\n\n\treturn 0;\n}\n\nstatic void exit_origin_hash(void)\n{\n\tkfree(_origins);\n\tkfree(_dm_origins);\n}\n\nstatic unsigned int origin_hash(struct block_device *bdev)\n{\n\treturn bdev->bd_dev & ORIGIN_MASK;\n}\n\nstatic struct origin *__lookup_origin(struct block_device *origin)\n{\n\tstruct list_head *ol;\n\tstruct origin *o;\n\n\tol = &_origins[origin_hash(origin)];\n\tlist_for_each_entry(o, ol, hash_list)\n\t\tif (bdev_equal(o->bdev, origin))\n\t\t\treturn o;\n\n\treturn NULL;\n}\n\nstatic void __insert_origin(struct origin *o)\n{\n\tstruct list_head *sl = &_origins[origin_hash(o->bdev)];\n\n\tlist_add_tail(&o->hash_list, sl);\n}\n\nstatic struct dm_origin *__lookup_dm_origin(struct block_device *origin)\n{\n\tstruct list_head *ol;\n\tstruct dm_origin *o;\n\n\tol = &_dm_origins[origin_hash(origin)];\n\tlist_for_each_entry(o, ol, hash_list)\n\t\tif (bdev_equal(o->dev->bdev, origin))\n\t\t\treturn o;\n\n\treturn NULL;\n}\n\nstatic void __insert_dm_origin(struct dm_origin *o)\n{\n\tstruct list_head *sl = &_dm_origins[origin_hash(o->dev->bdev)];\n\n\tlist_add_tail(&o->hash_list, sl);\n}\n\nstatic void __remove_dm_origin(struct dm_origin *o)\n{\n\tlist_del(&o->hash_list);\n}\n\n \nstatic int __find_snapshots_sharing_cow(struct dm_snapshot *snap,\n\t\t\t\t\tstruct dm_snapshot **snap_src,\n\t\t\t\t\tstruct dm_snapshot **snap_dest,\n\t\t\t\t\tstruct dm_snapshot **snap_merge)\n{\n\tstruct dm_snapshot *s;\n\tstruct origin *o;\n\tint count = 0;\n\tint active;\n\n\to = __lookup_origin(snap->origin->bdev);\n\tif (!o)\n\t\tgoto out;\n\n\tlist_for_each_entry(s, &o->snapshots, list) {\n\t\tif (dm_target_is_snapshot_merge(s->ti) && snap_merge)\n\t\t\t*snap_merge = s;\n\t\tif (!bdev_equal(s->cow->bdev, snap->cow->bdev))\n\t\t\tcontinue;\n\n\t\tdown_read(&s->lock);\n\t\tactive = s->active;\n\t\tup_read(&s->lock);\n\n\t\tif (active) {\n\t\t\tif (snap_src)\n\t\t\t\t*snap_src = s;\n\t\t} else if (snap_dest)\n\t\t\t*snap_dest = s;\n\n\t\tcount++;\n\t}\n\nout:\n\treturn count;\n}\n\n \nstatic int __validate_exception_handover(struct dm_snapshot *snap)\n{\n\tstruct dm_snapshot *snap_src = NULL, *snap_dest = NULL;\n\tstruct dm_snapshot *snap_merge = NULL;\n\n\t \n\tif ((__find_snapshots_sharing_cow(snap, &snap_src, &snap_dest,\n\t\t\t\t\t  &snap_merge) == 2) ||\n\t    snap_dest) {\n\t\tsnap->ti->error = \"Snapshot cow pairing for exception table handover failed\";\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (!snap_src)\n\t\treturn 0;\n\n\t \n\tif (!dm_target_is_snapshot_merge(snap->ti))\n\t\treturn 1;\n\n\t \n\tif (snap_merge) {\n\t\tsnap->ti->error = \"A snapshot is already merging.\";\n\t\treturn -EINVAL;\n\t}\n\n\tif (!snap_src->store->type->prepare_merge ||\n\t    !snap_src->store->type->commit_merge) {\n\t\tsnap->ti->error = \"Snapshot exception store does not support snapshot-merge.\";\n\t\treturn -EINVAL;\n\t}\n\n\treturn 1;\n}\n\nstatic void __insert_snapshot(struct origin *o, struct dm_snapshot *s)\n{\n\tstruct dm_snapshot *l;\n\n\t \n\tlist_for_each_entry(l, &o->snapshots, list)\n\t\tif (l->store->chunk_size < s->store->chunk_size)\n\t\t\tbreak;\n\tlist_add_tail(&s->list, &l->list);\n}\n\n \nstatic int register_snapshot(struct dm_snapshot *snap)\n{\n\tstruct origin *o, *new_o = NULL;\n\tstruct block_device *bdev = snap->origin->bdev;\n\tint r = 0;\n\n\tnew_o = kmalloc(sizeof(*new_o), GFP_KERNEL);\n\tif (!new_o)\n\t\treturn -ENOMEM;\n\n\tdown_write(&_origins_lock);\n\n\tr = __validate_exception_handover(snap);\n\tif (r < 0) {\n\t\tkfree(new_o);\n\t\tgoto out;\n\t}\n\n\to = __lookup_origin(bdev);\n\tif (o)\n\t\tkfree(new_o);\n\telse {\n\t\t \n\t\to = new_o;\n\n\t\t \n\t\tINIT_LIST_HEAD(&o->snapshots);\n\t\to->bdev = bdev;\n\n\t\t__insert_origin(o);\n\t}\n\n\t__insert_snapshot(o, snap);\n\nout:\n\tup_write(&_origins_lock);\n\n\treturn r;\n}\n\n \nstatic void reregister_snapshot(struct dm_snapshot *s)\n{\n\tstruct block_device *bdev = s->origin->bdev;\n\n\tdown_write(&_origins_lock);\n\n\tlist_del(&s->list);\n\t__insert_snapshot(__lookup_origin(bdev), s);\n\n\tup_write(&_origins_lock);\n}\n\nstatic void unregister_snapshot(struct dm_snapshot *s)\n{\n\tstruct origin *o;\n\n\tdown_write(&_origins_lock);\n\to = __lookup_origin(s->origin->bdev);\n\n\tlist_del(&s->list);\n\tif (o && list_empty(&o->snapshots)) {\n\t\tlist_del(&o->hash_list);\n\t\tkfree(o);\n\t}\n\n\tup_write(&_origins_lock);\n}\n\n \nstatic uint32_t exception_hash(struct dm_exception_table *et, chunk_t chunk);\n\n \nstruct dm_exception_table_lock {\n\tstruct hlist_bl_head *complete_slot;\n\tstruct hlist_bl_head *pending_slot;\n};\n\nstatic void dm_exception_table_lock_init(struct dm_snapshot *s, chunk_t chunk,\n\t\t\t\t\t struct dm_exception_table_lock *lock)\n{\n\tstruct dm_exception_table *complete = &s->complete;\n\tstruct dm_exception_table *pending = &s->pending;\n\n\tlock->complete_slot = &complete->table[exception_hash(complete, chunk)];\n\tlock->pending_slot = &pending->table[exception_hash(pending, chunk)];\n}\n\nstatic void dm_exception_table_lock(struct dm_exception_table_lock *lock)\n{\n\thlist_bl_lock(lock->complete_slot);\n\thlist_bl_lock(lock->pending_slot);\n}\n\nstatic void dm_exception_table_unlock(struct dm_exception_table_lock *lock)\n{\n\thlist_bl_unlock(lock->pending_slot);\n\thlist_bl_unlock(lock->complete_slot);\n}\n\nstatic int dm_exception_table_init(struct dm_exception_table *et,\n\t\t\t\t   uint32_t size, unsigned int hash_shift)\n{\n\tunsigned int i;\n\n\tet->hash_shift = hash_shift;\n\tet->hash_mask = size - 1;\n\tet->table = kvmalloc_array(size, sizeof(struct hlist_bl_head),\n\t\t\t\t   GFP_KERNEL);\n\tif (!et->table)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < size; i++)\n\t\tINIT_HLIST_BL_HEAD(et->table + i);\n\n\treturn 0;\n}\n\nstatic void dm_exception_table_exit(struct dm_exception_table *et,\n\t\t\t\t    struct kmem_cache *mem)\n{\n\tstruct hlist_bl_head *slot;\n\tstruct dm_exception *ex;\n\tstruct hlist_bl_node *pos, *n;\n\tint i, size;\n\n\tsize = et->hash_mask + 1;\n\tfor (i = 0; i < size; i++) {\n\t\tslot = et->table + i;\n\n\t\thlist_bl_for_each_entry_safe(ex, pos, n, slot, hash_list)\n\t\t\tkmem_cache_free(mem, ex);\n\t}\n\n\tkvfree(et->table);\n}\n\nstatic uint32_t exception_hash(struct dm_exception_table *et, chunk_t chunk)\n{\n\treturn (chunk >> et->hash_shift) & et->hash_mask;\n}\n\nstatic void dm_remove_exception(struct dm_exception *e)\n{\n\thlist_bl_del(&e->hash_list);\n}\n\n \nstatic struct dm_exception *dm_lookup_exception(struct dm_exception_table *et,\n\t\t\t\t\t\tchunk_t chunk)\n{\n\tstruct hlist_bl_head *slot;\n\tstruct hlist_bl_node *pos;\n\tstruct dm_exception *e;\n\n\tslot = &et->table[exception_hash(et, chunk)];\n\thlist_bl_for_each_entry(e, pos, slot, hash_list)\n\t\tif (chunk >= e->old_chunk &&\n\t\t    chunk <= e->old_chunk + dm_consecutive_chunk_count(e))\n\t\t\treturn e;\n\n\treturn NULL;\n}\n\nstatic struct dm_exception *alloc_completed_exception(gfp_t gfp)\n{\n\tstruct dm_exception *e;\n\n\te = kmem_cache_alloc(exception_cache, gfp);\n\tif (!e && gfp == GFP_NOIO)\n\t\te = kmem_cache_alloc(exception_cache, GFP_ATOMIC);\n\n\treturn e;\n}\n\nstatic void free_completed_exception(struct dm_exception *e)\n{\n\tkmem_cache_free(exception_cache, e);\n}\n\nstatic struct dm_snap_pending_exception *alloc_pending_exception(struct dm_snapshot *s)\n{\n\tstruct dm_snap_pending_exception *pe = mempool_alloc(&s->pending_pool,\n\t\t\t\t\t\t\t     GFP_NOIO);\n\n\tatomic_inc(&s->pending_exceptions_count);\n\tpe->snap = s;\n\n\treturn pe;\n}\n\nstatic void free_pending_exception(struct dm_snap_pending_exception *pe)\n{\n\tstruct dm_snapshot *s = pe->snap;\n\n\tmempool_free(pe, &s->pending_pool);\n\tsmp_mb__before_atomic();\n\tatomic_dec(&s->pending_exceptions_count);\n}\n\nstatic void dm_insert_exception(struct dm_exception_table *eh,\n\t\t\t\tstruct dm_exception *new_e)\n{\n\tstruct hlist_bl_head *l;\n\tstruct hlist_bl_node *pos;\n\tstruct dm_exception *e = NULL;\n\n\tl = &eh->table[exception_hash(eh, new_e->old_chunk)];\n\n\t \n\tif (!eh->hash_shift)\n\t\tgoto out;\n\n\t \n\thlist_bl_for_each_entry(e, pos, l, hash_list) {\n\t\t \n\t\tif (new_e->old_chunk == (e->old_chunk +\n\t\t\t\t\t dm_consecutive_chunk_count(e) + 1) &&\n\t\t    new_e->new_chunk == (dm_chunk_number(e->new_chunk) +\n\t\t\t\t\t dm_consecutive_chunk_count(e) + 1)) {\n\t\t\tdm_consecutive_chunk_count_inc(e);\n\t\t\tfree_completed_exception(new_e);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tif (new_e->old_chunk == (e->old_chunk - 1) &&\n\t\t    new_e->new_chunk == (dm_chunk_number(e->new_chunk) - 1)) {\n\t\t\tdm_consecutive_chunk_count_inc(e);\n\t\t\te->old_chunk--;\n\t\t\te->new_chunk--;\n\t\t\tfree_completed_exception(new_e);\n\t\t\treturn;\n\t\t}\n\n\t\tif (new_e->old_chunk < e->old_chunk)\n\t\t\tbreak;\n\t}\n\nout:\n\tif (!e) {\n\t\t \n\t\thlist_bl_add_head(&new_e->hash_list, l);\n\t} else if (new_e->old_chunk < e->old_chunk) {\n\t\t \n\t\thlist_bl_add_before(&new_e->hash_list, &e->hash_list);\n\t} else {\n\t\t \n\t\thlist_bl_add_behind(&new_e->hash_list, &e->hash_list);\n\t}\n}\n\n \nstatic int dm_add_exception(void *context, chunk_t old, chunk_t new)\n{\n\tstruct dm_exception_table_lock lock;\n\tstruct dm_snapshot *s = context;\n\tstruct dm_exception *e;\n\n\te = alloc_completed_exception(GFP_KERNEL);\n\tif (!e)\n\t\treturn -ENOMEM;\n\n\te->old_chunk = old;\n\n\t \n\te->new_chunk = new;\n\n\t \n\tdm_exception_table_lock_init(s, old, &lock);\n\n\tdm_exception_table_lock(&lock);\n\tdm_insert_exception(&s->complete, e);\n\tdm_exception_table_unlock(&lock);\n\n\treturn 0;\n}\n\n \nstatic uint32_t __minimum_chunk_size(struct origin *o)\n{\n\tstruct dm_snapshot *snap;\n\tunsigned int chunk_size = rounddown_pow_of_two(UINT_MAX);\n\n\tif (o)\n\t\tlist_for_each_entry(snap, &o->snapshots, list)\n\t\t\tchunk_size = min_not_zero(chunk_size,\n\t\t\t\t\t\t  snap->store->chunk_size);\n\n\treturn (uint32_t) chunk_size;\n}\n\n \nstatic int calc_max_buckets(void)\n{\n\t \n\tunsigned long mem = 2 * 1024 * 1024;\n\n\tmem /= sizeof(struct hlist_bl_head);\n\n\treturn mem;\n}\n\n \nstatic int init_hash_tables(struct dm_snapshot *s)\n{\n\tsector_t hash_size, cow_dev_size, max_buckets;\n\n\t \n\tcow_dev_size = get_dev_size(s->cow->bdev);\n\tmax_buckets = calc_max_buckets();\n\n\thash_size = cow_dev_size >> s->store->chunk_shift;\n\thash_size = min(hash_size, max_buckets);\n\n\tif (hash_size < 64)\n\t\thash_size = 64;\n\thash_size = rounddown_pow_of_two(hash_size);\n\tif (dm_exception_table_init(&s->complete, hash_size,\n\t\t\t\t    DM_CHUNK_CONSECUTIVE_BITS))\n\t\treturn -ENOMEM;\n\n\t \n\thash_size >>= 3;\n\tif (hash_size < 64)\n\t\thash_size = 64;\n\n\tif (dm_exception_table_init(&s->pending, hash_size, 0)) {\n\t\tdm_exception_table_exit(&s->complete, exception_cache);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void merge_shutdown(struct dm_snapshot *s)\n{\n\tclear_bit_unlock(RUNNING_MERGE, &s->state_bits);\n\tsmp_mb__after_atomic();\n\twake_up_bit(&s->state_bits, RUNNING_MERGE);\n}\n\nstatic struct bio *__release_queued_bios_after_merge(struct dm_snapshot *s)\n{\n\ts->first_merging_chunk = 0;\n\ts->num_merging_chunks = 0;\n\n\treturn bio_list_get(&s->bios_queued_during_merge);\n}\n\n \nstatic int __remove_single_exception_chunk(struct dm_snapshot *s,\n\t\t\t\t\t   chunk_t old_chunk)\n{\n\tstruct dm_exception *e;\n\n\te = dm_lookup_exception(&s->complete, old_chunk);\n\tif (!e) {\n\t\tDMERR(\"Corruption detected: exception for block %llu is on disk but not in memory\",\n\t\t      (unsigned long long)old_chunk);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (!dm_consecutive_chunk_count(e)) {\n\t\tdm_remove_exception(e);\n\t\tfree_completed_exception(e);\n\t\treturn 0;\n\t}\n\n\t \n\tif (old_chunk == e->old_chunk) {\n\t\te->old_chunk++;\n\t\te->new_chunk++;\n\t} else if (old_chunk != e->old_chunk +\n\t\t   dm_consecutive_chunk_count(e)) {\n\t\tDMERR(\"Attempt to merge block %llu from the middle of a chunk range [%llu - %llu]\",\n\t\t      (unsigned long long)old_chunk,\n\t\t      (unsigned long long)e->old_chunk,\n\t\t      (unsigned long long)\n\t\t      e->old_chunk + dm_consecutive_chunk_count(e));\n\t\treturn -EINVAL;\n\t}\n\n\tdm_consecutive_chunk_count_dec(e);\n\n\treturn 0;\n}\n\nstatic void flush_bios(struct bio *bio);\n\nstatic int remove_single_exception_chunk(struct dm_snapshot *s)\n{\n\tstruct bio *b = NULL;\n\tint r;\n\tchunk_t old_chunk = s->first_merging_chunk + s->num_merging_chunks - 1;\n\n\tdown_write(&s->lock);\n\n\t \n\tdo {\n\t\tr = __remove_single_exception_chunk(s, old_chunk);\n\t\tif (r)\n\t\t\tgoto out;\n\t} while (old_chunk-- > s->first_merging_chunk);\n\n\tb = __release_queued_bios_after_merge(s);\n\nout:\n\tup_write(&s->lock);\n\tif (b)\n\t\tflush_bios(b);\n\n\treturn r;\n}\n\nstatic int origin_write_extent(struct dm_snapshot *merging_snap,\n\t\t\t       sector_t sector, unsigned int chunk_size);\n\nstatic void merge_callback(int read_err, unsigned long write_err,\n\t\t\t   void *context);\n\nstatic uint64_t read_pending_exceptions_done_count(void)\n{\n\tuint64_t pending_exceptions_done;\n\n\tspin_lock(&_pending_exceptions_done_spinlock);\n\tpending_exceptions_done = _pending_exceptions_done_count;\n\tspin_unlock(&_pending_exceptions_done_spinlock);\n\n\treturn pending_exceptions_done;\n}\n\nstatic void increment_pending_exceptions_done_count(void)\n{\n\tspin_lock(&_pending_exceptions_done_spinlock);\n\t_pending_exceptions_done_count++;\n\tspin_unlock(&_pending_exceptions_done_spinlock);\n\n\twake_up_all(&_pending_exceptions_done);\n}\n\nstatic void snapshot_merge_next_chunks(struct dm_snapshot *s)\n{\n\tint i, linear_chunks;\n\tchunk_t old_chunk, new_chunk;\n\tstruct dm_io_region src, dest;\n\tsector_t io_size;\n\tuint64_t previous_count;\n\n\tBUG_ON(!test_bit(RUNNING_MERGE, &s->state_bits));\n\tif (unlikely(test_bit(SHUTDOWN_MERGE, &s->state_bits)))\n\t\tgoto shut;\n\n\t \n\tif (!s->valid) {\n\t\tDMERR(\"Snapshot is invalid: can't merge\");\n\t\tgoto shut;\n\t}\n\n\tlinear_chunks = s->store->type->prepare_merge(s->store, &old_chunk,\n\t\t\t\t\t\t      &new_chunk);\n\tif (linear_chunks <= 0) {\n\t\tif (linear_chunks < 0) {\n\t\t\tDMERR(\"Read error in exception store: shutting down merge\");\n\t\t\tdown_write(&s->lock);\n\t\t\ts->merge_failed = true;\n\t\t\tup_write(&s->lock);\n\t\t}\n\t\tgoto shut;\n\t}\n\n\t \n\told_chunk = old_chunk + 1 - linear_chunks;\n\tnew_chunk = new_chunk + 1 - linear_chunks;\n\n\t \n\tio_size = linear_chunks * s->store->chunk_size;\n\n\tdest.bdev = s->origin->bdev;\n\tdest.sector = chunk_to_sector(s->store, old_chunk);\n\tdest.count = min(io_size, get_dev_size(dest.bdev) - dest.sector);\n\n\tsrc.bdev = s->cow->bdev;\n\tsrc.sector = chunk_to_sector(s->store, new_chunk);\n\tsrc.count = dest.count;\n\n\t \n\tprevious_count = read_pending_exceptions_done_count();\n\twhile (origin_write_extent(s, dest.sector, io_size)) {\n\t\twait_event(_pending_exceptions_done,\n\t\t\t   (read_pending_exceptions_done_count() !=\n\t\t\t    previous_count));\n\t\t \n\t\tprevious_count = read_pending_exceptions_done_count();\n\t}\n\n\tdown_write(&s->lock);\n\ts->first_merging_chunk = old_chunk;\n\ts->num_merging_chunks = linear_chunks;\n\tup_write(&s->lock);\n\n\t \n\tfor (i = 0; i < linear_chunks; i++)\n\t\t__check_for_conflicting_io(s, old_chunk + i);\n\n\tdm_kcopyd_copy(s->kcopyd_client, &src, 1, &dest, 0, merge_callback, s);\n\treturn;\n\nshut:\n\tmerge_shutdown(s);\n}\n\nstatic void error_bios(struct bio *bio);\n\nstatic void merge_callback(int read_err, unsigned long write_err, void *context)\n{\n\tstruct dm_snapshot *s = context;\n\tstruct bio *b = NULL;\n\n\tif (read_err || write_err) {\n\t\tif (read_err)\n\t\t\tDMERR(\"Read error: shutting down merge.\");\n\t\telse\n\t\t\tDMERR(\"Write error: shutting down merge.\");\n\t\tgoto shut;\n\t}\n\n\tif (blkdev_issue_flush(s->origin->bdev) < 0) {\n\t\tDMERR(\"Flush after merge failed: shutting down merge\");\n\t\tgoto shut;\n\t}\n\n\tif (s->store->type->commit_merge(s->store,\n\t\t\t\t\t s->num_merging_chunks) < 0) {\n\t\tDMERR(\"Write error in exception store: shutting down merge\");\n\t\tgoto shut;\n\t}\n\n\tif (remove_single_exception_chunk(s) < 0)\n\t\tgoto shut;\n\n\tsnapshot_merge_next_chunks(s);\n\n\treturn;\n\nshut:\n\tdown_write(&s->lock);\n\ts->merge_failed = true;\n\tb = __release_queued_bios_after_merge(s);\n\tup_write(&s->lock);\n\terror_bios(b);\n\n\tmerge_shutdown(s);\n}\n\nstatic void start_merge(struct dm_snapshot *s)\n{\n\tif (!test_and_set_bit(RUNNING_MERGE, &s->state_bits))\n\t\tsnapshot_merge_next_chunks(s);\n}\n\n \nstatic void stop_merge(struct dm_snapshot *s)\n{\n\tset_bit(SHUTDOWN_MERGE, &s->state_bits);\n\twait_on_bit(&s->state_bits, RUNNING_MERGE, TASK_UNINTERRUPTIBLE);\n\tclear_bit(SHUTDOWN_MERGE, &s->state_bits);\n}\n\nstatic int parse_snapshot_features(struct dm_arg_set *as, struct dm_snapshot *s,\n\t\t\t\t   struct dm_target *ti)\n{\n\tint r;\n\tunsigned int argc;\n\tconst char *arg_name;\n\n\tstatic const struct dm_arg _args[] = {\n\t\t{0, 2, \"Invalid number of feature arguments\"},\n\t};\n\n\t \n\tif (!as->argc)\n\t\treturn 0;\n\n\tr = dm_read_arg_group(_args, as, &argc, &ti->error);\n\tif (r)\n\t\treturn -EINVAL;\n\n\twhile (argc && !r) {\n\t\targ_name = dm_shift_arg(as);\n\t\targc--;\n\n\t\tif (!strcasecmp(arg_name, \"discard_zeroes_cow\"))\n\t\t\ts->discard_zeroes_cow = true;\n\n\t\telse if (!strcasecmp(arg_name, \"discard_passdown_origin\"))\n\t\t\ts->discard_passdown_origin = true;\n\n\t\telse {\n\t\t\tti->error = \"Unrecognised feature requested\";\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!s->discard_zeroes_cow && s->discard_passdown_origin) {\n\t\t \n\t\tti->error = \"discard_passdown_origin feature depends on discard_zeroes_cow\";\n\t\tr = -EINVAL;\n\t}\n\n\treturn r;\n}\n\n \nstatic int snapshot_ctr(struct dm_target *ti, unsigned int argc, char **argv)\n{\n\tstruct dm_snapshot *s;\n\tstruct dm_arg_set as;\n\tint i;\n\tint r = -EINVAL;\n\tchar *origin_path, *cow_path;\n\tunsigned int args_used, num_flush_bios = 1;\n\tblk_mode_t origin_mode = BLK_OPEN_READ;\n\n\tif (argc < 4) {\n\t\tti->error = \"requires 4 or more arguments\";\n\t\tr = -EINVAL;\n\t\tgoto bad;\n\t}\n\n\tif (dm_target_is_snapshot_merge(ti)) {\n\t\tnum_flush_bios = 2;\n\t\torigin_mode = BLK_OPEN_WRITE;\n\t}\n\n\ts = kzalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s) {\n\t\tti->error = \"Cannot allocate private snapshot structure\";\n\t\tr = -ENOMEM;\n\t\tgoto bad;\n\t}\n\n\tas.argc = argc;\n\tas.argv = argv;\n\tdm_consume_args(&as, 4);\n\tr = parse_snapshot_features(&as, s, ti);\n\tif (r)\n\t\tgoto bad_features;\n\n\torigin_path = argv[0];\n\targv++;\n\targc--;\n\n\tr = dm_get_device(ti, origin_path, origin_mode, &s->origin);\n\tif (r) {\n\t\tti->error = \"Cannot get origin device\";\n\t\tgoto bad_origin;\n\t}\n\n\tcow_path = argv[0];\n\targv++;\n\targc--;\n\n\tr = dm_get_device(ti, cow_path, dm_table_get_mode(ti->table), &s->cow);\n\tif (r) {\n\t\tti->error = \"Cannot get COW device\";\n\t\tgoto bad_cow;\n\t}\n\tif (s->cow->bdev && s->cow->bdev == s->origin->bdev) {\n\t\tti->error = \"COW device cannot be the same as origin device\";\n\t\tr = -EINVAL;\n\t\tgoto bad_store;\n\t}\n\n\tr = dm_exception_store_create(ti, argc, argv, s, &args_used, &s->store);\n\tif (r) {\n\t\tti->error = \"Couldn't create exception store\";\n\t\tr = -EINVAL;\n\t\tgoto bad_store;\n\t}\n\n\targv += args_used;\n\targc -= args_used;\n\n\ts->ti = ti;\n\ts->valid = 1;\n\ts->snapshot_overflowed = 0;\n\ts->active = 0;\n\tatomic_set(&s->pending_exceptions_count, 0);\n\tspin_lock_init(&s->pe_allocation_lock);\n\ts->exception_start_sequence = 0;\n\ts->exception_complete_sequence = 0;\n\ts->out_of_order_tree = RB_ROOT;\n\tinit_rwsem(&s->lock);\n\tINIT_LIST_HEAD(&s->list);\n\tspin_lock_init(&s->pe_lock);\n\ts->state_bits = 0;\n\ts->merge_failed = false;\n\ts->first_merging_chunk = 0;\n\ts->num_merging_chunks = 0;\n\tbio_list_init(&s->bios_queued_during_merge);\n\n\t \n\tif (init_hash_tables(s)) {\n\t\tti->error = \"Unable to allocate hash table space\";\n\t\tr = -ENOMEM;\n\t\tgoto bad_hash_tables;\n\t}\n\n\tinit_waitqueue_head(&s->in_progress_wait);\n\n\ts->kcopyd_client = dm_kcopyd_client_create(&dm_kcopyd_throttle);\n\tif (IS_ERR(s->kcopyd_client)) {\n\t\tr = PTR_ERR(s->kcopyd_client);\n\t\tti->error = \"Could not create kcopyd client\";\n\t\tgoto bad_kcopyd;\n\t}\n\n\tr = mempool_init_slab_pool(&s->pending_pool, MIN_IOS, pending_cache);\n\tif (r) {\n\t\tti->error = \"Could not allocate mempool for pending exceptions\";\n\t\tgoto bad_pending_pool;\n\t}\n\n\tfor (i = 0; i < DM_TRACKED_CHUNK_HASH_SIZE; i++)\n\t\tINIT_HLIST_HEAD(&s->tracked_chunk_hash[i]);\n\n\tspin_lock_init(&s->tracked_chunk_lock);\n\n\tti->private = s;\n\tti->num_flush_bios = num_flush_bios;\n\tif (s->discard_zeroes_cow)\n\t\tti->num_discard_bios = (s->discard_passdown_origin ? 2 : 1);\n\tti->per_io_data_size = sizeof(struct dm_snap_tracked_chunk);\n\n\t \n\t \n\tr = register_snapshot(s);\n\tif (r == -ENOMEM) {\n\t\tti->error = \"Snapshot origin struct allocation failed\";\n\t\tgoto bad_load_and_register;\n\t} else if (r < 0) {\n\t\t \n\t\tgoto bad_load_and_register;\n\t}\n\n\t \n\tif (r > 0) {\n\t\ts->store->chunk_size = 0;\n\t\treturn 0;\n\t}\n\n\tr = s->store->type->read_metadata(s->store, dm_add_exception,\n\t\t\t\t\t  (void *)s);\n\tif (r < 0) {\n\t\tti->error = \"Failed to read snapshot metadata\";\n\t\tgoto bad_read_metadata;\n\t} else if (r > 0) {\n\t\ts->valid = 0;\n\t\tDMWARN(\"Snapshot is marked invalid.\");\n\t}\n\n\tif (!s->store->chunk_size) {\n\t\tti->error = \"Chunk size not set\";\n\t\tr = -EINVAL;\n\t\tgoto bad_read_metadata;\n\t}\n\n\tr = dm_set_target_max_io_len(ti, s->store->chunk_size);\n\tif (r)\n\t\tgoto bad_read_metadata;\n\n\treturn 0;\n\nbad_read_metadata:\n\tunregister_snapshot(s);\nbad_load_and_register:\n\tmempool_exit(&s->pending_pool);\nbad_pending_pool:\n\tdm_kcopyd_client_destroy(s->kcopyd_client);\nbad_kcopyd:\n\tdm_exception_table_exit(&s->pending, pending_cache);\n\tdm_exception_table_exit(&s->complete, exception_cache);\nbad_hash_tables:\n\tdm_exception_store_destroy(s->store);\nbad_store:\n\tdm_put_device(ti, s->cow);\nbad_cow:\n\tdm_put_device(ti, s->origin);\nbad_origin:\nbad_features:\n\tkfree(s);\nbad:\n\treturn r;\n}\n\nstatic void __free_exceptions(struct dm_snapshot *s)\n{\n\tdm_kcopyd_client_destroy(s->kcopyd_client);\n\ts->kcopyd_client = NULL;\n\n\tdm_exception_table_exit(&s->pending, pending_cache);\n\tdm_exception_table_exit(&s->complete, exception_cache);\n}\n\nstatic void __handover_exceptions(struct dm_snapshot *snap_src,\n\t\t\t\t  struct dm_snapshot *snap_dest)\n{\n\tunion {\n\t\tstruct dm_exception_table table_swap;\n\t\tstruct dm_exception_store *store_swap;\n\t} u;\n\n\t \n\tu.table_swap = snap_dest->complete;\n\tsnap_dest->complete = snap_src->complete;\n\tsnap_src->complete = u.table_swap;\n\n\tu.store_swap = snap_dest->store;\n\tsnap_dest->store = snap_src->store;\n\tsnap_dest->store->userspace_supports_overflow = u.store_swap->userspace_supports_overflow;\n\tsnap_src->store = u.store_swap;\n\n\tsnap_dest->store->snap = snap_dest;\n\tsnap_src->store->snap = snap_src;\n\n\tsnap_dest->ti->max_io_len = snap_dest->store->chunk_size;\n\tsnap_dest->valid = snap_src->valid;\n\tsnap_dest->snapshot_overflowed = snap_src->snapshot_overflowed;\n\n\t \n\tsnap_src->valid = 0;\n}\n\nstatic void snapshot_dtr(struct dm_target *ti)\n{\n#ifdef CONFIG_DM_DEBUG\n\tint i;\n#endif\n\tstruct dm_snapshot *s = ti->private;\n\tstruct dm_snapshot *snap_src = NULL, *snap_dest = NULL;\n\n\tdown_read(&_origins_lock);\n\t \n\t(void) __find_snapshots_sharing_cow(s, &snap_src, &snap_dest, NULL);\n\tif (snap_src && snap_dest && (s == snap_src)) {\n\t\tdown_write(&snap_dest->lock);\n\t\tsnap_dest->valid = 0;\n\t\tup_write(&snap_dest->lock);\n\t\tDMERR(\"Cancelling snapshot handover.\");\n\t}\n\tup_read(&_origins_lock);\n\n\tif (dm_target_is_snapshot_merge(ti))\n\t\tstop_merge(s);\n\n\t \n\t \n\tunregister_snapshot(s);\n\n\twhile (atomic_read(&s->pending_exceptions_count))\n\t\tfsleep(1000);\n\t \n\tsmp_mb();\n\n#ifdef CONFIG_DM_DEBUG\n\tfor (i = 0; i < DM_TRACKED_CHUNK_HASH_SIZE; i++)\n\t\tBUG_ON(!hlist_empty(&s->tracked_chunk_hash[i]));\n#endif\n\n\t__free_exceptions(s);\n\n\tmempool_exit(&s->pending_pool);\n\n\tdm_exception_store_destroy(s->store);\n\n\tdm_put_device(ti, s->cow);\n\n\tdm_put_device(ti, s->origin);\n\n\tWARN_ON(s->in_progress);\n\n\tkfree(s);\n}\n\nstatic void account_start_copy(struct dm_snapshot *s)\n{\n\tspin_lock(&s->in_progress_wait.lock);\n\ts->in_progress++;\n\tspin_unlock(&s->in_progress_wait.lock);\n}\n\nstatic void account_end_copy(struct dm_snapshot *s)\n{\n\tspin_lock(&s->in_progress_wait.lock);\n\tBUG_ON(!s->in_progress);\n\ts->in_progress--;\n\tif (likely(s->in_progress <= cow_threshold) &&\n\t    unlikely(waitqueue_active(&s->in_progress_wait)))\n\t\twake_up_locked(&s->in_progress_wait);\n\tspin_unlock(&s->in_progress_wait.lock);\n}\n\nstatic bool wait_for_in_progress(struct dm_snapshot *s, bool unlock_origins)\n{\n\tif (unlikely(s->in_progress > cow_threshold)) {\n\t\tspin_lock(&s->in_progress_wait.lock);\n\t\tif (likely(s->in_progress > cow_threshold)) {\n\t\t\t \n\t\t\tDECLARE_WAITQUEUE(wait, current);\n\n\t\t\t__add_wait_queue(&s->in_progress_wait, &wait);\n\t\t\t__set_current_state(TASK_UNINTERRUPTIBLE);\n\t\t\tspin_unlock(&s->in_progress_wait.lock);\n\t\t\tif (unlock_origins)\n\t\t\t\tup_read(&_origins_lock);\n\t\t\tio_schedule();\n\t\t\tremove_wait_queue(&s->in_progress_wait, &wait);\n\t\t\treturn false;\n\t\t}\n\t\tspin_unlock(&s->in_progress_wait.lock);\n\t}\n\treturn true;\n}\n\n \nstatic void flush_bios(struct bio *bio)\n{\n\tstruct bio *n;\n\n\twhile (bio) {\n\t\tn = bio->bi_next;\n\t\tbio->bi_next = NULL;\n\t\tsubmit_bio_noacct(bio);\n\t\tbio = n;\n\t}\n}\n\nstatic int do_origin(struct dm_dev *origin, struct bio *bio, bool limit);\n\n \nstatic void retry_origin_bios(struct dm_snapshot *s, struct bio *bio)\n{\n\tstruct bio *n;\n\tint r;\n\n\twhile (bio) {\n\t\tn = bio->bi_next;\n\t\tbio->bi_next = NULL;\n\t\tr = do_origin(s->origin, bio, false);\n\t\tif (r == DM_MAPIO_REMAPPED)\n\t\t\tsubmit_bio_noacct(bio);\n\t\tbio = n;\n\t}\n}\n\n \nstatic void error_bios(struct bio *bio)\n{\n\tstruct bio *n;\n\n\twhile (bio) {\n\t\tn = bio->bi_next;\n\t\tbio->bi_next = NULL;\n\t\tbio_io_error(bio);\n\t\tbio = n;\n\t}\n}\n\nstatic void __invalidate_snapshot(struct dm_snapshot *s, int err)\n{\n\tif (!s->valid)\n\t\treturn;\n\n\tif (err == -EIO)\n\t\tDMERR(\"Invalidating snapshot: Error reading/writing.\");\n\telse if (err == -ENOMEM)\n\t\tDMERR(\"Invalidating snapshot: Unable to allocate exception.\");\n\n\tif (s->store->type->drop_snapshot)\n\t\ts->store->type->drop_snapshot(s->store);\n\n\ts->valid = 0;\n\n\tdm_table_event(s->ti->table);\n}\n\nstatic void invalidate_snapshot(struct dm_snapshot *s, int err)\n{\n\tdown_write(&s->lock);\n\t__invalidate_snapshot(s, err);\n\tup_write(&s->lock);\n}\n\nstatic void pending_complete(void *context, int success)\n{\n\tstruct dm_snap_pending_exception *pe = context;\n\tstruct dm_exception *e;\n\tstruct dm_snapshot *s = pe->snap;\n\tstruct bio *origin_bios = NULL;\n\tstruct bio *snapshot_bios = NULL;\n\tstruct bio *full_bio = NULL;\n\tstruct dm_exception_table_lock lock;\n\tint error = 0;\n\n\tdm_exception_table_lock_init(s, pe->e.old_chunk, &lock);\n\n\tif (!success) {\n\t\t \n\t\tinvalidate_snapshot(s, -EIO);\n\t\terror = 1;\n\n\t\tdm_exception_table_lock(&lock);\n\t\tgoto out;\n\t}\n\n\te = alloc_completed_exception(GFP_NOIO);\n\tif (!e) {\n\t\tinvalidate_snapshot(s, -ENOMEM);\n\t\terror = 1;\n\n\t\tdm_exception_table_lock(&lock);\n\t\tgoto out;\n\t}\n\t*e = pe->e;\n\n\tdown_read(&s->lock);\n\tdm_exception_table_lock(&lock);\n\tif (!s->valid) {\n\t\tup_read(&s->lock);\n\t\tfree_completed_exception(e);\n\t\terror = 1;\n\n\t\tgoto out;\n\t}\n\n\t \n\tdm_insert_exception(&s->complete, e);\n\tup_read(&s->lock);\n\n\t \n\tif (__chunk_is_tracked(s, pe->e.old_chunk)) {\n\t\tdm_exception_table_unlock(&lock);\n\t\t__check_for_conflicting_io(s, pe->e.old_chunk);\n\t\tdm_exception_table_lock(&lock);\n\t}\n\nout:\n\t \n\tdm_remove_exception(&pe->e);\n\n\tdm_exception_table_unlock(&lock);\n\n\tsnapshot_bios = bio_list_get(&pe->snapshot_bios);\n\torigin_bios = bio_list_get(&pe->origin_bios);\n\tfull_bio = pe->full_bio;\n\tif (full_bio)\n\t\tfull_bio->bi_end_io = pe->full_bio_end_io;\n\tincrement_pending_exceptions_done_count();\n\n\t \n\tif (error) {\n\t\tif (full_bio)\n\t\t\tbio_io_error(full_bio);\n\t\terror_bios(snapshot_bios);\n\t} else {\n\t\tif (full_bio)\n\t\t\tbio_endio(full_bio);\n\t\tflush_bios(snapshot_bios);\n\t}\n\n\tretry_origin_bios(s, origin_bios);\n\n\tfree_pending_exception(pe);\n}\n\nstatic void complete_exception(struct dm_snap_pending_exception *pe)\n{\n\tstruct dm_snapshot *s = pe->snap;\n\n\t \n\ts->store->type->commit_exception(s->store, &pe->e, !pe->copy_error,\n\t\t\t\t\t pending_complete, pe);\n}\n\n \nstatic void copy_callback(int read_err, unsigned long write_err, void *context)\n{\n\tstruct dm_snap_pending_exception *pe = context;\n\tstruct dm_snapshot *s = pe->snap;\n\n\tpe->copy_error = read_err || write_err;\n\n\tif (pe->exception_sequence == s->exception_complete_sequence) {\n\t\tstruct rb_node *next;\n\n\t\ts->exception_complete_sequence++;\n\t\tcomplete_exception(pe);\n\n\t\tnext = rb_first(&s->out_of_order_tree);\n\t\twhile (next) {\n\t\t\tpe = rb_entry(next, struct dm_snap_pending_exception,\n\t\t\t\t\tout_of_order_node);\n\t\t\tif (pe->exception_sequence != s->exception_complete_sequence)\n\t\t\t\tbreak;\n\t\t\tnext = rb_next(next);\n\t\t\ts->exception_complete_sequence++;\n\t\t\trb_erase(&pe->out_of_order_node, &s->out_of_order_tree);\n\t\t\tcomplete_exception(pe);\n\t\t\tcond_resched();\n\t\t}\n\t} else {\n\t\tstruct rb_node *parent = NULL;\n\t\tstruct rb_node **p = &s->out_of_order_tree.rb_node;\n\t\tstruct dm_snap_pending_exception *pe2;\n\n\t\twhile (*p) {\n\t\t\tpe2 = rb_entry(*p, struct dm_snap_pending_exception, out_of_order_node);\n\t\t\tparent = *p;\n\n\t\t\tBUG_ON(pe->exception_sequence == pe2->exception_sequence);\n\t\t\tif (pe->exception_sequence < pe2->exception_sequence)\n\t\t\t\tp = &((*p)->rb_left);\n\t\t\telse\n\t\t\t\tp = &((*p)->rb_right);\n\t\t}\n\n\t\trb_link_node(&pe->out_of_order_node, parent, p);\n\t\trb_insert_color(&pe->out_of_order_node, &s->out_of_order_tree);\n\t}\n\taccount_end_copy(s);\n}\n\n \nstatic void start_copy(struct dm_snap_pending_exception *pe)\n{\n\tstruct dm_snapshot *s = pe->snap;\n\tstruct dm_io_region src, dest;\n\tstruct block_device *bdev = s->origin->bdev;\n\tsector_t dev_size;\n\n\tdev_size = get_dev_size(bdev);\n\n\tsrc.bdev = bdev;\n\tsrc.sector = chunk_to_sector(s->store, pe->e.old_chunk);\n\tsrc.count = min((sector_t)s->store->chunk_size, dev_size - src.sector);\n\n\tdest.bdev = s->cow->bdev;\n\tdest.sector = chunk_to_sector(s->store, pe->e.new_chunk);\n\tdest.count = src.count;\n\n\t \n\taccount_start_copy(s);\n\tdm_kcopyd_copy(s->kcopyd_client, &src, 1, &dest, 0, copy_callback, pe);\n}\n\nstatic void full_bio_end_io(struct bio *bio)\n{\n\tvoid *callback_data = bio->bi_private;\n\n\tdm_kcopyd_do_callback(callback_data, 0, bio->bi_status ? 1 : 0);\n}\n\nstatic void start_full_bio(struct dm_snap_pending_exception *pe,\n\t\t\t   struct bio *bio)\n{\n\tstruct dm_snapshot *s = pe->snap;\n\tvoid *callback_data;\n\n\tpe->full_bio = bio;\n\tpe->full_bio_end_io = bio->bi_end_io;\n\n\taccount_start_copy(s);\n\tcallback_data = dm_kcopyd_prepare_callback(s->kcopyd_client,\n\t\t\t\t\t\t   copy_callback, pe);\n\n\tbio->bi_end_io = full_bio_end_io;\n\tbio->bi_private = callback_data;\n\n\tsubmit_bio_noacct(bio);\n}\n\nstatic struct dm_snap_pending_exception *\n__lookup_pending_exception(struct dm_snapshot *s, chunk_t chunk)\n{\n\tstruct dm_exception *e = dm_lookup_exception(&s->pending, chunk);\n\n\tif (!e)\n\t\treturn NULL;\n\n\treturn container_of(e, struct dm_snap_pending_exception, e);\n}\n\n \nstatic struct dm_snap_pending_exception *\n__insert_pending_exception(struct dm_snapshot *s,\n\t\t\t   struct dm_snap_pending_exception *pe, chunk_t chunk)\n{\n\tpe->e.old_chunk = chunk;\n\tbio_list_init(&pe->origin_bios);\n\tbio_list_init(&pe->snapshot_bios);\n\tpe->started = 0;\n\tpe->full_bio = NULL;\n\n\tspin_lock(&s->pe_allocation_lock);\n\tif (s->store->type->prepare_exception(s->store, &pe->e)) {\n\t\tspin_unlock(&s->pe_allocation_lock);\n\t\tfree_pending_exception(pe);\n\t\treturn NULL;\n\t}\n\n\tpe->exception_sequence = s->exception_start_sequence++;\n\tspin_unlock(&s->pe_allocation_lock);\n\n\tdm_insert_exception(&s->pending, &pe->e);\n\n\treturn pe;\n}\n\n \nstatic struct dm_snap_pending_exception *\n__find_pending_exception(struct dm_snapshot *s,\n\t\t\t struct dm_snap_pending_exception *pe, chunk_t chunk)\n{\n\tstruct dm_snap_pending_exception *pe2;\n\n\tpe2 = __lookup_pending_exception(s, chunk);\n\tif (pe2) {\n\t\tfree_pending_exception(pe);\n\t\treturn pe2;\n\t}\n\n\treturn __insert_pending_exception(s, pe, chunk);\n}\n\nstatic void remap_exception(struct dm_snapshot *s, struct dm_exception *e,\n\t\t\t    struct bio *bio, chunk_t chunk)\n{\n\tbio_set_dev(bio, s->cow->bdev);\n\tbio->bi_iter.bi_sector =\n\t\tchunk_to_sector(s->store, dm_chunk_number(e->new_chunk) +\n\t\t\t\t(chunk - e->old_chunk)) +\n\t\t(bio->bi_iter.bi_sector & s->store->chunk_mask);\n}\n\nstatic void zero_callback(int read_err, unsigned long write_err, void *context)\n{\n\tstruct bio *bio = context;\n\tstruct dm_snapshot *s = bio->bi_private;\n\n\taccount_end_copy(s);\n\tbio->bi_status = write_err ? BLK_STS_IOERR : 0;\n\tbio_endio(bio);\n}\n\nstatic void zero_exception(struct dm_snapshot *s, struct dm_exception *e,\n\t\t\t   struct bio *bio, chunk_t chunk)\n{\n\tstruct dm_io_region dest;\n\n\tdest.bdev = s->cow->bdev;\n\tdest.sector = bio->bi_iter.bi_sector;\n\tdest.count = s->store->chunk_size;\n\n\taccount_start_copy(s);\n\tWARN_ON_ONCE(bio->bi_private);\n\tbio->bi_private = s;\n\tdm_kcopyd_zero(s->kcopyd_client, 1, &dest, 0, zero_callback, bio);\n}\n\nstatic bool io_overlaps_chunk(struct dm_snapshot *s, struct bio *bio)\n{\n\treturn bio->bi_iter.bi_size ==\n\t\t(s->store->chunk_size << SECTOR_SHIFT);\n}\n\nstatic int snapshot_map(struct dm_target *ti, struct bio *bio)\n{\n\tstruct dm_exception *e;\n\tstruct dm_snapshot *s = ti->private;\n\tint r = DM_MAPIO_REMAPPED;\n\tchunk_t chunk;\n\tstruct dm_snap_pending_exception *pe = NULL;\n\tstruct dm_exception_table_lock lock;\n\n\tinit_tracked_chunk(bio);\n\n\tif (bio->bi_opf & REQ_PREFLUSH) {\n\t\tbio_set_dev(bio, s->cow->bdev);\n\t\treturn DM_MAPIO_REMAPPED;\n\t}\n\n\tchunk = sector_to_chunk(s->store, bio->bi_iter.bi_sector);\n\tdm_exception_table_lock_init(s, chunk, &lock);\n\n\t \n\t \n\tif (!s->valid)\n\t\treturn DM_MAPIO_KILL;\n\n\tif (bio_data_dir(bio) == WRITE) {\n\t\twhile (unlikely(!wait_for_in_progress(s, false)))\n\t\t\t;  \n\t}\n\n\tdown_read(&s->lock);\n\tdm_exception_table_lock(&lock);\n\n\tif (!s->valid || (unlikely(s->snapshot_overflowed) &&\n\t    bio_data_dir(bio) == WRITE)) {\n\t\tr = DM_MAPIO_KILL;\n\t\tgoto out_unlock;\n\t}\n\n\tif (unlikely(bio_op(bio) == REQ_OP_DISCARD)) {\n\t\tif (s->discard_passdown_origin && dm_bio_get_target_bio_nr(bio)) {\n\t\t\t \n\t\t\tbio_set_dev(bio, s->origin->bdev);\n\t\t\ttrack_chunk(s, bio, chunk);\n\t\t\tgoto out_unlock;\n\t\t}\n\t\t \n\t}\n\n\t \n\te = dm_lookup_exception(&s->complete, chunk);\n\tif (e) {\n\t\tremap_exception(s, e, bio, chunk);\n\t\tif (unlikely(bio_op(bio) == REQ_OP_DISCARD) &&\n\t\t    io_overlaps_chunk(s, bio)) {\n\t\t\tdm_exception_table_unlock(&lock);\n\t\t\tup_read(&s->lock);\n\t\t\tzero_exception(s, e, bio, chunk);\n\t\t\tr = DM_MAPIO_SUBMITTED;  \n\t\t\tgoto out;\n\t\t}\n\t\tgoto out_unlock;\n\t}\n\n\tif (unlikely(bio_op(bio) == REQ_OP_DISCARD)) {\n\t\t \n\t\tbio_endio(bio);\n\t\tr = DM_MAPIO_SUBMITTED;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tif (bio_data_dir(bio) == WRITE) {\n\t\tpe = __lookup_pending_exception(s, chunk);\n\t\tif (!pe) {\n\t\t\tdm_exception_table_unlock(&lock);\n\t\t\tpe = alloc_pending_exception(s);\n\t\t\tdm_exception_table_lock(&lock);\n\n\t\t\te = dm_lookup_exception(&s->complete, chunk);\n\t\t\tif (e) {\n\t\t\t\tfree_pending_exception(pe);\n\t\t\t\tremap_exception(s, e, bio, chunk);\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\n\t\t\tpe = __find_pending_exception(s, pe, chunk);\n\t\t\tif (!pe) {\n\t\t\t\tdm_exception_table_unlock(&lock);\n\t\t\t\tup_read(&s->lock);\n\n\t\t\t\tdown_write(&s->lock);\n\n\t\t\t\tif (s->store->userspace_supports_overflow) {\n\t\t\t\t\tif (s->valid && !s->snapshot_overflowed) {\n\t\t\t\t\t\ts->snapshot_overflowed = 1;\n\t\t\t\t\t\tDMERR(\"Snapshot overflowed: Unable to allocate exception.\");\n\t\t\t\t\t}\n\t\t\t\t} else\n\t\t\t\t\t__invalidate_snapshot(s, -ENOMEM);\n\t\t\t\tup_write(&s->lock);\n\n\t\t\t\tr = DM_MAPIO_KILL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tremap_exception(s, &pe->e, bio, chunk);\n\n\t\tr = DM_MAPIO_SUBMITTED;\n\n\t\tif (!pe->started && io_overlaps_chunk(s, bio)) {\n\t\t\tpe->started = 1;\n\n\t\t\tdm_exception_table_unlock(&lock);\n\t\t\tup_read(&s->lock);\n\n\t\t\tstart_full_bio(pe, bio);\n\t\t\tgoto out;\n\t\t}\n\n\t\tbio_list_add(&pe->snapshot_bios, bio);\n\n\t\tif (!pe->started) {\n\t\t\t \n\t\t\tpe->started = 1;\n\n\t\t\tdm_exception_table_unlock(&lock);\n\t\t\tup_read(&s->lock);\n\n\t\t\tstart_copy(pe);\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tbio_set_dev(bio, s->origin->bdev);\n\t\ttrack_chunk(s, bio, chunk);\n\t}\n\nout_unlock:\n\tdm_exception_table_unlock(&lock);\n\tup_read(&s->lock);\nout:\n\treturn r;\n}\n\n \nstatic int snapshot_merge_map(struct dm_target *ti, struct bio *bio)\n{\n\tstruct dm_exception *e;\n\tstruct dm_snapshot *s = ti->private;\n\tint r = DM_MAPIO_REMAPPED;\n\tchunk_t chunk;\n\n\tinit_tracked_chunk(bio);\n\n\tif (bio->bi_opf & REQ_PREFLUSH) {\n\t\tif (!dm_bio_get_target_bio_nr(bio))\n\t\t\tbio_set_dev(bio, s->origin->bdev);\n\t\telse\n\t\t\tbio_set_dev(bio, s->cow->bdev);\n\t\treturn DM_MAPIO_REMAPPED;\n\t}\n\n\tif (unlikely(bio_op(bio) == REQ_OP_DISCARD)) {\n\t\t \n\t\tbio_endio(bio);\n\t\treturn DM_MAPIO_SUBMITTED;\n\t}\n\n\tchunk = sector_to_chunk(s->store, bio->bi_iter.bi_sector);\n\n\tdown_write(&s->lock);\n\n\t \n\tif (!s->valid)\n\t\tgoto redirect_to_origin;\n\n\t \n\te = dm_lookup_exception(&s->complete, chunk);\n\tif (e) {\n\t\t \n\t\tif (bio_data_dir(bio) == WRITE &&\n\t\t    chunk >= s->first_merging_chunk &&\n\t\t    chunk < (s->first_merging_chunk +\n\t\t\t     s->num_merging_chunks)) {\n\t\t\tbio_set_dev(bio, s->origin->bdev);\n\t\t\tbio_list_add(&s->bios_queued_during_merge, bio);\n\t\t\tr = DM_MAPIO_SUBMITTED;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tremap_exception(s, e, bio, chunk);\n\n\t\tif (bio_data_dir(bio) == WRITE)\n\t\t\ttrack_chunk(s, bio, chunk);\n\t\tgoto out_unlock;\n\t}\n\nredirect_to_origin:\n\tbio_set_dev(bio, s->origin->bdev);\n\n\tif (bio_data_dir(bio) == WRITE) {\n\t\tup_write(&s->lock);\n\t\treturn do_origin(s->origin, bio, false);\n\t}\n\nout_unlock:\n\tup_write(&s->lock);\n\n\treturn r;\n}\n\nstatic int snapshot_end_io(struct dm_target *ti, struct bio *bio,\n\t\tblk_status_t *error)\n{\n\tstruct dm_snapshot *s = ti->private;\n\n\tif (is_bio_tracked(bio))\n\t\tstop_tracking_chunk(s, bio);\n\n\treturn DM_ENDIO_DONE;\n}\n\nstatic void snapshot_merge_presuspend(struct dm_target *ti)\n{\n\tstruct dm_snapshot *s = ti->private;\n\n\tstop_merge(s);\n}\n\nstatic int snapshot_preresume(struct dm_target *ti)\n{\n\tint r = 0;\n\tstruct dm_snapshot *s = ti->private;\n\tstruct dm_snapshot *snap_src = NULL, *snap_dest = NULL;\n\n\tdown_read(&_origins_lock);\n\t(void) __find_snapshots_sharing_cow(s, &snap_src, &snap_dest, NULL);\n\tif (snap_src && snap_dest) {\n\t\tdown_read(&snap_src->lock);\n\t\tif (s == snap_src) {\n\t\t\tDMERR(\"Unable to resume snapshot source until handover completes.\");\n\t\t\tr = -EINVAL;\n\t\t} else if (!dm_suspended(snap_src->ti)) {\n\t\t\tDMERR(\"Unable to perform snapshot handover until source is suspended.\");\n\t\t\tr = -EINVAL;\n\t\t}\n\t\tup_read(&snap_src->lock);\n\t}\n\tup_read(&_origins_lock);\n\n\treturn r;\n}\n\nstatic void snapshot_resume(struct dm_target *ti)\n{\n\tstruct dm_snapshot *s = ti->private;\n\tstruct dm_snapshot *snap_src = NULL, *snap_dest = NULL, *snap_merging = NULL;\n\tstruct dm_origin *o;\n\tstruct mapped_device *origin_md = NULL;\n\tbool must_restart_merging = false;\n\n\tdown_read(&_origins_lock);\n\n\to = __lookup_dm_origin(s->origin->bdev);\n\tif (o)\n\t\torigin_md = dm_table_get_md(o->ti->table);\n\tif (!origin_md) {\n\t\t(void) __find_snapshots_sharing_cow(s, NULL, NULL, &snap_merging);\n\t\tif (snap_merging)\n\t\t\torigin_md = dm_table_get_md(snap_merging->ti->table);\n\t}\n\tif (origin_md == dm_table_get_md(ti->table))\n\t\torigin_md = NULL;\n\tif (origin_md) {\n\t\tif (dm_hold(origin_md))\n\t\t\torigin_md = NULL;\n\t}\n\n\tup_read(&_origins_lock);\n\n\tif (origin_md) {\n\t\tdm_internal_suspend_fast(origin_md);\n\t\tif (snap_merging && test_bit(RUNNING_MERGE, &snap_merging->state_bits)) {\n\t\t\tmust_restart_merging = true;\n\t\t\tstop_merge(snap_merging);\n\t\t}\n\t}\n\n\tdown_read(&_origins_lock);\n\n\t(void) __find_snapshots_sharing_cow(s, &snap_src, &snap_dest, NULL);\n\tif (snap_src && snap_dest) {\n\t\tdown_write(&snap_src->lock);\n\t\tdown_write_nested(&snap_dest->lock, SINGLE_DEPTH_NESTING);\n\t\t__handover_exceptions(snap_src, snap_dest);\n\t\tup_write(&snap_dest->lock);\n\t\tup_write(&snap_src->lock);\n\t}\n\n\tup_read(&_origins_lock);\n\n\tif (origin_md) {\n\t\tif (must_restart_merging)\n\t\t\tstart_merge(snap_merging);\n\t\tdm_internal_resume_fast(origin_md);\n\t\tdm_put(origin_md);\n\t}\n\n\t \n\treregister_snapshot(s);\n\n\tdown_write(&s->lock);\n\ts->active = 1;\n\tup_write(&s->lock);\n}\n\nstatic uint32_t get_origin_minimum_chunksize(struct block_device *bdev)\n{\n\tuint32_t min_chunksize;\n\n\tdown_read(&_origins_lock);\n\tmin_chunksize = __minimum_chunk_size(__lookup_origin(bdev));\n\tup_read(&_origins_lock);\n\n\treturn min_chunksize;\n}\n\nstatic void snapshot_merge_resume(struct dm_target *ti)\n{\n\tstruct dm_snapshot *s = ti->private;\n\n\t \n\tsnapshot_resume(ti);\n\n\t \n\tti->max_io_len = get_origin_minimum_chunksize(s->origin->bdev);\n\n\tstart_merge(s);\n}\n\nstatic void snapshot_status(struct dm_target *ti, status_type_t type,\n\t\t\t    unsigned int status_flags, char *result, unsigned int maxlen)\n{\n\tunsigned int sz = 0;\n\tstruct dm_snapshot *snap = ti->private;\n\tunsigned int num_features;\n\n\tswitch (type) {\n\tcase STATUSTYPE_INFO:\n\n\t\tdown_write(&snap->lock);\n\n\t\tif (!snap->valid)\n\t\t\tDMEMIT(\"Invalid\");\n\t\telse if (snap->merge_failed)\n\t\t\tDMEMIT(\"Merge failed\");\n\t\telse if (snap->snapshot_overflowed)\n\t\t\tDMEMIT(\"Overflow\");\n\t\telse {\n\t\t\tif (snap->store->type->usage) {\n\t\t\t\tsector_t total_sectors, sectors_allocated,\n\t\t\t\t\t metadata_sectors;\n\t\t\t\tsnap->store->type->usage(snap->store,\n\t\t\t\t\t\t\t &total_sectors,\n\t\t\t\t\t\t\t &sectors_allocated,\n\t\t\t\t\t\t\t &metadata_sectors);\n\t\t\t\tDMEMIT(\"%llu/%llu %llu\",\n\t\t\t\t       (unsigned long long)sectors_allocated,\n\t\t\t\t       (unsigned long long)total_sectors,\n\t\t\t\t       (unsigned long long)metadata_sectors);\n\t\t\t} else\n\t\t\t\tDMEMIT(\"Unknown\");\n\t\t}\n\n\t\tup_write(&snap->lock);\n\n\t\tbreak;\n\n\tcase STATUSTYPE_TABLE:\n\t\t \n\t\tDMEMIT(\"%s %s\", snap->origin->name, snap->cow->name);\n\t\tsz += snap->store->type->status(snap->store, type, result + sz,\n\t\t\t\t\t\tmaxlen - sz);\n\t\tnum_features = snap->discard_zeroes_cow + snap->discard_passdown_origin;\n\t\tif (num_features) {\n\t\t\tDMEMIT(\" %u\", num_features);\n\t\t\tif (snap->discard_zeroes_cow)\n\t\t\t\tDMEMIT(\" discard_zeroes_cow\");\n\t\t\tif (snap->discard_passdown_origin)\n\t\t\t\tDMEMIT(\" discard_passdown_origin\");\n\t\t}\n\t\tbreak;\n\n\tcase STATUSTYPE_IMA:\n\t\tDMEMIT_TARGET_NAME_VERSION(ti->type);\n\t\tDMEMIT(\",snap_origin_name=%s\", snap->origin->name);\n\t\tDMEMIT(\",snap_cow_name=%s\", snap->cow->name);\n\t\tDMEMIT(\",snap_valid=%c\", snap->valid ? 'y' : 'n');\n\t\tDMEMIT(\",snap_merge_failed=%c\", snap->merge_failed ? 'y' : 'n');\n\t\tDMEMIT(\",snapshot_overflowed=%c\", snap->snapshot_overflowed ? 'y' : 'n');\n\t\tDMEMIT(\";\");\n\t\tbreak;\n\t}\n}\n\nstatic int snapshot_iterate_devices(struct dm_target *ti,\n\t\t\t\t    iterate_devices_callout_fn fn, void *data)\n{\n\tstruct dm_snapshot *snap = ti->private;\n\tint r;\n\n\tr = fn(ti, snap->origin, 0, ti->len, data);\n\n\tif (!r)\n\t\tr = fn(ti, snap->cow, 0, get_dev_size(snap->cow->bdev), data);\n\n\treturn r;\n}\n\nstatic void snapshot_io_hints(struct dm_target *ti, struct queue_limits *limits)\n{\n\tstruct dm_snapshot *snap = ti->private;\n\n\tif (snap->discard_zeroes_cow) {\n\t\tstruct dm_snapshot *snap_src = NULL, *snap_dest = NULL;\n\n\t\tdown_read(&_origins_lock);\n\n\t\t(void) __find_snapshots_sharing_cow(snap, &snap_src, &snap_dest, NULL);\n\t\tif (snap_src && snap_dest)\n\t\t\tsnap = snap_src;\n\n\t\t \n\t\tlimits->discard_granularity = snap->store->chunk_size;\n\t\tlimits->max_discard_sectors = snap->store->chunk_size;\n\n\t\tup_read(&_origins_lock);\n\t}\n}\n\n \n \nstatic int __origin_write(struct list_head *snapshots, sector_t sector,\n\t\t\t  struct bio *bio)\n{\n\tint r = DM_MAPIO_REMAPPED;\n\tstruct dm_snapshot *snap;\n\tstruct dm_exception *e;\n\tstruct dm_snap_pending_exception *pe, *pe2;\n\tstruct dm_snap_pending_exception *pe_to_start_now = NULL;\n\tstruct dm_snap_pending_exception *pe_to_start_last = NULL;\n\tstruct dm_exception_table_lock lock;\n\tchunk_t chunk;\n\n\t \n\tlist_for_each_entry(snap, snapshots, list) {\n\t\t \n\t\tif (dm_target_is_snapshot_merge(snap->ti))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (sector >= dm_table_get_size(snap->ti->table))\n\t\t\tcontinue;\n\n\t\t \n\t\tchunk = sector_to_chunk(snap->store, sector);\n\t\tdm_exception_table_lock_init(snap, chunk, &lock);\n\n\t\tdown_read(&snap->lock);\n\t\tdm_exception_table_lock(&lock);\n\n\t\t \n\t\tif (!snap->valid || !snap->active)\n\t\t\tgoto next_snapshot;\n\n\t\tpe = __lookup_pending_exception(snap, chunk);\n\t\tif (!pe) {\n\t\t\t \n\t\t\te = dm_lookup_exception(&snap->complete, chunk);\n\t\t\tif (e)\n\t\t\t\tgoto next_snapshot;\n\n\t\t\tdm_exception_table_unlock(&lock);\n\t\t\tpe = alloc_pending_exception(snap);\n\t\t\tdm_exception_table_lock(&lock);\n\n\t\t\tpe2 = __lookup_pending_exception(snap, chunk);\n\n\t\t\tif (!pe2) {\n\t\t\t\te = dm_lookup_exception(&snap->complete, chunk);\n\t\t\t\tif (e) {\n\t\t\t\t\tfree_pending_exception(pe);\n\t\t\t\t\tgoto next_snapshot;\n\t\t\t\t}\n\n\t\t\t\tpe = __insert_pending_exception(snap, pe, chunk);\n\t\t\t\tif (!pe) {\n\t\t\t\t\tdm_exception_table_unlock(&lock);\n\t\t\t\t\tup_read(&snap->lock);\n\n\t\t\t\t\tinvalidate_snapshot(snap, -ENOMEM);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tfree_pending_exception(pe);\n\t\t\t\tpe = pe2;\n\t\t\t}\n\t\t}\n\n\t\tr = DM_MAPIO_SUBMITTED;\n\n\t\t \n\t\tif (bio) {\n\t\t\tbio_list_add(&pe->origin_bios, bio);\n\t\t\tbio = NULL;\n\n\t\t\tif (!pe->started) {\n\t\t\t\tpe->started = 1;\n\t\t\t\tpe_to_start_last = pe;\n\t\t\t}\n\t\t}\n\n\t\tif (!pe->started) {\n\t\t\tpe->started = 1;\n\t\t\tpe_to_start_now = pe;\n\t\t}\n\nnext_snapshot:\n\t\tdm_exception_table_unlock(&lock);\n\t\tup_read(&snap->lock);\n\n\t\tif (pe_to_start_now) {\n\t\t\tstart_copy(pe_to_start_now);\n\t\t\tpe_to_start_now = NULL;\n\t\t}\n\t}\n\n\t \n\tif (pe_to_start_last)\n\t\tstart_copy(pe_to_start_last);\n\n\treturn r;\n}\n\n \nstatic int do_origin(struct dm_dev *origin, struct bio *bio, bool limit)\n{\n\tstruct origin *o;\n\tint r = DM_MAPIO_REMAPPED;\n\nagain:\n\tdown_read(&_origins_lock);\n\to = __lookup_origin(origin->bdev);\n\tif (o) {\n\t\tif (limit) {\n\t\t\tstruct dm_snapshot *s;\n\n\t\t\tlist_for_each_entry(s, &o->snapshots, list)\n\t\t\t\tif (unlikely(!wait_for_in_progress(s, true)))\n\t\t\t\t\tgoto again;\n\t\t}\n\n\t\tr = __origin_write(&o->snapshots, bio->bi_iter.bi_sector, bio);\n\t}\n\tup_read(&_origins_lock);\n\n\treturn r;\n}\n\n \nstatic int origin_write_extent(struct dm_snapshot *merging_snap,\n\t\t\t       sector_t sector, unsigned int size)\n{\n\tint must_wait = 0;\n\tsector_t n;\n\tstruct origin *o;\n\n\t \n\tdown_read(&_origins_lock);\n\to = __lookup_origin(merging_snap->origin->bdev);\n\tfor (n = 0; n < size; n += merging_snap->ti->max_io_len)\n\t\tif (__origin_write(&o->snapshots, sector + n, NULL) ==\n\t\t    DM_MAPIO_SUBMITTED)\n\t\t\tmust_wait = 1;\n\tup_read(&_origins_lock);\n\n\treturn must_wait;\n}\n\n \n\n \nstatic int origin_ctr(struct dm_target *ti, unsigned int argc, char **argv)\n{\n\tint r;\n\tstruct dm_origin *o;\n\n\tif (argc != 1) {\n\t\tti->error = \"origin: incorrect number of arguments\";\n\t\treturn -EINVAL;\n\t}\n\n\to = kmalloc(sizeof(struct dm_origin), GFP_KERNEL);\n\tif (!o) {\n\t\tti->error = \"Cannot allocate private origin structure\";\n\t\tr = -ENOMEM;\n\t\tgoto bad_alloc;\n\t}\n\n\tr = dm_get_device(ti, argv[0], dm_table_get_mode(ti->table), &o->dev);\n\tif (r) {\n\t\tti->error = \"Cannot get target device\";\n\t\tgoto bad_open;\n\t}\n\n\to->ti = ti;\n\tti->private = o;\n\tti->num_flush_bios = 1;\n\n\treturn 0;\n\nbad_open:\n\tkfree(o);\nbad_alloc:\n\treturn r;\n}\n\nstatic void origin_dtr(struct dm_target *ti)\n{\n\tstruct dm_origin *o = ti->private;\n\n\tdm_put_device(ti, o->dev);\n\tkfree(o);\n}\n\nstatic int origin_map(struct dm_target *ti, struct bio *bio)\n{\n\tstruct dm_origin *o = ti->private;\n\tunsigned int available_sectors;\n\n\tbio_set_dev(bio, o->dev->bdev);\n\n\tif (unlikely(bio->bi_opf & REQ_PREFLUSH))\n\t\treturn DM_MAPIO_REMAPPED;\n\n\tif (bio_data_dir(bio) != WRITE)\n\t\treturn DM_MAPIO_REMAPPED;\n\n\tavailable_sectors = o->split_boundary -\n\t\t((unsigned int)bio->bi_iter.bi_sector & (o->split_boundary - 1));\n\n\tif (bio_sectors(bio) > available_sectors)\n\t\tdm_accept_partial_bio(bio, available_sectors);\n\n\t \n\treturn do_origin(o->dev, bio, true);\n}\n\n \nstatic void origin_resume(struct dm_target *ti)\n{\n\tstruct dm_origin *o = ti->private;\n\n\to->split_boundary = get_origin_minimum_chunksize(o->dev->bdev);\n\n\tdown_write(&_origins_lock);\n\t__insert_dm_origin(o);\n\tup_write(&_origins_lock);\n}\n\nstatic void origin_postsuspend(struct dm_target *ti)\n{\n\tstruct dm_origin *o = ti->private;\n\n\tdown_write(&_origins_lock);\n\t__remove_dm_origin(o);\n\tup_write(&_origins_lock);\n}\n\nstatic void origin_status(struct dm_target *ti, status_type_t type,\n\t\t\t  unsigned int status_flags, char *result, unsigned int maxlen)\n{\n\tstruct dm_origin *o = ti->private;\n\n\tswitch (type) {\n\tcase STATUSTYPE_INFO:\n\t\tresult[0] = '\\0';\n\t\tbreak;\n\n\tcase STATUSTYPE_TABLE:\n\t\tsnprintf(result, maxlen, \"%s\", o->dev->name);\n\t\tbreak;\n\tcase STATUSTYPE_IMA:\n\t\tresult[0] = '\\0';\n\t\tbreak;\n\t}\n}\n\nstatic int origin_iterate_devices(struct dm_target *ti,\n\t\t\t\t  iterate_devices_callout_fn fn, void *data)\n{\n\tstruct dm_origin *o = ti->private;\n\n\treturn fn(ti, o->dev, 0, ti->len, data);\n}\n\nstatic struct target_type origin_target = {\n\t.name    = \"snapshot-origin\",\n\t.version = {1, 9, 0},\n\t.module  = THIS_MODULE,\n\t.ctr     = origin_ctr,\n\t.dtr     = origin_dtr,\n\t.map     = origin_map,\n\t.resume  = origin_resume,\n\t.postsuspend = origin_postsuspend,\n\t.status  = origin_status,\n\t.iterate_devices = origin_iterate_devices,\n};\n\nstatic struct target_type snapshot_target = {\n\t.name    = \"snapshot\",\n\t.version = {1, 16, 0},\n\t.module  = THIS_MODULE,\n\t.ctr     = snapshot_ctr,\n\t.dtr     = snapshot_dtr,\n\t.map     = snapshot_map,\n\t.end_io  = snapshot_end_io,\n\t.preresume  = snapshot_preresume,\n\t.resume  = snapshot_resume,\n\t.status  = snapshot_status,\n\t.iterate_devices = snapshot_iterate_devices,\n\t.io_hints = snapshot_io_hints,\n};\n\nstatic struct target_type merge_target = {\n\t.name    = dm_snapshot_merge_target_name,\n\t.version = {1, 5, 0},\n\t.module  = THIS_MODULE,\n\t.ctr     = snapshot_ctr,\n\t.dtr     = snapshot_dtr,\n\t.map     = snapshot_merge_map,\n\t.end_io  = snapshot_end_io,\n\t.presuspend = snapshot_merge_presuspend,\n\t.preresume  = snapshot_preresume,\n\t.resume  = snapshot_merge_resume,\n\t.status  = snapshot_status,\n\t.iterate_devices = snapshot_iterate_devices,\n\t.io_hints = snapshot_io_hints,\n};\n\nstatic int __init dm_snapshot_init(void)\n{\n\tint r;\n\n\tr = dm_exception_store_init();\n\tif (r) {\n\t\tDMERR(\"Failed to initialize exception stores\");\n\t\treturn r;\n\t}\n\n\tr = init_origin_hash();\n\tif (r) {\n\t\tDMERR(\"init_origin_hash failed.\");\n\t\tgoto bad_origin_hash;\n\t}\n\n\texception_cache = KMEM_CACHE(dm_exception, 0);\n\tif (!exception_cache) {\n\t\tDMERR(\"Couldn't create exception cache.\");\n\t\tr = -ENOMEM;\n\t\tgoto bad_exception_cache;\n\t}\n\n\tpending_cache = KMEM_CACHE(dm_snap_pending_exception, 0);\n\tif (!pending_cache) {\n\t\tDMERR(\"Couldn't create pending cache.\");\n\t\tr = -ENOMEM;\n\t\tgoto bad_pending_cache;\n\t}\n\n\tr = dm_register_target(&snapshot_target);\n\tif (r < 0)\n\t\tgoto bad_register_snapshot_target;\n\n\tr = dm_register_target(&origin_target);\n\tif (r < 0)\n\t\tgoto bad_register_origin_target;\n\n\tr = dm_register_target(&merge_target);\n\tif (r < 0)\n\t\tgoto bad_register_merge_target;\n\n\treturn 0;\n\nbad_register_merge_target:\n\tdm_unregister_target(&origin_target);\nbad_register_origin_target:\n\tdm_unregister_target(&snapshot_target);\nbad_register_snapshot_target:\n\tkmem_cache_destroy(pending_cache);\nbad_pending_cache:\n\tkmem_cache_destroy(exception_cache);\nbad_exception_cache:\n\texit_origin_hash();\nbad_origin_hash:\n\tdm_exception_store_exit();\n\n\treturn r;\n}\n\nstatic void __exit dm_snapshot_exit(void)\n{\n\tdm_unregister_target(&snapshot_target);\n\tdm_unregister_target(&origin_target);\n\tdm_unregister_target(&merge_target);\n\n\texit_origin_hash();\n\tkmem_cache_destroy(pending_cache);\n\tkmem_cache_destroy(exception_cache);\n\n\tdm_exception_store_exit();\n}\n\n \nmodule_init(dm_snapshot_init);\nmodule_exit(dm_snapshot_exit);\n\nMODULE_DESCRIPTION(DM_NAME \" snapshot target\");\nMODULE_AUTHOR(\"Joe Thornber\");\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS(\"dm-snapshot-origin\");\nMODULE_ALIAS(\"dm-snapshot-merge\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}