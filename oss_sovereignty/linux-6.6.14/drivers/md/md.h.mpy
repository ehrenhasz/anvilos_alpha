{
  "module_name": "md.h",
  "hash_id": "4e22f44b158c654b6ddf3087cfdc2f44a5f3eb5a166a25f474641c5aeb9e596e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/md.h",
  "human_readable_source": " \n \n\n#ifndef _MD_MD_H\n#define _MD_MD_H\n\n#include <linux/blkdev.h>\n#include <linux/backing-dev.h>\n#include <linux/badblocks.h>\n#include <linux/kobject.h>\n#include <linux/list.h>\n#include <linux/mm.h>\n#include <linux/mutex.h>\n#include <linux/timer.h>\n#include <linux/wait.h>\n#include <linux/workqueue.h>\n#include \"md-cluster.h\"\n\n#define MaxSector (~(sector_t)0)\n\n \n#define\tMD_FAILFAST\t(REQ_FAILFAST_DEV | REQ_FAILFAST_TRANSPORT)\n\n \nstruct serial_in_rdev {\n\tstruct rb_root_cached serial_rb;\n\tspinlock_t serial_lock;\n\twait_queue_head_t serial_io_wait;\n};\n\n \nstruct md_rdev {\n\tstruct list_head same_set;\t \n\n\tsector_t sectors;\t\t \n\tstruct mddev *mddev;\t\t \n\tint last_events;\t\t \n\n\t \n\tstruct block_device *meta_bdev;\n\tstruct block_device *bdev;\t \n\n\tstruct page\t*sb_page, *bb_page;\n\tint\t\tsb_loaded;\n\t__u64\t\tsb_events;\n\tsector_t\tdata_offset;\t \n\tsector_t\tnew_data_offset; \n\tsector_t\tsb_start;\t \n\tint\t\tsb_size;\t \n\tint\t\tpreferred_minor;\t \n\n\tstruct kobject\tkobj;\n\n\t \n\n\tunsigned long\tflags;\t \n\twait_queue_head_t blocked_wait;\n\n\tint desc_nr;\t\t\t \n\tint raid_disk;\t\t\t \n\tint new_raid_disk;\t\t \n\tint saved_raid_disk;\t\t \n\tunion {\n\t\tsector_t recovery_offset; \n\t\tsector_t journal_tail;\t \n\t};\n\n\tatomic_t\tnr_pending;\t \n\tatomic_t\tread_errors;\t \n\ttime64_t\tlast_read_error;\t \n\tatomic_t\tcorrected_errors;  \n\n\tstruct serial_in_rdev *serial;   \n\n\tstruct kernfs_node *sysfs_state;  \n\t \n\tstruct kernfs_node *sysfs_unack_badblocks;\n\t \n\tstruct kernfs_node *sysfs_badblocks;\n\tstruct badblocks badblocks;\n\n\tstruct {\n\t\tshort offset;\t \n\t\tunsigned int size;\t \n\t\tsector_t sector;\t \n\t} ppl;\n};\nenum flag_bits {\n\tFaulty,\t\t\t \n\tIn_sync,\t\t \n\tBitmap_sync,\t\t \n\tWriteMostly,\t\t \n\tAutoDetected,\t\t \n\tBlocked,\t\t \n\tWriteErrorSeen,\t\t \n\tFaultRecorded,\t\t \n\tBlockedBadBlocks,\t \n\tWantReplacement,\t \n\tReplacement,\t\t \n\tCandidate,\t\t \n\tJournal,\t\t \n\tClusterRemove,\n\tRemoveSynchronized,\t \n\tExternalBbl,             \n\tFailFast,\t\t \n\tLastDev,\t\t \n\tCollisionCheck,\t\t \n\tHolder,\t\t\t \n};\n\nstatic inline int is_badblock(struct md_rdev *rdev, sector_t s, int sectors,\n\t\t\t      sector_t *first_bad, int *bad_sectors)\n{\n\tif (unlikely(rdev->badblocks.count)) {\n\t\tint rv = badblocks_check(&rdev->badblocks, rdev->data_offset + s,\n\t\t\t\t\tsectors,\n\t\t\t\t\tfirst_bad, bad_sectors);\n\t\tif (rv)\n\t\t\t*first_bad -= rdev->data_offset;\n\t\treturn rv;\n\t}\n\treturn 0;\n}\nextern int rdev_set_badblocks(struct md_rdev *rdev, sector_t s, int sectors,\n\t\t\t      int is_new);\nextern int rdev_clear_badblocks(struct md_rdev *rdev, sector_t s, int sectors,\n\t\t\t\tint is_new);\nstruct md_cluster_info;\n\n \nenum mddev_flags {\n\tMD_ARRAY_FIRST_USE,\n\tMD_CLOSING,\n\tMD_JOURNAL_CLEAN,\n\tMD_HAS_JOURNAL,\n\tMD_CLUSTER_RESYNC_LOCKED,\n\tMD_FAILFAST_SUPPORTED,\n\tMD_HAS_PPL,\n\tMD_HAS_MULTIPLE_PPLS,\n\tMD_ALLOW_SB_UPDATE,\n\tMD_UPDATING_SB,\n\tMD_NOT_READY,\n\tMD_BROKEN,\n\tMD_DELETED,\n};\n\nenum mddev_sb_flags {\n\tMD_SB_CHANGE_DEVS,\t\t \n\tMD_SB_CHANGE_CLEAN,\t \n\tMD_SB_CHANGE_PENDING,\t \n\tMD_SB_NEED_REWRITE,\t \n};\n\n#define NR_SERIAL_INFOS\t\t8\n \nstruct serial_info {\n\tstruct rb_node node;\n\tsector_t start;\t\t \n\tsector_t last;\t\t \n\tsector_t _subtree_last;  \n};\n\n \nenum {\n\t \n\tMD_RESYNC_NONE = 0,\n\t \n\tMD_RESYNC_YIELDED = 1,\n\t \n\tMD_RESYNC_DELAYED = 2,\n\t \n\tMD_RESYNC_ACTIVE = 3,\n};\n\nstruct mddev {\n\tvoid\t\t\t\t*private;\n\tstruct md_personality\t\t*pers;\n\tdev_t\t\t\t\tunit;\n\tint\t\t\t\tmd_minor;\n\tstruct list_head\t\tdisks;\n\tunsigned long\t\t\tflags;\n\tunsigned long\t\t\tsb_flags;\n\n\tint\t\t\t\tsuspended;\n\tstruct percpu_ref\t\tactive_io;\n\tint\t\t\t\tro;\n\tint\t\t\t\tsysfs_active;  \n\tstruct gendisk\t\t\t*gendisk;\n\n\tstruct kobject\t\t\tkobj;\n\tint\t\t\t\thold_active;\n#define\tUNTIL_IOCTL\t1\n#define\tUNTIL_STOP\t2\n\n\t \n\tint\t\t\t\tmajor_version,\n\t\t\t\t\tminor_version,\n\t\t\t\t\tpatch_version;\n\tint\t\t\t\tpersistent;\n\tint\t\t\t\texternal;\t \n\tchar\t\t\t\tmetadata_type[17];  \n\tint\t\t\t\tchunk_sectors;\n\ttime64_t\t\t\tctime, utime;\n\tint\t\t\t\tlevel, layout;\n\tchar\t\t\t\tclevel[16];\n\tint\t\t\t\traid_disks;\n\tint\t\t\t\tmax_disks;\n\tsector_t\t\t\tdev_sectors;\t \n\tsector_t\t\t\tarray_sectors;  \n\tint\t\t\t\texternal_size;  \n\t__u64\t\t\t\tevents;\n\t \n\tint\t\t\t\tcan_decrease_events;\n\n\tchar\t\t\t\tuuid[16];\n\n\t \n\tsector_t\t\t\treshape_position;\n\tint\t\t\t\tdelta_disks, new_level, new_layout;\n\tint\t\t\t\tnew_chunk_sectors;\n\tint\t\t\t\treshape_backwards;\n\n\tstruct md_thread __rcu\t\t*thread;\t \n\tstruct md_thread __rcu\t\t*sync_thread;\t \n\n\t \n\tchar\t\t\t\t*last_sync_action;\n\tsector_t\t\t\tcurr_resync;\t \n\t \n\tsector_t\t\t\tcurr_resync_completed;\n\tunsigned long\t\t\tresync_mark;\t \n\tsector_t\t\t\tresync_mark_cnt; \n\tsector_t\t\t\tcurr_mark_cnt;  \n\n\tsector_t\t\t\tresync_max_sectors;  \n\n\tatomic64_t\t\t\tresync_mismatches;  \n\n\t \n\tsector_t\t\t\tsuspend_lo;\n\tsector_t\t\t\tsuspend_hi;\n\t \n\tint\t\t\t\tsync_speed_min;\n\tint\t\t\t\tsync_speed_max;\n\n\t \n\tint\t\t\t\tparallel_resync;\n\n\tint\t\t\t\tok_start_degraded;\n\n\tunsigned long\t\t\trecovery;\n\t \n\tint\t\t\t\trecovery_disabled;\n\n\tint\t\t\t\tin_sync;\t \n\t \n\tstruct mutex\t\t\topen_mutex;\n\tstruct mutex\t\t\treconfig_mutex;\n\tatomic_t\t\t\tactive;\t\t \n\tatomic_t\t\t\topeners;\t \n\n\tint\t\t\t\tchanged;\t \n\tint\t\t\t\tdegraded;\t \n\n\tatomic_t\t\t\trecovery_active;  \n\twait_queue_head_t\t\trecovery_wait;\n\tsector_t\t\t\trecovery_cp;\n\tsector_t\t\t\tresync_min;\t \n\tsector_t\t\t\tresync_max;\t \n\n\tstruct kernfs_node\t\t*sysfs_state;\t \n\tstruct kernfs_node\t\t*sysfs_action;   \n\tstruct kernfs_node\t\t*sysfs_completed;\t \n\tstruct kernfs_node\t\t*sysfs_degraded;\t \n\tstruct kernfs_node\t\t*sysfs_level;\t\t \n\n\tstruct work_struct del_work;\t \n\n\t \n\tspinlock_t\t\t\tlock;\n\twait_queue_head_t\t\tsb_wait;\t \n\tatomic_t\t\t\tpending_writes;\t \n\n\tunsigned int\t\t\tsafemode;\t \n\tunsigned int\t\t\tsafemode_delay;\n\tstruct timer_list\t\tsafemode_timer;\n\tstruct percpu_ref\t\twrites_pending;\n\tint\t\t\t\tsync_checkers;\t \n\tstruct request_queue\t\t*queue;\t \n\n\tstruct bitmap\t\t\t*bitmap;  \n\tstruct {\n\t\tstruct file\t\t*file;  \n\t\tloff_t\t\t\toffset;  \n\t\tunsigned long\t\tspace;  \n\t\tloff_t\t\t\tdefault_offset;  \n\t\tunsigned long\t\tdefault_space;  \n\t\tstruct mutex\t\tmutex;\n\t\tunsigned long\t\tchunksize;\n\t\tunsigned long\t\tdaemon_sleep;  \n\t\tunsigned long\t\tmax_write_behind;  \n\t\tint\t\t\texternal;\n\t\tint\t\t\tnodes;  \n\t\tchar                    cluster_name[64];  \n\t} bitmap_info;\n\n\tatomic_t\t\t\tmax_corr_read_errors;  \n\tstruct list_head\t\tall_mddevs;\n\n\tconst struct attribute_group\t*to_remove;\n\n\tstruct bio_set\t\t\tbio_set;\n\tstruct bio_set\t\t\tsync_set;  \n\tstruct bio_set\t\t\tio_clone_set;\n\n\t \n\tstruct bio *flush_bio;\n\tatomic_t flush_pending;\n\tktime_t start_flush, prev_flush_start;  \n\tstruct work_struct flush_work;\n\tstruct work_struct event_work;\t \n\tmempool_t *serial_info_pool;\n\tvoid (*sync_super)(struct mddev *mddev, struct md_rdev *rdev);\n\tstruct md_cluster_info\t\t*cluster_info;\n\tunsigned int\t\t\tgood_device_nr;\t \n\tunsigned int\t\t\tnoio_flag;  \n\n\t \n\tstruct list_head\t\tdeleting;\n\n\t \n\tstruct mutex\t\t\tsync_mutex;\n\t \n\tatomic_t sync_seq;\n\n\tbool\thas_superblocks:1;\n\tbool\tfail_last_dev:1;\n\tbool\tserialize_policy:1;\n};\n\nenum recovery_flags {\n\t \n\tMD_RECOVERY_RUNNING,\t \n\tMD_RECOVERY_SYNC,\t \n\tMD_RECOVERY_RECOVER,\t \n\tMD_RECOVERY_INTR,\t \n\tMD_RECOVERY_DONE,\t \n\tMD_RECOVERY_NEEDED,\t \n\tMD_RECOVERY_REQUESTED,\t \n\tMD_RECOVERY_CHECK,\t \n\tMD_RECOVERY_RESHAPE,\t \n\tMD_RECOVERY_FROZEN,\t \n\tMD_RECOVERY_ERROR,\t \n\tMD_RECOVERY_WAIT,\t \n\tMD_RESYNCING_REMOTE,\t \n};\n\nenum md_ro_state {\n\tMD_RDWR,\n\tMD_RDONLY,\n\tMD_AUTO_READ,\n\tMD_MAX_STATE\n};\n\nstatic inline bool md_is_rdwr(struct mddev *mddev)\n{\n\treturn (mddev->ro == MD_RDWR);\n}\n\nstatic inline bool is_md_suspended(struct mddev *mddev)\n{\n\treturn percpu_ref_is_dying(&mddev->active_io);\n}\n\nstatic inline int __must_check mddev_lock(struct mddev *mddev)\n{\n\treturn mutex_lock_interruptible(&mddev->reconfig_mutex);\n}\n\n \nstatic inline void mddev_lock_nointr(struct mddev *mddev)\n{\n\tmutex_lock(&mddev->reconfig_mutex);\n}\n\nstatic inline int mddev_trylock(struct mddev *mddev)\n{\n\treturn mutex_trylock(&mddev->reconfig_mutex);\n}\nextern void mddev_unlock(struct mddev *mddev);\n\nstatic inline void md_sync_acct(struct block_device *bdev, unsigned long nr_sectors)\n{\n\tatomic_add(nr_sectors, &bdev->bd_disk->sync_io);\n}\n\nstatic inline void md_sync_acct_bio(struct bio *bio, unsigned long nr_sectors)\n{\n\tmd_sync_acct(bio->bi_bdev, nr_sectors);\n}\n\nstruct md_personality\n{\n\tchar *name;\n\tint level;\n\tstruct list_head list;\n\tstruct module *owner;\n\tbool __must_check (*make_request)(struct mddev *mddev, struct bio *bio);\n\t \n\tint (*run)(struct mddev *mddev);\n\t \n\tint (*start)(struct mddev *mddev);\n\tvoid (*free)(struct mddev *mddev, void *priv);\n\tvoid (*status)(struct seq_file *seq, struct mddev *mddev);\n\t \n\tvoid (*error_handler)(struct mddev *mddev, struct md_rdev *rdev);\n\tint (*hot_add_disk) (struct mddev *mddev, struct md_rdev *rdev);\n\tint (*hot_remove_disk) (struct mddev *mddev, struct md_rdev *rdev);\n\tint (*spare_active) (struct mddev *mddev);\n\tsector_t (*sync_request)(struct mddev *mddev, sector_t sector_nr, int *skipped);\n\tint (*resize) (struct mddev *mddev, sector_t sectors);\n\tsector_t (*size) (struct mddev *mddev, sector_t sectors, int raid_disks);\n\tint (*check_reshape) (struct mddev *mddev);\n\tint (*start_reshape) (struct mddev *mddev);\n\tvoid (*finish_reshape) (struct mddev *mddev);\n\tvoid (*update_reshape_pos) (struct mddev *mddev);\n\tvoid (*prepare_suspend) (struct mddev *mddev);\n\t \n\tvoid (*quiesce) (struct mddev *mddev, int quiesce);\n\t \n\tvoid *(*takeover) (struct mddev *mddev);\n\t \n\tint (*change_consistency_policy)(struct mddev *mddev, const char *buf);\n};\n\nstruct md_sysfs_entry {\n\tstruct attribute attr;\n\tssize_t (*show)(struct mddev *, char *);\n\tssize_t (*store)(struct mddev *, const char *, size_t);\n};\nextern const struct attribute_group md_bitmap_group;\n\nstatic inline struct kernfs_node *sysfs_get_dirent_safe(struct kernfs_node *sd, char *name)\n{\n\tif (sd)\n\t\treturn sysfs_get_dirent(sd, name);\n\treturn sd;\n}\nstatic inline void sysfs_notify_dirent_safe(struct kernfs_node *sd)\n{\n\tif (sd)\n\t\tsysfs_notify_dirent(sd);\n}\n\nstatic inline char * mdname (struct mddev * mddev)\n{\n\treturn mddev->gendisk ? mddev->gendisk->disk_name : \"mdX\";\n}\n\nstatic inline int sysfs_link_rdev(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tchar nm[20];\n\tif (!test_bit(Replacement, &rdev->flags) &&\n\t    !test_bit(Journal, &rdev->flags) &&\n\t    mddev->kobj.sd) {\n\t\tsprintf(nm, \"rd%d\", rdev->raid_disk);\n\t\treturn sysfs_create_link(&mddev->kobj, &rdev->kobj, nm);\n\t} else\n\t\treturn 0;\n}\n\nstatic inline void sysfs_unlink_rdev(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tchar nm[20];\n\tif (!test_bit(Replacement, &rdev->flags) &&\n\t    !test_bit(Journal, &rdev->flags) &&\n\t    mddev->kobj.sd) {\n\t\tsprintf(nm, \"rd%d\", rdev->raid_disk);\n\t\tsysfs_remove_link(&mddev->kobj, nm);\n\t}\n}\n\n \n#define rdev_for_each_list(rdev, tmp, head)\t\t\t\t\\\n\tlist_for_each_entry_safe(rdev, tmp, head, same_set)\n\n \n#define rdev_for_each(rdev, mddev)\t\t\t\t\\\n\tlist_for_each_entry(rdev, &((mddev)->disks), same_set)\n\n#define rdev_for_each_safe(rdev, tmp, mddev)\t\t\t\t\\\n\tlist_for_each_entry_safe(rdev, tmp, &((mddev)->disks), same_set)\n\n#define rdev_for_each_rcu(rdev, mddev)\t\t\t\t\\\n\tlist_for_each_entry_rcu(rdev, &((mddev)->disks), same_set)\n\nstruct md_thread {\n\tvoid\t\t\t(*run) (struct md_thread *thread);\n\tstruct mddev\t\t*mddev;\n\twait_queue_head_t\twqueue;\n\tunsigned long\t\tflags;\n\tstruct task_struct\t*tsk;\n\tunsigned long\t\ttimeout;\n\tvoid\t\t\t*private;\n};\n\nstruct md_io_clone {\n\tstruct mddev\t*mddev;\n\tstruct bio\t*orig_bio;\n\tunsigned long\tstart_time;\n\tstruct bio\tbio_clone;\n};\n\n#define THREAD_WAKEUP  0\n\nstatic inline void safe_put_page(struct page *p)\n{\n\tif (p) put_page(p);\n}\n\nextern int register_md_personality(struct md_personality *p);\nextern int unregister_md_personality(struct md_personality *p);\nextern int register_md_cluster_operations(struct md_cluster_operations *ops,\n\t\tstruct module *module);\nextern int unregister_md_cluster_operations(void);\nextern int md_setup_cluster(struct mddev *mddev, int nodes);\nextern void md_cluster_stop(struct mddev *mddev);\nextern struct md_thread *md_register_thread(\n\tvoid (*run)(struct md_thread *thread),\n\tstruct mddev *mddev,\n\tconst char *name);\nextern void md_unregister_thread(struct mddev *mddev, struct md_thread __rcu **threadp);\nextern void md_wakeup_thread(struct md_thread __rcu *thread);\nextern void md_check_recovery(struct mddev *mddev);\nextern void md_reap_sync_thread(struct mddev *mddev);\nextern int mddev_init_writes_pending(struct mddev *mddev);\nextern bool md_write_start(struct mddev *mddev, struct bio *bi);\nextern void md_write_inc(struct mddev *mddev, struct bio *bi);\nextern void md_write_end(struct mddev *mddev);\nextern void md_done_sync(struct mddev *mddev, int blocks, int ok);\nextern void md_error(struct mddev *mddev, struct md_rdev *rdev);\nextern void md_finish_reshape(struct mddev *mddev);\nvoid md_submit_discard_bio(struct mddev *mddev, struct md_rdev *rdev,\n\t\t\tstruct bio *bio, sector_t start, sector_t size);\nvoid md_account_bio(struct mddev *mddev, struct bio **bio);\n\nextern bool __must_check md_flush_request(struct mddev *mddev, struct bio *bio);\nextern void md_super_write(struct mddev *mddev, struct md_rdev *rdev,\n\t\t\t   sector_t sector, int size, struct page *page);\nextern int md_super_wait(struct mddev *mddev);\nextern int sync_page_io(struct md_rdev *rdev, sector_t sector, int size,\n\t\tstruct page *page, blk_opf_t opf, bool metadata_op);\nextern void md_do_sync(struct md_thread *thread);\nextern void md_new_event(void);\nextern void md_allow_write(struct mddev *mddev);\nextern void md_wait_for_blocked_rdev(struct md_rdev *rdev, struct mddev *mddev);\nextern void md_set_array_sectors(struct mddev *mddev, sector_t array_sectors);\nextern int md_check_no_bitmap(struct mddev *mddev);\nextern int md_integrity_register(struct mddev *mddev);\nextern int md_integrity_add_rdev(struct md_rdev *rdev, struct mddev *mddev);\nextern int strict_strtoul_scaled(const char *cp, unsigned long *res, int scale);\n\nextern void mddev_init(struct mddev *mddev);\nstruct mddev *md_alloc(dev_t dev, char *name);\nvoid mddev_put(struct mddev *mddev);\nextern int md_run(struct mddev *mddev);\nextern int md_start(struct mddev *mddev);\nextern void md_stop(struct mddev *mddev);\nextern void md_stop_writes(struct mddev *mddev);\nextern int md_rdev_init(struct md_rdev *rdev);\nextern void md_rdev_clear(struct md_rdev *rdev);\n\nextern void md_handle_request(struct mddev *mddev, struct bio *bio);\nextern void mddev_suspend(struct mddev *mddev);\nextern void mddev_resume(struct mddev *mddev);\n\nextern void md_reload_sb(struct mddev *mddev, int raid_disk);\nextern void md_update_sb(struct mddev *mddev, int force);\nextern void mddev_create_serial_pool(struct mddev *mddev, struct md_rdev *rdev,\n\t\t\t\t     bool is_suspend);\nextern void mddev_destroy_serial_pool(struct mddev *mddev, struct md_rdev *rdev,\n\t\t\t\t      bool is_suspend);\nstruct md_rdev *md_find_rdev_nr_rcu(struct mddev *mddev, int nr);\nstruct md_rdev *md_find_rdev_rcu(struct mddev *mddev, dev_t dev);\n\nstatic inline bool is_rdev_broken(struct md_rdev *rdev)\n{\n\treturn !disk_live(rdev->bdev->bd_disk);\n}\n\nstatic inline void rdev_dec_pending(struct md_rdev *rdev, struct mddev *mddev)\n{\n\tint faulty = test_bit(Faulty, &rdev->flags);\n\tif (atomic_dec_and_test(&rdev->nr_pending) && faulty) {\n\t\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\t\tmd_wakeup_thread(mddev->thread);\n\t}\n}\n\nextern struct md_cluster_operations *md_cluster_ops;\nstatic inline int mddev_is_clustered(struct mddev *mddev)\n{\n\treturn mddev->cluster_info && mddev->bitmap_info.nodes > 1;\n}\n\n \nstatic inline void mddev_clear_unsupported_flags(struct mddev *mddev,\n\tunsigned long unsupported_flags)\n{\n\tmddev->flags &= ~unsupported_flags;\n}\n\nstatic inline void mddev_check_write_zeroes(struct mddev *mddev, struct bio *bio)\n{\n\tif (bio_op(bio) == REQ_OP_WRITE_ZEROES &&\n\t    !bio->bi_bdev->bd_disk->queue->limits.max_write_zeroes_sectors)\n\t\tmddev->queue->limits.max_write_zeroes_sectors = 0;\n}\n\nstruct mdu_array_info_s;\nstruct mdu_disk_info_s;\n\nextern int mdp_major;\nextern struct workqueue_struct *md_bitmap_wq;\nvoid md_autostart_arrays(int part);\nint md_set_array_info(struct mddev *mddev, struct mdu_array_info_s *info);\nint md_add_new_disk(struct mddev *mddev, struct mdu_disk_info_s *info);\nint do_md_run(struct mddev *mddev);\n\nextern const struct block_device_operations md_fops;\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}