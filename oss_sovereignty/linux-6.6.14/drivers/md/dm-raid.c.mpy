{
  "module_name": "dm-raid.c",
  "hash_id": "7e904f32c6ba93b20d45f0e343872bb7f1a45d21c0fa9543f06e0b30d3cfc952",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-raid.c",
  "human_readable_source": "\n \n\n#include <linux/slab.h>\n#include <linux/module.h>\n\n#include \"md.h\"\n#include \"raid1.h\"\n#include \"raid5.h\"\n#include \"raid10.h\"\n#include \"md-bitmap.h\"\n\n#include <linux/device-mapper.h>\n\n#define DM_MSG_PREFIX \"raid\"\n#define\tMAX_RAID_DEVICES\t253  \n\n \n#define\tMIN_FREE_RESHAPE_SPACE to_sector(4*4096)\n\n \n#define\tMIN_RAID456_JOURNAL_SPACE (4*2048)\n\nstatic bool devices_handle_discard_safely;\n\n \n#define FirstUse 10\t\t \n\nstruct raid_dev {\n\t \n\tstruct dm_dev *meta_dev;\n\tstruct dm_dev *data_dev;\n\tstruct md_rdev rdev;\n};\n\n \n#define __CTR_FLAG_SYNC\t\t\t0     \n#define __CTR_FLAG_NOSYNC\t\t1     \n#define __CTR_FLAG_REBUILD\t\t2     \n#define __CTR_FLAG_DAEMON_SLEEP\t\t3     \n#define __CTR_FLAG_MIN_RECOVERY_RATE\t4     \n#define __CTR_FLAG_MAX_RECOVERY_RATE\t5     \n#define __CTR_FLAG_MAX_WRITE_BEHIND\t6     \n#define __CTR_FLAG_WRITE_MOSTLY\t\t7     \n#define __CTR_FLAG_STRIPE_CACHE\t\t8     \n#define __CTR_FLAG_REGION_SIZE\t\t9     \n#define __CTR_FLAG_RAID10_COPIES\t10    \n#define __CTR_FLAG_RAID10_FORMAT\t11    \n \n#define __CTR_FLAG_DELTA_DISKS\t\t12    \n#define __CTR_FLAG_DATA_OFFSET\t\t13    \n#define __CTR_FLAG_RAID10_USE_NEAR_SETS 14    \n\n \n#define __CTR_FLAG_JOURNAL_DEV\t\t15    \n\n \n#define __CTR_FLAG_JOURNAL_MODE\t\t16    \n\n \n#define CTR_FLAG_SYNC\t\t\t(1 << __CTR_FLAG_SYNC)\n#define CTR_FLAG_NOSYNC\t\t\t(1 << __CTR_FLAG_NOSYNC)\n#define CTR_FLAG_REBUILD\t\t(1 << __CTR_FLAG_REBUILD)\n#define CTR_FLAG_DAEMON_SLEEP\t\t(1 << __CTR_FLAG_DAEMON_SLEEP)\n#define CTR_FLAG_MIN_RECOVERY_RATE\t(1 << __CTR_FLAG_MIN_RECOVERY_RATE)\n#define CTR_FLAG_MAX_RECOVERY_RATE\t(1 << __CTR_FLAG_MAX_RECOVERY_RATE)\n#define CTR_FLAG_MAX_WRITE_BEHIND\t(1 << __CTR_FLAG_MAX_WRITE_BEHIND)\n#define CTR_FLAG_WRITE_MOSTLY\t\t(1 << __CTR_FLAG_WRITE_MOSTLY)\n#define CTR_FLAG_STRIPE_CACHE\t\t(1 << __CTR_FLAG_STRIPE_CACHE)\n#define CTR_FLAG_REGION_SIZE\t\t(1 << __CTR_FLAG_REGION_SIZE)\n#define CTR_FLAG_RAID10_COPIES\t\t(1 << __CTR_FLAG_RAID10_COPIES)\n#define CTR_FLAG_RAID10_FORMAT\t\t(1 << __CTR_FLAG_RAID10_FORMAT)\n#define CTR_FLAG_DELTA_DISKS\t\t(1 << __CTR_FLAG_DELTA_DISKS)\n#define CTR_FLAG_DATA_OFFSET\t\t(1 << __CTR_FLAG_DATA_OFFSET)\n#define CTR_FLAG_RAID10_USE_NEAR_SETS\t(1 << __CTR_FLAG_RAID10_USE_NEAR_SETS)\n#define CTR_FLAG_JOURNAL_DEV\t\t(1 << __CTR_FLAG_JOURNAL_DEV)\n#define CTR_FLAG_JOURNAL_MODE\t\t(1 << __CTR_FLAG_JOURNAL_MODE)\n\n \n \n#define\tCTR_FLAGS_ANY_SYNC\t\t(CTR_FLAG_SYNC | CTR_FLAG_NOSYNC)\n\n \n#define\tCTR_FLAG_OPTIONS_NO_ARGS\t(CTR_FLAGS_ANY_SYNC | \\\n\t\t\t\t\t CTR_FLAG_RAID10_USE_NEAR_SETS)\n\n \n#define CTR_FLAG_OPTIONS_ONE_ARG (CTR_FLAG_REBUILD | \\\n\t\t\t\t  CTR_FLAG_WRITE_MOSTLY | \\\n\t\t\t\t  CTR_FLAG_DAEMON_SLEEP | \\\n\t\t\t\t  CTR_FLAG_MIN_RECOVERY_RATE | \\\n\t\t\t\t  CTR_FLAG_MAX_RECOVERY_RATE | \\\n\t\t\t\t  CTR_FLAG_MAX_WRITE_BEHIND | \\\n\t\t\t\t  CTR_FLAG_STRIPE_CACHE | \\\n\t\t\t\t  CTR_FLAG_REGION_SIZE | \\\n\t\t\t\t  CTR_FLAG_RAID10_COPIES | \\\n\t\t\t\t  CTR_FLAG_RAID10_FORMAT | \\\n\t\t\t\t  CTR_FLAG_DELTA_DISKS | \\\n\t\t\t\t  CTR_FLAG_DATA_OFFSET | \\\n\t\t\t\t  CTR_FLAG_JOURNAL_DEV | \\\n\t\t\t\t  CTR_FLAG_JOURNAL_MODE)\n\n \n\n \n#define RAID0_VALID_FLAGS\t(CTR_FLAG_DATA_OFFSET)\n\n \n#define RAID1_VALID_FLAGS\t(CTR_FLAGS_ANY_SYNC | \\\n\t\t\t\t CTR_FLAG_REBUILD | \\\n\t\t\t\t CTR_FLAG_WRITE_MOSTLY | \\\n\t\t\t\t CTR_FLAG_DAEMON_SLEEP | \\\n\t\t\t\t CTR_FLAG_MIN_RECOVERY_RATE | \\\n\t\t\t\t CTR_FLAG_MAX_RECOVERY_RATE | \\\n\t\t\t\t CTR_FLAG_MAX_WRITE_BEHIND | \\\n\t\t\t\t CTR_FLAG_REGION_SIZE | \\\n\t\t\t\t CTR_FLAG_DELTA_DISKS | \\\n\t\t\t\t CTR_FLAG_DATA_OFFSET)\n\n \n#define RAID10_VALID_FLAGS\t(CTR_FLAGS_ANY_SYNC | \\\n\t\t\t\t CTR_FLAG_REBUILD | \\\n\t\t\t\t CTR_FLAG_DAEMON_SLEEP | \\\n\t\t\t\t CTR_FLAG_MIN_RECOVERY_RATE | \\\n\t\t\t\t CTR_FLAG_MAX_RECOVERY_RATE | \\\n\t\t\t\t CTR_FLAG_REGION_SIZE | \\\n\t\t\t\t CTR_FLAG_RAID10_COPIES | \\\n\t\t\t\t CTR_FLAG_RAID10_FORMAT | \\\n\t\t\t\t CTR_FLAG_DELTA_DISKS | \\\n\t\t\t\t CTR_FLAG_DATA_OFFSET | \\\n\t\t\t\t CTR_FLAG_RAID10_USE_NEAR_SETS)\n\n \n#define RAID45_VALID_FLAGS\t(CTR_FLAGS_ANY_SYNC | \\\n\t\t\t\t CTR_FLAG_REBUILD | \\\n\t\t\t\t CTR_FLAG_DAEMON_SLEEP | \\\n\t\t\t\t CTR_FLAG_MIN_RECOVERY_RATE | \\\n\t\t\t\t CTR_FLAG_MAX_RECOVERY_RATE | \\\n\t\t\t\t CTR_FLAG_STRIPE_CACHE | \\\n\t\t\t\t CTR_FLAG_REGION_SIZE | \\\n\t\t\t\t CTR_FLAG_DELTA_DISKS | \\\n\t\t\t\t CTR_FLAG_DATA_OFFSET | \\\n\t\t\t\t CTR_FLAG_JOURNAL_DEV | \\\n\t\t\t\t CTR_FLAG_JOURNAL_MODE)\n\n#define RAID6_VALID_FLAGS\t(CTR_FLAG_SYNC | \\\n\t\t\t\t CTR_FLAG_REBUILD | \\\n\t\t\t\t CTR_FLAG_DAEMON_SLEEP | \\\n\t\t\t\t CTR_FLAG_MIN_RECOVERY_RATE | \\\n\t\t\t\t CTR_FLAG_MAX_RECOVERY_RATE | \\\n\t\t\t\t CTR_FLAG_STRIPE_CACHE | \\\n\t\t\t\t CTR_FLAG_REGION_SIZE | \\\n\t\t\t\t CTR_FLAG_DELTA_DISKS | \\\n\t\t\t\t CTR_FLAG_DATA_OFFSET | \\\n\t\t\t\t CTR_FLAG_JOURNAL_DEV | \\\n\t\t\t\t CTR_FLAG_JOURNAL_MODE)\n \n\n \n#define RT_FLAG_RS_PRERESUMED\t\t0\n#define RT_FLAG_RS_RESUMED\t\t1\n#define RT_FLAG_RS_BITMAP_LOADED\t2\n#define RT_FLAG_UPDATE_SBS\t\t3\n#define RT_FLAG_RESHAPE_RS\t\t4\n#define RT_FLAG_RS_SUSPENDED\t\t5\n#define RT_FLAG_RS_IN_SYNC\t\t6\n#define RT_FLAG_RS_RESYNCING\t\t7\n#define RT_FLAG_RS_GROW\t\t\t8\n\n \n#define DISKS_ARRAY_ELEMS ((MAX_RAID_DEVICES + (sizeof(uint64_t) * 8 - 1)) / sizeof(uint64_t) / 8)\n\n \nstruct rs_layout {\n\tint new_level;\n\tint new_layout;\n\tint new_chunk_sectors;\n};\n\nstruct raid_set {\n\tstruct dm_target *ti;\n\n\tuint32_t stripe_cache_entries;\n\tunsigned long ctr_flags;\n\tunsigned long runtime_flags;\n\n\tuint64_t rebuild_disks[DISKS_ARRAY_ELEMS];\n\n\tint raid_disks;\n\tint delta_disks;\n\tint data_offset;\n\tint raid10_copies;\n\tint requested_bitmap_chunk_sectors;\n\n\tstruct mddev md;\n\tstruct raid_type *raid_type;\n\n\tsector_t array_sectors;\n\tsector_t dev_sectors;\n\n\t \n\tstruct journal_dev {\n\t\tstruct dm_dev *dev;\n\t\tstruct md_rdev rdev;\n\t\tint mode;\n\t} journal_dev;\n\n\tstruct raid_dev dev[];\n};\n\nstatic void rs_config_backup(struct raid_set *rs, struct rs_layout *l)\n{\n\tstruct mddev *mddev = &rs->md;\n\n\tl->new_level = mddev->new_level;\n\tl->new_layout = mddev->new_layout;\n\tl->new_chunk_sectors = mddev->new_chunk_sectors;\n}\n\nstatic void rs_config_restore(struct raid_set *rs, struct rs_layout *l)\n{\n\tstruct mddev *mddev = &rs->md;\n\n\tmddev->new_level = l->new_level;\n\tmddev->new_layout = l->new_layout;\n\tmddev->new_chunk_sectors = l->new_chunk_sectors;\n}\n\n \n#define\tALGORITHM_RAID10_DEFAULT\t0\n#define\tALGORITHM_RAID10_NEAR\t\t1\n#define\tALGORITHM_RAID10_OFFSET\t\t2\n#define\tALGORITHM_RAID10_FAR\t\t3\n\n \nstatic struct raid_type {\n\tconst char *name;\t\t \n\tconst char *descr;\t\t \n\tconst unsigned int parity_devs;\t \n\tconst unsigned int minimal_devs; \n\tconst unsigned int level;\t \n\tconst unsigned int algorithm;\t \n} raid_types[] = {\n\t{\"raid0\",\t  \"raid0 (striping)\",\t\t\t    0, 2, 0,  0  },\n\t{\"raid1\",\t  \"raid1 (mirroring)\",\t\t\t    0, 2, 1,  0  },\n\t{\"raid10_far\",\t  \"raid10 far (striped mirrors)\",\t    0, 2, 10, ALGORITHM_RAID10_FAR},\n\t{\"raid10_offset\", \"raid10 offset (striped mirrors)\",\t    0, 2, 10, ALGORITHM_RAID10_OFFSET},\n\t{\"raid10_near\",\t  \"raid10 near (striped mirrors)\",\t    0, 2, 10, ALGORITHM_RAID10_NEAR},\n\t{\"raid10\",\t  \"raid10 (striped mirrors)\",\t\t    0, 2, 10, ALGORITHM_RAID10_DEFAULT},\n\t{\"raid4\",\t  \"raid4 (dedicated first parity disk)\",    1, 2, 5,  ALGORITHM_PARITY_0},  \n\t{\"raid5_n\",\t  \"raid5 (dedicated last parity disk)\",\t    1, 2, 5,  ALGORITHM_PARITY_N},\n\t{\"raid5_ls\",\t  \"raid5 (left symmetric)\",\t\t    1, 2, 5,  ALGORITHM_LEFT_SYMMETRIC},\n\t{\"raid5_rs\",\t  \"raid5 (right symmetric)\",\t\t    1, 2, 5,  ALGORITHM_RIGHT_SYMMETRIC},\n\t{\"raid5_la\",\t  \"raid5 (left asymmetric)\",\t\t    1, 2, 5,  ALGORITHM_LEFT_ASYMMETRIC},\n\t{\"raid5_ra\",\t  \"raid5 (right asymmetric)\",\t\t    1, 2, 5,  ALGORITHM_RIGHT_ASYMMETRIC},\n\t{\"raid6_zr\",\t  \"raid6 (zero restart)\",\t\t    2, 4, 6,  ALGORITHM_ROTATING_ZERO_RESTART},\n\t{\"raid6_nr\",\t  \"raid6 (N restart)\",\t\t\t    2, 4, 6,  ALGORITHM_ROTATING_N_RESTART},\n\t{\"raid6_nc\",\t  \"raid6 (N continue)\",\t\t\t    2, 4, 6,  ALGORITHM_ROTATING_N_CONTINUE},\n\t{\"raid6_n_6\",\t  \"raid6 (dedicated parity/Q n/6)\",\t    2, 4, 6,  ALGORITHM_PARITY_N_6},\n\t{\"raid6_ls_6\",\t  \"raid6 (left symmetric dedicated Q 6)\",   2, 4, 6,  ALGORITHM_LEFT_SYMMETRIC_6},\n\t{\"raid6_rs_6\",\t  \"raid6 (right symmetric dedicated Q 6)\",  2, 4, 6,  ALGORITHM_RIGHT_SYMMETRIC_6},\n\t{\"raid6_la_6\",\t  \"raid6 (left asymmetric dedicated Q 6)\",  2, 4, 6,  ALGORITHM_LEFT_ASYMMETRIC_6},\n\t{\"raid6_ra_6\",\t  \"raid6 (right asymmetric dedicated Q 6)\", 2, 4, 6,  ALGORITHM_RIGHT_ASYMMETRIC_6}\n};\n\n \nstatic bool __within_range(long v, long min, long max)\n{\n\treturn v >= min && v <= max;\n}\n\n \nstatic struct arg_name_flag {\n\tconst unsigned long flag;\n\tconst char *name;\n} __arg_name_flags[] = {\n\t{ CTR_FLAG_SYNC, \"sync\"},\n\t{ CTR_FLAG_NOSYNC, \"nosync\"},\n\t{ CTR_FLAG_REBUILD, \"rebuild\"},\n\t{ CTR_FLAG_DAEMON_SLEEP, \"daemon_sleep\"},\n\t{ CTR_FLAG_MIN_RECOVERY_RATE, \"min_recovery_rate\"},\n\t{ CTR_FLAG_MAX_RECOVERY_RATE, \"max_recovery_rate\"},\n\t{ CTR_FLAG_MAX_WRITE_BEHIND, \"max_write_behind\"},\n\t{ CTR_FLAG_WRITE_MOSTLY, \"write_mostly\"},\n\t{ CTR_FLAG_STRIPE_CACHE, \"stripe_cache\"},\n\t{ CTR_FLAG_REGION_SIZE, \"region_size\"},\n\t{ CTR_FLAG_RAID10_COPIES, \"raid10_copies\"},\n\t{ CTR_FLAG_RAID10_FORMAT, \"raid10_format\"},\n\t{ CTR_FLAG_DATA_OFFSET, \"data_offset\"},\n\t{ CTR_FLAG_DELTA_DISKS, \"delta_disks\"},\n\t{ CTR_FLAG_RAID10_USE_NEAR_SETS, \"raid10_use_near_sets\"},\n\t{ CTR_FLAG_JOURNAL_DEV, \"journal_dev\" },\n\t{ CTR_FLAG_JOURNAL_MODE, \"journal_mode\" },\n};\n\n \nstatic const char *dm_raid_arg_name_by_flag(const uint32_t flag)\n{\n\tif (hweight32(flag) == 1) {\n\t\tstruct arg_name_flag *anf = __arg_name_flags + ARRAY_SIZE(__arg_name_flags);\n\n\t\twhile (anf-- > __arg_name_flags)\n\t\t\tif (flag & anf->flag)\n\t\t\t\treturn anf->name;\n\n\t} else\n\t\tDMERR(\"%s called with more than one flag!\", __func__);\n\n\treturn NULL;\n}\n\n \nstatic struct {\n\tconst int mode;\n\tconst char *param;\n} _raid456_journal_mode[] = {\n\t{ R5C_JOURNAL_MODE_WRITE_THROUGH, \"writethrough\" },\n\t{ R5C_JOURNAL_MODE_WRITE_BACK,    \"writeback\" }\n};\n\n \nstatic int dm_raid_journal_mode_to_md(const char *mode)\n{\n\tint m = ARRAY_SIZE(_raid456_journal_mode);\n\n\twhile (m--)\n\t\tif (!strcasecmp(mode, _raid456_journal_mode[m].param))\n\t\t\treturn _raid456_journal_mode[m].mode;\n\n\treturn -EINVAL;\n}\n\n \nstatic const char *md_journal_mode_to_dm_raid(const int mode)\n{\n\tint m = ARRAY_SIZE(_raid456_journal_mode);\n\n\twhile (m--)\n\t\tif (mode == _raid456_journal_mode[m].mode)\n\t\t\treturn _raid456_journal_mode[m].param;\n\n\treturn \"unknown\";\n}\n\n \n \nstatic bool rs_is_raid0(struct raid_set *rs)\n{\n\treturn !rs->md.level;\n}\n\n \nstatic bool rs_is_raid1(struct raid_set *rs)\n{\n\treturn rs->md.level == 1;\n}\n\n \nstatic bool rs_is_raid10(struct raid_set *rs)\n{\n\treturn rs->md.level == 10;\n}\n\n \nstatic bool rs_is_raid6(struct raid_set *rs)\n{\n\treturn rs->md.level == 6;\n}\n\n \nstatic bool rs_is_raid456(struct raid_set *rs)\n{\n\treturn __within_range(rs->md.level, 4, 6);\n}\n\n \nstatic bool __is_raid10_far(int layout);\nstatic bool rs_is_reshapable(struct raid_set *rs)\n{\n\treturn rs_is_raid456(rs) ||\n\t       (rs_is_raid10(rs) && !__is_raid10_far(rs->md.new_layout));\n}\n\n \nstatic bool rs_is_recovering(struct raid_set *rs)\n{\n\treturn rs->md.recovery_cp < rs->md.dev_sectors;\n}\n\n \nstatic bool rs_is_reshaping(struct raid_set *rs)\n{\n\treturn rs->md.reshape_position != MaxSector;\n}\n\n \n\n \nstatic bool rt_is_raid0(struct raid_type *rt)\n{\n\treturn !rt->level;\n}\n\n \nstatic bool rt_is_raid1(struct raid_type *rt)\n{\n\treturn rt->level == 1;\n}\n\n \nstatic bool rt_is_raid10(struct raid_type *rt)\n{\n\treturn rt->level == 10;\n}\n\n \nstatic bool rt_is_raid45(struct raid_type *rt)\n{\n\treturn __within_range(rt->level, 4, 5);\n}\n\n \nstatic bool rt_is_raid6(struct raid_type *rt)\n{\n\treturn rt->level == 6;\n}\n\n \nstatic bool rt_is_raid456(struct raid_type *rt)\n{\n\treturn __within_range(rt->level, 4, 6);\n}\n \n\n \nstatic unsigned long __valid_flags(struct raid_set *rs)\n{\n\tif (rt_is_raid0(rs->raid_type))\n\t\treturn RAID0_VALID_FLAGS;\n\telse if (rt_is_raid1(rs->raid_type))\n\t\treturn RAID1_VALID_FLAGS;\n\telse if (rt_is_raid10(rs->raid_type))\n\t\treturn RAID10_VALID_FLAGS;\n\telse if (rt_is_raid45(rs->raid_type))\n\t\treturn RAID45_VALID_FLAGS;\n\telse if (rt_is_raid6(rs->raid_type))\n\t\treturn RAID6_VALID_FLAGS;\n\n\treturn 0;\n}\n\n \nstatic int rs_check_for_valid_flags(struct raid_set *rs)\n{\n\tif (rs->ctr_flags & ~__valid_flags(rs)) {\n\t\trs->ti->error = \"Invalid flags combination\";\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \n#define RAID10_OFFSET\t\t\t(1 << 16)  \n#define RAID10_BROCKEN_USE_FAR_SETS\t(1 << 17)  \n#define RAID10_USE_FAR_SETS\t\t(1 << 18)  \n#define RAID10_FAR_COPIES_SHIFT\t\t8\t   \n\n \nstatic unsigned int __raid10_near_copies(int layout)\n{\n\treturn layout & 0xFF;\n}\n\n \nstatic unsigned int __raid10_far_copies(int layout)\n{\n\treturn __raid10_near_copies(layout >> RAID10_FAR_COPIES_SHIFT);\n}\n\n \nstatic bool __is_raid10_offset(int layout)\n{\n\treturn !!(layout & RAID10_OFFSET);\n}\n\n \nstatic bool __is_raid10_near(int layout)\n{\n\treturn !__is_raid10_offset(layout) && __raid10_near_copies(layout) > 1;\n}\n\n \nstatic bool __is_raid10_far(int layout)\n{\n\treturn !__is_raid10_offset(layout) && __raid10_far_copies(layout) > 1;\n}\n\n \nstatic const char *raid10_md_layout_to_format(int layout)\n{\n\t \n\tif (__is_raid10_offset(layout))\n\t\treturn \"offset\";\n\n\tif (__raid10_near_copies(layout) > 1)\n\t\treturn \"near\";\n\n\tif (__raid10_far_copies(layout) > 1)\n\t\treturn \"far\";\n\n\treturn \"unknown\";\n}\n\n \nstatic int raid10_name_to_format(const char *name)\n{\n\tif (!strcasecmp(name, \"near\"))\n\t\treturn ALGORITHM_RAID10_NEAR;\n\telse if (!strcasecmp(name, \"offset\"))\n\t\treturn ALGORITHM_RAID10_OFFSET;\n\telse if (!strcasecmp(name, \"far\"))\n\t\treturn ALGORITHM_RAID10_FAR;\n\n\treturn -EINVAL;\n}\n\n \nstatic unsigned int raid10_md_layout_to_copies(int layout)\n{\n\treturn max(__raid10_near_copies(layout), __raid10_far_copies(layout));\n}\n\n \nstatic int raid10_format_to_md_layout(struct raid_set *rs,\n\t\t\t\t      unsigned int algorithm,\n\t\t\t\t      unsigned int copies)\n{\n\tunsigned int n = 1, f = 1, r = 0;\n\n\t \n\tif (algorithm == ALGORITHM_RAID10_DEFAULT ||\n\t    algorithm == ALGORITHM_RAID10_NEAR)\n\t\tn = copies;\n\n\telse if (algorithm == ALGORITHM_RAID10_OFFSET) {\n\t\tf = copies;\n\t\tr = RAID10_OFFSET;\n\t\tif (!test_bit(__CTR_FLAG_RAID10_USE_NEAR_SETS, &rs->ctr_flags))\n\t\t\tr |= RAID10_USE_FAR_SETS;\n\n\t} else if (algorithm == ALGORITHM_RAID10_FAR) {\n\t\tf = copies;\n\t\tif (!test_bit(__CTR_FLAG_RAID10_USE_NEAR_SETS, &rs->ctr_flags))\n\t\t\tr |= RAID10_USE_FAR_SETS;\n\n\t} else\n\t\treturn -EINVAL;\n\n\treturn r | (f << RAID10_FAR_COPIES_SHIFT) | n;\n}\n \n\n \nstatic bool __got_raid10(struct raid_type *rtp, const int layout)\n{\n\tif (rtp->level == 10) {\n\t\tswitch (rtp->algorithm) {\n\t\tcase ALGORITHM_RAID10_DEFAULT:\n\t\tcase ALGORITHM_RAID10_NEAR:\n\t\t\treturn __is_raid10_near(layout);\n\t\tcase ALGORITHM_RAID10_OFFSET:\n\t\t\treturn __is_raid10_offset(layout);\n\t\tcase ALGORITHM_RAID10_FAR:\n\t\t\treturn __is_raid10_far(layout);\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn false;\n}\n\n \nstatic struct raid_type *get_raid_type(const char *name)\n{\n\tstruct raid_type *rtp = raid_types + ARRAY_SIZE(raid_types);\n\n\twhile (rtp-- > raid_types)\n\t\tif (!strcasecmp(rtp->name, name))\n\t\t\treturn rtp;\n\n\treturn NULL;\n}\n\n \nstatic struct raid_type *get_raid_type_by_ll(const int level, const int layout)\n{\n\tstruct raid_type *rtp = raid_types + ARRAY_SIZE(raid_types);\n\n\twhile (rtp-- > raid_types) {\n\t\t \n\t\tif (rtp->level == level &&\n\t\t    (__got_raid10(rtp, layout) || rtp->algorithm == layout))\n\t\t\treturn rtp;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic void rs_set_rdev_sectors(struct raid_set *rs)\n{\n\tstruct mddev *mddev = &rs->md;\n\tstruct md_rdev *rdev;\n\n\t \n\trdev_for_each(rdev, mddev)\n\t\tif (!test_bit(Journal, &rdev->flags))\n\t\t\trdev->sectors = mddev->dev_sectors;\n}\n\n \nstatic void rs_set_capacity(struct raid_set *rs)\n{\n\tstruct gendisk *gendisk = dm_disk(dm_table_get_md(rs->ti->table));\n\n\tset_capacity_and_notify(gendisk, rs->md.array_sectors);\n}\n\n \nstatic void rs_set_cur(struct raid_set *rs)\n{\n\tstruct mddev *mddev = &rs->md;\n\n\tmddev->new_level = mddev->level;\n\tmddev->new_layout = mddev->layout;\n\tmddev->new_chunk_sectors = mddev->chunk_sectors;\n}\n\n \nstatic void rs_set_new(struct raid_set *rs)\n{\n\tstruct mddev *mddev = &rs->md;\n\n\tmddev->level = mddev->new_level;\n\tmddev->layout = mddev->new_layout;\n\tmddev->chunk_sectors = mddev->new_chunk_sectors;\n\tmddev->raid_disks = rs->raid_disks;\n\tmddev->delta_disks = 0;\n}\n\nstatic struct raid_set *raid_set_alloc(struct dm_target *ti, struct raid_type *raid_type,\n\t\t\t\t       unsigned int raid_devs)\n{\n\tunsigned int i;\n\tstruct raid_set *rs;\n\n\tif (raid_devs <= raid_type->parity_devs) {\n\t\tti->error = \"Insufficient number of devices\";\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\trs = kzalloc(struct_size(rs, dev, raid_devs), GFP_KERNEL);\n\tif (!rs) {\n\t\tti->error = \"Cannot allocate raid context\";\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tmddev_init(&rs->md);\n\n\trs->raid_disks = raid_devs;\n\trs->delta_disks = 0;\n\n\trs->ti = ti;\n\trs->raid_type = raid_type;\n\trs->stripe_cache_entries = 256;\n\trs->md.raid_disks = raid_devs;\n\trs->md.level = raid_type->level;\n\trs->md.new_level = rs->md.level;\n\trs->md.layout = raid_type->algorithm;\n\trs->md.new_layout = rs->md.layout;\n\trs->md.delta_disks = 0;\n\trs->md.recovery_cp = MaxSector;\n\n\tfor (i = 0; i < raid_devs; i++)\n\t\tmd_rdev_init(&rs->dev[i].rdev);\n\n\t \n\n\treturn rs;\n}\n\n \nstatic void raid_set_free(struct raid_set *rs)\n{\n\tint i;\n\n\tif (rs->journal_dev.dev) {\n\t\tmd_rdev_clear(&rs->journal_dev.rdev);\n\t\tdm_put_device(rs->ti, rs->journal_dev.dev);\n\t}\n\n\tfor (i = 0; i < rs->raid_disks; i++) {\n\t\tif (rs->dev[i].meta_dev)\n\t\t\tdm_put_device(rs->ti, rs->dev[i].meta_dev);\n\t\tmd_rdev_clear(&rs->dev[i].rdev);\n\t\tif (rs->dev[i].data_dev)\n\t\t\tdm_put_device(rs->ti, rs->dev[i].data_dev);\n\t}\n\n\tkfree(rs);\n}\n\n \nstatic int parse_dev_params(struct raid_set *rs, struct dm_arg_set *as)\n{\n\tint i;\n\tint rebuild = 0;\n\tint metadata_available = 0;\n\tint r = 0;\n\tconst char *arg;\n\n\t \n\targ = dm_shift_arg(as);\n\tif (!arg)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < rs->raid_disks; i++) {\n\t\trs->dev[i].rdev.raid_disk = i;\n\n\t\trs->dev[i].meta_dev = NULL;\n\t\trs->dev[i].data_dev = NULL;\n\n\t\t \n\t\trs->dev[i].rdev.data_offset = 0;\n\t\trs->dev[i].rdev.new_data_offset = 0;\n\t\trs->dev[i].rdev.mddev = &rs->md;\n\n\t\targ = dm_shift_arg(as);\n\t\tif (!arg)\n\t\t\treturn -EINVAL;\n\n\t\tif (strcmp(arg, \"-\")) {\n\t\t\tr = dm_get_device(rs->ti, arg, dm_table_get_mode(rs->ti->table),\n\t\t\t\t\t  &rs->dev[i].meta_dev);\n\t\t\tif (r) {\n\t\t\t\trs->ti->error = \"RAID metadata device lookup failure\";\n\t\t\t\treturn r;\n\t\t\t}\n\n\t\t\trs->dev[i].rdev.sb_page = alloc_page(GFP_KERNEL);\n\t\t\tif (!rs->dev[i].rdev.sb_page) {\n\t\t\t\trs->ti->error = \"Failed to allocate superblock page\";\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\targ = dm_shift_arg(as);\n\t\tif (!arg)\n\t\t\treturn -EINVAL;\n\n\t\tif (!strcmp(arg, \"-\")) {\n\t\t\tif (!test_bit(In_sync, &rs->dev[i].rdev.flags) &&\n\t\t\t    (!rs->dev[i].rdev.recovery_offset)) {\n\t\t\t\trs->ti->error = \"Drive designated for rebuild not specified\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (rs->dev[i].meta_dev) {\n\t\t\t\trs->ti->error = \"No data device supplied with metadata device\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tr = dm_get_device(rs->ti, arg, dm_table_get_mode(rs->ti->table),\n\t\t\t\t  &rs->dev[i].data_dev);\n\t\tif (r) {\n\t\t\trs->ti->error = \"RAID device lookup failure\";\n\t\t\treturn r;\n\t\t}\n\n\t\tif (rs->dev[i].meta_dev) {\n\t\t\tmetadata_available = 1;\n\t\t\trs->dev[i].rdev.meta_bdev = rs->dev[i].meta_dev->bdev;\n\t\t}\n\t\trs->dev[i].rdev.bdev = rs->dev[i].data_dev->bdev;\n\t\tlist_add_tail(&rs->dev[i].rdev.same_set, &rs->md.disks);\n\t\tif (!test_bit(In_sync, &rs->dev[i].rdev.flags))\n\t\t\trebuild++;\n\t}\n\n\tif (rs->journal_dev.dev)\n\t\tlist_add_tail(&rs->journal_dev.rdev.same_set, &rs->md.disks);\n\n\tif (metadata_available) {\n\t\trs->md.external = 0;\n\t\trs->md.persistent = 1;\n\t\trs->md.major_version = 2;\n\t} else if (rebuild && !rs->md.recovery_cp) {\n\t\t \n\t\trs->ti->error = \"Unable to rebuild drive while array is not in-sync\";\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int validate_region_size(struct raid_set *rs, unsigned long region_size)\n{\n\tunsigned long min_region_size = rs->ti->len / (1 << 21);\n\n\tif (rs_is_raid0(rs))\n\t\treturn 0;\n\n\tif (!region_size) {\n\t\t \n\t\tif (min_region_size > (1 << 13)) {\n\t\t\t \n\t\t\tregion_size = roundup_pow_of_two(min_region_size);\n\t\t\tDMINFO(\"Choosing default region size of %lu sectors\",\n\t\t\t       region_size);\n\t\t} else {\n\t\t\tDMINFO(\"Choosing default region size of 4MiB\");\n\t\t\tregion_size = 1 << 13;  \n\t\t}\n\t} else {\n\t\t \n\t\tif (region_size > rs->ti->len) {\n\t\t\trs->ti->error = \"Supplied region size is too large\";\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (region_size < min_region_size) {\n\t\t\tDMERR(\"Supplied region_size (%lu sectors) below minimum (%lu)\",\n\t\t\t      region_size, min_region_size);\n\t\t\trs->ti->error = \"Supplied region size is too small\";\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!is_power_of_2(region_size)) {\n\t\t\trs->ti->error = \"Region size is not a power of 2\";\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (region_size < rs->md.chunk_sectors) {\n\t\t\trs->ti->error = \"Region size is smaller than the chunk size\";\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\trs->md.bitmap_info.chunksize = to_bytes(region_size);\n\n\treturn 0;\n}\n\n \nstatic int validate_raid_redundancy(struct raid_set *rs)\n{\n\tunsigned int i, rebuild_cnt = 0;\n\tunsigned int rebuilds_per_group = 0, copies, raid_disks;\n\tunsigned int group_size, last_group_start;\n\n\tfor (i = 0; i < rs->raid_disks; i++)\n\t\tif (!test_bit(FirstUse, &rs->dev[i].rdev.flags) &&\n\t\t    ((!test_bit(In_sync, &rs->dev[i].rdev.flags) ||\n\t\t      !rs->dev[i].rdev.sb_page)))\n\t\t\trebuild_cnt++;\n\n\tswitch (rs->md.level) {\n\tcase 0:\n\t\tbreak;\n\tcase 1:\n\t\tif (rebuild_cnt >= rs->md.raid_disks)\n\t\t\tgoto too_many;\n\t\tbreak;\n\tcase 4:\n\tcase 5:\n\tcase 6:\n\t\tif (rebuild_cnt > rs->raid_type->parity_devs)\n\t\t\tgoto too_many;\n\t\tbreak;\n\tcase 10:\n\t\tcopies = raid10_md_layout_to_copies(rs->md.new_layout);\n\t\tif (copies < 2) {\n\t\t\tDMERR(\"Bogus raid10 data copies < 2!\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (rebuild_cnt < copies)\n\t\t\tbreak;\n\n\t\t \n\t\traid_disks = min(rs->raid_disks, rs->md.raid_disks);\n\t\tif (__is_raid10_near(rs->md.new_layout)) {\n\t\t\tfor (i = 0; i < raid_disks; i++) {\n\t\t\t\tif (!(i % copies))\n\t\t\t\t\trebuilds_per_group = 0;\n\t\t\t\tif ((!rs->dev[i].rdev.sb_page ||\n\t\t\t\t    !test_bit(In_sync, &rs->dev[i].rdev.flags)) &&\n\t\t\t\t    (++rebuilds_per_group >= copies))\n\t\t\t\t\tgoto too_many;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tgroup_size = (raid_disks / copies);\n\t\tlast_group_start = (raid_disks / group_size) - 1;\n\t\tlast_group_start *= group_size;\n\t\tfor (i = 0; i < raid_disks; i++) {\n\t\t\tif (!(i % copies) && !(i > last_group_start))\n\t\t\t\trebuilds_per_group = 0;\n\t\t\tif ((!rs->dev[i].rdev.sb_page ||\n\t\t\t     !test_bit(In_sync, &rs->dev[i].rdev.flags)) &&\n\t\t\t    (++rebuilds_per_group >= copies))\n\t\t\t\tgoto too_many;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tif (rebuild_cnt)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n\ntoo_many:\n\treturn -EINVAL;\n}\n\n \nstatic int parse_raid_params(struct raid_set *rs, struct dm_arg_set *as,\n\t\t\t     unsigned int num_raid_params)\n{\n\tint value, raid10_format = ALGORITHM_RAID10_DEFAULT;\n\tunsigned int raid10_copies = 2;\n\tunsigned int i, write_mostly = 0;\n\tunsigned int region_size = 0;\n\tsector_t max_io_len;\n\tconst char *arg, *key;\n\tstruct raid_dev *rd;\n\tstruct raid_type *rt = rs->raid_type;\n\n\targ = dm_shift_arg(as);\n\tnum_raid_params--;  \n\n\tif (kstrtoint(arg, 10, &value) < 0) {\n\t\trs->ti->error = \"Bad numerical argument given for chunk_size\";\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (rt_is_raid1(rt)) {\n\t\tif (value)\n\t\t\tDMERR(\"Ignoring chunk size parameter for RAID 1\");\n\t\tvalue = 0;\n\t} else if (!is_power_of_2(value)) {\n\t\trs->ti->error = \"Chunk size must be a power of 2\";\n\t\treturn -EINVAL;\n\t} else if (value < 8) {\n\t\trs->ti->error = \"Chunk size value is too small\";\n\t\treturn -EINVAL;\n\t}\n\n\trs->md.new_chunk_sectors = rs->md.chunk_sectors = value;\n\n\t \n\tfor (i = 0; i < rs->raid_disks; i++) {\n\t\tset_bit(In_sync, &rs->dev[i].rdev.flags);\n\t\trs->dev[i].rdev.recovery_offset = MaxSector;\n\t}\n\n\t \n\tfor (i = 0; i < num_raid_params; i++) {\n\t\tkey = dm_shift_arg(as);\n\t\tif (!key) {\n\t\t\trs->ti->error = \"Not enough raid parameters given\";\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_NOSYNC))) {\n\t\t\tif (test_and_set_bit(__CTR_FLAG_NOSYNC, &rs->ctr_flags)) {\n\t\t\t\trs->ti->error = \"Only one 'nosync' argument allowed\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_SYNC))) {\n\t\t\tif (test_and_set_bit(__CTR_FLAG_SYNC, &rs->ctr_flags)) {\n\t\t\t\trs->ti->error = \"Only one 'sync' argument allowed\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\tif (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_RAID10_USE_NEAR_SETS))) {\n\t\t\tif (test_and_set_bit(__CTR_FLAG_RAID10_USE_NEAR_SETS, &rs->ctr_flags)) {\n\t\t\t\trs->ti->error = \"Only one 'raid10_use_new_sets' argument allowed\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\targ = dm_shift_arg(as);\n\t\ti++;  \n\t\tif (!arg) {\n\t\t\trs->ti->error = \"Wrong number of raid parameters given\";\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\t \n\t\tif (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_RAID10_FORMAT))) {\n\t\t\tif (test_and_set_bit(__CTR_FLAG_RAID10_FORMAT, &rs->ctr_flags)) {\n\t\t\t\trs->ti->error = \"Only one 'raid10_format' argument pair allowed\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (!rt_is_raid10(rt)) {\n\t\t\t\trs->ti->error = \"'raid10_format' is an invalid parameter for this RAID type\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\traid10_format = raid10_name_to_format(arg);\n\t\t\tif (raid10_format < 0) {\n\t\t\t\trs->ti->error = \"Invalid 'raid10_format' value given\";\n\t\t\t\treturn raid10_format;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_JOURNAL_DEV))) {\n\t\t\tint r;\n\t\t\tstruct md_rdev *jdev;\n\n\t\t\tif (test_and_set_bit(__CTR_FLAG_JOURNAL_DEV, &rs->ctr_flags)) {\n\t\t\t\trs->ti->error = \"Only one raid4/5/6 set journaling device allowed\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (!rt_is_raid456(rt)) {\n\t\t\t\trs->ti->error = \"'journal_dev' is an invalid parameter for this RAID type\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tr = dm_get_device(rs->ti, arg, dm_table_get_mode(rs->ti->table),\n\t\t\t\t\t  &rs->journal_dev.dev);\n\t\t\tif (r) {\n\t\t\t\trs->ti->error = \"raid4/5/6 journal device lookup failure\";\n\t\t\t\treturn r;\n\t\t\t}\n\t\t\tjdev = &rs->journal_dev.rdev;\n\t\t\tmd_rdev_init(jdev);\n\t\t\tjdev->mddev = &rs->md;\n\t\t\tjdev->bdev = rs->journal_dev.dev->bdev;\n\t\t\tjdev->sectors = bdev_nr_sectors(jdev->bdev);\n\t\t\tif (jdev->sectors < MIN_RAID456_JOURNAL_SPACE) {\n\t\t\t\trs->ti->error = \"No space for raid4/5/6 journal\";\n\t\t\t\treturn -ENOSPC;\n\t\t\t}\n\t\t\trs->journal_dev.mode = R5C_JOURNAL_MODE_WRITE_THROUGH;\n\t\t\tset_bit(Journal, &jdev->flags);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_JOURNAL_MODE))) {\n\t\t\tint r;\n\n\t\t\tif (!test_bit(__CTR_FLAG_JOURNAL_DEV, &rs->ctr_flags)) {\n\t\t\t\trs->ti->error = \"raid4/5/6 'journal_mode' is invalid without 'journal_dev'\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (test_and_set_bit(__CTR_FLAG_JOURNAL_MODE, &rs->ctr_flags)) {\n\t\t\t\trs->ti->error = \"Only one raid4/5/6 'journal_mode' argument allowed\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tr = dm_raid_journal_mode_to_md(arg);\n\t\t\tif (r < 0) {\n\t\t\t\trs->ti->error = \"Invalid 'journal_mode' argument\";\n\t\t\t\treturn r;\n\t\t\t}\n\t\t\trs->journal_dev.mode = r;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (kstrtoint(arg, 10, &value) < 0) {\n\t\t\trs->ti->error = \"Bad numerical argument given in raid params\";\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_REBUILD))) {\n\t\t\t \n\t\t\tif (!__within_range(value, 0, rs->raid_disks - 1)) {\n\t\t\t\trs->ti->error = \"Invalid rebuild index given\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (test_and_set_bit(value, (void *) rs->rebuild_disks)) {\n\t\t\t\trs->ti->error = \"rebuild for this index already given\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\trd = rs->dev + value;\n\t\t\tclear_bit(In_sync, &rd->rdev.flags);\n\t\t\tclear_bit(Faulty, &rd->rdev.flags);\n\t\t\trd->rdev.recovery_offset = 0;\n\t\t\tset_bit(__CTR_FLAG_REBUILD, &rs->ctr_flags);\n\t\t} else if (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_WRITE_MOSTLY))) {\n\t\t\tif (!rt_is_raid1(rt)) {\n\t\t\t\trs->ti->error = \"write_mostly option is only valid for RAID1\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (!__within_range(value, 0, rs->md.raid_disks - 1)) {\n\t\t\t\trs->ti->error = \"Invalid write_mostly index given\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\twrite_mostly++;\n\t\t\tset_bit(WriteMostly, &rs->dev[value].rdev.flags);\n\t\t\tset_bit(__CTR_FLAG_WRITE_MOSTLY, &rs->ctr_flags);\n\t\t} else if (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_MAX_WRITE_BEHIND))) {\n\t\t\tif (!rt_is_raid1(rt)) {\n\t\t\t\trs->ti->error = \"max_write_behind option is only valid for RAID1\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (test_and_set_bit(__CTR_FLAG_MAX_WRITE_BEHIND, &rs->ctr_flags)) {\n\t\t\t\trs->ti->error = \"Only one max_write_behind argument pair allowed\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (value < 0 || value / 2 > COUNTER_MAX) {\n\t\t\t\trs->ti->error = \"Max write-behind limit out of range\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\trs->md.bitmap_info.max_write_behind = value / 2;\n\t\t} else if (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_DAEMON_SLEEP))) {\n\t\t\tif (test_and_set_bit(__CTR_FLAG_DAEMON_SLEEP, &rs->ctr_flags)) {\n\t\t\t\trs->ti->error = \"Only one daemon_sleep argument pair allowed\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (value < 0) {\n\t\t\t\trs->ti->error = \"daemon sleep period out of range\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\trs->md.bitmap_info.daemon_sleep = value;\n\t\t} else if (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_DATA_OFFSET))) {\n\t\t\t \n\t\t\tif (test_and_set_bit(__CTR_FLAG_DATA_OFFSET, &rs->ctr_flags)) {\n\t\t\t\trs->ti->error = \"Only one data_offset argument pair allowed\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t \n\t\t\tif (value < 0 ||\n\t\t\t    (value && (value < MIN_FREE_RESHAPE_SPACE || value % to_sector(PAGE_SIZE)))) {\n\t\t\t\trs->ti->error = \"Bogus data_offset value\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\trs->data_offset = value;\n\t\t} else if (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_DELTA_DISKS))) {\n\t\t\t \n\t\t\tif (test_and_set_bit(__CTR_FLAG_DELTA_DISKS, &rs->ctr_flags)) {\n\t\t\t\trs->ti->error = \"Only one delta_disks argument pair allowed\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t \n\t\t\tif (!__within_range(abs(value), 1, MAX_RAID_DEVICES - rt->minimal_devs)) {\n\t\t\t\trs->ti->error = \"Too many delta_disk requested\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\trs->delta_disks = value;\n\t\t} else if (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_STRIPE_CACHE))) {\n\t\t\tif (test_and_set_bit(__CTR_FLAG_STRIPE_CACHE, &rs->ctr_flags)) {\n\t\t\t\trs->ti->error = \"Only one stripe_cache argument pair allowed\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (!rt_is_raid456(rt)) {\n\t\t\t\trs->ti->error = \"Inappropriate argument: stripe_cache\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (value < 0) {\n\t\t\t\trs->ti->error = \"Bogus stripe cache entries value\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\trs->stripe_cache_entries = value;\n\t\t} else if (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_MIN_RECOVERY_RATE))) {\n\t\t\tif (test_and_set_bit(__CTR_FLAG_MIN_RECOVERY_RATE, &rs->ctr_flags)) {\n\t\t\t\trs->ti->error = \"Only one min_recovery_rate argument pair allowed\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (value < 0) {\n\t\t\t\trs->ti->error = \"min_recovery_rate out of range\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\trs->md.sync_speed_min = value;\n\t\t} else if (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_MAX_RECOVERY_RATE))) {\n\t\t\tif (test_and_set_bit(__CTR_FLAG_MAX_RECOVERY_RATE, &rs->ctr_flags)) {\n\t\t\t\trs->ti->error = \"Only one max_recovery_rate argument pair allowed\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (value < 0) {\n\t\t\t\trs->ti->error = \"max_recovery_rate out of range\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\trs->md.sync_speed_max = value;\n\t\t} else if (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_REGION_SIZE))) {\n\t\t\tif (test_and_set_bit(__CTR_FLAG_REGION_SIZE, &rs->ctr_flags)) {\n\t\t\t\trs->ti->error = \"Only one region_size argument pair allowed\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tregion_size = value;\n\t\t\trs->requested_bitmap_chunk_sectors = value;\n\t\t} else if (!strcasecmp(key, dm_raid_arg_name_by_flag(CTR_FLAG_RAID10_COPIES))) {\n\t\t\tif (test_and_set_bit(__CTR_FLAG_RAID10_COPIES, &rs->ctr_flags)) {\n\t\t\t\trs->ti->error = \"Only one raid10_copies argument pair allowed\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (!__within_range(value, 2, rs->md.raid_disks)) {\n\t\t\t\trs->ti->error = \"Bad value for 'raid10_copies'\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\traid10_copies = value;\n\t\t} else {\n\t\t\tDMERR(\"Unable to parse RAID parameter: %s\", key);\n\t\t\trs->ti->error = \"Unable to parse RAID parameter\";\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (test_bit(__CTR_FLAG_SYNC, &rs->ctr_flags) &&\n\t    test_bit(__CTR_FLAG_NOSYNC, &rs->ctr_flags)) {\n\t\trs->ti->error = \"sync and nosync are mutually exclusive\";\n\t\treturn -EINVAL;\n\t}\n\n\tif (test_bit(__CTR_FLAG_REBUILD, &rs->ctr_flags) &&\n\t    (test_bit(__CTR_FLAG_SYNC, &rs->ctr_flags) ||\n\t     test_bit(__CTR_FLAG_NOSYNC, &rs->ctr_flags))) {\n\t\trs->ti->error = \"sync/nosync and rebuild are mutually exclusive\";\n\t\treturn -EINVAL;\n\t}\n\n\tif (write_mostly >= rs->md.raid_disks) {\n\t\trs->ti->error = \"Can't set all raid1 devices to write_mostly\";\n\t\treturn -EINVAL;\n\t}\n\n\tif (rs->md.sync_speed_max &&\n\t    rs->md.sync_speed_min > rs->md.sync_speed_max) {\n\t\trs->ti->error = \"Bogus recovery rates\";\n\t\treturn -EINVAL;\n\t}\n\n\tif (validate_region_size(rs, region_size))\n\t\treturn -EINVAL;\n\n\tif (rs->md.chunk_sectors)\n\t\tmax_io_len = rs->md.chunk_sectors;\n\telse\n\t\tmax_io_len = region_size;\n\n\tif (dm_set_target_max_io_len(rs->ti, max_io_len))\n\t\treturn -EINVAL;\n\n\tif (rt_is_raid10(rt)) {\n\t\tif (raid10_copies > rs->md.raid_disks) {\n\t\t\trs->ti->error = \"Not enough devices to satisfy specification\";\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\trs->md.new_layout = raid10_format_to_md_layout(rs, raid10_format, raid10_copies);\n\t\tif (rs->md.new_layout < 0) {\n\t\t\trs->ti->error = \"Error getting raid10 format\";\n\t\t\treturn rs->md.new_layout;\n\t\t}\n\n\t\trt = get_raid_type_by_ll(10, rs->md.new_layout);\n\t\tif (!rt) {\n\t\t\trs->ti->error = \"Failed to recognize new raid10 layout\";\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif ((rt->algorithm == ALGORITHM_RAID10_DEFAULT ||\n\t\t     rt->algorithm == ALGORITHM_RAID10_NEAR) &&\n\t\t    test_bit(__CTR_FLAG_RAID10_USE_NEAR_SETS, &rs->ctr_flags)) {\n\t\t\trs->ti->error = \"RAID10 format 'near' and 'raid10_use_near_sets' are incompatible\";\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\trs->raid10_copies = raid10_copies;\n\n\t \n\trs->md.persistent = 0;\n\trs->md.external = 1;\n\n\t \n\treturn rs_check_for_valid_flags(rs);\n}\n\n \nstatic int rs_set_raid456_stripe_cache(struct raid_set *rs)\n{\n\tint r;\n\tstruct r5conf *conf;\n\tstruct mddev *mddev = &rs->md;\n\tuint32_t min_stripes = max(mddev->chunk_sectors, mddev->new_chunk_sectors) / 2;\n\tuint32_t nr_stripes = rs->stripe_cache_entries;\n\n\tif (!rt_is_raid456(rs->raid_type)) {\n\t\trs->ti->error = \"Inappropriate raid level; cannot change stripe_cache size\";\n\t\treturn -EINVAL;\n\t}\n\n\tif (nr_stripes < min_stripes) {\n\t\tDMINFO(\"Adjusting requested %u stripe cache entries to %u to suit stripe size\",\n\t\t       nr_stripes, min_stripes);\n\t\tnr_stripes = min_stripes;\n\t}\n\n\tconf = mddev->private;\n\tif (!conf) {\n\t\trs->ti->error = \"Cannot change stripe_cache size on inactive RAID set\";\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (conf->min_nr_stripes != nr_stripes) {\n\t\tr = raid5_set_cache_size(mddev, nr_stripes);\n\t\tif (r) {\n\t\t\trs->ti->error = \"Failed to set raid4/5/6 stripe cache size\";\n\t\t\treturn r;\n\t\t}\n\n\t\tDMINFO(\"%u stripe cache entries\", nr_stripes);\n\t}\n\n\treturn 0;\n}\n\n \nstatic unsigned int mddev_data_stripes(struct raid_set *rs)\n{\n\treturn rs->md.raid_disks - rs->raid_type->parity_devs;\n}\n\n \nstatic unsigned int rs_data_stripes(struct raid_set *rs)\n{\n\treturn rs->raid_disks - rs->raid_type->parity_devs;\n}\n\n \nstatic sector_t __rdev_sectors(struct raid_set *rs)\n{\n\tint i;\n\n\tfor (i = 0; i < rs->raid_disks; i++) {\n\t\tstruct md_rdev *rdev = &rs->dev[i].rdev;\n\n\t\tif (!test_bit(Journal, &rdev->flags) &&\n\t\t    rdev->bdev && rdev->sectors)\n\t\t\treturn rdev->sectors;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int _check_data_dev_sectors(struct raid_set *rs)\n{\n\tsector_t ds = ~0;\n\tstruct md_rdev *rdev;\n\n\trdev_for_each(rdev, &rs->md)\n\t\tif (!test_bit(Journal, &rdev->flags) && rdev->bdev) {\n\t\t\tds = min(ds, bdev_nr_sectors(rdev->bdev));\n\t\t\tif (ds < rs->md.dev_sectors) {\n\t\t\t\trs->ti->error = \"Component device(s) too small\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\n\treturn 0;\n}\n\n \nstatic int rs_set_dev_and_array_sectors(struct raid_set *rs, sector_t sectors, bool use_mddev)\n{\n\tint delta_disks;\n\tunsigned int data_stripes;\n\tsector_t array_sectors = sectors, dev_sectors = sectors;\n\tstruct mddev *mddev = &rs->md;\n\n\tif (use_mddev) {\n\t\tdelta_disks = mddev->delta_disks;\n\t\tdata_stripes = mddev_data_stripes(rs);\n\t} else {\n\t\tdelta_disks = rs->delta_disks;\n\t\tdata_stripes = rs_data_stripes(rs);\n\t}\n\n\t \n\tif (rt_is_raid1(rs->raid_type))\n\t\t;\n\telse if (rt_is_raid10(rs->raid_type)) {\n\t\tif (rs->raid10_copies < 2 ||\n\t\t    delta_disks < 0) {\n\t\t\trs->ti->error = \"Bogus raid10 data copies or delta disks\";\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tdev_sectors *= rs->raid10_copies;\n\t\tif (sector_div(dev_sectors, data_stripes))\n\t\t\tgoto bad;\n\n\t\tarray_sectors = (data_stripes + delta_disks) * dev_sectors;\n\t\tif (sector_div(array_sectors, rs->raid10_copies))\n\t\t\tgoto bad;\n\n\t} else if (sector_div(dev_sectors, data_stripes))\n\t\tgoto bad;\n\n\telse\n\t\t \n\t\tarray_sectors = (data_stripes + delta_disks) * dev_sectors;\n\n\tmddev->array_sectors = array_sectors;\n\tmddev->dev_sectors = dev_sectors;\n\trs_set_rdev_sectors(rs);\n\n\treturn _check_data_dev_sectors(rs);\nbad:\n\trs->ti->error = \"Target length not divisible by number of data devices\";\n\treturn -EINVAL;\n}\n\n \nstatic void rs_setup_recovery(struct raid_set *rs, sector_t dev_sectors)\n{\n\t \n\tif (rs_is_raid0(rs))\n\t\trs->md.recovery_cp = MaxSector;\n\t \n\telse if (rs_is_raid6(rs))\n\t\trs->md.recovery_cp = dev_sectors;\n\t \n\telse\n\t\trs->md.recovery_cp = test_bit(__CTR_FLAG_NOSYNC, &rs->ctr_flags)\n\t\t\t\t     ? MaxSector : dev_sectors;\n}\n\nstatic void do_table_event(struct work_struct *ws)\n{\n\tstruct raid_set *rs = container_of(ws, struct raid_set, md.event_work);\n\n\tsmp_rmb();  \n\tif (!rs_is_reshaping(rs)) {\n\t\tif (rs_is_raid10(rs))\n\t\t\trs_set_rdev_sectors(rs);\n\t\trs_set_capacity(rs);\n\t}\n\tdm_table_event(rs->ti->table);\n}\n\n \nstatic int rs_check_takeover(struct raid_set *rs)\n{\n\tstruct mddev *mddev = &rs->md;\n\tunsigned int near_copies;\n\n\tif (rs->md.degraded) {\n\t\trs->ti->error = \"Can't takeover degraded raid set\";\n\t\treturn -EPERM;\n\t}\n\n\tif (rs_is_reshaping(rs)) {\n\t\trs->ti->error = \"Can't takeover reshaping raid set\";\n\t\treturn -EPERM;\n\t}\n\n\tswitch (mddev->level) {\n\tcase 0:\n\t\t \n\t\tif ((mddev->new_level == 1 || mddev->new_level == 5) &&\n\t\t    mddev->raid_disks == 1)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (mddev->new_level == 10 &&\n\t\t    !(rs->raid_disks % mddev->raid_disks))\n\t\t\treturn 0;\n\n\t\t \n\t\tif (__within_range(mddev->new_level, 4, 6) &&\n\t\t    mddev->new_layout == ALGORITHM_PARITY_N &&\n\t\t    mddev->raid_disks > 1)\n\t\t\treturn 0;\n\n\t\tbreak;\n\n\tcase 10:\n\t\t \n\t\tif (__is_raid10_offset(mddev->layout))\n\t\t\tbreak;\n\n\t\tnear_copies = __raid10_near_copies(mddev->layout);\n\n\t\t \n\t\tif (mddev->new_level == 0) {\n\t\t\t \n\t\t\tif (near_copies > 1 &&\n\t\t\t    !(mddev->raid_disks % near_copies)) {\n\t\t\t\tmddev->raid_disks /= near_copies;\n\t\t\t\tmddev->delta_disks = mddev->raid_disks;\n\t\t\t\treturn 0;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (near_copies == 1 &&\n\t\t\t    __raid10_far_copies(mddev->layout) > 1)\n\t\t\t\treturn 0;\n\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (mddev->new_level == 1 &&\n\t\t    max(near_copies, __raid10_far_copies(mddev->layout)) == mddev->raid_disks)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (__within_range(mddev->new_level, 4, 5) &&\n\t\t    mddev->raid_disks == 2)\n\t\t\treturn 0;\n\t\tbreak;\n\n\tcase 1:\n\t\t \n\t\tif (__within_range(mddev->new_level, 4, 5) &&\n\t\t    mddev->raid_disks == 2) {\n\t\t\tmddev->degraded = 1;\n\t\t\treturn 0;\n\t\t}\n\n\t\t \n\t\tif (mddev->new_level == 0 &&\n\t\t    mddev->raid_disks == 1)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (mddev->new_level == 10)\n\t\t\treturn 0;\n\t\tbreak;\n\n\tcase 4:\n\t\t \n\t\tif (mddev->new_level == 0)\n\t\t\treturn 0;\n\n\t\t \n\t\tif ((mddev->new_level == 1 || mddev->new_level == 5) &&\n\t\t    mddev->raid_disks == 2)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (__within_range(mddev->new_level, 5, 6) &&\n\t\t    mddev->layout == ALGORITHM_PARITY_N)\n\t\t\treturn 0;\n\t\tbreak;\n\n\tcase 5:\n\t\t \n\t\tif (mddev->new_level == 0 &&\n\t\t    mddev->layout == ALGORITHM_PARITY_N)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (mddev->new_level == 4 &&\n\t\t    mddev->layout == ALGORITHM_PARITY_N)\n\t\t\treturn 0;\n\n\t\t \n\t\tif ((mddev->new_level == 1 || mddev->new_level == 4 || mddev->new_level == 10) &&\n\t\t    mddev->raid_disks == 2)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (mddev->new_level == 6 &&\n\t\t    ((mddev->layout == ALGORITHM_PARITY_N && mddev->new_layout == ALGORITHM_PARITY_N) ||\n\t\t      __within_range(mddev->new_layout, ALGORITHM_LEFT_ASYMMETRIC_6, ALGORITHM_RIGHT_SYMMETRIC_6)))\n\t\t\treturn 0;\n\t\tbreak;\n\n\tcase 6:\n\t\t \n\t\tif (mddev->new_level == 0 &&\n\t\t    mddev->layout == ALGORITHM_PARITY_N)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (mddev->new_level == 4 &&\n\t\t    mddev->layout == ALGORITHM_PARITY_N)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (mddev->new_level == 5 &&\n\t\t    ((mddev->layout == ALGORITHM_PARITY_N && mddev->new_layout == ALGORITHM_PARITY_N) ||\n\t\t     __within_range(mddev->new_layout, ALGORITHM_LEFT_ASYMMETRIC, ALGORITHM_RIGHT_SYMMETRIC)))\n\t\t\treturn 0;\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\trs->ti->error = \"takeover not possible\";\n\treturn -EINVAL;\n}\n\n \nstatic bool rs_takeover_requested(struct raid_set *rs)\n{\n\treturn rs->md.new_level != rs->md.level;\n}\n\n \nstatic bool rs_is_layout_change(struct raid_set *rs, bool use_mddev)\n{\n\treturn (use_mddev ? rs->md.delta_disks : rs->delta_disks) ||\n\t       rs->md.new_layout != rs->md.layout ||\n\t       rs->md.new_chunk_sectors != rs->md.chunk_sectors;\n}\n\n \nstatic bool rs_reshape_requested(struct raid_set *rs)\n{\n\tbool change;\n\tstruct mddev *mddev = &rs->md;\n\n\tif (rs_takeover_requested(rs))\n\t\treturn false;\n\n\tif (rs_is_raid0(rs))\n\t\treturn false;\n\n\tchange = rs_is_layout_change(rs, false);\n\n\t \n\tif (rs_is_raid1(rs)) {\n\t\tif (rs->delta_disks)\n\t\t\treturn !!rs->delta_disks;\n\n\t\treturn !change &&\n\t\t       mddev->raid_disks != rs->raid_disks;\n\t}\n\n\tif (rs_is_raid10(rs))\n\t\treturn change &&\n\t\t       !__is_raid10_far(mddev->new_layout) &&\n\t\t       rs->delta_disks >= 0;\n\n\treturn change;\n}\n\n \n#define\tFEATURE_FLAG_SUPPORTS_V190\t0x1  \n\n \n#define\tSB_FLAG_RESHAPE_ACTIVE\t\t0x1\n#define\tSB_FLAG_RESHAPE_BACKWARDS\t0x2\n\n \n#define DM_RAID_MAGIC 0x64526D44\nstruct dm_raid_superblock {\n\t__le32 magic;\t\t \n\t__le32 compat_features;\t \n\n\t__le32 num_devices;\t \n\t__le32 array_position;\t \n\n\t__le64 events;\t\t \n\t__le64 failed_devices;\t \n\t\t\t\t \n\n\t \n\t__le64 disk_recovery_offset;\n\n\t \n\t__le64 array_resync_offset;\n\n\t \n\t__le32 level;\n\t__le32 layout;\n\t__le32 stripe_sectors;\n\n\t \n\n\t__le32 flags;  \n\n\t \n\t__le64 reshape_position;\n\n\t \n\t__le32 new_level;\n\t__le32 new_layout;\n\t__le32 new_stripe_sectors;\n\t__le32 delta_disks;\n\n\t__le64 array_sectors;  \n\n\t \n\t__le64 data_offset;\n\t__le64 new_data_offset;\n\n\t__le64 sectors;  \n\n\t \n\t__le64 extended_failed_devices[DISKS_ARRAY_ELEMS - 1];\n\n\t__le32 incompat_features;\t \n\n\t \n} __packed;\n\n \nstatic int rs_check_reshape(struct raid_set *rs)\n{\n\tstruct mddev *mddev = &rs->md;\n\n\tif (!mddev->pers || !mddev->pers->check_reshape)\n\t\trs->ti->error = \"Reshape not supported\";\n\telse if (mddev->degraded)\n\t\trs->ti->error = \"Can't reshape degraded raid set\";\n\telse if (rs_is_recovering(rs))\n\t\trs->ti->error = \"Convert request on recovering raid set prohibited\";\n\telse if (rs_is_reshaping(rs))\n\t\trs->ti->error = \"raid set already reshaping!\";\n\telse if (!(rs_is_raid1(rs) || rs_is_raid10(rs) || rs_is_raid456(rs)))\n\t\trs->ti->error = \"Reshaping only supported for raid1/4/5/6/10\";\n\telse\n\t\treturn 0;\n\n\treturn -EPERM;\n}\n\nstatic int read_disk_sb(struct md_rdev *rdev, int size, bool force_reload)\n{\n\tBUG_ON(!rdev->sb_page);\n\n\tif (rdev->sb_loaded && !force_reload)\n\t\treturn 0;\n\n\trdev->sb_loaded = 0;\n\n\tif (!sync_page_io(rdev, 0, size, rdev->sb_page, REQ_OP_READ, true)) {\n\t\tDMERR(\"Failed to read superblock of device at position %d\",\n\t\t      rdev->raid_disk);\n\t\tmd_error(rdev->mddev, rdev);\n\t\tset_bit(Faulty, &rdev->flags);\n\t\treturn -EIO;\n\t}\n\n\trdev->sb_loaded = 1;\n\n\treturn 0;\n}\n\nstatic void sb_retrieve_failed_devices(struct dm_raid_superblock *sb, uint64_t *failed_devices)\n{\n\tfailed_devices[0] = le64_to_cpu(sb->failed_devices);\n\tmemset(failed_devices + 1, 0, sizeof(sb->extended_failed_devices));\n\n\tif (le32_to_cpu(sb->compat_features) & FEATURE_FLAG_SUPPORTS_V190) {\n\t\tint i = ARRAY_SIZE(sb->extended_failed_devices);\n\n\t\twhile (i--)\n\t\t\tfailed_devices[i+1] = le64_to_cpu(sb->extended_failed_devices[i]);\n\t}\n}\n\nstatic void sb_update_failed_devices(struct dm_raid_superblock *sb, uint64_t *failed_devices)\n{\n\tint i = ARRAY_SIZE(sb->extended_failed_devices);\n\n\tsb->failed_devices = cpu_to_le64(failed_devices[0]);\n\twhile (i--)\n\t\tsb->extended_failed_devices[i] = cpu_to_le64(failed_devices[i+1]);\n}\n\n \nstatic void super_sync(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tbool update_failed_devices = false;\n\tunsigned int i;\n\tuint64_t failed_devices[DISKS_ARRAY_ELEMS];\n\tstruct dm_raid_superblock *sb;\n\tstruct raid_set *rs = container_of(mddev, struct raid_set, md);\n\n\t \n\tif (!rdev->meta_bdev)\n\t\treturn;\n\n\tBUG_ON(!rdev->sb_page);\n\n\tsb = page_address(rdev->sb_page);\n\n\tsb_retrieve_failed_devices(sb, failed_devices);\n\n\tfor (i = 0; i < rs->raid_disks; i++)\n\t\tif (!rs->dev[i].data_dev || test_bit(Faulty, &rs->dev[i].rdev.flags)) {\n\t\t\tupdate_failed_devices = true;\n\t\t\tset_bit(i, (void *) failed_devices);\n\t\t}\n\n\tif (update_failed_devices)\n\t\tsb_update_failed_devices(sb, failed_devices);\n\n\tsb->magic = cpu_to_le32(DM_RAID_MAGIC);\n\tsb->compat_features = cpu_to_le32(FEATURE_FLAG_SUPPORTS_V190);\n\n\tsb->num_devices = cpu_to_le32(mddev->raid_disks);\n\tsb->array_position = cpu_to_le32(rdev->raid_disk);\n\n\tsb->events = cpu_to_le64(mddev->events);\n\n\tsb->disk_recovery_offset = cpu_to_le64(rdev->recovery_offset);\n\tsb->array_resync_offset = cpu_to_le64(mddev->recovery_cp);\n\n\tsb->level = cpu_to_le32(mddev->level);\n\tsb->layout = cpu_to_le32(mddev->layout);\n\tsb->stripe_sectors = cpu_to_le32(mddev->chunk_sectors);\n\n\t \n\tsb->new_level = cpu_to_le32(mddev->new_level);\n\tsb->new_layout = cpu_to_le32(mddev->new_layout);\n\tsb->new_stripe_sectors = cpu_to_le32(mddev->new_chunk_sectors);\n\n\tsb->delta_disks = cpu_to_le32(mddev->delta_disks);\n\n\tsmp_rmb();  \n\tsb->reshape_position = cpu_to_le64(mddev->reshape_position);\n\tif (le64_to_cpu(sb->reshape_position) != MaxSector) {\n\t\t \n\t\tsb->flags |= cpu_to_le32(SB_FLAG_RESHAPE_ACTIVE);\n\n\t\tif (mddev->delta_disks < 0 || mddev->reshape_backwards)\n\t\t\tsb->flags |= cpu_to_le32(SB_FLAG_RESHAPE_BACKWARDS);\n\t} else {\n\t\t \n\t\tsb->flags &= ~(cpu_to_le32(SB_FLAG_RESHAPE_ACTIVE|SB_FLAG_RESHAPE_BACKWARDS));\n\t}\n\n\tsb->array_sectors = cpu_to_le64(mddev->array_sectors);\n\tsb->data_offset = cpu_to_le64(rdev->data_offset);\n\tsb->new_data_offset = cpu_to_le64(rdev->new_data_offset);\n\tsb->sectors = cpu_to_le64(rdev->sectors);\n\tsb->incompat_features = cpu_to_le32(0);\n\n\t \n\tmemset(sb + 1, 0, rdev->sb_size - sizeof(*sb));\n}\n\n \nstatic int super_load(struct md_rdev *rdev, struct md_rdev *refdev)\n{\n\tint r;\n\tstruct dm_raid_superblock *sb;\n\tstruct dm_raid_superblock *refsb;\n\tuint64_t events_sb, events_refsb;\n\n\tr = read_disk_sb(rdev, rdev->sb_size, false);\n\tif (r)\n\t\treturn r;\n\n\tsb = page_address(rdev->sb_page);\n\n\t \n\tif ((sb->magic != cpu_to_le32(DM_RAID_MAGIC)) ||\n\t    (!test_bit(In_sync, &rdev->flags) && !rdev->recovery_offset)) {\n\t\tsuper_sync(rdev->mddev, rdev);\n\n\t\tset_bit(FirstUse, &rdev->flags);\n\t\tsb->compat_features = cpu_to_le32(FEATURE_FLAG_SUPPORTS_V190);\n\n\t\t \n\t\tset_bit(MD_SB_CHANGE_DEVS, &rdev->mddev->sb_flags);\n\n\t\t \n\t\treturn refdev ? 0 : 1;\n\t}\n\n\tif (!refdev)\n\t\treturn 1;\n\n\tevents_sb = le64_to_cpu(sb->events);\n\n\trefsb = page_address(refdev->sb_page);\n\tevents_refsb = le64_to_cpu(refsb->events);\n\n\treturn (events_sb > events_refsb) ? 1 : 0;\n}\n\nstatic int super_init_validation(struct raid_set *rs, struct md_rdev *rdev)\n{\n\tint role;\n\tstruct mddev *mddev = &rs->md;\n\tuint64_t events_sb;\n\tuint64_t failed_devices[DISKS_ARRAY_ELEMS];\n\tstruct dm_raid_superblock *sb;\n\tuint32_t new_devs = 0, rebuild_and_new = 0, rebuilds = 0;\n\tstruct md_rdev *r;\n\tstruct dm_raid_superblock *sb2;\n\n\tsb = page_address(rdev->sb_page);\n\tevents_sb = le64_to_cpu(sb->events);\n\n\t \n\tmddev->events = events_sb ? : 1;\n\n\tmddev->reshape_position = MaxSector;\n\n\tmddev->raid_disks = le32_to_cpu(sb->num_devices);\n\tmddev->level = le32_to_cpu(sb->level);\n\tmddev->layout = le32_to_cpu(sb->layout);\n\tmddev->chunk_sectors = le32_to_cpu(sb->stripe_sectors);\n\n\t \n\tif (le32_to_cpu(sb->compat_features) & FEATURE_FLAG_SUPPORTS_V190) {\n\t\t \n\t\tmddev->new_level = le32_to_cpu(sb->new_level);\n\t\tmddev->new_layout = le32_to_cpu(sb->new_layout);\n\t\tmddev->new_chunk_sectors = le32_to_cpu(sb->new_stripe_sectors);\n\t\tmddev->delta_disks = le32_to_cpu(sb->delta_disks);\n\t\tmddev->array_sectors = le64_to_cpu(sb->array_sectors);\n\n\t\t \n\t\tif (le32_to_cpu(sb->flags) & SB_FLAG_RESHAPE_ACTIVE) {\n\t\t\tif (test_bit(__CTR_FLAG_DELTA_DISKS, &rs->ctr_flags)) {\n\t\t\t\tDMERR(\"Reshape requested but raid set is still reshaping\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\tif (mddev->delta_disks < 0 ||\n\t\t\t    (!mddev->delta_disks && (le32_to_cpu(sb->flags) & SB_FLAG_RESHAPE_BACKWARDS)))\n\t\t\t\tmddev->reshape_backwards = 1;\n\t\t\telse\n\t\t\t\tmddev->reshape_backwards = 0;\n\n\t\t\tmddev->reshape_position = le64_to_cpu(sb->reshape_position);\n\t\t\trs->raid_type = get_raid_type_by_ll(mddev->level, mddev->layout);\n\t\t}\n\n\t} else {\n\t\t \n\t\tstruct raid_type *rt_cur = get_raid_type_by_ll(mddev->level, mddev->layout);\n\t\tstruct raid_type *rt_new = get_raid_type_by_ll(mddev->new_level, mddev->new_layout);\n\n\t\tif (rs_takeover_requested(rs)) {\n\t\t\tif (rt_cur && rt_new)\n\t\t\t\tDMERR(\"Takeover raid sets from %s to %s not yet supported by metadata. (raid level change)\",\n\t\t\t\t      rt_cur->name, rt_new->name);\n\t\t\telse\n\t\t\t\tDMERR(\"Takeover raid sets not yet supported by metadata. (raid level change)\");\n\t\t\treturn -EINVAL;\n\t\t} else if (rs_reshape_requested(rs)) {\n\t\t\tDMERR(\"Reshaping raid sets not yet supported by metadata. (raid layout change keeping level)\");\n\t\t\tif (mddev->layout != mddev->new_layout) {\n\t\t\t\tif (rt_cur && rt_new)\n\t\t\t\t\tDMERR(\"\t current layout %s vs new layout %s\",\n\t\t\t\t\t      rt_cur->name, rt_new->name);\n\t\t\t\telse\n\t\t\t\t\tDMERR(\"\t current layout 0x%X vs new layout 0x%X\",\n\t\t\t\t\t      le32_to_cpu(sb->layout), mddev->new_layout);\n\t\t\t}\n\t\t\tif (mddev->chunk_sectors != mddev->new_chunk_sectors)\n\t\t\t\tDMERR(\"\t current stripe sectors %u vs new stripe sectors %u\",\n\t\t\t\t      mddev->chunk_sectors, mddev->new_chunk_sectors);\n\t\t\tif (rs->delta_disks)\n\t\t\t\tDMERR(\"\t current %u disks vs new %u disks\",\n\t\t\t\t      mddev->raid_disks, mddev->raid_disks + rs->delta_disks);\n\t\t\tif (rs_is_raid10(rs)) {\n\t\t\t\tDMERR(\"\t Old layout: %s w/ %u copies\",\n\t\t\t\t      raid10_md_layout_to_format(mddev->layout),\n\t\t\t\t      raid10_md_layout_to_copies(mddev->layout));\n\t\t\t\tDMERR(\"\t New layout: %s w/ %u copies\",\n\t\t\t\t      raid10_md_layout_to_format(mddev->new_layout),\n\t\t\t\t      raid10_md_layout_to_copies(mddev->new_layout));\n\t\t\t}\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tDMINFO(\"Discovered old metadata format; upgrading to extended metadata format\");\n\t}\n\n\tif (!test_bit(__CTR_FLAG_NOSYNC, &rs->ctr_flags))\n\t\tmddev->recovery_cp = le64_to_cpu(sb->array_resync_offset);\n\n\t \n\trdev_for_each(r, mddev) {\n\t\tif (test_bit(Journal, &rdev->flags))\n\t\t\tcontinue;\n\n\t\tif (test_bit(FirstUse, &r->flags))\n\t\t\tnew_devs++;\n\n\t\tif (!test_bit(In_sync, &r->flags)) {\n\t\t\tDMINFO(\"Device %d specified for rebuild; clearing superblock\",\n\t\t\t\tr->raid_disk);\n\t\t\trebuilds++;\n\n\t\t\tif (test_bit(FirstUse, &r->flags))\n\t\t\t\trebuild_and_new++;\n\t\t}\n\t}\n\n\tif (new_devs == rs->raid_disks || !rebuilds) {\n\t\t \n\t\tif (new_devs == rs->raid_disks) {\n\t\t\tDMINFO(\"Superblocks created for new raid set\");\n\t\t\tset_bit(MD_ARRAY_FIRST_USE, &mddev->flags);\n\t\t} else if (new_devs != rebuilds &&\n\t\t\t   new_devs != rs->delta_disks) {\n\t\t\tDMERR(\"New device injected into existing raid set without \"\n\t\t\t      \"'delta_disks' or 'rebuild' parameter specified\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (new_devs && new_devs != rebuilds) {\n\t\tDMERR(\"%u 'rebuild' devices cannot be injected into\"\n\t\t      \" a raid set with %u other first-time devices\",\n\t\t      rebuilds, new_devs);\n\t\treturn -EINVAL;\n\t} else if (rebuilds) {\n\t\tif (rebuild_and_new && rebuilds != rebuild_and_new) {\n\t\t\tDMERR(\"new device%s provided without 'rebuild'\",\n\t\t\t      new_devs > 1 ? \"s\" : \"\");\n\t\t\treturn -EINVAL;\n\t\t} else if (!test_bit(__CTR_FLAG_REBUILD, &rs->ctr_flags) && rs_is_recovering(rs)) {\n\t\t\tDMERR(\"'rebuild' specified while raid set is not in-sync (recovery_cp=%llu)\",\n\t\t\t      (unsigned long long) mddev->recovery_cp);\n\t\t\treturn -EINVAL;\n\t\t} else if (rs_is_reshaping(rs)) {\n\t\t\tDMERR(\"'rebuild' specified while raid set is being reshaped (reshape_position=%llu)\",\n\t\t\t      (unsigned long long) mddev->reshape_position);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\tsb_retrieve_failed_devices(sb, failed_devices);\n\trdev_for_each(r, mddev) {\n\t\tif (test_bit(Journal, &rdev->flags) ||\n\t\t    !r->sb_page)\n\t\t\tcontinue;\n\t\tsb2 = page_address(r->sb_page);\n\t\tsb2->failed_devices = 0;\n\t\tmemset(sb2->extended_failed_devices, 0, sizeof(sb2->extended_failed_devices));\n\n\t\t \n\t\tif (!test_bit(FirstUse, &r->flags) && (r->raid_disk >= 0)) {\n\t\t\trole = le32_to_cpu(sb2->array_position);\n\t\t\tif (role < 0)\n\t\t\t\tcontinue;\n\n\t\t\tif (role != r->raid_disk) {\n\t\t\t\tif (rs_is_raid10(rs) && __is_raid10_near(mddev->layout)) {\n\t\t\t\t\tif (mddev->raid_disks % __raid10_near_copies(mddev->layout) ||\n\t\t\t\t\t    rs->raid_disks % rs->raid10_copies) {\n\t\t\t\t\t\trs->ti->error =\n\t\t\t\t\t\t\t\"Cannot change raid10 near set to odd # of devices!\";\n\t\t\t\t\t\treturn -EINVAL;\n\t\t\t\t\t}\n\n\t\t\t\t\tsb2->array_position = cpu_to_le32(r->raid_disk);\n\n\t\t\t\t} else if (!(rs_is_raid10(rs) && rt_is_raid0(rs->raid_type)) &&\n\t\t\t\t\t   !(rs_is_raid0(rs) && rt_is_raid10(rs->raid_type)) &&\n\t\t\t\t\t   !rt_is_raid1(rs->raid_type)) {\n\t\t\t\t\trs->ti->error = \"Cannot change device positions in raid set\";\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\t}\n\n\t\t\t\tDMINFO(\"raid device #%d now at position #%d\", role, r->raid_disk);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (test_bit(role, (void *) failed_devices))\n\t\t\t\tset_bit(Faulty, &r->flags);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int super_validate(struct raid_set *rs, struct md_rdev *rdev)\n{\n\tstruct mddev *mddev = &rs->md;\n\tstruct dm_raid_superblock *sb;\n\n\tif (rs_is_raid0(rs) || !rdev->sb_page || rdev->raid_disk < 0)\n\t\treturn 0;\n\n\tsb = page_address(rdev->sb_page);\n\n\t \n\tif (!mddev->events && super_init_validation(rs, rdev))\n\t\treturn -EINVAL;\n\n\tif (le32_to_cpu(sb->compat_features) &&\n\t    le32_to_cpu(sb->compat_features) != FEATURE_FLAG_SUPPORTS_V190) {\n\t\trs->ti->error = \"Unable to assemble array: Unknown flag(s) in compatible feature flags\";\n\t\treturn -EINVAL;\n\t}\n\n\tif (sb->incompat_features) {\n\t\trs->ti->error = \"Unable to assemble array: No incompatible feature flags supported yet\";\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tmddev->bitmap_info.offset = (rt_is_raid0(rs->raid_type) || rs->journal_dev.dev) ? 0 : to_sector(4096);\n\tmddev->bitmap_info.default_offset = mddev->bitmap_info.offset;\n\n\tif (!test_and_clear_bit(FirstUse, &rdev->flags)) {\n\t\t \n\t\tif (le32_to_cpu(sb->compat_features) & FEATURE_FLAG_SUPPORTS_V190)\n\t\t\trdev->sectors = le64_to_cpu(sb->sectors);\n\n\t\trdev->recovery_offset = le64_to_cpu(sb->disk_recovery_offset);\n\t\tif (rdev->recovery_offset == MaxSector)\n\t\t\tset_bit(In_sync, &rdev->flags);\n\t\t \n\t\telse if (!rs_is_reshaping(rs))\n\t\t\tclear_bit(In_sync, &rdev->flags);  \n\t}\n\n\t \n\tif (test_and_clear_bit(Faulty, &rdev->flags)) {\n\t\trdev->recovery_offset = 0;\n\t\tclear_bit(In_sync, &rdev->flags);\n\t\trdev->saved_raid_disk = rdev->raid_disk;\n\t}\n\n\t \n\trdev->data_offset = le64_to_cpu(sb->data_offset);\n\trdev->new_data_offset = le64_to_cpu(sb->new_data_offset);\n\n\treturn 0;\n}\n\n \nstatic int analyse_superblocks(struct dm_target *ti, struct raid_set *rs)\n{\n\tint r;\n\tstruct md_rdev *rdev, *freshest;\n\tstruct mddev *mddev = &rs->md;\n\n\tfreshest = NULL;\n\trdev_for_each(rdev, mddev) {\n\t\tif (test_bit(Journal, &rdev->flags))\n\t\t\tcontinue;\n\n\t\tif (!rdev->meta_bdev)\n\t\t\tcontinue;\n\n\t\t \n\t\trdev->sb_start = 0;\n\t\trdev->sb_size = bdev_logical_block_size(rdev->meta_bdev);\n\t\tif (rdev->sb_size < sizeof(struct dm_raid_superblock) || rdev->sb_size > PAGE_SIZE) {\n\t\t\tDMERR(\"superblock size of a logical block is no longer valid\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\tif (test_bit(__CTR_FLAG_SYNC, &rs->ctr_flags))\n\t\t\tcontinue;\n\n\t\tr = super_load(rdev, freshest);\n\n\t\tswitch (r) {\n\t\tcase 1:\n\t\t\tfreshest = rdev;\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\t \n\t\t\tif (rs_is_raid0(rs))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\trdev->raid_disk = rdev->saved_raid_disk = -1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!freshest)\n\t\treturn 0;\n\n\t \n\trs->ti->error = \"Unable to assemble array: Invalid superblocks\";\n\tif (super_validate(rs, freshest))\n\t\treturn -EINVAL;\n\n\tif (validate_raid_redundancy(rs)) {\n\t\trs->ti->error = \"Insufficient redundancy to activate array\";\n\t\treturn -EINVAL;\n\t}\n\n\trdev_for_each(rdev, mddev)\n\t\tif (!test_bit(Journal, &rdev->flags) &&\n\t\t    rdev != freshest &&\n\t\t    super_validate(rs, rdev))\n\t\t\treturn -EINVAL;\n\treturn 0;\n}\n\n \nstatic int rs_adjust_data_offsets(struct raid_set *rs)\n{\n\tsector_t data_offset = 0, new_data_offset = 0;\n\tstruct md_rdev *rdev;\n\n\t \n\tif (!test_bit(__CTR_FLAG_DATA_OFFSET, &rs->ctr_flags)) {\n\t\tif (!rs_is_reshapable(rs))\n\t\t\tgoto out;\n\n\t\treturn 0;\n\t}\n\n\t \n\trdev = &rs->dev[0].rdev;\n\n\tif (rs->delta_disks < 0) {\n\t\t \n\t\tdata_offset = 0;\n\t\tnew_data_offset = rs->data_offset;\n\n\t} else if (rs->delta_disks > 0) {\n\t\t \n\t\tdata_offset = rs->data_offset;\n\t\tnew_data_offset = 0;\n\n\t} else {\n\t\t \n\t\tdata_offset = rs->data_offset ? rdev->data_offset : 0;\n\t\tnew_data_offset = data_offset ? 0 : rs->data_offset;\n\t\tset_bit(RT_FLAG_UPDATE_SBS, &rs->runtime_flags);\n\t}\n\n\t \n\tif (rs->data_offset &&\n\t    bdev_nr_sectors(rdev->bdev) - rs->md.dev_sectors < MIN_FREE_RESHAPE_SPACE) {\n\t\trs->ti->error = data_offset ? \"No space for forward reshape\" :\n\t\t\t\t\t      \"No space for backward reshape\";\n\t\treturn -ENOSPC;\n\t}\nout:\n\t \n\tif (rs->md.recovery_cp < rs->md.dev_sectors)\n\t\trs->md.recovery_cp += rs->dev[0].rdev.data_offset;\n\n\t \n\trdev_for_each(rdev, &rs->md) {\n\t\tif (!test_bit(Journal, &rdev->flags)) {\n\t\t\trdev->data_offset = data_offset;\n\t\t\trdev->new_data_offset = new_data_offset;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic void __reorder_raid_disk_indexes(struct raid_set *rs)\n{\n\tint i = 0;\n\tstruct md_rdev *rdev;\n\n\trdev_for_each(rdev, &rs->md) {\n\t\tif (!test_bit(Journal, &rdev->flags)) {\n\t\t\trdev->raid_disk = i++;\n\t\t\trdev->saved_raid_disk = rdev->new_raid_disk = -1;\n\t\t}\n\t}\n}\n\n \nstatic int rs_setup_takeover(struct raid_set *rs)\n{\n\tstruct mddev *mddev = &rs->md;\n\tstruct md_rdev *rdev;\n\tunsigned int d = mddev->raid_disks = rs->raid_disks;\n\tsector_t new_data_offset = rs->dev[0].rdev.data_offset ? 0 : rs->data_offset;\n\n\tif (rt_is_raid10(rs->raid_type)) {\n\t\tif (rs_is_raid0(rs)) {\n\t\t\t \n\t\t\t__reorder_raid_disk_indexes(rs);\n\n\t\t\t \n\t\t\tmddev->layout = raid10_format_to_md_layout(rs, ALGORITHM_RAID10_FAR,\n\t\t\t\t\t\t\t\t   rs->raid10_copies);\n\t\t} else if (rs_is_raid1(rs))\n\t\t\t \n\t\t\tmddev->layout = raid10_format_to_md_layout(rs, ALGORITHM_RAID10_NEAR,\n\t\t\t\t\t\t\t\t   rs->raid_disks);\n\t\telse\n\t\t\treturn -EINVAL;\n\n\t}\n\n\tclear_bit(MD_ARRAY_FIRST_USE, &mddev->flags);\n\tmddev->recovery_cp = MaxSector;\n\n\twhile (d--) {\n\t\trdev = &rs->dev[d].rdev;\n\n\t\tif (test_bit(d, (void *) rs->rebuild_disks)) {\n\t\t\tclear_bit(In_sync, &rdev->flags);\n\t\t\tclear_bit(Faulty, &rdev->flags);\n\t\t\tmddev->recovery_cp = rdev->recovery_offset = 0;\n\t\t\t \n\t\t\tset_bit(MD_ARRAY_FIRST_USE, &mddev->flags);\n\t\t}\n\n\t\trdev->new_data_offset = new_data_offset;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int rs_prepare_reshape(struct raid_set *rs)\n{\n\tbool reshape;\n\tstruct mddev *mddev = &rs->md;\n\n\tif (rs_is_raid10(rs)) {\n\t\tif (rs->raid_disks != mddev->raid_disks &&\n\t\t    __is_raid10_near(mddev->layout) &&\n\t\t    rs->raid10_copies &&\n\t\t    rs->raid10_copies != __raid10_near_copies(mddev->layout)) {\n\t\t\t \n\t\t\tif (rs->raid_disks % rs->raid10_copies) {\n\t\t\t\trs->ti->error = \"Can't reshape raid10 mirror groups\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\n\t\t\t \n\t\t\t__reorder_raid_disk_indexes(rs);\n\t\t\tmddev->layout = raid10_format_to_md_layout(rs, ALGORITHM_RAID10_NEAR,\n\t\t\t\t\t\t\t\t   rs->raid10_copies);\n\t\t\tmddev->new_layout = mddev->layout;\n\t\t\treshape = false;\n\t\t} else\n\t\t\treshape = true;\n\n\t} else if (rs_is_raid456(rs))\n\t\treshape = true;\n\n\telse if (rs_is_raid1(rs)) {\n\t\tif (rs->delta_disks) {\n\t\t\t \n\t\t\tmddev->degraded = rs->delta_disks < 0 ? -rs->delta_disks : rs->delta_disks;\n\t\t\treshape = true;\n\t\t} else {\n\t\t\t \n\t\t\tmddev->raid_disks = rs->raid_disks;\n\t\t\treshape = false;\n\t\t}\n\t} else {\n\t\trs->ti->error = \"Called with bogus raid type\";\n\t\treturn -EINVAL;\n\t}\n\n\tif (reshape) {\n\t\tset_bit(RT_FLAG_RESHAPE_RS, &rs->runtime_flags);\n\t\tset_bit(RT_FLAG_UPDATE_SBS, &rs->runtime_flags);\n\t} else if (mddev->raid_disks < rs->raid_disks)\n\t\t \n\t\tset_bit(RT_FLAG_UPDATE_SBS, &rs->runtime_flags);\n\n\treturn 0;\n}\n\n \nstatic sector_t _get_reshape_sectors(struct raid_set *rs)\n{\n\tstruct md_rdev *rdev;\n\tsector_t reshape_sectors = 0;\n\n\trdev_for_each(rdev, &rs->md)\n\t\tif (!test_bit(Journal, &rdev->flags)) {\n\t\t\treshape_sectors = (rdev->data_offset > rdev->new_data_offset) ?\n\t\t\t\t\trdev->data_offset - rdev->new_data_offset :\n\t\t\t\t\trdev->new_data_offset - rdev->data_offset;\n\t\t\tbreak;\n\t\t}\n\n\treturn max(reshape_sectors, (sector_t) rs->data_offset);\n}\n\n \nstatic int rs_setup_reshape(struct raid_set *rs)\n{\n\tint r = 0;\n\tunsigned int cur_raid_devs, d;\n\tsector_t reshape_sectors = _get_reshape_sectors(rs);\n\tstruct mddev *mddev = &rs->md;\n\tstruct md_rdev *rdev;\n\n\tmddev->delta_disks = rs->delta_disks;\n\tcur_raid_devs = mddev->raid_disks;\n\n\t \n\tif (mddev->delta_disks &&\n\t    mddev->layout != mddev->new_layout) {\n\t\tDMINFO(\"Ignoring invalid layout change with delta_disks=%d\", rs->delta_disks);\n\t\tmddev->new_layout = mddev->layout;\n\t}\n\n\t \n\n\t \n\tif (rs->delta_disks > 0) {\n\t\t \n\t\tfor (d = cur_raid_devs; d < rs->raid_disks; d++) {\n\t\t\trdev = &rs->dev[d].rdev;\n\t\t\tclear_bit(In_sync, &rdev->flags);\n\n\t\t\t \n\t\t\trdev->saved_raid_disk = -1;\n\t\t\trdev->raid_disk = d;\n\n\t\t\trdev->sectors = mddev->dev_sectors;\n\t\t\trdev->recovery_offset = rs_is_raid1(rs) ? 0 : MaxSector;\n\t\t}\n\n\t\tmddev->reshape_backwards = 0;  \n\n\t \n\t} else if (rs->delta_disks < 0) {\n\t\tr = rs_set_dev_and_array_sectors(rs, rs->ti->len, true);\n\t\tmddev->reshape_backwards = 1;  \n\n\t \n\t} else {\n\t\t \n\t\tmddev->reshape_backwards = rs->dev[0].rdev.data_offset ? 0 : 1;\n\t}\n\n\t \n\tif (!mddev->reshape_backwards)\n\t\trdev_for_each(rdev, &rs->md)\n\t\t\tif (!test_bit(Journal, &rdev->flags))\n\t\t\t\trdev->sectors += reshape_sectors;\n\n\treturn r;\n}\n\n \nstatic void rs_reset_inconclusive_reshape(struct raid_set *rs)\n{\n\tif (!rs_is_reshaping(rs) && rs_is_layout_change(rs, true)) {\n\t\trs_set_cur(rs);\n\t\trs->md.delta_disks = 0;\n\t\trs->md.reshape_backwards = 0;\n\t}\n}\n\n \nstatic void configure_discard_support(struct raid_set *rs)\n{\n\tint i;\n\tbool raid456;\n\tstruct dm_target *ti = rs->ti;\n\n\t \n\traid456 = rs_is_raid456(rs);\n\n\tfor (i = 0; i < rs->raid_disks; i++) {\n\t\tif (!rs->dev[i].rdev.bdev ||\n\t\t    !bdev_max_discard_sectors(rs->dev[i].rdev.bdev))\n\t\t\treturn;\n\n\t\tif (raid456) {\n\t\t\tif (!devices_handle_discard_safely) {\n\t\t\t\tDMERR(\"raid456 discard support disabled due to discard_zeroes_data uncertainty.\");\n\t\t\t\tDMERR(\"Set dm-raid.devices_handle_discard_safely=Y to override.\");\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n\n\tti->num_discard_bios = 1;\n}\n\n \nstatic int raid_ctr(struct dm_target *ti, unsigned int argc, char **argv)\n{\n\tint r;\n\tbool resize = false;\n\tstruct raid_type *rt;\n\tunsigned int num_raid_params, num_raid_devs;\n\tsector_t sb_array_sectors, rdev_sectors, reshape_sectors;\n\tstruct raid_set *rs = NULL;\n\tconst char *arg;\n\tstruct rs_layout rs_layout;\n\tstruct dm_arg_set as = { argc, argv }, as_nrd;\n\tstruct dm_arg _args[] = {\n\t\t{ 0, as.argc, \"Cannot understand number of raid parameters\" },\n\t\t{ 1, 254, \"Cannot understand number of raid devices parameters\" }\n\t};\n\n\targ = dm_shift_arg(&as);\n\tif (!arg) {\n\t\tti->error = \"No arguments\";\n\t\treturn -EINVAL;\n\t}\n\n\trt = get_raid_type(arg);\n\tif (!rt) {\n\t\tti->error = \"Unrecognised raid_type\";\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (dm_read_arg_group(_args, &as, &num_raid_params, &ti->error))\n\t\treturn -EINVAL;\n\n\t \n\tas_nrd = as;\n\tdm_consume_args(&as_nrd, num_raid_params);\n\t_args[1].max = (as_nrd.argc - 1) / 2;\n\tif (dm_read_arg(_args + 1, &as_nrd, &num_raid_devs, &ti->error))\n\t\treturn -EINVAL;\n\n\tif (!__within_range(num_raid_devs, 1, MAX_RAID_DEVICES)) {\n\t\tti->error = \"Invalid number of supplied raid devices\";\n\t\treturn -EINVAL;\n\t}\n\n\trs = raid_set_alloc(ti, rt, num_raid_devs);\n\tif (IS_ERR(rs))\n\t\treturn PTR_ERR(rs);\n\n\tr = parse_raid_params(rs, &as, num_raid_params);\n\tif (r)\n\t\tgoto bad;\n\n\tr = parse_dev_params(rs, &as);\n\tif (r)\n\t\tgoto bad;\n\n\trs->md.sync_super = super_sync;\n\n\t \n\tr = rs_set_dev_and_array_sectors(rs, rs->ti->len, false);\n\tif (r)\n\t\tgoto bad;\n\n\t \n\trs->array_sectors = rs->md.array_sectors;\n\trs->dev_sectors = rs->md.dev_sectors;\n\n\t \n\trs_config_backup(rs, &rs_layout);\n\n\tr = analyse_superblocks(ti, rs);\n\tif (r)\n\t\tgoto bad;\n\n\t \n\tsb_array_sectors = rs->md.array_sectors;\n\trdev_sectors = __rdev_sectors(rs);\n\tif (!rdev_sectors) {\n\t\tti->error = \"Invalid rdev size\";\n\t\tr = -EINVAL;\n\t\tgoto bad;\n\t}\n\n\n\treshape_sectors = _get_reshape_sectors(rs);\n\tif (rs->dev_sectors != rdev_sectors) {\n\t\tresize = (rs->dev_sectors != rdev_sectors - reshape_sectors);\n\t\tif (rs->dev_sectors > rdev_sectors - reshape_sectors)\n\t\t\tset_bit(RT_FLAG_RS_GROW, &rs->runtime_flags);\n\t}\n\n\tINIT_WORK(&rs->md.event_work, do_table_event);\n\tti->private = rs;\n\tti->num_flush_bios = 1;\n\tti->needs_bio_set_dev = true;\n\n\t \n\trs_config_restore(rs, &rs_layout);\n\n\t \n\tif (test_bit(MD_ARRAY_FIRST_USE, &rs->md.flags)) {\n\t\t \n\t\tif (rs_is_raid6(rs) &&\n\t\t    test_bit(__CTR_FLAG_NOSYNC, &rs->ctr_flags)) {\n\t\t\tti->error = \"'nosync' not allowed for new raid6 set\";\n\t\t\tr = -EINVAL;\n\t\t\tgoto bad;\n\t\t}\n\t\trs_setup_recovery(rs, 0);\n\t\tset_bit(RT_FLAG_UPDATE_SBS, &rs->runtime_flags);\n\t\trs_set_new(rs);\n\t} else if (rs_is_recovering(rs)) {\n\t\t \n\t\tgoto size_check;\n\t} else if (rs_is_reshaping(rs)) {\n\t\t \n\t\tif (resize) {\n\t\t\tti->error = \"Can't resize a reshaping raid set\";\n\t\t\tr = -EPERM;\n\t\t\tgoto bad;\n\t\t}\n\t\t \n\t} else if (rs_takeover_requested(rs)) {\n\t\tif (rs_is_reshaping(rs)) {\n\t\t\tti->error = \"Can't takeover a reshaping raid set\";\n\t\t\tr = -EPERM;\n\t\t\tgoto bad;\n\t\t}\n\n\t\t \n\t\tif (test_bit(__CTR_FLAG_JOURNAL_DEV, &rs->ctr_flags)) {\n\t\t\tti->error = \"Can't takeover a journaled raid4/5/6 set\";\n\t\t\tr = -EPERM;\n\t\t\tgoto bad;\n\t\t}\n\n\t\t \n\t\tr = rs_check_takeover(rs);\n\t\tif (r)\n\t\t\tgoto bad;\n\n\t\tr = rs_setup_takeover(rs);\n\t\tif (r)\n\t\t\tgoto bad;\n\n\t\tset_bit(RT_FLAG_UPDATE_SBS, &rs->runtime_flags);\n\t\t \n\t\trs_setup_recovery(rs, MaxSector);\n\t\trs_set_new(rs);\n\t} else if (rs_reshape_requested(rs)) {\n\t\t \n\t\tclear_bit(RT_FLAG_RS_GROW, &rs->runtime_flags);\n\n\t\t \n\n\t\t \n\t\tif (test_bit(__CTR_FLAG_JOURNAL_DEV, &rs->ctr_flags)) {\n\t\t\tti->error = \"Can't reshape a journaled raid4/5/6 set\";\n\t\t\tr = -EPERM;\n\t\t\tgoto bad;\n\t\t}\n\n\t\t \n\t\tif (reshape_sectors || rs_is_raid1(rs)) {\n\t\t\t \n\t\t\tr = rs_prepare_reshape(rs);\n\t\t\tif (r)\n\t\t\t\tgoto bad;\n\n\t\t\t \n\t\t\trs_setup_recovery(rs, MaxSector);\n\t\t}\n\t\trs_set_cur(rs);\n\t} else {\nsize_check:\n\t\t \n\t\tif (test_bit(__CTR_FLAG_REBUILD, &rs->ctr_flags)) {\n\t\t\tclear_bit(RT_FLAG_RS_GROW, &rs->runtime_flags);\n\t\t\tset_bit(RT_FLAG_UPDATE_SBS, &rs->runtime_flags);\n\t\t\trs_setup_recovery(rs, MaxSector);\n\t\t} else if (test_bit(RT_FLAG_RS_GROW, &rs->runtime_flags)) {\n\t\t\t \n\t\t\tr = rs_set_dev_and_array_sectors(rs, sb_array_sectors, false);\n\t\t\tif (r)\n\t\t\t\tgoto bad;\n\n\t\t\trs_setup_recovery(rs, rs->md.recovery_cp < rs->md.dev_sectors ? rs->md.recovery_cp : rs->md.dev_sectors);\n\t\t} else {\n\t\t\t \n\t\t\tr = rs_set_dev_and_array_sectors(rs, rs->ti->len, false);\n\t\t\tif (r)\n\t\t\t\tgoto bad;\n\n\t\t\tif (sb_array_sectors > rs->array_sectors)\n\t\t\t\tset_bit(RT_FLAG_UPDATE_SBS, &rs->runtime_flags);\n\t\t}\n\t\trs_set_cur(rs);\n\t}\n\n\t \n\tr = rs_adjust_data_offsets(rs);\n\tif (r)\n\t\tgoto bad;\n\n\t \n\trs_reset_inconclusive_reshape(rs);\n\n\t \n\trs->md.ro = 1;\n\trs->md.in_sync = 1;\n\n\t \n\tset_bit(MD_RECOVERY_FROZEN, &rs->md.recovery);\n\n\t \n\tmddev_lock_nointr(&rs->md);\n\tr = md_run(&rs->md);\n\trs->md.in_sync = 0;  \n\tif (r) {\n\t\tti->error = \"Failed to run raid array\";\n\t\tmddev_unlock(&rs->md);\n\t\tgoto bad;\n\t}\n\n\tr = md_start(&rs->md);\n\tif (r) {\n\t\tti->error = \"Failed to start raid array\";\n\t\tgoto bad_unlock;\n\t}\n\n\t \n\tif (test_bit(__CTR_FLAG_JOURNAL_MODE, &rs->ctr_flags)) {\n\t\tr = r5c_journal_mode_set(&rs->md, rs->journal_dev.mode);\n\t\tif (r) {\n\t\t\tti->error = \"Failed to set raid4/5/6 journal mode\";\n\t\t\tgoto bad_unlock;\n\t\t}\n\t}\n\n\tmddev_suspend(&rs->md);\n\tset_bit(RT_FLAG_RS_SUSPENDED, &rs->runtime_flags);\n\n\t \n\tif (rs_is_raid456(rs)) {\n\t\tr = rs_set_raid456_stripe_cache(rs);\n\t\tif (r)\n\t\t\tgoto bad_unlock;\n\t}\n\n\t \n\tif (test_bit(RT_FLAG_RESHAPE_RS, &rs->runtime_flags)) {\n\t\tr = rs_check_reshape(rs);\n\t\tif (r)\n\t\t\tgoto bad_unlock;\n\n\t\t \n\t\trs_config_restore(rs, &rs_layout);\n\n\t\tif (rs->md.pers->start_reshape) {\n\t\t\tr = rs->md.pers->check_reshape(&rs->md);\n\t\t\tif (r) {\n\t\t\t\tti->error = \"Reshape check failed\";\n\t\t\t\tgoto bad_unlock;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tconfigure_discard_support(rs);\n\n\tmddev_unlock(&rs->md);\n\treturn 0;\n\nbad_unlock:\n\tmd_stop(&rs->md);\n\tmddev_unlock(&rs->md);\nbad:\n\traid_set_free(rs);\n\n\treturn r;\n}\n\nstatic void raid_dtr(struct dm_target *ti)\n{\n\tstruct raid_set *rs = ti->private;\n\n\tmddev_lock_nointr(&rs->md);\n\tmd_stop(&rs->md);\n\tmddev_unlock(&rs->md);\n\traid_set_free(rs);\n}\n\nstatic int raid_map(struct dm_target *ti, struct bio *bio)\n{\n\tstruct raid_set *rs = ti->private;\n\tstruct mddev *mddev = &rs->md;\n\n\t \n\tif (unlikely(bio_end_sector(bio) > mddev->array_sectors))\n\t\treturn DM_MAPIO_REQUEUE;\n\n\tmd_handle_request(mddev, bio);\n\n\treturn DM_MAPIO_SUBMITTED;\n}\n\n \nenum sync_state { st_frozen, st_reshape, st_resync, st_check, st_repair, st_recover, st_idle };\nstatic const char *sync_str(enum sync_state state)\n{\n\t \n\tstatic const char *sync_strs[] = {\n\t\t\"frozen\",\n\t\t\"reshape\",\n\t\t\"resync\",\n\t\t\"check\",\n\t\t\"repair\",\n\t\t\"recover\",\n\t\t\"idle\"\n\t};\n\n\treturn __within_range(state, 0, ARRAY_SIZE(sync_strs) - 1) ? sync_strs[state] : \"undef\";\n};\n\n \nstatic enum sync_state decipher_sync_action(struct mddev *mddev, unsigned long recovery)\n{\n\tif (test_bit(MD_RECOVERY_FROZEN, &recovery))\n\t\treturn st_frozen;\n\n\t \n\tif (!test_bit(MD_RECOVERY_DONE, &recovery) &&\n\t    (test_bit(MD_RECOVERY_RUNNING, &recovery) ||\n\t     (!mddev->ro && test_bit(MD_RECOVERY_NEEDED, &recovery)))) {\n\t\tif (test_bit(MD_RECOVERY_RESHAPE, &recovery))\n\t\t\treturn st_reshape;\n\n\t\tif (test_bit(MD_RECOVERY_SYNC, &recovery)) {\n\t\t\tif (!test_bit(MD_RECOVERY_REQUESTED, &recovery))\n\t\t\t\treturn st_resync;\n\t\t\tif (test_bit(MD_RECOVERY_CHECK, &recovery))\n\t\t\t\treturn st_check;\n\t\t\treturn st_repair;\n\t\t}\n\n\t\tif (test_bit(MD_RECOVERY_RECOVER, &recovery))\n\t\t\treturn st_recover;\n\n\t\tif (mddev->reshape_position != MaxSector)\n\t\t\treturn st_reshape;\n\t}\n\n\treturn st_idle;\n}\n\n \nstatic const char *__raid_dev_status(struct raid_set *rs, struct md_rdev *rdev)\n{\n\tif (!rdev->bdev)\n\t\treturn \"-\";\n\telse if (test_bit(Faulty, &rdev->flags))\n\t\treturn \"D\";\n\telse if (test_bit(Journal, &rdev->flags))\n\t\treturn (rs->journal_dev.mode == R5C_JOURNAL_MODE_WRITE_THROUGH) ? \"A\" : \"a\";\n\telse if (test_bit(RT_FLAG_RS_RESYNCING, &rs->runtime_flags) ||\n\t\t (!test_bit(RT_FLAG_RS_IN_SYNC, &rs->runtime_flags) &&\n\t\t  !test_bit(In_sync, &rdev->flags)))\n\t\treturn \"a\";\n\telse\n\t\treturn \"A\";\n}\n\n \nstatic sector_t rs_get_progress(struct raid_set *rs, unsigned long recovery,\n\t\t\t\tenum sync_state state, sector_t resync_max_sectors)\n{\n\tsector_t r;\n\tstruct mddev *mddev = &rs->md;\n\n\tclear_bit(RT_FLAG_RS_IN_SYNC, &rs->runtime_flags);\n\tclear_bit(RT_FLAG_RS_RESYNCING, &rs->runtime_flags);\n\n\tif (rs_is_raid0(rs)) {\n\t\tr = resync_max_sectors;\n\t\tset_bit(RT_FLAG_RS_IN_SYNC, &rs->runtime_flags);\n\n\t} else {\n\t\tif (state == st_idle && !test_bit(MD_RECOVERY_INTR, &recovery))\n\t\t\tr = mddev->recovery_cp;\n\t\telse\n\t\t\tr = mddev->curr_resync_completed;\n\n\t\tif (state == st_idle && r >= resync_max_sectors) {\n\t\t\t \n\t\t\t \n\t\t\tif (test_bit(MD_RECOVERY_RECOVER, &recovery))\n\t\t\t\tset_bit(RT_FLAG_RS_IN_SYNC, &rs->runtime_flags);\n\n\t\t} else if (state == st_recover)\n\t\t\t \n\t\t\t;\n\n\t\telse if (state == st_resync || state == st_reshape)\n\t\t\t \n\t\t\tset_bit(RT_FLAG_RS_RESYNCING, &rs->runtime_flags);\n\n\t\telse if (state == st_check || state == st_repair)\n\t\t\t \n\t\t\tset_bit(RT_FLAG_RS_IN_SYNC, &rs->runtime_flags);\n\n\t\telse if (test_bit(MD_RECOVERY_NEEDED, &recovery))\n\t\t\t \n\t\t\tset_bit(RT_FLAG_RS_RESYNCING, &rs->runtime_flags);\n\n\t\telse {\n\t\t\t \n\t\t\tstruct md_rdev *rdev;\n\n\t\t\tset_bit(RT_FLAG_RS_IN_SYNC, &rs->runtime_flags);\n\t\t\trdev_for_each(rdev, mddev)\n\t\t\t\tif (!test_bit(Journal, &rdev->flags) &&\n\t\t\t\t    !test_bit(In_sync, &rdev->flags)) {\n\t\t\t\t\tclear_bit(RT_FLAG_RS_IN_SYNC, &rs->runtime_flags);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t}\n\t}\n\n\treturn min(r, resync_max_sectors);\n}\n\n \nstatic const char *__get_dev_name(struct dm_dev *dev)\n{\n\treturn dev ? dev->name : \"-\";\n}\n\nstatic void raid_status(struct dm_target *ti, status_type_t type,\n\t\t\tunsigned int status_flags, char *result, unsigned int maxlen)\n{\n\tstruct raid_set *rs = ti->private;\n\tstruct mddev *mddev = &rs->md;\n\tstruct r5conf *conf = rs_is_raid456(rs) ? mddev->private : NULL;\n\tint i, max_nr_stripes = conf ? conf->max_nr_stripes : 0;\n\tunsigned long recovery;\n\tunsigned int raid_param_cnt = 1;  \n\tunsigned int sz = 0;\n\tunsigned int rebuild_writemostly_count = 0;\n\tsector_t progress, resync_max_sectors, resync_mismatches;\n\tenum sync_state state;\n\tstruct raid_type *rt;\n\n\tswitch (type) {\n\tcase STATUSTYPE_INFO:\n\t\t \n\t\trt = get_raid_type_by_ll(mddev->new_level, mddev->new_layout);\n\t\tif (!rt)\n\t\t\treturn;\n\n\t\tDMEMIT(\"%s %d \", rt->name, mddev->raid_disks);\n\n\t\t \n\t\tsmp_rmb();\n\t\t \n\t\tresync_max_sectors = test_bit(RT_FLAG_RS_PRERESUMED, &rs->runtime_flags) ?\n\t\t\t\t      mddev->resync_max_sectors : mddev->dev_sectors;\n\t\trecovery = rs->md.recovery;\n\t\tstate = decipher_sync_action(mddev, recovery);\n\t\tprogress = rs_get_progress(rs, recovery, state, resync_max_sectors);\n\t\tresync_mismatches = (mddev->last_sync_action && !strcasecmp(mddev->last_sync_action, \"check\")) ?\n\t\t\t\t    atomic64_read(&mddev->resync_mismatches) : 0;\n\n\t\t \n\t\tfor (i = 0; i < rs->raid_disks; i++)\n\t\t\tDMEMIT(__raid_dev_status(rs, &rs->dev[i].rdev));\n\n\t\t \n\t\tDMEMIT(\" %llu/%llu\", (unsigned long long) progress,\n\t\t\t\t     (unsigned long long) resync_max_sectors);\n\n\t\t \n\t\tDMEMIT(\" %s\", sync_str(state));\n\n\t\t \n\t\tDMEMIT(\" %llu\", (unsigned long long) resync_mismatches);\n\n\t\t \n\t\tDMEMIT(\" %llu\", (unsigned long long) rs->dev[0].rdev.data_offset);\n\n\t\t \n\t\tDMEMIT(\" %s\", test_bit(__CTR_FLAG_JOURNAL_DEV, &rs->ctr_flags) ?\n\t\t\t      __raid_dev_status(rs, &rs->journal_dev.rdev) : \"-\");\n\t\tbreak;\n\n\tcase STATUSTYPE_TABLE:\n\t\t \n\n\t\t \n\t\tfor (i = 0; i < rs->raid_disks; i++) {\n\t\t\trebuild_writemostly_count += (test_bit(i, (void *) rs->rebuild_disks) ? 2 : 0) +\n\t\t\t\t\t\t     (test_bit(WriteMostly, &rs->dev[i].rdev.flags) ? 2 : 0);\n\t\t}\n\t\trebuild_writemostly_count -= (test_bit(__CTR_FLAG_REBUILD, &rs->ctr_flags) ? 2 : 0) +\n\t\t\t\t\t     (test_bit(__CTR_FLAG_WRITE_MOSTLY, &rs->ctr_flags) ? 2 : 0);\n\t\t \n\t\traid_param_cnt += rebuild_writemostly_count +\n\t\t\t\t  hweight32(rs->ctr_flags & CTR_FLAG_OPTIONS_NO_ARGS) +\n\t\t\t\t  hweight32(rs->ctr_flags & CTR_FLAG_OPTIONS_ONE_ARG) * 2;\n\t\t \n\t\t \n\t\tDMEMIT(\"%s %u %u\", rs->raid_type->name, raid_param_cnt, mddev->new_chunk_sectors);\n\t\tif (test_bit(__CTR_FLAG_SYNC, &rs->ctr_flags))\n\t\t\tDMEMIT(\" %s\", dm_raid_arg_name_by_flag(CTR_FLAG_SYNC));\n\t\tif (test_bit(__CTR_FLAG_NOSYNC, &rs->ctr_flags))\n\t\t\tDMEMIT(\" %s\", dm_raid_arg_name_by_flag(CTR_FLAG_NOSYNC));\n\t\tif (test_bit(__CTR_FLAG_REBUILD, &rs->ctr_flags))\n\t\t\tfor (i = 0; i < rs->raid_disks; i++)\n\t\t\t\tif (test_bit(i, (void *) rs->rebuild_disks))\n\t\t\t\t\tDMEMIT(\" %s %u\", dm_raid_arg_name_by_flag(CTR_FLAG_REBUILD), i);\n\t\tif (test_bit(__CTR_FLAG_DAEMON_SLEEP, &rs->ctr_flags))\n\t\t\tDMEMIT(\" %s %lu\", dm_raid_arg_name_by_flag(CTR_FLAG_DAEMON_SLEEP),\n\t\t\t\t\t  mddev->bitmap_info.daemon_sleep);\n\t\tif (test_bit(__CTR_FLAG_MIN_RECOVERY_RATE, &rs->ctr_flags))\n\t\t\tDMEMIT(\" %s %d\", dm_raid_arg_name_by_flag(CTR_FLAG_MIN_RECOVERY_RATE),\n\t\t\t\t\t mddev->sync_speed_min);\n\t\tif (test_bit(__CTR_FLAG_MAX_RECOVERY_RATE, &rs->ctr_flags))\n\t\t\tDMEMIT(\" %s %d\", dm_raid_arg_name_by_flag(CTR_FLAG_MAX_RECOVERY_RATE),\n\t\t\t\t\t mddev->sync_speed_max);\n\t\tif (test_bit(__CTR_FLAG_WRITE_MOSTLY, &rs->ctr_flags))\n\t\t\tfor (i = 0; i < rs->raid_disks; i++)\n\t\t\t\tif (test_bit(WriteMostly, &rs->dev[i].rdev.flags))\n\t\t\t\t\tDMEMIT(\" %s %d\", dm_raid_arg_name_by_flag(CTR_FLAG_WRITE_MOSTLY),\n\t\t\t\t\t       rs->dev[i].rdev.raid_disk);\n\t\tif (test_bit(__CTR_FLAG_MAX_WRITE_BEHIND, &rs->ctr_flags))\n\t\t\tDMEMIT(\" %s %lu\", dm_raid_arg_name_by_flag(CTR_FLAG_MAX_WRITE_BEHIND),\n\t\t\t\t\t  mddev->bitmap_info.max_write_behind);\n\t\tif (test_bit(__CTR_FLAG_STRIPE_CACHE, &rs->ctr_flags))\n\t\t\tDMEMIT(\" %s %d\", dm_raid_arg_name_by_flag(CTR_FLAG_STRIPE_CACHE),\n\t\t\t\t\t max_nr_stripes);\n\t\tif (test_bit(__CTR_FLAG_REGION_SIZE, &rs->ctr_flags))\n\t\t\tDMEMIT(\" %s %llu\", dm_raid_arg_name_by_flag(CTR_FLAG_REGION_SIZE),\n\t\t\t\t\t   (unsigned long long) to_sector(mddev->bitmap_info.chunksize));\n\t\tif (test_bit(__CTR_FLAG_RAID10_COPIES, &rs->ctr_flags))\n\t\t\tDMEMIT(\" %s %d\", dm_raid_arg_name_by_flag(CTR_FLAG_RAID10_COPIES),\n\t\t\t\t\t raid10_md_layout_to_copies(mddev->layout));\n\t\tif (test_bit(__CTR_FLAG_RAID10_FORMAT, &rs->ctr_flags))\n\t\t\tDMEMIT(\" %s %s\", dm_raid_arg_name_by_flag(CTR_FLAG_RAID10_FORMAT),\n\t\t\t\t\t raid10_md_layout_to_format(mddev->layout));\n\t\tif (test_bit(__CTR_FLAG_DELTA_DISKS, &rs->ctr_flags))\n\t\t\tDMEMIT(\" %s %d\", dm_raid_arg_name_by_flag(CTR_FLAG_DELTA_DISKS),\n\t\t\t\t\t max(rs->delta_disks, mddev->delta_disks));\n\t\tif (test_bit(__CTR_FLAG_DATA_OFFSET, &rs->ctr_flags))\n\t\t\tDMEMIT(\" %s %llu\", dm_raid_arg_name_by_flag(CTR_FLAG_DATA_OFFSET),\n\t\t\t\t\t   (unsigned long long) rs->data_offset);\n\t\tif (test_bit(__CTR_FLAG_JOURNAL_DEV, &rs->ctr_flags))\n\t\t\tDMEMIT(\" %s %s\", dm_raid_arg_name_by_flag(CTR_FLAG_JOURNAL_DEV),\n\t\t\t\t\t__get_dev_name(rs->journal_dev.dev));\n\t\tif (test_bit(__CTR_FLAG_JOURNAL_MODE, &rs->ctr_flags))\n\t\t\tDMEMIT(\" %s %s\", dm_raid_arg_name_by_flag(CTR_FLAG_JOURNAL_MODE),\n\t\t\t\t\t md_journal_mode_to_dm_raid(rs->journal_dev.mode));\n\t\tDMEMIT(\" %d\", rs->raid_disks);\n\t\tfor (i = 0; i < rs->raid_disks; i++)\n\t\t\tDMEMIT(\" %s %s\", __get_dev_name(rs->dev[i].meta_dev),\n\t\t\t\t\t __get_dev_name(rs->dev[i].data_dev));\n\t\tbreak;\n\n\tcase STATUSTYPE_IMA:\n\t\trt = get_raid_type_by_ll(mddev->new_level, mddev->new_layout);\n\t\tif (!rt)\n\t\t\treturn;\n\n\t\tDMEMIT_TARGET_NAME_VERSION(ti->type);\n\t\tDMEMIT(\",raid_type=%s,raid_disks=%d\", rt->name, mddev->raid_disks);\n\n\t\t \n\t\tsmp_rmb();\n\t\trecovery = rs->md.recovery;\n\t\tstate = decipher_sync_action(mddev, recovery);\n\t\tDMEMIT(\",raid_state=%s\", sync_str(state));\n\n\t\tfor (i = 0; i < rs->raid_disks; i++) {\n\t\t\tDMEMIT(\",raid_device_%d_status=\", i);\n\t\t\tDMEMIT(__raid_dev_status(rs, &rs->dev[i].rdev));\n\t\t}\n\n\t\tif (rt_is_raid456(rt)) {\n\t\t\tDMEMIT(\",journal_dev_mode=\");\n\t\t\tswitch (rs->journal_dev.mode) {\n\t\t\tcase R5C_JOURNAL_MODE_WRITE_THROUGH:\n\t\t\t\tDMEMIT(\"%s\",\n\t\t\t\t       _raid456_journal_mode[R5C_JOURNAL_MODE_WRITE_THROUGH].param);\n\t\t\t\tbreak;\n\t\t\tcase R5C_JOURNAL_MODE_WRITE_BACK:\n\t\t\t\tDMEMIT(\"%s\",\n\t\t\t\t       _raid456_journal_mode[R5C_JOURNAL_MODE_WRITE_BACK].param);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tDMEMIT(\"invalid\");\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tDMEMIT(\";\");\n\t\tbreak;\n\t}\n}\n\nstatic int raid_message(struct dm_target *ti, unsigned int argc, char **argv,\n\t\t\tchar *result, unsigned int maxlen)\n{\n\tstruct raid_set *rs = ti->private;\n\tstruct mddev *mddev = &rs->md;\n\n\tif (!mddev->pers || !mddev->pers->sync_request)\n\t\treturn -EINVAL;\n\n\tif (!strcasecmp(argv[0], \"frozen\"))\n\t\tset_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\telse\n\t\tclear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\n\tif (!strcasecmp(argv[0], \"idle\") || !strcasecmp(argv[0], \"frozen\")) {\n\t\tif (mddev->sync_thread) {\n\t\t\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\t\t\tmd_reap_sync_thread(mddev);\n\t\t}\n\t} else if (decipher_sync_action(mddev, mddev->recovery) != st_idle)\n\t\treturn -EBUSY;\n\telse if (!strcasecmp(argv[0], \"resync\"))\n\t\t;  \n\telse if (!strcasecmp(argv[0], \"recover\"))\n\t\tset_bit(MD_RECOVERY_RECOVER, &mddev->recovery);\n\telse {\n\t\tif (!strcasecmp(argv[0], \"check\")) {\n\t\t\tset_bit(MD_RECOVERY_CHECK, &mddev->recovery);\n\t\t\tset_bit(MD_RECOVERY_REQUESTED, &mddev->recovery);\n\t\t\tset_bit(MD_RECOVERY_SYNC, &mddev->recovery);\n\t\t} else if (!strcasecmp(argv[0], \"repair\")) {\n\t\t\tset_bit(MD_RECOVERY_REQUESTED, &mddev->recovery);\n\t\t\tset_bit(MD_RECOVERY_SYNC, &mddev->recovery);\n\t\t} else\n\t\t\treturn -EINVAL;\n\t}\n\tif (mddev->ro == 2) {\n\t\t \n\t\tmddev->ro = 0;\n\t\tif (!mddev->suspended)\n\t\t\tmd_wakeup_thread(mddev->sync_thread);\n\t}\n\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\tif (!mddev->suspended)\n\t\tmd_wakeup_thread(mddev->thread);\n\n\treturn 0;\n}\n\nstatic int raid_iterate_devices(struct dm_target *ti,\n\t\t\t\titerate_devices_callout_fn fn, void *data)\n{\n\tstruct raid_set *rs = ti->private;\n\tunsigned int i;\n\tint r = 0;\n\n\tfor (i = 0; !r && i < rs->raid_disks; i++) {\n\t\tif (rs->dev[i].data_dev) {\n\t\t\tr = fn(ti, rs->dev[i].data_dev,\n\t\t\t       0,  \n\t\t\t       rs->md.dev_sectors, data);\n\t\t}\n\t}\n\n\treturn r;\n}\n\nstatic void raid_io_hints(struct dm_target *ti, struct queue_limits *limits)\n{\n\tstruct raid_set *rs = ti->private;\n\tunsigned int chunk_size_bytes = to_bytes(rs->md.chunk_sectors);\n\n\tblk_limits_io_min(limits, chunk_size_bytes);\n\tblk_limits_io_opt(limits, chunk_size_bytes * mddev_data_stripes(rs));\n}\n\nstatic void raid_postsuspend(struct dm_target *ti)\n{\n\tstruct raid_set *rs = ti->private;\n\n\tif (!test_and_set_bit(RT_FLAG_RS_SUSPENDED, &rs->runtime_flags)) {\n\t\t \n\t\tif (!test_bit(MD_RECOVERY_FROZEN, &rs->md.recovery))\n\t\t\tmd_stop_writes(&rs->md);\n\n\t\tmddev_lock_nointr(&rs->md);\n\t\tmddev_suspend(&rs->md);\n\t\tmddev_unlock(&rs->md);\n\t}\n}\n\nstatic void attempt_restore_of_faulty_devices(struct raid_set *rs)\n{\n\tint i;\n\tuint64_t cleared_failed_devices[DISKS_ARRAY_ELEMS];\n\tunsigned long flags;\n\tbool cleared = false;\n\tstruct dm_raid_superblock *sb;\n\tstruct mddev *mddev = &rs->md;\n\tstruct md_rdev *r;\n\n\t \n\tif (!mddev->pers || !mddev->pers->hot_add_disk || !mddev->pers->hot_remove_disk)\n\t\treturn;\n\n\tmemset(cleared_failed_devices, 0, sizeof(cleared_failed_devices));\n\n\tfor (i = 0; i < rs->raid_disks; i++) {\n\t\tr = &rs->dev[i].rdev;\n\t\t \n\t\tif (test_bit(Journal, &r->flags))\n\t\t\tcontinue;\n\n\t\tif (test_bit(Faulty, &r->flags) &&\n\t\t    r->meta_bdev && !read_disk_sb(r, r->sb_size, true)) {\n\t\t\tDMINFO(\"Faulty %s device #%d has readable super block.\"\n\t\t\t       \"  Attempting to revive it.\",\n\t\t\t       rs->raid_type->name, i);\n\n\t\t\t \n\t\t\tflags = r->flags;\n\t\t\tclear_bit(In_sync, &r->flags);  \n\t\t\tif (r->raid_disk >= 0) {\n\t\t\t\tif (mddev->pers->hot_remove_disk(mddev, r)) {\n\t\t\t\t\t \n\t\t\t\t\tr->flags = flags;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t} else\n\t\t\t\tr->raid_disk = r->saved_raid_disk = i;\n\n\t\t\tclear_bit(Faulty, &r->flags);\n\t\t\tclear_bit(WriteErrorSeen, &r->flags);\n\n\t\t\tif (mddev->pers->hot_add_disk(mddev, r)) {\n\t\t\t\t \n\t\t\t\tr->raid_disk = r->saved_raid_disk = -1;\n\t\t\t\tr->flags = flags;\n\t\t\t} else {\n\t\t\t\tclear_bit(In_sync, &r->flags);\n\t\t\t\tr->recovery_offset = 0;\n\t\t\t\tset_bit(i, (void *) cleared_failed_devices);\n\t\t\t\tcleared = true;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (cleared) {\n\t\tuint64_t failed_devices[DISKS_ARRAY_ELEMS];\n\n\t\trdev_for_each(r, &rs->md) {\n\t\t\tif (test_bit(Journal, &r->flags))\n\t\t\t\tcontinue;\n\n\t\t\tsb = page_address(r->sb_page);\n\t\t\tsb_retrieve_failed_devices(sb, failed_devices);\n\n\t\t\tfor (i = 0; i < DISKS_ARRAY_ELEMS; i++)\n\t\t\t\tfailed_devices[i] &= ~cleared_failed_devices[i];\n\n\t\t\tsb_update_failed_devices(sb, failed_devices);\n\t\t}\n\t}\n}\n\nstatic int __load_dirty_region_bitmap(struct raid_set *rs)\n{\n\tint r = 0;\n\n\t \n\tif (!rs_is_raid0(rs) &&\n\t    !test_and_set_bit(RT_FLAG_RS_BITMAP_LOADED, &rs->runtime_flags)) {\n\t\tr = md_bitmap_load(&rs->md);\n\t\tif (r)\n\t\t\tDMERR(\"Failed to load bitmap\");\n\t}\n\n\treturn r;\n}\n\n \nstatic void rs_update_sbs(struct raid_set *rs)\n{\n\tstruct mddev *mddev = &rs->md;\n\tint ro = mddev->ro;\n\n\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\tmddev->ro = 0;\n\tmd_update_sb(mddev, 1);\n\tmddev->ro = ro;\n}\n\n \nstatic int rs_start_reshape(struct raid_set *rs)\n{\n\tint r;\n\tstruct mddev *mddev = &rs->md;\n\tstruct md_personality *pers = mddev->pers;\n\n\t \n\tset_bit(MD_RECOVERY_WAIT, &mddev->recovery);\n\n\tr = rs_setup_reshape(rs);\n\tif (r)\n\t\treturn r;\n\n\t \n\tr = pers->check_reshape(mddev);\n\tif (r) {\n\t\trs->ti->error = \"pers->check_reshape() failed\";\n\t\treturn r;\n\t}\n\n\t \n\tif (pers->start_reshape) {\n\t\tr = pers->start_reshape(mddev);\n\t\tif (r) {\n\t\t\trs->ti->error = \"pers->start_reshape() failed\";\n\t\t\treturn r;\n\t\t}\n\t}\n\n\t \n\trs_update_sbs(rs);\n\n\treturn 0;\n}\n\nstatic int raid_preresume(struct dm_target *ti)\n{\n\tint r;\n\tstruct raid_set *rs = ti->private;\n\tstruct mddev *mddev = &rs->md;\n\n\t \n\tif (test_and_set_bit(RT_FLAG_RS_PRERESUMED, &rs->runtime_flags))\n\t\treturn 0;\n\n\t \n\tif (test_bit(RT_FLAG_UPDATE_SBS, &rs->runtime_flags))\n\t\trs_update_sbs(rs);\n\n\t \n\tr = __load_dirty_region_bitmap(rs);\n\tif (r)\n\t\treturn r;\n\n\t \n\tif (test_bit(RT_FLAG_RS_GROW, &rs->runtime_flags)) {\n\t\tmddev->array_sectors = rs->array_sectors;\n\t\tmddev->dev_sectors = rs->dev_sectors;\n\t\trs_set_rdev_sectors(rs);\n\t\trs_set_capacity(rs);\n\t}\n\n\t \n\tif (test_bit(RT_FLAG_RS_BITMAP_LOADED, &rs->runtime_flags) && mddev->bitmap &&\n\t    (test_bit(RT_FLAG_RS_GROW, &rs->runtime_flags) ||\n\t     (rs->requested_bitmap_chunk_sectors &&\n\t       mddev->bitmap_info.chunksize != to_bytes(rs->requested_bitmap_chunk_sectors)))) {\n\t\tint chunksize = to_bytes(rs->requested_bitmap_chunk_sectors) ?: mddev->bitmap_info.chunksize;\n\n\t\tr = md_bitmap_resize(mddev->bitmap, mddev->dev_sectors, chunksize, 0);\n\t\tif (r)\n\t\t\tDMERR(\"Failed to resize bitmap\");\n\t}\n\n\t \n\t \n\tset_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\tif (mddev->recovery_cp && mddev->recovery_cp < MaxSector) {\n\t\tset_bit(MD_RECOVERY_REQUESTED, &mddev->recovery);\n\t\tmddev->resync_min = mddev->recovery_cp;\n\t\tif (test_bit(RT_FLAG_RS_GROW, &rs->runtime_flags))\n\t\t\tmddev->resync_max_sectors = mddev->dev_sectors;\n\t}\n\n\t \n\tif (test_bit(RT_FLAG_RESHAPE_RS, &rs->runtime_flags)) {\n\t\t \n\t\trs_set_rdev_sectors(rs);\n\t\tmddev_lock_nointr(mddev);\n\t\tr = rs_start_reshape(rs);\n\t\tmddev_unlock(mddev);\n\t\tif (r)\n\t\t\tDMWARN(\"Failed to check/start reshape, continuing without change\");\n\t\tr = 0;\n\t}\n\n\treturn r;\n}\n\nstatic void raid_resume(struct dm_target *ti)\n{\n\tstruct raid_set *rs = ti->private;\n\tstruct mddev *mddev = &rs->md;\n\n\tif (test_and_set_bit(RT_FLAG_RS_RESUMED, &rs->runtime_flags)) {\n\t\t \n\t\tattempt_restore_of_faulty_devices(rs);\n\t}\n\n\tif (test_and_clear_bit(RT_FLAG_RS_SUSPENDED, &rs->runtime_flags)) {\n\t\t \n\t\tif (mddev->delta_disks < 0)\n\t\t\trs_set_capacity(rs);\n\n\t\tmddev_lock_nointr(mddev);\n\t\tclear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\t\tmddev->ro = 0;\n\t\tmddev->in_sync = 0;\n\t\tmddev_resume(mddev);\n\t\tmddev_unlock(mddev);\n\t}\n}\n\nstatic struct target_type raid_target = {\n\t.name = \"raid\",\n\t.version = {1, 15, 1},\n\t.module = THIS_MODULE,\n\t.ctr = raid_ctr,\n\t.dtr = raid_dtr,\n\t.map = raid_map,\n\t.status = raid_status,\n\t.message = raid_message,\n\t.iterate_devices = raid_iterate_devices,\n\t.io_hints = raid_io_hints,\n\t.postsuspend = raid_postsuspend,\n\t.preresume = raid_preresume,\n\t.resume = raid_resume,\n};\nmodule_dm(raid);\n\nmodule_param(devices_handle_discard_safely, bool, 0644);\nMODULE_PARM_DESC(devices_handle_discard_safely,\n\t\t \"Set to Y if all devices in each array reliably return zeroes on reads from discarded regions\");\n\nMODULE_DESCRIPTION(DM_NAME \" raid0/1/10/4/5/6 target\");\nMODULE_ALIAS(\"dm-raid0\");\nMODULE_ALIAS(\"dm-raid1\");\nMODULE_ALIAS(\"dm-raid10\");\nMODULE_ALIAS(\"dm-raid4\");\nMODULE_ALIAS(\"dm-raid5\");\nMODULE_ALIAS(\"dm-raid6\");\nMODULE_AUTHOR(\"Neil Brown <dm-devel@redhat.com>\");\nMODULE_AUTHOR(\"Heinz Mauelshagen <dm-devel@redhat.com>\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}