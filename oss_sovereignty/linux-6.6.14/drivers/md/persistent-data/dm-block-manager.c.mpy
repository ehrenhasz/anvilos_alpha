{
  "module_name": "dm-block-manager.c",
  "hash_id": "e5b7aaa83df7b16c4cbe3036068008d99dced24cdda3fb6fcc7231cce4e7024c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/persistent-data/dm-block-manager.c",
  "human_readable_source": "\n \n#include \"dm-block-manager.h\"\n#include \"dm-persistent-data-internal.h\"\n\n#include <linux/dm-bufio.h>\n#include <linux/crc32c.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/rwsem.h>\n#include <linux/device-mapper.h>\n#include <linux/stacktrace.h>\n#include <linux/sched/task.h>\n\n#define DM_MSG_PREFIX \"block manager\"\n\n \n\n#ifdef CONFIG_DM_DEBUG_BLOCK_MANAGER_LOCKING\n\n \n#define MAX_HOLDERS 4\n#define MAX_STACK 10\n\nstruct stack_store {\n\tunsigned int\tnr_entries;\n\tunsigned long\tentries[MAX_STACK];\n};\n\nstruct block_lock {\n\tspinlock_t lock;\n\t__s32 count;\n\tstruct list_head waiters;\n\tstruct task_struct *holders[MAX_HOLDERS];\n\n#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING\n\tstruct stack_store traces[MAX_HOLDERS];\n#endif\n};\n\nstruct waiter {\n\tstruct list_head list;\n\tstruct task_struct *task;\n\tint wants_write;\n};\n\nstatic unsigned int __find_holder(struct block_lock *lock,\n\t\t\t      struct task_struct *task)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < MAX_HOLDERS; i++)\n\t\tif (lock->holders[i] == task)\n\t\t\tbreak;\n\n\tBUG_ON(i == MAX_HOLDERS);\n\treturn i;\n}\n\n \nstatic void __add_holder(struct block_lock *lock, struct task_struct *task)\n{\n\tunsigned int h = __find_holder(lock, NULL);\n#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING\n\tstruct stack_store *t;\n#endif\n\n\tget_task_struct(task);\n\tlock->holders[h] = task;\n\n#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING\n\tt = lock->traces + h;\n\tt->nr_entries = stack_trace_save(t->entries, MAX_STACK, 2);\n#endif\n}\n\n \nstatic void __del_holder(struct block_lock *lock, struct task_struct *task)\n{\n\tunsigned int h = __find_holder(lock, task);\n\n\tlock->holders[h] = NULL;\n\tput_task_struct(task);\n}\n\nstatic int __check_holder(struct block_lock *lock)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < MAX_HOLDERS; i++) {\n\t\tif (lock->holders[i] == current) {\n\t\t\tDMERR(\"recursive lock detected in metadata\");\n#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING\n\t\t\tDMERR(\"previously held here:\");\n\t\t\tstack_trace_print(lock->traces[i].entries,\n\t\t\t\t\t  lock->traces[i].nr_entries, 4);\n\n\t\t\tDMERR(\"subsequent acquisition attempted here:\");\n\t\t\tdump_stack();\n#endif\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void __wait(struct waiter *w)\n{\n\tfor (;;) {\n\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\n\t\tif (!w->task)\n\t\t\tbreak;\n\n\t\tschedule();\n\t}\n\n\tset_current_state(TASK_RUNNING);\n}\n\nstatic void __wake_waiter(struct waiter *w)\n{\n\tstruct task_struct *task;\n\n\tlist_del(&w->list);\n\ttask = w->task;\n\tsmp_mb();\n\tw->task = NULL;\n\twake_up_process(task);\n}\n\n \nstatic void __wake_many(struct block_lock *lock)\n{\n\tstruct waiter *w, *tmp;\n\n\tBUG_ON(lock->count < 0);\n\tlist_for_each_entry_safe(w, tmp, &lock->waiters, list) {\n\t\tif (lock->count >= MAX_HOLDERS)\n\t\t\treturn;\n\n\t\tif (w->wants_write) {\n\t\t\tif (lock->count > 0)\n\t\t\t\treturn;  \n\n\t\t\tlock->count = -1;\n\t\t\t__add_holder(lock, w->task);\n\t\t\t__wake_waiter(w);\n\t\t\treturn;\n\t\t}\n\n\t\tlock->count++;\n\t\t__add_holder(lock, w->task);\n\t\t__wake_waiter(w);\n\t}\n}\n\nstatic void bl_init(struct block_lock *lock)\n{\n\tint i;\n\n\tspin_lock_init(&lock->lock);\n\tlock->count = 0;\n\tINIT_LIST_HEAD(&lock->waiters);\n\tfor (i = 0; i < MAX_HOLDERS; i++)\n\t\tlock->holders[i] = NULL;\n}\n\nstatic int __available_for_read(struct block_lock *lock)\n{\n\treturn lock->count >= 0 &&\n\t\tlock->count < MAX_HOLDERS &&\n\t\tlist_empty(&lock->waiters);\n}\n\nstatic int bl_down_read(struct block_lock *lock)\n{\n\tint r;\n\tstruct waiter w;\n\n\tspin_lock(&lock->lock);\n\tr = __check_holder(lock);\n\tif (r) {\n\t\tspin_unlock(&lock->lock);\n\t\treturn r;\n\t}\n\n\tif (__available_for_read(lock)) {\n\t\tlock->count++;\n\t\t__add_holder(lock, current);\n\t\tspin_unlock(&lock->lock);\n\t\treturn 0;\n\t}\n\n\tget_task_struct(current);\n\n\tw.task = current;\n\tw.wants_write = 0;\n\tlist_add_tail(&w.list, &lock->waiters);\n\tspin_unlock(&lock->lock);\n\n\t__wait(&w);\n\tput_task_struct(current);\n\treturn 0;\n}\n\nstatic int bl_down_read_nonblock(struct block_lock *lock)\n{\n\tint r;\n\n\tspin_lock(&lock->lock);\n\tr = __check_holder(lock);\n\tif (r)\n\t\tgoto out;\n\n\tif (__available_for_read(lock)) {\n\t\tlock->count++;\n\t\t__add_holder(lock, current);\n\t\tr = 0;\n\t} else\n\t\tr = -EWOULDBLOCK;\n\nout:\n\tspin_unlock(&lock->lock);\n\treturn r;\n}\n\nstatic void bl_up_read(struct block_lock *lock)\n{\n\tspin_lock(&lock->lock);\n\tBUG_ON(lock->count <= 0);\n\t__del_holder(lock, current);\n\t--lock->count;\n\tif (!list_empty(&lock->waiters))\n\t\t__wake_many(lock);\n\tspin_unlock(&lock->lock);\n}\n\nstatic int bl_down_write(struct block_lock *lock)\n{\n\tint r;\n\tstruct waiter w;\n\n\tspin_lock(&lock->lock);\n\tr = __check_holder(lock);\n\tif (r) {\n\t\tspin_unlock(&lock->lock);\n\t\treturn r;\n\t}\n\n\tif (lock->count == 0 && list_empty(&lock->waiters)) {\n\t\tlock->count = -1;\n\t\t__add_holder(lock, current);\n\t\tspin_unlock(&lock->lock);\n\t\treturn 0;\n\t}\n\n\tget_task_struct(current);\n\tw.task = current;\n\tw.wants_write = 1;\n\n\t \n\tlist_add(&w.list, &lock->waiters);\n\tspin_unlock(&lock->lock);\n\n\t__wait(&w);\n\tput_task_struct(current);\n\n\treturn 0;\n}\n\nstatic void bl_up_write(struct block_lock *lock)\n{\n\tspin_lock(&lock->lock);\n\t__del_holder(lock, current);\n\tlock->count = 0;\n\tif (!list_empty(&lock->waiters))\n\t\t__wake_many(lock);\n\tspin_unlock(&lock->lock);\n}\n\nstatic void report_recursive_bug(dm_block_t b, int r)\n{\n\tif (r == -EINVAL)\n\t\tDMERR(\"recursive acquisition of block %llu requested.\",\n\t\t      (unsigned long long) b);\n}\n\n#else   \n\n#define bl_init(x) do { } while (0)\n#define bl_down_read(x) 0\n#define bl_down_read_nonblock(x) 0\n#define bl_up_read(x) do { } while (0)\n#define bl_down_write(x) 0\n#define bl_up_write(x) do { } while (0)\n#define report_recursive_bug(x, y) do { } while (0)\n\n#endif  \n\n \n\n \nstatic struct dm_buffer *to_buffer(struct dm_block *b)\n{\n\treturn (struct dm_buffer *) b;\n}\n\ndm_block_t dm_block_location(struct dm_block *b)\n{\n\treturn dm_bufio_get_block_number(to_buffer(b));\n}\nEXPORT_SYMBOL_GPL(dm_block_location);\n\nvoid *dm_block_data(struct dm_block *b)\n{\n\treturn dm_bufio_get_block_data(to_buffer(b));\n}\nEXPORT_SYMBOL_GPL(dm_block_data);\n\nstruct buffer_aux {\n\tstruct dm_block_validator *validator;\n\tint write_locked;\n\n#ifdef CONFIG_DM_DEBUG_BLOCK_MANAGER_LOCKING\n\tstruct block_lock lock;\n#endif\n};\n\nstatic void dm_block_manager_alloc_callback(struct dm_buffer *buf)\n{\n\tstruct buffer_aux *aux = dm_bufio_get_aux_data(buf);\n\n\taux->validator = NULL;\n\tbl_init(&aux->lock);\n}\n\nstatic void dm_block_manager_write_callback(struct dm_buffer *buf)\n{\n\tstruct buffer_aux *aux = dm_bufio_get_aux_data(buf);\n\n\tif (aux->validator) {\n\t\taux->validator->prepare_for_write(aux->validator, (struct dm_block *) buf,\n\t\t\t dm_bufio_get_block_size(dm_bufio_get_client(buf)));\n\t}\n}\n\n \nstruct dm_block_manager {\n\tstruct dm_bufio_client *bufio;\n\tbool read_only:1;\n};\n\nstruct dm_block_manager *dm_block_manager_create(struct block_device *bdev,\n\t\t\t\t\t\t unsigned int block_size,\n\t\t\t\t\t\t unsigned int max_held_per_thread)\n{\n\tint r;\n\tstruct dm_block_manager *bm;\n\n\tbm = kmalloc(sizeof(*bm), GFP_KERNEL);\n\tif (!bm) {\n\t\tr = -ENOMEM;\n\t\tgoto bad;\n\t}\n\n\tbm->bufio = dm_bufio_client_create(bdev, block_size, max_held_per_thread,\n\t\t\t\t\t   sizeof(struct buffer_aux),\n\t\t\t\t\t   dm_block_manager_alloc_callback,\n\t\t\t\t\t   dm_block_manager_write_callback,\n\t\t\t\t\t   0);\n\tif (IS_ERR(bm->bufio)) {\n\t\tr = PTR_ERR(bm->bufio);\n\t\tkfree(bm);\n\t\tgoto bad;\n\t}\n\n\tbm->read_only = false;\n\n\treturn bm;\n\nbad:\n\treturn ERR_PTR(r);\n}\nEXPORT_SYMBOL_GPL(dm_block_manager_create);\n\nvoid dm_block_manager_destroy(struct dm_block_manager *bm)\n{\n\tdm_bufio_client_destroy(bm->bufio);\n\tkfree(bm);\n}\nEXPORT_SYMBOL_GPL(dm_block_manager_destroy);\n\nvoid dm_block_manager_reset(struct dm_block_manager *bm)\n{\n\tdm_bufio_client_reset(bm->bufio);\n}\nEXPORT_SYMBOL_GPL(dm_block_manager_reset);\n\nunsigned int dm_bm_block_size(struct dm_block_manager *bm)\n{\n\treturn dm_bufio_get_block_size(bm->bufio);\n}\nEXPORT_SYMBOL_GPL(dm_bm_block_size);\n\ndm_block_t dm_bm_nr_blocks(struct dm_block_manager *bm)\n{\n\treturn dm_bufio_get_device_size(bm->bufio);\n}\n\nstatic int dm_bm_validate_buffer(struct dm_block_manager *bm,\n\t\t\t\t struct dm_buffer *buf,\n\t\t\t\t struct buffer_aux *aux,\n\t\t\t\t struct dm_block_validator *v)\n{\n\tif (unlikely(!aux->validator)) {\n\t\tint r;\n\n\t\tif (!v)\n\t\t\treturn 0;\n\t\tr = v->check(v, (struct dm_block *) buf, dm_bufio_get_block_size(bm->bufio));\n\t\tif (unlikely(r)) {\n\t\t\tDMERR_LIMIT(\"%s validator check failed for block %llu\", v->name,\n\t\t\t\t    (unsigned long long) dm_bufio_get_block_number(buf));\n\t\t\treturn r;\n\t\t}\n\t\taux->validator = v;\n\t} else {\n\t\tif (unlikely(aux->validator != v)) {\n\t\t\tDMERR_LIMIT(\"validator mismatch (old=%s vs new=%s) for block %llu\",\n\t\t\t\t    aux->validator->name, v ? v->name : \"NULL\",\n\t\t\t\t    (unsigned long long) dm_bufio_get_block_number(buf));\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\nint dm_bm_read_lock(struct dm_block_manager *bm, dm_block_t b,\n\t\t    struct dm_block_validator *v,\n\t\t    struct dm_block **result)\n{\n\tstruct buffer_aux *aux;\n\tvoid *p;\n\tint r;\n\n\tp = dm_bufio_read(bm->bufio, b, (struct dm_buffer **) result);\n\tif (IS_ERR(p))\n\t\treturn PTR_ERR(p);\n\n\taux = dm_bufio_get_aux_data(to_buffer(*result));\n\tr = bl_down_read(&aux->lock);\n\tif (unlikely(r)) {\n\t\tdm_bufio_release(to_buffer(*result));\n\t\treport_recursive_bug(b, r);\n\t\treturn r;\n\t}\n\n\taux->write_locked = 0;\n\n\tr = dm_bm_validate_buffer(bm, to_buffer(*result), aux, v);\n\tif (unlikely(r)) {\n\t\tbl_up_read(&aux->lock);\n\t\tdm_bufio_release(to_buffer(*result));\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(dm_bm_read_lock);\n\nint dm_bm_write_lock(struct dm_block_manager *bm,\n\t\t     dm_block_t b, struct dm_block_validator *v,\n\t\t     struct dm_block **result)\n{\n\tstruct buffer_aux *aux;\n\tvoid *p;\n\tint r;\n\n\tif (dm_bm_is_read_only(bm))\n\t\treturn -EPERM;\n\n\tp = dm_bufio_read(bm->bufio, b, (struct dm_buffer **) result);\n\tif (IS_ERR(p))\n\t\treturn PTR_ERR(p);\n\n\taux = dm_bufio_get_aux_data(to_buffer(*result));\n\tr = bl_down_write(&aux->lock);\n\tif (r) {\n\t\tdm_bufio_release(to_buffer(*result));\n\t\treport_recursive_bug(b, r);\n\t\treturn r;\n\t}\n\n\taux->write_locked = 1;\n\n\tr = dm_bm_validate_buffer(bm, to_buffer(*result), aux, v);\n\tif (unlikely(r)) {\n\t\tbl_up_write(&aux->lock);\n\t\tdm_bufio_release(to_buffer(*result));\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(dm_bm_write_lock);\n\nint dm_bm_read_try_lock(struct dm_block_manager *bm,\n\t\t\tdm_block_t b, struct dm_block_validator *v,\n\t\t\tstruct dm_block **result)\n{\n\tstruct buffer_aux *aux;\n\tvoid *p;\n\tint r;\n\n\tp = dm_bufio_get(bm->bufio, b, (struct dm_buffer **) result);\n\tif (IS_ERR(p))\n\t\treturn PTR_ERR(p);\n\tif (unlikely(!p))\n\t\treturn -EWOULDBLOCK;\n\n\taux = dm_bufio_get_aux_data(to_buffer(*result));\n\tr = bl_down_read_nonblock(&aux->lock);\n\tif (r < 0) {\n\t\tdm_bufio_release(to_buffer(*result));\n\t\treport_recursive_bug(b, r);\n\t\treturn r;\n\t}\n\taux->write_locked = 0;\n\n\tr = dm_bm_validate_buffer(bm, to_buffer(*result), aux, v);\n\tif (unlikely(r)) {\n\t\tbl_up_read(&aux->lock);\n\t\tdm_bufio_release(to_buffer(*result));\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nint dm_bm_write_lock_zero(struct dm_block_manager *bm,\n\t\t\t  dm_block_t b, struct dm_block_validator *v,\n\t\t\t  struct dm_block **result)\n{\n\tint r;\n\tstruct buffer_aux *aux;\n\tvoid *p;\n\n\tif (dm_bm_is_read_only(bm))\n\t\treturn -EPERM;\n\n\tp = dm_bufio_new(bm->bufio, b, (struct dm_buffer **) result);\n\tif (IS_ERR(p))\n\t\treturn PTR_ERR(p);\n\n\tmemset(p, 0, dm_bm_block_size(bm));\n\n\taux = dm_bufio_get_aux_data(to_buffer(*result));\n\tr = bl_down_write(&aux->lock);\n\tif (r) {\n\t\tdm_bufio_release(to_buffer(*result));\n\t\treturn r;\n\t}\n\n\taux->write_locked = 1;\n\taux->validator = v;\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(dm_bm_write_lock_zero);\n\nvoid dm_bm_unlock(struct dm_block *b)\n{\n\tstruct buffer_aux *aux = dm_bufio_get_aux_data(to_buffer(b));\n\n\tif (aux->write_locked) {\n\t\tdm_bufio_mark_buffer_dirty(to_buffer(b));\n\t\tbl_up_write(&aux->lock);\n\t} else\n\t\tbl_up_read(&aux->lock);\n\n\tdm_bufio_release(to_buffer(b));\n}\nEXPORT_SYMBOL_GPL(dm_bm_unlock);\n\nint dm_bm_flush(struct dm_block_manager *bm)\n{\n\tif (dm_bm_is_read_only(bm))\n\t\treturn -EPERM;\n\n\treturn dm_bufio_write_dirty_buffers(bm->bufio);\n}\nEXPORT_SYMBOL_GPL(dm_bm_flush);\n\nvoid dm_bm_prefetch(struct dm_block_manager *bm, dm_block_t b)\n{\n\tdm_bufio_prefetch(bm->bufio, b, 1);\n}\n\nbool dm_bm_is_read_only(struct dm_block_manager *bm)\n{\n\treturn bm ? bm->read_only : true;\n}\nEXPORT_SYMBOL_GPL(dm_bm_is_read_only);\n\nvoid dm_bm_set_read_only(struct dm_block_manager *bm)\n{\n\tif (bm)\n\t\tbm->read_only = true;\n}\nEXPORT_SYMBOL_GPL(dm_bm_set_read_only);\n\nvoid dm_bm_set_read_write(struct dm_block_manager *bm)\n{\n\tif (bm)\n\t\tbm->read_only = false;\n}\nEXPORT_SYMBOL_GPL(dm_bm_set_read_write);\n\nu32 dm_bm_checksum(const void *data, size_t len, u32 init_xor)\n{\n\treturn crc32c(~(u32) 0, data, len) ^ init_xor;\n}\nEXPORT_SYMBOL_GPL(dm_bm_checksum);\n\n \n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Joe Thornber <dm-devel@redhat.com>\");\nMODULE_DESCRIPTION(\"Immutable metadata library for dm\");\n\n \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}