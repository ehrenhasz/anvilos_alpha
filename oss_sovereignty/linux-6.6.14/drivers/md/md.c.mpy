{
  "module_name": "md.c",
  "hash_id": "df05b5a482d0329172a7ce9f627ad7fc88d9738293760a8a5a0ac76e8f228731",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/md.c",
  "human_readable_source": "\n \n\n#include <linux/sched/mm.h>\n#include <linux/sched/signal.h>\n#include <linux/kthread.h>\n#include <linux/blkdev.h>\n#include <linux/blk-integrity.h>\n#include <linux/badblocks.h>\n#include <linux/sysctl.h>\n#include <linux/seq_file.h>\n#include <linux/fs.h>\n#include <linux/poll.h>\n#include <linux/ctype.h>\n#include <linux/string.h>\n#include <linux/hdreg.h>\n#include <linux/proc_fs.h>\n#include <linux/random.h>\n#include <linux/major.h>\n#include <linux/module.h>\n#include <linux/reboot.h>\n#include <linux/file.h>\n#include <linux/compat.h>\n#include <linux/delay.h>\n#include <linux/raid/md_p.h>\n#include <linux/raid/md_u.h>\n#include <linux/raid/detect.h>\n#include <linux/slab.h>\n#include <linux/percpu-refcount.h>\n#include <linux/part_stat.h>\n\n#include <trace/events/block.h>\n#include \"md.h\"\n#include \"md-bitmap.h\"\n#include \"md-cluster.h\"\n\n \nstatic LIST_HEAD(pers_list);\nstatic DEFINE_SPINLOCK(pers_lock);\n\nstatic const struct kobj_type md_ktype;\n\nstruct md_cluster_operations *md_cluster_ops;\nEXPORT_SYMBOL(md_cluster_ops);\nstatic struct module *md_cluster_mod;\n\nstatic DECLARE_WAIT_QUEUE_HEAD(resync_wait);\nstatic struct workqueue_struct *md_wq;\nstatic struct workqueue_struct *md_misc_wq;\nstruct workqueue_struct *md_bitmap_wq;\n\nstatic int remove_and_add_spares(struct mddev *mddev,\n\t\t\t\t struct md_rdev *this);\nstatic void mddev_detach(struct mddev *mddev);\nstatic void export_rdev(struct md_rdev *rdev, struct mddev *mddev);\nstatic void md_wakeup_thread_directly(struct md_thread __rcu *thread);\n\n \n#define MD_DEFAULT_MAX_CORRECTED_READ_ERRORS 20\n \n#define DEFAULT_SAFEMODE_DELAY ((200 * HZ)/1000 +1)\n \n\nstatic int sysctl_speed_limit_min = 1000;\nstatic int sysctl_speed_limit_max = 200000;\nstatic inline int speed_min(struct mddev *mddev)\n{\n\treturn mddev->sync_speed_min ?\n\t\tmddev->sync_speed_min : sysctl_speed_limit_min;\n}\n\nstatic inline int speed_max(struct mddev *mddev)\n{\n\treturn mddev->sync_speed_max ?\n\t\tmddev->sync_speed_max : sysctl_speed_limit_max;\n}\n\nstatic void rdev_uninit_serial(struct md_rdev *rdev)\n{\n\tif (!test_and_clear_bit(CollisionCheck, &rdev->flags))\n\t\treturn;\n\n\tkvfree(rdev->serial);\n\trdev->serial = NULL;\n}\n\nstatic void rdevs_uninit_serial(struct mddev *mddev)\n{\n\tstruct md_rdev *rdev;\n\n\trdev_for_each(rdev, mddev)\n\t\trdev_uninit_serial(rdev);\n}\n\nstatic int rdev_init_serial(struct md_rdev *rdev)\n{\n\t \n\tint i, serial_nums = 1 << ((PAGE_SHIFT - ilog2(sizeof(atomic_t))));\n\tstruct serial_in_rdev *serial = NULL;\n\n\tif (test_bit(CollisionCheck, &rdev->flags))\n\t\treturn 0;\n\n\tserial = kvmalloc(sizeof(struct serial_in_rdev) * serial_nums,\n\t\t\t  GFP_KERNEL);\n\tif (!serial)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < serial_nums; i++) {\n\t\tstruct serial_in_rdev *serial_tmp = &serial[i];\n\n\t\tspin_lock_init(&serial_tmp->serial_lock);\n\t\tserial_tmp->serial_rb = RB_ROOT_CACHED;\n\t\tinit_waitqueue_head(&serial_tmp->serial_io_wait);\n\t}\n\n\trdev->serial = serial;\n\tset_bit(CollisionCheck, &rdev->flags);\n\n\treturn 0;\n}\n\nstatic int rdevs_init_serial(struct mddev *mddev)\n{\n\tstruct md_rdev *rdev;\n\tint ret = 0;\n\n\trdev_for_each(rdev, mddev) {\n\t\tret = rdev_init_serial(rdev);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\t \n\tif (ret && !mddev->serial_info_pool)\n\t\trdevs_uninit_serial(mddev);\n\n\treturn ret;\n}\n\n \nstatic int rdev_need_serial(struct md_rdev *rdev)\n{\n\treturn (rdev && rdev->mddev->bitmap_info.max_write_behind > 0 &&\n\t\trdev->bdev->bd_disk->queue->nr_hw_queues != 1 &&\n\t\ttest_bit(WriteMostly, &rdev->flags));\n}\n\n \nvoid mddev_create_serial_pool(struct mddev *mddev, struct md_rdev *rdev,\n\t\t\t      bool is_suspend)\n{\n\tint ret = 0;\n\n\tif (rdev && !rdev_need_serial(rdev) &&\n\t    !test_bit(CollisionCheck, &rdev->flags))\n\t\treturn;\n\n\tif (!is_suspend)\n\t\tmddev_suspend(mddev);\n\n\tif (!rdev)\n\t\tret = rdevs_init_serial(mddev);\n\telse\n\t\tret = rdev_init_serial(rdev);\n\tif (ret)\n\t\tgoto abort;\n\n\tif (mddev->serial_info_pool == NULL) {\n\t\t \n\t\tmddev->serial_info_pool =\n\t\t\tmempool_create_kmalloc_pool(NR_SERIAL_INFOS,\n\t\t\t\t\t\tsizeof(struct serial_info));\n\t\tif (!mddev->serial_info_pool) {\n\t\t\trdevs_uninit_serial(mddev);\n\t\t\tpr_err(\"can't alloc memory pool for serialization\\n\");\n\t\t}\n\t}\n\nabort:\n\tif (!is_suspend)\n\t\tmddev_resume(mddev);\n}\n\n \nvoid mddev_destroy_serial_pool(struct mddev *mddev, struct md_rdev *rdev,\n\t\t\t       bool is_suspend)\n{\n\tif (rdev && !test_bit(CollisionCheck, &rdev->flags))\n\t\treturn;\n\n\tif (mddev->serial_info_pool) {\n\t\tstruct md_rdev *temp;\n\t\tint num = 0;  \n\n\t\tif (!is_suspend)\n\t\t\tmddev_suspend(mddev);\n\t\trdev_for_each(temp, mddev) {\n\t\t\tif (!rdev) {\n\t\t\t\tif (!mddev->serialize_policy ||\n\t\t\t\t    !rdev_need_serial(temp))\n\t\t\t\t\trdev_uninit_serial(temp);\n\t\t\t\telse\n\t\t\t\t\tnum++;\n\t\t\t} else if (temp != rdev &&\n\t\t\t\t   test_bit(CollisionCheck, &temp->flags))\n\t\t\t\tnum++;\n\t\t}\n\n\t\tif (rdev)\n\t\t\trdev_uninit_serial(rdev);\n\n\t\tif (num)\n\t\t\tpr_info(\"The mempool could be used by other devices\\n\");\n\t\telse {\n\t\t\tmempool_destroy(mddev->serial_info_pool);\n\t\t\tmddev->serial_info_pool = NULL;\n\t\t}\n\t\tif (!is_suspend)\n\t\t\tmddev_resume(mddev);\n\t}\n}\n\nstatic struct ctl_table_header *raid_table_header;\n\nstatic struct ctl_table raid_table[] = {\n\t{\n\t\t.procname\t= \"speed_limit_min\",\n\t\t.data\t\t= &sysctl_speed_limit_min,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= S_IRUGO|S_IWUSR,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"speed_limit_max\",\n\t\t.data\t\t= &sysctl_speed_limit_max,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= S_IRUGO|S_IWUSR,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{ }\n};\n\nstatic int start_readonly;\n\n \nstatic bool create_on_open = true;\n\n \nstatic DECLARE_WAIT_QUEUE_HEAD(md_event_waiters);\nstatic atomic_t md_event_count;\nvoid md_new_event(void)\n{\n\tatomic_inc(&md_event_count);\n\twake_up(&md_event_waiters);\n}\nEXPORT_SYMBOL_GPL(md_new_event);\n\n \nstatic LIST_HEAD(all_mddevs);\nstatic DEFINE_SPINLOCK(all_mddevs_lock);\n\n \nstatic bool is_suspended(struct mddev *mddev, struct bio *bio)\n{\n\tif (is_md_suspended(mddev))\n\t\treturn true;\n\tif (bio_data_dir(bio) != WRITE)\n\t\treturn false;\n\tif (mddev->suspend_lo >= mddev->suspend_hi)\n\t\treturn false;\n\tif (bio->bi_iter.bi_sector >= mddev->suspend_hi)\n\t\treturn false;\n\tif (bio_end_sector(bio) < mddev->suspend_lo)\n\t\treturn false;\n\treturn true;\n}\n\nvoid md_handle_request(struct mddev *mddev, struct bio *bio)\n{\ncheck_suspended:\n\tif (is_suspended(mddev, bio)) {\n\t\tDEFINE_WAIT(__wait);\n\t\t \n\t\tif (bio->bi_opf & REQ_NOWAIT) {\n\t\t\tbio_wouldblock_error(bio);\n\t\t\treturn;\n\t\t}\n\t\tfor (;;) {\n\t\t\tprepare_to_wait(&mddev->sb_wait, &__wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\t\tif (!is_suspended(mddev, bio))\n\t\t\t\tbreak;\n\t\t\tschedule();\n\t\t}\n\t\tfinish_wait(&mddev->sb_wait, &__wait);\n\t}\n\tif (!percpu_ref_tryget_live(&mddev->active_io))\n\t\tgoto check_suspended;\n\n\tif (!mddev->pers->make_request(mddev, bio)) {\n\t\tpercpu_ref_put(&mddev->active_io);\n\t\tgoto check_suspended;\n\t}\n\n\tpercpu_ref_put(&mddev->active_io);\n}\nEXPORT_SYMBOL(md_handle_request);\n\nstatic void md_submit_bio(struct bio *bio)\n{\n\tconst int rw = bio_data_dir(bio);\n\tstruct mddev *mddev = bio->bi_bdev->bd_disk->private_data;\n\n\tif (mddev == NULL || mddev->pers == NULL) {\n\t\tbio_io_error(bio);\n\t\treturn;\n\t}\n\n\tif (unlikely(test_bit(MD_BROKEN, &mddev->flags)) && (rw == WRITE)) {\n\t\tbio_io_error(bio);\n\t\treturn;\n\t}\n\n\tbio = bio_split_to_limits(bio);\n\tif (!bio)\n\t\treturn;\n\n\tif (mddev->ro == MD_RDONLY && unlikely(rw == WRITE)) {\n\t\tif (bio_sectors(bio) != 0)\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\tbio_endio(bio);\n\t\treturn;\n\t}\n\n\t \n\tbio->bi_opf &= ~REQ_NOMERGE;\n\n\tmd_handle_request(mddev, bio);\n}\n\n \nvoid mddev_suspend(struct mddev *mddev)\n{\n\tstruct md_thread *thread = rcu_dereference_protected(mddev->thread,\n\t\t\tlockdep_is_held(&mddev->reconfig_mutex));\n\n\tWARN_ON_ONCE(thread && current == thread->tsk);\n\tif (mddev->suspended++)\n\t\treturn;\n\twake_up(&mddev->sb_wait);\n\tset_bit(MD_ALLOW_SB_UPDATE, &mddev->flags);\n\tpercpu_ref_kill(&mddev->active_io);\n\n\tif (mddev->pers && mddev->pers->prepare_suspend)\n\t\tmddev->pers->prepare_suspend(mddev);\n\n\twait_event(mddev->sb_wait, percpu_ref_is_zero(&mddev->active_io));\n\tclear_bit_unlock(MD_ALLOW_SB_UPDATE, &mddev->flags);\n\twait_event(mddev->sb_wait, !test_bit(MD_UPDATING_SB, &mddev->flags));\n\n\tdel_timer_sync(&mddev->safemode_timer);\n\t \n\tmddev->noio_flag = memalloc_noio_save();\n}\nEXPORT_SYMBOL_GPL(mddev_suspend);\n\nvoid mddev_resume(struct mddev *mddev)\n{\n\tlockdep_assert_held(&mddev->reconfig_mutex);\n\tif (--mddev->suspended)\n\t\treturn;\n\n\t \n\tmemalloc_noio_restore(mddev->noio_flag);\n\n\tpercpu_ref_resurrect(&mddev->active_io);\n\twake_up(&mddev->sb_wait);\n\n\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\tmd_wakeup_thread(mddev->thread);\n\tmd_wakeup_thread(mddev->sync_thread);  \n}\nEXPORT_SYMBOL_GPL(mddev_resume);\n\n \n\nstatic void md_end_flush(struct bio *bio)\n{\n\tstruct md_rdev *rdev = bio->bi_private;\n\tstruct mddev *mddev = rdev->mddev;\n\n\tbio_put(bio);\n\n\trdev_dec_pending(rdev, mddev);\n\n\tif (atomic_dec_and_test(&mddev->flush_pending)) {\n\t\t \n\t\tpercpu_ref_put(&mddev->active_io);\n\n\t\t \n\t\tqueue_work(md_wq, &mddev->flush_work);\n\t}\n}\n\nstatic void md_submit_flush_data(struct work_struct *ws);\n\nstatic void submit_flushes(struct work_struct *ws)\n{\n\tstruct mddev *mddev = container_of(ws, struct mddev, flush_work);\n\tstruct md_rdev *rdev;\n\n\tmddev->start_flush = ktime_get_boottime();\n\tINIT_WORK(&mddev->flush_work, md_submit_flush_data);\n\tatomic_set(&mddev->flush_pending, 1);\n\trcu_read_lock();\n\trdev_for_each_rcu(rdev, mddev)\n\t\tif (rdev->raid_disk >= 0 &&\n\t\t    !test_bit(Faulty, &rdev->flags)) {\n\t\t\tstruct bio *bi;\n\n\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\trcu_read_unlock();\n\t\t\tbi = bio_alloc_bioset(rdev->bdev, 0,\n\t\t\t\t\t      REQ_OP_WRITE | REQ_PREFLUSH,\n\t\t\t\t\t      GFP_NOIO, &mddev->bio_set);\n\t\t\tbi->bi_end_io = md_end_flush;\n\t\t\tbi->bi_private = rdev;\n\t\t\tatomic_inc(&mddev->flush_pending);\n\t\t\tsubmit_bio(bi);\n\t\t\trcu_read_lock();\n\t\t}\n\trcu_read_unlock();\n\tif (atomic_dec_and_test(&mddev->flush_pending))\n\t\tqueue_work(md_wq, &mddev->flush_work);\n}\n\nstatic void md_submit_flush_data(struct work_struct *ws)\n{\n\tstruct mddev *mddev = container_of(ws, struct mddev, flush_work);\n\tstruct bio *bio = mddev->flush_bio;\n\n\t \n\tspin_lock_irq(&mddev->lock);\n\tmddev->prev_flush_start = mddev->start_flush;\n\tmddev->flush_bio = NULL;\n\tspin_unlock_irq(&mddev->lock);\n\twake_up(&mddev->sb_wait);\n\n\tif (bio->bi_iter.bi_size == 0) {\n\t\t \n\t\tbio_endio(bio);\n\t} else {\n\t\tbio->bi_opf &= ~REQ_PREFLUSH;\n\t\tmd_handle_request(mddev, bio);\n\t}\n}\n\n \nbool md_flush_request(struct mddev *mddev, struct bio *bio)\n{\n\tktime_t req_start = ktime_get_boottime();\n\tspin_lock_irq(&mddev->lock);\n\t \n\twait_event_lock_irq(mddev->sb_wait,\n\t\t\t    !mddev->flush_bio ||\n\t\t\t    ktime_before(req_start, mddev->prev_flush_start),\n\t\t\t    mddev->lock);\n\t \n\tif (ktime_after(req_start, mddev->prev_flush_start)) {\n\t\tWARN_ON(mddev->flush_bio);\n\t\t \n\t\tWARN_ON(percpu_ref_is_zero(&mddev->active_io));\n\t\tpercpu_ref_get(&mddev->active_io);\n\t\tmddev->flush_bio = bio;\n\t\tbio = NULL;\n\t}\n\tspin_unlock_irq(&mddev->lock);\n\n\tif (!bio) {\n\t\tINIT_WORK(&mddev->flush_work, submit_flushes);\n\t\tqueue_work(md_wq, &mddev->flush_work);\n\t} else {\n\t\t \n\t\tif (bio->bi_iter.bi_size == 0)\n\t\t\t \n\t\t\tbio_endio(bio);\n\t\telse {\n\t\t\tbio->bi_opf &= ~REQ_PREFLUSH;\n\t\t\treturn false;\n\t\t}\n\t}\n\treturn true;\n}\nEXPORT_SYMBOL(md_flush_request);\n\nstatic inline struct mddev *mddev_get(struct mddev *mddev)\n{\n\tlockdep_assert_held(&all_mddevs_lock);\n\n\tif (test_bit(MD_DELETED, &mddev->flags))\n\t\treturn NULL;\n\tatomic_inc(&mddev->active);\n\treturn mddev;\n}\n\nstatic void mddev_delayed_delete(struct work_struct *ws);\n\nvoid mddev_put(struct mddev *mddev)\n{\n\tif (!atomic_dec_and_lock(&mddev->active, &all_mddevs_lock))\n\t\treturn;\n\tif (!mddev->raid_disks && list_empty(&mddev->disks) &&\n\t    mddev->ctime == 0 && !mddev->hold_active) {\n\t\t \n\t\tset_bit(MD_DELETED, &mddev->flags);\n\n\t\t \n\t\tINIT_WORK(&mddev->del_work, mddev_delayed_delete);\n\t\tqueue_work(md_misc_wq, &mddev->del_work);\n\t}\n\tspin_unlock(&all_mddevs_lock);\n}\n\nstatic void md_safemode_timeout(struct timer_list *t);\n\nvoid mddev_init(struct mddev *mddev)\n{\n\tmutex_init(&mddev->open_mutex);\n\tmutex_init(&mddev->reconfig_mutex);\n\tmutex_init(&mddev->sync_mutex);\n\tmutex_init(&mddev->bitmap_info.mutex);\n\tINIT_LIST_HEAD(&mddev->disks);\n\tINIT_LIST_HEAD(&mddev->all_mddevs);\n\tINIT_LIST_HEAD(&mddev->deleting);\n\ttimer_setup(&mddev->safemode_timer, md_safemode_timeout, 0);\n\tatomic_set(&mddev->active, 1);\n\tatomic_set(&mddev->openers, 0);\n\tatomic_set(&mddev->sync_seq, 0);\n\tspin_lock_init(&mddev->lock);\n\tatomic_set(&mddev->flush_pending, 0);\n\tinit_waitqueue_head(&mddev->sb_wait);\n\tinit_waitqueue_head(&mddev->recovery_wait);\n\tmddev->reshape_position = MaxSector;\n\tmddev->reshape_backwards = 0;\n\tmddev->last_sync_action = \"none\";\n\tmddev->resync_min = 0;\n\tmddev->resync_max = MaxSector;\n\tmddev->level = LEVEL_NONE;\n}\nEXPORT_SYMBOL_GPL(mddev_init);\n\nstatic struct mddev *mddev_find_locked(dev_t unit)\n{\n\tstruct mddev *mddev;\n\n\tlist_for_each_entry(mddev, &all_mddevs, all_mddevs)\n\t\tif (mddev->unit == unit)\n\t\t\treturn mddev;\n\n\treturn NULL;\n}\n\n \nstatic dev_t mddev_alloc_unit(void)\n{\n\tstatic int next_minor = 512;\n\tint start = next_minor;\n\tbool is_free = 0;\n\tdev_t dev = 0;\n\n\twhile (!is_free) {\n\t\tdev = MKDEV(MD_MAJOR, next_minor);\n\t\tnext_minor++;\n\t\tif (next_minor > MINORMASK)\n\t\t\tnext_minor = 0;\n\t\tif (next_minor == start)\n\t\t\treturn 0;\t\t \n\t\tis_free = !mddev_find_locked(dev);\n\t}\n\n\treturn dev;\n}\n\nstatic struct mddev *mddev_alloc(dev_t unit)\n{\n\tstruct mddev *new;\n\tint error;\n\n\tif (unit && MAJOR(unit) != MD_MAJOR)\n\t\tunit &= ~((1 << MdpMinorShift) - 1);\n\n\tnew = kzalloc(sizeof(*new), GFP_KERNEL);\n\tif (!new)\n\t\treturn ERR_PTR(-ENOMEM);\n\tmddev_init(new);\n\n\tspin_lock(&all_mddevs_lock);\n\tif (unit) {\n\t\terror = -EEXIST;\n\t\tif (mddev_find_locked(unit))\n\t\t\tgoto out_free_new;\n\t\tnew->unit = unit;\n\t\tif (MAJOR(unit) == MD_MAJOR)\n\t\t\tnew->md_minor = MINOR(unit);\n\t\telse\n\t\t\tnew->md_minor = MINOR(unit) >> MdpMinorShift;\n\t\tnew->hold_active = UNTIL_IOCTL;\n\t} else {\n\t\terror = -ENODEV;\n\t\tnew->unit = mddev_alloc_unit();\n\t\tif (!new->unit)\n\t\t\tgoto out_free_new;\n\t\tnew->md_minor = MINOR(new->unit);\n\t\tnew->hold_active = UNTIL_STOP;\n\t}\n\n\tlist_add(&new->all_mddevs, &all_mddevs);\n\tspin_unlock(&all_mddevs_lock);\n\treturn new;\nout_free_new:\n\tspin_unlock(&all_mddevs_lock);\n\tkfree(new);\n\treturn ERR_PTR(error);\n}\n\nstatic void mddev_free(struct mddev *mddev)\n{\n\tspin_lock(&all_mddevs_lock);\n\tlist_del(&mddev->all_mddevs);\n\tspin_unlock(&all_mddevs_lock);\n\n\tkfree(mddev);\n}\n\nstatic const struct attribute_group md_redundancy_group;\n\nvoid mddev_unlock(struct mddev *mddev)\n{\n\tstruct md_rdev *rdev;\n\tstruct md_rdev *tmp;\n\tLIST_HEAD(delete);\n\n\tif (!list_empty(&mddev->deleting))\n\t\tlist_splice_init(&mddev->deleting, &delete);\n\n\tif (mddev->to_remove) {\n\t\t \n\t\tconst struct attribute_group *to_remove = mddev->to_remove;\n\t\tmddev->to_remove = NULL;\n\t\tmddev->sysfs_active = 1;\n\t\tmutex_unlock(&mddev->reconfig_mutex);\n\n\t\tif (mddev->kobj.sd) {\n\t\t\tif (to_remove != &md_redundancy_group)\n\t\t\t\tsysfs_remove_group(&mddev->kobj, to_remove);\n\t\t\tif (mddev->pers == NULL ||\n\t\t\t    mddev->pers->sync_request == NULL) {\n\t\t\t\tsysfs_remove_group(&mddev->kobj, &md_redundancy_group);\n\t\t\t\tif (mddev->sysfs_action)\n\t\t\t\t\tsysfs_put(mddev->sysfs_action);\n\t\t\t\tif (mddev->sysfs_completed)\n\t\t\t\t\tsysfs_put(mddev->sysfs_completed);\n\t\t\t\tif (mddev->sysfs_degraded)\n\t\t\t\t\tsysfs_put(mddev->sysfs_degraded);\n\t\t\t\tmddev->sysfs_action = NULL;\n\t\t\t\tmddev->sysfs_completed = NULL;\n\t\t\t\tmddev->sysfs_degraded = NULL;\n\t\t\t}\n\t\t}\n\t\tmddev->sysfs_active = 0;\n\t} else\n\t\tmutex_unlock(&mddev->reconfig_mutex);\n\n\tmd_wakeup_thread(mddev->thread);\n\twake_up(&mddev->sb_wait);\n\n\tlist_for_each_entry_safe(rdev, tmp, &delete, same_set) {\n\t\tlist_del_init(&rdev->same_set);\n\t\tkobject_del(&rdev->kobj);\n\t\texport_rdev(rdev, mddev);\n\t}\n}\nEXPORT_SYMBOL_GPL(mddev_unlock);\n\nstruct md_rdev *md_find_rdev_nr_rcu(struct mddev *mddev, int nr)\n{\n\tstruct md_rdev *rdev;\n\n\trdev_for_each_rcu(rdev, mddev)\n\t\tif (rdev->desc_nr == nr)\n\t\t\treturn rdev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(md_find_rdev_nr_rcu);\n\nstatic struct md_rdev *find_rdev(struct mddev *mddev, dev_t dev)\n{\n\tstruct md_rdev *rdev;\n\n\trdev_for_each(rdev, mddev)\n\t\tif (rdev->bdev->bd_dev == dev)\n\t\t\treturn rdev;\n\n\treturn NULL;\n}\n\nstruct md_rdev *md_find_rdev_rcu(struct mddev *mddev, dev_t dev)\n{\n\tstruct md_rdev *rdev;\n\n\trdev_for_each_rcu(rdev, mddev)\n\t\tif (rdev->bdev->bd_dev == dev)\n\t\t\treturn rdev;\n\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(md_find_rdev_rcu);\n\nstatic struct md_personality *find_pers(int level, char *clevel)\n{\n\tstruct md_personality *pers;\n\tlist_for_each_entry(pers, &pers_list, list) {\n\t\tif (level != LEVEL_NONE && pers->level == level)\n\t\t\treturn pers;\n\t\tif (strcmp(pers->name, clevel)==0)\n\t\t\treturn pers;\n\t}\n\treturn NULL;\n}\n\n \nstatic inline sector_t calc_dev_sboffset(struct md_rdev *rdev)\n{\n\treturn MD_NEW_SIZE_SECTORS(bdev_nr_sectors(rdev->bdev));\n}\n\nstatic int alloc_disk_sb(struct md_rdev *rdev)\n{\n\trdev->sb_page = alloc_page(GFP_KERNEL);\n\tif (!rdev->sb_page)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nvoid md_rdev_clear(struct md_rdev *rdev)\n{\n\tif (rdev->sb_page) {\n\t\tput_page(rdev->sb_page);\n\t\trdev->sb_loaded = 0;\n\t\trdev->sb_page = NULL;\n\t\trdev->sb_start = 0;\n\t\trdev->sectors = 0;\n\t}\n\tif (rdev->bb_page) {\n\t\tput_page(rdev->bb_page);\n\t\trdev->bb_page = NULL;\n\t}\n\tbadblocks_exit(&rdev->badblocks);\n}\nEXPORT_SYMBOL_GPL(md_rdev_clear);\n\nstatic void super_written(struct bio *bio)\n{\n\tstruct md_rdev *rdev = bio->bi_private;\n\tstruct mddev *mddev = rdev->mddev;\n\n\tif (bio->bi_status) {\n\t\tpr_err(\"md: %s gets error=%d\\n\", __func__,\n\t\t       blk_status_to_errno(bio->bi_status));\n\t\tmd_error(mddev, rdev);\n\t\tif (!test_bit(Faulty, &rdev->flags)\n\t\t    && (bio->bi_opf & MD_FAILFAST)) {\n\t\t\tset_bit(MD_SB_NEED_REWRITE, &mddev->sb_flags);\n\t\t\tset_bit(LastDev, &rdev->flags);\n\t\t}\n\t} else\n\t\tclear_bit(LastDev, &rdev->flags);\n\n\tbio_put(bio);\n\n\trdev_dec_pending(rdev, mddev);\n\n\tif (atomic_dec_and_test(&mddev->pending_writes))\n\t\twake_up(&mddev->sb_wait);\n}\n\nvoid md_super_write(struct mddev *mddev, struct md_rdev *rdev,\n\t\t   sector_t sector, int size, struct page *page)\n{\n\t \n\tstruct bio *bio;\n\n\tif (!page)\n\t\treturn;\n\n\tif (test_bit(Faulty, &rdev->flags))\n\t\treturn;\n\n\tbio = bio_alloc_bioset(rdev->meta_bdev ? rdev->meta_bdev : rdev->bdev,\n\t\t\t       1,\n\t\t\t       REQ_OP_WRITE | REQ_SYNC | REQ_PREFLUSH | REQ_FUA,\n\t\t\t       GFP_NOIO, &mddev->sync_set);\n\n\tatomic_inc(&rdev->nr_pending);\n\n\tbio->bi_iter.bi_sector = sector;\n\t__bio_add_page(bio, page, size, 0);\n\tbio->bi_private = rdev;\n\tbio->bi_end_io = super_written;\n\n\tif (test_bit(MD_FAILFAST_SUPPORTED, &mddev->flags) &&\n\t    test_bit(FailFast, &rdev->flags) &&\n\t    !test_bit(LastDev, &rdev->flags))\n\t\tbio->bi_opf |= MD_FAILFAST;\n\n\tatomic_inc(&mddev->pending_writes);\n\tsubmit_bio(bio);\n}\n\nint md_super_wait(struct mddev *mddev)\n{\n\t \n\twait_event(mddev->sb_wait, atomic_read(&mddev->pending_writes)==0);\n\tif (test_and_clear_bit(MD_SB_NEED_REWRITE, &mddev->sb_flags))\n\t\treturn -EAGAIN;\n\treturn 0;\n}\n\nint sync_page_io(struct md_rdev *rdev, sector_t sector, int size,\n\t\t struct page *page, blk_opf_t opf, bool metadata_op)\n{\n\tstruct bio bio;\n\tstruct bio_vec bvec;\n\n\tif (metadata_op && rdev->meta_bdev)\n\t\tbio_init(&bio, rdev->meta_bdev, &bvec, 1, opf);\n\telse\n\t\tbio_init(&bio, rdev->bdev, &bvec, 1, opf);\n\n\tif (metadata_op)\n\t\tbio.bi_iter.bi_sector = sector + rdev->sb_start;\n\telse if (rdev->mddev->reshape_position != MaxSector &&\n\t\t (rdev->mddev->reshape_backwards ==\n\t\t  (sector >= rdev->mddev->reshape_position)))\n\t\tbio.bi_iter.bi_sector = sector + rdev->new_data_offset;\n\telse\n\t\tbio.bi_iter.bi_sector = sector + rdev->data_offset;\n\t__bio_add_page(&bio, page, size, 0);\n\n\tsubmit_bio_wait(&bio);\n\n\treturn !bio.bi_status;\n}\nEXPORT_SYMBOL_GPL(sync_page_io);\n\nstatic int read_disk_sb(struct md_rdev *rdev, int size)\n{\n\tif (rdev->sb_loaded)\n\t\treturn 0;\n\n\tif (!sync_page_io(rdev, 0, size, rdev->sb_page, REQ_OP_READ, true))\n\t\tgoto fail;\n\trdev->sb_loaded = 1;\n\treturn 0;\n\nfail:\n\tpr_err(\"md: disabled device %pg, could not read superblock.\\n\",\n\t       rdev->bdev);\n\treturn -EINVAL;\n}\n\nstatic int md_uuid_equal(mdp_super_t *sb1, mdp_super_t *sb2)\n{\n\treturn\tsb1->set_uuid0 == sb2->set_uuid0 &&\n\t\tsb1->set_uuid1 == sb2->set_uuid1 &&\n\t\tsb1->set_uuid2 == sb2->set_uuid2 &&\n\t\tsb1->set_uuid3 == sb2->set_uuid3;\n}\n\nstatic int md_sb_equal(mdp_super_t *sb1, mdp_super_t *sb2)\n{\n\tint ret;\n\tmdp_super_t *tmp1, *tmp2;\n\n\ttmp1 = kmalloc(sizeof(*tmp1),GFP_KERNEL);\n\ttmp2 = kmalloc(sizeof(*tmp2),GFP_KERNEL);\n\n\tif (!tmp1 || !tmp2) {\n\t\tret = 0;\n\t\tgoto abort;\n\t}\n\n\t*tmp1 = *sb1;\n\t*tmp2 = *sb2;\n\n\t \n\ttmp1->nr_disks = 0;\n\ttmp2->nr_disks = 0;\n\n\tret = (memcmp(tmp1, tmp2, MD_SB_GENERIC_CONSTANT_WORDS * 4) == 0);\nabort:\n\tkfree(tmp1);\n\tkfree(tmp2);\n\treturn ret;\n}\n\nstatic u32 md_csum_fold(u32 csum)\n{\n\tcsum = (csum & 0xffff) + (csum >> 16);\n\treturn (csum & 0xffff) + (csum >> 16);\n}\n\nstatic unsigned int calc_sb_csum(mdp_super_t *sb)\n{\n\tu64 newcsum = 0;\n\tu32 *sb32 = (u32*)sb;\n\tint i;\n\tunsigned int disk_csum, csum;\n\n\tdisk_csum = sb->sb_csum;\n\tsb->sb_csum = 0;\n\n\tfor (i = 0; i < MD_SB_BYTES/4 ; i++)\n\t\tnewcsum += sb32[i];\n\tcsum = (newcsum & 0xffffffff) + (newcsum>>32);\n\n#ifdef CONFIG_ALPHA\n\t \n\tsb->sb_csum = md_csum_fold(disk_csum);\n#else\n\tsb->sb_csum = disk_csum;\n#endif\n\treturn csum;\n}\n\n \n\nstruct super_type  {\n\tchar\t\t    *name;\n\tstruct module\t    *owner;\n\tint\t\t    (*load_super)(struct md_rdev *rdev,\n\t\t\t\t\t  struct md_rdev *refdev,\n\t\t\t\t\t  int minor_version);\n\tint\t\t    (*validate_super)(struct mddev *mddev,\n\t\t\t\t\t      struct md_rdev *rdev);\n\tvoid\t\t    (*sync_super)(struct mddev *mddev,\n\t\t\t\t\t  struct md_rdev *rdev);\n\tunsigned long long  (*rdev_size_change)(struct md_rdev *rdev,\n\t\t\t\t\t\tsector_t num_sectors);\n\tint\t\t    (*allow_new_offset)(struct md_rdev *rdev,\n\t\t\t\t\t\tunsigned long long new_offset);\n};\n\n \nint md_check_no_bitmap(struct mddev *mddev)\n{\n\tif (!mddev->bitmap_info.file && !mddev->bitmap_info.offset)\n\t\treturn 0;\n\tpr_warn(\"%s: bitmaps are not supported for %s\\n\",\n\t\tmdname(mddev), mddev->pers->name);\n\treturn 1;\n}\nEXPORT_SYMBOL(md_check_no_bitmap);\n\n \nstatic int super_90_load(struct md_rdev *rdev, struct md_rdev *refdev, int minor_version)\n{\n\tmdp_super_t *sb;\n\tint ret;\n\tbool spare_disk = true;\n\n\t \n\trdev->sb_start = calc_dev_sboffset(rdev);\n\n\tret = read_disk_sb(rdev, MD_SB_BYTES);\n\tif (ret)\n\t\treturn ret;\n\n\tret = -EINVAL;\n\n\tsb = page_address(rdev->sb_page);\n\n\tif (sb->md_magic != MD_SB_MAGIC) {\n\t\tpr_warn(\"md: invalid raid superblock magic on %pg\\n\",\n\t\t\trdev->bdev);\n\t\tgoto abort;\n\t}\n\n\tif (sb->major_version != 0 ||\n\t    sb->minor_version < 90 ||\n\t    sb->minor_version > 91) {\n\t\tpr_warn(\"Bad version number %d.%d on %pg\\n\",\n\t\t\tsb->major_version, sb->minor_version, rdev->bdev);\n\t\tgoto abort;\n\t}\n\n\tif (sb->raid_disks <= 0)\n\t\tgoto abort;\n\n\tif (md_csum_fold(calc_sb_csum(sb)) != md_csum_fold(sb->sb_csum)) {\n\t\tpr_warn(\"md: invalid superblock checksum on %pg\\n\", rdev->bdev);\n\t\tgoto abort;\n\t}\n\n\trdev->preferred_minor = sb->md_minor;\n\trdev->data_offset = 0;\n\trdev->new_data_offset = 0;\n\trdev->sb_size = MD_SB_BYTES;\n\trdev->badblocks.shift = -1;\n\n\tif (sb->level == LEVEL_MULTIPATH)\n\t\trdev->desc_nr = -1;\n\telse\n\t\trdev->desc_nr = sb->this_disk.number;\n\n\t \n\tif (sb->level == LEVEL_MULTIPATH ||\n\t\t(rdev->desc_nr >= 0 &&\n\t\t rdev->desc_nr < MD_SB_DISKS &&\n\t\t sb->disks[rdev->desc_nr].state &\n\t\t ((1<<MD_DISK_SYNC) | (1 << MD_DISK_ACTIVE))))\n\t\tspare_disk = false;\n\n\tif (!refdev) {\n\t\tif (!spare_disk)\n\t\t\tret = 1;\n\t\telse\n\t\t\tret = 0;\n\t} else {\n\t\t__u64 ev1, ev2;\n\t\tmdp_super_t *refsb = page_address(refdev->sb_page);\n\t\tif (!md_uuid_equal(refsb, sb)) {\n\t\t\tpr_warn(\"md: %pg has different UUID to %pg\\n\",\n\t\t\t\trdev->bdev, refdev->bdev);\n\t\t\tgoto abort;\n\t\t}\n\t\tif (!md_sb_equal(refsb, sb)) {\n\t\t\tpr_warn(\"md: %pg has same UUID but different superblock to %pg\\n\",\n\t\t\t\trdev->bdev, refdev->bdev);\n\t\t\tgoto abort;\n\t\t}\n\t\tev1 = md_event(sb);\n\t\tev2 = md_event(refsb);\n\n\t\tif (!spare_disk && ev1 > ev2)\n\t\t\tret = 1;\n\t\telse\n\t\t\tret = 0;\n\t}\n\trdev->sectors = rdev->sb_start;\n\t \n\tif ((u64)rdev->sectors >= (2ULL << 32) && sb->level >= 1)\n\t\trdev->sectors = (sector_t)(2ULL << 32) - 2;\n\n\tif (rdev->sectors < ((sector_t)sb->size) * 2 && sb->level >= 1)\n\t\t \n\t\tret = -EINVAL;\n\n abort:\n\treturn ret;\n}\n\n \nstatic int super_90_validate(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tmdp_disk_t *desc;\n\tmdp_super_t *sb = page_address(rdev->sb_page);\n\t__u64 ev1 = md_event(sb);\n\n\trdev->raid_disk = -1;\n\tclear_bit(Faulty, &rdev->flags);\n\tclear_bit(In_sync, &rdev->flags);\n\tclear_bit(Bitmap_sync, &rdev->flags);\n\tclear_bit(WriteMostly, &rdev->flags);\n\n\tif (mddev->raid_disks == 0) {\n\t\tmddev->major_version = 0;\n\t\tmddev->minor_version = sb->minor_version;\n\t\tmddev->patch_version = sb->patch_version;\n\t\tmddev->external = 0;\n\t\tmddev->chunk_sectors = sb->chunk_size >> 9;\n\t\tmddev->ctime = sb->ctime;\n\t\tmddev->utime = sb->utime;\n\t\tmddev->level = sb->level;\n\t\tmddev->clevel[0] = 0;\n\t\tmddev->layout = sb->layout;\n\t\tmddev->raid_disks = sb->raid_disks;\n\t\tmddev->dev_sectors = ((sector_t)sb->size) * 2;\n\t\tmddev->events = ev1;\n\t\tmddev->bitmap_info.offset = 0;\n\t\tmddev->bitmap_info.space = 0;\n\t\t \n\t\tmddev->bitmap_info.default_offset = MD_SB_BYTES >> 9;\n\t\tmddev->bitmap_info.default_space = 64*2 - (MD_SB_BYTES >> 9);\n\t\tmddev->reshape_backwards = 0;\n\n\t\tif (mddev->minor_version >= 91) {\n\t\t\tmddev->reshape_position = sb->reshape_position;\n\t\t\tmddev->delta_disks = sb->delta_disks;\n\t\t\tmddev->new_level = sb->new_level;\n\t\t\tmddev->new_layout = sb->new_layout;\n\t\t\tmddev->new_chunk_sectors = sb->new_chunk >> 9;\n\t\t\tif (mddev->delta_disks < 0)\n\t\t\t\tmddev->reshape_backwards = 1;\n\t\t} else {\n\t\t\tmddev->reshape_position = MaxSector;\n\t\t\tmddev->delta_disks = 0;\n\t\t\tmddev->new_level = mddev->level;\n\t\t\tmddev->new_layout = mddev->layout;\n\t\t\tmddev->new_chunk_sectors = mddev->chunk_sectors;\n\t\t}\n\t\tif (mddev->level == 0)\n\t\t\tmddev->layout = -1;\n\n\t\tif (sb->state & (1<<MD_SB_CLEAN))\n\t\t\tmddev->recovery_cp = MaxSector;\n\t\telse {\n\t\t\tif (sb->events_hi == sb->cp_events_hi &&\n\t\t\t\tsb->events_lo == sb->cp_events_lo) {\n\t\t\t\tmddev->recovery_cp = sb->recovery_cp;\n\t\t\t} else\n\t\t\t\tmddev->recovery_cp = 0;\n\t\t}\n\n\t\tmemcpy(mddev->uuid+0, &sb->set_uuid0, 4);\n\t\tmemcpy(mddev->uuid+4, &sb->set_uuid1, 4);\n\t\tmemcpy(mddev->uuid+8, &sb->set_uuid2, 4);\n\t\tmemcpy(mddev->uuid+12,&sb->set_uuid3, 4);\n\n\t\tmddev->max_disks = MD_SB_DISKS;\n\n\t\tif (sb->state & (1<<MD_SB_BITMAP_PRESENT) &&\n\t\t    mddev->bitmap_info.file == NULL) {\n\t\t\tmddev->bitmap_info.offset =\n\t\t\t\tmddev->bitmap_info.default_offset;\n\t\t\tmddev->bitmap_info.space =\n\t\t\t\tmddev->bitmap_info.default_space;\n\t\t}\n\n\t} else if (mddev->pers == NULL) {\n\t\t \n\t\t++ev1;\n\t\tif (sb->disks[rdev->desc_nr].state & (\n\t\t\t    (1<<MD_DISK_SYNC) | (1 << MD_DISK_ACTIVE)))\n\t\t\tif (ev1 < mddev->events)\n\t\t\t\treturn -EINVAL;\n\t} else if (mddev->bitmap) {\n\t\t \n\t\tif (ev1 < mddev->bitmap->events_cleared)\n\t\t\treturn 0;\n\t\tif (ev1 < mddev->events)\n\t\t\tset_bit(Bitmap_sync, &rdev->flags);\n\t} else {\n\t\tif (ev1 < mddev->events)\n\t\t\t \n\t\t\treturn 0;\n\t}\n\n\tif (mddev->level != LEVEL_MULTIPATH) {\n\t\tdesc = sb->disks + rdev->desc_nr;\n\n\t\tif (desc->state & (1<<MD_DISK_FAULTY))\n\t\t\tset_bit(Faulty, &rdev->flags);\n\t\telse if (desc->state & (1<<MD_DISK_SYNC)  ) {\n\t\t\tset_bit(In_sync, &rdev->flags);\n\t\t\trdev->raid_disk = desc->raid_disk;\n\t\t\trdev->saved_raid_disk = desc->raid_disk;\n\t\t} else if (desc->state & (1<<MD_DISK_ACTIVE)) {\n\t\t\t \n\t\t\tif (mddev->minor_version >= 91) {\n\t\t\t\trdev->recovery_offset = 0;\n\t\t\t\trdev->raid_disk = desc->raid_disk;\n\t\t\t}\n\t\t}\n\t\tif (desc->state & (1<<MD_DISK_WRITEMOSTLY))\n\t\t\tset_bit(WriteMostly, &rdev->flags);\n\t\tif (desc->state & (1<<MD_DISK_FAILFAST))\n\t\t\tset_bit(FailFast, &rdev->flags);\n\t} else  \n\t\tset_bit(In_sync, &rdev->flags);\n\treturn 0;\n}\n\n \nstatic void super_90_sync(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tmdp_super_t *sb;\n\tstruct md_rdev *rdev2;\n\tint next_spare = mddev->raid_disks;\n\n\t \n\tint i;\n\tint active=0, working=0,failed=0,spare=0,nr_disks=0;\n\n\trdev->sb_size = MD_SB_BYTES;\n\n\tsb = page_address(rdev->sb_page);\n\n\tmemset(sb, 0, sizeof(*sb));\n\n\tsb->md_magic = MD_SB_MAGIC;\n\tsb->major_version = mddev->major_version;\n\tsb->patch_version = mddev->patch_version;\n\tsb->gvalid_words  = 0;  \n\tmemcpy(&sb->set_uuid0, mddev->uuid+0, 4);\n\tmemcpy(&sb->set_uuid1, mddev->uuid+4, 4);\n\tmemcpy(&sb->set_uuid2, mddev->uuid+8, 4);\n\tmemcpy(&sb->set_uuid3, mddev->uuid+12,4);\n\n\tsb->ctime = clamp_t(time64_t, mddev->ctime, 0, U32_MAX);\n\tsb->level = mddev->level;\n\tsb->size = mddev->dev_sectors / 2;\n\tsb->raid_disks = mddev->raid_disks;\n\tsb->md_minor = mddev->md_minor;\n\tsb->not_persistent = 0;\n\tsb->utime = clamp_t(time64_t, mddev->utime, 0, U32_MAX);\n\tsb->state = 0;\n\tsb->events_hi = (mddev->events>>32);\n\tsb->events_lo = (u32)mddev->events;\n\n\tif (mddev->reshape_position == MaxSector)\n\t\tsb->minor_version = 90;\n\telse {\n\t\tsb->minor_version = 91;\n\t\tsb->reshape_position = mddev->reshape_position;\n\t\tsb->new_level = mddev->new_level;\n\t\tsb->delta_disks = mddev->delta_disks;\n\t\tsb->new_layout = mddev->new_layout;\n\t\tsb->new_chunk = mddev->new_chunk_sectors << 9;\n\t}\n\tmddev->minor_version = sb->minor_version;\n\tif (mddev->in_sync)\n\t{\n\t\tsb->recovery_cp = mddev->recovery_cp;\n\t\tsb->cp_events_hi = (mddev->events>>32);\n\t\tsb->cp_events_lo = (u32)mddev->events;\n\t\tif (mddev->recovery_cp == MaxSector)\n\t\t\tsb->state = (1<< MD_SB_CLEAN);\n\t} else\n\t\tsb->recovery_cp = 0;\n\n\tsb->layout = mddev->layout;\n\tsb->chunk_size = mddev->chunk_sectors << 9;\n\n\tif (mddev->bitmap && mddev->bitmap_info.file == NULL)\n\t\tsb->state |= (1<<MD_SB_BITMAP_PRESENT);\n\n\tsb->disks[0].state = (1<<MD_DISK_REMOVED);\n\trdev_for_each(rdev2, mddev) {\n\t\tmdp_disk_t *d;\n\t\tint desc_nr;\n\t\tint is_active = test_bit(In_sync, &rdev2->flags);\n\n\t\tif (rdev2->raid_disk >= 0 &&\n\t\t    sb->minor_version >= 91)\n\t\t\t \n\t\t\tis_active = 1;\n\t\tif (rdev2->raid_disk < 0 ||\n\t\t    test_bit(Faulty, &rdev2->flags))\n\t\t\tis_active = 0;\n\t\tif (is_active)\n\t\t\tdesc_nr = rdev2->raid_disk;\n\t\telse\n\t\t\tdesc_nr = next_spare++;\n\t\trdev2->desc_nr = desc_nr;\n\t\td = &sb->disks[rdev2->desc_nr];\n\t\tnr_disks++;\n\t\td->number = rdev2->desc_nr;\n\t\td->major = MAJOR(rdev2->bdev->bd_dev);\n\t\td->minor = MINOR(rdev2->bdev->bd_dev);\n\t\tif (is_active)\n\t\t\td->raid_disk = rdev2->raid_disk;\n\t\telse\n\t\t\td->raid_disk = rdev2->desc_nr;  \n\t\tif (test_bit(Faulty, &rdev2->flags))\n\t\t\td->state = (1<<MD_DISK_FAULTY);\n\t\telse if (is_active) {\n\t\t\td->state = (1<<MD_DISK_ACTIVE);\n\t\t\tif (test_bit(In_sync, &rdev2->flags))\n\t\t\t\td->state |= (1<<MD_DISK_SYNC);\n\t\t\tactive++;\n\t\t\tworking++;\n\t\t} else {\n\t\t\td->state = 0;\n\t\t\tspare++;\n\t\t\tworking++;\n\t\t}\n\t\tif (test_bit(WriteMostly, &rdev2->flags))\n\t\t\td->state |= (1<<MD_DISK_WRITEMOSTLY);\n\t\tif (test_bit(FailFast, &rdev2->flags))\n\t\t\td->state |= (1<<MD_DISK_FAILFAST);\n\t}\n\t \n\tfor (i=0 ; i < mddev->raid_disks ; i++) {\n\t\tmdp_disk_t *d = &sb->disks[i];\n\t\tif (d->state == 0 && d->number == 0) {\n\t\t\td->number = i;\n\t\t\td->raid_disk = i;\n\t\t\td->state = (1<<MD_DISK_REMOVED);\n\t\t\td->state |= (1<<MD_DISK_FAULTY);\n\t\t\tfailed++;\n\t\t}\n\t}\n\tsb->nr_disks = nr_disks;\n\tsb->active_disks = active;\n\tsb->working_disks = working;\n\tsb->failed_disks = failed;\n\tsb->spare_disks = spare;\n\n\tsb->this_disk = sb->disks[rdev->desc_nr];\n\tsb->sb_csum = calc_sb_csum(sb);\n}\n\n \nstatic unsigned long long\nsuper_90_rdev_size_change(struct md_rdev *rdev, sector_t num_sectors)\n{\n\tif (num_sectors && num_sectors < rdev->mddev->dev_sectors)\n\t\treturn 0;  \n\tif (rdev->mddev->bitmap_info.offset)\n\t\treturn 0;  \n\trdev->sb_start = calc_dev_sboffset(rdev);\n\tif (!num_sectors || num_sectors > rdev->sb_start)\n\t\tnum_sectors = rdev->sb_start;\n\t \n\tif ((u64)num_sectors >= (2ULL << 32) && rdev->mddev->level >= 1)\n\t\tnum_sectors = (sector_t)(2ULL << 32) - 2;\n\tdo {\n\t\tmd_super_write(rdev->mddev, rdev, rdev->sb_start, rdev->sb_size,\n\t\t       rdev->sb_page);\n\t} while (md_super_wait(rdev->mddev) < 0);\n\treturn num_sectors;\n}\n\nstatic int\nsuper_90_allow_new_offset(struct md_rdev *rdev, unsigned long long new_offset)\n{\n\t \n\treturn new_offset == 0;\n}\n\n \n\nstatic __le32 calc_sb_1_csum(struct mdp_superblock_1 *sb)\n{\n\t__le32 disk_csum;\n\tu32 csum;\n\tunsigned long long newcsum;\n\tint size = 256 + le32_to_cpu(sb->max_dev)*2;\n\t__le32 *isuper = (__le32*)sb;\n\n\tdisk_csum = sb->sb_csum;\n\tsb->sb_csum = 0;\n\tnewcsum = 0;\n\tfor (; size >= 4; size -= 4)\n\t\tnewcsum += le32_to_cpu(*isuper++);\n\n\tif (size == 2)\n\t\tnewcsum += le16_to_cpu(*(__le16*) isuper);\n\n\tcsum = (newcsum & 0xffffffff) + (newcsum >> 32);\n\tsb->sb_csum = disk_csum;\n\treturn cpu_to_le32(csum);\n}\n\nstatic int super_1_load(struct md_rdev *rdev, struct md_rdev *refdev, int minor_version)\n{\n\tstruct mdp_superblock_1 *sb;\n\tint ret;\n\tsector_t sb_start;\n\tsector_t sectors;\n\tint bmask;\n\tbool spare_disk = true;\n\n\t \n\tswitch(minor_version) {\n\tcase 0:\n\t\tsb_start = bdev_nr_sectors(rdev->bdev) - 8 * 2;\n\t\tsb_start &= ~(sector_t)(4*2-1);\n\t\tbreak;\n\tcase 1:\n\t\tsb_start = 0;\n\t\tbreak;\n\tcase 2:\n\t\tsb_start = 8;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\trdev->sb_start = sb_start;\n\n\t \n\tret = read_disk_sb(rdev, 4096);\n\tif (ret) return ret;\n\n\tsb = page_address(rdev->sb_page);\n\n\tif (sb->magic != cpu_to_le32(MD_SB_MAGIC) ||\n\t    sb->major_version != cpu_to_le32(1) ||\n\t    le32_to_cpu(sb->max_dev) > (4096-256)/2 ||\n\t    le64_to_cpu(sb->super_offset) != rdev->sb_start ||\n\t    (le32_to_cpu(sb->feature_map) & ~MD_FEATURE_ALL) != 0)\n\t\treturn -EINVAL;\n\n\tif (calc_sb_1_csum(sb) != sb->sb_csum) {\n\t\tpr_warn(\"md: invalid superblock checksum on %pg\\n\",\n\t\t\trdev->bdev);\n\t\treturn -EINVAL;\n\t}\n\tif (le64_to_cpu(sb->data_size) < 10) {\n\t\tpr_warn(\"md: data_size too small on %pg\\n\",\n\t\t\trdev->bdev);\n\t\treturn -EINVAL;\n\t}\n\tif (sb->pad0 ||\n\t    sb->pad3[0] ||\n\t    memcmp(sb->pad3, sb->pad3+1, sizeof(sb->pad3) - sizeof(sb->pad3[1])))\n\t\t \n\t\treturn -EINVAL;\n\n\trdev->preferred_minor = 0xffff;\n\trdev->data_offset = le64_to_cpu(sb->data_offset);\n\trdev->new_data_offset = rdev->data_offset;\n\tif ((le32_to_cpu(sb->feature_map) & MD_FEATURE_RESHAPE_ACTIVE) &&\n\t    (le32_to_cpu(sb->feature_map) & MD_FEATURE_NEW_OFFSET))\n\t\trdev->new_data_offset += (s32)le32_to_cpu(sb->new_offset);\n\tatomic_set(&rdev->corrected_errors, le32_to_cpu(sb->cnt_corrected_read));\n\n\trdev->sb_size = le32_to_cpu(sb->max_dev) * 2 + 256;\n\tbmask = queue_logical_block_size(rdev->bdev->bd_disk->queue)-1;\n\tif (rdev->sb_size & bmask)\n\t\trdev->sb_size = (rdev->sb_size | bmask) + 1;\n\n\tif (minor_version\n\t    && rdev->data_offset < sb_start + (rdev->sb_size/512))\n\t\treturn -EINVAL;\n\tif (minor_version\n\t    && rdev->new_data_offset < sb_start + (rdev->sb_size/512))\n\t\treturn -EINVAL;\n\n\tif (sb->level == cpu_to_le32(LEVEL_MULTIPATH))\n\t\trdev->desc_nr = -1;\n\telse\n\t\trdev->desc_nr = le32_to_cpu(sb->dev_number);\n\n\tif (!rdev->bb_page) {\n\t\trdev->bb_page = alloc_page(GFP_KERNEL);\n\t\tif (!rdev->bb_page)\n\t\t\treturn -ENOMEM;\n\t}\n\tif ((le32_to_cpu(sb->feature_map) & MD_FEATURE_BAD_BLOCKS) &&\n\t    rdev->badblocks.count == 0) {\n\t\t \n\t\ts32 offset;\n\t\tsector_t bb_sector;\n\t\t__le64 *bbp;\n\t\tint i;\n\t\tint sectors = le16_to_cpu(sb->bblog_size);\n\t\tif (sectors > (PAGE_SIZE / 512))\n\t\t\treturn -EINVAL;\n\t\toffset = le32_to_cpu(sb->bblog_offset);\n\t\tif (offset == 0)\n\t\t\treturn -EINVAL;\n\t\tbb_sector = (long long)offset;\n\t\tif (!sync_page_io(rdev, bb_sector, sectors << 9,\n\t\t\t\t  rdev->bb_page, REQ_OP_READ, true))\n\t\t\treturn -EIO;\n\t\tbbp = (__le64 *)page_address(rdev->bb_page);\n\t\trdev->badblocks.shift = sb->bblog_shift;\n\t\tfor (i = 0 ; i < (sectors << (9-3)) ; i++, bbp++) {\n\t\t\tu64 bb = le64_to_cpu(*bbp);\n\t\t\tint count = bb & (0x3ff);\n\t\t\tu64 sector = bb >> 10;\n\t\t\tsector <<= sb->bblog_shift;\n\t\t\tcount <<= sb->bblog_shift;\n\t\t\tif (bb + 1 == 0)\n\t\t\t\tbreak;\n\t\t\tif (badblocks_set(&rdev->badblocks, sector, count, 1))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t} else if (sb->bblog_offset != 0)\n\t\trdev->badblocks.shift = 0;\n\n\tif ((le32_to_cpu(sb->feature_map) &\n\t    (MD_FEATURE_PPL | MD_FEATURE_MULTIPLE_PPLS))) {\n\t\trdev->ppl.offset = (__s16)le16_to_cpu(sb->ppl.offset);\n\t\trdev->ppl.size = le16_to_cpu(sb->ppl.size);\n\t\trdev->ppl.sector = rdev->sb_start + rdev->ppl.offset;\n\t}\n\n\tif ((le32_to_cpu(sb->feature_map) & MD_FEATURE_RAID0_LAYOUT) &&\n\t    sb->level != 0)\n\t\treturn -EINVAL;\n\n\t \n\tif (sb->level == cpu_to_le32(LEVEL_MULTIPATH) ||\n\t\t(rdev->desc_nr >= 0 &&\n\t\trdev->desc_nr < le32_to_cpu(sb->max_dev) &&\n\t\t(le16_to_cpu(sb->dev_roles[rdev->desc_nr]) < MD_DISK_ROLE_MAX ||\n\t\t le16_to_cpu(sb->dev_roles[rdev->desc_nr]) == MD_DISK_ROLE_JOURNAL)))\n\t\tspare_disk = false;\n\n\tif (!refdev) {\n\t\tif (!spare_disk)\n\t\t\tret = 1;\n\t\telse\n\t\t\tret = 0;\n\t} else {\n\t\t__u64 ev1, ev2;\n\t\tstruct mdp_superblock_1 *refsb = page_address(refdev->sb_page);\n\n\t\tif (memcmp(sb->set_uuid, refsb->set_uuid, 16) != 0 ||\n\t\t    sb->level != refsb->level ||\n\t\t    sb->layout != refsb->layout ||\n\t\t    sb->chunksize != refsb->chunksize) {\n\t\t\tpr_warn(\"md: %pg has strangely different superblock to %pg\\n\",\n\t\t\t\trdev->bdev,\n\t\t\t\trefdev->bdev);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tev1 = le64_to_cpu(sb->events);\n\t\tev2 = le64_to_cpu(refsb->events);\n\n\t\tif (!spare_disk && ev1 > ev2)\n\t\t\tret = 1;\n\t\telse\n\t\t\tret = 0;\n\t}\n\tif (minor_version)\n\t\tsectors = bdev_nr_sectors(rdev->bdev) - rdev->data_offset;\n\telse\n\t\tsectors = rdev->sb_start;\n\tif (sectors < le64_to_cpu(sb->data_size))\n\t\treturn -EINVAL;\n\trdev->sectors = le64_to_cpu(sb->data_size);\n\treturn ret;\n}\n\nstatic int super_1_validate(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tstruct mdp_superblock_1 *sb = page_address(rdev->sb_page);\n\t__u64 ev1 = le64_to_cpu(sb->events);\n\n\trdev->raid_disk = -1;\n\tclear_bit(Faulty, &rdev->flags);\n\tclear_bit(In_sync, &rdev->flags);\n\tclear_bit(Bitmap_sync, &rdev->flags);\n\tclear_bit(WriteMostly, &rdev->flags);\n\n\tif (mddev->raid_disks == 0) {\n\t\tmddev->major_version = 1;\n\t\tmddev->patch_version = 0;\n\t\tmddev->external = 0;\n\t\tmddev->chunk_sectors = le32_to_cpu(sb->chunksize);\n\t\tmddev->ctime = le64_to_cpu(sb->ctime);\n\t\tmddev->utime = le64_to_cpu(sb->utime);\n\t\tmddev->level = le32_to_cpu(sb->level);\n\t\tmddev->clevel[0] = 0;\n\t\tmddev->layout = le32_to_cpu(sb->layout);\n\t\tmddev->raid_disks = le32_to_cpu(sb->raid_disks);\n\t\tmddev->dev_sectors = le64_to_cpu(sb->size);\n\t\tmddev->events = ev1;\n\t\tmddev->bitmap_info.offset = 0;\n\t\tmddev->bitmap_info.space = 0;\n\t\t \n\t\tmddev->bitmap_info.default_offset = 1024 >> 9;\n\t\tmddev->bitmap_info.default_space = (4096-1024) >> 9;\n\t\tmddev->reshape_backwards = 0;\n\n\t\tmddev->recovery_cp = le64_to_cpu(sb->resync_offset);\n\t\tmemcpy(mddev->uuid, sb->set_uuid, 16);\n\n\t\tmddev->max_disks =  (4096-256)/2;\n\n\t\tif ((le32_to_cpu(sb->feature_map) & MD_FEATURE_BITMAP_OFFSET) &&\n\t\t    mddev->bitmap_info.file == NULL) {\n\t\t\tmddev->bitmap_info.offset =\n\t\t\t\t(__s32)le32_to_cpu(sb->bitmap_offset);\n\t\t\t \n\t\t\tif (mddev->minor_version > 0)\n\t\t\t\tmddev->bitmap_info.space = 0;\n\t\t\telse if (mddev->bitmap_info.offset > 0)\n\t\t\t\tmddev->bitmap_info.space =\n\t\t\t\t\t8 - mddev->bitmap_info.offset;\n\t\t\telse\n\t\t\t\tmddev->bitmap_info.space =\n\t\t\t\t\t-mddev->bitmap_info.offset;\n\t\t}\n\n\t\tif ((le32_to_cpu(sb->feature_map) & MD_FEATURE_RESHAPE_ACTIVE)) {\n\t\t\tmddev->reshape_position = le64_to_cpu(sb->reshape_position);\n\t\t\tmddev->delta_disks = le32_to_cpu(sb->delta_disks);\n\t\t\tmddev->new_level = le32_to_cpu(sb->new_level);\n\t\t\tmddev->new_layout = le32_to_cpu(sb->new_layout);\n\t\t\tmddev->new_chunk_sectors = le32_to_cpu(sb->new_chunk);\n\t\t\tif (mddev->delta_disks < 0 ||\n\t\t\t    (mddev->delta_disks == 0 &&\n\t\t\t     (le32_to_cpu(sb->feature_map)\n\t\t\t      & MD_FEATURE_RESHAPE_BACKWARDS)))\n\t\t\t\tmddev->reshape_backwards = 1;\n\t\t} else {\n\t\t\tmddev->reshape_position = MaxSector;\n\t\t\tmddev->delta_disks = 0;\n\t\t\tmddev->new_level = mddev->level;\n\t\t\tmddev->new_layout = mddev->layout;\n\t\t\tmddev->new_chunk_sectors = mddev->chunk_sectors;\n\t\t}\n\n\t\tif (mddev->level == 0 &&\n\t\t    !(le32_to_cpu(sb->feature_map) & MD_FEATURE_RAID0_LAYOUT))\n\t\t\tmddev->layout = -1;\n\n\t\tif (le32_to_cpu(sb->feature_map) & MD_FEATURE_JOURNAL)\n\t\t\tset_bit(MD_HAS_JOURNAL, &mddev->flags);\n\n\t\tif (le32_to_cpu(sb->feature_map) &\n\t\t    (MD_FEATURE_PPL | MD_FEATURE_MULTIPLE_PPLS)) {\n\t\t\tif (le32_to_cpu(sb->feature_map) &\n\t\t\t    (MD_FEATURE_BITMAP_OFFSET | MD_FEATURE_JOURNAL))\n\t\t\t\treturn -EINVAL;\n\t\t\tif ((le32_to_cpu(sb->feature_map) & MD_FEATURE_PPL) &&\n\t\t\t    (le32_to_cpu(sb->feature_map) &\n\t\t\t\t\t    MD_FEATURE_MULTIPLE_PPLS))\n\t\t\t\treturn -EINVAL;\n\t\t\tset_bit(MD_HAS_PPL, &mddev->flags);\n\t\t}\n\t} else if (mddev->pers == NULL) {\n\t\t \n\t\t++ev1;\n\t\tif (rdev->desc_nr >= 0 &&\n\t\t    rdev->desc_nr < le32_to_cpu(sb->max_dev) &&\n\t\t    (le16_to_cpu(sb->dev_roles[rdev->desc_nr]) < MD_DISK_ROLE_MAX ||\n\t\t     le16_to_cpu(sb->dev_roles[rdev->desc_nr]) == MD_DISK_ROLE_JOURNAL))\n\t\t\tif (ev1 < mddev->events)\n\t\t\t\treturn -EINVAL;\n\t} else if (mddev->bitmap) {\n\t\t \n\t\tif (ev1 < mddev->bitmap->events_cleared)\n\t\t\treturn 0;\n\t\tif (ev1 < mddev->events)\n\t\t\tset_bit(Bitmap_sync, &rdev->flags);\n\t} else {\n\t\tif (ev1 < mddev->events)\n\t\t\t \n\t\t\treturn 0;\n\t}\n\tif (mddev->level != LEVEL_MULTIPATH) {\n\t\tint role;\n\t\tif (rdev->desc_nr < 0 ||\n\t\t    rdev->desc_nr >= le32_to_cpu(sb->max_dev)) {\n\t\t\trole = MD_DISK_ROLE_SPARE;\n\t\t\trdev->desc_nr = -1;\n\t\t} else\n\t\t\trole = le16_to_cpu(sb->dev_roles[rdev->desc_nr]);\n\t\tswitch(role) {\n\t\tcase MD_DISK_ROLE_SPARE:  \n\t\t\tbreak;\n\t\tcase MD_DISK_ROLE_FAULTY:  \n\t\t\tset_bit(Faulty, &rdev->flags);\n\t\t\tbreak;\n\t\tcase MD_DISK_ROLE_JOURNAL:  \n\t\t\tif (!(le32_to_cpu(sb->feature_map) & MD_FEATURE_JOURNAL)) {\n\t\t\t\t \n\t\t\t\tpr_warn(\"md: journal device provided without journal feature, ignoring the device\\n\");\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tset_bit(Journal, &rdev->flags);\n\t\t\trdev->journal_tail = le64_to_cpu(sb->journal_tail);\n\t\t\trdev->raid_disk = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\trdev->saved_raid_disk = role;\n\t\t\tif ((le32_to_cpu(sb->feature_map) &\n\t\t\t     MD_FEATURE_RECOVERY_OFFSET)) {\n\t\t\t\trdev->recovery_offset = le64_to_cpu(sb->recovery_offset);\n\t\t\t\tif (!(le32_to_cpu(sb->feature_map) &\n\t\t\t\t      MD_FEATURE_RECOVERY_BITMAP))\n\t\t\t\t\trdev->saved_raid_disk = -1;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tif (!test_bit(MD_RECOVERY_FROZEN,\n\t\t\t\t\t      &mddev->recovery))\n\t\t\t\t\tset_bit(In_sync, &rdev->flags);\n\t\t\t}\n\t\t\trdev->raid_disk = role;\n\t\t\tbreak;\n\t\t}\n\t\tif (sb->devflags & WriteMostly1)\n\t\t\tset_bit(WriteMostly, &rdev->flags);\n\t\tif (sb->devflags & FailFast1)\n\t\t\tset_bit(FailFast, &rdev->flags);\n\t\tif (le32_to_cpu(sb->feature_map) & MD_FEATURE_REPLACEMENT)\n\t\t\tset_bit(Replacement, &rdev->flags);\n\t} else  \n\t\tset_bit(In_sync, &rdev->flags);\n\n\treturn 0;\n}\n\nstatic void super_1_sync(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tstruct mdp_superblock_1 *sb;\n\tstruct md_rdev *rdev2;\n\tint max_dev, i;\n\t \n\n\tsb = page_address(rdev->sb_page);\n\n\tsb->feature_map = 0;\n\tsb->pad0 = 0;\n\tsb->recovery_offset = cpu_to_le64(0);\n\tmemset(sb->pad3, 0, sizeof(sb->pad3));\n\n\tsb->utime = cpu_to_le64((__u64)mddev->utime);\n\tsb->events = cpu_to_le64(mddev->events);\n\tif (mddev->in_sync)\n\t\tsb->resync_offset = cpu_to_le64(mddev->recovery_cp);\n\telse if (test_bit(MD_JOURNAL_CLEAN, &mddev->flags))\n\t\tsb->resync_offset = cpu_to_le64(MaxSector);\n\telse\n\t\tsb->resync_offset = cpu_to_le64(0);\n\n\tsb->cnt_corrected_read = cpu_to_le32(atomic_read(&rdev->corrected_errors));\n\n\tsb->raid_disks = cpu_to_le32(mddev->raid_disks);\n\tsb->size = cpu_to_le64(mddev->dev_sectors);\n\tsb->chunksize = cpu_to_le32(mddev->chunk_sectors);\n\tsb->level = cpu_to_le32(mddev->level);\n\tsb->layout = cpu_to_le32(mddev->layout);\n\tif (test_bit(FailFast, &rdev->flags))\n\t\tsb->devflags |= FailFast1;\n\telse\n\t\tsb->devflags &= ~FailFast1;\n\n\tif (test_bit(WriteMostly, &rdev->flags))\n\t\tsb->devflags |= WriteMostly1;\n\telse\n\t\tsb->devflags &= ~WriteMostly1;\n\tsb->data_offset = cpu_to_le64(rdev->data_offset);\n\tsb->data_size = cpu_to_le64(rdev->sectors);\n\n\tif (mddev->bitmap && mddev->bitmap_info.file == NULL) {\n\t\tsb->bitmap_offset = cpu_to_le32((__u32)mddev->bitmap_info.offset);\n\t\tsb->feature_map = cpu_to_le32(MD_FEATURE_BITMAP_OFFSET);\n\t}\n\n\tif (rdev->raid_disk >= 0 && !test_bit(Journal, &rdev->flags) &&\n\t    !test_bit(In_sync, &rdev->flags)) {\n\t\tsb->feature_map |=\n\t\t\tcpu_to_le32(MD_FEATURE_RECOVERY_OFFSET);\n\t\tsb->recovery_offset =\n\t\t\tcpu_to_le64(rdev->recovery_offset);\n\t\tif (rdev->saved_raid_disk >= 0 && mddev->bitmap)\n\t\t\tsb->feature_map |=\n\t\t\t\tcpu_to_le32(MD_FEATURE_RECOVERY_BITMAP);\n\t}\n\t \n\tif (test_bit(Journal, &rdev->flags))\n\t\tsb->journal_tail = cpu_to_le64(rdev->journal_tail);\n\tif (test_bit(Replacement, &rdev->flags))\n\t\tsb->feature_map |=\n\t\t\tcpu_to_le32(MD_FEATURE_REPLACEMENT);\n\n\tif (mddev->reshape_position != MaxSector) {\n\t\tsb->feature_map |= cpu_to_le32(MD_FEATURE_RESHAPE_ACTIVE);\n\t\tsb->reshape_position = cpu_to_le64(mddev->reshape_position);\n\t\tsb->new_layout = cpu_to_le32(mddev->new_layout);\n\t\tsb->delta_disks = cpu_to_le32(mddev->delta_disks);\n\t\tsb->new_level = cpu_to_le32(mddev->new_level);\n\t\tsb->new_chunk = cpu_to_le32(mddev->new_chunk_sectors);\n\t\tif (mddev->delta_disks == 0 &&\n\t\t    mddev->reshape_backwards)\n\t\t\tsb->feature_map\n\t\t\t\t|= cpu_to_le32(MD_FEATURE_RESHAPE_BACKWARDS);\n\t\tif (rdev->new_data_offset != rdev->data_offset) {\n\t\t\tsb->feature_map\n\t\t\t\t|= cpu_to_le32(MD_FEATURE_NEW_OFFSET);\n\t\t\tsb->new_offset = cpu_to_le32((__u32)(rdev->new_data_offset\n\t\t\t\t\t\t\t     - rdev->data_offset));\n\t\t}\n\t}\n\n\tif (mddev_is_clustered(mddev))\n\t\tsb->feature_map |= cpu_to_le32(MD_FEATURE_CLUSTERED);\n\n\tif (rdev->badblocks.count == 0)\n\t\t  ;\n\telse if (sb->bblog_offset == 0)\n\t\t \n\t\tmd_error(mddev, rdev);\n\telse {\n\t\tstruct badblocks *bb = &rdev->badblocks;\n\t\t__le64 *bbp = (__le64 *)page_address(rdev->bb_page);\n\t\tu64 *p = bb->page;\n\t\tsb->feature_map |= cpu_to_le32(MD_FEATURE_BAD_BLOCKS);\n\t\tif (bb->changed) {\n\t\t\tunsigned seq;\n\nretry:\n\t\t\tseq = read_seqbegin(&bb->lock);\n\n\t\t\tmemset(bbp, 0xff, PAGE_SIZE);\n\n\t\t\tfor (i = 0 ; i < bb->count ; i++) {\n\t\t\t\tu64 internal_bb = p[i];\n\t\t\t\tu64 store_bb = ((BB_OFFSET(internal_bb) << 10)\n\t\t\t\t\t\t| BB_LEN(internal_bb));\n\t\t\t\tbbp[i] = cpu_to_le64(store_bb);\n\t\t\t}\n\t\t\tbb->changed = 0;\n\t\t\tif (read_seqretry(&bb->lock, seq))\n\t\t\t\tgoto retry;\n\n\t\t\tbb->sector = (rdev->sb_start +\n\t\t\t\t      (int)le32_to_cpu(sb->bblog_offset));\n\t\t\tbb->size = le16_to_cpu(sb->bblog_size);\n\t\t}\n\t}\n\n\tmax_dev = 0;\n\trdev_for_each(rdev2, mddev)\n\t\tif (rdev2->desc_nr+1 > max_dev)\n\t\t\tmax_dev = rdev2->desc_nr+1;\n\n\tif (max_dev > le32_to_cpu(sb->max_dev)) {\n\t\tint bmask;\n\t\tsb->max_dev = cpu_to_le32(max_dev);\n\t\trdev->sb_size = max_dev * 2 + 256;\n\t\tbmask = queue_logical_block_size(rdev->bdev->bd_disk->queue)-1;\n\t\tif (rdev->sb_size & bmask)\n\t\t\trdev->sb_size = (rdev->sb_size | bmask) + 1;\n\t} else\n\t\tmax_dev = le32_to_cpu(sb->max_dev);\n\n\tfor (i=0; i<max_dev;i++)\n\t\tsb->dev_roles[i] = cpu_to_le16(MD_DISK_ROLE_SPARE);\n\n\tif (test_bit(MD_HAS_JOURNAL, &mddev->flags))\n\t\tsb->feature_map |= cpu_to_le32(MD_FEATURE_JOURNAL);\n\n\tif (test_bit(MD_HAS_PPL, &mddev->flags)) {\n\t\tif (test_bit(MD_HAS_MULTIPLE_PPLS, &mddev->flags))\n\t\t\tsb->feature_map |=\n\t\t\t    cpu_to_le32(MD_FEATURE_MULTIPLE_PPLS);\n\t\telse\n\t\t\tsb->feature_map |= cpu_to_le32(MD_FEATURE_PPL);\n\t\tsb->ppl.offset = cpu_to_le16(rdev->ppl.offset);\n\t\tsb->ppl.size = cpu_to_le16(rdev->ppl.size);\n\t}\n\n\trdev_for_each(rdev2, mddev) {\n\t\ti = rdev2->desc_nr;\n\t\tif (test_bit(Faulty, &rdev2->flags))\n\t\t\tsb->dev_roles[i] = cpu_to_le16(MD_DISK_ROLE_FAULTY);\n\t\telse if (test_bit(In_sync, &rdev2->flags))\n\t\t\tsb->dev_roles[i] = cpu_to_le16(rdev2->raid_disk);\n\t\telse if (test_bit(Journal, &rdev2->flags))\n\t\t\tsb->dev_roles[i] = cpu_to_le16(MD_DISK_ROLE_JOURNAL);\n\t\telse if (rdev2->raid_disk >= 0)\n\t\t\tsb->dev_roles[i] = cpu_to_le16(rdev2->raid_disk);\n\t\telse\n\t\t\tsb->dev_roles[i] = cpu_to_le16(MD_DISK_ROLE_SPARE);\n\t}\n\n\tsb->sb_csum = calc_sb_1_csum(sb);\n}\n\nstatic sector_t super_1_choose_bm_space(sector_t dev_size)\n{\n\tsector_t bm_space;\n\n\t \n\tif (dev_size < 64*2)\n\t\tbm_space = 0;\n\telse if (dev_size - 64*2 >= 200*1024*1024*2)\n\t\tbm_space = 128*2;\n\telse if (dev_size - 4*2 > 8*1024*1024*2)\n\t\tbm_space = 64*2;\n\telse\n\t\tbm_space = 4*2;\n\treturn bm_space;\n}\n\nstatic unsigned long long\nsuper_1_rdev_size_change(struct md_rdev *rdev, sector_t num_sectors)\n{\n\tstruct mdp_superblock_1 *sb;\n\tsector_t max_sectors;\n\tif (num_sectors && num_sectors < rdev->mddev->dev_sectors)\n\t\treturn 0;  \n\tif (rdev->data_offset != rdev->new_data_offset)\n\t\treturn 0;  \n\tif (rdev->sb_start < rdev->data_offset) {\n\t\t \n\t\tmax_sectors = bdev_nr_sectors(rdev->bdev) - rdev->data_offset;\n\t\tif (!num_sectors || num_sectors > max_sectors)\n\t\t\tnum_sectors = max_sectors;\n\t} else if (rdev->mddev->bitmap_info.offset) {\n\t\t \n\t\treturn 0;\n\t} else {\n\t\t \n\t\tsector_t sb_start, bm_space;\n\t\tsector_t dev_size = bdev_nr_sectors(rdev->bdev);\n\n\t\t \n\t\tsb_start = dev_size - 8*2;\n\t\tsb_start &= ~(sector_t)(4*2 - 1);\n\n\t\tbm_space = super_1_choose_bm_space(dev_size);\n\n\t\t \n\t\tmax_sectors = sb_start - bm_space - 4*2;\n\n\t\tif (!num_sectors || num_sectors > max_sectors)\n\t\t\tnum_sectors = max_sectors;\n\t\trdev->sb_start = sb_start;\n\t}\n\tsb = page_address(rdev->sb_page);\n\tsb->data_size = cpu_to_le64(num_sectors);\n\tsb->super_offset = cpu_to_le64(rdev->sb_start);\n\tsb->sb_csum = calc_sb_1_csum(sb);\n\tdo {\n\t\tmd_super_write(rdev->mddev, rdev, rdev->sb_start, rdev->sb_size,\n\t\t\t       rdev->sb_page);\n\t} while (md_super_wait(rdev->mddev) < 0);\n\treturn num_sectors;\n\n}\n\nstatic int\nsuper_1_allow_new_offset(struct md_rdev *rdev,\n\t\t\t unsigned long long new_offset)\n{\n\t \n\tstruct bitmap *bitmap;\n\tif (new_offset >= rdev->data_offset)\n\t\treturn 1;\n\n\t \n\tif (rdev->mddev->minor_version == 0)\n\t\treturn 1;\n\n\t \n\tif (rdev->sb_start + (32+4)*2 > new_offset)\n\t\treturn 0;\n\tbitmap = rdev->mddev->bitmap;\n\tif (bitmap && !rdev->mddev->bitmap_info.file &&\n\t    rdev->sb_start + rdev->mddev->bitmap_info.offset +\n\t    bitmap->storage.file_pages * (PAGE_SIZE>>9) > new_offset)\n\t\treturn 0;\n\tif (rdev->badblocks.sector + rdev->badblocks.size > new_offset)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic struct super_type super_types[] = {\n\t[0] = {\n\t\t.name\t= \"0.90.0\",\n\t\t.owner\t= THIS_MODULE,\n\t\t.load_super\t    = super_90_load,\n\t\t.validate_super\t    = super_90_validate,\n\t\t.sync_super\t    = super_90_sync,\n\t\t.rdev_size_change   = super_90_rdev_size_change,\n\t\t.allow_new_offset   = super_90_allow_new_offset,\n\t},\n\t[1] = {\n\t\t.name\t= \"md-1\",\n\t\t.owner\t= THIS_MODULE,\n\t\t.load_super\t    = super_1_load,\n\t\t.validate_super\t    = super_1_validate,\n\t\t.sync_super\t    = super_1_sync,\n\t\t.rdev_size_change   = super_1_rdev_size_change,\n\t\t.allow_new_offset   = super_1_allow_new_offset,\n\t},\n};\n\nstatic void sync_super(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tif (mddev->sync_super) {\n\t\tmddev->sync_super(mddev, rdev);\n\t\treturn;\n\t}\n\n\tBUG_ON(mddev->major_version >= ARRAY_SIZE(super_types));\n\n\tsuper_types[mddev->major_version].sync_super(mddev, rdev);\n}\n\nstatic int match_mddev_units(struct mddev *mddev1, struct mddev *mddev2)\n{\n\tstruct md_rdev *rdev, *rdev2;\n\n\trcu_read_lock();\n\trdev_for_each_rcu(rdev, mddev1) {\n\t\tif (test_bit(Faulty, &rdev->flags) ||\n\t\t    test_bit(Journal, &rdev->flags) ||\n\t\t    rdev->raid_disk == -1)\n\t\t\tcontinue;\n\t\trdev_for_each_rcu(rdev2, mddev2) {\n\t\t\tif (test_bit(Faulty, &rdev2->flags) ||\n\t\t\t    test_bit(Journal, &rdev2->flags) ||\n\t\t\t    rdev2->raid_disk == -1)\n\t\t\t\tcontinue;\n\t\t\tif (rdev->bdev->bd_disk == rdev2->bdev->bd_disk) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn 0;\n}\n\nstatic LIST_HEAD(pending_raid_disks);\n\n \nint md_integrity_register(struct mddev *mddev)\n{\n\tstruct md_rdev *rdev, *reference = NULL;\n\n\tif (list_empty(&mddev->disks))\n\t\treturn 0;  \n\tif (!mddev->gendisk || blk_get_integrity(mddev->gendisk))\n\t\treturn 0;  \n\trdev_for_each(rdev, mddev) {\n\t\t \n\t\tif (test_bit(Faulty, &rdev->flags))\n\t\t\tcontinue;\n\t\tif (rdev->raid_disk < 0)\n\t\t\tcontinue;\n\t\tif (!reference) {\n\t\t\t \n\t\t\treference = rdev;\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tif (blk_integrity_compare(reference->bdev->bd_disk,\n\t\t\t\trdev->bdev->bd_disk) < 0)\n\t\t\treturn -EINVAL;\n\t}\n\tif (!reference || !bdev_get_integrity(reference->bdev))\n\t\treturn 0;\n\t \n\tblk_integrity_register(mddev->gendisk,\n\t\t\t       bdev_get_integrity(reference->bdev));\n\n\tpr_debug(\"md: data integrity enabled on %s\\n\", mdname(mddev));\n\tif (bioset_integrity_create(&mddev->bio_set, BIO_POOL_SIZE) ||\n\t    (mddev->level != 1 && mddev->level != 10 &&\n\t     bioset_integrity_create(&mddev->io_clone_set, BIO_POOL_SIZE))) {\n\t\t \n\t\tpr_err(\"md: failed to create integrity pool for %s\\n\",\n\t\t       mdname(mddev));\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(md_integrity_register);\n\n \nint md_integrity_add_rdev(struct md_rdev *rdev, struct mddev *mddev)\n{\n\tstruct blk_integrity *bi_mddev;\n\n\tif (!mddev->gendisk)\n\t\treturn 0;\n\n\tbi_mddev = blk_get_integrity(mddev->gendisk);\n\n\tif (!bi_mddev)  \n\t\treturn 0;\n\n\tif (blk_integrity_compare(mddev->gendisk, rdev->bdev->bd_disk) != 0) {\n\t\tpr_err(\"%s: incompatible integrity profile for %pg\\n\",\n\t\t       mdname(mddev), rdev->bdev);\n\t\treturn -ENXIO;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(md_integrity_add_rdev);\n\nstatic bool rdev_read_only(struct md_rdev *rdev)\n{\n\treturn bdev_read_only(rdev->bdev) ||\n\t\t(rdev->meta_bdev && bdev_read_only(rdev->meta_bdev));\n}\n\nstatic int bind_rdev_to_array(struct md_rdev *rdev, struct mddev *mddev)\n{\n\tchar b[BDEVNAME_SIZE];\n\tint err;\n\n\t \n\tif (find_rdev(mddev, rdev->bdev->bd_dev))\n\t\treturn -EEXIST;\n\n\tif (rdev_read_only(rdev) && mddev->pers)\n\t\treturn -EROFS;\n\n\t \n\tif (!test_bit(Journal, &rdev->flags) &&\n\t    rdev->sectors &&\n\t    (mddev->dev_sectors == 0 || rdev->sectors < mddev->dev_sectors)) {\n\t\tif (mddev->pers) {\n\t\t\t \n\t\t\tif (mddev->level > 0)\n\t\t\t\treturn -ENOSPC;\n\t\t} else\n\t\t\tmddev->dev_sectors = rdev->sectors;\n\t}\n\n\t \n\trcu_read_lock();\n\tif (rdev->desc_nr < 0) {\n\t\tint choice = 0;\n\t\tif (mddev->pers)\n\t\t\tchoice = mddev->raid_disks;\n\t\twhile (md_find_rdev_nr_rcu(mddev, choice))\n\t\t\tchoice++;\n\t\trdev->desc_nr = choice;\n\t} else {\n\t\tif (md_find_rdev_nr_rcu(mddev, rdev->desc_nr)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn -EBUSY;\n\t\t}\n\t}\n\trcu_read_unlock();\n\tif (!test_bit(Journal, &rdev->flags) &&\n\t    mddev->max_disks && rdev->desc_nr >= mddev->max_disks) {\n\t\tpr_warn(\"md: %s: array is limited to %d devices\\n\",\n\t\t\tmdname(mddev), mddev->max_disks);\n\t\treturn -EBUSY;\n\t}\n\tsnprintf(b, sizeof(b), \"%pg\", rdev->bdev);\n\tstrreplace(b, '/', '!');\n\n\trdev->mddev = mddev;\n\tpr_debug(\"md: bind<%s>\\n\", b);\n\n\tif (mddev->raid_disks)\n\t\tmddev_create_serial_pool(mddev, rdev, false);\n\n\tif ((err = kobject_add(&rdev->kobj, &mddev->kobj, \"dev-%s\", b)))\n\t\tgoto fail;\n\n\t \n\terr = sysfs_create_link(&rdev->kobj, bdev_kobj(rdev->bdev), \"block\");\n\trdev->sysfs_state = sysfs_get_dirent_safe(rdev->kobj.sd, \"state\");\n\trdev->sysfs_unack_badblocks =\n\t\tsysfs_get_dirent_safe(rdev->kobj.sd, \"unacknowledged_bad_blocks\");\n\trdev->sysfs_badblocks =\n\t\tsysfs_get_dirent_safe(rdev->kobj.sd, \"bad_blocks\");\n\n\tlist_add_rcu(&rdev->same_set, &mddev->disks);\n\tbd_link_disk_holder(rdev->bdev, mddev->gendisk);\n\n\t \n\tmddev->recovery_disabled++;\n\n\treturn 0;\n\n fail:\n\tpr_warn(\"md: failed to register dev-%s for %s\\n\",\n\t\tb, mdname(mddev));\n\treturn err;\n}\n\nvoid md_autodetect_dev(dev_t dev);\n\n \nstatic struct md_rdev claim_rdev;\n\nstatic void export_rdev(struct md_rdev *rdev, struct mddev *mddev)\n{\n\tpr_debug(\"md: export_rdev(%pg)\\n\", rdev->bdev);\n\tmd_rdev_clear(rdev);\n#ifndef MODULE\n\tif (test_bit(AutoDetected, &rdev->flags))\n\t\tmd_autodetect_dev(rdev->bdev->bd_dev);\n#endif\n\tblkdev_put(rdev->bdev,\n\t\t   test_bit(Holder, &rdev->flags) ? rdev : &claim_rdev);\n\trdev->bdev = NULL;\n\tkobject_put(&rdev->kobj);\n}\n\nstatic void md_kick_rdev_from_array(struct md_rdev *rdev)\n{\n\tstruct mddev *mddev = rdev->mddev;\n\n\tbd_unlink_disk_holder(rdev->bdev, rdev->mddev->gendisk);\n\tlist_del_rcu(&rdev->same_set);\n\tpr_debug(\"md: unbind<%pg>\\n\", rdev->bdev);\n\tmddev_destroy_serial_pool(rdev->mddev, rdev, false);\n\trdev->mddev = NULL;\n\tsysfs_remove_link(&rdev->kobj, \"block\");\n\tsysfs_put(rdev->sysfs_state);\n\tsysfs_put(rdev->sysfs_unack_badblocks);\n\tsysfs_put(rdev->sysfs_badblocks);\n\trdev->sysfs_state = NULL;\n\trdev->sysfs_unack_badblocks = NULL;\n\trdev->sysfs_badblocks = NULL;\n\trdev->badblocks.count = 0;\n\n\tsynchronize_rcu();\n\n\t \n\tlist_add(&rdev->same_set, &mddev->deleting);\n}\n\nstatic void export_array(struct mddev *mddev)\n{\n\tstruct md_rdev *rdev;\n\n\twhile (!list_empty(&mddev->disks)) {\n\t\trdev = list_first_entry(&mddev->disks, struct md_rdev,\n\t\t\t\t\tsame_set);\n\t\tmd_kick_rdev_from_array(rdev);\n\t}\n\tmddev->raid_disks = 0;\n\tmddev->major_version = 0;\n}\n\nstatic bool set_in_sync(struct mddev *mddev)\n{\n\tlockdep_assert_held(&mddev->lock);\n\tif (!mddev->in_sync) {\n\t\tmddev->sync_checkers++;\n\t\tspin_unlock(&mddev->lock);\n\t\tpercpu_ref_switch_to_atomic_sync(&mddev->writes_pending);\n\t\tspin_lock(&mddev->lock);\n\t\tif (!mddev->in_sync &&\n\t\t    percpu_ref_is_zero(&mddev->writes_pending)) {\n\t\t\tmddev->in_sync = 1;\n\t\t\t \n\t\t\tsmp_mb();\n\t\t\tset_bit(MD_SB_CHANGE_CLEAN, &mddev->sb_flags);\n\t\t\tsysfs_notify_dirent_safe(mddev->sysfs_state);\n\t\t}\n\t\tif (--mddev->sync_checkers == 0)\n\t\t\tpercpu_ref_switch_to_percpu(&mddev->writes_pending);\n\t}\n\tif (mddev->safemode == 1)\n\t\tmddev->safemode = 0;\n\treturn mddev->in_sync;\n}\n\nstatic void sync_sbs(struct mddev *mddev, int nospares)\n{\n\t \n\tstruct md_rdev *rdev;\n\trdev_for_each(rdev, mddev) {\n\t\tif (rdev->sb_events == mddev->events ||\n\t\t    (nospares &&\n\t\t     rdev->raid_disk < 0 &&\n\t\t     rdev->sb_events+1 == mddev->events)) {\n\t\t\t \n\t\t\trdev->sb_loaded = 2;\n\t\t} else {\n\t\t\tsync_super(mddev, rdev);\n\t\t\trdev->sb_loaded = 1;\n\t\t}\n\t}\n}\n\nstatic bool does_sb_need_changing(struct mddev *mddev)\n{\n\tstruct md_rdev *rdev = NULL, *iter;\n\tstruct mdp_superblock_1 *sb;\n\tint role;\n\n\t \n\trdev_for_each(iter, mddev)\n\t\tif ((iter->raid_disk >= 0) && !test_bit(Faulty, &iter->flags)) {\n\t\t\trdev = iter;\n\t\t\tbreak;\n\t\t}\n\n\t \n\tif (!rdev)\n\t\treturn false;\n\n\tsb = page_address(rdev->sb_page);\n\t \n\trdev_for_each(rdev, mddev) {\n\t\trole = le16_to_cpu(sb->dev_roles[rdev->desc_nr]);\n\t\t \n\t\tif (role == MD_DISK_ROLE_SPARE && rdev->raid_disk >= 0 &&\n\t\t    !test_bit(Faulty, &rdev->flags))\n\t\t\treturn true;\n\t\t \n\t\tif (test_bit(Faulty, &rdev->flags) && (role < MD_DISK_ROLE_MAX))\n\t\t\treturn true;\n\t}\n\n\t \n\tif ((mddev->dev_sectors != le64_to_cpu(sb->size)) ||\n\t    (mddev->reshape_position != le64_to_cpu(sb->reshape_position)) ||\n\t    (mddev->layout != le32_to_cpu(sb->layout)) ||\n\t    (mddev->raid_disks != le32_to_cpu(sb->raid_disks)) ||\n\t    (mddev->chunk_sectors != le32_to_cpu(sb->chunksize)))\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid md_update_sb(struct mddev *mddev, int force_change)\n{\n\tstruct md_rdev *rdev;\n\tint sync_req;\n\tint nospares = 0;\n\tint any_badblocks_changed = 0;\n\tint ret = -1;\n\n\tif (!md_is_rdwr(mddev)) {\n\t\tif (force_change)\n\t\t\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\t\treturn;\n\t}\n\nrepeat:\n\tif (mddev_is_clustered(mddev)) {\n\t\tif (test_and_clear_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags))\n\t\t\tforce_change = 1;\n\t\tif (test_and_clear_bit(MD_SB_CHANGE_CLEAN, &mddev->sb_flags))\n\t\t\tnospares = 1;\n\t\tret = md_cluster_ops->metadata_update_start(mddev);\n\t\t \n\t\tif (!does_sb_need_changing(mddev)) {\n\t\t\tif (ret == 0)\n\t\t\t\tmd_cluster_ops->metadata_update_cancel(mddev);\n\t\t\tbit_clear_unless(&mddev->sb_flags, BIT(MD_SB_CHANGE_PENDING),\n\t\t\t\t\t\t\t BIT(MD_SB_CHANGE_DEVS) |\n\t\t\t\t\t\t\t BIT(MD_SB_CHANGE_CLEAN));\n\t\t\treturn;\n\t\t}\n\t}\n\n\t \n\trdev_for_each(rdev, mddev) {\n\t\tif (rdev->raid_disk >= 0 &&\n\t\t    mddev->delta_disks >= 0 &&\n\t\t    test_bit(MD_RECOVERY_RUNNING, &mddev->recovery) &&\n\t\t    test_bit(MD_RECOVERY_RECOVER, &mddev->recovery) &&\n\t\t    !test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&\n\t\t    !test_bit(Journal, &rdev->flags) &&\n\t\t    !test_bit(In_sync, &rdev->flags) &&\n\t\t    mddev->curr_resync_completed > rdev->recovery_offset)\n\t\t\t\trdev->recovery_offset = mddev->curr_resync_completed;\n\n\t}\n\tif (!mddev->persistent) {\n\t\tclear_bit(MD_SB_CHANGE_CLEAN, &mddev->sb_flags);\n\t\tclear_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\t\tif (!mddev->external) {\n\t\t\tclear_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags);\n\t\t\trdev_for_each(rdev, mddev) {\n\t\t\t\tif (rdev->badblocks.changed) {\n\t\t\t\t\trdev->badblocks.changed = 0;\n\t\t\t\t\tack_all_badblocks(&rdev->badblocks);\n\t\t\t\t\tmd_error(mddev, rdev);\n\t\t\t\t}\n\t\t\t\tclear_bit(Blocked, &rdev->flags);\n\t\t\t\tclear_bit(BlockedBadBlocks, &rdev->flags);\n\t\t\t\twake_up(&rdev->blocked_wait);\n\t\t\t}\n\t\t}\n\t\twake_up(&mddev->sb_wait);\n\t\treturn;\n\t}\n\n\tspin_lock(&mddev->lock);\n\n\tmddev->utime = ktime_get_real_seconds();\n\n\tif (test_and_clear_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags))\n\t\tforce_change = 1;\n\tif (test_and_clear_bit(MD_SB_CHANGE_CLEAN, &mddev->sb_flags))\n\t\t \n\t\tnospares = 1;\n\tif (force_change)\n\t\tnospares = 0;\n\tif (mddev->degraded)\n\t\t \n\t\tnospares = 0;\n\n\tsync_req = mddev->in_sync;\n\n\t \n\tif (nospares\n\t    && (mddev->in_sync && mddev->recovery_cp == MaxSector)\n\t    && mddev->can_decrease_events\n\t    && mddev->events != 1) {\n\t\tmddev->events--;\n\t\tmddev->can_decrease_events = 0;\n\t} else {\n\t\t \n\t\tmddev->events ++;\n\t\tmddev->can_decrease_events = nospares;\n\t}\n\n\t \n\tWARN_ON(mddev->events == 0);\n\n\trdev_for_each(rdev, mddev) {\n\t\tif (rdev->badblocks.changed)\n\t\t\tany_badblocks_changed++;\n\t\tif (test_bit(Faulty, &rdev->flags))\n\t\t\tset_bit(FaultRecorded, &rdev->flags);\n\t}\n\n\tsync_sbs(mddev, nospares);\n\tspin_unlock(&mddev->lock);\n\n\tpr_debug(\"md: updating %s RAID superblock on device (in sync %d)\\n\",\n\t\t mdname(mddev), mddev->in_sync);\n\n\tif (mddev->queue)\n\t\tblk_add_trace_msg(mddev->queue, \"md md_update_sb\");\nrewrite:\n\tmd_bitmap_update_sb(mddev->bitmap);\n\trdev_for_each(rdev, mddev) {\n\t\tif (rdev->sb_loaded != 1)\n\t\t\tcontinue;  \n\n\t\tif (!test_bit(Faulty, &rdev->flags)) {\n\t\t\tmd_super_write(mddev,rdev,\n\t\t\t\t       rdev->sb_start, rdev->sb_size,\n\t\t\t\t       rdev->sb_page);\n\t\t\tpr_debug(\"md: (write) %pg's sb offset: %llu\\n\",\n\t\t\t\t rdev->bdev,\n\t\t\t\t (unsigned long long)rdev->sb_start);\n\t\t\trdev->sb_events = mddev->events;\n\t\t\tif (rdev->badblocks.size) {\n\t\t\t\tmd_super_write(mddev, rdev,\n\t\t\t\t\t       rdev->badblocks.sector,\n\t\t\t\t\t       rdev->badblocks.size << 9,\n\t\t\t\t\t       rdev->bb_page);\n\t\t\t\trdev->badblocks.size = 0;\n\t\t\t}\n\n\t\t} else\n\t\t\tpr_debug(\"md: %pg (skipping faulty)\\n\",\n\t\t\t\t rdev->bdev);\n\n\t\tif (mddev->level == LEVEL_MULTIPATH)\n\t\t\t \n\t\t\tbreak;\n\t}\n\tif (md_super_wait(mddev) < 0)\n\t\tgoto rewrite;\n\t \n\n\tif (mddev_is_clustered(mddev) && ret == 0)\n\t\tmd_cluster_ops->metadata_update_finish(mddev);\n\n\tif (mddev->in_sync != sync_req ||\n\t    !bit_clear_unless(&mddev->sb_flags, BIT(MD_SB_CHANGE_PENDING),\n\t\t\t       BIT(MD_SB_CHANGE_DEVS) | BIT(MD_SB_CHANGE_CLEAN)))\n\t\t \n\t\tgoto repeat;\n\twake_up(&mddev->sb_wait);\n\tif (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))\n\t\tsysfs_notify_dirent_safe(mddev->sysfs_completed);\n\n\trdev_for_each(rdev, mddev) {\n\t\tif (test_and_clear_bit(FaultRecorded, &rdev->flags))\n\t\t\tclear_bit(Blocked, &rdev->flags);\n\n\t\tif (any_badblocks_changed)\n\t\t\tack_all_badblocks(&rdev->badblocks);\n\t\tclear_bit(BlockedBadBlocks, &rdev->flags);\n\t\twake_up(&rdev->blocked_wait);\n\t}\n}\nEXPORT_SYMBOL(md_update_sb);\n\nstatic int add_bound_rdev(struct md_rdev *rdev)\n{\n\tstruct mddev *mddev = rdev->mddev;\n\tint err = 0;\n\tbool add_journal = test_bit(Journal, &rdev->flags);\n\n\tif (!mddev->pers->hot_remove_disk || add_journal) {\n\t\t \n\t\tsuper_types[mddev->major_version].\n\t\t\tvalidate_super(mddev, rdev);\n\t\tif (add_journal)\n\t\t\tmddev_suspend(mddev);\n\t\terr = mddev->pers->hot_add_disk(mddev, rdev);\n\t\tif (add_journal)\n\t\t\tmddev_resume(mddev);\n\t\tif (err) {\n\t\t\tmd_kick_rdev_from_array(rdev);\n\t\t\treturn err;\n\t\t}\n\t}\n\tsysfs_notify_dirent_safe(rdev->sysfs_state);\n\n\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\tif (mddev->degraded)\n\t\tset_bit(MD_RECOVERY_RECOVER, &mddev->recovery);\n\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\tmd_new_event();\n\tmd_wakeup_thread(mddev->thread);\n\treturn 0;\n}\n\n \nstatic int cmd_match(const char *cmd, const char *str)\n{\n\t \n\twhile (*cmd && *str && *cmd == *str) {\n\t\tcmd++;\n\t\tstr++;\n\t}\n\tif (*cmd == '\\n')\n\t\tcmd++;\n\tif (*str || *cmd)\n\t\treturn 0;\n\treturn 1;\n}\n\nstruct rdev_sysfs_entry {\n\tstruct attribute attr;\n\tssize_t (*show)(struct md_rdev *, char *);\n\tssize_t (*store)(struct md_rdev *, const char *, size_t);\n};\n\nstatic ssize_t\nstate_show(struct md_rdev *rdev, char *page)\n{\n\tchar *sep = \",\";\n\tsize_t len = 0;\n\tunsigned long flags = READ_ONCE(rdev->flags);\n\n\tif (test_bit(Faulty, &flags) ||\n\t    (!test_bit(ExternalBbl, &flags) &&\n\t    rdev->badblocks.unacked_exist))\n\t\tlen += sprintf(page+len, \"faulty%s\", sep);\n\tif (test_bit(In_sync, &flags))\n\t\tlen += sprintf(page+len, \"in_sync%s\", sep);\n\tif (test_bit(Journal, &flags))\n\t\tlen += sprintf(page+len, \"journal%s\", sep);\n\tif (test_bit(WriteMostly, &flags))\n\t\tlen += sprintf(page+len, \"write_mostly%s\", sep);\n\tif (test_bit(Blocked, &flags) ||\n\t    (rdev->badblocks.unacked_exist\n\t     && !test_bit(Faulty, &flags)))\n\t\tlen += sprintf(page+len, \"blocked%s\", sep);\n\tif (!test_bit(Faulty, &flags) &&\n\t    !test_bit(Journal, &flags) &&\n\t    !test_bit(In_sync, &flags))\n\t\tlen += sprintf(page+len, \"spare%s\", sep);\n\tif (test_bit(WriteErrorSeen, &flags))\n\t\tlen += sprintf(page+len, \"write_error%s\", sep);\n\tif (test_bit(WantReplacement, &flags))\n\t\tlen += sprintf(page+len, \"want_replacement%s\", sep);\n\tif (test_bit(Replacement, &flags))\n\t\tlen += sprintf(page+len, \"replacement%s\", sep);\n\tif (test_bit(ExternalBbl, &flags))\n\t\tlen += sprintf(page+len, \"external_bbl%s\", sep);\n\tif (test_bit(FailFast, &flags))\n\t\tlen += sprintf(page+len, \"failfast%s\", sep);\n\n\tif (len)\n\t\tlen -= strlen(sep);\n\n\treturn len+sprintf(page+len, \"\\n\");\n}\n\nstatic ssize_t\nstate_store(struct md_rdev *rdev, const char *buf, size_t len)\n{\n\t \n\n\tstruct mddev *mddev = rdev->mddev;\n\tint err = -EINVAL;\n\tbool need_update_sb = false;\n\n\tif (cmd_match(buf, \"faulty\") && rdev->mddev->pers) {\n\t\tmd_error(rdev->mddev, rdev);\n\n\t\tif (test_bit(MD_BROKEN, &rdev->mddev->flags))\n\t\t\terr = -EBUSY;\n\t\telse\n\t\t\terr = 0;\n\t} else if (cmd_match(buf, \"remove\")) {\n\t\tif (rdev->mddev->pers) {\n\t\t\tclear_bit(Blocked, &rdev->flags);\n\t\t\tremove_and_add_spares(rdev->mddev, rdev);\n\t\t}\n\t\tif (rdev->raid_disk >= 0)\n\t\t\terr = -EBUSY;\n\t\telse {\n\t\t\terr = 0;\n\t\t\tif (mddev_is_clustered(mddev))\n\t\t\t\terr = md_cluster_ops->remove_disk(mddev, rdev);\n\n\t\t\tif (err == 0) {\n\t\t\t\tmd_kick_rdev_from_array(rdev);\n\t\t\t\tif (mddev->pers) {\n\t\t\t\t\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\t\t\t\t\tmd_wakeup_thread(mddev->thread);\n\t\t\t\t}\n\t\t\t\tmd_new_event();\n\t\t\t}\n\t\t}\n\t} else if (cmd_match(buf, \"writemostly\")) {\n\t\tset_bit(WriteMostly, &rdev->flags);\n\t\tmddev_create_serial_pool(rdev->mddev, rdev, false);\n\t\tneed_update_sb = true;\n\t\terr = 0;\n\t} else if (cmd_match(buf, \"-writemostly\")) {\n\t\tmddev_destroy_serial_pool(rdev->mddev, rdev, false);\n\t\tclear_bit(WriteMostly, &rdev->flags);\n\t\tneed_update_sb = true;\n\t\terr = 0;\n\t} else if (cmd_match(buf, \"blocked\")) {\n\t\tset_bit(Blocked, &rdev->flags);\n\t\terr = 0;\n\t} else if (cmd_match(buf, \"-blocked\")) {\n\t\tif (!test_bit(Faulty, &rdev->flags) &&\n\t\t    !test_bit(ExternalBbl, &rdev->flags) &&\n\t\t    rdev->badblocks.unacked_exist) {\n\t\t\t \n\t\t\tmd_error(rdev->mddev, rdev);\n\t\t}\n\t\tclear_bit(Blocked, &rdev->flags);\n\t\tclear_bit(BlockedBadBlocks, &rdev->flags);\n\t\twake_up(&rdev->blocked_wait);\n\t\tset_bit(MD_RECOVERY_NEEDED, &rdev->mddev->recovery);\n\t\tmd_wakeup_thread(rdev->mddev->thread);\n\n\t\terr = 0;\n\t} else if (cmd_match(buf, \"insync\") && rdev->raid_disk == -1) {\n\t\tset_bit(In_sync, &rdev->flags);\n\t\terr = 0;\n\t} else if (cmd_match(buf, \"failfast\")) {\n\t\tset_bit(FailFast, &rdev->flags);\n\t\tneed_update_sb = true;\n\t\terr = 0;\n\t} else if (cmd_match(buf, \"-failfast\")) {\n\t\tclear_bit(FailFast, &rdev->flags);\n\t\tneed_update_sb = true;\n\t\terr = 0;\n\t} else if (cmd_match(buf, \"-insync\") && rdev->raid_disk >= 0 &&\n\t\t   !test_bit(Journal, &rdev->flags)) {\n\t\tif (rdev->mddev->pers == NULL) {\n\t\t\tclear_bit(In_sync, &rdev->flags);\n\t\t\trdev->saved_raid_disk = rdev->raid_disk;\n\t\t\trdev->raid_disk = -1;\n\t\t\terr = 0;\n\t\t}\n\t} else if (cmd_match(buf, \"write_error\")) {\n\t\tset_bit(WriteErrorSeen, &rdev->flags);\n\t\terr = 0;\n\t} else if (cmd_match(buf, \"-write_error\")) {\n\t\tclear_bit(WriteErrorSeen, &rdev->flags);\n\t\terr = 0;\n\t} else if (cmd_match(buf, \"want_replacement\")) {\n\t\t \n\t\tif (rdev->raid_disk >= 0 &&\n\t\t    !test_bit(Journal, &rdev->flags) &&\n\t\t    !test_bit(Replacement, &rdev->flags))\n\t\t\tset_bit(WantReplacement, &rdev->flags);\n\t\tset_bit(MD_RECOVERY_NEEDED, &rdev->mddev->recovery);\n\t\tmd_wakeup_thread(rdev->mddev->thread);\n\t\terr = 0;\n\t} else if (cmd_match(buf, \"-want_replacement\")) {\n\t\t \n\t\terr = 0;\n\t\tclear_bit(WantReplacement, &rdev->flags);\n\t} else if (cmd_match(buf, \"replacement\")) {\n\t\t \n\t\tif (rdev->mddev->pers)\n\t\t\terr = -EBUSY;\n\t\telse {\n\t\t\tset_bit(Replacement, &rdev->flags);\n\t\t\terr = 0;\n\t\t}\n\t} else if (cmd_match(buf, \"-replacement\")) {\n\t\t \n\t\tif (rdev->mddev->pers)\n\t\t\terr = -EBUSY;\n\t\telse {\n\t\t\tclear_bit(Replacement, &rdev->flags);\n\t\t\terr = 0;\n\t\t}\n\t} else if (cmd_match(buf, \"re-add\")) {\n\t\tif (!rdev->mddev->pers)\n\t\t\terr = -EINVAL;\n\t\telse if (test_bit(Faulty, &rdev->flags) && (rdev->raid_disk == -1) &&\n\t\t\t\trdev->saved_raid_disk >= 0) {\n\t\t\t \n\t\t\tif (!mddev_is_clustered(rdev->mddev) ||\n\t\t\t    (err = md_cluster_ops->gather_bitmaps(rdev)) == 0) {\n\t\t\t\tclear_bit(Faulty, &rdev->flags);\n\t\t\t\terr = add_bound_rdev(rdev);\n\t\t\t}\n\t\t} else\n\t\t\terr = -EBUSY;\n\t} else if (cmd_match(buf, \"external_bbl\") && (rdev->mddev->external)) {\n\t\tset_bit(ExternalBbl, &rdev->flags);\n\t\trdev->badblocks.shift = 0;\n\t\terr = 0;\n\t} else if (cmd_match(buf, \"-external_bbl\") && (rdev->mddev->external)) {\n\t\tclear_bit(ExternalBbl, &rdev->flags);\n\t\terr = 0;\n\t}\n\tif (need_update_sb)\n\t\tmd_update_sb(mddev, 1);\n\tif (!err)\n\t\tsysfs_notify_dirent_safe(rdev->sysfs_state);\n\treturn err ? err : len;\n}\nstatic struct rdev_sysfs_entry rdev_state =\n__ATTR_PREALLOC(state, S_IRUGO|S_IWUSR, state_show, state_store);\n\nstatic ssize_t\nerrors_show(struct md_rdev *rdev, char *page)\n{\n\treturn sprintf(page, \"%d\\n\", atomic_read(&rdev->corrected_errors));\n}\n\nstatic ssize_t\nerrors_store(struct md_rdev *rdev, const char *buf, size_t len)\n{\n\tunsigned int n;\n\tint rv;\n\n\trv = kstrtouint(buf, 10, &n);\n\tif (rv < 0)\n\t\treturn rv;\n\tatomic_set(&rdev->corrected_errors, n);\n\treturn len;\n}\nstatic struct rdev_sysfs_entry rdev_errors =\n__ATTR(errors, S_IRUGO|S_IWUSR, errors_show, errors_store);\n\nstatic ssize_t\nslot_show(struct md_rdev *rdev, char *page)\n{\n\tif (test_bit(Journal, &rdev->flags))\n\t\treturn sprintf(page, \"journal\\n\");\n\telse if (rdev->raid_disk < 0)\n\t\treturn sprintf(page, \"none\\n\");\n\telse\n\t\treturn sprintf(page, \"%d\\n\", rdev->raid_disk);\n}\n\nstatic ssize_t\nslot_store(struct md_rdev *rdev, const char *buf, size_t len)\n{\n\tint slot;\n\tint err;\n\n\tif (test_bit(Journal, &rdev->flags))\n\t\treturn -EBUSY;\n\tif (strncmp(buf, \"none\", 4)==0)\n\t\tslot = -1;\n\telse {\n\t\terr = kstrtouint(buf, 10, (unsigned int *)&slot);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (slot < 0)\n\t\t\t \n\t\t\treturn -ENOSPC;\n\t}\n\tif (rdev->mddev->pers && slot == -1) {\n\t\t \n\t\tif (rdev->raid_disk == -1)\n\t\t\treturn -EEXIST;\n\t\t \n\t\tif (rdev->mddev->pers->hot_remove_disk == NULL)\n\t\t\treturn -EINVAL;\n\t\tclear_bit(Blocked, &rdev->flags);\n\t\tremove_and_add_spares(rdev->mddev, rdev);\n\t\tif (rdev->raid_disk >= 0)\n\t\t\treturn -EBUSY;\n\t\tset_bit(MD_RECOVERY_NEEDED, &rdev->mddev->recovery);\n\t\tmd_wakeup_thread(rdev->mddev->thread);\n\t} else if (rdev->mddev->pers) {\n\t\t \n\t\tint err;\n\n\t\tif (rdev->raid_disk != -1)\n\t\t\treturn -EBUSY;\n\n\t\tif (test_bit(MD_RECOVERY_RUNNING, &rdev->mddev->recovery))\n\t\t\treturn -EBUSY;\n\n\t\tif (rdev->mddev->pers->hot_add_disk == NULL)\n\t\t\treturn -EINVAL;\n\n\t\tif (slot >= rdev->mddev->raid_disks &&\n\t\t    slot >= rdev->mddev->raid_disks + rdev->mddev->delta_disks)\n\t\t\treturn -ENOSPC;\n\n\t\trdev->raid_disk = slot;\n\t\tif (test_bit(In_sync, &rdev->flags))\n\t\t\trdev->saved_raid_disk = slot;\n\t\telse\n\t\t\trdev->saved_raid_disk = -1;\n\t\tclear_bit(In_sync, &rdev->flags);\n\t\tclear_bit(Bitmap_sync, &rdev->flags);\n\t\terr = rdev->mddev->pers->hot_add_disk(rdev->mddev, rdev);\n\t\tif (err) {\n\t\t\trdev->raid_disk = -1;\n\t\t\treturn err;\n\t\t} else\n\t\t\tsysfs_notify_dirent_safe(rdev->sysfs_state);\n\t\t ;\n\t\tsysfs_link_rdev(rdev->mddev, rdev);\n\t\t \n\t} else {\n\t\tif (slot >= rdev->mddev->raid_disks &&\n\t\t    slot >= rdev->mddev->raid_disks + rdev->mddev->delta_disks)\n\t\t\treturn -ENOSPC;\n\t\trdev->raid_disk = slot;\n\t\t \n\t\tclear_bit(Faulty, &rdev->flags);\n\t\tclear_bit(WriteMostly, &rdev->flags);\n\t\tset_bit(In_sync, &rdev->flags);\n\t\tsysfs_notify_dirent_safe(rdev->sysfs_state);\n\t}\n\treturn len;\n}\n\nstatic struct rdev_sysfs_entry rdev_slot =\n__ATTR(slot, S_IRUGO|S_IWUSR, slot_show, slot_store);\n\nstatic ssize_t\noffset_show(struct md_rdev *rdev, char *page)\n{\n\treturn sprintf(page, \"%llu\\n\", (unsigned long long)rdev->data_offset);\n}\n\nstatic ssize_t\noffset_store(struct md_rdev *rdev, const char *buf, size_t len)\n{\n\tunsigned long long offset;\n\tif (kstrtoull(buf, 10, &offset) < 0)\n\t\treturn -EINVAL;\n\tif (rdev->mddev->pers && rdev->raid_disk >= 0)\n\t\treturn -EBUSY;\n\tif (rdev->sectors && rdev->mddev->external)\n\t\t \n\t\treturn -EBUSY;\n\trdev->data_offset = offset;\n\trdev->new_data_offset = offset;\n\treturn len;\n}\n\nstatic struct rdev_sysfs_entry rdev_offset =\n__ATTR(offset, S_IRUGO|S_IWUSR, offset_show, offset_store);\n\nstatic ssize_t new_offset_show(struct md_rdev *rdev, char *page)\n{\n\treturn sprintf(page, \"%llu\\n\",\n\t\t       (unsigned long long)rdev->new_data_offset);\n}\n\nstatic ssize_t new_offset_store(struct md_rdev *rdev,\n\t\t\t\tconst char *buf, size_t len)\n{\n\tunsigned long long new_offset;\n\tstruct mddev *mddev = rdev->mddev;\n\n\tif (kstrtoull(buf, 10, &new_offset) < 0)\n\t\treturn -EINVAL;\n\n\tif (mddev->sync_thread ||\n\t    test_bit(MD_RECOVERY_RUNNING,&mddev->recovery))\n\t\treturn -EBUSY;\n\tif (new_offset == rdev->data_offset)\n\t\t \n\t\t;\n\telse if (new_offset > rdev->data_offset) {\n\t\t \n\t\tif (new_offset - rdev->data_offset\n\t\t    + mddev->dev_sectors > rdev->sectors)\n\t\t\t\treturn -E2BIG;\n\t}\n\t \n\n\t \n\tif (new_offset < rdev->data_offset &&\n\t    mddev->reshape_backwards)\n\t\treturn -EINVAL;\n\t \n\tif (new_offset > rdev->data_offset &&\n\t    !mddev->reshape_backwards)\n\t\treturn -EINVAL;\n\n\tif (mddev->pers && mddev->persistent &&\n\t    !super_types[mddev->major_version]\n\t    .allow_new_offset(rdev, new_offset))\n\t\treturn -E2BIG;\n\trdev->new_data_offset = new_offset;\n\tif (new_offset > rdev->data_offset)\n\t\tmddev->reshape_backwards = 1;\n\telse if (new_offset < rdev->data_offset)\n\t\tmddev->reshape_backwards = 0;\n\n\treturn len;\n}\nstatic struct rdev_sysfs_entry rdev_new_offset =\n__ATTR(new_offset, S_IRUGO|S_IWUSR, new_offset_show, new_offset_store);\n\nstatic ssize_t\nrdev_size_show(struct md_rdev *rdev, char *page)\n{\n\treturn sprintf(page, \"%llu\\n\", (unsigned long long)rdev->sectors / 2);\n}\n\nstatic int md_rdevs_overlap(struct md_rdev *a, struct md_rdev *b)\n{\n\t \n\tif (a->data_offset + a->sectors <= b->data_offset)\n\t\treturn false;\n\tif (b->data_offset + b->sectors <= a->data_offset)\n\t\treturn false;\n\treturn true;\n}\n\nstatic bool md_rdev_overlaps(struct md_rdev *rdev)\n{\n\tstruct mddev *mddev;\n\tstruct md_rdev *rdev2;\n\n\tspin_lock(&all_mddevs_lock);\n\tlist_for_each_entry(mddev, &all_mddevs, all_mddevs) {\n\t\tif (test_bit(MD_DELETED, &mddev->flags))\n\t\t\tcontinue;\n\t\trdev_for_each(rdev2, mddev) {\n\t\t\tif (rdev != rdev2 && rdev->bdev == rdev2->bdev &&\n\t\t\t    md_rdevs_overlap(rdev, rdev2)) {\n\t\t\t\tspin_unlock(&all_mddevs_lock);\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t}\n\tspin_unlock(&all_mddevs_lock);\n\treturn false;\n}\n\nstatic int strict_blocks_to_sectors(const char *buf, sector_t *sectors)\n{\n\tunsigned long long blocks;\n\tsector_t new;\n\n\tif (kstrtoull(buf, 10, &blocks) < 0)\n\t\treturn -EINVAL;\n\n\tif (blocks & 1ULL << (8 * sizeof(blocks) - 1))\n\t\treturn -EINVAL;  \n\n\tnew = blocks * 2;\n\tif (new != blocks * 2)\n\t\treturn -EINVAL;  \n\n\t*sectors = new;\n\treturn 0;\n}\n\nstatic ssize_t\nrdev_size_store(struct md_rdev *rdev, const char *buf, size_t len)\n{\n\tstruct mddev *my_mddev = rdev->mddev;\n\tsector_t oldsectors = rdev->sectors;\n\tsector_t sectors;\n\n\tif (test_bit(Journal, &rdev->flags))\n\t\treturn -EBUSY;\n\tif (strict_blocks_to_sectors(buf, &sectors) < 0)\n\t\treturn -EINVAL;\n\tif (rdev->data_offset != rdev->new_data_offset)\n\t\treturn -EINVAL;  \n\tif (my_mddev->pers && rdev->raid_disk >= 0) {\n\t\tif (my_mddev->persistent) {\n\t\t\tsectors = super_types[my_mddev->major_version].\n\t\t\t\trdev_size_change(rdev, sectors);\n\t\t\tif (!sectors)\n\t\t\t\treturn -EBUSY;\n\t\t} else if (!sectors)\n\t\t\tsectors = bdev_nr_sectors(rdev->bdev) -\n\t\t\t\trdev->data_offset;\n\t\tif (!my_mddev->pers->resize)\n\t\t\t \n\t\t\treturn -EINVAL;\n\t}\n\tif (sectors < my_mddev->dev_sectors)\n\t\treturn -EINVAL;  \n\n\trdev->sectors = sectors;\n\n\t \n\tif (sectors > oldsectors && my_mddev->external &&\n\t    md_rdev_overlaps(rdev)) {\n\t\t \n\t\trdev->sectors = oldsectors;\n\t\treturn -EBUSY;\n\t}\n\treturn len;\n}\n\nstatic struct rdev_sysfs_entry rdev_size =\n__ATTR(size, S_IRUGO|S_IWUSR, rdev_size_show, rdev_size_store);\n\nstatic ssize_t recovery_start_show(struct md_rdev *rdev, char *page)\n{\n\tunsigned long long recovery_start = rdev->recovery_offset;\n\n\tif (test_bit(In_sync, &rdev->flags) ||\n\t    recovery_start == MaxSector)\n\t\treturn sprintf(page, \"none\\n\");\n\n\treturn sprintf(page, \"%llu\\n\", recovery_start);\n}\n\nstatic ssize_t recovery_start_store(struct md_rdev *rdev, const char *buf, size_t len)\n{\n\tunsigned long long recovery_start;\n\n\tif (cmd_match(buf, \"none\"))\n\t\trecovery_start = MaxSector;\n\telse if (kstrtoull(buf, 10, &recovery_start))\n\t\treturn -EINVAL;\n\n\tif (rdev->mddev->pers &&\n\t    rdev->raid_disk >= 0)\n\t\treturn -EBUSY;\n\n\trdev->recovery_offset = recovery_start;\n\tif (recovery_start == MaxSector)\n\t\tset_bit(In_sync, &rdev->flags);\n\telse\n\t\tclear_bit(In_sync, &rdev->flags);\n\treturn len;\n}\n\nstatic struct rdev_sysfs_entry rdev_recovery_start =\n__ATTR(recovery_start, S_IRUGO|S_IWUSR, recovery_start_show, recovery_start_store);\n\n \nstatic ssize_t bb_show(struct md_rdev *rdev, char *page)\n{\n\treturn badblocks_show(&rdev->badblocks, page, 0);\n}\nstatic ssize_t bb_store(struct md_rdev *rdev, const char *page, size_t len)\n{\n\tint rv = badblocks_store(&rdev->badblocks, page, len, 0);\n\t \n\tif (test_and_clear_bit(BlockedBadBlocks, &rdev->flags))\n\t\twake_up(&rdev->blocked_wait);\n\treturn rv;\n}\nstatic struct rdev_sysfs_entry rdev_bad_blocks =\n__ATTR(bad_blocks, S_IRUGO|S_IWUSR, bb_show, bb_store);\n\nstatic ssize_t ubb_show(struct md_rdev *rdev, char *page)\n{\n\treturn badblocks_show(&rdev->badblocks, page, 1);\n}\nstatic ssize_t ubb_store(struct md_rdev *rdev, const char *page, size_t len)\n{\n\treturn badblocks_store(&rdev->badblocks, page, len, 1);\n}\nstatic struct rdev_sysfs_entry rdev_unack_bad_blocks =\n__ATTR(unacknowledged_bad_blocks, S_IRUGO|S_IWUSR, ubb_show, ubb_store);\n\nstatic ssize_t\nppl_sector_show(struct md_rdev *rdev, char *page)\n{\n\treturn sprintf(page, \"%llu\\n\", (unsigned long long)rdev->ppl.sector);\n}\n\nstatic ssize_t\nppl_sector_store(struct md_rdev *rdev, const char *buf, size_t len)\n{\n\tunsigned long long sector;\n\n\tif (kstrtoull(buf, 10, &sector) < 0)\n\t\treturn -EINVAL;\n\tif (sector != (sector_t)sector)\n\t\treturn -EINVAL;\n\n\tif (rdev->mddev->pers && test_bit(MD_HAS_PPL, &rdev->mddev->flags) &&\n\t    rdev->raid_disk >= 0)\n\t\treturn -EBUSY;\n\n\tif (rdev->mddev->persistent) {\n\t\tif (rdev->mddev->major_version == 0)\n\t\t\treturn -EINVAL;\n\t\tif ((sector > rdev->sb_start &&\n\t\t     sector - rdev->sb_start > S16_MAX) ||\n\t\t    (sector < rdev->sb_start &&\n\t\t     rdev->sb_start - sector > -S16_MIN))\n\t\t\treturn -EINVAL;\n\t\trdev->ppl.offset = sector - rdev->sb_start;\n\t} else if (!rdev->mddev->external) {\n\t\treturn -EBUSY;\n\t}\n\trdev->ppl.sector = sector;\n\treturn len;\n}\n\nstatic struct rdev_sysfs_entry rdev_ppl_sector =\n__ATTR(ppl_sector, S_IRUGO|S_IWUSR, ppl_sector_show, ppl_sector_store);\n\nstatic ssize_t\nppl_size_show(struct md_rdev *rdev, char *page)\n{\n\treturn sprintf(page, \"%u\\n\", rdev->ppl.size);\n}\n\nstatic ssize_t\nppl_size_store(struct md_rdev *rdev, const char *buf, size_t len)\n{\n\tunsigned int size;\n\n\tif (kstrtouint(buf, 10, &size) < 0)\n\t\treturn -EINVAL;\n\n\tif (rdev->mddev->pers && test_bit(MD_HAS_PPL, &rdev->mddev->flags) &&\n\t    rdev->raid_disk >= 0)\n\t\treturn -EBUSY;\n\n\tif (rdev->mddev->persistent) {\n\t\tif (rdev->mddev->major_version == 0)\n\t\t\treturn -EINVAL;\n\t\tif (size > U16_MAX)\n\t\t\treturn -EINVAL;\n\t} else if (!rdev->mddev->external) {\n\t\treturn -EBUSY;\n\t}\n\trdev->ppl.size = size;\n\treturn len;\n}\n\nstatic struct rdev_sysfs_entry rdev_ppl_size =\n__ATTR(ppl_size, S_IRUGO|S_IWUSR, ppl_size_show, ppl_size_store);\n\nstatic struct attribute *rdev_default_attrs[] = {\n\t&rdev_state.attr,\n\t&rdev_errors.attr,\n\t&rdev_slot.attr,\n\t&rdev_offset.attr,\n\t&rdev_new_offset.attr,\n\t&rdev_size.attr,\n\t&rdev_recovery_start.attr,\n\t&rdev_bad_blocks.attr,\n\t&rdev_unack_bad_blocks.attr,\n\t&rdev_ppl_sector.attr,\n\t&rdev_ppl_size.attr,\n\tNULL,\n};\nATTRIBUTE_GROUPS(rdev_default);\nstatic ssize_t\nrdev_attr_show(struct kobject *kobj, struct attribute *attr, char *page)\n{\n\tstruct rdev_sysfs_entry *entry = container_of(attr, struct rdev_sysfs_entry, attr);\n\tstruct md_rdev *rdev = container_of(kobj, struct md_rdev, kobj);\n\n\tif (!entry->show)\n\t\treturn -EIO;\n\tif (!rdev->mddev)\n\t\treturn -ENODEV;\n\treturn entry->show(rdev, page);\n}\n\nstatic ssize_t\nrdev_attr_store(struct kobject *kobj, struct attribute *attr,\n\t      const char *page, size_t length)\n{\n\tstruct rdev_sysfs_entry *entry = container_of(attr, struct rdev_sysfs_entry, attr);\n\tstruct md_rdev *rdev = container_of(kobj, struct md_rdev, kobj);\n\tstruct kernfs_node *kn = NULL;\n\tssize_t rv;\n\tstruct mddev *mddev = rdev->mddev;\n\n\tif (!entry->store)\n\t\treturn -EIO;\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\n\tif (entry->store == state_store && cmd_match(page, \"remove\"))\n\t\tkn = sysfs_break_active_protection(kobj, attr);\n\n\trv = mddev ? mddev_lock(mddev) : -ENODEV;\n\tif (!rv) {\n\t\tif (rdev->mddev == NULL)\n\t\t\trv = -ENODEV;\n\t\telse\n\t\t\trv = entry->store(rdev, page, length);\n\t\tmddev_unlock(mddev);\n\t}\n\n\tif (kn)\n\t\tsysfs_unbreak_active_protection(kn);\n\n\treturn rv;\n}\n\nstatic void rdev_free(struct kobject *ko)\n{\n\tstruct md_rdev *rdev = container_of(ko, struct md_rdev, kobj);\n\tkfree(rdev);\n}\nstatic const struct sysfs_ops rdev_sysfs_ops = {\n\t.show\t\t= rdev_attr_show,\n\t.store\t\t= rdev_attr_store,\n};\nstatic const struct kobj_type rdev_ktype = {\n\t.release\t= rdev_free,\n\t.sysfs_ops\t= &rdev_sysfs_ops,\n\t.default_groups\t= rdev_default_groups,\n};\n\nint md_rdev_init(struct md_rdev *rdev)\n{\n\trdev->desc_nr = -1;\n\trdev->saved_raid_disk = -1;\n\trdev->raid_disk = -1;\n\trdev->flags = 0;\n\trdev->data_offset = 0;\n\trdev->new_data_offset = 0;\n\trdev->sb_events = 0;\n\trdev->last_read_error = 0;\n\trdev->sb_loaded = 0;\n\trdev->bb_page = NULL;\n\tatomic_set(&rdev->nr_pending, 0);\n\tatomic_set(&rdev->read_errors, 0);\n\tatomic_set(&rdev->corrected_errors, 0);\n\n\tINIT_LIST_HEAD(&rdev->same_set);\n\tinit_waitqueue_head(&rdev->blocked_wait);\n\n\t \n\treturn badblocks_init(&rdev->badblocks, 0);\n}\nEXPORT_SYMBOL_GPL(md_rdev_init);\n\n \nstatic struct md_rdev *md_import_device(dev_t newdev, int super_format, int super_minor)\n{\n\tstruct md_rdev *rdev;\n\tstruct md_rdev *holder;\n\tsector_t size;\n\tint err;\n\n\trdev = kzalloc(sizeof(*rdev), GFP_KERNEL);\n\tif (!rdev)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\terr = md_rdev_init(rdev);\n\tif (err)\n\t\tgoto out_free_rdev;\n\terr = alloc_disk_sb(rdev);\n\tif (err)\n\t\tgoto out_clear_rdev;\n\n\tif (super_format == -2) {\n\t\tholder = &claim_rdev;\n\t} else {\n\t\tholder = rdev;\n\t\tset_bit(Holder, &rdev->flags);\n\t}\n\n\trdev->bdev = blkdev_get_by_dev(newdev, BLK_OPEN_READ | BLK_OPEN_WRITE,\n\t\t\t\t       holder, NULL);\n\tif (IS_ERR(rdev->bdev)) {\n\t\tpr_warn(\"md: could not open device unknown-block(%u,%u).\\n\",\n\t\t\tMAJOR(newdev), MINOR(newdev));\n\t\terr = PTR_ERR(rdev->bdev);\n\t\tgoto out_clear_rdev;\n\t}\n\n\tkobject_init(&rdev->kobj, &rdev_ktype);\n\n\tsize = bdev_nr_bytes(rdev->bdev) >> BLOCK_SIZE_BITS;\n\tif (!size) {\n\t\tpr_warn(\"md: %pg has zero or unknown size, marking faulty!\\n\",\n\t\t\trdev->bdev);\n\t\terr = -EINVAL;\n\t\tgoto out_blkdev_put;\n\t}\n\n\tif (super_format >= 0) {\n\t\terr = super_types[super_format].\n\t\t\tload_super(rdev, NULL, super_minor);\n\t\tif (err == -EINVAL) {\n\t\t\tpr_warn(\"md: %pg does not have a valid v%d.%d superblock, not importing!\\n\",\n\t\t\t\trdev->bdev,\n\t\t\t\tsuper_format, super_minor);\n\t\t\tgoto out_blkdev_put;\n\t\t}\n\t\tif (err < 0) {\n\t\t\tpr_warn(\"md: could not read %pg's sb, not importing!\\n\",\n\t\t\t\trdev->bdev);\n\t\t\tgoto out_blkdev_put;\n\t\t}\n\t}\n\n\treturn rdev;\n\nout_blkdev_put:\n\tblkdev_put(rdev->bdev, holder);\nout_clear_rdev:\n\tmd_rdev_clear(rdev);\nout_free_rdev:\n\tkfree(rdev);\n\treturn ERR_PTR(err);\n}\n\n \n\nstatic int analyze_sbs(struct mddev *mddev)\n{\n\tint i;\n\tstruct md_rdev *rdev, *freshest, *tmp;\n\n\tfreshest = NULL;\n\trdev_for_each_safe(rdev, tmp, mddev)\n\t\tswitch (super_types[mddev->major_version].\n\t\t\tload_super(rdev, freshest, mddev->minor_version)) {\n\t\tcase 1:\n\t\t\tfreshest = rdev;\n\t\t\tbreak;\n\t\tcase 0:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpr_warn(\"md: fatal superblock inconsistency in %pg -- removing from array\\n\",\n\t\t\t\trdev->bdev);\n\t\t\tmd_kick_rdev_from_array(rdev);\n\t\t}\n\n\t \n\tif (!freshest) {\n\t\tpr_warn(\"md: cannot find a valid disk\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tsuper_types[mddev->major_version].\n\t\tvalidate_super(mddev, freshest);\n\n\ti = 0;\n\trdev_for_each_safe(rdev, tmp, mddev) {\n\t\tif (mddev->max_disks &&\n\t\t    (rdev->desc_nr >= mddev->max_disks ||\n\t\t     i > mddev->max_disks)) {\n\t\t\tpr_warn(\"md: %s: %pg: only %d devices permitted\\n\",\n\t\t\t\tmdname(mddev), rdev->bdev,\n\t\t\t\tmddev->max_disks);\n\t\t\tmd_kick_rdev_from_array(rdev);\n\t\t\tcontinue;\n\t\t}\n\t\tif (rdev != freshest) {\n\t\t\tif (super_types[mddev->major_version].\n\t\t\t    validate_super(mddev, rdev)) {\n\t\t\t\tpr_warn(\"md: kicking non-fresh %pg from array!\\n\",\n\t\t\t\t\trdev->bdev);\n\t\t\t\tmd_kick_rdev_from_array(rdev);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tif (mddev->level == LEVEL_MULTIPATH) {\n\t\t\trdev->desc_nr = i++;\n\t\t\trdev->raid_disk = rdev->desc_nr;\n\t\t\tset_bit(In_sync, &rdev->flags);\n\t\t} else if (rdev->raid_disk >=\n\t\t\t    (mddev->raid_disks - min(0, mddev->delta_disks)) &&\n\t\t\t   !test_bit(Journal, &rdev->flags)) {\n\t\t\trdev->raid_disk = -1;\n\t\t\tclear_bit(In_sync, &rdev->flags);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nint strict_strtoul_scaled(const char *cp, unsigned long *res, int scale)\n{\n\tunsigned long result = 0;\n\tlong decimals = -1;\n\twhile (isdigit(*cp) || (*cp == '.' && decimals < 0)) {\n\t\tif (*cp == '.')\n\t\t\tdecimals = 0;\n\t\telse if (decimals < scale) {\n\t\t\tunsigned int value;\n\t\t\tvalue = *cp - '0';\n\t\t\tresult = result * 10 + value;\n\t\t\tif (decimals >= 0)\n\t\t\t\tdecimals++;\n\t\t}\n\t\tcp++;\n\t}\n\tif (*cp == '\\n')\n\t\tcp++;\n\tif (*cp)\n\t\treturn -EINVAL;\n\tif (decimals < 0)\n\t\tdecimals = 0;\n\t*res = result * int_pow(10, scale - decimals);\n\treturn 0;\n}\n\nstatic ssize_t\nsafe_delay_show(struct mddev *mddev, char *page)\n{\n\tunsigned int msec = ((unsigned long)mddev->safemode_delay*1000)/HZ;\n\n\treturn sprintf(page, \"%u.%03u\\n\", msec/1000, msec%1000);\n}\nstatic ssize_t\nsafe_delay_store(struct mddev *mddev, const char *cbuf, size_t len)\n{\n\tunsigned long msec;\n\n\tif (mddev_is_clustered(mddev)) {\n\t\tpr_warn(\"md: Safemode is disabled for clustered mode\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (strict_strtoul_scaled(cbuf, &msec, 3) < 0 || msec > UINT_MAX / HZ)\n\t\treturn -EINVAL;\n\tif (msec == 0)\n\t\tmddev->safemode_delay = 0;\n\telse {\n\t\tunsigned long old_delay = mddev->safemode_delay;\n\t\tunsigned long new_delay = (msec*HZ)/1000;\n\n\t\tif (new_delay == 0)\n\t\t\tnew_delay = 1;\n\t\tmddev->safemode_delay = new_delay;\n\t\tif (new_delay < old_delay || old_delay == 0)\n\t\t\tmod_timer(&mddev->safemode_timer, jiffies+1);\n\t}\n\treturn len;\n}\nstatic struct md_sysfs_entry md_safe_delay =\n__ATTR(safe_mode_delay, S_IRUGO|S_IWUSR,safe_delay_show, safe_delay_store);\n\nstatic ssize_t\nlevel_show(struct mddev *mddev, char *page)\n{\n\tstruct md_personality *p;\n\tint ret;\n\tspin_lock(&mddev->lock);\n\tp = mddev->pers;\n\tif (p)\n\t\tret = sprintf(page, \"%s\\n\", p->name);\n\telse if (mddev->clevel[0])\n\t\tret = sprintf(page, \"%s\\n\", mddev->clevel);\n\telse if (mddev->level != LEVEL_NONE)\n\t\tret = sprintf(page, \"%d\\n\", mddev->level);\n\telse\n\t\tret = 0;\n\tspin_unlock(&mddev->lock);\n\treturn ret;\n}\n\nstatic ssize_t\nlevel_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tchar clevel[16];\n\tssize_t rv;\n\tsize_t slen = len;\n\tstruct md_personality *pers, *oldpers;\n\tlong level;\n\tvoid *priv, *oldpriv;\n\tstruct md_rdev *rdev;\n\n\tif (slen == 0 || slen >= sizeof(clevel))\n\t\treturn -EINVAL;\n\n\trv = mddev_lock(mddev);\n\tif (rv)\n\t\treturn rv;\n\n\tif (mddev->pers == NULL) {\n\t\tstrncpy(mddev->clevel, buf, slen);\n\t\tif (mddev->clevel[slen-1] == '\\n')\n\t\t\tslen--;\n\t\tmddev->clevel[slen] = 0;\n\t\tmddev->level = LEVEL_NONE;\n\t\trv = len;\n\t\tgoto out_unlock;\n\t}\n\trv = -EROFS;\n\tif (!md_is_rdwr(mddev))\n\t\tgoto out_unlock;\n\n\t \n\n\trv = -EBUSY;\n\tif (mddev->sync_thread ||\n\t    test_bit(MD_RECOVERY_RUNNING, &mddev->recovery) ||\n\t    mddev->reshape_position != MaxSector ||\n\t    mddev->sysfs_active)\n\t\tgoto out_unlock;\n\n\trv = -EINVAL;\n\tif (!mddev->pers->quiesce) {\n\t\tpr_warn(\"md: %s: %s does not support online personality change\\n\",\n\t\t\tmdname(mddev), mddev->pers->name);\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tstrncpy(clevel, buf, slen);\n\tif (clevel[slen-1] == '\\n')\n\t\tslen--;\n\tclevel[slen] = 0;\n\tif (kstrtol(clevel, 10, &level))\n\t\tlevel = LEVEL_NONE;\n\n\tif (request_module(\"md-%s\", clevel) != 0)\n\t\trequest_module(\"md-level-%s\", clevel);\n\tspin_lock(&pers_lock);\n\tpers = find_pers(level, clevel);\n\tif (!pers || !try_module_get(pers->owner)) {\n\t\tspin_unlock(&pers_lock);\n\t\tpr_warn(\"md: personality %s not loaded\\n\", clevel);\n\t\trv = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\tspin_unlock(&pers_lock);\n\n\tif (pers == mddev->pers) {\n\t\t \n\t\tmodule_put(pers->owner);\n\t\trv = len;\n\t\tgoto out_unlock;\n\t}\n\tif (!pers->takeover) {\n\t\tmodule_put(pers->owner);\n\t\tpr_warn(\"md: %s: %s does not support personality takeover\\n\",\n\t\t\tmdname(mddev), clevel);\n\t\trv = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\trdev_for_each(rdev, mddev)\n\t\trdev->new_raid_disk = rdev->raid_disk;\n\n\t \n\tpriv = pers->takeover(mddev);\n\tif (IS_ERR(priv)) {\n\t\tmddev->new_level = mddev->level;\n\t\tmddev->new_layout = mddev->layout;\n\t\tmddev->new_chunk_sectors = mddev->chunk_sectors;\n\t\tmddev->raid_disks -= mddev->delta_disks;\n\t\tmddev->delta_disks = 0;\n\t\tmddev->reshape_backwards = 0;\n\t\tmodule_put(pers->owner);\n\t\tpr_warn(\"md: %s: %s would not accept array\\n\",\n\t\t\tmdname(mddev), clevel);\n\t\trv = PTR_ERR(priv);\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tmddev_suspend(mddev);\n\tmddev_detach(mddev);\n\n\tspin_lock(&mddev->lock);\n\toldpers = mddev->pers;\n\toldpriv = mddev->private;\n\tmddev->pers = pers;\n\tmddev->private = priv;\n\tstrscpy(mddev->clevel, pers->name, sizeof(mddev->clevel));\n\tmddev->level = mddev->new_level;\n\tmddev->layout = mddev->new_layout;\n\tmddev->chunk_sectors = mddev->new_chunk_sectors;\n\tmddev->delta_disks = 0;\n\tmddev->reshape_backwards = 0;\n\tmddev->degraded = 0;\n\tspin_unlock(&mddev->lock);\n\n\tif (oldpers->sync_request == NULL &&\n\t    mddev->external) {\n\t\t \n\t\tmddev->in_sync = 0;\n\t\tmddev->safemode_delay = 0;\n\t\tmddev->safemode = 0;\n\t}\n\n\toldpers->free(mddev, oldpriv);\n\n\tif (oldpers->sync_request == NULL &&\n\t    pers->sync_request != NULL) {\n\t\t \n\t\tif (sysfs_create_group(&mddev->kobj, &md_redundancy_group))\n\t\t\tpr_warn(\"md: cannot register extra attributes for %s\\n\",\n\t\t\t\tmdname(mddev));\n\t\tmddev->sysfs_action = sysfs_get_dirent(mddev->kobj.sd, \"sync_action\");\n\t\tmddev->sysfs_completed = sysfs_get_dirent_safe(mddev->kobj.sd, \"sync_completed\");\n\t\tmddev->sysfs_degraded = sysfs_get_dirent_safe(mddev->kobj.sd, \"degraded\");\n\t}\n\tif (oldpers->sync_request != NULL &&\n\t    pers->sync_request == NULL) {\n\t\t \n\t\tif (mddev->to_remove == NULL)\n\t\t\tmddev->to_remove = &md_redundancy_group;\n\t}\n\n\tmodule_put(oldpers->owner);\n\n\trdev_for_each(rdev, mddev) {\n\t\tif (rdev->raid_disk < 0)\n\t\t\tcontinue;\n\t\tif (rdev->new_raid_disk >= mddev->raid_disks)\n\t\t\trdev->new_raid_disk = -1;\n\t\tif (rdev->new_raid_disk == rdev->raid_disk)\n\t\t\tcontinue;\n\t\tsysfs_unlink_rdev(mddev, rdev);\n\t}\n\trdev_for_each(rdev, mddev) {\n\t\tif (rdev->raid_disk < 0)\n\t\t\tcontinue;\n\t\tif (rdev->new_raid_disk == rdev->raid_disk)\n\t\t\tcontinue;\n\t\trdev->raid_disk = rdev->new_raid_disk;\n\t\tif (rdev->raid_disk < 0)\n\t\t\tclear_bit(In_sync, &rdev->flags);\n\t\telse {\n\t\t\tif (sysfs_link_rdev(mddev, rdev))\n\t\t\t\tpr_warn(\"md: cannot register rd%d for %s after level change\\n\",\n\t\t\t\t\trdev->raid_disk, mdname(mddev));\n\t\t}\n\t}\n\n\tif (pers->sync_request == NULL) {\n\t\t \n\t\tmddev->in_sync = 1;\n\t\tdel_timer_sync(&mddev->safemode_timer);\n\t}\n\tblk_set_stacking_limits(&mddev->queue->limits);\n\tpers->run(mddev);\n\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\tmddev_resume(mddev);\n\tif (!mddev->thread)\n\t\tmd_update_sb(mddev, 1);\n\tsysfs_notify_dirent_safe(mddev->sysfs_level);\n\tmd_new_event();\n\trv = len;\nout_unlock:\n\tmddev_unlock(mddev);\n\treturn rv;\n}\n\nstatic struct md_sysfs_entry md_level =\n__ATTR(level, S_IRUGO|S_IWUSR, level_show, level_store);\n\nstatic ssize_t\nlayout_show(struct mddev *mddev, char *page)\n{\n\t \n\tif (mddev->reshape_position != MaxSector &&\n\t    mddev->layout != mddev->new_layout)\n\t\treturn sprintf(page, \"%d (%d)\\n\",\n\t\t\t       mddev->new_layout, mddev->layout);\n\treturn sprintf(page, \"%d\\n\", mddev->layout);\n}\n\nstatic ssize_t\nlayout_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tunsigned int n;\n\tint err;\n\n\terr = kstrtouint(buf, 10, &n);\n\tif (err < 0)\n\t\treturn err;\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\n\tif (mddev->pers) {\n\t\tif (mddev->pers->check_reshape == NULL)\n\t\t\terr = -EBUSY;\n\t\telse if (!md_is_rdwr(mddev))\n\t\t\terr = -EROFS;\n\t\telse {\n\t\t\tmddev->new_layout = n;\n\t\t\terr = mddev->pers->check_reshape(mddev);\n\t\t\tif (err)\n\t\t\t\tmddev->new_layout = mddev->layout;\n\t\t}\n\t} else {\n\t\tmddev->new_layout = n;\n\t\tif (mddev->reshape_position == MaxSector)\n\t\t\tmddev->layout = n;\n\t}\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\nstatic struct md_sysfs_entry md_layout =\n__ATTR(layout, S_IRUGO|S_IWUSR, layout_show, layout_store);\n\nstatic ssize_t\nraid_disks_show(struct mddev *mddev, char *page)\n{\n\tif (mddev->raid_disks == 0)\n\t\treturn 0;\n\tif (mddev->reshape_position != MaxSector &&\n\t    mddev->delta_disks != 0)\n\t\treturn sprintf(page, \"%d (%d)\\n\", mddev->raid_disks,\n\t\t\t       mddev->raid_disks - mddev->delta_disks);\n\treturn sprintf(page, \"%d\\n\", mddev->raid_disks);\n}\n\nstatic int update_raid_disks(struct mddev *mddev, int raid_disks);\n\nstatic ssize_t\nraid_disks_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tunsigned int n;\n\tint err;\n\n\terr = kstrtouint(buf, 10, &n);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tif (mddev->pers)\n\t\terr = update_raid_disks(mddev, n);\n\telse if (mddev->reshape_position != MaxSector) {\n\t\tstruct md_rdev *rdev;\n\t\tint olddisks = mddev->raid_disks - mddev->delta_disks;\n\n\t\terr = -EINVAL;\n\t\trdev_for_each(rdev, mddev) {\n\t\t\tif (olddisks < n &&\n\t\t\t    rdev->data_offset < rdev->new_data_offset)\n\t\t\t\tgoto out_unlock;\n\t\t\tif (olddisks > n &&\n\t\t\t    rdev->data_offset > rdev->new_data_offset)\n\t\t\t\tgoto out_unlock;\n\t\t}\n\t\terr = 0;\n\t\tmddev->delta_disks = n - olddisks;\n\t\tmddev->raid_disks = n;\n\t\tmddev->reshape_backwards = (mddev->delta_disks < 0);\n\t} else\n\t\tmddev->raid_disks = n;\nout_unlock:\n\tmddev_unlock(mddev);\n\treturn err ? err : len;\n}\nstatic struct md_sysfs_entry md_raid_disks =\n__ATTR(raid_disks, S_IRUGO|S_IWUSR, raid_disks_show, raid_disks_store);\n\nstatic ssize_t\nuuid_show(struct mddev *mddev, char *page)\n{\n\treturn sprintf(page, \"%pU\\n\", mddev->uuid);\n}\nstatic struct md_sysfs_entry md_uuid =\n__ATTR(uuid, S_IRUGO, uuid_show, NULL);\n\nstatic ssize_t\nchunk_size_show(struct mddev *mddev, char *page)\n{\n\tif (mddev->reshape_position != MaxSector &&\n\t    mddev->chunk_sectors != mddev->new_chunk_sectors)\n\t\treturn sprintf(page, \"%d (%d)\\n\",\n\t\t\t       mddev->new_chunk_sectors << 9,\n\t\t\t       mddev->chunk_sectors << 9);\n\treturn sprintf(page, \"%d\\n\", mddev->chunk_sectors << 9);\n}\n\nstatic ssize_t\nchunk_size_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tunsigned long n;\n\tint err;\n\n\terr = kstrtoul(buf, 10, &n);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tif (mddev->pers) {\n\t\tif (mddev->pers->check_reshape == NULL)\n\t\t\terr = -EBUSY;\n\t\telse if (!md_is_rdwr(mddev))\n\t\t\terr = -EROFS;\n\t\telse {\n\t\t\tmddev->new_chunk_sectors = n >> 9;\n\t\t\terr = mddev->pers->check_reshape(mddev);\n\t\t\tif (err)\n\t\t\t\tmddev->new_chunk_sectors = mddev->chunk_sectors;\n\t\t}\n\t} else {\n\t\tmddev->new_chunk_sectors = n >> 9;\n\t\tif (mddev->reshape_position == MaxSector)\n\t\t\tmddev->chunk_sectors = n >> 9;\n\t}\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\nstatic struct md_sysfs_entry md_chunk_size =\n__ATTR(chunk_size, S_IRUGO|S_IWUSR, chunk_size_show, chunk_size_store);\n\nstatic ssize_t\nresync_start_show(struct mddev *mddev, char *page)\n{\n\tif (mddev->recovery_cp == MaxSector)\n\t\treturn sprintf(page, \"none\\n\");\n\treturn sprintf(page, \"%llu\\n\", (unsigned long long)mddev->recovery_cp);\n}\n\nstatic ssize_t\nresync_start_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tunsigned long long n;\n\tint err;\n\n\tif (cmd_match(buf, \"none\"))\n\t\tn = MaxSector;\n\telse {\n\t\terr = kstrtoull(buf, 10, &n);\n\t\tif (err < 0)\n\t\t\treturn err;\n\t\tif (n != (sector_t)n)\n\t\t\treturn -EINVAL;\n\t}\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tif (mddev->pers && !test_bit(MD_RECOVERY_FROZEN, &mddev->recovery))\n\t\terr = -EBUSY;\n\n\tif (!err) {\n\t\tmddev->recovery_cp = n;\n\t\tif (mddev->pers)\n\t\t\tset_bit(MD_SB_CHANGE_CLEAN, &mddev->sb_flags);\n\t}\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\nstatic struct md_sysfs_entry md_resync_start =\n__ATTR_PREALLOC(resync_start, S_IRUGO|S_IWUSR,\n\t\tresync_start_show, resync_start_store);\n\n \nenum array_state { clear, inactive, suspended, readonly, read_auto, clean, active,\n\t\t   write_pending, active_idle, broken, bad_word};\nstatic char *array_states[] = {\n\t\"clear\", \"inactive\", \"suspended\", \"readonly\", \"read-auto\", \"clean\", \"active\",\n\t\"write-pending\", \"active-idle\", \"broken\", NULL };\n\nstatic int match_word(const char *word, char **list)\n{\n\tint n;\n\tfor (n=0; list[n]; n++)\n\t\tif (cmd_match(word, list[n]))\n\t\t\tbreak;\n\treturn n;\n}\n\nstatic ssize_t\narray_state_show(struct mddev *mddev, char *page)\n{\n\tenum array_state st = inactive;\n\n\tif (mddev->pers && !test_bit(MD_NOT_READY, &mddev->flags)) {\n\t\tswitch(mddev->ro) {\n\t\tcase MD_RDONLY:\n\t\t\tst = readonly;\n\t\t\tbreak;\n\t\tcase MD_AUTO_READ:\n\t\t\tst = read_auto;\n\t\t\tbreak;\n\t\tcase MD_RDWR:\n\t\t\tspin_lock(&mddev->lock);\n\t\t\tif (test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags))\n\t\t\t\tst = write_pending;\n\t\t\telse if (mddev->in_sync)\n\t\t\t\tst = clean;\n\t\t\telse if (mddev->safemode)\n\t\t\t\tst = active_idle;\n\t\t\telse\n\t\t\t\tst = active;\n\t\t\tspin_unlock(&mddev->lock);\n\t\t}\n\n\t\tif (test_bit(MD_BROKEN, &mddev->flags) && st == clean)\n\t\t\tst = broken;\n\t} else {\n\t\tif (list_empty(&mddev->disks) &&\n\t\t    mddev->raid_disks == 0 &&\n\t\t    mddev->dev_sectors == 0)\n\t\t\tst = clear;\n\t\telse\n\t\t\tst = inactive;\n\t}\n\treturn sprintf(page, \"%s\\n\", array_states[st]);\n}\n\nstatic int do_md_stop(struct mddev *mddev, int ro, struct block_device *bdev);\nstatic int md_set_readonly(struct mddev *mddev, struct block_device *bdev);\nstatic int restart_array(struct mddev *mddev);\n\nstatic ssize_t\narray_state_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tint err = 0;\n\tenum array_state st = match_word(buf, array_states);\n\n\tif (mddev->pers && (st == active || st == clean) &&\n\t    mddev->ro != MD_RDONLY) {\n\t\t \n\t\tspin_lock(&mddev->lock);\n\t\tif (st == active) {\n\t\t\trestart_array(mddev);\n\t\t\tclear_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags);\n\t\t\tmd_wakeup_thread(mddev->thread);\n\t\t\twake_up(&mddev->sb_wait);\n\t\t} else   {\n\t\t\trestart_array(mddev);\n\t\t\tif (!set_in_sync(mddev))\n\t\t\t\terr = -EBUSY;\n\t\t}\n\t\tif (!err)\n\t\t\tsysfs_notify_dirent_safe(mddev->sysfs_state);\n\t\tspin_unlock(&mddev->lock);\n\t\treturn err ?: len;\n\t}\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\terr = -EINVAL;\n\tswitch(st) {\n\tcase bad_word:\n\t\tbreak;\n\tcase clear:\n\t\t \n\t\terr = do_md_stop(mddev, 0, NULL);\n\t\tbreak;\n\tcase inactive:\n\t\t \n\t\tif (mddev->pers)\n\t\t\terr = do_md_stop(mddev, 2, NULL);\n\t\telse\n\t\t\terr = 0;  \n\t\tbreak;\n\tcase suspended:\n\t\tbreak;  \n\tcase readonly:\n\t\tif (mddev->pers)\n\t\t\terr = md_set_readonly(mddev, NULL);\n\t\telse {\n\t\t\tmddev->ro = MD_RDONLY;\n\t\t\tset_disk_ro(mddev->gendisk, 1);\n\t\t\terr = do_md_run(mddev);\n\t\t}\n\t\tbreak;\n\tcase read_auto:\n\t\tif (mddev->pers) {\n\t\t\tif (md_is_rdwr(mddev))\n\t\t\t\terr = md_set_readonly(mddev, NULL);\n\t\t\telse if (mddev->ro == MD_RDONLY)\n\t\t\t\terr = restart_array(mddev);\n\t\t\tif (err == 0) {\n\t\t\t\tmddev->ro = MD_AUTO_READ;\n\t\t\t\tset_disk_ro(mddev->gendisk, 0);\n\t\t\t}\n\t\t} else {\n\t\t\tmddev->ro = MD_AUTO_READ;\n\t\t\terr = do_md_run(mddev);\n\t\t}\n\t\tbreak;\n\tcase clean:\n\t\tif (mddev->pers) {\n\t\t\terr = restart_array(mddev);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tspin_lock(&mddev->lock);\n\t\t\tif (!set_in_sync(mddev))\n\t\t\t\terr = -EBUSY;\n\t\t\tspin_unlock(&mddev->lock);\n\t\t} else\n\t\t\terr = -EINVAL;\n\t\tbreak;\n\tcase active:\n\t\tif (mddev->pers) {\n\t\t\terr = restart_array(mddev);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tclear_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags);\n\t\t\twake_up(&mddev->sb_wait);\n\t\t\terr = 0;\n\t\t} else {\n\t\t\tmddev->ro = MD_RDWR;\n\t\t\tset_disk_ro(mddev->gendisk, 0);\n\t\t\terr = do_md_run(mddev);\n\t\t}\n\t\tbreak;\n\tcase write_pending:\n\tcase active_idle:\n\tcase broken:\n\t\t \n\t\tbreak;\n\t}\n\n\tif (!err) {\n\t\tif (mddev->hold_active == UNTIL_IOCTL)\n\t\t\tmddev->hold_active = 0;\n\t\tsysfs_notify_dirent_safe(mddev->sysfs_state);\n\t}\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\nstatic struct md_sysfs_entry md_array_state =\n__ATTR_PREALLOC(array_state, S_IRUGO|S_IWUSR, array_state_show, array_state_store);\n\nstatic ssize_t\nmax_corrected_read_errors_show(struct mddev *mddev, char *page) {\n\treturn sprintf(page, \"%d\\n\",\n\t\t       atomic_read(&mddev->max_corr_read_errors));\n}\n\nstatic ssize_t\nmax_corrected_read_errors_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tunsigned int n;\n\tint rv;\n\n\trv = kstrtouint(buf, 10, &n);\n\tif (rv < 0)\n\t\treturn rv;\n\tif (n > INT_MAX)\n\t\treturn -EINVAL;\n\tatomic_set(&mddev->max_corr_read_errors, n);\n\treturn len;\n}\n\nstatic struct md_sysfs_entry max_corr_read_errors =\n__ATTR(max_read_errors, S_IRUGO|S_IWUSR, max_corrected_read_errors_show,\n\tmax_corrected_read_errors_store);\n\nstatic ssize_t\nnull_show(struct mddev *mddev, char *page)\n{\n\treturn -EINVAL;\n}\n\nstatic ssize_t\nnew_dev_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\t \n\t \n\tchar *e;\n\tint major = simple_strtoul(buf, &e, 10);\n\tint minor;\n\tdev_t dev;\n\tstruct md_rdev *rdev;\n\tint err;\n\n\tif (!*buf || *e != ':' || !e[1] || e[1] == '\\n')\n\t\treturn -EINVAL;\n\tminor = simple_strtoul(e+1, &e, 10);\n\tif (*e && *e != '\\n')\n\t\treturn -EINVAL;\n\tdev = MKDEV(major, minor);\n\tif (major != MAJOR(dev) ||\n\t    minor != MINOR(dev))\n\t\treturn -EOVERFLOW;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tif (mddev->persistent) {\n\t\trdev = md_import_device(dev, mddev->major_version,\n\t\t\t\t\tmddev->minor_version);\n\t\tif (!IS_ERR(rdev) && !list_empty(&mddev->disks)) {\n\t\t\tstruct md_rdev *rdev0\n\t\t\t\t= list_entry(mddev->disks.next,\n\t\t\t\t\t     struct md_rdev, same_set);\n\t\t\terr = super_types[mddev->major_version]\n\t\t\t\t.load_super(rdev, rdev0, mddev->minor_version);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out;\n\t\t}\n\t} else if (mddev->external)\n\t\trdev = md_import_device(dev, -2, -1);\n\telse\n\t\trdev = md_import_device(dev, -1, -1);\n\n\tif (IS_ERR(rdev)) {\n\t\tmddev_unlock(mddev);\n\t\treturn PTR_ERR(rdev);\n\t}\n\terr = bind_rdev_to_array(rdev, mddev);\n out:\n\tif (err)\n\t\texport_rdev(rdev, mddev);\n\tmddev_unlock(mddev);\n\tif (!err)\n\t\tmd_new_event();\n\treturn err ? err : len;\n}\n\nstatic struct md_sysfs_entry md_new_device =\n__ATTR(new_dev, S_IWUSR, null_show, new_dev_store);\n\nstatic ssize_t\nbitmap_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tchar *end;\n\tunsigned long chunk, end_chunk;\n\tint err;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tif (!mddev->bitmap)\n\t\tgoto out;\n\t \n\twhile (*buf) {\n\t\tchunk = end_chunk = simple_strtoul(buf, &end, 0);\n\t\tif (buf == end) break;\n\t\tif (*end == '-') {  \n\t\t\tbuf = end + 1;\n\t\t\tend_chunk = simple_strtoul(buf, &end, 0);\n\t\t\tif (buf == end) break;\n\t\t}\n\t\tif (*end && !isspace(*end)) break;\n\t\tmd_bitmap_dirty_bits(mddev->bitmap, chunk, end_chunk);\n\t\tbuf = skip_spaces(end);\n\t}\n\tmd_bitmap_unplug(mddev->bitmap);  \nout:\n\tmddev_unlock(mddev);\n\treturn len;\n}\n\nstatic struct md_sysfs_entry md_bitmap =\n__ATTR(bitmap_set_bits, S_IWUSR, null_show, bitmap_store);\n\nstatic ssize_t\nsize_show(struct mddev *mddev, char *page)\n{\n\treturn sprintf(page, \"%llu\\n\",\n\t\t(unsigned long long)mddev->dev_sectors / 2);\n}\n\nstatic int update_size(struct mddev *mddev, sector_t num_sectors);\n\nstatic ssize_t\nsize_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\t \n\tsector_t sectors;\n\tint err = strict_blocks_to_sectors(buf, &sectors);\n\n\tif (err < 0)\n\t\treturn err;\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tif (mddev->pers) {\n\t\terr = update_size(mddev, sectors);\n\t\tif (err == 0)\n\t\t\tmd_update_sb(mddev, 1);\n\t} else {\n\t\tif (mddev->dev_sectors == 0 ||\n\t\t    mddev->dev_sectors > sectors)\n\t\t\tmddev->dev_sectors = sectors;\n\t\telse\n\t\t\terr = -ENOSPC;\n\t}\n\tmddev_unlock(mddev);\n\treturn err ? err : len;\n}\n\nstatic struct md_sysfs_entry md_size =\n__ATTR(component_size, S_IRUGO|S_IWUSR, size_show, size_store);\n\n \nstatic ssize_t\nmetadata_show(struct mddev *mddev, char *page)\n{\n\tif (mddev->persistent)\n\t\treturn sprintf(page, \"%d.%d\\n\",\n\t\t\t       mddev->major_version, mddev->minor_version);\n\telse if (mddev->external)\n\t\treturn sprintf(page, \"external:%s\\n\", mddev->metadata_type);\n\telse\n\t\treturn sprintf(page, \"none\\n\");\n}\n\nstatic ssize_t\nmetadata_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tint major, minor;\n\tchar *e;\n\tint err;\n\t \n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\terr = -EBUSY;\n\tif (mddev->external && strncmp(buf, \"external:\", 9) == 0)\n\t\t;\n\telse if (!list_empty(&mddev->disks))\n\t\tgoto out_unlock;\n\n\terr = 0;\n\tif (cmd_match(buf, \"none\")) {\n\t\tmddev->persistent = 0;\n\t\tmddev->external = 0;\n\t\tmddev->major_version = 0;\n\t\tmddev->minor_version = 90;\n\t\tgoto out_unlock;\n\t}\n\tif (strncmp(buf, \"external:\", 9) == 0) {\n\t\tsize_t namelen = len-9;\n\t\tif (namelen >= sizeof(mddev->metadata_type))\n\t\t\tnamelen = sizeof(mddev->metadata_type)-1;\n\t\tstrncpy(mddev->metadata_type, buf+9, namelen);\n\t\tmddev->metadata_type[namelen] = 0;\n\t\tif (namelen && mddev->metadata_type[namelen-1] == '\\n')\n\t\t\tmddev->metadata_type[--namelen] = 0;\n\t\tmddev->persistent = 0;\n\t\tmddev->external = 1;\n\t\tmddev->major_version = 0;\n\t\tmddev->minor_version = 90;\n\t\tgoto out_unlock;\n\t}\n\tmajor = simple_strtoul(buf, &e, 10);\n\terr = -EINVAL;\n\tif (e==buf || *e != '.')\n\t\tgoto out_unlock;\n\tbuf = e+1;\n\tminor = simple_strtoul(buf, &e, 10);\n\tif (e==buf || (*e && *e != '\\n') )\n\t\tgoto out_unlock;\n\terr = -ENOENT;\n\tif (major >= ARRAY_SIZE(super_types) || super_types[major].name == NULL)\n\t\tgoto out_unlock;\n\tmddev->major_version = major;\n\tmddev->minor_version = minor;\n\tmddev->persistent = 1;\n\tmddev->external = 0;\n\terr = 0;\nout_unlock:\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry md_metadata =\n__ATTR_PREALLOC(metadata_version, S_IRUGO|S_IWUSR, metadata_show, metadata_store);\n\nstatic ssize_t\naction_show(struct mddev *mddev, char *page)\n{\n\tchar *type = \"idle\";\n\tunsigned long recovery = mddev->recovery;\n\tif (test_bit(MD_RECOVERY_FROZEN, &recovery))\n\t\ttype = \"frozen\";\n\telse if (test_bit(MD_RECOVERY_RUNNING, &recovery) ||\n\t    (md_is_rdwr(mddev) && test_bit(MD_RECOVERY_NEEDED, &recovery))) {\n\t\tif (test_bit(MD_RECOVERY_RESHAPE, &recovery))\n\t\t\ttype = \"reshape\";\n\t\telse if (test_bit(MD_RECOVERY_SYNC, &recovery)) {\n\t\t\tif (!test_bit(MD_RECOVERY_REQUESTED, &recovery))\n\t\t\t\ttype = \"resync\";\n\t\t\telse if (test_bit(MD_RECOVERY_CHECK, &recovery))\n\t\t\t\ttype = \"check\";\n\t\t\telse\n\t\t\t\ttype = \"repair\";\n\t\t} else if (test_bit(MD_RECOVERY_RECOVER, &recovery))\n\t\t\ttype = \"recover\";\n\t\telse if (mddev->reshape_position != MaxSector)\n\t\t\ttype = \"reshape\";\n\t}\n\treturn sprintf(page, \"%s\\n\", type);\n}\n\nstatic void stop_sync_thread(struct mddev *mddev)\n{\n\tif (!test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))\n\t\treturn;\n\n\tif (mddev_lock(mddev))\n\t\treturn;\n\n\t \n\tif (!test_bit(MD_RECOVERY_RUNNING, &mddev->recovery)) {\n\t\tmddev_unlock(mddev);\n\t\treturn;\n\t}\n\n\tif (work_pending(&mddev->del_work))\n\t\tflush_workqueue(md_misc_wq);\n\n\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\t \n\tmd_wakeup_thread_directly(mddev->sync_thread);\n\n\tmddev_unlock(mddev);\n}\n\nstatic void idle_sync_thread(struct mddev *mddev)\n{\n\tint sync_seq = atomic_read(&mddev->sync_seq);\n\n\tmutex_lock(&mddev->sync_mutex);\n\tclear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\tstop_sync_thread(mddev);\n\n\twait_event(resync_wait, sync_seq != atomic_read(&mddev->sync_seq) ||\n\t\t\t!test_bit(MD_RECOVERY_RUNNING, &mddev->recovery));\n\n\tmutex_unlock(&mddev->sync_mutex);\n}\n\nstatic void frozen_sync_thread(struct mddev *mddev)\n{\n\tmutex_lock(&mddev->sync_mutex);\n\tset_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\tstop_sync_thread(mddev);\n\n\twait_event(resync_wait, mddev->sync_thread == NULL &&\n\t\t\t!test_bit(MD_RECOVERY_RUNNING, &mddev->recovery));\n\n\tmutex_unlock(&mddev->sync_mutex);\n}\n\nstatic ssize_t\naction_store(struct mddev *mddev, const char *page, size_t len)\n{\n\tif (!mddev->pers || !mddev->pers->sync_request)\n\t\treturn -EINVAL;\n\n\n\tif (cmd_match(page, \"idle\"))\n\t\tidle_sync_thread(mddev);\n\telse if (cmd_match(page, \"frozen\"))\n\t\tfrozen_sync_thread(mddev);\n\telse if (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))\n\t\treturn -EBUSY;\n\telse if (cmd_match(page, \"resync\"))\n\t\tclear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\telse if (cmd_match(page, \"recover\")) {\n\t\tclear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\t\tset_bit(MD_RECOVERY_RECOVER, &mddev->recovery);\n\t} else if (cmd_match(page, \"reshape\")) {\n\t\tint err;\n\t\tif (mddev->pers->start_reshape == NULL)\n\t\t\treturn -EINVAL;\n\t\terr = mddev_lock(mddev);\n\t\tif (!err) {\n\t\t\tif (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery)) {\n\t\t\t\terr =  -EBUSY;\n\t\t\t} else if (mddev->reshape_position == MaxSector ||\n\t\t\t\t   mddev->pers->check_reshape == NULL ||\n\t\t\t\t   mddev->pers->check_reshape(mddev)) {\n\t\t\t\tclear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\t\t\t\terr = mddev->pers->start_reshape(mddev);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tclear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\t\t\t}\n\t\t\tmddev_unlock(mddev);\n\t\t}\n\t\tif (err)\n\t\t\treturn err;\n\t\tsysfs_notify_dirent_safe(mddev->sysfs_degraded);\n\t} else {\n\t\tif (cmd_match(page, \"check\"))\n\t\t\tset_bit(MD_RECOVERY_CHECK, &mddev->recovery);\n\t\telse if (!cmd_match(page, \"repair\"))\n\t\t\treturn -EINVAL;\n\t\tclear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\t\tset_bit(MD_RECOVERY_REQUESTED, &mddev->recovery);\n\t\tset_bit(MD_RECOVERY_SYNC, &mddev->recovery);\n\t}\n\tif (mddev->ro == MD_AUTO_READ) {\n\t\t \n\t\tmddev->ro = MD_RDWR;\n\t\tmd_wakeup_thread(mddev->sync_thread);\n\t}\n\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\tmd_wakeup_thread(mddev->thread);\n\tsysfs_notify_dirent_safe(mddev->sysfs_action);\n\treturn len;\n}\n\nstatic struct md_sysfs_entry md_scan_mode =\n__ATTR_PREALLOC(sync_action, S_IRUGO|S_IWUSR, action_show, action_store);\n\nstatic ssize_t\nlast_sync_action_show(struct mddev *mddev, char *page)\n{\n\treturn sprintf(page, \"%s\\n\", mddev->last_sync_action);\n}\n\nstatic struct md_sysfs_entry md_last_scan_mode = __ATTR_RO(last_sync_action);\n\nstatic ssize_t\nmismatch_cnt_show(struct mddev *mddev, char *page)\n{\n\treturn sprintf(page, \"%llu\\n\",\n\t\t       (unsigned long long)\n\t\t       atomic64_read(&mddev->resync_mismatches));\n}\n\nstatic struct md_sysfs_entry md_mismatches = __ATTR_RO(mismatch_cnt);\n\nstatic ssize_t\nsync_min_show(struct mddev *mddev, char *page)\n{\n\treturn sprintf(page, \"%d (%s)\\n\", speed_min(mddev),\n\t\t       mddev->sync_speed_min ? \"local\": \"system\");\n}\n\nstatic ssize_t\nsync_min_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tunsigned int min;\n\tint rv;\n\n\tif (strncmp(buf, \"system\", 6)==0) {\n\t\tmin = 0;\n\t} else {\n\t\trv = kstrtouint(buf, 10, &min);\n\t\tif (rv < 0)\n\t\t\treturn rv;\n\t\tif (min == 0)\n\t\t\treturn -EINVAL;\n\t}\n\tmddev->sync_speed_min = min;\n\treturn len;\n}\n\nstatic struct md_sysfs_entry md_sync_min =\n__ATTR(sync_speed_min, S_IRUGO|S_IWUSR, sync_min_show, sync_min_store);\n\nstatic ssize_t\nsync_max_show(struct mddev *mddev, char *page)\n{\n\treturn sprintf(page, \"%d (%s)\\n\", speed_max(mddev),\n\t\t       mddev->sync_speed_max ? \"local\": \"system\");\n}\n\nstatic ssize_t\nsync_max_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tunsigned int max;\n\tint rv;\n\n\tif (strncmp(buf, \"system\", 6)==0) {\n\t\tmax = 0;\n\t} else {\n\t\trv = kstrtouint(buf, 10, &max);\n\t\tif (rv < 0)\n\t\t\treturn rv;\n\t\tif (max == 0)\n\t\t\treturn -EINVAL;\n\t}\n\tmddev->sync_speed_max = max;\n\treturn len;\n}\n\nstatic struct md_sysfs_entry md_sync_max =\n__ATTR(sync_speed_max, S_IRUGO|S_IWUSR, sync_max_show, sync_max_store);\n\nstatic ssize_t\ndegraded_show(struct mddev *mddev, char *page)\n{\n\treturn sprintf(page, \"%d\\n\", mddev->degraded);\n}\nstatic struct md_sysfs_entry md_degraded = __ATTR_RO(degraded);\n\nstatic ssize_t\nsync_force_parallel_show(struct mddev *mddev, char *page)\n{\n\treturn sprintf(page, \"%d\\n\", mddev->parallel_resync);\n}\n\nstatic ssize_t\nsync_force_parallel_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tlong n;\n\n\tif (kstrtol(buf, 10, &n))\n\t\treturn -EINVAL;\n\n\tif (n != 0 && n != 1)\n\t\treturn -EINVAL;\n\n\tmddev->parallel_resync = n;\n\n\tif (mddev->sync_thread)\n\t\twake_up(&resync_wait);\n\n\treturn len;\n}\n\n \nstatic struct md_sysfs_entry md_sync_force_parallel =\n__ATTR(sync_force_parallel, S_IRUGO|S_IWUSR,\n       sync_force_parallel_show, sync_force_parallel_store);\n\nstatic ssize_t\nsync_speed_show(struct mddev *mddev, char *page)\n{\n\tunsigned long resync, dt, db;\n\tif (mddev->curr_resync == MD_RESYNC_NONE)\n\t\treturn sprintf(page, \"none\\n\");\n\tresync = mddev->curr_mark_cnt - atomic_read(&mddev->recovery_active);\n\tdt = (jiffies - mddev->resync_mark) / HZ;\n\tif (!dt) dt++;\n\tdb = resync - mddev->resync_mark_cnt;\n\treturn sprintf(page, \"%lu\\n\", db/dt/2);  \n}\n\nstatic struct md_sysfs_entry md_sync_speed = __ATTR_RO(sync_speed);\n\nstatic ssize_t\nsync_completed_show(struct mddev *mddev, char *page)\n{\n\tunsigned long long max_sectors, resync;\n\n\tif (!test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))\n\t\treturn sprintf(page, \"none\\n\");\n\n\tif (mddev->curr_resync == MD_RESYNC_YIELDED ||\n\t    mddev->curr_resync == MD_RESYNC_DELAYED)\n\t\treturn sprintf(page, \"delayed\\n\");\n\n\tif (test_bit(MD_RECOVERY_SYNC, &mddev->recovery) ||\n\t    test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery))\n\t\tmax_sectors = mddev->resync_max_sectors;\n\telse\n\t\tmax_sectors = mddev->dev_sectors;\n\n\tresync = mddev->curr_resync_completed;\n\treturn sprintf(page, \"%llu / %llu\\n\", resync, max_sectors);\n}\n\nstatic struct md_sysfs_entry md_sync_completed =\n\t__ATTR_PREALLOC(sync_completed, S_IRUGO, sync_completed_show, NULL);\n\nstatic ssize_t\nmin_sync_show(struct mddev *mddev, char *page)\n{\n\treturn sprintf(page, \"%llu\\n\",\n\t\t       (unsigned long long)mddev->resync_min);\n}\nstatic ssize_t\nmin_sync_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tunsigned long long min;\n\tint err;\n\n\tif (kstrtoull(buf, 10, &min))\n\t\treturn -EINVAL;\n\n\tspin_lock(&mddev->lock);\n\terr = -EINVAL;\n\tif (min > mddev->resync_max)\n\t\tgoto out_unlock;\n\n\terr = -EBUSY;\n\tif (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))\n\t\tgoto out_unlock;\n\n\t \n\tmddev->resync_min = round_down(min, 8);\n\terr = 0;\n\nout_unlock:\n\tspin_unlock(&mddev->lock);\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry md_min_sync =\n__ATTR(sync_min, S_IRUGO|S_IWUSR, min_sync_show, min_sync_store);\n\nstatic ssize_t\nmax_sync_show(struct mddev *mddev, char *page)\n{\n\tif (mddev->resync_max == MaxSector)\n\t\treturn sprintf(page, \"max\\n\");\n\telse\n\t\treturn sprintf(page, \"%llu\\n\",\n\t\t\t       (unsigned long long)mddev->resync_max);\n}\nstatic ssize_t\nmax_sync_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tint err;\n\tspin_lock(&mddev->lock);\n\tif (strncmp(buf, \"max\", 3) == 0)\n\t\tmddev->resync_max = MaxSector;\n\telse {\n\t\tunsigned long long max;\n\t\tint chunk;\n\n\t\terr = -EINVAL;\n\t\tif (kstrtoull(buf, 10, &max))\n\t\t\tgoto out_unlock;\n\t\tif (max < mddev->resync_min)\n\t\t\tgoto out_unlock;\n\n\t\terr = -EBUSY;\n\t\tif (max < mddev->resync_max && md_is_rdwr(mddev) &&\n\t\t    test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))\n\t\t\tgoto out_unlock;\n\n\t\t \n\t\tchunk = mddev->chunk_sectors;\n\t\tif (chunk) {\n\t\t\tsector_t temp = max;\n\n\t\t\terr = -EINVAL;\n\t\t\tif (sector_div(temp, chunk))\n\t\t\t\tgoto out_unlock;\n\t\t}\n\t\tmddev->resync_max = max;\n\t}\n\twake_up(&mddev->recovery_wait);\n\terr = 0;\nout_unlock:\n\tspin_unlock(&mddev->lock);\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry md_max_sync =\n__ATTR(sync_max, S_IRUGO|S_IWUSR, max_sync_show, max_sync_store);\n\nstatic ssize_t\nsuspend_lo_show(struct mddev *mddev, char *page)\n{\n\treturn sprintf(page, \"%llu\\n\", (unsigned long long)mddev->suspend_lo);\n}\n\nstatic ssize_t\nsuspend_lo_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tunsigned long long new;\n\tint err;\n\n\terr = kstrtoull(buf, 10, &new);\n\tif (err < 0)\n\t\treturn err;\n\tif (new != (sector_t)new)\n\t\treturn -EINVAL;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\terr = -EINVAL;\n\tif (mddev->pers == NULL ||\n\t    mddev->pers->quiesce == NULL)\n\t\tgoto unlock;\n\tmddev_suspend(mddev);\n\tmddev->suspend_lo = new;\n\tmddev_resume(mddev);\n\n\terr = 0;\nunlock:\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\nstatic struct md_sysfs_entry md_suspend_lo =\n__ATTR(suspend_lo, S_IRUGO|S_IWUSR, suspend_lo_show, suspend_lo_store);\n\nstatic ssize_t\nsuspend_hi_show(struct mddev *mddev, char *page)\n{\n\treturn sprintf(page, \"%llu\\n\", (unsigned long long)mddev->suspend_hi);\n}\n\nstatic ssize_t\nsuspend_hi_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tunsigned long long new;\n\tint err;\n\n\terr = kstrtoull(buf, 10, &new);\n\tif (err < 0)\n\t\treturn err;\n\tif (new != (sector_t)new)\n\t\treturn -EINVAL;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\terr = -EINVAL;\n\tif (mddev->pers == NULL)\n\t\tgoto unlock;\n\n\tmddev_suspend(mddev);\n\tmddev->suspend_hi = new;\n\tmddev_resume(mddev);\n\n\terr = 0;\nunlock:\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\nstatic struct md_sysfs_entry md_suspend_hi =\n__ATTR(suspend_hi, S_IRUGO|S_IWUSR, suspend_hi_show, suspend_hi_store);\n\nstatic ssize_t\nreshape_position_show(struct mddev *mddev, char *page)\n{\n\tif (mddev->reshape_position != MaxSector)\n\t\treturn sprintf(page, \"%llu\\n\",\n\t\t\t       (unsigned long long)mddev->reshape_position);\n\tstrcpy(page, \"none\\n\");\n\treturn 5;\n}\n\nstatic ssize_t\nreshape_position_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tstruct md_rdev *rdev;\n\tunsigned long long new;\n\tint err;\n\n\terr = kstrtoull(buf, 10, &new);\n\tif (err < 0)\n\t\treturn err;\n\tif (new != (sector_t)new)\n\t\treturn -EINVAL;\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\terr = -EBUSY;\n\tif (mddev->pers)\n\t\tgoto unlock;\n\tmddev->reshape_position = new;\n\tmddev->delta_disks = 0;\n\tmddev->reshape_backwards = 0;\n\tmddev->new_level = mddev->level;\n\tmddev->new_layout = mddev->layout;\n\tmddev->new_chunk_sectors = mddev->chunk_sectors;\n\trdev_for_each(rdev, mddev)\n\t\trdev->new_data_offset = rdev->data_offset;\n\terr = 0;\nunlock:\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry md_reshape_position =\n__ATTR(reshape_position, S_IRUGO|S_IWUSR, reshape_position_show,\n       reshape_position_store);\n\nstatic ssize_t\nreshape_direction_show(struct mddev *mddev, char *page)\n{\n\treturn sprintf(page, \"%s\\n\",\n\t\t       mddev->reshape_backwards ? \"backwards\" : \"forwards\");\n}\n\nstatic ssize_t\nreshape_direction_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tint backwards = 0;\n\tint err;\n\n\tif (cmd_match(buf, \"forwards\"))\n\t\tbackwards = 0;\n\telse if (cmd_match(buf, \"backwards\"))\n\t\tbackwards = 1;\n\telse\n\t\treturn -EINVAL;\n\tif (mddev->reshape_backwards == backwards)\n\t\treturn len;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\t \n\tif (mddev->delta_disks)\n\t\terr = -EBUSY;\n\telse if (mddev->persistent &&\n\t    mddev->major_version == 0)\n\t\terr =  -EINVAL;\n\telse\n\t\tmddev->reshape_backwards = backwards;\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry md_reshape_direction =\n__ATTR(reshape_direction, S_IRUGO|S_IWUSR, reshape_direction_show,\n       reshape_direction_store);\n\nstatic ssize_t\narray_size_show(struct mddev *mddev, char *page)\n{\n\tif (mddev->external_size)\n\t\treturn sprintf(page, \"%llu\\n\",\n\t\t\t       (unsigned long long)mddev->array_sectors/2);\n\telse\n\t\treturn sprintf(page, \"default\\n\");\n}\n\nstatic ssize_t\narray_size_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tsector_t sectors;\n\tint err;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\n\t \n\tif (mddev_is_clustered(mddev)) {\n\t\tmddev_unlock(mddev);\n\t\treturn -EINVAL;\n\t}\n\n\tif (strncmp(buf, \"default\", 7) == 0) {\n\t\tif (mddev->pers)\n\t\t\tsectors = mddev->pers->size(mddev, 0, 0);\n\t\telse\n\t\t\tsectors = mddev->array_sectors;\n\n\t\tmddev->external_size = 0;\n\t} else {\n\t\tif (strict_blocks_to_sectors(buf, &sectors) < 0)\n\t\t\terr = -EINVAL;\n\t\telse if (mddev->pers && mddev->pers->size(mddev, 0, 0) < sectors)\n\t\t\terr = -E2BIG;\n\t\telse\n\t\t\tmddev->external_size = 1;\n\t}\n\n\tif (!err) {\n\t\tmddev->array_sectors = sectors;\n\t\tif (mddev->pers)\n\t\t\tset_capacity_and_notify(mddev->gendisk,\n\t\t\t\t\t\tmddev->array_sectors);\n\t}\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry md_array_size =\n__ATTR(array_size, S_IRUGO|S_IWUSR, array_size_show,\n       array_size_store);\n\nstatic ssize_t\nconsistency_policy_show(struct mddev *mddev, char *page)\n{\n\tint ret;\n\n\tif (test_bit(MD_HAS_JOURNAL, &mddev->flags)) {\n\t\tret = sprintf(page, \"journal\\n\");\n\t} else if (test_bit(MD_HAS_PPL, &mddev->flags)) {\n\t\tret = sprintf(page, \"ppl\\n\");\n\t} else if (mddev->bitmap) {\n\t\tret = sprintf(page, \"bitmap\\n\");\n\t} else if (mddev->pers) {\n\t\tif (mddev->pers->sync_request)\n\t\t\tret = sprintf(page, \"resync\\n\");\n\t\telse\n\t\t\tret = sprintf(page, \"none\\n\");\n\t} else {\n\t\tret = sprintf(page, \"unknown\\n\");\n\t}\n\n\treturn ret;\n}\n\nstatic ssize_t\nconsistency_policy_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tint err = 0;\n\n\tif (mddev->pers) {\n\t\tif (mddev->pers->change_consistency_policy)\n\t\t\terr = mddev->pers->change_consistency_policy(mddev, buf);\n\t\telse\n\t\t\terr = -EBUSY;\n\t} else if (mddev->external && strncmp(buf, \"ppl\", 3) == 0) {\n\t\tset_bit(MD_HAS_PPL, &mddev->flags);\n\t} else {\n\t\terr = -EINVAL;\n\t}\n\n\treturn err ? err : len;\n}\n\nstatic struct md_sysfs_entry md_consistency_policy =\n__ATTR(consistency_policy, S_IRUGO | S_IWUSR, consistency_policy_show,\n       consistency_policy_store);\n\nstatic ssize_t fail_last_dev_show(struct mddev *mddev, char *page)\n{\n\treturn sprintf(page, \"%d\\n\", mddev->fail_last_dev);\n}\n\n \nstatic ssize_t\nfail_last_dev_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tint ret;\n\tbool value;\n\n\tret = kstrtobool(buf, &value);\n\tif (ret)\n\t\treturn ret;\n\n\tif (value != mddev->fail_last_dev)\n\t\tmddev->fail_last_dev = value;\n\n\treturn len;\n}\nstatic struct md_sysfs_entry md_fail_last_dev =\n__ATTR(fail_last_dev, S_IRUGO | S_IWUSR, fail_last_dev_show,\n       fail_last_dev_store);\n\nstatic ssize_t serialize_policy_show(struct mddev *mddev, char *page)\n{\n\tif (mddev->pers == NULL || (mddev->pers->level != 1))\n\t\treturn sprintf(page, \"n/a\\n\");\n\telse\n\t\treturn sprintf(page, \"%d\\n\", mddev->serialize_policy);\n}\n\n \nstatic ssize_t\nserialize_policy_store(struct mddev *mddev, const char *buf, size_t len)\n{\n\tint err;\n\tbool value;\n\n\terr = kstrtobool(buf, &value);\n\tif (err)\n\t\treturn err;\n\n\tif (value == mddev->serialize_policy)\n\t\treturn len;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tif (mddev->pers == NULL || (mddev->pers->level != 1)) {\n\t\tpr_err(\"md: serialize_policy is only effective for raid1\\n\");\n\t\terr = -EINVAL;\n\t\tgoto unlock;\n\t}\n\n\tmddev_suspend(mddev);\n\tif (value)\n\t\tmddev_create_serial_pool(mddev, NULL, true);\n\telse\n\t\tmddev_destroy_serial_pool(mddev, NULL, true);\n\tmddev->serialize_policy = value;\n\tmddev_resume(mddev);\nunlock:\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry md_serialize_policy =\n__ATTR(serialize_policy, S_IRUGO | S_IWUSR, serialize_policy_show,\n       serialize_policy_store);\n\n\nstatic struct attribute *md_default_attrs[] = {\n\t&md_level.attr,\n\t&md_layout.attr,\n\t&md_raid_disks.attr,\n\t&md_uuid.attr,\n\t&md_chunk_size.attr,\n\t&md_size.attr,\n\t&md_resync_start.attr,\n\t&md_metadata.attr,\n\t&md_new_device.attr,\n\t&md_safe_delay.attr,\n\t&md_array_state.attr,\n\t&md_reshape_position.attr,\n\t&md_reshape_direction.attr,\n\t&md_array_size.attr,\n\t&max_corr_read_errors.attr,\n\t&md_consistency_policy.attr,\n\t&md_fail_last_dev.attr,\n\t&md_serialize_policy.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group md_default_group = {\n\t.attrs = md_default_attrs,\n};\n\nstatic struct attribute *md_redundancy_attrs[] = {\n\t&md_scan_mode.attr,\n\t&md_last_scan_mode.attr,\n\t&md_mismatches.attr,\n\t&md_sync_min.attr,\n\t&md_sync_max.attr,\n\t&md_sync_speed.attr,\n\t&md_sync_force_parallel.attr,\n\t&md_sync_completed.attr,\n\t&md_min_sync.attr,\n\t&md_max_sync.attr,\n\t&md_suspend_lo.attr,\n\t&md_suspend_hi.attr,\n\t&md_bitmap.attr,\n\t&md_degraded.attr,\n\tNULL,\n};\nstatic const struct attribute_group md_redundancy_group = {\n\t.name = NULL,\n\t.attrs = md_redundancy_attrs,\n};\n\nstatic const struct attribute_group *md_attr_groups[] = {\n\t&md_default_group,\n\t&md_bitmap_group,\n\tNULL,\n};\n\nstatic ssize_t\nmd_attr_show(struct kobject *kobj, struct attribute *attr, char *page)\n{\n\tstruct md_sysfs_entry *entry = container_of(attr, struct md_sysfs_entry, attr);\n\tstruct mddev *mddev = container_of(kobj, struct mddev, kobj);\n\tssize_t rv;\n\n\tif (!entry->show)\n\t\treturn -EIO;\n\tspin_lock(&all_mddevs_lock);\n\tif (!mddev_get(mddev)) {\n\t\tspin_unlock(&all_mddevs_lock);\n\t\treturn -EBUSY;\n\t}\n\tspin_unlock(&all_mddevs_lock);\n\n\trv = entry->show(mddev, page);\n\tmddev_put(mddev);\n\treturn rv;\n}\n\nstatic ssize_t\nmd_attr_store(struct kobject *kobj, struct attribute *attr,\n\t      const char *page, size_t length)\n{\n\tstruct md_sysfs_entry *entry = container_of(attr, struct md_sysfs_entry, attr);\n\tstruct mddev *mddev = container_of(kobj, struct mddev, kobj);\n\tssize_t rv;\n\n\tif (!entry->store)\n\t\treturn -EIO;\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\tspin_lock(&all_mddevs_lock);\n\tif (!mddev_get(mddev)) {\n\t\tspin_unlock(&all_mddevs_lock);\n\t\treturn -EBUSY;\n\t}\n\tspin_unlock(&all_mddevs_lock);\n\trv = entry->store(mddev, page, length);\n\tmddev_put(mddev);\n\treturn rv;\n}\n\nstatic void md_kobj_release(struct kobject *ko)\n{\n\tstruct mddev *mddev = container_of(ko, struct mddev, kobj);\n\n\tif (mddev->sysfs_state)\n\t\tsysfs_put(mddev->sysfs_state);\n\tif (mddev->sysfs_level)\n\t\tsysfs_put(mddev->sysfs_level);\n\n\tdel_gendisk(mddev->gendisk);\n\tput_disk(mddev->gendisk);\n}\n\nstatic const struct sysfs_ops md_sysfs_ops = {\n\t.show\t= md_attr_show,\n\t.store\t= md_attr_store,\n};\nstatic const struct kobj_type md_ktype = {\n\t.release\t= md_kobj_release,\n\t.sysfs_ops\t= &md_sysfs_ops,\n\t.default_groups\t= md_attr_groups,\n};\n\nint mdp_major = 0;\n\nstatic void mddev_delayed_delete(struct work_struct *ws)\n{\n\tstruct mddev *mddev = container_of(ws, struct mddev, del_work);\n\n\tkobject_put(&mddev->kobj);\n}\n\nstatic void no_op(struct percpu_ref *r) {}\n\nint mddev_init_writes_pending(struct mddev *mddev)\n{\n\tif (mddev->writes_pending.percpu_count_ptr)\n\t\treturn 0;\n\tif (percpu_ref_init(&mddev->writes_pending, no_op,\n\t\t\t    PERCPU_REF_ALLOW_REINIT, GFP_KERNEL) < 0)\n\t\treturn -ENOMEM;\n\t \n\tpercpu_ref_put(&mddev->writes_pending);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(mddev_init_writes_pending);\n\nstruct mddev *md_alloc(dev_t dev, char *name)\n{\n\t \n\tstatic DEFINE_MUTEX(disks_mutex);\n\tstruct mddev *mddev;\n\tstruct gendisk *disk;\n\tint partitioned;\n\tint shift;\n\tint unit;\n\tint error ;\n\n\t \n\tflush_workqueue(md_misc_wq);\n\n\tmutex_lock(&disks_mutex);\n\tmddev = mddev_alloc(dev);\n\tif (IS_ERR(mddev)) {\n\t\terror = PTR_ERR(mddev);\n\t\tgoto out_unlock;\n\t}\n\n\tpartitioned = (MAJOR(mddev->unit) != MD_MAJOR);\n\tshift = partitioned ? MdpMinorShift : 0;\n\tunit = MINOR(mddev->unit) >> shift;\n\n\tif (name && !dev) {\n\t\t \n\t\tstruct mddev *mddev2;\n\t\tspin_lock(&all_mddevs_lock);\n\n\t\tlist_for_each_entry(mddev2, &all_mddevs, all_mddevs)\n\t\t\tif (mddev2->gendisk &&\n\t\t\t    strcmp(mddev2->gendisk->disk_name, name) == 0) {\n\t\t\t\tspin_unlock(&all_mddevs_lock);\n\t\t\t\terror = -EEXIST;\n\t\t\t\tgoto out_free_mddev;\n\t\t\t}\n\t\tspin_unlock(&all_mddevs_lock);\n\t}\n\tif (name && dev)\n\t\t \n\t\tmddev->hold_active = UNTIL_STOP;\n\n\terror = -ENOMEM;\n\tdisk = blk_alloc_disk(NUMA_NO_NODE);\n\tif (!disk)\n\t\tgoto out_free_mddev;\n\n\tdisk->major = MAJOR(mddev->unit);\n\tdisk->first_minor = unit << shift;\n\tdisk->minors = 1 << shift;\n\tif (name)\n\t\tstrcpy(disk->disk_name, name);\n\telse if (partitioned)\n\t\tsprintf(disk->disk_name, \"md_d%d\", unit);\n\telse\n\t\tsprintf(disk->disk_name, \"md%d\", unit);\n\tdisk->fops = &md_fops;\n\tdisk->private_data = mddev;\n\n\tmddev->queue = disk->queue;\n\tblk_set_stacking_limits(&mddev->queue->limits);\n\tblk_queue_write_cache(mddev->queue, true, true);\n\tdisk->events |= DISK_EVENT_MEDIA_CHANGE;\n\tmddev->gendisk = disk;\n\terror = add_disk(disk);\n\tif (error)\n\t\tgoto out_put_disk;\n\n\tkobject_init(&mddev->kobj, &md_ktype);\n\terror = kobject_add(&mddev->kobj, &disk_to_dev(disk)->kobj, \"%s\", \"md\");\n\tif (error) {\n\t\t \n\t\tmddev->hold_active = 0;\n\t\tmutex_unlock(&disks_mutex);\n\t\tmddev_put(mddev);\n\t\treturn ERR_PTR(error);\n\t}\n\n\tkobject_uevent(&mddev->kobj, KOBJ_ADD);\n\tmddev->sysfs_state = sysfs_get_dirent_safe(mddev->kobj.sd, \"array_state\");\n\tmddev->sysfs_level = sysfs_get_dirent_safe(mddev->kobj.sd, \"level\");\n\tmutex_unlock(&disks_mutex);\n\treturn mddev;\n\nout_put_disk:\n\tput_disk(disk);\nout_free_mddev:\n\tmddev_free(mddev);\nout_unlock:\n\tmutex_unlock(&disks_mutex);\n\treturn ERR_PTR(error);\n}\n\nstatic int md_alloc_and_put(dev_t dev, char *name)\n{\n\tstruct mddev *mddev = md_alloc(dev, name);\n\n\tif (IS_ERR(mddev))\n\t\treturn PTR_ERR(mddev);\n\tmddev_put(mddev);\n\treturn 0;\n}\n\nstatic void md_probe(dev_t dev)\n{\n\tif (MAJOR(dev) == MD_MAJOR && MINOR(dev) >= 512)\n\t\treturn;\n\tif (create_on_open)\n\t\tmd_alloc_and_put(dev, NULL);\n}\n\nstatic int add_named_array(const char *val, const struct kernel_param *kp)\n{\n\t \n\tint len = strlen(val);\n\tchar buf[DISK_NAME_LEN];\n\tunsigned long devnum;\n\n\twhile (len && val[len-1] == '\\n')\n\t\tlen--;\n\tif (len >= DISK_NAME_LEN)\n\t\treturn -E2BIG;\n\tstrscpy(buf, val, len+1);\n\tif (strncmp(buf, \"md_\", 3) == 0)\n\t\treturn md_alloc_and_put(0, buf);\n\tif (strncmp(buf, \"md\", 2) == 0 &&\n\t    isdigit(buf[2]) &&\n\t    kstrtoul(buf+2, 10, &devnum) == 0 &&\n\t    devnum <= MINORMASK)\n\t\treturn md_alloc_and_put(MKDEV(MD_MAJOR, devnum), NULL);\n\n\treturn -EINVAL;\n}\n\nstatic void md_safemode_timeout(struct timer_list *t)\n{\n\tstruct mddev *mddev = from_timer(mddev, t, safemode_timer);\n\n\tmddev->safemode = 1;\n\tif (mddev->external)\n\t\tsysfs_notify_dirent_safe(mddev->sysfs_state);\n\n\tmd_wakeup_thread(mddev->thread);\n}\n\nstatic int start_dirty_degraded;\nstatic void active_io_release(struct percpu_ref *ref)\n{\n\tstruct mddev *mddev = container_of(ref, struct mddev, active_io);\n\n\twake_up(&mddev->sb_wait);\n}\n\nint md_run(struct mddev *mddev)\n{\n\tint err;\n\tstruct md_rdev *rdev;\n\tstruct md_personality *pers;\n\tbool nowait = true;\n\n\tif (list_empty(&mddev->disks))\n\t\t \n\t\treturn -EINVAL;\n\n\tif (mddev->pers)\n\t\treturn -EBUSY;\n\t \n\tif (mddev->sysfs_active)\n\t\treturn -EBUSY;\n\n\t \n\tif (!mddev->raid_disks) {\n\t\tif (!mddev->persistent)\n\t\t\treturn -EINVAL;\n\t\terr = analyze_sbs(mddev);\n\t\tif (err)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (mddev->level != LEVEL_NONE)\n\t\trequest_module(\"md-level-%d\", mddev->level);\n\telse if (mddev->clevel[0])\n\t\trequest_module(\"md-%s\", mddev->clevel);\n\n\t \n\tmddev->has_superblocks = false;\n\trdev_for_each(rdev, mddev) {\n\t\tif (test_bit(Faulty, &rdev->flags))\n\t\t\tcontinue;\n\t\tsync_blockdev(rdev->bdev);\n\t\tinvalidate_bdev(rdev->bdev);\n\t\tif (mddev->ro != MD_RDONLY && rdev_read_only(rdev)) {\n\t\t\tmddev->ro = MD_RDONLY;\n\t\t\tif (mddev->gendisk)\n\t\t\t\tset_disk_ro(mddev->gendisk, 1);\n\t\t}\n\n\t\tif (rdev->sb_page)\n\t\t\tmddev->has_superblocks = true;\n\n\t\t \n\t\tif (rdev->meta_bdev) {\n\t\t\t ;\n\t\t} else if (rdev->data_offset < rdev->sb_start) {\n\t\t\tif (mddev->dev_sectors &&\n\t\t\t    rdev->data_offset + mddev->dev_sectors\n\t\t\t    > rdev->sb_start) {\n\t\t\t\tpr_warn(\"md: %s: data overlaps metadata\\n\",\n\t\t\t\t\tmdname(mddev));\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else {\n\t\t\tif (rdev->sb_start + rdev->sb_size/512\n\t\t\t    > rdev->data_offset) {\n\t\t\t\tpr_warn(\"md: %s: metadata overlaps data\\n\",\n\t\t\t\t\tmdname(mddev));\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t\tsysfs_notify_dirent_safe(rdev->sysfs_state);\n\t\tnowait = nowait && bdev_nowait(rdev->bdev);\n\t}\n\n\terr = percpu_ref_init(&mddev->active_io, active_io_release,\n\t\t\t\tPERCPU_REF_ALLOW_REINIT, GFP_KERNEL);\n\tif (err)\n\t\treturn err;\n\n\tif (!bioset_initialized(&mddev->bio_set)) {\n\t\terr = bioset_init(&mddev->bio_set, BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS);\n\t\tif (err)\n\t\t\tgoto exit_active_io;\n\t}\n\tif (!bioset_initialized(&mddev->sync_set)) {\n\t\terr = bioset_init(&mddev->sync_set, BIO_POOL_SIZE, 0, BIOSET_NEED_BVECS);\n\t\tif (err)\n\t\t\tgoto exit_bio_set;\n\t}\n\n\tif (!bioset_initialized(&mddev->io_clone_set)) {\n\t\terr = bioset_init(&mddev->io_clone_set, BIO_POOL_SIZE,\n\t\t\t\t  offsetof(struct md_io_clone, bio_clone), 0);\n\t\tif (err)\n\t\t\tgoto exit_sync_set;\n\t}\n\n\tspin_lock(&pers_lock);\n\tpers = find_pers(mddev->level, mddev->clevel);\n\tif (!pers || !try_module_get(pers->owner)) {\n\t\tspin_unlock(&pers_lock);\n\t\tif (mddev->level != LEVEL_NONE)\n\t\t\tpr_warn(\"md: personality for level %d is not loaded!\\n\",\n\t\t\t\tmddev->level);\n\t\telse\n\t\t\tpr_warn(\"md: personality for level %s is not loaded!\\n\",\n\t\t\t\tmddev->clevel);\n\t\terr = -EINVAL;\n\t\tgoto abort;\n\t}\n\tspin_unlock(&pers_lock);\n\tif (mddev->level != pers->level) {\n\t\tmddev->level = pers->level;\n\t\tmddev->new_level = pers->level;\n\t}\n\tstrscpy(mddev->clevel, pers->name, sizeof(mddev->clevel));\n\n\tif (mddev->reshape_position != MaxSector &&\n\t    pers->start_reshape == NULL) {\n\t\t \n\t\tmodule_put(pers->owner);\n\t\terr = -EINVAL;\n\t\tgoto abort;\n\t}\n\n\tif (pers->sync_request) {\n\t\t \n\t\tstruct md_rdev *rdev2;\n\t\tint warned = 0;\n\n\t\trdev_for_each(rdev, mddev)\n\t\t\trdev_for_each(rdev2, mddev) {\n\t\t\t\tif (rdev < rdev2 &&\n\t\t\t\t    rdev->bdev->bd_disk ==\n\t\t\t\t    rdev2->bdev->bd_disk) {\n\t\t\t\t\tpr_warn(\"%s: WARNING: %pg appears to be on the same physical disk as %pg.\\n\",\n\t\t\t\t\t\tmdname(mddev),\n\t\t\t\t\t\trdev->bdev,\n\t\t\t\t\t\trdev2->bdev);\n\t\t\t\t\twarned = 1;\n\t\t\t\t}\n\t\t\t}\n\n\t\tif (warned)\n\t\t\tpr_warn(\"True protection against single-disk failure might be compromised.\\n\");\n\t}\n\n\tmddev->recovery = 0;\n\t \n\tmddev->resync_max_sectors = mddev->dev_sectors;\n\n\tmddev->ok_start_degraded = start_dirty_degraded;\n\n\tif (start_readonly && md_is_rdwr(mddev))\n\t\tmddev->ro = MD_AUTO_READ;  \n\n\terr = pers->run(mddev);\n\tif (err)\n\t\tpr_warn(\"md: pers->run() failed ...\\n\");\n\telse if (pers->size(mddev, 0, 0) < mddev->array_sectors) {\n\t\tWARN_ONCE(!mddev->external_size,\n\t\t\t  \"%s: default size too small, but 'external_size' not in effect?\\n\",\n\t\t\t  __func__);\n\t\tpr_warn(\"md: invalid array_size %llu > default size %llu\\n\",\n\t\t\t(unsigned long long)mddev->array_sectors / 2,\n\t\t\t(unsigned long long)pers->size(mddev, 0, 0) / 2);\n\t\terr = -EINVAL;\n\t}\n\tif (err == 0 && pers->sync_request &&\n\t    (mddev->bitmap_info.file || mddev->bitmap_info.offset)) {\n\t\tstruct bitmap *bitmap;\n\n\t\tbitmap = md_bitmap_create(mddev, -1);\n\t\tif (IS_ERR(bitmap)) {\n\t\t\terr = PTR_ERR(bitmap);\n\t\t\tpr_warn(\"%s: failed to create bitmap (%d)\\n\",\n\t\t\t\tmdname(mddev), err);\n\t\t} else\n\t\t\tmddev->bitmap = bitmap;\n\n\t}\n\tif (err)\n\t\tgoto bitmap_abort;\n\n\tif (mddev->bitmap_info.max_write_behind > 0) {\n\t\tbool create_pool = false;\n\n\t\trdev_for_each(rdev, mddev) {\n\t\t\tif (test_bit(WriteMostly, &rdev->flags) &&\n\t\t\t    rdev_init_serial(rdev))\n\t\t\t\tcreate_pool = true;\n\t\t}\n\t\tif (create_pool && mddev->serial_info_pool == NULL) {\n\t\t\tmddev->serial_info_pool =\n\t\t\t\tmempool_create_kmalloc_pool(NR_SERIAL_INFOS,\n\t\t\t\t\t\t    sizeof(struct serial_info));\n\t\t\tif (!mddev->serial_info_pool) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto bitmap_abort;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (mddev->queue) {\n\t\tbool nonrot = true;\n\n\t\trdev_for_each(rdev, mddev) {\n\t\t\tif (rdev->raid_disk >= 0 && !bdev_nonrot(rdev->bdev)) {\n\t\t\t\tnonrot = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (mddev->degraded)\n\t\t\tnonrot = false;\n\t\tif (nonrot)\n\t\t\tblk_queue_flag_set(QUEUE_FLAG_NONROT, mddev->queue);\n\t\telse\n\t\t\tblk_queue_flag_clear(QUEUE_FLAG_NONROT, mddev->queue);\n\t\tblk_queue_flag_set(QUEUE_FLAG_IO_STAT, mddev->queue);\n\n\t\t \n\t\tif (nowait)\n\t\t\tblk_queue_flag_set(QUEUE_FLAG_NOWAIT, mddev->queue);\n\t}\n\tif (pers->sync_request) {\n\t\tif (mddev->kobj.sd &&\n\t\t    sysfs_create_group(&mddev->kobj, &md_redundancy_group))\n\t\t\tpr_warn(\"md: cannot register extra attributes for %s\\n\",\n\t\t\t\tmdname(mddev));\n\t\tmddev->sysfs_action = sysfs_get_dirent_safe(mddev->kobj.sd, \"sync_action\");\n\t\tmddev->sysfs_completed = sysfs_get_dirent_safe(mddev->kobj.sd, \"sync_completed\");\n\t\tmddev->sysfs_degraded = sysfs_get_dirent_safe(mddev->kobj.sd, \"degraded\");\n\t} else if (mddev->ro == MD_AUTO_READ)\n\t\tmddev->ro = MD_RDWR;\n\n\tatomic_set(&mddev->max_corr_read_errors,\n\t\t   MD_DEFAULT_MAX_CORRECTED_READ_ERRORS);\n\tmddev->safemode = 0;\n\tif (mddev_is_clustered(mddev))\n\t\tmddev->safemode_delay = 0;\n\telse\n\t\tmddev->safemode_delay = DEFAULT_SAFEMODE_DELAY;\n\tmddev->in_sync = 1;\n\tsmp_wmb();\n\tspin_lock(&mddev->lock);\n\tmddev->pers = pers;\n\tspin_unlock(&mddev->lock);\n\trdev_for_each(rdev, mddev)\n\t\tif (rdev->raid_disk >= 0)\n\t\t\tsysfs_link_rdev(mddev, rdev);  \n\n\tif (mddev->degraded && md_is_rdwr(mddev))\n\t\t \n\t\tset_bit(MD_RECOVERY_RECOVER, &mddev->recovery);\n\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\n\tif (mddev->sb_flags)\n\t\tmd_update_sb(mddev, 0);\n\n\tmd_new_event();\n\treturn 0;\n\nbitmap_abort:\n\tmddev_detach(mddev);\n\tif (mddev->private)\n\t\tpers->free(mddev, mddev->private);\n\tmddev->private = NULL;\n\tmodule_put(pers->owner);\n\tmd_bitmap_destroy(mddev);\nabort:\n\tbioset_exit(&mddev->io_clone_set);\nexit_sync_set:\n\tbioset_exit(&mddev->sync_set);\nexit_bio_set:\n\tbioset_exit(&mddev->bio_set);\nexit_active_io:\n\tpercpu_ref_exit(&mddev->active_io);\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(md_run);\n\nint do_md_run(struct mddev *mddev)\n{\n\tint err;\n\n\tset_bit(MD_NOT_READY, &mddev->flags);\n\terr = md_run(mddev);\n\tif (err)\n\t\tgoto out;\n\terr = md_bitmap_load(mddev);\n\tif (err) {\n\t\tmd_bitmap_destroy(mddev);\n\t\tgoto out;\n\t}\n\n\tif (mddev_is_clustered(mddev))\n\t\tmd_allow_write(mddev);\n\n\t \n\tmd_start(mddev);\n\n\tmd_wakeup_thread(mddev->thread);\n\tmd_wakeup_thread(mddev->sync_thread);  \n\n\tset_capacity_and_notify(mddev->gendisk, mddev->array_sectors);\n\tclear_bit(MD_NOT_READY, &mddev->flags);\n\tmddev->changed = 1;\n\tkobject_uevent(&disk_to_dev(mddev->gendisk)->kobj, KOBJ_CHANGE);\n\tsysfs_notify_dirent_safe(mddev->sysfs_state);\n\tsysfs_notify_dirent_safe(mddev->sysfs_action);\n\tsysfs_notify_dirent_safe(mddev->sysfs_degraded);\nout:\n\tclear_bit(MD_NOT_READY, &mddev->flags);\n\treturn err;\n}\n\nint md_start(struct mddev *mddev)\n{\n\tint ret = 0;\n\n\tif (mddev->pers->start) {\n\t\tset_bit(MD_RECOVERY_WAIT, &mddev->recovery);\n\t\tmd_wakeup_thread(mddev->thread);\n\t\tret = mddev->pers->start(mddev);\n\t\tclear_bit(MD_RECOVERY_WAIT, &mddev->recovery);\n\t\tmd_wakeup_thread(mddev->sync_thread);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(md_start);\n\nstatic int restart_array(struct mddev *mddev)\n{\n\tstruct gendisk *disk = mddev->gendisk;\n\tstruct md_rdev *rdev;\n\tbool has_journal = false;\n\tbool has_readonly = false;\n\n\t \n\tif (list_empty(&mddev->disks))\n\t\treturn -ENXIO;\n\tif (!mddev->pers)\n\t\treturn -EINVAL;\n\tif (md_is_rdwr(mddev))\n\t\treturn -EBUSY;\n\n\trcu_read_lock();\n\trdev_for_each_rcu(rdev, mddev) {\n\t\tif (test_bit(Journal, &rdev->flags) &&\n\t\t    !test_bit(Faulty, &rdev->flags))\n\t\t\thas_journal = true;\n\t\tif (rdev_read_only(rdev))\n\t\t\thas_readonly = true;\n\t}\n\trcu_read_unlock();\n\tif (test_bit(MD_HAS_JOURNAL, &mddev->flags) && !has_journal)\n\t\t \n\t\t\treturn -EINVAL;\n\tif (has_readonly)\n\t\treturn -EROFS;\n\n\tmddev->safemode = 0;\n\tmddev->ro = MD_RDWR;\n\tset_disk_ro(disk, 0);\n\tpr_debug(\"md: %s switched to read-write mode.\\n\", mdname(mddev));\n\t \n\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\tmd_wakeup_thread(mddev->thread);\n\tmd_wakeup_thread(mddev->sync_thread);\n\tsysfs_notify_dirent_safe(mddev->sysfs_state);\n\treturn 0;\n}\n\nstatic void md_clean(struct mddev *mddev)\n{\n\tmddev->array_sectors = 0;\n\tmddev->external_size = 0;\n\tmddev->dev_sectors = 0;\n\tmddev->raid_disks = 0;\n\tmddev->recovery_cp = 0;\n\tmddev->resync_min = 0;\n\tmddev->resync_max = MaxSector;\n\tmddev->reshape_position = MaxSector;\n\t \n\tmddev->persistent = 0;\n\tmddev->level = LEVEL_NONE;\n\tmddev->clevel[0] = 0;\n\tmddev->flags = 0;\n\tmddev->sb_flags = 0;\n\tmddev->ro = MD_RDWR;\n\tmddev->metadata_type[0] = 0;\n\tmddev->chunk_sectors = 0;\n\tmddev->ctime = mddev->utime = 0;\n\tmddev->layout = 0;\n\tmddev->max_disks = 0;\n\tmddev->events = 0;\n\tmddev->can_decrease_events = 0;\n\tmddev->delta_disks = 0;\n\tmddev->reshape_backwards = 0;\n\tmddev->new_level = LEVEL_NONE;\n\tmddev->new_layout = 0;\n\tmddev->new_chunk_sectors = 0;\n\tmddev->curr_resync = MD_RESYNC_NONE;\n\tatomic64_set(&mddev->resync_mismatches, 0);\n\tmddev->suspend_lo = mddev->suspend_hi = 0;\n\tmddev->sync_speed_min = mddev->sync_speed_max = 0;\n\tmddev->recovery = 0;\n\tmddev->in_sync = 0;\n\tmddev->changed = 0;\n\tmddev->degraded = 0;\n\tmddev->safemode = 0;\n\tmddev->private = NULL;\n\tmddev->cluster_info = NULL;\n\tmddev->bitmap_info.offset = 0;\n\tmddev->bitmap_info.default_offset = 0;\n\tmddev->bitmap_info.default_space = 0;\n\tmddev->bitmap_info.chunksize = 0;\n\tmddev->bitmap_info.daemon_sleep = 0;\n\tmddev->bitmap_info.max_write_behind = 0;\n\tmddev->bitmap_info.nodes = 0;\n}\n\nstatic void __md_stop_writes(struct mddev *mddev)\n{\n\tset_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\tif (work_pending(&mddev->del_work))\n\t\tflush_workqueue(md_misc_wq);\n\tif (mddev->sync_thread) {\n\t\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\t\tmd_reap_sync_thread(mddev);\n\t}\n\n\tdel_timer_sync(&mddev->safemode_timer);\n\n\tif (mddev->pers && mddev->pers->quiesce) {\n\t\tmddev->pers->quiesce(mddev, 1);\n\t\tmddev->pers->quiesce(mddev, 0);\n\t}\n\tmd_bitmap_flush(mddev);\n\n\tif (md_is_rdwr(mddev) &&\n\t    ((!mddev->in_sync && !mddev_is_clustered(mddev)) ||\n\t     mddev->sb_flags)) {\n\t\t \n\t\tif (!mddev_is_clustered(mddev))\n\t\t\tmddev->in_sync = 1;\n\t\tmd_update_sb(mddev, 1);\n\t}\n\t \n\tmddev->serialize_policy = 0;\n\tmddev_destroy_serial_pool(mddev, NULL, true);\n}\n\nvoid md_stop_writes(struct mddev *mddev)\n{\n\tmddev_lock_nointr(mddev);\n\t__md_stop_writes(mddev);\n\tmddev_unlock(mddev);\n}\nEXPORT_SYMBOL_GPL(md_stop_writes);\n\nstatic void mddev_detach(struct mddev *mddev)\n{\n\tmd_bitmap_wait_behind_writes(mddev);\n\tif (mddev->pers && mddev->pers->quiesce && !is_md_suspended(mddev)) {\n\t\tmddev->pers->quiesce(mddev, 1);\n\t\tmddev->pers->quiesce(mddev, 0);\n\t}\n\tmd_unregister_thread(mddev, &mddev->thread);\n\tif (mddev->queue)\n\t\tblk_sync_queue(mddev->queue);  \n}\n\nstatic void __md_stop(struct mddev *mddev)\n{\n\tstruct md_personality *pers = mddev->pers;\n\tmd_bitmap_destroy(mddev);\n\tmddev_detach(mddev);\n\t \n\tif (mddev->event_work.func)\n\t\tflush_workqueue(md_misc_wq);\n\tspin_lock(&mddev->lock);\n\tmddev->pers = NULL;\n\tspin_unlock(&mddev->lock);\n\tif (mddev->private)\n\t\tpers->free(mddev, mddev->private);\n\tmddev->private = NULL;\n\tif (pers->sync_request && mddev->to_remove == NULL)\n\t\tmddev->to_remove = &md_redundancy_group;\n\tmodule_put(pers->owner);\n\tclear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\n\tpercpu_ref_exit(&mddev->active_io);\n\tbioset_exit(&mddev->bio_set);\n\tbioset_exit(&mddev->sync_set);\n\tbioset_exit(&mddev->io_clone_set);\n}\n\nvoid md_stop(struct mddev *mddev)\n{\n\tlockdep_assert_held(&mddev->reconfig_mutex);\n\n\t \n\t__md_stop_writes(mddev);\n\t__md_stop(mddev);\n\tpercpu_ref_exit(&mddev->writes_pending);\n}\n\nEXPORT_SYMBOL_GPL(md_stop);\n\nstatic int md_set_readonly(struct mddev *mddev, struct block_device *bdev)\n{\n\tint err = 0;\n\tint did_freeze = 0;\n\n\tif (mddev->external && test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags))\n\t\treturn -EBUSY;\n\n\tif (!test_bit(MD_RECOVERY_FROZEN, &mddev->recovery)) {\n\t\tdid_freeze = 1;\n\t\tset_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\t\tmd_wakeup_thread(mddev->thread);\n\t}\n\tif (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))\n\t\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\n\t \n\tmd_wakeup_thread_directly(mddev->sync_thread);\n\n\tmddev_unlock(mddev);\n\twait_event(resync_wait, !test_bit(MD_RECOVERY_RUNNING,\n\t\t\t\t\t  &mddev->recovery));\n\twait_event(mddev->sb_wait,\n\t\t   !test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags));\n\tmddev_lock_nointr(mddev);\n\n\tmutex_lock(&mddev->open_mutex);\n\tif ((mddev->pers && atomic_read(&mddev->openers) > !!bdev) ||\n\t    mddev->sync_thread ||\n\t    test_bit(MD_RECOVERY_RUNNING, &mddev->recovery)) {\n\t\tpr_warn(\"md: %s still in use.\\n\",mdname(mddev));\n\t\terr = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tif (mddev->pers) {\n\t\t__md_stop_writes(mddev);\n\n\t\tif (mddev->ro == MD_RDONLY) {\n\t\t\terr  = -ENXIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tmddev->ro = MD_RDONLY;\n\t\tset_disk_ro(mddev->gendisk, 1);\n\t}\n\nout:\n\tif ((mddev->pers && !err) || did_freeze) {\n\t\tclear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\t\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\t\tmd_wakeup_thread(mddev->thread);\n\t\tsysfs_notify_dirent_safe(mddev->sysfs_state);\n\t}\n\n\tmutex_unlock(&mddev->open_mutex);\n\treturn err;\n}\n\n \nstatic int do_md_stop(struct mddev *mddev, int mode,\n\t\t      struct block_device *bdev)\n{\n\tstruct gendisk *disk = mddev->gendisk;\n\tstruct md_rdev *rdev;\n\tint did_freeze = 0;\n\n\tif (!test_bit(MD_RECOVERY_FROZEN, &mddev->recovery)) {\n\t\tdid_freeze = 1;\n\t\tset_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\t\tmd_wakeup_thread(mddev->thread);\n\t}\n\tif (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))\n\t\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\n\t \n\tmd_wakeup_thread_directly(mddev->sync_thread);\n\n\tmddev_unlock(mddev);\n\twait_event(resync_wait, (mddev->sync_thread == NULL &&\n\t\t\t\t !test_bit(MD_RECOVERY_RUNNING,\n\t\t\t\t\t   &mddev->recovery)));\n\tmddev_lock_nointr(mddev);\n\n\tmutex_lock(&mddev->open_mutex);\n\tif ((mddev->pers && atomic_read(&mddev->openers) > !!bdev) ||\n\t    mddev->sysfs_active ||\n\t    mddev->sync_thread ||\n\t    test_bit(MD_RECOVERY_RUNNING, &mddev->recovery)) {\n\t\tpr_warn(\"md: %s still in use.\\n\",mdname(mddev));\n\t\tmutex_unlock(&mddev->open_mutex);\n\t\tif (did_freeze) {\n\t\t\tclear_bit(MD_RECOVERY_FROZEN, &mddev->recovery);\n\t\t\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\t\t\tmd_wakeup_thread(mddev->thread);\n\t\t}\n\t\treturn -EBUSY;\n\t}\n\tif (mddev->pers) {\n\t\tif (!md_is_rdwr(mddev))\n\t\t\tset_disk_ro(disk, 0);\n\n\t\t__md_stop_writes(mddev);\n\t\t__md_stop(mddev);\n\n\t\t \n\t\tsysfs_notify_dirent_safe(mddev->sysfs_state);\n\n\t\trdev_for_each(rdev, mddev)\n\t\t\tif (rdev->raid_disk >= 0)\n\t\t\t\tsysfs_unlink_rdev(mddev, rdev);\n\n\t\tset_capacity_and_notify(disk, 0);\n\t\tmutex_unlock(&mddev->open_mutex);\n\t\tmddev->changed = 1;\n\n\t\tif (!md_is_rdwr(mddev))\n\t\t\tmddev->ro = MD_RDWR;\n\t} else\n\t\tmutex_unlock(&mddev->open_mutex);\n\t \n\tif (mode == 0) {\n\t\tpr_info(\"md: %s stopped.\\n\", mdname(mddev));\n\n\t\tif (mddev->bitmap_info.file) {\n\t\t\tstruct file *f = mddev->bitmap_info.file;\n\t\t\tspin_lock(&mddev->lock);\n\t\t\tmddev->bitmap_info.file = NULL;\n\t\t\tspin_unlock(&mddev->lock);\n\t\t\tfput(f);\n\t\t}\n\t\tmddev->bitmap_info.offset = 0;\n\n\t\texport_array(mddev);\n\n\t\tmd_clean(mddev);\n\t\tif (mddev->hold_active == UNTIL_STOP)\n\t\t\tmddev->hold_active = 0;\n\t}\n\tmd_new_event();\n\tsysfs_notify_dirent_safe(mddev->sysfs_state);\n\treturn 0;\n}\n\n#ifndef MODULE\nstatic void autorun_array(struct mddev *mddev)\n{\n\tstruct md_rdev *rdev;\n\tint err;\n\n\tif (list_empty(&mddev->disks))\n\t\treturn;\n\n\tpr_info(\"md: running: \");\n\n\trdev_for_each(rdev, mddev) {\n\t\tpr_cont(\"<%pg>\", rdev->bdev);\n\t}\n\tpr_cont(\"\\n\");\n\n\terr = do_md_run(mddev);\n\tif (err) {\n\t\tpr_warn(\"md: do_md_run() returned %d\\n\", err);\n\t\tdo_md_stop(mddev, 0, NULL);\n\t}\n}\n\n \nstatic void autorun_devices(int part)\n{\n\tstruct md_rdev *rdev0, *rdev, *tmp;\n\tstruct mddev *mddev;\n\n\tpr_info(\"md: autorun ...\\n\");\n\twhile (!list_empty(&pending_raid_disks)) {\n\t\tint unit;\n\t\tdev_t dev;\n\t\tLIST_HEAD(candidates);\n\t\trdev0 = list_entry(pending_raid_disks.next,\n\t\t\t\t\t struct md_rdev, same_set);\n\n\t\tpr_debug(\"md: considering %pg ...\\n\", rdev0->bdev);\n\t\tINIT_LIST_HEAD(&candidates);\n\t\trdev_for_each_list(rdev, tmp, &pending_raid_disks)\n\t\t\tif (super_90_load(rdev, rdev0, 0) >= 0) {\n\t\t\t\tpr_debug(\"md:  adding %pg ...\\n\",\n\t\t\t\t\t rdev->bdev);\n\t\t\t\tlist_move(&rdev->same_set, &candidates);\n\t\t\t}\n\t\t \n\t\tif (part) {\n\t\t\tdev = MKDEV(mdp_major,\n\t\t\t\t    rdev0->preferred_minor << MdpMinorShift);\n\t\t\tunit = MINOR(dev) >> MdpMinorShift;\n\t\t} else {\n\t\t\tdev = MKDEV(MD_MAJOR, rdev0->preferred_minor);\n\t\t\tunit = MINOR(dev);\n\t\t}\n\t\tif (rdev0->preferred_minor != unit) {\n\t\t\tpr_warn(\"md: unit number in %pg is bad: %d\\n\",\n\t\t\t\trdev0->bdev, rdev0->preferred_minor);\n\t\t\tbreak;\n\t\t}\n\n\t\tmddev = md_alloc(dev, NULL);\n\t\tif (IS_ERR(mddev))\n\t\t\tbreak;\n\n\t\tif (mddev_lock(mddev))\n\t\t\tpr_warn(\"md: %s locked, cannot run\\n\", mdname(mddev));\n\t\telse if (mddev->raid_disks || mddev->major_version\n\t\t\t || !list_empty(&mddev->disks)) {\n\t\t\tpr_warn(\"md: %s already running, cannot run %pg\\n\",\n\t\t\t\tmdname(mddev), rdev0->bdev);\n\t\t\tmddev_unlock(mddev);\n\t\t} else {\n\t\t\tpr_debug(\"md: created %s\\n\", mdname(mddev));\n\t\t\tmddev->persistent = 1;\n\t\t\trdev_for_each_list(rdev, tmp, &candidates) {\n\t\t\t\tlist_del_init(&rdev->same_set);\n\t\t\t\tif (bind_rdev_to_array(rdev, mddev))\n\t\t\t\t\texport_rdev(rdev, mddev);\n\t\t\t}\n\t\t\tautorun_array(mddev);\n\t\t\tmddev_unlock(mddev);\n\t\t}\n\t\t \n\t\trdev_for_each_list(rdev, tmp, &candidates) {\n\t\t\tlist_del_init(&rdev->same_set);\n\t\t\texport_rdev(rdev, mddev);\n\t\t}\n\t\tmddev_put(mddev);\n\t}\n\tpr_info(\"md: ... autorun DONE.\\n\");\n}\n#endif  \n\nstatic int get_version(void __user *arg)\n{\n\tmdu_version_t ver;\n\n\tver.major = MD_MAJOR_VERSION;\n\tver.minor = MD_MINOR_VERSION;\n\tver.patchlevel = MD_PATCHLEVEL_VERSION;\n\n\tif (copy_to_user(arg, &ver, sizeof(ver)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int get_array_info(struct mddev *mddev, void __user *arg)\n{\n\tmdu_array_info_t info;\n\tint nr,working,insync,failed,spare;\n\tstruct md_rdev *rdev;\n\n\tnr = working = insync = failed = spare = 0;\n\trcu_read_lock();\n\trdev_for_each_rcu(rdev, mddev) {\n\t\tnr++;\n\t\tif (test_bit(Faulty, &rdev->flags))\n\t\t\tfailed++;\n\t\telse {\n\t\t\tworking++;\n\t\t\tif (test_bit(In_sync, &rdev->flags))\n\t\t\t\tinsync++;\n\t\t\telse if (test_bit(Journal, &rdev->flags))\n\t\t\t\t \n\t\t\t\t;\n\t\t\telse\n\t\t\t\tspare++;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tinfo.major_version = mddev->major_version;\n\tinfo.minor_version = mddev->minor_version;\n\tinfo.patch_version = MD_PATCHLEVEL_VERSION;\n\tinfo.ctime         = clamp_t(time64_t, mddev->ctime, 0, U32_MAX);\n\tinfo.level         = mddev->level;\n\tinfo.size          = mddev->dev_sectors / 2;\n\tif (info.size != mddev->dev_sectors / 2)  \n\t\tinfo.size = -1;\n\tinfo.nr_disks      = nr;\n\tinfo.raid_disks    = mddev->raid_disks;\n\tinfo.md_minor      = mddev->md_minor;\n\tinfo.not_persistent= !mddev->persistent;\n\n\tinfo.utime         = clamp_t(time64_t, mddev->utime, 0, U32_MAX);\n\tinfo.state         = 0;\n\tif (mddev->in_sync)\n\t\tinfo.state = (1<<MD_SB_CLEAN);\n\tif (mddev->bitmap && mddev->bitmap_info.offset)\n\t\tinfo.state |= (1<<MD_SB_BITMAP_PRESENT);\n\tif (mddev_is_clustered(mddev))\n\t\tinfo.state |= (1<<MD_SB_CLUSTERED);\n\tinfo.active_disks  = insync;\n\tinfo.working_disks = working;\n\tinfo.failed_disks  = failed;\n\tinfo.spare_disks   = spare;\n\n\tinfo.layout        = mddev->layout;\n\tinfo.chunk_size    = mddev->chunk_sectors << 9;\n\n\tif (copy_to_user(arg, &info, sizeof(info)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int get_bitmap_file(struct mddev *mddev, void __user * arg)\n{\n\tmdu_bitmap_file_t *file = NULL;  \n\tchar *ptr;\n\tint err;\n\n\tfile = kzalloc(sizeof(*file), GFP_NOIO);\n\tif (!file)\n\t\treturn -ENOMEM;\n\n\terr = 0;\n\tspin_lock(&mddev->lock);\n\t \n\tif (mddev->bitmap_info.file) {\n\t\tptr = file_path(mddev->bitmap_info.file, file->pathname,\n\t\t\t\tsizeof(file->pathname));\n\t\tif (IS_ERR(ptr))\n\t\t\terr = PTR_ERR(ptr);\n\t\telse\n\t\t\tmemmove(file->pathname, ptr,\n\t\t\t\tsizeof(file->pathname)-(ptr-file->pathname));\n\t}\n\tspin_unlock(&mddev->lock);\n\n\tif (err == 0 &&\n\t    copy_to_user(arg, file, sizeof(*file)))\n\t\terr = -EFAULT;\n\n\tkfree(file);\n\treturn err;\n}\n\nstatic int get_disk_info(struct mddev *mddev, void __user * arg)\n{\n\tmdu_disk_info_t info;\n\tstruct md_rdev *rdev;\n\n\tif (copy_from_user(&info, arg, sizeof(info)))\n\t\treturn -EFAULT;\n\n\trcu_read_lock();\n\trdev = md_find_rdev_nr_rcu(mddev, info.number);\n\tif (rdev) {\n\t\tinfo.major = MAJOR(rdev->bdev->bd_dev);\n\t\tinfo.minor = MINOR(rdev->bdev->bd_dev);\n\t\tinfo.raid_disk = rdev->raid_disk;\n\t\tinfo.state = 0;\n\t\tif (test_bit(Faulty, &rdev->flags))\n\t\t\tinfo.state |= (1<<MD_DISK_FAULTY);\n\t\telse if (test_bit(In_sync, &rdev->flags)) {\n\t\t\tinfo.state |= (1<<MD_DISK_ACTIVE);\n\t\t\tinfo.state |= (1<<MD_DISK_SYNC);\n\t\t}\n\t\tif (test_bit(Journal, &rdev->flags))\n\t\t\tinfo.state |= (1<<MD_DISK_JOURNAL);\n\t\tif (test_bit(WriteMostly, &rdev->flags))\n\t\t\tinfo.state |= (1<<MD_DISK_WRITEMOSTLY);\n\t\tif (test_bit(FailFast, &rdev->flags))\n\t\t\tinfo.state |= (1<<MD_DISK_FAILFAST);\n\t} else {\n\t\tinfo.major = info.minor = 0;\n\t\tinfo.raid_disk = -1;\n\t\tinfo.state = (1<<MD_DISK_REMOVED);\n\t}\n\trcu_read_unlock();\n\n\tif (copy_to_user(arg, &info, sizeof(info)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nint md_add_new_disk(struct mddev *mddev, struct mdu_disk_info_s *info)\n{\n\tstruct md_rdev *rdev;\n\tdev_t dev = MKDEV(info->major,info->minor);\n\n\tif (mddev_is_clustered(mddev) &&\n\t\t!(info->state & ((1 << MD_DISK_CLUSTER_ADD) | (1 << MD_DISK_CANDIDATE)))) {\n\t\tpr_warn(\"%s: Cannot add to clustered mddev.\\n\",\n\t\t\tmdname(mddev));\n\t\treturn -EINVAL;\n\t}\n\n\tif (info->major != MAJOR(dev) || info->minor != MINOR(dev))\n\t\treturn -EOVERFLOW;\n\n\tif (!mddev->raid_disks) {\n\t\tint err;\n\t\t \n\t\trdev = md_import_device(dev, mddev->major_version, mddev->minor_version);\n\t\tif (IS_ERR(rdev)) {\n\t\t\tpr_warn(\"md: md_import_device returned %ld\\n\",\n\t\t\t\tPTR_ERR(rdev));\n\t\t\treturn PTR_ERR(rdev);\n\t\t}\n\t\tif (!list_empty(&mddev->disks)) {\n\t\t\tstruct md_rdev *rdev0\n\t\t\t\t= list_entry(mddev->disks.next,\n\t\t\t\t\t     struct md_rdev, same_set);\n\t\t\terr = super_types[mddev->major_version]\n\t\t\t\t.load_super(rdev, rdev0, mddev->minor_version);\n\t\t\tif (err < 0) {\n\t\t\t\tpr_warn(\"md: %pg has different UUID to %pg\\n\",\n\t\t\t\t\trdev->bdev,\n\t\t\t\t\trdev0->bdev);\n\t\t\t\texport_rdev(rdev, mddev);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t}\n\t\terr = bind_rdev_to_array(rdev, mddev);\n\t\tif (err)\n\t\t\texport_rdev(rdev, mddev);\n\t\treturn err;\n\t}\n\n\t \n\tif (mddev->pers) {\n\t\tint err;\n\t\tif (!mddev->pers->hot_add_disk) {\n\t\t\tpr_warn(\"%s: personality does not support diskops!\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (mddev->persistent)\n\t\t\trdev = md_import_device(dev, mddev->major_version,\n\t\t\t\t\t\tmddev->minor_version);\n\t\telse\n\t\t\trdev = md_import_device(dev, -1, -1);\n\t\tif (IS_ERR(rdev)) {\n\t\t\tpr_warn(\"md: md_import_device returned %ld\\n\",\n\t\t\t\tPTR_ERR(rdev));\n\t\t\treturn PTR_ERR(rdev);\n\t\t}\n\t\t \n\t\tif (!mddev->persistent) {\n\t\t\tif (info->state & (1<<MD_DISK_SYNC)  &&\n\t\t\t    info->raid_disk < mddev->raid_disks) {\n\t\t\t\trdev->raid_disk = info->raid_disk;\n\t\t\t\tclear_bit(Bitmap_sync, &rdev->flags);\n\t\t\t} else\n\t\t\t\trdev->raid_disk = -1;\n\t\t\trdev->saved_raid_disk = rdev->raid_disk;\n\t\t} else\n\t\t\tsuper_types[mddev->major_version].\n\t\t\t\tvalidate_super(mddev, rdev);\n\t\tif ((info->state & (1<<MD_DISK_SYNC)) &&\n\t\t     rdev->raid_disk != info->raid_disk) {\n\t\t\t \n\t\t\texport_rdev(rdev, mddev);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tclear_bit(In_sync, &rdev->flags);  \n\t\tif (info->state & (1<<MD_DISK_WRITEMOSTLY))\n\t\t\tset_bit(WriteMostly, &rdev->flags);\n\t\telse\n\t\t\tclear_bit(WriteMostly, &rdev->flags);\n\t\tif (info->state & (1<<MD_DISK_FAILFAST))\n\t\t\tset_bit(FailFast, &rdev->flags);\n\t\telse\n\t\t\tclear_bit(FailFast, &rdev->flags);\n\n\t\tif (info->state & (1<<MD_DISK_JOURNAL)) {\n\t\t\tstruct md_rdev *rdev2;\n\t\t\tbool has_journal = false;\n\n\t\t\t \n\t\t\trdev_for_each(rdev2, mddev) {\n\t\t\t\tif (test_bit(Journal, &rdev2->flags)) {\n\t\t\t\t\thas_journal = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (has_journal || mddev->bitmap) {\n\t\t\t\texport_rdev(rdev, mddev);\n\t\t\t\treturn -EBUSY;\n\t\t\t}\n\t\t\tset_bit(Journal, &rdev->flags);\n\t\t}\n\t\t \n\t\tif (mddev_is_clustered(mddev)) {\n\t\t\tif (info->state & (1 << MD_DISK_CANDIDATE))\n\t\t\t\tset_bit(Candidate, &rdev->flags);\n\t\t\telse if (info->state & (1 << MD_DISK_CLUSTER_ADD)) {\n\t\t\t\t \n\t\t\t\terr = md_cluster_ops->add_new_disk(mddev, rdev);\n\t\t\t\tif (err) {\n\t\t\t\t\texport_rdev(rdev, mddev);\n\t\t\t\t\treturn err;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\trdev->raid_disk = -1;\n\t\terr = bind_rdev_to_array(rdev, mddev);\n\n\t\tif (err)\n\t\t\texport_rdev(rdev, mddev);\n\n\t\tif (mddev_is_clustered(mddev)) {\n\t\t\tif (info->state & (1 << MD_DISK_CANDIDATE)) {\n\t\t\t\tif (!err) {\n\t\t\t\t\terr = md_cluster_ops->new_disk_ack(mddev,\n\t\t\t\t\t\terr == 0);\n\t\t\t\t\tif (err)\n\t\t\t\t\t\tmd_kick_rdev_from_array(rdev);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (err)\n\t\t\t\t\tmd_cluster_ops->add_new_disk_cancel(mddev);\n\t\t\t\telse\n\t\t\t\t\terr = add_bound_rdev(rdev);\n\t\t\t}\n\n\t\t} else if (!err)\n\t\t\terr = add_bound_rdev(rdev);\n\n\t\treturn err;\n\t}\n\n\t \n\tif (mddev->major_version != 0) {\n\t\tpr_warn(\"%s: ADD_NEW_DISK not supported\\n\", mdname(mddev));\n\t\treturn -EINVAL;\n\t}\n\n\tif (!(info->state & (1<<MD_DISK_FAULTY))) {\n\t\tint err;\n\t\trdev = md_import_device(dev, -1, 0);\n\t\tif (IS_ERR(rdev)) {\n\t\t\tpr_warn(\"md: error, md_import_device() returned %ld\\n\",\n\t\t\t\tPTR_ERR(rdev));\n\t\t\treturn PTR_ERR(rdev);\n\t\t}\n\t\trdev->desc_nr = info->number;\n\t\tif (info->raid_disk < mddev->raid_disks)\n\t\t\trdev->raid_disk = info->raid_disk;\n\t\telse\n\t\t\trdev->raid_disk = -1;\n\n\t\tif (rdev->raid_disk < mddev->raid_disks)\n\t\t\tif (info->state & (1<<MD_DISK_SYNC))\n\t\t\t\tset_bit(In_sync, &rdev->flags);\n\n\t\tif (info->state & (1<<MD_DISK_WRITEMOSTLY))\n\t\t\tset_bit(WriteMostly, &rdev->flags);\n\t\tif (info->state & (1<<MD_DISK_FAILFAST))\n\t\t\tset_bit(FailFast, &rdev->flags);\n\n\t\tif (!mddev->persistent) {\n\t\t\tpr_debug(\"md: nonpersistent superblock ...\\n\");\n\t\t\trdev->sb_start = bdev_nr_sectors(rdev->bdev);\n\t\t} else\n\t\t\trdev->sb_start = calc_dev_sboffset(rdev);\n\t\trdev->sectors = rdev->sb_start;\n\n\t\terr = bind_rdev_to_array(rdev, mddev);\n\t\tif (err) {\n\t\t\texport_rdev(rdev, mddev);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int hot_remove_disk(struct mddev *mddev, dev_t dev)\n{\n\tstruct md_rdev *rdev;\n\n\tif (!mddev->pers)\n\t\treturn -ENODEV;\n\n\trdev = find_rdev(mddev, dev);\n\tif (!rdev)\n\t\treturn -ENXIO;\n\n\tif (rdev->raid_disk < 0)\n\t\tgoto kick_rdev;\n\n\tclear_bit(Blocked, &rdev->flags);\n\tremove_and_add_spares(mddev, rdev);\n\n\tif (rdev->raid_disk >= 0)\n\t\tgoto busy;\n\nkick_rdev:\n\tif (mddev_is_clustered(mddev)) {\n\t\tif (md_cluster_ops->remove_disk(mddev, rdev))\n\t\t\tgoto busy;\n\t}\n\n\tmd_kick_rdev_from_array(rdev);\n\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\tif (mddev->thread)\n\t\tmd_wakeup_thread(mddev->thread);\n\telse\n\t\tmd_update_sb(mddev, 1);\n\tmd_new_event();\n\n\treturn 0;\nbusy:\n\tpr_debug(\"md: cannot remove active disk %pg from %s ...\\n\",\n\t\t rdev->bdev, mdname(mddev));\n\treturn -EBUSY;\n}\n\nstatic int hot_add_disk(struct mddev *mddev, dev_t dev)\n{\n\tint err;\n\tstruct md_rdev *rdev;\n\n\tif (!mddev->pers)\n\t\treturn -ENODEV;\n\n\tif (mddev->major_version != 0) {\n\t\tpr_warn(\"%s: HOT_ADD may only be used with version-0 superblocks.\\n\",\n\t\t\tmdname(mddev));\n\t\treturn -EINVAL;\n\t}\n\tif (!mddev->pers->hot_add_disk) {\n\t\tpr_warn(\"%s: personality does not support diskops!\\n\",\n\t\t\tmdname(mddev));\n\t\treturn -EINVAL;\n\t}\n\n\trdev = md_import_device(dev, -1, 0);\n\tif (IS_ERR(rdev)) {\n\t\tpr_warn(\"md: error, md_import_device() returned %ld\\n\",\n\t\t\tPTR_ERR(rdev));\n\t\treturn -EINVAL;\n\t}\n\n\tif (mddev->persistent)\n\t\trdev->sb_start = calc_dev_sboffset(rdev);\n\telse\n\t\trdev->sb_start = bdev_nr_sectors(rdev->bdev);\n\n\trdev->sectors = rdev->sb_start;\n\n\tif (test_bit(Faulty, &rdev->flags)) {\n\t\tpr_warn(\"md: can not hot-add faulty %pg disk to %s!\\n\",\n\t\t\trdev->bdev, mdname(mddev));\n\t\terr = -EINVAL;\n\t\tgoto abort_export;\n\t}\n\n\tclear_bit(In_sync, &rdev->flags);\n\trdev->desc_nr = -1;\n\trdev->saved_raid_disk = -1;\n\terr = bind_rdev_to_array(rdev, mddev);\n\tif (err)\n\t\tgoto abort_export;\n\n\t \n\n\trdev->raid_disk = -1;\n\n\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\tif (!mddev->thread)\n\t\tmd_update_sb(mddev, 1);\n\t \n\tif (!bdev_nowait(rdev->bdev)) {\n\t\tpr_info(\"%s: Disabling nowait because %pg does not support nowait\\n\",\n\t\t\tmdname(mddev), rdev->bdev);\n\t\tblk_queue_flag_clear(QUEUE_FLAG_NOWAIT, mddev->queue);\n\t}\n\t \n\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\tmd_wakeup_thread(mddev->thread);\n\tmd_new_event();\n\treturn 0;\n\nabort_export:\n\texport_rdev(rdev, mddev);\n\treturn err;\n}\n\nstatic int set_bitmap_file(struct mddev *mddev, int fd)\n{\n\tint err = 0;\n\n\tif (mddev->pers) {\n\t\tif (!mddev->pers->quiesce || !mddev->thread)\n\t\t\treturn -EBUSY;\n\t\tif (mddev->recovery || mddev->sync_thread)\n\t\t\treturn -EBUSY;\n\t\t \n\t}\n\n\tif (fd >= 0) {\n\t\tstruct inode *inode;\n\t\tstruct file *f;\n\n\t\tif (mddev->bitmap || mddev->bitmap_info.file)\n\t\t\treturn -EEXIST;  \n\n\t\tif (!IS_ENABLED(CONFIG_MD_BITMAP_FILE)) {\n\t\t\tpr_warn(\"%s: bitmap files not supported by this kernel\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tpr_warn(\"%s: using deprecated bitmap file support\\n\",\n\t\t\tmdname(mddev));\n\n\t\tf = fget(fd);\n\n\t\tif (f == NULL) {\n\t\t\tpr_warn(\"%s: error: failed to get bitmap file\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\treturn -EBADF;\n\t\t}\n\n\t\tinode = f->f_mapping->host;\n\t\tif (!S_ISREG(inode->i_mode)) {\n\t\t\tpr_warn(\"%s: error: bitmap file must be a regular file\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\terr = -EBADF;\n\t\t} else if (!(f->f_mode & FMODE_WRITE)) {\n\t\t\tpr_warn(\"%s: error: bitmap file must open for write\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\terr = -EBADF;\n\t\t} else if (atomic_read(&inode->i_writecount) != 1) {\n\t\t\tpr_warn(\"%s: error: bitmap file is already in use\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\terr = -EBUSY;\n\t\t}\n\t\tif (err) {\n\t\t\tfput(f);\n\t\t\treturn err;\n\t\t}\n\t\tmddev->bitmap_info.file = f;\n\t\tmddev->bitmap_info.offset = 0;  \n\t} else if (mddev->bitmap == NULL)\n\t\treturn -ENOENT;  \n\terr = 0;\n\tif (mddev->pers) {\n\t\tif (fd >= 0) {\n\t\t\tstruct bitmap *bitmap;\n\n\t\t\tbitmap = md_bitmap_create(mddev, -1);\n\t\t\tmddev_suspend(mddev);\n\t\t\tif (!IS_ERR(bitmap)) {\n\t\t\t\tmddev->bitmap = bitmap;\n\t\t\t\terr = md_bitmap_load(mddev);\n\t\t\t} else\n\t\t\t\terr = PTR_ERR(bitmap);\n\t\t\tif (err) {\n\t\t\t\tmd_bitmap_destroy(mddev);\n\t\t\t\tfd = -1;\n\t\t\t}\n\t\t\tmddev_resume(mddev);\n\t\t} else if (fd < 0) {\n\t\t\tmddev_suspend(mddev);\n\t\t\tmd_bitmap_destroy(mddev);\n\t\t\tmddev_resume(mddev);\n\t\t}\n\t}\n\tif (fd < 0) {\n\t\tstruct file *f = mddev->bitmap_info.file;\n\t\tif (f) {\n\t\t\tspin_lock(&mddev->lock);\n\t\t\tmddev->bitmap_info.file = NULL;\n\t\t\tspin_unlock(&mddev->lock);\n\t\t\tfput(f);\n\t\t}\n\t}\n\n\treturn err;\n}\n\n \nint md_set_array_info(struct mddev *mddev, struct mdu_array_info_s *info)\n{\n\tif (info->raid_disks == 0) {\n\t\t \n\t\tif (info->major_version < 0 ||\n\t\t    info->major_version >= ARRAY_SIZE(super_types) ||\n\t\t    super_types[info->major_version].name == NULL) {\n\t\t\t \n\t\t\tpr_warn(\"md: superblock version %d not known\\n\",\n\t\t\t\tinfo->major_version);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmddev->major_version = info->major_version;\n\t\tmddev->minor_version = info->minor_version;\n\t\tmddev->patch_version = info->patch_version;\n\t\tmddev->persistent = !info->not_persistent;\n\t\t \n\t\tmddev->ctime         = ktime_get_real_seconds();\n\t\treturn 0;\n\t}\n\tmddev->major_version = MD_MAJOR_VERSION;\n\tmddev->minor_version = MD_MINOR_VERSION;\n\tmddev->patch_version = MD_PATCHLEVEL_VERSION;\n\tmddev->ctime         = ktime_get_real_seconds();\n\n\tmddev->level         = info->level;\n\tmddev->clevel[0]     = 0;\n\tmddev->dev_sectors   = 2 * (sector_t)info->size;\n\tmddev->raid_disks    = info->raid_disks;\n\t \n\tif (info->state & (1<<MD_SB_CLEAN))\n\t\tmddev->recovery_cp = MaxSector;\n\telse\n\t\tmddev->recovery_cp = 0;\n\tmddev->persistent    = ! info->not_persistent;\n\tmddev->external\t     = 0;\n\n\tmddev->layout        = info->layout;\n\tif (mddev->level == 0)\n\t\t \n\t\tmddev->layout = -1;\n\tmddev->chunk_sectors = info->chunk_size >> 9;\n\n\tif (mddev->persistent) {\n\t\tmddev->max_disks = MD_SB_DISKS;\n\t\tmddev->flags = 0;\n\t\tmddev->sb_flags = 0;\n\t}\n\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\n\tmddev->bitmap_info.default_offset = MD_SB_BYTES >> 9;\n\tmddev->bitmap_info.default_space = 64*2 - (MD_SB_BYTES >> 9);\n\tmddev->bitmap_info.offset = 0;\n\n\tmddev->reshape_position = MaxSector;\n\n\t \n\tget_random_bytes(mddev->uuid, 16);\n\n\tmddev->new_level = mddev->level;\n\tmddev->new_chunk_sectors = mddev->chunk_sectors;\n\tmddev->new_layout = mddev->layout;\n\tmddev->delta_disks = 0;\n\tmddev->reshape_backwards = 0;\n\n\treturn 0;\n}\n\nvoid md_set_array_sectors(struct mddev *mddev, sector_t array_sectors)\n{\n\tlockdep_assert_held(&mddev->reconfig_mutex);\n\n\tif (mddev->external_size)\n\t\treturn;\n\n\tmddev->array_sectors = array_sectors;\n}\nEXPORT_SYMBOL(md_set_array_sectors);\n\nstatic int update_size(struct mddev *mddev, sector_t num_sectors)\n{\n\tstruct md_rdev *rdev;\n\tint rv;\n\tint fit = (num_sectors == 0);\n\tsector_t old_dev_sectors = mddev->dev_sectors;\n\n\tif (mddev->pers->resize == NULL)\n\t\treturn -EINVAL;\n\t \n\tif (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery) ||\n\t    mddev->sync_thread)\n\t\treturn -EBUSY;\n\tif (!md_is_rdwr(mddev))\n\t\treturn -EROFS;\n\n\trdev_for_each(rdev, mddev) {\n\t\tsector_t avail = rdev->sectors;\n\n\t\tif (fit && (num_sectors == 0 || num_sectors > avail))\n\t\t\tnum_sectors = avail;\n\t\tif (avail < num_sectors)\n\t\t\treturn -ENOSPC;\n\t}\n\trv = mddev->pers->resize(mddev, num_sectors);\n\tif (!rv) {\n\t\tif (mddev_is_clustered(mddev))\n\t\t\tmd_cluster_ops->update_size(mddev, old_dev_sectors);\n\t\telse if (mddev->queue) {\n\t\t\tset_capacity_and_notify(mddev->gendisk,\n\t\t\t\t\t\tmddev->array_sectors);\n\t\t}\n\t}\n\treturn rv;\n}\n\nstatic int update_raid_disks(struct mddev *mddev, int raid_disks)\n{\n\tint rv;\n\tstruct md_rdev *rdev;\n\t \n\tif (mddev->pers->check_reshape == NULL)\n\t\treturn -EINVAL;\n\tif (!md_is_rdwr(mddev))\n\t\treturn -EROFS;\n\tif (raid_disks <= 0 ||\n\t    (mddev->max_disks && raid_disks >= mddev->max_disks))\n\t\treturn -EINVAL;\n\tif (mddev->sync_thread ||\n\t    test_bit(MD_RECOVERY_RUNNING, &mddev->recovery) ||\n\t    test_bit(MD_RESYNCING_REMOTE, &mddev->recovery) ||\n\t    mddev->reshape_position != MaxSector)\n\t\treturn -EBUSY;\n\n\trdev_for_each(rdev, mddev) {\n\t\tif (mddev->raid_disks < raid_disks &&\n\t\t    rdev->data_offset < rdev->new_data_offset)\n\t\t\treturn -EINVAL;\n\t\tif (mddev->raid_disks > raid_disks &&\n\t\t    rdev->data_offset > rdev->new_data_offset)\n\t\t\treturn -EINVAL;\n\t}\n\n\tmddev->delta_disks = raid_disks - mddev->raid_disks;\n\tif (mddev->delta_disks < 0)\n\t\tmddev->reshape_backwards = 1;\n\telse if (mddev->delta_disks > 0)\n\t\tmddev->reshape_backwards = 0;\n\n\trv = mddev->pers->check_reshape(mddev);\n\tif (rv < 0) {\n\t\tmddev->delta_disks = 0;\n\t\tmddev->reshape_backwards = 0;\n\t}\n\treturn rv;\n}\n\n \nstatic int update_array_info(struct mddev *mddev, mdu_array_info_t *info)\n{\n\tint rv = 0;\n\tint cnt = 0;\n\tint state = 0;\n\n\t \n\tif (mddev->bitmap && mddev->bitmap_info.offset)\n\t\tstate |= (1 << MD_SB_BITMAP_PRESENT);\n\n\tif (mddev->major_version != info->major_version ||\n\t    mddev->minor_version != info->minor_version ||\n \n\t    mddev->ctime         != info->ctime         ||\n\t    mddev->level         != info->level         ||\n \n\t    mddev->persistent\t != !info->not_persistent ||\n\t    mddev->chunk_sectors != info->chunk_size >> 9 ||\n\t     \n\t    ((state^info->state) & 0xfffffe00)\n\t\t)\n\t\treturn -EINVAL;\n\t \n\tif (info->size >= 0 && mddev->dev_sectors / 2 != info->size)\n\t\tcnt++;\n\tif (mddev->raid_disks != info->raid_disks)\n\t\tcnt++;\n\tif (mddev->layout != info->layout)\n\t\tcnt++;\n\tif ((state ^ info->state) & (1<<MD_SB_BITMAP_PRESENT))\n\t\tcnt++;\n\tif (cnt == 0)\n\t\treturn 0;\n\tif (cnt > 1)\n\t\treturn -EINVAL;\n\n\tif (mddev->layout != info->layout) {\n\t\t \n\t\tif (mddev->pers->check_reshape == NULL)\n\t\t\treturn -EINVAL;\n\t\telse {\n\t\t\tmddev->new_layout = info->layout;\n\t\t\trv = mddev->pers->check_reshape(mddev);\n\t\t\tif (rv)\n\t\t\t\tmddev->new_layout = mddev->layout;\n\t\t\treturn rv;\n\t\t}\n\t}\n\tif (info->size >= 0 && mddev->dev_sectors / 2 != info->size)\n\t\trv = update_size(mddev, (sector_t)info->size * 2);\n\n\tif (mddev->raid_disks    != info->raid_disks)\n\t\trv = update_raid_disks(mddev, info->raid_disks);\n\n\tif ((state ^ info->state) & (1<<MD_SB_BITMAP_PRESENT)) {\n\t\tif (mddev->pers->quiesce == NULL || mddev->thread == NULL) {\n\t\t\trv = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tif (mddev->recovery || mddev->sync_thread) {\n\t\t\trv = -EBUSY;\n\t\t\tgoto err;\n\t\t}\n\t\tif (info->state & (1<<MD_SB_BITMAP_PRESENT)) {\n\t\t\tstruct bitmap *bitmap;\n\t\t\t \n\t\t\tif (mddev->bitmap) {\n\t\t\t\trv = -EEXIST;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tif (mddev->bitmap_info.default_offset == 0) {\n\t\t\t\trv = -EINVAL;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tmddev->bitmap_info.offset =\n\t\t\t\tmddev->bitmap_info.default_offset;\n\t\t\tmddev->bitmap_info.space =\n\t\t\t\tmddev->bitmap_info.default_space;\n\t\t\tbitmap = md_bitmap_create(mddev, -1);\n\t\t\tmddev_suspend(mddev);\n\t\t\tif (!IS_ERR(bitmap)) {\n\t\t\t\tmddev->bitmap = bitmap;\n\t\t\t\trv = md_bitmap_load(mddev);\n\t\t\t} else\n\t\t\t\trv = PTR_ERR(bitmap);\n\t\t\tif (rv)\n\t\t\t\tmd_bitmap_destroy(mddev);\n\t\t\tmddev_resume(mddev);\n\t\t} else {\n\t\t\t \n\t\t\tif (!mddev->bitmap) {\n\t\t\t\trv = -ENOENT;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tif (mddev->bitmap->storage.file) {\n\t\t\t\trv = -EINVAL;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tif (mddev->bitmap_info.nodes) {\n\t\t\t\t \n\t\t\t\tif (md_cluster_ops->lock_all_bitmaps(mddev) <= 0) {\n\t\t\t\t\tpr_warn(\"md: can't change bitmap to none since the array is in use by more than one node\\n\");\n\t\t\t\t\trv = -EPERM;\n\t\t\t\t\tmd_cluster_ops->unlock_all_bitmaps(mddev);\n\t\t\t\t\tgoto err;\n\t\t\t\t}\n\n\t\t\t\tmddev->bitmap_info.nodes = 0;\n\t\t\t\tmd_cluster_ops->leave(mddev);\n\t\t\t\tmodule_put(md_cluster_mod);\n\t\t\t\tmddev->safemode_delay = DEFAULT_SAFEMODE_DELAY;\n\t\t\t}\n\t\t\tmddev_suspend(mddev);\n\t\t\tmd_bitmap_destroy(mddev);\n\t\t\tmddev_resume(mddev);\n\t\t\tmddev->bitmap_info.offset = 0;\n\t\t}\n\t}\n\tmd_update_sb(mddev, 1);\n\treturn rv;\nerr:\n\treturn rv;\n}\n\nstatic int set_disk_faulty(struct mddev *mddev, dev_t dev)\n{\n\tstruct md_rdev *rdev;\n\tint err = 0;\n\n\tif (mddev->pers == NULL)\n\t\treturn -ENODEV;\n\n\trcu_read_lock();\n\trdev = md_find_rdev_rcu(mddev, dev);\n\tif (!rdev)\n\t\terr =  -ENODEV;\n\telse {\n\t\tmd_error(mddev, rdev);\n\t\tif (test_bit(MD_BROKEN, &mddev->flags))\n\t\t\terr = -EBUSY;\n\t}\n\trcu_read_unlock();\n\treturn err;\n}\n\n \nstatic int md_getgeo(struct block_device *bdev, struct hd_geometry *geo)\n{\n\tstruct mddev *mddev = bdev->bd_disk->private_data;\n\n\tgeo->heads = 2;\n\tgeo->sectors = 4;\n\tgeo->cylinders = mddev->array_sectors / 8;\n\treturn 0;\n}\n\nstatic inline bool md_ioctl_valid(unsigned int cmd)\n{\n\tswitch (cmd) {\n\tcase ADD_NEW_DISK:\n\tcase GET_ARRAY_INFO:\n\tcase GET_BITMAP_FILE:\n\tcase GET_DISK_INFO:\n\tcase HOT_ADD_DISK:\n\tcase HOT_REMOVE_DISK:\n\tcase RAID_VERSION:\n\tcase RESTART_ARRAY_RW:\n\tcase RUN_ARRAY:\n\tcase SET_ARRAY_INFO:\n\tcase SET_BITMAP_FILE:\n\tcase SET_DISK_FAULTY:\n\tcase STOP_ARRAY:\n\tcase STOP_ARRAY_RO:\n\tcase CLUSTERED_DISK_NACK:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic int __md_set_array_info(struct mddev *mddev, void __user *argp)\n{\n\tmdu_array_info_t info;\n\tint err;\n\n\tif (!argp)\n\t\tmemset(&info, 0, sizeof(info));\n\telse if (copy_from_user(&info, argp, sizeof(info)))\n\t\treturn -EFAULT;\n\n\tif (mddev->pers) {\n\t\terr = update_array_info(mddev, &info);\n\t\tif (err)\n\t\t\tpr_warn(\"md: couldn't update array info. %d\\n\", err);\n\t\treturn err;\n\t}\n\n\tif (!list_empty(&mddev->disks)) {\n\t\tpr_warn(\"md: array %s already has disks!\\n\", mdname(mddev));\n\t\treturn -EBUSY;\n\t}\n\n\tif (mddev->raid_disks) {\n\t\tpr_warn(\"md: array %s already initialised!\\n\", mdname(mddev));\n\t\treturn -EBUSY;\n\t}\n\n\terr = md_set_array_info(mddev, &info);\n\tif (err)\n\t\tpr_warn(\"md: couldn't set array info. %d\\n\", err);\n\n\treturn err;\n}\n\nstatic int md_ioctl(struct block_device *bdev, blk_mode_t mode,\n\t\t\tunsigned int cmd, unsigned long arg)\n{\n\tint err = 0;\n\tvoid __user *argp = (void __user *)arg;\n\tstruct mddev *mddev = NULL;\n\tbool did_set_md_closing = false;\n\n\tif (!md_ioctl_valid(cmd))\n\t\treturn -ENOTTY;\n\n\tswitch (cmd) {\n\tcase RAID_VERSION:\n\tcase GET_ARRAY_INFO:\n\tcase GET_DISK_INFO:\n\t\tbreak;\n\tdefault:\n\t\tif (!capable(CAP_SYS_ADMIN))\n\t\t\treturn -EACCES;\n\t}\n\n\t \n\tswitch (cmd) {\n\tcase RAID_VERSION:\n\t\terr = get_version(argp);\n\t\tgoto out;\n\tdefault:;\n\t}\n\n\t \n\n\tmddev = bdev->bd_disk->private_data;\n\n\tif (!mddev) {\n\t\tBUG();\n\t\tgoto out;\n\t}\n\n\t \n\tswitch (cmd) {\n\tcase GET_ARRAY_INFO:\n\t\tif (!mddev->raid_disks && !mddev->external)\n\t\t\terr = -ENODEV;\n\t\telse\n\t\t\terr = get_array_info(mddev, argp);\n\t\tgoto out;\n\n\tcase GET_DISK_INFO:\n\t\tif (!mddev->raid_disks && !mddev->external)\n\t\t\terr = -ENODEV;\n\t\telse\n\t\t\terr = get_disk_info(mddev, argp);\n\t\tgoto out;\n\n\tcase SET_DISK_FAULTY:\n\t\terr = set_disk_faulty(mddev, new_decode_dev(arg));\n\t\tgoto out;\n\n\tcase GET_BITMAP_FILE:\n\t\terr = get_bitmap_file(mddev, argp);\n\t\tgoto out;\n\n\t}\n\n\tif (cmd == HOT_REMOVE_DISK)\n\t\t \n\t\twait_event_interruptible_timeout(mddev->sb_wait,\n\t\t\t\t\t\t !test_bit(MD_RECOVERY_NEEDED,\n\t\t\t\t\t\t\t   &mddev->recovery),\n\t\t\t\t\t\t msecs_to_jiffies(5000));\n\tif (cmd == STOP_ARRAY || cmd == STOP_ARRAY_RO) {\n\t\t \n\t\tmutex_lock(&mddev->open_mutex);\n\t\tif (mddev->pers && atomic_read(&mddev->openers) > 1) {\n\t\t\tmutex_unlock(&mddev->open_mutex);\n\t\t\terr = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\t\tif (test_and_set_bit(MD_CLOSING, &mddev->flags)) {\n\t\t\tmutex_unlock(&mddev->open_mutex);\n\t\t\terr = -EBUSY;\n\t\t\tgoto out;\n\t\t}\n\t\tdid_set_md_closing = true;\n\t\tmutex_unlock(&mddev->open_mutex);\n\t\tsync_blockdev(bdev);\n\t}\n\terr = mddev_lock(mddev);\n\tif (err) {\n\t\tpr_debug(\"md: ioctl lock interrupted, reason %d, cmd %d\\n\",\n\t\t\t err, cmd);\n\t\tgoto out;\n\t}\n\n\tif (cmd == SET_ARRAY_INFO) {\n\t\terr = __md_set_array_info(mddev, argp);\n\t\tgoto unlock;\n\t}\n\n\t \n\t \n\tif ((!mddev->raid_disks && !mddev->external)\n\t    && cmd != ADD_NEW_DISK && cmd != STOP_ARRAY\n\t    && cmd != RUN_ARRAY && cmd != SET_BITMAP_FILE\n\t    && cmd != GET_BITMAP_FILE) {\n\t\terr = -ENODEV;\n\t\tgoto unlock;\n\t}\n\n\t \n\tswitch (cmd) {\n\tcase RESTART_ARRAY_RW:\n\t\terr = restart_array(mddev);\n\t\tgoto unlock;\n\n\tcase STOP_ARRAY:\n\t\terr = do_md_stop(mddev, 0, bdev);\n\t\tgoto unlock;\n\n\tcase STOP_ARRAY_RO:\n\t\terr = md_set_readonly(mddev, bdev);\n\t\tgoto unlock;\n\n\tcase HOT_REMOVE_DISK:\n\t\terr = hot_remove_disk(mddev, new_decode_dev(arg));\n\t\tgoto unlock;\n\n\tcase ADD_NEW_DISK:\n\t\t \n\t\tif (mddev->pers) {\n\t\t\tmdu_disk_info_t info;\n\t\t\tif (copy_from_user(&info, argp, sizeof(info)))\n\t\t\t\terr = -EFAULT;\n\t\t\telse if (!(info.state & (1<<MD_DISK_SYNC)))\n\t\t\t\t \n\t\t\t\tbreak;\n\t\t\telse\n\t\t\t\terr = md_add_new_disk(mddev, &info);\n\t\t\tgoto unlock;\n\t\t}\n\t\tbreak;\n\t}\n\n\t \n\tif (!md_is_rdwr(mddev) && mddev->pers) {\n\t\tif (mddev->ro != MD_AUTO_READ) {\n\t\t\terr = -EROFS;\n\t\t\tgoto unlock;\n\t\t}\n\t\tmddev->ro = MD_RDWR;\n\t\tsysfs_notify_dirent_safe(mddev->sysfs_state);\n\t\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\t\t \n\t\t \n\t\tif (test_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags)) {\n\t\t\tmddev_unlock(mddev);\n\t\t\twait_event(mddev->sb_wait,\n\t\t\t\t   !test_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags) &&\n\t\t\t\t   !test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags));\n\t\t\tmddev_lock_nointr(mddev);\n\t\t}\n\t}\n\n\tswitch (cmd) {\n\tcase ADD_NEW_DISK:\n\t{\n\t\tmdu_disk_info_t info;\n\t\tif (copy_from_user(&info, argp, sizeof(info)))\n\t\t\terr = -EFAULT;\n\t\telse\n\t\t\terr = md_add_new_disk(mddev, &info);\n\t\tgoto unlock;\n\t}\n\n\tcase CLUSTERED_DISK_NACK:\n\t\tif (mddev_is_clustered(mddev))\n\t\t\tmd_cluster_ops->new_disk_ack(mddev, false);\n\t\telse\n\t\t\terr = -EINVAL;\n\t\tgoto unlock;\n\n\tcase HOT_ADD_DISK:\n\t\terr = hot_add_disk(mddev, new_decode_dev(arg));\n\t\tgoto unlock;\n\n\tcase RUN_ARRAY:\n\t\terr = do_md_run(mddev);\n\t\tgoto unlock;\n\n\tcase SET_BITMAP_FILE:\n\t\terr = set_bitmap_file(mddev, (int)arg);\n\t\tgoto unlock;\n\n\tdefault:\n\t\terr = -EINVAL;\n\t\tgoto unlock;\n\t}\n\nunlock:\n\tif (mddev->hold_active == UNTIL_IOCTL &&\n\t    err != -EINVAL)\n\t\tmddev->hold_active = 0;\n\tmddev_unlock(mddev);\nout:\n\tif(did_set_md_closing)\n\t\tclear_bit(MD_CLOSING, &mddev->flags);\n\treturn err;\n}\n#ifdef CONFIG_COMPAT\nstatic int md_compat_ioctl(struct block_device *bdev, blk_mode_t mode,\n\t\t    unsigned int cmd, unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase HOT_REMOVE_DISK:\n\tcase HOT_ADD_DISK:\n\tcase SET_DISK_FAULTY:\n\tcase SET_BITMAP_FILE:\n\t\t \n\t\tbreak;\n\tdefault:\n\t\targ = (unsigned long)compat_ptr(arg);\n\t\tbreak;\n\t}\n\n\treturn md_ioctl(bdev, mode, cmd, arg);\n}\n#endif  \n\nstatic int md_set_read_only(struct block_device *bdev, bool ro)\n{\n\tstruct mddev *mddev = bdev->bd_disk->private_data;\n\tint err;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\n\tif (!mddev->raid_disks && !mddev->external) {\n\t\terr = -ENODEV;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tif (!ro && mddev->ro == MD_RDONLY && mddev->pers) {\n\t\terr = restart_array(mddev);\n\t\tif (err)\n\t\t\tgoto out_unlock;\n\t\tmddev->ro = MD_AUTO_READ;\n\t}\n\nout_unlock:\n\tmddev_unlock(mddev);\n\treturn err;\n}\n\nstatic int md_open(struct gendisk *disk, blk_mode_t mode)\n{\n\tstruct mddev *mddev;\n\tint err;\n\n\tspin_lock(&all_mddevs_lock);\n\tmddev = mddev_get(disk->private_data);\n\tspin_unlock(&all_mddevs_lock);\n\tif (!mddev)\n\t\treturn -ENODEV;\n\n\terr = mutex_lock_interruptible(&mddev->open_mutex);\n\tif (err)\n\t\tgoto out;\n\n\terr = -ENODEV;\n\tif (test_bit(MD_CLOSING, &mddev->flags))\n\t\tgoto out_unlock;\n\n\tatomic_inc(&mddev->openers);\n\tmutex_unlock(&mddev->open_mutex);\n\n\tdisk_check_media_change(disk);\n\treturn 0;\n\nout_unlock:\n\tmutex_unlock(&mddev->open_mutex);\nout:\n\tmddev_put(mddev);\n\treturn err;\n}\n\nstatic void md_release(struct gendisk *disk)\n{\n\tstruct mddev *mddev = disk->private_data;\n\n\tBUG_ON(!mddev);\n\tatomic_dec(&mddev->openers);\n\tmddev_put(mddev);\n}\n\nstatic unsigned int md_check_events(struct gendisk *disk, unsigned int clearing)\n{\n\tstruct mddev *mddev = disk->private_data;\n\tunsigned int ret = 0;\n\n\tif (mddev->changed)\n\t\tret = DISK_EVENT_MEDIA_CHANGE;\n\tmddev->changed = 0;\n\treturn ret;\n}\n\nstatic void md_free_disk(struct gendisk *disk)\n{\n\tstruct mddev *mddev = disk->private_data;\n\n\tpercpu_ref_exit(&mddev->writes_pending);\n\tmddev_free(mddev);\n}\n\nconst struct block_device_operations md_fops =\n{\n\t.owner\t\t= THIS_MODULE,\n\t.submit_bio\t= md_submit_bio,\n\t.open\t\t= md_open,\n\t.release\t= md_release,\n\t.ioctl\t\t= md_ioctl,\n#ifdef CONFIG_COMPAT\n\t.compat_ioctl\t= md_compat_ioctl,\n#endif\n\t.getgeo\t\t= md_getgeo,\n\t.check_events\t= md_check_events,\n\t.set_read_only\t= md_set_read_only,\n\t.free_disk\t= md_free_disk,\n};\n\nstatic int md_thread(void *arg)\n{\n\tstruct md_thread *thread = arg;\n\n\t \n\n\tallow_signal(SIGKILL);\n\twhile (!kthread_should_stop()) {\n\n\t\t \n\t\tif (signal_pending(current))\n\t\t\tflush_signals(current);\n\n\t\twait_event_interruptible_timeout\n\t\t\t(thread->wqueue,\n\t\t\t test_bit(THREAD_WAKEUP, &thread->flags)\n\t\t\t || kthread_should_stop() || kthread_should_park(),\n\t\t\t thread->timeout);\n\n\t\tclear_bit(THREAD_WAKEUP, &thread->flags);\n\t\tif (kthread_should_park())\n\t\t\tkthread_parkme();\n\t\tif (!kthread_should_stop())\n\t\t\tthread->run(thread);\n\t}\n\n\treturn 0;\n}\n\nstatic void md_wakeup_thread_directly(struct md_thread __rcu *thread)\n{\n\tstruct md_thread *t;\n\n\trcu_read_lock();\n\tt = rcu_dereference(thread);\n\tif (t)\n\t\twake_up_process(t->tsk);\n\trcu_read_unlock();\n}\n\nvoid md_wakeup_thread(struct md_thread __rcu *thread)\n{\n\tstruct md_thread *t;\n\n\trcu_read_lock();\n\tt = rcu_dereference(thread);\n\tif (t) {\n\t\tpr_debug(\"md: waking up MD thread %s.\\n\", t->tsk->comm);\n\t\tset_bit(THREAD_WAKEUP, &t->flags);\n\t\twake_up(&t->wqueue);\n\t}\n\trcu_read_unlock();\n}\nEXPORT_SYMBOL(md_wakeup_thread);\n\nstruct md_thread *md_register_thread(void (*run) (struct md_thread *),\n\t\tstruct mddev *mddev, const char *name)\n{\n\tstruct md_thread *thread;\n\n\tthread = kzalloc(sizeof(struct md_thread), GFP_KERNEL);\n\tif (!thread)\n\t\treturn NULL;\n\n\tinit_waitqueue_head(&thread->wqueue);\n\n\tthread->run = run;\n\tthread->mddev = mddev;\n\tthread->timeout = MAX_SCHEDULE_TIMEOUT;\n\tthread->tsk = kthread_run(md_thread, thread,\n\t\t\t\t  \"%s_%s\",\n\t\t\t\t  mdname(thread->mddev),\n\t\t\t\t  name);\n\tif (IS_ERR(thread->tsk)) {\n\t\tkfree(thread);\n\t\treturn NULL;\n\t}\n\treturn thread;\n}\nEXPORT_SYMBOL(md_register_thread);\n\nvoid md_unregister_thread(struct mddev *mddev, struct md_thread __rcu **threadp)\n{\n\tstruct md_thread *thread = rcu_dereference_protected(*threadp,\n\t\t\t\t\tlockdep_is_held(&mddev->reconfig_mutex));\n\n\tif (!thread)\n\t\treturn;\n\n\trcu_assign_pointer(*threadp, NULL);\n\tsynchronize_rcu();\n\n\tpr_debug(\"interrupting MD-thread pid %d\\n\", task_pid_nr(thread->tsk));\n\tkthread_stop(thread->tsk);\n\tkfree(thread);\n}\nEXPORT_SYMBOL(md_unregister_thread);\n\nvoid md_error(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tif (!rdev || test_bit(Faulty, &rdev->flags))\n\t\treturn;\n\n\tif (!mddev->pers || !mddev->pers->error_handler)\n\t\treturn;\n\tmddev->pers->error_handler(mddev, rdev);\n\n\tif (mddev->pers->level == 0 || mddev->pers->level == LEVEL_LINEAR)\n\t\treturn;\n\n\tif (mddev->degraded && !test_bit(MD_BROKEN, &mddev->flags))\n\t\tset_bit(MD_RECOVERY_RECOVER, &mddev->recovery);\n\tsysfs_notify_dirent_safe(rdev->sysfs_state);\n\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\tif (!test_bit(MD_BROKEN, &mddev->flags)) {\n\t\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\t\tmd_wakeup_thread(mddev->thread);\n\t}\n\tif (mddev->event_work.func)\n\t\tqueue_work(md_misc_wq, &mddev->event_work);\n\tmd_new_event();\n}\nEXPORT_SYMBOL(md_error);\n\n \n\nstatic void status_unused(struct seq_file *seq)\n{\n\tint i = 0;\n\tstruct md_rdev *rdev;\n\n\tseq_printf(seq, \"unused devices: \");\n\n\tlist_for_each_entry(rdev, &pending_raid_disks, same_set) {\n\t\ti++;\n\t\tseq_printf(seq, \"%pg \", rdev->bdev);\n\t}\n\tif (!i)\n\t\tseq_printf(seq, \"<none>\");\n\n\tseq_printf(seq, \"\\n\");\n}\n\nstatic int status_resync(struct seq_file *seq, struct mddev *mddev)\n{\n\tsector_t max_sectors, resync, res;\n\tunsigned long dt, db = 0;\n\tsector_t rt, curr_mark_cnt, resync_mark_cnt;\n\tint scale, recovery_active;\n\tunsigned int per_milli;\n\n\tif (test_bit(MD_RECOVERY_SYNC, &mddev->recovery) ||\n\t    test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery))\n\t\tmax_sectors = mddev->resync_max_sectors;\n\telse\n\t\tmax_sectors = mddev->dev_sectors;\n\n\tresync = mddev->curr_resync;\n\tif (resync < MD_RESYNC_ACTIVE) {\n\t\tif (test_bit(MD_RECOVERY_DONE, &mddev->recovery))\n\t\t\t \n\t\t\tresync = max_sectors;\n\t} else if (resync > max_sectors) {\n\t\tresync = max_sectors;\n\t} else {\n\t\tres = atomic_read(&mddev->recovery_active);\n\t\t \n\t\tif (resync < res || resync - res < MD_RESYNC_ACTIVE)\n\t\t\tresync = MD_RESYNC_ACTIVE;\n\t\telse\n\t\t\tresync -= res;\n\t}\n\n\tif (resync == MD_RESYNC_NONE) {\n\t\tif (test_bit(MD_RESYNCING_REMOTE, &mddev->recovery)) {\n\t\t\tstruct md_rdev *rdev;\n\n\t\t\trdev_for_each(rdev, mddev)\n\t\t\t\tif (rdev->raid_disk >= 0 &&\n\t\t\t\t    !test_bit(Faulty, &rdev->flags) &&\n\t\t\t\t    rdev->recovery_offset != MaxSector &&\n\t\t\t\t    rdev->recovery_offset) {\n\t\t\t\t\tseq_printf(seq, \"\\trecover=REMOTE\");\n\t\t\t\t\treturn 1;\n\t\t\t\t}\n\t\t\tif (mddev->reshape_position != MaxSector)\n\t\t\t\tseq_printf(seq, \"\\treshape=REMOTE\");\n\t\t\telse\n\t\t\t\tseq_printf(seq, \"\\tresync=REMOTE\");\n\t\t\treturn 1;\n\t\t}\n\t\tif (mddev->recovery_cp < MaxSector) {\n\t\t\tseq_printf(seq, \"\\tresync=PENDING\");\n\t\t\treturn 1;\n\t\t}\n\t\treturn 0;\n\t}\n\tif (resync < MD_RESYNC_ACTIVE) {\n\t\tseq_printf(seq, \"\\tresync=DELAYED\");\n\t\treturn 1;\n\t}\n\n\tWARN_ON(max_sectors == 0);\n\t \n\tscale = 10;\n\tif (sizeof(sector_t) > sizeof(unsigned long)) {\n\t\twhile ( max_sectors/2 > (1ULL<<(scale+32)))\n\t\t\tscale++;\n\t}\n\tres = (resync>>scale)*1000;\n\tsector_div(res, (u32)((max_sectors>>scale)+1));\n\n\tper_milli = res;\n\t{\n\t\tint i, x = per_milli/50, y = 20-x;\n\t\tseq_printf(seq, \"[\");\n\t\tfor (i = 0; i < x; i++)\n\t\t\tseq_printf(seq, \"=\");\n\t\tseq_printf(seq, \">\");\n\t\tfor (i = 0; i < y; i++)\n\t\t\tseq_printf(seq, \".\");\n\t\tseq_printf(seq, \"] \");\n\t}\n\tseq_printf(seq, \" %s =%3u.%u%% (%llu/%llu)\",\n\t\t   (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery)?\n\t\t    \"reshape\" :\n\t\t    (test_bit(MD_RECOVERY_CHECK, &mddev->recovery)?\n\t\t     \"check\" :\n\t\t     (test_bit(MD_RECOVERY_SYNC, &mddev->recovery) ?\n\t\t      \"resync\" : \"recovery\"))),\n\t\t   per_milli/10, per_milli % 10,\n\t\t   (unsigned long long) resync/2,\n\t\t   (unsigned long long) max_sectors/2);\n\n\t \n\tdt = ((jiffies - mddev->resync_mark) / HZ);\n\tif (!dt) dt++;\n\n\tcurr_mark_cnt = mddev->curr_mark_cnt;\n\trecovery_active = atomic_read(&mddev->recovery_active);\n\tresync_mark_cnt = mddev->resync_mark_cnt;\n\n\tif (curr_mark_cnt >= (recovery_active + resync_mark_cnt))\n\t\tdb = curr_mark_cnt - (recovery_active + resync_mark_cnt);\n\n\trt = max_sectors - resync;     \n\trt = div64_u64(rt, db/32+1);\n\trt *= dt;\n\trt >>= 5;\n\n\tseq_printf(seq, \" finish=%lu.%lumin\", (unsigned long)rt / 60,\n\t\t   ((unsigned long)rt % 60)/6);\n\n\tseq_printf(seq, \" speed=%ldK/sec\", db/2/dt);\n\treturn 1;\n}\n\nstatic void *md_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tstruct list_head *tmp;\n\tloff_t l = *pos;\n\tstruct mddev *mddev;\n\n\tif (l == 0x10000) {\n\t\t++*pos;\n\t\treturn (void *)2;\n\t}\n\tif (l > 0x10000)\n\t\treturn NULL;\n\tif (!l--)\n\t\t \n\t\treturn (void*)1;\n\n\tspin_lock(&all_mddevs_lock);\n\tlist_for_each(tmp,&all_mddevs)\n\t\tif (!l--) {\n\t\t\tmddev = list_entry(tmp, struct mddev, all_mddevs);\n\t\t\tif (!mddev_get(mddev))\n\t\t\t\tcontinue;\n\t\t\tspin_unlock(&all_mddevs_lock);\n\t\t\treturn mddev;\n\t\t}\n\tspin_unlock(&all_mddevs_lock);\n\tif (!l--)\n\t\treturn (void*)2; \n\treturn NULL;\n}\n\nstatic void *md_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tstruct list_head *tmp;\n\tstruct mddev *next_mddev, *mddev = v;\n\tstruct mddev *to_put = NULL;\n\n\t++*pos;\n\tif (v == (void*)2)\n\t\treturn NULL;\n\n\tspin_lock(&all_mddevs_lock);\n\tif (v == (void*)1) {\n\t\ttmp = all_mddevs.next;\n\t} else {\n\t\tto_put = mddev;\n\t\ttmp = mddev->all_mddevs.next;\n\t}\n\n\tfor (;;) {\n\t\tif (tmp == &all_mddevs) {\n\t\t\tnext_mddev = (void*)2;\n\t\t\t*pos = 0x10000;\n\t\t\tbreak;\n\t\t}\n\t\tnext_mddev = list_entry(tmp, struct mddev, all_mddevs);\n\t\tif (mddev_get(next_mddev))\n\t\t\tbreak;\n\t\tmddev = next_mddev;\n\t\ttmp = mddev->all_mddevs.next;\n\t}\n\tspin_unlock(&all_mddevs_lock);\n\n\tif (to_put)\n\t\tmddev_put(to_put);\n\treturn next_mddev;\n\n}\n\nstatic void md_seq_stop(struct seq_file *seq, void *v)\n{\n\tstruct mddev *mddev = v;\n\n\tif (mddev && v != (void*)1 && v != (void*)2)\n\t\tmddev_put(mddev);\n}\n\nstatic int md_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct mddev *mddev = v;\n\tsector_t sectors;\n\tstruct md_rdev *rdev;\n\n\tif (v == (void*)1) {\n\t\tstruct md_personality *pers;\n\t\tseq_printf(seq, \"Personalities : \");\n\t\tspin_lock(&pers_lock);\n\t\tlist_for_each_entry(pers, &pers_list, list)\n\t\t\tseq_printf(seq, \"[%s] \", pers->name);\n\n\t\tspin_unlock(&pers_lock);\n\t\tseq_printf(seq, \"\\n\");\n\t\tseq->poll_event = atomic_read(&md_event_count);\n\t\treturn 0;\n\t}\n\tif (v == (void*)2) {\n\t\tstatus_unused(seq);\n\t\treturn 0;\n\t}\n\n\tspin_lock(&mddev->lock);\n\tif (mddev->pers || mddev->raid_disks || !list_empty(&mddev->disks)) {\n\t\tseq_printf(seq, \"%s : %sactive\", mdname(mddev),\n\t\t\t\t\t\tmddev->pers ? \"\" : \"in\");\n\t\tif (mddev->pers) {\n\t\t\tif (mddev->ro == MD_RDONLY)\n\t\t\t\tseq_printf(seq, \" (read-only)\");\n\t\t\tif (mddev->ro == MD_AUTO_READ)\n\t\t\t\tseq_printf(seq, \" (auto-read-only)\");\n\t\t\tseq_printf(seq, \" %s\", mddev->pers->name);\n\t\t}\n\n\t\tsectors = 0;\n\t\trcu_read_lock();\n\t\trdev_for_each_rcu(rdev, mddev) {\n\t\t\tseq_printf(seq, \" %pg[%d]\", rdev->bdev, rdev->desc_nr);\n\n\t\t\tif (test_bit(WriteMostly, &rdev->flags))\n\t\t\t\tseq_printf(seq, \"(W)\");\n\t\t\tif (test_bit(Journal, &rdev->flags))\n\t\t\t\tseq_printf(seq, \"(J)\");\n\t\t\tif (test_bit(Faulty, &rdev->flags)) {\n\t\t\t\tseq_printf(seq, \"(F)\");\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (rdev->raid_disk < 0)\n\t\t\t\tseq_printf(seq, \"(S)\");  \n\t\t\tif (test_bit(Replacement, &rdev->flags))\n\t\t\t\tseq_printf(seq, \"(R)\");\n\t\t\tsectors += rdev->sectors;\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tif (!list_empty(&mddev->disks)) {\n\t\t\tif (mddev->pers)\n\t\t\t\tseq_printf(seq, \"\\n      %llu blocks\",\n\t\t\t\t\t   (unsigned long long)\n\t\t\t\t\t   mddev->array_sectors / 2);\n\t\t\telse\n\t\t\t\tseq_printf(seq, \"\\n      %llu blocks\",\n\t\t\t\t\t   (unsigned long long)sectors / 2);\n\t\t}\n\t\tif (mddev->persistent) {\n\t\t\tif (mddev->major_version != 0 ||\n\t\t\t    mddev->minor_version != 90) {\n\t\t\t\tseq_printf(seq,\" super %d.%d\",\n\t\t\t\t\t   mddev->major_version,\n\t\t\t\t\t   mddev->minor_version);\n\t\t\t}\n\t\t} else if (mddev->external)\n\t\t\tseq_printf(seq, \" super external:%s\",\n\t\t\t\t   mddev->metadata_type);\n\t\telse\n\t\t\tseq_printf(seq, \" super non-persistent\");\n\n\t\tif (mddev->pers) {\n\t\t\tmddev->pers->status(seq, mddev);\n\t\t\tseq_printf(seq, \"\\n      \");\n\t\t\tif (mddev->pers->sync_request) {\n\t\t\t\tif (status_resync(seq, mddev))\n\t\t\t\t\tseq_printf(seq, \"\\n      \");\n\t\t\t}\n\t\t} else\n\t\t\tseq_printf(seq, \"\\n       \");\n\n\t\tmd_bitmap_status(seq, mddev->bitmap);\n\n\t\tseq_printf(seq, \"\\n\");\n\t}\n\tspin_unlock(&mddev->lock);\n\n\treturn 0;\n}\n\nstatic const struct seq_operations md_seq_ops = {\n\t.start  = md_seq_start,\n\t.next   = md_seq_next,\n\t.stop   = md_seq_stop,\n\t.show   = md_seq_show,\n};\n\nstatic int md_seq_open(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *seq;\n\tint error;\n\n\terror = seq_open(file, &md_seq_ops);\n\tif (error)\n\t\treturn error;\n\n\tseq = file->private_data;\n\tseq->poll_event = atomic_read(&md_event_count);\n\treturn error;\n}\n\nstatic int md_unloading;\nstatic __poll_t mdstat_poll(struct file *filp, poll_table *wait)\n{\n\tstruct seq_file *seq = filp->private_data;\n\t__poll_t mask;\n\n\tif (md_unloading)\n\t\treturn EPOLLIN|EPOLLRDNORM|EPOLLERR|EPOLLPRI;\n\tpoll_wait(filp, &md_event_waiters, wait);\n\n\t \n\tmask = EPOLLIN | EPOLLRDNORM;\n\n\tif (seq->poll_event != atomic_read(&md_event_count))\n\t\tmask |= EPOLLERR | EPOLLPRI;\n\treturn mask;\n}\n\nstatic const struct proc_ops mdstat_proc_ops = {\n\t.proc_open\t= md_seq_open,\n\t.proc_read\t= seq_read,\n\t.proc_lseek\t= seq_lseek,\n\t.proc_release\t= seq_release,\n\t.proc_poll\t= mdstat_poll,\n};\n\nint register_md_personality(struct md_personality *p)\n{\n\tpr_debug(\"md: %s personality registered for level %d\\n\",\n\t\t p->name, p->level);\n\tspin_lock(&pers_lock);\n\tlist_add_tail(&p->list, &pers_list);\n\tspin_unlock(&pers_lock);\n\treturn 0;\n}\nEXPORT_SYMBOL(register_md_personality);\n\nint unregister_md_personality(struct md_personality *p)\n{\n\tpr_debug(\"md: %s personality unregistered\\n\", p->name);\n\tspin_lock(&pers_lock);\n\tlist_del_init(&p->list);\n\tspin_unlock(&pers_lock);\n\treturn 0;\n}\nEXPORT_SYMBOL(unregister_md_personality);\n\nint register_md_cluster_operations(struct md_cluster_operations *ops,\n\t\t\t\t   struct module *module)\n{\n\tint ret = 0;\n\tspin_lock(&pers_lock);\n\tif (md_cluster_ops != NULL)\n\t\tret = -EALREADY;\n\telse {\n\t\tmd_cluster_ops = ops;\n\t\tmd_cluster_mod = module;\n\t}\n\tspin_unlock(&pers_lock);\n\treturn ret;\n}\nEXPORT_SYMBOL(register_md_cluster_operations);\n\nint unregister_md_cluster_operations(void)\n{\n\tspin_lock(&pers_lock);\n\tmd_cluster_ops = NULL;\n\tspin_unlock(&pers_lock);\n\treturn 0;\n}\nEXPORT_SYMBOL(unregister_md_cluster_operations);\n\nint md_setup_cluster(struct mddev *mddev, int nodes)\n{\n\tint ret;\n\tif (!md_cluster_ops)\n\t\trequest_module(\"md-cluster\");\n\tspin_lock(&pers_lock);\n\t \n\tif (!md_cluster_ops || !try_module_get(md_cluster_mod)) {\n\t\tpr_warn(\"can't find md-cluster module or get its reference.\\n\");\n\t\tspin_unlock(&pers_lock);\n\t\treturn -ENOENT;\n\t}\n\tspin_unlock(&pers_lock);\n\n\tret = md_cluster_ops->join(mddev, nodes);\n\tif (!ret)\n\t\tmddev->safemode_delay = 0;\n\treturn ret;\n}\n\nvoid md_cluster_stop(struct mddev *mddev)\n{\n\tif (!md_cluster_ops)\n\t\treturn;\n\tmd_cluster_ops->leave(mddev);\n\tmodule_put(md_cluster_mod);\n}\n\nstatic int is_mddev_idle(struct mddev *mddev, int init)\n{\n\tstruct md_rdev *rdev;\n\tint idle;\n\tint curr_events;\n\n\tidle = 1;\n\trcu_read_lock();\n\trdev_for_each_rcu(rdev, mddev) {\n\t\tstruct gendisk *disk = rdev->bdev->bd_disk;\n\t\tcurr_events = (int)part_stat_read_accum(disk->part0, sectors) -\n\t\t\t      atomic_read(&disk->sync_io);\n\t\t \n\t\tif (init || curr_events - rdev->last_events > 64) {\n\t\t\trdev->last_events = curr_events;\n\t\t\tidle = 0;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn idle;\n}\n\nvoid md_done_sync(struct mddev *mddev, int blocks, int ok)\n{\n\t \n\tatomic_sub(blocks, &mddev->recovery_active);\n\twake_up(&mddev->recovery_wait);\n\tif (!ok) {\n\t\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\t\tset_bit(MD_RECOVERY_ERROR, &mddev->recovery);\n\t\tmd_wakeup_thread(mddev->thread);\n\t\t \n\t}\n}\nEXPORT_SYMBOL(md_done_sync);\n\n \nbool md_write_start(struct mddev *mddev, struct bio *bi)\n{\n\tint did_change = 0;\n\n\tif (bio_data_dir(bi) != WRITE)\n\t\treturn true;\n\n\tBUG_ON(mddev->ro == MD_RDONLY);\n\tif (mddev->ro == MD_AUTO_READ) {\n\t\t \n\t\tmddev->ro = MD_RDWR;\n\t\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\t\tmd_wakeup_thread(mddev->thread);\n\t\tmd_wakeup_thread(mddev->sync_thread);\n\t\tdid_change = 1;\n\t}\n\trcu_read_lock();\n\tpercpu_ref_get(&mddev->writes_pending);\n\tsmp_mb();  \n\tif (mddev->safemode == 1)\n\t\tmddev->safemode = 0;\n\t \n\tif (mddev->in_sync || mddev->sync_checkers) {\n\t\tspin_lock(&mddev->lock);\n\t\tif (mddev->in_sync) {\n\t\t\tmddev->in_sync = 0;\n\t\t\tset_bit(MD_SB_CHANGE_CLEAN, &mddev->sb_flags);\n\t\t\tset_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags);\n\t\t\tmd_wakeup_thread(mddev->thread);\n\t\t\tdid_change = 1;\n\t\t}\n\t\tspin_unlock(&mddev->lock);\n\t}\n\trcu_read_unlock();\n\tif (did_change)\n\t\tsysfs_notify_dirent_safe(mddev->sysfs_state);\n\tif (!mddev->has_superblocks)\n\t\treturn true;\n\twait_event(mddev->sb_wait,\n\t\t   !test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags) ||\n\t\t   is_md_suspended(mddev));\n\tif (test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags)) {\n\t\tpercpu_ref_put(&mddev->writes_pending);\n\t\treturn false;\n\t}\n\treturn true;\n}\nEXPORT_SYMBOL(md_write_start);\n\n \nvoid md_write_inc(struct mddev *mddev, struct bio *bi)\n{\n\tif (bio_data_dir(bi) != WRITE)\n\t\treturn;\n\tWARN_ON_ONCE(mddev->in_sync || !md_is_rdwr(mddev));\n\tpercpu_ref_get(&mddev->writes_pending);\n}\nEXPORT_SYMBOL(md_write_inc);\n\nvoid md_write_end(struct mddev *mddev)\n{\n\tpercpu_ref_put(&mddev->writes_pending);\n\n\tif (mddev->safemode == 2)\n\t\tmd_wakeup_thread(mddev->thread);\n\telse if (mddev->safemode_delay)\n\t\t \n\t\tmod_timer(&mddev->safemode_timer,\n\t\t\t  roundup(jiffies, mddev->safemode_delay) +\n\t\t\t  mddev->safemode_delay);\n}\n\nEXPORT_SYMBOL(md_write_end);\n\n \nvoid md_submit_discard_bio(struct mddev *mddev, struct md_rdev *rdev,\n\t\t\tstruct bio *bio, sector_t start, sector_t size)\n{\n\tstruct bio *discard_bio = NULL;\n\n\tif (__blkdev_issue_discard(rdev->bdev, start, size, GFP_NOIO,\n\t\t\t&discard_bio) || !discard_bio)\n\t\treturn;\n\n\tbio_chain(discard_bio, bio);\n\tbio_clone_blkg_association(discard_bio, bio);\n\tif (mddev->gendisk)\n\t\ttrace_block_bio_remap(discard_bio,\n\t\t\t\tdisk_devt(mddev->gendisk),\n\t\t\t\tbio->bi_iter.bi_sector);\n\tsubmit_bio_noacct(discard_bio);\n}\nEXPORT_SYMBOL_GPL(md_submit_discard_bio);\n\nstatic void md_end_clone_io(struct bio *bio)\n{\n\tstruct md_io_clone *md_io_clone = bio->bi_private;\n\tstruct bio *orig_bio = md_io_clone->orig_bio;\n\tstruct mddev *mddev = md_io_clone->mddev;\n\n\tif (bio->bi_status && !orig_bio->bi_status)\n\t\torig_bio->bi_status = bio->bi_status;\n\n\tif (md_io_clone->start_time)\n\t\tbio_end_io_acct(orig_bio, md_io_clone->start_time);\n\n\tbio_put(bio);\n\tbio_endio(orig_bio);\n\tpercpu_ref_put(&mddev->active_io);\n}\n\nstatic void md_clone_bio(struct mddev *mddev, struct bio **bio)\n{\n\tstruct block_device *bdev = (*bio)->bi_bdev;\n\tstruct md_io_clone *md_io_clone;\n\tstruct bio *clone =\n\t\tbio_alloc_clone(bdev, *bio, GFP_NOIO, &mddev->io_clone_set);\n\n\tmd_io_clone = container_of(clone, struct md_io_clone, bio_clone);\n\tmd_io_clone->orig_bio = *bio;\n\tmd_io_clone->mddev = mddev;\n\tif (blk_queue_io_stat(bdev->bd_disk->queue))\n\t\tmd_io_clone->start_time = bio_start_io_acct(*bio);\n\n\tclone->bi_end_io = md_end_clone_io;\n\tclone->bi_private = md_io_clone;\n\t*bio = clone;\n}\n\nvoid md_account_bio(struct mddev *mddev, struct bio **bio)\n{\n\tpercpu_ref_get(&mddev->active_io);\n\tmd_clone_bio(mddev, bio);\n}\nEXPORT_SYMBOL_GPL(md_account_bio);\n\n \nvoid md_allow_write(struct mddev *mddev)\n{\n\tif (!mddev->pers)\n\t\treturn;\n\tif (!md_is_rdwr(mddev))\n\t\treturn;\n\tif (!mddev->pers->sync_request)\n\t\treturn;\n\n\tspin_lock(&mddev->lock);\n\tif (mddev->in_sync) {\n\t\tmddev->in_sync = 0;\n\t\tset_bit(MD_SB_CHANGE_CLEAN, &mddev->sb_flags);\n\t\tset_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags);\n\t\tif (mddev->safemode_delay &&\n\t\t    mddev->safemode == 0)\n\t\t\tmddev->safemode = 1;\n\t\tspin_unlock(&mddev->lock);\n\t\tmd_update_sb(mddev, 0);\n\t\tsysfs_notify_dirent_safe(mddev->sysfs_state);\n\t\t \n\t\twait_event(mddev->sb_wait,\n\t\t\t   !test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags));\n\t} else\n\t\tspin_unlock(&mddev->lock);\n}\nEXPORT_SYMBOL_GPL(md_allow_write);\n\n#define SYNC_MARKS\t10\n#define\tSYNC_MARK_STEP\t(3*HZ)\n#define UPDATE_FREQUENCY (5*60*HZ)\nvoid md_do_sync(struct md_thread *thread)\n{\n\tstruct mddev *mddev = thread->mddev;\n\tstruct mddev *mddev2;\n\tunsigned int currspeed = 0, window;\n\tsector_t max_sectors,j, io_sectors, recovery_done;\n\tunsigned long mark[SYNC_MARKS];\n\tunsigned long update_time;\n\tsector_t mark_cnt[SYNC_MARKS];\n\tint last_mark,m;\n\tsector_t last_check;\n\tint skipped = 0;\n\tstruct md_rdev *rdev;\n\tchar *desc, *action = NULL;\n\tstruct blk_plug plug;\n\tint ret;\n\n\t \n\tif (test_bit(MD_RECOVERY_DONE, &mddev->recovery) ||\n\t    test_bit(MD_RECOVERY_WAIT, &mddev->recovery))\n\t\treturn;\n\tif (!md_is_rdwr(mddev)) { \n\t\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\t\treturn;\n\t}\n\n\tif (mddev_is_clustered(mddev)) {\n\t\tret = md_cluster_ops->resync_start(mddev);\n\t\tif (ret)\n\t\t\tgoto skip;\n\n\t\tset_bit(MD_CLUSTER_RESYNC_LOCKED, &mddev->flags);\n\t\tif (!(test_bit(MD_RECOVERY_SYNC, &mddev->recovery) ||\n\t\t\ttest_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) ||\n\t\t\ttest_bit(MD_RECOVERY_RECOVER, &mddev->recovery))\n\t\t     && ((unsigned long long)mddev->curr_resync_completed\n\t\t\t < (unsigned long long)mddev->resync_max_sectors))\n\t\t\tgoto skip;\n\t}\n\n\tif (test_bit(MD_RECOVERY_SYNC, &mddev->recovery)) {\n\t\tif (test_bit(MD_RECOVERY_CHECK, &mddev->recovery)) {\n\t\t\tdesc = \"data-check\";\n\t\t\taction = \"check\";\n\t\t} else if (test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery)) {\n\t\t\tdesc = \"requested-resync\";\n\t\t\taction = \"repair\";\n\t\t} else\n\t\t\tdesc = \"resync\";\n\t} else if (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery))\n\t\tdesc = \"reshape\";\n\telse\n\t\tdesc = \"recovery\";\n\n\tmddev->last_sync_action = action ?: desc;\n\n\t \n\n\tdo {\n\t\tint mddev2_minor = -1;\n\t\tmddev->curr_resync = MD_RESYNC_DELAYED;\n\n\ttry_again:\n\t\tif (test_bit(MD_RECOVERY_INTR, &mddev->recovery))\n\t\t\tgoto skip;\n\t\tspin_lock(&all_mddevs_lock);\n\t\tlist_for_each_entry(mddev2, &all_mddevs, all_mddevs) {\n\t\t\tif (test_bit(MD_DELETED, &mddev2->flags))\n\t\t\t\tcontinue;\n\t\t\tif (mddev2 == mddev)\n\t\t\t\tcontinue;\n\t\t\tif (!mddev->parallel_resync\n\t\t\t&&  mddev2->curr_resync\n\t\t\t&&  match_mddev_units(mddev, mddev2)) {\n\t\t\t\tDEFINE_WAIT(wq);\n\t\t\t\tif (mddev < mddev2 &&\n\t\t\t\t    mddev->curr_resync == MD_RESYNC_DELAYED) {\n\t\t\t\t\t \n\t\t\t\t\tmddev->curr_resync = MD_RESYNC_YIELDED;\n\t\t\t\t\twake_up(&resync_wait);\n\t\t\t\t}\n\t\t\t\tif (mddev > mddev2 &&\n\t\t\t\t    mddev->curr_resync == MD_RESYNC_YIELDED)\n\t\t\t\t\t \n\t\t\t\t\tcontinue;\n\t\t\t\t \n\t\t\t\tprepare_to_wait(&resync_wait, &wq, TASK_INTERRUPTIBLE);\n\t\t\t\tif (!test_bit(MD_RECOVERY_INTR, &mddev->recovery) &&\n\t\t\t\t    mddev2->curr_resync >= mddev->curr_resync) {\n\t\t\t\t\tif (mddev2_minor != mddev2->md_minor) {\n\t\t\t\t\t\tmddev2_minor = mddev2->md_minor;\n\t\t\t\t\t\tpr_info(\"md: delaying %s of %s until %s has finished (they share one or more physical units)\\n\",\n\t\t\t\t\t\t\tdesc, mdname(mddev),\n\t\t\t\t\t\t\tmdname(mddev2));\n\t\t\t\t\t}\n\t\t\t\t\tspin_unlock(&all_mddevs_lock);\n\n\t\t\t\t\tif (signal_pending(current))\n\t\t\t\t\t\tflush_signals(current);\n\t\t\t\t\tschedule();\n\t\t\t\t\tfinish_wait(&resync_wait, &wq);\n\t\t\t\t\tgoto try_again;\n\t\t\t\t}\n\t\t\t\tfinish_wait(&resync_wait, &wq);\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&all_mddevs_lock);\n\t} while (mddev->curr_resync < MD_RESYNC_DELAYED);\n\n\tj = 0;\n\tif (test_bit(MD_RECOVERY_SYNC, &mddev->recovery)) {\n\t\t \n\t\tmax_sectors = mddev->resync_max_sectors;\n\t\tatomic64_set(&mddev->resync_mismatches, 0);\n\t\t \n\t\tif (test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery))\n\t\t\tj = mddev->resync_min;\n\t\telse if (!mddev->bitmap)\n\t\t\tj = mddev->recovery_cp;\n\n\t} else if (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery)) {\n\t\tmax_sectors = mddev->resync_max_sectors;\n\t\t \n\t\tif (mddev_is_clustered(mddev) &&\n\t\t    mddev->reshape_position != MaxSector)\n\t\t\tj = mddev->reshape_position;\n\t} else {\n\t\t \n\t\tmax_sectors = mddev->dev_sectors;\n\t\tj = MaxSector;\n\t\trcu_read_lock();\n\t\trdev_for_each_rcu(rdev, mddev)\n\t\t\tif (rdev->raid_disk >= 0 &&\n\t\t\t    !test_bit(Journal, &rdev->flags) &&\n\t\t\t    !test_bit(Faulty, &rdev->flags) &&\n\t\t\t    !test_bit(In_sync, &rdev->flags) &&\n\t\t\t    rdev->recovery_offset < j)\n\t\t\t\tj = rdev->recovery_offset;\n\t\trcu_read_unlock();\n\n\t\t \n\t\tif (mddev->bitmap) {\n\t\t\tmddev->pers->quiesce(mddev, 1);\n\t\t\tmddev->pers->quiesce(mddev, 0);\n\t\t}\n\t}\n\n\tpr_info(\"md: %s of RAID array %s\\n\", desc, mdname(mddev));\n\tpr_debug(\"md: minimum _guaranteed_  speed: %d KB/sec/disk.\\n\", speed_min(mddev));\n\tpr_debug(\"md: using maximum available idle IO bandwidth (but not more than %d KB/sec) for %s.\\n\",\n\t\t speed_max(mddev), desc);\n\n\tis_mddev_idle(mddev, 1);  \n\n\tio_sectors = 0;\n\tfor (m = 0; m < SYNC_MARKS; m++) {\n\t\tmark[m] = jiffies;\n\t\tmark_cnt[m] = io_sectors;\n\t}\n\tlast_mark = 0;\n\tmddev->resync_mark = mark[last_mark];\n\tmddev->resync_mark_cnt = mark_cnt[last_mark];\n\n\t \n\twindow = 32 * (PAGE_SIZE / 512);\n\tpr_debug(\"md: using %dk window, over a total of %lluk.\\n\",\n\t\t window/2, (unsigned long long)max_sectors/2);\n\n\tatomic_set(&mddev->recovery_active, 0);\n\tlast_check = 0;\n\n\tif (j >= MD_RESYNC_ACTIVE) {\n\t\tpr_debug(\"md: resuming %s of %s from checkpoint.\\n\",\n\t\t\t desc, mdname(mddev));\n\t\tmddev->curr_resync = j;\n\t} else\n\t\tmddev->curr_resync = MD_RESYNC_ACTIVE;  \n\tmddev->curr_resync_completed = j;\n\tsysfs_notify_dirent_safe(mddev->sysfs_completed);\n\tmd_new_event();\n\tupdate_time = jiffies;\n\n\tblk_start_plug(&plug);\n\twhile (j < max_sectors) {\n\t\tsector_t sectors;\n\n\t\tskipped = 0;\n\n\t\tif (!test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&\n\t\t    ((mddev->curr_resync > mddev->curr_resync_completed &&\n\t\t      (mddev->curr_resync - mddev->curr_resync_completed)\n\t\t      > (max_sectors >> 4)) ||\n\t\t     time_after_eq(jiffies, update_time + UPDATE_FREQUENCY) ||\n\t\t     (j - mddev->curr_resync_completed)*2\n\t\t     >= mddev->resync_max - mddev->curr_resync_completed ||\n\t\t     mddev->curr_resync_completed > mddev->resync_max\n\t\t\t    )) {\n\t\t\t \n\t\t\twait_event(mddev->recovery_wait,\n\t\t\t\t   atomic_read(&mddev->recovery_active) == 0);\n\t\t\tmddev->curr_resync_completed = j;\n\t\t\tif (test_bit(MD_RECOVERY_SYNC, &mddev->recovery) &&\n\t\t\t    j > mddev->recovery_cp)\n\t\t\t\tmddev->recovery_cp = j;\n\t\t\tupdate_time = jiffies;\n\t\t\tset_bit(MD_SB_CHANGE_CLEAN, &mddev->sb_flags);\n\t\t\tsysfs_notify_dirent_safe(mddev->sysfs_completed);\n\t\t}\n\n\t\twhile (j >= mddev->resync_max &&\n\t\t       !test_bit(MD_RECOVERY_INTR, &mddev->recovery)) {\n\t\t\t \n\t\t\tflush_signals(current);  \n\t\t\twait_event_interruptible(mddev->recovery_wait,\n\t\t\t\t\t\t mddev->resync_max > j\n\t\t\t\t\t\t || test_bit(MD_RECOVERY_INTR,\n\t\t\t\t\t\t\t     &mddev->recovery));\n\t\t}\n\n\t\tif (test_bit(MD_RECOVERY_INTR, &mddev->recovery))\n\t\t\tbreak;\n\n\t\tsectors = mddev->pers->sync_request(mddev, j, &skipped);\n\t\tif (sectors == 0) {\n\t\t\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!skipped) {  \n\t\t\tio_sectors += sectors;\n\t\t\tatomic_add(sectors, &mddev->recovery_active);\n\t\t}\n\n\t\tif (test_bit(MD_RECOVERY_INTR, &mddev->recovery))\n\t\t\tbreak;\n\n\t\tj += sectors;\n\t\tif (j > max_sectors)\n\t\t\t \n\t\t\tj = max_sectors;\n\t\tif (j >= MD_RESYNC_ACTIVE)\n\t\t\tmddev->curr_resync = j;\n\t\tmddev->curr_mark_cnt = io_sectors;\n\t\tif (last_check == 0)\n\t\t\t \n\t\t\tmd_new_event();\n\n\t\tif (last_check + window > io_sectors || j == max_sectors)\n\t\t\tcontinue;\n\n\t\tlast_check = io_sectors;\n\trepeat:\n\t\tif (time_after_eq(jiffies, mark[last_mark] + SYNC_MARK_STEP )) {\n\t\t\t \n\t\t\tint next = (last_mark+1) % SYNC_MARKS;\n\n\t\t\tmddev->resync_mark = mark[next];\n\t\t\tmddev->resync_mark_cnt = mark_cnt[next];\n\t\t\tmark[next] = jiffies;\n\t\t\tmark_cnt[next] = io_sectors - atomic_read(&mddev->recovery_active);\n\t\t\tlast_mark = next;\n\t\t}\n\n\t\tif (test_bit(MD_RECOVERY_INTR, &mddev->recovery))\n\t\t\tbreak;\n\n\t\t \n\t\tcond_resched();\n\n\t\trecovery_done = io_sectors - atomic_read(&mddev->recovery_active);\n\t\tcurrspeed = ((unsigned long)(recovery_done - mddev->resync_mark_cnt))/2\n\t\t\t/((jiffies-mddev->resync_mark)/HZ +1) +1;\n\n\t\tif (currspeed > speed_min(mddev)) {\n\t\t\tif (currspeed > speed_max(mddev)) {\n\t\t\t\tmsleep(500);\n\t\t\t\tgoto repeat;\n\t\t\t}\n\t\t\tif (!is_mddev_idle(mddev, 0)) {\n\t\t\t\t \n\t\t\t\twait_event(mddev->recovery_wait,\n\t\t\t\t\t   !atomic_read(&mddev->recovery_active));\n\t\t\t}\n\t\t}\n\t}\n\tpr_info(\"md: %s: %s %s.\\n\",mdname(mddev), desc,\n\t\ttest_bit(MD_RECOVERY_INTR, &mddev->recovery)\n\t\t? \"interrupted\" : \"done\");\n\t \n\tblk_finish_plug(&plug);\n\twait_event(mddev->recovery_wait, !atomic_read(&mddev->recovery_active));\n\n\tif (!test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&\n\t    !test_bit(MD_RECOVERY_INTR, &mddev->recovery) &&\n\t    mddev->curr_resync >= MD_RESYNC_ACTIVE) {\n\t\tmddev->curr_resync_completed = mddev->curr_resync;\n\t\tsysfs_notify_dirent_safe(mddev->sysfs_completed);\n\t}\n\tmddev->pers->sync_request(mddev, max_sectors, &skipped);\n\n\tif (!test_bit(MD_RECOVERY_CHECK, &mddev->recovery) &&\n\t    mddev->curr_resync > MD_RESYNC_ACTIVE) {\n\t\tif (test_bit(MD_RECOVERY_SYNC, &mddev->recovery)) {\n\t\t\tif (test_bit(MD_RECOVERY_INTR, &mddev->recovery)) {\n\t\t\t\tif (mddev->curr_resync >= mddev->recovery_cp) {\n\t\t\t\t\tpr_debug(\"md: checkpointing %s of %s.\\n\",\n\t\t\t\t\t\t desc, mdname(mddev));\n\t\t\t\t\tif (test_bit(MD_RECOVERY_ERROR,\n\t\t\t\t\t\t&mddev->recovery))\n\t\t\t\t\t\tmddev->recovery_cp =\n\t\t\t\t\t\t\tmddev->curr_resync_completed;\n\t\t\t\t\telse\n\t\t\t\t\t\tmddev->recovery_cp =\n\t\t\t\t\t\t\tmddev->curr_resync;\n\t\t\t\t}\n\t\t\t} else\n\t\t\t\tmddev->recovery_cp = MaxSector;\n\t\t} else {\n\t\t\tif (!test_bit(MD_RECOVERY_INTR, &mddev->recovery))\n\t\t\t\tmddev->curr_resync = MaxSector;\n\t\t\tif (!test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&\n\t\t\t    test_bit(MD_RECOVERY_RECOVER, &mddev->recovery)) {\n\t\t\t\trcu_read_lock();\n\t\t\t\trdev_for_each_rcu(rdev, mddev)\n\t\t\t\t\tif (rdev->raid_disk >= 0 &&\n\t\t\t\t\t    mddev->delta_disks >= 0 &&\n\t\t\t\t\t    !test_bit(Journal, &rdev->flags) &&\n\t\t\t\t\t    !test_bit(Faulty, &rdev->flags) &&\n\t\t\t\t\t    !test_bit(In_sync, &rdev->flags) &&\n\t\t\t\t\t    rdev->recovery_offset < mddev->curr_resync)\n\t\t\t\t\t\trdev->recovery_offset = mddev->curr_resync;\n\t\t\t\trcu_read_unlock();\n\t\t\t}\n\t\t}\n\t}\n skip:\n\t \n\tset_mask_bits(&mddev->sb_flags, 0,\n\t\t      BIT(MD_SB_CHANGE_PENDING) | BIT(MD_SB_CHANGE_DEVS));\n\n\tif (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&\n\t\t\t!test_bit(MD_RECOVERY_INTR, &mddev->recovery) &&\n\t\t\tmddev->delta_disks > 0 &&\n\t\t\tmddev->pers->finish_reshape &&\n\t\t\tmddev->pers->size &&\n\t\t\tmddev->queue) {\n\t\tmddev_lock_nointr(mddev);\n\t\tmd_set_array_sectors(mddev, mddev->pers->size(mddev, 0, 0));\n\t\tmddev_unlock(mddev);\n\t\tif (!mddev_is_clustered(mddev))\n\t\t\tset_capacity_and_notify(mddev->gendisk,\n\t\t\t\t\t\tmddev->array_sectors);\n\t}\n\n\tspin_lock(&mddev->lock);\n\tif (!test_bit(MD_RECOVERY_INTR, &mddev->recovery)) {\n\t\t \n\t\tif (test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery))\n\t\t\tmddev->resync_min = 0;\n\t\tmddev->resync_max = MaxSector;\n\t} else if (test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery))\n\t\tmddev->resync_min = mddev->curr_resync_completed;\n\tset_bit(MD_RECOVERY_DONE, &mddev->recovery);\n\tmddev->curr_resync = MD_RESYNC_NONE;\n\tspin_unlock(&mddev->lock);\n\n\twake_up(&resync_wait);\n\twake_up(&mddev->sb_wait);\n\tmd_wakeup_thread(mddev->thread);\n\treturn;\n}\nEXPORT_SYMBOL_GPL(md_do_sync);\n\nstatic int remove_and_add_spares(struct mddev *mddev,\n\t\t\t\t struct md_rdev *this)\n{\n\tstruct md_rdev *rdev;\n\tint spares = 0;\n\tint removed = 0;\n\tbool remove_some = false;\n\n\tif (this && test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))\n\t\t \n\t\treturn 0;\n\n\trdev_for_each(rdev, mddev) {\n\t\tif ((this == NULL || rdev == this) &&\n\t\t    rdev->raid_disk >= 0 &&\n\t\t    !test_bit(Blocked, &rdev->flags) &&\n\t\t    test_bit(Faulty, &rdev->flags) &&\n\t\t    atomic_read(&rdev->nr_pending)==0) {\n\t\t\t \n\t\t\tremove_some = true;\n\t\t\tset_bit(RemoveSynchronized, &rdev->flags);\n\t\t}\n\t}\n\n\tif (remove_some)\n\t\tsynchronize_rcu();\n\trdev_for_each(rdev, mddev) {\n\t\tif ((this == NULL || rdev == this) &&\n\t\t    rdev->raid_disk >= 0 &&\n\t\t    !test_bit(Blocked, &rdev->flags) &&\n\t\t    ((test_bit(RemoveSynchronized, &rdev->flags) ||\n\t\t     (!test_bit(In_sync, &rdev->flags) &&\n\t\t      !test_bit(Journal, &rdev->flags))) &&\n\t\t    atomic_read(&rdev->nr_pending)==0)) {\n\t\t\tif (mddev->pers->hot_remove_disk(\n\t\t\t\t    mddev, rdev) == 0) {\n\t\t\t\tsysfs_unlink_rdev(mddev, rdev);\n\t\t\t\trdev->saved_raid_disk = rdev->raid_disk;\n\t\t\t\trdev->raid_disk = -1;\n\t\t\t\tremoved++;\n\t\t\t}\n\t\t}\n\t\tif (remove_some && test_bit(RemoveSynchronized, &rdev->flags))\n\t\t\tclear_bit(RemoveSynchronized, &rdev->flags);\n\t}\n\n\tif (removed && mddev->kobj.sd)\n\t\tsysfs_notify_dirent_safe(mddev->sysfs_degraded);\n\n\tif (this && removed)\n\t\tgoto no_add;\n\n\trdev_for_each(rdev, mddev) {\n\t\tif (this && this != rdev)\n\t\t\tcontinue;\n\t\tif (test_bit(Candidate, &rdev->flags))\n\t\t\tcontinue;\n\t\tif (rdev->raid_disk >= 0 &&\n\t\t    !test_bit(In_sync, &rdev->flags) &&\n\t\t    !test_bit(Journal, &rdev->flags) &&\n\t\t    !test_bit(Faulty, &rdev->flags))\n\t\t\tspares++;\n\t\tif (rdev->raid_disk >= 0)\n\t\t\tcontinue;\n\t\tif (test_bit(Faulty, &rdev->flags))\n\t\t\tcontinue;\n\t\tif (!test_bit(Journal, &rdev->flags)) {\n\t\t\tif (!md_is_rdwr(mddev) &&\n\t\t\t    !(rdev->saved_raid_disk >= 0 &&\n\t\t\t      !test_bit(Bitmap_sync, &rdev->flags)))\n\t\t\t\tcontinue;\n\n\t\t\trdev->recovery_offset = 0;\n\t\t}\n\t\tif (mddev->pers->hot_add_disk(mddev, rdev) == 0) {\n\t\t\t \n\t\t\tsysfs_link_rdev(mddev, rdev);\n\t\t\tif (!test_bit(Journal, &rdev->flags))\n\t\t\t\tspares++;\n\t\t\tmd_new_event();\n\t\t\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\t\t}\n\t}\nno_add:\n\tif (removed)\n\t\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\treturn spares;\n}\n\nstatic void md_start_sync(struct work_struct *ws)\n{\n\tstruct mddev *mddev = container_of(ws, struct mddev, del_work);\n\n\trcu_assign_pointer(mddev->sync_thread,\n\t\t\t   md_register_thread(md_do_sync, mddev, \"resync\"));\n\tif (!mddev->sync_thread) {\n\t\tpr_warn(\"%s: could not start resync thread...\\n\",\n\t\t\tmdname(mddev));\n\t\t \n\t\tclear_bit(MD_RECOVERY_SYNC, &mddev->recovery);\n\t\tclear_bit(MD_RECOVERY_RESHAPE, &mddev->recovery);\n\t\tclear_bit(MD_RECOVERY_REQUESTED, &mddev->recovery);\n\t\tclear_bit(MD_RECOVERY_CHECK, &mddev->recovery);\n\t\tclear_bit(MD_RECOVERY_RUNNING, &mddev->recovery);\n\t\twake_up(&resync_wait);\n\t\tif (test_and_clear_bit(MD_RECOVERY_RECOVER,\n\t\t\t\t       &mddev->recovery))\n\t\t\tif (mddev->sysfs_action)\n\t\t\t\tsysfs_notify_dirent_safe(mddev->sysfs_action);\n\t} else\n\t\tmd_wakeup_thread(mddev->sync_thread);\n\tsysfs_notify_dirent_safe(mddev->sysfs_action);\n\tmd_new_event();\n}\n\n \nvoid md_check_recovery(struct mddev *mddev)\n{\n\tif (test_bit(MD_ALLOW_SB_UPDATE, &mddev->flags) && mddev->sb_flags) {\n\t\t \n\t\tset_bit(MD_UPDATING_SB, &mddev->flags);\n\t\tsmp_mb__after_atomic();\n\t\tif (test_bit(MD_ALLOW_SB_UPDATE, &mddev->flags))\n\t\t\tmd_update_sb(mddev, 0);\n\t\tclear_bit_unlock(MD_UPDATING_SB, &mddev->flags);\n\t\twake_up(&mddev->sb_wait);\n\t}\n\n\tif (is_md_suspended(mddev))\n\t\treturn;\n\n\tif (mddev->bitmap)\n\t\tmd_bitmap_daemon_work(mddev);\n\n\tif (signal_pending(current)) {\n\t\tif (mddev->pers->sync_request && !mddev->external) {\n\t\t\tpr_debug(\"md: %s in immediate safe mode\\n\",\n\t\t\t\t mdname(mddev));\n\t\t\tmddev->safemode = 2;\n\t\t}\n\t\tflush_signals(current);\n\t}\n\n\tif (!md_is_rdwr(mddev) &&\n\t    !test_bit(MD_RECOVERY_NEEDED, &mddev->recovery))\n\t\treturn;\n\tif ( ! (\n\t\t(mddev->sb_flags & ~ (1<<MD_SB_CHANGE_PENDING)) ||\n\t\ttest_bit(MD_RECOVERY_NEEDED, &mddev->recovery) ||\n\t\ttest_bit(MD_RECOVERY_DONE, &mddev->recovery) ||\n\t\t(mddev->external == 0 && mddev->safemode == 1) ||\n\t\t(mddev->safemode == 2\n\t\t && !mddev->in_sync && mddev->recovery_cp == MaxSector)\n\t\t))\n\t\treturn;\n\n\tif (mddev_trylock(mddev)) {\n\t\tint spares = 0;\n\t\tbool try_set_sync = mddev->safemode != 0;\n\n\t\tif (!mddev->external && mddev->safemode == 1)\n\t\t\tmddev->safemode = 0;\n\n\t\tif (!md_is_rdwr(mddev)) {\n\t\t\tstruct md_rdev *rdev;\n\t\t\tif (!mddev->external && mddev->in_sync)\n\t\t\t\t \n\t\t\t\trdev_for_each(rdev, mddev)\n\t\t\t\t\tclear_bit(Blocked, &rdev->flags);\n\t\t\t \n\t\t\tremove_and_add_spares(mddev, NULL);\n\t\t\t \n\t\t\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\t\t\tmd_reap_sync_thread(mddev);\n\t\t\tclear_bit(MD_RECOVERY_RECOVER, &mddev->recovery);\n\t\t\tclear_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\t\t\tclear_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags);\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tif (mddev_is_clustered(mddev)) {\n\t\t\tstruct md_rdev *rdev, *tmp;\n\t\t\t \n\t\t\trdev_for_each_safe(rdev, tmp, mddev) {\n\t\t\t\tif (test_and_clear_bit(ClusterRemove, &rdev->flags) &&\n\t\t\t\t\t\trdev->raid_disk < 0)\n\t\t\t\t\tmd_kick_rdev_from_array(rdev);\n\t\t\t}\n\t\t}\n\n\t\tif (try_set_sync && !mddev->external && !mddev->in_sync) {\n\t\t\tspin_lock(&mddev->lock);\n\t\t\tset_in_sync(mddev);\n\t\t\tspin_unlock(&mddev->lock);\n\t\t}\n\n\t\tif (mddev->sb_flags)\n\t\t\tmd_update_sb(mddev, 0);\n\n\t\t \n\t\tif (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery)) {\n\t\t\tif (!test_bit(MD_RECOVERY_DONE, &mddev->recovery)) {\n\t\t\t\t \n\t\t\t\tclear_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\t\t\t\tgoto unlock;\n\t\t\t}\n\n\t\t\tif (WARN_ON_ONCE(!mddev->sync_thread))\n\t\t\t\tgoto unlock;\n\n\t\t\tmd_reap_sync_thread(mddev);\n\t\t\tgoto unlock;\n\t\t}\n\n\t\t \n\t\tmddev->curr_resync_completed = 0;\n\t\tspin_lock(&mddev->lock);\n\t\tset_bit(MD_RECOVERY_RUNNING, &mddev->recovery);\n\t\tspin_unlock(&mddev->lock);\n\t\t \n\t\tclear_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\t\tclear_bit(MD_RECOVERY_DONE, &mddev->recovery);\n\n\t\tif (!test_and_clear_bit(MD_RECOVERY_NEEDED, &mddev->recovery) ||\n\t\t    test_bit(MD_RECOVERY_FROZEN, &mddev->recovery))\n\t\t\tgoto not_running;\n\t\t \n\n\t\tif (mddev->reshape_position != MaxSector) {\n\t\t\tif (mddev->pers->check_reshape == NULL ||\n\t\t\t    mddev->pers->check_reshape(mddev) != 0)\n\t\t\t\t \n\t\t\t\tgoto not_running;\n\t\t\tset_bit(MD_RECOVERY_RESHAPE, &mddev->recovery);\n\t\t\tclear_bit(MD_RECOVERY_RECOVER, &mddev->recovery);\n\t\t} else if ((spares = remove_and_add_spares(mddev, NULL))) {\n\t\t\tclear_bit(MD_RECOVERY_SYNC, &mddev->recovery);\n\t\t\tclear_bit(MD_RECOVERY_CHECK, &mddev->recovery);\n\t\t\tclear_bit(MD_RECOVERY_REQUESTED, &mddev->recovery);\n\t\t\tset_bit(MD_RECOVERY_RECOVER, &mddev->recovery);\n\t\t} else if (mddev->recovery_cp < MaxSector) {\n\t\t\tset_bit(MD_RECOVERY_SYNC, &mddev->recovery);\n\t\t\tclear_bit(MD_RECOVERY_RECOVER, &mddev->recovery);\n\t\t} else if (!test_bit(MD_RECOVERY_SYNC, &mddev->recovery))\n\t\t\t \n\t\t\tgoto not_running;\n\n\t\tif (mddev->pers->sync_request) {\n\t\t\tif (spares) {\n\t\t\t\t \n\t\t\t\tmd_bitmap_write_all(mddev->bitmap);\n\t\t\t}\n\t\t\tINIT_WORK(&mddev->del_work, md_start_sync);\n\t\t\tqueue_work(md_misc_wq, &mddev->del_work);\n\t\t\tgoto unlock;\n\t\t}\n\tnot_running:\n\t\tif (!mddev->sync_thread) {\n\t\t\tclear_bit(MD_RECOVERY_RUNNING, &mddev->recovery);\n\t\t\twake_up(&resync_wait);\n\t\t\tif (test_and_clear_bit(MD_RECOVERY_RECOVER,\n\t\t\t\t\t       &mddev->recovery))\n\t\t\t\tif (mddev->sysfs_action)\n\t\t\t\t\tsysfs_notify_dirent_safe(mddev->sysfs_action);\n\t\t}\n\tunlock:\n\t\twake_up(&mddev->sb_wait);\n\t\tmddev_unlock(mddev);\n\t}\n}\nEXPORT_SYMBOL(md_check_recovery);\n\nvoid md_reap_sync_thread(struct mddev *mddev)\n{\n\tstruct md_rdev *rdev;\n\tsector_t old_dev_sectors = mddev->dev_sectors;\n\tbool is_reshaped = false;\n\n\t \n\tmd_unregister_thread(mddev, &mddev->sync_thread);\n\tatomic_inc(&mddev->sync_seq);\n\n\tif (!test_bit(MD_RECOVERY_INTR, &mddev->recovery) &&\n\t    !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery) &&\n\t    mddev->degraded != mddev->raid_disks) {\n\t\t \n\t\t \n\t\tif (mddev->pers->spare_active(mddev)) {\n\t\t\tsysfs_notify_dirent_safe(mddev->sysfs_degraded);\n\t\t\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\t\t}\n\t}\n\tif (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&\n\t    mddev->pers->finish_reshape) {\n\t\tmddev->pers->finish_reshape(mddev);\n\t\tif (mddev_is_clustered(mddev))\n\t\t\tis_reshaped = true;\n\t}\n\n\t \n\tif (!mddev->degraded)\n\t\trdev_for_each(rdev, mddev)\n\t\t\trdev->saved_raid_disk = -1;\n\n\tmd_update_sb(mddev, 1);\n\t \n\tif (test_and_clear_bit(MD_CLUSTER_RESYNC_LOCKED, &mddev->flags))\n\t\tmd_cluster_ops->resync_finish(mddev);\n\tclear_bit(MD_RECOVERY_RUNNING, &mddev->recovery);\n\tclear_bit(MD_RECOVERY_DONE, &mddev->recovery);\n\tclear_bit(MD_RECOVERY_SYNC, &mddev->recovery);\n\tclear_bit(MD_RECOVERY_RESHAPE, &mddev->recovery);\n\tclear_bit(MD_RECOVERY_REQUESTED, &mddev->recovery);\n\tclear_bit(MD_RECOVERY_CHECK, &mddev->recovery);\n\t \n\tif (mddev_is_clustered(mddev) && is_reshaped\n\t\t\t\t      && !test_bit(MD_CLOSING, &mddev->flags))\n\t\tmd_cluster_ops->update_size(mddev, old_dev_sectors);\n\t \n\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\tsysfs_notify_dirent_safe(mddev->sysfs_completed);\n\tsysfs_notify_dirent_safe(mddev->sysfs_action);\n\tmd_new_event();\n\tif (mddev->event_work.func)\n\t\tqueue_work(md_misc_wq, &mddev->event_work);\n\twake_up(&resync_wait);\n}\nEXPORT_SYMBOL(md_reap_sync_thread);\n\nvoid md_wait_for_blocked_rdev(struct md_rdev *rdev, struct mddev *mddev)\n{\n\tsysfs_notify_dirent_safe(rdev->sysfs_state);\n\twait_event_timeout(rdev->blocked_wait,\n\t\t\t   !test_bit(Blocked, &rdev->flags) &&\n\t\t\t   !test_bit(BlockedBadBlocks, &rdev->flags),\n\t\t\t   msecs_to_jiffies(5000));\n\trdev_dec_pending(rdev, mddev);\n}\nEXPORT_SYMBOL(md_wait_for_blocked_rdev);\n\nvoid md_finish_reshape(struct mddev *mddev)\n{\n\t \n\tstruct md_rdev *rdev;\n\n\trdev_for_each(rdev, mddev) {\n\t\tif (rdev->data_offset > rdev->new_data_offset)\n\t\t\trdev->sectors += rdev->data_offset - rdev->new_data_offset;\n\t\telse\n\t\t\trdev->sectors -= rdev->new_data_offset - rdev->data_offset;\n\t\trdev->data_offset = rdev->new_data_offset;\n\t}\n}\nEXPORT_SYMBOL(md_finish_reshape);\n\n \n\n \nint rdev_set_badblocks(struct md_rdev *rdev, sector_t s, int sectors,\n\t\t       int is_new)\n{\n\tstruct mddev *mddev = rdev->mddev;\n\tint rv;\n\tif (is_new)\n\t\ts += rdev->new_data_offset;\n\telse\n\t\ts += rdev->data_offset;\n\trv = badblocks_set(&rdev->badblocks, s, sectors, 0);\n\tif (rv == 0) {\n\t\t \n\t\tif (test_bit(ExternalBbl, &rdev->flags))\n\t\t\tsysfs_notify_dirent_safe(rdev->sysfs_unack_badblocks);\n\t\tsysfs_notify_dirent_safe(rdev->sysfs_state);\n\t\tset_mask_bits(&mddev->sb_flags, 0,\n\t\t\t      BIT(MD_SB_CHANGE_CLEAN) | BIT(MD_SB_CHANGE_PENDING));\n\t\tmd_wakeup_thread(rdev->mddev->thread);\n\t\treturn 1;\n\t} else\n\t\treturn 0;\n}\nEXPORT_SYMBOL_GPL(rdev_set_badblocks);\n\nint rdev_clear_badblocks(struct md_rdev *rdev, sector_t s, int sectors,\n\t\t\t int is_new)\n{\n\tint rv;\n\tif (is_new)\n\t\ts += rdev->new_data_offset;\n\telse\n\t\ts += rdev->data_offset;\n\trv = badblocks_clear(&rdev->badblocks, s, sectors);\n\tif ((rv == 0) && test_bit(ExternalBbl, &rdev->flags))\n\t\tsysfs_notify_dirent_safe(rdev->sysfs_badblocks);\n\treturn rv;\n}\nEXPORT_SYMBOL_GPL(rdev_clear_badblocks);\n\nstatic int md_notify_reboot(struct notifier_block *this,\n\t\t\t    unsigned long code, void *x)\n{\n\tstruct mddev *mddev, *n;\n\tint need_delay = 0;\n\n\tspin_lock(&all_mddevs_lock);\n\tlist_for_each_entry_safe(mddev, n, &all_mddevs, all_mddevs) {\n\t\tif (!mddev_get(mddev))\n\t\t\tcontinue;\n\t\tspin_unlock(&all_mddevs_lock);\n\t\tif (mddev_trylock(mddev)) {\n\t\t\tif (mddev->pers)\n\t\t\t\t__md_stop_writes(mddev);\n\t\t\tif (mddev->persistent)\n\t\t\t\tmddev->safemode = 2;\n\t\t\tmddev_unlock(mddev);\n\t\t}\n\t\tneed_delay = 1;\n\t\tmddev_put(mddev);\n\t\tspin_lock(&all_mddevs_lock);\n\t}\n\tspin_unlock(&all_mddevs_lock);\n\n\t \n\tif (need_delay)\n\t\tmsleep(1000);\n\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block md_notifier = {\n\t.notifier_call\t= md_notify_reboot,\n\t.next\t\t= NULL,\n\t.priority\t= INT_MAX,  \n};\n\nstatic void md_geninit(void)\n{\n\tpr_debug(\"md: sizeof(mdp_super_t) = %d\\n\", (int)sizeof(mdp_super_t));\n\n\tproc_create(\"mdstat\", S_IRUGO, NULL, &mdstat_proc_ops);\n}\n\nstatic int __init md_init(void)\n{\n\tint ret = -ENOMEM;\n\n\tmd_wq = alloc_workqueue(\"md\", WQ_MEM_RECLAIM, 0);\n\tif (!md_wq)\n\t\tgoto err_wq;\n\n\tmd_misc_wq = alloc_workqueue(\"md_misc\", 0, 0);\n\tif (!md_misc_wq)\n\t\tgoto err_misc_wq;\n\n\tmd_bitmap_wq = alloc_workqueue(\"md_bitmap\", WQ_MEM_RECLAIM | WQ_UNBOUND,\n\t\t\t\t       0);\n\tif (!md_bitmap_wq)\n\t\tgoto err_bitmap_wq;\n\n\tret = __register_blkdev(MD_MAJOR, \"md\", md_probe);\n\tif (ret < 0)\n\t\tgoto err_md;\n\n\tret = __register_blkdev(0, \"mdp\", md_probe);\n\tif (ret < 0)\n\t\tgoto err_mdp;\n\tmdp_major = ret;\n\n\tregister_reboot_notifier(&md_notifier);\n\traid_table_header = register_sysctl(\"dev/raid\", raid_table);\n\n\tmd_geninit();\n\treturn 0;\n\nerr_mdp:\n\tunregister_blkdev(MD_MAJOR, \"md\");\nerr_md:\n\tdestroy_workqueue(md_bitmap_wq);\nerr_bitmap_wq:\n\tdestroy_workqueue(md_misc_wq);\nerr_misc_wq:\n\tdestroy_workqueue(md_wq);\nerr_wq:\n\treturn ret;\n}\n\nstatic void check_sb_changes(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tstruct mdp_superblock_1 *sb = page_address(rdev->sb_page);\n\tstruct md_rdev *rdev2, *tmp;\n\tint role, ret;\n\n\t \n\tif (mddev->dev_sectors != le64_to_cpu(sb->size)) {\n\t\tret = mddev->pers->resize(mddev, le64_to_cpu(sb->size));\n\t\tif (ret)\n\t\t\tpr_info(\"md-cluster: resize failed\\n\");\n\t\telse\n\t\t\tmd_bitmap_update_sb(mddev->bitmap);\n\t}\n\n\t \n\trdev_for_each_safe(rdev2, tmp, mddev) {\n\t\tif (test_bit(Faulty, &rdev2->flags))\n\t\t\tcontinue;\n\n\t\t \n\t\trole = le16_to_cpu(sb->dev_roles[rdev2->desc_nr]);\n\n\t\tif (test_bit(Candidate, &rdev2->flags)) {\n\t\t\tif (role == MD_DISK_ROLE_FAULTY) {\n\t\t\t\tpr_info(\"md: Removing Candidate device %pg because add failed\\n\",\n\t\t\t\t\trdev2->bdev);\n\t\t\t\tmd_kick_rdev_from_array(rdev2);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\telse\n\t\t\t\tclear_bit(Candidate, &rdev2->flags);\n\t\t}\n\n\t\tif (role != rdev2->raid_disk) {\n\t\t\t \n\t\t\tif (rdev2->raid_disk == -1 && role != MD_DISK_ROLE_SPARE &&\n\t\t\t    !(le32_to_cpu(sb->feature_map) &\n\t\t\t      MD_FEATURE_RESHAPE_ACTIVE)) {\n\t\t\t\trdev2->saved_raid_disk = role;\n\t\t\t\tret = remove_and_add_spares(mddev, rdev2);\n\t\t\t\tpr_info(\"Activated spare: %pg\\n\",\n\t\t\t\t\trdev2->bdev);\n\t\t\t\t \n\t\t\t\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\t\t\t\tmd_wakeup_thread(mddev->thread);\n\t\t\t}\n\t\t\t \n\t\t\tif (role == MD_DISK_ROLE_FAULTY ||\n\t\t\t    role == MD_DISK_ROLE_JOURNAL) {\n\t\t\t\tmd_error(mddev, rdev2);\n\t\t\t\tclear_bit(Blocked, &rdev2->flags);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (mddev->raid_disks != le32_to_cpu(sb->raid_disks)) {\n\t\tret = update_raid_disks(mddev, le32_to_cpu(sb->raid_disks));\n\t\tif (ret)\n\t\t\tpr_warn(\"md: updating array disks failed. %d\\n\", ret);\n\t}\n\n\t \n\tif (test_bit(MD_RESYNCING_REMOTE, &mddev->recovery) &&\n\t    (le32_to_cpu(sb->feature_map) & MD_FEATURE_RESHAPE_ACTIVE)) {\n\t\t \n\t\tmddev->reshape_position = le64_to_cpu(sb->reshape_position);\n\t\tif (mddev->pers->update_reshape_pos)\n\t\t\tmddev->pers->update_reshape_pos(mddev);\n\t\tif (mddev->pers->start_reshape)\n\t\t\tmddev->pers->start_reshape(mddev);\n\t} else if (test_bit(MD_RESYNCING_REMOTE, &mddev->recovery) &&\n\t\t   mddev->reshape_position != MaxSector &&\n\t\t   !(le32_to_cpu(sb->feature_map) & MD_FEATURE_RESHAPE_ACTIVE)) {\n\t\t \n\t\tmddev->reshape_position = MaxSector;\n\t\tif (mddev->pers->update_reshape_pos)\n\t\t\tmddev->pers->update_reshape_pos(mddev);\n\t}\n\n\t \n\tmddev->events = le64_to_cpu(sb->events);\n}\n\nstatic int read_rdev(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tint err;\n\tstruct page *swapout = rdev->sb_page;\n\tstruct mdp_superblock_1 *sb;\n\n\t \n\trdev->sb_page = NULL;\n\terr = alloc_disk_sb(rdev);\n\tif (err == 0) {\n\t\tClearPageUptodate(rdev->sb_page);\n\t\trdev->sb_loaded = 0;\n\t\terr = super_types[mddev->major_version].\n\t\t\tload_super(rdev, NULL, mddev->minor_version);\n\t}\n\tif (err < 0) {\n\t\tpr_warn(\"%s: %d Could not reload rdev(%d) err: %d. Restoring old values\\n\",\n\t\t\t\t__func__, __LINE__, rdev->desc_nr, err);\n\t\tif (rdev->sb_page)\n\t\t\tput_page(rdev->sb_page);\n\t\trdev->sb_page = swapout;\n\t\trdev->sb_loaded = 1;\n\t\treturn err;\n\t}\n\n\tsb = page_address(rdev->sb_page);\n\t \n\n\tif ((le32_to_cpu(sb->feature_map) & MD_FEATURE_RECOVERY_OFFSET))\n\t\trdev->recovery_offset = le64_to_cpu(sb->recovery_offset);\n\n\t \n\tif (rdev->recovery_offset == MaxSector &&\n\t    !test_bit(In_sync, &rdev->flags) &&\n\t    mddev->pers->spare_active(mddev))\n\t\tsysfs_notify_dirent_safe(mddev->sysfs_degraded);\n\n\tput_page(swapout);\n\treturn 0;\n}\n\nvoid md_reload_sb(struct mddev *mddev, int nr)\n{\n\tstruct md_rdev *rdev = NULL, *iter;\n\tint err;\n\n\t \n\trdev_for_each_rcu(iter, mddev) {\n\t\tif (iter->desc_nr == nr) {\n\t\t\trdev = iter;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!rdev) {\n\t\tpr_warn(\"%s: %d Could not find rdev with nr %d\\n\", __func__, __LINE__, nr);\n\t\treturn;\n\t}\n\n\terr = read_rdev(mddev, rdev);\n\tif (err < 0)\n\t\treturn;\n\n\tcheck_sb_changes(mddev, rdev);\n\n\t \n\trdev_for_each_rcu(rdev, mddev) {\n\t\tif (!test_bit(Faulty, &rdev->flags))\n\t\t\tread_rdev(mddev, rdev);\n\t}\n}\nEXPORT_SYMBOL(md_reload_sb);\n\n#ifndef MODULE\n\n \n\nstatic DEFINE_MUTEX(detected_devices_mutex);\nstatic LIST_HEAD(all_detected_devices);\nstruct detected_devices_node {\n\tstruct list_head list;\n\tdev_t dev;\n};\n\nvoid md_autodetect_dev(dev_t dev)\n{\n\tstruct detected_devices_node *node_detected_dev;\n\n\tnode_detected_dev = kzalloc(sizeof(*node_detected_dev), GFP_KERNEL);\n\tif (node_detected_dev) {\n\t\tnode_detected_dev->dev = dev;\n\t\tmutex_lock(&detected_devices_mutex);\n\t\tlist_add_tail(&node_detected_dev->list, &all_detected_devices);\n\t\tmutex_unlock(&detected_devices_mutex);\n\t}\n}\n\nvoid md_autostart_arrays(int part)\n{\n\tstruct md_rdev *rdev;\n\tstruct detected_devices_node *node_detected_dev;\n\tdev_t dev;\n\tint i_scanned, i_passed;\n\n\ti_scanned = 0;\n\ti_passed = 0;\n\n\tpr_info(\"md: Autodetecting RAID arrays.\\n\");\n\n\tmutex_lock(&detected_devices_mutex);\n\twhile (!list_empty(&all_detected_devices) && i_scanned < INT_MAX) {\n\t\ti_scanned++;\n\t\tnode_detected_dev = list_entry(all_detected_devices.next,\n\t\t\t\t\tstruct detected_devices_node, list);\n\t\tlist_del(&node_detected_dev->list);\n\t\tdev = node_detected_dev->dev;\n\t\tkfree(node_detected_dev);\n\t\tmutex_unlock(&detected_devices_mutex);\n\t\trdev = md_import_device(dev,0, 90);\n\t\tmutex_lock(&detected_devices_mutex);\n\t\tif (IS_ERR(rdev))\n\t\t\tcontinue;\n\n\t\tif (test_bit(Faulty, &rdev->flags))\n\t\t\tcontinue;\n\n\t\tset_bit(AutoDetected, &rdev->flags);\n\t\tlist_add(&rdev->same_set, &pending_raid_disks);\n\t\ti_passed++;\n\t}\n\tmutex_unlock(&detected_devices_mutex);\n\n\tpr_debug(\"md: Scanned %d and added %d devices.\\n\", i_scanned, i_passed);\n\n\tautorun_devices(part);\n}\n\n#endif  \n\nstatic __exit void md_exit(void)\n{\n\tstruct mddev *mddev, *n;\n\tint delay = 1;\n\n\tunregister_blkdev(MD_MAJOR,\"md\");\n\tunregister_blkdev(mdp_major, \"mdp\");\n\tunregister_reboot_notifier(&md_notifier);\n\tunregister_sysctl_table(raid_table_header);\n\n\t \n\tmd_unloading = 1;\n\twhile (waitqueue_active(&md_event_waiters)) {\n\t\t \n\t\twake_up(&md_event_waiters);\n\t\tmsleep(delay);\n\t\tdelay += delay;\n\t}\n\tremove_proc_entry(\"mdstat\", NULL);\n\n\tspin_lock(&all_mddevs_lock);\n\tlist_for_each_entry_safe(mddev, n, &all_mddevs, all_mddevs) {\n\t\tif (!mddev_get(mddev))\n\t\t\tcontinue;\n\t\tspin_unlock(&all_mddevs_lock);\n\t\texport_array(mddev);\n\t\tmddev->ctime = 0;\n\t\tmddev->hold_active = 0;\n\t\t \n\t\tmddev_put(mddev);\n\t\tspin_lock(&all_mddevs_lock);\n\t}\n\tspin_unlock(&all_mddevs_lock);\n\n\tdestroy_workqueue(md_misc_wq);\n\tdestroy_workqueue(md_bitmap_wq);\n\tdestroy_workqueue(md_wq);\n}\n\nsubsys_initcall(md_init);\nmodule_exit(md_exit)\n\nstatic int get_ro(char *buffer, const struct kernel_param *kp)\n{\n\treturn sprintf(buffer, \"%d\\n\", start_readonly);\n}\nstatic int set_ro(const char *val, const struct kernel_param *kp)\n{\n\treturn kstrtouint(val, 10, (unsigned int *)&start_readonly);\n}\n\nmodule_param_call(start_ro, set_ro, get_ro, NULL, S_IRUSR|S_IWUSR);\nmodule_param(start_dirty_degraded, int, S_IRUGO|S_IWUSR);\nmodule_param_call(new_array, add_named_array, NULL, NULL, S_IWUSR);\nmodule_param(create_on_open, bool, S_IRUSR|S_IWUSR);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"MD RAID framework\");\nMODULE_ALIAS(\"md\");\nMODULE_ALIAS_BLOCKDEV_MAJOR(MD_MAJOR);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}