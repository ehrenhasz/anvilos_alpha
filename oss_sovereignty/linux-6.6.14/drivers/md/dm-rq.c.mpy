{
  "module_name": "dm-rq.c",
  "hash_id": "60ed5d717ca937e17370d420b0f0305c70829d3304a2a4ffb7dbe03e46394201",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-rq.c",
  "human_readable_source": "\n \n\n#include \"dm-core.h\"\n#include \"dm-rq.h\"\n\n#include <linux/blk-mq.h>\n\n#define DM_MSG_PREFIX \"core-rq\"\n\n \nstruct dm_rq_target_io {\n\tstruct mapped_device *md;\n\tstruct dm_target *ti;\n\tstruct request *orig, *clone;\n\tstruct kthread_work work;\n\tblk_status_t error;\n\tunion map_info info;\n\tstruct dm_stats_aux stats_aux;\n\tunsigned long duration_jiffies;\n\tunsigned int n_sectors;\n\tunsigned int completed;\n};\n\n#define DM_MQ_NR_HW_QUEUES 1\n#define DM_MQ_QUEUE_DEPTH 2048\nstatic unsigned int dm_mq_nr_hw_queues = DM_MQ_NR_HW_QUEUES;\nstatic unsigned int dm_mq_queue_depth = DM_MQ_QUEUE_DEPTH;\n\n \n#define RESERVED_REQUEST_BASED_IOS\t256\nstatic unsigned int reserved_rq_based_ios = RESERVED_REQUEST_BASED_IOS;\n\nunsigned int dm_get_reserved_rq_based_ios(void)\n{\n\treturn __dm_get_module_param(&reserved_rq_based_ios,\n\t\t\t\t     RESERVED_REQUEST_BASED_IOS, DM_RESERVED_MAX_IOS);\n}\n\nstatic unsigned int dm_get_blk_mq_nr_hw_queues(void)\n{\n\treturn __dm_get_module_param(&dm_mq_nr_hw_queues, 1, 32);\n}\n\nstatic unsigned int dm_get_blk_mq_queue_depth(void)\n{\n\treturn __dm_get_module_param(&dm_mq_queue_depth,\n\t\t\t\t     DM_MQ_QUEUE_DEPTH, BLK_MQ_MAX_DEPTH);\n}\n\nint dm_request_based(struct mapped_device *md)\n{\n\treturn queue_is_mq(md->queue);\n}\n\nvoid dm_start_queue(struct request_queue *q)\n{\n\tblk_mq_unquiesce_queue(q);\n\tblk_mq_kick_requeue_list(q);\n}\n\nvoid dm_stop_queue(struct request_queue *q)\n{\n\tblk_mq_quiesce_queue(q);\n}\n\n \nstatic void end_clone_bio(struct bio *clone)\n{\n\tstruct dm_rq_clone_bio_info *info =\n\t\tcontainer_of(clone, struct dm_rq_clone_bio_info, clone);\n\tstruct dm_rq_target_io *tio = info->tio;\n\tunsigned int nr_bytes = info->orig->bi_iter.bi_size;\n\tblk_status_t error = clone->bi_status;\n\tbool is_last = !clone->bi_next;\n\n\tbio_put(clone);\n\n\tif (tio->error)\n\t\t \n\t\treturn;\n\telse if (error) {\n\t\t \n\t\ttio->error = error;\n\t\tgoto exit;\n\t}\n\n\t \n\ttio->completed += nr_bytes;\n\n\t \n\tif (is_last)\n exit:\n\t\tblk_update_request(tio->orig, BLK_STS_OK, tio->completed);\n}\n\nstatic struct dm_rq_target_io *tio_from_request(struct request *rq)\n{\n\treturn blk_mq_rq_to_pdu(rq);\n}\n\nstatic void rq_end_stats(struct mapped_device *md, struct request *orig)\n{\n\tif (unlikely(dm_stats_used(&md->stats))) {\n\t\tstruct dm_rq_target_io *tio = tio_from_request(orig);\n\n\t\ttio->duration_jiffies = jiffies - tio->duration_jiffies;\n\t\tdm_stats_account_io(&md->stats, rq_data_dir(orig),\n\t\t\t\t    blk_rq_pos(orig), tio->n_sectors, true,\n\t\t\t\t    tio->duration_jiffies, &tio->stats_aux);\n\t}\n}\n\n \nstatic void rq_completed(struct mapped_device *md)\n{\n\t \n\tdm_put(md);\n}\n\n \nstatic void dm_end_request(struct request *clone, blk_status_t error)\n{\n\tstruct dm_rq_target_io *tio = clone->end_io_data;\n\tstruct mapped_device *md = tio->md;\n\tstruct request *rq = tio->orig;\n\n\tblk_rq_unprep_clone(clone);\n\ttio->ti->type->release_clone_rq(clone, NULL);\n\n\trq_end_stats(md, rq);\n\tblk_mq_end_request(rq, error);\n\trq_completed(md);\n}\n\nstatic void __dm_mq_kick_requeue_list(struct request_queue *q, unsigned long msecs)\n{\n\tblk_mq_delay_kick_requeue_list(q, msecs);\n}\n\nvoid dm_mq_kick_requeue_list(struct mapped_device *md)\n{\n\t__dm_mq_kick_requeue_list(md->queue, 0);\n}\nEXPORT_SYMBOL(dm_mq_kick_requeue_list);\n\nstatic void dm_mq_delay_requeue_request(struct request *rq, unsigned long msecs)\n{\n\tblk_mq_requeue_request(rq, false);\n\t__dm_mq_kick_requeue_list(rq->q, msecs);\n}\n\nstatic void dm_requeue_original_request(struct dm_rq_target_io *tio, bool delay_requeue)\n{\n\tstruct mapped_device *md = tio->md;\n\tstruct request *rq = tio->orig;\n\tunsigned long delay_ms = delay_requeue ? 100 : 0;\n\n\trq_end_stats(md, rq);\n\tif (tio->clone) {\n\t\tblk_rq_unprep_clone(tio->clone);\n\t\ttio->ti->type->release_clone_rq(tio->clone, NULL);\n\t}\n\n\tdm_mq_delay_requeue_request(rq, delay_ms);\n\trq_completed(md);\n}\n\nstatic void dm_done(struct request *clone, blk_status_t error, bool mapped)\n{\n\tint r = DM_ENDIO_DONE;\n\tstruct dm_rq_target_io *tio = clone->end_io_data;\n\tdm_request_endio_fn rq_end_io = NULL;\n\n\tif (tio->ti) {\n\t\trq_end_io = tio->ti->type->rq_end_io;\n\n\t\tif (mapped && rq_end_io)\n\t\t\tr = rq_end_io(tio->ti, clone, error, &tio->info);\n\t}\n\n\tif (unlikely(error == BLK_STS_TARGET)) {\n\t\tif (req_op(clone) == REQ_OP_DISCARD &&\n\t\t    !clone->q->limits.max_discard_sectors)\n\t\t\tdisable_discard(tio->md);\n\t\telse if (req_op(clone) == REQ_OP_WRITE_ZEROES &&\n\t\t\t !clone->q->limits.max_write_zeroes_sectors)\n\t\t\tdisable_write_zeroes(tio->md);\n\t}\n\n\tswitch (r) {\n\tcase DM_ENDIO_DONE:\n\t\t \n\t\tdm_end_request(clone, error);\n\t\tbreak;\n\tcase DM_ENDIO_INCOMPLETE:\n\t\t \n\t\treturn;\n\tcase DM_ENDIO_REQUEUE:\n\t\t \n\t\tdm_requeue_original_request(tio, false);\n\t\tbreak;\n\tcase DM_ENDIO_DELAY_REQUEUE:\n\t\t \n\t\tdm_requeue_original_request(tio, true);\n\t\tbreak;\n\tdefault:\n\t\tDMCRIT(\"unimplemented target endio return value: %d\", r);\n\t\tBUG();\n\t}\n}\n\n \nstatic void dm_softirq_done(struct request *rq)\n{\n\tbool mapped = true;\n\tstruct dm_rq_target_io *tio = tio_from_request(rq);\n\tstruct request *clone = tio->clone;\n\n\tif (!clone) {\n\t\tstruct mapped_device *md = tio->md;\n\n\t\trq_end_stats(md, rq);\n\t\tblk_mq_end_request(rq, tio->error);\n\t\trq_completed(md);\n\t\treturn;\n\t}\n\n\tif (rq->rq_flags & RQF_FAILED)\n\t\tmapped = false;\n\n\tdm_done(clone, tio->error, mapped);\n}\n\n \nstatic void dm_complete_request(struct request *rq, blk_status_t error)\n{\n\tstruct dm_rq_target_io *tio = tio_from_request(rq);\n\n\ttio->error = error;\n\tif (likely(!blk_should_fake_timeout(rq->q)))\n\t\tblk_mq_complete_request(rq);\n}\n\n \nstatic void dm_kill_unmapped_request(struct request *rq, blk_status_t error)\n{\n\trq->rq_flags |= RQF_FAILED;\n\tdm_complete_request(rq, error);\n}\n\nstatic enum rq_end_io_ret end_clone_request(struct request *clone,\n\t\t\t\t\t    blk_status_t error)\n{\n\tstruct dm_rq_target_io *tio = clone->end_io_data;\n\n\tdm_complete_request(tio->orig, error);\n\treturn RQ_END_IO_NONE;\n}\n\nstatic int dm_rq_bio_constructor(struct bio *bio, struct bio *bio_orig,\n\t\t\t\t void *data)\n{\n\tstruct dm_rq_target_io *tio = data;\n\tstruct dm_rq_clone_bio_info *info =\n\t\tcontainer_of(bio, struct dm_rq_clone_bio_info, clone);\n\n\tinfo->orig = bio_orig;\n\tinfo->tio = tio;\n\tbio->bi_end_io = end_clone_bio;\n\n\treturn 0;\n}\n\nstatic int setup_clone(struct request *clone, struct request *rq,\n\t\t       struct dm_rq_target_io *tio, gfp_t gfp_mask)\n{\n\tint r;\n\n\tr = blk_rq_prep_clone(clone, rq, &tio->md->mempools->bs, gfp_mask,\n\t\t\t      dm_rq_bio_constructor, tio);\n\tif (r)\n\t\treturn r;\n\n\tclone->end_io = end_clone_request;\n\tclone->end_io_data = tio;\n\n\ttio->clone = clone;\n\n\treturn 0;\n}\n\nstatic void init_tio(struct dm_rq_target_io *tio, struct request *rq,\n\t\t     struct mapped_device *md)\n{\n\ttio->md = md;\n\ttio->ti = NULL;\n\ttio->clone = NULL;\n\ttio->orig = rq;\n\ttio->error = 0;\n\ttio->completed = 0;\n\t \n\tif (!md->init_tio_pdu)\n\t\tmemset(&tio->info, 0, sizeof(tio->info));\n}\n\n \nstatic int map_request(struct dm_rq_target_io *tio)\n{\n\tint r;\n\tstruct dm_target *ti = tio->ti;\n\tstruct mapped_device *md = tio->md;\n\tstruct request *rq = tio->orig;\n\tstruct request *clone = NULL;\n\tblk_status_t ret;\n\n\tr = ti->type->clone_and_map_rq(ti, rq, &tio->info, &clone);\n\tswitch (r) {\n\tcase DM_MAPIO_SUBMITTED:\n\t\t \n\t\tbreak;\n\tcase DM_MAPIO_REMAPPED:\n\t\tif (setup_clone(clone, rq, tio, GFP_ATOMIC)) {\n\t\t\t \n\t\t\tti->type->release_clone_rq(clone, &tio->info);\n\t\t\treturn DM_MAPIO_REQUEUE;\n\t\t}\n\n\t\t \n\t\ttrace_block_rq_remap(clone, disk_devt(dm_disk(md)),\n\t\t\t\t     blk_rq_pos(rq));\n\t\tret = blk_insert_cloned_request(clone);\n\t\tswitch (ret) {\n\t\tcase BLK_STS_OK:\n\t\t\tbreak;\n\t\tcase BLK_STS_RESOURCE:\n\t\tcase BLK_STS_DEV_RESOURCE:\n\t\t\tblk_rq_unprep_clone(clone);\n\t\t\tblk_mq_cleanup_rq(clone);\n\t\t\ttio->ti->type->release_clone_rq(clone, &tio->info);\n\t\t\ttio->clone = NULL;\n\t\t\treturn DM_MAPIO_REQUEUE;\n\t\tdefault:\n\t\t\t \n\t\t\tdm_complete_request(rq, ret);\n\t\t}\n\t\tbreak;\n\tcase DM_MAPIO_REQUEUE:\n\t\t \n\t\tbreak;\n\tcase DM_MAPIO_DELAY_REQUEUE:\n\t\t \n\t\tdm_requeue_original_request(tio, true);\n\t\tbreak;\n\tcase DM_MAPIO_KILL:\n\t\t \n\t\tdm_kill_unmapped_request(rq, BLK_STS_IOERR);\n\t\tbreak;\n\tdefault:\n\t\tDMCRIT(\"unimplemented target map return value: %d\", r);\n\t\tBUG();\n\t}\n\n\treturn r;\n}\n\n \nssize_t dm_attr_rq_based_seq_io_merge_deadline_show(struct mapped_device *md, char *buf)\n{\n\treturn sprintf(buf, \"%u\\n\", 0);\n}\n\nssize_t dm_attr_rq_based_seq_io_merge_deadline_store(struct mapped_device *md,\n\t\t\t\t\t\t     const char *buf, size_t count)\n{\n\treturn count;\n}\n\nstatic void dm_start_request(struct mapped_device *md, struct request *orig)\n{\n\tblk_mq_start_request(orig);\n\n\tif (unlikely(dm_stats_used(&md->stats))) {\n\t\tstruct dm_rq_target_io *tio = tio_from_request(orig);\n\n\t\ttio->duration_jiffies = jiffies;\n\t\ttio->n_sectors = blk_rq_sectors(orig);\n\t\tdm_stats_account_io(&md->stats, rq_data_dir(orig),\n\t\t\t\t    blk_rq_pos(orig), tio->n_sectors, false, 0,\n\t\t\t\t    &tio->stats_aux);\n\t}\n\n\t \n\tdm_get(md);\n}\n\nstatic int dm_mq_init_request(struct blk_mq_tag_set *set, struct request *rq,\n\t\t\t      unsigned int hctx_idx, unsigned int numa_node)\n{\n\tstruct mapped_device *md = set->driver_data;\n\tstruct dm_rq_target_io *tio = blk_mq_rq_to_pdu(rq);\n\n\t \n\ttio->md = md;\n\n\tif (md->init_tio_pdu) {\n\t\t \n\t\ttio->info.ptr = tio + 1;\n\t}\n\n\treturn 0;\n}\n\nstatic blk_status_t dm_mq_queue_rq(struct blk_mq_hw_ctx *hctx,\n\t\t\t  const struct blk_mq_queue_data *bd)\n{\n\tstruct request *rq = bd->rq;\n\tstruct dm_rq_target_io *tio = blk_mq_rq_to_pdu(rq);\n\tstruct mapped_device *md = tio->md;\n\tstruct dm_target *ti = md->immutable_target;\n\n\t \n\tif (unlikely(test_bit(DMF_BLOCK_IO_FOR_SUSPEND, &md->flags)))\n\t\treturn BLK_STS_RESOURCE;\n\n\tif (unlikely(!ti)) {\n\t\tint srcu_idx;\n\t\tstruct dm_table *map;\n\n\t\tmap = dm_get_live_table(md, &srcu_idx);\n\t\tif (unlikely(!map)) {\n\t\t\tdm_put_live_table(md, srcu_idx);\n\t\t\treturn BLK_STS_RESOURCE;\n\t\t}\n\t\tti = dm_table_find_target(map, 0);\n\t\tdm_put_live_table(md, srcu_idx);\n\t}\n\n\tif (ti->type->busy && ti->type->busy(ti))\n\t\treturn BLK_STS_RESOURCE;\n\n\tdm_start_request(md, rq);\n\n\t \n\tinit_tio(tio, rq, md);\n\n\t \n\ttio->ti = ti;\n\n\t \n\tif (map_request(tio) == DM_MAPIO_REQUEUE) {\n\t\t \n\t\trq_end_stats(md, rq);\n\t\trq_completed(md);\n\t\treturn BLK_STS_RESOURCE;\n\t}\n\n\treturn BLK_STS_OK;\n}\n\nstatic const struct blk_mq_ops dm_mq_ops = {\n\t.queue_rq = dm_mq_queue_rq,\n\t.complete = dm_softirq_done,\n\t.init_request = dm_mq_init_request,\n};\n\nint dm_mq_init_request_queue(struct mapped_device *md, struct dm_table *t)\n{\n\tstruct dm_target *immutable_tgt;\n\tint err;\n\n\tmd->tag_set = kzalloc_node(sizeof(struct blk_mq_tag_set), GFP_KERNEL, md->numa_node_id);\n\tif (!md->tag_set)\n\t\treturn -ENOMEM;\n\n\tmd->tag_set->ops = &dm_mq_ops;\n\tmd->tag_set->queue_depth = dm_get_blk_mq_queue_depth();\n\tmd->tag_set->numa_node = md->numa_node_id;\n\tmd->tag_set->flags = BLK_MQ_F_SHOULD_MERGE | BLK_MQ_F_STACKING;\n\tmd->tag_set->nr_hw_queues = dm_get_blk_mq_nr_hw_queues();\n\tmd->tag_set->driver_data = md;\n\n\tmd->tag_set->cmd_size = sizeof(struct dm_rq_target_io);\n\timmutable_tgt = dm_table_get_immutable_target(t);\n\tif (immutable_tgt && immutable_tgt->per_io_data_size) {\n\t\t \n\t\tmd->tag_set->cmd_size += immutable_tgt->per_io_data_size;\n\t\tmd->init_tio_pdu = true;\n\t}\n\n\terr = blk_mq_alloc_tag_set(md->tag_set);\n\tif (err)\n\t\tgoto out_kfree_tag_set;\n\n\terr = blk_mq_init_allocated_queue(md->tag_set, md->queue);\n\tif (err)\n\t\tgoto out_tag_set;\n\treturn 0;\n\nout_tag_set:\n\tblk_mq_free_tag_set(md->tag_set);\nout_kfree_tag_set:\n\tkfree(md->tag_set);\n\tmd->tag_set = NULL;\n\n\treturn err;\n}\n\nvoid dm_mq_cleanup_mapped_device(struct mapped_device *md)\n{\n\tif (md->tag_set) {\n\t\tblk_mq_free_tag_set(md->tag_set);\n\t\tkfree(md->tag_set);\n\t\tmd->tag_set = NULL;\n\t}\n}\n\nmodule_param(reserved_rq_based_ios, uint, 0644);\nMODULE_PARM_DESC(reserved_rq_based_ios, \"Reserved IOs in request-based mempools\");\n\n \nstatic bool use_blk_mq = true;\nmodule_param(use_blk_mq, bool, 0644);\nMODULE_PARM_DESC(use_blk_mq, \"Use block multiqueue for request-based DM devices\");\n\nmodule_param(dm_mq_nr_hw_queues, uint, 0644);\nMODULE_PARM_DESC(dm_mq_nr_hw_queues, \"Number of hardware queues for request-based dm-mq devices\");\n\nmodule_param(dm_mq_queue_depth, uint, 0644);\nMODULE_PARM_DESC(dm_mq_queue_depth, \"Queue depth for request-based dm-mq devices\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}