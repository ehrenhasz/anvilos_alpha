{
  "module_name": "dm-stats.c",
  "hash_id": "2b5d18794d45d2c58ed328a4f8253e23b9395cb6c04b44a6bb479c384b75288e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-stats.c",
  "human_readable_source": "\n#include <linux/errno.h>\n#include <linux/numa.h>\n#include <linux/slab.h>\n#include <linux/rculist.h>\n#include <linux/threads.h>\n#include <linux/preempt.h>\n#include <linux/irqflags.h>\n#include <linux/vmalloc.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/device-mapper.h>\n\n#include \"dm-core.h\"\n#include \"dm-stats.h\"\n\n#define DM_MSG_PREFIX \"stats\"\n\nstatic int dm_stat_need_rcu_barrier;\n\n \nstruct dm_stat_percpu {\n\tunsigned long long sectors[2];\n\tunsigned long long ios[2];\n\tunsigned long long merges[2];\n\tunsigned long long ticks[2];\n\tunsigned long long io_ticks[2];\n\tunsigned long long io_ticks_total;\n\tunsigned long long time_in_queue;\n\tunsigned long long *histogram;\n};\n\nstruct dm_stat_shared {\n\tatomic_t in_flight[2];\n\tunsigned long long stamp;\n\tstruct dm_stat_percpu tmp;\n};\n\nstruct dm_stat {\n\tstruct list_head list_entry;\n\tint id;\n\tunsigned int stat_flags;\n\tsize_t n_entries;\n\tsector_t start;\n\tsector_t end;\n\tsector_t step;\n\tunsigned int n_histogram_entries;\n\tunsigned long long *histogram_boundaries;\n\tconst char *program_id;\n\tconst char *aux_data;\n\tstruct rcu_head rcu_head;\n\tsize_t shared_alloc_size;\n\tsize_t percpu_alloc_size;\n\tsize_t histogram_alloc_size;\n\tstruct dm_stat_percpu *stat_percpu[NR_CPUS];\n\tstruct dm_stat_shared stat_shared[];\n};\n\n#define STAT_PRECISE_TIMESTAMPS\t\t1\n\nstruct dm_stats_last_position {\n\tsector_t last_sector;\n\tunsigned int last_rw;\n};\n\n \n#define DM_STATS_MEMORY_FACTOR\t\t4\n#define DM_STATS_VMALLOC_FACTOR\t\t2\n\nstatic DEFINE_SPINLOCK(shared_memory_lock);\n\nstatic unsigned long shared_memory_amount;\n\nstatic bool __check_shared_memory(size_t alloc_size)\n{\n\tsize_t a;\n\n\ta = shared_memory_amount + alloc_size;\n\tif (a < shared_memory_amount)\n\t\treturn false;\n\tif (a >> PAGE_SHIFT > totalram_pages() / DM_STATS_MEMORY_FACTOR)\n\t\treturn false;\n#ifdef CONFIG_MMU\n\tif (a > (VMALLOC_END - VMALLOC_START) / DM_STATS_VMALLOC_FACTOR)\n\t\treturn false;\n#endif\n\treturn true;\n}\n\nstatic bool check_shared_memory(size_t alloc_size)\n{\n\tbool ret;\n\n\tspin_lock_irq(&shared_memory_lock);\n\n\tret = __check_shared_memory(alloc_size);\n\n\tspin_unlock_irq(&shared_memory_lock);\n\n\treturn ret;\n}\n\nstatic bool claim_shared_memory(size_t alloc_size)\n{\n\tspin_lock_irq(&shared_memory_lock);\n\n\tif (!__check_shared_memory(alloc_size)) {\n\t\tspin_unlock_irq(&shared_memory_lock);\n\t\treturn false;\n\t}\n\n\tshared_memory_amount += alloc_size;\n\n\tspin_unlock_irq(&shared_memory_lock);\n\n\treturn true;\n}\n\nstatic void free_shared_memory(size_t alloc_size)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&shared_memory_lock, flags);\n\n\tif (WARN_ON_ONCE(shared_memory_amount < alloc_size)) {\n\t\tspin_unlock_irqrestore(&shared_memory_lock, flags);\n\t\tDMCRIT(\"Memory usage accounting bug.\");\n\t\treturn;\n\t}\n\n\tshared_memory_amount -= alloc_size;\n\n\tspin_unlock_irqrestore(&shared_memory_lock, flags);\n}\n\nstatic void *dm_kvzalloc(size_t alloc_size, int node)\n{\n\tvoid *p;\n\n\tif (!claim_shared_memory(alloc_size))\n\t\treturn NULL;\n\n\tp = kvzalloc_node(alloc_size, GFP_KERNEL | __GFP_NOMEMALLOC, node);\n\tif (p)\n\t\treturn p;\n\n\tfree_shared_memory(alloc_size);\n\n\treturn NULL;\n}\n\nstatic void dm_kvfree(void *ptr, size_t alloc_size)\n{\n\tif (!ptr)\n\t\treturn;\n\n\tfree_shared_memory(alloc_size);\n\n\tkvfree(ptr);\n}\n\nstatic void dm_stat_free(struct rcu_head *head)\n{\n\tint cpu;\n\tstruct dm_stat *s = container_of(head, struct dm_stat, rcu_head);\n\n\tkfree(s->histogram_boundaries);\n\tkfree(s->program_id);\n\tkfree(s->aux_data);\n\tfor_each_possible_cpu(cpu) {\n\t\tdm_kvfree(s->stat_percpu[cpu][0].histogram, s->histogram_alloc_size);\n\t\tdm_kvfree(s->stat_percpu[cpu], s->percpu_alloc_size);\n\t}\n\tdm_kvfree(s->stat_shared[0].tmp.histogram, s->histogram_alloc_size);\n\tdm_kvfree(s, s->shared_alloc_size);\n}\n\nstatic int dm_stat_in_flight(struct dm_stat_shared *shared)\n{\n\treturn atomic_read(&shared->in_flight[READ]) +\n\t       atomic_read(&shared->in_flight[WRITE]);\n}\n\nint dm_stats_init(struct dm_stats *stats)\n{\n\tint cpu;\n\tstruct dm_stats_last_position *last;\n\n\tmutex_init(&stats->mutex);\n\tINIT_LIST_HEAD(&stats->list);\n\tstats->precise_timestamps = false;\n\tstats->last = alloc_percpu(struct dm_stats_last_position);\n\tif (!stats->last)\n\t\treturn -ENOMEM;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tlast = per_cpu_ptr(stats->last, cpu);\n\t\tlast->last_sector = (sector_t)ULLONG_MAX;\n\t\tlast->last_rw = UINT_MAX;\n\t}\n\n\treturn 0;\n}\n\nvoid dm_stats_cleanup(struct dm_stats *stats)\n{\n\tsize_t ni;\n\tstruct dm_stat *s;\n\tstruct dm_stat_shared *shared;\n\n\twhile (!list_empty(&stats->list)) {\n\t\ts = container_of(stats->list.next, struct dm_stat, list_entry);\n\t\tlist_del(&s->list_entry);\n\t\tfor (ni = 0; ni < s->n_entries; ni++) {\n\t\t\tshared = &s->stat_shared[ni];\n\t\t\tif (WARN_ON(dm_stat_in_flight(shared))) {\n\t\t\t\tDMCRIT(\"leaked in-flight counter at index %lu \"\n\t\t\t\t       \"(start %llu, end %llu, step %llu): reads %d, writes %d\",\n\t\t\t\t       (unsigned long)ni,\n\t\t\t\t       (unsigned long long)s->start,\n\t\t\t\t       (unsigned long long)s->end,\n\t\t\t\t       (unsigned long long)s->step,\n\t\t\t\t       atomic_read(&shared->in_flight[READ]),\n\t\t\t\t       atomic_read(&shared->in_flight[WRITE]));\n\t\t\t}\n\t\t\tcond_resched();\n\t\t}\n\t\tdm_stat_free(&s->rcu_head);\n\t}\n\tfree_percpu(stats->last);\n\tmutex_destroy(&stats->mutex);\n}\n\nstatic void dm_stats_recalc_precise_timestamps(struct dm_stats *stats)\n{\n\tstruct list_head *l;\n\tstruct dm_stat *tmp_s;\n\tbool precise_timestamps = false;\n\n\tlist_for_each(l, &stats->list) {\n\t\ttmp_s = container_of(l, struct dm_stat, list_entry);\n\t\tif (tmp_s->stat_flags & STAT_PRECISE_TIMESTAMPS) {\n\t\t\tprecise_timestamps = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tstats->precise_timestamps = precise_timestamps;\n}\n\nstatic int dm_stats_create(struct dm_stats *stats, sector_t start, sector_t end,\n\t\t\t   sector_t step, unsigned int stat_flags,\n\t\t\t   unsigned int n_histogram_entries,\n\t\t\t   unsigned long long *histogram_boundaries,\n\t\t\t   const char *program_id, const char *aux_data,\n\t\t\t   void (*suspend_callback)(struct mapped_device *),\n\t\t\t   void (*resume_callback)(struct mapped_device *),\n\t\t\t   struct mapped_device *md)\n{\n\tstruct list_head *l;\n\tstruct dm_stat *s, *tmp_s;\n\tsector_t n_entries;\n\tsize_t ni;\n\tsize_t shared_alloc_size;\n\tsize_t percpu_alloc_size;\n\tsize_t histogram_alloc_size;\n\tstruct dm_stat_percpu *p;\n\tint cpu;\n\tint ret_id;\n\tint r;\n\n\tif (end < start || !step)\n\t\treturn -EINVAL;\n\n\tn_entries = end - start;\n\tif (dm_sector_div64(n_entries, step))\n\t\tn_entries++;\n\n\tif (n_entries != (size_t)n_entries || !(size_t)(n_entries + 1))\n\t\treturn -EOVERFLOW;\n\n\tshared_alloc_size = struct_size(s, stat_shared, n_entries);\n\tif ((shared_alloc_size - sizeof(struct dm_stat)) / sizeof(struct dm_stat_shared) != n_entries)\n\t\treturn -EOVERFLOW;\n\n\tpercpu_alloc_size = (size_t)n_entries * sizeof(struct dm_stat_percpu);\n\tif (percpu_alloc_size / sizeof(struct dm_stat_percpu) != n_entries)\n\t\treturn -EOVERFLOW;\n\n\thistogram_alloc_size = (n_histogram_entries + 1) * (size_t)n_entries * sizeof(unsigned long long);\n\tif (histogram_alloc_size / (n_histogram_entries + 1) != (size_t)n_entries * sizeof(unsigned long long))\n\t\treturn -EOVERFLOW;\n\n\tif (!check_shared_memory(shared_alloc_size + histogram_alloc_size +\n\t\t\t\t num_possible_cpus() * (percpu_alloc_size + histogram_alloc_size)))\n\t\treturn -ENOMEM;\n\n\ts = dm_kvzalloc(shared_alloc_size, NUMA_NO_NODE);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\ts->stat_flags = stat_flags;\n\ts->n_entries = n_entries;\n\ts->start = start;\n\ts->end = end;\n\ts->step = step;\n\ts->shared_alloc_size = shared_alloc_size;\n\ts->percpu_alloc_size = percpu_alloc_size;\n\ts->histogram_alloc_size = histogram_alloc_size;\n\n\ts->n_histogram_entries = n_histogram_entries;\n\ts->histogram_boundaries = kmemdup(histogram_boundaries,\n\t\t\t\t\t  s->n_histogram_entries * sizeof(unsigned long long), GFP_KERNEL);\n\tif (!s->histogram_boundaries) {\n\t\tr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\ts->program_id = kstrdup(program_id, GFP_KERNEL);\n\tif (!s->program_id) {\n\t\tr = -ENOMEM;\n\t\tgoto out;\n\t}\n\ts->aux_data = kstrdup(aux_data, GFP_KERNEL);\n\tif (!s->aux_data) {\n\t\tr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfor (ni = 0; ni < n_entries; ni++) {\n\t\tatomic_set(&s->stat_shared[ni].in_flight[READ], 0);\n\t\tatomic_set(&s->stat_shared[ni].in_flight[WRITE], 0);\n\t\tcond_resched();\n\t}\n\n\tif (s->n_histogram_entries) {\n\t\tunsigned long long *hi;\n\n\t\thi = dm_kvzalloc(s->histogram_alloc_size, NUMA_NO_NODE);\n\t\tif (!hi) {\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tfor (ni = 0; ni < n_entries; ni++) {\n\t\t\ts->stat_shared[ni].tmp.histogram = hi;\n\t\t\thi += s->n_histogram_entries + 1;\n\t\t\tcond_resched();\n\t\t}\n\t}\n\n\tfor_each_possible_cpu(cpu) {\n\t\tp = dm_kvzalloc(percpu_alloc_size, cpu_to_node(cpu));\n\t\tif (!p) {\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\ts->stat_percpu[cpu] = p;\n\t\tif (s->n_histogram_entries) {\n\t\t\tunsigned long long *hi;\n\n\t\t\thi = dm_kvzalloc(s->histogram_alloc_size, cpu_to_node(cpu));\n\t\t\tif (!hi) {\n\t\t\t\tr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tfor (ni = 0; ni < n_entries; ni++) {\n\t\t\t\tp[ni].histogram = hi;\n\t\t\t\thi += s->n_histogram_entries + 1;\n\t\t\t\tcond_resched();\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tsuspend_callback(md);\n\n\tmutex_lock(&stats->mutex);\n\ts->id = 0;\n\tlist_for_each(l, &stats->list) {\n\t\ttmp_s = container_of(l, struct dm_stat, list_entry);\n\t\tif (WARN_ON(tmp_s->id < s->id)) {\n\t\t\tr = -EINVAL;\n\t\t\tgoto out_unlock_resume;\n\t\t}\n\t\tif (tmp_s->id > s->id)\n\t\t\tbreak;\n\t\tif (unlikely(s->id == INT_MAX)) {\n\t\t\tr = -ENFILE;\n\t\t\tgoto out_unlock_resume;\n\t\t}\n\t\ts->id++;\n\t}\n\tret_id = s->id;\n\tlist_add_tail_rcu(&s->list_entry, l);\n\n\tdm_stats_recalc_precise_timestamps(stats);\n\n\tif (!static_key_enabled(&stats_enabled.key))\n\t\tstatic_branch_enable(&stats_enabled);\n\n\tmutex_unlock(&stats->mutex);\n\n\tresume_callback(md);\n\n\treturn ret_id;\n\nout_unlock_resume:\n\tmutex_unlock(&stats->mutex);\n\tresume_callback(md);\nout:\n\tdm_stat_free(&s->rcu_head);\n\treturn r;\n}\n\nstatic struct dm_stat *__dm_stats_find(struct dm_stats *stats, int id)\n{\n\tstruct dm_stat *s;\n\n\tlist_for_each_entry(s, &stats->list, list_entry) {\n\t\tif (s->id > id)\n\t\t\tbreak;\n\t\tif (s->id == id)\n\t\t\treturn s;\n\t}\n\n\treturn NULL;\n}\n\nstatic int dm_stats_delete(struct dm_stats *stats, int id)\n{\n\tstruct dm_stat *s;\n\tint cpu;\n\n\tmutex_lock(&stats->mutex);\n\n\ts = __dm_stats_find(stats, id);\n\tif (!s) {\n\t\tmutex_unlock(&stats->mutex);\n\t\treturn -ENOENT;\n\t}\n\n\tlist_del_rcu(&s->list_entry);\n\n\tdm_stats_recalc_precise_timestamps(stats);\n\n\tmutex_unlock(&stats->mutex);\n\n\t \n\tfor_each_possible_cpu(cpu)\n\t\tif (is_vmalloc_addr(s->stat_percpu) ||\n\t\t    is_vmalloc_addr(s->stat_percpu[cpu][0].histogram))\n\t\t\tgoto do_sync_free;\n\tif (is_vmalloc_addr(s) ||\n\t    is_vmalloc_addr(s->stat_shared[0].tmp.histogram)) {\ndo_sync_free:\n\t\tsynchronize_rcu_expedited();\n\t\tdm_stat_free(&s->rcu_head);\n\t} else {\n\t\tWRITE_ONCE(dm_stat_need_rcu_barrier, 1);\n\t\tcall_rcu(&s->rcu_head, dm_stat_free);\n\t}\n\treturn 0;\n}\n\nstatic int dm_stats_list(struct dm_stats *stats, const char *program,\n\t\t\t char *result, unsigned int maxlen)\n{\n\tstruct dm_stat *s;\n\tsector_t len;\n\tunsigned int sz = 0;\n\n\t \n\n\tmutex_lock(&stats->mutex);\n\tlist_for_each_entry(s, &stats->list, list_entry) {\n\t\tif (!program || !strcmp(program, s->program_id)) {\n\t\t\tlen = s->end - s->start;\n\t\t\tDMEMIT(\"%d: %llu+%llu %llu %s %s\", s->id,\n\t\t\t\t(unsigned long long)s->start,\n\t\t\t\t(unsigned long long)len,\n\t\t\t\t(unsigned long long)s->step,\n\t\t\t\ts->program_id,\n\t\t\t\ts->aux_data);\n\t\t\tif (s->stat_flags & STAT_PRECISE_TIMESTAMPS)\n\t\t\t\tDMEMIT(\" precise_timestamps\");\n\t\t\tif (s->n_histogram_entries) {\n\t\t\t\tunsigned int i;\n\n\t\t\t\tDMEMIT(\" histogram:\");\n\t\t\t\tfor (i = 0; i < s->n_histogram_entries; i++) {\n\t\t\t\t\tif (i)\n\t\t\t\t\t\tDMEMIT(\",\");\n\t\t\t\t\tDMEMIT(\"%llu\", s->histogram_boundaries[i]);\n\t\t\t\t}\n\t\t\t}\n\t\t\tDMEMIT(\"\\n\");\n\t\t}\n\t\tcond_resched();\n\t}\n\tmutex_unlock(&stats->mutex);\n\n\treturn 1;\n}\n\nstatic void dm_stat_round(struct dm_stat *s, struct dm_stat_shared *shared,\n\t\t\t  struct dm_stat_percpu *p)\n{\n\t \n\tunsigned long long now, difference;\n\tunsigned int in_flight_read, in_flight_write;\n\n\tif (likely(!(s->stat_flags & STAT_PRECISE_TIMESTAMPS)))\n\t\tnow = jiffies;\n\telse\n\t\tnow = ktime_to_ns(ktime_get());\n\n\tdifference = now - shared->stamp;\n\tif (!difference)\n\t\treturn;\n\n\tin_flight_read = (unsigned int)atomic_read(&shared->in_flight[READ]);\n\tin_flight_write = (unsigned int)atomic_read(&shared->in_flight[WRITE]);\n\tif (in_flight_read)\n\t\tp->io_ticks[READ] += difference;\n\tif (in_flight_write)\n\t\tp->io_ticks[WRITE] += difference;\n\tif (in_flight_read + in_flight_write) {\n\t\tp->io_ticks_total += difference;\n\t\tp->time_in_queue += (in_flight_read + in_flight_write) * difference;\n\t}\n\tshared->stamp = now;\n}\n\nstatic void dm_stat_for_entry(struct dm_stat *s, size_t entry,\n\t\t\t      int idx, sector_t len,\n\t\t\t      struct dm_stats_aux *stats_aux, bool end,\n\t\t\t      unsigned long duration_jiffies)\n{\n\tstruct dm_stat_shared *shared = &s->stat_shared[entry];\n\tstruct dm_stat_percpu *p;\n\n\t \n#if BITS_PER_LONG == 32\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n#else\n\tpreempt_disable();\n#endif\n\tp = &s->stat_percpu[smp_processor_id()][entry];\n\n\tif (!end) {\n\t\tdm_stat_round(s, shared, p);\n\t\tatomic_inc(&shared->in_flight[idx]);\n\t} else {\n\t\tunsigned long long duration;\n\n\t\tdm_stat_round(s, shared, p);\n\t\tatomic_dec(&shared->in_flight[idx]);\n\t\tp->sectors[idx] += len;\n\t\tp->ios[idx] += 1;\n\t\tp->merges[idx] += stats_aux->merged;\n\t\tif (!(s->stat_flags & STAT_PRECISE_TIMESTAMPS)) {\n\t\t\tp->ticks[idx] += duration_jiffies;\n\t\t\tduration = jiffies_to_msecs(duration_jiffies);\n\t\t} else {\n\t\t\tp->ticks[idx] += stats_aux->duration_ns;\n\t\t\tduration = stats_aux->duration_ns;\n\t\t}\n\t\tif (s->n_histogram_entries) {\n\t\t\tunsigned int lo = 0, hi = s->n_histogram_entries + 1;\n\n\t\t\twhile (lo + 1 < hi) {\n\t\t\t\tunsigned int mid = (lo + hi) / 2;\n\n\t\t\t\tif (s->histogram_boundaries[mid - 1] > duration)\n\t\t\t\t\thi = mid;\n\t\t\t\telse\n\t\t\t\t\tlo = mid;\n\t\t\t}\n\t\t\tp->histogram[lo]++;\n\t\t}\n\t}\n\n#if BITS_PER_LONG == 32\n\tlocal_irq_restore(flags);\n#else\n\tpreempt_enable();\n#endif\n}\n\nstatic void __dm_stat_bio(struct dm_stat *s, int bi_rw,\n\t\t\t  sector_t bi_sector, sector_t end_sector,\n\t\t\t  bool end, unsigned long duration_jiffies,\n\t\t\t  struct dm_stats_aux *stats_aux)\n{\n\tsector_t rel_sector, offset, todo, fragment_len;\n\tsize_t entry;\n\n\tif (end_sector <= s->start || bi_sector >= s->end)\n\t\treturn;\n\tif (unlikely(bi_sector < s->start)) {\n\t\trel_sector = 0;\n\t\ttodo = end_sector - s->start;\n\t} else {\n\t\trel_sector = bi_sector - s->start;\n\t\ttodo = end_sector - bi_sector;\n\t}\n\tif (unlikely(end_sector > s->end))\n\t\ttodo -= (end_sector - s->end);\n\n\toffset = dm_sector_div64(rel_sector, s->step);\n\tentry = rel_sector;\n\tdo {\n\t\tif (WARN_ON_ONCE(entry >= s->n_entries)) {\n\t\t\tDMCRIT(\"Invalid area access in region id %d\", s->id);\n\t\t\treturn;\n\t\t}\n\t\tfragment_len = todo;\n\t\tif (fragment_len > s->step - offset)\n\t\t\tfragment_len = s->step - offset;\n\t\tdm_stat_for_entry(s, entry, bi_rw, fragment_len,\n\t\t\t\t  stats_aux, end, duration_jiffies);\n\t\ttodo -= fragment_len;\n\t\tentry++;\n\t\toffset = 0;\n\t} while (unlikely(todo != 0));\n}\n\nvoid dm_stats_account_io(struct dm_stats *stats, unsigned long bi_rw,\n\t\t\t sector_t bi_sector, unsigned int bi_sectors, bool end,\n\t\t\t unsigned long start_time,\n\t\t\t struct dm_stats_aux *stats_aux)\n{\n\tstruct dm_stat *s;\n\tsector_t end_sector;\n\tstruct dm_stats_last_position *last;\n\tbool got_precise_time;\n\tunsigned long duration_jiffies = 0;\n\n\tif (unlikely(!bi_sectors))\n\t\treturn;\n\n\tend_sector = bi_sector + bi_sectors;\n\n\tif (!end) {\n\t\t \n\t\tlast = raw_cpu_ptr(stats->last);\n\t\tstats_aux->merged =\n\t\t\t(bi_sector == (READ_ONCE(last->last_sector) &&\n\t\t\t\t       ((bi_rw == WRITE) ==\n\t\t\t\t\t(READ_ONCE(last->last_rw) == WRITE))\n\t\t\t\t       ));\n\t\tWRITE_ONCE(last->last_sector, end_sector);\n\t\tWRITE_ONCE(last->last_rw, bi_rw);\n\t} else\n\t\tduration_jiffies = jiffies - start_time;\n\n\trcu_read_lock();\n\n\tgot_precise_time = false;\n\tlist_for_each_entry_rcu(s, &stats->list, list_entry) {\n\t\tif (s->stat_flags & STAT_PRECISE_TIMESTAMPS && !got_precise_time) {\n\t\t\t \n\t\t\tif (end)\n\t\t\t\tstats_aux->duration_ns = ktime_to_ns(ktime_get()) - stats_aux->duration_ns;\n\t\t\tgot_precise_time = true;\n\t\t}\n\t\t__dm_stat_bio(s, bi_rw, bi_sector, end_sector, end, duration_jiffies, stats_aux);\n\t}\n\n\trcu_read_unlock();\n}\n\nstatic void __dm_stat_init_temporary_percpu_totals(struct dm_stat_shared *shared,\n\t\t\t\t\t\t   struct dm_stat *s, size_t x)\n{\n\tint cpu;\n\tstruct dm_stat_percpu *p;\n\n\tlocal_irq_disable();\n\tp = &s->stat_percpu[smp_processor_id()][x];\n\tdm_stat_round(s, shared, p);\n\tlocal_irq_enable();\n\n\tshared->tmp.sectors[READ] = 0;\n\tshared->tmp.sectors[WRITE] = 0;\n\tshared->tmp.ios[READ] = 0;\n\tshared->tmp.ios[WRITE] = 0;\n\tshared->tmp.merges[READ] = 0;\n\tshared->tmp.merges[WRITE] = 0;\n\tshared->tmp.ticks[READ] = 0;\n\tshared->tmp.ticks[WRITE] = 0;\n\tshared->tmp.io_ticks[READ] = 0;\n\tshared->tmp.io_ticks[WRITE] = 0;\n\tshared->tmp.io_ticks_total = 0;\n\tshared->tmp.time_in_queue = 0;\n\n\tif (s->n_histogram_entries)\n\t\tmemset(shared->tmp.histogram, 0, (s->n_histogram_entries + 1) * sizeof(unsigned long long));\n\n\tfor_each_possible_cpu(cpu) {\n\t\tp = &s->stat_percpu[cpu][x];\n\t\tshared->tmp.sectors[READ] += READ_ONCE(p->sectors[READ]);\n\t\tshared->tmp.sectors[WRITE] += READ_ONCE(p->sectors[WRITE]);\n\t\tshared->tmp.ios[READ] += READ_ONCE(p->ios[READ]);\n\t\tshared->tmp.ios[WRITE] += READ_ONCE(p->ios[WRITE]);\n\t\tshared->tmp.merges[READ] += READ_ONCE(p->merges[READ]);\n\t\tshared->tmp.merges[WRITE] += READ_ONCE(p->merges[WRITE]);\n\t\tshared->tmp.ticks[READ] += READ_ONCE(p->ticks[READ]);\n\t\tshared->tmp.ticks[WRITE] += READ_ONCE(p->ticks[WRITE]);\n\t\tshared->tmp.io_ticks[READ] += READ_ONCE(p->io_ticks[READ]);\n\t\tshared->tmp.io_ticks[WRITE] += READ_ONCE(p->io_ticks[WRITE]);\n\t\tshared->tmp.io_ticks_total += READ_ONCE(p->io_ticks_total);\n\t\tshared->tmp.time_in_queue += READ_ONCE(p->time_in_queue);\n\t\tif (s->n_histogram_entries) {\n\t\t\tunsigned int i;\n\n\t\t\tfor (i = 0; i < s->n_histogram_entries + 1; i++)\n\t\t\t\tshared->tmp.histogram[i] += READ_ONCE(p->histogram[i]);\n\t\t}\n\t}\n}\n\nstatic void __dm_stat_clear(struct dm_stat *s, size_t idx_start, size_t idx_end,\n\t\t\t    bool init_tmp_percpu_totals)\n{\n\tsize_t x;\n\tstruct dm_stat_shared *shared;\n\tstruct dm_stat_percpu *p;\n\n\tfor (x = idx_start; x < idx_end; x++) {\n\t\tshared = &s->stat_shared[x];\n\t\tif (init_tmp_percpu_totals)\n\t\t\t__dm_stat_init_temporary_percpu_totals(shared, s, x);\n\t\tlocal_irq_disable();\n\t\tp = &s->stat_percpu[smp_processor_id()][x];\n\t\tp->sectors[READ] -= shared->tmp.sectors[READ];\n\t\tp->sectors[WRITE] -= shared->tmp.sectors[WRITE];\n\t\tp->ios[READ] -= shared->tmp.ios[READ];\n\t\tp->ios[WRITE] -= shared->tmp.ios[WRITE];\n\t\tp->merges[READ] -= shared->tmp.merges[READ];\n\t\tp->merges[WRITE] -= shared->tmp.merges[WRITE];\n\t\tp->ticks[READ] -= shared->tmp.ticks[READ];\n\t\tp->ticks[WRITE] -= shared->tmp.ticks[WRITE];\n\t\tp->io_ticks[READ] -= shared->tmp.io_ticks[READ];\n\t\tp->io_ticks[WRITE] -= shared->tmp.io_ticks[WRITE];\n\t\tp->io_ticks_total -= shared->tmp.io_ticks_total;\n\t\tp->time_in_queue -= shared->tmp.time_in_queue;\n\t\tlocal_irq_enable();\n\t\tif (s->n_histogram_entries) {\n\t\t\tunsigned int i;\n\n\t\t\tfor (i = 0; i < s->n_histogram_entries + 1; i++) {\n\t\t\t\tlocal_irq_disable();\n\t\t\t\tp = &s->stat_percpu[smp_processor_id()][x];\n\t\t\t\tp->histogram[i] -= shared->tmp.histogram[i];\n\t\t\t\tlocal_irq_enable();\n\t\t\t}\n\t\t}\n\t\tcond_resched();\n\t}\n}\n\nstatic int dm_stats_clear(struct dm_stats *stats, int id)\n{\n\tstruct dm_stat *s;\n\n\tmutex_lock(&stats->mutex);\n\n\ts = __dm_stats_find(stats, id);\n\tif (!s) {\n\t\tmutex_unlock(&stats->mutex);\n\t\treturn -ENOENT;\n\t}\n\n\t__dm_stat_clear(s, 0, s->n_entries, true);\n\n\tmutex_unlock(&stats->mutex);\n\n\treturn 1;\n}\n\n \nstatic unsigned long long dm_jiffies_to_msec64(struct dm_stat *s, unsigned long long j)\n{\n\tunsigned long long result;\n\tunsigned int mult;\n\n\tif (s->stat_flags & STAT_PRECISE_TIMESTAMPS)\n\t\treturn j;\n\n\tresult = 0;\n\tif (j)\n\t\tresult = jiffies_to_msecs(j & 0x3fffff);\n\tif (j >= 1 << 22) {\n\t\tmult = jiffies_to_msecs(1 << 22);\n\t\tresult += (unsigned long long)mult * (unsigned long long)jiffies_to_msecs((j >> 22) & 0x3fffff);\n\t}\n\tif (j >= 1ULL << 44)\n\t\tresult += (unsigned long long)mult * (unsigned long long)mult * (unsigned long long)jiffies_to_msecs(j >> 44);\n\n\treturn result;\n}\n\nstatic int dm_stats_print(struct dm_stats *stats, int id,\n\t\t\t  size_t idx_start, size_t idx_len,\n\t\t\t  bool clear, char *result, unsigned int maxlen)\n{\n\tunsigned int sz = 0;\n\tstruct dm_stat *s;\n\tsize_t x;\n\tsector_t start, end, step;\n\tsize_t idx_end;\n\tstruct dm_stat_shared *shared;\n\n\t \n\n\tmutex_lock(&stats->mutex);\n\n\ts = __dm_stats_find(stats, id);\n\tif (!s) {\n\t\tmutex_unlock(&stats->mutex);\n\t\treturn -ENOENT;\n\t}\n\n\tidx_end = idx_start + idx_len;\n\tif (idx_end < idx_start ||\n\t    idx_end > s->n_entries)\n\t\tidx_end = s->n_entries;\n\n\tif (idx_start > idx_end)\n\t\tidx_start = idx_end;\n\n\tstep = s->step;\n\tstart = s->start + (step * idx_start);\n\n\tfor (x = idx_start; x < idx_end; x++, start = end) {\n\t\tshared = &s->stat_shared[x];\n\t\tend = start + step;\n\t\tif (unlikely(end > s->end))\n\t\t\tend = s->end;\n\n\t\t__dm_stat_init_temporary_percpu_totals(shared, s, x);\n\n\t\tDMEMIT(\"%llu+%llu %llu %llu %llu %llu %llu %llu %llu %llu %d %llu %llu %llu %llu\",\n\t\t       (unsigned long long)start,\n\t\t       (unsigned long long)step,\n\t\t       shared->tmp.ios[READ],\n\t\t       shared->tmp.merges[READ],\n\t\t       shared->tmp.sectors[READ],\n\t\t       dm_jiffies_to_msec64(s, shared->tmp.ticks[READ]),\n\t\t       shared->tmp.ios[WRITE],\n\t\t       shared->tmp.merges[WRITE],\n\t\t       shared->tmp.sectors[WRITE],\n\t\t       dm_jiffies_to_msec64(s, shared->tmp.ticks[WRITE]),\n\t\t       dm_stat_in_flight(shared),\n\t\t       dm_jiffies_to_msec64(s, shared->tmp.io_ticks_total),\n\t\t       dm_jiffies_to_msec64(s, shared->tmp.time_in_queue),\n\t\t       dm_jiffies_to_msec64(s, shared->tmp.io_ticks[READ]),\n\t\t       dm_jiffies_to_msec64(s, shared->tmp.io_ticks[WRITE]));\n\t\tif (s->n_histogram_entries) {\n\t\t\tunsigned int i;\n\n\t\t\tfor (i = 0; i < s->n_histogram_entries + 1; i++)\n\t\t\t\tDMEMIT(\"%s%llu\", !i ? \" \" : \":\", shared->tmp.histogram[i]);\n\t\t}\n\t\tDMEMIT(\"\\n\");\n\n\t\tif (unlikely(sz + 1 >= maxlen))\n\t\t\tgoto buffer_overflow;\n\n\t\tcond_resched();\n\t}\n\n\tif (clear)\n\t\t__dm_stat_clear(s, idx_start, idx_end, false);\n\nbuffer_overflow:\n\tmutex_unlock(&stats->mutex);\n\n\treturn 1;\n}\n\nstatic int dm_stats_set_aux(struct dm_stats *stats, int id, const char *aux_data)\n{\n\tstruct dm_stat *s;\n\tconst char *new_aux_data;\n\n\tmutex_lock(&stats->mutex);\n\n\ts = __dm_stats_find(stats, id);\n\tif (!s) {\n\t\tmutex_unlock(&stats->mutex);\n\t\treturn -ENOENT;\n\t}\n\n\tnew_aux_data = kstrdup(aux_data, GFP_KERNEL);\n\tif (!new_aux_data) {\n\t\tmutex_unlock(&stats->mutex);\n\t\treturn -ENOMEM;\n\t}\n\n\tkfree(s->aux_data);\n\ts->aux_data = new_aux_data;\n\n\tmutex_unlock(&stats->mutex);\n\n\treturn 0;\n}\n\nstatic int parse_histogram(const char *h, unsigned int *n_histogram_entries,\n\t\t\t   unsigned long long **histogram_boundaries)\n{\n\tconst char *q;\n\tunsigned int n;\n\tunsigned long long last;\n\n\t*n_histogram_entries = 1;\n\tfor (q = h; *q; q++)\n\t\tif (*q == ',')\n\t\t\t(*n_histogram_entries)++;\n\n\t*histogram_boundaries = kmalloc_array(*n_histogram_entries,\n\t\t\t\t\t      sizeof(unsigned long long),\n\t\t\t\t\t      GFP_KERNEL);\n\tif (!*histogram_boundaries)\n\t\treturn -ENOMEM;\n\n\tn = 0;\n\tlast = 0;\n\twhile (1) {\n\t\tunsigned long long hi;\n\t\tint s;\n\t\tchar ch;\n\n\t\ts = sscanf(h, \"%llu%c\", &hi, &ch);\n\t\tif (!s || (s == 2 && ch != ','))\n\t\t\treturn -EINVAL;\n\t\tif (hi <= last)\n\t\t\treturn -EINVAL;\n\t\tlast = hi;\n\t\t(*histogram_boundaries)[n] = hi;\n\t\tif (s == 1)\n\t\t\treturn 0;\n\t\th = strchr(h, ',') + 1;\n\t\tn++;\n\t}\n}\n\nstatic int message_stats_create(struct mapped_device *md,\n\t\t\t\tunsigned int argc, char **argv,\n\t\t\t\tchar *result, unsigned int maxlen)\n{\n\tint r;\n\tint id;\n\tchar dummy;\n\tunsigned long long start, end, len, step;\n\tunsigned int divisor;\n\tconst char *program_id, *aux_data;\n\tunsigned int stat_flags = 0;\n\tunsigned int n_histogram_entries = 0;\n\tunsigned long long *histogram_boundaries = NULL;\n\tstruct dm_arg_set as, as_backup;\n\tconst char *a;\n\tunsigned int feature_args;\n\n\t \n\n\tif (argc < 3)\n\t\tgoto ret_einval;\n\n\tas.argc = argc;\n\tas.argv = argv;\n\tdm_consume_args(&as, 1);\n\n\ta = dm_shift_arg(&as);\n\tif (!strcmp(a, \"-\")) {\n\t\tstart = 0;\n\t\tlen = dm_get_size(md);\n\t\tif (!len)\n\t\t\tlen = 1;\n\t} else if (sscanf(a, \"%llu+%llu%c\", &start, &len, &dummy) != 2 ||\n\t\t   start != (sector_t)start || len != (sector_t)len)\n\t\tgoto ret_einval;\n\n\tend = start + len;\n\tif (start >= end)\n\t\tgoto ret_einval;\n\n\ta = dm_shift_arg(&as);\n\tif (sscanf(a, \"/%u%c\", &divisor, &dummy) == 1) {\n\t\tif (!divisor)\n\t\t\treturn -EINVAL;\n\t\tstep = end - start;\n\t\tif (do_div(step, divisor))\n\t\t\tstep++;\n\t\tif (!step)\n\t\t\tstep = 1;\n\t} else if (sscanf(a, \"%llu%c\", &step, &dummy) != 1 ||\n\t\t   step != (sector_t)step || !step)\n\t\tgoto ret_einval;\n\n\tas_backup = as;\n\ta = dm_shift_arg(&as);\n\tif (a && sscanf(a, \"%u%c\", &feature_args, &dummy) == 1) {\n\t\twhile (feature_args--) {\n\t\t\ta = dm_shift_arg(&as);\n\t\t\tif (!a)\n\t\t\t\tgoto ret_einval;\n\t\t\tif (!strcasecmp(a, \"precise_timestamps\"))\n\t\t\t\tstat_flags |= STAT_PRECISE_TIMESTAMPS;\n\t\t\telse if (!strncasecmp(a, \"histogram:\", 10)) {\n\t\t\t\tif (n_histogram_entries)\n\t\t\t\t\tgoto ret_einval;\n\t\t\t\tr = parse_histogram(a + 10, &n_histogram_entries, &histogram_boundaries);\n\t\t\t\tif (r)\n\t\t\t\t\tgoto ret;\n\t\t\t} else\n\t\t\t\tgoto ret_einval;\n\t\t}\n\t} else {\n\t\tas = as_backup;\n\t}\n\n\tprogram_id = \"-\";\n\taux_data = \"-\";\n\n\ta = dm_shift_arg(&as);\n\tif (a)\n\t\tprogram_id = a;\n\n\ta = dm_shift_arg(&as);\n\tif (a)\n\t\taux_data = a;\n\n\tif (as.argc)\n\t\tgoto ret_einval;\n\n\t \n\tsnprintf(result, maxlen, \"%d\", INT_MAX);\n\tif (dm_message_test_buffer_overflow(result, maxlen)) {\n\t\tr = 1;\n\t\tgoto ret;\n\t}\n\n\tid = dm_stats_create(dm_get_stats(md), start, end, step, stat_flags,\n\t\t\t     n_histogram_entries, histogram_boundaries, program_id, aux_data,\n\t\t\t     dm_internal_suspend_fast, dm_internal_resume_fast, md);\n\tif (id < 0) {\n\t\tr = id;\n\t\tgoto ret;\n\t}\n\n\tsnprintf(result, maxlen, \"%d\", id);\n\n\tr = 1;\n\tgoto ret;\n\nret_einval:\n\tr = -EINVAL;\nret:\n\tkfree(histogram_boundaries);\n\treturn r;\n}\n\nstatic int message_stats_delete(struct mapped_device *md,\n\t\t\t\tunsigned int argc, char **argv)\n{\n\tint id;\n\tchar dummy;\n\n\tif (argc != 2)\n\t\treturn -EINVAL;\n\n\tif (sscanf(argv[1], \"%d%c\", &id, &dummy) != 1 || id < 0)\n\t\treturn -EINVAL;\n\n\treturn dm_stats_delete(dm_get_stats(md), id);\n}\n\nstatic int message_stats_clear(struct mapped_device *md,\n\t\t\t       unsigned int argc, char **argv)\n{\n\tint id;\n\tchar dummy;\n\n\tif (argc != 2)\n\t\treturn -EINVAL;\n\n\tif (sscanf(argv[1], \"%d%c\", &id, &dummy) != 1 || id < 0)\n\t\treturn -EINVAL;\n\n\treturn dm_stats_clear(dm_get_stats(md), id);\n}\n\nstatic int message_stats_list(struct mapped_device *md,\n\t\t\t      unsigned int argc, char **argv,\n\t\t\t      char *result, unsigned int maxlen)\n{\n\tint r;\n\tconst char *program = NULL;\n\n\tif (argc < 1 || argc > 2)\n\t\treturn -EINVAL;\n\n\tif (argc > 1) {\n\t\tprogram = kstrdup(argv[1], GFP_KERNEL);\n\t\tif (!program)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tr = dm_stats_list(dm_get_stats(md), program, result, maxlen);\n\n\tkfree(program);\n\n\treturn r;\n}\n\nstatic int message_stats_print(struct mapped_device *md,\n\t\t\t       unsigned int argc, char **argv, bool clear,\n\t\t\t       char *result, unsigned int maxlen)\n{\n\tint id;\n\tchar dummy;\n\tunsigned long idx_start = 0, idx_len = ULONG_MAX;\n\n\tif (argc != 2 && argc != 4)\n\t\treturn -EINVAL;\n\n\tif (sscanf(argv[1], \"%d%c\", &id, &dummy) != 1 || id < 0)\n\t\treturn -EINVAL;\n\n\tif (argc > 3) {\n\t\tif (strcmp(argv[2], \"-\") &&\n\t\t    sscanf(argv[2], \"%lu%c\", &idx_start, &dummy) != 1)\n\t\t\treturn -EINVAL;\n\t\tif (strcmp(argv[3], \"-\") &&\n\t\t    sscanf(argv[3], \"%lu%c\", &idx_len, &dummy) != 1)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn dm_stats_print(dm_get_stats(md), id, idx_start, idx_len, clear,\n\t\t\t      result, maxlen);\n}\n\nstatic int message_stats_set_aux(struct mapped_device *md,\n\t\t\t\t unsigned int argc, char **argv)\n{\n\tint id;\n\tchar dummy;\n\n\tif (argc != 3)\n\t\treturn -EINVAL;\n\n\tif (sscanf(argv[1], \"%d%c\", &id, &dummy) != 1 || id < 0)\n\t\treturn -EINVAL;\n\n\treturn dm_stats_set_aux(dm_get_stats(md), id, argv[2]);\n}\n\nint dm_stats_message(struct mapped_device *md, unsigned int argc, char **argv,\n\t\t     char *result, unsigned int maxlen)\n{\n\tint r;\n\n\t \n\tif (!strcasecmp(argv[0], \"@stats_create\"))\n\t\tr = message_stats_create(md, argc, argv, result, maxlen);\n\telse if (!strcasecmp(argv[0], \"@stats_delete\"))\n\t\tr = message_stats_delete(md, argc, argv);\n\telse if (!strcasecmp(argv[0], \"@stats_clear\"))\n\t\tr = message_stats_clear(md, argc, argv);\n\telse if (!strcasecmp(argv[0], \"@stats_list\"))\n\t\tr = message_stats_list(md, argc, argv, result, maxlen);\n\telse if (!strcasecmp(argv[0], \"@stats_print\"))\n\t\tr = message_stats_print(md, argc, argv, false, result, maxlen);\n\telse if (!strcasecmp(argv[0], \"@stats_print_clear\"))\n\t\tr = message_stats_print(md, argc, argv, true, result, maxlen);\n\telse if (!strcasecmp(argv[0], \"@stats_set_aux\"))\n\t\tr = message_stats_set_aux(md, argc, argv);\n\telse\n\t\treturn 2;  \n\n\tif (r == -EINVAL)\n\t\tDMCRIT(\"Invalid parameters for message %s\", argv[0]);\n\n\treturn r;\n}\n\nint __init dm_statistics_init(void)\n{\n\tshared_memory_amount = 0;\n\tdm_stat_need_rcu_barrier = 0;\n\treturn 0;\n}\n\nvoid dm_statistics_exit(void)\n{\n\tif (dm_stat_need_rcu_barrier)\n\t\trcu_barrier();\n\tif (WARN_ON(shared_memory_amount))\n\t\tDMCRIT(\"shared_memory_amount leaked: %lu\", shared_memory_amount);\n}\n\nmodule_param_named(stats_current_allocated_bytes, shared_memory_amount, ulong, 0444);\nMODULE_PARM_DESC(stats_current_allocated_bytes, \"Memory currently used by statistics\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}