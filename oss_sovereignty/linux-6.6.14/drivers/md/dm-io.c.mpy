{
  "module_name": "dm-io.c",
  "hash_id": "e771988befc9a3b9d300b42510d018369f8594a93c957aa218075f372179be86",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-io.c",
  "human_readable_source": "\n \n\n#include \"dm-core.h\"\n\n#include <linux/device-mapper.h>\n\n#include <linux/bio.h>\n#include <linux/completion.h>\n#include <linux/mempool.h>\n#include <linux/module.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/dm-io.h>\n\n#define DM_MSG_PREFIX \"io\"\n\n#define DM_IO_MAX_REGIONS\tBITS_PER_LONG\n\nstruct dm_io_client {\n\tmempool_t pool;\n\tstruct bio_set bios;\n};\n\n \nstruct io {\n\tunsigned long error_bits;\n\tatomic_t count;\n\tstruct dm_io_client *client;\n\tio_notify_fn callback;\n\tvoid *context;\n\tvoid *vma_invalidate_address;\n\tunsigned long vma_invalidate_size;\n} __aligned(DM_IO_MAX_REGIONS);\n\nstatic struct kmem_cache *_dm_io_cache;\n\n \nstruct dm_io_client *dm_io_client_create(void)\n{\n\tstruct dm_io_client *client;\n\tunsigned int min_ios = dm_get_reserved_bio_based_ios();\n\tint ret;\n\n\tclient = kzalloc(sizeof(*client), GFP_KERNEL);\n\tif (!client)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = mempool_init_slab_pool(&client->pool, min_ios, _dm_io_cache);\n\tif (ret)\n\t\tgoto bad;\n\n\tret = bioset_init(&client->bios, min_ios, 0, BIOSET_NEED_BVECS);\n\tif (ret)\n\t\tgoto bad;\n\n\treturn client;\n\nbad:\n\tmempool_exit(&client->pool);\n\tkfree(client);\n\treturn ERR_PTR(ret);\n}\nEXPORT_SYMBOL(dm_io_client_create);\n\nvoid dm_io_client_destroy(struct dm_io_client *client)\n{\n\tmempool_exit(&client->pool);\n\tbioset_exit(&client->bios);\n\tkfree(client);\n}\nEXPORT_SYMBOL(dm_io_client_destroy);\n\n \nstatic void store_io_and_region_in_bio(struct bio *bio, struct io *io,\n\t\t\t\t       unsigned int region)\n{\n\tif (unlikely(!IS_ALIGNED((unsigned long)io, DM_IO_MAX_REGIONS))) {\n\t\tDMCRIT(\"Unaligned struct io pointer %p\", io);\n\t\tBUG();\n\t}\n\n\tbio->bi_private = (void *)((unsigned long)io | region);\n}\n\nstatic void retrieve_io_and_region_from_bio(struct bio *bio, struct io **io,\n\t\t\t\t       unsigned int *region)\n{\n\tunsigned long val = (unsigned long)bio->bi_private;\n\n\t*io = (void *)(val & -(unsigned long)DM_IO_MAX_REGIONS);\n\t*region = val & (DM_IO_MAX_REGIONS - 1);\n}\n\n \nstatic void complete_io(struct io *io)\n{\n\tunsigned long error_bits = io->error_bits;\n\tio_notify_fn fn = io->callback;\n\tvoid *context = io->context;\n\n\tif (io->vma_invalidate_size)\n\t\tinvalidate_kernel_vmap_range(io->vma_invalidate_address,\n\t\t\t\t\t     io->vma_invalidate_size);\n\n\tmempool_free(io, &io->client->pool);\n\tfn(error_bits, context);\n}\n\nstatic void dec_count(struct io *io, unsigned int region, blk_status_t error)\n{\n\tif (error)\n\t\tset_bit(region, &io->error_bits);\n\n\tif (atomic_dec_and_test(&io->count))\n\t\tcomplete_io(io);\n}\n\nstatic void endio(struct bio *bio)\n{\n\tstruct io *io;\n\tunsigned int region;\n\tblk_status_t error;\n\n\tif (bio->bi_status && bio_data_dir(bio) == READ)\n\t\tzero_fill_bio(bio);\n\n\t \n\tretrieve_io_and_region_from_bio(bio, &io, &region);\n\n\terror = bio->bi_status;\n\tbio_put(bio);\n\n\tdec_count(io, region, error);\n}\n\n \nstruct dpages {\n\tvoid (*get_page)(struct dpages *dp,\n\t\t\t struct page **p, unsigned long *len, unsigned int *offset);\n\tvoid (*next_page)(struct dpages *dp);\n\n\tunion {\n\t\tunsigned int context_u;\n\t\tstruct bvec_iter context_bi;\n\t};\n\tvoid *context_ptr;\n\n\tvoid *vma_invalidate_address;\n\tunsigned long vma_invalidate_size;\n};\n\n \nstatic void list_get_page(struct dpages *dp,\n\t\t  struct page **p, unsigned long *len, unsigned int *offset)\n{\n\tunsigned int o = dp->context_u;\n\tstruct page_list *pl = dp->context_ptr;\n\n\t*p = pl->page;\n\t*len = PAGE_SIZE - o;\n\t*offset = o;\n}\n\nstatic void list_next_page(struct dpages *dp)\n{\n\tstruct page_list *pl = dp->context_ptr;\n\n\tdp->context_ptr = pl->next;\n\tdp->context_u = 0;\n}\n\nstatic void list_dp_init(struct dpages *dp, struct page_list *pl, unsigned int offset)\n{\n\tdp->get_page = list_get_page;\n\tdp->next_page = list_next_page;\n\tdp->context_u = offset;\n\tdp->context_ptr = pl;\n}\n\n \nstatic void bio_get_page(struct dpages *dp, struct page **p,\n\t\t\t unsigned long *len, unsigned int *offset)\n{\n\tstruct bio_vec bvec = bvec_iter_bvec((struct bio_vec *)dp->context_ptr,\n\t\t\t\t\t     dp->context_bi);\n\n\t*p = bvec.bv_page;\n\t*len = bvec.bv_len;\n\t*offset = bvec.bv_offset;\n\n\t \n\tdp->context_bi.bi_sector = (sector_t)bvec.bv_len;\n}\n\nstatic void bio_next_page(struct dpages *dp)\n{\n\tunsigned int len = (unsigned int)dp->context_bi.bi_sector;\n\n\tbvec_iter_advance((struct bio_vec *)dp->context_ptr,\n\t\t\t  &dp->context_bi, len);\n}\n\nstatic void bio_dp_init(struct dpages *dp, struct bio *bio)\n{\n\tdp->get_page = bio_get_page;\n\tdp->next_page = bio_next_page;\n\n\t \n\tdp->context_ptr = bio->bi_io_vec;\n\tdp->context_bi = bio->bi_iter;\n}\n\n \nstatic void vm_get_page(struct dpages *dp,\n\t\t struct page **p, unsigned long *len, unsigned int *offset)\n{\n\t*p = vmalloc_to_page(dp->context_ptr);\n\t*offset = dp->context_u;\n\t*len = PAGE_SIZE - dp->context_u;\n}\n\nstatic void vm_next_page(struct dpages *dp)\n{\n\tdp->context_ptr += PAGE_SIZE - dp->context_u;\n\tdp->context_u = 0;\n}\n\nstatic void vm_dp_init(struct dpages *dp, void *data)\n{\n\tdp->get_page = vm_get_page;\n\tdp->next_page = vm_next_page;\n\tdp->context_u = offset_in_page(data);\n\tdp->context_ptr = data;\n}\n\n \nstatic void km_get_page(struct dpages *dp, struct page **p, unsigned long *len,\n\t\t\tunsigned int *offset)\n{\n\t*p = virt_to_page(dp->context_ptr);\n\t*offset = dp->context_u;\n\t*len = PAGE_SIZE - dp->context_u;\n}\n\nstatic void km_next_page(struct dpages *dp)\n{\n\tdp->context_ptr += PAGE_SIZE - dp->context_u;\n\tdp->context_u = 0;\n}\n\nstatic void km_dp_init(struct dpages *dp, void *data)\n{\n\tdp->get_page = km_get_page;\n\tdp->next_page = km_next_page;\n\tdp->context_u = offset_in_page(data);\n\tdp->context_ptr = data;\n}\n\n \nstatic void do_region(const blk_opf_t opf, unsigned int region,\n\t\t      struct dm_io_region *where, struct dpages *dp,\n\t\t      struct io *io)\n{\n\tstruct bio *bio;\n\tstruct page *page;\n\tunsigned long len;\n\tunsigned int offset;\n\tunsigned int num_bvecs;\n\tsector_t remaining = where->count;\n\tstruct request_queue *q = bdev_get_queue(where->bdev);\n\tsector_t num_sectors;\n\tunsigned int special_cmd_max_sectors;\n\tconst enum req_op op = opf & REQ_OP_MASK;\n\n\t \n\tif (op == REQ_OP_DISCARD)\n\t\tspecial_cmd_max_sectors = bdev_max_discard_sectors(where->bdev);\n\telse if (op == REQ_OP_WRITE_ZEROES)\n\t\tspecial_cmd_max_sectors = q->limits.max_write_zeroes_sectors;\n\tif ((op == REQ_OP_DISCARD || op == REQ_OP_WRITE_ZEROES) &&\n\t    special_cmd_max_sectors == 0) {\n\t\tatomic_inc(&io->count);\n\t\tdec_count(io, region, BLK_STS_NOTSUPP);\n\t\treturn;\n\t}\n\n\t \n\tdo {\n\t\t \n\t\tswitch (op) {\n\t\tcase REQ_OP_DISCARD:\n\t\tcase REQ_OP_WRITE_ZEROES:\n\t\t\tnum_bvecs = 0;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tnum_bvecs = bio_max_segs(dm_sector_div_up(remaining,\n\t\t\t\t\t\t(PAGE_SIZE >> SECTOR_SHIFT)));\n\t\t}\n\n\t\tbio = bio_alloc_bioset(where->bdev, num_bvecs, opf, GFP_NOIO,\n\t\t\t\t       &io->client->bios);\n\t\tbio->bi_iter.bi_sector = where->sector + (where->count - remaining);\n\t\tbio->bi_end_io = endio;\n\t\tstore_io_and_region_in_bio(bio, io, region);\n\n\t\tif (op == REQ_OP_DISCARD || op == REQ_OP_WRITE_ZEROES) {\n\t\t\tnum_sectors = min_t(sector_t, special_cmd_max_sectors, remaining);\n\t\t\tbio->bi_iter.bi_size = num_sectors << SECTOR_SHIFT;\n\t\t\tremaining -= num_sectors;\n\t\t} else {\n\t\t\twhile (remaining) {\n\t\t\t\t \n\t\t\t\tdp->get_page(dp, &page, &len, &offset);\n\t\t\t\tlen = min(len, to_bytes(remaining));\n\t\t\t\tif (!bio_add_page(bio, page, len, offset))\n\t\t\t\t\tbreak;\n\n\t\t\t\toffset = 0;\n\t\t\t\tremaining -= to_sector(len);\n\t\t\t\tdp->next_page(dp);\n\t\t\t}\n\t\t}\n\n\t\tatomic_inc(&io->count);\n\t\tsubmit_bio(bio);\n\t} while (remaining);\n}\n\nstatic void dispatch_io(blk_opf_t opf, unsigned int num_regions,\n\t\t\tstruct dm_io_region *where, struct dpages *dp,\n\t\t\tstruct io *io, int sync)\n{\n\tint i;\n\tstruct dpages old_pages = *dp;\n\n\tBUG_ON(num_regions > DM_IO_MAX_REGIONS);\n\n\tif (sync)\n\t\topf |= REQ_SYNC;\n\n\t \n\tfor (i = 0; i < num_regions; i++) {\n\t\t*dp = old_pages;\n\t\tif (where[i].count || (opf & REQ_PREFLUSH))\n\t\t\tdo_region(opf, i, where + i, dp, io);\n\t}\n\n\t \n\tdec_count(io, 0, 0);\n}\n\nstruct sync_io {\n\tunsigned long error_bits;\n\tstruct completion wait;\n};\n\nstatic void sync_io_complete(unsigned long error, void *context)\n{\n\tstruct sync_io *sio = context;\n\n\tsio->error_bits = error;\n\tcomplete(&sio->wait);\n}\n\nstatic int sync_io(struct dm_io_client *client, unsigned int num_regions,\n\t\t   struct dm_io_region *where, blk_opf_t opf, struct dpages *dp,\n\t\t   unsigned long *error_bits)\n{\n\tstruct io *io;\n\tstruct sync_io sio;\n\n\tif (num_regions > 1 && !op_is_write(opf)) {\n\t\tWARN_ON(1);\n\t\treturn -EIO;\n\t}\n\n\tinit_completion(&sio.wait);\n\n\tio = mempool_alloc(&client->pool, GFP_NOIO);\n\tio->error_bits = 0;\n\tatomic_set(&io->count, 1);  \n\tio->client = client;\n\tio->callback = sync_io_complete;\n\tio->context = &sio;\n\n\tio->vma_invalidate_address = dp->vma_invalidate_address;\n\tio->vma_invalidate_size = dp->vma_invalidate_size;\n\n\tdispatch_io(opf, num_regions, where, dp, io, 1);\n\n\twait_for_completion_io(&sio.wait);\n\n\tif (error_bits)\n\t\t*error_bits = sio.error_bits;\n\n\treturn sio.error_bits ? -EIO : 0;\n}\n\nstatic int async_io(struct dm_io_client *client, unsigned int num_regions,\n\t\t    struct dm_io_region *where, blk_opf_t opf,\n\t\t    struct dpages *dp, io_notify_fn fn, void *context)\n{\n\tstruct io *io;\n\n\tif (num_regions > 1 && !op_is_write(opf)) {\n\t\tWARN_ON(1);\n\t\tfn(1, context);\n\t\treturn -EIO;\n\t}\n\n\tio = mempool_alloc(&client->pool, GFP_NOIO);\n\tio->error_bits = 0;\n\tatomic_set(&io->count, 1);  \n\tio->client = client;\n\tio->callback = fn;\n\tio->context = context;\n\n\tio->vma_invalidate_address = dp->vma_invalidate_address;\n\tio->vma_invalidate_size = dp->vma_invalidate_size;\n\n\tdispatch_io(opf, num_regions, where, dp, io, 0);\n\treturn 0;\n}\n\nstatic int dp_init(struct dm_io_request *io_req, struct dpages *dp,\n\t\t   unsigned long size)\n{\n\t \n\n\tdp->vma_invalidate_address = NULL;\n\tdp->vma_invalidate_size = 0;\n\n\tswitch (io_req->mem.type) {\n\tcase DM_IO_PAGE_LIST:\n\t\tlist_dp_init(dp, io_req->mem.ptr.pl, io_req->mem.offset);\n\t\tbreak;\n\n\tcase DM_IO_BIO:\n\t\tbio_dp_init(dp, io_req->mem.ptr.bio);\n\t\tbreak;\n\n\tcase DM_IO_VMA:\n\t\tflush_kernel_vmap_range(io_req->mem.ptr.vma, size);\n\t\tif ((io_req->bi_opf & REQ_OP_MASK) == REQ_OP_READ) {\n\t\t\tdp->vma_invalidate_address = io_req->mem.ptr.vma;\n\t\t\tdp->vma_invalidate_size = size;\n\t\t}\n\t\tvm_dp_init(dp, io_req->mem.ptr.vma);\n\t\tbreak;\n\n\tcase DM_IO_KMEM:\n\t\tkm_dp_init(dp, io_req->mem.ptr.addr);\n\t\tbreak;\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint dm_io(struct dm_io_request *io_req, unsigned int num_regions,\n\t  struct dm_io_region *where, unsigned long *sync_error_bits)\n{\n\tint r;\n\tstruct dpages dp;\n\n\tr = dp_init(io_req, &dp, (unsigned long)where->count << SECTOR_SHIFT);\n\tif (r)\n\t\treturn r;\n\n\tif (!io_req->notify.fn)\n\t\treturn sync_io(io_req->client, num_regions, where,\n\t\t\t       io_req->bi_opf, &dp, sync_error_bits);\n\n\treturn async_io(io_req->client, num_regions, where,\n\t\t\tio_req->bi_opf, &dp, io_req->notify.fn,\n\t\t\tio_req->notify.context);\n}\nEXPORT_SYMBOL(dm_io);\n\nint __init dm_io_init(void)\n{\n\t_dm_io_cache = KMEM_CACHE(io, 0);\n\tif (!_dm_io_cache)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nvoid dm_io_exit(void)\n{\n\tkmem_cache_destroy(_dm_io_cache);\n\t_dm_io_cache = NULL;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}