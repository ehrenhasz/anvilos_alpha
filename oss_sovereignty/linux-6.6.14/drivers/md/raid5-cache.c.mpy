{
  "module_name": "raid5-cache.c",
  "hash_id": "3e2e4978f8e1b44b12d7c17f36c0107416a15cddb5404886755c054e25a7eafc",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/raid5-cache.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/wait.h>\n#include <linux/blkdev.h>\n#include <linux/slab.h>\n#include <linux/raid/md_p.h>\n#include <linux/crc32c.h>\n#include <linux/random.h>\n#include <linux/kthread.h>\n#include <linux/types.h>\n#include \"md.h\"\n#include \"raid5.h\"\n#include \"md-bitmap.h\"\n#include \"raid5-log.h\"\n\n \n#define BLOCK_SECTORS (8)\n#define BLOCK_SECTOR_SHIFT (3)\n\n \n#define RECLAIM_MAX_FREE_SPACE (10 * 1024 * 1024 * 2)  \n#define RECLAIM_MAX_FREE_SPACE_SHIFT (2)\n\n \n#define R5C_RECLAIM_WAKEUP_INTERVAL (30 * HZ)\n \n#define R5C_FULL_STRIPE_FLUSH_BATCH(conf) (conf->max_nr_stripes / 4)\n \n#define R5C_RECLAIM_STRIPE_GROUP (NR_STRIPE_HASH_LOCKS * 2)\n\n \n#define R5L_POOL_SIZE\t4\n\nstatic char *r5c_journal_mode_str[] = {\"write-through\",\n\t\t\t\t       \"write-back\"};\n \n\nstruct r5l_log {\n\tstruct md_rdev *rdev;\n\n\tu32 uuid_checksum;\n\n\tsector_t device_size;\t\t \n\tsector_t max_free_space;\t \n\n\tsector_t last_checkpoint;\t \n\tu64 last_cp_seq;\t\t \n\n\tsector_t log_start;\t\t \n\tu64 seq;\t\t\t \n\n\tsector_t next_checkpoint;\n\n\tstruct mutex io_mutex;\n\tstruct r5l_io_unit *current_io;\t \n\n\tspinlock_t io_list_lock;\n\tstruct list_head running_ios;\t \n\tstruct list_head io_end_ios;\t \n\tstruct list_head flushing_ios;\t \n\tstruct list_head finished_ios;\t \n\tstruct bio flush_bio;\n\n\tstruct list_head no_mem_stripes;    \n\n\tstruct kmem_cache *io_kc;\n\tmempool_t io_pool;\n\tstruct bio_set bs;\n\tmempool_t meta_pool;\n\n\tstruct md_thread __rcu *reclaim_thread;\n\tunsigned long reclaim_target;\t \n\twait_queue_head_t iounit_wait;\n\n\tstruct list_head no_space_stripes;  \n\tspinlock_t no_space_stripes_lock;\n\n\tbool need_cache_flush;\n\n\t \n\tenum r5c_journal_mode r5c_journal_mode;\n\n\t \n\tstruct list_head stripe_in_journal_list;\n\n\tspinlock_t stripe_in_journal_lock;\n\tatomic_t stripe_in_journal_count;\n\n\t \n\tstruct work_struct deferred_io_work;\n\t \n\tstruct work_struct disable_writeback_work;\n\n\t \n\tspinlock_t tree_lock;\n\tstruct radix_tree_root big_stripe_tree;\n};\n\n \n\n \n#define R5C_RADIX_COUNT_SHIFT 2\n\n \nstatic inline sector_t r5c_tree_index(struct r5conf *conf,\n\t\t\t\t      sector_t sect)\n{\n\tsector_div(sect, conf->chunk_sectors);\n\treturn sect;\n}\n\n \nstruct r5l_io_unit {\n\tstruct r5l_log *log;\n\n\tstruct page *meta_page;\t \n\tint meta_offset;\t \n\n\tstruct bio *current_bio; \n\n\tatomic_t pending_stripe; \n\tu64 seq;\t\t \n\tsector_t log_start;\t \n\tsector_t log_end;\t \n\tstruct list_head log_sibling;  \n\tstruct list_head stripe_list;  \n\n\tint state;\n\tbool need_split_bio;\n\tstruct bio *split_bio;\n\n\tunsigned int has_flush:1;\t\t \n\tunsigned int has_fua:1;\t\t\t \n\tunsigned int has_null_flush:1;\t\t \n\tunsigned int has_flush_payload:1;\t \n\t \n\tunsigned int io_deferred:1;\n\n\tstruct bio_list flush_barriers;    \n};\n\n \nenum r5l_io_unit_state {\n\tIO_UNIT_RUNNING = 0,\t \n\tIO_UNIT_IO_START = 1,\t \n\tIO_UNIT_IO_END = 2,\t \n\tIO_UNIT_STRIPE_END = 3,\t \n};\n\nbool r5c_is_writeback(struct r5l_log *log)\n{\n\treturn (log != NULL &&\n\t\tlog->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_BACK);\n}\n\nstatic sector_t r5l_ring_add(struct r5l_log *log, sector_t start, sector_t inc)\n{\n\tstart += inc;\n\tif (start >= log->device_size)\n\t\tstart = start - log->device_size;\n\treturn start;\n}\n\nstatic sector_t r5l_ring_distance(struct r5l_log *log, sector_t start,\n\t\t\t\t  sector_t end)\n{\n\tif (end >= start)\n\t\treturn end - start;\n\telse\n\t\treturn end + log->device_size - start;\n}\n\nstatic bool r5l_has_free_space(struct r5l_log *log, sector_t size)\n{\n\tsector_t used_size;\n\n\tused_size = r5l_ring_distance(log, log->last_checkpoint,\n\t\t\t\t\tlog->log_start);\n\n\treturn log->device_size > used_size + size;\n}\n\nstatic void __r5l_set_io_unit_state(struct r5l_io_unit *io,\n\t\t\t\t    enum r5l_io_unit_state state)\n{\n\tif (WARN_ON(io->state >= state))\n\t\treturn;\n\tio->state = state;\n}\n\nstatic void\nr5c_return_dev_pending_writes(struct r5conf *conf, struct r5dev *dev)\n{\n\tstruct bio *wbi, *wbi2;\n\n\twbi = dev->written;\n\tdev->written = NULL;\n\twhile (wbi && wbi->bi_iter.bi_sector <\n\t       dev->sector + RAID5_STRIPE_SECTORS(conf)) {\n\t\twbi2 = r5_next_bio(conf, wbi, dev->sector);\n\t\tmd_write_end(conf->mddev);\n\t\tbio_endio(wbi);\n\t\twbi = wbi2;\n\t}\n}\n\nvoid r5c_handle_cached_data_endio(struct r5conf *conf,\n\t\t\t\t  struct stripe_head *sh, int disks)\n{\n\tint i;\n\n\tfor (i = sh->disks; i--; ) {\n\t\tif (sh->dev[i].written) {\n\t\t\tset_bit(R5_UPTODATE, &sh->dev[i].flags);\n\t\t\tr5c_return_dev_pending_writes(conf, &sh->dev[i]);\n\t\t\tmd_bitmap_endwrite(conf->mddev->bitmap, sh->sector,\n\t\t\t\t\t   RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t\t   !test_bit(STRIPE_DEGRADED, &sh->state),\n\t\t\t\t\t   0);\n\t\t}\n\t}\n}\n\nvoid r5l_wake_reclaim(struct r5l_log *log, sector_t space);\n\n \nvoid r5c_check_stripe_cache_usage(struct r5conf *conf)\n{\n\tint total_cached;\n\n\tif (!r5c_is_writeback(conf->log))\n\t\treturn;\n\n\ttotal_cached = atomic_read(&conf->r5c_cached_partial_stripes) +\n\t\tatomic_read(&conf->r5c_cached_full_stripes);\n\n\t \n\tif (total_cached > conf->min_nr_stripes * 1 / 2 ||\n\t    atomic_read(&conf->empty_inactive_list_nr) > 0)\n\t\tr5l_wake_reclaim(conf->log, 0);\n}\n\n \nvoid r5c_check_cached_full_stripe(struct r5conf *conf)\n{\n\tif (!r5c_is_writeback(conf->log))\n\t\treturn;\n\n\t \n\tif (atomic_read(&conf->r5c_cached_full_stripes) >=\n\t    min(R5C_FULL_STRIPE_FLUSH_BATCH(conf),\n\t\tconf->chunk_sectors >> RAID5_STRIPE_SHIFT(conf)))\n\t\tr5l_wake_reclaim(conf->log, 0);\n}\n\n \nstatic sector_t r5c_log_required_to_flush_cache(struct r5conf *conf)\n{\n\tstruct r5l_log *log = conf->log;\n\n\tif (!r5c_is_writeback(log))\n\t\treturn 0;\n\n\treturn BLOCK_SECTORS *\n\t\t((conf->max_degraded + 1) * atomic_read(&log->stripe_in_journal_count) +\n\t\t (conf->raid_disks - conf->max_degraded) * (conf->group_cnt + 1));\n}\n\n \nstatic inline void r5c_update_log_state(struct r5l_log *log)\n{\n\tstruct r5conf *conf = log->rdev->mddev->private;\n\tsector_t free_space;\n\tsector_t reclaim_space;\n\tbool wake_reclaim = false;\n\n\tif (!r5c_is_writeback(log))\n\t\treturn;\n\n\tfree_space = r5l_ring_distance(log, log->log_start,\n\t\t\t\t       log->last_checkpoint);\n\treclaim_space = r5c_log_required_to_flush_cache(conf);\n\tif (free_space < 2 * reclaim_space)\n\t\tset_bit(R5C_LOG_CRITICAL, &conf->cache_state);\n\telse {\n\t\tif (test_bit(R5C_LOG_CRITICAL, &conf->cache_state))\n\t\t\twake_reclaim = true;\n\t\tclear_bit(R5C_LOG_CRITICAL, &conf->cache_state);\n\t}\n\tif (free_space < 3 * reclaim_space)\n\t\tset_bit(R5C_LOG_TIGHT, &conf->cache_state);\n\telse\n\t\tclear_bit(R5C_LOG_TIGHT, &conf->cache_state);\n\n\tif (wake_reclaim)\n\t\tr5l_wake_reclaim(log, 0);\n}\n\n \nvoid r5c_make_stripe_write_out(struct stripe_head *sh)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tstruct r5l_log *log = conf->log;\n\n\tBUG_ON(!r5c_is_writeback(log));\n\n\tWARN_ON(!test_bit(STRIPE_R5C_CACHING, &sh->state));\n\tclear_bit(STRIPE_R5C_CACHING, &sh->state);\n\n\tif (!test_and_set_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\tatomic_inc(&conf->preread_active_stripes);\n}\n\nstatic void r5c_handle_data_cached(struct stripe_head *sh)\n{\n\tint i;\n\n\tfor (i = sh->disks; i--; )\n\t\tif (test_and_clear_bit(R5_Wantwrite, &sh->dev[i].flags)) {\n\t\t\tset_bit(R5_InJournal, &sh->dev[i].flags);\n\t\t\tclear_bit(R5_LOCKED, &sh->dev[i].flags);\n\t\t}\n\tclear_bit(STRIPE_LOG_TRAPPED, &sh->state);\n}\n\n \nstatic void r5c_handle_parity_cached(struct stripe_head *sh)\n{\n\tint i;\n\n\tfor (i = sh->disks; i--; )\n\t\tif (test_bit(R5_InJournal, &sh->dev[i].flags))\n\t\t\tset_bit(R5_Wantwrite, &sh->dev[i].flags);\n}\n\n \nstatic void r5c_finish_cache_stripe(struct stripe_head *sh)\n{\n\tstruct r5l_log *log = sh->raid_conf->log;\n\n\tif (log->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_THROUGH) {\n\t\tBUG_ON(test_bit(STRIPE_R5C_CACHING, &sh->state));\n\t\t \n\t\tset_bit(R5_InJournal, &sh->dev[sh->pd_idx].flags);\n\t} else if (test_bit(STRIPE_R5C_CACHING, &sh->state)) {\n\t\tr5c_handle_data_cached(sh);\n\t} else {\n\t\tr5c_handle_parity_cached(sh);\n\t\tset_bit(R5_InJournal, &sh->dev[sh->pd_idx].flags);\n\t}\n}\n\nstatic void r5l_io_run_stripes(struct r5l_io_unit *io)\n{\n\tstruct stripe_head *sh, *next;\n\n\tlist_for_each_entry_safe(sh, next, &io->stripe_list, log_list) {\n\t\tlist_del_init(&sh->log_list);\n\n\t\tr5c_finish_cache_stripe(sh);\n\n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\traid5_release_stripe(sh);\n\t}\n}\n\nstatic void r5l_log_run_stripes(struct r5l_log *log)\n{\n\tstruct r5l_io_unit *io, *next;\n\n\tlockdep_assert_held(&log->io_list_lock);\n\n\tlist_for_each_entry_safe(io, next, &log->running_ios, log_sibling) {\n\t\t \n\t\tif (io->state < IO_UNIT_IO_END)\n\t\t\tbreak;\n\n\t\tlist_move_tail(&io->log_sibling, &log->finished_ios);\n\t\tr5l_io_run_stripes(io);\n\t}\n}\n\nstatic void r5l_move_to_end_ios(struct r5l_log *log)\n{\n\tstruct r5l_io_unit *io, *next;\n\n\tlockdep_assert_held(&log->io_list_lock);\n\n\tlist_for_each_entry_safe(io, next, &log->running_ios, log_sibling) {\n\t\t \n\t\tif (io->state < IO_UNIT_IO_END)\n\t\t\tbreak;\n\t\tlist_move_tail(&io->log_sibling, &log->io_end_ios);\n\t}\n}\n\nstatic void __r5l_stripe_write_finished(struct r5l_io_unit *io);\nstatic void r5l_log_endio(struct bio *bio)\n{\n\tstruct r5l_io_unit *io = bio->bi_private;\n\tstruct r5l_io_unit *io_deferred;\n\tstruct r5l_log *log = io->log;\n\tunsigned long flags;\n\tbool has_null_flush;\n\tbool has_flush_payload;\n\n\tif (bio->bi_status)\n\t\tmd_error(log->rdev->mddev, log->rdev);\n\n\tbio_put(bio);\n\tmempool_free(io->meta_page, &log->meta_pool);\n\n\tspin_lock_irqsave(&log->io_list_lock, flags);\n\t__r5l_set_io_unit_state(io, IO_UNIT_IO_END);\n\n\t \n\thas_null_flush = io->has_null_flush;\n\thas_flush_payload = io->has_flush_payload;\n\n\tif (log->need_cache_flush && !list_empty(&io->stripe_list))\n\t\tr5l_move_to_end_ios(log);\n\telse\n\t\tr5l_log_run_stripes(log);\n\tif (!list_empty(&log->running_ios)) {\n\t\t \n\t\tio_deferred = list_first_entry(&log->running_ios,\n\t\t\t\t\t       struct r5l_io_unit, log_sibling);\n\t\tif (io_deferred->io_deferred)\n\t\t\tschedule_work(&log->deferred_io_work);\n\t}\n\n\tspin_unlock_irqrestore(&log->io_list_lock, flags);\n\n\tif (log->need_cache_flush)\n\t\tmd_wakeup_thread(log->rdev->mddev->thread);\n\n\t \n\tif (has_null_flush) {\n\t\tstruct bio *bi;\n\n\t\tWARN_ON(bio_list_empty(&io->flush_barriers));\n\t\twhile ((bi = bio_list_pop(&io->flush_barriers)) != NULL) {\n\t\t\tbio_endio(bi);\n\t\t\tif (atomic_dec_and_test(&io->pending_stripe)) {\n\t\t\t\t__r5l_stripe_write_finished(io);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n\t \n\tif (has_flush_payload)\n\t\tif (atomic_dec_and_test(&io->pending_stripe))\n\t\t\t__r5l_stripe_write_finished(io);\n}\n\nstatic void r5l_do_submit_io(struct r5l_log *log, struct r5l_io_unit *io)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&log->io_list_lock, flags);\n\t__r5l_set_io_unit_state(io, IO_UNIT_IO_START);\n\tspin_unlock_irqrestore(&log->io_list_lock, flags);\n\n\t \n\tif (io->split_bio) {\n\t\tif (io->has_flush)\n\t\t\tio->split_bio->bi_opf |= REQ_PREFLUSH;\n\t\tif (io->has_fua)\n\t\t\tio->split_bio->bi_opf |= REQ_FUA;\n\t\tsubmit_bio(io->split_bio);\n\t}\n\n\tif (io->has_flush)\n\t\tio->current_bio->bi_opf |= REQ_PREFLUSH;\n\tif (io->has_fua)\n\t\tio->current_bio->bi_opf |= REQ_FUA;\n\tsubmit_bio(io->current_bio);\n}\n\n \nstatic void r5l_submit_io_async(struct work_struct *work)\n{\n\tstruct r5l_log *log = container_of(work, struct r5l_log,\n\t\t\t\t\t   deferred_io_work);\n\tstruct r5l_io_unit *io = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&log->io_list_lock, flags);\n\tif (!list_empty(&log->running_ios)) {\n\t\tio = list_first_entry(&log->running_ios, struct r5l_io_unit,\n\t\t\t\t      log_sibling);\n\t\tif (!io->io_deferred)\n\t\t\tio = NULL;\n\t\telse\n\t\t\tio->io_deferred = 0;\n\t}\n\tspin_unlock_irqrestore(&log->io_list_lock, flags);\n\tif (io)\n\t\tr5l_do_submit_io(log, io);\n}\n\nstatic void r5c_disable_writeback_async(struct work_struct *work)\n{\n\tstruct r5l_log *log = container_of(work, struct r5l_log,\n\t\t\t\t\t   disable_writeback_work);\n\tstruct mddev *mddev = log->rdev->mddev;\n\tstruct r5conf *conf = mddev->private;\n\tint locked = 0;\n\n\tif (log->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_THROUGH)\n\t\treturn;\n\tpr_info(\"md/raid:%s: Disabling writeback cache for degraded array.\\n\",\n\t\tmdname(mddev));\n\n\t \n\twait_event(mddev->sb_wait,\n\t\t   conf->log == NULL ||\n\t\t   (!test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags) &&\n\t\t    (locked = mddev_trylock(mddev))));\n\tif (locked) {\n\t\tmddev_suspend(mddev);\n\t\tlog->r5c_journal_mode = R5C_JOURNAL_MODE_WRITE_THROUGH;\n\t\tmddev_resume(mddev);\n\t\tmddev_unlock(mddev);\n\t}\n}\n\nstatic void r5l_submit_current_io(struct r5l_log *log)\n{\n\tstruct r5l_io_unit *io = log->current_io;\n\tstruct r5l_meta_block *block;\n\tunsigned long flags;\n\tu32 crc;\n\tbool do_submit = true;\n\n\tif (!io)\n\t\treturn;\n\n\tblock = page_address(io->meta_page);\n\tblock->meta_size = cpu_to_le32(io->meta_offset);\n\tcrc = crc32c_le(log->uuid_checksum, block, PAGE_SIZE);\n\tblock->checksum = cpu_to_le32(crc);\n\n\tlog->current_io = NULL;\n\tspin_lock_irqsave(&log->io_list_lock, flags);\n\tif (io->has_flush || io->has_fua) {\n\t\tif (io != list_first_entry(&log->running_ios,\n\t\t\t\t\t   struct r5l_io_unit, log_sibling)) {\n\t\t\tio->io_deferred = 1;\n\t\t\tdo_submit = false;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&log->io_list_lock, flags);\n\tif (do_submit)\n\t\tr5l_do_submit_io(log, io);\n}\n\nstatic struct bio *r5l_bio_alloc(struct r5l_log *log)\n{\n\tstruct bio *bio = bio_alloc_bioset(log->rdev->bdev, BIO_MAX_VECS,\n\t\t\t\t\t   REQ_OP_WRITE, GFP_NOIO, &log->bs);\n\n\tbio->bi_iter.bi_sector = log->rdev->data_offset + log->log_start;\n\n\treturn bio;\n}\n\nstatic void r5_reserve_log_entry(struct r5l_log *log, struct r5l_io_unit *io)\n{\n\tlog->log_start = r5l_ring_add(log, log->log_start, BLOCK_SECTORS);\n\n\tr5c_update_log_state(log);\n\t \n\tif (log->log_start == 0)\n\t\tio->need_split_bio = true;\n\n\tio->log_end = log->log_start;\n}\n\nstatic struct r5l_io_unit *r5l_new_meta(struct r5l_log *log)\n{\n\tstruct r5l_io_unit *io;\n\tstruct r5l_meta_block *block;\n\n\tio = mempool_alloc(&log->io_pool, GFP_ATOMIC);\n\tif (!io)\n\t\treturn NULL;\n\tmemset(io, 0, sizeof(*io));\n\n\tio->log = log;\n\tINIT_LIST_HEAD(&io->log_sibling);\n\tINIT_LIST_HEAD(&io->stripe_list);\n\tbio_list_init(&io->flush_barriers);\n\tio->state = IO_UNIT_RUNNING;\n\n\tio->meta_page = mempool_alloc(&log->meta_pool, GFP_NOIO);\n\tblock = page_address(io->meta_page);\n\tclear_page(block);\n\tblock->magic = cpu_to_le32(R5LOG_MAGIC);\n\tblock->version = R5LOG_VERSION;\n\tblock->seq = cpu_to_le64(log->seq);\n\tblock->position = cpu_to_le64(log->log_start);\n\n\tio->log_start = log->log_start;\n\tio->meta_offset = sizeof(struct r5l_meta_block);\n\tio->seq = log->seq++;\n\n\tio->current_bio = r5l_bio_alloc(log);\n\tio->current_bio->bi_end_io = r5l_log_endio;\n\tio->current_bio->bi_private = io;\n\t__bio_add_page(io->current_bio, io->meta_page, PAGE_SIZE, 0);\n\n\tr5_reserve_log_entry(log, io);\n\n\tspin_lock_irq(&log->io_list_lock);\n\tlist_add_tail(&io->log_sibling, &log->running_ios);\n\tspin_unlock_irq(&log->io_list_lock);\n\n\treturn io;\n}\n\nstatic int r5l_get_meta(struct r5l_log *log, unsigned int payload_size)\n{\n\tif (log->current_io &&\n\t    log->current_io->meta_offset + payload_size > PAGE_SIZE)\n\t\tr5l_submit_current_io(log);\n\n\tif (!log->current_io) {\n\t\tlog->current_io = r5l_new_meta(log);\n\t\tif (!log->current_io)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void r5l_append_payload_meta(struct r5l_log *log, u16 type,\n\t\t\t\t    sector_t location,\n\t\t\t\t    u32 checksum1, u32 checksum2,\n\t\t\t\t    bool checksum2_valid)\n{\n\tstruct r5l_io_unit *io = log->current_io;\n\tstruct r5l_payload_data_parity *payload;\n\n\tpayload = page_address(io->meta_page) + io->meta_offset;\n\tpayload->header.type = cpu_to_le16(type);\n\tpayload->header.flags = cpu_to_le16(0);\n\tpayload->size = cpu_to_le32((1 + !!checksum2_valid) <<\n\t\t\t\t    (PAGE_SHIFT - 9));\n\tpayload->location = cpu_to_le64(location);\n\tpayload->checksum[0] = cpu_to_le32(checksum1);\n\tif (checksum2_valid)\n\t\tpayload->checksum[1] = cpu_to_le32(checksum2);\n\n\tio->meta_offset += sizeof(struct r5l_payload_data_parity) +\n\t\tsizeof(__le32) * (1 + !!checksum2_valid);\n}\n\nstatic void r5l_append_payload_page(struct r5l_log *log, struct page *page)\n{\n\tstruct r5l_io_unit *io = log->current_io;\n\n\tif (io->need_split_bio) {\n\t\tBUG_ON(io->split_bio);\n\t\tio->split_bio = io->current_bio;\n\t\tio->current_bio = r5l_bio_alloc(log);\n\t\tbio_chain(io->current_bio, io->split_bio);\n\t\tio->need_split_bio = false;\n\t}\n\n\tif (!bio_add_page(io->current_bio, page, PAGE_SIZE, 0))\n\t\tBUG();\n\n\tr5_reserve_log_entry(log, io);\n}\n\nstatic void r5l_append_flush_payload(struct r5l_log *log, sector_t sect)\n{\n\tstruct mddev *mddev = log->rdev->mddev;\n\tstruct r5conf *conf = mddev->private;\n\tstruct r5l_io_unit *io;\n\tstruct r5l_payload_flush *payload;\n\tint meta_size;\n\n\t \n\tif (conf->quiesce)\n\t\treturn;\n\n\tmutex_lock(&log->io_mutex);\n\tmeta_size = sizeof(struct r5l_payload_flush) + sizeof(__le64);\n\n\tif (r5l_get_meta(log, meta_size)) {\n\t\tmutex_unlock(&log->io_mutex);\n\t\treturn;\n\t}\n\n\t \n\tio = log->current_io;\n\tpayload = page_address(io->meta_page) + io->meta_offset;\n\tpayload->header.type = cpu_to_le16(R5LOG_PAYLOAD_FLUSH);\n\tpayload->header.flags = cpu_to_le16(0);\n\tpayload->size = cpu_to_le32(sizeof(__le64));\n\tpayload->flush_stripes[0] = cpu_to_le64(sect);\n\tio->meta_offset += meta_size;\n\t \n\tif (!io->has_flush_payload) {\n\t\tio->has_flush_payload = 1;\n\t\tatomic_inc(&io->pending_stripe);\n\t}\n\tmutex_unlock(&log->io_mutex);\n}\n\nstatic int r5l_log_stripe(struct r5l_log *log, struct stripe_head *sh,\n\t\t\t   int data_pages, int parity_pages)\n{\n\tint i;\n\tint meta_size;\n\tint ret;\n\tstruct r5l_io_unit *io;\n\n\tmeta_size =\n\t\t((sizeof(struct r5l_payload_data_parity) + sizeof(__le32))\n\t\t * data_pages) +\n\t\tsizeof(struct r5l_payload_data_parity) +\n\t\tsizeof(__le32) * parity_pages;\n\n\tret = r5l_get_meta(log, meta_size);\n\tif (ret)\n\t\treturn ret;\n\n\tio = log->current_io;\n\n\tif (test_and_clear_bit(STRIPE_R5C_PREFLUSH, &sh->state))\n\t\tio->has_flush = 1;\n\n\tfor (i = 0; i < sh->disks; i++) {\n\t\tif (!test_bit(R5_Wantwrite, &sh->dev[i].flags) ||\n\t\t    test_bit(R5_InJournal, &sh->dev[i].flags))\n\t\t\tcontinue;\n\t\tif (i == sh->pd_idx || i == sh->qd_idx)\n\t\t\tcontinue;\n\t\tif (test_bit(R5_WantFUA, &sh->dev[i].flags) &&\n\t\t    log->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_BACK) {\n\t\t\tio->has_fua = 1;\n\t\t\t \n\t\t\tio->has_flush = 1;\n\t\t}\n\t\tr5l_append_payload_meta(log, R5LOG_PAYLOAD_DATA,\n\t\t\t\t\traid5_compute_blocknr(sh, i, 0),\n\t\t\t\t\tsh->dev[i].log_checksum, 0, false);\n\t\tr5l_append_payload_page(log, sh->dev[i].page);\n\t}\n\n\tif (parity_pages == 2) {\n\t\tr5l_append_payload_meta(log, R5LOG_PAYLOAD_PARITY,\n\t\t\t\t\tsh->sector, sh->dev[sh->pd_idx].log_checksum,\n\t\t\t\t\tsh->dev[sh->qd_idx].log_checksum, true);\n\t\tr5l_append_payload_page(log, sh->dev[sh->pd_idx].page);\n\t\tr5l_append_payload_page(log, sh->dev[sh->qd_idx].page);\n\t} else if (parity_pages == 1) {\n\t\tr5l_append_payload_meta(log, R5LOG_PAYLOAD_PARITY,\n\t\t\t\t\tsh->sector, sh->dev[sh->pd_idx].log_checksum,\n\t\t\t\t\t0, false);\n\t\tr5l_append_payload_page(log, sh->dev[sh->pd_idx].page);\n\t} else   \n\t\tBUG_ON(parity_pages != 0);\n\n\tlist_add_tail(&sh->log_list, &io->stripe_list);\n\tatomic_inc(&io->pending_stripe);\n\tsh->log_io = io;\n\n\tif (log->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_THROUGH)\n\t\treturn 0;\n\n\tif (sh->log_start == MaxSector) {\n\t\tBUG_ON(!list_empty(&sh->r5c));\n\t\tsh->log_start = io->log_start;\n\t\tspin_lock_irq(&log->stripe_in_journal_lock);\n\t\tlist_add_tail(&sh->r5c,\n\t\t\t      &log->stripe_in_journal_list);\n\t\tspin_unlock_irq(&log->stripe_in_journal_lock);\n\t\tatomic_inc(&log->stripe_in_journal_count);\n\t}\n\treturn 0;\n}\n\n \nstatic inline void r5l_add_no_space_stripe(struct r5l_log *log,\n\t\t\t\t\t   struct stripe_head *sh)\n{\n\tspin_lock(&log->no_space_stripes_lock);\n\tlist_add_tail(&sh->log_list, &log->no_space_stripes);\n\tspin_unlock(&log->no_space_stripes_lock);\n}\n\n \nint r5l_write_stripe(struct r5l_log *log, struct stripe_head *sh)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tint write_disks = 0;\n\tint data_pages, parity_pages;\n\tint reserve;\n\tint i;\n\tint ret = 0;\n\tbool wake_reclaim = false;\n\n\tif (!log)\n\t\treturn -EAGAIN;\n\t \n\tif (sh->log_io || !test_bit(R5_Wantwrite, &sh->dev[sh->pd_idx].flags) ||\n\t    test_bit(STRIPE_SYNCING, &sh->state)) {\n\t\t \n\t\tclear_bit(STRIPE_LOG_TRAPPED, &sh->state);\n\t\treturn -EAGAIN;\n\t}\n\n\tWARN_ON(test_bit(STRIPE_R5C_CACHING, &sh->state));\n\n\tfor (i = 0; i < sh->disks; i++) {\n\t\tvoid *addr;\n\n\t\tif (!test_bit(R5_Wantwrite, &sh->dev[i].flags) ||\n\t\t    test_bit(R5_InJournal, &sh->dev[i].flags))\n\t\t\tcontinue;\n\n\t\twrite_disks++;\n\t\t \n\t\tif (test_bit(STRIPE_LOG_TRAPPED, &sh->state))\n\t\t\tcontinue;\n\t\taddr = kmap_atomic(sh->dev[i].page);\n\t\tsh->dev[i].log_checksum = crc32c_le(log->uuid_checksum,\n\t\t\t\t\t\t    addr, PAGE_SIZE);\n\t\tkunmap_atomic(addr);\n\t}\n\tparity_pages = 1 + !!(sh->qd_idx >= 0);\n\tdata_pages = write_disks - parity_pages;\n\n\tset_bit(STRIPE_LOG_TRAPPED, &sh->state);\n\t \n\tclear_bit(STRIPE_DELAYED, &sh->state);\n\tatomic_inc(&sh->count);\n\n\tmutex_lock(&log->io_mutex);\n\t \n\treserve = (1 + write_disks) << (PAGE_SHIFT - 9);\n\n\tif (log->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_THROUGH) {\n\t\tif (!r5l_has_free_space(log, reserve)) {\n\t\t\tr5l_add_no_space_stripe(log, sh);\n\t\t\twake_reclaim = true;\n\t\t} else {\n\t\t\tret = r5l_log_stripe(log, sh, data_pages, parity_pages);\n\t\t\tif (ret) {\n\t\t\t\tspin_lock_irq(&log->io_list_lock);\n\t\t\t\tlist_add_tail(&sh->log_list,\n\t\t\t\t\t      &log->no_mem_stripes);\n\t\t\t\tspin_unlock_irq(&log->io_list_lock);\n\t\t\t}\n\t\t}\n\t} else {   \n\t\t \n\t\tif (test_bit(R5C_LOG_CRITICAL, &conf->cache_state) &&\n\t\t    sh->log_start == MaxSector) {\n\t\t\tr5l_add_no_space_stripe(log, sh);\n\t\t\twake_reclaim = true;\n\t\t\treserve = 0;\n\t\t} else if (!r5l_has_free_space(log, reserve)) {\n\t\t\tif (sh->log_start == log->last_checkpoint)\n\t\t\t\tBUG();\n\t\t\telse\n\t\t\t\tr5l_add_no_space_stripe(log, sh);\n\t\t} else {\n\t\t\tret = r5l_log_stripe(log, sh, data_pages, parity_pages);\n\t\t\tif (ret) {\n\t\t\t\tspin_lock_irq(&log->io_list_lock);\n\t\t\t\tlist_add_tail(&sh->log_list,\n\t\t\t\t\t      &log->no_mem_stripes);\n\t\t\t\tspin_unlock_irq(&log->io_list_lock);\n\t\t\t}\n\t\t}\n\t}\n\n\tmutex_unlock(&log->io_mutex);\n\tif (wake_reclaim)\n\t\tr5l_wake_reclaim(log, reserve);\n\treturn 0;\n}\n\nvoid r5l_write_stripe_run(struct r5l_log *log)\n{\n\tif (!log)\n\t\treturn;\n\tmutex_lock(&log->io_mutex);\n\tr5l_submit_current_io(log);\n\tmutex_unlock(&log->io_mutex);\n}\n\nint r5l_handle_flush_request(struct r5l_log *log, struct bio *bio)\n{\n\tif (log->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_THROUGH) {\n\t\t \n\t\tif (bio->bi_iter.bi_size == 0) {\n\t\t\tbio_endio(bio);\n\t\t\treturn 0;\n\t\t}\n\t\tbio->bi_opf &= ~REQ_PREFLUSH;\n\t} else {\n\t\t \n\t\tif (bio->bi_iter.bi_size == 0) {\n\t\t\tmutex_lock(&log->io_mutex);\n\t\t\tr5l_get_meta(log, 0);\n\t\t\tbio_list_add(&log->current_io->flush_barriers, bio);\n\t\t\tlog->current_io->has_flush = 1;\n\t\t\tlog->current_io->has_null_flush = 1;\n\t\t\tatomic_inc(&log->current_io->pending_stripe);\n\t\t\tr5l_submit_current_io(log);\n\t\t\tmutex_unlock(&log->io_mutex);\n\t\t\treturn 0;\n\t\t}\n\t}\n\treturn -EAGAIN;\n}\n\n \nstatic void r5l_run_no_space_stripes(struct r5l_log *log)\n{\n\tstruct stripe_head *sh;\n\n\tspin_lock(&log->no_space_stripes_lock);\n\twhile (!list_empty(&log->no_space_stripes)) {\n\t\tsh = list_first_entry(&log->no_space_stripes,\n\t\t\t\t      struct stripe_head, log_list);\n\t\tlist_del_init(&sh->log_list);\n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\traid5_release_stripe(sh);\n\t}\n\tspin_unlock(&log->no_space_stripes_lock);\n}\n\n \nstatic sector_t r5c_calculate_new_cp(struct r5conf *conf)\n{\n\tstruct stripe_head *sh;\n\tstruct r5l_log *log = conf->log;\n\tsector_t new_cp;\n\tunsigned long flags;\n\n\tif (log->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_THROUGH)\n\t\treturn log->next_checkpoint;\n\n\tspin_lock_irqsave(&log->stripe_in_journal_lock, flags);\n\tif (list_empty(&conf->log->stripe_in_journal_list)) {\n\t\t \n\t\tspin_unlock_irqrestore(&log->stripe_in_journal_lock, flags);\n\t\treturn log->next_checkpoint;\n\t}\n\tsh = list_first_entry(&conf->log->stripe_in_journal_list,\n\t\t\t      struct stripe_head, r5c);\n\tnew_cp = sh->log_start;\n\tspin_unlock_irqrestore(&log->stripe_in_journal_lock, flags);\n\treturn new_cp;\n}\n\nstatic sector_t r5l_reclaimable_space(struct r5l_log *log)\n{\n\tstruct r5conf *conf = log->rdev->mddev->private;\n\n\treturn r5l_ring_distance(log, log->last_checkpoint,\n\t\t\t\t r5c_calculate_new_cp(conf));\n}\n\nstatic void r5l_run_no_mem_stripe(struct r5l_log *log)\n{\n\tstruct stripe_head *sh;\n\n\tlockdep_assert_held(&log->io_list_lock);\n\n\tif (!list_empty(&log->no_mem_stripes)) {\n\t\tsh = list_first_entry(&log->no_mem_stripes,\n\t\t\t\t      struct stripe_head, log_list);\n\t\tlist_del_init(&sh->log_list);\n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\traid5_release_stripe(sh);\n\t}\n}\n\nstatic bool r5l_complete_finished_ios(struct r5l_log *log)\n{\n\tstruct r5l_io_unit *io, *next;\n\tbool found = false;\n\n\tlockdep_assert_held(&log->io_list_lock);\n\n\tlist_for_each_entry_safe(io, next, &log->finished_ios, log_sibling) {\n\t\t \n\t\tif (io->state < IO_UNIT_STRIPE_END)\n\t\t\tbreak;\n\n\t\tlog->next_checkpoint = io->log_start;\n\n\t\tlist_del(&io->log_sibling);\n\t\tmempool_free(io, &log->io_pool);\n\t\tr5l_run_no_mem_stripe(log);\n\n\t\tfound = true;\n\t}\n\n\treturn found;\n}\n\nstatic void __r5l_stripe_write_finished(struct r5l_io_unit *io)\n{\n\tstruct r5l_log *log = io->log;\n\tstruct r5conf *conf = log->rdev->mddev->private;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&log->io_list_lock, flags);\n\t__r5l_set_io_unit_state(io, IO_UNIT_STRIPE_END);\n\n\tif (!r5l_complete_finished_ios(log)) {\n\t\tspin_unlock_irqrestore(&log->io_list_lock, flags);\n\t\treturn;\n\t}\n\n\tif (r5l_reclaimable_space(log) > log->max_free_space ||\n\t    test_bit(R5C_LOG_TIGHT, &conf->cache_state))\n\t\tr5l_wake_reclaim(log, 0);\n\n\tspin_unlock_irqrestore(&log->io_list_lock, flags);\n\twake_up(&log->iounit_wait);\n}\n\nvoid r5l_stripe_write_finished(struct stripe_head *sh)\n{\n\tstruct r5l_io_unit *io;\n\n\tio = sh->log_io;\n\tsh->log_io = NULL;\n\n\tif (io && atomic_dec_and_test(&io->pending_stripe))\n\t\t__r5l_stripe_write_finished(io);\n}\n\nstatic void r5l_log_flush_endio(struct bio *bio)\n{\n\tstruct r5l_log *log = container_of(bio, struct r5l_log,\n\t\tflush_bio);\n\tunsigned long flags;\n\tstruct r5l_io_unit *io;\n\n\tif (bio->bi_status)\n\t\tmd_error(log->rdev->mddev, log->rdev);\n\tbio_uninit(bio);\n\n\tspin_lock_irqsave(&log->io_list_lock, flags);\n\tlist_for_each_entry(io, &log->flushing_ios, log_sibling)\n\t\tr5l_io_run_stripes(io);\n\tlist_splice_tail_init(&log->flushing_ios, &log->finished_ios);\n\tspin_unlock_irqrestore(&log->io_list_lock, flags);\n}\n\n \nvoid r5l_flush_stripe_to_raid(struct r5l_log *log)\n{\n\tbool do_flush;\n\n\tif (!log || !log->need_cache_flush)\n\t\treturn;\n\n\tspin_lock_irq(&log->io_list_lock);\n\t \n\tif (!list_empty(&log->flushing_ios)) {\n\t\tspin_unlock_irq(&log->io_list_lock);\n\t\treturn;\n\t}\n\tlist_splice_tail_init(&log->io_end_ios, &log->flushing_ios);\n\tdo_flush = !list_empty(&log->flushing_ios);\n\tspin_unlock_irq(&log->io_list_lock);\n\n\tif (!do_flush)\n\t\treturn;\n\tbio_init(&log->flush_bio, log->rdev->bdev, NULL, 0,\n\t\t  REQ_OP_WRITE | REQ_PREFLUSH);\n\tlog->flush_bio.bi_end_io = r5l_log_flush_endio;\n\tsubmit_bio(&log->flush_bio);\n}\n\nstatic void r5l_write_super(struct r5l_log *log, sector_t cp);\nstatic void r5l_write_super_and_discard_space(struct r5l_log *log,\n\tsector_t end)\n{\n\tstruct block_device *bdev = log->rdev->bdev;\n\tstruct mddev *mddev;\n\n\tr5l_write_super(log, end);\n\n\tif (!bdev_max_discard_sectors(bdev))\n\t\treturn;\n\n\tmddev = log->rdev->mddev;\n\t \n\tset_mask_bits(&mddev->sb_flags, 0,\n\t\tBIT(MD_SB_CHANGE_DEVS) | BIT(MD_SB_CHANGE_PENDING));\n\tif (!mddev_trylock(mddev))\n\t\treturn;\n\tmd_update_sb(mddev, 1);\n\tmddev_unlock(mddev);\n\n\t \n\tif (log->last_checkpoint < end) {\n\t\tblkdev_issue_discard(bdev,\n\t\t\t\tlog->last_checkpoint + log->rdev->data_offset,\n\t\t\t\tend - log->last_checkpoint, GFP_NOIO);\n\t} else {\n\t\tblkdev_issue_discard(bdev,\n\t\t\t\tlog->last_checkpoint + log->rdev->data_offset,\n\t\t\t\tlog->device_size - log->last_checkpoint,\n\t\t\t\tGFP_NOIO);\n\t\tblkdev_issue_discard(bdev, log->rdev->data_offset, end,\n\t\t\t\tGFP_NOIO);\n\t}\n}\n\n \nstatic void r5c_flush_stripe(struct r5conf *conf, struct stripe_head *sh)\n{\n\tBUG_ON(list_empty(&sh->lru));\n\tBUG_ON(!test_bit(STRIPE_R5C_CACHING, &sh->state));\n\tBUG_ON(test_bit(STRIPE_HANDLE, &sh->state));\n\n\t \n\tBUG_ON(test_bit(STRIPE_ON_RELEASE_LIST, &sh->state));\n\tlockdep_assert_held(&conf->device_lock);\n\n\tlist_del_init(&sh->lru);\n\tatomic_inc(&sh->count);\n\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\tatomic_inc(&conf->active_stripes);\n\tr5c_make_stripe_write_out(sh);\n\n\tif (test_bit(STRIPE_R5C_PARTIAL_STRIPE, &sh->state))\n\t\tatomic_inc(&conf->r5c_flushing_partial_stripes);\n\telse\n\t\tatomic_inc(&conf->r5c_flushing_full_stripes);\n\traid5_release_stripe(sh);\n}\n\n \nvoid r5c_flush_cache(struct r5conf *conf, int num)\n{\n\tint count;\n\tstruct stripe_head *sh, *next;\n\n\tlockdep_assert_held(&conf->device_lock);\n\tif (!conf->log)\n\t\treturn;\n\n\tcount = 0;\n\tlist_for_each_entry_safe(sh, next, &conf->r5c_full_stripe_list, lru) {\n\t\tr5c_flush_stripe(conf, sh);\n\t\tcount++;\n\t}\n\n\tif (count >= num)\n\t\treturn;\n\tlist_for_each_entry_safe(sh, next,\n\t\t\t\t &conf->r5c_partial_stripe_list, lru) {\n\t\tr5c_flush_stripe(conf, sh);\n\t\tif (++count >= num)\n\t\t\tbreak;\n\t}\n}\n\nstatic void r5c_do_reclaim(struct r5conf *conf)\n{\n\tstruct r5l_log *log = conf->log;\n\tstruct stripe_head *sh;\n\tint count = 0;\n\tunsigned long flags;\n\tint total_cached;\n\tint stripes_to_flush;\n\tint flushing_partial, flushing_full;\n\n\tif (!r5c_is_writeback(log))\n\t\treturn;\n\n\tflushing_partial = atomic_read(&conf->r5c_flushing_partial_stripes);\n\tflushing_full = atomic_read(&conf->r5c_flushing_full_stripes);\n\ttotal_cached = atomic_read(&conf->r5c_cached_partial_stripes) +\n\t\tatomic_read(&conf->r5c_cached_full_stripes) -\n\t\tflushing_full - flushing_partial;\n\n\tif (total_cached > conf->min_nr_stripes * 3 / 4 ||\n\t    atomic_read(&conf->empty_inactive_list_nr) > 0)\n\t\t \n\t\tstripes_to_flush = R5C_RECLAIM_STRIPE_GROUP;\n\telse if (total_cached > conf->min_nr_stripes * 1 / 2 ||\n\t\t atomic_read(&conf->r5c_cached_full_stripes) - flushing_full >\n\t\t R5C_FULL_STRIPE_FLUSH_BATCH(conf))\n\t\t \n\t\tstripes_to_flush = 0;\n\telse\n\t\t \n\t\tstripes_to_flush = -1;\n\n\tif (stripes_to_flush >= 0) {\n\t\tspin_lock_irqsave(&conf->device_lock, flags);\n\t\tr5c_flush_cache(conf, stripes_to_flush);\n\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\t}\n\n\t \n\tif (test_bit(R5C_LOG_TIGHT, &conf->cache_state)) {\n\t\tspin_lock_irqsave(&log->stripe_in_journal_lock, flags);\n\t\tspin_lock(&conf->device_lock);\n\t\tlist_for_each_entry(sh, &log->stripe_in_journal_list, r5c) {\n\t\t\t \n\t\t\tif (!list_empty(&sh->lru) &&\n\t\t\t    !test_bit(STRIPE_HANDLE, &sh->state) &&\n\t\t\t    atomic_read(&sh->count) == 0) {\n\t\t\t\tr5c_flush_stripe(conf, sh);\n\t\t\t\tif (count++ >= R5C_RECLAIM_STRIPE_GROUP)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&conf->device_lock);\n\t\tspin_unlock_irqrestore(&log->stripe_in_journal_lock, flags);\n\t}\n\n\tif (!test_bit(R5C_LOG_CRITICAL, &conf->cache_state))\n\t\tr5l_run_no_space_stripes(log);\n\n\tmd_wakeup_thread(conf->mddev->thread);\n}\n\nstatic void r5l_do_reclaim(struct r5l_log *log)\n{\n\tstruct r5conf *conf = log->rdev->mddev->private;\n\tsector_t reclaim_target = xchg(&log->reclaim_target, 0);\n\tsector_t reclaimable;\n\tsector_t next_checkpoint;\n\tbool write_super;\n\n\tspin_lock_irq(&log->io_list_lock);\n\twrite_super = r5l_reclaimable_space(log) > log->max_free_space ||\n\t\treclaim_target != 0 || !list_empty(&log->no_space_stripes);\n\t \n\twhile (1) {\n\t\treclaimable = r5l_reclaimable_space(log);\n\t\tif (reclaimable >= reclaim_target ||\n\t\t    (list_empty(&log->running_ios) &&\n\t\t     list_empty(&log->io_end_ios) &&\n\t\t     list_empty(&log->flushing_ios) &&\n\t\t     list_empty(&log->finished_ios)))\n\t\t\tbreak;\n\n\t\tmd_wakeup_thread(log->rdev->mddev->thread);\n\t\twait_event_lock_irq(log->iounit_wait,\n\t\t\t\t    r5l_reclaimable_space(log) > reclaimable,\n\t\t\t\t    log->io_list_lock);\n\t}\n\n\tnext_checkpoint = r5c_calculate_new_cp(conf);\n\tspin_unlock_irq(&log->io_list_lock);\n\n\tif (reclaimable == 0 || !write_super)\n\t\treturn;\n\n\t \n\tr5l_write_super_and_discard_space(log, next_checkpoint);\n\n\tmutex_lock(&log->io_mutex);\n\tlog->last_checkpoint = next_checkpoint;\n\tr5c_update_log_state(log);\n\tmutex_unlock(&log->io_mutex);\n\n\tr5l_run_no_space_stripes(log);\n}\n\nstatic void r5l_reclaim_thread(struct md_thread *thread)\n{\n\tstruct mddev *mddev = thread->mddev;\n\tstruct r5conf *conf = mddev->private;\n\tstruct r5l_log *log = conf->log;\n\n\tif (!log)\n\t\treturn;\n\tr5c_do_reclaim(conf);\n\tr5l_do_reclaim(log);\n}\n\nvoid r5l_wake_reclaim(struct r5l_log *log, sector_t space)\n{\n\tunsigned long target;\n\tunsigned long new = (unsigned long)space;  \n\n\tif (!log)\n\t\treturn;\n\n\ttarget = READ_ONCE(log->reclaim_target);\n\tdo {\n\t\tif (new < target)\n\t\t\treturn;\n\t} while (!try_cmpxchg(&log->reclaim_target, &target, new));\n\tmd_wakeup_thread(log->reclaim_thread);\n}\n\nvoid r5l_quiesce(struct r5l_log *log, int quiesce)\n{\n\tstruct mddev *mddev = log->rdev->mddev;\n\tstruct md_thread *thread = rcu_dereference_protected(\n\t\tlog->reclaim_thread, lockdep_is_held(&mddev->reconfig_mutex));\n\n\tif (quiesce) {\n\t\t \n\t\twake_up(&mddev->sb_wait);\n\t\tkthread_park(thread->tsk);\n\t\tr5l_wake_reclaim(log, MaxSector);\n\t\tr5l_do_reclaim(log);\n\t} else\n\t\tkthread_unpark(thread->tsk);\n}\n\nbool r5l_log_disk_error(struct r5conf *conf)\n{\n\tstruct r5l_log *log = conf->log;\n\n\t \n\tif (!log)\n\t\treturn test_bit(MD_HAS_JOURNAL, &conf->mddev->flags);\n\telse\n\t\treturn test_bit(Faulty, &log->rdev->flags);\n}\n\n#define R5L_RECOVERY_PAGE_POOL_SIZE 256\n\nstruct r5l_recovery_ctx {\n\tstruct page *meta_page;\t\t \n\tsector_t meta_total_blocks;\t \n\tsector_t pos;\t\t\t \n\tu64 seq;\t\t\t \n\tint data_parity_stripes;\t \n\tint data_only_stripes;\t\t \n\tstruct list_head cached_list;\n\n\t \n\tstruct page *ra_pool[R5L_RECOVERY_PAGE_POOL_SIZE];\n\tstruct bio_vec ra_bvec[R5L_RECOVERY_PAGE_POOL_SIZE];\n\tsector_t pool_offset;\t \n\tint total_pages;\t \n\tint valid_pages;\t \n};\n\nstatic int r5l_recovery_allocate_ra_pool(struct r5l_log *log,\n\t\t\t\t\t    struct r5l_recovery_ctx *ctx)\n{\n\tstruct page *page;\n\n\tctx->valid_pages = 0;\n\tctx->total_pages = 0;\n\twhile (ctx->total_pages < R5L_RECOVERY_PAGE_POOL_SIZE) {\n\t\tpage = alloc_page(GFP_KERNEL);\n\n\t\tif (!page)\n\t\t\tbreak;\n\t\tctx->ra_pool[ctx->total_pages] = page;\n\t\tctx->total_pages += 1;\n\t}\n\n\tif (ctx->total_pages == 0)\n\t\treturn -ENOMEM;\n\n\tctx->pool_offset = 0;\n\treturn 0;\n}\n\nstatic void r5l_recovery_free_ra_pool(struct r5l_log *log,\n\t\t\t\t\tstruct r5l_recovery_ctx *ctx)\n{\n\tint i;\n\n\tfor (i = 0; i < ctx->total_pages; ++i)\n\t\tput_page(ctx->ra_pool[i]);\n}\n\n \nstatic int r5l_recovery_fetch_ra_pool(struct r5l_log *log,\n\t\t\t\t      struct r5l_recovery_ctx *ctx,\n\t\t\t\t      sector_t offset)\n{\n\tstruct bio bio;\n\tint ret;\n\n\tbio_init(&bio, log->rdev->bdev, ctx->ra_bvec,\n\t\t R5L_RECOVERY_PAGE_POOL_SIZE, REQ_OP_READ);\n\tbio.bi_iter.bi_sector = log->rdev->data_offset + offset;\n\n\tctx->valid_pages = 0;\n\tctx->pool_offset = offset;\n\n\twhile (ctx->valid_pages < ctx->total_pages) {\n\t\t__bio_add_page(&bio, ctx->ra_pool[ctx->valid_pages], PAGE_SIZE,\n\t\t\t       0);\n\t\tctx->valid_pages += 1;\n\n\t\toffset = r5l_ring_add(log, offset, BLOCK_SECTORS);\n\n\t\tif (offset == 0)   \n\t\t\tbreak;\n\t}\n\n\tret = submit_bio_wait(&bio);\n\tbio_uninit(&bio);\n\treturn ret;\n}\n\n \nstatic int r5l_recovery_read_page(struct r5l_log *log,\n\t\t\t\t  struct r5l_recovery_ctx *ctx,\n\t\t\t\t  struct page *page,\n\t\t\t\t  sector_t offset)\n{\n\tint ret;\n\n\tif (offset < ctx->pool_offset ||\n\t    offset >= ctx->pool_offset + ctx->valid_pages * BLOCK_SECTORS) {\n\t\tret = r5l_recovery_fetch_ra_pool(log, ctx, offset);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tBUG_ON(offset < ctx->pool_offset ||\n\t       offset >= ctx->pool_offset + ctx->valid_pages * BLOCK_SECTORS);\n\n\tmemcpy(page_address(page),\n\t       page_address(ctx->ra_pool[(offset - ctx->pool_offset) >>\n\t\t\t\t\t BLOCK_SECTOR_SHIFT]),\n\t       PAGE_SIZE);\n\treturn 0;\n}\n\nstatic int r5l_recovery_read_meta_block(struct r5l_log *log,\n\t\t\t\t\tstruct r5l_recovery_ctx *ctx)\n{\n\tstruct page *page = ctx->meta_page;\n\tstruct r5l_meta_block *mb;\n\tu32 crc, stored_crc;\n\tint ret;\n\n\tret = r5l_recovery_read_page(log, ctx, page, ctx->pos);\n\tif (ret != 0)\n\t\treturn ret;\n\n\tmb = page_address(page);\n\tstored_crc = le32_to_cpu(mb->checksum);\n\tmb->checksum = 0;\n\n\tif (le32_to_cpu(mb->magic) != R5LOG_MAGIC ||\n\t    le64_to_cpu(mb->seq) != ctx->seq ||\n\t    mb->version != R5LOG_VERSION ||\n\t    le64_to_cpu(mb->position) != ctx->pos)\n\t\treturn -EINVAL;\n\n\tcrc = crc32c_le(log->uuid_checksum, mb, PAGE_SIZE);\n\tif (stored_crc != crc)\n\t\treturn -EINVAL;\n\n\tif (le32_to_cpu(mb->meta_size) > PAGE_SIZE)\n\t\treturn -EINVAL;\n\n\tctx->meta_total_blocks = BLOCK_SECTORS;\n\n\treturn 0;\n}\n\nstatic void\nr5l_recovery_create_empty_meta_block(struct r5l_log *log,\n\t\t\t\t     struct page *page,\n\t\t\t\t     sector_t pos, u64 seq)\n{\n\tstruct r5l_meta_block *mb;\n\n\tmb = page_address(page);\n\tclear_page(mb);\n\tmb->magic = cpu_to_le32(R5LOG_MAGIC);\n\tmb->version = R5LOG_VERSION;\n\tmb->meta_size = cpu_to_le32(sizeof(struct r5l_meta_block));\n\tmb->seq = cpu_to_le64(seq);\n\tmb->position = cpu_to_le64(pos);\n}\n\nstatic int r5l_log_write_empty_meta_block(struct r5l_log *log, sector_t pos,\n\t\t\t\t\t  u64 seq)\n{\n\tstruct page *page;\n\tstruct r5l_meta_block *mb;\n\n\tpage = alloc_page(GFP_KERNEL);\n\tif (!page)\n\t\treturn -ENOMEM;\n\tr5l_recovery_create_empty_meta_block(log, page, pos, seq);\n\tmb = page_address(page);\n\tmb->checksum = cpu_to_le32(crc32c_le(log->uuid_checksum,\n\t\t\t\t\t     mb, PAGE_SIZE));\n\tif (!sync_page_io(log->rdev, pos, PAGE_SIZE, page, REQ_OP_WRITE |\n\t\t\t  REQ_SYNC | REQ_FUA, false)) {\n\t\t__free_page(page);\n\t\treturn -EIO;\n\t}\n\t__free_page(page);\n\treturn 0;\n}\n\n \nstatic void r5l_recovery_load_data(struct r5l_log *log,\n\t\t\t\t   struct stripe_head *sh,\n\t\t\t\t   struct r5l_recovery_ctx *ctx,\n\t\t\t\t   struct r5l_payload_data_parity *payload,\n\t\t\t\t   sector_t log_offset)\n{\n\tstruct mddev *mddev = log->rdev->mddev;\n\tstruct r5conf *conf = mddev->private;\n\tint dd_idx;\n\n\traid5_compute_sector(conf,\n\t\t\t     le64_to_cpu(payload->location), 0,\n\t\t\t     &dd_idx, sh);\n\tr5l_recovery_read_page(log, ctx, sh->dev[dd_idx].page, log_offset);\n\tsh->dev[dd_idx].log_checksum =\n\t\tle32_to_cpu(payload->checksum[0]);\n\tctx->meta_total_blocks += BLOCK_SECTORS;\n\n\tset_bit(R5_Wantwrite, &sh->dev[dd_idx].flags);\n\tset_bit(STRIPE_R5C_CACHING, &sh->state);\n}\n\nstatic void r5l_recovery_load_parity(struct r5l_log *log,\n\t\t\t\t     struct stripe_head *sh,\n\t\t\t\t     struct r5l_recovery_ctx *ctx,\n\t\t\t\t     struct r5l_payload_data_parity *payload,\n\t\t\t\t     sector_t log_offset)\n{\n\tstruct mddev *mddev = log->rdev->mddev;\n\tstruct r5conf *conf = mddev->private;\n\n\tctx->meta_total_blocks += BLOCK_SECTORS * conf->max_degraded;\n\tr5l_recovery_read_page(log, ctx, sh->dev[sh->pd_idx].page, log_offset);\n\tsh->dev[sh->pd_idx].log_checksum =\n\t\tle32_to_cpu(payload->checksum[0]);\n\tset_bit(R5_Wantwrite, &sh->dev[sh->pd_idx].flags);\n\n\tif (sh->qd_idx >= 0) {\n\t\tr5l_recovery_read_page(\n\t\t\tlog, ctx, sh->dev[sh->qd_idx].page,\n\t\t\tr5l_ring_add(log, log_offset, BLOCK_SECTORS));\n\t\tsh->dev[sh->qd_idx].log_checksum =\n\t\t\tle32_to_cpu(payload->checksum[1]);\n\t\tset_bit(R5_Wantwrite, &sh->dev[sh->qd_idx].flags);\n\t}\n\tclear_bit(STRIPE_R5C_CACHING, &sh->state);\n}\n\nstatic void r5l_recovery_reset_stripe(struct stripe_head *sh)\n{\n\tint i;\n\n\tsh->state = 0;\n\tsh->log_start = MaxSector;\n\tfor (i = sh->disks; i--; )\n\t\tsh->dev[i].flags = 0;\n}\n\nstatic void\nr5l_recovery_replay_one_stripe(struct r5conf *conf,\n\t\t\t       struct stripe_head *sh,\n\t\t\t       struct r5l_recovery_ctx *ctx)\n{\n\tstruct md_rdev *rdev, *rrdev;\n\tint disk_index;\n\tint data_count = 0;\n\n\tfor (disk_index = 0; disk_index < sh->disks; disk_index++) {\n\t\tif (!test_bit(R5_Wantwrite, &sh->dev[disk_index].flags))\n\t\t\tcontinue;\n\t\tif (disk_index == sh->qd_idx || disk_index == sh->pd_idx)\n\t\t\tcontinue;\n\t\tdata_count++;\n\t}\n\n\t \n\tif (data_count == 0)\n\t\tgoto out;\n\n\tfor (disk_index = 0; disk_index < sh->disks; disk_index++) {\n\t\tif (!test_bit(R5_Wantwrite, &sh->dev[disk_index].flags))\n\t\t\tcontinue;\n\n\t\t \n\t\trcu_read_lock();\n\t\trdev = rcu_dereference(conf->disks[disk_index].rdev);\n\t\tif (rdev) {\n\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\trcu_read_unlock();\n\t\t\tsync_page_io(rdev, sh->sector, PAGE_SIZE,\n\t\t\t\t     sh->dev[disk_index].page, REQ_OP_WRITE,\n\t\t\t\t     false);\n\t\t\trdev_dec_pending(rdev, rdev->mddev);\n\t\t\trcu_read_lock();\n\t\t}\n\t\trrdev = rcu_dereference(conf->disks[disk_index].replacement);\n\t\tif (rrdev) {\n\t\t\tatomic_inc(&rrdev->nr_pending);\n\t\t\trcu_read_unlock();\n\t\t\tsync_page_io(rrdev, sh->sector, PAGE_SIZE,\n\t\t\t\t     sh->dev[disk_index].page, REQ_OP_WRITE,\n\t\t\t\t     false);\n\t\t\trdev_dec_pending(rrdev, rrdev->mddev);\n\t\t\trcu_read_lock();\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\tctx->data_parity_stripes++;\nout:\n\tr5l_recovery_reset_stripe(sh);\n}\n\nstatic struct stripe_head *\nr5c_recovery_alloc_stripe(\n\t\tstruct r5conf *conf,\n\t\tsector_t stripe_sect,\n\t\tint noblock)\n{\n\tstruct stripe_head *sh;\n\n\tsh = raid5_get_active_stripe(conf, NULL, stripe_sect,\n\t\t\t\t     noblock ? R5_GAS_NOBLOCK : 0);\n\tif (!sh)\n\t\treturn NULL;   \n\n\tr5l_recovery_reset_stripe(sh);\n\n\treturn sh;\n}\n\nstatic struct stripe_head *\nr5c_recovery_lookup_stripe(struct list_head *list, sector_t sect)\n{\n\tstruct stripe_head *sh;\n\n\tlist_for_each_entry(sh, list, lru)\n\t\tif (sh->sector == sect)\n\t\t\treturn sh;\n\treturn NULL;\n}\n\nstatic void\nr5c_recovery_drop_stripes(struct list_head *cached_stripe_list,\n\t\t\t  struct r5l_recovery_ctx *ctx)\n{\n\tstruct stripe_head *sh, *next;\n\n\tlist_for_each_entry_safe(sh, next, cached_stripe_list, lru) {\n\t\tr5l_recovery_reset_stripe(sh);\n\t\tlist_del_init(&sh->lru);\n\t\traid5_release_stripe(sh);\n\t}\n}\n\nstatic void\nr5c_recovery_replay_stripes(struct list_head *cached_stripe_list,\n\t\t\t    struct r5l_recovery_ctx *ctx)\n{\n\tstruct stripe_head *sh, *next;\n\n\tlist_for_each_entry_safe(sh, next, cached_stripe_list, lru)\n\t\tif (!test_bit(STRIPE_R5C_CACHING, &sh->state)) {\n\t\t\tr5l_recovery_replay_one_stripe(sh->raid_conf, sh, ctx);\n\t\t\tlist_del_init(&sh->lru);\n\t\t\traid5_release_stripe(sh);\n\t\t}\n}\n\n \nstatic int\nr5l_recovery_verify_data_checksum(struct r5l_log *log,\n\t\t\t\t  struct r5l_recovery_ctx *ctx,\n\t\t\t\t  struct page *page,\n\t\t\t\t  sector_t log_offset, __le32 log_checksum)\n{\n\tvoid *addr;\n\tu32 checksum;\n\n\tr5l_recovery_read_page(log, ctx, page, log_offset);\n\taddr = kmap_atomic(page);\n\tchecksum = crc32c_le(log->uuid_checksum, addr, PAGE_SIZE);\n\tkunmap_atomic(addr);\n\treturn (le32_to_cpu(log_checksum) == checksum) ? 0 : -EINVAL;\n}\n\n \nstatic int\nr5l_recovery_verify_data_checksum_for_mb(struct r5l_log *log,\n\t\t\t\t\t struct r5l_recovery_ctx *ctx)\n{\n\tstruct mddev *mddev = log->rdev->mddev;\n\tstruct r5conf *conf = mddev->private;\n\tstruct r5l_meta_block *mb = page_address(ctx->meta_page);\n\tsector_t mb_offset = sizeof(struct r5l_meta_block);\n\tsector_t log_offset = r5l_ring_add(log, ctx->pos, BLOCK_SECTORS);\n\tstruct page *page;\n\tstruct r5l_payload_data_parity *payload;\n\tstruct r5l_payload_flush *payload_flush;\n\n\tpage = alloc_page(GFP_KERNEL);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\twhile (mb_offset < le32_to_cpu(mb->meta_size)) {\n\t\tpayload = (void *)mb + mb_offset;\n\t\tpayload_flush = (void *)mb + mb_offset;\n\n\t\tif (le16_to_cpu(payload->header.type) == R5LOG_PAYLOAD_DATA) {\n\t\t\tif (r5l_recovery_verify_data_checksum(\n\t\t\t\t    log, ctx, page, log_offset,\n\t\t\t\t    payload->checksum[0]) < 0)\n\t\t\t\tgoto mismatch;\n\t\t} else if (le16_to_cpu(payload->header.type) == R5LOG_PAYLOAD_PARITY) {\n\t\t\tif (r5l_recovery_verify_data_checksum(\n\t\t\t\t    log, ctx, page, log_offset,\n\t\t\t\t    payload->checksum[0]) < 0)\n\t\t\t\tgoto mismatch;\n\t\t\tif (conf->max_degraded == 2 &&  \n\t\t\t    r5l_recovery_verify_data_checksum(\n\t\t\t\t    log, ctx, page,\n\t\t\t\t    r5l_ring_add(log, log_offset,\n\t\t\t\t\t\t BLOCK_SECTORS),\n\t\t\t\t    payload->checksum[1]) < 0)\n\t\t\t\tgoto mismatch;\n\t\t} else if (le16_to_cpu(payload->header.type) == R5LOG_PAYLOAD_FLUSH) {\n\t\t\t \n\t\t} else  \n\t\t\tgoto mismatch;\n\n\t\tif (le16_to_cpu(payload->header.type) == R5LOG_PAYLOAD_FLUSH) {\n\t\t\tmb_offset += sizeof(struct r5l_payload_flush) +\n\t\t\t\tle32_to_cpu(payload_flush->size);\n\t\t} else {\n\t\t\t \n\t\t\tlog_offset = r5l_ring_add(log, log_offset,\n\t\t\t\t\t\t  le32_to_cpu(payload->size));\n\t\t\tmb_offset += sizeof(struct r5l_payload_data_parity) +\n\t\t\t\tsizeof(__le32) *\n\t\t\t\t(le32_to_cpu(payload->size) >> (PAGE_SHIFT - 9));\n\t\t}\n\n\t}\n\n\tput_page(page);\n\treturn 0;\n\nmismatch:\n\tput_page(page);\n\treturn -EINVAL;\n}\n\n \nstatic int\nr5c_recovery_analyze_meta_block(struct r5l_log *log,\n\t\t\t\tstruct r5l_recovery_ctx *ctx,\n\t\t\t\tstruct list_head *cached_stripe_list)\n{\n\tstruct mddev *mddev = log->rdev->mddev;\n\tstruct r5conf *conf = mddev->private;\n\tstruct r5l_meta_block *mb;\n\tstruct r5l_payload_data_parity *payload;\n\tstruct r5l_payload_flush *payload_flush;\n\tint mb_offset;\n\tsector_t log_offset;\n\tsector_t stripe_sect;\n\tstruct stripe_head *sh;\n\tint ret;\n\n\t \n\tret = r5l_recovery_verify_data_checksum_for_mb(log, ctx);\n\tif (ret == -EINVAL)\n\t\treturn -EAGAIN;\n\telse if (ret)\n\t\treturn ret;    \n\n\tmb = page_address(ctx->meta_page);\n\tmb_offset = sizeof(struct r5l_meta_block);\n\tlog_offset = r5l_ring_add(log, ctx->pos, BLOCK_SECTORS);\n\n\twhile (mb_offset < le32_to_cpu(mb->meta_size)) {\n\t\tint dd;\n\n\t\tpayload = (void *)mb + mb_offset;\n\t\tpayload_flush = (void *)mb + mb_offset;\n\n\t\tif (le16_to_cpu(payload->header.type) == R5LOG_PAYLOAD_FLUSH) {\n\t\t\tint i, count;\n\n\t\t\tcount = le32_to_cpu(payload_flush->size) / sizeof(__le64);\n\t\t\tfor (i = 0; i < count; ++i) {\n\t\t\t\tstripe_sect = le64_to_cpu(payload_flush->flush_stripes[i]);\n\t\t\t\tsh = r5c_recovery_lookup_stripe(cached_stripe_list,\n\t\t\t\t\t\t\t\tstripe_sect);\n\t\t\t\tif (sh) {\n\t\t\t\t\tWARN_ON(test_bit(STRIPE_R5C_CACHING, &sh->state));\n\t\t\t\t\tr5l_recovery_reset_stripe(sh);\n\t\t\t\t\tlist_del_init(&sh->lru);\n\t\t\t\t\traid5_release_stripe(sh);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tmb_offset += sizeof(struct r5l_payload_flush) +\n\t\t\t\tle32_to_cpu(payload_flush->size);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tstripe_sect = (le16_to_cpu(payload->header.type) == R5LOG_PAYLOAD_DATA) ?\n\t\t\traid5_compute_sector(\n\t\t\t\tconf, le64_to_cpu(payload->location), 0, &dd,\n\t\t\t\tNULL)\n\t\t\t: le64_to_cpu(payload->location);\n\n\t\tsh = r5c_recovery_lookup_stripe(cached_stripe_list,\n\t\t\t\t\t\tstripe_sect);\n\n\t\tif (!sh) {\n\t\t\tsh = r5c_recovery_alloc_stripe(conf, stripe_sect, 1);\n\t\t\t \n\t\t\tif (!sh) {\n\t\t\t\tr5c_recovery_replay_stripes(\n\t\t\t\t\tcached_stripe_list, ctx);\n\t\t\t\tsh = r5c_recovery_alloc_stripe(\n\t\t\t\t\tconf, stripe_sect, 1);\n\t\t\t}\n\t\t\tif (!sh) {\n\t\t\t\tint new_size = conf->min_nr_stripes * 2;\n\t\t\t\tpr_debug(\"md/raid:%s: Increasing stripe cache size to %d to recovery data on journal.\\n\",\n\t\t\t\t\tmdname(mddev),\n\t\t\t\t\tnew_size);\n\t\t\t\tret = raid5_set_cache_size(mddev, new_size);\n\t\t\t\tif (conf->min_nr_stripes <= new_size / 2) {\n\t\t\t\t\tpr_err(\"md/raid:%s: Cannot increase cache size, ret=%d, new_size=%d, min_nr_stripes=%d, max_nr_stripes=%d\\n\",\n\t\t\t\t\t\tmdname(mddev),\n\t\t\t\t\t\tret,\n\t\t\t\t\t\tnew_size,\n\t\t\t\t\t\tconf->min_nr_stripes,\n\t\t\t\t\t\tconf->max_nr_stripes);\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\t}\n\t\t\t\tsh = r5c_recovery_alloc_stripe(\n\t\t\t\t\tconf, stripe_sect, 0);\n\t\t\t}\n\t\t\tif (!sh) {\n\t\t\t\tpr_err(\"md/raid:%s: Cannot get enough stripes due to memory pressure. Recovery failed.\\n\",\n\t\t\t\t\tmdname(mddev));\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\tlist_add_tail(&sh->lru, cached_stripe_list);\n\t\t}\n\n\t\tif (le16_to_cpu(payload->header.type) == R5LOG_PAYLOAD_DATA) {\n\t\t\tif (!test_bit(STRIPE_R5C_CACHING, &sh->state) &&\n\t\t\t    test_bit(R5_Wantwrite, &sh->dev[sh->pd_idx].flags)) {\n\t\t\t\tr5l_recovery_replay_one_stripe(conf, sh, ctx);\n\t\t\t\tlist_move_tail(&sh->lru, cached_stripe_list);\n\t\t\t}\n\t\t\tr5l_recovery_load_data(log, sh, ctx, payload,\n\t\t\t\t\t       log_offset);\n\t\t} else if (le16_to_cpu(payload->header.type) == R5LOG_PAYLOAD_PARITY)\n\t\t\tr5l_recovery_load_parity(log, sh, ctx, payload,\n\t\t\t\t\t\t log_offset);\n\t\telse\n\t\t\treturn -EINVAL;\n\n\t\tlog_offset = r5l_ring_add(log, log_offset,\n\t\t\t\t\t  le32_to_cpu(payload->size));\n\n\t\tmb_offset += sizeof(struct r5l_payload_data_parity) +\n\t\t\tsizeof(__le32) *\n\t\t\t(le32_to_cpu(payload->size) >> (PAGE_SHIFT - 9));\n\t}\n\n\treturn 0;\n}\n\n \nstatic void r5c_recovery_load_one_stripe(struct r5l_log *log,\n\t\t\t\t\t struct stripe_head *sh)\n{\n\tstruct r5dev *dev;\n\tint i;\n\n\tfor (i = sh->disks; i--; ) {\n\t\tdev = sh->dev + i;\n\t\tif (test_and_clear_bit(R5_Wantwrite, &dev->flags)) {\n\t\t\tset_bit(R5_InJournal, &dev->flags);\n\t\t\tset_bit(R5_UPTODATE, &dev->flags);\n\t\t}\n\t}\n}\n\n \nstatic int r5c_recovery_flush_log(struct r5l_log *log,\n\t\t\t\t  struct r5l_recovery_ctx *ctx)\n{\n\tstruct stripe_head *sh;\n\tint ret = 0;\n\n\t \n\twhile (1) {\n\t\tif (r5l_recovery_read_meta_block(log, ctx))\n\t\t\tbreak;\n\n\t\tret = r5c_recovery_analyze_meta_block(log, ctx,\n\t\t\t\t\t\t      &ctx->cached_list);\n\t\t \n\t\tif (ret && ret != -EAGAIN)\n\t\t\tbreak;    \n\t\tctx->seq++;\n\t\tctx->pos = r5l_ring_add(log, ctx->pos, ctx->meta_total_blocks);\n\t}\n\n\tif (ret == -ENOMEM) {\n\t\tr5c_recovery_drop_stripes(&ctx->cached_list, ctx);\n\t\treturn ret;\n\t}\n\n\t \n\tr5c_recovery_replay_stripes(&ctx->cached_list, ctx);\n\n\t \n\tlist_for_each_entry(sh, &ctx->cached_list, lru) {\n\t\tWARN_ON(!test_bit(STRIPE_R5C_CACHING, &sh->state));\n\t\tr5c_recovery_load_one_stripe(log, sh);\n\t\tctx->data_only_stripes++;\n\t}\n\n\treturn 0;\n}\n\n \n\n \nstatic int\nr5c_recovery_rewrite_data_only_stripes(struct r5l_log *log,\n\t\t\t\t       struct r5l_recovery_ctx *ctx)\n{\n\tstruct stripe_head *sh;\n\tstruct mddev *mddev = log->rdev->mddev;\n\tstruct page *page;\n\tsector_t next_checkpoint = MaxSector;\n\n\tpage = alloc_page(GFP_KERNEL);\n\tif (!page) {\n\t\tpr_err(\"md/raid:%s: cannot allocate memory to rewrite data only stripes\\n\",\n\t\t       mdname(mddev));\n\t\treturn -ENOMEM;\n\t}\n\n\tWARN_ON(list_empty(&ctx->cached_list));\n\n\tlist_for_each_entry(sh, &ctx->cached_list, lru) {\n\t\tstruct r5l_meta_block *mb;\n\t\tint i;\n\t\tint offset;\n\t\tsector_t write_pos;\n\n\t\tWARN_ON(!test_bit(STRIPE_R5C_CACHING, &sh->state));\n\t\tr5l_recovery_create_empty_meta_block(log, page,\n\t\t\t\t\t\t     ctx->pos, ctx->seq);\n\t\tmb = page_address(page);\n\t\toffset = le32_to_cpu(mb->meta_size);\n\t\twrite_pos = r5l_ring_add(log, ctx->pos, BLOCK_SECTORS);\n\n\t\tfor (i = sh->disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tstruct r5l_payload_data_parity *payload;\n\t\t\tvoid *addr;\n\n\t\t\tif (test_bit(R5_InJournal, &dev->flags)) {\n\t\t\t\tpayload = (void *)mb + offset;\n\t\t\t\tpayload->header.type = cpu_to_le16(\n\t\t\t\t\tR5LOG_PAYLOAD_DATA);\n\t\t\t\tpayload->size = cpu_to_le32(BLOCK_SECTORS);\n\t\t\t\tpayload->location = cpu_to_le64(\n\t\t\t\t\traid5_compute_blocknr(sh, i, 0));\n\t\t\t\taddr = kmap_atomic(dev->page);\n\t\t\t\tpayload->checksum[0] = cpu_to_le32(\n\t\t\t\t\tcrc32c_le(log->uuid_checksum, addr,\n\t\t\t\t\t\t  PAGE_SIZE));\n\t\t\t\tkunmap_atomic(addr);\n\t\t\t\tsync_page_io(log->rdev, write_pos, PAGE_SIZE,\n\t\t\t\t\t     dev->page, REQ_OP_WRITE, false);\n\t\t\t\twrite_pos = r5l_ring_add(log, write_pos,\n\t\t\t\t\t\t\t BLOCK_SECTORS);\n\t\t\t\toffset += sizeof(__le32) +\n\t\t\t\t\tsizeof(struct r5l_payload_data_parity);\n\n\t\t\t}\n\t\t}\n\t\tmb->meta_size = cpu_to_le32(offset);\n\t\tmb->checksum = cpu_to_le32(crc32c_le(log->uuid_checksum,\n\t\t\t\t\t\t     mb, PAGE_SIZE));\n\t\tsync_page_io(log->rdev, ctx->pos, PAGE_SIZE, page,\n\t\t\t     REQ_OP_WRITE | REQ_SYNC | REQ_FUA, false);\n\t\tsh->log_start = ctx->pos;\n\t\tlist_add_tail(&sh->r5c, &log->stripe_in_journal_list);\n\t\tatomic_inc(&log->stripe_in_journal_count);\n\t\tctx->pos = write_pos;\n\t\tctx->seq += 1;\n\t\tnext_checkpoint = sh->log_start;\n\t}\n\tlog->next_checkpoint = next_checkpoint;\n\t__free_page(page);\n\treturn 0;\n}\n\nstatic void r5c_recovery_flush_data_only_stripes(struct r5l_log *log,\n\t\t\t\t\t\t struct r5l_recovery_ctx *ctx)\n{\n\tstruct mddev *mddev = log->rdev->mddev;\n\tstruct r5conf *conf = mddev->private;\n\tstruct stripe_head *sh, *next;\n\tbool cleared_pending = false;\n\n\tif (ctx->data_only_stripes == 0)\n\t\treturn;\n\n\tif (test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags)) {\n\t\tcleared_pending = true;\n\t\tclear_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags);\n\t}\n\tlog->r5c_journal_mode = R5C_JOURNAL_MODE_WRITE_BACK;\n\n\tlist_for_each_entry_safe(sh, next, &ctx->cached_list, lru) {\n\t\tr5c_make_stripe_write_out(sh);\n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\tlist_del_init(&sh->lru);\n\t\traid5_release_stripe(sh);\n\t}\n\n\t \n\twait_event(conf->wait_for_quiescent,\n\t\t   atomic_read(&conf->active_stripes) == 0);\n\n\tlog->r5c_journal_mode = R5C_JOURNAL_MODE_WRITE_THROUGH;\n\tif (cleared_pending)\n\t\tset_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags);\n}\n\nstatic int r5l_recovery_log(struct r5l_log *log)\n{\n\tstruct mddev *mddev = log->rdev->mddev;\n\tstruct r5l_recovery_ctx *ctx;\n\tint ret;\n\tsector_t pos;\n\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tctx->pos = log->last_checkpoint;\n\tctx->seq = log->last_cp_seq;\n\tINIT_LIST_HEAD(&ctx->cached_list);\n\tctx->meta_page = alloc_page(GFP_KERNEL);\n\n\tif (!ctx->meta_page) {\n\t\tret =  -ENOMEM;\n\t\tgoto meta_page;\n\t}\n\n\tif (r5l_recovery_allocate_ra_pool(log, ctx) != 0) {\n\t\tret = -ENOMEM;\n\t\tgoto ra_pool;\n\t}\n\n\tret = r5c_recovery_flush_log(log, ctx);\n\n\tif (ret)\n\t\tgoto error;\n\n\tpos = ctx->pos;\n\tctx->seq += 10000;\n\n\tif ((ctx->data_only_stripes == 0) && (ctx->data_parity_stripes == 0))\n\t\tpr_info(\"md/raid:%s: starting from clean shutdown\\n\",\n\t\t\t mdname(mddev));\n\telse\n\t\tpr_info(\"md/raid:%s: recovering %d data-only stripes and %d data-parity stripes\\n\",\n\t\t\t mdname(mddev), ctx->data_only_stripes,\n\t\t\t ctx->data_parity_stripes);\n\n\tif (ctx->data_only_stripes == 0) {\n\t\tlog->next_checkpoint = ctx->pos;\n\t\tr5l_log_write_empty_meta_block(log, ctx->pos, ctx->seq++);\n\t\tctx->pos = r5l_ring_add(log, ctx->pos, BLOCK_SECTORS);\n\t} else if (r5c_recovery_rewrite_data_only_stripes(log, ctx)) {\n\t\tpr_err(\"md/raid:%s: failed to rewrite stripes to journal\\n\",\n\t\t       mdname(mddev));\n\t\tret =  -EIO;\n\t\tgoto error;\n\t}\n\n\tlog->log_start = ctx->pos;\n\tlog->seq = ctx->seq;\n\tlog->last_checkpoint = pos;\n\tr5l_write_super(log, pos);\n\n\tr5c_recovery_flush_data_only_stripes(log, ctx);\n\tret = 0;\nerror:\n\tr5l_recovery_free_ra_pool(log, ctx);\nra_pool:\n\t__free_page(ctx->meta_page);\nmeta_page:\n\tkfree(ctx);\n\treturn ret;\n}\n\nstatic void r5l_write_super(struct r5l_log *log, sector_t cp)\n{\n\tstruct mddev *mddev = log->rdev->mddev;\n\n\tlog->rdev->journal_tail = cp;\n\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n}\n\nstatic ssize_t r5c_journal_mode_show(struct mddev *mddev, char *page)\n{\n\tstruct r5conf *conf;\n\tint ret;\n\n\tret = mddev_lock(mddev);\n\tif (ret)\n\t\treturn ret;\n\n\tconf = mddev->private;\n\tif (!conf || !conf->log)\n\t\tgoto out_unlock;\n\n\tswitch (conf->log->r5c_journal_mode) {\n\tcase R5C_JOURNAL_MODE_WRITE_THROUGH:\n\t\tret = snprintf(\n\t\t\tpage, PAGE_SIZE, \"[%s] %s\\n\",\n\t\t\tr5c_journal_mode_str[R5C_JOURNAL_MODE_WRITE_THROUGH],\n\t\t\tr5c_journal_mode_str[R5C_JOURNAL_MODE_WRITE_BACK]);\n\t\tbreak;\n\tcase R5C_JOURNAL_MODE_WRITE_BACK:\n\t\tret = snprintf(\n\t\t\tpage, PAGE_SIZE, \"%s [%s]\\n\",\n\t\t\tr5c_journal_mode_str[R5C_JOURNAL_MODE_WRITE_THROUGH],\n\t\t\tr5c_journal_mode_str[R5C_JOURNAL_MODE_WRITE_BACK]);\n\t\tbreak;\n\tdefault:\n\t\tret = 0;\n\t}\n\nout_unlock:\n\tmddev_unlock(mddev);\n\treturn ret;\n}\n\n \nint r5c_journal_mode_set(struct mddev *mddev, int mode)\n{\n\tstruct r5conf *conf;\n\n\tif (mode < R5C_JOURNAL_MODE_WRITE_THROUGH ||\n\t    mode > R5C_JOURNAL_MODE_WRITE_BACK)\n\t\treturn -EINVAL;\n\n\tconf = mddev->private;\n\tif (!conf || !conf->log)\n\t\treturn -ENODEV;\n\n\tif (raid5_calc_degraded(conf) > 0 &&\n\t    mode == R5C_JOURNAL_MODE_WRITE_BACK)\n\t\treturn -EINVAL;\n\n\tmddev_suspend(mddev);\n\tconf->log->r5c_journal_mode = mode;\n\tmddev_resume(mddev);\n\n\tpr_debug(\"md/raid:%s: setting r5c cache mode to %d: %s\\n\",\n\t\t mdname(mddev), mode, r5c_journal_mode_str[mode]);\n\treturn 0;\n}\nEXPORT_SYMBOL(r5c_journal_mode_set);\n\nstatic ssize_t r5c_journal_mode_store(struct mddev *mddev,\n\t\t\t\t      const char *page, size_t length)\n{\n\tint mode = ARRAY_SIZE(r5c_journal_mode_str);\n\tsize_t len = length;\n\tint ret;\n\n\tif (len < 2)\n\t\treturn -EINVAL;\n\n\tif (page[len - 1] == '\\n')\n\t\tlen--;\n\n\twhile (mode--)\n\t\tif (strlen(r5c_journal_mode_str[mode]) == len &&\n\t\t    !strncmp(page, r5c_journal_mode_str[mode], len))\n\t\t\tbreak;\n\tret = mddev_lock(mddev);\n\tif (ret)\n\t\treturn ret;\n\tret = r5c_journal_mode_set(mddev, mode);\n\tmddev_unlock(mddev);\n\treturn ret ?: length;\n}\n\nstruct md_sysfs_entry\nr5c_journal_mode = __ATTR(journal_mode, 0644,\n\t\t\t  r5c_journal_mode_show, r5c_journal_mode_store);\n\n \nint r5c_try_caching_write(struct r5conf *conf,\n\t\t\t  struct stripe_head *sh,\n\t\t\t  struct stripe_head_state *s,\n\t\t\t  int disks)\n{\n\tstruct r5l_log *log = conf->log;\n\tint i;\n\tstruct r5dev *dev;\n\tint to_cache = 0;\n\tvoid __rcu **pslot;\n\tsector_t tree_index;\n\tint ret;\n\tuintptr_t refcount;\n\n\tBUG_ON(!r5c_is_writeback(log));\n\n\tif (!test_bit(STRIPE_R5C_CACHING, &sh->state)) {\n\t\t \n\n\t\t \n\t\tif (s->injournal > 0 || s->written > 0)\n\t\t\treturn -EAGAIN;\n\t\t \n\t\tset_bit(STRIPE_R5C_CACHING, &sh->state);\n\t}\n\n\t \n\tif (s->failed || test_bit(STRIPE_SYNCING, &sh->state)) {\n\t\tr5c_make_stripe_write_out(sh);\n\t\treturn -EAGAIN;\n\t}\n\n\tfor (i = disks; i--; ) {\n\t\tdev = &sh->dev[i];\n\t\t \n\t\tif (dev->towrite && !test_bit(R5_OVERWRITE, &dev->flags) &&\n\t\t    !test_bit(R5_InJournal, &dev->flags)) {\n\t\t\tr5c_make_stripe_write_out(sh);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\n\t \n\tif (!test_bit(STRIPE_R5C_PARTIAL_STRIPE, &sh->state) &&\n\t    !test_bit(STRIPE_R5C_FULL_STRIPE, &sh->state)) {\n\t\ttree_index = r5c_tree_index(conf, sh->sector);\n\t\tspin_lock(&log->tree_lock);\n\t\tpslot = radix_tree_lookup_slot(&log->big_stripe_tree,\n\t\t\t\t\t       tree_index);\n\t\tif (pslot) {\n\t\t\trefcount = (uintptr_t)radix_tree_deref_slot_protected(\n\t\t\t\tpslot, &log->tree_lock) >>\n\t\t\t\tR5C_RADIX_COUNT_SHIFT;\n\t\t\tradix_tree_replace_slot(\n\t\t\t\t&log->big_stripe_tree, pslot,\n\t\t\t\t(void *)((refcount + 1) << R5C_RADIX_COUNT_SHIFT));\n\t\t} else {\n\t\t\t \n\t\t\tret = radix_tree_insert(\n\t\t\t\t&log->big_stripe_tree, tree_index,\n\t\t\t\t(void *)(1 << R5C_RADIX_COUNT_SHIFT));\n\t\t\tif (ret) {\n\t\t\t\tspin_unlock(&log->tree_lock);\n\t\t\t\tr5c_make_stripe_write_out(sh);\n\t\t\t\treturn -EAGAIN;\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&log->tree_lock);\n\n\t\t \n\t\tset_bit(STRIPE_R5C_PARTIAL_STRIPE, &sh->state);\n\t\tatomic_inc(&conf->r5c_cached_partial_stripes);\n\t}\n\n\tfor (i = disks; i--; ) {\n\t\tdev = &sh->dev[i];\n\t\tif (dev->towrite) {\n\t\t\tset_bit(R5_Wantwrite, &dev->flags);\n\t\t\tset_bit(R5_Wantdrain, &dev->flags);\n\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\tto_cache++;\n\t\t}\n\t}\n\n\tif (to_cache) {\n\t\tset_bit(STRIPE_OP_BIODRAIN, &s->ops_request);\n\t\t \n\t\tset_bit(STRIPE_LOG_TRAPPED, &sh->state);\n\t}\n\n\treturn 0;\n}\n\n \nvoid r5c_release_extra_page(struct stripe_head *sh)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tint i;\n\tbool using_disk_info_extra_page;\n\n\tusing_disk_info_extra_page =\n\t\tsh->dev[0].orig_page == conf->disks[0].extra_page;\n\n\tfor (i = sh->disks; i--; )\n\t\tif (sh->dev[i].page != sh->dev[i].orig_page) {\n\t\t\tstruct page *p = sh->dev[i].orig_page;\n\n\t\t\tsh->dev[i].orig_page = sh->dev[i].page;\n\t\t\tclear_bit(R5_OrigPageUPTDODATE, &sh->dev[i].flags);\n\n\t\t\tif (!using_disk_info_extra_page)\n\t\t\t\tput_page(p);\n\t\t}\n\n\tif (using_disk_info_extra_page) {\n\t\tclear_bit(R5C_EXTRA_PAGE_IN_USE, &conf->cache_state);\n\t\tmd_wakeup_thread(conf->mddev->thread);\n\t}\n}\n\nvoid r5c_use_extra_page(struct stripe_head *sh)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tint i;\n\tstruct r5dev *dev;\n\n\tfor (i = sh->disks; i--; ) {\n\t\tdev = &sh->dev[i];\n\t\tif (dev->orig_page != dev->page)\n\t\t\tput_page(dev->orig_page);\n\t\tdev->orig_page = conf->disks[i].extra_page;\n\t}\n}\n\n \nvoid r5c_finish_stripe_write_out(struct r5conf *conf,\n\t\t\t\t struct stripe_head *sh,\n\t\t\t\t struct stripe_head_state *s)\n{\n\tstruct r5l_log *log = conf->log;\n\tint i;\n\tint do_wakeup = 0;\n\tsector_t tree_index;\n\tvoid __rcu **pslot;\n\tuintptr_t refcount;\n\n\tif (!log || !test_bit(R5_InJournal, &sh->dev[sh->pd_idx].flags))\n\t\treturn;\n\n\tWARN_ON(test_bit(STRIPE_R5C_CACHING, &sh->state));\n\tclear_bit(R5_InJournal, &sh->dev[sh->pd_idx].flags);\n\n\tif (log->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_THROUGH)\n\t\treturn;\n\n\tfor (i = sh->disks; i--; ) {\n\t\tclear_bit(R5_InJournal, &sh->dev[i].flags);\n\t\tif (test_and_clear_bit(R5_Overlap, &sh->dev[i].flags))\n\t\t\tdo_wakeup = 1;\n\t}\n\n\t \n\ts->injournal = 0;\n\n\tif (test_and_clear_bit(STRIPE_FULL_WRITE, &sh->state))\n\t\tif (atomic_dec_and_test(&conf->pending_full_writes))\n\t\t\tmd_wakeup_thread(conf->mddev->thread);\n\n\tif (do_wakeup)\n\t\twake_up(&conf->wait_for_overlap);\n\n\tspin_lock_irq(&log->stripe_in_journal_lock);\n\tlist_del_init(&sh->r5c);\n\tspin_unlock_irq(&log->stripe_in_journal_lock);\n\tsh->log_start = MaxSector;\n\n\tatomic_dec(&log->stripe_in_journal_count);\n\tr5c_update_log_state(log);\n\n\t \n\tif (test_bit(STRIPE_R5C_PARTIAL_STRIPE, &sh->state) ||\n\t    test_bit(STRIPE_R5C_FULL_STRIPE, &sh->state)) {\n\t\ttree_index = r5c_tree_index(conf, sh->sector);\n\t\tspin_lock(&log->tree_lock);\n\t\tpslot = radix_tree_lookup_slot(&log->big_stripe_tree,\n\t\t\t\t\t       tree_index);\n\t\tBUG_ON(pslot == NULL);\n\t\trefcount = (uintptr_t)radix_tree_deref_slot_protected(\n\t\t\tpslot, &log->tree_lock) >>\n\t\t\tR5C_RADIX_COUNT_SHIFT;\n\t\tif (refcount == 1)\n\t\t\tradix_tree_delete(&log->big_stripe_tree, tree_index);\n\t\telse\n\t\t\tradix_tree_replace_slot(\n\t\t\t\t&log->big_stripe_tree, pslot,\n\t\t\t\t(void *)((refcount - 1) << R5C_RADIX_COUNT_SHIFT));\n\t\tspin_unlock(&log->tree_lock);\n\t}\n\n\tif (test_and_clear_bit(STRIPE_R5C_PARTIAL_STRIPE, &sh->state)) {\n\t\tBUG_ON(atomic_read(&conf->r5c_cached_partial_stripes) == 0);\n\t\tatomic_dec(&conf->r5c_flushing_partial_stripes);\n\t\tatomic_dec(&conf->r5c_cached_partial_stripes);\n\t}\n\n\tif (test_and_clear_bit(STRIPE_R5C_FULL_STRIPE, &sh->state)) {\n\t\tBUG_ON(atomic_read(&conf->r5c_cached_full_stripes) == 0);\n\t\tatomic_dec(&conf->r5c_flushing_full_stripes);\n\t\tatomic_dec(&conf->r5c_cached_full_stripes);\n\t}\n\n\tr5l_append_flush_payload(log, sh->sector);\n\t \n\tif (test_bit(STRIPE_SYNC_REQUESTED, &sh->state))\n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n}\n\nint r5c_cache_data(struct r5l_log *log, struct stripe_head *sh)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tint pages = 0;\n\tint reserve;\n\tint i;\n\tint ret = 0;\n\n\tBUG_ON(!log);\n\n\tfor (i = 0; i < sh->disks; i++) {\n\t\tvoid *addr;\n\n\t\tif (!test_bit(R5_Wantwrite, &sh->dev[i].flags))\n\t\t\tcontinue;\n\t\taddr = kmap_atomic(sh->dev[i].page);\n\t\tsh->dev[i].log_checksum = crc32c_le(log->uuid_checksum,\n\t\t\t\t\t\t    addr, PAGE_SIZE);\n\t\tkunmap_atomic(addr);\n\t\tpages++;\n\t}\n\tWARN_ON(pages == 0);\n\n\t \n\tclear_bit(STRIPE_DELAYED, &sh->state);\n\tatomic_inc(&sh->count);\n\n\tmutex_lock(&log->io_mutex);\n\t \n\treserve = (1 + pages) << (PAGE_SHIFT - 9);\n\n\tif (test_bit(R5C_LOG_CRITICAL, &conf->cache_state) &&\n\t    sh->log_start == MaxSector)\n\t\tr5l_add_no_space_stripe(log, sh);\n\telse if (!r5l_has_free_space(log, reserve)) {\n\t\tif (sh->log_start == log->last_checkpoint)\n\t\t\tBUG();\n\t\telse\n\t\t\tr5l_add_no_space_stripe(log, sh);\n\t} else {\n\t\tret = r5l_log_stripe(log, sh, pages, 0);\n\t\tif (ret) {\n\t\t\tspin_lock_irq(&log->io_list_lock);\n\t\t\tlist_add_tail(&sh->log_list, &log->no_mem_stripes);\n\t\t\tspin_unlock_irq(&log->io_list_lock);\n\t\t}\n\t}\n\n\tmutex_unlock(&log->io_mutex);\n\treturn 0;\n}\n\n \nbool r5c_big_stripe_cached(struct r5conf *conf, sector_t sect)\n{\n\tstruct r5l_log *log = conf->log;\n\tsector_t tree_index;\n\tvoid *slot;\n\n\tif (!log)\n\t\treturn false;\n\n\tWARN_ON_ONCE(!rcu_read_lock_held());\n\ttree_index = r5c_tree_index(conf, sect);\n\tslot = radix_tree_lookup(&log->big_stripe_tree, tree_index);\n\treturn slot != NULL;\n}\n\nstatic int r5l_load_log(struct r5l_log *log)\n{\n\tstruct md_rdev *rdev = log->rdev;\n\tstruct page *page;\n\tstruct r5l_meta_block *mb;\n\tsector_t cp = log->rdev->journal_tail;\n\tu32 stored_crc, expected_crc;\n\tbool create_super = false;\n\tint ret = 0;\n\n\t \n\tif (cp >= rdev->sectors || round_down(cp, BLOCK_SECTORS) != cp)\n\t\tcp = 0;\n\tpage = alloc_page(GFP_KERNEL);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tif (!sync_page_io(rdev, cp, PAGE_SIZE, page, REQ_OP_READ, false)) {\n\t\tret = -EIO;\n\t\tgoto ioerr;\n\t}\n\tmb = page_address(page);\n\n\tif (le32_to_cpu(mb->magic) != R5LOG_MAGIC ||\n\t    mb->version != R5LOG_VERSION) {\n\t\tcreate_super = true;\n\t\tgoto create;\n\t}\n\tstored_crc = le32_to_cpu(mb->checksum);\n\tmb->checksum = 0;\n\texpected_crc = crc32c_le(log->uuid_checksum, mb, PAGE_SIZE);\n\tif (stored_crc != expected_crc) {\n\t\tcreate_super = true;\n\t\tgoto create;\n\t}\n\tif (le64_to_cpu(mb->position) != cp) {\n\t\tcreate_super = true;\n\t\tgoto create;\n\t}\ncreate:\n\tif (create_super) {\n\t\tlog->last_cp_seq = get_random_u32();\n\t\tcp = 0;\n\t\tr5l_log_write_empty_meta_block(log, cp, log->last_cp_seq);\n\t\t \n\t\tr5l_write_super(log, cp);\n\t} else\n\t\tlog->last_cp_seq = le64_to_cpu(mb->seq);\n\n\tlog->device_size = round_down(rdev->sectors, BLOCK_SECTORS);\n\tlog->max_free_space = log->device_size >> RECLAIM_MAX_FREE_SPACE_SHIFT;\n\tif (log->max_free_space > RECLAIM_MAX_FREE_SPACE)\n\t\tlog->max_free_space = RECLAIM_MAX_FREE_SPACE;\n\tlog->last_checkpoint = cp;\n\n\t__free_page(page);\n\n\tif (create_super) {\n\t\tlog->log_start = r5l_ring_add(log, cp, BLOCK_SECTORS);\n\t\tlog->seq = log->last_cp_seq + 1;\n\t\tlog->next_checkpoint = cp;\n\t} else\n\t\tret = r5l_recovery_log(log);\n\n\tr5c_update_log_state(log);\n\treturn ret;\nioerr:\n\t__free_page(page);\n\treturn ret;\n}\n\nint r5l_start(struct r5l_log *log)\n{\n\tint ret;\n\n\tif (!log)\n\t\treturn 0;\n\n\tret = r5l_load_log(log);\n\tif (ret) {\n\t\tstruct mddev *mddev = log->rdev->mddev;\n\t\tstruct r5conf *conf = mddev->private;\n\n\t\tr5l_exit_log(conf);\n\t}\n\treturn ret;\n}\n\nvoid r5c_update_on_rdev_error(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tstruct r5conf *conf = mddev->private;\n\tstruct r5l_log *log = conf->log;\n\n\tif (!log)\n\t\treturn;\n\n\tif ((raid5_calc_degraded(conf) > 0 ||\n\t     test_bit(Journal, &rdev->flags)) &&\n\t    conf->log->r5c_journal_mode == R5C_JOURNAL_MODE_WRITE_BACK)\n\t\tschedule_work(&log->disable_writeback_work);\n}\n\nint r5l_init_log(struct r5conf *conf, struct md_rdev *rdev)\n{\n\tstruct r5l_log *log;\n\tstruct md_thread *thread;\n\tint ret;\n\n\tpr_debug(\"md/raid:%s: using device %pg as journal\\n\",\n\t\t mdname(conf->mddev), rdev->bdev);\n\n\tif (PAGE_SIZE != 4096)\n\t\treturn -EINVAL;\n\n\t \n\tif (sizeof(struct r5l_meta_block) +\n\t    ((sizeof(struct r5l_payload_data_parity) + sizeof(__le32)) *\n\t     conf->raid_disks) > PAGE_SIZE) {\n\t\tpr_err(\"md/raid:%s: write journal/cache doesn't work for array with %d disks\\n\",\n\t\t       mdname(conf->mddev), conf->raid_disks);\n\t\treturn -EINVAL;\n\t}\n\n\tlog = kzalloc(sizeof(*log), GFP_KERNEL);\n\tif (!log)\n\t\treturn -ENOMEM;\n\tlog->rdev = rdev;\n\tlog->need_cache_flush = bdev_write_cache(rdev->bdev);\n\tlog->uuid_checksum = crc32c_le(~0, rdev->mddev->uuid,\n\t\t\t\t       sizeof(rdev->mddev->uuid));\n\n\tmutex_init(&log->io_mutex);\n\n\tspin_lock_init(&log->io_list_lock);\n\tINIT_LIST_HEAD(&log->running_ios);\n\tINIT_LIST_HEAD(&log->io_end_ios);\n\tINIT_LIST_HEAD(&log->flushing_ios);\n\tINIT_LIST_HEAD(&log->finished_ios);\n\n\tlog->io_kc = KMEM_CACHE(r5l_io_unit, 0);\n\tif (!log->io_kc)\n\t\tgoto io_kc;\n\n\tret = mempool_init_slab_pool(&log->io_pool, R5L_POOL_SIZE, log->io_kc);\n\tif (ret)\n\t\tgoto io_pool;\n\n\tret = bioset_init(&log->bs, R5L_POOL_SIZE, 0, BIOSET_NEED_BVECS);\n\tif (ret)\n\t\tgoto io_bs;\n\n\tret = mempool_init_page_pool(&log->meta_pool, R5L_POOL_SIZE, 0);\n\tif (ret)\n\t\tgoto out_mempool;\n\n\tspin_lock_init(&log->tree_lock);\n\tINIT_RADIX_TREE(&log->big_stripe_tree, GFP_NOWAIT | __GFP_NOWARN);\n\n\tthread = md_register_thread(r5l_reclaim_thread, log->rdev->mddev,\n\t\t\t\t    \"reclaim\");\n\tif (!thread)\n\t\tgoto reclaim_thread;\n\n\tthread->timeout = R5C_RECLAIM_WAKEUP_INTERVAL;\n\trcu_assign_pointer(log->reclaim_thread, thread);\n\n\tinit_waitqueue_head(&log->iounit_wait);\n\n\tINIT_LIST_HEAD(&log->no_mem_stripes);\n\n\tINIT_LIST_HEAD(&log->no_space_stripes);\n\tspin_lock_init(&log->no_space_stripes_lock);\n\n\tINIT_WORK(&log->deferred_io_work, r5l_submit_io_async);\n\tINIT_WORK(&log->disable_writeback_work, r5c_disable_writeback_async);\n\n\tlog->r5c_journal_mode = R5C_JOURNAL_MODE_WRITE_THROUGH;\n\tINIT_LIST_HEAD(&log->stripe_in_journal_list);\n\tspin_lock_init(&log->stripe_in_journal_lock);\n\tatomic_set(&log->stripe_in_journal_count, 0);\n\n\tconf->log = log;\n\n\tset_bit(MD_HAS_JOURNAL, &conf->mddev->flags);\n\treturn 0;\n\nreclaim_thread:\n\tmempool_exit(&log->meta_pool);\nout_mempool:\n\tbioset_exit(&log->bs);\nio_bs:\n\tmempool_exit(&log->io_pool);\nio_pool:\n\tkmem_cache_destroy(log->io_kc);\nio_kc:\n\tkfree(log);\n\treturn -EINVAL;\n}\n\nvoid r5l_exit_log(struct r5conf *conf)\n{\n\tstruct r5l_log *log = conf->log;\n\n\tmd_unregister_thread(conf->mddev, &log->reclaim_thread);\n\n\t \n\tconf->log = NULL;\n\twake_up(&conf->mddev->sb_wait);\n\tflush_work(&log->disable_writeback_work);\n\n\tmempool_exit(&log->meta_pool);\n\tbioset_exit(&log->bs);\n\tmempool_exit(&log->io_pool);\n\tkmem_cache_destroy(log->io_kc);\n\tkfree(log);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}