{
  "module_name": "dm-mpath.c",
  "hash_id": "d855fbf68b79257eb18076c3dbeacae53d1dad2509d2aac1234ba64fbdac3874",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-mpath.c",
  "human_readable_source": "\n \n\n#include <linux/device-mapper.h>\n\n#include \"dm-rq.h\"\n#include \"dm-bio-record.h\"\n#include \"dm-path-selector.h\"\n#include \"dm-uevent.h\"\n\n#include <linux/blkdev.h>\n#include <linux/ctype.h>\n#include <linux/init.h>\n#include <linux/mempool.h>\n#include <linux/module.h>\n#include <linux/pagemap.h>\n#include <linux/slab.h>\n#include <linux/time.h>\n#include <linux/timer.h>\n#include <linux/workqueue.h>\n#include <linux/delay.h>\n#include <scsi/scsi_dh.h>\n#include <linux/atomic.h>\n#include <linux/blk-mq.h>\n\nstatic struct workqueue_struct *dm_mpath_wq;\n\n#define DM_MSG_PREFIX \"multipath\"\n#define DM_PG_INIT_DELAY_MSECS 2000\n#define DM_PG_INIT_DELAY_DEFAULT ((unsigned int) -1)\n#define QUEUE_IF_NO_PATH_TIMEOUT_DEFAULT 0\n\nstatic unsigned long queue_if_no_path_timeout_secs = QUEUE_IF_NO_PATH_TIMEOUT_DEFAULT;\n\n \nstruct pgpath {\n\tstruct list_head list;\n\n\tstruct priority_group *pg;\t \n\tunsigned int fail_count;\t\t \n\n\tstruct dm_path path;\n\tstruct delayed_work activate_path;\n\n\tbool is_active:1;\t\t \n};\n\n#define path_to_pgpath(__pgp) container_of((__pgp), struct pgpath, path)\n\n \nstruct priority_group {\n\tstruct list_head list;\n\n\tstruct multipath *m;\t\t \n\tstruct path_selector ps;\n\n\tunsigned int pg_num;\t\t \n\tunsigned int nr_pgpaths;\t\t \n\tstruct list_head pgpaths;\n\n\tbool bypassed:1;\t\t \n};\n\n \nstruct multipath {\n\tunsigned long flags;\t\t \n\n\tspinlock_t lock;\n\tenum dm_queue_mode queue_mode;\n\n\tstruct pgpath *current_pgpath;\n\tstruct priority_group *current_pg;\n\tstruct priority_group *next_pg;\t \n\n\tatomic_t nr_valid_paths;\t \n\tunsigned int nr_priority_groups;\n\tstruct list_head priority_groups;\n\n\tconst char *hw_handler_name;\n\tchar *hw_handler_params;\n\twait_queue_head_t pg_init_wait;\t \n\tunsigned int pg_init_retries;\t \n\tunsigned int pg_init_delay_msecs;\t \n\tatomic_t pg_init_in_progress;\t \n\tatomic_t pg_init_count;\t\t \n\n\tstruct mutex work_mutex;\n\tstruct work_struct trigger_event;\n\tstruct dm_target *ti;\n\n\tstruct work_struct process_queued_bios;\n\tstruct bio_list queued_bios;\n\n\tstruct timer_list nopath_timer;\t \n};\n\n \nstruct dm_mpath_io {\n\tstruct pgpath *pgpath;\n\tsize_t nr_bytes;\n\tu64 start_time_ns;\n};\n\ntypedef int (*action_fn) (struct pgpath *pgpath);\n\nstatic struct workqueue_struct *kmultipathd, *kmpath_handlerd;\nstatic void trigger_event(struct work_struct *work);\nstatic void activate_or_offline_path(struct pgpath *pgpath);\nstatic void activate_path_work(struct work_struct *work);\nstatic void process_queued_bios(struct work_struct *work);\nstatic void queue_if_no_path_timeout_work(struct timer_list *t);\n\n \n#define MPATHF_QUEUE_IO 0\t\t\t \n#define MPATHF_QUEUE_IF_NO_PATH 1\t\t \n#define MPATHF_SAVED_QUEUE_IF_NO_PATH 2\t\t \n#define MPATHF_RETAIN_ATTACHED_HW_HANDLER 3\t \n#define MPATHF_PG_INIT_DISABLED 4\t\t \n#define MPATHF_PG_INIT_REQUIRED 5\t\t \n#define MPATHF_PG_INIT_DELAY_RETRY 6\t\t \n\nstatic bool mpath_double_check_test_bit(int MPATHF_bit, struct multipath *m)\n{\n\tbool r = test_bit(MPATHF_bit, &m->flags);\n\n\tif (r) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&m->lock, flags);\n\t\tr = test_bit(MPATHF_bit, &m->flags);\n\t\tspin_unlock_irqrestore(&m->lock, flags);\n\t}\n\n\treturn r;\n}\n\n \nstatic struct pgpath *alloc_pgpath(void)\n{\n\tstruct pgpath *pgpath = kzalloc(sizeof(*pgpath), GFP_KERNEL);\n\n\tif (!pgpath)\n\t\treturn NULL;\n\n\tpgpath->is_active = true;\n\n\treturn pgpath;\n}\n\nstatic void free_pgpath(struct pgpath *pgpath)\n{\n\tkfree(pgpath);\n}\n\nstatic struct priority_group *alloc_priority_group(void)\n{\n\tstruct priority_group *pg;\n\n\tpg = kzalloc(sizeof(*pg), GFP_KERNEL);\n\n\tif (pg)\n\t\tINIT_LIST_HEAD(&pg->pgpaths);\n\n\treturn pg;\n}\n\nstatic void free_pgpaths(struct list_head *pgpaths, struct dm_target *ti)\n{\n\tstruct pgpath *pgpath, *tmp;\n\n\tlist_for_each_entry_safe(pgpath, tmp, pgpaths, list) {\n\t\tlist_del(&pgpath->list);\n\t\tdm_put_device(ti, pgpath->path.dev);\n\t\tfree_pgpath(pgpath);\n\t}\n}\n\nstatic void free_priority_group(struct priority_group *pg,\n\t\t\t\tstruct dm_target *ti)\n{\n\tstruct path_selector *ps = &pg->ps;\n\n\tif (ps->type) {\n\t\tps->type->destroy(ps);\n\t\tdm_put_path_selector(ps->type);\n\t}\n\n\tfree_pgpaths(&pg->pgpaths, ti);\n\tkfree(pg);\n}\n\nstatic struct multipath *alloc_multipath(struct dm_target *ti)\n{\n\tstruct multipath *m;\n\n\tm = kzalloc(sizeof(*m), GFP_KERNEL);\n\tif (m) {\n\t\tINIT_LIST_HEAD(&m->priority_groups);\n\t\tspin_lock_init(&m->lock);\n\t\tatomic_set(&m->nr_valid_paths, 0);\n\t\tINIT_WORK(&m->trigger_event, trigger_event);\n\t\tmutex_init(&m->work_mutex);\n\n\t\tm->queue_mode = DM_TYPE_NONE;\n\n\t\tm->ti = ti;\n\t\tti->private = m;\n\n\t\ttimer_setup(&m->nopath_timer, queue_if_no_path_timeout_work, 0);\n\t}\n\n\treturn m;\n}\n\nstatic int alloc_multipath_stage2(struct dm_target *ti, struct multipath *m)\n{\n\tif (m->queue_mode == DM_TYPE_NONE) {\n\t\tm->queue_mode = DM_TYPE_REQUEST_BASED;\n\t} else if (m->queue_mode == DM_TYPE_BIO_BASED) {\n\t\tINIT_WORK(&m->process_queued_bios, process_queued_bios);\n\t\t \n\t\tset_bit(MPATHF_RETAIN_ATTACHED_HW_HANDLER, &m->flags);\n\t}\n\n\tdm_table_set_type(ti->table, m->queue_mode);\n\n\t \n\tset_bit(MPATHF_QUEUE_IO, &m->flags);\n\tatomic_set(&m->pg_init_in_progress, 0);\n\tatomic_set(&m->pg_init_count, 0);\n\tm->pg_init_delay_msecs = DM_PG_INIT_DELAY_DEFAULT;\n\tinit_waitqueue_head(&m->pg_init_wait);\n\n\treturn 0;\n}\n\nstatic void free_multipath(struct multipath *m)\n{\n\tstruct priority_group *pg, *tmp;\n\n\tlist_for_each_entry_safe(pg, tmp, &m->priority_groups, list) {\n\t\tlist_del(&pg->list);\n\t\tfree_priority_group(pg, m->ti);\n\t}\n\n\tkfree(m->hw_handler_name);\n\tkfree(m->hw_handler_params);\n\tmutex_destroy(&m->work_mutex);\n\tkfree(m);\n}\n\nstatic struct dm_mpath_io *get_mpio(union map_info *info)\n{\n\treturn info->ptr;\n}\n\nstatic size_t multipath_per_bio_data_size(void)\n{\n\treturn sizeof(struct dm_mpath_io) + sizeof(struct dm_bio_details);\n}\n\nstatic struct dm_mpath_io *get_mpio_from_bio(struct bio *bio)\n{\n\treturn dm_per_bio_data(bio, multipath_per_bio_data_size());\n}\n\nstatic struct dm_bio_details *get_bio_details_from_mpio(struct dm_mpath_io *mpio)\n{\n\t \n\tvoid *bio_details = mpio + 1;\n\treturn bio_details;\n}\n\nstatic void multipath_init_per_bio_data(struct bio *bio, struct dm_mpath_io **mpio_p)\n{\n\tstruct dm_mpath_io *mpio = get_mpio_from_bio(bio);\n\tstruct dm_bio_details *bio_details = get_bio_details_from_mpio(mpio);\n\n\tmpio->nr_bytes = bio->bi_iter.bi_size;\n\tmpio->pgpath = NULL;\n\tmpio->start_time_ns = 0;\n\t*mpio_p = mpio;\n\n\tdm_bio_record(bio_details, bio);\n}\n\n \nstatic int __pg_init_all_paths(struct multipath *m)\n{\n\tstruct pgpath *pgpath;\n\tunsigned long pg_init_delay = 0;\n\n\tlockdep_assert_held(&m->lock);\n\n\tif (atomic_read(&m->pg_init_in_progress) || test_bit(MPATHF_PG_INIT_DISABLED, &m->flags))\n\t\treturn 0;\n\n\tatomic_inc(&m->pg_init_count);\n\tclear_bit(MPATHF_PG_INIT_REQUIRED, &m->flags);\n\n\t \n\tif (!m->current_pg)\n\t\treturn 0;\n\n\tif (test_bit(MPATHF_PG_INIT_DELAY_RETRY, &m->flags))\n\t\tpg_init_delay = msecs_to_jiffies(m->pg_init_delay_msecs != DM_PG_INIT_DELAY_DEFAULT ?\n\t\t\t\t\t\t m->pg_init_delay_msecs : DM_PG_INIT_DELAY_MSECS);\n\tlist_for_each_entry(pgpath, &m->current_pg->pgpaths, list) {\n\t\t \n\t\tif (!pgpath->is_active)\n\t\t\tcontinue;\n\t\tif (queue_delayed_work(kmpath_handlerd, &pgpath->activate_path,\n\t\t\t\t       pg_init_delay))\n\t\t\tatomic_inc(&m->pg_init_in_progress);\n\t}\n\treturn atomic_read(&m->pg_init_in_progress);\n}\n\nstatic int pg_init_all_paths(struct multipath *m)\n{\n\tint ret;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&m->lock, flags);\n\tret = __pg_init_all_paths(m);\n\tspin_unlock_irqrestore(&m->lock, flags);\n\n\treturn ret;\n}\n\nstatic void __switch_pg(struct multipath *m, struct priority_group *pg)\n{\n\tlockdep_assert_held(&m->lock);\n\n\tm->current_pg = pg;\n\n\t \n\tif (m->hw_handler_name) {\n\t\tset_bit(MPATHF_PG_INIT_REQUIRED, &m->flags);\n\t\tset_bit(MPATHF_QUEUE_IO, &m->flags);\n\t} else {\n\t\tclear_bit(MPATHF_PG_INIT_REQUIRED, &m->flags);\n\t\tclear_bit(MPATHF_QUEUE_IO, &m->flags);\n\t}\n\n\tatomic_set(&m->pg_init_count, 0);\n}\n\nstatic struct pgpath *choose_path_in_pg(struct multipath *m,\n\t\t\t\t\tstruct priority_group *pg,\n\t\t\t\t\tsize_t nr_bytes)\n{\n\tunsigned long flags;\n\tstruct dm_path *path;\n\tstruct pgpath *pgpath;\n\n\tpath = pg->ps.type->select_path(&pg->ps, nr_bytes);\n\tif (!path)\n\t\treturn ERR_PTR(-ENXIO);\n\n\tpgpath = path_to_pgpath(path);\n\n\tif (unlikely(READ_ONCE(m->current_pg) != pg)) {\n\t\t \n\t\tspin_lock_irqsave(&m->lock, flags);\n\t\tm->current_pgpath = pgpath;\n\t\t__switch_pg(m, pg);\n\t\tspin_unlock_irqrestore(&m->lock, flags);\n\t}\n\n\treturn pgpath;\n}\n\nstatic struct pgpath *choose_pgpath(struct multipath *m, size_t nr_bytes)\n{\n\tunsigned long flags;\n\tstruct priority_group *pg;\n\tstruct pgpath *pgpath;\n\tunsigned int bypassed = 1;\n\n\tif (!atomic_read(&m->nr_valid_paths)) {\n\t\tspin_lock_irqsave(&m->lock, flags);\n\t\tclear_bit(MPATHF_QUEUE_IO, &m->flags);\n\t\tspin_unlock_irqrestore(&m->lock, flags);\n\t\tgoto failed;\n\t}\n\n\t \n\tif (READ_ONCE(m->next_pg)) {\n\t\tspin_lock_irqsave(&m->lock, flags);\n\t\tpg = m->next_pg;\n\t\tif (!pg) {\n\t\t\tspin_unlock_irqrestore(&m->lock, flags);\n\t\t\tgoto check_current_pg;\n\t\t}\n\t\tm->next_pg = NULL;\n\t\tspin_unlock_irqrestore(&m->lock, flags);\n\t\tpgpath = choose_path_in_pg(m, pg, nr_bytes);\n\t\tif (!IS_ERR_OR_NULL(pgpath))\n\t\t\treturn pgpath;\n\t}\n\n\t \ncheck_current_pg:\n\tpg = READ_ONCE(m->current_pg);\n\tif (pg) {\n\t\tpgpath = choose_path_in_pg(m, pg, nr_bytes);\n\t\tif (!IS_ERR_OR_NULL(pgpath))\n\t\t\treturn pgpath;\n\t}\n\n\t \n\tdo {\n\t\tlist_for_each_entry(pg, &m->priority_groups, list) {\n\t\t\tif (pg->bypassed == !!bypassed)\n\t\t\t\tcontinue;\n\t\t\tpgpath = choose_path_in_pg(m, pg, nr_bytes);\n\t\t\tif (!IS_ERR_OR_NULL(pgpath)) {\n\t\t\t\tif (!bypassed) {\n\t\t\t\t\tspin_lock_irqsave(&m->lock, flags);\n\t\t\t\t\tset_bit(MPATHF_PG_INIT_DELAY_RETRY, &m->flags);\n\t\t\t\t\tspin_unlock_irqrestore(&m->lock, flags);\n\t\t\t\t}\n\t\t\t\treturn pgpath;\n\t\t\t}\n\t\t}\n\t} while (bypassed--);\n\nfailed:\n\tspin_lock_irqsave(&m->lock, flags);\n\tm->current_pgpath = NULL;\n\tm->current_pg = NULL;\n\tspin_unlock_irqrestore(&m->lock, flags);\n\n\treturn NULL;\n}\n\n \n#define dm_report_EIO(m)\t\t\t\t\t\t\\\n\tDMDEBUG_LIMIT(\"%s: returning EIO; QIFNP = %d; SQIFNP = %d; DNFS = %d\", \\\n\t\t      dm_table_device_name((m)->ti->table),\t\t\\\n\t\t      test_bit(MPATHF_QUEUE_IF_NO_PATH, &(m)->flags),\t\\\n\t\t      test_bit(MPATHF_SAVED_QUEUE_IF_NO_PATH, &(m)->flags), \\\n\t\t      dm_noflush_suspending((m)->ti))\n\n \nstatic bool __must_push_back(struct multipath *m)\n{\n\treturn dm_noflush_suspending(m->ti);\n}\n\nstatic bool must_push_back_rq(struct multipath *m)\n{\n\tunsigned long flags;\n\tbool ret;\n\n\tspin_lock_irqsave(&m->lock, flags);\n\tret = (test_bit(MPATHF_QUEUE_IF_NO_PATH, &m->flags) || __must_push_back(m));\n\tspin_unlock_irqrestore(&m->lock, flags);\n\n\treturn ret;\n}\n\n \nstatic int multipath_clone_and_map(struct dm_target *ti, struct request *rq,\n\t\t\t\t   union map_info *map_context,\n\t\t\t\t   struct request **__clone)\n{\n\tstruct multipath *m = ti->private;\n\tsize_t nr_bytes = blk_rq_bytes(rq);\n\tstruct pgpath *pgpath;\n\tstruct block_device *bdev;\n\tstruct dm_mpath_io *mpio = get_mpio(map_context);\n\tstruct request_queue *q;\n\tstruct request *clone;\n\n\t \n\tpgpath = READ_ONCE(m->current_pgpath);\n\tif (!pgpath || !mpath_double_check_test_bit(MPATHF_QUEUE_IO, m))\n\t\tpgpath = choose_pgpath(m, nr_bytes);\n\n\tif (!pgpath) {\n\t\tif (must_push_back_rq(m))\n\t\t\treturn DM_MAPIO_DELAY_REQUEUE;\n\t\tdm_report_EIO(m);\t \n\t\treturn DM_MAPIO_KILL;\n\t} else if (mpath_double_check_test_bit(MPATHF_QUEUE_IO, m) ||\n\t\t   mpath_double_check_test_bit(MPATHF_PG_INIT_REQUIRED, m)) {\n\t\tpg_init_all_paths(m);\n\t\treturn DM_MAPIO_DELAY_REQUEUE;\n\t}\n\n\tmpio->pgpath = pgpath;\n\tmpio->nr_bytes = nr_bytes;\n\n\tbdev = pgpath->path.dev->bdev;\n\tq = bdev_get_queue(bdev);\n\tclone = blk_mq_alloc_request(q, rq->cmd_flags | REQ_NOMERGE,\n\t\t\tBLK_MQ_REQ_NOWAIT);\n\tif (IS_ERR(clone)) {\n\t\t \n\t\tif (blk_queue_dying(q)) {\n\t\t\tatomic_inc(&m->pg_init_in_progress);\n\t\t\tactivate_or_offline_path(pgpath);\n\t\t\treturn DM_MAPIO_DELAY_REQUEUE;\n\t\t}\n\n\t\t \n\t\treturn DM_MAPIO_REQUEUE;\n\t}\n\tclone->bio = clone->biotail = NULL;\n\tclone->cmd_flags |= REQ_FAILFAST_TRANSPORT;\n\t*__clone = clone;\n\n\tif (pgpath->pg->ps.type->start_io)\n\t\tpgpath->pg->ps.type->start_io(&pgpath->pg->ps,\n\t\t\t\t\t      &pgpath->path,\n\t\t\t\t\t      nr_bytes);\n\treturn DM_MAPIO_REMAPPED;\n}\n\nstatic void multipath_release_clone(struct request *clone,\n\t\t\t\t    union map_info *map_context)\n{\n\tif (unlikely(map_context)) {\n\t\t \n\t\tstruct dm_mpath_io *mpio = get_mpio(map_context);\n\t\tstruct pgpath *pgpath = mpio->pgpath;\n\n\t\tif (pgpath && pgpath->pg->ps.type->end_io)\n\t\t\tpgpath->pg->ps.type->end_io(&pgpath->pg->ps,\n\t\t\t\t\t\t    &pgpath->path,\n\t\t\t\t\t\t    mpio->nr_bytes,\n\t\t\t\t\t\t    clone->io_start_time_ns);\n\t}\n\n\tblk_mq_free_request(clone);\n}\n\n \n\nstatic void __multipath_queue_bio(struct multipath *m, struct bio *bio)\n{\n\t \n\tbio_list_add(&m->queued_bios, bio);\n\tif (!test_bit(MPATHF_QUEUE_IO, &m->flags))\n\t\tqueue_work(kmultipathd, &m->process_queued_bios);\n}\n\nstatic void multipath_queue_bio(struct multipath *m, struct bio *bio)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&m->lock, flags);\n\t__multipath_queue_bio(m, bio);\n\tspin_unlock_irqrestore(&m->lock, flags);\n}\n\nstatic struct pgpath *__map_bio(struct multipath *m, struct bio *bio)\n{\n\tstruct pgpath *pgpath;\n\tunsigned long flags;\n\n\t \n\tpgpath = READ_ONCE(m->current_pgpath);\n\tif (!pgpath || !mpath_double_check_test_bit(MPATHF_QUEUE_IO, m))\n\t\tpgpath = choose_pgpath(m, bio->bi_iter.bi_size);\n\n\tif (!pgpath) {\n\t\tspin_lock_irqsave(&m->lock, flags);\n\t\tif (test_bit(MPATHF_QUEUE_IF_NO_PATH, &m->flags)) {\n\t\t\t__multipath_queue_bio(m, bio);\n\t\t\tpgpath = ERR_PTR(-EAGAIN);\n\t\t}\n\t\tspin_unlock_irqrestore(&m->lock, flags);\n\n\t} else if (mpath_double_check_test_bit(MPATHF_QUEUE_IO, m) ||\n\t\t   mpath_double_check_test_bit(MPATHF_PG_INIT_REQUIRED, m)) {\n\t\tmultipath_queue_bio(m, bio);\n\t\tpg_init_all_paths(m);\n\t\treturn ERR_PTR(-EAGAIN);\n\t}\n\n\treturn pgpath;\n}\n\nstatic int __multipath_map_bio(struct multipath *m, struct bio *bio,\n\t\t\t       struct dm_mpath_io *mpio)\n{\n\tstruct pgpath *pgpath = __map_bio(m, bio);\n\n\tif (IS_ERR(pgpath))\n\t\treturn DM_MAPIO_SUBMITTED;\n\n\tif (!pgpath) {\n\t\tif (__must_push_back(m))\n\t\t\treturn DM_MAPIO_REQUEUE;\n\t\tdm_report_EIO(m);\n\t\treturn DM_MAPIO_KILL;\n\t}\n\n\tmpio->pgpath = pgpath;\n\n\tif (dm_ps_use_hr_timer(pgpath->pg->ps.type))\n\t\tmpio->start_time_ns = ktime_get_ns();\n\n\tbio->bi_status = 0;\n\tbio_set_dev(bio, pgpath->path.dev->bdev);\n\tbio->bi_opf |= REQ_FAILFAST_TRANSPORT;\n\n\tif (pgpath->pg->ps.type->start_io)\n\t\tpgpath->pg->ps.type->start_io(&pgpath->pg->ps,\n\t\t\t\t\t      &pgpath->path,\n\t\t\t\t\t      mpio->nr_bytes);\n\treturn DM_MAPIO_REMAPPED;\n}\n\nstatic int multipath_map_bio(struct dm_target *ti, struct bio *bio)\n{\n\tstruct multipath *m = ti->private;\n\tstruct dm_mpath_io *mpio = NULL;\n\n\tmultipath_init_per_bio_data(bio, &mpio);\n\treturn __multipath_map_bio(m, bio, mpio);\n}\n\nstatic void process_queued_io_list(struct multipath *m)\n{\n\tif (m->queue_mode == DM_TYPE_REQUEST_BASED)\n\t\tdm_mq_kick_requeue_list(dm_table_get_md(m->ti->table));\n\telse if (m->queue_mode == DM_TYPE_BIO_BASED)\n\t\tqueue_work(kmultipathd, &m->process_queued_bios);\n}\n\nstatic void process_queued_bios(struct work_struct *work)\n{\n\tint r;\n\tunsigned long flags;\n\tstruct bio *bio;\n\tstruct bio_list bios;\n\tstruct blk_plug plug;\n\tstruct multipath *m =\n\t\tcontainer_of(work, struct multipath, process_queued_bios);\n\n\tbio_list_init(&bios);\n\n\tspin_lock_irqsave(&m->lock, flags);\n\n\tif (bio_list_empty(&m->queued_bios)) {\n\t\tspin_unlock_irqrestore(&m->lock, flags);\n\t\treturn;\n\t}\n\n\tbio_list_merge(&bios, &m->queued_bios);\n\tbio_list_init(&m->queued_bios);\n\n\tspin_unlock_irqrestore(&m->lock, flags);\n\n\tblk_start_plug(&plug);\n\twhile ((bio = bio_list_pop(&bios))) {\n\t\tstruct dm_mpath_io *mpio = get_mpio_from_bio(bio);\n\n\t\tdm_bio_restore(get_bio_details_from_mpio(mpio), bio);\n\t\tr = __multipath_map_bio(m, bio, mpio);\n\t\tswitch (r) {\n\t\tcase DM_MAPIO_KILL:\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\t\tbio_endio(bio);\n\t\t\tbreak;\n\t\tcase DM_MAPIO_REQUEUE:\n\t\t\tbio->bi_status = BLK_STS_DM_REQUEUE;\n\t\t\tbio_endio(bio);\n\t\t\tbreak;\n\t\tcase DM_MAPIO_REMAPPED:\n\t\t\tsubmit_bio_noacct(bio);\n\t\t\tbreak;\n\t\tcase DM_MAPIO_SUBMITTED:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ONCE(true, \"__multipath_map_bio() returned %d\\n\", r);\n\t\t}\n\t}\n\tblk_finish_plug(&plug);\n}\n\n \nstatic int queue_if_no_path(struct multipath *m, bool f_queue_if_no_path,\n\t\t\t    bool save_old_value, const char *caller)\n{\n\tunsigned long flags;\n\tbool queue_if_no_path_bit, saved_queue_if_no_path_bit;\n\tconst char *dm_dev_name = dm_table_device_name(m->ti->table);\n\n\tDMDEBUG(\"%s: %s caller=%s f_queue_if_no_path=%d save_old_value=%d\",\n\t\tdm_dev_name, __func__, caller, f_queue_if_no_path, save_old_value);\n\n\tspin_lock_irqsave(&m->lock, flags);\n\n\tqueue_if_no_path_bit = test_bit(MPATHF_QUEUE_IF_NO_PATH, &m->flags);\n\tsaved_queue_if_no_path_bit = test_bit(MPATHF_SAVED_QUEUE_IF_NO_PATH, &m->flags);\n\n\tif (save_old_value) {\n\t\tif (unlikely(!queue_if_no_path_bit && saved_queue_if_no_path_bit)) {\n\t\t\tDMERR(\"%s: QIFNP disabled but saved as enabled, saving again loses state, not saving!\",\n\t\t\t      dm_dev_name);\n\t\t} else\n\t\t\tassign_bit(MPATHF_SAVED_QUEUE_IF_NO_PATH, &m->flags, queue_if_no_path_bit);\n\t} else if (!f_queue_if_no_path && saved_queue_if_no_path_bit) {\n\t\t \n\t\tclear_bit(MPATHF_SAVED_QUEUE_IF_NO_PATH, &m->flags);\n\t}\n\tassign_bit(MPATHF_QUEUE_IF_NO_PATH, &m->flags, f_queue_if_no_path);\n\n\tDMDEBUG(\"%s: after %s changes; QIFNP = %d; SQIFNP = %d; DNFS = %d\",\n\t\tdm_dev_name, __func__,\n\t\ttest_bit(MPATHF_QUEUE_IF_NO_PATH, &m->flags),\n\t\ttest_bit(MPATHF_SAVED_QUEUE_IF_NO_PATH, &m->flags),\n\t\tdm_noflush_suspending(m->ti));\n\n\tspin_unlock_irqrestore(&m->lock, flags);\n\n\tif (!f_queue_if_no_path) {\n\t\tdm_table_run_md_queue_async(m->ti->table);\n\t\tprocess_queued_io_list(m);\n\t}\n\n\treturn 0;\n}\n\n \nstatic void queue_if_no_path_timeout_work(struct timer_list *t)\n{\n\tstruct multipath *m = from_timer(m, t, nopath_timer);\n\n\tDMWARN(\"queue_if_no_path timeout on %s, failing queued IO\",\n\t       dm_table_device_name(m->ti->table));\n\tqueue_if_no_path(m, false, false, __func__);\n}\n\n \nstatic void enable_nopath_timeout(struct multipath *m)\n{\n\tunsigned long queue_if_no_path_timeout =\n\t\tREAD_ONCE(queue_if_no_path_timeout_secs) * HZ;\n\n\tlockdep_assert_held(&m->lock);\n\n\tif (queue_if_no_path_timeout > 0 &&\n\t    atomic_read(&m->nr_valid_paths) == 0 &&\n\t    test_bit(MPATHF_QUEUE_IF_NO_PATH, &m->flags)) {\n\t\tmod_timer(&m->nopath_timer,\n\t\t\t  jiffies + queue_if_no_path_timeout);\n\t}\n}\n\nstatic void disable_nopath_timeout(struct multipath *m)\n{\n\tdel_timer_sync(&m->nopath_timer);\n}\n\n \nstatic void trigger_event(struct work_struct *work)\n{\n\tstruct multipath *m =\n\t\tcontainer_of(work, struct multipath, trigger_event);\n\n\tdm_table_event(m->ti->table);\n}\n\n \nstatic int parse_path_selector(struct dm_arg_set *as, struct priority_group *pg,\n\t\t\t       struct dm_target *ti)\n{\n\tint r;\n\tstruct path_selector_type *pst;\n\tunsigned int ps_argc;\n\n\tstatic const struct dm_arg _args[] = {\n\t\t{0, 1024, \"invalid number of path selector args\"},\n\t};\n\n\tpst = dm_get_path_selector(dm_shift_arg(as));\n\tif (!pst) {\n\t\tti->error = \"unknown path selector type\";\n\t\treturn -EINVAL;\n\t}\n\n\tr = dm_read_arg_group(_args, as, &ps_argc, &ti->error);\n\tif (r) {\n\t\tdm_put_path_selector(pst);\n\t\treturn -EINVAL;\n\t}\n\n\tr = pst->create(&pg->ps, ps_argc, as->argv);\n\tif (r) {\n\t\tdm_put_path_selector(pst);\n\t\tti->error = \"path selector constructor failed\";\n\t\treturn r;\n\t}\n\n\tpg->ps.type = pst;\n\tdm_consume_args(as, ps_argc);\n\n\treturn 0;\n}\n\nstatic int setup_scsi_dh(struct block_device *bdev, struct multipath *m,\n\t\t\t const char **attached_handler_name, char **error)\n{\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\tint r;\n\n\tif (mpath_double_check_test_bit(MPATHF_RETAIN_ATTACHED_HW_HANDLER, m)) {\nretain:\n\t\tif (*attached_handler_name) {\n\t\t\t \n\t\t\tif (m->hw_handler_name && strcmp(*attached_handler_name, m->hw_handler_name)) {\n\t\t\t\tkfree(m->hw_handler_params);\n\t\t\t\tm->hw_handler_params = NULL;\n\t\t\t}\n\n\t\t\t \n\t\t\tkfree(m->hw_handler_name);\n\t\t\tm->hw_handler_name = *attached_handler_name;\n\t\t\t*attached_handler_name = NULL;\n\t\t}\n\t}\n\n\tif (m->hw_handler_name) {\n\t\tr = scsi_dh_attach(q, m->hw_handler_name);\n\t\tif (r == -EBUSY) {\n\t\t\tDMINFO(\"retaining handler on device %pg\", bdev);\n\t\t\tgoto retain;\n\t\t}\n\t\tif (r < 0) {\n\t\t\t*error = \"error attaching hardware handler\";\n\t\t\treturn r;\n\t\t}\n\n\t\tif (m->hw_handler_params) {\n\t\t\tr = scsi_dh_set_params(q, m->hw_handler_params);\n\t\t\tif (r < 0) {\n\t\t\t\t*error = \"unable to set hardware handler parameters\";\n\t\t\t\treturn r;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic struct pgpath *parse_path(struct dm_arg_set *as, struct path_selector *ps,\n\t\t\t\t struct dm_target *ti)\n{\n\tint r;\n\tstruct pgpath *p;\n\tstruct multipath *m = ti->private;\n\tstruct request_queue *q;\n\tconst char *attached_handler_name = NULL;\n\n\t \n\tif (as->argc < 1) {\n\t\tti->error = \"no device given\";\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tp = alloc_pgpath();\n\tif (!p)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tr = dm_get_device(ti, dm_shift_arg(as), dm_table_get_mode(ti->table),\n\t\t\t  &p->path.dev);\n\tif (r) {\n\t\tti->error = \"error getting device\";\n\t\tgoto bad;\n\t}\n\n\tq = bdev_get_queue(p->path.dev->bdev);\n\tattached_handler_name = scsi_dh_attached_handler_name(q, GFP_KERNEL);\n\tif (attached_handler_name || m->hw_handler_name) {\n\t\tINIT_DELAYED_WORK(&p->activate_path, activate_path_work);\n\t\tr = setup_scsi_dh(p->path.dev->bdev, m, &attached_handler_name, &ti->error);\n\t\tkfree(attached_handler_name);\n\t\tif (r) {\n\t\t\tdm_put_device(ti, p->path.dev);\n\t\t\tgoto bad;\n\t\t}\n\t}\n\n\tr = ps->type->add_path(ps, &p->path, as->argc, as->argv, &ti->error);\n\tif (r) {\n\t\tdm_put_device(ti, p->path.dev);\n\t\tgoto bad;\n\t}\n\n\treturn p;\n bad:\n\tfree_pgpath(p);\n\treturn ERR_PTR(r);\n}\n\nstatic struct priority_group *parse_priority_group(struct dm_arg_set *as,\n\t\t\t\t\t\t   struct multipath *m)\n{\n\tstatic const struct dm_arg _args[] = {\n\t\t{1, 1024, \"invalid number of paths\"},\n\t\t{0, 1024, \"invalid number of selector args\"}\n\t};\n\n\tint r;\n\tunsigned int i, nr_selector_args, nr_args;\n\tstruct priority_group *pg;\n\tstruct dm_target *ti = m->ti;\n\n\tif (as->argc < 2) {\n\t\tas->argc = 0;\n\t\tti->error = \"not enough priority group arguments\";\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tpg = alloc_priority_group();\n\tif (!pg) {\n\t\tti->error = \"couldn't allocate priority group\";\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tpg->m = m;\n\n\tr = parse_path_selector(as, pg, ti);\n\tif (r)\n\t\tgoto bad;\n\n\t \n\tr = dm_read_arg(_args, as, &pg->nr_pgpaths, &ti->error);\n\tif (r)\n\t\tgoto bad;\n\n\tr = dm_read_arg(_args + 1, as, &nr_selector_args, &ti->error);\n\tif (r)\n\t\tgoto bad;\n\n\tnr_args = 1 + nr_selector_args;\n\tfor (i = 0; i < pg->nr_pgpaths; i++) {\n\t\tstruct pgpath *pgpath;\n\t\tstruct dm_arg_set path_args;\n\n\t\tif (as->argc < nr_args) {\n\t\t\tti->error = \"not enough path parameters\";\n\t\t\tr = -EINVAL;\n\t\t\tgoto bad;\n\t\t}\n\n\t\tpath_args.argc = nr_args;\n\t\tpath_args.argv = as->argv;\n\n\t\tpgpath = parse_path(&path_args, &pg->ps, ti);\n\t\tif (IS_ERR(pgpath)) {\n\t\t\tr = PTR_ERR(pgpath);\n\t\t\tgoto bad;\n\t\t}\n\n\t\tpgpath->pg = pg;\n\t\tlist_add_tail(&pgpath->list, &pg->pgpaths);\n\t\tdm_consume_args(as, nr_args);\n\t}\n\n\treturn pg;\n\n bad:\n\tfree_priority_group(pg, ti);\n\treturn ERR_PTR(r);\n}\n\nstatic int parse_hw_handler(struct dm_arg_set *as, struct multipath *m)\n{\n\tunsigned int hw_argc;\n\tint ret;\n\tstruct dm_target *ti = m->ti;\n\n\tstatic const struct dm_arg _args[] = {\n\t\t{0, 1024, \"invalid number of hardware handler args\"},\n\t};\n\n\tif (dm_read_arg_group(_args, as, &hw_argc, &ti->error))\n\t\treturn -EINVAL;\n\n\tif (!hw_argc)\n\t\treturn 0;\n\n\tif (m->queue_mode == DM_TYPE_BIO_BASED) {\n\t\tdm_consume_args(as, hw_argc);\n\t\tDMERR(\"bio-based multipath doesn't allow hardware handler args\");\n\t\treturn 0;\n\t}\n\n\tm->hw_handler_name = kstrdup(dm_shift_arg(as), GFP_KERNEL);\n\tif (!m->hw_handler_name)\n\t\treturn -EINVAL;\n\n\tif (hw_argc > 1) {\n\t\tchar *p;\n\t\tint i, j, len = 4;\n\n\t\tfor (i = 0; i <= hw_argc - 2; i++)\n\t\t\tlen += strlen(as->argv[i]) + 1;\n\t\tp = m->hw_handler_params = kzalloc(len, GFP_KERNEL);\n\t\tif (!p) {\n\t\t\tti->error = \"memory allocation failed\";\n\t\t\tret = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\t\tj = sprintf(p, \"%d\", hw_argc - 1);\n\t\tfor (i = 0, p += j + 1; i <= hw_argc - 2; i++, p += j + 1)\n\t\t\tj = sprintf(p, \"%s\", as->argv[i]);\n\t}\n\tdm_consume_args(as, hw_argc - 1);\n\n\treturn 0;\nfail:\n\tkfree(m->hw_handler_name);\n\tm->hw_handler_name = NULL;\n\treturn ret;\n}\n\nstatic int parse_features(struct dm_arg_set *as, struct multipath *m)\n{\n\tint r;\n\tunsigned int argc;\n\tstruct dm_target *ti = m->ti;\n\tconst char *arg_name;\n\n\tstatic const struct dm_arg _args[] = {\n\t\t{0, 8, \"invalid number of feature args\"},\n\t\t{1, 50, \"pg_init_retries must be between 1 and 50\"},\n\t\t{0, 60000, \"pg_init_delay_msecs must be between 0 and 60000\"},\n\t};\n\n\tr = dm_read_arg_group(_args, as, &argc, &ti->error);\n\tif (r)\n\t\treturn -EINVAL;\n\n\tif (!argc)\n\t\treturn 0;\n\n\tdo {\n\t\targ_name = dm_shift_arg(as);\n\t\targc--;\n\n\t\tif (!strcasecmp(arg_name, \"queue_if_no_path\")) {\n\t\t\tr = queue_if_no_path(m, true, false, __func__);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strcasecmp(arg_name, \"retain_attached_hw_handler\")) {\n\t\t\tset_bit(MPATHF_RETAIN_ATTACHED_HW_HANDLER, &m->flags);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strcasecmp(arg_name, \"pg_init_retries\") &&\n\t\t    (argc >= 1)) {\n\t\t\tr = dm_read_arg(_args + 1, as, &m->pg_init_retries, &ti->error);\n\t\t\targc--;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strcasecmp(arg_name, \"pg_init_delay_msecs\") &&\n\t\t    (argc >= 1)) {\n\t\t\tr = dm_read_arg(_args + 2, as, &m->pg_init_delay_msecs, &ti->error);\n\t\t\targc--;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strcasecmp(arg_name, \"queue_mode\") &&\n\t\t    (argc >= 1)) {\n\t\t\tconst char *queue_mode_name = dm_shift_arg(as);\n\n\t\t\tif (!strcasecmp(queue_mode_name, \"bio\"))\n\t\t\t\tm->queue_mode = DM_TYPE_BIO_BASED;\n\t\t\telse if (!strcasecmp(queue_mode_name, \"rq\") ||\n\t\t\t\t !strcasecmp(queue_mode_name, \"mq\"))\n\t\t\t\tm->queue_mode = DM_TYPE_REQUEST_BASED;\n\t\t\telse {\n\t\t\t\tti->error = \"Unknown 'queue_mode' requested\";\n\t\t\t\tr = -EINVAL;\n\t\t\t}\n\t\t\targc--;\n\t\t\tcontinue;\n\t\t}\n\n\t\tti->error = \"Unrecognised multipath feature request\";\n\t\tr = -EINVAL;\n\t} while (argc && !r);\n\n\treturn r;\n}\n\nstatic int multipath_ctr(struct dm_target *ti, unsigned int argc, char **argv)\n{\n\t \n\tstatic const struct dm_arg _args[] = {\n\t\t{0, 1024, \"invalid number of priority groups\"},\n\t\t{0, 1024, \"invalid initial priority group number\"},\n\t};\n\n\tint r;\n\tstruct multipath *m;\n\tstruct dm_arg_set as;\n\tunsigned int pg_count = 0;\n\tunsigned int next_pg_num;\n\tunsigned long flags;\n\n\tas.argc = argc;\n\tas.argv = argv;\n\n\tm = alloc_multipath(ti);\n\tif (!m) {\n\t\tti->error = \"can't allocate multipath\";\n\t\treturn -EINVAL;\n\t}\n\n\tr = parse_features(&as, m);\n\tif (r)\n\t\tgoto bad;\n\n\tr = alloc_multipath_stage2(ti, m);\n\tif (r)\n\t\tgoto bad;\n\n\tr = parse_hw_handler(&as, m);\n\tif (r)\n\t\tgoto bad;\n\n\tr = dm_read_arg(_args, &as, &m->nr_priority_groups, &ti->error);\n\tif (r)\n\t\tgoto bad;\n\n\tr = dm_read_arg(_args + 1, &as, &next_pg_num, &ti->error);\n\tif (r)\n\t\tgoto bad;\n\n\tif ((!m->nr_priority_groups && next_pg_num) ||\n\t    (m->nr_priority_groups && !next_pg_num)) {\n\t\tti->error = \"invalid initial priority group\";\n\t\tr = -EINVAL;\n\t\tgoto bad;\n\t}\n\n\t \n\twhile (as.argc) {\n\t\tstruct priority_group *pg;\n\t\tunsigned int nr_valid_paths = atomic_read(&m->nr_valid_paths);\n\n\t\tpg = parse_priority_group(&as, m);\n\t\tif (IS_ERR(pg)) {\n\t\t\tr = PTR_ERR(pg);\n\t\t\tgoto bad;\n\t\t}\n\n\t\tnr_valid_paths += pg->nr_pgpaths;\n\t\tatomic_set(&m->nr_valid_paths, nr_valid_paths);\n\n\t\tlist_add_tail(&pg->list, &m->priority_groups);\n\t\tpg_count++;\n\t\tpg->pg_num = pg_count;\n\t\tif (!--next_pg_num)\n\t\t\tm->next_pg = pg;\n\t}\n\n\tif (pg_count != m->nr_priority_groups) {\n\t\tti->error = \"priority group count mismatch\";\n\t\tr = -EINVAL;\n\t\tgoto bad;\n\t}\n\n\tspin_lock_irqsave(&m->lock, flags);\n\tenable_nopath_timeout(m);\n\tspin_unlock_irqrestore(&m->lock, flags);\n\n\tti->num_flush_bios = 1;\n\tti->num_discard_bios = 1;\n\tti->num_write_zeroes_bios = 1;\n\tif (m->queue_mode == DM_TYPE_BIO_BASED)\n\t\tti->per_io_data_size = multipath_per_bio_data_size();\n\telse\n\t\tti->per_io_data_size = sizeof(struct dm_mpath_io);\n\n\treturn 0;\n\n bad:\n\tfree_multipath(m);\n\treturn r;\n}\n\nstatic void multipath_wait_for_pg_init_completion(struct multipath *m)\n{\n\tDEFINE_WAIT(wait);\n\n\twhile (1) {\n\t\tprepare_to_wait(&m->pg_init_wait, &wait, TASK_UNINTERRUPTIBLE);\n\n\t\tif (!atomic_read(&m->pg_init_in_progress))\n\t\t\tbreak;\n\n\t\tio_schedule();\n\t}\n\tfinish_wait(&m->pg_init_wait, &wait);\n}\n\nstatic void flush_multipath_work(struct multipath *m)\n{\n\tif (m->hw_handler_name) {\n\t\tunsigned long flags;\n\n\t\tif (!atomic_read(&m->pg_init_in_progress))\n\t\t\tgoto skip;\n\n\t\tspin_lock_irqsave(&m->lock, flags);\n\t\tif (atomic_read(&m->pg_init_in_progress) &&\n\t\t    !test_and_set_bit(MPATHF_PG_INIT_DISABLED, &m->flags)) {\n\t\t\tspin_unlock_irqrestore(&m->lock, flags);\n\n\t\t\tflush_workqueue(kmpath_handlerd);\n\t\t\tmultipath_wait_for_pg_init_completion(m);\n\n\t\t\tspin_lock_irqsave(&m->lock, flags);\n\t\t\tclear_bit(MPATHF_PG_INIT_DISABLED, &m->flags);\n\t\t}\n\t\tspin_unlock_irqrestore(&m->lock, flags);\n\t}\nskip:\n\tif (m->queue_mode == DM_TYPE_BIO_BASED)\n\t\tflush_work(&m->process_queued_bios);\n\tflush_work(&m->trigger_event);\n}\n\nstatic void multipath_dtr(struct dm_target *ti)\n{\n\tstruct multipath *m = ti->private;\n\n\tdisable_nopath_timeout(m);\n\tflush_multipath_work(m);\n\tfree_multipath(m);\n}\n\n \nstatic int fail_path(struct pgpath *pgpath)\n{\n\tunsigned long flags;\n\tstruct multipath *m = pgpath->pg->m;\n\n\tspin_lock_irqsave(&m->lock, flags);\n\n\tif (!pgpath->is_active)\n\t\tgoto out;\n\n\tDMWARN(\"%s: Failing path %s.\",\n\t       dm_table_device_name(m->ti->table),\n\t       pgpath->path.dev->name);\n\n\tpgpath->pg->ps.type->fail_path(&pgpath->pg->ps, &pgpath->path);\n\tpgpath->is_active = false;\n\tpgpath->fail_count++;\n\n\tatomic_dec(&m->nr_valid_paths);\n\n\tif (pgpath == m->current_pgpath)\n\t\tm->current_pgpath = NULL;\n\n\tdm_path_uevent(DM_UEVENT_PATH_FAILED, m->ti,\n\t\t       pgpath->path.dev->name, atomic_read(&m->nr_valid_paths));\n\n\tqueue_work(dm_mpath_wq, &m->trigger_event);\n\n\tenable_nopath_timeout(m);\n\nout:\n\tspin_unlock_irqrestore(&m->lock, flags);\n\n\treturn 0;\n}\n\n \nstatic int reinstate_path(struct pgpath *pgpath)\n{\n\tint r = 0, run_queue = 0;\n\tunsigned long flags;\n\tstruct multipath *m = pgpath->pg->m;\n\tunsigned int nr_valid_paths;\n\n\tspin_lock_irqsave(&m->lock, flags);\n\n\tif (pgpath->is_active)\n\t\tgoto out;\n\n\tDMWARN(\"%s: Reinstating path %s.\",\n\t       dm_table_device_name(m->ti->table),\n\t       pgpath->path.dev->name);\n\n\tr = pgpath->pg->ps.type->reinstate_path(&pgpath->pg->ps, &pgpath->path);\n\tif (r)\n\t\tgoto out;\n\n\tpgpath->is_active = true;\n\n\tnr_valid_paths = atomic_inc_return(&m->nr_valid_paths);\n\tif (nr_valid_paths == 1) {\n\t\tm->current_pgpath = NULL;\n\t\trun_queue = 1;\n\t} else if (m->hw_handler_name && (m->current_pg == pgpath->pg)) {\n\t\tif (queue_work(kmpath_handlerd, &pgpath->activate_path.work))\n\t\t\tatomic_inc(&m->pg_init_in_progress);\n\t}\n\n\tdm_path_uevent(DM_UEVENT_PATH_REINSTATED, m->ti,\n\t\t       pgpath->path.dev->name, nr_valid_paths);\n\n\tschedule_work(&m->trigger_event);\n\nout:\n\tspin_unlock_irqrestore(&m->lock, flags);\n\tif (run_queue) {\n\t\tdm_table_run_md_queue_async(m->ti->table);\n\t\tprocess_queued_io_list(m);\n\t}\n\n\tif (pgpath->is_active)\n\t\tdisable_nopath_timeout(m);\n\n\treturn r;\n}\n\n \nstatic int action_dev(struct multipath *m, struct dm_dev *dev,\n\t\t      action_fn action)\n{\n\tint r = -EINVAL;\n\tstruct pgpath *pgpath;\n\tstruct priority_group *pg;\n\n\tlist_for_each_entry(pg, &m->priority_groups, list) {\n\t\tlist_for_each_entry(pgpath, &pg->pgpaths, list) {\n\t\t\tif (pgpath->path.dev == dev)\n\t\t\t\tr = action(pgpath);\n\t\t}\n\t}\n\n\treturn r;\n}\n\n \nstatic void bypass_pg(struct multipath *m, struct priority_group *pg,\n\t\t      bool bypassed)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&m->lock, flags);\n\n\tpg->bypassed = bypassed;\n\tm->current_pgpath = NULL;\n\tm->current_pg = NULL;\n\n\tspin_unlock_irqrestore(&m->lock, flags);\n\n\tschedule_work(&m->trigger_event);\n}\n\n \nstatic int switch_pg_num(struct multipath *m, const char *pgstr)\n{\n\tstruct priority_group *pg;\n\tunsigned int pgnum;\n\tunsigned long flags;\n\tchar dummy;\n\n\tif (!pgstr || (sscanf(pgstr, \"%u%c\", &pgnum, &dummy) != 1) || !pgnum ||\n\t    !m->nr_priority_groups || (pgnum > m->nr_priority_groups)) {\n\t\tDMWARN(\"invalid PG number supplied to %s\", __func__);\n\t\treturn -EINVAL;\n\t}\n\n\tspin_lock_irqsave(&m->lock, flags);\n\tlist_for_each_entry(pg, &m->priority_groups, list) {\n\t\tpg->bypassed = false;\n\t\tif (--pgnum)\n\t\t\tcontinue;\n\n\t\tm->current_pgpath = NULL;\n\t\tm->current_pg = NULL;\n\t\tm->next_pg = pg;\n\t}\n\tspin_unlock_irqrestore(&m->lock, flags);\n\n\tschedule_work(&m->trigger_event);\n\treturn 0;\n}\n\n \nstatic int bypass_pg_num(struct multipath *m, const char *pgstr, bool bypassed)\n{\n\tstruct priority_group *pg;\n\tunsigned int pgnum;\n\tchar dummy;\n\n\tif (!pgstr || (sscanf(pgstr, \"%u%c\", &pgnum, &dummy) != 1) || !pgnum ||\n\t    !m->nr_priority_groups || (pgnum > m->nr_priority_groups)) {\n\t\tDMWARN(\"invalid PG number supplied to bypass_pg\");\n\t\treturn -EINVAL;\n\t}\n\n\tlist_for_each_entry(pg, &m->priority_groups, list) {\n\t\tif (!--pgnum)\n\t\t\tbreak;\n\t}\n\n\tbypass_pg(m, pg, bypassed);\n\treturn 0;\n}\n\n \nstatic bool pg_init_limit_reached(struct multipath *m, struct pgpath *pgpath)\n{\n\tunsigned long flags;\n\tbool limit_reached = false;\n\n\tspin_lock_irqsave(&m->lock, flags);\n\n\tif (atomic_read(&m->pg_init_count) <= m->pg_init_retries &&\n\t    !test_bit(MPATHF_PG_INIT_DISABLED, &m->flags))\n\t\tset_bit(MPATHF_PG_INIT_REQUIRED, &m->flags);\n\telse\n\t\tlimit_reached = true;\n\n\tspin_unlock_irqrestore(&m->lock, flags);\n\n\treturn limit_reached;\n}\n\nstatic void pg_init_done(void *data, int errors)\n{\n\tstruct pgpath *pgpath = data;\n\tstruct priority_group *pg = pgpath->pg;\n\tstruct multipath *m = pg->m;\n\tunsigned long flags;\n\tbool delay_retry = false;\n\n\t \n\tswitch (errors) {\n\tcase SCSI_DH_OK:\n\t\tbreak;\n\tcase SCSI_DH_NOSYS:\n\t\tif (!m->hw_handler_name) {\n\t\t\terrors = 0;\n\t\t\tbreak;\n\t\t}\n\t\tDMERR(\"Could not failover the device: Handler scsi_dh_%s \"\n\t\t      \"Error %d.\", m->hw_handler_name, errors);\n\t\t \n\t\tfail_path(pgpath);\n\t\tbreak;\n\tcase SCSI_DH_DEV_TEMP_BUSY:\n\t\t \n\t\tbypass_pg(m, pg, true);\n\t\tbreak;\n\tcase SCSI_DH_RETRY:\n\t\t \n\t\tdelay_retry = true;\n\t\tfallthrough;\n\tcase SCSI_DH_IMM_RETRY:\n\tcase SCSI_DH_RES_TEMP_UNAVAIL:\n\t\tif (pg_init_limit_reached(m, pgpath))\n\t\t\tfail_path(pgpath);\n\t\terrors = 0;\n\t\tbreak;\n\tcase SCSI_DH_DEV_OFFLINED:\n\tdefault:\n\t\t \n\t\tfail_path(pgpath);\n\t}\n\n\tspin_lock_irqsave(&m->lock, flags);\n\tif (errors) {\n\t\tif (pgpath == m->current_pgpath) {\n\t\t\tDMERR(\"Could not failover device. Error %d.\", errors);\n\t\t\tm->current_pgpath = NULL;\n\t\t\tm->current_pg = NULL;\n\t\t}\n\t} else if (!test_bit(MPATHF_PG_INIT_REQUIRED, &m->flags))\n\t\tpg->bypassed = false;\n\n\tif (atomic_dec_return(&m->pg_init_in_progress) > 0)\n\t\t \n\t\tgoto out;\n\n\tif (test_bit(MPATHF_PG_INIT_REQUIRED, &m->flags)) {\n\t\tif (delay_retry)\n\t\t\tset_bit(MPATHF_PG_INIT_DELAY_RETRY, &m->flags);\n\t\telse\n\t\t\tclear_bit(MPATHF_PG_INIT_DELAY_RETRY, &m->flags);\n\n\t\tif (__pg_init_all_paths(m))\n\t\t\tgoto out;\n\t}\n\tclear_bit(MPATHF_QUEUE_IO, &m->flags);\n\n\tprocess_queued_io_list(m);\n\n\t \n\twake_up(&m->pg_init_wait);\n\nout:\n\tspin_unlock_irqrestore(&m->lock, flags);\n}\n\nstatic void activate_or_offline_path(struct pgpath *pgpath)\n{\n\tstruct request_queue *q = bdev_get_queue(pgpath->path.dev->bdev);\n\n\tif (pgpath->is_active && !blk_queue_dying(q))\n\t\tscsi_dh_activate(q, pg_init_done, pgpath);\n\telse\n\t\tpg_init_done(pgpath, SCSI_DH_DEV_OFFLINED);\n}\n\nstatic void activate_path_work(struct work_struct *work)\n{\n\tstruct pgpath *pgpath =\n\t\tcontainer_of(work, struct pgpath, activate_path.work);\n\n\tactivate_or_offline_path(pgpath);\n}\n\nstatic int multipath_end_io(struct dm_target *ti, struct request *clone,\n\t\t\t    blk_status_t error, union map_info *map_context)\n{\n\tstruct dm_mpath_io *mpio = get_mpio(map_context);\n\tstruct pgpath *pgpath = mpio->pgpath;\n\tint r = DM_ENDIO_DONE;\n\n\t \n\tif (error && blk_path_error(error)) {\n\t\tstruct multipath *m = ti->private;\n\n\t\tif (error == BLK_STS_RESOURCE)\n\t\t\tr = DM_ENDIO_DELAY_REQUEUE;\n\t\telse\n\t\t\tr = DM_ENDIO_REQUEUE;\n\n\t\tif (pgpath)\n\t\t\tfail_path(pgpath);\n\n\t\tif (!atomic_read(&m->nr_valid_paths) &&\n\t\t    !must_push_back_rq(m)) {\n\t\t\tif (error == BLK_STS_IOERR)\n\t\t\t\tdm_report_EIO(m);\n\t\t\t \n\t\t\tr = DM_ENDIO_DONE;\n\t\t}\n\t}\n\n\tif (pgpath) {\n\t\tstruct path_selector *ps = &pgpath->pg->ps;\n\n\t\tif (ps->type->end_io)\n\t\t\tps->type->end_io(ps, &pgpath->path, mpio->nr_bytes,\n\t\t\t\t\t clone->io_start_time_ns);\n\t}\n\n\treturn r;\n}\n\nstatic int multipath_end_io_bio(struct dm_target *ti, struct bio *clone,\n\t\t\t\tblk_status_t *error)\n{\n\tstruct multipath *m = ti->private;\n\tstruct dm_mpath_io *mpio = get_mpio_from_bio(clone);\n\tstruct pgpath *pgpath = mpio->pgpath;\n\tunsigned long flags;\n\tint r = DM_ENDIO_DONE;\n\n\tif (!*error || !blk_path_error(*error))\n\t\tgoto done;\n\n\tif (pgpath)\n\t\tfail_path(pgpath);\n\n\tif (!atomic_read(&m->nr_valid_paths)) {\n\t\tspin_lock_irqsave(&m->lock, flags);\n\t\tif (!test_bit(MPATHF_QUEUE_IF_NO_PATH, &m->flags)) {\n\t\t\tif (__must_push_back(m)) {\n\t\t\t\tr = DM_ENDIO_REQUEUE;\n\t\t\t} else {\n\t\t\t\tdm_report_EIO(m);\n\t\t\t\t*error = BLK_STS_IOERR;\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&m->lock, flags);\n\t\t\tgoto done;\n\t\t}\n\t\tspin_unlock_irqrestore(&m->lock, flags);\n\t}\n\n\tmultipath_queue_bio(m, clone);\n\tr = DM_ENDIO_INCOMPLETE;\ndone:\n\tif (pgpath) {\n\t\tstruct path_selector *ps = &pgpath->pg->ps;\n\n\t\tif (ps->type->end_io)\n\t\t\tps->type->end_io(ps, &pgpath->path, mpio->nr_bytes,\n\t\t\t\t\t (mpio->start_time_ns ?:\n\t\t\t\t\t  dm_start_time_ns_from_clone(clone)));\n\t}\n\n\treturn r;\n}\n\n \nstatic void multipath_presuspend(struct dm_target *ti)\n{\n\tstruct multipath *m = ti->private;\n\n\t \n\tif (m->queue_mode == DM_TYPE_BIO_BASED || !dm_noflush_suspending(m->ti))\n\t\tqueue_if_no_path(m, false, true, __func__);\n}\n\nstatic void multipath_postsuspend(struct dm_target *ti)\n{\n\tstruct multipath *m = ti->private;\n\n\tmutex_lock(&m->work_mutex);\n\tflush_multipath_work(m);\n\tmutex_unlock(&m->work_mutex);\n}\n\n \nstatic void multipath_resume(struct dm_target *ti)\n{\n\tstruct multipath *m = ti->private;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&m->lock, flags);\n\tif (test_bit(MPATHF_SAVED_QUEUE_IF_NO_PATH, &m->flags)) {\n\t\tset_bit(MPATHF_QUEUE_IF_NO_PATH, &m->flags);\n\t\tclear_bit(MPATHF_SAVED_QUEUE_IF_NO_PATH, &m->flags);\n\t}\n\n\tDMDEBUG(\"%s: %s finished; QIFNP = %d; SQIFNP = %d\",\n\t\tdm_table_device_name(m->ti->table), __func__,\n\t\ttest_bit(MPATHF_QUEUE_IF_NO_PATH, &m->flags),\n\t\ttest_bit(MPATHF_SAVED_QUEUE_IF_NO_PATH, &m->flags));\n\n\tspin_unlock_irqrestore(&m->lock, flags);\n}\n\n \nstatic void multipath_status(struct dm_target *ti, status_type_t type,\n\t\t\t     unsigned int status_flags, char *result, unsigned int maxlen)\n{\n\tint sz = 0, pg_counter, pgpath_counter;\n\tunsigned long flags;\n\tstruct multipath *m = ti->private;\n\tstruct priority_group *pg;\n\tstruct pgpath *p;\n\tunsigned int pg_num;\n\tchar state;\n\n\tspin_lock_irqsave(&m->lock, flags);\n\n\t \n\tif (type == STATUSTYPE_INFO)\n\t\tDMEMIT(\"2 %u %u \", test_bit(MPATHF_QUEUE_IO, &m->flags),\n\t\t       atomic_read(&m->pg_init_count));\n\telse {\n\t\tDMEMIT(\"%u \", test_bit(MPATHF_QUEUE_IF_NO_PATH, &m->flags) +\n\t\t\t      (m->pg_init_retries > 0) * 2 +\n\t\t\t      (m->pg_init_delay_msecs != DM_PG_INIT_DELAY_DEFAULT) * 2 +\n\t\t\t      test_bit(MPATHF_RETAIN_ATTACHED_HW_HANDLER, &m->flags) +\n\t\t\t      (m->queue_mode != DM_TYPE_REQUEST_BASED) * 2);\n\n\t\tif (test_bit(MPATHF_QUEUE_IF_NO_PATH, &m->flags))\n\t\t\tDMEMIT(\"queue_if_no_path \");\n\t\tif (m->pg_init_retries)\n\t\t\tDMEMIT(\"pg_init_retries %u \", m->pg_init_retries);\n\t\tif (m->pg_init_delay_msecs != DM_PG_INIT_DELAY_DEFAULT)\n\t\t\tDMEMIT(\"pg_init_delay_msecs %u \", m->pg_init_delay_msecs);\n\t\tif (test_bit(MPATHF_RETAIN_ATTACHED_HW_HANDLER, &m->flags))\n\t\t\tDMEMIT(\"retain_attached_hw_handler \");\n\t\tif (m->queue_mode != DM_TYPE_REQUEST_BASED) {\n\t\t\tswitch (m->queue_mode) {\n\t\t\tcase DM_TYPE_BIO_BASED:\n\t\t\t\tDMEMIT(\"queue_mode bio \");\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tWARN_ON_ONCE(true);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!m->hw_handler_name || type == STATUSTYPE_INFO)\n\t\tDMEMIT(\"0 \");\n\telse\n\t\tDMEMIT(\"1 %s \", m->hw_handler_name);\n\n\tDMEMIT(\"%u \", m->nr_priority_groups);\n\n\tif (m->next_pg)\n\t\tpg_num = m->next_pg->pg_num;\n\telse if (m->current_pg)\n\t\tpg_num = m->current_pg->pg_num;\n\telse\n\t\tpg_num = (m->nr_priority_groups ? 1 : 0);\n\n\tDMEMIT(\"%u \", pg_num);\n\n\tswitch (type) {\n\tcase STATUSTYPE_INFO:\n\t\tlist_for_each_entry(pg, &m->priority_groups, list) {\n\t\t\tif (pg->bypassed)\n\t\t\t\tstate = 'D';\t \n\t\t\telse if (pg == m->current_pg)\n\t\t\t\tstate = 'A';\t \n\t\t\telse\n\t\t\t\tstate = 'E';\t \n\n\t\t\tDMEMIT(\"%c \", state);\n\n\t\t\tif (pg->ps.type->status)\n\t\t\t\tsz += pg->ps.type->status(&pg->ps, NULL, type,\n\t\t\t\t\t\t\t  result + sz,\n\t\t\t\t\t\t\t  maxlen - sz);\n\t\t\telse\n\t\t\t\tDMEMIT(\"0 \");\n\n\t\t\tDMEMIT(\"%u %u \", pg->nr_pgpaths,\n\t\t\t       pg->ps.type->info_args);\n\n\t\t\tlist_for_each_entry(p, &pg->pgpaths, list) {\n\t\t\t\tDMEMIT(\"%s %s %u \", p->path.dev->name,\n\t\t\t\t       p->is_active ? \"A\" : \"F\",\n\t\t\t\t       p->fail_count);\n\t\t\t\tif (pg->ps.type->status)\n\t\t\t\t\tsz += pg->ps.type->status(&pg->ps,\n\t\t\t\t\t      &p->path, type, result + sz,\n\t\t\t\t\t      maxlen - sz);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\tcase STATUSTYPE_TABLE:\n\t\tlist_for_each_entry(pg, &m->priority_groups, list) {\n\t\t\tDMEMIT(\"%s \", pg->ps.type->name);\n\n\t\t\tif (pg->ps.type->status)\n\t\t\t\tsz += pg->ps.type->status(&pg->ps, NULL, type,\n\t\t\t\t\t\t\t  result + sz,\n\t\t\t\t\t\t\t  maxlen - sz);\n\t\t\telse\n\t\t\t\tDMEMIT(\"0 \");\n\n\t\t\tDMEMIT(\"%u %u \", pg->nr_pgpaths,\n\t\t\t       pg->ps.type->table_args);\n\n\t\t\tlist_for_each_entry(p, &pg->pgpaths, list) {\n\t\t\t\tDMEMIT(\"%s \", p->path.dev->name);\n\t\t\t\tif (pg->ps.type->status)\n\t\t\t\t\tsz += pg->ps.type->status(&pg->ps,\n\t\t\t\t\t      &p->path, type, result + sz,\n\t\t\t\t\t      maxlen - sz);\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\tcase STATUSTYPE_IMA:\n\t\tsz = 0;  \n\n\t\tDMEMIT_TARGET_NAME_VERSION(ti->type);\n\t\tDMEMIT(\",nr_priority_groups=%u\", m->nr_priority_groups);\n\n\t\tpg_counter = 0;\n\t\tlist_for_each_entry(pg, &m->priority_groups, list) {\n\t\t\tif (pg->bypassed)\n\t\t\t\tstate = 'D';\t \n\t\t\telse if (pg == m->current_pg)\n\t\t\t\tstate = 'A';\t \n\t\t\telse\n\t\t\t\tstate = 'E';\t \n\t\t\tDMEMIT(\",pg_state_%d=%c\", pg_counter, state);\n\t\t\tDMEMIT(\",nr_pgpaths_%d=%u\", pg_counter, pg->nr_pgpaths);\n\t\t\tDMEMIT(\",path_selector_name_%d=%s\", pg_counter, pg->ps.type->name);\n\n\t\t\tpgpath_counter = 0;\n\t\t\tlist_for_each_entry(p, &pg->pgpaths, list) {\n\t\t\t\tDMEMIT(\",path_name_%d_%d=%s,is_active_%d_%d=%c,fail_count_%d_%d=%u\",\n\t\t\t\t       pg_counter, pgpath_counter, p->path.dev->name,\n\t\t\t\t       pg_counter, pgpath_counter, p->is_active ? 'A' : 'F',\n\t\t\t\t       pg_counter, pgpath_counter, p->fail_count);\n\t\t\t\tif (pg->ps.type->status) {\n\t\t\t\t\tDMEMIT(\",path_selector_status_%d_%d=\",\n\t\t\t\t\t       pg_counter, pgpath_counter);\n\t\t\t\t\tsz += pg->ps.type->status(&pg->ps, &p->path,\n\t\t\t\t\t\t\t\t  type, result + sz,\n\t\t\t\t\t\t\t\t  maxlen - sz);\n\t\t\t\t}\n\t\t\t\tpgpath_counter++;\n\t\t\t}\n\t\t\tpg_counter++;\n\t\t}\n\t\tDMEMIT(\";\");\n\t\tbreak;\n\t}\n\n\tspin_unlock_irqrestore(&m->lock, flags);\n}\n\nstatic int multipath_message(struct dm_target *ti, unsigned int argc, char **argv,\n\t\t\t     char *result, unsigned int maxlen)\n{\n\tint r = -EINVAL;\n\tstruct dm_dev *dev;\n\tstruct multipath *m = ti->private;\n\taction_fn action;\n\tunsigned long flags;\n\n\tmutex_lock(&m->work_mutex);\n\n\tif (dm_suspended(ti)) {\n\t\tr = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tif (argc == 1) {\n\t\tif (!strcasecmp(argv[0], \"queue_if_no_path\")) {\n\t\t\tr = queue_if_no_path(m, true, false, __func__);\n\t\t\tspin_lock_irqsave(&m->lock, flags);\n\t\t\tenable_nopath_timeout(m);\n\t\t\tspin_unlock_irqrestore(&m->lock, flags);\n\t\t\tgoto out;\n\t\t} else if (!strcasecmp(argv[0], \"fail_if_no_path\")) {\n\t\t\tr = queue_if_no_path(m, false, false, __func__);\n\t\t\tdisable_nopath_timeout(m);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (argc != 2) {\n\t\tDMWARN(\"Invalid multipath message arguments. Expected 2 arguments, got %d.\", argc);\n\t\tgoto out;\n\t}\n\n\tif (!strcasecmp(argv[0], \"disable_group\")) {\n\t\tr = bypass_pg_num(m, argv[1], true);\n\t\tgoto out;\n\t} else if (!strcasecmp(argv[0], \"enable_group\")) {\n\t\tr = bypass_pg_num(m, argv[1], false);\n\t\tgoto out;\n\t} else if (!strcasecmp(argv[0], \"switch_group\")) {\n\t\tr = switch_pg_num(m, argv[1]);\n\t\tgoto out;\n\t} else if (!strcasecmp(argv[0], \"reinstate_path\"))\n\t\taction = reinstate_path;\n\telse if (!strcasecmp(argv[0], \"fail_path\"))\n\t\taction = fail_path;\n\telse {\n\t\tDMWARN(\"Unrecognised multipath message received: %s\", argv[0]);\n\t\tgoto out;\n\t}\n\n\tr = dm_get_device(ti, argv[1], dm_table_get_mode(ti->table), &dev);\n\tif (r) {\n\t\tDMWARN(\"message: error getting device %s\",\n\t\t       argv[1]);\n\t\tgoto out;\n\t}\n\n\tr = action_dev(m, dev, action);\n\n\tdm_put_device(ti, dev);\n\nout:\n\tmutex_unlock(&m->work_mutex);\n\treturn r;\n}\n\nstatic int multipath_prepare_ioctl(struct dm_target *ti,\n\t\t\t\t   struct block_device **bdev)\n{\n\tstruct multipath *m = ti->private;\n\tstruct pgpath *pgpath;\n\tunsigned long flags;\n\tint r;\n\n\tpgpath = READ_ONCE(m->current_pgpath);\n\tif (!pgpath || !mpath_double_check_test_bit(MPATHF_QUEUE_IO, m))\n\t\tpgpath = choose_pgpath(m, 0);\n\n\tif (pgpath) {\n\t\tif (!mpath_double_check_test_bit(MPATHF_QUEUE_IO, m)) {\n\t\t\t*bdev = pgpath->path.dev->bdev;\n\t\t\tr = 0;\n\t\t} else {\n\t\t\t \n\t\t\tr = -ENOTCONN;\n\t\t}\n\t} else {\n\t\t \n\t\tr = -EIO;\n\t\tspin_lock_irqsave(&m->lock, flags);\n\t\tif (test_bit(MPATHF_QUEUE_IF_NO_PATH, &m->flags))\n\t\t\tr = -ENOTCONN;\n\t\tspin_unlock_irqrestore(&m->lock, flags);\n\t}\n\n\tif (r == -ENOTCONN) {\n\t\tif (!READ_ONCE(m->current_pg)) {\n\t\t\t \n\t\t\t(void) choose_pgpath(m, 0);\n\t\t}\n\t\tspin_lock_irqsave(&m->lock, flags);\n\t\tif (test_bit(MPATHF_PG_INIT_REQUIRED, &m->flags))\n\t\t\t(void) __pg_init_all_paths(m);\n\t\tspin_unlock_irqrestore(&m->lock, flags);\n\t\tdm_table_run_md_queue_async(m->ti->table);\n\t\tprocess_queued_io_list(m);\n\t}\n\n\t \n\tif (!r && ti->len != bdev_nr_sectors((*bdev)))\n\t\treturn 1;\n\treturn r;\n}\n\nstatic int multipath_iterate_devices(struct dm_target *ti,\n\t\t\t\t     iterate_devices_callout_fn fn, void *data)\n{\n\tstruct multipath *m = ti->private;\n\tstruct priority_group *pg;\n\tstruct pgpath *p;\n\tint ret = 0;\n\n\tlist_for_each_entry(pg, &m->priority_groups, list) {\n\t\tlist_for_each_entry(p, &pg->pgpaths, list) {\n\t\t\tret = fn(ti, p->path.dev, ti->begin, ti->len, data);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic int pgpath_busy(struct pgpath *pgpath)\n{\n\tstruct request_queue *q = bdev_get_queue(pgpath->path.dev->bdev);\n\n\treturn blk_lld_busy(q);\n}\n\n \nstatic int multipath_busy(struct dm_target *ti)\n{\n\tbool busy = false, has_active = false;\n\tstruct multipath *m = ti->private;\n\tstruct priority_group *pg, *next_pg;\n\tstruct pgpath *pgpath;\n\n\t \n\tif (atomic_read(&m->pg_init_in_progress))\n\t\treturn true;\n\n\t \n\tif (!atomic_read(&m->nr_valid_paths)) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&m->lock, flags);\n\t\tif (test_bit(MPATHF_QUEUE_IF_NO_PATH, &m->flags)) {\n\t\t\tspin_unlock_irqrestore(&m->lock, flags);\n\t\t\treturn (m->queue_mode != DM_TYPE_REQUEST_BASED);\n\t\t}\n\t\tspin_unlock_irqrestore(&m->lock, flags);\n\t}\n\n\t \n\tpg = READ_ONCE(m->current_pg);\n\tnext_pg = READ_ONCE(m->next_pg);\n\tif (unlikely(!READ_ONCE(m->current_pgpath) && next_pg))\n\t\tpg = next_pg;\n\n\tif (!pg) {\n\t\t \n\t\treturn busy;\n\t}\n\n\t \n\tbusy = true;\n\tlist_for_each_entry(pgpath, &pg->pgpaths, list) {\n\t\tif (pgpath->is_active) {\n\t\t\thas_active = true;\n\t\t\tif (!pgpath_busy(pgpath)) {\n\t\t\t\tbusy = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!has_active) {\n\t\t \n\t\tbusy = false;\n\t}\n\n\treturn busy;\n}\n\n \nstatic struct target_type multipath_target = {\n\t.name = \"multipath\",\n\t.version = {1, 14, 0},\n\t.features = DM_TARGET_SINGLETON | DM_TARGET_IMMUTABLE |\n\t\t    DM_TARGET_PASSES_INTEGRITY,\n\t.module = THIS_MODULE,\n\t.ctr = multipath_ctr,\n\t.dtr = multipath_dtr,\n\t.clone_and_map_rq = multipath_clone_and_map,\n\t.release_clone_rq = multipath_release_clone,\n\t.rq_end_io = multipath_end_io,\n\t.map = multipath_map_bio,\n\t.end_io = multipath_end_io_bio,\n\t.presuspend = multipath_presuspend,\n\t.postsuspend = multipath_postsuspend,\n\t.resume = multipath_resume,\n\t.status = multipath_status,\n\t.message = multipath_message,\n\t.prepare_ioctl = multipath_prepare_ioctl,\n\t.iterate_devices = multipath_iterate_devices,\n\t.busy = multipath_busy,\n};\n\nstatic int __init dm_multipath_init(void)\n{\n\tint r = -ENOMEM;\n\n\tkmultipathd = alloc_workqueue(\"kmpathd\", WQ_MEM_RECLAIM, 0);\n\tif (!kmultipathd) {\n\t\tDMERR(\"failed to create workqueue kmpathd\");\n\t\tgoto bad_alloc_kmultipathd;\n\t}\n\n\t \n\tkmpath_handlerd = alloc_ordered_workqueue(\"kmpath_handlerd\",\n\t\t\t\t\t\t  WQ_MEM_RECLAIM);\n\tif (!kmpath_handlerd) {\n\t\tDMERR(\"failed to create workqueue kmpath_handlerd\");\n\t\tgoto bad_alloc_kmpath_handlerd;\n\t}\n\n\tdm_mpath_wq = alloc_workqueue(\"dm_mpath_wq\", 0, 0);\n\tif (!dm_mpath_wq) {\n\t\tDMERR(\"failed to create workqueue dm_mpath_wq\");\n\t\tgoto bad_alloc_dm_mpath_wq;\n\t}\n\n\tr = dm_register_target(&multipath_target);\n\tif (r < 0)\n\t\tgoto bad_register_target;\n\n\treturn 0;\n\nbad_register_target:\n\tdestroy_workqueue(dm_mpath_wq);\nbad_alloc_dm_mpath_wq:\n\tdestroy_workqueue(kmpath_handlerd);\nbad_alloc_kmpath_handlerd:\n\tdestroy_workqueue(kmultipathd);\nbad_alloc_kmultipathd:\n\treturn r;\n}\n\nstatic void __exit dm_multipath_exit(void)\n{\n\tdestroy_workqueue(dm_mpath_wq);\n\tdestroy_workqueue(kmpath_handlerd);\n\tdestroy_workqueue(kmultipathd);\n\n\tdm_unregister_target(&multipath_target);\n}\n\nmodule_init(dm_multipath_init);\nmodule_exit(dm_multipath_exit);\n\nmodule_param_named(queue_if_no_path_timeout_secs, queue_if_no_path_timeout_secs, ulong, 0644);\nMODULE_PARM_DESC(queue_if_no_path_timeout_secs, \"No available paths queue IO timeout in seconds\");\n\nMODULE_DESCRIPTION(DM_NAME \" multipath target\");\nMODULE_AUTHOR(\"Sistina Software <dm-devel@redhat.com>\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}