{
  "module_name": "raid10.c",
  "hash_id": "d81e07db9946042886a71e5579f2ce5218a2324cfb6a0cbb849c36dce2d2850b",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/raid10.c",
  "human_readable_source": "\n \n\n#include <linux/slab.h>\n#include <linux/delay.h>\n#include <linux/blkdev.h>\n#include <linux/module.h>\n#include <linux/seq_file.h>\n#include <linux/ratelimit.h>\n#include <linux/kthread.h>\n#include <linux/raid/md_p.h>\n#include <trace/events/block.h>\n#include \"md.h\"\n#include \"raid10.h\"\n#include \"raid0.h\"\n#include \"md-bitmap.h\"\n\n \n\nstatic void allow_barrier(struct r10conf *conf);\nstatic void lower_barrier(struct r10conf *conf);\nstatic int _enough(struct r10conf *conf, int previous, int ignore);\nstatic int enough(struct r10conf *conf, int ignore);\nstatic sector_t reshape_request(struct mddev *mddev, sector_t sector_nr,\n\t\t\t\tint *skipped);\nstatic void reshape_request_write(struct mddev *mddev, struct r10bio *r10_bio);\nstatic void end_reshape_write(struct bio *bio);\nstatic void end_reshape(struct r10conf *conf);\n\n#define raid10_log(md, fmt, args...)\t\t\t\t\\\n\tdo { if ((md)->queue) blk_add_trace_msg((md)->queue, \"raid10 \" fmt, ##args); } while (0)\n\n#include \"raid1-10.c\"\n\n#define NULL_CMD\n#define cmd_before(conf, cmd) \\\n\tdo { \\\n\t\twrite_sequnlock_irq(&(conf)->resync_lock); \\\n\t\tcmd; \\\n\t} while (0)\n#define cmd_after(conf) write_seqlock_irq(&(conf)->resync_lock)\n\n#define wait_event_barrier_cmd(conf, cond, cmd) \\\n\twait_event_cmd((conf)->wait_barrier, cond, cmd_before(conf, cmd), \\\n\t\t       cmd_after(conf))\n\n#define wait_event_barrier(conf, cond) \\\n\twait_event_barrier_cmd(conf, cond, NULL_CMD)\n\n \nstatic inline struct r10bio *get_resync_r10bio(struct bio *bio)\n{\n\treturn get_resync_pages(bio)->raid_bio;\n}\n\nstatic void * r10bio_pool_alloc(gfp_t gfp_flags, void *data)\n{\n\tstruct r10conf *conf = data;\n\tint size = offsetof(struct r10bio, devs[conf->geo.raid_disks]);\n\n\t \n\treturn kzalloc(size, gfp_flags);\n}\n\n#define RESYNC_SECTORS (RESYNC_BLOCK_SIZE >> 9)\n \n#define RESYNC_WINDOW (1024*1024)\n \n#define RESYNC_DEPTH (32*1024*1024/RESYNC_BLOCK_SIZE)\n#define CLUSTER_RESYNC_WINDOW (32 * RESYNC_WINDOW)\n#define CLUSTER_RESYNC_WINDOW_SECTORS (CLUSTER_RESYNC_WINDOW >> 9)\n\n \nstatic void * r10buf_pool_alloc(gfp_t gfp_flags, void *data)\n{\n\tstruct r10conf *conf = data;\n\tstruct r10bio *r10_bio;\n\tstruct bio *bio;\n\tint j;\n\tint nalloc, nalloc_rp;\n\tstruct resync_pages *rps;\n\n\tr10_bio = r10bio_pool_alloc(gfp_flags, conf);\n\tif (!r10_bio)\n\t\treturn NULL;\n\n\tif (test_bit(MD_RECOVERY_SYNC, &conf->mddev->recovery) ||\n\t    test_bit(MD_RECOVERY_RESHAPE, &conf->mddev->recovery))\n\t\tnalloc = conf->copies;  \n\telse\n\t\tnalloc = 2;  \n\n\t \n\tif (!conf->have_replacement)\n\t\tnalloc_rp = nalloc;\n\telse\n\t\tnalloc_rp = nalloc * 2;\n\trps = kmalloc_array(nalloc_rp, sizeof(struct resync_pages), gfp_flags);\n\tif (!rps)\n\t\tgoto out_free_r10bio;\n\n\t \n\tfor (j = nalloc ; j-- ; ) {\n\t\tbio = bio_kmalloc(RESYNC_PAGES, gfp_flags);\n\t\tif (!bio)\n\t\t\tgoto out_free_bio;\n\t\tbio_init(bio, NULL, bio->bi_inline_vecs, RESYNC_PAGES, 0);\n\t\tr10_bio->devs[j].bio = bio;\n\t\tif (!conf->have_replacement)\n\t\t\tcontinue;\n\t\tbio = bio_kmalloc(RESYNC_PAGES, gfp_flags);\n\t\tif (!bio)\n\t\t\tgoto out_free_bio;\n\t\tbio_init(bio, NULL, bio->bi_inline_vecs, RESYNC_PAGES, 0);\n\t\tr10_bio->devs[j].repl_bio = bio;\n\t}\n\t \n\tfor (j = 0; j < nalloc; j++) {\n\t\tstruct bio *rbio = r10_bio->devs[j].repl_bio;\n\t\tstruct resync_pages *rp, *rp_repl;\n\n\t\trp = &rps[j];\n\t\tif (rbio)\n\t\t\trp_repl = &rps[nalloc + j];\n\n\t\tbio = r10_bio->devs[j].bio;\n\n\t\tif (!j || test_bit(MD_RECOVERY_SYNC,\n\t\t\t\t   &conf->mddev->recovery)) {\n\t\t\tif (resync_alloc_pages(rp, gfp_flags))\n\t\t\t\tgoto out_free_pages;\n\t\t} else {\n\t\t\tmemcpy(rp, &rps[0], sizeof(*rp));\n\t\t\tresync_get_all_pages(rp);\n\t\t}\n\n\t\trp->raid_bio = r10_bio;\n\t\tbio->bi_private = rp;\n\t\tif (rbio) {\n\t\t\tmemcpy(rp_repl, rp, sizeof(*rp));\n\t\t\trbio->bi_private = rp_repl;\n\t\t}\n\t}\n\n\treturn r10_bio;\n\nout_free_pages:\n\twhile (--j >= 0)\n\t\tresync_free_pages(&rps[j]);\n\n\tj = 0;\nout_free_bio:\n\tfor ( ; j < nalloc; j++) {\n\t\tif (r10_bio->devs[j].bio)\n\t\t\tbio_uninit(r10_bio->devs[j].bio);\n\t\tkfree(r10_bio->devs[j].bio);\n\t\tif (r10_bio->devs[j].repl_bio)\n\t\t\tbio_uninit(r10_bio->devs[j].repl_bio);\n\t\tkfree(r10_bio->devs[j].repl_bio);\n\t}\n\tkfree(rps);\nout_free_r10bio:\n\trbio_pool_free(r10_bio, conf);\n\treturn NULL;\n}\n\nstatic void r10buf_pool_free(void *__r10_bio, void *data)\n{\n\tstruct r10conf *conf = data;\n\tstruct r10bio *r10bio = __r10_bio;\n\tint j;\n\tstruct resync_pages *rp = NULL;\n\n\tfor (j = conf->copies; j--; ) {\n\t\tstruct bio *bio = r10bio->devs[j].bio;\n\n\t\tif (bio) {\n\t\t\trp = get_resync_pages(bio);\n\t\t\tresync_free_pages(rp);\n\t\t\tbio_uninit(bio);\n\t\t\tkfree(bio);\n\t\t}\n\n\t\tbio = r10bio->devs[j].repl_bio;\n\t\tif (bio) {\n\t\t\tbio_uninit(bio);\n\t\t\tkfree(bio);\n\t\t}\n\t}\n\n\t \n\tkfree(rp);\n\n\trbio_pool_free(r10bio, conf);\n}\n\nstatic void put_all_bios(struct r10conf *conf, struct r10bio *r10_bio)\n{\n\tint i;\n\n\tfor (i = 0; i < conf->geo.raid_disks; i++) {\n\t\tstruct bio **bio = & r10_bio->devs[i].bio;\n\t\tif (!BIO_SPECIAL(*bio))\n\t\t\tbio_put(*bio);\n\t\t*bio = NULL;\n\t\tbio = &r10_bio->devs[i].repl_bio;\n\t\tif (r10_bio->read_slot < 0 && !BIO_SPECIAL(*bio))\n\t\t\tbio_put(*bio);\n\t\t*bio = NULL;\n\t}\n}\n\nstatic void free_r10bio(struct r10bio *r10_bio)\n{\n\tstruct r10conf *conf = r10_bio->mddev->private;\n\n\tput_all_bios(conf, r10_bio);\n\tmempool_free(r10_bio, &conf->r10bio_pool);\n}\n\nstatic void put_buf(struct r10bio *r10_bio)\n{\n\tstruct r10conf *conf = r10_bio->mddev->private;\n\n\tmempool_free(r10_bio, &conf->r10buf_pool);\n\n\tlower_barrier(conf);\n}\n\nstatic void wake_up_barrier(struct r10conf *conf)\n{\n\tif (wq_has_sleeper(&conf->wait_barrier))\n\t\twake_up(&conf->wait_barrier);\n}\n\nstatic void reschedule_retry(struct r10bio *r10_bio)\n{\n\tunsigned long flags;\n\tstruct mddev *mddev = r10_bio->mddev;\n\tstruct r10conf *conf = mddev->private;\n\n\tspin_lock_irqsave(&conf->device_lock, flags);\n\tlist_add(&r10_bio->retry_list, &conf->retry_list);\n\tconf->nr_queued ++;\n\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\n\t \n\twake_up(&conf->wait_barrier);\n\n\tmd_wakeup_thread(mddev->thread);\n}\n\n \nstatic void raid_end_bio_io(struct r10bio *r10_bio)\n{\n\tstruct bio *bio = r10_bio->master_bio;\n\tstruct r10conf *conf = r10_bio->mddev->private;\n\n\tif (!test_bit(R10BIO_Uptodate, &r10_bio->state))\n\t\tbio->bi_status = BLK_STS_IOERR;\n\n\tbio_endio(bio);\n\t \n\tallow_barrier(conf);\n\n\tfree_r10bio(r10_bio);\n}\n\n \nstatic inline void update_head_pos(int slot, struct r10bio *r10_bio)\n{\n\tstruct r10conf *conf = r10_bio->mddev->private;\n\n\tconf->mirrors[r10_bio->devs[slot].devnum].head_position =\n\t\tr10_bio->devs[slot].addr + (r10_bio->sectors);\n}\n\n \nstatic int find_bio_disk(struct r10conf *conf, struct r10bio *r10_bio,\n\t\t\t struct bio *bio, int *slotp, int *replp)\n{\n\tint slot;\n\tint repl = 0;\n\n\tfor (slot = 0; slot < conf->geo.raid_disks; slot++) {\n\t\tif (r10_bio->devs[slot].bio == bio)\n\t\t\tbreak;\n\t\tif (r10_bio->devs[slot].repl_bio == bio) {\n\t\t\trepl = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tupdate_head_pos(slot, r10_bio);\n\n\tif (slotp)\n\t\t*slotp = slot;\n\tif (replp)\n\t\t*replp = repl;\n\treturn r10_bio->devs[slot].devnum;\n}\n\nstatic void raid10_end_read_request(struct bio *bio)\n{\n\tint uptodate = !bio->bi_status;\n\tstruct r10bio *r10_bio = bio->bi_private;\n\tint slot;\n\tstruct md_rdev *rdev;\n\tstruct r10conf *conf = r10_bio->mddev->private;\n\n\tslot = r10_bio->read_slot;\n\trdev = r10_bio->devs[slot].rdev;\n\t \n\tupdate_head_pos(slot, r10_bio);\n\n\tif (uptodate) {\n\t\t \n\t\tset_bit(R10BIO_Uptodate, &r10_bio->state);\n\t} else {\n\t\t \n\t\tif (!_enough(conf, test_bit(R10BIO_Previous, &r10_bio->state),\n\t\t\t     rdev->raid_disk))\n\t\t\tuptodate = 1;\n\t}\n\tif (uptodate) {\n\t\traid_end_bio_io(r10_bio);\n\t\trdev_dec_pending(rdev, conf->mddev);\n\t} else {\n\t\t \n\t\tpr_err_ratelimited(\"md/raid10:%s: %pg: rescheduling sector %llu\\n\",\n\t\t\t\t   mdname(conf->mddev),\n\t\t\t\t   rdev->bdev,\n\t\t\t\t   (unsigned long long)r10_bio->sector);\n\t\tset_bit(R10BIO_ReadError, &r10_bio->state);\n\t\treschedule_retry(r10_bio);\n\t}\n}\n\nstatic void close_write(struct r10bio *r10_bio)\n{\n\t \n\tmd_bitmap_endwrite(r10_bio->mddev->bitmap, r10_bio->sector,\n\t\t\t   r10_bio->sectors,\n\t\t\t   !test_bit(R10BIO_Degraded, &r10_bio->state),\n\t\t\t   0);\n\tmd_write_end(r10_bio->mddev);\n}\n\nstatic void one_write_done(struct r10bio *r10_bio)\n{\n\tif (atomic_dec_and_test(&r10_bio->remaining)) {\n\t\tif (test_bit(R10BIO_WriteError, &r10_bio->state))\n\t\t\treschedule_retry(r10_bio);\n\t\telse {\n\t\t\tclose_write(r10_bio);\n\t\t\tif (test_bit(R10BIO_MadeGood, &r10_bio->state))\n\t\t\t\treschedule_retry(r10_bio);\n\t\t\telse\n\t\t\t\traid_end_bio_io(r10_bio);\n\t\t}\n\t}\n}\n\nstatic void raid10_end_write_request(struct bio *bio)\n{\n\tstruct r10bio *r10_bio = bio->bi_private;\n\tint dev;\n\tint dec_rdev = 1;\n\tstruct r10conf *conf = r10_bio->mddev->private;\n\tint slot, repl;\n\tstruct md_rdev *rdev = NULL;\n\tstruct bio *to_put = NULL;\n\tbool discard_error;\n\n\tdiscard_error = bio->bi_status && bio_op(bio) == REQ_OP_DISCARD;\n\n\tdev = find_bio_disk(conf, r10_bio, bio, &slot, &repl);\n\n\tif (repl)\n\t\trdev = conf->mirrors[dev].replacement;\n\tif (!rdev) {\n\t\tsmp_rmb();\n\t\trepl = 0;\n\t\trdev = conf->mirrors[dev].rdev;\n\t}\n\t \n\tif (bio->bi_status && !discard_error) {\n\t\tif (repl)\n\t\t\t \n\t\t\tmd_error(rdev->mddev, rdev);\n\t\telse {\n\t\t\tset_bit(WriteErrorSeen,\t&rdev->flags);\n\t\t\tif (!test_and_set_bit(WantReplacement, &rdev->flags))\n\t\t\t\tset_bit(MD_RECOVERY_NEEDED,\n\t\t\t\t\t&rdev->mddev->recovery);\n\n\t\t\tdec_rdev = 0;\n\t\t\tif (test_bit(FailFast, &rdev->flags) &&\n\t\t\t    (bio->bi_opf & MD_FAILFAST)) {\n\t\t\t\tmd_error(rdev->mddev, rdev);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (!test_bit(Faulty, &rdev->flags))\n\t\t\t\tset_bit(R10BIO_WriteError, &r10_bio->state);\n\t\t\telse {\n\t\t\t\t \n\t\t\t\tset_bit(R10BIO_Degraded, &r10_bio->state);\n\t\t\t\tr10_bio->devs[slot].bio = NULL;\n\t\t\t\tto_put = bio;\n\t\t\t\tdec_rdev = 1;\n\t\t\t}\n\t\t}\n\t} else {\n\t\t \n\t\tsector_t first_bad;\n\t\tint bad_sectors;\n\n\t\t \n\t\tif (test_bit(In_sync, &rdev->flags) &&\n\t\t    !test_bit(Faulty, &rdev->flags))\n\t\t\tset_bit(R10BIO_Uptodate, &r10_bio->state);\n\n\t\t \n\t\tif (is_badblock(rdev,\n\t\t\t\tr10_bio->devs[slot].addr,\n\t\t\t\tr10_bio->sectors,\n\t\t\t\t&first_bad, &bad_sectors) && !discard_error) {\n\t\t\tbio_put(bio);\n\t\t\tif (repl)\n\t\t\t\tr10_bio->devs[slot].repl_bio = IO_MADE_GOOD;\n\t\t\telse\n\t\t\t\tr10_bio->devs[slot].bio = IO_MADE_GOOD;\n\t\t\tdec_rdev = 0;\n\t\t\tset_bit(R10BIO_MadeGood, &r10_bio->state);\n\t\t}\n\t}\n\n\t \n\tone_write_done(r10_bio);\n\tif (dec_rdev)\n\t\trdev_dec_pending(rdev, conf->mddev);\n\tif (to_put)\n\t\tbio_put(to_put);\n}\n\n \n\nstatic void __raid10_find_phys(struct geom *geo, struct r10bio *r10bio)\n{\n\tint n,f;\n\tsector_t sector;\n\tsector_t chunk;\n\tsector_t stripe;\n\tint dev;\n\tint slot = 0;\n\tint last_far_set_start, last_far_set_size;\n\n\tlast_far_set_start = (geo->raid_disks / geo->far_set_size) - 1;\n\tlast_far_set_start *= geo->far_set_size;\n\n\tlast_far_set_size = geo->far_set_size;\n\tlast_far_set_size += (geo->raid_disks % geo->far_set_size);\n\n\t \n\tchunk = r10bio->sector >> geo->chunk_shift;\n\tsector = r10bio->sector & geo->chunk_mask;\n\n\tchunk *= geo->near_copies;\n\tstripe = chunk;\n\tdev = sector_div(stripe, geo->raid_disks);\n\tif (geo->far_offset)\n\t\tstripe *= geo->far_copies;\n\n\tsector += stripe << geo->chunk_shift;\n\n\t \n\tfor (n = 0; n < geo->near_copies; n++) {\n\t\tint d = dev;\n\t\tint set;\n\t\tsector_t s = sector;\n\t\tr10bio->devs[slot].devnum = d;\n\t\tr10bio->devs[slot].addr = s;\n\t\tslot++;\n\n\t\tfor (f = 1; f < geo->far_copies; f++) {\n\t\t\tset = d / geo->far_set_size;\n\t\t\td += geo->near_copies;\n\n\t\t\tif ((geo->raid_disks % geo->far_set_size) &&\n\t\t\t    (d > last_far_set_start)) {\n\t\t\t\td -= last_far_set_start;\n\t\t\t\td %= last_far_set_size;\n\t\t\t\td += last_far_set_start;\n\t\t\t} else {\n\t\t\t\td %= geo->far_set_size;\n\t\t\t\td += geo->far_set_size * set;\n\t\t\t}\n\t\t\ts += geo->stride;\n\t\t\tr10bio->devs[slot].devnum = d;\n\t\t\tr10bio->devs[slot].addr = s;\n\t\t\tslot++;\n\t\t}\n\t\tdev++;\n\t\tif (dev >= geo->raid_disks) {\n\t\t\tdev = 0;\n\t\t\tsector += (geo->chunk_mask + 1);\n\t\t}\n\t}\n}\n\nstatic void raid10_find_phys(struct r10conf *conf, struct r10bio *r10bio)\n{\n\tstruct geom *geo = &conf->geo;\n\n\tif (conf->reshape_progress != MaxSector &&\n\t    ((r10bio->sector >= conf->reshape_progress) !=\n\t     conf->mddev->reshape_backwards)) {\n\t\tset_bit(R10BIO_Previous, &r10bio->state);\n\t\tgeo = &conf->prev;\n\t} else\n\t\tclear_bit(R10BIO_Previous, &r10bio->state);\n\n\t__raid10_find_phys(geo, r10bio);\n}\n\nstatic sector_t raid10_find_virt(struct r10conf *conf, sector_t sector, int dev)\n{\n\tsector_t offset, chunk, vchunk;\n\t \n\tstruct geom *geo = &conf->geo;\n\tint far_set_start = (dev / geo->far_set_size) * geo->far_set_size;\n\tint far_set_size = geo->far_set_size;\n\tint last_far_set_start;\n\n\tif (geo->raid_disks % geo->far_set_size) {\n\t\tlast_far_set_start = (geo->raid_disks / geo->far_set_size) - 1;\n\t\tlast_far_set_start *= geo->far_set_size;\n\n\t\tif (dev >= last_far_set_start) {\n\t\t\tfar_set_size = geo->far_set_size;\n\t\t\tfar_set_size += (geo->raid_disks % geo->far_set_size);\n\t\t\tfar_set_start = last_far_set_start;\n\t\t}\n\t}\n\n\toffset = sector & geo->chunk_mask;\n\tif (geo->far_offset) {\n\t\tint fc;\n\t\tchunk = sector >> geo->chunk_shift;\n\t\tfc = sector_div(chunk, geo->far_copies);\n\t\tdev -= fc * geo->near_copies;\n\t\tif (dev < far_set_start)\n\t\t\tdev += far_set_size;\n\t} else {\n\t\twhile (sector >= geo->stride) {\n\t\t\tsector -= geo->stride;\n\t\t\tif (dev < (geo->near_copies + far_set_start))\n\t\t\t\tdev += far_set_size - geo->near_copies;\n\t\t\telse\n\t\t\t\tdev -= geo->near_copies;\n\t\t}\n\t\tchunk = sector >> geo->chunk_shift;\n\t}\n\tvchunk = chunk * geo->raid_disks + dev;\n\tsector_div(vchunk, geo->near_copies);\n\treturn (vchunk << geo->chunk_shift) + offset;\n}\n\n \n\n \nstatic struct md_rdev *read_balance(struct r10conf *conf,\n\t\t\t\t    struct r10bio *r10_bio,\n\t\t\t\t    int *max_sectors)\n{\n\tconst sector_t this_sector = r10_bio->sector;\n\tint disk, slot;\n\tint sectors = r10_bio->sectors;\n\tint best_good_sectors;\n\tsector_t new_distance, best_dist;\n\tstruct md_rdev *best_dist_rdev, *best_pending_rdev, *rdev = NULL;\n\tint do_balance;\n\tint best_dist_slot, best_pending_slot;\n\tbool has_nonrot_disk = false;\n\tunsigned int min_pending;\n\tstruct geom *geo = &conf->geo;\n\n\traid10_find_phys(conf, r10_bio);\n\trcu_read_lock();\n\tbest_dist_slot = -1;\n\tmin_pending = UINT_MAX;\n\tbest_dist_rdev = NULL;\n\tbest_pending_rdev = NULL;\n\tbest_dist = MaxSector;\n\tbest_good_sectors = 0;\n\tdo_balance = 1;\n\tclear_bit(R10BIO_FailFast, &r10_bio->state);\n\t \n\tif ((conf->mddev->recovery_cp < MaxSector\n\t     && (this_sector + sectors >= conf->next_resync)) ||\n\t    (mddev_is_clustered(conf->mddev) &&\n\t     md_cluster_ops->area_resyncing(conf->mddev, READ, this_sector,\n\t\t\t\t\t    this_sector + sectors)))\n\t\tdo_balance = 0;\n\n\tfor (slot = 0; slot < conf->copies ; slot++) {\n\t\tsector_t first_bad;\n\t\tint bad_sectors;\n\t\tsector_t dev_sector;\n\t\tunsigned int pending;\n\t\tbool nonrot;\n\n\t\tif (r10_bio->devs[slot].bio == IO_BLOCKED)\n\t\t\tcontinue;\n\t\tdisk = r10_bio->devs[slot].devnum;\n\t\trdev = rcu_dereference(conf->mirrors[disk].replacement);\n\t\tif (rdev == NULL || test_bit(Faulty, &rdev->flags) ||\n\t\t    r10_bio->devs[slot].addr + sectors >\n\t\t    rdev->recovery_offset) {\n\t\t\t \n\t\t\tsmp_mb();\n\t\t\trdev = rcu_dereference(conf->mirrors[disk].rdev);\n\t\t}\n\t\tif (rdev == NULL ||\n\t\t    test_bit(Faulty, &rdev->flags))\n\t\t\tcontinue;\n\t\tif (!test_bit(In_sync, &rdev->flags) &&\n\t\t    r10_bio->devs[slot].addr + sectors > rdev->recovery_offset)\n\t\t\tcontinue;\n\n\t\tdev_sector = r10_bio->devs[slot].addr;\n\t\tif (is_badblock(rdev, dev_sector, sectors,\n\t\t\t\t&first_bad, &bad_sectors)) {\n\t\t\tif (best_dist < MaxSector)\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\tif (first_bad <= dev_sector) {\n\t\t\t\t \n\t\t\t\tbad_sectors -= (dev_sector - first_bad);\n\t\t\t\tif (!do_balance && sectors > bad_sectors)\n\t\t\t\t\tsectors = bad_sectors;\n\t\t\t\tif (best_good_sectors > sectors)\n\t\t\t\t\tbest_good_sectors = sectors;\n\t\t\t} else {\n\t\t\t\tsector_t good_sectors =\n\t\t\t\t\tfirst_bad - dev_sector;\n\t\t\t\tif (good_sectors > best_good_sectors) {\n\t\t\t\t\tbest_good_sectors = good_sectors;\n\t\t\t\t\tbest_dist_slot = slot;\n\t\t\t\t\tbest_dist_rdev = rdev;\n\t\t\t\t}\n\t\t\t\tif (!do_balance)\n\t\t\t\t\t \n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t} else\n\t\t\tbest_good_sectors = sectors;\n\n\t\tif (!do_balance)\n\t\t\tbreak;\n\n\t\tnonrot = bdev_nonrot(rdev->bdev);\n\t\thas_nonrot_disk |= nonrot;\n\t\tpending = atomic_read(&rdev->nr_pending);\n\t\tif (min_pending > pending && nonrot) {\n\t\t\tmin_pending = pending;\n\t\t\tbest_pending_slot = slot;\n\t\t\tbest_pending_rdev = rdev;\n\t\t}\n\n\t\tif (best_dist_slot >= 0)\n\t\t\t \n\t\t\tset_bit(R10BIO_FailFast, &r10_bio->state);\n\t\t \n\t\tif (geo->near_copies > 1 && !pending)\n\t\t\tnew_distance = 0;\n\n\t\t \n\t\telse if (geo->far_copies > 1)\n\t\t\tnew_distance = r10_bio->devs[slot].addr;\n\t\telse\n\t\t\tnew_distance = abs(r10_bio->devs[slot].addr -\n\t\t\t\t\t   conf->mirrors[disk].head_position);\n\n\t\tif (new_distance < best_dist) {\n\t\t\tbest_dist = new_distance;\n\t\t\tbest_dist_slot = slot;\n\t\t\tbest_dist_rdev = rdev;\n\t\t}\n\t}\n\tif (slot >= conf->copies) {\n\t\tif (has_nonrot_disk) {\n\t\t\tslot = best_pending_slot;\n\t\t\trdev = best_pending_rdev;\n\t\t} else {\n\t\t\tslot = best_dist_slot;\n\t\t\trdev = best_dist_rdev;\n\t\t}\n\t}\n\n\tif (slot >= 0) {\n\t\tatomic_inc(&rdev->nr_pending);\n\t\tr10_bio->read_slot = slot;\n\t} else\n\t\trdev = NULL;\n\trcu_read_unlock();\n\t*max_sectors = best_good_sectors;\n\n\treturn rdev;\n}\n\nstatic void flush_pending_writes(struct r10conf *conf)\n{\n\t \n\tspin_lock_irq(&conf->device_lock);\n\n\tif (conf->pending_bio_list.head) {\n\t\tstruct blk_plug plug;\n\t\tstruct bio *bio;\n\n\t\tbio = bio_list_get(&conf->pending_bio_list);\n\t\tspin_unlock_irq(&conf->device_lock);\n\n\t\t \n\t\t__set_current_state(TASK_RUNNING);\n\n\t\tblk_start_plug(&plug);\n\t\traid1_prepare_flush_writes(conf->mddev->bitmap);\n\t\twake_up(&conf->wait_barrier);\n\n\t\twhile (bio) {  \n\t\t\tstruct bio *next = bio->bi_next;\n\n\t\t\traid1_submit_write(bio);\n\t\t\tbio = next;\n\t\t\tcond_resched();\n\t\t}\n\t\tblk_finish_plug(&plug);\n\t} else\n\t\tspin_unlock_irq(&conf->device_lock);\n}\n\n \n\nstatic void raise_barrier(struct r10conf *conf, int force)\n{\n\twrite_seqlock_irq(&conf->resync_lock);\n\n\tif (WARN_ON_ONCE(force && !conf->barrier))\n\t\tforce = false;\n\n\t \n\twait_event_barrier(conf, force || !conf->nr_waiting);\n\n\t \n\tWRITE_ONCE(conf->barrier, conf->barrier + 1);\n\n\t \n\twait_event_barrier(conf, !atomic_read(&conf->nr_pending) &&\n\t\t\t\t conf->barrier < RESYNC_DEPTH);\n\n\twrite_sequnlock_irq(&conf->resync_lock);\n}\n\nstatic void lower_barrier(struct r10conf *conf)\n{\n\tunsigned long flags;\n\n\twrite_seqlock_irqsave(&conf->resync_lock, flags);\n\tWRITE_ONCE(conf->barrier, conf->barrier - 1);\n\twrite_sequnlock_irqrestore(&conf->resync_lock, flags);\n\twake_up(&conf->wait_barrier);\n}\n\nstatic bool stop_waiting_barrier(struct r10conf *conf)\n{\n\tstruct bio_list *bio_list = current->bio_list;\n\tstruct md_thread *thread;\n\n\t \n\tif (!conf->barrier)\n\t\treturn true;\n\n\t \n\tif (atomic_read(&conf->nr_pending) && bio_list &&\n\t    (!bio_list_empty(&bio_list[0]) || !bio_list_empty(&bio_list[1])))\n\t\treturn true;\n\n\t \n\tthread = rcu_dereference_protected(conf->mddev->thread, true);\n\t \n\tif (thread->tsk == current) {\n\t\tWARN_ON_ONCE(atomic_read(&conf->nr_pending) == 0);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool wait_barrier_nolock(struct r10conf *conf)\n{\n\tunsigned int seq = read_seqbegin(&conf->resync_lock);\n\n\tif (READ_ONCE(conf->barrier))\n\t\treturn false;\n\n\tatomic_inc(&conf->nr_pending);\n\tif (!read_seqretry(&conf->resync_lock, seq))\n\t\treturn true;\n\n\tif (atomic_dec_and_test(&conf->nr_pending))\n\t\twake_up_barrier(conf);\n\n\treturn false;\n}\n\nstatic bool wait_barrier(struct r10conf *conf, bool nowait)\n{\n\tbool ret = true;\n\n\tif (wait_barrier_nolock(conf))\n\t\treturn true;\n\n\twrite_seqlock_irq(&conf->resync_lock);\n\tif (conf->barrier) {\n\t\t \n\t\tif (nowait) {\n\t\t\tret = false;\n\t\t} else {\n\t\t\tconf->nr_waiting++;\n\t\t\traid10_log(conf->mddev, \"wait barrier\");\n\t\t\twait_event_barrier(conf, stop_waiting_barrier(conf));\n\t\t\tconf->nr_waiting--;\n\t\t}\n\t\tif (!conf->nr_waiting)\n\t\t\twake_up(&conf->wait_barrier);\n\t}\n\t \n\tif (ret)\n\t\tatomic_inc(&conf->nr_pending);\n\twrite_sequnlock_irq(&conf->resync_lock);\n\treturn ret;\n}\n\nstatic void allow_barrier(struct r10conf *conf)\n{\n\tif ((atomic_dec_and_test(&conf->nr_pending)) ||\n\t\t\t(conf->array_freeze_pending))\n\t\twake_up_barrier(conf);\n}\n\nstatic void freeze_array(struct r10conf *conf, int extra)\n{\n\t \n\twrite_seqlock_irq(&conf->resync_lock);\n\tconf->array_freeze_pending++;\n\tWRITE_ONCE(conf->barrier, conf->barrier + 1);\n\tconf->nr_waiting++;\n\twait_event_barrier_cmd(conf, atomic_read(&conf->nr_pending) ==\n\t\t\tconf->nr_queued + extra, flush_pending_writes(conf));\n\tconf->array_freeze_pending--;\n\twrite_sequnlock_irq(&conf->resync_lock);\n}\n\nstatic void unfreeze_array(struct r10conf *conf)\n{\n\t \n\twrite_seqlock_irq(&conf->resync_lock);\n\tWRITE_ONCE(conf->barrier, conf->barrier - 1);\n\tconf->nr_waiting--;\n\twake_up(&conf->wait_barrier);\n\twrite_sequnlock_irq(&conf->resync_lock);\n}\n\nstatic sector_t choose_data_offset(struct r10bio *r10_bio,\n\t\t\t\t   struct md_rdev *rdev)\n{\n\tif (!test_bit(MD_RECOVERY_RESHAPE, &rdev->mddev->recovery) ||\n\t    test_bit(R10BIO_Previous, &r10_bio->state))\n\t\treturn rdev->data_offset;\n\telse\n\t\treturn rdev->new_data_offset;\n}\n\nstatic void raid10_unplug(struct blk_plug_cb *cb, bool from_schedule)\n{\n\tstruct raid1_plug_cb *plug = container_of(cb, struct raid1_plug_cb, cb);\n\tstruct mddev *mddev = plug->cb.data;\n\tstruct r10conf *conf = mddev->private;\n\tstruct bio *bio;\n\n\tif (from_schedule) {\n\t\tspin_lock_irq(&conf->device_lock);\n\t\tbio_list_merge(&conf->pending_bio_list, &plug->pending);\n\t\tspin_unlock_irq(&conf->device_lock);\n\t\twake_up_barrier(conf);\n\t\tmd_wakeup_thread(mddev->thread);\n\t\tkfree(plug);\n\t\treturn;\n\t}\n\n\t \n\tbio = bio_list_get(&plug->pending);\n\traid1_prepare_flush_writes(mddev->bitmap);\n\twake_up_barrier(conf);\n\n\twhile (bio) {  \n\t\tstruct bio *next = bio->bi_next;\n\n\t\traid1_submit_write(bio);\n\t\tbio = next;\n\t\tcond_resched();\n\t}\n\tkfree(plug);\n}\n\n \nstatic bool regular_request_wait(struct mddev *mddev, struct r10conf *conf,\n\t\t\t\t struct bio *bio, sector_t sectors)\n{\n\t \n\tif (!wait_barrier(conf, bio->bi_opf & REQ_NOWAIT)) {\n\t\tbio_wouldblock_error(bio);\n\t\treturn false;\n\t}\n\twhile (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&\n\t    bio->bi_iter.bi_sector < conf->reshape_progress &&\n\t    bio->bi_iter.bi_sector + sectors > conf->reshape_progress) {\n\t\tallow_barrier(conf);\n\t\tif (bio->bi_opf & REQ_NOWAIT) {\n\t\t\tbio_wouldblock_error(bio);\n\t\t\treturn false;\n\t\t}\n\t\traid10_log(conf->mddev, \"wait reshape\");\n\t\twait_event(conf->wait_barrier,\n\t\t\t   conf->reshape_progress <= bio->bi_iter.bi_sector ||\n\t\t\t   conf->reshape_progress >= bio->bi_iter.bi_sector +\n\t\t\t   sectors);\n\t\twait_barrier(conf, false);\n\t}\n\treturn true;\n}\n\nstatic void raid10_read_request(struct mddev *mddev, struct bio *bio,\n\t\t\t\tstruct r10bio *r10_bio, bool io_accounting)\n{\n\tstruct r10conf *conf = mddev->private;\n\tstruct bio *read_bio;\n\tconst enum req_op op = bio_op(bio);\n\tconst blk_opf_t do_sync = bio->bi_opf & REQ_SYNC;\n\tint max_sectors;\n\tstruct md_rdev *rdev;\n\tchar b[BDEVNAME_SIZE];\n\tint slot = r10_bio->read_slot;\n\tstruct md_rdev *err_rdev = NULL;\n\tgfp_t gfp = GFP_NOIO;\n\n\tif (slot >= 0 && r10_bio->devs[slot].rdev) {\n\t\t \n\t\tint disk;\n\t\t \n\t\tgfp = GFP_NOIO | __GFP_HIGH;\n\n\t\trcu_read_lock();\n\t\tdisk = r10_bio->devs[slot].devnum;\n\t\terr_rdev = rcu_dereference(conf->mirrors[disk].rdev);\n\t\tif (err_rdev)\n\t\t\tsnprintf(b, sizeof(b), \"%pg\", err_rdev->bdev);\n\t\telse {\n\t\t\tstrcpy(b, \"???\");\n\t\t\t \n\t\t\terr_rdev = r10_bio->devs[slot].rdev;\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tif (!regular_request_wait(mddev, conf, bio, r10_bio->sectors))\n\t\treturn;\n\trdev = read_balance(conf, r10_bio, &max_sectors);\n\tif (!rdev) {\n\t\tif (err_rdev) {\n\t\t\tpr_crit_ratelimited(\"md/raid10:%s: %s: unrecoverable I/O read error for block %llu\\n\",\n\t\t\t\t\t    mdname(mddev), b,\n\t\t\t\t\t    (unsigned long long)r10_bio->sector);\n\t\t}\n\t\traid_end_bio_io(r10_bio);\n\t\treturn;\n\t}\n\tif (err_rdev)\n\t\tpr_err_ratelimited(\"md/raid10:%s: %pg: redirecting sector %llu to another mirror\\n\",\n\t\t\t\t   mdname(mddev),\n\t\t\t\t   rdev->bdev,\n\t\t\t\t   (unsigned long long)r10_bio->sector);\n\tif (max_sectors < bio_sectors(bio)) {\n\t\tstruct bio *split = bio_split(bio, max_sectors,\n\t\t\t\t\t      gfp, &conf->bio_split);\n\t\tbio_chain(split, bio);\n\t\tallow_barrier(conf);\n\t\tsubmit_bio_noacct(bio);\n\t\twait_barrier(conf, false);\n\t\tbio = split;\n\t\tr10_bio->master_bio = bio;\n\t\tr10_bio->sectors = max_sectors;\n\t}\n\tslot = r10_bio->read_slot;\n\n\tif (io_accounting) {\n\t\tmd_account_bio(mddev, &bio);\n\t\tr10_bio->master_bio = bio;\n\t}\n\tread_bio = bio_alloc_clone(rdev->bdev, bio, gfp, &mddev->bio_set);\n\n\tr10_bio->devs[slot].bio = read_bio;\n\tr10_bio->devs[slot].rdev = rdev;\n\n\tread_bio->bi_iter.bi_sector = r10_bio->devs[slot].addr +\n\t\tchoose_data_offset(r10_bio, rdev);\n\tread_bio->bi_end_io = raid10_end_read_request;\n\tread_bio->bi_opf = op | do_sync;\n\tif (test_bit(FailFast, &rdev->flags) &&\n\t    test_bit(R10BIO_FailFast, &r10_bio->state))\n\t        read_bio->bi_opf |= MD_FAILFAST;\n\tread_bio->bi_private = r10_bio;\n\n\tif (mddev->gendisk)\n\t        trace_block_bio_remap(read_bio, disk_devt(mddev->gendisk),\n\t                              r10_bio->sector);\n\tsubmit_bio_noacct(read_bio);\n\treturn;\n}\n\nstatic void raid10_write_one_disk(struct mddev *mddev, struct r10bio *r10_bio,\n\t\t\t\t  struct bio *bio, bool replacement,\n\t\t\t\t  int n_copy)\n{\n\tconst enum req_op op = bio_op(bio);\n\tconst blk_opf_t do_sync = bio->bi_opf & REQ_SYNC;\n\tconst blk_opf_t do_fua = bio->bi_opf & REQ_FUA;\n\tunsigned long flags;\n\tstruct r10conf *conf = mddev->private;\n\tstruct md_rdev *rdev;\n\tint devnum = r10_bio->devs[n_copy].devnum;\n\tstruct bio *mbio;\n\n\tif (replacement) {\n\t\trdev = conf->mirrors[devnum].replacement;\n\t\tif (rdev == NULL) {\n\t\t\t \n\t\t\tsmp_mb();\n\t\t\trdev = conf->mirrors[devnum].rdev;\n\t\t}\n\t} else\n\t\trdev = conf->mirrors[devnum].rdev;\n\n\tmbio = bio_alloc_clone(rdev->bdev, bio, GFP_NOIO, &mddev->bio_set);\n\tif (replacement)\n\t\tr10_bio->devs[n_copy].repl_bio = mbio;\n\telse\n\t\tr10_bio->devs[n_copy].bio = mbio;\n\n\tmbio->bi_iter.bi_sector\t= (r10_bio->devs[n_copy].addr +\n\t\t\t\t   choose_data_offset(r10_bio, rdev));\n\tmbio->bi_end_io\t= raid10_end_write_request;\n\tmbio->bi_opf = op | do_sync | do_fua;\n\tif (!replacement && test_bit(FailFast,\n\t\t\t\t     &conf->mirrors[devnum].rdev->flags)\n\t\t\t && enough(conf, devnum))\n\t\tmbio->bi_opf |= MD_FAILFAST;\n\tmbio->bi_private = r10_bio;\n\n\tif (conf->mddev->gendisk)\n\t\ttrace_block_bio_remap(mbio, disk_devt(conf->mddev->gendisk),\n\t\t\t\t      r10_bio->sector);\n\t \n\tmbio->bi_bdev = (void *)rdev;\n\n\tatomic_inc(&r10_bio->remaining);\n\n\tif (!raid1_add_bio_to_plug(mddev, mbio, raid10_unplug, conf->copies)) {\n\t\tspin_lock_irqsave(&conf->device_lock, flags);\n\t\tbio_list_add(&conf->pending_bio_list, mbio);\n\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\t\tmd_wakeup_thread(mddev->thread);\n\t}\n}\n\nstatic struct md_rdev *dereference_rdev_and_rrdev(struct raid10_info *mirror,\n\t\t\t\t\t\t  struct md_rdev **prrdev)\n{\n\tstruct md_rdev *rdev, *rrdev;\n\n\trrdev = rcu_dereference(mirror->replacement);\n\t \n\tsmp_mb();\n\trdev = rcu_dereference(mirror->rdev);\n\tif (rdev == rrdev)\n\t\trrdev = NULL;\n\n\t*prrdev = rrdev;\n\treturn rdev;\n}\n\nstatic void wait_blocked_dev(struct mddev *mddev, struct r10bio *r10_bio)\n{\n\tint i;\n\tstruct r10conf *conf = mddev->private;\n\tstruct md_rdev *blocked_rdev;\n\nretry_wait:\n\tblocked_rdev = NULL;\n\trcu_read_lock();\n\tfor (i = 0; i < conf->copies; i++) {\n\t\tstruct md_rdev *rdev, *rrdev;\n\n\t\trdev = dereference_rdev_and_rrdev(&conf->mirrors[i], &rrdev);\n\t\tif (rdev && unlikely(test_bit(Blocked, &rdev->flags))) {\n\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\tblocked_rdev = rdev;\n\t\t\tbreak;\n\t\t}\n\t\tif (rrdev && unlikely(test_bit(Blocked, &rrdev->flags))) {\n\t\t\tatomic_inc(&rrdev->nr_pending);\n\t\t\tblocked_rdev = rrdev;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (rdev && test_bit(WriteErrorSeen, &rdev->flags)) {\n\t\t\tsector_t first_bad;\n\t\t\tsector_t dev_sector = r10_bio->devs[i].addr;\n\t\t\tint bad_sectors;\n\t\t\tint is_bad;\n\n\t\t\t \n\t\t\tif (!r10_bio->sectors)\n\t\t\t\tcontinue;\n\n\t\t\tis_bad = is_badblock(rdev, dev_sector, r10_bio->sectors,\n\t\t\t\t\t     &first_bad, &bad_sectors);\n\t\t\tif (is_bad < 0) {\n\t\t\t\t \n\t\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\t\tset_bit(BlockedBadBlocks, &rdev->flags);\n\t\t\t\tblocked_rdev = rdev;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tif (unlikely(blocked_rdev)) {\n\t\t \n\t\tallow_barrier(conf);\n\t\traid10_log(conf->mddev, \"%s wait rdev %d blocked\",\n\t\t\t\t__func__, blocked_rdev->raid_disk);\n\t\tmd_wait_for_blocked_rdev(blocked_rdev, mddev);\n\t\twait_barrier(conf, false);\n\t\tgoto retry_wait;\n\t}\n}\n\nstatic void raid10_write_request(struct mddev *mddev, struct bio *bio,\n\t\t\t\t struct r10bio *r10_bio)\n{\n\tstruct r10conf *conf = mddev->private;\n\tint i;\n\tsector_t sectors;\n\tint max_sectors;\n\n\tif ((mddev_is_clustered(mddev) &&\n\t     md_cluster_ops->area_resyncing(mddev, WRITE,\n\t\t\t\t\t    bio->bi_iter.bi_sector,\n\t\t\t\t\t    bio_end_sector(bio)))) {\n\t\tDEFINE_WAIT(w);\n\t\t \n\t\tif (bio->bi_opf & REQ_NOWAIT) {\n\t\t\tbio_wouldblock_error(bio);\n\t\t\treturn;\n\t\t}\n\t\tfor (;;) {\n\t\t\tprepare_to_wait(&conf->wait_barrier,\n\t\t\t\t\t&w, TASK_IDLE);\n\t\t\tif (!md_cluster_ops->area_resyncing(mddev, WRITE,\n\t\t\t\t bio->bi_iter.bi_sector, bio_end_sector(bio)))\n\t\t\t\tbreak;\n\t\t\tschedule();\n\t\t}\n\t\tfinish_wait(&conf->wait_barrier, &w);\n\t}\n\n\tsectors = r10_bio->sectors;\n\tif (!regular_request_wait(mddev, conf, bio, sectors))\n\t\treturn;\n\tif (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&\n\t    (mddev->reshape_backwards\n\t     ? (bio->bi_iter.bi_sector < conf->reshape_safe &&\n\t\tbio->bi_iter.bi_sector + sectors > conf->reshape_progress)\n\t     : (bio->bi_iter.bi_sector + sectors > conf->reshape_safe &&\n\t\tbio->bi_iter.bi_sector < conf->reshape_progress))) {\n\t\t \n\t\tmddev->reshape_position = conf->reshape_progress;\n\t\tset_mask_bits(&mddev->sb_flags, 0,\n\t\t\t      BIT(MD_SB_CHANGE_DEVS) | BIT(MD_SB_CHANGE_PENDING));\n\t\tmd_wakeup_thread(mddev->thread);\n\t\tif (bio->bi_opf & REQ_NOWAIT) {\n\t\t\tallow_barrier(conf);\n\t\t\tbio_wouldblock_error(bio);\n\t\t\treturn;\n\t\t}\n\t\traid10_log(conf->mddev, \"wait reshape metadata\");\n\t\twait_event(mddev->sb_wait,\n\t\t\t   !test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags));\n\n\t\tconf->reshape_safe = mddev->reshape_position;\n\t}\n\n\t \n\n\tr10_bio->read_slot = -1;  \n\traid10_find_phys(conf, r10_bio);\n\n\twait_blocked_dev(mddev, r10_bio);\n\n\trcu_read_lock();\n\tmax_sectors = r10_bio->sectors;\n\n\tfor (i = 0;  i < conf->copies; i++) {\n\t\tint d = r10_bio->devs[i].devnum;\n\t\tstruct md_rdev *rdev, *rrdev;\n\n\t\trdev = dereference_rdev_and_rrdev(&conf->mirrors[d], &rrdev);\n\t\tif (rdev && (test_bit(Faulty, &rdev->flags)))\n\t\t\trdev = NULL;\n\t\tif (rrdev && (test_bit(Faulty, &rrdev->flags)))\n\t\t\trrdev = NULL;\n\n\t\tr10_bio->devs[i].bio = NULL;\n\t\tr10_bio->devs[i].repl_bio = NULL;\n\n\t\tif (!rdev && !rrdev) {\n\t\t\tset_bit(R10BIO_Degraded, &r10_bio->state);\n\t\t\tcontinue;\n\t\t}\n\t\tif (rdev && test_bit(WriteErrorSeen, &rdev->flags)) {\n\t\t\tsector_t first_bad;\n\t\t\tsector_t dev_sector = r10_bio->devs[i].addr;\n\t\t\tint bad_sectors;\n\t\t\tint is_bad;\n\n\t\t\tis_bad = is_badblock(rdev, dev_sector, max_sectors,\n\t\t\t\t\t     &first_bad, &bad_sectors);\n\t\t\tif (is_bad && first_bad <= dev_sector) {\n\t\t\t\t \n\t\t\t\tbad_sectors -= (dev_sector - first_bad);\n\t\t\t\tif (bad_sectors < max_sectors)\n\t\t\t\t\t \n\t\t\t\t\tmax_sectors = bad_sectors;\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (is_bad) {\n\t\t\t\tint good_sectors = first_bad - dev_sector;\n\t\t\t\tif (good_sectors < max_sectors)\n\t\t\t\t\tmax_sectors = good_sectors;\n\t\t\t}\n\t\t}\n\t\tif (rdev) {\n\t\t\tr10_bio->devs[i].bio = bio;\n\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t}\n\t\tif (rrdev) {\n\t\t\tr10_bio->devs[i].repl_bio = bio;\n\t\t\tatomic_inc(&rrdev->nr_pending);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tif (max_sectors < r10_bio->sectors)\n\t\tr10_bio->sectors = max_sectors;\n\n\tif (r10_bio->sectors < bio_sectors(bio)) {\n\t\tstruct bio *split = bio_split(bio, r10_bio->sectors,\n\t\t\t\t\t      GFP_NOIO, &conf->bio_split);\n\t\tbio_chain(split, bio);\n\t\tallow_barrier(conf);\n\t\tsubmit_bio_noacct(bio);\n\t\twait_barrier(conf, false);\n\t\tbio = split;\n\t\tr10_bio->master_bio = bio;\n\t}\n\n\tmd_account_bio(mddev, &bio);\n\tr10_bio->master_bio = bio;\n\tatomic_set(&r10_bio->remaining, 1);\n\tmd_bitmap_startwrite(mddev->bitmap, r10_bio->sector, r10_bio->sectors, 0);\n\n\tfor (i = 0; i < conf->copies; i++) {\n\t\tif (r10_bio->devs[i].bio)\n\t\t\traid10_write_one_disk(mddev, r10_bio, bio, false, i);\n\t\tif (r10_bio->devs[i].repl_bio)\n\t\t\traid10_write_one_disk(mddev, r10_bio, bio, true, i);\n\t}\n\tone_write_done(r10_bio);\n}\n\nstatic void __make_request(struct mddev *mddev, struct bio *bio, int sectors)\n{\n\tstruct r10conf *conf = mddev->private;\n\tstruct r10bio *r10_bio;\n\n\tr10_bio = mempool_alloc(&conf->r10bio_pool, GFP_NOIO);\n\n\tr10_bio->master_bio = bio;\n\tr10_bio->sectors = sectors;\n\n\tr10_bio->mddev = mddev;\n\tr10_bio->sector = bio->bi_iter.bi_sector;\n\tr10_bio->state = 0;\n\tr10_bio->read_slot = -1;\n\tmemset(r10_bio->devs, 0, sizeof(r10_bio->devs[0]) *\n\t\t\tconf->geo.raid_disks);\n\n\tif (bio_data_dir(bio) == READ)\n\t\traid10_read_request(mddev, bio, r10_bio, true);\n\telse\n\t\traid10_write_request(mddev, bio, r10_bio);\n}\n\nstatic void raid_end_discard_bio(struct r10bio *r10bio)\n{\n\tstruct r10conf *conf = r10bio->mddev->private;\n\tstruct r10bio *first_r10bio;\n\n\twhile (atomic_dec_and_test(&r10bio->remaining)) {\n\n\t\tallow_barrier(conf);\n\n\t\tif (!test_bit(R10BIO_Discard, &r10bio->state)) {\n\t\t\tfirst_r10bio = (struct r10bio *)r10bio->master_bio;\n\t\t\tfree_r10bio(r10bio);\n\t\t\tr10bio = first_r10bio;\n\t\t} else {\n\t\t\tmd_write_end(r10bio->mddev);\n\t\t\tbio_endio(r10bio->master_bio);\n\t\t\tfree_r10bio(r10bio);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void raid10_end_discard_request(struct bio *bio)\n{\n\tstruct r10bio *r10_bio = bio->bi_private;\n\tstruct r10conf *conf = r10_bio->mddev->private;\n\tstruct md_rdev *rdev = NULL;\n\tint dev;\n\tint slot, repl;\n\n\t \n\tif (!test_bit(R10BIO_Uptodate, &r10_bio->state))\n\t\tset_bit(R10BIO_Uptodate, &r10_bio->state);\n\n\tdev = find_bio_disk(conf, r10_bio, bio, &slot, &repl);\n\tif (repl)\n\t\trdev = conf->mirrors[dev].replacement;\n\tif (!rdev) {\n\t\t \n\t\tsmp_rmb();\n\t\trdev = conf->mirrors[dev].rdev;\n\t}\n\n\traid_end_discard_bio(r10_bio);\n\trdev_dec_pending(rdev, conf->mddev);\n}\n\n \nstatic int raid10_handle_discard(struct mddev *mddev, struct bio *bio)\n{\n\tstruct r10conf *conf = mddev->private;\n\tstruct geom *geo = &conf->geo;\n\tint far_copies = geo->far_copies;\n\tbool first_copy = true;\n\tstruct r10bio *r10_bio, *first_r10bio;\n\tstruct bio *split;\n\tint disk;\n\tsector_t chunk;\n\tunsigned int stripe_size;\n\tunsigned int stripe_data_disks;\n\tsector_t split_size;\n\tsector_t bio_start, bio_end;\n\tsector_t first_stripe_index, last_stripe_index;\n\tsector_t start_disk_offset;\n\tunsigned int start_disk_index;\n\tsector_t end_disk_offset;\n\tunsigned int end_disk_index;\n\tunsigned int remainder;\n\n\tif (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery))\n\t\treturn -EAGAIN;\n\n\tif (WARN_ON_ONCE(bio->bi_opf & REQ_NOWAIT)) {\n\t\tbio_wouldblock_error(bio);\n\t\treturn 0;\n\t}\n\twait_barrier(conf, false);\n\n\t \n\tif (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery))\n\t\tgoto out;\n\n\tif (geo->near_copies)\n\t\tstripe_data_disks = geo->raid_disks / geo->near_copies +\n\t\t\t\t\tgeo->raid_disks % geo->near_copies;\n\telse\n\t\tstripe_data_disks = geo->raid_disks;\n\n\tstripe_size = stripe_data_disks << geo->chunk_shift;\n\n\tbio_start = bio->bi_iter.bi_sector;\n\tbio_end = bio_end_sector(bio);\n\n\t \n\tif (bio_sectors(bio) < stripe_size*2)\n\t\tgoto out;\n\n\t \n\tdiv_u64_rem(bio_start, stripe_size, &remainder);\n\tif (remainder) {\n\t\tsplit_size = stripe_size - remainder;\n\t\tsplit = bio_split(bio, split_size, GFP_NOIO, &conf->bio_split);\n\t\tbio_chain(split, bio);\n\t\tallow_barrier(conf);\n\t\t \n\t\tsubmit_bio_noacct(split);\n\t\twait_barrier(conf, false);\n\t}\n\tdiv_u64_rem(bio_end, stripe_size, &remainder);\n\tif (remainder) {\n\t\tsplit_size = bio_sectors(bio) - remainder;\n\t\tsplit = bio_split(bio, split_size, GFP_NOIO, &conf->bio_split);\n\t\tbio_chain(split, bio);\n\t\tallow_barrier(conf);\n\t\t \n\t\tsubmit_bio_noacct(bio);\n\t\tbio = split;\n\t\twait_barrier(conf, false);\n\t}\n\n\tbio_start = bio->bi_iter.bi_sector;\n\tbio_end = bio_end_sector(bio);\n\n\t \n\tchunk = bio_start >> geo->chunk_shift;\n\tchunk *= geo->near_copies;\n\tfirst_stripe_index = chunk;\n\tstart_disk_index = sector_div(first_stripe_index, geo->raid_disks);\n\tif (geo->far_offset)\n\t\tfirst_stripe_index *= geo->far_copies;\n\tstart_disk_offset = (bio_start & geo->chunk_mask) +\n\t\t\t\t(first_stripe_index << geo->chunk_shift);\n\n\tchunk = bio_end >> geo->chunk_shift;\n\tchunk *= geo->near_copies;\n\tlast_stripe_index = chunk;\n\tend_disk_index = sector_div(last_stripe_index, geo->raid_disks);\n\tif (geo->far_offset)\n\t\tlast_stripe_index *= geo->far_copies;\n\tend_disk_offset = (bio_end & geo->chunk_mask) +\n\t\t\t\t(last_stripe_index << geo->chunk_shift);\n\nretry_discard:\n\tr10_bio = mempool_alloc(&conf->r10bio_pool, GFP_NOIO);\n\tr10_bio->mddev = mddev;\n\tr10_bio->state = 0;\n\tr10_bio->sectors = 0;\n\tmemset(r10_bio->devs, 0, sizeof(r10_bio->devs[0]) * geo->raid_disks);\n\twait_blocked_dev(mddev, r10_bio);\n\n\t \n\tif (first_copy) {\n\t\tr10_bio->master_bio = bio;\n\t\tset_bit(R10BIO_Discard, &r10_bio->state);\n\t\tfirst_copy = false;\n\t\tfirst_r10bio = r10_bio;\n\t} else\n\t\tr10_bio->master_bio = (struct bio *)first_r10bio;\n\n\t \n\trcu_read_lock();\n\tfor (disk = 0; disk < geo->raid_disks; disk++) {\n\t\tstruct md_rdev *rdev, *rrdev;\n\n\t\trdev = dereference_rdev_and_rrdev(&conf->mirrors[disk], &rrdev);\n\t\tr10_bio->devs[disk].bio = NULL;\n\t\tr10_bio->devs[disk].repl_bio = NULL;\n\n\t\tif (rdev && (test_bit(Faulty, &rdev->flags)))\n\t\t\trdev = NULL;\n\t\tif (rrdev && (test_bit(Faulty, &rrdev->flags)))\n\t\t\trrdev = NULL;\n\t\tif (!rdev && !rrdev)\n\t\t\tcontinue;\n\n\t\tif (rdev) {\n\t\t\tr10_bio->devs[disk].bio = bio;\n\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t}\n\t\tif (rrdev) {\n\t\t\tr10_bio->devs[disk].repl_bio = bio;\n\t\t\tatomic_inc(&rrdev->nr_pending);\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\tatomic_set(&r10_bio->remaining, 1);\n\tfor (disk = 0; disk < geo->raid_disks; disk++) {\n\t\tsector_t dev_start, dev_end;\n\t\tstruct bio *mbio, *rbio = NULL;\n\n\t\t \n\t\tif (disk < start_disk_index)\n\t\t\tdev_start = (first_stripe_index + 1) * mddev->chunk_sectors;\n\t\telse if (disk > start_disk_index)\n\t\t\tdev_start = first_stripe_index * mddev->chunk_sectors;\n\t\telse\n\t\t\tdev_start = start_disk_offset;\n\n\t\tif (disk < end_disk_index)\n\t\t\tdev_end = (last_stripe_index + 1) * mddev->chunk_sectors;\n\t\telse if (disk > end_disk_index)\n\t\t\tdev_end = last_stripe_index * mddev->chunk_sectors;\n\t\telse\n\t\t\tdev_end = end_disk_offset;\n\n\t\t \n\t\tif (r10_bio->devs[disk].bio) {\n\t\t\tstruct md_rdev *rdev = conf->mirrors[disk].rdev;\n\t\t\tmbio = bio_alloc_clone(bio->bi_bdev, bio, GFP_NOIO,\n\t\t\t\t\t       &mddev->bio_set);\n\t\t\tmbio->bi_end_io = raid10_end_discard_request;\n\t\t\tmbio->bi_private = r10_bio;\n\t\t\tr10_bio->devs[disk].bio = mbio;\n\t\t\tr10_bio->devs[disk].devnum = disk;\n\t\t\tatomic_inc(&r10_bio->remaining);\n\t\t\tmd_submit_discard_bio(mddev, rdev, mbio,\n\t\t\t\t\tdev_start + choose_data_offset(r10_bio, rdev),\n\t\t\t\t\tdev_end - dev_start);\n\t\t\tbio_endio(mbio);\n\t\t}\n\t\tif (r10_bio->devs[disk].repl_bio) {\n\t\t\tstruct md_rdev *rrdev = conf->mirrors[disk].replacement;\n\t\t\trbio = bio_alloc_clone(bio->bi_bdev, bio, GFP_NOIO,\n\t\t\t\t\t       &mddev->bio_set);\n\t\t\trbio->bi_end_io = raid10_end_discard_request;\n\t\t\trbio->bi_private = r10_bio;\n\t\t\tr10_bio->devs[disk].repl_bio = rbio;\n\t\t\tr10_bio->devs[disk].devnum = disk;\n\t\t\tatomic_inc(&r10_bio->remaining);\n\t\t\tmd_submit_discard_bio(mddev, rrdev, rbio,\n\t\t\t\t\tdev_start + choose_data_offset(r10_bio, rrdev),\n\t\t\t\t\tdev_end - dev_start);\n\t\t\tbio_endio(rbio);\n\t\t}\n\t}\n\n\tif (!geo->far_offset && --far_copies) {\n\t\tfirst_stripe_index += geo->stride >> geo->chunk_shift;\n\t\tstart_disk_offset += geo->stride;\n\t\tlast_stripe_index += geo->stride >> geo->chunk_shift;\n\t\tend_disk_offset += geo->stride;\n\t\tatomic_inc(&first_r10bio->remaining);\n\t\traid_end_discard_bio(r10_bio);\n\t\twait_barrier(conf, false);\n\t\tgoto retry_discard;\n\t}\n\n\traid_end_discard_bio(r10_bio);\n\n\treturn 0;\nout:\n\tallow_barrier(conf);\n\treturn -EAGAIN;\n}\n\nstatic bool raid10_make_request(struct mddev *mddev, struct bio *bio)\n{\n\tstruct r10conf *conf = mddev->private;\n\tsector_t chunk_mask = (conf->geo.chunk_mask & conf->prev.chunk_mask);\n\tint chunk_sects = chunk_mask + 1;\n\tint sectors = bio_sectors(bio);\n\n\tif (unlikely(bio->bi_opf & REQ_PREFLUSH)\n\t    && md_flush_request(mddev, bio))\n\t\treturn true;\n\n\tif (!md_write_start(mddev, bio))\n\t\treturn false;\n\n\tif (unlikely(bio_op(bio) == REQ_OP_DISCARD))\n\t\tif (!raid10_handle_discard(mddev, bio))\n\t\t\treturn true;\n\n\t \n\tif (unlikely((bio->bi_iter.bi_sector & chunk_mask) +\n\t\t     sectors > chunk_sects\n\t\t     && (conf->geo.near_copies < conf->geo.raid_disks\n\t\t\t || conf->prev.near_copies <\n\t\t\t conf->prev.raid_disks)))\n\t\tsectors = chunk_sects -\n\t\t\t(bio->bi_iter.bi_sector &\n\t\t\t (chunk_sects - 1));\n\t__make_request(mddev, bio, sectors);\n\n\t \n\twake_up_barrier(conf);\n\treturn true;\n}\n\nstatic void raid10_status(struct seq_file *seq, struct mddev *mddev)\n{\n\tstruct r10conf *conf = mddev->private;\n\tint i;\n\n\tif (conf->geo.near_copies < conf->geo.raid_disks)\n\t\tseq_printf(seq, \" %dK chunks\", mddev->chunk_sectors / 2);\n\tif (conf->geo.near_copies > 1)\n\t\tseq_printf(seq, \" %d near-copies\", conf->geo.near_copies);\n\tif (conf->geo.far_copies > 1) {\n\t\tif (conf->geo.far_offset)\n\t\t\tseq_printf(seq, \" %d offset-copies\", conf->geo.far_copies);\n\t\telse\n\t\t\tseq_printf(seq, \" %d far-copies\", conf->geo.far_copies);\n\t\tif (conf->geo.far_set_size != conf->geo.raid_disks)\n\t\t\tseq_printf(seq, \" %d devices per set\", conf->geo.far_set_size);\n\t}\n\tseq_printf(seq, \" [%d/%d] [\", conf->geo.raid_disks,\n\t\t\t\t\tconf->geo.raid_disks - mddev->degraded);\n\trcu_read_lock();\n\tfor (i = 0; i < conf->geo.raid_disks; i++) {\n\t\tstruct md_rdev *rdev = rcu_dereference(conf->mirrors[i].rdev);\n\t\tseq_printf(seq, \"%s\", rdev && test_bit(In_sync, &rdev->flags) ? \"U\" : \"_\");\n\t}\n\trcu_read_unlock();\n\tseq_printf(seq, \"]\");\n}\n\n \nstatic int _enough(struct r10conf *conf, int previous, int ignore)\n{\n\tint first = 0;\n\tint has_enough = 0;\n\tint disks, ncopies;\n\tif (previous) {\n\t\tdisks = conf->prev.raid_disks;\n\t\tncopies = conf->prev.near_copies;\n\t} else {\n\t\tdisks = conf->geo.raid_disks;\n\t\tncopies = conf->geo.near_copies;\n\t}\n\n\trcu_read_lock();\n\tdo {\n\t\tint n = conf->copies;\n\t\tint cnt = 0;\n\t\tint this = first;\n\t\twhile (n--) {\n\t\t\tstruct md_rdev *rdev;\n\t\t\tif (this != ignore &&\n\t\t\t    (rdev = rcu_dereference(conf->mirrors[this].rdev)) &&\n\t\t\t    test_bit(In_sync, &rdev->flags))\n\t\t\t\tcnt++;\n\t\t\tthis = (this+1) % disks;\n\t\t}\n\t\tif (cnt == 0)\n\t\t\tgoto out;\n\t\tfirst = (first + ncopies) % disks;\n\t} while (first != 0);\n\thas_enough = 1;\nout:\n\trcu_read_unlock();\n\treturn has_enough;\n}\n\nstatic int enough(struct r10conf *conf, int ignore)\n{\n\t \n\treturn _enough(conf, 0, ignore) &&\n\t\t_enough(conf, 1, ignore);\n}\n\n \nstatic void raid10_error(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tstruct r10conf *conf = mddev->private;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&conf->device_lock, flags);\n\n\tif (test_bit(In_sync, &rdev->flags) && !enough(conf, rdev->raid_disk)) {\n\t\tset_bit(MD_BROKEN, &mddev->flags);\n\n\t\tif (!mddev->fail_last_dev) {\n\t\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\t\t\treturn;\n\t\t}\n\t}\n\tif (test_and_clear_bit(In_sync, &rdev->flags))\n\t\tmddev->degraded++;\n\n\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\tset_bit(Blocked, &rdev->flags);\n\tset_bit(Faulty, &rdev->flags);\n\tset_mask_bits(&mddev->sb_flags, 0,\n\t\t      BIT(MD_SB_CHANGE_DEVS) | BIT(MD_SB_CHANGE_PENDING));\n\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\tpr_crit(\"md/raid10:%s: Disk failure on %pg, disabling device.\\n\"\n\t\t\"md/raid10:%s: Operation continuing on %d devices.\\n\",\n\t\tmdname(mddev), rdev->bdev,\n\t\tmdname(mddev), conf->geo.raid_disks - mddev->degraded);\n}\n\nstatic void print_conf(struct r10conf *conf)\n{\n\tint i;\n\tstruct md_rdev *rdev;\n\n\tpr_debug(\"RAID10 conf printout:\\n\");\n\tif (!conf) {\n\t\tpr_debug(\"(!conf)\\n\");\n\t\treturn;\n\t}\n\tpr_debug(\" --- wd:%d rd:%d\\n\", conf->geo.raid_disks - conf->mddev->degraded,\n\t\t conf->geo.raid_disks);\n\n\t \n\tfor (i = 0; i < conf->geo.raid_disks; i++) {\n\t\trdev = conf->mirrors[i].rdev;\n\t\tif (rdev)\n\t\t\tpr_debug(\" disk %d, wo:%d, o:%d, dev:%pg\\n\",\n\t\t\t\t i, !test_bit(In_sync, &rdev->flags),\n\t\t\t\t !test_bit(Faulty, &rdev->flags),\n\t\t\t\t rdev->bdev);\n\t}\n}\n\nstatic void close_sync(struct r10conf *conf)\n{\n\twait_barrier(conf, false);\n\tallow_barrier(conf);\n\n\tmempool_exit(&conf->r10buf_pool);\n}\n\nstatic int raid10_spare_active(struct mddev *mddev)\n{\n\tint i;\n\tstruct r10conf *conf = mddev->private;\n\tstruct raid10_info *tmp;\n\tint count = 0;\n\tunsigned long flags;\n\n\t \n\tfor (i = 0; i < conf->geo.raid_disks; i++) {\n\t\ttmp = conf->mirrors + i;\n\t\tif (tmp->replacement\n\t\t    && tmp->replacement->recovery_offset == MaxSector\n\t\t    && !test_bit(Faulty, &tmp->replacement->flags)\n\t\t    && !test_and_set_bit(In_sync, &tmp->replacement->flags)) {\n\t\t\t \n\t\t\tif (!tmp->rdev\n\t\t\t    || !test_and_clear_bit(In_sync, &tmp->rdev->flags))\n\t\t\t\tcount++;\n\t\t\tif (tmp->rdev) {\n\t\t\t\t \n\t\t\t\tset_bit(Faulty, &tmp->rdev->flags);\n\t\t\t\tsysfs_notify_dirent_safe(\n\t\t\t\t\ttmp->rdev->sysfs_state);\n\t\t\t}\n\t\t\tsysfs_notify_dirent_safe(tmp->replacement->sysfs_state);\n\t\t} else if (tmp->rdev\n\t\t\t   && tmp->rdev->recovery_offset == MaxSector\n\t\t\t   && !test_bit(Faulty, &tmp->rdev->flags)\n\t\t\t   && !test_and_set_bit(In_sync, &tmp->rdev->flags)) {\n\t\t\tcount++;\n\t\t\tsysfs_notify_dirent_safe(tmp->rdev->sysfs_state);\n\t\t}\n\t}\n\tspin_lock_irqsave(&conf->device_lock, flags);\n\tmddev->degraded -= count;\n\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\n\tprint_conf(conf);\n\treturn count;\n}\n\nstatic int raid10_add_disk(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tstruct r10conf *conf = mddev->private;\n\tint err = -EEXIST;\n\tint mirror, repl_slot = -1;\n\tint first = 0;\n\tint last = conf->geo.raid_disks - 1;\n\tstruct raid10_info *p;\n\n\tif (mddev->recovery_cp < MaxSector)\n\t\t \n\t\treturn -EBUSY;\n\tif (rdev->saved_raid_disk < 0 && !_enough(conf, 1, -1))\n\t\treturn -EINVAL;\n\n\tif (md_integrity_add_rdev(rdev, mddev))\n\t\treturn -ENXIO;\n\n\tif (rdev->raid_disk >= 0)\n\t\tfirst = last = rdev->raid_disk;\n\n\tif (rdev->saved_raid_disk >= first &&\n\t    rdev->saved_raid_disk < conf->geo.raid_disks &&\n\t    conf->mirrors[rdev->saved_raid_disk].rdev == NULL)\n\t\tmirror = rdev->saved_raid_disk;\n\telse\n\t\tmirror = first;\n\tfor ( ; mirror <= last ; mirror++) {\n\t\tp = &conf->mirrors[mirror];\n\t\tif (p->recovery_disabled == mddev->recovery_disabled)\n\t\t\tcontinue;\n\t\tif (p->rdev) {\n\t\t\tif (test_bit(WantReplacement, &p->rdev->flags) &&\n\t\t\t    p->replacement == NULL && repl_slot < 0)\n\t\t\t\trepl_slot = mirror;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (mddev->gendisk)\n\t\t\tdisk_stack_limits(mddev->gendisk, rdev->bdev,\n\t\t\t\t\t  rdev->data_offset << 9);\n\n\t\tp->head_position = 0;\n\t\tp->recovery_disabled = mddev->recovery_disabled - 1;\n\t\trdev->raid_disk = mirror;\n\t\terr = 0;\n\t\tif (rdev->saved_raid_disk != mirror)\n\t\t\tconf->fullsync = 1;\n\t\trcu_assign_pointer(p->rdev, rdev);\n\t\tbreak;\n\t}\n\n\tif (err && repl_slot >= 0) {\n\t\tp = &conf->mirrors[repl_slot];\n\t\tclear_bit(In_sync, &rdev->flags);\n\t\tset_bit(Replacement, &rdev->flags);\n\t\trdev->raid_disk = repl_slot;\n\t\terr = 0;\n\t\tif (mddev->gendisk)\n\t\t\tdisk_stack_limits(mddev->gendisk, rdev->bdev,\n\t\t\t\t\t  rdev->data_offset << 9);\n\t\tconf->fullsync = 1;\n\t\trcu_assign_pointer(p->replacement, rdev);\n\t}\n\n\tprint_conf(conf);\n\treturn err;\n}\n\nstatic int raid10_remove_disk(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tstruct r10conf *conf = mddev->private;\n\tint err = 0;\n\tint number = rdev->raid_disk;\n\tstruct md_rdev **rdevp;\n\tstruct raid10_info *p;\n\n\tprint_conf(conf);\n\tif (unlikely(number >= mddev->raid_disks))\n\t\treturn 0;\n\tp = conf->mirrors + number;\n\tif (rdev == p->rdev)\n\t\trdevp = &p->rdev;\n\telse if (rdev == p->replacement)\n\t\trdevp = &p->replacement;\n\telse\n\t\treturn 0;\n\n\tif (test_bit(In_sync, &rdev->flags) ||\n\t    atomic_read(&rdev->nr_pending)) {\n\t\terr = -EBUSY;\n\t\tgoto abort;\n\t}\n\t \n\tif (!test_bit(Faulty, &rdev->flags) &&\n\t    mddev->recovery_disabled != p->recovery_disabled &&\n\t    (!p->replacement || p->replacement == rdev) &&\n\t    number < conf->geo.raid_disks &&\n\t    enough(conf, -1)) {\n\t\terr = -EBUSY;\n\t\tgoto abort;\n\t}\n\t*rdevp = NULL;\n\tif (!test_bit(RemoveSynchronized, &rdev->flags)) {\n\t\tsynchronize_rcu();\n\t\tif (atomic_read(&rdev->nr_pending)) {\n\t\t\t \n\t\t\terr = -EBUSY;\n\t\t\t*rdevp = rdev;\n\t\t\tgoto abort;\n\t\t}\n\t}\n\tif (p->replacement) {\n\t\t \n\t\tp->rdev = p->replacement;\n\t\tclear_bit(Replacement, &p->replacement->flags);\n\t\tsmp_mb();  \n\t\tp->replacement = NULL;\n\t}\n\n\tclear_bit(WantReplacement, &rdev->flags);\n\terr = md_integrity_register(mddev);\n\nabort:\n\n\tprint_conf(conf);\n\treturn err;\n}\n\nstatic void __end_sync_read(struct r10bio *r10_bio, struct bio *bio, int d)\n{\n\tstruct r10conf *conf = r10_bio->mddev->private;\n\n\tif (!bio->bi_status)\n\t\tset_bit(R10BIO_Uptodate, &r10_bio->state);\n\telse\n\t\t \n\t\tatomic_add(r10_bio->sectors,\n\t\t\t   &conf->mirrors[d].rdev->corrected_errors);\n\n\t \n\trdev_dec_pending(conf->mirrors[d].rdev, conf->mddev);\n\tif (test_bit(R10BIO_IsRecover, &r10_bio->state) ||\n\t    atomic_dec_and_test(&r10_bio->remaining)) {\n\t\t \n\t\treschedule_retry(r10_bio);\n\t}\n}\n\nstatic void end_sync_read(struct bio *bio)\n{\n\tstruct r10bio *r10_bio = get_resync_r10bio(bio);\n\tstruct r10conf *conf = r10_bio->mddev->private;\n\tint d = find_bio_disk(conf, r10_bio, bio, NULL, NULL);\n\n\t__end_sync_read(r10_bio, bio, d);\n}\n\nstatic void end_reshape_read(struct bio *bio)\n{\n\t \n\tstruct r10bio *r10_bio = bio->bi_private;\n\n\t__end_sync_read(r10_bio, bio, r10_bio->read_slot);\n}\n\nstatic void end_sync_request(struct r10bio *r10_bio)\n{\n\tstruct mddev *mddev = r10_bio->mddev;\n\n\twhile (atomic_dec_and_test(&r10_bio->remaining)) {\n\t\tif (r10_bio->master_bio == NULL) {\n\t\t\t \n\t\t\tsector_t s = r10_bio->sectors;\n\t\t\tif (test_bit(R10BIO_MadeGood, &r10_bio->state) ||\n\t\t\t    test_bit(R10BIO_WriteError, &r10_bio->state))\n\t\t\t\treschedule_retry(r10_bio);\n\t\t\telse\n\t\t\t\tput_buf(r10_bio);\n\t\t\tmd_done_sync(mddev, s, 1);\n\t\t\tbreak;\n\t\t} else {\n\t\t\tstruct r10bio *r10_bio2 = (struct r10bio *)r10_bio->master_bio;\n\t\t\tif (test_bit(R10BIO_MadeGood, &r10_bio->state) ||\n\t\t\t    test_bit(R10BIO_WriteError, &r10_bio->state))\n\t\t\t\treschedule_retry(r10_bio);\n\t\t\telse\n\t\t\t\tput_buf(r10_bio);\n\t\t\tr10_bio = r10_bio2;\n\t\t}\n\t}\n}\n\nstatic void end_sync_write(struct bio *bio)\n{\n\tstruct r10bio *r10_bio = get_resync_r10bio(bio);\n\tstruct mddev *mddev = r10_bio->mddev;\n\tstruct r10conf *conf = mddev->private;\n\tint d;\n\tsector_t first_bad;\n\tint bad_sectors;\n\tint slot;\n\tint repl;\n\tstruct md_rdev *rdev = NULL;\n\n\td = find_bio_disk(conf, r10_bio, bio, &slot, &repl);\n\tif (repl)\n\t\trdev = conf->mirrors[d].replacement;\n\telse\n\t\trdev = conf->mirrors[d].rdev;\n\n\tif (bio->bi_status) {\n\t\tif (repl)\n\t\t\tmd_error(mddev, rdev);\n\t\telse {\n\t\t\tset_bit(WriteErrorSeen, &rdev->flags);\n\t\t\tif (!test_and_set_bit(WantReplacement, &rdev->flags))\n\t\t\t\tset_bit(MD_RECOVERY_NEEDED,\n\t\t\t\t\t&rdev->mddev->recovery);\n\t\t\tset_bit(R10BIO_WriteError, &r10_bio->state);\n\t\t}\n\t} else if (is_badblock(rdev,\n\t\t\t     r10_bio->devs[slot].addr,\n\t\t\t     r10_bio->sectors,\n\t\t\t     &first_bad, &bad_sectors))\n\t\tset_bit(R10BIO_MadeGood, &r10_bio->state);\n\n\trdev_dec_pending(rdev, mddev);\n\n\tend_sync_request(r10_bio);\n}\n\n \n \nstatic void sync_request_write(struct mddev *mddev, struct r10bio *r10_bio)\n{\n\tstruct r10conf *conf = mddev->private;\n\tint i, first;\n\tstruct bio *tbio, *fbio;\n\tint vcnt;\n\tstruct page **tpages, **fpages;\n\n\tatomic_set(&r10_bio->remaining, 1);\n\n\t \n\tfor (i=0; i<conf->copies; i++)\n\t\tif (!r10_bio->devs[i].bio->bi_status)\n\t\t\tbreak;\n\n\tif (i == conf->copies)\n\t\tgoto done;\n\n\tfirst = i;\n\tfbio = r10_bio->devs[i].bio;\n\tfbio->bi_iter.bi_size = r10_bio->sectors << 9;\n\tfbio->bi_iter.bi_idx = 0;\n\tfpages = get_resync_pages(fbio)->pages;\n\n\tvcnt = (r10_bio->sectors + (PAGE_SIZE >> 9) - 1) >> (PAGE_SHIFT - 9);\n\t \n\tfor (i=0 ; i < conf->copies ; i++) {\n\t\tint  j, d;\n\t\tstruct md_rdev *rdev;\n\t\tstruct resync_pages *rp;\n\n\t\ttbio = r10_bio->devs[i].bio;\n\n\t\tif (tbio->bi_end_io != end_sync_read)\n\t\t\tcontinue;\n\t\tif (i == first)\n\t\t\tcontinue;\n\n\t\ttpages = get_resync_pages(tbio)->pages;\n\t\td = r10_bio->devs[i].devnum;\n\t\trdev = conf->mirrors[d].rdev;\n\t\tif (!r10_bio->devs[i].bio->bi_status) {\n\t\t\t \n\t\t\tint sectors = r10_bio->sectors;\n\t\t\tfor (j = 0; j < vcnt; j++) {\n\t\t\t\tint len = PAGE_SIZE;\n\t\t\t\tif (sectors < (len / 512))\n\t\t\t\t\tlen = sectors * 512;\n\t\t\t\tif (memcmp(page_address(fpages[j]),\n\t\t\t\t\t   page_address(tpages[j]),\n\t\t\t\t\t   len))\n\t\t\t\t\tbreak;\n\t\t\t\tsectors -= len/512;\n\t\t\t}\n\t\t\tif (j == vcnt)\n\t\t\t\tcontinue;\n\t\t\tatomic64_add(r10_bio->sectors, &mddev->resync_mismatches);\n\t\t\tif (test_bit(MD_RECOVERY_CHECK, &mddev->recovery))\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t} else if (test_bit(FailFast, &rdev->flags)) {\n\t\t\t \n\t\t\tmd_error(rdev->mddev, rdev);\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\trp = get_resync_pages(tbio);\n\t\tbio_reset(tbio, conf->mirrors[d].rdev->bdev, REQ_OP_WRITE);\n\n\t\tmd_bio_reset_resync_pages(tbio, rp, fbio->bi_iter.bi_size);\n\n\t\trp->raid_bio = r10_bio;\n\t\ttbio->bi_private = rp;\n\t\ttbio->bi_iter.bi_sector = r10_bio->devs[i].addr;\n\t\ttbio->bi_end_io = end_sync_write;\n\n\t\tbio_copy_data(tbio, fbio);\n\n\t\tatomic_inc(&conf->mirrors[d].rdev->nr_pending);\n\t\tatomic_inc(&r10_bio->remaining);\n\t\tmd_sync_acct(conf->mirrors[d].rdev->bdev, bio_sectors(tbio));\n\n\t\tif (test_bit(FailFast, &conf->mirrors[d].rdev->flags))\n\t\t\ttbio->bi_opf |= MD_FAILFAST;\n\t\ttbio->bi_iter.bi_sector += conf->mirrors[d].rdev->data_offset;\n\t\tsubmit_bio_noacct(tbio);\n\t}\n\n\t \n\tfor (i = 0; i < conf->copies; i++) {\n\t\tint d;\n\n\t\ttbio = r10_bio->devs[i].repl_bio;\n\t\tif (!tbio || !tbio->bi_end_io)\n\t\t\tcontinue;\n\t\tif (r10_bio->devs[i].bio->bi_end_io != end_sync_write\n\t\t    && r10_bio->devs[i].bio != fbio)\n\t\t\tbio_copy_data(tbio, fbio);\n\t\td = r10_bio->devs[i].devnum;\n\t\tatomic_inc(&r10_bio->remaining);\n\t\tmd_sync_acct(conf->mirrors[d].replacement->bdev,\n\t\t\t     bio_sectors(tbio));\n\t\tsubmit_bio_noacct(tbio);\n\t}\n\ndone:\n\tif (atomic_dec_and_test(&r10_bio->remaining)) {\n\t\tmd_done_sync(mddev, r10_bio->sectors, 1);\n\t\tput_buf(r10_bio);\n\t}\n}\n\n \nstatic void fix_recovery_read_error(struct r10bio *r10_bio)\n{\n\t \n\tstruct mddev *mddev = r10_bio->mddev;\n\tstruct r10conf *conf = mddev->private;\n\tstruct bio *bio = r10_bio->devs[0].bio;\n\tsector_t sect = 0;\n\tint sectors = r10_bio->sectors;\n\tint idx = 0;\n\tint dr = r10_bio->devs[0].devnum;\n\tint dw = r10_bio->devs[1].devnum;\n\tstruct page **pages = get_resync_pages(bio)->pages;\n\n\twhile (sectors) {\n\t\tint s = sectors;\n\t\tstruct md_rdev *rdev;\n\t\tsector_t addr;\n\t\tint ok;\n\n\t\tif (s > (PAGE_SIZE>>9))\n\t\t\ts = PAGE_SIZE >> 9;\n\n\t\trdev = conf->mirrors[dr].rdev;\n\t\taddr = r10_bio->devs[0].addr + sect,\n\t\tok = sync_page_io(rdev,\n\t\t\t\t  addr,\n\t\t\t\t  s << 9,\n\t\t\t\t  pages[idx],\n\t\t\t\t  REQ_OP_READ, false);\n\t\tif (ok) {\n\t\t\trdev = conf->mirrors[dw].rdev;\n\t\t\taddr = r10_bio->devs[1].addr + sect;\n\t\t\tok = sync_page_io(rdev,\n\t\t\t\t\t  addr,\n\t\t\t\t\t  s << 9,\n\t\t\t\t\t  pages[idx],\n\t\t\t\t\t  REQ_OP_WRITE, false);\n\t\t\tif (!ok) {\n\t\t\t\tset_bit(WriteErrorSeen, &rdev->flags);\n\t\t\t\tif (!test_and_set_bit(WantReplacement,\n\t\t\t\t\t\t      &rdev->flags))\n\t\t\t\t\tset_bit(MD_RECOVERY_NEEDED,\n\t\t\t\t\t\t&rdev->mddev->recovery);\n\t\t\t}\n\t\t}\n\t\tif (!ok) {\n\t\t\t \n\t\t\trdev_set_badblocks(rdev, addr, s, 0);\n\n\t\t\tif (rdev != conf->mirrors[dw].rdev) {\n\t\t\t\t \n\t\t\t\tstruct md_rdev *rdev2 = conf->mirrors[dw].rdev;\n\t\t\t\taddr = r10_bio->devs[1].addr + sect;\n\t\t\t\tok = rdev_set_badblocks(rdev2, addr, s, 0);\n\t\t\t\tif (!ok) {\n\t\t\t\t\t \n\t\t\t\t\tpr_notice(\"md/raid10:%s: recovery aborted due to read error\\n\",\n\t\t\t\t\t\t  mdname(mddev));\n\n\t\t\t\t\tconf->mirrors[dw].recovery_disabled\n\t\t\t\t\t\t= mddev->recovery_disabled;\n\t\t\t\t\tset_bit(MD_RECOVERY_INTR,\n\t\t\t\t\t\t&mddev->recovery);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tsectors -= s;\n\t\tsect += s;\n\t\tidx++;\n\t}\n}\n\nstatic void recovery_request_write(struct mddev *mddev, struct r10bio *r10_bio)\n{\n\tstruct r10conf *conf = mddev->private;\n\tint d;\n\tstruct bio *wbio = r10_bio->devs[1].bio;\n\tstruct bio *wbio2 = r10_bio->devs[1].repl_bio;\n\n\t \n\tif (wbio2 && !wbio2->bi_end_io)\n\t\twbio2 = NULL;\n\n\tif (!test_bit(R10BIO_Uptodate, &r10_bio->state)) {\n\t\tfix_recovery_read_error(r10_bio);\n\t\tif (wbio->bi_end_io)\n\t\t\tend_sync_request(r10_bio);\n\t\tif (wbio2)\n\t\t\tend_sync_request(r10_bio);\n\t\treturn;\n\t}\n\n\t \n\td = r10_bio->devs[1].devnum;\n\tif (wbio->bi_end_io) {\n\t\tatomic_inc(&conf->mirrors[d].rdev->nr_pending);\n\t\tmd_sync_acct(conf->mirrors[d].rdev->bdev, bio_sectors(wbio));\n\t\tsubmit_bio_noacct(wbio);\n\t}\n\tif (wbio2) {\n\t\tatomic_inc(&conf->mirrors[d].replacement->nr_pending);\n\t\tmd_sync_acct(conf->mirrors[d].replacement->bdev,\n\t\t\t     bio_sectors(wbio2));\n\t\tsubmit_bio_noacct(wbio2);\n\t}\n}\n\n \nstatic void check_decay_read_errors(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tlong cur_time_mon;\n\tunsigned long hours_since_last;\n\tunsigned int read_errors = atomic_read(&rdev->read_errors);\n\n\tcur_time_mon = ktime_get_seconds();\n\n\tif (rdev->last_read_error == 0) {\n\t\t \n\t\trdev->last_read_error = cur_time_mon;\n\t\treturn;\n\t}\n\n\thours_since_last = (long)(cur_time_mon -\n\t\t\t    rdev->last_read_error) / 3600;\n\n\trdev->last_read_error = cur_time_mon;\n\n\t \n\tif (hours_since_last >= 8 * sizeof(read_errors))\n\t\tatomic_set(&rdev->read_errors, 0);\n\telse\n\t\tatomic_set(&rdev->read_errors, read_errors >> hours_since_last);\n}\n\nstatic int r10_sync_page_io(struct md_rdev *rdev, sector_t sector,\n\t\t\t    int sectors, struct page *page, enum req_op op)\n{\n\tsector_t first_bad;\n\tint bad_sectors;\n\n\tif (is_badblock(rdev, sector, sectors, &first_bad, &bad_sectors)\n\t    && (op == REQ_OP_READ || test_bit(WriteErrorSeen, &rdev->flags)))\n\t\treturn -1;\n\tif (sync_page_io(rdev, sector, sectors << 9, page, op, false))\n\t\t \n\t\treturn 1;\n\tif (op == REQ_OP_WRITE) {\n\t\tset_bit(WriteErrorSeen, &rdev->flags);\n\t\tif (!test_and_set_bit(WantReplacement, &rdev->flags))\n\t\t\tset_bit(MD_RECOVERY_NEEDED,\n\t\t\t\t&rdev->mddev->recovery);\n\t}\n\t \n\tif (!rdev_set_badblocks(rdev, sector, sectors, 0))\n\t\tmd_error(rdev->mddev, rdev);\n\treturn 0;\n}\n\n \n\nstatic void fix_read_error(struct r10conf *conf, struct mddev *mddev, struct r10bio *r10_bio)\n{\n\tint sect = 0;  \n\tint sectors = r10_bio->sectors, slot = r10_bio->read_slot;\n\tstruct md_rdev *rdev;\n\tint max_read_errors = atomic_read(&mddev->max_corr_read_errors);\n\tint d = r10_bio->devs[slot].devnum;\n\n\t \n\trdev = conf->mirrors[d].rdev;\n\n\tif (test_bit(Faulty, &rdev->flags))\n\t\t \n\t\treturn;\n\n\tcheck_decay_read_errors(mddev, rdev);\n\tatomic_inc(&rdev->read_errors);\n\tif (atomic_read(&rdev->read_errors) > max_read_errors) {\n\t\tpr_notice(\"md/raid10:%s: %pg: Raid device exceeded read_error threshold [cur %d:max %d]\\n\",\n\t\t\t  mdname(mddev), rdev->bdev,\n\t\t\t  atomic_read(&rdev->read_errors), max_read_errors);\n\t\tpr_notice(\"md/raid10:%s: %pg: Failing raid device\\n\",\n\t\t\t  mdname(mddev), rdev->bdev);\n\t\tmd_error(mddev, rdev);\n\t\tr10_bio->devs[slot].bio = IO_BLOCKED;\n\t\treturn;\n\t}\n\n\twhile(sectors) {\n\t\tint s = sectors;\n\t\tint sl = slot;\n\t\tint success = 0;\n\t\tint start;\n\n\t\tif (s > (PAGE_SIZE>>9))\n\t\t\ts = PAGE_SIZE >> 9;\n\n\t\trcu_read_lock();\n\t\tdo {\n\t\t\tsector_t first_bad;\n\t\t\tint bad_sectors;\n\n\t\t\td = r10_bio->devs[sl].devnum;\n\t\t\trdev = rcu_dereference(conf->mirrors[d].rdev);\n\t\t\tif (rdev &&\n\t\t\t    test_bit(In_sync, &rdev->flags) &&\n\t\t\t    !test_bit(Faulty, &rdev->flags) &&\n\t\t\t    is_badblock(rdev, r10_bio->devs[sl].addr + sect, s,\n\t\t\t\t\t&first_bad, &bad_sectors) == 0) {\n\t\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\t\trcu_read_unlock();\n\t\t\t\tsuccess = sync_page_io(rdev,\n\t\t\t\t\t\t       r10_bio->devs[sl].addr +\n\t\t\t\t\t\t       sect,\n\t\t\t\t\t\t       s<<9,\n\t\t\t\t\t\t       conf->tmppage,\n\t\t\t\t\t\t       REQ_OP_READ, false);\n\t\t\t\trdev_dec_pending(rdev, mddev);\n\t\t\t\trcu_read_lock();\n\t\t\t\tif (success)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tsl++;\n\t\t\tif (sl == conf->copies)\n\t\t\t\tsl = 0;\n\t\t} while (sl != slot);\n\t\trcu_read_unlock();\n\n\t\tif (!success) {\n\t\t\t \n\t\t\tint dn = r10_bio->devs[slot].devnum;\n\t\t\trdev = conf->mirrors[dn].rdev;\n\n\t\t\tif (!rdev_set_badblocks(\n\t\t\t\t    rdev,\n\t\t\t\t    r10_bio->devs[slot].addr\n\t\t\t\t    + sect,\n\t\t\t\t    s, 0)) {\n\t\t\t\tmd_error(mddev, rdev);\n\t\t\t\tr10_bio->devs[slot].bio\n\t\t\t\t\t= IO_BLOCKED;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tstart = sl;\n\t\t \n\t\trcu_read_lock();\n\t\twhile (sl != slot) {\n\t\t\tif (sl==0)\n\t\t\t\tsl = conf->copies;\n\t\t\tsl--;\n\t\t\td = r10_bio->devs[sl].devnum;\n\t\t\trdev = rcu_dereference(conf->mirrors[d].rdev);\n\t\t\tif (!rdev ||\n\t\t\t    test_bit(Faulty, &rdev->flags) ||\n\t\t\t    !test_bit(In_sync, &rdev->flags))\n\t\t\t\tcontinue;\n\n\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\trcu_read_unlock();\n\t\t\tif (r10_sync_page_io(rdev,\n\t\t\t\t\t     r10_bio->devs[sl].addr +\n\t\t\t\t\t     sect,\n\t\t\t\t\t     s, conf->tmppage, REQ_OP_WRITE)\n\t\t\t    == 0) {\n\t\t\t\t \n\t\t\t\tpr_notice(\"md/raid10:%s: read correction write failed (%d sectors at %llu on %pg)\\n\",\n\t\t\t\t\t  mdname(mddev), s,\n\t\t\t\t\t  (unsigned long long)(\n\t\t\t\t\t\t  sect +\n\t\t\t\t\t\t  choose_data_offset(r10_bio,\n\t\t\t\t\t\t\t\t     rdev)),\n\t\t\t\t\t  rdev->bdev);\n\t\t\t\tpr_notice(\"md/raid10:%s: %pg: failing drive\\n\",\n\t\t\t\t\t  mdname(mddev),\n\t\t\t\t\t  rdev->bdev);\n\t\t\t}\n\t\t\trdev_dec_pending(rdev, mddev);\n\t\t\trcu_read_lock();\n\t\t}\n\t\tsl = start;\n\t\twhile (sl != slot) {\n\t\t\tif (sl==0)\n\t\t\t\tsl = conf->copies;\n\t\t\tsl--;\n\t\t\td = r10_bio->devs[sl].devnum;\n\t\t\trdev = rcu_dereference(conf->mirrors[d].rdev);\n\t\t\tif (!rdev ||\n\t\t\t    test_bit(Faulty, &rdev->flags) ||\n\t\t\t    !test_bit(In_sync, &rdev->flags))\n\t\t\t\tcontinue;\n\n\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\trcu_read_unlock();\n\t\t\tswitch (r10_sync_page_io(rdev,\n\t\t\t\t\t     r10_bio->devs[sl].addr +\n\t\t\t\t\t     sect,\n\t\t\t\t\t     s, conf->tmppage, REQ_OP_READ)) {\n\t\t\tcase 0:\n\t\t\t\t \n\t\t\t\tpr_notice(\"md/raid10:%s: unable to read back corrected sectors (%d sectors at %llu on %pg)\\n\",\n\t\t\t\t       mdname(mddev), s,\n\t\t\t\t       (unsigned long long)(\n\t\t\t\t\t       sect +\n\t\t\t\t\t       choose_data_offset(r10_bio, rdev)),\n\t\t\t\t       rdev->bdev);\n\t\t\t\tpr_notice(\"md/raid10:%s: %pg: failing drive\\n\",\n\t\t\t\t       mdname(mddev),\n\t\t\t\t       rdev->bdev);\n\t\t\t\tbreak;\n\t\t\tcase 1:\n\t\t\t\tpr_info(\"md/raid10:%s: read error corrected (%d sectors at %llu on %pg)\\n\",\n\t\t\t\t       mdname(mddev), s,\n\t\t\t\t       (unsigned long long)(\n\t\t\t\t\t       sect +\n\t\t\t\t\t       choose_data_offset(r10_bio, rdev)),\n\t\t\t\t       rdev->bdev);\n\t\t\t\tatomic_add(s, &rdev->corrected_errors);\n\t\t\t}\n\n\t\t\trdev_dec_pending(rdev, mddev);\n\t\t\trcu_read_lock();\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tsectors -= s;\n\t\tsect += s;\n\t}\n}\n\nstatic int narrow_write_error(struct r10bio *r10_bio, int i)\n{\n\tstruct bio *bio = r10_bio->master_bio;\n\tstruct mddev *mddev = r10_bio->mddev;\n\tstruct r10conf *conf = mddev->private;\n\tstruct md_rdev *rdev = conf->mirrors[r10_bio->devs[i].devnum].rdev;\n\t \n\n\tint block_sectors;\n\tsector_t sector;\n\tint sectors;\n\tint sect_to_write = r10_bio->sectors;\n\tint ok = 1;\n\n\tif (rdev->badblocks.shift < 0)\n\t\treturn 0;\n\n\tblock_sectors = roundup(1 << rdev->badblocks.shift,\n\t\t\t\tbdev_logical_block_size(rdev->bdev) >> 9);\n\tsector = r10_bio->sector;\n\tsectors = ((r10_bio->sector + block_sectors)\n\t\t   & ~(sector_t)(block_sectors - 1))\n\t\t- sector;\n\n\twhile (sect_to_write) {\n\t\tstruct bio *wbio;\n\t\tsector_t wsector;\n\t\tif (sectors > sect_to_write)\n\t\t\tsectors = sect_to_write;\n\t\t \n\t\twbio = bio_alloc_clone(rdev->bdev, bio, GFP_NOIO,\n\t\t\t\t       &mddev->bio_set);\n\t\tbio_trim(wbio, sector - bio->bi_iter.bi_sector, sectors);\n\t\twsector = r10_bio->devs[i].addr + (sector - r10_bio->sector);\n\t\twbio->bi_iter.bi_sector = wsector +\n\t\t\t\t   choose_data_offset(r10_bio, rdev);\n\t\twbio->bi_opf = REQ_OP_WRITE;\n\n\t\tif (submit_bio_wait(wbio) < 0)\n\t\t\t \n\t\t\tok = rdev_set_badblocks(rdev, wsector,\n\t\t\t\t\t\tsectors, 0)\n\t\t\t\t&& ok;\n\n\t\tbio_put(wbio);\n\t\tsect_to_write -= sectors;\n\t\tsector += sectors;\n\t\tsectors = block_sectors;\n\t}\n\treturn ok;\n}\n\nstatic void handle_read_error(struct mddev *mddev, struct r10bio *r10_bio)\n{\n\tint slot = r10_bio->read_slot;\n\tstruct bio *bio;\n\tstruct r10conf *conf = mddev->private;\n\tstruct md_rdev *rdev = r10_bio->devs[slot].rdev;\n\n\t \n\tbio = r10_bio->devs[slot].bio;\n\tbio_put(bio);\n\tr10_bio->devs[slot].bio = NULL;\n\n\tif (mddev->ro)\n\t\tr10_bio->devs[slot].bio = IO_BLOCKED;\n\telse if (!test_bit(FailFast, &rdev->flags)) {\n\t\tfreeze_array(conf, 1);\n\t\tfix_read_error(conf, mddev, r10_bio);\n\t\tunfreeze_array(conf);\n\t} else\n\t\tmd_error(mddev, rdev);\n\n\trdev_dec_pending(rdev, mddev);\n\tr10_bio->state = 0;\n\traid10_read_request(mddev, r10_bio->master_bio, r10_bio, false);\n\t \n\tallow_barrier(conf);\n}\n\nstatic void handle_write_completed(struct r10conf *conf, struct r10bio *r10_bio)\n{\n\t \n\tint m;\n\tstruct md_rdev *rdev;\n\n\tif (test_bit(R10BIO_IsSync, &r10_bio->state) ||\n\t    test_bit(R10BIO_IsRecover, &r10_bio->state)) {\n\t\tfor (m = 0; m < conf->copies; m++) {\n\t\t\tint dev = r10_bio->devs[m].devnum;\n\t\t\trdev = conf->mirrors[dev].rdev;\n\t\t\tif (r10_bio->devs[m].bio == NULL ||\n\t\t\t\tr10_bio->devs[m].bio->bi_end_io == NULL)\n\t\t\t\tcontinue;\n\t\t\tif (!r10_bio->devs[m].bio->bi_status) {\n\t\t\t\trdev_clear_badblocks(\n\t\t\t\t\trdev,\n\t\t\t\t\tr10_bio->devs[m].addr,\n\t\t\t\t\tr10_bio->sectors, 0);\n\t\t\t} else {\n\t\t\t\tif (!rdev_set_badblocks(\n\t\t\t\t\t    rdev,\n\t\t\t\t\t    r10_bio->devs[m].addr,\n\t\t\t\t\t    r10_bio->sectors, 0))\n\t\t\t\t\tmd_error(conf->mddev, rdev);\n\t\t\t}\n\t\t\trdev = conf->mirrors[dev].replacement;\n\t\t\tif (r10_bio->devs[m].repl_bio == NULL ||\n\t\t\t\tr10_bio->devs[m].repl_bio->bi_end_io == NULL)\n\t\t\t\tcontinue;\n\n\t\t\tif (!r10_bio->devs[m].repl_bio->bi_status) {\n\t\t\t\trdev_clear_badblocks(\n\t\t\t\t\trdev,\n\t\t\t\t\tr10_bio->devs[m].addr,\n\t\t\t\t\tr10_bio->sectors, 0);\n\t\t\t} else {\n\t\t\t\tif (!rdev_set_badblocks(\n\t\t\t\t\t    rdev,\n\t\t\t\t\t    r10_bio->devs[m].addr,\n\t\t\t\t\t    r10_bio->sectors, 0))\n\t\t\t\t\tmd_error(conf->mddev, rdev);\n\t\t\t}\n\t\t}\n\t\tput_buf(r10_bio);\n\t} else {\n\t\tbool fail = false;\n\t\tfor (m = 0; m < conf->copies; m++) {\n\t\t\tint dev = r10_bio->devs[m].devnum;\n\t\t\tstruct bio *bio = r10_bio->devs[m].bio;\n\t\t\trdev = conf->mirrors[dev].rdev;\n\t\t\tif (bio == IO_MADE_GOOD) {\n\t\t\t\trdev_clear_badblocks(\n\t\t\t\t\trdev,\n\t\t\t\t\tr10_bio->devs[m].addr,\n\t\t\t\t\tr10_bio->sectors, 0);\n\t\t\t\trdev_dec_pending(rdev, conf->mddev);\n\t\t\t} else if (bio != NULL && bio->bi_status) {\n\t\t\t\tfail = true;\n\t\t\t\tif (!narrow_write_error(r10_bio, m)) {\n\t\t\t\t\tmd_error(conf->mddev, rdev);\n\t\t\t\t\tset_bit(R10BIO_Degraded,\n\t\t\t\t\t\t&r10_bio->state);\n\t\t\t\t}\n\t\t\t\trdev_dec_pending(rdev, conf->mddev);\n\t\t\t}\n\t\t\tbio = r10_bio->devs[m].repl_bio;\n\t\t\trdev = conf->mirrors[dev].replacement;\n\t\t\tif (rdev && bio == IO_MADE_GOOD) {\n\t\t\t\trdev_clear_badblocks(\n\t\t\t\t\trdev,\n\t\t\t\t\tr10_bio->devs[m].addr,\n\t\t\t\t\tr10_bio->sectors, 0);\n\t\t\t\trdev_dec_pending(rdev, conf->mddev);\n\t\t\t}\n\t\t}\n\t\tif (fail) {\n\t\t\tspin_lock_irq(&conf->device_lock);\n\t\t\tlist_add(&r10_bio->retry_list, &conf->bio_end_io_list);\n\t\t\tconf->nr_queued++;\n\t\t\tspin_unlock_irq(&conf->device_lock);\n\t\t\t \n\t\t\twake_up(&conf->wait_barrier);\n\t\t\tmd_wakeup_thread(conf->mddev->thread);\n\t\t} else {\n\t\t\tif (test_bit(R10BIO_WriteError,\n\t\t\t\t     &r10_bio->state))\n\t\t\t\tclose_write(r10_bio);\n\t\t\traid_end_bio_io(r10_bio);\n\t\t}\n\t}\n}\n\nstatic void raid10d(struct md_thread *thread)\n{\n\tstruct mddev *mddev = thread->mddev;\n\tstruct r10bio *r10_bio;\n\tunsigned long flags;\n\tstruct r10conf *conf = mddev->private;\n\tstruct list_head *head = &conf->retry_list;\n\tstruct blk_plug plug;\n\n\tmd_check_recovery(mddev);\n\n\tif (!list_empty_careful(&conf->bio_end_io_list) &&\n\t    !test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags)) {\n\t\tLIST_HEAD(tmp);\n\t\tspin_lock_irqsave(&conf->device_lock, flags);\n\t\tif (!test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags)) {\n\t\t\twhile (!list_empty(&conf->bio_end_io_list)) {\n\t\t\t\tlist_move(conf->bio_end_io_list.prev, &tmp);\n\t\t\t\tconf->nr_queued--;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\t\twhile (!list_empty(&tmp)) {\n\t\t\tr10_bio = list_first_entry(&tmp, struct r10bio,\n\t\t\t\t\t\t   retry_list);\n\t\t\tlist_del(&r10_bio->retry_list);\n\t\t\tif (mddev->degraded)\n\t\t\t\tset_bit(R10BIO_Degraded, &r10_bio->state);\n\n\t\t\tif (test_bit(R10BIO_WriteError,\n\t\t\t\t     &r10_bio->state))\n\t\t\t\tclose_write(r10_bio);\n\t\t\traid_end_bio_io(r10_bio);\n\t\t}\n\t}\n\n\tblk_start_plug(&plug);\n\tfor (;;) {\n\n\t\tflush_pending_writes(conf);\n\n\t\tspin_lock_irqsave(&conf->device_lock, flags);\n\t\tif (list_empty(head)) {\n\t\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\t\t\tbreak;\n\t\t}\n\t\tr10_bio = list_entry(head->prev, struct r10bio, retry_list);\n\t\tlist_del(head->prev);\n\t\tconf->nr_queued--;\n\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\n\t\tmddev = r10_bio->mddev;\n\t\tconf = mddev->private;\n\t\tif (test_bit(R10BIO_MadeGood, &r10_bio->state) ||\n\t\t    test_bit(R10BIO_WriteError, &r10_bio->state))\n\t\t\thandle_write_completed(conf, r10_bio);\n\t\telse if (test_bit(R10BIO_IsReshape, &r10_bio->state))\n\t\t\treshape_request_write(mddev, r10_bio);\n\t\telse if (test_bit(R10BIO_IsSync, &r10_bio->state))\n\t\t\tsync_request_write(mddev, r10_bio);\n\t\telse if (test_bit(R10BIO_IsRecover, &r10_bio->state))\n\t\t\trecovery_request_write(mddev, r10_bio);\n\t\telse if (test_bit(R10BIO_ReadError, &r10_bio->state))\n\t\t\thandle_read_error(mddev, r10_bio);\n\t\telse\n\t\t\tWARN_ON_ONCE(1);\n\n\t\tcond_resched();\n\t\tif (mddev->sb_flags & ~(1<<MD_SB_CHANGE_PENDING))\n\t\t\tmd_check_recovery(mddev);\n\t}\n\tblk_finish_plug(&plug);\n}\n\nstatic int init_resync(struct r10conf *conf)\n{\n\tint ret, buffs, i;\n\n\tbuffs = RESYNC_WINDOW / RESYNC_BLOCK_SIZE;\n\tBUG_ON(mempool_initialized(&conf->r10buf_pool));\n\tconf->have_replacement = 0;\n\tfor (i = 0; i < conf->geo.raid_disks; i++)\n\t\tif (conf->mirrors[i].replacement)\n\t\t\tconf->have_replacement = 1;\n\tret = mempool_init(&conf->r10buf_pool, buffs,\n\t\t\t   r10buf_pool_alloc, r10buf_pool_free, conf);\n\tif (ret)\n\t\treturn ret;\n\tconf->next_resync = 0;\n\treturn 0;\n}\n\nstatic struct r10bio *raid10_alloc_init_r10buf(struct r10conf *conf)\n{\n\tstruct r10bio *r10bio = mempool_alloc(&conf->r10buf_pool, GFP_NOIO);\n\tstruct rsync_pages *rp;\n\tstruct bio *bio;\n\tint nalloc;\n\tint i;\n\n\tif (test_bit(MD_RECOVERY_SYNC, &conf->mddev->recovery) ||\n\t    test_bit(MD_RECOVERY_RESHAPE, &conf->mddev->recovery))\n\t\tnalloc = conf->copies;  \n\telse\n\t\tnalloc = 2;  \n\n\tfor (i = 0; i < nalloc; i++) {\n\t\tbio = r10bio->devs[i].bio;\n\t\trp = bio->bi_private;\n\t\tbio_reset(bio, NULL, 0);\n\t\tbio->bi_private = rp;\n\t\tbio = r10bio->devs[i].repl_bio;\n\t\tif (bio) {\n\t\t\trp = bio->bi_private;\n\t\t\tbio_reset(bio, NULL, 0);\n\t\t\tbio->bi_private = rp;\n\t\t}\n\t}\n\treturn r10bio;\n}\n\n \nstatic void raid10_set_cluster_sync_high(struct r10conf *conf)\n{\n\tsector_t window_size;\n\tint extra_chunk, chunks;\n\n\t \n\n\tchunks = conf->geo.raid_disks / conf->geo.near_copies;\n\tif (conf->geo.raid_disks % conf->geo.near_copies == 0)\n\t\textra_chunk = 0;\n\telse\n\t\textra_chunk = 1;\n\twindow_size = (chunks + extra_chunk) * conf->mddev->chunk_sectors;\n\n\t \n\twindow_size = (CLUSTER_RESYNC_WINDOW_SECTORS > window_size) ?\n\t\t\tCLUSTER_RESYNC_WINDOW_SECTORS : window_size;\n\n\tconf->cluster_sync_high = conf->cluster_sync_low + window_size;\n}\n\n \n\nstatic sector_t raid10_sync_request(struct mddev *mddev, sector_t sector_nr,\n\t\t\t     int *skipped)\n{\n\tstruct r10conf *conf = mddev->private;\n\tstruct r10bio *r10_bio;\n\tstruct bio *biolist = NULL, *bio;\n\tsector_t max_sector, nr_sectors;\n\tint i;\n\tint max_sync;\n\tsector_t sync_blocks;\n\tsector_t sectors_skipped = 0;\n\tint chunks_skipped = 0;\n\tsector_t chunk_mask = conf->geo.chunk_mask;\n\tint page_idx = 0;\n\tint error_disk = -1;\n\n\t \n\tif (mddev->bitmap == NULL &&\n\t    mddev->recovery_cp == MaxSector &&\n\t    mddev->reshape_position == MaxSector &&\n\t    !test_bit(MD_RECOVERY_SYNC, &mddev->recovery) &&\n\t    !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery) &&\n\t    !test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&\n\t    conf->fullsync == 0) {\n\t\t*skipped = 1;\n\t\treturn mddev->dev_sectors - sector_nr;\n\t}\n\n\tif (!mempool_initialized(&conf->r10buf_pool))\n\t\tif (init_resync(conf))\n\t\t\treturn 0;\n\n skipped:\n\tmax_sector = mddev->dev_sectors;\n\tif (test_bit(MD_RECOVERY_SYNC, &mddev->recovery) ||\n\t    test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery))\n\t\tmax_sector = mddev->resync_max_sectors;\n\tif (sector_nr >= max_sector) {\n\t\tconf->cluster_sync_low = 0;\n\t\tconf->cluster_sync_high = 0;\n\n\t\t \n\t\tif (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery)) {\n\t\t\tend_reshape(conf);\n\t\t\tclose_sync(conf);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (mddev->curr_resync < max_sector) {  \n\t\t\tif (test_bit(MD_RECOVERY_SYNC, &mddev->recovery))\n\t\t\t\tmd_bitmap_end_sync(mddev->bitmap, mddev->curr_resync,\n\t\t\t\t\t\t   &sync_blocks, 1);\n\t\t\telse for (i = 0; i < conf->geo.raid_disks; i++) {\n\t\t\t\tsector_t sect =\n\t\t\t\t\traid10_find_virt(conf, mddev->curr_resync, i);\n\t\t\t\tmd_bitmap_end_sync(mddev->bitmap, sect,\n\t\t\t\t\t\t   &sync_blocks, 1);\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tif ((!mddev->bitmap || conf->fullsync)\n\t\t\t    && conf->have_replacement\n\t\t\t    && test_bit(MD_RECOVERY_SYNC, &mddev->recovery)) {\n\t\t\t\t \n\t\t\t\trcu_read_lock();\n\t\t\t\tfor (i = 0; i < conf->geo.raid_disks; i++) {\n\t\t\t\t\tstruct md_rdev *rdev =\n\t\t\t\t\t\trcu_dereference(conf->mirrors[i].replacement);\n\t\t\t\t\tif (rdev)\n\t\t\t\t\t\trdev->recovery_offset = MaxSector;\n\t\t\t\t}\n\t\t\t\trcu_read_unlock();\n\t\t\t}\n\t\t\tconf->fullsync = 0;\n\t\t}\n\t\tmd_bitmap_close_sync(mddev->bitmap);\n\t\tclose_sync(conf);\n\t\t*skipped = 1;\n\t\treturn sectors_skipped;\n\t}\n\n\tif (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery))\n\t\treturn reshape_request(mddev, sector_nr, skipped);\n\n\tif (chunks_skipped >= conf->geo.raid_disks) {\n\t\tpr_err(\"md/raid10:%s: %s fails\\n\", mdname(mddev),\n\t\t\ttest_bit(MD_RECOVERY_SYNC, &mddev->recovery) ?  \"resync\" : \"recovery\");\n\t\tif (error_disk >= 0 &&\n\t\t    !test_bit(MD_RECOVERY_SYNC, &mddev->recovery)) {\n\t\t\t \n\t\t\tconf->mirrors[error_disk].recovery_disabled =\n\t\t\t\t\t\tmddev->recovery_disabled;\n\t\t\treturn 0;\n\t\t}\n\t\t \n\t\t*skipped = 1;\n\t\treturn (max_sector - sector_nr) + sectors_skipped;\n\t}\n\n\tif (max_sector > mddev->resync_max)\n\t\tmax_sector = mddev->resync_max;  \n\n\t \n\tif (conf->geo.near_copies < conf->geo.raid_disks &&\n\t    max_sector > (sector_nr | chunk_mask))\n\t\tmax_sector = (sector_nr | chunk_mask) + 1;\n\n\t \n\tif (conf->nr_waiting)\n\t\tschedule_timeout_uninterruptible(1);\n\n\t \n\t \n\n\tmax_sync = RESYNC_PAGES << (PAGE_SHIFT-9);\n\tif (!test_bit(MD_RECOVERY_SYNC, &mddev->recovery)) {\n\t\t \n\t\tint j;\n\t\tr10_bio = NULL;\n\n\t\tfor (i = 0 ; i < conf->geo.raid_disks; i++) {\n\t\t\tint still_degraded;\n\t\t\tstruct r10bio *rb2;\n\t\t\tsector_t sect;\n\t\t\tint must_sync;\n\t\t\tint any_working;\n\t\t\tstruct raid10_info *mirror = &conf->mirrors[i];\n\t\t\tstruct md_rdev *mrdev, *mreplace;\n\n\t\t\trcu_read_lock();\n\t\t\tmrdev = rcu_dereference(mirror->rdev);\n\t\t\tmreplace = rcu_dereference(mirror->replacement);\n\n\t\t\tif (mrdev && (test_bit(Faulty, &mrdev->flags) ||\n\t\t\t    test_bit(In_sync, &mrdev->flags)))\n\t\t\t\tmrdev = NULL;\n\t\t\tif (mreplace && test_bit(Faulty, &mreplace->flags))\n\t\t\t\tmreplace = NULL;\n\n\t\t\tif (!mrdev && !mreplace) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tstill_degraded = 0;\n\t\t\t \n\t\t\trb2 = r10_bio;\n\t\t\tsect = raid10_find_virt(conf, sector_nr, i);\n\t\t\tif (sect >= mddev->resync_max_sectors) {\n\t\t\t\t \n\t\t\t\trcu_read_unlock();\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t \n\t\t\tmust_sync = md_bitmap_start_sync(mddev->bitmap, sect,\n\t\t\t\t\t\t\t &sync_blocks, 1);\n\t\t\tif (sync_blocks < max_sync)\n\t\t\t\tmax_sync = sync_blocks;\n\t\t\tif (!must_sync &&\n\t\t\t    mreplace == NULL &&\n\t\t\t    !conf->fullsync) {\n\t\t\t\t \n\t\t\t\tchunks_skipped = -1;\n\t\t\t\trcu_read_unlock();\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (mrdev)\n\t\t\t\tatomic_inc(&mrdev->nr_pending);\n\t\t\tif (mreplace)\n\t\t\t\tatomic_inc(&mreplace->nr_pending);\n\t\t\trcu_read_unlock();\n\n\t\t\tr10_bio = raid10_alloc_init_r10buf(conf);\n\t\t\tr10_bio->state = 0;\n\t\t\traise_barrier(conf, rb2 != NULL);\n\t\t\tatomic_set(&r10_bio->remaining, 0);\n\n\t\t\tr10_bio->master_bio = (struct bio*)rb2;\n\t\t\tif (rb2)\n\t\t\t\tatomic_inc(&rb2->remaining);\n\t\t\tr10_bio->mddev = mddev;\n\t\t\tset_bit(R10BIO_IsRecover, &r10_bio->state);\n\t\t\tr10_bio->sector = sect;\n\n\t\t\traid10_find_phys(conf, r10_bio);\n\n\t\t\t \n\t\t\trcu_read_lock();\n\t\t\tfor (j = 0; j < conf->geo.raid_disks; j++) {\n\t\t\t\tstruct md_rdev *rdev = rcu_dereference(\n\t\t\t\t\tconf->mirrors[j].rdev);\n\t\t\t\tif (rdev == NULL || test_bit(Faulty, &rdev->flags)) {\n\t\t\t\t\tstill_degraded = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tmust_sync = md_bitmap_start_sync(mddev->bitmap, sect,\n\t\t\t\t\t\t\t &sync_blocks, still_degraded);\n\n\t\t\tany_working = 0;\n\t\t\tfor (j=0; j<conf->copies;j++) {\n\t\t\t\tint k;\n\t\t\t\tint d = r10_bio->devs[j].devnum;\n\t\t\t\tsector_t from_addr, to_addr;\n\t\t\t\tstruct md_rdev *rdev =\n\t\t\t\t\trcu_dereference(conf->mirrors[d].rdev);\n\t\t\t\tsector_t sector, first_bad;\n\t\t\t\tint bad_sectors;\n\t\t\t\tif (!rdev ||\n\t\t\t\t    !test_bit(In_sync, &rdev->flags))\n\t\t\t\t\tcontinue;\n\t\t\t\t \n\t\t\t\tany_working = 1;\n\t\t\t\tsector = r10_bio->devs[j].addr;\n\n\t\t\t\tif (is_badblock(rdev, sector, max_sync,\n\t\t\t\t\t\t&first_bad, &bad_sectors)) {\n\t\t\t\t\tif (first_bad > sector)\n\t\t\t\t\t\tmax_sync = first_bad - sector;\n\t\t\t\t\telse {\n\t\t\t\t\t\tbad_sectors -= (sector\n\t\t\t\t\t\t\t\t- first_bad);\n\t\t\t\t\t\tif (max_sync > bad_sectors)\n\t\t\t\t\t\t\tmax_sync = bad_sectors;\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tbio = r10_bio->devs[0].bio;\n\t\t\t\tbio->bi_next = biolist;\n\t\t\t\tbiolist = bio;\n\t\t\t\tbio->bi_end_io = end_sync_read;\n\t\t\t\tbio->bi_opf = REQ_OP_READ;\n\t\t\t\tif (test_bit(FailFast, &rdev->flags))\n\t\t\t\t\tbio->bi_opf |= MD_FAILFAST;\n\t\t\t\tfrom_addr = r10_bio->devs[j].addr;\n\t\t\t\tbio->bi_iter.bi_sector = from_addr +\n\t\t\t\t\trdev->data_offset;\n\t\t\t\tbio_set_dev(bio, rdev->bdev);\n\t\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\t\t \n\n\t\t\t\tfor (k=0; k<conf->copies; k++)\n\t\t\t\t\tif (r10_bio->devs[k].devnum == i)\n\t\t\t\t\t\tbreak;\n\t\t\t\tBUG_ON(k == conf->copies);\n\t\t\t\tto_addr = r10_bio->devs[k].addr;\n\t\t\t\tr10_bio->devs[0].devnum = d;\n\t\t\t\tr10_bio->devs[0].addr = from_addr;\n\t\t\t\tr10_bio->devs[1].devnum = i;\n\t\t\t\tr10_bio->devs[1].addr = to_addr;\n\n\t\t\t\tif (mrdev) {\n\t\t\t\t\tbio = r10_bio->devs[1].bio;\n\t\t\t\t\tbio->bi_next = biolist;\n\t\t\t\t\tbiolist = bio;\n\t\t\t\t\tbio->bi_end_io = end_sync_write;\n\t\t\t\t\tbio->bi_opf = REQ_OP_WRITE;\n\t\t\t\t\tbio->bi_iter.bi_sector = to_addr\n\t\t\t\t\t\t+ mrdev->data_offset;\n\t\t\t\t\tbio_set_dev(bio, mrdev->bdev);\n\t\t\t\t\tatomic_inc(&r10_bio->remaining);\n\t\t\t\t} else\n\t\t\t\t\tr10_bio->devs[1].bio->bi_end_io = NULL;\n\n\t\t\t\t \n\t\t\t\tbio = r10_bio->devs[1].repl_bio;\n\t\t\t\tif (bio)\n\t\t\t\t\tbio->bi_end_io = NULL;\n\t\t\t\t \n\t\t\t\tif (!mreplace)\n\t\t\t\t\tbreak;\n\t\t\t\tbio->bi_next = biolist;\n\t\t\t\tbiolist = bio;\n\t\t\t\tbio->bi_end_io = end_sync_write;\n\t\t\t\tbio->bi_opf = REQ_OP_WRITE;\n\t\t\t\tbio->bi_iter.bi_sector = to_addr +\n\t\t\t\t\tmreplace->data_offset;\n\t\t\t\tbio_set_dev(bio, mreplace->bdev);\n\t\t\t\tatomic_inc(&r10_bio->remaining);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\trcu_read_unlock();\n\t\t\tif (j == conf->copies) {\n\t\t\t\t \n\t\t\t\tif (any_working) {\n\t\t\t\t\t \n\t\t\t\t\tint k;\n\t\t\t\t\tfor (k = 0; k < conf->copies; k++)\n\t\t\t\t\t\tif (r10_bio->devs[k].devnum == i)\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\tif (mrdev && !test_bit(In_sync,\n\t\t\t\t\t\t      &mrdev->flags)\n\t\t\t\t\t    && !rdev_set_badblocks(\n\t\t\t\t\t\t    mrdev,\n\t\t\t\t\t\t    r10_bio->devs[k].addr,\n\t\t\t\t\t\t    max_sync, 0))\n\t\t\t\t\t\tany_working = 0;\n\t\t\t\t\tif (mreplace &&\n\t\t\t\t\t    !rdev_set_badblocks(\n\t\t\t\t\t\t    mreplace,\n\t\t\t\t\t\t    r10_bio->devs[k].addr,\n\t\t\t\t\t\t    max_sync, 0))\n\t\t\t\t\t\tany_working = 0;\n\t\t\t\t}\n\t\t\t\tif (!any_working)  {\n\t\t\t\t\tif (!test_and_set_bit(MD_RECOVERY_INTR,\n\t\t\t\t\t\t\t      &mddev->recovery))\n\t\t\t\t\t\tpr_warn(\"md/raid10:%s: insufficient working devices for recovery.\\n\",\n\t\t\t\t\t\t       mdname(mddev));\n\t\t\t\t\tmirror->recovery_disabled\n\t\t\t\t\t\t= mddev->recovery_disabled;\n\t\t\t\t} else {\n\t\t\t\t\terror_disk = i;\n\t\t\t\t}\n\t\t\t\tput_buf(r10_bio);\n\t\t\t\tif (rb2)\n\t\t\t\t\tatomic_dec(&rb2->remaining);\n\t\t\t\tr10_bio = rb2;\n\t\t\t\tif (mrdev)\n\t\t\t\t\trdev_dec_pending(mrdev, mddev);\n\t\t\t\tif (mreplace)\n\t\t\t\t\trdev_dec_pending(mreplace, mddev);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (mrdev)\n\t\t\t\trdev_dec_pending(mrdev, mddev);\n\t\t\tif (mreplace)\n\t\t\t\trdev_dec_pending(mreplace, mddev);\n\t\t\tif (r10_bio->devs[0].bio->bi_opf & MD_FAILFAST) {\n\t\t\t\t \n\t\t\t\tint targets = 1;\n\t\t\t\tfor (; j < conf->copies; j++) {\n\t\t\t\t\tint d = r10_bio->devs[j].devnum;\n\t\t\t\t\tif (conf->mirrors[d].rdev &&\n\t\t\t\t\t    test_bit(In_sync,\n\t\t\t\t\t\t      &conf->mirrors[d].rdev->flags))\n\t\t\t\t\t\ttargets++;\n\t\t\t\t}\n\t\t\t\tif (targets == 1)\n\t\t\t\t\tr10_bio->devs[0].bio->bi_opf\n\t\t\t\t\t\t&= ~MD_FAILFAST;\n\t\t\t}\n\t\t}\n\t\tif (biolist == NULL) {\n\t\t\twhile (r10_bio) {\n\t\t\t\tstruct r10bio *rb2 = r10_bio;\n\t\t\t\tr10_bio = (struct r10bio*) rb2->master_bio;\n\t\t\t\trb2->master_bio = NULL;\n\t\t\t\tput_buf(rb2);\n\t\t\t}\n\t\t\tgoto giveup;\n\t\t}\n\t} else {\n\t\t \n\t\tint count = 0;\n\n\t\t \n\t\tmd_bitmap_cond_end_sync(mddev->bitmap, sector_nr,\n\t\t\t\t\tmddev_is_clustered(mddev) &&\n\t\t\t\t\t(sector_nr + 2 * RESYNC_SECTORS > conf->cluster_sync_high));\n\n\t\tif (!md_bitmap_start_sync(mddev->bitmap, sector_nr,\n\t\t\t\t\t  &sync_blocks, mddev->degraded) &&\n\t\t    !conf->fullsync && !test_bit(MD_RECOVERY_REQUESTED,\n\t\t\t\t\t\t &mddev->recovery)) {\n\t\t\t \n\t\t\t*skipped = 1;\n\t\t\treturn sync_blocks + sectors_skipped;\n\t\t}\n\t\tif (sync_blocks < max_sync)\n\t\t\tmax_sync = sync_blocks;\n\t\tr10_bio = raid10_alloc_init_r10buf(conf);\n\t\tr10_bio->state = 0;\n\n\t\tr10_bio->mddev = mddev;\n\t\tatomic_set(&r10_bio->remaining, 0);\n\t\traise_barrier(conf, 0);\n\t\tconf->next_resync = sector_nr;\n\n\t\tr10_bio->master_bio = NULL;\n\t\tr10_bio->sector = sector_nr;\n\t\tset_bit(R10BIO_IsSync, &r10_bio->state);\n\t\traid10_find_phys(conf, r10_bio);\n\t\tr10_bio->sectors = (sector_nr | chunk_mask) - sector_nr + 1;\n\n\t\tfor (i = 0; i < conf->copies; i++) {\n\t\t\tint d = r10_bio->devs[i].devnum;\n\t\t\tsector_t first_bad, sector;\n\t\t\tint bad_sectors;\n\t\t\tstruct md_rdev *rdev;\n\n\t\t\tif (r10_bio->devs[i].repl_bio)\n\t\t\t\tr10_bio->devs[i].repl_bio->bi_end_io = NULL;\n\n\t\t\tbio = r10_bio->devs[i].bio;\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\t\trcu_read_lock();\n\t\t\trdev = rcu_dereference(conf->mirrors[d].rdev);\n\t\t\tif (rdev == NULL || test_bit(Faulty, &rdev->flags)) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tsector = r10_bio->devs[i].addr;\n\t\t\tif (is_badblock(rdev, sector, max_sync,\n\t\t\t\t\t&first_bad, &bad_sectors)) {\n\t\t\t\tif (first_bad > sector)\n\t\t\t\t\tmax_sync = first_bad - sector;\n\t\t\t\telse {\n\t\t\t\t\tbad_sectors -= (sector - first_bad);\n\t\t\t\t\tif (max_sync > bad_sectors)\n\t\t\t\t\t\tmax_sync = bad_sectors;\n\t\t\t\t\trcu_read_unlock();\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t}\n\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\tatomic_inc(&r10_bio->remaining);\n\t\t\tbio->bi_next = biolist;\n\t\t\tbiolist = bio;\n\t\t\tbio->bi_end_io = end_sync_read;\n\t\t\tbio->bi_opf = REQ_OP_READ;\n\t\t\tif (test_bit(FailFast, &rdev->flags))\n\t\t\t\tbio->bi_opf |= MD_FAILFAST;\n\t\t\tbio->bi_iter.bi_sector = sector + rdev->data_offset;\n\t\t\tbio_set_dev(bio, rdev->bdev);\n\t\t\tcount++;\n\n\t\t\trdev = rcu_dereference(conf->mirrors[d].replacement);\n\t\t\tif (rdev == NULL || test_bit(Faulty, &rdev->flags)) {\n\t\t\t\trcu_read_unlock();\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tatomic_inc(&rdev->nr_pending);\n\n\t\t\t \n\t\t\tbio = r10_bio->devs[i].repl_bio;\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\n\t\t\tsector = r10_bio->devs[i].addr;\n\t\t\tbio->bi_next = biolist;\n\t\t\tbiolist = bio;\n\t\t\tbio->bi_end_io = end_sync_write;\n\t\t\tbio->bi_opf = REQ_OP_WRITE;\n\t\t\tif (test_bit(FailFast, &rdev->flags))\n\t\t\t\tbio->bi_opf |= MD_FAILFAST;\n\t\t\tbio->bi_iter.bi_sector = sector + rdev->data_offset;\n\t\t\tbio_set_dev(bio, rdev->bdev);\n\t\t\tcount++;\n\t\t\trcu_read_unlock();\n\t\t}\n\n\t\tif (count < 2) {\n\t\t\tfor (i=0; i<conf->copies; i++) {\n\t\t\t\tint d = r10_bio->devs[i].devnum;\n\t\t\t\tif (r10_bio->devs[i].bio->bi_end_io)\n\t\t\t\t\trdev_dec_pending(conf->mirrors[d].rdev,\n\t\t\t\t\t\t\t mddev);\n\t\t\t\tif (r10_bio->devs[i].repl_bio &&\n\t\t\t\t    r10_bio->devs[i].repl_bio->bi_end_io)\n\t\t\t\t\trdev_dec_pending(\n\t\t\t\t\t\tconf->mirrors[d].replacement,\n\t\t\t\t\t\tmddev);\n\t\t\t}\n\t\t\tput_buf(r10_bio);\n\t\t\tbiolist = NULL;\n\t\t\tgoto giveup;\n\t\t}\n\t}\n\n\tnr_sectors = 0;\n\tif (sector_nr + max_sync < max_sector)\n\t\tmax_sector = sector_nr + max_sync;\n\tdo {\n\t\tstruct page *page;\n\t\tint len = PAGE_SIZE;\n\t\tif (sector_nr + (len>>9) > max_sector)\n\t\t\tlen = (max_sector - sector_nr) << 9;\n\t\tif (len == 0)\n\t\t\tbreak;\n\t\tfor (bio= biolist ; bio ; bio=bio->bi_next) {\n\t\t\tstruct resync_pages *rp = get_resync_pages(bio);\n\t\t\tpage = resync_fetch_page(rp, page_idx);\n\t\t\tif (WARN_ON(!bio_add_page(bio, page, len, 0))) {\n\t\t\t\tbio->bi_status = BLK_STS_RESOURCE;\n\t\t\t\tbio_endio(bio);\n\t\t\t\tgoto giveup;\n\t\t\t}\n\t\t}\n\t\tnr_sectors += len>>9;\n\t\tsector_nr += len>>9;\n\t} while (++page_idx < RESYNC_PAGES);\n\tr10_bio->sectors = nr_sectors;\n\n\tif (mddev_is_clustered(mddev) &&\n\t    test_bit(MD_RECOVERY_SYNC, &mddev->recovery)) {\n\t\t \n\t\tif (conf->cluster_sync_high < sector_nr + nr_sectors) {\n\t\t\tconf->cluster_sync_low = mddev->curr_resync_completed;\n\t\t\traid10_set_cluster_sync_high(conf);\n\t\t\t \n\t\t\tmd_cluster_ops->resync_info_update(mddev,\n\t\t\t\t\t\tconf->cluster_sync_low,\n\t\t\t\t\t\tconf->cluster_sync_high);\n\t\t}\n\t} else if (mddev_is_clustered(mddev)) {\n\t\t \n\t\tsector_t sect_va1, sect_va2;\n\t\tbool broadcast_msg = false;\n\n\t\tfor (i = 0; i < conf->geo.raid_disks; i++) {\n\t\t\t \n\t\t\tsect_va1 = raid10_find_virt(conf, sector_nr, i);\n\n\t\t\tif (conf->cluster_sync_high < sect_va1 + nr_sectors) {\n\t\t\t\tbroadcast_msg = true;\n\t\t\t\t \n\t\t\t\tsect_va2 = raid10_find_virt(conf,\n\t\t\t\t\tmddev->curr_resync_completed, i);\n\n\t\t\t\tif (conf->cluster_sync_low == 0 ||\n\t\t\t\t    conf->cluster_sync_low > sect_va2)\n\t\t\t\t\tconf->cluster_sync_low = sect_va2;\n\t\t\t}\n\t\t}\n\t\tif (broadcast_msg) {\n\t\t\traid10_set_cluster_sync_high(conf);\n\t\t\tmd_cluster_ops->resync_info_update(mddev,\n\t\t\t\t\t\tconf->cluster_sync_low,\n\t\t\t\t\t\tconf->cluster_sync_high);\n\t\t}\n\t}\n\n\twhile (biolist) {\n\t\tbio = biolist;\n\t\tbiolist = biolist->bi_next;\n\n\t\tbio->bi_next = NULL;\n\t\tr10_bio = get_resync_r10bio(bio);\n\t\tr10_bio->sectors = nr_sectors;\n\n\t\tif (bio->bi_end_io == end_sync_read) {\n\t\t\tmd_sync_acct_bio(bio, nr_sectors);\n\t\t\tbio->bi_status = 0;\n\t\t\tsubmit_bio_noacct(bio);\n\t\t}\n\t}\n\n\tif (sectors_skipped)\n\t\t \n\t\tmd_done_sync(mddev, sectors_skipped, 1);\n\n\treturn sectors_skipped + nr_sectors;\n giveup:\n\t \n\tif (sector_nr + max_sync < max_sector)\n\t\tmax_sector = sector_nr + max_sync;\n\n\tsectors_skipped += (max_sector - sector_nr);\n\tchunks_skipped ++;\n\tsector_nr = max_sector;\n\tgoto skipped;\n}\n\nstatic sector_t\nraid10_size(struct mddev *mddev, sector_t sectors, int raid_disks)\n{\n\tsector_t size;\n\tstruct r10conf *conf = mddev->private;\n\n\tif (!raid_disks)\n\t\traid_disks = min(conf->geo.raid_disks,\n\t\t\t\t conf->prev.raid_disks);\n\tif (!sectors)\n\t\tsectors = conf->dev_sectors;\n\n\tsize = sectors >> conf->geo.chunk_shift;\n\tsector_div(size, conf->geo.far_copies);\n\tsize = size * raid_disks;\n\tsector_div(size, conf->geo.near_copies);\n\n\treturn size << conf->geo.chunk_shift;\n}\n\nstatic void calc_sectors(struct r10conf *conf, sector_t size)\n{\n\t \n\n\tsize = size >> conf->geo.chunk_shift;\n\tsector_div(size, conf->geo.far_copies);\n\tsize = size * conf->geo.raid_disks;\n\tsector_div(size, conf->geo.near_copies);\n\t \n\t \n\tsize = size * conf->copies;\n\n\t \n\tsize = DIV_ROUND_UP_SECTOR_T(size, conf->geo.raid_disks);\n\n\tconf->dev_sectors = size << conf->geo.chunk_shift;\n\n\tif (conf->geo.far_offset)\n\t\tconf->geo.stride = 1 << conf->geo.chunk_shift;\n\telse {\n\t\tsector_div(size, conf->geo.far_copies);\n\t\tconf->geo.stride = size << conf->geo.chunk_shift;\n\t}\n}\n\nenum geo_type {geo_new, geo_old, geo_start};\nstatic int setup_geo(struct geom *geo, struct mddev *mddev, enum geo_type new)\n{\n\tint nc, fc, fo;\n\tint layout, chunk, disks;\n\tswitch (new) {\n\tcase geo_old:\n\t\tlayout = mddev->layout;\n\t\tchunk = mddev->chunk_sectors;\n\t\tdisks = mddev->raid_disks - mddev->delta_disks;\n\t\tbreak;\n\tcase geo_new:\n\t\tlayout = mddev->new_layout;\n\t\tchunk = mddev->new_chunk_sectors;\n\t\tdisks = mddev->raid_disks;\n\t\tbreak;\n\tdefault:  \n\tcase geo_start:  \n\t\tlayout = mddev->new_layout;\n\t\tchunk = mddev->new_chunk_sectors;\n\t\tdisks = mddev->raid_disks + mddev->delta_disks;\n\t\tbreak;\n\t}\n\tif (layout >> 19)\n\t\treturn -1;\n\tif (chunk < (PAGE_SIZE >> 9) ||\n\t    !is_power_of_2(chunk))\n\t\treturn -2;\n\tnc = layout & 255;\n\tfc = (layout >> 8) & 255;\n\tfo = layout & (1<<16);\n\tgeo->raid_disks = disks;\n\tgeo->near_copies = nc;\n\tgeo->far_copies = fc;\n\tgeo->far_offset = fo;\n\tswitch (layout >> 17) {\n\tcase 0:\t \n\t\tgeo->far_set_size = disks;\n\t\tbreak;\n\tcase 1:  \n\t\tgeo->far_set_size = disks/fc;\n\t\tWARN(geo->far_set_size < fc,\n\t\t     \"This RAID10 layout does not provide data safety - please backup and create new array\\n\");\n\t\tbreak;\n\tcase 2:  \n\t\tgeo->far_set_size = fc * nc;\n\t\tbreak;\n\tdefault:  \n\t\treturn -1;\n\t}\n\tgeo->chunk_mask = chunk - 1;\n\tgeo->chunk_shift = ffz(~chunk);\n\treturn nc*fc;\n}\n\nstatic void raid10_free_conf(struct r10conf *conf)\n{\n\tif (!conf)\n\t\treturn;\n\n\tmempool_exit(&conf->r10bio_pool);\n\tkfree(conf->mirrors);\n\tkfree(conf->mirrors_old);\n\tkfree(conf->mirrors_new);\n\tsafe_put_page(conf->tmppage);\n\tbioset_exit(&conf->bio_split);\n\tkfree(conf);\n}\n\nstatic struct r10conf *setup_conf(struct mddev *mddev)\n{\n\tstruct r10conf *conf = NULL;\n\tint err = -EINVAL;\n\tstruct geom geo;\n\tint copies;\n\n\tcopies = setup_geo(&geo, mddev, geo_new);\n\n\tif (copies == -2) {\n\t\tpr_warn(\"md/raid10:%s: chunk size must be at least PAGE_SIZE(%ld) and be a power of 2.\\n\",\n\t\t\tmdname(mddev), PAGE_SIZE);\n\t\tgoto out;\n\t}\n\n\tif (copies < 2 || copies > mddev->raid_disks) {\n\t\tpr_warn(\"md/raid10:%s: unsupported raid10 layout: 0x%8x\\n\",\n\t\t\tmdname(mddev), mddev->new_layout);\n\t\tgoto out;\n\t}\n\n\terr = -ENOMEM;\n\tconf = kzalloc(sizeof(struct r10conf), GFP_KERNEL);\n\tif (!conf)\n\t\tgoto out;\n\n\t \n\tconf->mirrors = kcalloc(mddev->raid_disks + max(0, -mddev->delta_disks),\n\t\t\t\tsizeof(struct raid10_info),\n\t\t\t\tGFP_KERNEL);\n\tif (!conf->mirrors)\n\t\tgoto out;\n\n\tconf->tmppage = alloc_page(GFP_KERNEL);\n\tif (!conf->tmppage)\n\t\tgoto out;\n\n\tconf->geo = geo;\n\tconf->copies = copies;\n\terr = mempool_init(&conf->r10bio_pool, NR_RAID_BIOS, r10bio_pool_alloc,\n\t\t\t   rbio_pool_free, conf);\n\tif (err)\n\t\tgoto out;\n\n\terr = bioset_init(&conf->bio_split, BIO_POOL_SIZE, 0, 0);\n\tif (err)\n\t\tgoto out;\n\n\tcalc_sectors(conf, mddev->dev_sectors);\n\tif (mddev->reshape_position == MaxSector) {\n\t\tconf->prev = conf->geo;\n\t\tconf->reshape_progress = MaxSector;\n\t} else {\n\t\tif (setup_geo(&conf->prev, mddev, geo_old) != conf->copies) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tconf->reshape_progress = mddev->reshape_position;\n\t\tif (conf->prev.far_offset)\n\t\t\tconf->prev.stride = 1 << conf->prev.chunk_shift;\n\t\telse\n\t\t\t \n\t\t\tconf->prev.stride = conf->dev_sectors;\n\t}\n\tconf->reshape_safe = conf->reshape_progress;\n\tspin_lock_init(&conf->device_lock);\n\tINIT_LIST_HEAD(&conf->retry_list);\n\tINIT_LIST_HEAD(&conf->bio_end_io_list);\n\n\tseqlock_init(&conf->resync_lock);\n\tinit_waitqueue_head(&conf->wait_barrier);\n\tatomic_set(&conf->nr_pending, 0);\n\n\terr = -ENOMEM;\n\trcu_assign_pointer(conf->thread,\n\t\t\t   md_register_thread(raid10d, mddev, \"raid10\"));\n\tif (!conf->thread)\n\t\tgoto out;\n\n\tconf->mddev = mddev;\n\treturn conf;\n\n out:\n\traid10_free_conf(conf);\n\treturn ERR_PTR(err);\n}\n\nstatic void raid10_set_io_opt(struct r10conf *conf)\n{\n\tint raid_disks = conf->geo.raid_disks;\n\n\tif (!(conf->geo.raid_disks % conf->geo.near_copies))\n\t\traid_disks /= conf->geo.near_copies;\n\tblk_queue_io_opt(conf->mddev->queue, (conf->mddev->chunk_sectors << 9) *\n\t\t\t raid_disks);\n}\n\nstatic int raid10_run(struct mddev *mddev)\n{\n\tstruct r10conf *conf;\n\tint i, disk_idx;\n\tstruct raid10_info *disk;\n\tstruct md_rdev *rdev;\n\tsector_t size;\n\tsector_t min_offset_diff = 0;\n\tint first = 1;\n\n\tif (mddev_init_writes_pending(mddev) < 0)\n\t\treturn -ENOMEM;\n\n\tif (mddev->private == NULL) {\n\t\tconf = setup_conf(mddev);\n\t\tif (IS_ERR(conf))\n\t\t\treturn PTR_ERR(conf);\n\t\tmddev->private = conf;\n\t}\n\tconf = mddev->private;\n\tif (!conf)\n\t\tgoto out;\n\n\trcu_assign_pointer(mddev->thread, conf->thread);\n\trcu_assign_pointer(conf->thread, NULL);\n\n\tif (mddev_is_clustered(conf->mddev)) {\n\t\tint fc, fo;\n\n\t\tfc = (mddev->layout >> 8) & 255;\n\t\tfo = mddev->layout & (1<<16);\n\t\tif (fc > 1 || fo > 0) {\n\t\t\tpr_err(\"only near layout is supported by clustered\"\n\t\t\t\t\" raid10\\n\");\n\t\t\tgoto out_free_conf;\n\t\t}\n\t}\n\n\tif (mddev->queue) {\n\t\tblk_queue_max_write_zeroes_sectors(mddev->queue, 0);\n\t\tblk_queue_io_min(mddev->queue, mddev->chunk_sectors << 9);\n\t\traid10_set_io_opt(conf);\n\t}\n\n\trdev_for_each(rdev, mddev) {\n\t\tlong long diff;\n\n\t\tdisk_idx = rdev->raid_disk;\n\t\tif (disk_idx < 0)\n\t\t\tcontinue;\n\t\tif (disk_idx >= conf->geo.raid_disks &&\n\t\t    disk_idx >= conf->prev.raid_disks)\n\t\t\tcontinue;\n\t\tdisk = conf->mirrors + disk_idx;\n\n\t\tif (test_bit(Replacement, &rdev->flags)) {\n\t\t\tif (disk->replacement)\n\t\t\t\tgoto out_free_conf;\n\t\t\tdisk->replacement = rdev;\n\t\t} else {\n\t\t\tif (disk->rdev)\n\t\t\t\tgoto out_free_conf;\n\t\t\tdisk->rdev = rdev;\n\t\t}\n\t\tdiff = (rdev->new_data_offset - rdev->data_offset);\n\t\tif (!mddev->reshape_backwards)\n\t\t\tdiff = -diff;\n\t\tif (diff < 0)\n\t\t\tdiff = 0;\n\t\tif (first || diff < min_offset_diff)\n\t\t\tmin_offset_diff = diff;\n\n\t\tif (mddev->gendisk)\n\t\t\tdisk_stack_limits(mddev->gendisk, rdev->bdev,\n\t\t\t\t\t  rdev->data_offset << 9);\n\n\t\tdisk->head_position = 0;\n\t\tfirst = 0;\n\t}\n\n\t \n\tif (!enough(conf, -1)) {\n\t\tpr_err(\"md/raid10:%s: not enough operational mirrors.\\n\",\n\t\t       mdname(mddev));\n\t\tgoto out_free_conf;\n\t}\n\n\tif (conf->reshape_progress != MaxSector) {\n\t\t \n\t\tif (conf->geo.far_copies != 1 &&\n\t\t    conf->geo.far_offset == 0)\n\t\t\tgoto out_free_conf;\n\t\tif (conf->prev.far_copies != 1 &&\n\t\t    conf->prev.far_offset == 0)\n\t\t\tgoto out_free_conf;\n\t}\n\n\tmddev->degraded = 0;\n\tfor (i = 0;\n\t     i < conf->geo.raid_disks\n\t\t     || i < conf->prev.raid_disks;\n\t     i++) {\n\n\t\tdisk = conf->mirrors + i;\n\n\t\tif (!disk->rdev && disk->replacement) {\n\t\t\t \n\t\t\tdisk->rdev = disk->replacement;\n\t\t\tdisk->replacement = NULL;\n\t\t\tclear_bit(Replacement, &disk->rdev->flags);\n\t\t}\n\n\t\tif (!disk->rdev ||\n\t\t    !test_bit(In_sync, &disk->rdev->flags)) {\n\t\t\tdisk->head_position = 0;\n\t\t\tmddev->degraded++;\n\t\t\tif (disk->rdev &&\n\t\t\t    disk->rdev->saved_raid_disk < 0)\n\t\t\t\tconf->fullsync = 1;\n\t\t}\n\n\t\tif (disk->replacement &&\n\t\t    !test_bit(In_sync, &disk->replacement->flags) &&\n\t\t    disk->replacement->saved_raid_disk < 0) {\n\t\t\tconf->fullsync = 1;\n\t\t}\n\n\t\tdisk->recovery_disabled = mddev->recovery_disabled - 1;\n\t}\n\n\tif (mddev->recovery_cp != MaxSector)\n\t\tpr_notice(\"md/raid10:%s: not clean -- starting background reconstruction\\n\",\n\t\t\t  mdname(mddev));\n\tpr_info(\"md/raid10:%s: active with %d out of %d devices\\n\",\n\t\tmdname(mddev), conf->geo.raid_disks - mddev->degraded,\n\t\tconf->geo.raid_disks);\n\t \n\tmddev->dev_sectors = conf->dev_sectors;\n\tsize = raid10_size(mddev, 0, 0);\n\tmd_set_array_sectors(mddev, size);\n\tmddev->resync_max_sectors = size;\n\tset_bit(MD_FAILFAST_SUPPORTED, &mddev->flags);\n\n\tif (md_integrity_register(mddev))\n\t\tgoto out_free_conf;\n\n\tif (conf->reshape_progress != MaxSector) {\n\t\tunsigned long before_length, after_length;\n\n\t\tbefore_length = ((1 << conf->prev.chunk_shift) *\n\t\t\t\t conf->prev.far_copies);\n\t\tafter_length = ((1 << conf->geo.chunk_shift) *\n\t\t\t\tconf->geo.far_copies);\n\n\t\tif (max(before_length, after_length) > min_offset_diff) {\n\t\t\t \n\t\t\tpr_warn(\"md/raid10: offset difference not enough to continue reshape\\n\");\n\t\t\tgoto out_free_conf;\n\t\t}\n\t\tconf->offset_diff = min_offset_diff;\n\n\t\tclear_bit(MD_RECOVERY_SYNC, &mddev->recovery);\n\t\tclear_bit(MD_RECOVERY_CHECK, &mddev->recovery);\n\t\tset_bit(MD_RECOVERY_RESHAPE, &mddev->recovery);\n\t\tset_bit(MD_RECOVERY_RUNNING, &mddev->recovery);\n\t\trcu_assign_pointer(mddev->sync_thread,\n\t\t\tmd_register_thread(md_do_sync, mddev, \"reshape\"));\n\t\tif (!mddev->sync_thread)\n\t\t\tgoto out_free_conf;\n\t}\n\n\treturn 0;\n\nout_free_conf:\n\tmd_unregister_thread(mddev, &mddev->thread);\n\traid10_free_conf(conf);\n\tmddev->private = NULL;\nout:\n\treturn -EIO;\n}\n\nstatic void raid10_free(struct mddev *mddev, void *priv)\n{\n\traid10_free_conf(priv);\n}\n\nstatic void raid10_quiesce(struct mddev *mddev, int quiesce)\n{\n\tstruct r10conf *conf = mddev->private;\n\n\tif (quiesce)\n\t\traise_barrier(conf, 0);\n\telse\n\t\tlower_barrier(conf);\n}\n\nstatic int raid10_resize(struct mddev *mddev, sector_t sectors)\n{\n\t \n\tstruct r10conf *conf = mddev->private;\n\tsector_t oldsize, size;\n\n\tif (mddev->reshape_position != MaxSector)\n\t\treturn -EBUSY;\n\n\tif (conf->geo.far_copies > 1 && !conf->geo.far_offset)\n\t\treturn -EINVAL;\n\n\toldsize = raid10_size(mddev, 0, 0);\n\tsize = raid10_size(mddev, sectors, 0);\n\tif (mddev->external_size &&\n\t    mddev->array_sectors > size)\n\t\treturn -EINVAL;\n\tif (mddev->bitmap) {\n\t\tint ret = md_bitmap_resize(mddev->bitmap, size, 0, 0);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\tmd_set_array_sectors(mddev, size);\n\tif (sectors > mddev->dev_sectors &&\n\t    mddev->recovery_cp > oldsize) {\n\t\tmddev->recovery_cp = oldsize;\n\t\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\t}\n\tcalc_sectors(conf, sectors);\n\tmddev->dev_sectors = conf->dev_sectors;\n\tmddev->resync_max_sectors = size;\n\treturn 0;\n}\n\nstatic void *raid10_takeover_raid0(struct mddev *mddev, sector_t size, int devs)\n{\n\tstruct md_rdev *rdev;\n\tstruct r10conf *conf;\n\n\tif (mddev->degraded > 0) {\n\t\tpr_warn(\"md/raid10:%s: Error: degraded raid0!\\n\",\n\t\t\tmdname(mddev));\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tsector_div(size, devs);\n\n\t \n\tmddev->new_level = 10;\n\t \n\tmddev->new_layout = (1<<8) + 2;\n\tmddev->new_chunk_sectors = mddev->chunk_sectors;\n\tmddev->delta_disks = mddev->raid_disks;\n\tmddev->raid_disks *= 2;\n\t \n\tmddev->recovery_cp = MaxSector;\n\tmddev->dev_sectors = size;\n\n\tconf = setup_conf(mddev);\n\tif (!IS_ERR(conf)) {\n\t\trdev_for_each(rdev, mddev)\n\t\t\tif (rdev->raid_disk >= 0) {\n\t\t\t\trdev->new_raid_disk = rdev->raid_disk * 2;\n\t\t\t\trdev->sectors = size;\n\t\t\t}\n\t}\n\n\treturn conf;\n}\n\nstatic void *raid10_takeover(struct mddev *mddev)\n{\n\tstruct r0conf *raid0_conf;\n\n\t \n\tif (mddev->level == 0) {\n\t\t \n\t\traid0_conf = mddev->private;\n\t\tif (raid0_conf->nr_strip_zones > 1) {\n\t\t\tpr_warn(\"md/raid10:%s: cannot takeover raid 0 with more than one zone.\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t\treturn raid10_takeover_raid0(mddev,\n\t\t\traid0_conf->strip_zone->zone_end,\n\t\t\traid0_conf->strip_zone->nb_dev);\n\t}\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic int raid10_check_reshape(struct mddev *mddev)\n{\n\t \n\tstruct r10conf *conf = mddev->private;\n\tstruct geom geo;\n\n\tif (conf->geo.far_copies != 1 && !conf->geo.far_offset)\n\t\treturn -EINVAL;\n\n\tif (setup_geo(&geo, mddev, geo_start) != conf->copies)\n\t\t \n\t\treturn -EINVAL;\n\tif (geo.far_copies > 1 && !geo.far_offset)\n\t\t \n\t\treturn -EINVAL;\n\n\tif (mddev->array_sectors & geo.chunk_mask)\n\t\t\t \n\t\t\treturn -EINVAL;\n\n\tif (!enough(conf, -1))\n\t\treturn -EINVAL;\n\n\tkfree(conf->mirrors_new);\n\tconf->mirrors_new = NULL;\n\tif (mddev->delta_disks > 0) {\n\t\t \n\t\tconf->mirrors_new =\n\t\t\tkcalloc(mddev->raid_disks + mddev->delta_disks,\n\t\t\t\tsizeof(struct raid10_info),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!conf->mirrors_new)\n\t\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\n \nstatic int calc_degraded(struct r10conf *conf)\n{\n\tint degraded, degraded2;\n\tint i;\n\n\trcu_read_lock();\n\tdegraded = 0;\n\t \n\tfor (i = 0; i < conf->prev.raid_disks; i++) {\n\t\tstruct md_rdev *rdev = rcu_dereference(conf->mirrors[i].rdev);\n\t\tif (!rdev || test_bit(Faulty, &rdev->flags))\n\t\t\tdegraded++;\n\t\telse if (!test_bit(In_sync, &rdev->flags))\n\t\t\t \n\t\t\tdegraded++;\n\t}\n\trcu_read_unlock();\n\tif (conf->geo.raid_disks == conf->prev.raid_disks)\n\t\treturn degraded;\n\trcu_read_lock();\n\tdegraded2 = 0;\n\tfor (i = 0; i < conf->geo.raid_disks; i++) {\n\t\tstruct md_rdev *rdev = rcu_dereference(conf->mirrors[i].rdev);\n\t\tif (!rdev || test_bit(Faulty, &rdev->flags))\n\t\t\tdegraded2++;\n\t\telse if (!test_bit(In_sync, &rdev->flags)) {\n\t\t\t \n\t\t\tif (conf->geo.raid_disks <= conf->prev.raid_disks)\n\t\t\t\tdegraded2++;\n\t\t}\n\t}\n\trcu_read_unlock();\n\tif (degraded2 > degraded)\n\t\treturn degraded2;\n\treturn degraded;\n}\n\nstatic int raid10_start_reshape(struct mddev *mddev)\n{\n\t \n\n\tunsigned long before_length, after_length;\n\tsector_t min_offset_diff = 0;\n\tint first = 1;\n\tstruct geom new;\n\tstruct r10conf *conf = mddev->private;\n\tstruct md_rdev *rdev;\n\tint spares = 0;\n\tint ret;\n\n\tif (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))\n\t\treturn -EBUSY;\n\n\tif (setup_geo(&new, mddev, geo_start) != conf->copies)\n\t\treturn -EINVAL;\n\n\tbefore_length = ((1 << conf->prev.chunk_shift) *\n\t\t\t conf->prev.far_copies);\n\tafter_length = ((1 << conf->geo.chunk_shift) *\n\t\t\tconf->geo.far_copies);\n\n\trdev_for_each(rdev, mddev) {\n\t\tif (!test_bit(In_sync, &rdev->flags)\n\t\t    && !test_bit(Faulty, &rdev->flags))\n\t\t\tspares++;\n\t\tif (rdev->raid_disk >= 0) {\n\t\t\tlong long diff = (rdev->new_data_offset\n\t\t\t\t\t  - rdev->data_offset);\n\t\t\tif (!mddev->reshape_backwards)\n\t\t\t\tdiff = -diff;\n\t\t\tif (diff < 0)\n\t\t\t\tdiff = 0;\n\t\t\tif (first || diff < min_offset_diff)\n\t\t\t\tmin_offset_diff = diff;\n\t\t\tfirst = 0;\n\t\t}\n\t}\n\n\tif (max(before_length, after_length) > min_offset_diff)\n\t\treturn -EINVAL;\n\n\tif (spares < mddev->delta_disks)\n\t\treturn -EINVAL;\n\n\tconf->offset_diff = min_offset_diff;\n\tspin_lock_irq(&conf->device_lock);\n\tif (conf->mirrors_new) {\n\t\tmemcpy(conf->mirrors_new, conf->mirrors,\n\t\t       sizeof(struct raid10_info)*conf->prev.raid_disks);\n\t\tsmp_mb();\n\t\tkfree(conf->mirrors_old);\n\t\tconf->mirrors_old = conf->mirrors;\n\t\tconf->mirrors = conf->mirrors_new;\n\t\tconf->mirrors_new = NULL;\n\t}\n\tsetup_geo(&conf->geo, mddev, geo_start);\n\tsmp_mb();\n\tif (mddev->reshape_backwards) {\n\t\tsector_t size = raid10_size(mddev, 0, 0);\n\t\tif (size < mddev->array_sectors) {\n\t\t\tspin_unlock_irq(&conf->device_lock);\n\t\t\tpr_warn(\"md/raid10:%s: array size must be reduce before number of disks\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmddev->resync_max_sectors = size;\n\t\tconf->reshape_progress = size;\n\t} else\n\t\tconf->reshape_progress = 0;\n\tconf->reshape_safe = conf->reshape_progress;\n\tspin_unlock_irq(&conf->device_lock);\n\n\tif (mddev->delta_disks && mddev->bitmap) {\n\t\tstruct mdp_superblock_1 *sb = NULL;\n\t\tsector_t oldsize, newsize;\n\n\t\toldsize = raid10_size(mddev, 0, 0);\n\t\tnewsize = raid10_size(mddev, 0, conf->geo.raid_disks);\n\n\t\tif (!mddev_is_clustered(mddev)) {\n\t\t\tret = md_bitmap_resize(mddev->bitmap, newsize, 0, 0);\n\t\t\tif (ret)\n\t\t\t\tgoto abort;\n\t\t\telse\n\t\t\t\tgoto out;\n\t\t}\n\n\t\trdev_for_each(rdev, mddev) {\n\t\t\tif (rdev->raid_disk > -1 &&\n\t\t\t    !test_bit(Faulty, &rdev->flags))\n\t\t\t\tsb = page_address(rdev->sb_page);\n\t\t}\n\n\t\t \n\t\tif ((sb && (le32_to_cpu(sb->feature_map) &\n\t\t\t    MD_FEATURE_RESHAPE_ACTIVE)) || (oldsize == newsize))\n\t\t\tgoto out;\n\n\t\tret = md_bitmap_resize(mddev->bitmap, newsize, 0, 0);\n\t\tif (ret)\n\t\t\tgoto abort;\n\n\t\tret = md_cluster_ops->resize_bitmaps(mddev, newsize, oldsize);\n\t\tif (ret) {\n\t\t\tmd_bitmap_resize(mddev->bitmap, oldsize, 0, 0);\n\t\t\tgoto abort;\n\t\t}\n\t}\nout:\n\tif (mddev->delta_disks > 0) {\n\t\trdev_for_each(rdev, mddev)\n\t\t\tif (rdev->raid_disk < 0 &&\n\t\t\t    !test_bit(Faulty, &rdev->flags)) {\n\t\t\t\tif (raid10_add_disk(mddev, rdev) == 0) {\n\t\t\t\t\tif (rdev->raid_disk >=\n\t\t\t\t\t    conf->prev.raid_disks)\n\t\t\t\t\t\tset_bit(In_sync, &rdev->flags);\n\t\t\t\t\telse\n\t\t\t\t\t\trdev->recovery_offset = 0;\n\n\t\t\t\t\t \n\t\t\t\t\tsysfs_link_rdev(mddev, rdev);\n\t\t\t\t}\n\t\t\t} else if (rdev->raid_disk >= conf->prev.raid_disks\n\t\t\t\t   && !test_bit(Faulty, &rdev->flags)) {\n\t\t\t\t \n\t\t\t\tset_bit(In_sync, &rdev->flags);\n\t\t\t}\n\t}\n\t \n\tspin_lock_irq(&conf->device_lock);\n\tmddev->degraded = calc_degraded(conf);\n\tspin_unlock_irq(&conf->device_lock);\n\tmddev->raid_disks = conf->geo.raid_disks;\n\tmddev->reshape_position = conf->reshape_progress;\n\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\n\tclear_bit(MD_RECOVERY_SYNC, &mddev->recovery);\n\tclear_bit(MD_RECOVERY_CHECK, &mddev->recovery);\n\tclear_bit(MD_RECOVERY_DONE, &mddev->recovery);\n\tset_bit(MD_RECOVERY_RESHAPE, &mddev->recovery);\n\tset_bit(MD_RECOVERY_RUNNING, &mddev->recovery);\n\n\trcu_assign_pointer(mddev->sync_thread,\n\t\t\t   md_register_thread(md_do_sync, mddev, \"reshape\"));\n\tif (!mddev->sync_thread) {\n\t\tret = -EAGAIN;\n\t\tgoto abort;\n\t}\n\tconf->reshape_checkpoint = jiffies;\n\tmd_wakeup_thread(mddev->sync_thread);\n\tmd_new_event();\n\treturn 0;\n\nabort:\n\tmddev->recovery = 0;\n\tspin_lock_irq(&conf->device_lock);\n\tconf->geo = conf->prev;\n\tmddev->raid_disks = conf->geo.raid_disks;\n\trdev_for_each(rdev, mddev)\n\t\trdev->new_data_offset = rdev->data_offset;\n\tsmp_wmb();\n\tconf->reshape_progress = MaxSector;\n\tconf->reshape_safe = MaxSector;\n\tmddev->reshape_position = MaxSector;\n\tspin_unlock_irq(&conf->device_lock);\n\treturn ret;\n}\n\n \nstatic sector_t last_dev_address(sector_t s, struct geom *geo)\n{\n\ts = (s | geo->chunk_mask) + 1;\n\ts >>= geo->chunk_shift;\n\ts *= geo->near_copies;\n\ts = DIV_ROUND_UP_SECTOR_T(s, geo->raid_disks);\n\ts *= geo->far_copies;\n\ts <<= geo->chunk_shift;\n\treturn s;\n}\n\n \nstatic sector_t first_dev_address(sector_t s, struct geom *geo)\n{\n\ts >>= geo->chunk_shift;\n\ts *= geo->near_copies;\n\tsector_div(s, geo->raid_disks);\n\ts *= geo->far_copies;\n\ts <<= geo->chunk_shift;\n\treturn s;\n}\n\nstatic sector_t reshape_request(struct mddev *mddev, sector_t sector_nr,\n\t\t\t\tint *skipped)\n{\n\t \n\tstruct r10conf *conf = mddev->private;\n\tstruct r10bio *r10_bio;\n\tsector_t next, safe, last;\n\tint max_sectors;\n\tint nr_sectors;\n\tint s;\n\tstruct md_rdev *rdev;\n\tint need_flush = 0;\n\tstruct bio *blist;\n\tstruct bio *bio, *read_bio;\n\tint sectors_done = 0;\n\tstruct page **pages;\n\n\tif (sector_nr == 0) {\n\t\t \n\t\tif (mddev->reshape_backwards &&\n\t\t    conf->reshape_progress < raid10_size(mddev, 0, 0)) {\n\t\t\tsector_nr = (raid10_size(mddev, 0, 0)\n\t\t\t\t     - conf->reshape_progress);\n\t\t} else if (!mddev->reshape_backwards &&\n\t\t\t   conf->reshape_progress > 0)\n\t\t\tsector_nr = conf->reshape_progress;\n\t\tif (sector_nr) {\n\t\t\tmddev->curr_resync_completed = sector_nr;\n\t\t\tsysfs_notify_dirent_safe(mddev->sysfs_completed);\n\t\t\t*skipped = 1;\n\t\t\treturn sector_nr;\n\t\t}\n\t}\n\n\t \n\tif (mddev->reshape_backwards) {\n\t\t \n\t\tnext = first_dev_address(conf->reshape_progress - 1,\n\t\t\t\t\t &conf->geo);\n\n\t\t \n\t\tsafe = last_dev_address(conf->reshape_safe - 1,\n\t\t\t\t\t&conf->prev);\n\n\t\tif (next + conf->offset_diff < safe)\n\t\t\tneed_flush = 1;\n\n\t\tlast = conf->reshape_progress - 1;\n\t\tsector_nr = last & ~(sector_t)(conf->geo.chunk_mask\n\t\t\t\t\t       & conf->prev.chunk_mask);\n\t\tif (sector_nr + RESYNC_SECTORS < last)\n\t\t\tsector_nr = last + 1 - RESYNC_SECTORS;\n\t} else {\n\t\t \n\t\tnext = last_dev_address(conf->reshape_progress, &conf->geo);\n\n\t\t \n\t\tsafe = first_dev_address(conf->reshape_safe, &conf->prev);\n\n\t\t \n\t\tif (next > safe + conf->offset_diff)\n\t\t\tneed_flush = 1;\n\n\t\tsector_nr = conf->reshape_progress;\n\t\tlast  = sector_nr | (conf->geo.chunk_mask\n\t\t\t\t     & conf->prev.chunk_mask);\n\n\t\tif (sector_nr + RESYNC_SECTORS <= last)\n\t\t\tlast = sector_nr + RESYNC_SECTORS - 1;\n\t}\n\n\tif (need_flush ||\n\t    time_after(jiffies, conf->reshape_checkpoint + 10*HZ)) {\n\t\t \n\t\twait_barrier(conf, false);\n\t\tmddev->reshape_position = conf->reshape_progress;\n\t\tif (mddev->reshape_backwards)\n\t\t\tmddev->curr_resync_completed = raid10_size(mddev, 0, 0)\n\t\t\t\t- conf->reshape_progress;\n\t\telse\n\t\t\tmddev->curr_resync_completed = conf->reshape_progress;\n\t\tconf->reshape_checkpoint = jiffies;\n\t\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\t\tmd_wakeup_thread(mddev->thread);\n\t\twait_event(mddev->sb_wait, mddev->sb_flags == 0 ||\n\t\t\t   test_bit(MD_RECOVERY_INTR, &mddev->recovery));\n\t\tif (test_bit(MD_RECOVERY_INTR, &mddev->recovery)) {\n\t\t\tallow_barrier(conf);\n\t\t\treturn sectors_done;\n\t\t}\n\t\tconf->reshape_safe = mddev->reshape_position;\n\t\tallow_barrier(conf);\n\t}\n\n\traise_barrier(conf, 0);\nread_more:\n\t \n\tr10_bio = raid10_alloc_init_r10buf(conf);\n\tr10_bio->state = 0;\n\traise_barrier(conf, 1);\n\tatomic_set(&r10_bio->remaining, 0);\n\tr10_bio->mddev = mddev;\n\tr10_bio->sector = sector_nr;\n\tset_bit(R10BIO_IsReshape, &r10_bio->state);\n\tr10_bio->sectors = last - sector_nr + 1;\n\trdev = read_balance(conf, r10_bio, &max_sectors);\n\tBUG_ON(!test_bit(R10BIO_Previous, &r10_bio->state));\n\n\tif (!rdev) {\n\t\t \n\t\t \n\t\tmempool_free(r10_bio, &conf->r10buf_pool);\n\t\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\t\treturn sectors_done;\n\t}\n\n\tread_bio = bio_alloc_bioset(rdev->bdev, RESYNC_PAGES, REQ_OP_READ,\n\t\t\t\t    GFP_KERNEL, &mddev->bio_set);\n\tread_bio->bi_iter.bi_sector = (r10_bio->devs[r10_bio->read_slot].addr\n\t\t\t       + rdev->data_offset);\n\tread_bio->bi_private = r10_bio;\n\tread_bio->bi_end_io = end_reshape_read;\n\tr10_bio->master_bio = read_bio;\n\tr10_bio->read_slot = r10_bio->devs[r10_bio->read_slot].devnum;\n\n\t \n\tif (mddev_is_clustered(mddev) && conf->cluster_sync_high <= sector_nr) {\n\t\tstruct mdp_superblock_1 *sb = NULL;\n\t\tint sb_reshape_pos = 0;\n\n\t\tconf->cluster_sync_low = sector_nr;\n\t\tconf->cluster_sync_high = sector_nr + CLUSTER_RESYNC_WINDOW_SECTORS;\n\t\tsb = page_address(rdev->sb_page);\n\t\tif (sb) {\n\t\t\tsb_reshape_pos = le64_to_cpu(sb->reshape_position);\n\t\t\t \n\t\t\tif (sb_reshape_pos < conf->cluster_sync_low)\n\t\t\t\tconf->cluster_sync_low = sb_reshape_pos;\n\t\t}\n\n\t\tmd_cluster_ops->resync_info_update(mddev, conf->cluster_sync_low,\n\t\t\t\t\t\t\t  conf->cluster_sync_high);\n\t}\n\n\t \n\t__raid10_find_phys(&conf->geo, r10_bio);\n\n\tblist = read_bio;\n\tread_bio->bi_next = NULL;\n\n\trcu_read_lock();\n\tfor (s = 0; s < conf->copies*2; s++) {\n\t\tstruct bio *b;\n\t\tint d = r10_bio->devs[s/2].devnum;\n\t\tstruct md_rdev *rdev2;\n\t\tif (s&1) {\n\t\t\trdev2 = rcu_dereference(conf->mirrors[d].replacement);\n\t\t\tb = r10_bio->devs[s/2].repl_bio;\n\t\t} else {\n\t\t\trdev2 = rcu_dereference(conf->mirrors[d].rdev);\n\t\t\tb = r10_bio->devs[s/2].bio;\n\t\t}\n\t\tif (!rdev2 || test_bit(Faulty, &rdev2->flags))\n\t\t\tcontinue;\n\n\t\tbio_set_dev(b, rdev2->bdev);\n\t\tb->bi_iter.bi_sector = r10_bio->devs[s/2].addr +\n\t\t\trdev2->new_data_offset;\n\t\tb->bi_end_io = end_reshape_write;\n\t\tb->bi_opf = REQ_OP_WRITE;\n\t\tb->bi_next = blist;\n\t\tblist = b;\n\t}\n\n\t \n\n\tnr_sectors = 0;\n\tpages = get_resync_pages(r10_bio->devs[0].bio)->pages;\n\tfor (s = 0 ; s < max_sectors; s += PAGE_SIZE >> 9) {\n\t\tstruct page *page = pages[s / (PAGE_SIZE >> 9)];\n\t\tint len = (max_sectors - s) << 9;\n\t\tif (len > PAGE_SIZE)\n\t\t\tlen = PAGE_SIZE;\n\t\tfor (bio = blist; bio ; bio = bio->bi_next) {\n\t\t\tif (WARN_ON(!bio_add_page(bio, page, len, 0))) {\n\t\t\t\tbio->bi_status = BLK_STS_RESOURCE;\n\t\t\t\tbio_endio(bio);\n\t\t\t\treturn sectors_done;\n\t\t\t}\n\t\t}\n\t\tsector_nr += len >> 9;\n\t\tnr_sectors += len >> 9;\n\t}\n\trcu_read_unlock();\n\tr10_bio->sectors = nr_sectors;\n\n\t \n\tmd_sync_acct_bio(read_bio, r10_bio->sectors);\n\tatomic_inc(&r10_bio->remaining);\n\tread_bio->bi_next = NULL;\n\tsubmit_bio_noacct(read_bio);\n\tsectors_done += nr_sectors;\n\tif (sector_nr <= last)\n\t\tgoto read_more;\n\n\tlower_barrier(conf);\n\n\t \n\tif (mddev->reshape_backwards)\n\t\tconf->reshape_progress -= sectors_done;\n\telse\n\t\tconf->reshape_progress += sectors_done;\n\n\treturn sectors_done;\n}\n\nstatic void end_reshape_request(struct r10bio *r10_bio);\nstatic int handle_reshape_read_error(struct mddev *mddev,\n\t\t\t\t     struct r10bio *r10_bio);\nstatic void reshape_request_write(struct mddev *mddev, struct r10bio *r10_bio)\n{\n\t \n\tstruct r10conf *conf = mddev->private;\n\tint s;\n\n\tif (!test_bit(R10BIO_Uptodate, &r10_bio->state))\n\t\tif (handle_reshape_read_error(mddev, r10_bio) < 0) {\n\t\t\t \n\t\t\tmd_done_sync(mddev, r10_bio->sectors, 0);\n\t\t\treturn;\n\t\t}\n\n\t \n\tatomic_set(&r10_bio->remaining, 1);\n\tfor (s = 0; s < conf->copies*2; s++) {\n\t\tstruct bio *b;\n\t\tint d = r10_bio->devs[s/2].devnum;\n\t\tstruct md_rdev *rdev;\n\t\trcu_read_lock();\n\t\tif (s&1) {\n\t\t\trdev = rcu_dereference(conf->mirrors[d].replacement);\n\t\t\tb = r10_bio->devs[s/2].repl_bio;\n\t\t} else {\n\t\t\trdev = rcu_dereference(conf->mirrors[d].rdev);\n\t\t\tb = r10_bio->devs[s/2].bio;\n\t\t}\n\t\tif (!rdev || test_bit(Faulty, &rdev->flags)) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\t\tatomic_inc(&rdev->nr_pending);\n\t\trcu_read_unlock();\n\t\tmd_sync_acct_bio(b, r10_bio->sectors);\n\t\tatomic_inc(&r10_bio->remaining);\n\t\tb->bi_next = NULL;\n\t\tsubmit_bio_noacct(b);\n\t}\n\tend_reshape_request(r10_bio);\n}\n\nstatic void end_reshape(struct r10conf *conf)\n{\n\tif (test_bit(MD_RECOVERY_INTR, &conf->mddev->recovery))\n\t\treturn;\n\n\tspin_lock_irq(&conf->device_lock);\n\tconf->prev = conf->geo;\n\tmd_finish_reshape(conf->mddev);\n\tsmp_wmb();\n\tconf->reshape_progress = MaxSector;\n\tconf->reshape_safe = MaxSector;\n\tspin_unlock_irq(&conf->device_lock);\n\n\tif (conf->mddev->queue)\n\t\traid10_set_io_opt(conf);\n\tconf->fullsync = 0;\n}\n\nstatic void raid10_update_reshape_pos(struct mddev *mddev)\n{\n\tstruct r10conf *conf = mddev->private;\n\tsector_t lo, hi;\n\n\tmd_cluster_ops->resync_info_get(mddev, &lo, &hi);\n\tif (((mddev->reshape_position <= hi) && (mddev->reshape_position >= lo))\n\t    || mddev->reshape_position == MaxSector)\n\t\tconf->reshape_progress = mddev->reshape_position;\n\telse\n\t\tWARN_ON_ONCE(1);\n}\n\nstatic int handle_reshape_read_error(struct mddev *mddev,\n\t\t\t\t     struct r10bio *r10_bio)\n{\n\t \n\tint sectors = r10_bio->sectors;\n\tstruct r10conf *conf = mddev->private;\n\tstruct r10bio *r10b;\n\tint slot = 0;\n\tint idx = 0;\n\tstruct page **pages;\n\n\tr10b = kmalloc(struct_size(r10b, devs, conf->copies), GFP_NOIO);\n\tif (!r10b) {\n\t\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tpages = get_resync_pages(r10_bio->devs[0].bio)->pages;\n\n\tr10b->sector = r10_bio->sector;\n\t__raid10_find_phys(&conf->prev, r10b);\n\n\twhile (sectors) {\n\t\tint s = sectors;\n\t\tint success = 0;\n\t\tint first_slot = slot;\n\n\t\tif (s > (PAGE_SIZE >> 9))\n\t\t\ts = PAGE_SIZE >> 9;\n\n\t\trcu_read_lock();\n\t\twhile (!success) {\n\t\t\tint d = r10b->devs[slot].devnum;\n\t\t\tstruct md_rdev *rdev = rcu_dereference(conf->mirrors[d].rdev);\n\t\t\tsector_t addr;\n\t\t\tif (rdev == NULL ||\n\t\t\t    test_bit(Faulty, &rdev->flags) ||\n\t\t\t    !test_bit(In_sync, &rdev->flags))\n\t\t\t\tgoto failed;\n\n\t\t\taddr = r10b->devs[slot].addr + idx * PAGE_SIZE;\n\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\trcu_read_unlock();\n\t\t\tsuccess = sync_page_io(rdev,\n\t\t\t\t\t       addr,\n\t\t\t\t\t       s << 9,\n\t\t\t\t\t       pages[idx],\n\t\t\t\t\t       REQ_OP_READ, false);\n\t\t\trdev_dec_pending(rdev, mddev);\n\t\t\trcu_read_lock();\n\t\t\tif (success)\n\t\t\t\tbreak;\n\t\tfailed:\n\t\t\tslot++;\n\t\t\tif (slot >= conf->copies)\n\t\t\t\tslot = 0;\n\t\t\tif (slot == first_slot)\n\t\t\t\tbreak;\n\t\t}\n\t\trcu_read_unlock();\n\t\tif (!success) {\n\t\t\t \n\t\t\tset_bit(MD_RECOVERY_INTR,\n\t\t\t\t&mddev->recovery);\n\t\t\tkfree(r10b);\n\t\t\treturn -EIO;\n\t\t}\n\t\tsectors -= s;\n\t\tidx++;\n\t}\n\tkfree(r10b);\n\treturn 0;\n}\n\nstatic void end_reshape_write(struct bio *bio)\n{\n\tstruct r10bio *r10_bio = get_resync_r10bio(bio);\n\tstruct mddev *mddev = r10_bio->mddev;\n\tstruct r10conf *conf = mddev->private;\n\tint d;\n\tint slot;\n\tint repl;\n\tstruct md_rdev *rdev = NULL;\n\n\td = find_bio_disk(conf, r10_bio, bio, &slot, &repl);\n\tif (repl)\n\t\trdev = conf->mirrors[d].replacement;\n\tif (!rdev) {\n\t\tsmp_mb();\n\t\trdev = conf->mirrors[d].rdev;\n\t}\n\n\tif (bio->bi_status) {\n\t\t \n\t\tmd_error(mddev, rdev);\n\t}\n\n\trdev_dec_pending(rdev, mddev);\n\tend_reshape_request(r10_bio);\n}\n\nstatic void end_reshape_request(struct r10bio *r10_bio)\n{\n\tif (!atomic_dec_and_test(&r10_bio->remaining))\n\t\treturn;\n\tmd_done_sync(r10_bio->mddev, r10_bio->sectors, 1);\n\tbio_put(r10_bio->master_bio);\n\tput_buf(r10_bio);\n}\n\nstatic void raid10_finish_reshape(struct mddev *mddev)\n{\n\tstruct r10conf *conf = mddev->private;\n\n\tif (test_bit(MD_RECOVERY_INTR, &mddev->recovery))\n\t\treturn;\n\n\tif (mddev->delta_disks > 0) {\n\t\tif (mddev->recovery_cp > mddev->resync_max_sectors) {\n\t\t\tmddev->recovery_cp = mddev->resync_max_sectors;\n\t\t\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\t\t}\n\t\tmddev->resync_max_sectors = mddev->array_sectors;\n\t} else {\n\t\tint d;\n\t\trcu_read_lock();\n\t\tfor (d = conf->geo.raid_disks ;\n\t\t     d < conf->geo.raid_disks - mddev->delta_disks;\n\t\t     d++) {\n\t\t\tstruct md_rdev *rdev = rcu_dereference(conf->mirrors[d].rdev);\n\t\t\tif (rdev)\n\t\t\t\tclear_bit(In_sync, &rdev->flags);\n\t\t\trdev = rcu_dereference(conf->mirrors[d].replacement);\n\t\t\tif (rdev)\n\t\t\t\tclear_bit(In_sync, &rdev->flags);\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\tmddev->layout = mddev->new_layout;\n\tmddev->chunk_sectors = 1 << conf->geo.chunk_shift;\n\tmddev->reshape_position = MaxSector;\n\tmddev->delta_disks = 0;\n\tmddev->reshape_backwards = 0;\n}\n\nstatic struct md_personality raid10_personality =\n{\n\t.name\t\t= \"raid10\",\n\t.level\t\t= 10,\n\t.owner\t\t= THIS_MODULE,\n\t.make_request\t= raid10_make_request,\n\t.run\t\t= raid10_run,\n\t.free\t\t= raid10_free,\n\t.status\t\t= raid10_status,\n\t.error_handler\t= raid10_error,\n\t.hot_add_disk\t= raid10_add_disk,\n\t.hot_remove_disk= raid10_remove_disk,\n\t.spare_active\t= raid10_spare_active,\n\t.sync_request\t= raid10_sync_request,\n\t.quiesce\t= raid10_quiesce,\n\t.size\t\t= raid10_size,\n\t.resize\t\t= raid10_resize,\n\t.takeover\t= raid10_takeover,\n\t.check_reshape\t= raid10_check_reshape,\n\t.start_reshape\t= raid10_start_reshape,\n\t.finish_reshape\t= raid10_finish_reshape,\n\t.update_reshape_pos = raid10_update_reshape_pos,\n};\n\nstatic int __init raid_init(void)\n{\n\treturn register_md_personality(&raid10_personality);\n}\n\nstatic void raid_exit(void)\n{\n\tunregister_md_personality(&raid10_personality);\n}\n\nmodule_init(raid_init);\nmodule_exit(raid_exit);\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"RAID10 (striped mirror) personality for MD\");\nMODULE_ALIAS(\"md-personality-9\");  \nMODULE_ALIAS(\"md-raid10\");\nMODULE_ALIAS(\"md-level-10\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}