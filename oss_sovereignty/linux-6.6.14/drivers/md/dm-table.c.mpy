{
  "module_name": "dm-table.c",
  "hash_id": "b230e45796f2239a8fe3de1207b1df0271a4d11298d6fc38b79bf5b01e44efd0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-table.c",
  "human_readable_source": "\n \n\n#include \"dm-core.h\"\n#include \"dm-rq.h\"\n\n#include <linux/module.h>\n#include <linux/vmalloc.h>\n#include <linux/blkdev.h>\n#include <linux/blk-integrity.h>\n#include <linux/namei.h>\n#include <linux/ctype.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/interrupt.h>\n#include <linux/mutex.h>\n#include <linux/delay.h>\n#include <linux/atomic.h>\n#include <linux/blk-mq.h>\n#include <linux/mount.h>\n#include <linux/dax.h>\n\n#define DM_MSG_PREFIX \"table\"\n\n#define NODE_SIZE L1_CACHE_BYTES\n#define KEYS_PER_NODE (NODE_SIZE / sizeof(sector_t))\n#define CHILDREN_PER_NODE (KEYS_PER_NODE + 1)\n\n \nstatic unsigned int int_log(unsigned int n, unsigned int base)\n{\n\tint result = 0;\n\n\twhile (n > 1) {\n\t\tn = dm_div_up(n, base);\n\t\tresult++;\n\t}\n\n\treturn result;\n}\n\n \nstatic inline unsigned int get_child(unsigned int n, unsigned int k)\n{\n\treturn (n * CHILDREN_PER_NODE) + k;\n}\n\n \nstatic inline sector_t *get_node(struct dm_table *t,\n\t\t\t\t unsigned int l, unsigned int n)\n{\n\treturn t->index[l] + (n * KEYS_PER_NODE);\n}\n\n \nstatic sector_t high(struct dm_table *t, unsigned int l, unsigned int n)\n{\n\tfor (; l < t->depth - 1; l++)\n\t\tn = get_child(n, CHILDREN_PER_NODE - 1);\n\n\tif (n >= t->counts[l])\n\t\treturn (sector_t) -1;\n\n\treturn get_node(t, l, n)[KEYS_PER_NODE - 1];\n}\n\n \nstatic int setup_btree_index(unsigned int l, struct dm_table *t)\n{\n\tunsigned int n, k;\n\tsector_t *node;\n\n\tfor (n = 0U; n < t->counts[l]; n++) {\n\t\tnode = get_node(t, l, n);\n\n\t\tfor (k = 0U; k < KEYS_PER_NODE; k++)\n\t\t\tnode[k] = high(t, l + 1, get_child(n, k));\n\t}\n\n\treturn 0;\n}\n\n \nstatic int alloc_targets(struct dm_table *t, unsigned int num)\n{\n\tsector_t *n_highs;\n\tstruct dm_target *n_targets;\n\n\t \n\tn_highs = kvcalloc(num, sizeof(struct dm_target) + sizeof(sector_t),\n\t\t\t   GFP_KERNEL);\n\tif (!n_highs)\n\t\treturn -ENOMEM;\n\n\tn_targets = (struct dm_target *) (n_highs + num);\n\n\tmemset(n_highs, -1, sizeof(*n_highs) * num);\n\tkvfree(t->highs);\n\n\tt->num_allocated = num;\n\tt->highs = n_highs;\n\tt->targets = n_targets;\n\n\treturn 0;\n}\n\nint dm_table_create(struct dm_table **result, blk_mode_t mode,\n\t\t    unsigned int num_targets, struct mapped_device *md)\n{\n\tstruct dm_table *t = kzalloc(sizeof(*t), GFP_KERNEL);\n\n\tif (!t)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&t->devices);\n\tinit_rwsem(&t->devices_lock);\n\n\tif (!num_targets)\n\t\tnum_targets = KEYS_PER_NODE;\n\n\tnum_targets = dm_round_up(num_targets, KEYS_PER_NODE);\n\n\tif (!num_targets) {\n\t\tkfree(t);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (alloc_targets(t, num_targets)) {\n\t\tkfree(t);\n\t\treturn -ENOMEM;\n\t}\n\n\tt->type = DM_TYPE_NONE;\n\tt->mode = mode;\n\tt->md = md;\n\t*result = t;\n\treturn 0;\n}\n\nstatic void free_devices(struct list_head *devices, struct mapped_device *md)\n{\n\tstruct list_head *tmp, *next;\n\n\tlist_for_each_safe(tmp, next, devices) {\n\t\tstruct dm_dev_internal *dd =\n\t\t    list_entry(tmp, struct dm_dev_internal, list);\n\t\tDMWARN(\"%s: dm_table_destroy: dm_put_device call missing for %s\",\n\t\t       dm_device_name(md), dd->dm_dev->name);\n\t\tdm_put_table_device(md, dd->dm_dev);\n\t\tkfree(dd);\n\t}\n}\n\nstatic void dm_table_destroy_crypto_profile(struct dm_table *t);\n\nvoid dm_table_destroy(struct dm_table *t)\n{\n\tif (!t)\n\t\treturn;\n\n\t \n\tif (t->depth >= 2)\n\t\tkvfree(t->index[t->depth - 2]);\n\n\t \n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tif (ti->type->dtr)\n\t\t\tti->type->dtr(ti);\n\n\t\tdm_put_target_type(ti->type);\n\t}\n\n\tkvfree(t->highs);\n\n\t \n\tfree_devices(&t->devices, t->md);\n\n\tdm_free_md_mempools(t->mempools);\n\n\tdm_table_destroy_crypto_profile(t);\n\n\tkfree(t);\n}\n\n \nstatic struct dm_dev_internal *find_device(struct list_head *l, dev_t dev)\n{\n\tstruct dm_dev_internal *dd;\n\n\tlist_for_each_entry(dd, l, list)\n\t\tif (dd->dm_dev->bdev->bd_dev == dev)\n\t\t\treturn dd;\n\n\treturn NULL;\n}\n\n \nstatic int device_area_is_invalid(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t\t  sector_t start, sector_t len, void *data)\n{\n\tstruct queue_limits *limits = data;\n\tstruct block_device *bdev = dev->bdev;\n\tsector_t dev_size = bdev_nr_sectors(bdev);\n\tunsigned short logical_block_size_sectors =\n\t\tlimits->logical_block_size >> SECTOR_SHIFT;\n\n\tif (!dev_size)\n\t\treturn 0;\n\n\tif ((start >= dev_size) || (start + len > dev_size)) {\n\t\tDMERR(\"%s: %pg too small for target: start=%llu, len=%llu, dev_size=%llu\",\n\t\t      dm_device_name(ti->table->md), bdev,\n\t\t      (unsigned long long)start,\n\t\t      (unsigned long long)len,\n\t\t      (unsigned long long)dev_size);\n\t\treturn 1;\n\t}\n\n\t \n\tif (bdev_is_zoned(bdev)) {\n\t\tunsigned int zone_sectors = bdev_zone_sectors(bdev);\n\n\t\tif (start & (zone_sectors - 1)) {\n\t\t\tDMERR(\"%s: start=%llu not aligned to h/w zone size %u of %pg\",\n\t\t\t      dm_device_name(ti->table->md),\n\t\t\t      (unsigned long long)start,\n\t\t\t      zone_sectors, bdev);\n\t\t\treturn 1;\n\t\t}\n\n\t\t \n\t\tif (len & (zone_sectors - 1)) {\n\t\t\tDMERR(\"%s: len=%llu not aligned to h/w zone size %u of %pg\",\n\t\t\t      dm_device_name(ti->table->md),\n\t\t\t      (unsigned long long)len,\n\t\t\t      zone_sectors, bdev);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tif (logical_block_size_sectors <= 1)\n\t\treturn 0;\n\n\tif (start & (logical_block_size_sectors - 1)) {\n\t\tDMERR(\"%s: start=%llu not aligned to h/w logical block size %u of %pg\",\n\t\t      dm_device_name(ti->table->md),\n\t\t      (unsigned long long)start,\n\t\t      limits->logical_block_size, bdev);\n\t\treturn 1;\n\t}\n\n\tif (len & (logical_block_size_sectors - 1)) {\n\t\tDMERR(\"%s: len=%llu not aligned to h/w logical block size %u of %pg\",\n\t\t      dm_device_name(ti->table->md),\n\t\t      (unsigned long long)len,\n\t\t      limits->logical_block_size, bdev);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int upgrade_mode(struct dm_dev_internal *dd, blk_mode_t new_mode,\n\t\t\tstruct mapped_device *md)\n{\n\tint r;\n\tstruct dm_dev *old_dev, *new_dev;\n\n\told_dev = dd->dm_dev;\n\n\tr = dm_get_table_device(md, dd->dm_dev->bdev->bd_dev,\n\t\t\t\tdd->dm_dev->mode | new_mode, &new_dev);\n\tif (r)\n\t\treturn r;\n\n\tdd->dm_dev = new_dev;\n\tdm_put_table_device(md, old_dev);\n\n\treturn 0;\n}\n\n \nint __ref dm_get_device(struct dm_target *ti, const char *path, blk_mode_t mode,\n\t\t  struct dm_dev **result)\n{\n\tint r;\n\tdev_t dev;\n\tunsigned int major, minor;\n\tchar dummy;\n\tstruct dm_dev_internal *dd;\n\tstruct dm_table *t = ti->table;\n\n\tBUG_ON(!t);\n\n\tif (sscanf(path, \"%u:%u%c\", &major, &minor, &dummy) == 2) {\n\t\t \n\t\tdev = MKDEV(major, minor);\n\t\tif (MAJOR(dev) != major || MINOR(dev) != minor)\n\t\t\treturn -EOVERFLOW;\n\t} else {\n\t\tr = lookup_bdev(path, &dev);\n#ifndef MODULE\n\t\tif (r && system_state < SYSTEM_RUNNING)\n\t\t\tr = early_lookup_bdev(path, &dev);\n#endif\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\tif (dev == disk_devt(t->md->disk))\n\t\treturn -EINVAL;\n\n\tdown_write(&t->devices_lock);\n\n\tdd = find_device(&t->devices, dev);\n\tif (!dd) {\n\t\tdd = kmalloc(sizeof(*dd), GFP_KERNEL);\n\t\tif (!dd) {\n\t\t\tr = -ENOMEM;\n\t\t\tgoto unlock_ret_r;\n\t\t}\n\n\t\tr = dm_get_table_device(t->md, dev, mode, &dd->dm_dev);\n\t\tif (r) {\n\t\t\tkfree(dd);\n\t\t\tgoto unlock_ret_r;\n\t\t}\n\n\t\trefcount_set(&dd->count, 1);\n\t\tlist_add(&dd->list, &t->devices);\n\t\tgoto out;\n\n\t} else if (dd->dm_dev->mode != (mode | dd->dm_dev->mode)) {\n\t\tr = upgrade_mode(dd, mode, t->md);\n\t\tif (r)\n\t\t\tgoto unlock_ret_r;\n\t}\n\trefcount_inc(&dd->count);\nout:\n\tup_write(&t->devices_lock);\n\t*result = dd->dm_dev;\n\treturn 0;\n\nunlock_ret_r:\n\tup_write(&t->devices_lock);\n\treturn r;\n}\nEXPORT_SYMBOL(dm_get_device);\n\nstatic int dm_set_device_limits(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t\tsector_t start, sector_t len, void *data)\n{\n\tstruct queue_limits *limits = data;\n\tstruct block_device *bdev = dev->bdev;\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\tif (unlikely(!q)) {\n\t\tDMWARN(\"%s: Cannot set limits for nonexistent device %pg\",\n\t\t       dm_device_name(ti->table->md), bdev);\n\t\treturn 0;\n\t}\n\n\tif (blk_stack_limits(limits, &q->limits,\n\t\t\tget_start_sect(bdev) + start) < 0)\n\t\tDMWARN(\"%s: adding target device %pg caused an alignment inconsistency: \"\n\t\t       \"physical_block_size=%u, logical_block_size=%u, \"\n\t\t       \"alignment_offset=%u, start=%llu\",\n\t\t       dm_device_name(ti->table->md), bdev,\n\t\t       q->limits.physical_block_size,\n\t\t       q->limits.logical_block_size,\n\t\t       q->limits.alignment_offset,\n\t\t       (unsigned long long) start << SECTOR_SHIFT);\n\treturn 0;\n}\n\n \nvoid dm_put_device(struct dm_target *ti, struct dm_dev *d)\n{\n\tint found = 0;\n\tstruct dm_table *t = ti->table;\n\tstruct list_head *devices = &t->devices;\n\tstruct dm_dev_internal *dd;\n\n\tdown_write(&t->devices_lock);\n\n\tlist_for_each_entry(dd, devices, list) {\n\t\tif (dd->dm_dev == d) {\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (!found) {\n\t\tDMERR(\"%s: device %s not in table devices list\",\n\t\t      dm_device_name(t->md), d->name);\n\t\tgoto unlock_ret;\n\t}\n\tif (refcount_dec_and_test(&dd->count)) {\n\t\tdm_put_table_device(t->md, d);\n\t\tlist_del(&dd->list);\n\t\tkfree(dd);\n\t}\n\nunlock_ret:\n\tup_write(&t->devices_lock);\n}\nEXPORT_SYMBOL(dm_put_device);\n\n \nstatic int adjoin(struct dm_table *t, struct dm_target *ti)\n{\n\tstruct dm_target *prev;\n\n\tif (!t->num_targets)\n\t\treturn !ti->begin;\n\n\tprev = &t->targets[t->num_targets - 1];\n\treturn (ti->begin == (prev->begin + prev->len));\n}\n\n \nstatic char **realloc_argv(unsigned int *size, char **old_argv)\n{\n\tchar **argv;\n\tunsigned int new_size;\n\tgfp_t gfp;\n\n\tif (*size) {\n\t\tnew_size = *size * 2;\n\t\tgfp = GFP_KERNEL;\n\t} else {\n\t\tnew_size = 8;\n\t\tgfp = GFP_NOIO;\n\t}\n\targv = kmalloc_array(new_size, sizeof(*argv), gfp);\n\tif (argv && old_argv) {\n\t\tmemcpy(argv, old_argv, *size * sizeof(*argv));\n\t\t*size = new_size;\n\t}\n\n\tkfree(old_argv);\n\treturn argv;\n}\n\n \nint dm_split_args(int *argc, char ***argvp, char *input)\n{\n\tchar *start, *end = input, *out, **argv = NULL;\n\tunsigned int array_size = 0;\n\n\t*argc = 0;\n\n\tif (!input) {\n\t\t*argvp = NULL;\n\t\treturn 0;\n\t}\n\n\targv = realloc_argv(&array_size, argv);\n\tif (!argv)\n\t\treturn -ENOMEM;\n\n\twhile (1) {\n\t\t \n\t\tstart = skip_spaces(end);\n\n\t\tif (!*start)\n\t\t\tbreak;\t \n\n\t\t \n\t\tend = out = start;\n\t\twhile (*end) {\n\t\t\t \n\t\t\tif (*end == '\\\\' && *(end + 1)) {\n\t\t\t\t*out++ = *(end + 1);\n\t\t\t\tend += 2;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (isspace(*end))\n\t\t\t\tbreak;\t \n\n\t\t\t*out++ = *end++;\n\t\t}\n\n\t\t \n\t\tif ((*argc + 1) > array_size) {\n\t\t\targv = realloc_argv(&array_size, argv);\n\t\t\tif (!argv)\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\t \n\t\tif (*end)\n\t\t\tend++;\n\n\t\t \n\t\t*out = '\\0';\n\t\targv[*argc] = start;\n\t\t(*argc)++;\n\t}\n\n\t*argvp = argv;\n\treturn 0;\n}\n\n \nstatic int validate_hardware_logical_block_alignment(struct dm_table *t,\n\t\t\t\t\t\t     struct queue_limits *limits)\n{\n\t \n\tunsigned short device_logical_block_size_sects =\n\t\tlimits->logical_block_size >> SECTOR_SHIFT;\n\n\t \n\tunsigned short next_target_start = 0;\n\n\t \n\tunsigned short remaining = 0;\n\n\tstruct dm_target *ti;\n\tstruct queue_limits ti_limits;\n\tunsigned int i;\n\n\t \n\tfor (i = 0; i < t->num_targets; i++) {\n\t\tti = dm_table_get_target(t, i);\n\n\t\tblk_set_stacking_limits(&ti_limits);\n\n\t\t \n\t\tif (ti->type->iterate_devices)\n\t\t\tti->type->iterate_devices(ti, dm_set_device_limits,\n\t\t\t\t\t\t  &ti_limits);\n\n\t\t \n\t\tif (remaining < ti->len &&\n\t\t    remaining & ((ti_limits.logical_block_size >>\n\t\t\t\t  SECTOR_SHIFT) - 1))\n\t\t\tbreak;\t \n\n\t\tnext_target_start =\n\t\t    (unsigned short) ((next_target_start + ti->len) &\n\t\t\t\t      (device_logical_block_size_sects - 1));\n\t\tremaining = next_target_start ?\n\t\t    device_logical_block_size_sects - next_target_start : 0;\n\t}\n\n\tif (remaining) {\n\t\tDMERR(\"%s: table line %u (start sect %llu len %llu) \"\n\t\t      \"not aligned to h/w logical block size %u\",\n\t\t      dm_device_name(t->md), i,\n\t\t      (unsigned long long) ti->begin,\n\t\t      (unsigned long long) ti->len,\n\t\t      limits->logical_block_size);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint dm_table_add_target(struct dm_table *t, const char *type,\n\t\t\tsector_t start, sector_t len, char *params)\n{\n\tint r = -EINVAL, argc;\n\tchar **argv;\n\tstruct dm_target *ti;\n\n\tif (t->singleton) {\n\t\tDMERR(\"%s: target type %s must appear alone in table\",\n\t\t      dm_device_name(t->md), t->targets->type->name);\n\t\treturn -EINVAL;\n\t}\n\n\tBUG_ON(t->num_targets >= t->num_allocated);\n\n\tti = t->targets + t->num_targets;\n\tmemset(ti, 0, sizeof(*ti));\n\n\tif (!len) {\n\t\tDMERR(\"%s: zero-length target\", dm_device_name(t->md));\n\t\treturn -EINVAL;\n\t}\n\n\tti->type = dm_get_target_type(type);\n\tif (!ti->type) {\n\t\tDMERR(\"%s: %s: unknown target type\", dm_device_name(t->md), type);\n\t\treturn -EINVAL;\n\t}\n\n\tif (dm_target_needs_singleton(ti->type)) {\n\t\tif (t->num_targets) {\n\t\t\tti->error = \"singleton target type must appear alone in table\";\n\t\t\tgoto bad;\n\t\t}\n\t\tt->singleton = true;\n\t}\n\n\tif (dm_target_always_writeable(ti->type) &&\n\t    !(t->mode & BLK_OPEN_WRITE)) {\n\t\tti->error = \"target type may not be included in a read-only table\";\n\t\tgoto bad;\n\t}\n\n\tif (t->immutable_target_type) {\n\t\tif (t->immutable_target_type != ti->type) {\n\t\t\tti->error = \"immutable target type cannot be mixed with other target types\";\n\t\t\tgoto bad;\n\t\t}\n\t} else if (dm_target_is_immutable(ti->type)) {\n\t\tif (t->num_targets) {\n\t\t\tti->error = \"immutable target type cannot be mixed with other target types\";\n\t\t\tgoto bad;\n\t\t}\n\t\tt->immutable_target_type = ti->type;\n\t}\n\n\tif (dm_target_has_integrity(ti->type))\n\t\tt->integrity_added = 1;\n\n\tti->table = t;\n\tti->begin = start;\n\tti->len = len;\n\tti->error = \"Unknown error\";\n\n\t \n\tif (!adjoin(t, ti)) {\n\t\tti->error = \"Gap in table\";\n\t\tgoto bad;\n\t}\n\n\tr = dm_split_args(&argc, &argv, params);\n\tif (r) {\n\t\tti->error = \"couldn't split parameters\";\n\t\tgoto bad;\n\t}\n\n\tr = ti->type->ctr(ti, argc, argv);\n\tkfree(argv);\n\tif (r)\n\t\tgoto bad;\n\n\tt->highs[t->num_targets++] = ti->begin + ti->len - 1;\n\n\tif (!ti->num_discard_bios && ti->discards_supported)\n\t\tDMWARN(\"%s: %s: ignoring discards_supported because num_discard_bios is zero.\",\n\t\t       dm_device_name(t->md), type);\n\n\tif (ti->limit_swap_bios && !static_key_enabled(&swap_bios_enabled.key))\n\t\tstatic_branch_enable(&swap_bios_enabled);\n\n\treturn 0;\n\n bad:\n\tDMERR(\"%s: %s: %s (%pe)\", dm_device_name(t->md), type, ti->error, ERR_PTR(r));\n\tdm_put_target_type(ti->type);\n\treturn r;\n}\n\n \nstatic int validate_next_arg(const struct dm_arg *arg, struct dm_arg_set *arg_set,\n\t\t\t     unsigned int *value, char **error, unsigned int grouped)\n{\n\tconst char *arg_str = dm_shift_arg(arg_set);\n\tchar dummy;\n\n\tif (!arg_str ||\n\t    (sscanf(arg_str, \"%u%c\", value, &dummy) != 1) ||\n\t    (*value < arg->min) ||\n\t    (*value > arg->max) ||\n\t    (grouped && arg_set->argc < *value)) {\n\t\t*error = arg->error;\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint dm_read_arg(const struct dm_arg *arg, struct dm_arg_set *arg_set,\n\t\tunsigned int *value, char **error)\n{\n\treturn validate_next_arg(arg, arg_set, value, error, 0);\n}\nEXPORT_SYMBOL(dm_read_arg);\n\nint dm_read_arg_group(const struct dm_arg *arg, struct dm_arg_set *arg_set,\n\t\t      unsigned int *value, char **error)\n{\n\treturn validate_next_arg(arg, arg_set, value, error, 1);\n}\nEXPORT_SYMBOL(dm_read_arg_group);\n\nconst char *dm_shift_arg(struct dm_arg_set *as)\n{\n\tchar *r;\n\n\tif (as->argc) {\n\t\tas->argc--;\n\t\tr = *as->argv;\n\t\tas->argv++;\n\t\treturn r;\n\t}\n\n\treturn NULL;\n}\nEXPORT_SYMBOL(dm_shift_arg);\n\nvoid dm_consume_args(struct dm_arg_set *as, unsigned int num_args)\n{\n\tBUG_ON(as->argc < num_args);\n\tas->argc -= num_args;\n\tas->argv += num_args;\n}\nEXPORT_SYMBOL(dm_consume_args);\n\nstatic bool __table_type_bio_based(enum dm_queue_mode table_type)\n{\n\treturn (table_type == DM_TYPE_BIO_BASED ||\n\t\ttable_type == DM_TYPE_DAX_BIO_BASED);\n}\n\nstatic bool __table_type_request_based(enum dm_queue_mode table_type)\n{\n\treturn table_type == DM_TYPE_REQUEST_BASED;\n}\n\nvoid dm_table_set_type(struct dm_table *t, enum dm_queue_mode type)\n{\n\tt->type = type;\n}\nEXPORT_SYMBOL_GPL(dm_table_set_type);\n\n \nstatic int device_not_dax_capable(struct dm_target *ti, struct dm_dev *dev,\n\t\t\tsector_t start, sector_t len, void *data)\n{\n\tif (dev->dax_dev)\n\t\treturn false;\n\n\tDMDEBUG(\"%pg: error: dax unsupported by block device\", dev->bdev);\n\treturn true;\n}\n\n \nstatic int device_not_dax_synchronous_capable(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t\t\t      sector_t start, sector_t len, void *data)\n{\n\treturn !dev->dax_dev || !dax_synchronous(dev->dax_dev);\n}\n\nstatic bool dm_table_supports_dax(struct dm_table *t,\n\t\t\t\t  iterate_devices_callout_fn iterate_fn)\n{\n\t \n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tif (!ti->type->direct_access)\n\t\t\treturn false;\n\n\t\tif (!ti->type->iterate_devices ||\n\t\t    ti->type->iterate_devices(ti, iterate_fn, NULL))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int device_is_rq_stackable(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t\t  sector_t start, sector_t len, void *data)\n{\n\tstruct block_device *bdev = dev->bdev;\n\tstruct request_queue *q = bdev_get_queue(bdev);\n\n\t \n\tif (bdev_is_partition(bdev))\n\t\treturn false;\n\n\treturn queue_is_mq(q);\n}\n\nstatic int dm_table_determine_type(struct dm_table *t)\n{\n\tunsigned int bio_based = 0, request_based = 0, hybrid = 0;\n\tstruct dm_target *ti;\n\tstruct list_head *devices = dm_table_get_devices(t);\n\tenum dm_queue_mode live_md_type = dm_get_md_type(t->md);\n\n\tif (t->type != DM_TYPE_NONE) {\n\t\t \n\t\tif (t->type == DM_TYPE_BIO_BASED) {\n\t\t\t \n\t\t\tgoto verify_bio_based;\n\t\t}\n\t\tBUG_ON(t->type == DM_TYPE_DAX_BIO_BASED);\n\t\tgoto verify_rq_based;\n\t}\n\n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tti = dm_table_get_target(t, i);\n\t\tif (dm_target_hybrid(ti))\n\t\t\thybrid = 1;\n\t\telse if (dm_target_request_based(ti))\n\t\t\trequest_based = 1;\n\t\telse\n\t\t\tbio_based = 1;\n\n\t\tif (bio_based && request_based) {\n\t\t\tDMERR(\"Inconsistent table: different target types can't be mixed up\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (hybrid && !bio_based && !request_based) {\n\t\t \n\t\tif (__table_type_request_based(live_md_type))\n\t\t\trequest_based = 1;\n\t\telse\n\t\t\tbio_based = 1;\n\t}\n\n\tif (bio_based) {\nverify_bio_based:\n\t\t \n\t\tt->type = DM_TYPE_BIO_BASED;\n\t\tif (dm_table_supports_dax(t, device_not_dax_capable) ||\n\t\t    (list_empty(devices) && live_md_type == DM_TYPE_DAX_BIO_BASED)) {\n\t\t\tt->type = DM_TYPE_DAX_BIO_BASED;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tBUG_ON(!request_based);  \n\n\tt->type = DM_TYPE_REQUEST_BASED;\n\nverify_rq_based:\n\t \n\tif (t->num_targets > 1) {\n\t\tDMERR(\"request-based DM doesn't support multiple targets\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (list_empty(devices)) {\n\t\tint srcu_idx;\n\t\tstruct dm_table *live_table = dm_get_live_table(t->md, &srcu_idx);\n\n\t\t \n\t\tif (live_table)\n\t\t\tt->type = live_table->type;\n\t\tdm_put_live_table(t->md, srcu_idx);\n\t\treturn 0;\n\t}\n\n\tti = dm_table_get_immutable_target(t);\n\tif (!ti) {\n\t\tDMERR(\"table load rejected: immutable target is required\");\n\t\treturn -EINVAL;\n\t} else if (ti->max_io_len) {\n\t\tDMERR(\"table load rejected: immutable target that splits IO is not supported\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (!ti->type->iterate_devices ||\n\t    !ti->type->iterate_devices(ti, device_is_rq_stackable, NULL)) {\n\t\tDMERR(\"table load rejected: including non-request-stackable devices\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nenum dm_queue_mode dm_table_get_type(struct dm_table *t)\n{\n\treturn t->type;\n}\n\nstruct target_type *dm_table_get_immutable_target_type(struct dm_table *t)\n{\n\treturn t->immutable_target_type;\n}\n\nstruct dm_target *dm_table_get_immutable_target(struct dm_table *t)\n{\n\t \n\tif (t->num_targets > 1 ||\n\t    !dm_target_is_immutable(t->targets[0].type))\n\t\treturn NULL;\n\n\treturn t->targets;\n}\n\nstruct dm_target *dm_table_get_wildcard_target(struct dm_table *t)\n{\n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tif (dm_target_is_wildcard(ti->type))\n\t\t\treturn ti;\n\t}\n\n\treturn NULL;\n}\n\nbool dm_table_bio_based(struct dm_table *t)\n{\n\treturn __table_type_bio_based(dm_table_get_type(t));\n}\n\nbool dm_table_request_based(struct dm_table *t)\n{\n\treturn __table_type_request_based(dm_table_get_type(t));\n}\n\nstatic bool dm_table_supports_poll(struct dm_table *t);\n\nstatic int dm_table_alloc_md_mempools(struct dm_table *t, struct mapped_device *md)\n{\n\tenum dm_queue_mode type = dm_table_get_type(t);\n\tunsigned int per_io_data_size = 0, front_pad, io_front_pad;\n\tunsigned int min_pool_size = 0, pool_size;\n\tstruct dm_md_mempools *pools;\n\n\tif (unlikely(type == DM_TYPE_NONE)) {\n\t\tDMERR(\"no table type is set, can't allocate mempools\");\n\t\treturn -EINVAL;\n\t}\n\n\tpools = kzalloc_node(sizeof(*pools), GFP_KERNEL, md->numa_node_id);\n\tif (!pools)\n\t\treturn -ENOMEM;\n\n\tif (type == DM_TYPE_REQUEST_BASED) {\n\t\tpool_size = dm_get_reserved_rq_based_ios();\n\t\tfront_pad = offsetof(struct dm_rq_clone_bio_info, clone);\n\t\tgoto init_bs;\n\t}\n\n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tper_io_data_size = max(per_io_data_size, ti->per_io_data_size);\n\t\tmin_pool_size = max(min_pool_size, ti->num_flush_bios);\n\t}\n\tpool_size = max(dm_get_reserved_bio_based_ios(), min_pool_size);\n\tfront_pad = roundup(per_io_data_size,\n\t\t__alignof__(struct dm_target_io)) + DM_TARGET_IO_BIO_OFFSET;\n\n\tio_front_pad = roundup(per_io_data_size,\n\t\t__alignof__(struct dm_io)) + DM_IO_BIO_OFFSET;\n\tif (bioset_init(&pools->io_bs, pool_size, io_front_pad,\n\t\t\tdm_table_supports_poll(t) ? BIOSET_PERCPU_CACHE : 0))\n\t\tgoto out_free_pools;\n\tif (t->integrity_supported &&\n\t    bioset_integrity_create(&pools->io_bs, pool_size))\n\t\tgoto out_free_pools;\ninit_bs:\n\tif (bioset_init(&pools->bs, pool_size, front_pad, 0))\n\t\tgoto out_free_pools;\n\tif (t->integrity_supported &&\n\t    bioset_integrity_create(&pools->bs, pool_size))\n\t\tgoto out_free_pools;\n\n\tt->mempools = pools;\n\treturn 0;\n\nout_free_pools:\n\tdm_free_md_mempools(pools);\n\treturn -ENOMEM;\n}\n\nstatic int setup_indexes(struct dm_table *t)\n{\n\tint i;\n\tunsigned int total = 0;\n\tsector_t *indexes;\n\n\t \n\tfor (i = t->depth - 2; i >= 0; i--) {\n\t\tt->counts[i] = dm_div_up(t->counts[i + 1], CHILDREN_PER_NODE);\n\t\ttotal += t->counts[i];\n\t}\n\n\tindexes = kvcalloc(total, NODE_SIZE, GFP_KERNEL);\n\tif (!indexes)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (i = t->depth - 2; i >= 0; i--) {\n\t\tt->index[i] = indexes;\n\t\tindexes += (KEYS_PER_NODE * t->counts[i]);\n\t\tsetup_btree_index(i, t);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int dm_table_build_index(struct dm_table *t)\n{\n\tint r = 0;\n\tunsigned int leaf_nodes;\n\n\t \n\tleaf_nodes = dm_div_up(t->num_targets, KEYS_PER_NODE);\n\tt->depth = 1 + int_log(leaf_nodes, CHILDREN_PER_NODE);\n\n\t \n\tt->counts[t->depth - 1] = leaf_nodes;\n\tt->index[t->depth - 1] = t->highs;\n\n\tif (t->depth >= 2)\n\t\tr = setup_indexes(t);\n\n\treturn r;\n}\n\nstatic bool integrity_profile_exists(struct gendisk *disk)\n{\n\treturn !!blk_get_integrity(disk);\n}\n\n \nstatic struct gendisk *dm_table_get_integrity_disk(struct dm_table *t)\n{\n\tstruct list_head *devices = dm_table_get_devices(t);\n\tstruct dm_dev_internal *dd = NULL;\n\tstruct gendisk *prev_disk = NULL, *template_disk = NULL;\n\n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tif (!dm_target_passes_integrity(ti->type))\n\t\t\tgoto no_integrity;\n\t}\n\n\tlist_for_each_entry(dd, devices, list) {\n\t\ttemplate_disk = dd->dm_dev->bdev->bd_disk;\n\t\tif (!integrity_profile_exists(template_disk))\n\t\t\tgoto no_integrity;\n\t\telse if (prev_disk &&\n\t\t\t blk_integrity_compare(prev_disk, template_disk) < 0)\n\t\t\tgoto no_integrity;\n\t\tprev_disk = template_disk;\n\t}\n\n\treturn template_disk;\n\nno_integrity:\n\tif (prev_disk)\n\t\tDMWARN(\"%s: integrity not set: %s and %s profile mismatch\",\n\t\t       dm_device_name(t->md),\n\t\t       prev_disk->disk_name,\n\t\t       template_disk->disk_name);\n\treturn NULL;\n}\n\n \nstatic int dm_table_register_integrity(struct dm_table *t)\n{\n\tstruct mapped_device *md = t->md;\n\tstruct gendisk *template_disk = NULL;\n\n\t \n\tif (t->integrity_added)\n\t\treturn 0;\n\n\ttemplate_disk = dm_table_get_integrity_disk(t);\n\tif (!template_disk)\n\t\treturn 0;\n\n\tif (!integrity_profile_exists(dm_disk(md))) {\n\t\tt->integrity_supported = true;\n\t\t \n\t\tblk_integrity_register(dm_disk(md),\n\t\t\t\t       blk_get_integrity(template_disk));\n\t\treturn 0;\n\t}\n\n\t \n\tif (blk_integrity_compare(dm_disk(md), template_disk) < 0) {\n\t\tDMERR(\"%s: conflict with existing integrity profile: %s profile mismatch\",\n\t\t      dm_device_name(t->md),\n\t\t      template_disk->disk_name);\n\t\treturn 1;\n\t}\n\n\t \n\tt->integrity_supported = true;\n\treturn 0;\n}\n\n#ifdef CONFIG_BLK_INLINE_ENCRYPTION\n\nstruct dm_crypto_profile {\n\tstruct blk_crypto_profile profile;\n\tstruct mapped_device *md;\n};\n\nstatic int dm_keyslot_evict_callback(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t\t     sector_t start, sector_t len, void *data)\n{\n\tconst struct blk_crypto_key *key = data;\n\n\tblk_crypto_evict_key(dev->bdev, key);\n\treturn 0;\n}\n\n \nstatic int dm_keyslot_evict(struct blk_crypto_profile *profile,\n\t\t\t    const struct blk_crypto_key *key, unsigned int slot)\n{\n\tstruct mapped_device *md =\n\t\tcontainer_of(profile, struct dm_crypto_profile, profile)->md;\n\tstruct dm_table *t;\n\tint srcu_idx;\n\n\tt = dm_get_live_table(md, &srcu_idx);\n\tif (!t)\n\t\treturn 0;\n\n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tif (!ti->type->iterate_devices)\n\t\t\tcontinue;\n\t\tti->type->iterate_devices(ti, dm_keyslot_evict_callback,\n\t\t\t\t\t  (void *)key);\n\t}\n\n\tdm_put_live_table(md, srcu_idx);\n\treturn 0;\n}\n\nstatic int\ndevice_intersect_crypto_capabilities(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t\t     sector_t start, sector_t len, void *data)\n{\n\tstruct blk_crypto_profile *parent = data;\n\tstruct blk_crypto_profile *child =\n\t\tbdev_get_queue(dev->bdev)->crypto_profile;\n\n\tblk_crypto_intersect_capabilities(parent, child);\n\treturn 0;\n}\n\nvoid dm_destroy_crypto_profile(struct blk_crypto_profile *profile)\n{\n\tstruct dm_crypto_profile *dmcp = container_of(profile,\n\t\t\t\t\t\t      struct dm_crypto_profile,\n\t\t\t\t\t\t      profile);\n\n\tif (!profile)\n\t\treturn;\n\n\tblk_crypto_profile_destroy(profile);\n\tkfree(dmcp);\n}\n\nstatic void dm_table_destroy_crypto_profile(struct dm_table *t)\n{\n\tdm_destroy_crypto_profile(t->crypto_profile);\n\tt->crypto_profile = NULL;\n}\n\n \nstatic int dm_table_construct_crypto_profile(struct dm_table *t)\n{\n\tstruct dm_crypto_profile *dmcp;\n\tstruct blk_crypto_profile *profile;\n\tunsigned int i;\n\tbool empty_profile = true;\n\n\tdmcp = kmalloc(sizeof(*dmcp), GFP_KERNEL);\n\tif (!dmcp)\n\t\treturn -ENOMEM;\n\tdmcp->md = t->md;\n\n\tprofile = &dmcp->profile;\n\tblk_crypto_profile_init(profile, 0);\n\tprofile->ll_ops.keyslot_evict = dm_keyslot_evict;\n\tprofile->max_dun_bytes_supported = UINT_MAX;\n\tmemset(profile->modes_supported, 0xFF,\n\t       sizeof(profile->modes_supported));\n\n\tfor (i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tif (!dm_target_passes_crypto(ti->type)) {\n\t\t\tblk_crypto_intersect_capabilities(profile, NULL);\n\t\t\tbreak;\n\t\t}\n\t\tif (!ti->type->iterate_devices)\n\t\t\tcontinue;\n\t\tti->type->iterate_devices(ti,\n\t\t\t\t\t  device_intersect_crypto_capabilities,\n\t\t\t\t\t  profile);\n\t}\n\n\tif (t->md->queue &&\n\t    !blk_crypto_has_capabilities(profile,\n\t\t\t\t\t t->md->queue->crypto_profile)) {\n\t\tDMERR(\"Inline encryption capabilities of new DM table were more restrictive than the old table's. This is not supported!\");\n\t\tdm_destroy_crypto_profile(profile);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(profile->modes_supported); i++) {\n\t\tif (profile->modes_supported[i]) {\n\t\t\tempty_profile = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (empty_profile) {\n\t\tdm_destroy_crypto_profile(profile);\n\t\tprofile = NULL;\n\t}\n\n\t \n\tt->crypto_profile = profile;\n\n\treturn 0;\n}\n\nstatic void dm_update_crypto_profile(struct request_queue *q,\n\t\t\t\t     struct dm_table *t)\n{\n\tif (!t->crypto_profile)\n\t\treturn;\n\n\t \n\tif (!q->crypto_profile) {\n\t\tblk_crypto_register(t->crypto_profile, q);\n\t} else {\n\t\tblk_crypto_update_capabilities(q->crypto_profile,\n\t\t\t\t\t       t->crypto_profile);\n\t\tdm_destroy_crypto_profile(t->crypto_profile);\n\t}\n\tt->crypto_profile = NULL;\n}\n\n#else  \n\nstatic int dm_table_construct_crypto_profile(struct dm_table *t)\n{\n\treturn 0;\n}\n\nvoid dm_destroy_crypto_profile(struct blk_crypto_profile *profile)\n{\n}\n\nstatic void dm_table_destroy_crypto_profile(struct dm_table *t)\n{\n}\n\nstatic void dm_update_crypto_profile(struct request_queue *q,\n\t\t\t\t     struct dm_table *t)\n{\n}\n\n#endif  \n\n \nint dm_table_complete(struct dm_table *t)\n{\n\tint r;\n\n\tr = dm_table_determine_type(t);\n\tif (r) {\n\t\tDMERR(\"unable to determine table type\");\n\t\treturn r;\n\t}\n\n\tr = dm_table_build_index(t);\n\tif (r) {\n\t\tDMERR(\"unable to build btrees\");\n\t\treturn r;\n\t}\n\n\tr = dm_table_register_integrity(t);\n\tif (r) {\n\t\tDMERR(\"could not register integrity profile.\");\n\t\treturn r;\n\t}\n\n\tr = dm_table_construct_crypto_profile(t);\n\tif (r) {\n\t\tDMERR(\"could not construct crypto profile.\");\n\t\treturn r;\n\t}\n\n\tr = dm_table_alloc_md_mempools(t, t->md);\n\tif (r)\n\t\tDMERR(\"unable to allocate mempools\");\n\n\treturn r;\n}\n\nstatic DEFINE_MUTEX(_event_lock);\nvoid dm_table_event_callback(struct dm_table *t,\n\t\t\t     void (*fn)(void *), void *context)\n{\n\tmutex_lock(&_event_lock);\n\tt->event_fn = fn;\n\tt->event_context = context;\n\tmutex_unlock(&_event_lock);\n}\n\nvoid dm_table_event(struct dm_table *t)\n{\n\tmutex_lock(&_event_lock);\n\tif (t->event_fn)\n\t\tt->event_fn(t->event_context);\n\tmutex_unlock(&_event_lock);\n}\nEXPORT_SYMBOL(dm_table_event);\n\ninline sector_t dm_table_get_size(struct dm_table *t)\n{\n\treturn t->num_targets ? (t->highs[t->num_targets - 1] + 1) : 0;\n}\nEXPORT_SYMBOL(dm_table_get_size);\n\n \nstruct dm_target *dm_table_find_target(struct dm_table *t, sector_t sector)\n{\n\tunsigned int l, n = 0, k = 0;\n\tsector_t *node;\n\n\tif (unlikely(sector >= dm_table_get_size(t)))\n\t\treturn NULL;\n\n\tfor (l = 0; l < t->depth; l++) {\n\t\tn = get_child(n, k);\n\t\tnode = get_node(t, l, n);\n\n\t\tfor (k = 0; k < KEYS_PER_NODE; k++)\n\t\t\tif (node[k] >= sector)\n\t\t\t\tbreak;\n\t}\n\n\treturn &t->targets[(KEYS_PER_NODE * n) + k];\n}\n\nstatic int device_not_poll_capable(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t\t   sector_t start, sector_t len, void *data)\n{\n\tstruct request_queue *q = bdev_get_queue(dev->bdev);\n\n\treturn !test_bit(QUEUE_FLAG_POLL, &q->queue_flags);\n}\n\n \nstatic bool dm_table_any_dev_attr(struct dm_table *t,\n\t\t\t\t  iterate_devices_callout_fn func, void *data)\n{\n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tif (ti->type->iterate_devices &&\n\t\t    ti->type->iterate_devices(ti, func, data))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int count_device(struct dm_target *ti, struct dm_dev *dev,\n\t\t\tsector_t start, sector_t len, void *data)\n{\n\tunsigned int *num_devices = data;\n\n\t(*num_devices)++;\n\n\treturn 0;\n}\n\nstatic bool dm_table_supports_poll(struct dm_table *t)\n{\n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tif (!ti->type->iterate_devices ||\n\t\t    ti->type->iterate_devices(ti, device_not_poll_capable, NULL))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nbool dm_table_has_no_data_devices(struct dm_table *t)\n{\n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\t\tunsigned int num_devices = 0;\n\n\t\tif (!ti->type->iterate_devices)\n\t\t\treturn false;\n\n\t\tti->type->iterate_devices(ti, count_device, &num_devices);\n\t\tif (num_devices)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int device_not_zoned_model(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t\t  sector_t start, sector_t len, void *data)\n{\n\tstruct request_queue *q = bdev_get_queue(dev->bdev);\n\tenum blk_zoned_model *zoned_model = data;\n\n\treturn blk_queue_zoned_model(q) != *zoned_model;\n}\n\n \nstatic bool dm_table_supports_zoned_model(struct dm_table *t,\n\t\t\t\t\t  enum blk_zoned_model zoned_model)\n{\n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tif (dm_target_supports_zoned_hm(ti->type)) {\n\t\t\tif (!ti->type->iterate_devices ||\n\t\t\t    ti->type->iterate_devices(ti, device_not_zoned_model,\n\t\t\t\t\t\t      &zoned_model))\n\t\t\t\treturn false;\n\t\t} else if (!dm_target_supports_mixed_zoned_model(ti->type)) {\n\t\t\tif (zoned_model == BLK_ZONED_HM)\n\t\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic int device_not_matches_zone_sectors(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t\t\t   sector_t start, sector_t len, void *data)\n{\n\tunsigned int *zone_sectors = data;\n\n\tif (!bdev_is_zoned(dev->bdev))\n\t\treturn 0;\n\treturn bdev_zone_sectors(dev->bdev) != *zone_sectors;\n}\n\n \nstatic int validate_hardware_zoned_model(struct dm_table *t,\n\t\t\t\t\t enum blk_zoned_model zoned_model,\n\t\t\t\t\t unsigned int zone_sectors)\n{\n\tif (zoned_model == BLK_ZONED_NONE)\n\t\treturn 0;\n\n\tif (!dm_table_supports_zoned_model(t, zoned_model)) {\n\t\tDMERR(\"%s: zoned model is not consistent across all devices\",\n\t\t      dm_device_name(t->md));\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (!zone_sectors || !is_power_of_2(zone_sectors))\n\t\treturn -EINVAL;\n\n\tif (dm_table_any_dev_attr(t, device_not_matches_zone_sectors, &zone_sectors)) {\n\t\tDMERR(\"%s: zone sectors is not consistent across all zoned devices\",\n\t\t      dm_device_name(t->md));\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nint dm_calculate_queue_limits(struct dm_table *t,\n\t\t\t      struct queue_limits *limits)\n{\n\tstruct queue_limits ti_limits;\n\tenum blk_zoned_model zoned_model = BLK_ZONED_NONE;\n\tunsigned int zone_sectors = 0;\n\n\tblk_set_stacking_limits(limits);\n\n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tblk_set_stacking_limits(&ti_limits);\n\n\t\tif (!ti->type->iterate_devices) {\n\t\t\t \n\t\t\tif (ti->type->io_hints)\n\t\t\t\tti->type->io_hints(ti, &ti_limits);\n\t\t\tgoto combine_limits;\n\t\t}\n\n\t\t \n\t\tti->type->iterate_devices(ti, dm_set_device_limits,\n\t\t\t\t\t  &ti_limits);\n\n\t\tif (zoned_model == BLK_ZONED_NONE && ti_limits.zoned != BLK_ZONED_NONE) {\n\t\t\t \n\t\t\tzoned_model = ti_limits.zoned;\n\t\t\tzone_sectors = ti_limits.chunk_sectors;\n\t\t}\n\n\t\t \n\t\tif (ti->type->io_hints)\n\t\t\tti->type->io_hints(ti, &ti_limits);\n\n\t\t \n\t\tif (ti->type->iterate_devices(ti, device_area_is_invalid,\n\t\t\t\t\t      &ti_limits))\n\t\t\treturn -EINVAL;\n\ncombine_limits:\n\t\t \n\t\tif (blk_stack_limits(limits, &ti_limits, 0) < 0)\n\t\t\tDMWARN(\"%s: adding target device (start sect %llu len %llu) \"\n\t\t\t       \"caused an alignment inconsistency\",\n\t\t\t       dm_device_name(t->md),\n\t\t\t       (unsigned long long) ti->begin,\n\t\t\t       (unsigned long long) ti->len);\n\t}\n\n\t \n\tif (limits->zoned != BLK_ZONED_NONE) {\n\t\t \n\t\tzoned_model = limits->zoned;\n\t\tzone_sectors = limits->chunk_sectors;\n\t}\n\tif (validate_hardware_zoned_model(t, zoned_model, zone_sectors))\n\t\treturn -EINVAL;\n\n\treturn validate_hardware_logical_block_alignment(t, limits);\n}\n\n \nstatic void dm_table_verify_integrity(struct dm_table *t)\n{\n\tstruct gendisk *template_disk = NULL;\n\n\tif (t->integrity_added)\n\t\treturn;\n\n\tif (t->integrity_supported) {\n\t\t \n\t\ttemplate_disk = dm_table_get_integrity_disk(t);\n\t\tif (template_disk &&\n\t\t    blk_integrity_compare(dm_disk(t->md), template_disk) >= 0)\n\t\t\treturn;\n\t}\n\n\tif (integrity_profile_exists(dm_disk(t->md))) {\n\t\tDMWARN(\"%s: unable to establish an integrity profile\",\n\t\t       dm_device_name(t->md));\n\t\tblk_integrity_unregister(dm_disk(t->md));\n\t}\n}\n\nstatic int device_flush_capable(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t\tsector_t start, sector_t len, void *data)\n{\n\tunsigned long flush = (unsigned long) data;\n\tstruct request_queue *q = bdev_get_queue(dev->bdev);\n\n\treturn (q->queue_flags & flush);\n}\n\nstatic bool dm_table_supports_flush(struct dm_table *t, unsigned long flush)\n{\n\t \n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tif (!ti->num_flush_bios)\n\t\t\tcontinue;\n\n\t\tif (ti->flush_supported)\n\t\t\treturn true;\n\n\t\tif (ti->type->iterate_devices &&\n\t\t    ti->type->iterate_devices(ti, device_flush_capable, (void *) flush))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int device_dax_write_cache_enabled(struct dm_target *ti,\n\t\t\t\t\t  struct dm_dev *dev, sector_t start,\n\t\t\t\t\t  sector_t len, void *data)\n{\n\tstruct dax_device *dax_dev = dev->dax_dev;\n\n\tif (!dax_dev)\n\t\treturn false;\n\n\tif (dax_write_cache_enabled(dax_dev))\n\t\treturn true;\n\treturn false;\n}\n\nstatic int device_is_rotational(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t\tsector_t start, sector_t len, void *data)\n{\n\treturn !bdev_nonrot(dev->bdev);\n}\n\nstatic int device_is_not_random(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t     sector_t start, sector_t len, void *data)\n{\n\tstruct request_queue *q = bdev_get_queue(dev->bdev);\n\n\treturn !blk_queue_add_random(q);\n}\n\nstatic int device_not_write_zeroes_capable(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t\t\t   sector_t start, sector_t len, void *data)\n{\n\tstruct request_queue *q = bdev_get_queue(dev->bdev);\n\n\treturn !q->limits.max_write_zeroes_sectors;\n}\n\nstatic bool dm_table_supports_write_zeroes(struct dm_table *t)\n{\n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tif (!ti->num_write_zeroes_bios)\n\t\t\treturn false;\n\n\t\tif (!ti->type->iterate_devices ||\n\t\t    ti->type->iterate_devices(ti, device_not_write_zeroes_capable, NULL))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int device_not_nowait_capable(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t\t     sector_t start, sector_t len, void *data)\n{\n\treturn !bdev_nowait(dev->bdev);\n}\n\nstatic bool dm_table_supports_nowait(struct dm_table *t)\n{\n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tif (!dm_target_supports_nowait(ti->type))\n\t\t\treturn false;\n\n\t\tif (!ti->type->iterate_devices ||\n\t\t    ti->type->iterate_devices(ti, device_not_nowait_capable, NULL))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int device_not_discard_capable(struct dm_target *ti, struct dm_dev *dev,\n\t\t\t\t      sector_t start, sector_t len, void *data)\n{\n\treturn !bdev_max_discard_sectors(dev->bdev);\n}\n\nstatic bool dm_table_supports_discards(struct dm_table *t)\n{\n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tif (!ti->num_discard_bios)\n\t\t\treturn false;\n\n\t\t \n\t\tif (!ti->discards_supported &&\n\t\t    (!ti->type->iterate_devices ||\n\t\t     ti->type->iterate_devices(ti, device_not_discard_capable, NULL)))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int device_not_secure_erase_capable(struct dm_target *ti,\n\t\t\t\t\t   struct dm_dev *dev, sector_t start,\n\t\t\t\t\t   sector_t len, void *data)\n{\n\treturn !bdev_max_secure_erase_sectors(dev->bdev);\n}\n\nstatic bool dm_table_supports_secure_erase(struct dm_table *t)\n{\n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tif (!ti->num_secure_erase_bios)\n\t\t\treturn false;\n\n\t\tif (!ti->type->iterate_devices ||\n\t\t    ti->type->iterate_devices(ti, device_not_secure_erase_capable, NULL))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int device_requires_stable_pages(struct dm_target *ti,\n\t\t\t\t\tstruct dm_dev *dev, sector_t start,\n\t\t\t\t\tsector_t len, void *data)\n{\n\treturn bdev_stable_writes(dev->bdev);\n}\n\nint dm_table_set_restrictions(struct dm_table *t, struct request_queue *q,\n\t\t\t      struct queue_limits *limits)\n{\n\tbool wc = false, fua = false;\n\tint r;\n\n\t \n\tq->limits = *limits;\n\n\tif (dm_table_supports_nowait(t))\n\t\tblk_queue_flag_set(QUEUE_FLAG_NOWAIT, q);\n\telse\n\t\tblk_queue_flag_clear(QUEUE_FLAG_NOWAIT, q);\n\n\tif (!dm_table_supports_discards(t)) {\n\t\tq->limits.max_discard_sectors = 0;\n\t\tq->limits.max_hw_discard_sectors = 0;\n\t\tq->limits.discard_granularity = 0;\n\t\tq->limits.discard_alignment = 0;\n\t\tq->limits.discard_misaligned = 0;\n\t}\n\n\tif (!dm_table_supports_secure_erase(t))\n\t\tq->limits.max_secure_erase_sectors = 0;\n\n\tif (dm_table_supports_flush(t, (1UL << QUEUE_FLAG_WC))) {\n\t\twc = true;\n\t\tif (dm_table_supports_flush(t, (1UL << QUEUE_FLAG_FUA)))\n\t\t\tfua = true;\n\t}\n\tblk_queue_write_cache(q, wc, fua);\n\n\tif (dm_table_supports_dax(t, device_not_dax_capable)) {\n\t\tblk_queue_flag_set(QUEUE_FLAG_DAX, q);\n\t\tif (dm_table_supports_dax(t, device_not_dax_synchronous_capable))\n\t\t\tset_dax_synchronous(t->md->dax_dev);\n\t} else\n\t\tblk_queue_flag_clear(QUEUE_FLAG_DAX, q);\n\n\tif (dm_table_any_dev_attr(t, device_dax_write_cache_enabled, NULL))\n\t\tdax_write_cache(t->md->dax_dev, true);\n\n\t \n\tif (dm_table_any_dev_attr(t, device_is_rotational, NULL))\n\t\tblk_queue_flag_clear(QUEUE_FLAG_NONROT, q);\n\telse\n\t\tblk_queue_flag_set(QUEUE_FLAG_NONROT, q);\n\n\tif (!dm_table_supports_write_zeroes(t))\n\t\tq->limits.max_write_zeroes_sectors = 0;\n\n\tdm_table_verify_integrity(t);\n\n\t \n\tif (dm_table_any_dev_attr(t, device_requires_stable_pages, NULL))\n\t\tblk_queue_flag_set(QUEUE_FLAG_STABLE_WRITES, q);\n\telse\n\t\tblk_queue_flag_clear(QUEUE_FLAG_STABLE_WRITES, q);\n\n\t \n\tif (blk_queue_add_random(q) &&\n\t    dm_table_any_dev_attr(t, device_is_not_random, NULL))\n\t\tblk_queue_flag_clear(QUEUE_FLAG_ADD_RANDOM, q);\n\n\t \n\tif (blk_queue_is_zoned(q)) {\n\t\tr = dm_set_zones_restrictions(t, q);\n\t\tif (r)\n\t\t\treturn r;\n\t\tif (!static_key_enabled(&zoned_enabled.key))\n\t\t\tstatic_branch_enable(&zoned_enabled);\n\t}\n\n\tdm_update_crypto_profile(q, t);\n\tdisk_update_readahead(t->md->disk);\n\n\t \n\tif (__table_type_bio_based(t->type)) {\n\t\tif (dm_table_supports_poll(t))\n\t\t\tblk_queue_flag_set(QUEUE_FLAG_POLL, q);\n\t\telse\n\t\t\tblk_queue_flag_clear(QUEUE_FLAG_POLL, q);\n\t}\n\n\treturn 0;\n}\n\nstruct list_head *dm_table_get_devices(struct dm_table *t)\n{\n\treturn &t->devices;\n}\n\nblk_mode_t dm_table_get_mode(struct dm_table *t)\n{\n\treturn t->mode;\n}\nEXPORT_SYMBOL(dm_table_get_mode);\n\nenum suspend_mode {\n\tPRESUSPEND,\n\tPRESUSPEND_UNDO,\n\tPOSTSUSPEND,\n};\n\nstatic void suspend_targets(struct dm_table *t, enum suspend_mode mode)\n{\n\tlockdep_assert_held(&t->md->suspend_lock);\n\n\tfor (unsigned int i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tswitch (mode) {\n\t\tcase PRESUSPEND:\n\t\t\tif (ti->type->presuspend)\n\t\t\t\tti->type->presuspend(ti);\n\t\t\tbreak;\n\t\tcase PRESUSPEND_UNDO:\n\t\t\tif (ti->type->presuspend_undo)\n\t\t\t\tti->type->presuspend_undo(ti);\n\t\t\tbreak;\n\t\tcase POSTSUSPEND:\n\t\t\tif (ti->type->postsuspend)\n\t\t\t\tti->type->postsuspend(ti);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nvoid dm_table_presuspend_targets(struct dm_table *t)\n{\n\tif (!t)\n\t\treturn;\n\n\tsuspend_targets(t, PRESUSPEND);\n}\n\nvoid dm_table_presuspend_undo_targets(struct dm_table *t)\n{\n\tif (!t)\n\t\treturn;\n\n\tsuspend_targets(t, PRESUSPEND_UNDO);\n}\n\nvoid dm_table_postsuspend_targets(struct dm_table *t)\n{\n\tif (!t)\n\t\treturn;\n\n\tsuspend_targets(t, POSTSUSPEND);\n}\n\nint dm_table_resume_targets(struct dm_table *t)\n{\n\tunsigned int i;\n\tint r = 0;\n\n\tlockdep_assert_held(&t->md->suspend_lock);\n\n\tfor (i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tif (!ti->type->preresume)\n\t\t\tcontinue;\n\n\t\tr = ti->type->preresume(ti);\n\t\tif (r) {\n\t\t\tDMERR(\"%s: %s: preresume failed, error = %d\",\n\t\t\t      dm_device_name(t->md), ti->type->name, r);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\tfor (i = 0; i < t->num_targets; i++) {\n\t\tstruct dm_target *ti = dm_table_get_target(t, i);\n\n\t\tif (ti->type->resume)\n\t\t\tti->type->resume(ti);\n\t}\n\n\treturn 0;\n}\n\nstruct mapped_device *dm_table_get_md(struct dm_table *t)\n{\n\treturn t->md;\n}\nEXPORT_SYMBOL(dm_table_get_md);\n\nconst char *dm_table_device_name(struct dm_table *t)\n{\n\treturn dm_device_name(t->md);\n}\nEXPORT_SYMBOL_GPL(dm_table_device_name);\n\nvoid dm_table_run_md_queue_async(struct dm_table *t)\n{\n\tif (!dm_table_request_based(t))\n\t\treturn;\n\n\tif (t->md->queue)\n\t\tblk_mq_run_hw_queues(t->md->queue, true);\n}\nEXPORT_SYMBOL(dm_table_run_md_queue_async);\n\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}