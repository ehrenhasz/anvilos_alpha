{
  "module_name": "dm-cache-target.c",
  "hash_id": "2933ab639f5b1e609c08b1d954b3ad57bbd2c018a08bfdc2b045617fdd580865",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-cache-target.c",
  "human_readable_source": "\n \n\n#include \"dm.h\"\n#include \"dm-bio-prison-v2.h\"\n#include \"dm-bio-record.h\"\n#include \"dm-cache-metadata.h\"\n#include \"dm-io-tracker.h\"\n\n#include <linux/dm-io.h>\n#include <linux/dm-kcopyd.h>\n#include <linux/jiffies.h>\n#include <linux/init.h>\n#include <linux/mempool.h>\n#include <linux/module.h>\n#include <linux/rwsem.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n\n#define DM_MSG_PREFIX \"cache\"\n\nDECLARE_DM_KCOPYD_THROTTLE_WITH_MODULE_PARM(cache_copy_throttle,\n\t\"A percentage of time allocated for copying to and/or from cache\");\n\n \n\n \n\n \n\n \nstruct continuation {\n\tstruct work_struct ws;\n\tblk_status_t input;\n};\n\nstatic inline void init_continuation(struct continuation *k,\n\t\t\t\t     void (*fn)(struct work_struct *))\n{\n\tINIT_WORK(&k->ws, fn);\n\tk->input = 0;\n}\n\nstatic inline void queue_continuation(struct workqueue_struct *wq,\n\t\t\t\t      struct continuation *k)\n{\n\tqueue_work(wq, &k->ws);\n}\n\n \n\n \nstruct batcher {\n\t \n\tblk_status_t (*commit_op)(void *context);\n\tvoid *commit_context;\n\n\t \n\tvoid (*issue_op)(struct bio *bio, void *context);\n\tvoid *issue_context;\n\n\t \n\tstruct workqueue_struct *wq;\n\n\tspinlock_t lock;\n\tstruct list_head work_items;\n\tstruct bio_list bios;\n\tstruct work_struct commit_work;\n\n\tbool commit_scheduled;\n};\n\nstatic void __commit(struct work_struct *_ws)\n{\n\tstruct batcher *b = container_of(_ws, struct batcher, commit_work);\n\tblk_status_t r;\n\tstruct list_head work_items;\n\tstruct work_struct *ws, *tmp;\n\tstruct continuation *k;\n\tstruct bio *bio;\n\tstruct bio_list bios;\n\n\tINIT_LIST_HEAD(&work_items);\n\tbio_list_init(&bios);\n\n\t \n\tspin_lock_irq(&b->lock);\n\tlist_splice_init(&b->work_items, &work_items);\n\tbio_list_merge(&bios, &b->bios);\n\tbio_list_init(&b->bios);\n\tb->commit_scheduled = false;\n\tspin_unlock_irq(&b->lock);\n\n\tr = b->commit_op(b->commit_context);\n\n\tlist_for_each_entry_safe(ws, tmp, &work_items, entry) {\n\t\tk = container_of(ws, struct continuation, ws);\n\t\tk->input = r;\n\t\tINIT_LIST_HEAD(&ws->entry);  \n\t\tqueue_work(b->wq, ws);\n\t}\n\n\twhile ((bio = bio_list_pop(&bios))) {\n\t\tif (r) {\n\t\t\tbio->bi_status = r;\n\t\t\tbio_endio(bio);\n\t\t} else\n\t\t\tb->issue_op(bio, b->issue_context);\n\t}\n}\n\nstatic void batcher_init(struct batcher *b,\n\t\t\t blk_status_t (*commit_op)(void *),\n\t\t\t void *commit_context,\n\t\t\t void (*issue_op)(struct bio *bio, void *),\n\t\t\t void *issue_context,\n\t\t\t struct workqueue_struct *wq)\n{\n\tb->commit_op = commit_op;\n\tb->commit_context = commit_context;\n\tb->issue_op = issue_op;\n\tb->issue_context = issue_context;\n\tb->wq = wq;\n\n\tspin_lock_init(&b->lock);\n\tINIT_LIST_HEAD(&b->work_items);\n\tbio_list_init(&b->bios);\n\tINIT_WORK(&b->commit_work, __commit);\n\tb->commit_scheduled = false;\n}\n\nstatic void async_commit(struct batcher *b)\n{\n\tqueue_work(b->wq, &b->commit_work);\n}\n\nstatic void continue_after_commit(struct batcher *b, struct continuation *k)\n{\n\tbool commit_scheduled;\n\n\tspin_lock_irq(&b->lock);\n\tcommit_scheduled = b->commit_scheduled;\n\tlist_add_tail(&k->ws.entry, &b->work_items);\n\tspin_unlock_irq(&b->lock);\n\n\tif (commit_scheduled)\n\t\tasync_commit(b);\n}\n\n \nstatic void issue_after_commit(struct batcher *b, struct bio *bio)\n{\n\tbool commit_scheduled;\n\n\tspin_lock_irq(&b->lock);\n\tcommit_scheduled = b->commit_scheduled;\n\tbio_list_add(&b->bios, bio);\n\tspin_unlock_irq(&b->lock);\n\n\tif (commit_scheduled)\n\t\tasync_commit(b);\n}\n\n \nstatic void schedule_commit(struct batcher *b)\n{\n\tbool immediate;\n\n\tspin_lock_irq(&b->lock);\n\timmediate = !list_empty(&b->work_items) || !bio_list_empty(&b->bios);\n\tb->commit_scheduled = true;\n\tspin_unlock_irq(&b->lock);\n\n\tif (immediate)\n\t\tasync_commit(b);\n}\n\n \nstruct dm_hook_info {\n\tbio_end_io_t *bi_end_io;\n};\n\nstatic void dm_hook_bio(struct dm_hook_info *h, struct bio *bio,\n\t\t\tbio_end_io_t *bi_end_io, void *bi_private)\n{\n\th->bi_end_io = bio->bi_end_io;\n\n\tbio->bi_end_io = bi_end_io;\n\tbio->bi_private = bi_private;\n}\n\nstatic void dm_unhook_bio(struct dm_hook_info *h, struct bio *bio)\n{\n\tbio->bi_end_io = h->bi_end_io;\n}\n\n \n\n#define MIGRATION_POOL_SIZE 128\n#define COMMIT_PERIOD HZ\n#define MIGRATION_COUNT_WINDOW 10\n\n \n#define DATA_DEV_BLOCK_SIZE_MIN_SECTORS (32 * 1024 >> SECTOR_SHIFT)\n#define DATA_DEV_BLOCK_SIZE_MAX_SECTORS (1024 * 1024 * 1024 >> SECTOR_SHIFT)\n\nenum cache_metadata_mode {\n\tCM_WRITE,\t\t \n\tCM_READ_ONLY,\t\t \n\tCM_FAIL\n};\n\nenum cache_io_mode {\n\t \n\tCM_IO_WRITEBACK,\n\n\t \n\tCM_IO_WRITETHROUGH,\n\n\t \n\tCM_IO_PASSTHROUGH\n};\n\nstruct cache_features {\n\tenum cache_metadata_mode mode;\n\tenum cache_io_mode io_mode;\n\tunsigned int metadata_version;\n\tbool discard_passdown:1;\n};\n\nstruct cache_stats {\n\tatomic_t read_hit;\n\tatomic_t read_miss;\n\tatomic_t write_hit;\n\tatomic_t write_miss;\n\tatomic_t demotion;\n\tatomic_t promotion;\n\tatomic_t writeback;\n\tatomic_t copies_avoided;\n\tatomic_t cache_cell_clash;\n\tatomic_t commit_count;\n\tatomic_t discard_count;\n};\n\nstruct cache {\n\tstruct dm_target *ti;\n\tspinlock_t lock;\n\n\t \n\tint sectors_per_block_shift;\n\tsector_t sectors_per_block;\n\n\tstruct dm_cache_metadata *cmd;\n\n\t \n\tstruct dm_dev *metadata_dev;\n\n\t \n\tstruct dm_dev *origin_dev;\n\n\t \n\tstruct dm_dev *cache_dev;\n\n\t \n\tdm_oblock_t origin_blocks;\n\tsector_t origin_sectors;\n\n\t \n\tdm_cblock_t cache_size;\n\n\t \n\tspinlock_t invalidation_lock;\n\tstruct list_head invalidation_requests;\n\n\tsector_t migration_threshold;\n\twait_queue_head_t migration_wait;\n\tatomic_t nr_allocated_migrations;\n\n\t \n\tatomic_t nr_io_migrations;\n\n\tstruct bio_list deferred_bios;\n\n\tstruct rw_semaphore quiesce_lock;\n\n\t \n\tdm_dblock_t discard_nr_blocks;\n\tunsigned long *discard_bitset;\n\tuint32_t discard_block_size;  \n\n\t \n\tunsigned int nr_ctr_args;\n\tconst char **ctr_args;\n\n\tstruct dm_kcopyd_client *copier;\n\tstruct work_struct deferred_bio_worker;\n\tstruct work_struct migration_worker;\n\tstruct workqueue_struct *wq;\n\tstruct delayed_work waker;\n\tstruct dm_bio_prison_v2 *prison;\n\n\t \n\tunsigned long *dirty_bitset;\n\tatomic_t nr_dirty;\n\n\tunsigned int policy_nr_args;\n\tstruct dm_cache_policy *policy;\n\n\t \n\tstruct cache_features features;\n\n\tstruct cache_stats stats;\n\n\tbool need_tick_bio:1;\n\tbool sized:1;\n\tbool invalidate:1;\n\tbool commit_requested:1;\n\tbool loaded_mappings:1;\n\tbool loaded_discards:1;\n\n\tstruct rw_semaphore background_work_lock;\n\n\tstruct batcher committer;\n\tstruct work_struct commit_ws;\n\n\tstruct dm_io_tracker tracker;\n\n\tmempool_t migration_pool;\n\n\tstruct bio_set bs;\n};\n\nstruct per_bio_data {\n\tbool tick:1;\n\tunsigned int req_nr:2;\n\tstruct dm_bio_prison_cell_v2 *cell;\n\tstruct dm_hook_info hook_info;\n\tsector_t len;\n};\n\nstruct dm_cache_migration {\n\tstruct continuation k;\n\tstruct cache *cache;\n\n\tstruct policy_work *op;\n\tstruct bio *overwrite_bio;\n\tstruct dm_bio_prison_cell_v2 *cell;\n\n\tdm_cblock_t invalidate_cblock;\n\tdm_oblock_t invalidate_oblock;\n};\n\n \n\nstatic bool writethrough_mode(struct cache *cache)\n{\n\treturn cache->features.io_mode == CM_IO_WRITETHROUGH;\n}\n\nstatic bool writeback_mode(struct cache *cache)\n{\n\treturn cache->features.io_mode == CM_IO_WRITEBACK;\n}\n\nstatic inline bool passthrough_mode(struct cache *cache)\n{\n\treturn unlikely(cache->features.io_mode == CM_IO_PASSTHROUGH);\n}\n\n \n\nstatic void wake_deferred_bio_worker(struct cache *cache)\n{\n\tqueue_work(cache->wq, &cache->deferred_bio_worker);\n}\n\nstatic void wake_migration_worker(struct cache *cache)\n{\n\tif (passthrough_mode(cache))\n\t\treturn;\n\n\tqueue_work(cache->wq, &cache->migration_worker);\n}\n\n \n\nstatic struct dm_bio_prison_cell_v2 *alloc_prison_cell(struct cache *cache)\n{\n\treturn dm_bio_prison_alloc_cell_v2(cache->prison, GFP_NOIO);\n}\n\nstatic void free_prison_cell(struct cache *cache, struct dm_bio_prison_cell_v2 *cell)\n{\n\tdm_bio_prison_free_cell_v2(cache->prison, cell);\n}\n\nstatic struct dm_cache_migration *alloc_migration(struct cache *cache)\n{\n\tstruct dm_cache_migration *mg;\n\n\tmg = mempool_alloc(&cache->migration_pool, GFP_NOIO);\n\n\tmemset(mg, 0, sizeof(*mg));\n\n\tmg->cache = cache;\n\tatomic_inc(&cache->nr_allocated_migrations);\n\n\treturn mg;\n}\n\nstatic void free_migration(struct dm_cache_migration *mg)\n{\n\tstruct cache *cache = mg->cache;\n\n\tif (atomic_dec_and_test(&cache->nr_allocated_migrations))\n\t\twake_up(&cache->migration_wait);\n\n\tmempool_free(mg, &cache->migration_pool);\n}\n\n \n\nstatic inline dm_oblock_t oblock_succ(dm_oblock_t b)\n{\n\treturn to_oblock(from_oblock(b) + 1ull);\n}\n\nstatic void build_key(dm_oblock_t begin, dm_oblock_t end, struct dm_cell_key_v2 *key)\n{\n\tkey->virtual = 0;\n\tkey->dev = 0;\n\tkey->block_begin = from_oblock(begin);\n\tkey->block_end = from_oblock(end);\n}\n\n \n#define WRITE_LOCK_LEVEL 0\n#define READ_WRITE_LOCK_LEVEL 1\n\nstatic unsigned int lock_level(struct bio *bio)\n{\n\treturn bio_data_dir(bio) == WRITE ?\n\t\tWRITE_LOCK_LEVEL :\n\t\tREAD_WRITE_LOCK_LEVEL;\n}\n\n \n\nstatic struct per_bio_data *get_per_bio_data(struct bio *bio)\n{\n\tstruct per_bio_data *pb = dm_per_bio_data(bio, sizeof(struct per_bio_data));\n\n\tBUG_ON(!pb);\n\treturn pb;\n}\n\nstatic struct per_bio_data *init_per_bio_data(struct bio *bio)\n{\n\tstruct per_bio_data *pb = get_per_bio_data(bio);\n\n\tpb->tick = false;\n\tpb->req_nr = dm_bio_get_target_bio_nr(bio);\n\tpb->cell = NULL;\n\tpb->len = 0;\n\n\treturn pb;\n}\n\n \n\nstatic void defer_bio(struct cache *cache, struct bio *bio)\n{\n\tspin_lock_irq(&cache->lock);\n\tbio_list_add(&cache->deferred_bios, bio);\n\tspin_unlock_irq(&cache->lock);\n\n\twake_deferred_bio_worker(cache);\n}\n\nstatic void defer_bios(struct cache *cache, struct bio_list *bios)\n{\n\tspin_lock_irq(&cache->lock);\n\tbio_list_merge(&cache->deferred_bios, bios);\n\tbio_list_init(bios);\n\tspin_unlock_irq(&cache->lock);\n\n\twake_deferred_bio_worker(cache);\n}\n\n \n\nstatic bool bio_detain_shared(struct cache *cache, dm_oblock_t oblock, struct bio *bio)\n{\n\tbool r;\n\tstruct per_bio_data *pb;\n\tstruct dm_cell_key_v2 key;\n\tdm_oblock_t end = to_oblock(from_oblock(oblock) + 1ULL);\n\tstruct dm_bio_prison_cell_v2 *cell_prealloc, *cell;\n\n\tcell_prealloc = alloc_prison_cell(cache);  \n\n\tbuild_key(oblock, end, &key);\n\tr = dm_cell_get_v2(cache->prison, &key, lock_level(bio), bio, cell_prealloc, &cell);\n\tif (!r) {\n\t\t \n\t\tfree_prison_cell(cache, cell_prealloc);\n\t\treturn r;\n\t}\n\n\tif (cell != cell_prealloc)\n\t\tfree_prison_cell(cache, cell_prealloc);\n\n\tpb = get_per_bio_data(bio);\n\tpb->cell = cell;\n\n\treturn r;\n}\n\n \n\nstatic bool is_dirty(struct cache *cache, dm_cblock_t b)\n{\n\treturn test_bit(from_cblock(b), cache->dirty_bitset);\n}\n\nstatic void set_dirty(struct cache *cache, dm_cblock_t cblock)\n{\n\tif (!test_and_set_bit(from_cblock(cblock), cache->dirty_bitset)) {\n\t\tatomic_inc(&cache->nr_dirty);\n\t\tpolicy_set_dirty(cache->policy, cblock);\n\t}\n}\n\n \nstatic void force_set_dirty(struct cache *cache, dm_cblock_t cblock)\n{\n\tif (!test_and_set_bit(from_cblock(cblock), cache->dirty_bitset))\n\t\tatomic_inc(&cache->nr_dirty);\n\tpolicy_set_dirty(cache->policy, cblock);\n}\n\nstatic void force_clear_dirty(struct cache *cache, dm_cblock_t cblock)\n{\n\tif (test_and_clear_bit(from_cblock(cblock), cache->dirty_bitset)) {\n\t\tif (atomic_dec_return(&cache->nr_dirty) == 0)\n\t\t\tdm_table_event(cache->ti->table);\n\t}\n\n\tpolicy_clear_dirty(cache->policy, cblock);\n}\n\n \n\nstatic bool block_size_is_power_of_two(struct cache *cache)\n{\n\treturn cache->sectors_per_block_shift >= 0;\n}\n\nstatic dm_block_t block_div(dm_block_t b, uint32_t n)\n{\n\tdo_div(b, n);\n\n\treturn b;\n}\n\nstatic dm_block_t oblocks_per_dblock(struct cache *cache)\n{\n\tdm_block_t oblocks = cache->discard_block_size;\n\n\tif (block_size_is_power_of_two(cache))\n\t\toblocks >>= cache->sectors_per_block_shift;\n\telse\n\t\toblocks = block_div(oblocks, cache->sectors_per_block);\n\n\treturn oblocks;\n}\n\nstatic dm_dblock_t oblock_to_dblock(struct cache *cache, dm_oblock_t oblock)\n{\n\treturn to_dblock(block_div(from_oblock(oblock),\n\t\t\t\t   oblocks_per_dblock(cache)));\n}\n\nstatic void set_discard(struct cache *cache, dm_dblock_t b)\n{\n\tBUG_ON(from_dblock(b) >= from_dblock(cache->discard_nr_blocks));\n\tatomic_inc(&cache->stats.discard_count);\n\n\tspin_lock_irq(&cache->lock);\n\tset_bit(from_dblock(b), cache->discard_bitset);\n\tspin_unlock_irq(&cache->lock);\n}\n\nstatic void clear_discard(struct cache *cache, dm_dblock_t b)\n{\n\tspin_lock_irq(&cache->lock);\n\tclear_bit(from_dblock(b), cache->discard_bitset);\n\tspin_unlock_irq(&cache->lock);\n}\n\nstatic bool is_discarded(struct cache *cache, dm_dblock_t b)\n{\n\tint r;\n\n\tspin_lock_irq(&cache->lock);\n\tr = test_bit(from_dblock(b), cache->discard_bitset);\n\tspin_unlock_irq(&cache->lock);\n\n\treturn r;\n}\n\nstatic bool is_discarded_oblock(struct cache *cache, dm_oblock_t b)\n{\n\tint r;\n\n\tspin_lock_irq(&cache->lock);\n\tr = test_bit(from_dblock(oblock_to_dblock(cache, b)),\n\t\t     cache->discard_bitset);\n\tspin_unlock_irq(&cache->lock);\n\n\treturn r;\n}\n\n \nstatic void remap_to_origin(struct cache *cache, struct bio *bio)\n{\n\tbio_set_dev(bio, cache->origin_dev->bdev);\n}\n\nstatic void remap_to_cache(struct cache *cache, struct bio *bio,\n\t\t\t   dm_cblock_t cblock)\n{\n\tsector_t bi_sector = bio->bi_iter.bi_sector;\n\tsector_t block = from_cblock(cblock);\n\n\tbio_set_dev(bio, cache->cache_dev->bdev);\n\tif (!block_size_is_power_of_two(cache))\n\t\tbio->bi_iter.bi_sector =\n\t\t\t(block * cache->sectors_per_block) +\n\t\t\tsector_div(bi_sector, cache->sectors_per_block);\n\telse\n\t\tbio->bi_iter.bi_sector =\n\t\t\t(block << cache->sectors_per_block_shift) |\n\t\t\t(bi_sector & (cache->sectors_per_block - 1));\n}\n\nstatic void check_if_tick_bio_needed(struct cache *cache, struct bio *bio)\n{\n\tstruct per_bio_data *pb;\n\n\tspin_lock_irq(&cache->lock);\n\tif (cache->need_tick_bio && !op_is_flush(bio->bi_opf) &&\n\t    bio_op(bio) != REQ_OP_DISCARD) {\n\t\tpb = get_per_bio_data(bio);\n\t\tpb->tick = true;\n\t\tcache->need_tick_bio = false;\n\t}\n\tspin_unlock_irq(&cache->lock);\n}\n\nstatic void remap_to_origin_clear_discard(struct cache *cache, struct bio *bio,\n\t\t\t\t\t  dm_oblock_t oblock)\n{\n\t\n\tcheck_if_tick_bio_needed(cache, bio);\n\tremap_to_origin(cache, bio);\n\tif (bio_data_dir(bio) == WRITE)\n\t\tclear_discard(cache, oblock_to_dblock(cache, oblock));\n}\n\nstatic void remap_to_cache_dirty(struct cache *cache, struct bio *bio,\n\t\t\t\t dm_oblock_t oblock, dm_cblock_t cblock)\n{\n\tcheck_if_tick_bio_needed(cache, bio);\n\tremap_to_cache(cache, bio, cblock);\n\tif (bio_data_dir(bio) == WRITE) {\n\t\tset_dirty(cache, cblock);\n\t\tclear_discard(cache, oblock_to_dblock(cache, oblock));\n\t}\n}\n\nstatic dm_oblock_t get_bio_block(struct cache *cache, struct bio *bio)\n{\n\tsector_t block_nr = bio->bi_iter.bi_sector;\n\n\tif (!block_size_is_power_of_two(cache))\n\t\t(void) sector_div(block_nr, cache->sectors_per_block);\n\telse\n\t\tblock_nr >>= cache->sectors_per_block_shift;\n\n\treturn to_oblock(block_nr);\n}\n\nstatic bool accountable_bio(struct cache *cache, struct bio *bio)\n{\n\treturn bio_op(bio) != REQ_OP_DISCARD;\n}\n\nstatic void accounted_begin(struct cache *cache, struct bio *bio)\n{\n\tstruct per_bio_data *pb;\n\n\tif (accountable_bio(cache, bio)) {\n\t\tpb = get_per_bio_data(bio);\n\t\tpb->len = bio_sectors(bio);\n\t\tdm_iot_io_begin(&cache->tracker, pb->len);\n\t}\n}\n\nstatic void accounted_complete(struct cache *cache, struct bio *bio)\n{\n\tstruct per_bio_data *pb = get_per_bio_data(bio);\n\n\tdm_iot_io_end(&cache->tracker, pb->len);\n}\n\nstatic void accounted_request(struct cache *cache, struct bio *bio)\n{\n\taccounted_begin(cache, bio);\n\tdm_submit_bio_remap(bio, NULL);\n}\n\nstatic void issue_op(struct bio *bio, void *context)\n{\n\tstruct cache *cache = context;\n\n\taccounted_request(cache, bio);\n}\n\n \nstatic void remap_to_origin_and_cache(struct cache *cache, struct bio *bio,\n\t\t\t\t      dm_oblock_t oblock, dm_cblock_t cblock)\n{\n\tstruct bio *origin_bio = bio_alloc_clone(cache->origin_dev->bdev, bio,\n\t\t\t\t\t\t GFP_NOIO, &cache->bs);\n\n\tBUG_ON(!origin_bio);\n\n\tbio_chain(origin_bio, bio);\n\n\tif (bio_data_dir(origin_bio) == WRITE)\n\t\tclear_discard(cache, oblock_to_dblock(cache, oblock));\n\tsubmit_bio(origin_bio);\n\n\tremap_to_cache(cache, bio, cblock);\n}\n\n \nstatic enum cache_metadata_mode get_cache_mode(struct cache *cache)\n{\n\treturn cache->features.mode;\n}\n\nstatic const char *cache_device_name(struct cache *cache)\n{\n\treturn dm_table_device_name(cache->ti->table);\n}\n\nstatic void notify_mode_switch(struct cache *cache, enum cache_metadata_mode mode)\n{\n\tstatic const char *descs[] = {\n\t\t\"write\",\n\t\t\"read-only\",\n\t\t\"fail\"\n\t};\n\n\tdm_table_event(cache->ti->table);\n\tDMINFO(\"%s: switching cache to %s mode\",\n\t       cache_device_name(cache), descs[(int)mode]);\n}\n\nstatic void set_cache_mode(struct cache *cache, enum cache_metadata_mode new_mode)\n{\n\tbool needs_check;\n\tenum cache_metadata_mode old_mode = get_cache_mode(cache);\n\n\tif (dm_cache_metadata_needs_check(cache->cmd, &needs_check)) {\n\t\tDMERR(\"%s: unable to read needs_check flag, setting failure mode.\",\n\t\t      cache_device_name(cache));\n\t\tnew_mode = CM_FAIL;\n\t}\n\n\tif (new_mode == CM_WRITE && needs_check) {\n\t\tDMERR(\"%s: unable to switch cache to write mode until repaired.\",\n\t\t      cache_device_name(cache));\n\t\tif (old_mode != new_mode)\n\t\t\tnew_mode = old_mode;\n\t\telse\n\t\t\tnew_mode = CM_READ_ONLY;\n\t}\n\n\t \n\tif (old_mode == CM_FAIL)\n\t\tnew_mode = CM_FAIL;\n\n\tswitch (new_mode) {\n\tcase CM_FAIL:\n\tcase CM_READ_ONLY:\n\t\tdm_cache_metadata_set_read_only(cache->cmd);\n\t\tbreak;\n\n\tcase CM_WRITE:\n\t\tdm_cache_metadata_set_read_write(cache->cmd);\n\t\tbreak;\n\t}\n\n\tcache->features.mode = new_mode;\n\n\tif (new_mode != old_mode)\n\t\tnotify_mode_switch(cache, new_mode);\n}\n\nstatic void abort_transaction(struct cache *cache)\n{\n\tconst char *dev_name = cache_device_name(cache);\n\n\tif (get_cache_mode(cache) >= CM_READ_ONLY)\n\t\treturn;\n\n\tDMERR_LIMIT(\"%s: aborting current metadata transaction\", dev_name);\n\tif (dm_cache_metadata_abort(cache->cmd)) {\n\t\tDMERR(\"%s: failed to abort metadata transaction\", dev_name);\n\t\tset_cache_mode(cache, CM_FAIL);\n\t}\n\n\tif (dm_cache_metadata_set_needs_check(cache->cmd)) {\n\t\tDMERR(\"%s: failed to set 'needs_check' flag in metadata\", dev_name);\n\t\tset_cache_mode(cache, CM_FAIL);\n\t}\n}\n\nstatic void metadata_operation_failed(struct cache *cache, const char *op, int r)\n{\n\tDMERR_LIMIT(\"%s: metadata operation '%s' failed: error = %d\",\n\t\t    cache_device_name(cache), op, r);\n\tabort_transaction(cache);\n\tset_cache_mode(cache, CM_READ_ONLY);\n}\n\n \n\nstatic void load_stats(struct cache *cache)\n{\n\tstruct dm_cache_statistics stats;\n\n\tdm_cache_metadata_get_stats(cache->cmd, &stats);\n\tatomic_set(&cache->stats.read_hit, stats.read_hits);\n\tatomic_set(&cache->stats.read_miss, stats.read_misses);\n\tatomic_set(&cache->stats.write_hit, stats.write_hits);\n\tatomic_set(&cache->stats.write_miss, stats.write_misses);\n}\n\nstatic void save_stats(struct cache *cache)\n{\n\tstruct dm_cache_statistics stats;\n\n\tif (get_cache_mode(cache) >= CM_READ_ONLY)\n\t\treturn;\n\n\tstats.read_hits = atomic_read(&cache->stats.read_hit);\n\tstats.read_misses = atomic_read(&cache->stats.read_miss);\n\tstats.write_hits = atomic_read(&cache->stats.write_hit);\n\tstats.write_misses = atomic_read(&cache->stats.write_miss);\n\n\tdm_cache_metadata_set_stats(cache->cmd, &stats);\n}\n\nstatic void update_stats(struct cache_stats *stats, enum policy_operation op)\n{\n\tswitch (op) {\n\tcase POLICY_PROMOTE:\n\t\tatomic_inc(&stats->promotion);\n\t\tbreak;\n\n\tcase POLICY_DEMOTE:\n\t\tatomic_inc(&stats->demotion);\n\t\tbreak;\n\n\tcase POLICY_WRITEBACK:\n\t\tatomic_inc(&stats->writeback);\n\t\tbreak;\n\t}\n}\n\n \nstatic void inc_io_migrations(struct cache *cache)\n{\n\tatomic_inc(&cache->nr_io_migrations);\n}\n\nstatic void dec_io_migrations(struct cache *cache)\n{\n\tatomic_dec(&cache->nr_io_migrations);\n}\n\nstatic bool discard_or_flush(struct bio *bio)\n{\n\treturn bio_op(bio) == REQ_OP_DISCARD || op_is_flush(bio->bi_opf);\n}\n\nstatic void calc_discard_block_range(struct cache *cache, struct bio *bio,\n\t\t\t\t     dm_dblock_t *b, dm_dblock_t *e)\n{\n\tsector_t sb = bio->bi_iter.bi_sector;\n\tsector_t se = bio_end_sector(bio);\n\n\t*b = to_dblock(dm_sector_div_up(sb, cache->discard_block_size));\n\n\tif (se - sb < cache->discard_block_size)\n\t\t*e = *b;\n\telse\n\t\t*e = to_dblock(block_div(se, cache->discard_block_size));\n}\n\n \n\nstatic void prevent_background_work(struct cache *cache)\n{\n\tlockdep_off();\n\tdown_write(&cache->background_work_lock);\n\tlockdep_on();\n}\n\nstatic void allow_background_work(struct cache *cache)\n{\n\tlockdep_off();\n\tup_write(&cache->background_work_lock);\n\tlockdep_on();\n}\n\nstatic bool background_work_begin(struct cache *cache)\n{\n\tbool r;\n\n\tlockdep_off();\n\tr = down_read_trylock(&cache->background_work_lock);\n\tlockdep_on();\n\n\treturn r;\n}\n\nstatic void background_work_end(struct cache *cache)\n{\n\tlockdep_off();\n\tup_read(&cache->background_work_lock);\n\tlockdep_on();\n}\n\n \n\nstatic bool bio_writes_complete_block(struct cache *cache, struct bio *bio)\n{\n\treturn (bio_data_dir(bio) == WRITE) &&\n\t\t(bio->bi_iter.bi_size == (cache->sectors_per_block << SECTOR_SHIFT));\n}\n\nstatic bool optimisable_bio(struct cache *cache, struct bio *bio, dm_oblock_t block)\n{\n\treturn writeback_mode(cache) &&\n\t\t(is_discarded_oblock(cache, block) || bio_writes_complete_block(cache, bio));\n}\n\nstatic void quiesce(struct dm_cache_migration *mg,\n\t\t    void (*continuation)(struct work_struct *))\n{\n\tinit_continuation(&mg->k, continuation);\n\tdm_cell_quiesce_v2(mg->cache->prison, mg->cell, &mg->k.ws);\n}\n\nstatic struct dm_cache_migration *ws_to_mg(struct work_struct *ws)\n{\n\tstruct continuation *k = container_of(ws, struct continuation, ws);\n\n\treturn container_of(k, struct dm_cache_migration, k);\n}\n\nstatic void copy_complete(int read_err, unsigned long write_err, void *context)\n{\n\tstruct dm_cache_migration *mg = container_of(context, struct dm_cache_migration, k);\n\n\tif (read_err || write_err)\n\t\tmg->k.input = BLK_STS_IOERR;\n\n\tqueue_continuation(mg->cache->wq, &mg->k);\n}\n\nstatic void copy(struct dm_cache_migration *mg, bool promote)\n{\n\tstruct dm_io_region o_region, c_region;\n\tstruct cache *cache = mg->cache;\n\n\to_region.bdev = cache->origin_dev->bdev;\n\to_region.sector = from_oblock(mg->op->oblock) * cache->sectors_per_block;\n\to_region.count = cache->sectors_per_block;\n\n\tc_region.bdev = cache->cache_dev->bdev;\n\tc_region.sector = from_cblock(mg->op->cblock) * cache->sectors_per_block;\n\tc_region.count = cache->sectors_per_block;\n\n\tif (promote)\n\t\tdm_kcopyd_copy(cache->copier, &o_region, 1, &c_region, 0, copy_complete, &mg->k);\n\telse\n\t\tdm_kcopyd_copy(cache->copier, &c_region, 1, &o_region, 0, copy_complete, &mg->k);\n}\n\nstatic void bio_drop_shared_lock(struct cache *cache, struct bio *bio)\n{\n\tstruct per_bio_data *pb = get_per_bio_data(bio);\n\n\tif (pb->cell && dm_cell_put_v2(cache->prison, pb->cell))\n\t\tfree_prison_cell(cache, pb->cell);\n\tpb->cell = NULL;\n}\n\nstatic void overwrite_endio(struct bio *bio)\n{\n\tstruct dm_cache_migration *mg = bio->bi_private;\n\tstruct cache *cache = mg->cache;\n\tstruct per_bio_data *pb = get_per_bio_data(bio);\n\n\tdm_unhook_bio(&pb->hook_info, bio);\n\n\tif (bio->bi_status)\n\t\tmg->k.input = bio->bi_status;\n\n\tqueue_continuation(cache->wq, &mg->k);\n}\n\nstatic void overwrite(struct dm_cache_migration *mg,\n\t\t      void (*continuation)(struct work_struct *))\n{\n\tstruct bio *bio = mg->overwrite_bio;\n\tstruct per_bio_data *pb = get_per_bio_data(bio);\n\n\tdm_hook_bio(&pb->hook_info, bio, overwrite_endio, mg);\n\n\t \n\tif (mg->op->op == POLICY_PROMOTE)\n\t\tremap_to_cache(mg->cache, bio, mg->op->cblock);\n\telse\n\t\tremap_to_origin(mg->cache, bio);\n\n\tinit_continuation(&mg->k, continuation);\n\taccounted_request(mg->cache, bio);\n}\n\n \nstatic void mg_complete(struct dm_cache_migration *mg, bool success)\n{\n\tstruct bio_list bios;\n\tstruct cache *cache = mg->cache;\n\tstruct policy_work *op = mg->op;\n\tdm_cblock_t cblock = op->cblock;\n\n\tif (success)\n\t\tupdate_stats(&cache->stats, op->op);\n\n\tswitch (op->op) {\n\tcase POLICY_PROMOTE:\n\t\tclear_discard(cache, oblock_to_dblock(cache, op->oblock));\n\t\tpolicy_complete_background_work(cache->policy, op, success);\n\n\t\tif (mg->overwrite_bio) {\n\t\t\tif (success)\n\t\t\t\tforce_set_dirty(cache, cblock);\n\t\t\telse if (mg->k.input)\n\t\t\t\tmg->overwrite_bio->bi_status = mg->k.input;\n\t\t\telse\n\t\t\t\tmg->overwrite_bio->bi_status = BLK_STS_IOERR;\n\t\t\tbio_endio(mg->overwrite_bio);\n\t\t} else {\n\t\t\tif (success)\n\t\t\t\tforce_clear_dirty(cache, cblock);\n\t\t\tdec_io_migrations(cache);\n\t\t}\n\t\tbreak;\n\n\tcase POLICY_DEMOTE:\n\t\t \n\t\tif (success)\n\t\t\tforce_clear_dirty(cache, cblock);\n\t\tpolicy_complete_background_work(cache->policy, op, success);\n\t\tdec_io_migrations(cache);\n\t\tbreak;\n\n\tcase POLICY_WRITEBACK:\n\t\tif (success)\n\t\t\tforce_clear_dirty(cache, cblock);\n\t\tpolicy_complete_background_work(cache->policy, op, success);\n\t\tdec_io_migrations(cache);\n\t\tbreak;\n\t}\n\n\tbio_list_init(&bios);\n\tif (mg->cell) {\n\t\tif (dm_cell_unlock_v2(cache->prison, mg->cell, &bios))\n\t\t\tfree_prison_cell(cache, mg->cell);\n\t}\n\n\tfree_migration(mg);\n\tdefer_bios(cache, &bios);\n\twake_migration_worker(cache);\n\n\tbackground_work_end(cache);\n}\n\nstatic void mg_success(struct work_struct *ws)\n{\n\tstruct dm_cache_migration *mg = ws_to_mg(ws);\n\n\tmg_complete(mg, mg->k.input == 0);\n}\n\nstatic void mg_update_metadata(struct work_struct *ws)\n{\n\tint r;\n\tstruct dm_cache_migration *mg = ws_to_mg(ws);\n\tstruct cache *cache = mg->cache;\n\tstruct policy_work *op = mg->op;\n\n\tswitch (op->op) {\n\tcase POLICY_PROMOTE:\n\t\tr = dm_cache_insert_mapping(cache->cmd, op->cblock, op->oblock);\n\t\tif (r) {\n\t\t\tDMERR_LIMIT(\"%s: migration failed; couldn't insert mapping\",\n\t\t\t\t    cache_device_name(cache));\n\t\t\tmetadata_operation_failed(cache, \"dm_cache_insert_mapping\", r);\n\n\t\t\tmg_complete(mg, false);\n\t\t\treturn;\n\t\t}\n\t\tmg_complete(mg, true);\n\t\tbreak;\n\n\tcase POLICY_DEMOTE:\n\t\tr = dm_cache_remove_mapping(cache->cmd, op->cblock);\n\t\tif (r) {\n\t\t\tDMERR_LIMIT(\"%s: migration failed; couldn't update on disk metadata\",\n\t\t\t\t    cache_device_name(cache));\n\t\t\tmetadata_operation_failed(cache, \"dm_cache_remove_mapping\", r);\n\n\t\t\tmg_complete(mg, false);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tinit_continuation(&mg->k, mg_success);\n\t\tcontinue_after_commit(&cache->committer, &mg->k);\n\t\tschedule_commit(&cache->committer);\n\t\tbreak;\n\n\tcase POLICY_WRITEBACK:\n\t\tmg_complete(mg, true);\n\t\tbreak;\n\t}\n}\n\nstatic void mg_update_metadata_after_copy(struct work_struct *ws)\n{\n\tstruct dm_cache_migration *mg = ws_to_mg(ws);\n\n\t \n\tif (mg->k.input)\n\t\tmg_complete(mg, false);\n\telse\n\t\tmg_update_metadata(ws);\n}\n\nstatic void mg_upgrade_lock(struct work_struct *ws)\n{\n\tint r;\n\tstruct dm_cache_migration *mg = ws_to_mg(ws);\n\n\t \n\tif (mg->k.input)\n\t\tmg_complete(mg, false);\n\n\telse {\n\t\t \n\t\tr = dm_cell_lock_promote_v2(mg->cache->prison, mg->cell,\n\t\t\t\t\t    READ_WRITE_LOCK_LEVEL);\n\t\tif (r < 0)\n\t\t\tmg_complete(mg, false);\n\n\t\telse if (r)\n\t\t\tquiesce(mg, mg_update_metadata);\n\n\t\telse\n\t\t\tmg_update_metadata(ws);\n\t}\n}\n\nstatic void mg_full_copy(struct work_struct *ws)\n{\n\tstruct dm_cache_migration *mg = ws_to_mg(ws);\n\tstruct cache *cache = mg->cache;\n\tstruct policy_work *op = mg->op;\n\tbool is_policy_promote = (op->op == POLICY_PROMOTE);\n\n\tif ((!is_policy_promote && !is_dirty(cache, op->cblock)) ||\n\t    is_discarded_oblock(cache, op->oblock)) {\n\t\tmg_upgrade_lock(ws);\n\t\treturn;\n\t}\n\n\tinit_continuation(&mg->k, mg_upgrade_lock);\n\tcopy(mg, is_policy_promote);\n}\n\nstatic void mg_copy(struct work_struct *ws)\n{\n\tstruct dm_cache_migration *mg = ws_to_mg(ws);\n\n\tif (mg->overwrite_bio) {\n\t\t \n\t\tif (!optimisable_bio(mg->cache, mg->overwrite_bio, mg->op->oblock)) {\n\t\t\t \n\t\t\tbool rb = bio_detain_shared(mg->cache, mg->op->oblock, mg->overwrite_bio);\n\n\t\t\tBUG_ON(rb);  \n\t\t\tmg->overwrite_bio = NULL;\n\t\t\tinc_io_migrations(mg->cache);\n\t\t\tmg_full_copy(ws);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\toverwrite(mg, mg_update_metadata_after_copy);\n\n\t} else\n\t\tmg_full_copy(ws);\n}\n\nstatic int mg_lock_writes(struct dm_cache_migration *mg)\n{\n\tint r;\n\tstruct dm_cell_key_v2 key;\n\tstruct cache *cache = mg->cache;\n\tstruct dm_bio_prison_cell_v2 *prealloc;\n\n\tprealloc = alloc_prison_cell(cache);\n\n\t \n\tbuild_key(mg->op->oblock, oblock_succ(mg->op->oblock), &key);\n\tr = dm_cell_lock_v2(cache->prison, &key,\n\t\t\t    mg->overwrite_bio ?  READ_WRITE_LOCK_LEVEL : WRITE_LOCK_LEVEL,\n\t\t\t    prealloc, &mg->cell);\n\tif (r < 0) {\n\t\tfree_prison_cell(cache, prealloc);\n\t\tmg_complete(mg, false);\n\t\treturn r;\n\t}\n\n\tif (mg->cell != prealloc)\n\t\tfree_prison_cell(cache, prealloc);\n\n\tif (r == 0)\n\t\tmg_copy(&mg->k.ws);\n\telse\n\t\tquiesce(mg, mg_copy);\n\n\treturn 0;\n}\n\nstatic int mg_start(struct cache *cache, struct policy_work *op, struct bio *bio)\n{\n\tstruct dm_cache_migration *mg;\n\n\tif (!background_work_begin(cache)) {\n\t\tpolicy_complete_background_work(cache->policy, op, false);\n\t\treturn -EPERM;\n\t}\n\n\tmg = alloc_migration(cache);\n\n\tmg->op = op;\n\tmg->overwrite_bio = bio;\n\n\tif (!bio)\n\t\tinc_io_migrations(cache);\n\n\treturn mg_lock_writes(mg);\n}\n\n \n\nstatic void invalidate_complete(struct dm_cache_migration *mg, bool success)\n{\n\tstruct bio_list bios;\n\tstruct cache *cache = mg->cache;\n\n\tbio_list_init(&bios);\n\tif (dm_cell_unlock_v2(cache->prison, mg->cell, &bios))\n\t\tfree_prison_cell(cache, mg->cell);\n\n\tif (!success && mg->overwrite_bio)\n\t\tbio_io_error(mg->overwrite_bio);\n\n\tfree_migration(mg);\n\tdefer_bios(cache, &bios);\n\n\tbackground_work_end(cache);\n}\n\nstatic void invalidate_completed(struct work_struct *ws)\n{\n\tstruct dm_cache_migration *mg = ws_to_mg(ws);\n\n\tinvalidate_complete(mg, !mg->k.input);\n}\n\nstatic int invalidate_cblock(struct cache *cache, dm_cblock_t cblock)\n{\n\tint r;\n\n\tr = policy_invalidate_mapping(cache->policy, cblock);\n\tif (!r) {\n\t\tr = dm_cache_remove_mapping(cache->cmd, cblock);\n\t\tif (r) {\n\t\t\tDMERR_LIMIT(\"%s: invalidation failed; couldn't update on disk metadata\",\n\t\t\t\t    cache_device_name(cache));\n\t\t\tmetadata_operation_failed(cache, \"dm_cache_remove_mapping\", r);\n\t\t}\n\n\t} else if (r == -ENODATA) {\n\t\t \n\t\tr = 0;\n\n\t} else\n\t\tDMERR(\"%s: policy_invalidate_mapping failed\", cache_device_name(cache));\n\n\treturn r;\n}\n\nstatic void invalidate_remove(struct work_struct *ws)\n{\n\tint r;\n\tstruct dm_cache_migration *mg = ws_to_mg(ws);\n\tstruct cache *cache = mg->cache;\n\n\tr = invalidate_cblock(cache, mg->invalidate_cblock);\n\tif (r) {\n\t\tinvalidate_complete(mg, false);\n\t\treturn;\n\t}\n\n\tinit_continuation(&mg->k, invalidate_completed);\n\tcontinue_after_commit(&cache->committer, &mg->k);\n\tremap_to_origin_clear_discard(cache, mg->overwrite_bio, mg->invalidate_oblock);\n\tmg->overwrite_bio = NULL;\n\tschedule_commit(&cache->committer);\n}\n\nstatic int invalidate_lock(struct dm_cache_migration *mg)\n{\n\tint r;\n\tstruct dm_cell_key_v2 key;\n\tstruct cache *cache = mg->cache;\n\tstruct dm_bio_prison_cell_v2 *prealloc;\n\n\tprealloc = alloc_prison_cell(cache);\n\n\tbuild_key(mg->invalidate_oblock, oblock_succ(mg->invalidate_oblock), &key);\n\tr = dm_cell_lock_v2(cache->prison, &key,\n\t\t\t    READ_WRITE_LOCK_LEVEL, prealloc, &mg->cell);\n\tif (r < 0) {\n\t\tfree_prison_cell(cache, prealloc);\n\t\tinvalidate_complete(mg, false);\n\t\treturn r;\n\t}\n\n\tif (mg->cell != prealloc)\n\t\tfree_prison_cell(cache, prealloc);\n\n\tif (r)\n\t\tquiesce(mg, invalidate_remove);\n\n\telse {\n\t\t \n\t\tinit_continuation(&mg->k, invalidate_remove);\n\t\tqueue_work(cache->wq, &mg->k.ws);\n\t}\n\n\treturn 0;\n}\n\nstatic int invalidate_start(struct cache *cache, dm_cblock_t cblock,\n\t\t\t    dm_oblock_t oblock, struct bio *bio)\n{\n\tstruct dm_cache_migration *mg;\n\n\tif (!background_work_begin(cache))\n\t\treturn -EPERM;\n\n\tmg = alloc_migration(cache);\n\n\tmg->overwrite_bio = bio;\n\tmg->invalidate_cblock = cblock;\n\tmg->invalidate_oblock = oblock;\n\n\treturn invalidate_lock(mg);\n}\n\n \n\nenum busy {\n\tIDLE,\n\tBUSY\n};\n\nstatic enum busy spare_migration_bandwidth(struct cache *cache)\n{\n\tbool idle = dm_iot_idle_for(&cache->tracker, HZ);\n\tsector_t current_volume = (atomic_read(&cache->nr_io_migrations) + 1) *\n\t\tcache->sectors_per_block;\n\n\tif (idle && current_volume <= cache->migration_threshold)\n\t\treturn IDLE;\n\telse\n\t\treturn BUSY;\n}\n\nstatic void inc_hit_counter(struct cache *cache, struct bio *bio)\n{\n\tatomic_inc(bio_data_dir(bio) == READ ?\n\t\t   &cache->stats.read_hit : &cache->stats.write_hit);\n}\n\nstatic void inc_miss_counter(struct cache *cache, struct bio *bio)\n{\n\tatomic_inc(bio_data_dir(bio) == READ ?\n\t\t   &cache->stats.read_miss : &cache->stats.write_miss);\n}\n\n \n\nstatic int map_bio(struct cache *cache, struct bio *bio, dm_oblock_t block,\n\t\t   bool *commit_needed)\n{\n\tint r, data_dir;\n\tbool rb, background_queued;\n\tdm_cblock_t cblock;\n\n\t*commit_needed = false;\n\n\trb = bio_detain_shared(cache, block, bio);\n\tif (!rb) {\n\t\t \n\t\t*commit_needed = true;\n\t\treturn DM_MAPIO_SUBMITTED;\n\t}\n\n\tdata_dir = bio_data_dir(bio);\n\n\tif (optimisable_bio(cache, bio, block)) {\n\t\tstruct policy_work *op = NULL;\n\n\t\tr = policy_lookup_with_work(cache->policy, block, &cblock, data_dir, true, &op);\n\t\tif (unlikely(r && r != -ENOENT)) {\n\t\t\tDMERR_LIMIT(\"%s: policy_lookup_with_work() failed with r = %d\",\n\t\t\t\t    cache_device_name(cache), r);\n\t\t\tbio_io_error(bio);\n\t\t\treturn DM_MAPIO_SUBMITTED;\n\t\t}\n\n\t\tif (r == -ENOENT && op) {\n\t\t\tbio_drop_shared_lock(cache, bio);\n\t\t\tBUG_ON(op->op != POLICY_PROMOTE);\n\t\t\tmg_start(cache, op, bio);\n\t\t\treturn DM_MAPIO_SUBMITTED;\n\t\t}\n\t} else {\n\t\tr = policy_lookup(cache->policy, block, &cblock, data_dir, false, &background_queued);\n\t\tif (unlikely(r && r != -ENOENT)) {\n\t\t\tDMERR_LIMIT(\"%s: policy_lookup() failed with r = %d\",\n\t\t\t\t    cache_device_name(cache), r);\n\t\t\tbio_io_error(bio);\n\t\t\treturn DM_MAPIO_SUBMITTED;\n\t\t}\n\n\t\tif (background_queued)\n\t\t\twake_migration_worker(cache);\n\t}\n\n\tif (r == -ENOENT) {\n\t\tstruct per_bio_data *pb = get_per_bio_data(bio);\n\n\t\t \n\t\tinc_miss_counter(cache, bio);\n\t\tif (pb->req_nr == 0) {\n\t\t\taccounted_begin(cache, bio);\n\t\t\tremap_to_origin_clear_discard(cache, bio, block);\n\t\t} else {\n\t\t\t \n\t\t\tbio_endio(bio);\n\t\t\treturn DM_MAPIO_SUBMITTED;\n\t\t}\n\t} else {\n\t\t \n\t\tinc_hit_counter(cache, bio);\n\n\t\t \n\t\tif (passthrough_mode(cache)) {\n\t\t\tif (bio_data_dir(bio) == WRITE) {\n\t\t\t\tbio_drop_shared_lock(cache, bio);\n\t\t\t\tatomic_inc(&cache->stats.demotion);\n\t\t\t\tinvalidate_start(cache, cblock, block, bio);\n\t\t\t} else\n\t\t\t\tremap_to_origin_clear_discard(cache, bio, block);\n\t\t} else {\n\t\t\tif (bio_data_dir(bio) == WRITE && writethrough_mode(cache) &&\n\t\t\t    !is_dirty(cache, cblock)) {\n\t\t\t\tremap_to_origin_and_cache(cache, bio, block, cblock);\n\t\t\t\taccounted_begin(cache, bio);\n\t\t\t} else\n\t\t\t\tremap_to_cache_dirty(cache, bio, block, cblock);\n\t\t}\n\t}\n\n\t \n\tif (bio->bi_opf & REQ_FUA) {\n\t\t \n\t\taccounted_complete(cache, bio);\n\t\tissue_after_commit(&cache->committer, bio);\n\t\t*commit_needed = true;\n\t\treturn DM_MAPIO_SUBMITTED;\n\t}\n\n\treturn DM_MAPIO_REMAPPED;\n}\n\nstatic bool process_bio(struct cache *cache, struct bio *bio)\n{\n\tbool commit_needed;\n\n\tif (map_bio(cache, bio, get_bio_block(cache, bio), &commit_needed) == DM_MAPIO_REMAPPED)\n\t\tdm_submit_bio_remap(bio, NULL);\n\n\treturn commit_needed;\n}\n\n \nstatic int commit(struct cache *cache, bool clean_shutdown)\n{\n\tint r;\n\n\tif (get_cache_mode(cache) >= CM_READ_ONLY)\n\t\treturn -EINVAL;\n\n\tatomic_inc(&cache->stats.commit_count);\n\tr = dm_cache_commit(cache->cmd, clean_shutdown);\n\tif (r)\n\t\tmetadata_operation_failed(cache, \"dm_cache_commit\", r);\n\n\treturn r;\n}\n\n \nstatic blk_status_t commit_op(void *context)\n{\n\tstruct cache *cache = context;\n\n\tif (dm_cache_changed_this_transaction(cache->cmd))\n\t\treturn errno_to_blk_status(commit(cache, false));\n\n\treturn 0;\n}\n\n \n\nstatic bool process_flush_bio(struct cache *cache, struct bio *bio)\n{\n\tstruct per_bio_data *pb = get_per_bio_data(bio);\n\n\tif (!pb->req_nr)\n\t\tremap_to_origin(cache, bio);\n\telse\n\t\tremap_to_cache(cache, bio, 0);\n\n\tissue_after_commit(&cache->committer, bio);\n\treturn true;\n}\n\nstatic bool process_discard_bio(struct cache *cache, struct bio *bio)\n{\n\tdm_dblock_t b, e;\n\n\t \n\tcalc_discard_block_range(cache, bio, &b, &e);\n\twhile (b != e) {\n\t\tset_discard(cache, b);\n\t\tb = to_dblock(from_dblock(b) + 1);\n\t}\n\n\tif (cache->features.discard_passdown) {\n\t\tremap_to_origin(cache, bio);\n\t\tdm_submit_bio_remap(bio, NULL);\n\t} else\n\t\tbio_endio(bio);\n\n\treturn false;\n}\n\nstatic void process_deferred_bios(struct work_struct *ws)\n{\n\tstruct cache *cache = container_of(ws, struct cache, deferred_bio_worker);\n\n\tbool commit_needed = false;\n\tstruct bio_list bios;\n\tstruct bio *bio;\n\n\tbio_list_init(&bios);\n\n\tspin_lock_irq(&cache->lock);\n\tbio_list_merge(&bios, &cache->deferred_bios);\n\tbio_list_init(&cache->deferred_bios);\n\tspin_unlock_irq(&cache->lock);\n\n\twhile ((bio = bio_list_pop(&bios))) {\n\t\tif (bio->bi_opf & REQ_PREFLUSH)\n\t\t\tcommit_needed = process_flush_bio(cache, bio) || commit_needed;\n\n\t\telse if (bio_op(bio) == REQ_OP_DISCARD)\n\t\t\tcommit_needed = process_discard_bio(cache, bio) || commit_needed;\n\n\t\telse\n\t\t\tcommit_needed = process_bio(cache, bio) || commit_needed;\n\t\tcond_resched();\n\t}\n\n\tif (commit_needed)\n\t\tschedule_commit(&cache->committer);\n}\n\n \nstatic void requeue_deferred_bios(struct cache *cache)\n{\n\tstruct bio *bio;\n\tstruct bio_list bios;\n\n\tbio_list_init(&bios);\n\tbio_list_merge(&bios, &cache->deferred_bios);\n\tbio_list_init(&cache->deferred_bios);\n\n\twhile ((bio = bio_list_pop(&bios))) {\n\t\tbio->bi_status = BLK_STS_DM_REQUEUE;\n\t\tbio_endio(bio);\n\t\tcond_resched();\n\t}\n}\n\n \nstatic void do_waker(struct work_struct *ws)\n{\n\tstruct cache *cache = container_of(to_delayed_work(ws), struct cache, waker);\n\n\tpolicy_tick(cache->policy, true);\n\twake_migration_worker(cache);\n\tschedule_commit(&cache->committer);\n\tqueue_delayed_work(cache->wq, &cache->waker, COMMIT_PERIOD);\n}\n\nstatic void check_migrations(struct work_struct *ws)\n{\n\tint r;\n\tstruct policy_work *op;\n\tstruct cache *cache = container_of(ws, struct cache, migration_worker);\n\tenum busy b;\n\n\tfor (;;) {\n\t\tb = spare_migration_bandwidth(cache);\n\n\t\tr = policy_get_background_work(cache->policy, b == IDLE, &op);\n\t\tif (r == -ENODATA)\n\t\t\tbreak;\n\n\t\tif (r) {\n\t\t\tDMERR_LIMIT(\"%s: policy_background_work failed\",\n\t\t\t\t    cache_device_name(cache));\n\t\t\tbreak;\n\t\t}\n\n\t\tr = mg_start(cache, op, NULL);\n\t\tif (r)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\t}\n}\n\n \n\n \nstatic void destroy(struct cache *cache)\n{\n\tunsigned int i;\n\n\tmempool_exit(&cache->migration_pool);\n\n\tif (cache->prison)\n\t\tdm_bio_prison_destroy_v2(cache->prison);\n\n\tcancel_delayed_work_sync(&cache->waker);\n\tif (cache->wq)\n\t\tdestroy_workqueue(cache->wq);\n\n\tif (cache->dirty_bitset)\n\t\tfree_bitset(cache->dirty_bitset);\n\n\tif (cache->discard_bitset)\n\t\tfree_bitset(cache->discard_bitset);\n\n\tif (cache->copier)\n\t\tdm_kcopyd_client_destroy(cache->copier);\n\n\tif (cache->cmd)\n\t\tdm_cache_metadata_close(cache->cmd);\n\n\tif (cache->metadata_dev)\n\t\tdm_put_device(cache->ti, cache->metadata_dev);\n\n\tif (cache->origin_dev)\n\t\tdm_put_device(cache->ti, cache->origin_dev);\n\n\tif (cache->cache_dev)\n\t\tdm_put_device(cache->ti, cache->cache_dev);\n\n\tif (cache->policy)\n\t\tdm_cache_policy_destroy(cache->policy);\n\n\tfor (i = 0; i < cache->nr_ctr_args ; i++)\n\t\tkfree(cache->ctr_args[i]);\n\tkfree(cache->ctr_args);\n\n\tbioset_exit(&cache->bs);\n\n\tkfree(cache);\n}\n\nstatic void cache_dtr(struct dm_target *ti)\n{\n\tstruct cache *cache = ti->private;\n\n\tdestroy(cache);\n}\n\nstatic sector_t get_dev_size(struct dm_dev *dev)\n{\n\treturn bdev_nr_sectors(dev->bdev);\n}\n\n \n\n \nstruct cache_args {\n\tstruct dm_target *ti;\n\n\tstruct dm_dev *metadata_dev;\n\n\tstruct dm_dev *cache_dev;\n\tsector_t cache_sectors;\n\n\tstruct dm_dev *origin_dev;\n\tsector_t origin_sectors;\n\n\tuint32_t block_size;\n\n\tconst char *policy_name;\n\tint policy_argc;\n\tconst char **policy_argv;\n\n\tstruct cache_features features;\n};\n\nstatic void destroy_cache_args(struct cache_args *ca)\n{\n\tif (ca->metadata_dev)\n\t\tdm_put_device(ca->ti, ca->metadata_dev);\n\n\tif (ca->cache_dev)\n\t\tdm_put_device(ca->ti, ca->cache_dev);\n\n\tif (ca->origin_dev)\n\t\tdm_put_device(ca->ti, ca->origin_dev);\n\n\tkfree(ca);\n}\n\nstatic bool at_least_one_arg(struct dm_arg_set *as, char **error)\n{\n\tif (!as->argc) {\n\t\t*error = \"Insufficient args\";\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic int parse_metadata_dev(struct cache_args *ca, struct dm_arg_set *as,\n\t\t\t      char **error)\n{\n\tint r;\n\tsector_t metadata_dev_size;\n\n\tif (!at_least_one_arg(as, error))\n\t\treturn -EINVAL;\n\n\tr = dm_get_device(ca->ti, dm_shift_arg(as),\n\t\t\t  BLK_OPEN_READ | BLK_OPEN_WRITE, &ca->metadata_dev);\n\tif (r) {\n\t\t*error = \"Error opening metadata device\";\n\t\treturn r;\n\t}\n\n\tmetadata_dev_size = get_dev_size(ca->metadata_dev);\n\tif (metadata_dev_size > DM_CACHE_METADATA_MAX_SECTORS_WARNING)\n\t\tDMWARN(\"Metadata device %pg is larger than %u sectors: excess space will not be used.\",\n\t\t       ca->metadata_dev->bdev, THIN_METADATA_MAX_SECTORS);\n\n\treturn 0;\n}\n\nstatic int parse_cache_dev(struct cache_args *ca, struct dm_arg_set *as,\n\t\t\t   char **error)\n{\n\tint r;\n\n\tif (!at_least_one_arg(as, error))\n\t\treturn -EINVAL;\n\n\tr = dm_get_device(ca->ti, dm_shift_arg(as),\n\t\t\t  BLK_OPEN_READ | BLK_OPEN_WRITE, &ca->cache_dev);\n\tif (r) {\n\t\t*error = \"Error opening cache device\";\n\t\treturn r;\n\t}\n\tca->cache_sectors = get_dev_size(ca->cache_dev);\n\n\treturn 0;\n}\n\nstatic int parse_origin_dev(struct cache_args *ca, struct dm_arg_set *as,\n\t\t\t    char **error)\n{\n\tint r;\n\n\tif (!at_least_one_arg(as, error))\n\t\treturn -EINVAL;\n\n\tr = dm_get_device(ca->ti, dm_shift_arg(as),\n\t\t\t  BLK_OPEN_READ | BLK_OPEN_WRITE, &ca->origin_dev);\n\tif (r) {\n\t\t*error = \"Error opening origin device\";\n\t\treturn r;\n\t}\n\n\tca->origin_sectors = get_dev_size(ca->origin_dev);\n\tif (ca->ti->len > ca->origin_sectors) {\n\t\t*error = \"Device size larger than cached device\";\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int parse_block_size(struct cache_args *ca, struct dm_arg_set *as,\n\t\t\t    char **error)\n{\n\tunsigned long block_size;\n\n\tif (!at_least_one_arg(as, error))\n\t\treturn -EINVAL;\n\n\tif (kstrtoul(dm_shift_arg(as), 10, &block_size) || !block_size ||\n\t    block_size < DATA_DEV_BLOCK_SIZE_MIN_SECTORS ||\n\t    block_size > DATA_DEV_BLOCK_SIZE_MAX_SECTORS ||\n\t    block_size & (DATA_DEV_BLOCK_SIZE_MIN_SECTORS - 1)) {\n\t\t*error = \"Invalid data block size\";\n\t\treturn -EINVAL;\n\t}\n\n\tif (block_size > ca->cache_sectors) {\n\t\t*error = \"Data block size is larger than the cache device\";\n\t\treturn -EINVAL;\n\t}\n\n\tca->block_size = block_size;\n\n\treturn 0;\n}\n\nstatic void init_features(struct cache_features *cf)\n{\n\tcf->mode = CM_WRITE;\n\tcf->io_mode = CM_IO_WRITEBACK;\n\tcf->metadata_version = 1;\n\tcf->discard_passdown = true;\n}\n\nstatic int parse_features(struct cache_args *ca, struct dm_arg_set *as,\n\t\t\t  char **error)\n{\n\tstatic const struct dm_arg _args[] = {\n\t\t{0, 3, \"Invalid number of cache feature arguments\"},\n\t};\n\n\tint r, mode_ctr = 0;\n\tunsigned int argc;\n\tconst char *arg;\n\tstruct cache_features *cf = &ca->features;\n\n\tinit_features(cf);\n\n\tr = dm_read_arg_group(_args, as, &argc, error);\n\tif (r)\n\t\treturn -EINVAL;\n\n\twhile (argc--) {\n\t\targ = dm_shift_arg(as);\n\n\t\tif (!strcasecmp(arg, \"writeback\")) {\n\t\t\tcf->io_mode = CM_IO_WRITEBACK;\n\t\t\tmode_ctr++;\n\t\t}\n\n\t\telse if (!strcasecmp(arg, \"writethrough\")) {\n\t\t\tcf->io_mode = CM_IO_WRITETHROUGH;\n\t\t\tmode_ctr++;\n\t\t}\n\n\t\telse if (!strcasecmp(arg, \"passthrough\")) {\n\t\t\tcf->io_mode = CM_IO_PASSTHROUGH;\n\t\t\tmode_ctr++;\n\t\t}\n\n\t\telse if (!strcasecmp(arg, \"metadata2\"))\n\t\t\tcf->metadata_version = 2;\n\n\t\telse if (!strcasecmp(arg, \"no_discard_passdown\"))\n\t\t\tcf->discard_passdown = false;\n\n\t\telse {\n\t\t\t*error = \"Unrecognised cache feature requested\";\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (mode_ctr > 1) {\n\t\t*error = \"Duplicate cache io_mode features requested\";\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int parse_policy(struct cache_args *ca, struct dm_arg_set *as,\n\t\t\tchar **error)\n{\n\tstatic const struct dm_arg _args[] = {\n\t\t{0, 1024, \"Invalid number of policy arguments\"},\n\t};\n\n\tint r;\n\n\tif (!at_least_one_arg(as, error))\n\t\treturn -EINVAL;\n\n\tca->policy_name = dm_shift_arg(as);\n\n\tr = dm_read_arg_group(_args, as, &ca->policy_argc, error);\n\tif (r)\n\t\treturn -EINVAL;\n\n\tca->policy_argv = (const char **)as->argv;\n\tdm_consume_args(as, ca->policy_argc);\n\n\treturn 0;\n}\n\nstatic int parse_cache_args(struct cache_args *ca, int argc, char **argv,\n\t\t\t    char **error)\n{\n\tint r;\n\tstruct dm_arg_set as;\n\n\tas.argc = argc;\n\tas.argv = argv;\n\n\tr = parse_metadata_dev(ca, &as, error);\n\tif (r)\n\t\treturn r;\n\n\tr = parse_cache_dev(ca, &as, error);\n\tif (r)\n\t\treturn r;\n\n\tr = parse_origin_dev(ca, &as, error);\n\tif (r)\n\t\treturn r;\n\n\tr = parse_block_size(ca, &as, error);\n\tif (r)\n\t\treturn r;\n\n\tr = parse_features(ca, &as, error);\n\tif (r)\n\t\treturn r;\n\n\tr = parse_policy(ca, &as, error);\n\tif (r)\n\t\treturn r;\n\n\treturn 0;\n}\n\n \n\nstatic struct kmem_cache *migration_cache;\n\n#define NOT_CORE_OPTION 1\n\nstatic int process_config_option(struct cache *cache, const char *key, const char *value)\n{\n\tunsigned long tmp;\n\n\tif (!strcasecmp(key, \"migration_threshold\")) {\n\t\tif (kstrtoul(value, 10, &tmp))\n\t\t\treturn -EINVAL;\n\n\t\tcache->migration_threshold = tmp;\n\t\treturn 0;\n\t}\n\n\treturn NOT_CORE_OPTION;\n}\n\nstatic int set_config_value(struct cache *cache, const char *key, const char *value)\n{\n\tint r = process_config_option(cache, key, value);\n\n\tif (r == NOT_CORE_OPTION)\n\t\tr = policy_set_config_value(cache->policy, key, value);\n\n\tif (r)\n\t\tDMWARN(\"bad config value for %s: %s\", key, value);\n\n\treturn r;\n}\n\nstatic int set_config_values(struct cache *cache, int argc, const char **argv)\n{\n\tint r = 0;\n\n\tif (argc & 1) {\n\t\tDMWARN(\"Odd number of policy arguments given but they should be <key> <value> pairs.\");\n\t\treturn -EINVAL;\n\t}\n\n\twhile (argc) {\n\t\tr = set_config_value(cache, argv[0], argv[1]);\n\t\tif (r)\n\t\t\tbreak;\n\n\t\targc -= 2;\n\t\targv += 2;\n\t}\n\n\treturn r;\n}\n\nstatic int create_cache_policy(struct cache *cache, struct cache_args *ca,\n\t\t\t       char **error)\n{\n\tstruct dm_cache_policy *p = dm_cache_policy_create(ca->policy_name,\n\t\t\t\t\t\t\t   cache->cache_size,\n\t\t\t\t\t\t\t   cache->origin_sectors,\n\t\t\t\t\t\t\t   cache->sectors_per_block);\n\tif (IS_ERR(p)) {\n\t\t*error = \"Error creating cache's policy\";\n\t\treturn PTR_ERR(p);\n\t}\n\tcache->policy = p;\n\tBUG_ON(!cache->policy);\n\n\treturn 0;\n}\n\n \n#define MAX_DISCARD_BLOCKS (1 << 14)\n\nstatic bool too_many_discard_blocks(sector_t discard_block_size,\n\t\t\t\t    sector_t origin_size)\n{\n\t(void) sector_div(origin_size, discard_block_size);\n\n\treturn origin_size > MAX_DISCARD_BLOCKS;\n}\n\nstatic sector_t calculate_discard_block_size(sector_t cache_block_size,\n\t\t\t\t\t     sector_t origin_size)\n{\n\tsector_t discard_block_size = cache_block_size;\n\n\tif (origin_size)\n\t\twhile (too_many_discard_blocks(discard_block_size, origin_size))\n\t\t\tdiscard_block_size *= 2;\n\n\treturn discard_block_size;\n}\n\nstatic void set_cache_size(struct cache *cache, dm_cblock_t size)\n{\n\tdm_block_t nr_blocks = from_cblock(size);\n\n\tif (nr_blocks > (1 << 20) && cache->cache_size != size)\n\t\tDMWARN_LIMIT(\"You have created a cache device with a lot of individual cache blocks (%llu)\\n\"\n\t\t\t     \"All these mappings can consume a lot of kernel memory, and take some time to read/write.\\n\"\n\t\t\t     \"Please consider increasing the cache block size to reduce the overall cache block count.\",\n\t\t\t     (unsigned long long) nr_blocks);\n\n\tcache->cache_size = size;\n}\n\n#define DEFAULT_MIGRATION_THRESHOLD 2048\n\nstatic int cache_create(struct cache_args *ca, struct cache **result)\n{\n\tint r = 0;\n\tchar **error = &ca->ti->error;\n\tstruct cache *cache;\n\tstruct dm_target *ti = ca->ti;\n\tdm_block_t origin_blocks;\n\tstruct dm_cache_metadata *cmd;\n\tbool may_format = ca->features.mode == CM_WRITE;\n\n\tcache = kzalloc(sizeof(*cache), GFP_KERNEL);\n\tif (!cache)\n\t\treturn -ENOMEM;\n\n\tcache->ti = ca->ti;\n\tti->private = cache;\n\tti->accounts_remapped_io = true;\n\tti->num_flush_bios = 2;\n\tti->flush_supported = true;\n\n\tti->num_discard_bios = 1;\n\tti->discards_supported = true;\n\n\tti->per_io_data_size = sizeof(struct per_bio_data);\n\n\tcache->features = ca->features;\n\tif (writethrough_mode(cache)) {\n\t\t \n\t\tr = bioset_init(&cache->bs, BIO_POOL_SIZE, 0, 0);\n\t\tif (r)\n\t\t\tgoto bad;\n\t}\n\n\tcache->metadata_dev = ca->metadata_dev;\n\tcache->origin_dev = ca->origin_dev;\n\tcache->cache_dev = ca->cache_dev;\n\n\tca->metadata_dev = ca->origin_dev = ca->cache_dev = NULL;\n\n\torigin_blocks = cache->origin_sectors = ca->origin_sectors;\n\torigin_blocks = block_div(origin_blocks, ca->block_size);\n\tcache->origin_blocks = to_oblock(origin_blocks);\n\n\tcache->sectors_per_block = ca->block_size;\n\tif (dm_set_target_max_io_len(ti, cache->sectors_per_block)) {\n\t\tr = -EINVAL;\n\t\tgoto bad;\n\t}\n\n\tif (ca->block_size & (ca->block_size - 1)) {\n\t\tdm_block_t cache_size = ca->cache_sectors;\n\n\t\tcache->sectors_per_block_shift = -1;\n\t\tcache_size = block_div(cache_size, ca->block_size);\n\t\tset_cache_size(cache, to_cblock(cache_size));\n\t} else {\n\t\tcache->sectors_per_block_shift = __ffs(ca->block_size);\n\t\tset_cache_size(cache, to_cblock(ca->cache_sectors >> cache->sectors_per_block_shift));\n\t}\n\n\tr = create_cache_policy(cache, ca, error);\n\tif (r)\n\t\tgoto bad;\n\n\tcache->policy_nr_args = ca->policy_argc;\n\tcache->migration_threshold = DEFAULT_MIGRATION_THRESHOLD;\n\n\tr = set_config_values(cache, ca->policy_argc, ca->policy_argv);\n\tif (r) {\n\t\t*error = \"Error setting cache policy's config values\";\n\t\tgoto bad;\n\t}\n\n\tcmd = dm_cache_metadata_open(cache->metadata_dev->bdev,\n\t\t\t\t     ca->block_size, may_format,\n\t\t\t\t     dm_cache_policy_get_hint_size(cache->policy),\n\t\t\t\t     ca->features.metadata_version);\n\tif (IS_ERR(cmd)) {\n\t\t*error = \"Error creating metadata object\";\n\t\tr = PTR_ERR(cmd);\n\t\tgoto bad;\n\t}\n\tcache->cmd = cmd;\n\tset_cache_mode(cache, CM_WRITE);\n\tif (get_cache_mode(cache) != CM_WRITE) {\n\t\t*error = \"Unable to get write access to metadata, please check/repair metadata.\";\n\t\tr = -EINVAL;\n\t\tgoto bad;\n\t}\n\n\tif (passthrough_mode(cache)) {\n\t\tbool all_clean;\n\n\t\tr = dm_cache_metadata_all_clean(cache->cmd, &all_clean);\n\t\tif (r) {\n\t\t\t*error = \"dm_cache_metadata_all_clean() failed\";\n\t\t\tgoto bad;\n\t\t}\n\n\t\tif (!all_clean) {\n\t\t\t*error = \"Cannot enter passthrough mode unless all blocks are clean\";\n\t\t\tr = -EINVAL;\n\t\t\tgoto bad;\n\t\t}\n\n\t\tpolicy_allow_migrations(cache->policy, false);\n\t}\n\n\tspin_lock_init(&cache->lock);\n\tbio_list_init(&cache->deferred_bios);\n\tatomic_set(&cache->nr_allocated_migrations, 0);\n\tatomic_set(&cache->nr_io_migrations, 0);\n\tinit_waitqueue_head(&cache->migration_wait);\n\n\tr = -ENOMEM;\n\tatomic_set(&cache->nr_dirty, 0);\n\tcache->dirty_bitset = alloc_bitset(from_cblock(cache->cache_size));\n\tif (!cache->dirty_bitset) {\n\t\t*error = \"could not allocate dirty bitset\";\n\t\tgoto bad;\n\t}\n\tclear_bitset(cache->dirty_bitset, from_cblock(cache->cache_size));\n\n\tcache->discard_block_size =\n\t\tcalculate_discard_block_size(cache->sectors_per_block,\n\t\t\t\t\t     cache->origin_sectors);\n\tcache->discard_nr_blocks = to_dblock(dm_sector_div_up(cache->origin_sectors,\n\t\t\t\t\t\t\t      cache->discard_block_size));\n\tcache->discard_bitset = alloc_bitset(from_dblock(cache->discard_nr_blocks));\n\tif (!cache->discard_bitset) {\n\t\t*error = \"could not allocate discard bitset\";\n\t\tgoto bad;\n\t}\n\tclear_bitset(cache->discard_bitset, from_dblock(cache->discard_nr_blocks));\n\n\tcache->copier = dm_kcopyd_client_create(&dm_kcopyd_throttle);\n\tif (IS_ERR(cache->copier)) {\n\t\t*error = \"could not create kcopyd client\";\n\t\tr = PTR_ERR(cache->copier);\n\t\tgoto bad;\n\t}\n\n\tcache->wq = alloc_workqueue(\"dm-\" DM_MSG_PREFIX, WQ_MEM_RECLAIM, 0);\n\tif (!cache->wq) {\n\t\t*error = \"could not create workqueue for metadata object\";\n\t\tgoto bad;\n\t}\n\tINIT_WORK(&cache->deferred_bio_worker, process_deferred_bios);\n\tINIT_WORK(&cache->migration_worker, check_migrations);\n\tINIT_DELAYED_WORK(&cache->waker, do_waker);\n\n\tcache->prison = dm_bio_prison_create_v2(cache->wq);\n\tif (!cache->prison) {\n\t\t*error = \"could not create bio prison\";\n\t\tgoto bad;\n\t}\n\n\tr = mempool_init_slab_pool(&cache->migration_pool, MIGRATION_POOL_SIZE,\n\t\t\t\t   migration_cache);\n\tif (r) {\n\t\t*error = \"Error creating cache's migration mempool\";\n\t\tgoto bad;\n\t}\n\n\tcache->need_tick_bio = true;\n\tcache->sized = false;\n\tcache->invalidate = false;\n\tcache->commit_requested = false;\n\tcache->loaded_mappings = false;\n\tcache->loaded_discards = false;\n\n\tload_stats(cache);\n\n\tatomic_set(&cache->stats.demotion, 0);\n\tatomic_set(&cache->stats.promotion, 0);\n\tatomic_set(&cache->stats.copies_avoided, 0);\n\tatomic_set(&cache->stats.cache_cell_clash, 0);\n\tatomic_set(&cache->stats.commit_count, 0);\n\tatomic_set(&cache->stats.discard_count, 0);\n\n\tspin_lock_init(&cache->invalidation_lock);\n\tINIT_LIST_HEAD(&cache->invalidation_requests);\n\n\tbatcher_init(&cache->committer, commit_op, cache,\n\t\t     issue_op, cache, cache->wq);\n\tdm_iot_init(&cache->tracker);\n\n\tinit_rwsem(&cache->background_work_lock);\n\tprevent_background_work(cache);\n\n\t*result = cache;\n\treturn 0;\nbad:\n\tdestroy(cache);\n\treturn r;\n}\n\nstatic int copy_ctr_args(struct cache *cache, int argc, const char **argv)\n{\n\tunsigned int i;\n\tconst char **copy;\n\n\tcopy = kcalloc(argc, sizeof(*copy), GFP_KERNEL);\n\tif (!copy)\n\t\treturn -ENOMEM;\n\tfor (i = 0; i < argc; i++) {\n\t\tcopy[i] = kstrdup(argv[i], GFP_KERNEL);\n\t\tif (!copy[i]) {\n\t\t\twhile (i--)\n\t\t\t\tkfree(copy[i]);\n\t\t\tkfree(copy);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tcache->nr_ctr_args = argc;\n\tcache->ctr_args = copy;\n\n\treturn 0;\n}\n\nstatic int cache_ctr(struct dm_target *ti, unsigned int argc, char **argv)\n{\n\tint r = -EINVAL;\n\tstruct cache_args *ca;\n\tstruct cache *cache = NULL;\n\n\tca = kzalloc(sizeof(*ca), GFP_KERNEL);\n\tif (!ca) {\n\t\tti->error = \"Error allocating memory for cache\";\n\t\treturn -ENOMEM;\n\t}\n\tca->ti = ti;\n\n\tr = parse_cache_args(ca, argc, argv, &ti->error);\n\tif (r)\n\t\tgoto out;\n\n\tr = cache_create(ca, &cache);\n\tif (r)\n\t\tgoto out;\n\n\tr = copy_ctr_args(cache, argc - 3, (const char **)argv + 3);\n\tif (r) {\n\t\tdestroy(cache);\n\t\tgoto out;\n\t}\n\n\tti->private = cache;\nout:\n\tdestroy_cache_args(ca);\n\treturn r;\n}\n\n \n\nstatic int cache_map(struct dm_target *ti, struct bio *bio)\n{\n\tstruct cache *cache = ti->private;\n\n\tint r;\n\tbool commit_needed;\n\tdm_oblock_t block = get_bio_block(cache, bio);\n\n\tinit_per_bio_data(bio);\n\tif (unlikely(from_oblock(block) >= from_oblock(cache->origin_blocks))) {\n\t\t \n\t\tremap_to_origin(cache, bio);\n\t\taccounted_begin(cache, bio);\n\t\treturn DM_MAPIO_REMAPPED;\n\t}\n\n\tif (discard_or_flush(bio)) {\n\t\tdefer_bio(cache, bio);\n\t\treturn DM_MAPIO_SUBMITTED;\n\t}\n\n\tr = map_bio(cache, bio, block, &commit_needed);\n\tif (commit_needed)\n\t\tschedule_commit(&cache->committer);\n\n\treturn r;\n}\n\nstatic int cache_end_io(struct dm_target *ti, struct bio *bio, blk_status_t *error)\n{\n\tstruct cache *cache = ti->private;\n\tunsigned long flags;\n\tstruct per_bio_data *pb = get_per_bio_data(bio);\n\n\tif (pb->tick) {\n\t\tpolicy_tick(cache->policy, false);\n\n\t\tspin_lock_irqsave(&cache->lock, flags);\n\t\tcache->need_tick_bio = true;\n\t\tspin_unlock_irqrestore(&cache->lock, flags);\n\t}\n\n\tbio_drop_shared_lock(cache, bio);\n\taccounted_complete(cache, bio);\n\n\treturn DM_ENDIO_DONE;\n}\n\nstatic int write_dirty_bitset(struct cache *cache)\n{\n\tint r;\n\n\tif (get_cache_mode(cache) >= CM_READ_ONLY)\n\t\treturn -EINVAL;\n\n\tr = dm_cache_set_dirty_bits(cache->cmd, from_cblock(cache->cache_size), cache->dirty_bitset);\n\tif (r)\n\t\tmetadata_operation_failed(cache, \"dm_cache_set_dirty_bits\", r);\n\n\treturn r;\n}\n\nstatic int write_discard_bitset(struct cache *cache)\n{\n\tunsigned int i, r;\n\n\tif (get_cache_mode(cache) >= CM_READ_ONLY)\n\t\treturn -EINVAL;\n\n\tr = dm_cache_discard_bitset_resize(cache->cmd, cache->discard_block_size,\n\t\t\t\t\t   cache->discard_nr_blocks);\n\tif (r) {\n\t\tDMERR(\"%s: could not resize on-disk discard bitset\", cache_device_name(cache));\n\t\tmetadata_operation_failed(cache, \"dm_cache_discard_bitset_resize\", r);\n\t\treturn r;\n\t}\n\n\tfor (i = 0; i < from_dblock(cache->discard_nr_blocks); i++) {\n\t\tr = dm_cache_set_discard(cache->cmd, to_dblock(i),\n\t\t\t\t\t is_discarded(cache, to_dblock(i)));\n\t\tif (r) {\n\t\t\tmetadata_operation_failed(cache, \"dm_cache_set_discard\", r);\n\t\t\treturn r;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int write_hints(struct cache *cache)\n{\n\tint r;\n\n\tif (get_cache_mode(cache) >= CM_READ_ONLY)\n\t\treturn -EINVAL;\n\n\tr = dm_cache_write_hints(cache->cmd, cache->policy);\n\tif (r) {\n\t\tmetadata_operation_failed(cache, \"dm_cache_write_hints\", r);\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\n \nstatic bool sync_metadata(struct cache *cache)\n{\n\tint r1, r2, r3, r4;\n\n\tr1 = write_dirty_bitset(cache);\n\tif (r1)\n\t\tDMERR(\"%s: could not write dirty bitset\", cache_device_name(cache));\n\n\tr2 = write_discard_bitset(cache);\n\tif (r2)\n\t\tDMERR(\"%s: could not write discard bitset\", cache_device_name(cache));\n\n\tsave_stats(cache);\n\n\tr3 = write_hints(cache);\n\tif (r3)\n\t\tDMERR(\"%s: could not write hints\", cache_device_name(cache));\n\n\t \n\tr4 = commit(cache, !r1 && !r2 && !r3);\n\tif (r4)\n\t\tDMERR(\"%s: could not write cache metadata\", cache_device_name(cache));\n\n\treturn !r1 && !r2 && !r3 && !r4;\n}\n\nstatic void cache_postsuspend(struct dm_target *ti)\n{\n\tstruct cache *cache = ti->private;\n\n\tprevent_background_work(cache);\n\tBUG_ON(atomic_read(&cache->nr_io_migrations));\n\n\tcancel_delayed_work_sync(&cache->waker);\n\tdrain_workqueue(cache->wq);\n\tWARN_ON(cache->tracker.in_flight);\n\n\t \n\trequeue_deferred_bios(cache);\n\n\tif (get_cache_mode(cache) == CM_WRITE)\n\t\t(void) sync_metadata(cache);\n}\n\nstatic int load_mapping(void *context, dm_oblock_t oblock, dm_cblock_t cblock,\n\t\t\tbool dirty, uint32_t hint, bool hint_valid)\n{\n\tstruct cache *cache = context;\n\n\tif (dirty) {\n\t\tset_bit(from_cblock(cblock), cache->dirty_bitset);\n\t\tatomic_inc(&cache->nr_dirty);\n\t} else\n\t\tclear_bit(from_cblock(cblock), cache->dirty_bitset);\n\n\treturn policy_load_mapping(cache->policy, oblock, cblock, dirty, hint, hint_valid);\n}\n\n \nstruct discard_load_info {\n\tstruct cache *cache;\n\n\t \n\tdm_block_t block_size;\n\tdm_block_t discard_begin, discard_end;\n};\n\nstatic void discard_load_info_init(struct cache *cache,\n\t\t\t\t   struct discard_load_info *li)\n{\n\tli->cache = cache;\n\tli->discard_begin = li->discard_end = 0;\n}\n\nstatic void set_discard_range(struct discard_load_info *li)\n{\n\tsector_t b, e;\n\n\tif (li->discard_begin == li->discard_end)\n\t\treturn;\n\n\t \n\tb = li->discard_begin * li->block_size;\n\te = li->discard_end * li->block_size;\n\n\t \n\tb = dm_sector_div_up(b, li->cache->discard_block_size);\n\tsector_div(e, li->cache->discard_block_size);\n\n\t \n\tif (e > from_dblock(li->cache->discard_nr_blocks))\n\t\te = from_dblock(li->cache->discard_nr_blocks);\n\n\tfor (; b < e; b++)\n\t\tset_discard(li->cache, to_dblock(b));\n}\n\nstatic int load_discard(void *context, sector_t discard_block_size,\n\t\t\tdm_dblock_t dblock, bool discard)\n{\n\tstruct discard_load_info *li = context;\n\n\tli->block_size = discard_block_size;\n\n\tif (discard) {\n\t\tif (from_dblock(dblock) == li->discard_end)\n\t\t\t \n\t\t\tli->discard_end = li->discard_end + 1ULL;\n\n\t\telse {\n\t\t\t \n\t\t\tset_discard_range(li);\n\t\t\tli->discard_begin = from_dblock(dblock);\n\t\t\tli->discard_end = li->discard_begin + 1ULL;\n\t\t}\n\t} else {\n\t\tset_discard_range(li);\n\t\tli->discard_begin = li->discard_end = 0;\n\t}\n\n\treturn 0;\n}\n\nstatic dm_cblock_t get_cache_dev_size(struct cache *cache)\n{\n\tsector_t size = get_dev_size(cache->cache_dev);\n\t(void) sector_div(size, cache->sectors_per_block);\n\treturn to_cblock(size);\n}\n\nstatic bool can_resize(struct cache *cache, dm_cblock_t new_size)\n{\n\tif (from_cblock(new_size) > from_cblock(cache->cache_size)) {\n\t\tif (cache->sized) {\n\t\t\tDMERR(\"%s: unable to extend cache due to missing cache table reload\",\n\t\t\t      cache_device_name(cache));\n\t\t\treturn false;\n\t\t}\n\t}\n\n\t \n\twhile (from_cblock(new_size) < from_cblock(cache->cache_size)) {\n\t\tnew_size = to_cblock(from_cblock(new_size) + 1);\n\t\tif (is_dirty(cache, new_size)) {\n\t\t\tDMERR(\"%s: unable to shrink cache; cache block %llu is dirty\",\n\t\t\t      cache_device_name(cache),\n\t\t\t      (unsigned long long) from_cblock(new_size));\n\t\t\treturn false;\n\t\t}\n\t}\n\n\treturn true;\n}\n\nstatic int resize_cache_dev(struct cache *cache, dm_cblock_t new_size)\n{\n\tint r;\n\n\tr = dm_cache_resize(cache->cmd, new_size);\n\tif (r) {\n\t\tDMERR(\"%s: could not resize cache metadata\", cache_device_name(cache));\n\t\tmetadata_operation_failed(cache, \"dm_cache_resize\", r);\n\t\treturn r;\n\t}\n\n\tset_cache_size(cache, new_size);\n\n\treturn 0;\n}\n\nstatic int cache_preresume(struct dm_target *ti)\n{\n\tint r = 0;\n\tstruct cache *cache = ti->private;\n\tdm_cblock_t csize = get_cache_dev_size(cache);\n\n\t \n\tif (!cache->sized) {\n\t\tr = resize_cache_dev(cache, csize);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tcache->sized = true;\n\n\t} else if (csize != cache->cache_size) {\n\t\tif (!can_resize(cache, csize))\n\t\t\treturn -EINVAL;\n\n\t\tr = resize_cache_dev(cache, csize);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tif (!cache->loaded_mappings) {\n\t\tr = dm_cache_load_mappings(cache->cmd, cache->policy,\n\t\t\t\t\t   load_mapping, cache);\n\t\tif (r) {\n\t\t\tDMERR(\"%s: could not load cache mappings\", cache_device_name(cache));\n\t\t\tmetadata_operation_failed(cache, \"dm_cache_load_mappings\", r);\n\t\t\treturn r;\n\t\t}\n\n\t\tcache->loaded_mappings = true;\n\t}\n\n\tif (!cache->loaded_discards) {\n\t\tstruct discard_load_info li;\n\n\t\t \n\t\tclear_bitset(cache->discard_bitset, from_dblock(cache->discard_nr_blocks));\n\n\t\tdiscard_load_info_init(cache, &li);\n\t\tr = dm_cache_load_discards(cache->cmd, load_discard, &li);\n\t\tif (r) {\n\t\t\tDMERR(\"%s: could not load origin discards\", cache_device_name(cache));\n\t\t\tmetadata_operation_failed(cache, \"dm_cache_load_discards\", r);\n\t\t\treturn r;\n\t\t}\n\t\tset_discard_range(&li);\n\n\t\tcache->loaded_discards = true;\n\t}\n\n\treturn r;\n}\n\nstatic void cache_resume(struct dm_target *ti)\n{\n\tstruct cache *cache = ti->private;\n\n\tcache->need_tick_bio = true;\n\tallow_background_work(cache);\n\tdo_waker(&cache->waker.work);\n}\n\nstatic void emit_flags(struct cache *cache, char *result,\n\t\t       unsigned int maxlen, ssize_t *sz_ptr)\n{\n\tssize_t sz = *sz_ptr;\n\tstruct cache_features *cf = &cache->features;\n\tunsigned int count = (cf->metadata_version == 2) + !cf->discard_passdown + 1;\n\n\tDMEMIT(\"%u \", count);\n\n\tif (cf->metadata_version == 2)\n\t\tDMEMIT(\"metadata2 \");\n\n\tif (writethrough_mode(cache))\n\t\tDMEMIT(\"writethrough \");\n\n\telse if (passthrough_mode(cache))\n\t\tDMEMIT(\"passthrough \");\n\n\telse if (writeback_mode(cache))\n\t\tDMEMIT(\"writeback \");\n\n\telse {\n\t\tDMEMIT(\"unknown \");\n\t\tDMERR(\"%s: internal error: unknown io mode: %d\",\n\t\t      cache_device_name(cache), (int) cf->io_mode);\n\t}\n\n\tif (!cf->discard_passdown)\n\t\tDMEMIT(\"no_discard_passdown \");\n\n\t*sz_ptr = sz;\n}\n\n \nstatic void cache_status(struct dm_target *ti, status_type_t type,\n\t\t\t unsigned int status_flags, char *result, unsigned int maxlen)\n{\n\tint r = 0;\n\tunsigned int i;\n\tssize_t sz = 0;\n\tdm_block_t nr_free_blocks_metadata = 0;\n\tdm_block_t nr_blocks_metadata = 0;\n\tchar buf[BDEVNAME_SIZE];\n\tstruct cache *cache = ti->private;\n\tdm_cblock_t residency;\n\tbool needs_check;\n\n\tswitch (type) {\n\tcase STATUSTYPE_INFO:\n\t\tif (get_cache_mode(cache) == CM_FAIL) {\n\t\t\tDMEMIT(\"Fail\");\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (!(status_flags & DM_STATUS_NOFLUSH_FLAG) && !dm_suspended(ti))\n\t\t\t(void) commit(cache, false);\n\n\t\tr = dm_cache_get_free_metadata_block_count(cache->cmd, &nr_free_blocks_metadata);\n\t\tif (r) {\n\t\t\tDMERR(\"%s: dm_cache_get_free_metadata_block_count returned %d\",\n\t\t\t      cache_device_name(cache), r);\n\t\t\tgoto err;\n\t\t}\n\n\t\tr = dm_cache_get_metadata_dev_size(cache->cmd, &nr_blocks_metadata);\n\t\tif (r) {\n\t\t\tDMERR(\"%s: dm_cache_get_metadata_dev_size returned %d\",\n\t\t\t      cache_device_name(cache), r);\n\t\t\tgoto err;\n\t\t}\n\n\t\tresidency = policy_residency(cache->policy);\n\n\t\tDMEMIT(\"%u %llu/%llu %llu %llu/%llu %u %u %u %u %u %u %lu \",\n\t\t       (unsigned int)DM_CACHE_METADATA_BLOCK_SIZE,\n\t\t       (unsigned long long)(nr_blocks_metadata - nr_free_blocks_metadata),\n\t\t       (unsigned long long)nr_blocks_metadata,\n\t\t       (unsigned long long)cache->sectors_per_block,\n\t\t       (unsigned long long) from_cblock(residency),\n\t\t       (unsigned long long) from_cblock(cache->cache_size),\n\t\t       (unsigned int) atomic_read(&cache->stats.read_hit),\n\t\t       (unsigned int) atomic_read(&cache->stats.read_miss),\n\t\t       (unsigned int) atomic_read(&cache->stats.write_hit),\n\t\t       (unsigned int) atomic_read(&cache->stats.write_miss),\n\t\t       (unsigned int) atomic_read(&cache->stats.demotion),\n\t\t       (unsigned int) atomic_read(&cache->stats.promotion),\n\t\t       (unsigned long) atomic_read(&cache->nr_dirty));\n\n\t\temit_flags(cache, result, maxlen, &sz);\n\n\t\tDMEMIT(\"2 migration_threshold %llu \", (unsigned long long) cache->migration_threshold);\n\n\t\tDMEMIT(\"%s \", dm_cache_policy_get_name(cache->policy));\n\t\tif (sz < maxlen) {\n\t\t\tr = policy_emit_config_values(cache->policy, result, maxlen, &sz);\n\t\t\tif (r)\n\t\t\t\tDMERR(\"%s: policy_emit_config_values returned %d\",\n\t\t\t\t      cache_device_name(cache), r);\n\t\t}\n\n\t\tif (get_cache_mode(cache) == CM_READ_ONLY)\n\t\t\tDMEMIT(\"ro \");\n\t\telse\n\t\t\tDMEMIT(\"rw \");\n\n\t\tr = dm_cache_metadata_needs_check(cache->cmd, &needs_check);\n\n\t\tif (r || needs_check)\n\t\t\tDMEMIT(\"needs_check \");\n\t\telse\n\t\t\tDMEMIT(\"- \");\n\n\t\tbreak;\n\n\tcase STATUSTYPE_TABLE:\n\t\tformat_dev_t(buf, cache->metadata_dev->bdev->bd_dev);\n\t\tDMEMIT(\"%s \", buf);\n\t\tformat_dev_t(buf, cache->cache_dev->bdev->bd_dev);\n\t\tDMEMIT(\"%s \", buf);\n\t\tformat_dev_t(buf, cache->origin_dev->bdev->bd_dev);\n\t\tDMEMIT(\"%s\", buf);\n\n\t\tfor (i = 0; i < cache->nr_ctr_args - 1; i++)\n\t\t\tDMEMIT(\" %s\", cache->ctr_args[i]);\n\t\tif (cache->nr_ctr_args)\n\t\t\tDMEMIT(\" %s\", cache->ctr_args[cache->nr_ctr_args - 1]);\n\t\tbreak;\n\n\tcase STATUSTYPE_IMA:\n\t\tDMEMIT_TARGET_NAME_VERSION(ti->type);\n\t\tif (get_cache_mode(cache) == CM_FAIL)\n\t\t\tDMEMIT(\",metadata_mode=fail\");\n\t\telse if (get_cache_mode(cache) == CM_READ_ONLY)\n\t\t\tDMEMIT(\",metadata_mode=ro\");\n\t\telse\n\t\t\tDMEMIT(\",metadata_mode=rw\");\n\n\t\tformat_dev_t(buf, cache->metadata_dev->bdev->bd_dev);\n\t\tDMEMIT(\",cache_metadata_device=%s\", buf);\n\t\tformat_dev_t(buf, cache->cache_dev->bdev->bd_dev);\n\t\tDMEMIT(\",cache_device=%s\", buf);\n\t\tformat_dev_t(buf, cache->origin_dev->bdev->bd_dev);\n\t\tDMEMIT(\",cache_origin_device=%s\", buf);\n\t\tDMEMIT(\",writethrough=%c\", writethrough_mode(cache) ? 'y' : 'n');\n\t\tDMEMIT(\",writeback=%c\", writeback_mode(cache) ? 'y' : 'n');\n\t\tDMEMIT(\",passthrough=%c\", passthrough_mode(cache) ? 'y' : 'n');\n\t\tDMEMIT(\",metadata2=%c\", cache->features.metadata_version == 2 ? 'y' : 'n');\n\t\tDMEMIT(\",no_discard_passdown=%c\", cache->features.discard_passdown ? 'n' : 'y');\n\t\tDMEMIT(\";\");\n\t\tbreak;\n\t}\n\n\treturn;\n\nerr:\n\tDMEMIT(\"Error\");\n}\n\n \nstruct cblock_range {\n\tdm_cblock_t begin;\n\tdm_cblock_t end;\n};\n\n \nstatic int parse_cblock_range(struct cache *cache, const char *str,\n\t\t\t      struct cblock_range *result)\n{\n\tchar dummy;\n\tuint64_t b, e;\n\tint r;\n\n\t \n\tr = sscanf(str, \"%llu-%llu%c\", &b, &e, &dummy);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (r == 2) {\n\t\tresult->begin = to_cblock(b);\n\t\tresult->end = to_cblock(e);\n\t\treturn 0;\n\t}\n\n\t \n\tr = sscanf(str, \"%llu%c\", &b, &dummy);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (r == 1) {\n\t\tresult->begin = to_cblock(b);\n\t\tresult->end = to_cblock(from_cblock(result->begin) + 1u);\n\t\treturn 0;\n\t}\n\n\tDMERR(\"%s: invalid cblock range '%s'\", cache_device_name(cache), str);\n\treturn -EINVAL;\n}\n\nstatic int validate_cblock_range(struct cache *cache, struct cblock_range *range)\n{\n\tuint64_t b = from_cblock(range->begin);\n\tuint64_t e = from_cblock(range->end);\n\tuint64_t n = from_cblock(cache->cache_size);\n\n\tif (b >= n) {\n\t\tDMERR(\"%s: begin cblock out of range: %llu >= %llu\",\n\t\t      cache_device_name(cache), b, n);\n\t\treturn -EINVAL;\n\t}\n\n\tif (e > n) {\n\t\tDMERR(\"%s: end cblock out of range: %llu > %llu\",\n\t\t      cache_device_name(cache), e, n);\n\t\treturn -EINVAL;\n\t}\n\n\tif (b >= e) {\n\t\tDMERR(\"%s: invalid cblock range: %llu >= %llu\",\n\t\t      cache_device_name(cache), b, e);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic inline dm_cblock_t cblock_succ(dm_cblock_t b)\n{\n\treturn to_cblock(from_cblock(b) + 1);\n}\n\nstatic int request_invalidation(struct cache *cache, struct cblock_range *range)\n{\n\tint r = 0;\n\n\t \n\twhile (range->begin != range->end) {\n\t\tr = invalidate_cblock(cache, range->begin);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\trange->begin = cblock_succ(range->begin);\n\t}\n\n\tcache->commit_requested = true;\n\treturn r;\n}\n\nstatic int process_invalidate_cblocks_message(struct cache *cache, unsigned int count,\n\t\t\t\t\t      const char **cblock_ranges)\n{\n\tint r = 0;\n\tunsigned int i;\n\tstruct cblock_range range;\n\n\tif (!passthrough_mode(cache)) {\n\t\tDMERR(\"%s: cache has to be in passthrough mode for invalidation\",\n\t\t      cache_device_name(cache));\n\t\treturn -EPERM;\n\t}\n\n\tfor (i = 0; i < count; i++) {\n\t\tr = parse_cblock_range(cache, cblock_ranges[i], &range);\n\t\tif (r)\n\t\t\tbreak;\n\n\t\tr = validate_cblock_range(cache, &range);\n\t\tif (r)\n\t\t\tbreak;\n\n\t\t \n\t\tr = request_invalidation(cache, &range);\n\t\tif (r)\n\t\t\tbreak;\n\t}\n\n\treturn r;\n}\n\n \nstatic int cache_message(struct dm_target *ti, unsigned int argc, char **argv,\n\t\t\t char *result, unsigned int maxlen)\n{\n\tstruct cache *cache = ti->private;\n\n\tif (!argc)\n\t\treturn -EINVAL;\n\n\tif (get_cache_mode(cache) >= CM_READ_ONLY) {\n\t\tDMERR(\"%s: unable to service cache target messages in READ_ONLY or FAIL mode\",\n\t\t      cache_device_name(cache));\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!strcasecmp(argv[0], \"invalidate_cblocks\"))\n\t\treturn process_invalidate_cblocks_message(cache, argc - 1, (const char **) argv + 1);\n\n\tif (argc != 2)\n\t\treturn -EINVAL;\n\n\treturn set_config_value(cache, argv[0], argv[1]);\n}\n\nstatic int cache_iterate_devices(struct dm_target *ti,\n\t\t\t\t iterate_devices_callout_fn fn, void *data)\n{\n\tint r = 0;\n\tstruct cache *cache = ti->private;\n\n\tr = fn(ti, cache->cache_dev, 0, get_dev_size(cache->cache_dev), data);\n\tif (!r)\n\t\tr = fn(ti, cache->origin_dev, 0, ti->len, data);\n\n\treturn r;\n}\n\n \nstatic void disable_passdown_if_not_supported(struct cache *cache)\n{\n\tstruct block_device *origin_bdev = cache->origin_dev->bdev;\n\tstruct queue_limits *origin_limits = &bdev_get_queue(origin_bdev)->limits;\n\tconst char *reason = NULL;\n\n\tif (!cache->features.discard_passdown)\n\t\treturn;\n\n\tif (!bdev_max_discard_sectors(origin_bdev))\n\t\treason = \"discard unsupported\";\n\n\telse if (origin_limits->max_discard_sectors < cache->sectors_per_block)\n\t\treason = \"max discard sectors smaller than a block\";\n\n\tif (reason) {\n\t\tDMWARN(\"Origin device (%pg) %s: Disabling discard passdown.\",\n\t\t       origin_bdev, reason);\n\t\tcache->features.discard_passdown = false;\n\t}\n}\n\nstatic void set_discard_limits(struct cache *cache, struct queue_limits *limits)\n{\n\tstruct block_device *origin_bdev = cache->origin_dev->bdev;\n\tstruct queue_limits *origin_limits = &bdev_get_queue(origin_bdev)->limits;\n\n\tif (!cache->features.discard_passdown) {\n\t\t \n\t\tlimits->max_discard_sectors = min_t(sector_t, cache->discard_block_size * 1024,\n\t\t\t\t\t\t    cache->origin_sectors);\n\t\tlimits->discard_granularity = cache->discard_block_size << SECTOR_SHIFT;\n\t\treturn;\n\t}\n\n\t \n\tlimits->max_discard_sectors = origin_limits->max_discard_sectors;\n\tlimits->max_hw_discard_sectors = origin_limits->max_hw_discard_sectors;\n\tlimits->discard_granularity = origin_limits->discard_granularity;\n\tlimits->discard_alignment = origin_limits->discard_alignment;\n\tlimits->discard_misaligned = origin_limits->discard_misaligned;\n}\n\nstatic void cache_io_hints(struct dm_target *ti, struct queue_limits *limits)\n{\n\tstruct cache *cache = ti->private;\n\tuint64_t io_opt_sectors = limits->io_opt >> SECTOR_SHIFT;\n\n\t \n\tif (io_opt_sectors < cache->sectors_per_block ||\n\t    do_div(io_opt_sectors, cache->sectors_per_block)) {\n\t\tblk_limits_io_min(limits, cache->sectors_per_block << SECTOR_SHIFT);\n\t\tblk_limits_io_opt(limits, cache->sectors_per_block << SECTOR_SHIFT);\n\t}\n\n\tdisable_passdown_if_not_supported(cache);\n\tset_discard_limits(cache, limits);\n}\n\n \n\nstatic struct target_type cache_target = {\n\t.name = \"cache\",\n\t.version = {2, 2, 0},\n\t.module = THIS_MODULE,\n\t.ctr = cache_ctr,\n\t.dtr = cache_dtr,\n\t.map = cache_map,\n\t.end_io = cache_end_io,\n\t.postsuspend = cache_postsuspend,\n\t.preresume = cache_preresume,\n\t.resume = cache_resume,\n\t.status = cache_status,\n\t.message = cache_message,\n\t.iterate_devices = cache_iterate_devices,\n\t.io_hints = cache_io_hints,\n};\n\nstatic int __init dm_cache_init(void)\n{\n\tint r;\n\n\tmigration_cache = KMEM_CACHE(dm_cache_migration, 0);\n\tif (!migration_cache)\n\t\treturn -ENOMEM;\n\n\tr = dm_register_target(&cache_target);\n\tif (r) {\n\t\tkmem_cache_destroy(migration_cache);\n\t\treturn r;\n\t}\n\n\treturn 0;\n}\n\nstatic void __exit dm_cache_exit(void)\n{\n\tdm_unregister_target(&cache_target);\n\tkmem_cache_destroy(migration_cache);\n}\n\nmodule_init(dm_cache_init);\nmodule_exit(dm_cache_exit);\n\nMODULE_DESCRIPTION(DM_NAME \" cache target\");\nMODULE_AUTHOR(\"Joe Thornber <ejt@redhat.com>\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}