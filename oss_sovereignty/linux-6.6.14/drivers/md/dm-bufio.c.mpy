{
  "module_name": "dm-bufio.c",
  "hash_id": "d2e989473bdba6c84a3955b746bbfc7971ac902d3fa5d6b41f78a455abea8e51",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-bufio.c",
  "human_readable_source": "\n \n\n#include <linux/dm-bufio.h>\n\n#include <linux/device-mapper.h>\n#include <linux/dm-io.h>\n#include <linux/slab.h>\n#include <linux/sched/mm.h>\n#include <linux/jiffies.h>\n#include <linux/vmalloc.h>\n#include <linux/shrinker.h>\n#include <linux/module.h>\n#include <linux/rbtree.h>\n#include <linux/stacktrace.h>\n#include <linux/jump_label.h>\n\n#include \"dm.h\"\n\n#define DM_MSG_PREFIX \"bufio\"\n\n \n#define DM_BUFIO_MIN_BUFFERS\t\t8\n\n#define DM_BUFIO_MEMORY_PERCENT\t\t2\n#define DM_BUFIO_VMALLOC_PERCENT\t25\n#define DM_BUFIO_WRITEBACK_RATIO\t3\n#define DM_BUFIO_LOW_WATERMARK_RATIO\t16\n\n \n#define DM_BUFIO_WORK_TIMER_SECS\t30\n\n \n#define DM_BUFIO_DEFAULT_AGE_SECS\t300\n\n \n#define DM_BUFIO_DEFAULT_RETAIN_BYTES   (256 * 1024)\n\n \n#define DM_BUFIO_WRITE_ALIGN\t\t4096\n\n \n#define LIST_CLEAN\t0\n#define LIST_DIRTY\t1\n#define LIST_SIZE\t2\n\n \n\n \nstruct lru_entry {\n\tstruct list_head list;\n\tatomic_t referenced;\n};\n\nstruct lru_iter {\n\tstruct lru *lru;\n\tstruct list_head list;\n\tstruct lru_entry *stop;\n\tstruct lru_entry *e;\n};\n\nstruct lru {\n\tstruct list_head *cursor;\n\tunsigned long count;\n\n\tstruct list_head iterators;\n};\n\n \n\nstatic void lru_init(struct lru *lru)\n{\n\tlru->cursor = NULL;\n\tlru->count = 0;\n\tINIT_LIST_HEAD(&lru->iterators);\n}\n\nstatic void lru_destroy(struct lru *lru)\n{\n\tWARN_ON_ONCE(lru->cursor);\n\tWARN_ON_ONCE(!list_empty(&lru->iterators));\n}\n\n \nstatic void lru_insert(struct lru *lru, struct lru_entry *le)\n{\n\t \n\tatomic_set(&le->referenced, 0);\n\n\tif (lru->cursor) {\n\t\tlist_add_tail(&le->list, lru->cursor);\n\t} else {\n\t\tINIT_LIST_HEAD(&le->list);\n\t\tlru->cursor = &le->list;\n\t}\n\tlru->count++;\n}\n\n \n\n \nstatic inline struct lru_entry *to_le(struct list_head *l)\n{\n\treturn container_of(l, struct lru_entry, list);\n}\n\n \nstatic void lru_iter_begin(struct lru *lru, struct lru_iter *it)\n{\n\tit->lru = lru;\n\tit->stop = lru->cursor ? to_le(lru->cursor->prev) : NULL;\n\tit->e = lru->cursor ? to_le(lru->cursor) : NULL;\n\tlist_add(&it->list, &lru->iterators);\n}\n\n \nstatic inline void lru_iter_end(struct lru_iter *it)\n{\n\tlist_del(&it->list);\n}\n\n \ntypedef bool (*iter_predicate)(struct lru_entry *le, void *context);\n\n \nstatic struct lru_entry *lru_iter_next(struct lru_iter *it,\n\t\t\t\t       iter_predicate pred, void *context)\n{\n\tstruct lru_entry *e;\n\n\twhile (it->e) {\n\t\te = it->e;\n\n\t\t \n\t\tif (it->e == it->stop)\n\t\t\tit->e = NULL;\n\t\telse\n\t\t\tit->e = to_le(it->e->list.next);\n\n\t\tif (pred(e, context))\n\t\t\treturn e;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic void lru_iter_invalidate(struct lru *lru, struct lru_entry *e)\n{\n\tstruct lru_iter *it;\n\n\tlist_for_each_entry(it, &lru->iterators, list) {\n\t\t \n\t\tif (it->e == e) {\n\t\t\tit->e = to_le(it->e->list.next);\n\t\t\tif (it->e == e)\n\t\t\t\tit->e = NULL;\n\t\t}\n\n\t\t \n\t\tif (it->stop == e) {\n\t\t\tit->stop = to_le(it->stop->list.prev);\n\t\t\tif (it->stop == e)\n\t\t\t\tit->stop = NULL;\n\t\t}\n\t}\n}\n\n \n\n \nstatic void lru_remove(struct lru *lru, struct lru_entry *le)\n{\n\tlru_iter_invalidate(lru, le);\n\tif (lru->count == 1) {\n\t\tlru->cursor = NULL;\n\t} else {\n\t\tif (lru->cursor == &le->list)\n\t\t\tlru->cursor = lru->cursor->next;\n\t\tlist_del(&le->list);\n\t}\n\tlru->count--;\n}\n\n \nstatic inline void lru_reference(struct lru_entry *le)\n{\n\tatomic_set(&le->referenced, 1);\n}\n\n \n\n \nenum evict_result {\n\tER_EVICT,\n\tER_DONT_EVICT,\n\tER_STOP,  \n};\n\ntypedef enum evict_result (*le_predicate)(struct lru_entry *le, void *context);\n\nstatic struct lru_entry *lru_evict(struct lru *lru, le_predicate pred, void *context, bool no_sleep)\n{\n\tunsigned long tested = 0;\n\tstruct list_head *h = lru->cursor;\n\tstruct lru_entry *le;\n\n\tif (!h)\n\t\treturn NULL;\n\t \n\twhile (tested < lru->count) {\n\t\tle = container_of(h, struct lru_entry, list);\n\n\t\tif (atomic_read(&le->referenced)) {\n\t\t\tatomic_set(&le->referenced, 0);\n\t\t} else {\n\t\t\ttested++;\n\t\t\tswitch (pred(le, context)) {\n\t\t\tcase ER_EVICT:\n\t\t\t\t \n\t\t\t\tlru->cursor = le->list.next;\n\t\t\t\tlru_remove(lru, le);\n\t\t\t\treturn le;\n\n\t\t\tcase ER_DONT_EVICT:\n\t\t\t\tbreak;\n\n\t\t\tcase ER_STOP:\n\t\t\t\tlru->cursor = le->list.next;\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t}\n\n\t\th = h->next;\n\n\t\tif (!no_sleep)\n\t\t\tcond_resched();\n\t}\n\n\treturn NULL;\n}\n\n \n\n \n#define B_READING\t0\n#define B_WRITING\t1\n#define B_DIRTY\t\t2\n\n \nenum data_mode {\n\tDATA_MODE_SLAB = 0,\n\tDATA_MODE_GET_FREE_PAGES = 1,\n\tDATA_MODE_VMALLOC = 2,\n\tDATA_MODE_LIMIT = 3\n};\n\nstruct dm_buffer {\n\t \n\tstruct rb_node node;\n\n\t \n\tsector_t block;\n\tvoid *data;\n\tunsigned char data_mode;\t\t \n\n\t \n\tatomic_t hold_count;\n\tunsigned long last_accessed;\n\n\t \n\tunsigned long state;\n\tstruct lru_entry lru;\n\tunsigned char list_mode;\t\t \n\tblk_status_t read_error;\n\tblk_status_t write_error;\n\tunsigned int dirty_start;\n\tunsigned int dirty_end;\n\tunsigned int write_start;\n\tunsigned int write_end;\n\tstruct list_head write_list;\n\tstruct dm_bufio_client *c;\n\tvoid (*end_io)(struct dm_buffer *b, blk_status_t bs);\n#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING\n#define MAX_STACK 10\n\tunsigned int stack_len;\n\tunsigned long stack_entries[MAX_STACK];\n#endif\n};\n\n \n\n \n\nstruct buffer_tree {\n\tunion {\n\t\tstruct rw_semaphore lock;\n\t\trwlock_t spinlock;\n\t} u;\n\tstruct rb_root root;\n} ____cacheline_aligned_in_smp;\n\nstruct dm_buffer_cache {\n\tstruct lru lru[LIST_SIZE];\n\t \n\tunsigned int num_locks;\n\tbool no_sleep;\n\tstruct buffer_tree trees[];\n};\n\nstatic DEFINE_STATIC_KEY_FALSE(no_sleep_enabled);\n\nstatic inline unsigned int cache_index(sector_t block, unsigned int num_locks)\n{\n\treturn dm_hash_locks_index(block, num_locks);\n}\n\nstatic inline void cache_read_lock(struct dm_buffer_cache *bc, sector_t block)\n{\n\tif (static_branch_unlikely(&no_sleep_enabled) && bc->no_sleep)\n\t\tread_lock_bh(&bc->trees[cache_index(block, bc->num_locks)].u.spinlock);\n\telse\n\t\tdown_read(&bc->trees[cache_index(block, bc->num_locks)].u.lock);\n}\n\nstatic inline void cache_read_unlock(struct dm_buffer_cache *bc, sector_t block)\n{\n\tif (static_branch_unlikely(&no_sleep_enabled) && bc->no_sleep)\n\t\tread_unlock_bh(&bc->trees[cache_index(block, bc->num_locks)].u.spinlock);\n\telse\n\t\tup_read(&bc->trees[cache_index(block, bc->num_locks)].u.lock);\n}\n\nstatic inline void cache_write_lock(struct dm_buffer_cache *bc, sector_t block)\n{\n\tif (static_branch_unlikely(&no_sleep_enabled) && bc->no_sleep)\n\t\twrite_lock_bh(&bc->trees[cache_index(block, bc->num_locks)].u.spinlock);\n\telse\n\t\tdown_write(&bc->trees[cache_index(block, bc->num_locks)].u.lock);\n}\n\nstatic inline void cache_write_unlock(struct dm_buffer_cache *bc, sector_t block)\n{\n\tif (static_branch_unlikely(&no_sleep_enabled) && bc->no_sleep)\n\t\twrite_unlock_bh(&bc->trees[cache_index(block, bc->num_locks)].u.spinlock);\n\telse\n\t\tup_write(&bc->trees[cache_index(block, bc->num_locks)].u.lock);\n}\n\n \nstruct lock_history {\n\tstruct dm_buffer_cache *cache;\n\tbool write;\n\tunsigned int previous;\n\tunsigned int no_previous;\n};\n\nstatic void lh_init(struct lock_history *lh, struct dm_buffer_cache *cache, bool write)\n{\n\tlh->cache = cache;\n\tlh->write = write;\n\tlh->no_previous = cache->num_locks;\n\tlh->previous = lh->no_previous;\n}\n\nstatic void __lh_lock(struct lock_history *lh, unsigned int index)\n{\n\tif (lh->write) {\n\t\tif (static_branch_unlikely(&no_sleep_enabled) && lh->cache->no_sleep)\n\t\t\twrite_lock_bh(&lh->cache->trees[index].u.spinlock);\n\t\telse\n\t\t\tdown_write(&lh->cache->trees[index].u.lock);\n\t} else {\n\t\tif (static_branch_unlikely(&no_sleep_enabled) && lh->cache->no_sleep)\n\t\t\tread_lock_bh(&lh->cache->trees[index].u.spinlock);\n\t\telse\n\t\t\tdown_read(&lh->cache->trees[index].u.lock);\n\t}\n}\n\nstatic void __lh_unlock(struct lock_history *lh, unsigned int index)\n{\n\tif (lh->write) {\n\t\tif (static_branch_unlikely(&no_sleep_enabled) && lh->cache->no_sleep)\n\t\t\twrite_unlock_bh(&lh->cache->trees[index].u.spinlock);\n\t\telse\n\t\t\tup_write(&lh->cache->trees[index].u.lock);\n\t} else {\n\t\tif (static_branch_unlikely(&no_sleep_enabled) && lh->cache->no_sleep)\n\t\t\tread_unlock_bh(&lh->cache->trees[index].u.spinlock);\n\t\telse\n\t\t\tup_read(&lh->cache->trees[index].u.lock);\n\t}\n}\n\n \nstatic void lh_exit(struct lock_history *lh)\n{\n\tif (lh->previous != lh->no_previous) {\n\t\t__lh_unlock(lh, lh->previous);\n\t\tlh->previous = lh->no_previous;\n\t}\n}\n\n \nstatic void lh_next(struct lock_history *lh, sector_t b)\n{\n\tunsigned int index = cache_index(b, lh->no_previous);  \n\n\tif (lh->previous != lh->no_previous) {\n\t\tif (lh->previous != index) {\n\t\t\t__lh_unlock(lh, lh->previous);\n\t\t\t__lh_lock(lh, index);\n\t\t\tlh->previous = index;\n\t\t}\n\t} else {\n\t\t__lh_lock(lh, index);\n\t\tlh->previous = index;\n\t}\n}\n\nstatic inline struct dm_buffer *le_to_buffer(struct lru_entry *le)\n{\n\treturn container_of(le, struct dm_buffer, lru);\n}\n\nstatic struct dm_buffer *list_to_buffer(struct list_head *l)\n{\n\tstruct lru_entry *le = list_entry(l, struct lru_entry, list);\n\n\tif (!le)\n\t\treturn NULL;\n\n\treturn le_to_buffer(le);\n}\n\nstatic void cache_init(struct dm_buffer_cache *bc, unsigned int num_locks, bool no_sleep)\n{\n\tunsigned int i;\n\n\tbc->num_locks = num_locks;\n\tbc->no_sleep = no_sleep;\n\n\tfor (i = 0; i < bc->num_locks; i++) {\n\t\tif (no_sleep)\n\t\t\trwlock_init(&bc->trees[i].u.spinlock);\n\t\telse\n\t\t\tinit_rwsem(&bc->trees[i].u.lock);\n\t\tbc->trees[i].root = RB_ROOT;\n\t}\n\n\tlru_init(&bc->lru[LIST_CLEAN]);\n\tlru_init(&bc->lru[LIST_DIRTY]);\n}\n\nstatic void cache_destroy(struct dm_buffer_cache *bc)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < bc->num_locks; i++)\n\t\tWARN_ON_ONCE(!RB_EMPTY_ROOT(&bc->trees[i].root));\n\n\tlru_destroy(&bc->lru[LIST_CLEAN]);\n\tlru_destroy(&bc->lru[LIST_DIRTY]);\n}\n\n \n\n \nstatic inline unsigned long cache_count(struct dm_buffer_cache *bc, int list_mode)\n{\n\treturn bc->lru[list_mode].count;\n}\n\nstatic inline unsigned long cache_total(struct dm_buffer_cache *bc)\n{\n\treturn cache_count(bc, LIST_CLEAN) + cache_count(bc, LIST_DIRTY);\n}\n\n \n\n \nstatic struct dm_buffer *__cache_get(const struct rb_root *root, sector_t block)\n{\n\tstruct rb_node *n = root->rb_node;\n\tstruct dm_buffer *b;\n\n\twhile (n) {\n\t\tb = container_of(n, struct dm_buffer, node);\n\n\t\tif (b->block == block)\n\t\t\treturn b;\n\n\t\tn = block < b->block ? n->rb_left : n->rb_right;\n\t}\n\n\treturn NULL;\n}\n\nstatic void __cache_inc_buffer(struct dm_buffer *b)\n{\n\tatomic_inc(&b->hold_count);\n\tWRITE_ONCE(b->last_accessed, jiffies);\n}\n\nstatic struct dm_buffer *cache_get(struct dm_buffer_cache *bc, sector_t block)\n{\n\tstruct dm_buffer *b;\n\n\tcache_read_lock(bc, block);\n\tb = __cache_get(&bc->trees[cache_index(block, bc->num_locks)].root, block);\n\tif (b) {\n\t\tlru_reference(&b->lru);\n\t\t__cache_inc_buffer(b);\n\t}\n\tcache_read_unlock(bc, block);\n\n\treturn b;\n}\n\n \n\n \nstatic bool cache_put(struct dm_buffer_cache *bc, struct dm_buffer *b)\n{\n\tbool r;\n\n\tcache_read_lock(bc, b->block);\n\tBUG_ON(!atomic_read(&b->hold_count));\n\tr = atomic_dec_and_test(&b->hold_count);\n\tcache_read_unlock(bc, b->block);\n\n\treturn r;\n}\n\n \n\ntypedef enum evict_result (*b_predicate)(struct dm_buffer *, void *);\n\n \nstruct evict_wrapper {\n\tstruct lock_history *lh;\n\tb_predicate pred;\n\tvoid *context;\n};\n\n \nstatic enum evict_result __evict_pred(struct lru_entry *le, void *context)\n{\n\tstruct evict_wrapper *w = context;\n\tstruct dm_buffer *b = le_to_buffer(le);\n\n\tlh_next(w->lh, b->block);\n\n\tif (atomic_read(&b->hold_count))\n\t\treturn ER_DONT_EVICT;\n\n\treturn w->pred(b, w->context);\n}\n\nstatic struct dm_buffer *__cache_evict(struct dm_buffer_cache *bc, int list_mode,\n\t\t\t\t       b_predicate pred, void *context,\n\t\t\t\t       struct lock_history *lh)\n{\n\tstruct evict_wrapper w = {.lh = lh, .pred = pred, .context = context};\n\tstruct lru_entry *le;\n\tstruct dm_buffer *b;\n\n\tle = lru_evict(&bc->lru[list_mode], __evict_pred, &w, bc->no_sleep);\n\tif (!le)\n\t\treturn NULL;\n\n\tb = le_to_buffer(le);\n\t \n\trb_erase(&b->node, &bc->trees[cache_index(b->block, bc->num_locks)].root);\n\n\treturn b;\n}\n\nstatic struct dm_buffer *cache_evict(struct dm_buffer_cache *bc, int list_mode,\n\t\t\t\t     b_predicate pred, void *context)\n{\n\tstruct dm_buffer *b;\n\tstruct lock_history lh;\n\n\tlh_init(&lh, bc, true);\n\tb = __cache_evict(bc, list_mode, pred, context, &lh);\n\tlh_exit(&lh);\n\n\treturn b;\n}\n\n \n\n \nstatic void cache_mark(struct dm_buffer_cache *bc, struct dm_buffer *b, int list_mode)\n{\n\tcache_write_lock(bc, b->block);\n\tif (list_mode != b->list_mode) {\n\t\tlru_remove(&bc->lru[b->list_mode], &b->lru);\n\t\tb->list_mode = list_mode;\n\t\tlru_insert(&bc->lru[b->list_mode], &b->lru);\n\t}\n\tcache_write_unlock(bc, b->block);\n}\n\n \n\n \nstatic void __cache_mark_many(struct dm_buffer_cache *bc, int old_mode, int new_mode,\n\t\t\t      b_predicate pred, void *context, struct lock_history *lh)\n{\n\tstruct lru_entry *le;\n\tstruct dm_buffer *b;\n\tstruct evict_wrapper w = {.lh = lh, .pred = pred, .context = context};\n\n\twhile (true) {\n\t\tle = lru_evict(&bc->lru[old_mode], __evict_pred, &w, bc->no_sleep);\n\t\tif (!le)\n\t\t\tbreak;\n\n\t\tb = le_to_buffer(le);\n\t\tb->list_mode = new_mode;\n\t\tlru_insert(&bc->lru[b->list_mode], &b->lru);\n\t}\n}\n\nstatic void cache_mark_many(struct dm_buffer_cache *bc, int old_mode, int new_mode,\n\t\t\t    b_predicate pred, void *context)\n{\n\tstruct lock_history lh;\n\n\tlh_init(&lh, bc, true);\n\t__cache_mark_many(bc, old_mode, new_mode, pred, context, &lh);\n\tlh_exit(&lh);\n}\n\n \n\n \n\n \nenum it_action {\n\tIT_NEXT,\n\tIT_COMPLETE,\n};\n\ntypedef enum it_action (*iter_fn)(struct dm_buffer *b, void *context);\n\nstatic void __cache_iterate(struct dm_buffer_cache *bc, int list_mode,\n\t\t\t    iter_fn fn, void *context, struct lock_history *lh)\n{\n\tstruct lru *lru = &bc->lru[list_mode];\n\tstruct lru_entry *le, *first;\n\n\tif (!lru->cursor)\n\t\treturn;\n\n\tfirst = le = to_le(lru->cursor);\n\tdo {\n\t\tstruct dm_buffer *b = le_to_buffer(le);\n\n\t\tlh_next(lh, b->block);\n\n\t\tswitch (fn(b, context)) {\n\t\tcase IT_NEXT:\n\t\t\tbreak;\n\n\t\tcase IT_COMPLETE:\n\t\t\treturn;\n\t\t}\n\t\tcond_resched();\n\n\t\tle = to_le(le->list.next);\n\t} while (le != first);\n}\n\nstatic void cache_iterate(struct dm_buffer_cache *bc, int list_mode,\n\t\t\t  iter_fn fn, void *context)\n{\n\tstruct lock_history lh;\n\n\tlh_init(&lh, bc, false);\n\t__cache_iterate(bc, list_mode, fn, context, &lh);\n\tlh_exit(&lh);\n}\n\n \n\n \nstatic bool __cache_insert(struct rb_root *root, struct dm_buffer *b)\n{\n\tstruct rb_node **new = &root->rb_node, *parent = NULL;\n\tstruct dm_buffer *found;\n\n\twhile (*new) {\n\t\tfound = container_of(*new, struct dm_buffer, node);\n\n\t\tif (found->block == b->block)\n\t\t\treturn false;\n\n\t\tparent = *new;\n\t\tnew = b->block < found->block ?\n\t\t\t&found->node.rb_left : &found->node.rb_right;\n\t}\n\n\trb_link_node(&b->node, parent, new);\n\trb_insert_color(&b->node, root);\n\n\treturn true;\n}\n\nstatic bool cache_insert(struct dm_buffer_cache *bc, struct dm_buffer *b)\n{\n\tbool r;\n\n\tif (WARN_ON_ONCE(b->list_mode >= LIST_SIZE))\n\t\treturn false;\n\n\tcache_write_lock(bc, b->block);\n\tBUG_ON(atomic_read(&b->hold_count) != 1);\n\tr = __cache_insert(&bc->trees[cache_index(b->block, bc->num_locks)].root, b);\n\tif (r)\n\t\tlru_insert(&bc->lru[b->list_mode], &b->lru);\n\tcache_write_unlock(bc, b->block);\n\n\treturn r;\n}\n\n \n\n \nstatic bool cache_remove(struct dm_buffer_cache *bc, struct dm_buffer *b)\n{\n\tbool r;\n\n\tcache_write_lock(bc, b->block);\n\n\tif (atomic_read(&b->hold_count) != 1) {\n\t\tr = false;\n\t} else {\n\t\tr = true;\n\t\trb_erase(&b->node, &bc->trees[cache_index(b->block, bc->num_locks)].root);\n\t\tlru_remove(&bc->lru[b->list_mode], &b->lru);\n\t}\n\n\tcache_write_unlock(bc, b->block);\n\n\treturn r;\n}\n\n \n\ntypedef void (*b_release)(struct dm_buffer *);\n\nstatic struct dm_buffer *__find_next(struct rb_root *root, sector_t block)\n{\n\tstruct rb_node *n = root->rb_node;\n\tstruct dm_buffer *b;\n\tstruct dm_buffer *best = NULL;\n\n\twhile (n) {\n\t\tb = container_of(n, struct dm_buffer, node);\n\n\t\tif (b->block == block)\n\t\t\treturn b;\n\n\t\tif (block <= b->block) {\n\t\t\tn = n->rb_left;\n\t\t\tbest = b;\n\t\t} else {\n\t\t\tn = n->rb_right;\n\t\t}\n\t}\n\n\treturn best;\n}\n\nstatic void __remove_range(struct dm_buffer_cache *bc,\n\t\t\t   struct rb_root *root,\n\t\t\t   sector_t begin, sector_t end,\n\t\t\t   b_predicate pred, b_release release)\n{\n\tstruct dm_buffer *b;\n\n\twhile (true) {\n\t\tcond_resched();\n\n\t\tb = __find_next(root, begin);\n\t\tif (!b || (b->block >= end))\n\t\t\tbreak;\n\n\t\tbegin = b->block + 1;\n\n\t\tif (atomic_read(&b->hold_count))\n\t\t\tcontinue;\n\n\t\tif (pred(b, NULL) == ER_EVICT) {\n\t\t\trb_erase(&b->node, root);\n\t\t\tlru_remove(&bc->lru[b->list_mode], &b->lru);\n\t\t\trelease(b);\n\t\t}\n\t}\n}\n\nstatic void cache_remove_range(struct dm_buffer_cache *bc,\n\t\t\t       sector_t begin, sector_t end,\n\t\t\t       b_predicate pred, b_release release)\n{\n\tunsigned int i;\n\n\tBUG_ON(bc->no_sleep);\n\tfor (i = 0; i < bc->num_locks; i++) {\n\t\tdown_write(&bc->trees[i].u.lock);\n\t\t__remove_range(bc, &bc->trees[i].root, begin, end, pred, release);\n\t\tup_write(&bc->trees[i].u.lock);\n\t}\n}\n\n \n\n \nstruct dm_bufio_client {\n\tstruct block_device *bdev;\n\tunsigned int block_size;\n\ts8 sectors_per_block_bits;\n\n\tbool no_sleep;\n\tstruct mutex lock;\n\tspinlock_t spinlock;\n\n\tint async_write_error;\n\n\tvoid (*alloc_callback)(struct dm_buffer *buf);\n\tvoid (*write_callback)(struct dm_buffer *buf);\n\tstruct kmem_cache *slab_buffer;\n\tstruct kmem_cache *slab_cache;\n\tstruct dm_io_client *dm_io;\n\n\tstruct list_head reserved_buffers;\n\tunsigned int need_reserved_buffers;\n\n\tunsigned int minimum_buffers;\n\n\tsector_t start;\n\n\tstruct shrinker shrinker;\n\tstruct work_struct shrink_work;\n\tatomic_long_t need_shrink;\n\n\twait_queue_head_t free_buffer_wait;\n\n\tstruct list_head client_list;\n\n\t \n\tunsigned long oldest_buffer;\n\n\tstruct dm_buffer_cache cache;  \n};\n\n \n\n#define dm_bufio_in_request()\t(!!current->bio_list)\n\nstatic void dm_bufio_lock(struct dm_bufio_client *c)\n{\n\tif (static_branch_unlikely(&no_sleep_enabled) && c->no_sleep)\n\t\tspin_lock_bh(&c->spinlock);\n\telse\n\t\tmutex_lock_nested(&c->lock, dm_bufio_in_request());\n}\n\nstatic void dm_bufio_unlock(struct dm_bufio_client *c)\n{\n\tif (static_branch_unlikely(&no_sleep_enabled) && c->no_sleep)\n\t\tspin_unlock_bh(&c->spinlock);\n\telse\n\t\tmutex_unlock(&c->lock);\n}\n\n \n\n \nstatic unsigned long dm_bufio_default_cache_size;\n\n \nstatic unsigned long dm_bufio_cache_size;\n\n \nstatic unsigned long dm_bufio_cache_size_latch;\n\nstatic DEFINE_SPINLOCK(global_spinlock);\n\n \nstatic unsigned int dm_bufio_max_age = DM_BUFIO_DEFAULT_AGE_SECS;\nstatic unsigned long dm_bufio_retain_bytes = DM_BUFIO_DEFAULT_RETAIN_BYTES;\n\nstatic unsigned long dm_bufio_peak_allocated;\nstatic unsigned long dm_bufio_allocated_kmem_cache;\nstatic unsigned long dm_bufio_allocated_get_free_pages;\nstatic unsigned long dm_bufio_allocated_vmalloc;\nstatic unsigned long dm_bufio_current_allocated;\n\n \n\n \nstatic int dm_bufio_client_count;\n\n \nstatic LIST_HEAD(dm_bufio_all_clients);\n\n \nstatic DEFINE_MUTEX(dm_bufio_clients_lock);\n\nstatic struct workqueue_struct *dm_bufio_wq;\nstatic struct delayed_work dm_bufio_cleanup_old_work;\nstatic struct work_struct dm_bufio_replacement_work;\n\n\n#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING\nstatic void buffer_record_stack(struct dm_buffer *b)\n{\n\tb->stack_len = stack_trace_save(b->stack_entries, MAX_STACK, 2);\n}\n#endif\n\n \n\nstatic void adjust_total_allocated(struct dm_buffer *b, bool unlink)\n{\n\tunsigned char data_mode;\n\tlong diff;\n\n\tstatic unsigned long * const class_ptr[DATA_MODE_LIMIT] = {\n\t\t&dm_bufio_allocated_kmem_cache,\n\t\t&dm_bufio_allocated_get_free_pages,\n\t\t&dm_bufio_allocated_vmalloc,\n\t};\n\n\tdata_mode = b->data_mode;\n\tdiff = (long)b->c->block_size;\n\tif (unlink)\n\t\tdiff = -diff;\n\n\tspin_lock(&global_spinlock);\n\n\t*class_ptr[data_mode] += diff;\n\n\tdm_bufio_current_allocated += diff;\n\n\tif (dm_bufio_current_allocated > dm_bufio_peak_allocated)\n\t\tdm_bufio_peak_allocated = dm_bufio_current_allocated;\n\n\tif (!unlink) {\n\t\tif (dm_bufio_current_allocated > dm_bufio_cache_size)\n\t\t\tqueue_work(dm_bufio_wq, &dm_bufio_replacement_work);\n\t}\n\n\tspin_unlock(&global_spinlock);\n}\n\n \nstatic void __cache_size_refresh(void)\n{\n\tif (WARN_ON(!mutex_is_locked(&dm_bufio_clients_lock)))\n\t\treturn;\n\tif (WARN_ON(dm_bufio_client_count < 0))\n\t\treturn;\n\n\tdm_bufio_cache_size_latch = READ_ONCE(dm_bufio_cache_size);\n\n\t \n\tif (!dm_bufio_cache_size_latch) {\n\t\t(void)cmpxchg(&dm_bufio_cache_size, 0,\n\t\t\t      dm_bufio_default_cache_size);\n\t\tdm_bufio_cache_size_latch = dm_bufio_default_cache_size;\n\t}\n}\n\n \nstatic void *alloc_buffer_data(struct dm_bufio_client *c, gfp_t gfp_mask,\n\t\t\t       unsigned char *data_mode)\n{\n\tif (unlikely(c->slab_cache != NULL)) {\n\t\t*data_mode = DATA_MODE_SLAB;\n\t\treturn kmem_cache_alloc(c->slab_cache, gfp_mask);\n\t}\n\n\tif (c->block_size <= KMALLOC_MAX_SIZE &&\n\t    gfp_mask & __GFP_NORETRY) {\n\t\t*data_mode = DATA_MODE_GET_FREE_PAGES;\n\t\treturn (void *)__get_free_pages(gfp_mask,\n\t\t\t\t\t\tc->sectors_per_block_bits - (PAGE_SHIFT - SECTOR_SHIFT));\n\t}\n\n\t*data_mode = DATA_MODE_VMALLOC;\n\n\treturn __vmalloc(c->block_size, gfp_mask);\n}\n\n \nstatic void free_buffer_data(struct dm_bufio_client *c,\n\t\t\t     void *data, unsigned char data_mode)\n{\n\tswitch (data_mode) {\n\tcase DATA_MODE_SLAB:\n\t\tkmem_cache_free(c->slab_cache, data);\n\t\tbreak;\n\n\tcase DATA_MODE_GET_FREE_PAGES:\n\t\tfree_pages((unsigned long)data,\n\t\t\t   c->sectors_per_block_bits - (PAGE_SHIFT - SECTOR_SHIFT));\n\t\tbreak;\n\n\tcase DATA_MODE_VMALLOC:\n\t\tvfree(data);\n\t\tbreak;\n\n\tdefault:\n\t\tDMCRIT(\"dm_bufio_free_buffer_data: bad data mode: %d\",\n\t\t       data_mode);\n\t\tBUG();\n\t}\n}\n\n \nstatic struct dm_buffer *alloc_buffer(struct dm_bufio_client *c, gfp_t gfp_mask)\n{\n\tstruct dm_buffer *b = kmem_cache_alloc(c->slab_buffer, gfp_mask);\n\n\tif (!b)\n\t\treturn NULL;\n\n\tb->c = c;\n\n\tb->data = alloc_buffer_data(c, gfp_mask, &b->data_mode);\n\tif (!b->data) {\n\t\tkmem_cache_free(c->slab_buffer, b);\n\t\treturn NULL;\n\t}\n\tadjust_total_allocated(b, false);\n\n#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING\n\tb->stack_len = 0;\n#endif\n\treturn b;\n}\n\n \nstatic void free_buffer(struct dm_buffer *b)\n{\n\tstruct dm_bufio_client *c = b->c;\n\n\tadjust_total_allocated(b, true);\n\tfree_buffer_data(c, b->data, b->data_mode);\n\tkmem_cache_free(c->slab_buffer, b);\n}\n\n \n\n \nstatic void dmio_complete(unsigned long error, void *context)\n{\n\tstruct dm_buffer *b = context;\n\n\tb->end_io(b, unlikely(error != 0) ? BLK_STS_IOERR : 0);\n}\n\nstatic void use_dmio(struct dm_buffer *b, enum req_op op, sector_t sector,\n\t\t     unsigned int n_sectors, unsigned int offset)\n{\n\tint r;\n\tstruct dm_io_request io_req = {\n\t\t.bi_opf = op,\n\t\t.notify.fn = dmio_complete,\n\t\t.notify.context = b,\n\t\t.client = b->c->dm_io,\n\t};\n\tstruct dm_io_region region = {\n\t\t.bdev = b->c->bdev,\n\t\t.sector = sector,\n\t\t.count = n_sectors,\n\t};\n\n\tif (b->data_mode != DATA_MODE_VMALLOC) {\n\t\tio_req.mem.type = DM_IO_KMEM;\n\t\tio_req.mem.ptr.addr = (char *)b->data + offset;\n\t} else {\n\t\tio_req.mem.type = DM_IO_VMA;\n\t\tio_req.mem.ptr.vma = (char *)b->data + offset;\n\t}\n\n\tr = dm_io(&io_req, 1, &region, NULL);\n\tif (unlikely(r))\n\t\tb->end_io(b, errno_to_blk_status(r));\n}\n\nstatic void bio_complete(struct bio *bio)\n{\n\tstruct dm_buffer *b = bio->bi_private;\n\tblk_status_t status = bio->bi_status;\n\n\tbio_uninit(bio);\n\tkfree(bio);\n\tb->end_io(b, status);\n}\n\nstatic void use_bio(struct dm_buffer *b, enum req_op op, sector_t sector,\n\t\t    unsigned int n_sectors, unsigned int offset)\n{\n\tstruct bio *bio;\n\tchar *ptr;\n\tunsigned int len;\n\n\tbio = bio_kmalloc(1, GFP_NOWAIT | __GFP_NORETRY | __GFP_NOWARN);\n\tif (!bio) {\n\t\tuse_dmio(b, op, sector, n_sectors, offset);\n\t\treturn;\n\t}\n\tbio_init(bio, b->c->bdev, bio->bi_inline_vecs, 1, op);\n\tbio->bi_iter.bi_sector = sector;\n\tbio->bi_end_io = bio_complete;\n\tbio->bi_private = b;\n\n\tptr = (char *)b->data + offset;\n\tlen = n_sectors << SECTOR_SHIFT;\n\n\t__bio_add_page(bio, virt_to_page(ptr), len, offset_in_page(ptr));\n\n\tsubmit_bio(bio);\n}\n\nstatic inline sector_t block_to_sector(struct dm_bufio_client *c, sector_t block)\n{\n\tsector_t sector;\n\n\tif (likely(c->sectors_per_block_bits >= 0))\n\t\tsector = block << c->sectors_per_block_bits;\n\telse\n\t\tsector = block * (c->block_size >> SECTOR_SHIFT);\n\tsector += c->start;\n\n\treturn sector;\n}\n\nstatic void submit_io(struct dm_buffer *b, enum req_op op,\n\t\t      void (*end_io)(struct dm_buffer *, blk_status_t))\n{\n\tunsigned int n_sectors;\n\tsector_t sector;\n\tunsigned int offset, end;\n\n\tb->end_io = end_io;\n\n\tsector = block_to_sector(b->c, b->block);\n\n\tif (op != REQ_OP_WRITE) {\n\t\tn_sectors = b->c->block_size >> SECTOR_SHIFT;\n\t\toffset = 0;\n\t} else {\n\t\tif (b->c->write_callback)\n\t\t\tb->c->write_callback(b);\n\t\toffset = b->write_start;\n\t\tend = b->write_end;\n\t\toffset &= -DM_BUFIO_WRITE_ALIGN;\n\t\tend += DM_BUFIO_WRITE_ALIGN - 1;\n\t\tend &= -DM_BUFIO_WRITE_ALIGN;\n\t\tif (unlikely(end > b->c->block_size))\n\t\t\tend = b->c->block_size;\n\n\t\tsector += offset >> SECTOR_SHIFT;\n\t\tn_sectors = (end - offset) >> SECTOR_SHIFT;\n\t}\n\n\tif (b->data_mode != DATA_MODE_VMALLOC)\n\t\tuse_bio(b, op, sector, n_sectors, offset);\n\telse\n\t\tuse_dmio(b, op, sector, n_sectors, offset);\n}\n\n \n\n \nstatic void write_endio(struct dm_buffer *b, blk_status_t status)\n{\n\tb->write_error = status;\n\tif (unlikely(status)) {\n\t\tstruct dm_bufio_client *c = b->c;\n\n\t\t(void)cmpxchg(&c->async_write_error, 0,\n\t\t\t\tblk_status_to_errno(status));\n\t}\n\n\tBUG_ON(!test_bit(B_WRITING, &b->state));\n\n\tsmp_mb__before_atomic();\n\tclear_bit(B_WRITING, &b->state);\n\tsmp_mb__after_atomic();\n\n\twake_up_bit(&b->state, B_WRITING);\n}\n\n \nstatic void __write_dirty_buffer(struct dm_buffer *b,\n\t\t\t\t struct list_head *write_list)\n{\n\tif (!test_bit(B_DIRTY, &b->state))\n\t\treturn;\n\n\tclear_bit(B_DIRTY, &b->state);\n\twait_on_bit_lock_io(&b->state, B_WRITING, TASK_UNINTERRUPTIBLE);\n\n\tb->write_start = b->dirty_start;\n\tb->write_end = b->dirty_end;\n\n\tif (!write_list)\n\t\tsubmit_io(b, REQ_OP_WRITE, write_endio);\n\telse\n\t\tlist_add_tail(&b->write_list, write_list);\n}\n\nstatic void __flush_write_list(struct list_head *write_list)\n{\n\tstruct blk_plug plug;\n\n\tblk_start_plug(&plug);\n\twhile (!list_empty(write_list)) {\n\t\tstruct dm_buffer *b =\n\t\t\tlist_entry(write_list->next, struct dm_buffer, write_list);\n\t\tlist_del(&b->write_list);\n\t\tsubmit_io(b, REQ_OP_WRITE, write_endio);\n\t\tcond_resched();\n\t}\n\tblk_finish_plug(&plug);\n}\n\n \nstatic void __make_buffer_clean(struct dm_buffer *b)\n{\n\tBUG_ON(atomic_read(&b->hold_count));\n\n\t \n\tif (!smp_load_acquire(&b->state))\t \n\t\treturn;\n\n\twait_on_bit_io(&b->state, B_READING, TASK_UNINTERRUPTIBLE);\n\t__write_dirty_buffer(b, NULL);\n\twait_on_bit_io(&b->state, B_WRITING, TASK_UNINTERRUPTIBLE);\n}\n\nstatic enum evict_result is_clean(struct dm_buffer *b, void *context)\n{\n\tstruct dm_bufio_client *c = context;\n\n\t \n\tif (WARN_ON_ONCE(test_bit(B_WRITING, &b->state)))\n\t\treturn ER_DONT_EVICT;\n\tif (WARN_ON_ONCE(test_bit(B_DIRTY, &b->state)))\n\t\treturn ER_DONT_EVICT;\n\tif (WARN_ON_ONCE(b->list_mode != LIST_CLEAN))\n\t\treturn ER_DONT_EVICT;\n\n\tif (static_branch_unlikely(&no_sleep_enabled) && c->no_sleep &&\n\t    unlikely(test_bit(B_READING, &b->state)))\n\t\treturn ER_DONT_EVICT;\n\n\treturn ER_EVICT;\n}\n\nstatic enum evict_result is_dirty(struct dm_buffer *b, void *context)\n{\n\t \n\tif (WARN_ON_ONCE(test_bit(B_READING, &b->state)))\n\t\treturn ER_DONT_EVICT;\n\tif (WARN_ON_ONCE(b->list_mode != LIST_DIRTY))\n\t\treturn ER_DONT_EVICT;\n\n\treturn ER_EVICT;\n}\n\n \nstatic struct dm_buffer *__get_unclaimed_buffer(struct dm_bufio_client *c)\n{\n\tstruct dm_buffer *b;\n\n\tb = cache_evict(&c->cache, LIST_CLEAN, is_clean, c);\n\tif (b) {\n\t\t \n\t\t__make_buffer_clean(b);\n\t\treturn b;\n\t}\n\n\tif (static_branch_unlikely(&no_sleep_enabled) && c->no_sleep)\n\t\treturn NULL;\n\n\tb = cache_evict(&c->cache, LIST_DIRTY, is_dirty, NULL);\n\tif (b) {\n\t\t__make_buffer_clean(b);\n\t\treturn b;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic void __wait_for_free_buffer(struct dm_bufio_client *c)\n{\n\tDECLARE_WAITQUEUE(wait, current);\n\n\tadd_wait_queue(&c->free_buffer_wait, &wait);\n\tset_current_state(TASK_UNINTERRUPTIBLE);\n\tdm_bufio_unlock(c);\n\n\t \n\tio_schedule_timeout(5 * HZ);\n\n\tremove_wait_queue(&c->free_buffer_wait, &wait);\n\n\tdm_bufio_lock(c);\n}\n\nenum new_flag {\n\tNF_FRESH = 0,\n\tNF_READ = 1,\n\tNF_GET = 2,\n\tNF_PREFETCH = 3\n};\n\n \nstatic struct dm_buffer *__alloc_buffer_wait_no_callback(struct dm_bufio_client *c, enum new_flag nf)\n{\n\tstruct dm_buffer *b;\n\tbool tried_noio_alloc = false;\n\n\t \n\twhile (1) {\n\t\tif (dm_bufio_cache_size_latch != 1) {\n\t\t\tb = alloc_buffer(c, GFP_NOWAIT | __GFP_NORETRY | __GFP_NOMEMALLOC | __GFP_NOWARN);\n\t\t\tif (b)\n\t\t\t\treturn b;\n\t\t}\n\n\t\tif (nf == NF_PREFETCH)\n\t\t\treturn NULL;\n\n\t\tif (dm_bufio_cache_size_latch != 1 && !tried_noio_alloc) {\n\t\t\tdm_bufio_unlock(c);\n\t\t\tb = alloc_buffer(c, GFP_NOIO | __GFP_NORETRY | __GFP_NOMEMALLOC | __GFP_NOWARN);\n\t\t\tdm_bufio_lock(c);\n\t\t\tif (b)\n\t\t\t\treturn b;\n\t\t\ttried_noio_alloc = true;\n\t\t}\n\n\t\tif (!list_empty(&c->reserved_buffers)) {\n\t\t\tb = list_to_buffer(c->reserved_buffers.next);\n\t\t\tlist_del(&b->lru.list);\n\t\t\tc->need_reserved_buffers++;\n\n\t\t\treturn b;\n\t\t}\n\n\t\tb = __get_unclaimed_buffer(c);\n\t\tif (b)\n\t\t\treturn b;\n\n\t\t__wait_for_free_buffer(c);\n\t}\n}\n\nstatic struct dm_buffer *__alloc_buffer_wait(struct dm_bufio_client *c, enum new_flag nf)\n{\n\tstruct dm_buffer *b = __alloc_buffer_wait_no_callback(c, nf);\n\n\tif (!b)\n\t\treturn NULL;\n\n\tif (c->alloc_callback)\n\t\tc->alloc_callback(b);\n\n\treturn b;\n}\n\n \nstatic void __free_buffer_wake(struct dm_buffer *b)\n{\n\tstruct dm_bufio_client *c = b->c;\n\n\tb->block = -1;\n\tif (!c->need_reserved_buffers)\n\t\tfree_buffer(b);\n\telse {\n\t\tlist_add(&b->lru.list, &c->reserved_buffers);\n\t\tc->need_reserved_buffers--;\n\t}\n\n\t \n\tif (unlikely(waitqueue_active(&c->free_buffer_wait)))\n\t\twake_up(&c->free_buffer_wait);\n}\n\nstatic enum evict_result cleaned(struct dm_buffer *b, void *context)\n{\n\tif (WARN_ON_ONCE(test_bit(B_READING, &b->state)))\n\t\treturn ER_DONT_EVICT;  \n\n\tif (test_bit(B_DIRTY, &b->state) || test_bit(B_WRITING, &b->state))\n\t\treturn ER_DONT_EVICT;\n\telse\n\t\treturn ER_EVICT;\n}\n\nstatic void __move_clean_buffers(struct dm_bufio_client *c)\n{\n\tcache_mark_many(&c->cache, LIST_DIRTY, LIST_CLEAN, cleaned, NULL);\n}\n\nstruct write_context {\n\tint no_wait;\n\tstruct list_head *write_list;\n};\n\nstatic enum it_action write_one(struct dm_buffer *b, void *context)\n{\n\tstruct write_context *wc = context;\n\n\tif (wc->no_wait && test_bit(B_WRITING, &b->state))\n\t\treturn IT_COMPLETE;\n\n\t__write_dirty_buffer(b, wc->write_list);\n\treturn IT_NEXT;\n}\n\nstatic void __write_dirty_buffers_async(struct dm_bufio_client *c, int no_wait,\n\t\t\t\t\tstruct list_head *write_list)\n{\n\tstruct write_context wc = {.no_wait = no_wait, .write_list = write_list};\n\n\t__move_clean_buffers(c);\n\tcache_iterate(&c->cache, LIST_DIRTY, write_one, &wc);\n}\n\n \nstatic void __check_watermark(struct dm_bufio_client *c,\n\t\t\t      struct list_head *write_list)\n{\n\tif (cache_count(&c->cache, LIST_DIRTY) >\n\t    cache_count(&c->cache, LIST_CLEAN) * DM_BUFIO_WRITEBACK_RATIO)\n\t\t__write_dirty_buffers_async(c, 1, write_list);\n}\n\n \n\nstatic void cache_put_and_wake(struct dm_bufio_client *c, struct dm_buffer *b)\n{\n\t \n\tif (cache_put(&c->cache, b) &&\n\t    unlikely(waitqueue_active(&c->free_buffer_wait)))\n\t\twake_up(&c->free_buffer_wait);\n}\n\n \nstatic struct dm_buffer *__bufio_new(struct dm_bufio_client *c, sector_t block,\n\t\t\t\t     enum new_flag nf, int *need_submit,\n\t\t\t\t     struct list_head *write_list)\n{\n\tstruct dm_buffer *b, *new_b = NULL;\n\n\t*need_submit = 0;\n\n\t \n\tif (WARN_ON_ONCE(nf == NF_GET))\n\t\treturn NULL;\n\n\tnew_b = __alloc_buffer_wait(c, nf);\n\tif (!new_b)\n\t\treturn NULL;\n\n\t \n\tb = cache_get(&c->cache, block);\n\tif (b) {\n\t\t__free_buffer_wake(new_b);\n\t\tgoto found_buffer;\n\t}\n\n\t__check_watermark(c, write_list);\n\n\tb = new_b;\n\tatomic_set(&b->hold_count, 1);\n\tWRITE_ONCE(b->last_accessed, jiffies);\n\tb->block = block;\n\tb->read_error = 0;\n\tb->write_error = 0;\n\tb->list_mode = LIST_CLEAN;\n\n\tif (nf == NF_FRESH)\n\t\tb->state = 0;\n\telse {\n\t\tb->state = 1 << B_READING;\n\t\t*need_submit = 1;\n\t}\n\n\t \n\tcache_insert(&c->cache, b);\n\n\treturn b;\n\nfound_buffer:\n\tif (nf == NF_PREFETCH) {\n\t\tcache_put_and_wake(c, b);\n\t\treturn NULL;\n\t}\n\n\t \n\tif (nf == NF_GET && unlikely(test_bit_acquire(B_READING, &b->state))) {\n\t\tcache_put_and_wake(c, b);\n\t\treturn NULL;\n\t}\n\n\treturn b;\n}\n\n \nstatic void read_endio(struct dm_buffer *b, blk_status_t status)\n{\n\tb->read_error = status;\n\n\tBUG_ON(!test_bit(B_READING, &b->state));\n\n\tsmp_mb__before_atomic();\n\tclear_bit(B_READING, &b->state);\n\tsmp_mb__after_atomic();\n\n\twake_up_bit(&b->state, B_READING);\n}\n\n \nstatic void *new_read(struct dm_bufio_client *c, sector_t block,\n\t\t      enum new_flag nf, struct dm_buffer **bp)\n{\n\tint need_submit = 0;\n\tstruct dm_buffer *b;\n\n\tLIST_HEAD(write_list);\n\n\t*bp = NULL;\n\n\t \n\tb = cache_get(&c->cache, block);\n\tif (b) {\n\t\tif (nf == NF_PREFETCH) {\n\t\t\tcache_put_and_wake(c, b);\n\t\t\treturn NULL;\n\t\t}\n\n\t\t \n\t\tif (nf == NF_GET && unlikely(test_bit_acquire(B_READING, &b->state))) {\n\t\t\tcache_put_and_wake(c, b);\n\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tif (!b) {\n\t\tif (nf == NF_GET)\n\t\t\treturn NULL;\n\n\t\tdm_bufio_lock(c);\n\t\tb = __bufio_new(c, block, nf, &need_submit, &write_list);\n\t\tdm_bufio_unlock(c);\n\t}\n\n#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING\n\tif (b && (atomic_read(&b->hold_count) == 1))\n\t\tbuffer_record_stack(b);\n#endif\n\n\t__flush_write_list(&write_list);\n\n\tif (!b)\n\t\treturn NULL;\n\n\tif (need_submit)\n\t\tsubmit_io(b, REQ_OP_READ, read_endio);\n\n\tif (nf != NF_GET)\t \n\t\twait_on_bit_io(&b->state, B_READING, TASK_UNINTERRUPTIBLE);\n\n\tif (b->read_error) {\n\t\tint error = blk_status_to_errno(b->read_error);\n\n\t\tdm_bufio_release(b);\n\n\t\treturn ERR_PTR(error);\n\t}\n\n\t*bp = b;\n\n\treturn b->data;\n}\n\nvoid *dm_bufio_get(struct dm_bufio_client *c, sector_t block,\n\t\t   struct dm_buffer **bp)\n{\n\treturn new_read(c, block, NF_GET, bp);\n}\nEXPORT_SYMBOL_GPL(dm_bufio_get);\n\nvoid *dm_bufio_read(struct dm_bufio_client *c, sector_t block,\n\t\t    struct dm_buffer **bp)\n{\n\tif (WARN_ON_ONCE(dm_bufio_in_request()))\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn new_read(c, block, NF_READ, bp);\n}\nEXPORT_SYMBOL_GPL(dm_bufio_read);\n\nvoid *dm_bufio_new(struct dm_bufio_client *c, sector_t block,\n\t\t   struct dm_buffer **bp)\n{\n\tif (WARN_ON_ONCE(dm_bufio_in_request()))\n\t\treturn ERR_PTR(-EINVAL);\n\n\treturn new_read(c, block, NF_FRESH, bp);\n}\nEXPORT_SYMBOL_GPL(dm_bufio_new);\n\nvoid dm_bufio_prefetch(struct dm_bufio_client *c,\n\t\t       sector_t block, unsigned int n_blocks)\n{\n\tstruct blk_plug plug;\n\n\tLIST_HEAD(write_list);\n\n\tif (WARN_ON_ONCE(dm_bufio_in_request()))\n\t\treturn;  \n\n\tblk_start_plug(&plug);\n\n\tfor (; n_blocks--; block++) {\n\t\tint need_submit;\n\t\tstruct dm_buffer *b;\n\n\t\tb = cache_get(&c->cache, block);\n\t\tif (b) {\n\t\t\t \n\t\t\tcache_put_and_wake(c, b);\n\t\t\tcontinue;\n\t\t}\n\n\t\tdm_bufio_lock(c);\n\t\tb = __bufio_new(c, block, NF_PREFETCH, &need_submit,\n\t\t\t\t&write_list);\n\t\tif (unlikely(!list_empty(&write_list))) {\n\t\t\tdm_bufio_unlock(c);\n\t\t\tblk_finish_plug(&plug);\n\t\t\t__flush_write_list(&write_list);\n\t\t\tblk_start_plug(&plug);\n\t\t\tdm_bufio_lock(c);\n\t\t}\n\t\tif (unlikely(b != NULL)) {\n\t\t\tdm_bufio_unlock(c);\n\n\t\t\tif (need_submit)\n\t\t\t\tsubmit_io(b, REQ_OP_READ, read_endio);\n\t\t\tdm_bufio_release(b);\n\n\t\t\tcond_resched();\n\n\t\t\tif (!n_blocks)\n\t\t\t\tgoto flush_plug;\n\t\t\tdm_bufio_lock(c);\n\t\t}\n\t\tdm_bufio_unlock(c);\n\t}\n\nflush_plug:\n\tblk_finish_plug(&plug);\n}\nEXPORT_SYMBOL_GPL(dm_bufio_prefetch);\n\nvoid dm_bufio_release(struct dm_buffer *b)\n{\n\tstruct dm_bufio_client *c = b->c;\n\n\t \n\tif ((b->read_error || b->write_error) &&\n\t    !test_bit_acquire(B_READING, &b->state) &&\n\t    !test_bit(B_WRITING, &b->state) &&\n\t    !test_bit(B_DIRTY, &b->state)) {\n\t\tdm_bufio_lock(c);\n\n\t\t \n\t\tif (cache_remove(&c->cache, b)) {\n\t\t\t__free_buffer_wake(b);\n\t\t\tdm_bufio_unlock(c);\n\t\t\treturn;\n\t\t}\n\n\t\tdm_bufio_unlock(c);\n\t}\n\n\tcache_put_and_wake(c, b);\n}\nEXPORT_SYMBOL_GPL(dm_bufio_release);\n\nvoid dm_bufio_mark_partial_buffer_dirty(struct dm_buffer *b,\n\t\t\t\t\tunsigned int start, unsigned int end)\n{\n\tstruct dm_bufio_client *c = b->c;\n\n\tBUG_ON(start >= end);\n\tBUG_ON(end > b->c->block_size);\n\n\tdm_bufio_lock(c);\n\n\tBUG_ON(test_bit(B_READING, &b->state));\n\n\tif (!test_and_set_bit(B_DIRTY, &b->state)) {\n\t\tb->dirty_start = start;\n\t\tb->dirty_end = end;\n\t\tcache_mark(&c->cache, b, LIST_DIRTY);\n\t} else {\n\t\tif (start < b->dirty_start)\n\t\t\tb->dirty_start = start;\n\t\tif (end > b->dirty_end)\n\t\t\tb->dirty_end = end;\n\t}\n\n\tdm_bufio_unlock(c);\n}\nEXPORT_SYMBOL_GPL(dm_bufio_mark_partial_buffer_dirty);\n\nvoid dm_bufio_mark_buffer_dirty(struct dm_buffer *b)\n{\n\tdm_bufio_mark_partial_buffer_dirty(b, 0, b->c->block_size);\n}\nEXPORT_SYMBOL_GPL(dm_bufio_mark_buffer_dirty);\n\nvoid dm_bufio_write_dirty_buffers_async(struct dm_bufio_client *c)\n{\n\tLIST_HEAD(write_list);\n\n\tif (WARN_ON_ONCE(dm_bufio_in_request()))\n\t\treturn;  \n\n\tdm_bufio_lock(c);\n\t__write_dirty_buffers_async(c, 0, &write_list);\n\tdm_bufio_unlock(c);\n\t__flush_write_list(&write_list);\n}\nEXPORT_SYMBOL_GPL(dm_bufio_write_dirty_buffers_async);\n\n \nstatic bool is_writing(struct lru_entry *e, void *context)\n{\n\tstruct dm_buffer *b = le_to_buffer(e);\n\n\treturn test_bit(B_WRITING, &b->state);\n}\n\nint dm_bufio_write_dirty_buffers(struct dm_bufio_client *c)\n{\n\tint a, f;\n\tunsigned long nr_buffers;\n\tstruct lru_entry *e;\n\tstruct lru_iter it;\n\n\tLIST_HEAD(write_list);\n\n\tdm_bufio_lock(c);\n\t__write_dirty_buffers_async(c, 0, &write_list);\n\tdm_bufio_unlock(c);\n\t__flush_write_list(&write_list);\n\tdm_bufio_lock(c);\n\n\tnr_buffers = cache_count(&c->cache, LIST_DIRTY);\n\tlru_iter_begin(&c->cache.lru[LIST_DIRTY], &it);\n\twhile ((e = lru_iter_next(&it, is_writing, c))) {\n\t\tstruct dm_buffer *b = le_to_buffer(e);\n\t\t__cache_inc_buffer(b);\n\n\t\tBUG_ON(test_bit(B_READING, &b->state));\n\n\t\tif (nr_buffers) {\n\t\t\tnr_buffers--;\n\t\t\tdm_bufio_unlock(c);\n\t\t\twait_on_bit_io(&b->state, B_WRITING, TASK_UNINTERRUPTIBLE);\n\t\t\tdm_bufio_lock(c);\n\t\t} else {\n\t\t\twait_on_bit_io(&b->state, B_WRITING, TASK_UNINTERRUPTIBLE);\n\t\t}\n\n\t\tif (!test_bit(B_DIRTY, &b->state) && !test_bit(B_WRITING, &b->state))\n\t\t\tcache_mark(&c->cache, b, LIST_CLEAN);\n\n\t\tcache_put_and_wake(c, b);\n\n\t\tcond_resched();\n\t}\n\tlru_iter_end(&it);\n\n\twake_up(&c->free_buffer_wait);\n\tdm_bufio_unlock(c);\n\n\ta = xchg(&c->async_write_error, 0);\n\tf = dm_bufio_issue_flush(c);\n\tif (a)\n\t\treturn a;\n\n\treturn f;\n}\nEXPORT_SYMBOL_GPL(dm_bufio_write_dirty_buffers);\n\n \nint dm_bufio_issue_flush(struct dm_bufio_client *c)\n{\n\tstruct dm_io_request io_req = {\n\t\t.bi_opf = REQ_OP_WRITE | REQ_PREFLUSH | REQ_SYNC,\n\t\t.mem.type = DM_IO_KMEM,\n\t\t.mem.ptr.addr = NULL,\n\t\t.client = c->dm_io,\n\t};\n\tstruct dm_io_region io_reg = {\n\t\t.bdev = c->bdev,\n\t\t.sector = 0,\n\t\t.count = 0,\n\t};\n\n\tif (WARN_ON_ONCE(dm_bufio_in_request()))\n\t\treturn -EINVAL;\n\n\treturn dm_io(&io_req, 1, &io_reg, NULL);\n}\nEXPORT_SYMBOL_GPL(dm_bufio_issue_flush);\n\n \nint dm_bufio_issue_discard(struct dm_bufio_client *c, sector_t block, sector_t count)\n{\n\tstruct dm_io_request io_req = {\n\t\t.bi_opf = REQ_OP_DISCARD | REQ_SYNC,\n\t\t.mem.type = DM_IO_KMEM,\n\t\t.mem.ptr.addr = NULL,\n\t\t.client = c->dm_io,\n\t};\n\tstruct dm_io_region io_reg = {\n\t\t.bdev = c->bdev,\n\t\t.sector = block_to_sector(c, block),\n\t\t.count = block_to_sector(c, count),\n\t};\n\n\tif (WARN_ON_ONCE(dm_bufio_in_request()))\n\t\treturn -EINVAL;  \n\n\treturn dm_io(&io_req, 1, &io_reg, NULL);\n}\nEXPORT_SYMBOL_GPL(dm_bufio_issue_discard);\n\nstatic bool forget_buffer(struct dm_bufio_client *c, sector_t block)\n{\n\tstruct dm_buffer *b;\n\n\tb = cache_get(&c->cache, block);\n\tif (b) {\n\t\tif (likely(!smp_load_acquire(&b->state))) {\n\t\t\tif (cache_remove(&c->cache, b))\n\t\t\t\t__free_buffer_wake(b);\n\t\t\telse\n\t\t\t\tcache_put_and_wake(c, b);\n\t\t} else {\n\t\t\tcache_put_and_wake(c, b);\n\t\t}\n\t}\n\n\treturn b ? true : false;\n}\n\n \nvoid dm_bufio_forget(struct dm_bufio_client *c, sector_t block)\n{\n\tdm_bufio_lock(c);\n\tforget_buffer(c, block);\n\tdm_bufio_unlock(c);\n}\nEXPORT_SYMBOL_GPL(dm_bufio_forget);\n\nstatic enum evict_result idle(struct dm_buffer *b, void *context)\n{\n\treturn b->state ? ER_DONT_EVICT : ER_EVICT;\n}\n\nvoid dm_bufio_forget_buffers(struct dm_bufio_client *c, sector_t block, sector_t n_blocks)\n{\n\tdm_bufio_lock(c);\n\tcache_remove_range(&c->cache, block, block + n_blocks, idle, __free_buffer_wake);\n\tdm_bufio_unlock(c);\n}\nEXPORT_SYMBOL_GPL(dm_bufio_forget_buffers);\n\nvoid dm_bufio_set_minimum_buffers(struct dm_bufio_client *c, unsigned int n)\n{\n\tc->minimum_buffers = n;\n}\nEXPORT_SYMBOL_GPL(dm_bufio_set_minimum_buffers);\n\nunsigned int dm_bufio_get_block_size(struct dm_bufio_client *c)\n{\n\treturn c->block_size;\n}\nEXPORT_SYMBOL_GPL(dm_bufio_get_block_size);\n\nsector_t dm_bufio_get_device_size(struct dm_bufio_client *c)\n{\n\tsector_t s = bdev_nr_sectors(c->bdev);\n\n\tif (s >= c->start)\n\t\ts -= c->start;\n\telse\n\t\ts = 0;\n\tif (likely(c->sectors_per_block_bits >= 0))\n\t\ts >>= c->sectors_per_block_bits;\n\telse\n\t\tsector_div(s, c->block_size >> SECTOR_SHIFT);\n\treturn s;\n}\nEXPORT_SYMBOL_GPL(dm_bufio_get_device_size);\n\nstruct dm_io_client *dm_bufio_get_dm_io_client(struct dm_bufio_client *c)\n{\n\treturn c->dm_io;\n}\nEXPORT_SYMBOL_GPL(dm_bufio_get_dm_io_client);\n\nsector_t dm_bufio_get_block_number(struct dm_buffer *b)\n{\n\treturn b->block;\n}\nEXPORT_SYMBOL_GPL(dm_bufio_get_block_number);\n\nvoid *dm_bufio_get_block_data(struct dm_buffer *b)\n{\n\treturn b->data;\n}\nEXPORT_SYMBOL_GPL(dm_bufio_get_block_data);\n\nvoid *dm_bufio_get_aux_data(struct dm_buffer *b)\n{\n\treturn b + 1;\n}\nEXPORT_SYMBOL_GPL(dm_bufio_get_aux_data);\n\nstruct dm_bufio_client *dm_bufio_get_client(struct dm_buffer *b)\n{\n\treturn b->c;\n}\nEXPORT_SYMBOL_GPL(dm_bufio_get_client);\n\nstatic enum it_action warn_leak(struct dm_buffer *b, void *context)\n{\n\tbool *warned = context;\n\n\tWARN_ON(!(*warned));\n\t*warned = true;\n\tDMERR(\"leaked buffer %llx, hold count %u, list %d\",\n\t      (unsigned long long)b->block, atomic_read(&b->hold_count), b->list_mode);\n#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING\n\tstack_trace_print(b->stack_entries, b->stack_len, 1);\n\t \n\tatomic_set(&b->hold_count, 0);\n#endif\n\treturn IT_NEXT;\n}\n\nstatic void drop_buffers(struct dm_bufio_client *c)\n{\n\tint i;\n\tstruct dm_buffer *b;\n\n\tif (WARN_ON(dm_bufio_in_request()))\n\t\treturn;  \n\n\t \n\tdm_bufio_write_dirty_buffers_async(c);\n\n\tdm_bufio_lock(c);\n\n\twhile ((b = __get_unclaimed_buffer(c)))\n\t\t__free_buffer_wake(b);\n\n\tfor (i = 0; i < LIST_SIZE; i++) {\n\t\tbool warned = false;\n\n\t\tcache_iterate(&c->cache, i, warn_leak, &warned);\n\t}\n\n#ifdef CONFIG_DM_DEBUG_BLOCK_STACK_TRACING\n\twhile ((b = __get_unclaimed_buffer(c)))\n\t\t__free_buffer_wake(b);\n#endif\n\n\tfor (i = 0; i < LIST_SIZE; i++)\n\t\tWARN_ON(cache_count(&c->cache, i));\n\n\tdm_bufio_unlock(c);\n}\n\nstatic unsigned long get_retain_buffers(struct dm_bufio_client *c)\n{\n\tunsigned long retain_bytes = READ_ONCE(dm_bufio_retain_bytes);\n\n\tif (likely(c->sectors_per_block_bits >= 0))\n\t\tretain_bytes >>= c->sectors_per_block_bits + SECTOR_SHIFT;\n\telse\n\t\tretain_bytes /= c->block_size;\n\n\treturn retain_bytes;\n}\n\nstatic void __scan(struct dm_bufio_client *c)\n{\n\tint l;\n\tstruct dm_buffer *b;\n\tunsigned long freed = 0;\n\tunsigned long retain_target = get_retain_buffers(c);\n\tunsigned long count = cache_total(&c->cache);\n\n\tfor (l = 0; l < LIST_SIZE; l++) {\n\t\twhile (true) {\n\t\t\tif (count - freed <= retain_target)\n\t\t\t\tatomic_long_set(&c->need_shrink, 0);\n\t\t\tif (!atomic_long_read(&c->need_shrink))\n\t\t\t\tbreak;\n\n\t\t\tb = cache_evict(&c->cache, l,\n\t\t\t\t\tl == LIST_CLEAN ? is_clean : is_dirty, c);\n\t\t\tif (!b)\n\t\t\t\tbreak;\n\n\t\t\t__make_buffer_clean(b);\n\t\t\t__free_buffer_wake(b);\n\n\t\t\tatomic_long_dec(&c->need_shrink);\n\t\t\tfreed++;\n\t\t\tcond_resched();\n\t\t}\n\t}\n}\n\nstatic void shrink_work(struct work_struct *w)\n{\n\tstruct dm_bufio_client *c = container_of(w, struct dm_bufio_client, shrink_work);\n\n\tdm_bufio_lock(c);\n\t__scan(c);\n\tdm_bufio_unlock(c);\n}\n\nstatic unsigned long dm_bufio_shrink_scan(struct shrinker *shrink, struct shrink_control *sc)\n{\n\tstruct dm_bufio_client *c;\n\n\tc = container_of(shrink, struct dm_bufio_client, shrinker);\n\tatomic_long_add(sc->nr_to_scan, &c->need_shrink);\n\tqueue_work(dm_bufio_wq, &c->shrink_work);\n\n\treturn sc->nr_to_scan;\n}\n\nstatic unsigned long dm_bufio_shrink_count(struct shrinker *shrink, struct shrink_control *sc)\n{\n\tstruct dm_bufio_client *c = container_of(shrink, struct dm_bufio_client, shrinker);\n\tunsigned long count = cache_total(&c->cache);\n\tunsigned long retain_target = get_retain_buffers(c);\n\tunsigned long queued_for_cleanup = atomic_long_read(&c->need_shrink);\n\n\tif (unlikely(count < retain_target))\n\t\tcount = 0;\n\telse\n\t\tcount -= retain_target;\n\n\tif (unlikely(count < queued_for_cleanup))\n\t\tcount = 0;\n\telse\n\t\tcount -= queued_for_cleanup;\n\n\treturn count;\n}\n\n \nstruct dm_bufio_client *dm_bufio_client_create(struct block_device *bdev, unsigned int block_size,\n\t\t\t\t\t       unsigned int reserved_buffers, unsigned int aux_size,\n\t\t\t\t\t       void (*alloc_callback)(struct dm_buffer *),\n\t\t\t\t\t       void (*write_callback)(struct dm_buffer *),\n\t\t\t\t\t       unsigned int flags)\n{\n\tint r;\n\tunsigned int num_locks;\n\tstruct dm_bufio_client *c;\n\tchar slab_name[27];\n\n\tif (!block_size || block_size & ((1 << SECTOR_SHIFT) - 1)) {\n\t\tDMERR(\"%s: block size not specified or is not multiple of 512b\", __func__);\n\t\tr = -EINVAL;\n\t\tgoto bad_client;\n\t}\n\n\tnum_locks = dm_num_hash_locks();\n\tc = kzalloc(sizeof(*c) + (num_locks * sizeof(struct buffer_tree)), GFP_KERNEL);\n\tif (!c) {\n\t\tr = -ENOMEM;\n\t\tgoto bad_client;\n\t}\n\tcache_init(&c->cache, num_locks, (flags & DM_BUFIO_CLIENT_NO_SLEEP) != 0);\n\n\tc->bdev = bdev;\n\tc->block_size = block_size;\n\tif (is_power_of_2(block_size))\n\t\tc->sectors_per_block_bits = __ffs(block_size) - SECTOR_SHIFT;\n\telse\n\t\tc->sectors_per_block_bits = -1;\n\n\tc->alloc_callback = alloc_callback;\n\tc->write_callback = write_callback;\n\n\tif (flags & DM_BUFIO_CLIENT_NO_SLEEP) {\n\t\tc->no_sleep = true;\n\t\tstatic_branch_inc(&no_sleep_enabled);\n\t}\n\n\tmutex_init(&c->lock);\n\tspin_lock_init(&c->spinlock);\n\tINIT_LIST_HEAD(&c->reserved_buffers);\n\tc->need_reserved_buffers = reserved_buffers;\n\n\tdm_bufio_set_minimum_buffers(c, DM_BUFIO_MIN_BUFFERS);\n\n\tinit_waitqueue_head(&c->free_buffer_wait);\n\tc->async_write_error = 0;\n\n\tc->dm_io = dm_io_client_create();\n\tif (IS_ERR(c->dm_io)) {\n\t\tr = PTR_ERR(c->dm_io);\n\t\tgoto bad_dm_io;\n\t}\n\n\tif (block_size <= KMALLOC_MAX_SIZE &&\n\t    (block_size < PAGE_SIZE || !is_power_of_2(block_size))) {\n\t\tunsigned int align = min(1U << __ffs(block_size), (unsigned int)PAGE_SIZE);\n\n\t\tsnprintf(slab_name, sizeof(slab_name), \"dm_bufio_cache-%u\", block_size);\n\t\tc->slab_cache = kmem_cache_create(slab_name, block_size, align,\n\t\t\t\t\t\t  SLAB_RECLAIM_ACCOUNT, NULL);\n\t\tif (!c->slab_cache) {\n\t\t\tr = -ENOMEM;\n\t\t\tgoto bad;\n\t\t}\n\t}\n\tif (aux_size)\n\t\tsnprintf(slab_name, sizeof(slab_name), \"dm_bufio_buffer-%u\", aux_size);\n\telse\n\t\tsnprintf(slab_name, sizeof(slab_name), \"dm_bufio_buffer\");\n\tc->slab_buffer = kmem_cache_create(slab_name, sizeof(struct dm_buffer) + aux_size,\n\t\t\t\t\t   0, SLAB_RECLAIM_ACCOUNT, NULL);\n\tif (!c->slab_buffer) {\n\t\tr = -ENOMEM;\n\t\tgoto bad;\n\t}\n\n\twhile (c->need_reserved_buffers) {\n\t\tstruct dm_buffer *b = alloc_buffer(c, GFP_KERNEL);\n\n\t\tif (!b) {\n\t\t\tr = -ENOMEM;\n\t\t\tgoto bad;\n\t\t}\n\t\t__free_buffer_wake(b);\n\t}\n\n\tINIT_WORK(&c->shrink_work, shrink_work);\n\tatomic_long_set(&c->need_shrink, 0);\n\n\tc->shrinker.count_objects = dm_bufio_shrink_count;\n\tc->shrinker.scan_objects = dm_bufio_shrink_scan;\n\tc->shrinker.seeks = 1;\n\tc->shrinker.batch = 0;\n\tr = register_shrinker(&c->shrinker, \"dm-bufio:(%u:%u)\",\n\t\t\t      MAJOR(bdev->bd_dev), MINOR(bdev->bd_dev));\n\tif (r)\n\t\tgoto bad;\n\n\tmutex_lock(&dm_bufio_clients_lock);\n\tdm_bufio_client_count++;\n\tlist_add(&c->client_list, &dm_bufio_all_clients);\n\t__cache_size_refresh();\n\tmutex_unlock(&dm_bufio_clients_lock);\n\n\treturn c;\n\nbad:\n\twhile (!list_empty(&c->reserved_buffers)) {\n\t\tstruct dm_buffer *b = list_to_buffer(c->reserved_buffers.next);\n\n\t\tlist_del(&b->lru.list);\n\t\tfree_buffer(b);\n\t}\n\tkmem_cache_destroy(c->slab_cache);\n\tkmem_cache_destroy(c->slab_buffer);\n\tdm_io_client_destroy(c->dm_io);\nbad_dm_io:\n\tmutex_destroy(&c->lock);\n\tif (c->no_sleep)\n\t\tstatic_branch_dec(&no_sleep_enabled);\n\tkfree(c);\nbad_client:\n\treturn ERR_PTR(r);\n}\nEXPORT_SYMBOL_GPL(dm_bufio_client_create);\n\n \nvoid dm_bufio_client_destroy(struct dm_bufio_client *c)\n{\n\tunsigned int i;\n\n\tdrop_buffers(c);\n\n\tunregister_shrinker(&c->shrinker);\n\tflush_work(&c->shrink_work);\n\n\tmutex_lock(&dm_bufio_clients_lock);\n\n\tlist_del(&c->client_list);\n\tdm_bufio_client_count--;\n\t__cache_size_refresh();\n\n\tmutex_unlock(&dm_bufio_clients_lock);\n\n\tWARN_ON(c->need_reserved_buffers);\n\n\twhile (!list_empty(&c->reserved_buffers)) {\n\t\tstruct dm_buffer *b = list_to_buffer(c->reserved_buffers.next);\n\n\t\tlist_del(&b->lru.list);\n\t\tfree_buffer(b);\n\t}\n\n\tfor (i = 0; i < LIST_SIZE; i++)\n\t\tif (cache_count(&c->cache, i))\n\t\t\tDMERR(\"leaked buffer count %d: %lu\", i, cache_count(&c->cache, i));\n\n\tfor (i = 0; i < LIST_SIZE; i++)\n\t\tWARN_ON(cache_count(&c->cache, i));\n\n\tcache_destroy(&c->cache);\n\tkmem_cache_destroy(c->slab_cache);\n\tkmem_cache_destroy(c->slab_buffer);\n\tdm_io_client_destroy(c->dm_io);\n\tmutex_destroy(&c->lock);\n\tif (c->no_sleep)\n\t\tstatic_branch_dec(&no_sleep_enabled);\n\tkfree(c);\n}\nEXPORT_SYMBOL_GPL(dm_bufio_client_destroy);\n\nvoid dm_bufio_client_reset(struct dm_bufio_client *c)\n{\n\tdrop_buffers(c);\n\tflush_work(&c->shrink_work);\n}\nEXPORT_SYMBOL_GPL(dm_bufio_client_reset);\n\nvoid dm_bufio_set_sector_offset(struct dm_bufio_client *c, sector_t start)\n{\n\tc->start = start;\n}\nEXPORT_SYMBOL_GPL(dm_bufio_set_sector_offset);\n\n \n\nstatic unsigned int get_max_age_hz(void)\n{\n\tunsigned int max_age = READ_ONCE(dm_bufio_max_age);\n\n\tif (max_age > UINT_MAX / HZ)\n\t\tmax_age = UINT_MAX / HZ;\n\n\treturn max_age * HZ;\n}\n\nstatic bool older_than(struct dm_buffer *b, unsigned long age_hz)\n{\n\treturn time_after_eq(jiffies, READ_ONCE(b->last_accessed) + age_hz);\n}\n\nstruct evict_params {\n\tgfp_t gfp;\n\tunsigned long age_hz;\n\n\t \n\tunsigned long last_accessed;\n};\n\n \nstatic enum evict_result select_for_evict(struct dm_buffer *b, void *context)\n{\n\tstruct evict_params *params = context;\n\n\tif (!(params->gfp & __GFP_FS) ||\n\t    (static_branch_unlikely(&no_sleep_enabled) && b->c->no_sleep)) {\n\t\tif (test_bit_acquire(B_READING, &b->state) ||\n\t\t    test_bit(B_WRITING, &b->state) ||\n\t\t    test_bit(B_DIRTY, &b->state))\n\t\t\treturn ER_DONT_EVICT;\n\t}\n\n\treturn older_than(b, params->age_hz) ? ER_EVICT : ER_STOP;\n}\n\nstatic unsigned long __evict_many(struct dm_bufio_client *c,\n\t\t\t\t  struct evict_params *params,\n\t\t\t\t  int list_mode, unsigned long max_count)\n{\n\tunsigned long count;\n\tunsigned long last_accessed;\n\tstruct dm_buffer *b;\n\n\tfor (count = 0; count < max_count; count++) {\n\t\tb = cache_evict(&c->cache, list_mode, select_for_evict, params);\n\t\tif (!b)\n\t\t\tbreak;\n\n\t\tlast_accessed = READ_ONCE(b->last_accessed);\n\t\tif (time_after_eq(params->last_accessed, last_accessed))\n\t\t\tparams->last_accessed = last_accessed;\n\n\t\t__make_buffer_clean(b);\n\t\t__free_buffer_wake(b);\n\n\t\tcond_resched();\n\t}\n\n\treturn count;\n}\n\nstatic void evict_old_buffers(struct dm_bufio_client *c, unsigned long age_hz)\n{\n\tstruct evict_params params = {.gfp = 0, .age_hz = age_hz, .last_accessed = 0};\n\tunsigned long retain = get_retain_buffers(c);\n\tunsigned long count;\n\tLIST_HEAD(write_list);\n\n\tdm_bufio_lock(c);\n\n\t__check_watermark(c, &write_list);\n\tif (unlikely(!list_empty(&write_list))) {\n\t\tdm_bufio_unlock(c);\n\t\t__flush_write_list(&write_list);\n\t\tdm_bufio_lock(c);\n\t}\n\n\tcount = cache_total(&c->cache);\n\tif (count > retain)\n\t\t__evict_many(c, &params, LIST_CLEAN, count - retain);\n\n\tdm_bufio_unlock(c);\n}\n\nstatic void cleanup_old_buffers(void)\n{\n\tunsigned long max_age_hz = get_max_age_hz();\n\tstruct dm_bufio_client *c;\n\n\tmutex_lock(&dm_bufio_clients_lock);\n\n\t__cache_size_refresh();\n\n\tlist_for_each_entry(c, &dm_bufio_all_clients, client_list)\n\t\tevict_old_buffers(c, max_age_hz);\n\n\tmutex_unlock(&dm_bufio_clients_lock);\n}\n\nstatic void work_fn(struct work_struct *w)\n{\n\tcleanup_old_buffers();\n\n\tqueue_delayed_work(dm_bufio_wq, &dm_bufio_cleanup_old_work,\n\t\t\t   DM_BUFIO_WORK_TIMER_SECS * HZ);\n}\n\n \n\n \nstatic struct dm_bufio_client *__pop_client(void)\n{\n\tstruct list_head *h;\n\n\tif (list_empty(&dm_bufio_all_clients))\n\t\treturn NULL;\n\n\th = dm_bufio_all_clients.next;\n\tlist_del(h);\n\treturn container_of(h, struct dm_bufio_client, client_list);\n}\n\n \nstatic void __insert_client(struct dm_bufio_client *new_client)\n{\n\tstruct dm_bufio_client *c;\n\tstruct list_head *h = dm_bufio_all_clients.next;\n\n\twhile (h != &dm_bufio_all_clients) {\n\t\tc = container_of(h, struct dm_bufio_client, client_list);\n\t\tif (time_after_eq(c->oldest_buffer, new_client->oldest_buffer))\n\t\t\tbreak;\n\t\th = h->next;\n\t}\n\n\tlist_add_tail(&new_client->client_list, h);\n}\n\nstatic unsigned long __evict_a_few(unsigned long nr_buffers)\n{\n\tunsigned long count;\n\tstruct dm_bufio_client *c;\n\tstruct evict_params params = {\n\t\t.gfp = GFP_KERNEL,\n\t\t.age_hz = 0,\n\t\t \n\t\t.last_accessed = jiffies\n\t};\n\n\tc = __pop_client();\n\tif (!c)\n\t\treturn 0;\n\n\tdm_bufio_lock(c);\n\tcount = __evict_many(c, &params, LIST_CLEAN, nr_buffers);\n\tdm_bufio_unlock(c);\n\n\tif (count)\n\t\tc->oldest_buffer = params.last_accessed;\n\t__insert_client(c);\n\n\treturn count;\n}\n\nstatic void check_watermarks(void)\n{\n\tLIST_HEAD(write_list);\n\tstruct dm_bufio_client *c;\n\n\tmutex_lock(&dm_bufio_clients_lock);\n\tlist_for_each_entry(c, &dm_bufio_all_clients, client_list) {\n\t\tdm_bufio_lock(c);\n\t\t__check_watermark(c, &write_list);\n\t\tdm_bufio_unlock(c);\n\t}\n\tmutex_unlock(&dm_bufio_clients_lock);\n\n\t__flush_write_list(&write_list);\n}\n\nstatic void evict_old(void)\n{\n\tunsigned long threshold = dm_bufio_cache_size -\n\t\tdm_bufio_cache_size / DM_BUFIO_LOW_WATERMARK_RATIO;\n\n\tmutex_lock(&dm_bufio_clients_lock);\n\twhile (dm_bufio_current_allocated > threshold) {\n\t\tif (!__evict_a_few(64))\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\tmutex_unlock(&dm_bufio_clients_lock);\n}\n\nstatic void do_global_cleanup(struct work_struct *w)\n{\n\tcheck_watermarks();\n\tevict_old();\n}\n\n \n\n \nstatic int __init dm_bufio_init(void)\n{\n\t__u64 mem;\n\n\tdm_bufio_allocated_kmem_cache = 0;\n\tdm_bufio_allocated_get_free_pages = 0;\n\tdm_bufio_allocated_vmalloc = 0;\n\tdm_bufio_current_allocated = 0;\n\n\tmem = (__u64)mult_frac(totalram_pages() - totalhigh_pages(),\n\t\t\t       DM_BUFIO_MEMORY_PERCENT, 100) << PAGE_SHIFT;\n\n\tif (mem > ULONG_MAX)\n\t\tmem = ULONG_MAX;\n\n#ifdef CONFIG_MMU\n\tif (mem > mult_frac(VMALLOC_TOTAL, DM_BUFIO_VMALLOC_PERCENT, 100))\n\t\tmem = mult_frac(VMALLOC_TOTAL, DM_BUFIO_VMALLOC_PERCENT, 100);\n#endif\n\n\tdm_bufio_default_cache_size = mem;\n\n\tmutex_lock(&dm_bufio_clients_lock);\n\t__cache_size_refresh();\n\tmutex_unlock(&dm_bufio_clients_lock);\n\n\tdm_bufio_wq = alloc_workqueue(\"dm_bufio_cache\", WQ_MEM_RECLAIM, 0);\n\tif (!dm_bufio_wq)\n\t\treturn -ENOMEM;\n\n\tINIT_DELAYED_WORK(&dm_bufio_cleanup_old_work, work_fn);\n\tINIT_WORK(&dm_bufio_replacement_work, do_global_cleanup);\n\tqueue_delayed_work(dm_bufio_wq, &dm_bufio_cleanup_old_work,\n\t\t\t   DM_BUFIO_WORK_TIMER_SECS * HZ);\n\n\treturn 0;\n}\n\n \nstatic void __exit dm_bufio_exit(void)\n{\n\tint bug = 0;\n\n\tcancel_delayed_work_sync(&dm_bufio_cleanup_old_work);\n\tdestroy_workqueue(dm_bufio_wq);\n\n\tif (dm_bufio_client_count) {\n\t\tDMCRIT(\"%s: dm_bufio_client_count leaked: %d\",\n\t\t\t__func__, dm_bufio_client_count);\n\t\tbug = 1;\n\t}\n\n\tif (dm_bufio_current_allocated) {\n\t\tDMCRIT(\"%s: dm_bufio_current_allocated leaked: %lu\",\n\t\t\t__func__, dm_bufio_current_allocated);\n\t\tbug = 1;\n\t}\n\n\tif (dm_bufio_allocated_get_free_pages) {\n\t\tDMCRIT(\"%s: dm_bufio_allocated_get_free_pages leaked: %lu\",\n\t\t       __func__, dm_bufio_allocated_get_free_pages);\n\t\tbug = 1;\n\t}\n\n\tif (dm_bufio_allocated_vmalloc) {\n\t\tDMCRIT(\"%s: dm_bufio_vmalloc leaked: %lu\",\n\t\t       __func__, dm_bufio_allocated_vmalloc);\n\t\tbug = 1;\n\t}\n\n\tWARN_ON(bug);  \n}\n\nmodule_init(dm_bufio_init)\nmodule_exit(dm_bufio_exit)\n\nmodule_param_named(max_cache_size_bytes, dm_bufio_cache_size, ulong, 0644);\nMODULE_PARM_DESC(max_cache_size_bytes, \"Size of metadata cache\");\n\nmodule_param_named(max_age_seconds, dm_bufio_max_age, uint, 0644);\nMODULE_PARM_DESC(max_age_seconds, \"Max age of a buffer in seconds\");\n\nmodule_param_named(retain_bytes, dm_bufio_retain_bytes, ulong, 0644);\nMODULE_PARM_DESC(retain_bytes, \"Try to keep at least this many bytes cached in memory\");\n\nmodule_param_named(peak_allocated_bytes, dm_bufio_peak_allocated, ulong, 0644);\nMODULE_PARM_DESC(peak_allocated_bytes, \"Tracks the maximum allocated memory\");\n\nmodule_param_named(allocated_kmem_cache_bytes, dm_bufio_allocated_kmem_cache, ulong, 0444);\nMODULE_PARM_DESC(allocated_kmem_cache_bytes, \"Memory allocated with kmem_cache_alloc\");\n\nmodule_param_named(allocated_get_free_pages_bytes, dm_bufio_allocated_get_free_pages, ulong, 0444);\nMODULE_PARM_DESC(allocated_get_free_pages_bytes, \"Memory allocated with get_free_pages\");\n\nmodule_param_named(allocated_vmalloc_bytes, dm_bufio_allocated_vmalloc, ulong, 0444);\nMODULE_PARM_DESC(allocated_vmalloc_bytes, \"Memory allocated with vmalloc\");\n\nmodule_param_named(current_allocated_bytes, dm_bufio_current_allocated, ulong, 0444);\nMODULE_PARM_DESC(current_allocated_bytes, \"Memory currently used by the cache\");\n\nMODULE_AUTHOR(\"Mikulas Patocka <dm-devel@redhat.com>\");\nMODULE_DESCRIPTION(DM_NAME \" buffered I/O library\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}