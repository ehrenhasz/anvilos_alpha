{
  "module_name": "dm-region-hash.c",
  "hash_id": "c4d53479ce61c7f8fdce76652773c3479c27e8b0cf403c99818b900b03ce5b5d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-region-hash.c",
  "human_readable_source": "\n \n\n#include <linux/dm-dirty-log.h>\n#include <linux/dm-region-hash.h>\n\n#include <linux/ctype.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n\n#include \"dm.h\"\n\n#define\tDM_MSG_PREFIX\t\"region hash\"\n\n \nstruct dm_region_hash {\n\tuint32_t region_size;\n\tunsigned int region_shift;\n\n\t \n\tstruct dm_dirty_log *log;\n\n\t \n\trwlock_t hash_lock;\n\tunsigned int mask;\n\tunsigned int nr_buckets;\n\tunsigned int prime;\n\tunsigned int shift;\n\tstruct list_head *buckets;\n\n\t \n\tint flush_failure;\n\n\tunsigned int max_recovery;  \n\n\tspinlock_t region_lock;\n\tatomic_t recovery_in_flight;\n\tstruct list_head clean_regions;\n\tstruct list_head quiesced_regions;\n\tstruct list_head recovered_regions;\n\tstruct list_head failed_recovered_regions;\n\tstruct semaphore recovery_count;\n\n\tmempool_t region_pool;\n\n\tvoid *context;\n\tsector_t target_begin;\n\n\t \n\tvoid (*dispatch_bios)(void *context, struct bio_list *bios);\n\n\t \n\tvoid (*wakeup_workers)(void *context);\n\n\t \n\tvoid (*wakeup_all_recovery_waiters)(void *context);\n};\n\nstruct dm_region {\n\tstruct dm_region_hash *rh;\t \n\tregion_t key;\n\tint state;\n\n\tstruct list_head hash_list;\n\tstruct list_head list;\n\n\tatomic_t pending;\n\tstruct bio_list delayed_bios;\n};\n\n \nstatic region_t dm_rh_sector_to_region(struct dm_region_hash *rh, sector_t sector)\n{\n\treturn sector >> rh->region_shift;\n}\n\nsector_t dm_rh_region_to_sector(struct dm_region_hash *rh, region_t region)\n{\n\treturn region << rh->region_shift;\n}\nEXPORT_SYMBOL_GPL(dm_rh_region_to_sector);\n\nregion_t dm_rh_bio_to_region(struct dm_region_hash *rh, struct bio *bio)\n{\n\treturn dm_rh_sector_to_region(rh, bio->bi_iter.bi_sector -\n\t\t\t\t      rh->target_begin);\n}\nEXPORT_SYMBOL_GPL(dm_rh_bio_to_region);\n\nvoid *dm_rh_region_context(struct dm_region *reg)\n{\n\treturn reg->rh->context;\n}\nEXPORT_SYMBOL_GPL(dm_rh_region_context);\n\nregion_t dm_rh_get_region_key(struct dm_region *reg)\n{\n\treturn reg->key;\n}\nEXPORT_SYMBOL_GPL(dm_rh_get_region_key);\n\nsector_t dm_rh_get_region_size(struct dm_region_hash *rh)\n{\n\treturn rh->region_size;\n}\nEXPORT_SYMBOL_GPL(dm_rh_get_region_size);\n\n \n#define RH_HASH_MULT 2654435387U\n#define RH_HASH_SHIFT 12\n\n#define MIN_REGIONS 64\nstruct dm_region_hash *dm_region_hash_create(\n\t\tvoid *context, void (*dispatch_bios)(void *context,\n\t\t\t\t\t\t     struct bio_list *bios),\n\t\tvoid (*wakeup_workers)(void *context),\n\t\tvoid (*wakeup_all_recovery_waiters)(void *context),\n\t\tsector_t target_begin, unsigned int max_recovery,\n\t\tstruct dm_dirty_log *log, uint32_t region_size,\n\t\tregion_t nr_regions)\n{\n\tstruct dm_region_hash *rh;\n\tunsigned int nr_buckets, max_buckets;\n\tsize_t i;\n\tint ret;\n\n\t \n\tmax_buckets = nr_regions >> 6;\n\tfor (nr_buckets = 128u; nr_buckets < max_buckets; nr_buckets <<= 1)\n\t\t;\n\tnr_buckets >>= 1;\n\n\trh = kzalloc(sizeof(*rh), GFP_KERNEL);\n\tif (!rh) {\n\t\tDMERR(\"unable to allocate region hash memory\");\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\trh->context = context;\n\trh->dispatch_bios = dispatch_bios;\n\trh->wakeup_workers = wakeup_workers;\n\trh->wakeup_all_recovery_waiters = wakeup_all_recovery_waiters;\n\trh->target_begin = target_begin;\n\trh->max_recovery = max_recovery;\n\trh->log = log;\n\trh->region_size = region_size;\n\trh->region_shift = __ffs(region_size);\n\trwlock_init(&rh->hash_lock);\n\trh->mask = nr_buckets - 1;\n\trh->nr_buckets = nr_buckets;\n\n\trh->shift = RH_HASH_SHIFT;\n\trh->prime = RH_HASH_MULT;\n\n\trh->buckets = vmalloc(array_size(nr_buckets, sizeof(*rh->buckets)));\n\tif (!rh->buckets) {\n\t\tDMERR(\"unable to allocate region hash bucket memory\");\n\t\tkfree(rh);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tfor (i = 0; i < nr_buckets; i++)\n\t\tINIT_LIST_HEAD(rh->buckets + i);\n\n\tspin_lock_init(&rh->region_lock);\n\tsema_init(&rh->recovery_count, 0);\n\tatomic_set(&rh->recovery_in_flight, 0);\n\tINIT_LIST_HEAD(&rh->clean_regions);\n\tINIT_LIST_HEAD(&rh->quiesced_regions);\n\tINIT_LIST_HEAD(&rh->recovered_regions);\n\tINIT_LIST_HEAD(&rh->failed_recovered_regions);\n\trh->flush_failure = 0;\n\n\tret = mempool_init_kmalloc_pool(&rh->region_pool, MIN_REGIONS,\n\t\t\t\t\tsizeof(struct dm_region));\n\tif (ret) {\n\t\tvfree(rh->buckets);\n\t\tkfree(rh);\n\t\trh = ERR_PTR(-ENOMEM);\n\t}\n\n\treturn rh;\n}\nEXPORT_SYMBOL_GPL(dm_region_hash_create);\n\nvoid dm_region_hash_destroy(struct dm_region_hash *rh)\n{\n\tunsigned int h;\n\tstruct dm_region *reg, *nreg;\n\n\tBUG_ON(!list_empty(&rh->quiesced_regions));\n\tfor (h = 0; h < rh->nr_buckets; h++) {\n\t\tlist_for_each_entry_safe(reg, nreg, rh->buckets + h,\n\t\t\t\t\t hash_list) {\n\t\t\tBUG_ON(atomic_read(&reg->pending));\n\t\t\tmempool_free(reg, &rh->region_pool);\n\t\t}\n\t}\n\n\tif (rh->log)\n\t\tdm_dirty_log_destroy(rh->log);\n\n\tmempool_exit(&rh->region_pool);\n\tvfree(rh->buckets);\n\tkfree(rh);\n}\nEXPORT_SYMBOL_GPL(dm_region_hash_destroy);\n\nstruct dm_dirty_log *dm_rh_dirty_log(struct dm_region_hash *rh)\n{\n\treturn rh->log;\n}\nEXPORT_SYMBOL_GPL(dm_rh_dirty_log);\n\nstatic unsigned int rh_hash(struct dm_region_hash *rh, region_t region)\n{\n\treturn (unsigned int) ((region * rh->prime) >> rh->shift) & rh->mask;\n}\n\nstatic struct dm_region *__rh_lookup(struct dm_region_hash *rh, region_t region)\n{\n\tstruct dm_region *reg;\n\tstruct list_head *bucket = rh->buckets + rh_hash(rh, region);\n\n\tlist_for_each_entry(reg, bucket, hash_list)\n\t\tif (reg->key == region)\n\t\t\treturn reg;\n\n\treturn NULL;\n}\n\nstatic void __rh_insert(struct dm_region_hash *rh, struct dm_region *reg)\n{\n\tlist_add(&reg->hash_list, rh->buckets + rh_hash(rh, reg->key));\n}\n\nstatic struct dm_region *__rh_alloc(struct dm_region_hash *rh, region_t region)\n{\n\tstruct dm_region *reg, *nreg;\n\n\tnreg = mempool_alloc(&rh->region_pool, GFP_ATOMIC);\n\tif (unlikely(!nreg))\n\t\tnreg = kmalloc(sizeof(*nreg), GFP_NOIO | __GFP_NOFAIL);\n\n\tnreg->state = rh->log->type->in_sync(rh->log, region, 1) ?\n\t\t      DM_RH_CLEAN : DM_RH_NOSYNC;\n\tnreg->rh = rh;\n\tnreg->key = region;\n\tINIT_LIST_HEAD(&nreg->list);\n\tatomic_set(&nreg->pending, 0);\n\tbio_list_init(&nreg->delayed_bios);\n\n\twrite_lock_irq(&rh->hash_lock);\n\treg = __rh_lookup(rh, region);\n\tif (reg)\n\t\t \n\t\tmempool_free(nreg, &rh->region_pool);\n\telse {\n\t\t__rh_insert(rh, nreg);\n\t\tif (nreg->state == DM_RH_CLEAN) {\n\t\t\tspin_lock(&rh->region_lock);\n\t\t\tlist_add(&nreg->list, &rh->clean_regions);\n\t\t\tspin_unlock(&rh->region_lock);\n\t\t}\n\n\t\treg = nreg;\n\t}\n\twrite_unlock_irq(&rh->hash_lock);\n\n\treturn reg;\n}\n\nstatic struct dm_region *__rh_find(struct dm_region_hash *rh, region_t region)\n{\n\tstruct dm_region *reg;\n\n\treg = __rh_lookup(rh, region);\n\tif (!reg) {\n\t\tread_unlock(&rh->hash_lock);\n\t\treg = __rh_alloc(rh, region);\n\t\tread_lock(&rh->hash_lock);\n\t}\n\n\treturn reg;\n}\n\nint dm_rh_get_state(struct dm_region_hash *rh, region_t region, int may_block)\n{\n\tint r;\n\tstruct dm_region *reg;\n\n\tread_lock(&rh->hash_lock);\n\treg = __rh_lookup(rh, region);\n\tread_unlock(&rh->hash_lock);\n\n\tif (reg)\n\t\treturn reg->state;\n\n\t \n\tr = rh->log->type->in_sync(rh->log, region, may_block);\n\n\t \n\treturn r == 1 ? DM_RH_CLEAN : DM_RH_NOSYNC;\n}\nEXPORT_SYMBOL_GPL(dm_rh_get_state);\n\nstatic void complete_resync_work(struct dm_region *reg, int success)\n{\n\tstruct dm_region_hash *rh = reg->rh;\n\n\trh->log->type->set_region_sync(rh->log, reg->key, success);\n\n\t \n\trh->dispatch_bios(rh->context, &reg->delayed_bios);\n\tif (atomic_dec_and_test(&rh->recovery_in_flight))\n\t\trh->wakeup_all_recovery_waiters(rh->context);\n\tup(&rh->recovery_count);\n}\n\n \nvoid dm_rh_mark_nosync(struct dm_region_hash *rh, struct bio *bio)\n{\n\tunsigned long flags;\n\tstruct dm_dirty_log *log = rh->log;\n\tstruct dm_region *reg;\n\tregion_t region = dm_rh_bio_to_region(rh, bio);\n\tint recovering = 0;\n\n\tif (bio->bi_opf & REQ_PREFLUSH) {\n\t\trh->flush_failure = 1;\n\t\treturn;\n\t}\n\n\tif (bio_op(bio) == REQ_OP_DISCARD)\n\t\treturn;\n\n\t \n\tlog->type->set_region_sync(log, region, 0);\n\n\tread_lock(&rh->hash_lock);\n\treg = __rh_find(rh, region);\n\tread_unlock(&rh->hash_lock);\n\n\t \n\tBUG_ON(!reg);\n\tBUG_ON(!list_empty(&reg->list));\n\n\tspin_lock_irqsave(&rh->region_lock, flags);\n\t \n\trecovering = (reg->state == DM_RH_RECOVERING);\n\treg->state = DM_RH_NOSYNC;\n\tBUG_ON(!list_empty(&reg->list));\n\tspin_unlock_irqrestore(&rh->region_lock, flags);\n\n\tif (recovering)\n\t\tcomplete_resync_work(reg, 0);\n}\nEXPORT_SYMBOL_GPL(dm_rh_mark_nosync);\n\nvoid dm_rh_update_states(struct dm_region_hash *rh, int errors_handled)\n{\n\tstruct dm_region *reg, *next;\n\n\tLIST_HEAD(clean);\n\tLIST_HEAD(recovered);\n\tLIST_HEAD(failed_recovered);\n\n\t \n\twrite_lock_irq(&rh->hash_lock);\n\tspin_lock(&rh->region_lock);\n\tif (!list_empty(&rh->clean_regions)) {\n\t\tlist_splice_init(&rh->clean_regions, &clean);\n\n\t\tlist_for_each_entry(reg, &clean, list)\n\t\t\tlist_del(&reg->hash_list);\n\t}\n\n\tif (!list_empty(&rh->recovered_regions)) {\n\t\tlist_splice_init(&rh->recovered_regions, &recovered);\n\n\t\tlist_for_each_entry(reg, &recovered, list)\n\t\t\tlist_del(&reg->hash_list);\n\t}\n\n\tif (!list_empty(&rh->failed_recovered_regions)) {\n\t\tlist_splice_init(&rh->failed_recovered_regions,\n\t\t\t\t &failed_recovered);\n\n\t\tlist_for_each_entry(reg, &failed_recovered, list)\n\t\t\tlist_del(&reg->hash_list);\n\t}\n\n\tspin_unlock(&rh->region_lock);\n\twrite_unlock_irq(&rh->hash_lock);\n\n\t \n\tlist_for_each_entry_safe(reg, next, &recovered, list) {\n\t\trh->log->type->clear_region(rh->log, reg->key);\n\t\tcomplete_resync_work(reg, 1);\n\t\tmempool_free(reg, &rh->region_pool);\n\t}\n\n\tlist_for_each_entry_safe(reg, next, &failed_recovered, list) {\n\t\tcomplete_resync_work(reg, errors_handled ? 0 : 1);\n\t\tmempool_free(reg, &rh->region_pool);\n\t}\n\n\tlist_for_each_entry_safe(reg, next, &clean, list) {\n\t\trh->log->type->clear_region(rh->log, reg->key);\n\t\tmempool_free(reg, &rh->region_pool);\n\t}\n\n\trh->log->type->flush(rh->log);\n}\nEXPORT_SYMBOL_GPL(dm_rh_update_states);\n\nstatic void rh_inc(struct dm_region_hash *rh, region_t region)\n{\n\tstruct dm_region *reg;\n\n\tread_lock(&rh->hash_lock);\n\treg = __rh_find(rh, region);\n\n\tspin_lock_irq(&rh->region_lock);\n\tatomic_inc(&reg->pending);\n\n\tif (reg->state == DM_RH_CLEAN) {\n\t\treg->state = DM_RH_DIRTY;\n\t\tlist_del_init(&reg->list);\t \n\t\tspin_unlock_irq(&rh->region_lock);\n\n\t\trh->log->type->mark_region(rh->log, reg->key);\n\t} else\n\t\tspin_unlock_irq(&rh->region_lock);\n\n\n\tread_unlock(&rh->hash_lock);\n}\n\nvoid dm_rh_inc_pending(struct dm_region_hash *rh, struct bio_list *bios)\n{\n\tstruct bio *bio;\n\n\tfor (bio = bios->head; bio; bio = bio->bi_next) {\n\t\tif (bio->bi_opf & REQ_PREFLUSH || bio_op(bio) == REQ_OP_DISCARD)\n\t\t\tcontinue;\n\t\trh_inc(rh, dm_rh_bio_to_region(rh, bio));\n\t}\n}\nEXPORT_SYMBOL_GPL(dm_rh_inc_pending);\n\nvoid dm_rh_dec(struct dm_region_hash *rh, region_t region)\n{\n\tunsigned long flags;\n\tstruct dm_region *reg;\n\tint should_wake = 0;\n\n\tread_lock(&rh->hash_lock);\n\treg = __rh_lookup(rh, region);\n\tread_unlock(&rh->hash_lock);\n\n\tspin_lock_irqsave(&rh->region_lock, flags);\n\tif (atomic_dec_and_test(&reg->pending)) {\n\t\t \n\n\t\t \n\t\tif (unlikely(rh->flush_failure)) {\n\t\t\t \n\t\t\treg->state = DM_RH_NOSYNC;\n\t\t} else if (reg->state == DM_RH_RECOVERING) {\n\t\t\tlist_add_tail(&reg->list, &rh->quiesced_regions);\n\t\t} else if (reg->state == DM_RH_DIRTY) {\n\t\t\treg->state = DM_RH_CLEAN;\n\t\t\tlist_add(&reg->list, &rh->clean_regions);\n\t\t}\n\t\tshould_wake = 1;\n\t}\n\tspin_unlock_irqrestore(&rh->region_lock, flags);\n\n\tif (should_wake)\n\t\trh->wakeup_workers(rh->context);\n}\nEXPORT_SYMBOL_GPL(dm_rh_dec);\n\n \nstatic int __rh_recovery_prepare(struct dm_region_hash *rh)\n{\n\tint r;\n\tregion_t region;\n\tstruct dm_region *reg;\n\n\t \n\tr = rh->log->type->get_resync_work(rh->log, &region);\n\tif (r <= 0)\n\t\treturn r;\n\n\t \n\tread_lock(&rh->hash_lock);\n\treg = __rh_find(rh, region);\n\tread_unlock(&rh->hash_lock);\n\n\tspin_lock_irq(&rh->region_lock);\n\treg->state = DM_RH_RECOVERING;\n\n\t \n\tif (atomic_read(&reg->pending))\n\t\tlist_del_init(&reg->list);\n\telse\n\t\tlist_move(&reg->list, &rh->quiesced_regions);\n\n\tspin_unlock_irq(&rh->region_lock);\n\n\treturn 1;\n}\n\nvoid dm_rh_recovery_prepare(struct dm_region_hash *rh)\n{\n\t \n\tatomic_inc(&rh->recovery_in_flight);\n\n\twhile (!down_trylock(&rh->recovery_count)) {\n\t\tatomic_inc(&rh->recovery_in_flight);\n\t\tif (__rh_recovery_prepare(rh) <= 0) {\n\t\t\tatomic_dec(&rh->recovery_in_flight);\n\t\t\tup(&rh->recovery_count);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tif (atomic_dec_and_test(&rh->recovery_in_flight))\n\t\trh->wakeup_all_recovery_waiters(rh->context);\n}\nEXPORT_SYMBOL_GPL(dm_rh_recovery_prepare);\n\n \nstruct dm_region *dm_rh_recovery_start(struct dm_region_hash *rh)\n{\n\tstruct dm_region *reg = NULL;\n\n\tspin_lock_irq(&rh->region_lock);\n\tif (!list_empty(&rh->quiesced_regions)) {\n\t\treg = list_entry(rh->quiesced_regions.next,\n\t\t\t\t struct dm_region, list);\n\t\tlist_del_init(&reg->list);   \n\t}\n\tspin_unlock_irq(&rh->region_lock);\n\n\treturn reg;\n}\nEXPORT_SYMBOL_GPL(dm_rh_recovery_start);\n\nvoid dm_rh_recovery_end(struct dm_region *reg, int success)\n{\n\tstruct dm_region_hash *rh = reg->rh;\n\n\tspin_lock_irq(&rh->region_lock);\n\tif (success)\n\t\tlist_add(&reg->list, &reg->rh->recovered_regions);\n\telse\n\t\tlist_add(&reg->list, &reg->rh->failed_recovered_regions);\n\n\tspin_unlock_irq(&rh->region_lock);\n\n\trh->wakeup_workers(rh->context);\n}\nEXPORT_SYMBOL_GPL(dm_rh_recovery_end);\n\n \nint dm_rh_recovery_in_flight(struct dm_region_hash *rh)\n{\n\treturn atomic_read(&rh->recovery_in_flight);\n}\nEXPORT_SYMBOL_GPL(dm_rh_recovery_in_flight);\n\nint dm_rh_flush(struct dm_region_hash *rh)\n{\n\treturn rh->log->type->flush(rh->log);\n}\nEXPORT_SYMBOL_GPL(dm_rh_flush);\n\nvoid dm_rh_delay(struct dm_region_hash *rh, struct bio *bio)\n{\n\tstruct dm_region *reg;\n\n\tread_lock(&rh->hash_lock);\n\treg = __rh_find(rh, dm_rh_bio_to_region(rh, bio));\n\tbio_list_add(&reg->delayed_bios, bio);\n\tread_unlock(&rh->hash_lock);\n}\nEXPORT_SYMBOL_GPL(dm_rh_delay);\n\nvoid dm_rh_stop_recovery(struct dm_region_hash *rh)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < rh->max_recovery; i++)\n\t\tdown(&rh->recovery_count);\n}\nEXPORT_SYMBOL_GPL(dm_rh_stop_recovery);\n\nvoid dm_rh_start_recovery(struct dm_region_hash *rh)\n{\n\tint i;\n\n\tfor (i = 0; i < rh->max_recovery; i++)\n\t\tup(&rh->recovery_count);\n\n\trh->wakeup_workers(rh->context);\n}\nEXPORT_SYMBOL_GPL(dm_rh_start_recovery);\n\nMODULE_DESCRIPTION(DM_NAME \" region hash\");\nMODULE_AUTHOR(\"Joe Thornber/Heinz Mauelshagen <dm-devel@redhat.com>\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}