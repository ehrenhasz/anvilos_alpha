{
  "module_name": "dm-cache-policy-smq.c",
  "hash_id": "c45f5678dc43fe9088d2421cb50a50c96854c52b92d941c5dcf6494b002ccfc0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-cache-policy-smq.c",
  "human_readable_source": "\n \n\n#include \"dm-cache-background-tracker.h\"\n#include \"dm-cache-policy-internal.h\"\n#include \"dm-cache-policy.h\"\n#include \"dm.h\"\n\n#include <linux/hash.h>\n#include <linux/jiffies.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/vmalloc.h>\n#include <linux/math64.h>\n\n#define DM_MSG_PREFIX \"cache-policy-smq\"\n\n \n\n \nstatic unsigned int safe_div(unsigned int n, unsigned int d)\n{\n\treturn d ? n / d : 0u;\n}\n\nstatic unsigned int safe_mod(unsigned int n, unsigned int d)\n{\n\treturn d ? n % d : 0u;\n}\n\n \n\nstruct entry {\n\tunsigned int hash_next:28;\n\tunsigned int prev:28;\n\tunsigned int next:28;\n\tunsigned int level:6;\n\tbool dirty:1;\n\tbool allocated:1;\n\tbool sentinel:1;\n\tbool pending_work:1;\n\n\tdm_oblock_t oblock;\n};\n\n \n\n#define INDEXER_NULL ((1u << 28u) - 1u)\n\n \nstruct entry_space {\n\tstruct entry *begin;\n\tstruct entry *end;\n};\n\nstatic int space_init(struct entry_space *es, unsigned int nr_entries)\n{\n\tif (!nr_entries) {\n\t\tes->begin = es->end = NULL;\n\t\treturn 0;\n\t}\n\n\tes->begin = vzalloc(array_size(nr_entries, sizeof(struct entry)));\n\tif (!es->begin)\n\t\treturn -ENOMEM;\n\n\tes->end = es->begin + nr_entries;\n\treturn 0;\n}\n\nstatic void space_exit(struct entry_space *es)\n{\n\tvfree(es->begin);\n}\n\nstatic struct entry *__get_entry(struct entry_space *es, unsigned int block)\n{\n\tstruct entry *e;\n\n\te = es->begin + block;\n\tBUG_ON(e >= es->end);\n\n\treturn e;\n}\n\nstatic unsigned int to_index(struct entry_space *es, struct entry *e)\n{\n\tBUG_ON(e < es->begin || e >= es->end);\n\treturn e - es->begin;\n}\n\nstatic struct entry *to_entry(struct entry_space *es, unsigned int block)\n{\n\tif (block == INDEXER_NULL)\n\t\treturn NULL;\n\n\treturn __get_entry(es, block);\n}\n\n \n\nstruct ilist {\n\tunsigned int nr_elts;\t \n\tunsigned int head, tail;\n};\n\nstatic void l_init(struct ilist *l)\n{\n\tl->nr_elts = 0;\n\tl->head = l->tail = INDEXER_NULL;\n}\n\nstatic struct entry *l_head(struct entry_space *es, struct ilist *l)\n{\n\treturn to_entry(es, l->head);\n}\n\nstatic struct entry *l_tail(struct entry_space *es, struct ilist *l)\n{\n\treturn to_entry(es, l->tail);\n}\n\nstatic struct entry *l_next(struct entry_space *es, struct entry *e)\n{\n\treturn to_entry(es, e->next);\n}\n\nstatic struct entry *l_prev(struct entry_space *es, struct entry *e)\n{\n\treturn to_entry(es, e->prev);\n}\n\nstatic bool l_empty(struct ilist *l)\n{\n\treturn l->head == INDEXER_NULL;\n}\n\nstatic void l_add_head(struct entry_space *es, struct ilist *l, struct entry *e)\n{\n\tstruct entry *head = l_head(es, l);\n\n\te->next = l->head;\n\te->prev = INDEXER_NULL;\n\n\tif (head)\n\t\thead->prev = l->head = to_index(es, e);\n\telse\n\t\tl->head = l->tail = to_index(es, e);\n\n\tif (!e->sentinel)\n\t\tl->nr_elts++;\n}\n\nstatic void l_add_tail(struct entry_space *es, struct ilist *l, struct entry *e)\n{\n\tstruct entry *tail = l_tail(es, l);\n\n\te->next = INDEXER_NULL;\n\te->prev = l->tail;\n\n\tif (tail)\n\t\ttail->next = l->tail = to_index(es, e);\n\telse\n\t\tl->head = l->tail = to_index(es, e);\n\n\tif (!e->sentinel)\n\t\tl->nr_elts++;\n}\n\nstatic void l_add_before(struct entry_space *es, struct ilist *l,\n\t\t\t struct entry *old, struct entry *e)\n{\n\tstruct entry *prev = l_prev(es, old);\n\n\tif (!prev)\n\t\tl_add_head(es, l, e);\n\n\telse {\n\t\te->prev = old->prev;\n\t\te->next = to_index(es, old);\n\t\tprev->next = old->prev = to_index(es, e);\n\n\t\tif (!e->sentinel)\n\t\t\tl->nr_elts++;\n\t}\n}\n\nstatic void l_del(struct entry_space *es, struct ilist *l, struct entry *e)\n{\n\tstruct entry *prev = l_prev(es, e);\n\tstruct entry *next = l_next(es, e);\n\n\tif (prev)\n\t\tprev->next = e->next;\n\telse\n\t\tl->head = e->next;\n\n\tif (next)\n\t\tnext->prev = e->prev;\n\telse\n\t\tl->tail = e->prev;\n\n\tif (!e->sentinel)\n\t\tl->nr_elts--;\n}\n\nstatic struct entry *l_pop_head(struct entry_space *es, struct ilist *l)\n{\n\tstruct entry *e;\n\n\tfor (e = l_head(es, l); e; e = l_next(es, e))\n\t\tif (!e->sentinel) {\n\t\t\tl_del(es, l, e);\n\t\t\treturn e;\n\t\t}\n\n\treturn NULL;\n}\n\nstatic struct entry *l_pop_tail(struct entry_space *es, struct ilist *l)\n{\n\tstruct entry *e;\n\n\tfor (e = l_tail(es, l); e; e = l_prev(es, e))\n\t\tif (!e->sentinel) {\n\t\t\tl_del(es, l, e);\n\t\t\treturn e;\n\t\t}\n\n\treturn NULL;\n}\n\n \n\n \n#define MAX_LEVELS 64u\n\nstruct queue {\n\tstruct entry_space *es;\n\n\tunsigned int nr_elts;\n\tunsigned int nr_levels;\n\tstruct ilist qs[MAX_LEVELS];\n\n\t \n\tunsigned int last_target_nr_elts;\n\tunsigned int nr_top_levels;\n\tunsigned int nr_in_top_levels;\n\tunsigned int target_count[MAX_LEVELS];\n};\n\nstatic void q_init(struct queue *q, struct entry_space *es, unsigned int nr_levels)\n{\n\tunsigned int i;\n\n\tq->es = es;\n\tq->nr_elts = 0;\n\tq->nr_levels = nr_levels;\n\n\tfor (i = 0; i < q->nr_levels; i++) {\n\t\tl_init(q->qs + i);\n\t\tq->target_count[i] = 0u;\n\t}\n\n\tq->last_target_nr_elts = 0u;\n\tq->nr_top_levels = 0u;\n\tq->nr_in_top_levels = 0u;\n}\n\nstatic unsigned int q_size(struct queue *q)\n{\n\treturn q->nr_elts;\n}\n\n \nstatic void q_push(struct queue *q, struct entry *e)\n{\n\tBUG_ON(e->pending_work);\n\n\tif (!e->sentinel)\n\t\tq->nr_elts++;\n\n\tl_add_tail(q->es, q->qs + e->level, e);\n}\n\nstatic void q_push_front(struct queue *q, struct entry *e)\n{\n\tBUG_ON(e->pending_work);\n\n\tif (!e->sentinel)\n\t\tq->nr_elts++;\n\n\tl_add_head(q->es, q->qs + e->level, e);\n}\n\nstatic void q_push_before(struct queue *q, struct entry *old, struct entry *e)\n{\n\tBUG_ON(e->pending_work);\n\n\tif (!e->sentinel)\n\t\tq->nr_elts++;\n\n\tl_add_before(q->es, q->qs + e->level, old, e);\n}\n\nstatic void q_del(struct queue *q, struct entry *e)\n{\n\tl_del(q->es, q->qs + e->level, e);\n\tif (!e->sentinel)\n\t\tq->nr_elts--;\n}\n\n \nstatic struct entry *q_peek(struct queue *q, unsigned int max_level, bool can_cross_sentinel)\n{\n\tunsigned int level;\n\tstruct entry *e;\n\n\tmax_level = min(max_level, q->nr_levels);\n\n\tfor (level = 0; level < max_level; level++)\n\t\tfor (e = l_head(q->es, q->qs + level); e; e = l_next(q->es, e)) {\n\t\t\tif (e->sentinel) {\n\t\t\t\tif (can_cross_sentinel)\n\t\t\t\t\tcontinue;\n\t\t\t\telse\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\treturn e;\n\t\t}\n\n\treturn NULL;\n}\n\nstatic struct entry *q_pop(struct queue *q)\n{\n\tstruct entry *e = q_peek(q, q->nr_levels, true);\n\n\tif (e)\n\t\tq_del(q, e);\n\n\treturn e;\n}\n\n \nstatic struct entry *__redist_pop_from(struct queue *q, unsigned int level)\n{\n\tstruct entry *e;\n\n\tfor (; level < q->nr_levels; level++)\n\t\tfor (e = l_head(q->es, q->qs + level); e; e = l_next(q->es, e))\n\t\t\tif (!e->sentinel) {\n\t\t\t\tl_del(q->es, q->qs + e->level, e);\n\t\t\t\treturn e;\n\t\t\t}\n\n\treturn NULL;\n}\n\nstatic void q_set_targets_subrange_(struct queue *q, unsigned int nr_elts,\n\t\t\t\t    unsigned int lbegin, unsigned int lend)\n{\n\tunsigned int level, nr_levels, entries_per_level, remainder;\n\n\tBUG_ON(lbegin > lend);\n\tBUG_ON(lend > q->nr_levels);\n\tnr_levels = lend - lbegin;\n\tentries_per_level = safe_div(nr_elts, nr_levels);\n\tremainder = safe_mod(nr_elts, nr_levels);\n\n\tfor (level = lbegin; level < lend; level++)\n\t\tq->target_count[level] =\n\t\t\t(level < (lbegin + remainder)) ? entries_per_level + 1u : entries_per_level;\n}\n\n \nstatic void q_set_targets(struct queue *q)\n{\n\tif (q->last_target_nr_elts == q->nr_elts)\n\t\treturn;\n\n\tq->last_target_nr_elts = q->nr_elts;\n\n\tif (q->nr_top_levels > q->nr_levels)\n\t\tq_set_targets_subrange_(q, q->nr_elts, 0, q->nr_levels);\n\n\telse {\n\t\tq_set_targets_subrange_(q, q->nr_in_top_levels,\n\t\t\t\t\tq->nr_levels - q->nr_top_levels, q->nr_levels);\n\n\t\tif (q->nr_in_top_levels < q->nr_elts)\n\t\t\tq_set_targets_subrange_(q, q->nr_elts - q->nr_in_top_levels,\n\t\t\t\t\t\t0, q->nr_levels - q->nr_top_levels);\n\t\telse\n\t\t\tq_set_targets_subrange_(q, 0, 0, q->nr_levels - q->nr_top_levels);\n\t}\n}\n\nstatic void q_redistribute(struct queue *q)\n{\n\tunsigned int target, level;\n\tstruct ilist *l, *l_above;\n\tstruct entry *e;\n\n\tq_set_targets(q);\n\n\tfor (level = 0u; level < q->nr_levels - 1u; level++) {\n\t\tl = q->qs + level;\n\t\ttarget = q->target_count[level];\n\n\t\t \n\t\twhile (l->nr_elts < target) {\n\t\t\te = __redist_pop_from(q, level + 1u);\n\t\t\tif (!e) {\n\t\t\t\t \n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\te->level = level;\n\t\t\tl_add_tail(q->es, l, e);\n\t\t}\n\n\t\t \n\t\tl_above = q->qs + level + 1u;\n\t\twhile (l->nr_elts > target) {\n\t\t\te = l_pop_tail(q->es, l);\n\n\t\t\tif (!e)\n\t\t\t\t \n\t\t\t\tbreak;\n\n\t\t\te->level = level + 1u;\n\t\t\tl_add_tail(q->es, l_above, e);\n\t\t}\n\t}\n}\n\nstatic void q_requeue(struct queue *q, struct entry *e, unsigned int extra_levels,\n\t\t      struct entry *s1, struct entry *s2)\n{\n\tstruct entry *de;\n\tunsigned int sentinels_passed = 0;\n\tunsigned int new_level = min(q->nr_levels - 1u, e->level + extra_levels);\n\n\t \n\tif (extra_levels && (e->level < q->nr_levels - 1u)) {\n\t\tfor (de = l_head(q->es, q->qs + new_level); de && de->sentinel; de = l_next(q->es, de))\n\t\t\tsentinels_passed++;\n\n\t\tif (de) {\n\t\t\tq_del(q, de);\n\t\t\tde->level = e->level;\n\t\t\tif (s1) {\n\t\t\t\tswitch (sentinels_passed) {\n\t\t\t\tcase 0:\n\t\t\t\t\tq_push_before(q, s1, de);\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 1:\n\t\t\t\t\tq_push_before(q, s2, de);\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\tq_push(q, de);\n\t\t\t\t}\n\t\t\t} else\n\t\t\t\tq_push(q, de);\n\t\t}\n\t}\n\n\tq_del(q, e);\n\te->level = new_level;\n\tq_push(q, e);\n}\n\n \n\n#define FP_SHIFT 8\n#define SIXTEENTH (1u << (FP_SHIFT - 4u))\n#define EIGHTH (1u << (FP_SHIFT - 3u))\n\nstruct stats {\n\tunsigned int hit_threshold;\n\tunsigned int hits;\n\tunsigned int misses;\n};\n\nenum performance {\n\tQ_POOR,\n\tQ_FAIR,\n\tQ_WELL\n};\n\nstatic void stats_init(struct stats *s, unsigned int nr_levels)\n{\n\ts->hit_threshold = (nr_levels * 3u) / 4u;\n\ts->hits = 0u;\n\ts->misses = 0u;\n}\n\nstatic void stats_reset(struct stats *s)\n{\n\ts->hits = s->misses = 0u;\n}\n\nstatic void stats_level_accessed(struct stats *s, unsigned int level)\n{\n\tif (level >= s->hit_threshold)\n\t\ts->hits++;\n\telse\n\t\ts->misses++;\n}\n\nstatic void stats_miss(struct stats *s)\n{\n\ts->misses++;\n}\n\n \nstatic enum performance stats_assess(struct stats *s)\n{\n\tunsigned int confidence = safe_div(s->hits << FP_SHIFT, s->hits + s->misses);\n\n\tif (confidence < SIXTEENTH)\n\t\treturn Q_POOR;\n\n\telse if (confidence < EIGHTH)\n\t\treturn Q_FAIR;\n\n\telse\n\t\treturn Q_WELL;\n}\n\n \n\nstruct smq_hash_table {\n\tstruct entry_space *es;\n\tunsigned long long hash_bits;\n\tunsigned int *buckets;\n};\n\n \nstatic int h_init(struct smq_hash_table *ht, struct entry_space *es, unsigned int nr_entries)\n{\n\tunsigned int i, nr_buckets;\n\n\tht->es = es;\n\tnr_buckets = roundup_pow_of_two(max(nr_entries / 4u, 16u));\n\tht->hash_bits = __ffs(nr_buckets);\n\n\tht->buckets = vmalloc(array_size(nr_buckets, sizeof(*ht->buckets)));\n\tif (!ht->buckets)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nr_buckets; i++)\n\t\tht->buckets[i] = INDEXER_NULL;\n\n\treturn 0;\n}\n\nstatic void h_exit(struct smq_hash_table *ht)\n{\n\tvfree(ht->buckets);\n}\n\nstatic struct entry *h_head(struct smq_hash_table *ht, unsigned int bucket)\n{\n\treturn to_entry(ht->es, ht->buckets[bucket]);\n}\n\nstatic struct entry *h_next(struct smq_hash_table *ht, struct entry *e)\n{\n\treturn to_entry(ht->es, e->hash_next);\n}\n\nstatic void __h_insert(struct smq_hash_table *ht, unsigned int bucket, struct entry *e)\n{\n\te->hash_next = ht->buckets[bucket];\n\tht->buckets[bucket] = to_index(ht->es, e);\n}\n\nstatic void h_insert(struct smq_hash_table *ht, struct entry *e)\n{\n\tunsigned int h = hash_64(from_oblock(e->oblock), ht->hash_bits);\n\n\t__h_insert(ht, h, e);\n}\n\nstatic struct entry *__h_lookup(struct smq_hash_table *ht, unsigned int h, dm_oblock_t oblock,\n\t\t\t\tstruct entry **prev)\n{\n\tstruct entry *e;\n\n\t*prev = NULL;\n\tfor (e = h_head(ht, h); e; e = h_next(ht, e)) {\n\t\tif (e->oblock == oblock)\n\t\t\treturn e;\n\n\t\t*prev = e;\n\t}\n\n\treturn NULL;\n}\n\nstatic void __h_unlink(struct smq_hash_table *ht, unsigned int h,\n\t\t       struct entry *e, struct entry *prev)\n{\n\tif (prev)\n\t\tprev->hash_next = e->hash_next;\n\telse\n\t\tht->buckets[h] = e->hash_next;\n}\n\n \nstatic struct entry *h_lookup(struct smq_hash_table *ht, dm_oblock_t oblock)\n{\n\tstruct entry *e, *prev;\n\tunsigned int h = hash_64(from_oblock(oblock), ht->hash_bits);\n\n\te = __h_lookup(ht, h, oblock, &prev);\n\tif (e && prev) {\n\t\t \n\t\t__h_unlink(ht, h, e, prev);\n\t\t__h_insert(ht, h, e);\n\t}\n\n\treturn e;\n}\n\nstatic void h_remove(struct smq_hash_table *ht, struct entry *e)\n{\n\tunsigned int h = hash_64(from_oblock(e->oblock), ht->hash_bits);\n\tstruct entry *prev;\n\n\t \n\te = __h_lookup(ht, h, e->oblock, &prev);\n\tif (e)\n\t\t__h_unlink(ht, h, e, prev);\n}\n\n \n\nstruct entry_alloc {\n\tstruct entry_space *es;\n\tunsigned int begin;\n\n\tunsigned int nr_allocated;\n\tstruct ilist free;\n};\n\nstatic void init_allocator(struct entry_alloc *ea, struct entry_space *es,\n\t\t\t   unsigned int begin, unsigned int end)\n{\n\tunsigned int i;\n\n\tea->es = es;\n\tea->nr_allocated = 0u;\n\tea->begin = begin;\n\n\tl_init(&ea->free);\n\tfor (i = begin; i != end; i++)\n\t\tl_add_tail(ea->es, &ea->free, __get_entry(ea->es, i));\n}\n\nstatic void init_entry(struct entry *e)\n{\n\t \n\te->hash_next = INDEXER_NULL;\n\te->next = INDEXER_NULL;\n\te->prev = INDEXER_NULL;\n\te->level = 0u;\n\te->dirty = true;\t \n\te->allocated = true;\n\te->sentinel = false;\n\te->pending_work = false;\n}\n\nstatic struct entry *alloc_entry(struct entry_alloc *ea)\n{\n\tstruct entry *e;\n\n\tif (l_empty(&ea->free))\n\t\treturn NULL;\n\n\te = l_pop_head(ea->es, &ea->free);\n\tinit_entry(e);\n\tea->nr_allocated++;\n\n\treturn e;\n}\n\n \nstatic struct entry *alloc_particular_entry(struct entry_alloc *ea, unsigned int i)\n{\n\tstruct entry *e = __get_entry(ea->es, ea->begin + i);\n\n\tBUG_ON(e->allocated);\n\n\tl_del(ea->es, &ea->free, e);\n\tinit_entry(e);\n\tea->nr_allocated++;\n\n\treturn e;\n}\n\nstatic void free_entry(struct entry_alloc *ea, struct entry *e)\n{\n\tBUG_ON(!ea->nr_allocated);\n\tBUG_ON(!e->allocated);\n\n\tea->nr_allocated--;\n\te->allocated = false;\n\tl_add_tail(ea->es, &ea->free, e);\n}\n\nstatic bool allocator_empty(struct entry_alloc *ea)\n{\n\treturn l_empty(&ea->free);\n}\n\nstatic unsigned int get_index(struct entry_alloc *ea, struct entry *e)\n{\n\treturn to_index(ea->es, e) - ea->begin;\n}\n\nstatic struct entry *get_entry(struct entry_alloc *ea, unsigned int index)\n{\n\treturn __get_entry(ea->es, ea->begin + index);\n}\n\n \n\n#define NR_HOTSPOT_LEVELS 64u\n#define NR_CACHE_LEVELS 64u\n\n#define WRITEBACK_PERIOD (10ul * HZ)\n#define DEMOTE_PERIOD (60ul * HZ)\n\n#define HOTSPOT_UPDATE_PERIOD (HZ)\n#define CACHE_UPDATE_PERIOD (60ul * HZ)\n\nstruct smq_policy {\n\tstruct dm_cache_policy policy;\n\n\t \n\tspinlock_t lock;\n\tdm_cblock_t cache_size;\n\tsector_t cache_block_size;\n\n\tsector_t hotspot_block_size;\n\tunsigned int nr_hotspot_blocks;\n\tunsigned int cache_blocks_per_hotspot_block;\n\tunsigned int hotspot_level_jump;\n\n\tstruct entry_space es;\n\tstruct entry_alloc writeback_sentinel_alloc;\n\tstruct entry_alloc demote_sentinel_alloc;\n\tstruct entry_alloc hotspot_alloc;\n\tstruct entry_alloc cache_alloc;\n\n\tunsigned long *hotspot_hit_bits;\n\tunsigned long *cache_hit_bits;\n\n\t \n\tstruct queue hotspot;\n\tstruct queue clean;\n\tstruct queue dirty;\n\n\tstruct stats hotspot_stats;\n\tstruct stats cache_stats;\n\n\t \n\tunsigned int tick;\n\n\t \n\tstruct smq_hash_table table;\n\tstruct smq_hash_table hotspot_table;\n\n\tbool current_writeback_sentinels;\n\tunsigned long next_writeback_period;\n\n\tbool current_demote_sentinels;\n\tunsigned long next_demote_period;\n\n\tunsigned int write_promote_level;\n\tunsigned int read_promote_level;\n\n\tunsigned long next_hotspot_period;\n\tunsigned long next_cache_period;\n\n\tstruct background_tracker *bg_work;\n\n\tbool migrations_allowed:1;\n\n\t \n\tbool cleaner:1;\n};\n\n \n\nstatic struct entry *get_sentinel(struct entry_alloc *ea, unsigned int level, bool which)\n{\n\treturn get_entry(ea, which ? level : NR_CACHE_LEVELS + level);\n}\n\nstatic struct entry *writeback_sentinel(struct smq_policy *mq, unsigned int level)\n{\n\treturn get_sentinel(&mq->writeback_sentinel_alloc, level, mq->current_writeback_sentinels);\n}\n\nstatic struct entry *demote_sentinel(struct smq_policy *mq, unsigned int level)\n{\n\treturn get_sentinel(&mq->demote_sentinel_alloc, level, mq->current_demote_sentinels);\n}\n\nstatic void __update_writeback_sentinels(struct smq_policy *mq)\n{\n\tunsigned int level;\n\tstruct queue *q = &mq->dirty;\n\tstruct entry *sentinel;\n\n\tfor (level = 0; level < q->nr_levels; level++) {\n\t\tsentinel = writeback_sentinel(mq, level);\n\t\tq_del(q, sentinel);\n\t\tq_push(q, sentinel);\n\t}\n}\n\nstatic void __update_demote_sentinels(struct smq_policy *mq)\n{\n\tunsigned int level;\n\tstruct queue *q = &mq->clean;\n\tstruct entry *sentinel;\n\n\tfor (level = 0; level < q->nr_levels; level++) {\n\t\tsentinel = demote_sentinel(mq, level);\n\t\tq_del(q, sentinel);\n\t\tq_push(q, sentinel);\n\t}\n}\n\nstatic void update_sentinels(struct smq_policy *mq)\n{\n\tif (time_after(jiffies, mq->next_writeback_period)) {\n\t\tmq->next_writeback_period = jiffies + WRITEBACK_PERIOD;\n\t\tmq->current_writeback_sentinels = !mq->current_writeback_sentinels;\n\t\t__update_writeback_sentinels(mq);\n\t}\n\n\tif (time_after(jiffies, mq->next_demote_period)) {\n\t\tmq->next_demote_period = jiffies + DEMOTE_PERIOD;\n\t\tmq->current_demote_sentinels = !mq->current_demote_sentinels;\n\t\t__update_demote_sentinels(mq);\n\t}\n}\n\nstatic void __sentinels_init(struct smq_policy *mq)\n{\n\tunsigned int level;\n\tstruct entry *sentinel;\n\n\tfor (level = 0; level < NR_CACHE_LEVELS; level++) {\n\t\tsentinel = writeback_sentinel(mq, level);\n\t\tsentinel->level = level;\n\t\tq_push(&mq->dirty, sentinel);\n\n\t\tsentinel = demote_sentinel(mq, level);\n\t\tsentinel->level = level;\n\t\tq_push(&mq->clean, sentinel);\n\t}\n}\n\nstatic void sentinels_init(struct smq_policy *mq)\n{\n\tmq->next_writeback_period = jiffies + WRITEBACK_PERIOD;\n\tmq->next_demote_period = jiffies + DEMOTE_PERIOD;\n\n\tmq->current_writeback_sentinels = false;\n\tmq->current_demote_sentinels = false;\n\t__sentinels_init(mq);\n\n\tmq->current_writeback_sentinels = !mq->current_writeback_sentinels;\n\tmq->current_demote_sentinels = !mq->current_demote_sentinels;\n\t__sentinels_init(mq);\n}\n\n \n\nstatic void del_queue(struct smq_policy *mq, struct entry *e)\n{\n\tq_del(e->dirty ? &mq->dirty : &mq->clean, e);\n}\n\nstatic void push_queue(struct smq_policy *mq, struct entry *e)\n{\n\tif (e->dirty)\n\t\tq_push(&mq->dirty, e);\n\telse\n\t\tq_push(&mq->clean, e);\n}\n\n \nstatic void push(struct smq_policy *mq, struct entry *e)\n{\n\th_insert(&mq->table, e);\n\tif (!e->pending_work)\n\t\tpush_queue(mq, e);\n}\n\nstatic void push_queue_front(struct smq_policy *mq, struct entry *e)\n{\n\tif (e->dirty)\n\t\tq_push_front(&mq->dirty, e);\n\telse\n\t\tq_push_front(&mq->clean, e);\n}\n\nstatic void push_front(struct smq_policy *mq, struct entry *e)\n{\n\th_insert(&mq->table, e);\n\tif (!e->pending_work)\n\t\tpush_queue_front(mq, e);\n}\n\nstatic dm_cblock_t infer_cblock(struct smq_policy *mq, struct entry *e)\n{\n\treturn to_cblock(get_index(&mq->cache_alloc, e));\n}\n\nstatic void requeue(struct smq_policy *mq, struct entry *e)\n{\n\t \n\tif (e->pending_work)\n\t\treturn;\n\n\tif (!test_and_set_bit(from_cblock(infer_cblock(mq, e)), mq->cache_hit_bits)) {\n\t\tif (!e->dirty) {\n\t\t\tq_requeue(&mq->clean, e, 1u, NULL, NULL);\n\t\t\treturn;\n\t\t}\n\n\t\tq_requeue(&mq->dirty, e, 1u,\n\t\t\t  get_sentinel(&mq->writeback_sentinel_alloc, e->level, !mq->current_writeback_sentinels),\n\t\t\t  get_sentinel(&mq->writeback_sentinel_alloc, e->level, mq->current_writeback_sentinels));\n\t}\n}\n\nstatic unsigned int default_promote_level(struct smq_policy *mq)\n{\n\t \n\tstatic const unsigned int table[] = {\n\t\t1, 1, 1, 2, 4, 6, 7, 8, 7, 6, 4, 4, 3, 3, 2, 2, 1\n\t};\n\n\tunsigned int hits = mq->cache_stats.hits;\n\tunsigned int misses = mq->cache_stats.misses;\n\tunsigned int index = safe_div(hits << 4u, hits + misses);\n\treturn table[index];\n}\n\nstatic void update_promote_levels(struct smq_policy *mq)\n{\n\t \n\tunsigned int threshold_level = allocator_empty(&mq->cache_alloc) ?\n\t\tdefault_promote_level(mq) : (NR_HOTSPOT_LEVELS / 2u);\n\n\tthreshold_level = max(threshold_level, NR_HOTSPOT_LEVELS);\n\n\t \n\tswitch (stats_assess(&mq->hotspot_stats)) {\n\tcase Q_POOR:\n\t\tthreshold_level /= 4u;\n\t\tbreak;\n\n\tcase Q_FAIR:\n\t\tthreshold_level /= 2u;\n\t\tbreak;\n\n\tcase Q_WELL:\n\t\tbreak;\n\t}\n\n\tmq->read_promote_level = NR_HOTSPOT_LEVELS - threshold_level;\n\tmq->write_promote_level = (NR_HOTSPOT_LEVELS - threshold_level);\n}\n\n \nstatic void update_level_jump(struct smq_policy *mq)\n{\n\tswitch (stats_assess(&mq->hotspot_stats)) {\n\tcase Q_POOR:\n\t\tmq->hotspot_level_jump = 4u;\n\t\tbreak;\n\n\tcase Q_FAIR:\n\t\tmq->hotspot_level_jump = 2u;\n\t\tbreak;\n\n\tcase Q_WELL:\n\t\tmq->hotspot_level_jump = 1u;\n\t\tbreak;\n\t}\n}\n\nstatic void end_hotspot_period(struct smq_policy *mq)\n{\n\tclear_bitset(mq->hotspot_hit_bits, mq->nr_hotspot_blocks);\n\tupdate_promote_levels(mq);\n\n\tif (time_after(jiffies, mq->next_hotspot_period)) {\n\t\tupdate_level_jump(mq);\n\t\tq_redistribute(&mq->hotspot);\n\t\tstats_reset(&mq->hotspot_stats);\n\t\tmq->next_hotspot_period = jiffies + HOTSPOT_UPDATE_PERIOD;\n\t}\n}\n\nstatic void end_cache_period(struct smq_policy *mq)\n{\n\tif (time_after(jiffies, mq->next_cache_period)) {\n\t\tclear_bitset(mq->cache_hit_bits, from_cblock(mq->cache_size));\n\n\t\tq_redistribute(&mq->dirty);\n\t\tq_redistribute(&mq->clean);\n\t\tstats_reset(&mq->cache_stats);\n\n\t\tmq->next_cache_period = jiffies + CACHE_UPDATE_PERIOD;\n\t}\n}\n\n \n\n \n#define CLEAN_TARGET 25u\n#define FREE_TARGET 25u\n\nstatic unsigned int percent_to_target(struct smq_policy *mq, unsigned int p)\n{\n\treturn from_cblock(mq->cache_size) * p / 100u;\n}\n\nstatic bool clean_target_met(struct smq_policy *mq, bool idle)\n{\n\t \n\tif (idle || mq->cleaner) {\n\t\t \n\t\treturn q_size(&mq->dirty) == 0u;\n\t}\n\n\t \n\treturn true;\n}\n\nstatic bool free_target_met(struct smq_policy *mq)\n{\n\tunsigned int nr_free;\n\n\tnr_free = from_cblock(mq->cache_size) - mq->cache_alloc.nr_allocated;\n\treturn (nr_free + btracker_nr_demotions_queued(mq->bg_work)) >=\n\t\tpercent_to_target(mq, FREE_TARGET);\n}\n\n \n\nstatic void mark_pending(struct smq_policy *mq, struct entry *e)\n{\n\tBUG_ON(e->sentinel);\n\tBUG_ON(!e->allocated);\n\tBUG_ON(e->pending_work);\n\te->pending_work = true;\n}\n\nstatic void clear_pending(struct smq_policy *mq, struct entry *e)\n{\n\tBUG_ON(!e->pending_work);\n\te->pending_work = false;\n}\n\nstatic void queue_writeback(struct smq_policy *mq, bool idle)\n{\n\tint r;\n\tstruct policy_work work;\n\tstruct entry *e;\n\n\te = q_peek(&mq->dirty, mq->dirty.nr_levels, idle);\n\tif (e) {\n\t\tmark_pending(mq, e);\n\t\tq_del(&mq->dirty, e);\n\n\t\twork.op = POLICY_WRITEBACK;\n\t\twork.oblock = e->oblock;\n\t\twork.cblock = infer_cblock(mq, e);\n\n\t\tr = btracker_queue(mq->bg_work, &work, NULL);\n\t\tif (r) {\n\t\t\tclear_pending(mq, e);\n\t\t\tq_push_front(&mq->dirty, e);\n\t\t}\n\t}\n}\n\nstatic void queue_demotion(struct smq_policy *mq)\n{\n\tint r;\n\tstruct policy_work work;\n\tstruct entry *e;\n\n\tif (WARN_ON_ONCE(!mq->migrations_allowed))\n\t\treturn;\n\n\te = q_peek(&mq->clean, mq->clean.nr_levels / 2, true);\n\tif (!e) {\n\t\tif (!clean_target_met(mq, true))\n\t\t\tqueue_writeback(mq, false);\n\t\treturn;\n\t}\n\n\tmark_pending(mq, e);\n\tq_del(&mq->clean, e);\n\n\twork.op = POLICY_DEMOTE;\n\twork.oblock = e->oblock;\n\twork.cblock = infer_cblock(mq, e);\n\tr = btracker_queue(mq->bg_work, &work, NULL);\n\tif (r) {\n\t\tclear_pending(mq, e);\n\t\tq_push_front(&mq->clean, e);\n\t}\n}\n\nstatic void queue_promotion(struct smq_policy *mq, dm_oblock_t oblock,\n\t\t\t    struct policy_work **workp)\n{\n\tint r;\n\tstruct entry *e;\n\tstruct policy_work work;\n\n\tif (!mq->migrations_allowed)\n\t\treturn;\n\n\tif (allocator_empty(&mq->cache_alloc)) {\n\t\t \n\t\tif (!free_target_met(mq))\n\t\t\tqueue_demotion(mq);\n\t\treturn;\n\t}\n\n\tif (btracker_promotion_already_present(mq->bg_work, oblock))\n\t\treturn;\n\n\t \n\te = alloc_entry(&mq->cache_alloc);\n\tBUG_ON(!e);\n\te->pending_work = true;\n\twork.op = POLICY_PROMOTE;\n\twork.oblock = oblock;\n\twork.cblock = infer_cblock(mq, e);\n\tr = btracker_queue(mq->bg_work, &work, workp);\n\tif (r)\n\t\tfree_entry(&mq->cache_alloc, e);\n}\n\n \n\nenum promote_result {\n\tPROMOTE_NOT,\n\tPROMOTE_TEMPORARY,\n\tPROMOTE_PERMANENT\n};\n\n \nstatic enum promote_result maybe_promote(bool promote)\n{\n\treturn promote ? PROMOTE_PERMANENT : PROMOTE_NOT;\n}\n\nstatic enum promote_result should_promote(struct smq_policy *mq, struct entry *hs_e,\n\t\t\t\t\t  int data_dir, bool fast_promote)\n{\n\tif (data_dir == WRITE) {\n\t\tif (!allocator_empty(&mq->cache_alloc) && fast_promote)\n\t\t\treturn PROMOTE_TEMPORARY;\n\n\t\treturn maybe_promote(hs_e->level >= mq->write_promote_level);\n\t} else\n\t\treturn maybe_promote(hs_e->level >= mq->read_promote_level);\n}\n\nstatic dm_oblock_t to_hblock(struct smq_policy *mq, dm_oblock_t b)\n{\n\tsector_t r = from_oblock(b);\n\t(void) sector_div(r, mq->cache_blocks_per_hotspot_block);\n\treturn to_oblock(r);\n}\n\nstatic struct entry *update_hotspot_queue(struct smq_policy *mq, dm_oblock_t b)\n{\n\tunsigned int hi;\n\tdm_oblock_t hb = to_hblock(mq, b);\n\tstruct entry *e = h_lookup(&mq->hotspot_table, hb);\n\n\tif (e) {\n\t\tstats_level_accessed(&mq->hotspot_stats, e->level);\n\n\t\thi = get_index(&mq->hotspot_alloc, e);\n\t\tq_requeue(&mq->hotspot, e,\n\t\t\t  test_and_set_bit(hi, mq->hotspot_hit_bits) ?\n\t\t\t  0u : mq->hotspot_level_jump,\n\t\t\t  NULL, NULL);\n\n\t} else {\n\t\tstats_miss(&mq->hotspot_stats);\n\n\t\te = alloc_entry(&mq->hotspot_alloc);\n\t\tif (!e) {\n\t\t\te = q_pop(&mq->hotspot);\n\t\t\tif (e) {\n\t\t\t\th_remove(&mq->hotspot_table, e);\n\t\t\t\thi = get_index(&mq->hotspot_alloc, e);\n\t\t\t\tclear_bit(hi, mq->hotspot_hit_bits);\n\t\t\t}\n\n\t\t}\n\n\t\tif (e) {\n\t\t\te->oblock = hb;\n\t\t\tq_push(&mq->hotspot, e);\n\t\t\th_insert(&mq->hotspot_table, e);\n\t\t}\n\t}\n\n\treturn e;\n}\n\n \n\n \n\nstatic struct smq_policy *to_smq_policy(struct dm_cache_policy *p)\n{\n\treturn container_of(p, struct smq_policy, policy);\n}\n\nstatic void smq_destroy(struct dm_cache_policy *p)\n{\n\tstruct smq_policy *mq = to_smq_policy(p);\n\n\tbtracker_destroy(mq->bg_work);\n\th_exit(&mq->hotspot_table);\n\th_exit(&mq->table);\n\tfree_bitset(mq->hotspot_hit_bits);\n\tfree_bitset(mq->cache_hit_bits);\n\tspace_exit(&mq->es);\n\tkfree(mq);\n}\n\n \n\nstatic int __lookup(struct smq_policy *mq, dm_oblock_t oblock, dm_cblock_t *cblock,\n\t\t    int data_dir, bool fast_copy,\n\t\t    struct policy_work **work, bool *background_work)\n{\n\tstruct entry *e, *hs_e;\n\tenum promote_result pr;\n\n\t*background_work = false;\n\n\te = h_lookup(&mq->table, oblock);\n\tif (e) {\n\t\tstats_level_accessed(&mq->cache_stats, e->level);\n\n\t\trequeue(mq, e);\n\t\t*cblock = infer_cblock(mq, e);\n\t\treturn 0;\n\n\t} else {\n\t\tstats_miss(&mq->cache_stats);\n\n\t\t \n\t\ths_e = update_hotspot_queue(mq, oblock);\n\n\t\tpr = should_promote(mq, hs_e, data_dir, fast_copy);\n\t\tif (pr != PROMOTE_NOT) {\n\t\t\tqueue_promotion(mq, oblock, work);\n\t\t\t*background_work = true;\n\t\t}\n\n\t\treturn -ENOENT;\n\t}\n}\n\nstatic int smq_lookup(struct dm_cache_policy *p, dm_oblock_t oblock, dm_cblock_t *cblock,\n\t\t      int data_dir, bool fast_copy,\n\t\t      bool *background_work)\n{\n\tint r;\n\tunsigned long flags;\n\tstruct smq_policy *mq = to_smq_policy(p);\n\n\tspin_lock_irqsave(&mq->lock, flags);\n\tr = __lookup(mq, oblock, cblock,\n\t\t     data_dir, fast_copy,\n\t\t     NULL, background_work);\n\tspin_unlock_irqrestore(&mq->lock, flags);\n\n\treturn r;\n}\n\nstatic int smq_lookup_with_work(struct dm_cache_policy *p,\n\t\t\t\tdm_oblock_t oblock, dm_cblock_t *cblock,\n\t\t\t\tint data_dir, bool fast_copy,\n\t\t\t\tstruct policy_work **work)\n{\n\tint r;\n\tbool background_queued;\n\tunsigned long flags;\n\tstruct smq_policy *mq = to_smq_policy(p);\n\n\tspin_lock_irqsave(&mq->lock, flags);\n\tr = __lookup(mq, oblock, cblock, data_dir, fast_copy, work, &background_queued);\n\tspin_unlock_irqrestore(&mq->lock, flags);\n\n\treturn r;\n}\n\nstatic int smq_get_background_work(struct dm_cache_policy *p, bool idle,\n\t\t\t\t   struct policy_work **result)\n{\n\tint r;\n\tunsigned long flags;\n\tstruct smq_policy *mq = to_smq_policy(p);\n\n\tspin_lock_irqsave(&mq->lock, flags);\n\tr = btracker_issue(mq->bg_work, result);\n\tif (r == -ENODATA) {\n\t\tif (!clean_target_met(mq, idle)) {\n\t\t\tqueue_writeback(mq, idle);\n\t\t\tr = btracker_issue(mq->bg_work, result);\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&mq->lock, flags);\n\n\treturn r;\n}\n\n \nstatic void __complete_background_work(struct smq_policy *mq,\n\t\t\t\t       struct policy_work *work,\n\t\t\t\t       bool success)\n{\n\tstruct entry *e = get_entry(&mq->cache_alloc,\n\t\t\t\t    from_cblock(work->cblock));\n\n\tswitch (work->op) {\n\tcase POLICY_PROMOTE:\n\t\t\n\t\tclear_pending(mq, e);\n\t\tif (success) {\n\t\t\te->oblock = work->oblock;\n\t\t\te->level = NR_CACHE_LEVELS - 1;\n\t\t\tpush(mq, e);\n\t\t\t\n\t\t} else {\n\t\t\tfree_entry(&mq->cache_alloc, e);\n\t\t\t\n\t\t}\n\t\tbreak;\n\n\tcase POLICY_DEMOTE:\n\t\t\n\t\tif (success) {\n\t\t\th_remove(&mq->table, e);\n\t\t\tfree_entry(&mq->cache_alloc, e);\n\t\t\t\n\t\t} else {\n\t\t\tclear_pending(mq, e);\n\t\t\tpush_queue(mq, e);\n\t\t\t\n\t\t}\n\t\tbreak;\n\n\tcase POLICY_WRITEBACK:\n\t\t\n\t\tclear_pending(mq, e);\n\t\tpush_queue(mq, e);\n\t\t\n\t\tbreak;\n\t}\n\n\tbtracker_complete(mq->bg_work, work);\n}\n\nstatic void smq_complete_background_work(struct dm_cache_policy *p,\n\t\t\t\t\t struct policy_work *work,\n\t\t\t\t\t bool success)\n{\n\tunsigned long flags;\n\tstruct smq_policy *mq = to_smq_policy(p);\n\n\tspin_lock_irqsave(&mq->lock, flags);\n\t__complete_background_work(mq, work, success);\n\tspin_unlock_irqrestore(&mq->lock, flags);\n}\n\n\nstatic void __smq_set_clear_dirty(struct smq_policy *mq, dm_cblock_t cblock, bool set)\n{\n\tstruct entry *e = get_entry(&mq->cache_alloc, from_cblock(cblock));\n\n\tif (e->pending_work)\n\t\te->dirty = set;\n\telse {\n\t\tdel_queue(mq, e);\n\t\te->dirty = set;\n\t\tpush_queue(mq, e);\n\t}\n}\n\nstatic void smq_set_dirty(struct dm_cache_policy *p, dm_cblock_t cblock)\n{\n\tunsigned long flags;\n\tstruct smq_policy *mq = to_smq_policy(p);\n\n\tspin_lock_irqsave(&mq->lock, flags);\n\t__smq_set_clear_dirty(mq, cblock, true);\n\tspin_unlock_irqrestore(&mq->lock, flags);\n}\n\nstatic void smq_clear_dirty(struct dm_cache_policy *p, dm_cblock_t cblock)\n{\n\tstruct smq_policy *mq = to_smq_policy(p);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&mq->lock, flags);\n\t__smq_set_clear_dirty(mq, cblock, false);\n\tspin_unlock_irqrestore(&mq->lock, flags);\n}\n\nstatic unsigned int random_level(dm_cblock_t cblock)\n{\n\treturn hash_32(from_cblock(cblock), 9) & (NR_CACHE_LEVELS - 1);\n}\n\nstatic int smq_load_mapping(struct dm_cache_policy *p,\n\t\t\t    dm_oblock_t oblock, dm_cblock_t cblock,\n\t\t\t    bool dirty, uint32_t hint, bool hint_valid)\n{\n\tstruct smq_policy *mq = to_smq_policy(p);\n\tstruct entry *e;\n\n\te = alloc_particular_entry(&mq->cache_alloc, from_cblock(cblock));\n\te->oblock = oblock;\n\te->dirty = dirty;\n\te->level = hint_valid ? min(hint, NR_CACHE_LEVELS - 1) : random_level(cblock);\n\te->pending_work = false;\n\n\t \n\tpush_front(mq, e);\n\n\treturn 0;\n}\n\nstatic int smq_invalidate_mapping(struct dm_cache_policy *p, dm_cblock_t cblock)\n{\n\tstruct smq_policy *mq = to_smq_policy(p);\n\tstruct entry *e = get_entry(&mq->cache_alloc, from_cblock(cblock));\n\n\tif (!e->allocated)\n\t\treturn -ENODATA;\n\n\t\n\tdel_queue(mq, e);\n\th_remove(&mq->table, e);\n\tfree_entry(&mq->cache_alloc, e);\n\treturn 0;\n}\n\nstatic uint32_t smq_get_hint(struct dm_cache_policy *p, dm_cblock_t cblock)\n{\n\tstruct smq_policy *mq = to_smq_policy(p);\n\tstruct entry *e = get_entry(&mq->cache_alloc, from_cblock(cblock));\n\n\tif (!e->allocated)\n\t\treturn 0;\n\n\treturn e->level;\n}\n\nstatic dm_cblock_t smq_residency(struct dm_cache_policy *p)\n{\n\tdm_cblock_t r;\n\tunsigned long flags;\n\tstruct smq_policy *mq = to_smq_policy(p);\n\n\tspin_lock_irqsave(&mq->lock, flags);\n\tr = to_cblock(mq->cache_alloc.nr_allocated);\n\tspin_unlock_irqrestore(&mq->lock, flags);\n\n\treturn r;\n}\n\nstatic void smq_tick(struct dm_cache_policy *p, bool can_block)\n{\n\tstruct smq_policy *mq = to_smq_policy(p);\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&mq->lock, flags);\n\tmq->tick++;\n\tupdate_sentinels(mq);\n\tend_hotspot_period(mq);\n\tend_cache_period(mq);\n\tspin_unlock_irqrestore(&mq->lock, flags);\n}\n\nstatic void smq_allow_migrations(struct dm_cache_policy *p, bool allow)\n{\n\tstruct smq_policy *mq = to_smq_policy(p);\n\n\tmq->migrations_allowed = allow;\n}\n\n \nstatic int mq_set_config_value(struct dm_cache_policy *p,\n\t\t\t       const char *key, const char *value)\n{\n\tunsigned long tmp;\n\n\tif (kstrtoul(value, 10, &tmp))\n\t\treturn -EINVAL;\n\n\tif (!strcasecmp(key, \"random_threshold\") ||\n\t    !strcasecmp(key, \"sequential_threshold\") ||\n\t    !strcasecmp(key, \"discard_promote_adjustment\") ||\n\t    !strcasecmp(key, \"read_promote_adjustment\") ||\n\t    !strcasecmp(key, \"write_promote_adjustment\")) {\n\t\tDMWARN(\"tunable '%s' no longer has any effect, mq policy is now an alias for smq\", key);\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int mq_emit_config_values(struct dm_cache_policy *p, char *result,\n\t\t\t\t unsigned int maxlen, ssize_t *sz_ptr)\n{\n\tssize_t sz = *sz_ptr;\n\n\tDMEMIT(\"10 random_threshold 0 \"\n\t       \"sequential_threshold 0 \"\n\t       \"discard_promote_adjustment 0 \"\n\t       \"read_promote_adjustment 0 \"\n\t       \"write_promote_adjustment 0 \");\n\n\t*sz_ptr = sz;\n\treturn 0;\n}\n\n \nstatic void init_policy_functions(struct smq_policy *mq, bool mimic_mq)\n{\n\tmq->policy.destroy = smq_destroy;\n\tmq->policy.lookup = smq_lookup;\n\tmq->policy.lookup_with_work = smq_lookup_with_work;\n\tmq->policy.get_background_work = smq_get_background_work;\n\tmq->policy.complete_background_work = smq_complete_background_work;\n\tmq->policy.set_dirty = smq_set_dirty;\n\tmq->policy.clear_dirty = smq_clear_dirty;\n\tmq->policy.load_mapping = smq_load_mapping;\n\tmq->policy.invalidate_mapping = smq_invalidate_mapping;\n\tmq->policy.get_hint = smq_get_hint;\n\tmq->policy.residency = smq_residency;\n\tmq->policy.tick = smq_tick;\n\tmq->policy.allow_migrations = smq_allow_migrations;\n\n\tif (mimic_mq) {\n\t\tmq->policy.set_config_value = mq_set_config_value;\n\t\tmq->policy.emit_config_values = mq_emit_config_values;\n\t}\n}\n\nstatic bool too_many_hotspot_blocks(sector_t origin_size,\n\t\t\t\t    sector_t hotspot_block_size,\n\t\t\t\t    unsigned int nr_hotspot_blocks)\n{\n\treturn (hotspot_block_size * nr_hotspot_blocks) > origin_size;\n}\n\nstatic void calc_hotspot_params(sector_t origin_size,\n\t\t\t\tsector_t cache_block_size,\n\t\t\t\tunsigned int nr_cache_blocks,\n\t\t\t\tsector_t *hotspot_block_size,\n\t\t\t\tunsigned int *nr_hotspot_blocks)\n{\n\t*hotspot_block_size = cache_block_size * 16u;\n\t*nr_hotspot_blocks = max(nr_cache_blocks / 4u, 1024u);\n\n\twhile ((*hotspot_block_size > cache_block_size) &&\n\t       too_many_hotspot_blocks(origin_size, *hotspot_block_size, *nr_hotspot_blocks))\n\t\t*hotspot_block_size /= 2u;\n}\n\nstatic struct dm_cache_policy *\n__smq_create(dm_cblock_t cache_size, sector_t origin_size, sector_t cache_block_size,\n\t     bool mimic_mq, bool migrations_allowed, bool cleaner)\n{\n\tunsigned int i;\n\tunsigned int nr_sentinels_per_queue = 2u * NR_CACHE_LEVELS;\n\tunsigned int total_sentinels = 2u * nr_sentinels_per_queue;\n\tstruct smq_policy *mq = kzalloc(sizeof(*mq), GFP_KERNEL);\n\n\tif (!mq)\n\t\treturn NULL;\n\n\tinit_policy_functions(mq, mimic_mq);\n\tmq->cache_size = cache_size;\n\tmq->cache_block_size = cache_block_size;\n\n\tcalc_hotspot_params(origin_size, cache_block_size, from_cblock(cache_size),\n\t\t\t    &mq->hotspot_block_size, &mq->nr_hotspot_blocks);\n\n\tmq->cache_blocks_per_hotspot_block = div64_u64(mq->hotspot_block_size, mq->cache_block_size);\n\tmq->hotspot_level_jump = 1u;\n\tif (space_init(&mq->es, total_sentinels + mq->nr_hotspot_blocks + from_cblock(cache_size))) {\n\t\tDMERR(\"couldn't initialize entry space\");\n\t\tgoto bad_pool_init;\n\t}\n\n\tinit_allocator(&mq->writeback_sentinel_alloc, &mq->es, 0, nr_sentinels_per_queue);\n\tfor (i = 0; i < nr_sentinels_per_queue; i++)\n\t\tget_entry(&mq->writeback_sentinel_alloc, i)->sentinel = true;\n\n\tinit_allocator(&mq->demote_sentinel_alloc, &mq->es, nr_sentinels_per_queue, total_sentinels);\n\tfor (i = 0; i < nr_sentinels_per_queue; i++)\n\t\tget_entry(&mq->demote_sentinel_alloc, i)->sentinel = true;\n\n\tinit_allocator(&mq->hotspot_alloc, &mq->es, total_sentinels,\n\t\t       total_sentinels + mq->nr_hotspot_blocks);\n\n\tinit_allocator(&mq->cache_alloc, &mq->es,\n\t\t       total_sentinels + mq->nr_hotspot_blocks,\n\t\t       total_sentinels + mq->nr_hotspot_blocks + from_cblock(cache_size));\n\n\tmq->hotspot_hit_bits = alloc_bitset(mq->nr_hotspot_blocks);\n\tif (!mq->hotspot_hit_bits) {\n\t\tDMERR(\"couldn't allocate hotspot hit bitset\");\n\t\tgoto bad_hotspot_hit_bits;\n\t}\n\tclear_bitset(mq->hotspot_hit_bits, mq->nr_hotspot_blocks);\n\n\tif (from_cblock(cache_size)) {\n\t\tmq->cache_hit_bits = alloc_bitset(from_cblock(cache_size));\n\t\tif (!mq->cache_hit_bits) {\n\t\t\tDMERR(\"couldn't allocate cache hit bitset\");\n\t\t\tgoto bad_cache_hit_bits;\n\t\t}\n\t\tclear_bitset(mq->cache_hit_bits, from_cblock(mq->cache_size));\n\t} else\n\t\tmq->cache_hit_bits = NULL;\n\n\tmq->tick = 0;\n\tspin_lock_init(&mq->lock);\n\n\tq_init(&mq->hotspot, &mq->es, NR_HOTSPOT_LEVELS);\n\tmq->hotspot.nr_top_levels = 8;\n\tmq->hotspot.nr_in_top_levels = min(mq->nr_hotspot_blocks / NR_HOTSPOT_LEVELS,\n\t\t\t\t\t   from_cblock(mq->cache_size) / mq->cache_blocks_per_hotspot_block);\n\n\tq_init(&mq->clean, &mq->es, NR_CACHE_LEVELS);\n\tq_init(&mq->dirty, &mq->es, NR_CACHE_LEVELS);\n\n\tstats_init(&mq->hotspot_stats, NR_HOTSPOT_LEVELS);\n\tstats_init(&mq->cache_stats, NR_CACHE_LEVELS);\n\n\tif (h_init(&mq->table, &mq->es, from_cblock(cache_size)))\n\t\tgoto bad_alloc_table;\n\n\tif (h_init(&mq->hotspot_table, &mq->es, mq->nr_hotspot_blocks))\n\t\tgoto bad_alloc_hotspot_table;\n\n\tsentinels_init(mq);\n\tmq->write_promote_level = mq->read_promote_level = NR_HOTSPOT_LEVELS;\n\n\tmq->next_hotspot_period = jiffies;\n\tmq->next_cache_period = jiffies;\n\n\tmq->bg_work = btracker_create(4096);  \n\tif (!mq->bg_work)\n\t\tgoto bad_btracker;\n\n\tmq->migrations_allowed = migrations_allowed;\n\tmq->cleaner = cleaner;\n\n\treturn &mq->policy;\n\nbad_btracker:\n\th_exit(&mq->hotspot_table);\nbad_alloc_hotspot_table:\n\th_exit(&mq->table);\nbad_alloc_table:\n\tfree_bitset(mq->cache_hit_bits);\nbad_cache_hit_bits:\n\tfree_bitset(mq->hotspot_hit_bits);\nbad_hotspot_hit_bits:\n\tspace_exit(&mq->es);\nbad_pool_init:\n\tkfree(mq);\n\n\treturn NULL;\n}\n\nstatic struct dm_cache_policy *smq_create(dm_cblock_t cache_size,\n\t\t\t\t\t  sector_t origin_size,\n\t\t\t\t\t  sector_t cache_block_size)\n{\n\treturn __smq_create(cache_size, origin_size, cache_block_size,\n\t\t\t    false, true, false);\n}\n\nstatic struct dm_cache_policy *mq_create(dm_cblock_t cache_size,\n\t\t\t\t\t sector_t origin_size,\n\t\t\t\t\t sector_t cache_block_size)\n{\n\treturn __smq_create(cache_size, origin_size, cache_block_size,\n\t\t\t    true, true, false);\n}\n\nstatic struct dm_cache_policy *cleaner_create(dm_cblock_t cache_size,\n\t\t\t\t\t      sector_t origin_size,\n\t\t\t\t\t      sector_t cache_block_size)\n{\n\treturn __smq_create(cache_size, origin_size, cache_block_size,\n\t\t\t    false, false, true);\n}\n\n \n\nstatic struct dm_cache_policy_type smq_policy_type = {\n\t.name = \"smq\",\n\t.version = {2, 0, 0},\n\t.hint_size = 4,\n\t.owner = THIS_MODULE,\n\t.create = smq_create\n};\n\nstatic struct dm_cache_policy_type mq_policy_type = {\n\t.name = \"mq\",\n\t.version = {2, 0, 0},\n\t.hint_size = 4,\n\t.owner = THIS_MODULE,\n\t.create = mq_create,\n};\n\nstatic struct dm_cache_policy_type cleaner_policy_type = {\n\t.name = \"cleaner\",\n\t.version = {2, 0, 0},\n\t.hint_size = 4,\n\t.owner = THIS_MODULE,\n\t.create = cleaner_create,\n};\n\nstatic struct dm_cache_policy_type default_policy_type = {\n\t.name = \"default\",\n\t.version = {2, 0, 0},\n\t.hint_size = 4,\n\t.owner = THIS_MODULE,\n\t.create = smq_create,\n\t.real = &smq_policy_type\n};\n\nstatic int __init smq_init(void)\n{\n\tint r;\n\n\tr = dm_cache_policy_register(&smq_policy_type);\n\tif (r) {\n\t\tDMERR(\"register failed %d\", r);\n\t\treturn -ENOMEM;\n\t}\n\n\tr = dm_cache_policy_register(&mq_policy_type);\n\tif (r) {\n\t\tDMERR(\"register failed (as mq) %d\", r);\n\t\tgoto out_mq;\n\t}\n\n\tr = dm_cache_policy_register(&cleaner_policy_type);\n\tif (r) {\n\t\tDMERR(\"register failed (as cleaner) %d\", r);\n\t\tgoto out_cleaner;\n\t}\n\n\tr = dm_cache_policy_register(&default_policy_type);\n\tif (r) {\n\t\tDMERR(\"register failed (as default) %d\", r);\n\t\tgoto out_default;\n\t}\n\n\treturn 0;\n\nout_default:\n\tdm_cache_policy_unregister(&cleaner_policy_type);\nout_cleaner:\n\tdm_cache_policy_unregister(&mq_policy_type);\nout_mq:\n\tdm_cache_policy_unregister(&smq_policy_type);\n\n\treturn -ENOMEM;\n}\n\nstatic void __exit smq_exit(void)\n{\n\tdm_cache_policy_unregister(&cleaner_policy_type);\n\tdm_cache_policy_unregister(&smq_policy_type);\n\tdm_cache_policy_unregister(&mq_policy_type);\n\tdm_cache_policy_unregister(&default_policy_type);\n}\n\nmodule_init(smq_init);\nmodule_exit(smq_exit);\n\nMODULE_AUTHOR(\"Joe Thornber <dm-devel@redhat.com>\");\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"smq cache policy\");\n\nMODULE_ALIAS(\"dm-cache-default\");\nMODULE_ALIAS(\"dm-cache-mq\");\nMODULE_ALIAS(\"dm-cache-cleaner\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}