{
  "module_name": "dm-zoned-target.c",
  "hash_id": "7b5c8fd25130fd60d1523e53a8b0e8c199d89c8a9c780b40decefa45d8a0ae31",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-zoned-target.c",
  "human_readable_source": "\n \n\n#include \"dm-zoned.h\"\n\n#include <linux/module.h>\n\n#define\tDM_MSG_PREFIX\t\t\"zoned\"\n\n#define DMZ_MIN_BIOS\t\t8192\n\n \nstruct dmz_bioctx {\n\tstruct dmz_dev\t\t*dev;\n\tstruct dm_zone\t\t*zone;\n\tstruct bio\t\t*bio;\n\trefcount_t\t\tref;\n};\n\n \nstruct dm_chunk_work {\n\tstruct work_struct\twork;\n\trefcount_t\t\trefcount;\n\tstruct dmz_target\t*target;\n\tunsigned int\t\tchunk;\n\tstruct bio_list\t\tbio_list;\n};\n\n \nstruct dmz_target {\n\tstruct dm_dev\t\t**ddev;\n\tunsigned int\t\tnr_ddevs;\n\n\tunsigned int\t\tflags;\n\n\t \n\tstruct dmz_dev\t\t*dev;\n\n\t \n\tstruct dmz_metadata     *metadata;\n\n\t \n\tstruct radix_tree_root\tchunk_rxtree;\n\tstruct workqueue_struct *chunk_wq;\n\tstruct mutex\t\tchunk_lock;\n\n\t \n\tstruct bio_set\t\tbio_set;\n\n\t \n\tspinlock_t\t\tflush_lock;\n\tstruct bio_list\t\tflush_list;\n\tstruct delayed_work\tflush_work;\n\tstruct workqueue_struct *flush_wq;\n};\n\n \n#define DMZ_FLUSH_PERIOD\t(10 * HZ)\n\n \nstatic inline void dmz_bio_endio(struct bio *bio, blk_status_t status)\n{\n\tstruct dmz_bioctx *bioctx =\n\t\tdm_per_bio_data(bio, sizeof(struct dmz_bioctx));\n\n\tif (status != BLK_STS_OK && bio->bi_status == BLK_STS_OK)\n\t\tbio->bi_status = status;\n\tif (bioctx->dev && bio->bi_status != BLK_STS_OK)\n\t\tbioctx->dev->flags |= DMZ_CHECK_BDEV;\n\n\tif (refcount_dec_and_test(&bioctx->ref)) {\n\t\tstruct dm_zone *zone = bioctx->zone;\n\n\t\tif (zone) {\n\t\t\tif (bio->bi_status != BLK_STS_OK &&\n\t\t\t    bio_op(bio) == REQ_OP_WRITE &&\n\t\t\t    dmz_is_seq(zone))\n\t\t\t\tset_bit(DMZ_SEQ_WRITE_ERR, &zone->flags);\n\t\t\tdmz_deactivate_zone(zone);\n\t\t}\n\t\tbio_endio(bio);\n\t}\n}\n\n \nstatic void dmz_clone_endio(struct bio *clone)\n{\n\tstruct dmz_bioctx *bioctx = clone->bi_private;\n\tblk_status_t status = clone->bi_status;\n\n\tbio_put(clone);\n\tdmz_bio_endio(bioctx->bio, status);\n}\n\n \nstatic int dmz_submit_bio(struct dmz_target *dmz, struct dm_zone *zone,\n\t\t\t  struct bio *bio, sector_t chunk_block,\n\t\t\t  unsigned int nr_blocks)\n{\n\tstruct dmz_bioctx *bioctx =\n\t\tdm_per_bio_data(bio, sizeof(struct dmz_bioctx));\n\tstruct dmz_dev *dev = zone->dev;\n\tstruct bio *clone;\n\n\tif (dev->flags & DMZ_BDEV_DYING)\n\t\treturn -EIO;\n\n\tclone = bio_alloc_clone(dev->bdev, bio, GFP_NOIO, &dmz->bio_set);\n\tif (!clone)\n\t\treturn -ENOMEM;\n\n\tbioctx->dev = dev;\n\tclone->bi_iter.bi_sector =\n\t\tdmz_start_sect(dmz->metadata, zone) + dmz_blk2sect(chunk_block);\n\tclone->bi_iter.bi_size = dmz_blk2sect(nr_blocks) << SECTOR_SHIFT;\n\tclone->bi_end_io = dmz_clone_endio;\n\tclone->bi_private = bioctx;\n\n\tbio_advance(bio, clone->bi_iter.bi_size);\n\n\trefcount_inc(&bioctx->ref);\n\tsubmit_bio_noacct(clone);\n\n\tif (bio_op(bio) == REQ_OP_WRITE && dmz_is_seq(zone))\n\t\tzone->wp_block += nr_blocks;\n\n\treturn 0;\n}\n\n \nstatic void dmz_handle_read_zero(struct dmz_target *dmz, struct bio *bio,\n\t\t\t\t sector_t chunk_block, unsigned int nr_blocks)\n{\n\tunsigned int size = nr_blocks << DMZ_BLOCK_SHIFT;\n\n\t \n\tswap(bio->bi_iter.bi_size, size);\n\tzero_fill_bio(bio);\n\tswap(bio->bi_iter.bi_size, size);\n\n\tbio_advance(bio, size);\n}\n\n \nstatic int dmz_handle_read(struct dmz_target *dmz, struct dm_zone *zone,\n\t\t\t   struct bio *bio)\n{\n\tstruct dmz_metadata *zmd = dmz->metadata;\n\tsector_t chunk_block = dmz_chunk_block(zmd, dmz_bio_block(bio));\n\tunsigned int nr_blocks = dmz_bio_blocks(bio);\n\tsector_t end_block = chunk_block + nr_blocks;\n\tstruct dm_zone *rzone, *bzone;\n\tint ret;\n\n\t \n\tif (!zone) {\n\t\tzero_fill_bio(bio);\n\t\treturn 0;\n\t}\n\n\tDMDEBUG(\"(%s): READ chunk %llu -> %s zone %u, block %llu, %u blocks\",\n\t\tdmz_metadata_label(zmd),\n\t\t(unsigned long long)dmz_bio_chunk(zmd, bio),\n\t\t(dmz_is_rnd(zone) ? \"RND\" :\n\t\t (dmz_is_cache(zone) ? \"CACHE\" : \"SEQ\")),\n\t\tzone->id,\n\t\t(unsigned long long)chunk_block, nr_blocks);\n\n\t \n\tbzone = zone->bzone;\n\twhile (chunk_block < end_block) {\n\t\tnr_blocks = 0;\n\t\tif (dmz_is_rnd(zone) || dmz_is_cache(zone) ||\n\t\t    chunk_block < zone->wp_block) {\n\t\t\t \n\t\t\tret = dmz_block_valid(zmd, zone, chunk_block);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t\tif (ret > 0) {\n\t\t\t\t \n\t\t\t\tnr_blocks = ret;\n\t\t\t\trzone = zone;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (!nr_blocks && bzone) {\n\t\t\tret = dmz_block_valid(zmd, bzone, chunk_block);\n\t\t\tif (ret < 0)\n\t\t\t\treturn ret;\n\t\t\tif (ret > 0) {\n\t\t\t\t \n\t\t\t\tnr_blocks = ret;\n\t\t\t\trzone = bzone;\n\t\t\t}\n\t\t}\n\n\t\tif (nr_blocks) {\n\t\t\t \n\t\t\tnr_blocks = min_t(unsigned int, nr_blocks,\n\t\t\t\t\t  end_block - chunk_block);\n\t\t\tret = dmz_submit_bio(dmz, rzone, bio,\n\t\t\t\t\t     chunk_block, nr_blocks);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t\tchunk_block += nr_blocks;\n\t\t} else {\n\t\t\t \n\t\t\tdmz_handle_read_zero(dmz, bio, chunk_block, 1);\n\t\t\tchunk_block++;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int dmz_handle_direct_write(struct dmz_target *dmz,\n\t\t\t\t   struct dm_zone *zone, struct bio *bio,\n\t\t\t\t   sector_t chunk_block,\n\t\t\t\t   unsigned int nr_blocks)\n{\n\tstruct dmz_metadata *zmd = dmz->metadata;\n\tstruct dm_zone *bzone = zone->bzone;\n\tint ret;\n\n\tif (dmz_is_readonly(zone))\n\t\treturn -EROFS;\n\n\t \n\tret = dmz_submit_bio(dmz, zone, bio, chunk_block, nr_blocks);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = dmz_validate_blocks(zmd, zone, chunk_block, nr_blocks);\n\tif (ret == 0 && bzone)\n\t\tret = dmz_invalidate_blocks(zmd, bzone, chunk_block, nr_blocks);\n\n\treturn ret;\n}\n\n \nstatic int dmz_handle_buffered_write(struct dmz_target *dmz,\n\t\t\t\t     struct dm_zone *zone, struct bio *bio,\n\t\t\t\t     sector_t chunk_block,\n\t\t\t\t     unsigned int nr_blocks)\n{\n\tstruct dmz_metadata *zmd = dmz->metadata;\n\tstruct dm_zone *bzone;\n\tint ret;\n\n\t \n\tbzone = dmz_get_chunk_buffer(zmd, zone);\n\tif (IS_ERR(bzone))\n\t\treturn PTR_ERR(bzone);\n\n\tif (dmz_is_readonly(bzone))\n\t\treturn -EROFS;\n\n\t \n\tret = dmz_submit_bio(dmz, bzone, bio, chunk_block, nr_blocks);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = dmz_validate_blocks(zmd, bzone, chunk_block, nr_blocks);\n\tif (ret == 0 && chunk_block < zone->wp_block)\n\t\tret = dmz_invalidate_blocks(zmd, zone, chunk_block, nr_blocks);\n\n\treturn ret;\n}\n\n \nstatic int dmz_handle_write(struct dmz_target *dmz, struct dm_zone *zone,\n\t\t\t    struct bio *bio)\n{\n\tstruct dmz_metadata *zmd = dmz->metadata;\n\tsector_t chunk_block = dmz_chunk_block(zmd, dmz_bio_block(bio));\n\tunsigned int nr_blocks = dmz_bio_blocks(bio);\n\n\tif (!zone)\n\t\treturn -ENOSPC;\n\n\tDMDEBUG(\"(%s): WRITE chunk %llu -> %s zone %u, block %llu, %u blocks\",\n\t\tdmz_metadata_label(zmd),\n\t\t(unsigned long long)dmz_bio_chunk(zmd, bio),\n\t\t(dmz_is_rnd(zone) ? \"RND\" :\n\t\t (dmz_is_cache(zone) ? \"CACHE\" : \"SEQ\")),\n\t\tzone->id,\n\t\t(unsigned long long)chunk_block, nr_blocks);\n\n\tif (dmz_is_rnd(zone) || dmz_is_cache(zone) ||\n\t    chunk_block == zone->wp_block) {\n\t\t \n\t\treturn dmz_handle_direct_write(dmz, zone, bio,\n\t\t\t\t\t       chunk_block, nr_blocks);\n\t}\n\n\t \n\treturn dmz_handle_buffered_write(dmz, zone, bio, chunk_block, nr_blocks);\n}\n\n \nstatic int dmz_handle_discard(struct dmz_target *dmz, struct dm_zone *zone,\n\t\t\t      struct bio *bio)\n{\n\tstruct dmz_metadata *zmd = dmz->metadata;\n\tsector_t block = dmz_bio_block(bio);\n\tunsigned int nr_blocks = dmz_bio_blocks(bio);\n\tsector_t chunk_block = dmz_chunk_block(zmd, block);\n\tint ret = 0;\n\n\t \n\tif (!zone)\n\t\treturn 0;\n\n\tif (dmz_is_readonly(zone))\n\t\treturn -EROFS;\n\n\tDMDEBUG(\"(%s): DISCARD chunk %llu -> zone %u, block %llu, %u blocks\",\n\t\tdmz_metadata_label(dmz->metadata),\n\t\t(unsigned long long)dmz_bio_chunk(zmd, bio),\n\t\tzone->id,\n\t\t(unsigned long long)chunk_block, nr_blocks);\n\n\t \n\tif (dmz_is_rnd(zone) || dmz_is_cache(zone) ||\n\t    chunk_block < zone->wp_block)\n\t\tret = dmz_invalidate_blocks(zmd, zone, chunk_block, nr_blocks);\n\tif (ret == 0 && zone->bzone)\n\t\tret = dmz_invalidate_blocks(zmd, zone->bzone,\n\t\t\t\t\t    chunk_block, nr_blocks);\n\treturn ret;\n}\n\n \nstatic void dmz_handle_bio(struct dmz_target *dmz, struct dm_chunk_work *cw,\n\t\t\t   struct bio *bio)\n{\n\tstruct dmz_bioctx *bioctx =\n\t\tdm_per_bio_data(bio, sizeof(struct dmz_bioctx));\n\tstruct dmz_metadata *zmd = dmz->metadata;\n\tstruct dm_zone *zone;\n\tint ret;\n\n\tdmz_lock_metadata(zmd);\n\n\t \n\tzone = dmz_get_chunk_mapping(zmd, dmz_bio_chunk(zmd, bio),\n\t\t\t\t     bio_op(bio));\n\tif (IS_ERR(zone)) {\n\t\tret = PTR_ERR(zone);\n\t\tgoto out;\n\t}\n\n\t \n\tif (zone) {\n\t\tdmz_activate_zone(zone);\n\t\tbioctx->zone = zone;\n\t\tdmz_reclaim_bio_acc(zone->dev->reclaim);\n\t}\n\n\tswitch (bio_op(bio)) {\n\tcase REQ_OP_READ:\n\t\tret = dmz_handle_read(dmz, zone, bio);\n\t\tbreak;\n\tcase REQ_OP_WRITE:\n\t\tret = dmz_handle_write(dmz, zone, bio);\n\t\tbreak;\n\tcase REQ_OP_DISCARD:\n\tcase REQ_OP_WRITE_ZEROES:\n\t\tret = dmz_handle_discard(dmz, zone, bio);\n\t\tbreak;\n\tdefault:\n\t\tDMERR(\"(%s): Unsupported BIO operation 0x%x\",\n\t\t      dmz_metadata_label(dmz->metadata), bio_op(bio));\n\t\tret = -EIO;\n\t}\n\n\t \n\tif (zone)\n\t\tdmz_put_chunk_mapping(zmd, zone);\nout:\n\tdmz_bio_endio(bio, errno_to_blk_status(ret));\n\n\tdmz_unlock_metadata(zmd);\n}\n\n \nstatic inline void dmz_get_chunk_work(struct dm_chunk_work *cw)\n{\n\trefcount_inc(&cw->refcount);\n}\n\n \nstatic void dmz_put_chunk_work(struct dm_chunk_work *cw)\n{\n\tif (refcount_dec_and_test(&cw->refcount)) {\n\t\tWARN_ON(!bio_list_empty(&cw->bio_list));\n\t\tradix_tree_delete(&cw->target->chunk_rxtree, cw->chunk);\n\t\tkfree(cw);\n\t}\n}\n\n \nstatic void dmz_chunk_work(struct work_struct *work)\n{\n\tstruct dm_chunk_work *cw = container_of(work, struct dm_chunk_work, work);\n\tstruct dmz_target *dmz = cw->target;\n\tstruct bio *bio;\n\n\tmutex_lock(&dmz->chunk_lock);\n\n\t \n\twhile ((bio = bio_list_pop(&cw->bio_list))) {\n\t\tmutex_unlock(&dmz->chunk_lock);\n\t\tdmz_handle_bio(dmz, cw, bio);\n\t\tmutex_lock(&dmz->chunk_lock);\n\t\tdmz_put_chunk_work(cw);\n\t}\n\n\t \n\tdmz_put_chunk_work(cw);\n\n\tmutex_unlock(&dmz->chunk_lock);\n}\n\n \nstatic void dmz_flush_work(struct work_struct *work)\n{\n\tstruct dmz_target *dmz = container_of(work, struct dmz_target, flush_work.work);\n\tstruct bio *bio;\n\tint ret;\n\n\t \n\tret = dmz_flush_metadata(dmz->metadata);\n\tif (ret)\n\t\tDMDEBUG(\"(%s): Metadata flush failed, rc=%d\",\n\t\t\tdmz_metadata_label(dmz->metadata), ret);\n\n\t \n\twhile (1) {\n\t\tspin_lock(&dmz->flush_lock);\n\t\tbio = bio_list_pop(&dmz->flush_list);\n\t\tspin_unlock(&dmz->flush_lock);\n\n\t\tif (!bio)\n\t\t\tbreak;\n\n\t\tdmz_bio_endio(bio, errno_to_blk_status(ret));\n\t}\n\n\tqueue_delayed_work(dmz->flush_wq, &dmz->flush_work, DMZ_FLUSH_PERIOD);\n}\n\n \nstatic int dmz_queue_chunk_work(struct dmz_target *dmz, struct bio *bio)\n{\n\tunsigned int chunk = dmz_bio_chunk(dmz->metadata, bio);\n\tstruct dm_chunk_work *cw;\n\tint ret = 0;\n\n\tmutex_lock(&dmz->chunk_lock);\n\n\t \n\tcw = radix_tree_lookup(&dmz->chunk_rxtree, chunk);\n\tif (cw) {\n\t\tdmz_get_chunk_work(cw);\n\t} else {\n\t\t \n\t\tcw = kmalloc(sizeof(struct dm_chunk_work), GFP_NOIO);\n\t\tif (unlikely(!cw)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tINIT_WORK(&cw->work, dmz_chunk_work);\n\t\trefcount_set(&cw->refcount, 1);\n\t\tcw->target = dmz;\n\t\tcw->chunk = chunk;\n\t\tbio_list_init(&cw->bio_list);\n\n\t\tret = radix_tree_insert(&dmz->chunk_rxtree, chunk, cw);\n\t\tif (unlikely(ret)) {\n\t\t\tkfree(cw);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tbio_list_add(&cw->bio_list, bio);\n\n\tif (queue_work(dmz->chunk_wq, &cw->work))\n\t\tdmz_get_chunk_work(cw);\nout:\n\tmutex_unlock(&dmz->chunk_lock);\n\treturn ret;\n}\n\n \nbool dmz_bdev_is_dying(struct dmz_dev *dmz_dev)\n{\n\tif (dmz_dev->flags & DMZ_BDEV_DYING)\n\t\treturn true;\n\n\tif (dmz_dev->flags & DMZ_CHECK_BDEV)\n\t\treturn !dmz_check_bdev(dmz_dev);\n\n\tif (blk_queue_dying(bdev_get_queue(dmz_dev->bdev))) {\n\t\tdmz_dev_warn(dmz_dev, \"Backing device queue dying\");\n\t\tdmz_dev->flags |= DMZ_BDEV_DYING;\n\t}\n\n\treturn dmz_dev->flags & DMZ_BDEV_DYING;\n}\n\n \nbool dmz_check_bdev(struct dmz_dev *dmz_dev)\n{\n\tstruct gendisk *disk;\n\n\tdmz_dev->flags &= ~DMZ_CHECK_BDEV;\n\n\tif (dmz_bdev_is_dying(dmz_dev))\n\t\treturn false;\n\n\tdisk = dmz_dev->bdev->bd_disk;\n\tif (disk->fops->check_events &&\n\t    disk->fops->check_events(disk, 0) & DISK_EVENT_MEDIA_CHANGE) {\n\t\tdmz_dev_warn(dmz_dev, \"Backing device offline\");\n\t\tdmz_dev->flags |= DMZ_BDEV_DYING;\n\t}\n\n\treturn !(dmz_dev->flags & DMZ_BDEV_DYING);\n}\n\n \nstatic int dmz_map(struct dm_target *ti, struct bio *bio)\n{\n\tstruct dmz_target *dmz = ti->private;\n\tstruct dmz_metadata *zmd = dmz->metadata;\n\tstruct dmz_bioctx *bioctx = dm_per_bio_data(bio, sizeof(struct dmz_bioctx));\n\tsector_t sector = bio->bi_iter.bi_sector;\n\tunsigned int nr_sectors = bio_sectors(bio);\n\tsector_t chunk_sector;\n\tint ret;\n\n\tif (dmz_dev_is_dying(zmd))\n\t\treturn DM_MAPIO_KILL;\n\n\tDMDEBUG(\"(%s): BIO op %d sector %llu + %u => chunk %llu, block %llu, %u blocks\",\n\t\tdmz_metadata_label(zmd),\n\t\tbio_op(bio), (unsigned long long)sector, nr_sectors,\n\t\t(unsigned long long)dmz_bio_chunk(zmd, bio),\n\t\t(unsigned long long)dmz_chunk_block(zmd, dmz_bio_block(bio)),\n\t\t(unsigned int)dmz_bio_blocks(bio));\n\n\tif (!nr_sectors && bio_op(bio) != REQ_OP_WRITE)\n\t\treturn DM_MAPIO_REMAPPED;\n\n\t \n\tif ((nr_sectors & DMZ_BLOCK_SECTORS_MASK) || (sector & DMZ_BLOCK_SECTORS_MASK))\n\t\treturn DM_MAPIO_KILL;\n\n\t \n\tbioctx->dev = NULL;\n\tbioctx->zone = NULL;\n\tbioctx->bio = bio;\n\trefcount_set(&bioctx->ref, 1);\n\n\t \n\tif (!nr_sectors && bio_op(bio) == REQ_OP_WRITE) {\n\t\tspin_lock(&dmz->flush_lock);\n\t\tbio_list_add(&dmz->flush_list, bio);\n\t\tspin_unlock(&dmz->flush_lock);\n\t\tmod_delayed_work(dmz->flush_wq, &dmz->flush_work, 0);\n\t\treturn DM_MAPIO_SUBMITTED;\n\t}\n\n\t \n\tchunk_sector = sector & (dmz_zone_nr_sectors(zmd) - 1);\n\tif (chunk_sector + nr_sectors > dmz_zone_nr_sectors(zmd))\n\t\tdm_accept_partial_bio(bio, dmz_zone_nr_sectors(zmd) - chunk_sector);\n\n\t \n\tret = dmz_queue_chunk_work(dmz, bio);\n\tif (ret) {\n\t\tDMDEBUG(\"(%s): BIO op %d, can't process chunk %llu, err %i\",\n\t\t\tdmz_metadata_label(zmd),\n\t\t\tbio_op(bio), (u64)dmz_bio_chunk(zmd, bio),\n\t\t\tret);\n\t\treturn DM_MAPIO_REQUEUE;\n\t}\n\n\treturn DM_MAPIO_SUBMITTED;\n}\n\n \nstatic int dmz_get_zoned_device(struct dm_target *ti, char *path,\n\t\t\t\tint idx, int nr_devs)\n{\n\tstruct dmz_target *dmz = ti->private;\n\tstruct dm_dev *ddev;\n\tstruct dmz_dev *dev;\n\tint ret;\n\tstruct block_device *bdev;\n\n\t \n\tret = dm_get_device(ti, path, dm_table_get_mode(ti->table), &ddev);\n\tif (ret) {\n\t\tti->error = \"Get target device failed\";\n\t\treturn ret;\n\t}\n\n\tbdev = ddev->bdev;\n\tif (bdev_zoned_model(bdev) == BLK_ZONED_NONE) {\n\t\tif (nr_devs == 1) {\n\t\t\tti->error = \"Invalid regular device\";\n\t\t\tgoto err;\n\t\t}\n\t\tif (idx != 0) {\n\t\t\tti->error = \"First device must be a regular device\";\n\t\t\tgoto err;\n\t\t}\n\t\tif (dmz->ddev[0]) {\n\t\t\tti->error = \"Too many regular devices\";\n\t\t\tgoto err;\n\t\t}\n\t\tdev = &dmz->dev[idx];\n\t\tdev->flags = DMZ_BDEV_REGULAR;\n\t} else {\n\t\tif (dmz->ddev[idx]) {\n\t\t\tti->error = \"Too many zoned devices\";\n\t\t\tgoto err;\n\t\t}\n\t\tif (nr_devs > 1 && idx == 0) {\n\t\t\tti->error = \"First device must be a regular device\";\n\t\t\tgoto err;\n\t\t}\n\t\tdev = &dmz->dev[idx];\n\t}\n\tdev->bdev = bdev;\n\tdev->dev_idx = idx;\n\n\tdev->capacity = bdev_nr_sectors(bdev);\n\tif (ti->begin) {\n\t\tti->error = \"Partial mapping is not supported\";\n\t\tgoto err;\n\t}\n\n\tdmz->ddev[idx] = ddev;\n\n\treturn 0;\nerr:\n\tdm_put_device(ti, ddev);\n\treturn -EINVAL;\n}\n\n \nstatic void dmz_put_zoned_devices(struct dm_target *ti)\n{\n\tstruct dmz_target *dmz = ti->private;\n\tint i;\n\n\tfor (i = 0; i < dmz->nr_ddevs; i++)\n\t\tif (dmz->ddev[i])\n\t\t\tdm_put_device(ti, dmz->ddev[i]);\n\n\tkfree(dmz->ddev);\n}\n\nstatic int dmz_fixup_devices(struct dm_target *ti)\n{\n\tstruct dmz_target *dmz = ti->private;\n\tstruct dmz_dev *reg_dev = NULL;\n\tsector_t zone_nr_sectors = 0;\n\tint i;\n\n\t \n\tif (dmz->nr_ddevs > 1) {\n\t\treg_dev = &dmz->dev[0];\n\t\tif (!(reg_dev->flags & DMZ_BDEV_REGULAR)) {\n\t\t\tti->error = \"Primary disk is not a regular device\";\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tfor (i = 1; i < dmz->nr_ddevs; i++) {\n\t\t\tstruct dmz_dev *zoned_dev = &dmz->dev[i];\n\t\t\tstruct block_device *bdev = zoned_dev->bdev;\n\n\t\t\tif (zoned_dev->flags & DMZ_BDEV_REGULAR) {\n\t\t\t\tti->error = \"Secondary disk is not a zoned device\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tif (zone_nr_sectors &&\n\t\t\t    zone_nr_sectors != bdev_zone_sectors(bdev)) {\n\t\t\t\tti->error = \"Zone nr sectors mismatch\";\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tzone_nr_sectors = bdev_zone_sectors(bdev);\n\t\t\tzoned_dev->zone_nr_sectors = zone_nr_sectors;\n\t\t\tzoned_dev->nr_zones = bdev_nr_zones(bdev);\n\t\t}\n\t} else {\n\t\tstruct dmz_dev *zoned_dev = &dmz->dev[0];\n\t\tstruct block_device *bdev = zoned_dev->bdev;\n\n\t\tif (zoned_dev->flags & DMZ_BDEV_REGULAR) {\n\t\t\tti->error = \"Disk is not a zoned device\";\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tzoned_dev->zone_nr_sectors = bdev_zone_sectors(bdev);\n\t\tzoned_dev->nr_zones = bdev_nr_zones(bdev);\n\t}\n\n\tif (reg_dev) {\n\t\tsector_t zone_offset;\n\n\t\treg_dev->zone_nr_sectors = zone_nr_sectors;\n\t\treg_dev->nr_zones =\n\t\t\tDIV_ROUND_UP_SECTOR_T(reg_dev->capacity,\n\t\t\t\t\t      reg_dev->zone_nr_sectors);\n\t\treg_dev->zone_offset = 0;\n\t\tzone_offset = reg_dev->nr_zones;\n\t\tfor (i = 1; i < dmz->nr_ddevs; i++) {\n\t\t\tdmz->dev[i].zone_offset = zone_offset;\n\t\t\tzone_offset += dmz->dev[i].nr_zones;\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nstatic int dmz_ctr(struct dm_target *ti, unsigned int argc, char **argv)\n{\n\tstruct dmz_target *dmz;\n\tint ret, i;\n\n\t \n\tif (argc < 1) {\n\t\tti->error = \"Invalid argument count\";\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tdmz = kzalloc(sizeof(struct dmz_target), GFP_KERNEL);\n\tif (!dmz) {\n\t\tti->error = \"Unable to allocate the zoned target descriptor\";\n\t\treturn -ENOMEM;\n\t}\n\tdmz->dev = kcalloc(argc, sizeof(struct dmz_dev), GFP_KERNEL);\n\tif (!dmz->dev) {\n\t\tti->error = \"Unable to allocate the zoned device descriptors\";\n\t\tkfree(dmz);\n\t\treturn -ENOMEM;\n\t}\n\tdmz->ddev = kcalloc(argc, sizeof(struct dm_dev *), GFP_KERNEL);\n\tif (!dmz->ddev) {\n\t\tti->error = \"Unable to allocate the dm device descriptors\";\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\tdmz->nr_ddevs = argc;\n\n\tti->private = dmz;\n\n\t \n\tfor (i = 0; i < argc; i++) {\n\t\tret = dmz_get_zoned_device(ti, argv[i], i, argc);\n\t\tif (ret)\n\t\t\tgoto err_dev;\n\t}\n\tret = dmz_fixup_devices(ti);\n\tif (ret)\n\t\tgoto err_dev;\n\n\t \n\tret = dmz_ctr_metadata(dmz->dev, argc, &dmz->metadata,\n\t\t\t       dm_table_device_name(ti->table));\n\tif (ret) {\n\t\tti->error = \"Metadata initialization failed\";\n\t\tgoto err_dev;\n\t}\n\n\t \n\tti->max_io_len = dmz_zone_nr_sectors(dmz->metadata);\n\tti->num_flush_bios = 1;\n\tti->num_discard_bios = 1;\n\tti->num_write_zeroes_bios = 1;\n\tti->per_io_data_size = sizeof(struct dmz_bioctx);\n\tti->flush_supported = true;\n\tti->discards_supported = true;\n\n\t \n\tti->len = (sector_t)dmz_nr_chunks(dmz->metadata) <<\n\t\tdmz_zone_nr_sectors_shift(dmz->metadata);\n\n\t \n\tret = bioset_init(&dmz->bio_set, DMZ_MIN_BIOS, 0, 0);\n\tif (ret) {\n\t\tti->error = \"Create BIO set failed\";\n\t\tgoto err_meta;\n\t}\n\n\t \n\tmutex_init(&dmz->chunk_lock);\n\tINIT_RADIX_TREE(&dmz->chunk_rxtree, GFP_NOIO);\n\tdmz->chunk_wq = alloc_workqueue(\"dmz_cwq_%s\",\n\t\t\t\t\tWQ_MEM_RECLAIM | WQ_UNBOUND, 0,\n\t\t\t\t\tdmz_metadata_label(dmz->metadata));\n\tif (!dmz->chunk_wq) {\n\t\tti->error = \"Create chunk workqueue failed\";\n\t\tret = -ENOMEM;\n\t\tgoto err_bio;\n\t}\n\n\t \n\tspin_lock_init(&dmz->flush_lock);\n\tbio_list_init(&dmz->flush_list);\n\tINIT_DELAYED_WORK(&dmz->flush_work, dmz_flush_work);\n\tdmz->flush_wq = alloc_ordered_workqueue(\"dmz_fwq_%s\", WQ_MEM_RECLAIM,\n\t\t\t\t\t\tdmz_metadata_label(dmz->metadata));\n\tif (!dmz->flush_wq) {\n\t\tti->error = \"Create flush workqueue failed\";\n\t\tret = -ENOMEM;\n\t\tgoto err_cwq;\n\t}\n\tmod_delayed_work(dmz->flush_wq, &dmz->flush_work, DMZ_FLUSH_PERIOD);\n\n\t \n\tfor (i = 0; i < dmz->nr_ddevs; i++) {\n\t\tret = dmz_ctr_reclaim(dmz->metadata, &dmz->dev[i].reclaim, i);\n\t\tif (ret) {\n\t\t\tti->error = \"Zone reclaim initialization failed\";\n\t\t\tgoto err_fwq;\n\t\t}\n\t}\n\n\tDMINFO(\"(%s): Target device: %llu 512-byte logical sectors (%llu blocks)\",\n\t       dmz_metadata_label(dmz->metadata),\n\t       (unsigned long long)ti->len,\n\t       (unsigned long long)dmz_sect2blk(ti->len));\n\n\treturn 0;\nerr_fwq:\n\tdestroy_workqueue(dmz->flush_wq);\nerr_cwq:\n\tdestroy_workqueue(dmz->chunk_wq);\nerr_bio:\n\tmutex_destroy(&dmz->chunk_lock);\n\tbioset_exit(&dmz->bio_set);\nerr_meta:\n\tdmz_dtr_metadata(dmz->metadata);\nerr_dev:\n\tdmz_put_zoned_devices(ti);\nerr:\n\tkfree(dmz->dev);\n\tkfree(dmz);\n\n\treturn ret;\n}\n\n \nstatic void dmz_dtr(struct dm_target *ti)\n{\n\tstruct dmz_target *dmz = ti->private;\n\tint i;\n\n\tdestroy_workqueue(dmz->chunk_wq);\n\n\tfor (i = 0; i < dmz->nr_ddevs; i++)\n\t\tdmz_dtr_reclaim(dmz->dev[i].reclaim);\n\n\tcancel_delayed_work_sync(&dmz->flush_work);\n\tdestroy_workqueue(dmz->flush_wq);\n\n\t(void) dmz_flush_metadata(dmz->metadata);\n\n\tdmz_dtr_metadata(dmz->metadata);\n\n\tbioset_exit(&dmz->bio_set);\n\n\tdmz_put_zoned_devices(ti);\n\n\tmutex_destroy(&dmz->chunk_lock);\n\n\tkfree(dmz->dev);\n\tkfree(dmz);\n}\n\n \nstatic void dmz_io_hints(struct dm_target *ti, struct queue_limits *limits)\n{\n\tstruct dmz_target *dmz = ti->private;\n\tunsigned int chunk_sectors = dmz_zone_nr_sectors(dmz->metadata);\n\n\tlimits->logical_block_size = DMZ_BLOCK_SIZE;\n\tlimits->physical_block_size = DMZ_BLOCK_SIZE;\n\n\tblk_limits_io_min(limits, DMZ_BLOCK_SIZE);\n\tblk_limits_io_opt(limits, DMZ_BLOCK_SIZE);\n\n\tlimits->discard_alignment = 0;\n\tlimits->discard_granularity = DMZ_BLOCK_SIZE;\n\tlimits->max_discard_sectors = chunk_sectors;\n\tlimits->max_hw_discard_sectors = chunk_sectors;\n\tlimits->max_write_zeroes_sectors = chunk_sectors;\n\n\t \n\tlimits->chunk_sectors = chunk_sectors;\n\tlimits->max_sectors = chunk_sectors;\n\n\t \n\tlimits->zoned = BLK_ZONED_NONE;\n}\n\n \nstatic int dmz_prepare_ioctl(struct dm_target *ti, struct block_device **bdev)\n{\n\tstruct dmz_target *dmz = ti->private;\n\tstruct dmz_dev *dev = &dmz->dev[0];\n\n\tif (!dmz_check_bdev(dev))\n\t\treturn -EIO;\n\n\t*bdev = dev->bdev;\n\n\treturn 0;\n}\n\n \nstatic void dmz_suspend(struct dm_target *ti)\n{\n\tstruct dmz_target *dmz = ti->private;\n\tint i;\n\n\tflush_workqueue(dmz->chunk_wq);\n\tfor (i = 0; i < dmz->nr_ddevs; i++)\n\t\tdmz_suspend_reclaim(dmz->dev[i].reclaim);\n\tcancel_delayed_work_sync(&dmz->flush_work);\n}\n\n \nstatic void dmz_resume(struct dm_target *ti)\n{\n\tstruct dmz_target *dmz = ti->private;\n\tint i;\n\n\tqueue_delayed_work(dmz->flush_wq, &dmz->flush_work, DMZ_FLUSH_PERIOD);\n\tfor (i = 0; i < dmz->nr_ddevs; i++)\n\t\tdmz_resume_reclaim(dmz->dev[i].reclaim);\n}\n\nstatic int dmz_iterate_devices(struct dm_target *ti,\n\t\t\t       iterate_devices_callout_fn fn, void *data)\n{\n\tstruct dmz_target *dmz = ti->private;\n\tunsigned int zone_nr_sectors = dmz_zone_nr_sectors(dmz->metadata);\n\tsector_t capacity;\n\tint i, r;\n\n\tfor (i = 0; i < dmz->nr_ddevs; i++) {\n\t\tcapacity = dmz->dev[i].capacity & ~(zone_nr_sectors - 1);\n\t\tr = fn(ti, dmz->ddev[i], 0, capacity, data);\n\t\tif (r)\n\t\t\tbreak;\n\t}\n\treturn r;\n}\n\nstatic void dmz_status(struct dm_target *ti, status_type_t type,\n\t\t       unsigned int status_flags, char *result,\n\t\t       unsigned int maxlen)\n{\n\tstruct dmz_target *dmz = ti->private;\n\tssize_t sz = 0;\n\tchar buf[BDEVNAME_SIZE];\n\tstruct dmz_dev *dev;\n\tint i;\n\n\tswitch (type) {\n\tcase STATUSTYPE_INFO:\n\t\tDMEMIT(\"%u zones %u/%u cache\",\n\t\t       dmz_nr_zones(dmz->metadata),\n\t\t       dmz_nr_unmap_cache_zones(dmz->metadata),\n\t\t       dmz_nr_cache_zones(dmz->metadata));\n\t\tfor (i = 0; i < dmz->nr_ddevs; i++) {\n\t\t\t \n\t\t\tif ((i == 0) &&\n\t\t\t    (dmz_nr_cache_zones(dmz->metadata) > 0))\n\t\t\t\tcontinue;\n\t\t\tDMEMIT(\" %u/%u random %u/%u sequential\",\n\t\t\t       dmz_nr_unmap_rnd_zones(dmz->metadata, i),\n\t\t\t       dmz_nr_rnd_zones(dmz->metadata, i),\n\t\t\t       dmz_nr_unmap_seq_zones(dmz->metadata, i),\n\t\t\t       dmz_nr_seq_zones(dmz->metadata, i));\n\t\t}\n\t\tbreak;\n\tcase STATUSTYPE_TABLE:\n\t\tdev = &dmz->dev[0];\n\t\tformat_dev_t(buf, dev->bdev->bd_dev);\n\t\tDMEMIT(\"%s\", buf);\n\t\tfor (i = 1; i < dmz->nr_ddevs; i++) {\n\t\t\tdev = &dmz->dev[i];\n\t\t\tformat_dev_t(buf, dev->bdev->bd_dev);\n\t\t\tDMEMIT(\" %s\", buf);\n\t\t}\n\t\tbreak;\n\tcase STATUSTYPE_IMA:\n\t\t*result = '\\0';\n\t\tbreak;\n\t}\n}\n\nstatic int dmz_message(struct dm_target *ti, unsigned int argc, char **argv,\n\t\t       char *result, unsigned int maxlen)\n{\n\tstruct dmz_target *dmz = ti->private;\n\tint r = -EINVAL;\n\n\tif (!strcasecmp(argv[0], \"reclaim\")) {\n\t\tint i;\n\n\t\tfor (i = 0; i < dmz->nr_ddevs; i++)\n\t\t\tdmz_schedule_reclaim(dmz->dev[i].reclaim);\n\t\tr = 0;\n\t} else\n\t\tDMERR(\"unrecognized message %s\", argv[0]);\n\treturn r;\n}\n\nstatic struct target_type zoned_target = {\n\t.name\t\t = \"zoned\",\n\t.version\t = {2, 0, 0},\n\t.features\t = DM_TARGET_SINGLETON | DM_TARGET_MIXED_ZONED_MODEL,\n\t.module\t\t = THIS_MODULE,\n\t.ctr\t\t = dmz_ctr,\n\t.dtr\t\t = dmz_dtr,\n\t.map\t\t = dmz_map,\n\t.io_hints\t = dmz_io_hints,\n\t.prepare_ioctl\t = dmz_prepare_ioctl,\n\t.postsuspend\t = dmz_suspend,\n\t.resume\t\t = dmz_resume,\n\t.iterate_devices = dmz_iterate_devices,\n\t.status\t\t = dmz_status,\n\t.message\t = dmz_message,\n};\nmodule_dm(zoned);\n\nMODULE_DESCRIPTION(DM_NAME \" target for zoned block devices\");\nMODULE_AUTHOR(\"Damien Le Moal <damien.lemoal@wdc.com>\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}