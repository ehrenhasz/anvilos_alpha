{
  "module_name": "dm-writecache.c",
  "hash_id": "12a9227a4a51f564cbbe6a7a9171b22afcb4b87fb42de69fbdeb5a8ee034064e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-writecache.c",
  "human_readable_source": "\n \n\n#include <linux/device-mapper.h>\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/vmalloc.h>\n#include <linux/kthread.h>\n#include <linux/dm-io.h>\n#include <linux/dm-kcopyd.h>\n#include <linux/dax.h>\n#include <linux/pfn_t.h>\n#include <linux/libnvdimm.h>\n#include <linux/delay.h>\n#include \"dm-io-tracker.h\"\n\n#define DM_MSG_PREFIX \"writecache\"\n\n#define HIGH_WATERMARK\t\t\t50\n#define LOW_WATERMARK\t\t\t45\n#define MAX_WRITEBACK_JOBS\t\tmin(0x10000000 / PAGE_SIZE, totalram_pages() / 16)\n#define ENDIO_LATENCY\t\t\t16\n#define WRITEBACK_LATENCY\t\t64\n#define AUTOCOMMIT_BLOCKS_SSD\t\t65536\n#define AUTOCOMMIT_BLOCKS_PMEM\t\t64\n#define AUTOCOMMIT_MSEC\t\t\t1000\n#define MAX_AGE_DIV\t\t\t16\n#define MAX_AGE_UNSPECIFIED\t\t-1UL\n#define PAUSE_WRITEBACK\t\t\t(HZ * 3)\n\n#define BITMAP_GRANULARITY\t65536\n#if BITMAP_GRANULARITY < PAGE_SIZE\n#undef BITMAP_GRANULARITY\n#define BITMAP_GRANULARITY\tPAGE_SIZE\n#endif\n\n#if IS_ENABLED(CONFIG_ARCH_HAS_PMEM_API) && IS_ENABLED(CONFIG_FS_DAX)\n#define DM_WRITECACHE_HAS_PMEM\n#endif\n\n#ifdef DM_WRITECACHE_HAS_PMEM\n#define pmem_assign(dest, src)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\ttypeof(dest) uniq = (src);\t\t\t\t\\\n\tmemcpy_flushcache(&(dest), &uniq, sizeof(dest));\t\\\n} while (0)\n#else\n#define pmem_assign(dest, src)\t((dest) = (src))\n#endif\n\n#if IS_ENABLED(CONFIG_ARCH_HAS_COPY_MC) && defined(DM_WRITECACHE_HAS_PMEM)\n#define DM_WRITECACHE_HANDLE_HARDWARE_ERRORS\n#endif\n\n#define MEMORY_SUPERBLOCK_MAGIC\t\t0x23489321\n#define MEMORY_SUPERBLOCK_VERSION\t1\n\nstruct wc_memory_entry {\n\t__le64 original_sector;\n\t__le64 seq_count;\n};\n\nstruct wc_memory_superblock {\n\tunion {\n\t\tstruct {\n\t\t\t__le32 magic;\n\t\t\t__le32 version;\n\t\t\t__le32 block_size;\n\t\t\t__le32 pad;\n\t\t\t__le64 n_blocks;\n\t\t\t__le64 seq_count;\n\t\t};\n\t\t__le64 padding[8];\n\t};\n\tstruct wc_memory_entry entries[];\n};\n\nstruct wc_entry {\n\tstruct rb_node rb_node;\n\tstruct list_head lru;\n\tunsigned short wc_list_contiguous;\n#if BITS_PER_LONG == 64\n\tbool write_in_progress : 1;\n\tunsigned long index : 47;\n#else\n\tbool write_in_progress;\n\tunsigned long index;\n#endif\n\tunsigned long age;\n#ifdef DM_WRITECACHE_HANDLE_HARDWARE_ERRORS\n\tuint64_t original_sector;\n\tuint64_t seq_count;\n#endif\n};\n\n#ifdef DM_WRITECACHE_HAS_PMEM\n#define WC_MODE_PMEM(wc)\t\t\t((wc)->pmem_mode)\n#define WC_MODE_FUA(wc)\t\t\t\t((wc)->writeback_fua)\n#else\n#define WC_MODE_PMEM(wc)\t\t\tfalse\n#define WC_MODE_FUA(wc)\t\t\t\tfalse\n#endif\n#define WC_MODE_SORT_FREELIST(wc)\t\t(!WC_MODE_PMEM(wc))\n\nstruct dm_writecache {\n\tstruct mutex lock;\n\tstruct list_head lru;\n\tunion {\n\t\tstruct list_head freelist;\n\t\tstruct {\n\t\t\tstruct rb_root freetree;\n\t\t\tstruct wc_entry *current_free;\n\t\t};\n\t};\n\tstruct rb_root tree;\n\n\tsize_t freelist_size;\n\tsize_t writeback_size;\n\tsize_t freelist_high_watermark;\n\tsize_t freelist_low_watermark;\n\tunsigned long max_age;\n\tunsigned long pause;\n\n\tunsigned int uncommitted_blocks;\n\tunsigned int autocommit_blocks;\n\tunsigned int max_writeback_jobs;\n\n\tint error;\n\n\tunsigned long autocommit_jiffies;\n\tstruct timer_list autocommit_timer;\n\tstruct wait_queue_head freelist_wait;\n\n\tstruct timer_list max_age_timer;\n\n\tatomic_t bio_in_progress[2];\n\tstruct wait_queue_head bio_in_progress_wait[2];\n\n\tstruct dm_target *ti;\n\tstruct dm_dev *dev;\n\tstruct dm_dev *ssd_dev;\n\tsector_t start_sector;\n\tvoid *memory_map;\n\tuint64_t memory_map_size;\n\tsize_t metadata_sectors;\n\tsize_t n_blocks;\n\tuint64_t seq_count;\n\tsector_t data_device_sectors;\n\tvoid *block_start;\n\tstruct wc_entry *entries;\n\tunsigned int block_size;\n\tunsigned char block_size_bits;\n\n\tbool pmem_mode:1;\n\tbool writeback_fua:1;\n\n\tbool overwrote_committed:1;\n\tbool memory_vmapped:1;\n\n\tbool start_sector_set:1;\n\tbool high_wm_percent_set:1;\n\tbool low_wm_percent_set:1;\n\tbool max_writeback_jobs_set:1;\n\tbool autocommit_blocks_set:1;\n\tbool autocommit_time_set:1;\n\tbool max_age_set:1;\n\tbool writeback_fua_set:1;\n\tbool flush_on_suspend:1;\n\tbool cleaner:1;\n\tbool cleaner_set:1;\n\tbool metadata_only:1;\n\tbool pause_set:1;\n\n\tunsigned int high_wm_percent_value;\n\tunsigned int low_wm_percent_value;\n\tunsigned int autocommit_time_value;\n\tunsigned int max_age_value;\n\tunsigned int pause_value;\n\n\tunsigned int writeback_all;\n\tstruct workqueue_struct *writeback_wq;\n\tstruct work_struct writeback_work;\n\tstruct work_struct flush_work;\n\n\tstruct dm_io_tracker iot;\n\n\tstruct dm_io_client *dm_io;\n\n\traw_spinlock_t endio_list_lock;\n\tstruct list_head endio_list;\n\tstruct task_struct *endio_thread;\n\n\tstruct task_struct *flush_thread;\n\tstruct bio_list flush_list;\n\n\tstruct dm_kcopyd_client *dm_kcopyd;\n\tunsigned long *dirty_bitmap;\n\tunsigned int dirty_bitmap_size;\n\n\tstruct bio_set bio_set;\n\tmempool_t copy_pool;\n\n\tstruct {\n\t\tunsigned long long reads;\n\t\tunsigned long long read_hits;\n\t\tunsigned long long writes;\n\t\tunsigned long long write_hits_uncommitted;\n\t\tunsigned long long write_hits_committed;\n\t\tunsigned long long writes_around;\n\t\tunsigned long long writes_allocate;\n\t\tunsigned long long writes_blocked_on_freelist;\n\t\tunsigned long long flushes;\n\t\tunsigned long long discards;\n\t} stats;\n};\n\n#define WB_LIST_INLINE\t\t16\n\nstruct writeback_struct {\n\tstruct list_head endio_entry;\n\tstruct dm_writecache *wc;\n\tstruct wc_entry **wc_list;\n\tunsigned int wc_list_n;\n\tstruct wc_entry *wc_list_inline[WB_LIST_INLINE];\n\tstruct bio bio;\n};\n\nstruct copy_struct {\n\tstruct list_head endio_entry;\n\tstruct dm_writecache *wc;\n\tstruct wc_entry *e;\n\tunsigned int n_entries;\n\tint error;\n};\n\nDECLARE_DM_KCOPYD_THROTTLE_WITH_MODULE_PARM(dm_writecache_throttle,\n\t\t\t\t\t    \"A percentage of time allocated for data copying\");\n\nstatic void wc_lock(struct dm_writecache *wc)\n{\n\tmutex_lock(&wc->lock);\n}\n\nstatic void wc_unlock(struct dm_writecache *wc)\n{\n\tmutex_unlock(&wc->lock);\n}\n\n#ifdef DM_WRITECACHE_HAS_PMEM\nstatic int persistent_memory_claim(struct dm_writecache *wc)\n{\n\tint r;\n\tloff_t s;\n\tlong p, da;\n\tpfn_t pfn;\n\tint id;\n\tstruct page **pages;\n\tsector_t offset;\n\n\twc->memory_vmapped = false;\n\n\ts = wc->memory_map_size;\n\tp = s >> PAGE_SHIFT;\n\tif (!p) {\n\t\tr = -EINVAL;\n\t\tgoto err1;\n\t}\n\tif (p != s >> PAGE_SHIFT) {\n\t\tr = -EOVERFLOW;\n\t\tgoto err1;\n\t}\n\n\toffset = get_start_sect(wc->ssd_dev->bdev);\n\tif (offset & (PAGE_SIZE / 512 - 1)) {\n\t\tr = -EINVAL;\n\t\tgoto err1;\n\t}\n\toffset >>= PAGE_SHIFT - 9;\n\n\tid = dax_read_lock();\n\n\tda = dax_direct_access(wc->ssd_dev->dax_dev, offset, p, DAX_ACCESS,\n\t\t\t&wc->memory_map, &pfn);\n\tif (da < 0) {\n\t\twc->memory_map = NULL;\n\t\tr = da;\n\t\tgoto err2;\n\t}\n\tif (!pfn_t_has_page(pfn)) {\n\t\twc->memory_map = NULL;\n\t\tr = -EOPNOTSUPP;\n\t\tgoto err2;\n\t}\n\tif (da != p) {\n\t\tlong i;\n\n\t\twc->memory_map = NULL;\n\t\tpages = kvmalloc_array(p, sizeof(struct page *), GFP_KERNEL);\n\t\tif (!pages) {\n\t\t\tr = -ENOMEM;\n\t\t\tgoto err2;\n\t\t}\n\t\ti = 0;\n\t\tdo {\n\t\t\tlong daa;\n\n\t\t\tdaa = dax_direct_access(wc->ssd_dev->dax_dev, offset + i,\n\t\t\t\t\tp - i, DAX_ACCESS, NULL, &pfn);\n\t\t\tif (daa <= 0) {\n\t\t\t\tr = daa ? daa : -EINVAL;\n\t\t\t\tgoto err3;\n\t\t\t}\n\t\t\tif (!pfn_t_has_page(pfn)) {\n\t\t\t\tr = -EOPNOTSUPP;\n\t\t\t\tgoto err3;\n\t\t\t}\n\t\t\twhile (daa-- && i < p) {\n\t\t\t\tpages[i++] = pfn_t_to_page(pfn);\n\t\t\t\tpfn.val++;\n\t\t\t\tif (!(i & 15))\n\t\t\t\t\tcond_resched();\n\t\t\t}\n\t\t} while (i < p);\n\t\twc->memory_map = vmap(pages, p, VM_MAP, PAGE_KERNEL);\n\t\tif (!wc->memory_map) {\n\t\t\tr = -ENOMEM;\n\t\t\tgoto err3;\n\t\t}\n\t\tkvfree(pages);\n\t\twc->memory_vmapped = true;\n\t}\n\n\tdax_read_unlock(id);\n\n\twc->memory_map += (size_t)wc->start_sector << SECTOR_SHIFT;\n\twc->memory_map_size -= (size_t)wc->start_sector << SECTOR_SHIFT;\n\n\treturn 0;\nerr3:\n\tkvfree(pages);\nerr2:\n\tdax_read_unlock(id);\nerr1:\n\treturn r;\n}\n#else\nstatic int persistent_memory_claim(struct dm_writecache *wc)\n{\n\treturn -EOPNOTSUPP;\n}\n#endif\n\nstatic void persistent_memory_release(struct dm_writecache *wc)\n{\n\tif (wc->memory_vmapped)\n\t\tvunmap(wc->memory_map - ((size_t)wc->start_sector << SECTOR_SHIFT));\n}\n\nstatic struct page *persistent_memory_page(void *addr)\n{\n\tif (is_vmalloc_addr(addr))\n\t\treturn vmalloc_to_page(addr);\n\telse\n\t\treturn virt_to_page(addr);\n}\n\nstatic unsigned int persistent_memory_page_offset(void *addr)\n{\n\treturn (unsigned long)addr & (PAGE_SIZE - 1);\n}\n\nstatic void persistent_memory_flush_cache(void *ptr, size_t size)\n{\n\tif (is_vmalloc_addr(ptr))\n\t\tflush_kernel_vmap_range(ptr, size);\n}\n\nstatic void persistent_memory_invalidate_cache(void *ptr, size_t size)\n{\n\tif (is_vmalloc_addr(ptr))\n\t\tinvalidate_kernel_vmap_range(ptr, size);\n}\n\nstatic struct wc_memory_superblock *sb(struct dm_writecache *wc)\n{\n\treturn wc->memory_map;\n}\n\nstatic struct wc_memory_entry *memory_entry(struct dm_writecache *wc, struct wc_entry *e)\n{\n\treturn &sb(wc)->entries[e->index];\n}\n\nstatic void *memory_data(struct dm_writecache *wc, struct wc_entry *e)\n{\n\treturn (char *)wc->block_start + (e->index << wc->block_size_bits);\n}\n\nstatic sector_t cache_sector(struct dm_writecache *wc, struct wc_entry *e)\n{\n\treturn wc->start_sector + wc->metadata_sectors +\n\t\t((sector_t)e->index << (wc->block_size_bits - SECTOR_SHIFT));\n}\n\nstatic uint64_t read_original_sector(struct dm_writecache *wc, struct wc_entry *e)\n{\n#ifdef DM_WRITECACHE_HANDLE_HARDWARE_ERRORS\n\treturn e->original_sector;\n#else\n\treturn le64_to_cpu(memory_entry(wc, e)->original_sector);\n#endif\n}\n\nstatic uint64_t read_seq_count(struct dm_writecache *wc, struct wc_entry *e)\n{\n#ifdef DM_WRITECACHE_HANDLE_HARDWARE_ERRORS\n\treturn e->seq_count;\n#else\n\treturn le64_to_cpu(memory_entry(wc, e)->seq_count);\n#endif\n}\n\nstatic void clear_seq_count(struct dm_writecache *wc, struct wc_entry *e)\n{\n#ifdef DM_WRITECACHE_HANDLE_HARDWARE_ERRORS\n\te->seq_count = -1;\n#endif\n\tpmem_assign(memory_entry(wc, e)->seq_count, cpu_to_le64(-1));\n}\n\nstatic void write_original_sector_seq_count(struct dm_writecache *wc, struct wc_entry *e,\n\t\t\t\t\t    uint64_t original_sector, uint64_t seq_count)\n{\n\tstruct wc_memory_entry me;\n#ifdef DM_WRITECACHE_HANDLE_HARDWARE_ERRORS\n\te->original_sector = original_sector;\n\te->seq_count = seq_count;\n#endif\n\tme.original_sector = cpu_to_le64(original_sector);\n\tme.seq_count = cpu_to_le64(seq_count);\n\tpmem_assign(*memory_entry(wc, e), me);\n}\n\n#define writecache_error(wc, err, msg, arg...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (!cmpxchg(&(wc)->error, 0, err))\t\t\t\t\\\n\t\tDMERR(msg, ##arg);\t\t\t\t\t\\\n\twake_up(&(wc)->freelist_wait);\t\t\t\t\t\\\n} while (0)\n\n#define writecache_has_error(wc)\t(unlikely(READ_ONCE((wc)->error)))\n\nstatic void writecache_flush_all_metadata(struct dm_writecache *wc)\n{\n\tif (!WC_MODE_PMEM(wc))\n\t\tmemset(wc->dirty_bitmap, -1, wc->dirty_bitmap_size);\n}\n\nstatic void writecache_flush_region(struct dm_writecache *wc, void *ptr, size_t size)\n{\n\tif (!WC_MODE_PMEM(wc))\n\t\t__set_bit(((char *)ptr - (char *)wc->memory_map) / BITMAP_GRANULARITY,\n\t\t\t  wc->dirty_bitmap);\n}\n\nstatic void writecache_disk_flush(struct dm_writecache *wc, struct dm_dev *dev);\n\nstruct io_notify {\n\tstruct dm_writecache *wc;\n\tstruct completion c;\n\tatomic_t count;\n};\n\nstatic void writecache_notify_io(unsigned long error, void *context)\n{\n\tstruct io_notify *endio = context;\n\n\tif (unlikely(error != 0))\n\t\twritecache_error(endio->wc, -EIO, \"error writing metadata\");\n\tBUG_ON(atomic_read(&endio->count) <= 0);\n\tif (atomic_dec_and_test(&endio->count))\n\t\tcomplete(&endio->c);\n}\n\nstatic void writecache_wait_for_ios(struct dm_writecache *wc, int direction)\n{\n\twait_event(wc->bio_in_progress_wait[direction],\n\t\t   !atomic_read(&wc->bio_in_progress[direction]));\n}\n\nstatic void ssd_commit_flushed(struct dm_writecache *wc, bool wait_for_ios)\n{\n\tstruct dm_io_region region;\n\tstruct dm_io_request req;\n\tstruct io_notify endio = {\n\t\twc,\n\t\tCOMPLETION_INITIALIZER_ONSTACK(endio.c),\n\t\tATOMIC_INIT(1),\n\t};\n\tunsigned int bitmap_bits = wc->dirty_bitmap_size * 8;\n\tunsigned int i = 0;\n\n\twhile (1) {\n\t\tunsigned int j;\n\n\t\ti = find_next_bit(wc->dirty_bitmap, bitmap_bits, i);\n\t\tif (unlikely(i == bitmap_bits))\n\t\t\tbreak;\n\t\tj = find_next_zero_bit(wc->dirty_bitmap, bitmap_bits, i);\n\n\t\tregion.bdev = wc->ssd_dev->bdev;\n\t\tregion.sector = (sector_t)i * (BITMAP_GRANULARITY >> SECTOR_SHIFT);\n\t\tregion.count = (sector_t)(j - i) * (BITMAP_GRANULARITY >> SECTOR_SHIFT);\n\n\t\tif (unlikely(region.sector >= wc->metadata_sectors))\n\t\t\tbreak;\n\t\tif (unlikely(region.sector + region.count > wc->metadata_sectors))\n\t\t\tregion.count = wc->metadata_sectors - region.sector;\n\n\t\tregion.sector += wc->start_sector;\n\t\tatomic_inc(&endio.count);\n\t\treq.bi_opf = REQ_OP_WRITE | REQ_SYNC;\n\t\treq.mem.type = DM_IO_VMA;\n\t\treq.mem.ptr.vma = (char *)wc->memory_map + (size_t)i * BITMAP_GRANULARITY;\n\t\treq.client = wc->dm_io;\n\t\treq.notify.fn = writecache_notify_io;\n\t\treq.notify.context = &endio;\n\n\t\t \n\t\t(void) dm_io(&req, 1, &region, NULL);\n\t\ti = j;\n\t}\n\n\twritecache_notify_io(0, &endio);\n\twait_for_completion_io(&endio.c);\n\n\tif (wait_for_ios)\n\t\twritecache_wait_for_ios(wc, WRITE);\n\n\twritecache_disk_flush(wc, wc->ssd_dev);\n\n\tmemset(wc->dirty_bitmap, 0, wc->dirty_bitmap_size);\n}\n\nstatic void ssd_commit_superblock(struct dm_writecache *wc)\n{\n\tint r;\n\tstruct dm_io_region region;\n\tstruct dm_io_request req;\n\n\tregion.bdev = wc->ssd_dev->bdev;\n\tregion.sector = 0;\n\tregion.count = max(4096U, wc->block_size) >> SECTOR_SHIFT;\n\n\tif (unlikely(region.sector + region.count > wc->metadata_sectors))\n\t\tregion.count = wc->metadata_sectors - region.sector;\n\n\tregion.sector += wc->start_sector;\n\n\treq.bi_opf = REQ_OP_WRITE | REQ_SYNC | REQ_FUA;\n\treq.mem.type = DM_IO_VMA;\n\treq.mem.ptr.vma = (char *)wc->memory_map;\n\treq.client = wc->dm_io;\n\treq.notify.fn = NULL;\n\treq.notify.context = NULL;\n\n\tr = dm_io(&req, 1, &region, NULL);\n\tif (unlikely(r))\n\t\twritecache_error(wc, r, \"error writing superblock\");\n}\n\nstatic void writecache_commit_flushed(struct dm_writecache *wc, bool wait_for_ios)\n{\n\tif (WC_MODE_PMEM(wc))\n\t\tpmem_wmb();\n\telse\n\t\tssd_commit_flushed(wc, wait_for_ios);\n}\n\nstatic void writecache_disk_flush(struct dm_writecache *wc, struct dm_dev *dev)\n{\n\tint r;\n\tstruct dm_io_region region;\n\tstruct dm_io_request req;\n\n\tregion.bdev = dev->bdev;\n\tregion.sector = 0;\n\tregion.count = 0;\n\treq.bi_opf = REQ_OP_WRITE | REQ_PREFLUSH;\n\treq.mem.type = DM_IO_KMEM;\n\treq.mem.ptr.addr = NULL;\n\treq.client = wc->dm_io;\n\treq.notify.fn = NULL;\n\n\tr = dm_io(&req, 1, &region, NULL);\n\tif (unlikely(r))\n\t\twritecache_error(wc, r, \"error flushing metadata: %d\", r);\n}\n\n#define WFE_RETURN_FOLLOWING\t1\n#define WFE_LOWEST_SEQ\t\t2\n\nstatic struct wc_entry *writecache_find_entry(struct dm_writecache *wc,\n\t\t\t\t\t      uint64_t block, int flags)\n{\n\tstruct wc_entry *e;\n\tstruct rb_node *node = wc->tree.rb_node;\n\n\tif (unlikely(!node))\n\t\treturn NULL;\n\n\twhile (1) {\n\t\te = container_of(node, struct wc_entry, rb_node);\n\t\tif (read_original_sector(wc, e) == block)\n\t\t\tbreak;\n\n\t\tnode = (read_original_sector(wc, e) >= block ?\n\t\t\te->rb_node.rb_left : e->rb_node.rb_right);\n\t\tif (unlikely(!node)) {\n\t\t\tif (!(flags & WFE_RETURN_FOLLOWING))\n\t\t\t\treturn NULL;\n\t\t\tif (read_original_sector(wc, e) >= block)\n\t\t\t\treturn e;\n\n\t\t\tnode = rb_next(&e->rb_node);\n\t\t\tif (unlikely(!node))\n\t\t\t\treturn NULL;\n\n\t\t\te = container_of(node, struct wc_entry, rb_node);\n\t\t\treturn e;\n\t\t}\n\t}\n\n\twhile (1) {\n\t\tstruct wc_entry *e2;\n\n\t\tif (flags & WFE_LOWEST_SEQ)\n\t\t\tnode = rb_prev(&e->rb_node);\n\t\telse\n\t\t\tnode = rb_next(&e->rb_node);\n\t\tif (unlikely(!node))\n\t\t\treturn e;\n\t\te2 = container_of(node, struct wc_entry, rb_node);\n\t\tif (read_original_sector(wc, e2) != block)\n\t\t\treturn e;\n\t\te = e2;\n\t}\n}\n\nstatic void writecache_insert_entry(struct dm_writecache *wc, struct wc_entry *ins)\n{\n\tstruct wc_entry *e;\n\tstruct rb_node **node = &wc->tree.rb_node, *parent = NULL;\n\n\twhile (*node) {\n\t\te = container_of(*node, struct wc_entry, rb_node);\n\t\tparent = &e->rb_node;\n\t\tif (read_original_sector(wc, e) > read_original_sector(wc, ins))\n\t\t\tnode = &parent->rb_left;\n\t\telse\n\t\t\tnode = &parent->rb_right;\n\t}\n\trb_link_node(&ins->rb_node, parent, node);\n\trb_insert_color(&ins->rb_node, &wc->tree);\n\tlist_add(&ins->lru, &wc->lru);\n\tins->age = jiffies;\n}\n\nstatic void writecache_unlink(struct dm_writecache *wc, struct wc_entry *e)\n{\n\tlist_del(&e->lru);\n\trb_erase(&e->rb_node, &wc->tree);\n}\n\nstatic void writecache_add_to_freelist(struct dm_writecache *wc, struct wc_entry *e)\n{\n\tif (WC_MODE_SORT_FREELIST(wc)) {\n\t\tstruct rb_node **node = &wc->freetree.rb_node, *parent = NULL;\n\n\t\tif (unlikely(!*node))\n\t\t\twc->current_free = e;\n\t\twhile (*node) {\n\t\t\tparent = *node;\n\t\t\tif (&e->rb_node < *node)\n\t\t\t\tnode = &parent->rb_left;\n\t\t\telse\n\t\t\t\tnode = &parent->rb_right;\n\t\t}\n\t\trb_link_node(&e->rb_node, parent, node);\n\t\trb_insert_color(&e->rb_node, &wc->freetree);\n\t} else {\n\t\tlist_add_tail(&e->lru, &wc->freelist);\n\t}\n\twc->freelist_size++;\n}\n\nstatic inline void writecache_verify_watermark(struct dm_writecache *wc)\n{\n\tif (unlikely(wc->freelist_size + wc->writeback_size <= wc->freelist_high_watermark))\n\t\tqueue_work(wc->writeback_wq, &wc->writeback_work);\n}\n\nstatic void writecache_max_age_timer(struct timer_list *t)\n{\n\tstruct dm_writecache *wc = from_timer(wc, t, max_age_timer);\n\n\tif (!dm_suspended(wc->ti) && !writecache_has_error(wc)) {\n\t\tqueue_work(wc->writeback_wq, &wc->writeback_work);\n\t\tmod_timer(&wc->max_age_timer, jiffies + wc->max_age / MAX_AGE_DIV);\n\t}\n}\n\nstatic struct wc_entry *writecache_pop_from_freelist(struct dm_writecache *wc, sector_t expected_sector)\n{\n\tstruct wc_entry *e;\n\n\tif (WC_MODE_SORT_FREELIST(wc)) {\n\t\tstruct rb_node *next;\n\n\t\tif (unlikely(!wc->current_free))\n\t\t\treturn NULL;\n\t\te = wc->current_free;\n\t\tif (expected_sector != (sector_t)-1 && unlikely(cache_sector(wc, e) != expected_sector))\n\t\t\treturn NULL;\n\t\tnext = rb_next(&e->rb_node);\n\t\trb_erase(&e->rb_node, &wc->freetree);\n\t\tif (unlikely(!next))\n\t\t\tnext = rb_first(&wc->freetree);\n\t\twc->current_free = next ? container_of(next, struct wc_entry, rb_node) : NULL;\n\t} else {\n\t\tif (unlikely(list_empty(&wc->freelist)))\n\t\t\treturn NULL;\n\t\te = container_of(wc->freelist.next, struct wc_entry, lru);\n\t\tif (expected_sector != (sector_t)-1 && unlikely(cache_sector(wc, e) != expected_sector))\n\t\t\treturn NULL;\n\t\tlist_del(&e->lru);\n\t}\n\twc->freelist_size--;\n\n\twritecache_verify_watermark(wc);\n\n\treturn e;\n}\n\nstatic void writecache_free_entry(struct dm_writecache *wc, struct wc_entry *e)\n{\n\twritecache_unlink(wc, e);\n\twritecache_add_to_freelist(wc, e);\n\tclear_seq_count(wc, e);\n\twritecache_flush_region(wc, memory_entry(wc, e), sizeof(struct wc_memory_entry));\n\tif (unlikely(waitqueue_active(&wc->freelist_wait)))\n\t\twake_up(&wc->freelist_wait);\n}\n\nstatic void writecache_wait_on_freelist(struct dm_writecache *wc)\n{\n\tDEFINE_WAIT(wait);\n\n\tprepare_to_wait(&wc->freelist_wait, &wait, TASK_UNINTERRUPTIBLE);\n\twc_unlock(wc);\n\tio_schedule();\n\tfinish_wait(&wc->freelist_wait, &wait);\n\twc_lock(wc);\n}\n\nstatic void writecache_poison_lists(struct dm_writecache *wc)\n{\n\t \n\tmemset(&wc->tree, -1, sizeof(wc->tree));\n\twc->lru.next = LIST_POISON1;\n\twc->lru.prev = LIST_POISON2;\n\twc->freelist.next = LIST_POISON1;\n\twc->freelist.prev = LIST_POISON2;\n}\n\nstatic void writecache_flush_entry(struct dm_writecache *wc, struct wc_entry *e)\n{\n\twritecache_flush_region(wc, memory_entry(wc, e), sizeof(struct wc_memory_entry));\n\tif (WC_MODE_PMEM(wc))\n\t\twritecache_flush_region(wc, memory_data(wc, e), wc->block_size);\n}\n\nstatic bool writecache_entry_is_committed(struct dm_writecache *wc, struct wc_entry *e)\n{\n\treturn read_seq_count(wc, e) < wc->seq_count;\n}\n\nstatic void writecache_flush(struct dm_writecache *wc)\n{\n\tstruct wc_entry *e, *e2;\n\tbool need_flush_after_free;\n\n\twc->uncommitted_blocks = 0;\n\tdel_timer(&wc->autocommit_timer);\n\n\tif (list_empty(&wc->lru))\n\t\treturn;\n\n\te = container_of(wc->lru.next, struct wc_entry, lru);\n\tif (writecache_entry_is_committed(wc, e)) {\n\t\tif (wc->overwrote_committed) {\n\t\t\twritecache_wait_for_ios(wc, WRITE);\n\t\t\twritecache_disk_flush(wc, wc->ssd_dev);\n\t\t\twc->overwrote_committed = false;\n\t\t}\n\t\treturn;\n\t}\n\twhile (1) {\n\t\twritecache_flush_entry(wc, e);\n\t\tif (unlikely(e->lru.next == &wc->lru))\n\t\t\tbreak;\n\t\te2 = container_of(e->lru.next, struct wc_entry, lru);\n\t\tif (writecache_entry_is_committed(wc, e2))\n\t\t\tbreak;\n\t\te = e2;\n\t\tcond_resched();\n\t}\n\twritecache_commit_flushed(wc, true);\n\n\twc->seq_count++;\n\tpmem_assign(sb(wc)->seq_count, cpu_to_le64(wc->seq_count));\n\tif (WC_MODE_PMEM(wc))\n\t\twritecache_commit_flushed(wc, false);\n\telse\n\t\tssd_commit_superblock(wc);\n\n\twc->overwrote_committed = false;\n\n\tneed_flush_after_free = false;\n\twhile (1) {\n\t\t \n\t\tstruct rb_node *rb_node = rb_prev(&e->rb_node);\n\n\t\tif (rb_node) {\n\t\t\te2 = container_of(rb_node, struct wc_entry, rb_node);\n\t\t\tif (read_original_sector(wc, e2) == read_original_sector(wc, e) &&\n\t\t\t    likely(!e2->write_in_progress)) {\n\t\t\t\twritecache_free_entry(wc, e2);\n\t\t\t\tneed_flush_after_free = true;\n\t\t\t}\n\t\t}\n\t\tif (unlikely(e->lru.prev == &wc->lru))\n\t\t\tbreak;\n\t\te = container_of(e->lru.prev, struct wc_entry, lru);\n\t\tcond_resched();\n\t}\n\n\tif (need_flush_after_free)\n\t\twritecache_commit_flushed(wc, false);\n}\n\nstatic void writecache_flush_work(struct work_struct *work)\n{\n\tstruct dm_writecache *wc = container_of(work, struct dm_writecache, flush_work);\n\n\twc_lock(wc);\n\twritecache_flush(wc);\n\twc_unlock(wc);\n}\n\nstatic void writecache_autocommit_timer(struct timer_list *t)\n{\n\tstruct dm_writecache *wc = from_timer(wc, t, autocommit_timer);\n\n\tif (!writecache_has_error(wc))\n\t\tqueue_work(wc->writeback_wq, &wc->flush_work);\n}\n\nstatic void writecache_schedule_autocommit(struct dm_writecache *wc)\n{\n\tif (!timer_pending(&wc->autocommit_timer))\n\t\tmod_timer(&wc->autocommit_timer, jiffies + wc->autocommit_jiffies);\n}\n\nstatic void writecache_discard(struct dm_writecache *wc, sector_t start, sector_t end)\n{\n\tstruct wc_entry *e;\n\tbool discarded_something = false;\n\n\te = writecache_find_entry(wc, start, WFE_RETURN_FOLLOWING | WFE_LOWEST_SEQ);\n\tif (unlikely(!e))\n\t\treturn;\n\n\twhile (read_original_sector(wc, e) < end) {\n\t\tstruct rb_node *node = rb_next(&e->rb_node);\n\n\t\tif (likely(!e->write_in_progress)) {\n\t\t\tif (!discarded_something) {\n\t\t\t\tif (!WC_MODE_PMEM(wc)) {\n\t\t\t\t\twritecache_wait_for_ios(wc, READ);\n\t\t\t\t\twritecache_wait_for_ios(wc, WRITE);\n\t\t\t\t}\n\t\t\t\tdiscarded_something = true;\n\t\t\t}\n\t\t\tif (!writecache_entry_is_committed(wc, e))\n\t\t\t\twc->uncommitted_blocks--;\n\t\t\twritecache_free_entry(wc, e);\n\t\t}\n\n\t\tif (unlikely(!node))\n\t\t\tbreak;\n\n\t\te = container_of(node, struct wc_entry, rb_node);\n\t}\n\n\tif (discarded_something)\n\t\twritecache_commit_flushed(wc, false);\n}\n\nstatic bool writecache_wait_for_writeback(struct dm_writecache *wc)\n{\n\tif (wc->writeback_size) {\n\t\twritecache_wait_on_freelist(wc);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void writecache_suspend(struct dm_target *ti)\n{\n\tstruct dm_writecache *wc = ti->private;\n\tbool flush_on_suspend;\n\n\tdel_timer_sync(&wc->autocommit_timer);\n\tdel_timer_sync(&wc->max_age_timer);\n\n\twc_lock(wc);\n\twritecache_flush(wc);\n\tflush_on_suspend = wc->flush_on_suspend;\n\tif (flush_on_suspend) {\n\t\twc->flush_on_suspend = false;\n\t\twc->writeback_all++;\n\t\tqueue_work(wc->writeback_wq, &wc->writeback_work);\n\t}\n\twc_unlock(wc);\n\n\tdrain_workqueue(wc->writeback_wq);\n\n\twc_lock(wc);\n\tif (flush_on_suspend)\n\t\twc->writeback_all--;\n\twhile (writecache_wait_for_writeback(wc))\n\t\t;\n\n\tif (WC_MODE_PMEM(wc))\n\t\tpersistent_memory_flush_cache(wc->memory_map, wc->memory_map_size);\n\n\twritecache_poison_lists(wc);\n\n\twc_unlock(wc);\n}\n\nstatic int writecache_alloc_entries(struct dm_writecache *wc)\n{\n\tsize_t b;\n\n\tif (wc->entries)\n\t\treturn 0;\n\twc->entries = vmalloc(array_size(sizeof(struct wc_entry), wc->n_blocks));\n\tif (!wc->entries)\n\t\treturn -ENOMEM;\n\tfor (b = 0; b < wc->n_blocks; b++) {\n\t\tstruct wc_entry *e = &wc->entries[b];\n\n\t\te->index = b;\n\t\te->write_in_progress = false;\n\t\tcond_resched();\n\t}\n\n\treturn 0;\n}\n\nstatic int writecache_read_metadata(struct dm_writecache *wc, sector_t n_sectors)\n{\n\tstruct dm_io_region region;\n\tstruct dm_io_request req;\n\n\tregion.bdev = wc->ssd_dev->bdev;\n\tregion.sector = wc->start_sector;\n\tregion.count = n_sectors;\n\treq.bi_opf = REQ_OP_READ | REQ_SYNC;\n\treq.mem.type = DM_IO_VMA;\n\treq.mem.ptr.vma = (char *)wc->memory_map;\n\treq.client = wc->dm_io;\n\treq.notify.fn = NULL;\n\n\treturn dm_io(&req, 1, &region, NULL);\n}\n\nstatic void writecache_resume(struct dm_target *ti)\n{\n\tstruct dm_writecache *wc = ti->private;\n\tsize_t b;\n\tbool need_flush = false;\n\t__le64 sb_seq_count;\n\tint r;\n\n\twc_lock(wc);\n\n\twc->data_device_sectors = bdev_nr_sectors(wc->dev->bdev);\n\n\tif (WC_MODE_PMEM(wc)) {\n\t\tpersistent_memory_invalidate_cache(wc->memory_map, wc->memory_map_size);\n\t} else {\n\t\tr = writecache_read_metadata(wc, wc->metadata_sectors);\n\t\tif (r) {\n\t\t\tsize_t sb_entries_offset;\n\n\t\t\twritecache_error(wc, r, \"unable to read metadata: %d\", r);\n\t\t\tsb_entries_offset = offsetof(struct wc_memory_superblock, entries);\n\t\t\tmemset((char *)wc->memory_map + sb_entries_offset, -1,\n\t\t\t       (wc->metadata_sectors << SECTOR_SHIFT) - sb_entries_offset);\n\t\t}\n\t}\n\n\twc->tree = RB_ROOT;\n\tINIT_LIST_HEAD(&wc->lru);\n\tif (WC_MODE_SORT_FREELIST(wc)) {\n\t\twc->freetree = RB_ROOT;\n\t\twc->current_free = NULL;\n\t} else {\n\t\tINIT_LIST_HEAD(&wc->freelist);\n\t}\n\twc->freelist_size = 0;\n\n\tr = copy_mc_to_kernel(&sb_seq_count, &sb(wc)->seq_count,\n\t\t\t      sizeof(uint64_t));\n\tif (r) {\n\t\twritecache_error(wc, r, \"hardware memory error when reading superblock: %d\", r);\n\t\tsb_seq_count = cpu_to_le64(0);\n\t}\n\twc->seq_count = le64_to_cpu(sb_seq_count);\n\n#ifdef DM_WRITECACHE_HANDLE_HARDWARE_ERRORS\n\tfor (b = 0; b < wc->n_blocks; b++) {\n\t\tstruct wc_entry *e = &wc->entries[b];\n\t\tstruct wc_memory_entry wme;\n\n\t\tif (writecache_has_error(wc)) {\n\t\t\te->original_sector = -1;\n\t\t\te->seq_count = -1;\n\t\t\tcontinue;\n\t\t}\n\t\tr = copy_mc_to_kernel(&wme, memory_entry(wc, e),\n\t\t\t\t      sizeof(struct wc_memory_entry));\n\t\tif (r) {\n\t\t\twritecache_error(wc, r, \"hardware memory error when reading metadata entry %lu: %d\",\n\t\t\t\t\t (unsigned long)b, r);\n\t\t\te->original_sector = -1;\n\t\t\te->seq_count = -1;\n\t\t} else {\n\t\t\te->original_sector = le64_to_cpu(wme.original_sector);\n\t\t\te->seq_count = le64_to_cpu(wme.seq_count);\n\t\t}\n\t\tcond_resched();\n\t}\n#endif\n\tfor (b = 0; b < wc->n_blocks; b++) {\n\t\tstruct wc_entry *e = &wc->entries[b];\n\n\t\tif (!writecache_entry_is_committed(wc, e)) {\n\t\t\tif (read_seq_count(wc, e) != -1) {\nerase_this:\n\t\t\t\tclear_seq_count(wc, e);\n\t\t\t\tneed_flush = true;\n\t\t\t}\n\t\t\twritecache_add_to_freelist(wc, e);\n\t\t} else {\n\t\t\tstruct wc_entry *old;\n\n\t\t\told = writecache_find_entry(wc, read_original_sector(wc, e), 0);\n\t\t\tif (!old) {\n\t\t\t\twritecache_insert_entry(wc, e);\n\t\t\t} else {\n\t\t\t\tif (read_seq_count(wc, old) == read_seq_count(wc, e)) {\n\t\t\t\t\twritecache_error(wc, -EINVAL,\n\t\t\t\t\t\t \"two identical entries, position %llu, sector %llu, sequence %llu\",\n\t\t\t\t\t\t (unsigned long long)b, (unsigned long long)read_original_sector(wc, e),\n\t\t\t\t\t\t (unsigned long long)read_seq_count(wc, e));\n\t\t\t\t}\n\t\t\t\tif (read_seq_count(wc, old) > read_seq_count(wc, e)) {\n\t\t\t\t\tgoto erase_this;\n\t\t\t\t} else {\n\t\t\t\t\twritecache_free_entry(wc, old);\n\t\t\t\t\twritecache_insert_entry(wc, e);\n\t\t\t\t\tneed_flush = true;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tcond_resched();\n\t}\n\n\tif (need_flush) {\n\t\twritecache_flush_all_metadata(wc);\n\t\twritecache_commit_flushed(wc, false);\n\t}\n\n\twritecache_verify_watermark(wc);\n\n\tif (wc->max_age != MAX_AGE_UNSPECIFIED)\n\t\tmod_timer(&wc->max_age_timer, jiffies + wc->max_age / MAX_AGE_DIV);\n\n\twc_unlock(wc);\n}\n\nstatic int process_flush_mesg(unsigned int argc, char **argv, struct dm_writecache *wc)\n{\n\tif (argc != 1)\n\t\treturn -EINVAL;\n\n\twc_lock(wc);\n\tif (dm_suspended(wc->ti)) {\n\t\twc_unlock(wc);\n\t\treturn -EBUSY;\n\t}\n\tif (writecache_has_error(wc)) {\n\t\twc_unlock(wc);\n\t\treturn -EIO;\n\t}\n\n\twritecache_flush(wc);\n\twc->writeback_all++;\n\tqueue_work(wc->writeback_wq, &wc->writeback_work);\n\twc_unlock(wc);\n\n\tflush_workqueue(wc->writeback_wq);\n\n\twc_lock(wc);\n\twc->writeback_all--;\n\tif (writecache_has_error(wc)) {\n\t\twc_unlock(wc);\n\t\treturn -EIO;\n\t}\n\twc_unlock(wc);\n\n\treturn 0;\n}\n\nstatic int process_flush_on_suspend_mesg(unsigned int argc, char **argv, struct dm_writecache *wc)\n{\n\tif (argc != 1)\n\t\treturn -EINVAL;\n\n\twc_lock(wc);\n\twc->flush_on_suspend = true;\n\twc_unlock(wc);\n\n\treturn 0;\n}\n\nstatic void activate_cleaner(struct dm_writecache *wc)\n{\n\twc->flush_on_suspend = true;\n\twc->cleaner = true;\n\twc->freelist_high_watermark = wc->n_blocks;\n\twc->freelist_low_watermark = wc->n_blocks;\n}\n\nstatic int process_cleaner_mesg(unsigned int argc, char **argv, struct dm_writecache *wc)\n{\n\tif (argc != 1)\n\t\treturn -EINVAL;\n\n\twc_lock(wc);\n\tactivate_cleaner(wc);\n\tif (!dm_suspended(wc->ti))\n\t\twritecache_verify_watermark(wc);\n\twc_unlock(wc);\n\n\treturn 0;\n}\n\nstatic int process_clear_stats_mesg(unsigned int argc, char **argv, struct dm_writecache *wc)\n{\n\tif (argc != 1)\n\t\treturn -EINVAL;\n\n\twc_lock(wc);\n\tmemset(&wc->stats, 0, sizeof(wc->stats));\n\twc_unlock(wc);\n\n\treturn 0;\n}\n\nstatic int writecache_message(struct dm_target *ti, unsigned int argc, char **argv,\n\t\t\t      char *result, unsigned int maxlen)\n{\n\tint r = -EINVAL;\n\tstruct dm_writecache *wc = ti->private;\n\n\tif (!strcasecmp(argv[0], \"flush\"))\n\t\tr = process_flush_mesg(argc, argv, wc);\n\telse if (!strcasecmp(argv[0], \"flush_on_suspend\"))\n\t\tr = process_flush_on_suspend_mesg(argc, argv, wc);\n\telse if (!strcasecmp(argv[0], \"cleaner\"))\n\t\tr = process_cleaner_mesg(argc, argv, wc);\n\telse if (!strcasecmp(argv[0], \"clear_stats\"))\n\t\tr = process_clear_stats_mesg(argc, argv, wc);\n\telse\n\t\tDMERR(\"unrecognised message received: %s\", argv[0]);\n\n\treturn r;\n}\n\nstatic void memcpy_flushcache_optimized(void *dest, void *source, size_t size)\n{\n\t \n#ifdef CONFIG_X86\n\tif (static_cpu_has(X86_FEATURE_CLFLUSHOPT) &&\n\t    likely(boot_cpu_data.x86_clflush_size == 64) &&\n\t    likely(size >= 768)) {\n\t\tdo {\n\t\t\tmemcpy((void *)dest, (void *)source, 64);\n\t\t\tclflushopt((void *)dest);\n\t\t\tdest += 64;\n\t\t\tsource += 64;\n\t\t\tsize -= 64;\n\t\t} while (size >= 64);\n\t\treturn;\n\t}\n#endif\n\tmemcpy_flushcache(dest, source, size);\n}\n\nstatic void bio_copy_block(struct dm_writecache *wc, struct bio *bio, void *data)\n{\n\tvoid *buf;\n\tunsigned int size;\n\tint rw = bio_data_dir(bio);\n\tunsigned int remaining_size = wc->block_size;\n\n\tdo {\n\t\tstruct bio_vec bv = bio_iter_iovec(bio, bio->bi_iter);\n\n\t\tbuf = bvec_kmap_local(&bv);\n\t\tsize = bv.bv_len;\n\t\tif (unlikely(size > remaining_size))\n\t\t\tsize = remaining_size;\n\n\t\tif (rw == READ) {\n\t\t\tint r;\n\n\t\t\tr = copy_mc_to_kernel(buf, data, size);\n\t\t\tflush_dcache_page(bio_page(bio));\n\t\t\tif (unlikely(r)) {\n\t\t\t\twritecache_error(wc, r, \"hardware memory error when reading data: %d\", r);\n\t\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\t\t}\n\t\t} else {\n\t\t\tflush_dcache_page(bio_page(bio));\n\t\t\tmemcpy_flushcache_optimized(data, buf, size);\n\t\t}\n\n\t\tkunmap_local(buf);\n\n\t\tdata = (char *)data + size;\n\t\tremaining_size -= size;\n\t\tbio_advance(bio, size);\n\t} while (unlikely(remaining_size));\n}\n\nstatic int writecache_flush_thread(void *data)\n{\n\tstruct dm_writecache *wc = data;\n\n\twhile (1) {\n\t\tstruct bio *bio;\n\n\t\twc_lock(wc);\n\t\tbio = bio_list_pop(&wc->flush_list);\n\t\tif (!bio) {\n\t\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t\twc_unlock(wc);\n\n\t\t\tif (unlikely(kthread_should_stop())) {\n\t\t\t\tset_current_state(TASK_RUNNING);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tschedule();\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (bio_op(bio) == REQ_OP_DISCARD) {\n\t\t\twritecache_discard(wc, bio->bi_iter.bi_sector,\n\t\t\t\t\t   bio_end_sector(bio));\n\t\t\twc_unlock(wc);\n\t\t\tbio_set_dev(bio, wc->dev->bdev);\n\t\t\tsubmit_bio_noacct(bio);\n\t\t} else {\n\t\t\twritecache_flush(wc);\n\t\t\twc_unlock(wc);\n\t\t\tif (writecache_has_error(wc))\n\t\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\t\tbio_endio(bio);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void writecache_offload_bio(struct dm_writecache *wc, struct bio *bio)\n{\n\tif (bio_list_empty(&wc->flush_list))\n\t\twake_up_process(wc->flush_thread);\n\tbio_list_add(&wc->flush_list, bio);\n}\n\nenum wc_map_op {\n\tWC_MAP_SUBMIT,\n\tWC_MAP_REMAP,\n\tWC_MAP_REMAP_ORIGIN,\n\tWC_MAP_RETURN,\n\tWC_MAP_ERROR,\n};\n\nstatic void writecache_map_remap_origin(struct dm_writecache *wc, struct bio *bio,\n\t\t\t\t\tstruct wc_entry *e)\n{\n\tif (e) {\n\t\tsector_t next_boundary =\n\t\t\tread_original_sector(wc, e) - bio->bi_iter.bi_sector;\n\t\tif (next_boundary < bio->bi_iter.bi_size >> SECTOR_SHIFT)\n\t\t\tdm_accept_partial_bio(bio, next_boundary);\n\t}\n}\n\nstatic enum wc_map_op writecache_map_read(struct dm_writecache *wc, struct bio *bio)\n{\n\tenum wc_map_op map_op;\n\tstruct wc_entry *e;\n\nread_next_block:\n\twc->stats.reads++;\n\te = writecache_find_entry(wc, bio->bi_iter.bi_sector, WFE_RETURN_FOLLOWING);\n\tif (e && read_original_sector(wc, e) == bio->bi_iter.bi_sector) {\n\t\twc->stats.read_hits++;\n\t\tif (WC_MODE_PMEM(wc)) {\n\t\t\tbio_copy_block(wc, bio, memory_data(wc, e));\n\t\t\tif (bio->bi_iter.bi_size)\n\t\t\t\tgoto read_next_block;\n\t\t\tmap_op = WC_MAP_SUBMIT;\n\t\t} else {\n\t\t\tdm_accept_partial_bio(bio, wc->block_size >> SECTOR_SHIFT);\n\t\t\tbio_set_dev(bio, wc->ssd_dev->bdev);\n\t\t\tbio->bi_iter.bi_sector = cache_sector(wc, e);\n\t\t\tif (!writecache_entry_is_committed(wc, e))\n\t\t\t\twritecache_wait_for_ios(wc, WRITE);\n\t\t\tmap_op = WC_MAP_REMAP;\n\t\t}\n\t} else {\n\t\twritecache_map_remap_origin(wc, bio, e);\n\t\twc->stats.reads += (bio->bi_iter.bi_size - wc->block_size) >> wc->block_size_bits;\n\t\tmap_op = WC_MAP_REMAP_ORIGIN;\n\t}\n\n\treturn map_op;\n}\n\nstatic void writecache_bio_copy_ssd(struct dm_writecache *wc, struct bio *bio,\n\t\t\t\t    struct wc_entry *e, bool search_used)\n{\n\tunsigned int bio_size = wc->block_size;\n\tsector_t start_cache_sec = cache_sector(wc, e);\n\tsector_t current_cache_sec = start_cache_sec + (bio_size >> SECTOR_SHIFT);\n\n\twhile (bio_size < bio->bi_iter.bi_size) {\n\t\tif (!search_used) {\n\t\t\tstruct wc_entry *f = writecache_pop_from_freelist(wc, current_cache_sec);\n\n\t\t\tif (!f)\n\t\t\t\tbreak;\n\t\t\twrite_original_sector_seq_count(wc, f, bio->bi_iter.bi_sector +\n\t\t\t\t\t\t\t(bio_size >> SECTOR_SHIFT), wc->seq_count);\n\t\t\twritecache_insert_entry(wc, f);\n\t\t\twc->uncommitted_blocks++;\n\t\t} else {\n\t\t\tstruct wc_entry *f;\n\t\t\tstruct rb_node *next = rb_next(&e->rb_node);\n\n\t\t\tif (!next)\n\t\t\t\tbreak;\n\t\t\tf = container_of(next, struct wc_entry, rb_node);\n\t\t\tif (f != e + 1)\n\t\t\t\tbreak;\n\t\t\tif (read_original_sector(wc, f) !=\n\t\t\t    read_original_sector(wc, e) + (wc->block_size >> SECTOR_SHIFT))\n\t\t\t\tbreak;\n\t\t\tif (unlikely(f->write_in_progress))\n\t\t\t\tbreak;\n\t\t\tif (writecache_entry_is_committed(wc, f))\n\t\t\t\twc->overwrote_committed = true;\n\t\t\te = f;\n\t\t}\n\t\tbio_size += wc->block_size;\n\t\tcurrent_cache_sec += wc->block_size >> SECTOR_SHIFT;\n\t}\n\n\tbio_set_dev(bio, wc->ssd_dev->bdev);\n\tbio->bi_iter.bi_sector = start_cache_sec;\n\tdm_accept_partial_bio(bio, bio_size >> SECTOR_SHIFT);\n\n\twc->stats.writes += bio->bi_iter.bi_size >> wc->block_size_bits;\n\twc->stats.writes_allocate += (bio->bi_iter.bi_size - wc->block_size) >> wc->block_size_bits;\n\n\tif (unlikely(wc->uncommitted_blocks >= wc->autocommit_blocks)) {\n\t\twc->uncommitted_blocks = 0;\n\t\tqueue_work(wc->writeback_wq, &wc->flush_work);\n\t} else {\n\t\twritecache_schedule_autocommit(wc);\n\t}\n}\n\nstatic enum wc_map_op writecache_map_write(struct dm_writecache *wc, struct bio *bio)\n{\n\tstruct wc_entry *e;\n\n\tdo {\n\t\tbool found_entry = false;\n\t\tbool search_used = false;\n\n\t\tif (writecache_has_error(wc)) {\n\t\t\twc->stats.writes += bio->bi_iter.bi_size >> wc->block_size_bits;\n\t\t\treturn WC_MAP_ERROR;\n\t\t}\n\t\te = writecache_find_entry(wc, bio->bi_iter.bi_sector, 0);\n\t\tif (e) {\n\t\t\tif (!writecache_entry_is_committed(wc, e)) {\n\t\t\t\twc->stats.write_hits_uncommitted++;\n\t\t\t\tsearch_used = true;\n\t\t\t\tgoto bio_copy;\n\t\t\t}\n\t\t\twc->stats.write_hits_committed++;\n\t\t\tif (!WC_MODE_PMEM(wc) && !e->write_in_progress) {\n\t\t\t\twc->overwrote_committed = true;\n\t\t\t\tsearch_used = true;\n\t\t\t\tgoto bio_copy;\n\t\t\t}\n\t\t\tfound_entry = true;\n\t\t} else {\n\t\t\tif (unlikely(wc->cleaner) ||\n\t\t\t    (wc->metadata_only && !(bio->bi_opf & REQ_META)))\n\t\t\t\tgoto direct_write;\n\t\t}\n\t\te = writecache_pop_from_freelist(wc, (sector_t)-1);\n\t\tif (unlikely(!e)) {\n\t\t\tif (!WC_MODE_PMEM(wc) && !found_entry) {\ndirect_write:\n\t\t\t\te = writecache_find_entry(wc, bio->bi_iter.bi_sector, WFE_RETURN_FOLLOWING);\n\t\t\t\twritecache_map_remap_origin(wc, bio, e);\n\t\t\t\twc->stats.writes_around += bio->bi_iter.bi_size >> wc->block_size_bits;\n\t\t\t\twc->stats.writes += bio->bi_iter.bi_size >> wc->block_size_bits;\n\t\t\t\treturn WC_MAP_REMAP_ORIGIN;\n\t\t\t}\n\t\t\twc->stats.writes_blocked_on_freelist++;\n\t\t\twritecache_wait_on_freelist(wc);\n\t\t\tcontinue;\n\t\t}\n\t\twrite_original_sector_seq_count(wc, e, bio->bi_iter.bi_sector, wc->seq_count);\n\t\twritecache_insert_entry(wc, e);\n\t\twc->uncommitted_blocks++;\n\t\twc->stats.writes_allocate++;\nbio_copy:\n\t\tif (WC_MODE_PMEM(wc)) {\n\t\t\tbio_copy_block(wc, bio, memory_data(wc, e));\n\t\t\twc->stats.writes++;\n\t\t} else {\n\t\t\twritecache_bio_copy_ssd(wc, bio, e, search_used);\n\t\t\treturn WC_MAP_REMAP;\n\t\t}\n\t} while (bio->bi_iter.bi_size);\n\n\tif (unlikely(bio->bi_opf & REQ_FUA || wc->uncommitted_blocks >= wc->autocommit_blocks))\n\t\twritecache_flush(wc);\n\telse\n\t\twritecache_schedule_autocommit(wc);\n\n\treturn WC_MAP_SUBMIT;\n}\n\nstatic enum wc_map_op writecache_map_flush(struct dm_writecache *wc, struct bio *bio)\n{\n\tif (writecache_has_error(wc))\n\t\treturn WC_MAP_ERROR;\n\n\tif (WC_MODE_PMEM(wc)) {\n\t\twc->stats.flushes++;\n\t\twritecache_flush(wc);\n\t\tif (writecache_has_error(wc))\n\t\t\treturn WC_MAP_ERROR;\n\t\telse if (unlikely(wc->cleaner) || unlikely(wc->metadata_only))\n\t\t\treturn WC_MAP_REMAP_ORIGIN;\n\t\treturn WC_MAP_SUBMIT;\n\t}\n\t \n\tif (dm_bio_get_target_bio_nr(bio))\n\t\treturn WC_MAP_REMAP_ORIGIN;\n\twc->stats.flushes++;\n\twritecache_offload_bio(wc, bio);\n\treturn WC_MAP_RETURN;\n}\n\nstatic enum wc_map_op writecache_map_discard(struct dm_writecache *wc, struct bio *bio)\n{\n\twc->stats.discards += bio->bi_iter.bi_size >> wc->block_size_bits;\n\n\tif (writecache_has_error(wc))\n\t\treturn WC_MAP_ERROR;\n\n\tif (WC_MODE_PMEM(wc)) {\n\t\twritecache_discard(wc, bio->bi_iter.bi_sector, bio_end_sector(bio));\n\t\treturn WC_MAP_REMAP_ORIGIN;\n\t}\n\t \n\twritecache_offload_bio(wc, bio);\n\treturn WC_MAP_RETURN;\n}\n\nstatic int writecache_map(struct dm_target *ti, struct bio *bio)\n{\n\tstruct dm_writecache *wc = ti->private;\n\tenum wc_map_op map_op;\n\n\tbio->bi_private = NULL;\n\n\twc_lock(wc);\n\n\tif (unlikely(bio->bi_opf & REQ_PREFLUSH)) {\n\t\tmap_op = writecache_map_flush(wc, bio);\n\t\tgoto done;\n\t}\n\n\tbio->bi_iter.bi_sector = dm_target_offset(ti, bio->bi_iter.bi_sector);\n\n\tif (unlikely((((unsigned int)bio->bi_iter.bi_sector | bio_sectors(bio)) &\n\t\t\t\t(wc->block_size / 512 - 1)) != 0)) {\n\t\tDMERR(\"I/O is not aligned, sector %llu, size %u, block size %u\",\n\t\t      (unsigned long long)bio->bi_iter.bi_sector,\n\t\t      bio->bi_iter.bi_size, wc->block_size);\n\t\tmap_op = WC_MAP_ERROR;\n\t\tgoto done;\n\t}\n\n\tif (unlikely(bio_op(bio) == REQ_OP_DISCARD)) {\n\t\tmap_op = writecache_map_discard(wc, bio);\n\t\tgoto done;\n\t}\n\n\tif (bio_data_dir(bio) == READ)\n\t\tmap_op = writecache_map_read(wc, bio);\n\telse\n\t\tmap_op = writecache_map_write(wc, bio);\ndone:\n\tswitch (map_op) {\n\tcase WC_MAP_REMAP_ORIGIN:\n\t\tif (likely(wc->pause != 0)) {\n\t\t\tif (bio_op(bio) == REQ_OP_WRITE) {\n\t\t\t\tdm_iot_io_begin(&wc->iot, 1);\n\t\t\t\tbio->bi_private = (void *)2;\n\t\t\t}\n\t\t}\n\t\tbio_set_dev(bio, wc->dev->bdev);\n\t\twc_unlock(wc);\n\t\treturn DM_MAPIO_REMAPPED;\n\n\tcase WC_MAP_REMAP:\n\t\t \n\t\tbio->bi_private = (void *)1;\n\t\tatomic_inc(&wc->bio_in_progress[bio_data_dir(bio)]);\n\t\twc_unlock(wc);\n\t\treturn DM_MAPIO_REMAPPED;\n\n\tcase WC_MAP_SUBMIT:\n\t\twc_unlock(wc);\n\t\tbio_endio(bio);\n\t\treturn DM_MAPIO_SUBMITTED;\n\n\tcase WC_MAP_RETURN:\n\t\twc_unlock(wc);\n\t\treturn DM_MAPIO_SUBMITTED;\n\n\tcase WC_MAP_ERROR:\n\t\twc_unlock(wc);\n\t\tbio_io_error(bio);\n\t\treturn DM_MAPIO_SUBMITTED;\n\n\tdefault:\n\t\tBUG();\n\t\twc_unlock(wc);\n\t\treturn DM_MAPIO_KILL;\n\t}\n}\n\nstatic int writecache_end_io(struct dm_target *ti, struct bio *bio, blk_status_t *status)\n{\n\tstruct dm_writecache *wc = ti->private;\n\n\tif (bio->bi_private == (void *)1) {\n\t\tint dir = bio_data_dir(bio);\n\n\t\tif (atomic_dec_and_test(&wc->bio_in_progress[dir]))\n\t\t\tif (unlikely(waitqueue_active(&wc->bio_in_progress_wait[dir])))\n\t\t\t\twake_up(&wc->bio_in_progress_wait[dir]);\n\t} else if (bio->bi_private == (void *)2) {\n\t\tdm_iot_io_end(&wc->iot, 1);\n\t}\n\treturn 0;\n}\n\nstatic int writecache_iterate_devices(struct dm_target *ti,\n\t\t\t\t      iterate_devices_callout_fn fn, void *data)\n{\n\tstruct dm_writecache *wc = ti->private;\n\n\treturn fn(ti, wc->dev, 0, ti->len, data);\n}\n\nstatic void writecache_io_hints(struct dm_target *ti, struct queue_limits *limits)\n{\n\tstruct dm_writecache *wc = ti->private;\n\n\tif (limits->logical_block_size < wc->block_size)\n\t\tlimits->logical_block_size = wc->block_size;\n\n\tif (limits->physical_block_size < wc->block_size)\n\t\tlimits->physical_block_size = wc->block_size;\n\n\tif (limits->io_min < wc->block_size)\n\t\tlimits->io_min = wc->block_size;\n}\n\n\nstatic void writecache_writeback_endio(struct bio *bio)\n{\n\tstruct writeback_struct *wb = container_of(bio, struct writeback_struct, bio);\n\tstruct dm_writecache *wc = wb->wc;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&wc->endio_list_lock, flags);\n\tif (unlikely(list_empty(&wc->endio_list)))\n\t\twake_up_process(wc->endio_thread);\n\tlist_add_tail(&wb->endio_entry, &wc->endio_list);\n\traw_spin_unlock_irqrestore(&wc->endio_list_lock, flags);\n}\n\nstatic void writecache_copy_endio(int read_err, unsigned long write_err, void *ptr)\n{\n\tstruct copy_struct *c = ptr;\n\tstruct dm_writecache *wc = c->wc;\n\n\tc->error = likely(!(read_err | write_err)) ? 0 : -EIO;\n\n\traw_spin_lock_irq(&wc->endio_list_lock);\n\tif (unlikely(list_empty(&wc->endio_list)))\n\t\twake_up_process(wc->endio_thread);\n\tlist_add_tail(&c->endio_entry, &wc->endio_list);\n\traw_spin_unlock_irq(&wc->endio_list_lock);\n}\n\nstatic void __writecache_endio_pmem(struct dm_writecache *wc, struct list_head *list)\n{\n\tunsigned int i;\n\tstruct writeback_struct *wb;\n\tstruct wc_entry *e;\n\tunsigned long n_walked = 0;\n\n\tdo {\n\t\twb = list_entry(list->next, struct writeback_struct, endio_entry);\n\t\tlist_del(&wb->endio_entry);\n\n\t\tif (unlikely(wb->bio.bi_status != BLK_STS_OK))\n\t\t\twritecache_error(wc, blk_status_to_errno(wb->bio.bi_status),\n\t\t\t\t\t\"write error %d\", wb->bio.bi_status);\n\t\ti = 0;\n\t\tdo {\n\t\t\te = wb->wc_list[i];\n\t\t\tBUG_ON(!e->write_in_progress);\n\t\t\te->write_in_progress = false;\n\t\t\tINIT_LIST_HEAD(&e->lru);\n\t\t\tif (!writecache_has_error(wc))\n\t\t\t\twritecache_free_entry(wc, e);\n\t\t\tBUG_ON(!wc->writeback_size);\n\t\t\twc->writeback_size--;\n\t\t\tn_walked++;\n\t\t\tif (unlikely(n_walked >= ENDIO_LATENCY)) {\n\t\t\t\twritecache_commit_flushed(wc, false);\n\t\t\t\twc_unlock(wc);\n\t\t\t\twc_lock(wc);\n\t\t\t\tn_walked = 0;\n\t\t\t}\n\t\t} while (++i < wb->wc_list_n);\n\n\t\tif (wb->wc_list != wb->wc_list_inline)\n\t\t\tkfree(wb->wc_list);\n\t\tbio_put(&wb->bio);\n\t} while (!list_empty(list));\n}\n\nstatic void __writecache_endio_ssd(struct dm_writecache *wc, struct list_head *list)\n{\n\tstruct copy_struct *c;\n\tstruct wc_entry *e;\n\n\tdo {\n\t\tc = list_entry(list->next, struct copy_struct, endio_entry);\n\t\tlist_del(&c->endio_entry);\n\n\t\tif (unlikely(c->error))\n\t\t\twritecache_error(wc, c->error, \"copy error\");\n\n\t\te = c->e;\n\t\tdo {\n\t\t\tBUG_ON(!e->write_in_progress);\n\t\t\te->write_in_progress = false;\n\t\t\tINIT_LIST_HEAD(&e->lru);\n\t\t\tif (!writecache_has_error(wc))\n\t\t\t\twritecache_free_entry(wc, e);\n\n\t\t\tBUG_ON(!wc->writeback_size);\n\t\t\twc->writeback_size--;\n\t\t\te++;\n\t\t} while (--c->n_entries);\n\t\tmempool_free(c, &wc->copy_pool);\n\t} while (!list_empty(list));\n}\n\nstatic int writecache_endio_thread(void *data)\n{\n\tstruct dm_writecache *wc = data;\n\n\twhile (1) {\n\t\tstruct list_head list;\n\n\t\traw_spin_lock_irq(&wc->endio_list_lock);\n\t\tif (!list_empty(&wc->endio_list))\n\t\t\tgoto pop_from_list;\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\traw_spin_unlock_irq(&wc->endio_list_lock);\n\n\t\tif (unlikely(kthread_should_stop())) {\n\t\t\tset_current_state(TASK_RUNNING);\n\t\t\tbreak;\n\t\t}\n\n\t\tschedule();\n\n\t\tcontinue;\n\npop_from_list:\n\t\tlist = wc->endio_list;\n\t\tlist.next->prev = list.prev->next = &list;\n\t\tINIT_LIST_HEAD(&wc->endio_list);\n\t\traw_spin_unlock_irq(&wc->endio_list_lock);\n\n\t\tif (!WC_MODE_FUA(wc))\n\t\t\twritecache_disk_flush(wc, wc->dev);\n\n\t\twc_lock(wc);\n\n\t\tif (WC_MODE_PMEM(wc)) {\n\t\t\t__writecache_endio_pmem(wc, &list);\n\t\t} else {\n\t\t\t__writecache_endio_ssd(wc, &list);\n\t\t\twritecache_wait_for_ios(wc, READ);\n\t\t}\n\n\t\twritecache_commit_flushed(wc, false);\n\n\t\twc_unlock(wc);\n\t}\n\n\treturn 0;\n}\n\nstatic bool wc_add_block(struct writeback_struct *wb, struct wc_entry *e)\n{\n\tstruct dm_writecache *wc = wb->wc;\n\tunsigned int block_size = wc->block_size;\n\tvoid *address = memory_data(wc, e);\n\n\tpersistent_memory_flush_cache(address, block_size);\n\n\tif (unlikely(bio_end_sector(&wb->bio) >= wc->data_device_sectors))\n\t\treturn true;\n\n\treturn bio_add_page(&wb->bio, persistent_memory_page(address),\n\t\t\t    block_size, persistent_memory_page_offset(address)) != 0;\n}\n\nstruct writeback_list {\n\tstruct list_head list;\n\tsize_t size;\n};\n\nstatic void __writeback_throttle(struct dm_writecache *wc, struct writeback_list *wbl)\n{\n\tif (unlikely(wc->max_writeback_jobs)) {\n\t\tif (READ_ONCE(wc->writeback_size) - wbl->size >= wc->max_writeback_jobs) {\n\t\t\twc_lock(wc);\n\t\t\twhile (wc->writeback_size - wbl->size >= wc->max_writeback_jobs)\n\t\t\t\twritecache_wait_on_freelist(wc);\n\t\t\twc_unlock(wc);\n\t\t}\n\t}\n\tcond_resched();\n}\n\nstatic void __writecache_writeback_pmem(struct dm_writecache *wc, struct writeback_list *wbl)\n{\n\tstruct wc_entry *e, *f;\n\tstruct bio *bio;\n\tstruct writeback_struct *wb;\n\tunsigned int max_pages;\n\n\twhile (wbl->size) {\n\t\twbl->size--;\n\t\te = container_of(wbl->list.prev, struct wc_entry, lru);\n\t\tlist_del(&e->lru);\n\n\t\tmax_pages = e->wc_list_contiguous;\n\n\t\tbio = bio_alloc_bioset(wc->dev->bdev, max_pages, REQ_OP_WRITE,\n\t\t\t\t       GFP_NOIO, &wc->bio_set);\n\t\twb = container_of(bio, struct writeback_struct, bio);\n\t\twb->wc = wc;\n\t\tbio->bi_end_io = writecache_writeback_endio;\n\t\tbio->bi_iter.bi_sector = read_original_sector(wc, e);\n\n\t\tif (unlikely(max_pages > WB_LIST_INLINE))\n\t\t\twb->wc_list = kmalloc_array(max_pages, sizeof(struct wc_entry *),\n\t\t\t\t\t\t    GFP_NOIO | __GFP_NORETRY |\n\t\t\t\t\t\t    __GFP_NOMEMALLOC | __GFP_NOWARN);\n\n\t\tif (likely(max_pages <= WB_LIST_INLINE) || unlikely(!wb->wc_list)) {\n\t\t\twb->wc_list = wb->wc_list_inline;\n\t\t\tmax_pages = WB_LIST_INLINE;\n\t\t}\n\n\t\tBUG_ON(!wc_add_block(wb, e));\n\n\t\twb->wc_list[0] = e;\n\t\twb->wc_list_n = 1;\n\n\t\twhile (wbl->size && wb->wc_list_n < max_pages) {\n\t\t\tf = container_of(wbl->list.prev, struct wc_entry, lru);\n\t\t\tif (read_original_sector(wc, f) !=\n\t\t\t    read_original_sector(wc, e) + (wc->block_size >> SECTOR_SHIFT))\n\t\t\t\tbreak;\n\t\t\tif (!wc_add_block(wb, f))\n\t\t\t\tbreak;\n\t\t\twbl->size--;\n\t\t\tlist_del(&f->lru);\n\t\t\twb->wc_list[wb->wc_list_n++] = f;\n\t\t\te = f;\n\t\t}\n\t\tif (WC_MODE_FUA(wc))\n\t\t\tbio->bi_opf |= REQ_FUA;\n\t\tif (writecache_has_error(wc)) {\n\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\t\tbio_endio(bio);\n\t\t} else if (unlikely(!bio_sectors(bio))) {\n\t\t\tbio->bi_status = BLK_STS_OK;\n\t\t\tbio_endio(bio);\n\t\t} else {\n\t\t\tsubmit_bio(bio);\n\t\t}\n\n\t\t__writeback_throttle(wc, wbl);\n\t}\n}\n\nstatic void __writecache_writeback_ssd(struct dm_writecache *wc, struct writeback_list *wbl)\n{\n\tstruct wc_entry *e, *f;\n\tstruct dm_io_region from, to;\n\tstruct copy_struct *c;\n\n\twhile (wbl->size) {\n\t\tunsigned int n_sectors;\n\n\t\twbl->size--;\n\t\te = container_of(wbl->list.prev, struct wc_entry, lru);\n\t\tlist_del(&e->lru);\n\n\t\tn_sectors = e->wc_list_contiguous << (wc->block_size_bits - SECTOR_SHIFT);\n\n\t\tfrom.bdev = wc->ssd_dev->bdev;\n\t\tfrom.sector = cache_sector(wc, e);\n\t\tfrom.count = n_sectors;\n\t\tto.bdev = wc->dev->bdev;\n\t\tto.sector = read_original_sector(wc, e);\n\t\tto.count = n_sectors;\n\n\t\tc = mempool_alloc(&wc->copy_pool, GFP_NOIO);\n\t\tc->wc = wc;\n\t\tc->e = e;\n\t\tc->n_entries = e->wc_list_contiguous;\n\n\t\twhile ((n_sectors -= wc->block_size >> SECTOR_SHIFT)) {\n\t\t\twbl->size--;\n\t\t\tf = container_of(wbl->list.prev, struct wc_entry, lru);\n\t\t\tBUG_ON(f != e + 1);\n\t\t\tlist_del(&f->lru);\n\t\t\te = f;\n\t\t}\n\n\t\tif (unlikely(to.sector + to.count > wc->data_device_sectors)) {\n\t\t\tif (to.sector >= wc->data_device_sectors) {\n\t\t\t\twritecache_copy_endio(0, 0, c);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tfrom.count = to.count = wc->data_device_sectors - to.sector;\n\t\t}\n\n\t\tdm_kcopyd_copy(wc->dm_kcopyd, &from, 1, &to, 0, writecache_copy_endio, c);\n\n\t\t__writeback_throttle(wc, wbl);\n\t}\n}\n\nstatic void writecache_writeback(struct work_struct *work)\n{\n\tstruct dm_writecache *wc = container_of(work, struct dm_writecache, writeback_work);\n\tstruct blk_plug plug;\n\tstruct wc_entry *f, *g, *e = NULL;\n\tstruct rb_node *node, *next_node;\n\tstruct list_head skipped;\n\tstruct writeback_list wbl;\n\tunsigned long n_walked;\n\n\tif (!WC_MODE_PMEM(wc)) {\n\t\t \n\t\tdm_kcopyd_client_flush(wc->dm_kcopyd);\n\t}\n\n\tif (likely(wc->pause != 0)) {\n\t\twhile (1) {\n\t\t\tunsigned long idle;\n\n\t\t\tif (unlikely(wc->cleaner) || unlikely(wc->writeback_all) ||\n\t\t\t    unlikely(dm_suspended(wc->ti)))\n\t\t\t\tbreak;\n\t\t\tidle = dm_iot_idle_time(&wc->iot);\n\t\t\tif (idle >= wc->pause)\n\t\t\t\tbreak;\n\t\t\tidle = wc->pause - idle;\n\t\t\tif (idle > HZ)\n\t\t\t\tidle = HZ;\n\t\t\tschedule_timeout_idle(idle);\n\t\t}\n\t}\n\n\twc_lock(wc);\nrestart:\n\tif (writecache_has_error(wc)) {\n\t\twc_unlock(wc);\n\t\treturn;\n\t}\n\n\tif (unlikely(wc->writeback_all)) {\n\t\tif (writecache_wait_for_writeback(wc))\n\t\t\tgoto restart;\n\t}\n\n\tif (wc->overwrote_committed)\n\t\twritecache_wait_for_ios(wc, WRITE);\n\n\tn_walked = 0;\n\tINIT_LIST_HEAD(&skipped);\n\tINIT_LIST_HEAD(&wbl.list);\n\twbl.size = 0;\n\twhile (!list_empty(&wc->lru) &&\n\t       (wc->writeback_all ||\n\t\twc->freelist_size + wc->writeback_size <= wc->freelist_low_watermark ||\n\t\t(jiffies - container_of(wc->lru.prev, struct wc_entry, lru)->age >=\n\t\t wc->max_age - wc->max_age / MAX_AGE_DIV))) {\n\n\t\tn_walked++;\n\t\tif (unlikely(n_walked > WRITEBACK_LATENCY) &&\n\t\t    likely(!wc->writeback_all)) {\n\t\t\tif (likely(!dm_suspended(wc->ti)))\n\t\t\t\tqueue_work(wc->writeback_wq, &wc->writeback_work);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (unlikely(wc->writeback_all)) {\n\t\t\tif (unlikely(!e)) {\n\t\t\t\twritecache_flush(wc);\n\t\t\t\te = container_of(rb_first(&wc->tree), struct wc_entry, rb_node);\n\t\t\t} else\n\t\t\t\te = g;\n\t\t} else\n\t\t\te = container_of(wc->lru.prev, struct wc_entry, lru);\n\t\tBUG_ON(e->write_in_progress);\n\t\tif (unlikely(!writecache_entry_is_committed(wc, e)))\n\t\t\twritecache_flush(wc);\n\n\t\tnode = rb_prev(&e->rb_node);\n\t\tif (node) {\n\t\t\tf = container_of(node, struct wc_entry, rb_node);\n\t\t\tif (unlikely(read_original_sector(wc, f) ==\n\t\t\t\t     read_original_sector(wc, e))) {\n\t\t\t\tBUG_ON(!f->write_in_progress);\n\t\t\t\tlist_move(&e->lru, &skipped);\n\t\t\t\tcond_resched();\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\twc->writeback_size++;\n\t\tlist_move(&e->lru, &wbl.list);\n\t\twbl.size++;\n\t\te->write_in_progress = true;\n\t\te->wc_list_contiguous = 1;\n\n\t\tf = e;\n\n\t\twhile (1) {\n\t\t\tnext_node = rb_next(&f->rb_node);\n\t\t\tif (unlikely(!next_node))\n\t\t\t\tbreak;\n\t\t\tg = container_of(next_node, struct wc_entry, rb_node);\n\t\t\tif (unlikely(read_original_sector(wc, g) ==\n\t\t\t    read_original_sector(wc, f))) {\n\t\t\t\tf = g;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (read_original_sector(wc, g) !=\n\t\t\t    read_original_sector(wc, f) + (wc->block_size >> SECTOR_SHIFT))\n\t\t\t\tbreak;\n\t\t\tif (unlikely(g->write_in_progress))\n\t\t\t\tbreak;\n\t\t\tif (unlikely(!writecache_entry_is_committed(wc, g)))\n\t\t\t\tbreak;\n\n\t\t\tif (!WC_MODE_PMEM(wc)) {\n\t\t\t\tif (g != f + 1)\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tn_walked++;\n\t\t\t\n\t\t\t\n\n\t\t\twc->writeback_size++;\n\t\t\tlist_move(&g->lru, &wbl.list);\n\t\t\twbl.size++;\n\t\t\tg->write_in_progress = true;\n\t\t\tg->wc_list_contiguous = BIO_MAX_VECS;\n\t\t\tf = g;\n\t\t\te->wc_list_contiguous++;\n\t\t\tif (unlikely(e->wc_list_contiguous == BIO_MAX_VECS)) {\n\t\t\t\tif (unlikely(wc->writeback_all)) {\n\t\t\t\t\tnext_node = rb_next(&f->rb_node);\n\t\t\t\t\tif (likely(next_node))\n\t\t\t\t\t\tg = container_of(next_node, struct wc_entry, rb_node);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tcond_resched();\n\t}\n\n\tif (!list_empty(&skipped)) {\n\t\tlist_splice_tail(&skipped, &wc->lru);\n\t\t \n\t\tif (unlikely(!wbl.size))\n\t\t\twritecache_wait_for_writeback(wc);\n\t}\n\n\twc_unlock(wc);\n\n\tblk_start_plug(&plug);\n\n\tif (WC_MODE_PMEM(wc))\n\t\t__writecache_writeback_pmem(wc, &wbl);\n\telse\n\t\t__writecache_writeback_ssd(wc, &wbl);\n\n\tblk_finish_plug(&plug);\n\n\tif (unlikely(wc->writeback_all)) {\n\t\twc_lock(wc);\n\t\twhile (writecache_wait_for_writeback(wc))\n\t\t\t;\n\t\twc_unlock(wc);\n\t}\n}\n\nstatic int calculate_memory_size(uint64_t device_size, unsigned int block_size,\n\t\t\t\t size_t *n_blocks_p, size_t *n_metadata_blocks_p)\n{\n\tuint64_t n_blocks, offset;\n\tstruct wc_entry e;\n\n\tn_blocks = device_size;\n\tdo_div(n_blocks, block_size + sizeof(struct wc_memory_entry));\n\n\twhile (1) {\n\t\tif (!n_blocks)\n\t\t\treturn -ENOSPC;\n\t\t \n\t\tif (n_blocks >= ((size_t)-sizeof(struct wc_memory_superblock) /\n\t\t\t\t sizeof(struct wc_memory_entry)))\n\t\t\treturn -EFBIG;\n\t\toffset = offsetof(struct wc_memory_superblock, entries[n_blocks]);\n\t\toffset = (offset + block_size - 1) & ~(uint64_t)(block_size - 1);\n\t\tif (offset + n_blocks * block_size <= device_size)\n\t\t\tbreak;\n\t\tn_blocks--;\n\t}\n\n\t \n\te.index = n_blocks;\n\tif (e.index != n_blocks)\n\t\treturn -EFBIG;\n\n\tif (n_blocks_p)\n\t\t*n_blocks_p = n_blocks;\n\tif (n_metadata_blocks_p)\n\t\t*n_metadata_blocks_p = offset >> __ffs(block_size);\n\treturn 0;\n}\n\nstatic int init_memory(struct dm_writecache *wc)\n{\n\tsize_t b;\n\tint r;\n\n\tr = calculate_memory_size(wc->memory_map_size, wc->block_size, &wc->n_blocks, NULL);\n\tif (r)\n\t\treturn r;\n\n\tr = writecache_alloc_entries(wc);\n\tif (r)\n\t\treturn r;\n\n\tfor (b = 0; b < ARRAY_SIZE(sb(wc)->padding); b++)\n\t\tpmem_assign(sb(wc)->padding[b], cpu_to_le64(0));\n\tpmem_assign(sb(wc)->version, cpu_to_le32(MEMORY_SUPERBLOCK_VERSION));\n\tpmem_assign(sb(wc)->block_size, cpu_to_le32(wc->block_size));\n\tpmem_assign(sb(wc)->n_blocks, cpu_to_le64(wc->n_blocks));\n\tpmem_assign(sb(wc)->seq_count, cpu_to_le64(0));\n\n\tfor (b = 0; b < wc->n_blocks; b++) {\n\t\twrite_original_sector_seq_count(wc, &wc->entries[b], -1, -1);\n\t\tcond_resched();\n\t}\n\n\twritecache_flush_all_metadata(wc);\n\twritecache_commit_flushed(wc, false);\n\tpmem_assign(sb(wc)->magic, cpu_to_le32(MEMORY_SUPERBLOCK_MAGIC));\n\twritecache_flush_region(wc, &sb(wc)->magic, sizeof(sb(wc)->magic));\n\twritecache_commit_flushed(wc, false);\n\n\treturn 0;\n}\n\nstatic void writecache_dtr(struct dm_target *ti)\n{\n\tstruct dm_writecache *wc = ti->private;\n\n\tif (!wc)\n\t\treturn;\n\n\tif (wc->endio_thread)\n\t\tkthread_stop(wc->endio_thread);\n\n\tif (wc->flush_thread)\n\t\tkthread_stop(wc->flush_thread);\n\n\tbioset_exit(&wc->bio_set);\n\n\tmempool_exit(&wc->copy_pool);\n\n\tif (wc->writeback_wq)\n\t\tdestroy_workqueue(wc->writeback_wq);\n\n\tif (wc->dev)\n\t\tdm_put_device(ti, wc->dev);\n\n\tif (wc->ssd_dev)\n\t\tdm_put_device(ti, wc->ssd_dev);\n\n\tvfree(wc->entries);\n\n\tif (wc->memory_map) {\n\t\tif (WC_MODE_PMEM(wc))\n\t\t\tpersistent_memory_release(wc);\n\t\telse\n\t\t\tvfree(wc->memory_map);\n\t}\n\n\tif (wc->dm_kcopyd)\n\t\tdm_kcopyd_client_destroy(wc->dm_kcopyd);\n\n\tif (wc->dm_io)\n\t\tdm_io_client_destroy(wc->dm_io);\n\n\tvfree(wc->dirty_bitmap);\n\n\tkfree(wc);\n}\n\nstatic int writecache_ctr(struct dm_target *ti, unsigned int argc, char **argv)\n{\n\tstruct dm_writecache *wc;\n\tstruct dm_arg_set as;\n\tconst char *string;\n\tunsigned int opt_params;\n\tsize_t offset, data_size;\n\tint i, r;\n\tchar dummy;\n\tint high_wm_percent = HIGH_WATERMARK;\n\tint low_wm_percent = LOW_WATERMARK;\n\tuint64_t x;\n\tstruct wc_memory_superblock s;\n\n\tstatic struct dm_arg _args[] = {\n\t\t{0, 18, \"Invalid number of feature args\"},\n\t};\n\n\tas.argc = argc;\n\tas.argv = argv;\n\n\twc = kzalloc(sizeof(struct dm_writecache), GFP_KERNEL);\n\tif (!wc) {\n\t\tti->error = \"Cannot allocate writecache structure\";\n\t\tr = -ENOMEM;\n\t\tgoto bad;\n\t}\n\tti->private = wc;\n\twc->ti = ti;\n\n\tmutex_init(&wc->lock);\n\twc->max_age = MAX_AGE_UNSPECIFIED;\n\twritecache_poison_lists(wc);\n\tinit_waitqueue_head(&wc->freelist_wait);\n\ttimer_setup(&wc->autocommit_timer, writecache_autocommit_timer, 0);\n\ttimer_setup(&wc->max_age_timer, writecache_max_age_timer, 0);\n\n\tfor (i = 0; i < 2; i++) {\n\t\tatomic_set(&wc->bio_in_progress[i], 0);\n\t\tinit_waitqueue_head(&wc->bio_in_progress_wait[i]);\n\t}\n\n\twc->dm_io = dm_io_client_create();\n\tif (IS_ERR(wc->dm_io)) {\n\t\tr = PTR_ERR(wc->dm_io);\n\t\tti->error = \"Unable to allocate dm-io client\";\n\t\twc->dm_io = NULL;\n\t\tgoto bad;\n\t}\n\n\twc->writeback_wq = alloc_workqueue(\"writecache-writeback\", WQ_MEM_RECLAIM, 1);\n\tif (!wc->writeback_wq) {\n\t\tr = -ENOMEM;\n\t\tti->error = \"Could not allocate writeback workqueue\";\n\t\tgoto bad;\n\t}\n\tINIT_WORK(&wc->writeback_work, writecache_writeback);\n\tINIT_WORK(&wc->flush_work, writecache_flush_work);\n\n\tdm_iot_init(&wc->iot);\n\n\traw_spin_lock_init(&wc->endio_list_lock);\n\tINIT_LIST_HEAD(&wc->endio_list);\n\twc->endio_thread = kthread_run(writecache_endio_thread, wc, \"writecache_endio\");\n\tif (IS_ERR(wc->endio_thread)) {\n\t\tr = PTR_ERR(wc->endio_thread);\n\t\twc->endio_thread = NULL;\n\t\tti->error = \"Couldn't spawn endio thread\";\n\t\tgoto bad;\n\t}\n\n\t \n\tstring = dm_shift_arg(&as);\n\tif (!string)\n\t\tgoto bad_arguments;\n\n\tif (!strcasecmp(string, \"s\")) {\n\t\twc->pmem_mode = false;\n\t} else if (!strcasecmp(string, \"p\")) {\n#ifdef DM_WRITECACHE_HAS_PMEM\n\t\twc->pmem_mode = true;\n\t\twc->writeback_fua = true;\n#else\n\t\t \n\t\tr = -EOPNOTSUPP;\n\t\tti->error = \"Persistent memory or DAX not supported on this system\";\n\t\tgoto bad;\n#endif\n\t} else {\n\t\tgoto bad_arguments;\n\t}\n\n\tif (WC_MODE_PMEM(wc)) {\n\t\tr = bioset_init(&wc->bio_set, BIO_POOL_SIZE,\n\t\t\t\toffsetof(struct writeback_struct, bio),\n\t\t\t\tBIOSET_NEED_BVECS);\n\t\tif (r) {\n\t\t\tti->error = \"Could not allocate bio set\";\n\t\t\tgoto bad;\n\t\t}\n\t} else {\n\t\twc->pause = PAUSE_WRITEBACK;\n\t\tr = mempool_init_kmalloc_pool(&wc->copy_pool, 1, sizeof(struct copy_struct));\n\t\tif (r) {\n\t\t\tti->error = \"Could not allocate mempool\";\n\t\t\tgoto bad;\n\t\t}\n\t}\n\n\t \n\tstring = dm_shift_arg(&as);\n\tif (!string)\n\t\tgoto bad_arguments;\n\tr = dm_get_device(ti, string, dm_table_get_mode(ti->table), &wc->dev);\n\tif (r) {\n\t\tti->error = \"Origin data device lookup failed\";\n\t\tgoto bad;\n\t}\n\n\t \n\tstring = dm_shift_arg(&as);\n\tif (!string)\n\t\tgoto bad_arguments;\n\n\tr = dm_get_device(ti, string, dm_table_get_mode(ti->table), &wc->ssd_dev);\n\tif (r) {\n\t\tti->error = \"Cache data device lookup failed\";\n\t\tgoto bad;\n\t}\n\twc->memory_map_size = bdev_nr_bytes(wc->ssd_dev->bdev);\n\n\t \n\tstring = dm_shift_arg(&as);\n\tif (!string)\n\t\tgoto bad_arguments;\n\tif (sscanf(string, \"%u%c\", &wc->block_size, &dummy) != 1 ||\n\t    wc->block_size < 512 || wc->block_size > PAGE_SIZE ||\n\t    (wc->block_size & (wc->block_size - 1))) {\n\t\tr = -EINVAL;\n\t\tti->error = \"Invalid block size\";\n\t\tgoto bad;\n\t}\n\tif (wc->block_size < bdev_logical_block_size(wc->dev->bdev) ||\n\t    wc->block_size < bdev_logical_block_size(wc->ssd_dev->bdev)) {\n\t\tr = -EINVAL;\n\t\tti->error = \"Block size is smaller than device logical block size\";\n\t\tgoto bad;\n\t}\n\twc->block_size_bits = __ffs(wc->block_size);\n\n\twc->max_writeback_jobs = MAX_WRITEBACK_JOBS;\n\twc->autocommit_blocks = !WC_MODE_PMEM(wc) ? AUTOCOMMIT_BLOCKS_SSD : AUTOCOMMIT_BLOCKS_PMEM;\n\twc->autocommit_jiffies = msecs_to_jiffies(AUTOCOMMIT_MSEC);\n\n\t \n\tr = dm_read_arg_group(_args, &as, &opt_params, &ti->error);\n\tif (r)\n\t\tgoto bad;\n\n\twhile (opt_params) {\n\t\tstring = dm_shift_arg(&as), opt_params--;\n\t\tif (!strcasecmp(string, \"start_sector\") && opt_params >= 1) {\n\t\t\tunsigned long long start_sector;\n\n\t\t\tstring = dm_shift_arg(&as), opt_params--;\n\t\t\tif (sscanf(string, \"%llu%c\", &start_sector, &dummy) != 1)\n\t\t\t\tgoto invalid_optional;\n\t\t\twc->start_sector = start_sector;\n\t\t\twc->start_sector_set = true;\n\t\t\tif (wc->start_sector != start_sector ||\n\t\t\t    wc->start_sector >= wc->memory_map_size >> SECTOR_SHIFT)\n\t\t\t\tgoto invalid_optional;\n\t\t} else if (!strcasecmp(string, \"high_watermark\") && opt_params >= 1) {\n\t\t\tstring = dm_shift_arg(&as), opt_params--;\n\t\t\tif (sscanf(string, \"%d%c\", &high_wm_percent, &dummy) != 1)\n\t\t\t\tgoto invalid_optional;\n\t\t\tif (high_wm_percent < 0 || high_wm_percent > 100)\n\t\t\t\tgoto invalid_optional;\n\t\t\twc->high_wm_percent_value = high_wm_percent;\n\t\t\twc->high_wm_percent_set = true;\n\t\t} else if (!strcasecmp(string, \"low_watermark\") && opt_params >= 1) {\n\t\t\tstring = dm_shift_arg(&as), opt_params--;\n\t\t\tif (sscanf(string, \"%d%c\", &low_wm_percent, &dummy) != 1)\n\t\t\t\tgoto invalid_optional;\n\t\t\tif (low_wm_percent < 0 || low_wm_percent > 100)\n\t\t\t\tgoto invalid_optional;\n\t\t\twc->low_wm_percent_value = low_wm_percent;\n\t\t\twc->low_wm_percent_set = true;\n\t\t} else if (!strcasecmp(string, \"writeback_jobs\") && opt_params >= 1) {\n\t\t\tstring = dm_shift_arg(&as), opt_params--;\n\t\t\tif (sscanf(string, \"%u%c\", &wc->max_writeback_jobs, &dummy) != 1)\n\t\t\t\tgoto invalid_optional;\n\t\t\twc->max_writeback_jobs_set = true;\n\t\t} else if (!strcasecmp(string, \"autocommit_blocks\") && opt_params >= 1) {\n\t\t\tstring = dm_shift_arg(&as), opt_params--;\n\t\t\tif (sscanf(string, \"%u%c\", &wc->autocommit_blocks, &dummy) != 1)\n\t\t\t\tgoto invalid_optional;\n\t\t\twc->autocommit_blocks_set = true;\n\t\t} else if (!strcasecmp(string, \"autocommit_time\") && opt_params >= 1) {\n\t\t\tunsigned int autocommit_msecs;\n\n\t\t\tstring = dm_shift_arg(&as), opt_params--;\n\t\t\tif (sscanf(string, \"%u%c\", &autocommit_msecs, &dummy) != 1)\n\t\t\t\tgoto invalid_optional;\n\t\t\tif (autocommit_msecs > 3600000)\n\t\t\t\tgoto invalid_optional;\n\t\t\twc->autocommit_jiffies = msecs_to_jiffies(autocommit_msecs);\n\t\t\twc->autocommit_time_value = autocommit_msecs;\n\t\t\twc->autocommit_time_set = true;\n\t\t} else if (!strcasecmp(string, \"max_age\") && opt_params >= 1) {\n\t\t\tunsigned int max_age_msecs;\n\n\t\t\tstring = dm_shift_arg(&as), opt_params--;\n\t\t\tif (sscanf(string, \"%u%c\", &max_age_msecs, &dummy) != 1)\n\t\t\t\tgoto invalid_optional;\n\t\t\tif (max_age_msecs > 86400000)\n\t\t\t\tgoto invalid_optional;\n\t\t\twc->max_age = msecs_to_jiffies(max_age_msecs);\n\t\t\twc->max_age_set = true;\n\t\t\twc->max_age_value = max_age_msecs;\n\t\t} else if (!strcasecmp(string, \"cleaner\")) {\n\t\t\twc->cleaner_set = true;\n\t\t\twc->cleaner = true;\n\t\t} else if (!strcasecmp(string, \"fua\")) {\n\t\t\tif (WC_MODE_PMEM(wc)) {\n\t\t\t\twc->writeback_fua = true;\n\t\t\t\twc->writeback_fua_set = true;\n\t\t\t} else\n\t\t\t\tgoto invalid_optional;\n\t\t} else if (!strcasecmp(string, \"nofua\")) {\n\t\t\tif (WC_MODE_PMEM(wc)) {\n\t\t\t\twc->writeback_fua = false;\n\t\t\t\twc->writeback_fua_set = true;\n\t\t\t} else\n\t\t\t\tgoto invalid_optional;\n\t\t} else if (!strcasecmp(string, \"metadata_only\")) {\n\t\t\twc->metadata_only = true;\n\t\t} else if (!strcasecmp(string, \"pause_writeback\") && opt_params >= 1) {\n\t\t\tunsigned int pause_msecs;\n\n\t\t\tif (WC_MODE_PMEM(wc))\n\t\t\t\tgoto invalid_optional;\n\t\t\tstring = dm_shift_arg(&as), opt_params--;\n\t\t\tif (sscanf(string, \"%u%c\", &pause_msecs, &dummy) != 1)\n\t\t\t\tgoto invalid_optional;\n\t\t\tif (pause_msecs > 60000)\n\t\t\t\tgoto invalid_optional;\n\t\t\twc->pause = msecs_to_jiffies(pause_msecs);\n\t\t\twc->pause_set = true;\n\t\t\twc->pause_value = pause_msecs;\n\t\t} else {\ninvalid_optional:\n\t\t\tr = -EINVAL;\n\t\t\tti->error = \"Invalid optional argument\";\n\t\t\tgoto bad;\n\t\t}\n\t}\n\n\tif (high_wm_percent < low_wm_percent) {\n\t\tr = -EINVAL;\n\t\tti->error = \"High watermark must be greater than or equal to low watermark\";\n\t\tgoto bad;\n\t}\n\n\tif (WC_MODE_PMEM(wc)) {\n\t\tif (!dax_synchronous(wc->ssd_dev->dax_dev)) {\n\t\t\tr = -EOPNOTSUPP;\n\t\t\tti->error = \"Asynchronous persistent memory not supported as pmem cache\";\n\t\t\tgoto bad;\n\t\t}\n\n\t\tr = persistent_memory_claim(wc);\n\t\tif (r) {\n\t\t\tti->error = \"Unable to map persistent memory for cache\";\n\t\t\tgoto bad;\n\t\t}\n\t} else {\n\t\tsize_t n_blocks, n_metadata_blocks;\n\t\tuint64_t n_bitmap_bits;\n\n\t\twc->memory_map_size -= (uint64_t)wc->start_sector << SECTOR_SHIFT;\n\n\t\tbio_list_init(&wc->flush_list);\n\t\twc->flush_thread = kthread_run(writecache_flush_thread, wc, \"dm_writecache_flush\");\n\t\tif (IS_ERR(wc->flush_thread)) {\n\t\t\tr = PTR_ERR(wc->flush_thread);\n\t\t\twc->flush_thread = NULL;\n\t\t\tti->error = \"Couldn't spawn flush thread\";\n\t\t\tgoto bad;\n\t\t}\n\n\t\tr = calculate_memory_size(wc->memory_map_size, wc->block_size,\n\t\t\t\t\t  &n_blocks, &n_metadata_blocks);\n\t\tif (r) {\n\t\t\tti->error = \"Invalid device size\";\n\t\t\tgoto bad;\n\t\t}\n\n\t\tn_bitmap_bits = (((uint64_t)n_metadata_blocks << wc->block_size_bits) +\n\t\t\t\t BITMAP_GRANULARITY - 1) / BITMAP_GRANULARITY;\n\t\t \n\t\tif (n_bitmap_bits > 1U << 31) {\n\t\t\tr = -EFBIG;\n\t\t\tti->error = \"Invalid device size\";\n\t\t\tgoto bad;\n\t\t}\n\n\t\twc->memory_map = vmalloc(n_metadata_blocks << wc->block_size_bits);\n\t\tif (!wc->memory_map) {\n\t\t\tr = -ENOMEM;\n\t\t\tti->error = \"Unable to allocate memory for metadata\";\n\t\t\tgoto bad;\n\t\t}\n\n\t\twc->dm_kcopyd = dm_kcopyd_client_create(&dm_kcopyd_throttle);\n\t\tif (IS_ERR(wc->dm_kcopyd)) {\n\t\t\tr = PTR_ERR(wc->dm_kcopyd);\n\t\t\tti->error = \"Unable to allocate dm-kcopyd client\";\n\t\t\twc->dm_kcopyd = NULL;\n\t\t\tgoto bad;\n\t\t}\n\n\t\twc->metadata_sectors = n_metadata_blocks << (wc->block_size_bits - SECTOR_SHIFT);\n\t\twc->dirty_bitmap_size = (n_bitmap_bits + BITS_PER_LONG - 1) /\n\t\t\tBITS_PER_LONG * sizeof(unsigned long);\n\t\twc->dirty_bitmap = vzalloc(wc->dirty_bitmap_size);\n\t\tif (!wc->dirty_bitmap) {\n\t\t\tr = -ENOMEM;\n\t\t\tti->error = \"Unable to allocate dirty bitmap\";\n\t\t\tgoto bad;\n\t\t}\n\n\t\tr = writecache_read_metadata(wc, wc->block_size >> SECTOR_SHIFT);\n\t\tif (r) {\n\t\t\tti->error = \"Unable to read first block of metadata\";\n\t\t\tgoto bad;\n\t\t}\n\t}\n\n\tr = copy_mc_to_kernel(&s, sb(wc), sizeof(struct wc_memory_superblock));\n\tif (r) {\n\t\tti->error = \"Hardware memory error when reading superblock\";\n\t\tgoto bad;\n\t}\n\tif (!le32_to_cpu(s.magic) && !le32_to_cpu(s.version)) {\n\t\tr = init_memory(wc);\n\t\tif (r) {\n\t\t\tti->error = \"Unable to initialize device\";\n\t\t\tgoto bad;\n\t\t}\n\t\tr = copy_mc_to_kernel(&s, sb(wc),\n\t\t\t\t      sizeof(struct wc_memory_superblock));\n\t\tif (r) {\n\t\t\tti->error = \"Hardware memory error when reading superblock\";\n\t\t\tgoto bad;\n\t\t}\n\t}\n\n\tif (le32_to_cpu(s.magic) != MEMORY_SUPERBLOCK_MAGIC) {\n\t\tti->error = \"Invalid magic in the superblock\";\n\t\tr = -EINVAL;\n\t\tgoto bad;\n\t}\n\n\tif (le32_to_cpu(s.version) != MEMORY_SUPERBLOCK_VERSION) {\n\t\tti->error = \"Invalid version in the superblock\";\n\t\tr = -EINVAL;\n\t\tgoto bad;\n\t}\n\n\tif (le32_to_cpu(s.block_size) != wc->block_size) {\n\t\tti->error = \"Block size does not match superblock\";\n\t\tr = -EINVAL;\n\t\tgoto bad;\n\t}\n\n\twc->n_blocks = le64_to_cpu(s.n_blocks);\n\n\toffset = wc->n_blocks * sizeof(struct wc_memory_entry);\n\tif (offset / sizeof(struct wc_memory_entry) != le64_to_cpu(sb(wc)->n_blocks)) {\noverflow:\n\t\tti->error = \"Overflow in size calculation\";\n\t\tr = -EINVAL;\n\t\tgoto bad;\n\t}\n\toffset += sizeof(struct wc_memory_superblock);\n\tif (offset < sizeof(struct wc_memory_superblock))\n\t\tgoto overflow;\n\toffset = (offset + wc->block_size - 1) & ~(size_t)(wc->block_size - 1);\n\tdata_size = wc->n_blocks * (size_t)wc->block_size;\n\tif (!offset || (data_size / wc->block_size != wc->n_blocks) ||\n\t    (offset + data_size < offset))\n\t\tgoto overflow;\n\tif (offset + data_size > wc->memory_map_size) {\n\t\tti->error = \"Memory area is too small\";\n\t\tr = -EINVAL;\n\t\tgoto bad;\n\t}\n\n\twc->metadata_sectors = offset >> SECTOR_SHIFT;\n\twc->block_start = (char *)sb(wc) + offset;\n\n\tx = (uint64_t)wc->n_blocks * (100 - high_wm_percent);\n\tx += 50;\n\tdo_div(x, 100);\n\twc->freelist_high_watermark = x;\n\tx = (uint64_t)wc->n_blocks * (100 - low_wm_percent);\n\tx += 50;\n\tdo_div(x, 100);\n\twc->freelist_low_watermark = x;\n\n\tif (wc->cleaner)\n\t\tactivate_cleaner(wc);\n\n\tr = writecache_alloc_entries(wc);\n\tif (r) {\n\t\tti->error = \"Cannot allocate memory\";\n\t\tgoto bad;\n\t}\n\n\tti->num_flush_bios = WC_MODE_PMEM(wc) ? 1 : 2;\n\tti->flush_supported = true;\n\tti->num_discard_bios = 1;\n\n\tif (WC_MODE_PMEM(wc))\n\t\tpersistent_memory_flush_cache(wc->memory_map, wc->memory_map_size);\n\n\treturn 0;\n\nbad_arguments:\n\tr = -EINVAL;\n\tti->error = \"Bad arguments\";\nbad:\n\twritecache_dtr(ti);\n\treturn r;\n}\n\nstatic void writecache_status(struct dm_target *ti, status_type_t type,\n\t\t\t      unsigned int status_flags, char *result, unsigned int maxlen)\n{\n\tstruct dm_writecache *wc = ti->private;\n\tunsigned int extra_args;\n\tunsigned int sz = 0;\n\n\tswitch (type) {\n\tcase STATUSTYPE_INFO:\n\t\tDMEMIT(\"%ld %llu %llu %llu %llu %llu %llu %llu %llu %llu %llu %llu %llu %llu\",\n\t\t       writecache_has_error(wc),\n\t\t       (unsigned long long)wc->n_blocks, (unsigned long long)wc->freelist_size,\n\t\t       (unsigned long long)wc->writeback_size,\n\t\t       wc->stats.reads,\n\t\t       wc->stats.read_hits,\n\t\t       wc->stats.writes,\n\t\t       wc->stats.write_hits_uncommitted,\n\t\t       wc->stats.write_hits_committed,\n\t\t       wc->stats.writes_around,\n\t\t       wc->stats.writes_allocate,\n\t\t       wc->stats.writes_blocked_on_freelist,\n\t\t       wc->stats.flushes,\n\t\t       wc->stats.discards);\n\t\tbreak;\n\tcase STATUSTYPE_TABLE:\n\t\tDMEMIT(\"%c %s %s %u \", WC_MODE_PMEM(wc) ? 'p' : 's',\n\t\t\t\twc->dev->name, wc->ssd_dev->name, wc->block_size);\n\t\textra_args = 0;\n\t\tif (wc->start_sector_set)\n\t\t\textra_args += 2;\n\t\tif (wc->high_wm_percent_set)\n\t\t\textra_args += 2;\n\t\tif (wc->low_wm_percent_set)\n\t\t\textra_args += 2;\n\t\tif (wc->max_writeback_jobs_set)\n\t\t\textra_args += 2;\n\t\tif (wc->autocommit_blocks_set)\n\t\t\textra_args += 2;\n\t\tif (wc->autocommit_time_set)\n\t\t\textra_args += 2;\n\t\tif (wc->max_age_set)\n\t\t\textra_args += 2;\n\t\tif (wc->cleaner_set)\n\t\t\textra_args++;\n\t\tif (wc->writeback_fua_set)\n\t\t\textra_args++;\n\t\tif (wc->metadata_only)\n\t\t\textra_args++;\n\t\tif (wc->pause_set)\n\t\t\textra_args += 2;\n\n\t\tDMEMIT(\"%u\", extra_args);\n\t\tif (wc->start_sector_set)\n\t\t\tDMEMIT(\" start_sector %llu\", (unsigned long long)wc->start_sector);\n\t\tif (wc->high_wm_percent_set)\n\t\t\tDMEMIT(\" high_watermark %u\", wc->high_wm_percent_value);\n\t\tif (wc->low_wm_percent_set)\n\t\t\tDMEMIT(\" low_watermark %u\", wc->low_wm_percent_value);\n\t\tif (wc->max_writeback_jobs_set)\n\t\t\tDMEMIT(\" writeback_jobs %u\", wc->max_writeback_jobs);\n\t\tif (wc->autocommit_blocks_set)\n\t\t\tDMEMIT(\" autocommit_blocks %u\", wc->autocommit_blocks);\n\t\tif (wc->autocommit_time_set)\n\t\t\tDMEMIT(\" autocommit_time %u\", wc->autocommit_time_value);\n\t\tif (wc->max_age_set)\n\t\t\tDMEMIT(\" max_age %u\", wc->max_age_value);\n\t\tif (wc->cleaner_set)\n\t\t\tDMEMIT(\" cleaner\");\n\t\tif (wc->writeback_fua_set)\n\t\t\tDMEMIT(\" %sfua\", wc->writeback_fua ? \"\" : \"no\");\n\t\tif (wc->metadata_only)\n\t\t\tDMEMIT(\" metadata_only\");\n\t\tif (wc->pause_set)\n\t\t\tDMEMIT(\" pause_writeback %u\", wc->pause_value);\n\t\tbreak;\n\tcase STATUSTYPE_IMA:\n\t\t*result = '\\0';\n\t\tbreak;\n\t}\n}\n\nstatic struct target_type writecache_target = {\n\t.name\t\t\t= \"writecache\",\n\t.version\t\t= {1, 6, 0},\n\t.module\t\t\t= THIS_MODULE,\n\t.ctr\t\t\t= writecache_ctr,\n\t.dtr\t\t\t= writecache_dtr,\n\t.status\t\t\t= writecache_status,\n\t.postsuspend\t\t= writecache_suspend,\n\t.resume\t\t\t= writecache_resume,\n\t.message\t\t= writecache_message,\n\t.map\t\t\t= writecache_map,\n\t.end_io\t\t\t= writecache_end_io,\n\t.iterate_devices\t= writecache_iterate_devices,\n\t.io_hints\t\t= writecache_io_hints,\n};\nmodule_dm(writecache);\n\nMODULE_DESCRIPTION(DM_NAME \" writecache target\");\nMODULE_AUTHOR(\"Mikulas Patocka <dm-devel@redhat.com>\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}