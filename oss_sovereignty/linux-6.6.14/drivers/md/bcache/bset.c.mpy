{
  "module_name": "bset.c",
  "hash_id": "56c73798c62ab9a777cb8f45c22e1a8a3588326b6c29ec20c03a0b03800fb2bc",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/bcache/bset.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"bcache: %s() \" fmt, __func__\n\n#include \"util.h\"\n#include \"bset.h\"\n\n#include <linux/console.h>\n#include <linux/sched/clock.h>\n#include <linux/random.h>\n#include <linux/prefetch.h>\n\n#ifdef CONFIG_BCACHE_DEBUG\n\nvoid bch_dump_bset(struct btree_keys *b, struct bset *i, unsigned int set)\n{\n\tstruct bkey *k, *next;\n\n\tfor (k = i->start; k < bset_bkey_last(i); k = next) {\n\t\tnext = bkey_next(k);\n\n\t\tpr_err(\"block %u key %u/%u: \", set,\n\t\t       (unsigned int) ((u64 *) k - i->d), i->keys);\n\n\t\tif (b->ops->key_dump)\n\t\t\tb->ops->key_dump(b, k);\n\t\telse\n\t\t\tpr_cont(\"%llu:%llu\\n\", KEY_INODE(k), KEY_OFFSET(k));\n\n\t\tif (next < bset_bkey_last(i) &&\n\t\t    bkey_cmp(k, b->ops->is_extents ?\n\t\t\t     &START_KEY(next) : next) > 0)\n\t\t\tpr_err(\"Key skipped backwards\\n\");\n\t}\n}\n\nvoid bch_dump_bucket(struct btree_keys *b)\n{\n\tunsigned int i;\n\n\tconsole_lock();\n\tfor (i = 0; i <= b->nsets; i++)\n\t\tbch_dump_bset(b, b->set[i].data,\n\t\t\t      bset_sector_offset(b, b->set[i].data));\n\tconsole_unlock();\n}\n\nint __bch_count_data(struct btree_keys *b)\n{\n\tunsigned int ret = 0;\n\tstruct btree_iter iter;\n\tstruct bkey *k;\n\n\tif (b->ops->is_extents)\n\t\tfor_each_key(b, k, &iter)\n\t\t\tret += KEY_SIZE(k);\n\treturn ret;\n}\n\nvoid __bch_check_keys(struct btree_keys *b, const char *fmt, ...)\n{\n\tva_list args;\n\tstruct bkey *k, *p = NULL;\n\tstruct btree_iter iter;\n\tconst char *err;\n\n\tfor_each_key(b, k, &iter) {\n\t\tif (b->ops->is_extents) {\n\t\t\terr = \"Keys out of order\";\n\t\t\tif (p && bkey_cmp(&START_KEY(p), &START_KEY(k)) > 0)\n\t\t\t\tgoto bug;\n\n\t\t\tif (bch_ptr_invalid(b, k))\n\t\t\t\tcontinue;\n\n\t\t\terr =  \"Overlapping keys\";\n\t\t\tif (p && bkey_cmp(p, &START_KEY(k)) > 0)\n\t\t\t\tgoto bug;\n\t\t} else {\n\t\t\tif (bch_ptr_bad(b, k))\n\t\t\t\tcontinue;\n\n\t\t\terr = \"Duplicate keys\";\n\t\t\tif (p && !bkey_cmp(p, k))\n\t\t\t\tgoto bug;\n\t\t}\n\t\tp = k;\n\t}\n#if 0\n\terr = \"Key larger than btree node key\";\n\tif (p && bkey_cmp(p, &b->key) > 0)\n\t\tgoto bug;\n#endif\n\treturn;\nbug:\n\tbch_dump_bucket(b);\n\n\tva_start(args, fmt);\n\tvprintk(fmt, args);\n\tva_end(args);\n\n\tpanic(\"bch_check_keys error:  %s:\\n\", err);\n}\n\nstatic void bch_btree_iter_next_check(struct btree_iter *iter)\n{\n\tstruct bkey *k = iter->data->k, *next = bkey_next(k);\n\n\tif (next < iter->data->end &&\n\t    bkey_cmp(k, iter->b->ops->is_extents ?\n\t\t     &START_KEY(next) : next) > 0) {\n\t\tbch_dump_bucket(iter->b);\n\t\tpanic(\"Key skipped backwards\\n\");\n\t}\n}\n\n#else\n\nstatic inline void bch_btree_iter_next_check(struct btree_iter *iter) {}\n\n#endif\n\n \n\nint __bch_keylist_realloc(struct keylist *l, unsigned int u64s)\n{\n\tsize_t oldsize = bch_keylist_nkeys(l);\n\tsize_t newsize = oldsize + u64s;\n\tuint64_t *old_keys = l->keys_p == l->inline_keys ? NULL : l->keys_p;\n\tuint64_t *new_keys;\n\n\tnewsize = roundup_pow_of_two(newsize);\n\n\tif (newsize <= KEYLIST_INLINE ||\n\t    roundup_pow_of_two(oldsize) == newsize)\n\t\treturn 0;\n\n\tnew_keys = krealloc(old_keys, sizeof(uint64_t) * newsize, GFP_NOIO);\n\n\tif (!new_keys)\n\t\treturn -ENOMEM;\n\n\tif (!old_keys)\n\t\tmemcpy(new_keys, l->inline_keys, sizeof(uint64_t) * oldsize);\n\n\tl->keys_p = new_keys;\n\tl->top_p = new_keys + oldsize;\n\n\treturn 0;\n}\n\n \nstruct bkey *bch_keylist_pop(struct keylist *l)\n{\n\tstruct bkey *k = l->keys;\n\n\tif (k == l->top)\n\t\treturn NULL;\n\n\twhile (bkey_next(k) != l->top)\n\t\tk = bkey_next(k);\n\n\treturn l->top = k;\n}\n\n \nvoid bch_keylist_pop_front(struct keylist *l)\n{\n\tl->top_p -= bkey_u64s(l->keys);\n\n\tmemmove(l->keys,\n\t\tbkey_next(l->keys),\n\t\tbch_keylist_bytes(l));\n}\n\n \n\nvoid bch_bkey_copy_single_ptr(struct bkey *dest, const struct bkey *src,\n\t\t\t      unsigned int i)\n{\n\tBUG_ON(i > KEY_PTRS(src));\n\n\t \n\tmemcpy(dest, src, 2 * sizeof(uint64_t));\n\tdest->ptr[0] = src->ptr[i];\n\tSET_KEY_PTRS(dest, 1);\n\t \n\tSET_KEY_CSUM(dest, 0);\n}\n\nbool __bch_cut_front(const struct bkey *where, struct bkey *k)\n{\n\tunsigned int i, len = 0;\n\n\tif (bkey_cmp(where, &START_KEY(k)) <= 0)\n\t\treturn false;\n\n\tif (bkey_cmp(where, k) < 0)\n\t\tlen = KEY_OFFSET(k) - KEY_OFFSET(where);\n\telse\n\t\tbkey_copy_key(k, where);\n\n\tfor (i = 0; i < KEY_PTRS(k); i++)\n\t\tSET_PTR_OFFSET(k, i, PTR_OFFSET(k, i) + KEY_SIZE(k) - len);\n\n\tBUG_ON(len > KEY_SIZE(k));\n\tSET_KEY_SIZE(k, len);\n\treturn true;\n}\n\nbool __bch_cut_back(const struct bkey *where, struct bkey *k)\n{\n\tunsigned int len = 0;\n\n\tif (bkey_cmp(where, k) >= 0)\n\t\treturn false;\n\n\tBUG_ON(KEY_INODE(where) != KEY_INODE(k));\n\n\tif (bkey_cmp(where, &START_KEY(k)) > 0)\n\t\tlen = KEY_OFFSET(where) - KEY_START(k);\n\n\tbkey_copy_key(k, where);\n\n\tBUG_ON(len > KEY_SIZE(k));\n\tSET_KEY_SIZE(k, len);\n\treturn true;\n}\n\n \n\n \n#define BKEY_MID_BITS\t\t3\n#define BKEY_EXPONENT_BITS\t7\n#define BKEY_MANTISSA_BITS\t(32 - BKEY_MID_BITS - BKEY_EXPONENT_BITS)\n#define BKEY_MANTISSA_MASK\t((1 << BKEY_MANTISSA_BITS) - 1)\n\nstruct bkey_float {\n\tunsigned int\texponent:BKEY_EXPONENT_BITS;\n\tunsigned int\tm:BKEY_MID_BITS;\n\tunsigned int\tmantissa:BKEY_MANTISSA_BITS;\n} __packed;\n\n \n\n#define BSET_CACHELINE\t\t128\n\n \nstatic inline size_t btree_keys_bytes(struct btree_keys *b)\n{\n\treturn PAGE_SIZE << b->page_order;\n}\n\nstatic inline size_t btree_keys_cachelines(struct btree_keys *b)\n{\n\treturn btree_keys_bytes(b) / BSET_CACHELINE;\n}\n\n \nstatic inline size_t bset_tree_bytes(struct btree_keys *b)\n{\n\treturn btree_keys_cachelines(b) * sizeof(struct bkey_float);\n}\n\n \nstatic inline size_t bset_prev_bytes(struct btree_keys *b)\n{\n\treturn btree_keys_cachelines(b) * sizeof(uint8_t);\n}\n\n \n\nvoid bch_btree_keys_free(struct btree_keys *b)\n{\n\tstruct bset_tree *t = b->set;\n\n\tif (bset_prev_bytes(b) < PAGE_SIZE)\n\t\tkfree(t->prev);\n\telse\n\t\tfree_pages((unsigned long) t->prev,\n\t\t\t   get_order(bset_prev_bytes(b)));\n\n\tif (bset_tree_bytes(b) < PAGE_SIZE)\n\t\tkfree(t->tree);\n\telse\n\t\tfree_pages((unsigned long) t->tree,\n\t\t\t   get_order(bset_tree_bytes(b)));\n\n\tfree_pages((unsigned long) t->data, b->page_order);\n\n\tt->prev = NULL;\n\tt->tree = NULL;\n\tt->data = NULL;\n}\n\nint bch_btree_keys_alloc(struct btree_keys *b,\n\t\t\t unsigned int page_order,\n\t\t\t gfp_t gfp)\n{\n\tstruct bset_tree *t = b->set;\n\n\tBUG_ON(t->data);\n\n\tb->page_order = page_order;\n\n\tt->data = (void *) __get_free_pages(__GFP_COMP|gfp, b->page_order);\n\tif (!t->data)\n\t\tgoto err;\n\n\tt->tree = bset_tree_bytes(b) < PAGE_SIZE\n\t\t? kmalloc(bset_tree_bytes(b), gfp)\n\t\t: (void *) __get_free_pages(gfp, get_order(bset_tree_bytes(b)));\n\tif (!t->tree)\n\t\tgoto err;\n\n\tt->prev = bset_prev_bytes(b) < PAGE_SIZE\n\t\t? kmalloc(bset_prev_bytes(b), gfp)\n\t\t: (void *) __get_free_pages(gfp, get_order(bset_prev_bytes(b)));\n\tif (!t->prev)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tbch_btree_keys_free(b);\n\treturn -ENOMEM;\n}\n\nvoid bch_btree_keys_init(struct btree_keys *b, const struct btree_keys_ops *ops,\n\t\t\t bool *expensive_debug_checks)\n{\n\tb->ops = ops;\n\tb->expensive_debug_checks = expensive_debug_checks;\n\tb->nsets = 0;\n\tb->last_set_unwritten = 0;\n\n\t \n}\n\n \n\n \nstatic unsigned int inorder_next(unsigned int j, unsigned int size)\n{\n\tif (j * 2 + 1 < size) {\n\t\tj = j * 2 + 1;\n\n\t\twhile (j * 2 < size)\n\t\t\tj *= 2;\n\t} else\n\t\tj >>= ffz(j) + 1;\n\n\treturn j;\n}\n\n \nstatic unsigned int inorder_prev(unsigned int j, unsigned int size)\n{\n\tif (j * 2 < size) {\n\t\tj = j * 2;\n\n\t\twhile (j * 2 + 1 < size)\n\t\t\tj = j * 2 + 1;\n\t} else\n\t\tj >>= ffs(j);\n\n\treturn j;\n}\n\n \nstatic unsigned int __to_inorder(unsigned int j,\n\t\t\t\t  unsigned int size,\n\t\t\t\t  unsigned int extra)\n{\n\tunsigned int b = fls(j);\n\tunsigned int shift = fls(size - 1) - b;\n\n\tj  ^= 1U << (b - 1);\n\tj <<= 1;\n\tj  |= 1;\n\tj <<= shift;\n\n\tif (j > extra)\n\t\tj -= (j - extra) >> 1;\n\n\treturn j;\n}\n\n \nstatic unsigned int to_inorder(unsigned int j, struct bset_tree *t)\n{\n\treturn __to_inorder(j, t->size, t->extra);\n}\n\nstatic unsigned int __inorder_to_tree(unsigned int j,\n\t\t\t\t      unsigned int size,\n\t\t\t\t      unsigned int extra)\n{\n\tunsigned int shift;\n\n\tif (j > extra)\n\t\tj += j - extra;\n\n\tshift = ffs(j);\n\n\tj >>= shift;\n\tj  |= roundup_pow_of_two(size) >> shift;\n\n\treturn j;\n}\n\n \nstatic unsigned int inorder_to_tree(unsigned int j, struct bset_tree *t)\n{\n\treturn __inorder_to_tree(j, t->size, t->extra);\n}\n\n#if 0\nvoid inorder_test(void)\n{\n\tunsigned long done = 0;\n\tktime_t start = ktime_get();\n\n\tfor (unsigned int size = 2;\n\t     size < 65536000;\n\t     size++) {\n\t\tunsigned int extra =\n\t\t\t(size - rounddown_pow_of_two(size - 1)) << 1;\n\t\tunsigned int i = 1, j = rounddown_pow_of_two(size - 1);\n\n\t\tif (!(size % 4096))\n\t\t\tpr_notice(\"loop %u, %llu per us\\n\", size,\n\t\t\t       done / ktime_us_delta(ktime_get(), start));\n\n\t\twhile (1) {\n\t\t\tif (__inorder_to_tree(i, size, extra) != j)\n\t\t\t\tpanic(\"size %10u j %10u i %10u\", size, j, i);\n\n\t\t\tif (__to_inorder(j, size, extra) != i)\n\t\t\t\tpanic(\"size %10u j %10u i %10u\", size, j, i);\n\n\t\t\tif (j == rounddown_pow_of_two(size) - 1)\n\t\t\t\tbreak;\n\n\t\t\tBUG_ON(inorder_prev(inorder_next(j, size), size) != j);\n\n\t\t\tj = inorder_next(j, size);\n\t\t\ti++;\n\t\t}\n\n\t\tdone += size - 1;\n\t}\n}\n#endif\n\n \n\nstatic struct bkey *cacheline_to_bkey(struct bset_tree *t,\n\t\t\t\t      unsigned int cacheline,\n\t\t\t\t      unsigned int offset)\n{\n\treturn ((void *) t->data) + cacheline * BSET_CACHELINE + offset * 8;\n}\n\nstatic unsigned int bkey_to_cacheline(struct bset_tree *t, struct bkey *k)\n{\n\treturn ((void *) k - (void *) t->data) / BSET_CACHELINE;\n}\n\nstatic unsigned int bkey_to_cacheline_offset(struct bset_tree *t,\n\t\t\t\t\t unsigned int cacheline,\n\t\t\t\t\t struct bkey *k)\n{\n\treturn (u64 *) k - (u64 *) cacheline_to_bkey(t, cacheline, 0);\n}\n\nstatic struct bkey *tree_to_bkey(struct bset_tree *t, unsigned int j)\n{\n\treturn cacheline_to_bkey(t, to_inorder(j, t), t->tree[j].m);\n}\n\nstatic struct bkey *tree_to_prev_bkey(struct bset_tree *t, unsigned int j)\n{\n\treturn (void *) (((uint64_t *) tree_to_bkey(t, j)) - t->prev[j]);\n}\n\n \nstatic struct bkey *table_to_bkey(struct bset_tree *t, unsigned int cacheline)\n{\n\treturn cacheline_to_bkey(t, cacheline, t->prev[cacheline]);\n}\n\nstatic inline uint64_t shrd128(uint64_t high, uint64_t low, uint8_t shift)\n{\n\tlow >>= shift;\n\tlow  |= (high << 1) << (63U - shift);\n\treturn low;\n}\n\n \nstatic inline unsigned int bfloat_mantissa(const struct bkey *k,\n\t\t\t\t       struct bkey_float *f)\n{\n\tconst uint64_t *p = &k->low - (f->exponent >> 6);\n\n\treturn shrd128(p[-1], p[0], f->exponent & 63) & BKEY_MANTISSA_MASK;\n}\n\nstatic void make_bfloat(struct bset_tree *t, unsigned int j)\n{\n\tstruct bkey_float *f = &t->tree[j];\n\tstruct bkey *m = tree_to_bkey(t, j);\n\tstruct bkey *p = tree_to_prev_bkey(t, j);\n\n\tstruct bkey *l = is_power_of_2(j)\n\t\t? t->data->start\n\t\t: tree_to_prev_bkey(t, j >> ffs(j));\n\n\tstruct bkey *r = is_power_of_2(j + 1)\n\t\t? bset_bkey_idx(t->data, t->data->keys - bkey_u64s(&t->end))\n\t\t: tree_to_bkey(t, j >> (ffz(j) + 1));\n\n\tBUG_ON(m < l || m > r);\n\tBUG_ON(bkey_next(p) != m);\n\n\t \n\tif (KEY_INODE(l) != KEY_INODE(r))\n\t\tf->exponent = fls64(KEY_INODE(r) ^ KEY_INODE(l)) + 64;\n\telse\n\t\tf->exponent = fls64(r->low ^ l->low);\n\n\tf->exponent = max_t(int, f->exponent - BKEY_MANTISSA_BITS, 0);\n\n\t \n\n\tif (bfloat_mantissa(m, f) != bfloat_mantissa(p, f))\n\t\tf->mantissa = bfloat_mantissa(m, f) - 1;\n\telse\n\t\tf->exponent = 127;\n}\n\nstatic void bset_alloc_tree(struct btree_keys *b, struct bset_tree *t)\n{\n\tif (t != b->set) {\n\t\tunsigned int j = roundup(t[-1].size,\n\t\t\t\t     64 / sizeof(struct bkey_float));\n\n\t\tt->tree = t[-1].tree + j;\n\t\tt->prev = t[-1].prev + j;\n\t}\n\n\twhile (t < b->set + MAX_BSETS)\n\t\tt++->size = 0;\n}\n\nstatic void bch_bset_build_unwritten_tree(struct btree_keys *b)\n{\n\tstruct bset_tree *t = bset_tree_last(b);\n\n\tBUG_ON(b->last_set_unwritten);\n\tb->last_set_unwritten = 1;\n\n\tbset_alloc_tree(b, t);\n\n\tif (t->tree != b->set->tree + btree_keys_cachelines(b)) {\n\t\tt->prev[0] = bkey_to_cacheline_offset(t, 0, t->data->start);\n\t\tt->size = 1;\n\t}\n}\n\nvoid bch_bset_init_next(struct btree_keys *b, struct bset *i, uint64_t magic)\n{\n\tif (i != b->set->data) {\n\t\tb->set[++b->nsets].data = i;\n\t\ti->seq = b->set->data->seq;\n\t} else\n\t\tget_random_bytes(&i->seq, sizeof(uint64_t));\n\n\ti->magic\t= magic;\n\ti->version\t= 0;\n\ti->keys\t\t= 0;\n\n\tbch_bset_build_unwritten_tree(b);\n}\n\n \nvoid bch_bset_build_written_tree(struct btree_keys *b)\n{\n\tstruct bset_tree *t = bset_tree_last(b);\n\tstruct bkey *prev = NULL, *k = t->data->start;\n\tunsigned int j, cacheline = 1;\n\n\tb->last_set_unwritten = 0;\n\n\tbset_alloc_tree(b, t);\n\n\tt->size = min_t(unsigned int,\n\t\t\tbkey_to_cacheline(t, bset_bkey_last(t->data)),\n\t\t\tb->set->tree + btree_keys_cachelines(b) - t->tree);\n\n\tif (t->size < 2) {\n\t\tt->size = 0;\n\t\treturn;\n\t}\n\n\tt->extra = (t->size - rounddown_pow_of_two(t->size - 1)) << 1;\n\n\t \n\tfor (j = inorder_next(0, t->size);\n\t     j;\n\t     j = inorder_next(j, t->size)) {\n\t\twhile (bkey_to_cacheline(t, k) < cacheline) {\n\t\t\tprev = k;\n\t\t\tk = bkey_next(k);\n\t\t}\n\n\t\tt->prev[j] = bkey_u64s(prev);\n\t\tt->tree[j].m = bkey_to_cacheline_offset(t, cacheline++, k);\n\t}\n\n\twhile (bkey_next(k) != bset_bkey_last(t->data))\n\t\tk = bkey_next(k);\n\n\tt->end = *k;\n\n\t \n\tfor (j = inorder_next(0, t->size);\n\t     j;\n\t     j = inorder_next(j, t->size))\n\t\tmake_bfloat(t, j);\n}\n\n \n\nvoid bch_bset_fix_invalidated_key(struct btree_keys *b, struct bkey *k)\n{\n\tstruct bset_tree *t;\n\tunsigned int inorder, j = 1;\n\n\tfor (t = b->set; t <= bset_tree_last(b); t++)\n\t\tif (k < bset_bkey_last(t->data))\n\t\t\tgoto found_set;\n\n\tBUG();\nfound_set:\n\tif (!t->size || !bset_written(b, t))\n\t\treturn;\n\n\tinorder = bkey_to_cacheline(t, k);\n\n\tif (k == t->data->start)\n\t\tgoto fix_left;\n\n\tif (bkey_next(k) == bset_bkey_last(t->data)) {\n\t\tt->end = *k;\n\t\tgoto fix_right;\n\t}\n\n\tj = inorder_to_tree(inorder, t);\n\n\tif (j &&\n\t    j < t->size &&\n\t    k == tree_to_bkey(t, j))\nfix_left:\tdo {\n\t\t\tmake_bfloat(t, j);\n\t\t\tj = j * 2;\n\t\t} while (j < t->size);\n\n\tj = inorder_to_tree(inorder + 1, t);\n\n\tif (j &&\n\t    j < t->size &&\n\t    k == tree_to_prev_bkey(t, j))\nfix_right:\tdo {\n\t\t\tmake_bfloat(t, j);\n\t\t\tj = j * 2 + 1;\n\t\t} while (j < t->size);\n}\n\nstatic void bch_bset_fix_lookup_table(struct btree_keys *b,\n\t\t\t\t      struct bset_tree *t,\n\t\t\t\t      struct bkey *k)\n{\n\tunsigned int shift = bkey_u64s(k);\n\tunsigned int j = bkey_to_cacheline(t, k);\n\n\t \n\tif (!t->size)\n\t\treturn;\n\n\t \n\twhile (j < t->size &&\n\t       table_to_bkey(t, j) <= k)\n\t\tj++;\n\n\t \n\tfor (; j < t->size; j++) {\n\t\tt->prev[j] += shift;\n\n\t\tif (t->prev[j] > 7) {\n\t\t\tk = table_to_bkey(t, j - 1);\n\n\t\t\twhile (k < cacheline_to_bkey(t, j, 0))\n\t\t\t\tk = bkey_next(k);\n\n\t\t\tt->prev[j] = bkey_to_cacheline_offset(t, j, k);\n\t\t}\n\t}\n\n\tif (t->size == b->set->tree + btree_keys_cachelines(b) - t->tree)\n\t\treturn;\n\n\t \n\n\tfor (k = table_to_bkey(t, t->size - 1);\n\t     k != bset_bkey_last(t->data);\n\t     k = bkey_next(k))\n\t\tif (t->size == bkey_to_cacheline(t, k)) {\n\t\t\tt->prev[t->size] =\n\t\t\t\tbkey_to_cacheline_offset(t, t->size, k);\n\t\t\tt->size++;\n\t\t}\n}\n\n \nbool bch_bkey_try_merge(struct btree_keys *b, struct bkey *l, struct bkey *r)\n{\n\tif (!b->ops->key_merge)\n\t\treturn false;\n\n\t \n\tif (!bch_bkey_equal_header(l, r) ||\n\t     bkey_cmp(l, &START_KEY(r)))\n\t\treturn false;\n\n\treturn b->ops->key_merge(b, l, r);\n}\n\nvoid bch_bset_insert(struct btree_keys *b, struct bkey *where,\n\t\t     struct bkey *insert)\n{\n\tstruct bset_tree *t = bset_tree_last(b);\n\n\tBUG_ON(!b->last_set_unwritten);\n\tBUG_ON(bset_byte_offset(b, t->data) +\n\t       __set_bytes(t->data, t->data->keys + bkey_u64s(insert)) >\n\t       PAGE_SIZE << b->page_order);\n\n\tmemmove((uint64_t *) where + bkey_u64s(insert),\n\t\twhere,\n\t\t(void *) bset_bkey_last(t->data) - (void *) where);\n\n\tt->data->keys += bkey_u64s(insert);\n\tbkey_copy(where, insert);\n\tbch_bset_fix_lookup_table(b, t, where);\n}\n\nunsigned int bch_btree_insert_key(struct btree_keys *b, struct bkey *k,\n\t\t\t      struct bkey *replace_key)\n{\n\tunsigned int status = BTREE_INSERT_STATUS_NO_INSERT;\n\tstruct bset *i = bset_tree_last(b)->data;\n\tstruct bkey *m, *prev = NULL;\n\tstruct btree_iter iter;\n\tstruct bkey preceding_key_on_stack = ZERO_KEY;\n\tstruct bkey *preceding_key_p = &preceding_key_on_stack;\n\n\tBUG_ON(b->ops->is_extents && !KEY_SIZE(k));\n\n\t \n\tif (b->ops->is_extents)\n\t\tpreceding_key(&START_KEY(k), &preceding_key_p);\n\telse\n\t\tpreceding_key(k, &preceding_key_p);\n\n\tm = bch_btree_iter_init(b, &iter, preceding_key_p);\n\n\tif (b->ops->insert_fixup(b, k, &iter, replace_key))\n\t\treturn status;\n\n\tstatus = BTREE_INSERT_STATUS_INSERT;\n\n\twhile (m != bset_bkey_last(i) &&\n\t       bkey_cmp(k, b->ops->is_extents ? &START_KEY(m) : m) > 0) {\n\t\tprev = m;\n\t\tm = bkey_next(m);\n\t}\n\n\t \n\tstatus = BTREE_INSERT_STATUS_BACK_MERGE;\n\tif (prev &&\n\t    bch_bkey_try_merge(b, prev, k))\n\t\tgoto merged;\n#if 0\n\tstatus = BTREE_INSERT_STATUS_OVERWROTE;\n\tif (m != bset_bkey_last(i) &&\n\t    KEY_PTRS(m) == KEY_PTRS(k) && !KEY_SIZE(m))\n\t\tgoto copy;\n#endif\n\tstatus = BTREE_INSERT_STATUS_FRONT_MERGE;\n\tif (m != bset_bkey_last(i) &&\n\t    bch_bkey_try_merge(b, k, m))\n\t\tgoto copy;\n\n\tbch_bset_insert(b, m, k);\ncopy:\tbkey_copy(m, k);\nmerged:\n\treturn status;\n}\n\n \n\nstruct bset_search_iter {\n\tstruct bkey *l, *r;\n};\n\nstatic struct bset_search_iter bset_search_write_set(struct bset_tree *t,\n\t\t\t\t\t\t     const struct bkey *search)\n{\n\tunsigned int li = 0, ri = t->size;\n\n\twhile (li + 1 != ri) {\n\t\tunsigned int m = (li + ri) >> 1;\n\n\t\tif (bkey_cmp(table_to_bkey(t, m), search) > 0)\n\t\t\tri = m;\n\t\telse\n\t\t\tli = m;\n\t}\n\n\treturn (struct bset_search_iter) {\n\t\ttable_to_bkey(t, li),\n\t\tri < t->size ? table_to_bkey(t, ri) : bset_bkey_last(t->data)\n\t};\n}\n\nstatic struct bset_search_iter bset_search_tree(struct bset_tree *t,\n\t\t\t\t\t\tconst struct bkey *search)\n{\n\tstruct bkey *l, *r;\n\tstruct bkey_float *f;\n\tunsigned int inorder, j, n = 1;\n\n\tdo {\n\t\tunsigned int p = n << 4;\n\n\t\tif (p < t->size)\n\t\t\tprefetch(&t->tree[p]);\n\n\t\tj = n;\n\t\tf = &t->tree[j];\n\n\t\tif (likely(f->exponent != 127)) {\n\t\t\tif (f->mantissa >= bfloat_mantissa(search, f))\n\t\t\t\tn = j * 2;\n\t\t\telse\n\t\t\t\tn = j * 2 + 1;\n\t\t} else {\n\t\t\tif (bkey_cmp(tree_to_bkey(t, j), search) > 0)\n\t\t\t\tn = j * 2;\n\t\t\telse\n\t\t\t\tn = j * 2 + 1;\n\t\t}\n\t} while (n < t->size);\n\n\tinorder = to_inorder(j, t);\n\n\t \n\tif (n & 1) {\n\t\tl = cacheline_to_bkey(t, inorder, f->m);\n\n\t\tif (++inorder != t->size) {\n\t\t\tf = &t->tree[inorder_next(j, t->size)];\n\t\t\tr = cacheline_to_bkey(t, inorder, f->m);\n\t\t} else\n\t\t\tr = bset_bkey_last(t->data);\n\t} else {\n\t\tr = cacheline_to_bkey(t, inorder, f->m);\n\n\t\tif (--inorder) {\n\t\t\tf = &t->tree[inorder_prev(j, t->size)];\n\t\t\tl = cacheline_to_bkey(t, inorder, f->m);\n\t\t} else\n\t\t\tl = t->data->start;\n\t}\n\n\treturn (struct bset_search_iter) {l, r};\n}\n\nstruct bkey *__bch_bset_search(struct btree_keys *b, struct bset_tree *t,\n\t\t\t       const struct bkey *search)\n{\n\tstruct bset_search_iter i;\n\n\t \n\n\tif (unlikely(!t->size)) {\n\t\ti.l = t->data->start;\n\t\ti.r = bset_bkey_last(t->data);\n\t} else if (bset_written(b, t)) {\n\t\t \n\n\t\tif (unlikely(bkey_cmp(search, &t->end) >= 0))\n\t\t\treturn bset_bkey_last(t->data);\n\n\t\tif (unlikely(bkey_cmp(search, t->data->start) < 0))\n\t\t\treturn t->data->start;\n\n\t\ti = bset_search_tree(t, search);\n\t} else {\n\t\tBUG_ON(!b->nsets &&\n\t\t       t->size < bkey_to_cacheline(t, bset_bkey_last(t->data)));\n\n\t\ti = bset_search_write_set(t, search);\n\t}\n\n\tif (btree_keys_expensive_checks(b)) {\n\t\tBUG_ON(bset_written(b, t) &&\n\t\t       i.l != t->data->start &&\n\t\t       bkey_cmp(tree_to_prev_bkey(t,\n\t\t\t  inorder_to_tree(bkey_to_cacheline(t, i.l), t)),\n\t\t\t\tsearch) > 0);\n\n\t\tBUG_ON(i.r != bset_bkey_last(t->data) &&\n\t\t       bkey_cmp(i.r, search) <= 0);\n\t}\n\n\twhile (likely(i.l != i.r) &&\n\t       bkey_cmp(i.l, search) <= 0)\n\t\ti.l = bkey_next(i.l);\n\n\treturn i.l;\n}\n\n \n\ntypedef bool (btree_iter_cmp_fn)(struct btree_iter_set,\n\t\t\t\t struct btree_iter_set);\n\nstatic inline bool btree_iter_cmp(struct btree_iter_set l,\n\t\t\t\t  struct btree_iter_set r)\n{\n\treturn bkey_cmp(l.k, r.k) > 0;\n}\n\nstatic inline bool btree_iter_end(struct btree_iter *iter)\n{\n\treturn !iter->used;\n}\n\nvoid bch_btree_iter_push(struct btree_iter *iter, struct bkey *k,\n\t\t\t struct bkey *end)\n{\n\tif (k != end)\n\t\tBUG_ON(!heap_add(iter,\n\t\t\t\t ((struct btree_iter_set) { k, end }),\n\t\t\t\t btree_iter_cmp));\n}\n\nstatic struct bkey *__bch_btree_iter_init(struct btree_keys *b,\n\t\t\t\t\t  struct btree_iter *iter,\n\t\t\t\t\t  struct bkey *search,\n\t\t\t\t\t  struct bset_tree *start)\n{\n\tstruct bkey *ret = NULL;\n\n\titer->size = ARRAY_SIZE(iter->data);\n\titer->used = 0;\n\n#ifdef CONFIG_BCACHE_DEBUG\n\titer->b = b;\n#endif\n\n\tfor (; start <= bset_tree_last(b); start++) {\n\t\tret = bch_bset_search(b, start, search);\n\t\tbch_btree_iter_push(iter, ret, bset_bkey_last(start->data));\n\t}\n\n\treturn ret;\n}\n\nstruct bkey *bch_btree_iter_init(struct btree_keys *b,\n\t\t\t\t struct btree_iter *iter,\n\t\t\t\t struct bkey *search)\n{\n\treturn __bch_btree_iter_init(b, iter, search, b->set);\n}\n\nstatic inline struct bkey *__bch_btree_iter_next(struct btree_iter *iter,\n\t\t\t\t\t\t btree_iter_cmp_fn *cmp)\n{\n\tstruct btree_iter_set b __maybe_unused;\n\tstruct bkey *ret = NULL;\n\n\tif (!btree_iter_end(iter)) {\n\t\tbch_btree_iter_next_check(iter);\n\n\t\tret = iter->data->k;\n\t\titer->data->k = bkey_next(iter->data->k);\n\n\t\tif (iter->data->k > iter->data->end) {\n\t\t\tWARN_ONCE(1, \"bset was corrupt!\\n\");\n\t\t\titer->data->k = iter->data->end;\n\t\t}\n\n\t\tif (iter->data->k == iter->data->end)\n\t\t\theap_pop(iter, b, cmp);\n\t\telse\n\t\t\theap_sift(iter, 0, cmp);\n\t}\n\n\treturn ret;\n}\n\nstruct bkey *bch_btree_iter_next(struct btree_iter *iter)\n{\n\treturn __bch_btree_iter_next(iter, btree_iter_cmp);\n\n}\n\nstruct bkey *bch_btree_iter_next_filter(struct btree_iter *iter,\n\t\t\t\t\tstruct btree_keys *b, ptr_filter_fn fn)\n{\n\tstruct bkey *ret;\n\n\tdo {\n\t\tret = bch_btree_iter_next(iter);\n\t} while (ret && fn(b, ret));\n\n\treturn ret;\n}\n\n \n\nvoid bch_bset_sort_state_free(struct bset_sort_state *state)\n{\n\tmempool_exit(&state->pool);\n}\n\nint bch_bset_sort_state_init(struct bset_sort_state *state,\n\t\t\t     unsigned int page_order)\n{\n\tspin_lock_init(&state->time.lock);\n\n\tstate->page_order = page_order;\n\tstate->crit_factor = int_sqrt(1 << page_order);\n\n\treturn mempool_init_page_pool(&state->pool, 1, page_order);\n}\n\nstatic void btree_mergesort(struct btree_keys *b, struct bset *out,\n\t\t\t    struct btree_iter *iter,\n\t\t\t    bool fixup, bool remove_stale)\n{\n\tint i;\n\tstruct bkey *k, *last = NULL;\n\tBKEY_PADDED(k) tmp;\n\tbool (*bad)(struct btree_keys *, const struct bkey *) = remove_stale\n\t\t? bch_ptr_bad\n\t\t: bch_ptr_invalid;\n\n\t \n\tfor (i = iter->used / 2 - 1; i >= 0; --i)\n\t\theap_sift(iter, i, b->ops->sort_cmp);\n\n\twhile (!btree_iter_end(iter)) {\n\t\tif (b->ops->sort_fixup && fixup)\n\t\t\tk = b->ops->sort_fixup(iter, &tmp.k);\n\t\telse\n\t\t\tk = NULL;\n\n\t\tif (!k)\n\t\t\tk = __bch_btree_iter_next(iter, b->ops->sort_cmp);\n\n\t\tif (bad(b, k))\n\t\t\tcontinue;\n\n\t\tif (!last) {\n\t\t\tlast = out->start;\n\t\t\tbkey_copy(last, k);\n\t\t} else if (!bch_bkey_try_merge(b, last, k)) {\n\t\t\tlast = bkey_next(last);\n\t\t\tbkey_copy(last, k);\n\t\t}\n\t}\n\n\tout->keys = last ? (uint64_t *) bkey_next(last) - out->d : 0;\n\n\tpr_debug(\"sorted %i keys\\n\", out->keys);\n}\n\nstatic void __btree_sort(struct btree_keys *b, struct btree_iter *iter,\n\t\t\t unsigned int start, unsigned int order, bool fixup,\n\t\t\t struct bset_sort_state *state)\n{\n\tuint64_t start_time;\n\tbool used_mempool = false;\n\tstruct bset *out = (void *) __get_free_pages(__GFP_NOWARN|GFP_NOWAIT,\n\t\t\t\t\t\t     order);\n\tif (!out) {\n\t\tstruct page *outp;\n\n\t\tBUG_ON(order > state->page_order);\n\n\t\toutp = mempool_alloc(&state->pool, GFP_NOIO);\n\t\tout = page_address(outp);\n\t\tused_mempool = true;\n\t\torder = state->page_order;\n\t}\n\n\tstart_time = local_clock();\n\n\tbtree_mergesort(b, out, iter, fixup, false);\n\tb->nsets = start;\n\n\tif (!start && order == b->page_order) {\n\t\t \n\n\t\tout->magic\t= b->set->data->magic;\n\t\tout->seq\t= b->set->data->seq;\n\t\tout->version\t= b->set->data->version;\n\t\tswap(out, b->set->data);\n\t} else {\n\t\tb->set[start].data->keys = out->keys;\n\t\tmemcpy(b->set[start].data->start, out->start,\n\t\t       (void *) bset_bkey_last(out) - (void *) out->start);\n\t}\n\n\tif (used_mempool)\n\t\tmempool_free(virt_to_page(out), &state->pool);\n\telse\n\t\tfree_pages((unsigned long) out, order);\n\n\tbch_bset_build_written_tree(b);\n\n\tif (!start)\n\t\tbch_time_stats_update(&state->time, start_time);\n}\n\nvoid bch_btree_sort_partial(struct btree_keys *b, unsigned int start,\n\t\t\t    struct bset_sort_state *state)\n{\n\tsize_t order = b->page_order, keys = 0;\n\tstruct btree_iter iter;\n\tint oldsize = bch_count_data(b);\n\n\t__bch_btree_iter_init(b, &iter, NULL, &b->set[start]);\n\n\tif (start) {\n\t\tunsigned int i;\n\n\t\tfor (i = start; i <= b->nsets; i++)\n\t\t\tkeys += b->set[i].data->keys;\n\n\t\torder = get_order(__set_bytes(b->set->data, keys));\n\t}\n\n\t__btree_sort(b, &iter, start, order, false, state);\n\n\tEBUG_ON(oldsize >= 0 && bch_count_data(b) != oldsize);\n}\n\nvoid bch_btree_sort_and_fix_extents(struct btree_keys *b,\n\t\t\t\t    struct btree_iter *iter,\n\t\t\t\t    struct bset_sort_state *state)\n{\n\t__btree_sort(b, iter, 0, b->page_order, true, state);\n}\n\nvoid bch_btree_sort_into(struct btree_keys *b, struct btree_keys *new,\n\t\t\t struct bset_sort_state *state)\n{\n\tuint64_t start_time = local_clock();\n\tstruct btree_iter iter;\n\n\tbch_btree_iter_init(b, &iter, NULL);\n\n\tbtree_mergesort(b, new->set->data, &iter, false, true);\n\n\tbch_time_stats_update(&state->time, start_time);\n\n\tnew->set->size = 0;  \n}\n\n#define SORT_CRIT\t(4096 / sizeof(uint64_t))\n\nvoid bch_btree_sort_lazy(struct btree_keys *b, struct bset_sort_state *state)\n{\n\tunsigned int crit = SORT_CRIT;\n\tint i;\n\n\t \n\tif (!b->nsets)\n\t\tgoto out;\n\n\tfor (i = b->nsets - 1; i >= 0; --i) {\n\t\tcrit *= state->crit_factor;\n\n\t\tif (b->set[i].data->keys < crit) {\n\t\t\tbch_btree_sort_partial(b, i, state);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t \n\tif (b->nsets + 1 == MAX_BSETS) {\n\t\tbch_btree_sort(b, state);\n\t\treturn;\n\t}\n\nout:\n\tbch_bset_build_written_tree(b);\n}\n\nvoid bch_btree_keys_stats(struct btree_keys *b, struct bset_stats *stats)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i <= b->nsets; i++) {\n\t\tstruct bset_tree *t = &b->set[i];\n\t\tsize_t bytes = t->data->keys * sizeof(uint64_t);\n\t\tsize_t j;\n\n\t\tif (bset_written(b, t)) {\n\t\t\tstats->sets_written++;\n\t\t\tstats->bytes_written += bytes;\n\n\t\t\tstats->floats += t->size - 1;\n\n\t\t\tfor (j = 1; j < t->size; j++)\n\t\t\t\tif (t->tree[j].exponent == 127)\n\t\t\t\t\tstats->failed++;\n\t\t} else {\n\t\t\tstats->sets_unwritten++;\n\t\t\tstats->bytes_unwritten += bytes;\n\t\t}\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}