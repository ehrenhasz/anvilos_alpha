{
  "module_name": "bcache.h",
  "hash_id": "0ea11380695382bfd31efc055edc4c413941098eabc43c10d385d3610edbf728",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/bcache/bcache.h",
  "human_readable_source": " \n#ifndef _BCACHE_H\n#define _BCACHE_H\n\n \n\n#define pr_fmt(fmt) \"bcache: %s() \" fmt, __func__\n\n#include <linux/bio.h>\n#include <linux/kobject.h>\n#include <linux/list.h>\n#include <linux/mutex.h>\n#include <linux/rbtree.h>\n#include <linux/rwsem.h>\n#include <linux/refcount.h>\n#include <linux/types.h>\n#include <linux/workqueue.h>\n#include <linux/kthread.h>\n\n#include \"bcache_ondisk.h\"\n#include \"bset.h\"\n#include \"util.h\"\n#include \"closure.h\"\n\nstruct bucket {\n\tatomic_t\tpin;\n\tuint16_t\tprio;\n\tuint8_t\t\tgen;\n\tuint8_t\t\tlast_gc;  \n\tuint16_t\tgc_mark;  \n};\n\n \n\nBITMASK(GC_MARK,\t struct bucket, gc_mark, 0, 2);\n#define GC_MARK_RECLAIMABLE\t1\n#define GC_MARK_DIRTY\t\t2\n#define GC_MARK_METADATA\t3\n#define GC_SECTORS_USED_SIZE\t13\n#define MAX_GC_SECTORS_USED\t(~(~0ULL << GC_SECTORS_USED_SIZE))\nBITMASK(GC_SECTORS_USED, struct bucket, gc_mark, 2, GC_SECTORS_USED_SIZE);\nBITMASK(GC_MOVE, struct bucket, gc_mark, 15, 1);\n\n#include \"journal.h\"\n#include \"stats.h\"\nstruct search;\nstruct btree;\nstruct keybuf;\n\nstruct keybuf_key {\n\tstruct rb_node\t\tnode;\n\tBKEY_PADDED(key);\n\tvoid\t\t\t*private;\n};\n\nstruct keybuf {\n\tstruct bkey\t\tlast_scanned;\n\tspinlock_t\t\tlock;\n\n\t \n\tstruct bkey\t\tstart;\n\tstruct bkey\t\tend;\n\n\tstruct rb_root\t\tkeys;\n\n#define KEYBUF_NR\t\t500\n\tDECLARE_ARRAY_ALLOCATOR(struct keybuf_key, freelist, KEYBUF_NR);\n};\n\nstruct bcache_device {\n\tstruct closure\t\tcl;\n\n\tstruct kobject\t\tkobj;\n\n\tstruct cache_set\t*c;\n\tunsigned int\t\tid;\n#define BCACHEDEVNAME_SIZE\t12\n\tchar\t\t\tname[BCACHEDEVNAME_SIZE];\n\n\tstruct gendisk\t\t*disk;\n\n\tunsigned long\t\tflags;\n#define BCACHE_DEV_CLOSING\t\t0\n#define BCACHE_DEV_DETACHING\t\t1\n#define BCACHE_DEV_UNLINK_DONE\t\t2\n#define BCACHE_DEV_WB_RUNNING\t\t3\n#define BCACHE_DEV_RATE_DW_RUNNING\t4\n\tint\t\t\tnr_stripes;\n#define BCH_MIN_STRIPE_SZ\t\t((4 << 20) >> SECTOR_SHIFT)\n\tunsigned int\t\tstripe_size;\n\tatomic_t\t\t*stripe_sectors_dirty;\n\tunsigned long\t\t*full_dirty_stripes;\n\n\tstruct bio_set\t\tbio_split;\n\n\tunsigned int\t\tdata_csum:1;\n\n\tint (*cache_miss)(struct btree *b, struct search *s,\n\t\t\t  struct bio *bio, unsigned int sectors);\n\tint (*ioctl)(struct bcache_device *d, blk_mode_t mode,\n\t\t     unsigned int cmd, unsigned long arg);\n};\n\nstruct io {\n\t \n\tstruct hlist_node\thash;\n\tstruct list_head\tlru;\n\n\tunsigned long\t\tjiffies;\n\tunsigned int\t\tsequential;\n\tsector_t\t\tlast;\n};\n\nenum stop_on_failure {\n\tBCH_CACHED_DEV_STOP_AUTO = 0,\n\tBCH_CACHED_DEV_STOP_ALWAYS,\n\tBCH_CACHED_DEV_STOP_MODE_MAX,\n};\n\nstruct cached_dev {\n\tstruct list_head\tlist;\n\tstruct bcache_device\tdisk;\n\tstruct block_device\t*bdev;\n\n\tstruct cache_sb\t\tsb;\n\tstruct cache_sb_disk\t*sb_disk;\n\tstruct bio\t\tsb_bio;\n\tstruct bio_vec\t\tsb_bv[1];\n\tstruct closure\t\tsb_write;\n\tstruct semaphore\tsb_write_mutex;\n\n\t \n\trefcount_t\t\tcount;\n\tstruct work_struct\tdetach;\n\n\t \n\tatomic_t\t\trunning;\n\n\t \n\tstruct rw_semaphore\twriteback_lock;\n\n\t \n\tatomic_t\t\thas_dirty;\n\n#define BCH_CACHE_READA_ALL\t\t0\n#define BCH_CACHE_READA_META_ONLY\t1\n\tunsigned int\t\tcache_readahead_policy;\n\tstruct bch_ratelimit\twriteback_rate;\n\tstruct delayed_work\twriteback_rate_update;\n\n\t \n\tstruct semaphore\tin_flight;\n\tstruct task_struct\t*writeback_thread;\n\tstruct workqueue_struct\t*writeback_write_wq;\n\n\tstruct keybuf\t\twriteback_keys;\n\n\tstruct task_struct\t*status_update_thread;\n\t \n\tstruct closure_waitlist writeback_ordering_wait;\n\tatomic_t\t\twriteback_sequence_next;\n\n\t \n#define RECENT_IO_BITS\t7\n#define RECENT_IO\t(1 << RECENT_IO_BITS)\n\tstruct io\t\tio[RECENT_IO];\n\tstruct hlist_head\tio_hash[RECENT_IO + 1];\n\tstruct list_head\tio_lru;\n\tspinlock_t\t\tio_lock;\n\n\tstruct cache_accounting\taccounting;\n\n\t \n\tunsigned int\t\tsequential_cutoff;\n\n\tunsigned int\t\tio_disable:1;\n\tunsigned int\t\tverify:1;\n\tunsigned int\t\tbypass_torture_test:1;\n\n\tunsigned int\t\tpartial_stripes_expensive:1;\n\tunsigned int\t\twriteback_metadata:1;\n\tunsigned int\t\twriteback_running:1;\n\tunsigned int\t\twriteback_consider_fragment:1;\n\tunsigned char\t\twriteback_percent;\n\tunsigned int\t\twriteback_delay;\n\n\tuint64_t\t\twriteback_rate_target;\n\tint64_t\t\t\twriteback_rate_proportional;\n\tint64_t\t\t\twriteback_rate_integral;\n\tint64_t\t\t\twriteback_rate_integral_scaled;\n\tint32_t\t\t\twriteback_rate_change;\n\n\tunsigned int\t\twriteback_rate_update_seconds;\n\tunsigned int\t\twriteback_rate_i_term_inverse;\n\tunsigned int\t\twriteback_rate_p_term_inverse;\n\tunsigned int\t\twriteback_rate_fp_term_low;\n\tunsigned int\t\twriteback_rate_fp_term_mid;\n\tunsigned int\t\twriteback_rate_fp_term_high;\n\tunsigned int\t\twriteback_rate_minimum;\n\n\tenum stop_on_failure\tstop_when_cache_set_failed;\n#define DEFAULT_CACHED_DEV_ERROR_LIMIT\t64\n\tatomic_t\t\tio_errors;\n\tunsigned int\t\terror_limit;\n\tunsigned int\t\toffline_seconds;\n\n\t \n#define BCH_WBRATE_UPDATE_MAX_SKIPS\t15\n\tunsigned int\t\trate_update_retry;\n};\n\nenum alloc_reserve {\n\tRESERVE_BTREE,\n\tRESERVE_PRIO,\n\tRESERVE_MOVINGGC,\n\tRESERVE_NONE,\n\tRESERVE_NR,\n};\n\nstruct cache {\n\tstruct cache_set\t*set;\n\tstruct cache_sb\t\tsb;\n\tstruct cache_sb_disk\t*sb_disk;\n\tstruct bio\t\tsb_bio;\n\tstruct bio_vec\t\tsb_bv[1];\n\n\tstruct kobject\t\tkobj;\n\tstruct block_device\t*bdev;\n\n\tstruct task_struct\t*alloc_thread;\n\n\tstruct closure\t\tprio;\n\tstruct prio_set\t\t*disk_buckets;\n\n\t \n\tuint64_t\t\t*prio_buckets;\n\tuint64_t\t\t*prio_last_buckets;\n\n\t \n\tDECLARE_FIFO(long, free)[RESERVE_NR];\n\tDECLARE_FIFO(long, free_inc);\n\n\tsize_t\t\t\tfifo_last_bucket;\n\n\t \n\tstruct bucket\t\t*buckets;\n\n\tDECLARE_HEAP(struct bucket *, heap);\n\n\t \n\tunsigned int\t\tinvalidate_needs_gc;\n\n\tbool\t\t\tdiscard;  \n\n\tstruct journal_device\tjournal;\n\n\t \n#define IO_ERROR_SHIFT\t\t20\n\tatomic_t\t\tio_errors;\n\tatomic_t\t\tio_count;\n\n\tatomic_long_t\t\tmeta_sectors_written;\n\tatomic_long_t\t\tbtree_sectors_written;\n\tatomic_long_t\t\tsectors_written;\n};\n\nstruct gc_stat {\n\tsize_t\t\t\tnodes;\n\tsize_t\t\t\tnodes_pre;\n\tsize_t\t\t\tkey_bytes;\n\n\tsize_t\t\t\tnkeys;\n\tuint64_t\t\tdata;\t \n\tunsigned int\t\tin_use;  \n};\n\n \n#define CACHE_SET_UNREGISTERING\t\t0\n#define\tCACHE_SET_STOPPING\t\t1\n#define\tCACHE_SET_RUNNING\t\t2\n#define CACHE_SET_IO_DISABLE\t\t3\n\nstruct cache_set {\n\tstruct closure\t\tcl;\n\n\tstruct list_head\tlist;\n\tstruct kobject\t\tkobj;\n\tstruct kobject\t\tinternal;\n\tstruct dentry\t\t*debug;\n\tstruct cache_accounting accounting;\n\n\tunsigned long\t\tflags;\n\tatomic_t\t\tidle_counter;\n\tatomic_t\t\tat_max_writeback_rate;\n\n\tstruct cache\t\t*cache;\n\n\tstruct bcache_device\t**devices;\n\tunsigned int\t\tdevices_max_used;\n\tatomic_t\t\tattached_dev_nr;\n\tstruct list_head\tcached_devs;\n\tuint64_t\t\tcached_dev_sectors;\n\tatomic_long_t\t\tflash_dev_dirty_sectors;\n\tstruct closure\t\tcaching;\n\n\tstruct closure\t\tsb_write;\n\tstruct semaphore\tsb_write_mutex;\n\n\tmempool_t\t\tsearch;\n\tmempool_t\t\tbio_meta;\n\tstruct bio_set\t\tbio_split;\n\n\t \n\tstruct shrinker\t\tshrink;\n\n\t \n\tstruct mutex\t\tbucket_lock;\n\n\t \n\tunsigned short\t\tbucket_bits;\n\n\t \n\tunsigned short\t\tblock_bits;\n\n\t \n\tunsigned int\t\tbtree_pages;\n\n\t \n\tstruct list_head\tbtree_cache;\n\tstruct list_head\tbtree_cache_freeable;\n\tstruct list_head\tbtree_cache_freed;\n\n\t \n\tunsigned int\t\tbtree_cache_used;\n\n\t \n\twait_queue_head_t\tbtree_cache_wait;\n\tstruct task_struct\t*btree_cache_alloc_lock;\n\tspinlock_t\t\tbtree_cannibalize_lock;\n\n\t \n\tatomic_t\t\tprio_blocked;\n\twait_queue_head_t\tbucket_wait;\n\n\t \n\tatomic_t\t\trescale;\n\t \n\tatomic_t\t\tsearch_inflight;\n\t \n\tuint16_t\t\tmin_prio;\n\n\t \n\tuint8_t\t\t\tneed_gc;\n\tstruct gc_stat\t\tgc_stats;\n\tsize_t\t\t\tnbuckets;\n\tsize_t\t\t\tavail_nbuckets;\n\n\tstruct task_struct\t*gc_thread;\n\t \n\tstruct bkey\t\tgc_done;\n\n\t \n#define BCH_ENABLE_AUTO_GC\t1\n#define BCH_DO_AUTO_GC\t\t2\n\tuint8_t\t\t\tgc_after_writeback;\n\n\t \n\tint\t\t\tgc_mark_valid;\n\n\t \n\tatomic_t\t\tsectors_to_gc;\n\twait_queue_head_t\tgc_wait;\n\n\tstruct keybuf\t\tmoving_gc_keys;\n\t \n\tstruct semaphore\tmoving_in_flight;\n\n\tstruct workqueue_struct\t*moving_gc_wq;\n\n\tstruct btree\t\t*root;\n\n#ifdef CONFIG_BCACHE_DEBUG\n\tstruct btree\t\t*verify_data;\n\tstruct bset\t\t*verify_ondisk;\n\tstruct mutex\t\tverify_lock;\n#endif\n\n\tuint8_t\t\t\tset_uuid[16];\n\tunsigned int\t\tnr_uuids;\n\tstruct uuid_entry\t*uuids;\n\tBKEY_PADDED(uuid_bucket);\n\tstruct closure\t\tuuid_write;\n\tstruct semaphore\tuuid_write_mutex;\n\n\t \n\tmempool_t\t\tfill_iter;\n\n\tstruct bset_sort_state\tsort;\n\n\t \n\tstruct list_head\tdata_buckets;\n\tspinlock_t\t\tdata_bucket_lock;\n\n\tstruct journal\t\tjournal;\n\n#define CONGESTED_MAX\t\t1024\n\tunsigned int\t\tcongested_last_us;\n\tatomic_t\t\tcongested;\n\n\t \n\tunsigned int\t\tcongested_read_threshold_us;\n\tunsigned int\t\tcongested_write_threshold_us;\n\n\tstruct time_stats\tbtree_gc_time;\n\tstruct time_stats\tbtree_split_time;\n\tstruct time_stats\tbtree_read_time;\n\n\tatomic_long_t\t\tcache_read_races;\n\tatomic_long_t\t\twriteback_keys_done;\n\tatomic_long_t\t\twriteback_keys_failed;\n\n\tatomic_long_t\t\treclaim;\n\tatomic_long_t\t\treclaimed_journal_buckets;\n\tatomic_long_t\t\tflush_write;\n\n\tenum\t\t\t{\n\t\tON_ERROR_UNREGISTER,\n\t\tON_ERROR_PANIC,\n\t}\t\t\ton_error;\n#define DEFAULT_IO_ERROR_LIMIT 8\n\tunsigned int\t\terror_limit;\n\tunsigned int\t\terror_decay;\n\n\tunsigned short\t\tjournal_delay_ms;\n\tbool\t\t\texpensive_debug_checks;\n\tunsigned int\t\tverify:1;\n\tunsigned int\t\tkey_merging_disabled:1;\n\tunsigned int\t\tgc_always_rewrite:1;\n\tunsigned int\t\tshrinker_disabled:1;\n\tunsigned int\t\tcopy_gc_enabled:1;\n\tunsigned int\t\tidle_max_writeback_rate_enabled:1;\n\n#define BUCKET_HASH_BITS\t12\n\tstruct hlist_head\tbucket_hash[1 << BUCKET_HASH_BITS];\n};\n\nstruct bbio {\n\tunsigned int\t\tsubmit_time_us;\n\tunion {\n\t\tstruct bkey\tkey;\n\t\tuint64_t\t_pad[3];\n\t\t \n\t};\n\tstruct bio\t\tbio;\n};\n\n#define BTREE_PRIO\t\tUSHRT_MAX\n#define INITIAL_PRIO\t\t32768U\n\n#define btree_bytes(c)\t\t((c)->btree_pages * PAGE_SIZE)\n#define btree_blocks(b)\t\t\t\t\t\t\t\\\n\t((unsigned int) (KEY_SIZE(&b->key) >> (b)->c->block_bits))\n\n#define btree_default_blocks(c)\t\t\t\t\t\t\\\n\t((unsigned int) ((PAGE_SECTORS * (c)->btree_pages) >> (c)->block_bits))\n\n#define bucket_bytes(ca)\t((ca)->sb.bucket_size << 9)\n#define block_bytes(ca)\t\t((ca)->sb.block_size << 9)\n\nstatic inline unsigned int meta_bucket_pages(struct cache_sb *sb)\n{\n\tunsigned int n, max_pages;\n\n\tmax_pages = min_t(unsigned int,\n\t\t\t  __rounddown_pow_of_two(USHRT_MAX) / PAGE_SECTORS,\n\t\t\t  MAX_ORDER_NR_PAGES);\n\n\tn = sb->bucket_size / PAGE_SECTORS;\n\tif (n > max_pages)\n\t\tn = max_pages;\n\n\treturn n;\n}\n\nstatic inline unsigned int meta_bucket_bytes(struct cache_sb *sb)\n{\n\treturn meta_bucket_pages(sb) << PAGE_SHIFT;\n}\n\n#define prios_per_bucket(ca)\t\t\t\t\t\t\\\n\t((meta_bucket_bytes(&(ca)->sb) - sizeof(struct prio_set)) /\t\\\n\t sizeof(struct bucket_disk))\n\n#define prio_buckets(ca)\t\t\t\t\t\t\\\n\tDIV_ROUND_UP((size_t) (ca)->sb.nbuckets, prios_per_bucket(ca))\n\nstatic inline size_t sector_to_bucket(struct cache_set *c, sector_t s)\n{\n\treturn s >> c->bucket_bits;\n}\n\nstatic inline sector_t bucket_to_sector(struct cache_set *c, size_t b)\n{\n\treturn ((sector_t) b) << c->bucket_bits;\n}\n\nstatic inline sector_t bucket_remainder(struct cache_set *c, sector_t s)\n{\n\treturn s & (c->cache->sb.bucket_size - 1);\n}\n\nstatic inline size_t PTR_BUCKET_NR(struct cache_set *c,\n\t\t\t\t   const struct bkey *k,\n\t\t\t\t   unsigned int ptr)\n{\n\treturn sector_to_bucket(c, PTR_OFFSET(k, ptr));\n}\n\nstatic inline struct bucket *PTR_BUCKET(struct cache_set *c,\n\t\t\t\t\tconst struct bkey *k,\n\t\t\t\t\tunsigned int ptr)\n{\n\treturn c->cache->buckets + PTR_BUCKET_NR(c, k, ptr);\n}\n\nstatic inline uint8_t gen_after(uint8_t a, uint8_t b)\n{\n\tuint8_t r = a - b;\n\n\treturn r > 128U ? 0 : r;\n}\n\nstatic inline uint8_t ptr_stale(struct cache_set *c, const struct bkey *k,\n\t\t\t\tunsigned int i)\n{\n\treturn gen_after(PTR_BUCKET(c, k, i)->gen, PTR_GEN(k, i));\n}\n\nstatic inline bool ptr_available(struct cache_set *c, const struct bkey *k,\n\t\t\t\t unsigned int i)\n{\n\treturn (PTR_DEV(k, i) < MAX_CACHES_PER_SET) && c->cache;\n}\n\n \n\n \n#define csum_set(i)\t\t\t\t\t\t\t\\\n\tbch_crc64(((void *) (i)) + sizeof(uint64_t),\t\t\t\\\n\t\t  ((void *) bset_bkey_last(i)) -\t\t\t\\\n\t\t  (((void *) (i)) + sizeof(uint64_t)))\n\n \n\n#define btree_bug(b, ...)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (bch_cache_set_error((b)->c, __VA_ARGS__))\t\t\t\\\n\t\tdump_stack();\t\t\t\t\t\t\\\n} while (0)\n\n#define cache_bug(c, ...)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (bch_cache_set_error(c, __VA_ARGS__))\t\t\t\\\n\t\tdump_stack();\t\t\t\t\t\t\\\n} while (0)\n\n#define btree_bug_on(cond, b, ...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (cond)\t\t\t\t\t\t\t\\\n\t\tbtree_bug(b, __VA_ARGS__);\t\t\t\t\\\n} while (0)\n\n#define cache_bug_on(cond, c, ...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (cond)\t\t\t\t\t\t\t\\\n\t\tcache_bug(c, __VA_ARGS__);\t\t\t\t\\\n} while (0)\n\n#define cache_set_err_on(cond, c, ...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (cond)\t\t\t\t\t\t\t\\\n\t\tbch_cache_set_error(c, __VA_ARGS__);\t\t\t\\\n} while (0)\n\n \n\n#define for_each_bucket(b, ca)\t\t\t\t\t\t\\\n\tfor (b = (ca)->buckets + (ca)->sb.first_bucket;\t\t\t\\\n\t     b < (ca)->buckets + (ca)->sb.nbuckets; b++)\n\nstatic inline void cached_dev_put(struct cached_dev *dc)\n{\n\tif (refcount_dec_and_test(&dc->count))\n\t\tschedule_work(&dc->detach);\n}\n\nstatic inline bool cached_dev_get(struct cached_dev *dc)\n{\n\tif (!refcount_inc_not_zero(&dc->count))\n\t\treturn false;\n\n\t \n\tsmp_mb__after_atomic();\n\treturn true;\n}\n\n \n\nstatic inline uint8_t bucket_gc_gen(struct bucket *b)\n{\n\treturn b->gen - b->last_gc;\n}\n\n#define BUCKET_GC_GEN_MAX\t96U\n\n#define kobj_attribute_write(n, fn)\t\t\t\t\t\\\n\tstatic struct kobj_attribute ksysfs_##n = __ATTR(n, 0200, NULL, fn)\n\n#define kobj_attribute_rw(n, show, store)\t\t\t\t\\\n\tstatic struct kobj_attribute ksysfs_##n =\t\t\t\\\n\t\t__ATTR(n, 0600, show, store)\n\nstatic inline void wake_up_allocators(struct cache_set *c)\n{\n\tstruct cache *ca = c->cache;\n\n\twake_up_process(ca->alloc_thread);\n}\n\nstatic inline void closure_bio_submit(struct cache_set *c,\n\t\t\t\t      struct bio *bio,\n\t\t\t\t      struct closure *cl)\n{\n\tclosure_get(cl);\n\tif (unlikely(test_bit(CACHE_SET_IO_DISABLE, &c->flags))) {\n\t\tbio->bi_status = BLK_STS_IOERR;\n\t\tbio_endio(bio);\n\t\treturn;\n\t}\n\tsubmit_bio_noacct(bio);\n}\n\n \nstatic inline void wait_for_kthread_stop(void)\n{\n\twhile (!kthread_should_stop()) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tschedule();\n\t}\n}\n\n \n\nvoid bch_count_backing_io_errors(struct cached_dev *dc, struct bio *bio);\nvoid bch_count_io_errors(struct cache *ca, blk_status_t error,\n\t\t\t int is_read, const char *m);\nvoid bch_bbio_count_io_errors(struct cache_set *c, struct bio *bio,\n\t\t\t      blk_status_t error, const char *m);\nvoid bch_bbio_endio(struct cache_set *c, struct bio *bio,\n\t\t    blk_status_t error, const char *m);\nvoid bch_bbio_free(struct bio *bio, struct cache_set *c);\nstruct bio *bch_bbio_alloc(struct cache_set *c);\n\nvoid __bch_submit_bbio(struct bio *bio, struct cache_set *c);\nvoid bch_submit_bbio(struct bio *bio, struct cache_set *c,\n\t\t     struct bkey *k, unsigned int ptr);\n\nuint8_t bch_inc_gen(struct cache *ca, struct bucket *b);\nvoid bch_rescale_priorities(struct cache_set *c, int sectors);\n\nbool bch_can_invalidate_bucket(struct cache *ca, struct bucket *b);\nvoid __bch_invalidate_one_bucket(struct cache *ca, struct bucket *b);\n\nvoid __bch_bucket_free(struct cache *ca, struct bucket *b);\nvoid bch_bucket_free(struct cache_set *c, struct bkey *k);\n\nlong bch_bucket_alloc(struct cache *ca, unsigned int reserve, bool wait);\nint __bch_bucket_alloc_set(struct cache_set *c, unsigned int reserve,\n\t\t\t   struct bkey *k, bool wait);\nint bch_bucket_alloc_set(struct cache_set *c, unsigned int reserve,\n\t\t\t struct bkey *k, bool wait);\nbool bch_alloc_sectors(struct cache_set *c, struct bkey *k,\n\t\t       unsigned int sectors, unsigned int write_point,\n\t\t       unsigned int write_prio, bool wait);\nbool bch_cached_dev_error(struct cached_dev *dc);\n\n__printf(2, 3)\nbool bch_cache_set_error(struct cache_set *c, const char *fmt, ...);\n\nint bch_prio_write(struct cache *ca, bool wait);\nvoid bch_write_bdev_super(struct cached_dev *dc, struct closure *parent);\n\nextern struct workqueue_struct *bcache_wq;\nextern struct workqueue_struct *bch_journal_wq;\nextern struct workqueue_struct *bch_flush_wq;\nextern struct mutex bch_register_lock;\nextern struct list_head bch_cache_sets;\n\nextern const struct kobj_type bch_cached_dev_ktype;\nextern const struct kobj_type bch_flash_dev_ktype;\nextern const struct kobj_type bch_cache_set_ktype;\nextern const struct kobj_type bch_cache_set_internal_ktype;\nextern const struct kobj_type bch_cache_ktype;\n\nvoid bch_cached_dev_release(struct kobject *kobj);\nvoid bch_flash_dev_release(struct kobject *kobj);\nvoid bch_cache_set_release(struct kobject *kobj);\nvoid bch_cache_release(struct kobject *kobj);\n\nint bch_uuid_write(struct cache_set *c);\nvoid bcache_write_super(struct cache_set *c);\n\nint bch_flash_dev_create(struct cache_set *c, uint64_t size);\n\nint bch_cached_dev_attach(struct cached_dev *dc, struct cache_set *c,\n\t\t\t  uint8_t *set_uuid);\nvoid bch_cached_dev_detach(struct cached_dev *dc);\nint bch_cached_dev_run(struct cached_dev *dc);\nvoid bcache_device_stop(struct bcache_device *d);\n\nvoid bch_cache_set_unregister(struct cache_set *c);\nvoid bch_cache_set_stop(struct cache_set *c);\n\nstruct cache_set *bch_cache_set_alloc(struct cache_sb *sb);\nvoid bch_btree_cache_free(struct cache_set *c);\nint bch_btree_cache_alloc(struct cache_set *c);\nvoid bch_moving_init_cache_set(struct cache_set *c);\nint bch_open_buckets_alloc(struct cache_set *c);\nvoid bch_open_buckets_free(struct cache_set *c);\n\nint bch_cache_allocator_start(struct cache *ca);\n\nvoid bch_debug_exit(void);\nvoid bch_debug_init(void);\nvoid bch_request_exit(void);\nint bch_request_init(void);\nvoid bch_btree_exit(void);\nint bch_btree_init(void);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}