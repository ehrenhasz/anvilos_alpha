{
  "module_name": "btree.c",
  "hash_id": "fdd2a8195484081cf416c892c5f96c5fd344fd92a2b0bf69bc4b9d5e35a38e7e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/bcache/btree.c",
  "human_readable_source": "\n \n\n#include \"bcache.h\"\n#include \"btree.h\"\n#include \"debug.h\"\n#include \"extents.h\"\n\n#include <linux/slab.h>\n#include <linux/bitops.h>\n#include <linux/hash.h>\n#include <linux/kthread.h>\n#include <linux/prefetch.h>\n#include <linux/random.h>\n#include <linux/rcupdate.h>\n#include <linux/sched/clock.h>\n#include <linux/rculist.h>\n#include <linux/delay.h>\n#include <trace/events/bcache.h>\n\n \n\n#define MAX_NEED_GC\t\t64\n#define MAX_SAVE_PRIO\t\t72\n#define MAX_GC_TIMES\t\t100\n#define MIN_GC_NODES\t\t100\n#define GC_SLEEP_MS\t\t100\n\n#define PTR_DIRTY_BIT\t\t(((uint64_t) 1 << 36))\n\n#define PTR_HASH(c, k)\t\t\t\t\t\t\t\\\n\t(((k)->ptr[0] >> c->bucket_bits) | PTR_GEN(k, 0))\n\nstatic struct workqueue_struct *btree_io_wq;\n\n#define insert_lock(s, b)\t((b)->level <= (s)->lock)\n\n\nstatic inline struct bset *write_block(struct btree *b)\n{\n\treturn ((void *) btree_bset_first(b)) + b->written * block_bytes(b->c->cache);\n}\n\nstatic void bch_btree_init_next(struct btree *b)\n{\n\t \n\tif (b->level && b->keys.nsets)\n\t\tbch_btree_sort(&b->keys, &b->c->sort);\n\telse\n\t\tbch_btree_sort_lazy(&b->keys, &b->c->sort);\n\n\tif (b->written < btree_blocks(b))\n\t\tbch_bset_init_next(&b->keys, write_block(b),\n\t\t\t\t   bset_magic(&b->c->cache->sb));\n\n}\n\n \n\nvoid bkey_put(struct cache_set *c, struct bkey *k)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < KEY_PTRS(k); i++)\n\t\tif (ptr_available(c, k, i))\n\t\t\tatomic_dec_bug(&PTR_BUCKET(c, k, i)->pin);\n}\n\n \n\nstatic uint64_t btree_csum_set(struct btree *b, struct bset *i)\n{\n\tuint64_t crc = b->key.ptr[0];\n\tvoid *data = (void *) i + 8, *end = bset_bkey_last(i);\n\n\tcrc = crc64_be(crc, data, end - data);\n\treturn crc ^ 0xffffffffffffffffULL;\n}\n\nvoid bch_btree_node_read_done(struct btree *b)\n{\n\tconst char *err = \"bad btree header\";\n\tstruct bset *i = btree_bset_first(b);\n\tstruct btree_iter *iter;\n\n\t \n\titer = mempool_alloc(&b->c->fill_iter, GFP_NOIO);\n\titer->size = b->c->cache->sb.bucket_size / b->c->cache->sb.block_size;\n\titer->used = 0;\n\n#ifdef CONFIG_BCACHE_DEBUG\n\titer->b = &b->keys;\n#endif\n\n\tif (!i->seq)\n\t\tgoto err;\n\n\tfor (;\n\t     b->written < btree_blocks(b) && i->seq == b->keys.set[0].data->seq;\n\t     i = write_block(b)) {\n\t\terr = \"unsupported bset version\";\n\t\tif (i->version > BCACHE_BSET_VERSION)\n\t\t\tgoto err;\n\n\t\terr = \"bad btree header\";\n\t\tif (b->written + set_blocks(i, block_bytes(b->c->cache)) >\n\t\t    btree_blocks(b))\n\t\t\tgoto err;\n\n\t\terr = \"bad magic\";\n\t\tif (i->magic != bset_magic(&b->c->cache->sb))\n\t\t\tgoto err;\n\n\t\terr = \"bad checksum\";\n\t\tswitch (i->version) {\n\t\tcase 0:\n\t\t\tif (i->csum != csum_set(i))\n\t\t\t\tgoto err;\n\t\t\tbreak;\n\t\tcase BCACHE_BSET_VERSION:\n\t\t\tif (i->csum != btree_csum_set(b, i))\n\t\t\t\tgoto err;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = \"empty set\";\n\t\tif (i != b->keys.set[0].data && !i->keys)\n\t\t\tgoto err;\n\n\t\tbch_btree_iter_push(iter, i->start, bset_bkey_last(i));\n\n\t\tb->written += set_blocks(i, block_bytes(b->c->cache));\n\t}\n\n\terr = \"corrupted btree\";\n\tfor (i = write_block(b);\n\t     bset_sector_offset(&b->keys, i) < KEY_SIZE(&b->key);\n\t     i = ((void *) i) + block_bytes(b->c->cache))\n\t\tif (i->seq == b->keys.set[0].data->seq)\n\t\t\tgoto err;\n\n\tbch_btree_sort_and_fix_extents(&b->keys, iter, &b->c->sort);\n\n\ti = b->keys.set[0].data;\n\terr = \"short btree key\";\n\tif (b->keys.set[0].size &&\n\t    bkey_cmp(&b->key, &b->keys.set[0].end) < 0)\n\t\tgoto err;\n\n\tif (b->written < btree_blocks(b))\n\t\tbch_bset_init_next(&b->keys, write_block(b),\n\t\t\t\t   bset_magic(&b->c->cache->sb));\nout:\n\tmempool_free(iter, &b->c->fill_iter);\n\treturn;\nerr:\n\tset_btree_node_io_error(b);\n\tbch_cache_set_error(b->c, \"%s at bucket %zu, block %u, %u keys\",\n\t\t\t    err, PTR_BUCKET_NR(b->c, &b->key, 0),\n\t\t\t    bset_block_offset(b, i), i->keys);\n\tgoto out;\n}\n\nstatic void btree_node_read_endio(struct bio *bio)\n{\n\tstruct closure *cl = bio->bi_private;\n\n\tclosure_put(cl);\n}\n\nstatic void bch_btree_node_read(struct btree *b)\n{\n\tuint64_t start_time = local_clock();\n\tstruct closure cl;\n\tstruct bio *bio;\n\n\ttrace_bcache_btree_read(b);\n\n\tclosure_init_stack(&cl);\n\n\tbio = bch_bbio_alloc(b->c);\n\tbio->bi_iter.bi_size = KEY_SIZE(&b->key) << 9;\n\tbio->bi_end_io\t= btree_node_read_endio;\n\tbio->bi_private\t= &cl;\n\tbio->bi_opf = REQ_OP_READ | REQ_META;\n\n\tbch_bio_map(bio, b->keys.set[0].data);\n\n\tbch_submit_bbio(bio, b->c, &b->key, 0);\n\tclosure_sync(&cl);\n\n\tif (bio->bi_status)\n\t\tset_btree_node_io_error(b);\n\n\tbch_bbio_free(bio, b->c);\n\n\tif (btree_node_io_error(b))\n\t\tgoto err;\n\n\tbch_btree_node_read_done(b);\n\tbch_time_stats_update(&b->c->btree_read_time, start_time);\n\n\treturn;\nerr:\n\tbch_cache_set_error(b->c, \"io error reading bucket %zu\",\n\t\t\t    PTR_BUCKET_NR(b->c, &b->key, 0));\n}\n\nstatic void btree_complete_write(struct btree *b, struct btree_write *w)\n{\n\tif (w->prio_blocked &&\n\t    !atomic_sub_return(w->prio_blocked, &b->c->prio_blocked))\n\t\twake_up_allocators(b->c);\n\n\tif (w->journal) {\n\t\tatomic_dec_bug(w->journal);\n\t\t__closure_wake_up(&b->c->journal.wait);\n\t}\n\n\tw->prio_blocked\t= 0;\n\tw->journal\t= NULL;\n}\n\nstatic void btree_node_write_unlock(struct closure *cl)\n{\n\tstruct btree *b = container_of(cl, struct btree, io);\n\n\tup(&b->io_mutex);\n}\n\nstatic void __btree_node_write_done(struct closure *cl)\n{\n\tstruct btree *b = container_of(cl, struct btree, io);\n\tstruct btree_write *w = btree_prev_write(b);\n\n\tbch_bbio_free(b->bio, b->c);\n\tb->bio = NULL;\n\tbtree_complete_write(b, w);\n\n\tif (btree_node_dirty(b))\n\t\tqueue_delayed_work(btree_io_wq, &b->work, 30 * HZ);\n\n\tclosure_return_with_destructor(cl, btree_node_write_unlock);\n}\n\nstatic void btree_node_write_done(struct closure *cl)\n{\n\tstruct btree *b = container_of(cl, struct btree, io);\n\n\tbio_free_pages(b->bio);\n\t__btree_node_write_done(cl);\n}\n\nstatic void btree_node_write_endio(struct bio *bio)\n{\n\tstruct closure *cl = bio->bi_private;\n\tstruct btree *b = container_of(cl, struct btree, io);\n\n\tif (bio->bi_status)\n\t\tset_btree_node_io_error(b);\n\n\tbch_bbio_count_io_errors(b->c, bio, bio->bi_status, \"writing btree\");\n\tclosure_put(cl);\n}\n\nstatic void do_btree_node_write(struct btree *b)\n{\n\tstruct closure *cl = &b->io;\n\tstruct bset *i = btree_bset_last(b);\n\tBKEY_PADDED(key) k;\n\n\ti->version\t= BCACHE_BSET_VERSION;\n\ti->csum\t\t= btree_csum_set(b, i);\n\n\tBUG_ON(b->bio);\n\tb->bio = bch_bbio_alloc(b->c);\n\n\tb->bio->bi_end_io\t= btree_node_write_endio;\n\tb->bio->bi_private\t= cl;\n\tb->bio->bi_iter.bi_size\t= roundup(set_bytes(i), block_bytes(b->c->cache));\n\tb->bio->bi_opf\t\t= REQ_OP_WRITE | REQ_META | REQ_FUA;\n\tbch_bio_map(b->bio, i);\n\n\t \n\n\tbkey_copy(&k.key, &b->key);\n\tSET_PTR_OFFSET(&k.key, 0, PTR_OFFSET(&k.key, 0) +\n\t\t       bset_sector_offset(&b->keys, i));\n\n\tif (!bch_bio_alloc_pages(b->bio, __GFP_NOWARN|GFP_NOWAIT)) {\n\t\tstruct bio_vec *bv;\n\t\tvoid *addr = (void *) ((unsigned long) i & ~(PAGE_SIZE - 1));\n\t\tstruct bvec_iter_all iter_all;\n\n\t\tbio_for_each_segment_all(bv, b->bio, iter_all) {\n\t\t\tmemcpy(page_address(bv->bv_page), addr, PAGE_SIZE);\n\t\t\taddr += PAGE_SIZE;\n\t\t}\n\n\t\tbch_submit_bbio(b->bio, b->c, &k.key, 0);\n\n\t\tcontinue_at(cl, btree_node_write_done, NULL);\n\t} else {\n\t\t \n\t\tb->bio->bi_vcnt = 0;\n\t\tbch_bio_map(b->bio, i);\n\n\t\tbch_submit_bbio(b->bio, b->c, &k.key, 0);\n\n\t\tclosure_sync(cl);\n\t\tcontinue_at_nobarrier(cl, __btree_node_write_done, NULL);\n\t}\n}\n\nvoid __bch_btree_node_write(struct btree *b, struct closure *parent)\n{\n\tstruct bset *i = btree_bset_last(b);\n\n\tlockdep_assert_held(&b->write_lock);\n\n\ttrace_bcache_btree_write(b);\n\n\tBUG_ON(current->bio_list);\n\tBUG_ON(b->written >= btree_blocks(b));\n\tBUG_ON(b->written && !i->keys);\n\tBUG_ON(btree_bset_first(b)->seq != i->seq);\n\tbch_check_keys(&b->keys, \"writing\");\n\n\tcancel_delayed_work(&b->work);\n\n\t \n\tdown(&b->io_mutex);\n\tclosure_init(&b->io, parent ?: &b->c->cl);\n\n\tclear_bit(BTREE_NODE_dirty,\t &b->flags);\n\tchange_bit(BTREE_NODE_write_idx, &b->flags);\n\n\tdo_btree_node_write(b);\n\n\tatomic_long_add(set_blocks(i, block_bytes(b->c->cache)) * b->c->cache->sb.block_size,\n\t\t\t&b->c->cache->btree_sectors_written);\n\n\tb->written += set_blocks(i, block_bytes(b->c->cache));\n}\n\nvoid bch_btree_node_write(struct btree *b, struct closure *parent)\n{\n\tunsigned int nsets = b->keys.nsets;\n\n\tlockdep_assert_held(&b->lock);\n\n\t__bch_btree_node_write(b, parent);\n\n\t \n\tif (nsets && !b->keys.nsets)\n\t\tbch_btree_verify(b);\n\n\tbch_btree_init_next(b);\n}\n\nstatic void bch_btree_node_write_sync(struct btree *b)\n{\n\tstruct closure cl;\n\n\tclosure_init_stack(&cl);\n\n\tmutex_lock(&b->write_lock);\n\tbch_btree_node_write(b, &cl);\n\tmutex_unlock(&b->write_lock);\n\n\tclosure_sync(&cl);\n}\n\nstatic void btree_node_write_work(struct work_struct *w)\n{\n\tstruct btree *b = container_of(to_delayed_work(w), struct btree, work);\n\n\tmutex_lock(&b->write_lock);\n\tif (btree_node_dirty(b))\n\t\t__bch_btree_node_write(b, NULL);\n\tmutex_unlock(&b->write_lock);\n}\n\nstatic void bch_btree_leaf_dirty(struct btree *b, atomic_t *journal_ref)\n{\n\tstruct bset *i = btree_bset_last(b);\n\tstruct btree_write *w = btree_current_write(b);\n\n\tlockdep_assert_held(&b->write_lock);\n\n\tBUG_ON(!b->written);\n\tBUG_ON(!i->keys);\n\n\tif (!btree_node_dirty(b))\n\t\tqueue_delayed_work(btree_io_wq, &b->work, 30 * HZ);\n\n\tset_btree_node_dirty(b);\n\n\t \n\tif (journal_ref) {\n\t\tif (w->journal &&\n\t\t    journal_pin_cmp(b->c, w->journal, journal_ref)) {\n\t\t\tatomic_dec_bug(w->journal);\n\t\t\tw->journal = NULL;\n\t\t}\n\n\t\tif (!w->journal) {\n\t\t\tw->journal = journal_ref;\n\t\t\tatomic_inc(w->journal);\n\t\t}\n\t}\n\n\t \n\tif (set_bytes(i) > PAGE_SIZE - 48 &&\n\t    !current->bio_list)\n\t\tbch_btree_node_write(b, NULL);\n}\n\n \n\n#define mca_reserve(c)\t(((!IS_ERR_OR_NULL(c->root) && c->root->level) \\\n\t\t\t  ? c->root->level : 1) * 8 + 16)\n#define mca_can_free(c)\t\t\t\t\t\t\\\n\tmax_t(int, 0, c->btree_cache_used - mca_reserve(c))\n\nstatic void mca_data_free(struct btree *b)\n{\n\tBUG_ON(b->io_mutex.count != 1);\n\n\tbch_btree_keys_free(&b->keys);\n\n\tb->c->btree_cache_used--;\n\tlist_move(&b->list, &b->c->btree_cache_freed);\n}\n\nstatic void mca_bucket_free(struct btree *b)\n{\n\tBUG_ON(btree_node_dirty(b));\n\n\tb->key.ptr[0] = 0;\n\thlist_del_init_rcu(&b->hash);\n\tlist_move(&b->list, &b->c->btree_cache_freeable);\n}\n\nstatic unsigned int btree_order(struct bkey *k)\n{\n\treturn ilog2(KEY_SIZE(k) / PAGE_SECTORS ?: 1);\n}\n\nstatic void mca_data_alloc(struct btree *b, struct bkey *k, gfp_t gfp)\n{\n\tif (!bch_btree_keys_alloc(&b->keys,\n\t\t\t\t  max_t(unsigned int,\n\t\t\t\t\tilog2(b->c->btree_pages),\n\t\t\t\t\tbtree_order(k)),\n\t\t\t\t  gfp)) {\n\t\tb->c->btree_cache_used++;\n\t\tlist_move(&b->list, &b->c->btree_cache);\n\t} else {\n\t\tlist_move(&b->list, &b->c->btree_cache_freed);\n\t}\n}\n\n#define cmp_int(l, r)\t\t((l > r) - (l < r))\n\n#ifdef CONFIG_PROVE_LOCKING\nstatic int btree_lock_cmp_fn(const struct lockdep_map *_a,\n\t\t\t     const struct lockdep_map *_b)\n{\n\tconst struct btree *a = container_of(_a, struct btree, lock.dep_map);\n\tconst struct btree *b = container_of(_b, struct btree, lock.dep_map);\n\n\treturn -cmp_int(a->level, b->level) ?: bkey_cmp(&a->key, &b->key);\n}\n\nstatic void btree_lock_print_fn(const struct lockdep_map *map)\n{\n\tconst struct btree *b = container_of(map, struct btree, lock.dep_map);\n\n\tprintk(KERN_CONT \" l=%u %llu:%llu\", b->level,\n\t       KEY_INODE(&b->key), KEY_OFFSET(&b->key));\n}\n#endif\n\nstatic struct btree *mca_bucket_alloc(struct cache_set *c,\n\t\t\t\t      struct bkey *k, gfp_t gfp)\n{\n\t \n\tstruct btree *b = kzalloc(sizeof(struct btree), gfp);\n\n\tif (!b)\n\t\treturn NULL;\n\n\tinit_rwsem(&b->lock);\n\tlock_set_cmp_fn(&b->lock, btree_lock_cmp_fn, btree_lock_print_fn);\n\tmutex_init(&b->write_lock);\n\tlockdep_set_novalidate_class(&b->write_lock);\n\tINIT_LIST_HEAD(&b->list);\n\tINIT_DELAYED_WORK(&b->work, btree_node_write_work);\n\tb->c = c;\n\tsema_init(&b->io_mutex, 1);\n\n\tmca_data_alloc(b, k, gfp);\n\treturn b;\n}\n\nstatic int mca_reap(struct btree *b, unsigned int min_order, bool flush)\n{\n\tstruct closure cl;\n\n\tclosure_init_stack(&cl);\n\tlockdep_assert_held(&b->c->bucket_lock);\n\n\tif (!down_write_trylock(&b->lock))\n\t\treturn -ENOMEM;\n\n\tBUG_ON(btree_node_dirty(b) && !b->keys.set[0].data);\n\n\tif (b->keys.page_order < min_order)\n\t\tgoto out_unlock;\n\n\tif (!flush) {\n\t\tif (btree_node_dirty(b))\n\t\t\tgoto out_unlock;\n\n\t\tif (down_trylock(&b->io_mutex))\n\t\t\tgoto out_unlock;\n\t\tup(&b->io_mutex);\n\t}\n\nretry:\n\t \n\tmutex_lock(&b->write_lock);\n\t \n\tif (btree_node_journal_flush(b)) {\n\t\tpr_debug(\"bnode %p is flushing by journal, retry\\n\", b);\n\t\tmutex_unlock(&b->write_lock);\n\t\tudelay(1);\n\t\tgoto retry;\n\t}\n\n\tif (btree_node_dirty(b))\n\t\t__bch_btree_node_write(b, &cl);\n\tmutex_unlock(&b->write_lock);\n\n\tclosure_sync(&cl);\n\n\t \n\tdown(&b->io_mutex);\n\tup(&b->io_mutex);\n\n\treturn 0;\nout_unlock:\n\trw_unlock(true, b);\n\treturn -ENOMEM;\n}\n\nstatic unsigned long bch_mca_scan(struct shrinker *shrink,\n\t\t\t\t  struct shrink_control *sc)\n{\n\tstruct cache_set *c = container_of(shrink, struct cache_set, shrink);\n\tstruct btree *b, *t;\n\tunsigned long i, nr = sc->nr_to_scan;\n\tunsigned long freed = 0;\n\tunsigned int btree_cache_used;\n\n\tif (c->shrinker_disabled)\n\t\treturn SHRINK_STOP;\n\n\tif (c->btree_cache_alloc_lock)\n\t\treturn SHRINK_STOP;\n\n\t \n\tif (sc->gfp_mask & __GFP_IO)\n\t\tmutex_lock(&c->bucket_lock);\n\telse if (!mutex_trylock(&c->bucket_lock))\n\t\treturn -1;\n\n\t \n\tnr /= c->btree_pages;\n\tif (nr == 0)\n\t\tnr = 1;\n\tnr = min_t(unsigned long, nr, mca_can_free(c));\n\n\ti = 0;\n\tbtree_cache_used = c->btree_cache_used;\n\tlist_for_each_entry_safe_reverse(b, t, &c->btree_cache_freeable, list) {\n\t\tif (nr <= 0)\n\t\t\tgoto out;\n\n\t\tif (!mca_reap(b, 0, false)) {\n\t\t\tmca_data_free(b);\n\t\t\trw_unlock(true, b);\n\t\t\tfreed++;\n\t\t}\n\t\tnr--;\n\t\ti++;\n\t}\n\n\tlist_for_each_entry_safe_reverse(b, t, &c->btree_cache, list) {\n\t\tif (nr <= 0 || i >= btree_cache_used)\n\t\t\tgoto out;\n\n\t\tif (!mca_reap(b, 0, false)) {\n\t\t\tmca_bucket_free(b);\n\t\t\tmca_data_free(b);\n\t\t\trw_unlock(true, b);\n\t\t\tfreed++;\n\t\t}\n\n\t\tnr--;\n\t\ti++;\n\t}\nout:\n\tmutex_unlock(&c->bucket_lock);\n\treturn freed * c->btree_pages;\n}\n\nstatic unsigned long bch_mca_count(struct shrinker *shrink,\n\t\t\t\t   struct shrink_control *sc)\n{\n\tstruct cache_set *c = container_of(shrink, struct cache_set, shrink);\n\n\tif (c->shrinker_disabled)\n\t\treturn 0;\n\n\tif (c->btree_cache_alloc_lock)\n\t\treturn 0;\n\n\treturn mca_can_free(c) * c->btree_pages;\n}\n\nvoid bch_btree_cache_free(struct cache_set *c)\n{\n\tstruct btree *b;\n\tstruct closure cl;\n\n\tclosure_init_stack(&cl);\n\n\tif (c->shrink.list.next)\n\t\tunregister_shrinker(&c->shrink);\n\n\tmutex_lock(&c->bucket_lock);\n\n#ifdef CONFIG_BCACHE_DEBUG\n\tif (c->verify_data)\n\t\tlist_move(&c->verify_data->list, &c->btree_cache);\n\n\tfree_pages((unsigned long) c->verify_ondisk, ilog2(meta_bucket_pages(&c->cache->sb)));\n#endif\n\n\tlist_splice(&c->btree_cache_freeable,\n\t\t    &c->btree_cache);\n\n\twhile (!list_empty(&c->btree_cache)) {\n\t\tb = list_first_entry(&c->btree_cache, struct btree, list);\n\n\t\t \n\t\tif (btree_node_dirty(b)) {\n\t\t\tbtree_complete_write(b, btree_current_write(b));\n\t\t\tclear_bit(BTREE_NODE_dirty, &b->flags);\n\t\t}\n\t\tmca_data_free(b);\n\t}\n\n\twhile (!list_empty(&c->btree_cache_freed)) {\n\t\tb = list_first_entry(&c->btree_cache_freed,\n\t\t\t\t     struct btree, list);\n\t\tlist_del(&b->list);\n\t\tcancel_delayed_work_sync(&b->work);\n\t\tkfree(b);\n\t}\n\n\tmutex_unlock(&c->bucket_lock);\n}\n\nint bch_btree_cache_alloc(struct cache_set *c)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < mca_reserve(c); i++)\n\t\tif (!mca_bucket_alloc(c, &ZERO_KEY, GFP_KERNEL))\n\t\t\treturn -ENOMEM;\n\n\tlist_splice_init(&c->btree_cache,\n\t\t\t &c->btree_cache_freeable);\n\n#ifdef CONFIG_BCACHE_DEBUG\n\tmutex_init(&c->verify_lock);\n\n\tc->verify_ondisk = (void *)\n\t\t__get_free_pages(GFP_KERNEL|__GFP_COMP,\n\t\t\t\t ilog2(meta_bucket_pages(&c->cache->sb)));\n\tif (!c->verify_ondisk) {\n\t\t \n\t\treturn -ENOMEM;\n\t}\n\n\tc->verify_data = mca_bucket_alloc(c, &ZERO_KEY, GFP_KERNEL);\n\n\tif (c->verify_data &&\n\t    c->verify_data->keys.set->data)\n\t\tlist_del_init(&c->verify_data->list);\n\telse\n\t\tc->verify_data = NULL;\n#endif\n\n\tc->shrink.count_objects = bch_mca_count;\n\tc->shrink.scan_objects = bch_mca_scan;\n\tc->shrink.seeks = 4;\n\tc->shrink.batch = c->btree_pages * 2;\n\n\tif (register_shrinker(&c->shrink, \"md-bcache:%pU\", c->set_uuid))\n\t\tpr_warn(\"bcache: %s: could not register shrinker\\n\",\n\t\t\t\t__func__);\n\n\treturn 0;\n}\n\n \n\nstatic struct hlist_head *mca_hash(struct cache_set *c, struct bkey *k)\n{\n\treturn &c->bucket_hash[hash_32(PTR_HASH(c, k), BUCKET_HASH_BITS)];\n}\n\nstatic struct btree *mca_find(struct cache_set *c, struct bkey *k)\n{\n\tstruct btree *b;\n\n\trcu_read_lock();\n\thlist_for_each_entry_rcu(b, mca_hash(c, k), hash)\n\t\tif (PTR_HASH(c, &b->key) == PTR_HASH(c, k))\n\t\t\tgoto out;\n\tb = NULL;\nout:\n\trcu_read_unlock();\n\treturn b;\n}\n\nstatic int mca_cannibalize_lock(struct cache_set *c, struct btree_op *op)\n{\n\tspin_lock(&c->btree_cannibalize_lock);\n\tif (likely(c->btree_cache_alloc_lock == NULL)) {\n\t\tc->btree_cache_alloc_lock = current;\n\t} else if (c->btree_cache_alloc_lock != current) {\n\t\tif (op)\n\t\t\tprepare_to_wait(&c->btree_cache_wait, &op->wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tspin_unlock(&c->btree_cannibalize_lock);\n\t\treturn -EINTR;\n\t}\n\tspin_unlock(&c->btree_cannibalize_lock);\n\n\treturn 0;\n}\n\nstatic struct btree *mca_cannibalize(struct cache_set *c, struct btree_op *op,\n\t\t\t\t     struct bkey *k)\n{\n\tstruct btree *b;\n\n\ttrace_bcache_btree_cache_cannibalize(c);\n\n\tif (mca_cannibalize_lock(c, op))\n\t\treturn ERR_PTR(-EINTR);\n\n\tlist_for_each_entry_reverse(b, &c->btree_cache, list)\n\t\tif (!mca_reap(b, btree_order(k), false))\n\t\t\treturn b;\n\n\tlist_for_each_entry_reverse(b, &c->btree_cache, list)\n\t\tif (!mca_reap(b, btree_order(k), true))\n\t\t\treturn b;\n\n\tWARN(1, \"btree cache cannibalize failed\\n\");\n\treturn ERR_PTR(-ENOMEM);\n}\n\n \nvoid bch_cannibalize_unlock(struct cache_set *c)\n{\n\tspin_lock(&c->btree_cannibalize_lock);\n\tif (c->btree_cache_alloc_lock == current) {\n\t\tc->btree_cache_alloc_lock = NULL;\n\t\twake_up(&c->btree_cache_wait);\n\t}\n\tspin_unlock(&c->btree_cannibalize_lock);\n}\n\nstatic struct btree *mca_alloc(struct cache_set *c, struct btree_op *op,\n\t\t\t       struct bkey *k, int level)\n{\n\tstruct btree *b;\n\n\tBUG_ON(current->bio_list);\n\n\tlockdep_assert_held(&c->bucket_lock);\n\n\tif (mca_find(c, k))\n\t\treturn NULL;\n\n\t \n\tlist_for_each_entry(b, &c->btree_cache_freeable, list)\n\t\tif (!mca_reap(b, btree_order(k), false))\n\t\t\tgoto out;\n\n\t \n\tlist_for_each_entry(b, &c->btree_cache_freed, list)\n\t\tif (!mca_reap(b, 0, false)) {\n\t\t\tmca_data_alloc(b, k, __GFP_NOWARN|GFP_NOIO);\n\t\t\tif (!b->keys.set[0].data)\n\t\t\t\tgoto err;\n\t\t\telse\n\t\t\t\tgoto out;\n\t\t}\n\n\tb = mca_bucket_alloc(c, k, __GFP_NOWARN|GFP_NOIO);\n\tif (!b)\n\t\tgoto err;\n\n\tBUG_ON(!down_write_trylock(&b->lock));\n\tif (!b->keys.set->data)\n\t\tgoto err;\nout:\n\tBUG_ON(b->io_mutex.count != 1);\n\n\tbkey_copy(&b->key, k);\n\tlist_move(&b->list, &c->btree_cache);\n\thlist_del_init_rcu(&b->hash);\n\thlist_add_head_rcu(&b->hash, mca_hash(c, k));\n\n\tlock_set_subclass(&b->lock.dep_map, level + 1, _THIS_IP_);\n\tb->parent\t= (void *) ~0UL;\n\tb->flags\t= 0;\n\tb->written\t= 0;\n\tb->level\t= level;\n\n\tif (!b->level)\n\t\tbch_btree_keys_init(&b->keys, &bch_extent_keys_ops,\n\t\t\t\t    &b->c->expensive_debug_checks);\n\telse\n\t\tbch_btree_keys_init(&b->keys, &bch_btree_keys_ops,\n\t\t\t\t    &b->c->expensive_debug_checks);\n\n\treturn b;\nerr:\n\tif (b)\n\t\trw_unlock(true, b);\n\n\tb = mca_cannibalize(c, op, k);\n\tif (!IS_ERR(b))\n\t\tgoto out;\n\n\treturn b;\n}\n\n \nstruct btree *bch_btree_node_get(struct cache_set *c, struct btree_op *op,\n\t\t\t\t struct bkey *k, int level, bool write,\n\t\t\t\t struct btree *parent)\n{\n\tint i = 0;\n\tstruct btree *b;\n\n\tBUG_ON(level < 0);\nretry:\n\tb = mca_find(c, k);\n\n\tif (!b) {\n\t\tif (current->bio_list)\n\t\t\treturn ERR_PTR(-EAGAIN);\n\n\t\tmutex_lock(&c->bucket_lock);\n\t\tb = mca_alloc(c, op, k, level);\n\t\tmutex_unlock(&c->bucket_lock);\n\n\t\tif (!b)\n\t\t\tgoto retry;\n\t\tif (IS_ERR(b))\n\t\t\treturn b;\n\n\t\tbch_btree_node_read(b);\n\n\t\tif (!write)\n\t\t\tdowngrade_write(&b->lock);\n\t} else {\n\t\trw_lock(write, b, level);\n\t\tif (PTR_HASH(c, &b->key) != PTR_HASH(c, k)) {\n\t\t\trw_unlock(write, b);\n\t\t\tgoto retry;\n\t\t}\n\t\tBUG_ON(b->level != level);\n\t}\n\n\tif (btree_node_io_error(b)) {\n\t\trw_unlock(write, b);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\n\tBUG_ON(!b->written);\n\n\tb->parent = parent;\n\n\tfor (; i <= b->keys.nsets && b->keys.set[i].size; i++) {\n\t\tprefetch(b->keys.set[i].tree);\n\t\tprefetch(b->keys.set[i].data);\n\t}\n\n\tfor (; i <= b->keys.nsets; i++)\n\t\tprefetch(b->keys.set[i].data);\n\n\treturn b;\n}\n\nstatic void btree_node_prefetch(struct btree *parent, struct bkey *k)\n{\n\tstruct btree *b;\n\n\tmutex_lock(&parent->c->bucket_lock);\n\tb = mca_alloc(parent->c, NULL, k, parent->level - 1);\n\tmutex_unlock(&parent->c->bucket_lock);\n\n\tif (!IS_ERR_OR_NULL(b)) {\n\t\tb->parent = parent;\n\t\tbch_btree_node_read(b);\n\t\trw_unlock(true, b);\n\t}\n}\n\n \n\nstatic void btree_node_free(struct btree *b)\n{\n\ttrace_bcache_btree_node_free(b);\n\n\tBUG_ON(b == b->c->root);\n\nretry:\n\tmutex_lock(&b->write_lock);\n\t \n\tif (btree_node_journal_flush(b)) {\n\t\tmutex_unlock(&b->write_lock);\n\t\tpr_debug(\"bnode %p journal_flush set, retry\\n\", b);\n\t\tudelay(1);\n\t\tgoto retry;\n\t}\n\n\tif (btree_node_dirty(b)) {\n\t\tbtree_complete_write(b, btree_current_write(b));\n\t\tclear_bit(BTREE_NODE_dirty, &b->flags);\n\t}\n\n\tmutex_unlock(&b->write_lock);\n\n\tcancel_delayed_work(&b->work);\n\n\tmutex_lock(&b->c->bucket_lock);\n\tbch_bucket_free(b->c, &b->key);\n\tmca_bucket_free(b);\n\tmutex_unlock(&b->c->bucket_lock);\n}\n\n \nstruct btree *__bch_btree_node_alloc(struct cache_set *c, struct btree_op *op,\n\t\t\t\t     int level, bool wait,\n\t\t\t\t     struct btree *parent)\n{\n\tBKEY_PADDED(key) k;\n\tstruct btree *b;\n\n\tmutex_lock(&c->bucket_lock);\nretry:\n\t \n\tb = ERR_PTR(-EAGAIN);\n\tif (__bch_bucket_alloc_set(c, RESERVE_BTREE, &k.key, wait))\n\t\tgoto err;\n\n\tbkey_put(c, &k.key);\n\tSET_KEY_SIZE(&k.key, c->btree_pages * PAGE_SECTORS);\n\n\tb = mca_alloc(c, op, &k.key, level);\n\tif (IS_ERR(b))\n\t\tgoto err_free;\n\n\tif (!b) {\n\t\tcache_bug(c,\n\t\t\t\"Tried to allocate bucket that was in btree cache\");\n\t\tgoto retry;\n\t}\n\n\tb->parent = parent;\n\tbch_bset_init_next(&b->keys, b->keys.set->data, bset_magic(&b->c->cache->sb));\n\n\tmutex_unlock(&c->bucket_lock);\n\n\ttrace_bcache_btree_node_alloc(b);\n\treturn b;\nerr_free:\n\tbch_bucket_free(c, &k.key);\nerr:\n\tmutex_unlock(&c->bucket_lock);\n\n\ttrace_bcache_btree_node_alloc_fail(c);\n\treturn b;\n}\n\nstatic struct btree *bch_btree_node_alloc(struct cache_set *c,\n\t\t\t\t\t  struct btree_op *op, int level,\n\t\t\t\t\t  struct btree *parent)\n{\n\treturn __bch_btree_node_alloc(c, op, level, op != NULL, parent);\n}\n\nstatic struct btree *btree_node_alloc_replacement(struct btree *b,\n\t\t\t\t\t\t  struct btree_op *op)\n{\n\tstruct btree *n = bch_btree_node_alloc(b->c, op, b->level, b->parent);\n\n\tif (!IS_ERR(n)) {\n\t\tmutex_lock(&n->write_lock);\n\t\tbch_btree_sort_into(&b->keys, &n->keys, &b->c->sort);\n\t\tbkey_copy_key(&n->key, &b->key);\n\t\tmutex_unlock(&n->write_lock);\n\t}\n\n\treturn n;\n}\n\nstatic void make_btree_freeing_key(struct btree *b, struct bkey *k)\n{\n\tunsigned int i;\n\n\tmutex_lock(&b->c->bucket_lock);\n\n\tatomic_inc(&b->c->prio_blocked);\n\n\tbkey_copy(k, &b->key);\n\tbkey_copy_key(k, &ZERO_KEY);\n\n\tfor (i = 0; i < KEY_PTRS(k); i++)\n\t\tSET_PTR_GEN(k, i,\n\t\t\t    bch_inc_gen(b->c->cache,\n\t\t\t\t\tPTR_BUCKET(b->c, &b->key, i)));\n\n\tmutex_unlock(&b->c->bucket_lock);\n}\n\nstatic int btree_check_reserve(struct btree *b, struct btree_op *op)\n{\n\tstruct cache_set *c = b->c;\n\tstruct cache *ca = c->cache;\n\tunsigned int reserve = (c->root->level - b->level) * 2 + 1;\n\n\tmutex_lock(&c->bucket_lock);\n\n\tif (fifo_used(&ca->free[RESERVE_BTREE]) < reserve) {\n\t\tif (op)\n\t\t\tprepare_to_wait(&c->btree_cache_wait, &op->wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tmutex_unlock(&c->bucket_lock);\n\t\treturn -EINTR;\n\t}\n\n\tmutex_unlock(&c->bucket_lock);\n\n\treturn mca_cannibalize_lock(b->c, op);\n}\n\n \n\nstatic uint8_t __bch_btree_mark_key(struct cache_set *c, int level,\n\t\t\t\t    struct bkey *k)\n{\n\tuint8_t stale = 0;\n\tunsigned int i;\n\tstruct bucket *g;\n\n\t \n\tif (!bkey_cmp(k, &ZERO_KEY))\n\t\treturn stale;\n\n\tfor (i = 0; i < KEY_PTRS(k); i++) {\n\t\tif (!ptr_available(c, k, i))\n\t\t\tcontinue;\n\n\t\tg = PTR_BUCKET(c, k, i);\n\n\t\tif (gen_after(g->last_gc, PTR_GEN(k, i)))\n\t\t\tg->last_gc = PTR_GEN(k, i);\n\n\t\tif (ptr_stale(c, k, i)) {\n\t\t\tstale = max(stale, ptr_stale(c, k, i));\n\t\t\tcontinue;\n\t\t}\n\n\t\tcache_bug_on(GC_MARK(g) &&\n\t\t\t     (GC_MARK(g) == GC_MARK_METADATA) != (level != 0),\n\t\t\t     c, \"inconsistent ptrs: mark = %llu, level = %i\",\n\t\t\t     GC_MARK(g), level);\n\n\t\tif (level)\n\t\t\tSET_GC_MARK(g, GC_MARK_METADATA);\n\t\telse if (KEY_DIRTY(k))\n\t\t\tSET_GC_MARK(g, GC_MARK_DIRTY);\n\t\telse if (!GC_MARK(g))\n\t\t\tSET_GC_MARK(g, GC_MARK_RECLAIMABLE);\n\n\t\t \n\t\tSET_GC_SECTORS_USED(g, min_t(unsigned int,\n\t\t\t\t\t     GC_SECTORS_USED(g) + KEY_SIZE(k),\n\t\t\t\t\t     MAX_GC_SECTORS_USED));\n\n\t\tBUG_ON(!GC_SECTORS_USED(g));\n\t}\n\n\treturn stale;\n}\n\n#define btree_mark_key(b, k)\t__bch_btree_mark_key(b->c, b->level, k)\n\nvoid bch_initial_mark_key(struct cache_set *c, int level, struct bkey *k)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < KEY_PTRS(k); i++)\n\t\tif (ptr_available(c, k, i) &&\n\t\t    !ptr_stale(c, k, i)) {\n\t\t\tstruct bucket *b = PTR_BUCKET(c, k, i);\n\n\t\t\tb->gen = PTR_GEN(k, i);\n\n\t\t\tif (level && bkey_cmp(k, &ZERO_KEY))\n\t\t\t\tb->prio = BTREE_PRIO;\n\t\t\telse if (!level && b->prio == BTREE_PRIO)\n\t\t\t\tb->prio = INITIAL_PRIO;\n\t\t}\n\n\t__bch_btree_mark_key(c, level, k);\n}\n\nvoid bch_update_bucket_in_use(struct cache_set *c, struct gc_stat *stats)\n{\n\tstats->in_use = (c->nbuckets - c->avail_nbuckets) * 100 / c->nbuckets;\n}\n\nstatic bool btree_gc_mark_node(struct btree *b, struct gc_stat *gc)\n{\n\tuint8_t stale = 0;\n\tunsigned int keys = 0, good_keys = 0;\n\tstruct bkey *k;\n\tstruct btree_iter iter;\n\tstruct bset_tree *t;\n\n\tgc->nodes++;\n\n\tfor_each_key_filter(&b->keys, k, &iter, bch_ptr_invalid) {\n\t\tstale = max(stale, btree_mark_key(b, k));\n\t\tkeys++;\n\n\t\tif (bch_ptr_bad(&b->keys, k))\n\t\t\tcontinue;\n\n\t\tgc->key_bytes += bkey_u64s(k);\n\t\tgc->nkeys++;\n\t\tgood_keys++;\n\n\t\tgc->data += KEY_SIZE(k);\n\t}\n\n\tfor (t = b->keys.set; t <= &b->keys.set[b->keys.nsets]; t++)\n\t\tbtree_bug_on(t->size &&\n\t\t\t     bset_written(&b->keys, t) &&\n\t\t\t     bkey_cmp(&b->key, &t->end) < 0,\n\t\t\t     b, \"found short btree key in gc\");\n\n\tif (b->c->gc_always_rewrite)\n\t\treturn true;\n\n\tif (stale > 10)\n\t\treturn true;\n\n\tif ((keys - good_keys) * 2 > keys)\n\t\treturn true;\n\n\treturn false;\n}\n\n#define GC_MERGE_NODES\t4U\n\nstruct gc_merge_info {\n\tstruct btree\t*b;\n\tunsigned int\tkeys;\n};\n\nstatic int bch_btree_insert_node(struct btree *b, struct btree_op *op,\n\t\t\t\t struct keylist *insert_keys,\n\t\t\t\t atomic_t *journal_ref,\n\t\t\t\t struct bkey *replace_key);\n\nstatic int btree_gc_coalesce(struct btree *b, struct btree_op *op,\n\t\t\t     struct gc_stat *gc, struct gc_merge_info *r)\n{\n\tunsigned int i, nodes = 0, keys = 0, blocks;\n\tstruct btree *new_nodes[GC_MERGE_NODES];\n\tstruct keylist keylist;\n\tstruct closure cl;\n\tstruct bkey *k;\n\n\tbch_keylist_init(&keylist);\n\n\tif (btree_check_reserve(b, NULL))\n\t\treturn 0;\n\n\tmemset(new_nodes, 0, sizeof(new_nodes));\n\tclosure_init_stack(&cl);\n\n\twhile (nodes < GC_MERGE_NODES && !IS_ERR_OR_NULL(r[nodes].b))\n\t\tkeys += r[nodes++].keys;\n\n\tblocks = btree_default_blocks(b->c) * 2 / 3;\n\n\tif (nodes < 2 ||\n\t    __set_blocks(b->keys.set[0].data, keys,\n\t\t\t block_bytes(b->c->cache)) > blocks * (nodes - 1))\n\t\treturn 0;\n\n\tfor (i = 0; i < nodes; i++) {\n\t\tnew_nodes[i] = btree_node_alloc_replacement(r[i].b, NULL);\n\t\tif (IS_ERR(new_nodes[i]))\n\t\t\tgoto out_nocoalesce;\n\t}\n\n\t \n\tif (btree_check_reserve(b, NULL))\n\t\tgoto out_nocoalesce;\n\n\tfor (i = 0; i < nodes; i++)\n\t\tmutex_lock(&new_nodes[i]->write_lock);\n\n\tfor (i = nodes - 1; i > 0; --i) {\n\t\tstruct bset *n1 = btree_bset_first(new_nodes[i]);\n\t\tstruct bset *n2 = btree_bset_first(new_nodes[i - 1]);\n\t\tstruct bkey *k, *last = NULL;\n\n\t\tkeys = 0;\n\n\t\tif (i > 1) {\n\t\t\tfor (k = n2->start;\n\t\t\t     k < bset_bkey_last(n2);\n\t\t\t     k = bkey_next(k)) {\n\t\t\t\tif (__set_blocks(n1, n1->keys + keys +\n\t\t\t\t\t\t bkey_u64s(k),\n\t\t\t\t\t\t block_bytes(b->c->cache)) > blocks)\n\t\t\t\t\tbreak;\n\n\t\t\t\tlast = k;\n\t\t\t\tkeys += bkey_u64s(k);\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tif (__set_blocks(n1, n1->keys + n2->keys,\n\t\t\t\t\t block_bytes(b->c->cache)) >\n\t\t\t    btree_blocks(new_nodes[i]))\n\t\t\t\tgoto out_unlock_nocoalesce;\n\n\t\t\tkeys = n2->keys;\n\t\t\t \n\t\t\tlast = &r->b->key;\n\t\t}\n\n\t\tBUG_ON(__set_blocks(n1, n1->keys + keys, block_bytes(b->c->cache)) >\n\t\t       btree_blocks(new_nodes[i]));\n\n\t\tif (last)\n\t\t\tbkey_copy_key(&new_nodes[i]->key, last);\n\n\t\tmemcpy(bset_bkey_last(n1),\n\t\t       n2->start,\n\t\t       (void *) bset_bkey_idx(n2, keys) - (void *) n2->start);\n\n\t\tn1->keys += keys;\n\t\tr[i].keys = n1->keys;\n\n\t\tmemmove(n2->start,\n\t\t\tbset_bkey_idx(n2, keys),\n\t\t\t(void *) bset_bkey_last(n2) -\n\t\t\t(void *) bset_bkey_idx(n2, keys));\n\n\t\tn2->keys -= keys;\n\n\t\tif (__bch_keylist_realloc(&keylist,\n\t\t\t\t\t  bkey_u64s(&new_nodes[i]->key)))\n\t\t\tgoto out_unlock_nocoalesce;\n\n\t\tbch_btree_node_write(new_nodes[i], &cl);\n\t\tbch_keylist_add(&keylist, &new_nodes[i]->key);\n\t}\n\n\tfor (i = 0; i < nodes; i++)\n\t\tmutex_unlock(&new_nodes[i]->write_lock);\n\n\tclosure_sync(&cl);\n\n\t \n\tBUG_ON(btree_bset_first(new_nodes[0])->keys);\n\tbtree_node_free(new_nodes[0]);\n\trw_unlock(true, new_nodes[0]);\n\tnew_nodes[0] = NULL;\n\n\tfor (i = 0; i < nodes; i++) {\n\t\tif (__bch_keylist_realloc(&keylist, bkey_u64s(&r[i].b->key)))\n\t\t\tgoto out_nocoalesce;\n\n\t\tmake_btree_freeing_key(r[i].b, keylist.top);\n\t\tbch_keylist_push(&keylist);\n\t}\n\n\tbch_btree_insert_node(b, op, &keylist, NULL, NULL);\n\tBUG_ON(!bch_keylist_empty(&keylist));\n\n\tfor (i = 0; i < nodes; i++) {\n\t\tbtree_node_free(r[i].b);\n\t\trw_unlock(true, r[i].b);\n\n\t\tr[i].b = new_nodes[i];\n\t}\n\n\tmemmove(r, r + 1, sizeof(r[0]) * (nodes - 1));\n\tr[nodes - 1].b = ERR_PTR(-EINTR);\n\n\ttrace_bcache_btree_gc_coalesce(nodes);\n\tgc->nodes--;\n\n\tbch_keylist_free(&keylist);\n\n\t \n\treturn -EINTR;\n\nout_unlock_nocoalesce:\n\tfor (i = 0; i < nodes; i++)\n\t\tmutex_unlock(&new_nodes[i]->write_lock);\n\nout_nocoalesce:\n\tclosure_sync(&cl);\n\n\twhile ((k = bch_keylist_pop(&keylist)))\n\t\tif (!bkey_cmp(k, &ZERO_KEY))\n\t\t\tatomic_dec(&b->c->prio_blocked);\n\tbch_keylist_free(&keylist);\n\n\tfor (i = 0; i < nodes; i++)\n\t\tif (!IS_ERR_OR_NULL(new_nodes[i])) {\n\t\t\tbtree_node_free(new_nodes[i]);\n\t\t\trw_unlock(true, new_nodes[i]);\n\t\t}\n\treturn 0;\n}\n\nstatic int btree_gc_rewrite_node(struct btree *b, struct btree_op *op,\n\t\t\t\t struct btree *replace)\n{\n\tstruct keylist keys;\n\tstruct btree *n;\n\n\tif (btree_check_reserve(b, NULL))\n\t\treturn 0;\n\n\tn = btree_node_alloc_replacement(replace, NULL);\n\tif (IS_ERR(n))\n\t\treturn 0;\n\n\t \n\tif (btree_check_reserve(b, NULL)) {\n\t\tbtree_node_free(n);\n\t\trw_unlock(true, n);\n\t\treturn 0;\n\t}\n\n\tbch_btree_node_write_sync(n);\n\n\tbch_keylist_init(&keys);\n\tbch_keylist_add(&keys, &n->key);\n\n\tmake_btree_freeing_key(replace, keys.top);\n\tbch_keylist_push(&keys);\n\n\tbch_btree_insert_node(b, op, &keys, NULL, NULL);\n\tBUG_ON(!bch_keylist_empty(&keys));\n\n\tbtree_node_free(replace);\n\trw_unlock(true, n);\n\n\t \n\treturn -EINTR;\n}\n\nstatic unsigned int btree_gc_count_keys(struct btree *b)\n{\n\tstruct bkey *k;\n\tstruct btree_iter iter;\n\tunsigned int ret = 0;\n\n\tfor_each_key_filter(&b->keys, k, &iter, bch_ptr_bad)\n\t\tret += bkey_u64s(k);\n\n\treturn ret;\n}\n\nstatic size_t btree_gc_min_nodes(struct cache_set *c)\n{\n\tsize_t min_nodes;\n\n\t \n\tmin_nodes = c->gc_stats.nodes / MAX_GC_TIMES;\n\tif (min_nodes < MIN_GC_NODES)\n\t\tmin_nodes = MIN_GC_NODES;\n\n\treturn min_nodes;\n}\n\n\nstatic int btree_gc_recurse(struct btree *b, struct btree_op *op,\n\t\t\t    struct closure *writes, struct gc_stat *gc)\n{\n\tint ret = 0;\n\tbool should_rewrite;\n\tstruct bkey *k;\n\tstruct btree_iter iter;\n\tstruct gc_merge_info r[GC_MERGE_NODES];\n\tstruct gc_merge_info *i, *last = r + ARRAY_SIZE(r) - 1;\n\n\tbch_btree_iter_init(&b->keys, &iter, &b->c->gc_done);\n\n\tfor (i = r; i < r + ARRAY_SIZE(r); i++)\n\t\ti->b = ERR_PTR(-EINTR);\n\n\twhile (1) {\n\t\tk = bch_btree_iter_next_filter(&iter, &b->keys, bch_ptr_bad);\n\t\tif (k) {\n\t\t\tr->b = bch_btree_node_get(b->c, op, k, b->level - 1,\n\t\t\t\t\t\t  true, b);\n\t\t\tif (IS_ERR(r->b)) {\n\t\t\t\tret = PTR_ERR(r->b);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tr->keys = btree_gc_count_keys(r->b);\n\n\t\t\tret = btree_gc_coalesce(b, op, gc, r);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!last->b)\n\t\t\tbreak;\n\n\t\tif (!IS_ERR(last->b)) {\n\t\t\tshould_rewrite = btree_gc_mark_node(last->b, gc);\n\t\t\tif (should_rewrite) {\n\t\t\t\tret = btree_gc_rewrite_node(b, op, last->b);\n\t\t\t\tif (ret)\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (last->b->level) {\n\t\t\t\tret = btree_gc_recurse(last->b, op, writes, gc);\n\t\t\t\tif (ret)\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbkey_copy_key(&b->c->gc_done, &last->b->key);\n\n\t\t\t \n\t\t\tmutex_lock(&last->b->write_lock);\n\t\t\tif (btree_node_dirty(last->b))\n\t\t\t\tbch_btree_node_write(last->b, writes);\n\t\t\tmutex_unlock(&last->b->write_lock);\n\t\t\trw_unlock(true, last->b);\n\t\t}\n\n\t\tmemmove(r + 1, r, sizeof(r[0]) * (GC_MERGE_NODES - 1));\n\t\tr->b = NULL;\n\n\t\tif (atomic_read(&b->c->search_inflight) &&\n\t\t    gc->nodes >= gc->nodes_pre + btree_gc_min_nodes(b->c)) {\n\t\t\tgc->nodes_pre =  gc->nodes;\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (need_resched()) {\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfor (i = r; i < r + ARRAY_SIZE(r); i++)\n\t\tif (!IS_ERR_OR_NULL(i->b)) {\n\t\t\tmutex_lock(&i->b->write_lock);\n\t\t\tif (btree_node_dirty(i->b))\n\t\t\t\tbch_btree_node_write(i->b, writes);\n\t\t\tmutex_unlock(&i->b->write_lock);\n\t\t\trw_unlock(true, i->b);\n\t\t}\n\n\treturn ret;\n}\n\nstatic int bch_btree_gc_root(struct btree *b, struct btree_op *op,\n\t\t\t     struct closure *writes, struct gc_stat *gc)\n{\n\tstruct btree *n = NULL;\n\tint ret = 0;\n\tbool should_rewrite;\n\n\tshould_rewrite = btree_gc_mark_node(b, gc);\n\tif (should_rewrite) {\n\t\tn = btree_node_alloc_replacement(b, NULL);\n\n\t\tif (!IS_ERR(n)) {\n\t\t\tbch_btree_node_write_sync(n);\n\n\t\t\tbch_btree_set_root(n);\n\t\t\tbtree_node_free(b);\n\t\t\trw_unlock(true, n);\n\n\t\t\treturn -EINTR;\n\t\t}\n\t}\n\n\t__bch_btree_mark_key(b->c, b->level + 1, &b->key);\n\n\tif (b->level) {\n\t\tret = btree_gc_recurse(b, op, writes, gc);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tbkey_copy_key(&b->c->gc_done, &b->key);\n\n\treturn ret;\n}\n\nstatic void btree_gc_start(struct cache_set *c)\n{\n\tstruct cache *ca;\n\tstruct bucket *b;\n\n\tif (!c->gc_mark_valid)\n\t\treturn;\n\n\tmutex_lock(&c->bucket_lock);\n\n\tc->gc_mark_valid = 0;\n\tc->gc_done = ZERO_KEY;\n\n\tca = c->cache;\n\tfor_each_bucket(b, ca) {\n\t\tb->last_gc = b->gen;\n\t\tif (!atomic_read(&b->pin)) {\n\t\t\tSET_GC_MARK(b, 0);\n\t\t\tSET_GC_SECTORS_USED(b, 0);\n\t\t}\n\t}\n\n\tmutex_unlock(&c->bucket_lock);\n}\n\nstatic void bch_btree_gc_finish(struct cache_set *c)\n{\n\tstruct bucket *b;\n\tstruct cache *ca;\n\tunsigned int i, j;\n\tuint64_t *k;\n\n\tmutex_lock(&c->bucket_lock);\n\n\tset_gc_sectors(c);\n\tc->gc_mark_valid = 1;\n\tc->need_gc\t= 0;\n\n\tfor (i = 0; i < KEY_PTRS(&c->uuid_bucket); i++)\n\t\tSET_GC_MARK(PTR_BUCKET(c, &c->uuid_bucket, i),\n\t\t\t    GC_MARK_METADATA);\n\n\t \n\trcu_read_lock();\n\tfor (i = 0; i < c->devices_max_used; i++) {\n\t\tstruct bcache_device *d = c->devices[i];\n\t\tstruct cached_dev *dc;\n\t\tstruct keybuf_key *w, *n;\n\n\t\tif (!d || UUID_FLASH_ONLY(&c->uuids[i]))\n\t\t\tcontinue;\n\t\tdc = container_of(d, struct cached_dev, disk);\n\n\t\tspin_lock(&dc->writeback_keys.lock);\n\t\trbtree_postorder_for_each_entry_safe(w, n,\n\t\t\t\t\t&dc->writeback_keys.keys, node)\n\t\t\tfor (j = 0; j < KEY_PTRS(&w->key); j++)\n\t\t\t\tSET_GC_MARK(PTR_BUCKET(c, &w->key, j),\n\t\t\t\t\t    GC_MARK_DIRTY);\n\t\tspin_unlock(&dc->writeback_keys.lock);\n\t}\n\trcu_read_unlock();\n\n\tc->avail_nbuckets = 0;\n\n\tca = c->cache;\n\tca->invalidate_needs_gc = 0;\n\n\tfor (k = ca->sb.d; k < ca->sb.d + ca->sb.keys; k++)\n\t\tSET_GC_MARK(ca->buckets + *k, GC_MARK_METADATA);\n\n\tfor (k = ca->prio_buckets;\n\t     k < ca->prio_buckets + prio_buckets(ca) * 2; k++)\n\t\tSET_GC_MARK(ca->buckets + *k, GC_MARK_METADATA);\n\n\tfor_each_bucket(b, ca) {\n\t\tc->need_gc\t= max(c->need_gc, bucket_gc_gen(b));\n\n\t\tif (atomic_read(&b->pin))\n\t\t\tcontinue;\n\n\t\tBUG_ON(!GC_MARK(b) && GC_SECTORS_USED(b));\n\n\t\tif (!GC_MARK(b) || GC_MARK(b) == GC_MARK_RECLAIMABLE)\n\t\t\tc->avail_nbuckets++;\n\t}\n\n\tmutex_unlock(&c->bucket_lock);\n}\n\nstatic void bch_btree_gc(struct cache_set *c)\n{\n\tint ret;\n\tstruct gc_stat stats;\n\tstruct closure writes;\n\tstruct btree_op op;\n\tuint64_t start_time = local_clock();\n\n\ttrace_bcache_gc_start(c);\n\n\tmemset(&stats, 0, sizeof(struct gc_stat));\n\tclosure_init_stack(&writes);\n\tbch_btree_op_init(&op, SHRT_MAX);\n\n\tbtree_gc_start(c);\n\n\t \n\tdo {\n\t\tret = bcache_btree_root(gc_root, c, &op, &writes, &stats);\n\t\tclosure_sync(&writes);\n\t\tcond_resched();\n\n\t\tif (ret == -EAGAIN)\n\t\t\tschedule_timeout_interruptible(msecs_to_jiffies\n\t\t\t\t\t\t       (GC_SLEEP_MS));\n\t\telse if (ret)\n\t\t\tpr_warn(\"gc failed!\\n\");\n\t} while (ret && !test_bit(CACHE_SET_IO_DISABLE, &c->flags));\n\n\tbch_btree_gc_finish(c);\n\twake_up_allocators(c);\n\n\tbch_time_stats_update(&c->btree_gc_time, start_time);\n\n\tstats.key_bytes *= sizeof(uint64_t);\n\tstats.data\t<<= 9;\n\tbch_update_bucket_in_use(c, &stats);\n\tmemcpy(&c->gc_stats, &stats, sizeof(struct gc_stat));\n\n\ttrace_bcache_gc_end(c);\n\n\tbch_moving_gc(c);\n}\n\nstatic bool gc_should_run(struct cache_set *c)\n{\n\tstruct cache *ca = c->cache;\n\n\tif (ca->invalidate_needs_gc)\n\t\treturn true;\n\n\tif (atomic_read(&c->sectors_to_gc) < 0)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int bch_gc_thread(void *arg)\n{\n\tstruct cache_set *c = arg;\n\n\twhile (1) {\n\t\twait_event_interruptible(c->gc_wait,\n\t\t\t   kthread_should_stop() ||\n\t\t\t   test_bit(CACHE_SET_IO_DISABLE, &c->flags) ||\n\t\t\t   gc_should_run(c));\n\n\t\tif (kthread_should_stop() ||\n\t\t    test_bit(CACHE_SET_IO_DISABLE, &c->flags))\n\t\t\tbreak;\n\n\t\tset_gc_sectors(c);\n\t\tbch_btree_gc(c);\n\t}\n\n\twait_for_kthread_stop();\n\treturn 0;\n}\n\nint bch_gc_thread_start(struct cache_set *c)\n{\n\tc->gc_thread = kthread_run(bch_gc_thread, c, \"bcache_gc\");\n\treturn PTR_ERR_OR_ZERO(c->gc_thread);\n}\n\n \n\nstatic int bch_btree_check_recurse(struct btree *b, struct btree_op *op)\n{\n\tint ret = 0;\n\tstruct bkey *k, *p = NULL;\n\tstruct btree_iter iter;\n\n\tfor_each_key_filter(&b->keys, k, &iter, bch_ptr_invalid)\n\t\tbch_initial_mark_key(b->c, b->level, k);\n\n\tbch_initial_mark_key(b->c, b->level + 1, &b->key);\n\n\tif (b->level) {\n\t\tbch_btree_iter_init(&b->keys, &iter, NULL);\n\n\t\tdo {\n\t\t\tk = bch_btree_iter_next_filter(&iter, &b->keys,\n\t\t\t\t\t\t       bch_ptr_bad);\n\t\t\tif (k) {\n\t\t\t\tbtree_node_prefetch(b, k);\n\t\t\t\t \n\t\t\t\tb->c->gc_stats.nodes++;\n\t\t\t}\n\n\t\t\tif (p)\n\t\t\t\tret = bcache_btree(check_recurse, p, b, op);\n\n\t\t\tp = k;\n\t\t} while (p && !ret);\n\t}\n\n\treturn ret;\n}\n\n\nstatic int bch_btree_check_thread(void *arg)\n{\n\tint ret;\n\tstruct btree_check_info *info = arg;\n\tstruct btree_check_state *check_state = info->state;\n\tstruct cache_set *c = check_state->c;\n\tstruct btree_iter iter;\n\tstruct bkey *k, *p;\n\tint cur_idx, prev_idx, skip_nr;\n\n\tk = p = NULL;\n\tcur_idx = prev_idx = 0;\n\tret = 0;\n\n\t \n\tbch_btree_iter_init(&c->root->keys, &iter, NULL);\n\tk = bch_btree_iter_next_filter(&iter, &c->root->keys, bch_ptr_bad);\n\tBUG_ON(!k);\n\n\tp = k;\n\twhile (k) {\n\t\t \n\t\tspin_lock(&check_state->idx_lock);\n\t\tcur_idx = check_state->key_idx;\n\t\tcheck_state->key_idx++;\n\t\tspin_unlock(&check_state->idx_lock);\n\n\t\tskip_nr = cur_idx - prev_idx;\n\n\t\twhile (skip_nr) {\n\t\t\tk = bch_btree_iter_next_filter(&iter,\n\t\t\t\t\t\t       &c->root->keys,\n\t\t\t\t\t\t       bch_ptr_bad);\n\t\t\tif (k)\n\t\t\t\tp = k;\n\t\t\telse {\n\t\t\t\t \n\t\t\t\tatomic_set(&check_state->enough, 1);\n\t\t\t\t \n\t\t\t\tsmp_mb__after_atomic();\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tskip_nr--;\n\t\t\tcond_resched();\n\t\t}\n\n\t\tif (p) {\n\t\t\tstruct btree_op op;\n\n\t\t\tbtree_node_prefetch(c->root, p);\n\t\t\tc->gc_stats.nodes++;\n\t\t\tbch_btree_op_init(&op, 0);\n\t\t\tret = bcache_btree(check_recurse, p, c->root, &op);\n\t\t\t \n\t\t\tbch_cannibalize_unlock(c);\n\t\t\tfinish_wait(&c->btree_cache_wait, &(&op)->wait);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t\tp = NULL;\n\t\tprev_idx = cur_idx;\n\t\tcond_resched();\n\t}\n\nout:\n\tinfo->result = ret;\n\t \n\tsmp_mb__before_atomic();\n\tif (atomic_dec_and_test(&check_state->started))\n\t\twake_up(&check_state->wait);\n\n\treturn ret;\n}\n\n\n\nstatic int bch_btree_chkthread_nr(void)\n{\n\tint n = num_online_cpus()/2;\n\n\tif (n == 0)\n\t\tn = 1;\n\telse if (n > BCH_BTR_CHKTHREAD_MAX)\n\t\tn = BCH_BTR_CHKTHREAD_MAX;\n\n\treturn n;\n}\n\nint bch_btree_check(struct cache_set *c)\n{\n\tint ret = 0;\n\tint i;\n\tstruct bkey *k = NULL;\n\tstruct btree_iter iter;\n\tstruct btree_check_state check_state;\n\n\t \n\tfor_each_key_filter(&c->root->keys, k, &iter, bch_ptr_invalid)\n\t\tbch_initial_mark_key(c, c->root->level, k);\n\n\tbch_initial_mark_key(c, c->root->level + 1, &c->root->key);\n\n\tif (c->root->level == 0)\n\t\treturn 0;\n\n\tmemset(&check_state, 0, sizeof(struct btree_check_state));\n\tcheck_state.c = c;\n\tcheck_state.total_threads = bch_btree_chkthread_nr();\n\tcheck_state.key_idx = 0;\n\tspin_lock_init(&check_state.idx_lock);\n\tatomic_set(&check_state.started, 0);\n\tatomic_set(&check_state.enough, 0);\n\tinit_waitqueue_head(&check_state.wait);\n\n\trw_lock(0, c->root, c->root->level);\n\t \n\tfor (i = 0; i < check_state.total_threads; i++) {\n\t\t \n\t\tsmp_mb__before_atomic();\n\t\tif (atomic_read(&check_state.enough))\n\t\t\tbreak;\n\n\t\tcheck_state.infos[i].result = 0;\n\t\tcheck_state.infos[i].state = &check_state;\n\n\t\tcheck_state.infos[i].thread =\n\t\t\tkthread_run(bch_btree_check_thread,\n\t\t\t\t    &check_state.infos[i],\n\t\t\t\t    \"bch_btrchk[%d]\", i);\n\t\tif (IS_ERR(check_state.infos[i].thread)) {\n\t\t\tpr_err(\"fails to run thread bch_btrchk[%d]\\n\", i);\n\t\t\tfor (--i; i >= 0; i--)\n\t\t\t\tkthread_stop(check_state.infos[i].thread);\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tatomic_inc(&check_state.started);\n\t}\n\n\t \n\twait_event(check_state.wait, atomic_read(&check_state.started) == 0);\n\n\tfor (i = 0; i < check_state.total_threads; i++) {\n\t\tif (check_state.infos[i].result) {\n\t\t\tret = check_state.infos[i].result;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\trw_unlock(0, c->root);\n\treturn ret;\n}\n\nvoid bch_initial_gc_finish(struct cache_set *c)\n{\n\tstruct cache *ca = c->cache;\n\tstruct bucket *b;\n\n\tbch_btree_gc_finish(c);\n\n\tmutex_lock(&c->bucket_lock);\n\n\t \n\tfor_each_bucket(b, ca) {\n\t\tif (fifo_full(&ca->free[RESERVE_PRIO]) &&\n\t\t    fifo_full(&ca->free[RESERVE_BTREE]))\n\t\t\tbreak;\n\n\t\tif (bch_can_invalidate_bucket(ca, b) &&\n\t\t    !GC_MARK(b)) {\n\t\t\t__bch_invalidate_one_bucket(ca, b);\n\t\t\tif (!fifo_push(&ca->free[RESERVE_PRIO],\n\t\t\t   b - ca->buckets))\n\t\t\t\tfifo_push(&ca->free[RESERVE_BTREE],\n\t\t\t\t\t  b - ca->buckets);\n\t\t}\n\t}\n\n\tmutex_unlock(&c->bucket_lock);\n}\n\n \n\nstatic bool btree_insert_key(struct btree *b, struct bkey *k,\n\t\t\t     struct bkey *replace_key)\n{\n\tunsigned int status;\n\n\tBUG_ON(bkey_cmp(k, &b->key) > 0);\n\n\tstatus = bch_btree_insert_key(&b->keys, k, replace_key);\n\tif (status != BTREE_INSERT_STATUS_NO_INSERT) {\n\t\tbch_check_keys(&b->keys, \"%u for %s\", status,\n\t\t\t       replace_key ? \"replace\" : \"insert\");\n\n\t\ttrace_bcache_btree_insert_key(b, k, replace_key != NULL,\n\t\t\t\t\t      status);\n\t\treturn true;\n\t} else\n\t\treturn false;\n}\n\nstatic size_t insert_u64s_remaining(struct btree *b)\n{\n\tlong ret = bch_btree_keys_u64s_remaining(&b->keys);\n\n\t \n\tif (b->keys.ops->is_extents)\n\t\tret -= KEY_MAX_U64S;\n\n\treturn max(ret, 0L);\n}\n\nstatic bool bch_btree_insert_keys(struct btree *b, struct btree_op *op,\n\t\t\t\t  struct keylist *insert_keys,\n\t\t\t\t  struct bkey *replace_key)\n{\n\tbool ret = false;\n\tint oldsize = bch_count_data(&b->keys);\n\n\twhile (!bch_keylist_empty(insert_keys)) {\n\t\tstruct bkey *k = insert_keys->keys;\n\n\t\tif (bkey_u64s(k) > insert_u64s_remaining(b))\n\t\t\tbreak;\n\n\t\tif (bkey_cmp(k, &b->key) <= 0) {\n\t\t\tif (!b->level)\n\t\t\t\tbkey_put(b->c, k);\n\n\t\t\tret |= btree_insert_key(b, k, replace_key);\n\t\t\tbch_keylist_pop_front(insert_keys);\n\t\t} else if (bkey_cmp(&START_KEY(k), &b->key) < 0) {\n\t\t\tBKEY_PADDED(key) temp;\n\t\t\tbkey_copy(&temp.key, insert_keys->keys);\n\n\t\t\tbch_cut_back(&b->key, &temp.key);\n\t\t\tbch_cut_front(&b->key, insert_keys->keys);\n\n\t\t\tret |= btree_insert_key(b, &temp.key, replace_key);\n\t\t\tbreak;\n\t\t} else {\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!ret)\n\t\top->insert_collision = true;\n\n\tBUG_ON(!bch_keylist_empty(insert_keys) && b->level);\n\n\tBUG_ON(bch_count_data(&b->keys) < oldsize);\n\treturn ret;\n}\n\nstatic int btree_split(struct btree *b, struct btree_op *op,\n\t\t       struct keylist *insert_keys,\n\t\t       struct bkey *replace_key)\n{\n\tbool split;\n\tstruct btree *n1, *n2 = NULL, *n3 = NULL;\n\tuint64_t start_time = local_clock();\n\tstruct closure cl;\n\tstruct keylist parent_keys;\n\n\tclosure_init_stack(&cl);\n\tbch_keylist_init(&parent_keys);\n\n\tif (btree_check_reserve(b, op)) {\n\t\tif (!b->level)\n\t\t\treturn -EINTR;\n\t\telse\n\t\t\tWARN(1, \"insufficient reserve for split\\n\");\n\t}\n\n\tn1 = btree_node_alloc_replacement(b, op);\n\tif (IS_ERR(n1))\n\t\tgoto err;\n\n\tsplit = set_blocks(btree_bset_first(n1),\n\t\t\t   block_bytes(n1->c->cache)) > (btree_blocks(b) * 4) / 5;\n\n\tif (split) {\n\t\tunsigned int keys = 0;\n\n\t\ttrace_bcache_btree_node_split(b, btree_bset_first(n1)->keys);\n\n\t\tn2 = bch_btree_node_alloc(b->c, op, b->level, b->parent);\n\t\tif (IS_ERR(n2))\n\t\t\tgoto err_free1;\n\n\t\tif (!b->parent) {\n\t\t\tn3 = bch_btree_node_alloc(b->c, op, b->level + 1, NULL);\n\t\t\tif (IS_ERR(n3))\n\t\t\t\tgoto err_free2;\n\t\t}\n\n\t\tmutex_lock(&n1->write_lock);\n\t\tmutex_lock(&n2->write_lock);\n\n\t\tbch_btree_insert_keys(n1, op, insert_keys, replace_key);\n\n\t\t \n\n\t\twhile (keys < (btree_bset_first(n1)->keys * 3) / 5)\n\t\t\tkeys += bkey_u64s(bset_bkey_idx(btree_bset_first(n1),\n\t\t\t\t\t\t\tkeys));\n\n\t\tbkey_copy_key(&n1->key,\n\t\t\t      bset_bkey_idx(btree_bset_first(n1), keys));\n\t\tkeys += bkey_u64s(bset_bkey_idx(btree_bset_first(n1), keys));\n\n\t\tbtree_bset_first(n2)->keys = btree_bset_first(n1)->keys - keys;\n\t\tbtree_bset_first(n1)->keys = keys;\n\n\t\tmemcpy(btree_bset_first(n2)->start,\n\t\t       bset_bkey_last(btree_bset_first(n1)),\n\t\t       btree_bset_first(n2)->keys * sizeof(uint64_t));\n\n\t\tbkey_copy_key(&n2->key, &b->key);\n\n\t\tbch_keylist_add(&parent_keys, &n2->key);\n\t\tbch_btree_node_write(n2, &cl);\n\t\tmutex_unlock(&n2->write_lock);\n\t\trw_unlock(true, n2);\n\t} else {\n\t\ttrace_bcache_btree_node_compact(b, btree_bset_first(n1)->keys);\n\n\t\tmutex_lock(&n1->write_lock);\n\t\tbch_btree_insert_keys(n1, op, insert_keys, replace_key);\n\t}\n\n\tbch_keylist_add(&parent_keys, &n1->key);\n\tbch_btree_node_write(n1, &cl);\n\tmutex_unlock(&n1->write_lock);\n\n\tif (n3) {\n\t\t \n\t\tmutex_lock(&n3->write_lock);\n\t\tbkey_copy_key(&n3->key, &MAX_KEY);\n\t\tbch_btree_insert_keys(n3, op, &parent_keys, NULL);\n\t\tbch_btree_node_write(n3, &cl);\n\t\tmutex_unlock(&n3->write_lock);\n\n\t\tclosure_sync(&cl);\n\t\tbch_btree_set_root(n3);\n\t\trw_unlock(true, n3);\n\t} else if (!b->parent) {\n\t\t \n\t\tclosure_sync(&cl);\n\t\tbch_btree_set_root(n1);\n\t} else {\n\t\t \n\t\tclosure_sync(&cl);\n\t\tmake_btree_freeing_key(b, parent_keys.top);\n\t\tbch_keylist_push(&parent_keys);\n\n\t\tbch_btree_insert_node(b->parent, op, &parent_keys, NULL, NULL);\n\t\tBUG_ON(!bch_keylist_empty(&parent_keys));\n\t}\n\n\tbtree_node_free(b);\n\trw_unlock(true, n1);\n\n\tbch_time_stats_update(&b->c->btree_split_time, start_time);\n\n\treturn 0;\nerr_free2:\n\tbkey_put(b->c, &n2->key);\n\tbtree_node_free(n2);\n\trw_unlock(true, n2);\nerr_free1:\n\tbkey_put(b->c, &n1->key);\n\tbtree_node_free(n1);\n\trw_unlock(true, n1);\nerr:\n\tWARN(1, \"bcache: btree split failed (level %u)\", b->level);\n\n\tif (n3 == ERR_PTR(-EAGAIN) ||\n\t    n2 == ERR_PTR(-EAGAIN) ||\n\t    n1 == ERR_PTR(-EAGAIN))\n\t\treturn -EAGAIN;\n\n\treturn -ENOMEM;\n}\n\nstatic int bch_btree_insert_node(struct btree *b, struct btree_op *op,\n\t\t\t\t struct keylist *insert_keys,\n\t\t\t\t atomic_t *journal_ref,\n\t\t\t\t struct bkey *replace_key)\n{\n\tstruct closure cl;\n\n\tBUG_ON(b->level && replace_key);\n\n\tclosure_init_stack(&cl);\n\n\tmutex_lock(&b->write_lock);\n\n\tif (write_block(b) != btree_bset_last(b) &&\n\t    b->keys.last_set_unwritten)\n\t\tbch_btree_init_next(b);  \n\n\tif (bch_keylist_nkeys(insert_keys) > insert_u64s_remaining(b)) {\n\t\tmutex_unlock(&b->write_lock);\n\t\tgoto split;\n\t}\n\n\tBUG_ON(write_block(b) != btree_bset_last(b));\n\n\tif (bch_btree_insert_keys(b, op, insert_keys, replace_key)) {\n\t\tif (!b->level)\n\t\t\tbch_btree_leaf_dirty(b, journal_ref);\n\t\telse\n\t\t\tbch_btree_node_write(b, &cl);\n\t}\n\n\tmutex_unlock(&b->write_lock);\n\n\t \n\tclosure_sync(&cl);\n\n\treturn 0;\nsplit:\n\tif (current->bio_list) {\n\t\top->lock = b->c->root->level + 1;\n\t\treturn -EAGAIN;\n\t} else if (op->lock <= b->c->root->level) {\n\t\top->lock = b->c->root->level + 1;\n\t\treturn -EINTR;\n\t} else {\n\t\t \n\t\tint ret = btree_split(b, op, insert_keys, replace_key);\n\n\t\tif (bch_keylist_empty(insert_keys))\n\t\t\treturn 0;\n\t\telse if (!ret)\n\t\t\treturn -EINTR;\n\t\treturn ret;\n\t}\n}\n\nint bch_btree_insert_check_key(struct btree *b, struct btree_op *op,\n\t\t\t       struct bkey *check_key)\n{\n\tint ret = -EINTR;\n\tuint64_t btree_ptr = b->key.ptr[0];\n\tunsigned long seq = b->seq;\n\tstruct keylist insert;\n\tbool upgrade = op->lock == -1;\n\n\tbch_keylist_init(&insert);\n\n\tif (upgrade) {\n\t\trw_unlock(false, b);\n\t\trw_lock(true, b, b->level);\n\n\t\tif (b->key.ptr[0] != btree_ptr ||\n\t\t    b->seq != seq + 1) {\n\t\t\top->lock = b->level;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tSET_KEY_PTRS(check_key, 1);\n\tget_random_bytes(&check_key->ptr[0], sizeof(uint64_t));\n\n\tSET_PTR_DEV(check_key, 0, PTR_CHECK_DEV);\n\n\tbch_keylist_add(&insert, check_key);\n\n\tret = bch_btree_insert_node(b, op, &insert, NULL, NULL);\n\n\tBUG_ON(!ret && !bch_keylist_empty(&insert));\nout:\n\tif (upgrade)\n\t\tdowngrade_write(&b->lock);\n\treturn ret;\n}\n\nstruct btree_insert_op {\n\tstruct btree_op\top;\n\tstruct keylist\t*keys;\n\tatomic_t\t*journal_ref;\n\tstruct bkey\t*replace_key;\n};\n\nstatic int btree_insert_fn(struct btree_op *b_op, struct btree *b)\n{\n\tstruct btree_insert_op *op = container_of(b_op,\n\t\t\t\t\tstruct btree_insert_op, op);\n\n\tint ret = bch_btree_insert_node(b, &op->op, op->keys,\n\t\t\t\t\top->journal_ref, op->replace_key);\n\tif (ret && !bch_keylist_empty(op->keys))\n\t\treturn ret;\n\telse\n\t\treturn MAP_DONE;\n}\n\nint bch_btree_insert(struct cache_set *c, struct keylist *keys,\n\t\t     atomic_t *journal_ref, struct bkey *replace_key)\n{\n\tstruct btree_insert_op op;\n\tint ret = 0;\n\n\tBUG_ON(current->bio_list);\n\tBUG_ON(bch_keylist_empty(keys));\n\n\tbch_btree_op_init(&op.op, 0);\n\top.keys\t\t= keys;\n\top.journal_ref\t= journal_ref;\n\top.replace_key\t= replace_key;\n\n\twhile (!ret && !bch_keylist_empty(keys)) {\n\t\top.op.lock = 0;\n\t\tret = bch_btree_map_leaf_nodes(&op.op, c,\n\t\t\t\t\t       &START_KEY(keys->keys),\n\t\t\t\t\t       btree_insert_fn);\n\t}\n\n\tif (ret) {\n\t\tstruct bkey *k;\n\n\t\tpr_err(\"error %i\\n\", ret);\n\n\t\twhile ((k = bch_keylist_pop(keys)))\n\t\t\tbkey_put(c, k);\n\t} else if (op.op.insert_collision)\n\t\tret = -ESRCH;\n\n\treturn ret;\n}\n\nvoid bch_btree_set_root(struct btree *b)\n{\n\tunsigned int i;\n\tstruct closure cl;\n\n\tclosure_init_stack(&cl);\n\n\ttrace_bcache_btree_set_root(b);\n\n\tBUG_ON(!b->written);\n\n\tfor (i = 0; i < KEY_PTRS(&b->key); i++)\n\t\tBUG_ON(PTR_BUCKET(b->c, &b->key, i)->prio != BTREE_PRIO);\n\n\tmutex_lock(&b->c->bucket_lock);\n\tlist_del_init(&b->list);\n\tmutex_unlock(&b->c->bucket_lock);\n\n\tb->c->root = b;\n\n\tbch_journal_meta(b->c, &cl);\n\tclosure_sync(&cl);\n}\n\n \n\nstatic int bch_btree_map_nodes_recurse(struct btree *b, struct btree_op *op,\n\t\t\t\t       struct bkey *from,\n\t\t\t\t       btree_map_nodes_fn *fn, int flags)\n{\n\tint ret = MAP_CONTINUE;\n\n\tif (b->level) {\n\t\tstruct bkey *k;\n\t\tstruct btree_iter iter;\n\n\t\tbch_btree_iter_init(&b->keys, &iter, from);\n\n\t\twhile ((k = bch_btree_iter_next_filter(&iter, &b->keys,\n\t\t\t\t\t\t       bch_ptr_bad))) {\n\t\t\tret = bcache_btree(map_nodes_recurse, k, b,\n\t\t\t\t    op, from, fn, flags);\n\t\t\tfrom = NULL;\n\n\t\t\tif (ret != MAP_CONTINUE)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (!b->level || flags == MAP_ALL_NODES)\n\t\tret = fn(op, b);\n\n\treturn ret;\n}\n\nint __bch_btree_map_nodes(struct btree_op *op, struct cache_set *c,\n\t\t\t  struct bkey *from, btree_map_nodes_fn *fn, int flags)\n{\n\treturn bcache_btree_root(map_nodes_recurse, c, op, from, fn, flags);\n}\n\nint bch_btree_map_keys_recurse(struct btree *b, struct btree_op *op,\n\t\t\t\t      struct bkey *from, btree_map_keys_fn *fn,\n\t\t\t\t      int flags)\n{\n\tint ret = MAP_CONTINUE;\n\tstruct bkey *k;\n\tstruct btree_iter iter;\n\n\tbch_btree_iter_init(&b->keys, &iter, from);\n\n\twhile ((k = bch_btree_iter_next_filter(&iter, &b->keys, bch_ptr_bad))) {\n\t\tret = !b->level\n\t\t\t? fn(op, b, k)\n\t\t\t: bcache_btree(map_keys_recurse, k,\n\t\t\t\t       b, op, from, fn, flags);\n\t\tfrom = NULL;\n\n\t\tif (ret != MAP_CONTINUE)\n\t\t\treturn ret;\n\t}\n\n\tif (!b->level && (flags & MAP_END_KEY))\n\t\tret = fn(op, b, &KEY(KEY_INODE(&b->key),\n\t\t\t\t     KEY_OFFSET(&b->key), 0));\n\n\treturn ret;\n}\n\nint bch_btree_map_keys(struct btree_op *op, struct cache_set *c,\n\t\t       struct bkey *from, btree_map_keys_fn *fn, int flags)\n{\n\treturn bcache_btree_root(map_keys_recurse, c, op, from, fn, flags);\n}\n\n \n\nstatic inline int keybuf_cmp(struct keybuf_key *l, struct keybuf_key *r)\n{\n\t \n\tif (bkey_cmp(&l->key, &START_KEY(&r->key)) <= 0)\n\t\treturn -1;\n\tif (bkey_cmp(&START_KEY(&l->key), &r->key) >= 0)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic inline int keybuf_nonoverlapping_cmp(struct keybuf_key *l,\n\t\t\t\t\t    struct keybuf_key *r)\n{\n\treturn clamp_t(int64_t, bkey_cmp(&l->key, &r->key), -1, 1);\n}\n\nstruct refill {\n\tstruct btree_op\top;\n\tunsigned int\tnr_found;\n\tstruct keybuf\t*buf;\n\tstruct bkey\t*end;\n\tkeybuf_pred_fn\t*pred;\n};\n\nstatic int refill_keybuf_fn(struct btree_op *op, struct btree *b,\n\t\t\t    struct bkey *k)\n{\n\tstruct refill *refill = container_of(op, struct refill, op);\n\tstruct keybuf *buf = refill->buf;\n\tint ret = MAP_CONTINUE;\n\n\tif (bkey_cmp(k, refill->end) > 0) {\n\t\tret = MAP_DONE;\n\t\tgoto out;\n\t}\n\n\tif (!KEY_SIZE(k))  \n\t\tgoto out;\n\n\tif (refill->pred(buf, k)) {\n\t\tstruct keybuf_key *w;\n\n\t\tspin_lock(&buf->lock);\n\n\t\tw = array_alloc(&buf->freelist);\n\t\tif (!w) {\n\t\t\tspin_unlock(&buf->lock);\n\t\t\treturn MAP_DONE;\n\t\t}\n\n\t\tw->private = NULL;\n\t\tbkey_copy(&w->key, k);\n\n\t\tif (RB_INSERT(&buf->keys, w, node, keybuf_cmp))\n\t\t\tarray_free(&buf->freelist, w);\n\t\telse\n\t\t\trefill->nr_found++;\n\n\t\tif (array_freelist_empty(&buf->freelist))\n\t\t\tret = MAP_DONE;\n\n\t\tspin_unlock(&buf->lock);\n\t}\nout:\n\tbuf->last_scanned = *k;\n\treturn ret;\n}\n\nvoid bch_refill_keybuf(struct cache_set *c, struct keybuf *buf,\n\t\t       struct bkey *end, keybuf_pred_fn *pred)\n{\n\tstruct bkey start = buf->last_scanned;\n\tstruct refill refill;\n\n\tcond_resched();\n\n\tbch_btree_op_init(&refill.op, -1);\n\trefill.nr_found\t= 0;\n\trefill.buf\t= buf;\n\trefill.end\t= end;\n\trefill.pred\t= pred;\n\n\tbch_btree_map_keys(&refill.op, c, &buf->last_scanned,\n\t\t\t   refill_keybuf_fn, MAP_END_KEY);\n\n\ttrace_bcache_keyscan(refill.nr_found,\n\t\t\t     KEY_INODE(&start), KEY_OFFSET(&start),\n\t\t\t     KEY_INODE(&buf->last_scanned),\n\t\t\t     KEY_OFFSET(&buf->last_scanned));\n\n\tspin_lock(&buf->lock);\n\n\tif (!RB_EMPTY_ROOT(&buf->keys)) {\n\t\tstruct keybuf_key *w;\n\n\t\tw = RB_FIRST(&buf->keys, struct keybuf_key, node);\n\t\tbuf->start\t= START_KEY(&w->key);\n\n\t\tw = RB_LAST(&buf->keys, struct keybuf_key, node);\n\t\tbuf->end\t= w->key;\n\t} else {\n\t\tbuf->start\t= MAX_KEY;\n\t\tbuf->end\t= MAX_KEY;\n\t}\n\n\tspin_unlock(&buf->lock);\n}\n\nstatic void __bch_keybuf_del(struct keybuf *buf, struct keybuf_key *w)\n{\n\trb_erase(&w->node, &buf->keys);\n\tarray_free(&buf->freelist, w);\n}\n\nvoid bch_keybuf_del(struct keybuf *buf, struct keybuf_key *w)\n{\n\tspin_lock(&buf->lock);\n\t__bch_keybuf_del(buf, w);\n\tspin_unlock(&buf->lock);\n}\n\nbool bch_keybuf_check_overlapping(struct keybuf *buf, struct bkey *start,\n\t\t\t\t  struct bkey *end)\n{\n\tbool ret = false;\n\tstruct keybuf_key *p, *w, s;\n\n\ts.key = *start;\n\n\tif (bkey_cmp(end, &buf->start) <= 0 ||\n\t    bkey_cmp(start, &buf->end) >= 0)\n\t\treturn false;\n\n\tspin_lock(&buf->lock);\n\tw = RB_GREATER(&buf->keys, s, node, keybuf_nonoverlapping_cmp);\n\n\twhile (w && bkey_cmp(&START_KEY(&w->key), end) < 0) {\n\t\tp = w;\n\t\tw = RB_NEXT(w, node);\n\n\t\tif (p->private)\n\t\t\tret = true;\n\t\telse\n\t\t\t__bch_keybuf_del(buf, p);\n\t}\n\n\tspin_unlock(&buf->lock);\n\treturn ret;\n}\n\nstruct keybuf_key *bch_keybuf_next(struct keybuf *buf)\n{\n\tstruct keybuf_key *w;\n\n\tspin_lock(&buf->lock);\n\n\tw = RB_FIRST(&buf->keys, struct keybuf_key, node);\n\n\twhile (w && w->private)\n\t\tw = RB_NEXT(w, node);\n\n\tif (w)\n\t\tw->private = ERR_PTR(-EINTR);\n\n\tspin_unlock(&buf->lock);\n\treturn w;\n}\n\nstruct keybuf_key *bch_keybuf_next_rescan(struct cache_set *c,\n\t\t\t\t\t  struct keybuf *buf,\n\t\t\t\t\t  struct bkey *end,\n\t\t\t\t\t  keybuf_pred_fn *pred)\n{\n\tstruct keybuf_key *ret;\n\n\twhile (1) {\n\t\tret = bch_keybuf_next(buf);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tif (bkey_cmp(&buf->last_scanned, end) >= 0) {\n\t\t\tpr_debug(\"scan finished\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tbch_refill_keybuf(c, buf, end, pred);\n\t}\n\n\treturn ret;\n}\n\nvoid bch_keybuf_init(struct keybuf *buf)\n{\n\tbuf->last_scanned\t= MAX_KEY;\n\tbuf->keys\t\t= RB_ROOT;\n\n\tspin_lock_init(&buf->lock);\n\tarray_allocator_init(&buf->freelist);\n}\n\nvoid bch_btree_exit(void)\n{\n\tif (btree_io_wq)\n\t\tdestroy_workqueue(btree_io_wq);\n}\n\nint __init bch_btree_init(void)\n{\n\tbtree_io_wq = alloc_workqueue(\"bch_btree_io\", WQ_MEM_RECLAIM, 0);\n\tif (!btree_io_wq)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}