{
  "module_name": "extents.c",
  "hash_id": "4eea3da643a898674700c8e516eb37a80511c40769ca58a02e7d1130c55d1e3a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/bcache/extents.c",
  "human_readable_source": "\n \n\n#include \"bcache.h\"\n#include \"btree.h\"\n#include \"debug.h\"\n#include \"extents.h\"\n#include \"writeback.h\"\n\nstatic void sort_key_next(struct btree_iter *iter,\n\t\t\t  struct btree_iter_set *i)\n{\n\ti->k = bkey_next(i->k);\n\n\tif (i->k == i->end)\n\t\t*i = iter->data[--iter->used];\n}\n\nstatic bool bch_key_sort_cmp(struct btree_iter_set l,\n\t\t\t     struct btree_iter_set r)\n{\n\tint64_t c = bkey_cmp(l.k, r.k);\n\n\treturn c ? c > 0 : l.k < r.k;\n}\n\nstatic bool __ptr_invalid(struct cache_set *c, const struct bkey *k)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < KEY_PTRS(k); i++)\n\t\tif (ptr_available(c, k, i)) {\n\t\t\tstruct cache *ca = c->cache;\n\t\t\tsize_t bucket = PTR_BUCKET_NR(c, k, i);\n\t\t\tsize_t r = bucket_remainder(c, PTR_OFFSET(k, i));\n\n\t\t\tif (KEY_SIZE(k) + r > c->cache->sb.bucket_size ||\n\t\t\t    bucket <  ca->sb.first_bucket ||\n\t\t\t    bucket >= ca->sb.nbuckets)\n\t\t\t\treturn true;\n\t\t}\n\n\treturn false;\n}\n\n \n\nstatic const char *bch_ptr_status(struct cache_set *c, const struct bkey *k)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < KEY_PTRS(k); i++)\n\t\tif (ptr_available(c, k, i)) {\n\t\t\tstruct cache *ca = c->cache;\n\t\t\tsize_t bucket = PTR_BUCKET_NR(c, k, i);\n\t\t\tsize_t r = bucket_remainder(c, PTR_OFFSET(k, i));\n\n\t\t\tif (KEY_SIZE(k) + r > c->cache->sb.bucket_size)\n\t\t\t\treturn \"bad, length too big\";\n\t\t\tif (bucket <  ca->sb.first_bucket)\n\t\t\t\treturn \"bad, short offset\";\n\t\t\tif (bucket >= ca->sb.nbuckets)\n\t\t\t\treturn \"bad, offset past end of device\";\n\t\t\tif (ptr_stale(c, k, i))\n\t\t\t\treturn \"stale\";\n\t\t}\n\n\tif (!bkey_cmp(k, &ZERO_KEY))\n\t\treturn \"bad, null key\";\n\tif (!KEY_PTRS(k))\n\t\treturn \"bad, no pointers\";\n\tif (!KEY_SIZE(k))\n\t\treturn \"zeroed key\";\n\treturn \"\";\n}\n\nvoid bch_extent_to_text(char *buf, size_t size, const struct bkey *k)\n{\n\tunsigned int i = 0;\n\tchar *out = buf, *end = buf + size;\n\n#define p(...)\t(out += scnprintf(out, end - out, __VA_ARGS__))\n\n\tp(\"%llu:%llu len %llu -> [\", KEY_INODE(k), KEY_START(k), KEY_SIZE(k));\n\n\tfor (i = 0; i < KEY_PTRS(k); i++) {\n\t\tif (i)\n\t\t\tp(\", \");\n\n\t\tif (PTR_DEV(k, i) == PTR_CHECK_DEV)\n\t\t\tp(\"check dev\");\n\t\telse\n\t\t\tp(\"%llu:%llu gen %llu\", PTR_DEV(k, i),\n\t\t\t  PTR_OFFSET(k, i), PTR_GEN(k, i));\n\t}\n\n\tp(\"]\");\n\n\tif (KEY_DIRTY(k))\n\t\tp(\" dirty\");\n\tif (KEY_CSUM(k))\n\t\tp(\" cs%llu %llx\", KEY_CSUM(k), k->ptr[1]);\n#undef p\n}\n\nstatic void bch_bkey_dump(struct btree_keys *keys, const struct bkey *k)\n{\n\tstruct btree *b = container_of(keys, struct btree, keys);\n\tunsigned int j;\n\tchar buf[80];\n\n\tbch_extent_to_text(buf, sizeof(buf), k);\n\tpr_cont(\" %s\", buf);\n\n\tfor (j = 0; j < KEY_PTRS(k); j++) {\n\t\tsize_t n = PTR_BUCKET_NR(b->c, k, j);\n\n\t\tpr_cont(\" bucket %zu\", n);\n\t\tif (n >= b->c->cache->sb.first_bucket && n < b->c->cache->sb.nbuckets)\n\t\t\tpr_cont(\" prio %i\",\n\t\t\t\tPTR_BUCKET(b->c, k, j)->prio);\n\t}\n\n\tpr_cont(\" %s\\n\", bch_ptr_status(b->c, k));\n}\n\n \n\nbool __bch_btree_ptr_invalid(struct cache_set *c, const struct bkey *k)\n{\n\tchar buf[80];\n\n\tif (!KEY_PTRS(k) || !KEY_SIZE(k) || KEY_DIRTY(k))\n\t\tgoto bad;\n\n\tif (__ptr_invalid(c, k))\n\t\tgoto bad;\n\n\treturn false;\nbad:\n\tbch_extent_to_text(buf, sizeof(buf), k);\n\tcache_bug(c, \"spotted btree ptr %s: %s\", buf, bch_ptr_status(c, k));\n\treturn true;\n}\n\nstatic bool bch_btree_ptr_invalid(struct btree_keys *bk, const struct bkey *k)\n{\n\tstruct btree *b = container_of(bk, struct btree, keys);\n\n\treturn __bch_btree_ptr_invalid(b->c, k);\n}\n\nstatic bool btree_ptr_bad_expensive(struct btree *b, const struct bkey *k)\n{\n\tunsigned int i;\n\tchar buf[80];\n\tstruct bucket *g;\n\n\tif (mutex_trylock(&b->c->bucket_lock)) {\n\t\tfor (i = 0; i < KEY_PTRS(k); i++)\n\t\t\tif (ptr_available(b->c, k, i)) {\n\t\t\t\tg = PTR_BUCKET(b->c, k, i);\n\n\t\t\t\tif (KEY_DIRTY(k) ||\n\t\t\t\t    g->prio != BTREE_PRIO ||\n\t\t\t\t    (b->c->gc_mark_valid &&\n\t\t\t\t     GC_MARK(g) != GC_MARK_METADATA))\n\t\t\t\t\tgoto err;\n\t\t\t}\n\n\t\tmutex_unlock(&b->c->bucket_lock);\n\t}\n\n\treturn false;\nerr:\n\tmutex_unlock(&b->c->bucket_lock);\n\tbch_extent_to_text(buf, sizeof(buf), k);\n\tbtree_bug(b,\n\"inconsistent btree pointer %s: bucket %zi pin %i prio %i gen %i last_gc %i mark %llu\",\n\t\t  buf, PTR_BUCKET_NR(b->c, k, i), atomic_read(&g->pin),\n\t\t  g->prio, g->gen, g->last_gc, GC_MARK(g));\n\treturn true;\n}\n\nstatic bool bch_btree_ptr_bad(struct btree_keys *bk, const struct bkey *k)\n{\n\tstruct btree *b = container_of(bk, struct btree, keys);\n\tunsigned int i;\n\n\tif (!bkey_cmp(k, &ZERO_KEY) ||\n\t    !KEY_PTRS(k) ||\n\t    bch_ptr_invalid(bk, k))\n\t\treturn true;\n\n\tfor (i = 0; i < KEY_PTRS(k); i++)\n\t\tif (!ptr_available(b->c, k, i) ||\n\t\t    ptr_stale(b->c, k, i))\n\t\t\treturn true;\n\n\tif (expensive_debug_checks(b->c) &&\n\t    btree_ptr_bad_expensive(b, k))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool bch_btree_ptr_insert_fixup(struct btree_keys *bk,\n\t\t\t\t       struct bkey *insert,\n\t\t\t\t       struct btree_iter *iter,\n\t\t\t\t       struct bkey *replace_key)\n{\n\tstruct btree *b = container_of(bk, struct btree, keys);\n\n\tif (!KEY_OFFSET(insert))\n\t\tbtree_current_write(b)->prio_blocked++;\n\n\treturn false;\n}\n\nconst struct btree_keys_ops bch_btree_keys_ops = {\n\t.sort_cmp\t= bch_key_sort_cmp,\n\t.insert_fixup\t= bch_btree_ptr_insert_fixup,\n\t.key_invalid\t= bch_btree_ptr_invalid,\n\t.key_bad\t= bch_btree_ptr_bad,\n\t.key_to_text\t= bch_extent_to_text,\n\t.key_dump\t= bch_bkey_dump,\n};\n\n \n\n \nstatic bool bch_extent_sort_cmp(struct btree_iter_set l,\n\t\t\t\tstruct btree_iter_set r)\n{\n\tint64_t c = bkey_cmp(&START_KEY(l.k), &START_KEY(r.k));\n\n\treturn c ? c > 0 : l.k < r.k;\n}\n\nstatic struct bkey *bch_extent_sort_fixup(struct btree_iter *iter,\n\t\t\t\t\t  struct bkey *tmp)\n{\n\twhile (iter->used > 1) {\n\t\tstruct btree_iter_set *top = iter->data, *i = top + 1;\n\n\t\tif (iter->used > 2 &&\n\t\t    bch_extent_sort_cmp(i[0], i[1]))\n\t\t\ti++;\n\n\t\tif (bkey_cmp(top->k, &START_KEY(i->k)) <= 0)\n\t\t\tbreak;\n\n\t\tif (!KEY_SIZE(i->k)) {\n\t\t\tsort_key_next(iter, i);\n\t\t\theap_sift(iter, i - top, bch_extent_sort_cmp);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (top->k > i->k) {\n\t\t\tif (bkey_cmp(top->k, i->k) >= 0)\n\t\t\t\tsort_key_next(iter, i);\n\t\t\telse\n\t\t\t\tbch_cut_front(top->k, i->k);\n\n\t\t\theap_sift(iter, i - top, bch_extent_sort_cmp);\n\t\t} else {\n\t\t\t \n\t\t\tBUG_ON(!bkey_cmp(&START_KEY(top->k), &START_KEY(i->k)));\n\n\t\t\tif (bkey_cmp(i->k, top->k) < 0) {\n\t\t\t\tbkey_copy(tmp, top->k);\n\n\t\t\t\tbch_cut_back(&START_KEY(i->k), tmp);\n\t\t\t\tbch_cut_front(i->k, top->k);\n\t\t\t\theap_sift(iter, 0, bch_extent_sort_cmp);\n\n\t\t\t\treturn tmp;\n\t\t\t} else {\n\t\t\t\tbch_cut_back(&START_KEY(i->k), top->k);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic void bch_subtract_dirty(struct bkey *k,\n\t\t\t   struct cache_set *c,\n\t\t\t   uint64_t offset,\n\t\t\t   int sectors)\n{\n\tif (KEY_DIRTY(k))\n\t\tbcache_dev_sectors_dirty_add(c, KEY_INODE(k),\n\t\t\t\t\t     offset, -sectors);\n}\n\nstatic bool bch_extent_insert_fixup(struct btree_keys *b,\n\t\t\t\t    struct bkey *insert,\n\t\t\t\t    struct btree_iter *iter,\n\t\t\t\t    struct bkey *replace_key)\n{\n\tstruct cache_set *c = container_of(b, struct btree, keys)->c;\n\n\tuint64_t old_offset;\n\tunsigned int old_size, sectors_found = 0;\n\n\tBUG_ON(!KEY_OFFSET(insert));\n\tBUG_ON(!KEY_SIZE(insert));\n\n\twhile (1) {\n\t\tstruct bkey *k = bch_btree_iter_next(iter);\n\n\t\tif (!k)\n\t\t\tbreak;\n\n\t\tif (bkey_cmp(&START_KEY(k), insert) >= 0) {\n\t\t\tif (KEY_SIZE(k))\n\t\t\t\tbreak;\n\t\t\telse\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tif (bkey_cmp(k, &START_KEY(insert)) <= 0)\n\t\t\tcontinue;\n\n\t\told_offset = KEY_START(k);\n\t\told_size = KEY_SIZE(k);\n\n\t\t \n\n\t\tif (replace_key && KEY_SIZE(k)) {\n\t\t\t \n\t\t\tunsigned int i;\n\t\t\tuint64_t offset = KEY_START(k) -\n\t\t\t\tKEY_START(replace_key);\n\n\t\t\t \n\t\t\tif (KEY_START(k) < KEY_START(replace_key) ||\n\t\t\t    KEY_OFFSET(k) > KEY_OFFSET(replace_key))\n\t\t\t\tgoto check_failed;\n\n\t\t\t \n\t\t\tif (KEY_START(k) > KEY_START(insert) + sectors_found)\n\t\t\t\tgoto check_failed;\n\n\t\t\tif (!bch_bkey_equal_header(k, replace_key))\n\t\t\t\tgoto check_failed;\n\n\t\t\t \n\t\t\toffset <<= 8;\n\n\t\t\tBUG_ON(!KEY_PTRS(replace_key));\n\n\t\t\tfor (i = 0; i < KEY_PTRS(replace_key); i++)\n\t\t\t\tif (k->ptr[i] != replace_key->ptr[i] + offset)\n\t\t\t\t\tgoto check_failed;\n\n\t\t\tsectors_found = KEY_OFFSET(k) - KEY_START(insert);\n\t\t}\n\n\t\tif (bkey_cmp(insert, k) < 0 &&\n\t\t    bkey_cmp(&START_KEY(insert), &START_KEY(k)) > 0) {\n\t\t\t \n\n\t\t\tstruct bkey *top;\n\n\t\t\tbch_subtract_dirty(k, c, KEY_START(insert),\n\t\t\t\t       KEY_SIZE(insert));\n\n\t\t\tif (bkey_written(b, k)) {\n\t\t\t\t \n\t\t\t\ttop = bch_bset_search(b, bset_tree_last(b),\n\t\t\t\t\t\t      insert);\n\t\t\t\tbch_bset_insert(b, top, k);\n\t\t\t} else {\n\t\t\t\tBKEY_PADDED(key) temp;\n\t\t\t\tbkey_copy(&temp.key, k);\n\t\t\t\tbch_bset_insert(b, k, &temp.key);\n\t\t\t\ttop = bkey_next(k);\n\t\t\t}\n\n\t\t\tbch_cut_front(insert, top);\n\t\t\tbch_cut_back(&START_KEY(insert), k);\n\t\t\tbch_bset_fix_invalidated_key(b, k);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (bkey_cmp(insert, k) < 0) {\n\t\t\tbch_cut_front(insert, k);\n\t\t} else {\n\t\t\tif (bkey_cmp(&START_KEY(insert), &START_KEY(k)) > 0)\n\t\t\t\told_offset = KEY_START(insert);\n\n\t\t\tif (bkey_written(b, k) &&\n\t\t\t    bkey_cmp(&START_KEY(insert), &START_KEY(k)) <= 0) {\n\t\t\t\t \n\t\t\t\tbch_cut_front(k, k);\n\t\t\t} else {\n\t\t\t\t__bch_cut_back(&START_KEY(insert), k);\n\t\t\t\tbch_bset_fix_invalidated_key(b, k);\n\t\t\t}\n\t\t}\n\n\t\tbch_subtract_dirty(k, c, old_offset, old_size - KEY_SIZE(k));\n\t}\n\ncheck_failed:\n\tif (replace_key) {\n\t\tif (!sectors_found) {\n\t\t\treturn true;\n\t\t} else if (sectors_found < KEY_SIZE(insert)) {\n\t\t\tSET_KEY_OFFSET(insert, KEY_OFFSET(insert) -\n\t\t\t\t       (KEY_SIZE(insert) - sectors_found));\n\t\t\tSET_KEY_SIZE(insert, sectors_found);\n\t\t}\n\t}\nout:\n\tif (KEY_DIRTY(insert))\n\t\tbcache_dev_sectors_dirty_add(c, KEY_INODE(insert),\n\t\t\t\t\t     KEY_START(insert),\n\t\t\t\t\t     KEY_SIZE(insert));\n\n\treturn false;\n}\n\nbool __bch_extent_invalid(struct cache_set *c, const struct bkey *k)\n{\n\tchar buf[80];\n\n\tif (!KEY_SIZE(k))\n\t\treturn true;\n\n\tif (KEY_SIZE(k) > KEY_OFFSET(k))\n\t\tgoto bad;\n\n\tif (__ptr_invalid(c, k))\n\t\tgoto bad;\n\n\treturn false;\nbad:\n\tbch_extent_to_text(buf, sizeof(buf), k);\n\tcache_bug(c, \"spotted extent %s: %s\", buf, bch_ptr_status(c, k));\n\treturn true;\n}\n\nstatic bool bch_extent_invalid(struct btree_keys *bk, const struct bkey *k)\n{\n\tstruct btree *b = container_of(bk, struct btree, keys);\n\n\treturn __bch_extent_invalid(b->c, k);\n}\n\nstatic bool bch_extent_bad_expensive(struct btree *b, const struct bkey *k,\n\t\t\t\t     unsigned int ptr)\n{\n\tstruct bucket *g = PTR_BUCKET(b->c, k, ptr);\n\tchar buf[80];\n\n\tif (mutex_trylock(&b->c->bucket_lock)) {\n\t\tif (b->c->gc_mark_valid &&\n\t\t    (!GC_MARK(g) ||\n\t\t     GC_MARK(g) == GC_MARK_METADATA ||\n\t\t     (GC_MARK(g) != GC_MARK_DIRTY && KEY_DIRTY(k))))\n\t\t\tgoto err;\n\n\t\tif (g->prio == BTREE_PRIO)\n\t\t\tgoto err;\n\n\t\tmutex_unlock(&b->c->bucket_lock);\n\t}\n\n\treturn false;\nerr:\n\tmutex_unlock(&b->c->bucket_lock);\n\tbch_extent_to_text(buf, sizeof(buf), k);\n\tbtree_bug(b,\n\"inconsistent extent pointer %s:\\nbucket %zu pin %i prio %i gen %i last_gc %i mark %llu\",\n\t\t  buf, PTR_BUCKET_NR(b->c, k, ptr), atomic_read(&g->pin),\n\t\t  g->prio, g->gen, g->last_gc, GC_MARK(g));\n\treturn true;\n}\n\nstatic bool bch_extent_bad(struct btree_keys *bk, const struct bkey *k)\n{\n\tstruct btree *b = container_of(bk, struct btree, keys);\n\tunsigned int i, stale;\n\tchar buf[80];\n\n\tif (!KEY_PTRS(k) ||\n\t    bch_extent_invalid(bk, k))\n\t\treturn true;\n\n\tfor (i = 0; i < KEY_PTRS(k); i++)\n\t\tif (!ptr_available(b->c, k, i))\n\t\t\treturn true;\n\n\tfor (i = 0; i < KEY_PTRS(k); i++) {\n\t\tstale = ptr_stale(b->c, k, i);\n\n\t\tif (stale && KEY_DIRTY(k)) {\n\t\t\tbch_extent_to_text(buf, sizeof(buf), k);\n\t\t\tpr_info(\"stale dirty pointer, stale %u, key: %s\\n\",\n\t\t\t\tstale, buf);\n\t\t}\n\n\t\tbtree_bug_on(stale > BUCKET_GC_GEN_MAX, b,\n\t\t\t     \"key too stale: %i, need_gc %u\",\n\t\t\t     stale, b->c->need_gc);\n\n\t\tif (stale)\n\t\t\treturn true;\n\n\t\tif (expensive_debug_checks(b->c) &&\n\t\t    bch_extent_bad_expensive(b, k, i))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic uint64_t merge_chksums(struct bkey *l, struct bkey *r)\n{\n\treturn (l->ptr[KEY_PTRS(l)] + r->ptr[KEY_PTRS(r)]) &\n\t\t~((uint64_t)1 << 63);\n}\n\nstatic bool bch_extent_merge(struct btree_keys *bk,\n\t\t\t     struct bkey *l,\n\t\t\t     struct bkey *r)\n{\n\tstruct btree *b = container_of(bk, struct btree, keys);\n\tunsigned int i;\n\n\tif (key_merging_disabled(b->c))\n\t\treturn false;\n\n\tfor (i = 0; i < KEY_PTRS(l); i++)\n\t\tif (l->ptr[i] + MAKE_PTR(0, KEY_SIZE(l), 0) != r->ptr[i] ||\n\t\t    PTR_BUCKET_NR(b->c, l, i) != PTR_BUCKET_NR(b->c, r, i))\n\t\t\treturn false;\n\n\t \n\tif (KEY_SIZE(l) + KEY_SIZE(r) > USHRT_MAX) {\n\t\tSET_KEY_OFFSET(l, KEY_OFFSET(l) + USHRT_MAX - KEY_SIZE(l));\n\t\tSET_KEY_SIZE(l, USHRT_MAX);\n\n\t\tbch_cut_front(l, r);\n\t\treturn false;\n\t}\n\n\tif (KEY_CSUM(l)) {\n\t\tif (KEY_CSUM(r))\n\t\t\tl->ptr[KEY_PTRS(l)] = merge_chksums(l, r);\n\t\telse\n\t\t\tSET_KEY_CSUM(l, 0);\n\t}\n\n\tSET_KEY_OFFSET(l, KEY_OFFSET(l) + KEY_SIZE(r));\n\tSET_KEY_SIZE(l, KEY_SIZE(l) + KEY_SIZE(r));\n\n\treturn true;\n}\n\nconst struct btree_keys_ops bch_extent_keys_ops = {\n\t.sort_cmp\t= bch_extent_sort_cmp,\n\t.sort_fixup\t= bch_extent_sort_fixup,\n\t.insert_fixup\t= bch_extent_insert_fixup,\n\t.key_invalid\t= bch_extent_invalid,\n\t.key_bad\t= bch_extent_bad,\n\t.key_merge\t= bch_extent_merge,\n\t.key_to_text\t= bch_extent_to_text,\n\t.key_dump\t= bch_bkey_dump,\n\t.is_extents\t= true,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}