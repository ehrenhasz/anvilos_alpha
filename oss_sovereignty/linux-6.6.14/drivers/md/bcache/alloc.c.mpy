{
  "module_name": "alloc.c",
  "hash_id": "239392dc15e65a65510dbf86e291b6a7541d05fd4898553a187458dd2f7342e6",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/bcache/alloc.c",
  "human_readable_source": "\n \n\n#include \"bcache.h\"\n#include \"btree.h\"\n\n#include <linux/blkdev.h>\n#include <linux/kthread.h>\n#include <linux/random.h>\n#include <trace/events/bcache.h>\n\n#define MAX_OPEN_BUCKETS 128\n\n \n\nuint8_t bch_inc_gen(struct cache *ca, struct bucket *b)\n{\n\tuint8_t ret = ++b->gen;\n\n\tca->set->need_gc = max(ca->set->need_gc, bucket_gc_gen(b));\n\tWARN_ON_ONCE(ca->set->need_gc > BUCKET_GC_GEN_MAX);\n\n\treturn ret;\n}\n\nvoid bch_rescale_priorities(struct cache_set *c, int sectors)\n{\n\tstruct cache *ca;\n\tstruct bucket *b;\n\tunsigned long next = c->nbuckets * c->cache->sb.bucket_size / 1024;\n\tint r;\n\n\tatomic_sub(sectors, &c->rescale);\n\n\tdo {\n\t\tr = atomic_read(&c->rescale);\n\n\t\tif (r >= 0)\n\t\t\treturn;\n\t} while (atomic_cmpxchg(&c->rescale, r, r + next) != r);\n\n\tmutex_lock(&c->bucket_lock);\n\n\tc->min_prio = USHRT_MAX;\n\n\tca = c->cache;\n\tfor_each_bucket(b, ca)\n\t\tif (b->prio &&\n\t\t    b->prio != BTREE_PRIO &&\n\t\t    !atomic_read(&b->pin)) {\n\t\t\tb->prio--;\n\t\t\tc->min_prio = min(c->min_prio, b->prio);\n\t\t}\n\n\tmutex_unlock(&c->bucket_lock);\n}\n\n \n\nstatic inline bool can_inc_bucket_gen(struct bucket *b)\n{\n\treturn bucket_gc_gen(b) < BUCKET_GC_GEN_MAX;\n}\n\nbool bch_can_invalidate_bucket(struct cache *ca, struct bucket *b)\n{\n\tBUG_ON(!ca->set->gc_mark_valid);\n\n\treturn (!GC_MARK(b) ||\n\t\tGC_MARK(b) == GC_MARK_RECLAIMABLE) &&\n\t\t!atomic_read(&b->pin) &&\n\t\tcan_inc_bucket_gen(b);\n}\n\nvoid __bch_invalidate_one_bucket(struct cache *ca, struct bucket *b)\n{\n\tlockdep_assert_held(&ca->set->bucket_lock);\n\tBUG_ON(GC_MARK(b) && GC_MARK(b) != GC_MARK_RECLAIMABLE);\n\n\tif (GC_SECTORS_USED(b))\n\t\ttrace_bcache_invalidate(ca, b - ca->buckets);\n\n\tbch_inc_gen(ca, b);\n\tb->prio = INITIAL_PRIO;\n\tatomic_inc(&b->pin);\n}\n\nstatic void bch_invalidate_one_bucket(struct cache *ca, struct bucket *b)\n{\n\t__bch_invalidate_one_bucket(ca, b);\n\n\tfifo_push(&ca->free_inc, b - ca->buckets);\n}\n\n \n\n#define bucket_prio(b)\t\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tunsigned int min_prio = (INITIAL_PRIO - ca->set->min_prio) / 8;\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t(b->prio - ca->set->min_prio + min_prio) * GC_SECTORS_USED(b);\t\\\n})\n\n#define bucket_max_cmp(l, r)\t(bucket_prio(l) < bucket_prio(r))\n#define bucket_min_cmp(l, r)\t(bucket_prio(l) > bucket_prio(r))\n\nstatic void invalidate_buckets_lru(struct cache *ca)\n{\n\tstruct bucket *b;\n\tssize_t i;\n\n\tca->heap.used = 0;\n\n\tfor_each_bucket(b, ca) {\n\t\tif (!bch_can_invalidate_bucket(ca, b))\n\t\t\tcontinue;\n\n\t\tif (!heap_full(&ca->heap))\n\t\t\theap_add(&ca->heap, b, bucket_max_cmp);\n\t\telse if (bucket_max_cmp(b, heap_peek(&ca->heap))) {\n\t\t\tca->heap.data[0] = b;\n\t\t\theap_sift(&ca->heap, 0, bucket_max_cmp);\n\t\t}\n\t}\n\n\tfor (i = ca->heap.used / 2 - 1; i >= 0; --i)\n\t\theap_sift(&ca->heap, i, bucket_min_cmp);\n\n\twhile (!fifo_full(&ca->free_inc)) {\n\t\tif (!heap_pop(&ca->heap, b, bucket_min_cmp)) {\n\t\t\t \n\t\t\tca->invalidate_needs_gc = 1;\n\t\t\twake_up_gc(ca->set);\n\t\t\treturn;\n\t\t}\n\n\t\tbch_invalidate_one_bucket(ca, b);\n\t}\n}\n\nstatic void invalidate_buckets_fifo(struct cache *ca)\n{\n\tstruct bucket *b;\n\tsize_t checked = 0;\n\n\twhile (!fifo_full(&ca->free_inc)) {\n\t\tif (ca->fifo_last_bucket <  ca->sb.first_bucket ||\n\t\t    ca->fifo_last_bucket >= ca->sb.nbuckets)\n\t\t\tca->fifo_last_bucket = ca->sb.first_bucket;\n\n\t\tb = ca->buckets + ca->fifo_last_bucket++;\n\n\t\tif (bch_can_invalidate_bucket(ca, b))\n\t\t\tbch_invalidate_one_bucket(ca, b);\n\n\t\tif (++checked >= ca->sb.nbuckets) {\n\t\t\tca->invalidate_needs_gc = 1;\n\t\t\twake_up_gc(ca->set);\n\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic void invalidate_buckets_random(struct cache *ca)\n{\n\tstruct bucket *b;\n\tsize_t checked = 0;\n\n\twhile (!fifo_full(&ca->free_inc)) {\n\t\tsize_t n;\n\n\t\tget_random_bytes(&n, sizeof(n));\n\n\t\tn %= (size_t) (ca->sb.nbuckets - ca->sb.first_bucket);\n\t\tn += ca->sb.first_bucket;\n\n\t\tb = ca->buckets + n;\n\n\t\tif (bch_can_invalidate_bucket(ca, b))\n\t\t\tbch_invalidate_one_bucket(ca, b);\n\n\t\tif (++checked >= ca->sb.nbuckets / 2) {\n\t\t\tca->invalidate_needs_gc = 1;\n\t\t\twake_up_gc(ca->set);\n\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic void invalidate_buckets(struct cache *ca)\n{\n\tBUG_ON(ca->invalidate_needs_gc);\n\n\tswitch (CACHE_REPLACEMENT(&ca->sb)) {\n\tcase CACHE_REPLACEMENT_LRU:\n\t\tinvalidate_buckets_lru(ca);\n\t\tbreak;\n\tcase CACHE_REPLACEMENT_FIFO:\n\t\tinvalidate_buckets_fifo(ca);\n\t\tbreak;\n\tcase CACHE_REPLACEMENT_RANDOM:\n\t\tinvalidate_buckets_random(ca);\n\t\tbreak;\n\t}\n}\n\n#define allocator_wait(ca, cond)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\twhile (1) {\t\t\t\t\t\t\t\\\n\t\tset_current_state(TASK_INTERRUPTIBLE);\t\t\t\\\n\t\tif (cond)\t\t\t\t\t\t\\\n\t\t\tbreak;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\tmutex_unlock(&(ca)->set->bucket_lock);\t\t\t\\\n\t\tif (kthread_should_stop() ||\t\t\t\t\\\n\t\t    test_bit(CACHE_SET_IO_DISABLE, &ca->set->flags)) {\t\\\n\t\t\tset_current_state(TASK_RUNNING);\t\t\\\n\t\t\tgoto out;\t\t\t\t\t\\\n\t\t}\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\tschedule();\t\t\t\t\t\t\\\n\t\tmutex_lock(&(ca)->set->bucket_lock);\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\t__set_current_state(TASK_RUNNING);\t\t\t\t\\\n} while (0)\n\nstatic int bch_allocator_push(struct cache *ca, long bucket)\n{\n\tunsigned int i;\n\n\t \n\tif (fifo_push(&ca->free[RESERVE_PRIO], bucket))\n\t\treturn true;\n\n\tfor (i = 0; i < RESERVE_NR; i++)\n\t\tif (fifo_push(&ca->free[i], bucket))\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic int bch_allocator_thread(void *arg)\n{\n\tstruct cache *ca = arg;\n\n\tmutex_lock(&ca->set->bucket_lock);\n\n\twhile (1) {\n\t\t \n\t\twhile (1) {\n\t\t\tlong bucket;\n\n\t\t\tif (!fifo_pop(&ca->free_inc, bucket))\n\t\t\t\tbreak;\n\n\t\t\tif (ca->discard) {\n\t\t\t\tmutex_unlock(&ca->set->bucket_lock);\n\t\t\t\tblkdev_issue_discard(ca->bdev,\n\t\t\t\t\tbucket_to_sector(ca->set, bucket),\n\t\t\t\t\tca->sb.bucket_size, GFP_KERNEL);\n\t\t\t\tmutex_lock(&ca->set->bucket_lock);\n\t\t\t}\n\n\t\t\tallocator_wait(ca, bch_allocator_push(ca, bucket));\n\t\t\twake_up(&ca->set->btree_cache_wait);\n\t\t\twake_up(&ca->set->bucket_wait);\n\t\t}\n\n\t\t \n\nretry_invalidate:\n\t\tallocator_wait(ca, ca->set->gc_mark_valid &&\n\t\t\t       !ca->invalidate_needs_gc);\n\t\tinvalidate_buckets(ca);\n\n\t\t \n\t\tallocator_wait(ca, !atomic_read(&ca->set->prio_blocked));\n\t\tif (CACHE_SYNC(&ca->sb)) {\n\t\t\t \n\t\t\tif (!fifo_full(&ca->free_inc))\n\t\t\t\tgoto retry_invalidate;\n\n\t\t\tif (bch_prio_write(ca, false) < 0) {\n\t\t\t\tca->invalidate_needs_gc = 1;\n\t\t\t\twake_up_gc(ca->set);\n\t\t\t}\n\t\t}\n\t}\nout:\n\twait_for_kthread_stop();\n\treturn 0;\n}\n\n \n\nlong bch_bucket_alloc(struct cache *ca, unsigned int reserve, bool wait)\n{\n\tDEFINE_WAIT(w);\n\tstruct bucket *b;\n\tlong r;\n\n\n\t \n\tif (unlikely(test_bit(CACHE_SET_IO_DISABLE, &ca->set->flags)))\n\t\treturn -1;\n\n\t \n\tif (fifo_pop(&ca->free[RESERVE_NONE], r) ||\n\t    fifo_pop(&ca->free[reserve], r))\n\t\tgoto out;\n\n\tif (!wait) {\n\t\ttrace_bcache_alloc_fail(ca, reserve);\n\t\treturn -1;\n\t}\n\n\tdo {\n\t\tprepare_to_wait(&ca->set->bucket_wait, &w,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\n\t\tmutex_unlock(&ca->set->bucket_lock);\n\t\tschedule();\n\t\tmutex_lock(&ca->set->bucket_lock);\n\t} while (!fifo_pop(&ca->free[RESERVE_NONE], r) &&\n\t\t !fifo_pop(&ca->free[reserve], r));\n\n\tfinish_wait(&ca->set->bucket_wait, &w);\nout:\n\tif (ca->alloc_thread)\n\t\twake_up_process(ca->alloc_thread);\n\n\ttrace_bcache_alloc(ca, reserve);\n\n\tif (expensive_debug_checks(ca->set)) {\n\t\tsize_t iter;\n\t\tlong i;\n\t\tunsigned int j;\n\n\t\tfor (iter = 0; iter < prio_buckets(ca) * 2; iter++)\n\t\t\tBUG_ON(ca->prio_buckets[iter] == (uint64_t) r);\n\n\t\tfor (j = 0; j < RESERVE_NR; j++)\n\t\t\tfifo_for_each(i, &ca->free[j], iter)\n\t\t\t\tBUG_ON(i == r);\n\t\tfifo_for_each(i, &ca->free_inc, iter)\n\t\t\tBUG_ON(i == r);\n\t}\n\n\tb = ca->buckets + r;\n\n\tBUG_ON(atomic_read(&b->pin) != 1);\n\n\tSET_GC_SECTORS_USED(b, ca->sb.bucket_size);\n\n\tif (reserve <= RESERVE_PRIO) {\n\t\tSET_GC_MARK(b, GC_MARK_METADATA);\n\t\tSET_GC_MOVE(b, 0);\n\t\tb->prio = BTREE_PRIO;\n\t} else {\n\t\tSET_GC_MARK(b, GC_MARK_RECLAIMABLE);\n\t\tSET_GC_MOVE(b, 0);\n\t\tb->prio = INITIAL_PRIO;\n\t}\n\n\tif (ca->set->avail_nbuckets > 0) {\n\t\tca->set->avail_nbuckets--;\n\t\tbch_update_bucket_in_use(ca->set, &ca->set->gc_stats);\n\t}\n\n\treturn r;\n}\n\nvoid __bch_bucket_free(struct cache *ca, struct bucket *b)\n{\n\tSET_GC_MARK(b, 0);\n\tSET_GC_SECTORS_USED(b, 0);\n\n\tif (ca->set->avail_nbuckets < ca->set->nbuckets) {\n\t\tca->set->avail_nbuckets++;\n\t\tbch_update_bucket_in_use(ca->set, &ca->set->gc_stats);\n\t}\n}\n\nvoid bch_bucket_free(struct cache_set *c, struct bkey *k)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < KEY_PTRS(k); i++)\n\t\t__bch_bucket_free(c->cache, PTR_BUCKET(c, k, i));\n}\n\nint __bch_bucket_alloc_set(struct cache_set *c, unsigned int reserve,\n\t\t\t   struct bkey *k, bool wait)\n{\n\tstruct cache *ca;\n\tlong b;\n\n\t \n\tif (unlikely(test_bit(CACHE_SET_IO_DISABLE, &c->flags)))\n\t\treturn -1;\n\n\tlockdep_assert_held(&c->bucket_lock);\n\n\tbkey_init(k);\n\n\tca = c->cache;\n\tb = bch_bucket_alloc(ca, reserve, wait);\n\tif (b == -1)\n\t\tgoto err;\n\n\tk->ptr[0] = MAKE_PTR(ca->buckets[b].gen,\n\t\t\t     bucket_to_sector(c, b),\n\t\t\t     ca->sb.nr_this_dev);\n\n\tSET_KEY_PTRS(k, 1);\n\n\treturn 0;\nerr:\n\tbch_bucket_free(c, k);\n\tbkey_put(c, k);\n\treturn -1;\n}\n\nint bch_bucket_alloc_set(struct cache_set *c, unsigned int reserve,\n\t\t\t struct bkey *k, bool wait)\n{\n\tint ret;\n\n\tmutex_lock(&c->bucket_lock);\n\tret = __bch_bucket_alloc_set(c, reserve, k, wait);\n\tmutex_unlock(&c->bucket_lock);\n\treturn ret;\n}\n\n \n\nstruct open_bucket {\n\tstruct list_head\tlist;\n\tunsigned int\t\tlast_write_point;\n\tunsigned int\t\tsectors_free;\n\tBKEY_PADDED(key);\n};\n\n \nstatic struct open_bucket *pick_data_bucket(struct cache_set *c,\n\t\t\t\t\t    const struct bkey *search,\n\t\t\t\t\t    unsigned int write_point,\n\t\t\t\t\t    struct bkey *alloc)\n{\n\tstruct open_bucket *ret, *ret_task = NULL;\n\n\tlist_for_each_entry_reverse(ret, &c->data_buckets, list)\n\t\tif (UUID_FLASH_ONLY(&c->uuids[KEY_INODE(&ret->key)]) !=\n\t\t    UUID_FLASH_ONLY(&c->uuids[KEY_INODE(search)]))\n\t\t\tcontinue;\n\t\telse if (!bkey_cmp(&ret->key, search))\n\t\t\tgoto found;\n\t\telse if (ret->last_write_point == write_point)\n\t\t\tret_task = ret;\n\n\tret = ret_task ?: list_first_entry(&c->data_buckets,\n\t\t\t\t\t   struct open_bucket, list);\nfound:\n\tif (!ret->sectors_free && KEY_PTRS(alloc)) {\n\t\tret->sectors_free = c->cache->sb.bucket_size;\n\t\tbkey_copy(&ret->key, alloc);\n\t\tbkey_init(alloc);\n\t}\n\n\tif (!ret->sectors_free)\n\t\tret = NULL;\n\n\treturn ret;\n}\n\n \nbool bch_alloc_sectors(struct cache_set *c,\n\t\t       struct bkey *k,\n\t\t       unsigned int sectors,\n\t\t       unsigned int write_point,\n\t\t       unsigned int write_prio,\n\t\t       bool wait)\n{\n\tstruct open_bucket *b;\n\tBKEY_PADDED(key) alloc;\n\tunsigned int i;\n\n\t \n\n\tbkey_init(&alloc.key);\n\tspin_lock(&c->data_bucket_lock);\n\n\twhile (!(b = pick_data_bucket(c, k, write_point, &alloc.key))) {\n\t\tunsigned int watermark = write_prio\n\t\t\t? RESERVE_MOVINGGC\n\t\t\t: RESERVE_NONE;\n\n\t\tspin_unlock(&c->data_bucket_lock);\n\n\t\tif (bch_bucket_alloc_set(c, watermark, &alloc.key, wait))\n\t\t\treturn false;\n\n\t\tspin_lock(&c->data_bucket_lock);\n\t}\n\n\t \n\tif (KEY_PTRS(&alloc.key))\n\t\tbkey_put(c, &alloc.key);\n\n\tfor (i = 0; i < KEY_PTRS(&b->key); i++)\n\t\tEBUG_ON(ptr_stale(c, &b->key, i));\n\n\t \n\n\tfor (i = 0; i < KEY_PTRS(&b->key); i++)\n\t\tk->ptr[i] = b->key.ptr[i];\n\n\tsectors = min(sectors, b->sectors_free);\n\n\tSET_KEY_OFFSET(k, KEY_OFFSET(k) + sectors);\n\tSET_KEY_SIZE(k, sectors);\n\tSET_KEY_PTRS(k, KEY_PTRS(&b->key));\n\n\t \n\tlist_move_tail(&b->list, &c->data_buckets);\n\tbkey_copy_key(&b->key, k);\n\tb->last_write_point = write_point;\n\n\tb->sectors_free\t-= sectors;\n\n\tfor (i = 0; i < KEY_PTRS(&b->key); i++) {\n\t\tSET_PTR_OFFSET(&b->key, i, PTR_OFFSET(&b->key, i) + sectors);\n\n\t\tatomic_long_add(sectors,\n\t\t\t\t&c->cache->sectors_written);\n\t}\n\n\tif (b->sectors_free < c->cache->sb.block_size)\n\t\tb->sectors_free = 0;\n\n\t \n\tif (b->sectors_free)\n\t\tfor (i = 0; i < KEY_PTRS(&b->key); i++)\n\t\t\tatomic_inc(&PTR_BUCKET(c, &b->key, i)->pin);\n\n\tspin_unlock(&c->data_bucket_lock);\n\treturn true;\n}\n\n \n\nvoid bch_open_buckets_free(struct cache_set *c)\n{\n\tstruct open_bucket *b;\n\n\twhile (!list_empty(&c->data_buckets)) {\n\t\tb = list_first_entry(&c->data_buckets,\n\t\t\t\t     struct open_bucket, list);\n\t\tlist_del(&b->list);\n\t\tkfree(b);\n\t}\n}\n\nint bch_open_buckets_alloc(struct cache_set *c)\n{\n\tint i;\n\n\tspin_lock_init(&c->data_bucket_lock);\n\n\tfor (i = 0; i < MAX_OPEN_BUCKETS; i++) {\n\t\tstruct open_bucket *b = kzalloc(sizeof(*b), GFP_KERNEL);\n\n\t\tif (!b)\n\t\t\treturn -ENOMEM;\n\n\t\tlist_add(&b->list, &c->data_buckets);\n\t}\n\n\treturn 0;\n}\n\nint bch_cache_allocator_start(struct cache *ca)\n{\n\tstruct task_struct *k = kthread_run(bch_allocator_thread,\n\t\t\t\t\t    ca, \"bcache_allocator\");\n\tif (IS_ERR(k))\n\t\treturn PTR_ERR(k);\n\n\tca->alloc_thread = k;\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}