{
  "module_name": "movinggc.c",
  "hash_id": "806d319c62789b6d4e57a78685cea01b5fae22baad89357946ce3509d23da280",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/bcache/movinggc.c",
  "human_readable_source": "\n \n\n#include \"bcache.h\"\n#include \"btree.h\"\n#include \"debug.h\"\n#include \"request.h\"\n\n#include <trace/events/bcache.h>\n\nstruct moving_io {\n\tstruct closure\t\tcl;\n\tstruct keybuf_key\t*w;\n\tstruct data_insert_op\top;\n\tstruct bbio\t\tbio;\n};\n\nstatic bool moving_pred(struct keybuf *buf, struct bkey *k)\n{\n\tstruct cache_set *c = container_of(buf, struct cache_set,\n\t\t\t\t\t   moving_gc_keys);\n\tunsigned int i;\n\n\tfor (i = 0; i < KEY_PTRS(k); i++)\n\t\tif (ptr_available(c, k, i) &&\n\t\t    GC_MOVE(PTR_BUCKET(c, k, i)))\n\t\t\treturn true;\n\n\treturn false;\n}\n\n \n\nstatic void moving_io_destructor(struct closure *cl)\n{\n\tstruct moving_io *io = container_of(cl, struct moving_io, cl);\n\n\tkfree(io);\n}\n\nstatic void write_moving_finish(struct closure *cl)\n{\n\tstruct moving_io *io = container_of(cl, struct moving_io, cl);\n\tstruct bio *bio = &io->bio.bio;\n\n\tbio_free_pages(bio);\n\n\tif (io->op.replace_collision)\n\t\ttrace_bcache_gc_copy_collision(&io->w->key);\n\n\tbch_keybuf_del(&io->op.c->moving_gc_keys, io->w);\n\n\tup(&io->op.c->moving_in_flight);\n\n\tclosure_return_with_destructor(cl, moving_io_destructor);\n}\n\nstatic void read_moving_endio(struct bio *bio)\n{\n\tstruct bbio *b = container_of(bio, struct bbio, bio);\n\tstruct moving_io *io = container_of(bio->bi_private,\n\t\t\t\t\t    struct moving_io, cl);\n\n\tif (bio->bi_status)\n\t\tio->op.status = bio->bi_status;\n\telse if (!KEY_DIRTY(&b->key) &&\n\t\t ptr_stale(io->op.c, &b->key, 0)) {\n\t\tio->op.status = BLK_STS_IOERR;\n\t}\n\n\tbch_bbio_endio(io->op.c, bio, bio->bi_status, \"reading data to move\");\n}\n\nstatic void moving_init(struct moving_io *io)\n{\n\tstruct bio *bio = &io->bio.bio;\n\n\tbio_init(bio, NULL, bio->bi_inline_vecs,\n\t\t DIV_ROUND_UP(KEY_SIZE(&io->w->key), PAGE_SECTORS), 0);\n\tbio_get(bio);\n\tbio_set_prio(bio, IOPRIO_PRIO_VALUE(IOPRIO_CLASS_IDLE, 0));\n\n\tbio->bi_iter.bi_size\t= KEY_SIZE(&io->w->key) << 9;\n\tbio->bi_private\t\t= &io->cl;\n\tbch_bio_map(bio, NULL);\n}\n\nstatic void write_moving(struct closure *cl)\n{\n\tstruct moving_io *io = container_of(cl, struct moving_io, cl);\n\tstruct data_insert_op *op = &io->op;\n\n\tif (!op->status) {\n\t\tmoving_init(io);\n\n\t\tio->bio.bio.bi_iter.bi_sector = KEY_START(&io->w->key);\n\t\top->write_prio\t\t= 1;\n\t\top->bio\t\t\t= &io->bio.bio;\n\n\t\top->writeback\t\t= KEY_DIRTY(&io->w->key);\n\t\top->csum\t\t= KEY_CSUM(&io->w->key);\n\n\t\tbkey_copy(&op->replace_key, &io->w->key);\n\t\top->replace\t\t= true;\n\n\t\tclosure_call(&op->cl, bch_data_insert, NULL, cl);\n\t}\n\n\tcontinue_at(cl, write_moving_finish, op->wq);\n}\n\nstatic void read_moving_submit(struct closure *cl)\n{\n\tstruct moving_io *io = container_of(cl, struct moving_io, cl);\n\tstruct bio *bio = &io->bio.bio;\n\n\tbch_submit_bbio(bio, io->op.c, &io->w->key, 0);\n\n\tcontinue_at(cl, write_moving, io->op.wq);\n}\n\nstatic void read_moving(struct cache_set *c)\n{\n\tstruct keybuf_key *w;\n\tstruct moving_io *io;\n\tstruct bio *bio;\n\tstruct closure cl;\n\n\tclosure_init_stack(&cl);\n\n\t \n\n\twhile (!test_bit(CACHE_SET_STOPPING, &c->flags)) {\n\t\tw = bch_keybuf_next_rescan(c, &c->moving_gc_keys,\n\t\t\t\t\t   &MAX_KEY, moving_pred);\n\t\tif (!w)\n\t\t\tbreak;\n\n\t\tif (ptr_stale(c, &w->key, 0)) {\n\t\t\tbch_keybuf_del(&c->moving_gc_keys, w);\n\t\t\tcontinue;\n\t\t}\n\n\t\tio = kzalloc(struct_size(io, bio.bio.bi_inline_vecs,\n\t\t\t\t\t DIV_ROUND_UP(KEY_SIZE(&w->key), PAGE_SECTORS)),\n\t\t\t     GFP_KERNEL);\n\t\tif (!io)\n\t\t\tgoto err;\n\n\t\tw->private\t= io;\n\t\tio->w\t\t= w;\n\t\tio->op.inode\t= KEY_INODE(&w->key);\n\t\tio->op.c\t= c;\n\t\tio->op.wq\t= c->moving_gc_wq;\n\n\t\tmoving_init(io);\n\t\tbio = &io->bio.bio;\n\n\t\tbio->bi_opf = REQ_OP_READ;\n\t\tbio->bi_end_io\t= read_moving_endio;\n\n\t\tif (bch_bio_alloc_pages(bio, GFP_KERNEL))\n\t\t\tgoto err;\n\n\t\ttrace_bcache_gc_copy(&w->key);\n\n\t\tdown(&c->moving_in_flight);\n\t\tclosure_call(&io->cl, read_moving_submit, NULL, &cl);\n\t}\n\n\tif (0) {\nerr:\t\tif (!IS_ERR_OR_NULL(w->private))\n\t\t\tkfree(w->private);\n\n\t\tbch_keybuf_del(&c->moving_gc_keys, w);\n\t}\n\n\tclosure_sync(&cl);\n}\n\nstatic bool bucket_cmp(struct bucket *l, struct bucket *r)\n{\n\treturn GC_SECTORS_USED(l) < GC_SECTORS_USED(r);\n}\n\nstatic unsigned int bucket_heap_top(struct cache *ca)\n{\n\tstruct bucket *b;\n\n\treturn (b = heap_peek(&ca->heap)) ? GC_SECTORS_USED(b) : 0;\n}\n\nvoid bch_moving_gc(struct cache_set *c)\n{\n\tstruct cache *ca = c->cache;\n\tstruct bucket *b;\n\tunsigned long sectors_to_move, reserve_sectors;\n\n\tif (!c->copy_gc_enabled)\n\t\treturn;\n\n\tmutex_lock(&c->bucket_lock);\n\n\tsectors_to_move = 0;\n\treserve_sectors = ca->sb.bucket_size *\n\t\t\t     fifo_used(&ca->free[RESERVE_MOVINGGC]);\n\n\tca->heap.used = 0;\n\n\tfor_each_bucket(b, ca) {\n\t\tif (GC_MARK(b) == GC_MARK_METADATA ||\n\t\t    !GC_SECTORS_USED(b) ||\n\t\t    GC_SECTORS_USED(b) == ca->sb.bucket_size ||\n\t\t    atomic_read(&b->pin))\n\t\t\tcontinue;\n\n\t\tif (!heap_full(&ca->heap)) {\n\t\t\tsectors_to_move += GC_SECTORS_USED(b);\n\t\t\theap_add(&ca->heap, b, bucket_cmp);\n\t\t} else if (bucket_cmp(b, heap_peek(&ca->heap))) {\n\t\t\tsectors_to_move -= bucket_heap_top(ca);\n\t\t\tsectors_to_move += GC_SECTORS_USED(b);\n\n\t\t\tca->heap.data[0] = b;\n\t\t\theap_sift(&ca->heap, 0, bucket_cmp);\n\t\t}\n\t}\n\n\twhile (sectors_to_move > reserve_sectors) {\n\t\theap_pop(&ca->heap, b, bucket_cmp);\n\t\tsectors_to_move -= GC_SECTORS_USED(b);\n\t}\n\n\twhile (heap_pop(&ca->heap, b, bucket_cmp))\n\t\tSET_GC_MOVE(b, 1);\n\n\tmutex_unlock(&c->bucket_lock);\n\n\tc->moving_gc_keys.last_scanned = ZERO_KEY;\n\n\tread_moving(c);\n}\n\nvoid bch_moving_init_cache_set(struct cache_set *c)\n{\n\tbch_keybuf_init(&c->moving_gc_keys);\n\tsema_init(&c->moving_in_flight, 64);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}