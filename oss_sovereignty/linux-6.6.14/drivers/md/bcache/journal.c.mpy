{
  "module_name": "journal.c",
  "hash_id": "f0efda9719330b50cc0abaf1ebd3e9ebe35be39c4651bcc4816a25ba131029bc",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/bcache/journal.c",
  "human_readable_source": "\n \n\n#include \"bcache.h\"\n#include \"btree.h\"\n#include \"debug.h\"\n#include \"extents.h\"\n\n#include <trace/events/bcache.h>\n\n \n\nstatic void journal_read_endio(struct bio *bio)\n{\n\tstruct closure *cl = bio->bi_private;\n\n\tclosure_put(cl);\n}\n\nstatic int journal_read_bucket(struct cache *ca, struct list_head *list,\n\t\t\t       unsigned int bucket_index)\n{\n\tstruct journal_device *ja = &ca->journal;\n\tstruct bio *bio = &ja->bio;\n\n\tstruct journal_replay *i;\n\tstruct jset *j, *data = ca->set->journal.w[0].data;\n\tstruct closure cl;\n\tunsigned int len, left, offset = 0;\n\tint ret = 0;\n\tsector_t bucket = bucket_to_sector(ca->set, ca->sb.d[bucket_index]);\n\n\tclosure_init_stack(&cl);\n\n\tpr_debug(\"reading %u\\n\", bucket_index);\n\n\twhile (offset < ca->sb.bucket_size) {\nreread:\t\tleft = ca->sb.bucket_size - offset;\n\t\tlen = min_t(unsigned int, left, PAGE_SECTORS << JSET_BITS);\n\n\t\tbio_reset(bio, ca->bdev, REQ_OP_READ);\n\t\tbio->bi_iter.bi_sector\t= bucket + offset;\n\t\tbio->bi_iter.bi_size\t= len << 9;\n\n\t\tbio->bi_end_io\t= journal_read_endio;\n\t\tbio->bi_private = &cl;\n\t\tbch_bio_map(bio, data);\n\n\t\tclosure_bio_submit(ca->set, bio, &cl);\n\t\tclosure_sync(&cl);\n\n\t\t \n\n\t\tj = data;\n\t\twhile (len) {\n\t\t\tstruct list_head *where;\n\t\t\tsize_t blocks, bytes = set_bytes(j);\n\n\t\t\tif (j->magic != jset_magic(&ca->sb)) {\n\t\t\t\tpr_debug(\"%u: bad magic\\n\", bucket_index);\n\t\t\t\treturn ret;\n\t\t\t}\n\n\t\t\tif (bytes > left << 9 ||\n\t\t\t    bytes > PAGE_SIZE << JSET_BITS) {\n\t\t\t\tpr_info(\"%u: too big, %zu bytes, offset %u\\n\",\n\t\t\t\t\tbucket_index, bytes, offset);\n\t\t\t\treturn ret;\n\t\t\t}\n\n\t\t\tif (bytes > len << 9)\n\t\t\t\tgoto reread;\n\n\t\t\tif (j->csum != csum_set(j)) {\n\t\t\t\tpr_info(\"%u: bad csum, %zu bytes, offset %u\\n\",\n\t\t\t\t\tbucket_index, bytes, offset);\n\t\t\t\treturn ret;\n\t\t\t}\n\n\t\t\tblocks = set_blocks(j, block_bytes(ca));\n\n\t\t\t \n\n\t\t\t \n\t\t\twhile (!list_empty(list)) {\n\t\t\t\ti = list_first_entry(list,\n\t\t\t\t\tstruct journal_replay, list);\n\t\t\t\tif (i->j.seq >= j->last_seq)\n\t\t\t\t\tbreak;\n\t\t\t\tlist_del(&i->list);\n\t\t\t\tkfree(i);\n\t\t\t}\n\n\t\t\t \n\t\t\tlist_for_each_entry_reverse(i, list, list) {\n\t\t\t\tif (j->seq == i->j.seq)\n\t\t\t\t\tgoto next_set;\n\n\t\t\t\t \n\t\t\t\tif (j->seq < i->j.last_seq)\n\t\t\t\t\tgoto next_set;\n\n\t\t\t\t \n\t\t\t\tif (j->seq > i->j.seq) {\n\t\t\t\t\twhere = &i->list;\n\t\t\t\t\tgoto add;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\twhere = list;\nadd:\n\t\t\ti = kmalloc(offsetof(struct journal_replay, j) +\n\t\t\t\t    bytes, GFP_KERNEL);\n\t\t\tif (!i)\n\t\t\t\treturn -ENOMEM;\n\t\t\tunsafe_memcpy(&i->j, j, bytes,\n\t\t\t\t );\n\t\t\t \n\t\t\tlist_add(&i->list, where);\n\t\t\tret = 1;\n\n\t\t\tif (j->seq > ja->seq[bucket_index])\n\t\t\t\tja->seq[bucket_index] = j->seq;\nnext_set:\n\t\t\toffset\t+= blocks * ca->sb.block_size;\n\t\t\tlen\t-= blocks * ca->sb.block_size;\n\t\t\tj = ((void *) j) + blocks * block_bytes(ca);\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nint bch_journal_read(struct cache_set *c, struct list_head *list)\n{\n#define read_bucket(b)\t\t\t\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\t\\\n\t\tret = journal_read_bucket(ca, list, b);\t\t\t\\\n\t\t__set_bit(b, bitmap);\t\t\t\t\t\\\n\t\tif (ret < 0)\t\t\t\t\t\t\\\n\t\t\treturn ret;\t\t\t\t\t\\\n\t\tret;\t\t\t\t\t\t\t\\\n\t})\n\n\tstruct cache *ca = c->cache;\n\tint ret = 0;\n\tstruct journal_device *ja = &ca->journal;\n\tDECLARE_BITMAP(bitmap, SB_JOURNAL_BUCKETS);\n\tunsigned int i, l, r, m;\n\tuint64_t seq;\n\n\tbitmap_zero(bitmap, SB_JOURNAL_BUCKETS);\n\tpr_debug(\"%u journal buckets\\n\", ca->sb.njournal_buckets);\n\n\t \n\tfor (i = 0; i < ca->sb.njournal_buckets; i++) {\n\t\t \n\t\tl = (i * 2654435769U) % ca->sb.njournal_buckets;\n\n\t\tif (test_bit(l, bitmap))\n\t\t\tbreak;\n\n\t\tif (read_bucket(l))\n\t\t\tgoto bsearch;\n\t}\n\n\t \n\tpr_debug(\"falling back to linear search\\n\");\n\n\tfor_each_clear_bit(l, bitmap, ca->sb.njournal_buckets)\n\t\tif (read_bucket(l))\n\t\t\tgoto bsearch;\n\n\t \n\tif (l == ca->sb.njournal_buckets)\n\t\tgoto out;\nbsearch:\n\tBUG_ON(list_empty(list));\n\n\t \n\tm = l;\n\tr = find_next_bit(bitmap, ca->sb.njournal_buckets, l + 1);\n\tpr_debug(\"starting binary search, l %u r %u\\n\", l, r);\n\n\twhile (l + 1 < r) {\n\t\tseq = list_entry(list->prev, struct journal_replay,\n\t\t\t\t list)->j.seq;\n\n\t\tm = (l + r) >> 1;\n\t\tread_bucket(m);\n\n\t\tif (seq != list_entry(list->prev, struct journal_replay,\n\t\t\t\t      list)->j.seq)\n\t\t\tl = m;\n\t\telse\n\t\t\tr = m;\n\t}\n\n\t \n\tpr_debug(\"finishing up: m %u njournal_buckets %u\\n\",\n\t\t m, ca->sb.njournal_buckets);\n\tl = m;\n\n\twhile (1) {\n\t\tif (!l--)\n\t\t\tl = ca->sb.njournal_buckets - 1;\n\n\t\tif (l == m)\n\t\t\tbreak;\n\n\t\tif (test_bit(l, bitmap))\n\t\t\tcontinue;\n\n\t\tif (!read_bucket(l))\n\t\t\tbreak;\n\t}\n\n\tseq = 0;\n\n\tfor (i = 0; i < ca->sb.njournal_buckets; i++)\n\t\tif (ja->seq[i] > seq) {\n\t\t\tseq = ja->seq[i];\n\t\t\t \n\t\t\tja->cur_idx = i;\n\t\t\tja->last_idx = ja->discard_idx = (i + 1) %\n\t\t\t\tca->sb.njournal_buckets;\n\n\t\t}\n\nout:\n\tif (!list_empty(list))\n\t\tc->journal.seq = list_entry(list->prev,\n\t\t\t\t\t    struct journal_replay,\n\t\t\t\t\t    list)->j.seq;\n\n\treturn 0;\n#undef read_bucket\n}\n\nvoid bch_journal_mark(struct cache_set *c, struct list_head *list)\n{\n\tatomic_t p = { 0 };\n\tstruct bkey *k;\n\tstruct journal_replay *i;\n\tstruct journal *j = &c->journal;\n\tuint64_t last = j->seq;\n\n\t \n\n\tlist_for_each_entry_reverse(i, list, list) {\n\t\tBUG_ON(last < i->j.seq);\n\t\ti->pin = NULL;\n\n\t\twhile (last-- != i->j.seq)\n\t\t\tif (fifo_free(&j->pin) > 1) {\n\t\t\t\tfifo_push_front(&j->pin, p);\n\t\t\t\tatomic_set(&fifo_front(&j->pin), 0);\n\t\t\t}\n\n\t\tif (fifo_free(&j->pin) > 1) {\n\t\t\tfifo_push_front(&j->pin, p);\n\t\t\ti->pin = &fifo_front(&j->pin);\n\t\t\tatomic_set(i->pin, 1);\n\t\t}\n\n\t\tfor (k = i->j.start;\n\t\t     k < bset_bkey_last(&i->j);\n\t\t     k = bkey_next(k))\n\t\t\tif (!__bch_extent_invalid(c, k)) {\n\t\t\t\tunsigned int j;\n\n\t\t\t\tfor (j = 0; j < KEY_PTRS(k); j++)\n\t\t\t\t\tif (ptr_available(c, k, j))\n\t\t\t\t\t\tatomic_inc(&PTR_BUCKET(c, k, j)->pin);\n\n\t\t\t\tbch_initial_mark_key(c, 0, k);\n\t\t\t}\n\t}\n}\n\nstatic bool is_discard_enabled(struct cache_set *s)\n{\n\tstruct cache *ca = s->cache;\n\n\tif (ca->discard)\n\t\treturn true;\n\n\treturn false;\n}\n\nint bch_journal_replay(struct cache_set *s, struct list_head *list)\n{\n\tint ret = 0, keys = 0, entries = 0;\n\tstruct bkey *k;\n\tstruct journal_replay *i =\n\t\tlist_entry(list->prev, struct journal_replay, list);\n\n\tuint64_t start = i->j.last_seq, end = i->j.seq, n = start;\n\tstruct keylist keylist;\n\n\tlist_for_each_entry(i, list, list) {\n\t\tBUG_ON(i->pin && atomic_read(i->pin) != 1);\n\n\t\tif (n != i->j.seq) {\n\t\t\tif (n == start && is_discard_enabled(s))\n\t\t\t\tpr_info(\"journal entries %llu-%llu may be discarded! (replaying %llu-%llu)\\n\",\n\t\t\t\t\tn, i->j.seq - 1, start, end);\n\t\t\telse {\n\t\t\t\tpr_err(\"journal entries %llu-%llu missing! (replaying %llu-%llu)\\n\",\n\t\t\t\t\tn, i->j.seq - 1, start, end);\n\t\t\t\tret = -EIO;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t}\n\n\t\tfor (k = i->j.start;\n\t\t     k < bset_bkey_last(&i->j);\n\t\t     k = bkey_next(k)) {\n\t\t\ttrace_bcache_journal_replay_key(k);\n\n\t\t\tbch_keylist_init_single(&keylist, k);\n\n\t\t\tret = bch_btree_insert(s, &keylist, i->pin, NULL);\n\t\t\tif (ret)\n\t\t\t\tgoto err;\n\n\t\t\tBUG_ON(!bch_keylist_empty(&keylist));\n\t\t\tkeys++;\n\n\t\t\tcond_resched();\n\t\t}\n\n\t\tif (i->pin)\n\t\t\tatomic_dec(i->pin);\n\t\tn = i->j.seq + 1;\n\t\tentries++;\n\t}\n\n\tpr_info(\"journal replay done, %i keys in %i entries, seq %llu\\n\",\n\t\tkeys, entries, end);\nerr:\n\twhile (!list_empty(list)) {\n\t\ti = list_first_entry(list, struct journal_replay, list);\n\t\tlist_del(&i->list);\n\t\tkfree(i);\n\t}\n\n\treturn ret;\n}\n\nvoid bch_journal_space_reserve(struct journal *j)\n{\n\tj->do_reserve = true;\n}\n\n \n\nstatic void btree_flush_write(struct cache_set *c)\n{\n\tstruct btree *b, *t, *btree_nodes[BTREE_FLUSH_NR];\n\tunsigned int i, nr;\n\tint ref_nr;\n\tatomic_t *fifo_front_p, *now_fifo_front_p;\n\tsize_t mask;\n\n\tif (c->journal.btree_flushing)\n\t\treturn;\n\n\tspin_lock(&c->journal.flush_write_lock);\n\tif (c->journal.btree_flushing) {\n\t\tspin_unlock(&c->journal.flush_write_lock);\n\t\treturn;\n\t}\n\tc->journal.btree_flushing = true;\n\tspin_unlock(&c->journal.flush_write_lock);\n\n\t \n\tspin_lock(&c->journal.lock);\n\tfifo_front_p = &fifo_front(&c->journal.pin);\n\tref_nr = atomic_read(fifo_front_p);\n\tif (ref_nr <= 0) {\n\t\t \n\t\tspin_unlock(&c->journal.lock);\n\t\tgoto out;\n\t}\n\tspin_unlock(&c->journal.lock);\n\n\tmask = c->journal.pin.mask;\n\tnr = 0;\n\tatomic_long_inc(&c->flush_write);\n\tmemset(btree_nodes, 0, sizeof(btree_nodes));\n\n\tmutex_lock(&c->bucket_lock);\n\tlist_for_each_entry_safe_reverse(b, t, &c->btree_cache, list) {\n\t\t \n\t\tnow_fifo_front_p = &fifo_front(&c->journal.pin);\n\t\t \n\t\tif (now_fifo_front_p != fifo_front_p)\n\t\t\tbreak;\n\t\t \n\t\tref_nr = atomic_read(fifo_front_p);\n\t\tif (nr >= ref_nr)\n\t\t\tbreak;\n\n\t\tif (btree_node_journal_flush(b))\n\t\t\tpr_err(\"BUG: flush_write bit should not be set here!\\n\");\n\n\t\tmutex_lock(&b->write_lock);\n\n\t\tif (!btree_node_dirty(b)) {\n\t\t\tmutex_unlock(&b->write_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!btree_current_write(b)->journal) {\n\t\t\tmutex_unlock(&b->write_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (((btree_current_write(b)->journal - fifo_front_p) &\n\t\t     mask) != 0) {\n\t\t\tmutex_unlock(&b->write_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tset_btree_node_journal_flush(b);\n\n\t\tmutex_unlock(&b->write_lock);\n\n\t\tbtree_nodes[nr++] = b;\n\t\t \n\t\tif (nr == BTREE_FLUSH_NR)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&c->bucket_lock);\n\n\tfor (i = 0; i < nr; i++) {\n\t\tb = btree_nodes[i];\n\t\tif (!b) {\n\t\t\tpr_err(\"BUG: btree_nodes[%d] is NULL\\n\", i);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!btree_node_journal_flush(b)) {\n\t\t\tpr_err(\"BUG: bnode %p: journal_flush bit cleaned\\n\", b);\n\t\t\tcontinue;\n\t\t}\n\n\t\tmutex_lock(&b->write_lock);\n\t\tif (!btree_current_write(b)->journal) {\n\t\t\tclear_bit(BTREE_NODE_journal_flush, &b->flags);\n\t\t\tmutex_unlock(&b->write_lock);\n\t\t\tpr_debug(\"bnode %p: written by others\\n\", b);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!btree_node_dirty(b)) {\n\t\t\tclear_bit(BTREE_NODE_journal_flush, &b->flags);\n\t\t\tmutex_unlock(&b->write_lock);\n\t\t\tpr_debug(\"bnode %p: dirty bit cleaned by others\\n\", b);\n\t\t\tcontinue;\n\t\t}\n\n\t\t__bch_btree_node_write(b, NULL);\n\t\tclear_bit(BTREE_NODE_journal_flush, &b->flags);\n\t\tmutex_unlock(&b->write_lock);\n\t}\n\nout:\n\tspin_lock(&c->journal.flush_write_lock);\n\tc->journal.btree_flushing = false;\n\tspin_unlock(&c->journal.flush_write_lock);\n}\n\n#define last_seq(j)\t((j)->seq - fifo_used(&(j)->pin) + 1)\n\nstatic void journal_discard_endio(struct bio *bio)\n{\n\tstruct journal_device *ja =\n\t\tcontainer_of(bio, struct journal_device, discard_bio);\n\tstruct cache *ca = container_of(ja, struct cache, journal);\n\n\tatomic_set(&ja->discard_in_flight, DISCARD_DONE);\n\n\tclosure_wake_up(&ca->set->journal.wait);\n\tclosure_put(&ca->set->cl);\n}\n\nstatic void journal_discard_work(struct work_struct *work)\n{\n\tstruct journal_device *ja =\n\t\tcontainer_of(work, struct journal_device, discard_work);\n\n\tsubmit_bio(&ja->discard_bio);\n}\n\nstatic void do_journal_discard(struct cache *ca)\n{\n\tstruct journal_device *ja = &ca->journal;\n\tstruct bio *bio = &ja->discard_bio;\n\n\tif (!ca->discard) {\n\t\tja->discard_idx = ja->last_idx;\n\t\treturn;\n\t}\n\n\tswitch (atomic_read(&ja->discard_in_flight)) {\n\tcase DISCARD_IN_FLIGHT:\n\t\treturn;\n\n\tcase DISCARD_DONE:\n\t\tja->discard_idx = (ja->discard_idx + 1) %\n\t\t\tca->sb.njournal_buckets;\n\n\t\tatomic_set(&ja->discard_in_flight, DISCARD_READY);\n\t\tfallthrough;\n\n\tcase DISCARD_READY:\n\t\tif (ja->discard_idx == ja->last_idx)\n\t\t\treturn;\n\n\t\tatomic_set(&ja->discard_in_flight, DISCARD_IN_FLIGHT);\n\n\t\tbio_init(bio, ca->bdev, bio->bi_inline_vecs, 1, REQ_OP_DISCARD);\n\t\tbio->bi_iter.bi_sector\t= bucket_to_sector(ca->set,\n\t\t\t\t\t\tca->sb.d[ja->discard_idx]);\n\t\tbio->bi_iter.bi_size\t= bucket_bytes(ca);\n\t\tbio->bi_end_io\t\t= journal_discard_endio;\n\n\t\tclosure_get(&ca->set->cl);\n\t\tINIT_WORK(&ja->discard_work, journal_discard_work);\n\t\tqueue_work(bch_journal_wq, &ja->discard_work);\n\t}\n}\n\nstatic unsigned int free_journal_buckets(struct cache_set *c)\n{\n\tstruct journal *j = &c->journal;\n\tstruct cache *ca = c->cache;\n\tstruct journal_device *ja = &c->cache->journal;\n\tunsigned int n;\n\n\t \n\tif (ja->cur_idx >= ja->discard_idx)\n\t\tn = ca->sb.njournal_buckets +  ja->discard_idx - ja->cur_idx;\n\telse\n\t\tn = ja->discard_idx - ja->cur_idx;\n\n\tif (n > (1 + j->do_reserve))\n\t\treturn n - (1 + j->do_reserve);\n\n\treturn 0;\n}\n\nstatic void journal_reclaim(struct cache_set *c)\n{\n\tstruct bkey *k = &c->journal.key;\n\tstruct cache *ca = c->cache;\n\tuint64_t last_seq;\n\tstruct journal_device *ja = &ca->journal;\n\tatomic_t p __maybe_unused;\n\n\tatomic_long_inc(&c->reclaim);\n\n\twhile (!atomic_read(&fifo_front(&c->journal.pin)))\n\t\tfifo_pop(&c->journal.pin, p);\n\n\tlast_seq = last_seq(&c->journal);\n\n\t \n\n\twhile (ja->last_idx != ja->cur_idx &&\n\t       ja->seq[ja->last_idx] < last_seq)\n\t\tja->last_idx = (ja->last_idx + 1) %\n\t\t\tca->sb.njournal_buckets;\n\n\tdo_journal_discard(ca);\n\n\tif (c->journal.blocks_free)\n\t\tgoto out;\n\n\tif (!free_journal_buckets(c))\n\t\tgoto out;\n\n\tja->cur_idx = (ja->cur_idx + 1) % ca->sb.njournal_buckets;\n\tk->ptr[0] = MAKE_PTR(0,\n\t\t\t     bucket_to_sector(c, ca->sb.d[ja->cur_idx]),\n\t\t\t     ca->sb.nr_this_dev);\n\tatomic_long_inc(&c->reclaimed_journal_buckets);\n\n\tbkey_init(k);\n\tSET_KEY_PTRS(k, 1);\n\tc->journal.blocks_free = ca->sb.bucket_size >> c->block_bits;\n\nout:\n\tif (!journal_full(&c->journal))\n\t\t__closure_wake_up(&c->journal.wait);\n}\n\nvoid bch_journal_next(struct journal *j)\n{\n\tatomic_t p = { 1 };\n\n\tj->cur = (j->cur == j->w)\n\t\t? &j->w[1]\n\t\t: &j->w[0];\n\n\t \n\tBUG_ON(!fifo_push(&j->pin, p));\n\tatomic_set(&fifo_back(&j->pin), 1);\n\n\tj->cur->data->seq\t= ++j->seq;\n\tj->cur->dirty\t\t= false;\n\tj->cur->need_write\t= false;\n\tj->cur->data->keys\t= 0;\n\n\tif (fifo_full(&j->pin))\n\t\tpr_debug(\"journal_pin full (%zu)\\n\", fifo_used(&j->pin));\n}\n\nstatic void journal_write_endio(struct bio *bio)\n{\n\tstruct journal_write *w = bio->bi_private;\n\n\tcache_set_err_on(bio->bi_status, w->c, \"journal io error\");\n\tclosure_put(&w->c->journal.io);\n}\n\nstatic void journal_write(struct closure *cl);\n\nstatic void journal_write_done(struct closure *cl)\n{\n\tstruct journal *j = container_of(cl, struct journal, io);\n\tstruct journal_write *w = (j->cur == j->w)\n\t\t? &j->w[1]\n\t\t: &j->w[0];\n\n\t__closure_wake_up(&w->wait);\n\tcontinue_at_nobarrier(cl, journal_write, bch_journal_wq);\n}\n\nstatic void journal_write_unlock(struct closure *cl)\n\t__releases(&c->journal.lock)\n{\n\tstruct cache_set *c = container_of(cl, struct cache_set, journal.io);\n\n\tc->journal.io_in_flight = 0;\n\tspin_unlock(&c->journal.lock);\n}\n\nstatic void journal_write_unlocked(struct closure *cl)\n\t__releases(c->journal.lock)\n{\n\tstruct cache_set *c = container_of(cl, struct cache_set, journal.io);\n\tstruct cache *ca = c->cache;\n\tstruct journal_write *w = c->journal.cur;\n\tstruct bkey *k = &c->journal.key;\n\tunsigned int i, sectors = set_blocks(w->data, block_bytes(ca)) *\n\t\tca->sb.block_size;\n\n\tstruct bio *bio;\n\tstruct bio_list list;\n\n\tbio_list_init(&list);\n\n\tif (!w->need_write) {\n\t\tclosure_return_with_destructor(cl, journal_write_unlock);\n\t\treturn;\n\t} else if (journal_full(&c->journal)) {\n\t\tjournal_reclaim(c);\n\t\tspin_unlock(&c->journal.lock);\n\n\t\tbtree_flush_write(c);\n\t\tcontinue_at(cl, journal_write, bch_journal_wq);\n\t\treturn;\n\t}\n\n\tc->journal.blocks_free -= set_blocks(w->data, block_bytes(ca));\n\n\tw->data->btree_level = c->root->level;\n\n\tbkey_copy(&w->data->btree_root, &c->root->key);\n\tbkey_copy(&w->data->uuid_bucket, &c->uuid_bucket);\n\n\tw->data->prio_bucket[ca->sb.nr_this_dev] = ca->prio_buckets[0];\n\tw->data->magic\t\t= jset_magic(&ca->sb);\n\tw->data->version\t= BCACHE_JSET_VERSION;\n\tw->data->last_seq\t= last_seq(&c->journal);\n\tw->data->csum\t\t= csum_set(w->data);\n\n\tfor (i = 0; i < KEY_PTRS(k); i++) {\n\t\tca = c->cache;\n\t\tbio = &ca->journal.bio;\n\n\t\tatomic_long_add(sectors, &ca->meta_sectors_written);\n\n\t\tbio_reset(bio, ca->bdev, REQ_OP_WRITE | \n\t\t\t  REQ_SYNC | REQ_META | REQ_PREFLUSH | REQ_FUA);\n\t\tbio->bi_iter.bi_sector\t= PTR_OFFSET(k, i);\n\t\tbio->bi_iter.bi_size = sectors << 9;\n\n\t\tbio->bi_end_io\t= journal_write_endio;\n\t\tbio->bi_private = w;\n\t\tbch_bio_map(bio, w->data);\n\n\t\ttrace_bcache_journal_write(bio, w->data->keys);\n\t\tbio_list_add(&list, bio);\n\n\t\tSET_PTR_OFFSET(k, i, PTR_OFFSET(k, i) + sectors);\n\n\t\tca->journal.seq[ca->journal.cur_idx] = w->data->seq;\n\t}\n\n\t \n\tBUG_ON(i == 0);\n\n\tatomic_dec_bug(&fifo_back(&c->journal.pin));\n\tbch_journal_next(&c->journal);\n\tjournal_reclaim(c);\n\n\tspin_unlock(&c->journal.lock);\n\n\twhile ((bio = bio_list_pop(&list)))\n\t\tclosure_bio_submit(c, bio, cl);\n\n\tcontinue_at(cl, journal_write_done, NULL);\n}\n\nstatic void journal_write(struct closure *cl)\n{\n\tstruct cache_set *c = container_of(cl, struct cache_set, journal.io);\n\n\tspin_lock(&c->journal.lock);\n\tjournal_write_unlocked(cl);\n}\n\nstatic void journal_try_write(struct cache_set *c)\n\t__releases(c->journal.lock)\n{\n\tstruct closure *cl = &c->journal.io;\n\tstruct journal_write *w = c->journal.cur;\n\n\tw->need_write = true;\n\n\tif (!c->journal.io_in_flight) {\n\t\tc->journal.io_in_flight = 1;\n\t\tclosure_call(cl, journal_write_unlocked, NULL, &c->cl);\n\t} else {\n\t\tspin_unlock(&c->journal.lock);\n\t}\n}\n\nstatic struct journal_write *journal_wait_for_write(struct cache_set *c,\n\t\t\t\t\t\t    unsigned int nkeys)\n\t__acquires(&c->journal.lock)\n{\n\tsize_t sectors;\n\tstruct closure cl;\n\tbool wait = false;\n\tstruct cache *ca = c->cache;\n\n\tclosure_init_stack(&cl);\n\n\tspin_lock(&c->journal.lock);\n\n\twhile (1) {\n\t\tstruct journal_write *w = c->journal.cur;\n\n\t\tsectors = __set_blocks(w->data, w->data->keys + nkeys,\n\t\t\t\t       block_bytes(ca)) * ca->sb.block_size;\n\n\t\tif (sectors <= min_t(size_t,\n\t\t\t\t     c->journal.blocks_free * ca->sb.block_size,\n\t\t\t\t     PAGE_SECTORS << JSET_BITS))\n\t\t\treturn w;\n\n\t\tif (wait)\n\t\t\tclosure_wait(&c->journal.wait, &cl);\n\n\t\tif (!journal_full(&c->journal)) {\n\t\t\tif (wait)\n\t\t\t\ttrace_bcache_journal_entry_full(c);\n\n\t\t\t \n\t\t\tBUG_ON(!w->data->keys);\n\n\t\t\tjournal_try_write(c);  \n\t\t} else {\n\t\t\tif (wait)\n\t\t\t\ttrace_bcache_journal_full(c);\n\n\t\t\tjournal_reclaim(c);\n\t\t\tspin_unlock(&c->journal.lock);\n\n\t\t\tbtree_flush_write(c);\n\t\t}\n\n\t\tclosure_sync(&cl);\n\t\tspin_lock(&c->journal.lock);\n\t\twait = true;\n\t}\n}\n\nstatic void journal_write_work(struct work_struct *work)\n{\n\tstruct cache_set *c = container_of(to_delayed_work(work),\n\t\t\t\t\t   struct cache_set,\n\t\t\t\t\t   journal.work);\n\tspin_lock(&c->journal.lock);\n\tif (c->journal.cur->dirty)\n\t\tjournal_try_write(c);\n\telse\n\t\tspin_unlock(&c->journal.lock);\n}\n\n \n\natomic_t *bch_journal(struct cache_set *c,\n\t\t      struct keylist *keys,\n\t\t      struct closure *parent)\n{\n\tstruct journal_write *w;\n\tatomic_t *ret;\n\n\t \n\tif (unlikely(test_bit(CACHE_SET_IO_DISABLE, &c->flags)))\n\t\treturn NULL;\n\n\tif (!CACHE_SYNC(&c->cache->sb))\n\t\treturn NULL;\n\n\tw = journal_wait_for_write(c, bch_keylist_nkeys(keys));\n\n\tmemcpy(bset_bkey_last(w->data), keys->keys, bch_keylist_bytes(keys));\n\tw->data->keys += bch_keylist_nkeys(keys);\n\n\tret = &fifo_back(&c->journal.pin);\n\tatomic_inc(ret);\n\n\tif (parent) {\n\t\tclosure_wait(&w->wait, parent);\n\t\tjournal_try_write(c);\n\t} else if (!w->dirty) {\n\t\tw->dirty = true;\n\t\tqueue_delayed_work(bch_flush_wq, &c->journal.work,\n\t\t\t\t   msecs_to_jiffies(c->journal_delay_ms));\n\t\tspin_unlock(&c->journal.lock);\n\t} else {\n\t\tspin_unlock(&c->journal.lock);\n\t}\n\n\n\treturn ret;\n}\n\nvoid bch_journal_meta(struct cache_set *c, struct closure *cl)\n{\n\tstruct keylist keys;\n\tatomic_t *ref;\n\n\tbch_keylist_init(&keys);\n\n\tref = bch_journal(c, &keys, cl);\n\tif (ref)\n\t\tatomic_dec_bug(ref);\n}\n\nvoid bch_journal_free(struct cache_set *c)\n{\n\tfree_pages((unsigned long) c->journal.w[1].data, JSET_BITS);\n\tfree_pages((unsigned long) c->journal.w[0].data, JSET_BITS);\n\tfree_fifo(&c->journal.pin);\n}\n\nint bch_journal_alloc(struct cache_set *c)\n{\n\tstruct journal *j = &c->journal;\n\n\tspin_lock_init(&j->lock);\n\tspin_lock_init(&j->flush_write_lock);\n\tINIT_DELAYED_WORK(&j->work, journal_write_work);\n\n\tc->journal_delay_ms = 100;\n\n\tj->w[0].c = c;\n\tj->w[1].c = c;\n\n\tif (!(init_fifo(&j->pin, JOURNAL_PIN, GFP_KERNEL)) ||\n\t    !(j->w[0].data = (void *) __get_free_pages(GFP_KERNEL|__GFP_COMP, JSET_BITS)) ||\n\t    !(j->w[1].data = (void *) __get_free_pages(GFP_KERNEL|__GFP_COMP, JSET_BITS)))\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}