{
  "module_name": "request.c",
  "hash_id": "4282abf66482ac2f64a2c4c469098f60b8af398add93e86d0fc3157d9ebe73df",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/bcache/request.c",
  "human_readable_source": "\n \n\n#include \"bcache.h\"\n#include \"btree.h\"\n#include \"debug.h\"\n#include \"request.h\"\n#include \"writeback.h\"\n\n#include <linux/module.h>\n#include <linux/hash.h>\n#include <linux/random.h>\n#include <linux/backing-dev.h>\n\n#include <trace/events/bcache.h>\n\n#define CUTOFF_CACHE_ADD\t95\n#define CUTOFF_CACHE_READA\t90\n\nstruct kmem_cache *bch_search_cache;\n\nstatic void bch_data_insert_start(struct closure *cl);\n\nstatic unsigned int cache_mode(struct cached_dev *dc)\n{\n\treturn BDEV_CACHE_MODE(&dc->sb);\n}\n\nstatic bool verify(struct cached_dev *dc)\n{\n\treturn dc->verify;\n}\n\nstatic void bio_csum(struct bio *bio, struct bkey *k)\n{\n\tstruct bio_vec bv;\n\tstruct bvec_iter iter;\n\tuint64_t csum = 0;\n\n\tbio_for_each_segment(bv, bio, iter) {\n\t\tvoid *d = bvec_kmap_local(&bv);\n\n\t\tcsum = crc64_be(csum, d, bv.bv_len);\n\t\tkunmap_local(d);\n\t}\n\n\tk->ptr[KEY_PTRS(k)] = csum & (~0ULL >> 1);\n}\n\n \n\nstatic void bch_data_insert_keys(struct closure *cl)\n{\n\tstruct data_insert_op *op = container_of(cl, struct data_insert_op, cl);\n\tatomic_t *journal_ref = NULL;\n\tstruct bkey *replace_key = op->replace ? &op->replace_key : NULL;\n\tint ret;\n\n\tif (!op->replace)\n\t\tjournal_ref = bch_journal(op->c, &op->insert_keys,\n\t\t\t\t\t  op->flush_journal ? cl : NULL);\n\n\tret = bch_btree_insert(op->c, &op->insert_keys,\n\t\t\t       journal_ref, replace_key);\n\tif (ret == -ESRCH) {\n\t\top->replace_collision = true;\n\t} else if (ret) {\n\t\top->status\t\t= BLK_STS_RESOURCE;\n\t\top->insert_data_done\t= true;\n\t}\n\n\tif (journal_ref)\n\t\tatomic_dec_bug(journal_ref);\n\n\tif (!op->insert_data_done) {\n\t\tcontinue_at(cl, bch_data_insert_start, op->wq);\n\t\treturn;\n\t}\n\n\tbch_keylist_free(&op->insert_keys);\n\tclosure_return(cl);\n}\n\nstatic int bch_keylist_realloc(struct keylist *l, unsigned int u64s,\n\t\t\t       struct cache_set *c)\n{\n\tsize_t oldsize = bch_keylist_nkeys(l);\n\tsize_t newsize = oldsize + u64s;\n\n\t \n\tif (newsize * sizeof(uint64_t) > block_bytes(c->cache) - sizeof(struct jset))\n\t\treturn -ENOMEM;\n\n\treturn __bch_keylist_realloc(l, u64s);\n}\n\nstatic void bch_data_invalidate(struct closure *cl)\n{\n\tstruct data_insert_op *op = container_of(cl, struct data_insert_op, cl);\n\tstruct bio *bio = op->bio;\n\n\tpr_debug(\"invalidating %i sectors from %llu\\n\",\n\t\t bio_sectors(bio), (uint64_t) bio->bi_iter.bi_sector);\n\n\twhile (bio_sectors(bio)) {\n\t\tunsigned int sectors = min(bio_sectors(bio),\n\t\t\t\t       1U << (KEY_SIZE_BITS - 1));\n\n\t\tif (bch_keylist_realloc(&op->insert_keys, 2, op->c))\n\t\t\tgoto out;\n\n\t\tbio->bi_iter.bi_sector\t+= sectors;\n\t\tbio->bi_iter.bi_size\t-= sectors << 9;\n\n\t\tbch_keylist_add(&op->insert_keys,\n\t\t\t\t&KEY(op->inode,\n\t\t\t\t     bio->bi_iter.bi_sector,\n\t\t\t\t     sectors));\n\t}\n\n\top->insert_data_done = true;\n\t \n\tbio_put(bio);\nout:\n\tcontinue_at(cl, bch_data_insert_keys, op->wq);\n}\n\nstatic void bch_data_insert_error(struct closure *cl)\n{\n\tstruct data_insert_op *op = container_of(cl, struct data_insert_op, cl);\n\n\t \n\n\tstruct bkey *src = op->insert_keys.keys, *dst = op->insert_keys.keys;\n\n\twhile (src != op->insert_keys.top) {\n\t\tstruct bkey *n = bkey_next(src);\n\n\t\tSET_KEY_PTRS(src, 0);\n\t\tmemmove(dst, src, bkey_bytes(src));\n\n\t\tdst = bkey_next(dst);\n\t\tsrc = n;\n\t}\n\n\top->insert_keys.top = dst;\n\n\tbch_data_insert_keys(cl);\n}\n\nstatic void bch_data_insert_endio(struct bio *bio)\n{\n\tstruct closure *cl = bio->bi_private;\n\tstruct data_insert_op *op = container_of(cl, struct data_insert_op, cl);\n\n\tif (bio->bi_status) {\n\t\t \n\t\tif (op->writeback)\n\t\t\top->status = bio->bi_status;\n\t\telse if (!op->replace)\n\t\t\tset_closure_fn(cl, bch_data_insert_error, op->wq);\n\t\telse\n\t\t\tset_closure_fn(cl, NULL, NULL);\n\t}\n\n\tbch_bbio_endio(op->c, bio, bio->bi_status, \"writing data to cache\");\n}\n\nstatic void bch_data_insert_start(struct closure *cl)\n{\n\tstruct data_insert_op *op = container_of(cl, struct data_insert_op, cl);\n\tstruct bio *bio = op->bio, *n;\n\n\tif (op->bypass)\n\t\treturn bch_data_invalidate(cl);\n\n\tif (atomic_sub_return(bio_sectors(bio), &op->c->sectors_to_gc) < 0)\n\t\twake_up_gc(op->c);\n\n\t \n\tbio->bi_opf &= ~(REQ_PREFLUSH|REQ_FUA);\n\n\tdo {\n\t\tunsigned int i;\n\t\tstruct bkey *k;\n\t\tstruct bio_set *split = &op->c->bio_split;\n\n\t\t \n\t\tif (bch_keylist_realloc(&op->insert_keys,\n\t\t\t\t\t3 + (op->csum ? 1 : 0),\n\t\t\t\t\top->c)) {\n\t\t\tcontinue_at(cl, bch_data_insert_keys, op->wq);\n\t\t\treturn;\n\t\t}\n\n\t\tk = op->insert_keys.top;\n\t\tbkey_init(k);\n\t\tSET_KEY_INODE(k, op->inode);\n\t\tSET_KEY_OFFSET(k, bio->bi_iter.bi_sector);\n\n\t\tif (!bch_alloc_sectors(op->c, k, bio_sectors(bio),\n\t\t\t\t       op->write_point, op->write_prio,\n\t\t\t\t       op->writeback))\n\t\t\tgoto err;\n\n\t\tn = bio_next_split(bio, KEY_SIZE(k), GFP_NOIO, split);\n\n\t\tn->bi_end_io\t= bch_data_insert_endio;\n\t\tn->bi_private\t= cl;\n\n\t\tif (op->writeback) {\n\t\t\tSET_KEY_DIRTY(k, true);\n\n\t\t\tfor (i = 0; i < KEY_PTRS(k); i++)\n\t\t\t\tSET_GC_MARK(PTR_BUCKET(op->c, k, i),\n\t\t\t\t\t    GC_MARK_DIRTY);\n\t\t}\n\n\t\tSET_KEY_CSUM(k, op->csum);\n\t\tif (KEY_CSUM(k))\n\t\t\tbio_csum(n, k);\n\n\t\ttrace_bcache_cache_insert(k);\n\t\tbch_keylist_push(&op->insert_keys);\n\n\t\tn->bi_opf = REQ_OP_WRITE;\n\t\tbch_submit_bbio(n, op->c, k, 0);\n\t} while (n != bio);\n\n\top->insert_data_done = true;\n\tcontinue_at(cl, bch_data_insert_keys, op->wq);\n\treturn;\nerr:\n\t \n\tBUG_ON(op->writeback);\n\n\t \n\n\tif (!op->replace) {\n\t\t \n\t\top->bypass = true;\n\t\treturn bch_data_invalidate(cl);\n\t} else {\n\t\t \n\t\top->insert_data_done = true;\n\t\tbio_put(bio);\n\n\t\tif (!bch_keylist_empty(&op->insert_keys))\n\t\t\tcontinue_at(cl, bch_data_insert_keys, op->wq);\n\t\telse\n\t\t\tclosure_return(cl);\n\t}\n}\n\n \nvoid bch_data_insert(struct closure *cl)\n{\n\tstruct data_insert_op *op = container_of(cl, struct data_insert_op, cl);\n\n\ttrace_bcache_write(op->c, op->inode, op->bio,\n\t\t\t   op->writeback, op->bypass);\n\n\tbch_keylist_init(&op->insert_keys);\n\tbio_get(op->bio);\n\tbch_data_insert_start(cl);\n}\n\n \nunsigned int bch_get_congested(const struct cache_set *c)\n{\n\tint i;\n\n\tif (!c->congested_read_threshold_us &&\n\t    !c->congested_write_threshold_us)\n\t\treturn 0;\n\n\ti = (local_clock_us() - c->congested_last_us) / 1024;\n\tif (i < 0)\n\t\treturn 0;\n\n\ti += atomic_read(&c->congested);\n\tif (i >= 0)\n\t\treturn 0;\n\n\ti += CONGESTED_MAX;\n\n\tif (i > 0)\n\t\ti = fract_exp_two(i, 6);\n\n\ti -= hweight32(get_random_u32());\n\n\treturn i > 0 ? i : 1;\n}\n\nstatic void add_sequential(struct task_struct *t)\n{\n\tewma_add(t->sequential_io_avg,\n\t\t t->sequential_io, 8, 0);\n\n\tt->sequential_io = 0;\n}\n\nstatic struct hlist_head *iohash(struct cached_dev *dc, uint64_t k)\n{\n\treturn &dc->io_hash[hash_64(k, RECENT_IO_BITS)];\n}\n\nstatic bool check_should_bypass(struct cached_dev *dc, struct bio *bio)\n{\n\tstruct cache_set *c = dc->disk.c;\n\tunsigned int mode = cache_mode(dc);\n\tunsigned int sectors, congested;\n\tstruct task_struct *task = current;\n\tstruct io *i;\n\n\tif (test_bit(BCACHE_DEV_DETACHING, &dc->disk.flags) ||\n\t    c->gc_stats.in_use > CUTOFF_CACHE_ADD ||\n\t    (bio_op(bio) == REQ_OP_DISCARD))\n\t\tgoto skip;\n\n\tif (mode == CACHE_MODE_NONE ||\n\t    (mode == CACHE_MODE_WRITEAROUND &&\n\t     op_is_write(bio_op(bio))))\n\t\tgoto skip;\n\n\t \n\tif ((bio->bi_opf & (REQ_RAHEAD|REQ_BACKGROUND))) {\n\t\tif (!(bio->bi_opf & (REQ_META|REQ_PRIO)) &&\n\t\t    (dc->cache_readahead_policy != BCH_CACHE_READA_ALL))\n\t\t\tgoto skip;\n\t}\n\n\tif (bio->bi_iter.bi_sector & (c->cache->sb.block_size - 1) ||\n\t    bio_sectors(bio) & (c->cache->sb.block_size - 1)) {\n\t\tpr_debug(\"skipping unaligned io\\n\");\n\t\tgoto skip;\n\t}\n\n\tif (bypass_torture_test(dc)) {\n\t\tif (get_random_u32_below(4) == 3)\n\t\t\tgoto skip;\n\t\telse\n\t\t\tgoto rescale;\n\t}\n\n\tcongested = bch_get_congested(c);\n\tif (!congested && !dc->sequential_cutoff)\n\t\tgoto rescale;\n\n\tspin_lock(&dc->io_lock);\n\n\thlist_for_each_entry(i, iohash(dc, bio->bi_iter.bi_sector), hash)\n\t\tif (i->last == bio->bi_iter.bi_sector &&\n\t\t    time_before(jiffies, i->jiffies))\n\t\t\tgoto found;\n\n\ti = list_first_entry(&dc->io_lru, struct io, lru);\n\n\tadd_sequential(task);\n\ti->sequential = 0;\nfound:\n\tif (i->sequential + bio->bi_iter.bi_size > i->sequential)\n\t\ti->sequential\t+= bio->bi_iter.bi_size;\n\n\ti->last\t\t\t = bio_end_sector(bio);\n\ti->jiffies\t\t = jiffies + msecs_to_jiffies(5000);\n\ttask->sequential_io\t = i->sequential;\n\n\thlist_del(&i->hash);\n\thlist_add_head(&i->hash, iohash(dc, i->last));\n\tlist_move_tail(&i->lru, &dc->io_lru);\n\n\tspin_unlock(&dc->io_lock);\n\n\tsectors = max(task->sequential_io,\n\t\t      task->sequential_io_avg) >> 9;\n\n\tif (dc->sequential_cutoff &&\n\t    sectors >= dc->sequential_cutoff >> 9) {\n\t\ttrace_bcache_bypass_sequential(bio);\n\t\tgoto skip;\n\t}\n\n\tif (congested && sectors >= congested) {\n\t\ttrace_bcache_bypass_congested(bio);\n\t\tgoto skip;\n\t}\n\nrescale:\n\tbch_rescale_priorities(c, bio_sectors(bio));\n\treturn false;\nskip:\n\tbch_mark_sectors_bypassed(c, dc, bio_sectors(bio));\n\treturn true;\n}\n\n \n\nstruct search {\n\t \n\tstruct closure\t\tcl;\n\n\tstruct bbio\t\tbio;\n\tstruct bio\t\t*orig_bio;\n\tstruct bio\t\t*cache_miss;\n\tstruct bcache_device\t*d;\n\n\tunsigned int\t\tinsert_bio_sectors;\n\tunsigned int\t\trecoverable:1;\n\tunsigned int\t\twrite:1;\n\tunsigned int\t\tread_dirty_data:1;\n\tunsigned int\t\tcache_missed:1;\n\n\tstruct block_device\t*orig_bdev;\n\tunsigned long\t\tstart_time;\n\n\tstruct btree_op\t\top;\n\tstruct data_insert_op\tiop;\n};\n\nstatic void bch_cache_read_endio(struct bio *bio)\n{\n\tstruct bbio *b = container_of(bio, struct bbio, bio);\n\tstruct closure *cl = bio->bi_private;\n\tstruct search *s = container_of(cl, struct search, cl);\n\n\t \n\n\tif (bio->bi_status)\n\t\ts->iop.status = bio->bi_status;\n\telse if (!KEY_DIRTY(&b->key) &&\n\t\t ptr_stale(s->iop.c, &b->key, 0)) {\n\t\tatomic_long_inc(&s->iop.c->cache_read_races);\n\t\ts->iop.status = BLK_STS_IOERR;\n\t}\n\n\tbch_bbio_endio(s->iop.c, bio, bio->bi_status, \"reading from cache\");\n}\n\n \nstatic int cache_lookup_fn(struct btree_op *op, struct btree *b, struct bkey *k)\n{\n\tstruct search *s = container_of(op, struct search, op);\n\tstruct bio *n, *bio = &s->bio.bio;\n\tstruct bkey *bio_key;\n\tunsigned int ptr;\n\n\tif (bkey_cmp(k, &KEY(s->iop.inode, bio->bi_iter.bi_sector, 0)) <= 0)\n\t\treturn MAP_CONTINUE;\n\n\tif (KEY_INODE(k) != s->iop.inode ||\n\t    KEY_START(k) > bio->bi_iter.bi_sector) {\n\t\tunsigned int bio_sectors = bio_sectors(bio);\n\t\tunsigned int sectors = KEY_INODE(k) == s->iop.inode\n\t\t\t? min_t(uint64_t, INT_MAX,\n\t\t\t\tKEY_START(k) - bio->bi_iter.bi_sector)\n\t\t\t: INT_MAX;\n\t\tint ret = s->d->cache_miss(b, s, bio, sectors);\n\n\t\tif (ret != MAP_CONTINUE)\n\t\t\treturn ret;\n\n\t\t \n\t\tBUG_ON(bio_sectors <= sectors);\n\t}\n\n\tif (!KEY_SIZE(k))\n\t\treturn MAP_CONTINUE;\n\n\t \n\tptr = 0;\n\n\tPTR_BUCKET(b->c, k, ptr)->prio = INITIAL_PRIO;\n\n\tif (KEY_DIRTY(k))\n\t\ts->read_dirty_data = true;\n\n\tn = bio_next_split(bio, min_t(uint64_t, INT_MAX,\n\t\t\t\t      KEY_OFFSET(k) - bio->bi_iter.bi_sector),\n\t\t\t   GFP_NOIO, &s->d->bio_split);\n\n\tbio_key = &container_of(n, struct bbio, bio)->key;\n\tbch_bkey_copy_single_ptr(bio_key, k, ptr);\n\n\tbch_cut_front(&KEY(s->iop.inode, n->bi_iter.bi_sector, 0), bio_key);\n\tbch_cut_back(&KEY(s->iop.inode, bio_end_sector(n), 0), bio_key);\n\n\tn->bi_end_io\t= bch_cache_read_endio;\n\tn->bi_private\t= &s->cl;\n\n\t \n\n\t__bch_submit_bbio(n, b->c);\n\treturn n == bio ? MAP_DONE : MAP_CONTINUE;\n}\n\nstatic void cache_lookup(struct closure *cl)\n{\n\tstruct search *s = container_of(cl, struct search, iop.cl);\n\tstruct bio *bio = &s->bio.bio;\n\tstruct cached_dev *dc;\n\tint ret;\n\n\tbch_btree_op_init(&s->op, -1);\n\n\tret = bch_btree_map_keys(&s->op, s->iop.c,\n\t\t\t\t &KEY(s->iop.inode, bio->bi_iter.bi_sector, 0),\n\t\t\t\t cache_lookup_fn, MAP_END_KEY);\n\tif (ret == -EAGAIN) {\n\t\tcontinue_at(cl, cache_lookup, bcache_wq);\n\t\treturn;\n\t}\n\n\t \n\tif (ret < 0) {\n\t\tBUG_ON(ret == -EINTR);\n\t\tif (s->d && s->d->c &&\n\t\t\t\t!UUID_FLASH_ONLY(&s->d->c->uuids[s->d->id])) {\n\t\t\tdc = container_of(s->d, struct cached_dev, disk);\n\t\t\tif (dc && atomic_read(&dc->has_dirty))\n\t\t\t\ts->recoverable = false;\n\t\t}\n\t\tif (!s->iop.status)\n\t\t\ts->iop.status = BLK_STS_IOERR;\n\t}\n\n\tclosure_return(cl);\n}\n\n \n\nstatic void request_endio(struct bio *bio)\n{\n\tstruct closure *cl = bio->bi_private;\n\n\tif (bio->bi_status) {\n\t\tstruct search *s = container_of(cl, struct search, cl);\n\n\t\ts->iop.status = bio->bi_status;\n\t\t \n\t\ts->recoverable = false;\n\t}\n\n\tbio_put(bio);\n\tclosure_put(cl);\n}\n\nstatic void backing_request_endio(struct bio *bio)\n{\n\tstruct closure *cl = bio->bi_private;\n\n\tif (bio->bi_status) {\n\t\tstruct search *s = container_of(cl, struct search, cl);\n\t\tstruct cached_dev *dc = container_of(s->d,\n\t\t\t\t\t\t     struct cached_dev, disk);\n\t\t \n\t\tif (unlikely(s->iop.writeback &&\n\t\t\t     bio->bi_opf & REQ_PREFLUSH)) {\n\t\t\tpr_err(\"Can't flush %pg: returned bi_status %i\\n\",\n\t\t\t\tdc->bdev, bio->bi_status);\n\t\t} else {\n\t\t\t \n\t\t\ts->iop.status = bio->bi_status;\n\t\t}\n\t\ts->recoverable = false;\n\t\t \n\t\tbch_count_backing_io_errors(dc, bio);\n\t}\n\n\tbio_put(bio);\n\tclosure_put(cl);\n}\n\nstatic void bio_complete(struct search *s)\n{\n\tif (s->orig_bio) {\n\t\t \n\t\tbio_end_io_acct_remapped(s->orig_bio, s->start_time,\n\t\t\t\t\t s->orig_bdev);\n\t\ttrace_bcache_request_end(s->d, s->orig_bio);\n\t\ts->orig_bio->bi_status = s->iop.status;\n\t\tbio_endio(s->orig_bio);\n\t\ts->orig_bio = NULL;\n\t}\n}\n\nstatic void do_bio_hook(struct search *s,\n\t\t\tstruct bio *orig_bio,\n\t\t\tbio_end_io_t *end_io_fn)\n{\n\tstruct bio *bio = &s->bio.bio;\n\n\tbio_init_clone(orig_bio->bi_bdev, bio, orig_bio, GFP_NOIO);\n\t \n\tbio->bi_end_io\t\t= end_io_fn;\n\tbio->bi_private\t\t= &s->cl;\n\n\tbio_cnt_set(bio, 3);\n}\n\nstatic void search_free(struct closure *cl)\n{\n\tstruct search *s = container_of(cl, struct search, cl);\n\n\tatomic_dec(&s->iop.c->search_inflight);\n\n\tif (s->iop.bio)\n\t\tbio_put(s->iop.bio);\n\n\tbio_complete(s);\n\tclosure_debug_destroy(cl);\n\tmempool_free(s, &s->iop.c->search);\n}\n\nstatic inline struct search *search_alloc(struct bio *bio,\n\t\tstruct bcache_device *d, struct block_device *orig_bdev,\n\t\tunsigned long start_time)\n{\n\tstruct search *s;\n\n\ts = mempool_alloc(&d->c->search, GFP_NOIO);\n\n\tclosure_init(&s->cl, NULL);\n\tdo_bio_hook(s, bio, request_endio);\n\tatomic_inc(&d->c->search_inflight);\n\n\ts->orig_bio\t\t= bio;\n\ts->cache_miss\t\t= NULL;\n\ts->cache_missed\t\t= 0;\n\ts->d\t\t\t= d;\n\ts->recoverable\t\t= 1;\n\ts->write\t\t= op_is_write(bio_op(bio));\n\ts->read_dirty_data\t= 0;\n\t \n\ts->orig_bdev\t\t= orig_bdev;\n\ts->start_time\t\t= start_time;\n\ts->iop.c\t\t= d->c;\n\ts->iop.bio\t\t= NULL;\n\ts->iop.inode\t\t= d->id;\n\ts->iop.write_point\t= hash_long((unsigned long) current, 16);\n\ts->iop.write_prio\t= 0;\n\ts->iop.status\t\t= 0;\n\ts->iop.flags\t\t= 0;\n\ts->iop.flush_journal\t= op_is_flush(bio->bi_opf);\n\ts->iop.wq\t\t= bcache_wq;\n\n\treturn s;\n}\n\n \n\nstatic void cached_dev_bio_complete(struct closure *cl)\n{\n\tstruct search *s = container_of(cl, struct search, cl);\n\tstruct cached_dev *dc = container_of(s->d, struct cached_dev, disk);\n\n\tcached_dev_put(dc);\n\tsearch_free(cl);\n}\n\n \n\nstatic void cached_dev_read_error_done(struct closure *cl)\n{\n\tstruct search *s = container_of(cl, struct search, cl);\n\n\tif (s->iop.replace_collision)\n\t\tbch_mark_cache_miss_collision(s->iop.c, s->d);\n\n\tif (s->iop.bio)\n\t\tbio_free_pages(s->iop.bio);\n\n\tcached_dev_bio_complete(cl);\n}\n\nstatic void cached_dev_read_error(struct closure *cl)\n{\n\tstruct search *s = container_of(cl, struct search, cl);\n\tstruct bio *bio = &s->bio.bio;\n\n\t \n\tif (s->recoverable && !s->read_dirty_data) {\n\t\t \n\t\ttrace_bcache_read_retry(s->orig_bio);\n\n\t\ts->iop.status = 0;\n\t\tdo_bio_hook(s, s->orig_bio, backing_request_endio);\n\n\t\t \n\n\t\t \n\t\tclosure_bio_submit(s->iop.c, bio, cl);\n\t}\n\n\tcontinue_at(cl, cached_dev_read_error_done, NULL);\n}\n\nstatic void cached_dev_cache_miss_done(struct closure *cl)\n{\n\tstruct search *s = container_of(cl, struct search, cl);\n\tstruct bcache_device *d = s->d;\n\n\tif (s->iop.replace_collision)\n\t\tbch_mark_cache_miss_collision(s->iop.c, s->d);\n\n\tif (s->iop.bio)\n\t\tbio_free_pages(s->iop.bio);\n\n\tcached_dev_bio_complete(cl);\n\tclosure_put(&d->cl);\n}\n\nstatic void cached_dev_read_done(struct closure *cl)\n{\n\tstruct search *s = container_of(cl, struct search, cl);\n\tstruct cached_dev *dc = container_of(s->d, struct cached_dev, disk);\n\n\t \n\n\tif (s->iop.bio) {\n\t\tbio_reset(s->iop.bio, s->cache_miss->bi_bdev, REQ_OP_READ);\n\t\ts->iop.bio->bi_iter.bi_sector =\n\t\t\ts->cache_miss->bi_iter.bi_sector;\n\t\ts->iop.bio->bi_iter.bi_size = s->insert_bio_sectors << 9;\n\t\tbio_clone_blkg_association(s->iop.bio, s->cache_miss);\n\t\tbch_bio_map(s->iop.bio, NULL);\n\n\t\tbio_copy_data(s->cache_miss, s->iop.bio);\n\n\t\tbio_put(s->cache_miss);\n\t\ts->cache_miss = NULL;\n\t}\n\n\tif (verify(dc) && s->recoverable && !s->read_dirty_data)\n\t\tbch_data_verify(dc, s->orig_bio);\n\n\tclosure_get(&dc->disk.cl);\n\tbio_complete(s);\n\n\tif (s->iop.bio &&\n\t    !test_bit(CACHE_SET_STOPPING, &s->iop.c->flags)) {\n\t\tBUG_ON(!s->iop.replace);\n\t\tclosure_call(&s->iop.cl, bch_data_insert, NULL, cl);\n\t}\n\n\tcontinue_at(cl, cached_dev_cache_miss_done, NULL);\n}\n\nstatic void cached_dev_read_done_bh(struct closure *cl)\n{\n\tstruct search *s = container_of(cl, struct search, cl);\n\tstruct cached_dev *dc = container_of(s->d, struct cached_dev, disk);\n\n\tbch_mark_cache_accounting(s->iop.c, s->d,\n\t\t\t\t  !s->cache_missed, s->iop.bypass);\n\ttrace_bcache_read(s->orig_bio, !s->cache_missed, s->iop.bypass);\n\n\tif (s->iop.status)\n\t\tcontinue_at_nobarrier(cl, cached_dev_read_error, bcache_wq);\n\telse if (s->iop.bio || verify(dc))\n\t\tcontinue_at_nobarrier(cl, cached_dev_read_done, bcache_wq);\n\telse\n\t\tcontinue_at_nobarrier(cl, cached_dev_bio_complete, NULL);\n}\n\nstatic int cached_dev_cache_miss(struct btree *b, struct search *s,\n\t\t\t\t struct bio *bio, unsigned int sectors)\n{\n\tint ret = MAP_CONTINUE;\n\tstruct cached_dev *dc = container_of(s->d, struct cached_dev, disk);\n\tstruct bio *miss, *cache_bio;\n\tunsigned int size_limit;\n\n\ts->cache_missed = 1;\n\n\tif (s->cache_miss || s->iop.bypass) {\n\t\tmiss = bio_next_split(bio, sectors, GFP_NOIO, &s->d->bio_split);\n\t\tret = miss == bio ? MAP_DONE : MAP_CONTINUE;\n\t\tgoto out_submit;\n\t}\n\n\t \n\tsize_limit = min_t(unsigned int, BIO_MAX_VECS * PAGE_SECTORS,\n\t\t\t   (1 << KEY_SIZE_BITS) - 1);\n\ts->insert_bio_sectors = min3(size_limit, sectors, bio_sectors(bio));\n\n\ts->iop.replace_key = KEY(s->iop.inode,\n\t\t\t\t bio->bi_iter.bi_sector + s->insert_bio_sectors,\n\t\t\t\t s->insert_bio_sectors);\n\n\tret = bch_btree_insert_check_key(b, &s->op, &s->iop.replace_key);\n\tif (ret)\n\t\treturn ret;\n\n\ts->iop.replace = true;\n\n\tmiss = bio_next_split(bio, s->insert_bio_sectors, GFP_NOIO,\n\t\t\t      &s->d->bio_split);\n\n\t \n\tret = miss == bio ? MAP_DONE : -EINTR;\n\n\tcache_bio = bio_alloc_bioset(miss->bi_bdev,\n\t\t\tDIV_ROUND_UP(s->insert_bio_sectors, PAGE_SECTORS),\n\t\t\t0, GFP_NOWAIT, &dc->disk.bio_split);\n\tif (!cache_bio)\n\t\tgoto out_submit;\n\n\tcache_bio->bi_iter.bi_sector\t= miss->bi_iter.bi_sector;\n\tcache_bio->bi_iter.bi_size\t= s->insert_bio_sectors << 9;\n\n\tcache_bio->bi_end_io\t= backing_request_endio;\n\tcache_bio->bi_private\t= &s->cl;\n\n\tbch_bio_map(cache_bio, NULL);\n\tif (bch_bio_alloc_pages(cache_bio, __GFP_NOWARN|GFP_NOIO))\n\t\tgoto out_put;\n\n\ts->cache_miss\t= miss;\n\ts->iop.bio\t= cache_bio;\n\tbio_get(cache_bio);\n\t \n\tclosure_bio_submit(s->iop.c, cache_bio, &s->cl);\n\n\treturn ret;\nout_put:\n\tbio_put(cache_bio);\nout_submit:\n\tmiss->bi_end_io\t\t= backing_request_endio;\n\tmiss->bi_private\t= &s->cl;\n\t \n\tclosure_bio_submit(s->iop.c, miss, &s->cl);\n\treturn ret;\n}\n\nstatic void cached_dev_read(struct cached_dev *dc, struct search *s)\n{\n\tstruct closure *cl = &s->cl;\n\n\tclosure_call(&s->iop.cl, cache_lookup, NULL, cl);\n\tcontinue_at(cl, cached_dev_read_done_bh, NULL);\n}\n\n \n\nstatic void cached_dev_write_complete(struct closure *cl)\n{\n\tstruct search *s = container_of(cl, struct search, cl);\n\tstruct cached_dev *dc = container_of(s->d, struct cached_dev, disk);\n\n\tup_read_non_owner(&dc->writeback_lock);\n\tcached_dev_bio_complete(cl);\n}\n\nstatic void cached_dev_write(struct cached_dev *dc, struct search *s)\n{\n\tstruct closure *cl = &s->cl;\n\tstruct bio *bio = &s->bio.bio;\n\tstruct bkey start = KEY(dc->disk.id, bio->bi_iter.bi_sector, 0);\n\tstruct bkey end = KEY(dc->disk.id, bio_end_sector(bio), 0);\n\n\tbch_keybuf_check_overlapping(&s->iop.c->moving_gc_keys, &start, &end);\n\n\tdown_read_non_owner(&dc->writeback_lock);\n\tif (bch_keybuf_check_overlapping(&dc->writeback_keys, &start, &end)) {\n\t\t \n\t\ts->iop.bypass = false;\n\t\ts->iop.writeback = true;\n\t}\n\n\t \n\tif (bio_op(bio) == REQ_OP_DISCARD)\n\t\ts->iop.bypass = true;\n\n\tif (should_writeback(dc, s->orig_bio,\n\t\t\t     cache_mode(dc),\n\t\t\t     s->iop.bypass)) {\n\t\ts->iop.bypass = false;\n\t\ts->iop.writeback = true;\n\t}\n\n\tif (s->iop.bypass) {\n\t\ts->iop.bio = s->orig_bio;\n\t\tbio_get(s->iop.bio);\n\n\t\tif (bio_op(bio) == REQ_OP_DISCARD &&\n\t\t    !bdev_max_discard_sectors(dc->bdev))\n\t\t\tgoto insert_data;\n\n\t\t \n\t\tbio->bi_end_io = backing_request_endio;\n\t\tclosure_bio_submit(s->iop.c, bio, cl);\n\n\t} else if (s->iop.writeback) {\n\t\tbch_writeback_add(dc);\n\t\ts->iop.bio = bio;\n\n\t\tif (bio->bi_opf & REQ_PREFLUSH) {\n\t\t\t \n\t\t\tstruct bio *flush;\n\n\t\t\tflush = bio_alloc_bioset(bio->bi_bdev, 0,\n\t\t\t\t\t\t REQ_OP_WRITE | REQ_PREFLUSH,\n\t\t\t\t\t\t GFP_NOIO, &dc->disk.bio_split);\n\t\t\tif (!flush) {\n\t\t\t\ts->iop.status = BLK_STS_RESOURCE;\n\t\t\t\tgoto insert_data;\n\t\t\t}\n\t\t\tflush->bi_end_io = backing_request_endio;\n\t\t\tflush->bi_private = cl;\n\t\t\t \n\t\t\tclosure_bio_submit(s->iop.c, flush, cl);\n\t\t}\n\t} else {\n\t\ts->iop.bio = bio_alloc_clone(bio->bi_bdev, bio, GFP_NOIO,\n\t\t\t\t\t     &dc->disk.bio_split);\n\t\t \n\t\tbio->bi_end_io = backing_request_endio;\n\t\tclosure_bio_submit(s->iop.c, bio, cl);\n\t}\n\ninsert_data:\n\tclosure_call(&s->iop.cl, bch_data_insert, NULL, cl);\n\tcontinue_at(cl, cached_dev_write_complete, NULL);\n}\n\nstatic void cached_dev_nodata(struct closure *cl)\n{\n\tstruct search *s = container_of(cl, struct search, cl);\n\tstruct bio *bio = &s->bio.bio;\n\n\tif (s->iop.flush_journal)\n\t\tbch_journal_meta(s->iop.c, cl);\n\n\t \n\tbio->bi_end_io = backing_request_endio;\n\tclosure_bio_submit(s->iop.c, bio, cl);\n\n\tcontinue_at(cl, cached_dev_bio_complete, NULL);\n}\n\nstruct detached_dev_io_private {\n\tstruct bcache_device\t*d;\n\tunsigned long\t\tstart_time;\n\tbio_end_io_t\t\t*bi_end_io;\n\tvoid\t\t\t*bi_private;\n\tstruct block_device\t*orig_bdev;\n};\n\nstatic void detached_dev_end_io(struct bio *bio)\n{\n\tstruct detached_dev_io_private *ddip;\n\n\tddip = bio->bi_private;\n\tbio->bi_end_io = ddip->bi_end_io;\n\tbio->bi_private = ddip->bi_private;\n\n\t \n\tbio_end_io_acct_remapped(bio, ddip->start_time, ddip->orig_bdev);\n\n\tif (bio->bi_status) {\n\t\tstruct cached_dev *dc = container_of(ddip->d,\n\t\t\t\t\t\t     struct cached_dev, disk);\n\t\t \n\t\tbch_count_backing_io_errors(dc, bio);\n\t}\n\n\tkfree(ddip);\n\tbio->bi_end_io(bio);\n}\n\nstatic void detached_dev_do_request(struct bcache_device *d, struct bio *bio,\n\t\tstruct block_device *orig_bdev, unsigned long start_time)\n{\n\tstruct detached_dev_io_private *ddip;\n\tstruct cached_dev *dc = container_of(d, struct cached_dev, disk);\n\n\t \n\tddip = kzalloc(sizeof(struct detached_dev_io_private), GFP_NOIO);\n\tif (!ddip) {\n\t\tbio->bi_status = BLK_STS_RESOURCE;\n\t\tbio->bi_end_io(bio);\n\t\treturn;\n\t}\n\n\tddip->d = d;\n\t \n\tddip->orig_bdev = orig_bdev;\n\tddip->start_time = start_time;\n\tddip->bi_end_io = bio->bi_end_io;\n\tddip->bi_private = bio->bi_private;\n\tbio->bi_end_io = detached_dev_end_io;\n\tbio->bi_private = ddip;\n\n\tif ((bio_op(bio) == REQ_OP_DISCARD) &&\n\t    !bdev_max_discard_sectors(dc->bdev))\n\t\tbio->bi_end_io(bio);\n\telse\n\t\tsubmit_bio_noacct(bio);\n}\n\nstatic void quit_max_writeback_rate(struct cache_set *c,\n\t\t\t\t    struct cached_dev *this_dc)\n{\n\tint i;\n\tstruct bcache_device *d;\n\tstruct cached_dev *dc;\n\n\t \n\tif (mutex_trylock(&bch_register_lock)) {\n\t\tfor (i = 0; i < c->devices_max_used; i++) {\n\t\t\tif (!c->devices[i])\n\t\t\t\tcontinue;\n\n\t\t\tif (UUID_FLASH_ONLY(&c->uuids[i]))\n\t\t\t\tcontinue;\n\n\t\t\td = c->devices[i];\n\t\t\tdc = container_of(d, struct cached_dev, disk);\n\t\t\t \n\t\t\tatomic_long_set(&dc->writeback_rate.rate, 1);\n\t\t}\n\t\tmutex_unlock(&bch_register_lock);\n\t} else\n\t\tatomic_long_set(&this_dc->writeback_rate.rate, 1);\n}\n\n \n\nvoid cached_dev_submit_bio(struct bio *bio)\n{\n\tstruct search *s;\n\tstruct block_device *orig_bdev = bio->bi_bdev;\n\tstruct bcache_device *d = orig_bdev->bd_disk->private_data;\n\tstruct cached_dev *dc = container_of(d, struct cached_dev, disk);\n\tunsigned long start_time;\n\tint rw = bio_data_dir(bio);\n\n\tif (unlikely((d->c && test_bit(CACHE_SET_IO_DISABLE, &d->c->flags)) ||\n\t\t     dc->io_disable)) {\n\t\tbio->bi_status = BLK_STS_IOERR;\n\t\tbio_endio(bio);\n\t\treturn;\n\t}\n\n\tif (likely(d->c)) {\n\t\tif (atomic_read(&d->c->idle_counter))\n\t\t\tatomic_set(&d->c->idle_counter, 0);\n\t\t \n\t\tif (unlikely(atomic_read(&d->c->at_max_writeback_rate) == 1)) {\n\t\t\tatomic_set(&d->c->at_max_writeback_rate, 0);\n\t\t\tquit_max_writeback_rate(d->c, dc);\n\t\t}\n\t}\n\n\tstart_time = bio_start_io_acct(bio);\n\n\tbio_set_dev(bio, dc->bdev);\n\tbio->bi_iter.bi_sector += dc->sb.data_offset;\n\n\tif (cached_dev_get(dc)) {\n\t\ts = search_alloc(bio, d, orig_bdev, start_time);\n\t\ttrace_bcache_request_start(s->d, bio);\n\n\t\tif (!bio->bi_iter.bi_size) {\n\t\t\t \n\t\t\tcontinue_at_nobarrier(&s->cl,\n\t\t\t\t\t      cached_dev_nodata,\n\t\t\t\t\t      bcache_wq);\n\t\t} else {\n\t\t\ts->iop.bypass = check_should_bypass(dc, bio);\n\n\t\t\tif (rw)\n\t\t\t\tcached_dev_write(dc, s);\n\t\t\telse\n\t\t\t\tcached_dev_read(dc, s);\n\t\t}\n\t} else\n\t\t \n\t\tdetached_dev_do_request(d, bio, orig_bdev, start_time);\n}\n\nstatic int cached_dev_ioctl(struct bcache_device *d, blk_mode_t mode,\n\t\t\t    unsigned int cmd, unsigned long arg)\n{\n\tstruct cached_dev *dc = container_of(d, struct cached_dev, disk);\n\n\tif (dc->io_disable)\n\t\treturn -EIO;\n\tif (!dc->bdev->bd_disk->fops->ioctl)\n\t\treturn -ENOTTY;\n\treturn dc->bdev->bd_disk->fops->ioctl(dc->bdev, mode, cmd, arg);\n}\n\nvoid bch_cached_dev_request_init(struct cached_dev *dc)\n{\n\tdc->disk.cache_miss\t\t\t= cached_dev_cache_miss;\n\tdc->disk.ioctl\t\t\t\t= cached_dev_ioctl;\n}\n\n \n\nstatic int flash_dev_cache_miss(struct btree *b, struct search *s,\n\t\t\t\tstruct bio *bio, unsigned int sectors)\n{\n\tunsigned int bytes = min(sectors, bio_sectors(bio)) << 9;\n\n\tswap(bio->bi_iter.bi_size, bytes);\n\tzero_fill_bio(bio);\n\tswap(bio->bi_iter.bi_size, bytes);\n\n\tbio_advance(bio, bytes);\n\n\tif (!bio->bi_iter.bi_size)\n\t\treturn MAP_DONE;\n\n\treturn MAP_CONTINUE;\n}\n\nstatic void flash_dev_nodata(struct closure *cl)\n{\n\tstruct search *s = container_of(cl, struct search, cl);\n\n\tif (s->iop.flush_journal)\n\t\tbch_journal_meta(s->iop.c, cl);\n\n\tcontinue_at(cl, search_free, NULL);\n}\n\nvoid flash_dev_submit_bio(struct bio *bio)\n{\n\tstruct search *s;\n\tstruct closure *cl;\n\tstruct bcache_device *d = bio->bi_bdev->bd_disk->private_data;\n\n\tif (unlikely(d->c && test_bit(CACHE_SET_IO_DISABLE, &d->c->flags))) {\n\t\tbio->bi_status = BLK_STS_IOERR;\n\t\tbio_endio(bio);\n\t\treturn;\n\t}\n\n\ts = search_alloc(bio, d, bio->bi_bdev, bio_start_io_acct(bio));\n\tcl = &s->cl;\n\tbio = &s->bio.bio;\n\n\ttrace_bcache_request_start(s->d, bio);\n\n\tif (!bio->bi_iter.bi_size) {\n\t\t \n\t\tcontinue_at_nobarrier(&s->cl,\n\t\t\t\t      flash_dev_nodata,\n\t\t\t\t      bcache_wq);\n\t\treturn;\n\t} else if (bio_data_dir(bio)) {\n\t\tbch_keybuf_check_overlapping(&s->iop.c->moving_gc_keys,\n\t\t\t\t\t&KEY(d->id, bio->bi_iter.bi_sector, 0),\n\t\t\t\t\t&KEY(d->id, bio_end_sector(bio), 0));\n\n\t\ts->iop.bypass\t\t= (bio_op(bio) == REQ_OP_DISCARD) != 0;\n\t\ts->iop.writeback\t= true;\n\t\ts->iop.bio\t\t= bio;\n\n\t\tclosure_call(&s->iop.cl, bch_data_insert, NULL, cl);\n\t} else {\n\t\tclosure_call(&s->iop.cl, cache_lookup, NULL, cl);\n\t}\n\n\tcontinue_at(cl, search_free, NULL);\n}\n\nstatic int flash_dev_ioctl(struct bcache_device *d, blk_mode_t mode,\n\t\t\t   unsigned int cmd, unsigned long arg)\n{\n\treturn -ENOTTY;\n}\n\nvoid bch_flash_dev_request_init(struct bcache_device *d)\n{\n\td->cache_miss\t\t\t\t= flash_dev_cache_miss;\n\td->ioctl\t\t\t\t= flash_dev_ioctl;\n}\n\nvoid bch_request_exit(void)\n{\n\tkmem_cache_destroy(bch_search_cache);\n}\n\nint __init bch_request_init(void)\n{\n\tbch_search_cache = KMEM_CACHE(search, 0);\n\tif (!bch_search_cache)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}