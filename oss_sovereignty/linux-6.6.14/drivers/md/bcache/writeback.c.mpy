{
  "module_name": "writeback.c",
  "hash_id": "1bc40ab9ef4923dd3d528cd9851069f80ba0e4e6443b174632f6d39bf67b8dc7",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/bcache/writeback.c",
  "human_readable_source": "\n \n\n#include \"bcache.h\"\n#include \"btree.h\"\n#include \"debug.h\"\n#include \"writeback.h\"\n\n#include <linux/delay.h>\n#include <linux/kthread.h>\n#include <linux/sched/clock.h>\n#include <trace/events/bcache.h>\n\nstatic void update_gc_after_writeback(struct cache_set *c)\n{\n\tif (c->gc_after_writeback != (BCH_ENABLE_AUTO_GC) ||\n\t    c->gc_stats.in_use < BCH_AUTO_GC_DIRTY_THRESHOLD)\n\t\treturn;\n\n\tc->gc_after_writeback |= BCH_DO_AUTO_GC;\n}\n\n \nstatic uint64_t __calc_target_rate(struct cached_dev *dc)\n{\n\tstruct cache_set *c = dc->disk.c;\n\n\t \n\tuint64_t cache_sectors = c->nbuckets * c->cache->sb.bucket_size -\n\t\t\t\tatomic_long_read(&c->flash_dev_dirty_sectors);\n\n\t \n\tuint32_t bdev_share =\n\t\tdiv64_u64(bdev_nr_sectors(dc->bdev) << WRITEBACK_SHARE_SHIFT,\n\t\t\t\tc->cached_dev_sectors);\n\n\tuint64_t cache_dirty_target =\n\t\tdiv_u64(cache_sectors * dc->writeback_percent, 100);\n\n\t \n\tif (bdev_share < 1)\n\t\tbdev_share = 1;\n\n\treturn (cache_dirty_target * bdev_share) >> WRITEBACK_SHARE_SHIFT;\n}\n\nstatic void __update_writeback_rate(struct cached_dev *dc)\n{\n\t \n\tint64_t target = __calc_target_rate(dc);\n\tint64_t dirty = bcache_dev_sectors_dirty(&dc->disk);\n\tint64_t error = dirty - target;\n\tint64_t proportional_scaled =\n\t\tdiv_s64(error, dc->writeback_rate_p_term_inverse);\n\tint64_t integral_scaled;\n\tuint32_t new_rate;\n\n\t \n\tstruct cache_set *c = dc->disk.c;\n\n\tint64_t dirty_buckets = c->nbuckets - c->avail_nbuckets;\n\n\tif (dc->writeback_consider_fragment &&\n\t\tc->gc_stats.in_use > BCH_WRITEBACK_FRAGMENT_THRESHOLD_LOW && dirty > 0) {\n\t\tint64_t fragment =\n\t\t\tdiv_s64((dirty_buckets *  c->cache->sb.bucket_size), dirty);\n\t\tint64_t fp_term;\n\t\tint64_t fps;\n\n\t\tif (c->gc_stats.in_use <= BCH_WRITEBACK_FRAGMENT_THRESHOLD_MID) {\n\t\t\tfp_term = (int64_t)dc->writeback_rate_fp_term_low *\n\t\t\t(c->gc_stats.in_use - BCH_WRITEBACK_FRAGMENT_THRESHOLD_LOW);\n\t\t} else if (c->gc_stats.in_use <= BCH_WRITEBACK_FRAGMENT_THRESHOLD_HIGH) {\n\t\t\tfp_term = (int64_t)dc->writeback_rate_fp_term_mid *\n\t\t\t(c->gc_stats.in_use - BCH_WRITEBACK_FRAGMENT_THRESHOLD_MID);\n\t\t} else {\n\t\t\tfp_term = (int64_t)dc->writeback_rate_fp_term_high *\n\t\t\t(c->gc_stats.in_use - BCH_WRITEBACK_FRAGMENT_THRESHOLD_HIGH);\n\t\t}\n\t\tfps = div_s64(dirty, dirty_buckets) * fp_term;\n\t\tif (fragment > 3 && fps > proportional_scaled) {\n\t\t\t \n\t\t\tproportional_scaled = fps;\n\t\t}\n\t}\n\n\tif ((error < 0 && dc->writeback_rate_integral > 0) ||\n\t    (error > 0 && time_before64(local_clock(),\n\t\t\t dc->writeback_rate.next + NSEC_PER_MSEC))) {\n\t\t \n\t\tdc->writeback_rate_integral += error *\n\t\t\tdc->writeback_rate_update_seconds;\n\t}\n\n\tintegral_scaled = div_s64(dc->writeback_rate_integral,\n\t\t\tdc->writeback_rate_i_term_inverse);\n\n\tnew_rate = clamp_t(int32_t, (proportional_scaled + integral_scaled),\n\t\t\tdc->writeback_rate_minimum, NSEC_PER_SEC);\n\n\tdc->writeback_rate_proportional = proportional_scaled;\n\tdc->writeback_rate_integral_scaled = integral_scaled;\n\tdc->writeback_rate_change = new_rate -\n\t\t\tatomic_long_read(&dc->writeback_rate.rate);\n\tatomic_long_set(&dc->writeback_rate.rate, new_rate);\n\tdc->writeback_rate_target = target;\n}\n\nstatic bool idle_counter_exceeded(struct cache_set *c)\n{\n\tint counter, dev_nr;\n\n\t \n\tcounter = atomic_inc_return(&c->idle_counter);\n\tif (counter <= 0) {\n\t\tatomic_set(&c->idle_counter, 0);\n\t\treturn false;\n\t}\n\n\tdev_nr = atomic_read(&c->attached_dev_nr);\n\tif (dev_nr == 0)\n\t\treturn false;\n\n\t \n\tif (counter < (dev_nr * dev_nr * 6))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic bool set_at_max_writeback_rate(struct cache_set *c,\n\t\t\t\t       struct cached_dev *dc)\n{\n\t \n\tif (!c->idle_max_writeback_rate_enabled)\n\t\treturn false;\n\n\t \n\tif (!c->gc_mark_valid)\n\t\treturn false;\n\n\tif (!idle_counter_exceeded(c))\n\t\treturn false;\n\n\tif (atomic_read(&c->at_max_writeback_rate) != 1)\n\t\tatomic_set(&c->at_max_writeback_rate, 1);\n\n\tatomic_long_set(&dc->writeback_rate.rate, INT_MAX);\n\n\t \n\tdc->writeback_rate_proportional = 0;\n\tdc->writeback_rate_integral_scaled = 0;\n\tdc->writeback_rate_change = 0;\n\n\t \n\tif (!idle_counter_exceeded(c) ||\n\t    !atomic_read(&c->at_max_writeback_rate))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void update_writeback_rate(struct work_struct *work)\n{\n\tstruct cached_dev *dc = container_of(to_delayed_work(work),\n\t\t\t\t\t     struct cached_dev,\n\t\t\t\t\t     writeback_rate_update);\n\tstruct cache_set *c = dc->disk.c;\n\n\t \n\tset_bit(BCACHE_DEV_RATE_DW_RUNNING, &dc->disk.flags);\n\t \n\tsmp_mb__after_atomic();\n\n\t \n\tif (!test_bit(BCACHE_DEV_WB_RUNNING, &dc->disk.flags) ||\n\t    test_bit(CACHE_SET_IO_DISABLE, &c->flags)) {\n\t\tclear_bit(BCACHE_DEV_RATE_DW_RUNNING, &dc->disk.flags);\n\t\t \n\t\tsmp_mb__after_atomic();\n\t\treturn;\n\t}\n\n\t \n\tif (atomic_read(&dc->has_dirty) && dc->writeback_percent &&\n\t    !set_at_max_writeback_rate(c, dc)) {\n\t\tdo {\n\t\t\tif (!down_read_trylock((&dc->writeback_lock))) {\n\t\t\t\tdc->rate_update_retry++;\n\t\t\t\tif (dc->rate_update_retry <=\n\t\t\t\t    BCH_WBRATE_UPDATE_MAX_SKIPS)\n\t\t\t\t\tbreak;\n\t\t\t\tdown_read(&dc->writeback_lock);\n\t\t\t\tdc->rate_update_retry = 0;\n\t\t\t}\n\t\t\t__update_writeback_rate(dc);\n\t\t\tupdate_gc_after_writeback(c);\n\t\t\tup_read(&dc->writeback_lock);\n\t\t} while (0);\n\t}\n\n\n\t \n\tif (test_bit(BCACHE_DEV_WB_RUNNING, &dc->disk.flags) &&\n\t    !test_bit(CACHE_SET_IO_DISABLE, &c->flags)) {\n\t\tschedule_delayed_work(&dc->writeback_rate_update,\n\t\t\t      dc->writeback_rate_update_seconds * HZ);\n\t}\n\n\t \n\tclear_bit(BCACHE_DEV_RATE_DW_RUNNING, &dc->disk.flags);\n\t \n\tsmp_mb__after_atomic();\n}\n\nstatic unsigned int writeback_delay(struct cached_dev *dc,\n\t\t\t\t    unsigned int sectors)\n{\n\tif (test_bit(BCACHE_DEV_DETACHING, &dc->disk.flags) ||\n\t    !dc->writeback_percent)\n\t\treturn 0;\n\n\treturn bch_next_delay(&dc->writeback_rate, sectors);\n}\n\nstruct dirty_io {\n\tstruct closure\t\tcl;\n\tstruct cached_dev\t*dc;\n\tuint16_t\t\tsequence;\n\tstruct bio\t\tbio;\n};\n\nstatic void dirty_init(struct keybuf_key *w)\n{\n\tstruct dirty_io *io = w->private;\n\tstruct bio *bio = &io->bio;\n\n\tbio_init(bio, NULL, bio->bi_inline_vecs,\n\t\t DIV_ROUND_UP(KEY_SIZE(&w->key), PAGE_SECTORS), 0);\n\tif (!io->dc->writeback_percent)\n\t\tbio_set_prio(bio, IOPRIO_PRIO_VALUE(IOPRIO_CLASS_IDLE, 0));\n\n\tbio->bi_iter.bi_size\t= KEY_SIZE(&w->key) << 9;\n\tbio->bi_private\t\t= w;\n\tbch_bio_map(bio, NULL);\n}\n\nstatic void dirty_io_destructor(struct closure *cl)\n{\n\tstruct dirty_io *io = container_of(cl, struct dirty_io, cl);\n\n\tkfree(io);\n}\n\nstatic void write_dirty_finish(struct closure *cl)\n{\n\tstruct dirty_io *io = container_of(cl, struct dirty_io, cl);\n\tstruct keybuf_key *w = io->bio.bi_private;\n\tstruct cached_dev *dc = io->dc;\n\n\tbio_free_pages(&io->bio);\n\n\t \n\tif (KEY_DIRTY(&w->key)) {\n\t\tint ret;\n\t\tunsigned int i;\n\t\tstruct keylist keys;\n\n\t\tbch_keylist_init(&keys);\n\n\t\tbkey_copy(keys.top, &w->key);\n\t\tSET_KEY_DIRTY(keys.top, false);\n\t\tbch_keylist_push(&keys);\n\n\t\tfor (i = 0; i < KEY_PTRS(&w->key); i++)\n\t\t\tatomic_inc(&PTR_BUCKET(dc->disk.c, &w->key, i)->pin);\n\n\t\tret = bch_btree_insert(dc->disk.c, &keys, NULL, &w->key);\n\n\t\tif (ret)\n\t\t\ttrace_bcache_writeback_collision(&w->key);\n\n\t\tatomic_long_inc(ret\n\t\t\t\t? &dc->disk.c->writeback_keys_failed\n\t\t\t\t: &dc->disk.c->writeback_keys_done);\n\t}\n\n\tbch_keybuf_del(&dc->writeback_keys, w);\n\tup(&dc->in_flight);\n\n\tclosure_return_with_destructor(cl, dirty_io_destructor);\n}\n\nstatic void dirty_endio(struct bio *bio)\n{\n\tstruct keybuf_key *w = bio->bi_private;\n\tstruct dirty_io *io = w->private;\n\n\tif (bio->bi_status) {\n\t\tSET_KEY_DIRTY(&w->key, false);\n\t\tbch_count_backing_io_errors(io->dc, bio);\n\t}\n\n\tclosure_put(&io->cl);\n}\n\nstatic void write_dirty(struct closure *cl)\n{\n\tstruct dirty_io *io = container_of(cl, struct dirty_io, cl);\n\tstruct keybuf_key *w = io->bio.bi_private;\n\tstruct cached_dev *dc = io->dc;\n\n\tuint16_t next_sequence;\n\n\tif (atomic_read(&dc->writeback_sequence_next) != io->sequence) {\n\t\t \n\t\tclosure_wait(&dc->writeback_ordering_wait, cl);\n\n\t\tif (atomic_read(&dc->writeback_sequence_next) == io->sequence) {\n\t\t\t \n\t\t\tclosure_wake_up(&dc->writeback_ordering_wait);\n\t\t}\n\n\t\tcontinue_at(cl, write_dirty, io->dc->writeback_write_wq);\n\t\treturn;\n\t}\n\n\tnext_sequence = io->sequence + 1;\n\n\t \n\tif (KEY_DIRTY(&w->key)) {\n\t\tdirty_init(w);\n\t\tio->bio.bi_opf = REQ_OP_WRITE;\n\t\tio->bio.bi_iter.bi_sector = KEY_START(&w->key);\n\t\tbio_set_dev(&io->bio, io->dc->bdev);\n\t\tio->bio.bi_end_io\t= dirty_endio;\n\n\t\t \n\t\tclosure_bio_submit(io->dc->disk.c, &io->bio, cl);\n\t}\n\n\tatomic_set(&dc->writeback_sequence_next, next_sequence);\n\tclosure_wake_up(&dc->writeback_ordering_wait);\n\n\tcontinue_at(cl, write_dirty_finish, io->dc->writeback_write_wq);\n}\n\nstatic void read_dirty_endio(struct bio *bio)\n{\n\tstruct keybuf_key *w = bio->bi_private;\n\tstruct dirty_io *io = w->private;\n\n\t \n\tbch_count_io_errors(io->dc->disk.c->cache,\n\t\t\t    bio->bi_status, 1,\n\t\t\t    \"reading dirty data from cache\");\n\n\tdirty_endio(bio);\n}\n\nstatic void read_dirty_submit(struct closure *cl)\n{\n\tstruct dirty_io *io = container_of(cl, struct dirty_io, cl);\n\n\tclosure_bio_submit(io->dc->disk.c, &io->bio, cl);\n\n\tcontinue_at(cl, write_dirty, io->dc->writeback_write_wq);\n}\n\nstatic void read_dirty(struct cached_dev *dc)\n{\n\tunsigned int delay = 0;\n\tstruct keybuf_key *next, *keys[MAX_WRITEBACKS_IN_PASS], *w;\n\tsize_t size;\n\tint nk, i;\n\tstruct dirty_io *io;\n\tstruct closure cl;\n\tuint16_t sequence = 0;\n\n\tBUG_ON(!llist_empty(&dc->writeback_ordering_wait.list));\n\tatomic_set(&dc->writeback_sequence_next, sequence);\n\tclosure_init_stack(&cl);\n\n\t \n\n\tnext = bch_keybuf_next(&dc->writeback_keys);\n\n\twhile (!kthread_should_stop() &&\n\t       !test_bit(CACHE_SET_IO_DISABLE, &dc->disk.c->flags) &&\n\t       next) {\n\t\tsize = 0;\n\t\tnk = 0;\n\n\t\tdo {\n\t\t\tBUG_ON(ptr_stale(dc->disk.c, &next->key, 0));\n\n\t\t\t \n\t\t\tif (nk >= MAX_WRITEBACKS_IN_PASS)\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tif (size >= MAX_WRITESIZE_IN_PASS)\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tif ((nk != 0) && bkey_cmp(&keys[nk-1]->key,\n\t\t\t\t\t\t&START_KEY(&next->key)))\n\t\t\t\tbreak;\n\n\t\t\tsize += KEY_SIZE(&next->key);\n\t\t\tkeys[nk++] = next;\n\t\t} while ((next = bch_keybuf_next(&dc->writeback_keys)));\n\n\t\t \n\t\tfor (i = 0; i < nk; i++) {\n\t\t\tw = keys[i];\n\n\t\t\tio = kzalloc(struct_size(io, bio.bi_inline_vecs,\n\t\t\t\t\t\tDIV_ROUND_UP(KEY_SIZE(&w->key), PAGE_SECTORS)),\n\t\t\t\t     GFP_KERNEL);\n\t\t\tif (!io)\n\t\t\t\tgoto err;\n\n\t\t\tw->private\t= io;\n\t\t\tio->dc\t\t= dc;\n\t\t\tio->sequence    = sequence++;\n\n\t\t\tdirty_init(w);\n\t\t\tio->bio.bi_opf = REQ_OP_READ;\n\t\t\tio->bio.bi_iter.bi_sector = PTR_OFFSET(&w->key, 0);\n\t\t\tbio_set_dev(&io->bio, dc->disk.c->cache->bdev);\n\t\t\tio->bio.bi_end_io\t= read_dirty_endio;\n\n\t\t\tif (bch_bio_alloc_pages(&io->bio, GFP_KERNEL))\n\t\t\t\tgoto err_free;\n\n\t\t\ttrace_bcache_writeback(&w->key);\n\n\t\t\tdown(&dc->in_flight);\n\n\t\t\t \n\t\t\tclosure_call(&io->cl, read_dirty_submit, NULL, &cl);\n\t\t}\n\n\t\tdelay = writeback_delay(dc, size);\n\n\t\twhile (!kthread_should_stop() &&\n\t\t       !test_bit(CACHE_SET_IO_DISABLE, &dc->disk.c->flags) &&\n\t\t       delay) {\n\t\t\tschedule_timeout_interruptible(delay);\n\t\t\tdelay = writeback_delay(dc, 0);\n\t\t}\n\t}\n\n\tif (0) {\nerr_free:\n\t\tkfree(w->private);\nerr:\n\t\tbch_keybuf_del(&dc->writeback_keys, w);\n\t}\n\n\t \n\tclosure_sync(&cl);\n}\n\n \n\nvoid bcache_dev_sectors_dirty_add(struct cache_set *c, unsigned int inode,\n\t\t\t\t  uint64_t offset, int nr_sectors)\n{\n\tstruct bcache_device *d = c->devices[inode];\n\tunsigned int stripe_offset, sectors_dirty;\n\tint stripe;\n\n\tif (!d)\n\t\treturn;\n\n\tstripe = offset_to_stripe(d, offset);\n\tif (stripe < 0)\n\t\treturn;\n\n\tif (UUID_FLASH_ONLY(&c->uuids[inode]))\n\t\tatomic_long_add(nr_sectors, &c->flash_dev_dirty_sectors);\n\n\tstripe_offset = offset & (d->stripe_size - 1);\n\n\twhile (nr_sectors) {\n\t\tint s = min_t(unsigned int, abs(nr_sectors),\n\t\t\t      d->stripe_size - stripe_offset);\n\n\t\tif (nr_sectors < 0)\n\t\t\ts = -s;\n\n\t\tif (stripe >= d->nr_stripes)\n\t\t\treturn;\n\n\t\tsectors_dirty = atomic_add_return(s,\n\t\t\t\t\td->stripe_sectors_dirty + stripe);\n\t\tif (sectors_dirty == d->stripe_size) {\n\t\t\tif (!test_bit(stripe, d->full_dirty_stripes))\n\t\t\t\tset_bit(stripe, d->full_dirty_stripes);\n\t\t} else {\n\t\t\tif (test_bit(stripe, d->full_dirty_stripes))\n\t\t\t\tclear_bit(stripe, d->full_dirty_stripes);\n\t\t}\n\n\t\tnr_sectors -= s;\n\t\tstripe_offset = 0;\n\t\tstripe++;\n\t}\n}\n\nstatic bool dirty_pred(struct keybuf *buf, struct bkey *k)\n{\n\tstruct cached_dev *dc = container_of(buf,\n\t\t\t\t\t     struct cached_dev,\n\t\t\t\t\t     writeback_keys);\n\n\tBUG_ON(KEY_INODE(k) != dc->disk.id);\n\n\treturn KEY_DIRTY(k);\n}\n\nstatic void refill_full_stripes(struct cached_dev *dc)\n{\n\tstruct keybuf *buf = &dc->writeback_keys;\n\tunsigned int start_stripe, next_stripe;\n\tint stripe;\n\tbool wrapped = false;\n\n\tstripe = offset_to_stripe(&dc->disk, KEY_OFFSET(&buf->last_scanned));\n\tif (stripe < 0)\n\t\tstripe = 0;\n\n\tstart_stripe = stripe;\n\n\twhile (1) {\n\t\tstripe = find_next_bit(dc->disk.full_dirty_stripes,\n\t\t\t\t       dc->disk.nr_stripes, stripe);\n\n\t\tif (stripe == dc->disk.nr_stripes)\n\t\t\tgoto next;\n\n\t\tnext_stripe = find_next_zero_bit(dc->disk.full_dirty_stripes,\n\t\t\t\t\t\t dc->disk.nr_stripes, stripe);\n\n\t\tbuf->last_scanned = KEY(dc->disk.id,\n\t\t\t\t\tstripe * dc->disk.stripe_size, 0);\n\n\t\tbch_refill_keybuf(dc->disk.c, buf,\n\t\t\t\t  &KEY(dc->disk.id,\n\t\t\t\t       next_stripe * dc->disk.stripe_size, 0),\n\t\t\t\t  dirty_pred);\n\n\t\tif (array_freelist_empty(&buf->freelist))\n\t\t\treturn;\n\n\t\tstripe = next_stripe;\nnext:\n\t\tif (wrapped && stripe > start_stripe)\n\t\t\treturn;\n\n\t\tif (stripe == dc->disk.nr_stripes) {\n\t\t\tstripe = 0;\n\t\t\twrapped = true;\n\t\t}\n\t}\n}\n\n \nstatic bool refill_dirty(struct cached_dev *dc)\n{\n\tstruct keybuf *buf = &dc->writeback_keys;\n\tstruct bkey start = KEY(dc->disk.id, 0, 0);\n\tstruct bkey end = KEY(dc->disk.id, MAX_KEY_OFFSET, 0);\n\tstruct bkey start_pos;\n\n\t \n\tif (bkey_cmp(&buf->last_scanned, &start) < 0 ||\n\t    bkey_cmp(&buf->last_scanned, &end) > 0)\n\t\tbuf->last_scanned = start;\n\n\tif (dc->partial_stripes_expensive) {\n\t\trefill_full_stripes(dc);\n\t\tif (array_freelist_empty(&buf->freelist))\n\t\t\treturn false;\n\t}\n\n\tstart_pos = buf->last_scanned;\n\tbch_refill_keybuf(dc->disk.c, buf, &end, dirty_pred);\n\n\tif (bkey_cmp(&buf->last_scanned, &end) < 0)\n\t\treturn false;\n\n\t \n\tbuf->last_scanned = start;\n\tbch_refill_keybuf(dc->disk.c, buf, &start_pos, dirty_pred);\n\n\treturn bkey_cmp(&buf->last_scanned, &start_pos) >= 0;\n}\n\nstatic int bch_writeback_thread(void *arg)\n{\n\tstruct cached_dev *dc = arg;\n\tstruct cache_set *c = dc->disk.c;\n\tbool searched_full_index;\n\n\tbch_ratelimit_reset(&dc->writeback_rate);\n\n\twhile (!kthread_should_stop() &&\n\t       !test_bit(CACHE_SET_IO_DISABLE, &c->flags)) {\n\t\tdown_write(&dc->writeback_lock);\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t \n\t\tif (!test_bit(BCACHE_DEV_DETACHING, &dc->disk.flags) &&\n\t\t    (!atomic_read(&dc->has_dirty) || !dc->writeback_running)) {\n\t\t\tup_write(&dc->writeback_lock);\n\n\t\t\tif (kthread_should_stop() ||\n\t\t\t    test_bit(CACHE_SET_IO_DISABLE, &c->flags)) {\n\t\t\t\tset_current_state(TASK_RUNNING);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tschedule();\n\t\t\tcontinue;\n\t\t}\n\t\tset_current_state(TASK_RUNNING);\n\n\t\tsearched_full_index = refill_dirty(dc);\n\n\t\tif (searched_full_index &&\n\t\t    RB_EMPTY_ROOT(&dc->writeback_keys.keys)) {\n\t\t\tatomic_set(&dc->has_dirty, 0);\n\t\t\tSET_BDEV_STATE(&dc->sb, BDEV_STATE_CLEAN);\n\t\t\tbch_write_bdev_super(dc, NULL);\n\t\t\t \n\t\t\tif (test_bit(BCACHE_DEV_DETACHING, &dc->disk.flags)) {\n\t\t\t\tstruct closure cl;\n\n\t\t\t\tclosure_init_stack(&cl);\n\t\t\t\tmemset(&dc->sb.set_uuid, 0, 16);\n\t\t\t\tSET_BDEV_STATE(&dc->sb, BDEV_STATE_NONE);\n\n\t\t\t\tbch_write_bdev_super(dc, &cl);\n\t\t\t\tclosure_sync(&cl);\n\n\t\t\t\tup_write(&dc->writeback_lock);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (c->gc_after_writeback ==\n\t\t\t    (BCH_ENABLE_AUTO_GC|BCH_DO_AUTO_GC)) {\n\t\t\t\tc->gc_after_writeback &= ~BCH_DO_AUTO_GC;\n\t\t\t\tforce_wake_up_gc(c);\n\t\t\t}\n\t\t}\n\n\t\tup_write(&dc->writeback_lock);\n\n\t\tread_dirty(dc);\n\n\t\tif (searched_full_index) {\n\t\t\tunsigned int delay = dc->writeback_delay * HZ;\n\n\t\t\twhile (delay &&\n\t\t\t       !kthread_should_stop() &&\n\t\t\t       !test_bit(CACHE_SET_IO_DISABLE, &c->flags) &&\n\t\t\t       !test_bit(BCACHE_DEV_DETACHING, &dc->disk.flags))\n\t\t\t\tdelay = schedule_timeout_interruptible(delay);\n\n\t\t\tbch_ratelimit_reset(&dc->writeback_rate);\n\t\t}\n\t}\n\n\tif (dc->writeback_write_wq)\n\t\tdestroy_workqueue(dc->writeback_write_wq);\n\n\tcached_dev_put(dc);\n\twait_for_kthread_stop();\n\n\treturn 0;\n}\n\n \n#define INIT_KEYS_EACH_TIME\t500000\n\nstruct sectors_dirty_init {\n\tstruct btree_op\top;\n\tunsigned int\tinode;\n\tsize_t\t\tcount;\n};\n\nstatic int sectors_dirty_init_fn(struct btree_op *_op, struct btree *b,\n\t\t\t\t struct bkey *k)\n{\n\tstruct sectors_dirty_init *op = container_of(_op,\n\t\t\t\t\t\tstruct sectors_dirty_init, op);\n\tif (KEY_INODE(k) > op->inode)\n\t\treturn MAP_DONE;\n\n\tif (KEY_DIRTY(k))\n\t\tbcache_dev_sectors_dirty_add(b->c, KEY_INODE(k),\n\t\t\t\t\t     KEY_START(k), KEY_SIZE(k));\n\n\top->count++;\n\tif (!(op->count % INIT_KEYS_EACH_TIME))\n\t\tcond_resched();\n\n\treturn MAP_CONTINUE;\n}\n\nstatic int bch_root_node_dirty_init(struct cache_set *c,\n\t\t\t\t     struct bcache_device *d,\n\t\t\t\t     struct bkey *k)\n{\n\tstruct sectors_dirty_init op;\n\tint ret;\n\n\tbch_btree_op_init(&op.op, -1);\n\top.inode = d->id;\n\top.count = 0;\n\n\tret = bcache_btree(map_keys_recurse,\n\t\t\t   k,\n\t\t\t   c->root,\n\t\t\t   &op.op,\n\t\t\t   &KEY(op.inode, 0, 0),\n\t\t\t   sectors_dirty_init_fn,\n\t\t\t   0);\n\tif (ret < 0)\n\t\tpr_warn(\"sectors dirty init failed, ret=%d!\\n\", ret);\n\n\t \n\tbch_cannibalize_unlock(c);\n\tfinish_wait(&c->btree_cache_wait, &(&op.op)->wait);\n\n\treturn ret;\n}\n\nstatic int bch_dirty_init_thread(void *arg)\n{\n\tstruct dirty_init_thrd_info *info = arg;\n\tstruct bch_dirty_init_state *state = info->state;\n\tstruct cache_set *c = state->c;\n\tstruct btree_iter iter;\n\tstruct bkey *k, *p;\n\tint cur_idx, prev_idx, skip_nr;\n\n\tk = p = NULL;\n\tprev_idx = 0;\n\n\tbch_btree_iter_init(&c->root->keys, &iter, NULL);\n\tk = bch_btree_iter_next_filter(&iter, &c->root->keys, bch_ptr_bad);\n\tBUG_ON(!k);\n\n\tp = k;\n\n\twhile (k) {\n\t\tspin_lock(&state->idx_lock);\n\t\tcur_idx = state->key_idx;\n\t\tstate->key_idx++;\n\t\tspin_unlock(&state->idx_lock);\n\n\t\tskip_nr = cur_idx - prev_idx;\n\n\t\twhile (skip_nr) {\n\t\t\tk = bch_btree_iter_next_filter(&iter,\n\t\t\t\t\t\t       &c->root->keys,\n\t\t\t\t\t\t       bch_ptr_bad);\n\t\t\tif (k)\n\t\t\t\tp = k;\n\t\t\telse {\n\t\t\t\tatomic_set(&state->enough, 1);\n\t\t\t\t \n\t\t\t\tsmp_mb__after_atomic();\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tskip_nr--;\n\t\t}\n\n\t\tif (p) {\n\t\t\tif (bch_root_node_dirty_init(c, state->d, p) < 0)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tp = NULL;\n\t\tprev_idx = cur_idx;\n\t}\n\nout:\n\t \n\tsmp_mb__before_atomic();\n\tif (atomic_dec_and_test(&state->started))\n\t\twake_up(&state->wait);\n\n\treturn 0;\n}\n\nstatic int bch_btre_dirty_init_thread_nr(void)\n{\n\tint n = num_online_cpus()/2;\n\n\tif (n == 0)\n\t\tn = 1;\n\telse if (n > BCH_DIRTY_INIT_THRD_MAX)\n\t\tn = BCH_DIRTY_INIT_THRD_MAX;\n\n\treturn n;\n}\n\nvoid bch_sectors_dirty_init(struct bcache_device *d)\n{\n\tint i;\n\tstruct btree *b = NULL;\n\tstruct bkey *k = NULL;\n\tstruct btree_iter iter;\n\tstruct sectors_dirty_init op;\n\tstruct cache_set *c = d->c;\n\tstruct bch_dirty_init_state state;\n\nretry_lock:\n\tb = c->root;\n\trw_lock(0, b, b->level);\n\tif (b != c->root) {\n\t\trw_unlock(0, b);\n\t\tgoto retry_lock;\n\t}\n\n\t \n\tif (c->root->level == 0) {\n\t\tbch_btree_op_init(&op.op, -1);\n\t\top.inode = d->id;\n\t\top.count = 0;\n\n\t\tfor_each_key_filter(&c->root->keys,\n\t\t\t\t    k, &iter, bch_ptr_invalid) {\n\t\t\tif (KEY_INODE(k) != op.inode)\n\t\t\t\tcontinue;\n\t\t\tsectors_dirty_init_fn(&op.op, c->root, k);\n\t\t}\n\n\t\trw_unlock(0, b);\n\t\treturn;\n\t}\n\n\tmemset(&state, 0, sizeof(struct bch_dirty_init_state));\n\tstate.c = c;\n\tstate.d = d;\n\tstate.total_threads = bch_btre_dirty_init_thread_nr();\n\tstate.key_idx = 0;\n\tspin_lock_init(&state.idx_lock);\n\tatomic_set(&state.started, 0);\n\tatomic_set(&state.enough, 0);\n\tinit_waitqueue_head(&state.wait);\n\n\tfor (i = 0; i < state.total_threads; i++) {\n\t\t \n\t\tsmp_mb__before_atomic();\n\t\tif (atomic_read(&state.enough))\n\t\t\tbreak;\n\n\t\tatomic_inc(&state.started);\n\t\tstate.infos[i].state = &state;\n\t\tstate.infos[i].thread =\n\t\t\tkthread_run(bch_dirty_init_thread, &state.infos[i],\n\t\t\t\t    \"bch_dirtcnt[%d]\", i);\n\t\tif (IS_ERR(state.infos[i].thread)) {\n\t\t\tpr_err(\"fails to run thread bch_dirty_init[%d]\\n\", i);\n\t\t\tatomic_dec(&state.started);\n\t\t\tfor (--i; i >= 0; i--)\n\t\t\t\tkthread_stop(state.infos[i].thread);\n\t\t\tgoto out;\n\t\t}\n\t}\n\nout:\n\t \n\twait_event(state.wait, atomic_read(&state.started) == 0);\n\trw_unlock(0, b);\n}\n\nvoid bch_cached_dev_writeback_init(struct cached_dev *dc)\n{\n\tsema_init(&dc->in_flight, 64);\n\tinit_rwsem(&dc->writeback_lock);\n\tbch_keybuf_init(&dc->writeback_keys);\n\n\tdc->writeback_metadata\t\t= true;\n\tdc->writeback_running\t\t= false;\n\tdc->writeback_consider_fragment = true;\n\tdc->writeback_percent\t\t= 10;\n\tdc->writeback_delay\t\t= 30;\n\tatomic_long_set(&dc->writeback_rate.rate, 1024);\n\tdc->writeback_rate_minimum\t= 8;\n\n\tdc->writeback_rate_update_seconds = WRITEBACK_RATE_UPDATE_SECS_DEFAULT;\n\tdc->writeback_rate_p_term_inverse = 40;\n\tdc->writeback_rate_fp_term_low = 1;\n\tdc->writeback_rate_fp_term_mid = 10;\n\tdc->writeback_rate_fp_term_high = 1000;\n\tdc->writeback_rate_i_term_inverse = 10000;\n\n\t \n\tdc->rate_update_retry = 0;\n\n\tWARN_ON(test_and_clear_bit(BCACHE_DEV_WB_RUNNING, &dc->disk.flags));\n\tINIT_DELAYED_WORK(&dc->writeback_rate_update, update_writeback_rate);\n}\n\nint bch_cached_dev_writeback_start(struct cached_dev *dc)\n{\n\tdc->writeback_write_wq = alloc_workqueue(\"bcache_writeback_wq\",\n\t\t\t\t\t\tWQ_MEM_RECLAIM, 0);\n\tif (!dc->writeback_write_wq)\n\t\treturn -ENOMEM;\n\n\tcached_dev_get(dc);\n\tdc->writeback_thread = kthread_create(bch_writeback_thread, dc,\n\t\t\t\t\t      \"bcache_writeback\");\n\tif (IS_ERR(dc->writeback_thread)) {\n\t\tcached_dev_put(dc);\n\t\tdestroy_workqueue(dc->writeback_write_wq);\n\t\treturn PTR_ERR(dc->writeback_thread);\n\t}\n\tdc->writeback_running = true;\n\n\tWARN_ON(test_and_set_bit(BCACHE_DEV_WB_RUNNING, &dc->disk.flags));\n\tschedule_delayed_work(&dc->writeback_rate_update,\n\t\t\t      dc->writeback_rate_update_seconds * HZ);\n\n\tbch_writeback_queue(dc);\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}