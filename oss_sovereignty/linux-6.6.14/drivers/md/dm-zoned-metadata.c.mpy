{
  "module_name": "dm-zoned-metadata.c",
  "hash_id": "8c93e071d792bb1c1fa1a86ad6c9400c6fcac4abf1ab340f39903ff887e93ef4",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/dm-zoned-metadata.c",
  "human_readable_source": "\n \n\n#include \"dm-zoned.h\"\n\n#include <linux/module.h>\n#include <linux/crc32.h>\n#include <linux/sched/mm.h>\n\n#define\tDM_MSG_PREFIX\t\t\"zoned metadata\"\n\n \n#define DMZ_META_VER\t2\n\n \n#define DMZ_MAGIC\t((((unsigned int)('D')) << 24) | \\\n\t\t\t (((unsigned int)('Z')) << 16) | \\\n\t\t\t (((unsigned int)('B')) <<  8) | \\\n\t\t\t ((unsigned int)('D')))\n\n \nstruct dmz_super {\n\t \n\t__le32\t\tmagic;\t\t\t \n\n\t \n\t__le32\t\tversion;\t\t \n\n\t \n\t__le64\t\tgen;\t\t\t \n\n\t \n\t__le64\t\tsb_block;\t\t \n\n\t \n\t__le32\t\tnr_meta_blocks;\t\t \n\n\t \n\t__le32\t\tnr_reserved_seq;\t \n\n\t \n\t__le32\t\tnr_chunks;\t\t \n\n\t \n\t__le32\t\tnr_map_blocks;\t\t \n\n\t \n\t__le32\t\tnr_bitmap_blocks;\t \n\n\t \n\t__le32\t\tcrc;\t\t\t \n\n\t \n\tu8\t\tdmz_label[32];\t\t \n\n\t \n\tu8\t\tdmz_uuid[16];\t\t \n\n\t \n\tu8\t\tdev_uuid[16];\t\t \n\n\t \n\tu8\t\treserved[400];\t\t \n};\n\n \nstruct dmz_map {\n\t__le32\t\t\tdzone_id;\n\t__le32\t\t\tbzone_id;\n};\n\n \n#define DMZ_MAP_ENTRIES\t\t(DMZ_BLOCK_SIZE / sizeof(struct dmz_map))\n#define DMZ_MAP_ENTRIES_SHIFT\t(ilog2(DMZ_MAP_ENTRIES))\n#define DMZ_MAP_ENTRIES_MASK\t(DMZ_MAP_ENTRIES - 1)\n#define DMZ_MAP_UNMAPPED\tUINT_MAX\n\n \nstruct dmz_mblock {\n\tstruct rb_node\t\tnode;\n\tstruct list_head\tlink;\n\tsector_t\t\tno;\n\tunsigned int\t\tref;\n\tunsigned long\t\tstate;\n\tstruct page\t\t*page;\n\tvoid\t\t\t*data;\n};\n\n \nenum {\n\tDMZ_META_DIRTY,\n\tDMZ_META_READING,\n\tDMZ_META_WRITING,\n\tDMZ_META_ERROR,\n};\n\n \nstruct dmz_sb {\n\tsector_t\t\tblock;\n\tstruct dmz_dev\t\t*dev;\n\tstruct dmz_mblock\t*mblk;\n\tstruct dmz_super\t*sb;\n\tstruct dm_zone\t\t*zone;\n};\n\n \nstruct dmz_metadata {\n\tstruct dmz_dev\t\t*dev;\n\tunsigned int\t\tnr_devs;\n\n\tchar\t\t\tdevname[BDEVNAME_SIZE];\n\tchar\t\t\tlabel[BDEVNAME_SIZE];\n\tuuid_t\t\t\tuuid;\n\n\tsector_t\t\tzone_bitmap_size;\n\tunsigned int\t\tzone_nr_bitmap_blocks;\n\tunsigned int\t\tzone_bits_per_mblk;\n\n\tsector_t\t\tzone_nr_blocks;\n\tsector_t\t\tzone_nr_blocks_shift;\n\n\tsector_t\t\tzone_nr_sectors;\n\tsector_t\t\tzone_nr_sectors_shift;\n\n\tunsigned int\t\tnr_bitmap_blocks;\n\tunsigned int\t\tnr_map_blocks;\n\n\tunsigned int\t\tnr_zones;\n\tunsigned int\t\tnr_useable_zones;\n\tunsigned int\t\tnr_meta_blocks;\n\tunsigned int\t\tnr_meta_zones;\n\tunsigned int\t\tnr_data_zones;\n\tunsigned int\t\tnr_cache_zones;\n\tunsigned int\t\tnr_rnd_zones;\n\tunsigned int\t\tnr_reserved_seq;\n\tunsigned int\t\tnr_chunks;\n\n\t \n\tstruct xarray\t\tzones;\n\n\tstruct dmz_sb\t\tsb[2];\n\tunsigned int\t\tmblk_primary;\n\tunsigned int\t\tsb_version;\n\tu64\t\t\tsb_gen;\n\tunsigned int\t\tmin_nr_mblks;\n\tunsigned int\t\tmax_nr_mblks;\n\tatomic_t\t\tnr_mblks;\n\tstruct rw_semaphore\tmblk_sem;\n\tstruct mutex\t\tmblk_flush_lock;\n\tspinlock_t\t\tmblk_lock;\n\tstruct rb_root\t\tmblk_rbtree;\n\tstruct list_head\tmblk_lru_list;\n\tstruct list_head\tmblk_dirty_list;\n\tstruct shrinker\t\tmblk_shrinker;\n\n\t \n\tstruct mutex\t\tmap_lock;\n\tstruct dmz_mblock\t**map_mblk;\n\n\tunsigned int\t\tnr_cache;\n\tatomic_t\t\tunmap_nr_cache;\n\tstruct list_head\tunmap_cache_list;\n\tstruct list_head\tmap_cache_list;\n\n\tatomic_t\t\tnr_reserved_seq_zones;\n\tstruct list_head\treserved_seq_zones_list;\n\n\twait_queue_head_t\tfree_wq;\n};\n\n#define dmz_zmd_info(zmd, format, args...)\t\\\n\tDMINFO(\"(%s): \" format, (zmd)->label, ## args)\n\n#define dmz_zmd_err(zmd, format, args...)\t\\\n\tDMERR(\"(%s): \" format, (zmd)->label, ## args)\n\n#define dmz_zmd_warn(zmd, format, args...)\t\\\n\tDMWARN(\"(%s): \" format, (zmd)->label, ## args)\n\n#define dmz_zmd_debug(zmd, format, args...)\t\\\n\tDMDEBUG(\"(%s): \" format, (zmd)->label, ## args)\n \nstatic unsigned int dmz_dev_zone_id(struct dmz_metadata *zmd, struct dm_zone *zone)\n{\n\tif (WARN_ON(!zone))\n\t\treturn 0;\n\n\treturn zone->id - zone->dev->zone_offset;\n}\n\nsector_t dmz_start_sect(struct dmz_metadata *zmd, struct dm_zone *zone)\n{\n\tunsigned int zone_id = dmz_dev_zone_id(zmd, zone);\n\n\treturn (sector_t)zone_id << zmd->zone_nr_sectors_shift;\n}\n\nsector_t dmz_start_block(struct dmz_metadata *zmd, struct dm_zone *zone)\n{\n\tunsigned int zone_id = dmz_dev_zone_id(zmd, zone);\n\n\treturn (sector_t)zone_id << zmd->zone_nr_blocks_shift;\n}\n\nunsigned int dmz_zone_nr_blocks(struct dmz_metadata *zmd)\n{\n\treturn zmd->zone_nr_blocks;\n}\n\nunsigned int dmz_zone_nr_blocks_shift(struct dmz_metadata *zmd)\n{\n\treturn zmd->zone_nr_blocks_shift;\n}\n\nunsigned int dmz_zone_nr_sectors(struct dmz_metadata *zmd)\n{\n\treturn zmd->zone_nr_sectors;\n}\n\nunsigned int dmz_zone_nr_sectors_shift(struct dmz_metadata *zmd)\n{\n\treturn zmd->zone_nr_sectors_shift;\n}\n\nunsigned int dmz_nr_zones(struct dmz_metadata *zmd)\n{\n\treturn zmd->nr_zones;\n}\n\nunsigned int dmz_nr_chunks(struct dmz_metadata *zmd)\n{\n\treturn zmd->nr_chunks;\n}\n\nunsigned int dmz_nr_rnd_zones(struct dmz_metadata *zmd, int idx)\n{\n\treturn zmd->dev[idx].nr_rnd;\n}\n\nunsigned int dmz_nr_unmap_rnd_zones(struct dmz_metadata *zmd, int idx)\n{\n\treturn atomic_read(&zmd->dev[idx].unmap_nr_rnd);\n}\n\nunsigned int dmz_nr_cache_zones(struct dmz_metadata *zmd)\n{\n\treturn zmd->nr_cache;\n}\n\nunsigned int dmz_nr_unmap_cache_zones(struct dmz_metadata *zmd)\n{\n\treturn atomic_read(&zmd->unmap_nr_cache);\n}\n\nunsigned int dmz_nr_seq_zones(struct dmz_metadata *zmd, int idx)\n{\n\treturn zmd->dev[idx].nr_seq;\n}\n\nunsigned int dmz_nr_unmap_seq_zones(struct dmz_metadata *zmd, int idx)\n{\n\treturn atomic_read(&zmd->dev[idx].unmap_nr_seq);\n}\n\nstatic struct dm_zone *dmz_get(struct dmz_metadata *zmd, unsigned int zone_id)\n{\n\treturn xa_load(&zmd->zones, zone_id);\n}\n\nstatic struct dm_zone *dmz_insert(struct dmz_metadata *zmd,\n\t\t\t\t  unsigned int zone_id, struct dmz_dev *dev)\n{\n\tstruct dm_zone *zone = kzalloc(sizeof(struct dm_zone), GFP_KERNEL);\n\n\tif (!zone)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (xa_insert(&zmd->zones, zone_id, zone, GFP_KERNEL)) {\n\t\tkfree(zone);\n\t\treturn ERR_PTR(-EBUSY);\n\t}\n\n\tINIT_LIST_HEAD(&zone->link);\n\tatomic_set(&zone->refcount, 0);\n\tzone->id = zone_id;\n\tzone->chunk = DMZ_MAP_UNMAPPED;\n\tzone->dev = dev;\n\n\treturn zone;\n}\n\nconst char *dmz_metadata_label(struct dmz_metadata *zmd)\n{\n\treturn (const char *)zmd->label;\n}\n\nbool dmz_check_dev(struct dmz_metadata *zmd)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < zmd->nr_devs; i++) {\n\t\tif (!dmz_check_bdev(&zmd->dev[i]))\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nbool dmz_dev_is_dying(struct dmz_metadata *zmd)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < zmd->nr_devs; i++) {\n\t\tif (dmz_bdev_is_dying(&zmd->dev[i]))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nvoid dmz_lock_map(struct dmz_metadata *zmd)\n{\n\tmutex_lock(&zmd->map_lock);\n}\n\nvoid dmz_unlock_map(struct dmz_metadata *zmd)\n{\n\tmutex_unlock(&zmd->map_lock);\n}\n\n \nvoid dmz_lock_metadata(struct dmz_metadata *zmd)\n{\n\tdown_read(&zmd->mblk_sem);\n}\n\nvoid dmz_unlock_metadata(struct dmz_metadata *zmd)\n{\n\tup_read(&zmd->mblk_sem);\n}\n\n \nvoid dmz_lock_flush(struct dmz_metadata *zmd)\n{\n\tmutex_lock(&zmd->mblk_flush_lock);\n}\n\nvoid dmz_unlock_flush(struct dmz_metadata *zmd)\n{\n\tmutex_unlock(&zmd->mblk_flush_lock);\n}\n\n \nstatic struct dmz_mblock *dmz_alloc_mblock(struct dmz_metadata *zmd,\n\t\t\t\t\t   sector_t mblk_no)\n{\n\tstruct dmz_mblock *mblk = NULL;\n\n\t \n\tif (zmd->max_nr_mblks && atomic_read(&zmd->nr_mblks) > zmd->max_nr_mblks) {\n\t\tspin_lock(&zmd->mblk_lock);\n\t\tmblk = list_first_entry_or_null(&zmd->mblk_lru_list,\n\t\t\t\t\t\tstruct dmz_mblock, link);\n\t\tif (mblk) {\n\t\t\tlist_del_init(&mblk->link);\n\t\t\trb_erase(&mblk->node, &zmd->mblk_rbtree);\n\t\t\tmblk->no = mblk_no;\n\t\t}\n\t\tspin_unlock(&zmd->mblk_lock);\n\t\tif (mblk)\n\t\t\treturn mblk;\n\t}\n\n\t \n\tmblk = kmalloc(sizeof(struct dmz_mblock), GFP_NOIO);\n\tif (!mblk)\n\t\treturn NULL;\n\n\tmblk->page = alloc_page(GFP_NOIO);\n\tif (!mblk->page) {\n\t\tkfree(mblk);\n\t\treturn NULL;\n\t}\n\n\tRB_CLEAR_NODE(&mblk->node);\n\tINIT_LIST_HEAD(&mblk->link);\n\tmblk->ref = 0;\n\tmblk->state = 0;\n\tmblk->no = mblk_no;\n\tmblk->data = page_address(mblk->page);\n\n\tatomic_inc(&zmd->nr_mblks);\n\n\treturn mblk;\n}\n\n \nstatic void dmz_free_mblock(struct dmz_metadata *zmd, struct dmz_mblock *mblk)\n{\n\t__free_pages(mblk->page, 0);\n\tkfree(mblk);\n\n\tatomic_dec(&zmd->nr_mblks);\n}\n\n \nstatic void dmz_insert_mblock(struct dmz_metadata *zmd, struct dmz_mblock *mblk)\n{\n\tstruct rb_root *root = &zmd->mblk_rbtree;\n\tstruct rb_node **new = &(root->rb_node), *parent = NULL;\n\tstruct dmz_mblock *b;\n\n\t \n\twhile (*new) {\n\t\tb = container_of(*new, struct dmz_mblock, node);\n\t\tparent = *new;\n\t\tnew = (b->no < mblk->no) ? &((*new)->rb_left) : &((*new)->rb_right);\n\t}\n\n\t \n\trb_link_node(&mblk->node, parent, new);\n\trb_insert_color(&mblk->node, root);\n}\n\n \nstatic struct dmz_mblock *dmz_get_mblock_fast(struct dmz_metadata *zmd,\n\t\t\t\t\t      sector_t mblk_no)\n{\n\tstruct rb_root *root = &zmd->mblk_rbtree;\n\tstruct rb_node *node = root->rb_node;\n\tstruct dmz_mblock *mblk;\n\n\twhile (node) {\n\t\tmblk = container_of(node, struct dmz_mblock, node);\n\t\tif (mblk->no == mblk_no) {\n\t\t\t \n\t\t\tmblk->ref++;\n\t\t\tif (mblk->ref == 1 &&\n\t\t\t    !test_bit(DMZ_META_DIRTY, &mblk->state))\n\t\t\t\tlist_del_init(&mblk->link);\n\t\t\treturn mblk;\n\t\t}\n\t\tnode = (mblk->no < mblk_no) ? node->rb_left : node->rb_right;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic void dmz_mblock_bio_end_io(struct bio *bio)\n{\n\tstruct dmz_mblock *mblk = bio->bi_private;\n\tint flag;\n\n\tif (bio->bi_status)\n\t\tset_bit(DMZ_META_ERROR, &mblk->state);\n\n\tif (bio_op(bio) == REQ_OP_WRITE)\n\t\tflag = DMZ_META_WRITING;\n\telse\n\t\tflag = DMZ_META_READING;\n\n\tclear_bit_unlock(flag, &mblk->state);\n\tsmp_mb__after_atomic();\n\twake_up_bit(&mblk->state, flag);\n\n\tbio_put(bio);\n}\n\n \nstatic struct dmz_mblock *dmz_get_mblock_slow(struct dmz_metadata *zmd,\n\t\t\t\t\t      sector_t mblk_no)\n{\n\tstruct dmz_mblock *mblk, *m;\n\tsector_t block = zmd->sb[zmd->mblk_primary].block + mblk_no;\n\tstruct dmz_dev *dev = zmd->sb[zmd->mblk_primary].dev;\n\tstruct bio *bio;\n\n\tif (dmz_bdev_is_dying(dev))\n\t\treturn ERR_PTR(-EIO);\n\n\t \n\tmblk = dmz_alloc_mblock(zmd, mblk_no);\n\tif (!mblk)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbio = bio_alloc(dev->bdev, 1, REQ_OP_READ | REQ_META | REQ_PRIO,\n\t\t\tGFP_NOIO);\n\n\tspin_lock(&zmd->mblk_lock);\n\n\t \n\tm = dmz_get_mblock_fast(zmd, mblk_no);\n\tif (m) {\n\t\tspin_unlock(&zmd->mblk_lock);\n\t\tdmz_free_mblock(zmd, mblk);\n\t\tbio_put(bio);\n\t\treturn m;\n\t}\n\n\tmblk->ref++;\n\tset_bit(DMZ_META_READING, &mblk->state);\n\tdmz_insert_mblock(zmd, mblk);\n\n\tspin_unlock(&zmd->mblk_lock);\n\n\t \n\tbio->bi_iter.bi_sector = dmz_blk2sect(block);\n\tbio->bi_private = mblk;\n\tbio->bi_end_io = dmz_mblock_bio_end_io;\n\t__bio_add_page(bio, mblk->page, DMZ_BLOCK_SIZE, 0);\n\tsubmit_bio(bio);\n\n\treturn mblk;\n}\n\n \nstatic unsigned long dmz_shrink_mblock_cache(struct dmz_metadata *zmd,\n\t\t\t\t\t     unsigned long limit)\n{\n\tstruct dmz_mblock *mblk;\n\tunsigned long count = 0;\n\n\tif (!zmd->max_nr_mblks)\n\t\treturn 0;\n\n\twhile (!list_empty(&zmd->mblk_lru_list) &&\n\t       atomic_read(&zmd->nr_mblks) > zmd->min_nr_mblks &&\n\t       count < limit) {\n\t\tmblk = list_first_entry(&zmd->mblk_lru_list,\n\t\t\t\t\tstruct dmz_mblock, link);\n\t\tlist_del_init(&mblk->link);\n\t\trb_erase(&mblk->node, &zmd->mblk_rbtree);\n\t\tdmz_free_mblock(zmd, mblk);\n\t\tcount++;\n\t}\n\n\treturn count;\n}\n\n \nstatic unsigned long dmz_mblock_shrinker_count(struct shrinker *shrink,\n\t\t\t\t\t       struct shrink_control *sc)\n{\n\tstruct dmz_metadata *zmd = container_of(shrink, struct dmz_metadata, mblk_shrinker);\n\n\treturn atomic_read(&zmd->nr_mblks);\n}\n\n \nstatic unsigned long dmz_mblock_shrinker_scan(struct shrinker *shrink,\n\t\t\t\t\t      struct shrink_control *sc)\n{\n\tstruct dmz_metadata *zmd = container_of(shrink, struct dmz_metadata, mblk_shrinker);\n\tunsigned long count;\n\n\tspin_lock(&zmd->mblk_lock);\n\tcount = dmz_shrink_mblock_cache(zmd, sc->nr_to_scan);\n\tspin_unlock(&zmd->mblk_lock);\n\n\treturn count ? count : SHRINK_STOP;\n}\n\n \nstatic void dmz_release_mblock(struct dmz_metadata *zmd,\n\t\t\t       struct dmz_mblock *mblk)\n{\n\n\tif (!mblk)\n\t\treturn;\n\n\tspin_lock(&zmd->mblk_lock);\n\n\tmblk->ref--;\n\tif (mblk->ref == 0) {\n\t\tif (test_bit(DMZ_META_ERROR, &mblk->state)) {\n\t\t\trb_erase(&mblk->node, &zmd->mblk_rbtree);\n\t\t\tdmz_free_mblock(zmd, mblk);\n\t\t} else if (!test_bit(DMZ_META_DIRTY, &mblk->state)) {\n\t\t\tlist_add_tail(&mblk->link, &zmd->mblk_lru_list);\n\t\t\tdmz_shrink_mblock_cache(zmd, 1);\n\t\t}\n\t}\n\n\tspin_unlock(&zmd->mblk_lock);\n}\n\n \nstatic struct dmz_mblock *dmz_get_mblock(struct dmz_metadata *zmd,\n\t\t\t\t\t sector_t mblk_no)\n{\n\tstruct dmz_mblock *mblk;\n\tstruct dmz_dev *dev = zmd->sb[zmd->mblk_primary].dev;\n\n\t \n\tspin_lock(&zmd->mblk_lock);\n\tmblk = dmz_get_mblock_fast(zmd, mblk_no);\n\tspin_unlock(&zmd->mblk_lock);\n\n\tif (!mblk) {\n\t\t \n\t\tmblk = dmz_get_mblock_slow(zmd, mblk_no);\n\t\tif (IS_ERR(mblk))\n\t\t\treturn mblk;\n\t}\n\n\t \n\twait_on_bit_io(&mblk->state, DMZ_META_READING,\n\t\t       TASK_UNINTERRUPTIBLE);\n\tif (test_bit(DMZ_META_ERROR, &mblk->state)) {\n\t\tdmz_release_mblock(zmd, mblk);\n\t\tdmz_check_bdev(dev);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\n\treturn mblk;\n}\n\n \nstatic void dmz_dirty_mblock(struct dmz_metadata *zmd, struct dmz_mblock *mblk)\n{\n\tspin_lock(&zmd->mblk_lock);\n\tif (!test_and_set_bit(DMZ_META_DIRTY, &mblk->state))\n\t\tlist_add_tail(&mblk->link, &zmd->mblk_dirty_list);\n\tspin_unlock(&zmd->mblk_lock);\n}\n\n \nstatic int dmz_write_mblock(struct dmz_metadata *zmd, struct dmz_mblock *mblk,\n\t\t\t    unsigned int set)\n{\n\tstruct dmz_dev *dev = zmd->sb[set].dev;\n\tsector_t block = zmd->sb[set].block + mblk->no;\n\tstruct bio *bio;\n\n\tif (dmz_bdev_is_dying(dev))\n\t\treturn -EIO;\n\n\tbio = bio_alloc(dev->bdev, 1, REQ_OP_WRITE | REQ_META | REQ_PRIO,\n\t\t\tGFP_NOIO);\n\n\tset_bit(DMZ_META_WRITING, &mblk->state);\n\n\tbio->bi_iter.bi_sector = dmz_blk2sect(block);\n\tbio->bi_private = mblk;\n\tbio->bi_end_io = dmz_mblock_bio_end_io;\n\t__bio_add_page(bio, mblk->page, DMZ_BLOCK_SIZE, 0);\n\tsubmit_bio(bio);\n\n\treturn 0;\n}\n\n \nstatic int dmz_rdwr_block(struct dmz_dev *dev, enum req_op op,\n\t\t\t  sector_t block, struct page *page)\n{\n\tstruct bio *bio;\n\tint ret;\n\n\tif (WARN_ON(!dev))\n\t\treturn -EIO;\n\n\tif (dmz_bdev_is_dying(dev))\n\t\treturn -EIO;\n\n\tbio = bio_alloc(dev->bdev, 1, op | REQ_SYNC | REQ_META | REQ_PRIO,\n\t\t\tGFP_NOIO);\n\tbio->bi_iter.bi_sector = dmz_blk2sect(block);\n\t__bio_add_page(bio, page, DMZ_BLOCK_SIZE, 0);\n\tret = submit_bio_wait(bio);\n\tbio_put(bio);\n\n\tif (ret)\n\t\tdmz_check_bdev(dev);\n\treturn ret;\n}\n\n \nstatic int dmz_write_sb(struct dmz_metadata *zmd, unsigned int set)\n{\n\tstruct dmz_mblock *mblk = zmd->sb[set].mblk;\n\tstruct dmz_super *sb = zmd->sb[set].sb;\n\tstruct dmz_dev *dev = zmd->sb[set].dev;\n\tsector_t sb_block;\n\tu64 sb_gen = zmd->sb_gen + 1;\n\tint ret;\n\n\tsb->magic = cpu_to_le32(DMZ_MAGIC);\n\n\tsb->version = cpu_to_le32(zmd->sb_version);\n\tif (zmd->sb_version > 1) {\n\t\tBUILD_BUG_ON(UUID_SIZE != 16);\n\t\texport_uuid(sb->dmz_uuid, &zmd->uuid);\n\t\tmemcpy(sb->dmz_label, zmd->label, BDEVNAME_SIZE);\n\t\texport_uuid(sb->dev_uuid, &dev->uuid);\n\t}\n\n\tsb->gen = cpu_to_le64(sb_gen);\n\n\t \n\tsb_block = zmd->sb[set].zone->id << zmd->zone_nr_blocks_shift;\n\tsb->sb_block = cpu_to_le64(sb_block);\n\tsb->nr_meta_blocks = cpu_to_le32(zmd->nr_meta_blocks);\n\tsb->nr_reserved_seq = cpu_to_le32(zmd->nr_reserved_seq);\n\tsb->nr_chunks = cpu_to_le32(zmd->nr_chunks);\n\n\tsb->nr_map_blocks = cpu_to_le32(zmd->nr_map_blocks);\n\tsb->nr_bitmap_blocks = cpu_to_le32(zmd->nr_bitmap_blocks);\n\n\tsb->crc = 0;\n\tsb->crc = cpu_to_le32(crc32_le(sb_gen, (unsigned char *)sb, DMZ_BLOCK_SIZE));\n\n\tret = dmz_rdwr_block(dev, REQ_OP_WRITE, zmd->sb[set].block,\n\t\t\t     mblk->page);\n\tif (ret == 0)\n\t\tret = blkdev_issue_flush(dev->bdev);\n\n\treturn ret;\n}\n\n \nstatic int dmz_write_dirty_mblocks(struct dmz_metadata *zmd,\n\t\t\t\t   struct list_head *write_list,\n\t\t\t\t   unsigned int set)\n{\n\tstruct dmz_mblock *mblk;\n\tstruct dmz_dev *dev = zmd->sb[set].dev;\n\tstruct blk_plug plug;\n\tint ret = 0, nr_mblks_submitted = 0;\n\n\t \n\tblk_start_plug(&plug);\n\tlist_for_each_entry(mblk, write_list, link) {\n\t\tret = dmz_write_mblock(zmd, mblk, set);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tnr_mblks_submitted++;\n\t}\n\tblk_finish_plug(&plug);\n\n\t \n\tlist_for_each_entry(mblk, write_list, link) {\n\t\tif (!nr_mblks_submitted)\n\t\t\tbreak;\n\t\twait_on_bit_io(&mblk->state, DMZ_META_WRITING,\n\t\t\t       TASK_UNINTERRUPTIBLE);\n\t\tif (test_bit(DMZ_META_ERROR, &mblk->state)) {\n\t\t\tclear_bit(DMZ_META_ERROR, &mblk->state);\n\t\t\tdmz_check_bdev(dev);\n\t\t\tret = -EIO;\n\t\t}\n\t\tnr_mblks_submitted--;\n\t}\n\n\t \n\tif (ret == 0)\n\t\tret = blkdev_issue_flush(dev->bdev);\n\n\treturn ret;\n}\n\n \nstatic int dmz_log_dirty_mblocks(struct dmz_metadata *zmd,\n\t\t\t\t struct list_head *write_list)\n{\n\tunsigned int log_set = zmd->mblk_primary ^ 0x1;\n\tint ret;\n\n\t \n\tret = dmz_write_dirty_mblocks(zmd, write_list, log_set);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tret = dmz_write_sb(zmd, log_set);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\n \nint dmz_flush_metadata(struct dmz_metadata *zmd)\n{\n\tstruct dmz_mblock *mblk;\n\tstruct list_head write_list;\n\tstruct dmz_dev *dev;\n\tint ret;\n\n\tif (WARN_ON(!zmd))\n\t\treturn 0;\n\n\tINIT_LIST_HEAD(&write_list);\n\n\t \n\tdown_write(&zmd->mblk_sem);\n\tdev = zmd->sb[zmd->mblk_primary].dev;\n\n\t \n\tdmz_lock_flush(zmd);\n\n\tif (dmz_bdev_is_dying(dev)) {\n\t\tret = -EIO;\n\t\tgoto out;\n\t}\n\n\t \n\tspin_lock(&zmd->mblk_lock);\n\tlist_splice_init(&zmd->mblk_dirty_list, &write_list);\n\tspin_unlock(&zmd->mblk_lock);\n\n\t \n\tif (list_empty(&write_list)) {\n\t\tret = blkdev_issue_flush(dev->bdev);\n\t\tgoto err;\n\t}\n\n\t \n\tret = dmz_log_dirty_mblocks(zmd, &write_list);\n\tif (ret)\n\t\tgoto err;\n\n\t \n\tret = dmz_write_dirty_mblocks(zmd, &write_list, zmd->mblk_primary);\n\tif (ret)\n\t\tgoto err;\n\n\tret = dmz_write_sb(zmd, zmd->mblk_primary);\n\tif (ret)\n\t\tgoto err;\n\n\twhile (!list_empty(&write_list)) {\n\t\tmblk = list_first_entry(&write_list, struct dmz_mblock, link);\n\t\tlist_del_init(&mblk->link);\n\n\t\tspin_lock(&zmd->mblk_lock);\n\t\tclear_bit(DMZ_META_DIRTY, &mblk->state);\n\t\tif (mblk->ref == 0)\n\t\t\tlist_add_tail(&mblk->link, &zmd->mblk_lru_list);\n\t\tspin_unlock(&zmd->mblk_lock);\n\t}\n\n\tzmd->sb_gen++;\nout:\n\tdmz_unlock_flush(zmd);\n\tup_write(&zmd->mblk_sem);\n\n\treturn ret;\n\nerr:\n\tif (!list_empty(&write_list)) {\n\t\tspin_lock(&zmd->mblk_lock);\n\t\tlist_splice(&write_list, &zmd->mblk_dirty_list);\n\t\tspin_unlock(&zmd->mblk_lock);\n\t}\n\tif (!dmz_check_bdev(dev))\n\t\tret = -EIO;\n\tgoto out;\n}\n\n \nstatic int dmz_check_sb(struct dmz_metadata *zmd, struct dmz_sb *dsb,\n\t\t\tbool tertiary)\n{\n\tstruct dmz_super *sb = dsb->sb;\n\tstruct dmz_dev *dev = dsb->dev;\n\tunsigned int nr_meta_zones, nr_data_zones;\n\tu32 crc, stored_crc;\n\tu64 gen, sb_block;\n\n\tif (le32_to_cpu(sb->magic) != DMZ_MAGIC) {\n\t\tdmz_dev_err(dev, \"Invalid meta magic (needed 0x%08x, got 0x%08x)\",\n\t\t\t    DMZ_MAGIC, le32_to_cpu(sb->magic));\n\t\treturn -ENXIO;\n\t}\n\n\tzmd->sb_version = le32_to_cpu(sb->version);\n\tif (zmd->sb_version > DMZ_META_VER) {\n\t\tdmz_dev_err(dev, \"Invalid meta version (needed %d, got %d)\",\n\t\t\t    DMZ_META_VER, zmd->sb_version);\n\t\treturn -EINVAL;\n\t}\n\tif (zmd->sb_version < 2 && tertiary) {\n\t\tdmz_dev_err(dev, \"Tertiary superblocks are not supported\");\n\t\treturn -EINVAL;\n\t}\n\n\tgen = le64_to_cpu(sb->gen);\n\tstored_crc = le32_to_cpu(sb->crc);\n\tsb->crc = 0;\n\tcrc = crc32_le(gen, (unsigned char *)sb, DMZ_BLOCK_SIZE);\n\tif (crc != stored_crc) {\n\t\tdmz_dev_err(dev, \"Invalid checksum (needed 0x%08x, got 0x%08x)\",\n\t\t\t    crc, stored_crc);\n\t\treturn -ENXIO;\n\t}\n\n\tsb_block = le64_to_cpu(sb->sb_block);\n\tif (sb_block != (u64)dsb->zone->id << zmd->zone_nr_blocks_shift) {\n\t\tdmz_dev_err(dev, \"Invalid superblock position (is %llu expected %llu)\",\n\t\t\t    sb_block, (u64)dsb->zone->id << zmd->zone_nr_blocks_shift);\n\t\treturn -EINVAL;\n\t}\n\tif (zmd->sb_version > 1) {\n\t\tuuid_t sb_uuid;\n\n\t\timport_uuid(&sb_uuid, sb->dmz_uuid);\n\t\tif (uuid_is_null(&sb_uuid)) {\n\t\t\tdmz_dev_err(dev, \"NULL DM-Zoned uuid\");\n\t\t\treturn -ENXIO;\n\t\t} else if (uuid_is_null(&zmd->uuid)) {\n\t\t\tuuid_copy(&zmd->uuid, &sb_uuid);\n\t\t} else if (!uuid_equal(&zmd->uuid, &sb_uuid)) {\n\t\t\tdmz_dev_err(dev, \"mismatching DM-Zoned uuid, is %pUl expected %pUl\",\n\t\t\t\t    &sb_uuid, &zmd->uuid);\n\t\t\treturn -ENXIO;\n\t\t}\n\t\tif (!strlen(zmd->label))\n\t\t\tmemcpy(zmd->label, sb->dmz_label, BDEVNAME_SIZE);\n\t\telse if (memcmp(zmd->label, sb->dmz_label, BDEVNAME_SIZE)) {\n\t\t\tdmz_dev_err(dev, \"mismatching DM-Zoned label, is %s expected %s\",\n\t\t\t\t    sb->dmz_label, zmd->label);\n\t\t\treturn -ENXIO;\n\t\t}\n\t\timport_uuid(&dev->uuid, sb->dev_uuid);\n\t\tif (uuid_is_null(&dev->uuid)) {\n\t\t\tdmz_dev_err(dev, \"NULL device uuid\");\n\t\t\treturn -ENXIO;\n\t\t}\n\n\t\tif (tertiary) {\n\t\t\t \n\t\t\tif (gen != 0)\n\t\t\t\tdmz_dev_warn(dev, \"Invalid generation %llu\",\n\t\t\t\t\t    gen);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tnr_meta_zones = (le32_to_cpu(sb->nr_meta_blocks) + zmd->zone_nr_blocks - 1)\n\t\t>> zmd->zone_nr_blocks_shift;\n\tif (!nr_meta_zones ||\n\t    (zmd->nr_devs <= 1 && nr_meta_zones >= zmd->nr_rnd_zones) ||\n\t    (zmd->nr_devs > 1 && nr_meta_zones >= zmd->nr_cache_zones)) {\n\t\tdmz_dev_err(dev, \"Invalid number of metadata blocks\");\n\t\treturn -ENXIO;\n\t}\n\n\tif (!le32_to_cpu(sb->nr_reserved_seq) ||\n\t    le32_to_cpu(sb->nr_reserved_seq) >= (zmd->nr_useable_zones - nr_meta_zones)) {\n\t\tdmz_dev_err(dev, \"Invalid number of reserved sequential zones\");\n\t\treturn -ENXIO;\n\t}\n\n\tnr_data_zones = zmd->nr_useable_zones -\n\t\t(nr_meta_zones * 2 + le32_to_cpu(sb->nr_reserved_seq));\n\tif (le32_to_cpu(sb->nr_chunks) > nr_data_zones) {\n\t\tdmz_dev_err(dev, \"Invalid number of chunks %u / %u\",\n\t\t\t    le32_to_cpu(sb->nr_chunks), nr_data_zones);\n\t\treturn -ENXIO;\n\t}\n\n\t \n\tzmd->nr_meta_blocks = le32_to_cpu(sb->nr_meta_blocks);\n\tzmd->nr_reserved_seq = le32_to_cpu(sb->nr_reserved_seq);\n\tzmd->nr_chunks = le32_to_cpu(sb->nr_chunks);\n\tzmd->nr_map_blocks = le32_to_cpu(sb->nr_map_blocks);\n\tzmd->nr_bitmap_blocks = le32_to_cpu(sb->nr_bitmap_blocks);\n\tzmd->nr_meta_zones = nr_meta_zones;\n\tzmd->nr_data_zones = nr_data_zones;\n\n\treturn 0;\n}\n\n \nstatic int dmz_read_sb(struct dmz_metadata *zmd, struct dmz_sb *sb, int set)\n{\n\tdmz_zmd_debug(zmd, \"read superblock set %d dev %pg block %llu\",\n\t\t      set, sb->dev->bdev, sb->block);\n\n\treturn dmz_rdwr_block(sb->dev, REQ_OP_READ,\n\t\t\t      sb->block, sb->mblk->page);\n}\n\n \nstatic int dmz_lookup_secondary_sb(struct dmz_metadata *zmd)\n{\n\tunsigned int zone_nr_blocks = zmd->zone_nr_blocks;\n\tstruct dmz_mblock *mblk;\n\tunsigned int zone_id = zmd->sb[0].zone->id;\n\tint i;\n\n\t \n\tmblk = dmz_alloc_mblock(zmd, 0);\n\tif (!mblk)\n\t\treturn -ENOMEM;\n\n\tzmd->sb[1].mblk = mblk;\n\tzmd->sb[1].sb = mblk->data;\n\n\t \n\tzmd->sb[1].block = zmd->sb[0].block + zone_nr_blocks;\n\tzmd->sb[1].zone = dmz_get(zmd, zone_id + 1);\n\tzmd->sb[1].dev = zmd->sb[0].dev;\n\tfor (i = 1; i < zmd->nr_rnd_zones; i++) {\n\t\tif (dmz_read_sb(zmd, &zmd->sb[1], 1) != 0)\n\t\t\tbreak;\n\t\tif (le32_to_cpu(zmd->sb[1].sb->magic) == DMZ_MAGIC)\n\t\t\treturn 0;\n\t\tzmd->sb[1].block += zone_nr_blocks;\n\t\tzmd->sb[1].zone = dmz_get(zmd, zone_id + i);\n\t}\n\n\tdmz_free_mblock(zmd, mblk);\n\tzmd->sb[1].mblk = NULL;\n\tzmd->sb[1].zone = NULL;\n\tzmd->sb[1].dev = NULL;\n\n\treturn -EIO;\n}\n\n \nstatic int dmz_get_sb(struct dmz_metadata *zmd, struct dmz_sb *sb, int set)\n{\n\tstruct dmz_mblock *mblk;\n\tint ret;\n\n\t \n\tmblk = dmz_alloc_mblock(zmd, 0);\n\tif (!mblk)\n\t\treturn -ENOMEM;\n\n\tsb->mblk = mblk;\n\tsb->sb = mblk->data;\n\n\t \n\tret = dmz_read_sb(zmd, sb, set);\n\tif (ret) {\n\t\tdmz_free_mblock(zmd, mblk);\n\t\tsb->mblk = NULL;\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int dmz_recover_mblocks(struct dmz_metadata *zmd, unsigned int dst_set)\n{\n\tunsigned int src_set = dst_set ^ 0x1;\n\tstruct page *page;\n\tint i, ret;\n\n\tdmz_dev_warn(zmd->sb[dst_set].dev,\n\t\t     \"Metadata set %u invalid: recovering\", dst_set);\n\n\tif (dst_set == 0)\n\t\tzmd->sb[0].block = dmz_start_block(zmd, zmd->sb[0].zone);\n\telse\n\t\tzmd->sb[1].block = dmz_start_block(zmd, zmd->sb[1].zone);\n\n\tpage = alloc_page(GFP_NOIO);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (i = 1; i < zmd->nr_meta_blocks; i++) {\n\t\tret = dmz_rdwr_block(zmd->sb[src_set].dev, REQ_OP_READ,\n\t\t\t\t     zmd->sb[src_set].block + i, page);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tret = dmz_rdwr_block(zmd->sb[dst_set].dev, REQ_OP_WRITE,\n\t\t\t\t     zmd->sb[dst_set].block + i, page);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\t \n\tif (!zmd->sb[dst_set].mblk) {\n\t\tzmd->sb[dst_set].mblk = dmz_alloc_mblock(zmd, 0);\n\t\tif (!zmd->sb[dst_set].mblk) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tzmd->sb[dst_set].sb = zmd->sb[dst_set].mblk->data;\n\t}\n\n\tret = dmz_write_sb(zmd, dst_set);\nout:\n\t__free_pages(page, 0);\n\n\treturn ret;\n}\n\n \nstatic int dmz_load_sb(struct dmz_metadata *zmd)\n{\n\tbool sb_good[2] = {false, false};\n\tu64 sb_gen[2] = {0, 0};\n\tint ret;\n\n\tif (!zmd->sb[0].zone) {\n\t\tdmz_zmd_err(zmd, \"Primary super block zone not set\");\n\t\treturn -ENXIO;\n\t}\n\n\t \n\tzmd->sb[0].block = dmz_start_block(zmd, zmd->sb[0].zone);\n\tzmd->sb[0].dev = zmd->sb[0].zone->dev;\n\tret = dmz_get_sb(zmd, &zmd->sb[0], 0);\n\tif (ret) {\n\t\tdmz_dev_err(zmd->sb[0].dev, \"Read primary super block failed\");\n\t\treturn ret;\n\t}\n\n\tret = dmz_check_sb(zmd, &zmd->sb[0], false);\n\n\t \n\tif (ret == 0) {\n\t\tsb_good[0] = true;\n\t\tif (!zmd->sb[1].zone) {\n\t\t\tunsigned int zone_id =\n\t\t\t\tzmd->sb[0].zone->id + zmd->nr_meta_zones;\n\n\t\t\tzmd->sb[1].zone = dmz_get(zmd, zone_id);\n\t\t}\n\t\tzmd->sb[1].block = dmz_start_block(zmd, zmd->sb[1].zone);\n\t\tzmd->sb[1].dev = zmd->sb[0].dev;\n\t\tret = dmz_get_sb(zmd, &zmd->sb[1], 1);\n\t} else\n\t\tret = dmz_lookup_secondary_sb(zmd);\n\n\tif (ret) {\n\t\tdmz_dev_err(zmd->sb[1].dev, \"Read secondary super block failed\");\n\t\treturn ret;\n\t}\n\n\tret = dmz_check_sb(zmd, &zmd->sb[1], false);\n\tif (ret == 0)\n\t\tsb_good[1] = true;\n\n\t \n\tif (!sb_good[0] && !sb_good[1]) {\n\t\tdmz_zmd_err(zmd, \"No valid super block found\");\n\t\treturn -EIO;\n\t}\n\n\tif (sb_good[0])\n\t\tsb_gen[0] = le64_to_cpu(zmd->sb[0].sb->gen);\n\telse {\n\t\tret = dmz_recover_mblocks(zmd, 0);\n\t\tif (ret) {\n\t\t\tdmz_dev_err(zmd->sb[0].dev,\n\t\t\t\t    \"Recovery of superblock 0 failed\");\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\tif (sb_good[1])\n\t\tsb_gen[1] = le64_to_cpu(zmd->sb[1].sb->gen);\n\telse {\n\t\tret = dmz_recover_mblocks(zmd, 1);\n\n\t\tif (ret) {\n\t\t\tdmz_dev_err(zmd->sb[1].dev,\n\t\t\t\t    \"Recovery of superblock 1 failed\");\n\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\tif (sb_gen[0] >= sb_gen[1]) {\n\t\tzmd->sb_gen = sb_gen[0];\n\t\tzmd->mblk_primary = 0;\n\t} else {\n\t\tzmd->sb_gen = sb_gen[1];\n\t\tzmd->mblk_primary = 1;\n\t}\n\n\tdmz_dev_debug(zmd->sb[zmd->mblk_primary].dev,\n\t\t      \"Using super block %u (gen %llu)\",\n\t\t      zmd->mblk_primary, zmd->sb_gen);\n\n\tif (zmd->sb_version > 1) {\n\t\tint i;\n\t\tstruct dmz_sb *sb;\n\n\t\tsb = kzalloc(sizeof(struct dmz_sb), GFP_KERNEL);\n\t\tif (!sb)\n\t\t\treturn -ENOMEM;\n\t\tfor (i = 1; i < zmd->nr_devs; i++) {\n\t\t\tsb->block = 0;\n\t\t\tsb->zone = dmz_get(zmd, zmd->dev[i].zone_offset);\n\t\t\tsb->dev = &zmd->dev[i];\n\t\t\tif (!dmz_is_meta(sb->zone)) {\n\t\t\t\tdmz_dev_err(sb->dev,\n\t\t\t\t\t    \"Tertiary super block zone %u not marked as metadata zone\",\n\t\t\t\t\t    sb->zone->id);\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out_kfree;\n\t\t\t}\n\t\t\tret = dmz_get_sb(zmd, sb, i + 1);\n\t\t\tif (ret) {\n\t\t\t\tdmz_dev_err(sb->dev,\n\t\t\t\t\t    \"Read tertiary super block failed\");\n\t\t\t\tdmz_free_mblock(zmd, sb->mblk);\n\t\t\t\tgoto out_kfree;\n\t\t\t}\n\t\t\tret = dmz_check_sb(zmd, sb, true);\n\t\t\tdmz_free_mblock(zmd, sb->mblk);\n\t\t\tif (ret == -EINVAL)\n\t\t\t\tgoto out_kfree;\n\t\t}\nout_kfree:\n\t\tkfree(sb);\n\t}\n\treturn ret;\n}\n\n \nstatic int dmz_init_zone(struct blk_zone *blkz, unsigned int num, void *data)\n{\n\tstruct dmz_dev *dev = data;\n\tstruct dmz_metadata *zmd = dev->metadata;\n\tint idx = num + dev->zone_offset;\n\tstruct dm_zone *zone;\n\n\tzone = dmz_insert(zmd, idx, dev);\n\tif (IS_ERR(zone))\n\t\treturn PTR_ERR(zone);\n\n\tif (blkz->len != zmd->zone_nr_sectors) {\n\t\tif (zmd->sb_version > 1) {\n\t\t\t \n\t\t\tset_bit(DMZ_OFFLINE, &zone->flags);\n\t\t\treturn 0;\n\t\t} else if (blkz->start + blkz->len == dev->capacity)\n\t\t\treturn 0;\n\t\treturn -ENXIO;\n\t}\n\n\t \n\tif (blkz->capacity != blkz->len)\n\t\treturn -ENXIO;\n\n\tswitch (blkz->type) {\n\tcase BLK_ZONE_TYPE_CONVENTIONAL:\n\t\tset_bit(DMZ_RND, &zone->flags);\n\t\tbreak;\n\tcase BLK_ZONE_TYPE_SEQWRITE_REQ:\n\tcase BLK_ZONE_TYPE_SEQWRITE_PREF:\n\t\tset_bit(DMZ_SEQ, &zone->flags);\n\t\tbreak;\n\tdefault:\n\t\treturn -ENXIO;\n\t}\n\n\tif (dmz_is_rnd(zone))\n\t\tzone->wp_block = 0;\n\telse\n\t\tzone->wp_block = dmz_sect2blk(blkz->wp - blkz->start);\n\n\tif (blkz->cond == BLK_ZONE_COND_OFFLINE)\n\t\tset_bit(DMZ_OFFLINE, &zone->flags);\n\telse if (blkz->cond == BLK_ZONE_COND_READONLY)\n\t\tset_bit(DMZ_READ_ONLY, &zone->flags);\n\telse {\n\t\tzmd->nr_useable_zones++;\n\t\tif (dmz_is_rnd(zone)) {\n\t\t\tzmd->nr_rnd_zones++;\n\t\t\tif (zmd->nr_devs == 1 && !zmd->sb[0].zone) {\n\t\t\t\t \n\t\t\t\tzmd->sb[0].zone = zone;\n\t\t\t}\n\t\t}\n\t\tif (zmd->nr_devs > 1 && num == 0) {\n\t\t\t \n\t\t\tset_bit(DMZ_META, &zone->flags);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int dmz_emulate_zones(struct dmz_metadata *zmd, struct dmz_dev *dev)\n{\n\tint idx;\n\tsector_t zone_offset = 0;\n\n\tfor (idx = 0; idx < dev->nr_zones; idx++) {\n\t\tstruct dm_zone *zone;\n\n\t\tzone = dmz_insert(zmd, idx, dev);\n\t\tif (IS_ERR(zone))\n\t\t\treturn PTR_ERR(zone);\n\t\tset_bit(DMZ_CACHE, &zone->flags);\n\t\tzone->wp_block = 0;\n\t\tzmd->nr_cache_zones++;\n\t\tzmd->nr_useable_zones++;\n\t\tif (dev->capacity - zone_offset < zmd->zone_nr_sectors) {\n\t\t\t \n\t\t\tset_bit(DMZ_OFFLINE, &zone->flags);\n\t\t\tbreak;\n\t\t}\n\t\tzone_offset += zmd->zone_nr_sectors;\n\t}\n\treturn 0;\n}\n\n \nstatic void dmz_drop_zones(struct dmz_metadata *zmd)\n{\n\tint idx;\n\n\tfor (idx = 0; idx < zmd->nr_zones; idx++) {\n\t\tstruct dm_zone *zone = xa_load(&zmd->zones, idx);\n\n\t\tkfree(zone);\n\t\txa_erase(&zmd->zones, idx);\n\t}\n\txa_destroy(&zmd->zones);\n}\n\n \nstatic int dmz_init_zones(struct dmz_metadata *zmd)\n{\n\tint i, ret;\n\tstruct dmz_dev *zoned_dev = &zmd->dev[0];\n\n\t \n\tzmd->zone_nr_sectors = zmd->dev[0].zone_nr_sectors;\n\tzmd->zone_nr_sectors_shift = ilog2(zmd->zone_nr_sectors);\n\tzmd->zone_nr_blocks = dmz_sect2blk(zmd->zone_nr_sectors);\n\tzmd->zone_nr_blocks_shift = ilog2(zmd->zone_nr_blocks);\n\tzmd->zone_bitmap_size = zmd->zone_nr_blocks >> 3;\n\tzmd->zone_nr_bitmap_blocks =\n\t\tmax_t(sector_t, 1, zmd->zone_bitmap_size >> DMZ_BLOCK_SHIFT);\n\tzmd->zone_bits_per_mblk = min_t(sector_t, zmd->zone_nr_blocks,\n\t\t\t\t\tDMZ_BLOCK_SIZE_BITS);\n\n\t \n\tzmd->nr_zones = 0;\n\tfor (i = 0; i < zmd->nr_devs; i++) {\n\t\tstruct dmz_dev *dev = &zmd->dev[i];\n\n\t\tdev->metadata = zmd;\n\t\tzmd->nr_zones += dev->nr_zones;\n\n\t\tatomic_set(&dev->unmap_nr_rnd, 0);\n\t\tINIT_LIST_HEAD(&dev->unmap_rnd_list);\n\t\tINIT_LIST_HEAD(&dev->map_rnd_list);\n\n\t\tatomic_set(&dev->unmap_nr_seq, 0);\n\t\tINIT_LIST_HEAD(&dev->unmap_seq_list);\n\t\tINIT_LIST_HEAD(&dev->map_seq_list);\n\t}\n\n\tif (!zmd->nr_zones) {\n\t\tDMERR(\"(%s): No zones found\", zmd->devname);\n\t\treturn -ENXIO;\n\t}\n\txa_init(&zmd->zones);\n\n\tDMDEBUG(\"(%s): Using %zu B for zone information\",\n\t\tzmd->devname, sizeof(struct dm_zone) * zmd->nr_zones);\n\n\tif (zmd->nr_devs > 1) {\n\t\tret = dmz_emulate_zones(zmd, &zmd->dev[0]);\n\t\tif (ret < 0) {\n\t\t\tDMDEBUG(\"(%s): Failed to emulate zones, error %d\",\n\t\t\t\tzmd->devname, ret);\n\t\t\tdmz_drop_zones(zmd);\n\t\t\treturn ret;\n\t\t}\n\n\t\t \n\t\tzmd->sb[0].zone = dmz_get(zmd, 0);\n\n\t\tfor (i = 1; i < zmd->nr_devs; i++) {\n\t\t\tzoned_dev = &zmd->dev[i];\n\n\t\t\tret = blkdev_report_zones(zoned_dev->bdev, 0,\n\t\t\t\t\t\t  BLK_ALL_ZONES,\n\t\t\t\t\t\t  dmz_init_zone, zoned_dev);\n\t\t\tif (ret < 0) {\n\t\t\t\tDMDEBUG(\"(%s): Failed to report zones, error %d\",\n\t\t\t\t\tzmd->devname, ret);\n\t\t\t\tdmz_drop_zones(zmd);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\t\treturn 0;\n\t}\n\n\t \n\tret = blkdev_report_zones(zoned_dev->bdev, 0, BLK_ALL_ZONES,\n\t\t\t\t  dmz_init_zone, zoned_dev);\n\tif (ret < 0) {\n\t\tDMDEBUG(\"(%s): Failed to report zones, error %d\",\n\t\t\tzmd->devname, ret);\n\t\tdmz_drop_zones(zmd);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int dmz_update_zone_cb(struct blk_zone *blkz, unsigned int idx,\n\t\t\t      void *data)\n{\n\tstruct dm_zone *zone = data;\n\n\tclear_bit(DMZ_OFFLINE, &zone->flags);\n\tclear_bit(DMZ_READ_ONLY, &zone->flags);\n\tif (blkz->cond == BLK_ZONE_COND_OFFLINE)\n\t\tset_bit(DMZ_OFFLINE, &zone->flags);\n\telse if (blkz->cond == BLK_ZONE_COND_READONLY)\n\t\tset_bit(DMZ_READ_ONLY, &zone->flags);\n\n\tif (dmz_is_seq(zone))\n\t\tzone->wp_block = dmz_sect2blk(blkz->wp - blkz->start);\n\telse\n\t\tzone->wp_block = 0;\n\treturn 0;\n}\n\n \nstatic int dmz_update_zone(struct dmz_metadata *zmd, struct dm_zone *zone)\n{\n\tstruct dmz_dev *dev = zone->dev;\n\tunsigned int noio_flag;\n\tint ret;\n\n\tif (dev->flags & DMZ_BDEV_REGULAR)\n\t\treturn 0;\n\n\t \n\tnoio_flag = memalloc_noio_save();\n\tret = blkdev_report_zones(dev->bdev, dmz_start_sect(zmd, zone), 1,\n\t\t\t\t  dmz_update_zone_cb, zone);\n\tmemalloc_noio_restore(noio_flag);\n\n\tif (ret == 0)\n\t\tret = -EIO;\n\tif (ret < 0) {\n\t\tdmz_dev_err(dev, \"Get zone %u report failed\",\n\t\t\t    zone->id);\n\t\tdmz_check_bdev(dev);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int dmz_handle_seq_write_err(struct dmz_metadata *zmd,\n\t\t\t\t    struct dm_zone *zone)\n{\n\tstruct dmz_dev *dev = zone->dev;\n\tunsigned int wp = 0;\n\tint ret;\n\n\twp = zone->wp_block;\n\tret = dmz_update_zone(zmd, zone);\n\tif (ret)\n\t\treturn ret;\n\n\tdmz_dev_warn(dev, \"Processing zone %u write error (zone wp %u/%u)\",\n\t\t     zone->id, zone->wp_block, wp);\n\n\tif (zone->wp_block < wp) {\n\t\tdmz_invalidate_blocks(zmd, zone, zone->wp_block,\n\t\t\t\t      wp - zone->wp_block);\n\t}\n\n\treturn 0;\n}\n\n \nstatic int dmz_reset_zone(struct dmz_metadata *zmd, struct dm_zone *zone)\n{\n\tint ret;\n\n\t \n\tif (dmz_is_offline(zone) ||\n\t    dmz_is_readonly(zone) ||\n\t    dmz_is_rnd(zone))\n\t\treturn 0;\n\n\tif (!dmz_is_empty(zone) || dmz_seq_write_err(zone)) {\n\t\tstruct dmz_dev *dev = zone->dev;\n\n\t\tret = blkdev_zone_mgmt(dev->bdev, REQ_OP_ZONE_RESET,\n\t\t\t\t       dmz_start_sect(zmd, zone),\n\t\t\t\t       zmd->zone_nr_sectors, GFP_NOIO);\n\t\tif (ret) {\n\t\t\tdmz_dev_err(dev, \"Reset zone %u failed %d\",\n\t\t\t\t    zone->id, ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t \n\tclear_bit(DMZ_SEQ_WRITE_ERR, &zone->flags);\n\tzone->wp_block = 0;\n\n\treturn 0;\n}\n\nstatic void dmz_get_zone_weight(struct dmz_metadata *zmd, struct dm_zone *zone);\n\n \nstatic int dmz_load_mapping(struct dmz_metadata *zmd)\n{\n\tstruct dm_zone *dzone, *bzone;\n\tstruct dmz_mblock *dmap_mblk = NULL;\n\tstruct dmz_map *dmap;\n\tunsigned int i = 0, e = 0, chunk = 0;\n\tunsigned int dzone_id;\n\tunsigned int bzone_id;\n\n\t \n\tzmd->map_mblk = kcalloc(zmd->nr_map_blocks,\n\t\t\t\tsizeof(struct dmz_mblk *), GFP_KERNEL);\n\tif (!zmd->map_mblk)\n\t\treturn -ENOMEM;\n\n\t \n\twhile (chunk < zmd->nr_chunks) {\n\t\tif (!dmap_mblk) {\n\t\t\t \n\t\t\tdmap_mblk = dmz_get_mblock(zmd, i + 1);\n\t\t\tif (IS_ERR(dmap_mblk))\n\t\t\t\treturn PTR_ERR(dmap_mblk);\n\t\t\tzmd->map_mblk[i] = dmap_mblk;\n\t\t\tdmap = dmap_mblk->data;\n\t\t\ti++;\n\t\t\te = 0;\n\t\t}\n\n\t\t \n\t\tdzone_id = le32_to_cpu(dmap[e].dzone_id);\n\t\tif (dzone_id == DMZ_MAP_UNMAPPED)\n\t\t\tgoto next;\n\n\t\tif (dzone_id >= zmd->nr_zones) {\n\t\t\tdmz_zmd_err(zmd, \"Chunk %u mapping: invalid data zone ID %u\",\n\t\t\t\t    chunk, dzone_id);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tdzone = dmz_get(zmd, dzone_id);\n\t\tif (!dzone) {\n\t\t\tdmz_zmd_err(zmd, \"Chunk %u mapping: data zone %u not present\",\n\t\t\t\t    chunk, dzone_id);\n\t\t\treturn -EIO;\n\t\t}\n\t\tset_bit(DMZ_DATA, &dzone->flags);\n\t\tdzone->chunk = chunk;\n\t\tdmz_get_zone_weight(zmd, dzone);\n\n\t\tif (dmz_is_cache(dzone))\n\t\t\tlist_add_tail(&dzone->link, &zmd->map_cache_list);\n\t\telse if (dmz_is_rnd(dzone))\n\t\t\tlist_add_tail(&dzone->link, &dzone->dev->map_rnd_list);\n\t\telse\n\t\t\tlist_add_tail(&dzone->link, &dzone->dev->map_seq_list);\n\n\t\t \n\t\tbzone_id = le32_to_cpu(dmap[e].bzone_id);\n\t\tif (bzone_id == DMZ_MAP_UNMAPPED)\n\t\t\tgoto next;\n\n\t\tif (bzone_id >= zmd->nr_zones) {\n\t\t\tdmz_zmd_err(zmd, \"Chunk %u mapping: invalid buffer zone ID %u\",\n\t\t\t\t    chunk, bzone_id);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tbzone = dmz_get(zmd, bzone_id);\n\t\tif (!bzone) {\n\t\t\tdmz_zmd_err(zmd, \"Chunk %u mapping: buffer zone %u not present\",\n\t\t\t\t    chunk, bzone_id);\n\t\t\treturn -EIO;\n\t\t}\n\t\tif (!dmz_is_rnd(bzone) && !dmz_is_cache(bzone)) {\n\t\t\tdmz_zmd_err(zmd, \"Chunk %u mapping: invalid buffer zone %u\",\n\t\t\t\t    chunk, bzone_id);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tset_bit(DMZ_DATA, &bzone->flags);\n\t\tset_bit(DMZ_BUF, &bzone->flags);\n\t\tbzone->chunk = chunk;\n\t\tbzone->bzone = dzone;\n\t\tdzone->bzone = bzone;\n\t\tdmz_get_zone_weight(zmd, bzone);\n\t\tif (dmz_is_cache(bzone))\n\t\t\tlist_add_tail(&bzone->link, &zmd->map_cache_list);\n\t\telse\n\t\t\tlist_add_tail(&bzone->link, &bzone->dev->map_rnd_list);\nnext:\n\t\tchunk++;\n\t\te++;\n\t\tif (e >= DMZ_MAP_ENTRIES)\n\t\t\tdmap_mblk = NULL;\n\t}\n\n\t \n\tfor (i = 0; i < zmd->nr_zones; i++) {\n\t\tdzone = dmz_get(zmd, i);\n\t\tif (!dzone)\n\t\t\tcontinue;\n\t\tif (dmz_is_meta(dzone))\n\t\t\tcontinue;\n\t\tif (dmz_is_offline(dzone))\n\t\t\tcontinue;\n\n\t\tif (dmz_is_cache(dzone))\n\t\t\tzmd->nr_cache++;\n\t\telse if (dmz_is_rnd(dzone))\n\t\t\tdzone->dev->nr_rnd++;\n\t\telse\n\t\t\tdzone->dev->nr_seq++;\n\n\t\tif (dmz_is_data(dzone)) {\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tset_bit(DMZ_DATA, &dzone->flags);\n\t\tdzone->chunk = DMZ_MAP_UNMAPPED;\n\t\tif (dmz_is_cache(dzone)) {\n\t\t\tlist_add_tail(&dzone->link, &zmd->unmap_cache_list);\n\t\t\tatomic_inc(&zmd->unmap_nr_cache);\n\t\t} else if (dmz_is_rnd(dzone)) {\n\t\t\tlist_add_tail(&dzone->link,\n\t\t\t\t      &dzone->dev->unmap_rnd_list);\n\t\t\tatomic_inc(&dzone->dev->unmap_nr_rnd);\n\t\t} else if (atomic_read(&zmd->nr_reserved_seq_zones) < zmd->nr_reserved_seq) {\n\t\t\tlist_add_tail(&dzone->link, &zmd->reserved_seq_zones_list);\n\t\t\tset_bit(DMZ_RESERVED, &dzone->flags);\n\t\t\tatomic_inc(&zmd->nr_reserved_seq_zones);\n\t\t\tdzone->dev->nr_seq--;\n\t\t} else {\n\t\t\tlist_add_tail(&dzone->link,\n\t\t\t\t      &dzone->dev->unmap_seq_list);\n\t\t\tatomic_inc(&dzone->dev->unmap_nr_seq);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic void dmz_set_chunk_mapping(struct dmz_metadata *zmd, unsigned int chunk,\n\t\t\t\t  unsigned int dzone_id, unsigned int bzone_id)\n{\n\tstruct dmz_mblock *dmap_mblk = zmd->map_mblk[chunk >> DMZ_MAP_ENTRIES_SHIFT];\n\tstruct dmz_map *dmap = dmap_mblk->data;\n\tint map_idx = chunk & DMZ_MAP_ENTRIES_MASK;\n\n\tdmap[map_idx].dzone_id = cpu_to_le32(dzone_id);\n\tdmap[map_idx].bzone_id = cpu_to_le32(bzone_id);\n\tdmz_dirty_mblock(zmd, dmap_mblk);\n}\n\n \nstatic void __dmz_lru_zone(struct dmz_metadata *zmd, struct dm_zone *zone)\n{\n\tif (list_empty(&zone->link))\n\t\treturn;\n\n\tlist_del_init(&zone->link);\n\tif (dmz_is_seq(zone)) {\n\t\t \n\t\tlist_add_tail(&zone->link, &zone->dev->map_seq_list);\n\t} else if (dmz_is_cache(zone)) {\n\t\t \n\t\tlist_add_tail(&zone->link, &zmd->map_cache_list);\n\t} else {\n\t\t \n\t\tlist_add_tail(&zone->link, &zone->dev->map_rnd_list);\n\t}\n}\n\n \nstatic void dmz_lru_zone(struct dmz_metadata *zmd, struct dm_zone *zone)\n{\n\t__dmz_lru_zone(zmd, zone);\n\tif (zone->bzone)\n\t\t__dmz_lru_zone(zmd, zone->bzone);\n}\n\n \nstatic void dmz_wait_for_free_zones(struct dmz_metadata *zmd)\n{\n\tDEFINE_WAIT(wait);\n\n\tprepare_to_wait(&zmd->free_wq, &wait, TASK_UNINTERRUPTIBLE);\n\tdmz_unlock_map(zmd);\n\tdmz_unlock_metadata(zmd);\n\n\tio_schedule_timeout(HZ);\n\n\tdmz_lock_metadata(zmd);\n\tdmz_lock_map(zmd);\n\tfinish_wait(&zmd->free_wq, &wait);\n}\n\n \nint dmz_lock_zone_reclaim(struct dm_zone *zone)\n{\n\t \n\tif (dmz_is_active(zone))\n\t\treturn 0;\n\n\treturn !test_and_set_bit(DMZ_RECLAIM, &zone->flags);\n}\n\n \nvoid dmz_unlock_zone_reclaim(struct dm_zone *zone)\n{\n\tWARN_ON(dmz_is_active(zone));\n\tWARN_ON(!dmz_in_reclaim(zone));\n\n\tclear_bit_unlock(DMZ_RECLAIM, &zone->flags);\n\tsmp_mb__after_atomic();\n\twake_up_bit(&zone->flags, DMZ_RECLAIM);\n}\n\n \nstatic void dmz_wait_for_reclaim(struct dmz_metadata *zmd, struct dm_zone *zone)\n{\n\tdmz_unlock_map(zmd);\n\tdmz_unlock_metadata(zmd);\n\tset_bit(DMZ_RECLAIM_TERMINATE, &zone->flags);\n\twait_on_bit_timeout(&zone->flags, DMZ_RECLAIM, TASK_UNINTERRUPTIBLE, HZ);\n\tclear_bit(DMZ_RECLAIM_TERMINATE, &zone->flags);\n\tdmz_lock_metadata(zmd);\n\tdmz_lock_map(zmd);\n}\n\n \nstatic struct dm_zone *dmz_get_rnd_zone_for_reclaim(struct dmz_metadata *zmd,\n\t\t\t\t\t\t    unsigned int idx, bool idle)\n{\n\tstruct dm_zone *dzone = NULL;\n\tstruct dm_zone *zone, *maxw_z = NULL;\n\tstruct list_head *zone_list;\n\n\t \n\tif (zmd->nr_cache) {\n\t\tzone_list = &zmd->map_cache_list;\n\t\t \n\t\tif (idle && list_empty(zone_list))\n\t\t\tzone_list = &zmd->dev[idx].map_rnd_list;\n\t} else\n\t\tzone_list = &zmd->dev[idx].map_rnd_list;\n\n\t \n\tlist_for_each_entry(zone, zone_list, link) {\n\t\tif (dmz_is_buf(zone)) {\n\t\t\tdzone = zone->bzone;\n\t\t\tif (dmz_is_rnd(dzone) && dzone->dev->dev_idx != idx)\n\t\t\t\tcontinue;\n\t\t\tif (!maxw_z || maxw_z->weight < dzone->weight)\n\t\t\t\tmaxw_z = dzone;\n\t\t} else {\n\t\t\tdzone = zone;\n\t\t\tif (dmz_lock_zone_reclaim(dzone))\n\t\t\t\treturn dzone;\n\t\t}\n\t}\n\n\tif (maxw_z && dmz_lock_zone_reclaim(maxw_z))\n\t\treturn maxw_z;\n\n\t \n\tlist_for_each_entry(zone, zone_list, link) {\n\t\tif (dmz_is_buf(zone)) {\n\t\t\tdzone = zone->bzone;\n\t\t\tif (dmz_is_rnd(dzone) && dzone->dev->dev_idx != idx)\n\t\t\t\tcontinue;\n\t\t} else\n\t\t\tdzone = zone;\n\t\tif (dmz_lock_zone_reclaim(dzone))\n\t\t\treturn dzone;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic struct dm_zone *dmz_get_seq_zone_for_reclaim(struct dmz_metadata *zmd,\n\t\t\t\t\t\t    unsigned int idx)\n{\n\tstruct dm_zone *zone;\n\n\tlist_for_each_entry(zone, &zmd->dev[idx].map_seq_list, link) {\n\t\tif (!zone->bzone)\n\t\t\tcontinue;\n\t\tif (dmz_lock_zone_reclaim(zone))\n\t\t\treturn zone;\n\t}\n\n\treturn NULL;\n}\n\n \nstruct dm_zone *dmz_get_zone_for_reclaim(struct dmz_metadata *zmd,\n\t\t\t\t\t unsigned int dev_idx, bool idle)\n{\n\tstruct dm_zone *zone = NULL;\n\n\t \n\tdmz_lock_map(zmd);\n\tif (list_empty(&zmd->reserved_seq_zones_list))\n\t\tzone = dmz_get_seq_zone_for_reclaim(zmd, dev_idx);\n\tif (!zone)\n\t\tzone = dmz_get_rnd_zone_for_reclaim(zmd, dev_idx, idle);\n\tdmz_unlock_map(zmd);\n\n\treturn zone;\n}\n\n \nstruct dm_zone *dmz_get_chunk_mapping(struct dmz_metadata *zmd,\n\t\t\t\t      unsigned int chunk, enum req_op op)\n{\n\tstruct dmz_mblock *dmap_mblk = zmd->map_mblk[chunk >> DMZ_MAP_ENTRIES_SHIFT];\n\tstruct dmz_map *dmap = dmap_mblk->data;\n\tint dmap_idx = chunk & DMZ_MAP_ENTRIES_MASK;\n\tunsigned int dzone_id;\n\tstruct dm_zone *dzone = NULL;\n\tint ret = 0;\n\tint alloc_flags = zmd->nr_cache ? DMZ_ALLOC_CACHE : DMZ_ALLOC_RND;\n\n\tdmz_lock_map(zmd);\nagain:\n\t \n\tdzone_id = le32_to_cpu(dmap[dmap_idx].dzone_id);\n\tif (dzone_id == DMZ_MAP_UNMAPPED) {\n\t\t \n\t\tif (op != REQ_OP_WRITE)\n\t\t\tgoto out;\n\n\t\t \n\t\tdzone = dmz_alloc_zone(zmd, 0, alloc_flags);\n\t\tif (!dzone) {\n\t\t\tif (dmz_dev_is_dying(zmd)) {\n\t\t\t\tdzone = ERR_PTR(-EIO);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tdmz_wait_for_free_zones(zmd);\n\t\t\tgoto again;\n\t\t}\n\n\t\tdmz_map_zone(zmd, dzone, chunk);\n\n\t} else {\n\t\t \n\t\tdzone = dmz_get(zmd, dzone_id);\n\t\tif (!dzone) {\n\t\t\tdzone = ERR_PTR(-EIO);\n\t\t\tgoto out;\n\t\t}\n\t\tif (dzone->chunk != chunk) {\n\t\t\tdzone = ERR_PTR(-EIO);\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tif (dmz_seq_write_err(dzone)) {\n\t\t\tret = dmz_handle_seq_write_err(zmd, dzone);\n\t\t\tif (ret) {\n\t\t\t\tdzone = ERR_PTR(-EIO);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tclear_bit(DMZ_SEQ_WRITE_ERR, &dzone->flags);\n\t\t}\n\t}\n\n\t \n\tif (dmz_in_reclaim(dzone)) {\n\t\tdmz_wait_for_reclaim(zmd, dzone);\n\t\tgoto again;\n\t}\n\tdmz_activate_zone(dzone);\n\tdmz_lru_zone(zmd, dzone);\nout:\n\tdmz_unlock_map(zmd);\n\n\treturn dzone;\n}\n\n \nvoid dmz_put_chunk_mapping(struct dmz_metadata *zmd, struct dm_zone *dzone)\n{\n\tstruct dm_zone *bzone;\n\n\tdmz_lock_map(zmd);\n\n\tbzone = dzone->bzone;\n\tif (bzone) {\n\t\tif (dmz_weight(bzone))\n\t\t\tdmz_lru_zone(zmd, bzone);\n\t\telse {\n\t\t\t \n\t\t\tdmz_unmap_zone(zmd, bzone);\n\t\t\tdmz_free_zone(zmd, bzone);\n\t\t\tbzone = NULL;\n\t\t}\n\t}\n\n\t \n\tdmz_deactivate_zone(dzone);\n\tif (dmz_is_active(dzone) || bzone || dmz_weight(dzone))\n\t\tdmz_lru_zone(zmd, dzone);\n\telse {\n\t\t \n\t\tdmz_unmap_zone(zmd, dzone);\n\t\tdmz_free_zone(zmd, dzone);\n\t}\n\n\tdmz_unlock_map(zmd);\n}\n\n \nstruct dm_zone *dmz_get_chunk_buffer(struct dmz_metadata *zmd,\n\t\t\t\t     struct dm_zone *dzone)\n{\n\tstruct dm_zone *bzone;\n\tint alloc_flags = zmd->nr_cache ? DMZ_ALLOC_CACHE : DMZ_ALLOC_RND;\n\n\tdmz_lock_map(zmd);\nagain:\n\tbzone = dzone->bzone;\n\tif (bzone)\n\t\tgoto out;\n\n\t \n\tbzone = dmz_alloc_zone(zmd, 0, alloc_flags);\n\tif (!bzone) {\n\t\tif (dmz_dev_is_dying(zmd)) {\n\t\t\tbzone = ERR_PTR(-EIO);\n\t\t\tgoto out;\n\t\t}\n\t\tdmz_wait_for_free_zones(zmd);\n\t\tgoto again;\n\t}\n\n\t \n\tdmz_set_chunk_mapping(zmd, dzone->chunk, dzone->id, bzone->id);\n\n\tset_bit(DMZ_BUF, &bzone->flags);\n\tbzone->chunk = dzone->chunk;\n\tbzone->bzone = dzone;\n\tdzone->bzone = bzone;\n\tif (dmz_is_cache(bzone))\n\t\tlist_add_tail(&bzone->link, &zmd->map_cache_list);\n\telse\n\t\tlist_add_tail(&bzone->link, &bzone->dev->map_rnd_list);\nout:\n\tdmz_unlock_map(zmd);\n\n\treturn bzone;\n}\n\n \nstruct dm_zone *dmz_alloc_zone(struct dmz_metadata *zmd, unsigned int dev_idx,\n\t\t\t       unsigned long flags)\n{\n\tstruct list_head *list;\n\tstruct dm_zone *zone;\n\tint i;\n\n\t \n\tif (!(flags & DMZ_ALLOC_RECLAIM)) {\n\t\tfor (i = 0; i < zmd->nr_devs; i++)\n\t\t\tdmz_schedule_reclaim(zmd->dev[i].reclaim);\n\t}\n\n\ti = 0;\nagain:\n\tif (flags & DMZ_ALLOC_CACHE)\n\t\tlist = &zmd->unmap_cache_list;\n\telse if (flags & DMZ_ALLOC_RND)\n\t\tlist = &zmd->dev[dev_idx].unmap_rnd_list;\n\telse\n\t\tlist = &zmd->dev[dev_idx].unmap_seq_list;\n\n\tif (list_empty(list)) {\n\t\t \n\t\tif (!(flags & DMZ_ALLOC_RECLAIM))\n\t\t\treturn NULL;\n\t\t \n\t\tif (i < zmd->nr_devs) {\n\t\t\tdev_idx = (dev_idx + 1) % zmd->nr_devs;\n\t\t\ti++;\n\t\t\tgoto again;\n\t\t}\n\n\t\t \n\t\tzone = list_first_entry_or_null(&zmd->reserved_seq_zones_list,\n\t\t\t\t\t\tstruct dm_zone, link);\n\t\tif (zone) {\n\t\t\tlist_del_init(&zone->link);\n\t\t\tatomic_dec(&zmd->nr_reserved_seq_zones);\n\t\t}\n\t\treturn zone;\n\t}\n\n\tzone = list_first_entry(list, struct dm_zone, link);\n\tlist_del_init(&zone->link);\n\n\tif (dmz_is_cache(zone))\n\t\tatomic_dec(&zmd->unmap_nr_cache);\n\telse if (dmz_is_rnd(zone))\n\t\tatomic_dec(&zone->dev->unmap_nr_rnd);\n\telse\n\t\tatomic_dec(&zone->dev->unmap_nr_seq);\n\n\tif (dmz_is_offline(zone)) {\n\t\tdmz_zmd_warn(zmd, \"Zone %u is offline\", zone->id);\n\t\tzone = NULL;\n\t\tgoto again;\n\t}\n\tif (dmz_is_meta(zone)) {\n\t\tdmz_zmd_warn(zmd, \"Zone %u has metadata\", zone->id);\n\t\tzone = NULL;\n\t\tgoto again;\n\t}\n\treturn zone;\n}\n\n \nvoid dmz_free_zone(struct dmz_metadata *zmd, struct dm_zone *zone)\n{\n\t \n\tif (dmz_is_seq(zone))\n\t\tdmz_reset_zone(zmd, zone);\n\n\t \n\tif (dmz_is_cache(zone)) {\n\t\tlist_add_tail(&zone->link, &zmd->unmap_cache_list);\n\t\tatomic_inc(&zmd->unmap_nr_cache);\n\t} else if (dmz_is_rnd(zone)) {\n\t\tlist_add_tail(&zone->link, &zone->dev->unmap_rnd_list);\n\t\tatomic_inc(&zone->dev->unmap_nr_rnd);\n\t} else if (dmz_is_reserved(zone)) {\n\t\tlist_add_tail(&zone->link, &zmd->reserved_seq_zones_list);\n\t\tatomic_inc(&zmd->nr_reserved_seq_zones);\n\t} else {\n\t\tlist_add_tail(&zone->link, &zone->dev->unmap_seq_list);\n\t\tatomic_inc(&zone->dev->unmap_nr_seq);\n\t}\n\n\twake_up_all(&zmd->free_wq);\n}\n\n \nvoid dmz_map_zone(struct dmz_metadata *zmd, struct dm_zone *dzone,\n\t\t  unsigned int chunk)\n{\n\t \n\tdmz_set_chunk_mapping(zmd, chunk, dzone->id,\n\t\t\t      DMZ_MAP_UNMAPPED);\n\tdzone->chunk = chunk;\n\tif (dmz_is_cache(dzone))\n\t\tlist_add_tail(&dzone->link, &zmd->map_cache_list);\n\telse if (dmz_is_rnd(dzone))\n\t\tlist_add_tail(&dzone->link, &dzone->dev->map_rnd_list);\n\telse\n\t\tlist_add_tail(&dzone->link, &dzone->dev->map_seq_list);\n}\n\n \nvoid dmz_unmap_zone(struct dmz_metadata *zmd, struct dm_zone *zone)\n{\n\tunsigned int chunk = zone->chunk;\n\tunsigned int dzone_id;\n\n\tif (chunk == DMZ_MAP_UNMAPPED) {\n\t\t \n\t\treturn;\n\t}\n\n\tif (test_and_clear_bit(DMZ_BUF, &zone->flags)) {\n\t\t \n\t\tdzone_id = zone->bzone->id;\n\t\tzone->bzone->bzone = NULL;\n\t\tzone->bzone = NULL;\n\n\t} else {\n\t\t \n\t\tif (WARN_ON(zone->bzone)) {\n\t\t\tzone->bzone->bzone = NULL;\n\t\t\tzone->bzone = NULL;\n\t\t}\n\t\tdzone_id = DMZ_MAP_UNMAPPED;\n\t}\n\n\tdmz_set_chunk_mapping(zmd, chunk, dzone_id, DMZ_MAP_UNMAPPED);\n\n\tzone->chunk = DMZ_MAP_UNMAPPED;\n\tlist_del_init(&zone->link);\n}\n\n \nstatic unsigned int dmz_set_bits(unsigned long *bitmap,\n\t\t\t\t unsigned int bit, unsigned int nr_bits)\n{\n\tunsigned long *addr;\n\tunsigned int end = bit + nr_bits;\n\tunsigned int n = 0;\n\n\twhile (bit < end) {\n\t\tif (((bit & (BITS_PER_LONG - 1)) == 0) &&\n\t\t    ((end - bit) >= BITS_PER_LONG)) {\n\t\t\t \n\t\t\taddr = bitmap + BIT_WORD(bit);\n\t\t\tif (*addr == 0) {\n\t\t\t\t*addr = ULONG_MAX;\n\t\t\t\tn += BITS_PER_LONG;\n\t\t\t\tbit += BITS_PER_LONG;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (!test_and_set_bit(bit, bitmap))\n\t\t\tn++;\n\t\tbit++;\n\t}\n\n\treturn n;\n}\n\n \nstatic struct dmz_mblock *dmz_get_bitmap(struct dmz_metadata *zmd,\n\t\t\t\t\t struct dm_zone *zone,\n\t\t\t\t\t sector_t chunk_block)\n{\n\tsector_t bitmap_block = 1 + zmd->nr_map_blocks +\n\t\t(sector_t)(zone->id * zmd->zone_nr_bitmap_blocks) +\n\t\t(chunk_block >> DMZ_BLOCK_SHIFT_BITS);\n\n\treturn dmz_get_mblock(zmd, bitmap_block);\n}\n\n \nint dmz_copy_valid_blocks(struct dmz_metadata *zmd, struct dm_zone *from_zone,\n\t\t\t  struct dm_zone *to_zone)\n{\n\tstruct dmz_mblock *from_mblk, *to_mblk;\n\tsector_t chunk_block = 0;\n\n\t \n\twhile (chunk_block < zmd->zone_nr_blocks) {\n\t\tfrom_mblk = dmz_get_bitmap(zmd, from_zone, chunk_block);\n\t\tif (IS_ERR(from_mblk))\n\t\t\treturn PTR_ERR(from_mblk);\n\t\tto_mblk = dmz_get_bitmap(zmd, to_zone, chunk_block);\n\t\tif (IS_ERR(to_mblk)) {\n\t\t\tdmz_release_mblock(zmd, from_mblk);\n\t\t\treturn PTR_ERR(to_mblk);\n\t\t}\n\n\t\tmemcpy(to_mblk->data, from_mblk->data, DMZ_BLOCK_SIZE);\n\t\tdmz_dirty_mblock(zmd, to_mblk);\n\n\t\tdmz_release_mblock(zmd, to_mblk);\n\t\tdmz_release_mblock(zmd, from_mblk);\n\n\t\tchunk_block += zmd->zone_bits_per_mblk;\n\t}\n\n\tto_zone->weight = from_zone->weight;\n\n\treturn 0;\n}\n\n \nint dmz_merge_valid_blocks(struct dmz_metadata *zmd, struct dm_zone *from_zone,\n\t\t\t   struct dm_zone *to_zone, sector_t chunk_block)\n{\n\tunsigned int nr_blocks;\n\tint ret;\n\n\t \n\twhile (chunk_block < zmd->zone_nr_blocks) {\n\t\t \n\t\tret = dmz_first_valid_block(zmd, from_zone, &chunk_block);\n\t\tif (ret <= 0)\n\t\t\treturn ret;\n\n\t\tnr_blocks = ret;\n\t\tret = dmz_validate_blocks(zmd, to_zone, chunk_block, nr_blocks);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tchunk_block += nr_blocks;\n\t}\n\n\treturn 0;\n}\n\n \nint dmz_validate_blocks(struct dmz_metadata *zmd, struct dm_zone *zone,\n\t\t\tsector_t chunk_block, unsigned int nr_blocks)\n{\n\tunsigned int count, bit, nr_bits;\n\tunsigned int zone_nr_blocks = zmd->zone_nr_blocks;\n\tstruct dmz_mblock *mblk;\n\tunsigned int n = 0;\n\n\tdmz_zmd_debug(zmd, \"=> VALIDATE zone %u, block %llu, %u blocks\",\n\t\t      zone->id, (unsigned long long)chunk_block,\n\t\t      nr_blocks);\n\n\tWARN_ON(chunk_block + nr_blocks > zone_nr_blocks);\n\n\twhile (nr_blocks) {\n\t\t \n\t\tmblk = dmz_get_bitmap(zmd, zone, chunk_block);\n\t\tif (IS_ERR(mblk))\n\t\t\treturn PTR_ERR(mblk);\n\n\t\t \n\t\tbit = chunk_block & DMZ_BLOCK_MASK_BITS;\n\t\tnr_bits = min(nr_blocks, zmd->zone_bits_per_mblk - bit);\n\n\t\tcount = dmz_set_bits((unsigned long *)mblk->data, bit, nr_bits);\n\t\tif (count) {\n\t\t\tdmz_dirty_mblock(zmd, mblk);\n\t\t\tn += count;\n\t\t}\n\t\tdmz_release_mblock(zmd, mblk);\n\n\t\tnr_blocks -= nr_bits;\n\t\tchunk_block += nr_bits;\n\t}\n\n\tif (likely(zone->weight + n <= zone_nr_blocks))\n\t\tzone->weight += n;\n\telse {\n\t\tdmz_zmd_warn(zmd, \"Zone %u: weight %u should be <= %u\",\n\t\t\t     zone->id, zone->weight,\n\t\t\t     zone_nr_blocks - n);\n\t\tzone->weight = zone_nr_blocks;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int dmz_clear_bits(unsigned long *bitmap, int bit, int nr_bits)\n{\n\tunsigned long *addr;\n\tint end = bit + nr_bits;\n\tint n = 0;\n\n\twhile (bit < end) {\n\t\tif (((bit & (BITS_PER_LONG - 1)) == 0) &&\n\t\t    ((end - bit) >= BITS_PER_LONG)) {\n\t\t\t \n\t\t\taddr = bitmap + BIT_WORD(bit);\n\t\t\tif (*addr == ULONG_MAX) {\n\t\t\t\t*addr = 0;\n\t\t\t\tn += BITS_PER_LONG;\n\t\t\t\tbit += BITS_PER_LONG;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (test_and_clear_bit(bit, bitmap))\n\t\t\tn++;\n\t\tbit++;\n\t}\n\n\treturn n;\n}\n\n \nint dmz_invalidate_blocks(struct dmz_metadata *zmd, struct dm_zone *zone,\n\t\t\t  sector_t chunk_block, unsigned int nr_blocks)\n{\n\tunsigned int count, bit, nr_bits;\n\tstruct dmz_mblock *mblk;\n\tunsigned int n = 0;\n\n\tdmz_zmd_debug(zmd, \"=> INVALIDATE zone %u, block %llu, %u blocks\",\n\t\t      zone->id, (u64)chunk_block, nr_blocks);\n\n\tWARN_ON(chunk_block + nr_blocks > zmd->zone_nr_blocks);\n\n\twhile (nr_blocks) {\n\t\t \n\t\tmblk = dmz_get_bitmap(zmd, zone, chunk_block);\n\t\tif (IS_ERR(mblk))\n\t\t\treturn PTR_ERR(mblk);\n\n\t\t \n\t\tbit = chunk_block & DMZ_BLOCK_MASK_BITS;\n\t\tnr_bits = min(nr_blocks, zmd->zone_bits_per_mblk - bit);\n\n\t\tcount = dmz_clear_bits((unsigned long *)mblk->data,\n\t\t\t\t       bit, nr_bits);\n\t\tif (count) {\n\t\t\tdmz_dirty_mblock(zmd, mblk);\n\t\t\tn += count;\n\t\t}\n\t\tdmz_release_mblock(zmd, mblk);\n\n\t\tnr_blocks -= nr_bits;\n\t\tchunk_block += nr_bits;\n\t}\n\n\tif (zone->weight >= n)\n\t\tzone->weight -= n;\n\telse {\n\t\tdmz_zmd_warn(zmd, \"Zone %u: weight %u should be >= %u\",\n\t\t\t     zone->id, zone->weight, n);\n\t\tzone->weight = 0;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int dmz_test_block(struct dmz_metadata *zmd, struct dm_zone *zone,\n\t\t\t  sector_t chunk_block)\n{\n\tstruct dmz_mblock *mblk;\n\tint ret;\n\n\tWARN_ON(chunk_block >= zmd->zone_nr_blocks);\n\n\t \n\tmblk = dmz_get_bitmap(zmd, zone, chunk_block);\n\tif (IS_ERR(mblk))\n\t\treturn PTR_ERR(mblk);\n\n\t \n\tret = test_bit(chunk_block & DMZ_BLOCK_MASK_BITS,\n\t\t       (unsigned long *) mblk->data) != 0;\n\n\tdmz_release_mblock(zmd, mblk);\n\n\treturn ret;\n}\n\n \nstatic int dmz_to_next_set_block(struct dmz_metadata *zmd, struct dm_zone *zone,\n\t\t\t\t sector_t chunk_block, unsigned int nr_blocks,\n\t\t\t\t int set)\n{\n\tstruct dmz_mblock *mblk;\n\tunsigned int bit, set_bit, nr_bits;\n\tunsigned int zone_bits = zmd->zone_bits_per_mblk;\n\tunsigned long *bitmap;\n\tint n = 0;\n\n\tWARN_ON(chunk_block + nr_blocks > zmd->zone_nr_blocks);\n\n\twhile (nr_blocks) {\n\t\t \n\t\tmblk = dmz_get_bitmap(zmd, zone, chunk_block);\n\t\tif (IS_ERR(mblk))\n\t\t\treturn PTR_ERR(mblk);\n\n\t\t \n\t\tbitmap = (unsigned long *) mblk->data;\n\t\tbit = chunk_block & DMZ_BLOCK_MASK_BITS;\n\t\tnr_bits = min(nr_blocks, zone_bits - bit);\n\t\tif (set)\n\t\t\tset_bit = find_next_bit(bitmap, zone_bits, bit);\n\t\telse\n\t\t\tset_bit = find_next_zero_bit(bitmap, zone_bits, bit);\n\t\tdmz_release_mblock(zmd, mblk);\n\n\t\tn += set_bit - bit;\n\t\tif (set_bit < zone_bits)\n\t\t\tbreak;\n\n\t\tnr_blocks -= nr_bits;\n\t\tchunk_block += nr_bits;\n\t}\n\n\treturn n;\n}\n\n \nint dmz_block_valid(struct dmz_metadata *zmd, struct dm_zone *zone,\n\t\t    sector_t chunk_block)\n{\n\tint valid;\n\n\tvalid = dmz_test_block(zmd, zone, chunk_block);\n\tif (valid <= 0)\n\t\treturn valid;\n\n\t \n\treturn dmz_to_next_set_block(zmd, zone, chunk_block,\n\t\t\t\t     zmd->zone_nr_blocks - chunk_block, 0);\n}\n\n \nint dmz_first_valid_block(struct dmz_metadata *zmd, struct dm_zone *zone,\n\t\t\t  sector_t *chunk_block)\n{\n\tsector_t start_block = *chunk_block;\n\tint ret;\n\n\tret = dmz_to_next_set_block(zmd, zone, start_block,\n\t\t\t\t    zmd->zone_nr_blocks - start_block, 1);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tstart_block += ret;\n\t*chunk_block = start_block;\n\n\treturn dmz_to_next_set_block(zmd, zone, start_block,\n\t\t\t\t     zmd->zone_nr_blocks - start_block, 0);\n}\n\n \nstatic int dmz_count_bits(void *bitmap, int bit, int nr_bits)\n{\n\tunsigned long *addr;\n\tint end = bit + nr_bits;\n\tint n = 0;\n\n\twhile (bit < end) {\n\t\tif (((bit & (BITS_PER_LONG - 1)) == 0) &&\n\t\t    ((end - bit) >= BITS_PER_LONG)) {\n\t\t\taddr = (unsigned long *)bitmap + BIT_WORD(bit);\n\t\t\tif (*addr == ULONG_MAX) {\n\t\t\t\tn += BITS_PER_LONG;\n\t\t\t\tbit += BITS_PER_LONG;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\n\t\tif (test_bit(bit, bitmap))\n\t\t\tn++;\n\t\tbit++;\n\t}\n\n\treturn n;\n}\n\n \nstatic void dmz_get_zone_weight(struct dmz_metadata *zmd, struct dm_zone *zone)\n{\n\tstruct dmz_mblock *mblk;\n\tsector_t chunk_block = 0;\n\tunsigned int bit, nr_bits;\n\tunsigned int nr_blocks = zmd->zone_nr_blocks;\n\tvoid *bitmap;\n\tint n = 0;\n\n\twhile (nr_blocks) {\n\t\t \n\t\tmblk = dmz_get_bitmap(zmd, zone, chunk_block);\n\t\tif (IS_ERR(mblk)) {\n\t\t\tn = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tbitmap = mblk->data;\n\t\tbit = chunk_block & DMZ_BLOCK_MASK_BITS;\n\t\tnr_bits = min(nr_blocks, zmd->zone_bits_per_mblk - bit);\n\t\tn += dmz_count_bits(bitmap, bit, nr_bits);\n\n\t\tdmz_release_mblock(zmd, mblk);\n\n\t\tnr_blocks -= nr_bits;\n\t\tchunk_block += nr_bits;\n\t}\n\n\tzone->weight = n;\n}\n\n \nstatic void dmz_cleanup_metadata(struct dmz_metadata *zmd)\n{\n\tstruct rb_root *root;\n\tstruct dmz_mblock *mblk, *next;\n\tint i;\n\n\t \n\tif (zmd->map_mblk) {\n\t\tfor (i = 0; i < zmd->nr_map_blocks; i++)\n\t\t\tdmz_release_mblock(zmd, zmd->map_mblk[i]);\n\t\tkfree(zmd->map_mblk);\n\t\tzmd->map_mblk = NULL;\n\t}\n\n\t \n\tfor (i = 0; i < 2; i++) {\n\t\tif (zmd->sb[i].mblk) {\n\t\t\tdmz_free_mblock(zmd, zmd->sb[i].mblk);\n\t\t\tzmd->sb[i].mblk = NULL;\n\t\t}\n\t}\n\n\t \n\twhile (!list_empty(&zmd->mblk_dirty_list)) {\n\t\tmblk = list_first_entry(&zmd->mblk_dirty_list,\n\t\t\t\t\tstruct dmz_mblock, link);\n\t\tdmz_zmd_warn(zmd, \"mblock %llu still in dirty list (ref %u)\",\n\t\t\t     (u64)mblk->no, mblk->ref);\n\t\tlist_del_init(&mblk->link);\n\t\trb_erase(&mblk->node, &zmd->mblk_rbtree);\n\t\tdmz_free_mblock(zmd, mblk);\n\t}\n\n\twhile (!list_empty(&zmd->mblk_lru_list)) {\n\t\tmblk = list_first_entry(&zmd->mblk_lru_list,\n\t\t\t\t\tstruct dmz_mblock, link);\n\t\tlist_del_init(&mblk->link);\n\t\trb_erase(&mblk->node, &zmd->mblk_rbtree);\n\t\tdmz_free_mblock(zmd, mblk);\n\t}\n\n\t \n\troot = &zmd->mblk_rbtree;\n\trbtree_postorder_for_each_entry_safe(mblk, next, root, node) {\n\t\tdmz_zmd_warn(zmd, \"mblock %llu ref %u still in rbtree\",\n\t\t\t     (u64)mblk->no, mblk->ref);\n\t\tmblk->ref = 0;\n\t\tdmz_free_mblock(zmd, mblk);\n\t}\n\n\t \n\tdmz_drop_zones(zmd);\n\n\tmutex_destroy(&zmd->mblk_flush_lock);\n\tmutex_destroy(&zmd->map_lock);\n}\n\nstatic void dmz_print_dev(struct dmz_metadata *zmd, int num)\n{\n\tstruct dmz_dev *dev = &zmd->dev[num];\n\n\tif (bdev_zoned_model(dev->bdev) == BLK_ZONED_NONE)\n\t\tdmz_dev_info(dev, \"Regular block device\");\n\telse\n\t\tdmz_dev_info(dev, \"Host-%s zoned block device\",\n\t\t\t     bdev_zoned_model(dev->bdev) == BLK_ZONED_HA ?\n\t\t\t     \"aware\" : \"managed\");\n\tif (zmd->sb_version > 1) {\n\t\tsector_t sector_offset =\n\t\t\tdev->zone_offset << zmd->zone_nr_sectors_shift;\n\n\t\tdmz_dev_info(dev, \"  %llu 512-byte logical sectors (offset %llu)\",\n\t\t\t     (u64)dev->capacity, (u64)sector_offset);\n\t\tdmz_dev_info(dev, \"  %u zones of %llu 512-byte logical sectors (offset %llu)\",\n\t\t\t     dev->nr_zones, (u64)zmd->zone_nr_sectors,\n\t\t\t     (u64)dev->zone_offset);\n\t} else {\n\t\tdmz_dev_info(dev, \"  %llu 512-byte logical sectors\",\n\t\t\t     (u64)dev->capacity);\n\t\tdmz_dev_info(dev, \"  %u zones of %llu 512-byte logical sectors\",\n\t\t\t     dev->nr_zones, (u64)zmd->zone_nr_sectors);\n\t}\n}\n\n \nint dmz_ctr_metadata(struct dmz_dev *dev, int num_dev,\n\t\t     struct dmz_metadata **metadata,\n\t\t     const char *devname)\n{\n\tstruct dmz_metadata *zmd;\n\tunsigned int i;\n\tstruct dm_zone *zone;\n\tint ret;\n\n\tzmd = kzalloc(sizeof(struct dmz_metadata), GFP_KERNEL);\n\tif (!zmd)\n\t\treturn -ENOMEM;\n\n\tstrcpy(zmd->devname, devname);\n\tzmd->dev = dev;\n\tzmd->nr_devs = num_dev;\n\tzmd->mblk_rbtree = RB_ROOT;\n\tinit_rwsem(&zmd->mblk_sem);\n\tmutex_init(&zmd->mblk_flush_lock);\n\tspin_lock_init(&zmd->mblk_lock);\n\tINIT_LIST_HEAD(&zmd->mblk_lru_list);\n\tINIT_LIST_HEAD(&zmd->mblk_dirty_list);\n\n\tmutex_init(&zmd->map_lock);\n\n\tatomic_set(&zmd->unmap_nr_cache, 0);\n\tINIT_LIST_HEAD(&zmd->unmap_cache_list);\n\tINIT_LIST_HEAD(&zmd->map_cache_list);\n\n\tatomic_set(&zmd->nr_reserved_seq_zones, 0);\n\tINIT_LIST_HEAD(&zmd->reserved_seq_zones_list);\n\n\tinit_waitqueue_head(&zmd->free_wq);\n\n\t \n\tret = dmz_init_zones(zmd);\n\tif (ret)\n\t\tgoto err;\n\n\t \n\tret = dmz_load_sb(zmd);\n\tif (ret)\n\t\tgoto err;\n\n\t \n\tfor (i = 0; i < zmd->nr_meta_zones << 1; i++) {\n\t\tzone = dmz_get(zmd, zmd->sb[0].zone->id + i);\n\t\tif (!zone) {\n\t\t\tdmz_zmd_err(zmd,\n\t\t\t\t    \"metadata zone %u not present\", i);\n\t\t\tret = -ENXIO;\n\t\t\tgoto err;\n\t\t}\n\t\tif (!dmz_is_rnd(zone) && !dmz_is_cache(zone)) {\n\t\t\tdmz_zmd_err(zmd,\n\t\t\t\t    \"metadata zone %d is not random\", i);\n\t\t\tret = -ENXIO;\n\t\t\tgoto err;\n\t\t}\n\t\tset_bit(DMZ_META, &zone->flags);\n\t}\n\t \n\tret = dmz_load_mapping(zmd);\n\tif (ret)\n\t\tgoto err;\n\n\t \n\tzmd->min_nr_mblks = 2 + zmd->nr_map_blocks + zmd->zone_nr_bitmap_blocks * 16;\n\tzmd->max_nr_mblks = zmd->min_nr_mblks + 512;\n\tzmd->mblk_shrinker.count_objects = dmz_mblock_shrinker_count;\n\tzmd->mblk_shrinker.scan_objects = dmz_mblock_shrinker_scan;\n\tzmd->mblk_shrinker.seeks = DEFAULT_SEEKS;\n\n\t \n\tret = register_shrinker(&zmd->mblk_shrinker, \"dm-zoned-meta:(%u:%u)\",\n\t\t\t\tMAJOR(dev->bdev->bd_dev),\n\t\t\t\tMINOR(dev->bdev->bd_dev));\n\tif (ret) {\n\t\tdmz_zmd_err(zmd, \"Register metadata cache shrinker failed\");\n\t\tgoto err;\n\t}\n\n\tdmz_zmd_info(zmd, \"DM-Zoned metadata version %d\", zmd->sb_version);\n\tfor (i = 0; i < zmd->nr_devs; i++)\n\t\tdmz_print_dev(zmd, i);\n\n\tdmz_zmd_info(zmd, \"  %u zones of %llu 512-byte logical sectors\",\n\t\t     zmd->nr_zones, (u64)zmd->zone_nr_sectors);\n\tdmz_zmd_debug(zmd, \"  %u metadata zones\",\n\t\t      zmd->nr_meta_zones * 2);\n\tdmz_zmd_debug(zmd, \"  %u data zones for %u chunks\",\n\t\t      zmd->nr_data_zones, zmd->nr_chunks);\n\tdmz_zmd_debug(zmd, \"    %u cache zones (%u unmapped)\",\n\t\t      zmd->nr_cache, atomic_read(&zmd->unmap_nr_cache));\n\tfor (i = 0; i < zmd->nr_devs; i++) {\n\t\tdmz_zmd_debug(zmd, \"    %u random zones (%u unmapped)\",\n\t\t\t      dmz_nr_rnd_zones(zmd, i),\n\t\t\t      dmz_nr_unmap_rnd_zones(zmd, i));\n\t\tdmz_zmd_debug(zmd, \"    %u sequential zones (%u unmapped)\",\n\t\t\t      dmz_nr_seq_zones(zmd, i),\n\t\t\t      dmz_nr_unmap_seq_zones(zmd, i));\n\t}\n\tdmz_zmd_debug(zmd, \"  %u reserved sequential data zones\",\n\t\t      zmd->nr_reserved_seq);\n\tdmz_zmd_debug(zmd, \"Format:\");\n\tdmz_zmd_debug(zmd, \"%u metadata blocks per set (%u max cache)\",\n\t\t      zmd->nr_meta_blocks, zmd->max_nr_mblks);\n\tdmz_zmd_debug(zmd, \"  %u data zone mapping blocks\",\n\t\t      zmd->nr_map_blocks);\n\tdmz_zmd_debug(zmd, \"  %u bitmap blocks\",\n\t\t      zmd->nr_bitmap_blocks);\n\n\t*metadata = zmd;\n\n\treturn 0;\nerr:\n\tdmz_cleanup_metadata(zmd);\n\tkfree(zmd);\n\t*metadata = NULL;\n\n\treturn ret;\n}\n\n \nvoid dmz_dtr_metadata(struct dmz_metadata *zmd)\n{\n\tunregister_shrinker(&zmd->mblk_shrinker);\n\tdmz_cleanup_metadata(zmd);\n\tkfree(zmd);\n}\n\n \nint dmz_resume_metadata(struct dmz_metadata *zmd)\n{\n\tstruct dm_zone *zone;\n\tsector_t wp_block;\n\tunsigned int i;\n\tint ret;\n\n\t \n\tfor (i = 0; i < zmd->nr_zones; i++) {\n\t\tzone = dmz_get(zmd, i);\n\t\tif (!zone) {\n\t\t\tdmz_zmd_err(zmd, \"Unable to get zone %u\", i);\n\t\t\treturn -EIO;\n\t\t}\n\t\twp_block = zone->wp_block;\n\n\t\tret = dmz_update_zone(zmd, zone);\n\t\tif (ret) {\n\t\t\tdmz_zmd_err(zmd, \"Broken zone %u\", i);\n\t\t\treturn ret;\n\t\t}\n\n\t\tif (dmz_is_offline(zone)) {\n\t\t\tdmz_zmd_warn(zmd, \"Zone %u is offline\", i);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!dmz_is_seq(zone))\n\t\t\tzone->wp_block = 0;\n\t\telse if (zone->wp_block != wp_block) {\n\t\t\tdmz_zmd_err(zmd, \"Zone %u: Invalid wp (%llu / %llu)\",\n\t\t\t\t    i, (u64)zone->wp_block, (u64)wp_block);\n\t\t\tzone->wp_block = wp_block;\n\t\t\tdmz_invalidate_blocks(zmd, zone, zone->wp_block,\n\t\t\t\t\t      zmd->zone_nr_blocks - zone->wp_block);\n\t\t}\n\t}\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}