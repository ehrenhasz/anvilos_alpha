{
  "module_name": "raid5.c",
  "hash_id": "44559ebb70b1e84930d5824bbe6015dd90997ec6fae2d359fb8d3485592e24e8",
  "original_prompt": "Ingested from linux-6.6.14/drivers/md/raid5.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/blkdev.h>\n#include <linux/delay.h>\n#include <linux/kthread.h>\n#include <linux/raid/pq.h>\n#include <linux/async_tx.h>\n#include <linux/module.h>\n#include <linux/async.h>\n#include <linux/seq_file.h>\n#include <linux/cpu.h>\n#include <linux/slab.h>\n#include <linux/ratelimit.h>\n#include <linux/nodemask.h>\n\n#include <trace/events/block.h>\n#include <linux/list_sort.h>\n\n#include \"md.h\"\n#include \"raid5.h\"\n#include \"raid0.h\"\n#include \"md-bitmap.h\"\n#include \"raid5-log.h\"\n\n#define UNSUPPORTED_MDDEV_FLAGS\t(1L << MD_FAILFAST_SUPPORTED)\n\n#define cpu_to_group(cpu) cpu_to_node(cpu)\n#define ANY_GROUP NUMA_NO_NODE\n\n#define RAID5_MAX_REQ_STRIPES 256\n\nstatic bool devices_handle_discard_safely = false;\nmodule_param(devices_handle_discard_safely, bool, 0644);\nMODULE_PARM_DESC(devices_handle_discard_safely,\n\t\t \"Set to Y if all devices in each array reliably return zeroes on reads from discarded regions\");\nstatic struct workqueue_struct *raid5_wq;\n\nstatic inline struct hlist_head *stripe_hash(struct r5conf *conf, sector_t sect)\n{\n\tint hash = (sect >> RAID5_STRIPE_SHIFT(conf)) & HASH_MASK;\n\treturn &conf->stripe_hashtbl[hash];\n}\n\nstatic inline int stripe_hash_locks_hash(struct r5conf *conf, sector_t sect)\n{\n\treturn (sect >> RAID5_STRIPE_SHIFT(conf)) & STRIPE_HASH_LOCKS_MASK;\n}\n\nstatic inline void lock_device_hash_lock(struct r5conf *conf, int hash)\n\t__acquires(&conf->device_lock)\n{\n\tspin_lock_irq(conf->hash_locks + hash);\n\tspin_lock(&conf->device_lock);\n}\n\nstatic inline void unlock_device_hash_lock(struct r5conf *conf, int hash)\n\t__releases(&conf->device_lock)\n{\n\tspin_unlock(&conf->device_lock);\n\tspin_unlock_irq(conf->hash_locks + hash);\n}\n\nstatic inline void lock_all_device_hash_locks_irq(struct r5conf *conf)\n\t__acquires(&conf->device_lock)\n{\n\tint i;\n\tspin_lock_irq(conf->hash_locks);\n\tfor (i = 1; i < NR_STRIPE_HASH_LOCKS; i++)\n\t\tspin_lock_nest_lock(conf->hash_locks + i, conf->hash_locks);\n\tspin_lock(&conf->device_lock);\n}\n\nstatic inline void unlock_all_device_hash_locks_irq(struct r5conf *conf)\n\t__releases(&conf->device_lock)\n{\n\tint i;\n\tspin_unlock(&conf->device_lock);\n\tfor (i = NR_STRIPE_HASH_LOCKS - 1; i; i--)\n\t\tspin_unlock(conf->hash_locks + i);\n\tspin_unlock_irq(conf->hash_locks);\n}\n\n \nstatic inline int raid6_d0(struct stripe_head *sh)\n{\n\tif (sh->ddf_layout)\n\t\t \n\t\treturn 0;\n\t \n\tif (sh->qd_idx == sh->disks - 1)\n\t\treturn 0;\n\telse\n\t\treturn sh->qd_idx + 1;\n}\nstatic inline int raid6_next_disk(int disk, int raid_disks)\n{\n\tdisk++;\n\treturn (disk < raid_disks) ? disk : 0;\n}\n\n \nstatic int raid6_idx_to_slot(int idx, struct stripe_head *sh,\n\t\t\t     int *count, int syndrome_disks)\n{\n\tint slot = *count;\n\n\tif (sh->ddf_layout)\n\t\t(*count)++;\n\tif (idx == sh->pd_idx)\n\t\treturn syndrome_disks;\n\tif (idx == sh->qd_idx)\n\t\treturn syndrome_disks + 1;\n\tif (!sh->ddf_layout)\n\t\t(*count)++;\n\treturn slot;\n}\n\nstatic void print_raid5_conf (struct r5conf *conf);\n\nstatic int stripe_operations_active(struct stripe_head *sh)\n{\n\treturn sh->check_state || sh->reconstruct_state ||\n\t       test_bit(STRIPE_BIOFILL_RUN, &sh->state) ||\n\t       test_bit(STRIPE_COMPUTE_RUN, &sh->state);\n}\n\nstatic bool stripe_is_lowprio(struct stripe_head *sh)\n{\n\treturn (test_bit(STRIPE_R5C_FULL_STRIPE, &sh->state) ||\n\t\ttest_bit(STRIPE_R5C_PARTIAL_STRIPE, &sh->state)) &&\n\t       !test_bit(STRIPE_R5C_CACHING, &sh->state);\n}\n\nstatic void raid5_wakeup_stripe_thread(struct stripe_head *sh)\n\t__must_hold(&sh->raid_conf->device_lock)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tstruct r5worker_group *group;\n\tint thread_cnt;\n\tint i, cpu = sh->cpu;\n\n\tif (!cpu_online(cpu)) {\n\t\tcpu = cpumask_any(cpu_online_mask);\n\t\tsh->cpu = cpu;\n\t}\n\n\tif (list_empty(&sh->lru)) {\n\t\tstruct r5worker_group *group;\n\t\tgroup = conf->worker_groups + cpu_to_group(cpu);\n\t\tif (stripe_is_lowprio(sh))\n\t\t\tlist_add_tail(&sh->lru, &group->loprio_list);\n\t\telse\n\t\t\tlist_add_tail(&sh->lru, &group->handle_list);\n\t\tgroup->stripes_cnt++;\n\t\tsh->group = group;\n\t}\n\n\tif (conf->worker_cnt_per_group == 0) {\n\t\tmd_wakeup_thread(conf->mddev->thread);\n\t\treturn;\n\t}\n\n\tgroup = conf->worker_groups + cpu_to_group(sh->cpu);\n\n\tgroup->workers[0].working = true;\n\t \n\tqueue_work_on(sh->cpu, raid5_wq, &group->workers[0].work);\n\n\tthread_cnt = group->stripes_cnt / MAX_STRIPE_BATCH - 1;\n\t \n\tfor (i = 1; i < conf->worker_cnt_per_group && thread_cnt > 0; i++) {\n\t\tif (group->workers[i].working == false) {\n\t\t\tgroup->workers[i].working = true;\n\t\t\tqueue_work_on(sh->cpu, raid5_wq,\n\t\t\t\t      &group->workers[i].work);\n\t\t\tthread_cnt--;\n\t\t}\n\t}\n}\n\nstatic void do_release_stripe(struct r5conf *conf, struct stripe_head *sh,\n\t\t\t      struct list_head *temp_inactive_list)\n\t__must_hold(&conf->device_lock)\n{\n\tint i;\n\tint injournal = 0;\t \n\n\tBUG_ON(!list_empty(&sh->lru));\n\tBUG_ON(atomic_read(&conf->active_stripes)==0);\n\n\tif (r5c_is_writeback(conf->log))\n\t\tfor (i = sh->disks; i--; )\n\t\t\tif (test_bit(R5_InJournal, &sh->dev[i].flags))\n\t\t\t\tinjournal++;\n\t \n\tif (test_bit(STRIPE_SYNC_REQUESTED, &sh->state) ||\n\t    (conf->quiesce && r5c_is_writeback(conf->log) &&\n\t     !test_bit(STRIPE_HANDLE, &sh->state) && injournal != 0)) {\n\t\tif (test_bit(STRIPE_R5C_CACHING, &sh->state))\n\t\t\tr5c_make_stripe_write_out(sh);\n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t}\n\n\tif (test_bit(STRIPE_HANDLE, &sh->state)) {\n\t\tif (test_bit(STRIPE_DELAYED, &sh->state) &&\n\t\t    !test_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\t\tlist_add_tail(&sh->lru, &conf->delayed_list);\n\t\telse if (test_bit(STRIPE_BIT_DELAY, &sh->state) &&\n\t\t\t   sh->bm_seq - conf->seq_write > 0)\n\t\t\tlist_add_tail(&sh->lru, &conf->bitmap_list);\n\t\telse {\n\t\t\tclear_bit(STRIPE_DELAYED, &sh->state);\n\t\t\tclear_bit(STRIPE_BIT_DELAY, &sh->state);\n\t\t\tif (conf->worker_cnt_per_group == 0) {\n\t\t\t\tif (stripe_is_lowprio(sh))\n\t\t\t\t\tlist_add_tail(&sh->lru,\n\t\t\t\t\t\t\t&conf->loprio_list);\n\t\t\t\telse\n\t\t\t\t\tlist_add_tail(&sh->lru,\n\t\t\t\t\t\t\t&conf->handle_list);\n\t\t\t} else {\n\t\t\t\traid5_wakeup_stripe_thread(sh);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t\tmd_wakeup_thread(conf->mddev->thread);\n\t} else {\n\t\tBUG_ON(stripe_operations_active(sh));\n\t\tif (test_and_clear_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\t\tif (atomic_dec_return(&conf->preread_active_stripes)\n\t\t\t    < IO_THRESHOLD)\n\t\t\t\tmd_wakeup_thread(conf->mddev->thread);\n\t\tatomic_dec(&conf->active_stripes);\n\t\tif (!test_bit(STRIPE_EXPANDING, &sh->state)) {\n\t\t\tif (!r5c_is_writeback(conf->log))\n\t\t\t\tlist_add_tail(&sh->lru, temp_inactive_list);\n\t\t\telse {\n\t\t\t\tWARN_ON(test_bit(R5_InJournal, &sh->dev[sh->pd_idx].flags));\n\t\t\t\tif (injournal == 0)\n\t\t\t\t\tlist_add_tail(&sh->lru, temp_inactive_list);\n\t\t\t\telse if (injournal == conf->raid_disks - conf->max_degraded) {\n\t\t\t\t\t \n\t\t\t\t\tif (!test_and_set_bit(STRIPE_R5C_FULL_STRIPE, &sh->state))\n\t\t\t\t\t\tatomic_inc(&conf->r5c_cached_full_stripes);\n\t\t\t\t\tif (test_and_clear_bit(STRIPE_R5C_PARTIAL_STRIPE, &sh->state))\n\t\t\t\t\t\tatomic_dec(&conf->r5c_cached_partial_stripes);\n\t\t\t\t\tlist_add_tail(&sh->lru, &conf->r5c_full_stripe_list);\n\t\t\t\t\tr5c_check_cached_full_stripe(conf);\n\t\t\t\t} else\n\t\t\t\t\t \n\t\t\t\t\tlist_add_tail(&sh->lru, &conf->r5c_partial_stripe_list);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void __release_stripe(struct r5conf *conf, struct stripe_head *sh,\n\t\t\t     struct list_head *temp_inactive_list)\n\t__must_hold(&conf->device_lock)\n{\n\tif (atomic_dec_and_test(&sh->count))\n\t\tdo_release_stripe(conf, sh, temp_inactive_list);\n}\n\n \nstatic void release_inactive_stripe_list(struct r5conf *conf,\n\t\t\t\t\t struct list_head *temp_inactive_list,\n\t\t\t\t\t int hash)\n{\n\tint size;\n\tbool do_wakeup = false;\n\tunsigned long flags;\n\n\tif (hash == NR_STRIPE_HASH_LOCKS) {\n\t\tsize = NR_STRIPE_HASH_LOCKS;\n\t\thash = NR_STRIPE_HASH_LOCKS - 1;\n\t} else\n\t\tsize = 1;\n\twhile (size) {\n\t\tstruct list_head *list = &temp_inactive_list[size - 1];\n\n\t\t \n\t\tif (!list_empty_careful(list)) {\n\t\t\tspin_lock_irqsave(conf->hash_locks + hash, flags);\n\t\t\tif (list_empty(conf->inactive_list + hash) &&\n\t\t\t    !list_empty(list))\n\t\t\t\tatomic_dec(&conf->empty_inactive_list_nr);\n\t\t\tlist_splice_tail_init(list, conf->inactive_list + hash);\n\t\t\tdo_wakeup = true;\n\t\t\tspin_unlock_irqrestore(conf->hash_locks + hash, flags);\n\t\t}\n\t\tsize--;\n\t\thash--;\n\t}\n\n\tif (do_wakeup) {\n\t\twake_up(&conf->wait_for_stripe);\n\t\tif (atomic_read(&conf->active_stripes) == 0)\n\t\t\twake_up(&conf->wait_for_quiescent);\n\t\tif (conf->retry_read_aligned)\n\t\t\tmd_wakeup_thread(conf->mddev->thread);\n\t}\n}\n\nstatic int release_stripe_list(struct r5conf *conf,\n\t\t\t       struct list_head *temp_inactive_list)\n\t__must_hold(&conf->device_lock)\n{\n\tstruct stripe_head *sh, *t;\n\tint count = 0;\n\tstruct llist_node *head;\n\n\thead = llist_del_all(&conf->released_stripes);\n\thead = llist_reverse_order(head);\n\tllist_for_each_entry_safe(sh, t, head, release_list) {\n\t\tint hash;\n\n\t\t \n\t\tsmp_mb();\n\t\tclear_bit(STRIPE_ON_RELEASE_LIST, &sh->state);\n\t\t \n\t\thash = sh->hash_lock_index;\n\t\t__release_stripe(conf, sh, &temp_inactive_list[hash]);\n\t\tcount++;\n\t}\n\n\treturn count;\n}\n\nvoid raid5_release_stripe(struct stripe_head *sh)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tunsigned long flags;\n\tstruct list_head list;\n\tint hash;\n\tbool wakeup;\n\n\t \n\tif (atomic_add_unless(&sh->count, -1, 1))\n\t\treturn;\n\n\tif (unlikely(!conf->mddev->thread) ||\n\t\ttest_and_set_bit(STRIPE_ON_RELEASE_LIST, &sh->state))\n\t\tgoto slow_path;\n\twakeup = llist_add(&sh->release_list, &conf->released_stripes);\n\tif (wakeup)\n\t\tmd_wakeup_thread(conf->mddev->thread);\n\treturn;\nslow_path:\n\t \n\tif (atomic_dec_and_lock_irqsave(&sh->count, &conf->device_lock, flags)) {\n\t\tINIT_LIST_HEAD(&list);\n\t\thash = sh->hash_lock_index;\n\t\tdo_release_stripe(conf, sh, &list);\n\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\t\trelease_inactive_stripe_list(conf, &list, hash);\n\t}\n}\n\nstatic inline void remove_hash(struct stripe_head *sh)\n{\n\tpr_debug(\"remove_hash(), stripe %llu\\n\",\n\t\t(unsigned long long)sh->sector);\n\n\thlist_del_init(&sh->hash);\n}\n\nstatic inline void insert_hash(struct r5conf *conf, struct stripe_head *sh)\n{\n\tstruct hlist_head *hp = stripe_hash(conf, sh->sector);\n\n\tpr_debug(\"insert_hash(), stripe %llu\\n\",\n\t\t(unsigned long long)sh->sector);\n\n\thlist_add_head(&sh->hash, hp);\n}\n\n \nstatic struct stripe_head *get_free_stripe(struct r5conf *conf, int hash)\n{\n\tstruct stripe_head *sh = NULL;\n\tstruct list_head *first;\n\n\tif (list_empty(conf->inactive_list + hash))\n\t\tgoto out;\n\tfirst = (conf->inactive_list + hash)->next;\n\tsh = list_entry(first, struct stripe_head, lru);\n\tlist_del_init(first);\n\tremove_hash(sh);\n\tatomic_inc(&conf->active_stripes);\n\tBUG_ON(hash != sh->hash_lock_index);\n\tif (list_empty(conf->inactive_list + hash))\n\t\tatomic_inc(&conf->empty_inactive_list_nr);\nout:\n\treturn sh;\n}\n\n#if PAGE_SIZE != DEFAULT_STRIPE_SIZE\nstatic void free_stripe_pages(struct stripe_head *sh)\n{\n\tint i;\n\tstruct page *p;\n\n\t \n\tif (!sh->pages)\n\t\treturn;\n\n\tfor (i = 0; i < sh->nr_pages; i++) {\n\t\tp = sh->pages[i];\n\t\tif (p)\n\t\t\tput_page(p);\n\t\tsh->pages[i] = NULL;\n\t}\n}\n\nstatic int alloc_stripe_pages(struct stripe_head *sh, gfp_t gfp)\n{\n\tint i;\n\tstruct page *p;\n\n\tfor (i = 0; i < sh->nr_pages; i++) {\n\t\t \n\t\tif (sh->pages[i])\n\t\t\tcontinue;\n\n\t\tp = alloc_page(gfp);\n\t\tif (!p) {\n\t\t\tfree_stripe_pages(sh);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tsh->pages[i] = p;\n\t}\n\treturn 0;\n}\n\nstatic int\ninit_stripe_shared_pages(struct stripe_head *sh, struct r5conf *conf, int disks)\n{\n\tint nr_pages, cnt;\n\n\tif (sh->pages)\n\t\treturn 0;\n\n\t \n\tcnt = PAGE_SIZE / conf->stripe_size;\n\tnr_pages = (disks + cnt - 1) / cnt;\n\n\tsh->pages = kcalloc(nr_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!sh->pages)\n\t\treturn -ENOMEM;\n\tsh->nr_pages = nr_pages;\n\tsh->stripes_per_page = cnt;\n\treturn 0;\n}\n#endif\n\nstatic void shrink_buffers(struct stripe_head *sh)\n{\n\tint i;\n\tint num = sh->raid_conf->pool_size;\n\n#if PAGE_SIZE == DEFAULT_STRIPE_SIZE\n\tfor (i = 0; i < num ; i++) {\n\t\tstruct page *p;\n\n\t\tWARN_ON(sh->dev[i].page != sh->dev[i].orig_page);\n\t\tp = sh->dev[i].page;\n\t\tif (!p)\n\t\t\tcontinue;\n\t\tsh->dev[i].page = NULL;\n\t\tput_page(p);\n\t}\n#else\n\tfor (i = 0; i < num; i++)\n\t\tsh->dev[i].page = NULL;\n\tfree_stripe_pages(sh);  \n#endif\n}\n\nstatic int grow_buffers(struct stripe_head *sh, gfp_t gfp)\n{\n\tint i;\n\tint num = sh->raid_conf->pool_size;\n\n#if PAGE_SIZE == DEFAULT_STRIPE_SIZE\n\tfor (i = 0; i < num; i++) {\n\t\tstruct page *page;\n\n\t\tif (!(page = alloc_page(gfp))) {\n\t\t\treturn 1;\n\t\t}\n\t\tsh->dev[i].page = page;\n\t\tsh->dev[i].orig_page = page;\n\t\tsh->dev[i].offset = 0;\n\t}\n#else\n\tif (alloc_stripe_pages(sh, gfp))\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num; i++) {\n\t\tsh->dev[i].page = raid5_get_dev_page(sh, i);\n\t\tsh->dev[i].orig_page = sh->dev[i].page;\n\t\tsh->dev[i].offset = raid5_get_page_offset(sh, i);\n\t}\n#endif\n\treturn 0;\n}\n\nstatic void stripe_set_idx(sector_t stripe, struct r5conf *conf, int previous,\n\t\t\t    struct stripe_head *sh);\n\nstatic void init_stripe(struct stripe_head *sh, sector_t sector, int previous)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tint i, seq;\n\n\tBUG_ON(atomic_read(&sh->count) != 0);\n\tBUG_ON(test_bit(STRIPE_HANDLE, &sh->state));\n\tBUG_ON(stripe_operations_active(sh));\n\tBUG_ON(sh->batch_head);\n\n\tpr_debug(\"init_stripe called, stripe %llu\\n\",\n\t\t(unsigned long long)sector);\nretry:\n\tseq = read_seqcount_begin(&conf->gen_lock);\n\tsh->generation = conf->generation - previous;\n\tsh->disks = previous ? conf->previous_raid_disks : conf->raid_disks;\n\tsh->sector = sector;\n\tstripe_set_idx(sector, conf, previous, sh);\n\tsh->state = 0;\n\n\tfor (i = sh->disks; i--; ) {\n\t\tstruct r5dev *dev = &sh->dev[i];\n\n\t\tif (dev->toread || dev->read || dev->towrite || dev->written ||\n\t\t    test_bit(R5_LOCKED, &dev->flags)) {\n\t\t\tpr_err(\"sector=%llx i=%d %p %p %p %p %d\\n\",\n\t\t\t       (unsigned long long)sh->sector, i, dev->toread,\n\t\t\t       dev->read, dev->towrite, dev->written,\n\t\t\t       test_bit(R5_LOCKED, &dev->flags));\n\t\t\tWARN_ON(1);\n\t\t}\n\t\tdev->flags = 0;\n\t\tdev->sector = raid5_compute_blocknr(sh, i, previous);\n\t}\n\tif (read_seqcount_retry(&conf->gen_lock, seq))\n\t\tgoto retry;\n\tsh->overwrite_disks = 0;\n\tinsert_hash(conf, sh);\n\tsh->cpu = smp_processor_id();\n\tset_bit(STRIPE_BATCH_READY, &sh->state);\n}\n\nstatic struct stripe_head *__find_stripe(struct r5conf *conf, sector_t sector,\n\t\t\t\t\t short generation)\n{\n\tstruct stripe_head *sh;\n\n\tpr_debug(\"__find_stripe, sector %llu\\n\", (unsigned long long)sector);\n\thlist_for_each_entry(sh, stripe_hash(conf, sector), hash)\n\t\tif (sh->sector == sector && sh->generation == generation)\n\t\t\treturn sh;\n\tpr_debug(\"__stripe %llu not in cache\\n\", (unsigned long long)sector);\n\treturn NULL;\n}\n\nstatic struct stripe_head *find_get_stripe(struct r5conf *conf,\n\t\tsector_t sector, short generation, int hash)\n{\n\tint inc_empty_inactive_list_flag;\n\tstruct stripe_head *sh;\n\n\tsh = __find_stripe(conf, sector, generation);\n\tif (!sh)\n\t\treturn NULL;\n\n\tif (atomic_inc_not_zero(&sh->count))\n\t\treturn sh;\n\n\t \n\n\tspin_lock(&conf->device_lock);\n\tif (!atomic_read(&sh->count)) {\n\t\tif (!test_bit(STRIPE_HANDLE, &sh->state))\n\t\t\tatomic_inc(&conf->active_stripes);\n\t\tBUG_ON(list_empty(&sh->lru) &&\n\t\t       !test_bit(STRIPE_EXPANDING, &sh->state));\n\t\tinc_empty_inactive_list_flag = 0;\n\t\tif (!list_empty(conf->inactive_list + hash))\n\t\t\tinc_empty_inactive_list_flag = 1;\n\t\tlist_del_init(&sh->lru);\n\t\tif (list_empty(conf->inactive_list + hash) &&\n\t\t    inc_empty_inactive_list_flag)\n\t\t\tatomic_inc(&conf->empty_inactive_list_nr);\n\t\tif (sh->group) {\n\t\t\tsh->group->stripes_cnt--;\n\t\t\tsh->group = NULL;\n\t\t}\n\t}\n\tatomic_inc(&sh->count);\n\tspin_unlock(&conf->device_lock);\n\n\treturn sh;\n}\n\n \nint raid5_calc_degraded(struct r5conf *conf)\n{\n\tint degraded, degraded2;\n\tint i;\n\n\trcu_read_lock();\n\tdegraded = 0;\n\tfor (i = 0; i < conf->previous_raid_disks; i++) {\n\t\tstruct md_rdev *rdev = rcu_dereference(conf->disks[i].rdev);\n\t\tif (rdev && test_bit(Faulty, &rdev->flags))\n\t\t\trdev = rcu_dereference(conf->disks[i].replacement);\n\t\tif (!rdev || test_bit(Faulty, &rdev->flags))\n\t\t\tdegraded++;\n\t\telse if (test_bit(In_sync, &rdev->flags))\n\t\t\t;\n\t\telse\n\t\t\t \n\t\t\tif (conf->raid_disks >= conf->previous_raid_disks)\n\t\t\t\tdegraded++;\n\t}\n\trcu_read_unlock();\n\tif (conf->raid_disks == conf->previous_raid_disks)\n\t\treturn degraded;\n\trcu_read_lock();\n\tdegraded2 = 0;\n\tfor (i = 0; i < conf->raid_disks; i++) {\n\t\tstruct md_rdev *rdev = rcu_dereference(conf->disks[i].rdev);\n\t\tif (rdev && test_bit(Faulty, &rdev->flags))\n\t\t\trdev = rcu_dereference(conf->disks[i].replacement);\n\t\tif (!rdev || test_bit(Faulty, &rdev->flags))\n\t\t\tdegraded2++;\n\t\telse if (test_bit(In_sync, &rdev->flags))\n\t\t\t;\n\t\telse\n\t\t\t \n\t\t\tif (conf->raid_disks <= conf->previous_raid_disks)\n\t\t\t\tdegraded2++;\n\t}\n\trcu_read_unlock();\n\tif (degraded2 > degraded)\n\t\treturn degraded2;\n\treturn degraded;\n}\n\nstatic bool has_failed(struct r5conf *conf)\n{\n\tint degraded = conf->mddev->degraded;\n\n\tif (test_bit(MD_BROKEN, &conf->mddev->flags))\n\t\treturn true;\n\n\tif (conf->mddev->reshape_position != MaxSector)\n\t\tdegraded = raid5_calc_degraded(conf);\n\n\treturn degraded > conf->max_degraded;\n}\n\nenum stripe_result {\n\tSTRIPE_SUCCESS = 0,\n\tSTRIPE_RETRY,\n\tSTRIPE_SCHEDULE_AND_RETRY,\n\tSTRIPE_FAIL,\n};\n\nstruct stripe_request_ctx {\n\t \n\tstruct stripe_head *batch_last;\n\n\t \n\tsector_t first_sector;\n\n\t \n\tsector_t last_sector;\n\n\t \n\tDECLARE_BITMAP(sectors_to_do, RAID5_MAX_REQ_STRIPES + 1);\n\n\t \n\tbool do_flush;\n};\n\n \nstatic bool is_inactive_blocked(struct r5conf *conf, int hash)\n{\n\tif (list_empty(conf->inactive_list + hash))\n\t\treturn false;\n\n\tif (!test_bit(R5_INACTIVE_BLOCKED, &conf->cache_state))\n\t\treturn true;\n\n\treturn (atomic_read(&conf->active_stripes) <\n\t\t(conf->max_nr_stripes * 3 / 4));\n}\n\nstruct stripe_head *raid5_get_active_stripe(struct r5conf *conf,\n\t\tstruct stripe_request_ctx *ctx, sector_t sector,\n\t\tunsigned int flags)\n{\n\tstruct stripe_head *sh;\n\tint hash = stripe_hash_locks_hash(conf, sector);\n\tint previous = !!(flags & R5_GAS_PREVIOUS);\n\n\tpr_debug(\"get_stripe, sector %llu\\n\", (unsigned long long)sector);\n\n\tspin_lock_irq(conf->hash_locks + hash);\n\n\tfor (;;) {\n\t\tif (!(flags & R5_GAS_NOQUIESCE) && conf->quiesce) {\n\t\t\t \n\t\t\tif (ctx && ctx->batch_last) {\n\t\t\t\traid5_release_stripe(ctx->batch_last);\n\t\t\t\tctx->batch_last = NULL;\n\t\t\t}\n\n\t\t\twait_event_lock_irq(conf->wait_for_quiescent,\n\t\t\t\t\t    !conf->quiesce,\n\t\t\t\t\t    *(conf->hash_locks + hash));\n\t\t}\n\n\t\tsh = find_get_stripe(conf, sector, conf->generation - previous,\n\t\t\t\t     hash);\n\t\tif (sh)\n\t\t\tbreak;\n\n\t\tif (!test_bit(R5_INACTIVE_BLOCKED, &conf->cache_state)) {\n\t\t\tsh = get_free_stripe(conf, hash);\n\t\t\tif (sh) {\n\t\t\t\tr5c_check_stripe_cache_usage(conf);\n\t\t\t\tinit_stripe(sh, sector, previous);\n\t\t\t\tatomic_inc(&sh->count);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (!test_bit(R5_DID_ALLOC, &conf->cache_state))\n\t\t\t\tset_bit(R5_ALLOC_MORE, &conf->cache_state);\n\t\t}\n\n\t\tif (flags & R5_GAS_NOBLOCK)\n\t\t\tbreak;\n\n\t\tset_bit(R5_INACTIVE_BLOCKED, &conf->cache_state);\n\t\tr5l_wake_reclaim(conf->log, 0);\n\n\t\t \n\t\tif (ctx && ctx->batch_last) {\n\t\t\traid5_release_stripe(ctx->batch_last);\n\t\t\tctx->batch_last = NULL;\n\t\t}\n\n\t\twait_event_lock_irq(conf->wait_for_stripe,\n\t\t\t\t    is_inactive_blocked(conf, hash),\n\t\t\t\t    *(conf->hash_locks + hash));\n\t\tclear_bit(R5_INACTIVE_BLOCKED, &conf->cache_state);\n\t}\n\n\tspin_unlock_irq(conf->hash_locks + hash);\n\treturn sh;\n}\n\nstatic bool is_full_stripe_write(struct stripe_head *sh)\n{\n\tBUG_ON(sh->overwrite_disks > (sh->disks - sh->raid_conf->max_degraded));\n\treturn sh->overwrite_disks == (sh->disks - sh->raid_conf->max_degraded);\n}\n\nstatic void lock_two_stripes(struct stripe_head *sh1, struct stripe_head *sh2)\n\t\t__acquires(&sh1->stripe_lock)\n\t\t__acquires(&sh2->stripe_lock)\n{\n\tif (sh1 > sh2) {\n\t\tspin_lock_irq(&sh2->stripe_lock);\n\t\tspin_lock_nested(&sh1->stripe_lock, 1);\n\t} else {\n\t\tspin_lock_irq(&sh1->stripe_lock);\n\t\tspin_lock_nested(&sh2->stripe_lock, 1);\n\t}\n}\n\nstatic void unlock_two_stripes(struct stripe_head *sh1, struct stripe_head *sh2)\n\t\t__releases(&sh1->stripe_lock)\n\t\t__releases(&sh2->stripe_lock)\n{\n\tspin_unlock(&sh1->stripe_lock);\n\tspin_unlock_irq(&sh2->stripe_lock);\n}\n\n \nstatic bool stripe_can_batch(struct stripe_head *sh)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\n\tif (raid5_has_log(conf) || raid5_has_ppl(conf))\n\t\treturn false;\n\treturn test_bit(STRIPE_BATCH_READY, &sh->state) &&\n\t\t!test_bit(STRIPE_BITMAP_PENDING, &sh->state) &&\n\t\tis_full_stripe_write(sh);\n}\n\n \nstatic void stripe_add_to_batch_list(struct r5conf *conf,\n\t\tstruct stripe_head *sh, struct stripe_head *last_sh)\n{\n\tstruct stripe_head *head;\n\tsector_t head_sector, tmp_sec;\n\tint hash;\n\tint dd_idx;\n\n\t \n\ttmp_sec = sh->sector;\n\tif (!sector_div(tmp_sec, conf->chunk_sectors))\n\t\treturn;\n\thead_sector = sh->sector - RAID5_STRIPE_SECTORS(conf);\n\n\tif (last_sh && head_sector == last_sh->sector) {\n\t\thead = last_sh;\n\t\tatomic_inc(&head->count);\n\t} else {\n\t\thash = stripe_hash_locks_hash(conf, head_sector);\n\t\tspin_lock_irq(conf->hash_locks + hash);\n\t\thead = find_get_stripe(conf, head_sector, conf->generation,\n\t\t\t\t       hash);\n\t\tspin_unlock_irq(conf->hash_locks + hash);\n\t\tif (!head)\n\t\t\treturn;\n\t\tif (!stripe_can_batch(head))\n\t\t\tgoto out;\n\t}\n\n\tlock_two_stripes(head, sh);\n\t \n\tif (!stripe_can_batch(head) || !stripe_can_batch(sh))\n\t\tgoto unlock_out;\n\n\tif (sh->batch_head)\n\t\tgoto unlock_out;\n\n\tdd_idx = 0;\n\twhile (dd_idx == sh->pd_idx || dd_idx == sh->qd_idx)\n\t\tdd_idx++;\n\tif (head->dev[dd_idx].towrite->bi_opf != sh->dev[dd_idx].towrite->bi_opf ||\n\t    bio_op(head->dev[dd_idx].towrite) != bio_op(sh->dev[dd_idx].towrite))\n\t\tgoto unlock_out;\n\n\tif (head->batch_head) {\n\t\tspin_lock(&head->batch_head->batch_lock);\n\t\t \n\t\tif (!stripe_can_batch(head)) {\n\t\t\tspin_unlock(&head->batch_head->batch_lock);\n\t\t\tgoto unlock_out;\n\t\t}\n\t\t \n\t\tsh->batch_head = head->batch_head;\n\n\t\t \n\t\tlist_add(&sh->batch_list, &head->batch_list);\n\t\tspin_unlock(&head->batch_head->batch_lock);\n\t} else {\n\t\thead->batch_head = head;\n\t\tsh->batch_head = head->batch_head;\n\t\tspin_lock(&head->batch_lock);\n\t\tlist_add_tail(&sh->batch_list, &head->batch_list);\n\t\tspin_unlock(&head->batch_lock);\n\t}\n\n\tif (test_and_clear_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\tif (atomic_dec_return(&conf->preread_active_stripes)\n\t\t    < IO_THRESHOLD)\n\t\t\tmd_wakeup_thread(conf->mddev->thread);\n\n\tif (test_and_clear_bit(STRIPE_BIT_DELAY, &sh->state)) {\n\t\tint seq = sh->bm_seq;\n\t\tif (test_bit(STRIPE_BIT_DELAY, &sh->batch_head->state) &&\n\t\t    sh->batch_head->bm_seq > seq)\n\t\t\tseq = sh->batch_head->bm_seq;\n\t\tset_bit(STRIPE_BIT_DELAY, &sh->batch_head->state);\n\t\tsh->batch_head->bm_seq = seq;\n\t}\n\n\tatomic_inc(&sh->count);\nunlock_out:\n\tunlock_two_stripes(head, sh);\nout:\n\traid5_release_stripe(head);\n}\n\n \nstatic int use_new_offset(struct r5conf *conf, struct stripe_head *sh)\n{\n\tsector_t progress = conf->reshape_progress;\n\t \n\tsmp_rmb();\n\tif (progress == MaxSector)\n\t\treturn 0;\n\tif (sh->generation == conf->generation - 1)\n\t\treturn 0;\n\t \n\treturn 1;\n}\n\nstatic void dispatch_bio_list(struct bio_list *tmp)\n{\n\tstruct bio *bio;\n\n\twhile ((bio = bio_list_pop(tmp)))\n\t\tsubmit_bio_noacct(bio);\n}\n\nstatic int cmp_stripe(void *priv, const struct list_head *a,\n\t\t      const struct list_head *b)\n{\n\tconst struct r5pending_data *da = list_entry(a,\n\t\t\t\tstruct r5pending_data, sibling);\n\tconst struct r5pending_data *db = list_entry(b,\n\t\t\t\tstruct r5pending_data, sibling);\n\tif (da->sector > db->sector)\n\t\treturn 1;\n\tif (da->sector < db->sector)\n\t\treturn -1;\n\treturn 0;\n}\n\nstatic void dispatch_defer_bios(struct r5conf *conf, int target,\n\t\t\t\tstruct bio_list *list)\n{\n\tstruct r5pending_data *data;\n\tstruct list_head *first, *next = NULL;\n\tint cnt = 0;\n\n\tif (conf->pending_data_cnt == 0)\n\t\treturn;\n\n\tlist_sort(NULL, &conf->pending_list, cmp_stripe);\n\n\tfirst = conf->pending_list.next;\n\n\t \n\tif (conf->next_pending_data)\n\t\tlist_move_tail(&conf->pending_list,\n\t\t\t\t&conf->next_pending_data->sibling);\n\n\twhile (!list_empty(&conf->pending_list)) {\n\t\tdata = list_first_entry(&conf->pending_list,\n\t\t\tstruct r5pending_data, sibling);\n\t\tif (&data->sibling == first)\n\t\t\tfirst = data->sibling.next;\n\t\tnext = data->sibling.next;\n\n\t\tbio_list_merge(list, &data->bios);\n\t\tlist_move(&data->sibling, &conf->free_list);\n\t\tcnt++;\n\t\tif (cnt >= target)\n\t\t\tbreak;\n\t}\n\tconf->pending_data_cnt -= cnt;\n\tBUG_ON(conf->pending_data_cnt < 0 || cnt < target);\n\n\tif (next != &conf->pending_list)\n\t\tconf->next_pending_data = list_entry(next,\n\t\t\t\tstruct r5pending_data, sibling);\n\telse\n\t\tconf->next_pending_data = NULL;\n\t \n\tif (first != &conf->pending_list)\n\t\tlist_move_tail(&conf->pending_list, first);\n}\n\nstatic void flush_deferred_bios(struct r5conf *conf)\n{\n\tstruct bio_list tmp = BIO_EMPTY_LIST;\n\n\tif (conf->pending_data_cnt == 0)\n\t\treturn;\n\n\tspin_lock(&conf->pending_bios_lock);\n\tdispatch_defer_bios(conf, conf->pending_data_cnt, &tmp);\n\tBUG_ON(conf->pending_data_cnt != 0);\n\tspin_unlock(&conf->pending_bios_lock);\n\n\tdispatch_bio_list(&tmp);\n}\n\nstatic void defer_issue_bios(struct r5conf *conf, sector_t sector,\n\t\t\t\tstruct bio_list *bios)\n{\n\tstruct bio_list tmp = BIO_EMPTY_LIST;\n\tstruct r5pending_data *ent;\n\n\tspin_lock(&conf->pending_bios_lock);\n\tent = list_first_entry(&conf->free_list, struct r5pending_data,\n\t\t\t\t\t\t\tsibling);\n\tlist_move_tail(&ent->sibling, &conf->pending_list);\n\tent->sector = sector;\n\tbio_list_init(&ent->bios);\n\tbio_list_merge(&ent->bios, bios);\n\tconf->pending_data_cnt++;\n\tif (conf->pending_data_cnt >= PENDING_IO_MAX)\n\t\tdispatch_defer_bios(conf, PENDING_IO_ONE_FLUSH, &tmp);\n\n\tspin_unlock(&conf->pending_bios_lock);\n\n\tdispatch_bio_list(&tmp);\n}\n\nstatic void\nraid5_end_read_request(struct bio *bi);\nstatic void\nraid5_end_write_request(struct bio *bi);\n\nstatic void ops_run_io(struct stripe_head *sh, struct stripe_head_state *s)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tint i, disks = sh->disks;\n\tstruct stripe_head *head_sh = sh;\n\tstruct bio_list pending_bios = BIO_EMPTY_LIST;\n\tstruct r5dev *dev;\n\tbool should_defer;\n\n\tmight_sleep();\n\n\tif (log_stripe(sh, s) == 0)\n\t\treturn;\n\n\tshould_defer = conf->batch_bio_dispatch && conf->group_cnt;\n\n\tfor (i = disks; i--; ) {\n\t\tenum req_op op;\n\t\tblk_opf_t op_flags = 0;\n\t\tint replace_only = 0;\n\t\tstruct bio *bi, *rbi;\n\t\tstruct md_rdev *rdev, *rrdev = NULL;\n\n\t\tsh = head_sh;\n\t\tif (test_and_clear_bit(R5_Wantwrite, &sh->dev[i].flags)) {\n\t\t\top = REQ_OP_WRITE;\n\t\t\tif (test_and_clear_bit(R5_WantFUA, &sh->dev[i].flags))\n\t\t\t\top_flags = REQ_FUA;\n\t\t\tif (test_bit(R5_Discard, &sh->dev[i].flags))\n\t\t\t\top = REQ_OP_DISCARD;\n\t\t} else if (test_and_clear_bit(R5_Wantread, &sh->dev[i].flags))\n\t\t\top = REQ_OP_READ;\n\t\telse if (test_and_clear_bit(R5_WantReplace,\n\t\t\t\t\t    &sh->dev[i].flags)) {\n\t\t\top = REQ_OP_WRITE;\n\t\t\treplace_only = 1;\n\t\t} else\n\t\t\tcontinue;\n\t\tif (test_and_clear_bit(R5_SyncIO, &sh->dev[i].flags))\n\t\t\top_flags |= REQ_SYNC;\n\nagain:\n\t\tdev = &sh->dev[i];\n\t\tbi = &dev->req;\n\t\trbi = &dev->rreq;  \n\n\t\trcu_read_lock();\n\t\trrdev = rcu_dereference(conf->disks[i].replacement);\n\t\tsmp_mb();  \n\t\trdev = rcu_dereference(conf->disks[i].rdev);\n\t\tif (!rdev) {\n\t\t\trdev = rrdev;\n\t\t\trrdev = NULL;\n\t\t}\n\t\tif (op_is_write(op)) {\n\t\t\tif (replace_only)\n\t\t\t\trdev = NULL;\n\t\t\tif (rdev == rrdev)\n\t\t\t\t \n\t\t\t\trrdev = NULL;\n\t\t} else {\n\t\t\tif (test_bit(R5_ReadRepl, &head_sh->dev[i].flags) && rrdev)\n\t\t\t\trdev = rrdev;\n\t\t\trrdev = NULL;\n\t\t}\n\n\t\tif (rdev && test_bit(Faulty, &rdev->flags))\n\t\t\trdev = NULL;\n\t\tif (rdev)\n\t\t\tatomic_inc(&rdev->nr_pending);\n\t\tif (rrdev && test_bit(Faulty, &rrdev->flags))\n\t\t\trrdev = NULL;\n\t\tif (rrdev)\n\t\t\tatomic_inc(&rrdev->nr_pending);\n\t\trcu_read_unlock();\n\n\t\t \n\t\twhile (op_is_write(op) && rdev &&\n\t\t       test_bit(WriteErrorSeen, &rdev->flags)) {\n\t\t\tsector_t first_bad;\n\t\t\tint bad_sectors;\n\t\t\tint bad = is_badblock(rdev, sh->sector, RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t\t      &first_bad, &bad_sectors);\n\t\t\tif (!bad)\n\t\t\t\tbreak;\n\n\t\t\tif (bad < 0) {\n\t\t\t\tset_bit(BlockedBadBlocks, &rdev->flags);\n\t\t\t\tif (!conf->mddev->external &&\n\t\t\t\t    conf->mddev->sb_flags) {\n\t\t\t\t\t \n\t\t\t\t\tmd_check_recovery(conf->mddev);\n\t\t\t\t}\n\t\t\t\t \n\t\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\t\tmd_wait_for_blocked_rdev(rdev, conf->mddev);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\trdev_dec_pending(rdev, conf->mddev);\n\t\t\t\trdev = NULL;\n\t\t\t}\n\t\t}\n\n\t\tif (rdev) {\n\t\t\tif (s->syncing || s->expanding || s->expanded\n\t\t\t    || s->replacing)\n\t\t\t\tmd_sync_acct(rdev->bdev, RAID5_STRIPE_SECTORS(conf));\n\n\t\t\tset_bit(STRIPE_IO_STARTED, &sh->state);\n\n\t\t\tbio_init(bi, rdev->bdev, &dev->vec, 1, op | op_flags);\n\t\t\tbi->bi_end_io = op_is_write(op)\n\t\t\t\t? raid5_end_write_request\n\t\t\t\t: raid5_end_read_request;\n\t\t\tbi->bi_private = sh;\n\n\t\t\tpr_debug(\"%s: for %llu schedule op %d on disc %d\\n\",\n\t\t\t\t__func__, (unsigned long long)sh->sector,\n\t\t\t\tbi->bi_opf, i);\n\t\t\tatomic_inc(&sh->count);\n\t\t\tif (sh != head_sh)\n\t\t\t\tatomic_inc(&head_sh->count);\n\t\t\tif (use_new_offset(conf, sh))\n\t\t\t\tbi->bi_iter.bi_sector = (sh->sector\n\t\t\t\t\t\t + rdev->new_data_offset);\n\t\t\telse\n\t\t\t\tbi->bi_iter.bi_sector = (sh->sector\n\t\t\t\t\t\t + rdev->data_offset);\n\t\t\tif (test_bit(R5_ReadNoMerge, &head_sh->dev[i].flags))\n\t\t\t\tbi->bi_opf |= REQ_NOMERGE;\n\n\t\t\tif (test_bit(R5_SkipCopy, &sh->dev[i].flags))\n\t\t\t\tWARN_ON(test_bit(R5_UPTODATE, &sh->dev[i].flags));\n\n\t\t\tif (!op_is_write(op) &&\n\t\t\t    test_bit(R5_InJournal, &sh->dev[i].flags))\n\t\t\t\t \n\t\t\t\tsh->dev[i].vec.bv_page = sh->dev[i].orig_page;\n\t\t\telse\n\t\t\t\tsh->dev[i].vec.bv_page = sh->dev[i].page;\n\t\t\tbi->bi_vcnt = 1;\n\t\t\tbi->bi_io_vec[0].bv_len = RAID5_STRIPE_SIZE(conf);\n\t\t\tbi->bi_io_vec[0].bv_offset = sh->dev[i].offset;\n\t\t\tbi->bi_iter.bi_size = RAID5_STRIPE_SIZE(conf);\n\t\t\t \n\t\t\tif (op == REQ_OP_DISCARD)\n\t\t\t\tbi->bi_vcnt = 0;\n\t\t\tif (rrdev)\n\t\t\t\tset_bit(R5_DOUBLE_LOCKED, &sh->dev[i].flags);\n\n\t\t\tif (conf->mddev->gendisk)\n\t\t\t\ttrace_block_bio_remap(bi,\n\t\t\t\t\t\tdisk_devt(conf->mddev->gendisk),\n\t\t\t\t\t\tsh->dev[i].sector);\n\t\t\tif (should_defer && op_is_write(op))\n\t\t\t\tbio_list_add(&pending_bios, bi);\n\t\t\telse\n\t\t\t\tsubmit_bio_noacct(bi);\n\t\t}\n\t\tif (rrdev) {\n\t\t\tif (s->syncing || s->expanding || s->expanded\n\t\t\t    || s->replacing)\n\t\t\t\tmd_sync_acct(rrdev->bdev, RAID5_STRIPE_SECTORS(conf));\n\n\t\t\tset_bit(STRIPE_IO_STARTED, &sh->state);\n\n\t\t\tbio_init(rbi, rrdev->bdev, &dev->rvec, 1, op | op_flags);\n\t\t\tBUG_ON(!op_is_write(op));\n\t\t\trbi->bi_end_io = raid5_end_write_request;\n\t\t\trbi->bi_private = sh;\n\n\t\t\tpr_debug(\"%s: for %llu schedule op %d on \"\n\t\t\t\t \"replacement disc %d\\n\",\n\t\t\t\t__func__, (unsigned long long)sh->sector,\n\t\t\t\trbi->bi_opf, i);\n\t\t\tatomic_inc(&sh->count);\n\t\t\tif (sh != head_sh)\n\t\t\t\tatomic_inc(&head_sh->count);\n\t\t\tif (use_new_offset(conf, sh))\n\t\t\t\trbi->bi_iter.bi_sector = (sh->sector\n\t\t\t\t\t\t  + rrdev->new_data_offset);\n\t\t\telse\n\t\t\t\trbi->bi_iter.bi_sector = (sh->sector\n\t\t\t\t\t\t  + rrdev->data_offset);\n\t\t\tif (test_bit(R5_SkipCopy, &sh->dev[i].flags))\n\t\t\t\tWARN_ON(test_bit(R5_UPTODATE, &sh->dev[i].flags));\n\t\t\tsh->dev[i].rvec.bv_page = sh->dev[i].page;\n\t\t\trbi->bi_vcnt = 1;\n\t\t\trbi->bi_io_vec[0].bv_len = RAID5_STRIPE_SIZE(conf);\n\t\t\trbi->bi_io_vec[0].bv_offset = sh->dev[i].offset;\n\t\t\trbi->bi_iter.bi_size = RAID5_STRIPE_SIZE(conf);\n\t\t\t \n\t\t\tif (op == REQ_OP_DISCARD)\n\t\t\t\trbi->bi_vcnt = 0;\n\t\t\tif (conf->mddev->gendisk)\n\t\t\t\ttrace_block_bio_remap(rbi,\n\t\t\t\t\t\tdisk_devt(conf->mddev->gendisk),\n\t\t\t\t\t\tsh->dev[i].sector);\n\t\t\tif (should_defer && op_is_write(op))\n\t\t\t\tbio_list_add(&pending_bios, rbi);\n\t\t\telse\n\t\t\t\tsubmit_bio_noacct(rbi);\n\t\t}\n\t\tif (!rdev && !rrdev) {\n\t\t\tif (op_is_write(op))\n\t\t\t\tset_bit(STRIPE_DEGRADED, &sh->state);\n\t\t\tpr_debug(\"skip op %d on disc %d for sector %llu\\n\",\n\t\t\t\tbi->bi_opf, i, (unsigned long long)sh->sector);\n\t\t\tclear_bit(R5_LOCKED, &sh->dev[i].flags);\n\t\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\t}\n\n\t\tif (!head_sh->batch_head)\n\t\t\tcontinue;\n\t\tsh = list_first_entry(&sh->batch_list, struct stripe_head,\n\t\t\t\t      batch_list);\n\t\tif (sh != head_sh)\n\t\t\tgoto again;\n\t}\n\n\tif (should_defer && !bio_list_empty(&pending_bios))\n\t\tdefer_issue_bios(conf, head_sh->sector, &pending_bios);\n}\n\nstatic struct dma_async_tx_descriptor *\nasync_copy_data(int frombio, struct bio *bio, struct page **page,\n\tunsigned int poff, sector_t sector, struct dma_async_tx_descriptor *tx,\n\tstruct stripe_head *sh, int no_skipcopy)\n{\n\tstruct bio_vec bvl;\n\tstruct bvec_iter iter;\n\tstruct page *bio_page;\n\tint page_offset;\n\tstruct async_submit_ctl submit;\n\tenum async_tx_flags flags = 0;\n\tstruct r5conf *conf = sh->raid_conf;\n\n\tif (bio->bi_iter.bi_sector >= sector)\n\t\tpage_offset = (signed)(bio->bi_iter.bi_sector - sector) * 512;\n\telse\n\t\tpage_offset = (signed)(sector - bio->bi_iter.bi_sector) * -512;\n\n\tif (frombio)\n\t\tflags |= ASYNC_TX_FENCE;\n\tinit_async_submit(&submit, flags, tx, NULL, NULL, NULL);\n\n\tbio_for_each_segment(bvl, bio, iter) {\n\t\tint len = bvl.bv_len;\n\t\tint clen;\n\t\tint b_offset = 0;\n\n\t\tif (page_offset < 0) {\n\t\t\tb_offset = -page_offset;\n\t\t\tpage_offset += b_offset;\n\t\t\tlen -= b_offset;\n\t\t}\n\n\t\tif (len > 0 && page_offset + len > RAID5_STRIPE_SIZE(conf))\n\t\t\tclen = RAID5_STRIPE_SIZE(conf) - page_offset;\n\t\telse\n\t\t\tclen = len;\n\n\t\tif (clen > 0) {\n\t\t\tb_offset += bvl.bv_offset;\n\t\t\tbio_page = bvl.bv_page;\n\t\t\tif (frombio) {\n\t\t\t\tif (conf->skip_copy &&\n\t\t\t\t    b_offset == 0 && page_offset == 0 &&\n\t\t\t\t    clen == RAID5_STRIPE_SIZE(conf) &&\n\t\t\t\t    !no_skipcopy)\n\t\t\t\t\t*page = bio_page;\n\t\t\t\telse\n\t\t\t\t\ttx = async_memcpy(*page, bio_page, page_offset + poff,\n\t\t\t\t\t\t  b_offset, clen, &submit);\n\t\t\t} else\n\t\t\t\ttx = async_memcpy(bio_page, *page, b_offset,\n\t\t\t\t\t\t  page_offset + poff, clen, &submit);\n\t\t}\n\t\t \n\t\tsubmit.depend_tx = tx;\n\n\t\tif (clen < len)  \n\t\t\tbreak;\n\t\tpage_offset +=  len;\n\t}\n\n\treturn tx;\n}\n\nstatic void ops_complete_biofill(void *stripe_head_ref)\n{\n\tstruct stripe_head *sh = stripe_head_ref;\n\tint i;\n\tstruct r5conf *conf = sh->raid_conf;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\t \n\tfor (i = sh->disks; i--; ) {\n\t\tstruct r5dev *dev = &sh->dev[i];\n\n\t\t \n\t\t \n\t\tif (test_and_clear_bit(R5_Wantfill, &dev->flags)) {\n\t\t\tstruct bio *rbi, *rbi2;\n\n\t\t\tBUG_ON(!dev->read);\n\t\t\trbi = dev->read;\n\t\t\tdev->read = NULL;\n\t\t\twhile (rbi && rbi->bi_iter.bi_sector <\n\t\t\t\tdev->sector + RAID5_STRIPE_SECTORS(conf)) {\n\t\t\t\trbi2 = r5_next_bio(conf, rbi, dev->sector);\n\t\t\t\tbio_endio(rbi);\n\t\t\t\trbi = rbi2;\n\t\t\t}\n\t\t}\n\t}\n\tclear_bit(STRIPE_BIOFILL_RUN, &sh->state);\n\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\traid5_release_stripe(sh);\n}\n\nstatic void ops_run_biofill(struct stripe_head *sh)\n{\n\tstruct dma_async_tx_descriptor *tx = NULL;\n\tstruct async_submit_ctl submit;\n\tint i;\n\tstruct r5conf *conf = sh->raid_conf;\n\n\tBUG_ON(sh->batch_head);\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tfor (i = sh->disks; i--; ) {\n\t\tstruct r5dev *dev = &sh->dev[i];\n\t\tif (test_bit(R5_Wantfill, &dev->flags)) {\n\t\t\tstruct bio *rbi;\n\t\t\tspin_lock_irq(&sh->stripe_lock);\n\t\t\tdev->read = rbi = dev->toread;\n\t\t\tdev->toread = NULL;\n\t\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\t\twhile (rbi && rbi->bi_iter.bi_sector <\n\t\t\t\tdev->sector + RAID5_STRIPE_SECTORS(conf)) {\n\t\t\t\ttx = async_copy_data(0, rbi, &dev->page,\n\t\t\t\t\t\t     dev->offset,\n\t\t\t\t\t\t     dev->sector, tx, sh, 0);\n\t\t\t\trbi = r5_next_bio(conf, rbi, dev->sector);\n\t\t\t}\n\t\t}\n\t}\n\n\tatomic_inc(&sh->count);\n\tinit_async_submit(&submit, ASYNC_TX_ACK, tx, ops_complete_biofill, sh, NULL);\n\tasync_trigger_callback(&submit);\n}\n\nstatic void mark_target_uptodate(struct stripe_head *sh, int target)\n{\n\tstruct r5dev *tgt;\n\n\tif (target < 0)\n\t\treturn;\n\n\ttgt = &sh->dev[target];\n\tset_bit(R5_UPTODATE, &tgt->flags);\n\tBUG_ON(!test_bit(R5_Wantcompute, &tgt->flags));\n\tclear_bit(R5_Wantcompute, &tgt->flags);\n}\n\nstatic void ops_complete_compute(void *stripe_head_ref)\n{\n\tstruct stripe_head *sh = stripe_head_ref;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\t \n\tmark_target_uptodate(sh, sh->ops.target);\n\tmark_target_uptodate(sh, sh->ops.target2);\n\n\tclear_bit(STRIPE_COMPUTE_RUN, &sh->state);\n\tif (sh->check_state == check_state_compute_run)\n\t\tsh->check_state = check_state_compute_result;\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\traid5_release_stripe(sh);\n}\n\n \nstatic struct page **to_addr_page(struct raid5_percpu *percpu, int i)\n{\n\treturn percpu->scribble + i * percpu->scribble_obj_size;\n}\n\n \nstatic addr_conv_t *to_addr_conv(struct stripe_head *sh,\n\t\t\t\t struct raid5_percpu *percpu, int i)\n{\n\treturn (void *) (to_addr_page(percpu, i) + sh->disks + 2);\n}\n\n \nstatic unsigned int *\nto_addr_offs(struct stripe_head *sh, struct raid5_percpu *percpu)\n{\n\treturn (unsigned int *) (to_addr_conv(sh, percpu, 0) + sh->disks + 2);\n}\n\nstatic struct dma_async_tx_descriptor *\nops_run_compute5(struct stripe_head *sh, struct raid5_percpu *percpu)\n{\n\tint disks = sh->disks;\n\tstruct page **xor_srcs = to_addr_page(percpu, 0);\n\tunsigned int *off_srcs = to_addr_offs(sh, percpu);\n\tint target = sh->ops.target;\n\tstruct r5dev *tgt = &sh->dev[target];\n\tstruct page *xor_dest = tgt->page;\n\tunsigned int off_dest = tgt->offset;\n\tint count = 0;\n\tstruct dma_async_tx_descriptor *tx;\n\tstruct async_submit_ctl submit;\n\tint i;\n\n\tBUG_ON(sh->batch_head);\n\n\tpr_debug(\"%s: stripe %llu block: %d\\n\",\n\t\t__func__, (unsigned long long)sh->sector, target);\n\tBUG_ON(!test_bit(R5_Wantcompute, &tgt->flags));\n\n\tfor (i = disks; i--; ) {\n\t\tif (i != target) {\n\t\t\toff_srcs[count] = sh->dev[i].offset;\n\t\t\txor_srcs[count++] = sh->dev[i].page;\n\t\t}\n\t}\n\n\tatomic_inc(&sh->count);\n\n\tinit_async_submit(&submit, ASYNC_TX_FENCE|ASYNC_TX_XOR_ZERO_DST, NULL,\n\t\t\t  ops_complete_compute, sh, to_addr_conv(sh, percpu, 0));\n\tif (unlikely(count == 1))\n\t\ttx = async_memcpy(xor_dest, xor_srcs[0], off_dest, off_srcs[0],\n\t\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf), &submit);\n\telse\n\t\ttx = async_xor_offs(xor_dest, off_dest, xor_srcs, off_srcs, count,\n\t\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf), &submit);\n\n\treturn tx;\n}\n\n \nstatic int set_syndrome_sources(struct page **srcs,\n\t\t\t\tunsigned int *offs,\n\t\t\t\tstruct stripe_head *sh,\n\t\t\t\tint srctype)\n{\n\tint disks = sh->disks;\n\tint syndrome_disks = sh->ddf_layout ? disks : (disks - 2);\n\tint d0_idx = raid6_d0(sh);\n\tint count;\n\tint i;\n\n\tfor (i = 0; i < disks; i++)\n\t\tsrcs[i] = NULL;\n\n\tcount = 0;\n\ti = d0_idx;\n\tdo {\n\t\tint slot = raid6_idx_to_slot(i, sh, &count, syndrome_disks);\n\t\tstruct r5dev *dev = &sh->dev[i];\n\n\t\tif (i == sh->qd_idx || i == sh->pd_idx ||\n\t\t    (srctype == SYNDROME_SRC_ALL) ||\n\t\t    (srctype == SYNDROME_SRC_WANT_DRAIN &&\n\t\t     (test_bit(R5_Wantdrain, &dev->flags) ||\n\t\t      test_bit(R5_InJournal, &dev->flags))) ||\n\t\t    (srctype == SYNDROME_SRC_WRITTEN &&\n\t\t     (dev->written ||\n\t\t      test_bit(R5_InJournal, &dev->flags)))) {\n\t\t\tif (test_bit(R5_InJournal, &dev->flags))\n\t\t\t\tsrcs[slot] = sh->dev[i].orig_page;\n\t\t\telse\n\t\t\t\tsrcs[slot] = sh->dev[i].page;\n\t\t\t \n\t\t\toffs[slot] = sh->dev[i].offset;\n\t\t}\n\t\ti = raid6_next_disk(i, disks);\n\t} while (i != d0_idx);\n\n\treturn syndrome_disks;\n}\n\nstatic struct dma_async_tx_descriptor *\nops_run_compute6_1(struct stripe_head *sh, struct raid5_percpu *percpu)\n{\n\tint disks = sh->disks;\n\tstruct page **blocks = to_addr_page(percpu, 0);\n\tunsigned int *offs = to_addr_offs(sh, percpu);\n\tint target;\n\tint qd_idx = sh->qd_idx;\n\tstruct dma_async_tx_descriptor *tx;\n\tstruct async_submit_ctl submit;\n\tstruct r5dev *tgt;\n\tstruct page *dest;\n\tunsigned int dest_off;\n\tint i;\n\tint count;\n\n\tBUG_ON(sh->batch_head);\n\tif (sh->ops.target < 0)\n\t\ttarget = sh->ops.target2;\n\telse if (sh->ops.target2 < 0)\n\t\ttarget = sh->ops.target;\n\telse\n\t\t \n\t\tBUG();\n\tBUG_ON(target < 0);\n\tpr_debug(\"%s: stripe %llu block: %d\\n\",\n\t\t__func__, (unsigned long long)sh->sector, target);\n\n\ttgt = &sh->dev[target];\n\tBUG_ON(!test_bit(R5_Wantcompute, &tgt->flags));\n\tdest = tgt->page;\n\tdest_off = tgt->offset;\n\n\tatomic_inc(&sh->count);\n\n\tif (target == qd_idx) {\n\t\tcount = set_syndrome_sources(blocks, offs, sh, SYNDROME_SRC_ALL);\n\t\tblocks[count] = NULL;  \n\t\tBUG_ON(blocks[count+1] != dest);  \n\t\tinit_async_submit(&submit, ASYNC_TX_FENCE, NULL,\n\t\t\t\t  ops_complete_compute, sh,\n\t\t\t\t  to_addr_conv(sh, percpu, 0));\n\t\ttx = async_gen_syndrome(blocks, offs, count+2,\n\t\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf), &submit);\n\t} else {\n\t\t \n\t\tcount = 0;\n\t\tfor (i = disks; i-- ; ) {\n\t\t\tif (i == target || i == qd_idx)\n\t\t\t\tcontinue;\n\t\t\toffs[count] = sh->dev[i].offset;\n\t\t\tblocks[count++] = sh->dev[i].page;\n\t\t}\n\n\t\tinit_async_submit(&submit, ASYNC_TX_FENCE|ASYNC_TX_XOR_ZERO_DST,\n\t\t\t\t  NULL, ops_complete_compute, sh,\n\t\t\t\t  to_addr_conv(sh, percpu, 0));\n\t\ttx = async_xor_offs(dest, dest_off, blocks, offs, count,\n\t\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf), &submit);\n\t}\n\n\treturn tx;\n}\n\nstatic struct dma_async_tx_descriptor *\nops_run_compute6_2(struct stripe_head *sh, struct raid5_percpu *percpu)\n{\n\tint i, count, disks = sh->disks;\n\tint syndrome_disks = sh->ddf_layout ? disks : disks-2;\n\tint d0_idx = raid6_d0(sh);\n\tint faila = -1, failb = -1;\n\tint target = sh->ops.target;\n\tint target2 = sh->ops.target2;\n\tstruct r5dev *tgt = &sh->dev[target];\n\tstruct r5dev *tgt2 = &sh->dev[target2];\n\tstruct dma_async_tx_descriptor *tx;\n\tstruct page **blocks = to_addr_page(percpu, 0);\n\tunsigned int *offs = to_addr_offs(sh, percpu);\n\tstruct async_submit_ctl submit;\n\n\tBUG_ON(sh->batch_head);\n\tpr_debug(\"%s: stripe %llu block1: %d block2: %d\\n\",\n\t\t __func__, (unsigned long long)sh->sector, target, target2);\n\tBUG_ON(target < 0 || target2 < 0);\n\tBUG_ON(!test_bit(R5_Wantcompute, &tgt->flags));\n\tBUG_ON(!test_bit(R5_Wantcompute, &tgt2->flags));\n\n\t \n\tfor (i = 0; i < disks ; i++) {\n\t\toffs[i] = 0;\n\t\tblocks[i] = NULL;\n\t}\n\tcount = 0;\n\ti = d0_idx;\n\tdo {\n\t\tint slot = raid6_idx_to_slot(i, sh, &count, syndrome_disks);\n\n\t\toffs[slot] = sh->dev[i].offset;\n\t\tblocks[slot] = sh->dev[i].page;\n\n\t\tif (i == target)\n\t\t\tfaila = slot;\n\t\tif (i == target2)\n\t\t\tfailb = slot;\n\t\ti = raid6_next_disk(i, disks);\n\t} while (i != d0_idx);\n\n\tBUG_ON(faila == failb);\n\tif (failb < faila)\n\t\tswap(faila, failb);\n\tpr_debug(\"%s: stripe: %llu faila: %d failb: %d\\n\",\n\t\t __func__, (unsigned long long)sh->sector, faila, failb);\n\n\tatomic_inc(&sh->count);\n\n\tif (failb == syndrome_disks+1) {\n\t\t \n\t\tif (faila == syndrome_disks) {\n\t\t\t \n\t\t\tinit_async_submit(&submit, ASYNC_TX_FENCE, NULL,\n\t\t\t\t\t  ops_complete_compute, sh,\n\t\t\t\t\t  to_addr_conv(sh, percpu, 0));\n\t\t\treturn async_gen_syndrome(blocks, offs, syndrome_disks+2,\n\t\t\t\t\t\t  RAID5_STRIPE_SIZE(sh->raid_conf),\n\t\t\t\t\t\t  &submit);\n\t\t} else {\n\t\t\tstruct page *dest;\n\t\t\tunsigned int dest_off;\n\t\t\tint data_target;\n\t\t\tint qd_idx = sh->qd_idx;\n\n\t\t\t \n\t\t\tif (target == qd_idx)\n\t\t\t\tdata_target = target2;\n\t\t\telse\n\t\t\t\tdata_target = target;\n\n\t\t\tcount = 0;\n\t\t\tfor (i = disks; i-- ; ) {\n\t\t\t\tif (i == data_target || i == qd_idx)\n\t\t\t\t\tcontinue;\n\t\t\t\toffs[count] = sh->dev[i].offset;\n\t\t\t\tblocks[count++] = sh->dev[i].page;\n\t\t\t}\n\t\t\tdest = sh->dev[data_target].page;\n\t\t\tdest_off = sh->dev[data_target].offset;\n\t\t\tinit_async_submit(&submit,\n\t\t\t\t\t  ASYNC_TX_FENCE|ASYNC_TX_XOR_ZERO_DST,\n\t\t\t\t\t  NULL, NULL, NULL,\n\t\t\t\t\t  to_addr_conv(sh, percpu, 0));\n\t\t\ttx = async_xor_offs(dest, dest_off, blocks, offs, count,\n\t\t\t\t       RAID5_STRIPE_SIZE(sh->raid_conf),\n\t\t\t\t       &submit);\n\n\t\t\tcount = set_syndrome_sources(blocks, offs, sh, SYNDROME_SRC_ALL);\n\t\t\tinit_async_submit(&submit, ASYNC_TX_FENCE, tx,\n\t\t\t\t\t  ops_complete_compute, sh,\n\t\t\t\t\t  to_addr_conv(sh, percpu, 0));\n\t\t\treturn async_gen_syndrome(blocks, offs, count+2,\n\t\t\t\t\t\t  RAID5_STRIPE_SIZE(sh->raid_conf),\n\t\t\t\t\t\t  &submit);\n\t\t}\n\t} else {\n\t\tinit_async_submit(&submit, ASYNC_TX_FENCE, NULL,\n\t\t\t\t  ops_complete_compute, sh,\n\t\t\t\t  to_addr_conv(sh, percpu, 0));\n\t\tif (failb == syndrome_disks) {\n\t\t\t \n\t\t\treturn async_raid6_datap_recov(syndrome_disks+2,\n\t\t\t\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf),\n\t\t\t\t\t\tfaila,\n\t\t\t\t\t\tblocks, offs, &submit);\n\t\t} else {\n\t\t\t \n\t\t\treturn async_raid6_2data_recov(syndrome_disks+2,\n\t\t\t\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf),\n\t\t\t\t\t\tfaila, failb,\n\t\t\t\t\t\tblocks, offs, &submit);\n\t\t}\n\t}\n}\n\nstatic void ops_complete_prexor(void *stripe_head_ref)\n{\n\tstruct stripe_head *sh = stripe_head_ref;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tif (r5c_is_writeback(sh->raid_conf->log))\n\t\t \n\t\tr5c_release_extra_page(sh);\n}\n\nstatic struct dma_async_tx_descriptor *\nops_run_prexor5(struct stripe_head *sh, struct raid5_percpu *percpu,\n\t\tstruct dma_async_tx_descriptor *tx)\n{\n\tint disks = sh->disks;\n\tstruct page **xor_srcs = to_addr_page(percpu, 0);\n\tunsigned int *off_srcs = to_addr_offs(sh, percpu);\n\tint count = 0, pd_idx = sh->pd_idx, i;\n\tstruct async_submit_ctl submit;\n\n\t \n\tunsigned int off_dest = off_srcs[count] = sh->dev[pd_idx].offset;\n\tstruct page *xor_dest = xor_srcs[count++] = sh->dev[pd_idx].page;\n\n\tBUG_ON(sh->batch_head);\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tfor (i = disks; i--; ) {\n\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t \n\t\tif (test_bit(R5_InJournal, &dev->flags)) {\n\t\t\t \n\t\t\toff_srcs[count] = dev->offset;\n\t\t\txor_srcs[count++] = dev->orig_page;\n\t\t} else if (test_bit(R5_Wantdrain, &dev->flags)) {\n\t\t\toff_srcs[count] = dev->offset;\n\t\t\txor_srcs[count++] = dev->page;\n\t\t}\n\t}\n\n\tinit_async_submit(&submit, ASYNC_TX_FENCE|ASYNC_TX_XOR_DROP_DST, tx,\n\t\t\t  ops_complete_prexor, sh, to_addr_conv(sh, percpu, 0));\n\ttx = async_xor_offs(xor_dest, off_dest, xor_srcs, off_srcs, count,\n\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf), &submit);\n\n\treturn tx;\n}\n\nstatic struct dma_async_tx_descriptor *\nops_run_prexor6(struct stripe_head *sh, struct raid5_percpu *percpu,\n\t\tstruct dma_async_tx_descriptor *tx)\n{\n\tstruct page **blocks = to_addr_page(percpu, 0);\n\tunsigned int *offs = to_addr_offs(sh, percpu);\n\tint count;\n\tstruct async_submit_ctl submit;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tcount = set_syndrome_sources(blocks, offs, sh, SYNDROME_SRC_WANT_DRAIN);\n\n\tinit_async_submit(&submit, ASYNC_TX_FENCE|ASYNC_TX_PQ_XOR_DST, tx,\n\t\t\t  ops_complete_prexor, sh, to_addr_conv(sh, percpu, 0));\n\ttx = async_gen_syndrome(blocks, offs, count+2,\n\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf), &submit);\n\n\treturn tx;\n}\n\nstatic struct dma_async_tx_descriptor *\nops_run_biodrain(struct stripe_head *sh, struct dma_async_tx_descriptor *tx)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tint disks = sh->disks;\n\tint i;\n\tstruct stripe_head *head_sh = sh;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tfor (i = disks; i--; ) {\n\t\tstruct r5dev *dev;\n\t\tstruct bio *chosen;\n\n\t\tsh = head_sh;\n\t\tif (test_and_clear_bit(R5_Wantdrain, &head_sh->dev[i].flags)) {\n\t\t\tstruct bio *wbi;\n\nagain:\n\t\t\tdev = &sh->dev[i];\n\t\t\t \n\t\t\tclear_bit(R5_InJournal, &dev->flags);\n\t\t\tspin_lock_irq(&sh->stripe_lock);\n\t\t\tchosen = dev->towrite;\n\t\t\tdev->towrite = NULL;\n\t\t\tsh->overwrite_disks = 0;\n\t\t\tBUG_ON(dev->written);\n\t\t\twbi = dev->written = chosen;\n\t\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\t\tWARN_ON(dev->page != dev->orig_page);\n\n\t\t\twhile (wbi && wbi->bi_iter.bi_sector <\n\t\t\t\tdev->sector + RAID5_STRIPE_SECTORS(conf)) {\n\t\t\t\tif (wbi->bi_opf & REQ_FUA)\n\t\t\t\t\tset_bit(R5_WantFUA, &dev->flags);\n\t\t\t\tif (wbi->bi_opf & REQ_SYNC)\n\t\t\t\t\tset_bit(R5_SyncIO, &dev->flags);\n\t\t\t\tif (bio_op(wbi) == REQ_OP_DISCARD)\n\t\t\t\t\tset_bit(R5_Discard, &dev->flags);\n\t\t\t\telse {\n\t\t\t\t\ttx = async_copy_data(1, wbi, &dev->page,\n\t\t\t\t\t\t\t     dev->offset,\n\t\t\t\t\t\t\t     dev->sector, tx, sh,\n\t\t\t\t\t\t\t     r5c_is_writeback(conf->log));\n\t\t\t\t\tif (dev->page != dev->orig_page &&\n\t\t\t\t\t    !r5c_is_writeback(conf->log)) {\n\t\t\t\t\t\tset_bit(R5_SkipCopy, &dev->flags);\n\t\t\t\t\t\tclear_bit(R5_UPTODATE, &dev->flags);\n\t\t\t\t\t\tclear_bit(R5_OVERWRITE, &dev->flags);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\twbi = r5_next_bio(conf, wbi, dev->sector);\n\t\t\t}\n\n\t\t\tif (head_sh->batch_head) {\n\t\t\t\tsh = list_first_entry(&sh->batch_list,\n\t\t\t\t\t\t      struct stripe_head,\n\t\t\t\t\t\t      batch_list);\n\t\t\t\tif (sh == head_sh)\n\t\t\t\t\tcontinue;\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn tx;\n}\n\nstatic void ops_complete_reconstruct(void *stripe_head_ref)\n{\n\tstruct stripe_head *sh = stripe_head_ref;\n\tint disks = sh->disks;\n\tint pd_idx = sh->pd_idx;\n\tint qd_idx = sh->qd_idx;\n\tint i;\n\tbool fua = false, sync = false, discard = false;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tfor (i = disks; i--; ) {\n\t\tfua |= test_bit(R5_WantFUA, &sh->dev[i].flags);\n\t\tsync |= test_bit(R5_SyncIO, &sh->dev[i].flags);\n\t\tdiscard |= test_bit(R5_Discard, &sh->dev[i].flags);\n\t}\n\n\tfor (i = disks; i--; ) {\n\t\tstruct r5dev *dev = &sh->dev[i];\n\n\t\tif (dev->written || i == pd_idx || i == qd_idx) {\n\t\t\tif (!discard && !test_bit(R5_SkipCopy, &dev->flags)) {\n\t\t\t\tset_bit(R5_UPTODATE, &dev->flags);\n\t\t\t\tif (test_bit(STRIPE_EXPAND_READY, &sh->state))\n\t\t\t\t\tset_bit(R5_Expanded, &dev->flags);\n\t\t\t}\n\t\t\tif (fua)\n\t\t\t\tset_bit(R5_WantFUA, &dev->flags);\n\t\t\tif (sync)\n\t\t\t\tset_bit(R5_SyncIO, &dev->flags);\n\t\t}\n\t}\n\n\tif (sh->reconstruct_state == reconstruct_state_drain_run)\n\t\tsh->reconstruct_state = reconstruct_state_drain_result;\n\telse if (sh->reconstruct_state == reconstruct_state_prexor_drain_run)\n\t\tsh->reconstruct_state = reconstruct_state_prexor_drain_result;\n\telse {\n\t\tBUG_ON(sh->reconstruct_state != reconstruct_state_run);\n\t\tsh->reconstruct_state = reconstruct_state_result;\n\t}\n\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\traid5_release_stripe(sh);\n}\n\nstatic void\nops_run_reconstruct5(struct stripe_head *sh, struct raid5_percpu *percpu,\n\t\t     struct dma_async_tx_descriptor *tx)\n{\n\tint disks = sh->disks;\n\tstruct page **xor_srcs;\n\tunsigned int *off_srcs;\n\tstruct async_submit_ctl submit;\n\tint count, pd_idx = sh->pd_idx, i;\n\tstruct page *xor_dest;\n\tunsigned int off_dest;\n\tint prexor = 0;\n\tunsigned long flags;\n\tint j = 0;\n\tstruct stripe_head *head_sh = sh;\n\tint last_stripe;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tfor (i = 0; i < sh->disks; i++) {\n\t\tif (pd_idx == i)\n\t\t\tcontinue;\n\t\tif (!test_bit(R5_Discard, &sh->dev[i].flags))\n\t\t\tbreak;\n\t}\n\tif (i >= sh->disks) {\n\t\tatomic_inc(&sh->count);\n\t\tset_bit(R5_Discard, &sh->dev[pd_idx].flags);\n\t\tops_complete_reconstruct(sh);\n\t\treturn;\n\t}\nagain:\n\tcount = 0;\n\txor_srcs = to_addr_page(percpu, j);\n\toff_srcs = to_addr_offs(sh, percpu);\n\t \n\tif (head_sh->reconstruct_state == reconstruct_state_prexor_drain_run) {\n\t\tprexor = 1;\n\t\toff_dest = off_srcs[count] = sh->dev[pd_idx].offset;\n\t\txor_dest = xor_srcs[count++] = sh->dev[pd_idx].page;\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (head_sh->dev[i].written ||\n\t\t\t    test_bit(R5_InJournal, &head_sh->dev[i].flags)) {\n\t\t\t\toff_srcs[count] = dev->offset;\n\t\t\t\txor_srcs[count++] = dev->page;\n\t\t\t}\n\t\t}\n\t} else {\n\t\txor_dest = sh->dev[pd_idx].page;\n\t\toff_dest = sh->dev[pd_idx].offset;\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (i != pd_idx) {\n\t\t\t\toff_srcs[count] = dev->offset;\n\t\t\t\txor_srcs[count++] = dev->page;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tlast_stripe = !head_sh->batch_head ||\n\t\tlist_first_entry(&sh->batch_list,\n\t\t\t\t struct stripe_head, batch_list) == head_sh;\n\tif (last_stripe) {\n\t\tflags = ASYNC_TX_ACK |\n\t\t\t(prexor ? ASYNC_TX_XOR_DROP_DST : ASYNC_TX_XOR_ZERO_DST);\n\n\t\tatomic_inc(&head_sh->count);\n\t\tinit_async_submit(&submit, flags, tx, ops_complete_reconstruct, head_sh,\n\t\t\t\t  to_addr_conv(sh, percpu, j));\n\t} else {\n\t\tflags = prexor ? ASYNC_TX_XOR_DROP_DST : ASYNC_TX_XOR_ZERO_DST;\n\t\tinit_async_submit(&submit, flags, tx, NULL, NULL,\n\t\t\t\t  to_addr_conv(sh, percpu, j));\n\t}\n\n\tif (unlikely(count == 1))\n\t\ttx = async_memcpy(xor_dest, xor_srcs[0], off_dest, off_srcs[0],\n\t\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf), &submit);\n\telse\n\t\ttx = async_xor_offs(xor_dest, off_dest, xor_srcs, off_srcs, count,\n\t\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf), &submit);\n\tif (!last_stripe) {\n\t\tj++;\n\t\tsh = list_first_entry(&sh->batch_list, struct stripe_head,\n\t\t\t\t      batch_list);\n\t\tgoto again;\n\t}\n}\n\nstatic void\nops_run_reconstruct6(struct stripe_head *sh, struct raid5_percpu *percpu,\n\t\t     struct dma_async_tx_descriptor *tx)\n{\n\tstruct async_submit_ctl submit;\n\tstruct page **blocks;\n\tunsigned int *offs;\n\tint count, i, j = 0;\n\tstruct stripe_head *head_sh = sh;\n\tint last_stripe;\n\tint synflags;\n\tunsigned long txflags;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__, (unsigned long long)sh->sector);\n\n\tfor (i = 0; i < sh->disks; i++) {\n\t\tif (sh->pd_idx == i || sh->qd_idx == i)\n\t\t\tcontinue;\n\t\tif (!test_bit(R5_Discard, &sh->dev[i].flags))\n\t\t\tbreak;\n\t}\n\tif (i >= sh->disks) {\n\t\tatomic_inc(&sh->count);\n\t\tset_bit(R5_Discard, &sh->dev[sh->pd_idx].flags);\n\t\tset_bit(R5_Discard, &sh->dev[sh->qd_idx].flags);\n\t\tops_complete_reconstruct(sh);\n\t\treturn;\n\t}\n\nagain:\n\tblocks = to_addr_page(percpu, j);\n\toffs = to_addr_offs(sh, percpu);\n\n\tif (sh->reconstruct_state == reconstruct_state_prexor_drain_run) {\n\t\tsynflags = SYNDROME_SRC_WRITTEN;\n\t\ttxflags = ASYNC_TX_ACK | ASYNC_TX_PQ_XOR_DST;\n\t} else {\n\t\tsynflags = SYNDROME_SRC_ALL;\n\t\ttxflags = ASYNC_TX_ACK;\n\t}\n\n\tcount = set_syndrome_sources(blocks, offs, sh, synflags);\n\tlast_stripe = !head_sh->batch_head ||\n\t\tlist_first_entry(&sh->batch_list,\n\t\t\t\t struct stripe_head, batch_list) == head_sh;\n\n\tif (last_stripe) {\n\t\tatomic_inc(&head_sh->count);\n\t\tinit_async_submit(&submit, txflags, tx, ops_complete_reconstruct,\n\t\t\t\t  head_sh, to_addr_conv(sh, percpu, j));\n\t} else\n\t\tinit_async_submit(&submit, 0, tx, NULL, NULL,\n\t\t\t\t  to_addr_conv(sh, percpu, j));\n\ttx = async_gen_syndrome(blocks, offs, count+2,\n\t\t\tRAID5_STRIPE_SIZE(sh->raid_conf),  &submit);\n\tif (!last_stripe) {\n\t\tj++;\n\t\tsh = list_first_entry(&sh->batch_list, struct stripe_head,\n\t\t\t\t      batch_list);\n\t\tgoto again;\n\t}\n}\n\nstatic void ops_complete_check(void *stripe_head_ref)\n{\n\tstruct stripe_head *sh = stripe_head_ref;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tsh->check_state = check_state_check_result;\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\traid5_release_stripe(sh);\n}\n\nstatic void ops_run_check_p(struct stripe_head *sh, struct raid5_percpu *percpu)\n{\n\tint disks = sh->disks;\n\tint pd_idx = sh->pd_idx;\n\tint qd_idx = sh->qd_idx;\n\tstruct page *xor_dest;\n\tunsigned int off_dest;\n\tstruct page **xor_srcs = to_addr_page(percpu, 0);\n\tunsigned int *off_srcs = to_addr_offs(sh, percpu);\n\tstruct dma_async_tx_descriptor *tx;\n\tstruct async_submit_ctl submit;\n\tint count;\n\tint i;\n\n\tpr_debug(\"%s: stripe %llu\\n\", __func__,\n\t\t(unsigned long long)sh->sector);\n\n\tBUG_ON(sh->batch_head);\n\tcount = 0;\n\txor_dest = sh->dev[pd_idx].page;\n\toff_dest = sh->dev[pd_idx].offset;\n\toff_srcs[count] = off_dest;\n\txor_srcs[count++] = xor_dest;\n\tfor (i = disks; i--; ) {\n\t\tif (i == pd_idx || i == qd_idx)\n\t\t\tcontinue;\n\t\toff_srcs[count] = sh->dev[i].offset;\n\t\txor_srcs[count++] = sh->dev[i].page;\n\t}\n\n\tinit_async_submit(&submit, 0, NULL, NULL, NULL,\n\t\t\t  to_addr_conv(sh, percpu, 0));\n\ttx = async_xor_val_offs(xor_dest, off_dest, xor_srcs, off_srcs, count,\n\t\t\t   RAID5_STRIPE_SIZE(sh->raid_conf),\n\t\t\t   &sh->ops.zero_sum_result, &submit);\n\n\tatomic_inc(&sh->count);\n\tinit_async_submit(&submit, ASYNC_TX_ACK, tx, ops_complete_check, sh, NULL);\n\ttx = async_trigger_callback(&submit);\n}\n\nstatic void ops_run_check_pq(struct stripe_head *sh, struct raid5_percpu *percpu, int checkp)\n{\n\tstruct page **srcs = to_addr_page(percpu, 0);\n\tunsigned int *offs = to_addr_offs(sh, percpu);\n\tstruct async_submit_ctl submit;\n\tint count;\n\n\tpr_debug(\"%s: stripe %llu checkp: %d\\n\", __func__,\n\t\t(unsigned long long)sh->sector, checkp);\n\n\tBUG_ON(sh->batch_head);\n\tcount = set_syndrome_sources(srcs, offs, sh, SYNDROME_SRC_ALL);\n\tif (!checkp)\n\t\tsrcs[count] = NULL;\n\n\tatomic_inc(&sh->count);\n\tinit_async_submit(&submit, ASYNC_TX_ACK, NULL, ops_complete_check,\n\t\t\t  sh, to_addr_conv(sh, percpu, 0));\n\tasync_syndrome_val(srcs, offs, count+2,\n\t\t\t   RAID5_STRIPE_SIZE(sh->raid_conf),\n\t\t\t   &sh->ops.zero_sum_result, percpu->spare_page, 0, &submit);\n}\n\nstatic void raid_run_ops(struct stripe_head *sh, unsigned long ops_request)\n{\n\tint overlap_clear = 0, i, disks = sh->disks;\n\tstruct dma_async_tx_descriptor *tx = NULL;\n\tstruct r5conf *conf = sh->raid_conf;\n\tint level = conf->level;\n\tstruct raid5_percpu *percpu;\n\n\tlocal_lock(&conf->percpu->lock);\n\tpercpu = this_cpu_ptr(conf->percpu);\n\tif (test_bit(STRIPE_OP_BIOFILL, &ops_request)) {\n\t\tops_run_biofill(sh);\n\t\toverlap_clear++;\n\t}\n\n\tif (test_bit(STRIPE_OP_COMPUTE_BLK, &ops_request)) {\n\t\tif (level < 6)\n\t\t\ttx = ops_run_compute5(sh, percpu);\n\t\telse {\n\t\t\tif (sh->ops.target2 < 0 || sh->ops.target < 0)\n\t\t\t\ttx = ops_run_compute6_1(sh, percpu);\n\t\t\telse\n\t\t\t\ttx = ops_run_compute6_2(sh, percpu);\n\t\t}\n\t\t \n\t\tif (tx && !test_bit(STRIPE_OP_RECONSTRUCT, &ops_request))\n\t\t\tasync_tx_ack(tx);\n\t}\n\n\tif (test_bit(STRIPE_OP_PREXOR, &ops_request)) {\n\t\tif (level < 6)\n\t\t\ttx = ops_run_prexor5(sh, percpu, tx);\n\t\telse\n\t\t\ttx = ops_run_prexor6(sh, percpu, tx);\n\t}\n\n\tif (test_bit(STRIPE_OP_PARTIAL_PARITY, &ops_request))\n\t\ttx = ops_run_partial_parity(sh, percpu, tx);\n\n\tif (test_bit(STRIPE_OP_BIODRAIN, &ops_request)) {\n\t\ttx = ops_run_biodrain(sh, tx);\n\t\toverlap_clear++;\n\t}\n\n\tif (test_bit(STRIPE_OP_RECONSTRUCT, &ops_request)) {\n\t\tif (level < 6)\n\t\t\tops_run_reconstruct5(sh, percpu, tx);\n\t\telse\n\t\t\tops_run_reconstruct6(sh, percpu, tx);\n\t}\n\n\tif (test_bit(STRIPE_OP_CHECK, &ops_request)) {\n\t\tif (sh->check_state == check_state_run)\n\t\t\tops_run_check_p(sh, percpu);\n\t\telse if (sh->check_state == check_state_run_q)\n\t\t\tops_run_check_pq(sh, percpu, 0);\n\t\telse if (sh->check_state == check_state_run_pq)\n\t\t\tops_run_check_pq(sh, percpu, 1);\n\t\telse\n\t\t\tBUG();\n\t}\n\n\tif (overlap_clear && !sh->batch_head) {\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (test_and_clear_bit(R5_Overlap, &dev->flags))\n\t\t\t\twake_up(&sh->raid_conf->wait_for_overlap);\n\t\t}\n\t}\n\tlocal_unlock(&conf->percpu->lock);\n}\n\nstatic void free_stripe(struct kmem_cache *sc, struct stripe_head *sh)\n{\n#if PAGE_SIZE != DEFAULT_STRIPE_SIZE\n\tkfree(sh->pages);\n#endif\n\tif (sh->ppl_page)\n\t\t__free_page(sh->ppl_page);\n\tkmem_cache_free(sc, sh);\n}\n\nstatic struct stripe_head *alloc_stripe(struct kmem_cache *sc, gfp_t gfp,\n\tint disks, struct r5conf *conf)\n{\n\tstruct stripe_head *sh;\n\n\tsh = kmem_cache_zalloc(sc, gfp);\n\tif (sh) {\n\t\tspin_lock_init(&sh->stripe_lock);\n\t\tspin_lock_init(&sh->batch_lock);\n\t\tINIT_LIST_HEAD(&sh->batch_list);\n\t\tINIT_LIST_HEAD(&sh->lru);\n\t\tINIT_LIST_HEAD(&sh->r5c);\n\t\tINIT_LIST_HEAD(&sh->log_list);\n\t\tatomic_set(&sh->count, 1);\n\t\tsh->raid_conf = conf;\n\t\tsh->log_start = MaxSector;\n\n\t\tif (raid5_has_ppl(conf)) {\n\t\t\tsh->ppl_page = alloc_page(gfp);\n\t\t\tif (!sh->ppl_page) {\n\t\t\t\tfree_stripe(sc, sh);\n\t\t\t\treturn NULL;\n\t\t\t}\n\t\t}\n#if PAGE_SIZE != DEFAULT_STRIPE_SIZE\n\t\tif (init_stripe_shared_pages(sh, conf, disks)) {\n\t\t\tfree_stripe(sc, sh);\n\t\t\treturn NULL;\n\t\t}\n#endif\n\t}\n\treturn sh;\n}\nstatic int grow_one_stripe(struct r5conf *conf, gfp_t gfp)\n{\n\tstruct stripe_head *sh;\n\n\tsh = alloc_stripe(conf->slab_cache, gfp, conf->pool_size, conf);\n\tif (!sh)\n\t\treturn 0;\n\n\tif (grow_buffers(sh, gfp)) {\n\t\tshrink_buffers(sh);\n\t\tfree_stripe(conf->slab_cache, sh);\n\t\treturn 0;\n\t}\n\tsh->hash_lock_index =\n\t\tconf->max_nr_stripes % NR_STRIPE_HASH_LOCKS;\n\t \n\tatomic_inc(&conf->active_stripes);\n\n\traid5_release_stripe(sh);\n\tconf->max_nr_stripes++;\n\treturn 1;\n}\n\nstatic int grow_stripes(struct r5conf *conf, int num)\n{\n\tstruct kmem_cache *sc;\n\tsize_t namelen = sizeof(conf->cache_name[0]);\n\tint devs = max(conf->raid_disks, conf->previous_raid_disks);\n\n\tif (conf->mddev->gendisk)\n\t\tsnprintf(conf->cache_name[0], namelen,\n\t\t\t\"raid%d-%s\", conf->level, mdname(conf->mddev));\n\telse\n\t\tsnprintf(conf->cache_name[0], namelen,\n\t\t\t\"raid%d-%p\", conf->level, conf->mddev);\n\tsnprintf(conf->cache_name[1], namelen, \"%.27s-alt\", conf->cache_name[0]);\n\n\tconf->active_name = 0;\n\tsc = kmem_cache_create(conf->cache_name[conf->active_name],\n\t\t\t       struct_size_t(struct stripe_head, dev, devs),\n\t\t\t       0, 0, NULL);\n\tif (!sc)\n\t\treturn 1;\n\tconf->slab_cache = sc;\n\tconf->pool_size = devs;\n\twhile (num--)\n\t\tif (!grow_one_stripe(conf, GFP_KERNEL))\n\t\t\treturn 1;\n\n\treturn 0;\n}\n\n \nstatic int scribble_alloc(struct raid5_percpu *percpu,\n\t\t\t  int num, int cnt)\n{\n\tsize_t obj_size =\n\t\tsizeof(struct page *) * (num + 2) +\n\t\tsizeof(addr_conv_t) * (num + 2) +\n\t\tsizeof(unsigned int) * (num + 2);\n\tvoid *scribble;\n\n\t \n\tscribble = kvmalloc_array(cnt, obj_size, GFP_KERNEL);\n\tif (!scribble)\n\t\treturn -ENOMEM;\n\n\tkvfree(percpu->scribble);\n\n\tpercpu->scribble = scribble;\n\tpercpu->scribble_obj_size = obj_size;\n\treturn 0;\n}\n\nstatic int resize_chunks(struct r5conf *conf, int new_disks, int new_sectors)\n{\n\tunsigned long cpu;\n\tint err = 0;\n\n\t \n\tif (conf->scribble_disks >= new_disks &&\n\t    conf->scribble_sectors >= new_sectors)\n\t\treturn 0;\n\tmddev_suspend(conf->mddev);\n\tcpus_read_lock();\n\n\tfor_each_present_cpu(cpu) {\n\t\tstruct raid5_percpu *percpu;\n\n\t\tpercpu = per_cpu_ptr(conf->percpu, cpu);\n\t\terr = scribble_alloc(percpu, new_disks,\n\t\t\t\t     new_sectors / RAID5_STRIPE_SECTORS(conf));\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\n\tcpus_read_unlock();\n\tmddev_resume(conf->mddev);\n\tif (!err) {\n\t\tconf->scribble_disks = new_disks;\n\t\tconf->scribble_sectors = new_sectors;\n\t}\n\treturn err;\n}\n\nstatic int resize_stripes(struct r5conf *conf, int newsize)\n{\n\t \n\tstruct stripe_head *osh, *nsh;\n\tLIST_HEAD(newstripes);\n\tstruct disk_info *ndisks;\n\tint err = 0;\n\tstruct kmem_cache *sc;\n\tint i;\n\tint hash, cnt;\n\n\tmd_allow_write(conf->mddev);\n\n\t \n\tsc = kmem_cache_create(conf->cache_name[1-conf->active_name],\n\t\t\t       struct_size_t(struct stripe_head, dev, newsize),\n\t\t\t       0, 0, NULL);\n\tif (!sc)\n\t\treturn -ENOMEM;\n\n\t \n\tmutex_lock(&conf->cache_size_mutex);\n\n\tfor (i = conf->max_nr_stripes; i; i--) {\n\t\tnsh = alloc_stripe(sc, GFP_KERNEL, newsize, conf);\n\t\tif (!nsh)\n\t\t\tbreak;\n\n\t\tlist_add(&nsh->lru, &newstripes);\n\t}\n\tif (i) {\n\t\t \n\t\twhile (!list_empty(&newstripes)) {\n\t\t\tnsh = list_entry(newstripes.next, struct stripe_head, lru);\n\t\t\tlist_del(&nsh->lru);\n\t\t\tfree_stripe(sc, nsh);\n\t\t}\n\t\tkmem_cache_destroy(sc);\n\t\tmutex_unlock(&conf->cache_size_mutex);\n\t\treturn -ENOMEM;\n\t}\n\t \n\thash = 0;\n\tcnt = 0;\n\tlist_for_each_entry(nsh, &newstripes, lru) {\n\t\tlock_device_hash_lock(conf, hash);\n\t\twait_event_cmd(conf->wait_for_stripe,\n\t\t\t\t    !list_empty(conf->inactive_list + hash),\n\t\t\t\t    unlock_device_hash_lock(conf, hash),\n\t\t\t\t    lock_device_hash_lock(conf, hash));\n\t\tosh = get_free_stripe(conf, hash);\n\t\tunlock_device_hash_lock(conf, hash);\n\n#if PAGE_SIZE != DEFAULT_STRIPE_SIZE\n\tfor (i = 0; i < osh->nr_pages; i++) {\n\t\tnsh->pages[i] = osh->pages[i];\n\t\tosh->pages[i] = NULL;\n\t}\n#endif\n\t\tfor(i=0; i<conf->pool_size; i++) {\n\t\t\tnsh->dev[i].page = osh->dev[i].page;\n\t\t\tnsh->dev[i].orig_page = osh->dev[i].page;\n\t\t\tnsh->dev[i].offset = osh->dev[i].offset;\n\t\t}\n\t\tnsh->hash_lock_index = hash;\n\t\tfree_stripe(conf->slab_cache, osh);\n\t\tcnt++;\n\t\tif (cnt >= conf->max_nr_stripes / NR_STRIPE_HASH_LOCKS +\n\t\t    !!((conf->max_nr_stripes % NR_STRIPE_HASH_LOCKS) > hash)) {\n\t\t\thash++;\n\t\t\tcnt = 0;\n\t\t}\n\t}\n\tkmem_cache_destroy(conf->slab_cache);\n\n\t \n\tndisks = kcalloc(newsize, sizeof(struct disk_info), GFP_NOIO);\n\tif (ndisks) {\n\t\tfor (i = 0; i < conf->pool_size; i++)\n\t\t\tndisks[i] = conf->disks[i];\n\n\t\tfor (i = conf->pool_size; i < newsize; i++) {\n\t\t\tndisks[i].extra_page = alloc_page(GFP_NOIO);\n\t\t\tif (!ndisks[i].extra_page)\n\t\t\t\terr = -ENOMEM;\n\t\t}\n\n\t\tif (err) {\n\t\t\tfor (i = conf->pool_size; i < newsize; i++)\n\t\t\t\tif (ndisks[i].extra_page)\n\t\t\t\t\tput_page(ndisks[i].extra_page);\n\t\t\tkfree(ndisks);\n\t\t} else {\n\t\t\tkfree(conf->disks);\n\t\t\tconf->disks = ndisks;\n\t\t}\n\t} else\n\t\terr = -ENOMEM;\n\n\tconf->slab_cache = sc;\n\tconf->active_name = 1-conf->active_name;\n\n\t \n\twhile(!list_empty(&newstripes)) {\n\t\tnsh = list_entry(newstripes.next, struct stripe_head, lru);\n\t\tlist_del_init(&nsh->lru);\n\n#if PAGE_SIZE != DEFAULT_STRIPE_SIZE\n\t\tfor (i = 0; i < nsh->nr_pages; i++) {\n\t\t\tif (nsh->pages[i])\n\t\t\t\tcontinue;\n\t\t\tnsh->pages[i] = alloc_page(GFP_NOIO);\n\t\t\tif (!nsh->pages[i])\n\t\t\t\terr = -ENOMEM;\n\t\t}\n\n\t\tfor (i = conf->raid_disks; i < newsize; i++) {\n\t\t\tif (nsh->dev[i].page)\n\t\t\t\tcontinue;\n\t\t\tnsh->dev[i].page = raid5_get_dev_page(nsh, i);\n\t\t\tnsh->dev[i].orig_page = nsh->dev[i].page;\n\t\t\tnsh->dev[i].offset = raid5_get_page_offset(nsh, i);\n\t\t}\n#else\n\t\tfor (i=conf->raid_disks; i < newsize; i++)\n\t\t\tif (nsh->dev[i].page == NULL) {\n\t\t\t\tstruct page *p = alloc_page(GFP_NOIO);\n\t\t\t\tnsh->dev[i].page = p;\n\t\t\t\tnsh->dev[i].orig_page = p;\n\t\t\t\tnsh->dev[i].offset = 0;\n\t\t\t\tif (!p)\n\t\t\t\t\terr = -ENOMEM;\n\t\t\t}\n#endif\n\t\traid5_release_stripe(nsh);\n\t}\n\t \n\n\tif (!err)\n\t\tconf->pool_size = newsize;\n\tmutex_unlock(&conf->cache_size_mutex);\n\n\treturn err;\n}\n\nstatic int drop_one_stripe(struct r5conf *conf)\n{\n\tstruct stripe_head *sh;\n\tint hash = (conf->max_nr_stripes - 1) & STRIPE_HASH_LOCKS_MASK;\n\n\tspin_lock_irq(conf->hash_locks + hash);\n\tsh = get_free_stripe(conf, hash);\n\tspin_unlock_irq(conf->hash_locks + hash);\n\tif (!sh)\n\t\treturn 0;\n\tBUG_ON(atomic_read(&sh->count));\n\tshrink_buffers(sh);\n\tfree_stripe(conf->slab_cache, sh);\n\tatomic_dec(&conf->active_stripes);\n\tconf->max_nr_stripes--;\n\treturn 1;\n}\n\nstatic void shrink_stripes(struct r5conf *conf)\n{\n\twhile (conf->max_nr_stripes &&\n\t       drop_one_stripe(conf))\n\t\t;\n\n\tkmem_cache_destroy(conf->slab_cache);\n\tconf->slab_cache = NULL;\n}\n\n \nstatic struct md_rdev *rdev_pend_deref(struct md_rdev __rcu *rdev)\n{\n\treturn rcu_dereference_protected(rdev,\n\t\t\tatomic_read(&rcu_access_pointer(rdev)->nr_pending));\n}\n\n \nstatic struct md_rdev *rdev_mdlock_deref(struct mddev *mddev,\n\t\t\t\t\t struct md_rdev __rcu *rdev)\n{\n\treturn rcu_dereference_protected(rdev,\n\t\t\tlockdep_is_held(&mddev->reconfig_mutex));\n}\n\nstatic void raid5_end_read_request(struct bio * bi)\n{\n\tstruct stripe_head *sh = bi->bi_private;\n\tstruct r5conf *conf = sh->raid_conf;\n\tint disks = sh->disks, i;\n\tstruct md_rdev *rdev = NULL;\n\tsector_t s;\n\n\tfor (i=0 ; i<disks; i++)\n\t\tif (bi == &sh->dev[i].req)\n\t\t\tbreak;\n\n\tpr_debug(\"end_read_request %llu/%d, count: %d, error %d.\\n\",\n\t\t(unsigned long long)sh->sector, i, atomic_read(&sh->count),\n\t\tbi->bi_status);\n\tif (i == disks) {\n\t\tBUG();\n\t\treturn;\n\t}\n\tif (test_bit(R5_ReadRepl, &sh->dev[i].flags))\n\t\t \n\t\trdev = rdev_pend_deref(conf->disks[i].replacement);\n\tif (!rdev)\n\t\trdev = rdev_pend_deref(conf->disks[i].rdev);\n\n\tif (use_new_offset(conf, sh))\n\t\ts = sh->sector + rdev->new_data_offset;\n\telse\n\t\ts = sh->sector + rdev->data_offset;\n\tif (!bi->bi_status) {\n\t\tset_bit(R5_UPTODATE, &sh->dev[i].flags);\n\t\tif (test_bit(R5_ReadError, &sh->dev[i].flags)) {\n\t\t\t \n\t\t\tpr_info_ratelimited(\n\t\t\t\t\"md/raid:%s: read error corrected (%lu sectors at %llu on %pg)\\n\",\n\t\t\t\tmdname(conf->mddev), RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t(unsigned long long)s,\n\t\t\t\trdev->bdev);\n\t\t\tatomic_add(RAID5_STRIPE_SECTORS(conf), &rdev->corrected_errors);\n\t\t\tclear_bit(R5_ReadError, &sh->dev[i].flags);\n\t\t\tclear_bit(R5_ReWrite, &sh->dev[i].flags);\n\t\t} else if (test_bit(R5_ReadNoMerge, &sh->dev[i].flags))\n\t\t\tclear_bit(R5_ReadNoMerge, &sh->dev[i].flags);\n\n\t\tif (test_bit(R5_InJournal, &sh->dev[i].flags))\n\t\t\t \n\t\t\tset_bit(R5_OrigPageUPTDODATE, &sh->dev[i].flags);\n\n\t\tif (atomic_read(&rdev->read_errors))\n\t\t\tatomic_set(&rdev->read_errors, 0);\n\t} else {\n\t\tint retry = 0;\n\t\tint set_bad = 0;\n\n\t\tclear_bit(R5_UPTODATE, &sh->dev[i].flags);\n\t\tif (!(bi->bi_status == BLK_STS_PROTECTION))\n\t\t\tatomic_inc(&rdev->read_errors);\n\t\tif (test_bit(R5_ReadRepl, &sh->dev[i].flags))\n\t\t\tpr_warn_ratelimited(\n\t\t\t\t\"md/raid:%s: read error on replacement device (sector %llu on %pg).\\n\",\n\t\t\t\tmdname(conf->mddev),\n\t\t\t\t(unsigned long long)s,\n\t\t\t\trdev->bdev);\n\t\telse if (conf->mddev->degraded >= conf->max_degraded) {\n\t\t\tset_bad = 1;\n\t\t\tpr_warn_ratelimited(\n\t\t\t\t\"md/raid:%s: read error not correctable (sector %llu on %pg).\\n\",\n\t\t\t\tmdname(conf->mddev),\n\t\t\t\t(unsigned long long)s,\n\t\t\t\trdev->bdev);\n\t\t} else if (test_bit(R5_ReWrite, &sh->dev[i].flags)) {\n\t\t\t \n\t\t\tset_bad = 1;\n\t\t\tpr_warn_ratelimited(\n\t\t\t\t\"md/raid:%s: read error NOT corrected!! (sector %llu on %pg).\\n\",\n\t\t\t\tmdname(conf->mddev),\n\t\t\t\t(unsigned long long)s,\n\t\t\t\trdev->bdev);\n\t\t} else if (atomic_read(&rdev->read_errors)\n\t\t\t > conf->max_nr_stripes) {\n\t\t\tif (!test_bit(Faulty, &rdev->flags)) {\n\t\t\t\tpr_warn(\"md/raid:%s: %d read_errors > %d stripes\\n\",\n\t\t\t\t    mdname(conf->mddev),\n\t\t\t\t    atomic_read(&rdev->read_errors),\n\t\t\t\t    conf->max_nr_stripes);\n\t\t\t\tpr_warn(\"md/raid:%s: Too many read errors, failing device %pg.\\n\",\n\t\t\t\t    mdname(conf->mddev), rdev->bdev);\n\t\t\t}\n\t\t} else\n\t\t\tretry = 1;\n\t\tif (set_bad && test_bit(In_sync, &rdev->flags)\n\t\t    && !test_bit(R5_ReadNoMerge, &sh->dev[i].flags))\n\t\t\tretry = 1;\n\t\tif (retry)\n\t\t\tif (sh->qd_idx >= 0 && sh->pd_idx == i)\n\t\t\t\tset_bit(R5_ReadError, &sh->dev[i].flags);\n\t\t\telse if (test_bit(R5_ReadNoMerge, &sh->dev[i].flags)) {\n\t\t\t\tset_bit(R5_ReadError, &sh->dev[i].flags);\n\t\t\t\tclear_bit(R5_ReadNoMerge, &sh->dev[i].flags);\n\t\t\t} else\n\t\t\t\tset_bit(R5_ReadNoMerge, &sh->dev[i].flags);\n\t\telse {\n\t\t\tclear_bit(R5_ReadError, &sh->dev[i].flags);\n\t\t\tclear_bit(R5_ReWrite, &sh->dev[i].flags);\n\t\t\tif (!(set_bad\n\t\t\t      && test_bit(In_sync, &rdev->flags)\n\t\t\t      && rdev_set_badblocks(\n\t\t\t\t      rdev, sh->sector, RAID5_STRIPE_SECTORS(conf), 0)))\n\t\t\t\tmd_error(conf->mddev, rdev);\n\t\t}\n\t}\n\trdev_dec_pending(rdev, conf->mddev);\n\tbio_uninit(bi);\n\tclear_bit(R5_LOCKED, &sh->dev[i].flags);\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\traid5_release_stripe(sh);\n}\n\nstatic void raid5_end_write_request(struct bio *bi)\n{\n\tstruct stripe_head *sh = bi->bi_private;\n\tstruct r5conf *conf = sh->raid_conf;\n\tint disks = sh->disks, i;\n\tstruct md_rdev *rdev;\n\tsector_t first_bad;\n\tint bad_sectors;\n\tint replacement = 0;\n\n\tfor (i = 0 ; i < disks; i++) {\n\t\tif (bi == &sh->dev[i].req) {\n\t\t\trdev = rdev_pend_deref(conf->disks[i].rdev);\n\t\t\tbreak;\n\t\t}\n\t\tif (bi == &sh->dev[i].rreq) {\n\t\t\trdev = rdev_pend_deref(conf->disks[i].replacement);\n\t\t\tif (rdev)\n\t\t\t\treplacement = 1;\n\t\t\telse\n\t\t\t\t \n\t\t\t\trdev = rdev_pend_deref(conf->disks[i].rdev);\n\t\t\tbreak;\n\t\t}\n\t}\n\tpr_debug(\"end_write_request %llu/%d, count %d, error: %d.\\n\",\n\t\t(unsigned long long)sh->sector, i, atomic_read(&sh->count),\n\t\tbi->bi_status);\n\tif (i == disks) {\n\t\tBUG();\n\t\treturn;\n\t}\n\n\tif (replacement) {\n\t\tif (bi->bi_status)\n\t\t\tmd_error(conf->mddev, rdev);\n\t\telse if (is_badblock(rdev, sh->sector,\n\t\t\t\t     RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t     &first_bad, &bad_sectors))\n\t\t\tset_bit(R5_MadeGoodRepl, &sh->dev[i].flags);\n\t} else {\n\t\tif (bi->bi_status) {\n\t\t\tset_bit(STRIPE_DEGRADED, &sh->state);\n\t\t\tset_bit(WriteErrorSeen, &rdev->flags);\n\t\t\tset_bit(R5_WriteError, &sh->dev[i].flags);\n\t\t\tif (!test_and_set_bit(WantReplacement, &rdev->flags))\n\t\t\t\tset_bit(MD_RECOVERY_NEEDED,\n\t\t\t\t\t&rdev->mddev->recovery);\n\t\t} else if (is_badblock(rdev, sh->sector,\n\t\t\t\t       RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t       &first_bad, &bad_sectors)) {\n\t\t\tset_bit(R5_MadeGood, &sh->dev[i].flags);\n\t\t\tif (test_bit(R5_ReadError, &sh->dev[i].flags))\n\t\t\t\t \n\t\t\t\tset_bit(R5_ReWrite, &sh->dev[i].flags);\n\t\t}\n\t}\n\trdev_dec_pending(rdev, conf->mddev);\n\n\tif (sh->batch_head && bi->bi_status && !replacement)\n\t\tset_bit(STRIPE_BATCH_ERR, &sh->batch_head->state);\n\n\tbio_uninit(bi);\n\tif (!test_and_clear_bit(R5_DOUBLE_LOCKED, &sh->dev[i].flags))\n\t\tclear_bit(R5_LOCKED, &sh->dev[i].flags);\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\n\tif (sh->batch_head && sh != sh->batch_head)\n\t\traid5_release_stripe(sh->batch_head);\n\traid5_release_stripe(sh);\n}\n\nstatic void raid5_error(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tstruct r5conf *conf = mddev->private;\n\tunsigned long flags;\n\tpr_debug(\"raid456: error called\\n\");\n\n\tpr_crit(\"md/raid:%s: Disk failure on %pg, disabling device.\\n\",\n\t\tmdname(mddev), rdev->bdev);\n\n\tspin_lock_irqsave(&conf->device_lock, flags);\n\tset_bit(Faulty, &rdev->flags);\n\tclear_bit(In_sync, &rdev->flags);\n\tmddev->degraded = raid5_calc_degraded(conf);\n\n\tif (has_failed(conf)) {\n\t\tset_bit(MD_BROKEN, &conf->mddev->flags);\n\t\tconf->recovery_disabled = mddev->recovery_disabled;\n\n\t\tpr_crit(\"md/raid:%s: Cannot continue operation (%d/%d failed).\\n\",\n\t\t\tmdname(mddev), mddev->degraded, conf->raid_disks);\n\t} else {\n\t\tpr_crit(\"md/raid:%s: Operation continuing on %d devices.\\n\",\n\t\t\tmdname(mddev), conf->raid_disks - mddev->degraded);\n\t}\n\n\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\tset_bit(MD_RECOVERY_INTR, &mddev->recovery);\n\n\tset_bit(Blocked, &rdev->flags);\n\tset_mask_bits(&mddev->sb_flags, 0,\n\t\t      BIT(MD_SB_CHANGE_DEVS) | BIT(MD_SB_CHANGE_PENDING));\n\tr5c_update_on_rdev_error(mddev, rdev);\n}\n\n \nsector_t raid5_compute_sector(struct r5conf *conf, sector_t r_sector,\n\t\t\t      int previous, int *dd_idx,\n\t\t\t      struct stripe_head *sh)\n{\n\tsector_t stripe, stripe2;\n\tsector_t chunk_number;\n\tunsigned int chunk_offset;\n\tint pd_idx, qd_idx;\n\tint ddf_layout = 0;\n\tsector_t new_sector;\n\tint algorithm = previous ? conf->prev_algo\n\t\t\t\t : conf->algorithm;\n\tint sectors_per_chunk = previous ? conf->prev_chunk_sectors\n\t\t\t\t\t : conf->chunk_sectors;\n\tint raid_disks = previous ? conf->previous_raid_disks\n\t\t\t\t  : conf->raid_disks;\n\tint data_disks = raid_disks - conf->max_degraded;\n\n\t \n\n\t \n\tchunk_offset = sector_div(r_sector, sectors_per_chunk);\n\tchunk_number = r_sector;\n\n\t \n\tstripe = chunk_number;\n\t*dd_idx = sector_div(stripe, data_disks);\n\tstripe2 = stripe;\n\t \n\tpd_idx = qd_idx = -1;\n\tswitch(conf->level) {\n\tcase 4:\n\t\tpd_idx = data_disks;\n\t\tbreak;\n\tcase 5:\n\t\tswitch (algorithm) {\n\t\tcase ALGORITHM_LEFT_ASYMMETRIC:\n\t\t\tpd_idx = data_disks - sector_div(stripe2, raid_disks);\n\t\t\tif (*dd_idx >= pd_idx)\n\t\t\t\t(*dd_idx)++;\n\t\t\tbreak;\n\t\tcase ALGORITHM_RIGHT_ASYMMETRIC:\n\t\t\tpd_idx = sector_div(stripe2, raid_disks);\n\t\t\tif (*dd_idx >= pd_idx)\n\t\t\t\t(*dd_idx)++;\n\t\t\tbreak;\n\t\tcase ALGORITHM_LEFT_SYMMETRIC:\n\t\t\tpd_idx = data_disks - sector_div(stripe2, raid_disks);\n\t\t\t*dd_idx = (pd_idx + 1 + *dd_idx) % raid_disks;\n\t\t\tbreak;\n\t\tcase ALGORITHM_RIGHT_SYMMETRIC:\n\t\t\tpd_idx = sector_div(stripe2, raid_disks);\n\t\t\t*dd_idx = (pd_idx + 1 + *dd_idx) % raid_disks;\n\t\t\tbreak;\n\t\tcase ALGORITHM_PARITY_0:\n\t\t\tpd_idx = 0;\n\t\t\t(*dd_idx)++;\n\t\t\tbreak;\n\t\tcase ALGORITHM_PARITY_N:\n\t\t\tpd_idx = data_disks;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t\tbreak;\n\tcase 6:\n\n\t\tswitch (algorithm) {\n\t\tcase ALGORITHM_LEFT_ASYMMETRIC:\n\t\t\tpd_idx = raid_disks - 1 - sector_div(stripe2, raid_disks);\n\t\t\tqd_idx = pd_idx + 1;\n\t\t\tif (pd_idx == raid_disks-1) {\n\t\t\t\t(*dd_idx)++;\t \n\t\t\t\tqd_idx = 0;\n\t\t\t} else if (*dd_idx >= pd_idx)\n\t\t\t\t(*dd_idx) += 2;  \n\t\t\tbreak;\n\t\tcase ALGORITHM_RIGHT_ASYMMETRIC:\n\t\t\tpd_idx = sector_div(stripe2, raid_disks);\n\t\t\tqd_idx = pd_idx + 1;\n\t\t\tif (pd_idx == raid_disks-1) {\n\t\t\t\t(*dd_idx)++;\t \n\t\t\t\tqd_idx = 0;\n\t\t\t} else if (*dd_idx >= pd_idx)\n\t\t\t\t(*dd_idx) += 2;  \n\t\t\tbreak;\n\t\tcase ALGORITHM_LEFT_SYMMETRIC:\n\t\t\tpd_idx = raid_disks - 1 - sector_div(stripe2, raid_disks);\n\t\t\tqd_idx = (pd_idx + 1) % raid_disks;\n\t\t\t*dd_idx = (pd_idx + 2 + *dd_idx) % raid_disks;\n\t\t\tbreak;\n\t\tcase ALGORITHM_RIGHT_SYMMETRIC:\n\t\t\tpd_idx = sector_div(stripe2, raid_disks);\n\t\t\tqd_idx = (pd_idx + 1) % raid_disks;\n\t\t\t*dd_idx = (pd_idx + 2 + *dd_idx) % raid_disks;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_PARITY_0:\n\t\t\tpd_idx = 0;\n\t\t\tqd_idx = 1;\n\t\t\t(*dd_idx) += 2;\n\t\t\tbreak;\n\t\tcase ALGORITHM_PARITY_N:\n\t\t\tpd_idx = data_disks;\n\t\t\tqd_idx = data_disks + 1;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_ROTATING_ZERO_RESTART:\n\t\t\t \n\t\t\tpd_idx = sector_div(stripe2, raid_disks);\n\t\t\tqd_idx = pd_idx + 1;\n\t\t\tif (pd_idx == raid_disks-1) {\n\t\t\t\t(*dd_idx)++;\t \n\t\t\t\tqd_idx = 0;\n\t\t\t} else if (*dd_idx >= pd_idx)\n\t\t\t\t(*dd_idx) += 2;  \n\t\t\tddf_layout = 1;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_ROTATING_N_RESTART:\n\t\t\t \n\t\t\tstripe2 += 1;\n\t\t\tpd_idx = raid_disks - 1 - sector_div(stripe2, raid_disks);\n\t\t\tqd_idx = pd_idx + 1;\n\t\t\tif (pd_idx == raid_disks-1) {\n\t\t\t\t(*dd_idx)++;\t \n\t\t\t\tqd_idx = 0;\n\t\t\t} else if (*dd_idx >= pd_idx)\n\t\t\t\t(*dd_idx) += 2;  \n\t\t\tddf_layout = 1;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_ROTATING_N_CONTINUE:\n\t\t\t \n\t\t\tpd_idx = raid_disks - 1 - sector_div(stripe2, raid_disks);\n\t\t\tqd_idx = (pd_idx + raid_disks - 1) % raid_disks;\n\t\t\t*dd_idx = (pd_idx + 1 + *dd_idx) % raid_disks;\n\t\t\tddf_layout = 1;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_LEFT_ASYMMETRIC_6:\n\t\t\t \n\t\t\tpd_idx = data_disks - sector_div(stripe2, raid_disks-1);\n\t\t\tif (*dd_idx >= pd_idx)\n\t\t\t\t(*dd_idx)++;\n\t\t\tqd_idx = raid_disks - 1;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_RIGHT_ASYMMETRIC_6:\n\t\t\tpd_idx = sector_div(stripe2, raid_disks-1);\n\t\t\tif (*dd_idx >= pd_idx)\n\t\t\t\t(*dd_idx)++;\n\t\t\tqd_idx = raid_disks - 1;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_LEFT_SYMMETRIC_6:\n\t\t\tpd_idx = data_disks - sector_div(stripe2, raid_disks-1);\n\t\t\t*dd_idx = (pd_idx + 1 + *dd_idx) % (raid_disks-1);\n\t\t\tqd_idx = raid_disks - 1;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_RIGHT_SYMMETRIC_6:\n\t\t\tpd_idx = sector_div(stripe2, raid_disks-1);\n\t\t\t*dd_idx = (pd_idx + 1 + *dd_idx) % (raid_disks-1);\n\t\t\tqd_idx = raid_disks - 1;\n\t\t\tbreak;\n\n\t\tcase ALGORITHM_PARITY_0_6:\n\t\t\tpd_idx = 0;\n\t\t\t(*dd_idx)++;\n\t\t\tqd_idx = raid_disks - 1;\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t\tbreak;\n\t}\n\n\tif (sh) {\n\t\tsh->pd_idx = pd_idx;\n\t\tsh->qd_idx = qd_idx;\n\t\tsh->ddf_layout = ddf_layout;\n\t}\n\t \n\tnew_sector = (sector_t)stripe * sectors_per_chunk + chunk_offset;\n\treturn new_sector;\n}\n\nsector_t raid5_compute_blocknr(struct stripe_head *sh, int i, int previous)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tint raid_disks = sh->disks;\n\tint data_disks = raid_disks - conf->max_degraded;\n\tsector_t new_sector = sh->sector, check;\n\tint sectors_per_chunk = previous ? conf->prev_chunk_sectors\n\t\t\t\t\t : conf->chunk_sectors;\n\tint algorithm = previous ? conf->prev_algo\n\t\t\t\t : conf->algorithm;\n\tsector_t stripe;\n\tint chunk_offset;\n\tsector_t chunk_number;\n\tint dummy1, dd_idx = i;\n\tsector_t r_sector;\n\tstruct stripe_head sh2;\n\n\tchunk_offset = sector_div(new_sector, sectors_per_chunk);\n\tstripe = new_sector;\n\n\tif (i == sh->pd_idx)\n\t\treturn 0;\n\tswitch(conf->level) {\n\tcase 4: break;\n\tcase 5:\n\t\tswitch (algorithm) {\n\t\tcase ALGORITHM_LEFT_ASYMMETRIC:\n\t\tcase ALGORITHM_RIGHT_ASYMMETRIC:\n\t\t\tif (i > sh->pd_idx)\n\t\t\t\ti--;\n\t\t\tbreak;\n\t\tcase ALGORITHM_LEFT_SYMMETRIC:\n\t\tcase ALGORITHM_RIGHT_SYMMETRIC:\n\t\t\tif (i < sh->pd_idx)\n\t\t\t\ti += raid_disks;\n\t\t\ti -= (sh->pd_idx + 1);\n\t\t\tbreak;\n\t\tcase ALGORITHM_PARITY_0:\n\t\t\ti -= 1;\n\t\t\tbreak;\n\t\tcase ALGORITHM_PARITY_N:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t\tbreak;\n\tcase 6:\n\t\tif (i == sh->qd_idx)\n\t\t\treturn 0;  \n\t\tswitch (algorithm) {\n\t\tcase ALGORITHM_LEFT_ASYMMETRIC:\n\t\tcase ALGORITHM_RIGHT_ASYMMETRIC:\n\t\tcase ALGORITHM_ROTATING_ZERO_RESTART:\n\t\tcase ALGORITHM_ROTATING_N_RESTART:\n\t\t\tif (sh->pd_idx == raid_disks-1)\n\t\t\t\ti--;\t \n\t\t\telse if (i > sh->pd_idx)\n\t\t\t\ti -= 2;  \n\t\t\tbreak;\n\t\tcase ALGORITHM_LEFT_SYMMETRIC:\n\t\tcase ALGORITHM_RIGHT_SYMMETRIC:\n\t\t\tif (sh->pd_idx == raid_disks-1)\n\t\t\t\ti--;  \n\t\t\telse {\n\t\t\t\t \n\t\t\t\tif (i < sh->pd_idx)\n\t\t\t\t\ti += raid_disks;\n\t\t\t\ti -= (sh->pd_idx + 2);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase ALGORITHM_PARITY_0:\n\t\t\ti -= 2;\n\t\t\tbreak;\n\t\tcase ALGORITHM_PARITY_N:\n\t\t\tbreak;\n\t\tcase ALGORITHM_ROTATING_N_CONTINUE:\n\t\t\t \n\t\t\tif (sh->pd_idx == 0)\n\t\t\t\ti--;\t \n\t\t\telse {\n\t\t\t\t \n\t\t\t\tif (i < sh->pd_idx)\n\t\t\t\t\ti += raid_disks;\n\t\t\t\ti -= (sh->pd_idx + 1);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase ALGORITHM_LEFT_ASYMMETRIC_6:\n\t\tcase ALGORITHM_RIGHT_ASYMMETRIC_6:\n\t\t\tif (i > sh->pd_idx)\n\t\t\t\ti--;\n\t\t\tbreak;\n\t\tcase ALGORITHM_LEFT_SYMMETRIC_6:\n\t\tcase ALGORITHM_RIGHT_SYMMETRIC_6:\n\t\t\tif (i < sh->pd_idx)\n\t\t\t\ti += data_disks + 1;\n\t\t\ti -= (sh->pd_idx + 1);\n\t\t\tbreak;\n\t\tcase ALGORITHM_PARITY_0_6:\n\t\t\ti -= 1;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t}\n\t\tbreak;\n\t}\n\n\tchunk_number = stripe * data_disks + i;\n\tr_sector = chunk_number * sectors_per_chunk + chunk_offset;\n\n\tcheck = raid5_compute_sector(conf, r_sector,\n\t\t\t\t     previous, &dummy1, &sh2);\n\tif (check != sh->sector || dummy1 != dd_idx || sh2.pd_idx != sh->pd_idx\n\t\t|| sh2.qd_idx != sh->qd_idx) {\n\t\tpr_warn(\"md/raid:%s: compute_blocknr: map not correct\\n\",\n\t\t\tmdname(conf->mddev));\n\t\treturn 0;\n\t}\n\treturn r_sector;\n}\n\n \nstatic inline bool delay_towrite(struct r5conf *conf,\n\t\t\t\t struct r5dev *dev,\n\t\t\t\t struct stripe_head_state *s)\n{\n\t \n\tif (!test_bit(R5_OVERWRITE, &dev->flags) &&\n\t    !test_bit(R5_Insync, &dev->flags) && s->injournal)\n\t\treturn true;\n\t \n\tif (test_bit(R5C_LOG_CRITICAL, &conf->cache_state) &&\n\t    s->injournal > 0)\n\t\treturn true;\n\t \n\tif (s->log_failed && s->injournal)\n\t\treturn true;\n\treturn false;\n}\n\nstatic void\nschedule_reconstruction(struct stripe_head *sh, struct stripe_head_state *s,\n\t\t\t int rcw, int expand)\n{\n\tint i, pd_idx = sh->pd_idx, qd_idx = sh->qd_idx, disks = sh->disks;\n\tstruct r5conf *conf = sh->raid_conf;\n\tint level = conf->level;\n\n\tif (rcw) {\n\t\t \n\t\tr5c_release_extra_page(sh);\n\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\n\t\t\tif (dev->towrite && !delay_towrite(conf, dev, s)) {\n\t\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\t\tset_bit(R5_Wantdrain, &dev->flags);\n\t\t\t\tif (!expand)\n\t\t\t\t\tclear_bit(R5_UPTODATE, &dev->flags);\n\t\t\t\ts->locked++;\n\t\t\t} else if (test_bit(R5_InJournal, &dev->flags)) {\n\t\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\t\ts->locked++;\n\t\t\t}\n\t\t}\n\t\t \n\t\tif (!expand) {\n\t\t\tif (!s->locked)\n\t\t\t\t \n\t\t\t\treturn;\n\t\t\tsh->reconstruct_state = reconstruct_state_drain_run;\n\t\t\tset_bit(STRIPE_OP_BIODRAIN, &s->ops_request);\n\t\t} else\n\t\t\tsh->reconstruct_state = reconstruct_state_run;\n\n\t\tset_bit(STRIPE_OP_RECONSTRUCT, &s->ops_request);\n\n\t\tif (s->locked + conf->max_degraded == disks)\n\t\t\tif (!test_and_set_bit(STRIPE_FULL_WRITE, &sh->state))\n\t\t\t\tatomic_inc(&conf->pending_full_writes);\n\t} else {\n\t\tBUG_ON(!(test_bit(R5_UPTODATE, &sh->dev[pd_idx].flags) ||\n\t\t\ttest_bit(R5_Wantcompute, &sh->dev[pd_idx].flags)));\n\t\tBUG_ON(level == 6 &&\n\t\t\t(!(test_bit(R5_UPTODATE, &sh->dev[qd_idx].flags) ||\n\t\t\t   test_bit(R5_Wantcompute, &sh->dev[qd_idx].flags))));\n\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (i == pd_idx || i == qd_idx)\n\t\t\t\tcontinue;\n\n\t\t\tif (dev->towrite &&\n\t\t\t    (test_bit(R5_UPTODATE, &dev->flags) ||\n\t\t\t     test_bit(R5_Wantcompute, &dev->flags))) {\n\t\t\t\tset_bit(R5_Wantdrain, &dev->flags);\n\t\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\t\tclear_bit(R5_UPTODATE, &dev->flags);\n\t\t\t\ts->locked++;\n\t\t\t} else if (test_bit(R5_InJournal, &dev->flags)) {\n\t\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\t\ts->locked++;\n\t\t\t}\n\t\t}\n\t\tif (!s->locked)\n\t\t\t \n\t\t\treturn;\n\t\tsh->reconstruct_state = reconstruct_state_prexor_drain_run;\n\t\tset_bit(STRIPE_OP_PREXOR, &s->ops_request);\n\t\tset_bit(STRIPE_OP_BIODRAIN, &s->ops_request);\n\t\tset_bit(STRIPE_OP_RECONSTRUCT, &s->ops_request);\n\t}\n\n\t \n\tset_bit(R5_LOCKED, &sh->dev[pd_idx].flags);\n\tclear_bit(R5_UPTODATE, &sh->dev[pd_idx].flags);\n\ts->locked++;\n\n\tif (level == 6) {\n\t\tint qd_idx = sh->qd_idx;\n\t\tstruct r5dev *dev = &sh->dev[qd_idx];\n\n\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\tclear_bit(R5_UPTODATE, &dev->flags);\n\t\ts->locked++;\n\t}\n\n\tif (raid5_has_ppl(sh->raid_conf) && sh->ppl_page &&\n\t    test_bit(STRIPE_OP_BIODRAIN, &s->ops_request) &&\n\t    !test_bit(STRIPE_FULL_WRITE, &sh->state) &&\n\t    test_bit(R5_Insync, &sh->dev[pd_idx].flags))\n\t\tset_bit(STRIPE_OP_PARTIAL_PARITY, &s->ops_request);\n\n\tpr_debug(\"%s: stripe %llu locked: %d ops_request: %lx\\n\",\n\t\t__func__, (unsigned long long)sh->sector,\n\t\ts->locked, s->ops_request);\n}\n\nstatic bool stripe_bio_overlaps(struct stripe_head *sh, struct bio *bi,\n\t\t\t\tint dd_idx, int forwrite)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tstruct bio **bip;\n\n\tpr_debug(\"checking bi b#%llu to stripe s#%llu\\n\",\n\t\t bi->bi_iter.bi_sector, sh->sector);\n\n\t \n\tif (sh->batch_head)\n\t\treturn true;\n\n\tif (forwrite)\n\t\tbip = &sh->dev[dd_idx].towrite;\n\telse\n\t\tbip = &sh->dev[dd_idx].toread;\n\n\twhile (*bip && (*bip)->bi_iter.bi_sector < bi->bi_iter.bi_sector) {\n\t\tif (bio_end_sector(*bip) > bi->bi_iter.bi_sector)\n\t\t\treturn true;\n\t\tbip = &(*bip)->bi_next;\n\t}\n\n\tif (*bip && (*bip)->bi_iter.bi_sector < bio_end_sector(bi))\n\t\treturn true;\n\n\tif (forwrite && raid5_has_ppl(conf)) {\n\t\t \n\t\tsector_t sector;\n\t\tsector_t first = 0;\n\t\tsector_t last = 0;\n\t\tint count = 0;\n\t\tint i;\n\n\t\tfor (i = 0; i < sh->disks; i++) {\n\t\t\tif (i != sh->pd_idx &&\n\t\t\t    (i == dd_idx || sh->dev[i].towrite)) {\n\t\t\t\tsector = sh->dev[i].sector;\n\t\t\t\tif (count == 0 || sector < first)\n\t\t\t\t\tfirst = sector;\n\t\t\t\tif (sector > last)\n\t\t\t\t\tlast = sector;\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\n\t\tif (first + conf->chunk_sectors * (count - 1) != last)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void __add_stripe_bio(struct stripe_head *sh, struct bio *bi,\n\t\t\t     int dd_idx, int forwrite, int previous)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tstruct bio **bip;\n\tint firstwrite = 0;\n\n\tif (forwrite) {\n\t\tbip = &sh->dev[dd_idx].towrite;\n\t\tif (!*bip)\n\t\t\tfirstwrite = 1;\n\t} else {\n\t\tbip = &sh->dev[dd_idx].toread;\n\t}\n\n\twhile (*bip && (*bip)->bi_iter.bi_sector < bi->bi_iter.bi_sector)\n\t\tbip = &(*bip)->bi_next;\n\n\tif (!forwrite || previous)\n\t\tclear_bit(STRIPE_BATCH_READY, &sh->state);\n\n\tBUG_ON(*bip && bi->bi_next && (*bip) != bi->bi_next);\n\tif (*bip)\n\t\tbi->bi_next = *bip;\n\t*bip = bi;\n\tbio_inc_remaining(bi);\n\tmd_write_inc(conf->mddev, bi);\n\n\tif (forwrite) {\n\t\t \n\t\tsector_t sector = sh->dev[dd_idx].sector;\n\t\tfor (bi=sh->dev[dd_idx].towrite;\n\t\t     sector < sh->dev[dd_idx].sector + RAID5_STRIPE_SECTORS(conf) &&\n\t\t\t     bi && bi->bi_iter.bi_sector <= sector;\n\t\t     bi = r5_next_bio(conf, bi, sh->dev[dd_idx].sector)) {\n\t\t\tif (bio_end_sector(bi) >= sector)\n\t\t\t\tsector = bio_end_sector(bi);\n\t\t}\n\t\tif (sector >= sh->dev[dd_idx].sector + RAID5_STRIPE_SECTORS(conf))\n\t\t\tif (!test_and_set_bit(R5_OVERWRITE, &sh->dev[dd_idx].flags))\n\t\t\t\tsh->overwrite_disks++;\n\t}\n\n\tpr_debug(\"added bi b#%llu to stripe s#%llu, disk %d, logical %llu\\n\",\n\t\t (*bip)->bi_iter.bi_sector, sh->sector, dd_idx,\n\t\t sh->dev[dd_idx].sector);\n\n\tif (conf->mddev->bitmap && firstwrite) {\n\t\t \n\t\tset_bit(STRIPE_BITMAP_PENDING, &sh->state);\n\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\tmd_bitmap_startwrite(conf->mddev->bitmap, sh->sector,\n\t\t\t\t     RAID5_STRIPE_SECTORS(conf), 0);\n\t\tspin_lock_irq(&sh->stripe_lock);\n\t\tclear_bit(STRIPE_BITMAP_PENDING, &sh->state);\n\t\tif (!sh->batch_head) {\n\t\t\tsh->bm_seq = conf->seq_flush+1;\n\t\t\tset_bit(STRIPE_BIT_DELAY, &sh->state);\n\t\t}\n\t}\n}\n\n \nstatic bool add_stripe_bio(struct stripe_head *sh, struct bio *bi,\n\t\t\t   int dd_idx, int forwrite, int previous)\n{\n\tspin_lock_irq(&sh->stripe_lock);\n\n\tif (stripe_bio_overlaps(sh, bi, dd_idx, forwrite)) {\n\t\tset_bit(R5_Overlap, &sh->dev[dd_idx].flags);\n\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\treturn false;\n\t}\n\n\t__add_stripe_bio(sh, bi, dd_idx, forwrite, previous);\n\tspin_unlock_irq(&sh->stripe_lock);\n\treturn true;\n}\n\nstatic void end_reshape(struct r5conf *conf);\n\nstatic void stripe_set_idx(sector_t stripe, struct r5conf *conf, int previous,\n\t\t\t    struct stripe_head *sh)\n{\n\tint sectors_per_chunk =\n\t\tprevious ? conf->prev_chunk_sectors : conf->chunk_sectors;\n\tint dd_idx;\n\tint chunk_offset = sector_div(stripe, sectors_per_chunk);\n\tint disks = previous ? conf->previous_raid_disks : conf->raid_disks;\n\n\traid5_compute_sector(conf,\n\t\t\t     stripe * (disks - conf->max_degraded)\n\t\t\t     *sectors_per_chunk + chunk_offset,\n\t\t\t     previous,\n\t\t\t     &dd_idx, sh);\n}\n\nstatic void\nhandle_failed_stripe(struct r5conf *conf, struct stripe_head *sh,\n\t\t     struct stripe_head_state *s, int disks)\n{\n\tint i;\n\tBUG_ON(sh->batch_head);\n\tfor (i = disks; i--; ) {\n\t\tstruct bio *bi;\n\t\tint bitmap_end = 0;\n\n\t\tif (test_bit(R5_ReadError, &sh->dev[i].flags)) {\n\t\t\tstruct md_rdev *rdev;\n\t\t\trcu_read_lock();\n\t\t\trdev = rcu_dereference(conf->disks[i].rdev);\n\t\t\tif (rdev && test_bit(In_sync, &rdev->flags) &&\n\t\t\t    !test_bit(Faulty, &rdev->flags))\n\t\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\telse\n\t\t\t\trdev = NULL;\n\t\t\trcu_read_unlock();\n\t\t\tif (rdev) {\n\t\t\t\tif (!rdev_set_badblocks(\n\t\t\t\t\t    rdev,\n\t\t\t\t\t    sh->sector,\n\t\t\t\t\t    RAID5_STRIPE_SECTORS(conf), 0))\n\t\t\t\t\tmd_error(conf->mddev, rdev);\n\t\t\t\trdev_dec_pending(rdev, conf->mddev);\n\t\t\t}\n\t\t}\n\t\tspin_lock_irq(&sh->stripe_lock);\n\t\t \n\t\tbi = sh->dev[i].towrite;\n\t\tsh->dev[i].towrite = NULL;\n\t\tsh->overwrite_disks = 0;\n\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\tif (bi)\n\t\t\tbitmap_end = 1;\n\n\t\tlog_stripe_write_finished(sh);\n\n\t\tif (test_and_clear_bit(R5_Overlap, &sh->dev[i].flags))\n\t\t\twake_up(&conf->wait_for_overlap);\n\n\t\twhile (bi && bi->bi_iter.bi_sector <\n\t\t\tsh->dev[i].sector + RAID5_STRIPE_SECTORS(conf)) {\n\t\t\tstruct bio *nextbi = r5_next_bio(conf, bi, sh->dev[i].sector);\n\n\t\t\tmd_write_end(conf->mddev);\n\t\t\tbio_io_error(bi);\n\t\t\tbi = nextbi;\n\t\t}\n\t\tif (bitmap_end)\n\t\t\tmd_bitmap_endwrite(conf->mddev->bitmap, sh->sector,\n\t\t\t\t\t   RAID5_STRIPE_SECTORS(conf), 0, 0);\n\t\tbitmap_end = 0;\n\t\t \n\t\tbi = sh->dev[i].written;\n\t\tsh->dev[i].written = NULL;\n\t\tif (test_and_clear_bit(R5_SkipCopy, &sh->dev[i].flags)) {\n\t\t\tWARN_ON(test_bit(R5_UPTODATE, &sh->dev[i].flags));\n\t\t\tsh->dev[i].page = sh->dev[i].orig_page;\n\t\t}\n\n\t\tif (bi) bitmap_end = 1;\n\t\twhile (bi && bi->bi_iter.bi_sector <\n\t\t       sh->dev[i].sector + RAID5_STRIPE_SECTORS(conf)) {\n\t\t\tstruct bio *bi2 = r5_next_bio(conf, bi, sh->dev[i].sector);\n\n\t\t\tmd_write_end(conf->mddev);\n\t\t\tbio_io_error(bi);\n\t\t\tbi = bi2;\n\t\t}\n\n\t\t \n\t\tif (!test_bit(R5_Wantfill, &sh->dev[i].flags) &&\n\t\t    s->failed > conf->max_degraded &&\n\t\t    (!test_bit(R5_Insync, &sh->dev[i].flags) ||\n\t\t      test_bit(R5_ReadError, &sh->dev[i].flags))) {\n\t\t\tspin_lock_irq(&sh->stripe_lock);\n\t\t\tbi = sh->dev[i].toread;\n\t\t\tsh->dev[i].toread = NULL;\n\t\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\t\tif (test_and_clear_bit(R5_Overlap, &sh->dev[i].flags))\n\t\t\t\twake_up(&conf->wait_for_overlap);\n\t\t\tif (bi)\n\t\t\t\ts->to_read--;\n\t\t\twhile (bi && bi->bi_iter.bi_sector <\n\t\t\t       sh->dev[i].sector + RAID5_STRIPE_SECTORS(conf)) {\n\t\t\t\tstruct bio *nextbi =\n\t\t\t\t\tr5_next_bio(conf, bi, sh->dev[i].sector);\n\n\t\t\t\tbio_io_error(bi);\n\t\t\t\tbi = nextbi;\n\t\t\t}\n\t\t}\n\t\tif (bitmap_end)\n\t\t\tmd_bitmap_endwrite(conf->mddev->bitmap, sh->sector,\n\t\t\t\t\t   RAID5_STRIPE_SECTORS(conf), 0, 0);\n\t\t \n\t\tclear_bit(R5_LOCKED, &sh->dev[i].flags);\n\t}\n\ts->to_write = 0;\n\ts->written = 0;\n\n\tif (test_and_clear_bit(STRIPE_FULL_WRITE, &sh->state))\n\t\tif (atomic_dec_and_test(&conf->pending_full_writes))\n\t\t\tmd_wakeup_thread(conf->mddev->thread);\n}\n\nstatic void\nhandle_failed_sync(struct r5conf *conf, struct stripe_head *sh,\n\t\t   struct stripe_head_state *s)\n{\n\tint abort = 0;\n\tint i;\n\n\tBUG_ON(sh->batch_head);\n\tclear_bit(STRIPE_SYNCING, &sh->state);\n\tif (test_and_clear_bit(R5_Overlap, &sh->dev[sh->pd_idx].flags))\n\t\twake_up(&conf->wait_for_overlap);\n\ts->syncing = 0;\n\ts->replacing = 0;\n\t \n\tif (test_bit(MD_RECOVERY_RECOVER, &conf->mddev->recovery)) {\n\t\t \n\t\trcu_read_lock();\n\t\tfor (i = 0; i < conf->raid_disks; i++) {\n\t\t\tstruct md_rdev *rdev = rcu_dereference(conf->disks[i].rdev);\n\t\t\tif (rdev\n\t\t\t    && !test_bit(Faulty, &rdev->flags)\n\t\t\t    && !test_bit(In_sync, &rdev->flags)\n\t\t\t    && !rdev_set_badblocks(rdev, sh->sector,\n\t\t\t\t\t\t   RAID5_STRIPE_SECTORS(conf), 0))\n\t\t\t\tabort = 1;\n\t\t\trdev = rcu_dereference(conf->disks[i].replacement);\n\t\t\tif (rdev\n\t\t\t    && !test_bit(Faulty, &rdev->flags)\n\t\t\t    && !test_bit(In_sync, &rdev->flags)\n\t\t\t    && !rdev_set_badblocks(rdev, sh->sector,\n\t\t\t\t\t\t   RAID5_STRIPE_SECTORS(conf), 0))\n\t\t\t\tabort = 1;\n\t\t}\n\t\trcu_read_unlock();\n\t\tif (abort)\n\t\t\tconf->recovery_disabled =\n\t\t\t\tconf->mddev->recovery_disabled;\n\t}\n\tmd_done_sync(conf->mddev, RAID5_STRIPE_SECTORS(conf), !abort);\n}\n\nstatic int want_replace(struct stripe_head *sh, int disk_idx)\n{\n\tstruct md_rdev *rdev;\n\tint rv = 0;\n\n\trcu_read_lock();\n\trdev = rcu_dereference(sh->raid_conf->disks[disk_idx].replacement);\n\tif (rdev\n\t    && !test_bit(Faulty, &rdev->flags)\n\t    && !test_bit(In_sync, &rdev->flags)\n\t    && (rdev->recovery_offset <= sh->sector\n\t\t|| rdev->mddev->recovery_cp <= sh->sector))\n\t\trv = 1;\n\trcu_read_unlock();\n\treturn rv;\n}\n\nstatic int need_this_block(struct stripe_head *sh, struct stripe_head_state *s,\n\t\t\t   int disk_idx, int disks)\n{\n\tstruct r5dev *dev = &sh->dev[disk_idx];\n\tstruct r5dev *fdev[2] = { &sh->dev[s->failed_num[0]],\n\t\t\t\t  &sh->dev[s->failed_num[1]] };\n\tint i;\n\tbool force_rcw = (sh->raid_conf->rmw_level == PARITY_DISABLE_RMW);\n\n\n\tif (test_bit(R5_LOCKED, &dev->flags) ||\n\t    test_bit(R5_UPTODATE, &dev->flags))\n\t\t \n\t\treturn 0;\n\n\tif (dev->toread ||\n\t    (dev->towrite && !test_bit(R5_OVERWRITE, &dev->flags)))\n\t\t \n\t\treturn 1;\n\n\tif (s->syncing || s->expanding ||\n\t    (s->replacing && want_replace(sh, disk_idx)))\n\t\t \n\t\treturn 1;\n\n\tif ((s->failed >= 1 && fdev[0]->toread) ||\n\t    (s->failed >= 2 && fdev[1]->toread))\n\t\t \n\t\treturn 1;\n\n\t \n\tif (!s->failed || !s->to_write)\n\t\treturn 0;\n\n\tif (test_bit(R5_Insync, &dev->flags) &&\n\t    !test_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\t \n\t\treturn 0;\n\n\tfor (i = 0; i < s->failed && i < 2; i++) {\n\t\tif (fdev[i]->towrite &&\n\t\t    !test_bit(R5_UPTODATE, &fdev[i]->flags) &&\n\t\t    !test_bit(R5_OVERWRITE, &fdev[i]->flags))\n\t\t\t \n\t\t\treturn 1;\n\n\t\tif (s->failed >= 2 &&\n\t\t    (fdev[i]->towrite ||\n\t\t     s->failed_num[i] == sh->pd_idx ||\n\t\t     s->failed_num[i] == sh->qd_idx) &&\n\t\t    !test_bit(R5_UPTODATE, &fdev[i]->flags))\n\t\t\t \n\t\t\tforce_rcw = true;\n\t}\n\n\t \n\tif (!force_rcw &&\n\t    sh->sector < sh->raid_conf->mddev->recovery_cp)\n\t\t \n\t\treturn 0;\n\tfor (i = 0; i < s->failed && i < 2; i++) {\n\t\tif (s->failed_num[i] != sh->pd_idx &&\n\t\t    s->failed_num[i] != sh->qd_idx &&\n\t\t    !test_bit(R5_UPTODATE, &fdev[i]->flags) &&\n\t\t    !test_bit(R5_OVERWRITE, &fdev[i]->flags))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int fetch_block(struct stripe_head *sh, struct stripe_head_state *s,\n\t\t       int disk_idx, int disks)\n{\n\tstruct r5dev *dev = &sh->dev[disk_idx];\n\n\t \n\tif (need_this_block(sh, s, disk_idx, disks)) {\n\t\t \n\t\tBUG_ON(test_bit(R5_Wantcompute, &dev->flags));\n\t\tBUG_ON(test_bit(R5_Wantread, &dev->flags));\n\t\tBUG_ON(sh->batch_head);\n\n\t\t \n\n\t\tif ((s->uptodate == disks - 1) &&\n\t\t    ((sh->qd_idx >= 0 && sh->pd_idx == disk_idx) ||\n\t\t    (s->failed && (disk_idx == s->failed_num[0] ||\n\t\t\t\t   disk_idx == s->failed_num[1])))) {\n\t\t\t \n\t\t\tpr_debug(\"Computing stripe %llu block %d\\n\",\n\t\t\t       (unsigned long long)sh->sector, disk_idx);\n\t\t\tset_bit(STRIPE_COMPUTE_RUN, &sh->state);\n\t\t\tset_bit(STRIPE_OP_COMPUTE_BLK, &s->ops_request);\n\t\t\tset_bit(R5_Wantcompute, &dev->flags);\n\t\t\tsh->ops.target = disk_idx;\n\t\t\tsh->ops.target2 = -1;  \n\t\t\ts->req_compute = 1;\n\t\t\t \n\t\t\ts->uptodate++;\n\t\t\treturn 1;\n\t\t} else if (s->uptodate == disks-2 && s->failed >= 2) {\n\t\t\t \n\t\t\tint other;\n\t\t\tfor (other = disks; other--; ) {\n\t\t\t\tif (other == disk_idx)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (!test_bit(R5_UPTODATE,\n\t\t\t\t      &sh->dev[other].flags))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tBUG_ON(other < 0);\n\t\t\tpr_debug(\"Computing stripe %llu blocks %d,%d\\n\",\n\t\t\t       (unsigned long long)sh->sector,\n\t\t\t       disk_idx, other);\n\t\t\tset_bit(STRIPE_COMPUTE_RUN, &sh->state);\n\t\t\tset_bit(STRIPE_OP_COMPUTE_BLK, &s->ops_request);\n\t\t\tset_bit(R5_Wantcompute, &sh->dev[disk_idx].flags);\n\t\t\tset_bit(R5_Wantcompute, &sh->dev[other].flags);\n\t\t\tsh->ops.target = disk_idx;\n\t\t\tsh->ops.target2 = other;\n\t\t\ts->uptodate += 2;\n\t\t\ts->req_compute = 1;\n\t\t\treturn 1;\n\t\t} else if (test_bit(R5_Insync, &dev->flags)) {\n\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\tset_bit(R5_Wantread, &dev->flags);\n\t\t\ts->locked++;\n\t\t\tpr_debug(\"Reading block %d (sync=%d)\\n\",\n\t\t\t\tdisk_idx, s->syncing);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic void handle_stripe_fill(struct stripe_head *sh,\n\t\t\t       struct stripe_head_state *s,\n\t\t\t       int disks)\n{\n\tint i;\n\n\t \n\tif (!test_bit(STRIPE_COMPUTE_RUN, &sh->state) && !sh->check_state &&\n\t    !sh->reconstruct_state) {\n\n\t\t \n\t\tif (s->to_read && s->injournal && s->failed) {\n\t\t\tif (test_bit(STRIPE_R5C_CACHING, &sh->state))\n\t\t\t\tr5c_make_stripe_write_out(sh);\n\t\t\tgoto out;\n\t\t}\n\n\t\tfor (i = disks; i--; )\n\t\t\tif (fetch_block(sh, s, i, disks))\n\t\t\t\tbreak;\n\t}\nout:\n\tset_bit(STRIPE_HANDLE, &sh->state);\n}\n\nstatic void break_stripe_batch_list(struct stripe_head *head_sh,\n\t\t\t\t    unsigned long handle_flags);\n \nstatic void handle_stripe_clean_event(struct r5conf *conf,\n\tstruct stripe_head *sh, int disks)\n{\n\tint i;\n\tstruct r5dev *dev;\n\tint discard_pending = 0;\n\tstruct stripe_head *head_sh = sh;\n\tbool do_endio = false;\n\n\tfor (i = disks; i--; )\n\t\tif (sh->dev[i].written) {\n\t\t\tdev = &sh->dev[i];\n\t\t\tif (!test_bit(R5_LOCKED, &dev->flags) &&\n\t\t\t    (test_bit(R5_UPTODATE, &dev->flags) ||\n\t\t\t     test_bit(R5_Discard, &dev->flags) ||\n\t\t\t     test_bit(R5_SkipCopy, &dev->flags))) {\n\t\t\t\t \n\t\t\t\tstruct bio *wbi, *wbi2;\n\t\t\t\tpr_debug(\"Return write for disc %d\\n\", i);\n\t\t\t\tif (test_and_clear_bit(R5_Discard, &dev->flags))\n\t\t\t\t\tclear_bit(R5_UPTODATE, &dev->flags);\n\t\t\t\tif (test_and_clear_bit(R5_SkipCopy, &dev->flags)) {\n\t\t\t\t\tWARN_ON(test_bit(R5_UPTODATE, &dev->flags));\n\t\t\t\t}\n\t\t\t\tdo_endio = true;\n\nreturnbi:\n\t\t\t\tdev->page = dev->orig_page;\n\t\t\t\twbi = dev->written;\n\t\t\t\tdev->written = NULL;\n\t\t\t\twhile (wbi && wbi->bi_iter.bi_sector <\n\t\t\t\t\tdev->sector + RAID5_STRIPE_SECTORS(conf)) {\n\t\t\t\t\twbi2 = r5_next_bio(conf, wbi, dev->sector);\n\t\t\t\t\tmd_write_end(conf->mddev);\n\t\t\t\t\tbio_endio(wbi);\n\t\t\t\t\twbi = wbi2;\n\t\t\t\t}\n\t\t\t\tmd_bitmap_endwrite(conf->mddev->bitmap, sh->sector,\n\t\t\t\t\t\t   RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t\t\t   !test_bit(STRIPE_DEGRADED, &sh->state),\n\t\t\t\t\t\t   0);\n\t\t\t\tif (head_sh->batch_head) {\n\t\t\t\t\tsh = list_first_entry(&sh->batch_list,\n\t\t\t\t\t\t\t      struct stripe_head,\n\t\t\t\t\t\t\t      batch_list);\n\t\t\t\t\tif (sh != head_sh) {\n\t\t\t\t\t\tdev = &sh->dev[i];\n\t\t\t\t\t\tgoto returnbi;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tsh = head_sh;\n\t\t\t\tdev = &sh->dev[i];\n\t\t\t} else if (test_bit(R5_Discard, &dev->flags))\n\t\t\t\tdiscard_pending = 1;\n\t\t}\n\n\tlog_stripe_write_finished(sh);\n\n\tif (!discard_pending &&\n\t    test_bit(R5_Discard, &sh->dev[sh->pd_idx].flags)) {\n\t\tint hash;\n\t\tclear_bit(R5_Discard, &sh->dev[sh->pd_idx].flags);\n\t\tclear_bit(R5_UPTODATE, &sh->dev[sh->pd_idx].flags);\n\t\tif (sh->qd_idx >= 0) {\n\t\t\tclear_bit(R5_Discard, &sh->dev[sh->qd_idx].flags);\n\t\t\tclear_bit(R5_UPTODATE, &sh->dev[sh->qd_idx].flags);\n\t\t}\n\t\t \n\t\tclear_bit(STRIPE_DISCARD, &sh->state);\n\t\t \nunhash:\n\t\thash = sh->hash_lock_index;\n\t\tspin_lock_irq(conf->hash_locks + hash);\n\t\tremove_hash(sh);\n\t\tspin_unlock_irq(conf->hash_locks + hash);\n\t\tif (head_sh->batch_head) {\n\t\t\tsh = list_first_entry(&sh->batch_list,\n\t\t\t\t\t      struct stripe_head, batch_list);\n\t\t\tif (sh != head_sh)\n\t\t\t\t\tgoto unhash;\n\t\t}\n\t\tsh = head_sh;\n\n\t\tif (test_bit(STRIPE_SYNC_REQUESTED, &sh->state))\n\t\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\n\t}\n\n\tif (test_and_clear_bit(STRIPE_FULL_WRITE, &sh->state))\n\t\tif (atomic_dec_and_test(&conf->pending_full_writes))\n\t\t\tmd_wakeup_thread(conf->mddev->thread);\n\n\tif (head_sh->batch_head && do_endio)\n\t\tbreak_stripe_batch_list(head_sh, STRIPE_EXPAND_SYNC_FLAGS);\n}\n\n \nstatic inline bool uptodate_for_rmw(struct r5dev *dev)\n{\n\treturn (test_bit(R5_UPTODATE, &dev->flags)) &&\n\t\t(!test_bit(R5_InJournal, &dev->flags) ||\n\t\t test_bit(R5_OrigPageUPTDODATE, &dev->flags));\n}\n\nstatic int handle_stripe_dirtying(struct r5conf *conf,\n\t\t\t\t  struct stripe_head *sh,\n\t\t\t\t  struct stripe_head_state *s,\n\t\t\t\t  int disks)\n{\n\tint rmw = 0, rcw = 0, i;\n\tsector_t recovery_cp = conf->mddev->recovery_cp;\n\n\t \n\tif (conf->rmw_level == PARITY_DISABLE_RMW ||\n\t    (recovery_cp < MaxSector && sh->sector >= recovery_cp &&\n\t     s->failed == 0)) {\n\t\t \n\t\trcw = 1; rmw = 2;\n\t\tpr_debug(\"force RCW rmw_level=%u, recovery_cp=%llu sh->sector=%llu\\n\",\n\t\t\t conf->rmw_level, (unsigned long long)recovery_cp,\n\t\t\t (unsigned long long)sh->sector);\n\t} else for (i = disks; i--; ) {\n\t\t \n\t\tstruct r5dev *dev = &sh->dev[i];\n\t\tif (((dev->towrite && !delay_towrite(conf, dev, s)) ||\n\t\t     i == sh->pd_idx || i == sh->qd_idx ||\n\t\t     test_bit(R5_InJournal, &dev->flags)) &&\n\t\t    !test_bit(R5_LOCKED, &dev->flags) &&\n\t\t    !(uptodate_for_rmw(dev) ||\n\t\t      test_bit(R5_Wantcompute, &dev->flags))) {\n\t\t\tif (test_bit(R5_Insync, &dev->flags))\n\t\t\t\trmw++;\n\t\t\telse\n\t\t\t\trmw += 2*disks;   \n\t\t}\n\t\t \n\t\tif (!test_bit(R5_OVERWRITE, &dev->flags) &&\n\t\t    i != sh->pd_idx && i != sh->qd_idx &&\n\t\t    !test_bit(R5_LOCKED, &dev->flags) &&\n\t\t    !(test_bit(R5_UPTODATE, &dev->flags) ||\n\t\t      test_bit(R5_Wantcompute, &dev->flags))) {\n\t\t\tif (test_bit(R5_Insync, &dev->flags))\n\t\t\t\trcw++;\n\t\t\telse\n\t\t\t\trcw += 2*disks;\n\t\t}\n\t}\n\n\tpr_debug(\"for sector %llu state 0x%lx, rmw=%d rcw=%d\\n\",\n\t\t (unsigned long long)sh->sector, sh->state, rmw, rcw);\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\tif ((rmw < rcw || (rmw == rcw && conf->rmw_level == PARITY_PREFER_RMW)) && rmw > 0) {\n\t\t \n\t\tif (conf->mddev->queue)\n\t\t\tblk_add_trace_msg(conf->mddev->queue,\n\t\t\t\t\t  \"raid5 rmw %llu %d\",\n\t\t\t\t\t  (unsigned long long)sh->sector, rmw);\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (test_bit(R5_InJournal, &dev->flags) &&\n\t\t\t    dev->page == dev->orig_page &&\n\t\t\t    !test_bit(R5_LOCKED, &sh->dev[sh->pd_idx].flags)) {\n\t\t\t\t \n\t\t\t\tstruct page *p = alloc_page(GFP_NOIO);\n\n\t\t\t\tif (p) {\n\t\t\t\t\tdev->orig_page = p;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\tif (!test_and_set_bit(R5C_EXTRA_PAGE_IN_USE,\n\t\t\t\t\t\t      &conf->cache_state)) {\n\t\t\t\t\tr5c_use_extra_page(sh);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\tset_bit(STRIPE_DELAYED, &sh->state);\n\t\t\t\ts->waiting_extra_page = 1;\n\t\t\t\treturn -EAGAIN;\n\t\t\t}\n\t\t}\n\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (((dev->towrite && !delay_towrite(conf, dev, s)) ||\n\t\t\t     i == sh->pd_idx || i == sh->qd_idx ||\n\t\t\t     test_bit(R5_InJournal, &dev->flags)) &&\n\t\t\t    !test_bit(R5_LOCKED, &dev->flags) &&\n\t\t\t    !(uptodate_for_rmw(dev) ||\n\t\t\t      test_bit(R5_Wantcompute, &dev->flags)) &&\n\t\t\t    test_bit(R5_Insync, &dev->flags)) {\n\t\t\t\tif (test_bit(STRIPE_PREREAD_ACTIVE,\n\t\t\t\t\t     &sh->state)) {\n\t\t\t\t\tpr_debug(\"Read_old block %d for r-m-w\\n\",\n\t\t\t\t\t\t i);\n\t\t\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\t\t\tset_bit(R5_Wantread, &dev->flags);\n\t\t\t\t\ts->locked++;\n\t\t\t\t} else\n\t\t\t\t\tset_bit(STRIPE_DELAYED, &sh->state);\n\t\t\t}\n\t\t}\n\t}\n\tif ((rcw < rmw || (rcw == rmw && conf->rmw_level != PARITY_PREFER_RMW)) && rcw > 0) {\n\t\t \n\t\tint qread =0;\n\t\trcw = 0;\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (!test_bit(R5_OVERWRITE, &dev->flags) &&\n\t\t\t    i != sh->pd_idx && i != sh->qd_idx &&\n\t\t\t    !test_bit(R5_LOCKED, &dev->flags) &&\n\t\t\t    !(test_bit(R5_UPTODATE, &dev->flags) ||\n\t\t\t      test_bit(R5_Wantcompute, &dev->flags))) {\n\t\t\t\trcw++;\n\t\t\t\tif (test_bit(R5_Insync, &dev->flags) &&\n\t\t\t\t    test_bit(STRIPE_PREREAD_ACTIVE,\n\t\t\t\t\t     &sh->state)) {\n\t\t\t\t\tpr_debug(\"Read_old block \"\n\t\t\t\t\t\t\"%d for Reconstruct\\n\", i);\n\t\t\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\t\t\tset_bit(R5_Wantread, &dev->flags);\n\t\t\t\t\ts->locked++;\n\t\t\t\t\tqread++;\n\t\t\t\t} else\n\t\t\t\t\tset_bit(STRIPE_DELAYED, &sh->state);\n\t\t\t}\n\t\t}\n\t\tif (rcw && conf->mddev->queue)\n\t\t\tblk_add_trace_msg(conf->mddev->queue, \"raid5 rcw %llu %d %d %d\",\n\t\t\t\t\t  (unsigned long long)sh->sector,\n\t\t\t\t\t  rcw, qread, test_bit(STRIPE_DELAYED, &sh->state));\n\t}\n\n\tif (rcw > disks && rmw > disks &&\n\t    !test_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\tset_bit(STRIPE_DELAYED, &sh->state);\n\n\t \n\t \n\tif ((s->req_compute || !test_bit(STRIPE_COMPUTE_RUN, &sh->state)) &&\n\t    (s->locked == 0 && (rcw == 0 || rmw == 0) &&\n\t     !test_bit(STRIPE_BIT_DELAY, &sh->state)))\n\t\tschedule_reconstruction(sh, s, rcw == 0, 0);\n\treturn 0;\n}\n\nstatic void handle_parity_checks5(struct r5conf *conf, struct stripe_head *sh,\n\t\t\t\tstruct stripe_head_state *s, int disks)\n{\n\tstruct r5dev *dev = NULL;\n\n\tBUG_ON(sh->batch_head);\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\n\tswitch (sh->check_state) {\n\tcase check_state_idle:\n\t\t \n\t\tif (s->failed == 0) {\n\t\t\tBUG_ON(s->uptodate != disks);\n\t\t\tsh->check_state = check_state_run;\n\t\t\tset_bit(STRIPE_OP_CHECK, &s->ops_request);\n\t\t\tclear_bit(R5_UPTODATE, &sh->dev[sh->pd_idx].flags);\n\t\t\ts->uptodate--;\n\t\t\tbreak;\n\t\t}\n\t\tdev = &sh->dev[s->failed_num[0]];\n\t\tfallthrough;\n\tcase check_state_compute_result:\n\t\tsh->check_state = check_state_idle;\n\t\tif (!dev)\n\t\t\tdev = &sh->dev[sh->pd_idx];\n\n\t\t \n\t\tif (test_bit(STRIPE_INSYNC, &sh->state))\n\t\t\tbreak;\n\n\t\t \n\t\tBUG_ON(!test_bit(R5_UPTODATE, &dev->flags));\n\t\tBUG_ON(s->uptodate != disks);\n\n\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\ts->locked++;\n\t\tset_bit(R5_Wantwrite, &dev->flags);\n\n\t\tclear_bit(STRIPE_DEGRADED, &sh->state);\n\t\tset_bit(STRIPE_INSYNC, &sh->state);\n\t\tbreak;\n\tcase check_state_run:\n\t\tbreak;  \n\tcase check_state_check_result:\n\t\tsh->check_state = check_state_idle;\n\n\t\t \n\t\tif (s->failed)\n\t\t\tbreak;\n\n\t\t \n\t\tif ((sh->ops.zero_sum_result & SUM_CHECK_P_RESULT) == 0)\n\t\t\t \n\t\t\tset_bit(STRIPE_INSYNC, &sh->state);\n\t\telse {\n\t\t\tatomic64_add(RAID5_STRIPE_SECTORS(conf), &conf->mddev->resync_mismatches);\n\t\t\tif (test_bit(MD_RECOVERY_CHECK, &conf->mddev->recovery)) {\n\t\t\t\t \n\t\t\t\tset_bit(STRIPE_INSYNC, &sh->state);\n\t\t\t\tpr_warn_ratelimited(\"%s: mismatch sector in range \"\n\t\t\t\t\t\t    \"%llu-%llu\\n\", mdname(conf->mddev),\n\t\t\t\t\t\t    (unsigned long long) sh->sector,\n\t\t\t\t\t\t    (unsigned long long) sh->sector +\n\t\t\t\t\t\t    RAID5_STRIPE_SECTORS(conf));\n\t\t\t} else {\n\t\t\t\tsh->check_state = check_state_compute_run;\n\t\t\t\tset_bit(STRIPE_COMPUTE_RUN, &sh->state);\n\t\t\t\tset_bit(STRIPE_OP_COMPUTE_BLK, &s->ops_request);\n\t\t\t\tset_bit(R5_Wantcompute,\n\t\t\t\t\t&sh->dev[sh->pd_idx].flags);\n\t\t\t\tsh->ops.target = sh->pd_idx;\n\t\t\t\tsh->ops.target2 = -1;\n\t\t\t\ts->uptodate++;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase check_state_compute_run:\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"%s: unknown check_state: %d sector: %llu\\n\",\n\t\t       __func__, sh->check_state,\n\t\t       (unsigned long long) sh->sector);\n\t\tBUG();\n\t}\n}\n\nstatic void handle_parity_checks6(struct r5conf *conf, struct stripe_head *sh,\n\t\t\t\t  struct stripe_head_state *s,\n\t\t\t\t  int disks)\n{\n\tint pd_idx = sh->pd_idx;\n\tint qd_idx = sh->qd_idx;\n\tstruct r5dev *dev;\n\n\tBUG_ON(sh->batch_head);\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\n\tBUG_ON(s->failed > 2);\n\n\t \n\n\tswitch (sh->check_state) {\n\tcase check_state_idle:\n\t\t \n\t\tif (s->failed == s->q_failed) {\n\t\t\t \n\t\t\tsh->check_state = check_state_run;\n\t\t}\n\t\tif (!s->q_failed && s->failed < 2) {\n\t\t\t \n\t\t\tif (sh->check_state == check_state_run)\n\t\t\t\tsh->check_state = check_state_run_pq;\n\t\t\telse\n\t\t\t\tsh->check_state = check_state_run_q;\n\t\t}\n\n\t\t \n\t\tsh->ops.zero_sum_result = 0;\n\n\t\tif (sh->check_state == check_state_run) {\n\t\t\t \n\t\t\tclear_bit(R5_UPTODATE, &sh->dev[pd_idx].flags);\n\t\t\ts->uptodate--;\n\t\t}\n\t\tif (sh->check_state >= check_state_run &&\n\t\t    sh->check_state <= check_state_run_pq) {\n\t\t\t \n\t\t\tset_bit(STRIPE_OP_CHECK, &s->ops_request);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tBUG_ON(s->failed != 2);\n\t\tfallthrough;\n\tcase check_state_compute_result:\n\t\tsh->check_state = check_state_idle;\n\n\t\t \n\t\tif (test_bit(STRIPE_INSYNC, &sh->state))\n\t\t\tbreak;\n\n\t\t \n\t\tdev = NULL;\n\t\tif (s->failed == 2) {\n\t\t\tdev = &sh->dev[s->failed_num[1]];\n\t\t\ts->locked++;\n\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\tset_bit(R5_Wantwrite, &dev->flags);\n\t\t}\n\t\tif (s->failed >= 1) {\n\t\t\tdev = &sh->dev[s->failed_num[0]];\n\t\t\ts->locked++;\n\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\tset_bit(R5_Wantwrite, &dev->flags);\n\t\t}\n\t\tif (sh->ops.zero_sum_result & SUM_CHECK_P_RESULT) {\n\t\t\tdev = &sh->dev[pd_idx];\n\t\t\ts->locked++;\n\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\tset_bit(R5_Wantwrite, &dev->flags);\n\t\t}\n\t\tif (sh->ops.zero_sum_result & SUM_CHECK_Q_RESULT) {\n\t\t\tdev = &sh->dev[qd_idx];\n\t\t\ts->locked++;\n\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\tset_bit(R5_Wantwrite, &dev->flags);\n\t\t}\n\t\tif (WARN_ONCE(dev && !test_bit(R5_UPTODATE, &dev->flags),\n\t\t\t      \"%s: disk%td not up to date\\n\",\n\t\t\t      mdname(conf->mddev),\n\t\t\t      dev - (struct r5dev *) &sh->dev)) {\n\t\t\tclear_bit(R5_LOCKED, &dev->flags);\n\t\t\tclear_bit(R5_Wantwrite, &dev->flags);\n\t\t\ts->locked--;\n\t\t}\n\t\tclear_bit(STRIPE_DEGRADED, &sh->state);\n\n\t\tset_bit(STRIPE_INSYNC, &sh->state);\n\t\tbreak;\n\tcase check_state_run:\n\tcase check_state_run_q:\n\tcase check_state_run_pq:\n\t\tbreak;  \n\tcase check_state_check_result:\n\t\tsh->check_state = check_state_idle;\n\n\t\t \n\t\tif (sh->ops.zero_sum_result == 0) {\n\t\t\t \n\t\t\tif (!s->failed)\n\t\t\t\tset_bit(STRIPE_INSYNC, &sh->state);\n\t\t\telse {\n\t\t\t\t \n\t\t\t\tsh->check_state = check_state_compute_result;\n\t\t\t\t \n\t\t\t}\n\t\t} else {\n\t\t\tatomic64_add(RAID5_STRIPE_SECTORS(conf), &conf->mddev->resync_mismatches);\n\t\t\tif (test_bit(MD_RECOVERY_CHECK, &conf->mddev->recovery)) {\n\t\t\t\t \n\t\t\t\tset_bit(STRIPE_INSYNC, &sh->state);\n\t\t\t\tpr_warn_ratelimited(\"%s: mismatch sector in range \"\n\t\t\t\t\t\t    \"%llu-%llu\\n\", mdname(conf->mddev),\n\t\t\t\t\t\t    (unsigned long long) sh->sector,\n\t\t\t\t\t\t    (unsigned long long) sh->sector +\n\t\t\t\t\t\t    RAID5_STRIPE_SECTORS(conf));\n\t\t\t} else {\n\t\t\t\tint *target = &sh->ops.target;\n\n\t\t\t\tsh->ops.target = -1;\n\t\t\t\tsh->ops.target2 = -1;\n\t\t\t\tsh->check_state = check_state_compute_run;\n\t\t\t\tset_bit(STRIPE_COMPUTE_RUN, &sh->state);\n\t\t\t\tset_bit(STRIPE_OP_COMPUTE_BLK, &s->ops_request);\n\t\t\t\tif (sh->ops.zero_sum_result & SUM_CHECK_P_RESULT) {\n\t\t\t\t\tset_bit(R5_Wantcompute,\n\t\t\t\t\t\t&sh->dev[pd_idx].flags);\n\t\t\t\t\t*target = pd_idx;\n\t\t\t\t\ttarget = &sh->ops.target2;\n\t\t\t\t\ts->uptodate++;\n\t\t\t\t}\n\t\t\t\tif (sh->ops.zero_sum_result & SUM_CHECK_Q_RESULT) {\n\t\t\t\t\tset_bit(R5_Wantcompute,\n\t\t\t\t\t\t&sh->dev[qd_idx].flags);\n\t\t\t\t\t*target = qd_idx;\n\t\t\t\t\ts->uptodate++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase check_state_compute_run:\n\t\tbreak;\n\tdefault:\n\t\tpr_warn(\"%s: unknown check_state: %d sector: %llu\\n\",\n\t\t\t__func__, sh->check_state,\n\t\t\t(unsigned long long) sh->sector);\n\t\tBUG();\n\t}\n}\n\nstatic void handle_stripe_expansion(struct r5conf *conf, struct stripe_head *sh)\n{\n\tint i;\n\n\t \n\tstruct dma_async_tx_descriptor *tx = NULL;\n\tBUG_ON(sh->batch_head);\n\tclear_bit(STRIPE_EXPAND_SOURCE, &sh->state);\n\tfor (i = 0; i < sh->disks; i++)\n\t\tif (i != sh->pd_idx && i != sh->qd_idx) {\n\t\t\tint dd_idx, j;\n\t\t\tstruct stripe_head *sh2;\n\t\t\tstruct async_submit_ctl submit;\n\n\t\t\tsector_t bn = raid5_compute_blocknr(sh, i, 1);\n\t\t\tsector_t s = raid5_compute_sector(conf, bn, 0,\n\t\t\t\t\t\t\t  &dd_idx, NULL);\n\t\t\tsh2 = raid5_get_active_stripe(conf, NULL, s,\n\t\t\t\tR5_GAS_NOBLOCK | R5_GAS_NOQUIESCE);\n\t\t\tif (sh2 == NULL)\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\tif (!test_bit(STRIPE_EXPANDING, &sh2->state) ||\n\t\t\t   test_bit(R5_Expanded, &sh2->dev[dd_idx].flags)) {\n\t\t\t\t \n\t\t\t\traid5_release_stripe(sh2);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tinit_async_submit(&submit, 0, tx, NULL, NULL, NULL);\n\t\t\ttx = async_memcpy(sh2->dev[dd_idx].page,\n\t\t\t\t\t  sh->dev[i].page, sh2->dev[dd_idx].offset,\n\t\t\t\t\t  sh->dev[i].offset, RAID5_STRIPE_SIZE(conf),\n\t\t\t\t\t  &submit);\n\n\t\t\tset_bit(R5_Expanded, &sh2->dev[dd_idx].flags);\n\t\t\tset_bit(R5_UPTODATE, &sh2->dev[dd_idx].flags);\n\t\t\tfor (j = 0; j < conf->raid_disks; j++)\n\t\t\t\tif (j != sh2->pd_idx &&\n\t\t\t\t    j != sh2->qd_idx &&\n\t\t\t\t    !test_bit(R5_Expanded, &sh2->dev[j].flags))\n\t\t\t\t\tbreak;\n\t\t\tif (j == conf->raid_disks) {\n\t\t\t\tset_bit(STRIPE_EXPAND_READY, &sh2->state);\n\t\t\t\tset_bit(STRIPE_HANDLE, &sh2->state);\n\t\t\t}\n\t\t\traid5_release_stripe(sh2);\n\n\t\t}\n\t \n\tasync_tx_quiesce(&tx);\n}\n\n \n\nstatic void analyse_stripe(struct stripe_head *sh, struct stripe_head_state *s)\n{\n\tstruct r5conf *conf = sh->raid_conf;\n\tint disks = sh->disks;\n\tstruct r5dev *dev;\n\tint i;\n\tint do_recovery = 0;\n\n\tmemset(s, 0, sizeof(*s));\n\n\ts->expanding = test_bit(STRIPE_EXPAND_SOURCE, &sh->state) && !sh->batch_head;\n\ts->expanded = test_bit(STRIPE_EXPAND_READY, &sh->state) && !sh->batch_head;\n\ts->failed_num[0] = -1;\n\ts->failed_num[1] = -1;\n\ts->log_failed = r5l_log_disk_error(conf);\n\n\t \n\trcu_read_lock();\n\tfor (i=disks; i--; ) {\n\t\tstruct md_rdev *rdev;\n\t\tsector_t first_bad;\n\t\tint bad_sectors;\n\t\tint is_bad = 0;\n\n\t\tdev = &sh->dev[i];\n\n\t\tpr_debug(\"check %d: state 0x%lx read %p write %p written %p\\n\",\n\t\t\t i, dev->flags,\n\t\t\t dev->toread, dev->towrite, dev->written);\n\t\t \n\t\tif (test_bit(R5_UPTODATE, &dev->flags) && dev->toread &&\n\t\t    !test_bit(STRIPE_BIOFILL_RUN, &sh->state))\n\t\t\tset_bit(R5_Wantfill, &dev->flags);\n\n\t\t \n\t\tif (test_bit(R5_LOCKED, &dev->flags))\n\t\t\ts->locked++;\n\t\tif (test_bit(R5_UPTODATE, &dev->flags))\n\t\t\ts->uptodate++;\n\t\tif (test_bit(R5_Wantcompute, &dev->flags)) {\n\t\t\ts->compute++;\n\t\t\tBUG_ON(s->compute > 2);\n\t\t}\n\n\t\tif (test_bit(R5_Wantfill, &dev->flags))\n\t\t\ts->to_fill++;\n\t\telse if (dev->toread)\n\t\t\ts->to_read++;\n\t\tif (dev->towrite) {\n\t\t\ts->to_write++;\n\t\t\tif (!test_bit(R5_OVERWRITE, &dev->flags))\n\t\t\t\ts->non_overwrite++;\n\t\t}\n\t\tif (dev->written)\n\t\t\ts->written++;\n\t\t \n\t\trdev = rcu_dereference(conf->disks[i].replacement);\n\t\tif (rdev && !test_bit(Faulty, &rdev->flags) &&\n\t\t    rdev->recovery_offset >= sh->sector + RAID5_STRIPE_SECTORS(conf) &&\n\t\t    !is_badblock(rdev, sh->sector, RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t &first_bad, &bad_sectors))\n\t\t\tset_bit(R5_ReadRepl, &dev->flags);\n\t\telse {\n\t\t\tif (rdev && !test_bit(Faulty, &rdev->flags))\n\t\t\t\tset_bit(R5_NeedReplace, &dev->flags);\n\t\t\telse\n\t\t\t\tclear_bit(R5_NeedReplace, &dev->flags);\n\t\t\trdev = rcu_dereference(conf->disks[i].rdev);\n\t\t\tclear_bit(R5_ReadRepl, &dev->flags);\n\t\t}\n\t\tif (rdev && test_bit(Faulty, &rdev->flags))\n\t\t\trdev = NULL;\n\t\tif (rdev) {\n\t\t\tis_bad = is_badblock(rdev, sh->sector, RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t\t     &first_bad, &bad_sectors);\n\t\t\tif (s->blocked_rdev == NULL\n\t\t\t    && (test_bit(Blocked, &rdev->flags)\n\t\t\t\t|| is_bad < 0)) {\n\t\t\t\tif (is_bad < 0)\n\t\t\t\t\tset_bit(BlockedBadBlocks,\n\t\t\t\t\t\t&rdev->flags);\n\t\t\t\ts->blocked_rdev = rdev;\n\t\t\t\tatomic_inc(&rdev->nr_pending);\n\t\t\t}\n\t\t}\n\t\tclear_bit(R5_Insync, &dev->flags);\n\t\tif (!rdev)\n\t\t\t ;\n\t\telse if (is_bad) {\n\t\t\t \n\t\t\tif (!test_bit(WriteErrorSeen, &rdev->flags) &&\n\t\t\t    test_bit(R5_UPTODATE, &dev->flags)) {\n\t\t\t\t \n\t\t\t\tset_bit(R5_Insync, &dev->flags);\n\t\t\t\tset_bit(R5_ReadError, &dev->flags);\n\t\t\t}\n\t\t} else if (test_bit(In_sync, &rdev->flags))\n\t\t\tset_bit(R5_Insync, &dev->flags);\n\t\telse if (sh->sector + RAID5_STRIPE_SECTORS(conf) <= rdev->recovery_offset)\n\t\t\t \n\t\t\tset_bit(R5_Insync, &dev->flags);\n\t\telse if (test_bit(R5_UPTODATE, &dev->flags) &&\n\t\t\t test_bit(R5_Expanded, &dev->flags))\n\t\t\t \n\t\t\tset_bit(R5_Insync, &dev->flags);\n\n\t\tif (test_bit(R5_WriteError, &dev->flags)) {\n\t\t\t \n\t\t\tstruct md_rdev *rdev2 = rcu_dereference(\n\t\t\t\tconf->disks[i].rdev);\n\t\t\tif (rdev2 == rdev)\n\t\t\t\tclear_bit(R5_Insync, &dev->flags);\n\t\t\tif (rdev2 && !test_bit(Faulty, &rdev2->flags)) {\n\t\t\t\ts->handle_bad_blocks = 1;\n\t\t\t\tatomic_inc(&rdev2->nr_pending);\n\t\t\t} else\n\t\t\t\tclear_bit(R5_WriteError, &dev->flags);\n\t\t}\n\t\tif (test_bit(R5_MadeGood, &dev->flags)) {\n\t\t\t \n\t\t\tstruct md_rdev *rdev2 = rcu_dereference(\n\t\t\t\tconf->disks[i].rdev);\n\t\t\tif (rdev2 && !test_bit(Faulty, &rdev2->flags)) {\n\t\t\t\ts->handle_bad_blocks = 1;\n\t\t\t\tatomic_inc(&rdev2->nr_pending);\n\t\t\t} else\n\t\t\t\tclear_bit(R5_MadeGood, &dev->flags);\n\t\t}\n\t\tif (test_bit(R5_MadeGoodRepl, &dev->flags)) {\n\t\t\tstruct md_rdev *rdev2 = rcu_dereference(\n\t\t\t\tconf->disks[i].replacement);\n\t\t\tif (rdev2 && !test_bit(Faulty, &rdev2->flags)) {\n\t\t\t\ts->handle_bad_blocks = 1;\n\t\t\t\tatomic_inc(&rdev2->nr_pending);\n\t\t\t} else\n\t\t\t\tclear_bit(R5_MadeGoodRepl, &dev->flags);\n\t\t}\n\t\tif (!test_bit(R5_Insync, &dev->flags)) {\n\t\t\t \n\t\t\tclear_bit(R5_ReadError, &dev->flags);\n\t\t\tclear_bit(R5_ReWrite, &dev->flags);\n\t\t}\n\t\tif (test_bit(R5_ReadError, &dev->flags))\n\t\t\tclear_bit(R5_Insync, &dev->flags);\n\t\tif (!test_bit(R5_Insync, &dev->flags)) {\n\t\t\tif (s->failed < 2)\n\t\t\t\ts->failed_num[s->failed] = i;\n\t\t\ts->failed++;\n\t\t\tif (rdev && !test_bit(Faulty, &rdev->flags))\n\t\t\t\tdo_recovery = 1;\n\t\t\telse if (!rdev) {\n\t\t\t\trdev = rcu_dereference(\n\t\t\t\t    conf->disks[i].replacement);\n\t\t\t\tif (rdev && !test_bit(Faulty, &rdev->flags))\n\t\t\t\t\tdo_recovery = 1;\n\t\t\t}\n\t\t}\n\n\t\tif (test_bit(R5_InJournal, &dev->flags))\n\t\t\ts->injournal++;\n\t\tif (test_bit(R5_InJournal, &dev->flags) && dev->written)\n\t\t\ts->just_cached++;\n\t}\n\tif (test_bit(STRIPE_SYNCING, &sh->state)) {\n\t\t \n\t\tif (do_recovery ||\n\t\t    sh->sector >= conf->mddev->recovery_cp ||\n\t\t    test_bit(MD_RECOVERY_REQUESTED, &(conf->mddev->recovery)))\n\t\t\ts->syncing = 1;\n\t\telse\n\t\t\ts->replacing = 1;\n\t}\n\trcu_read_unlock();\n}\n\n \nstatic int clear_batch_ready(struct stripe_head *sh)\n{\n\tstruct stripe_head *tmp;\n\tif (!test_and_clear_bit(STRIPE_BATCH_READY, &sh->state))\n\t\treturn (sh->batch_head && sh->batch_head != sh);\n\tspin_lock(&sh->stripe_lock);\n\tif (!sh->batch_head) {\n\t\tspin_unlock(&sh->stripe_lock);\n\t\treturn 0;\n\t}\n\n\t \n\tif (sh->batch_head != sh) {\n\t\tspin_unlock(&sh->stripe_lock);\n\t\treturn 1;\n\t}\n\tspin_lock(&sh->batch_lock);\n\tlist_for_each_entry(tmp, &sh->batch_list, batch_list)\n\t\tclear_bit(STRIPE_BATCH_READY, &tmp->state);\n\tspin_unlock(&sh->batch_lock);\n\tspin_unlock(&sh->stripe_lock);\n\n\t \n\treturn 0;\n}\n\nstatic void break_stripe_batch_list(struct stripe_head *head_sh,\n\t\t\t\t    unsigned long handle_flags)\n{\n\tstruct stripe_head *sh, *next;\n\tint i;\n\tint do_wakeup = 0;\n\n\tlist_for_each_entry_safe(sh, next, &head_sh->batch_list, batch_list) {\n\n\t\tlist_del_init(&sh->batch_list);\n\n\t\tWARN_ONCE(sh->state & ((1 << STRIPE_ACTIVE) |\n\t\t\t\t\t  (1 << STRIPE_SYNCING) |\n\t\t\t\t\t  (1 << STRIPE_REPLACED) |\n\t\t\t\t\t  (1 << STRIPE_DELAYED) |\n\t\t\t\t\t  (1 << STRIPE_BIT_DELAY) |\n\t\t\t\t\t  (1 << STRIPE_FULL_WRITE) |\n\t\t\t\t\t  (1 << STRIPE_BIOFILL_RUN) |\n\t\t\t\t\t  (1 << STRIPE_COMPUTE_RUN)  |\n\t\t\t\t\t  (1 << STRIPE_DISCARD) |\n\t\t\t\t\t  (1 << STRIPE_BATCH_READY) |\n\t\t\t\t\t  (1 << STRIPE_BATCH_ERR) |\n\t\t\t\t\t  (1 << STRIPE_BITMAP_PENDING)),\n\t\t\t\"stripe state: %lx\\n\", sh->state);\n\t\tWARN_ONCE(head_sh->state & ((1 << STRIPE_DISCARD) |\n\t\t\t\t\t      (1 << STRIPE_REPLACED)),\n\t\t\t\"head stripe state: %lx\\n\", head_sh->state);\n\n\t\tset_mask_bits(&sh->state, ~(STRIPE_EXPAND_SYNC_FLAGS |\n\t\t\t\t\t    (1 << STRIPE_PREREAD_ACTIVE) |\n\t\t\t\t\t    (1 << STRIPE_DEGRADED) |\n\t\t\t\t\t    (1 << STRIPE_ON_UNPLUG_LIST)),\n\t\t\t      head_sh->state & (1 << STRIPE_INSYNC));\n\n\t\tsh->check_state = head_sh->check_state;\n\t\tsh->reconstruct_state = head_sh->reconstruct_state;\n\t\tspin_lock_irq(&sh->stripe_lock);\n\t\tsh->batch_head = NULL;\n\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\tfor (i = 0; i < sh->disks; i++) {\n\t\t\tif (test_and_clear_bit(R5_Overlap, &sh->dev[i].flags))\n\t\t\t\tdo_wakeup = 1;\n\t\t\tsh->dev[i].flags = head_sh->dev[i].flags &\n\t\t\t\t(~((1 << R5_WriteError) | (1 << R5_Overlap)));\n\t\t}\n\t\tif (handle_flags == 0 ||\n\t\t    sh->state & handle_flags)\n\t\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\traid5_release_stripe(sh);\n\t}\n\tspin_lock_irq(&head_sh->stripe_lock);\n\thead_sh->batch_head = NULL;\n\tspin_unlock_irq(&head_sh->stripe_lock);\n\tfor (i = 0; i < head_sh->disks; i++)\n\t\tif (test_and_clear_bit(R5_Overlap, &head_sh->dev[i].flags))\n\t\t\tdo_wakeup = 1;\n\tif (head_sh->state & handle_flags)\n\t\tset_bit(STRIPE_HANDLE, &head_sh->state);\n\n\tif (do_wakeup)\n\t\twake_up(&head_sh->raid_conf->wait_for_overlap);\n}\n\nstatic void handle_stripe(struct stripe_head *sh)\n{\n\tstruct stripe_head_state s;\n\tstruct r5conf *conf = sh->raid_conf;\n\tint i;\n\tint prexor;\n\tint disks = sh->disks;\n\tstruct r5dev *pdev, *qdev;\n\n\tclear_bit(STRIPE_HANDLE, &sh->state);\n\n\t \n\tif (clear_batch_ready(sh))\n\t\treturn;\n\n\tif (test_and_set_bit_lock(STRIPE_ACTIVE, &sh->state)) {\n\t\t \n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\treturn;\n\t}\n\n\tif (test_and_clear_bit(STRIPE_BATCH_ERR, &sh->state))\n\t\tbreak_stripe_batch_list(sh, 0);\n\n\tif (test_bit(STRIPE_SYNC_REQUESTED, &sh->state) && !sh->batch_head) {\n\t\tspin_lock(&sh->stripe_lock);\n\t\t \n\t\tif (!test_bit(STRIPE_R5C_PARTIAL_STRIPE, &sh->state) &&\n\t\t    !test_bit(STRIPE_R5C_FULL_STRIPE, &sh->state) &&\n\t\t    !test_bit(STRIPE_DISCARD, &sh->state) &&\n\t\t    test_and_clear_bit(STRIPE_SYNC_REQUESTED, &sh->state)) {\n\t\t\tset_bit(STRIPE_SYNCING, &sh->state);\n\t\t\tclear_bit(STRIPE_INSYNC, &sh->state);\n\t\t\tclear_bit(STRIPE_REPLACED, &sh->state);\n\t\t}\n\t\tspin_unlock(&sh->stripe_lock);\n\t}\n\tclear_bit(STRIPE_DELAYED, &sh->state);\n\n\tpr_debug(\"handling stripe %llu, state=%#lx cnt=%d, \"\n\t\t\"pd_idx=%d, qd_idx=%d\\n, check:%d, reconstruct:%d\\n\",\n\t       (unsigned long long)sh->sector, sh->state,\n\t       atomic_read(&sh->count), sh->pd_idx, sh->qd_idx,\n\t       sh->check_state, sh->reconstruct_state);\n\n\tanalyse_stripe(sh, &s);\n\n\tif (test_bit(STRIPE_LOG_TRAPPED, &sh->state))\n\t\tgoto finish;\n\n\tif (s.handle_bad_blocks ||\n\t    test_bit(MD_SB_CHANGE_PENDING, &conf->mddev->sb_flags)) {\n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\tgoto finish;\n\t}\n\n\tif (unlikely(s.blocked_rdev)) {\n\t\tif (s.syncing || s.expanding || s.expanded ||\n\t\t    s.replacing || s.to_write || s.written) {\n\t\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\t\tgoto finish;\n\t\t}\n\t\t \n\t\trdev_dec_pending(s.blocked_rdev, conf->mddev);\n\t\ts.blocked_rdev = NULL;\n\t}\n\n\tif (s.to_fill && !test_bit(STRIPE_BIOFILL_RUN, &sh->state)) {\n\t\tset_bit(STRIPE_OP_BIOFILL, &s.ops_request);\n\t\tset_bit(STRIPE_BIOFILL_RUN, &sh->state);\n\t}\n\n\tpr_debug(\"locked=%d uptodate=%d to_read=%d\"\n\t       \" to_write=%d failed=%d failed_num=%d,%d\\n\",\n\t       s.locked, s.uptodate, s.to_read, s.to_write, s.failed,\n\t       s.failed_num[0], s.failed_num[1]);\n\t \n\tif (s.failed > conf->max_degraded ||\n\t    (s.log_failed && s.injournal == 0)) {\n\t\tsh->check_state = 0;\n\t\tsh->reconstruct_state = 0;\n\t\tbreak_stripe_batch_list(sh, 0);\n\t\tif (s.to_read+s.to_write+s.written)\n\t\t\thandle_failed_stripe(conf, sh, &s, disks);\n\t\tif (s.syncing + s.replacing)\n\t\t\thandle_failed_sync(conf, sh, &s);\n\t}\n\n\t \n\tprexor = 0;\n\tif (sh->reconstruct_state == reconstruct_state_prexor_drain_result)\n\t\tprexor = 1;\n\tif (sh->reconstruct_state == reconstruct_state_drain_result ||\n\t    sh->reconstruct_state == reconstruct_state_prexor_drain_result) {\n\t\tsh->reconstruct_state = reconstruct_state_idle;\n\n\t\t \n\t\tBUG_ON(!test_bit(R5_UPTODATE, &sh->dev[sh->pd_idx].flags) &&\n\t\t       !test_bit(R5_Discard, &sh->dev[sh->pd_idx].flags));\n\t\tBUG_ON(sh->qd_idx >= 0 &&\n\t\t       !test_bit(R5_UPTODATE, &sh->dev[sh->qd_idx].flags) &&\n\t\t       !test_bit(R5_Discard, &sh->dev[sh->qd_idx].flags));\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (test_bit(R5_LOCKED, &dev->flags) &&\n\t\t\t\t(i == sh->pd_idx || i == sh->qd_idx ||\n\t\t\t\t dev->written || test_bit(R5_InJournal,\n\t\t\t\t\t\t\t  &dev->flags))) {\n\t\t\t\tpr_debug(\"Writing block %d\\n\", i);\n\t\t\t\tset_bit(R5_Wantwrite, &dev->flags);\n\t\t\t\tif (prexor)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (s.failed > 1)\n\t\t\t\t\tcontinue;\n\t\t\t\tif (!test_bit(R5_Insync, &dev->flags) ||\n\t\t\t\t    ((i == sh->pd_idx || i == sh->qd_idx)  &&\n\t\t\t\t     s.failed == 0))\n\t\t\t\t\tset_bit(STRIPE_INSYNC, &sh->state);\n\t\t\t}\n\t\t}\n\t\tif (test_and_clear_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\t\ts.dec_preread_active = 1;\n\t}\n\n\t \n\tpdev = &sh->dev[sh->pd_idx];\n\ts.p_failed = (s.failed >= 1 && s.failed_num[0] == sh->pd_idx)\n\t\t|| (s.failed >= 2 && s.failed_num[1] == sh->pd_idx);\n\tqdev = &sh->dev[sh->qd_idx];\n\ts.q_failed = (s.failed >= 1 && s.failed_num[0] == sh->qd_idx)\n\t\t|| (s.failed >= 2 && s.failed_num[1] == sh->qd_idx)\n\t\t|| conf->level < 6;\n\n\tif (s.written &&\n\t    (s.p_failed || ((test_bit(R5_Insync, &pdev->flags)\n\t\t\t     && !test_bit(R5_LOCKED, &pdev->flags)\n\t\t\t     && (test_bit(R5_UPTODATE, &pdev->flags) ||\n\t\t\t\t test_bit(R5_Discard, &pdev->flags))))) &&\n\t    (s.q_failed || ((test_bit(R5_Insync, &qdev->flags)\n\t\t\t     && !test_bit(R5_LOCKED, &qdev->flags)\n\t\t\t     && (test_bit(R5_UPTODATE, &qdev->flags) ||\n\t\t\t\t test_bit(R5_Discard, &qdev->flags))))))\n\t\thandle_stripe_clean_event(conf, sh, disks);\n\n\tif (s.just_cached)\n\t\tr5c_handle_cached_data_endio(conf, sh, disks);\n\tlog_stripe_write_finished(sh);\n\n\t \n\tif (s.to_read || s.non_overwrite\n\t    || (s.to_write && s.failed)\n\t    || (s.syncing && (s.uptodate + s.compute < disks))\n\t    || s.replacing\n\t    || s.expanding)\n\t\thandle_stripe_fill(sh, &s, disks);\n\n\t \n\tr5c_finish_stripe_write_out(conf, sh, &s);\n\n\t \n\n\tif (!sh->reconstruct_state && !sh->check_state && !sh->log_io) {\n\t\tif (!r5c_is_writeback(conf->log)) {\n\t\t\tif (s.to_write)\n\t\t\t\thandle_stripe_dirtying(conf, sh, &s, disks);\n\t\t} else {  \n\t\t\tint ret = 0;\n\n\t\t\t \n\t\t\tif (s.to_write)\n\t\t\t\tret = r5c_try_caching_write(conf, sh, &s,\n\t\t\t\t\t\t\t    disks);\n\t\t\t \n\t\t\tif (ret == -EAGAIN ||\n\t\t\t     \n\t\t\t    (!test_bit(STRIPE_R5C_CACHING, &sh->state) &&\n\t\t\t     s.injournal > 0)) {\n\t\t\t\tret = handle_stripe_dirtying(conf, sh, &s,\n\t\t\t\t\t\t\t     disks);\n\t\t\t\tif (ret == -EAGAIN)\n\t\t\t\t\tgoto finish;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (sh->check_state ||\n\t    (s.syncing && s.locked == 0 &&\n\t     !test_bit(STRIPE_COMPUTE_RUN, &sh->state) &&\n\t     !test_bit(STRIPE_INSYNC, &sh->state))) {\n\t\tif (conf->level == 6)\n\t\t\thandle_parity_checks6(conf, sh, &s, disks);\n\t\telse\n\t\t\thandle_parity_checks5(conf, sh, &s, disks);\n\t}\n\n\tif ((s.replacing || s.syncing) && s.locked == 0\n\t    && !test_bit(STRIPE_COMPUTE_RUN, &sh->state)\n\t    && !test_bit(STRIPE_REPLACED, &sh->state)) {\n\t\t \n\t\tfor (i = 0; i < conf->raid_disks; i++)\n\t\t\tif (test_bit(R5_NeedReplace, &sh->dev[i].flags)) {\n\t\t\t\tWARN_ON(!test_bit(R5_UPTODATE, &sh->dev[i].flags));\n\t\t\t\tset_bit(R5_WantReplace, &sh->dev[i].flags);\n\t\t\t\tset_bit(R5_LOCKED, &sh->dev[i].flags);\n\t\t\t\ts.locked++;\n\t\t\t}\n\t\tif (s.replacing)\n\t\t\tset_bit(STRIPE_INSYNC, &sh->state);\n\t\tset_bit(STRIPE_REPLACED, &sh->state);\n\t}\n\tif ((s.syncing || s.replacing) && s.locked == 0 &&\n\t    !test_bit(STRIPE_COMPUTE_RUN, &sh->state) &&\n\t    test_bit(STRIPE_INSYNC, &sh->state)) {\n\t\tmd_done_sync(conf->mddev, RAID5_STRIPE_SECTORS(conf), 1);\n\t\tclear_bit(STRIPE_SYNCING, &sh->state);\n\t\tif (test_and_clear_bit(R5_Overlap, &sh->dev[sh->pd_idx].flags))\n\t\t\twake_up(&conf->wait_for_overlap);\n\t}\n\n\t \n\tif (s.failed <= conf->max_degraded && !conf->mddev->ro)\n\t\tfor (i = 0; i < s.failed; i++) {\n\t\t\tstruct r5dev *dev = &sh->dev[s.failed_num[i]];\n\t\t\tif (test_bit(R5_ReadError, &dev->flags)\n\t\t\t    && !test_bit(R5_LOCKED, &dev->flags)\n\t\t\t    && test_bit(R5_UPTODATE, &dev->flags)\n\t\t\t\t) {\n\t\t\t\tif (!test_bit(R5_ReWrite, &dev->flags)) {\n\t\t\t\t\tset_bit(R5_Wantwrite, &dev->flags);\n\t\t\t\t\tset_bit(R5_ReWrite, &dev->flags);\n\t\t\t\t} else\n\t\t\t\t\t \n\t\t\t\t\tset_bit(R5_Wantread, &dev->flags);\n\t\t\t\tset_bit(R5_LOCKED, &dev->flags);\n\t\t\t\ts.locked++;\n\t\t\t}\n\t\t}\n\n\t \n\tif (sh->reconstruct_state == reconstruct_state_result) {\n\t\tstruct stripe_head *sh_src\n\t\t\t= raid5_get_active_stripe(conf, NULL, sh->sector,\n\t\t\t\t\tR5_GAS_PREVIOUS | R5_GAS_NOBLOCK |\n\t\t\t\t\tR5_GAS_NOQUIESCE);\n\t\tif (sh_src && test_bit(STRIPE_EXPAND_SOURCE, &sh_src->state)) {\n\t\t\t \n\t\t\tset_bit(STRIPE_DELAYED, &sh->state);\n\t\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\t\tif (!test_and_set_bit(STRIPE_PREREAD_ACTIVE,\n\t\t\t\t\t      &sh_src->state))\n\t\t\t\tatomic_inc(&conf->preread_active_stripes);\n\t\t\traid5_release_stripe(sh_src);\n\t\t\tgoto finish;\n\t\t}\n\t\tif (sh_src)\n\t\t\traid5_release_stripe(sh_src);\n\n\t\tsh->reconstruct_state = reconstruct_state_idle;\n\t\tclear_bit(STRIPE_EXPANDING, &sh->state);\n\t\tfor (i = conf->raid_disks; i--; ) {\n\t\t\tset_bit(R5_Wantwrite, &sh->dev[i].flags);\n\t\t\tset_bit(R5_LOCKED, &sh->dev[i].flags);\n\t\t\ts.locked++;\n\t\t}\n\t}\n\n\tif (s.expanded && test_bit(STRIPE_EXPANDING, &sh->state) &&\n\t    !sh->reconstruct_state) {\n\t\t \n\t\tsh->disks = conf->raid_disks;\n\t\tstripe_set_idx(sh->sector, conf, 0, sh);\n\t\tschedule_reconstruction(sh, &s, 1, 1);\n\t} else if (s.expanded && !sh->reconstruct_state && s.locked == 0) {\n\t\tclear_bit(STRIPE_EXPAND_READY, &sh->state);\n\t\tatomic_dec(&conf->reshape_stripes);\n\t\twake_up(&conf->wait_for_overlap);\n\t\tmd_done_sync(conf->mddev, RAID5_STRIPE_SECTORS(conf), 1);\n\t}\n\n\tif (s.expanding && s.locked == 0 &&\n\t    !test_bit(STRIPE_COMPUTE_RUN, &sh->state))\n\t\thandle_stripe_expansion(conf, sh);\n\nfinish:\n\t \n\tif (unlikely(s.blocked_rdev)) {\n\t\tif (conf->mddev->external)\n\t\t\tmd_wait_for_blocked_rdev(s.blocked_rdev,\n\t\t\t\t\t\t conf->mddev);\n\t\telse\n\t\t\t \n\t\t\trdev_dec_pending(s.blocked_rdev,\n\t\t\t\t\t conf->mddev);\n\t}\n\n\tif (s.handle_bad_blocks)\n\t\tfor (i = disks; i--; ) {\n\t\t\tstruct md_rdev *rdev;\n\t\t\tstruct r5dev *dev = &sh->dev[i];\n\t\t\tif (test_and_clear_bit(R5_WriteError, &dev->flags)) {\n\t\t\t\t \n\t\t\t\trdev = rdev_pend_deref(conf->disks[i].rdev);\n\t\t\t\tif (!rdev_set_badblocks(rdev, sh->sector,\n\t\t\t\t\t\t\tRAID5_STRIPE_SECTORS(conf), 0))\n\t\t\t\t\tmd_error(conf->mddev, rdev);\n\t\t\t\trdev_dec_pending(rdev, conf->mddev);\n\t\t\t}\n\t\t\tif (test_and_clear_bit(R5_MadeGood, &dev->flags)) {\n\t\t\t\trdev = rdev_pend_deref(conf->disks[i].rdev);\n\t\t\t\trdev_clear_badblocks(rdev, sh->sector,\n\t\t\t\t\t\t     RAID5_STRIPE_SECTORS(conf), 0);\n\t\t\t\trdev_dec_pending(rdev, conf->mddev);\n\t\t\t}\n\t\t\tif (test_and_clear_bit(R5_MadeGoodRepl, &dev->flags)) {\n\t\t\t\trdev = rdev_pend_deref(conf->disks[i].replacement);\n\t\t\t\tif (!rdev)\n\t\t\t\t\t \n\t\t\t\t\trdev = rdev_pend_deref(conf->disks[i].rdev);\n\t\t\t\trdev_clear_badblocks(rdev, sh->sector,\n\t\t\t\t\t\t     RAID5_STRIPE_SECTORS(conf), 0);\n\t\t\t\trdev_dec_pending(rdev, conf->mddev);\n\t\t\t}\n\t\t}\n\n\tif (s.ops_request)\n\t\traid_run_ops(sh, s.ops_request);\n\n\tops_run_io(sh, &s);\n\n\tif (s.dec_preread_active) {\n\t\t \n\t\tatomic_dec(&conf->preread_active_stripes);\n\t\tif (atomic_read(&conf->preread_active_stripes) <\n\t\t    IO_THRESHOLD)\n\t\t\tmd_wakeup_thread(conf->mddev->thread);\n\t}\n\n\tclear_bit_unlock(STRIPE_ACTIVE, &sh->state);\n}\n\nstatic void raid5_activate_delayed(struct r5conf *conf)\n\t__must_hold(&conf->device_lock)\n{\n\tif (atomic_read(&conf->preread_active_stripes) < IO_THRESHOLD) {\n\t\twhile (!list_empty(&conf->delayed_list)) {\n\t\t\tstruct list_head *l = conf->delayed_list.next;\n\t\t\tstruct stripe_head *sh;\n\t\t\tsh = list_entry(l, struct stripe_head, lru);\n\t\t\tlist_del_init(l);\n\t\t\tclear_bit(STRIPE_DELAYED, &sh->state);\n\t\t\tif (!test_and_set_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\t\t\tatomic_inc(&conf->preread_active_stripes);\n\t\t\tlist_add_tail(&sh->lru, &conf->hold_list);\n\t\t\traid5_wakeup_stripe_thread(sh);\n\t\t}\n\t}\n}\n\nstatic void activate_bit_delay(struct r5conf *conf,\n\t\tstruct list_head *temp_inactive_list)\n\t__must_hold(&conf->device_lock)\n{\n\tstruct list_head head;\n\tlist_add(&head, &conf->bitmap_list);\n\tlist_del_init(&conf->bitmap_list);\n\twhile (!list_empty(&head)) {\n\t\tstruct stripe_head *sh = list_entry(head.next, struct stripe_head, lru);\n\t\tint hash;\n\t\tlist_del_init(&sh->lru);\n\t\tatomic_inc(&sh->count);\n\t\thash = sh->hash_lock_index;\n\t\t__release_stripe(conf, sh, &temp_inactive_list[hash]);\n\t}\n}\n\nstatic int in_chunk_boundary(struct mddev *mddev, struct bio *bio)\n{\n\tstruct r5conf *conf = mddev->private;\n\tsector_t sector = bio->bi_iter.bi_sector;\n\tunsigned int chunk_sectors;\n\tunsigned int bio_sectors = bio_sectors(bio);\n\n\tchunk_sectors = min(conf->chunk_sectors, conf->prev_chunk_sectors);\n\treturn  chunk_sectors >=\n\t\t((sector & (chunk_sectors - 1)) + bio_sectors);\n}\n\n \nstatic void add_bio_to_retry(struct bio *bi,struct r5conf *conf)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&conf->device_lock, flags);\n\n\tbi->bi_next = conf->retry_read_aligned_list;\n\tconf->retry_read_aligned_list = bi;\n\n\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\tmd_wakeup_thread(conf->mddev->thread);\n}\n\nstatic struct bio *remove_bio_from_retry(struct r5conf *conf,\n\t\t\t\t\t unsigned int *offset)\n{\n\tstruct bio *bi;\n\n\tbi = conf->retry_read_aligned;\n\tif (bi) {\n\t\t*offset = conf->retry_read_offset;\n\t\tconf->retry_read_aligned = NULL;\n\t\treturn bi;\n\t}\n\tbi = conf->retry_read_aligned_list;\n\tif(bi) {\n\t\tconf->retry_read_aligned_list = bi->bi_next;\n\t\tbi->bi_next = NULL;\n\t\t*offset = 0;\n\t}\n\n\treturn bi;\n}\n\n \nstatic void raid5_align_endio(struct bio *bi)\n{\n\tstruct bio *raid_bi = bi->bi_private;\n\tstruct md_rdev *rdev = (void *)raid_bi->bi_next;\n\tstruct mddev *mddev = rdev->mddev;\n\tstruct r5conf *conf = mddev->private;\n\tblk_status_t error = bi->bi_status;\n\n\tbio_put(bi);\n\traid_bi->bi_next = NULL;\n\trdev_dec_pending(rdev, conf->mddev);\n\n\tif (!error) {\n\t\tbio_endio(raid_bi);\n\t\tif (atomic_dec_and_test(&conf->active_aligned_reads))\n\t\t\twake_up(&conf->wait_for_quiescent);\n\t\treturn;\n\t}\n\n\tpr_debug(\"raid5_align_endio : io error...handing IO for a retry\\n\");\n\n\tadd_bio_to_retry(raid_bi, conf);\n}\n\nstatic int raid5_read_one_chunk(struct mddev *mddev, struct bio *raid_bio)\n{\n\tstruct r5conf *conf = mddev->private;\n\tstruct bio *align_bio;\n\tstruct md_rdev *rdev;\n\tsector_t sector, end_sector, first_bad;\n\tint bad_sectors, dd_idx;\n\tbool did_inc;\n\n\tif (!in_chunk_boundary(mddev, raid_bio)) {\n\t\tpr_debug(\"%s: non aligned\\n\", __func__);\n\t\treturn 0;\n\t}\n\n\tsector = raid5_compute_sector(conf, raid_bio->bi_iter.bi_sector, 0,\n\t\t\t\t      &dd_idx, NULL);\n\tend_sector = sector + bio_sectors(raid_bio);\n\n\trcu_read_lock();\n\tif (r5c_big_stripe_cached(conf, sector))\n\t\tgoto out_rcu_unlock;\n\n\trdev = rcu_dereference(conf->disks[dd_idx].replacement);\n\tif (!rdev || test_bit(Faulty, &rdev->flags) ||\n\t    rdev->recovery_offset < end_sector) {\n\t\trdev = rcu_dereference(conf->disks[dd_idx].rdev);\n\t\tif (!rdev)\n\t\t\tgoto out_rcu_unlock;\n\t\tif (test_bit(Faulty, &rdev->flags) ||\n\t\t    !(test_bit(In_sync, &rdev->flags) ||\n\t\t      rdev->recovery_offset >= end_sector))\n\t\t\tgoto out_rcu_unlock;\n\t}\n\n\tatomic_inc(&rdev->nr_pending);\n\trcu_read_unlock();\n\n\tif (is_badblock(rdev, sector, bio_sectors(raid_bio), &first_bad,\n\t\t\t&bad_sectors)) {\n\t\trdev_dec_pending(rdev, mddev);\n\t\treturn 0;\n\t}\n\n\tmd_account_bio(mddev, &raid_bio);\n\traid_bio->bi_next = (void *)rdev;\n\n\talign_bio = bio_alloc_clone(rdev->bdev, raid_bio, GFP_NOIO,\n\t\t\t\t    &mddev->bio_set);\n\talign_bio->bi_end_io = raid5_align_endio;\n\talign_bio->bi_private = raid_bio;\n\talign_bio->bi_iter.bi_sector = sector;\n\n\t \n\talign_bio->bi_iter.bi_sector += rdev->data_offset;\n\n\tdid_inc = false;\n\tif (conf->quiesce == 0) {\n\t\tatomic_inc(&conf->active_aligned_reads);\n\t\tdid_inc = true;\n\t}\n\t \n\tif (!did_inc || smp_load_acquire(&conf->quiesce) != 0) {\n\t\t \n\t\tif (did_inc && atomic_dec_and_test(&conf->active_aligned_reads))\n\t\t\twake_up(&conf->wait_for_quiescent);\n\t\tspin_lock_irq(&conf->device_lock);\n\t\twait_event_lock_irq(conf->wait_for_quiescent, conf->quiesce == 0,\n\t\t\t\t    conf->device_lock);\n\t\tatomic_inc(&conf->active_aligned_reads);\n\t\tspin_unlock_irq(&conf->device_lock);\n\t}\n\n\tif (mddev->gendisk)\n\t\ttrace_block_bio_remap(align_bio, disk_devt(mddev->gendisk),\n\t\t\t\t      raid_bio->bi_iter.bi_sector);\n\tsubmit_bio_noacct(align_bio);\n\treturn 1;\n\nout_rcu_unlock:\n\trcu_read_unlock();\n\treturn 0;\n}\n\nstatic struct bio *chunk_aligned_read(struct mddev *mddev, struct bio *raid_bio)\n{\n\tstruct bio *split;\n\tsector_t sector = raid_bio->bi_iter.bi_sector;\n\tunsigned chunk_sects = mddev->chunk_sectors;\n\tunsigned sectors = chunk_sects - (sector & (chunk_sects-1));\n\n\tif (sectors < bio_sectors(raid_bio)) {\n\t\tstruct r5conf *conf = mddev->private;\n\t\tsplit = bio_split(raid_bio, sectors, GFP_NOIO, &conf->bio_split);\n\t\tbio_chain(split, raid_bio);\n\t\tsubmit_bio_noacct(raid_bio);\n\t\traid_bio = split;\n\t}\n\n\tif (!raid5_read_one_chunk(mddev, raid_bio))\n\t\treturn raid_bio;\n\n\treturn NULL;\n}\n\n \nstatic struct stripe_head *__get_priority_stripe(struct r5conf *conf, int group)\n\t__must_hold(&conf->device_lock)\n{\n\tstruct stripe_head *sh, *tmp;\n\tstruct list_head *handle_list = NULL;\n\tstruct r5worker_group *wg;\n\tbool second_try = !r5c_is_writeback(conf->log) &&\n\t\t!r5l_log_disk_error(conf);\n\tbool try_loprio = test_bit(R5C_LOG_TIGHT, &conf->cache_state) ||\n\t\tr5l_log_disk_error(conf);\n\nagain:\n\twg = NULL;\n\tsh = NULL;\n\tif (conf->worker_cnt_per_group == 0) {\n\t\thandle_list = try_loprio ? &conf->loprio_list :\n\t\t\t\t\t&conf->handle_list;\n\t} else if (group != ANY_GROUP) {\n\t\thandle_list = try_loprio ? &conf->worker_groups[group].loprio_list :\n\t\t\t\t&conf->worker_groups[group].handle_list;\n\t\twg = &conf->worker_groups[group];\n\t} else {\n\t\tint i;\n\t\tfor (i = 0; i < conf->group_cnt; i++) {\n\t\t\thandle_list = try_loprio ? &conf->worker_groups[i].loprio_list :\n\t\t\t\t&conf->worker_groups[i].handle_list;\n\t\t\twg = &conf->worker_groups[i];\n\t\t\tif (!list_empty(handle_list))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tpr_debug(\"%s: handle: %s hold: %s full_writes: %d bypass_count: %d\\n\",\n\t\t  __func__,\n\t\t  list_empty(handle_list) ? \"empty\" : \"busy\",\n\t\t  list_empty(&conf->hold_list) ? \"empty\" : \"busy\",\n\t\t  atomic_read(&conf->pending_full_writes), conf->bypass_count);\n\n\tif (!list_empty(handle_list)) {\n\t\tsh = list_entry(handle_list->next, typeof(*sh), lru);\n\n\t\tif (list_empty(&conf->hold_list))\n\t\t\tconf->bypass_count = 0;\n\t\telse if (!test_bit(STRIPE_IO_STARTED, &sh->state)) {\n\t\t\tif (conf->hold_list.next == conf->last_hold)\n\t\t\t\tconf->bypass_count++;\n\t\t\telse {\n\t\t\t\tconf->last_hold = conf->hold_list.next;\n\t\t\t\tconf->bypass_count -= conf->bypass_threshold;\n\t\t\t\tif (conf->bypass_count < 0)\n\t\t\t\t\tconf->bypass_count = 0;\n\t\t\t}\n\t\t}\n\t} else if (!list_empty(&conf->hold_list) &&\n\t\t   ((conf->bypass_threshold &&\n\t\t     conf->bypass_count > conf->bypass_threshold) ||\n\t\t    atomic_read(&conf->pending_full_writes) == 0)) {\n\n\t\tlist_for_each_entry(tmp, &conf->hold_list,  lru) {\n\t\t\tif (conf->worker_cnt_per_group == 0 ||\n\t\t\t    group == ANY_GROUP ||\n\t\t\t    !cpu_online(tmp->cpu) ||\n\t\t\t    cpu_to_group(tmp->cpu) == group) {\n\t\t\t\tsh = tmp;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (sh) {\n\t\t\tconf->bypass_count -= conf->bypass_threshold;\n\t\t\tif (conf->bypass_count < 0)\n\t\t\t\tconf->bypass_count = 0;\n\t\t}\n\t\twg = NULL;\n\t}\n\n\tif (!sh) {\n\t\tif (second_try)\n\t\t\treturn NULL;\n\t\tsecond_try = true;\n\t\ttry_loprio = !try_loprio;\n\t\tgoto again;\n\t}\n\n\tif (wg) {\n\t\twg->stripes_cnt--;\n\t\tsh->group = NULL;\n\t}\n\tlist_del_init(&sh->lru);\n\tBUG_ON(atomic_inc_return(&sh->count) != 1);\n\treturn sh;\n}\n\nstruct raid5_plug_cb {\n\tstruct blk_plug_cb\tcb;\n\tstruct list_head\tlist;\n\tstruct list_head\ttemp_inactive_list[NR_STRIPE_HASH_LOCKS];\n};\n\nstatic void raid5_unplug(struct blk_plug_cb *blk_cb, bool from_schedule)\n{\n\tstruct raid5_plug_cb *cb = container_of(\n\t\tblk_cb, struct raid5_plug_cb, cb);\n\tstruct stripe_head *sh;\n\tstruct mddev *mddev = cb->cb.data;\n\tstruct r5conf *conf = mddev->private;\n\tint cnt = 0;\n\tint hash;\n\n\tif (cb->list.next && !list_empty(&cb->list)) {\n\t\tspin_lock_irq(&conf->device_lock);\n\t\twhile (!list_empty(&cb->list)) {\n\t\t\tsh = list_first_entry(&cb->list, struct stripe_head, lru);\n\t\t\tlist_del_init(&sh->lru);\n\t\t\t \n\t\t\tsmp_mb__before_atomic();\n\t\t\tclear_bit(STRIPE_ON_UNPLUG_LIST, &sh->state);\n\t\t\t \n\t\t\thash = sh->hash_lock_index;\n\t\t\t__release_stripe(conf, sh, &cb->temp_inactive_list[hash]);\n\t\t\tcnt++;\n\t\t}\n\t\tspin_unlock_irq(&conf->device_lock);\n\t}\n\trelease_inactive_stripe_list(conf, cb->temp_inactive_list,\n\t\t\t\t     NR_STRIPE_HASH_LOCKS);\n\tif (mddev->queue)\n\t\ttrace_block_unplug(mddev->queue, cnt, !from_schedule);\n\tkfree(cb);\n}\n\nstatic void release_stripe_plug(struct mddev *mddev,\n\t\t\t\tstruct stripe_head *sh)\n{\n\tstruct blk_plug_cb *blk_cb = blk_check_plugged(\n\t\traid5_unplug, mddev,\n\t\tsizeof(struct raid5_plug_cb));\n\tstruct raid5_plug_cb *cb;\n\n\tif (!blk_cb) {\n\t\traid5_release_stripe(sh);\n\t\treturn;\n\t}\n\n\tcb = container_of(blk_cb, struct raid5_plug_cb, cb);\n\n\tif (cb->list.next == NULL) {\n\t\tint i;\n\t\tINIT_LIST_HEAD(&cb->list);\n\t\tfor (i = 0; i < NR_STRIPE_HASH_LOCKS; i++)\n\t\t\tINIT_LIST_HEAD(cb->temp_inactive_list + i);\n\t}\n\n\tif (!test_and_set_bit(STRIPE_ON_UNPLUG_LIST, &sh->state))\n\t\tlist_add_tail(&sh->lru, &cb->list);\n\telse\n\t\traid5_release_stripe(sh);\n}\n\nstatic void make_discard_request(struct mddev *mddev, struct bio *bi)\n{\n\tstruct r5conf *conf = mddev->private;\n\tsector_t logical_sector, last_sector;\n\tstruct stripe_head *sh;\n\tint stripe_sectors;\n\n\t \n\tif (WARN_ON_ONCE(bi->bi_opf & REQ_NOWAIT))\n\t\treturn;\n\n\tif (mddev->reshape_position != MaxSector)\n\t\t \n\t\treturn;\n\n\tlogical_sector = bi->bi_iter.bi_sector & ~((sector_t)RAID5_STRIPE_SECTORS(conf)-1);\n\tlast_sector = bio_end_sector(bi);\n\n\tbi->bi_next = NULL;\n\n\tstripe_sectors = conf->chunk_sectors *\n\t\t(conf->raid_disks - conf->max_degraded);\n\tlogical_sector = DIV_ROUND_UP_SECTOR_T(logical_sector,\n\t\t\t\t\t       stripe_sectors);\n\tsector_div(last_sector, stripe_sectors);\n\n\tlogical_sector *= conf->chunk_sectors;\n\tlast_sector *= conf->chunk_sectors;\n\n\tfor (; logical_sector < last_sector;\n\t     logical_sector += RAID5_STRIPE_SECTORS(conf)) {\n\t\tDEFINE_WAIT(w);\n\t\tint d;\n\tagain:\n\t\tsh = raid5_get_active_stripe(conf, NULL, logical_sector, 0);\n\t\tprepare_to_wait(&conf->wait_for_overlap, &w,\n\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\tset_bit(R5_Overlap, &sh->dev[sh->pd_idx].flags);\n\t\tif (test_bit(STRIPE_SYNCING, &sh->state)) {\n\t\t\traid5_release_stripe(sh);\n\t\t\tschedule();\n\t\t\tgoto again;\n\t\t}\n\t\tclear_bit(R5_Overlap, &sh->dev[sh->pd_idx].flags);\n\t\tspin_lock_irq(&sh->stripe_lock);\n\t\tfor (d = 0; d < conf->raid_disks; d++) {\n\t\t\tif (d == sh->pd_idx || d == sh->qd_idx)\n\t\t\t\tcontinue;\n\t\t\tif (sh->dev[d].towrite || sh->dev[d].toread) {\n\t\t\t\tset_bit(R5_Overlap, &sh->dev[d].flags);\n\t\t\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\t\t\traid5_release_stripe(sh);\n\t\t\t\tschedule();\n\t\t\t\tgoto again;\n\t\t\t}\n\t\t}\n\t\tset_bit(STRIPE_DISCARD, &sh->state);\n\t\tfinish_wait(&conf->wait_for_overlap, &w);\n\t\tsh->overwrite_disks = 0;\n\t\tfor (d = 0; d < conf->raid_disks; d++) {\n\t\t\tif (d == sh->pd_idx || d == sh->qd_idx)\n\t\t\t\tcontinue;\n\t\t\tsh->dev[d].towrite = bi;\n\t\t\tset_bit(R5_OVERWRITE, &sh->dev[d].flags);\n\t\t\tbio_inc_remaining(bi);\n\t\t\tmd_write_inc(mddev, bi);\n\t\t\tsh->overwrite_disks++;\n\t\t}\n\t\tspin_unlock_irq(&sh->stripe_lock);\n\t\tif (conf->mddev->bitmap) {\n\t\t\tfor (d = 0;\n\t\t\t     d < conf->raid_disks - conf->max_degraded;\n\t\t\t     d++)\n\t\t\t\tmd_bitmap_startwrite(mddev->bitmap,\n\t\t\t\t\t\t     sh->sector,\n\t\t\t\t\t\t     RAID5_STRIPE_SECTORS(conf),\n\t\t\t\t\t\t     0);\n\t\t\tsh->bm_seq = conf->seq_flush + 1;\n\t\t\tset_bit(STRIPE_BIT_DELAY, &sh->state);\n\t\t}\n\n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\tclear_bit(STRIPE_DELAYED, &sh->state);\n\t\tif (!test_and_set_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\t\tatomic_inc(&conf->preread_active_stripes);\n\t\trelease_stripe_plug(mddev, sh);\n\t}\n\n\tbio_endio(bi);\n}\n\nstatic bool ahead_of_reshape(struct mddev *mddev, sector_t sector,\n\t\t\t     sector_t reshape_sector)\n{\n\treturn mddev->reshape_backwards ? sector < reshape_sector :\n\t\t\t\t\t  sector >= reshape_sector;\n}\n\nstatic bool range_ahead_of_reshape(struct mddev *mddev, sector_t min,\n\t\t\t\t   sector_t max, sector_t reshape_sector)\n{\n\treturn mddev->reshape_backwards ? max < reshape_sector :\n\t\t\t\t\t  min >= reshape_sector;\n}\n\nstatic bool stripe_ahead_of_reshape(struct mddev *mddev, struct r5conf *conf,\n\t\t\t\t    struct stripe_head *sh)\n{\n\tsector_t max_sector = 0, min_sector = MaxSector;\n\tbool ret = false;\n\tint dd_idx;\n\n\tfor (dd_idx = 0; dd_idx < sh->disks; dd_idx++) {\n\t\tif (dd_idx == sh->pd_idx || dd_idx == sh->qd_idx)\n\t\t\tcontinue;\n\n\t\tmin_sector = min(min_sector, sh->dev[dd_idx].sector);\n\t\tmax_sector = max(max_sector, sh->dev[dd_idx].sector);\n\t}\n\n\tspin_lock_irq(&conf->device_lock);\n\n\tif (!range_ahead_of_reshape(mddev, min_sector, max_sector,\n\t\t\t\t     conf->reshape_progress))\n\t\t \n\t\tret = true;\n\n\tspin_unlock_irq(&conf->device_lock);\n\n\treturn ret;\n}\n\nstatic int add_all_stripe_bios(struct r5conf *conf,\n\t\tstruct stripe_request_ctx *ctx, struct stripe_head *sh,\n\t\tstruct bio *bi, int forwrite, int previous)\n{\n\tint dd_idx;\n\tint ret = 1;\n\n\tspin_lock_irq(&sh->stripe_lock);\n\n\tfor (dd_idx = 0; dd_idx < sh->disks; dd_idx++) {\n\t\tstruct r5dev *dev = &sh->dev[dd_idx];\n\n\t\tif (dd_idx == sh->pd_idx || dd_idx == sh->qd_idx)\n\t\t\tcontinue;\n\n\t\tif (dev->sector < ctx->first_sector ||\n\t\t    dev->sector >= ctx->last_sector)\n\t\t\tcontinue;\n\n\t\tif (stripe_bio_overlaps(sh, bi, dd_idx, forwrite)) {\n\t\t\tset_bit(R5_Overlap, &dev->flags);\n\t\t\tret = 0;\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\tif (!ret)\n\t\tgoto out;\n\n\tfor (dd_idx = 0; dd_idx < sh->disks; dd_idx++) {\n\t\tstruct r5dev *dev = &sh->dev[dd_idx];\n\n\t\tif (dd_idx == sh->pd_idx || dd_idx == sh->qd_idx)\n\t\t\tcontinue;\n\n\t\tif (dev->sector < ctx->first_sector ||\n\t\t    dev->sector >= ctx->last_sector)\n\t\t\tcontinue;\n\n\t\t__add_stripe_bio(sh, bi, dd_idx, forwrite, previous);\n\t\tclear_bit((dev->sector - ctx->first_sector) >>\n\t\t\t  RAID5_STRIPE_SHIFT(conf), ctx->sectors_to_do);\n\t}\n\nout:\n\tspin_unlock_irq(&sh->stripe_lock);\n\treturn ret;\n}\n\nstatic bool reshape_inprogress(struct mddev *mddev)\n{\n\treturn test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery) &&\n\t       test_bit(MD_RECOVERY_RUNNING, &mddev->recovery) &&\n\t       !test_bit(MD_RECOVERY_DONE, &mddev->recovery) &&\n\t       !test_bit(MD_RECOVERY_INTR, &mddev->recovery);\n}\n\nstatic bool reshape_disabled(struct mddev *mddev)\n{\n\treturn is_md_suspended(mddev) || !md_is_rdwr(mddev);\n}\n\nstatic enum stripe_result make_stripe_request(struct mddev *mddev,\n\t\tstruct r5conf *conf, struct stripe_request_ctx *ctx,\n\t\tsector_t logical_sector, struct bio *bi)\n{\n\tconst int rw = bio_data_dir(bi);\n\tenum stripe_result ret;\n\tstruct stripe_head *sh;\n\tsector_t new_sector;\n\tint previous = 0, flags = 0;\n\tint seq, dd_idx;\n\n\tseq = read_seqcount_begin(&conf->gen_lock);\n\n\tif (unlikely(conf->reshape_progress != MaxSector)) {\n\t\t \n\t\tspin_lock_irq(&conf->device_lock);\n\t\tif (ahead_of_reshape(mddev, logical_sector,\n\t\t\t\t     conf->reshape_progress)) {\n\t\t\tprevious = 1;\n\t\t} else {\n\t\t\tif (ahead_of_reshape(mddev, logical_sector,\n\t\t\t\t\t     conf->reshape_safe)) {\n\t\t\t\tspin_unlock_irq(&conf->device_lock);\n\t\t\t\tret = STRIPE_SCHEDULE_AND_RETRY;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tspin_unlock_irq(&conf->device_lock);\n\t}\n\n\tnew_sector = raid5_compute_sector(conf, logical_sector, previous,\n\t\t\t\t\t  &dd_idx, NULL);\n\tpr_debug(\"raid456: %s, sector %llu logical %llu\\n\", __func__,\n\t\t new_sector, logical_sector);\n\n\tif (previous)\n\t\tflags |= R5_GAS_PREVIOUS;\n\tif (bi->bi_opf & REQ_RAHEAD)\n\t\tflags |= R5_GAS_NOBLOCK;\n\tsh = raid5_get_active_stripe(conf, ctx, new_sector, flags);\n\tif (unlikely(!sh)) {\n\t\t \n\t\tbi->bi_status = BLK_STS_IOERR;\n\t\treturn STRIPE_FAIL;\n\t}\n\n\tif (unlikely(previous) &&\n\t    stripe_ahead_of_reshape(mddev, conf, sh)) {\n\t\t \n\t\tret = STRIPE_SCHEDULE_AND_RETRY;\n\t\tgoto out_release;\n\t}\n\n\tif (read_seqcount_retry(&conf->gen_lock, seq)) {\n\t\t \n\t\tret = STRIPE_RETRY;\n\t\tgoto out_release;\n\t}\n\n\tif (test_bit(STRIPE_EXPANDING, &sh->state) ||\n\t    !add_all_stripe_bios(conf, ctx, sh, bi, rw, previous)) {\n\t\t \n\t\tmd_wakeup_thread(mddev->thread);\n\t\tret = STRIPE_SCHEDULE_AND_RETRY;\n\t\tgoto out_release;\n\t}\n\n\tif (stripe_can_batch(sh)) {\n\t\tstripe_add_to_batch_list(conf, sh, ctx->batch_last);\n\t\tif (ctx->batch_last)\n\t\t\traid5_release_stripe(ctx->batch_last);\n\t\tatomic_inc(&sh->count);\n\t\tctx->batch_last = sh;\n\t}\n\n\tif (ctx->do_flush) {\n\t\tset_bit(STRIPE_R5C_PREFLUSH, &sh->state);\n\t\t \n\t\tctx->do_flush = false;\n\t}\n\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\tclear_bit(STRIPE_DELAYED, &sh->state);\n\tif ((!sh->batch_head || sh == sh->batch_head) &&\n\t    (bi->bi_opf & REQ_SYNC) &&\n\t    !test_and_set_bit(STRIPE_PREREAD_ACTIVE, &sh->state))\n\t\tatomic_inc(&conf->preread_active_stripes);\n\n\trelease_stripe_plug(mddev, sh);\n\treturn STRIPE_SUCCESS;\n\nout_release:\n\traid5_release_stripe(sh);\nout:\n\tif (ret == STRIPE_SCHEDULE_AND_RETRY && !reshape_inprogress(mddev) &&\n\t    reshape_disabled(mddev)) {\n\t\tbi->bi_status = BLK_STS_IOERR;\n\t\tret = STRIPE_FAIL;\n\t\tpr_err(\"md/raid456:%s: io failed across reshape position while reshape can't make progress.\\n\",\n\t\t       mdname(mddev));\n\t}\n\n\treturn ret;\n}\n\n \nstatic sector_t raid5_bio_lowest_chunk_sector(struct r5conf *conf,\n\t\t\t\t\t      struct bio *bi)\n{\n\tint sectors_per_chunk = conf->chunk_sectors;\n\tint raid_disks = conf->raid_disks;\n\tint dd_idx;\n\tstruct stripe_head sh;\n\tunsigned int chunk_offset;\n\tsector_t r_sector = bi->bi_iter.bi_sector & ~((sector_t)RAID5_STRIPE_SECTORS(conf)-1);\n\tsector_t sector;\n\n\t \n\tsector = raid5_compute_sector(conf, r_sector, 0, &dd_idx, &sh);\n\tchunk_offset = sector_div(sector, sectors_per_chunk);\n\tif (sectors_per_chunk - chunk_offset >= bio_sectors(bi))\n\t\treturn r_sector;\n\t \n\tdd_idx++;\n\twhile (dd_idx == sh.pd_idx || dd_idx == sh.qd_idx)\n\t\tdd_idx++;\n\tif (dd_idx >= raid_disks)\n\t\treturn r_sector;\n\treturn r_sector + sectors_per_chunk - chunk_offset;\n}\n\nstatic bool raid5_make_request(struct mddev *mddev, struct bio * bi)\n{\n\tDEFINE_WAIT_FUNC(wait, woken_wake_function);\n\tstruct r5conf *conf = mddev->private;\n\tsector_t logical_sector;\n\tstruct stripe_request_ctx ctx = {};\n\tconst int rw = bio_data_dir(bi);\n\tenum stripe_result res;\n\tint s, stripe_cnt;\n\n\tif (unlikely(bi->bi_opf & REQ_PREFLUSH)) {\n\t\tint ret = log_handle_flush_request(conf, bi);\n\n\t\tif (ret == 0)\n\t\t\treturn true;\n\t\tif (ret == -ENODEV) {\n\t\t\tif (md_flush_request(mddev, bi))\n\t\t\t\treturn true;\n\t\t}\n\t\t \n\t\t \n\t\tctx.do_flush = bi->bi_opf & REQ_PREFLUSH;\n\t}\n\n\tif (!md_write_start(mddev, bi))\n\t\treturn false;\n\t \n\tif (rw == READ && mddev->degraded == 0 &&\n\t    mddev->reshape_position == MaxSector) {\n\t\tbi = chunk_aligned_read(mddev, bi);\n\t\tif (!bi)\n\t\t\treturn true;\n\t}\n\n\tif (unlikely(bio_op(bi) == REQ_OP_DISCARD)) {\n\t\tmake_discard_request(mddev, bi);\n\t\tmd_write_end(mddev);\n\t\treturn true;\n\t}\n\n\tlogical_sector = bi->bi_iter.bi_sector & ~((sector_t)RAID5_STRIPE_SECTORS(conf)-1);\n\tctx.first_sector = logical_sector;\n\tctx.last_sector = bio_end_sector(bi);\n\tbi->bi_next = NULL;\n\n\tstripe_cnt = DIV_ROUND_UP_SECTOR_T(ctx.last_sector - logical_sector,\n\t\t\t\t\t   RAID5_STRIPE_SECTORS(conf));\n\tbitmap_set(ctx.sectors_to_do, 0, stripe_cnt);\n\n\tpr_debug(\"raid456: %s, logical %llu to %llu\\n\", __func__,\n\t\t bi->bi_iter.bi_sector, ctx.last_sector);\n\n\t \n\tif ((bi->bi_opf & REQ_NOWAIT) &&\n\t    (conf->reshape_progress != MaxSector) &&\n\t    !ahead_of_reshape(mddev, logical_sector, conf->reshape_progress) &&\n\t    ahead_of_reshape(mddev, logical_sector, conf->reshape_safe)) {\n\t\tbio_wouldblock_error(bi);\n\t\tif (rw == WRITE)\n\t\t\tmd_write_end(mddev);\n\t\treturn true;\n\t}\n\tmd_account_bio(mddev, &bi);\n\n\t \n\tif (likely(conf->reshape_progress == MaxSector))\n\t\tlogical_sector = raid5_bio_lowest_chunk_sector(conf, bi);\n\ts = (logical_sector - ctx.first_sector) >> RAID5_STRIPE_SHIFT(conf);\n\n\tadd_wait_queue(&conf->wait_for_overlap, &wait);\n\twhile (1) {\n\t\tres = make_stripe_request(mddev, conf, &ctx, logical_sector,\n\t\t\t\t\t  bi);\n\t\tif (res == STRIPE_FAIL)\n\t\t\tbreak;\n\n\t\tif (res == STRIPE_RETRY)\n\t\t\tcontinue;\n\n\t\tif (res == STRIPE_SCHEDULE_AND_RETRY) {\n\t\t\t \n\t\t\tif (ctx.batch_last) {\n\t\t\t\traid5_release_stripe(ctx.batch_last);\n\t\t\t\tctx.batch_last = NULL;\n\t\t\t}\n\n\t\t\twait_woken(&wait, TASK_UNINTERRUPTIBLE,\n\t\t\t\t   MAX_SCHEDULE_TIMEOUT);\n\t\t\tcontinue;\n\t\t}\n\n\t\ts = find_next_bit_wrap(ctx.sectors_to_do, stripe_cnt, s);\n\t\tif (s == stripe_cnt)\n\t\t\tbreak;\n\n\t\tlogical_sector = ctx.first_sector +\n\t\t\t(s << RAID5_STRIPE_SHIFT(conf));\n\t}\n\tremove_wait_queue(&conf->wait_for_overlap, &wait);\n\n\tif (ctx.batch_last)\n\t\traid5_release_stripe(ctx.batch_last);\n\n\tif (rw == WRITE)\n\t\tmd_write_end(mddev);\n\tbio_endio(bi);\n\treturn true;\n}\n\nstatic sector_t raid5_size(struct mddev *mddev, sector_t sectors, int raid_disks);\n\nstatic sector_t reshape_request(struct mddev *mddev, sector_t sector_nr, int *skipped)\n{\n\t \n\tstruct r5conf *conf = mddev->private;\n\tstruct stripe_head *sh;\n\tstruct md_rdev *rdev;\n\tsector_t first_sector, last_sector;\n\tint raid_disks = conf->previous_raid_disks;\n\tint data_disks = raid_disks - conf->max_degraded;\n\tint new_data_disks = conf->raid_disks - conf->max_degraded;\n\tint i;\n\tint dd_idx;\n\tsector_t writepos, readpos, safepos;\n\tsector_t stripe_addr;\n\tint reshape_sectors;\n\tstruct list_head stripes;\n\tsector_t retn;\n\n\tif (sector_nr == 0) {\n\t\t \n\t\tif (mddev->reshape_backwards &&\n\t\t    conf->reshape_progress < raid5_size(mddev, 0, 0)) {\n\t\t\tsector_nr = raid5_size(mddev, 0, 0)\n\t\t\t\t- conf->reshape_progress;\n\t\t} else if (mddev->reshape_backwards &&\n\t\t\t   conf->reshape_progress == MaxSector) {\n\t\t\t \n\t\t\tsector_nr = MaxSector;\n\t\t} else if (!mddev->reshape_backwards &&\n\t\t\t   conf->reshape_progress > 0)\n\t\t\tsector_nr = conf->reshape_progress;\n\t\tsector_div(sector_nr, new_data_disks);\n\t\tif (sector_nr) {\n\t\t\tmddev->curr_resync_completed = sector_nr;\n\t\t\tsysfs_notify_dirent_safe(mddev->sysfs_completed);\n\t\t\t*skipped = 1;\n\t\t\tretn = sector_nr;\n\t\t\tgoto finish;\n\t\t}\n\t}\n\n\t \n\n\treshape_sectors = max(conf->chunk_sectors, conf->prev_chunk_sectors);\n\n\t \n\twritepos = conf->reshape_progress;\n\tsector_div(writepos, new_data_disks);\n\treadpos = conf->reshape_progress;\n\tsector_div(readpos, data_disks);\n\tsafepos = conf->reshape_safe;\n\tsector_div(safepos, data_disks);\n\tif (mddev->reshape_backwards) {\n\t\tBUG_ON(writepos < reshape_sectors);\n\t\twritepos -= reshape_sectors;\n\t\treadpos += reshape_sectors;\n\t\tsafepos += reshape_sectors;\n\t} else {\n\t\twritepos += reshape_sectors;\n\t\t \n\t\treadpos -= min_t(sector_t, reshape_sectors, readpos);\n\t\tsafepos -= min_t(sector_t, reshape_sectors, safepos);\n\t}\n\n\t \n\tif (mddev->reshape_backwards) {\n\t\tBUG_ON(conf->reshape_progress == 0);\n\t\tstripe_addr = writepos;\n\t\tBUG_ON((mddev->dev_sectors &\n\t\t\t~((sector_t)reshape_sectors - 1))\n\t\t       - reshape_sectors - stripe_addr\n\t\t       != sector_nr);\n\t} else {\n\t\tBUG_ON(writepos != sector_nr + reshape_sectors);\n\t\tstripe_addr = sector_nr;\n\t}\n\n\t \n\tif (conf->min_offset_diff < 0) {\n\t\tsafepos += -conf->min_offset_diff;\n\t\treadpos += -conf->min_offset_diff;\n\t} else\n\t\twritepos += conf->min_offset_diff;\n\n\tif ((mddev->reshape_backwards\n\t     ? (safepos > writepos && readpos < writepos)\n\t     : (safepos < writepos && readpos > writepos)) ||\n\t    time_after(jiffies, conf->reshape_checkpoint + 10*HZ)) {\n\t\t \n\t\twait_event(conf->wait_for_overlap,\n\t\t\t   atomic_read(&conf->reshape_stripes)==0\n\t\t\t   || test_bit(MD_RECOVERY_INTR, &mddev->recovery));\n\t\tif (atomic_read(&conf->reshape_stripes) != 0)\n\t\t\treturn 0;\n\t\tmddev->reshape_position = conf->reshape_progress;\n\t\tmddev->curr_resync_completed = sector_nr;\n\t\tif (!mddev->reshape_backwards)\n\t\t\t \n\t\t\trdev_for_each(rdev, mddev)\n\t\t\t\tif (rdev->raid_disk >= 0 &&\n\t\t\t\t    !test_bit(Journal, &rdev->flags) &&\n\t\t\t\t    !test_bit(In_sync, &rdev->flags) &&\n\t\t\t\t    rdev->recovery_offset < sector_nr)\n\t\t\t\t\trdev->recovery_offset = sector_nr;\n\n\t\tconf->reshape_checkpoint = jiffies;\n\t\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\t\tmd_wakeup_thread(mddev->thread);\n\t\twait_event(mddev->sb_wait, mddev->sb_flags == 0 ||\n\t\t\t   test_bit(MD_RECOVERY_INTR, &mddev->recovery));\n\t\tif (test_bit(MD_RECOVERY_INTR, &mddev->recovery))\n\t\t\treturn 0;\n\t\tspin_lock_irq(&conf->device_lock);\n\t\tconf->reshape_safe = mddev->reshape_position;\n\t\tspin_unlock_irq(&conf->device_lock);\n\t\twake_up(&conf->wait_for_overlap);\n\t\tsysfs_notify_dirent_safe(mddev->sysfs_completed);\n\t}\n\n\tINIT_LIST_HEAD(&stripes);\n\tfor (i = 0; i < reshape_sectors; i += RAID5_STRIPE_SECTORS(conf)) {\n\t\tint j;\n\t\tint skipped_disk = 0;\n\t\tsh = raid5_get_active_stripe(conf, NULL, stripe_addr+i,\n\t\t\t\t\t     R5_GAS_NOQUIESCE);\n\t\tset_bit(STRIPE_EXPANDING, &sh->state);\n\t\tatomic_inc(&conf->reshape_stripes);\n\t\t \n\t\tfor (j=sh->disks; j--;) {\n\t\t\tsector_t s;\n\t\t\tif (j == sh->pd_idx)\n\t\t\t\tcontinue;\n\t\t\tif (conf->level == 6 &&\n\t\t\t    j == sh->qd_idx)\n\t\t\t\tcontinue;\n\t\t\ts = raid5_compute_blocknr(sh, j, 0);\n\t\t\tif (s < raid5_size(mddev, 0, 0)) {\n\t\t\t\tskipped_disk = 1;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tmemset(page_address(sh->dev[j].page), 0, RAID5_STRIPE_SIZE(conf));\n\t\t\tset_bit(R5_Expanded, &sh->dev[j].flags);\n\t\t\tset_bit(R5_UPTODATE, &sh->dev[j].flags);\n\t\t}\n\t\tif (!skipped_disk) {\n\t\t\tset_bit(STRIPE_EXPAND_READY, &sh->state);\n\t\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\t}\n\t\tlist_add(&sh->lru, &stripes);\n\t}\n\tspin_lock_irq(&conf->device_lock);\n\tif (mddev->reshape_backwards)\n\t\tconf->reshape_progress -= reshape_sectors * new_data_disks;\n\telse\n\t\tconf->reshape_progress += reshape_sectors * new_data_disks;\n\tspin_unlock_irq(&conf->device_lock);\n\t \n\tfirst_sector =\n\t\traid5_compute_sector(conf, stripe_addr*(new_data_disks),\n\t\t\t\t     1, &dd_idx, NULL);\n\tlast_sector =\n\t\traid5_compute_sector(conf, ((stripe_addr+reshape_sectors)\n\t\t\t\t\t    * new_data_disks - 1),\n\t\t\t\t     1, &dd_idx, NULL);\n\tif (last_sector >= mddev->dev_sectors)\n\t\tlast_sector = mddev->dev_sectors - 1;\n\twhile (first_sector <= last_sector) {\n\t\tsh = raid5_get_active_stripe(conf, NULL, first_sector,\n\t\t\t\tR5_GAS_PREVIOUS | R5_GAS_NOQUIESCE);\n\t\tset_bit(STRIPE_EXPAND_SOURCE, &sh->state);\n\t\tset_bit(STRIPE_HANDLE, &sh->state);\n\t\traid5_release_stripe(sh);\n\t\tfirst_sector += RAID5_STRIPE_SECTORS(conf);\n\t}\n\t \n\twhile (!list_empty(&stripes)) {\n\t\tsh = list_entry(stripes.next, struct stripe_head, lru);\n\t\tlist_del_init(&sh->lru);\n\t\traid5_release_stripe(sh);\n\t}\n\t \n\tsector_nr += reshape_sectors;\n\tretn = reshape_sectors;\nfinish:\n\tif (mddev->curr_resync_completed > mddev->resync_max ||\n\t    (sector_nr - mddev->curr_resync_completed) * 2\n\t    >= mddev->resync_max - mddev->curr_resync_completed) {\n\t\t \n\t\twait_event(conf->wait_for_overlap,\n\t\t\t   atomic_read(&conf->reshape_stripes) == 0\n\t\t\t   || test_bit(MD_RECOVERY_INTR, &mddev->recovery));\n\t\tif (atomic_read(&conf->reshape_stripes) != 0)\n\t\t\tgoto ret;\n\t\tmddev->reshape_position = conf->reshape_progress;\n\t\tmddev->curr_resync_completed = sector_nr;\n\t\tif (!mddev->reshape_backwards)\n\t\t\t \n\t\t\trdev_for_each(rdev, mddev)\n\t\t\t\tif (rdev->raid_disk >= 0 &&\n\t\t\t\t    !test_bit(Journal, &rdev->flags) &&\n\t\t\t\t    !test_bit(In_sync, &rdev->flags) &&\n\t\t\t\t    rdev->recovery_offset < sector_nr)\n\t\t\t\t\trdev->recovery_offset = sector_nr;\n\t\tconf->reshape_checkpoint = jiffies;\n\t\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\t\tmd_wakeup_thread(mddev->thread);\n\t\twait_event(mddev->sb_wait,\n\t\t\t   !test_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags)\n\t\t\t   || test_bit(MD_RECOVERY_INTR, &mddev->recovery));\n\t\tif (test_bit(MD_RECOVERY_INTR, &mddev->recovery))\n\t\t\tgoto ret;\n\t\tspin_lock_irq(&conf->device_lock);\n\t\tconf->reshape_safe = mddev->reshape_position;\n\t\tspin_unlock_irq(&conf->device_lock);\n\t\twake_up(&conf->wait_for_overlap);\n\t\tsysfs_notify_dirent_safe(mddev->sysfs_completed);\n\t}\nret:\n\treturn retn;\n}\n\nstatic inline sector_t raid5_sync_request(struct mddev *mddev, sector_t sector_nr,\n\t\t\t\t\t  int *skipped)\n{\n\tstruct r5conf *conf = mddev->private;\n\tstruct stripe_head *sh;\n\tsector_t max_sector = mddev->dev_sectors;\n\tsector_t sync_blocks;\n\tint still_degraded = 0;\n\tint i;\n\n\tif (sector_nr >= max_sector) {\n\t\t \n\n\t\tif (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery)) {\n\t\t\tend_reshape(conf);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (mddev->curr_resync < max_sector)  \n\t\t\tmd_bitmap_end_sync(mddev->bitmap, mddev->curr_resync,\n\t\t\t\t\t   &sync_blocks, 1);\n\t\telse  \n\t\t\tconf->fullsync = 0;\n\t\tmd_bitmap_close_sync(mddev->bitmap);\n\n\t\treturn 0;\n\t}\n\n\t \n\twait_event(conf->wait_for_overlap, conf->quiesce != 2);\n\n\tif (test_bit(MD_RECOVERY_RESHAPE, &mddev->recovery))\n\t\treturn reshape_request(mddev, sector_nr, skipped);\n\n\t \n\n\t \n\tif (mddev->degraded >= conf->max_degraded &&\n\t    test_bit(MD_RECOVERY_SYNC, &mddev->recovery)) {\n\t\tsector_t rv = mddev->dev_sectors - sector_nr;\n\t\t*skipped = 1;\n\t\treturn rv;\n\t}\n\tif (!test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery) &&\n\t    !conf->fullsync &&\n\t    !md_bitmap_start_sync(mddev->bitmap, sector_nr, &sync_blocks, 1) &&\n\t    sync_blocks >= RAID5_STRIPE_SECTORS(conf)) {\n\t\t \n\t\tdo_div(sync_blocks, RAID5_STRIPE_SECTORS(conf));\n\t\t*skipped = 1;\n\t\t \n\t\treturn sync_blocks * RAID5_STRIPE_SECTORS(conf);\n\t}\n\n\tmd_bitmap_cond_end_sync(mddev->bitmap, sector_nr, false);\n\n\tsh = raid5_get_active_stripe(conf, NULL, sector_nr,\n\t\t\t\t     R5_GAS_NOBLOCK);\n\tif (sh == NULL) {\n\t\tsh = raid5_get_active_stripe(conf, NULL, sector_nr, 0);\n\t\t \n\t\tschedule_timeout_uninterruptible(1);\n\t}\n\t \n\trcu_read_lock();\n\tfor (i = 0; i < conf->raid_disks; i++) {\n\t\tstruct md_rdev *rdev = rcu_dereference(conf->disks[i].rdev);\n\n\t\tif (rdev == NULL || test_bit(Faulty, &rdev->flags))\n\t\t\tstill_degraded = 1;\n\t}\n\trcu_read_unlock();\n\n\tmd_bitmap_start_sync(mddev->bitmap, sector_nr, &sync_blocks, still_degraded);\n\n\tset_bit(STRIPE_SYNC_REQUESTED, &sh->state);\n\tset_bit(STRIPE_HANDLE, &sh->state);\n\n\traid5_release_stripe(sh);\n\n\treturn RAID5_STRIPE_SECTORS(conf);\n}\n\nstatic int  retry_aligned_read(struct r5conf *conf, struct bio *raid_bio,\n\t\t\t       unsigned int offset)\n{\n\t \n\tstruct stripe_head *sh;\n\tint dd_idx;\n\tsector_t sector, logical_sector, last_sector;\n\tint scnt = 0;\n\tint handled = 0;\n\n\tlogical_sector = raid_bio->bi_iter.bi_sector &\n\t\t~((sector_t)RAID5_STRIPE_SECTORS(conf)-1);\n\tsector = raid5_compute_sector(conf, logical_sector,\n\t\t\t\t      0, &dd_idx, NULL);\n\tlast_sector = bio_end_sector(raid_bio);\n\n\tfor (; logical_sector < last_sector;\n\t     logical_sector += RAID5_STRIPE_SECTORS(conf),\n\t\t     sector += RAID5_STRIPE_SECTORS(conf),\n\t\t     scnt++) {\n\n\t\tif (scnt < offset)\n\t\t\t \n\t\t\tcontinue;\n\n\t\tsh = raid5_get_active_stripe(conf, NULL, sector,\n\t\t\t\tR5_GAS_NOBLOCK | R5_GAS_NOQUIESCE);\n\t\tif (!sh) {\n\t\t\t \n\t\t\tconf->retry_read_aligned = raid_bio;\n\t\t\tconf->retry_read_offset = scnt;\n\t\t\treturn handled;\n\t\t}\n\n\t\tif (!add_stripe_bio(sh, raid_bio, dd_idx, 0, 0)) {\n\t\t\traid5_release_stripe(sh);\n\t\t\tconf->retry_read_aligned = raid_bio;\n\t\t\tconf->retry_read_offset = scnt;\n\t\t\treturn handled;\n\t\t}\n\n\t\tset_bit(R5_ReadNoMerge, &sh->dev[dd_idx].flags);\n\t\thandle_stripe(sh);\n\t\traid5_release_stripe(sh);\n\t\thandled++;\n\t}\n\n\tbio_endio(raid_bio);\n\n\tif (atomic_dec_and_test(&conf->active_aligned_reads))\n\t\twake_up(&conf->wait_for_quiescent);\n\treturn handled;\n}\n\nstatic int handle_active_stripes(struct r5conf *conf, int group,\n\t\t\t\t struct r5worker *worker,\n\t\t\t\t struct list_head *temp_inactive_list)\n\t\t__must_hold(&conf->device_lock)\n{\n\tstruct stripe_head *batch[MAX_STRIPE_BATCH], *sh;\n\tint i, batch_size = 0, hash;\n\tbool release_inactive = false;\n\n\twhile (batch_size < MAX_STRIPE_BATCH &&\n\t\t\t(sh = __get_priority_stripe(conf, group)) != NULL)\n\t\tbatch[batch_size++] = sh;\n\n\tif (batch_size == 0) {\n\t\tfor (i = 0; i < NR_STRIPE_HASH_LOCKS; i++)\n\t\t\tif (!list_empty(temp_inactive_list + i))\n\t\t\t\tbreak;\n\t\tif (i == NR_STRIPE_HASH_LOCKS) {\n\t\t\tspin_unlock_irq(&conf->device_lock);\n\t\t\tlog_flush_stripe_to_raid(conf);\n\t\t\tspin_lock_irq(&conf->device_lock);\n\t\t\treturn batch_size;\n\t\t}\n\t\trelease_inactive = true;\n\t}\n\tspin_unlock_irq(&conf->device_lock);\n\n\trelease_inactive_stripe_list(conf, temp_inactive_list,\n\t\t\t\t     NR_STRIPE_HASH_LOCKS);\n\n\tr5l_flush_stripe_to_raid(conf->log);\n\tif (release_inactive) {\n\t\tspin_lock_irq(&conf->device_lock);\n\t\treturn 0;\n\t}\n\n\tfor (i = 0; i < batch_size; i++)\n\t\thandle_stripe(batch[i]);\n\tlog_write_stripe_run(conf);\n\n\tcond_resched();\n\n\tspin_lock_irq(&conf->device_lock);\n\tfor (i = 0; i < batch_size; i++) {\n\t\thash = batch[i]->hash_lock_index;\n\t\t__release_stripe(conf, batch[i], &temp_inactive_list[hash]);\n\t}\n\treturn batch_size;\n}\n\nstatic void raid5_do_work(struct work_struct *work)\n{\n\tstruct r5worker *worker = container_of(work, struct r5worker, work);\n\tstruct r5worker_group *group = worker->group;\n\tstruct r5conf *conf = group->conf;\n\tstruct mddev *mddev = conf->mddev;\n\tint group_id = group - conf->worker_groups;\n\tint handled;\n\tstruct blk_plug plug;\n\n\tpr_debug(\"+++ raid5worker active\\n\");\n\n\tblk_start_plug(&plug);\n\thandled = 0;\n\tspin_lock_irq(&conf->device_lock);\n\twhile (1) {\n\t\tint batch_size, released;\n\n\t\treleased = release_stripe_list(conf, worker->temp_inactive_list);\n\n\t\tbatch_size = handle_active_stripes(conf, group_id, worker,\n\t\t\t\t\t\t   worker->temp_inactive_list);\n\t\tworker->working = false;\n\t\tif (!batch_size && !released)\n\t\t\tbreak;\n\t\thandled += batch_size;\n\t\twait_event_lock_irq(mddev->sb_wait,\n\t\t\t!test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags),\n\t\t\tconf->device_lock);\n\t}\n\tpr_debug(\"%d stripes handled\\n\", handled);\n\n\tspin_unlock_irq(&conf->device_lock);\n\n\tflush_deferred_bios(conf);\n\n\tr5l_flush_stripe_to_raid(conf->log);\n\n\tasync_tx_issue_pending_all();\n\tblk_finish_plug(&plug);\n\n\tpr_debug(\"--- raid5worker inactive\\n\");\n}\n\n \nstatic void raid5d(struct md_thread *thread)\n{\n\tstruct mddev *mddev = thread->mddev;\n\tstruct r5conf *conf = mddev->private;\n\tint handled;\n\tstruct blk_plug plug;\n\n\tpr_debug(\"+++ raid5d active\\n\");\n\n\tmd_check_recovery(mddev);\n\n\tblk_start_plug(&plug);\n\thandled = 0;\n\tspin_lock_irq(&conf->device_lock);\n\twhile (1) {\n\t\tstruct bio *bio;\n\t\tint batch_size, released;\n\t\tunsigned int offset;\n\n\t\treleased = release_stripe_list(conf, conf->temp_inactive_list);\n\t\tif (released)\n\t\t\tclear_bit(R5_DID_ALLOC, &conf->cache_state);\n\n\t\tif (\n\t\t    !list_empty(&conf->bitmap_list)) {\n\t\t\t \n\t\t\tconf->seq_flush++;\n\t\t\tspin_unlock_irq(&conf->device_lock);\n\t\t\tmd_bitmap_unplug(mddev->bitmap);\n\t\t\tspin_lock_irq(&conf->device_lock);\n\t\t\tconf->seq_write = conf->seq_flush;\n\t\t\tactivate_bit_delay(conf, conf->temp_inactive_list);\n\t\t}\n\t\traid5_activate_delayed(conf);\n\n\t\twhile ((bio = remove_bio_from_retry(conf, &offset))) {\n\t\t\tint ok;\n\t\t\tspin_unlock_irq(&conf->device_lock);\n\t\t\tok = retry_aligned_read(conf, bio, offset);\n\t\t\tspin_lock_irq(&conf->device_lock);\n\t\t\tif (!ok)\n\t\t\t\tbreak;\n\t\t\thandled++;\n\t\t}\n\n\t\tbatch_size = handle_active_stripes(conf, ANY_GROUP, NULL,\n\t\t\t\t\t\t   conf->temp_inactive_list);\n\t\tif (!batch_size && !released)\n\t\t\tbreak;\n\t\thandled += batch_size;\n\n\t\tif (mddev->sb_flags & ~(1 << MD_SB_CHANGE_PENDING)) {\n\t\t\tspin_unlock_irq(&conf->device_lock);\n\t\t\tmd_check_recovery(mddev);\n\t\t\tspin_lock_irq(&conf->device_lock);\n\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\n\t\twait_event_lock_irq(mddev->sb_wait,\n\t\t\t!test_bit(MD_SB_CHANGE_PENDING, &mddev->sb_flags),\n\t\t\tconf->device_lock);\n\t}\n\tpr_debug(\"%d stripes handled\\n\", handled);\n\n\tspin_unlock_irq(&conf->device_lock);\n\tif (test_and_clear_bit(R5_ALLOC_MORE, &conf->cache_state) &&\n\t    mutex_trylock(&conf->cache_size_mutex)) {\n\t\tgrow_one_stripe(conf, __GFP_NOWARN);\n\t\t \n\t\tset_bit(R5_DID_ALLOC, &conf->cache_state);\n\t\tmutex_unlock(&conf->cache_size_mutex);\n\t}\n\n\tflush_deferred_bios(conf);\n\n\tr5l_flush_stripe_to_raid(conf->log);\n\n\tasync_tx_issue_pending_all();\n\tblk_finish_plug(&plug);\n\n\tpr_debug(\"--- raid5d inactive\\n\");\n}\n\nstatic ssize_t\nraid5_show_stripe_cache_size(struct mddev *mddev, char *page)\n{\n\tstruct r5conf *conf;\n\tint ret = 0;\n\tspin_lock(&mddev->lock);\n\tconf = mddev->private;\n\tif (conf)\n\t\tret = sprintf(page, \"%d\\n\", conf->min_nr_stripes);\n\tspin_unlock(&mddev->lock);\n\treturn ret;\n}\n\nint\nraid5_set_cache_size(struct mddev *mddev, int size)\n{\n\tint result = 0;\n\tstruct r5conf *conf = mddev->private;\n\n\tif (size <= 16 || size > 32768)\n\t\treturn -EINVAL;\n\n\tconf->min_nr_stripes = size;\n\tmutex_lock(&conf->cache_size_mutex);\n\twhile (size < conf->max_nr_stripes &&\n\t       drop_one_stripe(conf))\n\t\t;\n\tmutex_unlock(&conf->cache_size_mutex);\n\n\tmd_allow_write(mddev);\n\n\tmutex_lock(&conf->cache_size_mutex);\n\twhile (size > conf->max_nr_stripes)\n\t\tif (!grow_one_stripe(conf, GFP_KERNEL)) {\n\t\t\tconf->min_nr_stripes = conf->max_nr_stripes;\n\t\t\tresult = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\tmutex_unlock(&conf->cache_size_mutex);\n\n\treturn result;\n}\nEXPORT_SYMBOL(raid5_set_cache_size);\n\nstatic ssize_t\nraid5_store_stripe_cache_size(struct mddev *mddev, const char *page, size_t len)\n{\n\tstruct r5conf *conf;\n\tunsigned long new;\n\tint err;\n\n\tif (len >= PAGE_SIZE)\n\t\treturn -EINVAL;\n\tif (kstrtoul(page, 10, &new))\n\t\treturn -EINVAL;\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tconf = mddev->private;\n\tif (!conf)\n\t\terr = -ENODEV;\n\telse\n\t\terr = raid5_set_cache_size(mddev, new);\n\tmddev_unlock(mddev);\n\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry\nraid5_stripecache_size = __ATTR(stripe_cache_size, S_IRUGO | S_IWUSR,\n\t\t\t\traid5_show_stripe_cache_size,\n\t\t\t\traid5_store_stripe_cache_size);\n\nstatic ssize_t\nraid5_show_rmw_level(struct mddev  *mddev, char *page)\n{\n\tstruct r5conf *conf = mddev->private;\n\tif (conf)\n\t\treturn sprintf(page, \"%d\\n\", conf->rmw_level);\n\telse\n\t\treturn 0;\n}\n\nstatic ssize_t\nraid5_store_rmw_level(struct mddev  *mddev, const char *page, size_t len)\n{\n\tstruct r5conf *conf = mddev->private;\n\tunsigned long new;\n\n\tif (!conf)\n\t\treturn -ENODEV;\n\n\tif (len >= PAGE_SIZE)\n\t\treturn -EINVAL;\n\n\tif (kstrtoul(page, 10, &new))\n\t\treturn -EINVAL;\n\n\tif (new != PARITY_DISABLE_RMW && !raid6_call.xor_syndrome)\n\t\treturn -EINVAL;\n\n\tif (new != PARITY_DISABLE_RMW &&\n\t    new != PARITY_ENABLE_RMW &&\n\t    new != PARITY_PREFER_RMW)\n\t\treturn -EINVAL;\n\n\tconf->rmw_level = new;\n\treturn len;\n}\n\nstatic struct md_sysfs_entry\nraid5_rmw_level = __ATTR(rmw_level, S_IRUGO | S_IWUSR,\n\t\t\t raid5_show_rmw_level,\n\t\t\t raid5_store_rmw_level);\n\nstatic ssize_t\nraid5_show_stripe_size(struct mddev  *mddev, char *page)\n{\n\tstruct r5conf *conf;\n\tint ret = 0;\n\n\tspin_lock(&mddev->lock);\n\tconf = mddev->private;\n\tif (conf)\n\t\tret = sprintf(page, \"%lu\\n\", RAID5_STRIPE_SIZE(conf));\n\tspin_unlock(&mddev->lock);\n\treturn ret;\n}\n\n#if PAGE_SIZE != DEFAULT_STRIPE_SIZE\nstatic ssize_t\nraid5_store_stripe_size(struct mddev  *mddev, const char *page, size_t len)\n{\n\tstruct r5conf *conf;\n\tunsigned long new;\n\tint err;\n\tint size;\n\n\tif (len >= PAGE_SIZE)\n\t\treturn -EINVAL;\n\tif (kstrtoul(page, 10, &new))\n\t\treturn -EINVAL;\n\n\t \n\tif (new % DEFAULT_STRIPE_SIZE != 0 ||\n\t\t\tnew > PAGE_SIZE || new == 0 ||\n\t\t\tnew != roundup_pow_of_two(new))\n\t\treturn -EINVAL;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\n\tconf = mddev->private;\n\tif (!conf) {\n\t\terr = -ENODEV;\n\t\tgoto out_unlock;\n\t}\n\n\tif (new == conf->stripe_size)\n\t\tgoto out_unlock;\n\n\tpr_debug(\"md/raid: change stripe_size from %lu to %lu\\n\",\n\t\t\tconf->stripe_size, new);\n\n\tif (mddev->sync_thread ||\n\t\ttest_bit(MD_RECOVERY_RUNNING, &mddev->recovery) ||\n\t\tmddev->reshape_position != MaxSector ||\n\t\tmddev->sysfs_active) {\n\t\terr = -EBUSY;\n\t\tgoto out_unlock;\n\t}\n\n\tmddev_suspend(mddev);\n\tmutex_lock(&conf->cache_size_mutex);\n\tsize = conf->max_nr_stripes;\n\n\tshrink_stripes(conf);\n\n\tconf->stripe_size = new;\n\tconf->stripe_shift = ilog2(new) - 9;\n\tconf->stripe_sectors = new >> 9;\n\tif (grow_stripes(conf, size)) {\n\t\tpr_warn(\"md/raid:%s: couldn't allocate buffers\\n\",\n\t\t\t\tmdname(mddev));\n\t\terr = -ENOMEM;\n\t}\n\tmutex_unlock(&conf->cache_size_mutex);\n\tmddev_resume(mddev);\n\nout_unlock:\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry\nraid5_stripe_size = __ATTR(stripe_size, 0644,\n\t\t\t raid5_show_stripe_size,\n\t\t\t raid5_store_stripe_size);\n#else\nstatic struct md_sysfs_entry\nraid5_stripe_size = __ATTR(stripe_size, 0444,\n\t\t\t raid5_show_stripe_size,\n\t\t\t NULL);\n#endif\n\nstatic ssize_t\nraid5_show_preread_threshold(struct mddev *mddev, char *page)\n{\n\tstruct r5conf *conf;\n\tint ret = 0;\n\tspin_lock(&mddev->lock);\n\tconf = mddev->private;\n\tif (conf)\n\t\tret = sprintf(page, \"%d\\n\", conf->bypass_threshold);\n\tspin_unlock(&mddev->lock);\n\treturn ret;\n}\n\nstatic ssize_t\nraid5_store_preread_threshold(struct mddev *mddev, const char *page, size_t len)\n{\n\tstruct r5conf *conf;\n\tunsigned long new;\n\tint err;\n\n\tif (len >= PAGE_SIZE)\n\t\treturn -EINVAL;\n\tif (kstrtoul(page, 10, &new))\n\t\treturn -EINVAL;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tconf = mddev->private;\n\tif (!conf)\n\t\terr = -ENODEV;\n\telse if (new > conf->min_nr_stripes)\n\t\terr = -EINVAL;\n\telse\n\t\tconf->bypass_threshold = new;\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry\nraid5_preread_bypass_threshold = __ATTR(preread_bypass_threshold,\n\t\t\t\t\tS_IRUGO | S_IWUSR,\n\t\t\t\t\traid5_show_preread_threshold,\n\t\t\t\t\traid5_store_preread_threshold);\n\nstatic ssize_t\nraid5_show_skip_copy(struct mddev *mddev, char *page)\n{\n\tstruct r5conf *conf;\n\tint ret = 0;\n\tspin_lock(&mddev->lock);\n\tconf = mddev->private;\n\tif (conf)\n\t\tret = sprintf(page, \"%d\\n\", conf->skip_copy);\n\tspin_unlock(&mddev->lock);\n\treturn ret;\n}\n\nstatic ssize_t\nraid5_store_skip_copy(struct mddev *mddev, const char *page, size_t len)\n{\n\tstruct r5conf *conf;\n\tunsigned long new;\n\tint err;\n\n\tif (len >= PAGE_SIZE)\n\t\treturn -EINVAL;\n\tif (kstrtoul(page, 10, &new))\n\t\treturn -EINVAL;\n\tnew = !!new;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tconf = mddev->private;\n\tif (!conf)\n\t\terr = -ENODEV;\n\telse if (new != conf->skip_copy) {\n\t\tstruct request_queue *q = mddev->queue;\n\n\t\tmddev_suspend(mddev);\n\t\tconf->skip_copy = new;\n\t\tif (new)\n\t\t\tblk_queue_flag_set(QUEUE_FLAG_STABLE_WRITES, q);\n\t\telse\n\t\t\tblk_queue_flag_clear(QUEUE_FLAG_STABLE_WRITES, q);\n\t\tmddev_resume(mddev);\n\t}\n\tmddev_unlock(mddev);\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry\nraid5_skip_copy = __ATTR(skip_copy, S_IRUGO | S_IWUSR,\n\t\t\t\t\traid5_show_skip_copy,\n\t\t\t\t\traid5_store_skip_copy);\n\nstatic ssize_t\nstripe_cache_active_show(struct mddev *mddev, char *page)\n{\n\tstruct r5conf *conf = mddev->private;\n\tif (conf)\n\t\treturn sprintf(page, \"%d\\n\", atomic_read(&conf->active_stripes));\n\telse\n\t\treturn 0;\n}\n\nstatic struct md_sysfs_entry\nraid5_stripecache_active = __ATTR_RO(stripe_cache_active);\n\nstatic ssize_t\nraid5_show_group_thread_cnt(struct mddev *mddev, char *page)\n{\n\tstruct r5conf *conf;\n\tint ret = 0;\n\tspin_lock(&mddev->lock);\n\tconf = mddev->private;\n\tif (conf)\n\t\tret = sprintf(page, \"%d\\n\", conf->worker_cnt_per_group);\n\tspin_unlock(&mddev->lock);\n\treturn ret;\n}\n\nstatic int alloc_thread_groups(struct r5conf *conf, int cnt,\n\t\t\t       int *group_cnt,\n\t\t\t       struct r5worker_group **worker_groups);\nstatic ssize_t\nraid5_store_group_thread_cnt(struct mddev *mddev, const char *page, size_t len)\n{\n\tstruct r5conf *conf;\n\tunsigned int new;\n\tint err;\n\tstruct r5worker_group *new_groups, *old_groups;\n\tint group_cnt;\n\n\tif (len >= PAGE_SIZE)\n\t\treturn -EINVAL;\n\tif (kstrtouint(page, 10, &new))\n\t\treturn -EINVAL;\n\t \n\tif (new > 8192)\n\t\treturn -EINVAL;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tconf = mddev->private;\n\tif (!conf)\n\t\terr = -ENODEV;\n\telse if (new != conf->worker_cnt_per_group) {\n\t\tmddev_suspend(mddev);\n\n\t\told_groups = conf->worker_groups;\n\t\tif (old_groups)\n\t\t\tflush_workqueue(raid5_wq);\n\n\t\terr = alloc_thread_groups(conf, new, &group_cnt, &new_groups);\n\t\tif (!err) {\n\t\t\tspin_lock_irq(&conf->device_lock);\n\t\t\tconf->group_cnt = group_cnt;\n\t\t\tconf->worker_cnt_per_group = new;\n\t\t\tconf->worker_groups = new_groups;\n\t\t\tspin_unlock_irq(&conf->device_lock);\n\n\t\t\tif (old_groups)\n\t\t\t\tkfree(old_groups[0].workers);\n\t\t\tkfree(old_groups);\n\t\t}\n\t\tmddev_resume(mddev);\n\t}\n\tmddev_unlock(mddev);\n\n\treturn err ?: len;\n}\n\nstatic struct md_sysfs_entry\nraid5_group_thread_cnt = __ATTR(group_thread_cnt, S_IRUGO | S_IWUSR,\n\t\t\t\traid5_show_group_thread_cnt,\n\t\t\t\traid5_store_group_thread_cnt);\n\nstatic struct attribute *raid5_attrs[] =  {\n\t&raid5_stripecache_size.attr,\n\t&raid5_stripecache_active.attr,\n\t&raid5_preread_bypass_threshold.attr,\n\t&raid5_group_thread_cnt.attr,\n\t&raid5_skip_copy.attr,\n\t&raid5_rmw_level.attr,\n\t&raid5_stripe_size.attr,\n\t&r5c_journal_mode.attr,\n\t&ppl_write_hint.attr,\n\tNULL,\n};\nstatic const struct attribute_group raid5_attrs_group = {\n\t.name = NULL,\n\t.attrs = raid5_attrs,\n};\n\nstatic int alloc_thread_groups(struct r5conf *conf, int cnt, int *group_cnt,\n\t\t\t       struct r5worker_group **worker_groups)\n{\n\tint i, j, k;\n\tssize_t size;\n\tstruct r5worker *workers;\n\n\tif (cnt == 0) {\n\t\t*group_cnt = 0;\n\t\t*worker_groups = NULL;\n\t\treturn 0;\n\t}\n\t*group_cnt = num_possible_nodes();\n\tsize = sizeof(struct r5worker) * cnt;\n\tworkers = kcalloc(size, *group_cnt, GFP_NOIO);\n\t*worker_groups = kcalloc(*group_cnt, sizeof(struct r5worker_group),\n\t\t\t\t GFP_NOIO);\n\tif (!*worker_groups || !workers) {\n\t\tkfree(workers);\n\t\tkfree(*worker_groups);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (i = 0; i < *group_cnt; i++) {\n\t\tstruct r5worker_group *group;\n\n\t\tgroup = &(*worker_groups)[i];\n\t\tINIT_LIST_HEAD(&group->handle_list);\n\t\tINIT_LIST_HEAD(&group->loprio_list);\n\t\tgroup->conf = conf;\n\t\tgroup->workers = workers + i * cnt;\n\n\t\tfor (j = 0; j < cnt; j++) {\n\t\t\tstruct r5worker *worker = group->workers + j;\n\t\t\tworker->group = group;\n\t\t\tINIT_WORK(&worker->work, raid5_do_work);\n\n\t\t\tfor (k = 0; k < NR_STRIPE_HASH_LOCKS; k++)\n\t\t\t\tINIT_LIST_HEAD(worker->temp_inactive_list + k);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void free_thread_groups(struct r5conf *conf)\n{\n\tif (conf->worker_groups)\n\t\tkfree(conf->worker_groups[0].workers);\n\tkfree(conf->worker_groups);\n\tconf->worker_groups = NULL;\n}\n\nstatic sector_t\nraid5_size(struct mddev *mddev, sector_t sectors, int raid_disks)\n{\n\tstruct r5conf *conf = mddev->private;\n\n\tif (!sectors)\n\t\tsectors = mddev->dev_sectors;\n\tif (!raid_disks)\n\t\t \n\t\traid_disks = min(conf->raid_disks, conf->previous_raid_disks);\n\n\tsectors &= ~((sector_t)conf->chunk_sectors - 1);\n\tsectors &= ~((sector_t)conf->prev_chunk_sectors - 1);\n\treturn sectors * (raid_disks - conf->max_degraded);\n}\n\nstatic void free_scratch_buffer(struct r5conf *conf, struct raid5_percpu *percpu)\n{\n\tsafe_put_page(percpu->spare_page);\n\tpercpu->spare_page = NULL;\n\tkvfree(percpu->scribble);\n\tpercpu->scribble = NULL;\n}\n\nstatic int alloc_scratch_buffer(struct r5conf *conf, struct raid5_percpu *percpu)\n{\n\tif (conf->level == 6 && !percpu->spare_page) {\n\t\tpercpu->spare_page = alloc_page(GFP_KERNEL);\n\t\tif (!percpu->spare_page)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (scribble_alloc(percpu,\n\t\t\t   max(conf->raid_disks,\n\t\t\t       conf->previous_raid_disks),\n\t\t\t   max(conf->chunk_sectors,\n\t\t\t       conf->prev_chunk_sectors)\n\t\t\t   / RAID5_STRIPE_SECTORS(conf))) {\n\t\tfree_scratch_buffer(conf, percpu);\n\t\treturn -ENOMEM;\n\t}\n\n\tlocal_lock_init(&percpu->lock);\n\treturn 0;\n}\n\nstatic int raid456_cpu_dead(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct r5conf *conf = hlist_entry_safe(node, struct r5conf, node);\n\n\tfree_scratch_buffer(conf, per_cpu_ptr(conf->percpu, cpu));\n\treturn 0;\n}\n\nstatic void raid5_free_percpu(struct r5conf *conf)\n{\n\tif (!conf->percpu)\n\t\treturn;\n\n\tcpuhp_state_remove_instance(CPUHP_MD_RAID5_PREPARE, &conf->node);\n\tfree_percpu(conf->percpu);\n}\n\nstatic void free_conf(struct r5conf *conf)\n{\n\tint i;\n\n\tlog_exit(conf);\n\n\tunregister_shrinker(&conf->shrinker);\n\tfree_thread_groups(conf);\n\tshrink_stripes(conf);\n\traid5_free_percpu(conf);\n\tfor (i = 0; i < conf->pool_size; i++)\n\t\tif (conf->disks[i].extra_page)\n\t\t\tput_page(conf->disks[i].extra_page);\n\tkfree(conf->disks);\n\tbioset_exit(&conf->bio_split);\n\tkfree(conf->stripe_hashtbl);\n\tkfree(conf->pending_data);\n\tkfree(conf);\n}\n\nstatic int raid456_cpu_up_prepare(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct r5conf *conf = hlist_entry_safe(node, struct r5conf, node);\n\tstruct raid5_percpu *percpu = per_cpu_ptr(conf->percpu, cpu);\n\n\tif (alloc_scratch_buffer(conf, percpu)) {\n\t\tpr_warn(\"%s: failed memory allocation for cpu%u\\n\",\n\t\t\t__func__, cpu);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic int raid5_alloc_percpu(struct r5conf *conf)\n{\n\tint err = 0;\n\n\tconf->percpu = alloc_percpu(struct raid5_percpu);\n\tif (!conf->percpu)\n\t\treturn -ENOMEM;\n\n\terr = cpuhp_state_add_instance(CPUHP_MD_RAID5_PREPARE, &conf->node);\n\tif (!err) {\n\t\tconf->scribble_disks = max(conf->raid_disks,\n\t\t\tconf->previous_raid_disks);\n\t\tconf->scribble_sectors = max(conf->chunk_sectors,\n\t\t\tconf->prev_chunk_sectors);\n\t}\n\treturn err;\n}\n\nstatic unsigned long raid5_cache_scan(struct shrinker *shrink,\n\t\t\t\t      struct shrink_control *sc)\n{\n\tstruct r5conf *conf = container_of(shrink, struct r5conf, shrinker);\n\tunsigned long ret = SHRINK_STOP;\n\n\tif (mutex_trylock(&conf->cache_size_mutex)) {\n\t\tret= 0;\n\t\twhile (ret < sc->nr_to_scan &&\n\t\t       conf->max_nr_stripes > conf->min_nr_stripes) {\n\t\t\tif (drop_one_stripe(conf) == 0) {\n\t\t\t\tret = SHRINK_STOP;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret++;\n\t\t}\n\t\tmutex_unlock(&conf->cache_size_mutex);\n\t}\n\treturn ret;\n}\n\nstatic unsigned long raid5_cache_count(struct shrinker *shrink,\n\t\t\t\t       struct shrink_control *sc)\n{\n\tstruct r5conf *conf = container_of(shrink, struct r5conf, shrinker);\n\n\tif (conf->max_nr_stripes < conf->min_nr_stripes)\n\t\t \n\t\treturn 0;\n\treturn conf->max_nr_stripes - conf->min_nr_stripes;\n}\n\nstatic struct r5conf *setup_conf(struct mddev *mddev)\n{\n\tstruct r5conf *conf;\n\tint raid_disk, memory, max_disks;\n\tstruct md_rdev *rdev;\n\tstruct disk_info *disk;\n\tchar pers_name[6];\n\tint i;\n\tint group_cnt;\n\tstruct r5worker_group *new_group;\n\tint ret = -ENOMEM;\n\n\tif (mddev->new_level != 5\n\t    && mddev->new_level != 4\n\t    && mddev->new_level != 6) {\n\t\tpr_warn(\"md/raid:%s: raid level not set to 4/5/6 (%d)\\n\",\n\t\t\tmdname(mddev), mddev->new_level);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\tif ((mddev->new_level == 5\n\t     && !algorithm_valid_raid5(mddev->new_layout)) ||\n\t    (mddev->new_level == 6\n\t     && !algorithm_valid_raid6(mddev->new_layout))) {\n\t\tpr_warn(\"md/raid:%s: layout %d not supported\\n\",\n\t\t\tmdname(mddev), mddev->new_layout);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\tif (mddev->new_level == 6 && mddev->raid_disks < 4) {\n\t\tpr_warn(\"md/raid:%s: not enough configured devices (%d, minimum 4)\\n\",\n\t\t\tmdname(mddev), mddev->raid_disks);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!mddev->new_chunk_sectors ||\n\t    (mddev->new_chunk_sectors << 9) % PAGE_SIZE ||\n\t    !is_power_of_2(mddev->new_chunk_sectors)) {\n\t\tpr_warn(\"md/raid:%s: invalid chunk size %d\\n\",\n\t\t\tmdname(mddev), mddev->new_chunk_sectors << 9);\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tconf = kzalloc(sizeof(struct r5conf), GFP_KERNEL);\n\tif (conf == NULL)\n\t\tgoto abort;\n\n#if PAGE_SIZE != DEFAULT_STRIPE_SIZE\n\tconf->stripe_size = DEFAULT_STRIPE_SIZE;\n\tconf->stripe_shift = ilog2(DEFAULT_STRIPE_SIZE) - 9;\n\tconf->stripe_sectors = DEFAULT_STRIPE_SIZE >> 9;\n#endif\n\tINIT_LIST_HEAD(&conf->free_list);\n\tINIT_LIST_HEAD(&conf->pending_list);\n\tconf->pending_data = kcalloc(PENDING_IO_MAX,\n\t\t\t\t     sizeof(struct r5pending_data),\n\t\t\t\t     GFP_KERNEL);\n\tif (!conf->pending_data)\n\t\tgoto abort;\n\tfor (i = 0; i < PENDING_IO_MAX; i++)\n\t\tlist_add(&conf->pending_data[i].sibling, &conf->free_list);\n\t \n\tif (!alloc_thread_groups(conf, 0, &group_cnt, &new_group)) {\n\t\tconf->group_cnt = group_cnt;\n\t\tconf->worker_cnt_per_group = 0;\n\t\tconf->worker_groups = new_group;\n\t} else\n\t\tgoto abort;\n\tspin_lock_init(&conf->device_lock);\n\tseqcount_spinlock_init(&conf->gen_lock, &conf->device_lock);\n\tmutex_init(&conf->cache_size_mutex);\n\n\tinit_waitqueue_head(&conf->wait_for_quiescent);\n\tinit_waitqueue_head(&conf->wait_for_stripe);\n\tinit_waitqueue_head(&conf->wait_for_overlap);\n\tINIT_LIST_HEAD(&conf->handle_list);\n\tINIT_LIST_HEAD(&conf->loprio_list);\n\tINIT_LIST_HEAD(&conf->hold_list);\n\tINIT_LIST_HEAD(&conf->delayed_list);\n\tINIT_LIST_HEAD(&conf->bitmap_list);\n\tinit_llist_head(&conf->released_stripes);\n\tatomic_set(&conf->active_stripes, 0);\n\tatomic_set(&conf->preread_active_stripes, 0);\n\tatomic_set(&conf->active_aligned_reads, 0);\n\tspin_lock_init(&conf->pending_bios_lock);\n\tconf->batch_bio_dispatch = true;\n\trdev_for_each(rdev, mddev) {\n\t\tif (test_bit(Journal, &rdev->flags))\n\t\t\tcontinue;\n\t\tif (bdev_nonrot(rdev->bdev)) {\n\t\t\tconf->batch_bio_dispatch = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tconf->bypass_threshold = BYPASS_THRESHOLD;\n\tconf->recovery_disabled = mddev->recovery_disabled - 1;\n\n\tconf->raid_disks = mddev->raid_disks;\n\tif (mddev->reshape_position == MaxSector)\n\t\tconf->previous_raid_disks = mddev->raid_disks;\n\telse\n\t\tconf->previous_raid_disks = mddev->raid_disks - mddev->delta_disks;\n\tmax_disks = max(conf->raid_disks, conf->previous_raid_disks);\n\n\tconf->disks = kcalloc(max_disks, sizeof(struct disk_info),\n\t\t\t      GFP_KERNEL);\n\n\tif (!conf->disks)\n\t\tgoto abort;\n\n\tfor (i = 0; i < max_disks; i++) {\n\t\tconf->disks[i].extra_page = alloc_page(GFP_KERNEL);\n\t\tif (!conf->disks[i].extra_page)\n\t\t\tgoto abort;\n\t}\n\n\tret = bioset_init(&conf->bio_split, BIO_POOL_SIZE, 0, 0);\n\tif (ret)\n\t\tgoto abort;\n\tconf->mddev = mddev;\n\n\tret = -ENOMEM;\n\tconf->stripe_hashtbl = kzalloc(PAGE_SIZE, GFP_KERNEL);\n\tif (!conf->stripe_hashtbl)\n\t\tgoto abort;\n\n\t \n\tspin_lock_init(conf->hash_locks);\n\tfor (i = 1; i < NR_STRIPE_HASH_LOCKS; i++)\n\t\tspin_lock_init(conf->hash_locks + i);\n\n\tfor (i = 0; i < NR_STRIPE_HASH_LOCKS; i++)\n\t\tINIT_LIST_HEAD(conf->inactive_list + i);\n\n\tfor (i = 0; i < NR_STRIPE_HASH_LOCKS; i++)\n\t\tINIT_LIST_HEAD(conf->temp_inactive_list + i);\n\n\tatomic_set(&conf->r5c_cached_full_stripes, 0);\n\tINIT_LIST_HEAD(&conf->r5c_full_stripe_list);\n\tatomic_set(&conf->r5c_cached_partial_stripes, 0);\n\tINIT_LIST_HEAD(&conf->r5c_partial_stripe_list);\n\tatomic_set(&conf->r5c_flushing_full_stripes, 0);\n\tatomic_set(&conf->r5c_flushing_partial_stripes, 0);\n\n\tconf->level = mddev->new_level;\n\tconf->chunk_sectors = mddev->new_chunk_sectors;\n\tret = raid5_alloc_percpu(conf);\n\tif (ret)\n\t\tgoto abort;\n\n\tpr_debug(\"raid456: run(%s) called.\\n\", mdname(mddev));\n\n\tret = -EIO;\n\trdev_for_each(rdev, mddev) {\n\t\traid_disk = rdev->raid_disk;\n\t\tif (raid_disk >= max_disks\n\t\t    || raid_disk < 0 || test_bit(Journal, &rdev->flags))\n\t\t\tcontinue;\n\t\tdisk = conf->disks + raid_disk;\n\n\t\tif (test_bit(Replacement, &rdev->flags)) {\n\t\t\tif (disk->replacement)\n\t\t\t\tgoto abort;\n\t\t\tRCU_INIT_POINTER(disk->replacement, rdev);\n\t\t} else {\n\t\t\tif (disk->rdev)\n\t\t\t\tgoto abort;\n\t\t\tRCU_INIT_POINTER(disk->rdev, rdev);\n\t\t}\n\n\t\tif (test_bit(In_sync, &rdev->flags)) {\n\t\t\tpr_info(\"md/raid:%s: device %pg operational as raid disk %d\\n\",\n\t\t\t\tmdname(mddev), rdev->bdev, raid_disk);\n\t\t} else if (rdev->saved_raid_disk != raid_disk)\n\t\t\t \n\t\t\tconf->fullsync = 1;\n\t}\n\n\tconf->level = mddev->new_level;\n\tif (conf->level == 6) {\n\t\tconf->max_degraded = 2;\n\t\tif (raid6_call.xor_syndrome)\n\t\t\tconf->rmw_level = PARITY_ENABLE_RMW;\n\t\telse\n\t\t\tconf->rmw_level = PARITY_DISABLE_RMW;\n\t} else {\n\t\tconf->max_degraded = 1;\n\t\tconf->rmw_level = PARITY_ENABLE_RMW;\n\t}\n\tconf->algorithm = mddev->new_layout;\n\tconf->reshape_progress = mddev->reshape_position;\n\tif (conf->reshape_progress != MaxSector) {\n\t\tconf->prev_chunk_sectors = mddev->chunk_sectors;\n\t\tconf->prev_algo = mddev->layout;\n\t} else {\n\t\tconf->prev_chunk_sectors = conf->chunk_sectors;\n\t\tconf->prev_algo = conf->algorithm;\n\t}\n\n\tconf->min_nr_stripes = NR_STRIPES;\n\tif (mddev->reshape_position != MaxSector) {\n\t\tint stripes = max_t(int,\n\t\t\t((mddev->chunk_sectors << 9) / RAID5_STRIPE_SIZE(conf)) * 4,\n\t\t\t((mddev->new_chunk_sectors << 9) / RAID5_STRIPE_SIZE(conf)) * 4);\n\t\tconf->min_nr_stripes = max(NR_STRIPES, stripes);\n\t\tif (conf->min_nr_stripes != NR_STRIPES)\n\t\t\tpr_info(\"md/raid:%s: force stripe size %d for reshape\\n\",\n\t\t\t\tmdname(mddev), conf->min_nr_stripes);\n\t}\n\tmemory = conf->min_nr_stripes * (sizeof(struct stripe_head) +\n\t\t max_disks * ((sizeof(struct bio) + PAGE_SIZE))) / 1024;\n\tatomic_set(&conf->empty_inactive_list_nr, NR_STRIPE_HASH_LOCKS);\n\tif (grow_stripes(conf, conf->min_nr_stripes)) {\n\t\tpr_warn(\"md/raid:%s: couldn't allocate %dkB for buffers\\n\",\n\t\t\tmdname(mddev), memory);\n\t\tret = -ENOMEM;\n\t\tgoto abort;\n\t} else\n\t\tpr_debug(\"md/raid:%s: allocated %dkB\\n\", mdname(mddev), memory);\n\t \n\tconf->shrinker.seeks = DEFAULT_SEEKS * conf->raid_disks * 4;\n\tconf->shrinker.scan_objects = raid5_cache_scan;\n\tconf->shrinker.count_objects = raid5_cache_count;\n\tconf->shrinker.batch = 128;\n\tconf->shrinker.flags = 0;\n\tret = register_shrinker(&conf->shrinker, \"md-raid5:%s\", mdname(mddev));\n\tif (ret) {\n\t\tpr_warn(\"md/raid:%s: couldn't register shrinker.\\n\",\n\t\t\tmdname(mddev));\n\t\tgoto abort;\n\t}\n\n\tsprintf(pers_name, \"raid%d\", mddev->new_level);\n\trcu_assign_pointer(conf->thread,\n\t\t\t   md_register_thread(raid5d, mddev, pers_name));\n\tif (!conf->thread) {\n\t\tpr_warn(\"md/raid:%s: couldn't allocate thread.\\n\",\n\t\t\tmdname(mddev));\n\t\tret = -ENOMEM;\n\t\tgoto abort;\n\t}\n\n\treturn conf;\n\n abort:\n\tif (conf)\n\t\tfree_conf(conf);\n\treturn ERR_PTR(ret);\n}\n\nstatic int only_parity(int raid_disk, int algo, int raid_disks, int max_degraded)\n{\n\tswitch (algo) {\n\tcase ALGORITHM_PARITY_0:\n\t\tif (raid_disk < max_degraded)\n\t\t\treturn 1;\n\t\tbreak;\n\tcase ALGORITHM_PARITY_N:\n\t\tif (raid_disk >= raid_disks - max_degraded)\n\t\t\treturn 1;\n\t\tbreak;\n\tcase ALGORITHM_PARITY_0_6:\n\t\tif (raid_disk == 0 ||\n\t\t    raid_disk == raid_disks - 1)\n\t\t\treturn 1;\n\t\tbreak;\n\tcase ALGORITHM_LEFT_ASYMMETRIC_6:\n\tcase ALGORITHM_RIGHT_ASYMMETRIC_6:\n\tcase ALGORITHM_LEFT_SYMMETRIC_6:\n\tcase ALGORITHM_RIGHT_SYMMETRIC_6:\n\t\tif (raid_disk == raid_disks - 1)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic void raid5_set_io_opt(struct r5conf *conf)\n{\n\tblk_queue_io_opt(conf->mddev->queue, (conf->chunk_sectors << 9) *\n\t\t\t (conf->raid_disks - conf->max_degraded));\n}\n\nstatic int raid5_run(struct mddev *mddev)\n{\n\tstruct r5conf *conf;\n\tint dirty_parity_disks = 0;\n\tstruct md_rdev *rdev;\n\tstruct md_rdev *journal_dev = NULL;\n\tsector_t reshape_offset = 0;\n\tint i;\n\tlong long min_offset_diff = 0;\n\tint first = 1;\n\n\tif (mddev_init_writes_pending(mddev) < 0)\n\t\treturn -ENOMEM;\n\n\tif (mddev->recovery_cp != MaxSector)\n\t\tpr_notice(\"md/raid:%s: not clean -- starting background reconstruction\\n\",\n\t\t\t  mdname(mddev));\n\n\trdev_for_each(rdev, mddev) {\n\t\tlong long diff;\n\n\t\tif (test_bit(Journal, &rdev->flags)) {\n\t\t\tjournal_dev = rdev;\n\t\t\tcontinue;\n\t\t}\n\t\tif (rdev->raid_disk < 0)\n\t\t\tcontinue;\n\t\tdiff = (rdev->new_data_offset - rdev->data_offset);\n\t\tif (first) {\n\t\t\tmin_offset_diff = diff;\n\t\t\tfirst = 0;\n\t\t} else if (mddev->reshape_backwards &&\n\t\t\t diff < min_offset_diff)\n\t\t\tmin_offset_diff = diff;\n\t\telse if (!mddev->reshape_backwards &&\n\t\t\t diff > min_offset_diff)\n\t\t\tmin_offset_diff = diff;\n\t}\n\n\tif ((test_bit(MD_HAS_JOURNAL, &mddev->flags) || journal_dev) &&\n\t    (mddev->bitmap_info.offset || mddev->bitmap_info.file)) {\n\t\tpr_notice(\"md/raid:%s: array cannot have both journal and bitmap\\n\",\n\t\t\t  mdname(mddev));\n\t\treturn -EINVAL;\n\t}\n\n\tif (mddev->reshape_position != MaxSector) {\n\t\t \n\t\tsector_t here_new, here_old;\n\t\tint old_disks;\n\t\tint max_degraded = (mddev->level == 6 ? 2 : 1);\n\t\tint chunk_sectors;\n\t\tint new_data_disks;\n\n\t\tif (journal_dev) {\n\t\t\tpr_warn(\"md/raid:%s: don't support reshape with journal - aborting.\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (mddev->new_level != mddev->level) {\n\t\t\tpr_warn(\"md/raid:%s: unsupported reshape required - aborting.\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\told_disks = mddev->raid_disks - mddev->delta_disks;\n\t\t \n\t\there_new = mddev->reshape_position;\n\t\tchunk_sectors = max(mddev->chunk_sectors, mddev->new_chunk_sectors);\n\t\tnew_data_disks = mddev->raid_disks - max_degraded;\n\t\tif (sector_div(here_new, chunk_sectors * new_data_disks)) {\n\t\t\tpr_warn(\"md/raid:%s: reshape_position not on a stripe boundary\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\treshape_offset = here_new * chunk_sectors;\n\t\t \n\t\there_old = mddev->reshape_position;\n\t\tsector_div(here_old, chunk_sectors * (old_disks-max_degraded));\n\t\t \n\t\tif (mddev->delta_disks == 0) {\n\t\t\t \n\t\t\tif (abs(min_offset_diff) >= mddev->chunk_sectors &&\n\t\t\t    abs(min_offset_diff) >= mddev->new_chunk_sectors)\n\t\t\t\t ;\n\t\t\telse if (mddev->ro == 0) {\n\t\t\t\tpr_warn(\"md/raid:%s: in-place reshape must be started in read-only mode - aborting\\n\",\n\t\t\t\t\tmdname(mddev));\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t} else if (mddev->reshape_backwards\n\t\t    ? (here_new * chunk_sectors + min_offset_diff <=\n\t\t       here_old * chunk_sectors)\n\t\t    : (here_new * chunk_sectors >=\n\t\t       here_old * chunk_sectors + (-min_offset_diff))) {\n\t\t\t \n\t\t\tpr_warn(\"md/raid:%s: reshape_position too early for auto-recovery - aborting.\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tpr_debug(\"md/raid:%s: reshape will continue\\n\", mdname(mddev));\n\t\t \n\t} else {\n\t\tBUG_ON(mddev->level != mddev->new_level);\n\t\tBUG_ON(mddev->layout != mddev->new_layout);\n\t\tBUG_ON(mddev->chunk_sectors != mddev->new_chunk_sectors);\n\t\tBUG_ON(mddev->delta_disks != 0);\n\t}\n\n\tif (test_bit(MD_HAS_JOURNAL, &mddev->flags) &&\n\t    test_bit(MD_HAS_PPL, &mddev->flags)) {\n\t\tpr_warn(\"md/raid:%s: using journal device and PPL not allowed - disabling PPL\\n\",\n\t\t\tmdname(mddev));\n\t\tclear_bit(MD_HAS_PPL, &mddev->flags);\n\t\tclear_bit(MD_HAS_MULTIPLE_PPLS, &mddev->flags);\n\t}\n\n\tif (mddev->private == NULL)\n\t\tconf = setup_conf(mddev);\n\telse\n\t\tconf = mddev->private;\n\n\tif (IS_ERR(conf))\n\t\treturn PTR_ERR(conf);\n\n\tif (test_bit(MD_HAS_JOURNAL, &mddev->flags)) {\n\t\tif (!journal_dev) {\n\t\t\tpr_warn(\"md/raid:%s: journal disk is missing, force array readonly\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\tmddev->ro = 1;\n\t\t\tset_disk_ro(mddev->gendisk, 1);\n\t\t} else if (mddev->recovery_cp == MaxSector)\n\t\t\tset_bit(MD_JOURNAL_CLEAN, &mddev->flags);\n\t}\n\n\tconf->min_offset_diff = min_offset_diff;\n\trcu_assign_pointer(mddev->thread, conf->thread);\n\trcu_assign_pointer(conf->thread, NULL);\n\tmddev->private = conf;\n\n\tfor (i = 0; i < conf->raid_disks && conf->previous_raid_disks;\n\t     i++) {\n\t\trdev = rdev_mdlock_deref(mddev, conf->disks[i].rdev);\n\t\tif (!rdev && conf->disks[i].replacement) {\n\t\t\t \n\t\t\trdev = rdev_mdlock_deref(mddev,\n\t\t\t\t\t\t conf->disks[i].replacement);\n\t\t\tconf->disks[i].replacement = NULL;\n\t\t\tclear_bit(Replacement, &rdev->flags);\n\t\t\trcu_assign_pointer(conf->disks[i].rdev, rdev);\n\t\t}\n\t\tif (!rdev)\n\t\t\tcontinue;\n\t\tif (rcu_access_pointer(conf->disks[i].replacement) &&\n\t\t    conf->reshape_progress != MaxSector) {\n\t\t\t \n\t\t\tpr_warn(\"md: cannot handle concurrent replacement and reshape.\\n\");\n\t\t\tgoto abort;\n\t\t}\n\t\tif (test_bit(In_sync, &rdev->flags))\n\t\t\tcontinue;\n\t\t \n\t\t \n\t\tif (mddev->major_version == 0 &&\n\t\t    mddev->minor_version > 90)\n\t\t\trdev->recovery_offset = reshape_offset;\n\n\t\tif (rdev->recovery_offset < reshape_offset) {\n\t\t\t \n\t\t\tif (!only_parity(rdev->raid_disk,\n\t\t\t\t\t conf->algorithm,\n\t\t\t\t\t conf->raid_disks,\n\t\t\t\t\t conf->max_degraded))\n\t\t\t\tcontinue;\n\t\t}\n\t\tif (!only_parity(rdev->raid_disk,\n\t\t\t\t conf->prev_algo,\n\t\t\t\t conf->previous_raid_disks,\n\t\t\t\t conf->max_degraded))\n\t\t\tcontinue;\n\t\tdirty_parity_disks++;\n\t}\n\n\t \n\tmddev->degraded = raid5_calc_degraded(conf);\n\n\tif (has_failed(conf)) {\n\t\tpr_crit(\"md/raid:%s: not enough operational devices (%d/%d failed)\\n\",\n\t\t\tmdname(mddev), mddev->degraded, conf->raid_disks);\n\t\tgoto abort;\n\t}\n\n\t \n\tmddev->dev_sectors &= ~((sector_t)mddev->chunk_sectors - 1);\n\tmddev->resync_max_sectors = mddev->dev_sectors;\n\n\tif (mddev->degraded > dirty_parity_disks &&\n\t    mddev->recovery_cp != MaxSector) {\n\t\tif (test_bit(MD_HAS_PPL, &mddev->flags))\n\t\t\tpr_crit(\"md/raid:%s: starting dirty degraded array with PPL.\\n\",\n\t\t\t\tmdname(mddev));\n\t\telse if (mddev->ok_start_degraded)\n\t\t\tpr_crit(\"md/raid:%s: starting dirty degraded array - data corruption possible.\\n\",\n\t\t\t\tmdname(mddev));\n\t\telse {\n\t\t\tpr_crit(\"md/raid:%s: cannot start dirty degraded array.\\n\",\n\t\t\t\tmdname(mddev));\n\t\t\tgoto abort;\n\t\t}\n\t}\n\n\tpr_info(\"md/raid:%s: raid level %d active with %d out of %d devices, algorithm %d\\n\",\n\t\tmdname(mddev), conf->level,\n\t\tmddev->raid_disks-mddev->degraded, mddev->raid_disks,\n\t\tmddev->new_layout);\n\n\tprint_raid5_conf(conf);\n\n\tif (conf->reshape_progress != MaxSector) {\n\t\tconf->reshape_safe = conf->reshape_progress;\n\t\tatomic_set(&conf->reshape_stripes, 0);\n\t\tclear_bit(MD_RECOVERY_SYNC, &mddev->recovery);\n\t\tclear_bit(MD_RECOVERY_CHECK, &mddev->recovery);\n\t\tset_bit(MD_RECOVERY_RESHAPE, &mddev->recovery);\n\t\tset_bit(MD_RECOVERY_RUNNING, &mddev->recovery);\n\t\trcu_assign_pointer(mddev->sync_thread,\n\t\t\tmd_register_thread(md_do_sync, mddev, \"reshape\"));\n\t\tif (!mddev->sync_thread)\n\t\t\tgoto abort;\n\t}\n\n\t \n\tif (mddev->to_remove == &raid5_attrs_group)\n\t\tmddev->to_remove = NULL;\n\telse if (mddev->kobj.sd &&\n\t    sysfs_create_group(&mddev->kobj, &raid5_attrs_group))\n\t\tpr_warn(\"raid5: failed to create sysfs attributes for %s\\n\",\n\t\t\tmdname(mddev));\n\tmd_set_array_sectors(mddev, raid5_size(mddev, 0, 0));\n\n\tif (mddev->queue) {\n\t\tint chunk_size;\n\t\t \n\t\tint data_disks = conf->previous_raid_disks - conf->max_degraded;\n\t\tint stripe = data_disks *\n\t\t\t((mddev->chunk_sectors << 9) / PAGE_SIZE);\n\n\t\tchunk_size = mddev->chunk_sectors << 9;\n\t\tblk_queue_io_min(mddev->queue, chunk_size);\n\t\traid5_set_io_opt(conf);\n\t\tmddev->queue->limits.raid_partial_stripes_expensive = 1;\n\t\t \n\t\tstripe = stripe * PAGE_SIZE;\n\t\tstripe = roundup_pow_of_two(stripe);\n\t\tmddev->queue->limits.discard_granularity = stripe;\n\n\t\tblk_queue_max_write_zeroes_sectors(mddev->queue, 0);\n\n\t\trdev_for_each(rdev, mddev) {\n\t\t\tdisk_stack_limits(mddev->gendisk, rdev->bdev,\n\t\t\t\t\t  rdev->data_offset << 9);\n\t\t\tdisk_stack_limits(mddev->gendisk, rdev->bdev,\n\t\t\t\t\t  rdev->new_data_offset << 9);\n\t\t}\n\n\t\t \n\t\tif (!devices_handle_discard_safely ||\n\t\t    mddev->queue->limits.max_discard_sectors < (stripe >> 9) ||\n\t\t    mddev->queue->limits.discard_granularity < stripe)\n\t\t\tblk_queue_max_discard_sectors(mddev->queue, 0);\n\n\t\t \n\t\tblk_queue_max_hw_sectors(mddev->queue,\n\t\t\tRAID5_MAX_REQ_STRIPES << RAID5_STRIPE_SHIFT(conf));\n\n\t\t \n\t\tblk_queue_max_segments(mddev->queue, USHRT_MAX);\n\t}\n\n\tif (log_init(conf, journal_dev, raid5_has_ppl(conf)))\n\t\tgoto abort;\n\n\treturn 0;\nabort:\n\tmd_unregister_thread(mddev, &mddev->thread);\n\tprint_raid5_conf(conf);\n\tfree_conf(conf);\n\tmddev->private = NULL;\n\tpr_warn(\"md/raid:%s: failed to run raid set.\\n\", mdname(mddev));\n\treturn -EIO;\n}\n\nstatic void raid5_free(struct mddev *mddev, void *priv)\n{\n\tstruct r5conf *conf = priv;\n\n\tfree_conf(conf);\n\tmddev->to_remove = &raid5_attrs_group;\n}\n\nstatic void raid5_status(struct seq_file *seq, struct mddev *mddev)\n{\n\tstruct r5conf *conf = mddev->private;\n\tint i;\n\n\tseq_printf(seq, \" level %d, %dk chunk, algorithm %d\", mddev->level,\n\t\tconf->chunk_sectors / 2, mddev->layout);\n\tseq_printf (seq, \" [%d/%d] [\", conf->raid_disks, conf->raid_disks - mddev->degraded);\n\trcu_read_lock();\n\tfor (i = 0; i < conf->raid_disks; i++) {\n\t\tstruct md_rdev *rdev = rcu_dereference(conf->disks[i].rdev);\n\t\tseq_printf (seq, \"%s\", rdev && test_bit(In_sync, &rdev->flags) ? \"U\" : \"_\");\n\t}\n\trcu_read_unlock();\n\tseq_printf (seq, \"]\");\n}\n\nstatic void print_raid5_conf (struct r5conf *conf)\n{\n\tstruct md_rdev *rdev;\n\tint i;\n\n\tpr_debug(\"RAID conf printout:\\n\");\n\tif (!conf) {\n\t\tpr_debug(\"(conf==NULL)\\n\");\n\t\treturn;\n\t}\n\tpr_debug(\" --- level:%d rd:%d wd:%d\\n\", conf->level,\n\t       conf->raid_disks,\n\t       conf->raid_disks - conf->mddev->degraded);\n\n\trcu_read_lock();\n\tfor (i = 0; i < conf->raid_disks; i++) {\n\t\trdev = rcu_dereference(conf->disks[i].rdev);\n\t\tif (rdev)\n\t\t\tpr_debug(\" disk %d, o:%d, dev:%pg\\n\",\n\t\t\t       i, !test_bit(Faulty, &rdev->flags),\n\t\t\t       rdev->bdev);\n\t}\n\trcu_read_unlock();\n}\n\nstatic int raid5_spare_active(struct mddev *mddev)\n{\n\tint i;\n\tstruct r5conf *conf = mddev->private;\n\tstruct md_rdev *rdev, *replacement;\n\tint count = 0;\n\tunsigned long flags;\n\n\tfor (i = 0; i < conf->raid_disks; i++) {\n\t\trdev = rdev_mdlock_deref(mddev, conf->disks[i].rdev);\n\t\treplacement = rdev_mdlock_deref(mddev,\n\t\t\t\t\t\tconf->disks[i].replacement);\n\t\tif (replacement\n\t\t    && replacement->recovery_offset == MaxSector\n\t\t    && !test_bit(Faulty, &replacement->flags)\n\t\t    && !test_and_set_bit(In_sync, &replacement->flags)) {\n\t\t\t \n\t\t\tif (!rdev\n\t\t\t    || !test_and_clear_bit(In_sync, &rdev->flags))\n\t\t\t\tcount++;\n\t\t\tif (rdev) {\n\t\t\t\t \n\t\t\t\tset_bit(Faulty, &rdev->flags);\n\t\t\t\tsysfs_notify_dirent_safe(\n\t\t\t\t\trdev->sysfs_state);\n\t\t\t}\n\t\t\tsysfs_notify_dirent_safe(replacement->sysfs_state);\n\t\t} else if (rdev\n\t\t    && rdev->recovery_offset == MaxSector\n\t\t    && !test_bit(Faulty, &rdev->flags)\n\t\t    && !test_and_set_bit(In_sync, &rdev->flags)) {\n\t\t\tcount++;\n\t\t\tsysfs_notify_dirent_safe(rdev->sysfs_state);\n\t\t}\n\t}\n\tspin_lock_irqsave(&conf->device_lock, flags);\n\tmddev->degraded = raid5_calc_degraded(conf);\n\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\tprint_raid5_conf(conf);\n\treturn count;\n}\n\nstatic int raid5_remove_disk(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tstruct r5conf *conf = mddev->private;\n\tint err = 0;\n\tint number = rdev->raid_disk;\n\tstruct md_rdev __rcu **rdevp;\n\tstruct disk_info *p;\n\tstruct md_rdev *tmp;\n\n\tprint_raid5_conf(conf);\n\tif (test_bit(Journal, &rdev->flags) && conf->log) {\n\t\t \n\t\tif (atomic_read(&conf->active_stripes) ||\n\t\t    atomic_read(&conf->r5c_cached_full_stripes) ||\n\t\t    atomic_read(&conf->r5c_cached_partial_stripes)) {\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tlog_exit(conf);\n\t\treturn 0;\n\t}\n\tif (unlikely(number >= conf->pool_size))\n\t\treturn 0;\n\tp = conf->disks + number;\n\tif (rdev == rcu_access_pointer(p->rdev))\n\t\trdevp = &p->rdev;\n\telse if (rdev == rcu_access_pointer(p->replacement))\n\t\trdevp = &p->replacement;\n\telse\n\t\treturn 0;\n\n\tif (number >= conf->raid_disks &&\n\t    conf->reshape_progress == MaxSector)\n\t\tclear_bit(In_sync, &rdev->flags);\n\n\tif (test_bit(In_sync, &rdev->flags) ||\n\t    atomic_read(&rdev->nr_pending)) {\n\t\terr = -EBUSY;\n\t\tgoto abort;\n\t}\n\t \n\tif (!test_bit(Faulty, &rdev->flags) &&\n\t    mddev->recovery_disabled != conf->recovery_disabled &&\n\t    !has_failed(conf) &&\n\t    (!rcu_access_pointer(p->replacement) ||\n\t     rcu_access_pointer(p->replacement) == rdev) &&\n\t    number < conf->raid_disks) {\n\t\terr = -EBUSY;\n\t\tgoto abort;\n\t}\n\t*rdevp = NULL;\n\tif (!test_bit(RemoveSynchronized, &rdev->flags)) {\n\t\tlockdep_assert_held(&mddev->reconfig_mutex);\n\t\tsynchronize_rcu();\n\t\tif (atomic_read(&rdev->nr_pending)) {\n\t\t\t \n\t\t\terr = -EBUSY;\n\t\t\trcu_assign_pointer(*rdevp, rdev);\n\t\t}\n\t}\n\tif (!err) {\n\t\terr = log_modify(conf, rdev, false);\n\t\tif (err)\n\t\t\tgoto abort;\n\t}\n\n\ttmp = rcu_access_pointer(p->replacement);\n\tif (tmp) {\n\t\t \n\t\trcu_assign_pointer(p->rdev, tmp);\n\t\tclear_bit(Replacement, &tmp->flags);\n\t\tsmp_mb();  \n\t\trcu_assign_pointer(p->replacement, NULL);\n\n\t\tif (!err)\n\t\t\terr = log_modify(conf, tmp, true);\n\t}\n\n\tclear_bit(WantReplacement, &rdev->flags);\nabort:\n\n\tprint_raid5_conf(conf);\n\treturn err;\n}\n\nstatic int raid5_add_disk(struct mddev *mddev, struct md_rdev *rdev)\n{\n\tstruct r5conf *conf = mddev->private;\n\tint ret, err = -EEXIST;\n\tint disk;\n\tstruct disk_info *p;\n\tstruct md_rdev *tmp;\n\tint first = 0;\n\tint last = conf->raid_disks - 1;\n\n\tif (test_bit(Journal, &rdev->flags)) {\n\t\tif (conf->log)\n\t\t\treturn -EBUSY;\n\n\t\trdev->raid_disk = 0;\n\t\t \n\t\tret = log_init(conf, rdev, false);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = r5l_start(conf->log);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\treturn 0;\n\t}\n\tif (mddev->recovery_disabled == conf->recovery_disabled)\n\t\treturn -EBUSY;\n\n\tif (rdev->saved_raid_disk < 0 && has_failed(conf))\n\t\t \n\t\treturn -EINVAL;\n\n\tif (rdev->raid_disk >= 0)\n\t\tfirst = last = rdev->raid_disk;\n\n\t \n\tif (rdev->saved_raid_disk >= first &&\n\t    rdev->saved_raid_disk <= last &&\n\t    conf->disks[rdev->saved_raid_disk].rdev == NULL)\n\t\tfirst = rdev->saved_raid_disk;\n\n\tfor (disk = first; disk <= last; disk++) {\n\t\tp = conf->disks + disk;\n\t\tif (p->rdev == NULL) {\n\t\t\tclear_bit(In_sync, &rdev->flags);\n\t\t\trdev->raid_disk = disk;\n\t\t\tif (rdev->saved_raid_disk != disk)\n\t\t\t\tconf->fullsync = 1;\n\t\t\trcu_assign_pointer(p->rdev, rdev);\n\n\t\t\terr = log_modify(conf, rdev, true);\n\n\t\t\tgoto out;\n\t\t}\n\t}\n\tfor (disk = first; disk <= last; disk++) {\n\t\tp = conf->disks + disk;\n\t\ttmp = rdev_mdlock_deref(mddev, p->rdev);\n\t\tif (test_bit(WantReplacement, &tmp->flags) &&\n\t\t    mddev->reshape_position == MaxSector &&\n\t\t    p->replacement == NULL) {\n\t\t\tclear_bit(In_sync, &rdev->flags);\n\t\t\tset_bit(Replacement, &rdev->flags);\n\t\t\trdev->raid_disk = disk;\n\t\t\terr = 0;\n\t\t\tconf->fullsync = 1;\n\t\t\trcu_assign_pointer(p->replacement, rdev);\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tprint_raid5_conf(conf);\n\treturn err;\n}\n\nstatic int raid5_resize(struct mddev *mddev, sector_t sectors)\n{\n\t \n\tsector_t newsize;\n\tstruct r5conf *conf = mddev->private;\n\n\tif (raid5_has_log(conf) || raid5_has_ppl(conf))\n\t\treturn -EINVAL;\n\tsectors &= ~((sector_t)conf->chunk_sectors - 1);\n\tnewsize = raid5_size(mddev, sectors, mddev->raid_disks);\n\tif (mddev->external_size &&\n\t    mddev->array_sectors > newsize)\n\t\treturn -EINVAL;\n\tif (mddev->bitmap) {\n\t\tint ret = md_bitmap_resize(mddev->bitmap, sectors, 0, 0);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\tmd_set_array_sectors(mddev, newsize);\n\tif (sectors > mddev->dev_sectors &&\n\t    mddev->recovery_cp > mddev->dev_sectors) {\n\t\tmddev->recovery_cp = mddev->dev_sectors;\n\t\tset_bit(MD_RECOVERY_NEEDED, &mddev->recovery);\n\t}\n\tmddev->dev_sectors = sectors;\n\tmddev->resync_max_sectors = sectors;\n\treturn 0;\n}\n\nstatic int check_stripe_cache(struct mddev *mddev)\n{\n\t \n\tstruct r5conf *conf = mddev->private;\n\tif (((mddev->chunk_sectors << 9) / RAID5_STRIPE_SIZE(conf)) * 4\n\t    > conf->min_nr_stripes ||\n\t    ((mddev->new_chunk_sectors << 9) / RAID5_STRIPE_SIZE(conf)) * 4\n\t    > conf->min_nr_stripes) {\n\t\tpr_warn(\"md/raid:%s: reshape: not enough stripes.  Needed %lu\\n\",\n\t\t\tmdname(mddev),\n\t\t\t((max(mddev->chunk_sectors, mddev->new_chunk_sectors) << 9)\n\t\t\t / RAID5_STRIPE_SIZE(conf))*4);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n\nstatic int check_reshape(struct mddev *mddev)\n{\n\tstruct r5conf *conf = mddev->private;\n\n\tif (raid5_has_log(conf) || raid5_has_ppl(conf))\n\t\treturn -EINVAL;\n\tif (mddev->delta_disks == 0 &&\n\t    mddev->new_layout == mddev->layout &&\n\t    mddev->new_chunk_sectors == mddev->chunk_sectors)\n\t\treturn 0;  \n\tif (has_failed(conf))\n\t\treturn -EINVAL;\n\tif (mddev->delta_disks < 0 && mddev->reshape_position == MaxSector) {\n\t\t \n\t\tint min = 2;\n\t\tif (mddev->level == 6)\n\t\t\tmin = 4;\n\t\tif (mddev->raid_disks + mddev->delta_disks < min)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (!check_stripe_cache(mddev))\n\t\treturn -ENOSPC;\n\n\tif (mddev->new_chunk_sectors > mddev->chunk_sectors ||\n\t    mddev->delta_disks > 0)\n\t\tif (resize_chunks(conf,\n\t\t\t\t  conf->previous_raid_disks\n\t\t\t\t  + max(0, mddev->delta_disks),\n\t\t\t\t  max(mddev->new_chunk_sectors,\n\t\t\t\t      mddev->chunk_sectors)\n\t\t\t    ) < 0)\n\t\t\treturn -ENOMEM;\n\n\tif (conf->previous_raid_disks + mddev->delta_disks <= conf->pool_size)\n\t\treturn 0;  \n\treturn resize_stripes(conf, (conf->previous_raid_disks\n\t\t\t\t     + mddev->delta_disks));\n}\n\nstatic int raid5_start_reshape(struct mddev *mddev)\n{\n\tstruct r5conf *conf = mddev->private;\n\tstruct md_rdev *rdev;\n\tint spares = 0;\n\tint i;\n\tunsigned long flags;\n\n\tif (test_bit(MD_RECOVERY_RUNNING, &mddev->recovery))\n\t\treturn -EBUSY;\n\n\tif (!check_stripe_cache(mddev))\n\t\treturn -ENOSPC;\n\n\tif (has_failed(conf))\n\t\treturn -EINVAL;\n\n\t \n\tif (mddev->recovery_cp < MaxSector)\n\t\treturn -EBUSY;\n\tfor (i = 0; i < conf->raid_disks; i++)\n\t\tif (rdev_mdlock_deref(mddev, conf->disks[i].replacement))\n\t\t\treturn -EBUSY;\n\n\trdev_for_each(rdev, mddev) {\n\t\tif (!test_bit(In_sync, &rdev->flags)\n\t\t    && !test_bit(Faulty, &rdev->flags))\n\t\t\tspares++;\n\t}\n\n\tif (spares - mddev->degraded < mddev->delta_disks - conf->max_degraded)\n\t\t \n\t\treturn -EINVAL;\n\n\t \n\tif (raid5_size(mddev, 0, conf->raid_disks + mddev->delta_disks)\n\t    < mddev->array_sectors) {\n\t\tpr_warn(\"md/raid:%s: array size must be reduced before number of disks\\n\",\n\t\t\tmdname(mddev));\n\t\treturn -EINVAL;\n\t}\n\n\tatomic_set(&conf->reshape_stripes, 0);\n\tspin_lock_irq(&conf->device_lock);\n\twrite_seqcount_begin(&conf->gen_lock);\n\tconf->previous_raid_disks = conf->raid_disks;\n\tconf->raid_disks += mddev->delta_disks;\n\tconf->prev_chunk_sectors = conf->chunk_sectors;\n\tconf->chunk_sectors = mddev->new_chunk_sectors;\n\tconf->prev_algo = conf->algorithm;\n\tconf->algorithm = mddev->new_layout;\n\tconf->generation++;\n\t \n\tsmp_mb();\n\tif (mddev->reshape_backwards)\n\t\tconf->reshape_progress = raid5_size(mddev, 0, 0);\n\telse\n\t\tconf->reshape_progress = 0;\n\tconf->reshape_safe = conf->reshape_progress;\n\twrite_seqcount_end(&conf->gen_lock);\n\tspin_unlock_irq(&conf->device_lock);\n\n\t \n\tmddev_suspend(mddev);\n\tmddev_resume(mddev);\n\n\t \n\tif (mddev->delta_disks >= 0) {\n\t\trdev_for_each(rdev, mddev)\n\t\t\tif (rdev->raid_disk < 0 &&\n\t\t\t    !test_bit(Faulty, &rdev->flags)) {\n\t\t\t\tif (raid5_add_disk(mddev, rdev) == 0) {\n\t\t\t\t\tif (rdev->raid_disk\n\t\t\t\t\t    >= conf->previous_raid_disks)\n\t\t\t\t\t\tset_bit(In_sync, &rdev->flags);\n\t\t\t\t\telse\n\t\t\t\t\t\trdev->recovery_offset = 0;\n\n\t\t\t\t\t \n\t\t\t\t\tsysfs_link_rdev(mddev, rdev);\n\t\t\t\t}\n\t\t\t} else if (rdev->raid_disk >= conf->previous_raid_disks\n\t\t\t\t   && !test_bit(Faulty, &rdev->flags)) {\n\t\t\t\t \n\t\t\t\tset_bit(In_sync, &rdev->flags);\n\t\t\t}\n\n\t\t \n\t\tspin_lock_irqsave(&conf->device_lock, flags);\n\t\tmddev->degraded = raid5_calc_degraded(conf);\n\t\tspin_unlock_irqrestore(&conf->device_lock, flags);\n\t}\n\tmddev->raid_disks = conf->raid_disks;\n\tmddev->reshape_position = conf->reshape_progress;\n\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\n\tclear_bit(MD_RECOVERY_SYNC, &mddev->recovery);\n\tclear_bit(MD_RECOVERY_CHECK, &mddev->recovery);\n\tclear_bit(MD_RECOVERY_DONE, &mddev->recovery);\n\tset_bit(MD_RECOVERY_RESHAPE, &mddev->recovery);\n\tset_bit(MD_RECOVERY_RUNNING, &mddev->recovery);\n\trcu_assign_pointer(mddev->sync_thread,\n\t\t\t   md_register_thread(md_do_sync, mddev, \"reshape\"));\n\tif (!mddev->sync_thread) {\n\t\tmddev->recovery = 0;\n\t\tspin_lock_irq(&conf->device_lock);\n\t\twrite_seqcount_begin(&conf->gen_lock);\n\t\tmddev->raid_disks = conf->raid_disks = conf->previous_raid_disks;\n\t\tmddev->new_chunk_sectors =\n\t\t\tconf->chunk_sectors = conf->prev_chunk_sectors;\n\t\tmddev->new_layout = conf->algorithm = conf->prev_algo;\n\t\trdev_for_each(rdev, mddev)\n\t\t\trdev->new_data_offset = rdev->data_offset;\n\t\tsmp_wmb();\n\t\tconf->generation --;\n\t\tconf->reshape_progress = MaxSector;\n\t\tmddev->reshape_position = MaxSector;\n\t\twrite_seqcount_end(&conf->gen_lock);\n\t\tspin_unlock_irq(&conf->device_lock);\n\t\treturn -EAGAIN;\n\t}\n\tconf->reshape_checkpoint = jiffies;\n\tmd_wakeup_thread(mddev->sync_thread);\n\tmd_new_event();\n\treturn 0;\n}\n\n \nstatic void end_reshape(struct r5conf *conf)\n{\n\n\tif (!test_bit(MD_RECOVERY_INTR, &conf->mddev->recovery)) {\n\t\tstruct md_rdev *rdev;\n\n\t\tspin_lock_irq(&conf->device_lock);\n\t\tconf->previous_raid_disks = conf->raid_disks;\n\t\tmd_finish_reshape(conf->mddev);\n\t\tsmp_wmb();\n\t\tconf->reshape_progress = MaxSector;\n\t\tconf->mddev->reshape_position = MaxSector;\n\t\trdev_for_each(rdev, conf->mddev)\n\t\t\tif (rdev->raid_disk >= 0 &&\n\t\t\t    !test_bit(Journal, &rdev->flags) &&\n\t\t\t    !test_bit(In_sync, &rdev->flags))\n\t\t\t\trdev->recovery_offset = MaxSector;\n\t\tspin_unlock_irq(&conf->device_lock);\n\t\twake_up(&conf->wait_for_overlap);\n\n\t\tif (conf->mddev->queue)\n\t\t\traid5_set_io_opt(conf);\n\t}\n}\n\n \nstatic void raid5_finish_reshape(struct mddev *mddev)\n{\n\tstruct r5conf *conf = mddev->private;\n\tstruct md_rdev *rdev;\n\n\tif (!test_bit(MD_RECOVERY_INTR, &mddev->recovery)) {\n\n\t\tif (mddev->delta_disks <= 0) {\n\t\t\tint d;\n\t\t\tspin_lock_irq(&conf->device_lock);\n\t\t\tmddev->degraded = raid5_calc_degraded(conf);\n\t\t\tspin_unlock_irq(&conf->device_lock);\n\t\t\tfor (d = conf->raid_disks ;\n\t\t\t     d < conf->raid_disks - mddev->delta_disks;\n\t\t\t     d++) {\n\t\t\t\trdev = rdev_mdlock_deref(mddev,\n\t\t\t\t\t\t\t conf->disks[d].rdev);\n\t\t\t\tif (rdev)\n\t\t\t\t\tclear_bit(In_sync, &rdev->flags);\n\t\t\t\trdev = rdev_mdlock_deref(mddev,\n\t\t\t\t\t\tconf->disks[d].replacement);\n\t\t\t\tif (rdev)\n\t\t\t\t\tclear_bit(In_sync, &rdev->flags);\n\t\t\t}\n\t\t}\n\t\tmddev->layout = conf->algorithm;\n\t\tmddev->chunk_sectors = conf->chunk_sectors;\n\t\tmddev->reshape_position = MaxSector;\n\t\tmddev->delta_disks = 0;\n\t\tmddev->reshape_backwards = 0;\n\t}\n}\n\nstatic void raid5_quiesce(struct mddev *mddev, int quiesce)\n{\n\tstruct r5conf *conf = mddev->private;\n\n\tif (quiesce) {\n\t\t \n\t\tlock_all_device_hash_locks_irq(conf);\n\t\t \n\t\tr5c_flush_cache(conf, INT_MAX);\n\t\t \n\t\tsmp_store_release(&conf->quiesce, 2);\n\t\twait_event_cmd(conf->wait_for_quiescent,\n\t\t\t\t    atomic_read(&conf->active_stripes) == 0 &&\n\t\t\t\t    atomic_read(&conf->active_aligned_reads) == 0,\n\t\t\t\t    unlock_all_device_hash_locks_irq(conf),\n\t\t\t\t    lock_all_device_hash_locks_irq(conf));\n\t\tconf->quiesce = 1;\n\t\tunlock_all_device_hash_locks_irq(conf);\n\t\t \n\t\twake_up(&conf->wait_for_overlap);\n\t} else {\n\t\t \n\t\tlock_all_device_hash_locks_irq(conf);\n\t\tconf->quiesce = 0;\n\t\twake_up(&conf->wait_for_quiescent);\n\t\twake_up(&conf->wait_for_overlap);\n\t\tunlock_all_device_hash_locks_irq(conf);\n\t}\n\tlog_quiesce(conf, quiesce);\n}\n\nstatic void *raid45_takeover_raid0(struct mddev *mddev, int level)\n{\n\tstruct r0conf *raid0_conf = mddev->private;\n\tsector_t sectors;\n\n\t \n\tif (raid0_conf->nr_strip_zones > 1) {\n\t\tpr_warn(\"md/raid:%s: cannot takeover raid0 with more than one zone.\\n\",\n\t\t\tmdname(mddev));\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tsectors = raid0_conf->strip_zone[0].zone_end;\n\tsector_div(sectors, raid0_conf->strip_zone[0].nb_dev);\n\tmddev->dev_sectors = sectors;\n\tmddev->new_level = level;\n\tmddev->new_layout = ALGORITHM_PARITY_N;\n\tmddev->new_chunk_sectors = mddev->chunk_sectors;\n\tmddev->raid_disks += 1;\n\tmddev->delta_disks = 1;\n\t \n\tmddev->recovery_cp = MaxSector;\n\n\treturn setup_conf(mddev);\n}\n\nstatic void *raid5_takeover_raid1(struct mddev *mddev)\n{\n\tint chunksect;\n\tvoid *ret;\n\n\tif (mddev->raid_disks != 2 ||\n\t    mddev->degraded > 1)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\n\tchunksect = 64*2;  \n\n\t \n\twhile (chunksect && (mddev->array_sectors & (chunksect-1)))\n\t\tchunksect >>= 1;\n\n\tif ((chunksect<<9) < RAID5_STRIPE_SIZE((struct r5conf *)mddev->private))\n\t\t \n\t\treturn ERR_PTR(-EINVAL);\n\n\tmddev->new_level = 5;\n\tmddev->new_layout = ALGORITHM_LEFT_SYMMETRIC;\n\tmddev->new_chunk_sectors = chunksect;\n\n\tret = setup_conf(mddev);\n\tif (!IS_ERR(ret))\n\t\tmddev_clear_unsupported_flags(mddev,\n\t\t\tUNSUPPORTED_MDDEV_FLAGS);\n\treturn ret;\n}\n\nstatic void *raid5_takeover_raid6(struct mddev *mddev)\n{\n\tint new_layout;\n\n\tswitch (mddev->layout) {\n\tcase ALGORITHM_LEFT_ASYMMETRIC_6:\n\t\tnew_layout = ALGORITHM_LEFT_ASYMMETRIC;\n\t\tbreak;\n\tcase ALGORITHM_RIGHT_ASYMMETRIC_6:\n\t\tnew_layout = ALGORITHM_RIGHT_ASYMMETRIC;\n\t\tbreak;\n\tcase ALGORITHM_LEFT_SYMMETRIC_6:\n\t\tnew_layout = ALGORITHM_LEFT_SYMMETRIC;\n\t\tbreak;\n\tcase ALGORITHM_RIGHT_SYMMETRIC_6:\n\t\tnew_layout = ALGORITHM_RIGHT_SYMMETRIC;\n\t\tbreak;\n\tcase ALGORITHM_PARITY_0_6:\n\t\tnew_layout = ALGORITHM_PARITY_0;\n\t\tbreak;\n\tcase ALGORITHM_PARITY_N:\n\t\tnew_layout = ALGORITHM_PARITY_N;\n\t\tbreak;\n\tdefault:\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tmddev->new_level = 5;\n\tmddev->new_layout = new_layout;\n\tmddev->delta_disks = -1;\n\tmddev->raid_disks -= 1;\n\treturn setup_conf(mddev);\n}\n\nstatic int raid5_check_reshape(struct mddev *mddev)\n{\n\t \n\tstruct r5conf *conf = mddev->private;\n\tint new_chunk = mddev->new_chunk_sectors;\n\n\tif (mddev->new_layout >= 0 && !algorithm_valid_raid5(mddev->new_layout))\n\t\treturn -EINVAL;\n\tif (new_chunk > 0) {\n\t\tif (!is_power_of_2(new_chunk))\n\t\t\treturn -EINVAL;\n\t\tif (new_chunk < (PAGE_SIZE>>9))\n\t\t\treturn -EINVAL;\n\t\tif (mddev->array_sectors & (new_chunk-1))\n\t\t\t \n\t\t\treturn -EINVAL;\n\t}\n\n\t \n\n\tif (mddev->raid_disks == 2) {\n\t\t \n\t\tif (mddev->new_layout >= 0) {\n\t\t\tconf->algorithm = mddev->new_layout;\n\t\t\tmddev->layout = mddev->new_layout;\n\t\t}\n\t\tif (new_chunk > 0) {\n\t\t\tconf->chunk_sectors = new_chunk ;\n\t\t\tmddev->chunk_sectors = new_chunk;\n\t\t}\n\t\tset_bit(MD_SB_CHANGE_DEVS, &mddev->sb_flags);\n\t\tmd_wakeup_thread(mddev->thread);\n\t}\n\treturn check_reshape(mddev);\n}\n\nstatic int raid6_check_reshape(struct mddev *mddev)\n{\n\tint new_chunk = mddev->new_chunk_sectors;\n\n\tif (mddev->new_layout >= 0 && !algorithm_valid_raid6(mddev->new_layout))\n\t\treturn -EINVAL;\n\tif (new_chunk > 0) {\n\t\tif (!is_power_of_2(new_chunk))\n\t\t\treturn -EINVAL;\n\t\tif (new_chunk < (PAGE_SIZE >> 9))\n\t\t\treturn -EINVAL;\n\t\tif (mddev->array_sectors & (new_chunk-1))\n\t\t\t \n\t\t\treturn -EINVAL;\n\t}\n\n\t \n\treturn check_reshape(mddev);\n}\n\nstatic void *raid5_takeover(struct mddev *mddev)\n{\n\t \n\tif (mddev->level == 0)\n\t\treturn raid45_takeover_raid0(mddev, 5);\n\tif (mddev->level == 1)\n\t\treturn raid5_takeover_raid1(mddev);\n\tif (mddev->level == 4) {\n\t\tmddev->new_layout = ALGORITHM_PARITY_N;\n\t\tmddev->new_level = 5;\n\t\treturn setup_conf(mddev);\n\t}\n\tif (mddev->level == 6)\n\t\treturn raid5_takeover_raid6(mddev);\n\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic void *raid4_takeover(struct mddev *mddev)\n{\n\t \n\tif (mddev->level == 0)\n\t\treturn raid45_takeover_raid0(mddev, 4);\n\tif (mddev->level == 5 &&\n\t    mddev->layout == ALGORITHM_PARITY_N) {\n\t\tmddev->new_layout = 0;\n\t\tmddev->new_level = 4;\n\t\treturn setup_conf(mddev);\n\t}\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic struct md_personality raid5_personality;\n\nstatic void *raid6_takeover(struct mddev *mddev)\n{\n\t \n\tint new_layout;\n\n\tif (mddev->pers != &raid5_personality)\n\t\treturn ERR_PTR(-EINVAL);\n\tif (mddev->degraded > 1)\n\t\treturn ERR_PTR(-EINVAL);\n\tif (mddev->raid_disks > 253)\n\t\treturn ERR_PTR(-EINVAL);\n\tif (mddev->raid_disks < 3)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tswitch (mddev->layout) {\n\tcase ALGORITHM_LEFT_ASYMMETRIC:\n\t\tnew_layout = ALGORITHM_LEFT_ASYMMETRIC_6;\n\t\tbreak;\n\tcase ALGORITHM_RIGHT_ASYMMETRIC:\n\t\tnew_layout = ALGORITHM_RIGHT_ASYMMETRIC_6;\n\t\tbreak;\n\tcase ALGORITHM_LEFT_SYMMETRIC:\n\t\tnew_layout = ALGORITHM_LEFT_SYMMETRIC_6;\n\t\tbreak;\n\tcase ALGORITHM_RIGHT_SYMMETRIC:\n\t\tnew_layout = ALGORITHM_RIGHT_SYMMETRIC_6;\n\t\tbreak;\n\tcase ALGORITHM_PARITY_0:\n\t\tnew_layout = ALGORITHM_PARITY_0_6;\n\t\tbreak;\n\tcase ALGORITHM_PARITY_N:\n\t\tnew_layout = ALGORITHM_PARITY_N;\n\t\tbreak;\n\tdefault:\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\tmddev->new_level = 6;\n\tmddev->new_layout = new_layout;\n\tmddev->delta_disks = 1;\n\tmddev->raid_disks += 1;\n\treturn setup_conf(mddev);\n}\n\nstatic int raid5_change_consistency_policy(struct mddev *mddev, const char *buf)\n{\n\tstruct r5conf *conf;\n\tint err;\n\n\terr = mddev_lock(mddev);\n\tif (err)\n\t\treturn err;\n\tconf = mddev->private;\n\tif (!conf) {\n\t\tmddev_unlock(mddev);\n\t\treturn -ENODEV;\n\t}\n\n\tif (strncmp(buf, \"ppl\", 3) == 0) {\n\t\t \n\t\tif (!raid5_has_ppl(conf) && conf->level == 5) {\n\t\t\terr = log_init(conf, NULL, true);\n\t\t\tif (!err) {\n\t\t\t\terr = resize_stripes(conf, conf->pool_size);\n\t\t\t\tif (err) {\n\t\t\t\t\tmddev_suspend(mddev);\n\t\t\t\t\tlog_exit(conf);\n\t\t\t\t\tmddev_resume(mddev);\n\t\t\t\t}\n\t\t\t}\n\t\t} else\n\t\t\terr = -EINVAL;\n\t} else if (strncmp(buf, \"resync\", 6) == 0) {\n\t\tif (raid5_has_ppl(conf)) {\n\t\t\tmddev_suspend(mddev);\n\t\t\tlog_exit(conf);\n\t\t\tmddev_resume(mddev);\n\t\t\terr = resize_stripes(conf, conf->pool_size);\n\t\t} else if (test_bit(MD_HAS_JOURNAL, &conf->mddev->flags) &&\n\t\t\t   r5l_log_disk_error(conf)) {\n\t\t\tbool journal_dev_exists = false;\n\t\t\tstruct md_rdev *rdev;\n\n\t\t\trdev_for_each(rdev, mddev)\n\t\t\t\tif (test_bit(Journal, &rdev->flags)) {\n\t\t\t\t\tjournal_dev_exists = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\tif (!journal_dev_exists) {\n\t\t\t\tmddev_suspend(mddev);\n\t\t\t\tclear_bit(MD_HAS_JOURNAL, &mddev->flags);\n\t\t\t\tmddev_resume(mddev);\n\t\t\t} else   \n\t\t\t\terr = -EBUSY;\n\t\t} else\n\t\t\terr = -EINVAL;\n\t} else {\n\t\terr = -EINVAL;\n\t}\n\n\tif (!err)\n\t\tmd_update_sb(mddev, 1);\n\n\tmddev_unlock(mddev);\n\n\treturn err;\n}\n\nstatic int raid5_start(struct mddev *mddev)\n{\n\tstruct r5conf *conf = mddev->private;\n\n\treturn r5l_start(conf->log);\n}\n\nstatic void raid5_prepare_suspend(struct mddev *mddev)\n{\n\tstruct r5conf *conf = mddev->private;\n\n\twait_event(mddev->sb_wait, !reshape_inprogress(mddev) ||\n\t\t\t\t    percpu_ref_is_zero(&mddev->active_io));\n\tif (percpu_ref_is_zero(&mddev->active_io))\n\t\treturn;\n\n\t \n\twake_up(&conf->wait_for_overlap);\n}\n\nstatic struct md_personality raid6_personality =\n{\n\t.name\t\t= \"raid6\",\n\t.level\t\t= 6,\n\t.owner\t\t= THIS_MODULE,\n\t.make_request\t= raid5_make_request,\n\t.run\t\t= raid5_run,\n\t.start\t\t= raid5_start,\n\t.free\t\t= raid5_free,\n\t.status\t\t= raid5_status,\n\t.error_handler\t= raid5_error,\n\t.hot_add_disk\t= raid5_add_disk,\n\t.hot_remove_disk= raid5_remove_disk,\n\t.spare_active\t= raid5_spare_active,\n\t.sync_request\t= raid5_sync_request,\n\t.resize\t\t= raid5_resize,\n\t.size\t\t= raid5_size,\n\t.check_reshape\t= raid6_check_reshape,\n\t.start_reshape  = raid5_start_reshape,\n\t.finish_reshape = raid5_finish_reshape,\n\t.prepare_suspend = raid5_prepare_suspend,\n\t.quiesce\t= raid5_quiesce,\n\t.takeover\t= raid6_takeover,\n\t.change_consistency_policy = raid5_change_consistency_policy,\n};\nstatic struct md_personality raid5_personality =\n{\n\t.name\t\t= \"raid5\",\n\t.level\t\t= 5,\n\t.owner\t\t= THIS_MODULE,\n\t.make_request\t= raid5_make_request,\n\t.run\t\t= raid5_run,\n\t.start\t\t= raid5_start,\n\t.free\t\t= raid5_free,\n\t.status\t\t= raid5_status,\n\t.error_handler\t= raid5_error,\n\t.hot_add_disk\t= raid5_add_disk,\n\t.hot_remove_disk= raid5_remove_disk,\n\t.spare_active\t= raid5_spare_active,\n\t.sync_request\t= raid5_sync_request,\n\t.resize\t\t= raid5_resize,\n\t.size\t\t= raid5_size,\n\t.check_reshape\t= raid5_check_reshape,\n\t.start_reshape  = raid5_start_reshape,\n\t.finish_reshape = raid5_finish_reshape,\n\t.prepare_suspend = raid5_prepare_suspend,\n\t.quiesce\t= raid5_quiesce,\n\t.takeover\t= raid5_takeover,\n\t.change_consistency_policy = raid5_change_consistency_policy,\n};\n\nstatic struct md_personality raid4_personality =\n{\n\t.name\t\t= \"raid4\",\n\t.level\t\t= 4,\n\t.owner\t\t= THIS_MODULE,\n\t.make_request\t= raid5_make_request,\n\t.run\t\t= raid5_run,\n\t.start\t\t= raid5_start,\n\t.free\t\t= raid5_free,\n\t.status\t\t= raid5_status,\n\t.error_handler\t= raid5_error,\n\t.hot_add_disk\t= raid5_add_disk,\n\t.hot_remove_disk= raid5_remove_disk,\n\t.spare_active\t= raid5_spare_active,\n\t.sync_request\t= raid5_sync_request,\n\t.resize\t\t= raid5_resize,\n\t.size\t\t= raid5_size,\n\t.check_reshape\t= raid5_check_reshape,\n\t.start_reshape  = raid5_start_reshape,\n\t.finish_reshape = raid5_finish_reshape,\n\t.prepare_suspend = raid5_prepare_suspend,\n\t.quiesce\t= raid5_quiesce,\n\t.takeover\t= raid4_takeover,\n\t.change_consistency_policy = raid5_change_consistency_policy,\n};\n\nstatic int __init raid5_init(void)\n{\n\tint ret;\n\n\traid5_wq = alloc_workqueue(\"raid5wq\",\n\t\tWQ_UNBOUND|WQ_MEM_RECLAIM|WQ_CPU_INTENSIVE|WQ_SYSFS, 0);\n\tif (!raid5_wq)\n\t\treturn -ENOMEM;\n\n\tret = cpuhp_setup_state_multi(CPUHP_MD_RAID5_PREPARE,\n\t\t\t\t      \"md/raid5:prepare\",\n\t\t\t\t      raid456_cpu_up_prepare,\n\t\t\t\t      raid456_cpu_dead);\n\tif (ret) {\n\t\tdestroy_workqueue(raid5_wq);\n\t\treturn ret;\n\t}\n\tregister_md_personality(&raid6_personality);\n\tregister_md_personality(&raid5_personality);\n\tregister_md_personality(&raid4_personality);\n\treturn 0;\n}\n\nstatic void raid5_exit(void)\n{\n\tunregister_md_personality(&raid6_personality);\n\tunregister_md_personality(&raid5_personality);\n\tunregister_md_personality(&raid4_personality);\n\tcpuhp_remove_multi_state(CPUHP_MD_RAID5_PREPARE);\n\tdestroy_workqueue(raid5_wq);\n}\n\nmodule_init(raid5_init);\nmodule_exit(raid5_exit);\nMODULE_LICENSE(\"GPL\");\nMODULE_DESCRIPTION(\"RAID4/5/6 (striping with parity) personality for MD\");\nMODULE_ALIAS(\"md-personality-4\");  \nMODULE_ALIAS(\"md-raid5\");\nMODULE_ALIAS(\"md-raid4\");\nMODULE_ALIAS(\"md-level-5\");\nMODULE_ALIAS(\"md-level-4\");\nMODULE_ALIAS(\"md-personality-8\");  \nMODULE_ALIAS(\"md-raid6\");\nMODULE_ALIAS(\"md-level-6\");\n\n \nMODULE_ALIAS(\"raid5\");\nMODULE_ALIAS(\"raid6\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}