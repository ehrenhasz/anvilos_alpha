{
  "module_name": "amd64_edac.c",
  "hash_id": "305658515340044ef0813bb4e44873b0ecfe72f7fb3f85c776b4f15961e6e7bd",
  "original_prompt": "Ingested from linux-6.6.14/drivers/edac/amd64_edac.c",
  "human_readable_source": "\n#include \"amd64_edac.h\"\n#include <asm/amd_nb.h>\n\nstatic struct edac_pci_ctl_info *pci_ctl;\n\n \nstatic int ecc_enable_override;\nmodule_param(ecc_enable_override, int, 0644);\n\nstatic struct msr __percpu *msrs;\n\nstatic inline u32 get_umc_reg(struct amd64_pvt *pvt, u32 reg)\n{\n\tif (!pvt->flags.zn_regs_v2)\n\t\treturn reg;\n\n\tswitch (reg) {\n\tcase UMCCH_ADDR_CFG:\t\treturn UMCCH_ADDR_CFG_DDR5;\n\tcase UMCCH_ADDR_MASK_SEC:\treturn UMCCH_ADDR_MASK_SEC_DDR5;\n\tcase UMCCH_DIMM_CFG:\t\treturn UMCCH_DIMM_CFG_DDR5;\n\t}\n\n\tWARN_ONCE(1, \"%s: unknown register 0x%x\", __func__, reg);\n\treturn 0;\n}\n\n \nstatic struct ecc_settings **ecc_stngs;\n\n \nstatic struct device *pci_ctl_dev;\n\n \nstatic const struct scrubrate {\n       u32 scrubval;            \n       u32 bandwidth;           \n} scrubrates[] = {\n\t{ 0x01, 1600000000UL},\n\t{ 0x02, 800000000UL},\n\t{ 0x03, 400000000UL},\n\t{ 0x04, 200000000UL},\n\t{ 0x05, 100000000UL},\n\t{ 0x06, 50000000UL},\n\t{ 0x07, 25000000UL},\n\t{ 0x08, 12284069UL},\n\t{ 0x09, 6274509UL},\n\t{ 0x0A, 3121951UL},\n\t{ 0x0B, 1560975UL},\n\t{ 0x0C, 781440UL},\n\t{ 0x0D, 390720UL},\n\t{ 0x0E, 195300UL},\n\t{ 0x0F, 97650UL},\n\t{ 0x10, 48854UL},\n\t{ 0x11, 24427UL},\n\t{ 0x12, 12213UL},\n\t{ 0x13, 6101UL},\n\t{ 0x14, 3051UL},\n\t{ 0x15, 1523UL},\n\t{ 0x16, 761UL},\n\t{ 0x00, 0UL},         \n};\n\nint __amd64_read_pci_cfg_dword(struct pci_dev *pdev, int offset,\n\t\t\t       u32 *val, const char *func)\n{\n\tint err = 0;\n\n\terr = pci_read_config_dword(pdev, offset, val);\n\tif (err)\n\t\tamd64_warn(\"%s: error reading F%dx%03x.\\n\",\n\t\t\t   func, PCI_FUNC(pdev->devfn), offset);\n\n\treturn err;\n}\n\nint __amd64_write_pci_cfg_dword(struct pci_dev *pdev, int offset,\n\t\t\t\tu32 val, const char *func)\n{\n\tint err = 0;\n\n\terr = pci_write_config_dword(pdev, offset, val);\n\tif (err)\n\t\tamd64_warn(\"%s: error writing to F%dx%03x.\\n\",\n\t\t\t   func, PCI_FUNC(pdev->devfn), offset);\n\n\treturn err;\n}\n\n \nstatic void f15h_select_dct(struct amd64_pvt *pvt, u8 dct)\n{\n\tu32 reg = 0;\n\n\tamd64_read_pci_cfg(pvt->F1, DCT_CFG_SEL, &reg);\n\treg &= (pvt->model == 0x30) ? ~3 : ~1;\n\treg |= dct;\n\tamd64_write_pci_cfg(pvt->F1, DCT_CFG_SEL, reg);\n}\n\n \nstatic inline int amd64_read_dct_pci_cfg(struct amd64_pvt *pvt, u8 dct,\n\t\t\t\t\t int offset, u32 *val)\n{\n\tswitch (pvt->fam) {\n\tcase 0xf:\n\t\tif (dct || offset >= 0x100)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\n\tcase 0x10:\n\t\tif (dct) {\n\t\t\t \n\t\t\tif (dct_ganging_enabled(pvt))\n\t\t\t\treturn 0;\n\n\t\t\toffset += 0x100;\n\t\t}\n\t\tbreak;\n\n\tcase 0x15:\n\t\t \n\t\tdct = (dct && pvt->model == 0x30) ? 3 : dct;\n\t\tf15h_select_dct(pvt, dct);\n\t\tbreak;\n\n\tcase 0x16:\n\t\tif (dct)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\treturn amd64_read_pci_cfg(pvt->F2, offset, val);\n}\n\n \n\n \nstatic int __set_scrub_rate(struct amd64_pvt *pvt, u32 new_bw, u32 min_rate)\n{\n\tu32 scrubval;\n\tint i;\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(scrubrates) - 1; i++) {\n\t\t \n\t\tif (scrubrates[i].scrubval < min_rate)\n\t\t\tcontinue;\n\n\t\tif (scrubrates[i].bandwidth <= new_bw)\n\t\t\tbreak;\n\t}\n\n\tscrubval = scrubrates[i].scrubval;\n\n\tif (pvt->fam == 0x15 && pvt->model == 0x60) {\n\t\tf15h_select_dct(pvt, 0);\n\t\tpci_write_bits32(pvt->F2, F15H_M60H_SCRCTRL, scrubval, 0x001F);\n\t\tf15h_select_dct(pvt, 1);\n\t\tpci_write_bits32(pvt->F2, F15H_M60H_SCRCTRL, scrubval, 0x001F);\n\t} else {\n\t\tpci_write_bits32(pvt->F3, SCRCTRL, scrubval, 0x001F);\n\t}\n\n\tif (scrubval)\n\t\treturn scrubrates[i].bandwidth;\n\n\treturn 0;\n}\n\nstatic int set_scrub_rate(struct mem_ctl_info *mci, u32 bw)\n{\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\tu32 min_scrubrate = 0x5;\n\n\tif (pvt->fam == 0xf)\n\t\tmin_scrubrate = 0x0;\n\n\tif (pvt->fam == 0x15) {\n\t\t \n\t\tif (pvt->model < 0x10)\n\t\t\tf15h_select_dct(pvt, 0);\n\n\t\tif (pvt->model == 0x60)\n\t\t\tmin_scrubrate = 0x6;\n\t}\n\treturn __set_scrub_rate(pvt, bw, min_scrubrate);\n}\n\nstatic int get_scrub_rate(struct mem_ctl_info *mci)\n{\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\tint i, retval = -EINVAL;\n\tu32 scrubval = 0;\n\n\tif (pvt->fam == 0x15) {\n\t\t \n\t\tif (pvt->model < 0x10)\n\t\t\tf15h_select_dct(pvt, 0);\n\n\t\tif (pvt->model == 0x60)\n\t\t\tamd64_read_pci_cfg(pvt->F2, F15H_M60H_SCRCTRL, &scrubval);\n\t\telse\n\t\t\tamd64_read_pci_cfg(pvt->F3, SCRCTRL, &scrubval);\n\t} else {\n\t\tamd64_read_pci_cfg(pvt->F3, SCRCTRL, &scrubval);\n\t}\n\n\tscrubval = scrubval & 0x001F;\n\n\tfor (i = 0; i < ARRAY_SIZE(scrubrates); i++) {\n\t\tif (scrubrates[i].scrubval == scrubval) {\n\t\t\tretval = scrubrates[i].bandwidth;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn retval;\n}\n\n \nstatic bool base_limit_match(struct amd64_pvt *pvt, u64 sys_addr, u8 nid)\n{\n\tu64 addr;\n\n\t \n\taddr = sys_addr & 0x000000ffffffffffull;\n\n\treturn ((addr >= get_dram_base(pvt, nid)) &&\n\t\t(addr <= get_dram_limit(pvt, nid)));\n}\n\n \nstatic struct mem_ctl_info *find_mc_by_sys_addr(struct mem_ctl_info *mci,\n\t\t\t\t\t\tu64 sys_addr)\n{\n\tstruct amd64_pvt *pvt;\n\tu8 node_id;\n\tu32 intlv_en, bits;\n\n\t \n\tpvt = mci->pvt_info;\n\n\t \n\tintlv_en = dram_intlv_en(pvt, 0);\n\n\tif (intlv_en == 0) {\n\t\tfor (node_id = 0; node_id < DRAM_RANGES; node_id++) {\n\t\t\tif (base_limit_match(pvt, sys_addr, node_id))\n\t\t\t\tgoto found;\n\t\t}\n\t\tgoto err_no_match;\n\t}\n\n\tif (unlikely((intlv_en != 0x01) &&\n\t\t     (intlv_en != 0x03) &&\n\t\t     (intlv_en != 0x07))) {\n\t\tamd64_warn(\"DRAM Base[IntlvEn] junk value: 0x%x, BIOS bug?\\n\", intlv_en);\n\t\treturn NULL;\n\t}\n\n\tbits = (((u32) sys_addr) >> 12) & intlv_en;\n\n\tfor (node_id = 0; ; ) {\n\t\tif ((dram_intlv_sel(pvt, node_id) & intlv_en) == bits)\n\t\t\tbreak;\t \n\n\t\tif (++node_id >= DRAM_RANGES)\n\t\t\tgoto err_no_match;\n\t}\n\n\t \n\tif (unlikely(!base_limit_match(pvt, sys_addr, node_id))) {\n\t\tamd64_warn(\"%s: sys_addr 0x%llx falls outside base/limit address\"\n\t\t\t   \"range for node %d with node interleaving enabled.\\n\",\n\t\t\t   __func__, sys_addr, node_id);\n\t\treturn NULL;\n\t}\n\nfound:\n\treturn edac_mc_find((int)node_id);\n\nerr_no_match:\n\tedac_dbg(2, \"sys_addr 0x%lx doesn't match any node\\n\",\n\t\t (unsigned long)sys_addr);\n\n\treturn NULL;\n}\n\n \nstatic void get_cs_base_and_mask(struct amd64_pvt *pvt, int csrow, u8 dct,\n\t\t\t\t u64 *base, u64 *mask)\n{\n\tu64 csbase, csmask, base_bits, mask_bits;\n\tu8 addr_shift;\n\n\tif (pvt->fam == 0xf && pvt->ext_model < K8_REV_F) {\n\t\tcsbase\t\t= pvt->csels[dct].csbases[csrow];\n\t\tcsmask\t\t= pvt->csels[dct].csmasks[csrow];\n\t\tbase_bits\t= GENMASK_ULL(31, 21) | GENMASK_ULL(15, 9);\n\t\tmask_bits\t= GENMASK_ULL(29, 21) | GENMASK_ULL(15, 9);\n\t\taddr_shift\t= 4;\n\n\t \n\t} else if (pvt->fam == 0x16 ||\n\t\t  (pvt->fam == 0x15 && pvt->model >= 0x30)) {\n\t\tcsbase          = pvt->csels[dct].csbases[csrow];\n\t\tcsmask          = pvt->csels[dct].csmasks[csrow >> 1];\n\n\t\t*base  = (csbase & GENMASK_ULL(15,  5)) << 6;\n\t\t*base |= (csbase & GENMASK_ULL(30, 19)) << 8;\n\n\t\t*mask = ~0ULL;\n\t\t \n\t\t*mask &= ~((GENMASK_ULL(15, 5)  << 6) |\n\t\t\t   (GENMASK_ULL(30, 19) << 8));\n\n\t\t*mask |= (csmask & GENMASK_ULL(15, 5))  << 6;\n\t\t*mask |= (csmask & GENMASK_ULL(30, 19)) << 8;\n\n\t\treturn;\n\t} else {\n\t\tcsbase\t\t= pvt->csels[dct].csbases[csrow];\n\t\tcsmask\t\t= pvt->csels[dct].csmasks[csrow >> 1];\n\t\taddr_shift\t= 8;\n\n\t\tif (pvt->fam == 0x15)\n\t\t\tbase_bits = mask_bits =\n\t\t\t\tGENMASK_ULL(30,19) | GENMASK_ULL(13,5);\n\t\telse\n\t\t\tbase_bits = mask_bits =\n\t\t\t\tGENMASK_ULL(28,19) | GENMASK_ULL(13,5);\n\t}\n\n\t*base  = (csbase & base_bits) << addr_shift;\n\n\t*mask  = ~0ULL;\n\t \n\t*mask &= ~(mask_bits << addr_shift);\n\t \n\t*mask |= (csmask & mask_bits) << addr_shift;\n}\n\n#define for_each_chip_select(i, dct, pvt) \\\n\tfor (i = 0; i < pvt->csels[dct].b_cnt; i++)\n\n#define chip_select_base(i, dct, pvt) \\\n\tpvt->csels[dct].csbases[i]\n\n#define for_each_chip_select_mask(i, dct, pvt) \\\n\tfor (i = 0; i < pvt->csels[dct].m_cnt; i++)\n\n#define for_each_umc(i) \\\n\tfor (i = 0; i < pvt->max_mcs; i++)\n\n \nstatic int input_addr_to_csrow(struct mem_ctl_info *mci, u64 input_addr)\n{\n\tstruct amd64_pvt *pvt;\n\tint csrow;\n\tu64 base, mask;\n\n\tpvt = mci->pvt_info;\n\n\tfor_each_chip_select(csrow, 0, pvt) {\n\t\tif (!csrow_enabled(csrow, 0, pvt))\n\t\t\tcontinue;\n\n\t\tget_cs_base_and_mask(pvt, csrow, 0, &base, &mask);\n\n\t\tmask = ~mask;\n\n\t\tif ((input_addr & mask) == (base & mask)) {\n\t\t\tedac_dbg(2, \"InputAddr 0x%lx matches csrow %d (node %d)\\n\",\n\t\t\t\t (unsigned long)input_addr, csrow,\n\t\t\t\t pvt->mc_node_id);\n\n\t\t\treturn csrow;\n\t\t}\n\t}\n\tedac_dbg(2, \"no matching csrow for InputAddr 0x%lx (MC node %d)\\n\",\n\t\t (unsigned long)input_addr, pvt->mc_node_id);\n\n\treturn -1;\n}\n\n \nstatic int get_dram_hole_info(struct mem_ctl_info *mci, u64 *hole_base,\n\t\t\t      u64 *hole_offset, u64 *hole_size)\n{\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\n\t \n\tif (pvt->fam == 0xf && pvt->ext_model < K8_REV_E) {\n\t\tedac_dbg(1, \"  revision %d for node %d does not support DHAR\\n\",\n\t\t\t pvt->ext_model, pvt->mc_node_id);\n\t\treturn 1;\n\t}\n\n\t \n\tif (pvt->fam >= 0x10 && !dhar_mem_hoist_valid(pvt)) {\n\t\tedac_dbg(1, \"  Dram Memory Hoisting is DISABLED on this system\\n\");\n\t\treturn 1;\n\t}\n\n\tif (!dhar_valid(pvt)) {\n\t\tedac_dbg(1, \"  Dram Memory Hoisting is DISABLED on this node %d\\n\",\n\t\t\t pvt->mc_node_id);\n\t\treturn 1;\n\t}\n\n\t \n\n\t \n\n\t*hole_base = dhar_base(pvt);\n\t*hole_size = (1ULL << 32) - *hole_base;\n\n\t*hole_offset = (pvt->fam > 0xf) ? f10_dhar_offset(pvt)\n\t\t\t\t\t: k8_dhar_offset(pvt);\n\n\tedac_dbg(1, \"  DHAR info for node %d base 0x%lx offset 0x%lx size 0x%lx\\n\",\n\t\t pvt->mc_node_id, (unsigned long)*hole_base,\n\t\t (unsigned long)*hole_offset, (unsigned long)*hole_size);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_EDAC_DEBUG\n#define EDAC_DCT_ATTR_SHOW(reg)\t\t\t\t\t\t\\\nstatic ssize_t reg##_show(struct device *dev,\t\t\t\t\\\n\t\t\t struct device_attribute *mattr, char *data)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tstruct mem_ctl_info *mci = to_mci(dev);\t\t\t\t\\\n\tstruct amd64_pvt *pvt = mci->pvt_info;\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\treturn sprintf(data, \"0x%016llx\\n\", (u64)pvt->reg);\t\t\\\n}\n\nEDAC_DCT_ATTR_SHOW(dhar);\nEDAC_DCT_ATTR_SHOW(dbam0);\nEDAC_DCT_ATTR_SHOW(top_mem);\nEDAC_DCT_ATTR_SHOW(top_mem2);\n\nstatic ssize_t dram_hole_show(struct device *dev, struct device_attribute *mattr,\n\t\t\t      char *data)\n{\n\tstruct mem_ctl_info *mci = to_mci(dev);\n\n\tu64 hole_base = 0;\n\tu64 hole_offset = 0;\n\tu64 hole_size = 0;\n\n\tget_dram_hole_info(mci, &hole_base, &hole_offset, &hole_size);\n\n\treturn sprintf(data, \"%llx %llx %llx\\n\", hole_base, hole_offset,\n\t\t\t\t\t\t hole_size);\n}\n\n \nstatic DEVICE_ATTR(dhar, S_IRUGO, dhar_show, NULL);\nstatic DEVICE_ATTR(dbam, S_IRUGO, dbam0_show, NULL);\nstatic DEVICE_ATTR(topmem, S_IRUGO, top_mem_show, NULL);\nstatic DEVICE_ATTR(topmem2, S_IRUGO, top_mem2_show, NULL);\nstatic DEVICE_ATTR_RO(dram_hole);\n\nstatic struct attribute *dbg_attrs[] = {\n\t&dev_attr_dhar.attr,\n\t&dev_attr_dbam.attr,\n\t&dev_attr_topmem.attr,\n\t&dev_attr_topmem2.attr,\n\t&dev_attr_dram_hole.attr,\n\tNULL\n};\n\nstatic const struct attribute_group dbg_group = {\n\t.attrs = dbg_attrs,\n};\n\nstatic ssize_t inject_section_show(struct device *dev,\n\t\t\t\t   struct device_attribute *mattr, char *buf)\n{\n\tstruct mem_ctl_info *mci = to_mci(dev);\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\treturn sprintf(buf, \"0x%x\\n\", pvt->injection.section);\n}\n\n \nstatic ssize_t inject_section_store(struct device *dev,\n\t\t\t\t    struct device_attribute *mattr,\n\t\t\t\t    const char *data, size_t count)\n{\n\tstruct mem_ctl_info *mci = to_mci(dev);\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\tunsigned long value;\n\tint ret;\n\n\tret = kstrtoul(data, 10, &value);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (value > 3) {\n\t\tamd64_warn(\"%s: invalid section 0x%lx\\n\", __func__, value);\n\t\treturn -EINVAL;\n\t}\n\n\tpvt->injection.section = (u32) value;\n\treturn count;\n}\n\nstatic ssize_t inject_word_show(struct device *dev,\n\t\t\t\tstruct device_attribute *mattr, char *buf)\n{\n\tstruct mem_ctl_info *mci = to_mci(dev);\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\treturn sprintf(buf, \"0x%x\\n\", pvt->injection.word);\n}\n\n \nstatic ssize_t inject_word_store(struct device *dev,\n\t\t\t\t struct device_attribute *mattr,\n\t\t\t\t const char *data, size_t count)\n{\n\tstruct mem_ctl_info *mci = to_mci(dev);\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\tunsigned long value;\n\tint ret;\n\n\tret = kstrtoul(data, 10, &value);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (value > 8) {\n\t\tamd64_warn(\"%s: invalid word 0x%lx\\n\", __func__, value);\n\t\treturn -EINVAL;\n\t}\n\n\tpvt->injection.word = (u32) value;\n\treturn count;\n}\n\nstatic ssize_t inject_ecc_vector_show(struct device *dev,\n\t\t\t\t      struct device_attribute *mattr,\n\t\t\t\t      char *buf)\n{\n\tstruct mem_ctl_info *mci = to_mci(dev);\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\treturn sprintf(buf, \"0x%x\\n\", pvt->injection.bit_map);\n}\n\n \nstatic ssize_t inject_ecc_vector_store(struct device *dev,\n\t\t\t\t       struct device_attribute *mattr,\n\t\t\t\t       const char *data, size_t count)\n{\n\tstruct mem_ctl_info *mci = to_mci(dev);\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\tunsigned long value;\n\tint ret;\n\n\tret = kstrtoul(data, 16, &value);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (value & 0xFFFF0000) {\n\t\tamd64_warn(\"%s: invalid EccVector: 0x%lx\\n\", __func__, value);\n\t\treturn -EINVAL;\n\t}\n\n\tpvt->injection.bit_map = (u32) value;\n\treturn count;\n}\n\n \nstatic ssize_t inject_read_store(struct device *dev,\n\t\t\t\t struct device_attribute *mattr,\n\t\t\t\t const char *data, size_t count)\n{\n\tstruct mem_ctl_info *mci = to_mci(dev);\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\tunsigned long value;\n\tu32 section, word_bits;\n\tint ret;\n\n\tret = kstrtoul(data, 10, &value);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tsection = F10_NB_ARRAY_DRAM | SET_NB_ARRAY_ADDR(pvt->injection.section);\n\n\tamd64_write_pci_cfg(pvt->F3, F10_NB_ARRAY_ADDR, section);\n\n\tword_bits = SET_NB_DRAM_INJECTION_READ(pvt->injection);\n\n\t \n\tamd64_write_pci_cfg(pvt->F3, F10_NB_ARRAY_DATA, word_bits);\n\n\tedac_dbg(0, \"section=0x%x word_bits=0x%x\\n\", section, word_bits);\n\n\treturn count;\n}\n\n \nstatic ssize_t inject_write_store(struct device *dev,\n\t\t\t\t  struct device_attribute *mattr,\n\t\t\t\t  const char *data, size_t count)\n{\n\tstruct mem_ctl_info *mci = to_mci(dev);\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\tu32 section, word_bits, tmp;\n\tunsigned long value;\n\tint ret;\n\n\tret = kstrtoul(data, 10, &value);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tsection = F10_NB_ARRAY_DRAM | SET_NB_ARRAY_ADDR(pvt->injection.section);\n\n\tamd64_write_pci_cfg(pvt->F3, F10_NB_ARRAY_ADDR, section);\n\n\tword_bits = SET_NB_DRAM_INJECTION_WRITE(pvt->injection);\n\n\tpr_notice_once(\"Don't forget to decrease MCE polling interval in\\n\"\n\t\t\t\"/sys/bus/machinecheck/devices/machinecheck<CPUNUM>/check_interval\\n\"\n\t\t\t\"so that you can get the error report faster.\\n\");\n\n\ton_each_cpu(disable_caches, NULL, 1);\n\n\t \n\tamd64_write_pci_cfg(pvt->F3, F10_NB_ARRAY_DATA, word_bits);\n\n retry:\n\t \n\tamd64_read_pci_cfg(pvt->F3, F10_NB_ARRAY_DATA, &tmp);\n\tif (tmp & F10_NB_ARR_ECC_WR_REQ) {\n\t\tcpu_relax();\n\t\tgoto retry;\n\t}\n\n\ton_each_cpu(enable_caches, NULL, 1);\n\n\tedac_dbg(0, \"section=0x%x word_bits=0x%x\\n\", section, word_bits);\n\n\treturn count;\n}\n\n \n\nstatic DEVICE_ATTR_RW(inject_section);\nstatic DEVICE_ATTR_RW(inject_word);\nstatic DEVICE_ATTR_RW(inject_ecc_vector);\nstatic DEVICE_ATTR_WO(inject_write);\nstatic DEVICE_ATTR_WO(inject_read);\n\nstatic struct attribute *inj_attrs[] = {\n\t&dev_attr_inject_section.attr,\n\t&dev_attr_inject_word.attr,\n\t&dev_attr_inject_ecc_vector.attr,\n\t&dev_attr_inject_write.attr,\n\t&dev_attr_inject_read.attr,\n\tNULL\n};\n\nstatic umode_t inj_is_visible(struct kobject *kobj, struct attribute *attr, int idx)\n{\n\tstruct device *dev = kobj_to_dev(kobj);\n\tstruct mem_ctl_info *mci = container_of(dev, struct mem_ctl_info, dev);\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\n\t \n\tif (pvt->fam >= 0x10 && pvt->fam <= 0x16)\n\t\treturn attr->mode;\n\n\treturn 0;\n}\n\nstatic const struct attribute_group inj_group = {\n\t.attrs = inj_attrs,\n\t.is_visible = inj_is_visible,\n};\n#endif  \n\n \nstatic u64 sys_addr_to_dram_addr(struct mem_ctl_info *mci, u64 sys_addr)\n{\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\tu64 dram_base, hole_base, hole_offset, hole_size, dram_addr;\n\tint ret;\n\n\tdram_base = get_dram_base(pvt, pvt->mc_node_id);\n\n\tret = get_dram_hole_info(mci, &hole_base, &hole_offset, &hole_size);\n\tif (!ret) {\n\t\tif ((sys_addr >= (1ULL << 32)) &&\n\t\t    (sys_addr < ((1ULL << 32) + hole_size))) {\n\t\t\t \n\t\t\tdram_addr = sys_addr - hole_offset;\n\n\t\t\tedac_dbg(2, \"using DHAR to translate SysAddr 0x%lx to DramAddr 0x%lx\\n\",\n\t\t\t\t (unsigned long)sys_addr,\n\t\t\t\t (unsigned long)dram_addr);\n\n\t\t\treturn dram_addr;\n\t\t}\n\t}\n\n\t \n\tdram_addr = (sys_addr & GENMASK_ULL(39, 0)) - dram_base;\n\n\tedac_dbg(2, \"using DRAM Base register to translate SysAddr 0x%lx to DramAddr 0x%lx\\n\",\n\t\t (unsigned long)sys_addr, (unsigned long)dram_addr);\n\treturn dram_addr;\n}\n\n \nstatic int num_node_interleave_bits(unsigned intlv_en)\n{\n\tstatic const int intlv_shift_table[] = { 0, 1, 0, 2, 0, 0, 0, 3 };\n\tint n;\n\n\tBUG_ON(intlv_en > 7);\n\tn = intlv_shift_table[intlv_en];\n\treturn n;\n}\n\n \nstatic u64 dram_addr_to_input_addr(struct mem_ctl_info *mci, u64 dram_addr)\n{\n\tstruct amd64_pvt *pvt;\n\tint intlv_shift;\n\tu64 input_addr;\n\n\tpvt = mci->pvt_info;\n\n\t \n\tintlv_shift = num_node_interleave_bits(dram_intlv_en(pvt, 0));\n\tinput_addr = ((dram_addr >> intlv_shift) & GENMASK_ULL(35, 12)) +\n\t\t      (dram_addr & 0xfff);\n\n\tedac_dbg(2, \"  Intlv Shift=%d DramAddr=0x%lx maps to InputAddr=0x%lx\\n\",\n\t\t intlv_shift, (unsigned long)dram_addr,\n\t\t (unsigned long)input_addr);\n\n\treturn input_addr;\n}\n\n \nstatic u64 sys_addr_to_input_addr(struct mem_ctl_info *mci, u64 sys_addr)\n{\n\tu64 input_addr;\n\n\tinput_addr =\n\t    dram_addr_to_input_addr(mci, sys_addr_to_dram_addr(mci, sys_addr));\n\n\tedac_dbg(2, \"SysAddr 0x%lx translates to InputAddr 0x%lx\\n\",\n\t\t (unsigned long)sys_addr, (unsigned long)input_addr);\n\n\treturn input_addr;\n}\n\n \nstatic inline void error_address_to_page_and_offset(u64 error_address,\n\t\t\t\t\t\t    struct err_info *err)\n{\n\terr->page = (u32) (error_address >> PAGE_SHIFT);\n\terr->offset = ((u32) error_address) & ~PAGE_MASK;\n}\n\n \nstatic int sys_addr_to_csrow(struct mem_ctl_info *mci, u64 sys_addr)\n{\n\tint csrow;\n\n\tcsrow = input_addr_to_csrow(mci, sys_addr_to_input_addr(mci, sys_addr));\n\n\tif (csrow == -1)\n\t\tamd64_mc_err(mci, \"Failed to translate InputAddr to csrow for \"\n\t\t\t\t  \"address 0x%lx\\n\", (unsigned long)sys_addr);\n\treturn csrow;\n}\n\n \nstatic struct local_node_map {\n\tu16 node_count;\n\tu16 base_node_id;\n} gpu_node_map;\n\n#define PCI_DEVICE_ID_AMD_MI200_DF_F1\t\t0x14d1\n#define REG_LOCAL_NODE_TYPE_MAP\t\t\t0x144\n\n \n#define LNTM_NODE_COUNT\t\t\t\tGENMASK(27, 16)\n#define LNTM_BASE_NODE_ID\t\t\tGENMASK(11, 0)\n\nstatic int gpu_get_node_map(void)\n{\n\tstruct pci_dev *pdev;\n\tint ret;\n\tu32 tmp;\n\n\t \n\tif (gpu_node_map.base_node_id)\n\t\treturn 0;\n\n\tpdev = pci_get_device(PCI_VENDOR_ID_AMD, PCI_DEVICE_ID_AMD_MI200_DF_F1, NULL);\n\tif (!pdev) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tret = pci_read_config_dword(pdev, REG_LOCAL_NODE_TYPE_MAP, &tmp);\n\tif (ret)\n\t\tgoto out;\n\n\tgpu_node_map.node_count = FIELD_GET(LNTM_NODE_COUNT, tmp);\n\tgpu_node_map.base_node_id = FIELD_GET(LNTM_BASE_NODE_ID, tmp);\n\nout:\n\tpci_dev_put(pdev);\n\treturn ret;\n}\n\nstatic int fixup_node_id(int node_id, struct mce *m)\n{\n\t \n\tu8 nid = (m->ipid >> 44) & 0xF;\n\n\tif (smca_get_bank_type(m->extcpu, m->bank) != SMCA_UMC_V2)\n\t\treturn node_id;\n\n\t \n\tif (nid < gpu_node_map.base_node_id)\n\t\treturn node_id;\n\n\t \n\treturn nid - gpu_node_map.base_node_id + 1;\n}\n\n \nstatic DEFINE_MUTEX(df_indirect_mutex);\n\n \n#define DF_BROADCAST\t0xFF\nstatic int __df_indirect_read(u16 node, u8 func, u16 reg, u8 instance_id, u32 *lo)\n{\n\tstruct pci_dev *F4;\n\tu32 ficaa;\n\tint err = -ENODEV;\n\n\tif (node >= amd_nb_num())\n\t\tgoto out;\n\n\tF4 = node_to_amd_nb(node)->link;\n\tif (!F4)\n\t\tgoto out;\n\n\tficaa  = (instance_id == DF_BROADCAST) ? 0 : 1;\n\tficaa |= reg & 0x3FC;\n\tficaa |= (func & 0x7) << 11;\n\tficaa |= instance_id << 16;\n\n\tmutex_lock(&df_indirect_mutex);\n\n\terr = pci_write_config_dword(F4, 0x5C, ficaa);\n\tif (err) {\n\t\tpr_warn(\"Error writing DF Indirect FICAA, FICAA=0x%x\\n\", ficaa);\n\t\tgoto out_unlock;\n\t}\n\n\terr = pci_read_config_dword(F4, 0x98, lo);\n\tif (err)\n\t\tpr_warn(\"Error reading DF Indirect FICAD LO, FICAA=0x%x.\\n\", ficaa);\n\nout_unlock:\n\tmutex_unlock(&df_indirect_mutex);\n\nout:\n\treturn err;\n}\n\nstatic int df_indirect_read_instance(u16 node, u8 func, u16 reg, u8 instance_id, u32 *lo)\n{\n\treturn __df_indirect_read(node, func, reg, instance_id, lo);\n}\n\nstatic int df_indirect_read_broadcast(u16 node, u8 func, u16 reg, u32 *lo)\n{\n\treturn __df_indirect_read(node, func, reg, DF_BROADCAST, lo);\n}\n\nstruct addr_ctx {\n\tu64 ret_addr;\n\tu32 tmp;\n\tu16 nid;\n\tu8 inst_id;\n};\n\nstatic int umc_normaddr_to_sysaddr(u64 norm_addr, u16 nid, u8 umc, u64 *sys_addr)\n{\n\tu64 dram_base_addr, dram_limit_addr, dram_hole_base;\n\n\tu8 die_id_shift, die_id_mask, socket_id_shift, socket_id_mask;\n\tu8 intlv_num_dies, intlv_num_chan, intlv_num_sockets;\n\tu8 intlv_addr_sel, intlv_addr_bit;\n\tu8 num_intlv_bits, hashed_bit;\n\tu8 lgcy_mmio_hole_en, base = 0;\n\tu8 cs_mask, cs_id = 0;\n\tbool hash_enabled = false;\n\n\tstruct addr_ctx ctx;\n\n\tmemset(&ctx, 0, sizeof(ctx));\n\n\t \n\tctx.ret_addr = norm_addr;\n\n\tctx.nid = nid;\n\tctx.inst_id = umc;\n\n\t \n\tif (df_indirect_read_instance(nid, 0, 0x1B4, umc, &ctx.tmp))\n\t\tgoto out_err;\n\n\t \n\tif (ctx.tmp & BIT(0)) {\n\t\tu64 hi_addr_offset = (ctx.tmp & GENMASK_ULL(31, 20)) << 8;\n\n\t\tif (norm_addr >= hi_addr_offset) {\n\t\t\tctx.ret_addr -= hi_addr_offset;\n\t\t\tbase = 1;\n\t\t}\n\t}\n\n\t \n\tif (df_indirect_read_instance(nid, 0, 0x110 + (8 * base), umc, &ctx.tmp))\n\t\tgoto out_err;\n\n\t \n\tif (!(ctx.tmp & BIT(0))) {\n\t\tpr_err(\"%s: Invalid DramBaseAddress range: 0x%x.\\n\",\n\t\t\t__func__, ctx.tmp);\n\t\tgoto out_err;\n\t}\n\n\tlgcy_mmio_hole_en = ctx.tmp & BIT(1);\n\tintlv_num_chan\t  = (ctx.tmp >> 4) & 0xF;\n\tintlv_addr_sel\t  = (ctx.tmp >> 8) & 0x7;\n\tdram_base_addr\t  = (ctx.tmp & GENMASK_ULL(31, 12)) << 16;\n\n\t \n\tif (intlv_addr_sel > 3) {\n\t\tpr_err(\"%s: Invalid interleave address select %d.\\n\",\n\t\t\t__func__, intlv_addr_sel);\n\t\tgoto out_err;\n\t}\n\n\t \n\tif (df_indirect_read_instance(nid, 0, 0x114 + (8 * base), umc, &ctx.tmp))\n\t\tgoto out_err;\n\n\tintlv_num_sockets = (ctx.tmp >> 8) & 0x1;\n\tintlv_num_dies\t  = (ctx.tmp >> 10) & 0x3;\n\tdram_limit_addr\t  = ((ctx.tmp & GENMASK_ULL(31, 12)) << 16) | GENMASK_ULL(27, 0);\n\n\tintlv_addr_bit = intlv_addr_sel + 8;\n\n\t \n\tswitch (intlv_num_chan) {\n\tcase 0:\tintlv_num_chan = 0; break;\n\tcase 1: intlv_num_chan = 1; break;\n\tcase 3: intlv_num_chan = 2; break;\n\tcase 5:\tintlv_num_chan = 3; break;\n\tcase 7:\tintlv_num_chan = 4; break;\n\n\tcase 8: intlv_num_chan = 1;\n\t\thash_enabled = true;\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"%s: Invalid number of interleaved channels %d.\\n\",\n\t\t\t__func__, intlv_num_chan);\n\t\tgoto out_err;\n\t}\n\n\tnum_intlv_bits = intlv_num_chan;\n\n\tif (intlv_num_dies > 2) {\n\t\tpr_err(\"%s: Invalid number of interleaved nodes/dies %d.\\n\",\n\t\t\t__func__, intlv_num_dies);\n\t\tgoto out_err;\n\t}\n\n\tnum_intlv_bits += intlv_num_dies;\n\n\t \n\tnum_intlv_bits += intlv_num_sockets;\n\n\t \n\tif (num_intlv_bits > 4) {\n\t\tpr_err(\"%s: Invalid interleave bits %d.\\n\",\n\t\t\t__func__, num_intlv_bits);\n\t\tgoto out_err;\n\t}\n\n\tif (num_intlv_bits > 0) {\n\t\tu64 temp_addr_x, temp_addr_i, temp_addr_y;\n\t\tu8 die_id_bit, sock_id_bit, cs_fabric_id;\n\n\t\t \n\t\tif (df_indirect_read_instance(nid, 0, 0x50, umc, &ctx.tmp))\n\t\t\tgoto out_err;\n\n\t\tcs_fabric_id = (ctx.tmp >> 8) & 0xFF;\n\t\tdie_id_bit   = 0;\n\n\t\t \n\t\tif (intlv_num_chan) {\n\t\t\tdie_id_bit = intlv_num_chan;\n\t\t\tcs_mask\t   = (1 << die_id_bit) - 1;\n\t\t\tcs_id\t   = cs_fabric_id & cs_mask;\n\t\t}\n\n\t\tsock_id_bit = die_id_bit;\n\n\t\t \n\t\tif (intlv_num_dies || intlv_num_sockets)\n\t\t\tif (df_indirect_read_broadcast(nid, 1, 0x208, &ctx.tmp))\n\t\t\t\tgoto out_err;\n\n\t\t \n\t\tif (intlv_num_dies) {\n\t\t\tsock_id_bit  = die_id_bit + intlv_num_dies;\n\t\t\tdie_id_shift = (ctx.tmp >> 24) & 0xF;\n\t\t\tdie_id_mask  = (ctx.tmp >> 8) & 0xFF;\n\n\t\t\tcs_id |= ((cs_fabric_id & die_id_mask) >> die_id_shift) << die_id_bit;\n\t\t}\n\n\t\t \n\t\tif (intlv_num_sockets) {\n\t\t\tsocket_id_shift\t= (ctx.tmp >> 28) & 0xF;\n\t\t\tsocket_id_mask\t= (ctx.tmp >> 16) & 0xFF;\n\n\t\t\tcs_id |= ((cs_fabric_id & socket_id_mask) >> socket_id_shift) << sock_id_bit;\n\t\t}\n\n\t\t \n\t\ttemp_addr_y = ctx.ret_addr & GENMASK_ULL(intlv_addr_bit - 1, 0);\n\t\ttemp_addr_i = (cs_id << intlv_addr_bit);\n\t\ttemp_addr_x = (ctx.ret_addr & GENMASK_ULL(63, intlv_addr_bit)) << num_intlv_bits;\n\t\tctx.ret_addr    = temp_addr_x | temp_addr_i | temp_addr_y;\n\t}\n\n\t \n\tctx.ret_addr += dram_base_addr;\n\n\t \n\tif (lgcy_mmio_hole_en) {\n\t\tif (df_indirect_read_broadcast(nid, 0, 0x104, &ctx.tmp))\n\t\t\tgoto out_err;\n\n\t\tdram_hole_base = ctx.tmp & GENMASK(31, 24);\n\t\tif (ctx.ret_addr >= dram_hole_base)\n\t\t\tctx.ret_addr += (BIT_ULL(32) - dram_hole_base);\n\t}\n\n\tif (hash_enabled) {\n\t\t \n\t\thashed_bit =\t(ctx.ret_addr >> 12) ^\n\t\t\t\t(ctx.ret_addr >> 18) ^\n\t\t\t\t(ctx.ret_addr >> 21) ^\n\t\t\t\t(ctx.ret_addr >> 30) ^\n\t\t\t\tcs_id;\n\n\t\thashed_bit &= BIT(0);\n\n\t\tif (hashed_bit != ((ctx.ret_addr >> intlv_addr_bit) & BIT(0)))\n\t\t\tctx.ret_addr ^= BIT(intlv_addr_bit);\n\t}\n\n\t \n\tif (ctx.ret_addr > dram_limit_addr)\n\t\tgoto out_err;\n\n\t*sys_addr = ctx.ret_addr;\n\treturn 0;\n\nout_err:\n\treturn -EINVAL;\n}\n\nstatic int get_channel_from_ecc_syndrome(struct mem_ctl_info *, u16);\n\n \nstatic unsigned long dct_determine_edac_cap(struct amd64_pvt *pvt)\n{\n\tunsigned long edac_cap = EDAC_FLAG_NONE;\n\tu8 bit;\n\n\tbit = (pvt->fam > 0xf || pvt->ext_model >= K8_REV_F)\n\t\t? 19\n\t\t: 17;\n\n\tif (pvt->dclr0 & BIT(bit))\n\t\tedac_cap = EDAC_FLAG_SECDED;\n\n\treturn edac_cap;\n}\n\nstatic unsigned long umc_determine_edac_cap(struct amd64_pvt *pvt)\n{\n\tu8 i, umc_en_mask = 0, dimm_ecc_en_mask = 0;\n\tunsigned long edac_cap = EDAC_FLAG_NONE;\n\n\tfor_each_umc(i) {\n\t\tif (!(pvt->umc[i].sdp_ctrl & UMC_SDP_INIT))\n\t\t\tcontinue;\n\n\t\tumc_en_mask |= BIT(i);\n\n\t\t \n\t\tif (pvt->umc[i].umc_cfg & BIT(12))\n\t\t\tdimm_ecc_en_mask |= BIT(i);\n\t}\n\n\tif (umc_en_mask == dimm_ecc_en_mask)\n\t\tedac_cap = EDAC_FLAG_SECDED;\n\n\treturn edac_cap;\n}\n\n \nstatic void dct_debug_display_dimm_sizes(struct amd64_pvt *pvt, u8 ctrl)\n{\n\tu32 *dcsb = ctrl ? pvt->csels[1].csbases : pvt->csels[0].csbases;\n\tu32 dbam  = ctrl ? pvt->dbam1 : pvt->dbam0;\n\tint dimm, size0, size1;\n\n\tif (pvt->fam == 0xf) {\n\t\t \n\t\tif (pvt->ext_model < K8_REV_F)\n\t\t\treturn;\n\n\t\tWARN_ON(ctrl != 0);\n\t}\n\n\tif (pvt->fam == 0x10) {\n\t\tdbam = (ctrl && !dct_ganging_enabled(pvt)) ? pvt->dbam1\n\t\t\t\t\t\t\t   : pvt->dbam0;\n\t\tdcsb = (ctrl && !dct_ganging_enabled(pvt)) ?\n\t\t\t\t pvt->csels[1].csbases :\n\t\t\t\t pvt->csels[0].csbases;\n\t} else if (ctrl) {\n\t\tdbam = pvt->dbam0;\n\t\tdcsb = pvt->csels[1].csbases;\n\t}\n\tedac_dbg(1, \"F2x%d80 (DRAM Bank Address Mapping): 0x%08x\\n\",\n\t\t ctrl, dbam);\n\n\tedac_printk(KERN_DEBUG, EDAC_MC, \"DCT%d chip selects:\\n\", ctrl);\n\n\t \n\tfor (dimm = 0; dimm < 4; dimm++) {\n\t\tsize0 = 0;\n\t\tif (dcsb[dimm * 2] & DCSB_CS_ENABLE)\n\t\t\t \n\t\t\tsize0 = pvt->ops->dbam_to_cs(pvt, ctrl,\n\t\t\t\t\t\t     DBAM_DIMM(dimm, dbam),\n\t\t\t\t\t\t     dimm);\n\n\t\tsize1 = 0;\n\t\tif (dcsb[dimm * 2 + 1] & DCSB_CS_ENABLE)\n\t\t\tsize1 = pvt->ops->dbam_to_cs(pvt, ctrl,\n\t\t\t\t\t\t     DBAM_DIMM(dimm, dbam),\n\t\t\t\t\t\t     dimm);\n\n\t\tamd64_info(EDAC_MC \": %d: %5dMB %d: %5dMB\\n\",\n\t\t\t   dimm * 2,     size0,\n\t\t\t   dimm * 2 + 1, size1);\n\t}\n}\n\n\nstatic void debug_dump_dramcfg_low(struct amd64_pvt *pvt, u32 dclr, int chan)\n{\n\tedac_dbg(1, \"F2x%d90 (DRAM Cfg Low): 0x%08x\\n\", chan, dclr);\n\n\tif (pvt->dram_type == MEM_LRDDR3) {\n\t\tu32 dcsm = pvt->csels[chan].csmasks[0];\n\t\t \n\t\tedac_dbg(1, \" LRDIMM %dx rank multiply\\n\", (dcsm & 0x3));\n\t}\n\n\tedac_dbg(1, \"All DIMMs support ECC:%s\\n\",\n\t\t    (dclr & BIT(19)) ? \"yes\" : \"no\");\n\n\n\tedac_dbg(1, \"  PAR/ERR parity: %s\\n\",\n\t\t (dclr & BIT(8)) ?  \"enabled\" : \"disabled\");\n\n\tif (pvt->fam == 0x10)\n\t\tedac_dbg(1, \"  DCT 128bit mode width: %s\\n\",\n\t\t\t (dclr & BIT(11)) ?  \"128b\" : \"64b\");\n\n\tedac_dbg(1, \"  x4 logical DIMMs present: L0: %s L1: %s L2: %s L3: %s\\n\",\n\t\t (dclr & BIT(12)) ?  \"yes\" : \"no\",\n\t\t (dclr & BIT(13)) ?  \"yes\" : \"no\",\n\t\t (dclr & BIT(14)) ?  \"yes\" : \"no\",\n\t\t (dclr & BIT(15)) ?  \"yes\" : \"no\");\n}\n\n#define CS_EVEN_PRIMARY\t\tBIT(0)\n#define CS_ODD_PRIMARY\t\tBIT(1)\n#define CS_EVEN_SECONDARY\tBIT(2)\n#define CS_ODD_SECONDARY\tBIT(3)\n#define CS_3R_INTERLEAVE\tBIT(4)\n\n#define CS_EVEN\t\t\t(CS_EVEN_PRIMARY | CS_EVEN_SECONDARY)\n#define CS_ODD\t\t\t(CS_ODD_PRIMARY | CS_ODD_SECONDARY)\n\nstatic int umc_get_cs_mode(int dimm, u8 ctrl, struct amd64_pvt *pvt)\n{\n\tu8 base, count = 0;\n\tint cs_mode = 0;\n\n\tif (csrow_enabled(2 * dimm, ctrl, pvt))\n\t\tcs_mode |= CS_EVEN_PRIMARY;\n\n\tif (csrow_enabled(2 * dimm + 1, ctrl, pvt))\n\t\tcs_mode |= CS_ODD_PRIMARY;\n\n\t \n\tif (csrow_sec_enabled(2 * dimm + 1, ctrl, pvt))\n\t\tcs_mode |= CS_ODD_SECONDARY;\n\n\t \n\tfor_each_chip_select(base, ctrl, pvt)\n\t\tcount += csrow_enabled(base, ctrl, pvt);\n\n\tif (count == 3 &&\n\t    pvt->csels[ctrl].csmasks[0] == pvt->csels[ctrl].csmasks[1]) {\n\t\tedac_dbg(1, \"3R interleaving in use.\\n\");\n\t\tcs_mode |= CS_3R_INTERLEAVE;\n\t}\n\n\treturn cs_mode;\n}\n\nstatic int __addr_mask_to_cs_size(u32 addr_mask_orig, unsigned int cs_mode,\n\t\t\t\t  int csrow_nr, int dimm)\n{\n\tu32 msb, weight, num_zero_bits;\n\tu32 addr_mask_deinterleaved;\n\tint size = 0;\n\n\t \n\tmsb = fls(addr_mask_orig) - 1;\n\tweight = hweight_long(addr_mask_orig);\n\tnum_zero_bits = msb - weight - !!(cs_mode & CS_3R_INTERLEAVE);\n\n\t \n\taddr_mask_deinterleaved = GENMASK_ULL(msb - num_zero_bits, 1);\n\n\tedac_dbg(1, \"CS%d DIMM%d AddrMasks:\\n\", csrow_nr, dimm);\n\tedac_dbg(1, \"  Original AddrMask: 0x%x\\n\", addr_mask_orig);\n\tedac_dbg(1, \"  Deinterleaved AddrMask: 0x%x\\n\", addr_mask_deinterleaved);\n\n\t \n\tsize = (addr_mask_deinterleaved >> 2) + 1;\n\n\t \n\treturn size >> 10;\n}\n\nstatic int umc_addr_mask_to_cs_size(struct amd64_pvt *pvt, u8 umc,\n\t\t\t\t    unsigned int cs_mode, int csrow_nr)\n{\n\tint cs_mask_nr = csrow_nr;\n\tu32 addr_mask_orig;\n\tint dimm, size = 0;\n\n\t \n\tif (!cs_mode)\n\t\treturn size;\n\n\t \n\tif (!(cs_mode & CS_EVEN) && !(csrow_nr & 1))\n\t\treturn size;\n\n\t \n\tif (!(cs_mode & CS_ODD) && (csrow_nr & 1))\n\t\treturn size;\n\n\t \n\tdimm = csrow_nr >> 1;\n\n\tif (!pvt->flags.zn_regs_v2)\n\t\tcs_mask_nr >>= 1;\n\n\t \n\tif ((csrow_nr & 1) && (cs_mode & CS_ODD_SECONDARY))\n\t\taddr_mask_orig = pvt->csels[umc].csmasks_sec[cs_mask_nr];\n\telse\n\t\taddr_mask_orig = pvt->csels[umc].csmasks[cs_mask_nr];\n\n\treturn __addr_mask_to_cs_size(addr_mask_orig, cs_mode, csrow_nr, dimm);\n}\n\nstatic void umc_debug_display_dimm_sizes(struct amd64_pvt *pvt, u8 ctrl)\n{\n\tint dimm, size0, size1, cs0, cs1, cs_mode;\n\n\tedac_printk(KERN_DEBUG, EDAC_MC, \"UMC%d chip selects:\\n\", ctrl);\n\n\tfor (dimm = 0; dimm < 2; dimm++) {\n\t\tcs0 = dimm * 2;\n\t\tcs1 = dimm * 2 + 1;\n\n\t\tcs_mode = umc_get_cs_mode(dimm, ctrl, pvt);\n\n\t\tsize0 = umc_addr_mask_to_cs_size(pvt, ctrl, cs_mode, cs0);\n\t\tsize1 = umc_addr_mask_to_cs_size(pvt, ctrl, cs_mode, cs1);\n\n\t\tamd64_info(EDAC_MC \": %d: %5dMB %d: %5dMB\\n\",\n\t\t\t\tcs0,\tsize0,\n\t\t\t\tcs1,\tsize1);\n\t}\n}\n\nstatic void umc_dump_misc_regs(struct amd64_pvt *pvt)\n{\n\tstruct amd64_umc *umc;\n\tu32 i, tmp, umc_base;\n\n\tfor_each_umc(i) {\n\t\tumc_base = get_umc_base(i);\n\t\tumc = &pvt->umc[i];\n\n\t\tedac_dbg(1, \"UMC%d DIMM cfg: 0x%x\\n\", i, umc->dimm_cfg);\n\t\tedac_dbg(1, \"UMC%d UMC cfg: 0x%x\\n\", i, umc->umc_cfg);\n\t\tedac_dbg(1, \"UMC%d SDP ctrl: 0x%x\\n\", i, umc->sdp_ctrl);\n\t\tedac_dbg(1, \"UMC%d ECC ctrl: 0x%x\\n\", i, umc->ecc_ctrl);\n\n\t\tamd_smn_read(pvt->mc_node_id, umc_base + UMCCH_ECC_BAD_SYMBOL, &tmp);\n\t\tedac_dbg(1, \"UMC%d ECC bad symbol: 0x%x\\n\", i, tmp);\n\n\t\tamd_smn_read(pvt->mc_node_id, umc_base + UMCCH_UMC_CAP, &tmp);\n\t\tedac_dbg(1, \"UMC%d UMC cap: 0x%x\\n\", i, tmp);\n\t\tedac_dbg(1, \"UMC%d UMC cap high: 0x%x\\n\", i, umc->umc_cap_hi);\n\n\t\tedac_dbg(1, \"UMC%d ECC capable: %s, ChipKill ECC capable: %s\\n\",\n\t\t\t\ti, (umc->umc_cap_hi & BIT(30)) ? \"yes\" : \"no\",\n\t\t\t\t    (umc->umc_cap_hi & BIT(31)) ? \"yes\" : \"no\");\n\t\tedac_dbg(1, \"UMC%d All DIMMs support ECC: %s\\n\",\n\t\t\t\ti, (umc->umc_cfg & BIT(12)) ? \"yes\" : \"no\");\n\t\tedac_dbg(1, \"UMC%d x4 DIMMs present: %s\\n\",\n\t\t\t\ti, (umc->dimm_cfg & BIT(6)) ? \"yes\" : \"no\");\n\t\tedac_dbg(1, \"UMC%d x16 DIMMs present: %s\\n\",\n\t\t\t\ti, (umc->dimm_cfg & BIT(7)) ? \"yes\" : \"no\");\n\n\t\tif (umc->dram_type == MEM_LRDDR4 || umc->dram_type == MEM_LRDDR5) {\n\t\t\tamd_smn_read(pvt->mc_node_id,\n\t\t\t\t     umc_base + get_umc_reg(pvt, UMCCH_ADDR_CFG),\n\t\t\t\t     &tmp);\n\t\t\tedac_dbg(1, \"UMC%d LRDIMM %dx rank multiply\\n\",\n\t\t\t\t\ti, 1 << ((tmp >> 4) & 0x3));\n\t\t}\n\n\t\tumc_debug_display_dimm_sizes(pvt, i);\n\t}\n}\n\nstatic void dct_dump_misc_regs(struct amd64_pvt *pvt)\n{\n\tedac_dbg(1, \"F3xE8 (NB Cap): 0x%08x\\n\", pvt->nbcap);\n\n\tedac_dbg(1, \"  NB two channel DRAM capable: %s\\n\",\n\t\t (pvt->nbcap & NBCAP_DCT_DUAL) ? \"yes\" : \"no\");\n\n\tedac_dbg(1, \"  ECC capable: %s, ChipKill ECC capable: %s\\n\",\n\t\t (pvt->nbcap & NBCAP_SECDED) ? \"yes\" : \"no\",\n\t\t (pvt->nbcap & NBCAP_CHIPKILL) ? \"yes\" : \"no\");\n\n\tdebug_dump_dramcfg_low(pvt, pvt->dclr0, 0);\n\n\tedac_dbg(1, \"F3xB0 (Online Spare): 0x%08x\\n\", pvt->online_spare);\n\n\tedac_dbg(1, \"F1xF0 (DRAM Hole Address): 0x%08x, base: 0x%08x, offset: 0x%08x\\n\",\n\t\t pvt->dhar, dhar_base(pvt),\n\t\t (pvt->fam == 0xf) ? k8_dhar_offset(pvt)\n\t\t\t\t   : f10_dhar_offset(pvt));\n\n\tdct_debug_display_dimm_sizes(pvt, 0);\n\n\t \n\tif (pvt->fam == 0xf)\n\t\treturn;\n\n\tdct_debug_display_dimm_sizes(pvt, 1);\n\n\t \n\tif (!dct_ganging_enabled(pvt))\n\t\tdebug_dump_dramcfg_low(pvt, pvt->dclr1, 1);\n\n\tedac_dbg(1, \"  DramHoleValid: %s\\n\", dhar_valid(pvt) ? \"yes\" : \"no\");\n\n\tamd64_info(\"using x%u syndromes.\\n\", pvt->ecc_sym_sz);\n}\n\n \nstatic void dct_prep_chip_selects(struct amd64_pvt *pvt)\n{\n\tif (pvt->fam == 0xf && pvt->ext_model < K8_REV_F) {\n\t\tpvt->csels[0].b_cnt = pvt->csels[1].b_cnt = 8;\n\t\tpvt->csels[0].m_cnt = pvt->csels[1].m_cnt = 8;\n\t} else if (pvt->fam == 0x15 && pvt->model == 0x30) {\n\t\tpvt->csels[0].b_cnt = pvt->csels[1].b_cnt = 4;\n\t\tpvt->csels[0].m_cnt = pvt->csels[1].m_cnt = 2;\n\t} else {\n\t\tpvt->csels[0].b_cnt = pvt->csels[1].b_cnt = 8;\n\t\tpvt->csels[0].m_cnt = pvt->csels[1].m_cnt = 4;\n\t}\n}\n\nstatic void umc_prep_chip_selects(struct amd64_pvt *pvt)\n{\n\tint umc;\n\n\tfor_each_umc(umc) {\n\t\tpvt->csels[umc].b_cnt = 4;\n\t\tpvt->csels[umc].m_cnt = pvt->flags.zn_regs_v2 ? 4 : 2;\n\t}\n}\n\nstatic void umc_read_base_mask(struct amd64_pvt *pvt)\n{\n\tu32 umc_base_reg, umc_base_reg_sec;\n\tu32 umc_mask_reg, umc_mask_reg_sec;\n\tu32 base_reg, base_reg_sec;\n\tu32 mask_reg, mask_reg_sec;\n\tu32 *base, *base_sec;\n\tu32 *mask, *mask_sec;\n\tint cs, umc;\n\n\tfor_each_umc(umc) {\n\t\tumc_base_reg = get_umc_base(umc) + UMCCH_BASE_ADDR;\n\t\tumc_base_reg_sec = get_umc_base(umc) + UMCCH_BASE_ADDR_SEC;\n\n\t\tfor_each_chip_select(cs, umc, pvt) {\n\t\t\tbase = &pvt->csels[umc].csbases[cs];\n\t\t\tbase_sec = &pvt->csels[umc].csbases_sec[cs];\n\n\t\t\tbase_reg = umc_base_reg + (cs * 4);\n\t\t\tbase_reg_sec = umc_base_reg_sec + (cs * 4);\n\n\t\t\tif (!amd_smn_read(pvt->mc_node_id, base_reg, base))\n\t\t\t\tedac_dbg(0, \"  DCSB%d[%d]=0x%08x reg: 0x%x\\n\",\n\t\t\t\t\t umc, cs, *base, base_reg);\n\n\t\t\tif (!amd_smn_read(pvt->mc_node_id, base_reg_sec, base_sec))\n\t\t\t\tedac_dbg(0, \"    DCSB_SEC%d[%d]=0x%08x reg: 0x%x\\n\",\n\t\t\t\t\t umc, cs, *base_sec, base_reg_sec);\n\t\t}\n\n\t\tumc_mask_reg = get_umc_base(umc) + UMCCH_ADDR_MASK;\n\t\tumc_mask_reg_sec = get_umc_base(umc) + get_umc_reg(pvt, UMCCH_ADDR_MASK_SEC);\n\n\t\tfor_each_chip_select_mask(cs, umc, pvt) {\n\t\t\tmask = &pvt->csels[umc].csmasks[cs];\n\t\t\tmask_sec = &pvt->csels[umc].csmasks_sec[cs];\n\n\t\t\tmask_reg = umc_mask_reg + (cs * 4);\n\t\t\tmask_reg_sec = umc_mask_reg_sec + (cs * 4);\n\n\t\t\tif (!amd_smn_read(pvt->mc_node_id, mask_reg, mask))\n\t\t\t\tedac_dbg(0, \"  DCSM%d[%d]=0x%08x reg: 0x%x\\n\",\n\t\t\t\t\t umc, cs, *mask, mask_reg);\n\n\t\t\tif (!amd_smn_read(pvt->mc_node_id, mask_reg_sec, mask_sec))\n\t\t\t\tedac_dbg(0, \"    DCSM_SEC%d[%d]=0x%08x reg: 0x%x\\n\",\n\t\t\t\t\t umc, cs, *mask_sec, mask_reg_sec);\n\t\t}\n\t}\n}\n\n \nstatic void dct_read_base_mask(struct amd64_pvt *pvt)\n{\n\tint cs;\n\n\tfor_each_chip_select(cs, 0, pvt) {\n\t\tint reg0   = DCSB0 + (cs * 4);\n\t\tint reg1   = DCSB1 + (cs * 4);\n\t\tu32 *base0 = &pvt->csels[0].csbases[cs];\n\t\tu32 *base1 = &pvt->csels[1].csbases[cs];\n\n\t\tif (!amd64_read_dct_pci_cfg(pvt, 0, reg0, base0))\n\t\t\tedac_dbg(0, \"  DCSB0[%d]=0x%08x reg: F2x%x\\n\",\n\t\t\t\t cs, *base0, reg0);\n\n\t\tif (pvt->fam == 0xf)\n\t\t\tcontinue;\n\n\t\tif (!amd64_read_dct_pci_cfg(pvt, 1, reg0, base1))\n\t\t\tedac_dbg(0, \"  DCSB1[%d]=0x%08x reg: F2x%x\\n\",\n\t\t\t\t cs, *base1, (pvt->fam == 0x10) ? reg1\n\t\t\t\t\t\t\t: reg0);\n\t}\n\n\tfor_each_chip_select_mask(cs, 0, pvt) {\n\t\tint reg0   = DCSM0 + (cs * 4);\n\t\tint reg1   = DCSM1 + (cs * 4);\n\t\tu32 *mask0 = &pvt->csels[0].csmasks[cs];\n\t\tu32 *mask1 = &pvt->csels[1].csmasks[cs];\n\n\t\tif (!amd64_read_dct_pci_cfg(pvt, 0, reg0, mask0))\n\t\t\tedac_dbg(0, \"    DCSM0[%d]=0x%08x reg: F2x%x\\n\",\n\t\t\t\t cs, *mask0, reg0);\n\n\t\tif (pvt->fam == 0xf)\n\t\t\tcontinue;\n\n\t\tif (!amd64_read_dct_pci_cfg(pvt, 1, reg0, mask1))\n\t\t\tedac_dbg(0, \"    DCSM1[%d]=0x%08x reg: F2x%x\\n\",\n\t\t\t\t cs, *mask1, (pvt->fam == 0x10) ? reg1\n\t\t\t\t\t\t\t: reg0);\n\t}\n}\n\nstatic void umc_determine_memory_type(struct amd64_pvt *pvt)\n{\n\tstruct amd64_umc *umc;\n\tu32 i;\n\n\tfor_each_umc(i) {\n\t\tumc = &pvt->umc[i];\n\n\t\tif (!(umc->sdp_ctrl & UMC_SDP_INIT)) {\n\t\t\tumc->dram_type = MEM_EMPTY;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (pvt->flags.zn_regs_v2 && ((umc->umc_cfg & GENMASK(2, 0)) == 0x1)) {\n\t\t\tif (umc->dimm_cfg & BIT(5))\n\t\t\t\tumc->dram_type = MEM_LRDDR5;\n\t\t\telse if (umc->dimm_cfg & BIT(4))\n\t\t\t\tumc->dram_type = MEM_RDDR5;\n\t\t\telse\n\t\t\t\tumc->dram_type = MEM_DDR5;\n\t\t} else {\n\t\t\tif (umc->dimm_cfg & BIT(5))\n\t\t\t\tumc->dram_type = MEM_LRDDR4;\n\t\t\telse if (umc->dimm_cfg & BIT(4))\n\t\t\t\tumc->dram_type = MEM_RDDR4;\n\t\t\telse\n\t\t\t\tumc->dram_type = MEM_DDR4;\n\t\t}\n\n\t\tedac_dbg(1, \"  UMC%d DIMM type: %s\\n\", i, edac_mem_types[umc->dram_type]);\n\t}\n}\n\nstatic void dct_determine_memory_type(struct amd64_pvt *pvt)\n{\n\tu32 dram_ctrl, dcsm;\n\n\tswitch (pvt->fam) {\n\tcase 0xf:\n\t\tif (pvt->ext_model >= K8_REV_F)\n\t\t\tgoto ddr3;\n\n\t\tpvt->dram_type = (pvt->dclr0 & BIT(18)) ? MEM_DDR : MEM_RDDR;\n\t\treturn;\n\n\tcase 0x10:\n\t\tif (pvt->dchr0 & DDR3_MODE)\n\t\t\tgoto ddr3;\n\n\t\tpvt->dram_type = (pvt->dclr0 & BIT(16)) ? MEM_DDR2 : MEM_RDDR2;\n\t\treturn;\n\n\tcase 0x15:\n\t\tif (pvt->model < 0x60)\n\t\t\tgoto ddr3;\n\n\t\t \n\t\tamd64_read_dct_pci_cfg(pvt, 0, DRAM_CONTROL, &dram_ctrl);\n\t\tdcsm = pvt->csels[0].csmasks[0];\n\n\t\tif (((dram_ctrl >> 8) & 0x7) == 0x2)\n\t\t\tpvt->dram_type = MEM_DDR4;\n\t\telse if (pvt->dclr0 & BIT(16))\n\t\t\tpvt->dram_type = MEM_DDR3;\n\t\telse if (dcsm & 0x3)\n\t\t\tpvt->dram_type = MEM_LRDDR3;\n\t\telse\n\t\t\tpvt->dram_type = MEM_RDDR3;\n\n\t\treturn;\n\n\tcase 0x16:\n\t\tgoto ddr3;\n\n\tdefault:\n\t\tWARN(1, KERN_ERR \"%s: Family??? 0x%x\\n\", __func__, pvt->fam);\n\t\tpvt->dram_type = MEM_EMPTY;\n\t}\n\n\tedac_dbg(1, \"  DIMM type: %s\\n\", edac_mem_types[pvt->dram_type]);\n\treturn;\n\nddr3:\n\tpvt->dram_type = (pvt->dclr0 & BIT(16)) ? MEM_DDR3 : MEM_RDDR3;\n}\n\n \nstatic u64 get_error_address(struct amd64_pvt *pvt, struct mce *m)\n{\n\tu16 mce_nid = topology_die_id(m->extcpu);\n\tstruct mem_ctl_info *mci;\n\tu8 start_bit = 1;\n\tu8 end_bit   = 47;\n\tu64 addr;\n\n\tmci = edac_mc_find(mce_nid);\n\tif (!mci)\n\t\treturn 0;\n\n\tpvt = mci->pvt_info;\n\n\tif (pvt->fam == 0xf) {\n\t\tstart_bit = 3;\n\t\tend_bit   = 39;\n\t}\n\n\taddr = m->addr & GENMASK_ULL(end_bit, start_bit);\n\n\t \n\tif (pvt->fam == 0x15) {\n\t\tu64 cc6_base, tmp_addr;\n\t\tu32 tmp;\n\t\tu8 intlv_en;\n\n\t\tif ((addr & GENMASK_ULL(47, 24)) >> 24 != 0x00fdf7)\n\t\t\treturn addr;\n\n\n\t\tamd64_read_pci_cfg(pvt->F1, DRAM_LOCAL_NODE_LIM, &tmp);\n\t\tintlv_en = tmp >> 21 & 0x7;\n\n\t\t \n\t\tcc6_base  = (tmp & GENMASK_ULL(20, 0)) << 3;\n\n\t\t \n\t\tcc6_base |= intlv_en ^ 0x7;\n\n\t\t \n\t\tcc6_base <<= 24;\n\n\t\tif (!intlv_en)\n\t\t\treturn cc6_base | (addr & GENMASK_ULL(23, 0));\n\n\t\tamd64_read_pci_cfg(pvt->F1, DRAM_LOCAL_NODE_BASE, &tmp);\n\n\t\t\t\t\t\t\t \n\t\ttmp_addr  = (addr & GENMASK_ULL(23, 12)) << __fls(intlv_en + 1);\n\n\t\t \n\t\ttmp_addr |= (tmp & GENMASK_ULL(23, 21)) >> 9;\n\n\t\t \n\t\ttmp_addr |= addr & GENMASK_ULL(11, 0);\n\n\t\treturn cc6_base | tmp_addr;\n\t}\n\n\treturn addr;\n}\n\nstatic struct pci_dev *pci_get_related_function(unsigned int vendor,\n\t\t\t\t\t\tunsigned int device,\n\t\t\t\t\t\tstruct pci_dev *related)\n{\n\tstruct pci_dev *dev = NULL;\n\n\twhile ((dev = pci_get_device(vendor, device, dev))) {\n\t\tif (pci_domain_nr(dev->bus) == pci_domain_nr(related->bus) &&\n\t\t    (dev->bus->number == related->bus->number) &&\n\t\t    (PCI_SLOT(dev->devfn) == PCI_SLOT(related->devfn)))\n\t\t\tbreak;\n\t}\n\n\treturn dev;\n}\n\nstatic void read_dram_base_limit_regs(struct amd64_pvt *pvt, unsigned range)\n{\n\tstruct amd_northbridge *nb;\n\tstruct pci_dev *f1 = NULL;\n\tunsigned int pci_func;\n\tint off = range << 3;\n\tu32 llim;\n\n\tamd64_read_pci_cfg(pvt->F1, DRAM_BASE_LO + off,  &pvt->ranges[range].base.lo);\n\tamd64_read_pci_cfg(pvt->F1, DRAM_LIMIT_LO + off, &pvt->ranges[range].lim.lo);\n\n\tif (pvt->fam == 0xf)\n\t\treturn;\n\n\tif (!dram_rw(pvt, range))\n\t\treturn;\n\n\tamd64_read_pci_cfg(pvt->F1, DRAM_BASE_HI + off,  &pvt->ranges[range].base.hi);\n\tamd64_read_pci_cfg(pvt->F1, DRAM_LIMIT_HI + off, &pvt->ranges[range].lim.hi);\n\n\t \n\tif (pvt->fam != 0x15)\n\t\treturn;\n\n\tnb = node_to_amd_nb(dram_dst_node(pvt, range));\n\tif (WARN_ON(!nb))\n\t\treturn;\n\n\tif (pvt->model == 0x60)\n\t\tpci_func = PCI_DEVICE_ID_AMD_15H_M60H_NB_F1;\n\telse if (pvt->model == 0x30)\n\t\tpci_func = PCI_DEVICE_ID_AMD_15H_M30H_NB_F1;\n\telse\n\t\tpci_func = PCI_DEVICE_ID_AMD_15H_NB_F1;\n\n\tf1 = pci_get_related_function(nb->misc->vendor, pci_func, nb->misc);\n\tif (WARN_ON(!f1))\n\t\treturn;\n\n\tamd64_read_pci_cfg(f1, DRAM_LOCAL_NODE_LIM, &llim);\n\n\tpvt->ranges[range].lim.lo &= GENMASK_ULL(15, 0);\n\n\t\t\t\t     \n\tpvt->ranges[range].lim.lo |= ((llim & 0x1fff) << 3 | 0x7) << 16;\n\n\tpvt->ranges[range].lim.hi &= GENMASK_ULL(7, 0);\n\n\t\t\t\t     \n\tpvt->ranges[range].lim.hi |= llim >> 13;\n\n\tpci_dev_put(f1);\n}\n\nstatic void k8_map_sysaddr_to_csrow(struct mem_ctl_info *mci, u64 sys_addr,\n\t\t\t\t    struct err_info *err)\n{\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\n\terror_address_to_page_and_offset(sys_addr, err);\n\n\t \n\terr->src_mci = find_mc_by_sys_addr(mci, sys_addr);\n\tif (!err->src_mci) {\n\t\tamd64_mc_err(mci, \"failed to map error addr 0x%lx to a node\\n\",\n\t\t\t     (unsigned long)sys_addr);\n\t\terr->err_code = ERR_NODE;\n\t\treturn;\n\t}\n\n\t \n\terr->csrow = sys_addr_to_csrow(err->src_mci, sys_addr);\n\tif (err->csrow < 0) {\n\t\terr->err_code = ERR_CSROW;\n\t\treturn;\n\t}\n\n\t \n\tif (pvt->nbcfg & NBCFG_CHIPKILL) {\n\t\terr->channel = get_channel_from_ecc_syndrome(mci, err->syndrome);\n\t\tif (err->channel < 0) {\n\t\t\t \n\t\t\tamd64_mc_warn(err->src_mci, \"unknown syndrome 0x%04x - \"\n\t\t\t\t      \"possible error reporting race\\n\",\n\t\t\t\t      err->syndrome);\n\t\t\terr->err_code = ERR_CHANNEL;\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\t \n\t\terr->channel = ((sys_addr & BIT(3)) != 0);\n\t}\n}\n\nstatic int ddr2_cs_size(unsigned i, bool dct_width)\n{\n\tunsigned shift = 0;\n\n\tif (i <= 2)\n\t\tshift = i;\n\telse if (!(i & 0x1))\n\t\tshift = i >> 1;\n\telse\n\t\tshift = (i + 1) >> 1;\n\n\treturn 128 << (shift + !!dct_width);\n}\n\nstatic int k8_dbam_to_chip_select(struct amd64_pvt *pvt, u8 dct,\n\t\t\t\t  unsigned cs_mode, int cs_mask_nr)\n{\n\tu32 dclr = dct ? pvt->dclr1 : pvt->dclr0;\n\n\tif (pvt->ext_model >= K8_REV_F) {\n\t\tWARN_ON(cs_mode > 11);\n\t\treturn ddr2_cs_size(cs_mode, dclr & WIDTH_128);\n\t}\n\telse if (pvt->ext_model >= K8_REV_D) {\n\t\tunsigned diff;\n\t\tWARN_ON(cs_mode > 10);\n\n\t\t \n\t\tdiff = cs_mode/3 + (unsigned)(cs_mode > 5);\n\n\t\treturn 32 << (cs_mode - diff);\n\t}\n\telse {\n\t\tWARN_ON(cs_mode > 6);\n\t\treturn 32 << cs_mode;\n\t}\n}\n\nstatic int ddr3_cs_size(unsigned i, bool dct_width)\n{\n\tunsigned shift = 0;\n\tint cs_size = 0;\n\n\tif (i == 0 || i == 3 || i == 4)\n\t\tcs_size = -1;\n\telse if (i <= 2)\n\t\tshift = i;\n\telse if (i == 12)\n\t\tshift = 7;\n\telse if (!(i & 0x1))\n\t\tshift = i >> 1;\n\telse\n\t\tshift = (i + 1) >> 1;\n\n\tif (cs_size != -1)\n\t\tcs_size = (128 * (1 << !!dct_width)) << shift;\n\n\treturn cs_size;\n}\n\nstatic int ddr3_lrdimm_cs_size(unsigned i, unsigned rank_multiply)\n{\n\tunsigned shift = 0;\n\tint cs_size = 0;\n\n\tif (i < 4 || i == 6)\n\t\tcs_size = -1;\n\telse if (i == 12)\n\t\tshift = 7;\n\telse if (!(i & 0x1))\n\t\tshift = i >> 1;\n\telse\n\t\tshift = (i + 1) >> 1;\n\n\tif (cs_size != -1)\n\t\tcs_size = rank_multiply * (128 << shift);\n\n\treturn cs_size;\n}\n\nstatic int ddr4_cs_size(unsigned i)\n{\n\tint cs_size = 0;\n\n\tif (i == 0)\n\t\tcs_size = -1;\n\telse if (i == 1)\n\t\tcs_size = 1024;\n\telse\n\t\t \n\t\tcs_size = 1024 * (1 << (i >> 1));\n\n\treturn cs_size;\n}\n\nstatic int f10_dbam_to_chip_select(struct amd64_pvt *pvt, u8 dct,\n\t\t\t\t   unsigned cs_mode, int cs_mask_nr)\n{\n\tu32 dclr = dct ? pvt->dclr1 : pvt->dclr0;\n\n\tWARN_ON(cs_mode > 11);\n\n\tif (pvt->dchr0 & DDR3_MODE || pvt->dchr1 & DDR3_MODE)\n\t\treturn ddr3_cs_size(cs_mode, dclr & WIDTH_128);\n\telse\n\t\treturn ddr2_cs_size(cs_mode, dclr & WIDTH_128);\n}\n\n \nstatic int f15_dbam_to_chip_select(struct amd64_pvt *pvt, u8 dct,\n\t\t\t\t   unsigned cs_mode, int cs_mask_nr)\n{\n\tWARN_ON(cs_mode > 12);\n\n\treturn ddr3_cs_size(cs_mode, false);\n}\n\n \nstatic int f15_m60h_dbam_to_chip_select(struct amd64_pvt *pvt, u8 dct,\n\t\t\t\t\tunsigned cs_mode, int cs_mask_nr)\n{\n\tint cs_size;\n\tu32 dcsm = pvt->csels[dct].csmasks[cs_mask_nr];\n\n\tWARN_ON(cs_mode > 12);\n\n\tif (pvt->dram_type == MEM_DDR4) {\n\t\tif (cs_mode > 9)\n\t\t\treturn -1;\n\n\t\tcs_size = ddr4_cs_size(cs_mode);\n\t} else if (pvt->dram_type == MEM_LRDDR3) {\n\t\tunsigned rank_multiply = dcsm & 0xf;\n\n\t\tif (rank_multiply == 3)\n\t\t\trank_multiply = 4;\n\t\tcs_size = ddr3_lrdimm_cs_size(cs_mode, rank_multiply);\n\t} else {\n\t\t \n\t\tif (cs_mode == 0x1)\n\t\t\treturn -1;\n\n\t\tcs_size = ddr3_cs_size(cs_mode, false);\n\t}\n\n\treturn cs_size;\n}\n\n \nstatic int f16_dbam_to_chip_select(struct amd64_pvt *pvt, u8 dct,\n\t\t\t\tunsigned cs_mode, int cs_mask_nr)\n{\n\tWARN_ON(cs_mode > 12);\n\n\tif (cs_mode == 6 || cs_mode == 8 ||\n\t    cs_mode == 9 || cs_mode == 12)\n\t\treturn -1;\n\telse\n\t\treturn ddr3_cs_size(cs_mode, false);\n}\n\nstatic void read_dram_ctl_register(struct amd64_pvt *pvt)\n{\n\n\tif (pvt->fam == 0xf)\n\t\treturn;\n\n\tif (!amd64_read_pci_cfg(pvt->F2, DCT_SEL_LO, &pvt->dct_sel_lo)) {\n\t\tedac_dbg(0, \"F2x110 (DCTSelLow): 0x%08x, High range addrs at: 0x%x\\n\",\n\t\t\t pvt->dct_sel_lo, dct_sel_baseaddr(pvt));\n\n\t\tedac_dbg(0, \"  DCTs operate in %s mode\\n\",\n\t\t\t (dct_ganging_enabled(pvt) ? \"ganged\" : \"unganged\"));\n\n\t\tif (!dct_ganging_enabled(pvt))\n\t\t\tedac_dbg(0, \"  Address range split per DCT: %s\\n\",\n\t\t\t\t (dct_high_range_enabled(pvt) ? \"yes\" : \"no\"));\n\n\t\tedac_dbg(0, \"  data interleave for ECC: %s, DRAM cleared since last warm reset: %s\\n\",\n\t\t\t (dct_data_intlv_enabled(pvt) ? \"enabled\" : \"disabled\"),\n\t\t\t (dct_memory_cleared(pvt) ? \"yes\" : \"no\"));\n\n\t\tedac_dbg(0, \"  channel interleave: %s, \"\n\t\t\t \"interleave bits selector: 0x%x\\n\",\n\t\t\t (dct_interleave_enabled(pvt) ? \"enabled\" : \"disabled\"),\n\t\t\t dct_sel_interleave_addr(pvt));\n\t}\n\n\tamd64_read_pci_cfg(pvt->F2, DCT_SEL_HI, &pvt->dct_sel_hi);\n}\n\n \nstatic u8 f15_m30h_determine_channel(struct amd64_pvt *pvt, u64 sys_addr,\n\t\t\t\t     u8 intlv_en, int num_dcts_intlv,\n\t\t\t\t     u32 dct_sel)\n{\n\tu8 channel = 0;\n\tu8 select;\n\n\tif (!(intlv_en))\n\t\treturn (u8)(dct_sel);\n\n\tif (num_dcts_intlv == 2) {\n\t\tselect = (sys_addr >> 8) & 0x3;\n\t\tchannel = select ? 0x3 : 0;\n\t} else if (num_dcts_intlv == 4) {\n\t\tu8 intlv_addr = dct_sel_interleave_addr(pvt);\n\t\tswitch (intlv_addr) {\n\t\tcase 0x4:\n\t\t\tchannel = (sys_addr >> 8) & 0x3;\n\t\t\tbreak;\n\t\tcase 0x5:\n\t\t\tchannel = (sys_addr >> 9) & 0x3;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn channel;\n}\n\n \nstatic u8 f1x_determine_channel(struct amd64_pvt *pvt, u64 sys_addr,\n\t\t\t\tbool hi_range_sel, u8 intlv_en)\n{\n\tu8 dct_sel_high = (pvt->dct_sel_lo >> 1) & 1;\n\n\tif (dct_ganging_enabled(pvt))\n\t\treturn 0;\n\n\tif (hi_range_sel)\n\t\treturn dct_sel_high;\n\n\t \n\tif (dct_interleave_enabled(pvt)) {\n\t\tu8 intlv_addr = dct_sel_interleave_addr(pvt);\n\n\t\t \n\t\tif (!intlv_addr)\n\t\t\treturn sys_addr >> 6 & 1;\n\n\t\tif (intlv_addr & 0x2) {\n\t\t\tu8 shift = intlv_addr & 0x1 ? 9 : 6;\n\t\t\tu32 temp = hweight_long((u32) ((sys_addr >> 16) & 0x1F)) & 1;\n\n\t\t\treturn ((sys_addr >> shift) & 1) ^ temp;\n\t\t}\n\n\t\tif (intlv_addr & 0x4) {\n\t\t\tu8 shift = intlv_addr & 0x1 ? 9 : 8;\n\n\t\t\treturn (sys_addr >> shift) & 1;\n\t\t}\n\n\t\treturn (sys_addr >> (12 + hweight8(intlv_en))) & 1;\n\t}\n\n\tif (dct_high_range_enabled(pvt))\n\t\treturn ~dct_sel_high & 1;\n\n\treturn 0;\n}\n\n \nstatic u64 f1x_get_norm_dct_addr(struct amd64_pvt *pvt, u8 range,\n\t\t\t\t u64 sys_addr, bool hi_rng,\n\t\t\t\t u32 dct_sel_base_addr)\n{\n\tu64 chan_off;\n\tu64 dram_base\t\t= get_dram_base(pvt, range);\n\tu64 hole_off\t\t= f10_dhar_offset(pvt);\n\tu64 dct_sel_base_off\t= (u64)(pvt->dct_sel_hi & 0xFFFFFC00) << 16;\n\n\tif (hi_rng) {\n\t\t \n\t\tif ((!(dct_sel_base_addr >> 16) ||\n\t\t     dct_sel_base_addr < dhar_base(pvt)) &&\n\t\t    dhar_valid(pvt) &&\n\t\t    (sys_addr >= BIT_64(32)))\n\t\t\tchan_off = hole_off;\n\t\telse\n\t\t\tchan_off = dct_sel_base_off;\n\t} else {\n\t\t \n\t\tif (dhar_valid(pvt) && (sys_addr >= BIT_64(32)))\n\t\t\tchan_off = hole_off;\n\t\telse\n\t\t\tchan_off = dram_base;\n\t}\n\n\treturn (sys_addr & GENMASK_ULL(47,6)) - (chan_off & GENMASK_ULL(47,23));\n}\n\n \nstatic int f10_process_possible_spare(struct amd64_pvt *pvt, u8 dct, int csrow)\n{\n\tint tmp_cs;\n\n\tif (online_spare_swap_done(pvt, dct) &&\n\t    csrow == online_spare_bad_dramcs(pvt, dct)) {\n\n\t\tfor_each_chip_select(tmp_cs, dct, pvt) {\n\t\t\tif (chip_select_base(tmp_cs, dct, pvt) & 0x2) {\n\t\t\t\tcsrow = tmp_cs;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\treturn csrow;\n}\n\n \nstatic int f1x_lookup_addr_in_dct(u64 in_addr, u8 nid, u8 dct)\n{\n\tstruct mem_ctl_info *mci;\n\tstruct amd64_pvt *pvt;\n\tu64 cs_base, cs_mask;\n\tint cs_found = -EINVAL;\n\tint csrow;\n\n\tmci = edac_mc_find(nid);\n\tif (!mci)\n\t\treturn cs_found;\n\n\tpvt = mci->pvt_info;\n\n\tedac_dbg(1, \"input addr: 0x%llx, DCT: %d\\n\", in_addr, dct);\n\n\tfor_each_chip_select(csrow, dct, pvt) {\n\t\tif (!csrow_enabled(csrow, dct, pvt))\n\t\t\tcontinue;\n\n\t\tget_cs_base_and_mask(pvt, csrow, dct, &cs_base, &cs_mask);\n\n\t\tedac_dbg(1, \"    CSROW=%d CSBase=0x%llx CSMask=0x%llx\\n\",\n\t\t\t csrow, cs_base, cs_mask);\n\n\t\tcs_mask = ~cs_mask;\n\n\t\tedac_dbg(1, \"    (InputAddr & ~CSMask)=0x%llx (CSBase & ~CSMask)=0x%llx\\n\",\n\t\t\t (in_addr & cs_mask), (cs_base & cs_mask));\n\n\t\tif ((in_addr & cs_mask) == (cs_base & cs_mask)) {\n\t\t\tif (pvt->fam == 0x15 && pvt->model >= 0x30) {\n\t\t\t\tcs_found =  csrow;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcs_found = f10_process_possible_spare(pvt, dct, csrow);\n\n\t\t\tedac_dbg(1, \" MATCH csrow=%d\\n\", cs_found);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn cs_found;\n}\n\n \nstatic u64 f1x_swap_interleaved_region(struct amd64_pvt *pvt, u64 sys_addr)\n{\n\tu32 swap_reg, swap_base, swap_limit, rgn_size, tmp_addr;\n\n\tif (pvt->fam == 0x10) {\n\t\t \n\t\tif (pvt->model < 4 || (pvt->model < 0xa && pvt->stepping < 3))\n\t\t\treturn sys_addr;\n\t}\n\n\tamd64_read_pci_cfg(pvt->F2, SWAP_INTLV_REG, &swap_reg);\n\n\tif (!(swap_reg & 0x1))\n\t\treturn sys_addr;\n\n\tswap_base\t= (swap_reg >> 3) & 0x7f;\n\tswap_limit\t= (swap_reg >> 11) & 0x7f;\n\trgn_size\t= (swap_reg >> 20) & 0x7f;\n\ttmp_addr\t= sys_addr >> 27;\n\n\tif (!(sys_addr >> 34) &&\n\t    (((tmp_addr >= swap_base) &&\n\t     (tmp_addr <= swap_limit)) ||\n\t     (tmp_addr < rgn_size)))\n\t\treturn sys_addr ^ (u64)swap_base << 27;\n\n\treturn sys_addr;\n}\n\n \nstatic int f1x_match_to_this_node(struct amd64_pvt *pvt, unsigned range,\n\t\t\t\t  u64 sys_addr, int *chan_sel)\n{\n\tint cs_found = -EINVAL;\n\tu64 chan_addr;\n\tu32 dct_sel_base;\n\tu8 channel;\n\tbool high_range = false;\n\n\tu8 node_id    = dram_dst_node(pvt, range);\n\tu8 intlv_en   = dram_intlv_en(pvt, range);\n\tu32 intlv_sel = dram_intlv_sel(pvt, range);\n\n\tedac_dbg(1, \"(range %d) SystemAddr= 0x%llx Limit=0x%llx\\n\",\n\t\t range, sys_addr, get_dram_limit(pvt, range));\n\n\tif (dhar_valid(pvt) &&\n\t    dhar_base(pvt) <= sys_addr &&\n\t    sys_addr < BIT_64(32)) {\n\t\tamd64_warn(\"Huh? Address is in the MMIO hole: 0x%016llx\\n\",\n\t\t\t    sys_addr);\n\t\treturn -EINVAL;\n\t}\n\n\tif (intlv_en && (intlv_sel != ((sys_addr >> 12) & intlv_en)))\n\t\treturn -EINVAL;\n\n\tsys_addr = f1x_swap_interleaved_region(pvt, sys_addr);\n\n\tdct_sel_base = dct_sel_baseaddr(pvt);\n\n\t \n\tif (dct_high_range_enabled(pvt) &&\n\t   !dct_ganging_enabled(pvt) &&\n\t   ((sys_addr >> 27) >= (dct_sel_base >> 11)))\n\t\thigh_range = true;\n\n\tchannel = f1x_determine_channel(pvt, sys_addr, high_range, intlv_en);\n\n\tchan_addr = f1x_get_norm_dct_addr(pvt, range, sys_addr,\n\t\t\t\t\t  high_range, dct_sel_base);\n\n\t \n\tif (intlv_en)\n\t\tchan_addr = ((chan_addr >> (12 + hweight8(intlv_en))) << 12) |\n\t\t\t    (chan_addr & 0xfff);\n\n\t \n\tif (dct_interleave_enabled(pvt) &&\n\t   !dct_high_range_enabled(pvt) &&\n\t   !dct_ganging_enabled(pvt)) {\n\n\t\tif (dct_sel_interleave_addr(pvt) != 1) {\n\t\t\tif (dct_sel_interleave_addr(pvt) == 0x3)\n\t\t\t\t \n\t\t\t\tchan_addr = ((chan_addr >> 10) << 9) |\n\t\t\t\t\t     (chan_addr & 0x1ff);\n\t\t\telse\n\t\t\t\t \n\t\t\t\tchan_addr = ((chan_addr >> 7) << 6) |\n\t\t\t\t\t     (chan_addr & 0x3f);\n\t\t} else\n\t\t\t \n\t\t\tchan_addr = ((chan_addr >> 13) << 12) |\n\t\t\t\t     (chan_addr & 0xfff);\n\t}\n\n\tedac_dbg(1, \"   Normalized DCT addr: 0x%llx\\n\", chan_addr);\n\n\tcs_found = f1x_lookup_addr_in_dct(chan_addr, node_id, channel);\n\n\tif (cs_found >= 0)\n\t\t*chan_sel = channel;\n\n\treturn cs_found;\n}\n\nstatic int f15_m30h_match_to_this_node(struct amd64_pvt *pvt, unsigned range,\n\t\t\t\t\tu64 sys_addr, int *chan_sel)\n{\n\tint cs_found = -EINVAL;\n\tint num_dcts_intlv = 0;\n\tu64 chan_addr, chan_offset;\n\tu64 dct_base, dct_limit;\n\tu32 dct_cont_base_reg, dct_cont_limit_reg, tmp;\n\tu8 channel, alias_channel, leg_mmio_hole, dct_sel, dct_offset_en;\n\n\tu64 dhar_offset\t\t= f10_dhar_offset(pvt);\n\tu8 intlv_addr\t\t= dct_sel_interleave_addr(pvt);\n\tu8 node_id\t\t= dram_dst_node(pvt, range);\n\tu8 intlv_en\t\t= dram_intlv_en(pvt, range);\n\n\tamd64_read_pci_cfg(pvt->F1, DRAM_CONT_BASE, &dct_cont_base_reg);\n\tamd64_read_pci_cfg(pvt->F1, DRAM_CONT_LIMIT, &dct_cont_limit_reg);\n\n\tdct_offset_en\t\t= (u8) ((dct_cont_base_reg >> 3) & BIT(0));\n\tdct_sel\t\t\t= (u8) ((dct_cont_base_reg >> 4) & 0x7);\n\n\tedac_dbg(1, \"(range %d) SystemAddr= 0x%llx Limit=0x%llx\\n\",\n\t\t range, sys_addr, get_dram_limit(pvt, range));\n\n\tif (!(get_dram_base(pvt, range)  <= sys_addr) &&\n\t    !(get_dram_limit(pvt, range) >= sys_addr))\n\t\treturn -EINVAL;\n\n\tif (dhar_valid(pvt) &&\n\t    dhar_base(pvt) <= sys_addr &&\n\t    sys_addr < BIT_64(32)) {\n\t\tamd64_warn(\"Huh? Address is in the MMIO hole: 0x%016llx\\n\",\n\t\t\t    sys_addr);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tdct_base = (u64) dct_sel_baseaddr(pvt);\n\tdct_limit = (dct_cont_limit_reg >> 11) & 0x1FFF;\n\n\tif (!(dct_cont_base_reg & BIT(0)) &&\n\t    !(dct_base <= (sys_addr >> 27) &&\n\t      dct_limit >= (sys_addr >> 27)))\n\t\treturn -EINVAL;\n\n\t \n\tnum_dcts_intlv = (int) hweight8(intlv_en);\n\n\tif (!(num_dcts_intlv % 2 == 0) || (num_dcts_intlv > 4))\n\t\treturn -EINVAL;\n\n\tif (pvt->model >= 0x60)\n\t\tchannel = f1x_determine_channel(pvt, sys_addr, false, intlv_en);\n\telse\n\t\tchannel = f15_m30h_determine_channel(pvt, sys_addr, intlv_en,\n\t\t\t\t\t\t     num_dcts_intlv, dct_sel);\n\n\t \n\tif (channel > 3)\n\t\treturn -EINVAL;\n\n\tleg_mmio_hole = (u8) (dct_cont_base_reg >> 1 & BIT(0));\n\n\t \n\tif (leg_mmio_hole && (sys_addr >= BIT_64(32)))\n\t\tchan_offset = dhar_offset;\n\telse\n\t\tchan_offset = dct_base << 27;\n\n\tchan_addr = sys_addr - chan_offset;\n\n\t \n\tif (num_dcts_intlv == 2) {\n\t\tif (intlv_addr == 0x4)\n\t\t\tchan_addr = ((chan_addr >> 9) << 8) |\n\t\t\t\t\t\t(chan_addr & 0xff);\n\t\telse if (intlv_addr == 0x5)\n\t\t\tchan_addr = ((chan_addr >> 10) << 9) |\n\t\t\t\t\t\t(chan_addr & 0x1ff);\n\t\telse\n\t\t\treturn -EINVAL;\n\n\t} else if (num_dcts_intlv == 4) {\n\t\tif (intlv_addr == 0x4)\n\t\t\tchan_addr = ((chan_addr >> 10) << 8) |\n\t\t\t\t\t\t\t(chan_addr & 0xff);\n\t\telse if (intlv_addr == 0x5)\n\t\t\tchan_addr = ((chan_addr >> 11) << 9) |\n\t\t\t\t\t\t\t(chan_addr & 0x1ff);\n\t\telse\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (dct_offset_en) {\n\t\tamd64_read_pci_cfg(pvt->F1,\n\t\t\t\t   DRAM_CONT_HIGH_OFF + (int) channel * 4,\n\t\t\t\t   &tmp);\n\t\tchan_addr +=  (u64) ((tmp >> 11) & 0xfff) << 27;\n\t}\n\n\tf15h_select_dct(pvt, channel);\n\n\tedac_dbg(1, \"   Normalized DCT addr: 0x%llx\\n\", chan_addr);\n\n\t \n\talias_channel =  (channel == 3) ? 1 : channel;\n\n\tcs_found = f1x_lookup_addr_in_dct(chan_addr, node_id, alias_channel);\n\n\tif (cs_found >= 0)\n\t\t*chan_sel = alias_channel;\n\n\treturn cs_found;\n}\n\nstatic int f1x_translate_sysaddr_to_cs(struct amd64_pvt *pvt,\n\t\t\t\t\tu64 sys_addr,\n\t\t\t\t\tint *chan_sel)\n{\n\tint cs_found = -EINVAL;\n\tunsigned range;\n\n\tfor (range = 0; range < DRAM_RANGES; range++) {\n\t\tif (!dram_rw(pvt, range))\n\t\t\tcontinue;\n\n\t\tif (pvt->fam == 0x15 && pvt->model >= 0x30)\n\t\t\tcs_found = f15_m30h_match_to_this_node(pvt, range,\n\t\t\t\t\t\t\t       sys_addr,\n\t\t\t\t\t\t\t       chan_sel);\n\n\t\telse if ((get_dram_base(pvt, range)  <= sys_addr) &&\n\t\t\t (get_dram_limit(pvt, range) >= sys_addr)) {\n\t\t\tcs_found = f1x_match_to_this_node(pvt, range,\n\t\t\t\t\t\t\t  sys_addr, chan_sel);\n\t\t\tif (cs_found >= 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\treturn cs_found;\n}\n\n \nstatic void f1x_map_sysaddr_to_csrow(struct mem_ctl_info *mci, u64 sys_addr,\n\t\t\t\t     struct err_info *err)\n{\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\n\terror_address_to_page_and_offset(sys_addr, err);\n\n\terr->csrow = f1x_translate_sysaddr_to_cs(pvt, sys_addr, &err->channel);\n\tif (err->csrow < 0) {\n\t\terr->err_code = ERR_CSROW;\n\t\treturn;\n\t}\n\n\t \n\tif (dct_ganging_enabled(pvt))\n\t\terr->channel = get_channel_from_ecc_syndrome(mci, err->syndrome);\n}\n\n \nstatic const u16 x4_vectors[] = {\n\t0x2f57, 0x1afe, 0x66cc, 0xdd88,\n\t0x11eb, 0x3396, 0x7f4c, 0xeac8,\n\t0x0001, 0x0002, 0x0004, 0x0008,\n\t0x1013, 0x3032, 0x4044, 0x8088,\n\t0x106b, 0x30d6, 0x70fc, 0xe0a8,\n\t0x4857, 0xc4fe, 0x13cc, 0x3288,\n\t0x1ac5, 0x2f4a, 0x5394, 0xa1e8,\n\t0x1f39, 0x251e, 0xbd6c, 0x6bd8,\n\t0x15c1, 0x2a42, 0x89ac, 0x4758,\n\t0x2b03, 0x1602, 0x4f0c, 0xca08,\n\t0x1f07, 0x3a0e, 0x6b04, 0xbd08,\n\t0x8ba7, 0x465e, 0x244c, 0x1cc8,\n\t0x2b87, 0x164e, 0x642c, 0xdc18,\n\t0x40b9, 0x80de, 0x1094, 0x20e8,\n\t0x27db, 0x1eb6, 0x9dac, 0x7b58,\n\t0x11c1, 0x2242, 0x84ac, 0x4c58,\n\t0x1be5, 0x2d7a, 0x5e34, 0xa718,\n\t0x4b39, 0x8d1e, 0x14b4, 0x28d8,\n\t0x4c97, 0xc87e, 0x11fc, 0x33a8,\n\t0x8e97, 0x497e, 0x2ffc, 0x1aa8,\n\t0x16b3, 0x3d62, 0x4f34, 0x8518,\n\t0x1e2f, 0x391a, 0x5cac, 0xf858,\n\t0x1d9f, 0x3b7a, 0x572c, 0xfe18,\n\t0x15f5, 0x2a5a, 0x5264, 0xa3b8,\n\t0x1dbb, 0x3b66, 0x715c, 0xe3f8,\n\t0x4397, 0xc27e, 0x17fc, 0x3ea8,\n\t0x1617, 0x3d3e, 0x6464, 0xb8b8,\n\t0x23ff, 0x12aa, 0xab6c, 0x56d8,\n\t0x2dfb, 0x1ba6, 0x913c, 0x7328,\n\t0x185d, 0x2ca6, 0x7914, 0x9e28,\n\t0x171b, 0x3e36, 0x7d7c, 0xebe8,\n\t0x4199, 0x82ee, 0x19f4, 0x2e58,\n\t0x4807, 0xc40e, 0x130c, 0x3208,\n\t0x1905, 0x2e0a, 0x5804, 0xac08,\n\t0x213f, 0x132a, 0xadfc, 0x5ba8,\n\t0x19a9, 0x2efe, 0xb5cc, 0x6f88,\n};\n\nstatic const u16 x8_vectors[] = {\n\t0x0145, 0x028a, 0x2374, 0x43c8, 0xa1f0, 0x0520, 0x0a40, 0x1480,\n\t0x0211, 0x0422, 0x0844, 0x1088, 0x01b0, 0x44e0, 0x23c0, 0xed80,\n\t0x1011, 0x0116, 0x022c, 0x0458, 0x08b0, 0x8c60, 0x2740, 0x4e80,\n\t0x0411, 0x0822, 0x1044, 0x0158, 0x02b0, 0x2360, 0x46c0, 0xab80,\n\t0x0811, 0x1022, 0x012c, 0x0258, 0x04b0, 0x4660, 0x8cc0, 0x2780,\n\t0x2071, 0x40e2, 0xa0c4, 0x0108, 0x0210, 0x0420, 0x0840, 0x1080,\n\t0x4071, 0x80e2, 0x0104, 0x0208, 0x0410, 0x0820, 0x1040, 0x2080,\n\t0x8071, 0x0102, 0x0204, 0x0408, 0x0810, 0x1020, 0x2040, 0x4080,\n\t0x019d, 0x03d6, 0x136c, 0x2198, 0x50b0, 0xb2e0, 0x0740, 0x0e80,\n\t0x0189, 0x03ea, 0x072c, 0x0e58, 0x1cb0, 0x56e0, 0x37c0, 0xf580,\n\t0x01fd, 0x0376, 0x06ec, 0x0bb8, 0x1110, 0x2220, 0x4440, 0x8880,\n\t0x0163, 0x02c6, 0x1104, 0x0758, 0x0eb0, 0x2be0, 0x6140, 0xc280,\n\t0x02fd, 0x01c6, 0x0b5c, 0x1108, 0x07b0, 0x25a0, 0x8840, 0x6180,\n\t0x0801, 0x012e, 0x025c, 0x04b8, 0x1370, 0x26e0, 0x57c0, 0xb580,\n\t0x0401, 0x0802, 0x015c, 0x02b8, 0x22b0, 0x13e0, 0x7140, 0xe280,\n\t0x0201, 0x0402, 0x0804, 0x01b8, 0x11b0, 0x31a0, 0x8040, 0x7180,\n\t0x0101, 0x0202, 0x0404, 0x0808, 0x1010, 0x2020, 0x4040, 0x8080,\n\t0x0001, 0x0002, 0x0004, 0x0008, 0x0010, 0x0020, 0x0040, 0x0080,\n\t0x0100, 0x0200, 0x0400, 0x0800, 0x1000, 0x2000, 0x4000, 0x8000,\n};\n\nstatic int decode_syndrome(u16 syndrome, const u16 *vectors, unsigned num_vecs,\n\t\t\t   unsigned v_dim)\n{\n\tunsigned int i, err_sym;\n\n\tfor (err_sym = 0; err_sym < num_vecs / v_dim; err_sym++) {\n\t\tu16 s = syndrome;\n\t\tunsigned v_idx =  err_sym * v_dim;\n\t\tunsigned v_end = (err_sym + 1) * v_dim;\n\n\t\t \n\t\tfor (i = 1; i < (1U << 16); i <<= 1) {\n\n\t\t\t \n\t\t\tif (v_idx < v_end && vectors[v_idx] & i) {\n\t\t\t\tu16 ev_comp = vectors[v_idx++];\n\n\t\t\t\t \n\t\t\t\tif (s & i) {\n\t\t\t\t\t \n\t\t\t\t\ts ^= ev_comp;\n\n\t\t\t\t\tif (!s)\n\t\t\t\t\t\treturn err_sym;\n\t\t\t\t}\n\n\t\t\t} else if (s & i)\n\t\t\t\t \n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tedac_dbg(0, \"syndrome(%x) not found\\n\", syndrome);\n\treturn -1;\n}\n\nstatic int map_err_sym_to_channel(int err_sym, int sym_size)\n{\n\tif (sym_size == 4)\n\t\tswitch (err_sym) {\n\t\tcase 0x20:\n\t\tcase 0x21:\n\t\t\treturn 0;\n\t\tcase 0x22:\n\t\tcase 0x23:\n\t\t\treturn 1;\n\t\tdefault:\n\t\t\treturn err_sym >> 4;\n\t\t}\n\t \n\telse\n\t\tswitch (err_sym) {\n\t\t \n\t\tcase 0x10:\n\t\t\tWARN(1, KERN_ERR \"Invalid error symbol: 0x%x\\n\",\n\t\t\t\t\t  err_sym);\n\t\t\treturn -1;\n\t\tcase 0x11:\n\t\t\treturn 0;\n\t\tcase 0x12:\n\t\t\treturn 1;\n\t\tdefault:\n\t\t\treturn err_sym >> 3;\n\t\t}\n\treturn -1;\n}\n\nstatic int get_channel_from_ecc_syndrome(struct mem_ctl_info *mci, u16 syndrome)\n{\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\tint err_sym = -1;\n\n\tif (pvt->ecc_sym_sz == 8)\n\t\terr_sym = decode_syndrome(syndrome, x8_vectors,\n\t\t\t\t\t  ARRAY_SIZE(x8_vectors),\n\t\t\t\t\t  pvt->ecc_sym_sz);\n\telse if (pvt->ecc_sym_sz == 4)\n\t\terr_sym = decode_syndrome(syndrome, x4_vectors,\n\t\t\t\t\t  ARRAY_SIZE(x4_vectors),\n\t\t\t\t\t  pvt->ecc_sym_sz);\n\telse {\n\t\tamd64_warn(\"Illegal syndrome type: %u\\n\", pvt->ecc_sym_sz);\n\t\treturn err_sym;\n\t}\n\n\treturn map_err_sym_to_channel(err_sym, pvt->ecc_sym_sz);\n}\n\nstatic void __log_ecc_error(struct mem_ctl_info *mci, struct err_info *err,\n\t\t\t    u8 ecc_type)\n{\n\tenum hw_event_mc_err_type err_type;\n\tconst char *string;\n\n\tif (ecc_type == 2)\n\t\terr_type = HW_EVENT_ERR_CORRECTED;\n\telse if (ecc_type == 1)\n\t\terr_type = HW_EVENT_ERR_UNCORRECTED;\n\telse if (ecc_type == 3)\n\t\terr_type = HW_EVENT_ERR_DEFERRED;\n\telse {\n\t\tWARN(1, \"Something is rotten in the state of Denmark.\\n\");\n\t\treturn;\n\t}\n\n\tswitch (err->err_code) {\n\tcase DECODE_OK:\n\t\tstring = \"\";\n\t\tbreak;\n\tcase ERR_NODE:\n\t\tstring = \"Failed to map error addr to a node\";\n\t\tbreak;\n\tcase ERR_CSROW:\n\t\tstring = \"Failed to map error addr to a csrow\";\n\t\tbreak;\n\tcase ERR_CHANNEL:\n\t\tstring = \"Unknown syndrome - possible error reporting race\";\n\t\tbreak;\n\tcase ERR_SYND:\n\t\tstring = \"MCA_SYND not valid - unknown syndrome and csrow\";\n\t\tbreak;\n\tcase ERR_NORM_ADDR:\n\t\tstring = \"Cannot decode normalized address\";\n\t\tbreak;\n\tdefault:\n\t\tstring = \"WTF error\";\n\t\tbreak;\n\t}\n\n\tedac_mc_handle_error(err_type, mci, 1,\n\t\t\t     err->page, err->offset, err->syndrome,\n\t\t\t     err->csrow, err->channel, -1,\n\t\t\t     string, \"\");\n}\n\nstatic inline void decode_bus_error(int node_id, struct mce *m)\n{\n\tstruct mem_ctl_info *mci;\n\tstruct amd64_pvt *pvt;\n\tu8 ecc_type = (m->status >> 45) & 0x3;\n\tu8 xec = XEC(m->status, 0x1f);\n\tu16 ec = EC(m->status);\n\tu64 sys_addr;\n\tstruct err_info err;\n\n\tmci = edac_mc_find(node_id);\n\tif (!mci)\n\t\treturn;\n\n\tpvt = mci->pvt_info;\n\n\t \n\tif (PP(ec) == NBSL_PP_OBS)\n\t\treturn;\n\n\t \n\tif (xec && xec != F10_NBSL_EXT_ERR_ECC)\n\t\treturn;\n\n\tmemset(&err, 0, sizeof(err));\n\n\tsys_addr = get_error_address(pvt, m);\n\n\tif (ecc_type == 2)\n\t\terr.syndrome = extract_syndrome(m->status);\n\n\tpvt->ops->map_sysaddr_to_csrow(mci, sys_addr, &err);\n\n\t__log_ecc_error(mci, &err, ecc_type);\n}\n\n \nstatic void umc_get_err_info(struct mce *m, struct err_info *err)\n{\n\terr->channel = (m->ipid & GENMASK(31, 0)) >> 20;\n\terr->csrow = m->synd & 0x7;\n}\n\nstatic void decode_umc_error(int node_id, struct mce *m)\n{\n\tu8 ecc_type = (m->status >> 45) & 0x3;\n\tstruct mem_ctl_info *mci;\n\tstruct amd64_pvt *pvt;\n\tstruct err_info err;\n\tu64 sys_addr;\n\n\tnode_id = fixup_node_id(node_id, m);\n\n\tmci = edac_mc_find(node_id);\n\tif (!mci)\n\t\treturn;\n\n\tpvt = mci->pvt_info;\n\n\tmemset(&err, 0, sizeof(err));\n\n\tif (m->status & MCI_STATUS_DEFERRED)\n\t\tecc_type = 3;\n\n\tif (!(m->status & MCI_STATUS_SYNDV)) {\n\t\terr.err_code = ERR_SYND;\n\t\tgoto log_error;\n\t}\n\n\tif (ecc_type == 2) {\n\t\tu8 length = (m->synd >> 18) & 0x3f;\n\n\t\tif (length)\n\t\t\terr.syndrome = (m->synd >> 32) & GENMASK(length - 1, 0);\n\t\telse\n\t\t\terr.err_code = ERR_CHANNEL;\n\t}\n\n\tpvt->ops->get_err_info(m, &err);\n\n\tif (umc_normaddr_to_sysaddr(m->addr, pvt->mc_node_id, err.channel, &sys_addr)) {\n\t\terr.err_code = ERR_NORM_ADDR;\n\t\tgoto log_error;\n\t}\n\n\terror_address_to_page_and_offset(sys_addr, &err);\n\nlog_error:\n\t__log_ecc_error(mci, &err, ecc_type);\n}\n\n \nstatic int\nreserve_mc_sibling_devs(struct amd64_pvt *pvt, u16 pci_id1, u16 pci_id2)\n{\n\t \n\tpvt->F1 = pci_get_related_function(pvt->F3->vendor, pci_id1, pvt->F3);\n\tif (!pvt->F1) {\n\t\tedac_dbg(1, \"F1 not found: device 0x%x\\n\", pci_id1);\n\t\treturn -ENODEV;\n\t}\n\n\t \n\tpvt->F2 = pci_get_related_function(pvt->F3->vendor, pci_id2, pvt->F3);\n\tif (!pvt->F2) {\n\t\tpci_dev_put(pvt->F1);\n\t\tpvt->F1 = NULL;\n\n\t\tedac_dbg(1, \"F2 not found: device 0x%x\\n\", pci_id2);\n\t\treturn -ENODEV;\n\t}\n\n\tif (!pci_ctl_dev)\n\t\tpci_ctl_dev = &pvt->F2->dev;\n\n\tedac_dbg(1, \"F1: %s\\n\", pci_name(pvt->F1));\n\tedac_dbg(1, \"F2: %s\\n\", pci_name(pvt->F2));\n\tedac_dbg(1, \"F3: %s\\n\", pci_name(pvt->F3));\n\n\treturn 0;\n}\n\nstatic void determine_ecc_sym_sz(struct amd64_pvt *pvt)\n{\n\tpvt->ecc_sym_sz = 4;\n\n\tif (pvt->fam >= 0x10) {\n\t\tu32 tmp;\n\n\t\tamd64_read_pci_cfg(pvt->F3, EXT_NB_MCA_CFG, &tmp);\n\t\t \n\t\tif (pvt->fam != 0x16)\n\t\t\tamd64_read_dct_pci_cfg(pvt, 1, DBAM0, &pvt->dbam1);\n\n\t\t \n\t\tif ((pvt->fam > 0x10 || pvt->model > 7) && tmp & BIT(25))\n\t\t\tpvt->ecc_sym_sz = 8;\n\t}\n}\n\n \nstatic void umc_read_mc_regs(struct amd64_pvt *pvt)\n{\n\tu8 nid = pvt->mc_node_id;\n\tstruct amd64_umc *umc;\n\tu32 i, umc_base;\n\n\t \n\tfor_each_umc(i) {\n\n\t\tumc_base = get_umc_base(i);\n\t\tumc = &pvt->umc[i];\n\n\t\tamd_smn_read(nid, umc_base + get_umc_reg(pvt, UMCCH_DIMM_CFG), &umc->dimm_cfg);\n\t\tamd_smn_read(nid, umc_base + UMCCH_UMC_CFG, &umc->umc_cfg);\n\t\tamd_smn_read(nid, umc_base + UMCCH_SDP_CTRL, &umc->sdp_ctrl);\n\t\tamd_smn_read(nid, umc_base + UMCCH_ECC_CTRL, &umc->ecc_ctrl);\n\t\tamd_smn_read(nid, umc_base + UMCCH_UMC_CAP_HI, &umc->umc_cap_hi);\n\t}\n}\n\n \nstatic void dct_read_mc_regs(struct amd64_pvt *pvt)\n{\n\tunsigned int range;\n\tu64 msr_val;\n\n\t \n\trdmsrl(MSR_K8_TOP_MEM1, pvt->top_mem);\n\tedac_dbg(0, \"  TOP_MEM:  0x%016llx\\n\", pvt->top_mem);\n\n\t \n\trdmsrl(MSR_AMD64_SYSCFG, msr_val);\n\tif (msr_val & BIT(21)) {\n\t\trdmsrl(MSR_K8_TOP_MEM2, pvt->top_mem2);\n\t\tedac_dbg(0, \"  TOP_MEM2: 0x%016llx\\n\", pvt->top_mem2);\n\t} else {\n\t\tedac_dbg(0, \"  TOP_MEM2 disabled\\n\");\n\t}\n\n\tamd64_read_pci_cfg(pvt->F3, NBCAP, &pvt->nbcap);\n\n\tread_dram_ctl_register(pvt);\n\n\tfor (range = 0; range < DRAM_RANGES; range++) {\n\t\tu8 rw;\n\n\t\t \n\t\tread_dram_base_limit_regs(pvt, range);\n\n\t\trw = dram_rw(pvt, range);\n\t\tif (!rw)\n\t\t\tcontinue;\n\n\t\tedac_dbg(1, \"  DRAM range[%d], base: 0x%016llx; limit: 0x%016llx\\n\",\n\t\t\t range,\n\t\t\t get_dram_base(pvt, range),\n\t\t\t get_dram_limit(pvt, range));\n\n\t\tedac_dbg(1, \"   IntlvEn=%s; Range access: %s%s IntlvSel=%d DstNode=%d\\n\",\n\t\t\t dram_intlv_en(pvt, range) ? \"Enabled\" : \"Disabled\",\n\t\t\t (rw & 0x1) ? \"R\" : \"-\",\n\t\t\t (rw & 0x2) ? \"W\" : \"-\",\n\t\t\t dram_intlv_sel(pvt, range),\n\t\t\t dram_dst_node(pvt, range));\n\t}\n\n\tamd64_read_pci_cfg(pvt->F1, DHAR, &pvt->dhar);\n\tamd64_read_dct_pci_cfg(pvt, 0, DBAM0, &pvt->dbam0);\n\n\tamd64_read_pci_cfg(pvt->F3, F10_ONLINE_SPARE, &pvt->online_spare);\n\n\tamd64_read_dct_pci_cfg(pvt, 0, DCLR0, &pvt->dclr0);\n\tamd64_read_dct_pci_cfg(pvt, 0, DCHR0, &pvt->dchr0);\n\n\tif (!dct_ganging_enabled(pvt)) {\n\t\tamd64_read_dct_pci_cfg(pvt, 1, DCLR0, &pvt->dclr1);\n\t\tamd64_read_dct_pci_cfg(pvt, 1, DCHR0, &pvt->dchr1);\n\t}\n\n\tdetermine_ecc_sym_sz(pvt);\n}\n\n \nstatic u32 dct_get_csrow_nr_pages(struct amd64_pvt *pvt, u8 dct, int csrow_nr)\n{\n\tu32 dbam = dct ? pvt->dbam1 : pvt->dbam0;\n\tu32 cs_mode, nr_pages;\n\n\tcsrow_nr >>= 1;\n\tcs_mode = DBAM_DIMM(csrow_nr, dbam);\n\n\tnr_pages   = pvt->ops->dbam_to_cs(pvt, dct, cs_mode, csrow_nr);\n\tnr_pages <<= 20 - PAGE_SHIFT;\n\n\tedac_dbg(0, \"csrow: %d, channel: %d, DBAM idx: %d\\n\",\n\t\t    csrow_nr, dct,  cs_mode);\n\tedac_dbg(0, \"nr_pages/channel: %u\\n\", nr_pages);\n\n\treturn nr_pages;\n}\n\nstatic u32 umc_get_csrow_nr_pages(struct amd64_pvt *pvt, u8 dct, int csrow_nr_orig)\n{\n\tint csrow_nr = csrow_nr_orig;\n\tu32 cs_mode, nr_pages;\n\n\tcs_mode = umc_get_cs_mode(csrow_nr >> 1, dct, pvt);\n\n\tnr_pages   = umc_addr_mask_to_cs_size(pvt, dct, cs_mode, csrow_nr);\n\tnr_pages <<= 20 - PAGE_SHIFT;\n\n\tedac_dbg(0, \"csrow: %d, channel: %d, cs_mode %d\\n\",\n\t\t csrow_nr_orig, dct,  cs_mode);\n\tedac_dbg(0, \"nr_pages/channel: %u\\n\", nr_pages);\n\n\treturn nr_pages;\n}\n\nstatic void umc_init_csrows(struct mem_ctl_info *mci)\n{\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\tenum edac_type edac_mode = EDAC_NONE;\n\tenum dev_type dev_type = DEV_UNKNOWN;\n\tstruct dimm_info *dimm;\n\tu8 umc, cs;\n\n\tif (mci->edac_ctl_cap & EDAC_FLAG_S16ECD16ED) {\n\t\tedac_mode = EDAC_S16ECD16ED;\n\t\tdev_type = DEV_X16;\n\t} else if (mci->edac_ctl_cap & EDAC_FLAG_S8ECD8ED) {\n\t\tedac_mode = EDAC_S8ECD8ED;\n\t\tdev_type = DEV_X8;\n\t} else if (mci->edac_ctl_cap & EDAC_FLAG_S4ECD4ED) {\n\t\tedac_mode = EDAC_S4ECD4ED;\n\t\tdev_type = DEV_X4;\n\t} else if (mci->edac_ctl_cap & EDAC_FLAG_SECDED) {\n\t\tedac_mode = EDAC_SECDED;\n\t}\n\n\tfor_each_umc(umc) {\n\t\tfor_each_chip_select(cs, umc, pvt) {\n\t\t\tif (!csrow_enabled(cs, umc, pvt))\n\t\t\t\tcontinue;\n\n\t\t\tdimm = mci->csrows[cs]->channels[umc]->dimm;\n\n\t\t\tedac_dbg(1, \"MC node: %d, csrow: %d\\n\",\n\t\t\t\t\tpvt->mc_node_id, cs);\n\n\t\t\tdimm->nr_pages = umc_get_csrow_nr_pages(pvt, umc, cs);\n\t\t\tdimm->mtype = pvt->umc[umc].dram_type;\n\t\t\tdimm->edac_mode = edac_mode;\n\t\t\tdimm->dtype = dev_type;\n\t\t\tdimm->grain = 64;\n\t\t}\n\t}\n}\n\n \nstatic void dct_init_csrows(struct mem_ctl_info *mci)\n{\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\tenum edac_type edac_mode = EDAC_NONE;\n\tstruct csrow_info *csrow;\n\tstruct dimm_info *dimm;\n\tint nr_pages = 0;\n\tint i, j;\n\tu32 val;\n\n\tamd64_read_pci_cfg(pvt->F3, NBCFG, &val);\n\n\tpvt->nbcfg = val;\n\n\tedac_dbg(0, \"node %d, NBCFG=0x%08x[ChipKillEccCap: %d|DramEccEn: %d]\\n\",\n\t\t pvt->mc_node_id, val,\n\t\t !!(val & NBCFG_CHIPKILL), !!(val & NBCFG_ECC_ENABLE));\n\n\t \n\tfor_each_chip_select(i, 0, pvt) {\n\t\tbool row_dct0 = !!csrow_enabled(i, 0, pvt);\n\t\tbool row_dct1 = false;\n\n\t\tif (pvt->fam != 0xf)\n\t\t\trow_dct1 = !!csrow_enabled(i, 1, pvt);\n\n\t\tif (!row_dct0 && !row_dct1)\n\t\t\tcontinue;\n\n\t\tcsrow = mci->csrows[i];\n\n\t\tedac_dbg(1, \"MC node: %d, csrow: %d\\n\",\n\t\t\t    pvt->mc_node_id, i);\n\n\t\tif (row_dct0) {\n\t\t\tnr_pages = dct_get_csrow_nr_pages(pvt, 0, i);\n\t\t\tcsrow->channels[0]->dimm->nr_pages = nr_pages;\n\t\t}\n\n\t\t \n\t\tif (pvt->fam != 0xf && row_dct1) {\n\t\t\tint row_dct1_pages = dct_get_csrow_nr_pages(pvt, 1, i);\n\n\t\t\tcsrow->channels[1]->dimm->nr_pages = row_dct1_pages;\n\t\t\tnr_pages += row_dct1_pages;\n\t\t}\n\n\t\tedac_dbg(1, \"Total csrow%d pages: %u\\n\", i, nr_pages);\n\n\t\t \n\t\tif (pvt->nbcfg & NBCFG_ECC_ENABLE) {\n\t\t\tedac_mode = (pvt->nbcfg & NBCFG_CHIPKILL)\n\t\t\t\t\t? EDAC_S4ECD4ED\n\t\t\t\t\t: EDAC_SECDED;\n\t\t}\n\n\t\tfor (j = 0; j < pvt->max_mcs; j++) {\n\t\t\tdimm = csrow->channels[j]->dimm;\n\t\t\tdimm->mtype = pvt->dram_type;\n\t\t\tdimm->edac_mode = edac_mode;\n\t\t\tdimm->grain = 64;\n\t\t}\n\t}\n}\n\n \nstatic void get_cpus_on_this_dct_cpumask(struct cpumask *mask, u16 nid)\n{\n\tint cpu;\n\n\tfor_each_online_cpu(cpu)\n\t\tif (topology_die_id(cpu) == nid)\n\t\t\tcpumask_set_cpu(cpu, mask);\n}\n\n \nstatic bool nb_mce_bank_enabled_on_node(u16 nid)\n{\n\tcpumask_var_t mask;\n\tint cpu, nbe;\n\tbool ret = false;\n\n\tif (!zalloc_cpumask_var(&mask, GFP_KERNEL)) {\n\t\tamd64_warn(\"%s: Error allocating mask\\n\", __func__);\n\t\treturn false;\n\t}\n\n\tget_cpus_on_this_dct_cpumask(mask, nid);\n\n\trdmsr_on_cpus(mask, MSR_IA32_MCG_CTL, msrs);\n\n\tfor_each_cpu(cpu, mask) {\n\t\tstruct msr *reg = per_cpu_ptr(msrs, cpu);\n\t\tnbe = reg->l & MSR_MCGCTL_NBE;\n\n\t\tedac_dbg(0, \"core: %u, MCG_CTL: 0x%llx, NB MSR is %s\\n\",\n\t\t\t cpu, reg->q,\n\t\t\t (nbe ? \"enabled\" : \"disabled\"));\n\n\t\tif (!nbe)\n\t\t\tgoto out;\n\t}\n\tret = true;\n\nout:\n\tfree_cpumask_var(mask);\n\treturn ret;\n}\n\nstatic int toggle_ecc_err_reporting(struct ecc_settings *s, u16 nid, bool on)\n{\n\tcpumask_var_t cmask;\n\tint cpu;\n\n\tif (!zalloc_cpumask_var(&cmask, GFP_KERNEL)) {\n\t\tamd64_warn(\"%s: error allocating mask\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\n\tget_cpus_on_this_dct_cpumask(cmask, nid);\n\n\trdmsr_on_cpus(cmask, MSR_IA32_MCG_CTL, msrs);\n\n\tfor_each_cpu(cpu, cmask) {\n\n\t\tstruct msr *reg = per_cpu_ptr(msrs, cpu);\n\n\t\tif (on) {\n\t\t\tif (reg->l & MSR_MCGCTL_NBE)\n\t\t\t\ts->flags.nb_mce_enable = 1;\n\n\t\t\treg->l |= MSR_MCGCTL_NBE;\n\t\t} else {\n\t\t\t \n\t\t\tif (!s->flags.nb_mce_enable)\n\t\t\t\treg->l &= ~MSR_MCGCTL_NBE;\n\t\t}\n\t}\n\twrmsr_on_cpus(cmask, MSR_IA32_MCG_CTL, msrs);\n\n\tfree_cpumask_var(cmask);\n\n\treturn 0;\n}\n\nstatic bool enable_ecc_error_reporting(struct ecc_settings *s, u16 nid,\n\t\t\t\t       struct pci_dev *F3)\n{\n\tbool ret = true;\n\tu32 value, mask = 0x3;\t\t \n\n\tif (toggle_ecc_err_reporting(s, nid, ON)) {\n\t\tamd64_warn(\"Error enabling ECC reporting over MCGCTL!\\n\");\n\t\treturn false;\n\t}\n\n\tamd64_read_pci_cfg(F3, NBCTL, &value);\n\n\ts->old_nbctl   = value & mask;\n\ts->nbctl_valid = true;\n\n\tvalue |= mask;\n\tamd64_write_pci_cfg(F3, NBCTL, value);\n\n\tamd64_read_pci_cfg(F3, NBCFG, &value);\n\n\tedac_dbg(0, \"1: node %d, NBCFG=0x%08x[DramEccEn: %d]\\n\",\n\t\t nid, value, !!(value & NBCFG_ECC_ENABLE));\n\n\tif (!(value & NBCFG_ECC_ENABLE)) {\n\t\tamd64_warn(\"DRAM ECC disabled on this node, enabling...\\n\");\n\n\t\ts->flags.nb_ecc_prev = 0;\n\n\t\t \n\t\tvalue |= NBCFG_ECC_ENABLE;\n\t\tamd64_write_pci_cfg(F3, NBCFG, value);\n\n\t\tamd64_read_pci_cfg(F3, NBCFG, &value);\n\n\t\tif (!(value & NBCFG_ECC_ENABLE)) {\n\t\t\tamd64_warn(\"Hardware rejected DRAM ECC enable,\"\n\t\t\t\t   \"check memory DIMM configuration.\\n\");\n\t\t\tret = false;\n\t\t} else {\n\t\t\tamd64_info(\"Hardware accepted DRAM ECC Enable\\n\");\n\t\t}\n\t} else {\n\t\ts->flags.nb_ecc_prev = 1;\n\t}\n\n\tedac_dbg(0, \"2: node %d, NBCFG=0x%08x[DramEccEn: %d]\\n\",\n\t\t nid, value, !!(value & NBCFG_ECC_ENABLE));\n\n\treturn ret;\n}\n\nstatic void restore_ecc_error_reporting(struct ecc_settings *s, u16 nid,\n\t\t\t\t\tstruct pci_dev *F3)\n{\n\tu32 value, mask = 0x3;\t\t \n\n\tif (!s->nbctl_valid)\n\t\treturn;\n\n\tamd64_read_pci_cfg(F3, NBCTL, &value);\n\tvalue &= ~mask;\n\tvalue |= s->old_nbctl;\n\n\tamd64_write_pci_cfg(F3, NBCTL, value);\n\n\t \n\tif (!s->flags.nb_ecc_prev) {\n\t\tamd64_read_pci_cfg(F3, NBCFG, &value);\n\t\tvalue &= ~NBCFG_ECC_ENABLE;\n\t\tamd64_write_pci_cfg(F3, NBCFG, value);\n\t}\n\n\t \n\tif (toggle_ecc_err_reporting(s, nid, OFF))\n\t\tamd64_warn(\"Error restoring NB MCGCTL settings!\\n\");\n}\n\nstatic bool dct_ecc_enabled(struct amd64_pvt *pvt)\n{\n\tu16 nid = pvt->mc_node_id;\n\tbool nb_mce_en = false;\n\tu8 ecc_en = 0;\n\tu32 value;\n\n\tamd64_read_pci_cfg(pvt->F3, NBCFG, &value);\n\n\tecc_en = !!(value & NBCFG_ECC_ENABLE);\n\n\tnb_mce_en = nb_mce_bank_enabled_on_node(nid);\n\tif (!nb_mce_en)\n\t\tedac_dbg(0, \"NB MCE bank disabled, set MSR 0x%08x[4] on node %d to enable.\\n\",\n\t\t\t MSR_IA32_MCG_CTL, nid);\n\n\tedac_dbg(3, \"Node %d: DRAM ECC %s.\\n\", nid, (ecc_en ? \"enabled\" : \"disabled\"));\n\n\tif (!ecc_en || !nb_mce_en)\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\nstatic bool umc_ecc_enabled(struct amd64_pvt *pvt)\n{\n\tu8 umc_en_mask = 0, ecc_en_mask = 0;\n\tu16 nid = pvt->mc_node_id;\n\tstruct amd64_umc *umc;\n\tu8 ecc_en = 0, i;\n\n\tfor_each_umc(i) {\n\t\tumc = &pvt->umc[i];\n\n\t\t \n\t\tif (!(umc->sdp_ctrl & UMC_SDP_INIT))\n\t\t\tcontinue;\n\n\t\tumc_en_mask |= BIT(i);\n\n\t\tif (umc->umc_cap_hi & UMC_ECC_ENABLED)\n\t\t\tecc_en_mask |= BIT(i);\n\t}\n\n\t \n\tif (umc_en_mask)\n\t\tecc_en = umc_en_mask == ecc_en_mask;\n\telse\n\t\tedac_dbg(0, \"Node %d: No enabled UMCs.\\n\", nid);\n\n\tedac_dbg(3, \"Node %d: DRAM ECC %s.\\n\", nid, (ecc_en ? \"enabled\" : \"disabled\"));\n\n\tif (!ecc_en)\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\nstatic inline void\numc_determine_edac_ctl_cap(struct mem_ctl_info *mci, struct amd64_pvt *pvt)\n{\n\tu8 i, ecc_en = 1, cpk_en = 1, dev_x4 = 1, dev_x16 = 1;\n\n\tfor_each_umc(i) {\n\t\tif (pvt->umc[i].sdp_ctrl & UMC_SDP_INIT) {\n\t\t\tecc_en &= !!(pvt->umc[i].umc_cap_hi & UMC_ECC_ENABLED);\n\t\t\tcpk_en &= !!(pvt->umc[i].umc_cap_hi & UMC_ECC_CHIPKILL_CAP);\n\n\t\t\tdev_x4  &= !!(pvt->umc[i].dimm_cfg & BIT(6));\n\t\t\tdev_x16 &= !!(pvt->umc[i].dimm_cfg & BIT(7));\n\t\t}\n\t}\n\n\t \n\tif (ecc_en) {\n\t\tmci->edac_ctl_cap |= EDAC_FLAG_SECDED;\n\n\t\tif (!cpk_en)\n\t\t\treturn;\n\n\t\tif (dev_x4)\n\t\t\tmci->edac_ctl_cap |= EDAC_FLAG_S4ECD4ED;\n\t\telse if (dev_x16)\n\t\t\tmci->edac_ctl_cap |= EDAC_FLAG_S16ECD16ED;\n\t\telse\n\t\t\tmci->edac_ctl_cap |= EDAC_FLAG_S8ECD8ED;\n\t}\n}\n\nstatic void dct_setup_mci_misc_attrs(struct mem_ctl_info *mci)\n{\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\n\tmci->mtype_cap\t\t= MEM_FLAG_DDR2 | MEM_FLAG_RDDR2;\n\tmci->edac_ctl_cap\t= EDAC_FLAG_NONE;\n\n\tif (pvt->nbcap & NBCAP_SECDED)\n\t\tmci->edac_ctl_cap |= EDAC_FLAG_SECDED;\n\n\tif (pvt->nbcap & NBCAP_CHIPKILL)\n\t\tmci->edac_ctl_cap |= EDAC_FLAG_S4ECD4ED;\n\n\tmci->edac_cap\t\t= dct_determine_edac_cap(pvt);\n\tmci->mod_name\t\t= EDAC_MOD_STR;\n\tmci->ctl_name\t\t= pvt->ctl_name;\n\tmci->dev_name\t\t= pci_name(pvt->F3);\n\tmci->ctl_page_to_phys\t= NULL;\n\n\t \n\tmci->set_sdram_scrub_rate = set_scrub_rate;\n\tmci->get_sdram_scrub_rate = get_scrub_rate;\n\n\tdct_init_csrows(mci);\n}\n\nstatic void umc_setup_mci_misc_attrs(struct mem_ctl_info *mci)\n{\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\n\tmci->mtype_cap\t\t= MEM_FLAG_DDR4 | MEM_FLAG_RDDR4;\n\tmci->edac_ctl_cap\t= EDAC_FLAG_NONE;\n\n\tumc_determine_edac_ctl_cap(mci, pvt);\n\n\tmci->edac_cap\t\t= umc_determine_edac_cap(pvt);\n\tmci->mod_name\t\t= EDAC_MOD_STR;\n\tmci->ctl_name\t\t= pvt->ctl_name;\n\tmci->dev_name\t\t= pci_name(pvt->F3);\n\tmci->ctl_page_to_phys\t= NULL;\n\n\tumc_init_csrows(mci);\n}\n\nstatic int dct_hw_info_get(struct amd64_pvt *pvt)\n{\n\tint ret = reserve_mc_sibling_devs(pvt, pvt->f1_id, pvt->f2_id);\n\n\tif (ret)\n\t\treturn ret;\n\n\tdct_prep_chip_selects(pvt);\n\tdct_read_base_mask(pvt);\n\tdct_read_mc_regs(pvt);\n\tdct_determine_memory_type(pvt);\n\n\treturn 0;\n}\n\nstatic int umc_hw_info_get(struct amd64_pvt *pvt)\n{\n\tpvt->umc = kcalloc(pvt->max_mcs, sizeof(struct amd64_umc), GFP_KERNEL);\n\tif (!pvt->umc)\n\t\treturn -ENOMEM;\n\n\tumc_prep_chip_selects(pvt);\n\tumc_read_base_mask(pvt);\n\tumc_read_mc_regs(pvt);\n\tumc_determine_memory_type(pvt);\n\n\treturn 0;\n}\n\n \nstatic void gpu_get_err_info(struct mce *m, struct err_info *err)\n{\n\tu8 ch = (m->ipid & GENMASK(31, 0)) >> 20;\n\tu8 phy = ((m->ipid >> 12) & 0xf);\n\n\terr->channel = ch % 2 ? phy + 4 : phy;\n\terr->csrow = phy;\n}\n\nstatic int gpu_addr_mask_to_cs_size(struct amd64_pvt *pvt, u8 umc,\n\t\t\t\t    unsigned int cs_mode, int csrow_nr)\n{\n\tu32 addr_mask_orig = pvt->csels[umc].csmasks[csrow_nr];\n\n\treturn __addr_mask_to_cs_size(addr_mask_orig, cs_mode, csrow_nr, csrow_nr >> 1);\n}\n\nstatic void gpu_debug_display_dimm_sizes(struct amd64_pvt *pvt, u8 ctrl)\n{\n\tint size, cs_mode, cs = 0;\n\n\tedac_printk(KERN_DEBUG, EDAC_MC, \"UMC%d chip selects:\\n\", ctrl);\n\n\tcs_mode = CS_EVEN_PRIMARY | CS_ODD_PRIMARY;\n\n\tfor_each_chip_select(cs, ctrl, pvt) {\n\t\tsize = gpu_addr_mask_to_cs_size(pvt, ctrl, cs_mode, cs);\n\t\tamd64_info(EDAC_MC \": %d: %5dMB\\n\", cs, size);\n\t}\n}\n\nstatic void gpu_dump_misc_regs(struct amd64_pvt *pvt)\n{\n\tstruct amd64_umc *umc;\n\tu32 i;\n\n\tfor_each_umc(i) {\n\t\tumc = &pvt->umc[i];\n\n\t\tedac_dbg(1, \"UMC%d UMC cfg: 0x%x\\n\", i, umc->umc_cfg);\n\t\tedac_dbg(1, \"UMC%d SDP ctrl: 0x%x\\n\", i, umc->sdp_ctrl);\n\t\tedac_dbg(1, \"UMC%d ECC ctrl: 0x%x\\n\", i, umc->ecc_ctrl);\n\t\tedac_dbg(1, \"UMC%d All HBMs support ECC: yes\\n\", i);\n\n\t\tgpu_debug_display_dimm_sizes(pvt, i);\n\t}\n}\n\nstatic u32 gpu_get_csrow_nr_pages(struct amd64_pvt *pvt, u8 dct, int csrow_nr)\n{\n\tu32 nr_pages;\n\tint cs_mode = CS_EVEN_PRIMARY | CS_ODD_PRIMARY;\n\n\tnr_pages   = gpu_addr_mask_to_cs_size(pvt, dct, cs_mode, csrow_nr);\n\tnr_pages <<= 20 - PAGE_SHIFT;\n\n\tedac_dbg(0, \"csrow: %d, channel: %d\\n\", csrow_nr, dct);\n\tedac_dbg(0, \"nr_pages/channel: %u\\n\", nr_pages);\n\n\treturn nr_pages;\n}\n\nstatic void gpu_init_csrows(struct mem_ctl_info *mci)\n{\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\tstruct dimm_info *dimm;\n\tu8 umc, cs;\n\n\tfor_each_umc(umc) {\n\t\tfor_each_chip_select(cs, umc, pvt) {\n\t\t\tif (!csrow_enabled(cs, umc, pvt))\n\t\t\t\tcontinue;\n\n\t\t\tdimm = mci->csrows[umc]->channels[cs]->dimm;\n\n\t\t\tedac_dbg(1, \"MC node: %d, csrow: %d\\n\",\n\t\t\t\t pvt->mc_node_id, cs);\n\n\t\t\tdimm->nr_pages = gpu_get_csrow_nr_pages(pvt, umc, cs);\n\t\t\tdimm->edac_mode = EDAC_SECDED;\n\t\t\tdimm->mtype = MEM_HBM2;\n\t\t\tdimm->dtype = DEV_X16;\n\t\t\tdimm->grain = 64;\n\t\t}\n\t}\n}\n\nstatic void gpu_setup_mci_misc_attrs(struct mem_ctl_info *mci)\n{\n\tstruct amd64_pvt *pvt = mci->pvt_info;\n\n\tmci->mtype_cap\t\t= MEM_FLAG_HBM2;\n\tmci->edac_ctl_cap\t= EDAC_FLAG_SECDED;\n\n\tmci->edac_cap\t\t= EDAC_FLAG_EC;\n\tmci->mod_name\t\t= EDAC_MOD_STR;\n\tmci->ctl_name\t\t= pvt->ctl_name;\n\tmci->dev_name\t\t= pci_name(pvt->F3);\n\tmci->ctl_page_to_phys\t= NULL;\n\n\tgpu_init_csrows(mci);\n}\n\n \nstatic bool gpu_ecc_enabled(struct amd64_pvt *pvt)\n{\n\treturn true;\n}\n\nstatic inline u32 gpu_get_umc_base(u8 umc, u8 channel)\n{\n\t \n\tumc *= 2;\n\n\tif (channel >= 4)\n\t\tumc++;\n\n\treturn 0x50000 + (umc << 20) + ((channel % 4) << 12);\n}\n\nstatic void gpu_read_mc_regs(struct amd64_pvt *pvt)\n{\n\tu8 nid = pvt->mc_node_id;\n\tstruct amd64_umc *umc;\n\tu32 i, umc_base;\n\n\t \n\tfor_each_umc(i) {\n\t\tumc_base = gpu_get_umc_base(i, 0);\n\t\tumc = &pvt->umc[i];\n\n\t\tamd_smn_read(nid, umc_base + UMCCH_UMC_CFG, &umc->umc_cfg);\n\t\tamd_smn_read(nid, umc_base + UMCCH_SDP_CTRL, &umc->sdp_ctrl);\n\t\tamd_smn_read(nid, umc_base + UMCCH_ECC_CTRL, &umc->ecc_ctrl);\n\t}\n}\n\nstatic void gpu_read_base_mask(struct amd64_pvt *pvt)\n{\n\tu32 base_reg, mask_reg;\n\tu32 *base, *mask;\n\tint umc, cs;\n\n\tfor_each_umc(umc) {\n\t\tfor_each_chip_select(cs, umc, pvt) {\n\t\t\tbase_reg = gpu_get_umc_base(umc, cs) + UMCCH_BASE_ADDR;\n\t\t\tbase = &pvt->csels[umc].csbases[cs];\n\n\t\t\tif (!amd_smn_read(pvt->mc_node_id, base_reg, base)) {\n\t\t\t\tedac_dbg(0, \"  DCSB%d[%d]=0x%08x reg: 0x%x\\n\",\n\t\t\t\t\t umc, cs, *base, base_reg);\n\t\t\t}\n\n\t\t\tmask_reg = gpu_get_umc_base(umc, cs) + UMCCH_ADDR_MASK;\n\t\t\tmask = &pvt->csels[umc].csmasks[cs];\n\n\t\t\tif (!amd_smn_read(pvt->mc_node_id, mask_reg, mask)) {\n\t\t\t\tedac_dbg(0, \"  DCSM%d[%d]=0x%08x reg: 0x%x\\n\",\n\t\t\t\t\t umc, cs, *mask, mask_reg);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void gpu_prep_chip_selects(struct amd64_pvt *pvt)\n{\n\tint umc;\n\n\tfor_each_umc(umc) {\n\t\tpvt->csels[umc].b_cnt = 8;\n\t\tpvt->csels[umc].m_cnt = 8;\n\t}\n}\n\nstatic int gpu_hw_info_get(struct amd64_pvt *pvt)\n{\n\tint ret;\n\n\tret = gpu_get_node_map();\n\tif (ret)\n\t\treturn ret;\n\n\tpvt->umc = kcalloc(pvt->max_mcs, sizeof(struct amd64_umc), GFP_KERNEL);\n\tif (!pvt->umc)\n\t\treturn -ENOMEM;\n\n\tgpu_prep_chip_selects(pvt);\n\tgpu_read_base_mask(pvt);\n\tgpu_read_mc_regs(pvt);\n\n\treturn 0;\n}\n\nstatic void hw_info_put(struct amd64_pvt *pvt)\n{\n\tpci_dev_put(pvt->F1);\n\tpci_dev_put(pvt->F2);\n\tkfree(pvt->umc);\n}\n\nstatic struct low_ops umc_ops = {\n\t.hw_info_get\t\t\t= umc_hw_info_get,\n\t.ecc_enabled\t\t\t= umc_ecc_enabled,\n\t.setup_mci_misc_attrs\t\t= umc_setup_mci_misc_attrs,\n\t.dump_misc_regs\t\t\t= umc_dump_misc_regs,\n\t.get_err_info\t\t\t= umc_get_err_info,\n};\n\nstatic struct low_ops gpu_ops = {\n\t.hw_info_get\t\t\t= gpu_hw_info_get,\n\t.ecc_enabled\t\t\t= gpu_ecc_enabled,\n\t.setup_mci_misc_attrs\t\t= gpu_setup_mci_misc_attrs,\n\t.dump_misc_regs\t\t\t= gpu_dump_misc_regs,\n\t.get_err_info\t\t\t= gpu_get_err_info,\n};\n\n \nstatic struct low_ops dct_ops = {\n\t.map_sysaddr_to_csrow\t\t= f1x_map_sysaddr_to_csrow,\n\t.dbam_to_cs\t\t\t= f16_dbam_to_chip_select,\n\t.hw_info_get\t\t\t= dct_hw_info_get,\n\t.ecc_enabled\t\t\t= dct_ecc_enabled,\n\t.setup_mci_misc_attrs\t\t= dct_setup_mci_misc_attrs,\n\t.dump_misc_regs\t\t\t= dct_dump_misc_regs,\n};\n\nstatic int per_family_init(struct amd64_pvt *pvt)\n{\n\tpvt->ext_model  = boot_cpu_data.x86_model >> 4;\n\tpvt->stepping\t= boot_cpu_data.x86_stepping;\n\tpvt->model\t= boot_cpu_data.x86_model;\n\tpvt->fam\t= boot_cpu_data.x86;\n\tpvt->max_mcs\t= 2;\n\n\t \n\tif (pvt->fam >= 0x17)\n\t\tpvt->ops = &umc_ops;\n\telse\n\t\tpvt->ops = &dct_ops;\n\n\tswitch (pvt->fam) {\n\tcase 0xf:\n\t\tpvt->ctl_name\t\t\t\t= (pvt->ext_model >= K8_REV_F) ?\n\t\t\t\t\t\t\t  \"K8 revF or later\" : \"K8 revE or earlier\";\n\t\tpvt->f1_id\t\t\t\t= PCI_DEVICE_ID_AMD_K8_NB_ADDRMAP;\n\t\tpvt->f2_id\t\t\t\t= PCI_DEVICE_ID_AMD_K8_NB_MEMCTL;\n\t\tpvt->ops->map_sysaddr_to_csrow\t\t= k8_map_sysaddr_to_csrow;\n\t\tpvt->ops->dbam_to_cs\t\t\t= k8_dbam_to_chip_select;\n\t\tbreak;\n\n\tcase 0x10:\n\t\tpvt->ctl_name\t\t\t\t= \"F10h\";\n\t\tpvt->f1_id\t\t\t\t= PCI_DEVICE_ID_AMD_10H_NB_MAP;\n\t\tpvt->f2_id\t\t\t\t= PCI_DEVICE_ID_AMD_10H_NB_DRAM;\n\t\tpvt->ops->dbam_to_cs\t\t\t= f10_dbam_to_chip_select;\n\t\tbreak;\n\n\tcase 0x15:\n\t\tswitch (pvt->model) {\n\t\tcase 0x30:\n\t\t\tpvt->ctl_name\t\t\t= \"F15h_M30h\";\n\t\t\tpvt->f1_id\t\t\t= PCI_DEVICE_ID_AMD_15H_M30H_NB_F1;\n\t\t\tpvt->f2_id\t\t\t= PCI_DEVICE_ID_AMD_15H_M30H_NB_F2;\n\t\t\tbreak;\n\t\tcase 0x60:\n\t\t\tpvt->ctl_name\t\t\t= \"F15h_M60h\";\n\t\t\tpvt->f1_id\t\t\t= PCI_DEVICE_ID_AMD_15H_M60H_NB_F1;\n\t\t\tpvt->f2_id\t\t\t= PCI_DEVICE_ID_AMD_15H_M60H_NB_F2;\n\t\t\tpvt->ops->dbam_to_cs\t\t= f15_m60h_dbam_to_chip_select;\n\t\t\tbreak;\n\t\tcase 0x13:\n\t\t\t \n\t\t\treturn -ENODEV;\n\t\tdefault:\n\t\t\tpvt->ctl_name\t\t\t= \"F15h\";\n\t\t\tpvt->f1_id\t\t\t= PCI_DEVICE_ID_AMD_15H_NB_F1;\n\t\t\tpvt->f2_id\t\t\t= PCI_DEVICE_ID_AMD_15H_NB_F2;\n\t\t\tpvt->ops->dbam_to_cs\t\t= f15_dbam_to_chip_select;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase 0x16:\n\t\tswitch (pvt->model) {\n\t\tcase 0x30:\n\t\t\tpvt->ctl_name\t\t\t= \"F16h_M30h\";\n\t\t\tpvt->f1_id\t\t\t= PCI_DEVICE_ID_AMD_16H_M30H_NB_F1;\n\t\t\tpvt->f2_id\t\t\t= PCI_DEVICE_ID_AMD_16H_M30H_NB_F2;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpvt->ctl_name\t\t\t= \"F16h\";\n\t\t\tpvt->f1_id\t\t\t= PCI_DEVICE_ID_AMD_16H_NB_F1;\n\t\t\tpvt->f2_id\t\t\t= PCI_DEVICE_ID_AMD_16H_NB_F2;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase 0x17:\n\t\tswitch (pvt->model) {\n\t\tcase 0x10 ... 0x2f:\n\t\t\tpvt->ctl_name\t\t\t= \"F17h_M10h\";\n\t\t\tbreak;\n\t\tcase 0x30 ... 0x3f:\n\t\t\tpvt->ctl_name\t\t\t= \"F17h_M30h\";\n\t\t\tpvt->max_mcs\t\t\t= 8;\n\t\t\tbreak;\n\t\tcase 0x60 ... 0x6f:\n\t\t\tpvt->ctl_name\t\t\t= \"F17h_M60h\";\n\t\t\tbreak;\n\t\tcase 0x70 ... 0x7f:\n\t\t\tpvt->ctl_name\t\t\t= \"F17h_M70h\";\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpvt->ctl_name\t\t\t= \"F17h\";\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase 0x18:\n\t\tpvt->ctl_name\t\t\t\t= \"F18h\";\n\t\tbreak;\n\n\tcase 0x19:\n\t\tswitch (pvt->model) {\n\t\tcase 0x00 ... 0x0f:\n\t\t\tpvt->ctl_name\t\t\t= \"F19h\";\n\t\t\tpvt->max_mcs\t\t\t= 8;\n\t\t\tbreak;\n\t\tcase 0x10 ... 0x1f:\n\t\t\tpvt->ctl_name\t\t\t= \"F19h_M10h\";\n\t\t\tpvt->max_mcs\t\t\t= 12;\n\t\t\tpvt->flags.zn_regs_v2\t\t= 1;\n\t\t\tbreak;\n\t\tcase 0x20 ... 0x2f:\n\t\t\tpvt->ctl_name\t\t\t= \"F19h_M20h\";\n\t\t\tbreak;\n\t\tcase 0x30 ... 0x3f:\n\t\t\tif (pvt->F3->device == PCI_DEVICE_ID_AMD_MI200_DF_F3) {\n\t\t\t\tpvt->ctl_name\t\t= \"MI200\";\n\t\t\t\tpvt->max_mcs\t\t= 4;\n\t\t\t\tpvt->ops\t\t= &gpu_ops;\n\t\t\t} else {\n\t\t\t\tpvt->ctl_name\t\t= \"F19h_M30h\";\n\t\t\t\tpvt->max_mcs\t\t= 8;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase 0x50 ... 0x5f:\n\t\t\tpvt->ctl_name\t\t\t= \"F19h_M50h\";\n\t\t\tbreak;\n\t\tcase 0x60 ... 0x6f:\n\t\t\tpvt->ctl_name\t\t\t= \"F19h_M60h\";\n\t\t\tpvt->flags.zn_regs_v2\t\t= 1;\n\t\t\tbreak;\n\t\tcase 0x70 ... 0x7f:\n\t\t\tpvt->ctl_name\t\t\t= \"F19h_M70h\";\n\t\t\tpvt->flags.zn_regs_v2\t\t= 1;\n\t\t\tbreak;\n\t\tcase 0xa0 ... 0xaf:\n\t\t\tpvt->ctl_name\t\t\t= \"F19h_MA0h\";\n\t\t\tpvt->max_mcs\t\t\t= 12;\n\t\t\tpvt->flags.zn_regs_v2\t\t= 1;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tcase 0x1A:\n\t\tswitch (pvt->model) {\n\t\tcase 0x00 ... 0x1f:\n\t\t\tpvt->ctl_name           = \"F1Ah\";\n\t\t\tpvt->max_mcs            = 12;\n\t\t\tpvt->flags.zn_regs_v2   = 1;\n\t\t\tbreak;\n\t\tcase 0x40 ... 0x4f:\n\t\t\tpvt->ctl_name           = \"F1Ah_M40h\";\n\t\t\tpvt->flags.zn_regs_v2   = 1;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tamd64_err(\"Unsupported family!\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\treturn 0;\n}\n\nstatic const struct attribute_group *amd64_edac_attr_groups[] = {\n#ifdef CONFIG_EDAC_DEBUG\n\t&dbg_group,\n\t&inj_group,\n#endif\n\tNULL\n};\n\nstatic int init_one_instance(struct amd64_pvt *pvt)\n{\n\tstruct mem_ctl_info *mci = NULL;\n\tstruct edac_mc_layer layers[2];\n\tint ret = -ENOMEM;\n\n\t \n\tlayers[0].type = EDAC_MC_LAYER_CHIP_SELECT;\n\tlayers[0].size = (pvt->F3->device == PCI_DEVICE_ID_AMD_MI200_DF_F3) ?\n\t\t\t pvt->max_mcs : pvt->csels[0].b_cnt;\n\tlayers[0].is_virt_csrow = true;\n\tlayers[1].type = EDAC_MC_LAYER_CHANNEL;\n\tlayers[1].size = (pvt->F3->device == PCI_DEVICE_ID_AMD_MI200_DF_F3) ?\n\t\t\t pvt->csels[0].b_cnt : pvt->max_mcs;\n\tlayers[1].is_virt_csrow = false;\n\n\tmci = edac_mc_alloc(pvt->mc_node_id, ARRAY_SIZE(layers), layers, 0);\n\tif (!mci)\n\t\treturn ret;\n\n\tmci->pvt_info = pvt;\n\tmci->pdev = &pvt->F3->dev;\n\n\tpvt->ops->setup_mci_misc_attrs(mci);\n\n\tret = -ENODEV;\n\tif (edac_mc_add_mc_with_groups(mci, amd64_edac_attr_groups)) {\n\t\tedac_dbg(1, \"failed edac_mc_add_mc()\\n\");\n\t\tedac_mc_free(mci);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic bool instance_has_memory(struct amd64_pvt *pvt)\n{\n\tbool cs_enabled = false;\n\tint cs = 0, dct = 0;\n\n\tfor (dct = 0; dct < pvt->max_mcs; dct++) {\n\t\tfor_each_chip_select(cs, dct, pvt)\n\t\t\tcs_enabled |= csrow_enabled(cs, dct, pvt);\n\t}\n\n\treturn cs_enabled;\n}\n\nstatic int probe_one_instance(unsigned int nid)\n{\n\tstruct pci_dev *F3 = node_to_amd_nb(nid)->misc;\n\tstruct amd64_pvt *pvt = NULL;\n\tstruct ecc_settings *s;\n\tint ret;\n\n\tret = -ENOMEM;\n\ts = kzalloc(sizeof(struct ecc_settings), GFP_KERNEL);\n\tif (!s)\n\t\tgoto err_out;\n\n\tecc_stngs[nid] = s;\n\n\tpvt = kzalloc(sizeof(struct amd64_pvt), GFP_KERNEL);\n\tif (!pvt)\n\t\tgoto err_settings;\n\n\tpvt->mc_node_id\t= nid;\n\tpvt->F3 = F3;\n\n\tret = per_family_init(pvt);\n\tif (ret < 0)\n\t\tgoto err_enable;\n\n\tret = pvt->ops->hw_info_get(pvt);\n\tif (ret < 0)\n\t\tgoto err_enable;\n\n\tret = 0;\n\tif (!instance_has_memory(pvt)) {\n\t\tamd64_info(\"Node %d: No DIMMs detected.\\n\", nid);\n\t\tgoto err_enable;\n\t}\n\n\tif (!pvt->ops->ecc_enabled(pvt)) {\n\t\tret = -ENODEV;\n\n\t\tif (!ecc_enable_override)\n\t\t\tgoto err_enable;\n\n\t\tif (boot_cpu_data.x86 >= 0x17) {\n\t\t\tamd64_warn(\"Forcing ECC on is not recommended on newer systems. Please enable ECC in BIOS.\");\n\t\t\tgoto err_enable;\n\t\t} else\n\t\t\tamd64_warn(\"Forcing ECC on!\\n\");\n\n\t\tif (!enable_ecc_error_reporting(s, nid, F3))\n\t\t\tgoto err_enable;\n\t}\n\n\tret = init_one_instance(pvt);\n\tif (ret < 0) {\n\t\tamd64_err(\"Error probing instance: %d\\n\", nid);\n\n\t\tif (boot_cpu_data.x86 < 0x17)\n\t\t\trestore_ecc_error_reporting(s, nid, F3);\n\n\t\tgoto err_enable;\n\t}\n\n\tamd64_info(\"%s detected (node %d).\\n\", pvt->ctl_name, pvt->mc_node_id);\n\n\t \n\tpvt->ops->dump_misc_regs(pvt);\n\n\treturn ret;\n\nerr_enable:\n\thw_info_put(pvt);\n\tkfree(pvt);\n\nerr_settings:\n\tkfree(s);\n\tecc_stngs[nid] = NULL;\n\nerr_out:\n\treturn ret;\n}\n\nstatic void remove_one_instance(unsigned int nid)\n{\n\tstruct pci_dev *F3 = node_to_amd_nb(nid)->misc;\n\tstruct ecc_settings *s = ecc_stngs[nid];\n\tstruct mem_ctl_info *mci;\n\tstruct amd64_pvt *pvt;\n\n\t \n\tmci = edac_mc_del_mc(&F3->dev);\n\tif (!mci)\n\t\treturn;\n\n\tpvt = mci->pvt_info;\n\n\trestore_ecc_error_reporting(s, nid, F3);\n\n\tkfree(ecc_stngs[nid]);\n\tecc_stngs[nid] = NULL;\n\n\t \n\tmci->pvt_info = NULL;\n\n\thw_info_put(pvt);\n\tkfree(pvt);\n\tedac_mc_free(mci);\n}\n\nstatic void setup_pci_device(void)\n{\n\tif (pci_ctl)\n\t\treturn;\n\n\tpci_ctl = edac_pci_create_generic_ctl(pci_ctl_dev, EDAC_MOD_STR);\n\tif (!pci_ctl) {\n\t\tpr_warn(\"%s(): Unable to create PCI control\\n\", __func__);\n\t\tpr_warn(\"%s(): PCI error report via EDAC not set\\n\", __func__);\n\t}\n}\n\nstatic const struct x86_cpu_id amd64_cpuids[] = {\n\tX86_MATCH_VENDOR_FAM(AMD,\t0x0F, NULL),\n\tX86_MATCH_VENDOR_FAM(AMD,\t0x10, NULL),\n\tX86_MATCH_VENDOR_FAM(AMD,\t0x15, NULL),\n\tX86_MATCH_VENDOR_FAM(AMD,\t0x16, NULL),\n\tX86_MATCH_VENDOR_FAM(AMD,\t0x17, NULL),\n\tX86_MATCH_VENDOR_FAM(HYGON,\t0x18, NULL),\n\tX86_MATCH_VENDOR_FAM(AMD,\t0x19, NULL),\n\tX86_MATCH_VENDOR_FAM(AMD,\t0x1A, NULL),\n\t{ }\n};\nMODULE_DEVICE_TABLE(x86cpu, amd64_cpuids);\n\nstatic int __init amd64_edac_init(void)\n{\n\tconst char *owner;\n\tint err = -ENODEV;\n\tint i;\n\n\tif (ghes_get_devices())\n\t\treturn -EBUSY;\n\n\towner = edac_get_owner();\n\tif (owner && strncmp(owner, EDAC_MOD_STR, sizeof(EDAC_MOD_STR)))\n\t\treturn -EBUSY;\n\n\tif (!x86_match_cpu(amd64_cpuids))\n\t\treturn -ENODEV;\n\n\tif (!amd_nb_num())\n\t\treturn -ENODEV;\n\n\topstate_init();\n\n\terr = -ENOMEM;\n\tecc_stngs = kcalloc(amd_nb_num(), sizeof(ecc_stngs[0]), GFP_KERNEL);\n\tif (!ecc_stngs)\n\t\tgoto err_free;\n\n\tmsrs = msrs_alloc();\n\tif (!msrs)\n\t\tgoto err_free;\n\n\tfor (i = 0; i < amd_nb_num(); i++) {\n\t\terr = probe_one_instance(i);\n\t\tif (err) {\n\t\t\t \n\t\t\twhile (--i >= 0)\n\t\t\t\tremove_one_instance(i);\n\n\t\t\tgoto err_pci;\n\t\t}\n\t}\n\n\tif (!edac_has_mcs()) {\n\t\terr = -ENODEV;\n\t\tgoto err_pci;\n\t}\n\n\t \n\tif (boot_cpu_data.x86 >= 0x17) {\n\t\tamd_register_ecc_decoder(decode_umc_error);\n\t} else {\n\t\tamd_register_ecc_decoder(decode_bus_error);\n\t\tsetup_pci_device();\n\t}\n\n#ifdef CONFIG_X86_32\n\tamd64_err(\"%s on 32-bit is unsupported. USE AT YOUR OWN RISK!\\n\", EDAC_MOD_STR);\n#endif\n\n\treturn 0;\n\nerr_pci:\n\tpci_ctl_dev = NULL;\n\n\tmsrs_free(msrs);\n\tmsrs = NULL;\n\nerr_free:\n\tkfree(ecc_stngs);\n\tecc_stngs = NULL;\n\n\treturn err;\n}\n\nstatic void __exit amd64_edac_exit(void)\n{\n\tint i;\n\n\tif (pci_ctl)\n\t\tedac_pci_release_generic_ctl(pci_ctl);\n\n\t \n\tif (boot_cpu_data.x86 >= 0x17)\n\t\tamd_unregister_ecc_decoder(decode_umc_error);\n\telse\n\t\tamd_unregister_ecc_decoder(decode_bus_error);\n\n\tfor (i = 0; i < amd_nb_num(); i++)\n\t\tremove_one_instance(i);\n\n\tkfree(ecc_stngs);\n\tecc_stngs = NULL;\n\n\tpci_ctl_dev = NULL;\n\n\tmsrs_free(msrs);\n\tmsrs = NULL;\n}\n\nmodule_init(amd64_edac_init);\nmodule_exit(amd64_edac_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"SoftwareBitMaker: Doug Thompson, Dave Peterson, Thayne Harbaugh; AMD\");\nMODULE_DESCRIPTION(\"MC support for AMD64 memory controllers\");\n\nmodule_param(edac_op_state, int, 0444);\nMODULE_PARM_DESC(edac_op_state, \"EDAC Error Reporting state: 0=Poll,1=NMI\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}