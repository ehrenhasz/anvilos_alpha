{
  "module_name": "raw_mode.c",
  "hash_id": "74c838795ca85b486d93a56dc517113dd29aacac3adecaa94874bc40796d225f",
  "original_prompt": "Ingested from linux-6.6.14/drivers/firmware/arm_scmi/raw_mode.c",
  "human_readable_source": "\n \n \n\n#include <linux/bitmap.h>\n#include <linux/debugfs.h>\n#include <linux/delay.h>\n#include <linux/device.h>\n#include <linux/export.h>\n#include <linux/io.h>\n#include <linux/kernel.h>\n#include <linux/fs.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/poll.h>\n#include <linux/of.h>\n#include <linux/slab.h>\n#include <linux/xarray.h>\n\n#include \"common.h\"\n\n#include \"raw_mode.h\"\n\n#include <trace/events/scmi.h>\n\n#define SCMI_XFER_RAW_MAX_RETRIES\t10\n\n \nstruct scmi_raw_queue {\n\tstruct list_head free_bufs;\n\t \n\tspinlock_t free_bufs_lock;\n\tstruct list_head msg_q;\n\t \n\tspinlock_t msg_q_lock;\n\twait_queue_head_t wq;\n};\n\n \nstruct scmi_raw_mode_info {\n\tunsigned int id;\n\tconst struct scmi_handle *handle;\n\tconst struct scmi_desc *desc;\n\tint tx_max_msg;\n\tstruct scmi_raw_queue *q[SCMI_RAW_MAX_QUEUE];\n\tstruct xarray chans_q;\n\tstruct list_head free_waiters;\n\t \n\tstruct mutex free_mtx;\n\tstruct list_head active_waiters;\n\t \n\tstruct mutex active_mtx;\n\tstruct work_struct waiters_work;\n\tstruct workqueue_struct\t*wait_wq;\n\tstruct dentry *dentry;\n\tvoid *gid;\n};\n\n \nstruct scmi_xfer_raw_waiter {\n\tunsigned long start_jiffies;\n\tstruct scmi_chan_info *cinfo;\n\tstruct scmi_xfer *xfer;\n\tstruct completion async_response;\n\tstruct list_head node;\n};\n\n \nstruct scmi_raw_buffer {\n\tsize_t max_len;\n\tstruct scmi_msg msg;\n\tstruct list_head node;\n};\n\n \nstruct scmi_dbg_raw_data {\n\tu8 chan_id;\n\tstruct scmi_raw_mode_info *raw;\n\tstruct scmi_msg tx;\n\tsize_t tx_size;\n\tsize_t tx_req_size;\n\tstruct scmi_msg rx;\n\tsize_t rx_size;\n};\n\nstatic struct scmi_raw_queue *\nscmi_raw_queue_select(struct scmi_raw_mode_info *raw, unsigned int idx,\n\t\t      unsigned int chan_id)\n{\n\tif (!chan_id)\n\t\treturn raw->q[idx];\n\n\treturn xa_load(&raw->chans_q, chan_id);\n}\n\nstatic struct scmi_raw_buffer *scmi_raw_buffer_get(struct scmi_raw_queue *q)\n{\n\tunsigned long flags;\n\tstruct scmi_raw_buffer *rb = NULL;\n\tstruct list_head *head = &q->free_bufs;\n\n\tspin_lock_irqsave(&q->free_bufs_lock, flags);\n\tif (!list_empty(head)) {\n\t\trb = list_first_entry(head, struct scmi_raw_buffer, node);\n\t\tlist_del_init(&rb->node);\n\t}\n\tspin_unlock_irqrestore(&q->free_bufs_lock, flags);\n\n\treturn rb;\n}\n\nstatic void scmi_raw_buffer_put(struct scmi_raw_queue *q,\n\t\t\t\tstruct scmi_raw_buffer *rb)\n{\n\tunsigned long flags;\n\n\t \n\trb->msg.len = rb->max_len;\n\n\tspin_lock_irqsave(&q->free_bufs_lock, flags);\n\tlist_add_tail(&rb->node, &q->free_bufs);\n\tspin_unlock_irqrestore(&q->free_bufs_lock, flags);\n}\n\nstatic void scmi_raw_buffer_enqueue(struct scmi_raw_queue *q,\n\t\t\t\t    struct scmi_raw_buffer *rb)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&q->msg_q_lock, flags);\n\tlist_add_tail(&rb->node, &q->msg_q);\n\tspin_unlock_irqrestore(&q->msg_q_lock, flags);\n\n\twake_up_interruptible(&q->wq);\n}\n\nstatic struct scmi_raw_buffer*\nscmi_raw_buffer_dequeue_unlocked(struct scmi_raw_queue *q)\n{\n\tstruct scmi_raw_buffer *rb = NULL;\n\n\tif (!list_empty(&q->msg_q)) {\n\t\trb = list_first_entry(&q->msg_q, struct scmi_raw_buffer, node);\n\t\tlist_del_init(&rb->node);\n\t}\n\n\treturn rb;\n}\n\nstatic struct scmi_raw_buffer *scmi_raw_buffer_dequeue(struct scmi_raw_queue *q)\n{\n\tunsigned long flags;\n\tstruct scmi_raw_buffer *rb;\n\n\tspin_lock_irqsave(&q->msg_q_lock, flags);\n\trb = scmi_raw_buffer_dequeue_unlocked(q);\n\tspin_unlock_irqrestore(&q->msg_q_lock, flags);\n\n\treturn rb;\n}\n\nstatic void scmi_raw_buffer_queue_flush(struct scmi_raw_queue *q)\n{\n\tstruct scmi_raw_buffer *rb;\n\n\tdo {\n\t\trb = scmi_raw_buffer_dequeue(q);\n\t\tif (rb)\n\t\t\tscmi_raw_buffer_put(q, rb);\n\t} while (rb);\n}\n\nstatic struct scmi_xfer_raw_waiter *\nscmi_xfer_raw_waiter_get(struct scmi_raw_mode_info *raw, struct scmi_xfer *xfer,\n\t\t\t struct scmi_chan_info *cinfo, bool async)\n{\n\tstruct scmi_xfer_raw_waiter *rw = NULL;\n\n\tmutex_lock(&raw->free_mtx);\n\tif (!list_empty(&raw->free_waiters)) {\n\t\trw = list_first_entry(&raw->free_waiters,\n\t\t\t\t      struct scmi_xfer_raw_waiter, node);\n\t\tlist_del_init(&rw->node);\n\n\t\tif (async) {\n\t\t\treinit_completion(&rw->async_response);\n\t\t\txfer->async_done = &rw->async_response;\n\t\t}\n\n\t\trw->cinfo = cinfo;\n\t\trw->xfer = xfer;\n\t}\n\tmutex_unlock(&raw->free_mtx);\n\n\treturn rw;\n}\n\nstatic void scmi_xfer_raw_waiter_put(struct scmi_raw_mode_info *raw,\n\t\t\t\t     struct scmi_xfer_raw_waiter *rw)\n{\n\tif (rw->xfer) {\n\t\trw->xfer->async_done = NULL;\n\t\trw->xfer = NULL;\n\t}\n\n\tmutex_lock(&raw->free_mtx);\n\tlist_add_tail(&rw->node, &raw->free_waiters);\n\tmutex_unlock(&raw->free_mtx);\n}\n\nstatic void scmi_xfer_raw_waiter_enqueue(struct scmi_raw_mode_info *raw,\n\t\t\t\t\t struct scmi_xfer_raw_waiter *rw)\n{\n\t \n\trw->start_jiffies = jiffies;\n\n\ttrace_scmi_xfer_response_wait(rw->xfer->transfer_id, rw->xfer->hdr.id,\n\t\t\t\t      rw->xfer->hdr.protocol_id,\n\t\t\t\t      rw->xfer->hdr.seq,\n\t\t\t\t      raw->desc->max_rx_timeout_ms,\n\t\t\t\t      rw->xfer->hdr.poll_completion);\n\n\tmutex_lock(&raw->active_mtx);\n\tlist_add_tail(&rw->node, &raw->active_waiters);\n\tmutex_unlock(&raw->active_mtx);\n\n\t \n\tqueue_work(raw->wait_wq, &raw->waiters_work);\n}\n\nstatic struct scmi_xfer_raw_waiter *\nscmi_xfer_raw_waiter_dequeue(struct scmi_raw_mode_info *raw)\n{\n\tstruct scmi_xfer_raw_waiter *rw = NULL;\n\n\tmutex_lock(&raw->active_mtx);\n\tif (!list_empty(&raw->active_waiters)) {\n\t\trw = list_first_entry(&raw->active_waiters,\n\t\t\t\t      struct scmi_xfer_raw_waiter, node);\n\t\tlist_del_init(&rw->node);\n\t}\n\tmutex_unlock(&raw->active_mtx);\n\n\treturn rw;\n}\n\n \nstatic void scmi_xfer_raw_worker(struct work_struct *work)\n{\n\tstruct scmi_raw_mode_info *raw;\n\tstruct device *dev;\n\tunsigned long max_tmo;\n\n\traw = container_of(work, struct scmi_raw_mode_info, waiters_work);\n\tdev = raw->handle->dev;\n\tmax_tmo = msecs_to_jiffies(raw->desc->max_rx_timeout_ms);\n\n\tdo {\n\t\tint ret = 0;\n\t\tunsigned int timeout_ms;\n\t\tunsigned long aging;\n\t\tstruct scmi_xfer *xfer;\n\t\tstruct scmi_xfer_raw_waiter *rw;\n\t\tstruct scmi_chan_info *cinfo;\n\n\t\trw = scmi_xfer_raw_waiter_dequeue(raw);\n\t\tif (!rw)\n\t\t\treturn;\n\n\t\tcinfo = rw->cinfo;\n\t\txfer = rw->xfer;\n\t\t \n\t\taging = jiffies - rw->start_jiffies;\n\t\ttimeout_ms = max_tmo > aging ?\n\t\t\tjiffies_to_msecs(max_tmo - aging) : 1;\n\n\t\tret = scmi_xfer_raw_wait_for_message_response(cinfo, xfer,\n\t\t\t\t\t\t\t      timeout_ms);\n\t\tif (!ret && xfer->hdr.status)\n\t\t\tret = scmi_to_linux_errno(xfer->hdr.status);\n\n\t\tif (raw->desc->ops->mark_txdone)\n\t\t\traw->desc->ops->mark_txdone(rw->cinfo, ret, xfer);\n\n\t\ttrace_scmi_xfer_end(xfer->transfer_id, xfer->hdr.id,\n\t\t\t\t    xfer->hdr.protocol_id, xfer->hdr.seq, ret);\n\n\t\t \n\t\tif (!ret && xfer->async_done) {\n\t\t\tunsigned long tmo = msecs_to_jiffies(SCMI_MAX_RESPONSE_TIMEOUT);\n\n\t\t\tif (!wait_for_completion_timeout(xfer->async_done, tmo))\n\t\t\t\tdev_err(dev,\n\t\t\t\t\t\"timed out in RAW delayed resp - HDR:%08X\\n\",\n\t\t\t\t\tpack_scmi_header(&xfer->hdr));\n\t\t}\n\n\t\t \n\t\tscmi_xfer_raw_put(raw->handle, xfer);\n\t\tscmi_xfer_raw_waiter_put(raw, rw);\n\t} while (1);\n}\n\nstatic void scmi_xfer_raw_reset(struct scmi_raw_mode_info *raw)\n{\n\tint i;\n\n\tdev_info(raw->handle->dev, \"Resetting SCMI Raw stack.\\n\");\n\n\tfor (i = 0; i < SCMI_RAW_MAX_QUEUE; i++)\n\t\tscmi_raw_buffer_queue_flush(raw->q[i]);\n}\n\n \nstatic int scmi_xfer_raw_get_init(struct scmi_raw_mode_info *raw, void *buf,\n\t\t\t\t  size_t len, struct scmi_xfer **p)\n{\n\tu32 msg_hdr;\n\tsize_t tx_size;\n\tstruct scmi_xfer *xfer;\n\tint ret, retry = SCMI_XFER_RAW_MAX_RETRIES;\n\tstruct device *dev = raw->handle->dev;\n\n\tif (!buf || len < sizeof(u32))\n\t\treturn -EINVAL;\n\n\ttx_size = len - sizeof(u32);\n\t \n\tif (tx_size > raw->desc->max_msg_size)\n\t\treturn -ERANGE;\n\n\txfer = scmi_xfer_raw_get(raw->handle);\n\tif (IS_ERR(xfer)) {\n\t\tdev_warn(dev, \"RAW - Cannot get a free RAW xfer !\\n\");\n\t\treturn PTR_ERR(xfer);\n\t}\n\n\t \n\tmsg_hdr = le32_to_cpu(*((__le32 *)buf));\n\tunpack_scmi_header(msg_hdr, &xfer->hdr);\n\txfer->hdr.seq = (u16)MSG_XTRACT_TOKEN(msg_hdr);\n\t \n\txfer->hdr.poll_completion = false;\n\txfer->hdr.status = SCMI_SUCCESS;\n\txfer->tx.len = tx_size;\n\txfer->rx.len = raw->desc->max_msg_size;\n\t \n\tmemset(xfer->tx.buf, 0x00, raw->desc->max_msg_size);\n\tif (xfer->tx.len)\n\t\tmemcpy(xfer->tx.buf, (u8 *)buf + sizeof(msg_hdr), xfer->tx.len);\n\t*p = xfer;\n\n\t \n\tdo {\n\t\tret = scmi_xfer_raw_inflight_register(raw->handle, xfer);\n\t\tif (ret) {\n\t\t\tdev_dbg(dev,\n\t\t\t\t\"...retrying[%d] inflight registration\\n\",\n\t\t\t\tretry);\n\t\t\tmsleep(raw->desc->max_rx_timeout_ms /\n\t\t\t       SCMI_XFER_RAW_MAX_RETRIES);\n\t\t}\n\t} while (ret && --retry);\n\n\tif (ret) {\n\t\tdev_warn(dev,\n\t\t\t \"RAW - Could NOT register xfer %d in-flight HDR:0x%08X\\n\",\n\t\t\t xfer->hdr.seq, msg_hdr);\n\t\tscmi_xfer_raw_put(raw->handle, xfer);\n\t}\n\n\treturn ret;\n}\n\n \nstatic int scmi_do_xfer_raw_start(struct scmi_raw_mode_info *raw,\n\t\t\t\t  struct scmi_xfer *xfer, u8 chan_id,\n\t\t\t\t  bool async)\n{\n\tint ret;\n\tstruct scmi_chan_info *cinfo;\n\tstruct scmi_xfer_raw_waiter *rw;\n\tstruct device *dev = raw->handle->dev;\n\n\tif (!chan_id)\n\t\tchan_id = xfer->hdr.protocol_id;\n\telse\n\t\txfer->flags |= SCMI_XFER_FLAG_CHAN_SET;\n\n\tcinfo = scmi_xfer_raw_channel_get(raw->handle, chan_id);\n\tif (IS_ERR(cinfo))\n\t\treturn PTR_ERR(cinfo);\n\n\trw = scmi_xfer_raw_waiter_get(raw, xfer, cinfo, async);\n\tif (!rw) {\n\t\tdev_warn(dev, \"RAW - Cannot get a free waiter !\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tif (is_polling_enabled(cinfo, raw->desc))\n\t\txfer->hdr.poll_completion = true;\n\n\treinit_completion(&xfer->done);\n\t \n\tsmp_store_mb(xfer->state, SCMI_XFER_SENT_OK);\n\n\ttrace_scmi_xfer_begin(xfer->transfer_id, xfer->hdr.id,\n\t\t\t      xfer->hdr.protocol_id, xfer->hdr.seq,\n\t\t\t      xfer->hdr.poll_completion);\n\n\tret = raw->desc->ops->send_message(rw->cinfo, xfer);\n\tif (ret) {\n\t\tdev_err(dev, \"Failed to send RAW message %d\\n\", ret);\n\t\tscmi_xfer_raw_waiter_put(raw, rw);\n\t\treturn ret;\n\t}\n\n\ttrace_scmi_msg_dump(raw->id, cinfo->id, xfer->hdr.protocol_id,\n\t\t\t    xfer->hdr.id, \"cmnd\", xfer->hdr.seq,\n\t\t\t    xfer->hdr.status,\n\t\t\t    xfer->tx.buf, xfer->tx.len);\n\n\tscmi_xfer_raw_waiter_enqueue(raw, rw);\n\n\treturn ret;\n}\n\n \nstatic int scmi_raw_message_send(struct scmi_raw_mode_info *raw,\n\t\t\t\t void *buf, size_t len, u8 chan_id, bool async)\n{\n\tint ret;\n\tstruct scmi_xfer *xfer;\n\n\tret = scmi_xfer_raw_get_init(raw, buf, len, &xfer);\n\tif (ret)\n\t\treturn ret;\n\n\tret = scmi_do_xfer_raw_start(raw, xfer, chan_id, async);\n\tif (ret)\n\t\tscmi_xfer_raw_put(raw->handle, xfer);\n\n\treturn ret;\n}\n\nstatic struct scmi_raw_buffer *\nscmi_raw_message_dequeue(struct scmi_raw_queue *q, bool o_nonblock)\n{\n\tunsigned long flags;\n\tstruct scmi_raw_buffer *rb;\n\n\tspin_lock_irqsave(&q->msg_q_lock, flags);\n\twhile (list_empty(&q->msg_q)) {\n\t\tspin_unlock_irqrestore(&q->msg_q_lock, flags);\n\n\t\tif (o_nonblock)\n\t\t\treturn ERR_PTR(-EAGAIN);\n\n\t\tif (wait_event_interruptible(q->wq, !list_empty(&q->msg_q)))\n\t\t\treturn ERR_PTR(-ERESTARTSYS);\n\n\t\tspin_lock_irqsave(&q->msg_q_lock, flags);\n\t}\n\n\trb = scmi_raw_buffer_dequeue_unlocked(q);\n\n\tspin_unlock_irqrestore(&q->msg_q_lock, flags);\n\n\treturn rb;\n}\n\n \nstatic int scmi_raw_message_receive(struct scmi_raw_mode_info *raw,\n\t\t\t\t    void *buf, size_t len, size_t *size,\n\t\t\t\t    unsigned int idx, unsigned int chan_id,\n\t\t\t\t    bool o_nonblock)\n{\n\tint ret = 0;\n\tstruct scmi_raw_buffer *rb;\n\tstruct scmi_raw_queue *q;\n\n\tq = scmi_raw_queue_select(raw, idx, chan_id);\n\tif (!q)\n\t\treturn -ENODEV;\n\n\trb = scmi_raw_message_dequeue(q, o_nonblock);\n\tif (IS_ERR(rb)) {\n\t\tdev_dbg(raw->handle->dev, \"RAW - No message available!\\n\");\n\t\treturn PTR_ERR(rb);\n\t}\n\n\tif (rb->msg.len <= len) {\n\t\tmemcpy(buf, rb->msg.buf, rb->msg.len);\n\t\t*size = rb->msg.len;\n\t} else {\n\t\tret = -ENOSPC;\n\t}\n\n\tscmi_raw_buffer_put(q, rb);\n\n\treturn ret;\n}\n\n \n\nstatic ssize_t scmi_dbg_raw_mode_common_read(struct file *filp,\n\t\t\t\t\t     char __user *buf,\n\t\t\t\t\t     size_t count, loff_t *ppos,\n\t\t\t\t\t     unsigned int idx)\n{\n\tssize_t cnt;\n\tstruct scmi_dbg_raw_data *rd = filp->private_data;\n\n\tif (!rd->rx_size) {\n\t\tint ret;\n\n\t\tret = scmi_raw_message_receive(rd->raw, rd->rx.buf, rd->rx.len,\n\t\t\t\t\t       &rd->rx_size, idx, rd->chan_id,\n\t\t\t\t\t       filp->f_flags & O_NONBLOCK);\n\t\tif (ret) {\n\t\t\trd->rx_size = 0;\n\t\t\treturn ret;\n\t\t}\n\n\t\t \n\t\t*ppos = 0;\n\t} else if (*ppos == rd->rx_size) {\n\t\t \n\t\trd->rx_size = 0;\n\t\treturn 0;\n\t}\n\n\tcnt = simple_read_from_buffer(buf, count, ppos,\n\t\t\t\t      rd->rx.buf, rd->rx_size);\n\n\treturn cnt;\n}\n\nstatic ssize_t scmi_dbg_raw_mode_common_write(struct file *filp,\n\t\t\t\t\t      const char __user *buf,\n\t\t\t\t\t      size_t count, loff_t *ppos,\n\t\t\t\t\t      bool async)\n{\n\tint ret;\n\tstruct scmi_dbg_raw_data *rd = filp->private_data;\n\n\tif (count > rd->tx.len - rd->tx_size)\n\t\treturn -ENOSPC;\n\n\t \n\tif (!rd->tx_size)\n\t\trd->tx_req_size = count;\n\n\t \n\tif (rd->tx_size < rd->tx_req_size) {\n\t\tssize_t cnt;\n\n\t\tcnt = simple_write_to_buffer(rd->tx.buf, rd->tx.len, ppos,\n\t\t\t\t\t     buf, count);\n\t\tif (cnt < 0)\n\t\t\treturn cnt;\n\n\t\trd->tx_size += cnt;\n\t\tif (cnt < count)\n\t\t\treturn cnt;\n\t}\n\n\tret = scmi_raw_message_send(rd->raw, rd->tx.buf, rd->tx_size,\n\t\t\t\t    rd->chan_id, async);\n\n\t \n\trd->tx_size = 0;\n\t*ppos = 0;\n\n\treturn ret ?: count;\n}\n\nstatic __poll_t scmi_test_dbg_raw_common_poll(struct file *filp,\n\t\t\t\t\t      struct poll_table_struct *wait,\n\t\t\t\t\t      unsigned int idx)\n{\n\tunsigned long flags;\n\tstruct scmi_dbg_raw_data *rd = filp->private_data;\n\tstruct scmi_raw_queue *q;\n\t__poll_t mask = 0;\n\n\tq = scmi_raw_queue_select(rd->raw, idx, rd->chan_id);\n\tif (!q)\n\t\treturn mask;\n\n\tpoll_wait(filp, &q->wq, wait);\n\n\tspin_lock_irqsave(&q->msg_q_lock, flags);\n\tif (!list_empty(&q->msg_q))\n\t\tmask = EPOLLIN | EPOLLRDNORM;\n\tspin_unlock_irqrestore(&q->msg_q_lock, flags);\n\n\treturn mask;\n}\n\nstatic ssize_t scmi_dbg_raw_mode_message_read(struct file *filp,\n\t\t\t\t\t      char __user *buf,\n\t\t\t\t\t      size_t count, loff_t *ppos)\n{\n\treturn scmi_dbg_raw_mode_common_read(filp, buf, count, ppos,\n\t\t\t\t\t     SCMI_RAW_REPLY_QUEUE);\n}\n\nstatic ssize_t scmi_dbg_raw_mode_message_write(struct file *filp,\n\t\t\t\t\t       const char __user *buf,\n\t\t\t\t\t       size_t count, loff_t *ppos)\n{\n\treturn scmi_dbg_raw_mode_common_write(filp, buf, count, ppos, false);\n}\n\nstatic __poll_t scmi_dbg_raw_mode_message_poll(struct file *filp,\n\t\t\t\t\t       struct poll_table_struct *wait)\n{\n\treturn scmi_test_dbg_raw_common_poll(filp, wait, SCMI_RAW_REPLY_QUEUE);\n}\n\nstatic int scmi_dbg_raw_mode_open(struct inode *inode, struct file *filp)\n{\n\tu8 id;\n\tstruct scmi_raw_mode_info *raw;\n\tstruct scmi_dbg_raw_data *rd;\n\tconst char *id_str = filp->f_path.dentry->d_parent->d_name.name;\n\n\tif (!inode->i_private)\n\t\treturn -ENODEV;\n\n\traw = inode->i_private;\n\trd = kzalloc(sizeof(*rd), GFP_KERNEL);\n\tif (!rd)\n\t\treturn -ENOMEM;\n\n\trd->rx.len = raw->desc->max_msg_size + sizeof(u32);\n\trd->rx.buf = kzalloc(rd->rx.len, GFP_KERNEL);\n\tif (!rd->rx.buf) {\n\t\tkfree(rd);\n\t\treturn -ENOMEM;\n\t}\n\n\trd->tx.len = raw->desc->max_msg_size + sizeof(u32);\n\trd->tx.buf = kzalloc(rd->tx.len, GFP_KERNEL);\n\tif (!rd->tx.buf) {\n\t\tkfree(rd->rx.buf);\n\t\tkfree(rd);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tif (!kstrtou8(id_str, 16, &id))\n\t\trd->chan_id = id;\n\n\trd->raw = raw;\n\tfilp->private_data = rd;\n\n\treturn 0;\n}\n\nstatic int scmi_dbg_raw_mode_release(struct inode *inode, struct file *filp)\n{\n\tstruct scmi_dbg_raw_data *rd = filp->private_data;\n\n\tkfree(rd->rx.buf);\n\tkfree(rd->tx.buf);\n\tkfree(rd);\n\n\treturn 0;\n}\n\nstatic ssize_t scmi_dbg_raw_mode_reset_write(struct file *filp,\n\t\t\t\t\t     const char __user *buf,\n\t\t\t\t\t     size_t count, loff_t *ppos)\n{\n\tstruct scmi_dbg_raw_data *rd = filp->private_data;\n\n\tscmi_xfer_raw_reset(rd->raw);\n\n\treturn count;\n}\n\nstatic const struct file_operations scmi_dbg_raw_mode_reset_fops = {\n\t.open = scmi_dbg_raw_mode_open,\n\t.release = scmi_dbg_raw_mode_release,\n\t.write = scmi_dbg_raw_mode_reset_write,\n\t.owner = THIS_MODULE,\n};\n\nstatic const struct file_operations scmi_dbg_raw_mode_message_fops = {\n\t.open = scmi_dbg_raw_mode_open,\n\t.release = scmi_dbg_raw_mode_release,\n\t.read = scmi_dbg_raw_mode_message_read,\n\t.write = scmi_dbg_raw_mode_message_write,\n\t.poll = scmi_dbg_raw_mode_message_poll,\n\t.owner = THIS_MODULE,\n};\n\nstatic ssize_t scmi_dbg_raw_mode_message_async_write(struct file *filp,\n\t\t\t\t\t\t     const char __user *buf,\n\t\t\t\t\t\t     size_t count, loff_t *ppos)\n{\n\treturn scmi_dbg_raw_mode_common_write(filp, buf, count, ppos, true);\n}\n\nstatic const struct file_operations scmi_dbg_raw_mode_message_async_fops = {\n\t.open = scmi_dbg_raw_mode_open,\n\t.release = scmi_dbg_raw_mode_release,\n\t.read = scmi_dbg_raw_mode_message_read,\n\t.write = scmi_dbg_raw_mode_message_async_write,\n\t.poll = scmi_dbg_raw_mode_message_poll,\n\t.owner = THIS_MODULE,\n};\n\nstatic ssize_t scmi_test_dbg_raw_mode_notif_read(struct file *filp,\n\t\t\t\t\t\t char __user *buf,\n\t\t\t\t\t\t size_t count, loff_t *ppos)\n{\n\treturn scmi_dbg_raw_mode_common_read(filp, buf, count, ppos,\n\t\t\t\t\t     SCMI_RAW_NOTIF_QUEUE);\n}\n\nstatic __poll_t\nscmi_test_dbg_raw_mode_notif_poll(struct file *filp,\n\t\t\t\t  struct poll_table_struct *wait)\n{\n\treturn scmi_test_dbg_raw_common_poll(filp, wait, SCMI_RAW_NOTIF_QUEUE);\n}\n\nstatic const struct file_operations scmi_dbg_raw_mode_notification_fops = {\n\t.open = scmi_dbg_raw_mode_open,\n\t.release = scmi_dbg_raw_mode_release,\n\t.read = scmi_test_dbg_raw_mode_notif_read,\n\t.poll = scmi_test_dbg_raw_mode_notif_poll,\n\t.owner = THIS_MODULE,\n};\n\nstatic ssize_t scmi_test_dbg_raw_mode_errors_read(struct file *filp,\n\t\t\t\t\t\t  char __user *buf,\n\t\t\t\t\t\t  size_t count, loff_t *ppos)\n{\n\treturn scmi_dbg_raw_mode_common_read(filp, buf, count, ppos,\n\t\t\t\t\t     SCMI_RAW_ERRS_QUEUE);\n}\n\nstatic __poll_t\nscmi_test_dbg_raw_mode_errors_poll(struct file *filp,\n\t\t\t\t   struct poll_table_struct *wait)\n{\n\treturn scmi_test_dbg_raw_common_poll(filp, wait, SCMI_RAW_ERRS_QUEUE);\n}\n\nstatic const struct file_operations scmi_dbg_raw_mode_errors_fops = {\n\t.open = scmi_dbg_raw_mode_open,\n\t.release = scmi_dbg_raw_mode_release,\n\t.read = scmi_test_dbg_raw_mode_errors_read,\n\t.poll = scmi_test_dbg_raw_mode_errors_poll,\n\t.owner = THIS_MODULE,\n};\n\nstatic struct scmi_raw_queue *\nscmi_raw_queue_init(struct scmi_raw_mode_info *raw)\n{\n\tint i;\n\tstruct scmi_raw_buffer *rb;\n\tstruct device *dev = raw->handle->dev;\n\tstruct scmi_raw_queue *q;\n\n\tq = devm_kzalloc(dev, sizeof(*q), GFP_KERNEL);\n\tif (!q)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\trb = devm_kcalloc(dev, raw->tx_max_msg, sizeof(*rb), GFP_KERNEL);\n\tif (!rb)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tspin_lock_init(&q->free_bufs_lock);\n\tINIT_LIST_HEAD(&q->free_bufs);\n\tfor (i = 0; i < raw->tx_max_msg; i++, rb++) {\n\t\trb->max_len = raw->desc->max_msg_size + sizeof(u32);\n\t\trb->msg.buf = devm_kzalloc(dev, rb->max_len, GFP_KERNEL);\n\t\tif (!rb->msg.buf)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\tscmi_raw_buffer_put(q, rb);\n\t}\n\n\tspin_lock_init(&q->msg_q_lock);\n\tINIT_LIST_HEAD(&q->msg_q);\n\tinit_waitqueue_head(&q->wq);\n\n\treturn q;\n}\n\nstatic int scmi_xfer_raw_worker_init(struct scmi_raw_mode_info *raw)\n{\n\tint i;\n\tstruct scmi_xfer_raw_waiter *rw;\n\tstruct device *dev = raw->handle->dev;\n\n\trw = devm_kcalloc(dev, raw->tx_max_msg, sizeof(*rw), GFP_KERNEL);\n\tif (!rw)\n\t\treturn -ENOMEM;\n\n\traw->wait_wq = alloc_workqueue(\"scmi-raw-wait-wq-%d\",\n\t\t\t\t       WQ_UNBOUND | WQ_FREEZABLE |\n\t\t\t\t       WQ_HIGHPRI | WQ_SYSFS, 0, raw->id);\n\tif (!raw->wait_wq)\n\t\treturn -ENOMEM;\n\n\tmutex_init(&raw->free_mtx);\n\tINIT_LIST_HEAD(&raw->free_waiters);\n\tmutex_init(&raw->active_mtx);\n\tINIT_LIST_HEAD(&raw->active_waiters);\n\n\tfor (i = 0; i < raw->tx_max_msg; i++, rw++) {\n\t\tinit_completion(&rw->async_response);\n\t\tscmi_xfer_raw_waiter_put(raw, rw);\n\t}\n\tINIT_WORK(&raw->waiters_work, scmi_xfer_raw_worker);\n\n\treturn 0;\n}\n\nstatic int scmi_raw_mode_setup(struct scmi_raw_mode_info *raw,\n\t\t\t       u8 *channels, int num_chans)\n{\n\tint ret, idx;\n\tvoid *gid;\n\tstruct device *dev = raw->handle->dev;\n\n\tgid = devres_open_group(dev, NULL, GFP_KERNEL);\n\tif (!gid)\n\t\treturn -ENOMEM;\n\n\tfor (idx = 0; idx < SCMI_RAW_MAX_QUEUE; idx++) {\n\t\traw->q[idx] = scmi_raw_queue_init(raw);\n\t\tif (IS_ERR(raw->q[idx])) {\n\t\t\tret = PTR_ERR(raw->q[idx]);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\txa_init(&raw->chans_q);\n\tif (num_chans > 1) {\n\t\tint i;\n\n\t\tfor (i = 0; i < num_chans; i++) {\n\t\t\tvoid *xret;\n\t\t\tstruct scmi_raw_queue *q;\n\n\t\t\tq = scmi_raw_queue_init(raw);\n\t\t\tif (IS_ERR(q)) {\n\t\t\t\tret = PTR_ERR(q);\n\t\t\t\tgoto err_xa;\n\t\t\t}\n\n\t\t\txret = xa_store(&raw->chans_q, channels[i], q,\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (xa_err(xret)) {\n\t\t\t\tdev_err(dev,\n\t\t\t\t\t\"Fail to allocate Raw queue 0x%02X\\n\",\n\t\t\t\t\tchannels[i]);\n\t\t\t\tret = xa_err(xret);\n\t\t\t\tgoto err_xa;\n\t\t\t}\n\t\t}\n\t}\n\n\tret = scmi_xfer_raw_worker_init(raw);\n\tif (ret)\n\t\tgoto err_xa;\n\n\tdevres_close_group(dev, gid);\n\traw->gid = gid;\n\n\treturn 0;\n\nerr_xa:\n\txa_destroy(&raw->chans_q);\nerr:\n\tdevres_release_group(dev, gid);\n\treturn ret;\n}\n\n \nvoid *scmi_raw_mode_init(const struct scmi_handle *handle,\n\t\t\t struct dentry *top_dentry, int instance_id,\n\t\t\t u8 *channels, int num_chans,\n\t\t\t const struct scmi_desc *desc, int tx_max_msg)\n{\n\tint ret;\n\tstruct scmi_raw_mode_info *raw;\n\tstruct device *dev;\n\n\tif (!handle || !desc)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tdev = handle->dev;\n\traw = devm_kzalloc(dev, sizeof(*raw), GFP_KERNEL);\n\tif (!raw)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\traw->handle = handle;\n\traw->desc = desc;\n\traw->tx_max_msg = tx_max_msg;\n\traw->id = instance_id;\n\n\tret = scmi_raw_mode_setup(raw, channels, num_chans);\n\tif (ret) {\n\t\tdevm_kfree(dev, raw);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\traw->dentry = debugfs_create_dir(\"raw\", top_dentry);\n\n\tdebugfs_create_file(\"reset\", 0200, raw->dentry, raw,\n\t\t\t    &scmi_dbg_raw_mode_reset_fops);\n\n\tdebugfs_create_file(\"message\", 0600, raw->dentry, raw,\n\t\t\t    &scmi_dbg_raw_mode_message_fops);\n\n\tdebugfs_create_file(\"message_async\", 0600, raw->dentry, raw,\n\t\t\t    &scmi_dbg_raw_mode_message_async_fops);\n\n\tdebugfs_create_file(\"notification\", 0400, raw->dentry, raw,\n\t\t\t    &scmi_dbg_raw_mode_notification_fops);\n\n\tdebugfs_create_file(\"errors\", 0400, raw->dentry, raw,\n\t\t\t    &scmi_dbg_raw_mode_errors_fops);\n\n\t \n\tif (num_chans > 1) {\n\t\tint i;\n\t\tstruct dentry *top_chans;\n\n\t\ttop_chans = debugfs_create_dir(\"channels\", raw->dentry);\n\n\t\tfor (i = 0; i < num_chans; i++) {\n\t\t\tchar cdir[8];\n\t\t\tstruct dentry *chd;\n\n\t\t\tsnprintf(cdir, 8, \"0x%02X\", channels[i]);\n\t\t\tchd = debugfs_create_dir(cdir, top_chans);\n\n\t\t\tdebugfs_create_file(\"message\", 0600, chd, raw,\n\t\t\t\t\t    &scmi_dbg_raw_mode_message_fops);\n\n\t\t\tdebugfs_create_file(\"message_async\", 0600, chd, raw,\n\t\t\t\t\t    &scmi_dbg_raw_mode_message_async_fops);\n\t\t}\n\t}\n\n\tdev_info(dev, \"SCMI RAW Mode initialized for instance %d\\n\", raw->id);\n\n\treturn raw;\n}\n\n \nvoid scmi_raw_mode_cleanup(void *r)\n{\n\tstruct scmi_raw_mode_info *raw = r;\n\n\tif (!raw)\n\t\treturn;\n\n\tdebugfs_remove_recursive(raw->dentry);\n\n\tcancel_work_sync(&raw->waiters_work);\n\tdestroy_workqueue(raw->wait_wq);\n\txa_destroy(&raw->chans_q);\n}\n\nstatic int scmi_xfer_raw_collect(void *msg, size_t *msg_len,\n\t\t\t\t struct scmi_xfer *xfer)\n{\n\t__le32 *m;\n\tsize_t msg_size;\n\n\tif (!xfer || !msg || !msg_len)\n\t\treturn -EINVAL;\n\n\t \n\tmsg_size = xfer->rx.len + sizeof(u32);\n\t \n\tif (xfer->hdr.type != MSG_TYPE_NOTIFICATION)\n\t\tmsg_size += sizeof(u32);\n\n\tif (msg_size > *msg_len)\n\t\treturn -ENOSPC;\n\n\tm = msg;\n\t*m = cpu_to_le32(pack_scmi_header(&xfer->hdr));\n\tif (xfer->hdr.type != MSG_TYPE_NOTIFICATION)\n\t\t*++m = cpu_to_le32(xfer->hdr.status);\n\n\tmemcpy(++m, xfer->rx.buf, xfer->rx.len);\n\n\t*msg_len = msg_size;\n\n\treturn 0;\n}\n\n \nvoid scmi_raw_message_report(void *r, struct scmi_xfer *xfer,\n\t\t\t     unsigned int idx, unsigned int chan_id)\n{\n\tint ret;\n\tunsigned long flags;\n\tstruct scmi_raw_buffer *rb;\n\tstruct device *dev;\n\tstruct scmi_raw_queue *q;\n\tstruct scmi_raw_mode_info *raw = r;\n\n\tif (!raw || (idx == SCMI_RAW_REPLY_QUEUE && !SCMI_XFER_IS_RAW(xfer)))\n\t\treturn;\n\n\tdev = raw->handle->dev;\n\tq = scmi_raw_queue_select(raw, idx,\n\t\t\t\t  SCMI_XFER_IS_CHAN_SET(xfer) ? chan_id : 0);\n\n\t \n\tspin_lock_irqsave(&q->msg_q_lock, flags);\n\trb = scmi_raw_buffer_get(q);\n\tif (!rb) {\n\t\t \n\t\tif (idx == SCMI_RAW_REPLY_QUEUE) {\n\t\t\tspin_unlock_irqrestore(&q->msg_q_lock, flags);\n\t\t\tdev_warn(dev,\n\t\t\t\t \"RAW[%d] - Buffers exhausted. Dropping report.\\n\",\n\t\t\t\t idx);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\trb = scmi_raw_buffer_dequeue_unlocked(q);\n\t\tif (WARN_ON(!rb)) {\n\t\t\tspin_unlock_irqrestore(&q->msg_q_lock, flags);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\trb->msg.len = rb->max_len;\n\n\t\tdev_warn_once(dev,\n\t\t\t      \"RAW[%d] - Buffers exhausted. Re-using oldest.\\n\",\n\t\t\t      idx);\n\t}\n\tspin_unlock_irqrestore(&q->msg_q_lock, flags);\n\n\tret = scmi_xfer_raw_collect(rb->msg.buf, &rb->msg.len, xfer);\n\tif (ret) {\n\t\tdev_warn(dev, \"RAW - Cannot collect xfer into buffer !\\n\");\n\t\tscmi_raw_buffer_put(q, rb);\n\t\treturn;\n\t}\n\n\tscmi_raw_buffer_enqueue(q, rb);\n}\n\nstatic void scmi_xfer_raw_fill(struct scmi_raw_mode_info *raw,\n\t\t\t       struct scmi_chan_info *cinfo,\n\t\t\t       struct scmi_xfer *xfer, u32 msg_hdr)\n{\n\t \n\tunpack_scmi_header(msg_hdr, &xfer->hdr);\n\txfer->hdr.seq = MSG_XTRACT_TOKEN(msg_hdr);\n\n\tmemset(xfer->rx.buf, 0x00, xfer->rx.len);\n\n\traw->desc->ops->fetch_response(cinfo, xfer);\n}\n\n \nvoid scmi_raw_error_report(void *r, struct scmi_chan_info *cinfo,\n\t\t\t   u32 msg_hdr, void *priv)\n{\n\tstruct scmi_xfer xfer;\n\tstruct scmi_raw_mode_info *raw = r;\n\n\tif (!raw)\n\t\treturn;\n\n\txfer.rx.len = raw->desc->max_msg_size;\n\txfer.rx.buf = kzalloc(xfer.rx.len, GFP_ATOMIC);\n\tif (!xfer.rx.buf) {\n\t\tdev_info(raw->handle->dev,\n\t\t\t \"Cannot report Raw error for HDR:0x%X - ENOMEM\\n\",\n\t\t\t msg_hdr);\n\t\treturn;\n\t}\n\n\t \n\tif (priv)\n\t\t \n\t\tsmp_store_mb(xfer.priv, priv);\n\n\tscmi_xfer_raw_fill(raw, cinfo, &xfer, msg_hdr);\n\tscmi_raw_message_report(raw, &xfer, SCMI_RAW_ERRS_QUEUE, 0);\n\n\tkfree(xfer.rx.buf);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}