{
  "module_name": "driver.c",
  "hash_id": "0e91fd788fa8b37836843f066cb13fd4ccb08c675c33945a05573d46dbaa47f8",
  "original_prompt": "Ingested from linux-6.6.14/drivers/firmware/arm_scmi/driver.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/bitmap.h>\n#include <linux/debugfs.h>\n#include <linux/device.h>\n#include <linux/export.h>\n#include <linux/idr.h>\n#include <linux/io.h>\n#include <linux/io-64-nonatomic-hi-lo.h>\n#include <linux/kernel.h>\n#include <linux/ktime.h>\n#include <linux/hashtable.h>\n#include <linux/list.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/platform_device.h>\n#include <linux/processor.h>\n#include <linux/refcount.h>\n#include <linux/slab.h>\n\n#include \"common.h\"\n#include \"notify.h\"\n\n#include \"raw_mode.h\"\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/scmi.h>\n\nstatic DEFINE_IDA(scmi_id);\n\nstatic DEFINE_IDR(scmi_protocols);\nstatic DEFINE_SPINLOCK(protocol_lock);\n\n \nstatic LIST_HEAD(scmi_list);\n \nstatic DEFINE_MUTEX(scmi_list_mutex);\n \nstatic atomic_t transfer_last_id;\n\nstatic struct dentry *scmi_top_dentry;\n\n \nstruct scmi_xfers_info {\n\tunsigned long *xfer_alloc_table;\n\tspinlock_t xfer_lock;\n\tint max_msg;\n\tstruct hlist_head free_xfers;\n\tDECLARE_HASHTABLE(pending_xfers, SCMI_PENDING_XFERS_HT_ORDER_SZ);\n};\n\n \nstruct scmi_protocol_instance {\n\tconst struct scmi_handle\t*handle;\n\tconst struct scmi_protocol\t*proto;\n\tvoid\t\t\t\t*gid;\n\trefcount_t\t\t\tusers;\n\tvoid\t\t\t\t*priv;\n\tstruct scmi_protocol_handle\tph;\n};\n\n#define ph_to_pi(h)\tcontainer_of(h, struct scmi_protocol_instance, ph)\n\n \nstruct scmi_debug_info {\n\tstruct dentry *top_dentry;\n\tconst char *name;\n\tconst char *type;\n\tbool is_atomic;\n};\n\n \nstruct scmi_info {\n\tint id;\n\tstruct device *dev;\n\tconst struct scmi_desc *desc;\n\tstruct scmi_revision_info version;\n\tstruct scmi_handle handle;\n\tstruct scmi_xfers_info tx_minfo;\n\tstruct scmi_xfers_info rx_minfo;\n\tstruct idr tx_idr;\n\tstruct idr rx_idr;\n\tstruct idr protocols;\n\t \n\tstruct mutex protocols_mtx;\n\tu8 *protocols_imp;\n\tstruct idr active_protocols;\n\tunsigned int atomic_threshold;\n\tvoid *notify_priv;\n\tstruct list_head node;\n\tint users;\n\tstruct notifier_block bus_nb;\n\tstruct notifier_block dev_req_nb;\n\t \n\tstruct mutex devreq_mtx;\n\tstruct scmi_debug_info *dbg;\n\tvoid *raw;\n};\n\n#define handle_to_scmi_info(h)\tcontainer_of(h, struct scmi_info, handle)\n#define bus_nb_to_scmi_info(nb)\tcontainer_of(nb, struct scmi_info, bus_nb)\n#define req_nb_to_scmi_info(nb)\tcontainer_of(nb, struct scmi_info, dev_req_nb)\n\nstatic const struct scmi_protocol *scmi_protocol_get(int protocol_id)\n{\n\tconst struct scmi_protocol *proto;\n\n\tproto = idr_find(&scmi_protocols, protocol_id);\n\tif (!proto || !try_module_get(proto->owner)) {\n\t\tpr_warn(\"SCMI Protocol 0x%x not found!\\n\", protocol_id);\n\t\treturn NULL;\n\t}\n\n\tpr_debug(\"Found SCMI Protocol 0x%x\\n\", protocol_id);\n\n\treturn proto;\n}\n\nstatic void scmi_protocol_put(int protocol_id)\n{\n\tconst struct scmi_protocol *proto;\n\n\tproto = idr_find(&scmi_protocols, protocol_id);\n\tif (proto)\n\t\tmodule_put(proto->owner);\n}\n\nint scmi_protocol_register(const struct scmi_protocol *proto)\n{\n\tint ret;\n\n\tif (!proto) {\n\t\tpr_err(\"invalid protocol\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!proto->instance_init) {\n\t\tpr_err(\"missing init for protocol 0x%x\\n\", proto->id);\n\t\treturn -EINVAL;\n\t}\n\n\tspin_lock(&protocol_lock);\n\tret = idr_alloc(&scmi_protocols, (void *)proto,\n\t\t\tproto->id, proto->id + 1, GFP_ATOMIC);\n\tspin_unlock(&protocol_lock);\n\tif (ret != proto->id) {\n\t\tpr_err(\"unable to allocate SCMI idr slot for 0x%x - err %d\\n\",\n\t\t       proto->id, ret);\n\t\treturn ret;\n\t}\n\n\tpr_debug(\"Registered SCMI Protocol 0x%x\\n\", proto->id);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(scmi_protocol_register);\n\nvoid scmi_protocol_unregister(const struct scmi_protocol *proto)\n{\n\tspin_lock(&protocol_lock);\n\tidr_remove(&scmi_protocols, proto->id);\n\tspin_unlock(&protocol_lock);\n\n\tpr_debug(\"Unregistered SCMI Protocol 0x%x\\n\", proto->id);\n}\nEXPORT_SYMBOL_GPL(scmi_protocol_unregister);\n\n \nstatic void scmi_create_protocol_devices(struct device_node *np,\n\t\t\t\t\t struct scmi_info *info,\n\t\t\t\t\t int prot_id, const char *name)\n{\n\tstruct scmi_device *sdev;\n\n\tmutex_lock(&info->devreq_mtx);\n\tsdev = scmi_device_create(np, info->dev, prot_id, name);\n\tif (name && !sdev)\n\t\tdev_err(info->dev,\n\t\t\t\"failed to create device for protocol 0x%X (%s)\\n\",\n\t\t\tprot_id, name);\n\tmutex_unlock(&info->devreq_mtx);\n}\n\nstatic void scmi_destroy_protocol_devices(struct scmi_info *info,\n\t\t\t\t\t  int prot_id, const char *name)\n{\n\tmutex_lock(&info->devreq_mtx);\n\tscmi_device_destroy(info->dev, prot_id, name);\n\tmutex_unlock(&info->devreq_mtx);\n}\n\nvoid scmi_notification_instance_data_set(const struct scmi_handle *handle,\n\t\t\t\t\t void *priv)\n{\n\tstruct scmi_info *info = handle_to_scmi_info(handle);\n\n\tinfo->notify_priv = priv;\n\t \n\tsmp_wmb();\n}\n\nvoid *scmi_notification_instance_data_get(const struct scmi_handle *handle)\n{\n\tstruct scmi_info *info = handle_to_scmi_info(handle);\n\n\t \n\tsmp_rmb();\n\treturn info->notify_priv;\n}\n\n \nstatic int scmi_xfer_token_set(struct scmi_xfers_info *minfo,\n\t\t\t       struct scmi_xfer *xfer)\n{\n\tunsigned long xfer_id, next_token;\n\n\t \n\tnext_token = (xfer->transfer_id & (MSG_TOKEN_MAX - 1));\n\n\t \n\txfer_id = find_next_zero_bit(minfo->xfer_alloc_table,\n\t\t\t\t     MSG_TOKEN_MAX, next_token);\n\tif (xfer_id == MSG_TOKEN_MAX) {\n\t\t \n\t\txfer_id = find_next_zero_bit(minfo->xfer_alloc_table,\n\t\t\t\t\t     MSG_TOKEN_MAX, 0);\n\t\t \n\t\tif (WARN_ON_ONCE(xfer_id == MSG_TOKEN_MAX))\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\tif (xfer_id != next_token)\n\t\tatomic_add((int)(xfer_id - next_token), &transfer_last_id);\n\n\txfer->hdr.seq = (u16)xfer_id;\n\n\treturn 0;\n}\n\n \nstatic inline void scmi_xfer_token_clear(struct scmi_xfers_info *minfo,\n\t\t\t\t\t struct scmi_xfer *xfer)\n{\n\tclear_bit(xfer->hdr.seq, minfo->xfer_alloc_table);\n}\n\n \nstatic inline void\nscmi_xfer_inflight_register_unlocked(struct scmi_xfer *xfer,\n\t\t\t\t     struct scmi_xfers_info *minfo)\n{\n\t \n\tset_bit(xfer->hdr.seq, minfo->xfer_alloc_table);\n\thash_add(minfo->pending_xfers, &xfer->node, xfer->hdr.seq);\n\txfer->pending = true;\n}\n\n \nstatic int scmi_xfer_inflight_register(struct scmi_xfer *xfer,\n\t\t\t\t       struct scmi_xfers_info *minfo)\n{\n\tint ret = 0;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&minfo->xfer_lock, flags);\n\tif (!test_bit(xfer->hdr.seq, minfo->xfer_alloc_table))\n\t\tscmi_xfer_inflight_register_unlocked(xfer, minfo);\n\telse\n\t\tret = -EBUSY;\n\tspin_unlock_irqrestore(&minfo->xfer_lock, flags);\n\n\treturn ret;\n}\n\n \nint scmi_xfer_raw_inflight_register(const struct scmi_handle *handle,\n\t\t\t\t    struct scmi_xfer *xfer)\n{\n\tstruct scmi_info *info = handle_to_scmi_info(handle);\n\n\treturn scmi_xfer_inflight_register(xfer, &info->tx_minfo);\n}\n\n \nstatic inline int scmi_xfer_pending_set(struct scmi_xfer *xfer,\n\t\t\t\t\tstruct scmi_xfers_info *minfo)\n{\n\tint ret;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&minfo->xfer_lock, flags);\n\t \n\tret = scmi_xfer_token_set(minfo, xfer);\n\tif (!ret)\n\t\tscmi_xfer_inflight_register_unlocked(xfer, minfo);\n\tspin_unlock_irqrestore(&minfo->xfer_lock, flags);\n\n\treturn ret;\n}\n\n \nstatic struct scmi_xfer *scmi_xfer_get(const struct scmi_handle *handle,\n\t\t\t\t       struct scmi_xfers_info *minfo)\n{\n\tunsigned long flags;\n\tstruct scmi_xfer *xfer;\n\n\tspin_lock_irqsave(&minfo->xfer_lock, flags);\n\tif (hlist_empty(&minfo->free_xfers)) {\n\t\tspin_unlock_irqrestore(&minfo->xfer_lock, flags);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\t \n\txfer = hlist_entry(minfo->free_xfers.first, struct scmi_xfer, node);\n\thlist_del_init(&xfer->node);\n\n\t \n\txfer->transfer_id = atomic_inc_return(&transfer_last_id);\n\n\trefcount_set(&xfer->users, 1);\n\tatomic_set(&xfer->busy, SCMI_XFER_FREE);\n\tspin_unlock_irqrestore(&minfo->xfer_lock, flags);\n\n\treturn xfer;\n}\n\n \nstruct scmi_xfer *scmi_xfer_raw_get(const struct scmi_handle *handle)\n{\n\tstruct scmi_xfer *xfer;\n\tstruct scmi_info *info = handle_to_scmi_info(handle);\n\n\txfer = scmi_xfer_get(handle, &info->tx_minfo);\n\tif (!IS_ERR(xfer))\n\t\txfer->flags |= SCMI_XFER_FLAG_IS_RAW;\n\n\treturn xfer;\n}\n\n \nstruct scmi_chan_info *\nscmi_xfer_raw_channel_get(const struct scmi_handle *handle, u8 protocol_id)\n{\n\tstruct scmi_chan_info *cinfo;\n\tstruct scmi_info *info = handle_to_scmi_info(handle);\n\n\tcinfo = idr_find(&info->tx_idr, protocol_id);\n\tif (!cinfo) {\n\t\tif (protocol_id == SCMI_PROTOCOL_BASE)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t \n\t\tcinfo = idr_find(&info->tx_idr, SCMI_PROTOCOL_BASE);\n\t\tif (!cinfo)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\tdev_warn_once(handle->dev,\n\t\t\t      \"Using Base channel for protocol 0x%X\\n\",\n\t\t\t      protocol_id);\n\t}\n\n\treturn cinfo;\n}\n\n \nstatic void\n__scmi_xfer_put(struct scmi_xfers_info *minfo, struct scmi_xfer *xfer)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&minfo->xfer_lock, flags);\n\tif (refcount_dec_and_test(&xfer->users)) {\n\t\tif (xfer->pending) {\n\t\t\tscmi_xfer_token_clear(minfo, xfer);\n\t\t\thash_del(&xfer->node);\n\t\t\txfer->pending = false;\n\t\t}\n\t\thlist_add_head(&xfer->node, &minfo->free_xfers);\n\t}\n\tspin_unlock_irqrestore(&minfo->xfer_lock, flags);\n}\n\n \nvoid scmi_xfer_raw_put(const struct scmi_handle *handle, struct scmi_xfer *xfer)\n{\n\tstruct scmi_info *info = handle_to_scmi_info(handle);\n\n\txfer->flags &= ~SCMI_XFER_FLAG_IS_RAW;\n\txfer->flags &= ~SCMI_XFER_FLAG_CHAN_SET;\n\treturn __scmi_xfer_put(&info->tx_minfo, xfer);\n}\n\n \nstatic struct scmi_xfer *\nscmi_xfer_lookup_unlocked(struct scmi_xfers_info *minfo, u16 xfer_id)\n{\n\tstruct scmi_xfer *xfer = NULL;\n\n\tif (test_bit(xfer_id, minfo->xfer_alloc_table))\n\t\txfer = XFER_FIND(minfo->pending_xfers, xfer_id);\n\n\treturn xfer ?: ERR_PTR(-EINVAL);\n}\n\n \nstatic inline int scmi_msg_response_validate(struct scmi_chan_info *cinfo,\n\t\t\t\t\t     u8 msg_type,\n\t\t\t\t\t     struct scmi_xfer *xfer)\n{\n\t \n\tif (msg_type == MSG_TYPE_DELAYED_RESP && !xfer->async_done) {\n\t\tdev_err(cinfo->dev,\n\t\t\t\"Delayed Response for %d not expected! Buggy F/W ?\\n\",\n\t\t\txfer->hdr.seq);\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (xfer->state) {\n\tcase SCMI_XFER_SENT_OK:\n\t\tif (msg_type == MSG_TYPE_DELAYED_RESP) {\n\t\t\t \n\t\t\txfer->hdr.status = SCMI_SUCCESS;\n\t\t\txfer->state = SCMI_XFER_RESP_OK;\n\t\t\tcomplete(&xfer->done);\n\t\t\tdev_warn(cinfo->dev,\n\t\t\t\t \"Received valid OoO Delayed Response for %d\\n\",\n\t\t\t\t xfer->hdr.seq);\n\t\t}\n\t\tbreak;\n\tcase SCMI_XFER_RESP_OK:\n\t\tif (msg_type != MSG_TYPE_DELAYED_RESP)\n\t\t\treturn -EINVAL;\n\t\tbreak;\n\tcase SCMI_XFER_DRESP_OK:\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nstatic inline void scmi_xfer_state_update(struct scmi_xfer *xfer, u8 msg_type)\n{\n\txfer->hdr.type = msg_type;\n\n\t \n\tif (xfer->hdr.type == MSG_TYPE_COMMAND)\n\t\txfer->state = SCMI_XFER_RESP_OK;\n\telse\n\t\txfer->state = SCMI_XFER_DRESP_OK;\n}\n\nstatic bool scmi_xfer_acquired(struct scmi_xfer *xfer)\n{\n\tint ret;\n\n\tret = atomic_cmpxchg(&xfer->busy, SCMI_XFER_FREE, SCMI_XFER_BUSY);\n\n\treturn ret == SCMI_XFER_FREE;\n}\n\n \nstatic inline struct scmi_xfer *\nscmi_xfer_command_acquire(struct scmi_chan_info *cinfo, u32 msg_hdr)\n{\n\tint ret;\n\tunsigned long flags;\n\tstruct scmi_xfer *xfer;\n\tstruct scmi_info *info = handle_to_scmi_info(cinfo->handle);\n\tstruct scmi_xfers_info *minfo = &info->tx_minfo;\n\tu8 msg_type = MSG_XTRACT_TYPE(msg_hdr);\n\tu16 xfer_id = MSG_XTRACT_TOKEN(msg_hdr);\n\n\t \n\tspin_lock_irqsave(&minfo->xfer_lock, flags);\n\txfer = scmi_xfer_lookup_unlocked(minfo, xfer_id);\n\tif (IS_ERR(xfer)) {\n\t\tdev_err(cinfo->dev,\n\t\t\t\"Message for %d type %d is not expected!\\n\",\n\t\t\txfer_id, msg_type);\n\t\tspin_unlock_irqrestore(&minfo->xfer_lock, flags);\n\t\treturn xfer;\n\t}\n\trefcount_inc(&xfer->users);\n\tspin_unlock_irqrestore(&minfo->xfer_lock, flags);\n\n\tspin_lock_irqsave(&xfer->lock, flags);\n\tret = scmi_msg_response_validate(cinfo, msg_type, xfer);\n\t \n\tif (!ret) {\n\t\tspin_until_cond(scmi_xfer_acquired(xfer));\n\t\tscmi_xfer_state_update(xfer, msg_type);\n\t}\n\tspin_unlock_irqrestore(&xfer->lock, flags);\n\n\tif (ret) {\n\t\tdev_err(cinfo->dev,\n\t\t\t\"Invalid message type:%d for %d - HDR:0x%X  state:%d\\n\",\n\t\t\tmsg_type, xfer_id, msg_hdr, xfer->state);\n\t\t \n\t\t__scmi_xfer_put(minfo, xfer);\n\t\txfer = ERR_PTR(-EINVAL);\n\t}\n\n\treturn xfer;\n}\n\nstatic inline void scmi_xfer_command_release(struct scmi_info *info,\n\t\t\t\t\t     struct scmi_xfer *xfer)\n{\n\tatomic_set(&xfer->busy, SCMI_XFER_FREE);\n\t__scmi_xfer_put(&info->tx_minfo, xfer);\n}\n\nstatic inline void scmi_clear_channel(struct scmi_info *info,\n\t\t\t\t      struct scmi_chan_info *cinfo)\n{\n\tif (info->desc->ops->clear_channel)\n\t\tinfo->desc->ops->clear_channel(cinfo);\n}\n\nstatic void scmi_handle_notification(struct scmi_chan_info *cinfo,\n\t\t\t\t     u32 msg_hdr, void *priv)\n{\n\tstruct scmi_xfer *xfer;\n\tstruct device *dev = cinfo->dev;\n\tstruct scmi_info *info = handle_to_scmi_info(cinfo->handle);\n\tstruct scmi_xfers_info *minfo = &info->rx_minfo;\n\tktime_t ts;\n\n\tts = ktime_get_boottime();\n\txfer = scmi_xfer_get(cinfo->handle, minfo);\n\tif (IS_ERR(xfer)) {\n\t\tdev_err(dev, \"failed to get free message slot (%ld)\\n\",\n\t\t\tPTR_ERR(xfer));\n\t\tscmi_clear_channel(info, cinfo);\n\t\treturn;\n\t}\n\n\tunpack_scmi_header(msg_hdr, &xfer->hdr);\n\tif (priv)\n\t\t \n\t\tsmp_store_mb(xfer->priv, priv);\n\tinfo->desc->ops->fetch_notification(cinfo, info->desc->max_msg_size,\n\t\t\t\t\t    xfer);\n\n\ttrace_scmi_msg_dump(info->id, cinfo->id, xfer->hdr.protocol_id,\n\t\t\t    xfer->hdr.id, \"NOTI\", xfer->hdr.seq,\n\t\t\t    xfer->hdr.status, xfer->rx.buf, xfer->rx.len);\n\n\tscmi_notify(cinfo->handle, xfer->hdr.protocol_id,\n\t\t    xfer->hdr.id, xfer->rx.buf, xfer->rx.len, ts);\n\n\ttrace_scmi_rx_done(xfer->transfer_id, xfer->hdr.id,\n\t\t\t   xfer->hdr.protocol_id, xfer->hdr.seq,\n\t\t\t   MSG_TYPE_NOTIFICATION);\n\n\tif (IS_ENABLED(CONFIG_ARM_SCMI_RAW_MODE_SUPPORT)) {\n\t\txfer->hdr.seq = MSG_XTRACT_TOKEN(msg_hdr);\n\t\tscmi_raw_message_report(info->raw, xfer, SCMI_RAW_NOTIF_QUEUE,\n\t\t\t\t\tcinfo->id);\n\t}\n\n\t__scmi_xfer_put(minfo, xfer);\n\n\tscmi_clear_channel(info, cinfo);\n}\n\nstatic void scmi_handle_response(struct scmi_chan_info *cinfo,\n\t\t\t\t u32 msg_hdr, void *priv)\n{\n\tstruct scmi_xfer *xfer;\n\tstruct scmi_info *info = handle_to_scmi_info(cinfo->handle);\n\n\txfer = scmi_xfer_command_acquire(cinfo, msg_hdr);\n\tif (IS_ERR(xfer)) {\n\t\tif (IS_ENABLED(CONFIG_ARM_SCMI_RAW_MODE_SUPPORT))\n\t\t\tscmi_raw_error_report(info->raw, cinfo, msg_hdr, priv);\n\n\t\tif (MSG_XTRACT_TYPE(msg_hdr) == MSG_TYPE_DELAYED_RESP)\n\t\t\tscmi_clear_channel(info, cinfo);\n\t\treturn;\n\t}\n\n\t \n\tif (xfer->hdr.type == MSG_TYPE_DELAYED_RESP)\n\t\txfer->rx.len = info->desc->max_msg_size;\n\n\tif (priv)\n\t\t \n\t\tsmp_store_mb(xfer->priv, priv);\n\tinfo->desc->ops->fetch_response(cinfo, xfer);\n\n\ttrace_scmi_msg_dump(info->id, cinfo->id, xfer->hdr.protocol_id,\n\t\t\t    xfer->hdr.id,\n\t\t\t    xfer->hdr.type == MSG_TYPE_DELAYED_RESP ?\n\t\t\t    (!SCMI_XFER_IS_RAW(xfer) ? \"DLYD\" : \"dlyd\") :\n\t\t\t    (!SCMI_XFER_IS_RAW(xfer) ? \"RESP\" : \"resp\"),\n\t\t\t    xfer->hdr.seq, xfer->hdr.status,\n\t\t\t    xfer->rx.buf, xfer->rx.len);\n\n\ttrace_scmi_rx_done(xfer->transfer_id, xfer->hdr.id,\n\t\t\t   xfer->hdr.protocol_id, xfer->hdr.seq,\n\t\t\t   xfer->hdr.type);\n\n\tif (xfer->hdr.type == MSG_TYPE_DELAYED_RESP) {\n\t\tscmi_clear_channel(info, cinfo);\n\t\tcomplete(xfer->async_done);\n\t} else {\n\t\tcomplete(&xfer->done);\n\t}\n\n\tif (IS_ENABLED(CONFIG_ARM_SCMI_RAW_MODE_SUPPORT)) {\n\t\t \n\t\tif (!xfer->hdr.poll_completion)\n\t\t\tscmi_raw_message_report(info->raw, xfer,\n\t\t\t\t\t\tSCMI_RAW_REPLY_QUEUE,\n\t\t\t\t\t\tcinfo->id);\n\t}\n\n\tscmi_xfer_command_release(info, xfer);\n}\n\n \nvoid scmi_rx_callback(struct scmi_chan_info *cinfo, u32 msg_hdr, void *priv)\n{\n\tu8 msg_type = MSG_XTRACT_TYPE(msg_hdr);\n\n\tswitch (msg_type) {\n\tcase MSG_TYPE_NOTIFICATION:\n\t\tscmi_handle_notification(cinfo, msg_hdr, priv);\n\t\tbreak;\n\tcase MSG_TYPE_COMMAND:\n\tcase MSG_TYPE_DELAYED_RESP:\n\t\tscmi_handle_response(cinfo, msg_hdr, priv);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"received unknown msg_type:%d\\n\", msg_type);\n\t\tbreak;\n\t}\n}\n\n \nstatic void xfer_put(const struct scmi_protocol_handle *ph,\n\t\t     struct scmi_xfer *xfer)\n{\n\tconst struct scmi_protocol_instance *pi = ph_to_pi(ph);\n\tstruct scmi_info *info = handle_to_scmi_info(pi->handle);\n\n\t__scmi_xfer_put(&info->tx_minfo, xfer);\n}\n\nstatic bool scmi_xfer_done_no_timeout(struct scmi_chan_info *cinfo,\n\t\t\t\t      struct scmi_xfer *xfer, ktime_t stop)\n{\n\tstruct scmi_info *info = handle_to_scmi_info(cinfo->handle);\n\n\t \n\treturn info->desc->ops->poll_done(cinfo, xfer) ||\n\t       try_wait_for_completion(&xfer->done) ||\n\t       ktime_after(ktime_get(), stop);\n}\n\nstatic int scmi_wait_for_reply(struct device *dev, const struct scmi_desc *desc,\n\t\t\t       struct scmi_chan_info *cinfo,\n\t\t\t       struct scmi_xfer *xfer, unsigned int timeout_ms)\n{\n\tint ret = 0;\n\n\tif (xfer->hdr.poll_completion) {\n\t\t \n\t\tif (!desc->sync_cmds_completed_on_ret) {\n\t\t\t \n\t\t\tktime_t stop = ktime_add_ms(ktime_get(), timeout_ms);\n\n\t\t\tspin_until_cond(scmi_xfer_done_no_timeout(cinfo,\n\t\t\t\t\t\t\t\t  xfer, stop));\n\t\t\tif (ktime_after(ktime_get(), stop)) {\n\t\t\t\tdev_err(dev,\n\t\t\t\t\t\"timed out in resp(caller: %pS) - polling\\n\",\n\t\t\t\t\t(void *)_RET_IP_);\n\t\t\t\tret = -ETIMEDOUT;\n\t\t\t}\n\t\t}\n\n\t\tif (!ret) {\n\t\t\tunsigned long flags;\n\t\t\tstruct scmi_info *info =\n\t\t\t\thandle_to_scmi_info(cinfo->handle);\n\n\t\t\t \n\t\t\tspin_lock_irqsave(&xfer->lock, flags);\n\t\t\tif (xfer->state == SCMI_XFER_SENT_OK) {\n\t\t\t\tdesc->ops->fetch_response(cinfo, xfer);\n\t\t\t\txfer->state = SCMI_XFER_RESP_OK;\n\t\t\t}\n\t\t\tspin_unlock_irqrestore(&xfer->lock, flags);\n\n\t\t\t \n\t\t\ttrace_scmi_msg_dump(info->id, cinfo->id,\n\t\t\t\t\t    xfer->hdr.protocol_id, xfer->hdr.id,\n\t\t\t\t\t    !SCMI_XFER_IS_RAW(xfer) ?\n\t\t\t\t\t    \"RESP\" : \"resp\",\n\t\t\t\t\t    xfer->hdr.seq, xfer->hdr.status,\n\t\t\t\t\t    xfer->rx.buf, xfer->rx.len);\n\n\t\t\tif (IS_ENABLED(CONFIG_ARM_SCMI_RAW_MODE_SUPPORT)) {\n\t\t\t\tstruct scmi_info *info =\n\t\t\t\t\thandle_to_scmi_info(cinfo->handle);\n\n\t\t\t\tscmi_raw_message_report(info->raw, xfer,\n\t\t\t\t\t\t\tSCMI_RAW_REPLY_QUEUE,\n\t\t\t\t\t\t\tcinfo->id);\n\t\t\t}\n\t\t}\n\t} else {\n\t\t \n\t\tif (!wait_for_completion_timeout(&xfer->done,\n\t\t\t\t\t\t msecs_to_jiffies(timeout_ms))) {\n\t\t\tdev_err(dev, \"timed out in resp(caller: %pS)\\n\",\n\t\t\t\t(void *)_RET_IP_);\n\t\t\tret = -ETIMEDOUT;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n \nstatic int scmi_wait_for_message_response(struct scmi_chan_info *cinfo,\n\t\t\t\t\t  struct scmi_xfer *xfer)\n{\n\tstruct scmi_info *info = handle_to_scmi_info(cinfo->handle);\n\tstruct device *dev = info->dev;\n\n\ttrace_scmi_xfer_response_wait(xfer->transfer_id, xfer->hdr.id,\n\t\t\t\t      xfer->hdr.protocol_id, xfer->hdr.seq,\n\t\t\t\t      info->desc->max_rx_timeout_ms,\n\t\t\t\t      xfer->hdr.poll_completion);\n\n\treturn scmi_wait_for_reply(dev, info->desc, cinfo, xfer,\n\t\t\t\t   info->desc->max_rx_timeout_ms);\n}\n\n \nint scmi_xfer_raw_wait_for_message_response(struct scmi_chan_info *cinfo,\n\t\t\t\t\t    struct scmi_xfer *xfer,\n\t\t\t\t\t    unsigned int timeout_ms)\n{\n\tint ret;\n\tstruct scmi_info *info = handle_to_scmi_info(cinfo->handle);\n\tstruct device *dev = info->dev;\n\n\tret = scmi_wait_for_reply(dev, info->desc, cinfo, xfer, timeout_ms);\n\tif (ret)\n\t\tdev_dbg(dev, \"timed out in RAW response - HDR:%08X\\n\",\n\t\t\tpack_scmi_header(&xfer->hdr));\n\n\treturn ret;\n}\n\n \nstatic int do_xfer(const struct scmi_protocol_handle *ph,\n\t\t   struct scmi_xfer *xfer)\n{\n\tint ret;\n\tconst struct scmi_protocol_instance *pi = ph_to_pi(ph);\n\tstruct scmi_info *info = handle_to_scmi_info(pi->handle);\n\tstruct device *dev = info->dev;\n\tstruct scmi_chan_info *cinfo;\n\n\t \n\tif (xfer->hdr.poll_completion &&\n\t    !is_transport_polling_capable(info->desc)) {\n\t\tdev_warn_once(dev,\n\t\t\t      \"Polling mode is not supported by transport.\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tcinfo = idr_find(&info->tx_idr, pi->proto->id);\n\tif (unlikely(!cinfo))\n\t\treturn -EINVAL;\n\n\t \n\tif (is_polling_enabled(cinfo, info->desc))\n\t\txfer->hdr.poll_completion = true;\n\n\t \n\txfer->hdr.protocol_id = pi->proto->id;\n\treinit_completion(&xfer->done);\n\n\ttrace_scmi_xfer_begin(xfer->transfer_id, xfer->hdr.id,\n\t\t\t      xfer->hdr.protocol_id, xfer->hdr.seq,\n\t\t\t      xfer->hdr.poll_completion);\n\n\t \n\txfer->hdr.status = SCMI_SUCCESS;\n\txfer->state = SCMI_XFER_SENT_OK;\n\t \n\tsmp_mb();\n\n\tret = info->desc->ops->send_message(cinfo, xfer);\n\tif (ret < 0) {\n\t\tdev_dbg(dev, \"Failed to send message %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\ttrace_scmi_msg_dump(info->id, cinfo->id, xfer->hdr.protocol_id,\n\t\t\t    xfer->hdr.id, \"CMND\", xfer->hdr.seq,\n\t\t\t    xfer->hdr.status, xfer->tx.buf, xfer->tx.len);\n\n\tret = scmi_wait_for_message_response(cinfo, xfer);\n\tif (!ret && xfer->hdr.status)\n\t\tret = scmi_to_linux_errno(xfer->hdr.status);\n\n\tif (info->desc->ops->mark_txdone)\n\t\tinfo->desc->ops->mark_txdone(cinfo, ret, xfer);\n\n\ttrace_scmi_xfer_end(xfer->transfer_id, xfer->hdr.id,\n\t\t\t    xfer->hdr.protocol_id, xfer->hdr.seq, ret);\n\n\treturn ret;\n}\n\nstatic void reset_rx_to_maxsz(const struct scmi_protocol_handle *ph,\n\t\t\t      struct scmi_xfer *xfer)\n{\n\tconst struct scmi_protocol_instance *pi = ph_to_pi(ph);\n\tstruct scmi_info *info = handle_to_scmi_info(pi->handle);\n\n\txfer->rx.len = info->desc->max_msg_size;\n}\n\n \nstatic int do_xfer_with_response(const struct scmi_protocol_handle *ph,\n\t\t\t\t struct scmi_xfer *xfer)\n{\n\tint ret, timeout = msecs_to_jiffies(SCMI_MAX_RESPONSE_TIMEOUT);\n\tDECLARE_COMPLETION_ONSTACK(async_response);\n\n\txfer->async_done = &async_response;\n\n\t \n\tWARN_ON_ONCE(xfer->hdr.poll_completion);\n\n\tret = do_xfer(ph, xfer);\n\tif (!ret) {\n\t\tif (!wait_for_completion_timeout(xfer->async_done, timeout)) {\n\t\t\tdev_err(ph->dev,\n\t\t\t\t\"timed out in delayed resp(caller: %pS)\\n\",\n\t\t\t\t(void *)_RET_IP_);\n\t\t\tret = -ETIMEDOUT;\n\t\t} else if (xfer->hdr.status) {\n\t\t\tret = scmi_to_linux_errno(xfer->hdr.status);\n\t\t}\n\t}\n\n\txfer->async_done = NULL;\n\treturn ret;\n}\n\n \nstatic int xfer_get_init(const struct scmi_protocol_handle *ph,\n\t\t\t u8 msg_id, size_t tx_size, size_t rx_size,\n\t\t\t struct scmi_xfer **p)\n{\n\tint ret;\n\tstruct scmi_xfer *xfer;\n\tconst struct scmi_protocol_instance *pi = ph_to_pi(ph);\n\tstruct scmi_info *info = handle_to_scmi_info(pi->handle);\n\tstruct scmi_xfers_info *minfo = &info->tx_minfo;\n\tstruct device *dev = info->dev;\n\n\t \n\tif (rx_size > info->desc->max_msg_size ||\n\t    tx_size > info->desc->max_msg_size)\n\t\treturn -ERANGE;\n\n\txfer = scmi_xfer_get(pi->handle, minfo);\n\tif (IS_ERR(xfer)) {\n\t\tret = PTR_ERR(xfer);\n\t\tdev_err(dev, \"failed to get free message slot(%d)\\n\", ret);\n\t\treturn ret;\n\t}\n\n\t \n\tret = scmi_xfer_pending_set(xfer, minfo);\n\tif (ret) {\n\t\tdev_err(pi->handle->dev,\n\t\t\t\"Failed to get monotonic token %d\\n\", ret);\n\t\t__scmi_xfer_put(minfo, xfer);\n\t\treturn ret;\n\t}\n\n\txfer->tx.len = tx_size;\n\txfer->rx.len = rx_size ? : info->desc->max_msg_size;\n\txfer->hdr.type = MSG_TYPE_COMMAND;\n\txfer->hdr.id = msg_id;\n\txfer->hdr.poll_completion = false;\n\n\t*p = xfer;\n\n\treturn 0;\n}\n\n \nstatic int version_get(const struct scmi_protocol_handle *ph, u32 *version)\n{\n\tint ret;\n\t__le32 *rev_info;\n\tstruct scmi_xfer *t;\n\n\tret = xfer_get_init(ph, PROTOCOL_VERSION, 0, sizeof(*version), &t);\n\tif (ret)\n\t\treturn ret;\n\n\tret = do_xfer(ph, t);\n\tif (!ret) {\n\t\trev_info = t->rx.buf;\n\t\t*version = le32_to_cpu(*rev_info);\n\t}\n\n\txfer_put(ph, t);\n\treturn ret;\n}\n\n \nstatic int scmi_set_protocol_priv(const struct scmi_protocol_handle *ph,\n\t\t\t\t  void *priv)\n{\n\tstruct scmi_protocol_instance *pi = ph_to_pi(ph);\n\n\tpi->priv = priv;\n\n\treturn 0;\n}\n\n \nstatic void *scmi_get_protocol_priv(const struct scmi_protocol_handle *ph)\n{\n\tconst struct scmi_protocol_instance *pi = ph_to_pi(ph);\n\n\treturn pi->priv;\n}\n\nstatic const struct scmi_xfer_ops xfer_ops = {\n\t.version_get = version_get,\n\t.xfer_get_init = xfer_get_init,\n\t.reset_rx_to_maxsz = reset_rx_to_maxsz,\n\t.do_xfer = do_xfer,\n\t.do_xfer_with_response = do_xfer_with_response,\n\t.xfer_put = xfer_put,\n};\n\nstruct scmi_msg_resp_domain_name_get {\n\t__le32 flags;\n\tu8 name[SCMI_MAX_STR_SIZE];\n};\n\n \nstatic int scmi_common_extended_name_get(const struct scmi_protocol_handle *ph,\n\t\t\t\t\t u8 cmd_id, u32 res_id, char *name,\n\t\t\t\t\t size_t len)\n{\n\tint ret;\n\tstruct scmi_xfer *t;\n\tstruct scmi_msg_resp_domain_name_get *resp;\n\n\tret = ph->xops->xfer_get_init(ph, cmd_id, sizeof(res_id),\n\t\t\t\t      sizeof(*resp), &t);\n\tif (ret)\n\t\tgoto out;\n\n\tput_unaligned_le32(res_id, t->tx.buf);\n\tresp = t->rx.buf;\n\n\tret = ph->xops->do_xfer(ph, t);\n\tif (!ret)\n\t\tstrscpy(name, resp->name, len);\n\n\tph->xops->xfer_put(ph, t);\nout:\n\tif (ret)\n\t\tdev_warn(ph->dev,\n\t\t\t \"Failed to get extended name - id:%u (ret:%d). Using %s\\n\",\n\t\t\t res_id, ret, name);\n\treturn ret;\n}\n\n \nstruct scmi_iterator {\n\tvoid *msg;\n\tvoid *resp;\n\tstruct scmi_xfer *t;\n\tconst struct scmi_protocol_handle *ph;\n\tstruct scmi_iterator_ops *ops;\n\tstruct scmi_iterator_state state;\n\tvoid *priv;\n};\n\nstatic void *scmi_iterator_init(const struct scmi_protocol_handle *ph,\n\t\t\t\tstruct scmi_iterator_ops *ops,\n\t\t\t\tunsigned int max_resources, u8 msg_id,\n\t\t\t\tsize_t tx_size, void *priv)\n{\n\tint ret;\n\tstruct scmi_iterator *i;\n\n\ti = devm_kzalloc(ph->dev, sizeof(*i), GFP_KERNEL);\n\tif (!i)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\ti->ph = ph;\n\ti->ops = ops;\n\ti->priv = priv;\n\n\tret = ph->xops->xfer_get_init(ph, msg_id, tx_size, 0, &i->t);\n\tif (ret) {\n\t\tdevm_kfree(ph->dev, i);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\ti->state.max_resources = max_resources;\n\ti->msg = i->t->tx.buf;\n\ti->resp = i->t->rx.buf;\n\n\treturn i;\n}\n\nstatic int scmi_iterator_run(void *iter)\n{\n\tint ret = -EINVAL;\n\tstruct scmi_iterator_ops *iops;\n\tconst struct scmi_protocol_handle *ph;\n\tstruct scmi_iterator_state *st;\n\tstruct scmi_iterator *i = iter;\n\n\tif (!i || !i->ops || !i->ph)\n\t\treturn ret;\n\n\tiops = i->ops;\n\tph = i->ph;\n\tst = &i->state;\n\n\tdo {\n\t\tiops->prepare_message(i->msg, st->desc_index, i->priv);\n\t\tret = ph->xops->do_xfer(ph, i->t);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tst->rx_len = i->t->rx.len;\n\t\tret = iops->update_state(st, i->resp, i->priv);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tif (st->num_returned > st->max_resources - st->desc_index) {\n\t\t\tdev_err(ph->dev,\n\t\t\t\t\"No. of resources can't exceed %d\\n\",\n\t\t\t\tst->max_resources);\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor (st->loop_idx = 0; st->loop_idx < st->num_returned;\n\t\t     st->loop_idx++) {\n\t\t\tret = iops->process_response(ph, i->resp, st, i->priv);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tst->desc_index += st->num_returned;\n\t\tph->xops->reset_rx_to_maxsz(ph, i->t);\n\t\t \n\t} while (st->num_returned && st->num_remaining);\n\nout:\n\t \n\tph->xops->xfer_put(ph, i->t);\n\tdevm_kfree(ph->dev, i);\n\n\treturn ret;\n}\n\nstruct scmi_msg_get_fc_info {\n\t__le32 domain;\n\t__le32 message_id;\n};\n\nstruct scmi_msg_resp_desc_fc {\n\t__le32 attr;\n#define SUPPORTS_DOORBELL(x)\t\t((x) & BIT(0))\n#define DOORBELL_REG_WIDTH(x)\t\tFIELD_GET(GENMASK(2, 1), (x))\n\t__le32 rate_limit;\n\t__le32 chan_addr_low;\n\t__le32 chan_addr_high;\n\t__le32 chan_size;\n\t__le32 db_addr_low;\n\t__le32 db_addr_high;\n\t__le32 db_set_lmask;\n\t__le32 db_set_hmask;\n\t__le32 db_preserve_lmask;\n\t__le32 db_preserve_hmask;\n};\n\nstatic void\nscmi_common_fastchannel_init(const struct scmi_protocol_handle *ph,\n\t\t\t     u8 describe_id, u32 message_id, u32 valid_size,\n\t\t\t     u32 domain, void __iomem **p_addr,\n\t\t\t     struct scmi_fc_db_info **p_db)\n{\n\tint ret;\n\tu32 flags;\n\tu64 phys_addr;\n\tu8 size;\n\tvoid __iomem *addr;\n\tstruct scmi_xfer *t;\n\tstruct scmi_fc_db_info *db = NULL;\n\tstruct scmi_msg_get_fc_info *info;\n\tstruct scmi_msg_resp_desc_fc *resp;\n\tconst struct scmi_protocol_instance *pi = ph_to_pi(ph);\n\n\tif (!p_addr) {\n\t\tret = -EINVAL;\n\t\tgoto err_out;\n\t}\n\n\tret = ph->xops->xfer_get_init(ph, describe_id,\n\t\t\t\t      sizeof(*info), sizeof(*resp), &t);\n\tif (ret)\n\t\tgoto err_out;\n\n\tinfo = t->tx.buf;\n\tinfo->domain = cpu_to_le32(domain);\n\tinfo->message_id = cpu_to_le32(message_id);\n\n\t \n\tret = ph->xops->do_xfer(ph, t);\n\tif (ret)\n\t\tgoto err_xfer;\n\n\tresp = t->rx.buf;\n\tflags = le32_to_cpu(resp->attr);\n\tsize = le32_to_cpu(resp->chan_size);\n\tif (size != valid_size) {\n\t\tret = -EINVAL;\n\t\tgoto err_xfer;\n\t}\n\n\tphys_addr = le32_to_cpu(resp->chan_addr_low);\n\tphys_addr |= (u64)le32_to_cpu(resp->chan_addr_high) << 32;\n\taddr = devm_ioremap(ph->dev, phys_addr, size);\n\tif (!addr) {\n\t\tret = -EADDRNOTAVAIL;\n\t\tgoto err_xfer;\n\t}\n\n\t*p_addr = addr;\n\n\tif (p_db && SUPPORTS_DOORBELL(flags)) {\n\t\tdb = devm_kzalloc(ph->dev, sizeof(*db), GFP_KERNEL);\n\t\tif (!db) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_db;\n\t\t}\n\n\t\tsize = 1 << DOORBELL_REG_WIDTH(flags);\n\t\tphys_addr = le32_to_cpu(resp->db_addr_low);\n\t\tphys_addr |= (u64)le32_to_cpu(resp->db_addr_high) << 32;\n\t\taddr = devm_ioremap(ph->dev, phys_addr, size);\n\t\tif (!addr) {\n\t\t\tret = -EADDRNOTAVAIL;\n\t\t\tgoto err_db_mem;\n\t\t}\n\n\t\tdb->addr = addr;\n\t\tdb->width = size;\n\t\tdb->set = le32_to_cpu(resp->db_set_lmask);\n\t\tdb->set |= (u64)le32_to_cpu(resp->db_set_hmask) << 32;\n\t\tdb->mask = le32_to_cpu(resp->db_preserve_lmask);\n\t\tdb->mask |= (u64)le32_to_cpu(resp->db_preserve_hmask) << 32;\n\n\t\t*p_db = db;\n\t}\n\n\tph->xops->xfer_put(ph, t);\n\n\tdev_dbg(ph->dev,\n\t\t\"Using valid FC for protocol %X [MSG_ID:%u / RES_ID:%u]\\n\",\n\t\tpi->proto->id, message_id, domain);\n\n\treturn;\n\nerr_db_mem:\n\tdevm_kfree(ph->dev, db);\n\nerr_db:\n\t*p_addr = NULL;\n\nerr_xfer:\n\tph->xops->xfer_put(ph, t);\n\nerr_out:\n\tdev_warn(ph->dev,\n\t\t \"Failed to get FC for protocol %X [MSG_ID:%u / RES_ID:%u] - ret:%d. Using regular messaging.\\n\",\n\t\t pi->proto->id, message_id, domain, ret);\n}\n\n#define SCMI_PROTO_FC_RING_DB(w)\t\t\t\\\ndo {\t\t\t\t\t\t\t\\\n\tu##w val = 0;\t\t\t\t\t\\\n\t\t\t\t\t\t\t\\\n\tif (db->mask)\t\t\t\t\t\\\n\t\tval = ioread##w(db->addr) & db->mask;\t\\\n\tiowrite##w((u##w)db->set | val, db->addr);\t\\\n} while (0)\n\nstatic void scmi_common_fastchannel_db_ring(struct scmi_fc_db_info *db)\n{\n\tif (!db || !db->addr)\n\t\treturn;\n\n\tif (db->width == 1)\n\t\tSCMI_PROTO_FC_RING_DB(8);\n\telse if (db->width == 2)\n\t\tSCMI_PROTO_FC_RING_DB(16);\n\telse if (db->width == 4)\n\t\tSCMI_PROTO_FC_RING_DB(32);\n\telse  \n#ifdef CONFIG_64BIT\n\t\tSCMI_PROTO_FC_RING_DB(64);\n#else\n\t{\n\t\tu64 val = 0;\n\n\t\tif (db->mask)\n\t\t\tval = ioread64_hi_lo(db->addr) & db->mask;\n\t\tiowrite64_hi_lo(db->set | val, db->addr);\n\t}\n#endif\n}\n\nstatic const struct scmi_proto_helpers_ops helpers_ops = {\n\t.extended_name_get = scmi_common_extended_name_get,\n\t.iter_response_init = scmi_iterator_init,\n\t.iter_response_run = scmi_iterator_run,\n\t.fastchannel_init = scmi_common_fastchannel_init,\n\t.fastchannel_db_ring = scmi_common_fastchannel_db_ring,\n};\n\n \nstruct scmi_revision_info *\nscmi_revision_area_get(const struct scmi_protocol_handle *ph)\n{\n\tconst struct scmi_protocol_instance *pi = ph_to_pi(ph);\n\n\treturn pi->handle->version;\n}\n\n \nstatic struct scmi_protocol_instance *\nscmi_alloc_init_protocol_instance(struct scmi_info *info,\n\t\t\t\t  const struct scmi_protocol *proto)\n{\n\tint ret = -ENOMEM;\n\tvoid *gid;\n\tstruct scmi_protocol_instance *pi;\n\tconst struct scmi_handle *handle = &info->handle;\n\n\t \n\tgid = devres_open_group(handle->dev, NULL, GFP_KERNEL);\n\tif (!gid) {\n\t\tscmi_protocol_put(proto->id);\n\t\tgoto out;\n\t}\n\n\tpi = devm_kzalloc(handle->dev, sizeof(*pi), GFP_KERNEL);\n\tif (!pi)\n\t\tgoto clean;\n\n\tpi->gid = gid;\n\tpi->proto = proto;\n\tpi->handle = handle;\n\tpi->ph.dev = handle->dev;\n\tpi->ph.xops = &xfer_ops;\n\tpi->ph.hops = &helpers_ops;\n\tpi->ph.set_priv = scmi_set_protocol_priv;\n\tpi->ph.get_priv = scmi_get_protocol_priv;\n\trefcount_set(&pi->users, 1);\n\t \n\tret = pi->proto->instance_init(&pi->ph);\n\tif (ret)\n\t\tgoto clean;\n\n\tret = idr_alloc(&info->protocols, pi, proto->id, proto->id + 1,\n\t\t\tGFP_KERNEL);\n\tif (ret != proto->id)\n\t\tgoto clean;\n\n\t \n\tif (pi->proto->events) {\n\t\tret = scmi_register_protocol_events(handle, pi->proto->id,\n\t\t\t\t\t\t    &pi->ph,\n\t\t\t\t\t\t    pi->proto->events);\n\t\tif (ret)\n\t\t\tdev_warn(handle->dev,\n\t\t\t\t \"Protocol:%X - Events Registration Failed - err:%d\\n\",\n\t\t\t\t pi->proto->id, ret);\n\t}\n\n\tdevres_close_group(handle->dev, pi->gid);\n\tdev_dbg(handle->dev, \"Initialized protocol: 0x%X\\n\", pi->proto->id);\n\n\treturn pi;\n\nclean:\n\t \n\tscmi_protocol_put(proto->id);\n\tdevres_release_group(handle->dev, gid);\nout:\n\treturn ERR_PTR(ret);\n}\n\n \nstatic struct scmi_protocol_instance * __must_check\nscmi_get_protocol_instance(const struct scmi_handle *handle, u8 protocol_id)\n{\n\tstruct scmi_protocol_instance *pi;\n\tstruct scmi_info *info = handle_to_scmi_info(handle);\n\n\tmutex_lock(&info->protocols_mtx);\n\tpi = idr_find(&info->protocols, protocol_id);\n\n\tif (pi) {\n\t\trefcount_inc(&pi->users);\n\t} else {\n\t\tconst struct scmi_protocol *proto;\n\n\t\t \n\t\tproto = scmi_protocol_get(protocol_id);\n\t\tif (proto)\n\t\t\tpi = scmi_alloc_init_protocol_instance(info, proto);\n\t\telse\n\t\t\tpi = ERR_PTR(-EPROBE_DEFER);\n\t}\n\tmutex_unlock(&info->protocols_mtx);\n\n\treturn pi;\n}\n\n \nint scmi_protocol_acquire(const struct scmi_handle *handle, u8 protocol_id)\n{\n\treturn PTR_ERR_OR_ZERO(scmi_get_protocol_instance(handle, protocol_id));\n}\n\n \nvoid scmi_protocol_release(const struct scmi_handle *handle, u8 protocol_id)\n{\n\tstruct scmi_info *info = handle_to_scmi_info(handle);\n\tstruct scmi_protocol_instance *pi;\n\n\tmutex_lock(&info->protocols_mtx);\n\tpi = idr_find(&info->protocols, protocol_id);\n\tif (WARN_ON(!pi))\n\t\tgoto out;\n\n\tif (refcount_dec_and_test(&pi->users)) {\n\t\tvoid *gid = pi->gid;\n\n\t\tif (pi->proto->events)\n\t\t\tscmi_deregister_protocol_events(handle, protocol_id);\n\n\t\tif (pi->proto->instance_deinit)\n\t\t\tpi->proto->instance_deinit(&pi->ph);\n\n\t\tidr_remove(&info->protocols, protocol_id);\n\n\t\tscmi_protocol_put(protocol_id);\n\n\t\tdevres_release_group(handle->dev, gid);\n\t\tdev_dbg(handle->dev, \"De-Initialized protocol: 0x%X\\n\",\n\t\t\tprotocol_id);\n\t}\n\nout:\n\tmutex_unlock(&info->protocols_mtx);\n}\n\nvoid scmi_setup_protocol_implemented(const struct scmi_protocol_handle *ph,\n\t\t\t\t     u8 *prot_imp)\n{\n\tconst struct scmi_protocol_instance *pi = ph_to_pi(ph);\n\tstruct scmi_info *info = handle_to_scmi_info(pi->handle);\n\n\tinfo->protocols_imp = prot_imp;\n}\n\nstatic bool\nscmi_is_protocol_implemented(const struct scmi_handle *handle, u8 prot_id)\n{\n\tint i;\n\tstruct scmi_info *info = handle_to_scmi_info(handle);\n\tstruct scmi_revision_info *rev = handle->version;\n\n\tif (!info->protocols_imp)\n\t\treturn false;\n\n\tfor (i = 0; i < rev->num_protocols; i++)\n\t\tif (info->protocols_imp[i] == prot_id)\n\t\t\treturn true;\n\treturn false;\n}\n\nstruct scmi_protocol_devres {\n\tconst struct scmi_handle *handle;\n\tu8 protocol_id;\n};\n\nstatic void scmi_devm_release_protocol(struct device *dev, void *res)\n{\n\tstruct scmi_protocol_devres *dres = res;\n\n\tscmi_protocol_release(dres->handle, dres->protocol_id);\n}\n\nstatic struct scmi_protocol_instance __must_check *\nscmi_devres_protocol_instance_get(struct scmi_device *sdev, u8 protocol_id)\n{\n\tstruct scmi_protocol_instance *pi;\n\tstruct scmi_protocol_devres *dres;\n\n\tdres = devres_alloc(scmi_devm_release_protocol,\n\t\t\t    sizeof(*dres), GFP_KERNEL);\n\tif (!dres)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tpi = scmi_get_protocol_instance(sdev->handle, protocol_id);\n\tif (IS_ERR(pi)) {\n\t\tdevres_free(dres);\n\t\treturn pi;\n\t}\n\n\tdres->handle = sdev->handle;\n\tdres->protocol_id = protocol_id;\n\tdevres_add(&sdev->dev, dres);\n\n\treturn pi;\n}\n\n \nstatic const void __must_check *\nscmi_devm_protocol_get(struct scmi_device *sdev, u8 protocol_id,\n\t\t       struct scmi_protocol_handle **ph)\n{\n\tstruct scmi_protocol_instance *pi;\n\n\tif (!ph)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tpi = scmi_devres_protocol_instance_get(sdev, protocol_id);\n\tif (IS_ERR(pi))\n\t\treturn pi;\n\n\t*ph = &pi->ph;\n\n\treturn pi->proto->ops;\n}\n\n \nstatic int __must_check scmi_devm_protocol_acquire(struct scmi_device *sdev,\n\t\t\t\t\t\t   u8 protocol_id)\n{\n\tstruct scmi_protocol_instance *pi;\n\n\tpi = scmi_devres_protocol_instance_get(sdev, protocol_id);\n\tif (IS_ERR(pi))\n\t\treturn PTR_ERR(pi);\n\n\treturn 0;\n}\n\nstatic int scmi_devm_protocol_match(struct device *dev, void *res, void *data)\n{\n\tstruct scmi_protocol_devres *dres = res;\n\n\tif (WARN_ON(!dres || !data))\n\t\treturn 0;\n\n\treturn dres->protocol_id == *((u8 *)data);\n}\n\n \nstatic void scmi_devm_protocol_put(struct scmi_device *sdev, u8 protocol_id)\n{\n\tint ret;\n\n\tret = devres_release(&sdev->dev, scmi_devm_release_protocol,\n\t\t\t     scmi_devm_protocol_match, &protocol_id);\n\tWARN_ON(ret);\n}\n\n \nstatic bool scmi_is_transport_atomic(const struct scmi_handle *handle,\n\t\t\t\t     unsigned int *atomic_threshold)\n{\n\tbool ret;\n\tstruct scmi_info *info = handle_to_scmi_info(handle);\n\n\tret = info->desc->atomic_enabled &&\n\t\tis_transport_polling_capable(info->desc);\n\tif (ret && atomic_threshold)\n\t\t*atomic_threshold = info->atomic_threshold;\n\n\treturn ret;\n}\n\n \nstatic struct scmi_handle *scmi_handle_get(struct device *dev)\n{\n\tstruct list_head *p;\n\tstruct scmi_info *info;\n\tstruct scmi_handle *handle = NULL;\n\n\tmutex_lock(&scmi_list_mutex);\n\tlist_for_each(p, &scmi_list) {\n\t\tinfo = list_entry(p, struct scmi_info, node);\n\t\tif (dev->parent == info->dev) {\n\t\t\tinfo->users++;\n\t\t\thandle = &info->handle;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&scmi_list_mutex);\n\n\treturn handle;\n}\n\n \nstatic int scmi_handle_put(const struct scmi_handle *handle)\n{\n\tstruct scmi_info *info;\n\n\tif (!handle)\n\t\treturn -EINVAL;\n\n\tinfo = handle_to_scmi_info(handle);\n\tmutex_lock(&scmi_list_mutex);\n\tif (!WARN_ON(!info->users))\n\t\tinfo->users--;\n\tmutex_unlock(&scmi_list_mutex);\n\n\treturn 0;\n}\n\nstatic void scmi_device_link_add(struct device *consumer,\n\t\t\t\t struct device *supplier)\n{\n\tstruct device_link *link;\n\n\tlink = device_link_add(consumer, supplier, DL_FLAG_AUTOREMOVE_CONSUMER);\n\n\tWARN_ON(!link);\n}\n\nstatic void scmi_set_handle(struct scmi_device *scmi_dev)\n{\n\tscmi_dev->handle = scmi_handle_get(&scmi_dev->dev);\n\tif (scmi_dev->handle)\n\t\tscmi_device_link_add(&scmi_dev->dev, scmi_dev->handle->dev);\n}\n\nstatic int __scmi_xfer_info_init(struct scmi_info *sinfo,\n\t\t\t\t struct scmi_xfers_info *info)\n{\n\tint i;\n\tstruct scmi_xfer *xfer;\n\tstruct device *dev = sinfo->dev;\n\tconst struct scmi_desc *desc = sinfo->desc;\n\n\t \n\tif (WARN_ON(!info->max_msg || info->max_msg > MSG_TOKEN_MAX)) {\n\t\tdev_err(dev,\n\t\t\t\"Invalid maximum messages %d, not in range [1 - %lu]\\n\",\n\t\t\tinfo->max_msg, MSG_TOKEN_MAX);\n\t\treturn -EINVAL;\n\t}\n\n\thash_init(info->pending_xfers);\n\n\t \n\tinfo->xfer_alloc_table = devm_bitmap_zalloc(dev, MSG_TOKEN_MAX,\n\t\t\t\t\t\t    GFP_KERNEL);\n\tif (!info->xfer_alloc_table)\n\t\treturn -ENOMEM;\n\n\t \n\tINIT_HLIST_HEAD(&info->free_xfers);\n\tfor (i = 0; i < info->max_msg; i++) {\n\t\txfer = devm_kzalloc(dev, sizeof(*xfer), GFP_KERNEL);\n\t\tif (!xfer)\n\t\t\treturn -ENOMEM;\n\n\t\txfer->rx.buf = devm_kcalloc(dev, sizeof(u8), desc->max_msg_size,\n\t\t\t\t\t    GFP_KERNEL);\n\t\tif (!xfer->rx.buf)\n\t\t\treturn -ENOMEM;\n\n\t\txfer->tx.buf = xfer->rx.buf;\n\t\tinit_completion(&xfer->done);\n\t\tspin_lock_init(&xfer->lock);\n\n\t\t \n\t\thlist_add_head(&xfer->node, &info->free_xfers);\n\t}\n\n\tspin_lock_init(&info->xfer_lock);\n\n\treturn 0;\n}\n\nstatic int scmi_channels_max_msg_configure(struct scmi_info *sinfo)\n{\n\tconst struct scmi_desc *desc = sinfo->desc;\n\n\tif (!desc->ops->get_max_msg) {\n\t\tsinfo->tx_minfo.max_msg = desc->max_msg;\n\t\tsinfo->rx_minfo.max_msg = desc->max_msg;\n\t} else {\n\t\tstruct scmi_chan_info *base_cinfo;\n\n\t\tbase_cinfo = idr_find(&sinfo->tx_idr, SCMI_PROTOCOL_BASE);\n\t\tif (!base_cinfo)\n\t\t\treturn -EINVAL;\n\t\tsinfo->tx_minfo.max_msg = desc->ops->get_max_msg(base_cinfo);\n\n\t\t \n\t\tbase_cinfo = idr_find(&sinfo->rx_idr, SCMI_PROTOCOL_BASE);\n\t\tif (base_cinfo)\n\t\t\tsinfo->rx_minfo.max_msg =\n\t\t\t\tdesc->ops->get_max_msg(base_cinfo);\n\t}\n\n\treturn 0;\n}\n\nstatic int scmi_xfer_info_init(struct scmi_info *sinfo)\n{\n\tint ret;\n\n\tret = scmi_channels_max_msg_configure(sinfo);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __scmi_xfer_info_init(sinfo, &sinfo->tx_minfo);\n\tif (!ret && !idr_is_empty(&sinfo->rx_idr))\n\t\tret = __scmi_xfer_info_init(sinfo, &sinfo->rx_minfo);\n\n\treturn ret;\n}\n\nstatic int scmi_chan_setup(struct scmi_info *info, struct device_node *of_node,\n\t\t\t   int prot_id, bool tx)\n{\n\tint ret, idx;\n\tchar name[32];\n\tstruct scmi_chan_info *cinfo;\n\tstruct idr *idr;\n\tstruct scmi_device *tdev = NULL;\n\n\t \n\tidx = tx ? 0 : 1;\n\tidr = tx ? &info->tx_idr : &info->rx_idr;\n\n\tif (!info->desc->ops->chan_available(of_node, idx)) {\n\t\tcinfo = idr_find(idr, SCMI_PROTOCOL_BASE);\n\t\tif (unlikely(!cinfo))  \n\t\t\treturn -EINVAL;\n\t\tgoto idr_alloc;\n\t}\n\n\tcinfo = devm_kzalloc(info->dev, sizeof(*cinfo), GFP_KERNEL);\n\tif (!cinfo)\n\t\treturn -ENOMEM;\n\n\tcinfo->rx_timeout_ms = info->desc->max_rx_timeout_ms;\n\n\t \n\tsnprintf(name, 32, \"__scmi_transport_device_%s_%02X\",\n\t\t idx ? \"rx\" : \"tx\", prot_id);\n\t \n\ttdev = scmi_device_create(of_node, info->dev, prot_id, name);\n\tif (!tdev) {\n\t\tdev_err(info->dev,\n\t\t\t\"failed to create transport device (%s)\\n\", name);\n\t\tdevm_kfree(info->dev, cinfo);\n\t\treturn -EINVAL;\n\t}\n\tof_node_get(of_node);\n\n\tcinfo->id = prot_id;\n\tcinfo->dev = &tdev->dev;\n\tret = info->desc->ops->chan_setup(cinfo, info->dev, tx);\n\tif (ret) {\n\t\tof_node_put(of_node);\n\t\tscmi_device_destroy(info->dev, prot_id, name);\n\t\tdevm_kfree(info->dev, cinfo);\n\t\treturn ret;\n\t}\n\n\tif (tx && is_polling_required(cinfo, info->desc)) {\n\t\tif (is_transport_polling_capable(info->desc))\n\t\t\tdev_info(&tdev->dev,\n\t\t\t\t \"Enabled polling mode TX channel - prot_id:%d\\n\",\n\t\t\t\t prot_id);\n\t\telse\n\t\t\tdev_warn(&tdev->dev,\n\t\t\t\t \"Polling mode NOT supported by transport.\\n\");\n\t}\n\nidr_alloc:\n\tret = idr_alloc(idr, cinfo, prot_id, prot_id + 1, GFP_KERNEL);\n\tif (ret != prot_id) {\n\t\tdev_err(info->dev,\n\t\t\t\"unable to allocate SCMI idr slot err %d\\n\", ret);\n\t\t \n\t\tif (tdev) {\n\t\t\tof_node_put(of_node);\n\t\t\tscmi_device_destroy(info->dev, prot_id, name);\n\t\t\tdevm_kfree(info->dev, cinfo);\n\t\t}\n\t\treturn ret;\n\t}\n\n\tcinfo->handle = &info->handle;\n\treturn 0;\n}\n\nstatic inline int\nscmi_txrx_setup(struct scmi_info *info, struct device_node *of_node,\n\t\tint prot_id)\n{\n\tint ret = scmi_chan_setup(info, of_node, prot_id, true);\n\n\tif (!ret) {\n\t\t \n\t\tret = scmi_chan_setup(info, of_node, prot_id, false);\n\t\tif (ret && ret != -ENOMEM)\n\t\t\tret = 0;\n\t}\n\n\treturn ret;\n}\n\n \nstatic int scmi_channels_setup(struct scmi_info *info)\n{\n\tint ret;\n\tstruct device_node *child, *top_np = info->dev->of_node;\n\n\t \n\tret = scmi_txrx_setup(info, top_np, SCMI_PROTOCOL_BASE);\n\tif (ret)\n\t\treturn ret;\n\n\tfor_each_available_child_of_node(top_np, child) {\n\t\tu32 prot_id;\n\n\t\tif (of_property_read_u32(child, \"reg\", &prot_id))\n\t\t\tcontinue;\n\n\t\tif (!FIELD_FIT(MSG_PROTOCOL_ID_MASK, prot_id))\n\t\t\tdev_err(info->dev,\n\t\t\t\t\"Out of range protocol %d\\n\", prot_id);\n\n\t\tret = scmi_txrx_setup(info, child, prot_id);\n\t\tif (ret) {\n\t\t\tof_node_put(child);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int scmi_chan_destroy(int id, void *p, void *idr)\n{\n\tstruct scmi_chan_info *cinfo = p;\n\n\tif (cinfo->dev) {\n\t\tstruct scmi_info *info = handle_to_scmi_info(cinfo->handle);\n\t\tstruct scmi_device *sdev = to_scmi_dev(cinfo->dev);\n\n\t\tof_node_put(cinfo->dev->of_node);\n\t\tscmi_device_destroy(info->dev, id, sdev->name);\n\t\tcinfo->dev = NULL;\n\t}\n\n\tidr_remove(idr, id);\n\n\treturn 0;\n}\n\nstatic void scmi_cleanup_channels(struct scmi_info *info, struct idr *idr)\n{\n\t \n\tidr_for_each(idr, info->desc->ops->chan_free, idr);\n\n\t \n\tidr_for_each(idr, scmi_chan_destroy, idr);\n\n\tidr_destroy(idr);\n}\n\nstatic void scmi_cleanup_txrx_channels(struct scmi_info *info)\n{\n\tscmi_cleanup_channels(info, &info->tx_idr);\n\n\tscmi_cleanup_channels(info, &info->rx_idr);\n}\n\nstatic int scmi_bus_notifier(struct notifier_block *nb,\n\t\t\t     unsigned long action, void *data)\n{\n\tstruct scmi_info *info = bus_nb_to_scmi_info(nb);\n\tstruct scmi_device *sdev = to_scmi_dev(data);\n\n\t \n\tif (!strncmp(sdev->name, \"__scmi_transport_device\", 23) ||\n\t    sdev->dev.parent != info->dev)\n\t\treturn NOTIFY_DONE;\n\n\tswitch (action) {\n\tcase BUS_NOTIFY_BIND_DRIVER:\n\t\t \n\t\tscmi_set_handle(sdev);\n\t\tbreak;\n\tcase BUS_NOTIFY_UNBOUND_DRIVER:\n\t\tscmi_handle_put(sdev->handle);\n\t\tsdev->handle = NULL;\n\t\tbreak;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n\n\tdev_dbg(info->dev, \"Device %s (%s) is now %s\\n\", dev_name(&sdev->dev),\n\t\tsdev->name, action == BUS_NOTIFY_BIND_DRIVER ?\n\t\t\"about to be BOUND.\" : \"UNBOUND.\");\n\n\treturn NOTIFY_OK;\n}\n\nstatic int scmi_device_request_notifier(struct notifier_block *nb,\n\t\t\t\t\tunsigned long action, void *data)\n{\n\tstruct device_node *np;\n\tstruct scmi_device_id *id_table = data;\n\tstruct scmi_info *info = req_nb_to_scmi_info(nb);\n\n\tnp = idr_find(&info->active_protocols, id_table->protocol_id);\n\tif (!np)\n\t\treturn NOTIFY_DONE;\n\n\tdev_dbg(info->dev, \"%sRequested device (%s) for protocol 0x%x\\n\",\n\t\taction == SCMI_BUS_NOTIFY_DEVICE_REQUEST ? \"\" : \"UN-\",\n\t\tid_table->name, id_table->protocol_id);\n\n\tswitch (action) {\n\tcase SCMI_BUS_NOTIFY_DEVICE_REQUEST:\n\t\tscmi_create_protocol_devices(np, info, id_table->protocol_id,\n\t\t\t\t\t     id_table->name);\n\t\tbreak;\n\tcase SCMI_BUS_NOTIFY_DEVICE_UNREQUEST:\n\t\tscmi_destroy_protocol_devices(info, id_table->protocol_id,\n\t\t\t\t\t      id_table->name);\n\t\tbreak;\n\tdefault:\n\t\treturn NOTIFY_DONE;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nstatic void scmi_debugfs_common_cleanup(void *d)\n{\n\tstruct scmi_debug_info *dbg = d;\n\n\tif (!dbg)\n\t\treturn;\n\n\tdebugfs_remove_recursive(dbg->top_dentry);\n\tkfree(dbg->name);\n\tkfree(dbg->type);\n}\n\nstatic struct scmi_debug_info *scmi_debugfs_common_setup(struct scmi_info *info)\n{\n\tchar top_dir[16];\n\tstruct dentry *trans, *top_dentry;\n\tstruct scmi_debug_info *dbg;\n\tconst char *c_ptr = NULL;\n\n\tdbg = devm_kzalloc(info->dev, sizeof(*dbg), GFP_KERNEL);\n\tif (!dbg)\n\t\treturn NULL;\n\n\tdbg->name = kstrdup(of_node_full_name(info->dev->of_node), GFP_KERNEL);\n\tif (!dbg->name) {\n\t\tdevm_kfree(info->dev, dbg);\n\t\treturn NULL;\n\t}\n\n\tof_property_read_string(info->dev->of_node, \"compatible\", &c_ptr);\n\tdbg->type = kstrdup(c_ptr, GFP_KERNEL);\n\tif (!dbg->type) {\n\t\tkfree(dbg->name);\n\t\tdevm_kfree(info->dev, dbg);\n\t\treturn NULL;\n\t}\n\n\tsnprintf(top_dir, 16, \"%d\", info->id);\n\ttop_dentry = debugfs_create_dir(top_dir, scmi_top_dentry);\n\ttrans = debugfs_create_dir(\"transport\", top_dentry);\n\n\tdbg->is_atomic = info->desc->atomic_enabled &&\n\t\t\t\tis_transport_polling_capable(info->desc);\n\n\tdebugfs_create_str(\"instance_name\", 0400, top_dentry,\n\t\t\t   (char **)&dbg->name);\n\n\tdebugfs_create_u32(\"atomic_threshold_us\", 0400, top_dentry,\n\t\t\t   &info->atomic_threshold);\n\n\tdebugfs_create_str(\"type\", 0400, trans, (char **)&dbg->type);\n\n\tdebugfs_create_bool(\"is_atomic\", 0400, trans, &dbg->is_atomic);\n\n\tdebugfs_create_u32(\"max_rx_timeout_ms\", 0400, trans,\n\t\t\t   (u32 *)&info->desc->max_rx_timeout_ms);\n\n\tdebugfs_create_u32(\"max_msg_size\", 0400, trans,\n\t\t\t   (u32 *)&info->desc->max_msg_size);\n\n\tdebugfs_create_u32(\"tx_max_msg\", 0400, trans,\n\t\t\t   (u32 *)&info->tx_minfo.max_msg);\n\n\tdebugfs_create_u32(\"rx_max_msg\", 0400, trans,\n\t\t\t   (u32 *)&info->rx_minfo.max_msg);\n\n\tdbg->top_dentry = top_dentry;\n\n\tif (devm_add_action_or_reset(info->dev,\n\t\t\t\t     scmi_debugfs_common_cleanup, dbg)) {\n\t\tscmi_debugfs_common_cleanup(dbg);\n\t\treturn NULL;\n\t}\n\n\treturn dbg;\n}\n\nstatic int scmi_debugfs_raw_mode_setup(struct scmi_info *info)\n{\n\tint id, num_chans = 0, ret = 0;\n\tstruct scmi_chan_info *cinfo;\n\tu8 channels[SCMI_MAX_CHANNELS] = {};\n\tDECLARE_BITMAP(protos, SCMI_MAX_CHANNELS) = {};\n\n\tif (!info->dbg)\n\t\treturn -EINVAL;\n\n\t \n\tidr_for_each_entry(&info->tx_idr, cinfo, id) {\n\t\t \n\t\tif (num_chans >= SCMI_MAX_CHANNELS || !cinfo) {\n\t\t\tdev_warn(info->dev,\n\t\t\t\t \"SCMI RAW - Error enumerating channels\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!test_bit(cinfo->id, protos)) {\n\t\t\tchannels[num_chans++] = cinfo->id;\n\t\t\tset_bit(cinfo->id, protos);\n\t\t}\n\t}\n\n\tinfo->raw = scmi_raw_mode_init(&info->handle, info->dbg->top_dentry,\n\t\t\t\t       info->id, channels, num_chans,\n\t\t\t\t       info->desc, info->tx_minfo.max_msg);\n\tif (IS_ERR(info->raw)) {\n\t\tdev_err(info->dev, \"Failed to initialize SCMI RAW Mode !\\n\");\n\t\tret = PTR_ERR(info->raw);\n\t\tinfo->raw = NULL;\n\t}\n\n\treturn ret;\n}\n\nstatic int scmi_probe(struct platform_device *pdev)\n{\n\tint ret;\n\tstruct scmi_handle *handle;\n\tconst struct scmi_desc *desc;\n\tstruct scmi_info *info;\n\tbool coex = IS_ENABLED(CONFIG_ARM_SCMI_RAW_MODE_SUPPORT_COEX);\n\tstruct device *dev = &pdev->dev;\n\tstruct device_node *child, *np = dev->of_node;\n\n\tdesc = of_device_get_match_data(dev);\n\tif (!desc)\n\t\treturn -EINVAL;\n\n\tinfo = devm_kzalloc(dev, sizeof(*info), GFP_KERNEL);\n\tif (!info)\n\t\treturn -ENOMEM;\n\n\tinfo->id = ida_alloc_min(&scmi_id, 0, GFP_KERNEL);\n\tif (info->id < 0)\n\t\treturn info->id;\n\n\tinfo->dev = dev;\n\tinfo->desc = desc;\n\tinfo->bus_nb.notifier_call = scmi_bus_notifier;\n\tinfo->dev_req_nb.notifier_call = scmi_device_request_notifier;\n\tINIT_LIST_HEAD(&info->node);\n\tidr_init(&info->protocols);\n\tmutex_init(&info->protocols_mtx);\n\tidr_init(&info->active_protocols);\n\tmutex_init(&info->devreq_mtx);\n\n\tplatform_set_drvdata(pdev, info);\n\tidr_init(&info->tx_idr);\n\tidr_init(&info->rx_idr);\n\n\thandle = &info->handle;\n\thandle->dev = info->dev;\n\thandle->version = &info->version;\n\thandle->devm_protocol_acquire = scmi_devm_protocol_acquire;\n\thandle->devm_protocol_get = scmi_devm_protocol_get;\n\thandle->devm_protocol_put = scmi_devm_protocol_put;\n\n\t \n\tif (!of_property_read_u32(np, \"atomic-threshold-us\",\n\t\t\t\t  &info->atomic_threshold))\n\t\tdev_info(dev,\n\t\t\t \"SCMI System wide atomic threshold set to %d us\\n\",\n\t\t\t info->atomic_threshold);\n\thandle->is_transport_atomic = scmi_is_transport_atomic;\n\n\tif (desc->ops->link_supplier) {\n\t\tret = desc->ops->link_supplier(dev);\n\t\tif (ret)\n\t\t\tgoto clear_ida;\n\t}\n\n\t \n\tret = scmi_channels_setup(info);\n\tif (ret)\n\t\tgoto clear_ida;\n\n\tret = bus_register_notifier(&scmi_bus_type, &info->bus_nb);\n\tif (ret)\n\t\tgoto clear_txrx_setup;\n\n\tret = blocking_notifier_chain_register(&scmi_requested_devices_nh,\n\t\t\t\t\t       &info->dev_req_nb);\n\tif (ret)\n\t\tgoto clear_bus_notifier;\n\n\tret = scmi_xfer_info_init(info);\n\tif (ret)\n\t\tgoto clear_dev_req_notifier;\n\n\tif (scmi_top_dentry) {\n\t\tinfo->dbg = scmi_debugfs_common_setup(info);\n\t\tif (!info->dbg)\n\t\t\tdev_warn(dev, \"Failed to setup SCMI debugfs.\\n\");\n\n\t\tif (IS_ENABLED(CONFIG_ARM_SCMI_RAW_MODE_SUPPORT)) {\n\t\t\tret = scmi_debugfs_raw_mode_setup(info);\n\t\t\tif (!coex) {\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto clear_dev_req_notifier;\n\n\t\t\t\t \n\t\t\t\treturn 0;\n\t\t\t}\n\n\t\t\t \n\t\t\tdev_info(dev, \"SCMI RAW Mode COEX enabled !\\n\");\n\t\t}\n\t}\n\n\tif (scmi_notification_init(handle))\n\t\tdev_err(dev, \"SCMI Notifications NOT available.\\n\");\n\n\tif (info->desc->atomic_enabled &&\n\t    !is_transport_polling_capable(info->desc))\n\t\tdev_err(dev,\n\t\t\t\"Transport is not polling capable. Atomic mode not supported.\\n\");\n\n\t \n\tret = scmi_protocol_acquire(handle, SCMI_PROTOCOL_BASE);\n\tif (ret) {\n\t\tdev_err(dev, \"unable to communicate with SCMI\\n\");\n\t\tif (coex)\n\t\t\treturn 0;\n\t\tgoto notification_exit;\n\t}\n\n\tmutex_lock(&scmi_list_mutex);\n\tlist_add_tail(&info->node, &scmi_list);\n\tmutex_unlock(&scmi_list_mutex);\n\n\tfor_each_available_child_of_node(np, child) {\n\t\tu32 prot_id;\n\n\t\tif (of_property_read_u32(child, \"reg\", &prot_id))\n\t\t\tcontinue;\n\n\t\tif (!FIELD_FIT(MSG_PROTOCOL_ID_MASK, prot_id))\n\t\t\tdev_err(dev, \"Out of range protocol %d\\n\", prot_id);\n\n\t\tif (!scmi_is_protocol_implemented(handle, prot_id)) {\n\t\t\tdev_err(dev, \"SCMI protocol %d not implemented\\n\",\n\t\t\t\tprot_id);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tret = idr_alloc(&info->active_protocols, child,\n\t\t\t\tprot_id, prot_id + 1, GFP_KERNEL);\n\t\tif (ret != prot_id) {\n\t\t\tdev_err(dev, \"SCMI protocol %d already activated. Skip\\n\",\n\t\t\t\tprot_id);\n\t\t\tcontinue;\n\t\t}\n\n\t\tof_node_get(child);\n\t\tscmi_create_protocol_devices(child, info, prot_id, NULL);\n\t}\n\n\treturn 0;\n\nnotification_exit:\n\tif (IS_ENABLED(CONFIG_ARM_SCMI_RAW_MODE_SUPPORT))\n\t\tscmi_raw_mode_cleanup(info->raw);\n\tscmi_notification_exit(&info->handle);\nclear_dev_req_notifier:\n\tblocking_notifier_chain_unregister(&scmi_requested_devices_nh,\n\t\t\t\t\t   &info->dev_req_nb);\nclear_bus_notifier:\n\tbus_unregister_notifier(&scmi_bus_type, &info->bus_nb);\nclear_txrx_setup:\n\tscmi_cleanup_txrx_channels(info);\nclear_ida:\n\tida_free(&scmi_id, info->id);\n\treturn ret;\n}\n\nstatic int scmi_remove(struct platform_device *pdev)\n{\n\tint id;\n\tstruct scmi_info *info = platform_get_drvdata(pdev);\n\tstruct device_node *child;\n\n\tif (IS_ENABLED(CONFIG_ARM_SCMI_RAW_MODE_SUPPORT))\n\t\tscmi_raw_mode_cleanup(info->raw);\n\n\tmutex_lock(&scmi_list_mutex);\n\tif (info->users)\n\t\tdev_warn(&pdev->dev,\n\t\t\t \"Still active SCMI users will be forcibly unbound.\\n\");\n\tlist_del(&info->node);\n\tmutex_unlock(&scmi_list_mutex);\n\n\tscmi_notification_exit(&info->handle);\n\n\tmutex_lock(&info->protocols_mtx);\n\tidr_destroy(&info->protocols);\n\tmutex_unlock(&info->protocols_mtx);\n\n\tidr_for_each_entry(&info->active_protocols, child, id)\n\t\tof_node_put(child);\n\tidr_destroy(&info->active_protocols);\n\n\tblocking_notifier_chain_unregister(&scmi_requested_devices_nh,\n\t\t\t\t\t   &info->dev_req_nb);\n\tbus_unregister_notifier(&scmi_bus_type, &info->bus_nb);\n\n\t \n\tscmi_cleanup_txrx_channels(info);\n\n\tida_free(&scmi_id, info->id);\n\n\treturn 0;\n}\n\nstatic ssize_t protocol_version_show(struct device *dev,\n\t\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct scmi_info *info = dev_get_drvdata(dev);\n\n\treturn sprintf(buf, \"%u.%u\\n\", info->version.major_ver,\n\t\t       info->version.minor_ver);\n}\nstatic DEVICE_ATTR_RO(protocol_version);\n\nstatic ssize_t firmware_version_show(struct device *dev,\n\t\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct scmi_info *info = dev_get_drvdata(dev);\n\n\treturn sprintf(buf, \"0x%x\\n\", info->version.impl_ver);\n}\nstatic DEVICE_ATTR_RO(firmware_version);\n\nstatic ssize_t vendor_id_show(struct device *dev,\n\t\t\t      struct device_attribute *attr, char *buf)\n{\n\tstruct scmi_info *info = dev_get_drvdata(dev);\n\n\treturn sprintf(buf, \"%s\\n\", info->version.vendor_id);\n}\nstatic DEVICE_ATTR_RO(vendor_id);\n\nstatic ssize_t sub_vendor_id_show(struct device *dev,\n\t\t\t\t  struct device_attribute *attr, char *buf)\n{\n\tstruct scmi_info *info = dev_get_drvdata(dev);\n\n\treturn sprintf(buf, \"%s\\n\", info->version.sub_vendor_id);\n}\nstatic DEVICE_ATTR_RO(sub_vendor_id);\n\nstatic struct attribute *versions_attrs[] = {\n\t&dev_attr_firmware_version.attr,\n\t&dev_attr_protocol_version.attr,\n\t&dev_attr_vendor_id.attr,\n\t&dev_attr_sub_vendor_id.attr,\n\tNULL,\n};\nATTRIBUTE_GROUPS(versions);\n\n \nstatic const struct of_device_id scmi_of_match[] = {\n#ifdef CONFIG_ARM_SCMI_TRANSPORT_MAILBOX\n\t{ .compatible = \"arm,scmi\", .data = &scmi_mailbox_desc },\n#endif\n#ifdef CONFIG_ARM_SCMI_TRANSPORT_OPTEE\n\t{ .compatible = \"linaro,scmi-optee\", .data = &scmi_optee_desc },\n#endif\n#ifdef CONFIG_ARM_SCMI_TRANSPORT_SMC\n\t{ .compatible = \"arm,scmi-smc\", .data = &scmi_smc_desc},\n\t{ .compatible = \"arm,scmi-smc-param\", .data = &scmi_smc_desc},\n#endif\n#ifdef CONFIG_ARM_SCMI_TRANSPORT_VIRTIO\n\t{ .compatible = \"arm,scmi-virtio\", .data = &scmi_virtio_desc},\n#endif\n\t{   },\n};\n\nMODULE_DEVICE_TABLE(of, scmi_of_match);\n\nstatic struct platform_driver scmi_driver = {\n\t.driver = {\n\t\t   .name = \"arm-scmi\",\n\t\t   .suppress_bind_attrs = true,\n\t\t   .of_match_table = scmi_of_match,\n\t\t   .dev_groups = versions_groups,\n\t\t   },\n\t.probe = scmi_probe,\n\t.remove = scmi_remove,\n};\n\n \nstatic inline int __scmi_transports_setup(bool init)\n{\n\tint ret = 0;\n\tconst struct of_device_id *trans;\n\n\tfor (trans = scmi_of_match; trans->data; trans++) {\n\t\tconst struct scmi_desc *tdesc = trans->data;\n\n\t\tif ((init && !tdesc->transport_init) ||\n\t\t    (!init && !tdesc->transport_exit))\n\t\t\tcontinue;\n\n\t\tif (init)\n\t\t\tret = tdesc->transport_init();\n\t\telse\n\t\t\ttdesc->transport_exit();\n\n\t\tif (ret) {\n\t\t\tpr_err(\"SCMI transport %s FAILED initialization!\\n\",\n\t\t\t       trans->compatible);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\nstatic int __init scmi_transports_init(void)\n{\n\treturn __scmi_transports_setup(true);\n}\n\nstatic void __exit scmi_transports_exit(void)\n{\n\t__scmi_transports_setup(false);\n}\n\nstatic struct dentry *scmi_debugfs_init(void)\n{\n\tstruct dentry *d;\n\n\td = debugfs_create_dir(\"scmi\", NULL);\n\tif (IS_ERR(d)) {\n\t\tpr_err(\"Could NOT create SCMI top dentry.\\n\");\n\t\treturn NULL;\n\t}\n\n\treturn d;\n}\n\nstatic int __init scmi_driver_init(void)\n{\n\tint ret;\n\n\t \n\tif (WARN_ON(!IS_ENABLED(CONFIG_ARM_SCMI_HAVE_TRANSPORT)))\n\t\treturn -EINVAL;\n\n\t \n\tret = scmi_transports_init();\n\tif (ret)\n\t\treturn ret;\n\n\tif (IS_ENABLED(CONFIG_ARM_SCMI_NEED_DEBUGFS))\n\t\tscmi_top_dentry = scmi_debugfs_init();\n\n\tscmi_base_register();\n\n\tscmi_clock_register();\n\tscmi_perf_register();\n\tscmi_power_register();\n\tscmi_reset_register();\n\tscmi_sensors_register();\n\tscmi_voltage_register();\n\tscmi_system_register();\n\tscmi_powercap_register();\n\n\treturn platform_driver_register(&scmi_driver);\n}\nmodule_init(scmi_driver_init);\n\nstatic void __exit scmi_driver_exit(void)\n{\n\tscmi_base_unregister();\n\n\tscmi_clock_unregister();\n\tscmi_perf_unregister();\n\tscmi_power_unregister();\n\tscmi_reset_unregister();\n\tscmi_sensors_unregister();\n\tscmi_voltage_unregister();\n\tscmi_system_unregister();\n\tscmi_powercap_unregister();\n\n\tscmi_transports_exit();\n\n\tplatform_driver_unregister(&scmi_driver);\n\n\tdebugfs_remove_recursive(scmi_top_dentry);\n}\nmodule_exit(scmi_driver_exit);\n\nMODULE_ALIAS(\"platform:arm-scmi\");\nMODULE_AUTHOR(\"Sudeep Holla <sudeep.holla@arm.com>\");\nMODULE_DESCRIPTION(\"ARM SCMI protocol driver\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}