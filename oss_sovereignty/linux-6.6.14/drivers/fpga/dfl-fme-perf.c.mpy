{
  "module_name": "dfl-fme-perf.c",
  "hash_id": "f32eeb782c4d1c07957afb0332f24e55241904b6b4e287f5e68e69c189db4006",
  "original_prompt": "Ingested from linux-6.6.14/drivers/fpga/dfl-fme-perf.c",
  "human_readable_source": "\n \n\n#include <linux/perf_event.h>\n#include \"dfl.h\"\n#include \"dfl-fme.h\"\n\n \n#define CACHE_CTRL\t\t\t0x8\n#define CACHE_RESET_CNTR\t\tBIT_ULL(0)\n#define CACHE_FREEZE_CNTR\t\tBIT_ULL(8)\n#define CACHE_CTRL_EVNT\t\t\tGENMASK_ULL(19, 16)\n#define CACHE_EVNT_RD_HIT\t\t0x0\n#define CACHE_EVNT_WR_HIT\t\t0x1\n#define CACHE_EVNT_RD_MISS\t\t0x2\n#define CACHE_EVNT_WR_MISS\t\t0x3\n#define CACHE_EVNT_RSVD\t\t\t0x4\n#define CACHE_EVNT_HOLD_REQ\t\t0x5\n#define CACHE_EVNT_DATA_WR_PORT_CONTEN\t0x6\n#define CACHE_EVNT_TAG_WR_PORT_CONTEN\t0x7\n#define CACHE_EVNT_TX_REQ_STALL\t\t0x8\n#define CACHE_EVNT_RX_REQ_STALL\t\t0x9\n#define CACHE_EVNT_EVICTIONS\t\t0xa\n#define CACHE_EVNT_MAX\t\t\tCACHE_EVNT_EVICTIONS\n#define CACHE_CHANNEL_SEL\t\tBIT_ULL(20)\n#define CACHE_CHANNEL_RD\t\t0\n#define CACHE_CHANNEL_WR\t\t1\n#define CACHE_CNTR0\t\t\t0x10\n#define CACHE_CNTR1\t\t\t0x18\n#define CACHE_CNTR_EVNT_CNTR\t\tGENMASK_ULL(47, 0)\n#define CACHE_CNTR_EVNT\t\t\tGENMASK_ULL(63, 60)\n\n \n#define FAB_CTRL\t\t\t0x20\n#define FAB_RESET_CNTR\t\t\tBIT_ULL(0)\n#define FAB_FREEZE_CNTR\t\t\tBIT_ULL(8)\n#define FAB_CTRL_EVNT\t\t\tGENMASK_ULL(19, 16)\n#define FAB_EVNT_PCIE0_RD\t\t0x0\n#define FAB_EVNT_PCIE0_WR\t\t0x1\n#define FAB_EVNT_PCIE1_RD\t\t0x2\n#define FAB_EVNT_PCIE1_WR\t\t0x3\n#define FAB_EVNT_UPI_RD\t\t\t0x4\n#define FAB_EVNT_UPI_WR\t\t\t0x5\n#define FAB_EVNT_MMIO_RD\t\t0x6\n#define FAB_EVNT_MMIO_WR\t\t0x7\n#define FAB_EVNT_MAX\t\t\tFAB_EVNT_MMIO_WR\n#define FAB_PORT_ID\t\t\tGENMASK_ULL(21, 20)\n#define FAB_PORT_FILTER\t\t\tBIT_ULL(23)\n#define FAB_PORT_FILTER_DISABLE\t\t0\n#define FAB_PORT_FILTER_ENABLE\t\t1\n#define FAB_CNTR\t\t\t0x28\n#define FAB_CNTR_EVNT_CNTR\t\tGENMASK_ULL(59, 0)\n#define FAB_CNTR_EVNT\t\t\tGENMASK_ULL(63, 60)\n\n \n#define CLK_CNTR\t\t\t0x30\n#define BASIC_EVNT_CLK\t\t\t0x0\n#define BASIC_EVNT_MAX\t\t\tBASIC_EVNT_CLK\n\n \n#define VTD_CTRL\t\t\t0x38\n#define VTD_RESET_CNTR\t\t\tBIT_ULL(0)\n#define VTD_FREEZE_CNTR\t\t\tBIT_ULL(8)\n#define VTD_CTRL_EVNT\t\t\tGENMASK_ULL(19, 16)\n#define VTD_EVNT_AFU_MEM_RD_TRANS\t0x0\n#define VTD_EVNT_AFU_MEM_WR_TRANS\t0x1\n#define VTD_EVNT_AFU_DEVTLB_RD_HIT\t0x2\n#define VTD_EVNT_AFU_DEVTLB_WR_HIT\t0x3\n#define VTD_EVNT_DEVTLB_4K_FILL\t\t0x4\n#define VTD_EVNT_DEVTLB_2M_FILL\t\t0x5\n#define VTD_EVNT_DEVTLB_1G_FILL\t\t0x6\n#define VTD_EVNT_MAX\t\t\tVTD_EVNT_DEVTLB_1G_FILL\n#define VTD_CNTR\t\t\t0x40\n#define VTD_CNTR_EVNT_CNTR\t\tGENMASK_ULL(47, 0)\n#define VTD_CNTR_EVNT\t\t\tGENMASK_ULL(63, 60)\n\n#define VTD_SIP_CTRL\t\t\t0x48\n#define VTD_SIP_RESET_CNTR\t\tBIT_ULL(0)\n#define VTD_SIP_FREEZE_CNTR\t\tBIT_ULL(8)\n#define VTD_SIP_CTRL_EVNT\t\tGENMASK_ULL(19, 16)\n#define VTD_SIP_EVNT_IOTLB_4K_HIT\t0x0\n#define VTD_SIP_EVNT_IOTLB_2M_HIT\t0x1\n#define VTD_SIP_EVNT_IOTLB_1G_HIT\t0x2\n#define VTD_SIP_EVNT_SLPWC_L3_HIT\t0x3\n#define VTD_SIP_EVNT_SLPWC_L4_HIT\t0x4\n#define VTD_SIP_EVNT_RCC_HIT\t\t0x5\n#define VTD_SIP_EVNT_IOTLB_4K_MISS\t0x6\n#define VTD_SIP_EVNT_IOTLB_2M_MISS\t0x7\n#define VTD_SIP_EVNT_IOTLB_1G_MISS\t0x8\n#define VTD_SIP_EVNT_SLPWC_L3_MISS\t0x9\n#define VTD_SIP_EVNT_SLPWC_L4_MISS\t0xa\n#define VTD_SIP_EVNT_RCC_MISS\t\t0xb\n#define VTD_SIP_EVNT_MAX\t\tVTD_SIP_EVNT_SLPWC_L4_MISS\n#define VTD_SIP_CNTR\t\t\t0X50\n#define VTD_SIP_CNTR_EVNT_CNTR\t\tGENMASK_ULL(47, 0)\n#define VTD_SIP_CNTR_EVNT\t\tGENMASK_ULL(63, 60)\n\n#define PERF_TIMEOUT\t\t\t30\n\n#define PERF_MAX_PORT_NUM\t\t1U\n\n \nstruct fme_perf_priv {\n\tstruct device *dev;\n\tvoid __iomem *ioaddr;\n\tstruct pmu pmu;\n\tu16 id;\n\n\tu32 fab_users;\n\tu32 fab_port_id;\n\tspinlock_t fab_lock;\n\n\tunsigned int cpu;\n\tstruct hlist_node node;\n\tenum cpuhp_state cpuhp_state;\n};\n\n \nstruct fme_perf_event_ops {\n\tint (*event_init)(struct fme_perf_priv *priv, u32 event, u32 portid);\n\tvoid (*event_destroy)(struct fme_perf_priv *priv, u32 event,\n\t\t\t      u32 portid);\n\tu64 (*read_counter)(struct fme_perf_priv *priv, u32 event, u32 portid);\n};\n\n#define to_fme_perf_priv(_pmu)\tcontainer_of(_pmu, struct fme_perf_priv, pmu)\n\nstatic ssize_t cpumask_show(struct device *dev,\n\t\t\t    struct device_attribute *attr, char *buf)\n{\n\tstruct pmu *pmu = dev_get_drvdata(dev);\n\tstruct fme_perf_priv *priv;\n\n\tpriv = to_fme_perf_priv(pmu);\n\n\treturn cpumap_print_to_pagebuf(true, buf, cpumask_of(priv->cpu));\n}\nstatic DEVICE_ATTR_RO(cpumask);\n\nstatic struct attribute *fme_perf_cpumask_attrs[] = {\n\t&dev_attr_cpumask.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group fme_perf_cpumask_group = {\n\t.attrs = fme_perf_cpumask_attrs,\n};\n\n#define FME_EVENT_MASK\t\tGENMASK_ULL(11, 0)\n#define FME_EVENT_SHIFT\t\t0\n#define FME_EVTYPE_MASK\t\tGENMASK_ULL(15, 12)\n#define FME_EVTYPE_SHIFT\t12\n#define FME_EVTYPE_BASIC\t0\n#define FME_EVTYPE_CACHE\t1\n#define FME_EVTYPE_FABRIC\t2\n#define FME_EVTYPE_VTD\t\t3\n#define FME_EVTYPE_VTD_SIP\t4\n#define FME_EVTYPE_MAX\t\tFME_EVTYPE_VTD_SIP\n#define FME_PORTID_MASK\t\tGENMASK_ULL(23, 16)\n#define FME_PORTID_SHIFT\t16\n#define FME_PORTID_ROOT\t\t(0xffU)\n\n#define get_event(_config)\tFIELD_GET(FME_EVENT_MASK, _config)\n#define get_evtype(_config)\tFIELD_GET(FME_EVTYPE_MASK, _config)\n#define get_portid(_config)\tFIELD_GET(FME_PORTID_MASK, _config)\n\nPMU_FORMAT_ATTR(event,\t\t\"config:0-11\");\nPMU_FORMAT_ATTR(evtype,\t\t\"config:12-15\");\nPMU_FORMAT_ATTR(portid,\t\t\"config:16-23\");\n\nstatic struct attribute *fme_perf_format_attrs[] = {\n\t&format_attr_event.attr,\n\t&format_attr_evtype.attr,\n\t&format_attr_portid.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group fme_perf_format_group = {\n\t.name = \"format\",\n\t.attrs = fme_perf_format_attrs,\n};\n\n \nstatic struct attribute *fme_perf_events_attrs_empty[] = {\n\tNULL,\n};\n\nstatic const struct attribute_group fme_perf_events_group = {\n\t.name = \"events\",\n\t.attrs = fme_perf_events_attrs_empty,\n};\n\nstatic const struct attribute_group *fme_perf_groups[] = {\n\t&fme_perf_format_group,\n\t&fme_perf_cpumask_group,\n\t&fme_perf_events_group,\n\tNULL,\n};\n\nstatic bool is_portid_root(u32 portid)\n{\n\treturn portid == FME_PORTID_ROOT;\n}\n\nstatic bool is_portid_port(u32 portid)\n{\n\treturn portid < PERF_MAX_PORT_NUM;\n}\n\nstatic bool is_portid_root_or_port(u32 portid)\n{\n\treturn is_portid_root(portid) || is_portid_port(portid);\n}\n\nstatic u64 fme_read_perf_cntr_reg(void __iomem *addr)\n{\n\tu32 low;\n\tu64 v;\n\n\t \n\tdo {\n\t\tv = readq(addr);\n\t\tlow = readl(addr);\n\t} while (((u32)v) > low);\n\n\treturn v;\n}\n\nstatic int basic_event_init(struct fme_perf_priv *priv, u32 event, u32 portid)\n{\n\tif (event <= BASIC_EVNT_MAX && is_portid_root(portid))\n\t\treturn 0;\n\n\treturn -EINVAL;\n}\n\nstatic u64 basic_read_event_counter(struct fme_perf_priv *priv,\n\t\t\t\t    u32 event, u32 portid)\n{\n\tvoid __iomem *base = priv->ioaddr;\n\n\treturn fme_read_perf_cntr_reg(base + CLK_CNTR);\n}\n\nstatic int cache_event_init(struct fme_perf_priv *priv, u32 event, u32 portid)\n{\n\tif (priv->id == FME_FEATURE_ID_GLOBAL_IPERF &&\n\t    event <= CACHE_EVNT_MAX && is_portid_root(portid))\n\t\treturn 0;\n\n\treturn -EINVAL;\n}\n\nstatic u64 cache_read_event_counter(struct fme_perf_priv *priv,\n\t\t\t\t    u32 event, u32 portid)\n{\n\tvoid __iomem *base = priv->ioaddr;\n\tu64 v, count;\n\tu8 channel;\n\n\tif (event == CACHE_EVNT_WR_HIT || event == CACHE_EVNT_WR_MISS ||\n\t    event == CACHE_EVNT_DATA_WR_PORT_CONTEN ||\n\t    event == CACHE_EVNT_TAG_WR_PORT_CONTEN)\n\t\tchannel = CACHE_CHANNEL_WR;\n\telse\n\t\tchannel = CACHE_CHANNEL_RD;\n\n\t \n\tv = readq(base + CACHE_CTRL);\n\tv &= ~(CACHE_CHANNEL_SEL | CACHE_CTRL_EVNT);\n\tv |= FIELD_PREP(CACHE_CHANNEL_SEL, channel);\n\tv |= FIELD_PREP(CACHE_CTRL_EVNT, event);\n\twriteq(v, base + CACHE_CTRL);\n\n\tif (readq_poll_timeout_atomic(base + CACHE_CNTR0, v,\n\t\t\t\t      FIELD_GET(CACHE_CNTR_EVNT, v) == event,\n\t\t\t\t      1, PERF_TIMEOUT)) {\n\t\tdev_err(priv->dev, \"timeout, unmatched cache event code in counter register.\\n\");\n\t\treturn 0;\n\t}\n\n\tv = fme_read_perf_cntr_reg(base + CACHE_CNTR0);\n\tcount = FIELD_GET(CACHE_CNTR_EVNT_CNTR, v);\n\tv = fme_read_perf_cntr_reg(base + CACHE_CNTR1);\n\tcount += FIELD_GET(CACHE_CNTR_EVNT_CNTR, v);\n\n\treturn count;\n}\n\nstatic bool is_fabric_event_supported(struct fme_perf_priv *priv, u32 event,\n\t\t\t\t      u32 portid)\n{\n\tif (event > FAB_EVNT_MAX || !is_portid_root_or_port(portid))\n\t\treturn false;\n\n\tif (priv->id == FME_FEATURE_ID_GLOBAL_DPERF &&\n\t    (event == FAB_EVNT_PCIE1_RD || event == FAB_EVNT_UPI_RD ||\n\t     event == FAB_EVNT_PCIE1_WR || event == FAB_EVNT_UPI_WR))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int fabric_event_init(struct fme_perf_priv *priv, u32 event, u32 portid)\n{\n\tvoid __iomem *base = priv->ioaddr;\n\tint ret = 0;\n\tu64 v;\n\n\tif (!is_fabric_event_supported(priv, event, portid))\n\t\treturn -EINVAL;\n\n\t \n\tspin_lock(&priv->fab_lock);\n\tif (priv->fab_users && priv->fab_port_id != portid) {\n\t\tdev_dbg(priv->dev, \"conflict fabric event monitoring mode.\\n\");\n\t\tret = -EOPNOTSUPP;\n\t\tgoto exit;\n\t}\n\n\tpriv->fab_users++;\n\n\t \n\tif (priv->fab_port_id == portid)\n\t\tgoto exit;\n\n\tpriv->fab_port_id = portid;\n\n\tv = readq(base + FAB_CTRL);\n\tv &= ~(FAB_PORT_FILTER | FAB_PORT_ID);\n\n\tif (is_portid_root(portid)) {\n\t\tv |= FIELD_PREP(FAB_PORT_FILTER, FAB_PORT_FILTER_DISABLE);\n\t} else {\n\t\tv |= FIELD_PREP(FAB_PORT_FILTER, FAB_PORT_FILTER_ENABLE);\n\t\tv |= FIELD_PREP(FAB_PORT_ID, portid);\n\t}\n\twriteq(v, base + FAB_CTRL);\n\nexit:\n\tspin_unlock(&priv->fab_lock);\n\treturn ret;\n}\n\nstatic void fabric_event_destroy(struct fme_perf_priv *priv, u32 event,\n\t\t\t\t u32 portid)\n{\n\tspin_lock(&priv->fab_lock);\n\tpriv->fab_users--;\n\tspin_unlock(&priv->fab_lock);\n}\n\nstatic u64 fabric_read_event_counter(struct fme_perf_priv *priv, u32 event,\n\t\t\t\t     u32 portid)\n{\n\tvoid __iomem *base = priv->ioaddr;\n\tu64 v;\n\n\tv = readq(base + FAB_CTRL);\n\tv &= ~FAB_CTRL_EVNT;\n\tv |= FIELD_PREP(FAB_CTRL_EVNT, event);\n\twriteq(v, base + FAB_CTRL);\n\n\tif (readq_poll_timeout_atomic(base + FAB_CNTR, v,\n\t\t\t\t      FIELD_GET(FAB_CNTR_EVNT, v) == event,\n\t\t\t\t      1, PERF_TIMEOUT)) {\n\t\tdev_err(priv->dev, \"timeout, unmatched fab event code in counter register.\\n\");\n\t\treturn 0;\n\t}\n\n\tv = fme_read_perf_cntr_reg(base + FAB_CNTR);\n\treturn FIELD_GET(FAB_CNTR_EVNT_CNTR, v);\n}\n\nstatic int vtd_event_init(struct fme_perf_priv *priv, u32 event, u32 portid)\n{\n\tif (priv->id == FME_FEATURE_ID_GLOBAL_IPERF &&\n\t    event <= VTD_EVNT_MAX && is_portid_port(portid))\n\t\treturn 0;\n\n\treturn -EINVAL;\n}\n\nstatic u64 vtd_read_event_counter(struct fme_perf_priv *priv, u32 event,\n\t\t\t\t  u32 portid)\n{\n\tvoid __iomem *base = priv->ioaddr;\n\tu64 v;\n\n\tevent += (portid * (VTD_EVNT_MAX + 1));\n\n\tv = readq(base + VTD_CTRL);\n\tv &= ~VTD_CTRL_EVNT;\n\tv |= FIELD_PREP(VTD_CTRL_EVNT, event);\n\twriteq(v, base + VTD_CTRL);\n\n\tif (readq_poll_timeout_atomic(base + VTD_CNTR, v,\n\t\t\t\t      FIELD_GET(VTD_CNTR_EVNT, v) == event,\n\t\t\t\t      1, PERF_TIMEOUT)) {\n\t\tdev_err(priv->dev, \"timeout, unmatched vtd event code in counter register.\\n\");\n\t\treturn 0;\n\t}\n\n\tv = fme_read_perf_cntr_reg(base + VTD_CNTR);\n\treturn FIELD_GET(VTD_CNTR_EVNT_CNTR, v);\n}\n\nstatic int vtd_sip_event_init(struct fme_perf_priv *priv, u32 event, u32 portid)\n{\n\tif (priv->id == FME_FEATURE_ID_GLOBAL_IPERF &&\n\t    event <= VTD_SIP_EVNT_MAX && is_portid_root(portid))\n\t\treturn 0;\n\n\treturn -EINVAL;\n}\n\nstatic u64 vtd_sip_read_event_counter(struct fme_perf_priv *priv, u32 event,\n\t\t\t\t      u32 portid)\n{\n\tvoid __iomem *base = priv->ioaddr;\n\tu64 v;\n\n\tv = readq(base + VTD_SIP_CTRL);\n\tv &= ~VTD_SIP_CTRL_EVNT;\n\tv |= FIELD_PREP(VTD_SIP_CTRL_EVNT, event);\n\twriteq(v, base + VTD_SIP_CTRL);\n\n\tif (readq_poll_timeout_atomic(base + VTD_SIP_CNTR, v,\n\t\t\t\t      FIELD_GET(VTD_SIP_CNTR_EVNT, v) == event,\n\t\t\t\t      1, PERF_TIMEOUT)) {\n\t\tdev_err(priv->dev, \"timeout, unmatched vtd sip event code in counter register\\n\");\n\t\treturn 0;\n\t}\n\n\tv = fme_read_perf_cntr_reg(base + VTD_SIP_CNTR);\n\treturn FIELD_GET(VTD_SIP_CNTR_EVNT_CNTR, v);\n}\n\nstatic struct fme_perf_event_ops fme_perf_event_ops[] = {\n\t[FME_EVTYPE_BASIC]\t= {.event_init = basic_event_init,\n\t\t\t\t   .read_counter = basic_read_event_counter,},\n\t[FME_EVTYPE_CACHE]\t= {.event_init = cache_event_init,\n\t\t\t\t   .read_counter = cache_read_event_counter,},\n\t[FME_EVTYPE_FABRIC]\t= {.event_init = fabric_event_init,\n\t\t\t\t   .event_destroy = fabric_event_destroy,\n\t\t\t\t   .read_counter = fabric_read_event_counter,},\n\t[FME_EVTYPE_VTD]\t= {.event_init = vtd_event_init,\n\t\t\t\t   .read_counter = vtd_read_event_counter,},\n\t[FME_EVTYPE_VTD_SIP]\t= {.event_init = vtd_sip_event_init,\n\t\t\t\t   .read_counter = vtd_sip_read_event_counter,},\n};\n\nstatic ssize_t fme_perf_event_show(struct device *dev,\n\t\t\t\t   struct device_attribute *attr, char *buf)\n{\n\tstruct dev_ext_attribute *eattr;\n\tunsigned long config;\n\tchar *ptr = buf;\n\n\teattr = container_of(attr, struct dev_ext_attribute, attr);\n\tconfig = (unsigned long)eattr->var;\n\n\tptr += sprintf(ptr, \"event=0x%02x\", (unsigned int)get_event(config));\n\tptr += sprintf(ptr, \",evtype=0x%02x\", (unsigned int)get_evtype(config));\n\n\tif (is_portid_root(get_portid(config)))\n\t\tptr += sprintf(ptr, \",portid=0x%02x\\n\", FME_PORTID_ROOT);\n\telse\n\t\tptr += sprintf(ptr, \",portid=?\\n\");\n\n\treturn (ssize_t)(ptr - buf);\n}\n\n#define FME_EVENT_ATTR(_name) \\\n\t__ATTR(_name, 0444, fme_perf_event_show, NULL)\n\n#define FME_PORT_EVENT_CONFIG(_event, _type)\t\t\t\t\\\n\t(void *)((((_event) << FME_EVENT_SHIFT) & FME_EVENT_MASK) |\t\\\n\t\t(((_type) << FME_EVTYPE_SHIFT) & FME_EVTYPE_MASK))\n\n#define FME_EVENT_CONFIG(_event, _type)\t\t\t\t\t\\\n\t(void *)((((_event) << FME_EVENT_SHIFT) & FME_EVENT_MASK) |\t\\\n\t\t(((_type) << FME_EVTYPE_SHIFT) & FME_EVTYPE_MASK) |\t\\\n\t\t(FME_PORTID_ROOT << FME_PORTID_SHIFT))\n\n \n#define FME_EVENT_BASIC(_name, _event)\t\t\t\t\t\\\nstatic struct dev_ext_attribute fme_perf_event_##_name = {\t\t\\\n\t.attr = FME_EVENT_ATTR(_name),\t\t\t\t\t\\\n\t.var = FME_EVENT_CONFIG(_event, FME_EVTYPE_BASIC),\t\t\\\n}\n\nFME_EVENT_BASIC(clock, BASIC_EVNT_CLK);\n\nstatic struct attribute *fme_perf_basic_events_attrs[] = {\n\t&fme_perf_event_clock.attr.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group fme_perf_basic_events_group = {\n\t.name = \"events\",\n\t.attrs = fme_perf_basic_events_attrs,\n};\n\n \n#define FME_EVENT_CACHE(_name, _event)\t\t\t\t\t\\\nstatic struct dev_ext_attribute fme_perf_event_cache_##_name = {\t\\\n\t.attr = FME_EVENT_ATTR(cache_##_name),\t\t\t\t\\\n\t.var = FME_EVENT_CONFIG(_event, FME_EVTYPE_CACHE),\t\t\\\n}\n\nFME_EVENT_CACHE(read_hit,     CACHE_EVNT_RD_HIT);\nFME_EVENT_CACHE(read_miss,    CACHE_EVNT_RD_MISS);\nFME_EVENT_CACHE(write_hit,    CACHE_EVNT_WR_HIT);\nFME_EVENT_CACHE(write_miss,   CACHE_EVNT_WR_MISS);\nFME_EVENT_CACHE(hold_request, CACHE_EVNT_HOLD_REQ);\nFME_EVENT_CACHE(tx_req_stall, CACHE_EVNT_TX_REQ_STALL);\nFME_EVENT_CACHE(rx_req_stall, CACHE_EVNT_RX_REQ_STALL);\nFME_EVENT_CACHE(eviction,     CACHE_EVNT_EVICTIONS);\nFME_EVENT_CACHE(data_write_port_contention, CACHE_EVNT_DATA_WR_PORT_CONTEN);\nFME_EVENT_CACHE(tag_write_port_contention,  CACHE_EVNT_TAG_WR_PORT_CONTEN);\n\nstatic struct attribute *fme_perf_cache_events_attrs[] = {\n\t&fme_perf_event_cache_read_hit.attr.attr,\n\t&fme_perf_event_cache_read_miss.attr.attr,\n\t&fme_perf_event_cache_write_hit.attr.attr,\n\t&fme_perf_event_cache_write_miss.attr.attr,\n\t&fme_perf_event_cache_hold_request.attr.attr,\n\t&fme_perf_event_cache_tx_req_stall.attr.attr,\n\t&fme_perf_event_cache_rx_req_stall.attr.attr,\n\t&fme_perf_event_cache_eviction.attr.attr,\n\t&fme_perf_event_cache_data_write_port_contention.attr.attr,\n\t&fme_perf_event_cache_tag_write_port_contention.attr.attr,\n\tNULL,\n};\n\nstatic umode_t fme_perf_events_visible(struct kobject *kobj,\n\t\t\t\t       struct attribute *attr, int n)\n{\n\tstruct pmu *pmu = dev_get_drvdata(kobj_to_dev(kobj));\n\tstruct fme_perf_priv *priv = to_fme_perf_priv(pmu);\n\n\treturn (priv->id == FME_FEATURE_ID_GLOBAL_IPERF) ? attr->mode : 0;\n}\n\nstatic const struct attribute_group fme_perf_cache_events_group = {\n\t.name = \"events\",\n\t.attrs = fme_perf_cache_events_attrs,\n\t.is_visible = fme_perf_events_visible,\n};\n\n \n#define FME_EVENT_FABRIC(_name, _event)\t\t\t\t\t\\\nstatic struct dev_ext_attribute fme_perf_event_fab_##_name = {\t\t\\\n\t.attr = FME_EVENT_ATTR(fab_##_name),\t\t\t\t\\\n\t.var = FME_EVENT_CONFIG(_event, FME_EVTYPE_FABRIC),\t\t\\\n}\n\n#define FME_EVENT_FABRIC_PORT(_name, _event)\t\t\t\t\\\nstatic struct dev_ext_attribute fme_perf_event_fab_port_##_name = {\t\\\n\t.attr = FME_EVENT_ATTR(fab_port_##_name),\t\t\t\\\n\t.var = FME_PORT_EVENT_CONFIG(_event, FME_EVTYPE_FABRIC),\t\\\n}\n\nFME_EVENT_FABRIC(pcie0_read,  FAB_EVNT_PCIE0_RD);\nFME_EVENT_FABRIC(pcie0_write, FAB_EVNT_PCIE0_WR);\nFME_EVENT_FABRIC(pcie1_read,  FAB_EVNT_PCIE1_RD);\nFME_EVENT_FABRIC(pcie1_write, FAB_EVNT_PCIE1_WR);\nFME_EVENT_FABRIC(upi_read,    FAB_EVNT_UPI_RD);\nFME_EVENT_FABRIC(upi_write,   FAB_EVNT_UPI_WR);\nFME_EVENT_FABRIC(mmio_read,   FAB_EVNT_MMIO_RD);\nFME_EVENT_FABRIC(mmio_write,  FAB_EVNT_MMIO_WR);\n\nFME_EVENT_FABRIC_PORT(pcie0_read,  FAB_EVNT_PCIE0_RD);\nFME_EVENT_FABRIC_PORT(pcie0_write, FAB_EVNT_PCIE0_WR);\nFME_EVENT_FABRIC_PORT(pcie1_read,  FAB_EVNT_PCIE1_RD);\nFME_EVENT_FABRIC_PORT(pcie1_write, FAB_EVNT_PCIE1_WR);\nFME_EVENT_FABRIC_PORT(upi_read,    FAB_EVNT_UPI_RD);\nFME_EVENT_FABRIC_PORT(upi_write,   FAB_EVNT_UPI_WR);\nFME_EVENT_FABRIC_PORT(mmio_read,   FAB_EVNT_MMIO_RD);\nFME_EVENT_FABRIC_PORT(mmio_write,  FAB_EVNT_MMIO_WR);\n\nstatic struct attribute *fme_perf_fabric_events_attrs[] = {\n\t&fme_perf_event_fab_pcie0_read.attr.attr,\n\t&fme_perf_event_fab_pcie0_write.attr.attr,\n\t&fme_perf_event_fab_pcie1_read.attr.attr,\n\t&fme_perf_event_fab_pcie1_write.attr.attr,\n\t&fme_perf_event_fab_upi_read.attr.attr,\n\t&fme_perf_event_fab_upi_write.attr.attr,\n\t&fme_perf_event_fab_mmio_read.attr.attr,\n\t&fme_perf_event_fab_mmio_write.attr.attr,\n\t&fme_perf_event_fab_port_pcie0_read.attr.attr,\n\t&fme_perf_event_fab_port_pcie0_write.attr.attr,\n\t&fme_perf_event_fab_port_pcie1_read.attr.attr,\n\t&fme_perf_event_fab_port_pcie1_write.attr.attr,\n\t&fme_perf_event_fab_port_upi_read.attr.attr,\n\t&fme_perf_event_fab_port_upi_write.attr.attr,\n\t&fme_perf_event_fab_port_mmio_read.attr.attr,\n\t&fme_perf_event_fab_port_mmio_write.attr.attr,\n\tNULL,\n};\n\nstatic umode_t fme_perf_fabric_events_visible(struct kobject *kobj,\n\t\t\t\t\t      struct attribute *attr, int n)\n{\n\tstruct pmu *pmu = dev_get_drvdata(kobj_to_dev(kobj));\n\tstruct fme_perf_priv *priv = to_fme_perf_priv(pmu);\n\tstruct dev_ext_attribute *eattr;\n\tunsigned long var;\n\n\teattr = container_of(attr, struct dev_ext_attribute, attr.attr);\n\tvar = (unsigned long)eattr->var;\n\n\tif (is_fabric_event_supported(priv, get_event(var), get_portid(var)))\n\t\treturn attr->mode;\n\n\treturn 0;\n}\n\nstatic const struct attribute_group fme_perf_fabric_events_group = {\n\t.name = \"events\",\n\t.attrs = fme_perf_fabric_events_attrs,\n\t.is_visible = fme_perf_fabric_events_visible,\n};\n\n \n#define FME_EVENT_VTD_PORT(_name, _event)\t\t\t\t\\\nstatic struct dev_ext_attribute fme_perf_event_vtd_port_##_name = {\t\\\n\t.attr = FME_EVENT_ATTR(vtd_port_##_name),\t\t\t\\\n\t.var = FME_PORT_EVENT_CONFIG(_event, FME_EVTYPE_VTD),\t\t\\\n}\n\nFME_EVENT_VTD_PORT(read_transaction,  VTD_EVNT_AFU_MEM_RD_TRANS);\nFME_EVENT_VTD_PORT(write_transaction, VTD_EVNT_AFU_MEM_WR_TRANS);\nFME_EVENT_VTD_PORT(devtlb_read_hit,   VTD_EVNT_AFU_DEVTLB_RD_HIT);\nFME_EVENT_VTD_PORT(devtlb_write_hit,  VTD_EVNT_AFU_DEVTLB_WR_HIT);\nFME_EVENT_VTD_PORT(devtlb_4k_fill,    VTD_EVNT_DEVTLB_4K_FILL);\nFME_EVENT_VTD_PORT(devtlb_2m_fill,    VTD_EVNT_DEVTLB_2M_FILL);\nFME_EVENT_VTD_PORT(devtlb_1g_fill,    VTD_EVNT_DEVTLB_1G_FILL);\n\nstatic struct attribute *fme_perf_vtd_events_attrs[] = {\n\t&fme_perf_event_vtd_port_read_transaction.attr.attr,\n\t&fme_perf_event_vtd_port_write_transaction.attr.attr,\n\t&fme_perf_event_vtd_port_devtlb_read_hit.attr.attr,\n\t&fme_perf_event_vtd_port_devtlb_write_hit.attr.attr,\n\t&fme_perf_event_vtd_port_devtlb_4k_fill.attr.attr,\n\t&fme_perf_event_vtd_port_devtlb_2m_fill.attr.attr,\n\t&fme_perf_event_vtd_port_devtlb_1g_fill.attr.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group fme_perf_vtd_events_group = {\n\t.name = \"events\",\n\t.attrs = fme_perf_vtd_events_attrs,\n\t.is_visible = fme_perf_events_visible,\n};\n\n \n#define FME_EVENT_VTD_SIP(_name, _event)\t\t\t\t\\\nstatic struct dev_ext_attribute fme_perf_event_vtd_sip_##_name = {\t\\\n\t.attr = FME_EVENT_ATTR(vtd_sip_##_name),\t\t\t\\\n\t.var = FME_EVENT_CONFIG(_event, FME_EVTYPE_VTD_SIP),\t\t\\\n}\n\nFME_EVENT_VTD_SIP(iotlb_4k_hit,  VTD_SIP_EVNT_IOTLB_4K_HIT);\nFME_EVENT_VTD_SIP(iotlb_2m_hit,  VTD_SIP_EVNT_IOTLB_2M_HIT);\nFME_EVENT_VTD_SIP(iotlb_1g_hit,  VTD_SIP_EVNT_IOTLB_1G_HIT);\nFME_EVENT_VTD_SIP(slpwc_l3_hit,  VTD_SIP_EVNT_SLPWC_L3_HIT);\nFME_EVENT_VTD_SIP(slpwc_l4_hit,  VTD_SIP_EVNT_SLPWC_L4_HIT);\nFME_EVENT_VTD_SIP(rcc_hit,       VTD_SIP_EVNT_RCC_HIT);\nFME_EVENT_VTD_SIP(iotlb_4k_miss, VTD_SIP_EVNT_IOTLB_4K_MISS);\nFME_EVENT_VTD_SIP(iotlb_2m_miss, VTD_SIP_EVNT_IOTLB_2M_MISS);\nFME_EVENT_VTD_SIP(iotlb_1g_miss, VTD_SIP_EVNT_IOTLB_1G_MISS);\nFME_EVENT_VTD_SIP(slpwc_l3_miss, VTD_SIP_EVNT_SLPWC_L3_MISS);\nFME_EVENT_VTD_SIP(slpwc_l4_miss, VTD_SIP_EVNT_SLPWC_L4_MISS);\nFME_EVENT_VTD_SIP(rcc_miss,      VTD_SIP_EVNT_RCC_MISS);\n\nstatic struct attribute *fme_perf_vtd_sip_events_attrs[] = {\n\t&fme_perf_event_vtd_sip_iotlb_4k_hit.attr.attr,\n\t&fme_perf_event_vtd_sip_iotlb_2m_hit.attr.attr,\n\t&fme_perf_event_vtd_sip_iotlb_1g_hit.attr.attr,\n\t&fme_perf_event_vtd_sip_slpwc_l3_hit.attr.attr,\n\t&fme_perf_event_vtd_sip_slpwc_l4_hit.attr.attr,\n\t&fme_perf_event_vtd_sip_rcc_hit.attr.attr,\n\t&fme_perf_event_vtd_sip_iotlb_4k_miss.attr.attr,\n\t&fme_perf_event_vtd_sip_iotlb_2m_miss.attr.attr,\n\t&fme_perf_event_vtd_sip_iotlb_1g_miss.attr.attr,\n\t&fme_perf_event_vtd_sip_slpwc_l3_miss.attr.attr,\n\t&fme_perf_event_vtd_sip_slpwc_l4_miss.attr.attr,\n\t&fme_perf_event_vtd_sip_rcc_miss.attr.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group fme_perf_vtd_sip_events_group = {\n\t.name = \"events\",\n\t.attrs = fme_perf_vtd_sip_events_attrs,\n\t.is_visible = fme_perf_events_visible,\n};\n\nstatic const struct attribute_group *fme_perf_events_groups[] = {\n\t&fme_perf_basic_events_group,\n\t&fme_perf_cache_events_group,\n\t&fme_perf_fabric_events_group,\n\t&fme_perf_vtd_events_group,\n\t&fme_perf_vtd_sip_events_group,\n\tNULL,\n};\n\nstatic struct fme_perf_event_ops *get_event_ops(u32 evtype)\n{\n\tif (evtype > FME_EVTYPE_MAX)\n\t\treturn NULL;\n\n\treturn &fme_perf_event_ops[evtype];\n}\n\nstatic void fme_perf_event_destroy(struct perf_event *event)\n{\n\tstruct fme_perf_event_ops *ops = get_event_ops(event->hw.event_base);\n\tstruct fme_perf_priv *priv = to_fme_perf_priv(event->pmu);\n\n\tif (ops->event_destroy)\n\t\tops->event_destroy(priv, event->hw.idx, event->hw.config_base);\n}\n\nstatic int fme_perf_event_init(struct perf_event *event)\n{\n\tstruct fme_perf_priv *priv = to_fme_perf_priv(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct fme_perf_event_ops *ops;\n\tu32 eventid, evtype, portid;\n\n\t \n\tif (event->attr.type != event->pmu->type)\n\t\treturn -ENOENT;\n\n\t \n\tif (is_sampling_event(event) || event->attach_state & PERF_ATTACH_TASK)\n\t\treturn -EINVAL;\n\n\tif (event->cpu < 0)\n\t\treturn -EINVAL;\n\n\tif (event->cpu != priv->cpu)\n\t\treturn -EINVAL;\n\n\teventid = get_event(event->attr.config);\n\tportid = get_portid(event->attr.config);\n\tevtype = get_evtype(event->attr.config);\n\tif (evtype > FME_EVTYPE_MAX)\n\t\treturn -EINVAL;\n\n\thwc->event_base = evtype;\n\thwc->idx = (int)eventid;\n\thwc->config_base = portid;\n\n\tevent->destroy = fme_perf_event_destroy;\n\n\tdev_dbg(priv->dev, \"%s event=0x%x, evtype=0x%x, portid=0x%x,\\n\",\n\t\t__func__, eventid, evtype, portid);\n\n\tops = get_event_ops(evtype);\n\tif (ops->event_init)\n\t\treturn ops->event_init(priv, eventid, portid);\n\n\treturn 0;\n}\n\nstatic void fme_perf_event_update(struct perf_event *event)\n{\n\tstruct fme_perf_event_ops *ops = get_event_ops(event->hw.event_base);\n\tstruct fme_perf_priv *priv = to_fme_perf_priv(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 now, prev, delta;\n\n\tnow = ops->read_counter(priv, (u32)hwc->idx, hwc->config_base);\n\tprev = local64_read(&hwc->prev_count);\n\tdelta = now - prev;\n\n\tlocal64_add(delta, &event->count);\n}\n\nstatic void fme_perf_event_start(struct perf_event *event, int flags)\n{\n\tstruct fme_perf_event_ops *ops = get_event_ops(event->hw.event_base);\n\tstruct fme_perf_priv *priv = to_fme_perf_priv(event->pmu);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 count;\n\n\tcount = ops->read_counter(priv, (u32)hwc->idx, hwc->config_base);\n\tlocal64_set(&hwc->prev_count, count);\n}\n\nstatic void fme_perf_event_stop(struct perf_event *event, int flags)\n{\n\tfme_perf_event_update(event);\n}\n\nstatic int fme_perf_event_add(struct perf_event *event, int flags)\n{\n\tif (flags & PERF_EF_START)\n\t\tfme_perf_event_start(event, flags);\n\n\treturn 0;\n}\n\nstatic void fme_perf_event_del(struct perf_event *event, int flags)\n{\n\tfme_perf_event_stop(event, PERF_EF_UPDATE);\n}\n\nstatic void fme_perf_event_read(struct perf_event *event)\n{\n\tfme_perf_event_update(event);\n}\n\nstatic void fme_perf_setup_hardware(struct fme_perf_priv *priv)\n{\n\tvoid __iomem *base = priv->ioaddr;\n\tu64 v;\n\n\t \n\tv = readq(base + FAB_CTRL);\n\n\tif (FIELD_GET(FAB_PORT_FILTER, v) == FAB_PORT_FILTER_DISABLE)\n\t\tpriv->fab_port_id = FME_PORTID_ROOT;\n\telse\n\t\tpriv->fab_port_id = FIELD_GET(FAB_PORT_ID, v);\n}\n\nstatic int fme_perf_pmu_register(struct platform_device *pdev,\n\t\t\t\t struct fme_perf_priv *priv)\n{\n\tstruct pmu *pmu = &priv->pmu;\n\tchar *name;\n\tint ret;\n\n\tspin_lock_init(&priv->fab_lock);\n\n\tfme_perf_setup_hardware(priv);\n\n\tpmu->task_ctx_nr =\tperf_invalid_context;\n\tpmu->attr_groups =\tfme_perf_groups;\n\tpmu->attr_update =\tfme_perf_events_groups;\n\tpmu->event_init =\tfme_perf_event_init;\n\tpmu->add =\t\tfme_perf_event_add;\n\tpmu->del =\t\tfme_perf_event_del;\n\tpmu->start =\t\tfme_perf_event_start;\n\tpmu->stop =\t\tfme_perf_event_stop;\n\tpmu->read =\t\tfme_perf_event_read;\n\tpmu->capabilities =\tPERF_PMU_CAP_NO_INTERRUPT |\n\t\t\t\tPERF_PMU_CAP_NO_EXCLUDE;\n\n\tname = devm_kasprintf(priv->dev, GFP_KERNEL, \"dfl_fme%d\", pdev->id);\n\n\tret = perf_pmu_register(pmu, name, -1);\n\tif (ret)\n\t\treturn ret;\n\n\treturn 0;\n}\n\nstatic void fme_perf_pmu_unregister(struct fme_perf_priv *priv)\n{\n\tperf_pmu_unregister(&priv->pmu);\n}\n\nstatic int fme_perf_offline_cpu(unsigned int cpu, struct hlist_node *node)\n{\n\tstruct fme_perf_priv *priv;\n\tint target;\n\n\tpriv = hlist_entry_safe(node, struct fme_perf_priv, node);\n\n\tif (cpu != priv->cpu)\n\t\treturn 0;\n\n\ttarget = cpumask_any_but(cpu_online_mask, cpu);\n\tif (target >= nr_cpu_ids)\n\t\treturn 0;\n\n\tpriv->cpu = target;\n\tperf_pmu_migrate_context(&priv->pmu, cpu, target);\n\n\treturn 0;\n}\n\nstatic int fme_perf_init(struct platform_device *pdev,\n\t\t\t struct dfl_feature *feature)\n{\n\tstruct fme_perf_priv *priv;\n\tint ret;\n\n\tpriv = devm_kzalloc(&pdev->dev, sizeof(*priv), GFP_KERNEL);\n\tif (!priv)\n\t\treturn -ENOMEM;\n\n\tpriv->dev = &pdev->dev;\n\tpriv->ioaddr = feature->ioaddr;\n\tpriv->id = feature->id;\n\tpriv->cpu = raw_smp_processor_id();\n\n\tret = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN,\n\t\t\t\t      \"perf/fpga/dfl_fme:online\",\n\t\t\t\t      NULL, fme_perf_offline_cpu);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tpriv->cpuhp_state = ret;\n\n\t \n\tret = cpuhp_state_add_instance_nocalls(priv->cpuhp_state, &priv->node);\n\tif (ret)\n\t\tgoto cpuhp_instance_err;\n\n\tret = fme_perf_pmu_register(pdev, priv);\n\tif (ret)\n\t\tgoto pmu_register_err;\n\n\tfeature->priv = priv;\n\treturn 0;\n\npmu_register_err:\n\tcpuhp_state_remove_instance_nocalls(priv->cpuhp_state, &priv->node);\ncpuhp_instance_err:\n\tcpuhp_remove_multi_state(priv->cpuhp_state);\n\treturn ret;\n}\n\nstatic void fme_perf_uinit(struct platform_device *pdev,\n\t\t\t   struct dfl_feature *feature)\n{\n\tstruct fme_perf_priv *priv = feature->priv;\n\n\tfme_perf_pmu_unregister(priv);\n\tcpuhp_state_remove_instance_nocalls(priv->cpuhp_state, &priv->node);\n\tcpuhp_remove_multi_state(priv->cpuhp_state);\n}\n\nconst struct dfl_feature_id fme_perf_id_table[] = {\n\t{.id = FME_FEATURE_ID_GLOBAL_IPERF,},\n\t{.id = FME_FEATURE_ID_GLOBAL_DPERF,},\n\t{0,}\n};\n\nconst struct dfl_feature_ops fme_perf_ops = {\n\t.init = fme_perf_init,\n\t.uinit = fme_perf_uinit,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}