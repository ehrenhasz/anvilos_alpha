{
  "module_name": "zns.c",
  "hash_id": "de90ec63c9413fd98b77f7dd6d471d10dfee7673119bad84454b145a4ce168b6",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvme/target/zns.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n#include <linux/nvme.h>\n#include <linux/blkdev.h>\n#include \"nvmet.h\"\n\n \n#define NVMET_MPSMIN_SHIFT\t12\n\nstatic inline u8 nvmet_zasl(unsigned int zone_append_sects)\n{\n\t \n\treturn ilog2(zone_append_sects >> (NVMET_MPSMIN_SHIFT - 9));\n}\n\nstatic int validate_conv_zones_cb(struct blk_zone *z,\n\t\t\t\t  unsigned int i, void *data)\n{\n\tif (z->type == BLK_ZONE_TYPE_CONVENTIONAL)\n\t\treturn -EOPNOTSUPP;\n\treturn 0;\n}\n\nbool nvmet_bdev_zns_enable(struct nvmet_ns *ns)\n{\n\tu8 zasl = nvmet_zasl(bdev_max_zone_append_sectors(ns->bdev));\n\tstruct gendisk *bd_disk = ns->bdev->bd_disk;\n\tint ret;\n\n\tif (ns->subsys->zasl) {\n\t\tif (ns->subsys->zasl > zasl)\n\t\t\treturn false;\n\t}\n\tns->subsys->zasl = zasl;\n\n\t \n\tif (get_capacity(bd_disk) & (bdev_zone_sectors(ns->bdev) - 1))\n\t\treturn false;\n\t \n\tif (ns->bdev->bd_disk->conv_zones_bitmap)\n\t\treturn false;\n\n\tret = blkdev_report_zones(ns->bdev, 0, bdev_nr_zones(ns->bdev),\n\t\t\t\t  validate_conv_zones_cb, NULL);\n\tif (ret < 0)\n\t\treturn false;\n\n\tns->blksize_shift = blksize_bits(bdev_logical_block_size(ns->bdev));\n\n\treturn true;\n}\n\nvoid nvmet_execute_identify_ctrl_zns(struct nvmet_req *req)\n{\n\tu8 zasl = req->sq->ctrl->subsys->zasl;\n\tstruct nvmet_ctrl *ctrl = req->sq->ctrl;\n\tstruct nvme_id_ctrl_zns *id;\n\tu16 status;\n\n\tid = kzalloc(sizeof(*id), GFP_KERNEL);\n\tif (!id) {\n\t\tstatus = NVME_SC_INTERNAL;\n\t\tgoto out;\n\t}\n\n\tif (ctrl->ops->get_mdts)\n\t\tid->zasl = min_t(u8, ctrl->ops->get_mdts(ctrl), zasl);\n\telse\n\t\tid->zasl = zasl;\n\n\tstatus = nvmet_copy_to_sgl(req, 0, id, sizeof(*id));\n\n\tkfree(id);\nout:\n\tnvmet_req_complete(req, status);\n}\n\nvoid nvmet_execute_identify_ns_zns(struct nvmet_req *req)\n{\n\tstruct nvme_id_ns_zns *id_zns = NULL;\n\tu64 zsze;\n\tu16 status;\n\tu32 mar, mor;\n\n\tif (le32_to_cpu(req->cmd->identify.nsid) == NVME_NSID_ALL) {\n\t\treq->error_loc = offsetof(struct nvme_identify, nsid);\n\t\tstatus = NVME_SC_INVALID_NS | NVME_SC_DNR;\n\t\tgoto out;\n\t}\n\n\tid_zns = kzalloc(sizeof(*id_zns), GFP_KERNEL);\n\tif (!id_zns) {\n\t\tstatus = NVME_SC_INTERNAL;\n\t\tgoto out;\n\t}\n\n\tstatus = nvmet_req_find_ns(req);\n\tif (status)\n\t\tgoto done;\n\n\tif (nvmet_ns_revalidate(req->ns)) {\n\t\tmutex_lock(&req->ns->subsys->lock);\n\t\tnvmet_ns_changed(req->ns->subsys, req->ns->nsid);\n\t\tmutex_unlock(&req->ns->subsys->lock);\n\t}\n\n\tif (!bdev_is_zoned(req->ns->bdev)) {\n\t\tstatus = NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t\treq->error_loc = offsetof(struct nvme_identify, nsid);\n\t\tgoto out;\n\t}\n\n\tzsze = (bdev_zone_sectors(req->ns->bdev) << 9) >>\n\t\t\t\t\treq->ns->blksize_shift;\n\tid_zns->lbafe[0].zsze = cpu_to_le64(zsze);\n\n\tmor = bdev_max_open_zones(req->ns->bdev);\n\tif (!mor)\n\t\tmor = U32_MAX;\n\telse\n\t\tmor--;\n\tid_zns->mor = cpu_to_le32(mor);\n\n\tmar = bdev_max_active_zones(req->ns->bdev);\n\tif (!mar)\n\t\tmar = U32_MAX;\n\telse\n\t\tmar--;\n\tid_zns->mar = cpu_to_le32(mar);\n\ndone:\n\tstatus = nvmet_copy_to_sgl(req, 0, id_zns, sizeof(*id_zns));\nout:\n\tkfree(id_zns);\n\tnvmet_req_complete(req, status);\n}\n\nstatic u16 nvmet_bdev_validate_zone_mgmt_recv(struct nvmet_req *req)\n{\n\tsector_t sect = nvmet_lba_to_sect(req->ns, req->cmd->zmr.slba);\n\tu32 out_bufsize = (le32_to_cpu(req->cmd->zmr.numd) + 1) << 2;\n\n\tif (sect >= get_capacity(req->ns->bdev->bd_disk)) {\n\t\treq->error_loc = offsetof(struct nvme_zone_mgmt_recv_cmd, slba);\n\t\treturn NVME_SC_LBA_RANGE | NVME_SC_DNR;\n\t}\n\n\tif (out_bufsize < sizeof(struct nvme_zone_report)) {\n\t\treq->error_loc = offsetof(struct nvme_zone_mgmt_recv_cmd, numd);\n\t\treturn NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t}\n\n\tif (req->cmd->zmr.zra != NVME_ZRA_ZONE_REPORT) {\n\t\treq->error_loc = offsetof(struct nvme_zone_mgmt_recv_cmd, zra);\n\t\treturn NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t}\n\n\tswitch (req->cmd->zmr.pr) {\n\tcase 0:\n\tcase 1:\n\t\tbreak;\n\tdefault:\n\t\treq->error_loc = offsetof(struct nvme_zone_mgmt_recv_cmd, pr);\n\t\treturn NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t}\n\n\tswitch (req->cmd->zmr.zrasf) {\n\tcase NVME_ZRASF_ZONE_REPORT_ALL:\n\tcase NVME_ZRASF_ZONE_STATE_EMPTY:\n\tcase NVME_ZRASF_ZONE_STATE_IMP_OPEN:\n\tcase NVME_ZRASF_ZONE_STATE_EXP_OPEN:\n\tcase NVME_ZRASF_ZONE_STATE_CLOSED:\n\tcase NVME_ZRASF_ZONE_STATE_FULL:\n\tcase NVME_ZRASF_ZONE_STATE_READONLY:\n\tcase NVME_ZRASF_ZONE_STATE_OFFLINE:\n\t\tbreak;\n\tdefault:\n\t\treq->error_loc =\n\t\t\toffsetof(struct nvme_zone_mgmt_recv_cmd, zrasf);\n\t\treturn NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t}\n\n\treturn NVME_SC_SUCCESS;\n}\n\nstruct nvmet_report_zone_data {\n\tstruct nvmet_req *req;\n\tu64 out_buf_offset;\n\tu64 out_nr_zones;\n\tu64 nr_zones;\n\tu8 zrasf;\n};\n\nstatic int nvmet_bdev_report_zone_cb(struct blk_zone *z, unsigned i, void *d)\n{\n\tstatic const unsigned int nvme_zrasf_to_blk_zcond[] = {\n\t\t[NVME_ZRASF_ZONE_STATE_EMPTY]\t = BLK_ZONE_COND_EMPTY,\n\t\t[NVME_ZRASF_ZONE_STATE_IMP_OPEN] = BLK_ZONE_COND_IMP_OPEN,\n\t\t[NVME_ZRASF_ZONE_STATE_EXP_OPEN] = BLK_ZONE_COND_EXP_OPEN,\n\t\t[NVME_ZRASF_ZONE_STATE_CLOSED]\t = BLK_ZONE_COND_CLOSED,\n\t\t[NVME_ZRASF_ZONE_STATE_READONLY] = BLK_ZONE_COND_READONLY,\n\t\t[NVME_ZRASF_ZONE_STATE_FULL]\t = BLK_ZONE_COND_FULL,\n\t\t[NVME_ZRASF_ZONE_STATE_OFFLINE]\t = BLK_ZONE_COND_OFFLINE,\n\t};\n\tstruct nvmet_report_zone_data *rz = d;\n\n\tif (rz->zrasf != NVME_ZRASF_ZONE_REPORT_ALL &&\n\t    z->cond != nvme_zrasf_to_blk_zcond[rz->zrasf])\n\t\treturn 0;\n\n\tif (rz->nr_zones < rz->out_nr_zones) {\n\t\tstruct nvme_zone_descriptor zdesc = { };\n\t\tu16 status;\n\n\t\tzdesc.zcap = nvmet_sect_to_lba(rz->req->ns, z->capacity);\n\t\tzdesc.zslba = nvmet_sect_to_lba(rz->req->ns, z->start);\n\t\tzdesc.wp = nvmet_sect_to_lba(rz->req->ns, z->wp);\n\t\tzdesc.za = z->reset ? 1 << 2 : 0;\n\t\tzdesc.zs = z->cond << 4;\n\t\tzdesc.zt = z->type;\n\n\t\tstatus = nvmet_copy_to_sgl(rz->req, rz->out_buf_offset, &zdesc,\n\t\t\t\t\t   sizeof(zdesc));\n\t\tif (status)\n\t\t\treturn -EINVAL;\n\n\t\trz->out_buf_offset += sizeof(zdesc);\n\t}\n\n\trz->nr_zones++;\n\n\treturn 0;\n}\n\nstatic unsigned long nvmet_req_nr_zones_from_slba(struct nvmet_req *req)\n{\n\tunsigned int sect = nvmet_lba_to_sect(req->ns, req->cmd->zmr.slba);\n\n\treturn bdev_nr_zones(req->ns->bdev) - bdev_zone_no(req->ns->bdev, sect);\n}\n\nstatic unsigned long get_nr_zones_from_buf(struct nvmet_req *req, u32 bufsize)\n{\n\tif (bufsize <= sizeof(struct nvme_zone_report))\n\t\treturn 0;\n\n\treturn (bufsize - sizeof(struct nvme_zone_report)) /\n\t\tsizeof(struct nvme_zone_descriptor);\n}\n\nstatic void nvmet_bdev_zone_zmgmt_recv_work(struct work_struct *w)\n{\n\tstruct nvmet_req *req = container_of(w, struct nvmet_req, z.zmgmt_work);\n\tsector_t start_sect = nvmet_lba_to_sect(req->ns, req->cmd->zmr.slba);\n\tunsigned long req_slba_nr_zones = nvmet_req_nr_zones_from_slba(req);\n\tu32 out_bufsize = (le32_to_cpu(req->cmd->zmr.numd) + 1) << 2;\n\t__le64 nr_zones;\n\tu16 status;\n\tint ret;\n\tstruct nvmet_report_zone_data rz_data = {\n\t\t.out_nr_zones = get_nr_zones_from_buf(req, out_bufsize),\n\t\t \n\t\t.out_buf_offset = sizeof(struct nvme_zone_report),\n\t\t.zrasf = req->cmd->zmr.zrasf,\n\t\t.nr_zones = 0,\n\t\t.req = req,\n\t};\n\n\tstatus = nvmet_bdev_validate_zone_mgmt_recv(req);\n\tif (status)\n\t\tgoto out;\n\n\tif (!req_slba_nr_zones) {\n\t\tstatus = NVME_SC_SUCCESS;\n\t\tgoto out;\n\t}\n\n\tret = blkdev_report_zones(req->ns->bdev, start_sect, req_slba_nr_zones,\n\t\t\t\t nvmet_bdev_report_zone_cb, &rz_data);\n\tif (ret < 0) {\n\t\tstatus = NVME_SC_INTERNAL;\n\t\tgoto out;\n\t}\n\n\t \n\tif (req->cmd->zmr.pr)\n\t\trz_data.nr_zones = min(rz_data.nr_zones, rz_data.out_nr_zones);\n\n\tnr_zones = cpu_to_le64(rz_data.nr_zones);\n\tstatus = nvmet_copy_to_sgl(req, 0, &nr_zones, sizeof(nr_zones));\n\nout:\n\tnvmet_req_complete(req, status);\n}\n\nvoid nvmet_bdev_execute_zone_mgmt_recv(struct nvmet_req *req)\n{\n\tINIT_WORK(&req->z.zmgmt_work, nvmet_bdev_zone_zmgmt_recv_work);\n\tqueue_work(zbd_wq, &req->z.zmgmt_work);\n}\n\nstatic inline enum req_op zsa_req_op(u8 zsa)\n{\n\tswitch (zsa) {\n\tcase NVME_ZONE_OPEN:\n\t\treturn REQ_OP_ZONE_OPEN;\n\tcase NVME_ZONE_CLOSE:\n\t\treturn REQ_OP_ZONE_CLOSE;\n\tcase NVME_ZONE_FINISH:\n\t\treturn REQ_OP_ZONE_FINISH;\n\tcase NVME_ZONE_RESET:\n\t\treturn REQ_OP_ZONE_RESET;\n\tdefault:\n\t\treturn REQ_OP_LAST;\n\t}\n}\n\nstatic u16 blkdev_zone_mgmt_errno_to_nvme_status(int ret)\n{\n\tswitch (ret) {\n\tcase 0:\n\t\treturn NVME_SC_SUCCESS;\n\tcase -EINVAL:\n\tcase -EIO:\n\t\treturn NVME_SC_ZONE_INVALID_TRANSITION | NVME_SC_DNR;\n\tdefault:\n\t\treturn NVME_SC_INTERNAL;\n\t}\n}\n\nstruct nvmet_zone_mgmt_send_all_data {\n\tunsigned long *zbitmap;\n\tstruct nvmet_req *req;\n};\n\nstatic int zmgmt_send_scan_cb(struct blk_zone *z, unsigned i, void *d)\n{\n\tstruct nvmet_zone_mgmt_send_all_data *data = d;\n\n\tswitch (zsa_req_op(data->req->cmd->zms.zsa)) {\n\tcase REQ_OP_ZONE_OPEN:\n\t\tswitch (z->cond) {\n\t\tcase BLK_ZONE_COND_CLOSED:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t\tbreak;\n\tcase REQ_OP_ZONE_CLOSE:\n\t\tswitch (z->cond) {\n\t\tcase BLK_ZONE_COND_IMP_OPEN:\n\t\tcase BLK_ZONE_COND_EXP_OPEN:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t\tbreak;\n\tcase REQ_OP_ZONE_FINISH:\n\t\tswitch (z->cond) {\n\t\tcase BLK_ZONE_COND_IMP_OPEN:\n\t\tcase BLK_ZONE_COND_EXP_OPEN:\n\t\tcase BLK_ZONE_COND_CLOSED:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn 0;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tset_bit(i, data->zbitmap);\n\n\treturn 0;\n}\n\nstatic u16 nvmet_bdev_zone_mgmt_emulate_all(struct nvmet_req *req)\n{\n\tstruct block_device *bdev = req->ns->bdev;\n\tunsigned int nr_zones = bdev_nr_zones(bdev);\n\tstruct bio *bio = NULL;\n\tsector_t sector = 0;\n\tint ret;\n\tstruct nvmet_zone_mgmt_send_all_data d = {\n\t\t.req = req,\n\t};\n\n\td.zbitmap = kcalloc_node(BITS_TO_LONGS(nr_zones), sizeof(*(d.zbitmap)),\n\t\t\t\t GFP_NOIO, bdev->bd_disk->node_id);\n\tif (!d.zbitmap) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t \n\tret = blkdev_report_zones(bdev, 0, nr_zones, zmgmt_send_scan_cb, &d);\n\tif (ret != nr_zones) {\n\t\tif (ret > 0)\n\t\t\tret = -EIO;\n\t\tgoto out;\n\t} else {\n\t\t \n\t\tret = 0;\n\t}\n\n\twhile (sector < bdev_nr_sectors(bdev)) {\n\t\tif (test_bit(disk_zone_no(bdev->bd_disk, sector), d.zbitmap)) {\n\t\t\tbio = blk_next_bio(bio, bdev, 0,\n\t\t\t\tzsa_req_op(req->cmd->zms.zsa) | REQ_SYNC,\n\t\t\t\tGFP_KERNEL);\n\t\t\tbio->bi_iter.bi_sector = sector;\n\t\t\t \n\t\t\tcond_resched();\n\t\t}\n\t\tsector += bdev_zone_sectors(bdev);\n\t}\n\n\tif (bio) {\n\t\tret = submit_bio_wait(bio);\n\t\tbio_put(bio);\n\t}\n\nout:\n\tkfree(d.zbitmap);\n\n\treturn blkdev_zone_mgmt_errno_to_nvme_status(ret);\n}\n\nstatic u16 nvmet_bdev_execute_zmgmt_send_all(struct nvmet_req *req)\n{\n\tint ret;\n\n\tswitch (zsa_req_op(req->cmd->zms.zsa)) {\n\tcase REQ_OP_ZONE_RESET:\n\t\tret = blkdev_zone_mgmt(req->ns->bdev, REQ_OP_ZONE_RESET, 0,\n\t\t\t\t       get_capacity(req->ns->bdev->bd_disk),\n\t\t\t\t       GFP_KERNEL);\n\t\tif (ret < 0)\n\t\t\treturn blkdev_zone_mgmt_errno_to_nvme_status(ret);\n\t\tbreak;\n\tcase REQ_OP_ZONE_OPEN:\n\tcase REQ_OP_ZONE_CLOSE:\n\tcase REQ_OP_ZONE_FINISH:\n\t\treturn nvmet_bdev_zone_mgmt_emulate_all(req);\n\tdefault:\n\t\t \n\t\treq->error_loc = offsetof(struct nvme_zone_mgmt_send_cmd, zsa);\n\t\treturn NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t}\n\n\treturn NVME_SC_SUCCESS;\n}\n\nstatic void nvmet_bdev_zmgmt_send_work(struct work_struct *w)\n{\n\tstruct nvmet_req *req = container_of(w, struct nvmet_req, z.zmgmt_work);\n\tsector_t sect = nvmet_lba_to_sect(req->ns, req->cmd->zms.slba);\n\tenum req_op op = zsa_req_op(req->cmd->zms.zsa);\n\tstruct block_device *bdev = req->ns->bdev;\n\tsector_t zone_sectors = bdev_zone_sectors(bdev);\n\tu16 status = NVME_SC_SUCCESS;\n\tint ret;\n\n\tif (op == REQ_OP_LAST) {\n\t\treq->error_loc = offsetof(struct nvme_zone_mgmt_send_cmd, zsa);\n\t\tstatus = NVME_SC_ZONE_INVALID_TRANSITION | NVME_SC_DNR;\n\t\tgoto out;\n\t}\n\n\t \n\tif (req->cmd->zms.select_all) {\n\t\tstatus = nvmet_bdev_execute_zmgmt_send_all(req);\n\t\tgoto out;\n\t}\n\n\tif (sect >= get_capacity(bdev->bd_disk)) {\n\t\treq->error_loc = offsetof(struct nvme_zone_mgmt_send_cmd, slba);\n\t\tstatus = NVME_SC_LBA_RANGE | NVME_SC_DNR;\n\t\tgoto out;\n\t}\n\n\tif (sect & (zone_sectors - 1)) {\n\t\treq->error_loc = offsetof(struct nvme_zone_mgmt_send_cmd, slba);\n\t\tstatus = NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t\tgoto out;\n\t}\n\n\tret = blkdev_zone_mgmt(bdev, op, sect, zone_sectors, GFP_KERNEL);\n\tif (ret < 0)\n\t\tstatus = blkdev_zone_mgmt_errno_to_nvme_status(ret);\n\nout:\n\tnvmet_req_complete(req, status);\n}\n\nvoid nvmet_bdev_execute_zone_mgmt_send(struct nvmet_req *req)\n{\n\tINIT_WORK(&req->z.zmgmt_work, nvmet_bdev_zmgmt_send_work);\n\tqueue_work(zbd_wq, &req->z.zmgmt_work);\n}\n\nstatic void nvmet_bdev_zone_append_bio_done(struct bio *bio)\n{\n\tstruct nvmet_req *req = bio->bi_private;\n\n\tif (bio->bi_status == BLK_STS_OK) {\n\t\treq->cqe->result.u64 =\n\t\t\tnvmet_sect_to_lba(req->ns, bio->bi_iter.bi_sector);\n\t}\n\n\tnvmet_req_complete(req, blk_to_nvme_status(req, bio->bi_status));\n\tnvmet_req_bio_put(req, bio);\n}\n\nvoid nvmet_bdev_execute_zone_append(struct nvmet_req *req)\n{\n\tsector_t sect = nvmet_lba_to_sect(req->ns, req->cmd->rw.slba);\n\tconst blk_opf_t opf = REQ_OP_ZONE_APPEND | REQ_SYNC | REQ_IDLE;\n\tu16 status = NVME_SC_SUCCESS;\n\tunsigned int total_len = 0;\n\tstruct scatterlist *sg;\n\tstruct bio *bio;\n\tint sg_cnt;\n\n\t \n\tif (!nvmet_check_transfer_len(req, nvmet_rw_data_len(req)))\n\t\treturn;\n\n\tif (!req->sg_cnt) {\n\t\tnvmet_req_complete(req, 0);\n\t\treturn;\n\t}\n\n\tif (sect >= get_capacity(req->ns->bdev->bd_disk)) {\n\t\treq->error_loc = offsetof(struct nvme_rw_command, slba);\n\t\tstatus = NVME_SC_LBA_RANGE | NVME_SC_DNR;\n\t\tgoto out;\n\t}\n\n\tif (sect & (bdev_zone_sectors(req->ns->bdev) - 1)) {\n\t\treq->error_loc = offsetof(struct nvme_rw_command, slba);\n\t\tstatus = NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t\tgoto out;\n\t}\n\n\tif (nvmet_use_inline_bvec(req)) {\n\t\tbio = &req->z.inline_bio;\n\t\tbio_init(bio, req->ns->bdev, req->inline_bvec,\n\t\t\t ARRAY_SIZE(req->inline_bvec), opf);\n\t} else {\n\t\tbio = bio_alloc(req->ns->bdev, req->sg_cnt, opf, GFP_KERNEL);\n\t}\n\n\tbio->bi_end_io = nvmet_bdev_zone_append_bio_done;\n\tbio->bi_iter.bi_sector = sect;\n\tbio->bi_private = req;\n\tif (req->cmd->rw.control & cpu_to_le16(NVME_RW_FUA))\n\t\tbio->bi_opf |= REQ_FUA;\n\n\tfor_each_sg(req->sg, sg, req->sg_cnt, sg_cnt) {\n\t\tstruct page *p = sg_page(sg);\n\t\tunsigned int l = sg->length;\n\t\tunsigned int o = sg->offset;\n\t\tunsigned int ret;\n\n\t\tret = bio_add_zone_append_page(bio, p, l, o);\n\t\tif (ret != sg->length) {\n\t\t\tstatus = NVME_SC_INTERNAL;\n\t\t\tgoto out_put_bio;\n\t\t}\n\t\ttotal_len += sg->length;\n\t}\n\n\tif (total_len != nvmet_rw_data_len(req)) {\n\t\tstatus = NVME_SC_INTERNAL | NVME_SC_DNR;\n\t\tgoto out_put_bio;\n\t}\n\n\tsubmit_bio(bio);\n\treturn;\n\nout_put_bio:\n\tnvmet_req_bio_put(req, bio);\nout:\n\tnvmet_req_complete(req, status);\n}\n\nu16 nvmet_bdev_zns_parse_io_cmd(struct nvmet_req *req)\n{\n\tstruct nvme_command *cmd = req->cmd;\n\n\tswitch (cmd->common.opcode) {\n\tcase nvme_cmd_zone_append:\n\t\treq->execute = nvmet_bdev_execute_zone_append;\n\t\treturn 0;\n\tcase nvme_cmd_zone_mgmt_recv:\n\t\treq->execute = nvmet_bdev_execute_zone_mgmt_recv;\n\t\treturn 0;\n\tcase nvme_cmd_zone_mgmt_send:\n\t\treq->execute = nvmet_bdev_execute_zone_mgmt_send;\n\t\treturn 0;\n\tdefault:\n\t\treturn nvmet_bdev_parse_io_cmd(req);\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}