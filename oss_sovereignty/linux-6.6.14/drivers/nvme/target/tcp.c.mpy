{
  "module_name": "tcp.c",
  "hash_id": "cd199ac9aae5aa88d2e7fd59b74caa8f34509ebcb7e2afe1b3f4468b8c7c370a",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvme/target/tcp.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/nvme-tcp.h>\n#include <net/sock.h>\n#include <net/tcp.h>\n#include <linux/inet.h>\n#include <linux/llist.h>\n#include <crypto/hash.h>\n#include <trace/events/sock.h>\n\n#include \"nvmet.h\"\n\n#define NVMET_TCP_DEF_INLINE_DATA_SIZE\t(4 * PAGE_SIZE)\n#define NVMET_TCP_MAXH2CDATA\t\t0x400000  \n\nstatic int param_store_val(const char *str, int *val, int min, int max)\n{\n\tint ret, new_val;\n\n\tret = kstrtoint(str, 10, &new_val);\n\tif (ret)\n\t\treturn -EINVAL;\n\n\tif (new_val < min || new_val > max)\n\t\treturn -EINVAL;\n\n\t*val = new_val;\n\treturn 0;\n}\n\nstatic int set_params(const char *str, const struct kernel_param *kp)\n{\n\treturn param_store_val(str, kp->arg, 0, INT_MAX);\n}\n\nstatic const struct kernel_param_ops set_param_ops = {\n\t.set\t= set_params,\n\t.get\t= param_get_int,\n};\n\n \nstatic int so_priority;\ndevice_param_cb(so_priority, &set_param_ops, &so_priority, 0644);\nMODULE_PARM_DESC(so_priority, \"nvmet tcp socket optimize priority: Default 0\");\n\n \nstatic int idle_poll_period_usecs;\ndevice_param_cb(idle_poll_period_usecs, &set_param_ops,\n\t\t&idle_poll_period_usecs, 0644);\nMODULE_PARM_DESC(idle_poll_period_usecs,\n\t\t\"nvmet tcp io_work poll till idle time period in usecs: Default 0\");\n\n#define NVMET_TCP_RECV_BUDGET\t\t8\n#define NVMET_TCP_SEND_BUDGET\t\t8\n#define NVMET_TCP_IO_WORK_BUDGET\t64\n\nenum nvmet_tcp_send_state {\n\tNVMET_TCP_SEND_DATA_PDU,\n\tNVMET_TCP_SEND_DATA,\n\tNVMET_TCP_SEND_R2T,\n\tNVMET_TCP_SEND_DDGST,\n\tNVMET_TCP_SEND_RESPONSE\n};\n\nenum nvmet_tcp_recv_state {\n\tNVMET_TCP_RECV_PDU,\n\tNVMET_TCP_RECV_DATA,\n\tNVMET_TCP_RECV_DDGST,\n\tNVMET_TCP_RECV_ERR,\n};\n\nenum {\n\tNVMET_TCP_F_INIT_FAILED = (1 << 0),\n};\n\nstruct nvmet_tcp_cmd {\n\tstruct nvmet_tcp_queue\t\t*queue;\n\tstruct nvmet_req\t\treq;\n\n\tstruct nvme_tcp_cmd_pdu\t\t*cmd_pdu;\n\tstruct nvme_tcp_rsp_pdu\t\t*rsp_pdu;\n\tstruct nvme_tcp_data_pdu\t*data_pdu;\n\tstruct nvme_tcp_r2t_pdu\t\t*r2t_pdu;\n\n\tu32\t\t\t\trbytes_done;\n\tu32\t\t\t\twbytes_done;\n\n\tu32\t\t\t\tpdu_len;\n\tu32\t\t\t\tpdu_recv;\n\tint\t\t\t\tsg_idx;\n\tstruct msghdr\t\t\trecv_msg;\n\tstruct bio_vec\t\t\t*iov;\n\tu32\t\t\t\tflags;\n\n\tstruct list_head\t\tentry;\n\tstruct llist_node\t\tlentry;\n\n\t \n\tu32\t\t\t\toffset;\n\tstruct scatterlist\t\t*cur_sg;\n\tenum nvmet_tcp_send_state\tstate;\n\n\t__le32\t\t\t\texp_ddgst;\n\t__le32\t\t\t\trecv_ddgst;\n};\n\nenum nvmet_tcp_queue_state {\n\tNVMET_TCP_Q_CONNECTING,\n\tNVMET_TCP_Q_LIVE,\n\tNVMET_TCP_Q_DISCONNECTING,\n};\n\nstruct nvmet_tcp_queue {\n\tstruct socket\t\t*sock;\n\tstruct nvmet_tcp_port\t*port;\n\tstruct work_struct\tio_work;\n\tstruct nvmet_cq\t\tnvme_cq;\n\tstruct nvmet_sq\t\tnvme_sq;\n\n\t \n\tstruct nvmet_tcp_cmd\t*cmds;\n\tunsigned int\t\tnr_cmds;\n\tstruct list_head\tfree_list;\n\tstruct llist_head\tresp_list;\n\tstruct list_head\tresp_send_list;\n\tint\t\t\tsend_list_len;\n\tstruct nvmet_tcp_cmd\t*snd_cmd;\n\n\t \n\tint\t\t\toffset;\n\tint\t\t\tleft;\n\tenum nvmet_tcp_recv_state rcv_state;\n\tstruct nvmet_tcp_cmd\t*cmd;\n\tunion nvme_tcp_pdu\tpdu;\n\n\t \n\tbool\t\t\thdr_digest;\n\tbool\t\t\tdata_digest;\n\tstruct ahash_request\t*snd_hash;\n\tstruct ahash_request\t*rcv_hash;\n\n\tunsigned long           poll_end;\n\n\tspinlock_t\t\tstate_lock;\n\tenum nvmet_tcp_queue_state state;\n\n\tstruct sockaddr_storage\tsockaddr;\n\tstruct sockaddr_storage\tsockaddr_peer;\n\tstruct work_struct\trelease_work;\n\n\tint\t\t\tidx;\n\tstruct list_head\tqueue_list;\n\n\tstruct nvmet_tcp_cmd\tconnect;\n\n\tstruct page_frag_cache\tpf_cache;\n\n\tvoid (*data_ready)(struct sock *);\n\tvoid (*state_change)(struct sock *);\n\tvoid (*write_space)(struct sock *);\n};\n\nstruct nvmet_tcp_port {\n\tstruct socket\t\t*sock;\n\tstruct work_struct\taccept_work;\n\tstruct nvmet_port\t*nport;\n\tstruct sockaddr_storage addr;\n\tvoid (*data_ready)(struct sock *);\n};\n\nstatic DEFINE_IDA(nvmet_tcp_queue_ida);\nstatic LIST_HEAD(nvmet_tcp_queue_list);\nstatic DEFINE_MUTEX(nvmet_tcp_queue_mutex);\n\nstatic struct workqueue_struct *nvmet_tcp_wq;\nstatic const struct nvmet_fabrics_ops nvmet_tcp_ops;\nstatic void nvmet_tcp_free_cmd(struct nvmet_tcp_cmd *c);\nstatic void nvmet_tcp_free_cmd_buffers(struct nvmet_tcp_cmd *cmd);\n\nstatic inline u16 nvmet_tcp_cmd_tag(struct nvmet_tcp_queue *queue,\n\t\tstruct nvmet_tcp_cmd *cmd)\n{\n\tif (unlikely(!queue->nr_cmds)) {\n\t\t \n\t\treturn USHRT_MAX;\n\t}\n\n\treturn cmd - queue->cmds;\n}\n\nstatic inline bool nvmet_tcp_has_data_in(struct nvmet_tcp_cmd *cmd)\n{\n\treturn nvme_is_write(cmd->req.cmd) &&\n\t\tcmd->rbytes_done < cmd->req.transfer_len;\n}\n\nstatic inline bool nvmet_tcp_need_data_in(struct nvmet_tcp_cmd *cmd)\n{\n\treturn nvmet_tcp_has_data_in(cmd) && !cmd->req.cqe->status;\n}\n\nstatic inline bool nvmet_tcp_need_data_out(struct nvmet_tcp_cmd *cmd)\n{\n\treturn !nvme_is_write(cmd->req.cmd) &&\n\t\tcmd->req.transfer_len > 0 &&\n\t\t!cmd->req.cqe->status;\n}\n\nstatic inline bool nvmet_tcp_has_inline_data(struct nvmet_tcp_cmd *cmd)\n{\n\treturn nvme_is_write(cmd->req.cmd) && cmd->pdu_len &&\n\t\t!cmd->rbytes_done;\n}\n\nstatic inline struct nvmet_tcp_cmd *\nnvmet_tcp_get_cmd(struct nvmet_tcp_queue *queue)\n{\n\tstruct nvmet_tcp_cmd *cmd;\n\n\tcmd = list_first_entry_or_null(&queue->free_list,\n\t\t\t\tstruct nvmet_tcp_cmd, entry);\n\tif (!cmd)\n\t\treturn NULL;\n\tlist_del_init(&cmd->entry);\n\n\tcmd->rbytes_done = cmd->wbytes_done = 0;\n\tcmd->pdu_len = 0;\n\tcmd->pdu_recv = 0;\n\tcmd->iov = NULL;\n\tcmd->flags = 0;\n\treturn cmd;\n}\n\nstatic inline void nvmet_tcp_put_cmd(struct nvmet_tcp_cmd *cmd)\n{\n\tif (unlikely(cmd == &cmd->queue->connect))\n\t\treturn;\n\n\tlist_add_tail(&cmd->entry, &cmd->queue->free_list);\n}\n\nstatic inline int queue_cpu(struct nvmet_tcp_queue *queue)\n{\n\treturn queue->sock->sk->sk_incoming_cpu;\n}\n\nstatic inline u8 nvmet_tcp_hdgst_len(struct nvmet_tcp_queue *queue)\n{\n\treturn queue->hdr_digest ? NVME_TCP_DIGEST_LENGTH : 0;\n}\n\nstatic inline u8 nvmet_tcp_ddgst_len(struct nvmet_tcp_queue *queue)\n{\n\treturn queue->data_digest ? NVME_TCP_DIGEST_LENGTH : 0;\n}\n\nstatic inline void nvmet_tcp_hdgst(struct ahash_request *hash,\n\t\tvoid *pdu, size_t len)\n{\n\tstruct scatterlist sg;\n\n\tsg_init_one(&sg, pdu, len);\n\tahash_request_set_crypt(hash, &sg, pdu + len, len);\n\tcrypto_ahash_digest(hash);\n}\n\nstatic int nvmet_tcp_verify_hdgst(struct nvmet_tcp_queue *queue,\n\tvoid *pdu, size_t len)\n{\n\tstruct nvme_tcp_hdr *hdr = pdu;\n\t__le32 recv_digest;\n\t__le32 exp_digest;\n\n\tif (unlikely(!(hdr->flags & NVME_TCP_F_HDGST))) {\n\t\tpr_err(\"queue %d: header digest enabled but no header digest\\n\",\n\t\t\tqueue->idx);\n\t\treturn -EPROTO;\n\t}\n\n\trecv_digest = *(__le32 *)(pdu + hdr->hlen);\n\tnvmet_tcp_hdgst(queue->rcv_hash, pdu, len);\n\texp_digest = *(__le32 *)(pdu + hdr->hlen);\n\tif (recv_digest != exp_digest) {\n\t\tpr_err(\"queue %d: header digest error: recv %#x expected %#x\\n\",\n\t\t\tqueue->idx, le32_to_cpu(recv_digest),\n\t\t\tle32_to_cpu(exp_digest));\n\t\treturn -EPROTO;\n\t}\n\n\treturn 0;\n}\n\nstatic int nvmet_tcp_check_ddgst(struct nvmet_tcp_queue *queue, void *pdu)\n{\n\tstruct nvme_tcp_hdr *hdr = pdu;\n\tu8 digest_len = nvmet_tcp_hdgst_len(queue);\n\tu32 len;\n\n\tlen = le32_to_cpu(hdr->plen) - hdr->hlen -\n\t\t(hdr->flags & NVME_TCP_F_HDGST ? digest_len : 0);\n\n\tif (unlikely(len && !(hdr->flags & NVME_TCP_F_DDGST))) {\n\t\tpr_err(\"queue %d: data digest flag is cleared\\n\", queue->idx);\n\t\treturn -EPROTO;\n\t}\n\n\treturn 0;\n}\n\nstatic void nvmet_tcp_free_cmd_buffers(struct nvmet_tcp_cmd *cmd)\n{\n\tkfree(cmd->iov);\n\tsgl_free(cmd->req.sg);\n\tcmd->iov = NULL;\n\tcmd->req.sg = NULL;\n}\n\nstatic void nvmet_tcp_build_pdu_iovec(struct nvmet_tcp_cmd *cmd)\n{\n\tstruct bio_vec *iov = cmd->iov;\n\tstruct scatterlist *sg;\n\tu32 length, offset, sg_offset;\n\tint nr_pages;\n\n\tlength = cmd->pdu_len;\n\tnr_pages = DIV_ROUND_UP(length, PAGE_SIZE);\n\toffset = cmd->rbytes_done;\n\tcmd->sg_idx = offset / PAGE_SIZE;\n\tsg_offset = offset % PAGE_SIZE;\n\tsg = &cmd->req.sg[cmd->sg_idx];\n\n\twhile (length) {\n\t\tu32 iov_len = min_t(u32, length, sg->length - sg_offset);\n\n\t\tbvec_set_page(iov, sg_page(sg), iov_len,\n\t\t\t\tsg->offset + sg_offset);\n\n\t\tlength -= iov_len;\n\t\tsg = sg_next(sg);\n\t\tiov++;\n\t\tsg_offset = 0;\n\t}\n\n\tiov_iter_bvec(&cmd->recv_msg.msg_iter, ITER_DEST, cmd->iov,\n\t\t      nr_pages, cmd->pdu_len);\n}\n\nstatic void nvmet_tcp_fatal_error(struct nvmet_tcp_queue *queue)\n{\n\tqueue->rcv_state = NVMET_TCP_RECV_ERR;\n\tif (queue->nvme_sq.ctrl)\n\t\tnvmet_ctrl_fatal_error(queue->nvme_sq.ctrl);\n\telse\n\t\tkernel_sock_shutdown(queue->sock, SHUT_RDWR);\n}\n\nstatic void nvmet_tcp_socket_error(struct nvmet_tcp_queue *queue, int status)\n{\n\tqueue->rcv_state = NVMET_TCP_RECV_ERR;\n\tif (status == -EPIPE || status == -ECONNRESET)\n\t\tkernel_sock_shutdown(queue->sock, SHUT_RDWR);\n\telse\n\t\tnvmet_tcp_fatal_error(queue);\n}\n\nstatic int nvmet_tcp_map_data(struct nvmet_tcp_cmd *cmd)\n{\n\tstruct nvme_sgl_desc *sgl = &cmd->req.cmd->common.dptr.sgl;\n\tu32 len = le32_to_cpu(sgl->length);\n\n\tif (!len)\n\t\treturn 0;\n\n\tif (sgl->type == ((NVME_SGL_FMT_DATA_DESC << 4) |\n\t\t\t  NVME_SGL_FMT_OFFSET)) {\n\t\tif (!nvme_is_write(cmd->req.cmd))\n\t\t\treturn NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\n\t\tif (len > cmd->req.port->inline_data_size)\n\t\t\treturn NVME_SC_SGL_INVALID_OFFSET | NVME_SC_DNR;\n\t\tcmd->pdu_len = len;\n\t}\n\tcmd->req.transfer_len += len;\n\n\tcmd->req.sg = sgl_alloc(len, GFP_KERNEL, &cmd->req.sg_cnt);\n\tif (!cmd->req.sg)\n\t\treturn NVME_SC_INTERNAL;\n\tcmd->cur_sg = cmd->req.sg;\n\n\tif (nvmet_tcp_has_data_in(cmd)) {\n\t\tcmd->iov = kmalloc_array(cmd->req.sg_cnt,\n\t\t\t\tsizeof(*cmd->iov), GFP_KERNEL);\n\t\tif (!cmd->iov)\n\t\t\tgoto err;\n\t}\n\n\treturn 0;\nerr:\n\tnvmet_tcp_free_cmd_buffers(cmd);\n\treturn NVME_SC_INTERNAL;\n}\n\nstatic void nvmet_tcp_calc_ddgst(struct ahash_request *hash,\n\t\tstruct nvmet_tcp_cmd *cmd)\n{\n\tahash_request_set_crypt(hash, cmd->req.sg,\n\t\t(void *)&cmd->exp_ddgst, cmd->req.transfer_len);\n\tcrypto_ahash_digest(hash);\n}\n\nstatic void nvmet_setup_c2h_data_pdu(struct nvmet_tcp_cmd *cmd)\n{\n\tstruct nvme_tcp_data_pdu *pdu = cmd->data_pdu;\n\tstruct nvmet_tcp_queue *queue = cmd->queue;\n\tu8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);\n\tu8 ddgst = nvmet_tcp_ddgst_len(cmd->queue);\n\n\tcmd->offset = 0;\n\tcmd->state = NVMET_TCP_SEND_DATA_PDU;\n\n\tpdu->hdr.type = nvme_tcp_c2h_data;\n\tpdu->hdr.flags = NVME_TCP_F_DATA_LAST | (queue->nvme_sq.sqhd_disabled ?\n\t\t\t\t\t\tNVME_TCP_F_DATA_SUCCESS : 0);\n\tpdu->hdr.hlen = sizeof(*pdu);\n\tpdu->hdr.pdo = pdu->hdr.hlen + hdgst;\n\tpdu->hdr.plen =\n\t\tcpu_to_le32(pdu->hdr.hlen + hdgst +\n\t\t\t\tcmd->req.transfer_len + ddgst);\n\tpdu->command_id = cmd->req.cqe->command_id;\n\tpdu->data_length = cpu_to_le32(cmd->req.transfer_len);\n\tpdu->data_offset = cpu_to_le32(cmd->wbytes_done);\n\n\tif (queue->data_digest) {\n\t\tpdu->hdr.flags |= NVME_TCP_F_DDGST;\n\t\tnvmet_tcp_calc_ddgst(queue->snd_hash, cmd);\n\t}\n\n\tif (cmd->queue->hdr_digest) {\n\t\tpdu->hdr.flags |= NVME_TCP_F_HDGST;\n\t\tnvmet_tcp_hdgst(queue->snd_hash, pdu, sizeof(*pdu));\n\t}\n}\n\nstatic void nvmet_setup_r2t_pdu(struct nvmet_tcp_cmd *cmd)\n{\n\tstruct nvme_tcp_r2t_pdu *pdu = cmd->r2t_pdu;\n\tstruct nvmet_tcp_queue *queue = cmd->queue;\n\tu8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);\n\n\tcmd->offset = 0;\n\tcmd->state = NVMET_TCP_SEND_R2T;\n\n\tpdu->hdr.type = nvme_tcp_r2t;\n\tpdu->hdr.flags = 0;\n\tpdu->hdr.hlen = sizeof(*pdu);\n\tpdu->hdr.pdo = 0;\n\tpdu->hdr.plen = cpu_to_le32(pdu->hdr.hlen + hdgst);\n\n\tpdu->command_id = cmd->req.cmd->common.command_id;\n\tpdu->ttag = nvmet_tcp_cmd_tag(cmd->queue, cmd);\n\tpdu->r2t_length = cpu_to_le32(cmd->req.transfer_len - cmd->rbytes_done);\n\tpdu->r2t_offset = cpu_to_le32(cmd->rbytes_done);\n\tif (cmd->queue->hdr_digest) {\n\t\tpdu->hdr.flags |= NVME_TCP_F_HDGST;\n\t\tnvmet_tcp_hdgst(queue->snd_hash, pdu, sizeof(*pdu));\n\t}\n}\n\nstatic void nvmet_setup_response_pdu(struct nvmet_tcp_cmd *cmd)\n{\n\tstruct nvme_tcp_rsp_pdu *pdu = cmd->rsp_pdu;\n\tstruct nvmet_tcp_queue *queue = cmd->queue;\n\tu8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);\n\n\tcmd->offset = 0;\n\tcmd->state = NVMET_TCP_SEND_RESPONSE;\n\n\tpdu->hdr.type = nvme_tcp_rsp;\n\tpdu->hdr.flags = 0;\n\tpdu->hdr.hlen = sizeof(*pdu);\n\tpdu->hdr.pdo = 0;\n\tpdu->hdr.plen = cpu_to_le32(pdu->hdr.hlen + hdgst);\n\tif (cmd->queue->hdr_digest) {\n\t\tpdu->hdr.flags |= NVME_TCP_F_HDGST;\n\t\tnvmet_tcp_hdgst(queue->snd_hash, pdu, sizeof(*pdu));\n\t}\n}\n\nstatic void nvmet_tcp_process_resp_list(struct nvmet_tcp_queue *queue)\n{\n\tstruct llist_node *node;\n\tstruct nvmet_tcp_cmd *cmd;\n\n\tfor (node = llist_del_all(&queue->resp_list); node; node = node->next) {\n\t\tcmd = llist_entry(node, struct nvmet_tcp_cmd, lentry);\n\t\tlist_add(&cmd->entry, &queue->resp_send_list);\n\t\tqueue->send_list_len++;\n\t}\n}\n\nstatic struct nvmet_tcp_cmd *nvmet_tcp_fetch_cmd(struct nvmet_tcp_queue *queue)\n{\n\tqueue->snd_cmd = list_first_entry_or_null(&queue->resp_send_list,\n\t\t\t\tstruct nvmet_tcp_cmd, entry);\n\tif (!queue->snd_cmd) {\n\t\tnvmet_tcp_process_resp_list(queue);\n\t\tqueue->snd_cmd =\n\t\t\tlist_first_entry_or_null(&queue->resp_send_list,\n\t\t\t\t\tstruct nvmet_tcp_cmd, entry);\n\t\tif (unlikely(!queue->snd_cmd))\n\t\t\treturn NULL;\n\t}\n\n\tlist_del_init(&queue->snd_cmd->entry);\n\tqueue->send_list_len--;\n\n\tif (nvmet_tcp_need_data_out(queue->snd_cmd))\n\t\tnvmet_setup_c2h_data_pdu(queue->snd_cmd);\n\telse if (nvmet_tcp_need_data_in(queue->snd_cmd))\n\t\tnvmet_setup_r2t_pdu(queue->snd_cmd);\n\telse\n\t\tnvmet_setup_response_pdu(queue->snd_cmd);\n\n\treturn queue->snd_cmd;\n}\n\nstatic void nvmet_tcp_queue_response(struct nvmet_req *req)\n{\n\tstruct nvmet_tcp_cmd *cmd =\n\t\tcontainer_of(req, struct nvmet_tcp_cmd, req);\n\tstruct nvmet_tcp_queue\t*queue = cmd->queue;\n\tstruct nvme_sgl_desc *sgl;\n\tu32 len;\n\n\tif (unlikely(cmd == queue->cmd)) {\n\t\tsgl = &cmd->req.cmd->common.dptr.sgl;\n\t\tlen = le32_to_cpu(sgl->length);\n\n\t\t \n\t\tif (queue->rcv_state == NVMET_TCP_RECV_PDU &&\n\t\t    len && len <= cmd->req.port->inline_data_size &&\n\t\t    nvme_is_write(cmd->req.cmd))\n\t\t\treturn;\n\t}\n\n\tllist_add(&cmd->lentry, &queue->resp_list);\n\tqueue_work_on(queue_cpu(queue), nvmet_tcp_wq, &cmd->queue->io_work);\n}\n\nstatic void nvmet_tcp_execute_request(struct nvmet_tcp_cmd *cmd)\n{\n\tif (unlikely(cmd->flags & NVMET_TCP_F_INIT_FAILED))\n\t\tnvmet_tcp_queue_response(&cmd->req);\n\telse\n\t\tcmd->req.execute(&cmd->req);\n}\n\nstatic int nvmet_try_send_data_pdu(struct nvmet_tcp_cmd *cmd)\n{\n\tstruct msghdr msg = {\n\t\t.msg_flags = MSG_DONTWAIT | MSG_MORE | MSG_SPLICE_PAGES,\n\t};\n\tstruct bio_vec bvec;\n\tu8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);\n\tint left = sizeof(*cmd->data_pdu) - cmd->offset + hdgst;\n\tint ret;\n\n\tbvec_set_virt(&bvec, (void *)cmd->data_pdu + cmd->offset, left);\n\tiov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);\n\tret = sock_sendmsg(cmd->queue->sock, &msg);\n\tif (ret <= 0)\n\t\treturn ret;\n\n\tcmd->offset += ret;\n\tleft -= ret;\n\n\tif (left)\n\t\treturn -EAGAIN;\n\n\tcmd->state = NVMET_TCP_SEND_DATA;\n\tcmd->offset  = 0;\n\treturn 1;\n}\n\nstatic int nvmet_try_send_data(struct nvmet_tcp_cmd *cmd, bool last_in_batch)\n{\n\tstruct nvmet_tcp_queue *queue = cmd->queue;\n\tint ret;\n\n\twhile (cmd->cur_sg) {\n\t\tstruct msghdr msg = {\n\t\t\t.msg_flags = MSG_DONTWAIT | MSG_SPLICE_PAGES,\n\t\t};\n\t\tstruct page *page = sg_page(cmd->cur_sg);\n\t\tstruct bio_vec bvec;\n\t\tu32 left = cmd->cur_sg->length - cmd->offset;\n\n\t\tif ((!last_in_batch && cmd->queue->send_list_len) ||\n\t\t    cmd->wbytes_done + left < cmd->req.transfer_len ||\n\t\t    queue->data_digest || !queue->nvme_sq.sqhd_disabled)\n\t\t\tmsg.msg_flags |= MSG_MORE;\n\n\t\tbvec_set_page(&bvec, page, left, cmd->offset);\n\t\tiov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);\n\t\tret = sock_sendmsg(cmd->queue->sock, &msg);\n\t\tif (ret <= 0)\n\t\t\treturn ret;\n\n\t\tcmd->offset += ret;\n\t\tcmd->wbytes_done += ret;\n\n\t\t \n\t\tif (cmd->offset == cmd->cur_sg->length) {\n\t\t\tcmd->cur_sg = sg_next(cmd->cur_sg);\n\t\t\tcmd->offset = 0;\n\t\t}\n\t}\n\n\tif (queue->data_digest) {\n\t\tcmd->state = NVMET_TCP_SEND_DDGST;\n\t\tcmd->offset = 0;\n\t} else {\n\t\tif (queue->nvme_sq.sqhd_disabled) {\n\t\t\tcmd->queue->snd_cmd = NULL;\n\t\t\tnvmet_tcp_put_cmd(cmd);\n\t\t} else {\n\t\t\tnvmet_setup_response_pdu(cmd);\n\t\t}\n\t}\n\n\tif (queue->nvme_sq.sqhd_disabled)\n\t\tnvmet_tcp_free_cmd_buffers(cmd);\n\n\treturn 1;\n\n}\n\nstatic int nvmet_try_send_response(struct nvmet_tcp_cmd *cmd,\n\t\tbool last_in_batch)\n{\n\tstruct msghdr msg = { .msg_flags = MSG_DONTWAIT | MSG_SPLICE_PAGES, };\n\tstruct bio_vec bvec;\n\tu8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);\n\tint left = sizeof(*cmd->rsp_pdu) - cmd->offset + hdgst;\n\tint ret;\n\n\tif (!last_in_batch && cmd->queue->send_list_len)\n\t\tmsg.msg_flags |= MSG_MORE;\n\telse\n\t\tmsg.msg_flags |= MSG_EOR;\n\n\tbvec_set_virt(&bvec, (void *)cmd->rsp_pdu + cmd->offset, left);\n\tiov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);\n\tret = sock_sendmsg(cmd->queue->sock, &msg);\n\tif (ret <= 0)\n\t\treturn ret;\n\tcmd->offset += ret;\n\tleft -= ret;\n\n\tif (left)\n\t\treturn -EAGAIN;\n\n\tnvmet_tcp_free_cmd_buffers(cmd);\n\tcmd->queue->snd_cmd = NULL;\n\tnvmet_tcp_put_cmd(cmd);\n\treturn 1;\n}\n\nstatic int nvmet_try_send_r2t(struct nvmet_tcp_cmd *cmd, bool last_in_batch)\n{\n\tstruct msghdr msg = { .msg_flags = MSG_DONTWAIT | MSG_SPLICE_PAGES, };\n\tstruct bio_vec bvec;\n\tu8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);\n\tint left = sizeof(*cmd->r2t_pdu) - cmd->offset + hdgst;\n\tint ret;\n\n\tif (!last_in_batch && cmd->queue->send_list_len)\n\t\tmsg.msg_flags |= MSG_MORE;\n\telse\n\t\tmsg.msg_flags |= MSG_EOR;\n\n\tbvec_set_virt(&bvec, (void *)cmd->r2t_pdu + cmd->offset, left);\n\tiov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);\n\tret = sock_sendmsg(cmd->queue->sock, &msg);\n\tif (ret <= 0)\n\t\treturn ret;\n\tcmd->offset += ret;\n\tleft -= ret;\n\n\tif (left)\n\t\treturn -EAGAIN;\n\n\tcmd->queue->snd_cmd = NULL;\n\treturn 1;\n}\n\nstatic int nvmet_try_send_ddgst(struct nvmet_tcp_cmd *cmd, bool last_in_batch)\n{\n\tstruct nvmet_tcp_queue *queue = cmd->queue;\n\tint left = NVME_TCP_DIGEST_LENGTH - cmd->offset;\n\tstruct msghdr msg = { .msg_flags = MSG_DONTWAIT };\n\tstruct kvec iov = {\n\t\t.iov_base = (u8 *)&cmd->exp_ddgst + cmd->offset,\n\t\t.iov_len = left\n\t};\n\tint ret;\n\n\tif (!last_in_batch && cmd->queue->send_list_len)\n\t\tmsg.msg_flags |= MSG_MORE;\n\telse\n\t\tmsg.msg_flags |= MSG_EOR;\n\n\tret = kernel_sendmsg(queue->sock, &msg, &iov, 1, iov.iov_len);\n\tif (unlikely(ret <= 0))\n\t\treturn ret;\n\n\tcmd->offset += ret;\n\tleft -= ret;\n\n\tif (left)\n\t\treturn -EAGAIN;\n\n\tif (queue->nvme_sq.sqhd_disabled) {\n\t\tcmd->queue->snd_cmd = NULL;\n\t\tnvmet_tcp_put_cmd(cmd);\n\t} else {\n\t\tnvmet_setup_response_pdu(cmd);\n\t}\n\treturn 1;\n}\n\nstatic int nvmet_tcp_try_send_one(struct nvmet_tcp_queue *queue,\n\t\tbool last_in_batch)\n{\n\tstruct nvmet_tcp_cmd *cmd = queue->snd_cmd;\n\tint ret = 0;\n\n\tif (!cmd || queue->state == NVMET_TCP_Q_DISCONNECTING) {\n\t\tcmd = nvmet_tcp_fetch_cmd(queue);\n\t\tif (unlikely(!cmd))\n\t\t\treturn 0;\n\t}\n\n\tif (cmd->state == NVMET_TCP_SEND_DATA_PDU) {\n\t\tret = nvmet_try_send_data_pdu(cmd);\n\t\tif (ret <= 0)\n\t\t\tgoto done_send;\n\t}\n\n\tif (cmd->state == NVMET_TCP_SEND_DATA) {\n\t\tret = nvmet_try_send_data(cmd, last_in_batch);\n\t\tif (ret <= 0)\n\t\t\tgoto done_send;\n\t}\n\n\tif (cmd->state == NVMET_TCP_SEND_DDGST) {\n\t\tret = nvmet_try_send_ddgst(cmd, last_in_batch);\n\t\tif (ret <= 0)\n\t\t\tgoto done_send;\n\t}\n\n\tif (cmd->state == NVMET_TCP_SEND_R2T) {\n\t\tret = nvmet_try_send_r2t(cmd, last_in_batch);\n\t\tif (ret <= 0)\n\t\t\tgoto done_send;\n\t}\n\n\tif (cmd->state == NVMET_TCP_SEND_RESPONSE)\n\t\tret = nvmet_try_send_response(cmd, last_in_batch);\n\ndone_send:\n\tif (ret < 0) {\n\t\tif (ret == -EAGAIN)\n\t\t\treturn 0;\n\t\treturn ret;\n\t}\n\n\treturn 1;\n}\n\nstatic int nvmet_tcp_try_send(struct nvmet_tcp_queue *queue,\n\t\tint budget, int *sends)\n{\n\tint i, ret = 0;\n\n\tfor (i = 0; i < budget; i++) {\n\t\tret = nvmet_tcp_try_send_one(queue, i == budget - 1);\n\t\tif (unlikely(ret < 0)) {\n\t\t\tnvmet_tcp_socket_error(queue, ret);\n\t\t\tgoto done;\n\t\t} else if (ret == 0) {\n\t\t\tbreak;\n\t\t}\n\t\t(*sends)++;\n\t}\ndone:\n\treturn ret;\n}\n\nstatic void nvmet_prepare_receive_pdu(struct nvmet_tcp_queue *queue)\n{\n\tqueue->offset = 0;\n\tqueue->left = sizeof(struct nvme_tcp_hdr);\n\tqueue->cmd = NULL;\n\tqueue->rcv_state = NVMET_TCP_RECV_PDU;\n}\n\nstatic void nvmet_tcp_free_crypto(struct nvmet_tcp_queue *queue)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(queue->rcv_hash);\n\n\tahash_request_free(queue->rcv_hash);\n\tahash_request_free(queue->snd_hash);\n\tcrypto_free_ahash(tfm);\n}\n\nstatic int nvmet_tcp_alloc_crypto(struct nvmet_tcp_queue *queue)\n{\n\tstruct crypto_ahash *tfm;\n\n\ttfm = crypto_alloc_ahash(\"crc32c\", 0, CRYPTO_ALG_ASYNC);\n\tif (IS_ERR(tfm))\n\t\treturn PTR_ERR(tfm);\n\n\tqueue->snd_hash = ahash_request_alloc(tfm, GFP_KERNEL);\n\tif (!queue->snd_hash)\n\t\tgoto free_tfm;\n\tahash_request_set_callback(queue->snd_hash, 0, NULL, NULL);\n\n\tqueue->rcv_hash = ahash_request_alloc(tfm, GFP_KERNEL);\n\tif (!queue->rcv_hash)\n\t\tgoto free_snd_hash;\n\tahash_request_set_callback(queue->rcv_hash, 0, NULL, NULL);\n\n\treturn 0;\nfree_snd_hash:\n\tahash_request_free(queue->snd_hash);\nfree_tfm:\n\tcrypto_free_ahash(tfm);\n\treturn -ENOMEM;\n}\n\n\nstatic int nvmet_tcp_handle_icreq(struct nvmet_tcp_queue *queue)\n{\n\tstruct nvme_tcp_icreq_pdu *icreq = &queue->pdu.icreq;\n\tstruct nvme_tcp_icresp_pdu *icresp = &queue->pdu.icresp;\n\tstruct msghdr msg = {};\n\tstruct kvec iov;\n\tint ret;\n\n\tif (le32_to_cpu(icreq->hdr.plen) != sizeof(struct nvme_tcp_icreq_pdu)) {\n\t\tpr_err(\"bad nvme-tcp pdu length (%d)\\n\",\n\t\t\tle32_to_cpu(icreq->hdr.plen));\n\t\tnvmet_tcp_fatal_error(queue);\n\t}\n\n\tif (icreq->pfv != NVME_TCP_PFV_1_0) {\n\t\tpr_err(\"queue %d: bad pfv %d\\n\", queue->idx, icreq->pfv);\n\t\treturn -EPROTO;\n\t}\n\n\tif (icreq->hpda != 0) {\n\t\tpr_err(\"queue %d: unsupported hpda %d\\n\", queue->idx,\n\t\t\ticreq->hpda);\n\t\treturn -EPROTO;\n\t}\n\n\tqueue->hdr_digest = !!(icreq->digest & NVME_TCP_HDR_DIGEST_ENABLE);\n\tqueue->data_digest = !!(icreq->digest & NVME_TCP_DATA_DIGEST_ENABLE);\n\tif (queue->hdr_digest || queue->data_digest) {\n\t\tret = nvmet_tcp_alloc_crypto(queue);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tmemset(icresp, 0, sizeof(*icresp));\n\ticresp->hdr.type = nvme_tcp_icresp;\n\ticresp->hdr.hlen = sizeof(*icresp);\n\ticresp->hdr.pdo = 0;\n\ticresp->hdr.plen = cpu_to_le32(icresp->hdr.hlen);\n\ticresp->pfv = cpu_to_le16(NVME_TCP_PFV_1_0);\n\ticresp->maxdata = cpu_to_le32(NVMET_TCP_MAXH2CDATA);\n\ticresp->cpda = 0;\n\tif (queue->hdr_digest)\n\t\ticresp->digest |= NVME_TCP_HDR_DIGEST_ENABLE;\n\tif (queue->data_digest)\n\t\ticresp->digest |= NVME_TCP_DATA_DIGEST_ENABLE;\n\n\tiov.iov_base = icresp;\n\tiov.iov_len = sizeof(*icresp);\n\tret = kernel_sendmsg(queue->sock, &msg, &iov, 1, iov.iov_len);\n\tif (ret < 0)\n\t\treturn ret;  \n\n\tqueue->state = NVMET_TCP_Q_LIVE;\n\tnvmet_prepare_receive_pdu(queue);\n\treturn 0;\n}\n\nstatic void nvmet_tcp_handle_req_failure(struct nvmet_tcp_queue *queue,\n\t\tstruct nvmet_tcp_cmd *cmd, struct nvmet_req *req)\n{\n\tsize_t data_len = le32_to_cpu(req->cmd->common.dptr.sgl.length);\n\tint ret;\n\n\t \n\tif (!nvme_is_write(cmd->req.cmd) || !data_len ||\n\t    data_len > cmd->req.port->inline_data_size) {\n\t\tnvmet_prepare_receive_pdu(queue);\n\t\treturn;\n\t}\n\n\tret = nvmet_tcp_map_data(cmd);\n\tif (unlikely(ret)) {\n\t\tpr_err(\"queue %d: failed to map data\\n\", queue->idx);\n\t\tnvmet_tcp_fatal_error(queue);\n\t\treturn;\n\t}\n\n\tqueue->rcv_state = NVMET_TCP_RECV_DATA;\n\tnvmet_tcp_build_pdu_iovec(cmd);\n\tcmd->flags |= NVMET_TCP_F_INIT_FAILED;\n}\n\nstatic int nvmet_tcp_handle_h2c_data_pdu(struct nvmet_tcp_queue *queue)\n{\n\tstruct nvme_tcp_data_pdu *data = &queue->pdu.data;\n\tstruct nvmet_tcp_cmd *cmd;\n\tunsigned int exp_data_len;\n\n\tif (likely(queue->nr_cmds)) {\n\t\tif (unlikely(data->ttag >= queue->nr_cmds)) {\n\t\t\tpr_err(\"queue %d: received out of bound ttag %u, nr_cmds %u\\n\",\n\t\t\t\tqueue->idx, data->ttag, queue->nr_cmds);\n\t\t\tnvmet_tcp_fatal_error(queue);\n\t\t\treturn -EPROTO;\n\t\t}\n\t\tcmd = &queue->cmds[data->ttag];\n\t} else {\n\t\tcmd = &queue->connect;\n\t}\n\n\tif (le32_to_cpu(data->data_offset) != cmd->rbytes_done) {\n\t\tpr_err(\"ttag %u unexpected data offset %u (expected %u)\\n\",\n\t\t\tdata->ttag, le32_to_cpu(data->data_offset),\n\t\t\tcmd->rbytes_done);\n\t\t \n\t\tnvmet_tcp_fatal_error(queue);\n\t\treturn -EPROTO;\n\t}\n\n\texp_data_len = le32_to_cpu(data->hdr.plen) -\n\t\t\tnvmet_tcp_hdgst_len(queue) -\n\t\t\tnvmet_tcp_ddgst_len(queue) -\n\t\t\tsizeof(*data);\n\n\tcmd->pdu_len = le32_to_cpu(data->data_length);\n\tif (unlikely(cmd->pdu_len != exp_data_len ||\n\t\t     cmd->pdu_len == 0 ||\n\t\t     cmd->pdu_len > NVMET_TCP_MAXH2CDATA)) {\n\t\tpr_err(\"H2CData PDU len %u is invalid\\n\", cmd->pdu_len);\n\t\t \n\t\tnvmet_tcp_fatal_error(queue);\n\t\treturn -EPROTO;\n\t}\n\tcmd->pdu_recv = 0;\n\tnvmet_tcp_build_pdu_iovec(cmd);\n\tqueue->cmd = cmd;\n\tqueue->rcv_state = NVMET_TCP_RECV_DATA;\n\n\treturn 0;\n}\n\nstatic int nvmet_tcp_done_recv_pdu(struct nvmet_tcp_queue *queue)\n{\n\tstruct nvme_tcp_hdr *hdr = &queue->pdu.cmd.hdr;\n\tstruct nvme_command *nvme_cmd = &queue->pdu.cmd.cmd;\n\tstruct nvmet_req *req;\n\tint ret;\n\n\tif (unlikely(queue->state == NVMET_TCP_Q_CONNECTING)) {\n\t\tif (hdr->type != nvme_tcp_icreq) {\n\t\t\tpr_err(\"unexpected pdu type (%d) before icreq\\n\",\n\t\t\t\thdr->type);\n\t\t\tnvmet_tcp_fatal_error(queue);\n\t\t\treturn -EPROTO;\n\t\t}\n\t\treturn nvmet_tcp_handle_icreq(queue);\n\t}\n\n\tif (unlikely(hdr->type == nvme_tcp_icreq)) {\n\t\tpr_err(\"queue %d: received icreq pdu in state %d\\n\",\n\t\t\tqueue->idx, queue->state);\n\t\tnvmet_tcp_fatal_error(queue);\n\t\treturn -EPROTO;\n\t}\n\n\tif (hdr->type == nvme_tcp_h2c_data) {\n\t\tret = nvmet_tcp_handle_h2c_data_pdu(queue);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t\treturn 0;\n\t}\n\n\tqueue->cmd = nvmet_tcp_get_cmd(queue);\n\tif (unlikely(!queue->cmd)) {\n\t\t \n\t\tpr_err(\"queue %d: out of commands (%d) send_list_len: %d, opcode: %d\",\n\t\t\tqueue->idx, queue->nr_cmds, queue->send_list_len,\n\t\t\tnvme_cmd->common.opcode);\n\t\tnvmet_tcp_fatal_error(queue);\n\t\treturn -ENOMEM;\n\t}\n\n\treq = &queue->cmd->req;\n\tmemcpy(req->cmd, nvme_cmd, sizeof(*nvme_cmd));\n\n\tif (unlikely(!nvmet_req_init(req, &queue->nvme_cq,\n\t\t\t&queue->nvme_sq, &nvmet_tcp_ops))) {\n\t\tpr_err(\"failed cmd %p id %d opcode %d, data_len: %d\\n\",\n\t\t\treq->cmd, req->cmd->common.command_id,\n\t\t\treq->cmd->common.opcode,\n\t\t\tle32_to_cpu(req->cmd->common.dptr.sgl.length));\n\n\t\tnvmet_tcp_handle_req_failure(queue, queue->cmd, req);\n\t\treturn 0;\n\t}\n\n\tret = nvmet_tcp_map_data(queue->cmd);\n\tif (unlikely(ret)) {\n\t\tpr_err(\"queue %d: failed to map data\\n\", queue->idx);\n\t\tif (nvmet_tcp_has_inline_data(queue->cmd))\n\t\t\tnvmet_tcp_fatal_error(queue);\n\t\telse\n\t\t\tnvmet_req_complete(req, ret);\n\t\tret = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tif (nvmet_tcp_need_data_in(queue->cmd)) {\n\t\tif (nvmet_tcp_has_inline_data(queue->cmd)) {\n\t\t\tqueue->rcv_state = NVMET_TCP_RECV_DATA;\n\t\t\tnvmet_tcp_build_pdu_iovec(queue->cmd);\n\t\t\treturn 0;\n\t\t}\n\t\t \n\t\tnvmet_tcp_queue_response(&queue->cmd->req);\n\t\tgoto out;\n\t}\n\n\tqueue->cmd->req.execute(&queue->cmd->req);\nout:\n\tnvmet_prepare_receive_pdu(queue);\n\treturn ret;\n}\n\nstatic const u8 nvme_tcp_pdu_sizes[] = {\n\t[nvme_tcp_icreq]\t= sizeof(struct nvme_tcp_icreq_pdu),\n\t[nvme_tcp_cmd]\t\t= sizeof(struct nvme_tcp_cmd_pdu),\n\t[nvme_tcp_h2c_data]\t= sizeof(struct nvme_tcp_data_pdu),\n};\n\nstatic inline u8 nvmet_tcp_pdu_size(u8 type)\n{\n\tsize_t idx = type;\n\n\treturn (idx < ARRAY_SIZE(nvme_tcp_pdu_sizes) &&\n\t\tnvme_tcp_pdu_sizes[idx]) ?\n\t\t\tnvme_tcp_pdu_sizes[idx] : 0;\n}\n\nstatic inline bool nvmet_tcp_pdu_valid(u8 type)\n{\n\tswitch (type) {\n\tcase nvme_tcp_icreq:\n\tcase nvme_tcp_cmd:\n\tcase nvme_tcp_h2c_data:\n\t\t \n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int nvmet_tcp_try_recv_pdu(struct nvmet_tcp_queue *queue)\n{\n\tstruct nvme_tcp_hdr *hdr = &queue->pdu.cmd.hdr;\n\tint len;\n\tstruct kvec iov;\n\tstruct msghdr msg = { .msg_flags = MSG_DONTWAIT };\n\nrecv:\n\tiov.iov_base = (void *)&queue->pdu + queue->offset;\n\tiov.iov_len = queue->left;\n\tlen = kernel_recvmsg(queue->sock, &msg, &iov, 1,\n\t\t\tiov.iov_len, msg.msg_flags);\n\tif (unlikely(len < 0))\n\t\treturn len;\n\n\tqueue->offset += len;\n\tqueue->left -= len;\n\tif (queue->left)\n\t\treturn -EAGAIN;\n\n\tif (queue->offset == sizeof(struct nvme_tcp_hdr)) {\n\t\tu8 hdgst = nvmet_tcp_hdgst_len(queue);\n\n\t\tif (unlikely(!nvmet_tcp_pdu_valid(hdr->type))) {\n\t\t\tpr_err(\"unexpected pdu type %d\\n\", hdr->type);\n\t\t\tnvmet_tcp_fatal_error(queue);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tif (unlikely(hdr->hlen != nvmet_tcp_pdu_size(hdr->type))) {\n\t\t\tpr_err(\"pdu %d bad hlen %d\\n\", hdr->type, hdr->hlen);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tqueue->left = hdr->hlen - queue->offset + hdgst;\n\t\tgoto recv;\n\t}\n\n\tif (queue->hdr_digest &&\n\t    nvmet_tcp_verify_hdgst(queue, &queue->pdu, hdr->hlen)) {\n\t\tnvmet_tcp_fatal_error(queue);  \n\t\treturn -EPROTO;\n\t}\n\n\tif (queue->data_digest &&\n\t    nvmet_tcp_check_ddgst(queue, &queue->pdu)) {\n\t\tnvmet_tcp_fatal_error(queue);  \n\t\treturn -EPROTO;\n\t}\n\n\treturn nvmet_tcp_done_recv_pdu(queue);\n}\n\nstatic void nvmet_tcp_prep_recv_ddgst(struct nvmet_tcp_cmd *cmd)\n{\n\tstruct nvmet_tcp_queue *queue = cmd->queue;\n\n\tnvmet_tcp_calc_ddgst(queue->rcv_hash, cmd);\n\tqueue->offset = 0;\n\tqueue->left = NVME_TCP_DIGEST_LENGTH;\n\tqueue->rcv_state = NVMET_TCP_RECV_DDGST;\n}\n\nstatic int nvmet_tcp_try_recv_data(struct nvmet_tcp_queue *queue)\n{\n\tstruct nvmet_tcp_cmd  *cmd = queue->cmd;\n\tint ret;\n\n\twhile (msg_data_left(&cmd->recv_msg)) {\n\t\tret = sock_recvmsg(cmd->queue->sock, &cmd->recv_msg,\n\t\t\tcmd->recv_msg.msg_flags);\n\t\tif (ret <= 0)\n\t\t\treturn ret;\n\n\t\tcmd->pdu_recv += ret;\n\t\tcmd->rbytes_done += ret;\n\t}\n\n\tif (queue->data_digest) {\n\t\tnvmet_tcp_prep_recv_ddgst(cmd);\n\t\treturn 0;\n\t}\n\n\tif (cmd->rbytes_done == cmd->req.transfer_len)\n\t\tnvmet_tcp_execute_request(cmd);\n\n\tnvmet_prepare_receive_pdu(queue);\n\treturn 0;\n}\n\nstatic int nvmet_tcp_try_recv_ddgst(struct nvmet_tcp_queue *queue)\n{\n\tstruct nvmet_tcp_cmd *cmd = queue->cmd;\n\tint ret;\n\tstruct msghdr msg = { .msg_flags = MSG_DONTWAIT };\n\tstruct kvec iov = {\n\t\t.iov_base = (void *)&cmd->recv_ddgst + queue->offset,\n\t\t.iov_len = queue->left\n\t};\n\n\tret = kernel_recvmsg(queue->sock, &msg, &iov, 1,\n\t\t\tiov.iov_len, msg.msg_flags);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\tqueue->offset += ret;\n\tqueue->left -= ret;\n\tif (queue->left)\n\t\treturn -EAGAIN;\n\n\tif (queue->data_digest && cmd->exp_ddgst != cmd->recv_ddgst) {\n\t\tpr_err(\"queue %d: cmd %d pdu (%d) data digest error: recv %#x expected %#x\\n\",\n\t\t\tqueue->idx, cmd->req.cmd->common.command_id,\n\t\t\tqueue->pdu.cmd.hdr.type, le32_to_cpu(cmd->recv_ddgst),\n\t\t\tle32_to_cpu(cmd->exp_ddgst));\n\t\tnvmet_req_uninit(&cmd->req);\n\t\tnvmet_tcp_free_cmd_buffers(cmd);\n\t\tnvmet_tcp_fatal_error(queue);\n\t\tret = -EPROTO;\n\t\tgoto out;\n\t}\n\n\tif (cmd->rbytes_done == cmd->req.transfer_len)\n\t\tnvmet_tcp_execute_request(cmd);\n\n\tret = 0;\nout:\n\tnvmet_prepare_receive_pdu(queue);\n\treturn ret;\n}\n\nstatic int nvmet_tcp_try_recv_one(struct nvmet_tcp_queue *queue)\n{\n\tint result = 0;\n\n\tif (unlikely(queue->rcv_state == NVMET_TCP_RECV_ERR))\n\t\treturn 0;\n\n\tif (queue->rcv_state == NVMET_TCP_RECV_PDU) {\n\t\tresult = nvmet_tcp_try_recv_pdu(queue);\n\t\tif (result != 0)\n\t\t\tgoto done_recv;\n\t}\n\n\tif (queue->rcv_state == NVMET_TCP_RECV_DATA) {\n\t\tresult = nvmet_tcp_try_recv_data(queue);\n\t\tif (result != 0)\n\t\t\tgoto done_recv;\n\t}\n\n\tif (queue->rcv_state == NVMET_TCP_RECV_DDGST) {\n\t\tresult = nvmet_tcp_try_recv_ddgst(queue);\n\t\tif (result != 0)\n\t\t\tgoto done_recv;\n\t}\n\ndone_recv:\n\tif (result < 0) {\n\t\tif (result == -EAGAIN)\n\t\t\treturn 0;\n\t\treturn result;\n\t}\n\treturn 1;\n}\n\nstatic int nvmet_tcp_try_recv(struct nvmet_tcp_queue *queue,\n\t\tint budget, int *recvs)\n{\n\tint i, ret = 0;\n\n\tfor (i = 0; i < budget; i++) {\n\t\tret = nvmet_tcp_try_recv_one(queue);\n\t\tif (unlikely(ret < 0)) {\n\t\t\tnvmet_tcp_socket_error(queue, ret);\n\t\t\tgoto done;\n\t\t} else if (ret == 0) {\n\t\t\tbreak;\n\t\t}\n\t\t(*recvs)++;\n\t}\ndone:\n\treturn ret;\n}\n\nstatic void nvmet_tcp_schedule_release_queue(struct nvmet_tcp_queue *queue)\n{\n\tspin_lock(&queue->state_lock);\n\tif (queue->state != NVMET_TCP_Q_DISCONNECTING) {\n\t\tqueue->state = NVMET_TCP_Q_DISCONNECTING;\n\t\tqueue_work(nvmet_wq, &queue->release_work);\n\t}\n\tspin_unlock(&queue->state_lock);\n}\n\nstatic inline void nvmet_tcp_arm_queue_deadline(struct nvmet_tcp_queue *queue)\n{\n\tqueue->poll_end = jiffies + usecs_to_jiffies(idle_poll_period_usecs);\n}\n\nstatic bool nvmet_tcp_check_queue_deadline(struct nvmet_tcp_queue *queue,\n\t\tint ops)\n{\n\tif (!idle_poll_period_usecs)\n\t\treturn false;\n\n\tif (ops)\n\t\tnvmet_tcp_arm_queue_deadline(queue);\n\n\treturn !time_after(jiffies, queue->poll_end);\n}\n\nstatic void nvmet_tcp_io_work(struct work_struct *w)\n{\n\tstruct nvmet_tcp_queue *queue =\n\t\tcontainer_of(w, struct nvmet_tcp_queue, io_work);\n\tbool pending;\n\tint ret, ops = 0;\n\n\tdo {\n\t\tpending = false;\n\n\t\tret = nvmet_tcp_try_recv(queue, NVMET_TCP_RECV_BUDGET, &ops);\n\t\tif (ret > 0)\n\t\t\tpending = true;\n\t\telse if (ret < 0)\n\t\t\treturn;\n\n\t\tret = nvmet_tcp_try_send(queue, NVMET_TCP_SEND_BUDGET, &ops);\n\t\tif (ret > 0)\n\t\t\tpending = true;\n\t\telse if (ret < 0)\n\t\t\treturn;\n\n\t} while (pending && ops < NVMET_TCP_IO_WORK_BUDGET);\n\n\t \n\tif (nvmet_tcp_check_queue_deadline(queue, ops) || pending)\n\t\tqueue_work_on(queue_cpu(queue), nvmet_tcp_wq, &queue->io_work);\n}\n\nstatic int nvmet_tcp_alloc_cmd(struct nvmet_tcp_queue *queue,\n\t\tstruct nvmet_tcp_cmd *c)\n{\n\tu8 hdgst = nvmet_tcp_hdgst_len(queue);\n\n\tc->queue = queue;\n\tc->req.port = queue->port->nport;\n\n\tc->cmd_pdu = page_frag_alloc(&queue->pf_cache,\n\t\t\tsizeof(*c->cmd_pdu) + hdgst, GFP_KERNEL | __GFP_ZERO);\n\tif (!c->cmd_pdu)\n\t\treturn -ENOMEM;\n\tc->req.cmd = &c->cmd_pdu->cmd;\n\n\tc->rsp_pdu = page_frag_alloc(&queue->pf_cache,\n\t\t\tsizeof(*c->rsp_pdu) + hdgst, GFP_KERNEL | __GFP_ZERO);\n\tif (!c->rsp_pdu)\n\t\tgoto out_free_cmd;\n\tc->req.cqe = &c->rsp_pdu->cqe;\n\n\tc->data_pdu = page_frag_alloc(&queue->pf_cache,\n\t\t\tsizeof(*c->data_pdu) + hdgst, GFP_KERNEL | __GFP_ZERO);\n\tif (!c->data_pdu)\n\t\tgoto out_free_rsp;\n\n\tc->r2t_pdu = page_frag_alloc(&queue->pf_cache,\n\t\t\tsizeof(*c->r2t_pdu) + hdgst, GFP_KERNEL | __GFP_ZERO);\n\tif (!c->r2t_pdu)\n\t\tgoto out_free_data;\n\n\tc->recv_msg.msg_flags = MSG_DONTWAIT | MSG_NOSIGNAL;\n\n\tlist_add_tail(&c->entry, &queue->free_list);\n\n\treturn 0;\nout_free_data:\n\tpage_frag_free(c->data_pdu);\nout_free_rsp:\n\tpage_frag_free(c->rsp_pdu);\nout_free_cmd:\n\tpage_frag_free(c->cmd_pdu);\n\treturn -ENOMEM;\n}\n\nstatic void nvmet_tcp_free_cmd(struct nvmet_tcp_cmd *c)\n{\n\tpage_frag_free(c->r2t_pdu);\n\tpage_frag_free(c->data_pdu);\n\tpage_frag_free(c->rsp_pdu);\n\tpage_frag_free(c->cmd_pdu);\n}\n\nstatic int nvmet_tcp_alloc_cmds(struct nvmet_tcp_queue *queue)\n{\n\tstruct nvmet_tcp_cmd *cmds;\n\tint i, ret = -EINVAL, nr_cmds = queue->nr_cmds;\n\n\tcmds = kcalloc(nr_cmds, sizeof(struct nvmet_tcp_cmd), GFP_KERNEL);\n\tif (!cmds)\n\t\tgoto out;\n\n\tfor (i = 0; i < nr_cmds; i++) {\n\t\tret = nvmet_tcp_alloc_cmd(queue, cmds + i);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t}\n\n\tqueue->cmds = cmds;\n\n\treturn 0;\nout_free:\n\twhile (--i >= 0)\n\t\tnvmet_tcp_free_cmd(cmds + i);\n\tkfree(cmds);\nout:\n\treturn ret;\n}\n\nstatic void nvmet_tcp_free_cmds(struct nvmet_tcp_queue *queue)\n{\n\tstruct nvmet_tcp_cmd *cmds = queue->cmds;\n\tint i;\n\n\tfor (i = 0; i < queue->nr_cmds; i++)\n\t\tnvmet_tcp_free_cmd(cmds + i);\n\n\tnvmet_tcp_free_cmd(&queue->connect);\n\tkfree(cmds);\n}\n\nstatic void nvmet_tcp_restore_socket_callbacks(struct nvmet_tcp_queue *queue)\n{\n\tstruct socket *sock = queue->sock;\n\n\twrite_lock_bh(&sock->sk->sk_callback_lock);\n\tsock->sk->sk_data_ready =  queue->data_ready;\n\tsock->sk->sk_state_change = queue->state_change;\n\tsock->sk->sk_write_space = queue->write_space;\n\tsock->sk->sk_user_data = NULL;\n\twrite_unlock_bh(&sock->sk->sk_callback_lock);\n}\n\nstatic void nvmet_tcp_uninit_data_in_cmds(struct nvmet_tcp_queue *queue)\n{\n\tstruct nvmet_tcp_cmd *cmd = queue->cmds;\n\tint i;\n\n\tfor (i = 0; i < queue->nr_cmds; i++, cmd++) {\n\t\tif (nvmet_tcp_need_data_in(cmd))\n\t\t\tnvmet_req_uninit(&cmd->req);\n\t}\n\n\tif (!queue->nr_cmds && nvmet_tcp_need_data_in(&queue->connect)) {\n\t\t \n\t\tnvmet_req_uninit(&queue->connect.req);\n\t}\n}\n\nstatic void nvmet_tcp_free_cmd_data_in_buffers(struct nvmet_tcp_queue *queue)\n{\n\tstruct nvmet_tcp_cmd *cmd = queue->cmds;\n\tint i;\n\n\tfor (i = 0; i < queue->nr_cmds; i++, cmd++) {\n\t\tif (nvmet_tcp_need_data_in(cmd))\n\t\t\tnvmet_tcp_free_cmd_buffers(cmd);\n\t}\n\n\tif (!queue->nr_cmds && nvmet_tcp_need_data_in(&queue->connect))\n\t\tnvmet_tcp_free_cmd_buffers(&queue->connect);\n}\n\nstatic void nvmet_tcp_release_queue_work(struct work_struct *w)\n{\n\tstruct page *page;\n\tstruct nvmet_tcp_queue *queue =\n\t\tcontainer_of(w, struct nvmet_tcp_queue, release_work);\n\n\tmutex_lock(&nvmet_tcp_queue_mutex);\n\tlist_del_init(&queue->queue_list);\n\tmutex_unlock(&nvmet_tcp_queue_mutex);\n\n\tnvmet_tcp_restore_socket_callbacks(queue);\n\tcancel_work_sync(&queue->io_work);\n\t \n\tqueue->rcv_state = NVMET_TCP_RECV_ERR;\n\n\tnvmet_tcp_uninit_data_in_cmds(queue);\n\tnvmet_sq_destroy(&queue->nvme_sq);\n\tcancel_work_sync(&queue->io_work);\n\tnvmet_tcp_free_cmd_data_in_buffers(queue);\n\tsock_release(queue->sock);\n\tnvmet_tcp_free_cmds(queue);\n\tif (queue->hdr_digest || queue->data_digest)\n\t\tnvmet_tcp_free_crypto(queue);\n\tida_free(&nvmet_tcp_queue_ida, queue->idx);\n\n\tpage = virt_to_head_page(queue->pf_cache.va);\n\t__page_frag_cache_drain(page, queue->pf_cache.pagecnt_bias);\n\tkfree(queue);\n}\n\nstatic void nvmet_tcp_data_ready(struct sock *sk)\n{\n\tstruct nvmet_tcp_queue *queue;\n\n\ttrace_sk_data_ready(sk);\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tqueue = sk->sk_user_data;\n\tif (likely(queue))\n\t\tqueue_work_on(queue_cpu(queue), nvmet_tcp_wq, &queue->io_work);\n\tread_unlock_bh(&sk->sk_callback_lock);\n}\n\nstatic void nvmet_tcp_write_space(struct sock *sk)\n{\n\tstruct nvmet_tcp_queue *queue;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tqueue = sk->sk_user_data;\n\tif (unlikely(!queue))\n\t\tgoto out;\n\n\tif (unlikely(queue->state == NVMET_TCP_Q_CONNECTING)) {\n\t\tqueue->write_space(sk);\n\t\tgoto out;\n\t}\n\n\tif (sk_stream_is_writeable(sk)) {\n\t\tclear_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\tqueue_work_on(queue_cpu(queue), nvmet_tcp_wq, &queue->io_work);\n\t}\nout:\n\tread_unlock_bh(&sk->sk_callback_lock);\n}\n\nstatic void nvmet_tcp_state_change(struct sock *sk)\n{\n\tstruct nvmet_tcp_queue *queue;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tqueue = sk->sk_user_data;\n\tif (!queue)\n\t\tgoto done;\n\n\tswitch (sk->sk_state) {\n\tcase TCP_FIN_WAIT2:\n\tcase TCP_LAST_ACK:\n\t\tbreak;\n\tcase TCP_FIN_WAIT1:\n\tcase TCP_CLOSE_WAIT:\n\tcase TCP_CLOSE:\n\t\t \n\t\tnvmet_tcp_schedule_release_queue(queue);\n\t\tbreak;\n\tdefault:\n\t\tpr_warn(\"queue %d unhandled state %d\\n\",\n\t\t\tqueue->idx, sk->sk_state);\n\t}\ndone:\n\tread_unlock_bh(&sk->sk_callback_lock);\n}\n\nstatic int nvmet_tcp_set_queue_sock(struct nvmet_tcp_queue *queue)\n{\n\tstruct socket *sock = queue->sock;\n\tstruct inet_sock *inet = inet_sk(sock->sk);\n\tint ret;\n\n\tret = kernel_getsockname(sock,\n\t\t(struct sockaddr *)&queue->sockaddr);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = kernel_getpeername(sock,\n\t\t(struct sockaddr *)&queue->sockaddr_peer);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tsock_no_linger(sock->sk);\n\n\tif (so_priority > 0)\n\t\tsock_set_priority(sock->sk, so_priority);\n\n\t \n\tif (inet->rcv_tos > 0)\n\t\tip_sock_set_tos(sock->sk, inet->rcv_tos);\n\n\tret = 0;\n\twrite_lock_bh(&sock->sk->sk_callback_lock);\n\tif (sock->sk->sk_state != TCP_ESTABLISHED) {\n\t\t \n\t\tret = -ENOTCONN;\n\t} else {\n\t\tsock->sk->sk_user_data = queue;\n\t\tqueue->data_ready = sock->sk->sk_data_ready;\n\t\tsock->sk->sk_data_ready = nvmet_tcp_data_ready;\n\t\tqueue->state_change = sock->sk->sk_state_change;\n\t\tsock->sk->sk_state_change = nvmet_tcp_state_change;\n\t\tqueue->write_space = sock->sk->sk_write_space;\n\t\tsock->sk->sk_write_space = nvmet_tcp_write_space;\n\t\tif (idle_poll_period_usecs)\n\t\t\tnvmet_tcp_arm_queue_deadline(queue);\n\t\tqueue_work_on(queue_cpu(queue), nvmet_tcp_wq, &queue->io_work);\n\t}\n\twrite_unlock_bh(&sock->sk->sk_callback_lock);\n\n\treturn ret;\n}\n\nstatic int nvmet_tcp_alloc_queue(struct nvmet_tcp_port *port,\n\t\tstruct socket *newsock)\n{\n\tstruct nvmet_tcp_queue *queue;\n\tint ret;\n\n\tqueue = kzalloc(sizeof(*queue), GFP_KERNEL);\n\tif (!queue)\n\t\treturn -ENOMEM;\n\n\tINIT_WORK(&queue->release_work, nvmet_tcp_release_queue_work);\n\tINIT_WORK(&queue->io_work, nvmet_tcp_io_work);\n\tqueue->sock = newsock;\n\tqueue->port = port;\n\tqueue->nr_cmds = 0;\n\tspin_lock_init(&queue->state_lock);\n\tqueue->state = NVMET_TCP_Q_CONNECTING;\n\tINIT_LIST_HEAD(&queue->free_list);\n\tinit_llist_head(&queue->resp_list);\n\tINIT_LIST_HEAD(&queue->resp_send_list);\n\n\tqueue->idx = ida_alloc(&nvmet_tcp_queue_ida, GFP_KERNEL);\n\tif (queue->idx < 0) {\n\t\tret = queue->idx;\n\t\tgoto out_free_queue;\n\t}\n\n\tret = nvmet_tcp_alloc_cmd(queue, &queue->connect);\n\tif (ret)\n\t\tgoto out_ida_remove;\n\n\tret = nvmet_sq_init(&queue->nvme_sq);\n\tif (ret)\n\t\tgoto out_free_connect;\n\n\tnvmet_prepare_receive_pdu(queue);\n\n\tmutex_lock(&nvmet_tcp_queue_mutex);\n\tlist_add_tail(&queue->queue_list, &nvmet_tcp_queue_list);\n\tmutex_unlock(&nvmet_tcp_queue_mutex);\n\n\tret = nvmet_tcp_set_queue_sock(queue);\n\tif (ret)\n\t\tgoto out_destroy_sq;\n\n\treturn 0;\nout_destroy_sq:\n\tmutex_lock(&nvmet_tcp_queue_mutex);\n\tlist_del_init(&queue->queue_list);\n\tmutex_unlock(&nvmet_tcp_queue_mutex);\n\tnvmet_sq_destroy(&queue->nvme_sq);\nout_free_connect:\n\tnvmet_tcp_free_cmd(&queue->connect);\nout_ida_remove:\n\tida_free(&nvmet_tcp_queue_ida, queue->idx);\nout_free_queue:\n\tkfree(queue);\n\treturn ret;\n}\n\nstatic void nvmet_tcp_accept_work(struct work_struct *w)\n{\n\tstruct nvmet_tcp_port *port =\n\t\tcontainer_of(w, struct nvmet_tcp_port, accept_work);\n\tstruct socket *newsock;\n\tint ret;\n\n\twhile (true) {\n\t\tret = kernel_accept(port->sock, &newsock, O_NONBLOCK);\n\t\tif (ret < 0) {\n\t\t\tif (ret != -EAGAIN)\n\t\t\t\tpr_warn(\"failed to accept err=%d\\n\", ret);\n\t\t\treturn;\n\t\t}\n\t\tret = nvmet_tcp_alloc_queue(port, newsock);\n\t\tif (ret) {\n\t\t\tpr_err(\"failed to allocate queue\\n\");\n\t\t\tsock_release(newsock);\n\t\t}\n\t}\n}\n\nstatic void nvmet_tcp_listen_data_ready(struct sock *sk)\n{\n\tstruct nvmet_tcp_port *port;\n\n\ttrace_sk_data_ready(sk);\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tport = sk->sk_user_data;\n\tif (!port)\n\t\tgoto out;\n\n\tif (sk->sk_state == TCP_LISTEN)\n\t\tqueue_work(nvmet_wq, &port->accept_work);\nout:\n\tread_unlock_bh(&sk->sk_callback_lock);\n}\n\nstatic int nvmet_tcp_add_port(struct nvmet_port *nport)\n{\n\tstruct nvmet_tcp_port *port;\n\t__kernel_sa_family_t af;\n\tint ret;\n\n\tport = kzalloc(sizeof(*port), GFP_KERNEL);\n\tif (!port)\n\t\treturn -ENOMEM;\n\n\tswitch (nport->disc_addr.adrfam) {\n\tcase NVMF_ADDR_FAMILY_IP4:\n\t\taf = AF_INET;\n\t\tbreak;\n\tcase NVMF_ADDR_FAMILY_IP6:\n\t\taf = AF_INET6;\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"address family %d not supported\\n\",\n\t\t\t\tnport->disc_addr.adrfam);\n\t\tret = -EINVAL;\n\t\tgoto err_port;\n\t}\n\n\tret = inet_pton_with_scope(&init_net, af, nport->disc_addr.traddr,\n\t\t\tnport->disc_addr.trsvcid, &port->addr);\n\tif (ret) {\n\t\tpr_err(\"malformed ip/port passed: %s:%s\\n\",\n\t\t\tnport->disc_addr.traddr, nport->disc_addr.trsvcid);\n\t\tgoto err_port;\n\t}\n\n\tport->nport = nport;\n\tINIT_WORK(&port->accept_work, nvmet_tcp_accept_work);\n\tif (port->nport->inline_data_size < 0)\n\t\tport->nport->inline_data_size = NVMET_TCP_DEF_INLINE_DATA_SIZE;\n\n\tret = sock_create(port->addr.ss_family, SOCK_STREAM,\n\t\t\t\tIPPROTO_TCP, &port->sock);\n\tif (ret) {\n\t\tpr_err(\"failed to create a socket\\n\");\n\t\tgoto err_port;\n\t}\n\n\tport->sock->sk->sk_user_data = port;\n\tport->data_ready = port->sock->sk->sk_data_ready;\n\tport->sock->sk->sk_data_ready = nvmet_tcp_listen_data_ready;\n\tsock_set_reuseaddr(port->sock->sk);\n\ttcp_sock_set_nodelay(port->sock->sk);\n\tif (so_priority > 0)\n\t\tsock_set_priority(port->sock->sk, so_priority);\n\n\tret = kernel_bind(port->sock, (struct sockaddr *)&port->addr,\n\t\t\tsizeof(port->addr));\n\tif (ret) {\n\t\tpr_err(\"failed to bind port socket %d\\n\", ret);\n\t\tgoto err_sock;\n\t}\n\n\tret = kernel_listen(port->sock, 128);\n\tif (ret) {\n\t\tpr_err(\"failed to listen %d on port sock\\n\", ret);\n\t\tgoto err_sock;\n\t}\n\n\tnport->priv = port;\n\tpr_info(\"enabling port %d (%pISpc)\\n\",\n\t\tle16_to_cpu(nport->disc_addr.portid), &port->addr);\n\n\treturn 0;\n\nerr_sock:\n\tsock_release(port->sock);\nerr_port:\n\tkfree(port);\n\treturn ret;\n}\n\nstatic void nvmet_tcp_destroy_port_queues(struct nvmet_tcp_port *port)\n{\n\tstruct nvmet_tcp_queue *queue;\n\n\tmutex_lock(&nvmet_tcp_queue_mutex);\n\tlist_for_each_entry(queue, &nvmet_tcp_queue_list, queue_list)\n\t\tif (queue->port == port)\n\t\t\tkernel_sock_shutdown(queue->sock, SHUT_RDWR);\n\tmutex_unlock(&nvmet_tcp_queue_mutex);\n}\n\nstatic void nvmet_tcp_remove_port(struct nvmet_port *nport)\n{\n\tstruct nvmet_tcp_port *port = nport->priv;\n\n\twrite_lock_bh(&port->sock->sk->sk_callback_lock);\n\tport->sock->sk->sk_data_ready = port->data_ready;\n\tport->sock->sk->sk_user_data = NULL;\n\twrite_unlock_bh(&port->sock->sk->sk_callback_lock);\n\tcancel_work_sync(&port->accept_work);\n\t \n\tnvmet_tcp_destroy_port_queues(port);\n\n\tsock_release(port->sock);\n\tkfree(port);\n}\n\nstatic void nvmet_tcp_delete_ctrl(struct nvmet_ctrl *ctrl)\n{\n\tstruct nvmet_tcp_queue *queue;\n\n\tmutex_lock(&nvmet_tcp_queue_mutex);\n\tlist_for_each_entry(queue, &nvmet_tcp_queue_list, queue_list)\n\t\tif (queue->nvme_sq.ctrl == ctrl)\n\t\t\tkernel_sock_shutdown(queue->sock, SHUT_RDWR);\n\tmutex_unlock(&nvmet_tcp_queue_mutex);\n}\n\nstatic u16 nvmet_tcp_install_queue(struct nvmet_sq *sq)\n{\n\tstruct nvmet_tcp_queue *queue =\n\t\tcontainer_of(sq, struct nvmet_tcp_queue, nvme_sq);\n\n\tif (sq->qid == 0) {\n\t\t \n\t\tflush_workqueue(nvmet_wq);\n\t}\n\n\tqueue->nr_cmds = sq->size * 2;\n\tif (nvmet_tcp_alloc_cmds(queue))\n\t\treturn NVME_SC_INTERNAL;\n\treturn 0;\n}\n\nstatic void nvmet_tcp_disc_port_addr(struct nvmet_req *req,\n\t\tstruct nvmet_port *nport, char *traddr)\n{\n\tstruct nvmet_tcp_port *port = nport->priv;\n\n\tif (inet_addr_is_any((struct sockaddr *)&port->addr)) {\n\t\tstruct nvmet_tcp_cmd *cmd =\n\t\t\tcontainer_of(req, struct nvmet_tcp_cmd, req);\n\t\tstruct nvmet_tcp_queue *queue = cmd->queue;\n\n\t\tsprintf(traddr, \"%pISc\", (struct sockaddr *)&queue->sockaddr);\n\t} else {\n\t\tmemcpy(traddr, nport->disc_addr.traddr, NVMF_TRADDR_SIZE);\n\t}\n}\n\nstatic const struct nvmet_fabrics_ops nvmet_tcp_ops = {\n\t.owner\t\t\t= THIS_MODULE,\n\t.type\t\t\t= NVMF_TRTYPE_TCP,\n\t.msdbd\t\t\t= 1,\n\t.add_port\t\t= nvmet_tcp_add_port,\n\t.remove_port\t\t= nvmet_tcp_remove_port,\n\t.queue_response\t\t= nvmet_tcp_queue_response,\n\t.delete_ctrl\t\t= nvmet_tcp_delete_ctrl,\n\t.install_queue\t\t= nvmet_tcp_install_queue,\n\t.disc_traddr\t\t= nvmet_tcp_disc_port_addr,\n};\n\nstatic int __init nvmet_tcp_init(void)\n{\n\tint ret;\n\n\tnvmet_tcp_wq = alloc_workqueue(\"nvmet_tcp_wq\",\n\t\t\t\tWQ_MEM_RECLAIM | WQ_HIGHPRI, 0);\n\tif (!nvmet_tcp_wq)\n\t\treturn -ENOMEM;\n\n\tret = nvmet_register_transport(&nvmet_tcp_ops);\n\tif (ret)\n\t\tgoto err;\n\n\treturn 0;\nerr:\n\tdestroy_workqueue(nvmet_tcp_wq);\n\treturn ret;\n}\n\nstatic void __exit nvmet_tcp_exit(void)\n{\n\tstruct nvmet_tcp_queue *queue;\n\n\tnvmet_unregister_transport(&nvmet_tcp_ops);\n\n\tflush_workqueue(nvmet_wq);\n\tmutex_lock(&nvmet_tcp_queue_mutex);\n\tlist_for_each_entry(queue, &nvmet_tcp_queue_list, queue_list)\n\t\tkernel_sock_shutdown(queue->sock, SHUT_RDWR);\n\tmutex_unlock(&nvmet_tcp_queue_mutex);\n\tflush_workqueue(nvmet_wq);\n\n\tdestroy_workqueue(nvmet_tcp_wq);\n}\n\nmodule_init(nvmet_tcp_init);\nmodule_exit(nvmet_tcp_exit);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_ALIAS(\"nvmet-transport-3\");  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}