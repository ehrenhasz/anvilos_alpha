{
  "module_name": "admin-cmd.c",
  "hash_id": "9d4c545265fcdc1b49c304aeeb56b22e1aa3c2ec73602b66bed2d672eb9f0efa",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvme/target/admin-cmd.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n#include <linux/module.h>\n#include <linux/rculist.h>\n#include <linux/part_stat.h>\n\n#include <generated/utsrelease.h>\n#include <asm/unaligned.h>\n#include \"nvmet.h\"\n\nu32 nvmet_get_log_page_len(struct nvme_command *cmd)\n{\n\tu32 len = le16_to_cpu(cmd->get_log_page.numdu);\n\n\tlen <<= 16;\n\tlen += le16_to_cpu(cmd->get_log_page.numdl);\n\t \n\tlen += 1;\n\tlen *= sizeof(u32);\n\n\treturn len;\n}\n\nstatic u32 nvmet_feat_data_len(struct nvmet_req *req, u32 cdw10)\n{\n\tswitch (cdw10 & 0xff) {\n\tcase NVME_FEAT_HOST_ID:\n\t\treturn sizeof(req->sq->ctrl->hostid);\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nu64 nvmet_get_log_page_offset(struct nvme_command *cmd)\n{\n\treturn le64_to_cpu(cmd->get_log_page.lpo);\n}\n\nstatic void nvmet_execute_get_log_page_noop(struct nvmet_req *req)\n{\n\tnvmet_req_complete(req, nvmet_zero_sgl(req, 0, req->transfer_len));\n}\n\nstatic void nvmet_execute_get_log_page_error(struct nvmet_req *req)\n{\n\tstruct nvmet_ctrl *ctrl = req->sq->ctrl;\n\tunsigned long flags;\n\toff_t offset = 0;\n\tu64 slot;\n\tu64 i;\n\n\tspin_lock_irqsave(&ctrl->error_lock, flags);\n\tslot = ctrl->err_counter % NVMET_ERROR_LOG_SLOTS;\n\n\tfor (i = 0; i < NVMET_ERROR_LOG_SLOTS; i++) {\n\t\tif (nvmet_copy_to_sgl(req, offset, &ctrl->slots[slot],\n\t\t\t\tsizeof(struct nvme_error_slot)))\n\t\t\tbreak;\n\n\t\tif (slot == 0)\n\t\t\tslot = NVMET_ERROR_LOG_SLOTS - 1;\n\t\telse\n\t\t\tslot--;\n\t\toffset += sizeof(struct nvme_error_slot);\n\t}\n\tspin_unlock_irqrestore(&ctrl->error_lock, flags);\n\tnvmet_req_complete(req, 0);\n}\n\nstatic u16 nvmet_get_smart_log_nsid(struct nvmet_req *req,\n\t\tstruct nvme_smart_log *slog)\n{\n\tu64 host_reads, host_writes, data_units_read, data_units_written;\n\tu16 status;\n\n\tstatus = nvmet_req_find_ns(req);\n\tif (status)\n\t\treturn status;\n\n\t \n\tif (!req->ns->bdev)\n\t\treturn NVME_SC_SUCCESS;\n\n\thost_reads = part_stat_read(req->ns->bdev, ios[READ]);\n\tdata_units_read =\n\t\tDIV_ROUND_UP(part_stat_read(req->ns->bdev, sectors[READ]), 1000);\n\thost_writes = part_stat_read(req->ns->bdev, ios[WRITE]);\n\tdata_units_written =\n\t\tDIV_ROUND_UP(part_stat_read(req->ns->bdev, sectors[WRITE]), 1000);\n\n\tput_unaligned_le64(host_reads, &slog->host_reads[0]);\n\tput_unaligned_le64(data_units_read, &slog->data_units_read[0]);\n\tput_unaligned_le64(host_writes, &slog->host_writes[0]);\n\tput_unaligned_le64(data_units_written, &slog->data_units_written[0]);\n\n\treturn NVME_SC_SUCCESS;\n}\n\nstatic u16 nvmet_get_smart_log_all(struct nvmet_req *req,\n\t\tstruct nvme_smart_log *slog)\n{\n\tu64 host_reads = 0, host_writes = 0;\n\tu64 data_units_read = 0, data_units_written = 0;\n\tstruct nvmet_ns *ns;\n\tstruct nvmet_ctrl *ctrl;\n\tunsigned long idx;\n\n\tctrl = req->sq->ctrl;\n\txa_for_each(&ctrl->subsys->namespaces, idx, ns) {\n\t\t \n\t\tif (!ns->bdev)\n\t\t\tcontinue;\n\t\thost_reads += part_stat_read(ns->bdev, ios[READ]);\n\t\tdata_units_read += DIV_ROUND_UP(\n\t\t\tpart_stat_read(ns->bdev, sectors[READ]), 1000);\n\t\thost_writes += part_stat_read(ns->bdev, ios[WRITE]);\n\t\tdata_units_written += DIV_ROUND_UP(\n\t\t\tpart_stat_read(ns->bdev, sectors[WRITE]), 1000);\n\t}\n\n\tput_unaligned_le64(host_reads, &slog->host_reads[0]);\n\tput_unaligned_le64(data_units_read, &slog->data_units_read[0]);\n\tput_unaligned_le64(host_writes, &slog->host_writes[0]);\n\tput_unaligned_le64(data_units_written, &slog->data_units_written[0]);\n\n\treturn NVME_SC_SUCCESS;\n}\n\nstatic void nvmet_execute_get_log_page_smart(struct nvmet_req *req)\n{\n\tstruct nvme_smart_log *log;\n\tu16 status = NVME_SC_INTERNAL;\n\tunsigned long flags;\n\n\tif (req->transfer_len != sizeof(*log))\n\t\tgoto out;\n\n\tlog = kzalloc(sizeof(*log), GFP_KERNEL);\n\tif (!log)\n\t\tgoto out;\n\n\tif (req->cmd->get_log_page.nsid == cpu_to_le32(NVME_NSID_ALL))\n\t\tstatus = nvmet_get_smart_log_all(req, log);\n\telse\n\t\tstatus = nvmet_get_smart_log_nsid(req, log);\n\tif (status)\n\t\tgoto out_free_log;\n\n\tspin_lock_irqsave(&req->sq->ctrl->error_lock, flags);\n\tput_unaligned_le64(req->sq->ctrl->err_counter,\n\t\t\t&log->num_err_log_entries);\n\tspin_unlock_irqrestore(&req->sq->ctrl->error_lock, flags);\n\n\tstatus = nvmet_copy_to_sgl(req, 0, log, sizeof(*log));\nout_free_log:\n\tkfree(log);\nout:\n\tnvmet_req_complete(req, status);\n}\n\nstatic void nvmet_get_cmd_effects_nvm(struct nvme_effects_log *log)\n{\n\tlog->acs[nvme_admin_get_log_page] =\n\tlog->acs[nvme_admin_identify] =\n\tlog->acs[nvme_admin_abort_cmd] =\n\tlog->acs[nvme_admin_set_features] =\n\tlog->acs[nvme_admin_get_features] =\n\tlog->acs[nvme_admin_async_event] =\n\tlog->acs[nvme_admin_keep_alive] =\n\t\tcpu_to_le32(NVME_CMD_EFFECTS_CSUPP);\n\n\tlog->iocs[nvme_cmd_read] =\n\tlog->iocs[nvme_cmd_flush] =\n\tlog->iocs[nvme_cmd_dsm]\t=\n\t\tcpu_to_le32(NVME_CMD_EFFECTS_CSUPP);\n\tlog->iocs[nvme_cmd_write] =\n\tlog->iocs[nvme_cmd_write_zeroes] =\n\t\tcpu_to_le32(NVME_CMD_EFFECTS_CSUPP | NVME_CMD_EFFECTS_LBCC);\n}\n\nstatic void nvmet_get_cmd_effects_zns(struct nvme_effects_log *log)\n{\n\tlog->iocs[nvme_cmd_zone_append] =\n\tlog->iocs[nvme_cmd_zone_mgmt_send] =\n\t\tcpu_to_le32(NVME_CMD_EFFECTS_CSUPP | NVME_CMD_EFFECTS_LBCC);\n\tlog->iocs[nvme_cmd_zone_mgmt_recv] =\n\t\tcpu_to_le32(NVME_CMD_EFFECTS_CSUPP);\n}\n\nstatic void nvmet_execute_get_log_cmd_effects_ns(struct nvmet_req *req)\n{\n\tstruct nvme_effects_log *log;\n\tu16 status = NVME_SC_SUCCESS;\n\n\tlog = kzalloc(sizeof(*log), GFP_KERNEL);\n\tif (!log) {\n\t\tstatus = NVME_SC_INTERNAL;\n\t\tgoto out;\n\t}\n\n\tswitch (req->cmd->get_log_page.csi) {\n\tcase NVME_CSI_NVM:\n\t\tnvmet_get_cmd_effects_nvm(log);\n\t\tbreak;\n\tcase NVME_CSI_ZNS:\n\t\tif (!IS_ENABLED(CONFIG_BLK_DEV_ZONED)) {\n\t\t\tstatus = NVME_SC_INVALID_IO_CMD_SET;\n\t\t\tgoto free;\n\t\t}\n\t\tnvmet_get_cmd_effects_nvm(log);\n\t\tnvmet_get_cmd_effects_zns(log);\n\t\tbreak;\n\tdefault:\n\t\tstatus = NVME_SC_INVALID_LOG_PAGE;\n\t\tgoto free;\n\t}\n\n\tstatus = nvmet_copy_to_sgl(req, 0, log, sizeof(*log));\nfree:\n\tkfree(log);\nout:\n\tnvmet_req_complete(req, status);\n}\n\nstatic void nvmet_execute_get_log_changed_ns(struct nvmet_req *req)\n{\n\tstruct nvmet_ctrl *ctrl = req->sq->ctrl;\n\tu16 status = NVME_SC_INTERNAL;\n\tsize_t len;\n\n\tif (req->transfer_len != NVME_MAX_CHANGED_NAMESPACES * sizeof(__le32))\n\t\tgoto out;\n\n\tmutex_lock(&ctrl->lock);\n\tif (ctrl->nr_changed_ns == U32_MAX)\n\t\tlen = sizeof(__le32);\n\telse\n\t\tlen = ctrl->nr_changed_ns * sizeof(__le32);\n\tstatus = nvmet_copy_to_sgl(req, 0, ctrl->changed_ns_list, len);\n\tif (!status)\n\t\tstatus = nvmet_zero_sgl(req, len, req->transfer_len - len);\n\tctrl->nr_changed_ns = 0;\n\tnvmet_clear_aen_bit(req, NVME_AEN_BIT_NS_ATTR);\n\tmutex_unlock(&ctrl->lock);\nout:\n\tnvmet_req_complete(req, status);\n}\n\nstatic u32 nvmet_format_ana_group(struct nvmet_req *req, u32 grpid,\n\t\tstruct nvme_ana_group_desc *desc)\n{\n\tstruct nvmet_ctrl *ctrl = req->sq->ctrl;\n\tstruct nvmet_ns *ns;\n\tunsigned long idx;\n\tu32 count = 0;\n\n\tif (!(req->cmd->get_log_page.lsp & NVME_ANA_LOG_RGO)) {\n\t\txa_for_each(&ctrl->subsys->namespaces, idx, ns)\n\t\t\tif (ns->anagrpid == grpid)\n\t\t\t\tdesc->nsids[count++] = cpu_to_le32(ns->nsid);\n\t}\n\n\tdesc->grpid = cpu_to_le32(grpid);\n\tdesc->nnsids = cpu_to_le32(count);\n\tdesc->chgcnt = cpu_to_le64(nvmet_ana_chgcnt);\n\tdesc->state = req->port->ana_state[grpid];\n\tmemset(desc->rsvd17, 0, sizeof(desc->rsvd17));\n\treturn struct_size(desc, nsids, count);\n}\n\nstatic void nvmet_execute_get_log_page_ana(struct nvmet_req *req)\n{\n\tstruct nvme_ana_rsp_hdr hdr = { 0, };\n\tstruct nvme_ana_group_desc *desc;\n\tsize_t offset = sizeof(struct nvme_ana_rsp_hdr);  \n\tsize_t len;\n\tu32 grpid;\n\tu16 ngrps = 0;\n\tu16 status;\n\n\tstatus = NVME_SC_INTERNAL;\n\tdesc = kmalloc(struct_size(desc, nsids, NVMET_MAX_NAMESPACES),\n\t\t       GFP_KERNEL);\n\tif (!desc)\n\t\tgoto out;\n\n\tdown_read(&nvmet_ana_sem);\n\tfor (grpid = 1; grpid <= NVMET_MAX_ANAGRPS; grpid++) {\n\t\tif (!nvmet_ana_group_enabled[grpid])\n\t\t\tcontinue;\n\t\tlen = nvmet_format_ana_group(req, grpid, desc);\n\t\tstatus = nvmet_copy_to_sgl(req, offset, desc, len);\n\t\tif (status)\n\t\t\tbreak;\n\t\toffset += len;\n\t\tngrps++;\n\t}\n\tfor ( ; grpid <= NVMET_MAX_ANAGRPS; grpid++) {\n\t\tif (nvmet_ana_group_enabled[grpid])\n\t\t\tngrps++;\n\t}\n\n\thdr.chgcnt = cpu_to_le64(nvmet_ana_chgcnt);\n\thdr.ngrps = cpu_to_le16(ngrps);\n\tnvmet_clear_aen_bit(req, NVME_AEN_BIT_ANA_CHANGE);\n\tup_read(&nvmet_ana_sem);\n\n\tkfree(desc);\n\n\t \n\tstatus = nvmet_copy_to_sgl(req, 0, &hdr, sizeof(hdr));\nout:\n\tnvmet_req_complete(req, status);\n}\n\nstatic void nvmet_execute_get_log_page(struct nvmet_req *req)\n{\n\tif (!nvmet_check_transfer_len(req, nvmet_get_log_page_len(req->cmd)))\n\t\treturn;\n\n\tswitch (req->cmd->get_log_page.lid) {\n\tcase NVME_LOG_ERROR:\n\t\treturn nvmet_execute_get_log_page_error(req);\n\tcase NVME_LOG_SMART:\n\t\treturn nvmet_execute_get_log_page_smart(req);\n\tcase NVME_LOG_FW_SLOT:\n\t\t \n\t\treturn nvmet_execute_get_log_page_noop(req);\n\tcase NVME_LOG_CHANGED_NS:\n\t\treturn nvmet_execute_get_log_changed_ns(req);\n\tcase NVME_LOG_CMD_EFFECTS:\n\t\treturn nvmet_execute_get_log_cmd_effects_ns(req);\n\tcase NVME_LOG_ANA:\n\t\treturn nvmet_execute_get_log_page_ana(req);\n\t}\n\tpr_debug(\"unhandled lid %d on qid %d\\n\",\n\t       req->cmd->get_log_page.lid, req->sq->qid);\n\treq->error_loc = offsetof(struct nvme_get_log_page_command, lid);\n\tnvmet_req_complete(req, NVME_SC_INVALID_FIELD | NVME_SC_DNR);\n}\n\nstatic void nvmet_execute_identify_ctrl(struct nvmet_req *req)\n{\n\tstruct nvmet_ctrl *ctrl = req->sq->ctrl;\n\tstruct nvmet_subsys *subsys = ctrl->subsys;\n\tstruct nvme_id_ctrl *id;\n\tu32 cmd_capsule_size;\n\tu16 status = 0;\n\n\tif (!subsys->subsys_discovered) {\n\t\tmutex_lock(&subsys->lock);\n\t\tsubsys->subsys_discovered = true;\n\t\tmutex_unlock(&subsys->lock);\n\t}\n\n\tid = kzalloc(sizeof(*id), GFP_KERNEL);\n\tif (!id) {\n\t\tstatus = NVME_SC_INTERNAL;\n\t\tgoto out;\n\t}\n\n\t \n\tid->vid = 0;\n\tid->ssvid = 0;\n\n\tmemcpy(id->sn, ctrl->subsys->serial, NVMET_SN_MAX_SIZE);\n\tmemcpy_and_pad(id->mn, sizeof(id->mn), subsys->model_number,\n\t\t       strlen(subsys->model_number), ' ');\n\tmemcpy_and_pad(id->fr, sizeof(id->fr),\n\t\t       subsys->firmware_rev, strlen(subsys->firmware_rev), ' ');\n\n\tput_unaligned_le24(subsys->ieee_oui, id->ieee);\n\n\tid->rab = 6;\n\n\tif (nvmet_is_disc_subsys(ctrl->subsys))\n\t\tid->cntrltype = NVME_CTRL_DISC;\n\telse\n\t\tid->cntrltype = NVME_CTRL_IO;\n\n\t \n\tid->cmic = NVME_CTRL_CMIC_MULTI_PORT | NVME_CTRL_CMIC_MULTI_CTRL |\n\t\tNVME_CTRL_CMIC_ANA;\n\n\t \n\tif (ctrl->ops->get_mdts)\n\t\tid->mdts = ctrl->ops->get_mdts(ctrl);\n\telse\n\t\tid->mdts = 0;\n\n\tid->cntlid = cpu_to_le16(ctrl->cntlid);\n\tid->ver = cpu_to_le32(ctrl->subsys->ver);\n\n\t \n\tid->oaes = cpu_to_le32(NVMET_AEN_CFG_OPTIONAL);\n\tid->ctratt = cpu_to_le32(NVME_CTRL_ATTR_HID_128_BIT |\n\t\tNVME_CTRL_ATTR_TBKAS);\n\n\tid->oacs = 0;\n\n\t \n\tid->acl = 3;\n\n\tid->aerl = NVMET_ASYNC_EVENTS - 1;\n\n\t \n\tid->frmw = (1 << 0) | (1 << 1);\n\tid->lpa = (1 << 0) | (1 << 1) | (1 << 2);\n\tid->elpe = NVMET_ERROR_LOG_SLOTS - 1;\n\tid->npss = 0;\n\n\t \n\tid->kas = cpu_to_le16(NVMET_KAS);\n\n\tid->sqes = (0x6 << 4) | 0x6;\n\tid->cqes = (0x4 << 4) | 0x4;\n\n\t \n\tid->maxcmd = cpu_to_le16(NVMET_MAX_CMD);\n\n\tid->nn = cpu_to_le32(NVMET_MAX_NAMESPACES);\n\tid->mnan = cpu_to_le32(NVMET_MAX_NAMESPACES);\n\tid->oncs = cpu_to_le16(NVME_CTRL_ONCS_DSM |\n\t\t\tNVME_CTRL_ONCS_WRITE_ZEROES);\n\n\t \n\tid->vwc = NVME_CTRL_VWC_PRESENT;\n\n\t \n\tid->awun = 0;\n\tid->awupf = 0;\n\n\tid->sgls = cpu_to_le32(1 << 0);\t \n\tif (ctrl->ops->flags & NVMF_KEYED_SGLS)\n\t\tid->sgls |= cpu_to_le32(1 << 2);\n\tif (req->port->inline_data_size)\n\t\tid->sgls |= cpu_to_le32(1 << 20);\n\n\tstrscpy(id->subnqn, ctrl->subsys->subsysnqn, sizeof(id->subnqn));\n\n\t \n\tcmd_capsule_size = sizeof(struct nvme_command);\n\tif (!ctrl->pi_support)\n\t\tcmd_capsule_size += req->port->inline_data_size;\n\tid->ioccsz = cpu_to_le32(cmd_capsule_size / 16);\n\n\t \n\tid->iorcsz = cpu_to_le32(sizeof(struct nvme_completion) / 16);\n\n\tid->msdbd = ctrl->ops->msdbd;\n\n\tid->anacap = (1 << 0) | (1 << 1) | (1 << 2) | (1 << 3) | (1 << 4);\n\tid->anatt = 10;  \n\tid->anagrpmax = cpu_to_le32(NVMET_MAX_ANAGRPS);\n\tid->nanagrpid = cpu_to_le32(NVMET_MAX_ANAGRPS);\n\n\t \n\tid->psd[0].max_power = cpu_to_le16(0x9c4);\n\tid->psd[0].entry_lat = cpu_to_le32(0x10);\n\tid->psd[0].exit_lat = cpu_to_le32(0x4);\n\n\tid->nwpc = 1 << 0;  \n\n\tstatus = nvmet_copy_to_sgl(req, 0, id, sizeof(*id));\n\n\tkfree(id);\nout:\n\tnvmet_req_complete(req, status);\n}\n\nstatic void nvmet_execute_identify_ns(struct nvmet_req *req)\n{\n\tstruct nvme_id_ns *id;\n\tu16 status;\n\n\tif (le32_to_cpu(req->cmd->identify.nsid) == NVME_NSID_ALL) {\n\t\treq->error_loc = offsetof(struct nvme_identify, nsid);\n\t\tstatus = NVME_SC_INVALID_NS | NVME_SC_DNR;\n\t\tgoto out;\n\t}\n\n\tid = kzalloc(sizeof(*id), GFP_KERNEL);\n\tif (!id) {\n\t\tstatus = NVME_SC_INTERNAL;\n\t\tgoto out;\n\t}\n\n\t \n\tstatus = nvmet_req_find_ns(req);\n\tif (status) {\n\t\tstatus = 0;\n\t\tgoto done;\n\t}\n\n\tif (nvmet_ns_revalidate(req->ns)) {\n\t\tmutex_lock(&req->ns->subsys->lock);\n\t\tnvmet_ns_changed(req->ns->subsys, req->ns->nsid);\n\t\tmutex_unlock(&req->ns->subsys->lock);\n\t}\n\n\t \n\tid->ncap = id->nsze =\n\t\tcpu_to_le64(req->ns->size >> req->ns->blksize_shift);\n\tswitch (req->port->ana_state[req->ns->anagrpid]) {\n\tcase NVME_ANA_INACCESSIBLE:\n\tcase NVME_ANA_PERSISTENT_LOSS:\n\t\tbreak;\n\tdefault:\n\t\tid->nuse = id->nsze;\n\t\tbreak;\n\t}\n\n\tif (req->ns->bdev)\n\t\tnvmet_bdev_set_limits(req->ns->bdev, id);\n\n\t \n\tid->nlbaf = 0;\n\tid->flbas = 0;\n\n\t \n\tid->nmic = NVME_NS_NMIC_SHARED;\n\tid->anagrpid = cpu_to_le32(req->ns->anagrpid);\n\n\tmemcpy(&id->nguid, &req->ns->nguid, sizeof(id->nguid));\n\n\tid->lbaf[0].ds = req->ns->blksize_shift;\n\n\tif (req->sq->ctrl->pi_support && nvmet_ns_has_pi(req->ns)) {\n\t\tid->dpc = NVME_NS_DPC_PI_FIRST | NVME_NS_DPC_PI_LAST |\n\t\t\t  NVME_NS_DPC_PI_TYPE1 | NVME_NS_DPC_PI_TYPE2 |\n\t\t\t  NVME_NS_DPC_PI_TYPE3;\n\t\tid->mc = NVME_MC_EXTENDED_LBA;\n\t\tid->dps = req->ns->pi_type;\n\t\tid->flbas = NVME_NS_FLBAS_META_EXT;\n\t\tid->lbaf[0].ms = cpu_to_le16(req->ns->metadata_size);\n\t}\n\n\tif (req->ns->readonly)\n\t\tid->nsattr |= NVME_NS_ATTR_RO;\ndone:\n\tif (!status)\n\t\tstatus = nvmet_copy_to_sgl(req, 0, id, sizeof(*id));\n\n\tkfree(id);\nout:\n\tnvmet_req_complete(req, status);\n}\n\nstatic void nvmet_execute_identify_nslist(struct nvmet_req *req)\n{\n\tstatic const int buf_size = NVME_IDENTIFY_DATA_SIZE;\n\tstruct nvmet_ctrl *ctrl = req->sq->ctrl;\n\tstruct nvmet_ns *ns;\n\tunsigned long idx;\n\tu32 min_nsid = le32_to_cpu(req->cmd->identify.nsid);\n\t__le32 *list;\n\tu16 status = 0;\n\tint i = 0;\n\n\tlist = kzalloc(buf_size, GFP_KERNEL);\n\tif (!list) {\n\t\tstatus = NVME_SC_INTERNAL;\n\t\tgoto out;\n\t}\n\n\txa_for_each(&ctrl->subsys->namespaces, idx, ns) {\n\t\tif (ns->nsid <= min_nsid)\n\t\t\tcontinue;\n\t\tlist[i++] = cpu_to_le32(ns->nsid);\n\t\tif (i == buf_size / sizeof(__le32))\n\t\t\tbreak;\n\t}\n\n\tstatus = nvmet_copy_to_sgl(req, 0, list, buf_size);\n\n\tkfree(list);\nout:\n\tnvmet_req_complete(req, status);\n}\n\nstatic u16 nvmet_copy_ns_identifier(struct nvmet_req *req, u8 type, u8 len,\n\t\t\t\t    void *id, off_t *off)\n{\n\tstruct nvme_ns_id_desc desc = {\n\t\t.nidt = type,\n\t\t.nidl = len,\n\t};\n\tu16 status;\n\n\tstatus = nvmet_copy_to_sgl(req, *off, &desc, sizeof(desc));\n\tif (status)\n\t\treturn status;\n\t*off += sizeof(desc);\n\n\tstatus = nvmet_copy_to_sgl(req, *off, id, len);\n\tif (status)\n\t\treturn status;\n\t*off += len;\n\n\treturn 0;\n}\n\nstatic void nvmet_execute_identify_desclist(struct nvmet_req *req)\n{\n\toff_t off = 0;\n\tu16 status;\n\n\tstatus = nvmet_req_find_ns(req);\n\tif (status)\n\t\tgoto out;\n\n\tif (memchr_inv(&req->ns->uuid, 0, sizeof(req->ns->uuid))) {\n\t\tstatus = nvmet_copy_ns_identifier(req, NVME_NIDT_UUID,\n\t\t\t\t\t\t  NVME_NIDT_UUID_LEN,\n\t\t\t\t\t\t  &req->ns->uuid, &off);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\tif (memchr_inv(req->ns->nguid, 0, sizeof(req->ns->nguid))) {\n\t\tstatus = nvmet_copy_ns_identifier(req, NVME_NIDT_NGUID,\n\t\t\t\t\t\t  NVME_NIDT_NGUID_LEN,\n\t\t\t\t\t\t  &req->ns->nguid, &off);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\n\tstatus = nvmet_copy_ns_identifier(req, NVME_NIDT_CSI,\n\t\t\t\t\t  NVME_NIDT_CSI_LEN,\n\t\t\t\t\t  &req->ns->csi, &off);\n\tif (status)\n\t\tgoto out;\n\n\tif (sg_zero_buffer(req->sg, req->sg_cnt, NVME_IDENTIFY_DATA_SIZE - off,\n\t\t\toff) != NVME_IDENTIFY_DATA_SIZE - off)\n\t\tstatus = NVME_SC_INTERNAL | NVME_SC_DNR;\n\nout:\n\tnvmet_req_complete(req, status);\n}\n\nstatic void nvmet_execute_identify_ctrl_nvm(struct nvmet_req *req)\n{\n\t \n\tnvmet_req_complete(req,\n\t\t   nvmet_zero_sgl(req, 0, sizeof(struct nvme_id_ctrl_nvm)));\n}\n\nstatic void nvmet_execute_identify(struct nvmet_req *req)\n{\n\tif (!nvmet_check_transfer_len(req, NVME_IDENTIFY_DATA_SIZE))\n\t\treturn;\n\n\tswitch (req->cmd->identify.cns) {\n\tcase NVME_ID_CNS_NS:\n\t\tnvmet_execute_identify_ns(req);\n\t\treturn;\n\tcase NVME_ID_CNS_CTRL:\n\t\tnvmet_execute_identify_ctrl(req);\n\t\treturn;\n\tcase NVME_ID_CNS_NS_ACTIVE_LIST:\n\t\tnvmet_execute_identify_nslist(req);\n\t\treturn;\n\tcase NVME_ID_CNS_NS_DESC_LIST:\n\t\tnvmet_execute_identify_desclist(req);\n\t\treturn;\n\tcase NVME_ID_CNS_CS_NS:\n\t\tswitch (req->cmd->identify.csi) {\n\t\tcase NVME_CSI_NVM:\n\t\t\t \n\t\t\tbreak;\n\t\tcase NVME_CSI_ZNS:\n\t\t\tif (IS_ENABLED(CONFIG_BLK_DEV_ZONED)) {\n\t\t\t\tnvmet_execute_identify_ns_zns(req);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase NVME_ID_CNS_CS_CTRL:\n\t\tswitch (req->cmd->identify.csi) {\n\t\tcase NVME_CSI_NVM:\n\t\t\tnvmet_execute_identify_ctrl_nvm(req);\n\t\t\treturn;\n\t\tcase NVME_CSI_ZNS:\n\t\t\tif (IS_ENABLED(CONFIG_BLK_DEV_ZONED)) {\n\t\t\t\tnvmet_execute_identify_ctrl_zns(req);\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\n\tpr_debug(\"unhandled identify cns %d on qid %d\\n\",\n\t       req->cmd->identify.cns, req->sq->qid);\n\treq->error_loc = offsetof(struct nvme_identify, cns);\n\tnvmet_req_complete(req, NVME_SC_INVALID_FIELD | NVME_SC_DNR);\n}\n\n \nstatic void nvmet_execute_abort(struct nvmet_req *req)\n{\n\tif (!nvmet_check_transfer_len(req, 0))\n\t\treturn;\n\tnvmet_set_result(req, 1);\n\tnvmet_req_complete(req, 0);\n}\n\nstatic u16 nvmet_write_protect_flush_sync(struct nvmet_req *req)\n{\n\tu16 status;\n\n\tif (req->ns->file)\n\t\tstatus = nvmet_file_flush(req);\n\telse\n\t\tstatus = nvmet_bdev_flush(req);\n\n\tif (status)\n\t\tpr_err(\"write protect flush failed nsid: %u\\n\", req->ns->nsid);\n\treturn status;\n}\n\nstatic u16 nvmet_set_feat_write_protect(struct nvmet_req *req)\n{\n\tu32 write_protect = le32_to_cpu(req->cmd->common.cdw11);\n\tstruct nvmet_subsys *subsys = nvmet_req_subsys(req);\n\tu16 status;\n\n\tstatus = nvmet_req_find_ns(req);\n\tif (status)\n\t\treturn status;\n\n\tmutex_lock(&subsys->lock);\n\tswitch (write_protect) {\n\tcase NVME_NS_WRITE_PROTECT:\n\t\treq->ns->readonly = true;\n\t\tstatus = nvmet_write_protect_flush_sync(req);\n\t\tif (status)\n\t\t\treq->ns->readonly = false;\n\t\tbreak;\n\tcase NVME_NS_NO_WRITE_PROTECT:\n\t\treq->ns->readonly = false;\n\t\tstatus = 0;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (!status)\n\t\tnvmet_ns_changed(subsys, req->ns->nsid);\n\tmutex_unlock(&subsys->lock);\n\treturn status;\n}\n\nu16 nvmet_set_feat_kato(struct nvmet_req *req)\n{\n\tu32 val32 = le32_to_cpu(req->cmd->common.cdw11);\n\n\tnvmet_stop_keep_alive_timer(req->sq->ctrl);\n\treq->sq->ctrl->kato = DIV_ROUND_UP(val32, 1000);\n\tnvmet_start_keep_alive_timer(req->sq->ctrl);\n\n\tnvmet_set_result(req, req->sq->ctrl->kato);\n\n\treturn 0;\n}\n\nu16 nvmet_set_feat_async_event(struct nvmet_req *req, u32 mask)\n{\n\tu32 val32 = le32_to_cpu(req->cmd->common.cdw11);\n\n\tif (val32 & ~mask) {\n\t\treq->error_loc = offsetof(struct nvme_common_command, cdw11);\n\t\treturn NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t}\n\n\tWRITE_ONCE(req->sq->ctrl->aen_enabled, val32);\n\tnvmet_set_result(req, val32);\n\n\treturn 0;\n}\n\nvoid nvmet_execute_set_features(struct nvmet_req *req)\n{\n\tstruct nvmet_subsys *subsys = nvmet_req_subsys(req);\n\tu32 cdw10 = le32_to_cpu(req->cmd->common.cdw10);\n\tu32 cdw11 = le32_to_cpu(req->cmd->common.cdw11);\n\tu16 status = 0;\n\tu16 nsqr;\n\tu16 ncqr;\n\n\tif (!nvmet_check_data_len_lte(req, 0))\n\t\treturn;\n\n\tswitch (cdw10 & 0xff) {\n\tcase NVME_FEAT_NUM_QUEUES:\n\t\tncqr = (cdw11 >> 16) & 0xffff;\n\t\tnsqr = cdw11 & 0xffff;\n\t\tif (ncqr == 0xffff || nsqr == 0xffff) {\n\t\t\tstatus = NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t\t\tbreak;\n\t\t}\n\t\tnvmet_set_result(req,\n\t\t\t(subsys->max_qid - 1) | ((subsys->max_qid - 1) << 16));\n\t\tbreak;\n\tcase NVME_FEAT_KATO:\n\t\tstatus = nvmet_set_feat_kato(req);\n\t\tbreak;\n\tcase NVME_FEAT_ASYNC_EVENT:\n\t\tstatus = nvmet_set_feat_async_event(req, NVMET_AEN_CFG_ALL);\n\t\tbreak;\n\tcase NVME_FEAT_HOST_ID:\n\t\tstatus = NVME_SC_CMD_SEQ_ERROR | NVME_SC_DNR;\n\t\tbreak;\n\tcase NVME_FEAT_WRITE_PROTECT:\n\t\tstatus = nvmet_set_feat_write_protect(req);\n\t\tbreak;\n\tdefault:\n\t\treq->error_loc = offsetof(struct nvme_common_command, cdw10);\n\t\tstatus = NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t\tbreak;\n\t}\n\n\tnvmet_req_complete(req, status);\n}\n\nstatic u16 nvmet_get_feat_write_protect(struct nvmet_req *req)\n{\n\tstruct nvmet_subsys *subsys = nvmet_req_subsys(req);\n\tu32 result;\n\n\tresult = nvmet_req_find_ns(req);\n\tif (result)\n\t\treturn result;\n\n\tmutex_lock(&subsys->lock);\n\tif (req->ns->readonly == true)\n\t\tresult = NVME_NS_WRITE_PROTECT;\n\telse\n\t\tresult = NVME_NS_NO_WRITE_PROTECT;\n\tnvmet_set_result(req, result);\n\tmutex_unlock(&subsys->lock);\n\n\treturn 0;\n}\n\nvoid nvmet_get_feat_kato(struct nvmet_req *req)\n{\n\tnvmet_set_result(req, req->sq->ctrl->kato * 1000);\n}\n\nvoid nvmet_get_feat_async_event(struct nvmet_req *req)\n{\n\tnvmet_set_result(req, READ_ONCE(req->sq->ctrl->aen_enabled));\n}\n\nvoid nvmet_execute_get_features(struct nvmet_req *req)\n{\n\tstruct nvmet_subsys *subsys = nvmet_req_subsys(req);\n\tu32 cdw10 = le32_to_cpu(req->cmd->common.cdw10);\n\tu16 status = 0;\n\n\tif (!nvmet_check_transfer_len(req, nvmet_feat_data_len(req, cdw10)))\n\t\treturn;\n\n\tswitch (cdw10 & 0xff) {\n\t \n#if 0\n\tcase NVME_FEAT_ARBITRATION:\n\t\tbreak;\n\tcase NVME_FEAT_POWER_MGMT:\n\t\tbreak;\n\tcase NVME_FEAT_TEMP_THRESH:\n\t\tbreak;\n\tcase NVME_FEAT_ERR_RECOVERY:\n\t\tbreak;\n\tcase NVME_FEAT_IRQ_COALESCE:\n\t\tbreak;\n\tcase NVME_FEAT_IRQ_CONFIG:\n\t\tbreak;\n\tcase NVME_FEAT_WRITE_ATOMIC:\n\t\tbreak;\n#endif\n\tcase NVME_FEAT_ASYNC_EVENT:\n\t\tnvmet_get_feat_async_event(req);\n\t\tbreak;\n\tcase NVME_FEAT_VOLATILE_WC:\n\t\tnvmet_set_result(req, 1);\n\t\tbreak;\n\tcase NVME_FEAT_NUM_QUEUES:\n\t\tnvmet_set_result(req,\n\t\t\t(subsys->max_qid-1) | ((subsys->max_qid-1) << 16));\n\t\tbreak;\n\tcase NVME_FEAT_KATO:\n\t\tnvmet_get_feat_kato(req);\n\t\tbreak;\n\tcase NVME_FEAT_HOST_ID:\n\t\t \n\t\tif (!(req->cmd->common.cdw11 & cpu_to_le32(1 << 0))) {\n\t\t\treq->error_loc =\n\t\t\t\toffsetof(struct nvme_common_command, cdw11);\n\t\t\tstatus = NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t\t\tbreak;\n\t\t}\n\n\t\tstatus = nvmet_copy_to_sgl(req, 0, &req->sq->ctrl->hostid,\n\t\t\t\tsizeof(req->sq->ctrl->hostid));\n\t\tbreak;\n\tcase NVME_FEAT_WRITE_PROTECT:\n\t\tstatus = nvmet_get_feat_write_protect(req);\n\t\tbreak;\n\tdefault:\n\t\treq->error_loc =\n\t\t\toffsetof(struct nvme_common_command, cdw10);\n\t\tstatus = NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t\tbreak;\n\t}\n\n\tnvmet_req_complete(req, status);\n}\n\nvoid nvmet_execute_async_event(struct nvmet_req *req)\n{\n\tstruct nvmet_ctrl *ctrl = req->sq->ctrl;\n\n\tif (!nvmet_check_transfer_len(req, 0))\n\t\treturn;\n\n\tmutex_lock(&ctrl->lock);\n\tif (ctrl->nr_async_event_cmds >= NVMET_ASYNC_EVENTS) {\n\t\tmutex_unlock(&ctrl->lock);\n\t\tnvmet_req_complete(req, NVME_SC_ASYNC_LIMIT | NVME_SC_DNR);\n\t\treturn;\n\t}\n\tctrl->async_event_cmds[ctrl->nr_async_event_cmds++] = req;\n\tmutex_unlock(&ctrl->lock);\n\n\tqueue_work(nvmet_wq, &ctrl->async_event_work);\n}\n\nvoid nvmet_execute_keep_alive(struct nvmet_req *req)\n{\n\tstruct nvmet_ctrl *ctrl = req->sq->ctrl;\n\tu16 status = 0;\n\n\tif (!nvmet_check_transfer_len(req, 0))\n\t\treturn;\n\n\tif (!ctrl->kato) {\n\t\tstatus = NVME_SC_KA_TIMEOUT_INVALID;\n\t\tgoto out;\n\t}\n\n\tpr_debug(\"ctrl %d update keep-alive timer for %d secs\\n\",\n\t\tctrl->cntlid, ctrl->kato);\n\tmod_delayed_work(system_wq, &ctrl->ka_work, ctrl->kato * HZ);\nout:\n\tnvmet_req_complete(req, status);\n}\n\nu16 nvmet_parse_admin_cmd(struct nvmet_req *req)\n{\n\tstruct nvme_command *cmd = req->cmd;\n\tu16 ret;\n\n\tif (nvme_is_fabrics(cmd))\n\t\treturn nvmet_parse_fabrics_admin_cmd(req);\n\tif (unlikely(!nvmet_check_auth_status(req)))\n\t\treturn NVME_SC_AUTH_REQUIRED | NVME_SC_DNR;\n\tif (nvmet_is_disc_subsys(nvmet_req_subsys(req)))\n\t\treturn nvmet_parse_discovery_cmd(req);\n\n\tret = nvmet_check_ctrl_status(req);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tif (nvmet_is_passthru_req(req))\n\t\treturn nvmet_parse_passthru_admin_cmd(req);\n\n\tswitch (cmd->common.opcode) {\n\tcase nvme_admin_get_log_page:\n\t\treq->execute = nvmet_execute_get_log_page;\n\t\treturn 0;\n\tcase nvme_admin_identify:\n\t\treq->execute = nvmet_execute_identify;\n\t\treturn 0;\n\tcase nvme_admin_abort_cmd:\n\t\treq->execute = nvmet_execute_abort;\n\t\treturn 0;\n\tcase nvme_admin_set_features:\n\t\treq->execute = nvmet_execute_set_features;\n\t\treturn 0;\n\tcase nvme_admin_get_features:\n\t\treq->execute = nvmet_execute_get_features;\n\t\treturn 0;\n\tcase nvme_admin_async_event:\n\t\treq->execute = nvmet_execute_async_event;\n\t\treturn 0;\n\tcase nvme_admin_keep_alive:\n\t\treq->execute = nvmet_execute_keep_alive;\n\t\treturn 0;\n\tdefault:\n\t\treturn nvmet_report_invalid_opcode(req);\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}