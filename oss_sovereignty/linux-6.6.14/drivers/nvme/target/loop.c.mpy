{
  "module_name": "loop.c",
  "hash_id": "4185dbc74d56f2e24692e4d80f9cc0ad9894b22b30a3a226a1f4b762a24c8cd0",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvme/target/loop.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n#include <linux/scatterlist.h>\n#include <linux/blk-mq.h>\n#include <linux/nvme.h>\n#include <linux/module.h>\n#include <linux/parser.h>\n#include \"nvmet.h\"\n#include \"../host/nvme.h\"\n#include \"../host/fabrics.h\"\n\n#define NVME_LOOP_MAX_SEGMENTS\t\t256\n\nstruct nvme_loop_iod {\n\tstruct nvme_request\tnvme_req;\n\tstruct nvme_command\tcmd;\n\tstruct nvme_completion\tcqe;\n\tstruct nvmet_req\treq;\n\tstruct nvme_loop_queue\t*queue;\n\tstruct work_struct\twork;\n\tstruct sg_table\t\tsg_table;\n\tstruct scatterlist\tfirst_sgl[];\n};\n\nstruct nvme_loop_ctrl {\n\tstruct nvme_loop_queue\t*queues;\n\n\tstruct blk_mq_tag_set\tadmin_tag_set;\n\n\tstruct list_head\tlist;\n\tstruct blk_mq_tag_set\ttag_set;\n\tstruct nvme_loop_iod\tasync_event_iod;\n\tstruct nvme_ctrl\tctrl;\n\n\tstruct nvmet_port\t*port;\n};\n\nstatic inline struct nvme_loop_ctrl *to_loop_ctrl(struct nvme_ctrl *ctrl)\n{\n\treturn container_of(ctrl, struct nvme_loop_ctrl, ctrl);\n}\n\nenum nvme_loop_queue_flags {\n\tNVME_LOOP_Q_LIVE\t= 0,\n};\n\nstruct nvme_loop_queue {\n\tstruct nvmet_cq\t\tnvme_cq;\n\tstruct nvmet_sq\t\tnvme_sq;\n\tstruct nvme_loop_ctrl\t*ctrl;\n\tunsigned long\t\tflags;\n};\n\nstatic LIST_HEAD(nvme_loop_ports);\nstatic DEFINE_MUTEX(nvme_loop_ports_mutex);\n\nstatic LIST_HEAD(nvme_loop_ctrl_list);\nstatic DEFINE_MUTEX(nvme_loop_ctrl_mutex);\n\nstatic void nvme_loop_queue_response(struct nvmet_req *nvme_req);\nstatic void nvme_loop_delete_ctrl(struct nvmet_ctrl *ctrl);\n\nstatic const struct nvmet_fabrics_ops nvme_loop_ops;\n\nstatic inline int nvme_loop_queue_idx(struct nvme_loop_queue *queue)\n{\n\treturn queue - queue->ctrl->queues;\n}\n\nstatic void nvme_loop_complete_rq(struct request *req)\n{\n\tstruct nvme_loop_iod *iod = blk_mq_rq_to_pdu(req);\n\n\tsg_free_table_chained(&iod->sg_table, NVME_INLINE_SG_CNT);\n\tnvme_complete_rq(req);\n}\n\nstatic struct blk_mq_tags *nvme_loop_tagset(struct nvme_loop_queue *queue)\n{\n\tu32 queue_idx = nvme_loop_queue_idx(queue);\n\n\tif (queue_idx == 0)\n\t\treturn queue->ctrl->admin_tag_set.tags[queue_idx];\n\treturn queue->ctrl->tag_set.tags[queue_idx - 1];\n}\n\nstatic void nvme_loop_queue_response(struct nvmet_req *req)\n{\n\tstruct nvme_loop_queue *queue =\n\t\tcontainer_of(req->sq, struct nvme_loop_queue, nvme_sq);\n\tstruct nvme_completion *cqe = req->cqe;\n\n\t \n\tif (unlikely(nvme_is_aen_req(nvme_loop_queue_idx(queue),\n\t\t\t\t     cqe->command_id))) {\n\t\tnvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,\n\t\t\t\t&cqe->result);\n\t} else {\n\t\tstruct request *rq;\n\n\t\trq = nvme_find_rq(nvme_loop_tagset(queue), cqe->command_id);\n\t\tif (!rq) {\n\t\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\t\"got bad command_id %#x on queue %d\\n\",\n\t\t\t\tcqe->command_id, nvme_loop_queue_idx(queue));\n\t\t\treturn;\n\t\t}\n\n\t\tif (!nvme_try_complete_req(rq, cqe->status, cqe->result))\n\t\t\tnvme_loop_complete_rq(rq);\n\t}\n}\n\nstatic void nvme_loop_execute_work(struct work_struct *work)\n{\n\tstruct nvme_loop_iod *iod =\n\t\tcontainer_of(work, struct nvme_loop_iod, work);\n\n\tiod->req.execute(&iod->req);\n}\n\nstatic blk_status_t nvme_loop_queue_rq(struct blk_mq_hw_ctx *hctx,\n\t\tconst struct blk_mq_queue_data *bd)\n{\n\tstruct nvme_ns *ns = hctx->queue->queuedata;\n\tstruct nvme_loop_queue *queue = hctx->driver_data;\n\tstruct request *req = bd->rq;\n\tstruct nvme_loop_iod *iod = blk_mq_rq_to_pdu(req);\n\tbool queue_ready = test_bit(NVME_LOOP_Q_LIVE, &queue->flags);\n\tblk_status_t ret;\n\n\tif (!nvme_check_ready(&queue->ctrl->ctrl, req, queue_ready))\n\t\treturn nvme_fail_nonready_command(&queue->ctrl->ctrl, req);\n\n\tret = nvme_setup_cmd(ns, req);\n\tif (ret)\n\t\treturn ret;\n\n\tnvme_start_request(req);\n\tiod->cmd.common.flags |= NVME_CMD_SGL_METABUF;\n\tiod->req.port = queue->ctrl->port;\n\tif (!nvmet_req_init(&iod->req, &queue->nvme_cq,\n\t\t\t&queue->nvme_sq, &nvme_loop_ops))\n\t\treturn BLK_STS_OK;\n\n\tif (blk_rq_nr_phys_segments(req)) {\n\t\tiod->sg_table.sgl = iod->first_sgl;\n\t\tif (sg_alloc_table_chained(&iod->sg_table,\n\t\t\t\tblk_rq_nr_phys_segments(req),\n\t\t\t\tiod->sg_table.sgl, NVME_INLINE_SG_CNT)) {\n\t\t\tnvme_cleanup_cmd(req);\n\t\t\treturn BLK_STS_RESOURCE;\n\t\t}\n\n\t\tiod->req.sg = iod->sg_table.sgl;\n\t\tiod->req.sg_cnt = blk_rq_map_sg(req->q, req, iod->sg_table.sgl);\n\t\tiod->req.transfer_len = blk_rq_payload_bytes(req);\n\t}\n\n\tqueue_work(nvmet_wq, &iod->work);\n\treturn BLK_STS_OK;\n}\n\nstatic void nvme_loop_submit_async_event(struct nvme_ctrl *arg)\n{\n\tstruct nvme_loop_ctrl *ctrl = to_loop_ctrl(arg);\n\tstruct nvme_loop_queue *queue = &ctrl->queues[0];\n\tstruct nvme_loop_iod *iod = &ctrl->async_event_iod;\n\n\tmemset(&iod->cmd, 0, sizeof(iod->cmd));\n\tiod->cmd.common.opcode = nvme_admin_async_event;\n\tiod->cmd.common.command_id = NVME_AQ_BLK_MQ_DEPTH;\n\tiod->cmd.common.flags |= NVME_CMD_SGL_METABUF;\n\n\tif (!nvmet_req_init(&iod->req, &queue->nvme_cq, &queue->nvme_sq,\n\t\t\t&nvme_loop_ops)) {\n\t\tdev_err(ctrl->ctrl.device, \"failed async event work\\n\");\n\t\treturn;\n\t}\n\n\tqueue_work(nvmet_wq, &iod->work);\n}\n\nstatic int nvme_loop_init_iod(struct nvme_loop_ctrl *ctrl,\n\t\tstruct nvme_loop_iod *iod, unsigned int queue_idx)\n{\n\tiod->req.cmd = &iod->cmd;\n\tiod->req.cqe = &iod->cqe;\n\tiod->queue = &ctrl->queues[queue_idx];\n\tINIT_WORK(&iod->work, nvme_loop_execute_work);\n\treturn 0;\n}\n\nstatic int nvme_loop_init_request(struct blk_mq_tag_set *set,\n\t\tstruct request *req, unsigned int hctx_idx,\n\t\tunsigned int numa_node)\n{\n\tstruct nvme_loop_ctrl *ctrl = to_loop_ctrl(set->driver_data);\n\tstruct nvme_loop_iod *iod = blk_mq_rq_to_pdu(req);\n\n\tnvme_req(req)->ctrl = &ctrl->ctrl;\n\tnvme_req(req)->cmd = &iod->cmd;\n\treturn nvme_loop_init_iod(ctrl, blk_mq_rq_to_pdu(req),\n\t\t\t(set == &ctrl->tag_set) ? hctx_idx + 1 : 0);\n}\n\nstatic struct lock_class_key loop_hctx_fq_lock_key;\n\nstatic int nvme_loop_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,\n\t\tunsigned int hctx_idx)\n{\n\tstruct nvme_loop_ctrl *ctrl = to_loop_ctrl(data);\n\tstruct nvme_loop_queue *queue = &ctrl->queues[hctx_idx + 1];\n\n\tBUG_ON(hctx_idx >= ctrl->ctrl.queue_count);\n\n\t \n\tblk_mq_hctx_set_fq_lock_class(hctx, &loop_hctx_fq_lock_key);\n\n\thctx->driver_data = queue;\n\treturn 0;\n}\n\nstatic int nvme_loop_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,\n\t\tunsigned int hctx_idx)\n{\n\tstruct nvme_loop_ctrl *ctrl = to_loop_ctrl(data);\n\tstruct nvme_loop_queue *queue = &ctrl->queues[0];\n\n\tBUG_ON(hctx_idx != 0);\n\n\thctx->driver_data = queue;\n\treturn 0;\n}\n\nstatic const struct blk_mq_ops nvme_loop_mq_ops = {\n\t.queue_rq\t= nvme_loop_queue_rq,\n\t.complete\t= nvme_loop_complete_rq,\n\t.init_request\t= nvme_loop_init_request,\n\t.init_hctx\t= nvme_loop_init_hctx,\n};\n\nstatic const struct blk_mq_ops nvme_loop_admin_mq_ops = {\n\t.queue_rq\t= nvme_loop_queue_rq,\n\t.complete\t= nvme_loop_complete_rq,\n\t.init_request\t= nvme_loop_init_request,\n\t.init_hctx\t= nvme_loop_init_admin_hctx,\n};\n\nstatic void nvme_loop_destroy_admin_queue(struct nvme_loop_ctrl *ctrl)\n{\n\tif (!test_and_clear_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[0].flags))\n\t\treturn;\n\tnvmet_sq_destroy(&ctrl->queues[0].nvme_sq);\n\tnvme_remove_admin_tag_set(&ctrl->ctrl);\n}\n\nstatic void nvme_loop_free_ctrl(struct nvme_ctrl *nctrl)\n{\n\tstruct nvme_loop_ctrl *ctrl = to_loop_ctrl(nctrl);\n\n\tif (list_empty(&ctrl->list))\n\t\tgoto free_ctrl;\n\n\tmutex_lock(&nvme_loop_ctrl_mutex);\n\tlist_del(&ctrl->list);\n\tmutex_unlock(&nvme_loop_ctrl_mutex);\n\n\tif (nctrl->tagset)\n\t\tnvme_remove_io_tag_set(nctrl);\n\tkfree(ctrl->queues);\n\tnvmf_free_options(nctrl->opts);\nfree_ctrl:\n\tkfree(ctrl);\n}\n\nstatic void nvme_loop_destroy_io_queues(struct nvme_loop_ctrl *ctrl)\n{\n\tint i;\n\n\tfor (i = 1; i < ctrl->ctrl.queue_count; i++) {\n\t\tclear_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[i].flags);\n\t\tnvmet_sq_destroy(&ctrl->queues[i].nvme_sq);\n\t}\n\tctrl->ctrl.queue_count = 1;\n}\n\nstatic int nvme_loop_init_io_queues(struct nvme_loop_ctrl *ctrl)\n{\n\tstruct nvmf_ctrl_options *opts = ctrl->ctrl.opts;\n\tunsigned int nr_io_queues;\n\tint ret, i;\n\n\tnr_io_queues = min(opts->nr_io_queues, num_online_cpus());\n\tret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);\n\tif (ret || !nr_io_queues)\n\t\treturn ret;\n\n\tdev_info(ctrl->ctrl.device, \"creating %d I/O queues.\\n\", nr_io_queues);\n\n\tfor (i = 1; i <= nr_io_queues; i++) {\n\t\tctrl->queues[i].ctrl = ctrl;\n\t\tret = nvmet_sq_init(&ctrl->queues[i].nvme_sq);\n\t\tif (ret)\n\t\t\tgoto out_destroy_queues;\n\n\t\tctrl->ctrl.queue_count++;\n\t}\n\n\treturn 0;\n\nout_destroy_queues:\n\tnvme_loop_destroy_io_queues(ctrl);\n\treturn ret;\n}\n\nstatic int nvme_loop_connect_io_queues(struct nvme_loop_ctrl *ctrl)\n{\n\tint i, ret;\n\n\tfor (i = 1; i < ctrl->ctrl.queue_count; i++) {\n\t\tret = nvmf_connect_io_queue(&ctrl->ctrl, i);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tset_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[i].flags);\n\t}\n\n\treturn 0;\n}\n\nstatic int nvme_loop_configure_admin_queue(struct nvme_loop_ctrl *ctrl)\n{\n\tint error;\n\n\tctrl->queues[0].ctrl = ctrl;\n\terror = nvmet_sq_init(&ctrl->queues[0].nvme_sq);\n\tif (error)\n\t\treturn error;\n\tctrl->ctrl.queue_count = 1;\n\n\terror = nvme_alloc_admin_tag_set(&ctrl->ctrl, &ctrl->admin_tag_set,\n\t\t\t&nvme_loop_admin_mq_ops,\n\t\t\tsizeof(struct nvme_loop_iod) +\n\t\t\tNVME_INLINE_SG_CNT * sizeof(struct scatterlist));\n\tif (error)\n\t\tgoto out_free_sq;\n\n\t \n\tclear_bit(NVME_CTRL_ADMIN_Q_STOPPED, &ctrl->ctrl.flags);\n\n\terror = nvmf_connect_admin_queue(&ctrl->ctrl);\n\tif (error)\n\t\tgoto out_cleanup_tagset;\n\n\tset_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[0].flags);\n\n\terror = nvme_enable_ctrl(&ctrl->ctrl);\n\tif (error)\n\t\tgoto out_cleanup_tagset;\n\n\tctrl->ctrl.max_hw_sectors =\n\t\t(NVME_LOOP_MAX_SEGMENTS - 1) << PAGE_SECTORS_SHIFT;\n\n\tnvme_unquiesce_admin_queue(&ctrl->ctrl);\n\n\terror = nvme_init_ctrl_finish(&ctrl->ctrl, false);\n\tif (error)\n\t\tgoto out_cleanup_tagset;\n\n\treturn 0;\n\nout_cleanup_tagset:\n\tclear_bit(NVME_LOOP_Q_LIVE, &ctrl->queues[0].flags);\n\tnvme_remove_admin_tag_set(&ctrl->ctrl);\nout_free_sq:\n\tnvmet_sq_destroy(&ctrl->queues[0].nvme_sq);\n\treturn error;\n}\n\nstatic void nvme_loop_shutdown_ctrl(struct nvme_loop_ctrl *ctrl)\n{\n\tif (ctrl->ctrl.queue_count > 1) {\n\t\tnvme_quiesce_io_queues(&ctrl->ctrl);\n\t\tnvme_cancel_tagset(&ctrl->ctrl);\n\t\tnvme_loop_destroy_io_queues(ctrl);\n\t}\n\n\tnvme_quiesce_admin_queue(&ctrl->ctrl);\n\tif (ctrl->ctrl.state == NVME_CTRL_LIVE)\n\t\tnvme_disable_ctrl(&ctrl->ctrl, true);\n\n\tnvme_cancel_admin_tagset(&ctrl->ctrl);\n\tnvme_loop_destroy_admin_queue(ctrl);\n}\n\nstatic void nvme_loop_delete_ctrl_host(struct nvme_ctrl *ctrl)\n{\n\tnvme_loop_shutdown_ctrl(to_loop_ctrl(ctrl));\n}\n\nstatic void nvme_loop_delete_ctrl(struct nvmet_ctrl *nctrl)\n{\n\tstruct nvme_loop_ctrl *ctrl;\n\n\tmutex_lock(&nvme_loop_ctrl_mutex);\n\tlist_for_each_entry(ctrl, &nvme_loop_ctrl_list, list) {\n\t\tif (ctrl->ctrl.cntlid == nctrl->cntlid)\n\t\t\tnvme_delete_ctrl(&ctrl->ctrl);\n\t}\n\tmutex_unlock(&nvme_loop_ctrl_mutex);\n}\n\nstatic void nvme_loop_reset_ctrl_work(struct work_struct *work)\n{\n\tstruct nvme_loop_ctrl *ctrl =\n\t\tcontainer_of(work, struct nvme_loop_ctrl, ctrl.reset_work);\n\tint ret;\n\n\tnvme_stop_ctrl(&ctrl->ctrl);\n\tnvme_loop_shutdown_ctrl(ctrl);\n\n\tif (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING)) {\n\t\tif (ctrl->ctrl.state != NVME_CTRL_DELETING &&\n\t\t    ctrl->ctrl.state != NVME_CTRL_DELETING_NOIO)\n\t\t\t \n\t\t\tWARN_ON_ONCE(1);\n\t\treturn;\n\t}\n\n\tret = nvme_loop_configure_admin_queue(ctrl);\n\tif (ret)\n\t\tgoto out_disable;\n\n\tret = nvme_loop_init_io_queues(ctrl);\n\tif (ret)\n\t\tgoto out_destroy_admin;\n\n\tret = nvme_loop_connect_io_queues(ctrl);\n\tif (ret)\n\t\tgoto out_destroy_io;\n\n\tblk_mq_update_nr_hw_queues(&ctrl->tag_set,\n\t\t\tctrl->ctrl.queue_count - 1);\n\n\tif (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE))\n\t\tWARN_ON_ONCE(1);\n\n\tnvme_start_ctrl(&ctrl->ctrl);\n\n\treturn;\n\nout_destroy_io:\n\tnvme_loop_destroy_io_queues(ctrl);\nout_destroy_admin:\n\tnvme_loop_destroy_admin_queue(ctrl);\nout_disable:\n\tdev_warn(ctrl->ctrl.device, \"Removing after reset failure\\n\");\n\tnvme_uninit_ctrl(&ctrl->ctrl);\n}\n\nstatic const struct nvme_ctrl_ops nvme_loop_ctrl_ops = {\n\t.name\t\t\t= \"loop\",\n\t.module\t\t\t= THIS_MODULE,\n\t.flags\t\t\t= NVME_F_FABRICS,\n\t.reg_read32\t\t= nvmf_reg_read32,\n\t.reg_read64\t\t= nvmf_reg_read64,\n\t.reg_write32\t\t= nvmf_reg_write32,\n\t.free_ctrl\t\t= nvme_loop_free_ctrl,\n\t.submit_async_event\t= nvme_loop_submit_async_event,\n\t.delete_ctrl\t\t= nvme_loop_delete_ctrl_host,\n\t.get_address\t\t= nvmf_get_address,\n};\n\nstatic int nvme_loop_create_io_queues(struct nvme_loop_ctrl *ctrl)\n{\n\tint ret;\n\n\tret = nvme_loop_init_io_queues(ctrl);\n\tif (ret)\n\t\treturn ret;\n\n\tret = nvme_alloc_io_tag_set(&ctrl->ctrl, &ctrl->tag_set,\n\t\t\t&nvme_loop_mq_ops, 1,\n\t\t\tsizeof(struct nvme_loop_iod) +\n\t\t\tNVME_INLINE_SG_CNT * sizeof(struct scatterlist));\n\tif (ret)\n\t\tgoto out_destroy_queues;\n\n\tret = nvme_loop_connect_io_queues(ctrl);\n\tif (ret)\n\t\tgoto out_cleanup_tagset;\n\n\treturn 0;\n\nout_cleanup_tagset:\n\tnvme_remove_io_tag_set(&ctrl->ctrl);\nout_destroy_queues:\n\tnvme_loop_destroy_io_queues(ctrl);\n\treturn ret;\n}\n\nstatic struct nvmet_port *nvme_loop_find_port(struct nvme_ctrl *ctrl)\n{\n\tstruct nvmet_port *p, *found = NULL;\n\n\tmutex_lock(&nvme_loop_ports_mutex);\n\tlist_for_each_entry(p, &nvme_loop_ports, entry) {\n\t\t \n\t\tif ((ctrl->opts->mask & NVMF_OPT_TRADDR) &&\n\t\t    strcmp(ctrl->opts->traddr, p->disc_addr.traddr))\n\t\t\tcontinue;\n\t\tfound = p;\n\t\tbreak;\n\t}\n\tmutex_unlock(&nvme_loop_ports_mutex);\n\treturn found;\n}\n\nstatic struct nvme_ctrl *nvme_loop_create_ctrl(struct device *dev,\n\t\tstruct nvmf_ctrl_options *opts)\n{\n\tstruct nvme_loop_ctrl *ctrl;\n\tint ret;\n\n\tctrl = kzalloc(sizeof(*ctrl), GFP_KERNEL);\n\tif (!ctrl)\n\t\treturn ERR_PTR(-ENOMEM);\n\tctrl->ctrl.opts = opts;\n\tINIT_LIST_HEAD(&ctrl->list);\n\n\tINIT_WORK(&ctrl->ctrl.reset_work, nvme_loop_reset_ctrl_work);\n\n\tret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_loop_ctrl_ops,\n\t\t\t\t0  );\n\tif (ret) {\n\t\tkfree(ctrl);\n\t\tgoto out;\n\t}\n\n\tif (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING))\n\t\tWARN_ON_ONCE(1);\n\n\tret = -ENOMEM;\n\n\tctrl->ctrl.kato = opts->kato;\n\tctrl->port = nvme_loop_find_port(&ctrl->ctrl);\n\n\tctrl->queues = kcalloc(opts->nr_io_queues + 1, sizeof(*ctrl->queues),\n\t\t\tGFP_KERNEL);\n\tif (!ctrl->queues)\n\t\tgoto out_uninit_ctrl;\n\n\tret = nvme_loop_configure_admin_queue(ctrl);\n\tif (ret)\n\t\tgoto out_free_queues;\n\n\tif (opts->queue_size > ctrl->ctrl.maxcmd) {\n\t\t \n\t\tdev_warn(ctrl->ctrl.device,\n\t\t\t\"queue_size %zu > ctrl maxcmd %u, clamping down\\n\",\n\t\t\topts->queue_size, ctrl->ctrl.maxcmd);\n\t\topts->queue_size = ctrl->ctrl.maxcmd;\n\t}\n\tctrl->ctrl.sqsize = opts->queue_size - 1;\n\n\tif (opts->nr_io_queues) {\n\t\tret = nvme_loop_create_io_queues(ctrl);\n\t\tif (ret)\n\t\t\tgoto out_remove_admin_queue;\n\t}\n\n\tnvme_loop_init_iod(ctrl, &ctrl->async_event_iod, 0);\n\n\tdev_info(ctrl->ctrl.device,\n\t\t \"new ctrl: \\\"%s\\\"\\n\", ctrl->ctrl.opts->subsysnqn);\n\n\tif (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE))\n\t\tWARN_ON_ONCE(1);\n\n\tmutex_lock(&nvme_loop_ctrl_mutex);\n\tlist_add_tail(&ctrl->list, &nvme_loop_ctrl_list);\n\tmutex_unlock(&nvme_loop_ctrl_mutex);\n\n\tnvme_start_ctrl(&ctrl->ctrl);\n\n\treturn &ctrl->ctrl;\n\nout_remove_admin_queue:\n\tnvme_loop_destroy_admin_queue(ctrl);\nout_free_queues:\n\tkfree(ctrl->queues);\nout_uninit_ctrl:\n\tnvme_uninit_ctrl(&ctrl->ctrl);\n\tnvme_put_ctrl(&ctrl->ctrl);\nout:\n\tif (ret > 0)\n\t\tret = -EIO;\n\treturn ERR_PTR(ret);\n}\n\nstatic int nvme_loop_add_port(struct nvmet_port *port)\n{\n\tmutex_lock(&nvme_loop_ports_mutex);\n\tlist_add_tail(&port->entry, &nvme_loop_ports);\n\tmutex_unlock(&nvme_loop_ports_mutex);\n\treturn 0;\n}\n\nstatic void nvme_loop_remove_port(struct nvmet_port *port)\n{\n\tmutex_lock(&nvme_loop_ports_mutex);\n\tlist_del_init(&port->entry);\n\tmutex_unlock(&nvme_loop_ports_mutex);\n\n\t \n\tflush_workqueue(nvme_delete_wq);\n}\n\nstatic const struct nvmet_fabrics_ops nvme_loop_ops = {\n\t.owner\t\t= THIS_MODULE,\n\t.type\t\t= NVMF_TRTYPE_LOOP,\n\t.add_port\t= nvme_loop_add_port,\n\t.remove_port\t= nvme_loop_remove_port,\n\t.queue_response = nvme_loop_queue_response,\n\t.delete_ctrl\t= nvme_loop_delete_ctrl,\n};\n\nstatic struct nvmf_transport_ops nvme_loop_transport = {\n\t.name\t\t= \"loop\",\n\t.module\t\t= THIS_MODULE,\n\t.create_ctrl\t= nvme_loop_create_ctrl,\n\t.allowed_opts\t= NVMF_OPT_TRADDR,\n};\n\nstatic int __init nvme_loop_init_module(void)\n{\n\tint ret;\n\n\tret = nvmet_register_transport(&nvme_loop_ops);\n\tif (ret)\n\t\treturn ret;\n\n\tret = nvmf_register_transport(&nvme_loop_transport);\n\tif (ret)\n\t\tnvmet_unregister_transport(&nvme_loop_ops);\n\n\treturn ret;\n}\n\nstatic void __exit nvme_loop_cleanup_module(void)\n{\n\tstruct nvme_loop_ctrl *ctrl, *next;\n\n\tnvmf_unregister_transport(&nvme_loop_transport);\n\tnvmet_unregister_transport(&nvme_loop_ops);\n\n\tmutex_lock(&nvme_loop_ctrl_mutex);\n\tlist_for_each_entry_safe(ctrl, next, &nvme_loop_ctrl_list, list)\n\t\tnvme_delete_ctrl(&ctrl->ctrl);\n\tmutex_unlock(&nvme_loop_ctrl_mutex);\n\n\tflush_workqueue(nvme_delete_wq);\n}\n\nmodule_init(nvme_loop_init_module);\nmodule_exit(nvme_loop_cleanup_module);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_ALIAS(\"nvmet-transport-254\");  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}