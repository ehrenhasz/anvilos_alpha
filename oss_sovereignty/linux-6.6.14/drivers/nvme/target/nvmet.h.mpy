{
  "module_name": "nvmet.h",
  "hash_id": "4a7683d8d6f8121750bf4b7f555bb459051b7510776426feca74b7090bcdb79d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvme/target/nvmet.h",
  "human_readable_source": " \n \n\n#ifndef _NVMET_H\n#define _NVMET_H\n\n#include <linux/dma-mapping.h>\n#include <linux/types.h>\n#include <linux/device.h>\n#include <linux/kref.h>\n#include <linux/percpu-refcount.h>\n#include <linux/list.h>\n#include <linux/mutex.h>\n#include <linux/uuid.h>\n#include <linux/nvme.h>\n#include <linux/configfs.h>\n#include <linux/rcupdate.h>\n#include <linux/blkdev.h>\n#include <linux/radix-tree.h>\n#include <linux/t10-pi.h>\n\n#define NVMET_DEFAULT_VS\t\tNVME_VS(1, 3, 0)\n\n#define NVMET_ASYNC_EVENTS\t\t4\n#define NVMET_ERROR_LOG_SLOTS\t\t128\n#define NVMET_NO_ERROR_LOC\t\t((u16)-1)\n#define NVMET_DEFAULT_CTRL_MODEL\t\"Linux\"\n#define NVMET_MN_MAX_SIZE\t\t40\n#define NVMET_SN_MAX_SIZE\t\t20\n#define NVMET_FR_MAX_SIZE\t\t8\n\n \n#define NVMET_AEN_CFG_OPTIONAL \\\n\t(NVME_AEN_CFG_NS_ATTR | NVME_AEN_CFG_ANA_CHANGE)\n#define NVMET_DISC_AEN_CFG_OPTIONAL \\\n\t(NVME_AEN_CFG_DISC_CHANGE)\n\n \n#define NVMET_AEN_CFG_ALL \\\n\t(NVME_SMART_CRIT_SPARE | NVME_SMART_CRIT_TEMPERATURE | \\\n\t NVME_SMART_CRIT_RELIABILITY | NVME_SMART_CRIT_MEDIA | \\\n\t NVME_SMART_CRIT_VOLATILE_MEMORY | NVMET_AEN_CFG_OPTIONAL)\n\n \n#define IPO_IATTR_CONNECT_DATA(x)\t\\\n\t(cpu_to_le32((1 << 16) | (offsetof(struct nvmf_connect_data, x))))\n#define IPO_IATTR_CONNECT_SQE(x)\t\\\n\t(cpu_to_le32(offsetof(struct nvmf_connect_command, x)))\n\nstruct nvmet_ns {\n\tstruct percpu_ref\tref;\n\tstruct block_device\t*bdev;\n\tstruct file\t\t*file;\n\tbool\t\t\treadonly;\n\tu32\t\t\tnsid;\n\tu32\t\t\tblksize_shift;\n\tloff_t\t\t\tsize;\n\tu8\t\t\tnguid[16];\n\tuuid_t\t\t\tuuid;\n\tu32\t\t\tanagrpid;\n\n\tbool\t\t\tbuffered_io;\n\tbool\t\t\tenabled;\n\tstruct nvmet_subsys\t*subsys;\n\tconst char\t\t*device_path;\n\n\tstruct config_group\tdevice_group;\n\tstruct config_group\tgroup;\n\n\tstruct completion\tdisable_done;\n\tmempool_t\t\t*bvec_pool;\n\n\tstruct pci_dev\t\t*p2p_dev;\n\tint\t\t\tuse_p2pmem;\n\tint\t\t\tpi_type;\n\tint\t\t\tmetadata_size;\n\tu8\t\t\tcsi;\n};\n\nstatic inline struct nvmet_ns *to_nvmet_ns(struct config_item *item)\n{\n\treturn container_of(to_config_group(item), struct nvmet_ns, group);\n}\n\nstatic inline struct device *nvmet_ns_dev(struct nvmet_ns *ns)\n{\n\treturn ns->bdev ? disk_to_dev(ns->bdev->bd_disk) : NULL;\n}\n\nstruct nvmet_cq {\n\tu16\t\t\tqid;\n\tu16\t\t\tsize;\n};\n\nstruct nvmet_sq {\n\tstruct nvmet_ctrl\t*ctrl;\n\tstruct percpu_ref\tref;\n\tu16\t\t\tqid;\n\tu16\t\t\tsize;\n\tu32\t\t\tsqhd;\n\tbool\t\t\tsqhd_disabled;\n#ifdef CONFIG_NVME_TARGET_AUTH\n\tbool\t\t\tauthenticated;\n\tstruct delayed_work\tauth_expired_work;\n\tu16\t\t\tdhchap_tid;\n\tu16\t\t\tdhchap_status;\n\tint\t\t\tdhchap_step;\n\tu8\t\t\t*dhchap_c1;\n\tu8\t\t\t*dhchap_c2;\n\tu32\t\t\tdhchap_s1;\n\tu32\t\t\tdhchap_s2;\n\tu8\t\t\t*dhchap_skey;\n\tint\t\t\tdhchap_skey_len;\n#endif\n\tstruct completion\tfree_done;\n\tstruct completion\tconfirm_done;\n};\n\nstruct nvmet_ana_group {\n\tstruct config_group\tgroup;\n\tstruct nvmet_port\t*port;\n\tu32\t\t\tgrpid;\n};\n\nstatic inline struct nvmet_ana_group *to_ana_group(struct config_item *item)\n{\n\treturn container_of(to_config_group(item), struct nvmet_ana_group,\n\t\t\tgroup);\n}\n\n \nstruct nvmet_port {\n\tstruct list_head\t\tentry;\n\tstruct nvmf_disc_rsp_page_entry\tdisc_addr;\n\tstruct config_group\t\tgroup;\n\tstruct config_group\t\tsubsys_group;\n\tstruct list_head\t\tsubsystems;\n\tstruct config_group\t\treferrals_group;\n\tstruct list_head\t\treferrals;\n\tstruct list_head\t\tglobal_entry;\n\tstruct config_group\t\tana_groups_group;\n\tstruct nvmet_ana_group\t\tana_default_group;\n\tenum nvme_ana_state\t\t*ana_state;\n\tvoid\t\t\t\t*priv;\n\tbool\t\t\t\tenabled;\n\tint\t\t\t\tinline_data_size;\n\tconst struct nvmet_fabrics_ops\t*tr_ops;\n\tbool\t\t\t\tpi_enable;\n};\n\nstatic inline struct nvmet_port *to_nvmet_port(struct config_item *item)\n{\n\treturn container_of(to_config_group(item), struct nvmet_port,\n\t\t\tgroup);\n}\n\nstatic inline struct nvmet_port *ana_groups_to_port(\n\t\tstruct config_item *item)\n{\n\treturn container_of(to_config_group(item), struct nvmet_port,\n\t\t\tana_groups_group);\n}\n\nstruct nvmet_ctrl {\n\tstruct nvmet_subsys\t*subsys;\n\tstruct nvmet_sq\t\t**sqs;\n\n\tbool\t\t\treset_tbkas;\n\n\tstruct mutex\t\tlock;\n\tu64\t\t\tcap;\n\tu32\t\t\tcc;\n\tu32\t\t\tcsts;\n\n\tuuid_t\t\t\thostid;\n\tu16\t\t\tcntlid;\n\tu32\t\t\tkato;\n\n\tstruct nvmet_port\t*port;\n\n\tu32\t\t\taen_enabled;\n\tunsigned long\t\taen_masked;\n\tstruct nvmet_req\t*async_event_cmds[NVMET_ASYNC_EVENTS];\n\tunsigned int\t\tnr_async_event_cmds;\n\tstruct list_head\tasync_events;\n\tstruct work_struct\tasync_event_work;\n\n\tstruct list_head\tsubsys_entry;\n\tstruct kref\t\tref;\n\tstruct delayed_work\tka_work;\n\tstruct work_struct\tfatal_err_work;\n\n\tconst struct nvmet_fabrics_ops *ops;\n\n\t__le32\t\t\t*changed_ns_list;\n\tu32\t\t\tnr_changed_ns;\n\n\tchar\t\t\tsubsysnqn[NVMF_NQN_FIELD_LEN];\n\tchar\t\t\thostnqn[NVMF_NQN_FIELD_LEN];\n\n\tstruct device\t\t*p2p_client;\n\tstruct radix_tree_root\tp2p_ns_map;\n\n\tspinlock_t\t\terror_lock;\n\tu64\t\t\terr_counter;\n\tstruct nvme_error_slot\tslots[NVMET_ERROR_LOG_SLOTS];\n\tbool\t\t\tpi_support;\n#ifdef CONFIG_NVME_TARGET_AUTH\n\tstruct nvme_dhchap_key\t*host_key;\n\tstruct nvme_dhchap_key\t*ctrl_key;\n\tu8\t\t\tshash_id;\n\tstruct crypto_kpp\t*dh_tfm;\n\tu8\t\t\tdh_gid;\n\tu8\t\t\t*dh_key;\n\tsize_t\t\t\tdh_keysize;\n#endif\n};\n\nstruct nvmet_subsys {\n\tenum nvme_subsys_type\ttype;\n\n\tstruct mutex\t\tlock;\n\tstruct kref\t\tref;\n\n\tstruct xarray\t\tnamespaces;\n\tunsigned int\t\tnr_namespaces;\n\tu32\t\t\tmax_nsid;\n\tu16\t\t\tcntlid_min;\n\tu16\t\t\tcntlid_max;\n\n\tstruct list_head\tctrls;\n\n\tstruct list_head\thosts;\n\tbool\t\t\tallow_any_host;\n\n\tu16\t\t\tmax_qid;\n\n\tu64\t\t\tver;\n\tchar\t\t\tserial[NVMET_SN_MAX_SIZE];\n\tbool\t\t\tsubsys_discovered;\n\tchar\t\t\t*subsysnqn;\n\tbool\t\t\tpi_support;\n\n\tstruct config_group\tgroup;\n\n\tstruct config_group\tnamespaces_group;\n\tstruct config_group\tallowed_hosts_group;\n\n\tchar\t\t\t*model_number;\n\tu32\t\t\tieee_oui;\n\tchar\t\t\t*firmware_rev;\n\n#ifdef CONFIG_NVME_TARGET_PASSTHRU\n\tstruct nvme_ctrl\t*passthru_ctrl;\n\tchar\t\t\t*passthru_ctrl_path;\n\tstruct config_group\tpassthru_group;\n\tunsigned int\t\tadmin_timeout;\n\tunsigned int\t\tio_timeout;\n\tunsigned int\t\tclear_ids;\n#endif  \n\n#ifdef CONFIG_BLK_DEV_ZONED\n\tu8\t\t\tzasl;\n#endif  \n};\n\nstatic inline struct nvmet_subsys *to_subsys(struct config_item *item)\n{\n\treturn container_of(to_config_group(item), struct nvmet_subsys, group);\n}\n\nstatic inline struct nvmet_subsys *namespaces_to_subsys(\n\t\tstruct config_item *item)\n{\n\treturn container_of(to_config_group(item), struct nvmet_subsys,\n\t\t\tnamespaces_group);\n}\n\nstruct nvmet_host {\n\tstruct config_group\tgroup;\n\tu8\t\t\t*dhchap_secret;\n\tu8\t\t\t*dhchap_ctrl_secret;\n\tu8\t\t\tdhchap_key_hash;\n\tu8\t\t\tdhchap_ctrl_key_hash;\n\tu8\t\t\tdhchap_hash_id;\n\tu8\t\t\tdhchap_dhgroup_id;\n};\n\nstatic inline struct nvmet_host *to_host(struct config_item *item)\n{\n\treturn container_of(to_config_group(item), struct nvmet_host, group);\n}\n\nstatic inline char *nvmet_host_name(struct nvmet_host *host)\n{\n\treturn config_item_name(&host->group.cg_item);\n}\n\nstruct nvmet_host_link {\n\tstruct list_head\tentry;\n\tstruct nvmet_host\t*host;\n};\n\nstruct nvmet_subsys_link {\n\tstruct list_head\tentry;\n\tstruct nvmet_subsys\t*subsys;\n};\n\nstruct nvmet_req;\nstruct nvmet_fabrics_ops {\n\tstruct module *owner;\n\tunsigned int type;\n\tunsigned int msdbd;\n\tunsigned int flags;\n#define NVMF_KEYED_SGLS\t\t\t(1 << 0)\n#define NVMF_METADATA_SUPPORTED\t\t(1 << 1)\n\tvoid (*queue_response)(struct nvmet_req *req);\n\tint (*add_port)(struct nvmet_port *port);\n\tvoid (*remove_port)(struct nvmet_port *port);\n\tvoid (*delete_ctrl)(struct nvmet_ctrl *ctrl);\n\tvoid (*disc_traddr)(struct nvmet_req *req,\n\t\t\tstruct nvmet_port *port, char *traddr);\n\tu16 (*install_queue)(struct nvmet_sq *nvme_sq);\n\tvoid (*discovery_chg)(struct nvmet_port *port);\n\tu8 (*get_mdts)(const struct nvmet_ctrl *ctrl);\n\tu16 (*get_max_queue_size)(const struct nvmet_ctrl *ctrl);\n};\n\n#define NVMET_MAX_INLINE_BIOVEC\t8\n#define NVMET_MAX_INLINE_DATA_LEN NVMET_MAX_INLINE_BIOVEC * PAGE_SIZE\n\nstruct nvmet_req {\n\tstruct nvme_command\t*cmd;\n\tstruct nvme_completion\t*cqe;\n\tstruct nvmet_sq\t\t*sq;\n\tstruct nvmet_cq\t\t*cq;\n\tstruct nvmet_ns\t\t*ns;\n\tstruct scatterlist\t*sg;\n\tstruct scatterlist\t*metadata_sg;\n\tstruct bio_vec\t\tinline_bvec[NVMET_MAX_INLINE_BIOVEC];\n\tunion {\n\t\tstruct {\n\t\t\tstruct bio      inline_bio;\n\t\t} b;\n\t\tstruct {\n\t\t\tbool\t\t\tmpool_alloc;\n\t\t\tstruct kiocb            iocb;\n\t\t\tstruct bio_vec          *bvec;\n\t\t\tstruct work_struct      work;\n\t\t} f;\n\t\tstruct {\n\t\t\tstruct bio\t\tinline_bio;\n\t\t\tstruct request\t\t*rq;\n\t\t\tstruct work_struct      work;\n\t\t\tbool\t\t\tuse_workqueue;\n\t\t} p;\n#ifdef CONFIG_BLK_DEV_ZONED\n\t\tstruct {\n\t\t\tstruct bio\t\tinline_bio;\n\t\t\tstruct work_struct\tzmgmt_work;\n\t\t} z;\n#endif  \n\t};\n\tint\t\t\tsg_cnt;\n\tint\t\t\tmetadata_sg_cnt;\n\t \n\tsize_t\t\t\ttransfer_len;\n\tsize_t\t\t\tmetadata_len;\n\n\tstruct nvmet_port\t*port;\n\n\tvoid (*execute)(struct nvmet_req *req);\n\tconst struct nvmet_fabrics_ops *ops;\n\n\tstruct pci_dev\t\t*p2p_dev;\n\tstruct device\t\t*p2p_client;\n\tu16\t\t\terror_loc;\n\tu64\t\t\terror_slba;\n};\n\n#define NVMET_MAX_MPOOL_BVEC\t\t16\nextern struct kmem_cache *nvmet_bvec_cache;\nextern struct workqueue_struct *buffered_io_wq;\nextern struct workqueue_struct *zbd_wq;\nextern struct workqueue_struct *nvmet_wq;\n\nstatic inline void nvmet_set_result(struct nvmet_req *req, u32 result)\n{\n\treq->cqe->result.u32 = cpu_to_le32(result);\n}\n\n \nstatic inline enum dma_data_direction\nnvmet_data_dir(struct nvmet_req *req)\n{\n\treturn nvme_is_write(req->cmd) ? DMA_FROM_DEVICE : DMA_TO_DEVICE;\n}\n\nstruct nvmet_async_event {\n\tstruct list_head\tentry;\n\tu8\t\t\tevent_type;\n\tu8\t\t\tevent_info;\n\tu8\t\t\tlog_page;\n};\n\nstatic inline void nvmet_clear_aen_bit(struct nvmet_req *req, u32 bn)\n{\n\tint rae = le32_to_cpu(req->cmd->common.cdw10) & 1 << 15;\n\n\tif (!rae)\n\t\tclear_bit(bn, &req->sq->ctrl->aen_masked);\n}\n\nstatic inline bool nvmet_aen_bit_disabled(struct nvmet_ctrl *ctrl, u32 bn)\n{\n\tif (!(READ_ONCE(ctrl->aen_enabled) & (1 << bn)))\n\t\treturn true;\n\treturn test_and_set_bit(bn, &ctrl->aen_masked);\n}\n\nvoid nvmet_get_feat_kato(struct nvmet_req *req);\nvoid nvmet_get_feat_async_event(struct nvmet_req *req);\nu16 nvmet_set_feat_kato(struct nvmet_req *req);\nu16 nvmet_set_feat_async_event(struct nvmet_req *req, u32 mask);\nvoid nvmet_execute_async_event(struct nvmet_req *req);\nvoid nvmet_start_keep_alive_timer(struct nvmet_ctrl *ctrl);\nvoid nvmet_stop_keep_alive_timer(struct nvmet_ctrl *ctrl);\n\nu16 nvmet_parse_connect_cmd(struct nvmet_req *req);\nvoid nvmet_bdev_set_limits(struct block_device *bdev, struct nvme_id_ns *id);\nu16 nvmet_bdev_parse_io_cmd(struct nvmet_req *req);\nu16 nvmet_file_parse_io_cmd(struct nvmet_req *req);\nu16 nvmet_bdev_zns_parse_io_cmd(struct nvmet_req *req);\nu16 nvmet_parse_admin_cmd(struct nvmet_req *req);\nu16 nvmet_parse_discovery_cmd(struct nvmet_req *req);\nu16 nvmet_parse_fabrics_admin_cmd(struct nvmet_req *req);\nu16 nvmet_parse_fabrics_io_cmd(struct nvmet_req *req);\n\nbool nvmet_req_init(struct nvmet_req *req, struct nvmet_cq *cq,\n\t\tstruct nvmet_sq *sq, const struct nvmet_fabrics_ops *ops);\nvoid nvmet_req_uninit(struct nvmet_req *req);\nbool nvmet_check_transfer_len(struct nvmet_req *req, size_t len);\nbool nvmet_check_data_len_lte(struct nvmet_req *req, size_t data_len);\nvoid nvmet_req_complete(struct nvmet_req *req, u16 status);\nint nvmet_req_alloc_sgls(struct nvmet_req *req);\nvoid nvmet_req_free_sgls(struct nvmet_req *req);\n\nvoid nvmet_execute_set_features(struct nvmet_req *req);\nvoid nvmet_execute_get_features(struct nvmet_req *req);\nvoid nvmet_execute_keep_alive(struct nvmet_req *req);\n\nvoid nvmet_cq_setup(struct nvmet_ctrl *ctrl, struct nvmet_cq *cq, u16 qid,\n\t\tu16 size);\nvoid nvmet_sq_setup(struct nvmet_ctrl *ctrl, struct nvmet_sq *sq, u16 qid,\n\t\tu16 size);\nvoid nvmet_sq_destroy(struct nvmet_sq *sq);\nint nvmet_sq_init(struct nvmet_sq *sq);\n\nvoid nvmet_ctrl_fatal_error(struct nvmet_ctrl *ctrl);\n\nvoid nvmet_update_cc(struct nvmet_ctrl *ctrl, u32 new);\nu16 nvmet_alloc_ctrl(const char *subsysnqn, const char *hostnqn,\n\t\tstruct nvmet_req *req, u32 kato, struct nvmet_ctrl **ctrlp);\nstruct nvmet_ctrl *nvmet_ctrl_find_get(const char *subsysnqn,\n\t\t\t\t       const char *hostnqn, u16 cntlid,\n\t\t\t\t       struct nvmet_req *req);\nvoid nvmet_ctrl_put(struct nvmet_ctrl *ctrl);\nu16 nvmet_check_ctrl_status(struct nvmet_req *req);\n\nstruct nvmet_subsys *nvmet_subsys_alloc(const char *subsysnqn,\n\t\tenum nvme_subsys_type type);\nvoid nvmet_subsys_put(struct nvmet_subsys *subsys);\nvoid nvmet_subsys_del_ctrls(struct nvmet_subsys *subsys);\n\nu16 nvmet_req_find_ns(struct nvmet_req *req);\nvoid nvmet_put_namespace(struct nvmet_ns *ns);\nint nvmet_ns_enable(struct nvmet_ns *ns);\nvoid nvmet_ns_disable(struct nvmet_ns *ns);\nstruct nvmet_ns *nvmet_ns_alloc(struct nvmet_subsys *subsys, u32 nsid);\nvoid nvmet_ns_free(struct nvmet_ns *ns);\n\nvoid nvmet_send_ana_event(struct nvmet_subsys *subsys,\n\t\tstruct nvmet_port *port);\nvoid nvmet_port_send_ana_event(struct nvmet_port *port);\n\nint nvmet_register_transport(const struct nvmet_fabrics_ops *ops);\nvoid nvmet_unregister_transport(const struct nvmet_fabrics_ops *ops);\n\nvoid nvmet_port_del_ctrls(struct nvmet_port *port,\n\t\t\t  struct nvmet_subsys *subsys);\n\nint nvmet_enable_port(struct nvmet_port *port);\nvoid nvmet_disable_port(struct nvmet_port *port);\n\nvoid nvmet_referral_enable(struct nvmet_port *parent, struct nvmet_port *port);\nvoid nvmet_referral_disable(struct nvmet_port *parent, struct nvmet_port *port);\n\nu16 nvmet_copy_to_sgl(struct nvmet_req *req, off_t off, const void *buf,\n\t\tsize_t len);\nu16 nvmet_copy_from_sgl(struct nvmet_req *req, off_t off, void *buf,\n\t\tsize_t len);\nu16 nvmet_zero_sgl(struct nvmet_req *req, off_t off, size_t len);\n\nu32 nvmet_get_log_page_len(struct nvme_command *cmd);\nu64 nvmet_get_log_page_offset(struct nvme_command *cmd);\n\nextern struct list_head *nvmet_ports;\nvoid nvmet_port_disc_changed(struct nvmet_port *port,\n\t\tstruct nvmet_subsys *subsys);\nvoid nvmet_subsys_disc_changed(struct nvmet_subsys *subsys,\n\t\tstruct nvmet_host *host);\nvoid nvmet_add_async_event(struct nvmet_ctrl *ctrl, u8 event_type,\n\t\tu8 event_info, u8 log_page);\n\n#define NVMET_QUEUE_SIZE\t1024\n#define NVMET_NR_QUEUES\t\t128\n#define NVMET_MAX_CMD\t\tNVMET_QUEUE_SIZE\n\n \n#define NVMET_MAX_NAMESPACES\t1024\n\n \n#define NVMET_MAX_ANAGRPS\t128\n#define NVMET_DEFAULT_ANA_GRPID\t1\n\n#define NVMET_KAS\t\t10\n#define NVMET_DISC_KATO_MS\t\t120000\n\nint __init nvmet_init_configfs(void);\nvoid __exit nvmet_exit_configfs(void);\n\nint __init nvmet_init_discovery(void);\nvoid nvmet_exit_discovery(void);\n\nextern struct nvmet_subsys *nvmet_disc_subsys;\nextern struct rw_semaphore nvmet_config_sem;\n\nextern u32 nvmet_ana_group_enabled[NVMET_MAX_ANAGRPS + 1];\nextern u64 nvmet_ana_chgcnt;\nextern struct rw_semaphore nvmet_ana_sem;\n\nbool nvmet_host_allowed(struct nvmet_subsys *subsys, const char *hostnqn);\n\nint nvmet_bdev_ns_enable(struct nvmet_ns *ns);\nint nvmet_file_ns_enable(struct nvmet_ns *ns);\nvoid nvmet_bdev_ns_disable(struct nvmet_ns *ns);\nvoid nvmet_file_ns_disable(struct nvmet_ns *ns);\nu16 nvmet_bdev_flush(struct nvmet_req *req);\nu16 nvmet_file_flush(struct nvmet_req *req);\nvoid nvmet_ns_changed(struct nvmet_subsys *subsys, u32 nsid);\nvoid nvmet_bdev_ns_revalidate(struct nvmet_ns *ns);\nvoid nvmet_file_ns_revalidate(struct nvmet_ns *ns);\nbool nvmet_ns_revalidate(struct nvmet_ns *ns);\nu16 blk_to_nvme_status(struct nvmet_req *req, blk_status_t blk_sts);\n\nbool nvmet_bdev_zns_enable(struct nvmet_ns *ns);\nvoid nvmet_execute_identify_ctrl_zns(struct nvmet_req *req);\nvoid nvmet_execute_identify_ns_zns(struct nvmet_req *req);\nvoid nvmet_bdev_execute_zone_mgmt_recv(struct nvmet_req *req);\nvoid nvmet_bdev_execute_zone_mgmt_send(struct nvmet_req *req);\nvoid nvmet_bdev_execute_zone_append(struct nvmet_req *req);\n\nstatic inline u32 nvmet_rw_data_len(struct nvmet_req *req)\n{\n\treturn ((u32)le16_to_cpu(req->cmd->rw.length) + 1) <<\n\t\t\treq->ns->blksize_shift;\n}\n\nstatic inline u32 nvmet_rw_metadata_len(struct nvmet_req *req)\n{\n\tif (!IS_ENABLED(CONFIG_BLK_DEV_INTEGRITY))\n\t\treturn 0;\n\treturn ((u32)le16_to_cpu(req->cmd->rw.length) + 1) *\n\t\t\treq->ns->metadata_size;\n}\n\nstatic inline u32 nvmet_dsm_len(struct nvmet_req *req)\n{\n\treturn (le32_to_cpu(req->cmd->dsm.nr) + 1) *\n\t\tsizeof(struct nvme_dsm_range);\n}\n\nstatic inline struct nvmet_subsys *nvmet_req_subsys(struct nvmet_req *req)\n{\n\treturn req->sq->ctrl->subsys;\n}\n\nstatic inline bool nvmet_is_disc_subsys(struct nvmet_subsys *subsys)\n{\n    return subsys->type != NVME_NQN_NVME;\n}\n\n#ifdef CONFIG_NVME_TARGET_PASSTHRU\nvoid nvmet_passthru_subsys_free(struct nvmet_subsys *subsys);\nint nvmet_passthru_ctrl_enable(struct nvmet_subsys *subsys);\nvoid nvmet_passthru_ctrl_disable(struct nvmet_subsys *subsys);\nu16 nvmet_parse_passthru_admin_cmd(struct nvmet_req *req);\nu16 nvmet_parse_passthru_io_cmd(struct nvmet_req *req);\nstatic inline bool nvmet_is_passthru_subsys(struct nvmet_subsys *subsys)\n{\n\treturn subsys->passthru_ctrl;\n}\n#else  \nstatic inline void nvmet_passthru_subsys_free(struct nvmet_subsys *subsys)\n{\n}\nstatic inline void nvmet_passthru_ctrl_disable(struct nvmet_subsys *subsys)\n{\n}\nstatic inline u16 nvmet_parse_passthru_admin_cmd(struct nvmet_req *req)\n{\n\treturn 0;\n}\nstatic inline u16 nvmet_parse_passthru_io_cmd(struct nvmet_req *req)\n{\n\treturn 0;\n}\nstatic inline bool nvmet_is_passthru_subsys(struct nvmet_subsys *subsys)\n{\n\treturn NULL;\n}\n#endif  \n\nstatic inline bool nvmet_is_passthru_req(struct nvmet_req *req)\n{\n\treturn nvmet_is_passthru_subsys(nvmet_req_subsys(req));\n}\n\nvoid nvmet_passthrough_override_cap(struct nvmet_ctrl *ctrl);\n\nu16 errno_to_nvme_status(struct nvmet_req *req, int errno);\nu16 nvmet_report_invalid_opcode(struct nvmet_req *req);\n\n \nstatic inline __le16 to0based(u32 a)\n{\n\treturn cpu_to_le16(max(1U, min(1U << 16, a)) - 1);\n}\n\nstatic inline bool nvmet_ns_has_pi(struct nvmet_ns *ns)\n{\n\tif (!IS_ENABLED(CONFIG_BLK_DEV_INTEGRITY))\n\t\treturn false;\n\treturn ns->pi_type && ns->metadata_size == sizeof(struct t10_pi_tuple);\n}\n\nstatic inline __le64 nvmet_sect_to_lba(struct nvmet_ns *ns, sector_t sect)\n{\n\treturn cpu_to_le64(sect >> (ns->blksize_shift - SECTOR_SHIFT));\n}\n\nstatic inline sector_t nvmet_lba_to_sect(struct nvmet_ns *ns, __le64 lba)\n{\n\treturn le64_to_cpu(lba) << (ns->blksize_shift - SECTOR_SHIFT);\n}\n\nstatic inline bool nvmet_use_inline_bvec(struct nvmet_req *req)\n{\n\treturn req->transfer_len <= NVMET_MAX_INLINE_DATA_LEN &&\n\t       req->sg_cnt <= NVMET_MAX_INLINE_BIOVEC;\n}\n\nstatic inline void nvmet_req_bio_put(struct nvmet_req *req, struct bio *bio)\n{\n\tif (bio != &req->b.inline_bio)\n\t\tbio_put(bio);\n}\n\n#ifdef CONFIG_NVME_TARGET_AUTH\nvoid nvmet_execute_auth_send(struct nvmet_req *req);\nvoid nvmet_execute_auth_receive(struct nvmet_req *req);\nint nvmet_auth_set_key(struct nvmet_host *host, const char *secret,\n\t\t       bool set_ctrl);\nint nvmet_auth_set_host_hash(struct nvmet_host *host, const char *hash);\nint nvmet_setup_auth(struct nvmet_ctrl *ctrl);\nvoid nvmet_auth_sq_init(struct nvmet_sq *sq);\nvoid nvmet_destroy_auth(struct nvmet_ctrl *ctrl);\nvoid nvmet_auth_sq_free(struct nvmet_sq *sq);\nint nvmet_setup_dhgroup(struct nvmet_ctrl *ctrl, u8 dhgroup_id);\nbool nvmet_check_auth_status(struct nvmet_req *req);\nint nvmet_auth_host_hash(struct nvmet_req *req, u8 *response,\n\t\t\t unsigned int hash_len);\nint nvmet_auth_ctrl_hash(struct nvmet_req *req, u8 *response,\n\t\t\t unsigned int hash_len);\nstatic inline bool nvmet_has_auth(struct nvmet_ctrl *ctrl)\n{\n\treturn ctrl->host_key != NULL;\n}\nint nvmet_auth_ctrl_exponential(struct nvmet_req *req,\n\t\t\t\tu8 *buf, int buf_size);\nint nvmet_auth_ctrl_sesskey(struct nvmet_req *req,\n\t\t\t    u8 *buf, int buf_size);\n#else\nstatic inline int nvmet_setup_auth(struct nvmet_ctrl *ctrl)\n{\n\treturn 0;\n}\nstatic inline void nvmet_auth_sq_init(struct nvmet_sq *sq)\n{\n}\nstatic inline void nvmet_destroy_auth(struct nvmet_ctrl *ctrl) {};\nstatic inline void nvmet_auth_sq_free(struct nvmet_sq *sq) {};\nstatic inline bool nvmet_check_auth_status(struct nvmet_req *req)\n{\n\treturn true;\n}\nstatic inline bool nvmet_has_auth(struct nvmet_ctrl *ctrl)\n{\n\treturn false;\n}\nstatic inline const char *nvmet_dhchap_dhgroup_name(u8 dhgid) { return NULL; }\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}