{
  "module_name": "core.c",
  "hash_id": "1a5148a3fa51ff89fbcee7ea7294936b5ce2be2ba38035d34d0b4221fe461f64",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvme/target/core.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n#include <linux/module.h>\n#include <linux/random.h>\n#include <linux/rculist.h>\n#include <linux/pci-p2pdma.h>\n#include <linux/scatterlist.h>\n\n#include <generated/utsrelease.h>\n\n#define CREATE_TRACE_POINTS\n#include \"trace.h\"\n\n#include \"nvmet.h\"\n\nstruct kmem_cache *nvmet_bvec_cache;\nstruct workqueue_struct *buffered_io_wq;\nstruct workqueue_struct *zbd_wq;\nstatic const struct nvmet_fabrics_ops *nvmet_transports[NVMF_TRTYPE_MAX];\nstatic DEFINE_IDA(cntlid_ida);\n\nstruct workqueue_struct *nvmet_wq;\nEXPORT_SYMBOL_GPL(nvmet_wq);\n\n \nDECLARE_RWSEM(nvmet_config_sem);\n\nu32 nvmet_ana_group_enabled[NVMET_MAX_ANAGRPS + 1];\nu64 nvmet_ana_chgcnt;\nDECLARE_RWSEM(nvmet_ana_sem);\n\ninline u16 errno_to_nvme_status(struct nvmet_req *req, int errno)\n{\n\tswitch (errno) {\n\tcase 0:\n\t\treturn NVME_SC_SUCCESS;\n\tcase -ENOSPC:\n\t\treq->error_loc = offsetof(struct nvme_rw_command, length);\n\t\treturn NVME_SC_CAP_EXCEEDED | NVME_SC_DNR;\n\tcase -EREMOTEIO:\n\t\treq->error_loc = offsetof(struct nvme_rw_command, slba);\n\t\treturn  NVME_SC_LBA_RANGE | NVME_SC_DNR;\n\tcase -EOPNOTSUPP:\n\t\treq->error_loc = offsetof(struct nvme_common_command, opcode);\n\t\tswitch (req->cmd->common.opcode) {\n\t\tcase nvme_cmd_dsm:\n\t\tcase nvme_cmd_write_zeroes:\n\t\t\treturn NVME_SC_ONCS_NOT_SUPPORTED | NVME_SC_DNR;\n\t\tdefault:\n\t\t\treturn NVME_SC_INVALID_OPCODE | NVME_SC_DNR;\n\t\t}\n\t\tbreak;\n\tcase -ENODATA:\n\t\treq->error_loc = offsetof(struct nvme_rw_command, nsid);\n\t\treturn NVME_SC_ACCESS_DENIED;\n\tcase -EIO:\n\t\tfallthrough;\n\tdefault:\n\t\treq->error_loc = offsetof(struct nvme_common_command, opcode);\n\t\treturn NVME_SC_INTERNAL | NVME_SC_DNR;\n\t}\n}\n\nu16 nvmet_report_invalid_opcode(struct nvmet_req *req)\n{\n\tpr_debug(\"unhandled cmd %d on qid %d\\n\", req->cmd->common.opcode,\n\t\t req->sq->qid);\n\n\treq->error_loc = offsetof(struct nvme_common_command, opcode);\n\treturn NVME_SC_INVALID_OPCODE | NVME_SC_DNR;\n}\n\nstatic struct nvmet_subsys *nvmet_find_get_subsys(struct nvmet_port *port,\n\t\tconst char *subsysnqn);\n\nu16 nvmet_copy_to_sgl(struct nvmet_req *req, off_t off, const void *buf,\n\t\tsize_t len)\n{\n\tif (sg_pcopy_from_buffer(req->sg, req->sg_cnt, buf, len, off) != len) {\n\t\treq->error_loc = offsetof(struct nvme_common_command, dptr);\n\t\treturn NVME_SC_SGL_INVALID_DATA | NVME_SC_DNR;\n\t}\n\treturn 0;\n}\n\nu16 nvmet_copy_from_sgl(struct nvmet_req *req, off_t off, void *buf, size_t len)\n{\n\tif (sg_pcopy_to_buffer(req->sg, req->sg_cnt, buf, len, off) != len) {\n\t\treq->error_loc = offsetof(struct nvme_common_command, dptr);\n\t\treturn NVME_SC_SGL_INVALID_DATA | NVME_SC_DNR;\n\t}\n\treturn 0;\n}\n\nu16 nvmet_zero_sgl(struct nvmet_req *req, off_t off, size_t len)\n{\n\tif (sg_zero_buffer(req->sg, req->sg_cnt, len, off) != len) {\n\t\treq->error_loc = offsetof(struct nvme_common_command, dptr);\n\t\treturn NVME_SC_SGL_INVALID_DATA | NVME_SC_DNR;\n\t}\n\treturn 0;\n}\n\nstatic u32 nvmet_max_nsid(struct nvmet_subsys *subsys)\n{\n\tstruct nvmet_ns *cur;\n\tunsigned long idx;\n\tu32 nsid = 0;\n\n\txa_for_each(&subsys->namespaces, idx, cur)\n\t\tnsid = cur->nsid;\n\n\treturn nsid;\n}\n\nstatic u32 nvmet_async_event_result(struct nvmet_async_event *aen)\n{\n\treturn aen->event_type | (aen->event_info << 8) | (aen->log_page << 16);\n}\n\nstatic void nvmet_async_events_failall(struct nvmet_ctrl *ctrl)\n{\n\tstruct nvmet_req *req;\n\n\tmutex_lock(&ctrl->lock);\n\twhile (ctrl->nr_async_event_cmds) {\n\t\treq = ctrl->async_event_cmds[--ctrl->nr_async_event_cmds];\n\t\tmutex_unlock(&ctrl->lock);\n\t\tnvmet_req_complete(req, NVME_SC_INTERNAL | NVME_SC_DNR);\n\t\tmutex_lock(&ctrl->lock);\n\t}\n\tmutex_unlock(&ctrl->lock);\n}\n\nstatic void nvmet_async_events_process(struct nvmet_ctrl *ctrl)\n{\n\tstruct nvmet_async_event *aen;\n\tstruct nvmet_req *req;\n\n\tmutex_lock(&ctrl->lock);\n\twhile (ctrl->nr_async_event_cmds && !list_empty(&ctrl->async_events)) {\n\t\taen = list_first_entry(&ctrl->async_events,\n\t\t\t\t       struct nvmet_async_event, entry);\n\t\treq = ctrl->async_event_cmds[--ctrl->nr_async_event_cmds];\n\t\tnvmet_set_result(req, nvmet_async_event_result(aen));\n\n\t\tlist_del(&aen->entry);\n\t\tkfree(aen);\n\n\t\tmutex_unlock(&ctrl->lock);\n\t\ttrace_nvmet_async_event(ctrl, req->cqe->result.u32);\n\t\tnvmet_req_complete(req, 0);\n\t\tmutex_lock(&ctrl->lock);\n\t}\n\tmutex_unlock(&ctrl->lock);\n}\n\nstatic void nvmet_async_events_free(struct nvmet_ctrl *ctrl)\n{\n\tstruct nvmet_async_event *aen, *tmp;\n\n\tmutex_lock(&ctrl->lock);\n\tlist_for_each_entry_safe(aen, tmp, &ctrl->async_events, entry) {\n\t\tlist_del(&aen->entry);\n\t\tkfree(aen);\n\t}\n\tmutex_unlock(&ctrl->lock);\n}\n\nstatic void nvmet_async_event_work(struct work_struct *work)\n{\n\tstruct nvmet_ctrl *ctrl =\n\t\tcontainer_of(work, struct nvmet_ctrl, async_event_work);\n\n\tnvmet_async_events_process(ctrl);\n}\n\nvoid nvmet_add_async_event(struct nvmet_ctrl *ctrl, u8 event_type,\n\t\tu8 event_info, u8 log_page)\n{\n\tstruct nvmet_async_event *aen;\n\n\taen = kmalloc(sizeof(*aen), GFP_KERNEL);\n\tif (!aen)\n\t\treturn;\n\n\taen->event_type = event_type;\n\taen->event_info = event_info;\n\taen->log_page = log_page;\n\n\tmutex_lock(&ctrl->lock);\n\tlist_add_tail(&aen->entry, &ctrl->async_events);\n\tmutex_unlock(&ctrl->lock);\n\n\tqueue_work(nvmet_wq, &ctrl->async_event_work);\n}\n\nstatic void nvmet_add_to_changed_ns_log(struct nvmet_ctrl *ctrl, __le32 nsid)\n{\n\tu32 i;\n\n\tmutex_lock(&ctrl->lock);\n\tif (ctrl->nr_changed_ns > NVME_MAX_CHANGED_NAMESPACES)\n\t\tgoto out_unlock;\n\n\tfor (i = 0; i < ctrl->nr_changed_ns; i++) {\n\t\tif (ctrl->changed_ns_list[i] == nsid)\n\t\t\tgoto out_unlock;\n\t}\n\n\tif (ctrl->nr_changed_ns == NVME_MAX_CHANGED_NAMESPACES) {\n\t\tctrl->changed_ns_list[0] = cpu_to_le32(0xffffffff);\n\t\tctrl->nr_changed_ns = U32_MAX;\n\t\tgoto out_unlock;\n\t}\n\n\tctrl->changed_ns_list[ctrl->nr_changed_ns++] = nsid;\nout_unlock:\n\tmutex_unlock(&ctrl->lock);\n}\n\nvoid nvmet_ns_changed(struct nvmet_subsys *subsys, u32 nsid)\n{\n\tstruct nvmet_ctrl *ctrl;\n\n\tlockdep_assert_held(&subsys->lock);\n\n\tlist_for_each_entry(ctrl, &subsys->ctrls, subsys_entry) {\n\t\tnvmet_add_to_changed_ns_log(ctrl, cpu_to_le32(nsid));\n\t\tif (nvmet_aen_bit_disabled(ctrl, NVME_AEN_BIT_NS_ATTR))\n\t\t\tcontinue;\n\t\tnvmet_add_async_event(ctrl, NVME_AER_TYPE_NOTICE,\n\t\t\t\tNVME_AER_NOTICE_NS_CHANGED,\n\t\t\t\tNVME_LOG_CHANGED_NS);\n\t}\n}\n\nvoid nvmet_send_ana_event(struct nvmet_subsys *subsys,\n\t\tstruct nvmet_port *port)\n{\n\tstruct nvmet_ctrl *ctrl;\n\n\tmutex_lock(&subsys->lock);\n\tlist_for_each_entry(ctrl, &subsys->ctrls, subsys_entry) {\n\t\tif (port && ctrl->port != port)\n\t\t\tcontinue;\n\t\tif (nvmet_aen_bit_disabled(ctrl, NVME_AEN_BIT_ANA_CHANGE))\n\t\t\tcontinue;\n\t\tnvmet_add_async_event(ctrl, NVME_AER_TYPE_NOTICE,\n\t\t\t\tNVME_AER_NOTICE_ANA, NVME_LOG_ANA);\n\t}\n\tmutex_unlock(&subsys->lock);\n}\n\nvoid nvmet_port_send_ana_event(struct nvmet_port *port)\n{\n\tstruct nvmet_subsys_link *p;\n\n\tdown_read(&nvmet_config_sem);\n\tlist_for_each_entry(p, &port->subsystems, entry)\n\t\tnvmet_send_ana_event(p->subsys, port);\n\tup_read(&nvmet_config_sem);\n}\n\nint nvmet_register_transport(const struct nvmet_fabrics_ops *ops)\n{\n\tint ret = 0;\n\n\tdown_write(&nvmet_config_sem);\n\tif (nvmet_transports[ops->type])\n\t\tret = -EINVAL;\n\telse\n\t\tnvmet_transports[ops->type] = ops;\n\tup_write(&nvmet_config_sem);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nvmet_register_transport);\n\nvoid nvmet_unregister_transport(const struct nvmet_fabrics_ops *ops)\n{\n\tdown_write(&nvmet_config_sem);\n\tnvmet_transports[ops->type] = NULL;\n\tup_write(&nvmet_config_sem);\n}\nEXPORT_SYMBOL_GPL(nvmet_unregister_transport);\n\nvoid nvmet_port_del_ctrls(struct nvmet_port *port, struct nvmet_subsys *subsys)\n{\n\tstruct nvmet_ctrl *ctrl;\n\n\tmutex_lock(&subsys->lock);\n\tlist_for_each_entry(ctrl, &subsys->ctrls, subsys_entry) {\n\t\tif (ctrl->port == port)\n\t\t\tctrl->ops->delete_ctrl(ctrl);\n\t}\n\tmutex_unlock(&subsys->lock);\n}\n\nint nvmet_enable_port(struct nvmet_port *port)\n{\n\tconst struct nvmet_fabrics_ops *ops;\n\tint ret;\n\n\tlockdep_assert_held(&nvmet_config_sem);\n\n\tops = nvmet_transports[port->disc_addr.trtype];\n\tif (!ops) {\n\t\tup_write(&nvmet_config_sem);\n\t\trequest_module(\"nvmet-transport-%d\", port->disc_addr.trtype);\n\t\tdown_write(&nvmet_config_sem);\n\t\tops = nvmet_transports[port->disc_addr.trtype];\n\t\tif (!ops) {\n\t\t\tpr_err(\"transport type %d not supported\\n\",\n\t\t\t\tport->disc_addr.trtype);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (!try_module_get(ops->owner))\n\t\treturn -EINVAL;\n\n\t \n\tif (port->pi_enable && !(ops->flags & NVMF_METADATA_SUPPORTED)) {\n\t\tpr_err(\"T10-PI is not supported by transport type %d\\n\",\n\t\t       port->disc_addr.trtype);\n\t\tret = -EINVAL;\n\t\tgoto out_put;\n\t}\n\n\tret = ops->add_port(port);\n\tif (ret)\n\t\tgoto out_put;\n\n\t \n\tif (port->inline_data_size < 0)\n\t\tport->inline_data_size = 0;\n\n\tport->enabled = true;\n\tport->tr_ops = ops;\n\treturn 0;\n\nout_put:\n\tmodule_put(ops->owner);\n\treturn ret;\n}\n\nvoid nvmet_disable_port(struct nvmet_port *port)\n{\n\tconst struct nvmet_fabrics_ops *ops;\n\n\tlockdep_assert_held(&nvmet_config_sem);\n\n\tport->enabled = false;\n\tport->tr_ops = NULL;\n\n\tops = nvmet_transports[port->disc_addr.trtype];\n\tops->remove_port(port);\n\tmodule_put(ops->owner);\n}\n\nstatic void nvmet_keep_alive_timer(struct work_struct *work)\n{\n\tstruct nvmet_ctrl *ctrl = container_of(to_delayed_work(work),\n\t\t\tstruct nvmet_ctrl, ka_work);\n\tbool reset_tbkas = ctrl->reset_tbkas;\n\n\tctrl->reset_tbkas = false;\n\tif (reset_tbkas) {\n\t\tpr_debug(\"ctrl %d reschedule traffic based keep-alive timer\\n\",\n\t\t\tctrl->cntlid);\n\t\tqueue_delayed_work(nvmet_wq, &ctrl->ka_work, ctrl->kato * HZ);\n\t\treturn;\n\t}\n\n\tpr_err(\"ctrl %d keep-alive timer (%d seconds) expired!\\n\",\n\t\tctrl->cntlid, ctrl->kato);\n\n\tnvmet_ctrl_fatal_error(ctrl);\n}\n\nvoid nvmet_start_keep_alive_timer(struct nvmet_ctrl *ctrl)\n{\n\tif (unlikely(ctrl->kato == 0))\n\t\treturn;\n\n\tpr_debug(\"ctrl %d start keep-alive timer for %d secs\\n\",\n\t\tctrl->cntlid, ctrl->kato);\n\n\tqueue_delayed_work(nvmet_wq, &ctrl->ka_work, ctrl->kato * HZ);\n}\n\nvoid nvmet_stop_keep_alive_timer(struct nvmet_ctrl *ctrl)\n{\n\tif (unlikely(ctrl->kato == 0))\n\t\treturn;\n\n\tpr_debug(\"ctrl %d stop keep-alive\\n\", ctrl->cntlid);\n\n\tcancel_delayed_work_sync(&ctrl->ka_work);\n}\n\nu16 nvmet_req_find_ns(struct nvmet_req *req)\n{\n\tu32 nsid = le32_to_cpu(req->cmd->common.nsid);\n\n\treq->ns = xa_load(&nvmet_req_subsys(req)->namespaces, nsid);\n\tif (unlikely(!req->ns)) {\n\t\treq->error_loc = offsetof(struct nvme_common_command, nsid);\n\t\treturn NVME_SC_INVALID_NS | NVME_SC_DNR;\n\t}\n\n\tpercpu_ref_get(&req->ns->ref);\n\treturn NVME_SC_SUCCESS;\n}\n\nstatic void nvmet_destroy_namespace(struct percpu_ref *ref)\n{\n\tstruct nvmet_ns *ns = container_of(ref, struct nvmet_ns, ref);\n\n\tcomplete(&ns->disable_done);\n}\n\nvoid nvmet_put_namespace(struct nvmet_ns *ns)\n{\n\tpercpu_ref_put(&ns->ref);\n}\n\nstatic void nvmet_ns_dev_disable(struct nvmet_ns *ns)\n{\n\tnvmet_bdev_ns_disable(ns);\n\tnvmet_file_ns_disable(ns);\n}\n\nstatic int nvmet_p2pmem_ns_enable(struct nvmet_ns *ns)\n{\n\tint ret;\n\tstruct pci_dev *p2p_dev;\n\n\tif (!ns->use_p2pmem)\n\t\treturn 0;\n\n\tif (!ns->bdev) {\n\t\tpr_err(\"peer-to-peer DMA is not supported by non-block device namespaces\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!blk_queue_pci_p2pdma(ns->bdev->bd_disk->queue)) {\n\t\tpr_err(\"peer-to-peer DMA is not supported by the driver of %s\\n\",\n\t\t       ns->device_path);\n\t\treturn -EINVAL;\n\t}\n\n\tif (ns->p2p_dev) {\n\t\tret = pci_p2pdma_distance(ns->p2p_dev, nvmet_ns_dev(ns), true);\n\t\tif (ret < 0)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\t \n\n\t\tp2p_dev = pci_p2pmem_find(nvmet_ns_dev(ns));\n\t\tif (!p2p_dev) {\n\t\t\tpr_err(\"no peer-to-peer memory is available for %s\\n\",\n\t\t\t       ns->device_path);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tpci_dev_put(p2p_dev);\n\t}\n\n\treturn 0;\n}\n\n \nstatic void nvmet_p2pmem_ns_add_p2p(struct nvmet_ctrl *ctrl,\n\t\t\t\t    struct nvmet_ns *ns)\n{\n\tstruct device *clients[2];\n\tstruct pci_dev *p2p_dev;\n\tint ret;\n\n\tif (!ctrl->p2p_client || !ns->use_p2pmem)\n\t\treturn;\n\n\tif (ns->p2p_dev) {\n\t\tret = pci_p2pdma_distance(ns->p2p_dev, ctrl->p2p_client, true);\n\t\tif (ret < 0)\n\t\t\treturn;\n\n\t\tp2p_dev = pci_dev_get(ns->p2p_dev);\n\t} else {\n\t\tclients[0] = ctrl->p2p_client;\n\t\tclients[1] = nvmet_ns_dev(ns);\n\n\t\tp2p_dev = pci_p2pmem_find_many(clients, ARRAY_SIZE(clients));\n\t\tif (!p2p_dev) {\n\t\t\tpr_err(\"no peer-to-peer memory is available that's supported by %s and %s\\n\",\n\t\t\t       dev_name(ctrl->p2p_client), ns->device_path);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tret = radix_tree_insert(&ctrl->p2p_ns_map, ns->nsid, p2p_dev);\n\tif (ret < 0)\n\t\tpci_dev_put(p2p_dev);\n\n\tpr_info(\"using p2pmem on %s for nsid %d\\n\", pci_name(p2p_dev),\n\t\tns->nsid);\n}\n\nbool nvmet_ns_revalidate(struct nvmet_ns *ns)\n{\n\tloff_t oldsize = ns->size;\n\n\tif (ns->bdev)\n\t\tnvmet_bdev_ns_revalidate(ns);\n\telse\n\t\tnvmet_file_ns_revalidate(ns);\n\n\treturn oldsize != ns->size;\n}\n\nint nvmet_ns_enable(struct nvmet_ns *ns)\n{\n\tstruct nvmet_subsys *subsys = ns->subsys;\n\tstruct nvmet_ctrl *ctrl;\n\tint ret;\n\n\tmutex_lock(&subsys->lock);\n\tret = 0;\n\n\tif (nvmet_is_passthru_subsys(subsys)) {\n\t\tpr_info(\"cannot enable both passthru and regular namespaces for a single subsystem\");\n\t\tgoto out_unlock;\n\t}\n\n\tif (ns->enabled)\n\t\tgoto out_unlock;\n\n\tret = -EMFILE;\n\tif (subsys->nr_namespaces == NVMET_MAX_NAMESPACES)\n\t\tgoto out_unlock;\n\n\tret = nvmet_bdev_ns_enable(ns);\n\tif (ret == -ENOTBLK)\n\t\tret = nvmet_file_ns_enable(ns);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tret = nvmet_p2pmem_ns_enable(ns);\n\tif (ret)\n\t\tgoto out_dev_disable;\n\n\tlist_for_each_entry(ctrl, &subsys->ctrls, subsys_entry)\n\t\tnvmet_p2pmem_ns_add_p2p(ctrl, ns);\n\n\tret = percpu_ref_init(&ns->ref, nvmet_destroy_namespace,\n\t\t\t\t0, GFP_KERNEL);\n\tif (ret)\n\t\tgoto out_dev_put;\n\n\tif (ns->nsid > subsys->max_nsid)\n\t\tsubsys->max_nsid = ns->nsid;\n\n\tret = xa_insert(&subsys->namespaces, ns->nsid, ns, GFP_KERNEL);\n\tif (ret)\n\t\tgoto out_restore_subsys_maxnsid;\n\n\tsubsys->nr_namespaces++;\n\n\tnvmet_ns_changed(subsys, ns->nsid);\n\tns->enabled = true;\n\tret = 0;\nout_unlock:\n\tmutex_unlock(&subsys->lock);\n\treturn ret;\n\nout_restore_subsys_maxnsid:\n\tsubsys->max_nsid = nvmet_max_nsid(subsys);\n\tpercpu_ref_exit(&ns->ref);\nout_dev_put:\n\tlist_for_each_entry(ctrl, &subsys->ctrls, subsys_entry)\n\t\tpci_dev_put(radix_tree_delete(&ctrl->p2p_ns_map, ns->nsid));\nout_dev_disable:\n\tnvmet_ns_dev_disable(ns);\n\tgoto out_unlock;\n}\n\nvoid nvmet_ns_disable(struct nvmet_ns *ns)\n{\n\tstruct nvmet_subsys *subsys = ns->subsys;\n\tstruct nvmet_ctrl *ctrl;\n\n\tmutex_lock(&subsys->lock);\n\tif (!ns->enabled)\n\t\tgoto out_unlock;\n\n\tns->enabled = false;\n\txa_erase(&ns->subsys->namespaces, ns->nsid);\n\tif (ns->nsid == subsys->max_nsid)\n\t\tsubsys->max_nsid = nvmet_max_nsid(subsys);\n\n\tlist_for_each_entry(ctrl, &subsys->ctrls, subsys_entry)\n\t\tpci_dev_put(radix_tree_delete(&ctrl->p2p_ns_map, ns->nsid));\n\n\tmutex_unlock(&subsys->lock);\n\n\t \n\tpercpu_ref_kill(&ns->ref);\n\tsynchronize_rcu();\n\twait_for_completion(&ns->disable_done);\n\tpercpu_ref_exit(&ns->ref);\n\n\tmutex_lock(&subsys->lock);\n\n\tsubsys->nr_namespaces--;\n\tnvmet_ns_changed(subsys, ns->nsid);\n\tnvmet_ns_dev_disable(ns);\nout_unlock:\n\tmutex_unlock(&subsys->lock);\n}\n\nvoid nvmet_ns_free(struct nvmet_ns *ns)\n{\n\tnvmet_ns_disable(ns);\n\n\tdown_write(&nvmet_ana_sem);\n\tnvmet_ana_group_enabled[ns->anagrpid]--;\n\tup_write(&nvmet_ana_sem);\n\n\tkfree(ns->device_path);\n\tkfree(ns);\n}\n\nstruct nvmet_ns *nvmet_ns_alloc(struct nvmet_subsys *subsys, u32 nsid)\n{\n\tstruct nvmet_ns *ns;\n\n\tns = kzalloc(sizeof(*ns), GFP_KERNEL);\n\tif (!ns)\n\t\treturn NULL;\n\n\tinit_completion(&ns->disable_done);\n\n\tns->nsid = nsid;\n\tns->subsys = subsys;\n\n\tdown_write(&nvmet_ana_sem);\n\tns->anagrpid = NVMET_DEFAULT_ANA_GRPID;\n\tnvmet_ana_group_enabled[ns->anagrpid]++;\n\tup_write(&nvmet_ana_sem);\n\n\tuuid_gen(&ns->uuid);\n\tns->buffered_io = false;\n\tns->csi = NVME_CSI_NVM;\n\n\treturn ns;\n}\n\nstatic void nvmet_update_sq_head(struct nvmet_req *req)\n{\n\tif (req->sq->size) {\n\t\tu32 old_sqhd, new_sqhd;\n\n\t\told_sqhd = READ_ONCE(req->sq->sqhd);\n\t\tdo {\n\t\t\tnew_sqhd = (old_sqhd + 1) % req->sq->size;\n\t\t} while (!try_cmpxchg(&req->sq->sqhd, &old_sqhd, new_sqhd));\n\t}\n\treq->cqe->sq_head = cpu_to_le16(req->sq->sqhd & 0x0000FFFF);\n}\n\nstatic void nvmet_set_error(struct nvmet_req *req, u16 status)\n{\n\tstruct nvmet_ctrl *ctrl = req->sq->ctrl;\n\tstruct nvme_error_slot *new_error_slot;\n\tunsigned long flags;\n\n\treq->cqe->status = cpu_to_le16(status << 1);\n\n\tif (!ctrl || req->error_loc == NVMET_NO_ERROR_LOC)\n\t\treturn;\n\n\tspin_lock_irqsave(&ctrl->error_lock, flags);\n\tctrl->err_counter++;\n\tnew_error_slot =\n\t\t&ctrl->slots[ctrl->err_counter % NVMET_ERROR_LOG_SLOTS];\n\n\tnew_error_slot->error_count = cpu_to_le64(ctrl->err_counter);\n\tnew_error_slot->sqid = cpu_to_le16(req->sq->qid);\n\tnew_error_slot->cmdid = cpu_to_le16(req->cmd->common.command_id);\n\tnew_error_slot->status_field = cpu_to_le16(status << 1);\n\tnew_error_slot->param_error_location = cpu_to_le16(req->error_loc);\n\tnew_error_slot->lba = cpu_to_le64(req->error_slba);\n\tnew_error_slot->nsid = req->cmd->common.nsid;\n\tspin_unlock_irqrestore(&ctrl->error_lock, flags);\n\n\t \n\treq->cqe->status |= cpu_to_le16(1 << 14);\n}\n\nstatic void __nvmet_req_complete(struct nvmet_req *req, u16 status)\n{\n\tstruct nvmet_ns *ns = req->ns;\n\n\tif (!req->sq->sqhd_disabled)\n\t\tnvmet_update_sq_head(req);\n\treq->cqe->sq_id = cpu_to_le16(req->sq->qid);\n\treq->cqe->command_id = req->cmd->common.command_id;\n\n\tif (unlikely(status))\n\t\tnvmet_set_error(req, status);\n\n\ttrace_nvmet_req_complete(req);\n\n\treq->ops->queue_response(req);\n\tif (ns)\n\t\tnvmet_put_namespace(ns);\n}\n\nvoid nvmet_req_complete(struct nvmet_req *req, u16 status)\n{\n\tstruct nvmet_sq *sq = req->sq;\n\n\t__nvmet_req_complete(req, status);\n\tpercpu_ref_put(&sq->ref);\n}\nEXPORT_SYMBOL_GPL(nvmet_req_complete);\n\nvoid nvmet_cq_setup(struct nvmet_ctrl *ctrl, struct nvmet_cq *cq,\n\t\tu16 qid, u16 size)\n{\n\tcq->qid = qid;\n\tcq->size = size;\n}\n\nvoid nvmet_sq_setup(struct nvmet_ctrl *ctrl, struct nvmet_sq *sq,\n\t\tu16 qid, u16 size)\n{\n\tsq->sqhd = 0;\n\tsq->qid = qid;\n\tsq->size = size;\n\n\tctrl->sqs[qid] = sq;\n}\n\nstatic void nvmet_confirm_sq(struct percpu_ref *ref)\n{\n\tstruct nvmet_sq *sq = container_of(ref, struct nvmet_sq, ref);\n\n\tcomplete(&sq->confirm_done);\n}\n\nvoid nvmet_sq_destroy(struct nvmet_sq *sq)\n{\n\tstruct nvmet_ctrl *ctrl = sq->ctrl;\n\n\t \n\tif (ctrl && ctrl->sqs && ctrl->sqs[0] == sq)\n\t\tnvmet_async_events_failall(ctrl);\n\tpercpu_ref_kill_and_confirm(&sq->ref, nvmet_confirm_sq);\n\twait_for_completion(&sq->confirm_done);\n\twait_for_completion(&sq->free_done);\n\tpercpu_ref_exit(&sq->ref);\n\tnvmet_auth_sq_free(sq);\n\n\tif (ctrl) {\n\t\t \n\t\tctrl->reset_tbkas = true;\n\t\tsq->ctrl->sqs[sq->qid] = NULL;\n\t\tnvmet_ctrl_put(ctrl);\n\t\tsq->ctrl = NULL;  \n\t}\n}\nEXPORT_SYMBOL_GPL(nvmet_sq_destroy);\n\nstatic void nvmet_sq_free(struct percpu_ref *ref)\n{\n\tstruct nvmet_sq *sq = container_of(ref, struct nvmet_sq, ref);\n\n\tcomplete(&sq->free_done);\n}\n\nint nvmet_sq_init(struct nvmet_sq *sq)\n{\n\tint ret;\n\n\tret = percpu_ref_init(&sq->ref, nvmet_sq_free, 0, GFP_KERNEL);\n\tif (ret) {\n\t\tpr_err(\"percpu_ref init failed!\\n\");\n\t\treturn ret;\n\t}\n\tinit_completion(&sq->free_done);\n\tinit_completion(&sq->confirm_done);\n\tnvmet_auth_sq_init(sq);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(nvmet_sq_init);\n\nstatic inline u16 nvmet_check_ana_state(struct nvmet_port *port,\n\t\tstruct nvmet_ns *ns)\n{\n\tenum nvme_ana_state state = port->ana_state[ns->anagrpid];\n\n\tif (unlikely(state == NVME_ANA_INACCESSIBLE))\n\t\treturn NVME_SC_ANA_INACCESSIBLE;\n\tif (unlikely(state == NVME_ANA_PERSISTENT_LOSS))\n\t\treturn NVME_SC_ANA_PERSISTENT_LOSS;\n\tif (unlikely(state == NVME_ANA_CHANGE))\n\t\treturn NVME_SC_ANA_TRANSITION;\n\treturn 0;\n}\n\nstatic inline u16 nvmet_io_cmd_check_access(struct nvmet_req *req)\n{\n\tif (unlikely(req->ns->readonly)) {\n\t\tswitch (req->cmd->common.opcode) {\n\t\tcase nvme_cmd_read:\n\t\tcase nvme_cmd_flush:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn NVME_SC_NS_WRITE_PROTECTED;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic u16 nvmet_parse_io_cmd(struct nvmet_req *req)\n{\n\tstruct nvme_command *cmd = req->cmd;\n\tu16 ret;\n\n\tif (nvme_is_fabrics(cmd))\n\t\treturn nvmet_parse_fabrics_io_cmd(req);\n\n\tif (unlikely(!nvmet_check_auth_status(req)))\n\t\treturn NVME_SC_AUTH_REQUIRED | NVME_SC_DNR;\n\n\tret = nvmet_check_ctrl_status(req);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tif (nvmet_is_passthru_req(req))\n\t\treturn nvmet_parse_passthru_io_cmd(req);\n\n\tret = nvmet_req_find_ns(req);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tret = nvmet_check_ana_state(req->port, req->ns);\n\tif (unlikely(ret)) {\n\t\treq->error_loc = offsetof(struct nvme_common_command, nsid);\n\t\treturn ret;\n\t}\n\tret = nvmet_io_cmd_check_access(req);\n\tif (unlikely(ret)) {\n\t\treq->error_loc = offsetof(struct nvme_common_command, nsid);\n\t\treturn ret;\n\t}\n\n\tswitch (req->ns->csi) {\n\tcase NVME_CSI_NVM:\n\t\tif (req->ns->file)\n\t\t\treturn nvmet_file_parse_io_cmd(req);\n\t\treturn nvmet_bdev_parse_io_cmd(req);\n\tcase NVME_CSI_ZNS:\n\t\tif (IS_ENABLED(CONFIG_BLK_DEV_ZONED))\n\t\t\treturn nvmet_bdev_zns_parse_io_cmd(req);\n\t\treturn NVME_SC_INVALID_IO_CMD_SET;\n\tdefault:\n\t\treturn NVME_SC_INVALID_IO_CMD_SET;\n\t}\n}\n\nbool nvmet_req_init(struct nvmet_req *req, struct nvmet_cq *cq,\n\t\tstruct nvmet_sq *sq, const struct nvmet_fabrics_ops *ops)\n{\n\tu8 flags = req->cmd->common.flags;\n\tu16 status;\n\n\treq->cq = cq;\n\treq->sq = sq;\n\treq->ops = ops;\n\treq->sg = NULL;\n\treq->metadata_sg = NULL;\n\treq->sg_cnt = 0;\n\treq->metadata_sg_cnt = 0;\n\treq->transfer_len = 0;\n\treq->metadata_len = 0;\n\treq->cqe->status = 0;\n\treq->cqe->sq_head = 0;\n\treq->ns = NULL;\n\treq->error_loc = NVMET_NO_ERROR_LOC;\n\treq->error_slba = 0;\n\n\t \n\tif (unlikely(flags & (NVME_CMD_FUSE_FIRST | NVME_CMD_FUSE_SECOND))) {\n\t\treq->error_loc = offsetof(struct nvme_common_command, flags);\n\t\tstatus = NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t\tgoto fail;\n\t}\n\n\t \n\tif (unlikely((flags & NVME_CMD_SGL_ALL) != NVME_CMD_SGL_METABUF)) {\n\t\treq->error_loc = offsetof(struct nvme_common_command, flags);\n\t\tstatus = NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t\tgoto fail;\n\t}\n\n\tif (unlikely(!req->sq->ctrl))\n\t\t \n\t\tstatus = nvmet_parse_connect_cmd(req);\n\telse if (likely(req->sq->qid != 0))\n\t\tstatus = nvmet_parse_io_cmd(req);\n\telse\n\t\tstatus = nvmet_parse_admin_cmd(req);\n\n\tif (status)\n\t\tgoto fail;\n\n\ttrace_nvmet_req_init(req, req->cmd);\n\n\tif (unlikely(!percpu_ref_tryget_live(&sq->ref))) {\n\t\tstatus = NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t\tgoto fail;\n\t}\n\n\tif (sq->ctrl)\n\t\tsq->ctrl->reset_tbkas = true;\n\n\treturn true;\n\nfail:\n\t__nvmet_req_complete(req, status);\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(nvmet_req_init);\n\nvoid nvmet_req_uninit(struct nvmet_req *req)\n{\n\tpercpu_ref_put(&req->sq->ref);\n\tif (req->ns)\n\t\tnvmet_put_namespace(req->ns);\n}\nEXPORT_SYMBOL_GPL(nvmet_req_uninit);\n\nbool nvmet_check_transfer_len(struct nvmet_req *req, size_t len)\n{\n\tif (unlikely(len != req->transfer_len)) {\n\t\treq->error_loc = offsetof(struct nvme_common_command, dptr);\n\t\tnvmet_req_complete(req, NVME_SC_SGL_INVALID_DATA | NVME_SC_DNR);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(nvmet_check_transfer_len);\n\nbool nvmet_check_data_len_lte(struct nvmet_req *req, size_t data_len)\n{\n\tif (unlikely(data_len > req->transfer_len)) {\n\t\treq->error_loc = offsetof(struct nvme_common_command, dptr);\n\t\tnvmet_req_complete(req, NVME_SC_SGL_INVALID_DATA | NVME_SC_DNR);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic unsigned int nvmet_data_transfer_len(struct nvmet_req *req)\n{\n\treturn req->transfer_len - req->metadata_len;\n}\n\nstatic int nvmet_req_alloc_p2pmem_sgls(struct pci_dev *p2p_dev,\n\t\tstruct nvmet_req *req)\n{\n\treq->sg = pci_p2pmem_alloc_sgl(p2p_dev, &req->sg_cnt,\n\t\t\tnvmet_data_transfer_len(req));\n\tif (!req->sg)\n\t\tgoto out_err;\n\n\tif (req->metadata_len) {\n\t\treq->metadata_sg = pci_p2pmem_alloc_sgl(p2p_dev,\n\t\t\t\t&req->metadata_sg_cnt, req->metadata_len);\n\t\tif (!req->metadata_sg)\n\t\t\tgoto out_free_sg;\n\t}\n\n\treq->p2p_dev = p2p_dev;\n\n\treturn 0;\nout_free_sg:\n\tpci_p2pmem_free_sgl(req->p2p_dev, req->sg);\nout_err:\n\treturn -ENOMEM;\n}\n\nstatic struct pci_dev *nvmet_req_find_p2p_dev(struct nvmet_req *req)\n{\n\tif (!IS_ENABLED(CONFIG_PCI_P2PDMA) ||\n\t    !req->sq->ctrl || !req->sq->qid || !req->ns)\n\t\treturn NULL;\n\treturn radix_tree_lookup(&req->sq->ctrl->p2p_ns_map, req->ns->nsid);\n}\n\nint nvmet_req_alloc_sgls(struct nvmet_req *req)\n{\n\tstruct pci_dev *p2p_dev = nvmet_req_find_p2p_dev(req);\n\n\tif (p2p_dev && !nvmet_req_alloc_p2pmem_sgls(p2p_dev, req))\n\t\treturn 0;\n\n\treq->sg = sgl_alloc(nvmet_data_transfer_len(req), GFP_KERNEL,\n\t\t\t    &req->sg_cnt);\n\tif (unlikely(!req->sg))\n\t\tgoto out;\n\n\tif (req->metadata_len) {\n\t\treq->metadata_sg = sgl_alloc(req->metadata_len, GFP_KERNEL,\n\t\t\t\t\t     &req->metadata_sg_cnt);\n\t\tif (unlikely(!req->metadata_sg))\n\t\t\tgoto out_free;\n\t}\n\n\treturn 0;\nout_free:\n\tsgl_free(req->sg);\nout:\n\treturn -ENOMEM;\n}\nEXPORT_SYMBOL_GPL(nvmet_req_alloc_sgls);\n\nvoid nvmet_req_free_sgls(struct nvmet_req *req)\n{\n\tif (req->p2p_dev) {\n\t\tpci_p2pmem_free_sgl(req->p2p_dev, req->sg);\n\t\tif (req->metadata_sg)\n\t\t\tpci_p2pmem_free_sgl(req->p2p_dev, req->metadata_sg);\n\t\treq->p2p_dev = NULL;\n\t} else {\n\t\tsgl_free(req->sg);\n\t\tif (req->metadata_sg)\n\t\t\tsgl_free(req->metadata_sg);\n\t}\n\n\treq->sg = NULL;\n\treq->metadata_sg = NULL;\n\treq->sg_cnt = 0;\n\treq->metadata_sg_cnt = 0;\n}\nEXPORT_SYMBOL_GPL(nvmet_req_free_sgls);\n\nstatic inline bool nvmet_cc_en(u32 cc)\n{\n\treturn (cc >> NVME_CC_EN_SHIFT) & 0x1;\n}\n\nstatic inline u8 nvmet_cc_css(u32 cc)\n{\n\treturn (cc >> NVME_CC_CSS_SHIFT) & 0x7;\n}\n\nstatic inline u8 nvmet_cc_mps(u32 cc)\n{\n\treturn (cc >> NVME_CC_MPS_SHIFT) & 0xf;\n}\n\nstatic inline u8 nvmet_cc_ams(u32 cc)\n{\n\treturn (cc >> NVME_CC_AMS_SHIFT) & 0x7;\n}\n\nstatic inline u8 nvmet_cc_shn(u32 cc)\n{\n\treturn (cc >> NVME_CC_SHN_SHIFT) & 0x3;\n}\n\nstatic inline u8 nvmet_cc_iosqes(u32 cc)\n{\n\treturn (cc >> NVME_CC_IOSQES_SHIFT) & 0xf;\n}\n\nstatic inline u8 nvmet_cc_iocqes(u32 cc)\n{\n\treturn (cc >> NVME_CC_IOCQES_SHIFT) & 0xf;\n}\n\nstatic inline bool nvmet_css_supported(u8 cc_css)\n{\n\tswitch (cc_css << NVME_CC_CSS_SHIFT) {\n\tcase NVME_CC_CSS_NVM:\n\tcase NVME_CC_CSS_CSI:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic void nvmet_start_ctrl(struct nvmet_ctrl *ctrl)\n{\n\tlockdep_assert_held(&ctrl->lock);\n\n\t \n\tif (!nvmet_is_disc_subsys(ctrl->subsys) &&\n\t    (nvmet_cc_iosqes(ctrl->cc) != NVME_NVM_IOSQES ||\n\t     nvmet_cc_iocqes(ctrl->cc) != NVME_NVM_IOCQES)) {\n\t\tctrl->csts = NVME_CSTS_CFS;\n\t\treturn;\n\t}\n\n\tif (nvmet_cc_mps(ctrl->cc) != 0 ||\n\t    nvmet_cc_ams(ctrl->cc) != 0 ||\n\t    !nvmet_css_supported(nvmet_cc_css(ctrl->cc))) {\n\t\tctrl->csts = NVME_CSTS_CFS;\n\t\treturn;\n\t}\n\n\tctrl->csts = NVME_CSTS_RDY;\n\n\t \n\tif (ctrl->kato)\n\t\tmod_delayed_work(nvmet_wq, &ctrl->ka_work, ctrl->kato * HZ);\n}\n\nstatic void nvmet_clear_ctrl(struct nvmet_ctrl *ctrl)\n{\n\tlockdep_assert_held(&ctrl->lock);\n\n\t \n\tctrl->csts &= ~NVME_CSTS_RDY;\n\tctrl->cc = 0;\n}\n\nvoid nvmet_update_cc(struct nvmet_ctrl *ctrl, u32 new)\n{\n\tu32 old;\n\n\tmutex_lock(&ctrl->lock);\n\told = ctrl->cc;\n\tctrl->cc = new;\n\n\tif (nvmet_cc_en(new) && !nvmet_cc_en(old))\n\t\tnvmet_start_ctrl(ctrl);\n\tif (!nvmet_cc_en(new) && nvmet_cc_en(old))\n\t\tnvmet_clear_ctrl(ctrl);\n\tif (nvmet_cc_shn(new) && !nvmet_cc_shn(old)) {\n\t\tnvmet_clear_ctrl(ctrl);\n\t\tctrl->csts |= NVME_CSTS_SHST_CMPLT;\n\t}\n\tif (!nvmet_cc_shn(new) && nvmet_cc_shn(old))\n\t\tctrl->csts &= ~NVME_CSTS_SHST_CMPLT;\n\tmutex_unlock(&ctrl->lock);\n}\n\nstatic void nvmet_init_cap(struct nvmet_ctrl *ctrl)\n{\n\t \n\tctrl->cap = (1ULL << 37);\n\t \n\tctrl->cap |= (1ULL << 43);\n\t \n\tctrl->cap |= (15ULL << 24);\n\t \n\tif (ctrl->ops->get_max_queue_size)\n\t\tctrl->cap |= ctrl->ops->get_max_queue_size(ctrl) - 1;\n\telse\n\t\tctrl->cap |= NVMET_QUEUE_SIZE - 1;\n\n\tif (nvmet_is_passthru_subsys(ctrl->subsys))\n\t\tnvmet_passthrough_override_cap(ctrl);\n}\n\nstruct nvmet_ctrl *nvmet_ctrl_find_get(const char *subsysnqn,\n\t\t\t\t       const char *hostnqn, u16 cntlid,\n\t\t\t\t       struct nvmet_req *req)\n{\n\tstruct nvmet_ctrl *ctrl = NULL;\n\tstruct nvmet_subsys *subsys;\n\n\tsubsys = nvmet_find_get_subsys(req->port, subsysnqn);\n\tif (!subsys) {\n\t\tpr_warn(\"connect request for invalid subsystem %s!\\n\",\n\t\t\tsubsysnqn);\n\t\treq->cqe->result.u32 = IPO_IATTR_CONNECT_DATA(subsysnqn);\n\t\tgoto out;\n\t}\n\n\tmutex_lock(&subsys->lock);\n\tlist_for_each_entry(ctrl, &subsys->ctrls, subsys_entry) {\n\t\tif (ctrl->cntlid == cntlid) {\n\t\t\tif (strncmp(hostnqn, ctrl->hostnqn, NVMF_NQN_SIZE)) {\n\t\t\t\tpr_warn(\"hostnqn mismatch.\\n\");\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (!kref_get_unless_zero(&ctrl->ref))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tgoto found;\n\t\t}\n\t}\n\n\tctrl = NULL;  \n\tpr_warn(\"could not find controller %d for subsys %s / host %s\\n\",\n\t\tcntlid, subsysnqn, hostnqn);\n\treq->cqe->result.u32 = IPO_IATTR_CONNECT_DATA(cntlid);\n\nfound:\n\tmutex_unlock(&subsys->lock);\n\tnvmet_subsys_put(subsys);\nout:\n\treturn ctrl;\n}\n\nu16 nvmet_check_ctrl_status(struct nvmet_req *req)\n{\n\tif (unlikely(!(req->sq->ctrl->cc & NVME_CC_ENABLE))) {\n\t\tpr_err(\"got cmd %d while CC.EN == 0 on qid = %d\\n\",\n\t\t       req->cmd->common.opcode, req->sq->qid);\n\t\treturn NVME_SC_CMD_SEQ_ERROR | NVME_SC_DNR;\n\t}\n\n\tif (unlikely(!(req->sq->ctrl->csts & NVME_CSTS_RDY))) {\n\t\tpr_err(\"got cmd %d while CSTS.RDY == 0 on qid = %d\\n\",\n\t\t       req->cmd->common.opcode, req->sq->qid);\n\t\treturn NVME_SC_CMD_SEQ_ERROR | NVME_SC_DNR;\n\t}\n\n\tif (unlikely(!nvmet_check_auth_status(req))) {\n\t\tpr_warn(\"qid %d not authenticated\\n\", req->sq->qid);\n\t\treturn NVME_SC_AUTH_REQUIRED | NVME_SC_DNR;\n\t}\n\treturn 0;\n}\n\nbool nvmet_host_allowed(struct nvmet_subsys *subsys, const char *hostnqn)\n{\n\tstruct nvmet_host_link *p;\n\n\tlockdep_assert_held(&nvmet_config_sem);\n\n\tif (subsys->allow_any_host)\n\t\treturn true;\n\n\tif (nvmet_is_disc_subsys(subsys))  \n\t\treturn true;\n\n\tlist_for_each_entry(p, &subsys->hosts, entry) {\n\t\tif (!strcmp(nvmet_host_name(p->host), hostnqn))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic void nvmet_setup_p2p_ns_map(struct nvmet_ctrl *ctrl,\n\t\tstruct nvmet_req *req)\n{\n\tstruct nvmet_ns *ns;\n\tunsigned long idx;\n\n\tif (!req->p2p_client)\n\t\treturn;\n\n\tctrl->p2p_client = get_device(req->p2p_client);\n\n\txa_for_each(&ctrl->subsys->namespaces, idx, ns)\n\t\tnvmet_p2pmem_ns_add_p2p(ctrl, ns);\n}\n\n \nstatic void nvmet_release_p2p_ns_map(struct nvmet_ctrl *ctrl)\n{\n\tstruct radix_tree_iter iter;\n\tvoid __rcu **slot;\n\n\tradix_tree_for_each_slot(slot, &ctrl->p2p_ns_map, &iter, 0)\n\t\tpci_dev_put(radix_tree_deref_slot(slot));\n\n\tput_device(ctrl->p2p_client);\n}\n\nstatic void nvmet_fatal_error_handler(struct work_struct *work)\n{\n\tstruct nvmet_ctrl *ctrl =\n\t\t\tcontainer_of(work, struct nvmet_ctrl, fatal_err_work);\n\n\tpr_err(\"ctrl %d fatal error occurred!\\n\", ctrl->cntlid);\n\tctrl->ops->delete_ctrl(ctrl);\n}\n\nu16 nvmet_alloc_ctrl(const char *subsysnqn, const char *hostnqn,\n\t\tstruct nvmet_req *req, u32 kato, struct nvmet_ctrl **ctrlp)\n{\n\tstruct nvmet_subsys *subsys;\n\tstruct nvmet_ctrl *ctrl;\n\tint ret;\n\tu16 status;\n\n\tstatus = NVME_SC_CONNECT_INVALID_PARAM | NVME_SC_DNR;\n\tsubsys = nvmet_find_get_subsys(req->port, subsysnqn);\n\tif (!subsys) {\n\t\tpr_warn(\"connect request for invalid subsystem %s!\\n\",\n\t\t\tsubsysnqn);\n\t\treq->cqe->result.u32 = IPO_IATTR_CONNECT_DATA(subsysnqn);\n\t\treq->error_loc = offsetof(struct nvme_common_command, dptr);\n\t\tgoto out;\n\t}\n\n\tdown_read(&nvmet_config_sem);\n\tif (!nvmet_host_allowed(subsys, hostnqn)) {\n\t\tpr_info(\"connect by host %s for subsystem %s not allowed\\n\",\n\t\t\thostnqn, subsysnqn);\n\t\treq->cqe->result.u32 = IPO_IATTR_CONNECT_DATA(hostnqn);\n\t\tup_read(&nvmet_config_sem);\n\t\tstatus = NVME_SC_CONNECT_INVALID_HOST | NVME_SC_DNR;\n\t\treq->error_loc = offsetof(struct nvme_common_command, dptr);\n\t\tgoto out_put_subsystem;\n\t}\n\tup_read(&nvmet_config_sem);\n\n\tstatus = NVME_SC_INTERNAL;\n\tctrl = kzalloc(sizeof(*ctrl), GFP_KERNEL);\n\tif (!ctrl)\n\t\tgoto out_put_subsystem;\n\tmutex_init(&ctrl->lock);\n\n\tctrl->port = req->port;\n\tctrl->ops = req->ops;\n\n#ifdef CONFIG_NVME_TARGET_PASSTHRU\n\t \n\tif (ctrl->port->disc_addr.trtype == NVMF_TRTYPE_LOOP)\n\t\tsubsys->clear_ids = 1;\n#endif\n\n\tINIT_WORK(&ctrl->async_event_work, nvmet_async_event_work);\n\tINIT_LIST_HEAD(&ctrl->async_events);\n\tINIT_RADIX_TREE(&ctrl->p2p_ns_map, GFP_KERNEL);\n\tINIT_WORK(&ctrl->fatal_err_work, nvmet_fatal_error_handler);\n\tINIT_DELAYED_WORK(&ctrl->ka_work, nvmet_keep_alive_timer);\n\n\tmemcpy(ctrl->subsysnqn, subsysnqn, NVMF_NQN_SIZE);\n\tmemcpy(ctrl->hostnqn, hostnqn, NVMF_NQN_SIZE);\n\n\tkref_init(&ctrl->ref);\n\tctrl->subsys = subsys;\n\tnvmet_init_cap(ctrl);\n\tWRITE_ONCE(ctrl->aen_enabled, NVMET_AEN_CFG_OPTIONAL);\n\n\tctrl->changed_ns_list = kmalloc_array(NVME_MAX_CHANGED_NAMESPACES,\n\t\t\tsizeof(__le32), GFP_KERNEL);\n\tif (!ctrl->changed_ns_list)\n\t\tgoto out_free_ctrl;\n\n\tctrl->sqs = kcalloc(subsys->max_qid + 1,\n\t\t\tsizeof(struct nvmet_sq *),\n\t\t\tGFP_KERNEL);\n\tif (!ctrl->sqs)\n\t\tgoto out_free_changed_ns_list;\n\n\tif (subsys->cntlid_min > subsys->cntlid_max)\n\t\tgoto out_free_sqs;\n\n\tret = ida_alloc_range(&cntlid_ida,\n\t\t\t     subsys->cntlid_min, subsys->cntlid_max,\n\t\t\t     GFP_KERNEL);\n\tif (ret < 0) {\n\t\tstatus = NVME_SC_CONNECT_CTRL_BUSY | NVME_SC_DNR;\n\t\tgoto out_free_sqs;\n\t}\n\tctrl->cntlid = ret;\n\n\t \n\tif (nvmet_is_disc_subsys(ctrl->subsys) && !kato)\n\t\tkato = NVMET_DISC_KATO_MS;\n\n\t \n\tctrl->kato = DIV_ROUND_UP(kato, 1000);\n\n\tctrl->err_counter = 0;\n\tspin_lock_init(&ctrl->error_lock);\n\n\tnvmet_start_keep_alive_timer(ctrl);\n\n\tmutex_lock(&subsys->lock);\n\tlist_add_tail(&ctrl->subsys_entry, &subsys->ctrls);\n\tnvmet_setup_p2p_ns_map(ctrl, req);\n\tmutex_unlock(&subsys->lock);\n\n\t*ctrlp = ctrl;\n\treturn 0;\n\nout_free_sqs:\n\tkfree(ctrl->sqs);\nout_free_changed_ns_list:\n\tkfree(ctrl->changed_ns_list);\nout_free_ctrl:\n\tkfree(ctrl);\nout_put_subsystem:\n\tnvmet_subsys_put(subsys);\nout:\n\treturn status;\n}\n\nstatic void nvmet_ctrl_free(struct kref *ref)\n{\n\tstruct nvmet_ctrl *ctrl = container_of(ref, struct nvmet_ctrl, ref);\n\tstruct nvmet_subsys *subsys = ctrl->subsys;\n\n\tmutex_lock(&subsys->lock);\n\tnvmet_release_p2p_ns_map(ctrl);\n\tlist_del(&ctrl->subsys_entry);\n\tmutex_unlock(&subsys->lock);\n\n\tnvmet_stop_keep_alive_timer(ctrl);\n\n\tflush_work(&ctrl->async_event_work);\n\tcancel_work_sync(&ctrl->fatal_err_work);\n\n\tnvmet_destroy_auth(ctrl);\n\n\tida_free(&cntlid_ida, ctrl->cntlid);\n\n\tnvmet_async_events_free(ctrl);\n\tkfree(ctrl->sqs);\n\tkfree(ctrl->changed_ns_list);\n\tkfree(ctrl);\n\n\tnvmet_subsys_put(subsys);\n}\n\nvoid nvmet_ctrl_put(struct nvmet_ctrl *ctrl)\n{\n\tkref_put(&ctrl->ref, nvmet_ctrl_free);\n}\n\nvoid nvmet_ctrl_fatal_error(struct nvmet_ctrl *ctrl)\n{\n\tmutex_lock(&ctrl->lock);\n\tif (!(ctrl->csts & NVME_CSTS_CFS)) {\n\t\tctrl->csts |= NVME_CSTS_CFS;\n\t\tqueue_work(nvmet_wq, &ctrl->fatal_err_work);\n\t}\n\tmutex_unlock(&ctrl->lock);\n}\nEXPORT_SYMBOL_GPL(nvmet_ctrl_fatal_error);\n\nstatic struct nvmet_subsys *nvmet_find_get_subsys(struct nvmet_port *port,\n\t\tconst char *subsysnqn)\n{\n\tstruct nvmet_subsys_link *p;\n\n\tif (!port)\n\t\treturn NULL;\n\n\tif (!strcmp(NVME_DISC_SUBSYS_NAME, subsysnqn)) {\n\t\tif (!kref_get_unless_zero(&nvmet_disc_subsys->ref))\n\t\t\treturn NULL;\n\t\treturn nvmet_disc_subsys;\n\t}\n\n\tdown_read(&nvmet_config_sem);\n\tlist_for_each_entry(p, &port->subsystems, entry) {\n\t\tif (!strncmp(p->subsys->subsysnqn, subsysnqn,\n\t\t\t\tNVMF_NQN_SIZE)) {\n\t\t\tif (!kref_get_unless_zero(&p->subsys->ref))\n\t\t\t\tbreak;\n\t\t\tup_read(&nvmet_config_sem);\n\t\t\treturn p->subsys;\n\t\t}\n\t}\n\tup_read(&nvmet_config_sem);\n\treturn NULL;\n}\n\nstruct nvmet_subsys *nvmet_subsys_alloc(const char *subsysnqn,\n\t\tenum nvme_subsys_type type)\n{\n\tstruct nvmet_subsys *subsys;\n\tchar serial[NVMET_SN_MAX_SIZE / 2];\n\tint ret;\n\n\tsubsys = kzalloc(sizeof(*subsys), GFP_KERNEL);\n\tif (!subsys)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tsubsys->ver = NVMET_DEFAULT_VS;\n\t \n\tget_random_bytes(&serial, sizeof(serial));\n\tbin2hex(subsys->serial, &serial, sizeof(serial));\n\n\tsubsys->model_number = kstrdup(NVMET_DEFAULT_CTRL_MODEL, GFP_KERNEL);\n\tif (!subsys->model_number) {\n\t\tret = -ENOMEM;\n\t\tgoto free_subsys;\n\t}\n\n\tsubsys->ieee_oui = 0;\n\n\tsubsys->firmware_rev = kstrndup(UTS_RELEASE, NVMET_FR_MAX_SIZE, GFP_KERNEL);\n\tif (!subsys->firmware_rev) {\n\t\tret = -ENOMEM;\n\t\tgoto free_mn;\n\t}\n\n\tswitch (type) {\n\tcase NVME_NQN_NVME:\n\t\tsubsys->max_qid = NVMET_NR_QUEUES;\n\t\tbreak;\n\tcase NVME_NQN_DISC:\n\tcase NVME_NQN_CURR:\n\t\tsubsys->max_qid = 0;\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"%s: Unknown Subsystem type - %d\\n\", __func__, type);\n\t\tret = -EINVAL;\n\t\tgoto free_fr;\n\t}\n\tsubsys->type = type;\n\tsubsys->subsysnqn = kstrndup(subsysnqn, NVMF_NQN_SIZE,\n\t\t\tGFP_KERNEL);\n\tif (!subsys->subsysnqn) {\n\t\tret = -ENOMEM;\n\t\tgoto free_fr;\n\t}\n\tsubsys->cntlid_min = NVME_CNTLID_MIN;\n\tsubsys->cntlid_max = NVME_CNTLID_MAX;\n\tkref_init(&subsys->ref);\n\n\tmutex_init(&subsys->lock);\n\txa_init(&subsys->namespaces);\n\tINIT_LIST_HEAD(&subsys->ctrls);\n\tINIT_LIST_HEAD(&subsys->hosts);\n\n\treturn subsys;\n\nfree_fr:\n\tkfree(subsys->firmware_rev);\nfree_mn:\n\tkfree(subsys->model_number);\nfree_subsys:\n\tkfree(subsys);\n\treturn ERR_PTR(ret);\n}\n\nstatic void nvmet_subsys_free(struct kref *ref)\n{\n\tstruct nvmet_subsys *subsys =\n\t\tcontainer_of(ref, struct nvmet_subsys, ref);\n\n\tWARN_ON_ONCE(!xa_empty(&subsys->namespaces));\n\n\txa_destroy(&subsys->namespaces);\n\tnvmet_passthru_subsys_free(subsys);\n\n\tkfree(subsys->subsysnqn);\n\tkfree(subsys->model_number);\n\tkfree(subsys->firmware_rev);\n\tkfree(subsys);\n}\n\nvoid nvmet_subsys_del_ctrls(struct nvmet_subsys *subsys)\n{\n\tstruct nvmet_ctrl *ctrl;\n\n\tmutex_lock(&subsys->lock);\n\tlist_for_each_entry(ctrl, &subsys->ctrls, subsys_entry)\n\t\tctrl->ops->delete_ctrl(ctrl);\n\tmutex_unlock(&subsys->lock);\n}\n\nvoid nvmet_subsys_put(struct nvmet_subsys *subsys)\n{\n\tkref_put(&subsys->ref, nvmet_subsys_free);\n}\n\nstatic int __init nvmet_init(void)\n{\n\tint error = -ENOMEM;\n\n\tnvmet_ana_group_enabled[NVMET_DEFAULT_ANA_GRPID] = 1;\n\n\tnvmet_bvec_cache = kmem_cache_create(\"nvmet-bvec\",\n\t\t\tNVMET_MAX_MPOOL_BVEC * sizeof(struct bio_vec), 0,\n\t\t\tSLAB_HWCACHE_ALIGN, NULL);\n\tif (!nvmet_bvec_cache)\n\t\treturn -ENOMEM;\n\n\tzbd_wq = alloc_workqueue(\"nvmet-zbd-wq\", WQ_MEM_RECLAIM, 0);\n\tif (!zbd_wq)\n\t\tgoto out_destroy_bvec_cache;\n\n\tbuffered_io_wq = alloc_workqueue(\"nvmet-buffered-io-wq\",\n\t\t\tWQ_MEM_RECLAIM, 0);\n\tif (!buffered_io_wq)\n\t\tgoto out_free_zbd_work_queue;\n\n\tnvmet_wq = alloc_workqueue(\"nvmet-wq\", WQ_MEM_RECLAIM, 0);\n\tif (!nvmet_wq)\n\t\tgoto out_free_buffered_work_queue;\n\n\terror = nvmet_init_discovery();\n\tif (error)\n\t\tgoto out_free_nvmet_work_queue;\n\n\terror = nvmet_init_configfs();\n\tif (error)\n\t\tgoto out_exit_discovery;\n\treturn 0;\n\nout_exit_discovery:\n\tnvmet_exit_discovery();\nout_free_nvmet_work_queue:\n\tdestroy_workqueue(nvmet_wq);\nout_free_buffered_work_queue:\n\tdestroy_workqueue(buffered_io_wq);\nout_free_zbd_work_queue:\n\tdestroy_workqueue(zbd_wq);\nout_destroy_bvec_cache:\n\tkmem_cache_destroy(nvmet_bvec_cache);\n\treturn error;\n}\n\nstatic void __exit nvmet_exit(void)\n{\n\tnvmet_exit_configfs();\n\tnvmet_exit_discovery();\n\tida_destroy(&cntlid_ida);\n\tdestroy_workqueue(nvmet_wq);\n\tdestroy_workqueue(buffered_io_wq);\n\tdestroy_workqueue(zbd_wq);\n\tkmem_cache_destroy(nvmet_bvec_cache);\n\n\tBUILD_BUG_ON(sizeof(struct nvmf_disc_rsp_page_entry) != 1024);\n\tBUILD_BUG_ON(sizeof(struct nvmf_disc_rsp_page_hdr) != 1024);\n}\n\nmodule_init(nvmet_init);\nmodule_exit(nvmet_exit);\n\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}