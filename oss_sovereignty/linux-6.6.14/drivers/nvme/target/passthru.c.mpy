{
  "module_name": "passthru.c",
  "hash_id": "93ec06bed0ad1c1d9b1bd21ede7808ca455fa71fa9f38a67dc82344039e701d1",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvme/target/passthru.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n#include <linux/module.h>\n\n#include \"../host/nvme.h\"\n#include \"nvmet.h\"\n\nMODULE_IMPORT_NS(NVME_TARGET_PASSTHRU);\n\n \nstatic DEFINE_XARRAY(passthru_subsystems);\n\nvoid nvmet_passthrough_override_cap(struct nvmet_ctrl *ctrl)\n{\n\t \n\tif (!nvme_multi_css(ctrl->subsys->passthru_ctrl))\n\t\tctrl->cap &= ~(1ULL << 43);\n}\n\nstatic u16 nvmet_passthru_override_id_descs(struct nvmet_req *req)\n{\n\tstruct nvmet_ctrl *ctrl = req->sq->ctrl;\n\tu16 status = NVME_SC_SUCCESS;\n\tint pos, len;\n\tbool csi_seen = false;\n\tvoid *data;\n\tu8 csi;\n\n\tif (!ctrl->subsys->clear_ids)\n\t\treturn status;\n\n\tdata = kzalloc(NVME_IDENTIFY_DATA_SIZE, GFP_KERNEL);\n\tif (!data)\n\t\treturn NVME_SC_INTERNAL;\n\n\tstatus = nvmet_copy_from_sgl(req, 0, data, NVME_IDENTIFY_DATA_SIZE);\n\tif (status)\n\t\tgoto out_free;\n\n\tfor (pos = 0; pos < NVME_IDENTIFY_DATA_SIZE; pos += len) {\n\t\tstruct nvme_ns_id_desc *cur = data + pos;\n\n\t\tif (cur->nidl == 0)\n\t\t\tbreak;\n\t\tif (cur->nidt == NVME_NIDT_CSI) {\n\t\t\tmemcpy(&csi, cur + 1, NVME_NIDT_CSI_LEN);\n\t\t\tcsi_seen = true;\n\t\t\tbreak;\n\t\t}\n\t\tlen = sizeof(struct nvme_ns_id_desc) + cur->nidl;\n\t}\n\n\tmemset(data, 0, NVME_IDENTIFY_DATA_SIZE);\n\tif (csi_seen) {\n\t\tstruct nvme_ns_id_desc *cur = data;\n\n\t\tcur->nidt = NVME_NIDT_CSI;\n\t\tcur->nidl = NVME_NIDT_CSI_LEN;\n\t\tmemcpy(cur + 1, &csi, NVME_NIDT_CSI_LEN);\n\t}\n\tstatus = nvmet_copy_to_sgl(req, 0, data, NVME_IDENTIFY_DATA_SIZE);\nout_free:\n\tkfree(data);\n\treturn status;\n}\n\nstatic u16 nvmet_passthru_override_id_ctrl(struct nvmet_req *req)\n{\n\tstruct nvmet_ctrl *ctrl = req->sq->ctrl;\n\tstruct nvme_ctrl *pctrl = ctrl->subsys->passthru_ctrl;\n\tu16 status = NVME_SC_SUCCESS;\n\tstruct nvme_id_ctrl *id;\n\tunsigned int max_hw_sectors;\n\tint page_shift;\n\n\tid = kzalloc(sizeof(*id), GFP_KERNEL);\n\tif (!id)\n\t\treturn NVME_SC_INTERNAL;\n\n\tstatus = nvmet_copy_from_sgl(req, 0, id, sizeof(*id));\n\tif (status)\n\t\tgoto out_free;\n\n\tid->cntlid = cpu_to_le16(ctrl->cntlid);\n\tid->ver = cpu_to_le32(ctrl->subsys->ver);\n\n\t \n\tmax_hw_sectors = min_not_zero(pctrl->max_segments << PAGE_SECTORS_SHIFT,\n\t\t\t\t      pctrl->max_hw_sectors);\n\n\t \n\tmax_hw_sectors = min_not_zero(BIO_MAX_VECS << PAGE_SECTORS_SHIFT,\n\t\t\t\t      max_hw_sectors);\n\n\tpage_shift = NVME_CAP_MPSMIN(ctrl->cap) + 12;\n\n\tid->mdts = ilog2(max_hw_sectors) + 9 - page_shift;\n\n\tid->acl = 3;\n\t \n\tid->aerl = NVMET_ASYNC_EVENTS - 1;\n\n\t \n\tid->kas = cpu_to_le16(NVMET_KAS);\n\n\t \n\tid->hmpre = 0;\n\tid->hmmin = 0;\n\n\tid->sqes = min_t(__u8, ((0x6 << 4) | 0x6), id->sqes);\n\tid->cqes = min_t(__u8, ((0x4 << 4) | 0x4), id->cqes);\n\tid->maxcmd = cpu_to_le16(NVMET_MAX_CMD);\n\n\t \n\tid->fuses = 0;\n\n\tid->sgls = cpu_to_le32(1 << 0);  \n\tif (ctrl->ops->flags & NVMF_KEYED_SGLS)\n\t\tid->sgls |= cpu_to_le32(1 << 2);\n\tif (req->port->inline_data_size)\n\t\tid->sgls |= cpu_to_le32(1 << 20);\n\n\t \n\tmemcpy(id->subnqn, ctrl->subsysnqn, sizeof(id->subnqn));\n\n\t \n\tid->ioccsz = cpu_to_le32((sizeof(struct nvme_command) +\n\t\t\t\treq->port->inline_data_size) / 16);\n\tid->iorcsz = cpu_to_le32(sizeof(struct nvme_completion) / 16);\n\n\tid->msdbd = ctrl->ops->msdbd;\n\n\t \n\tid->cmic |= 1 << 1;\n\n\t \n\tid->oncs &= cpu_to_le16(~NVME_CTRL_ONCS_RESERVATIONS);\n\n\tstatus = nvmet_copy_to_sgl(req, 0, id, sizeof(struct nvme_id_ctrl));\n\nout_free:\n\tkfree(id);\n\treturn status;\n}\n\nstatic u16 nvmet_passthru_override_id_ns(struct nvmet_req *req)\n{\n\tu16 status = NVME_SC_SUCCESS;\n\tstruct nvme_id_ns *id;\n\tint i;\n\n\tid = kzalloc(sizeof(*id), GFP_KERNEL);\n\tif (!id)\n\t\treturn NVME_SC_INTERNAL;\n\n\tstatus = nvmet_copy_from_sgl(req, 0, id, sizeof(struct nvme_id_ns));\n\tif (status)\n\t\tgoto out_free;\n\n\tfor (i = 0; i < (id->nlbaf + 1); i++)\n\t\tif (id->lbaf[i].ms)\n\t\t\tmemset(&id->lbaf[i], 0, sizeof(id->lbaf[i]));\n\n\tid->flbas = id->flbas & ~(1 << 4);\n\n\t \n\tid->mc = 0;\n\n\tif (req->sq->ctrl->subsys->clear_ids) {\n\t\tmemset(id->nguid, 0, NVME_NIDT_NGUID_LEN);\n\t\tmemset(id->eui64, 0, NVME_NIDT_EUI64_LEN);\n\t}\n\n\tstatus = nvmet_copy_to_sgl(req, 0, id, sizeof(*id));\n\nout_free:\n\tkfree(id);\n\treturn status;\n}\n\nstatic void nvmet_passthru_execute_cmd_work(struct work_struct *w)\n{\n\tstruct nvmet_req *req = container_of(w, struct nvmet_req, p.work);\n\tstruct request *rq = req->p.rq;\n\tstruct nvme_ctrl *ctrl = nvme_req(rq)->ctrl;\n\tstruct nvme_ns *ns = rq->q->queuedata;\n\tu32 effects;\n\tint status;\n\n\teffects = nvme_passthru_start(ctrl, ns, req->cmd->common.opcode);\n\tstatus = nvme_execute_rq(rq, false);\n\tif (status == NVME_SC_SUCCESS &&\n\t    req->cmd->common.opcode == nvme_admin_identify) {\n\t\tswitch (req->cmd->identify.cns) {\n\t\tcase NVME_ID_CNS_CTRL:\n\t\t\tnvmet_passthru_override_id_ctrl(req);\n\t\t\tbreak;\n\t\tcase NVME_ID_CNS_NS:\n\t\t\tnvmet_passthru_override_id_ns(req);\n\t\t\tbreak;\n\t\tcase NVME_ID_CNS_NS_DESC_LIST:\n\t\t\tnvmet_passthru_override_id_descs(req);\n\t\t\tbreak;\n\t\t}\n\t} else if (status < 0)\n\t\tstatus = NVME_SC_INTERNAL;\n\n\treq->cqe->result = nvme_req(rq)->result;\n\tnvmet_req_complete(req, status);\n\tblk_mq_free_request(rq);\n\n\tif (effects)\n\t\tnvme_passthru_end(ctrl, ns, effects, req->cmd, status);\n}\n\nstatic enum rq_end_io_ret nvmet_passthru_req_done(struct request *rq,\n\t\t\t\t\t\t  blk_status_t blk_status)\n{\n\tstruct nvmet_req *req = rq->end_io_data;\n\n\treq->cqe->result = nvme_req(rq)->result;\n\tnvmet_req_complete(req, nvme_req(rq)->status);\n\tblk_mq_free_request(rq);\n\treturn RQ_END_IO_NONE;\n}\n\nstatic int nvmet_passthru_map_sg(struct nvmet_req *req, struct request *rq)\n{\n\tstruct scatterlist *sg;\n\tstruct bio *bio;\n\tint i;\n\n\tif (req->sg_cnt > BIO_MAX_VECS)\n\t\treturn -EINVAL;\n\n\tif (nvmet_use_inline_bvec(req)) {\n\t\tbio = &req->p.inline_bio;\n\t\tbio_init(bio, NULL, req->inline_bvec,\n\t\t\t ARRAY_SIZE(req->inline_bvec), req_op(rq));\n\t} else {\n\t\tbio = bio_alloc(NULL, bio_max_segs(req->sg_cnt), req_op(rq),\n\t\t\t\tGFP_KERNEL);\n\t\tbio->bi_end_io = bio_put;\n\t}\n\n\tfor_each_sg(req->sg, sg, req->sg_cnt, i) {\n\t\tif (bio_add_pc_page(rq->q, bio, sg_page(sg), sg->length,\n\t\t\t\t    sg->offset) < sg->length) {\n\t\t\tnvmet_req_bio_put(req, bio);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tblk_rq_bio_prep(rq, bio, req->sg_cnt);\n\n\treturn 0;\n}\n\nstatic void nvmet_passthru_execute_cmd(struct nvmet_req *req)\n{\n\tstruct nvme_ctrl *ctrl = nvmet_req_subsys(req)->passthru_ctrl;\n\tstruct request_queue *q = ctrl->admin_q;\n\tstruct nvme_ns *ns = NULL;\n\tstruct request *rq = NULL;\n\tunsigned int timeout;\n\tu32 effects;\n\tu16 status;\n\tint ret;\n\n\tif (likely(req->sq->qid != 0)) {\n\t\tu32 nsid = le32_to_cpu(req->cmd->common.nsid);\n\n\t\tns = nvme_find_get_ns(ctrl, nsid);\n\t\tif (unlikely(!ns)) {\n\t\t\tpr_err(\"failed to get passthru ns nsid:%u\\n\", nsid);\n\t\t\tstatus = NVME_SC_INVALID_NS | NVME_SC_DNR;\n\t\t\tgoto out;\n\t\t}\n\n\t\tq = ns->queue;\n\t\ttimeout = nvmet_req_subsys(req)->io_timeout;\n\t} else {\n\t\ttimeout = nvmet_req_subsys(req)->admin_timeout;\n\t}\n\n\trq = blk_mq_alloc_request(q, nvme_req_op(req->cmd), 0);\n\tif (IS_ERR(rq)) {\n\t\tstatus = NVME_SC_INTERNAL;\n\t\tgoto out_put_ns;\n\t}\n\tnvme_init_request(rq, req->cmd);\n\n\tif (timeout)\n\t\trq->timeout = timeout;\n\n\tif (req->sg_cnt) {\n\t\tret = nvmet_passthru_map_sg(req, rq);\n\t\tif (unlikely(ret)) {\n\t\t\tstatus = NVME_SC_INTERNAL;\n\t\t\tgoto out_put_req;\n\t\t}\n\t}\n\n\t \n\teffects = nvme_command_effects(ctrl, ns, req->cmd->common.opcode);\n\tif (req->p.use_workqueue ||\n\t    (effects & ~(NVME_CMD_EFFECTS_CSUPP | NVME_CMD_EFFECTS_LBCC))) {\n\t\tINIT_WORK(&req->p.work, nvmet_passthru_execute_cmd_work);\n\t\treq->p.rq = rq;\n\t\tqueue_work(nvmet_wq, &req->p.work);\n\t} else {\n\t\trq->end_io = nvmet_passthru_req_done;\n\t\trq->end_io_data = req;\n\t\tblk_execute_rq_nowait(rq, false);\n\t}\n\n\tif (ns)\n\t\tnvme_put_ns(ns);\n\n\treturn;\n\nout_put_req:\n\tblk_mq_free_request(rq);\nout_put_ns:\n\tif (ns)\n\t\tnvme_put_ns(ns);\nout:\n\tnvmet_req_complete(req, status);\n}\n\n \nstatic void nvmet_passthru_set_host_behaviour(struct nvmet_req *req)\n{\n\tstruct nvme_ctrl *ctrl = nvmet_req_subsys(req)->passthru_ctrl;\n\tstruct nvme_feat_host_behavior *host;\n\tu16 status = NVME_SC_INTERNAL;\n\tint ret;\n\n\thost = kzalloc(sizeof(*host) * 2, GFP_KERNEL);\n\tif (!host)\n\t\tgoto out_complete_req;\n\n\tret = nvme_get_features(ctrl, NVME_FEAT_HOST_BEHAVIOR, 0,\n\t\t\t\thost, sizeof(*host), NULL);\n\tif (ret)\n\t\tgoto out_free_host;\n\n\tstatus = nvmet_copy_from_sgl(req, 0, &host[1], sizeof(*host));\n\tif (status)\n\t\tgoto out_free_host;\n\n\tif (memcmp(&host[0], &host[1], sizeof(host[0]))) {\n\t\tpr_warn(\"target host has requested different behaviour from the local host\\n\");\n\t\tstatus = NVME_SC_INTERNAL;\n\t}\n\nout_free_host:\n\tkfree(host);\nout_complete_req:\n\tnvmet_req_complete(req, status);\n}\n\nstatic u16 nvmet_setup_passthru_command(struct nvmet_req *req)\n{\n\treq->p.use_workqueue = false;\n\treq->execute = nvmet_passthru_execute_cmd;\n\treturn NVME_SC_SUCCESS;\n}\n\nu16 nvmet_parse_passthru_io_cmd(struct nvmet_req *req)\n{\n\t \n\tif (req->cmd->common.flags & ~NVME_CMD_SGL_ALL)\n\t\treturn NVME_SC_INVALID_FIELD;\n\n\tswitch (req->cmd->common.opcode) {\n\tcase nvme_cmd_resv_register:\n\tcase nvme_cmd_resv_report:\n\tcase nvme_cmd_resv_acquire:\n\tcase nvme_cmd_resv_release:\n\t\t \n\t\treturn NVME_SC_INVALID_OPCODE | NVME_SC_DNR;\n\t}\n\n\treturn nvmet_setup_passthru_command(req);\n}\n\n \nstatic u16 nvmet_passthru_get_set_features(struct nvmet_req *req)\n{\n\tswitch (le32_to_cpu(req->cmd->features.fid)) {\n\tcase NVME_FEAT_ARBITRATION:\n\tcase NVME_FEAT_POWER_MGMT:\n\tcase NVME_FEAT_LBA_RANGE:\n\tcase NVME_FEAT_TEMP_THRESH:\n\tcase NVME_FEAT_ERR_RECOVERY:\n\tcase NVME_FEAT_VOLATILE_WC:\n\tcase NVME_FEAT_WRITE_ATOMIC:\n\tcase NVME_FEAT_AUTO_PST:\n\tcase NVME_FEAT_TIMESTAMP:\n\tcase NVME_FEAT_HCTM:\n\tcase NVME_FEAT_NOPSC:\n\tcase NVME_FEAT_RRL:\n\tcase NVME_FEAT_PLM_CONFIG:\n\tcase NVME_FEAT_PLM_WINDOW:\n\tcase NVME_FEAT_HOST_BEHAVIOR:\n\tcase NVME_FEAT_SANITIZE:\n\tcase NVME_FEAT_VENDOR_START ... NVME_FEAT_VENDOR_END:\n\t\treturn nvmet_setup_passthru_command(req);\n\n\tcase NVME_FEAT_ASYNC_EVENT:\n\t\t \n\tcase NVME_FEAT_IRQ_COALESCE:\n\tcase NVME_FEAT_IRQ_CONFIG:\n\t\t \n\tcase NVME_FEAT_HOST_MEM_BUF:\n\t\t \n\tcase NVME_FEAT_SW_PROGRESS:\n\t\t \n\tcase NVME_FEAT_RESV_MASK:\n\tcase NVME_FEAT_RESV_PERSIST:\n\t\t \n\tdefault:\n\t\treturn NVME_SC_INVALID_OPCODE | NVME_SC_DNR;\n\t}\n}\n\nu16 nvmet_parse_passthru_admin_cmd(struct nvmet_req *req)\n{\n\t \n\tif (req->cmd->common.flags & ~NVME_CMD_SGL_ALL)\n\t\treturn NVME_SC_INVALID_FIELD;\n\n\t \n\tif (req->cmd->common.opcode >= nvme_admin_vendor_start)\n\t\treturn nvmet_setup_passthru_command(req);\n\n\tswitch (req->cmd->common.opcode) {\n\tcase nvme_admin_async_event:\n\t\treq->execute = nvmet_execute_async_event;\n\t\treturn NVME_SC_SUCCESS;\n\tcase nvme_admin_keep_alive:\n\t\t \n\t\treq->execute = nvmet_execute_keep_alive;\n\t\treturn NVME_SC_SUCCESS;\n\tcase nvme_admin_set_features:\n\t\tswitch (le32_to_cpu(req->cmd->features.fid)) {\n\t\tcase NVME_FEAT_ASYNC_EVENT:\n\t\tcase NVME_FEAT_KATO:\n\t\tcase NVME_FEAT_NUM_QUEUES:\n\t\tcase NVME_FEAT_HOST_ID:\n\t\t\treq->execute = nvmet_execute_set_features;\n\t\t\treturn NVME_SC_SUCCESS;\n\t\tcase NVME_FEAT_HOST_BEHAVIOR:\n\t\t\treq->execute = nvmet_passthru_set_host_behaviour;\n\t\t\treturn NVME_SC_SUCCESS;\n\t\tdefault:\n\t\t\treturn nvmet_passthru_get_set_features(req);\n\t\t}\n\t\tbreak;\n\tcase nvme_admin_get_features:\n\t\tswitch (le32_to_cpu(req->cmd->features.fid)) {\n\t\tcase NVME_FEAT_ASYNC_EVENT:\n\t\tcase NVME_FEAT_KATO:\n\t\tcase NVME_FEAT_NUM_QUEUES:\n\t\tcase NVME_FEAT_HOST_ID:\n\t\t\treq->execute = nvmet_execute_get_features;\n\t\t\treturn NVME_SC_SUCCESS;\n\t\tdefault:\n\t\t\treturn nvmet_passthru_get_set_features(req);\n\t\t}\n\t\tbreak;\n\tcase nvme_admin_identify:\n\t\tswitch (req->cmd->identify.cns) {\n\t\tcase NVME_ID_CNS_CTRL:\n\t\t\treq->execute = nvmet_passthru_execute_cmd;\n\t\t\treq->p.use_workqueue = true;\n\t\t\treturn NVME_SC_SUCCESS;\n\t\tcase NVME_ID_CNS_CS_CTRL:\n\t\t\tswitch (req->cmd->identify.csi) {\n\t\t\tcase NVME_CSI_ZNS:\n\t\t\t\treq->execute = nvmet_passthru_execute_cmd;\n\t\t\t\treq->p.use_workqueue = true;\n\t\t\t\treturn NVME_SC_SUCCESS;\n\t\t\t}\n\t\t\treturn NVME_SC_INVALID_OPCODE | NVME_SC_DNR;\n\t\tcase NVME_ID_CNS_NS:\n\t\t\treq->execute = nvmet_passthru_execute_cmd;\n\t\t\treq->p.use_workqueue = true;\n\t\t\treturn NVME_SC_SUCCESS;\n\t\tcase NVME_ID_CNS_CS_NS:\n\t\t\tswitch (req->cmd->identify.csi) {\n\t\t\tcase NVME_CSI_ZNS:\n\t\t\t\treq->execute = nvmet_passthru_execute_cmd;\n\t\t\t\treq->p.use_workqueue = true;\n\t\t\t\treturn NVME_SC_SUCCESS;\n\t\t\t}\n\t\t\treturn NVME_SC_INVALID_OPCODE | NVME_SC_DNR;\n\t\tdefault:\n\t\t\treturn nvmet_setup_passthru_command(req);\n\t\t}\n\tcase nvme_admin_get_log_page:\n\t\treturn nvmet_setup_passthru_command(req);\n\tdefault:\n\t\t \n\t\treturn nvmet_report_invalid_opcode(req);\n\t}\n}\n\nint nvmet_passthru_ctrl_enable(struct nvmet_subsys *subsys)\n{\n\tstruct nvme_ctrl *ctrl;\n\tstruct file *file;\n\tint ret = -EINVAL;\n\tvoid *old;\n\n\tmutex_lock(&subsys->lock);\n\tif (!subsys->passthru_ctrl_path)\n\t\tgoto out_unlock;\n\tif (subsys->passthru_ctrl)\n\t\tgoto out_unlock;\n\n\tif (subsys->nr_namespaces) {\n\t\tpr_info(\"cannot enable both passthru and regular namespaces for a single subsystem\");\n\t\tgoto out_unlock;\n\t}\n\n\tfile = filp_open(subsys->passthru_ctrl_path, O_RDWR, 0);\n\tif (IS_ERR(file)) {\n\t\tret = PTR_ERR(file);\n\t\tgoto out_unlock;\n\t}\n\n\tctrl = nvme_ctrl_from_file(file);\n\tif (!ctrl) {\n\t\tpr_err(\"failed to open nvme controller %s\\n\",\n\t\t       subsys->passthru_ctrl_path);\n\n\t\tgoto out_put_file;\n\t}\n\n\told = xa_cmpxchg(&passthru_subsystems, ctrl->cntlid, NULL,\n\t\t\t subsys, GFP_KERNEL);\n\tif (xa_is_err(old)) {\n\t\tret = xa_err(old);\n\t\tgoto out_put_file;\n\t}\n\n\tif (old)\n\t\tgoto out_put_file;\n\n\tsubsys->passthru_ctrl = ctrl;\n\tsubsys->ver = ctrl->vs;\n\n\tif (subsys->ver < NVME_VS(1, 2, 1)) {\n\t\tpr_warn(\"nvme controller version is too old: %llu.%llu.%llu, advertising 1.2.1\\n\",\n\t\t\tNVME_MAJOR(subsys->ver), NVME_MINOR(subsys->ver),\n\t\t\tNVME_TERTIARY(subsys->ver));\n\t\tsubsys->ver = NVME_VS(1, 2, 1);\n\t}\n\tnvme_get_ctrl(ctrl);\n\t__module_get(subsys->passthru_ctrl->ops->module);\n\tret = 0;\n\nout_put_file:\n\tfilp_close(file, NULL);\nout_unlock:\n\tmutex_unlock(&subsys->lock);\n\treturn ret;\n}\n\nstatic void __nvmet_passthru_ctrl_disable(struct nvmet_subsys *subsys)\n{\n\tif (subsys->passthru_ctrl) {\n\t\txa_erase(&passthru_subsystems, subsys->passthru_ctrl->cntlid);\n\t\tmodule_put(subsys->passthru_ctrl->ops->module);\n\t\tnvme_put_ctrl(subsys->passthru_ctrl);\n\t}\n\tsubsys->passthru_ctrl = NULL;\n\tsubsys->ver = NVMET_DEFAULT_VS;\n}\n\nvoid nvmet_passthru_ctrl_disable(struct nvmet_subsys *subsys)\n{\n\tmutex_lock(&subsys->lock);\n\t__nvmet_passthru_ctrl_disable(subsys);\n\tmutex_unlock(&subsys->lock);\n}\n\nvoid nvmet_passthru_subsys_free(struct nvmet_subsys *subsys)\n{\n\tmutex_lock(&subsys->lock);\n\t__nvmet_passthru_ctrl_disable(subsys);\n\tmutex_unlock(&subsys->lock);\n\tkfree(subsys->passthru_ctrl_path);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}