{
  "module_name": "fc.c",
  "hash_id": "9332d60c97d2d799a9bb17c224f65a4e47e80217d9fcb29b56e513dd54e91fde",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvme/target/fc.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n#include <linux/module.h>\n#include <linux/slab.h>\n#include <linux/blk-mq.h>\n#include <linux/parser.h>\n#include <linux/random.h>\n#include <uapi/scsi/fc/fc_fs.h>\n#include <uapi/scsi/fc/fc_els.h>\n\n#include \"nvmet.h\"\n#include <linux/nvme-fc-driver.h>\n#include <linux/nvme-fc.h>\n#include \"../host/fc.h\"\n\n\n \n\n\n#define NVMET_LS_CTX_COUNT\t\t256\n\nstruct nvmet_fc_tgtport;\nstruct nvmet_fc_tgt_assoc;\n\nstruct nvmet_fc_ls_iod {\t\t \n\tstruct nvmefc_ls_rsp\t\t*lsrsp;\n\tstruct nvmefc_tgt_fcp_req\t*fcpreq;\t \n\n\tstruct list_head\t\tls_rcv_list;  \n\n\tstruct nvmet_fc_tgtport\t\t*tgtport;\n\tstruct nvmet_fc_tgt_assoc\t*assoc;\n\tvoid\t\t\t\t*hosthandle;\n\n\tunion nvmefc_ls_requests\t*rqstbuf;\n\tunion nvmefc_ls_responses\t*rspbuf;\n\tu16\t\t\t\trqstdatalen;\n\tdma_addr_t\t\t\trspdma;\n\n\tstruct scatterlist\t\tsg[2];\n\n\tstruct work_struct\t\twork;\n} __aligned(sizeof(unsigned long long));\n\nstruct nvmet_fc_ls_req_op {\t\t \n\tstruct nvmefc_ls_req\t\tls_req;\n\n\tstruct nvmet_fc_tgtport\t\t*tgtport;\n\tvoid\t\t\t\t*hosthandle;\n\n\tint\t\t\t\tls_error;\n\tstruct list_head\t\tlsreq_list;  \n\tbool\t\t\t\treq_queued;\n};\n\n\n \n#define NVMET_FC_MAX_SEQ_LENGTH\t\t(256 * 1024)\n\nenum nvmet_fcp_datadir {\n\tNVMET_FCP_NODATA,\n\tNVMET_FCP_WRITE,\n\tNVMET_FCP_READ,\n\tNVMET_FCP_ABORTED,\n};\n\nstruct nvmet_fc_fcp_iod {\n\tstruct nvmefc_tgt_fcp_req\t*fcpreq;\n\n\tstruct nvme_fc_cmd_iu\t\tcmdiubuf;\n\tstruct nvme_fc_ersp_iu\t\trspiubuf;\n\tdma_addr_t\t\t\trspdma;\n\tstruct scatterlist\t\t*next_sg;\n\tstruct scatterlist\t\t*data_sg;\n\tint\t\t\t\tdata_sg_cnt;\n\tu32\t\t\t\toffset;\n\tenum nvmet_fcp_datadir\t\tio_dir;\n\tbool\t\t\t\tactive;\n\tbool\t\t\t\tabort;\n\tbool\t\t\t\taborted;\n\tbool\t\t\t\twritedataactive;\n\tspinlock_t\t\t\tflock;\n\n\tstruct nvmet_req\t\treq;\n\tstruct work_struct\t\tdefer_work;\n\n\tstruct nvmet_fc_tgtport\t\t*tgtport;\n\tstruct nvmet_fc_tgt_queue\t*queue;\n\n\tstruct list_head\t\tfcp_list;\t \n};\n\nstruct nvmet_fc_tgtport {\n\tstruct nvmet_fc_target_port\tfc_target_port;\n\n\tstruct list_head\t\ttgt_list;  \n\tstruct device\t\t\t*dev;\t \n\tstruct nvmet_fc_target_template\t*ops;\n\n\tstruct nvmet_fc_ls_iod\t\t*iod;\n\tspinlock_t\t\t\tlock;\n\tstruct list_head\t\tls_rcv_list;\n\tstruct list_head\t\tls_req_list;\n\tstruct list_head\t\tls_busylist;\n\tstruct list_head\t\tassoc_list;\n\tstruct list_head\t\thost_list;\n\tstruct ida\t\t\tassoc_cnt;\n\tstruct nvmet_fc_port_entry\t*pe;\n\tstruct kref\t\t\tref;\n\tu32\t\t\t\tmax_sg_cnt;\n};\n\nstruct nvmet_fc_port_entry {\n\tstruct nvmet_fc_tgtport\t\t*tgtport;\n\tstruct nvmet_port\t\t*port;\n\tu64\t\t\t\tnode_name;\n\tu64\t\t\t\tport_name;\n\tstruct list_head\t\tpe_list;\n};\n\nstruct nvmet_fc_defer_fcp_req {\n\tstruct list_head\t\treq_list;\n\tstruct nvmefc_tgt_fcp_req\t*fcp_req;\n};\n\nstruct nvmet_fc_tgt_queue {\n\tbool\t\t\t\tninetypercent;\n\tu16\t\t\t\tqid;\n\tu16\t\t\t\tsqsize;\n\tu16\t\t\t\tersp_ratio;\n\t__le16\t\t\t\tsqhd;\n\tatomic_t\t\t\tconnected;\n\tatomic_t\t\t\tsqtail;\n\tatomic_t\t\t\tzrspcnt;\n\tatomic_t\t\t\trsn;\n\tspinlock_t\t\t\tqlock;\n\tstruct nvmet_cq\t\t\tnvme_cq;\n\tstruct nvmet_sq\t\t\tnvme_sq;\n\tstruct nvmet_fc_tgt_assoc\t*assoc;\n\tstruct list_head\t\tfod_list;\n\tstruct list_head\t\tpending_cmd_list;\n\tstruct list_head\t\tavail_defer_list;\n\tstruct workqueue_struct\t\t*work_q;\n\tstruct kref\t\t\tref;\n\tstruct rcu_head\t\t\trcu;\n\tstruct nvmet_fc_fcp_iod\t\tfod[];\t\t \n} __aligned(sizeof(unsigned long long));\n\nstruct nvmet_fc_hostport {\n\tstruct nvmet_fc_tgtport\t\t*tgtport;\n\tvoid\t\t\t\t*hosthandle;\n\tstruct list_head\t\thost_list;\n\tstruct kref\t\t\tref;\n\tu8\t\t\t\tinvalid;\n};\n\nstruct nvmet_fc_tgt_assoc {\n\tu64\t\t\t\tassociation_id;\n\tu32\t\t\t\ta_id;\n\tatomic_t\t\t\tterminating;\n\tstruct nvmet_fc_tgtport\t\t*tgtport;\n\tstruct nvmet_fc_hostport\t*hostport;\n\tstruct nvmet_fc_ls_iod\t\t*rcv_disconn;\n\tstruct list_head\t\ta_list;\n\tstruct nvmet_fc_tgt_queue __rcu\t*queues[NVMET_NR_QUEUES + 1];\n\tstruct kref\t\t\tref;\n\tstruct work_struct\t\tdel_work;\n\tstruct rcu_head\t\t\trcu;\n};\n\n\nstatic inline int\nnvmet_fc_iodnum(struct nvmet_fc_ls_iod *iodptr)\n{\n\treturn (iodptr - iodptr->tgtport->iod);\n}\n\nstatic inline int\nnvmet_fc_fodnum(struct nvmet_fc_fcp_iod *fodptr)\n{\n\treturn (fodptr - fodptr->queue->fod);\n}\n\n\n \n#define BYTES_FOR_QID\t\t\tsizeof(u16)\n#define BYTES_FOR_QID_SHIFT\t\t(BYTES_FOR_QID * 8)\n#define NVMET_FC_QUEUEID_MASK\t\t((u64)((1 << BYTES_FOR_QID_SHIFT) - 1))\n\nstatic inline u64\nnvmet_fc_makeconnid(struct nvmet_fc_tgt_assoc *assoc, u16 qid)\n{\n\treturn (assoc->association_id | qid);\n}\n\nstatic inline u64\nnvmet_fc_getassociationid(u64 connectionid)\n{\n\treturn connectionid & ~NVMET_FC_QUEUEID_MASK;\n}\n\nstatic inline u16\nnvmet_fc_getqueueid(u64 connectionid)\n{\n\treturn (u16)(connectionid & NVMET_FC_QUEUEID_MASK);\n}\n\nstatic inline struct nvmet_fc_tgtport *\ntargetport_to_tgtport(struct nvmet_fc_target_port *targetport)\n{\n\treturn container_of(targetport, struct nvmet_fc_tgtport,\n\t\t\t\t fc_target_port);\n}\n\nstatic inline struct nvmet_fc_fcp_iod *\nnvmet_req_to_fod(struct nvmet_req *nvme_req)\n{\n\treturn container_of(nvme_req, struct nvmet_fc_fcp_iod, req);\n}\n\n\n \n\n\nstatic DEFINE_SPINLOCK(nvmet_fc_tgtlock);\n\nstatic LIST_HEAD(nvmet_fc_target_list);\nstatic DEFINE_IDA(nvmet_fc_tgtport_cnt);\nstatic LIST_HEAD(nvmet_fc_portentry_list);\n\n\nstatic void nvmet_fc_handle_ls_rqst_work(struct work_struct *work);\nstatic void nvmet_fc_fcp_rqst_op_defer_work(struct work_struct *work);\nstatic void nvmet_fc_tgt_a_put(struct nvmet_fc_tgt_assoc *assoc);\nstatic int nvmet_fc_tgt_a_get(struct nvmet_fc_tgt_assoc *assoc);\nstatic void nvmet_fc_tgt_q_put(struct nvmet_fc_tgt_queue *queue);\nstatic int nvmet_fc_tgt_q_get(struct nvmet_fc_tgt_queue *queue);\nstatic void nvmet_fc_tgtport_put(struct nvmet_fc_tgtport *tgtport);\nstatic int nvmet_fc_tgtport_get(struct nvmet_fc_tgtport *tgtport);\nstatic void nvmet_fc_handle_fcp_rqst(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\t\tstruct nvmet_fc_fcp_iod *fod);\nstatic void nvmet_fc_delete_target_assoc(struct nvmet_fc_tgt_assoc *assoc);\nstatic void nvmet_fc_xmt_ls_rsp(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_ls_iod *iod);\n\n\n \n\n \n\nstatic inline dma_addr_t\nfc_dma_map_single(struct device *dev, void *ptr, size_t size,\n\t\tenum dma_data_direction dir)\n{\n\treturn dev ? dma_map_single(dev, ptr, size, dir) : (dma_addr_t)0L;\n}\n\nstatic inline int\nfc_dma_mapping_error(struct device *dev, dma_addr_t dma_addr)\n{\n\treturn dev ? dma_mapping_error(dev, dma_addr) : 0;\n}\n\nstatic inline void\nfc_dma_unmap_single(struct device *dev, dma_addr_t addr, size_t size,\n\tenum dma_data_direction dir)\n{\n\tif (dev)\n\t\tdma_unmap_single(dev, addr, size, dir);\n}\n\nstatic inline void\nfc_dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr, size_t size,\n\t\tenum dma_data_direction dir)\n{\n\tif (dev)\n\t\tdma_sync_single_for_cpu(dev, addr, size, dir);\n}\n\nstatic inline void\nfc_dma_sync_single_for_device(struct device *dev, dma_addr_t addr, size_t size,\n\t\tenum dma_data_direction dir)\n{\n\tif (dev)\n\t\tdma_sync_single_for_device(dev, addr, size, dir);\n}\n\n \nstatic int\nfc_map_sg(struct scatterlist *sg, int nents)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tWARN_ON(nents == 0 || sg[0].length == 0);\n\n\tfor_each_sg(sg, s, nents, i) {\n\t\ts->dma_address = 0L;\n#ifdef CONFIG_NEED_SG_DMA_LENGTH\n\t\ts->dma_length = s->length;\n#endif\n\t}\n\treturn nents;\n}\n\nstatic inline int\nfc_dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\tenum dma_data_direction dir)\n{\n\treturn dev ? dma_map_sg(dev, sg, nents, dir) : fc_map_sg(sg, nents);\n}\n\nstatic inline void\nfc_dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\tenum dma_data_direction dir)\n{\n\tif (dev)\n\t\tdma_unmap_sg(dev, sg, nents, dir);\n}\n\n\n \n\n\nstatic void\n__nvmet_fc_finish_ls_req(struct nvmet_fc_ls_req_op *lsop)\n{\n\tstruct nvmet_fc_tgtport *tgtport = lsop->tgtport;\n\tstruct nvmefc_ls_req *lsreq = &lsop->ls_req;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\n\tif (!lsop->req_queued) {\n\t\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\t\treturn;\n\t}\n\n\tlist_del(&lsop->lsreq_list);\n\n\tlsop->req_queued = false;\n\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\n\tfc_dma_unmap_single(tgtport->dev, lsreq->rqstdma,\n\t\t\t\t  (lsreq->rqstlen + lsreq->rsplen),\n\t\t\t\t  DMA_BIDIRECTIONAL);\n\n\tnvmet_fc_tgtport_put(tgtport);\n}\n\nstatic int\n__nvmet_fc_send_ls_req(struct nvmet_fc_tgtport *tgtport,\n\t\tstruct nvmet_fc_ls_req_op *lsop,\n\t\tvoid (*done)(struct nvmefc_ls_req *req, int status))\n{\n\tstruct nvmefc_ls_req *lsreq = &lsop->ls_req;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tif (!tgtport->ops->ls_req)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!nvmet_fc_tgtport_get(tgtport))\n\t\treturn -ESHUTDOWN;\n\n\tlsreq->done = done;\n\tlsop->req_queued = false;\n\tINIT_LIST_HEAD(&lsop->lsreq_list);\n\n\tlsreq->rqstdma = fc_dma_map_single(tgtport->dev, lsreq->rqstaddr,\n\t\t\t\t  lsreq->rqstlen + lsreq->rsplen,\n\t\t\t\t  DMA_BIDIRECTIONAL);\n\tif (fc_dma_mapping_error(tgtport->dev, lsreq->rqstdma)) {\n\t\tret = -EFAULT;\n\t\tgoto out_puttgtport;\n\t}\n\tlsreq->rspdma = lsreq->rqstdma + lsreq->rqstlen;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\n\tlist_add_tail(&lsop->lsreq_list, &tgtport->ls_req_list);\n\n\tlsop->req_queued = true;\n\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\n\tret = tgtport->ops->ls_req(&tgtport->fc_target_port, lsop->hosthandle,\n\t\t\t\t   lsreq);\n\tif (ret)\n\t\tgoto out_unlink;\n\n\treturn 0;\n\nout_unlink:\n\tlsop->ls_error = ret;\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tlsop->req_queued = false;\n\tlist_del(&lsop->lsreq_list);\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\tfc_dma_unmap_single(tgtport->dev, lsreq->rqstdma,\n\t\t\t\t  (lsreq->rqstlen + lsreq->rsplen),\n\t\t\t\t  DMA_BIDIRECTIONAL);\nout_puttgtport:\n\tnvmet_fc_tgtport_put(tgtport);\n\n\treturn ret;\n}\n\nstatic int\nnvmet_fc_send_ls_req_async(struct nvmet_fc_tgtport *tgtport,\n\t\tstruct nvmet_fc_ls_req_op *lsop,\n\t\tvoid (*done)(struct nvmefc_ls_req *req, int status))\n{\n\t \n\n\treturn __nvmet_fc_send_ls_req(tgtport, lsop, done);\n}\n\nstatic void\nnvmet_fc_disconnect_assoc_done(struct nvmefc_ls_req *lsreq, int status)\n{\n\tstruct nvmet_fc_ls_req_op *lsop =\n\t\tcontainer_of(lsreq, struct nvmet_fc_ls_req_op, ls_req);\n\n\t__nvmet_fc_finish_ls_req(lsop);\n\n\t \n\n\tkfree(lsop);\n}\n\n \nstatic void\nnvmet_fc_xmt_disconnect_assoc(struct nvmet_fc_tgt_assoc *assoc)\n{\n\tstruct nvmet_fc_tgtport *tgtport = assoc->tgtport;\n\tstruct fcnvme_ls_disconnect_assoc_rqst *discon_rqst;\n\tstruct fcnvme_ls_disconnect_assoc_acc *discon_acc;\n\tstruct nvmet_fc_ls_req_op *lsop;\n\tstruct nvmefc_ls_req *lsreq;\n\tint ret;\n\n\t \n\tif (!tgtport->ops->ls_req || !assoc->hostport ||\n\t    assoc->hostport->invalid)\n\t\treturn;\n\n\tlsop = kzalloc((sizeof(*lsop) +\n\t\t\tsizeof(*discon_rqst) + sizeof(*discon_acc) +\n\t\t\ttgtport->ops->lsrqst_priv_sz), GFP_KERNEL);\n\tif (!lsop) {\n\t\tdev_info(tgtport->dev,\n\t\t\t\"{%d:%d} send Disconnect Association failed: ENOMEM\\n\",\n\t\t\ttgtport->fc_target_port.port_num, assoc->a_id);\n\t\treturn;\n\t}\n\n\tdiscon_rqst = (struct fcnvme_ls_disconnect_assoc_rqst *)&lsop[1];\n\tdiscon_acc = (struct fcnvme_ls_disconnect_assoc_acc *)&discon_rqst[1];\n\tlsreq = &lsop->ls_req;\n\tif (tgtport->ops->lsrqst_priv_sz)\n\t\tlsreq->private = (void *)&discon_acc[1];\n\telse\n\t\tlsreq->private = NULL;\n\n\tlsop->tgtport = tgtport;\n\tlsop->hosthandle = assoc->hostport->hosthandle;\n\n\tnvmefc_fmt_lsreq_discon_assoc(lsreq, discon_rqst, discon_acc,\n\t\t\t\tassoc->association_id);\n\n\tret = nvmet_fc_send_ls_req_async(tgtport, lsop,\n\t\t\t\tnvmet_fc_disconnect_assoc_done);\n\tif (ret) {\n\t\tdev_info(tgtport->dev,\n\t\t\t\"{%d:%d} XMT Disconnect Association failed: %d\\n\",\n\t\t\ttgtport->fc_target_port.port_num, assoc->a_id, ret);\n\t\tkfree(lsop);\n\t}\n}\n\n\n \n\n\nstatic int\nnvmet_fc_alloc_ls_iodlist(struct nvmet_fc_tgtport *tgtport)\n{\n\tstruct nvmet_fc_ls_iod *iod;\n\tint i;\n\n\tiod = kcalloc(NVMET_LS_CTX_COUNT, sizeof(struct nvmet_fc_ls_iod),\n\t\t\tGFP_KERNEL);\n\tif (!iod)\n\t\treturn -ENOMEM;\n\n\ttgtport->iod = iod;\n\n\tfor (i = 0; i < NVMET_LS_CTX_COUNT; iod++, i++) {\n\t\tINIT_WORK(&iod->work, nvmet_fc_handle_ls_rqst_work);\n\t\tiod->tgtport = tgtport;\n\t\tlist_add_tail(&iod->ls_rcv_list, &tgtport->ls_rcv_list);\n\n\t\tiod->rqstbuf = kzalloc(sizeof(union nvmefc_ls_requests) +\n\t\t\t\t       sizeof(union nvmefc_ls_responses),\n\t\t\t\t       GFP_KERNEL);\n\t\tif (!iod->rqstbuf)\n\t\t\tgoto out_fail;\n\n\t\tiod->rspbuf = (union nvmefc_ls_responses *)&iod->rqstbuf[1];\n\n\t\tiod->rspdma = fc_dma_map_single(tgtport->dev, iod->rspbuf,\n\t\t\t\t\t\tsizeof(*iod->rspbuf),\n\t\t\t\t\t\tDMA_TO_DEVICE);\n\t\tif (fc_dma_mapping_error(tgtport->dev, iod->rspdma))\n\t\t\tgoto out_fail;\n\t}\n\n\treturn 0;\n\nout_fail:\n\tkfree(iod->rqstbuf);\n\tlist_del(&iod->ls_rcv_list);\n\tfor (iod--, i--; i >= 0; iod--, i--) {\n\t\tfc_dma_unmap_single(tgtport->dev, iod->rspdma,\n\t\t\t\tsizeof(*iod->rspbuf), DMA_TO_DEVICE);\n\t\tkfree(iod->rqstbuf);\n\t\tlist_del(&iod->ls_rcv_list);\n\t}\n\n\tkfree(iod);\n\n\treturn -EFAULT;\n}\n\nstatic void\nnvmet_fc_free_ls_iodlist(struct nvmet_fc_tgtport *tgtport)\n{\n\tstruct nvmet_fc_ls_iod *iod = tgtport->iod;\n\tint i;\n\n\tfor (i = 0; i < NVMET_LS_CTX_COUNT; iod++, i++) {\n\t\tfc_dma_unmap_single(tgtport->dev,\n\t\t\t\tiod->rspdma, sizeof(*iod->rspbuf),\n\t\t\t\tDMA_TO_DEVICE);\n\t\tkfree(iod->rqstbuf);\n\t\tlist_del(&iod->ls_rcv_list);\n\t}\n\tkfree(tgtport->iod);\n}\n\nstatic struct nvmet_fc_ls_iod *\nnvmet_fc_alloc_ls_iod(struct nvmet_fc_tgtport *tgtport)\n{\n\tstruct nvmet_fc_ls_iod *iod;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tiod = list_first_entry_or_null(&tgtport->ls_rcv_list,\n\t\t\t\t\tstruct nvmet_fc_ls_iod, ls_rcv_list);\n\tif (iod)\n\t\tlist_move_tail(&iod->ls_rcv_list, &tgtport->ls_busylist);\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\treturn iod;\n}\n\n\nstatic void\nnvmet_fc_free_ls_iod(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tlist_move(&iod->ls_rcv_list, &tgtport->ls_rcv_list);\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n}\n\nstatic void\nnvmet_fc_prep_fcp_iodlist(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_tgt_queue *queue)\n{\n\tstruct nvmet_fc_fcp_iod *fod = queue->fod;\n\tint i;\n\n\tfor (i = 0; i < queue->sqsize; fod++, i++) {\n\t\tINIT_WORK(&fod->defer_work, nvmet_fc_fcp_rqst_op_defer_work);\n\t\tfod->tgtport = tgtport;\n\t\tfod->queue = queue;\n\t\tfod->active = false;\n\t\tfod->abort = false;\n\t\tfod->aborted = false;\n\t\tfod->fcpreq = NULL;\n\t\tlist_add_tail(&fod->fcp_list, &queue->fod_list);\n\t\tspin_lock_init(&fod->flock);\n\n\t\tfod->rspdma = fc_dma_map_single(tgtport->dev, &fod->rspiubuf,\n\t\t\t\t\tsizeof(fod->rspiubuf), DMA_TO_DEVICE);\n\t\tif (fc_dma_mapping_error(tgtport->dev, fod->rspdma)) {\n\t\t\tlist_del(&fod->fcp_list);\n\t\t\tfor (fod--, i--; i >= 0; fod--, i--) {\n\t\t\t\tfc_dma_unmap_single(tgtport->dev, fod->rspdma,\n\t\t\t\t\t\tsizeof(fod->rspiubuf),\n\t\t\t\t\t\tDMA_TO_DEVICE);\n\t\t\t\tfod->rspdma = 0L;\n\t\t\t\tlist_del(&fod->fcp_list);\n\t\t\t}\n\n\t\t\treturn;\n\t\t}\n\t}\n}\n\nstatic void\nnvmet_fc_destroy_fcp_iodlist(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_tgt_queue *queue)\n{\n\tstruct nvmet_fc_fcp_iod *fod = queue->fod;\n\tint i;\n\n\tfor (i = 0; i < queue->sqsize; fod++, i++) {\n\t\tif (fod->rspdma)\n\t\t\tfc_dma_unmap_single(tgtport->dev, fod->rspdma,\n\t\t\t\tsizeof(fod->rspiubuf), DMA_TO_DEVICE);\n\t}\n}\n\nstatic struct nvmet_fc_fcp_iod *\nnvmet_fc_alloc_fcp_iod(struct nvmet_fc_tgt_queue *queue)\n{\n\tstruct nvmet_fc_fcp_iod *fod;\n\n\tlockdep_assert_held(&queue->qlock);\n\n\tfod = list_first_entry_or_null(&queue->fod_list,\n\t\t\t\t\tstruct nvmet_fc_fcp_iod, fcp_list);\n\tif (fod) {\n\t\tlist_del(&fod->fcp_list);\n\t\tfod->active = true;\n\t\t \n\t}\n\treturn fod;\n}\n\n\nstatic void\nnvmet_fc_queue_fcp_req(struct nvmet_fc_tgtport *tgtport,\n\t\t       struct nvmet_fc_tgt_queue *queue,\n\t\t       struct nvmefc_tgt_fcp_req *fcpreq)\n{\n\tstruct nvmet_fc_fcp_iod *fod = fcpreq->nvmet_fc_private;\n\n\t \n\tfcpreq->hwqid = queue->qid ?\n\t\t\t((queue->qid - 1) % tgtport->ops->max_hw_queues) : 0;\n\n\tnvmet_fc_handle_fcp_rqst(tgtport, fod);\n}\n\nstatic void\nnvmet_fc_fcp_rqst_op_defer_work(struct work_struct *work)\n{\n\tstruct nvmet_fc_fcp_iod *fod =\n\t\tcontainer_of(work, struct nvmet_fc_fcp_iod, defer_work);\n\n\t \n\tnvmet_fc_queue_fcp_req(fod->tgtport, fod->queue, fod->fcpreq);\n\n}\n\nstatic void\nnvmet_fc_free_fcp_iod(struct nvmet_fc_tgt_queue *queue,\n\t\t\tstruct nvmet_fc_fcp_iod *fod)\n{\n\tstruct nvmefc_tgt_fcp_req *fcpreq = fod->fcpreq;\n\tstruct nvmet_fc_tgtport *tgtport = fod->tgtport;\n\tstruct nvmet_fc_defer_fcp_req *deferfcp;\n\tunsigned long flags;\n\n\tfc_dma_sync_single_for_cpu(tgtport->dev, fod->rspdma,\n\t\t\t\tsizeof(fod->rspiubuf), DMA_TO_DEVICE);\n\n\tfcpreq->nvmet_fc_private = NULL;\n\n\tfod->active = false;\n\tfod->abort = false;\n\tfod->aborted = false;\n\tfod->writedataactive = false;\n\tfod->fcpreq = NULL;\n\n\ttgtport->ops->fcp_req_release(&tgtport->fc_target_port, fcpreq);\n\n\t \n\tnvmet_fc_tgt_q_put(queue);\n\n\tspin_lock_irqsave(&queue->qlock, flags);\n\tdeferfcp = list_first_entry_or_null(&queue->pending_cmd_list,\n\t\t\t\tstruct nvmet_fc_defer_fcp_req, req_list);\n\tif (!deferfcp) {\n\t\tlist_add_tail(&fod->fcp_list, &fod->queue->fod_list);\n\t\tspin_unlock_irqrestore(&queue->qlock, flags);\n\t\treturn;\n\t}\n\n\t \n\tlist_del(&deferfcp->req_list);\n\n\tfcpreq = deferfcp->fcp_req;\n\n\t \n\tlist_add_tail(&deferfcp->req_list, &queue->avail_defer_list);\n\n\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\t \n\tmemcpy(&fod->cmdiubuf, fcpreq->rspaddr, fcpreq->rsplen);\n\n\t \n\tfcpreq->rspaddr = NULL;\n\tfcpreq->rsplen  = 0;\n\tfcpreq->nvmet_fc_private = fod;\n\tfod->fcpreq = fcpreq;\n\tfod->active = true;\n\n\t \n\ttgtport->ops->defer_rcv(&tgtport->fc_target_port, fcpreq);\n\n\t \n\n\tqueue_work(queue->work_q, &fod->defer_work);\n}\n\nstatic struct nvmet_fc_tgt_queue *\nnvmet_fc_alloc_target_queue(struct nvmet_fc_tgt_assoc *assoc,\n\t\t\tu16 qid, u16 sqsize)\n{\n\tstruct nvmet_fc_tgt_queue *queue;\n\tint ret;\n\n\tif (qid > NVMET_NR_QUEUES)\n\t\treturn NULL;\n\n\tqueue = kzalloc(struct_size(queue, fod, sqsize), GFP_KERNEL);\n\tif (!queue)\n\t\treturn NULL;\n\n\tif (!nvmet_fc_tgt_a_get(assoc))\n\t\tgoto out_free_queue;\n\n\tqueue->work_q = alloc_workqueue(\"ntfc%d.%d.%d\", 0, 0,\n\t\t\t\tassoc->tgtport->fc_target_port.port_num,\n\t\t\t\tassoc->a_id, qid);\n\tif (!queue->work_q)\n\t\tgoto out_a_put;\n\n\tqueue->qid = qid;\n\tqueue->sqsize = sqsize;\n\tqueue->assoc = assoc;\n\tINIT_LIST_HEAD(&queue->fod_list);\n\tINIT_LIST_HEAD(&queue->avail_defer_list);\n\tINIT_LIST_HEAD(&queue->pending_cmd_list);\n\tatomic_set(&queue->connected, 0);\n\tatomic_set(&queue->sqtail, 0);\n\tatomic_set(&queue->rsn, 1);\n\tatomic_set(&queue->zrspcnt, 0);\n\tspin_lock_init(&queue->qlock);\n\tkref_init(&queue->ref);\n\n\tnvmet_fc_prep_fcp_iodlist(assoc->tgtport, queue);\n\n\tret = nvmet_sq_init(&queue->nvme_sq);\n\tif (ret)\n\t\tgoto out_fail_iodlist;\n\n\tWARN_ON(assoc->queues[qid]);\n\trcu_assign_pointer(assoc->queues[qid], queue);\n\n\treturn queue;\n\nout_fail_iodlist:\n\tnvmet_fc_destroy_fcp_iodlist(assoc->tgtport, queue);\n\tdestroy_workqueue(queue->work_q);\nout_a_put:\n\tnvmet_fc_tgt_a_put(assoc);\nout_free_queue:\n\tkfree(queue);\n\treturn NULL;\n}\n\n\nstatic void\nnvmet_fc_tgt_queue_free(struct kref *ref)\n{\n\tstruct nvmet_fc_tgt_queue *queue =\n\t\tcontainer_of(ref, struct nvmet_fc_tgt_queue, ref);\n\n\trcu_assign_pointer(queue->assoc->queues[queue->qid], NULL);\n\n\tnvmet_fc_destroy_fcp_iodlist(queue->assoc->tgtport, queue);\n\n\tnvmet_fc_tgt_a_put(queue->assoc);\n\n\tdestroy_workqueue(queue->work_q);\n\n\tkfree_rcu(queue, rcu);\n}\n\nstatic void\nnvmet_fc_tgt_q_put(struct nvmet_fc_tgt_queue *queue)\n{\n\tkref_put(&queue->ref, nvmet_fc_tgt_queue_free);\n}\n\nstatic int\nnvmet_fc_tgt_q_get(struct nvmet_fc_tgt_queue *queue)\n{\n\treturn kref_get_unless_zero(&queue->ref);\n}\n\n\nstatic void\nnvmet_fc_delete_target_queue(struct nvmet_fc_tgt_queue *queue)\n{\n\tstruct nvmet_fc_tgtport *tgtport = queue->assoc->tgtport;\n\tstruct nvmet_fc_fcp_iod *fod = queue->fod;\n\tstruct nvmet_fc_defer_fcp_req *deferfcp, *tempptr;\n\tunsigned long flags;\n\tint i;\n\tbool disconnect;\n\n\tdisconnect = atomic_xchg(&queue->connected, 0);\n\n\t \n\tif (!disconnect)\n\t\treturn;\n\n\tspin_lock_irqsave(&queue->qlock, flags);\n\t \n\tfor (i = 0; i < queue->sqsize; fod++, i++) {\n\t\tif (fod->active) {\n\t\t\tspin_lock(&fod->flock);\n\t\t\tfod->abort = true;\n\t\t\t \n\t\t\tif (fod->writedataactive) {\n\t\t\t\tfod->aborted = true;\n\t\t\t\tspin_unlock(&fod->flock);\n\t\t\t\ttgtport->ops->fcp_abort(\n\t\t\t\t\t&tgtport->fc_target_port, fod->fcpreq);\n\t\t\t} else\n\t\t\t\tspin_unlock(&fod->flock);\n\t\t}\n\t}\n\n\t \n\tlist_for_each_entry_safe(deferfcp, tempptr, &queue->avail_defer_list,\n\t\t\t\treq_list) {\n\t\tlist_del(&deferfcp->req_list);\n\t\tkfree(deferfcp);\n\t}\n\n\tfor (;;) {\n\t\tdeferfcp = list_first_entry_or_null(&queue->pending_cmd_list,\n\t\t\t\tstruct nvmet_fc_defer_fcp_req, req_list);\n\t\tif (!deferfcp)\n\t\t\tbreak;\n\n\t\tlist_del(&deferfcp->req_list);\n\t\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\t\ttgtport->ops->defer_rcv(&tgtport->fc_target_port,\n\t\t\t\tdeferfcp->fcp_req);\n\n\t\ttgtport->ops->fcp_abort(&tgtport->fc_target_port,\n\t\t\t\tdeferfcp->fcp_req);\n\n\t\ttgtport->ops->fcp_req_release(&tgtport->fc_target_port,\n\t\t\t\tdeferfcp->fcp_req);\n\n\t\t \n\t\tnvmet_fc_tgt_q_put(queue);\n\n\t\tkfree(deferfcp);\n\n\t\tspin_lock_irqsave(&queue->qlock, flags);\n\t}\n\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\tflush_workqueue(queue->work_q);\n\n\tnvmet_sq_destroy(&queue->nvme_sq);\n\n\tnvmet_fc_tgt_q_put(queue);\n}\n\nstatic struct nvmet_fc_tgt_queue *\nnvmet_fc_find_target_queue(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tu64 connection_id)\n{\n\tstruct nvmet_fc_tgt_assoc *assoc;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tu64 association_id = nvmet_fc_getassociationid(connection_id);\n\tu16 qid = nvmet_fc_getqueueid(connection_id);\n\n\tif (qid > NVMET_NR_QUEUES)\n\t\treturn NULL;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(assoc, &tgtport->assoc_list, a_list) {\n\t\tif (association_id == assoc->association_id) {\n\t\t\tqueue = rcu_dereference(assoc->queues[qid]);\n\t\t\tif (queue &&\n\t\t\t    (!atomic_read(&queue->connected) ||\n\t\t\t     !nvmet_fc_tgt_q_get(queue)))\n\t\t\t\tqueue = NULL;\n\t\t\trcu_read_unlock();\n\t\t\treturn queue;\n\t\t}\n\t}\n\trcu_read_unlock();\n\treturn NULL;\n}\n\nstatic void\nnvmet_fc_hostport_free(struct kref *ref)\n{\n\tstruct nvmet_fc_hostport *hostport =\n\t\tcontainer_of(ref, struct nvmet_fc_hostport, ref);\n\tstruct nvmet_fc_tgtport *tgtport = hostport->tgtport;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tlist_del(&hostport->host_list);\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\tif (tgtport->ops->host_release && hostport->invalid)\n\t\ttgtport->ops->host_release(hostport->hosthandle);\n\tkfree(hostport);\n\tnvmet_fc_tgtport_put(tgtport);\n}\n\nstatic void\nnvmet_fc_hostport_put(struct nvmet_fc_hostport *hostport)\n{\n\tkref_put(&hostport->ref, nvmet_fc_hostport_free);\n}\n\nstatic int\nnvmet_fc_hostport_get(struct nvmet_fc_hostport *hostport)\n{\n\treturn kref_get_unless_zero(&hostport->ref);\n}\n\nstatic void\nnvmet_fc_free_hostport(struct nvmet_fc_hostport *hostport)\n{\n\t \n\tif (!hostport || !hostport->hosthandle)\n\t\treturn;\n\n\tnvmet_fc_hostport_put(hostport);\n}\n\nstatic struct nvmet_fc_hostport *\nnvmet_fc_match_hostport(struct nvmet_fc_tgtport *tgtport, void *hosthandle)\n{\n\tstruct nvmet_fc_hostport *host;\n\n\tlockdep_assert_held(&tgtport->lock);\n\n\tlist_for_each_entry(host, &tgtport->host_list, host_list) {\n\t\tif (host->hosthandle == hosthandle && !host->invalid) {\n\t\t\tif (nvmet_fc_hostport_get(host))\n\t\t\t\treturn (host);\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic struct nvmet_fc_hostport *\nnvmet_fc_alloc_hostport(struct nvmet_fc_tgtport *tgtport, void *hosthandle)\n{\n\tstruct nvmet_fc_hostport *newhost, *match = NULL;\n\tunsigned long flags;\n\n\t \n\tif (!hosthandle)\n\t\treturn NULL;\n\n\t \n\tif (!nvmet_fc_tgtport_get(tgtport))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tmatch = nvmet_fc_match_hostport(tgtport, hosthandle);\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\n\tif (match) {\n\t\t \n\t\tnvmet_fc_tgtport_put(tgtport);\n\t\treturn match;\n\t}\n\n\tnewhost = kzalloc(sizeof(*newhost), GFP_KERNEL);\n\tif (!newhost) {\n\t\t \n\t\tnvmet_fc_tgtport_put(tgtport);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tmatch = nvmet_fc_match_hostport(tgtport, hosthandle);\n\tif (match) {\n\t\t \n\t\tkfree(newhost);\n\t\tnewhost = match;\n\t\t \n\t\tnvmet_fc_tgtport_put(tgtport);\n\t} else {\n\t\tnewhost->tgtport = tgtport;\n\t\tnewhost->hosthandle = hosthandle;\n\t\tINIT_LIST_HEAD(&newhost->host_list);\n\t\tkref_init(&newhost->ref);\n\n\t\tlist_add_tail(&newhost->host_list, &tgtport->host_list);\n\t}\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\n\treturn newhost;\n}\n\nstatic void\nnvmet_fc_delete_assoc(struct work_struct *work)\n{\n\tstruct nvmet_fc_tgt_assoc *assoc =\n\t\tcontainer_of(work, struct nvmet_fc_tgt_assoc, del_work);\n\n\tnvmet_fc_delete_target_assoc(assoc);\n\tnvmet_fc_tgt_a_put(assoc);\n}\n\nstatic struct nvmet_fc_tgt_assoc *\nnvmet_fc_alloc_target_assoc(struct nvmet_fc_tgtport *tgtport, void *hosthandle)\n{\n\tstruct nvmet_fc_tgt_assoc *assoc, *tmpassoc;\n\tunsigned long flags;\n\tu64 ran;\n\tint idx;\n\tbool needrandom = true;\n\n\tassoc = kzalloc(sizeof(*assoc), GFP_KERNEL);\n\tif (!assoc)\n\t\treturn NULL;\n\n\tidx = ida_alloc(&tgtport->assoc_cnt, GFP_KERNEL);\n\tif (idx < 0)\n\t\tgoto out_free_assoc;\n\n\tif (!nvmet_fc_tgtport_get(tgtport))\n\t\tgoto out_ida;\n\n\tassoc->hostport = nvmet_fc_alloc_hostport(tgtport, hosthandle);\n\tif (IS_ERR(assoc->hostport))\n\t\tgoto out_put;\n\n\tassoc->tgtport = tgtport;\n\tassoc->a_id = idx;\n\tINIT_LIST_HEAD(&assoc->a_list);\n\tkref_init(&assoc->ref);\n\tINIT_WORK(&assoc->del_work, nvmet_fc_delete_assoc);\n\tatomic_set(&assoc->terminating, 0);\n\n\twhile (needrandom) {\n\t\tget_random_bytes(&ran, sizeof(ran) - BYTES_FOR_QID);\n\t\tran = ran << BYTES_FOR_QID_SHIFT;\n\n\t\tspin_lock_irqsave(&tgtport->lock, flags);\n\t\tneedrandom = false;\n\t\tlist_for_each_entry(tmpassoc, &tgtport->assoc_list, a_list) {\n\t\t\tif (ran == tmpassoc->association_id) {\n\t\t\t\tneedrandom = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (!needrandom) {\n\t\t\tassoc->association_id = ran;\n\t\t\tlist_add_tail_rcu(&assoc->a_list, &tgtport->assoc_list);\n\t\t}\n\t\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\t}\n\n\treturn assoc;\n\nout_put:\n\tnvmet_fc_tgtport_put(tgtport);\nout_ida:\n\tida_free(&tgtport->assoc_cnt, idx);\nout_free_assoc:\n\tkfree(assoc);\n\treturn NULL;\n}\n\nstatic void\nnvmet_fc_target_assoc_free(struct kref *ref)\n{\n\tstruct nvmet_fc_tgt_assoc *assoc =\n\t\tcontainer_of(ref, struct nvmet_fc_tgt_assoc, ref);\n\tstruct nvmet_fc_tgtport *tgtport = assoc->tgtport;\n\tstruct nvmet_fc_ls_iod\t*oldls;\n\tunsigned long flags;\n\n\t \n\tnvmet_fc_xmt_disconnect_assoc(assoc);\n\n\tnvmet_fc_free_hostport(assoc->hostport);\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tlist_del_rcu(&assoc->a_list);\n\toldls = assoc->rcv_disconn;\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\t \n\tif (oldls)\n\t\tnvmet_fc_xmt_ls_rsp(tgtport, oldls);\n\tida_free(&tgtport->assoc_cnt, assoc->a_id);\n\tdev_info(tgtport->dev,\n\t\t\"{%d:%d} Association freed\\n\",\n\t\ttgtport->fc_target_port.port_num, assoc->a_id);\n\tkfree_rcu(assoc, rcu);\n\tnvmet_fc_tgtport_put(tgtport);\n}\n\nstatic void\nnvmet_fc_tgt_a_put(struct nvmet_fc_tgt_assoc *assoc)\n{\n\tkref_put(&assoc->ref, nvmet_fc_target_assoc_free);\n}\n\nstatic int\nnvmet_fc_tgt_a_get(struct nvmet_fc_tgt_assoc *assoc)\n{\n\treturn kref_get_unless_zero(&assoc->ref);\n}\n\nstatic void\nnvmet_fc_delete_target_assoc(struct nvmet_fc_tgt_assoc *assoc)\n{\n\tstruct nvmet_fc_tgtport *tgtport = assoc->tgtport;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tint i, terminating;\n\n\tterminating = atomic_xchg(&assoc->terminating, 1);\n\n\t \n\tif (terminating)\n\t\treturn;\n\n\n\tfor (i = NVMET_NR_QUEUES; i >= 0; i--) {\n\t\trcu_read_lock();\n\t\tqueue = rcu_dereference(assoc->queues[i]);\n\t\tif (!queue) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!nvmet_fc_tgt_q_get(queue)) {\n\t\t\trcu_read_unlock();\n\t\t\tcontinue;\n\t\t}\n\t\trcu_read_unlock();\n\t\tnvmet_fc_delete_target_queue(queue);\n\t\tnvmet_fc_tgt_q_put(queue);\n\t}\n\n\tdev_info(tgtport->dev,\n\t\t\"{%d:%d} Association deleted\\n\",\n\t\ttgtport->fc_target_port.port_num, assoc->a_id);\n\n\tnvmet_fc_tgt_a_put(assoc);\n}\n\nstatic struct nvmet_fc_tgt_assoc *\nnvmet_fc_find_target_assoc(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tu64 association_id)\n{\n\tstruct nvmet_fc_tgt_assoc *assoc;\n\tstruct nvmet_fc_tgt_assoc *ret = NULL;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(assoc, &tgtport->assoc_list, a_list) {\n\t\tif (association_id == assoc->association_id) {\n\t\t\tret = assoc;\n\t\t\tif (!nvmet_fc_tgt_a_get(assoc))\n\t\t\t\tret = NULL;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic void\nnvmet_fc_portentry_bind(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_port_entry *pe,\n\t\t\tstruct nvmet_port *port)\n{\n\tlockdep_assert_held(&nvmet_fc_tgtlock);\n\n\tpe->tgtport = tgtport;\n\ttgtport->pe = pe;\n\n\tpe->port = port;\n\tport->priv = pe;\n\n\tpe->node_name = tgtport->fc_target_port.node_name;\n\tpe->port_name = tgtport->fc_target_port.port_name;\n\tINIT_LIST_HEAD(&pe->pe_list);\n\n\tlist_add_tail(&pe->pe_list, &nvmet_fc_portentry_list);\n}\n\nstatic void\nnvmet_fc_portentry_unbind(struct nvmet_fc_port_entry *pe)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tif (pe->tgtport)\n\t\tpe->tgtport->pe = NULL;\n\tlist_del(&pe->pe_list);\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n}\n\n \nstatic void\nnvmet_fc_portentry_unbind_tgt(struct nvmet_fc_tgtport *tgtport)\n{\n\tstruct nvmet_fc_port_entry *pe;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tpe = tgtport->pe;\n\tif (pe)\n\t\tpe->tgtport = NULL;\n\ttgtport->pe = NULL;\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n}\n\n \nstatic void\nnvmet_fc_portentry_rebind_tgt(struct nvmet_fc_tgtport *tgtport)\n{\n\tstruct nvmet_fc_port_entry *pe;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tlist_for_each_entry(pe, &nvmet_fc_portentry_list, pe_list) {\n\t\tif (tgtport->fc_target_port.node_name == pe->node_name &&\n\t\t    tgtport->fc_target_port.port_name == pe->port_name) {\n\t\t\tWARN_ON(pe->tgtport);\n\t\t\ttgtport->pe = pe;\n\t\t\tpe->tgtport = tgtport;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n}\n\n \nint\nnvmet_fc_register_targetport(struct nvmet_fc_port_info *pinfo,\n\t\t\tstruct nvmet_fc_target_template *template,\n\t\t\tstruct device *dev,\n\t\t\tstruct nvmet_fc_target_port **portptr)\n{\n\tstruct nvmet_fc_tgtport *newrec;\n\tunsigned long flags;\n\tint ret, idx;\n\n\tif (!template->xmt_ls_rsp || !template->fcp_op ||\n\t    !template->fcp_abort ||\n\t    !template->fcp_req_release || !template->targetport_delete ||\n\t    !template->max_hw_queues || !template->max_sgl_segments ||\n\t    !template->max_dif_sgl_segments || !template->dma_boundary) {\n\t\tret = -EINVAL;\n\t\tgoto out_regtgt_failed;\n\t}\n\n\tnewrec = kzalloc((sizeof(*newrec) + template->target_priv_sz),\n\t\t\t GFP_KERNEL);\n\tif (!newrec) {\n\t\tret = -ENOMEM;\n\t\tgoto out_regtgt_failed;\n\t}\n\n\tidx = ida_alloc(&nvmet_fc_tgtport_cnt, GFP_KERNEL);\n\tif (idx < 0) {\n\t\tret = -ENOSPC;\n\t\tgoto out_fail_kfree;\n\t}\n\n\tif (!get_device(dev) && dev) {\n\t\tret = -ENODEV;\n\t\tgoto out_ida_put;\n\t}\n\n\tnewrec->fc_target_port.node_name = pinfo->node_name;\n\tnewrec->fc_target_port.port_name = pinfo->port_name;\n\tif (template->target_priv_sz)\n\t\tnewrec->fc_target_port.private = &newrec[1];\n\telse\n\t\tnewrec->fc_target_port.private = NULL;\n\tnewrec->fc_target_port.port_id = pinfo->port_id;\n\tnewrec->fc_target_port.port_num = idx;\n\tINIT_LIST_HEAD(&newrec->tgt_list);\n\tnewrec->dev = dev;\n\tnewrec->ops = template;\n\tspin_lock_init(&newrec->lock);\n\tINIT_LIST_HEAD(&newrec->ls_rcv_list);\n\tINIT_LIST_HEAD(&newrec->ls_req_list);\n\tINIT_LIST_HEAD(&newrec->ls_busylist);\n\tINIT_LIST_HEAD(&newrec->assoc_list);\n\tINIT_LIST_HEAD(&newrec->host_list);\n\tkref_init(&newrec->ref);\n\tida_init(&newrec->assoc_cnt);\n\tnewrec->max_sg_cnt = template->max_sgl_segments;\n\n\tret = nvmet_fc_alloc_ls_iodlist(newrec);\n\tif (ret) {\n\t\tret = -ENOMEM;\n\t\tgoto out_free_newrec;\n\t}\n\n\tnvmet_fc_portentry_rebind_tgt(newrec);\n\n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tlist_add_tail(&newrec->tgt_list, &nvmet_fc_target_list);\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n\n\t*portptr = &newrec->fc_target_port;\n\treturn 0;\n\nout_free_newrec:\n\tput_device(dev);\nout_ida_put:\n\tida_free(&nvmet_fc_tgtport_cnt, idx);\nout_fail_kfree:\n\tkfree(newrec);\nout_regtgt_failed:\n\t*portptr = NULL;\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nvmet_fc_register_targetport);\n\n\nstatic void\nnvmet_fc_free_tgtport(struct kref *ref)\n{\n\tstruct nvmet_fc_tgtport *tgtport =\n\t\tcontainer_of(ref, struct nvmet_fc_tgtport, ref);\n\tstruct device *dev = tgtport->dev;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tlist_del(&tgtport->tgt_list);\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n\n\tnvmet_fc_free_ls_iodlist(tgtport);\n\n\t \n\ttgtport->ops->targetport_delete(&tgtport->fc_target_port);\n\n\tida_free(&nvmet_fc_tgtport_cnt,\n\t\t\ttgtport->fc_target_port.port_num);\n\n\tida_destroy(&tgtport->assoc_cnt);\n\n\tkfree(tgtport);\n\n\tput_device(dev);\n}\n\nstatic void\nnvmet_fc_tgtport_put(struct nvmet_fc_tgtport *tgtport)\n{\n\tkref_put(&tgtport->ref, nvmet_fc_free_tgtport);\n}\n\nstatic int\nnvmet_fc_tgtport_get(struct nvmet_fc_tgtport *tgtport)\n{\n\treturn kref_get_unless_zero(&tgtport->ref);\n}\n\nstatic void\n__nvmet_fc_free_assocs(struct nvmet_fc_tgtport *tgtport)\n{\n\tstruct nvmet_fc_tgt_assoc *assoc;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(assoc, &tgtport->assoc_list, a_list) {\n\t\tif (!nvmet_fc_tgt_a_get(assoc))\n\t\t\tcontinue;\n\t\tif (!queue_work(nvmet_wq, &assoc->del_work))\n\t\t\t \n\t\t\tnvmet_fc_tgt_a_put(assoc);\n\t}\n\trcu_read_unlock();\n}\n\n \nvoid\nnvmet_fc_invalidate_host(struct nvmet_fc_target_port *target_port,\n\t\t\tvoid *hosthandle)\n{\n\tstruct nvmet_fc_tgtport *tgtport = targetport_to_tgtport(target_port);\n\tstruct nvmet_fc_tgt_assoc *assoc, *next;\n\tunsigned long flags;\n\tbool noassoc = true;\n\n\tspin_lock_irqsave(&tgtport->lock, flags);\n\tlist_for_each_entry_safe(assoc, next,\n\t\t\t\t&tgtport->assoc_list, a_list) {\n\t\tif (!assoc->hostport ||\n\t\t    assoc->hostport->hosthandle != hosthandle)\n\t\t\tcontinue;\n\t\tif (!nvmet_fc_tgt_a_get(assoc))\n\t\t\tcontinue;\n\t\tassoc->hostport->invalid = 1;\n\t\tnoassoc = false;\n\t\tif (!queue_work(nvmet_wq, &assoc->del_work))\n\t\t\t \n\t\t\tnvmet_fc_tgt_a_put(assoc);\n\t}\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\n\t \n\tif (noassoc && tgtport->ops->host_release)\n\t\ttgtport->ops->host_release(hosthandle);\n}\nEXPORT_SYMBOL_GPL(nvmet_fc_invalidate_host);\n\n \nstatic void\nnvmet_fc_delete_ctrl(struct nvmet_ctrl *ctrl)\n{\n\tstruct nvmet_fc_tgtport *tgtport, *next;\n\tstruct nvmet_fc_tgt_assoc *assoc;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tunsigned long flags;\n\tbool found_ctrl = false;\n\n\t \n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tlist_for_each_entry_safe(tgtport, next, &nvmet_fc_target_list,\n\t\t\ttgt_list) {\n\t\tif (!nvmet_fc_tgtport_get(tgtport))\n\t\t\tcontinue;\n\t\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n\n\t\trcu_read_lock();\n\t\tlist_for_each_entry_rcu(assoc, &tgtport->assoc_list, a_list) {\n\t\t\tqueue = rcu_dereference(assoc->queues[0]);\n\t\t\tif (queue && queue->nvme_sq.ctrl == ctrl) {\n\t\t\t\tif (nvmet_fc_tgt_a_get(assoc))\n\t\t\t\t\tfound_ctrl = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\n\t\tnvmet_fc_tgtport_put(tgtport);\n\n\t\tif (found_ctrl) {\n\t\t\tif (!queue_work(nvmet_wq, &assoc->del_work))\n\t\t\t\t \n\t\t\t\tnvmet_fc_tgt_a_put(assoc);\n\t\t\treturn;\n\t\t}\n\n\t\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\t}\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n}\n\n \nint\nnvmet_fc_unregister_targetport(struct nvmet_fc_target_port *target_port)\n{\n\tstruct nvmet_fc_tgtport *tgtport = targetport_to_tgtport(target_port);\n\n\tnvmet_fc_portentry_unbind_tgt(tgtport);\n\n\t \n\t__nvmet_fc_free_assocs(tgtport);\n\n\t \n\n\tnvmet_fc_tgtport_put(tgtport);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(nvmet_fc_unregister_targetport);\n\n\n \n\n\nstatic void\nnvmet_fc_ls_create_association(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tstruct fcnvme_ls_cr_assoc_rqst *rqst = &iod->rqstbuf->rq_cr_assoc;\n\tstruct fcnvme_ls_cr_assoc_acc *acc = &iod->rspbuf->rsp_cr_assoc;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tint ret = 0;\n\n\tmemset(acc, 0, sizeof(*acc));\n\n\t \n\tif (iod->rqstdatalen < FCNVME_LSDESC_CRA_RQST_MINLEN)\n\t\tret = VERR_CR_ASSOC_LEN;\n\telse if (be32_to_cpu(rqst->desc_list_len) <\n\t\t\tFCNVME_LSDESC_CRA_RQST_MIN_LISTLEN)\n\t\tret = VERR_CR_ASSOC_RQST_LEN;\n\telse if (rqst->assoc_cmd.desc_tag !=\n\t\t\tcpu_to_be32(FCNVME_LSDESC_CREATE_ASSOC_CMD))\n\t\tret = VERR_CR_ASSOC_CMD;\n\telse if (be32_to_cpu(rqst->assoc_cmd.desc_len) <\n\t\t\tFCNVME_LSDESC_CRA_CMD_DESC_MIN_DESCLEN)\n\t\tret = VERR_CR_ASSOC_CMD_LEN;\n\telse if (!rqst->assoc_cmd.ersp_ratio ||\n\t\t (be16_to_cpu(rqst->assoc_cmd.ersp_ratio) >=\n\t\t\t\tbe16_to_cpu(rqst->assoc_cmd.sqsize)))\n\t\tret = VERR_ERSP_RATIO;\n\n\telse {\n\t\t \n\t\tiod->assoc = nvmet_fc_alloc_target_assoc(\n\t\t\t\t\t\ttgtport, iod->hosthandle);\n\t\tif (!iod->assoc)\n\t\t\tret = VERR_ASSOC_ALLOC_FAIL;\n\t\telse {\n\t\t\tqueue = nvmet_fc_alloc_target_queue(iod->assoc, 0,\n\t\t\t\t\tbe16_to_cpu(rqst->assoc_cmd.sqsize));\n\t\t\tif (!queue) {\n\t\t\t\tret = VERR_QUEUE_ALLOC_FAIL;\n\t\t\t\tnvmet_fc_tgt_a_put(iod->assoc);\n\t\t\t}\n\t\t}\n\t}\n\n\tif (ret) {\n\t\tdev_err(tgtport->dev,\n\t\t\t\"Create Association LS failed: %s\\n\",\n\t\t\tvalidation_errors[ret]);\n\t\tiod->lsrsp->rsplen = nvme_fc_format_rjt(acc,\n\t\t\t\tsizeof(*acc), rqst->w0.ls_cmd,\n\t\t\t\tFCNVME_RJT_RC_LOGIC,\n\t\t\t\tFCNVME_RJT_EXP_NONE, 0);\n\t\treturn;\n\t}\n\n\tqueue->ersp_ratio = be16_to_cpu(rqst->assoc_cmd.ersp_ratio);\n\tatomic_set(&queue->connected, 1);\n\tqueue->sqhd = 0;\t \n\n\tdev_info(tgtport->dev,\n\t\t\"{%d:%d} Association created\\n\",\n\t\ttgtport->fc_target_port.port_num, iod->assoc->a_id);\n\n\t \n\n\tiod->lsrsp->rsplen = sizeof(*acc);\n\n\tnvme_fc_format_rsp_hdr(acc, FCNVME_LS_ACC,\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_ls_cr_assoc_acc)),\n\t\t\tFCNVME_LS_CREATE_ASSOCIATION);\n\tacc->associd.desc_tag = cpu_to_be32(FCNVME_LSDESC_ASSOC_ID);\n\tacc->associd.desc_len =\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_assoc_id));\n\tacc->associd.association_id =\n\t\t\tcpu_to_be64(nvmet_fc_makeconnid(iod->assoc, 0));\n\tacc->connectid.desc_tag = cpu_to_be32(FCNVME_LSDESC_CONN_ID);\n\tacc->connectid.desc_len =\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_conn_id));\n\tacc->connectid.connection_id = acc->associd.association_id;\n}\n\nstatic void\nnvmet_fc_ls_create_connection(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tstruct fcnvme_ls_cr_conn_rqst *rqst = &iod->rqstbuf->rq_cr_conn;\n\tstruct fcnvme_ls_cr_conn_acc *acc = &iod->rspbuf->rsp_cr_conn;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tint ret = 0;\n\n\tmemset(acc, 0, sizeof(*acc));\n\n\tif (iod->rqstdatalen < sizeof(struct fcnvme_ls_cr_conn_rqst))\n\t\tret = VERR_CR_CONN_LEN;\n\telse if (rqst->desc_list_len !=\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_ls_cr_conn_rqst)))\n\t\tret = VERR_CR_CONN_RQST_LEN;\n\telse if (rqst->associd.desc_tag != cpu_to_be32(FCNVME_LSDESC_ASSOC_ID))\n\t\tret = VERR_ASSOC_ID;\n\telse if (rqst->associd.desc_len !=\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_assoc_id)))\n\t\tret = VERR_ASSOC_ID_LEN;\n\telse if (rqst->connect_cmd.desc_tag !=\n\t\t\tcpu_to_be32(FCNVME_LSDESC_CREATE_CONN_CMD))\n\t\tret = VERR_CR_CONN_CMD;\n\telse if (rqst->connect_cmd.desc_len !=\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_cr_conn_cmd)))\n\t\tret = VERR_CR_CONN_CMD_LEN;\n\telse if (!rqst->connect_cmd.ersp_ratio ||\n\t\t (be16_to_cpu(rqst->connect_cmd.ersp_ratio) >=\n\t\t\t\tbe16_to_cpu(rqst->connect_cmd.sqsize)))\n\t\tret = VERR_ERSP_RATIO;\n\n\telse {\n\t\t \n\t\tiod->assoc = nvmet_fc_find_target_assoc(tgtport,\n\t\t\t\tbe64_to_cpu(rqst->associd.association_id));\n\t\tif (!iod->assoc)\n\t\t\tret = VERR_NO_ASSOC;\n\t\telse {\n\t\t\tqueue = nvmet_fc_alloc_target_queue(iod->assoc,\n\t\t\t\t\tbe16_to_cpu(rqst->connect_cmd.qid),\n\t\t\t\t\tbe16_to_cpu(rqst->connect_cmd.sqsize));\n\t\t\tif (!queue)\n\t\t\t\tret = VERR_QUEUE_ALLOC_FAIL;\n\n\t\t\t \n\t\t\tnvmet_fc_tgt_a_put(iod->assoc);\n\t\t}\n\t}\n\n\tif (ret) {\n\t\tdev_err(tgtport->dev,\n\t\t\t\"Create Connection LS failed: %s\\n\",\n\t\t\tvalidation_errors[ret]);\n\t\tiod->lsrsp->rsplen = nvme_fc_format_rjt(acc,\n\t\t\t\tsizeof(*acc), rqst->w0.ls_cmd,\n\t\t\t\t(ret == VERR_NO_ASSOC) ?\n\t\t\t\t\tFCNVME_RJT_RC_INV_ASSOC :\n\t\t\t\t\tFCNVME_RJT_RC_LOGIC,\n\t\t\t\tFCNVME_RJT_EXP_NONE, 0);\n\t\treturn;\n\t}\n\n\tqueue->ersp_ratio = be16_to_cpu(rqst->connect_cmd.ersp_ratio);\n\tatomic_set(&queue->connected, 1);\n\tqueue->sqhd = 0;\t \n\n\t \n\n\tiod->lsrsp->rsplen = sizeof(*acc);\n\n\tnvme_fc_format_rsp_hdr(acc, FCNVME_LS_ACC,\n\t\t\tfcnvme_lsdesc_len(sizeof(struct fcnvme_ls_cr_conn_acc)),\n\t\t\tFCNVME_LS_CREATE_CONNECTION);\n\tacc->connectid.desc_tag = cpu_to_be32(FCNVME_LSDESC_CONN_ID);\n\tacc->connectid.desc_len =\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_lsdesc_conn_id));\n\tacc->connectid.connection_id =\n\t\t\tcpu_to_be64(nvmet_fc_makeconnid(iod->assoc,\n\t\t\t\tbe16_to_cpu(rqst->connect_cmd.qid)));\n}\n\n \nstatic int\nnvmet_fc_ls_disconnect(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tstruct fcnvme_ls_disconnect_assoc_rqst *rqst =\n\t\t\t\t\t\t&iod->rqstbuf->rq_dis_assoc;\n\tstruct fcnvme_ls_disconnect_assoc_acc *acc =\n\t\t\t\t\t\t&iod->rspbuf->rsp_dis_assoc;\n\tstruct nvmet_fc_tgt_assoc *assoc = NULL;\n\tstruct nvmet_fc_ls_iod *oldls = NULL;\n\tunsigned long flags;\n\tint ret = 0;\n\n\tmemset(acc, 0, sizeof(*acc));\n\n\tret = nvmefc_vldt_lsreq_discon_assoc(iod->rqstdatalen, rqst);\n\tif (!ret) {\n\t\t \n\t\tassoc = nvmet_fc_find_target_assoc(tgtport,\n\t\t\t\tbe64_to_cpu(rqst->associd.association_id));\n\t\tiod->assoc = assoc;\n\t\tif (!assoc)\n\t\t\tret = VERR_NO_ASSOC;\n\t}\n\n\tif (ret || !assoc) {\n\t\tdev_err(tgtport->dev,\n\t\t\t\"Disconnect LS failed: %s\\n\",\n\t\t\tvalidation_errors[ret]);\n\t\tiod->lsrsp->rsplen = nvme_fc_format_rjt(acc,\n\t\t\t\tsizeof(*acc), rqst->w0.ls_cmd,\n\t\t\t\t(ret == VERR_NO_ASSOC) ?\n\t\t\t\t\tFCNVME_RJT_RC_INV_ASSOC :\n\t\t\t\t\tFCNVME_RJT_RC_LOGIC,\n\t\t\t\tFCNVME_RJT_EXP_NONE, 0);\n\t\treturn true;\n\t}\n\n\t \n\n\tiod->lsrsp->rsplen = sizeof(*acc);\n\n\tnvme_fc_format_rsp_hdr(acc, FCNVME_LS_ACC,\n\t\t\tfcnvme_lsdesc_len(\n\t\t\t\tsizeof(struct fcnvme_ls_disconnect_assoc_acc)),\n\t\t\tFCNVME_LS_DISCONNECT_ASSOC);\n\n\t \n\tnvmet_fc_tgt_a_put(assoc);\n\n\t \n\tspin_lock_irqsave(&tgtport->lock, flags);\n\toldls = assoc->rcv_disconn;\n\tassoc->rcv_disconn = iod;\n\tspin_unlock_irqrestore(&tgtport->lock, flags);\n\n\tnvmet_fc_delete_target_assoc(assoc);\n\n\tif (oldls) {\n\t\tdev_info(tgtport->dev,\n\t\t\t\"{%d:%d} Multiple Disconnect Association LS's \"\n\t\t\t\"received\\n\",\n\t\t\ttgtport->fc_target_port.port_num, assoc->a_id);\n\t\t \n\t\toldls->lsrsp->rsplen = nvme_fc_format_rjt(oldls->rspbuf,\n\t\t\t\t\t\tsizeof(*iod->rspbuf),\n\t\t\t\t\t\t \n\t\t\t\t\t\trqst->w0.ls_cmd,\n\t\t\t\t\t\tFCNVME_RJT_RC_UNAB,\n\t\t\t\t\t\tFCNVME_RJT_EXP_NONE, 0);\n\t\tnvmet_fc_xmt_ls_rsp(tgtport, oldls);\n\t}\n\n\treturn false;\n}\n\n\n \n\n\nstatic void nvmet_fc_fcp_nvme_cmd_done(struct nvmet_req *nvme_req);\n\nstatic const struct nvmet_fabrics_ops nvmet_fc_tgt_fcp_ops;\n\nstatic void\nnvmet_fc_xmt_ls_rsp_done(struct nvmefc_ls_rsp *lsrsp)\n{\n\tstruct nvmet_fc_ls_iod *iod = lsrsp->nvme_fc_private;\n\tstruct nvmet_fc_tgtport *tgtport = iod->tgtport;\n\n\tfc_dma_sync_single_for_cpu(tgtport->dev, iod->rspdma,\n\t\t\t\tsizeof(*iod->rspbuf), DMA_TO_DEVICE);\n\tnvmet_fc_free_ls_iod(tgtport, iod);\n\tnvmet_fc_tgtport_put(tgtport);\n}\n\nstatic void\nnvmet_fc_xmt_ls_rsp(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tint ret;\n\n\tfc_dma_sync_single_for_device(tgtport->dev, iod->rspdma,\n\t\t\t\t  sizeof(*iod->rspbuf), DMA_TO_DEVICE);\n\n\tret = tgtport->ops->xmt_ls_rsp(&tgtport->fc_target_port, iod->lsrsp);\n\tif (ret)\n\t\tnvmet_fc_xmt_ls_rsp_done(iod->lsrsp);\n}\n\n \nstatic void\nnvmet_fc_handle_ls_rqst(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_ls_iod *iod)\n{\n\tstruct fcnvme_ls_rqst_w0 *w0 = &iod->rqstbuf->rq_cr_assoc.w0;\n\tbool sendrsp = true;\n\n\tiod->lsrsp->nvme_fc_private = iod;\n\tiod->lsrsp->rspbuf = iod->rspbuf;\n\tiod->lsrsp->rspdma = iod->rspdma;\n\tiod->lsrsp->done = nvmet_fc_xmt_ls_rsp_done;\n\t \n\tiod->lsrsp->rsplen = 0;\n\n\tiod->assoc = NULL;\n\n\t \n\tswitch (w0->ls_cmd) {\n\tcase FCNVME_LS_CREATE_ASSOCIATION:\n\t\t \n\t\tnvmet_fc_ls_create_association(tgtport, iod);\n\t\tbreak;\n\tcase FCNVME_LS_CREATE_CONNECTION:\n\t\t \n\t\tnvmet_fc_ls_create_connection(tgtport, iod);\n\t\tbreak;\n\tcase FCNVME_LS_DISCONNECT_ASSOC:\n\t\t \n\t\tsendrsp = nvmet_fc_ls_disconnect(tgtport, iod);\n\t\tbreak;\n\tdefault:\n\t\tiod->lsrsp->rsplen = nvme_fc_format_rjt(iod->rspbuf,\n\t\t\t\tsizeof(*iod->rspbuf), w0->ls_cmd,\n\t\t\t\tFCNVME_RJT_RC_INVAL, FCNVME_RJT_EXP_NONE, 0);\n\t}\n\n\tif (sendrsp)\n\t\tnvmet_fc_xmt_ls_rsp(tgtport, iod);\n}\n\n \nstatic void\nnvmet_fc_handle_ls_rqst_work(struct work_struct *work)\n{\n\tstruct nvmet_fc_ls_iod *iod =\n\t\tcontainer_of(work, struct nvmet_fc_ls_iod, work);\n\tstruct nvmet_fc_tgtport *tgtport = iod->tgtport;\n\n\tnvmet_fc_handle_ls_rqst(tgtport, iod);\n}\n\n\n \nint\nnvmet_fc_rcv_ls_req(struct nvmet_fc_target_port *target_port,\n\t\t\tvoid *hosthandle,\n\t\t\tstruct nvmefc_ls_rsp *lsrsp,\n\t\t\tvoid *lsreqbuf, u32 lsreqbuf_len)\n{\n\tstruct nvmet_fc_tgtport *tgtport = targetport_to_tgtport(target_port);\n\tstruct nvmet_fc_ls_iod *iod;\n\tstruct fcnvme_ls_rqst_w0 *w0 = (struct fcnvme_ls_rqst_w0 *)lsreqbuf;\n\n\tif (lsreqbuf_len > sizeof(union nvmefc_ls_requests)) {\n\t\tdev_info(tgtport->dev,\n\t\t\t\"RCV %s LS failed: payload too large (%d)\\n\",\n\t\t\t(w0->ls_cmd <= NVME_FC_LAST_LS_CMD_VALUE) ?\n\t\t\t\tnvmefc_ls_names[w0->ls_cmd] : \"\",\n\t\t\tlsreqbuf_len);\n\t\treturn -E2BIG;\n\t}\n\n\tif (!nvmet_fc_tgtport_get(tgtport)) {\n\t\tdev_info(tgtport->dev,\n\t\t\t\"RCV %s LS failed: target deleting\\n\",\n\t\t\t(w0->ls_cmd <= NVME_FC_LAST_LS_CMD_VALUE) ?\n\t\t\t\tnvmefc_ls_names[w0->ls_cmd] : \"\");\n\t\treturn -ESHUTDOWN;\n\t}\n\n\tiod = nvmet_fc_alloc_ls_iod(tgtport);\n\tif (!iod) {\n\t\tdev_info(tgtport->dev,\n\t\t\t\"RCV %s LS failed: context allocation failed\\n\",\n\t\t\t(w0->ls_cmd <= NVME_FC_LAST_LS_CMD_VALUE) ?\n\t\t\t\tnvmefc_ls_names[w0->ls_cmd] : \"\");\n\t\tnvmet_fc_tgtport_put(tgtport);\n\t\treturn -ENOENT;\n\t}\n\n\tiod->lsrsp = lsrsp;\n\tiod->fcpreq = NULL;\n\tmemcpy(iod->rqstbuf, lsreqbuf, lsreqbuf_len);\n\tiod->rqstdatalen = lsreqbuf_len;\n\tiod->hosthandle = hosthandle;\n\n\tqueue_work(nvmet_wq, &iod->work);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(nvmet_fc_rcv_ls_req);\n\n\n \n\nstatic int\nnvmet_fc_alloc_tgt_pgs(struct nvmet_fc_fcp_iod *fod)\n{\n\tstruct scatterlist *sg;\n\tunsigned int nent;\n\n\tsg = sgl_alloc(fod->req.transfer_len, GFP_KERNEL, &nent);\n\tif (!sg)\n\t\tgoto out;\n\n\tfod->data_sg = sg;\n\tfod->data_sg_cnt = nent;\n\tfod->data_sg_cnt = fc_dma_map_sg(fod->tgtport->dev, sg, nent,\n\t\t\t\t((fod->io_dir == NVMET_FCP_WRITE) ?\n\t\t\t\t\tDMA_FROM_DEVICE : DMA_TO_DEVICE));\n\t\t\t\t \n\tfod->next_sg = fod->data_sg;\n\n\treturn 0;\n\nout:\n\treturn NVME_SC_INTERNAL;\n}\n\nstatic void\nnvmet_fc_free_tgt_pgs(struct nvmet_fc_fcp_iod *fod)\n{\n\tif (!fod->data_sg || !fod->data_sg_cnt)\n\t\treturn;\n\n\tfc_dma_unmap_sg(fod->tgtport->dev, fod->data_sg, fod->data_sg_cnt,\n\t\t\t\t((fod->io_dir == NVMET_FCP_WRITE) ?\n\t\t\t\t\tDMA_FROM_DEVICE : DMA_TO_DEVICE));\n\tsgl_free(fod->data_sg);\n\tfod->data_sg = NULL;\n\tfod->data_sg_cnt = 0;\n}\n\n\nstatic bool\nqueue_90percent_full(struct nvmet_fc_tgt_queue *q, u32 sqhd)\n{\n\tu32 sqtail, used;\n\n\t \n\tsqtail = atomic_read(&q->sqtail) % q->sqsize;\n\n\tused = (sqtail < sqhd) ? (sqtail + q->sqsize - sqhd) : (sqtail - sqhd);\n\treturn ((used * 10) >= (((u32)(q->sqsize - 1) * 9)));\n}\n\n \nstatic void\nnvmet_fc_prep_fcp_rsp(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_fcp_iod *fod)\n{\n\tstruct nvme_fc_ersp_iu *ersp = &fod->rspiubuf;\n\tstruct nvme_common_command *sqe = &fod->cmdiubuf.sqe.common;\n\tstruct nvme_completion *cqe = &ersp->cqe;\n\tu32 *cqewd = (u32 *)cqe;\n\tbool send_ersp = false;\n\tu32 rsn, rspcnt, xfr_length;\n\n\tif (fod->fcpreq->op == NVMET_FCOP_READDATA_RSP)\n\t\txfr_length = fod->req.transfer_len;\n\telse\n\t\txfr_length = fod->offset;\n\n\t \n\trspcnt = atomic_inc_return(&fod->queue->zrspcnt);\n\tif (!(rspcnt % fod->queue->ersp_ratio) ||\n\t    nvme_is_fabrics((struct nvme_command *) sqe) ||\n\t    xfr_length != fod->req.transfer_len ||\n\t    (le16_to_cpu(cqe->status) & 0xFFFE) || cqewd[0] || cqewd[1] ||\n\t    (sqe->flags & (NVME_CMD_FUSE_FIRST | NVME_CMD_FUSE_SECOND)) ||\n\t    queue_90percent_full(fod->queue, le16_to_cpu(cqe->sq_head)))\n\t\tsend_ersp = true;\n\n\t \n\tfod->fcpreq->rspaddr = ersp;\n\tfod->fcpreq->rspdma = fod->rspdma;\n\n\tif (!send_ersp) {\n\t\tmemset(ersp, 0, NVME_FC_SIZEOF_ZEROS_RSP);\n\t\tfod->fcpreq->rsplen = NVME_FC_SIZEOF_ZEROS_RSP;\n\t} else {\n\t\tersp->iu_len = cpu_to_be16(sizeof(*ersp)/sizeof(u32));\n\t\trsn = atomic_inc_return(&fod->queue->rsn);\n\t\tersp->rsn = cpu_to_be32(rsn);\n\t\tersp->xfrd_len = cpu_to_be32(xfr_length);\n\t\tfod->fcpreq->rsplen = sizeof(*ersp);\n\t}\n\n\tfc_dma_sync_single_for_device(tgtport->dev, fod->rspdma,\n\t\t\t\t  sizeof(fod->rspiubuf), DMA_TO_DEVICE);\n}\n\nstatic void nvmet_fc_xmt_fcp_op_done(struct nvmefc_tgt_fcp_req *fcpreq);\n\nstatic void\nnvmet_fc_abort_op(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_fcp_iod *fod)\n{\n\tstruct nvmefc_tgt_fcp_req *fcpreq = fod->fcpreq;\n\n\t \n\tnvmet_fc_free_tgt_pgs(fod);\n\n\t \n\t \n\tif (!fod->aborted)\n\t\ttgtport->ops->fcp_abort(&tgtport->fc_target_port, fcpreq);\n\n\tnvmet_fc_free_fcp_iod(fod->queue, fod);\n}\n\nstatic void\nnvmet_fc_xmt_fcp_rsp(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_fcp_iod *fod)\n{\n\tint ret;\n\n\tfod->fcpreq->op = NVMET_FCOP_RSP;\n\tfod->fcpreq->timeout = 0;\n\n\tnvmet_fc_prep_fcp_rsp(tgtport, fod);\n\n\tret = tgtport->ops->fcp_op(&tgtport->fc_target_port, fod->fcpreq);\n\tif (ret)\n\t\tnvmet_fc_abort_op(tgtport, fod);\n}\n\nstatic void\nnvmet_fc_transfer_fcp_data(struct nvmet_fc_tgtport *tgtport,\n\t\t\t\tstruct nvmet_fc_fcp_iod *fod, u8 op)\n{\n\tstruct nvmefc_tgt_fcp_req *fcpreq = fod->fcpreq;\n\tstruct scatterlist *sg = fod->next_sg;\n\tunsigned long flags;\n\tu32 remaininglen = fod->req.transfer_len - fod->offset;\n\tu32 tlen = 0;\n\tint ret;\n\n\tfcpreq->op = op;\n\tfcpreq->offset = fod->offset;\n\tfcpreq->timeout = NVME_FC_TGTOP_TIMEOUT_SEC;\n\n\t \n\tfcpreq->sg = sg;\n\tfcpreq->sg_cnt = 0;\n\twhile (tlen < remaininglen &&\n\t       fcpreq->sg_cnt < tgtport->max_sg_cnt &&\n\t       tlen + sg_dma_len(sg) < NVMET_FC_MAX_SEQ_LENGTH) {\n\t\tfcpreq->sg_cnt++;\n\t\ttlen += sg_dma_len(sg);\n\t\tsg = sg_next(sg);\n\t}\n\tif (tlen < remaininglen && fcpreq->sg_cnt == 0) {\n\t\tfcpreq->sg_cnt++;\n\t\ttlen += min_t(u32, sg_dma_len(sg), remaininglen);\n\t\tsg = sg_next(sg);\n\t}\n\tif (tlen < remaininglen)\n\t\tfod->next_sg = sg;\n\telse\n\t\tfod->next_sg = NULL;\n\n\tfcpreq->transfer_length = tlen;\n\tfcpreq->transferred_length = 0;\n\tfcpreq->fcp_error = 0;\n\tfcpreq->rsplen = 0;\n\n\t \n\tif ((op == NVMET_FCOP_READDATA) &&\n\t    ((fod->offset + fcpreq->transfer_length) == fod->req.transfer_len) &&\n\t    (tgtport->ops->target_features & NVMET_FCTGTFEAT_READDATA_RSP)) {\n\t\tfcpreq->op = NVMET_FCOP_READDATA_RSP;\n\t\tnvmet_fc_prep_fcp_rsp(tgtport, fod);\n\t}\n\n\tret = tgtport->ops->fcp_op(&tgtport->fc_target_port, fod->fcpreq);\n\tif (ret) {\n\t\t \n\t\tfod->abort = true;\n\n\t\tif (op == NVMET_FCOP_WRITEDATA) {\n\t\t\tspin_lock_irqsave(&fod->flock, flags);\n\t\t\tfod->writedataactive = false;\n\t\t\tspin_unlock_irqrestore(&fod->flock, flags);\n\t\t\tnvmet_req_complete(&fod->req, NVME_SC_INTERNAL);\n\t\t} else   {\n\t\t\tfcpreq->fcp_error = ret;\n\t\t\tfcpreq->transferred_length = 0;\n\t\t\tnvmet_fc_xmt_fcp_op_done(fod->fcpreq);\n\t\t}\n\t}\n}\n\nstatic inline bool\n__nvmet_fc_fod_op_abort(struct nvmet_fc_fcp_iod *fod, bool abort)\n{\n\tstruct nvmefc_tgt_fcp_req *fcpreq = fod->fcpreq;\n\tstruct nvmet_fc_tgtport *tgtport = fod->tgtport;\n\n\t \n\tif (abort) {\n\t\tif (fcpreq->op == NVMET_FCOP_WRITEDATA) {\n\t\t\tnvmet_req_complete(&fod->req, NVME_SC_INTERNAL);\n\t\t\treturn true;\n\t\t}\n\n\t\tnvmet_fc_abort_op(tgtport, fod);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic void\nnvmet_fc_fod_op_done(struct nvmet_fc_fcp_iod *fod)\n{\n\tstruct nvmefc_tgt_fcp_req *fcpreq = fod->fcpreq;\n\tstruct nvmet_fc_tgtport *tgtport = fod->tgtport;\n\tunsigned long flags;\n\tbool abort;\n\n\tspin_lock_irqsave(&fod->flock, flags);\n\tabort = fod->abort;\n\tfod->writedataactive = false;\n\tspin_unlock_irqrestore(&fod->flock, flags);\n\n\tswitch (fcpreq->op) {\n\n\tcase NVMET_FCOP_WRITEDATA:\n\t\tif (__nvmet_fc_fod_op_abort(fod, abort))\n\t\t\treturn;\n\t\tif (fcpreq->fcp_error ||\n\t\t    fcpreq->transferred_length != fcpreq->transfer_length) {\n\t\t\tspin_lock_irqsave(&fod->flock, flags);\n\t\t\tfod->abort = true;\n\t\t\tspin_unlock_irqrestore(&fod->flock, flags);\n\n\t\t\tnvmet_req_complete(&fod->req, NVME_SC_INTERNAL);\n\t\t\treturn;\n\t\t}\n\n\t\tfod->offset += fcpreq->transferred_length;\n\t\tif (fod->offset != fod->req.transfer_len) {\n\t\t\tspin_lock_irqsave(&fod->flock, flags);\n\t\t\tfod->writedataactive = true;\n\t\t\tspin_unlock_irqrestore(&fod->flock, flags);\n\n\t\t\t \n\t\t\tnvmet_fc_transfer_fcp_data(tgtport, fod,\n\t\t\t\t\t\tNVMET_FCOP_WRITEDATA);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tfod->req.execute(&fod->req);\n\t\tbreak;\n\n\tcase NVMET_FCOP_READDATA:\n\tcase NVMET_FCOP_READDATA_RSP:\n\t\tif (__nvmet_fc_fod_op_abort(fod, abort))\n\t\t\treturn;\n\t\tif (fcpreq->fcp_error ||\n\t\t    fcpreq->transferred_length != fcpreq->transfer_length) {\n\t\t\tnvmet_fc_abort_op(tgtport, fod);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\n\t\tif (fcpreq->op == NVMET_FCOP_READDATA_RSP) {\n\t\t\t \n\t\t\tnvmet_fc_free_tgt_pgs(fod);\n\t\t\tnvmet_fc_free_fcp_iod(fod->queue, fod);\n\t\t\treturn;\n\t\t}\n\n\t\tfod->offset += fcpreq->transferred_length;\n\t\tif (fod->offset != fod->req.transfer_len) {\n\t\t\t \n\t\t\tnvmet_fc_transfer_fcp_data(tgtport, fod,\n\t\t\t\t\t\tNVMET_FCOP_READDATA);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\n\t\t \n\t\tnvmet_fc_free_tgt_pgs(fod);\n\n\t\tnvmet_fc_xmt_fcp_rsp(tgtport, fod);\n\n\t\tbreak;\n\n\tcase NVMET_FCOP_RSP:\n\t\tif (__nvmet_fc_fod_op_abort(fod, abort))\n\t\t\treturn;\n\t\tnvmet_fc_free_fcp_iod(fod->queue, fod);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void\nnvmet_fc_xmt_fcp_op_done(struct nvmefc_tgt_fcp_req *fcpreq)\n{\n\tstruct nvmet_fc_fcp_iod *fod = fcpreq->nvmet_fc_private;\n\n\tnvmet_fc_fod_op_done(fod);\n}\n\n \nstatic void\n__nvmet_fc_fcp_nvme_cmd_done(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_fcp_iod *fod, int status)\n{\n\tstruct nvme_common_command *sqe = &fod->cmdiubuf.sqe.common;\n\tstruct nvme_completion *cqe = &fod->rspiubuf.cqe;\n\tunsigned long flags;\n\tbool abort;\n\n\tspin_lock_irqsave(&fod->flock, flags);\n\tabort = fod->abort;\n\tspin_unlock_irqrestore(&fod->flock, flags);\n\n\t \n\tif (!status)\n\t\tfod->queue->sqhd = cqe->sq_head;\n\n\tif (abort) {\n\t\tnvmet_fc_abort_op(tgtport, fod);\n\t\treturn;\n\t}\n\n\t \n\tif (status) {\n\t\t \n\t\tmemset(cqe, 0, sizeof(*cqe));\n\t\tcqe->sq_head = fod->queue->sqhd;\t \n\t\tcqe->sq_id = cpu_to_le16(fod->queue->qid);\n\t\tcqe->command_id = sqe->command_id;\n\t\tcqe->status = cpu_to_le16(status);\n\t} else {\n\n\t\t \n\t\tif ((fod->io_dir == NVMET_FCP_READ) && (fod->data_sg_cnt)) {\n\t\t\t \n\t\t\tnvmet_fc_transfer_fcp_data(tgtport, fod,\n\t\t\t\t\t\tNVMET_FCOP_READDATA);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t}\n\n\t \n\tnvmet_fc_free_tgt_pgs(fod);\n\n\tnvmet_fc_xmt_fcp_rsp(tgtport, fod);\n}\n\n\nstatic void\nnvmet_fc_fcp_nvme_cmd_done(struct nvmet_req *nvme_req)\n{\n\tstruct nvmet_fc_fcp_iod *fod = nvmet_req_to_fod(nvme_req);\n\tstruct nvmet_fc_tgtport *tgtport = fod->tgtport;\n\n\t__nvmet_fc_fcp_nvme_cmd_done(tgtport, fod, 0);\n}\n\n\n \nstatic void\nnvmet_fc_handle_fcp_rqst(struct nvmet_fc_tgtport *tgtport,\n\t\t\tstruct nvmet_fc_fcp_iod *fod)\n{\n\tstruct nvme_fc_cmd_iu *cmdiu = &fod->cmdiubuf;\n\tu32 xfrlen = be32_to_cpu(cmdiu->data_len);\n\tint ret;\n\n\t \n\n\tfod->fcpreq->done = nvmet_fc_xmt_fcp_op_done;\n\n\tif (cmdiu->flags & FCNVME_CMD_FLAGS_WRITE) {\n\t\tfod->io_dir = NVMET_FCP_WRITE;\n\t\tif (!nvme_is_write(&cmdiu->sqe))\n\t\t\tgoto transport_error;\n\t} else if (cmdiu->flags & FCNVME_CMD_FLAGS_READ) {\n\t\tfod->io_dir = NVMET_FCP_READ;\n\t\tif (nvme_is_write(&cmdiu->sqe))\n\t\t\tgoto transport_error;\n\t} else {\n\t\tfod->io_dir = NVMET_FCP_NODATA;\n\t\tif (xfrlen)\n\t\t\tgoto transport_error;\n\t}\n\n\tfod->req.cmd = &fod->cmdiubuf.sqe;\n\tfod->req.cqe = &fod->rspiubuf.cqe;\n\tif (tgtport->pe)\n\t\tfod->req.port = tgtport->pe->port;\n\n\t \n\tmemset(&fod->rspiubuf, 0, sizeof(fod->rspiubuf));\n\n\tfod->data_sg = NULL;\n\tfod->data_sg_cnt = 0;\n\n\tret = nvmet_req_init(&fod->req,\n\t\t\t\t&fod->queue->nvme_cq,\n\t\t\t\t&fod->queue->nvme_sq,\n\t\t\t\t&nvmet_fc_tgt_fcp_ops);\n\tif (!ret) {\n\t\t \n\t\t \n\t\treturn;\n\t}\n\n\tfod->req.transfer_len = xfrlen;\n\n\t \n\tatomic_inc(&fod->queue->sqtail);\n\n\tif (fod->req.transfer_len) {\n\t\tret = nvmet_fc_alloc_tgt_pgs(fod);\n\t\tif (ret) {\n\t\t\tnvmet_req_complete(&fod->req, ret);\n\t\t\treturn;\n\t\t}\n\t}\n\tfod->req.sg = fod->data_sg;\n\tfod->req.sg_cnt = fod->data_sg_cnt;\n\tfod->offset = 0;\n\n\tif (fod->io_dir == NVMET_FCP_WRITE) {\n\t\t \n\t\tnvmet_fc_transfer_fcp_data(tgtport, fod, NVMET_FCOP_WRITEDATA);\n\t\treturn;\n\t}\n\n\t \n\tfod->req.execute(&fod->req);\n\treturn;\n\ntransport_error:\n\tnvmet_fc_abort_op(tgtport, fod);\n}\n\n \nint\nnvmet_fc_rcv_fcp_req(struct nvmet_fc_target_port *target_port,\n\t\t\tstruct nvmefc_tgt_fcp_req *fcpreq,\n\t\t\tvoid *cmdiubuf, u32 cmdiubuf_len)\n{\n\tstruct nvmet_fc_tgtport *tgtport = targetport_to_tgtport(target_port);\n\tstruct nvme_fc_cmd_iu *cmdiu = cmdiubuf;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tstruct nvmet_fc_fcp_iod *fod;\n\tstruct nvmet_fc_defer_fcp_req *deferfcp;\n\tunsigned long flags;\n\n\t \n\tif ((cmdiubuf_len != sizeof(*cmdiu)) ||\n\t\t\t(cmdiu->format_id != NVME_CMD_FORMAT_ID) ||\n\t\t\t(cmdiu->fc_id != NVME_CMD_FC_ID) ||\n\t\t\t(be16_to_cpu(cmdiu->iu_len) != (sizeof(*cmdiu)/4)))\n\t\treturn -EIO;\n\n\tqueue = nvmet_fc_find_target_queue(tgtport,\n\t\t\t\tbe64_to_cpu(cmdiu->connection_id));\n\tif (!queue)\n\t\treturn -ENOTCONN;\n\n\t \n\n\tspin_lock_irqsave(&queue->qlock, flags);\n\n\tfod = nvmet_fc_alloc_fcp_iod(queue);\n\tif (fod) {\n\t\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\t\tfcpreq->nvmet_fc_private = fod;\n\t\tfod->fcpreq = fcpreq;\n\n\t\tmemcpy(&fod->cmdiubuf, cmdiubuf, cmdiubuf_len);\n\n\t\tnvmet_fc_queue_fcp_req(tgtport, queue, fcpreq);\n\n\t\treturn 0;\n\t}\n\n\tif (!tgtport->ops->defer_rcv) {\n\t\tspin_unlock_irqrestore(&queue->qlock, flags);\n\t\t \n\t\tnvmet_fc_tgt_q_put(queue);\n\t\treturn -ENOENT;\n\t}\n\n\tdeferfcp = list_first_entry_or_null(&queue->avail_defer_list,\n\t\t\tstruct nvmet_fc_defer_fcp_req, req_list);\n\tif (deferfcp) {\n\t\t \n\t\tlist_del(&deferfcp->req_list);\n\t} else {\n\t\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\t\t \n\t\tdeferfcp = kmalloc(sizeof(*deferfcp), GFP_KERNEL);\n\t\tif (!deferfcp) {\n\t\t\t \n\t\t\tnvmet_fc_tgt_q_put(queue);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tspin_lock_irqsave(&queue->qlock, flags);\n\t}\n\n\t \n\tfcpreq->rspaddr = cmdiubuf;\n\tfcpreq->rsplen  = cmdiubuf_len;\n\tdeferfcp->fcp_req = fcpreq;\n\n\t \n\tlist_add_tail(&deferfcp->req_list, &queue->pending_cmd_list);\n\n\t \n\n\tspin_unlock_irqrestore(&queue->qlock, flags);\n\n\treturn -EOVERFLOW;\n}\nEXPORT_SYMBOL_GPL(nvmet_fc_rcv_fcp_req);\n\n \nvoid\nnvmet_fc_rcv_fcp_abort(struct nvmet_fc_target_port *target_port,\n\t\t\tstruct nvmefc_tgt_fcp_req *fcpreq)\n{\n\tstruct nvmet_fc_fcp_iod *fod = fcpreq->nvmet_fc_private;\n\tstruct nvmet_fc_tgt_queue *queue;\n\tunsigned long flags;\n\n\tif (!fod || fod->fcpreq != fcpreq)\n\t\t \n\t\treturn;\n\n\tqueue = fod->queue;\n\n\tspin_lock_irqsave(&queue->qlock, flags);\n\tif (fod->active) {\n\t\t \n\t\tspin_lock(&fod->flock);\n\t\tfod->abort = true;\n\t\tfod->aborted = true;\n\t\tspin_unlock(&fod->flock);\n\t}\n\tspin_unlock_irqrestore(&queue->qlock, flags);\n}\nEXPORT_SYMBOL_GPL(nvmet_fc_rcv_fcp_abort);\n\n\nstruct nvmet_fc_traddr {\n\tu64\tnn;\n\tu64\tpn;\n};\n\nstatic int\n__nvme_fc_parse_u64(substring_t *sstr, u64 *val)\n{\n\tu64 token64;\n\n\tif (match_u64(sstr, &token64))\n\t\treturn -EINVAL;\n\t*val = token64;\n\n\treturn 0;\n}\n\n \nstatic int\nnvme_fc_parse_traddr(struct nvmet_fc_traddr *traddr, char *buf, size_t blen)\n{\n\tchar name[2 + NVME_FC_TRADDR_HEXNAMELEN + 1];\n\tsubstring_t wwn = { name, &name[sizeof(name)-1] };\n\tint nnoffset, pnoffset;\n\n\t \n\tif (strnlen(buf, blen) == NVME_FC_TRADDR_MAXLENGTH &&\n\t\t\t!strncmp(buf, \"nn-0x\", NVME_FC_TRADDR_OXNNLEN) &&\n\t\t\t!strncmp(&buf[NVME_FC_TRADDR_MAX_PN_OFFSET],\n\t\t\t\t\"pn-0x\", NVME_FC_TRADDR_OXNNLEN)) {\n\t\tnnoffset = NVME_FC_TRADDR_OXNNLEN;\n\t\tpnoffset = NVME_FC_TRADDR_MAX_PN_OFFSET +\n\t\t\t\t\t\tNVME_FC_TRADDR_OXNNLEN;\n\t} else if ((strnlen(buf, blen) == NVME_FC_TRADDR_MINLENGTH &&\n\t\t\t!strncmp(buf, \"nn-\", NVME_FC_TRADDR_NNLEN) &&\n\t\t\t!strncmp(&buf[NVME_FC_TRADDR_MIN_PN_OFFSET],\n\t\t\t\t\"pn-\", NVME_FC_TRADDR_NNLEN))) {\n\t\tnnoffset = NVME_FC_TRADDR_NNLEN;\n\t\tpnoffset = NVME_FC_TRADDR_MIN_PN_OFFSET + NVME_FC_TRADDR_NNLEN;\n\t} else\n\t\tgoto out_einval;\n\n\tname[0] = '0';\n\tname[1] = 'x';\n\tname[2 + NVME_FC_TRADDR_HEXNAMELEN] = 0;\n\n\tmemcpy(&name[2], &buf[nnoffset], NVME_FC_TRADDR_HEXNAMELEN);\n\tif (__nvme_fc_parse_u64(&wwn, &traddr->nn))\n\t\tgoto out_einval;\n\n\tmemcpy(&name[2], &buf[pnoffset], NVME_FC_TRADDR_HEXNAMELEN);\n\tif (__nvme_fc_parse_u64(&wwn, &traddr->pn))\n\t\tgoto out_einval;\n\n\treturn 0;\n\nout_einval:\n\tpr_warn(\"%s: bad traddr string\\n\", __func__);\n\treturn -EINVAL;\n}\n\nstatic int\nnvmet_fc_add_port(struct nvmet_port *port)\n{\n\tstruct nvmet_fc_tgtport *tgtport;\n\tstruct nvmet_fc_port_entry *pe;\n\tstruct nvmet_fc_traddr traddr = { 0L, 0L };\n\tunsigned long flags;\n\tint ret;\n\n\t \n\tif ((port->disc_addr.trtype != NVMF_TRTYPE_FC) ||\n\t    (port->disc_addr.adrfam != NVMF_ADDR_FAMILY_FC))\n\t\treturn -EINVAL;\n\n\t \n\n\tret = nvme_fc_parse_traddr(&traddr, port->disc_addr.traddr,\n\t\t\tsizeof(port->disc_addr.traddr));\n\tif (ret)\n\t\treturn ret;\n\n\tpe = kzalloc(sizeof(*pe), GFP_KERNEL);\n\tif (!pe)\n\t\treturn -ENOMEM;\n\n\tret = -ENXIO;\n\tspin_lock_irqsave(&nvmet_fc_tgtlock, flags);\n\tlist_for_each_entry(tgtport, &nvmet_fc_target_list, tgt_list) {\n\t\tif ((tgtport->fc_target_port.node_name == traddr.nn) &&\n\t\t    (tgtport->fc_target_port.port_name == traddr.pn)) {\n\t\t\t \n\t\t\tif (!tgtport->pe) {\n\t\t\t\tnvmet_fc_portentry_bind(tgtport, pe, port);\n\t\t\t\tret = 0;\n\t\t\t} else\n\t\t\t\tret = -EALREADY;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irqrestore(&nvmet_fc_tgtlock, flags);\n\n\tif (ret)\n\t\tkfree(pe);\n\n\treturn ret;\n}\n\nstatic void\nnvmet_fc_remove_port(struct nvmet_port *port)\n{\n\tstruct nvmet_fc_port_entry *pe = port->priv;\n\n\tnvmet_fc_portentry_unbind(pe);\n\n\tkfree(pe);\n}\n\nstatic void\nnvmet_fc_discovery_chg(struct nvmet_port *port)\n{\n\tstruct nvmet_fc_port_entry *pe = port->priv;\n\tstruct nvmet_fc_tgtport *tgtport = pe->tgtport;\n\n\tif (tgtport && tgtport->ops->discovery_event)\n\t\ttgtport->ops->discovery_event(&tgtport->fc_target_port);\n}\n\nstatic const struct nvmet_fabrics_ops nvmet_fc_tgt_fcp_ops = {\n\t.owner\t\t\t= THIS_MODULE,\n\t.type\t\t\t= NVMF_TRTYPE_FC,\n\t.msdbd\t\t\t= 1,\n\t.add_port\t\t= nvmet_fc_add_port,\n\t.remove_port\t\t= nvmet_fc_remove_port,\n\t.queue_response\t\t= nvmet_fc_fcp_nvme_cmd_done,\n\t.delete_ctrl\t\t= nvmet_fc_delete_ctrl,\n\t.discovery_chg\t\t= nvmet_fc_discovery_chg,\n};\n\nstatic int __init nvmet_fc_init_module(void)\n{\n\treturn nvmet_register_transport(&nvmet_fc_tgt_fcp_ops);\n}\n\nstatic void __exit nvmet_fc_exit_module(void)\n{\n\t \n\tif (!list_empty(&nvmet_fc_target_list))\n\t\tpr_warn(\"%s: targetport list not empty\\n\", __func__);\n\n\tnvmet_unregister_transport(&nvmet_fc_tgt_fcp_ops);\n\n\tida_destroy(&nvmet_fc_tgtport_cnt);\n}\n\nmodule_init(nvmet_fc_init_module);\nmodule_exit(nvmet_fc_exit_module);\n\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}