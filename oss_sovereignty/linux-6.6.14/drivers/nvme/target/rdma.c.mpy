{
  "module_name": "rdma.c",
  "hash_id": "52148510b7a3eb8529747c3150d5fb1d0e62bbc06d96863d9a06433f02e57d77",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvme/target/rdma.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n#include <linux/atomic.h>\n#include <linux/blk-integrity.h>\n#include <linux/ctype.h>\n#include <linux/delay.h>\n#include <linux/err.h>\n#include <linux/init.h>\n#include <linux/module.h>\n#include <linux/nvme.h>\n#include <linux/slab.h>\n#include <linux/string.h>\n#include <linux/wait.h>\n#include <linux/inet.h>\n#include <asm/unaligned.h>\n\n#include <rdma/ib_verbs.h>\n#include <rdma/rdma_cm.h>\n#include <rdma/rw.h>\n#include <rdma/ib_cm.h>\n\n#include <linux/nvme-rdma.h>\n#include \"nvmet.h\"\n\n \n#define NVMET_RDMA_DEFAULT_INLINE_DATA_SIZE\tPAGE_SIZE\n#define NVMET_RDMA_MAX_INLINE_SGE\t\t4\n#define NVMET_RDMA_MAX_INLINE_DATA_SIZE\t\tmax_t(int, SZ_16K, PAGE_SIZE)\n\n \n#define NVMET_RDMA_MAX_MDTS\t\t\t8\n#define NVMET_RDMA_MAX_METADATA_MDTS\t\t5\n\nstruct nvmet_rdma_srq;\n\nstruct nvmet_rdma_cmd {\n\tstruct ib_sge\t\tsge[NVMET_RDMA_MAX_INLINE_SGE + 1];\n\tstruct ib_cqe\t\tcqe;\n\tstruct ib_recv_wr\twr;\n\tstruct scatterlist\tinline_sg[NVMET_RDMA_MAX_INLINE_SGE];\n\tstruct nvme_command     *nvme_cmd;\n\tstruct nvmet_rdma_queue\t*queue;\n\tstruct nvmet_rdma_srq   *nsrq;\n};\n\nenum {\n\tNVMET_RDMA_REQ_INLINE_DATA\t= (1 << 0),\n\tNVMET_RDMA_REQ_INVALIDATE_RKEY\t= (1 << 1),\n};\n\nstruct nvmet_rdma_rsp {\n\tstruct ib_sge\t\tsend_sge;\n\tstruct ib_cqe\t\tsend_cqe;\n\tstruct ib_send_wr\tsend_wr;\n\n\tstruct nvmet_rdma_cmd\t*cmd;\n\tstruct nvmet_rdma_queue\t*queue;\n\n\tstruct ib_cqe\t\tread_cqe;\n\tstruct ib_cqe\t\twrite_cqe;\n\tstruct rdma_rw_ctx\trw;\n\n\tstruct nvmet_req\treq;\n\n\tbool\t\t\tallocated;\n\tu8\t\t\tn_rdma;\n\tu32\t\t\tflags;\n\tu32\t\t\tinvalidate_rkey;\n\n\tstruct list_head\twait_list;\n\tstruct list_head\tfree_list;\n};\n\nenum nvmet_rdma_queue_state {\n\tNVMET_RDMA_Q_CONNECTING,\n\tNVMET_RDMA_Q_LIVE,\n\tNVMET_RDMA_Q_DISCONNECTING,\n};\n\nstruct nvmet_rdma_queue {\n\tstruct rdma_cm_id\t*cm_id;\n\tstruct ib_qp\t\t*qp;\n\tstruct nvmet_port\t*port;\n\tstruct ib_cq\t\t*cq;\n\tatomic_t\t\tsq_wr_avail;\n\tstruct nvmet_rdma_device *dev;\n\tstruct nvmet_rdma_srq   *nsrq;\n\tspinlock_t\t\tstate_lock;\n\tenum nvmet_rdma_queue_state state;\n\tstruct nvmet_cq\t\tnvme_cq;\n\tstruct nvmet_sq\t\tnvme_sq;\n\n\tstruct nvmet_rdma_rsp\t*rsps;\n\tstruct list_head\tfree_rsps;\n\tspinlock_t\t\trsps_lock;\n\tstruct nvmet_rdma_cmd\t*cmds;\n\n\tstruct work_struct\trelease_work;\n\tstruct list_head\trsp_wait_list;\n\tstruct list_head\trsp_wr_wait_list;\n\tspinlock_t\t\trsp_wr_wait_lock;\n\n\tint\t\t\tidx;\n\tint\t\t\thost_qid;\n\tint\t\t\tcomp_vector;\n\tint\t\t\trecv_queue_size;\n\tint\t\t\tsend_queue_size;\n\n\tstruct list_head\tqueue_list;\n};\n\nstruct nvmet_rdma_port {\n\tstruct nvmet_port\t*nport;\n\tstruct sockaddr_storage addr;\n\tstruct rdma_cm_id\t*cm_id;\n\tstruct delayed_work\trepair_work;\n};\n\nstruct nvmet_rdma_srq {\n\tstruct ib_srq            *srq;\n\tstruct nvmet_rdma_cmd    *cmds;\n\tstruct nvmet_rdma_device *ndev;\n};\n\nstruct nvmet_rdma_device {\n\tstruct ib_device\t*device;\n\tstruct ib_pd\t\t*pd;\n\tstruct nvmet_rdma_srq\t**srqs;\n\tint\t\t\tsrq_count;\n\tsize_t\t\t\tsrq_size;\n\tstruct kref\t\tref;\n\tstruct list_head\tentry;\n\tint\t\t\tinline_data_size;\n\tint\t\t\tinline_page_count;\n};\n\nstatic bool nvmet_rdma_use_srq;\nmodule_param_named(use_srq, nvmet_rdma_use_srq, bool, 0444);\nMODULE_PARM_DESC(use_srq, \"Use shared receive queue.\");\n\nstatic int srq_size_set(const char *val, const struct kernel_param *kp);\nstatic const struct kernel_param_ops srq_size_ops = {\n\t.set = srq_size_set,\n\t.get = param_get_int,\n};\n\nstatic int nvmet_rdma_srq_size = 1024;\nmodule_param_cb(srq_size, &srq_size_ops, &nvmet_rdma_srq_size, 0644);\nMODULE_PARM_DESC(srq_size, \"set Shared Receive Queue (SRQ) size, should >= 256 (default: 1024)\");\n\nstatic DEFINE_IDA(nvmet_rdma_queue_ida);\nstatic LIST_HEAD(nvmet_rdma_queue_list);\nstatic DEFINE_MUTEX(nvmet_rdma_queue_mutex);\n\nstatic LIST_HEAD(device_list);\nstatic DEFINE_MUTEX(device_list_mutex);\n\nstatic bool nvmet_rdma_execute_command(struct nvmet_rdma_rsp *rsp);\nstatic void nvmet_rdma_send_done(struct ib_cq *cq, struct ib_wc *wc);\nstatic void nvmet_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc);\nstatic void nvmet_rdma_read_data_done(struct ib_cq *cq, struct ib_wc *wc);\nstatic void nvmet_rdma_write_data_done(struct ib_cq *cq, struct ib_wc *wc);\nstatic void nvmet_rdma_qp_event(struct ib_event *event, void *priv);\nstatic void nvmet_rdma_queue_disconnect(struct nvmet_rdma_queue *queue);\nstatic void nvmet_rdma_free_rsp(struct nvmet_rdma_device *ndev,\n\t\t\t\tstruct nvmet_rdma_rsp *r);\nstatic int nvmet_rdma_alloc_rsp(struct nvmet_rdma_device *ndev,\n\t\t\t\tstruct nvmet_rdma_rsp *r);\n\nstatic const struct nvmet_fabrics_ops nvmet_rdma_ops;\n\nstatic int srq_size_set(const char *val, const struct kernel_param *kp)\n{\n\tint n = 0, ret;\n\n\tret = kstrtoint(val, 10, &n);\n\tif (ret != 0 || n < 256)\n\t\treturn -EINVAL;\n\n\treturn param_set_int(val, kp);\n}\n\nstatic int num_pages(int len)\n{\n\treturn 1 + (((len - 1) & PAGE_MASK) >> PAGE_SHIFT);\n}\n\nstatic inline bool nvmet_rdma_need_data_in(struct nvmet_rdma_rsp *rsp)\n{\n\treturn nvme_is_write(rsp->req.cmd) &&\n\t\trsp->req.transfer_len &&\n\t\t!(rsp->flags & NVMET_RDMA_REQ_INLINE_DATA);\n}\n\nstatic inline bool nvmet_rdma_need_data_out(struct nvmet_rdma_rsp *rsp)\n{\n\treturn !nvme_is_write(rsp->req.cmd) &&\n\t\trsp->req.transfer_len &&\n\t\t!rsp->req.cqe->status &&\n\t\t!(rsp->flags & NVMET_RDMA_REQ_INLINE_DATA);\n}\n\nstatic inline struct nvmet_rdma_rsp *\nnvmet_rdma_get_rsp(struct nvmet_rdma_queue *queue)\n{\n\tstruct nvmet_rdma_rsp *rsp;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&queue->rsps_lock, flags);\n\trsp = list_first_entry_or_null(&queue->free_rsps,\n\t\t\t\tstruct nvmet_rdma_rsp, free_list);\n\tif (likely(rsp))\n\t\tlist_del(&rsp->free_list);\n\tspin_unlock_irqrestore(&queue->rsps_lock, flags);\n\n\tif (unlikely(!rsp)) {\n\t\tint ret;\n\n\t\trsp = kzalloc(sizeof(*rsp), GFP_KERNEL);\n\t\tif (unlikely(!rsp))\n\t\t\treturn NULL;\n\t\tret = nvmet_rdma_alloc_rsp(queue->dev, rsp);\n\t\tif (unlikely(ret)) {\n\t\t\tkfree(rsp);\n\t\t\treturn NULL;\n\t\t}\n\n\t\trsp->allocated = true;\n\t}\n\n\treturn rsp;\n}\n\nstatic inline void\nnvmet_rdma_put_rsp(struct nvmet_rdma_rsp *rsp)\n{\n\tunsigned long flags;\n\n\tif (unlikely(rsp->allocated)) {\n\t\tnvmet_rdma_free_rsp(rsp->queue->dev, rsp);\n\t\tkfree(rsp);\n\t\treturn;\n\t}\n\n\tspin_lock_irqsave(&rsp->queue->rsps_lock, flags);\n\tlist_add_tail(&rsp->free_list, &rsp->queue->free_rsps);\n\tspin_unlock_irqrestore(&rsp->queue->rsps_lock, flags);\n}\n\nstatic void nvmet_rdma_free_inline_pages(struct nvmet_rdma_device *ndev,\n\t\t\t\tstruct nvmet_rdma_cmd *c)\n{\n\tstruct scatterlist *sg;\n\tstruct ib_sge *sge;\n\tint i;\n\n\tif (!ndev->inline_data_size)\n\t\treturn;\n\n\tsg = c->inline_sg;\n\tsge = &c->sge[1];\n\n\tfor (i = 0; i < ndev->inline_page_count; i++, sg++, sge++) {\n\t\tif (sge->length)\n\t\t\tib_dma_unmap_page(ndev->device, sge->addr,\n\t\t\t\t\tsge->length, DMA_FROM_DEVICE);\n\t\tif (sg_page(sg))\n\t\t\t__free_page(sg_page(sg));\n\t}\n}\n\nstatic int nvmet_rdma_alloc_inline_pages(struct nvmet_rdma_device *ndev,\n\t\t\t\tstruct nvmet_rdma_cmd *c)\n{\n\tstruct scatterlist *sg;\n\tstruct ib_sge *sge;\n\tstruct page *pg;\n\tint len;\n\tint i;\n\n\tif (!ndev->inline_data_size)\n\t\treturn 0;\n\n\tsg = c->inline_sg;\n\tsg_init_table(sg, ndev->inline_page_count);\n\tsge = &c->sge[1];\n\tlen = ndev->inline_data_size;\n\n\tfor (i = 0; i < ndev->inline_page_count; i++, sg++, sge++) {\n\t\tpg = alloc_page(GFP_KERNEL);\n\t\tif (!pg)\n\t\t\tgoto out_err;\n\t\tsg_assign_page(sg, pg);\n\t\tsge->addr = ib_dma_map_page(ndev->device,\n\t\t\tpg, 0, PAGE_SIZE, DMA_FROM_DEVICE);\n\t\tif (ib_dma_mapping_error(ndev->device, sge->addr))\n\t\t\tgoto out_err;\n\t\tsge->length = min_t(int, len, PAGE_SIZE);\n\t\tsge->lkey = ndev->pd->local_dma_lkey;\n\t\tlen -= sge->length;\n\t}\n\n\treturn 0;\nout_err:\n\tfor (; i >= 0; i--, sg--, sge--) {\n\t\tif (sge->length)\n\t\t\tib_dma_unmap_page(ndev->device, sge->addr,\n\t\t\t\t\tsge->length, DMA_FROM_DEVICE);\n\t\tif (sg_page(sg))\n\t\t\t__free_page(sg_page(sg));\n\t}\n\treturn -ENOMEM;\n}\n\nstatic int nvmet_rdma_alloc_cmd(struct nvmet_rdma_device *ndev,\n\t\t\tstruct nvmet_rdma_cmd *c, bool admin)\n{\n\t \n\tc->nvme_cmd = kmalloc(sizeof(*c->nvme_cmd), GFP_KERNEL);\n\tif (!c->nvme_cmd)\n\t\tgoto out;\n\n\tc->sge[0].addr = ib_dma_map_single(ndev->device, c->nvme_cmd,\n\t\t\tsizeof(*c->nvme_cmd), DMA_FROM_DEVICE);\n\tif (ib_dma_mapping_error(ndev->device, c->sge[0].addr))\n\t\tgoto out_free_cmd;\n\n\tc->sge[0].length = sizeof(*c->nvme_cmd);\n\tc->sge[0].lkey = ndev->pd->local_dma_lkey;\n\n\tif (!admin && nvmet_rdma_alloc_inline_pages(ndev, c))\n\t\tgoto out_unmap_cmd;\n\n\tc->cqe.done = nvmet_rdma_recv_done;\n\n\tc->wr.wr_cqe = &c->cqe;\n\tc->wr.sg_list = c->sge;\n\tc->wr.num_sge = admin ? 1 : ndev->inline_page_count + 1;\n\n\treturn 0;\n\nout_unmap_cmd:\n\tib_dma_unmap_single(ndev->device, c->sge[0].addr,\n\t\t\tsizeof(*c->nvme_cmd), DMA_FROM_DEVICE);\nout_free_cmd:\n\tkfree(c->nvme_cmd);\n\nout:\n\treturn -ENOMEM;\n}\n\nstatic void nvmet_rdma_free_cmd(struct nvmet_rdma_device *ndev,\n\t\tstruct nvmet_rdma_cmd *c, bool admin)\n{\n\tif (!admin)\n\t\tnvmet_rdma_free_inline_pages(ndev, c);\n\tib_dma_unmap_single(ndev->device, c->sge[0].addr,\n\t\t\t\tsizeof(*c->nvme_cmd), DMA_FROM_DEVICE);\n\tkfree(c->nvme_cmd);\n}\n\nstatic struct nvmet_rdma_cmd *\nnvmet_rdma_alloc_cmds(struct nvmet_rdma_device *ndev,\n\t\tint nr_cmds, bool admin)\n{\n\tstruct nvmet_rdma_cmd *cmds;\n\tint ret = -EINVAL, i;\n\n\tcmds = kcalloc(nr_cmds, sizeof(struct nvmet_rdma_cmd), GFP_KERNEL);\n\tif (!cmds)\n\t\tgoto out;\n\n\tfor (i = 0; i < nr_cmds; i++) {\n\t\tret = nvmet_rdma_alloc_cmd(ndev, cmds + i, admin);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t}\n\n\treturn cmds;\n\nout_free:\n\twhile (--i >= 0)\n\t\tnvmet_rdma_free_cmd(ndev, cmds + i, admin);\n\tkfree(cmds);\nout:\n\treturn ERR_PTR(ret);\n}\n\nstatic void nvmet_rdma_free_cmds(struct nvmet_rdma_device *ndev,\n\t\tstruct nvmet_rdma_cmd *cmds, int nr_cmds, bool admin)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_cmds; i++)\n\t\tnvmet_rdma_free_cmd(ndev, cmds + i, admin);\n\tkfree(cmds);\n}\n\nstatic int nvmet_rdma_alloc_rsp(struct nvmet_rdma_device *ndev,\n\t\tstruct nvmet_rdma_rsp *r)\n{\n\t \n\tr->req.cqe = kmalloc(sizeof(*r->req.cqe), GFP_KERNEL);\n\tif (!r->req.cqe)\n\t\tgoto out;\n\n\tr->send_sge.addr = ib_dma_map_single(ndev->device, r->req.cqe,\n\t\t\tsizeof(*r->req.cqe), DMA_TO_DEVICE);\n\tif (ib_dma_mapping_error(ndev->device, r->send_sge.addr))\n\t\tgoto out_free_rsp;\n\n\tif (ib_dma_pci_p2p_dma_supported(ndev->device))\n\t\tr->req.p2p_client = &ndev->device->dev;\n\tr->send_sge.length = sizeof(*r->req.cqe);\n\tr->send_sge.lkey = ndev->pd->local_dma_lkey;\n\n\tr->send_cqe.done = nvmet_rdma_send_done;\n\n\tr->send_wr.wr_cqe = &r->send_cqe;\n\tr->send_wr.sg_list = &r->send_sge;\n\tr->send_wr.num_sge = 1;\n\tr->send_wr.send_flags = IB_SEND_SIGNALED;\n\n\t \n\tr->read_cqe.done = nvmet_rdma_read_data_done;\n\t \n\tr->write_cqe.done = nvmet_rdma_write_data_done;\n\n\treturn 0;\n\nout_free_rsp:\n\tkfree(r->req.cqe);\nout:\n\treturn -ENOMEM;\n}\n\nstatic void nvmet_rdma_free_rsp(struct nvmet_rdma_device *ndev,\n\t\tstruct nvmet_rdma_rsp *r)\n{\n\tib_dma_unmap_single(ndev->device, r->send_sge.addr,\n\t\t\t\tsizeof(*r->req.cqe), DMA_TO_DEVICE);\n\tkfree(r->req.cqe);\n}\n\nstatic int\nnvmet_rdma_alloc_rsps(struct nvmet_rdma_queue *queue)\n{\n\tstruct nvmet_rdma_device *ndev = queue->dev;\n\tint nr_rsps = queue->recv_queue_size * 2;\n\tint ret = -EINVAL, i;\n\n\tqueue->rsps = kcalloc(nr_rsps, sizeof(struct nvmet_rdma_rsp),\n\t\t\tGFP_KERNEL);\n\tif (!queue->rsps)\n\t\tgoto out;\n\n\tfor (i = 0; i < nr_rsps; i++) {\n\t\tstruct nvmet_rdma_rsp *rsp = &queue->rsps[i];\n\n\t\tret = nvmet_rdma_alloc_rsp(ndev, rsp);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\n\t\tlist_add_tail(&rsp->free_list, &queue->free_rsps);\n\t}\n\n\treturn 0;\n\nout_free:\n\twhile (--i >= 0) {\n\t\tstruct nvmet_rdma_rsp *rsp = &queue->rsps[i];\n\n\t\tlist_del(&rsp->free_list);\n\t\tnvmet_rdma_free_rsp(ndev, rsp);\n\t}\n\tkfree(queue->rsps);\nout:\n\treturn ret;\n}\n\nstatic void nvmet_rdma_free_rsps(struct nvmet_rdma_queue *queue)\n{\n\tstruct nvmet_rdma_device *ndev = queue->dev;\n\tint i, nr_rsps = queue->recv_queue_size * 2;\n\n\tfor (i = 0; i < nr_rsps; i++) {\n\t\tstruct nvmet_rdma_rsp *rsp = &queue->rsps[i];\n\n\t\tlist_del(&rsp->free_list);\n\t\tnvmet_rdma_free_rsp(ndev, rsp);\n\t}\n\tkfree(queue->rsps);\n}\n\nstatic int nvmet_rdma_post_recv(struct nvmet_rdma_device *ndev,\n\t\tstruct nvmet_rdma_cmd *cmd)\n{\n\tint ret;\n\n\tib_dma_sync_single_for_device(ndev->device,\n\t\tcmd->sge[0].addr, cmd->sge[0].length,\n\t\tDMA_FROM_DEVICE);\n\n\tif (cmd->nsrq)\n\t\tret = ib_post_srq_recv(cmd->nsrq->srq, &cmd->wr, NULL);\n\telse\n\t\tret = ib_post_recv(cmd->queue->qp, &cmd->wr, NULL);\n\n\tif (unlikely(ret))\n\t\tpr_err(\"post_recv cmd failed\\n\");\n\n\treturn ret;\n}\n\nstatic void nvmet_rdma_process_wr_wait_list(struct nvmet_rdma_queue *queue)\n{\n\tspin_lock(&queue->rsp_wr_wait_lock);\n\twhile (!list_empty(&queue->rsp_wr_wait_list)) {\n\t\tstruct nvmet_rdma_rsp *rsp;\n\t\tbool ret;\n\n\t\trsp = list_entry(queue->rsp_wr_wait_list.next,\n\t\t\t\tstruct nvmet_rdma_rsp, wait_list);\n\t\tlist_del(&rsp->wait_list);\n\n\t\tspin_unlock(&queue->rsp_wr_wait_lock);\n\t\tret = nvmet_rdma_execute_command(rsp);\n\t\tspin_lock(&queue->rsp_wr_wait_lock);\n\n\t\tif (!ret) {\n\t\t\tlist_add(&rsp->wait_list, &queue->rsp_wr_wait_list);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&queue->rsp_wr_wait_lock);\n}\n\nstatic u16 nvmet_rdma_check_pi_status(struct ib_mr *sig_mr)\n{\n\tstruct ib_mr_status mr_status;\n\tint ret;\n\tu16 status = 0;\n\n\tret = ib_check_mr_status(sig_mr, IB_MR_CHECK_SIG_STATUS, &mr_status);\n\tif (ret) {\n\t\tpr_err(\"ib_check_mr_status failed, ret %d\\n\", ret);\n\t\treturn NVME_SC_INVALID_PI;\n\t}\n\n\tif (mr_status.fail_status & IB_MR_CHECK_SIG_STATUS) {\n\t\tswitch (mr_status.sig_err.err_type) {\n\t\tcase IB_SIG_BAD_GUARD:\n\t\t\tstatus = NVME_SC_GUARD_CHECK;\n\t\t\tbreak;\n\t\tcase IB_SIG_BAD_REFTAG:\n\t\t\tstatus = NVME_SC_REFTAG_CHECK;\n\t\t\tbreak;\n\t\tcase IB_SIG_BAD_APPTAG:\n\t\t\tstatus = NVME_SC_APPTAG_CHECK;\n\t\t\tbreak;\n\t\t}\n\t\tpr_err(\"PI error found type %d expected 0x%x vs actual 0x%x\\n\",\n\t\t       mr_status.sig_err.err_type,\n\t\t       mr_status.sig_err.expected,\n\t\t       mr_status.sig_err.actual);\n\t}\n\n\treturn status;\n}\n\nstatic void nvmet_rdma_set_sig_domain(struct blk_integrity *bi,\n\t\tstruct nvme_command *cmd, struct ib_sig_domain *domain,\n\t\tu16 control, u8 pi_type)\n{\n\tdomain->sig_type = IB_SIG_TYPE_T10_DIF;\n\tdomain->sig.dif.bg_type = IB_T10DIF_CRC;\n\tdomain->sig.dif.pi_interval = 1 << bi->interval_exp;\n\tdomain->sig.dif.ref_tag = le32_to_cpu(cmd->rw.reftag);\n\tif (control & NVME_RW_PRINFO_PRCHK_REF)\n\t\tdomain->sig.dif.ref_remap = true;\n\n\tdomain->sig.dif.app_tag = le16_to_cpu(cmd->rw.apptag);\n\tdomain->sig.dif.apptag_check_mask = le16_to_cpu(cmd->rw.appmask);\n\tdomain->sig.dif.app_escape = true;\n\tif (pi_type == NVME_NS_DPS_PI_TYPE3)\n\t\tdomain->sig.dif.ref_escape = true;\n}\n\nstatic void nvmet_rdma_set_sig_attrs(struct nvmet_req *req,\n\t\t\t\t     struct ib_sig_attrs *sig_attrs)\n{\n\tstruct nvme_command *cmd = req->cmd;\n\tu16 control = le16_to_cpu(cmd->rw.control);\n\tu8 pi_type = req->ns->pi_type;\n\tstruct blk_integrity *bi;\n\n\tbi = bdev_get_integrity(req->ns->bdev);\n\n\tmemset(sig_attrs, 0, sizeof(*sig_attrs));\n\n\tif (control & NVME_RW_PRINFO_PRACT) {\n\t\t \n\t\tsig_attrs->wire.sig_type = IB_SIG_TYPE_NONE;\n\t\tnvmet_rdma_set_sig_domain(bi, cmd, &sig_attrs->mem, control,\n\t\t\t\t\t  pi_type);\n\t\t \n\t\tcontrol &= ~NVME_RW_PRINFO_PRACT;\n\t\tcmd->rw.control = cpu_to_le16(control);\n\t\t \n\t\treq->transfer_len += req->metadata_len;\n\t} else {\n\t\t \n\t\tnvmet_rdma_set_sig_domain(bi, cmd, &sig_attrs->wire, control,\n\t\t\t\t\t  pi_type);\n\t\tnvmet_rdma_set_sig_domain(bi, cmd, &sig_attrs->mem, control,\n\t\t\t\t\t  pi_type);\n\t}\n\n\tif (control & NVME_RW_PRINFO_PRCHK_REF)\n\t\tsig_attrs->check_mask |= IB_SIG_CHECK_REFTAG;\n\tif (control & NVME_RW_PRINFO_PRCHK_GUARD)\n\t\tsig_attrs->check_mask |= IB_SIG_CHECK_GUARD;\n\tif (control & NVME_RW_PRINFO_PRCHK_APP)\n\t\tsig_attrs->check_mask |= IB_SIG_CHECK_APPTAG;\n}\n\nstatic int nvmet_rdma_rw_ctx_init(struct nvmet_rdma_rsp *rsp, u64 addr, u32 key,\n\t\t\t\t  struct ib_sig_attrs *sig_attrs)\n{\n\tstruct rdma_cm_id *cm_id = rsp->queue->cm_id;\n\tstruct nvmet_req *req = &rsp->req;\n\tint ret;\n\n\tif (req->metadata_len)\n\t\tret = rdma_rw_ctx_signature_init(&rsp->rw, cm_id->qp,\n\t\t\tcm_id->port_num, req->sg, req->sg_cnt,\n\t\t\treq->metadata_sg, req->metadata_sg_cnt, sig_attrs,\n\t\t\taddr, key, nvmet_data_dir(req));\n\telse\n\t\tret = rdma_rw_ctx_init(&rsp->rw, cm_id->qp, cm_id->port_num,\n\t\t\t\t       req->sg, req->sg_cnt, 0, addr, key,\n\t\t\t\t       nvmet_data_dir(req));\n\n\treturn ret;\n}\n\nstatic void nvmet_rdma_rw_ctx_destroy(struct nvmet_rdma_rsp *rsp)\n{\n\tstruct rdma_cm_id *cm_id = rsp->queue->cm_id;\n\tstruct nvmet_req *req = &rsp->req;\n\n\tif (req->metadata_len)\n\t\trdma_rw_ctx_destroy_signature(&rsp->rw, cm_id->qp,\n\t\t\tcm_id->port_num, req->sg, req->sg_cnt,\n\t\t\treq->metadata_sg, req->metadata_sg_cnt,\n\t\t\tnvmet_data_dir(req));\n\telse\n\t\trdma_rw_ctx_destroy(&rsp->rw, cm_id->qp, cm_id->port_num,\n\t\t\t\t    req->sg, req->sg_cnt, nvmet_data_dir(req));\n}\n\nstatic void nvmet_rdma_release_rsp(struct nvmet_rdma_rsp *rsp)\n{\n\tstruct nvmet_rdma_queue *queue = rsp->queue;\n\n\tatomic_add(1 + rsp->n_rdma, &queue->sq_wr_avail);\n\n\tif (rsp->n_rdma)\n\t\tnvmet_rdma_rw_ctx_destroy(rsp);\n\n\tif (rsp->req.sg != rsp->cmd->inline_sg)\n\t\tnvmet_req_free_sgls(&rsp->req);\n\n\tif (unlikely(!list_empty_careful(&queue->rsp_wr_wait_list)))\n\t\tnvmet_rdma_process_wr_wait_list(queue);\n\n\tnvmet_rdma_put_rsp(rsp);\n}\n\nstatic void nvmet_rdma_error_comp(struct nvmet_rdma_queue *queue)\n{\n\tif (queue->nvme_sq.ctrl) {\n\t\tnvmet_ctrl_fatal_error(queue->nvme_sq.ctrl);\n\t} else {\n\t\t \n\t\tnvmet_rdma_queue_disconnect(queue);\n\t}\n}\n\nstatic void nvmet_rdma_send_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct nvmet_rdma_rsp *rsp =\n\t\tcontainer_of(wc->wr_cqe, struct nvmet_rdma_rsp, send_cqe);\n\tstruct nvmet_rdma_queue *queue = wc->qp->qp_context;\n\n\tnvmet_rdma_release_rsp(rsp);\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS &&\n\t\t     wc->status != IB_WC_WR_FLUSH_ERR)) {\n\t\tpr_err(\"SEND for CQE 0x%p failed with status %s (%d).\\n\",\n\t\t\twc->wr_cqe, ib_wc_status_msg(wc->status), wc->status);\n\t\tnvmet_rdma_error_comp(queue);\n\t}\n}\n\nstatic void nvmet_rdma_queue_response(struct nvmet_req *req)\n{\n\tstruct nvmet_rdma_rsp *rsp =\n\t\tcontainer_of(req, struct nvmet_rdma_rsp, req);\n\tstruct rdma_cm_id *cm_id = rsp->queue->cm_id;\n\tstruct ib_send_wr *first_wr;\n\n\tif (rsp->flags & NVMET_RDMA_REQ_INVALIDATE_RKEY) {\n\t\trsp->send_wr.opcode = IB_WR_SEND_WITH_INV;\n\t\trsp->send_wr.ex.invalidate_rkey = rsp->invalidate_rkey;\n\t} else {\n\t\trsp->send_wr.opcode = IB_WR_SEND;\n\t}\n\n\tif (nvmet_rdma_need_data_out(rsp)) {\n\t\tif (rsp->req.metadata_len)\n\t\t\tfirst_wr = rdma_rw_ctx_wrs(&rsp->rw, cm_id->qp,\n\t\t\t\t\tcm_id->port_num, &rsp->write_cqe, NULL);\n\t\telse\n\t\t\tfirst_wr = rdma_rw_ctx_wrs(&rsp->rw, cm_id->qp,\n\t\t\t\t\tcm_id->port_num, NULL, &rsp->send_wr);\n\t} else {\n\t\tfirst_wr = &rsp->send_wr;\n\t}\n\n\tnvmet_rdma_post_recv(rsp->queue->dev, rsp->cmd);\n\n\tib_dma_sync_single_for_device(rsp->queue->dev->device,\n\t\trsp->send_sge.addr, rsp->send_sge.length,\n\t\tDMA_TO_DEVICE);\n\n\tif (unlikely(ib_post_send(cm_id->qp, first_wr, NULL))) {\n\t\tpr_err(\"sending cmd response failed\\n\");\n\t\tnvmet_rdma_release_rsp(rsp);\n\t}\n}\n\nstatic void nvmet_rdma_read_data_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct nvmet_rdma_rsp *rsp =\n\t\tcontainer_of(wc->wr_cqe, struct nvmet_rdma_rsp, read_cqe);\n\tstruct nvmet_rdma_queue *queue = wc->qp->qp_context;\n\tu16 status = 0;\n\n\tWARN_ON(rsp->n_rdma <= 0);\n\tatomic_add(rsp->n_rdma, &queue->sq_wr_avail);\n\trsp->n_rdma = 0;\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS)) {\n\t\tnvmet_rdma_rw_ctx_destroy(rsp);\n\t\tnvmet_req_uninit(&rsp->req);\n\t\tnvmet_rdma_release_rsp(rsp);\n\t\tif (wc->status != IB_WC_WR_FLUSH_ERR) {\n\t\t\tpr_info(\"RDMA READ for CQE 0x%p failed with status %s (%d).\\n\",\n\t\t\t\twc->wr_cqe, ib_wc_status_msg(wc->status), wc->status);\n\t\t\tnvmet_rdma_error_comp(queue);\n\t\t}\n\t\treturn;\n\t}\n\n\tif (rsp->req.metadata_len)\n\t\tstatus = nvmet_rdma_check_pi_status(rsp->rw.reg->mr);\n\tnvmet_rdma_rw_ctx_destroy(rsp);\n\n\tif (unlikely(status))\n\t\tnvmet_req_complete(&rsp->req, status);\n\telse\n\t\trsp->req.execute(&rsp->req);\n}\n\nstatic void nvmet_rdma_write_data_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct nvmet_rdma_rsp *rsp =\n\t\tcontainer_of(wc->wr_cqe, struct nvmet_rdma_rsp, write_cqe);\n\tstruct nvmet_rdma_queue *queue = wc->qp->qp_context;\n\tstruct rdma_cm_id *cm_id = rsp->queue->cm_id;\n\tu16 status;\n\n\tif (!IS_ENABLED(CONFIG_BLK_DEV_INTEGRITY))\n\t\treturn;\n\n\tWARN_ON(rsp->n_rdma <= 0);\n\tatomic_add(rsp->n_rdma, &queue->sq_wr_avail);\n\trsp->n_rdma = 0;\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS)) {\n\t\tnvmet_rdma_rw_ctx_destroy(rsp);\n\t\tnvmet_req_uninit(&rsp->req);\n\t\tnvmet_rdma_release_rsp(rsp);\n\t\tif (wc->status != IB_WC_WR_FLUSH_ERR) {\n\t\t\tpr_info(\"RDMA WRITE for CQE failed with status %s (%d).\\n\",\n\t\t\t\tib_wc_status_msg(wc->status), wc->status);\n\t\t\tnvmet_rdma_error_comp(queue);\n\t\t}\n\t\treturn;\n\t}\n\n\t \n\tstatus = nvmet_rdma_check_pi_status(rsp->rw.reg->mr);\n\tif (unlikely(status))\n\t\trsp->req.cqe->status = cpu_to_le16(status << 1);\n\tnvmet_rdma_rw_ctx_destroy(rsp);\n\n\tif (unlikely(ib_post_send(cm_id->qp, &rsp->send_wr, NULL))) {\n\t\tpr_err(\"sending cmd response failed\\n\");\n\t\tnvmet_rdma_release_rsp(rsp);\n\t}\n}\n\nstatic void nvmet_rdma_use_inline_sg(struct nvmet_rdma_rsp *rsp, u32 len,\n\t\tu64 off)\n{\n\tint sg_count = num_pages(len);\n\tstruct scatterlist *sg;\n\tint i;\n\n\tsg = rsp->cmd->inline_sg;\n\tfor (i = 0; i < sg_count; i++, sg++) {\n\t\tif (i < sg_count - 1)\n\t\t\tsg_unmark_end(sg);\n\t\telse\n\t\t\tsg_mark_end(sg);\n\t\tsg->offset = off;\n\t\tsg->length = min_t(int, len, PAGE_SIZE - off);\n\t\tlen -= sg->length;\n\t\tif (!i)\n\t\t\toff = 0;\n\t}\n\n\trsp->req.sg = rsp->cmd->inline_sg;\n\trsp->req.sg_cnt = sg_count;\n}\n\nstatic u16 nvmet_rdma_map_sgl_inline(struct nvmet_rdma_rsp *rsp)\n{\n\tstruct nvme_sgl_desc *sgl = &rsp->req.cmd->common.dptr.sgl;\n\tu64 off = le64_to_cpu(sgl->addr);\n\tu32 len = le32_to_cpu(sgl->length);\n\n\tif (!nvme_is_write(rsp->req.cmd)) {\n\t\trsp->req.error_loc =\n\t\t\toffsetof(struct nvme_common_command, opcode);\n\t\treturn NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t}\n\n\tif (off + len > rsp->queue->dev->inline_data_size) {\n\t\tpr_err(\"invalid inline data offset!\\n\");\n\t\treturn NVME_SC_SGL_INVALID_OFFSET | NVME_SC_DNR;\n\t}\n\n\t \n\tif (!len)\n\t\treturn 0;\n\n\tnvmet_rdma_use_inline_sg(rsp, len, off);\n\trsp->flags |= NVMET_RDMA_REQ_INLINE_DATA;\n\trsp->req.transfer_len += len;\n\treturn 0;\n}\n\nstatic u16 nvmet_rdma_map_sgl_keyed(struct nvmet_rdma_rsp *rsp,\n\t\tstruct nvme_keyed_sgl_desc *sgl, bool invalidate)\n{\n\tu64 addr = le64_to_cpu(sgl->addr);\n\tu32 key = get_unaligned_le32(sgl->key);\n\tstruct ib_sig_attrs sig_attrs;\n\tint ret;\n\n\trsp->req.transfer_len = get_unaligned_le24(sgl->length);\n\n\t \n\tif (!rsp->req.transfer_len)\n\t\treturn 0;\n\n\tif (rsp->req.metadata_len)\n\t\tnvmet_rdma_set_sig_attrs(&rsp->req, &sig_attrs);\n\n\tret = nvmet_req_alloc_sgls(&rsp->req);\n\tif (unlikely(ret < 0))\n\t\tgoto error_out;\n\n\tret = nvmet_rdma_rw_ctx_init(rsp, addr, key, &sig_attrs);\n\tif (unlikely(ret < 0))\n\t\tgoto error_out;\n\trsp->n_rdma += ret;\n\n\tif (invalidate) {\n\t\trsp->invalidate_rkey = key;\n\t\trsp->flags |= NVMET_RDMA_REQ_INVALIDATE_RKEY;\n\t}\n\n\treturn 0;\n\nerror_out:\n\trsp->req.transfer_len = 0;\n\treturn NVME_SC_INTERNAL;\n}\n\nstatic u16 nvmet_rdma_map_sgl(struct nvmet_rdma_rsp *rsp)\n{\n\tstruct nvme_keyed_sgl_desc *sgl = &rsp->req.cmd->common.dptr.ksgl;\n\n\tswitch (sgl->type >> 4) {\n\tcase NVME_SGL_FMT_DATA_DESC:\n\t\tswitch (sgl->type & 0xf) {\n\t\tcase NVME_SGL_FMT_OFFSET:\n\t\t\treturn nvmet_rdma_map_sgl_inline(rsp);\n\t\tdefault:\n\t\t\tpr_err(\"invalid SGL subtype: %#x\\n\", sgl->type);\n\t\t\trsp->req.error_loc =\n\t\t\t\toffsetof(struct nvme_common_command, dptr);\n\t\t\treturn NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t\t}\n\tcase NVME_KEY_SGL_FMT_DATA_DESC:\n\t\tswitch (sgl->type & 0xf) {\n\t\tcase NVME_SGL_FMT_ADDRESS | NVME_SGL_FMT_INVALIDATE:\n\t\t\treturn nvmet_rdma_map_sgl_keyed(rsp, sgl, true);\n\t\tcase NVME_SGL_FMT_ADDRESS:\n\t\t\treturn nvmet_rdma_map_sgl_keyed(rsp, sgl, false);\n\t\tdefault:\n\t\t\tpr_err(\"invalid SGL subtype: %#x\\n\", sgl->type);\n\t\t\trsp->req.error_loc =\n\t\t\t\toffsetof(struct nvme_common_command, dptr);\n\t\t\treturn NVME_SC_INVALID_FIELD | NVME_SC_DNR;\n\t\t}\n\tdefault:\n\t\tpr_err(\"invalid SGL type: %#x\\n\", sgl->type);\n\t\trsp->req.error_loc = offsetof(struct nvme_common_command, dptr);\n\t\treturn NVME_SC_SGL_INVALID_TYPE | NVME_SC_DNR;\n\t}\n}\n\nstatic bool nvmet_rdma_execute_command(struct nvmet_rdma_rsp *rsp)\n{\n\tstruct nvmet_rdma_queue *queue = rsp->queue;\n\n\tif (unlikely(atomic_sub_return(1 + rsp->n_rdma,\n\t\t\t&queue->sq_wr_avail) < 0)) {\n\t\tpr_debug(\"IB send queue full (needed %d): queue %u cntlid %u\\n\",\n\t\t\t\t1 + rsp->n_rdma, queue->idx,\n\t\t\t\tqueue->nvme_sq.ctrl->cntlid);\n\t\tatomic_add(1 + rsp->n_rdma, &queue->sq_wr_avail);\n\t\treturn false;\n\t}\n\n\tif (nvmet_rdma_need_data_in(rsp)) {\n\t\tif (rdma_rw_ctx_post(&rsp->rw, queue->qp,\n\t\t\t\tqueue->cm_id->port_num, &rsp->read_cqe, NULL))\n\t\t\tnvmet_req_complete(&rsp->req, NVME_SC_DATA_XFER_ERROR);\n\t} else {\n\t\trsp->req.execute(&rsp->req);\n\t}\n\n\treturn true;\n}\n\nstatic void nvmet_rdma_handle_command(struct nvmet_rdma_queue *queue,\n\t\tstruct nvmet_rdma_rsp *cmd)\n{\n\tu16 status;\n\n\tib_dma_sync_single_for_cpu(queue->dev->device,\n\t\tcmd->cmd->sge[0].addr, cmd->cmd->sge[0].length,\n\t\tDMA_FROM_DEVICE);\n\tib_dma_sync_single_for_cpu(queue->dev->device,\n\t\tcmd->send_sge.addr, cmd->send_sge.length,\n\t\tDMA_TO_DEVICE);\n\n\tif (!nvmet_req_init(&cmd->req, &queue->nvme_cq,\n\t\t\t&queue->nvme_sq, &nvmet_rdma_ops))\n\t\treturn;\n\n\tstatus = nvmet_rdma_map_sgl(cmd);\n\tif (status)\n\t\tgoto out_err;\n\n\tif (unlikely(!nvmet_rdma_execute_command(cmd))) {\n\t\tspin_lock(&queue->rsp_wr_wait_lock);\n\t\tlist_add_tail(&cmd->wait_list, &queue->rsp_wr_wait_list);\n\t\tspin_unlock(&queue->rsp_wr_wait_lock);\n\t}\n\n\treturn;\n\nout_err:\n\tnvmet_req_complete(&cmd->req, status);\n}\n\nstatic void nvmet_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct nvmet_rdma_cmd *cmd =\n\t\tcontainer_of(wc->wr_cqe, struct nvmet_rdma_cmd, cqe);\n\tstruct nvmet_rdma_queue *queue = wc->qp->qp_context;\n\tstruct nvmet_rdma_rsp *rsp;\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS)) {\n\t\tif (wc->status != IB_WC_WR_FLUSH_ERR) {\n\t\t\tpr_err(\"RECV for CQE 0x%p failed with status %s (%d)\\n\",\n\t\t\t\twc->wr_cqe, ib_wc_status_msg(wc->status),\n\t\t\t\twc->status);\n\t\t\tnvmet_rdma_error_comp(queue);\n\t\t}\n\t\treturn;\n\t}\n\n\tif (unlikely(wc->byte_len < sizeof(struct nvme_command))) {\n\t\tpr_err(\"Ctrl Fatal Error: capsule size less than 64 bytes\\n\");\n\t\tnvmet_rdma_error_comp(queue);\n\t\treturn;\n\t}\n\n\tcmd->queue = queue;\n\trsp = nvmet_rdma_get_rsp(queue);\n\tif (unlikely(!rsp)) {\n\t\t \n\t\tnvmet_rdma_post_recv(queue->dev, cmd);\n\t\treturn;\n\t}\n\trsp->queue = queue;\n\trsp->cmd = cmd;\n\trsp->flags = 0;\n\trsp->req.cmd = cmd->nvme_cmd;\n\trsp->req.port = queue->port;\n\trsp->n_rdma = 0;\n\n\tif (unlikely(queue->state != NVMET_RDMA_Q_LIVE)) {\n\t\tunsigned long flags;\n\n\t\tspin_lock_irqsave(&queue->state_lock, flags);\n\t\tif (queue->state == NVMET_RDMA_Q_CONNECTING)\n\t\t\tlist_add_tail(&rsp->wait_list, &queue->rsp_wait_list);\n\t\telse\n\t\t\tnvmet_rdma_put_rsp(rsp);\n\t\tspin_unlock_irqrestore(&queue->state_lock, flags);\n\t\treturn;\n\t}\n\n\tnvmet_rdma_handle_command(queue, rsp);\n}\n\nstatic void nvmet_rdma_destroy_srq(struct nvmet_rdma_srq *nsrq)\n{\n\tnvmet_rdma_free_cmds(nsrq->ndev, nsrq->cmds, nsrq->ndev->srq_size,\n\t\t\t     false);\n\tib_destroy_srq(nsrq->srq);\n\n\tkfree(nsrq);\n}\n\nstatic void nvmet_rdma_destroy_srqs(struct nvmet_rdma_device *ndev)\n{\n\tint i;\n\n\tif (!ndev->srqs)\n\t\treturn;\n\n\tfor (i = 0; i < ndev->srq_count; i++)\n\t\tnvmet_rdma_destroy_srq(ndev->srqs[i]);\n\n\tkfree(ndev->srqs);\n}\n\nstatic struct nvmet_rdma_srq *\nnvmet_rdma_init_srq(struct nvmet_rdma_device *ndev)\n{\n\tstruct ib_srq_init_attr srq_attr = { NULL, };\n\tsize_t srq_size = ndev->srq_size;\n\tstruct nvmet_rdma_srq *nsrq;\n\tstruct ib_srq *srq;\n\tint ret, i;\n\n\tnsrq = kzalloc(sizeof(*nsrq), GFP_KERNEL);\n\tif (!nsrq)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tsrq_attr.attr.max_wr = srq_size;\n\tsrq_attr.attr.max_sge = 1 + ndev->inline_page_count;\n\tsrq_attr.attr.srq_limit = 0;\n\tsrq_attr.srq_type = IB_SRQT_BASIC;\n\tsrq = ib_create_srq(ndev->pd, &srq_attr);\n\tif (IS_ERR(srq)) {\n\t\tret = PTR_ERR(srq);\n\t\tgoto out_free;\n\t}\n\n\tnsrq->cmds = nvmet_rdma_alloc_cmds(ndev, srq_size, false);\n\tif (IS_ERR(nsrq->cmds)) {\n\t\tret = PTR_ERR(nsrq->cmds);\n\t\tgoto out_destroy_srq;\n\t}\n\n\tnsrq->srq = srq;\n\tnsrq->ndev = ndev;\n\n\tfor (i = 0; i < srq_size; i++) {\n\t\tnsrq->cmds[i].nsrq = nsrq;\n\t\tret = nvmet_rdma_post_recv(ndev, &nsrq->cmds[i]);\n\t\tif (ret)\n\t\t\tgoto out_free_cmds;\n\t}\n\n\treturn nsrq;\n\nout_free_cmds:\n\tnvmet_rdma_free_cmds(ndev, nsrq->cmds, srq_size, false);\nout_destroy_srq:\n\tib_destroy_srq(srq);\nout_free:\n\tkfree(nsrq);\n\treturn ERR_PTR(ret);\n}\n\nstatic int nvmet_rdma_init_srqs(struct nvmet_rdma_device *ndev)\n{\n\tint i, ret;\n\n\tif (!ndev->device->attrs.max_srq_wr || !ndev->device->attrs.max_srq) {\n\t\t \n\t\tpr_info(\"SRQ requested but not supported.\\n\");\n\t\treturn 0;\n\t}\n\n\tndev->srq_size = min(ndev->device->attrs.max_srq_wr,\n\t\t\t     nvmet_rdma_srq_size);\n\tndev->srq_count = min(ndev->device->num_comp_vectors,\n\t\t\t      ndev->device->attrs.max_srq);\n\n\tndev->srqs = kcalloc(ndev->srq_count, sizeof(*ndev->srqs), GFP_KERNEL);\n\tif (!ndev->srqs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < ndev->srq_count; i++) {\n\t\tndev->srqs[i] = nvmet_rdma_init_srq(ndev);\n\t\tif (IS_ERR(ndev->srqs[i])) {\n\t\t\tret = PTR_ERR(ndev->srqs[i]);\n\t\t\tgoto err_srq;\n\t\t}\n\t}\n\n\treturn 0;\n\nerr_srq:\n\twhile (--i >= 0)\n\t\tnvmet_rdma_destroy_srq(ndev->srqs[i]);\n\tkfree(ndev->srqs);\n\treturn ret;\n}\n\nstatic void nvmet_rdma_free_dev(struct kref *ref)\n{\n\tstruct nvmet_rdma_device *ndev =\n\t\tcontainer_of(ref, struct nvmet_rdma_device, ref);\n\n\tmutex_lock(&device_list_mutex);\n\tlist_del(&ndev->entry);\n\tmutex_unlock(&device_list_mutex);\n\n\tnvmet_rdma_destroy_srqs(ndev);\n\tib_dealloc_pd(ndev->pd);\n\n\tkfree(ndev);\n}\n\nstatic struct nvmet_rdma_device *\nnvmet_rdma_find_get_device(struct rdma_cm_id *cm_id)\n{\n\tstruct nvmet_rdma_port *port = cm_id->context;\n\tstruct nvmet_port *nport = port->nport;\n\tstruct nvmet_rdma_device *ndev;\n\tint inline_page_count;\n\tint inline_sge_count;\n\tint ret;\n\n\tmutex_lock(&device_list_mutex);\n\tlist_for_each_entry(ndev, &device_list, entry) {\n\t\tif (ndev->device->node_guid == cm_id->device->node_guid &&\n\t\t    kref_get_unless_zero(&ndev->ref))\n\t\t\tgoto out_unlock;\n\t}\n\n\tndev = kzalloc(sizeof(*ndev), GFP_KERNEL);\n\tif (!ndev)\n\t\tgoto out_err;\n\n\tinline_page_count = num_pages(nport->inline_data_size);\n\tinline_sge_count = max(cm_id->device->attrs.max_sge_rd,\n\t\t\t\tcm_id->device->attrs.max_recv_sge) - 1;\n\tif (inline_page_count > inline_sge_count) {\n\t\tpr_warn(\"inline_data_size %d cannot be supported by device %s. Reducing to %lu.\\n\",\n\t\t\tnport->inline_data_size, cm_id->device->name,\n\t\t\tinline_sge_count * PAGE_SIZE);\n\t\tnport->inline_data_size = inline_sge_count * PAGE_SIZE;\n\t\tinline_page_count = inline_sge_count;\n\t}\n\tndev->inline_data_size = nport->inline_data_size;\n\tndev->inline_page_count = inline_page_count;\n\n\tif (nport->pi_enable && !(cm_id->device->attrs.kernel_cap_flags &\n\t\t\t\t  IBK_INTEGRITY_HANDOVER)) {\n\t\tpr_warn(\"T10-PI is not supported by device %s. Disabling it\\n\",\n\t\t\tcm_id->device->name);\n\t\tnport->pi_enable = false;\n\t}\n\n\tndev->device = cm_id->device;\n\tkref_init(&ndev->ref);\n\n\tndev->pd = ib_alloc_pd(ndev->device, 0);\n\tif (IS_ERR(ndev->pd))\n\t\tgoto out_free_dev;\n\n\tif (nvmet_rdma_use_srq) {\n\t\tret = nvmet_rdma_init_srqs(ndev);\n\t\tif (ret)\n\t\t\tgoto out_free_pd;\n\t}\n\n\tlist_add(&ndev->entry, &device_list);\nout_unlock:\n\tmutex_unlock(&device_list_mutex);\n\tpr_debug(\"added %s.\\n\", ndev->device->name);\n\treturn ndev;\n\nout_free_pd:\n\tib_dealloc_pd(ndev->pd);\nout_free_dev:\n\tkfree(ndev);\nout_err:\n\tmutex_unlock(&device_list_mutex);\n\treturn NULL;\n}\n\nstatic int nvmet_rdma_create_queue_ib(struct nvmet_rdma_queue *queue)\n{\n\tstruct ib_qp_init_attr qp_attr = { };\n\tstruct nvmet_rdma_device *ndev = queue->dev;\n\tint nr_cqe, ret, i, factor;\n\n\t \n\tnr_cqe = queue->recv_queue_size + 2 * queue->send_queue_size;\n\n\tqueue->cq = ib_cq_pool_get(ndev->device, nr_cqe + 1,\n\t\t\t\t   queue->comp_vector, IB_POLL_WORKQUEUE);\n\tif (IS_ERR(queue->cq)) {\n\t\tret = PTR_ERR(queue->cq);\n\t\tpr_err(\"failed to create CQ cqe= %d ret= %d\\n\",\n\t\t       nr_cqe + 1, ret);\n\t\tgoto out;\n\t}\n\n\tqp_attr.qp_context = queue;\n\tqp_attr.event_handler = nvmet_rdma_qp_event;\n\tqp_attr.send_cq = queue->cq;\n\tqp_attr.recv_cq = queue->cq;\n\tqp_attr.sq_sig_type = IB_SIGNAL_REQ_WR;\n\tqp_attr.qp_type = IB_QPT_RC;\n\t \n\tqp_attr.cap.max_send_wr = queue->send_queue_size + 1;\n\tfactor = rdma_rw_mr_factor(ndev->device, queue->cm_id->port_num,\n\t\t\t\t   1 << NVMET_RDMA_MAX_MDTS);\n\tqp_attr.cap.max_rdma_ctxs = queue->send_queue_size * factor;\n\tqp_attr.cap.max_send_sge = max(ndev->device->attrs.max_sge_rd,\n\t\t\t\t\tndev->device->attrs.max_send_sge);\n\n\tif (queue->nsrq) {\n\t\tqp_attr.srq = queue->nsrq->srq;\n\t} else {\n\t\t \n\t\tqp_attr.cap.max_recv_wr = 1 + queue->recv_queue_size;\n\t\tqp_attr.cap.max_recv_sge = 1 + ndev->inline_page_count;\n\t}\n\n\tif (queue->port->pi_enable && queue->host_qid)\n\t\tqp_attr.create_flags |= IB_QP_CREATE_INTEGRITY_EN;\n\n\tret = rdma_create_qp(queue->cm_id, ndev->pd, &qp_attr);\n\tif (ret) {\n\t\tpr_err(\"failed to create_qp ret= %d\\n\", ret);\n\t\tgoto err_destroy_cq;\n\t}\n\tqueue->qp = queue->cm_id->qp;\n\n\tatomic_set(&queue->sq_wr_avail, qp_attr.cap.max_send_wr);\n\n\tpr_debug(\"%s: max_cqe= %d max_sge= %d sq_size = %d cm_id= %p\\n\",\n\t\t __func__, queue->cq->cqe, qp_attr.cap.max_send_sge,\n\t\t qp_attr.cap.max_send_wr, queue->cm_id);\n\n\tif (!queue->nsrq) {\n\t\tfor (i = 0; i < queue->recv_queue_size; i++) {\n\t\t\tqueue->cmds[i].queue = queue;\n\t\t\tret = nvmet_rdma_post_recv(ndev, &queue->cmds[i]);\n\t\t\tif (ret)\n\t\t\t\tgoto err_destroy_qp;\n\t\t}\n\t}\n\nout:\n\treturn ret;\n\nerr_destroy_qp:\n\trdma_destroy_qp(queue->cm_id);\nerr_destroy_cq:\n\tib_cq_pool_put(queue->cq, nr_cqe + 1);\n\tgoto out;\n}\n\nstatic void nvmet_rdma_destroy_queue_ib(struct nvmet_rdma_queue *queue)\n{\n\tib_drain_qp(queue->qp);\n\tif (queue->cm_id)\n\t\trdma_destroy_id(queue->cm_id);\n\tib_destroy_qp(queue->qp);\n\tib_cq_pool_put(queue->cq, queue->recv_queue_size + 2 *\n\t\t       queue->send_queue_size + 1);\n}\n\nstatic void nvmet_rdma_free_queue(struct nvmet_rdma_queue *queue)\n{\n\tpr_debug(\"freeing queue %d\\n\", queue->idx);\n\n\tnvmet_sq_destroy(&queue->nvme_sq);\n\n\tnvmet_rdma_destroy_queue_ib(queue);\n\tif (!queue->nsrq) {\n\t\tnvmet_rdma_free_cmds(queue->dev, queue->cmds,\n\t\t\t\tqueue->recv_queue_size,\n\t\t\t\t!queue->host_qid);\n\t}\n\tnvmet_rdma_free_rsps(queue);\n\tida_free(&nvmet_rdma_queue_ida, queue->idx);\n\tkfree(queue);\n}\n\nstatic void nvmet_rdma_release_queue_work(struct work_struct *w)\n{\n\tstruct nvmet_rdma_queue *queue =\n\t\tcontainer_of(w, struct nvmet_rdma_queue, release_work);\n\tstruct nvmet_rdma_device *dev = queue->dev;\n\n\tnvmet_rdma_free_queue(queue);\n\n\tkref_put(&dev->ref, nvmet_rdma_free_dev);\n}\n\nstatic int\nnvmet_rdma_parse_cm_connect_req(struct rdma_conn_param *conn,\n\t\t\t\tstruct nvmet_rdma_queue *queue)\n{\n\tstruct nvme_rdma_cm_req *req;\n\n\treq = (struct nvme_rdma_cm_req *)conn->private_data;\n\tif (!req || conn->private_data_len == 0)\n\t\treturn NVME_RDMA_CM_INVALID_LEN;\n\n\tif (le16_to_cpu(req->recfmt) != NVME_RDMA_CM_FMT_1_0)\n\t\treturn NVME_RDMA_CM_INVALID_RECFMT;\n\n\tqueue->host_qid = le16_to_cpu(req->qid);\n\n\t \n\tqueue->recv_queue_size = le16_to_cpu(req->hsqsize) + 1;\n\tqueue->send_queue_size = le16_to_cpu(req->hrqsize);\n\n\tif (!queue->host_qid && queue->recv_queue_size > NVME_AQ_DEPTH)\n\t\treturn NVME_RDMA_CM_INVALID_HSQSIZE;\n\n\t \n\n\treturn 0;\n}\n\nstatic int nvmet_rdma_cm_reject(struct rdma_cm_id *cm_id,\n\t\t\t\tenum nvme_rdma_cm_status status)\n{\n\tstruct nvme_rdma_cm_rej rej;\n\n\tpr_debug(\"rejecting connect request: status %d (%s)\\n\",\n\t\t status, nvme_rdma_cm_msg(status));\n\n\trej.recfmt = cpu_to_le16(NVME_RDMA_CM_FMT_1_0);\n\trej.sts = cpu_to_le16(status);\n\n\treturn rdma_reject(cm_id, (void *)&rej, sizeof(rej),\n\t\t\t   IB_CM_REJ_CONSUMER_DEFINED);\n}\n\nstatic struct nvmet_rdma_queue *\nnvmet_rdma_alloc_queue(struct nvmet_rdma_device *ndev,\n\t\tstruct rdma_cm_id *cm_id,\n\t\tstruct rdma_cm_event *event)\n{\n\tstruct nvmet_rdma_port *port = cm_id->context;\n\tstruct nvmet_rdma_queue *queue;\n\tint ret;\n\n\tqueue = kzalloc(sizeof(*queue), GFP_KERNEL);\n\tif (!queue) {\n\t\tret = NVME_RDMA_CM_NO_RSC;\n\t\tgoto out_reject;\n\t}\n\n\tret = nvmet_sq_init(&queue->nvme_sq);\n\tif (ret) {\n\t\tret = NVME_RDMA_CM_NO_RSC;\n\t\tgoto out_free_queue;\n\t}\n\n\tret = nvmet_rdma_parse_cm_connect_req(&event->param.conn, queue);\n\tif (ret)\n\t\tgoto out_destroy_sq;\n\n\t \n\tINIT_WORK(&queue->release_work, nvmet_rdma_release_queue_work);\n\tqueue->dev = ndev;\n\tqueue->cm_id = cm_id;\n\tqueue->port = port->nport;\n\n\tspin_lock_init(&queue->state_lock);\n\tqueue->state = NVMET_RDMA_Q_CONNECTING;\n\tINIT_LIST_HEAD(&queue->rsp_wait_list);\n\tINIT_LIST_HEAD(&queue->rsp_wr_wait_list);\n\tspin_lock_init(&queue->rsp_wr_wait_lock);\n\tINIT_LIST_HEAD(&queue->free_rsps);\n\tspin_lock_init(&queue->rsps_lock);\n\tINIT_LIST_HEAD(&queue->queue_list);\n\n\tqueue->idx = ida_alloc(&nvmet_rdma_queue_ida, GFP_KERNEL);\n\tif (queue->idx < 0) {\n\t\tret = NVME_RDMA_CM_NO_RSC;\n\t\tgoto out_destroy_sq;\n\t}\n\n\t \n\tqueue->comp_vector = !queue->host_qid ? 0 :\n\t\tqueue->idx % ndev->device->num_comp_vectors;\n\n\n\tret = nvmet_rdma_alloc_rsps(queue);\n\tif (ret) {\n\t\tret = NVME_RDMA_CM_NO_RSC;\n\t\tgoto out_ida_remove;\n\t}\n\n\tif (ndev->srqs) {\n\t\tqueue->nsrq = ndev->srqs[queue->comp_vector % ndev->srq_count];\n\t} else {\n\t\tqueue->cmds = nvmet_rdma_alloc_cmds(ndev,\n\t\t\t\tqueue->recv_queue_size,\n\t\t\t\t!queue->host_qid);\n\t\tif (IS_ERR(queue->cmds)) {\n\t\t\tret = NVME_RDMA_CM_NO_RSC;\n\t\t\tgoto out_free_responses;\n\t\t}\n\t}\n\n\tret = nvmet_rdma_create_queue_ib(queue);\n\tif (ret) {\n\t\tpr_err(\"%s: creating RDMA queue failed (%d).\\n\",\n\t\t\t__func__, ret);\n\t\tret = NVME_RDMA_CM_NO_RSC;\n\t\tgoto out_free_cmds;\n\t}\n\n\treturn queue;\n\nout_free_cmds:\n\tif (!queue->nsrq) {\n\t\tnvmet_rdma_free_cmds(queue->dev, queue->cmds,\n\t\t\t\tqueue->recv_queue_size,\n\t\t\t\t!queue->host_qid);\n\t}\nout_free_responses:\n\tnvmet_rdma_free_rsps(queue);\nout_ida_remove:\n\tida_free(&nvmet_rdma_queue_ida, queue->idx);\nout_destroy_sq:\n\tnvmet_sq_destroy(&queue->nvme_sq);\nout_free_queue:\n\tkfree(queue);\nout_reject:\n\tnvmet_rdma_cm_reject(cm_id, ret);\n\treturn NULL;\n}\n\nstatic void nvmet_rdma_qp_event(struct ib_event *event, void *priv)\n{\n\tstruct nvmet_rdma_queue *queue = priv;\n\n\tswitch (event->event) {\n\tcase IB_EVENT_COMM_EST:\n\t\trdma_notify(queue->cm_id, event->event);\n\t\tbreak;\n\tcase IB_EVENT_QP_LAST_WQE_REACHED:\n\t\tpr_debug(\"received last WQE reached event for queue=0x%p\\n\",\n\t\t\t queue);\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"received IB QP event: %s (%d)\\n\",\n\t\t       ib_event_msg(event->event), event->event);\n\t\tbreak;\n\t}\n}\n\nstatic int nvmet_rdma_cm_accept(struct rdma_cm_id *cm_id,\n\t\tstruct nvmet_rdma_queue *queue,\n\t\tstruct rdma_conn_param *p)\n{\n\tstruct rdma_conn_param  param = { };\n\tstruct nvme_rdma_cm_rep priv = { };\n\tint ret = -ENOMEM;\n\n\tparam.rnr_retry_count = 7;\n\tparam.flow_control = 1;\n\tparam.initiator_depth = min_t(u8, p->initiator_depth,\n\t\tqueue->dev->device->attrs.max_qp_init_rd_atom);\n\tparam.private_data = &priv;\n\tparam.private_data_len = sizeof(priv);\n\tpriv.recfmt = cpu_to_le16(NVME_RDMA_CM_FMT_1_0);\n\tpriv.crqsize = cpu_to_le16(queue->recv_queue_size);\n\n\tret = rdma_accept(cm_id, &param);\n\tif (ret)\n\t\tpr_err(\"rdma_accept failed (error code = %d)\\n\", ret);\n\n\treturn ret;\n}\n\nstatic int nvmet_rdma_queue_connect(struct rdma_cm_id *cm_id,\n\t\tstruct rdma_cm_event *event)\n{\n\tstruct nvmet_rdma_device *ndev;\n\tstruct nvmet_rdma_queue *queue;\n\tint ret = -EINVAL;\n\n\tndev = nvmet_rdma_find_get_device(cm_id);\n\tif (!ndev) {\n\t\tnvmet_rdma_cm_reject(cm_id, NVME_RDMA_CM_NO_RSC);\n\t\treturn -ECONNREFUSED;\n\t}\n\n\tqueue = nvmet_rdma_alloc_queue(ndev, cm_id, event);\n\tif (!queue) {\n\t\tret = -ENOMEM;\n\t\tgoto put_device;\n\t}\n\n\tif (queue->host_qid == 0) {\n\t\t \n\t\tflush_workqueue(nvmet_wq);\n\t}\n\n\tret = nvmet_rdma_cm_accept(cm_id, queue, &event->param.conn);\n\tif (ret) {\n\t\t \n\t\tqueue->cm_id = NULL;\n\t\tgoto free_queue;\n\t}\n\n\tmutex_lock(&nvmet_rdma_queue_mutex);\n\tlist_add_tail(&queue->queue_list, &nvmet_rdma_queue_list);\n\tmutex_unlock(&nvmet_rdma_queue_mutex);\n\n\treturn 0;\n\nfree_queue:\n\tnvmet_rdma_free_queue(queue);\nput_device:\n\tkref_put(&ndev->ref, nvmet_rdma_free_dev);\n\n\treturn ret;\n}\n\nstatic void nvmet_rdma_queue_established(struct nvmet_rdma_queue *queue)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&queue->state_lock, flags);\n\tif (queue->state != NVMET_RDMA_Q_CONNECTING) {\n\t\tpr_warn(\"trying to establish a connected queue\\n\");\n\t\tgoto out_unlock;\n\t}\n\tqueue->state = NVMET_RDMA_Q_LIVE;\n\n\twhile (!list_empty(&queue->rsp_wait_list)) {\n\t\tstruct nvmet_rdma_rsp *cmd;\n\n\t\tcmd = list_first_entry(&queue->rsp_wait_list,\n\t\t\t\t\tstruct nvmet_rdma_rsp, wait_list);\n\t\tlist_del(&cmd->wait_list);\n\n\t\tspin_unlock_irqrestore(&queue->state_lock, flags);\n\t\tnvmet_rdma_handle_command(queue, cmd);\n\t\tspin_lock_irqsave(&queue->state_lock, flags);\n\t}\n\nout_unlock:\n\tspin_unlock_irqrestore(&queue->state_lock, flags);\n}\n\nstatic void __nvmet_rdma_queue_disconnect(struct nvmet_rdma_queue *queue)\n{\n\tbool disconnect = false;\n\tunsigned long flags;\n\n\tpr_debug(\"cm_id= %p queue->state= %d\\n\", queue->cm_id, queue->state);\n\n\tspin_lock_irqsave(&queue->state_lock, flags);\n\tswitch (queue->state) {\n\tcase NVMET_RDMA_Q_CONNECTING:\n\t\twhile (!list_empty(&queue->rsp_wait_list)) {\n\t\t\tstruct nvmet_rdma_rsp *rsp;\n\n\t\t\trsp = list_first_entry(&queue->rsp_wait_list,\n\t\t\t\t\t       struct nvmet_rdma_rsp,\n\t\t\t\t\t       wait_list);\n\t\t\tlist_del(&rsp->wait_list);\n\t\t\tnvmet_rdma_put_rsp(rsp);\n\t\t}\n\t\tfallthrough;\n\tcase NVMET_RDMA_Q_LIVE:\n\t\tqueue->state = NVMET_RDMA_Q_DISCONNECTING;\n\t\tdisconnect = true;\n\t\tbreak;\n\tcase NVMET_RDMA_Q_DISCONNECTING:\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&queue->state_lock, flags);\n\n\tif (disconnect) {\n\t\trdma_disconnect(queue->cm_id);\n\t\tqueue_work(nvmet_wq, &queue->release_work);\n\t}\n}\n\nstatic void nvmet_rdma_queue_disconnect(struct nvmet_rdma_queue *queue)\n{\n\tbool disconnect = false;\n\n\tmutex_lock(&nvmet_rdma_queue_mutex);\n\tif (!list_empty(&queue->queue_list)) {\n\t\tlist_del_init(&queue->queue_list);\n\t\tdisconnect = true;\n\t}\n\tmutex_unlock(&nvmet_rdma_queue_mutex);\n\n\tif (disconnect)\n\t\t__nvmet_rdma_queue_disconnect(queue);\n}\n\nstatic void nvmet_rdma_queue_connect_fail(struct rdma_cm_id *cm_id,\n\t\tstruct nvmet_rdma_queue *queue)\n{\n\tWARN_ON_ONCE(queue->state != NVMET_RDMA_Q_CONNECTING);\n\n\tmutex_lock(&nvmet_rdma_queue_mutex);\n\tif (!list_empty(&queue->queue_list))\n\t\tlist_del_init(&queue->queue_list);\n\tmutex_unlock(&nvmet_rdma_queue_mutex);\n\n\tpr_err(\"failed to connect queue %d\\n\", queue->idx);\n\tqueue_work(nvmet_wq, &queue->release_work);\n}\n\n \nstatic int nvmet_rdma_device_removal(struct rdma_cm_id *cm_id,\n\t\tstruct nvmet_rdma_queue *queue)\n{\n\tstruct nvmet_rdma_port *port;\n\n\tif (queue) {\n\t\t \n\t\treturn 0;\n\t}\n\n\tport = cm_id->context;\n\n\t \n\tif (xchg(&port->cm_id, NULL) != cm_id)\n\t\treturn 0;\n\n\t \n\treturn 1;\n}\n\nstatic int nvmet_rdma_cm_handler(struct rdma_cm_id *cm_id,\n\t\tstruct rdma_cm_event *event)\n{\n\tstruct nvmet_rdma_queue *queue = NULL;\n\tint ret = 0;\n\n\tif (cm_id->qp)\n\t\tqueue = cm_id->qp->qp_context;\n\n\tpr_debug(\"%s (%d): status %d id %p\\n\",\n\t\trdma_event_msg(event->event), event->event,\n\t\tevent->status, cm_id);\n\n\tswitch (event->event) {\n\tcase RDMA_CM_EVENT_CONNECT_REQUEST:\n\t\tret = nvmet_rdma_queue_connect(cm_id, event);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_ESTABLISHED:\n\t\tnvmet_rdma_queue_established(queue);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_ADDR_CHANGE:\n\t\tif (!queue) {\n\t\t\tstruct nvmet_rdma_port *port = cm_id->context;\n\n\t\t\tqueue_delayed_work(nvmet_wq, &port->repair_work, 0);\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tcase RDMA_CM_EVENT_DISCONNECTED:\n\tcase RDMA_CM_EVENT_TIMEWAIT_EXIT:\n\t\tnvmet_rdma_queue_disconnect(queue);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_DEVICE_REMOVAL:\n\t\tret = nvmet_rdma_device_removal(cm_id, queue);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_REJECTED:\n\t\tpr_debug(\"Connection rejected: %s\\n\",\n\t\t\t rdma_reject_msg(cm_id, event->status));\n\t\tfallthrough;\n\tcase RDMA_CM_EVENT_UNREACHABLE:\n\tcase RDMA_CM_EVENT_CONNECT_ERROR:\n\t\tnvmet_rdma_queue_connect_fail(cm_id, queue);\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"received unrecognized RDMA CM event %d\\n\",\n\t\t\tevent->event);\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void nvmet_rdma_delete_ctrl(struct nvmet_ctrl *ctrl)\n{\n\tstruct nvmet_rdma_queue *queue;\n\nrestart:\n\tmutex_lock(&nvmet_rdma_queue_mutex);\n\tlist_for_each_entry(queue, &nvmet_rdma_queue_list, queue_list) {\n\t\tif (queue->nvme_sq.ctrl == ctrl) {\n\t\t\tlist_del_init(&queue->queue_list);\n\t\t\tmutex_unlock(&nvmet_rdma_queue_mutex);\n\n\t\t\t__nvmet_rdma_queue_disconnect(queue);\n\t\t\tgoto restart;\n\t\t}\n\t}\n\tmutex_unlock(&nvmet_rdma_queue_mutex);\n}\n\nstatic void nvmet_rdma_destroy_port_queues(struct nvmet_rdma_port *port)\n{\n\tstruct nvmet_rdma_queue *queue, *tmp;\n\tstruct nvmet_port *nport = port->nport;\n\n\tmutex_lock(&nvmet_rdma_queue_mutex);\n\tlist_for_each_entry_safe(queue, tmp, &nvmet_rdma_queue_list,\n\t\t\t\t queue_list) {\n\t\tif (queue->port != nport)\n\t\t\tcontinue;\n\n\t\tlist_del_init(&queue->queue_list);\n\t\t__nvmet_rdma_queue_disconnect(queue);\n\t}\n\tmutex_unlock(&nvmet_rdma_queue_mutex);\n}\n\nstatic void nvmet_rdma_disable_port(struct nvmet_rdma_port *port)\n{\n\tstruct rdma_cm_id *cm_id = xchg(&port->cm_id, NULL);\n\n\tif (cm_id)\n\t\trdma_destroy_id(cm_id);\n\n\t \n\tnvmet_rdma_destroy_port_queues(port);\n}\n\nstatic int nvmet_rdma_enable_port(struct nvmet_rdma_port *port)\n{\n\tstruct sockaddr *addr = (struct sockaddr *)&port->addr;\n\tstruct rdma_cm_id *cm_id;\n\tint ret;\n\n\tcm_id = rdma_create_id(&init_net, nvmet_rdma_cm_handler, port,\n\t\t\tRDMA_PS_TCP, IB_QPT_RC);\n\tif (IS_ERR(cm_id)) {\n\t\tpr_err(\"CM ID creation failed\\n\");\n\t\treturn PTR_ERR(cm_id);\n\t}\n\n\t \n\tret = rdma_set_afonly(cm_id, 1);\n\tif (ret) {\n\t\tpr_err(\"rdma_set_afonly failed (%d)\\n\", ret);\n\t\tgoto out_destroy_id;\n\t}\n\n\tret = rdma_bind_addr(cm_id, addr);\n\tif (ret) {\n\t\tpr_err(\"binding CM ID to %pISpcs failed (%d)\\n\", addr, ret);\n\t\tgoto out_destroy_id;\n\t}\n\n\tret = rdma_listen(cm_id, 128);\n\tif (ret) {\n\t\tpr_err(\"listening to %pISpcs failed (%d)\\n\", addr, ret);\n\t\tgoto out_destroy_id;\n\t}\n\n\tport->cm_id = cm_id;\n\treturn 0;\n\nout_destroy_id:\n\trdma_destroy_id(cm_id);\n\treturn ret;\n}\n\nstatic void nvmet_rdma_repair_port_work(struct work_struct *w)\n{\n\tstruct nvmet_rdma_port *port = container_of(to_delayed_work(w),\n\t\t\tstruct nvmet_rdma_port, repair_work);\n\tint ret;\n\n\tnvmet_rdma_disable_port(port);\n\tret = nvmet_rdma_enable_port(port);\n\tif (ret)\n\t\tqueue_delayed_work(nvmet_wq, &port->repair_work, 5 * HZ);\n}\n\nstatic int nvmet_rdma_add_port(struct nvmet_port *nport)\n{\n\tstruct nvmet_rdma_port *port;\n\t__kernel_sa_family_t af;\n\tint ret;\n\n\tport = kzalloc(sizeof(*port), GFP_KERNEL);\n\tif (!port)\n\t\treturn -ENOMEM;\n\n\tnport->priv = port;\n\tport->nport = nport;\n\tINIT_DELAYED_WORK(&port->repair_work, nvmet_rdma_repair_port_work);\n\n\tswitch (nport->disc_addr.adrfam) {\n\tcase NVMF_ADDR_FAMILY_IP4:\n\t\taf = AF_INET;\n\t\tbreak;\n\tcase NVMF_ADDR_FAMILY_IP6:\n\t\taf = AF_INET6;\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"address family %d not supported\\n\",\n\t\t\tnport->disc_addr.adrfam);\n\t\tret = -EINVAL;\n\t\tgoto out_free_port;\n\t}\n\n\tif (nport->inline_data_size < 0) {\n\t\tnport->inline_data_size = NVMET_RDMA_DEFAULT_INLINE_DATA_SIZE;\n\t} else if (nport->inline_data_size > NVMET_RDMA_MAX_INLINE_DATA_SIZE) {\n\t\tpr_warn(\"inline_data_size %u is too large, reducing to %u\\n\",\n\t\t\tnport->inline_data_size,\n\t\t\tNVMET_RDMA_MAX_INLINE_DATA_SIZE);\n\t\tnport->inline_data_size = NVMET_RDMA_MAX_INLINE_DATA_SIZE;\n\t}\n\n\tret = inet_pton_with_scope(&init_net, af, nport->disc_addr.traddr,\n\t\t\tnport->disc_addr.trsvcid, &port->addr);\n\tif (ret) {\n\t\tpr_err(\"malformed ip/port passed: %s:%s\\n\",\n\t\t\tnport->disc_addr.traddr, nport->disc_addr.trsvcid);\n\t\tgoto out_free_port;\n\t}\n\n\tret = nvmet_rdma_enable_port(port);\n\tif (ret)\n\t\tgoto out_free_port;\n\n\tpr_info(\"enabling port %d (%pISpcs)\\n\",\n\t\tle16_to_cpu(nport->disc_addr.portid),\n\t\t(struct sockaddr *)&port->addr);\n\n\treturn 0;\n\nout_free_port:\n\tkfree(port);\n\treturn ret;\n}\n\nstatic void nvmet_rdma_remove_port(struct nvmet_port *nport)\n{\n\tstruct nvmet_rdma_port *port = nport->priv;\n\n\tcancel_delayed_work_sync(&port->repair_work);\n\tnvmet_rdma_disable_port(port);\n\tkfree(port);\n}\n\nstatic void nvmet_rdma_disc_port_addr(struct nvmet_req *req,\n\t\tstruct nvmet_port *nport, char *traddr)\n{\n\tstruct nvmet_rdma_port *port = nport->priv;\n\tstruct rdma_cm_id *cm_id = port->cm_id;\n\n\tif (inet_addr_is_any((struct sockaddr *)&cm_id->route.addr.src_addr)) {\n\t\tstruct nvmet_rdma_rsp *rsp =\n\t\t\tcontainer_of(req, struct nvmet_rdma_rsp, req);\n\t\tstruct rdma_cm_id *req_cm_id = rsp->queue->cm_id;\n\t\tstruct sockaddr *addr = (void *)&req_cm_id->route.addr.src_addr;\n\n\t\tsprintf(traddr, \"%pISc\", addr);\n\t} else {\n\t\tmemcpy(traddr, nport->disc_addr.traddr, NVMF_TRADDR_SIZE);\n\t}\n}\n\nstatic u8 nvmet_rdma_get_mdts(const struct nvmet_ctrl *ctrl)\n{\n\tif (ctrl->pi_support)\n\t\treturn NVMET_RDMA_MAX_METADATA_MDTS;\n\treturn NVMET_RDMA_MAX_MDTS;\n}\n\nstatic u16 nvmet_rdma_get_max_queue_size(const struct nvmet_ctrl *ctrl)\n{\n\treturn NVME_RDMA_MAX_QUEUE_SIZE;\n}\n\nstatic const struct nvmet_fabrics_ops nvmet_rdma_ops = {\n\t.owner\t\t\t= THIS_MODULE,\n\t.type\t\t\t= NVMF_TRTYPE_RDMA,\n\t.msdbd\t\t\t= 1,\n\t.flags\t\t\t= NVMF_KEYED_SGLS | NVMF_METADATA_SUPPORTED,\n\t.add_port\t\t= nvmet_rdma_add_port,\n\t.remove_port\t\t= nvmet_rdma_remove_port,\n\t.queue_response\t\t= nvmet_rdma_queue_response,\n\t.delete_ctrl\t\t= nvmet_rdma_delete_ctrl,\n\t.disc_traddr\t\t= nvmet_rdma_disc_port_addr,\n\t.get_mdts\t\t= nvmet_rdma_get_mdts,\n\t.get_max_queue_size\t= nvmet_rdma_get_max_queue_size,\n};\n\nstatic void nvmet_rdma_remove_one(struct ib_device *ib_device, void *client_data)\n{\n\tstruct nvmet_rdma_queue *queue, *tmp;\n\tstruct nvmet_rdma_device *ndev;\n\tbool found = false;\n\n\tmutex_lock(&device_list_mutex);\n\tlist_for_each_entry(ndev, &device_list, entry) {\n\t\tif (ndev->device == ib_device) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&device_list_mutex);\n\n\tif (!found)\n\t\treturn;\n\n\t \n\tmutex_lock(&nvmet_rdma_queue_mutex);\n\tlist_for_each_entry_safe(queue, tmp, &nvmet_rdma_queue_list,\n\t\t\t\t queue_list) {\n\t\tif (queue->dev->device != ib_device)\n\t\t\tcontinue;\n\n\t\tpr_info(\"Removing queue %d\\n\", queue->idx);\n\t\tlist_del_init(&queue->queue_list);\n\t\t__nvmet_rdma_queue_disconnect(queue);\n\t}\n\tmutex_unlock(&nvmet_rdma_queue_mutex);\n\n\tflush_workqueue(nvmet_wq);\n}\n\nstatic struct ib_client nvmet_rdma_ib_client = {\n\t.name   = \"nvmet_rdma\",\n\t.remove = nvmet_rdma_remove_one\n};\n\nstatic int __init nvmet_rdma_init(void)\n{\n\tint ret;\n\n\tret = ib_register_client(&nvmet_rdma_ib_client);\n\tif (ret)\n\t\treturn ret;\n\n\tret = nvmet_register_transport(&nvmet_rdma_ops);\n\tif (ret)\n\t\tgoto err_ib_client;\n\n\treturn 0;\n\nerr_ib_client:\n\tib_unregister_client(&nvmet_rdma_ib_client);\n\treturn ret;\n}\n\nstatic void __exit nvmet_rdma_exit(void)\n{\n\tnvmet_unregister_transport(&nvmet_rdma_ops);\n\tib_unregister_client(&nvmet_rdma_ib_client);\n\tWARN_ON_ONCE(!list_empty(&nvmet_rdma_queue_list));\n\tida_destroy(&nvmet_rdma_queue_ida);\n}\n\nmodule_init(nvmet_rdma_init);\nmodule_exit(nvmet_rdma_exit);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_ALIAS(\"nvmet-transport-1\");  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}