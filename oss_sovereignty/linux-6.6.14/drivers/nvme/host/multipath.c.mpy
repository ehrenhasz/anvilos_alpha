{
  "module_name": "multipath.c",
  "hash_id": "c2624bfbe1cdaa669b1e44e069455dd7cdf56924daf3e1e64a40f471eba00427",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvme/host/multipath.c",
  "human_readable_source": "\n \n\n#include <linux/backing-dev.h>\n#include <linux/moduleparam.h>\n#include <linux/vmalloc.h>\n#include <trace/events/block.h>\n#include \"nvme.h\"\n\nbool multipath = true;\nmodule_param(multipath, bool, 0444);\nMODULE_PARM_DESC(multipath,\n\t\"turn on native support for multiple controllers per subsystem\");\n\nstatic const char *nvme_iopolicy_names[] = {\n\t[NVME_IOPOLICY_NUMA]\t= \"numa\",\n\t[NVME_IOPOLICY_RR]\t= \"round-robin\",\n};\n\nstatic int iopolicy = NVME_IOPOLICY_NUMA;\n\nstatic int nvme_set_iopolicy(const char *val, const struct kernel_param *kp)\n{\n\tif (!val)\n\t\treturn -EINVAL;\n\tif (!strncmp(val, \"numa\", 4))\n\t\tiopolicy = NVME_IOPOLICY_NUMA;\n\telse if (!strncmp(val, \"round-robin\", 11))\n\t\tiopolicy = NVME_IOPOLICY_RR;\n\telse\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nvme_get_iopolicy(char *buf, const struct kernel_param *kp)\n{\n\treturn sprintf(buf, \"%s\\n\", nvme_iopolicy_names[iopolicy]);\n}\n\nmodule_param_call(iopolicy, nvme_set_iopolicy, nvme_get_iopolicy,\n\t&iopolicy, 0644);\nMODULE_PARM_DESC(iopolicy,\n\t\"Default multipath I/O policy; 'numa' (default) or 'round-robin'\");\n\nvoid nvme_mpath_default_iopolicy(struct nvme_subsystem *subsys)\n{\n\tsubsys->iopolicy = iopolicy;\n}\n\nvoid nvme_mpath_unfreeze(struct nvme_subsystem *subsys)\n{\n\tstruct nvme_ns_head *h;\n\n\tlockdep_assert_held(&subsys->lock);\n\tlist_for_each_entry(h, &subsys->nsheads, entry)\n\t\tif (h->disk)\n\t\t\tblk_mq_unfreeze_queue(h->disk->queue);\n}\n\nvoid nvme_mpath_wait_freeze(struct nvme_subsystem *subsys)\n{\n\tstruct nvme_ns_head *h;\n\n\tlockdep_assert_held(&subsys->lock);\n\tlist_for_each_entry(h, &subsys->nsheads, entry)\n\t\tif (h->disk)\n\t\t\tblk_mq_freeze_queue_wait(h->disk->queue);\n}\n\nvoid nvme_mpath_start_freeze(struct nvme_subsystem *subsys)\n{\n\tstruct nvme_ns_head *h;\n\n\tlockdep_assert_held(&subsys->lock);\n\tlist_for_each_entry(h, &subsys->nsheads, entry)\n\t\tif (h->disk)\n\t\t\tblk_freeze_queue_start(h->disk->queue);\n}\n\nvoid nvme_failover_req(struct request *req)\n{\n\tstruct nvme_ns *ns = req->q->queuedata;\n\tu16 status = nvme_req(req)->status & 0x7ff;\n\tunsigned long flags;\n\tstruct bio *bio;\n\n\tnvme_mpath_clear_current_path(ns);\n\n\t \n\tif (nvme_is_ana_error(status) && ns->ctrl->ana_log_buf) {\n\t\tset_bit(NVME_NS_ANA_PENDING, &ns->flags);\n\t\tqueue_work(nvme_wq, &ns->ctrl->ana_work);\n\t}\n\n\tspin_lock_irqsave(&ns->head->requeue_lock, flags);\n\tfor (bio = req->bio; bio; bio = bio->bi_next) {\n\t\tbio_set_dev(bio, ns->head->disk->part0);\n\t\tif (bio->bi_opf & REQ_POLLED) {\n\t\t\tbio->bi_opf &= ~REQ_POLLED;\n\t\t\tbio->bi_cookie = BLK_QC_T_NONE;\n\t\t}\n\t\t \n\t\tbio->bi_opf &= ~REQ_NOWAIT;\n\t}\n\tblk_steal_bios(&ns->head->requeue_list, req);\n\tspin_unlock_irqrestore(&ns->head->requeue_lock, flags);\n\n\tblk_mq_end_request(req, 0);\n\tkblockd_schedule_work(&ns->head->requeue_work);\n}\n\nvoid nvme_mpath_start_request(struct request *rq)\n{\n\tstruct nvme_ns *ns = rq->q->queuedata;\n\tstruct gendisk *disk = ns->head->disk;\n\n\tif (!blk_queue_io_stat(disk->queue) || blk_rq_is_passthrough(rq))\n\t\treturn;\n\n\tnvme_req(rq)->flags |= NVME_MPATH_IO_STATS;\n\tnvme_req(rq)->start_time = bdev_start_io_acct(disk->part0, req_op(rq),\n\t\t\t\t\t\t      jiffies);\n}\nEXPORT_SYMBOL_GPL(nvme_mpath_start_request);\n\nvoid nvme_mpath_end_request(struct request *rq)\n{\n\tstruct nvme_ns *ns = rq->q->queuedata;\n\n\tif (!(nvme_req(rq)->flags & NVME_MPATH_IO_STATS))\n\t\treturn;\n\tbdev_end_io_acct(ns->head->disk->part0, req_op(rq),\n\t\t\t blk_rq_bytes(rq) >> SECTOR_SHIFT,\n\t\t\t nvme_req(rq)->start_time);\n}\n\nvoid nvme_kick_requeue_lists(struct nvme_ctrl *ctrl)\n{\n\tstruct nvme_ns *ns;\n\n\tdown_read(&ctrl->namespaces_rwsem);\n\tlist_for_each_entry(ns, &ctrl->namespaces, list) {\n\t\tif (!ns->head->disk)\n\t\t\tcontinue;\n\t\tkblockd_schedule_work(&ns->head->requeue_work);\n\t\tif (ctrl->state == NVME_CTRL_LIVE)\n\t\t\tdisk_uevent(ns->head->disk, KOBJ_CHANGE);\n\t}\n\tup_read(&ctrl->namespaces_rwsem);\n}\n\nstatic const char *nvme_ana_state_names[] = {\n\t[0]\t\t\t\t= \"invalid state\",\n\t[NVME_ANA_OPTIMIZED]\t\t= \"optimized\",\n\t[NVME_ANA_NONOPTIMIZED]\t\t= \"non-optimized\",\n\t[NVME_ANA_INACCESSIBLE]\t\t= \"inaccessible\",\n\t[NVME_ANA_PERSISTENT_LOSS]\t= \"persistent-loss\",\n\t[NVME_ANA_CHANGE]\t\t= \"change\",\n};\n\nbool nvme_mpath_clear_current_path(struct nvme_ns *ns)\n{\n\tstruct nvme_ns_head *head = ns->head;\n\tbool changed = false;\n\tint node;\n\n\tif (!head)\n\t\tgoto out;\n\n\tfor_each_node(node) {\n\t\tif (ns == rcu_access_pointer(head->current_path[node])) {\n\t\t\trcu_assign_pointer(head->current_path[node], NULL);\n\t\t\tchanged = true;\n\t\t}\n\t}\nout:\n\treturn changed;\n}\n\nvoid nvme_mpath_clear_ctrl_paths(struct nvme_ctrl *ctrl)\n{\n\tstruct nvme_ns *ns;\n\n\tdown_read(&ctrl->namespaces_rwsem);\n\tlist_for_each_entry(ns, &ctrl->namespaces, list) {\n\t\tnvme_mpath_clear_current_path(ns);\n\t\tkblockd_schedule_work(&ns->head->requeue_work);\n\t}\n\tup_read(&ctrl->namespaces_rwsem);\n}\n\nvoid nvme_mpath_revalidate_paths(struct nvme_ns *ns)\n{\n\tstruct nvme_ns_head *head = ns->head;\n\tsector_t capacity = get_capacity(head->disk);\n\tint node;\n\tint srcu_idx;\n\n\tsrcu_idx = srcu_read_lock(&head->srcu);\n\tlist_for_each_entry_rcu(ns, &head->list, siblings) {\n\t\tif (capacity != get_capacity(ns->disk))\n\t\t\tclear_bit(NVME_NS_READY, &ns->flags);\n\t}\n\tsrcu_read_unlock(&head->srcu, srcu_idx);\n\n\tfor_each_node(node)\n\t\trcu_assign_pointer(head->current_path[node], NULL);\n\tkblockd_schedule_work(&head->requeue_work);\n}\n\nstatic bool nvme_path_is_disabled(struct nvme_ns *ns)\n{\n\t \n\tif (ns->ctrl->state != NVME_CTRL_LIVE &&\n\t    ns->ctrl->state != NVME_CTRL_DELETING)\n\t\treturn true;\n\tif (test_bit(NVME_NS_ANA_PENDING, &ns->flags) ||\n\t    !test_bit(NVME_NS_READY, &ns->flags))\n\t\treturn true;\n\treturn false;\n}\n\nstatic struct nvme_ns *__nvme_find_path(struct nvme_ns_head *head, int node)\n{\n\tint found_distance = INT_MAX, fallback_distance = INT_MAX, distance;\n\tstruct nvme_ns *found = NULL, *fallback = NULL, *ns;\n\n\tlist_for_each_entry_rcu(ns, &head->list, siblings) {\n\t\tif (nvme_path_is_disabled(ns))\n\t\t\tcontinue;\n\n\t\tif (READ_ONCE(head->subsys->iopolicy) == NVME_IOPOLICY_NUMA)\n\t\t\tdistance = node_distance(node, ns->ctrl->numa_node);\n\t\telse\n\t\t\tdistance = LOCAL_DISTANCE;\n\n\t\tswitch (ns->ana_state) {\n\t\tcase NVME_ANA_OPTIMIZED:\n\t\t\tif (distance < found_distance) {\n\t\t\t\tfound_distance = distance;\n\t\t\t\tfound = ns;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NVME_ANA_NONOPTIMIZED:\n\t\t\tif (distance < fallback_distance) {\n\t\t\t\tfallback_distance = distance;\n\t\t\t\tfallback = ns;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (!found)\n\t\tfound = fallback;\n\tif (found)\n\t\trcu_assign_pointer(head->current_path[node], found);\n\treturn found;\n}\n\nstatic struct nvme_ns *nvme_next_ns(struct nvme_ns_head *head,\n\t\tstruct nvme_ns *ns)\n{\n\tns = list_next_or_null_rcu(&head->list, &ns->siblings, struct nvme_ns,\n\t\t\tsiblings);\n\tif (ns)\n\t\treturn ns;\n\treturn list_first_or_null_rcu(&head->list, struct nvme_ns, siblings);\n}\n\nstatic struct nvme_ns *nvme_round_robin_path(struct nvme_ns_head *head,\n\t\tint node, struct nvme_ns *old)\n{\n\tstruct nvme_ns *ns, *found = NULL;\n\n\tif (list_is_singular(&head->list)) {\n\t\tif (nvme_path_is_disabled(old))\n\t\t\treturn NULL;\n\t\treturn old;\n\t}\n\n\tfor (ns = nvme_next_ns(head, old);\n\t     ns && ns != old;\n\t     ns = nvme_next_ns(head, ns)) {\n\t\tif (nvme_path_is_disabled(ns))\n\t\t\tcontinue;\n\n\t\tif (ns->ana_state == NVME_ANA_OPTIMIZED) {\n\t\t\tfound = ns;\n\t\t\tgoto out;\n\t\t}\n\t\tif (ns->ana_state == NVME_ANA_NONOPTIMIZED)\n\t\t\tfound = ns;\n\t}\n\n\t \n\tif (!nvme_path_is_disabled(old) &&\n\t    (old->ana_state == NVME_ANA_OPTIMIZED ||\n\t     (!found && old->ana_state == NVME_ANA_NONOPTIMIZED)))\n\t\treturn old;\n\n\tif (!found)\n\t\treturn NULL;\nout:\n\trcu_assign_pointer(head->current_path[node], found);\n\treturn found;\n}\n\nstatic inline bool nvme_path_is_optimized(struct nvme_ns *ns)\n{\n\treturn ns->ctrl->state == NVME_CTRL_LIVE &&\n\t\tns->ana_state == NVME_ANA_OPTIMIZED;\n}\n\ninline struct nvme_ns *nvme_find_path(struct nvme_ns_head *head)\n{\n\tint node = numa_node_id();\n\tstruct nvme_ns *ns;\n\n\tns = srcu_dereference(head->current_path[node], &head->srcu);\n\tif (unlikely(!ns))\n\t\treturn __nvme_find_path(head, node);\n\n\tif (READ_ONCE(head->subsys->iopolicy) == NVME_IOPOLICY_RR)\n\t\treturn nvme_round_robin_path(head, node, ns);\n\tif (unlikely(!nvme_path_is_optimized(ns)))\n\t\treturn __nvme_find_path(head, node);\n\treturn ns;\n}\n\nstatic bool nvme_available_path(struct nvme_ns_head *head)\n{\n\tstruct nvme_ns *ns;\n\n\tlist_for_each_entry_rcu(ns, &head->list, siblings) {\n\t\tif (test_bit(NVME_CTRL_FAILFAST_EXPIRED, &ns->ctrl->flags))\n\t\t\tcontinue;\n\t\tswitch (ns->ctrl->state) {\n\t\tcase NVME_CTRL_LIVE:\n\t\tcase NVME_CTRL_RESETTING:\n\t\tcase NVME_CTRL_CONNECTING:\n\t\t\t \n\t\t\treturn true;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic void nvme_ns_head_submit_bio(struct bio *bio)\n{\n\tstruct nvme_ns_head *head = bio->bi_bdev->bd_disk->private_data;\n\tstruct device *dev = disk_to_dev(head->disk);\n\tstruct nvme_ns *ns;\n\tint srcu_idx;\n\n\t \n\tbio = bio_split_to_limits(bio);\n\tif (!bio)\n\t\treturn;\n\n\tsrcu_idx = srcu_read_lock(&head->srcu);\n\tns = nvme_find_path(head);\n\tif (likely(ns)) {\n\t\tbio_set_dev(bio, ns->disk->part0);\n\t\tbio->bi_opf |= REQ_NVME_MPATH;\n\t\ttrace_block_bio_remap(bio, disk_devt(ns->head->disk),\n\t\t\t\t      bio->bi_iter.bi_sector);\n\t\tsubmit_bio_noacct(bio);\n\t} else if (nvme_available_path(head)) {\n\t\tdev_warn_ratelimited(dev, \"no usable path - requeuing I/O\\n\");\n\n\t\tspin_lock_irq(&head->requeue_lock);\n\t\tbio_list_add(&head->requeue_list, bio);\n\t\tspin_unlock_irq(&head->requeue_lock);\n\t} else {\n\t\tdev_warn_ratelimited(dev, \"no available path - failing I/O\\n\");\n\n\t\tbio_io_error(bio);\n\t}\n\n\tsrcu_read_unlock(&head->srcu, srcu_idx);\n}\n\nstatic int nvme_ns_head_open(struct gendisk *disk, blk_mode_t mode)\n{\n\tif (!nvme_tryget_ns_head(disk->private_data))\n\t\treturn -ENXIO;\n\treturn 0;\n}\n\nstatic void nvme_ns_head_release(struct gendisk *disk)\n{\n\tnvme_put_ns_head(disk->private_data);\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic int nvme_ns_head_report_zones(struct gendisk *disk, sector_t sector,\n\t\tunsigned int nr_zones, report_zones_cb cb, void *data)\n{\n\tstruct nvme_ns_head *head = disk->private_data;\n\tstruct nvme_ns *ns;\n\tint srcu_idx, ret = -EWOULDBLOCK;\n\n\tsrcu_idx = srcu_read_lock(&head->srcu);\n\tns = nvme_find_path(head);\n\tif (ns)\n\t\tret = nvme_ns_report_zones(ns, sector, nr_zones, cb, data);\n\tsrcu_read_unlock(&head->srcu, srcu_idx);\n\treturn ret;\n}\n#else\n#define nvme_ns_head_report_zones\tNULL\n#endif  \n\nconst struct block_device_operations nvme_ns_head_ops = {\n\t.owner\t\t= THIS_MODULE,\n\t.submit_bio\t= nvme_ns_head_submit_bio,\n\t.open\t\t= nvme_ns_head_open,\n\t.release\t= nvme_ns_head_release,\n\t.ioctl\t\t= nvme_ns_head_ioctl,\n\t.compat_ioctl\t= blkdev_compat_ptr_ioctl,\n\t.getgeo\t\t= nvme_getgeo,\n\t.report_zones\t= nvme_ns_head_report_zones,\n\t.pr_ops\t\t= &nvme_pr_ops,\n};\n\nstatic inline struct nvme_ns_head *cdev_to_ns_head(struct cdev *cdev)\n{\n\treturn container_of(cdev, struct nvme_ns_head, cdev);\n}\n\nstatic int nvme_ns_head_chr_open(struct inode *inode, struct file *file)\n{\n\tif (!nvme_tryget_ns_head(cdev_to_ns_head(inode->i_cdev)))\n\t\treturn -ENXIO;\n\treturn 0;\n}\n\nstatic int nvme_ns_head_chr_release(struct inode *inode, struct file *file)\n{\n\tnvme_put_ns_head(cdev_to_ns_head(inode->i_cdev));\n\treturn 0;\n}\n\nstatic const struct file_operations nvme_ns_head_chr_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= nvme_ns_head_chr_open,\n\t.release\t= nvme_ns_head_chr_release,\n\t.unlocked_ioctl\t= nvme_ns_head_chr_ioctl,\n\t.compat_ioctl\t= compat_ptr_ioctl,\n\t.uring_cmd\t= nvme_ns_head_chr_uring_cmd,\n\t.uring_cmd_iopoll = nvme_ns_chr_uring_cmd_iopoll,\n};\n\nstatic int nvme_add_ns_head_cdev(struct nvme_ns_head *head)\n{\n\tint ret;\n\n\thead->cdev_device.parent = &head->subsys->dev;\n\tret = dev_set_name(&head->cdev_device, \"ng%dn%d\",\n\t\t\t   head->subsys->instance, head->instance);\n\tif (ret)\n\t\treturn ret;\n\tret = nvme_cdev_add(&head->cdev, &head->cdev_device,\n\t\t\t    &nvme_ns_head_chr_fops, THIS_MODULE);\n\treturn ret;\n}\n\nstatic void nvme_requeue_work(struct work_struct *work)\n{\n\tstruct nvme_ns_head *head =\n\t\tcontainer_of(work, struct nvme_ns_head, requeue_work);\n\tstruct bio *bio, *next;\n\n\tspin_lock_irq(&head->requeue_lock);\n\tnext = bio_list_get(&head->requeue_list);\n\tspin_unlock_irq(&head->requeue_lock);\n\n\twhile ((bio = next) != NULL) {\n\t\tnext = bio->bi_next;\n\t\tbio->bi_next = NULL;\n\n\t\tsubmit_bio_noacct(bio);\n\t}\n}\n\nint nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl, struct nvme_ns_head *head)\n{\n\tbool vwc = false;\n\n\tmutex_init(&head->lock);\n\tbio_list_init(&head->requeue_list);\n\tspin_lock_init(&head->requeue_lock);\n\tINIT_WORK(&head->requeue_work, nvme_requeue_work);\n\n\t \n\tif (!(ctrl->subsys->cmic & NVME_CTRL_CMIC_MULTI_CTRL) ||\n\t    !nvme_is_unique_nsid(ctrl, head) || !multipath)\n\t\treturn 0;\n\n\thead->disk = blk_alloc_disk(ctrl->numa_node);\n\tif (!head->disk)\n\t\treturn -ENOMEM;\n\thead->disk->fops = &nvme_ns_head_ops;\n\thead->disk->private_data = head;\n\tsprintf(head->disk->disk_name, \"nvme%dn%d\",\n\t\t\tctrl->subsys->instance, head->instance);\n\n\tblk_queue_flag_set(QUEUE_FLAG_NONROT, head->disk->queue);\n\tblk_queue_flag_set(QUEUE_FLAG_NOWAIT, head->disk->queue);\n\tblk_queue_flag_set(QUEUE_FLAG_IO_STAT, head->disk->queue);\n\t \n\tif (ctrl->tagset->nr_maps > HCTX_TYPE_POLL &&\n\t    ctrl->tagset->map[HCTX_TYPE_POLL].nr_queues)\n\t\tblk_queue_flag_set(QUEUE_FLAG_POLL, head->disk->queue);\n\n\t \n\tblk_queue_logical_block_size(head->disk->queue, 512);\n\tblk_set_stacking_limits(&head->disk->queue->limits);\n\tblk_queue_dma_alignment(head->disk->queue, 3);\n\n\t \n\tif (ctrl->vwc & NVME_CTRL_VWC_PRESENT)\n\t\tvwc = true;\n\tblk_queue_write_cache(head->disk->queue, vwc, vwc);\n\treturn 0;\n}\n\nstatic void nvme_mpath_set_live(struct nvme_ns *ns)\n{\n\tstruct nvme_ns_head *head = ns->head;\n\tint rc;\n\n\tif (!head->disk)\n\t\treturn;\n\n\t \n\tif (!test_and_set_bit(NVME_NSHEAD_DISK_LIVE, &head->flags)) {\n\t\trc = device_add_disk(&head->subsys->dev, head->disk,\n\t\t\t\t     nvme_ns_id_attr_groups);\n\t\tif (rc) {\n\t\t\tclear_bit(NVME_NSHEAD_DISK_LIVE, &ns->flags);\n\t\t\treturn;\n\t\t}\n\t\tnvme_add_ns_head_cdev(head);\n\t}\n\n\tmutex_lock(&head->lock);\n\tif (nvme_path_is_optimized(ns)) {\n\t\tint node, srcu_idx;\n\n\t\tsrcu_idx = srcu_read_lock(&head->srcu);\n\t\tfor_each_node(node)\n\t\t\t__nvme_find_path(head, node);\n\t\tsrcu_read_unlock(&head->srcu, srcu_idx);\n\t}\n\tmutex_unlock(&head->lock);\n\n\tsynchronize_srcu(&head->srcu);\n\tkblockd_schedule_work(&head->requeue_work);\n}\n\nstatic int nvme_parse_ana_log(struct nvme_ctrl *ctrl, void *data,\n\t\tint (*cb)(struct nvme_ctrl *ctrl, struct nvme_ana_group_desc *,\n\t\t\tvoid *))\n{\n\tvoid *base = ctrl->ana_log_buf;\n\tsize_t offset = sizeof(struct nvme_ana_rsp_hdr);\n\tint error, i;\n\n\tlockdep_assert_held(&ctrl->ana_lock);\n\n\tfor (i = 0; i < le16_to_cpu(ctrl->ana_log_buf->ngrps); i++) {\n\t\tstruct nvme_ana_group_desc *desc = base + offset;\n\t\tu32 nr_nsids;\n\t\tsize_t nsid_buf_size;\n\n\t\tif (WARN_ON_ONCE(offset > ctrl->ana_log_size - sizeof(*desc)))\n\t\t\treturn -EINVAL;\n\n\t\tnr_nsids = le32_to_cpu(desc->nnsids);\n\t\tnsid_buf_size = flex_array_size(desc, nsids, nr_nsids);\n\n\t\tif (WARN_ON_ONCE(desc->grpid == 0))\n\t\t\treturn -EINVAL;\n\t\tif (WARN_ON_ONCE(le32_to_cpu(desc->grpid) > ctrl->anagrpmax))\n\t\t\treturn -EINVAL;\n\t\tif (WARN_ON_ONCE(desc->state == 0))\n\t\t\treturn -EINVAL;\n\t\tif (WARN_ON_ONCE(desc->state > NVME_ANA_CHANGE))\n\t\t\treturn -EINVAL;\n\n\t\toffset += sizeof(*desc);\n\t\tif (WARN_ON_ONCE(offset > ctrl->ana_log_size - nsid_buf_size))\n\t\t\treturn -EINVAL;\n\n\t\terror = cb(ctrl, desc, data);\n\t\tif (error)\n\t\t\treturn error;\n\n\t\toffset += nsid_buf_size;\n\t}\n\n\treturn 0;\n}\n\nstatic inline bool nvme_state_is_live(enum nvme_ana_state state)\n{\n\treturn state == NVME_ANA_OPTIMIZED || state == NVME_ANA_NONOPTIMIZED;\n}\n\nstatic void nvme_update_ns_ana_state(struct nvme_ana_group_desc *desc,\n\t\tstruct nvme_ns *ns)\n{\n\tns->ana_grpid = le32_to_cpu(desc->grpid);\n\tns->ana_state = desc->state;\n\tclear_bit(NVME_NS_ANA_PENDING, &ns->flags);\n\t \n\tif (nvme_state_is_live(ns->ana_state) &&\n\t    ns->ctrl->state == NVME_CTRL_LIVE)\n\t\tnvme_mpath_set_live(ns);\n}\n\nstatic int nvme_update_ana_state(struct nvme_ctrl *ctrl,\n\t\tstruct nvme_ana_group_desc *desc, void *data)\n{\n\tu32 nr_nsids = le32_to_cpu(desc->nnsids), n = 0;\n\tunsigned *nr_change_groups = data;\n\tstruct nvme_ns *ns;\n\n\tdev_dbg(ctrl->device, \"ANA group %d: %s.\\n\",\n\t\t\tle32_to_cpu(desc->grpid),\n\t\t\tnvme_ana_state_names[desc->state]);\n\n\tif (desc->state == NVME_ANA_CHANGE)\n\t\t(*nr_change_groups)++;\n\n\tif (!nr_nsids)\n\t\treturn 0;\n\n\tdown_read(&ctrl->namespaces_rwsem);\n\tlist_for_each_entry(ns, &ctrl->namespaces, list) {\n\t\tunsigned nsid;\nagain:\n\t\tnsid = le32_to_cpu(desc->nsids[n]);\n\t\tif (ns->head->ns_id < nsid)\n\t\t\tcontinue;\n\t\tif (ns->head->ns_id == nsid)\n\t\t\tnvme_update_ns_ana_state(desc, ns);\n\t\tif (++n == nr_nsids)\n\t\t\tbreak;\n\t\tif (ns->head->ns_id > nsid)\n\t\t\tgoto again;\n\t}\n\tup_read(&ctrl->namespaces_rwsem);\n\treturn 0;\n}\n\nstatic int nvme_read_ana_log(struct nvme_ctrl *ctrl)\n{\n\tu32 nr_change_groups = 0;\n\tint error;\n\n\tmutex_lock(&ctrl->ana_lock);\n\terror = nvme_get_log(ctrl, NVME_NSID_ALL, NVME_LOG_ANA, 0, NVME_CSI_NVM,\n\t\t\tctrl->ana_log_buf, ctrl->ana_log_size, 0);\n\tif (error) {\n\t\tdev_warn(ctrl->device, \"Failed to get ANA log: %d\\n\", error);\n\t\tgoto out_unlock;\n\t}\n\n\terror = nvme_parse_ana_log(ctrl, &nr_change_groups,\n\t\t\tnvme_update_ana_state);\n\tif (error)\n\t\tgoto out_unlock;\n\n\t \n\tif (nr_change_groups)\n\t\tmod_timer(&ctrl->anatt_timer, ctrl->anatt * HZ * 2 + jiffies);\n\telse\n\t\tdel_timer_sync(&ctrl->anatt_timer);\nout_unlock:\n\tmutex_unlock(&ctrl->ana_lock);\n\treturn error;\n}\n\nstatic void nvme_ana_work(struct work_struct *work)\n{\n\tstruct nvme_ctrl *ctrl = container_of(work, struct nvme_ctrl, ana_work);\n\n\tif (ctrl->state != NVME_CTRL_LIVE)\n\t\treturn;\n\n\tnvme_read_ana_log(ctrl);\n}\n\nvoid nvme_mpath_update(struct nvme_ctrl *ctrl)\n{\n\tu32 nr_change_groups = 0;\n\n\tif (!ctrl->ana_log_buf)\n\t\treturn;\n\n\tmutex_lock(&ctrl->ana_lock);\n\tnvme_parse_ana_log(ctrl, &nr_change_groups, nvme_update_ana_state);\n\tmutex_unlock(&ctrl->ana_lock);\n}\n\nstatic void nvme_anatt_timeout(struct timer_list *t)\n{\n\tstruct nvme_ctrl *ctrl = from_timer(ctrl, t, anatt_timer);\n\n\tdev_info(ctrl->device, \"ANATT timeout, resetting controller.\\n\");\n\tnvme_reset_ctrl(ctrl);\n}\n\nvoid nvme_mpath_stop(struct nvme_ctrl *ctrl)\n{\n\tif (!nvme_ctrl_use_ana(ctrl))\n\t\treturn;\n\tdel_timer_sync(&ctrl->anatt_timer);\n\tcancel_work_sync(&ctrl->ana_work);\n}\n\n#define SUBSYS_ATTR_RW(_name, _mode, _show, _store)  \\\n\tstruct device_attribute subsys_attr_##_name =\t\\\n\t\t__ATTR(_name, _mode, _show, _store)\n\nstatic ssize_t nvme_subsys_iopolicy_show(struct device *dev,\n\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct nvme_subsystem *subsys =\n\t\tcontainer_of(dev, struct nvme_subsystem, dev);\n\n\treturn sysfs_emit(buf, \"%s\\n\",\n\t\t\t  nvme_iopolicy_names[READ_ONCE(subsys->iopolicy)]);\n}\n\nstatic ssize_t nvme_subsys_iopolicy_store(struct device *dev,\n\t\tstruct device_attribute *attr, const char *buf, size_t count)\n{\n\tstruct nvme_subsystem *subsys =\n\t\tcontainer_of(dev, struct nvme_subsystem, dev);\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(nvme_iopolicy_names); i++) {\n\t\tif (sysfs_streq(buf, nvme_iopolicy_names[i])) {\n\t\t\tWRITE_ONCE(subsys->iopolicy, i);\n\t\t\treturn count;\n\t\t}\n\t}\n\n\treturn -EINVAL;\n}\nSUBSYS_ATTR_RW(iopolicy, S_IRUGO | S_IWUSR,\n\t\t      nvme_subsys_iopolicy_show, nvme_subsys_iopolicy_store);\n\nstatic ssize_t ana_grpid_show(struct device *dev, struct device_attribute *attr,\n\t\tchar *buf)\n{\n\treturn sysfs_emit(buf, \"%d\\n\", nvme_get_ns_from_dev(dev)->ana_grpid);\n}\nDEVICE_ATTR_RO(ana_grpid);\n\nstatic ssize_t ana_state_show(struct device *dev, struct device_attribute *attr,\n\t\tchar *buf)\n{\n\tstruct nvme_ns *ns = nvme_get_ns_from_dev(dev);\n\n\treturn sysfs_emit(buf, \"%s\\n\", nvme_ana_state_names[ns->ana_state]);\n}\nDEVICE_ATTR_RO(ana_state);\n\nstatic int nvme_lookup_ana_group_desc(struct nvme_ctrl *ctrl,\n\t\tstruct nvme_ana_group_desc *desc, void *data)\n{\n\tstruct nvme_ana_group_desc *dst = data;\n\n\tif (desc->grpid != dst->grpid)\n\t\treturn 0;\n\n\t*dst = *desc;\n\treturn -ENXIO;  \n}\n\nvoid nvme_mpath_add_disk(struct nvme_ns *ns, __le32 anagrpid)\n{\n\tif (nvme_ctrl_use_ana(ns->ctrl)) {\n\t\tstruct nvme_ana_group_desc desc = {\n\t\t\t.grpid = anagrpid,\n\t\t\t.state = 0,\n\t\t};\n\n\t\tmutex_lock(&ns->ctrl->ana_lock);\n\t\tns->ana_grpid = le32_to_cpu(anagrpid);\n\t\tnvme_parse_ana_log(ns->ctrl, &desc, nvme_lookup_ana_group_desc);\n\t\tmutex_unlock(&ns->ctrl->ana_lock);\n\t\tif (desc.state) {\n\t\t\t \n\t\t\tnvme_update_ns_ana_state(&desc, ns);\n\t\t} else {\n\t\t\t \n\t\t\tset_bit(NVME_NS_ANA_PENDING, &ns->flags);\n\t\t\tqueue_work(nvme_wq, &ns->ctrl->ana_work);\n\t\t}\n\t} else {\n\t\tns->ana_state = NVME_ANA_OPTIMIZED;\n\t\tnvme_mpath_set_live(ns);\n\t}\n\n\tif (blk_queue_stable_writes(ns->queue) && ns->head->disk)\n\t\tblk_queue_flag_set(QUEUE_FLAG_STABLE_WRITES,\n\t\t\t\t   ns->head->disk->queue);\n#ifdef CONFIG_BLK_DEV_ZONED\n\tif (blk_queue_is_zoned(ns->queue) && ns->head->disk)\n\t\tns->head->disk->nr_zones = ns->disk->nr_zones;\n#endif\n}\n\nvoid nvme_mpath_shutdown_disk(struct nvme_ns_head *head)\n{\n\tif (!head->disk)\n\t\treturn;\n\tkblockd_schedule_work(&head->requeue_work);\n\tif (test_bit(NVME_NSHEAD_DISK_LIVE, &head->flags)) {\n\t\tnvme_cdev_del(&head->cdev, &head->cdev_device);\n\t\tdel_gendisk(head->disk);\n\t}\n}\n\nvoid nvme_mpath_remove_disk(struct nvme_ns_head *head)\n{\n\tif (!head->disk)\n\t\treturn;\n\t \n\tkblockd_schedule_work(&head->requeue_work);\n\tflush_work(&head->requeue_work);\n\tput_disk(head->disk);\n}\n\nvoid nvme_mpath_init_ctrl(struct nvme_ctrl *ctrl)\n{\n\tmutex_init(&ctrl->ana_lock);\n\ttimer_setup(&ctrl->anatt_timer, nvme_anatt_timeout, 0);\n\tINIT_WORK(&ctrl->ana_work, nvme_ana_work);\n}\n\nint nvme_mpath_init_identify(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id)\n{\n\tsize_t max_transfer_size = ctrl->max_hw_sectors << SECTOR_SHIFT;\n\tsize_t ana_log_size;\n\tint error = 0;\n\n\t \n\tif (!multipath || !ctrl->subsys ||\n\t    !(ctrl->subsys->cmic & NVME_CTRL_CMIC_ANA))\n\t\treturn 0;\n\n\tif (!ctrl->max_namespaces ||\n\t    ctrl->max_namespaces > le32_to_cpu(id->nn)) {\n\t\tdev_err(ctrl->device,\n\t\t\t\"Invalid MNAN value %u\\n\", ctrl->max_namespaces);\n\t\treturn -EINVAL;\n\t}\n\n\tctrl->anacap = id->anacap;\n\tctrl->anatt = id->anatt;\n\tctrl->nanagrpid = le32_to_cpu(id->nanagrpid);\n\tctrl->anagrpmax = le32_to_cpu(id->anagrpmax);\n\n\tana_log_size = sizeof(struct nvme_ana_rsp_hdr) +\n\t\tctrl->nanagrpid * sizeof(struct nvme_ana_group_desc) +\n\t\tctrl->max_namespaces * sizeof(__le32);\n\tif (ana_log_size > max_transfer_size) {\n\t\tdev_err(ctrl->device,\n\t\t\t\"ANA log page size (%zd) larger than MDTS (%zd).\\n\",\n\t\t\tana_log_size, max_transfer_size);\n\t\tdev_err(ctrl->device, \"disabling ANA support.\\n\");\n\t\tgoto out_uninit;\n\t}\n\tif (ana_log_size > ctrl->ana_log_size) {\n\t\tnvme_mpath_stop(ctrl);\n\t\tnvme_mpath_uninit(ctrl);\n\t\tctrl->ana_log_buf = kvmalloc(ana_log_size, GFP_KERNEL);\n\t\tif (!ctrl->ana_log_buf)\n\t\t\treturn -ENOMEM;\n\t}\n\tctrl->ana_log_size = ana_log_size;\n\terror = nvme_read_ana_log(ctrl);\n\tif (error)\n\t\tgoto out_uninit;\n\treturn 0;\n\nout_uninit:\n\tnvme_mpath_uninit(ctrl);\n\treturn error;\n}\n\nvoid nvme_mpath_uninit(struct nvme_ctrl *ctrl)\n{\n\tkvfree(ctrl->ana_log_buf);\n\tctrl->ana_log_buf = NULL;\n\tctrl->ana_log_size = 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}