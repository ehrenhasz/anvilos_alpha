{
  "module_name": "pci.c",
  "hash_id": "3af48b4dfab4d4b1710bebf30ac437cd1f6890b6d959969f163b3754be39ebf5",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvme/host/pci.c",
  "human_readable_source": "\n \n\n#include <linux/acpi.h>\n#include <linux/async.h>\n#include <linux/blkdev.h>\n#include <linux/blk-mq.h>\n#include <linux/blk-mq-pci.h>\n#include <linux/blk-integrity.h>\n#include <linux/dmi.h>\n#include <linux/init.h>\n#include <linux/interrupt.h>\n#include <linux/io.h>\n#include <linux/kstrtox.h>\n#include <linux/memremap.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/mutex.h>\n#include <linux/once.h>\n#include <linux/pci.h>\n#include <linux/suspend.h>\n#include <linux/t10-pi.h>\n#include <linux/types.h>\n#include <linux/io-64-nonatomic-lo-hi.h>\n#include <linux/io-64-nonatomic-hi-lo.h>\n#include <linux/sed-opal.h>\n#include <linux/pci-p2pdma.h>\n\n#include \"trace.h\"\n#include \"nvme.h\"\n\n#define SQ_SIZE(q)\t((q)->q_depth << (q)->sqes)\n#define CQ_SIZE(q)\t((q)->q_depth * sizeof(struct nvme_completion))\n\n#define SGES_PER_PAGE\t(NVME_CTRL_PAGE_SIZE / sizeof(struct nvme_sgl_desc))\n\n \n#define NVME_MAX_KB_SZ\t8192\n#define NVME_MAX_SEGS\t128\n#define NVME_MAX_NR_ALLOCATIONS\t5\n\nstatic int use_threaded_interrupts;\nmodule_param(use_threaded_interrupts, int, 0444);\n\nstatic bool use_cmb_sqes = true;\nmodule_param(use_cmb_sqes, bool, 0444);\nMODULE_PARM_DESC(use_cmb_sqes, \"use controller's memory buffer for I/O SQes\");\n\nstatic unsigned int max_host_mem_size_mb = 128;\nmodule_param(max_host_mem_size_mb, uint, 0444);\nMODULE_PARM_DESC(max_host_mem_size_mb,\n\t\"Maximum Host Memory Buffer (HMB) size per controller (in MiB)\");\n\nstatic unsigned int sgl_threshold = SZ_32K;\nmodule_param(sgl_threshold, uint, 0644);\nMODULE_PARM_DESC(sgl_threshold,\n\t\t\"Use SGLs when average request segment size is larger or equal to \"\n\t\t\"this size. Use 0 to disable SGLs.\");\n\n#define NVME_PCI_MIN_QUEUE_SIZE 2\n#define NVME_PCI_MAX_QUEUE_SIZE 4095\nstatic int io_queue_depth_set(const char *val, const struct kernel_param *kp);\nstatic const struct kernel_param_ops io_queue_depth_ops = {\n\t.set = io_queue_depth_set,\n\t.get = param_get_uint,\n};\n\nstatic unsigned int io_queue_depth = 1024;\nmodule_param_cb(io_queue_depth, &io_queue_depth_ops, &io_queue_depth, 0644);\nMODULE_PARM_DESC(io_queue_depth, \"set io queue depth, should >= 2 and < 4096\");\n\nstatic int io_queue_count_set(const char *val, const struct kernel_param *kp)\n{\n\tunsigned int n;\n\tint ret;\n\n\tret = kstrtouint(val, 10, &n);\n\tif (ret != 0 || n > num_possible_cpus())\n\t\treturn -EINVAL;\n\treturn param_set_uint(val, kp);\n}\n\nstatic const struct kernel_param_ops io_queue_count_ops = {\n\t.set = io_queue_count_set,\n\t.get = param_get_uint,\n};\n\nstatic unsigned int write_queues;\nmodule_param_cb(write_queues, &io_queue_count_ops, &write_queues, 0644);\nMODULE_PARM_DESC(write_queues,\n\t\"Number of queues to use for writes. If not set, reads and writes \"\n\t\"will share a queue set.\");\n\nstatic unsigned int poll_queues;\nmodule_param_cb(poll_queues, &io_queue_count_ops, &poll_queues, 0644);\nMODULE_PARM_DESC(poll_queues, \"Number of queues to use for polled IO.\");\n\nstatic bool noacpi;\nmodule_param(noacpi, bool, 0444);\nMODULE_PARM_DESC(noacpi, \"disable acpi bios quirks\");\n\nstruct nvme_dev;\nstruct nvme_queue;\n\nstatic void nvme_dev_disable(struct nvme_dev *dev, bool shutdown);\nstatic void nvme_delete_io_queues(struct nvme_dev *dev);\nstatic void nvme_update_attrs(struct nvme_dev *dev);\n\n \nstruct nvme_dev {\n\tstruct nvme_queue *queues;\n\tstruct blk_mq_tag_set tagset;\n\tstruct blk_mq_tag_set admin_tagset;\n\tu32 __iomem *dbs;\n\tstruct device *dev;\n\tstruct dma_pool *prp_page_pool;\n\tstruct dma_pool *prp_small_pool;\n\tunsigned online_queues;\n\tunsigned max_qid;\n\tunsigned io_queues[HCTX_MAX_TYPES];\n\tunsigned int num_vecs;\n\tu32 q_depth;\n\tint io_sqes;\n\tu32 db_stride;\n\tvoid __iomem *bar;\n\tunsigned long bar_mapped_size;\n\tstruct mutex shutdown_lock;\n\tbool subsystem;\n\tu64 cmb_size;\n\tbool cmb_use_sqes;\n\tu32 cmbsz;\n\tu32 cmbloc;\n\tstruct nvme_ctrl ctrl;\n\tu32 last_ps;\n\tbool hmb;\n\n\tmempool_t *iod_mempool;\n\n\t \n\t__le32 *dbbuf_dbs;\n\tdma_addr_t dbbuf_dbs_dma_addr;\n\t__le32 *dbbuf_eis;\n\tdma_addr_t dbbuf_eis_dma_addr;\n\n\t \n\tu64 host_mem_size;\n\tu32 nr_host_mem_descs;\n\tdma_addr_t host_mem_descs_dma;\n\tstruct nvme_host_mem_buf_desc *host_mem_descs;\n\tvoid **host_mem_desc_bufs;\n\tunsigned int nr_allocated_queues;\n\tunsigned int nr_write_queues;\n\tunsigned int nr_poll_queues;\n};\n\nstatic int io_queue_depth_set(const char *val, const struct kernel_param *kp)\n{\n\treturn param_set_uint_minmax(val, kp, NVME_PCI_MIN_QUEUE_SIZE,\n\t\t\tNVME_PCI_MAX_QUEUE_SIZE);\n}\n\nstatic inline unsigned int sq_idx(unsigned int qid, u32 stride)\n{\n\treturn qid * 2 * stride;\n}\n\nstatic inline unsigned int cq_idx(unsigned int qid, u32 stride)\n{\n\treturn (qid * 2 + 1) * stride;\n}\n\nstatic inline struct nvme_dev *to_nvme_dev(struct nvme_ctrl *ctrl)\n{\n\treturn container_of(ctrl, struct nvme_dev, ctrl);\n}\n\n \nstruct nvme_queue {\n\tstruct nvme_dev *dev;\n\tspinlock_t sq_lock;\n\tvoid *sq_cmds;\n\t  \n\tspinlock_t cq_poll_lock ____cacheline_aligned_in_smp;\n\tstruct nvme_completion *cqes;\n\tdma_addr_t sq_dma_addr;\n\tdma_addr_t cq_dma_addr;\n\tu32 __iomem *q_db;\n\tu32 q_depth;\n\tu16 cq_vector;\n\tu16 sq_tail;\n\tu16 last_sq_tail;\n\tu16 cq_head;\n\tu16 qid;\n\tu8 cq_phase;\n\tu8 sqes;\n\tunsigned long flags;\n#define NVMEQ_ENABLED\t\t0\n#define NVMEQ_SQ_CMB\t\t1\n#define NVMEQ_DELETE_ERROR\t2\n#define NVMEQ_POLLED\t\t3\n\t__le32 *dbbuf_sq_db;\n\t__le32 *dbbuf_cq_db;\n\t__le32 *dbbuf_sq_ei;\n\t__le32 *dbbuf_cq_ei;\n\tstruct completion delete_done;\n};\n\nunion nvme_descriptor {\n\tstruct nvme_sgl_desc\t*sg_list;\n\t__le64\t\t\t*prp_list;\n};\n\n \nstruct nvme_iod {\n\tstruct nvme_request req;\n\tstruct nvme_command cmd;\n\tbool aborted;\n\ts8 nr_allocations;\t \n\tunsigned int dma_len;\t \n\tdma_addr_t first_dma;\n\tdma_addr_t meta_dma;\n\tstruct sg_table sgt;\n\tunion nvme_descriptor list[NVME_MAX_NR_ALLOCATIONS];\n};\n\nstatic inline unsigned int nvme_dbbuf_size(struct nvme_dev *dev)\n{\n\treturn dev->nr_allocated_queues * 8 * dev->db_stride;\n}\n\nstatic void nvme_dbbuf_dma_alloc(struct nvme_dev *dev)\n{\n\tunsigned int mem_size = nvme_dbbuf_size(dev);\n\n\tif (!(dev->ctrl.oacs & NVME_CTRL_OACS_DBBUF_SUPP))\n\t\treturn;\n\n\tif (dev->dbbuf_dbs) {\n\t\t \n\t\tmemset(dev->dbbuf_dbs, 0, mem_size);\n\t\tmemset(dev->dbbuf_eis, 0, mem_size);\n\t\treturn;\n\t}\n\n\tdev->dbbuf_dbs = dma_alloc_coherent(dev->dev, mem_size,\n\t\t\t\t\t    &dev->dbbuf_dbs_dma_addr,\n\t\t\t\t\t    GFP_KERNEL);\n\tif (!dev->dbbuf_dbs)\n\t\tgoto fail;\n\tdev->dbbuf_eis = dma_alloc_coherent(dev->dev, mem_size,\n\t\t\t\t\t    &dev->dbbuf_eis_dma_addr,\n\t\t\t\t\t    GFP_KERNEL);\n\tif (!dev->dbbuf_eis)\n\t\tgoto fail_free_dbbuf_dbs;\n\treturn;\n\nfail_free_dbbuf_dbs:\n\tdma_free_coherent(dev->dev, mem_size, dev->dbbuf_dbs,\n\t\t\t  dev->dbbuf_dbs_dma_addr);\n\tdev->dbbuf_dbs = NULL;\nfail:\n\tdev_warn(dev->dev, \"unable to allocate dma for dbbuf\\n\");\n}\n\nstatic void nvme_dbbuf_dma_free(struct nvme_dev *dev)\n{\n\tunsigned int mem_size = nvme_dbbuf_size(dev);\n\n\tif (dev->dbbuf_dbs) {\n\t\tdma_free_coherent(dev->dev, mem_size,\n\t\t\t\t  dev->dbbuf_dbs, dev->dbbuf_dbs_dma_addr);\n\t\tdev->dbbuf_dbs = NULL;\n\t}\n\tif (dev->dbbuf_eis) {\n\t\tdma_free_coherent(dev->dev, mem_size,\n\t\t\t\t  dev->dbbuf_eis, dev->dbbuf_eis_dma_addr);\n\t\tdev->dbbuf_eis = NULL;\n\t}\n}\n\nstatic void nvme_dbbuf_init(struct nvme_dev *dev,\n\t\t\t    struct nvme_queue *nvmeq, int qid)\n{\n\tif (!dev->dbbuf_dbs || !qid)\n\t\treturn;\n\n\tnvmeq->dbbuf_sq_db = &dev->dbbuf_dbs[sq_idx(qid, dev->db_stride)];\n\tnvmeq->dbbuf_cq_db = &dev->dbbuf_dbs[cq_idx(qid, dev->db_stride)];\n\tnvmeq->dbbuf_sq_ei = &dev->dbbuf_eis[sq_idx(qid, dev->db_stride)];\n\tnvmeq->dbbuf_cq_ei = &dev->dbbuf_eis[cq_idx(qid, dev->db_stride)];\n}\n\nstatic void nvme_dbbuf_free(struct nvme_queue *nvmeq)\n{\n\tif (!nvmeq->qid)\n\t\treturn;\n\n\tnvmeq->dbbuf_sq_db = NULL;\n\tnvmeq->dbbuf_cq_db = NULL;\n\tnvmeq->dbbuf_sq_ei = NULL;\n\tnvmeq->dbbuf_cq_ei = NULL;\n}\n\nstatic void nvme_dbbuf_set(struct nvme_dev *dev)\n{\n\tstruct nvme_command c = { };\n\tunsigned int i;\n\n\tif (!dev->dbbuf_dbs)\n\t\treturn;\n\n\tc.dbbuf.opcode = nvme_admin_dbbuf;\n\tc.dbbuf.prp1 = cpu_to_le64(dev->dbbuf_dbs_dma_addr);\n\tc.dbbuf.prp2 = cpu_to_le64(dev->dbbuf_eis_dma_addr);\n\n\tif (nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0)) {\n\t\tdev_warn(dev->ctrl.device, \"unable to set dbbuf\\n\");\n\t\t \n\t\tnvme_dbbuf_dma_free(dev);\n\n\t\tfor (i = 1; i <= dev->online_queues; i++)\n\t\t\tnvme_dbbuf_free(&dev->queues[i]);\n\t}\n}\n\nstatic inline int nvme_dbbuf_need_event(u16 event_idx, u16 new_idx, u16 old)\n{\n\treturn (u16)(new_idx - event_idx - 1) < (u16)(new_idx - old);\n}\n\n \nstatic bool nvme_dbbuf_update_and_check_event(u16 value, __le32 *dbbuf_db,\n\t\t\t\t\t      volatile __le32 *dbbuf_ei)\n{\n\tif (dbbuf_db) {\n\t\tu16 old_value, event_idx;\n\n\t\t \n\t\twmb();\n\n\t\told_value = le32_to_cpu(*dbbuf_db);\n\t\t*dbbuf_db = cpu_to_le32(value);\n\n\t\t \n\t\tmb();\n\n\t\tevent_idx = le32_to_cpu(*dbbuf_ei);\n\t\tif (!nvme_dbbuf_need_event(event_idx, value, old_value))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nstatic int nvme_pci_npages_prp(void)\n{\n\tunsigned max_bytes = (NVME_MAX_KB_SZ * 1024) + NVME_CTRL_PAGE_SIZE;\n\tunsigned nprps = DIV_ROUND_UP(max_bytes, NVME_CTRL_PAGE_SIZE);\n\treturn DIV_ROUND_UP(8 * nprps, NVME_CTRL_PAGE_SIZE - 8);\n}\n\nstatic int nvme_admin_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,\n\t\t\t\tunsigned int hctx_idx)\n{\n\tstruct nvme_dev *dev = to_nvme_dev(data);\n\tstruct nvme_queue *nvmeq = &dev->queues[0];\n\n\tWARN_ON(hctx_idx != 0);\n\tWARN_ON(dev->admin_tagset.tags[0] != hctx->tags);\n\n\thctx->driver_data = nvmeq;\n\treturn 0;\n}\n\nstatic int nvme_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,\n\t\t\t  unsigned int hctx_idx)\n{\n\tstruct nvme_dev *dev = to_nvme_dev(data);\n\tstruct nvme_queue *nvmeq = &dev->queues[hctx_idx + 1];\n\n\tWARN_ON(dev->tagset.tags[hctx_idx] != hctx->tags);\n\thctx->driver_data = nvmeq;\n\treturn 0;\n}\n\nstatic int nvme_pci_init_request(struct blk_mq_tag_set *set,\n\t\tstruct request *req, unsigned int hctx_idx,\n\t\tunsigned int numa_node)\n{\n\tstruct nvme_iod *iod = blk_mq_rq_to_pdu(req);\n\n\tnvme_req(req)->ctrl = set->driver_data;\n\tnvme_req(req)->cmd = &iod->cmd;\n\treturn 0;\n}\n\nstatic int queue_irq_offset(struct nvme_dev *dev)\n{\n\t \n\tif (dev->num_vecs > 1)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void nvme_pci_map_queues(struct blk_mq_tag_set *set)\n{\n\tstruct nvme_dev *dev = to_nvme_dev(set->driver_data);\n\tint i, qoff, offset;\n\n\toffset = queue_irq_offset(dev);\n\tfor (i = 0, qoff = 0; i < set->nr_maps; i++) {\n\t\tstruct blk_mq_queue_map *map = &set->map[i];\n\n\t\tmap->nr_queues = dev->io_queues[i];\n\t\tif (!map->nr_queues) {\n\t\t\tBUG_ON(i == HCTX_TYPE_DEFAULT);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tmap->queue_offset = qoff;\n\t\tif (i != HCTX_TYPE_POLL && offset)\n\t\t\tblk_mq_pci_map_queues(map, to_pci_dev(dev->dev), offset);\n\t\telse\n\t\t\tblk_mq_map_queues(map);\n\t\tqoff += map->nr_queues;\n\t\toffset += map->nr_queues;\n\t}\n}\n\n \nstatic inline void nvme_write_sq_db(struct nvme_queue *nvmeq, bool write_sq)\n{\n\tif (!write_sq) {\n\t\tu16 next_tail = nvmeq->sq_tail + 1;\n\n\t\tif (next_tail == nvmeq->q_depth)\n\t\t\tnext_tail = 0;\n\t\tif (next_tail != nvmeq->last_sq_tail)\n\t\t\treturn;\n\t}\n\n\tif (nvme_dbbuf_update_and_check_event(nvmeq->sq_tail,\n\t\t\tnvmeq->dbbuf_sq_db, nvmeq->dbbuf_sq_ei))\n\t\twritel(nvmeq->sq_tail, nvmeq->q_db);\n\tnvmeq->last_sq_tail = nvmeq->sq_tail;\n}\n\nstatic inline void nvme_sq_copy_cmd(struct nvme_queue *nvmeq,\n\t\t\t\t    struct nvme_command *cmd)\n{\n\tmemcpy(nvmeq->sq_cmds + (nvmeq->sq_tail << nvmeq->sqes),\n\t\tabsolute_pointer(cmd), sizeof(*cmd));\n\tif (++nvmeq->sq_tail == nvmeq->q_depth)\n\t\tnvmeq->sq_tail = 0;\n}\n\nstatic void nvme_commit_rqs(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct nvme_queue *nvmeq = hctx->driver_data;\n\n\tspin_lock(&nvmeq->sq_lock);\n\tif (nvmeq->sq_tail != nvmeq->last_sq_tail)\n\t\tnvme_write_sq_db(nvmeq, true);\n\tspin_unlock(&nvmeq->sq_lock);\n}\n\nstatic inline bool nvme_pci_use_sgls(struct nvme_dev *dev, struct request *req,\n\t\t\t\t     int nseg)\n{\n\tstruct nvme_queue *nvmeq = req->mq_hctx->driver_data;\n\tunsigned int avg_seg_size;\n\n\tavg_seg_size = DIV_ROUND_UP(blk_rq_payload_bytes(req), nseg);\n\n\tif (!nvme_ctrl_sgl_supported(&dev->ctrl))\n\t\treturn false;\n\tif (!nvmeq->qid)\n\t\treturn false;\n\tif (!sgl_threshold || avg_seg_size < sgl_threshold)\n\t\treturn false;\n\treturn true;\n}\n\nstatic void nvme_free_prps(struct nvme_dev *dev, struct request *req)\n{\n\tconst int last_prp = NVME_CTRL_PAGE_SIZE / sizeof(__le64) - 1;\n\tstruct nvme_iod *iod = blk_mq_rq_to_pdu(req);\n\tdma_addr_t dma_addr = iod->first_dma;\n\tint i;\n\n\tfor (i = 0; i < iod->nr_allocations; i++) {\n\t\t__le64 *prp_list = iod->list[i].prp_list;\n\t\tdma_addr_t next_dma_addr = le64_to_cpu(prp_list[last_prp]);\n\n\t\tdma_pool_free(dev->prp_page_pool, prp_list, dma_addr);\n\t\tdma_addr = next_dma_addr;\n\t}\n}\n\nstatic void nvme_unmap_data(struct nvme_dev *dev, struct request *req)\n{\n\tstruct nvme_iod *iod = blk_mq_rq_to_pdu(req);\n\n\tif (iod->dma_len) {\n\t\tdma_unmap_page(dev->dev, iod->first_dma, iod->dma_len,\n\t\t\t       rq_dma_dir(req));\n\t\treturn;\n\t}\n\n\tWARN_ON_ONCE(!iod->sgt.nents);\n\n\tdma_unmap_sgtable(dev->dev, &iod->sgt, rq_dma_dir(req), 0);\n\n\tif (iod->nr_allocations == 0)\n\t\tdma_pool_free(dev->prp_small_pool, iod->list[0].sg_list,\n\t\t\t      iod->first_dma);\n\telse if (iod->nr_allocations == 1)\n\t\tdma_pool_free(dev->prp_page_pool, iod->list[0].sg_list,\n\t\t\t      iod->first_dma);\n\telse\n\t\tnvme_free_prps(dev, req);\n\tmempool_free(iod->sgt.sgl, dev->iod_mempool);\n}\n\nstatic void nvme_print_sgl(struct scatterlist *sgl, int nents)\n{\n\tint i;\n\tstruct scatterlist *sg;\n\n\tfor_each_sg(sgl, sg, nents, i) {\n\t\tdma_addr_t phys = sg_phys(sg);\n\t\tpr_warn(\"sg[%d] phys_addr:%pad offset:%d length:%d \"\n\t\t\t\"dma_address:%pad dma_length:%d\\n\",\n\t\t\ti, &phys, sg->offset, sg->length, &sg_dma_address(sg),\n\t\t\tsg_dma_len(sg));\n\t}\n}\n\nstatic blk_status_t nvme_pci_setup_prps(struct nvme_dev *dev,\n\t\tstruct request *req, struct nvme_rw_command *cmnd)\n{\n\tstruct nvme_iod *iod = blk_mq_rq_to_pdu(req);\n\tstruct dma_pool *pool;\n\tint length = blk_rq_payload_bytes(req);\n\tstruct scatterlist *sg = iod->sgt.sgl;\n\tint dma_len = sg_dma_len(sg);\n\tu64 dma_addr = sg_dma_address(sg);\n\tint offset = dma_addr & (NVME_CTRL_PAGE_SIZE - 1);\n\t__le64 *prp_list;\n\tdma_addr_t prp_dma;\n\tint nprps, i;\n\n\tlength -= (NVME_CTRL_PAGE_SIZE - offset);\n\tif (length <= 0) {\n\t\tiod->first_dma = 0;\n\t\tgoto done;\n\t}\n\n\tdma_len -= (NVME_CTRL_PAGE_SIZE - offset);\n\tif (dma_len) {\n\t\tdma_addr += (NVME_CTRL_PAGE_SIZE - offset);\n\t} else {\n\t\tsg = sg_next(sg);\n\t\tdma_addr = sg_dma_address(sg);\n\t\tdma_len = sg_dma_len(sg);\n\t}\n\n\tif (length <= NVME_CTRL_PAGE_SIZE) {\n\t\tiod->first_dma = dma_addr;\n\t\tgoto done;\n\t}\n\n\tnprps = DIV_ROUND_UP(length, NVME_CTRL_PAGE_SIZE);\n\tif (nprps <= (256 / 8)) {\n\t\tpool = dev->prp_small_pool;\n\t\tiod->nr_allocations = 0;\n\t} else {\n\t\tpool = dev->prp_page_pool;\n\t\tiod->nr_allocations = 1;\n\t}\n\n\tprp_list = dma_pool_alloc(pool, GFP_ATOMIC, &prp_dma);\n\tif (!prp_list) {\n\t\tiod->nr_allocations = -1;\n\t\treturn BLK_STS_RESOURCE;\n\t}\n\tiod->list[0].prp_list = prp_list;\n\tiod->first_dma = prp_dma;\n\ti = 0;\n\tfor (;;) {\n\t\tif (i == NVME_CTRL_PAGE_SIZE >> 3) {\n\t\t\t__le64 *old_prp_list = prp_list;\n\t\t\tprp_list = dma_pool_alloc(pool, GFP_ATOMIC, &prp_dma);\n\t\t\tif (!prp_list)\n\t\t\t\tgoto free_prps;\n\t\t\tiod->list[iod->nr_allocations++].prp_list = prp_list;\n\t\t\tprp_list[0] = old_prp_list[i - 1];\n\t\t\told_prp_list[i - 1] = cpu_to_le64(prp_dma);\n\t\t\ti = 1;\n\t\t}\n\t\tprp_list[i++] = cpu_to_le64(dma_addr);\n\t\tdma_len -= NVME_CTRL_PAGE_SIZE;\n\t\tdma_addr += NVME_CTRL_PAGE_SIZE;\n\t\tlength -= NVME_CTRL_PAGE_SIZE;\n\t\tif (length <= 0)\n\t\t\tbreak;\n\t\tif (dma_len > 0)\n\t\t\tcontinue;\n\t\tif (unlikely(dma_len < 0))\n\t\t\tgoto bad_sgl;\n\t\tsg = sg_next(sg);\n\t\tdma_addr = sg_dma_address(sg);\n\t\tdma_len = sg_dma_len(sg);\n\t}\ndone:\n\tcmnd->dptr.prp1 = cpu_to_le64(sg_dma_address(iod->sgt.sgl));\n\tcmnd->dptr.prp2 = cpu_to_le64(iod->first_dma);\n\treturn BLK_STS_OK;\nfree_prps:\n\tnvme_free_prps(dev, req);\n\treturn BLK_STS_RESOURCE;\nbad_sgl:\n\tWARN(DO_ONCE(nvme_print_sgl, iod->sgt.sgl, iod->sgt.nents),\n\t\t\t\"Invalid SGL for payload:%d nents:%d\\n\",\n\t\t\tblk_rq_payload_bytes(req), iod->sgt.nents);\n\treturn BLK_STS_IOERR;\n}\n\nstatic void nvme_pci_sgl_set_data(struct nvme_sgl_desc *sge,\n\t\tstruct scatterlist *sg)\n{\n\tsge->addr = cpu_to_le64(sg_dma_address(sg));\n\tsge->length = cpu_to_le32(sg_dma_len(sg));\n\tsge->type = NVME_SGL_FMT_DATA_DESC << 4;\n}\n\nstatic void nvme_pci_sgl_set_seg(struct nvme_sgl_desc *sge,\n\t\tdma_addr_t dma_addr, int entries)\n{\n\tsge->addr = cpu_to_le64(dma_addr);\n\tsge->length = cpu_to_le32(entries * sizeof(*sge));\n\tsge->type = NVME_SGL_FMT_LAST_SEG_DESC << 4;\n}\n\nstatic blk_status_t nvme_pci_setup_sgls(struct nvme_dev *dev,\n\t\tstruct request *req, struct nvme_rw_command *cmd)\n{\n\tstruct nvme_iod *iod = blk_mq_rq_to_pdu(req);\n\tstruct dma_pool *pool;\n\tstruct nvme_sgl_desc *sg_list;\n\tstruct scatterlist *sg = iod->sgt.sgl;\n\tunsigned int entries = iod->sgt.nents;\n\tdma_addr_t sgl_dma;\n\tint i = 0;\n\n\t \n\tcmd->flags = NVME_CMD_SGL_METABUF;\n\n\tif (entries == 1) {\n\t\tnvme_pci_sgl_set_data(&cmd->dptr.sgl, sg);\n\t\treturn BLK_STS_OK;\n\t}\n\n\tif (entries <= (256 / sizeof(struct nvme_sgl_desc))) {\n\t\tpool = dev->prp_small_pool;\n\t\tiod->nr_allocations = 0;\n\t} else {\n\t\tpool = dev->prp_page_pool;\n\t\tiod->nr_allocations = 1;\n\t}\n\n\tsg_list = dma_pool_alloc(pool, GFP_ATOMIC, &sgl_dma);\n\tif (!sg_list) {\n\t\tiod->nr_allocations = -1;\n\t\treturn BLK_STS_RESOURCE;\n\t}\n\n\tiod->list[0].sg_list = sg_list;\n\tiod->first_dma = sgl_dma;\n\n\tnvme_pci_sgl_set_seg(&cmd->dptr.sgl, sgl_dma, entries);\n\tdo {\n\t\tnvme_pci_sgl_set_data(&sg_list[i++], sg);\n\t\tsg = sg_next(sg);\n\t} while (--entries > 0);\n\n\treturn BLK_STS_OK;\n}\n\nstatic blk_status_t nvme_setup_prp_simple(struct nvme_dev *dev,\n\t\tstruct request *req, struct nvme_rw_command *cmnd,\n\t\tstruct bio_vec *bv)\n{\n\tstruct nvme_iod *iod = blk_mq_rq_to_pdu(req);\n\tunsigned int offset = bv->bv_offset & (NVME_CTRL_PAGE_SIZE - 1);\n\tunsigned int first_prp_len = NVME_CTRL_PAGE_SIZE - offset;\n\n\tiod->first_dma = dma_map_bvec(dev->dev, bv, rq_dma_dir(req), 0);\n\tif (dma_mapping_error(dev->dev, iod->first_dma))\n\t\treturn BLK_STS_RESOURCE;\n\tiod->dma_len = bv->bv_len;\n\n\tcmnd->dptr.prp1 = cpu_to_le64(iod->first_dma);\n\tif (bv->bv_len > first_prp_len)\n\t\tcmnd->dptr.prp2 = cpu_to_le64(iod->first_dma + first_prp_len);\n\telse\n\t\tcmnd->dptr.prp2 = 0;\n\treturn BLK_STS_OK;\n}\n\nstatic blk_status_t nvme_setup_sgl_simple(struct nvme_dev *dev,\n\t\tstruct request *req, struct nvme_rw_command *cmnd,\n\t\tstruct bio_vec *bv)\n{\n\tstruct nvme_iod *iod = blk_mq_rq_to_pdu(req);\n\n\tiod->first_dma = dma_map_bvec(dev->dev, bv, rq_dma_dir(req), 0);\n\tif (dma_mapping_error(dev->dev, iod->first_dma))\n\t\treturn BLK_STS_RESOURCE;\n\tiod->dma_len = bv->bv_len;\n\n\tcmnd->flags = NVME_CMD_SGL_METABUF;\n\tcmnd->dptr.sgl.addr = cpu_to_le64(iod->first_dma);\n\tcmnd->dptr.sgl.length = cpu_to_le32(iod->dma_len);\n\tcmnd->dptr.sgl.type = NVME_SGL_FMT_DATA_DESC << 4;\n\treturn BLK_STS_OK;\n}\n\nstatic blk_status_t nvme_map_data(struct nvme_dev *dev, struct request *req,\n\t\tstruct nvme_command *cmnd)\n{\n\tstruct nvme_iod *iod = blk_mq_rq_to_pdu(req);\n\tblk_status_t ret = BLK_STS_RESOURCE;\n\tint rc;\n\n\tif (blk_rq_nr_phys_segments(req) == 1) {\n\t\tstruct nvme_queue *nvmeq = req->mq_hctx->driver_data;\n\t\tstruct bio_vec bv = req_bvec(req);\n\n\t\tif (!is_pci_p2pdma_page(bv.bv_page)) {\n\t\t\tif (bv.bv_offset + bv.bv_len <= NVME_CTRL_PAGE_SIZE * 2)\n\t\t\t\treturn nvme_setup_prp_simple(dev, req,\n\t\t\t\t\t\t\t     &cmnd->rw, &bv);\n\n\t\t\tif (nvmeq->qid && sgl_threshold &&\n\t\t\t    nvme_ctrl_sgl_supported(&dev->ctrl))\n\t\t\t\treturn nvme_setup_sgl_simple(dev, req,\n\t\t\t\t\t\t\t     &cmnd->rw, &bv);\n\t\t}\n\t}\n\n\tiod->dma_len = 0;\n\tiod->sgt.sgl = mempool_alloc(dev->iod_mempool, GFP_ATOMIC);\n\tif (!iod->sgt.sgl)\n\t\treturn BLK_STS_RESOURCE;\n\tsg_init_table(iod->sgt.sgl, blk_rq_nr_phys_segments(req));\n\tiod->sgt.orig_nents = blk_rq_map_sg(req->q, req, iod->sgt.sgl);\n\tif (!iod->sgt.orig_nents)\n\t\tgoto out_free_sg;\n\n\trc = dma_map_sgtable(dev->dev, &iod->sgt, rq_dma_dir(req),\n\t\t\t     DMA_ATTR_NO_WARN);\n\tif (rc) {\n\t\tif (rc == -EREMOTEIO)\n\t\t\tret = BLK_STS_TARGET;\n\t\tgoto out_free_sg;\n\t}\n\n\tif (nvme_pci_use_sgls(dev, req, iod->sgt.nents))\n\t\tret = nvme_pci_setup_sgls(dev, req, &cmnd->rw);\n\telse\n\t\tret = nvme_pci_setup_prps(dev, req, &cmnd->rw);\n\tif (ret != BLK_STS_OK)\n\t\tgoto out_unmap_sg;\n\treturn BLK_STS_OK;\n\nout_unmap_sg:\n\tdma_unmap_sgtable(dev->dev, &iod->sgt, rq_dma_dir(req), 0);\nout_free_sg:\n\tmempool_free(iod->sgt.sgl, dev->iod_mempool);\n\treturn ret;\n}\n\nstatic blk_status_t nvme_map_metadata(struct nvme_dev *dev, struct request *req,\n\t\tstruct nvme_command *cmnd)\n{\n\tstruct nvme_iod *iod = blk_mq_rq_to_pdu(req);\n\n\tiod->meta_dma = dma_map_bvec(dev->dev, rq_integrity_vec(req),\n\t\t\trq_dma_dir(req), 0);\n\tif (dma_mapping_error(dev->dev, iod->meta_dma))\n\t\treturn BLK_STS_IOERR;\n\tcmnd->rw.metadata = cpu_to_le64(iod->meta_dma);\n\treturn BLK_STS_OK;\n}\n\nstatic blk_status_t nvme_prep_rq(struct nvme_dev *dev, struct request *req)\n{\n\tstruct nvme_iod *iod = blk_mq_rq_to_pdu(req);\n\tblk_status_t ret;\n\n\tiod->aborted = false;\n\tiod->nr_allocations = -1;\n\tiod->sgt.nents = 0;\n\n\tret = nvme_setup_cmd(req->q->queuedata, req);\n\tif (ret)\n\t\treturn ret;\n\n\tif (blk_rq_nr_phys_segments(req)) {\n\t\tret = nvme_map_data(dev, req, &iod->cmd);\n\t\tif (ret)\n\t\t\tgoto out_free_cmd;\n\t}\n\n\tif (blk_integrity_rq(req)) {\n\t\tret = nvme_map_metadata(dev, req, &iod->cmd);\n\t\tif (ret)\n\t\t\tgoto out_unmap_data;\n\t}\n\n\tnvme_start_request(req);\n\treturn BLK_STS_OK;\nout_unmap_data:\n\tnvme_unmap_data(dev, req);\nout_free_cmd:\n\tnvme_cleanup_cmd(req);\n\treturn ret;\n}\n\n \nstatic blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,\n\t\t\t const struct blk_mq_queue_data *bd)\n{\n\tstruct nvme_queue *nvmeq = hctx->driver_data;\n\tstruct nvme_dev *dev = nvmeq->dev;\n\tstruct request *req = bd->rq;\n\tstruct nvme_iod *iod = blk_mq_rq_to_pdu(req);\n\tblk_status_t ret;\n\n\t \n\tif (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))\n\t\treturn BLK_STS_IOERR;\n\n\tif (unlikely(!nvme_check_ready(&dev->ctrl, req, true)))\n\t\treturn nvme_fail_nonready_command(&dev->ctrl, req);\n\n\tret = nvme_prep_rq(dev, req);\n\tif (unlikely(ret))\n\t\treturn ret;\n\tspin_lock(&nvmeq->sq_lock);\n\tnvme_sq_copy_cmd(nvmeq, &iod->cmd);\n\tnvme_write_sq_db(nvmeq, bd->last);\n\tspin_unlock(&nvmeq->sq_lock);\n\treturn BLK_STS_OK;\n}\n\nstatic void nvme_submit_cmds(struct nvme_queue *nvmeq, struct request **rqlist)\n{\n\tspin_lock(&nvmeq->sq_lock);\n\twhile (!rq_list_empty(*rqlist)) {\n\t\tstruct request *req = rq_list_pop(rqlist);\n\t\tstruct nvme_iod *iod = blk_mq_rq_to_pdu(req);\n\n\t\tnvme_sq_copy_cmd(nvmeq, &iod->cmd);\n\t}\n\tnvme_write_sq_db(nvmeq, true);\n\tspin_unlock(&nvmeq->sq_lock);\n}\n\nstatic bool nvme_prep_rq_batch(struct nvme_queue *nvmeq, struct request *req)\n{\n\t \n\tif (unlikely(!test_bit(NVMEQ_ENABLED, &nvmeq->flags)))\n\t\treturn false;\n\tif (unlikely(!nvme_check_ready(&nvmeq->dev->ctrl, req, true)))\n\t\treturn false;\n\n\treq->mq_hctx->tags->rqs[req->tag] = req;\n\treturn nvme_prep_rq(nvmeq->dev, req) == BLK_STS_OK;\n}\n\nstatic void nvme_queue_rqs(struct request **rqlist)\n{\n\tstruct request *req, *next, *prev = NULL;\n\tstruct request *requeue_list = NULL;\n\n\trq_list_for_each_safe(rqlist, req, next) {\n\t\tstruct nvme_queue *nvmeq = req->mq_hctx->driver_data;\n\n\t\tif (!nvme_prep_rq_batch(nvmeq, req)) {\n\t\t\t \n\t\t\trq_list_move(rqlist, &requeue_list, req, prev);\n\n\t\t\treq = prev;\n\t\t\tif (!req)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tif (!next || req->mq_hctx != next->mq_hctx) {\n\t\t\t \n\t\t\treq->rq_next = NULL;\n\t\t\tnvme_submit_cmds(nvmeq, rqlist);\n\t\t\t*rqlist = next;\n\t\t\tprev = NULL;\n\t\t} else\n\t\t\tprev = req;\n\t}\n\n\t*rqlist = requeue_list;\n}\n\nstatic __always_inline void nvme_pci_unmap_rq(struct request *req)\n{\n\tstruct nvme_queue *nvmeq = req->mq_hctx->driver_data;\n\tstruct nvme_dev *dev = nvmeq->dev;\n\n\tif (blk_integrity_rq(req)) {\n\t        struct nvme_iod *iod = blk_mq_rq_to_pdu(req);\n\n\t\tdma_unmap_page(dev->dev, iod->meta_dma,\n\t\t\t       rq_integrity_vec(req)->bv_len, rq_dma_dir(req));\n\t}\n\n\tif (blk_rq_nr_phys_segments(req))\n\t\tnvme_unmap_data(dev, req);\n}\n\nstatic void nvme_pci_complete_rq(struct request *req)\n{\n\tnvme_pci_unmap_rq(req);\n\tnvme_complete_rq(req);\n}\n\nstatic void nvme_pci_complete_batch(struct io_comp_batch *iob)\n{\n\tnvme_complete_batch(iob, nvme_pci_unmap_rq);\n}\n\n \nstatic inline bool nvme_cqe_pending(struct nvme_queue *nvmeq)\n{\n\tstruct nvme_completion *hcqe = &nvmeq->cqes[nvmeq->cq_head];\n\n\treturn (le16_to_cpu(READ_ONCE(hcqe->status)) & 1) == nvmeq->cq_phase;\n}\n\nstatic inline void nvme_ring_cq_doorbell(struct nvme_queue *nvmeq)\n{\n\tu16 head = nvmeq->cq_head;\n\n\tif (nvme_dbbuf_update_and_check_event(head, nvmeq->dbbuf_cq_db,\n\t\t\t\t\t      nvmeq->dbbuf_cq_ei))\n\t\twritel(head, nvmeq->q_db + nvmeq->dev->db_stride);\n}\n\nstatic inline struct blk_mq_tags *nvme_queue_tagset(struct nvme_queue *nvmeq)\n{\n\tif (!nvmeq->qid)\n\t\treturn nvmeq->dev->admin_tagset.tags[0];\n\treturn nvmeq->dev->tagset.tags[nvmeq->qid - 1];\n}\n\nstatic inline void nvme_handle_cqe(struct nvme_queue *nvmeq,\n\t\t\t\t   struct io_comp_batch *iob, u16 idx)\n{\n\tstruct nvme_completion *cqe = &nvmeq->cqes[idx];\n\t__u16 command_id = READ_ONCE(cqe->command_id);\n\tstruct request *req;\n\n\t \n\tif (unlikely(nvme_is_aen_req(nvmeq->qid, command_id))) {\n\t\tnvme_complete_async_event(&nvmeq->dev->ctrl,\n\t\t\t\tcqe->status, &cqe->result);\n\t\treturn;\n\t}\n\n\treq = nvme_find_rq(nvme_queue_tagset(nvmeq), command_id);\n\tif (unlikely(!req)) {\n\t\tdev_warn(nvmeq->dev->ctrl.device,\n\t\t\t\"invalid id %d completed on queue %d\\n\",\n\t\t\tcommand_id, le16_to_cpu(cqe->sq_id));\n\t\treturn;\n\t}\n\n\ttrace_nvme_sq(req, cqe->sq_head, nvmeq->sq_tail);\n\tif (!nvme_try_complete_req(req, cqe->status, cqe->result) &&\n\t    !blk_mq_add_to_batch(req, iob, nvme_req(req)->status,\n\t\t\t\t\tnvme_pci_complete_batch))\n\t\tnvme_pci_complete_rq(req);\n}\n\nstatic inline void nvme_update_cq_head(struct nvme_queue *nvmeq)\n{\n\tu32 tmp = nvmeq->cq_head + 1;\n\n\tif (tmp == nvmeq->q_depth) {\n\t\tnvmeq->cq_head = 0;\n\t\tnvmeq->cq_phase ^= 1;\n\t} else {\n\t\tnvmeq->cq_head = tmp;\n\t}\n}\n\nstatic inline int nvme_poll_cq(struct nvme_queue *nvmeq,\n\t\t\t       struct io_comp_batch *iob)\n{\n\tint found = 0;\n\n\twhile (nvme_cqe_pending(nvmeq)) {\n\t\tfound++;\n\t\t \n\t\tdma_rmb();\n\t\tnvme_handle_cqe(nvmeq, iob, nvmeq->cq_head);\n\t\tnvme_update_cq_head(nvmeq);\n\t}\n\n\tif (found)\n\t\tnvme_ring_cq_doorbell(nvmeq);\n\treturn found;\n}\n\nstatic irqreturn_t nvme_irq(int irq, void *data)\n{\n\tstruct nvme_queue *nvmeq = data;\n\tDEFINE_IO_COMP_BATCH(iob);\n\n\tif (nvme_poll_cq(nvmeq, &iob)) {\n\t\tif (!rq_list_empty(iob.req_list))\n\t\t\tnvme_pci_complete_batch(&iob);\n\t\treturn IRQ_HANDLED;\n\t}\n\treturn IRQ_NONE;\n}\n\nstatic irqreturn_t nvme_irq_check(int irq, void *data)\n{\n\tstruct nvme_queue *nvmeq = data;\n\n\tif (nvme_cqe_pending(nvmeq))\n\t\treturn IRQ_WAKE_THREAD;\n\treturn IRQ_NONE;\n}\n\n \nstatic void nvme_poll_irqdisable(struct nvme_queue *nvmeq)\n{\n\tstruct pci_dev *pdev = to_pci_dev(nvmeq->dev->dev);\n\n\tWARN_ON_ONCE(test_bit(NVMEQ_POLLED, &nvmeq->flags));\n\n\tdisable_irq(pci_irq_vector(pdev, nvmeq->cq_vector));\n\tnvme_poll_cq(nvmeq, NULL);\n\tenable_irq(pci_irq_vector(pdev, nvmeq->cq_vector));\n}\n\nstatic int nvme_poll(struct blk_mq_hw_ctx *hctx, struct io_comp_batch *iob)\n{\n\tstruct nvme_queue *nvmeq = hctx->driver_data;\n\tbool found;\n\n\tif (!nvme_cqe_pending(nvmeq))\n\t\treturn 0;\n\n\tspin_lock(&nvmeq->cq_poll_lock);\n\tfound = nvme_poll_cq(nvmeq, iob);\n\tspin_unlock(&nvmeq->cq_poll_lock);\n\n\treturn found;\n}\n\nstatic void nvme_pci_submit_async_event(struct nvme_ctrl *ctrl)\n{\n\tstruct nvme_dev *dev = to_nvme_dev(ctrl);\n\tstruct nvme_queue *nvmeq = &dev->queues[0];\n\tstruct nvme_command c = { };\n\n\tc.common.opcode = nvme_admin_async_event;\n\tc.common.command_id = NVME_AQ_BLK_MQ_DEPTH;\n\n\tspin_lock(&nvmeq->sq_lock);\n\tnvme_sq_copy_cmd(nvmeq, &c);\n\tnvme_write_sq_db(nvmeq, true);\n\tspin_unlock(&nvmeq->sq_lock);\n}\n\nstatic int adapter_delete_queue(struct nvme_dev *dev, u8 opcode, u16 id)\n{\n\tstruct nvme_command c = { };\n\n\tc.delete_queue.opcode = opcode;\n\tc.delete_queue.qid = cpu_to_le16(id);\n\n\treturn nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);\n}\n\nstatic int adapter_alloc_cq(struct nvme_dev *dev, u16 qid,\n\t\tstruct nvme_queue *nvmeq, s16 vector)\n{\n\tstruct nvme_command c = { };\n\tint flags = NVME_QUEUE_PHYS_CONTIG;\n\n\tif (!test_bit(NVMEQ_POLLED, &nvmeq->flags))\n\t\tflags |= NVME_CQ_IRQ_ENABLED;\n\n\t \n\tc.create_cq.opcode = nvme_admin_create_cq;\n\tc.create_cq.prp1 = cpu_to_le64(nvmeq->cq_dma_addr);\n\tc.create_cq.cqid = cpu_to_le16(qid);\n\tc.create_cq.qsize = cpu_to_le16(nvmeq->q_depth - 1);\n\tc.create_cq.cq_flags = cpu_to_le16(flags);\n\tc.create_cq.irq_vector = cpu_to_le16(vector);\n\n\treturn nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);\n}\n\nstatic int adapter_alloc_sq(struct nvme_dev *dev, u16 qid,\n\t\t\t\t\t\tstruct nvme_queue *nvmeq)\n{\n\tstruct nvme_ctrl *ctrl = &dev->ctrl;\n\tstruct nvme_command c = { };\n\tint flags = NVME_QUEUE_PHYS_CONTIG;\n\n\t \n\tif (ctrl->quirks & NVME_QUIRK_MEDIUM_PRIO_SQ)\n\t\tflags |= NVME_SQ_PRIO_MEDIUM;\n\n\t \n\tc.create_sq.opcode = nvme_admin_create_sq;\n\tc.create_sq.prp1 = cpu_to_le64(nvmeq->sq_dma_addr);\n\tc.create_sq.sqid = cpu_to_le16(qid);\n\tc.create_sq.qsize = cpu_to_le16(nvmeq->q_depth - 1);\n\tc.create_sq.sq_flags = cpu_to_le16(flags);\n\tc.create_sq.cqid = cpu_to_le16(qid);\n\n\treturn nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);\n}\n\nstatic int adapter_delete_cq(struct nvme_dev *dev, u16 cqid)\n{\n\treturn adapter_delete_queue(dev, nvme_admin_delete_cq, cqid);\n}\n\nstatic int adapter_delete_sq(struct nvme_dev *dev, u16 sqid)\n{\n\treturn adapter_delete_queue(dev, nvme_admin_delete_sq, sqid);\n}\n\nstatic enum rq_end_io_ret abort_endio(struct request *req, blk_status_t error)\n{\n\tstruct nvme_queue *nvmeq = req->mq_hctx->driver_data;\n\n\tdev_warn(nvmeq->dev->ctrl.device,\n\t\t \"Abort status: 0x%x\", nvme_req(req)->status);\n\tatomic_inc(&nvmeq->dev->ctrl.abort_limit);\n\tblk_mq_free_request(req);\n\treturn RQ_END_IO_NONE;\n}\n\nstatic bool nvme_should_reset(struct nvme_dev *dev, u32 csts)\n{\n\t \n\tbool nssro = dev->subsystem && (csts & NVME_CSTS_NSSRO);\n\n\t \n\tswitch (nvme_ctrl_state(&dev->ctrl)) {\n\tcase NVME_CTRL_RESETTING:\n\tcase NVME_CTRL_CONNECTING:\n\t\treturn false;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tif (!(csts & NVME_CSTS_CFS) && !nssro)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void nvme_warn_reset(struct nvme_dev *dev, u32 csts)\n{\n\t \n\tu16 pci_status;\n\tint result;\n\n\tresult = pci_read_config_word(to_pci_dev(dev->dev), PCI_STATUS,\n\t\t\t\t      &pci_status);\n\tif (result == PCIBIOS_SUCCESSFUL)\n\t\tdev_warn(dev->ctrl.device,\n\t\t\t \"controller is down; will reset: CSTS=0x%x, PCI_STATUS=0x%hx\\n\",\n\t\t\t csts, pci_status);\n\telse\n\t\tdev_warn(dev->ctrl.device,\n\t\t\t \"controller is down; will reset: CSTS=0x%x, PCI_STATUS read failed (%d)\\n\",\n\t\t\t csts, result);\n\n\tif (csts != ~0)\n\t\treturn;\n\n\tdev_warn(dev->ctrl.device,\n\t\t \"Does your device have a faulty power saving mode enabled?\\n\");\n\tdev_warn(dev->ctrl.device,\n\t\t \"Try \\\"nvme_core.default_ps_max_latency_us=0 pcie_aspm=off\\\" and report a bug\\n\");\n}\n\nstatic enum blk_eh_timer_return nvme_timeout(struct request *req)\n{\n\tstruct nvme_iod *iod = blk_mq_rq_to_pdu(req);\n\tstruct nvme_queue *nvmeq = req->mq_hctx->driver_data;\n\tstruct nvme_dev *dev = nvmeq->dev;\n\tstruct request *abort_req;\n\tstruct nvme_command cmd = { };\n\tu32 csts = readl(dev->bar + NVME_REG_CSTS);\n\n\t \n\tmb();\n\tif (pci_channel_offline(to_pci_dev(dev->dev)))\n\t\treturn BLK_EH_RESET_TIMER;\n\n\t \n\tif (nvme_should_reset(dev, csts)) {\n\t\tnvme_warn_reset(dev, csts);\n\t\tgoto disable;\n\t}\n\n\t \n\tif (test_bit(NVMEQ_POLLED, &nvmeq->flags))\n\t\tnvme_poll(req->mq_hctx, NULL);\n\telse\n\t\tnvme_poll_irqdisable(nvmeq);\n\n\tif (blk_mq_rq_state(req) != MQ_RQ_IN_FLIGHT) {\n\t\tdev_warn(dev->ctrl.device,\n\t\t\t \"I/O %d QID %d timeout, completion polled\\n\",\n\t\t\t req->tag, nvmeq->qid);\n\t\treturn BLK_EH_DONE;\n\t}\n\n\t \n\tswitch (nvme_ctrl_state(&dev->ctrl)) {\n\tcase NVME_CTRL_CONNECTING:\n\t\tnvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DELETING);\n\t\tfallthrough;\n\tcase NVME_CTRL_DELETING:\n\t\tdev_warn_ratelimited(dev->ctrl.device,\n\t\t\t \"I/O %d QID %d timeout, disable controller\\n\",\n\t\t\t req->tag, nvmeq->qid);\n\t\tnvme_req(req)->flags |= NVME_REQ_CANCELLED;\n\t\tnvme_dev_disable(dev, true);\n\t\treturn BLK_EH_DONE;\n\tcase NVME_CTRL_RESETTING:\n\t\treturn BLK_EH_RESET_TIMER;\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tif (!nvmeq->qid || iod->aborted) {\n\t\tdev_warn(dev->ctrl.device,\n\t\t\t \"I/O %d QID %d timeout, reset controller\\n\",\n\t\t\t req->tag, nvmeq->qid);\n\t\tnvme_req(req)->flags |= NVME_REQ_CANCELLED;\n\t\tgoto disable;\n\t}\n\n\tif (atomic_dec_return(&dev->ctrl.abort_limit) < 0) {\n\t\tatomic_inc(&dev->ctrl.abort_limit);\n\t\treturn BLK_EH_RESET_TIMER;\n\t}\n\tiod->aborted = true;\n\n\tcmd.abort.opcode = nvme_admin_abort_cmd;\n\tcmd.abort.cid = nvme_cid(req);\n\tcmd.abort.sqid = cpu_to_le16(nvmeq->qid);\n\n\tdev_warn(nvmeq->dev->ctrl.device,\n\t\t\"I/O %d (%s) QID %d timeout, aborting\\n\",\n\t\t req->tag,\n\t\t nvme_get_opcode_str(nvme_req(req)->cmd->common.opcode),\n\t\t nvmeq->qid);\n\n\tabort_req = blk_mq_alloc_request(dev->ctrl.admin_q, nvme_req_op(&cmd),\n\t\t\t\t\t BLK_MQ_REQ_NOWAIT);\n\tif (IS_ERR(abort_req)) {\n\t\tatomic_inc(&dev->ctrl.abort_limit);\n\t\treturn BLK_EH_RESET_TIMER;\n\t}\n\tnvme_init_request(abort_req, &cmd);\n\n\tabort_req->end_io = abort_endio;\n\tabort_req->end_io_data = NULL;\n\tblk_execute_rq_nowait(abort_req, false);\n\n\t \n\treturn BLK_EH_RESET_TIMER;\n\ndisable:\n\tif (!nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_RESETTING))\n\t\treturn BLK_EH_DONE;\n\n\tnvme_dev_disable(dev, false);\n\tif (nvme_try_sched_reset(&dev->ctrl))\n\t\tnvme_unquiesce_io_queues(&dev->ctrl);\n\treturn BLK_EH_DONE;\n}\n\nstatic void nvme_free_queue(struct nvme_queue *nvmeq)\n{\n\tdma_free_coherent(nvmeq->dev->dev, CQ_SIZE(nvmeq),\n\t\t\t\t(void *)nvmeq->cqes, nvmeq->cq_dma_addr);\n\tif (!nvmeq->sq_cmds)\n\t\treturn;\n\n\tif (test_and_clear_bit(NVMEQ_SQ_CMB, &nvmeq->flags)) {\n\t\tpci_free_p2pmem(to_pci_dev(nvmeq->dev->dev),\n\t\t\t\tnvmeq->sq_cmds, SQ_SIZE(nvmeq));\n\t} else {\n\t\tdma_free_coherent(nvmeq->dev->dev, SQ_SIZE(nvmeq),\n\t\t\t\tnvmeq->sq_cmds, nvmeq->sq_dma_addr);\n\t}\n}\n\nstatic void nvme_free_queues(struct nvme_dev *dev, int lowest)\n{\n\tint i;\n\n\tfor (i = dev->ctrl.queue_count - 1; i >= lowest; i--) {\n\t\tdev->ctrl.queue_count--;\n\t\tnvme_free_queue(&dev->queues[i]);\n\t}\n}\n\nstatic void nvme_suspend_queue(struct nvme_dev *dev, unsigned int qid)\n{\n\tstruct nvme_queue *nvmeq = &dev->queues[qid];\n\n\tif (!test_and_clear_bit(NVMEQ_ENABLED, &nvmeq->flags))\n\t\treturn;\n\n\t \n\tmb();\n\n\tnvmeq->dev->online_queues--;\n\tif (!nvmeq->qid && nvmeq->dev->ctrl.admin_q)\n\t\tnvme_quiesce_admin_queue(&nvmeq->dev->ctrl);\n\tif (!test_and_clear_bit(NVMEQ_POLLED, &nvmeq->flags))\n\t\tpci_free_irq(to_pci_dev(dev->dev), nvmeq->cq_vector, nvmeq);\n}\n\nstatic void nvme_suspend_io_queues(struct nvme_dev *dev)\n{\n\tint i;\n\n\tfor (i = dev->ctrl.queue_count - 1; i > 0; i--)\n\t\tnvme_suspend_queue(dev, i);\n}\n\n \nstatic void nvme_reap_pending_cqes(struct nvme_dev *dev)\n{\n\tint i;\n\n\tfor (i = dev->ctrl.queue_count - 1; i > 0; i--) {\n\t\tspin_lock(&dev->queues[i].cq_poll_lock);\n\t\tnvme_poll_cq(&dev->queues[i], NULL);\n\t\tspin_unlock(&dev->queues[i].cq_poll_lock);\n\t}\n}\n\nstatic int nvme_cmb_qdepth(struct nvme_dev *dev, int nr_io_queues,\n\t\t\t\tint entry_size)\n{\n\tint q_depth = dev->q_depth;\n\tunsigned q_size_aligned = roundup(q_depth * entry_size,\n\t\t\t\t\t  NVME_CTRL_PAGE_SIZE);\n\n\tif (q_size_aligned * nr_io_queues > dev->cmb_size) {\n\t\tu64 mem_per_q = div_u64(dev->cmb_size, nr_io_queues);\n\n\t\tmem_per_q = round_down(mem_per_q, NVME_CTRL_PAGE_SIZE);\n\t\tq_depth = div_u64(mem_per_q, entry_size);\n\n\t\t \n\t\tif (q_depth < 64)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn q_depth;\n}\n\nstatic int nvme_alloc_sq_cmds(struct nvme_dev *dev, struct nvme_queue *nvmeq,\n\t\t\t\tint qid)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev->dev);\n\n\tif (qid && dev->cmb_use_sqes && (dev->cmbsz & NVME_CMBSZ_SQS)) {\n\t\tnvmeq->sq_cmds = pci_alloc_p2pmem(pdev, SQ_SIZE(nvmeq));\n\t\tif (nvmeq->sq_cmds) {\n\t\t\tnvmeq->sq_dma_addr = pci_p2pmem_virt_to_bus(pdev,\n\t\t\t\t\t\t\tnvmeq->sq_cmds);\n\t\t\tif (nvmeq->sq_dma_addr) {\n\t\t\t\tset_bit(NVMEQ_SQ_CMB, &nvmeq->flags);\n\t\t\t\treturn 0;\n\t\t\t}\n\n\t\t\tpci_free_p2pmem(pdev, nvmeq->sq_cmds, SQ_SIZE(nvmeq));\n\t\t}\n\t}\n\n\tnvmeq->sq_cmds = dma_alloc_coherent(dev->dev, SQ_SIZE(nvmeq),\n\t\t\t\t&nvmeq->sq_dma_addr, GFP_KERNEL);\n\tif (!nvmeq->sq_cmds)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic int nvme_alloc_queue(struct nvme_dev *dev, int qid, int depth)\n{\n\tstruct nvme_queue *nvmeq = &dev->queues[qid];\n\n\tif (dev->ctrl.queue_count > qid)\n\t\treturn 0;\n\n\tnvmeq->sqes = qid ? dev->io_sqes : NVME_ADM_SQES;\n\tnvmeq->q_depth = depth;\n\tnvmeq->cqes = dma_alloc_coherent(dev->dev, CQ_SIZE(nvmeq),\n\t\t\t\t\t &nvmeq->cq_dma_addr, GFP_KERNEL);\n\tif (!nvmeq->cqes)\n\t\tgoto free_nvmeq;\n\n\tif (nvme_alloc_sq_cmds(dev, nvmeq, qid))\n\t\tgoto free_cqdma;\n\n\tnvmeq->dev = dev;\n\tspin_lock_init(&nvmeq->sq_lock);\n\tspin_lock_init(&nvmeq->cq_poll_lock);\n\tnvmeq->cq_head = 0;\n\tnvmeq->cq_phase = 1;\n\tnvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];\n\tnvmeq->qid = qid;\n\tdev->ctrl.queue_count++;\n\n\treturn 0;\n\n free_cqdma:\n\tdma_free_coherent(dev->dev, CQ_SIZE(nvmeq), (void *)nvmeq->cqes,\n\t\t\t  nvmeq->cq_dma_addr);\n free_nvmeq:\n\treturn -ENOMEM;\n}\n\nstatic int queue_request_irq(struct nvme_queue *nvmeq)\n{\n\tstruct pci_dev *pdev = to_pci_dev(nvmeq->dev->dev);\n\tint nr = nvmeq->dev->ctrl.instance;\n\n\tif (use_threaded_interrupts) {\n\t\treturn pci_request_irq(pdev, nvmeq->cq_vector, nvme_irq_check,\n\t\t\t\tnvme_irq, nvmeq, \"nvme%dq%d\", nr, nvmeq->qid);\n\t} else {\n\t\treturn pci_request_irq(pdev, nvmeq->cq_vector, nvme_irq,\n\t\t\t\tNULL, nvmeq, \"nvme%dq%d\", nr, nvmeq->qid);\n\t}\n}\n\nstatic void nvme_init_queue(struct nvme_queue *nvmeq, u16 qid)\n{\n\tstruct nvme_dev *dev = nvmeq->dev;\n\n\tnvmeq->sq_tail = 0;\n\tnvmeq->last_sq_tail = 0;\n\tnvmeq->cq_head = 0;\n\tnvmeq->cq_phase = 1;\n\tnvmeq->q_db = &dev->dbs[qid * 2 * dev->db_stride];\n\tmemset((void *)nvmeq->cqes, 0, CQ_SIZE(nvmeq));\n\tnvme_dbbuf_init(dev, nvmeq, qid);\n\tdev->online_queues++;\n\twmb();  \n}\n\n \nstatic int nvme_setup_io_queues_trylock(struct nvme_dev *dev)\n{\n\t \n\tif (!mutex_trylock(&dev->shutdown_lock))\n\t\treturn -ENODEV;\n\n\t \n\tif (nvme_ctrl_state(&dev->ctrl) != NVME_CTRL_CONNECTING) {\n\t\tmutex_unlock(&dev->shutdown_lock);\n\t\treturn -ENODEV;\n\t}\n\n\treturn 0;\n}\n\nstatic int nvme_create_queue(struct nvme_queue *nvmeq, int qid, bool polled)\n{\n\tstruct nvme_dev *dev = nvmeq->dev;\n\tint result;\n\tu16 vector = 0;\n\n\tclear_bit(NVMEQ_DELETE_ERROR, &nvmeq->flags);\n\n\t \n\tif (!polled)\n\t\tvector = dev->num_vecs == 1 ? 0 : qid;\n\telse\n\t\tset_bit(NVMEQ_POLLED, &nvmeq->flags);\n\n\tresult = adapter_alloc_cq(dev, qid, nvmeq, vector);\n\tif (result)\n\t\treturn result;\n\n\tresult = adapter_alloc_sq(dev, qid, nvmeq);\n\tif (result < 0)\n\t\treturn result;\n\tif (result)\n\t\tgoto release_cq;\n\n\tnvmeq->cq_vector = vector;\n\n\tresult = nvme_setup_io_queues_trylock(dev);\n\tif (result)\n\t\treturn result;\n\tnvme_init_queue(nvmeq, qid);\n\tif (!polled) {\n\t\tresult = queue_request_irq(nvmeq);\n\t\tif (result < 0)\n\t\t\tgoto release_sq;\n\t}\n\n\tset_bit(NVMEQ_ENABLED, &nvmeq->flags);\n\tmutex_unlock(&dev->shutdown_lock);\n\treturn result;\n\nrelease_sq:\n\tdev->online_queues--;\n\tmutex_unlock(&dev->shutdown_lock);\n\tadapter_delete_sq(dev, qid);\nrelease_cq:\n\tadapter_delete_cq(dev, qid);\n\treturn result;\n}\n\nstatic const struct blk_mq_ops nvme_mq_admin_ops = {\n\t.queue_rq\t= nvme_queue_rq,\n\t.complete\t= nvme_pci_complete_rq,\n\t.init_hctx\t= nvme_admin_init_hctx,\n\t.init_request\t= nvme_pci_init_request,\n\t.timeout\t= nvme_timeout,\n};\n\nstatic const struct blk_mq_ops nvme_mq_ops = {\n\t.queue_rq\t= nvme_queue_rq,\n\t.queue_rqs\t= nvme_queue_rqs,\n\t.complete\t= nvme_pci_complete_rq,\n\t.commit_rqs\t= nvme_commit_rqs,\n\t.init_hctx\t= nvme_init_hctx,\n\t.init_request\t= nvme_pci_init_request,\n\t.map_queues\t= nvme_pci_map_queues,\n\t.timeout\t= nvme_timeout,\n\t.poll\t\t= nvme_poll,\n};\n\nstatic void nvme_dev_remove_admin(struct nvme_dev *dev)\n{\n\tif (dev->ctrl.admin_q && !blk_queue_dying(dev->ctrl.admin_q)) {\n\t\t \n\t\tnvme_unquiesce_admin_queue(&dev->ctrl);\n\t\tnvme_remove_admin_tag_set(&dev->ctrl);\n\t}\n}\n\nstatic unsigned long db_bar_size(struct nvme_dev *dev, unsigned nr_io_queues)\n{\n\treturn NVME_REG_DBS + ((nr_io_queues + 1) * 8 * dev->db_stride);\n}\n\nstatic int nvme_remap_bar(struct nvme_dev *dev, unsigned long size)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev->dev);\n\n\tif (size <= dev->bar_mapped_size)\n\t\treturn 0;\n\tif (size > pci_resource_len(pdev, 0))\n\t\treturn -ENOMEM;\n\tif (dev->bar)\n\t\tiounmap(dev->bar);\n\tdev->bar = ioremap(pci_resource_start(pdev, 0), size);\n\tif (!dev->bar) {\n\t\tdev->bar_mapped_size = 0;\n\t\treturn -ENOMEM;\n\t}\n\tdev->bar_mapped_size = size;\n\tdev->dbs = dev->bar + NVME_REG_DBS;\n\n\treturn 0;\n}\n\nstatic int nvme_pci_configure_admin_queue(struct nvme_dev *dev)\n{\n\tint result;\n\tu32 aqa;\n\tstruct nvme_queue *nvmeq;\n\n\tresult = nvme_remap_bar(dev, db_bar_size(dev, 0));\n\tif (result < 0)\n\t\treturn result;\n\n\tdev->subsystem = readl(dev->bar + NVME_REG_VS) >= NVME_VS(1, 1, 0) ?\n\t\t\t\tNVME_CAP_NSSRC(dev->ctrl.cap) : 0;\n\n\tif (dev->subsystem &&\n\t    (readl(dev->bar + NVME_REG_CSTS) & NVME_CSTS_NSSRO))\n\t\twritel(NVME_CSTS_NSSRO, dev->bar + NVME_REG_CSTS);\n\n\t \n\tresult = nvme_disable_ctrl(&dev->ctrl, false);\n\tif (result < 0)\n\t\treturn result;\n\n\tresult = nvme_alloc_queue(dev, 0, NVME_AQ_DEPTH);\n\tif (result)\n\t\treturn result;\n\n\tdev->ctrl.numa_node = dev_to_node(dev->dev);\n\n\tnvmeq = &dev->queues[0];\n\taqa = nvmeq->q_depth - 1;\n\taqa |= aqa << 16;\n\n\twritel(aqa, dev->bar + NVME_REG_AQA);\n\tlo_hi_writeq(nvmeq->sq_dma_addr, dev->bar + NVME_REG_ASQ);\n\tlo_hi_writeq(nvmeq->cq_dma_addr, dev->bar + NVME_REG_ACQ);\n\n\tresult = nvme_enable_ctrl(&dev->ctrl);\n\tif (result)\n\t\treturn result;\n\n\tnvmeq->cq_vector = 0;\n\tnvme_init_queue(nvmeq, 0);\n\tresult = queue_request_irq(nvmeq);\n\tif (result) {\n\t\tdev->online_queues--;\n\t\treturn result;\n\t}\n\n\tset_bit(NVMEQ_ENABLED, &nvmeq->flags);\n\treturn result;\n}\n\nstatic int nvme_create_io_queues(struct nvme_dev *dev)\n{\n\tunsigned i, max, rw_queues;\n\tint ret = 0;\n\n\tfor (i = dev->ctrl.queue_count; i <= dev->max_qid; i++) {\n\t\tif (nvme_alloc_queue(dev, i, dev->q_depth)) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tmax = min(dev->max_qid, dev->ctrl.queue_count - 1);\n\tif (max != 1 && dev->io_queues[HCTX_TYPE_POLL]) {\n\t\trw_queues = dev->io_queues[HCTX_TYPE_DEFAULT] +\n\t\t\t\tdev->io_queues[HCTX_TYPE_READ];\n\t} else {\n\t\trw_queues = max;\n\t}\n\n\tfor (i = dev->online_queues; i <= max; i++) {\n\t\tbool polled = i > rw_queues;\n\n\t\tret = nvme_create_queue(&dev->queues[i], i, polled);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\t \n\treturn ret >= 0 ? 0 : ret;\n}\n\nstatic u64 nvme_cmb_size_unit(struct nvme_dev *dev)\n{\n\tu8 szu = (dev->cmbsz >> NVME_CMBSZ_SZU_SHIFT) & NVME_CMBSZ_SZU_MASK;\n\n\treturn 1ULL << (12 + 4 * szu);\n}\n\nstatic u32 nvme_cmb_size(struct nvme_dev *dev)\n{\n\treturn (dev->cmbsz >> NVME_CMBSZ_SZ_SHIFT) & NVME_CMBSZ_SZ_MASK;\n}\n\nstatic void nvme_map_cmb(struct nvme_dev *dev)\n{\n\tu64 size, offset;\n\tresource_size_t bar_size;\n\tstruct pci_dev *pdev = to_pci_dev(dev->dev);\n\tint bar;\n\n\tif (dev->cmb_size)\n\t\treturn;\n\n\tif (NVME_CAP_CMBS(dev->ctrl.cap))\n\t\twritel(NVME_CMBMSC_CRE, dev->bar + NVME_REG_CMBMSC);\n\n\tdev->cmbsz = readl(dev->bar + NVME_REG_CMBSZ);\n\tif (!dev->cmbsz)\n\t\treturn;\n\tdev->cmbloc = readl(dev->bar + NVME_REG_CMBLOC);\n\n\tsize = nvme_cmb_size_unit(dev) * nvme_cmb_size(dev);\n\toffset = nvme_cmb_size_unit(dev) * NVME_CMB_OFST(dev->cmbloc);\n\tbar = NVME_CMB_BIR(dev->cmbloc);\n\tbar_size = pci_resource_len(pdev, bar);\n\n\tif (offset > bar_size)\n\t\treturn;\n\n\t \n\tif (NVME_CAP_CMBS(dev->ctrl.cap)) {\n\t\thi_lo_writeq(NVME_CMBMSC_CRE | NVME_CMBMSC_CMSE |\n\t\t\t     (pci_bus_address(pdev, bar) + offset),\n\t\t\t     dev->bar + NVME_REG_CMBMSC);\n\t}\n\n\t \n\tif (size > bar_size - offset)\n\t\tsize = bar_size - offset;\n\n\tif (pci_p2pdma_add_resource(pdev, bar, size, offset)) {\n\t\tdev_warn(dev->ctrl.device,\n\t\t\t \"failed to register the CMB\\n\");\n\t\treturn;\n\t}\n\n\tdev->cmb_size = size;\n\tdev->cmb_use_sqes = use_cmb_sqes && (dev->cmbsz & NVME_CMBSZ_SQS);\n\n\tif ((dev->cmbsz & (NVME_CMBSZ_WDS | NVME_CMBSZ_RDS)) ==\n\t\t\t(NVME_CMBSZ_WDS | NVME_CMBSZ_RDS))\n\t\tpci_p2pmem_publish(pdev, true);\n\n\tnvme_update_attrs(dev);\n}\n\nstatic int nvme_set_host_mem(struct nvme_dev *dev, u32 bits)\n{\n\tu32 host_mem_size = dev->host_mem_size >> NVME_CTRL_PAGE_SHIFT;\n\tu64 dma_addr = dev->host_mem_descs_dma;\n\tstruct nvme_command c = { };\n\tint ret;\n\n\tc.features.opcode\t= nvme_admin_set_features;\n\tc.features.fid\t\t= cpu_to_le32(NVME_FEAT_HOST_MEM_BUF);\n\tc.features.dword11\t= cpu_to_le32(bits);\n\tc.features.dword12\t= cpu_to_le32(host_mem_size);\n\tc.features.dword13\t= cpu_to_le32(lower_32_bits(dma_addr));\n\tc.features.dword14\t= cpu_to_le32(upper_32_bits(dma_addr));\n\tc.features.dword15\t= cpu_to_le32(dev->nr_host_mem_descs);\n\n\tret = nvme_submit_sync_cmd(dev->ctrl.admin_q, &c, NULL, 0);\n\tif (ret) {\n\t\tdev_warn(dev->ctrl.device,\n\t\t\t \"failed to set host mem (err %d, flags %#x).\\n\",\n\t\t\t ret, bits);\n\t} else\n\t\tdev->hmb = bits & NVME_HOST_MEM_ENABLE;\n\n\treturn ret;\n}\n\nstatic void nvme_free_host_mem(struct nvme_dev *dev)\n{\n\tint i;\n\n\tfor (i = 0; i < dev->nr_host_mem_descs; i++) {\n\t\tstruct nvme_host_mem_buf_desc *desc = &dev->host_mem_descs[i];\n\t\tsize_t size = le32_to_cpu(desc->size) * NVME_CTRL_PAGE_SIZE;\n\n\t\tdma_free_attrs(dev->dev, size, dev->host_mem_desc_bufs[i],\n\t\t\t       le64_to_cpu(desc->addr),\n\t\t\t       DMA_ATTR_NO_KERNEL_MAPPING | DMA_ATTR_NO_WARN);\n\t}\n\n\tkfree(dev->host_mem_desc_bufs);\n\tdev->host_mem_desc_bufs = NULL;\n\tdma_free_coherent(dev->dev,\n\t\t\tdev->nr_host_mem_descs * sizeof(*dev->host_mem_descs),\n\t\t\tdev->host_mem_descs, dev->host_mem_descs_dma);\n\tdev->host_mem_descs = NULL;\n\tdev->nr_host_mem_descs = 0;\n}\n\nstatic int __nvme_alloc_host_mem(struct nvme_dev *dev, u64 preferred,\n\t\tu32 chunk_size)\n{\n\tstruct nvme_host_mem_buf_desc *descs;\n\tu32 max_entries, len;\n\tdma_addr_t descs_dma;\n\tint i = 0;\n\tvoid **bufs;\n\tu64 size, tmp;\n\n\ttmp = (preferred + chunk_size - 1);\n\tdo_div(tmp, chunk_size);\n\tmax_entries = tmp;\n\n\tif (dev->ctrl.hmmaxd && dev->ctrl.hmmaxd < max_entries)\n\t\tmax_entries = dev->ctrl.hmmaxd;\n\n\tdescs = dma_alloc_coherent(dev->dev, max_entries * sizeof(*descs),\n\t\t\t\t   &descs_dma, GFP_KERNEL);\n\tif (!descs)\n\t\tgoto out;\n\n\tbufs = kcalloc(max_entries, sizeof(*bufs), GFP_KERNEL);\n\tif (!bufs)\n\t\tgoto out_free_descs;\n\n\tfor (size = 0; size < preferred && i < max_entries; size += len) {\n\t\tdma_addr_t dma_addr;\n\n\t\tlen = min_t(u64, chunk_size, preferred - size);\n\t\tbufs[i] = dma_alloc_attrs(dev->dev, len, &dma_addr, GFP_KERNEL,\n\t\t\t\tDMA_ATTR_NO_KERNEL_MAPPING | DMA_ATTR_NO_WARN);\n\t\tif (!bufs[i])\n\t\t\tbreak;\n\n\t\tdescs[i].addr = cpu_to_le64(dma_addr);\n\t\tdescs[i].size = cpu_to_le32(len / NVME_CTRL_PAGE_SIZE);\n\t\ti++;\n\t}\n\n\tif (!size)\n\t\tgoto out_free_bufs;\n\n\tdev->nr_host_mem_descs = i;\n\tdev->host_mem_size = size;\n\tdev->host_mem_descs = descs;\n\tdev->host_mem_descs_dma = descs_dma;\n\tdev->host_mem_desc_bufs = bufs;\n\treturn 0;\n\nout_free_bufs:\n\twhile (--i >= 0) {\n\t\tsize_t size = le32_to_cpu(descs[i].size) * NVME_CTRL_PAGE_SIZE;\n\n\t\tdma_free_attrs(dev->dev, size, bufs[i],\n\t\t\t       le64_to_cpu(descs[i].addr),\n\t\t\t       DMA_ATTR_NO_KERNEL_MAPPING | DMA_ATTR_NO_WARN);\n\t}\n\n\tkfree(bufs);\nout_free_descs:\n\tdma_free_coherent(dev->dev, max_entries * sizeof(*descs), descs,\n\t\t\tdescs_dma);\nout:\n\tdev->host_mem_descs = NULL;\n\treturn -ENOMEM;\n}\n\nstatic int nvme_alloc_host_mem(struct nvme_dev *dev, u64 min, u64 preferred)\n{\n\tu64 min_chunk = min_t(u64, preferred, PAGE_SIZE * MAX_ORDER_NR_PAGES);\n\tu64 hmminds = max_t(u32, dev->ctrl.hmminds * 4096, PAGE_SIZE * 2);\n\tu64 chunk_size;\n\n\t \n\tfor (chunk_size = min_chunk; chunk_size >= hmminds; chunk_size /= 2) {\n\t\tif (!__nvme_alloc_host_mem(dev, preferred, chunk_size)) {\n\t\t\tif (!min || dev->host_mem_size >= min)\n\t\t\t\treturn 0;\n\t\t\tnvme_free_host_mem(dev);\n\t\t}\n\t}\n\n\treturn -ENOMEM;\n}\n\nstatic int nvme_setup_host_mem(struct nvme_dev *dev)\n{\n\tu64 max = (u64)max_host_mem_size_mb * SZ_1M;\n\tu64 preferred = (u64)dev->ctrl.hmpre * 4096;\n\tu64 min = (u64)dev->ctrl.hmmin * 4096;\n\tu32 enable_bits = NVME_HOST_MEM_ENABLE;\n\tint ret;\n\n\tif (!dev->ctrl.hmpre)\n\t\treturn 0;\n\n\tpreferred = min(preferred, max);\n\tif (min > max) {\n\t\tdev_warn(dev->ctrl.device,\n\t\t\t\"min host memory (%lld MiB) above limit (%d MiB).\\n\",\n\t\t\tmin >> ilog2(SZ_1M), max_host_mem_size_mb);\n\t\tnvme_free_host_mem(dev);\n\t\treturn 0;\n\t}\n\n\t \n\tif (dev->host_mem_descs) {\n\t\tif (dev->host_mem_size >= min)\n\t\t\tenable_bits |= NVME_HOST_MEM_RETURN;\n\t\telse\n\t\t\tnvme_free_host_mem(dev);\n\t}\n\n\tif (!dev->host_mem_descs) {\n\t\tif (nvme_alloc_host_mem(dev, min, preferred)) {\n\t\t\tdev_warn(dev->ctrl.device,\n\t\t\t\t\"failed to allocate host memory buffer.\\n\");\n\t\t\treturn 0;  \n\t\t}\n\n\t\tdev_info(dev->ctrl.device,\n\t\t\t\"allocated %lld MiB host memory buffer.\\n\",\n\t\t\tdev->host_mem_size >> ilog2(SZ_1M));\n\t}\n\n\tret = nvme_set_host_mem(dev, enable_bits);\n\tif (ret)\n\t\tnvme_free_host_mem(dev);\n\treturn ret;\n}\n\nstatic ssize_t cmb_show(struct device *dev, struct device_attribute *attr,\n\t\tchar *buf)\n{\n\tstruct nvme_dev *ndev = to_nvme_dev(dev_get_drvdata(dev));\n\n\treturn sysfs_emit(buf, \"cmbloc : x%08x\\ncmbsz  : x%08x\\n\",\n\t\t       ndev->cmbloc, ndev->cmbsz);\n}\nstatic DEVICE_ATTR_RO(cmb);\n\nstatic ssize_t cmbloc_show(struct device *dev, struct device_attribute *attr,\n\t\tchar *buf)\n{\n\tstruct nvme_dev *ndev = to_nvme_dev(dev_get_drvdata(dev));\n\n\treturn sysfs_emit(buf, \"%u\\n\", ndev->cmbloc);\n}\nstatic DEVICE_ATTR_RO(cmbloc);\n\nstatic ssize_t cmbsz_show(struct device *dev, struct device_attribute *attr,\n\t\tchar *buf)\n{\n\tstruct nvme_dev *ndev = to_nvme_dev(dev_get_drvdata(dev));\n\n\treturn sysfs_emit(buf, \"%u\\n\", ndev->cmbsz);\n}\nstatic DEVICE_ATTR_RO(cmbsz);\n\nstatic ssize_t hmb_show(struct device *dev, struct device_attribute *attr,\n\t\t\tchar *buf)\n{\n\tstruct nvme_dev *ndev = to_nvme_dev(dev_get_drvdata(dev));\n\n\treturn sysfs_emit(buf, \"%d\\n\", ndev->hmb);\n}\n\nstatic ssize_t hmb_store(struct device *dev, struct device_attribute *attr,\n\t\t\t const char *buf, size_t count)\n{\n\tstruct nvme_dev *ndev = to_nvme_dev(dev_get_drvdata(dev));\n\tbool new;\n\tint ret;\n\n\tif (kstrtobool(buf, &new) < 0)\n\t\treturn -EINVAL;\n\n\tif (new == ndev->hmb)\n\t\treturn count;\n\n\tif (new) {\n\t\tret = nvme_setup_host_mem(ndev);\n\t} else {\n\t\tret = nvme_set_host_mem(ndev, 0);\n\t\tif (!ret)\n\t\t\tnvme_free_host_mem(ndev);\n\t}\n\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn count;\n}\nstatic DEVICE_ATTR_RW(hmb);\n\nstatic umode_t nvme_pci_attrs_are_visible(struct kobject *kobj,\n\t\tstruct attribute *a, int n)\n{\n\tstruct nvme_ctrl *ctrl =\n\t\tdev_get_drvdata(container_of(kobj, struct device, kobj));\n\tstruct nvme_dev *dev = to_nvme_dev(ctrl);\n\n\tif (a == &dev_attr_cmb.attr ||\n\t    a == &dev_attr_cmbloc.attr ||\n\t    a == &dev_attr_cmbsz.attr) {\n\t    \tif (!dev->cmbsz)\n\t\t\treturn 0;\n\t}\n\tif (a == &dev_attr_hmb.attr && !ctrl->hmpre)\n\t\treturn 0;\n\n\treturn a->mode;\n}\n\nstatic struct attribute *nvme_pci_attrs[] = {\n\t&dev_attr_cmb.attr,\n\t&dev_attr_cmbloc.attr,\n\t&dev_attr_cmbsz.attr,\n\t&dev_attr_hmb.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group nvme_pci_dev_attrs_group = {\n\t.attrs\t\t= nvme_pci_attrs,\n\t.is_visible\t= nvme_pci_attrs_are_visible,\n};\n\nstatic const struct attribute_group *nvme_pci_dev_attr_groups[] = {\n\t&nvme_dev_attrs_group,\n\t&nvme_pci_dev_attrs_group,\n\tNULL,\n};\n\nstatic void nvme_update_attrs(struct nvme_dev *dev)\n{\n\tsysfs_update_group(&dev->ctrl.device->kobj, &nvme_pci_dev_attrs_group);\n}\n\n \nstatic void nvme_calc_irq_sets(struct irq_affinity *affd, unsigned int nrirqs)\n{\n\tstruct nvme_dev *dev = affd->priv;\n\tunsigned int nr_read_queues, nr_write_queues = dev->nr_write_queues;\n\n\t \n\tif (!nrirqs) {\n\t\tnrirqs = 1;\n\t\tnr_read_queues = 0;\n\t} else if (nrirqs == 1 || !nr_write_queues) {\n\t\tnr_read_queues = 0;\n\t} else if (nr_write_queues >= nrirqs) {\n\t\tnr_read_queues = 1;\n\t} else {\n\t\tnr_read_queues = nrirqs - nr_write_queues;\n\t}\n\n\tdev->io_queues[HCTX_TYPE_DEFAULT] = nrirqs - nr_read_queues;\n\taffd->set_size[HCTX_TYPE_DEFAULT] = nrirqs - nr_read_queues;\n\tdev->io_queues[HCTX_TYPE_READ] = nr_read_queues;\n\taffd->set_size[HCTX_TYPE_READ] = nr_read_queues;\n\taffd->nr_sets = nr_read_queues ? 2 : 1;\n}\n\nstatic int nvme_setup_irqs(struct nvme_dev *dev, unsigned int nr_io_queues)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev->dev);\n\tstruct irq_affinity affd = {\n\t\t.pre_vectors\t= 1,\n\t\t.calc_sets\t= nvme_calc_irq_sets,\n\t\t.priv\t\t= dev,\n\t};\n\tunsigned int irq_queues, poll_queues;\n\n\t \n\tpoll_queues = min(dev->nr_poll_queues, nr_io_queues - 1);\n\tdev->io_queues[HCTX_TYPE_POLL] = poll_queues;\n\n\t \n\tdev->io_queues[HCTX_TYPE_DEFAULT] = 1;\n\tdev->io_queues[HCTX_TYPE_READ] = 0;\n\n\t \n\tirq_queues = 1;\n\tif (!(dev->ctrl.quirks & NVME_QUIRK_SINGLE_VECTOR))\n\t\tirq_queues += (nr_io_queues - poll_queues);\n\treturn pci_alloc_irq_vectors_affinity(pdev, 1, irq_queues,\n\t\t\t      PCI_IRQ_ALL_TYPES | PCI_IRQ_AFFINITY, &affd);\n}\n\nstatic unsigned int nvme_max_io_queues(struct nvme_dev *dev)\n{\n\t \n\tif (dev->ctrl.quirks & NVME_QUIRK_SHARED_TAGS)\n\t\treturn 1;\n\treturn num_possible_cpus() + dev->nr_write_queues + dev->nr_poll_queues;\n}\n\nstatic int nvme_setup_io_queues(struct nvme_dev *dev)\n{\n\tstruct nvme_queue *adminq = &dev->queues[0];\n\tstruct pci_dev *pdev = to_pci_dev(dev->dev);\n\tunsigned int nr_io_queues;\n\tunsigned long size;\n\tint result;\n\n\t \n\tdev->nr_write_queues = write_queues;\n\tdev->nr_poll_queues = poll_queues;\n\n\tnr_io_queues = dev->nr_allocated_queues - 1;\n\tresult = nvme_set_queue_count(&dev->ctrl, &nr_io_queues);\n\tif (result < 0)\n\t\treturn result;\n\n\tif (nr_io_queues == 0)\n\t\treturn 0;\n\n\t \n\tresult = nvme_setup_io_queues_trylock(dev);\n\tif (result)\n\t\treturn result;\n\tif (test_and_clear_bit(NVMEQ_ENABLED, &adminq->flags))\n\t\tpci_free_irq(pdev, 0, adminq);\n\n\tif (dev->cmb_use_sqes) {\n\t\tresult = nvme_cmb_qdepth(dev, nr_io_queues,\n\t\t\t\tsizeof(struct nvme_command));\n\t\tif (result > 0) {\n\t\t\tdev->q_depth = result;\n\t\t\tdev->ctrl.sqsize = result - 1;\n\t\t} else {\n\t\t\tdev->cmb_use_sqes = false;\n\t\t}\n\t}\n\n\tdo {\n\t\tsize = db_bar_size(dev, nr_io_queues);\n\t\tresult = nvme_remap_bar(dev, size);\n\t\tif (!result)\n\t\t\tbreak;\n\t\tif (!--nr_io_queues) {\n\t\t\tresult = -ENOMEM;\n\t\t\tgoto out_unlock;\n\t\t}\n\t} while (1);\n\tadminq->q_db = dev->dbs;\n\n retry:\n\t \n\tif (test_and_clear_bit(NVMEQ_ENABLED, &adminq->flags))\n\t\tpci_free_irq(pdev, 0, adminq);\n\n\t \n\tpci_free_irq_vectors(pdev);\n\n\tresult = nvme_setup_irqs(dev, nr_io_queues);\n\tif (result <= 0) {\n\t\tresult = -EIO;\n\t\tgoto out_unlock;\n\t}\n\n\tdev->num_vecs = result;\n\tresult = max(result - 1, 1);\n\tdev->max_qid = result + dev->io_queues[HCTX_TYPE_POLL];\n\n\t \n\tresult = queue_request_irq(adminq);\n\tif (result)\n\t\tgoto out_unlock;\n\tset_bit(NVMEQ_ENABLED, &adminq->flags);\n\tmutex_unlock(&dev->shutdown_lock);\n\n\tresult = nvme_create_io_queues(dev);\n\tif (result || dev->online_queues < 2)\n\t\treturn result;\n\n\tif (dev->online_queues - 1 < dev->max_qid) {\n\t\tnr_io_queues = dev->online_queues - 1;\n\t\tnvme_delete_io_queues(dev);\n\t\tresult = nvme_setup_io_queues_trylock(dev);\n\t\tif (result)\n\t\t\treturn result;\n\t\tnvme_suspend_io_queues(dev);\n\t\tgoto retry;\n\t}\n\tdev_info(dev->ctrl.device, \"%d/%d/%d default/read/poll queues\\n\",\n\t\t\t\t\tdev->io_queues[HCTX_TYPE_DEFAULT],\n\t\t\t\t\tdev->io_queues[HCTX_TYPE_READ],\n\t\t\t\t\tdev->io_queues[HCTX_TYPE_POLL]);\n\treturn 0;\nout_unlock:\n\tmutex_unlock(&dev->shutdown_lock);\n\treturn result;\n}\n\nstatic enum rq_end_io_ret nvme_del_queue_end(struct request *req,\n\t\t\t\t\t     blk_status_t error)\n{\n\tstruct nvme_queue *nvmeq = req->end_io_data;\n\n\tblk_mq_free_request(req);\n\tcomplete(&nvmeq->delete_done);\n\treturn RQ_END_IO_NONE;\n}\n\nstatic enum rq_end_io_ret nvme_del_cq_end(struct request *req,\n\t\t\t\t\t  blk_status_t error)\n{\n\tstruct nvme_queue *nvmeq = req->end_io_data;\n\n\tif (error)\n\t\tset_bit(NVMEQ_DELETE_ERROR, &nvmeq->flags);\n\n\treturn nvme_del_queue_end(req, error);\n}\n\nstatic int nvme_delete_queue(struct nvme_queue *nvmeq, u8 opcode)\n{\n\tstruct request_queue *q = nvmeq->dev->ctrl.admin_q;\n\tstruct request *req;\n\tstruct nvme_command cmd = { };\n\n\tcmd.delete_queue.opcode = opcode;\n\tcmd.delete_queue.qid = cpu_to_le16(nvmeq->qid);\n\n\treq = blk_mq_alloc_request(q, nvme_req_op(&cmd), BLK_MQ_REQ_NOWAIT);\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\tnvme_init_request(req, &cmd);\n\n\tif (opcode == nvme_admin_delete_cq)\n\t\treq->end_io = nvme_del_cq_end;\n\telse\n\t\treq->end_io = nvme_del_queue_end;\n\treq->end_io_data = nvmeq;\n\n\tinit_completion(&nvmeq->delete_done);\n\tblk_execute_rq_nowait(req, false);\n\treturn 0;\n}\n\nstatic bool __nvme_delete_io_queues(struct nvme_dev *dev, u8 opcode)\n{\n\tint nr_queues = dev->online_queues - 1, sent = 0;\n\tunsigned long timeout;\n\n retry:\n\ttimeout = NVME_ADMIN_TIMEOUT;\n\twhile (nr_queues > 0) {\n\t\tif (nvme_delete_queue(&dev->queues[nr_queues], opcode))\n\t\t\tbreak;\n\t\tnr_queues--;\n\t\tsent++;\n\t}\n\twhile (sent) {\n\t\tstruct nvme_queue *nvmeq = &dev->queues[nr_queues + sent];\n\n\t\ttimeout = wait_for_completion_io_timeout(&nvmeq->delete_done,\n\t\t\t\ttimeout);\n\t\tif (timeout == 0)\n\t\t\treturn false;\n\n\t\tsent--;\n\t\tif (nr_queues)\n\t\t\tgoto retry;\n\t}\n\treturn true;\n}\n\nstatic void nvme_delete_io_queues(struct nvme_dev *dev)\n{\n\tif (__nvme_delete_io_queues(dev, nvme_admin_delete_sq))\n\t\t__nvme_delete_io_queues(dev, nvme_admin_delete_cq);\n}\n\nstatic unsigned int nvme_pci_nr_maps(struct nvme_dev *dev)\n{\n\tif (dev->io_queues[HCTX_TYPE_POLL])\n\t\treturn 3;\n\tif (dev->io_queues[HCTX_TYPE_READ])\n\t\treturn 2;\n\treturn 1;\n}\n\nstatic void nvme_pci_update_nr_queues(struct nvme_dev *dev)\n{\n\tblk_mq_update_nr_hw_queues(&dev->tagset, dev->online_queues - 1);\n\t \n\tnvme_free_queues(dev, dev->online_queues);\n}\n\nstatic int nvme_pci_enable(struct nvme_dev *dev)\n{\n\tint result = -ENOMEM;\n\tstruct pci_dev *pdev = to_pci_dev(dev->dev);\n\n\tif (pci_enable_device_mem(pdev))\n\t\treturn result;\n\n\tpci_set_master(pdev);\n\n\tif (readl(dev->bar + NVME_REG_CSTS) == -1) {\n\t\tresult = -ENODEV;\n\t\tgoto disable;\n\t}\n\n\t \n\tresult = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_ALL_TYPES);\n\tif (result < 0)\n\t\tgoto disable;\n\n\tdev->ctrl.cap = lo_hi_readq(dev->bar + NVME_REG_CAP);\n\n\tdev->q_depth = min_t(u32, NVME_CAP_MQES(dev->ctrl.cap) + 1,\n\t\t\t\tio_queue_depth);\n\tdev->db_stride = 1 << NVME_CAP_STRIDE(dev->ctrl.cap);\n\tdev->dbs = dev->bar + 4096;\n\n\t \n\tif (dev->ctrl.quirks & NVME_QUIRK_128_BYTES_SQES)\n\t\tdev->io_sqes = 7;\n\telse\n\t\tdev->io_sqes = NVME_NVM_IOSQES;\n\n\t \n\tif (pdev->vendor == PCI_VENDOR_ID_APPLE && pdev->device == 0x2001) {\n\t\tdev->q_depth = 2;\n\t\tdev_warn(dev->ctrl.device, \"detected Apple NVMe controller, \"\n\t\t\t\"set queue depth=%u to work around controller resets\\n\",\n\t\t\tdev->q_depth);\n\t} else if (pdev->vendor == PCI_VENDOR_ID_SAMSUNG &&\n\t\t   (pdev->device == 0xa821 || pdev->device == 0xa822) &&\n\t\t   NVME_CAP_MQES(dev->ctrl.cap) == 0) {\n\t\tdev->q_depth = 64;\n\t\tdev_err(dev->ctrl.device, \"detected PM1725 NVMe controller, \"\n                        \"set queue depth=%u\\n\", dev->q_depth);\n\t}\n\n\t \n\tif ((dev->ctrl.quirks & NVME_QUIRK_SHARED_TAGS) &&\n\t    (dev->q_depth < (NVME_AQ_DEPTH + 2))) {\n\t\tdev->q_depth = NVME_AQ_DEPTH + 2;\n\t\tdev_warn(dev->ctrl.device, \"IO queue depth clamped to %d\\n\",\n\t\t\t dev->q_depth);\n\t}\n\tdev->ctrl.sqsize = dev->q_depth - 1;  \n\n\tnvme_map_cmb(dev);\n\n\tpci_save_state(pdev);\n\n\tresult = nvme_pci_configure_admin_queue(dev);\n\tif (result)\n\t\tgoto free_irq;\n\treturn result;\n\n free_irq:\n\tpci_free_irq_vectors(pdev);\n disable:\n\tpci_disable_device(pdev);\n\treturn result;\n}\n\nstatic void nvme_dev_unmap(struct nvme_dev *dev)\n{\n\tif (dev->bar)\n\t\tiounmap(dev->bar);\n\tpci_release_mem_regions(to_pci_dev(dev->dev));\n}\n\nstatic bool nvme_pci_ctrl_is_dead(struct nvme_dev *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev->dev);\n\tu32 csts;\n\n\tif (!pci_is_enabled(pdev) || !pci_device_is_present(pdev))\n\t\treturn true;\n\tif (pdev->error_state != pci_channel_io_normal)\n\t\treturn true;\n\n\tcsts = readl(dev->bar + NVME_REG_CSTS);\n\treturn (csts & NVME_CSTS_CFS) || !(csts & NVME_CSTS_RDY);\n}\n\nstatic void nvme_dev_disable(struct nvme_dev *dev, bool shutdown)\n{\n\tenum nvme_ctrl_state state = nvme_ctrl_state(&dev->ctrl);\n\tstruct pci_dev *pdev = to_pci_dev(dev->dev);\n\tbool dead;\n\n\tmutex_lock(&dev->shutdown_lock);\n\tdead = nvme_pci_ctrl_is_dead(dev);\n\tif (state == NVME_CTRL_LIVE || state == NVME_CTRL_RESETTING) {\n\t\tif (pci_is_enabled(pdev))\n\t\t\tnvme_start_freeze(&dev->ctrl);\n\t\t \n\t\tif (!dead && shutdown)\n\t\t\tnvme_wait_freeze_timeout(&dev->ctrl, NVME_IO_TIMEOUT);\n\t}\n\n\tnvme_quiesce_io_queues(&dev->ctrl);\n\n\tif (!dead && dev->ctrl.queue_count > 0) {\n\t\tnvme_delete_io_queues(dev);\n\t\tnvme_disable_ctrl(&dev->ctrl, shutdown);\n\t\tnvme_poll_irqdisable(&dev->queues[0]);\n\t}\n\tnvme_suspend_io_queues(dev);\n\tnvme_suspend_queue(dev, 0);\n\tpci_free_irq_vectors(pdev);\n\tif (pci_is_enabled(pdev))\n\t\tpci_disable_device(pdev);\n\tnvme_reap_pending_cqes(dev);\n\n\tnvme_cancel_tagset(&dev->ctrl);\n\tnvme_cancel_admin_tagset(&dev->ctrl);\n\n\t \n\tif (shutdown) {\n\t\tnvme_unquiesce_io_queues(&dev->ctrl);\n\t\tif (dev->ctrl.admin_q && !blk_queue_dying(dev->ctrl.admin_q))\n\t\t\tnvme_unquiesce_admin_queue(&dev->ctrl);\n\t}\n\tmutex_unlock(&dev->shutdown_lock);\n}\n\nstatic int nvme_disable_prepare_reset(struct nvme_dev *dev, bool shutdown)\n{\n\tif (!nvme_wait_reset(&dev->ctrl))\n\t\treturn -EBUSY;\n\tnvme_dev_disable(dev, shutdown);\n\treturn 0;\n}\n\nstatic int nvme_setup_prp_pools(struct nvme_dev *dev)\n{\n\tdev->prp_page_pool = dma_pool_create(\"prp list page\", dev->dev,\n\t\t\t\t\t\tNVME_CTRL_PAGE_SIZE,\n\t\t\t\t\t\tNVME_CTRL_PAGE_SIZE, 0);\n\tif (!dev->prp_page_pool)\n\t\treturn -ENOMEM;\n\n\t \n\tdev->prp_small_pool = dma_pool_create(\"prp list 256\", dev->dev,\n\t\t\t\t\t\t256, 256, 0);\n\tif (!dev->prp_small_pool) {\n\t\tdma_pool_destroy(dev->prp_page_pool);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic void nvme_release_prp_pools(struct nvme_dev *dev)\n{\n\tdma_pool_destroy(dev->prp_page_pool);\n\tdma_pool_destroy(dev->prp_small_pool);\n}\n\nstatic int nvme_pci_alloc_iod_mempool(struct nvme_dev *dev)\n{\n\tsize_t alloc_size = sizeof(struct scatterlist) * NVME_MAX_SEGS;\n\n\tdev->iod_mempool = mempool_create_node(1,\n\t\t\tmempool_kmalloc, mempool_kfree,\n\t\t\t(void *)alloc_size, GFP_KERNEL,\n\t\t\tdev_to_node(dev->dev));\n\tif (!dev->iod_mempool)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void nvme_free_tagset(struct nvme_dev *dev)\n{\n\tif (dev->tagset.tags)\n\t\tnvme_remove_io_tag_set(&dev->ctrl);\n\tdev->ctrl.tagset = NULL;\n}\n\n \nstatic void nvme_pci_free_ctrl(struct nvme_ctrl *ctrl)\n{\n\tstruct nvme_dev *dev = to_nvme_dev(ctrl);\n\n\tnvme_free_tagset(dev);\n\tput_device(dev->dev);\n\tkfree(dev->queues);\n\tkfree(dev);\n}\n\nstatic void nvme_reset_work(struct work_struct *work)\n{\n\tstruct nvme_dev *dev =\n\t\tcontainer_of(work, struct nvme_dev, ctrl.reset_work);\n\tbool was_suspend = !!(dev->ctrl.ctrl_config & NVME_CC_SHN_NORMAL);\n\tint result;\n\n\tif (nvme_ctrl_state(&dev->ctrl) != NVME_CTRL_RESETTING) {\n\t\tdev_warn(dev->ctrl.device, \"ctrl state %d is not RESETTING\\n\",\n\t\t\t dev->ctrl.state);\n\t\tresult = -ENODEV;\n\t\tgoto out;\n\t}\n\n\t \n\tif (dev->ctrl.ctrl_config & NVME_CC_ENABLE)\n\t\tnvme_dev_disable(dev, false);\n\tnvme_sync_queues(&dev->ctrl);\n\n\tmutex_lock(&dev->shutdown_lock);\n\tresult = nvme_pci_enable(dev);\n\tif (result)\n\t\tgoto out_unlock;\n\tnvme_unquiesce_admin_queue(&dev->ctrl);\n\tmutex_unlock(&dev->shutdown_lock);\n\n\t \n\tif (!nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_CONNECTING)) {\n\t\tdev_warn(dev->ctrl.device,\n\t\t\t\"failed to mark controller CONNECTING\\n\");\n\t\tresult = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tresult = nvme_init_ctrl_finish(&dev->ctrl, was_suspend);\n\tif (result)\n\t\tgoto out;\n\n\tnvme_dbbuf_dma_alloc(dev);\n\n\tresult = nvme_setup_host_mem(dev);\n\tif (result < 0)\n\t\tgoto out;\n\n\tresult = nvme_setup_io_queues(dev);\n\tif (result)\n\t\tgoto out;\n\n\t \n\tif (dev->online_queues > 1) {\n\t\tnvme_unquiesce_io_queues(&dev->ctrl);\n\t\tnvme_wait_freeze(&dev->ctrl);\n\t\tnvme_pci_update_nr_queues(dev);\n\t\tnvme_dbbuf_set(dev);\n\t\tnvme_unfreeze(&dev->ctrl);\n\t} else {\n\t\tdev_warn(dev->ctrl.device, \"IO queues lost\\n\");\n\t\tnvme_mark_namespaces_dead(&dev->ctrl);\n\t\tnvme_unquiesce_io_queues(&dev->ctrl);\n\t\tnvme_remove_namespaces(&dev->ctrl);\n\t\tnvme_free_tagset(dev);\n\t}\n\n\t \n\tif (!nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_LIVE)) {\n\t\tdev_warn(dev->ctrl.device,\n\t\t\t\"failed to mark controller live state\\n\");\n\t\tresult = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tnvme_start_ctrl(&dev->ctrl);\n\treturn;\n\n out_unlock:\n\tmutex_unlock(&dev->shutdown_lock);\n out:\n\t \n\tdev_warn(dev->ctrl.device, \"Disabling device after reset failure: %d\\n\",\n\t\t result);\n\tnvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DELETING);\n\tnvme_dev_disable(dev, true);\n\tnvme_sync_queues(&dev->ctrl);\n\tnvme_mark_namespaces_dead(&dev->ctrl);\n\tnvme_unquiesce_io_queues(&dev->ctrl);\n\tnvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DEAD);\n}\n\nstatic int nvme_pci_reg_read32(struct nvme_ctrl *ctrl, u32 off, u32 *val)\n{\n\t*val = readl(to_nvme_dev(ctrl)->bar + off);\n\treturn 0;\n}\n\nstatic int nvme_pci_reg_write32(struct nvme_ctrl *ctrl, u32 off, u32 val)\n{\n\twritel(val, to_nvme_dev(ctrl)->bar + off);\n\treturn 0;\n}\n\nstatic int nvme_pci_reg_read64(struct nvme_ctrl *ctrl, u32 off, u64 *val)\n{\n\t*val = lo_hi_readq(to_nvme_dev(ctrl)->bar + off);\n\treturn 0;\n}\n\nstatic int nvme_pci_get_address(struct nvme_ctrl *ctrl, char *buf, int size)\n{\n\tstruct pci_dev *pdev = to_pci_dev(to_nvme_dev(ctrl)->dev);\n\n\treturn snprintf(buf, size, \"%s\\n\", dev_name(&pdev->dev));\n}\n\nstatic void nvme_pci_print_device_info(struct nvme_ctrl *ctrl)\n{\n\tstruct pci_dev *pdev = to_pci_dev(to_nvme_dev(ctrl)->dev);\n\tstruct nvme_subsystem *subsys = ctrl->subsys;\n\n\tdev_err(ctrl->device,\n\t\t\"VID:DID %04x:%04x model:%.*s firmware:%.*s\\n\",\n\t\tpdev->vendor, pdev->device,\n\t\tnvme_strlen(subsys->model, sizeof(subsys->model)),\n\t\tsubsys->model, nvme_strlen(subsys->firmware_rev,\n\t\t\t\t\t   sizeof(subsys->firmware_rev)),\n\t\tsubsys->firmware_rev);\n}\n\nstatic bool nvme_pci_supports_pci_p2pdma(struct nvme_ctrl *ctrl)\n{\n\tstruct nvme_dev *dev = to_nvme_dev(ctrl);\n\n\treturn dma_pci_p2pdma_supported(dev->dev);\n}\n\nstatic const struct nvme_ctrl_ops nvme_pci_ctrl_ops = {\n\t.name\t\t\t= \"pcie\",\n\t.module\t\t\t= THIS_MODULE,\n\t.flags\t\t\t= NVME_F_METADATA_SUPPORTED,\n\t.dev_attr_groups\t= nvme_pci_dev_attr_groups,\n\t.reg_read32\t\t= nvme_pci_reg_read32,\n\t.reg_write32\t\t= nvme_pci_reg_write32,\n\t.reg_read64\t\t= nvme_pci_reg_read64,\n\t.free_ctrl\t\t= nvme_pci_free_ctrl,\n\t.submit_async_event\t= nvme_pci_submit_async_event,\n\t.get_address\t\t= nvme_pci_get_address,\n\t.print_device_info\t= nvme_pci_print_device_info,\n\t.supports_pci_p2pdma\t= nvme_pci_supports_pci_p2pdma,\n};\n\nstatic int nvme_dev_map(struct nvme_dev *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev->dev);\n\n\tif (pci_request_mem_regions(pdev, \"nvme\"))\n\t\treturn -ENODEV;\n\n\tif (nvme_remap_bar(dev, NVME_REG_DBS + 4096))\n\t\tgoto release;\n\n\treturn 0;\n  release:\n\tpci_release_mem_regions(pdev);\n\treturn -ENODEV;\n}\n\nstatic unsigned long check_vendor_combination_bug(struct pci_dev *pdev)\n{\n\tif (pdev->vendor == 0x144d && pdev->device == 0xa802) {\n\t\t \n\t\tif (dmi_match(DMI_SYS_VENDOR, \"Dell Inc.\") &&\n\t\t    (dmi_match(DMI_PRODUCT_NAME, \"XPS 15 9550\") ||\n\t\t     dmi_match(DMI_PRODUCT_NAME, \"Precision 5510\")))\n\t\t\treturn NVME_QUIRK_NO_DEEPEST_PS;\n\t} else if (pdev->vendor == 0x144d && pdev->device == 0xa804) {\n\t\t \n\t\tif (dmi_match(DMI_BOARD_VENDOR, \"ASUSTeK COMPUTER INC.\") &&\n\t\t    (dmi_match(DMI_BOARD_NAME, \"PRIME B350M-A\") ||\n\t\t     dmi_match(DMI_BOARD_NAME, \"PRIME Z370-A\")))\n\t\t\treturn NVME_QUIRK_NO_APST;\n\t} else if ((pdev->vendor == 0x144d && (pdev->device == 0xa801 ||\n\t\t    pdev->device == 0xa808 || pdev->device == 0xa809)) ||\n\t\t   (pdev->vendor == 0x1e0f && pdev->device == 0x0001)) {\n\t\t \n\t\tif ((dmi_match(DMI_BOARD_VENDOR, \"LENOVO\")) &&\n\t\t     dmi_match(DMI_BOARD_NAME, \"LNVNB161216\"))\n\t\t\treturn NVME_QUIRK_SIMPLE_SUSPEND;\n\t} else if (pdev->vendor == 0x2646 && (pdev->device == 0x2263 ||\n\t\t   pdev->device == 0x500f)) {\n\t\t \n\t\tif (dmi_match(DMI_BOARD_NAME, \"NS5X_NS7XAU\") ||\n\t\t    dmi_match(DMI_BOARD_NAME, \"NS5x_7xAU\") ||\n\t\t    dmi_match(DMI_BOARD_NAME, \"NS5x_7xPU\") ||\n\t\t    dmi_match(DMI_BOARD_NAME, \"PH4PRX1_PH6PRX1\"))\n\t\t\treturn NVME_QUIRK_FORCE_NO_SIMPLE_SUSPEND;\n\t}\n\n\treturn 0;\n}\n\nstatic struct nvme_dev *nvme_pci_alloc_dev(struct pci_dev *pdev,\n\t\tconst struct pci_device_id *id)\n{\n\tunsigned long quirks = id->driver_data;\n\tint node = dev_to_node(&pdev->dev);\n\tstruct nvme_dev *dev;\n\tint ret = -ENOMEM;\n\n\tdev = kzalloc_node(sizeof(*dev), GFP_KERNEL, node);\n\tif (!dev)\n\t\treturn ERR_PTR(-ENOMEM);\n\tINIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);\n\tmutex_init(&dev->shutdown_lock);\n\n\tdev->nr_write_queues = write_queues;\n\tdev->nr_poll_queues = poll_queues;\n\tdev->nr_allocated_queues = nvme_max_io_queues(dev) + 1;\n\tdev->queues = kcalloc_node(dev->nr_allocated_queues,\n\t\t\tsizeof(struct nvme_queue), GFP_KERNEL, node);\n\tif (!dev->queues)\n\t\tgoto out_free_dev;\n\n\tdev->dev = get_device(&pdev->dev);\n\n\tquirks |= check_vendor_combination_bug(pdev);\n\tif (!noacpi &&\n\t    !(quirks & NVME_QUIRK_FORCE_NO_SIMPLE_SUSPEND) &&\n\t    acpi_storage_d3(&pdev->dev)) {\n\t\t \n\t\tdev_info(&pdev->dev,\n\t\t\t \"platform quirk: setting simple suspend\\n\");\n\t\tquirks |= NVME_QUIRK_SIMPLE_SUSPEND;\n\t}\n\tret = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,\n\t\t\t     quirks);\n\tif (ret)\n\t\tgoto out_put_device;\n\n\tif (dev->ctrl.quirks & NVME_QUIRK_DMA_ADDRESS_BITS_48)\n\t\tdma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(48));\n\telse\n\t\tdma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tdma_set_min_align_mask(&pdev->dev, NVME_CTRL_PAGE_SIZE - 1);\n\tdma_set_max_seg_size(&pdev->dev, 0xffffffff);\n\n\t \n\tdev->ctrl.max_hw_sectors = min_t(u32,\n\t\tNVME_MAX_KB_SZ << 1, dma_opt_mapping_size(&pdev->dev) >> 9);\n\tdev->ctrl.max_segments = NVME_MAX_SEGS;\n\n\t \n\tdev->ctrl.max_integrity_segments = 1;\n\treturn dev;\n\nout_put_device:\n\tput_device(dev->dev);\n\tkfree(dev->queues);\nout_free_dev:\n\tkfree(dev);\n\treturn ERR_PTR(ret);\n}\n\nstatic int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct nvme_dev *dev;\n\tint result = -ENOMEM;\n\n\tdev = nvme_pci_alloc_dev(pdev, id);\n\tif (IS_ERR(dev))\n\t\treturn PTR_ERR(dev);\n\n\tresult = nvme_dev_map(dev);\n\tif (result)\n\t\tgoto out_uninit_ctrl;\n\n\tresult = nvme_setup_prp_pools(dev);\n\tif (result)\n\t\tgoto out_dev_unmap;\n\n\tresult = nvme_pci_alloc_iod_mempool(dev);\n\tif (result)\n\t\tgoto out_release_prp_pools;\n\n\tdev_info(dev->ctrl.device, \"pci function %s\\n\", dev_name(&pdev->dev));\n\n\tresult = nvme_pci_enable(dev);\n\tif (result)\n\t\tgoto out_release_iod_mempool;\n\n\tresult = nvme_alloc_admin_tag_set(&dev->ctrl, &dev->admin_tagset,\n\t\t\t\t&nvme_mq_admin_ops, sizeof(struct nvme_iod));\n\tif (result)\n\t\tgoto out_disable;\n\n\t \n\tif (!nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_CONNECTING)) {\n\t\tdev_warn(dev->ctrl.device,\n\t\t\t\"failed to mark controller CONNECTING\\n\");\n\t\tresult = -EBUSY;\n\t\tgoto out_disable;\n\t}\n\n\tresult = nvme_init_ctrl_finish(&dev->ctrl, false);\n\tif (result)\n\t\tgoto out_disable;\n\n\tnvme_dbbuf_dma_alloc(dev);\n\n\tresult = nvme_setup_host_mem(dev);\n\tif (result < 0)\n\t\tgoto out_disable;\n\n\tresult = nvme_setup_io_queues(dev);\n\tif (result)\n\t\tgoto out_disable;\n\n\tif (dev->online_queues > 1) {\n\t\tnvme_alloc_io_tag_set(&dev->ctrl, &dev->tagset, &nvme_mq_ops,\n\t\t\t\tnvme_pci_nr_maps(dev), sizeof(struct nvme_iod));\n\t\tnvme_dbbuf_set(dev);\n\t}\n\n\tif (!dev->ctrl.tagset)\n\t\tdev_warn(dev->ctrl.device, \"IO queues not created\\n\");\n\n\tif (!nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_LIVE)) {\n\t\tdev_warn(dev->ctrl.device,\n\t\t\t\"failed to mark controller live state\\n\");\n\t\tresult = -ENODEV;\n\t\tgoto out_disable;\n\t}\n\n\tpci_set_drvdata(pdev, dev);\n\n\tnvme_start_ctrl(&dev->ctrl);\n\tnvme_put_ctrl(&dev->ctrl);\n\tflush_work(&dev->ctrl.scan_work);\n\treturn 0;\n\nout_disable:\n\tnvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DELETING);\n\tnvme_dev_disable(dev, true);\n\tnvme_free_host_mem(dev);\n\tnvme_dev_remove_admin(dev);\n\tnvme_dbbuf_dma_free(dev);\n\tnvme_free_queues(dev, 0);\nout_release_iod_mempool:\n\tmempool_destroy(dev->iod_mempool);\nout_release_prp_pools:\n\tnvme_release_prp_pools(dev);\nout_dev_unmap:\n\tnvme_dev_unmap(dev);\nout_uninit_ctrl:\n\tnvme_uninit_ctrl(&dev->ctrl);\n\tnvme_put_ctrl(&dev->ctrl);\n\treturn result;\n}\n\nstatic void nvme_reset_prepare(struct pci_dev *pdev)\n{\n\tstruct nvme_dev *dev = pci_get_drvdata(pdev);\n\n\t \n\tnvme_disable_prepare_reset(dev, false);\n\tnvme_sync_queues(&dev->ctrl);\n}\n\nstatic void nvme_reset_done(struct pci_dev *pdev)\n{\n\tstruct nvme_dev *dev = pci_get_drvdata(pdev);\n\n\tif (!nvme_try_sched_reset(&dev->ctrl))\n\t\tflush_work(&dev->ctrl.reset_work);\n}\n\nstatic void nvme_shutdown(struct pci_dev *pdev)\n{\n\tstruct nvme_dev *dev = pci_get_drvdata(pdev);\n\n\tnvme_disable_prepare_reset(dev, true);\n}\n\n \nstatic void nvme_remove(struct pci_dev *pdev)\n{\n\tstruct nvme_dev *dev = pci_get_drvdata(pdev);\n\n\tnvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DELETING);\n\tpci_set_drvdata(pdev, NULL);\n\n\tif (!pci_device_is_present(pdev)) {\n\t\tnvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_DEAD);\n\t\tnvme_dev_disable(dev, true);\n\t}\n\n\tflush_work(&dev->ctrl.reset_work);\n\tnvme_stop_ctrl(&dev->ctrl);\n\tnvme_remove_namespaces(&dev->ctrl);\n\tnvme_dev_disable(dev, true);\n\tnvme_free_host_mem(dev);\n\tnvme_dev_remove_admin(dev);\n\tnvme_dbbuf_dma_free(dev);\n\tnvme_free_queues(dev, 0);\n\tmempool_destroy(dev->iod_mempool);\n\tnvme_release_prp_pools(dev);\n\tnvme_dev_unmap(dev);\n\tnvme_uninit_ctrl(&dev->ctrl);\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic int nvme_get_power_state(struct nvme_ctrl *ctrl, u32 *ps)\n{\n\treturn nvme_get_features(ctrl, NVME_FEAT_POWER_MGMT, 0, NULL, 0, ps);\n}\n\nstatic int nvme_set_power_state(struct nvme_ctrl *ctrl, u32 ps)\n{\n\treturn nvme_set_features(ctrl, NVME_FEAT_POWER_MGMT, ps, NULL, 0, NULL);\n}\n\nstatic int nvme_resume(struct device *dev)\n{\n\tstruct nvme_dev *ndev = pci_get_drvdata(to_pci_dev(dev));\n\tstruct nvme_ctrl *ctrl = &ndev->ctrl;\n\n\tif (ndev->last_ps == U32_MAX ||\n\t    nvme_set_power_state(ctrl, ndev->last_ps) != 0)\n\t\tgoto reset;\n\tif (ctrl->hmpre && nvme_setup_host_mem(ndev))\n\t\tgoto reset;\n\n\treturn 0;\nreset:\n\treturn nvme_try_sched_reset(ctrl);\n}\n\nstatic int nvme_suspend(struct device *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct nvme_dev *ndev = pci_get_drvdata(pdev);\n\tstruct nvme_ctrl *ctrl = &ndev->ctrl;\n\tint ret = -EBUSY;\n\n\tndev->last_ps = U32_MAX;\n\n\t \n\tif (pm_suspend_via_firmware() || !ctrl->npss ||\n\t    !pcie_aspm_enabled(pdev) ||\n\t    (ndev->ctrl.quirks & NVME_QUIRK_SIMPLE_SUSPEND))\n\t\treturn nvme_disable_prepare_reset(ndev, true);\n\n\tnvme_start_freeze(ctrl);\n\tnvme_wait_freeze(ctrl);\n\tnvme_sync_queues(ctrl);\n\n\tif (nvme_ctrl_state(ctrl) != NVME_CTRL_LIVE)\n\t\tgoto unfreeze;\n\n\t \n\tif (ndev->hmb) {\n\t\tret = nvme_set_host_mem(ndev, 0);\n\t\tif (ret < 0)\n\t\t\tgoto unfreeze;\n\t}\n\n\tret = nvme_get_power_state(ctrl, &ndev->last_ps);\n\tif (ret < 0)\n\t\tgoto unfreeze;\n\n\t \n\tpci_save_state(pdev);\n\n\tret = nvme_set_power_state(ctrl, ctrl->npss);\n\tif (ret < 0)\n\t\tgoto unfreeze;\n\n\tif (ret) {\n\t\t \n\t\tpci_load_saved_state(pdev, NULL);\n\n\t\t \n\t\tret = nvme_disable_prepare_reset(ndev, true);\n\t\tctrl->npss = 0;\n\t}\nunfreeze:\n\tnvme_unfreeze(ctrl);\n\treturn ret;\n}\n\nstatic int nvme_simple_suspend(struct device *dev)\n{\n\tstruct nvme_dev *ndev = pci_get_drvdata(to_pci_dev(dev));\n\n\treturn nvme_disable_prepare_reset(ndev, true);\n}\n\nstatic int nvme_simple_resume(struct device *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct nvme_dev *ndev = pci_get_drvdata(pdev);\n\n\treturn nvme_try_sched_reset(&ndev->ctrl);\n}\n\nstatic const struct dev_pm_ops nvme_dev_pm_ops = {\n\t.suspend\t= nvme_suspend,\n\t.resume\t\t= nvme_resume,\n\t.freeze\t\t= nvme_simple_suspend,\n\t.thaw\t\t= nvme_simple_resume,\n\t.poweroff\t= nvme_simple_suspend,\n\t.restore\t= nvme_simple_resume,\n};\n#endif  \n\nstatic pci_ers_result_t nvme_error_detected(struct pci_dev *pdev,\n\t\t\t\t\t\tpci_channel_state_t state)\n{\n\tstruct nvme_dev *dev = pci_get_drvdata(pdev);\n\n\t \n\tswitch (state) {\n\tcase pci_channel_io_normal:\n\t\treturn PCI_ERS_RESULT_CAN_RECOVER;\n\tcase pci_channel_io_frozen:\n\t\tdev_warn(dev->ctrl.device,\n\t\t\t\"frozen state error detected, reset controller\\n\");\n\t\tif (!nvme_change_ctrl_state(&dev->ctrl, NVME_CTRL_RESETTING)) {\n\t\t\tnvme_dev_disable(dev, true);\n\t\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\t\t}\n\t\tnvme_dev_disable(dev, false);\n\t\treturn PCI_ERS_RESULT_NEED_RESET;\n\tcase pci_channel_io_perm_failure:\n\t\tdev_warn(dev->ctrl.device,\n\t\t\t\"failure state error detected, request disconnect\\n\");\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\t}\n\treturn PCI_ERS_RESULT_NEED_RESET;\n}\n\nstatic pci_ers_result_t nvme_slot_reset(struct pci_dev *pdev)\n{\n\tstruct nvme_dev *dev = pci_get_drvdata(pdev);\n\n\tdev_info(dev->ctrl.device, \"restart after slot reset\\n\");\n\tpci_restore_state(pdev);\n\tif (!nvme_try_sched_reset(&dev->ctrl))\n\t\tnvme_unquiesce_io_queues(&dev->ctrl);\n\treturn PCI_ERS_RESULT_RECOVERED;\n}\n\nstatic void nvme_error_resume(struct pci_dev *pdev)\n{\n\tstruct nvme_dev *dev = pci_get_drvdata(pdev);\n\n\tflush_work(&dev->ctrl.reset_work);\n}\n\nstatic const struct pci_error_handlers nvme_err_handler = {\n\t.error_detected\t= nvme_error_detected,\n\t.slot_reset\t= nvme_slot_reset,\n\t.resume\t\t= nvme_error_resume,\n\t.reset_prepare\t= nvme_reset_prepare,\n\t.reset_done\t= nvme_reset_done,\n};\n\nstatic const struct pci_device_id nvme_id_table[] = {\n\t{ PCI_VDEVICE(INTEL, 0x0953),\t \n\t\t.driver_data = NVME_QUIRK_STRIPE_SIZE |\n\t\t\t\tNVME_QUIRK_DEALLOCATE_ZEROES, },\n\t{ PCI_VDEVICE(INTEL, 0x0a53),\t \n\t\t.driver_data = NVME_QUIRK_STRIPE_SIZE |\n\t\t\t\tNVME_QUIRK_DEALLOCATE_ZEROES, },\n\t{ PCI_VDEVICE(INTEL, 0x0a54),\t \n\t\t.driver_data = NVME_QUIRK_STRIPE_SIZE |\n\t\t\t\tNVME_QUIRK_DEALLOCATE_ZEROES |\n\t\t\t\tNVME_QUIRK_IGNORE_DEV_SUBNQN |\n\t\t\t\tNVME_QUIRK_BOGUS_NID, },\n\t{ PCI_VDEVICE(INTEL, 0x0a55),\t \n\t\t.driver_data = NVME_QUIRK_STRIPE_SIZE |\n\t\t\t\tNVME_QUIRK_DEALLOCATE_ZEROES, },\n\t{ PCI_VDEVICE(INTEL, 0xf1a5),\t \n\t\t.driver_data = NVME_QUIRK_NO_DEEPEST_PS |\n\t\t\t\tNVME_QUIRK_MEDIUM_PRIO_SQ |\n\t\t\t\tNVME_QUIRK_NO_TEMP_THRESH_CHANGE |\n\t\t\t\tNVME_QUIRK_DISABLE_WRITE_ZEROES, },\n\t{ PCI_VDEVICE(INTEL, 0xf1a6),\t \n\t\t.driver_data = NVME_QUIRK_IGNORE_DEV_SUBNQN, },\n\t{ PCI_VDEVICE(INTEL, 0x5845),\t \n\t\t.driver_data = NVME_QUIRK_IDENTIFY_CNS |\n\t\t\t\tNVME_QUIRK_DISABLE_WRITE_ZEROES |\n\t\t\t\tNVME_QUIRK_BOGUS_NID, },\n\t{ PCI_VDEVICE(REDHAT, 0x0010),\t \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x126f, 0x2263),\t \n\t\t.driver_data = NVME_QUIRK_NO_NS_DESC_LIST |\n\t\t\t\tNVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1bb1, 0x0100),    \n\t\t.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY |\n\t\t\t\tNVME_QUIRK_NO_NS_DESC_LIST, },\n\t{ PCI_DEVICE(0x1c58, 0x0003),\t \n\t\t.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },\n\t{ PCI_DEVICE(0x1c58, 0x0023),\t \n\t\t.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },\n\t{ PCI_DEVICE(0x1c5f, 0x0540),\t \n\t\t.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },\n\t{ PCI_DEVICE(0x144d, 0xa821),    \n\t\t.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY, },\n\t{ PCI_DEVICE(0x144d, 0xa822),    \n\t\t.driver_data = NVME_QUIRK_DELAY_BEFORE_CHK_RDY |\n\t\t\t\tNVME_QUIRK_DISABLE_WRITE_ZEROES|\n\t\t\t\tNVME_QUIRK_IGNORE_DEV_SUBNQN, },\n\t{ PCI_DEVICE(0x1987, 0x5012),\t \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1987, 0x5016),\t \n\t\t.driver_data = NVME_QUIRK_IGNORE_DEV_SUBNQN |\n\t\t\t\tNVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1987, 0x5019),   \n\t\t.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES, },\n\t{ PCI_DEVICE(0x1987, 0x5021),    \n\t\t.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES, },\n\t{ PCI_DEVICE(0x1b4b, 0x1092),\t \n\t\t.driver_data = NVME_QUIRK_NO_NS_DESC_LIST |\n\t\t\t\tNVME_QUIRK_IGNORE_DEV_SUBNQN, },\n\t{ PCI_DEVICE(0x1cc1, 0x33f8),    \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x10ec, 0x5762),    \n\t\t.driver_data = NVME_QUIRK_IGNORE_DEV_SUBNQN |\n\t\t\t\tNVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x10ec, 0x5763),   \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1cc1, 0x8201),    \n\t\t.driver_data = NVME_QUIRK_NO_DEEPEST_PS |\n\t\t\t\tNVME_QUIRK_IGNORE_DEV_SUBNQN, },\n\t { PCI_DEVICE(0x1344, 0x5407),  \n\t\t.driver_data = NVME_QUIRK_IGNORE_DEV_SUBNQN },\n\t { PCI_DEVICE(0x1344, 0x6001),    \n\t\t .driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1c5c, 0x1504),    \n\t\t.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES, },\n\t{ PCI_DEVICE(0x1c5c, 0x174a),    \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x15b7, 0x2001),    \n\t\t.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES, },\n\t{ PCI_DEVICE(0x1d97, 0x2263),    \n\t\t.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES, },\n\t{ PCI_DEVICE(0x144d, 0xa80b),    \n\t\t.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES |\n\t\t\t\tNVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x144d, 0xa809),    \n\t\t.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES, },\n\t{ PCI_DEVICE(0x144d, 0xa802),    \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1cc4, 0x6303),    \n\t\t.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES, },\n\t{ PCI_DEVICE(0x1cc4, 0x6302),    \n\t\t.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES, },\n\t{ PCI_DEVICE(0x2646, 0x2262),    \n\t\t.driver_data = NVME_QUIRK_NO_DEEPEST_PS, },\n\t{ PCI_DEVICE(0x2646, 0x2263),    \n\t\t.driver_data = NVME_QUIRK_NO_DEEPEST_PS, },\n\t{ PCI_DEVICE(0x2646, 0x5013),    \n\t\t.driver_data = NVME_QUIRK_NO_SECONDARY_TEMP_THRESH, },\n\t{ PCI_DEVICE(0x2646, 0x5018),    \n\t\t.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES, },\n\t{ PCI_DEVICE(0x2646, 0x5016),    \n\t\t.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES, },\n\t{ PCI_DEVICE(0x2646, 0x501A),    \n\t\t.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES, },\n\t{ PCI_DEVICE(0x2646, 0x501B),    \n\t\t.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES, },\n\t{ PCI_DEVICE(0x2646, 0x501E),    \n\t\t.driver_data = NVME_QUIRK_DISABLE_WRITE_ZEROES, },\n\t{ PCI_DEVICE(0x1f40, 0x1202),    \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1f40, 0x5236),    \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1e4B, 0x1001),    \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1e4B, 0x1002),    \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1e4B, 0x1202),    \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1e4B, 0x1602),    \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1cc1, 0x5350),    \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1dbe, 0x5236),    \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1e49, 0x0021),    \n\t\t.driver_data = NVME_QUIRK_NO_DEEPEST_PS, },\n\t{ PCI_DEVICE(0x1e49, 0x0041),    \n\t\t.driver_data = NVME_QUIRK_NO_DEEPEST_PS, },\n\t{ PCI_DEVICE(0xc0a9, 0x540a),    \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1d97, 0x2263),  \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1d97, 0x1d97),  \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1d97, 0x2269),  \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID |\n\t\t\t\tNVME_QUIRK_IGNORE_DEV_SUBNQN, },\n\t{ PCI_DEVICE(0x10ec, 0x5763),  \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x1e4b, 0x1602),  \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(0x10ec, 0x5765),  \n\t\t.driver_data = NVME_QUIRK_BOGUS_NID, },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMAZON, 0x0061),\n\t\t.driver_data = NVME_QUIRK_DMA_ADDRESS_BITS_48, },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMAZON, 0x0065),\n\t\t.driver_data = NVME_QUIRK_DMA_ADDRESS_BITS_48, },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMAZON, 0x8061),\n\t\t.driver_data = NVME_QUIRK_DMA_ADDRESS_BITS_48, },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMAZON, 0xcd00),\n\t\t.driver_data = NVME_QUIRK_DMA_ADDRESS_BITS_48, },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMAZON, 0xcd01),\n\t\t.driver_data = NVME_QUIRK_DMA_ADDRESS_BITS_48, },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_AMAZON, 0xcd02),\n\t\t.driver_data = NVME_QUIRK_DMA_ADDRESS_BITS_48, },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_APPLE, 0x2001),\n\t\t.driver_data = NVME_QUIRK_SINGLE_VECTOR },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_APPLE, 0x2003) },\n\t{ PCI_DEVICE(PCI_VENDOR_ID_APPLE, 0x2005),\n\t\t.driver_data = NVME_QUIRK_SINGLE_VECTOR |\n\t\t\t\tNVME_QUIRK_128_BYTES_SQES |\n\t\t\t\tNVME_QUIRK_SHARED_TAGS |\n\t\t\t\tNVME_QUIRK_SKIP_CID_GEN |\n\t\t\t\tNVME_QUIRK_IDENTIFY_CNS },\n\t{ PCI_DEVICE_CLASS(PCI_CLASS_STORAGE_EXPRESS, 0xffffff) },\n\t{ 0, }\n};\nMODULE_DEVICE_TABLE(pci, nvme_id_table);\n\nstatic struct pci_driver nvme_driver = {\n\t.name\t\t= \"nvme\",\n\t.id_table\t= nvme_id_table,\n\t.probe\t\t= nvme_probe,\n\t.remove\t\t= nvme_remove,\n\t.shutdown\t= nvme_shutdown,\n\t.driver\t\t= {\n\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n#ifdef CONFIG_PM_SLEEP\n\t\t.pm\t\t= &nvme_dev_pm_ops,\n#endif\n\t},\n\t.sriov_configure = pci_sriov_configure_simple,\n\t.err_handler\t= &nvme_err_handler,\n};\n\nstatic int __init nvme_init(void)\n{\n\tBUILD_BUG_ON(sizeof(struct nvme_create_cq) != 64);\n\tBUILD_BUG_ON(sizeof(struct nvme_create_sq) != 64);\n\tBUILD_BUG_ON(sizeof(struct nvme_delete_queue) != 64);\n\tBUILD_BUG_ON(IRQ_AFFINITY_MAX_SETS < 2);\n\tBUILD_BUG_ON(NVME_MAX_SEGS > SGES_PER_PAGE);\n\tBUILD_BUG_ON(sizeof(struct scatterlist) * NVME_MAX_SEGS > PAGE_SIZE);\n\tBUILD_BUG_ON(nvme_pci_npages_prp() > NVME_MAX_NR_ALLOCATIONS);\n\n\treturn pci_register_driver(&nvme_driver);\n}\n\nstatic void __exit nvme_exit(void)\n{\n\tpci_unregister_driver(&nvme_driver);\n\tflush_workqueue(nvme_wq);\n}\n\nMODULE_AUTHOR(\"Matthew Wilcox <willy@linux.intel.com>\");\nMODULE_LICENSE(\"GPL\");\nMODULE_VERSION(\"1.0\");\nmodule_init(nvme_init);\nmodule_exit(nvme_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}