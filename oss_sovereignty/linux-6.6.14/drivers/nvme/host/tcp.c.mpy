{
  "module_name": "tcp.c",
  "hash_id": "7c2fcc685b1f07a050db530d16fd95d138c55eeb8a99dda1c599b8fa38312d7c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvme/host/tcp.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/nvme-tcp.h>\n#include <net/sock.h>\n#include <net/tcp.h>\n#include <linux/blk-mq.h>\n#include <crypto/hash.h>\n#include <net/busy_poll.h>\n#include <trace/events/sock.h>\n\n#include \"nvme.h\"\n#include \"fabrics.h\"\n\nstruct nvme_tcp_queue;\n\n \nstatic int so_priority;\nmodule_param(so_priority, int, 0644);\nMODULE_PARM_DESC(so_priority, \"nvme tcp socket optimize priority\");\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n \nstatic struct lock_class_key nvme_tcp_sk_key[2];\nstatic struct lock_class_key nvme_tcp_slock_key[2];\n\nstatic void nvme_tcp_reclassify_socket(struct socket *sock)\n{\n\tstruct sock *sk = sock->sk;\n\n\tif (WARN_ON_ONCE(!sock_allow_reclassification(sk)))\n\t\treturn;\n\n\tswitch (sk->sk_family) {\n\tcase AF_INET:\n\t\tsock_lock_init_class_and_name(sk, \"slock-AF_INET-NVME\",\n\t\t\t\t\t      &nvme_tcp_slock_key[0],\n\t\t\t\t\t      \"sk_lock-AF_INET-NVME\",\n\t\t\t\t\t      &nvme_tcp_sk_key[0]);\n\t\tbreak;\n\tcase AF_INET6:\n\t\tsock_lock_init_class_and_name(sk, \"slock-AF_INET6-NVME\",\n\t\t\t\t\t      &nvme_tcp_slock_key[1],\n\t\t\t\t\t      \"sk_lock-AF_INET6-NVME\",\n\t\t\t\t\t      &nvme_tcp_sk_key[1]);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n}\n#else\nstatic void nvme_tcp_reclassify_socket(struct socket *sock) { }\n#endif\n\nenum nvme_tcp_send_state {\n\tNVME_TCP_SEND_CMD_PDU = 0,\n\tNVME_TCP_SEND_H2C_PDU,\n\tNVME_TCP_SEND_DATA,\n\tNVME_TCP_SEND_DDGST,\n};\n\nstruct nvme_tcp_request {\n\tstruct nvme_request\treq;\n\tvoid\t\t\t*pdu;\n\tstruct nvme_tcp_queue\t*queue;\n\tu32\t\t\tdata_len;\n\tu32\t\t\tpdu_len;\n\tu32\t\t\tpdu_sent;\n\tu32\t\t\th2cdata_left;\n\tu32\t\t\th2cdata_offset;\n\tu16\t\t\tttag;\n\t__le16\t\t\tstatus;\n\tstruct list_head\tentry;\n\tstruct llist_node\tlentry;\n\t__le32\t\t\tddgst;\n\n\tstruct bio\t\t*curr_bio;\n\tstruct iov_iter\t\titer;\n\n\t \n\tsize_t\t\t\toffset;\n\tsize_t\t\t\tdata_sent;\n\tenum nvme_tcp_send_state state;\n};\n\nenum nvme_tcp_queue_flags {\n\tNVME_TCP_Q_ALLOCATED\t= 0,\n\tNVME_TCP_Q_LIVE\t\t= 1,\n\tNVME_TCP_Q_POLLING\t= 2,\n};\n\nenum nvme_tcp_recv_state {\n\tNVME_TCP_RECV_PDU = 0,\n\tNVME_TCP_RECV_DATA,\n\tNVME_TCP_RECV_DDGST,\n};\n\nstruct nvme_tcp_ctrl;\nstruct nvme_tcp_queue {\n\tstruct socket\t\t*sock;\n\tstruct work_struct\tio_work;\n\tint\t\t\tio_cpu;\n\n\tstruct mutex\t\tqueue_lock;\n\tstruct mutex\t\tsend_mutex;\n\tstruct llist_head\treq_list;\n\tstruct list_head\tsend_list;\n\n\t \n\tvoid\t\t\t*pdu;\n\tint\t\t\tpdu_remaining;\n\tint\t\t\tpdu_offset;\n\tsize_t\t\t\tdata_remaining;\n\tsize_t\t\t\tddgst_remaining;\n\tunsigned int\t\tnr_cqe;\n\n\t \n\tstruct nvme_tcp_request *request;\n\n\tu32\t\t\tmaxh2cdata;\n\tsize_t\t\t\tcmnd_capsule_len;\n\tstruct nvme_tcp_ctrl\t*ctrl;\n\tunsigned long\t\tflags;\n\tbool\t\t\trd_enabled;\n\n\tbool\t\t\thdr_digest;\n\tbool\t\t\tdata_digest;\n\tstruct ahash_request\t*rcv_hash;\n\tstruct ahash_request\t*snd_hash;\n\t__le32\t\t\texp_ddgst;\n\t__le32\t\t\trecv_ddgst;\n\n\tstruct page_frag_cache\tpf_cache;\n\n\tvoid (*state_change)(struct sock *);\n\tvoid (*data_ready)(struct sock *);\n\tvoid (*write_space)(struct sock *);\n};\n\nstruct nvme_tcp_ctrl {\n\t \n\tstruct nvme_tcp_queue\t*queues;\n\tstruct blk_mq_tag_set\ttag_set;\n\n\t \n\tstruct list_head\tlist;\n\tstruct blk_mq_tag_set\tadmin_tag_set;\n\tstruct sockaddr_storage addr;\n\tstruct sockaddr_storage src_addr;\n\tstruct nvme_ctrl\tctrl;\n\n\tstruct work_struct\terr_work;\n\tstruct delayed_work\tconnect_work;\n\tstruct nvme_tcp_request async_req;\n\tu32\t\t\tio_queues[HCTX_MAX_TYPES];\n};\n\nstatic LIST_HEAD(nvme_tcp_ctrl_list);\nstatic DEFINE_MUTEX(nvme_tcp_ctrl_mutex);\nstatic struct workqueue_struct *nvme_tcp_wq;\nstatic const struct blk_mq_ops nvme_tcp_mq_ops;\nstatic const struct blk_mq_ops nvme_tcp_admin_mq_ops;\nstatic int nvme_tcp_try_send(struct nvme_tcp_queue *queue);\n\nstatic inline struct nvme_tcp_ctrl *to_tcp_ctrl(struct nvme_ctrl *ctrl)\n{\n\treturn container_of(ctrl, struct nvme_tcp_ctrl, ctrl);\n}\n\nstatic inline int nvme_tcp_queue_id(struct nvme_tcp_queue *queue)\n{\n\treturn queue - queue->ctrl->queues;\n}\n\nstatic inline struct blk_mq_tags *nvme_tcp_tagset(struct nvme_tcp_queue *queue)\n{\n\tu32 queue_idx = nvme_tcp_queue_id(queue);\n\n\tif (queue_idx == 0)\n\t\treturn queue->ctrl->admin_tag_set.tags[queue_idx];\n\treturn queue->ctrl->tag_set.tags[queue_idx - 1];\n}\n\nstatic inline u8 nvme_tcp_hdgst_len(struct nvme_tcp_queue *queue)\n{\n\treturn queue->hdr_digest ? NVME_TCP_DIGEST_LENGTH : 0;\n}\n\nstatic inline u8 nvme_tcp_ddgst_len(struct nvme_tcp_queue *queue)\n{\n\treturn queue->data_digest ? NVME_TCP_DIGEST_LENGTH : 0;\n}\n\nstatic inline void *nvme_tcp_req_cmd_pdu(struct nvme_tcp_request *req)\n{\n\treturn req->pdu;\n}\n\nstatic inline void *nvme_tcp_req_data_pdu(struct nvme_tcp_request *req)\n{\n\t \n\treturn req->pdu + sizeof(struct nvme_tcp_cmd_pdu) -\n\t\tsizeof(struct nvme_tcp_data_pdu);\n}\n\nstatic inline size_t nvme_tcp_inline_data_size(struct nvme_tcp_request *req)\n{\n\tif (nvme_is_fabrics(req->req.cmd))\n\t\treturn NVME_TCP_ADMIN_CCSZ;\n\treturn req->queue->cmnd_capsule_len - sizeof(struct nvme_command);\n}\n\nstatic inline bool nvme_tcp_async_req(struct nvme_tcp_request *req)\n{\n\treturn req == &req->queue->ctrl->async_req;\n}\n\nstatic inline bool nvme_tcp_has_inline_data(struct nvme_tcp_request *req)\n{\n\tstruct request *rq;\n\n\tif (unlikely(nvme_tcp_async_req(req)))\n\t\treturn false;  \n\n\trq = blk_mq_rq_from_pdu(req);\n\n\treturn rq_data_dir(rq) == WRITE && req->data_len &&\n\t\treq->data_len <= nvme_tcp_inline_data_size(req);\n}\n\nstatic inline struct page *nvme_tcp_req_cur_page(struct nvme_tcp_request *req)\n{\n\treturn req->iter.bvec->bv_page;\n}\n\nstatic inline size_t nvme_tcp_req_cur_offset(struct nvme_tcp_request *req)\n{\n\treturn req->iter.bvec->bv_offset + req->iter.iov_offset;\n}\n\nstatic inline size_t nvme_tcp_req_cur_length(struct nvme_tcp_request *req)\n{\n\treturn min_t(size_t, iov_iter_single_seg_count(&req->iter),\n\t\t\treq->pdu_len - req->pdu_sent);\n}\n\nstatic inline size_t nvme_tcp_pdu_data_left(struct nvme_tcp_request *req)\n{\n\treturn rq_data_dir(blk_mq_rq_from_pdu(req)) == WRITE ?\n\t\t\treq->pdu_len - req->pdu_sent : 0;\n}\n\nstatic inline size_t nvme_tcp_pdu_last_send(struct nvme_tcp_request *req,\n\t\tint len)\n{\n\treturn nvme_tcp_pdu_data_left(req) <= len;\n}\n\nstatic void nvme_tcp_init_iter(struct nvme_tcp_request *req,\n\t\tunsigned int dir)\n{\n\tstruct request *rq = blk_mq_rq_from_pdu(req);\n\tstruct bio_vec *vec;\n\tunsigned int size;\n\tint nr_bvec;\n\tsize_t offset;\n\n\tif (rq->rq_flags & RQF_SPECIAL_PAYLOAD) {\n\t\tvec = &rq->special_vec;\n\t\tnr_bvec = 1;\n\t\tsize = blk_rq_payload_bytes(rq);\n\t\toffset = 0;\n\t} else {\n\t\tstruct bio *bio = req->curr_bio;\n\t\tstruct bvec_iter bi;\n\t\tstruct bio_vec bv;\n\n\t\tvec = __bvec_iter_bvec(bio->bi_io_vec, bio->bi_iter);\n\t\tnr_bvec = 0;\n\t\tbio_for_each_bvec(bv, bio, bi) {\n\t\t\tnr_bvec++;\n\t\t}\n\t\tsize = bio->bi_iter.bi_size;\n\t\toffset = bio->bi_iter.bi_bvec_done;\n\t}\n\n\tiov_iter_bvec(&req->iter, dir, vec, nr_bvec, size);\n\treq->iter.iov_offset = offset;\n}\n\nstatic inline void nvme_tcp_advance_req(struct nvme_tcp_request *req,\n\t\tint len)\n{\n\treq->data_sent += len;\n\treq->pdu_sent += len;\n\tiov_iter_advance(&req->iter, len);\n\tif (!iov_iter_count(&req->iter) &&\n\t    req->data_sent < req->data_len) {\n\t\treq->curr_bio = req->curr_bio->bi_next;\n\t\tnvme_tcp_init_iter(req, ITER_SOURCE);\n\t}\n}\n\nstatic inline void nvme_tcp_send_all(struct nvme_tcp_queue *queue)\n{\n\tint ret;\n\n\t \n\tdo {\n\t\tret = nvme_tcp_try_send(queue);\n\t} while (ret > 0);\n}\n\nstatic inline bool nvme_tcp_queue_more(struct nvme_tcp_queue *queue)\n{\n\treturn !list_empty(&queue->send_list) ||\n\t\t!llist_empty(&queue->req_list);\n}\n\nstatic inline void nvme_tcp_queue_request(struct nvme_tcp_request *req,\n\t\tbool sync, bool last)\n{\n\tstruct nvme_tcp_queue *queue = req->queue;\n\tbool empty;\n\n\tempty = llist_add(&req->lentry, &queue->req_list) &&\n\t\tlist_empty(&queue->send_list) && !queue->request;\n\n\t \n\tif (queue->io_cpu == raw_smp_processor_id() &&\n\t    sync && empty && mutex_trylock(&queue->send_mutex)) {\n\t\tnvme_tcp_send_all(queue);\n\t\tmutex_unlock(&queue->send_mutex);\n\t}\n\n\tif (last && nvme_tcp_queue_more(queue))\n\t\tqueue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);\n}\n\nstatic void nvme_tcp_process_req_list(struct nvme_tcp_queue *queue)\n{\n\tstruct nvme_tcp_request *req;\n\tstruct llist_node *node;\n\n\tfor (node = llist_del_all(&queue->req_list); node; node = node->next) {\n\t\treq = llist_entry(node, struct nvme_tcp_request, lentry);\n\t\tlist_add(&req->entry, &queue->send_list);\n\t}\n}\n\nstatic inline struct nvme_tcp_request *\nnvme_tcp_fetch_request(struct nvme_tcp_queue *queue)\n{\n\tstruct nvme_tcp_request *req;\n\n\treq = list_first_entry_or_null(&queue->send_list,\n\t\t\tstruct nvme_tcp_request, entry);\n\tif (!req) {\n\t\tnvme_tcp_process_req_list(queue);\n\t\treq = list_first_entry_or_null(&queue->send_list,\n\t\t\t\tstruct nvme_tcp_request, entry);\n\t\tif (unlikely(!req))\n\t\t\treturn NULL;\n\t}\n\n\tlist_del(&req->entry);\n\treturn req;\n}\n\nstatic inline void nvme_tcp_ddgst_final(struct ahash_request *hash,\n\t\t__le32 *dgst)\n{\n\tahash_request_set_crypt(hash, NULL, (u8 *)dgst, 0);\n\tcrypto_ahash_final(hash);\n}\n\nstatic inline void nvme_tcp_ddgst_update(struct ahash_request *hash,\n\t\tstruct page *page, off_t off, size_t len)\n{\n\tstruct scatterlist sg;\n\n\tsg_init_table(&sg, 1);\n\tsg_set_page(&sg, page, len, off);\n\tahash_request_set_crypt(hash, &sg, NULL, len);\n\tcrypto_ahash_update(hash);\n}\n\nstatic inline void nvme_tcp_hdgst(struct ahash_request *hash,\n\t\tvoid *pdu, size_t len)\n{\n\tstruct scatterlist sg;\n\n\tsg_init_one(&sg, pdu, len);\n\tahash_request_set_crypt(hash, &sg, pdu + len, len);\n\tcrypto_ahash_digest(hash);\n}\n\nstatic int nvme_tcp_verify_hdgst(struct nvme_tcp_queue *queue,\n\t\tvoid *pdu, size_t pdu_len)\n{\n\tstruct nvme_tcp_hdr *hdr = pdu;\n\t__le32 recv_digest;\n\t__le32 exp_digest;\n\n\tif (unlikely(!(hdr->flags & NVME_TCP_F_HDGST))) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"queue %d: header digest flag is cleared\\n\",\n\t\t\tnvme_tcp_queue_id(queue));\n\t\treturn -EPROTO;\n\t}\n\n\trecv_digest = *(__le32 *)(pdu + hdr->hlen);\n\tnvme_tcp_hdgst(queue->rcv_hash, pdu, pdu_len);\n\texp_digest = *(__le32 *)(pdu + hdr->hlen);\n\tif (recv_digest != exp_digest) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"header digest error: recv %#x expected %#x\\n\",\n\t\t\tle32_to_cpu(recv_digest), le32_to_cpu(exp_digest));\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic int nvme_tcp_check_ddgst(struct nvme_tcp_queue *queue, void *pdu)\n{\n\tstruct nvme_tcp_hdr *hdr = pdu;\n\tu8 digest_len = nvme_tcp_hdgst_len(queue);\n\tu32 len;\n\n\tlen = le32_to_cpu(hdr->plen) - hdr->hlen -\n\t\t((hdr->flags & NVME_TCP_F_HDGST) ? digest_len : 0);\n\n\tif (unlikely(len && !(hdr->flags & NVME_TCP_F_DDGST))) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"queue %d: data digest flag is cleared\\n\",\n\t\tnvme_tcp_queue_id(queue));\n\t\treturn -EPROTO;\n\t}\n\tcrypto_ahash_init(queue->rcv_hash);\n\n\treturn 0;\n}\n\nstatic void nvme_tcp_exit_request(struct blk_mq_tag_set *set,\n\t\tstruct request *rq, unsigned int hctx_idx)\n{\n\tstruct nvme_tcp_request *req = blk_mq_rq_to_pdu(rq);\n\n\tpage_frag_free(req->pdu);\n}\n\nstatic int nvme_tcp_init_request(struct blk_mq_tag_set *set,\n\t\tstruct request *rq, unsigned int hctx_idx,\n\t\tunsigned int numa_node)\n{\n\tstruct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(set->driver_data);\n\tstruct nvme_tcp_request *req = blk_mq_rq_to_pdu(rq);\n\tstruct nvme_tcp_cmd_pdu *pdu;\n\tint queue_idx = (set == &ctrl->tag_set) ? hctx_idx + 1 : 0;\n\tstruct nvme_tcp_queue *queue = &ctrl->queues[queue_idx];\n\tu8 hdgst = nvme_tcp_hdgst_len(queue);\n\n\treq->pdu = page_frag_alloc(&queue->pf_cache,\n\t\tsizeof(struct nvme_tcp_cmd_pdu) + hdgst,\n\t\tGFP_KERNEL | __GFP_ZERO);\n\tif (!req->pdu)\n\t\treturn -ENOMEM;\n\n\tpdu = req->pdu;\n\treq->queue = queue;\n\tnvme_req(rq)->ctrl = &ctrl->ctrl;\n\tnvme_req(rq)->cmd = &pdu->cmd;\n\n\treturn 0;\n}\n\nstatic int nvme_tcp_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,\n\t\tunsigned int hctx_idx)\n{\n\tstruct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(data);\n\tstruct nvme_tcp_queue *queue = &ctrl->queues[hctx_idx + 1];\n\n\thctx->driver_data = queue;\n\treturn 0;\n}\n\nstatic int nvme_tcp_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,\n\t\tunsigned int hctx_idx)\n{\n\tstruct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(data);\n\tstruct nvme_tcp_queue *queue = &ctrl->queues[0];\n\n\thctx->driver_data = queue;\n\treturn 0;\n}\n\nstatic enum nvme_tcp_recv_state\nnvme_tcp_recv_state(struct nvme_tcp_queue *queue)\n{\n\treturn  (queue->pdu_remaining) ? NVME_TCP_RECV_PDU :\n\t\t(queue->ddgst_remaining) ? NVME_TCP_RECV_DDGST :\n\t\tNVME_TCP_RECV_DATA;\n}\n\nstatic void nvme_tcp_init_recv_ctx(struct nvme_tcp_queue *queue)\n{\n\tqueue->pdu_remaining = sizeof(struct nvme_tcp_rsp_pdu) +\n\t\t\t\tnvme_tcp_hdgst_len(queue);\n\tqueue->pdu_offset = 0;\n\tqueue->data_remaining = -1;\n\tqueue->ddgst_remaining = 0;\n}\n\nstatic void nvme_tcp_error_recovery(struct nvme_ctrl *ctrl)\n{\n\tif (!nvme_change_ctrl_state(ctrl, NVME_CTRL_RESETTING))\n\t\treturn;\n\n\tdev_warn(ctrl->device, \"starting error recovery\\n\");\n\tqueue_work(nvme_reset_wq, &to_tcp_ctrl(ctrl)->err_work);\n}\n\nstatic int nvme_tcp_process_nvme_cqe(struct nvme_tcp_queue *queue,\n\t\tstruct nvme_completion *cqe)\n{\n\tstruct nvme_tcp_request *req;\n\tstruct request *rq;\n\n\trq = nvme_find_rq(nvme_tcp_tagset(queue), cqe->command_id);\n\tif (!rq) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"got bad cqe.command_id %#x on queue %d\\n\",\n\t\t\tcqe->command_id, nvme_tcp_queue_id(queue));\n\t\tnvme_tcp_error_recovery(&queue->ctrl->ctrl);\n\t\treturn -EINVAL;\n\t}\n\n\treq = blk_mq_rq_to_pdu(rq);\n\tif (req->status == cpu_to_le16(NVME_SC_SUCCESS))\n\t\treq->status = cqe->status;\n\n\tif (!nvme_try_complete_req(rq, req->status, cqe->result))\n\t\tnvme_complete_rq(rq);\n\tqueue->nr_cqe++;\n\n\treturn 0;\n}\n\nstatic int nvme_tcp_handle_c2h_data(struct nvme_tcp_queue *queue,\n\t\tstruct nvme_tcp_data_pdu *pdu)\n{\n\tstruct request *rq;\n\n\trq = nvme_find_rq(nvme_tcp_tagset(queue), pdu->command_id);\n\tif (!rq) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"got bad c2hdata.command_id %#x on queue %d\\n\",\n\t\t\tpdu->command_id, nvme_tcp_queue_id(queue));\n\t\treturn -ENOENT;\n\t}\n\n\tif (!blk_rq_payload_bytes(rq)) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"queue %d tag %#x unexpected data\\n\",\n\t\t\tnvme_tcp_queue_id(queue), rq->tag);\n\t\treturn -EIO;\n\t}\n\n\tqueue->data_remaining = le32_to_cpu(pdu->data_length);\n\n\tif (pdu->hdr.flags & NVME_TCP_F_DATA_SUCCESS &&\n\t    unlikely(!(pdu->hdr.flags & NVME_TCP_F_DATA_LAST))) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"queue %d tag %#x SUCCESS set but not last PDU\\n\",\n\t\t\tnvme_tcp_queue_id(queue), rq->tag);\n\t\tnvme_tcp_error_recovery(&queue->ctrl->ctrl);\n\t\treturn -EPROTO;\n\t}\n\n\treturn 0;\n}\n\nstatic int nvme_tcp_handle_comp(struct nvme_tcp_queue *queue,\n\t\tstruct nvme_tcp_rsp_pdu *pdu)\n{\n\tstruct nvme_completion *cqe = &pdu->cqe;\n\tint ret = 0;\n\n\t \n\tif (unlikely(nvme_is_aen_req(nvme_tcp_queue_id(queue),\n\t\t\t\t     cqe->command_id)))\n\t\tnvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,\n\t\t\t\t&cqe->result);\n\telse\n\t\tret = nvme_tcp_process_nvme_cqe(queue, cqe);\n\n\treturn ret;\n}\n\nstatic void nvme_tcp_setup_h2c_data_pdu(struct nvme_tcp_request *req)\n{\n\tstruct nvme_tcp_data_pdu *data = nvme_tcp_req_data_pdu(req);\n\tstruct nvme_tcp_queue *queue = req->queue;\n\tstruct request *rq = blk_mq_rq_from_pdu(req);\n\tu32 h2cdata_sent = req->pdu_len;\n\tu8 hdgst = nvme_tcp_hdgst_len(queue);\n\tu8 ddgst = nvme_tcp_ddgst_len(queue);\n\n\treq->state = NVME_TCP_SEND_H2C_PDU;\n\treq->offset = 0;\n\treq->pdu_len = min(req->h2cdata_left, queue->maxh2cdata);\n\treq->pdu_sent = 0;\n\treq->h2cdata_left -= req->pdu_len;\n\treq->h2cdata_offset += h2cdata_sent;\n\n\tmemset(data, 0, sizeof(*data));\n\tdata->hdr.type = nvme_tcp_h2c_data;\n\tif (!req->h2cdata_left)\n\t\tdata->hdr.flags = NVME_TCP_F_DATA_LAST;\n\tif (queue->hdr_digest)\n\t\tdata->hdr.flags |= NVME_TCP_F_HDGST;\n\tif (queue->data_digest)\n\t\tdata->hdr.flags |= NVME_TCP_F_DDGST;\n\tdata->hdr.hlen = sizeof(*data);\n\tdata->hdr.pdo = data->hdr.hlen + hdgst;\n\tdata->hdr.plen =\n\t\tcpu_to_le32(data->hdr.hlen + hdgst + req->pdu_len + ddgst);\n\tdata->ttag = req->ttag;\n\tdata->command_id = nvme_cid(rq);\n\tdata->data_offset = cpu_to_le32(req->h2cdata_offset);\n\tdata->data_length = cpu_to_le32(req->pdu_len);\n}\n\nstatic int nvme_tcp_handle_r2t(struct nvme_tcp_queue *queue,\n\t\tstruct nvme_tcp_r2t_pdu *pdu)\n{\n\tstruct nvme_tcp_request *req;\n\tstruct request *rq;\n\tu32 r2t_length = le32_to_cpu(pdu->r2t_length);\n\tu32 r2t_offset = le32_to_cpu(pdu->r2t_offset);\n\n\trq = nvme_find_rq(nvme_tcp_tagset(queue), pdu->command_id);\n\tif (!rq) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"got bad r2t.command_id %#x on queue %d\\n\",\n\t\t\tpdu->command_id, nvme_tcp_queue_id(queue));\n\t\treturn -ENOENT;\n\t}\n\treq = blk_mq_rq_to_pdu(rq);\n\n\tif (unlikely(!r2t_length)) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"req %d r2t len is %u, probably a bug...\\n\",\n\t\t\trq->tag, r2t_length);\n\t\treturn -EPROTO;\n\t}\n\n\tif (unlikely(req->data_sent + r2t_length > req->data_len)) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"req %d r2t len %u exceeded data len %u (%zu sent)\\n\",\n\t\t\trq->tag, r2t_length, req->data_len, req->data_sent);\n\t\treturn -EPROTO;\n\t}\n\n\tif (unlikely(r2t_offset < req->data_sent)) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"req %d unexpected r2t offset %u (expected %zu)\\n\",\n\t\t\trq->tag, r2t_offset, req->data_sent);\n\t\treturn -EPROTO;\n\t}\n\n\treq->pdu_len = 0;\n\treq->h2cdata_left = r2t_length;\n\treq->h2cdata_offset = r2t_offset;\n\treq->ttag = pdu->ttag;\n\n\tnvme_tcp_setup_h2c_data_pdu(req);\n\tnvme_tcp_queue_request(req, false, true);\n\n\treturn 0;\n}\n\nstatic int nvme_tcp_recv_pdu(struct nvme_tcp_queue *queue, struct sk_buff *skb,\n\t\tunsigned int *offset, size_t *len)\n{\n\tstruct nvme_tcp_hdr *hdr;\n\tchar *pdu = queue->pdu;\n\tsize_t rcv_len = min_t(size_t, *len, queue->pdu_remaining);\n\tint ret;\n\n\tret = skb_copy_bits(skb, *offset,\n\t\t&pdu[queue->pdu_offset], rcv_len);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tqueue->pdu_remaining -= rcv_len;\n\tqueue->pdu_offset += rcv_len;\n\t*offset += rcv_len;\n\t*len -= rcv_len;\n\tif (queue->pdu_remaining)\n\t\treturn 0;\n\n\thdr = queue->pdu;\n\tif (queue->hdr_digest) {\n\t\tret = nvme_tcp_verify_hdgst(queue, queue->pdu, hdr->hlen);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t}\n\n\n\tif (queue->data_digest) {\n\t\tret = nvme_tcp_check_ddgst(queue, queue->pdu);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t}\n\n\tswitch (hdr->type) {\n\tcase nvme_tcp_c2h_data:\n\t\treturn nvme_tcp_handle_c2h_data(queue, (void *)queue->pdu);\n\tcase nvme_tcp_rsp:\n\t\tnvme_tcp_init_recv_ctx(queue);\n\t\treturn nvme_tcp_handle_comp(queue, (void *)queue->pdu);\n\tcase nvme_tcp_r2t:\n\t\tnvme_tcp_init_recv_ctx(queue);\n\t\treturn nvme_tcp_handle_r2t(queue, (void *)queue->pdu);\n\tdefault:\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"unsupported pdu type (%d)\\n\", hdr->type);\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic inline void nvme_tcp_end_request(struct request *rq, u16 status)\n{\n\tunion nvme_result res = {};\n\n\tif (!nvme_try_complete_req(rq, cpu_to_le16(status << 1), res))\n\t\tnvme_complete_rq(rq);\n}\n\nstatic int nvme_tcp_recv_data(struct nvme_tcp_queue *queue, struct sk_buff *skb,\n\t\t\t      unsigned int *offset, size_t *len)\n{\n\tstruct nvme_tcp_data_pdu *pdu = (void *)queue->pdu;\n\tstruct request *rq =\n\t\tnvme_cid_to_rq(nvme_tcp_tagset(queue), pdu->command_id);\n\tstruct nvme_tcp_request *req = blk_mq_rq_to_pdu(rq);\n\n\twhile (true) {\n\t\tint recv_len, ret;\n\n\t\trecv_len = min_t(size_t, *len, queue->data_remaining);\n\t\tif (!recv_len)\n\t\t\tbreak;\n\n\t\tif (!iov_iter_count(&req->iter)) {\n\t\t\treq->curr_bio = req->curr_bio->bi_next;\n\n\t\t\t \n\t\t\tif (!req->curr_bio) {\n\t\t\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\t\t\"queue %d no space in request %#x\",\n\t\t\t\t\tnvme_tcp_queue_id(queue), rq->tag);\n\t\t\t\tnvme_tcp_init_recv_ctx(queue);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t\tnvme_tcp_init_iter(req, ITER_DEST);\n\t\t}\n\n\t\t \n\t\trecv_len = min_t(size_t, recv_len,\n\t\t\t\tiov_iter_count(&req->iter));\n\n\t\tif (queue->data_digest)\n\t\t\tret = skb_copy_and_hash_datagram_iter(skb, *offset,\n\t\t\t\t&req->iter, recv_len, queue->rcv_hash);\n\t\telse\n\t\t\tret = skb_copy_datagram_iter(skb, *offset,\n\t\t\t\t\t&req->iter, recv_len);\n\t\tif (ret) {\n\t\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\t\"queue %d failed to copy request %#x data\",\n\t\t\t\tnvme_tcp_queue_id(queue), rq->tag);\n\t\t\treturn ret;\n\t\t}\n\n\t\t*len -= recv_len;\n\t\t*offset += recv_len;\n\t\tqueue->data_remaining -= recv_len;\n\t}\n\n\tif (!queue->data_remaining) {\n\t\tif (queue->data_digest) {\n\t\t\tnvme_tcp_ddgst_final(queue->rcv_hash, &queue->exp_ddgst);\n\t\t\tqueue->ddgst_remaining = NVME_TCP_DIGEST_LENGTH;\n\t\t} else {\n\t\t\tif (pdu->hdr.flags & NVME_TCP_F_DATA_SUCCESS) {\n\t\t\t\tnvme_tcp_end_request(rq,\n\t\t\t\t\t\tle16_to_cpu(req->status));\n\t\t\t\tqueue->nr_cqe++;\n\t\t\t}\n\t\t\tnvme_tcp_init_recv_ctx(queue);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic int nvme_tcp_recv_ddgst(struct nvme_tcp_queue *queue,\n\t\tstruct sk_buff *skb, unsigned int *offset, size_t *len)\n{\n\tstruct nvme_tcp_data_pdu *pdu = (void *)queue->pdu;\n\tchar *ddgst = (char *)&queue->recv_ddgst;\n\tsize_t recv_len = min_t(size_t, *len, queue->ddgst_remaining);\n\toff_t off = NVME_TCP_DIGEST_LENGTH - queue->ddgst_remaining;\n\tint ret;\n\n\tret = skb_copy_bits(skb, *offset, &ddgst[off], recv_len);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tqueue->ddgst_remaining -= recv_len;\n\t*offset += recv_len;\n\t*len -= recv_len;\n\tif (queue->ddgst_remaining)\n\t\treturn 0;\n\n\tif (queue->recv_ddgst != queue->exp_ddgst) {\n\t\tstruct request *rq = nvme_cid_to_rq(nvme_tcp_tagset(queue),\n\t\t\t\t\tpdu->command_id);\n\t\tstruct nvme_tcp_request *req = blk_mq_rq_to_pdu(rq);\n\n\t\treq->status = cpu_to_le16(NVME_SC_DATA_XFER_ERROR);\n\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"data digest error: recv %#x expected %#x\\n\",\n\t\t\tle32_to_cpu(queue->recv_ddgst),\n\t\t\tle32_to_cpu(queue->exp_ddgst));\n\t}\n\n\tif (pdu->hdr.flags & NVME_TCP_F_DATA_SUCCESS) {\n\t\tstruct request *rq = nvme_cid_to_rq(nvme_tcp_tagset(queue),\n\t\t\t\t\tpdu->command_id);\n\t\tstruct nvme_tcp_request *req = blk_mq_rq_to_pdu(rq);\n\n\t\tnvme_tcp_end_request(rq, le16_to_cpu(req->status));\n\t\tqueue->nr_cqe++;\n\t}\n\n\tnvme_tcp_init_recv_ctx(queue);\n\treturn 0;\n}\n\nstatic int nvme_tcp_recv_skb(read_descriptor_t *desc, struct sk_buff *skb,\n\t\t\t     unsigned int offset, size_t len)\n{\n\tstruct nvme_tcp_queue *queue = desc->arg.data;\n\tsize_t consumed = len;\n\tint result;\n\n\tif (unlikely(!queue->rd_enabled))\n\t\treturn -EFAULT;\n\n\twhile (len) {\n\t\tswitch (nvme_tcp_recv_state(queue)) {\n\t\tcase NVME_TCP_RECV_PDU:\n\t\t\tresult = nvme_tcp_recv_pdu(queue, skb, &offset, &len);\n\t\t\tbreak;\n\t\tcase NVME_TCP_RECV_DATA:\n\t\t\tresult = nvme_tcp_recv_data(queue, skb, &offset, &len);\n\t\t\tbreak;\n\t\tcase NVME_TCP_RECV_DDGST:\n\t\t\tresult = nvme_tcp_recv_ddgst(queue, skb, &offset, &len);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tresult = -EFAULT;\n\t\t}\n\t\tif (result) {\n\t\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\t\"receive failed:  %d\\n\", result);\n\t\t\tqueue->rd_enabled = false;\n\t\t\tnvme_tcp_error_recovery(&queue->ctrl->ctrl);\n\t\t\treturn result;\n\t\t}\n\t}\n\n\treturn consumed;\n}\n\nstatic void nvme_tcp_data_ready(struct sock *sk)\n{\n\tstruct nvme_tcp_queue *queue;\n\n\ttrace_sk_data_ready(sk);\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tqueue = sk->sk_user_data;\n\tif (likely(queue && queue->rd_enabled) &&\n\t    !test_bit(NVME_TCP_Q_POLLING, &queue->flags))\n\t\tqueue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);\n\tread_unlock_bh(&sk->sk_callback_lock);\n}\n\nstatic void nvme_tcp_write_space(struct sock *sk)\n{\n\tstruct nvme_tcp_queue *queue;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tqueue = sk->sk_user_data;\n\tif (likely(queue && sk_stream_is_writeable(sk))) {\n\t\tclear_bit(SOCK_NOSPACE, &sk->sk_socket->flags);\n\t\tqueue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);\n\t}\n\tread_unlock_bh(&sk->sk_callback_lock);\n}\n\nstatic void nvme_tcp_state_change(struct sock *sk)\n{\n\tstruct nvme_tcp_queue *queue;\n\n\tread_lock_bh(&sk->sk_callback_lock);\n\tqueue = sk->sk_user_data;\n\tif (!queue)\n\t\tgoto done;\n\n\tswitch (sk->sk_state) {\n\tcase TCP_CLOSE:\n\tcase TCP_CLOSE_WAIT:\n\tcase TCP_LAST_ACK:\n\tcase TCP_FIN_WAIT1:\n\tcase TCP_FIN_WAIT2:\n\t\tnvme_tcp_error_recovery(&queue->ctrl->ctrl);\n\t\tbreak;\n\tdefault:\n\t\tdev_info(queue->ctrl->ctrl.device,\n\t\t\t\"queue %d socket state %d\\n\",\n\t\t\tnvme_tcp_queue_id(queue), sk->sk_state);\n\t}\n\n\tqueue->state_change(sk);\ndone:\n\tread_unlock_bh(&sk->sk_callback_lock);\n}\n\nstatic inline void nvme_tcp_done_send_req(struct nvme_tcp_queue *queue)\n{\n\tqueue->request = NULL;\n}\n\nstatic void nvme_tcp_fail_request(struct nvme_tcp_request *req)\n{\n\tif (nvme_tcp_async_req(req)) {\n\t\tunion nvme_result res = {};\n\n\t\tnvme_complete_async_event(&req->queue->ctrl->ctrl,\n\t\t\t\tcpu_to_le16(NVME_SC_HOST_PATH_ERROR), &res);\n\t} else {\n\t\tnvme_tcp_end_request(blk_mq_rq_from_pdu(req),\n\t\t\t\tNVME_SC_HOST_PATH_ERROR);\n\t}\n}\n\nstatic int nvme_tcp_try_send_data(struct nvme_tcp_request *req)\n{\n\tstruct nvme_tcp_queue *queue = req->queue;\n\tint req_data_len = req->data_len;\n\tu32 h2cdata_left = req->h2cdata_left;\n\n\twhile (true) {\n\t\tstruct bio_vec bvec;\n\t\tstruct msghdr msg = {\n\t\t\t.msg_flags = MSG_DONTWAIT | MSG_SPLICE_PAGES,\n\t\t};\n\t\tstruct page *page = nvme_tcp_req_cur_page(req);\n\t\tsize_t offset = nvme_tcp_req_cur_offset(req);\n\t\tsize_t len = nvme_tcp_req_cur_length(req);\n\t\tbool last = nvme_tcp_pdu_last_send(req, len);\n\t\tint req_data_sent = req->data_sent;\n\t\tint ret;\n\n\t\tif (last && !queue->data_digest && !nvme_tcp_queue_more(queue))\n\t\t\tmsg.msg_flags |= MSG_EOR;\n\t\telse\n\t\t\tmsg.msg_flags |= MSG_MORE;\n\n\t\tif (!sendpage_ok(page))\n\t\t\tmsg.msg_flags &= ~MSG_SPLICE_PAGES;\n\n\t\tbvec_set_page(&bvec, page, len, offset);\n\t\tiov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, len);\n\t\tret = sock_sendmsg(queue->sock, &msg);\n\t\tif (ret <= 0)\n\t\t\treturn ret;\n\n\t\tif (queue->data_digest)\n\t\t\tnvme_tcp_ddgst_update(queue->snd_hash, page,\n\t\t\t\t\toffset, ret);\n\n\t\t \n\t\tif (req_data_sent + ret < req_data_len)\n\t\t\tnvme_tcp_advance_req(req, ret);\n\n\t\t \n\t\tif (last && ret == len) {\n\t\t\tif (queue->data_digest) {\n\t\t\t\tnvme_tcp_ddgst_final(queue->snd_hash,\n\t\t\t\t\t&req->ddgst);\n\t\t\t\treq->state = NVME_TCP_SEND_DDGST;\n\t\t\t\treq->offset = 0;\n\t\t\t} else {\n\t\t\t\tif (h2cdata_left)\n\t\t\t\t\tnvme_tcp_setup_h2c_data_pdu(req);\n\t\t\t\telse\n\t\t\t\t\tnvme_tcp_done_send_req(queue);\n\t\t\t}\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn -EAGAIN;\n}\n\nstatic int nvme_tcp_try_send_cmd_pdu(struct nvme_tcp_request *req)\n{\n\tstruct nvme_tcp_queue *queue = req->queue;\n\tstruct nvme_tcp_cmd_pdu *pdu = nvme_tcp_req_cmd_pdu(req);\n\tstruct bio_vec bvec;\n\tstruct msghdr msg = { .msg_flags = MSG_DONTWAIT | MSG_SPLICE_PAGES, };\n\tbool inline_data = nvme_tcp_has_inline_data(req);\n\tu8 hdgst = nvme_tcp_hdgst_len(queue);\n\tint len = sizeof(*pdu) + hdgst - req->offset;\n\tint ret;\n\n\tif (inline_data || nvme_tcp_queue_more(queue))\n\t\tmsg.msg_flags |= MSG_MORE;\n\telse\n\t\tmsg.msg_flags |= MSG_EOR;\n\n\tif (queue->hdr_digest && !req->offset)\n\t\tnvme_tcp_hdgst(queue->snd_hash, pdu, sizeof(*pdu));\n\n\tbvec_set_virt(&bvec, (void *)pdu + req->offset, len);\n\tiov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, len);\n\tret = sock_sendmsg(queue->sock, &msg);\n\tif (unlikely(ret <= 0))\n\t\treturn ret;\n\n\tlen -= ret;\n\tif (!len) {\n\t\tif (inline_data) {\n\t\t\treq->state = NVME_TCP_SEND_DATA;\n\t\t\tif (queue->data_digest)\n\t\t\t\tcrypto_ahash_init(queue->snd_hash);\n\t\t} else {\n\t\t\tnvme_tcp_done_send_req(queue);\n\t\t}\n\t\treturn 1;\n\t}\n\treq->offset += ret;\n\n\treturn -EAGAIN;\n}\n\nstatic int nvme_tcp_try_send_data_pdu(struct nvme_tcp_request *req)\n{\n\tstruct nvme_tcp_queue *queue = req->queue;\n\tstruct nvme_tcp_data_pdu *pdu = nvme_tcp_req_data_pdu(req);\n\tstruct bio_vec bvec;\n\tstruct msghdr msg = { .msg_flags = MSG_DONTWAIT | MSG_MORE, };\n\tu8 hdgst = nvme_tcp_hdgst_len(queue);\n\tint len = sizeof(*pdu) - req->offset + hdgst;\n\tint ret;\n\n\tif (queue->hdr_digest && !req->offset)\n\t\tnvme_tcp_hdgst(queue->snd_hash, pdu, sizeof(*pdu));\n\n\tif (!req->h2cdata_left)\n\t\tmsg.msg_flags |= MSG_SPLICE_PAGES;\n\n\tbvec_set_virt(&bvec, (void *)pdu + req->offset, len);\n\tiov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, len);\n\tret = sock_sendmsg(queue->sock, &msg);\n\tif (unlikely(ret <= 0))\n\t\treturn ret;\n\n\tlen -= ret;\n\tif (!len) {\n\t\treq->state = NVME_TCP_SEND_DATA;\n\t\tif (queue->data_digest)\n\t\t\tcrypto_ahash_init(queue->snd_hash);\n\t\treturn 1;\n\t}\n\treq->offset += ret;\n\n\treturn -EAGAIN;\n}\n\nstatic int nvme_tcp_try_send_ddgst(struct nvme_tcp_request *req)\n{\n\tstruct nvme_tcp_queue *queue = req->queue;\n\tsize_t offset = req->offset;\n\tu32 h2cdata_left = req->h2cdata_left;\n\tint ret;\n\tstruct msghdr msg = { .msg_flags = MSG_DONTWAIT };\n\tstruct kvec iov = {\n\t\t.iov_base = (u8 *)&req->ddgst + req->offset,\n\t\t.iov_len = NVME_TCP_DIGEST_LENGTH - req->offset\n\t};\n\n\tif (nvme_tcp_queue_more(queue))\n\t\tmsg.msg_flags |= MSG_MORE;\n\telse\n\t\tmsg.msg_flags |= MSG_EOR;\n\n\tret = kernel_sendmsg(queue->sock, &msg, &iov, 1, iov.iov_len);\n\tif (unlikely(ret <= 0))\n\t\treturn ret;\n\n\tif (offset + ret == NVME_TCP_DIGEST_LENGTH) {\n\t\tif (h2cdata_left)\n\t\t\tnvme_tcp_setup_h2c_data_pdu(req);\n\t\telse\n\t\t\tnvme_tcp_done_send_req(queue);\n\t\treturn 1;\n\t}\n\n\treq->offset += ret;\n\treturn -EAGAIN;\n}\n\nstatic int nvme_tcp_try_send(struct nvme_tcp_queue *queue)\n{\n\tstruct nvme_tcp_request *req;\n\tunsigned int noreclaim_flag;\n\tint ret = 1;\n\n\tif (!queue->request) {\n\t\tqueue->request = nvme_tcp_fetch_request(queue);\n\t\tif (!queue->request)\n\t\t\treturn 0;\n\t}\n\treq = queue->request;\n\n\tnoreclaim_flag = memalloc_noreclaim_save();\n\tif (req->state == NVME_TCP_SEND_CMD_PDU) {\n\t\tret = nvme_tcp_try_send_cmd_pdu(req);\n\t\tif (ret <= 0)\n\t\t\tgoto done;\n\t\tif (!nvme_tcp_has_inline_data(req))\n\t\t\tgoto out;\n\t}\n\n\tif (req->state == NVME_TCP_SEND_H2C_PDU) {\n\t\tret = nvme_tcp_try_send_data_pdu(req);\n\t\tif (ret <= 0)\n\t\t\tgoto done;\n\t}\n\n\tif (req->state == NVME_TCP_SEND_DATA) {\n\t\tret = nvme_tcp_try_send_data(req);\n\t\tif (ret <= 0)\n\t\t\tgoto done;\n\t}\n\n\tif (req->state == NVME_TCP_SEND_DDGST)\n\t\tret = nvme_tcp_try_send_ddgst(req);\ndone:\n\tif (ret == -EAGAIN) {\n\t\tret = 0;\n\t} else if (ret < 0) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"failed to send request %d\\n\", ret);\n\t\tnvme_tcp_fail_request(queue->request);\n\t\tnvme_tcp_done_send_req(queue);\n\t}\nout:\n\tmemalloc_noreclaim_restore(noreclaim_flag);\n\treturn ret;\n}\n\nstatic int nvme_tcp_try_recv(struct nvme_tcp_queue *queue)\n{\n\tstruct socket *sock = queue->sock;\n\tstruct sock *sk = sock->sk;\n\tread_descriptor_t rd_desc;\n\tint consumed;\n\n\trd_desc.arg.data = queue;\n\trd_desc.count = 1;\n\tlock_sock(sk);\n\tqueue->nr_cqe = 0;\n\tconsumed = sock->ops->read_sock(sk, &rd_desc, nvme_tcp_recv_skb);\n\trelease_sock(sk);\n\treturn consumed;\n}\n\nstatic void nvme_tcp_io_work(struct work_struct *w)\n{\n\tstruct nvme_tcp_queue *queue =\n\t\tcontainer_of(w, struct nvme_tcp_queue, io_work);\n\tunsigned long deadline = jiffies + msecs_to_jiffies(1);\n\n\tdo {\n\t\tbool pending = false;\n\t\tint result;\n\n\t\tif (mutex_trylock(&queue->send_mutex)) {\n\t\t\tresult = nvme_tcp_try_send(queue);\n\t\t\tmutex_unlock(&queue->send_mutex);\n\t\t\tif (result > 0)\n\t\t\t\tpending = true;\n\t\t\telse if (unlikely(result < 0))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tresult = nvme_tcp_try_recv(queue);\n\t\tif (result > 0)\n\t\t\tpending = true;\n\t\telse if (unlikely(result < 0))\n\t\t\treturn;\n\n\t\tif (!pending || !queue->rd_enabled)\n\t\t\treturn;\n\n\t} while (!time_after(jiffies, deadline));  \n\n\tqueue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);\n}\n\nstatic void nvme_tcp_free_crypto(struct nvme_tcp_queue *queue)\n{\n\tstruct crypto_ahash *tfm = crypto_ahash_reqtfm(queue->rcv_hash);\n\n\tahash_request_free(queue->rcv_hash);\n\tahash_request_free(queue->snd_hash);\n\tcrypto_free_ahash(tfm);\n}\n\nstatic int nvme_tcp_alloc_crypto(struct nvme_tcp_queue *queue)\n{\n\tstruct crypto_ahash *tfm;\n\n\ttfm = crypto_alloc_ahash(\"crc32c\", 0, CRYPTO_ALG_ASYNC);\n\tif (IS_ERR(tfm))\n\t\treturn PTR_ERR(tfm);\n\n\tqueue->snd_hash = ahash_request_alloc(tfm, GFP_KERNEL);\n\tif (!queue->snd_hash)\n\t\tgoto free_tfm;\n\tahash_request_set_callback(queue->snd_hash, 0, NULL, NULL);\n\n\tqueue->rcv_hash = ahash_request_alloc(tfm, GFP_KERNEL);\n\tif (!queue->rcv_hash)\n\t\tgoto free_snd_hash;\n\tahash_request_set_callback(queue->rcv_hash, 0, NULL, NULL);\n\n\treturn 0;\nfree_snd_hash:\n\tahash_request_free(queue->snd_hash);\nfree_tfm:\n\tcrypto_free_ahash(tfm);\n\treturn -ENOMEM;\n}\n\nstatic void nvme_tcp_free_async_req(struct nvme_tcp_ctrl *ctrl)\n{\n\tstruct nvme_tcp_request *async = &ctrl->async_req;\n\n\tpage_frag_free(async->pdu);\n}\n\nstatic int nvme_tcp_alloc_async_req(struct nvme_tcp_ctrl *ctrl)\n{\n\tstruct nvme_tcp_queue *queue = &ctrl->queues[0];\n\tstruct nvme_tcp_request *async = &ctrl->async_req;\n\tu8 hdgst = nvme_tcp_hdgst_len(queue);\n\n\tasync->pdu = page_frag_alloc(&queue->pf_cache,\n\t\tsizeof(struct nvme_tcp_cmd_pdu) + hdgst,\n\t\tGFP_KERNEL | __GFP_ZERO);\n\tif (!async->pdu)\n\t\treturn -ENOMEM;\n\n\tasync->queue = &ctrl->queues[0];\n\treturn 0;\n}\n\nstatic void nvme_tcp_free_queue(struct nvme_ctrl *nctrl, int qid)\n{\n\tstruct page *page;\n\tstruct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(nctrl);\n\tstruct nvme_tcp_queue *queue = &ctrl->queues[qid];\n\tunsigned int noreclaim_flag;\n\n\tif (!test_and_clear_bit(NVME_TCP_Q_ALLOCATED, &queue->flags))\n\t\treturn;\n\n\tif (queue->hdr_digest || queue->data_digest)\n\t\tnvme_tcp_free_crypto(queue);\n\n\tif (queue->pf_cache.va) {\n\t\tpage = virt_to_head_page(queue->pf_cache.va);\n\t\t__page_frag_cache_drain(page, queue->pf_cache.pagecnt_bias);\n\t\tqueue->pf_cache.va = NULL;\n\t}\n\n\tnoreclaim_flag = memalloc_noreclaim_save();\n\tsock_release(queue->sock);\n\tmemalloc_noreclaim_restore(noreclaim_flag);\n\n\tkfree(queue->pdu);\n\tmutex_destroy(&queue->send_mutex);\n\tmutex_destroy(&queue->queue_lock);\n}\n\nstatic int nvme_tcp_init_connection(struct nvme_tcp_queue *queue)\n{\n\tstruct nvme_tcp_icreq_pdu *icreq;\n\tstruct nvme_tcp_icresp_pdu *icresp;\n\tstruct msghdr msg = {};\n\tstruct kvec iov;\n\tbool ctrl_hdgst, ctrl_ddgst;\n\tu32 maxh2cdata;\n\tint ret;\n\n\ticreq = kzalloc(sizeof(*icreq), GFP_KERNEL);\n\tif (!icreq)\n\t\treturn -ENOMEM;\n\n\ticresp = kzalloc(sizeof(*icresp), GFP_KERNEL);\n\tif (!icresp) {\n\t\tret = -ENOMEM;\n\t\tgoto free_icreq;\n\t}\n\n\ticreq->hdr.type = nvme_tcp_icreq;\n\ticreq->hdr.hlen = sizeof(*icreq);\n\ticreq->hdr.pdo = 0;\n\ticreq->hdr.plen = cpu_to_le32(icreq->hdr.hlen);\n\ticreq->pfv = cpu_to_le16(NVME_TCP_PFV_1_0);\n\ticreq->maxr2t = 0;  \n\ticreq->hpda = 0;  \n\tif (queue->hdr_digest)\n\t\ticreq->digest |= NVME_TCP_HDR_DIGEST_ENABLE;\n\tif (queue->data_digest)\n\t\ticreq->digest |= NVME_TCP_DATA_DIGEST_ENABLE;\n\n\tiov.iov_base = icreq;\n\tiov.iov_len = sizeof(*icreq);\n\tret = kernel_sendmsg(queue->sock, &msg, &iov, 1, iov.iov_len);\n\tif (ret < 0)\n\t\tgoto free_icresp;\n\n\tmemset(&msg, 0, sizeof(msg));\n\tiov.iov_base = icresp;\n\tiov.iov_len = sizeof(*icresp);\n\tret = kernel_recvmsg(queue->sock, &msg, &iov, 1,\n\t\t\tiov.iov_len, msg.msg_flags);\n\tif (ret < 0)\n\t\tgoto free_icresp;\n\n\tret = -EINVAL;\n\tif (icresp->hdr.type != nvme_tcp_icresp) {\n\t\tpr_err(\"queue %d: bad type returned %d\\n\",\n\t\t\tnvme_tcp_queue_id(queue), icresp->hdr.type);\n\t\tgoto free_icresp;\n\t}\n\n\tif (le32_to_cpu(icresp->hdr.plen) != sizeof(*icresp)) {\n\t\tpr_err(\"queue %d: bad pdu length returned %d\\n\",\n\t\t\tnvme_tcp_queue_id(queue), icresp->hdr.plen);\n\t\tgoto free_icresp;\n\t}\n\n\tif (icresp->pfv != NVME_TCP_PFV_1_0) {\n\t\tpr_err(\"queue %d: bad pfv returned %d\\n\",\n\t\t\tnvme_tcp_queue_id(queue), icresp->pfv);\n\t\tgoto free_icresp;\n\t}\n\n\tctrl_ddgst = !!(icresp->digest & NVME_TCP_DATA_DIGEST_ENABLE);\n\tif ((queue->data_digest && !ctrl_ddgst) ||\n\t    (!queue->data_digest && ctrl_ddgst)) {\n\t\tpr_err(\"queue %d: data digest mismatch host: %s ctrl: %s\\n\",\n\t\t\tnvme_tcp_queue_id(queue),\n\t\t\tqueue->data_digest ? \"enabled\" : \"disabled\",\n\t\t\tctrl_ddgst ? \"enabled\" : \"disabled\");\n\t\tgoto free_icresp;\n\t}\n\n\tctrl_hdgst = !!(icresp->digest & NVME_TCP_HDR_DIGEST_ENABLE);\n\tif ((queue->hdr_digest && !ctrl_hdgst) ||\n\t    (!queue->hdr_digest && ctrl_hdgst)) {\n\t\tpr_err(\"queue %d: header digest mismatch host: %s ctrl: %s\\n\",\n\t\t\tnvme_tcp_queue_id(queue),\n\t\t\tqueue->hdr_digest ? \"enabled\" : \"disabled\",\n\t\t\tctrl_hdgst ? \"enabled\" : \"disabled\");\n\t\tgoto free_icresp;\n\t}\n\n\tif (icresp->cpda != 0) {\n\t\tpr_err(\"queue %d: unsupported cpda returned %d\\n\",\n\t\t\tnvme_tcp_queue_id(queue), icresp->cpda);\n\t\tgoto free_icresp;\n\t}\n\n\tmaxh2cdata = le32_to_cpu(icresp->maxdata);\n\tif ((maxh2cdata % 4) || (maxh2cdata < NVME_TCP_MIN_MAXH2CDATA)) {\n\t\tpr_err(\"queue %d: invalid maxh2cdata returned %u\\n\",\n\t\t       nvme_tcp_queue_id(queue), maxh2cdata);\n\t\tgoto free_icresp;\n\t}\n\tqueue->maxh2cdata = maxh2cdata;\n\n\tret = 0;\nfree_icresp:\n\tkfree(icresp);\nfree_icreq:\n\tkfree(icreq);\n\treturn ret;\n}\n\nstatic bool nvme_tcp_admin_queue(struct nvme_tcp_queue *queue)\n{\n\treturn nvme_tcp_queue_id(queue) == 0;\n}\n\nstatic bool nvme_tcp_default_queue(struct nvme_tcp_queue *queue)\n{\n\tstruct nvme_tcp_ctrl *ctrl = queue->ctrl;\n\tint qid = nvme_tcp_queue_id(queue);\n\n\treturn !nvme_tcp_admin_queue(queue) &&\n\t\tqid < 1 + ctrl->io_queues[HCTX_TYPE_DEFAULT];\n}\n\nstatic bool nvme_tcp_read_queue(struct nvme_tcp_queue *queue)\n{\n\tstruct nvme_tcp_ctrl *ctrl = queue->ctrl;\n\tint qid = nvme_tcp_queue_id(queue);\n\n\treturn !nvme_tcp_admin_queue(queue) &&\n\t\t!nvme_tcp_default_queue(queue) &&\n\t\tqid < 1 + ctrl->io_queues[HCTX_TYPE_DEFAULT] +\n\t\t\t  ctrl->io_queues[HCTX_TYPE_READ];\n}\n\nstatic bool nvme_tcp_poll_queue(struct nvme_tcp_queue *queue)\n{\n\tstruct nvme_tcp_ctrl *ctrl = queue->ctrl;\n\tint qid = nvme_tcp_queue_id(queue);\n\n\treturn !nvme_tcp_admin_queue(queue) &&\n\t\t!nvme_tcp_default_queue(queue) &&\n\t\t!nvme_tcp_read_queue(queue) &&\n\t\tqid < 1 + ctrl->io_queues[HCTX_TYPE_DEFAULT] +\n\t\t\t  ctrl->io_queues[HCTX_TYPE_READ] +\n\t\t\t  ctrl->io_queues[HCTX_TYPE_POLL];\n}\n\nstatic void nvme_tcp_set_queue_io_cpu(struct nvme_tcp_queue *queue)\n{\n\tstruct nvme_tcp_ctrl *ctrl = queue->ctrl;\n\tint qid = nvme_tcp_queue_id(queue);\n\tint n = 0;\n\n\tif (nvme_tcp_default_queue(queue))\n\t\tn = qid - 1;\n\telse if (nvme_tcp_read_queue(queue))\n\t\tn = qid - ctrl->io_queues[HCTX_TYPE_DEFAULT] - 1;\n\telse if (nvme_tcp_poll_queue(queue))\n\t\tn = qid - ctrl->io_queues[HCTX_TYPE_DEFAULT] -\n\t\t\t\tctrl->io_queues[HCTX_TYPE_READ] - 1;\n\tqueue->io_cpu = cpumask_next_wrap(n - 1, cpu_online_mask, -1, false);\n}\n\nstatic int nvme_tcp_alloc_queue(struct nvme_ctrl *nctrl, int qid)\n{\n\tstruct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(nctrl);\n\tstruct nvme_tcp_queue *queue = &ctrl->queues[qid];\n\tint ret, rcv_pdu_size;\n\n\tmutex_init(&queue->queue_lock);\n\tqueue->ctrl = ctrl;\n\tinit_llist_head(&queue->req_list);\n\tINIT_LIST_HEAD(&queue->send_list);\n\tmutex_init(&queue->send_mutex);\n\tINIT_WORK(&queue->io_work, nvme_tcp_io_work);\n\n\tif (qid > 0)\n\t\tqueue->cmnd_capsule_len = nctrl->ioccsz * 16;\n\telse\n\t\tqueue->cmnd_capsule_len = sizeof(struct nvme_command) +\n\t\t\t\t\t\tNVME_TCP_ADMIN_CCSZ;\n\n\tret = sock_create(ctrl->addr.ss_family, SOCK_STREAM,\n\t\t\tIPPROTO_TCP, &queue->sock);\n\tif (ret) {\n\t\tdev_err(nctrl->device,\n\t\t\t\"failed to create socket: %d\\n\", ret);\n\t\tgoto err_destroy_mutex;\n\t}\n\n\tnvme_tcp_reclassify_socket(queue->sock);\n\n\t \n\ttcp_sock_set_syncnt(queue->sock->sk, 1);\n\n\t \n\ttcp_sock_set_nodelay(queue->sock->sk);\n\n\t \n\tsock_no_linger(queue->sock->sk);\n\n\tif (so_priority > 0)\n\t\tsock_set_priority(queue->sock->sk, so_priority);\n\n\t \n\tif (nctrl->opts->tos >= 0)\n\t\tip_sock_set_tos(queue->sock->sk, nctrl->opts->tos);\n\n\t \n\tqueue->sock->sk->sk_rcvtimeo = 10 * HZ;\n\n\tqueue->sock->sk->sk_allocation = GFP_ATOMIC;\n\tqueue->sock->sk->sk_use_task_frag = false;\n\tnvme_tcp_set_queue_io_cpu(queue);\n\tqueue->request = NULL;\n\tqueue->data_remaining = 0;\n\tqueue->ddgst_remaining = 0;\n\tqueue->pdu_remaining = 0;\n\tqueue->pdu_offset = 0;\n\tsk_set_memalloc(queue->sock->sk);\n\n\tif (nctrl->opts->mask & NVMF_OPT_HOST_TRADDR) {\n\t\tret = kernel_bind(queue->sock, (struct sockaddr *)&ctrl->src_addr,\n\t\t\tsizeof(ctrl->src_addr));\n\t\tif (ret) {\n\t\t\tdev_err(nctrl->device,\n\t\t\t\t\"failed to bind queue %d socket %d\\n\",\n\t\t\t\tqid, ret);\n\t\t\tgoto err_sock;\n\t\t}\n\t}\n\n\tif (nctrl->opts->mask & NVMF_OPT_HOST_IFACE) {\n\t\tchar *iface = nctrl->opts->host_iface;\n\t\tsockptr_t optval = KERNEL_SOCKPTR(iface);\n\n\t\tret = sock_setsockopt(queue->sock, SOL_SOCKET, SO_BINDTODEVICE,\n\t\t\t\t      optval, strlen(iface));\n\t\tif (ret) {\n\t\t\tdev_err(nctrl->device,\n\t\t\t  \"failed to bind to interface %s queue %d err %d\\n\",\n\t\t\t  iface, qid, ret);\n\t\t\tgoto err_sock;\n\t\t}\n\t}\n\n\tqueue->hdr_digest = nctrl->opts->hdr_digest;\n\tqueue->data_digest = nctrl->opts->data_digest;\n\tif (queue->hdr_digest || queue->data_digest) {\n\t\tret = nvme_tcp_alloc_crypto(queue);\n\t\tif (ret) {\n\t\t\tdev_err(nctrl->device,\n\t\t\t\t\"failed to allocate queue %d crypto\\n\", qid);\n\t\t\tgoto err_sock;\n\t\t}\n\t}\n\n\trcv_pdu_size = sizeof(struct nvme_tcp_rsp_pdu) +\n\t\t\tnvme_tcp_hdgst_len(queue);\n\tqueue->pdu = kmalloc(rcv_pdu_size, GFP_KERNEL);\n\tif (!queue->pdu) {\n\t\tret = -ENOMEM;\n\t\tgoto err_crypto;\n\t}\n\n\tdev_dbg(nctrl->device, \"connecting queue %d\\n\",\n\t\t\tnvme_tcp_queue_id(queue));\n\n\tret = kernel_connect(queue->sock, (struct sockaddr *)&ctrl->addr,\n\t\tsizeof(ctrl->addr), 0);\n\tif (ret) {\n\t\tdev_err(nctrl->device,\n\t\t\t\"failed to connect socket: %d\\n\", ret);\n\t\tgoto err_rcv_pdu;\n\t}\n\n\tret = nvme_tcp_init_connection(queue);\n\tif (ret)\n\t\tgoto err_init_connect;\n\n\tset_bit(NVME_TCP_Q_ALLOCATED, &queue->flags);\n\n\treturn 0;\n\nerr_init_connect:\n\tkernel_sock_shutdown(queue->sock, SHUT_RDWR);\nerr_rcv_pdu:\n\tkfree(queue->pdu);\nerr_crypto:\n\tif (queue->hdr_digest || queue->data_digest)\n\t\tnvme_tcp_free_crypto(queue);\nerr_sock:\n\tsock_release(queue->sock);\n\tqueue->sock = NULL;\nerr_destroy_mutex:\n\tmutex_destroy(&queue->send_mutex);\n\tmutex_destroy(&queue->queue_lock);\n\treturn ret;\n}\n\nstatic void nvme_tcp_restore_sock_ops(struct nvme_tcp_queue *queue)\n{\n\tstruct socket *sock = queue->sock;\n\n\twrite_lock_bh(&sock->sk->sk_callback_lock);\n\tsock->sk->sk_user_data  = NULL;\n\tsock->sk->sk_data_ready = queue->data_ready;\n\tsock->sk->sk_state_change = queue->state_change;\n\tsock->sk->sk_write_space  = queue->write_space;\n\twrite_unlock_bh(&sock->sk->sk_callback_lock);\n}\n\nstatic void __nvme_tcp_stop_queue(struct nvme_tcp_queue *queue)\n{\n\tkernel_sock_shutdown(queue->sock, SHUT_RDWR);\n\tnvme_tcp_restore_sock_ops(queue);\n\tcancel_work_sync(&queue->io_work);\n}\n\nstatic void nvme_tcp_stop_queue(struct nvme_ctrl *nctrl, int qid)\n{\n\tstruct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(nctrl);\n\tstruct nvme_tcp_queue *queue = &ctrl->queues[qid];\n\n\tif (!test_bit(NVME_TCP_Q_ALLOCATED, &queue->flags))\n\t\treturn;\n\n\tmutex_lock(&queue->queue_lock);\n\tif (test_and_clear_bit(NVME_TCP_Q_LIVE, &queue->flags))\n\t\t__nvme_tcp_stop_queue(queue);\n\tmutex_unlock(&queue->queue_lock);\n}\n\nstatic void nvme_tcp_setup_sock_ops(struct nvme_tcp_queue *queue)\n{\n\twrite_lock_bh(&queue->sock->sk->sk_callback_lock);\n\tqueue->sock->sk->sk_user_data = queue;\n\tqueue->state_change = queue->sock->sk->sk_state_change;\n\tqueue->data_ready = queue->sock->sk->sk_data_ready;\n\tqueue->write_space = queue->sock->sk->sk_write_space;\n\tqueue->sock->sk->sk_data_ready = nvme_tcp_data_ready;\n\tqueue->sock->sk->sk_state_change = nvme_tcp_state_change;\n\tqueue->sock->sk->sk_write_space = nvme_tcp_write_space;\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\tqueue->sock->sk->sk_ll_usec = 1;\n#endif\n\twrite_unlock_bh(&queue->sock->sk->sk_callback_lock);\n}\n\nstatic int nvme_tcp_start_queue(struct nvme_ctrl *nctrl, int idx)\n{\n\tstruct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(nctrl);\n\tstruct nvme_tcp_queue *queue = &ctrl->queues[idx];\n\tint ret;\n\n\tqueue->rd_enabled = true;\n\tnvme_tcp_init_recv_ctx(queue);\n\tnvme_tcp_setup_sock_ops(queue);\n\n\tif (idx)\n\t\tret = nvmf_connect_io_queue(nctrl, idx);\n\telse\n\t\tret = nvmf_connect_admin_queue(nctrl);\n\n\tif (!ret) {\n\t\tset_bit(NVME_TCP_Q_LIVE, &queue->flags);\n\t} else {\n\t\tif (test_bit(NVME_TCP_Q_ALLOCATED, &queue->flags))\n\t\t\t__nvme_tcp_stop_queue(queue);\n\t\tdev_err(nctrl->device,\n\t\t\t\"failed to connect queue: %d ret=%d\\n\", idx, ret);\n\t}\n\treturn ret;\n}\n\nstatic void nvme_tcp_free_admin_queue(struct nvme_ctrl *ctrl)\n{\n\tif (to_tcp_ctrl(ctrl)->async_req.pdu) {\n\t\tcancel_work_sync(&ctrl->async_event_work);\n\t\tnvme_tcp_free_async_req(to_tcp_ctrl(ctrl));\n\t\tto_tcp_ctrl(ctrl)->async_req.pdu = NULL;\n\t}\n\n\tnvme_tcp_free_queue(ctrl, 0);\n}\n\nstatic void nvme_tcp_free_io_queues(struct nvme_ctrl *ctrl)\n{\n\tint i;\n\n\tfor (i = 1; i < ctrl->queue_count; i++)\n\t\tnvme_tcp_free_queue(ctrl, i);\n}\n\nstatic void nvme_tcp_stop_io_queues(struct nvme_ctrl *ctrl)\n{\n\tint i;\n\n\tfor (i = 1; i < ctrl->queue_count; i++)\n\t\tnvme_tcp_stop_queue(ctrl, i);\n}\n\nstatic int nvme_tcp_start_io_queues(struct nvme_ctrl *ctrl,\n\t\t\t\t    int first, int last)\n{\n\tint i, ret;\n\n\tfor (i = first; i < last; i++) {\n\t\tret = nvme_tcp_start_queue(ctrl, i);\n\t\tif (ret)\n\t\t\tgoto out_stop_queues;\n\t}\n\n\treturn 0;\n\nout_stop_queues:\n\tfor (i--; i >= first; i--)\n\t\tnvme_tcp_stop_queue(ctrl, i);\n\treturn ret;\n}\n\nstatic int nvme_tcp_alloc_admin_queue(struct nvme_ctrl *ctrl)\n{\n\tint ret;\n\n\tret = nvme_tcp_alloc_queue(ctrl, 0);\n\tif (ret)\n\t\treturn ret;\n\n\tret = nvme_tcp_alloc_async_req(to_tcp_ctrl(ctrl));\n\tif (ret)\n\t\tgoto out_free_queue;\n\n\treturn 0;\n\nout_free_queue:\n\tnvme_tcp_free_queue(ctrl, 0);\n\treturn ret;\n}\n\nstatic int __nvme_tcp_alloc_io_queues(struct nvme_ctrl *ctrl)\n{\n\tint i, ret;\n\n\tfor (i = 1; i < ctrl->queue_count; i++) {\n\t\tret = nvme_tcp_alloc_queue(ctrl, i);\n\t\tif (ret)\n\t\t\tgoto out_free_queues;\n\t}\n\n\treturn 0;\n\nout_free_queues:\n\tfor (i--; i >= 1; i--)\n\t\tnvme_tcp_free_queue(ctrl, i);\n\n\treturn ret;\n}\n\nstatic int nvme_tcp_alloc_io_queues(struct nvme_ctrl *ctrl)\n{\n\tunsigned int nr_io_queues;\n\tint ret;\n\n\tnr_io_queues = nvmf_nr_io_queues(ctrl->opts);\n\tret = nvme_set_queue_count(ctrl, &nr_io_queues);\n\tif (ret)\n\t\treturn ret;\n\n\tif (nr_io_queues == 0) {\n\t\tdev_err(ctrl->device,\n\t\t\t\"unable to set any I/O queues\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tctrl->queue_count = nr_io_queues + 1;\n\tdev_info(ctrl->device,\n\t\t\"creating %d I/O queues.\\n\", nr_io_queues);\n\n\tnvmf_set_io_queues(ctrl->opts, nr_io_queues,\n\t\t\t   to_tcp_ctrl(ctrl)->io_queues);\n\treturn __nvme_tcp_alloc_io_queues(ctrl);\n}\n\nstatic void nvme_tcp_destroy_io_queues(struct nvme_ctrl *ctrl, bool remove)\n{\n\tnvme_tcp_stop_io_queues(ctrl);\n\tif (remove)\n\t\tnvme_remove_io_tag_set(ctrl);\n\tnvme_tcp_free_io_queues(ctrl);\n}\n\nstatic int nvme_tcp_configure_io_queues(struct nvme_ctrl *ctrl, bool new)\n{\n\tint ret, nr_queues;\n\n\tret = nvme_tcp_alloc_io_queues(ctrl);\n\tif (ret)\n\t\treturn ret;\n\n\tif (new) {\n\t\tret = nvme_alloc_io_tag_set(ctrl, &to_tcp_ctrl(ctrl)->tag_set,\n\t\t\t\t&nvme_tcp_mq_ops,\n\t\t\t\tctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2,\n\t\t\t\tsizeof(struct nvme_tcp_request));\n\t\tif (ret)\n\t\t\tgoto out_free_io_queues;\n\t}\n\n\t \n\tnr_queues = min(ctrl->tagset->nr_hw_queues + 1, ctrl->queue_count);\n\tret = nvme_tcp_start_io_queues(ctrl, 1, nr_queues);\n\tif (ret)\n\t\tgoto out_cleanup_connect_q;\n\n\tif (!new) {\n\t\tnvme_start_freeze(ctrl);\n\t\tnvme_unquiesce_io_queues(ctrl);\n\t\tif (!nvme_wait_freeze_timeout(ctrl, NVME_IO_TIMEOUT)) {\n\t\t\t \n\t\t\tret = -ENODEV;\n\t\t\tnvme_unfreeze(ctrl);\n\t\t\tgoto out_wait_freeze_timed_out;\n\t\t}\n\t\tblk_mq_update_nr_hw_queues(ctrl->tagset,\n\t\t\tctrl->queue_count - 1);\n\t\tnvme_unfreeze(ctrl);\n\t}\n\n\t \n\tret = nvme_tcp_start_io_queues(ctrl, nr_queues,\n\t\t\t\t       ctrl->tagset->nr_hw_queues + 1);\n\tif (ret)\n\t\tgoto out_wait_freeze_timed_out;\n\n\treturn 0;\n\nout_wait_freeze_timed_out:\n\tnvme_quiesce_io_queues(ctrl);\n\tnvme_sync_io_queues(ctrl);\n\tnvme_tcp_stop_io_queues(ctrl);\nout_cleanup_connect_q:\n\tnvme_cancel_tagset(ctrl);\n\tif (new)\n\t\tnvme_remove_io_tag_set(ctrl);\nout_free_io_queues:\n\tnvme_tcp_free_io_queues(ctrl);\n\treturn ret;\n}\n\nstatic void nvme_tcp_destroy_admin_queue(struct nvme_ctrl *ctrl, bool remove)\n{\n\tnvme_tcp_stop_queue(ctrl, 0);\n\tif (remove)\n\t\tnvme_remove_admin_tag_set(ctrl);\n\tnvme_tcp_free_admin_queue(ctrl);\n}\n\nstatic int nvme_tcp_configure_admin_queue(struct nvme_ctrl *ctrl, bool new)\n{\n\tint error;\n\n\terror = nvme_tcp_alloc_admin_queue(ctrl);\n\tif (error)\n\t\treturn error;\n\n\tif (new) {\n\t\terror = nvme_alloc_admin_tag_set(ctrl,\n\t\t\t\t&to_tcp_ctrl(ctrl)->admin_tag_set,\n\t\t\t\t&nvme_tcp_admin_mq_ops,\n\t\t\t\tsizeof(struct nvme_tcp_request));\n\t\tif (error)\n\t\t\tgoto out_free_queue;\n\t}\n\n\terror = nvme_tcp_start_queue(ctrl, 0);\n\tif (error)\n\t\tgoto out_cleanup_tagset;\n\n\terror = nvme_enable_ctrl(ctrl);\n\tif (error)\n\t\tgoto out_stop_queue;\n\n\tnvme_unquiesce_admin_queue(ctrl);\n\n\terror = nvme_init_ctrl_finish(ctrl, false);\n\tif (error)\n\t\tgoto out_quiesce_queue;\n\n\treturn 0;\n\nout_quiesce_queue:\n\tnvme_quiesce_admin_queue(ctrl);\n\tblk_sync_queue(ctrl->admin_q);\nout_stop_queue:\n\tnvme_tcp_stop_queue(ctrl, 0);\n\tnvme_cancel_admin_tagset(ctrl);\nout_cleanup_tagset:\n\tif (new)\n\t\tnvme_remove_admin_tag_set(ctrl);\nout_free_queue:\n\tnvme_tcp_free_admin_queue(ctrl);\n\treturn error;\n}\n\nstatic void nvme_tcp_teardown_admin_queue(struct nvme_ctrl *ctrl,\n\t\tbool remove)\n{\n\tnvme_quiesce_admin_queue(ctrl);\n\tblk_sync_queue(ctrl->admin_q);\n\tnvme_tcp_stop_queue(ctrl, 0);\n\tnvme_cancel_admin_tagset(ctrl);\n\tif (remove)\n\t\tnvme_unquiesce_admin_queue(ctrl);\n\tnvme_tcp_destroy_admin_queue(ctrl, remove);\n}\n\nstatic void nvme_tcp_teardown_io_queues(struct nvme_ctrl *ctrl,\n\t\tbool remove)\n{\n\tif (ctrl->queue_count <= 1)\n\t\treturn;\n\tnvme_quiesce_admin_queue(ctrl);\n\tnvme_quiesce_io_queues(ctrl);\n\tnvme_sync_io_queues(ctrl);\n\tnvme_tcp_stop_io_queues(ctrl);\n\tnvme_cancel_tagset(ctrl);\n\tif (remove)\n\t\tnvme_unquiesce_io_queues(ctrl);\n\tnvme_tcp_destroy_io_queues(ctrl, remove);\n}\n\nstatic void nvme_tcp_reconnect_or_remove(struct nvme_ctrl *ctrl)\n{\n\tenum nvme_ctrl_state state = nvme_ctrl_state(ctrl);\n\n\t \n\tif (state != NVME_CTRL_CONNECTING) {\n\t\tWARN_ON_ONCE(state == NVME_CTRL_NEW || state == NVME_CTRL_LIVE);\n\t\treturn;\n\t}\n\n\tif (nvmf_should_reconnect(ctrl)) {\n\t\tdev_info(ctrl->device, \"Reconnecting in %d seconds...\\n\",\n\t\t\tctrl->opts->reconnect_delay);\n\t\tqueue_delayed_work(nvme_wq, &to_tcp_ctrl(ctrl)->connect_work,\n\t\t\t\tctrl->opts->reconnect_delay * HZ);\n\t} else {\n\t\tdev_info(ctrl->device, \"Removing controller...\\n\");\n\t\tnvme_delete_ctrl(ctrl);\n\t}\n}\n\nstatic int nvme_tcp_setup_ctrl(struct nvme_ctrl *ctrl, bool new)\n{\n\tstruct nvmf_ctrl_options *opts = ctrl->opts;\n\tint ret;\n\n\tret = nvme_tcp_configure_admin_queue(ctrl, new);\n\tif (ret)\n\t\treturn ret;\n\n\tif (ctrl->icdoff) {\n\t\tret = -EOPNOTSUPP;\n\t\tdev_err(ctrl->device, \"icdoff is not supported!\\n\");\n\t\tgoto destroy_admin;\n\t}\n\n\tif (!nvme_ctrl_sgl_supported(ctrl)) {\n\t\tret = -EOPNOTSUPP;\n\t\tdev_err(ctrl->device, \"Mandatory sgls are not supported!\\n\");\n\t\tgoto destroy_admin;\n\t}\n\n\tif (opts->queue_size > ctrl->sqsize + 1)\n\t\tdev_warn(ctrl->device,\n\t\t\t\"queue_size %zu > ctrl sqsize %u, clamping down\\n\",\n\t\t\topts->queue_size, ctrl->sqsize + 1);\n\n\tif (ctrl->sqsize + 1 > ctrl->maxcmd) {\n\t\tdev_warn(ctrl->device,\n\t\t\t\"sqsize %u > ctrl maxcmd %u, clamping down\\n\",\n\t\t\tctrl->sqsize + 1, ctrl->maxcmd);\n\t\tctrl->sqsize = ctrl->maxcmd - 1;\n\t}\n\n\tif (ctrl->queue_count > 1) {\n\t\tret = nvme_tcp_configure_io_queues(ctrl, new);\n\t\tif (ret)\n\t\t\tgoto destroy_admin;\n\t}\n\n\tif (!nvme_change_ctrl_state(ctrl, NVME_CTRL_LIVE)) {\n\t\t \n\t\tenum nvme_ctrl_state state = nvme_ctrl_state(ctrl);\n\n\t\tWARN_ON_ONCE(state != NVME_CTRL_DELETING &&\n\t\t\t     state != NVME_CTRL_DELETING_NOIO);\n\t\tWARN_ON_ONCE(new);\n\t\tret = -EINVAL;\n\t\tgoto destroy_io;\n\t}\n\n\tnvme_start_ctrl(ctrl);\n\treturn 0;\n\ndestroy_io:\n\tif (ctrl->queue_count > 1) {\n\t\tnvme_quiesce_io_queues(ctrl);\n\t\tnvme_sync_io_queues(ctrl);\n\t\tnvme_tcp_stop_io_queues(ctrl);\n\t\tnvme_cancel_tagset(ctrl);\n\t\tnvme_tcp_destroy_io_queues(ctrl, new);\n\t}\ndestroy_admin:\n\tnvme_quiesce_admin_queue(ctrl);\n\tblk_sync_queue(ctrl->admin_q);\n\tnvme_tcp_stop_queue(ctrl, 0);\n\tnvme_cancel_admin_tagset(ctrl);\n\tnvme_tcp_destroy_admin_queue(ctrl, new);\n\treturn ret;\n}\n\nstatic void nvme_tcp_reconnect_ctrl_work(struct work_struct *work)\n{\n\tstruct nvme_tcp_ctrl *tcp_ctrl = container_of(to_delayed_work(work),\n\t\t\tstruct nvme_tcp_ctrl, connect_work);\n\tstruct nvme_ctrl *ctrl = &tcp_ctrl->ctrl;\n\n\t++ctrl->nr_reconnects;\n\n\tif (nvme_tcp_setup_ctrl(ctrl, false))\n\t\tgoto requeue;\n\n\tdev_info(ctrl->device, \"Successfully reconnected (%d attempt)\\n\",\n\t\t\tctrl->nr_reconnects);\n\n\tctrl->nr_reconnects = 0;\n\n\treturn;\n\nrequeue:\n\tdev_info(ctrl->device, \"Failed reconnect attempt %d\\n\",\n\t\t\tctrl->nr_reconnects);\n\tnvme_tcp_reconnect_or_remove(ctrl);\n}\n\nstatic void nvme_tcp_error_recovery_work(struct work_struct *work)\n{\n\tstruct nvme_tcp_ctrl *tcp_ctrl = container_of(work,\n\t\t\t\tstruct nvme_tcp_ctrl, err_work);\n\tstruct nvme_ctrl *ctrl = &tcp_ctrl->ctrl;\n\n\tnvme_stop_keep_alive(ctrl);\n\tflush_work(&ctrl->async_event_work);\n\tnvme_tcp_teardown_io_queues(ctrl, false);\n\t \n\tnvme_unquiesce_io_queues(ctrl);\n\tnvme_tcp_teardown_admin_queue(ctrl, false);\n\tnvme_unquiesce_admin_queue(ctrl);\n\tnvme_auth_stop(ctrl);\n\n\tif (!nvme_change_ctrl_state(ctrl, NVME_CTRL_CONNECTING)) {\n\t\t \n\t\tenum nvme_ctrl_state state = nvme_ctrl_state(ctrl);\n\n\t\tWARN_ON_ONCE(state != NVME_CTRL_DELETING &&\n\t\t\t     state != NVME_CTRL_DELETING_NOIO);\n\t\treturn;\n\t}\n\n\tnvme_tcp_reconnect_or_remove(ctrl);\n}\n\nstatic void nvme_tcp_teardown_ctrl(struct nvme_ctrl *ctrl, bool shutdown)\n{\n\tnvme_tcp_teardown_io_queues(ctrl, shutdown);\n\tnvme_quiesce_admin_queue(ctrl);\n\tnvme_disable_ctrl(ctrl, shutdown);\n\tnvme_tcp_teardown_admin_queue(ctrl, shutdown);\n}\n\nstatic void nvme_tcp_delete_ctrl(struct nvme_ctrl *ctrl)\n{\n\tnvme_tcp_teardown_ctrl(ctrl, true);\n}\n\nstatic void nvme_reset_ctrl_work(struct work_struct *work)\n{\n\tstruct nvme_ctrl *ctrl =\n\t\tcontainer_of(work, struct nvme_ctrl, reset_work);\n\n\tnvme_stop_ctrl(ctrl);\n\tnvme_tcp_teardown_ctrl(ctrl, false);\n\n\tif (!nvme_change_ctrl_state(ctrl, NVME_CTRL_CONNECTING)) {\n\t\t \n\t\tenum nvme_ctrl_state state = nvme_ctrl_state(ctrl);\n\n\t\tWARN_ON_ONCE(state != NVME_CTRL_DELETING &&\n\t\t\t     state != NVME_CTRL_DELETING_NOIO);\n\t\treturn;\n\t}\n\n\tif (nvme_tcp_setup_ctrl(ctrl, false))\n\t\tgoto out_fail;\n\n\treturn;\n\nout_fail:\n\t++ctrl->nr_reconnects;\n\tnvme_tcp_reconnect_or_remove(ctrl);\n}\n\nstatic void nvme_tcp_stop_ctrl(struct nvme_ctrl *ctrl)\n{\n\tflush_work(&to_tcp_ctrl(ctrl)->err_work);\n\tcancel_delayed_work_sync(&to_tcp_ctrl(ctrl)->connect_work);\n}\n\nstatic void nvme_tcp_free_ctrl(struct nvme_ctrl *nctrl)\n{\n\tstruct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(nctrl);\n\n\tif (list_empty(&ctrl->list))\n\t\tgoto free_ctrl;\n\n\tmutex_lock(&nvme_tcp_ctrl_mutex);\n\tlist_del(&ctrl->list);\n\tmutex_unlock(&nvme_tcp_ctrl_mutex);\n\n\tnvmf_free_options(nctrl->opts);\nfree_ctrl:\n\tkfree(ctrl->queues);\n\tkfree(ctrl);\n}\n\nstatic void nvme_tcp_set_sg_null(struct nvme_command *c)\n{\n\tstruct nvme_sgl_desc *sg = &c->common.dptr.sgl;\n\n\tsg->addr = 0;\n\tsg->length = 0;\n\tsg->type = (NVME_TRANSPORT_SGL_DATA_DESC << 4) |\n\t\t\tNVME_SGL_FMT_TRANSPORT_A;\n}\n\nstatic void nvme_tcp_set_sg_inline(struct nvme_tcp_queue *queue,\n\t\tstruct nvme_command *c, u32 data_len)\n{\n\tstruct nvme_sgl_desc *sg = &c->common.dptr.sgl;\n\n\tsg->addr = cpu_to_le64(queue->ctrl->ctrl.icdoff);\n\tsg->length = cpu_to_le32(data_len);\n\tsg->type = (NVME_SGL_FMT_DATA_DESC << 4) | NVME_SGL_FMT_OFFSET;\n}\n\nstatic void nvme_tcp_set_sg_host_data(struct nvme_command *c,\n\t\tu32 data_len)\n{\n\tstruct nvme_sgl_desc *sg = &c->common.dptr.sgl;\n\n\tsg->addr = 0;\n\tsg->length = cpu_to_le32(data_len);\n\tsg->type = (NVME_TRANSPORT_SGL_DATA_DESC << 4) |\n\t\t\tNVME_SGL_FMT_TRANSPORT_A;\n}\n\nstatic void nvme_tcp_submit_async_event(struct nvme_ctrl *arg)\n{\n\tstruct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(arg);\n\tstruct nvme_tcp_queue *queue = &ctrl->queues[0];\n\tstruct nvme_tcp_cmd_pdu *pdu = ctrl->async_req.pdu;\n\tstruct nvme_command *cmd = &pdu->cmd;\n\tu8 hdgst = nvme_tcp_hdgst_len(queue);\n\n\tmemset(pdu, 0, sizeof(*pdu));\n\tpdu->hdr.type = nvme_tcp_cmd;\n\tif (queue->hdr_digest)\n\t\tpdu->hdr.flags |= NVME_TCP_F_HDGST;\n\tpdu->hdr.hlen = sizeof(*pdu);\n\tpdu->hdr.plen = cpu_to_le32(pdu->hdr.hlen + hdgst);\n\n\tcmd->common.opcode = nvme_admin_async_event;\n\tcmd->common.command_id = NVME_AQ_BLK_MQ_DEPTH;\n\tcmd->common.flags |= NVME_CMD_SGL_METABUF;\n\tnvme_tcp_set_sg_null(cmd);\n\n\tctrl->async_req.state = NVME_TCP_SEND_CMD_PDU;\n\tctrl->async_req.offset = 0;\n\tctrl->async_req.curr_bio = NULL;\n\tctrl->async_req.data_len = 0;\n\n\tnvme_tcp_queue_request(&ctrl->async_req, true, true);\n}\n\nstatic void nvme_tcp_complete_timed_out(struct request *rq)\n{\n\tstruct nvme_tcp_request *req = blk_mq_rq_to_pdu(rq);\n\tstruct nvme_ctrl *ctrl = &req->queue->ctrl->ctrl;\n\n\tnvme_tcp_stop_queue(ctrl, nvme_tcp_queue_id(req->queue));\n\tnvmf_complete_timed_out_request(rq);\n}\n\nstatic enum blk_eh_timer_return nvme_tcp_timeout(struct request *rq)\n{\n\tstruct nvme_tcp_request *req = blk_mq_rq_to_pdu(rq);\n\tstruct nvme_ctrl *ctrl = &req->queue->ctrl->ctrl;\n\tstruct nvme_tcp_cmd_pdu *pdu = nvme_tcp_req_cmd_pdu(req);\n\tu8 opc = pdu->cmd.common.opcode, fctype = pdu->cmd.fabrics.fctype;\n\tint qid = nvme_tcp_queue_id(req->queue);\n\n\tdev_warn(ctrl->device,\n\t\t\"queue %d: timeout cid %#x type %d opcode %#x (%s)\\n\",\n\t\tnvme_tcp_queue_id(req->queue), nvme_cid(rq), pdu->hdr.type,\n\t\topc, nvme_opcode_str(qid, opc, fctype));\n\n\tif (nvme_ctrl_state(ctrl) != NVME_CTRL_LIVE) {\n\t\t \n\t\tnvme_tcp_complete_timed_out(rq);\n\t\treturn BLK_EH_DONE;\n\t}\n\n\t \n\tnvme_tcp_error_recovery(ctrl);\n\treturn BLK_EH_RESET_TIMER;\n}\n\nstatic blk_status_t nvme_tcp_map_data(struct nvme_tcp_queue *queue,\n\t\t\tstruct request *rq)\n{\n\tstruct nvme_tcp_request *req = blk_mq_rq_to_pdu(rq);\n\tstruct nvme_tcp_cmd_pdu *pdu = nvme_tcp_req_cmd_pdu(req);\n\tstruct nvme_command *c = &pdu->cmd;\n\n\tc->common.flags |= NVME_CMD_SGL_METABUF;\n\n\tif (!blk_rq_nr_phys_segments(rq))\n\t\tnvme_tcp_set_sg_null(c);\n\telse if (rq_data_dir(rq) == WRITE &&\n\t    req->data_len <= nvme_tcp_inline_data_size(req))\n\t\tnvme_tcp_set_sg_inline(queue, c, req->data_len);\n\telse\n\t\tnvme_tcp_set_sg_host_data(c, req->data_len);\n\n\treturn 0;\n}\n\nstatic blk_status_t nvme_tcp_setup_cmd_pdu(struct nvme_ns *ns,\n\t\tstruct request *rq)\n{\n\tstruct nvme_tcp_request *req = blk_mq_rq_to_pdu(rq);\n\tstruct nvme_tcp_cmd_pdu *pdu = nvme_tcp_req_cmd_pdu(req);\n\tstruct nvme_tcp_queue *queue = req->queue;\n\tu8 hdgst = nvme_tcp_hdgst_len(queue), ddgst = 0;\n\tblk_status_t ret;\n\n\tret = nvme_setup_cmd(ns, rq);\n\tif (ret)\n\t\treturn ret;\n\n\treq->state = NVME_TCP_SEND_CMD_PDU;\n\treq->status = cpu_to_le16(NVME_SC_SUCCESS);\n\treq->offset = 0;\n\treq->data_sent = 0;\n\treq->pdu_len = 0;\n\treq->pdu_sent = 0;\n\treq->h2cdata_left = 0;\n\treq->data_len = blk_rq_nr_phys_segments(rq) ?\n\t\t\t\tblk_rq_payload_bytes(rq) : 0;\n\treq->curr_bio = rq->bio;\n\tif (req->curr_bio && req->data_len)\n\t\tnvme_tcp_init_iter(req, rq_data_dir(rq));\n\n\tif (rq_data_dir(rq) == WRITE &&\n\t    req->data_len <= nvme_tcp_inline_data_size(req))\n\t\treq->pdu_len = req->data_len;\n\n\tpdu->hdr.type = nvme_tcp_cmd;\n\tpdu->hdr.flags = 0;\n\tif (queue->hdr_digest)\n\t\tpdu->hdr.flags |= NVME_TCP_F_HDGST;\n\tif (queue->data_digest && req->pdu_len) {\n\t\tpdu->hdr.flags |= NVME_TCP_F_DDGST;\n\t\tddgst = nvme_tcp_ddgst_len(queue);\n\t}\n\tpdu->hdr.hlen = sizeof(*pdu);\n\tpdu->hdr.pdo = req->pdu_len ? pdu->hdr.hlen + hdgst : 0;\n\tpdu->hdr.plen =\n\t\tcpu_to_le32(pdu->hdr.hlen + hdgst + req->pdu_len + ddgst);\n\n\tret = nvme_tcp_map_data(queue, rq);\n\tif (unlikely(ret)) {\n\t\tnvme_cleanup_cmd(rq);\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"Failed to map data (%d)\\n\", ret);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic void nvme_tcp_commit_rqs(struct blk_mq_hw_ctx *hctx)\n{\n\tstruct nvme_tcp_queue *queue = hctx->driver_data;\n\n\tif (!llist_empty(&queue->req_list))\n\t\tqueue_work_on(queue->io_cpu, nvme_tcp_wq, &queue->io_work);\n}\n\nstatic blk_status_t nvme_tcp_queue_rq(struct blk_mq_hw_ctx *hctx,\n\t\tconst struct blk_mq_queue_data *bd)\n{\n\tstruct nvme_ns *ns = hctx->queue->queuedata;\n\tstruct nvme_tcp_queue *queue = hctx->driver_data;\n\tstruct request *rq = bd->rq;\n\tstruct nvme_tcp_request *req = blk_mq_rq_to_pdu(rq);\n\tbool queue_ready = test_bit(NVME_TCP_Q_LIVE, &queue->flags);\n\tblk_status_t ret;\n\n\tif (!nvme_check_ready(&queue->ctrl->ctrl, rq, queue_ready))\n\t\treturn nvme_fail_nonready_command(&queue->ctrl->ctrl, rq);\n\n\tret = nvme_tcp_setup_cmd_pdu(ns, rq);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tnvme_start_request(rq);\n\n\tnvme_tcp_queue_request(req, true, bd->last);\n\n\treturn BLK_STS_OK;\n}\n\nstatic void nvme_tcp_map_queues(struct blk_mq_tag_set *set)\n{\n\tstruct nvme_tcp_ctrl *ctrl = to_tcp_ctrl(set->driver_data);\n\n\tnvmf_map_queues(set, &ctrl->ctrl, ctrl->io_queues);\n}\n\nstatic int nvme_tcp_poll(struct blk_mq_hw_ctx *hctx, struct io_comp_batch *iob)\n{\n\tstruct nvme_tcp_queue *queue = hctx->driver_data;\n\tstruct sock *sk = queue->sock->sk;\n\n\tif (!test_bit(NVME_TCP_Q_LIVE, &queue->flags))\n\t\treturn 0;\n\n\tset_bit(NVME_TCP_Q_POLLING, &queue->flags);\n\tif (sk_can_busy_loop(sk) && skb_queue_empty_lockless(&sk->sk_receive_queue))\n\t\tsk_busy_loop(sk, true);\n\tnvme_tcp_try_recv(queue);\n\tclear_bit(NVME_TCP_Q_POLLING, &queue->flags);\n\treturn queue->nr_cqe;\n}\n\nstatic int nvme_tcp_get_address(struct nvme_ctrl *ctrl, char *buf, int size)\n{\n\tstruct nvme_tcp_queue *queue = &to_tcp_ctrl(ctrl)->queues[0];\n\tstruct sockaddr_storage src_addr;\n\tint ret, len;\n\n\tlen = nvmf_get_address(ctrl, buf, size);\n\n\tmutex_lock(&queue->queue_lock);\n\n\tif (!test_bit(NVME_TCP_Q_LIVE, &queue->flags))\n\t\tgoto done;\n\tret = kernel_getsockname(queue->sock, (struct sockaddr *)&src_addr);\n\tif (ret > 0) {\n\t\tif (len > 0)\n\t\t\tlen--;  \n\t\tlen += scnprintf(buf + len, size - len, \"%ssrc_addr=%pISc\\n\",\n\t\t\t\t(len) ? \",\" : \"\", &src_addr);\n\t}\ndone:\n\tmutex_unlock(&queue->queue_lock);\n\n\treturn len;\n}\n\nstatic const struct blk_mq_ops nvme_tcp_mq_ops = {\n\t.queue_rq\t= nvme_tcp_queue_rq,\n\t.commit_rqs\t= nvme_tcp_commit_rqs,\n\t.complete\t= nvme_complete_rq,\n\t.init_request\t= nvme_tcp_init_request,\n\t.exit_request\t= nvme_tcp_exit_request,\n\t.init_hctx\t= nvme_tcp_init_hctx,\n\t.timeout\t= nvme_tcp_timeout,\n\t.map_queues\t= nvme_tcp_map_queues,\n\t.poll\t\t= nvme_tcp_poll,\n};\n\nstatic const struct blk_mq_ops nvme_tcp_admin_mq_ops = {\n\t.queue_rq\t= nvme_tcp_queue_rq,\n\t.complete\t= nvme_complete_rq,\n\t.init_request\t= nvme_tcp_init_request,\n\t.exit_request\t= nvme_tcp_exit_request,\n\t.init_hctx\t= nvme_tcp_init_admin_hctx,\n\t.timeout\t= nvme_tcp_timeout,\n};\n\nstatic const struct nvme_ctrl_ops nvme_tcp_ctrl_ops = {\n\t.name\t\t\t= \"tcp\",\n\t.module\t\t\t= THIS_MODULE,\n\t.flags\t\t\t= NVME_F_FABRICS | NVME_F_BLOCKING,\n\t.reg_read32\t\t= nvmf_reg_read32,\n\t.reg_read64\t\t= nvmf_reg_read64,\n\t.reg_write32\t\t= nvmf_reg_write32,\n\t.free_ctrl\t\t= nvme_tcp_free_ctrl,\n\t.submit_async_event\t= nvme_tcp_submit_async_event,\n\t.delete_ctrl\t\t= nvme_tcp_delete_ctrl,\n\t.get_address\t\t= nvme_tcp_get_address,\n\t.stop_ctrl\t\t= nvme_tcp_stop_ctrl,\n};\n\nstatic bool\nnvme_tcp_existing_controller(struct nvmf_ctrl_options *opts)\n{\n\tstruct nvme_tcp_ctrl *ctrl;\n\tbool found = false;\n\n\tmutex_lock(&nvme_tcp_ctrl_mutex);\n\tlist_for_each_entry(ctrl, &nvme_tcp_ctrl_list, list) {\n\t\tfound = nvmf_ip_options_match(&ctrl->ctrl, opts);\n\t\tif (found)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&nvme_tcp_ctrl_mutex);\n\n\treturn found;\n}\n\nstatic struct nvme_ctrl *nvme_tcp_create_ctrl(struct device *dev,\n\t\tstruct nvmf_ctrl_options *opts)\n{\n\tstruct nvme_tcp_ctrl *ctrl;\n\tint ret;\n\n\tctrl = kzalloc(sizeof(*ctrl), GFP_KERNEL);\n\tif (!ctrl)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tINIT_LIST_HEAD(&ctrl->list);\n\tctrl->ctrl.opts = opts;\n\tctrl->ctrl.queue_count = opts->nr_io_queues + opts->nr_write_queues +\n\t\t\t\topts->nr_poll_queues + 1;\n\tctrl->ctrl.sqsize = opts->queue_size - 1;\n\tctrl->ctrl.kato = opts->kato;\n\n\tINIT_DELAYED_WORK(&ctrl->connect_work,\n\t\t\tnvme_tcp_reconnect_ctrl_work);\n\tINIT_WORK(&ctrl->err_work, nvme_tcp_error_recovery_work);\n\tINIT_WORK(&ctrl->ctrl.reset_work, nvme_reset_ctrl_work);\n\n\tif (!(opts->mask & NVMF_OPT_TRSVCID)) {\n\t\topts->trsvcid =\n\t\t\tkstrdup(__stringify(NVME_TCP_DISC_PORT), GFP_KERNEL);\n\t\tif (!opts->trsvcid) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_free_ctrl;\n\t\t}\n\t\topts->mask |= NVMF_OPT_TRSVCID;\n\t}\n\n\tret = inet_pton_with_scope(&init_net, AF_UNSPEC,\n\t\t\topts->traddr, opts->trsvcid, &ctrl->addr);\n\tif (ret) {\n\t\tpr_err(\"malformed address passed: %s:%s\\n\",\n\t\t\topts->traddr, opts->trsvcid);\n\t\tgoto out_free_ctrl;\n\t}\n\n\tif (opts->mask & NVMF_OPT_HOST_TRADDR) {\n\t\tret = inet_pton_with_scope(&init_net, AF_UNSPEC,\n\t\t\topts->host_traddr, NULL, &ctrl->src_addr);\n\t\tif (ret) {\n\t\t\tpr_err(\"malformed src address passed: %s\\n\",\n\t\t\t       opts->host_traddr);\n\t\t\tgoto out_free_ctrl;\n\t\t}\n\t}\n\n\tif (opts->mask & NVMF_OPT_HOST_IFACE) {\n\t\tif (!__dev_get_by_name(&init_net, opts->host_iface)) {\n\t\t\tpr_err(\"invalid interface passed: %s\\n\",\n\t\t\t       opts->host_iface);\n\t\t\tret = -ENODEV;\n\t\t\tgoto out_free_ctrl;\n\t\t}\n\t}\n\n\tif (!opts->duplicate_connect && nvme_tcp_existing_controller(opts)) {\n\t\tret = -EALREADY;\n\t\tgoto out_free_ctrl;\n\t}\n\n\tctrl->queues = kcalloc(ctrl->ctrl.queue_count, sizeof(*ctrl->queues),\n\t\t\t\tGFP_KERNEL);\n\tif (!ctrl->queues) {\n\t\tret = -ENOMEM;\n\t\tgoto out_free_ctrl;\n\t}\n\n\tret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_tcp_ctrl_ops, 0);\n\tif (ret)\n\t\tgoto out_kfree_queues;\n\n\tif (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING)) {\n\t\tWARN_ON_ONCE(1);\n\t\tret = -EINTR;\n\t\tgoto out_uninit_ctrl;\n\t}\n\n\tret = nvme_tcp_setup_ctrl(&ctrl->ctrl, true);\n\tif (ret)\n\t\tgoto out_uninit_ctrl;\n\n\tdev_info(ctrl->ctrl.device, \"new ctrl: NQN \\\"%s\\\", addr %pISp\\n\",\n\t\tnvmf_ctrl_subsysnqn(&ctrl->ctrl), &ctrl->addr);\n\n\tmutex_lock(&nvme_tcp_ctrl_mutex);\n\tlist_add_tail(&ctrl->list, &nvme_tcp_ctrl_list);\n\tmutex_unlock(&nvme_tcp_ctrl_mutex);\n\n\treturn &ctrl->ctrl;\n\nout_uninit_ctrl:\n\tnvme_uninit_ctrl(&ctrl->ctrl);\n\tnvme_put_ctrl(&ctrl->ctrl);\n\tif (ret > 0)\n\t\tret = -EIO;\n\treturn ERR_PTR(ret);\nout_kfree_queues:\n\tkfree(ctrl->queues);\nout_free_ctrl:\n\tkfree(ctrl);\n\treturn ERR_PTR(ret);\n}\n\nstatic struct nvmf_transport_ops nvme_tcp_transport = {\n\t.name\t\t= \"tcp\",\n\t.module\t\t= THIS_MODULE,\n\t.required_opts\t= NVMF_OPT_TRADDR,\n\t.allowed_opts\t= NVMF_OPT_TRSVCID | NVMF_OPT_RECONNECT_DELAY |\n\t\t\t  NVMF_OPT_HOST_TRADDR | NVMF_OPT_CTRL_LOSS_TMO |\n\t\t\t  NVMF_OPT_HDR_DIGEST | NVMF_OPT_DATA_DIGEST |\n\t\t\t  NVMF_OPT_NR_WRITE_QUEUES | NVMF_OPT_NR_POLL_QUEUES |\n\t\t\t  NVMF_OPT_TOS | NVMF_OPT_HOST_IFACE,\n\t.create_ctrl\t= nvme_tcp_create_ctrl,\n};\n\nstatic int __init nvme_tcp_init_module(void)\n{\n\tBUILD_BUG_ON(sizeof(struct nvme_tcp_hdr) != 8);\n\tBUILD_BUG_ON(sizeof(struct nvme_tcp_cmd_pdu) != 72);\n\tBUILD_BUG_ON(sizeof(struct nvme_tcp_data_pdu) != 24);\n\tBUILD_BUG_ON(sizeof(struct nvme_tcp_rsp_pdu) != 24);\n\tBUILD_BUG_ON(sizeof(struct nvme_tcp_r2t_pdu) != 24);\n\tBUILD_BUG_ON(sizeof(struct nvme_tcp_icreq_pdu) != 128);\n\tBUILD_BUG_ON(sizeof(struct nvme_tcp_icresp_pdu) != 128);\n\tBUILD_BUG_ON(sizeof(struct nvme_tcp_term_pdu) != 24);\n\n\tnvme_tcp_wq = alloc_workqueue(\"nvme_tcp_wq\",\n\t\t\tWQ_MEM_RECLAIM | WQ_HIGHPRI, 0);\n\tif (!nvme_tcp_wq)\n\t\treturn -ENOMEM;\n\n\tnvmf_register_transport(&nvme_tcp_transport);\n\treturn 0;\n}\n\nstatic void __exit nvme_tcp_cleanup_module(void)\n{\n\tstruct nvme_tcp_ctrl *ctrl;\n\n\tnvmf_unregister_transport(&nvme_tcp_transport);\n\n\tmutex_lock(&nvme_tcp_ctrl_mutex);\n\tlist_for_each_entry(ctrl, &nvme_tcp_ctrl_list, list)\n\t\tnvme_delete_ctrl(&ctrl->ctrl);\n\tmutex_unlock(&nvme_tcp_ctrl_mutex);\n\tflush_workqueue(nvme_delete_wq);\n\n\tdestroy_workqueue(nvme_tcp_wq);\n}\n\nmodule_init(nvme_tcp_init_module);\nmodule_exit(nvme_tcp_cleanup_module);\n\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}