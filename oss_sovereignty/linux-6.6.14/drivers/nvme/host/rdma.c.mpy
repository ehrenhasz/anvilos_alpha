{
  "module_name": "rdma.c",
  "hash_id": "255434f964fac0193e70cfd31b61795c270fff9356a084314f44d2e1268296c6",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvme/host/rdma.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <rdma/mr_pool.h>\n#include <linux/err.h>\n#include <linux/string.h>\n#include <linux/atomic.h>\n#include <linux/blk-mq.h>\n#include <linux/blk-integrity.h>\n#include <linux/types.h>\n#include <linux/list.h>\n#include <linux/mutex.h>\n#include <linux/scatterlist.h>\n#include <linux/nvme.h>\n#include <asm/unaligned.h>\n\n#include <rdma/ib_verbs.h>\n#include <rdma/rdma_cm.h>\n#include <linux/nvme-rdma.h>\n\n#include \"nvme.h\"\n#include \"fabrics.h\"\n\n\n#define NVME_RDMA_CM_TIMEOUT_MS\t\t3000\t\t \n\n#define NVME_RDMA_MAX_SEGMENTS\t\t256\n\n#define NVME_RDMA_MAX_INLINE_SEGMENTS\t4\n\n#define NVME_RDMA_DATA_SGL_SIZE \\\n\t(sizeof(struct scatterlist) * NVME_INLINE_SG_CNT)\n#define NVME_RDMA_METADATA_SGL_SIZE \\\n\t(sizeof(struct scatterlist) * NVME_INLINE_METADATA_SG_CNT)\n\nstruct nvme_rdma_device {\n\tstruct ib_device\t*dev;\n\tstruct ib_pd\t\t*pd;\n\tstruct kref\t\tref;\n\tstruct list_head\tentry;\n\tunsigned int\t\tnum_inline_segments;\n};\n\nstruct nvme_rdma_qe {\n\tstruct ib_cqe\t\tcqe;\n\tvoid\t\t\t*data;\n\tu64\t\t\tdma;\n};\n\nstruct nvme_rdma_sgl {\n\tint\t\t\tnents;\n\tstruct sg_table\t\tsg_table;\n};\n\nstruct nvme_rdma_queue;\nstruct nvme_rdma_request {\n\tstruct nvme_request\treq;\n\tstruct ib_mr\t\t*mr;\n\tstruct nvme_rdma_qe\tsqe;\n\tunion nvme_result\tresult;\n\t__le16\t\t\tstatus;\n\trefcount_t\t\tref;\n\tstruct ib_sge\t\tsge[1 + NVME_RDMA_MAX_INLINE_SEGMENTS];\n\tu32\t\t\tnum_sge;\n\tstruct ib_reg_wr\treg_wr;\n\tstruct ib_cqe\t\treg_cqe;\n\tstruct nvme_rdma_queue  *queue;\n\tstruct nvme_rdma_sgl\tdata_sgl;\n\tstruct nvme_rdma_sgl\t*metadata_sgl;\n\tbool\t\t\tuse_sig_mr;\n};\n\nenum nvme_rdma_queue_flags {\n\tNVME_RDMA_Q_ALLOCATED\t\t= 0,\n\tNVME_RDMA_Q_LIVE\t\t= 1,\n\tNVME_RDMA_Q_TR_READY\t\t= 2,\n};\n\nstruct nvme_rdma_queue {\n\tstruct nvme_rdma_qe\t*rsp_ring;\n\tint\t\t\tqueue_size;\n\tsize_t\t\t\tcmnd_capsule_len;\n\tstruct nvme_rdma_ctrl\t*ctrl;\n\tstruct nvme_rdma_device\t*device;\n\tstruct ib_cq\t\t*ib_cq;\n\tstruct ib_qp\t\t*qp;\n\n\tunsigned long\t\tflags;\n\tstruct rdma_cm_id\t*cm_id;\n\tint\t\t\tcm_error;\n\tstruct completion\tcm_done;\n\tbool\t\t\tpi_support;\n\tint\t\t\tcq_size;\n\tstruct mutex\t\tqueue_lock;\n};\n\nstruct nvme_rdma_ctrl {\n\t \n\tstruct nvme_rdma_queue\t*queues;\n\n\t \n\tstruct blk_mq_tag_set\ttag_set;\n\tstruct work_struct\terr_work;\n\n\tstruct nvme_rdma_qe\tasync_event_sqe;\n\n\tstruct delayed_work\treconnect_work;\n\n\tstruct list_head\tlist;\n\n\tstruct blk_mq_tag_set\tadmin_tag_set;\n\tstruct nvme_rdma_device\t*device;\n\n\tu32\t\t\tmax_fr_pages;\n\n\tstruct sockaddr_storage addr;\n\tstruct sockaddr_storage src_addr;\n\n\tstruct nvme_ctrl\tctrl;\n\tbool\t\t\tuse_inline_data;\n\tu32\t\t\tio_queues[HCTX_MAX_TYPES];\n};\n\nstatic inline struct nvme_rdma_ctrl *to_rdma_ctrl(struct nvme_ctrl *ctrl)\n{\n\treturn container_of(ctrl, struct nvme_rdma_ctrl, ctrl);\n}\n\nstatic LIST_HEAD(device_list);\nstatic DEFINE_MUTEX(device_list_mutex);\n\nstatic LIST_HEAD(nvme_rdma_ctrl_list);\nstatic DEFINE_MUTEX(nvme_rdma_ctrl_mutex);\n\n \nstatic bool register_always = true;\nmodule_param(register_always, bool, 0444);\nMODULE_PARM_DESC(register_always,\n\t \"Use memory registration even for contiguous memory regions\");\n\nstatic int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,\n\t\tstruct rdma_cm_event *event);\nstatic void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc);\nstatic void nvme_rdma_complete_rq(struct request *rq);\n\nstatic const struct blk_mq_ops nvme_rdma_mq_ops;\nstatic const struct blk_mq_ops nvme_rdma_admin_mq_ops;\n\nstatic inline int nvme_rdma_queue_idx(struct nvme_rdma_queue *queue)\n{\n\treturn queue - queue->ctrl->queues;\n}\n\nstatic bool nvme_rdma_poll_queue(struct nvme_rdma_queue *queue)\n{\n\treturn nvme_rdma_queue_idx(queue) >\n\t\tqueue->ctrl->io_queues[HCTX_TYPE_DEFAULT] +\n\t\tqueue->ctrl->io_queues[HCTX_TYPE_READ];\n}\n\nstatic inline size_t nvme_rdma_inline_data_size(struct nvme_rdma_queue *queue)\n{\n\treturn queue->cmnd_capsule_len - sizeof(struct nvme_command);\n}\n\nstatic void nvme_rdma_free_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,\n\t\tsize_t capsule_size, enum dma_data_direction dir)\n{\n\tib_dma_unmap_single(ibdev, qe->dma, capsule_size, dir);\n\tkfree(qe->data);\n}\n\nstatic int nvme_rdma_alloc_qe(struct ib_device *ibdev, struct nvme_rdma_qe *qe,\n\t\tsize_t capsule_size, enum dma_data_direction dir)\n{\n\tqe->data = kzalloc(capsule_size, GFP_KERNEL);\n\tif (!qe->data)\n\t\treturn -ENOMEM;\n\n\tqe->dma = ib_dma_map_single(ibdev, qe->data, capsule_size, dir);\n\tif (ib_dma_mapping_error(ibdev, qe->dma)) {\n\t\tkfree(qe->data);\n\t\tqe->data = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nstatic void nvme_rdma_free_ring(struct ib_device *ibdev,\n\t\tstruct nvme_rdma_qe *ring, size_t ib_queue_size,\n\t\tsize_t capsule_size, enum dma_data_direction dir)\n{\n\tint i;\n\n\tfor (i = 0; i < ib_queue_size; i++)\n\t\tnvme_rdma_free_qe(ibdev, &ring[i], capsule_size, dir);\n\tkfree(ring);\n}\n\nstatic struct nvme_rdma_qe *nvme_rdma_alloc_ring(struct ib_device *ibdev,\n\t\tsize_t ib_queue_size, size_t capsule_size,\n\t\tenum dma_data_direction dir)\n{\n\tstruct nvme_rdma_qe *ring;\n\tint i;\n\n\tring = kcalloc(ib_queue_size, sizeof(struct nvme_rdma_qe), GFP_KERNEL);\n\tif (!ring)\n\t\treturn NULL;\n\n\t \n\tfor (i = 0; i < ib_queue_size; i++) {\n\t\tif (nvme_rdma_alloc_qe(ibdev, &ring[i], capsule_size, dir))\n\t\t\tgoto out_free_ring;\n\t}\n\n\treturn ring;\n\nout_free_ring:\n\tnvme_rdma_free_ring(ibdev, ring, i, capsule_size, dir);\n\treturn NULL;\n}\n\nstatic void nvme_rdma_qp_event(struct ib_event *event, void *context)\n{\n\tpr_debug(\"QP event %s (%d)\\n\",\n\t\t ib_event_msg(event->event), event->event);\n\n}\n\nstatic int nvme_rdma_wait_for_cm(struct nvme_rdma_queue *queue)\n{\n\tint ret;\n\n\tret = wait_for_completion_interruptible(&queue->cm_done);\n\tif (ret)\n\t\treturn ret;\n\tWARN_ON_ONCE(queue->cm_error > 0);\n\treturn queue->cm_error;\n}\n\nstatic int nvme_rdma_create_qp(struct nvme_rdma_queue *queue, const int factor)\n{\n\tstruct nvme_rdma_device *dev = queue->device;\n\tstruct ib_qp_init_attr init_attr;\n\tint ret;\n\n\tmemset(&init_attr, 0, sizeof(init_attr));\n\tinit_attr.event_handler = nvme_rdma_qp_event;\n\t \n\tinit_attr.cap.max_send_wr = factor * queue->queue_size + 1;\n\t \n\tinit_attr.cap.max_recv_wr = queue->queue_size + 1;\n\tinit_attr.cap.max_recv_sge = 1;\n\tinit_attr.cap.max_send_sge = 1 + dev->num_inline_segments;\n\tinit_attr.sq_sig_type = IB_SIGNAL_REQ_WR;\n\tinit_attr.qp_type = IB_QPT_RC;\n\tinit_attr.send_cq = queue->ib_cq;\n\tinit_attr.recv_cq = queue->ib_cq;\n\tif (queue->pi_support)\n\t\tinit_attr.create_flags |= IB_QP_CREATE_INTEGRITY_EN;\n\tinit_attr.qp_context = queue;\n\n\tret = rdma_create_qp(queue->cm_id, dev->pd, &init_attr);\n\n\tqueue->qp = queue->cm_id->qp;\n\treturn ret;\n}\n\nstatic void nvme_rdma_exit_request(struct blk_mq_tag_set *set,\n\t\tstruct request *rq, unsigned int hctx_idx)\n{\n\tstruct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);\n\n\tkfree(req->sqe.data);\n}\n\nstatic int nvme_rdma_init_request(struct blk_mq_tag_set *set,\n\t\tstruct request *rq, unsigned int hctx_idx,\n\t\tunsigned int numa_node)\n{\n\tstruct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(set->driver_data);\n\tstruct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);\n\tint queue_idx = (set == &ctrl->tag_set) ? hctx_idx + 1 : 0;\n\tstruct nvme_rdma_queue *queue = &ctrl->queues[queue_idx];\n\n\tnvme_req(rq)->ctrl = &ctrl->ctrl;\n\treq->sqe.data = kzalloc(sizeof(struct nvme_command), GFP_KERNEL);\n\tif (!req->sqe.data)\n\t\treturn -ENOMEM;\n\n\t \n\tif (queue->pi_support)\n\t\treq->metadata_sgl = (void *)nvme_req(rq) +\n\t\t\tsizeof(struct nvme_rdma_request) +\n\t\t\tNVME_RDMA_DATA_SGL_SIZE;\n\n\treq->queue = queue;\n\tnvme_req(rq)->cmd = req->sqe.data;\n\n\treturn 0;\n}\n\nstatic int nvme_rdma_init_hctx(struct blk_mq_hw_ctx *hctx, void *data,\n\t\tunsigned int hctx_idx)\n{\n\tstruct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(data);\n\tstruct nvme_rdma_queue *queue = &ctrl->queues[hctx_idx + 1];\n\n\tBUG_ON(hctx_idx >= ctrl->ctrl.queue_count);\n\n\thctx->driver_data = queue;\n\treturn 0;\n}\n\nstatic int nvme_rdma_init_admin_hctx(struct blk_mq_hw_ctx *hctx, void *data,\n\t\tunsigned int hctx_idx)\n{\n\tstruct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(data);\n\tstruct nvme_rdma_queue *queue = &ctrl->queues[0];\n\n\tBUG_ON(hctx_idx != 0);\n\n\thctx->driver_data = queue;\n\treturn 0;\n}\n\nstatic void nvme_rdma_free_dev(struct kref *ref)\n{\n\tstruct nvme_rdma_device *ndev =\n\t\tcontainer_of(ref, struct nvme_rdma_device, ref);\n\n\tmutex_lock(&device_list_mutex);\n\tlist_del(&ndev->entry);\n\tmutex_unlock(&device_list_mutex);\n\n\tib_dealloc_pd(ndev->pd);\n\tkfree(ndev);\n}\n\nstatic void nvme_rdma_dev_put(struct nvme_rdma_device *dev)\n{\n\tkref_put(&dev->ref, nvme_rdma_free_dev);\n}\n\nstatic int nvme_rdma_dev_get(struct nvme_rdma_device *dev)\n{\n\treturn kref_get_unless_zero(&dev->ref);\n}\n\nstatic struct nvme_rdma_device *\nnvme_rdma_find_get_device(struct rdma_cm_id *cm_id)\n{\n\tstruct nvme_rdma_device *ndev;\n\n\tmutex_lock(&device_list_mutex);\n\tlist_for_each_entry(ndev, &device_list, entry) {\n\t\tif (ndev->dev->node_guid == cm_id->device->node_guid &&\n\t\t    nvme_rdma_dev_get(ndev))\n\t\t\tgoto out_unlock;\n\t}\n\n\tndev = kzalloc(sizeof(*ndev), GFP_KERNEL);\n\tif (!ndev)\n\t\tgoto out_err;\n\n\tndev->dev = cm_id->device;\n\tkref_init(&ndev->ref);\n\n\tndev->pd = ib_alloc_pd(ndev->dev,\n\t\tregister_always ? 0 : IB_PD_UNSAFE_GLOBAL_RKEY);\n\tif (IS_ERR(ndev->pd))\n\t\tgoto out_free_dev;\n\n\tif (!(ndev->dev->attrs.device_cap_flags &\n\t      IB_DEVICE_MEM_MGT_EXTENSIONS)) {\n\t\tdev_err(&ndev->dev->dev,\n\t\t\t\"Memory registrations not supported.\\n\");\n\t\tgoto out_free_pd;\n\t}\n\n\tndev->num_inline_segments = min(NVME_RDMA_MAX_INLINE_SEGMENTS,\n\t\t\t\t\tndev->dev->attrs.max_send_sge - 1);\n\tlist_add(&ndev->entry, &device_list);\nout_unlock:\n\tmutex_unlock(&device_list_mutex);\n\treturn ndev;\n\nout_free_pd:\n\tib_dealloc_pd(ndev->pd);\nout_free_dev:\n\tkfree(ndev);\nout_err:\n\tmutex_unlock(&device_list_mutex);\n\treturn NULL;\n}\n\nstatic void nvme_rdma_free_cq(struct nvme_rdma_queue *queue)\n{\n\tif (nvme_rdma_poll_queue(queue))\n\t\tib_free_cq(queue->ib_cq);\n\telse\n\t\tib_cq_pool_put(queue->ib_cq, queue->cq_size);\n}\n\nstatic void nvme_rdma_destroy_queue_ib(struct nvme_rdma_queue *queue)\n{\n\tstruct nvme_rdma_device *dev;\n\tstruct ib_device *ibdev;\n\n\tif (!test_and_clear_bit(NVME_RDMA_Q_TR_READY, &queue->flags))\n\t\treturn;\n\n\tdev = queue->device;\n\tibdev = dev->dev;\n\n\tif (queue->pi_support)\n\t\tib_mr_pool_destroy(queue->qp, &queue->qp->sig_mrs);\n\tib_mr_pool_destroy(queue->qp, &queue->qp->rdma_mrs);\n\n\t \n\tib_destroy_qp(queue->qp);\n\tnvme_rdma_free_cq(queue);\n\n\tnvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,\n\t\t\tsizeof(struct nvme_completion), DMA_FROM_DEVICE);\n\n\tnvme_rdma_dev_put(dev);\n}\n\nstatic int nvme_rdma_get_max_fr_pages(struct ib_device *ibdev, bool pi_support)\n{\n\tu32 max_page_list_len;\n\n\tif (pi_support)\n\t\tmax_page_list_len = ibdev->attrs.max_pi_fast_reg_page_list_len;\n\telse\n\t\tmax_page_list_len = ibdev->attrs.max_fast_reg_page_list_len;\n\n\treturn min_t(u32, NVME_RDMA_MAX_SEGMENTS, max_page_list_len - 1);\n}\n\nstatic int nvme_rdma_create_cq(struct ib_device *ibdev,\n\t\tstruct nvme_rdma_queue *queue)\n{\n\tint ret, comp_vector, idx = nvme_rdma_queue_idx(queue);\n\n\t \n\tcomp_vector = (idx == 0 ? idx : idx - 1) % ibdev->num_comp_vectors;\n\n\t \n\tif (nvme_rdma_poll_queue(queue))\n\t\tqueue->ib_cq = ib_alloc_cq(ibdev, queue, queue->cq_size,\n\t\t\t\t\t   comp_vector, IB_POLL_DIRECT);\n\telse\n\t\tqueue->ib_cq = ib_cq_pool_get(ibdev, queue->cq_size,\n\t\t\t\t\t      comp_vector, IB_POLL_SOFTIRQ);\n\n\tif (IS_ERR(queue->ib_cq)) {\n\t\tret = PTR_ERR(queue->ib_cq);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int nvme_rdma_create_queue_ib(struct nvme_rdma_queue *queue)\n{\n\tstruct ib_device *ibdev;\n\tconst int send_wr_factor = 3;\t\t\t \n\tconst int cq_factor = send_wr_factor + 1;\t \n\tint ret, pages_per_mr;\n\n\tqueue->device = nvme_rdma_find_get_device(queue->cm_id);\n\tif (!queue->device) {\n\t\tdev_err(queue->cm_id->device->dev.parent,\n\t\t\t\"no client data found!\\n\");\n\t\treturn -ECONNREFUSED;\n\t}\n\tibdev = queue->device->dev;\n\n\t \n\tqueue->cq_size = cq_factor * queue->queue_size + 1;\n\n\tret = nvme_rdma_create_cq(ibdev, queue);\n\tif (ret)\n\t\tgoto out_put_dev;\n\n\tret = nvme_rdma_create_qp(queue, send_wr_factor);\n\tif (ret)\n\t\tgoto out_destroy_ib_cq;\n\n\tqueue->rsp_ring = nvme_rdma_alloc_ring(ibdev, queue->queue_size,\n\t\t\tsizeof(struct nvme_completion), DMA_FROM_DEVICE);\n\tif (!queue->rsp_ring) {\n\t\tret = -ENOMEM;\n\t\tgoto out_destroy_qp;\n\t}\n\n\t \n\tpages_per_mr = nvme_rdma_get_max_fr_pages(ibdev, queue->pi_support) + 1;\n\tret = ib_mr_pool_init(queue->qp, &queue->qp->rdma_mrs,\n\t\t\t      queue->queue_size,\n\t\t\t      IB_MR_TYPE_MEM_REG,\n\t\t\t      pages_per_mr, 0);\n\tif (ret) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"failed to initialize MR pool sized %d for QID %d\\n\",\n\t\t\tqueue->queue_size, nvme_rdma_queue_idx(queue));\n\t\tgoto out_destroy_ring;\n\t}\n\n\tif (queue->pi_support) {\n\t\tret = ib_mr_pool_init(queue->qp, &queue->qp->sig_mrs,\n\t\t\t\t      queue->queue_size, IB_MR_TYPE_INTEGRITY,\n\t\t\t\t      pages_per_mr, pages_per_mr);\n\t\tif (ret) {\n\t\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\t\"failed to initialize PI MR pool sized %d for QID %d\\n\",\n\t\t\t\tqueue->queue_size, nvme_rdma_queue_idx(queue));\n\t\t\tgoto out_destroy_mr_pool;\n\t\t}\n\t}\n\n\tset_bit(NVME_RDMA_Q_TR_READY, &queue->flags);\n\n\treturn 0;\n\nout_destroy_mr_pool:\n\tib_mr_pool_destroy(queue->qp, &queue->qp->rdma_mrs);\nout_destroy_ring:\n\tnvme_rdma_free_ring(ibdev, queue->rsp_ring, queue->queue_size,\n\t\t\t    sizeof(struct nvme_completion), DMA_FROM_DEVICE);\nout_destroy_qp:\n\trdma_destroy_qp(queue->cm_id);\nout_destroy_ib_cq:\n\tnvme_rdma_free_cq(queue);\nout_put_dev:\n\tnvme_rdma_dev_put(queue->device);\n\treturn ret;\n}\n\nstatic int nvme_rdma_alloc_queue(struct nvme_rdma_ctrl *ctrl,\n\t\tint idx, size_t queue_size)\n{\n\tstruct nvme_rdma_queue *queue;\n\tstruct sockaddr *src_addr = NULL;\n\tint ret;\n\n\tqueue = &ctrl->queues[idx];\n\tmutex_init(&queue->queue_lock);\n\tqueue->ctrl = ctrl;\n\tif (idx && ctrl->ctrl.max_integrity_segments)\n\t\tqueue->pi_support = true;\n\telse\n\t\tqueue->pi_support = false;\n\tinit_completion(&queue->cm_done);\n\n\tif (idx > 0)\n\t\tqueue->cmnd_capsule_len = ctrl->ctrl.ioccsz * 16;\n\telse\n\t\tqueue->cmnd_capsule_len = sizeof(struct nvme_command);\n\n\tqueue->queue_size = queue_size;\n\n\tqueue->cm_id = rdma_create_id(&init_net, nvme_rdma_cm_handler, queue,\n\t\t\tRDMA_PS_TCP, IB_QPT_RC);\n\tif (IS_ERR(queue->cm_id)) {\n\t\tdev_info(ctrl->ctrl.device,\n\t\t\t\"failed to create CM ID: %ld\\n\", PTR_ERR(queue->cm_id));\n\t\tret = PTR_ERR(queue->cm_id);\n\t\tgoto out_destroy_mutex;\n\t}\n\n\tif (ctrl->ctrl.opts->mask & NVMF_OPT_HOST_TRADDR)\n\t\tsrc_addr = (struct sockaddr *)&ctrl->src_addr;\n\n\tqueue->cm_error = -ETIMEDOUT;\n\tret = rdma_resolve_addr(queue->cm_id, src_addr,\n\t\t\t(struct sockaddr *)&ctrl->addr,\n\t\t\tNVME_RDMA_CM_TIMEOUT_MS);\n\tif (ret) {\n\t\tdev_info(ctrl->ctrl.device,\n\t\t\t\"rdma_resolve_addr failed (%d).\\n\", ret);\n\t\tgoto out_destroy_cm_id;\n\t}\n\n\tret = nvme_rdma_wait_for_cm(queue);\n\tif (ret) {\n\t\tdev_info(ctrl->ctrl.device,\n\t\t\t\"rdma connection establishment failed (%d)\\n\", ret);\n\t\tgoto out_destroy_cm_id;\n\t}\n\n\tset_bit(NVME_RDMA_Q_ALLOCATED, &queue->flags);\n\n\treturn 0;\n\nout_destroy_cm_id:\n\trdma_destroy_id(queue->cm_id);\n\tnvme_rdma_destroy_queue_ib(queue);\nout_destroy_mutex:\n\tmutex_destroy(&queue->queue_lock);\n\treturn ret;\n}\n\nstatic void __nvme_rdma_stop_queue(struct nvme_rdma_queue *queue)\n{\n\trdma_disconnect(queue->cm_id);\n\tib_drain_qp(queue->qp);\n}\n\nstatic void nvme_rdma_stop_queue(struct nvme_rdma_queue *queue)\n{\n\tif (!test_bit(NVME_RDMA_Q_ALLOCATED, &queue->flags))\n\t\treturn;\n\n\tmutex_lock(&queue->queue_lock);\n\tif (test_and_clear_bit(NVME_RDMA_Q_LIVE, &queue->flags))\n\t\t__nvme_rdma_stop_queue(queue);\n\tmutex_unlock(&queue->queue_lock);\n}\n\nstatic void nvme_rdma_free_queue(struct nvme_rdma_queue *queue)\n{\n\tif (!test_and_clear_bit(NVME_RDMA_Q_ALLOCATED, &queue->flags))\n\t\treturn;\n\n\trdma_destroy_id(queue->cm_id);\n\tnvme_rdma_destroy_queue_ib(queue);\n\tmutex_destroy(&queue->queue_lock);\n}\n\nstatic void nvme_rdma_free_io_queues(struct nvme_rdma_ctrl *ctrl)\n{\n\tint i;\n\n\tfor (i = 1; i < ctrl->ctrl.queue_count; i++)\n\t\tnvme_rdma_free_queue(&ctrl->queues[i]);\n}\n\nstatic void nvme_rdma_stop_io_queues(struct nvme_rdma_ctrl *ctrl)\n{\n\tint i;\n\n\tfor (i = 1; i < ctrl->ctrl.queue_count; i++)\n\t\tnvme_rdma_stop_queue(&ctrl->queues[i]);\n}\n\nstatic int nvme_rdma_start_queue(struct nvme_rdma_ctrl *ctrl, int idx)\n{\n\tstruct nvme_rdma_queue *queue = &ctrl->queues[idx];\n\tint ret;\n\n\tif (idx)\n\t\tret = nvmf_connect_io_queue(&ctrl->ctrl, idx);\n\telse\n\t\tret = nvmf_connect_admin_queue(&ctrl->ctrl);\n\n\tif (!ret) {\n\t\tset_bit(NVME_RDMA_Q_LIVE, &queue->flags);\n\t} else {\n\t\tif (test_bit(NVME_RDMA_Q_ALLOCATED, &queue->flags))\n\t\t\t__nvme_rdma_stop_queue(queue);\n\t\tdev_info(ctrl->ctrl.device,\n\t\t\t\"failed to connect queue: %d ret=%d\\n\", idx, ret);\n\t}\n\treturn ret;\n}\n\nstatic int nvme_rdma_start_io_queues(struct nvme_rdma_ctrl *ctrl,\n\t\t\t\t     int first, int last)\n{\n\tint i, ret = 0;\n\n\tfor (i = first; i < last; i++) {\n\t\tret = nvme_rdma_start_queue(ctrl, i);\n\t\tif (ret)\n\t\t\tgoto out_stop_queues;\n\t}\n\n\treturn 0;\n\nout_stop_queues:\n\tfor (i--; i >= first; i--)\n\t\tnvme_rdma_stop_queue(&ctrl->queues[i]);\n\treturn ret;\n}\n\nstatic int nvme_rdma_alloc_io_queues(struct nvme_rdma_ctrl *ctrl)\n{\n\tstruct nvmf_ctrl_options *opts = ctrl->ctrl.opts;\n\tunsigned int nr_io_queues;\n\tint i, ret;\n\n\tnr_io_queues = nvmf_nr_io_queues(opts);\n\tret = nvme_set_queue_count(&ctrl->ctrl, &nr_io_queues);\n\tif (ret)\n\t\treturn ret;\n\n\tif (nr_io_queues == 0) {\n\t\tdev_err(ctrl->ctrl.device,\n\t\t\t\"unable to set any I/O queues\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tctrl->ctrl.queue_count = nr_io_queues + 1;\n\tdev_info(ctrl->ctrl.device,\n\t\t\"creating %d I/O queues.\\n\", nr_io_queues);\n\n\tnvmf_set_io_queues(opts, nr_io_queues, ctrl->io_queues);\n\tfor (i = 1; i < ctrl->ctrl.queue_count; i++) {\n\t\tret = nvme_rdma_alloc_queue(ctrl, i,\n\t\t\t\tctrl->ctrl.sqsize + 1);\n\t\tif (ret)\n\t\t\tgoto out_free_queues;\n\t}\n\n\treturn 0;\n\nout_free_queues:\n\tfor (i--; i >= 1; i--)\n\t\tnvme_rdma_free_queue(&ctrl->queues[i]);\n\n\treturn ret;\n}\n\nstatic int nvme_rdma_alloc_tag_set(struct nvme_ctrl *ctrl)\n{\n\tunsigned int cmd_size = sizeof(struct nvme_rdma_request) +\n\t\t\t\tNVME_RDMA_DATA_SGL_SIZE;\n\n\tif (ctrl->max_integrity_segments)\n\t\tcmd_size += sizeof(struct nvme_rdma_sgl) +\n\t\t\t    NVME_RDMA_METADATA_SGL_SIZE;\n\n\treturn nvme_alloc_io_tag_set(ctrl, &to_rdma_ctrl(ctrl)->tag_set,\n\t\t\t&nvme_rdma_mq_ops,\n\t\t\tctrl->opts->nr_poll_queues ? HCTX_MAX_TYPES : 2,\n\t\t\tcmd_size);\n}\n\nstatic void nvme_rdma_destroy_admin_queue(struct nvme_rdma_ctrl *ctrl)\n{\n\tif (ctrl->async_event_sqe.data) {\n\t\tcancel_work_sync(&ctrl->ctrl.async_event_work);\n\t\tnvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,\n\t\t\t\tsizeof(struct nvme_command), DMA_TO_DEVICE);\n\t\tctrl->async_event_sqe.data = NULL;\n\t}\n\tnvme_rdma_free_queue(&ctrl->queues[0]);\n}\n\nstatic int nvme_rdma_configure_admin_queue(struct nvme_rdma_ctrl *ctrl,\n\t\tbool new)\n{\n\tbool pi_capable = false;\n\tint error;\n\n\terror = nvme_rdma_alloc_queue(ctrl, 0, NVME_AQ_DEPTH);\n\tif (error)\n\t\treturn error;\n\n\tctrl->device = ctrl->queues[0].device;\n\tctrl->ctrl.numa_node = ibdev_to_node(ctrl->device->dev);\n\n\t \n\tif (ctrl->device->dev->attrs.kernel_cap_flags &\n\t    IBK_INTEGRITY_HANDOVER)\n\t\tpi_capable = true;\n\n\tctrl->max_fr_pages = nvme_rdma_get_max_fr_pages(ctrl->device->dev,\n\t\t\t\t\t\t\tpi_capable);\n\n\t \n\terror = nvme_rdma_alloc_qe(ctrl->device->dev, &ctrl->async_event_sqe,\n\t\t\tsizeof(struct nvme_command), DMA_TO_DEVICE);\n\tif (error)\n\t\tgoto out_free_queue;\n\n\tif (new) {\n\t\terror = nvme_alloc_admin_tag_set(&ctrl->ctrl,\n\t\t\t\t&ctrl->admin_tag_set, &nvme_rdma_admin_mq_ops,\n\t\t\t\tsizeof(struct nvme_rdma_request) +\n\t\t\t\tNVME_RDMA_DATA_SGL_SIZE);\n\t\tif (error)\n\t\t\tgoto out_free_async_qe;\n\n\t}\n\n\terror = nvme_rdma_start_queue(ctrl, 0);\n\tif (error)\n\t\tgoto out_remove_admin_tag_set;\n\n\terror = nvme_enable_ctrl(&ctrl->ctrl);\n\tif (error)\n\t\tgoto out_stop_queue;\n\n\tctrl->ctrl.max_segments = ctrl->max_fr_pages;\n\tctrl->ctrl.max_hw_sectors = ctrl->max_fr_pages << (ilog2(SZ_4K) - 9);\n\tif (pi_capable)\n\t\tctrl->ctrl.max_integrity_segments = ctrl->max_fr_pages;\n\telse\n\t\tctrl->ctrl.max_integrity_segments = 0;\n\n\tnvme_unquiesce_admin_queue(&ctrl->ctrl);\n\n\terror = nvme_init_ctrl_finish(&ctrl->ctrl, false);\n\tif (error)\n\t\tgoto out_quiesce_queue;\n\n\treturn 0;\n\nout_quiesce_queue:\n\tnvme_quiesce_admin_queue(&ctrl->ctrl);\n\tblk_sync_queue(ctrl->ctrl.admin_q);\nout_stop_queue:\n\tnvme_rdma_stop_queue(&ctrl->queues[0]);\n\tnvme_cancel_admin_tagset(&ctrl->ctrl);\nout_remove_admin_tag_set:\n\tif (new)\n\t\tnvme_remove_admin_tag_set(&ctrl->ctrl);\nout_free_async_qe:\n\tif (ctrl->async_event_sqe.data) {\n\t\tnvme_rdma_free_qe(ctrl->device->dev, &ctrl->async_event_sqe,\n\t\t\tsizeof(struct nvme_command), DMA_TO_DEVICE);\n\t\tctrl->async_event_sqe.data = NULL;\n\t}\nout_free_queue:\n\tnvme_rdma_free_queue(&ctrl->queues[0]);\n\treturn error;\n}\n\nstatic int nvme_rdma_configure_io_queues(struct nvme_rdma_ctrl *ctrl, bool new)\n{\n\tint ret, nr_queues;\n\n\tret = nvme_rdma_alloc_io_queues(ctrl);\n\tif (ret)\n\t\treturn ret;\n\n\tif (new) {\n\t\tret = nvme_rdma_alloc_tag_set(&ctrl->ctrl);\n\t\tif (ret)\n\t\t\tgoto out_free_io_queues;\n\t}\n\n\t \n\tnr_queues = min(ctrl->tag_set.nr_hw_queues + 1, ctrl->ctrl.queue_count);\n\tret = nvme_rdma_start_io_queues(ctrl, 1, nr_queues);\n\tif (ret)\n\t\tgoto out_cleanup_tagset;\n\n\tif (!new) {\n\t\tnvme_start_freeze(&ctrl->ctrl);\n\t\tnvme_unquiesce_io_queues(&ctrl->ctrl);\n\t\tif (!nvme_wait_freeze_timeout(&ctrl->ctrl, NVME_IO_TIMEOUT)) {\n\t\t\t \n\t\t\tret = -ENODEV;\n\t\t\tnvme_unfreeze(&ctrl->ctrl);\n\t\t\tgoto out_wait_freeze_timed_out;\n\t\t}\n\t\tblk_mq_update_nr_hw_queues(ctrl->ctrl.tagset,\n\t\t\tctrl->ctrl.queue_count - 1);\n\t\tnvme_unfreeze(&ctrl->ctrl);\n\t}\n\n\t \n\tret = nvme_rdma_start_io_queues(ctrl, nr_queues,\n\t\t\t\t\tctrl->tag_set.nr_hw_queues + 1);\n\tif (ret)\n\t\tgoto out_wait_freeze_timed_out;\n\n\treturn 0;\n\nout_wait_freeze_timed_out:\n\tnvme_quiesce_io_queues(&ctrl->ctrl);\n\tnvme_sync_io_queues(&ctrl->ctrl);\n\tnvme_rdma_stop_io_queues(ctrl);\nout_cleanup_tagset:\n\tnvme_cancel_tagset(&ctrl->ctrl);\n\tif (new)\n\t\tnvme_remove_io_tag_set(&ctrl->ctrl);\nout_free_io_queues:\n\tnvme_rdma_free_io_queues(ctrl);\n\treturn ret;\n}\n\nstatic void nvme_rdma_teardown_admin_queue(struct nvme_rdma_ctrl *ctrl,\n\t\tbool remove)\n{\n\tnvme_quiesce_admin_queue(&ctrl->ctrl);\n\tblk_sync_queue(ctrl->ctrl.admin_q);\n\tnvme_rdma_stop_queue(&ctrl->queues[0]);\n\tnvme_cancel_admin_tagset(&ctrl->ctrl);\n\tif (remove) {\n\t\tnvme_unquiesce_admin_queue(&ctrl->ctrl);\n\t\tnvme_remove_admin_tag_set(&ctrl->ctrl);\n\t}\n\tnvme_rdma_destroy_admin_queue(ctrl);\n}\n\nstatic void nvme_rdma_teardown_io_queues(struct nvme_rdma_ctrl *ctrl,\n\t\tbool remove)\n{\n\tif (ctrl->ctrl.queue_count > 1) {\n\t\tnvme_quiesce_io_queues(&ctrl->ctrl);\n\t\tnvme_sync_io_queues(&ctrl->ctrl);\n\t\tnvme_rdma_stop_io_queues(ctrl);\n\t\tnvme_cancel_tagset(&ctrl->ctrl);\n\t\tif (remove) {\n\t\t\tnvme_unquiesce_io_queues(&ctrl->ctrl);\n\t\t\tnvme_remove_io_tag_set(&ctrl->ctrl);\n\t\t}\n\t\tnvme_rdma_free_io_queues(ctrl);\n\t}\n}\n\nstatic void nvme_rdma_stop_ctrl(struct nvme_ctrl *nctrl)\n{\n\tstruct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);\n\n\tflush_work(&ctrl->err_work);\n\tcancel_delayed_work_sync(&ctrl->reconnect_work);\n}\n\nstatic void nvme_rdma_free_ctrl(struct nvme_ctrl *nctrl)\n{\n\tstruct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(nctrl);\n\n\tif (list_empty(&ctrl->list))\n\t\tgoto free_ctrl;\n\n\tmutex_lock(&nvme_rdma_ctrl_mutex);\n\tlist_del(&ctrl->list);\n\tmutex_unlock(&nvme_rdma_ctrl_mutex);\n\n\tnvmf_free_options(nctrl->opts);\nfree_ctrl:\n\tkfree(ctrl->queues);\n\tkfree(ctrl);\n}\n\nstatic void nvme_rdma_reconnect_or_remove(struct nvme_rdma_ctrl *ctrl)\n{\n\tenum nvme_ctrl_state state = nvme_ctrl_state(&ctrl->ctrl);\n\n\t \n\tif (state != NVME_CTRL_CONNECTING) {\n\t\tWARN_ON_ONCE(state == NVME_CTRL_NEW || state == NVME_CTRL_LIVE);\n\t\treturn;\n\t}\n\n\tif (nvmf_should_reconnect(&ctrl->ctrl)) {\n\t\tdev_info(ctrl->ctrl.device, \"Reconnecting in %d seconds...\\n\",\n\t\t\tctrl->ctrl.opts->reconnect_delay);\n\t\tqueue_delayed_work(nvme_wq, &ctrl->reconnect_work,\n\t\t\t\tctrl->ctrl.opts->reconnect_delay * HZ);\n\t} else {\n\t\tnvme_delete_ctrl(&ctrl->ctrl);\n\t}\n}\n\nstatic int nvme_rdma_setup_ctrl(struct nvme_rdma_ctrl *ctrl, bool new)\n{\n\tint ret;\n\tbool changed;\n\n\tret = nvme_rdma_configure_admin_queue(ctrl, new);\n\tif (ret)\n\t\treturn ret;\n\n\tif (ctrl->ctrl.icdoff) {\n\t\tret = -EOPNOTSUPP;\n\t\tdev_err(ctrl->ctrl.device, \"icdoff is not supported!\\n\");\n\t\tgoto destroy_admin;\n\t}\n\n\tif (!(ctrl->ctrl.sgls & (1 << 2))) {\n\t\tret = -EOPNOTSUPP;\n\t\tdev_err(ctrl->ctrl.device,\n\t\t\t\"Mandatory keyed sgls are not supported!\\n\");\n\t\tgoto destroy_admin;\n\t}\n\n\tif (ctrl->ctrl.opts->queue_size > ctrl->ctrl.sqsize + 1) {\n\t\tdev_warn(ctrl->ctrl.device,\n\t\t\t\"queue_size %zu > ctrl sqsize %u, clamping down\\n\",\n\t\t\tctrl->ctrl.opts->queue_size, ctrl->ctrl.sqsize + 1);\n\t}\n\n\tif (ctrl->ctrl.sqsize + 1 > NVME_RDMA_MAX_QUEUE_SIZE) {\n\t\tdev_warn(ctrl->ctrl.device,\n\t\t\t\"ctrl sqsize %u > max queue size %u, clamping down\\n\",\n\t\t\tctrl->ctrl.sqsize + 1, NVME_RDMA_MAX_QUEUE_SIZE);\n\t\tctrl->ctrl.sqsize = NVME_RDMA_MAX_QUEUE_SIZE - 1;\n\t}\n\n\tif (ctrl->ctrl.sqsize + 1 > ctrl->ctrl.maxcmd) {\n\t\tdev_warn(ctrl->ctrl.device,\n\t\t\t\"sqsize %u > ctrl maxcmd %u, clamping down\\n\",\n\t\t\tctrl->ctrl.sqsize + 1, ctrl->ctrl.maxcmd);\n\t\tctrl->ctrl.sqsize = ctrl->ctrl.maxcmd - 1;\n\t}\n\n\tif (ctrl->ctrl.sgls & (1 << 20))\n\t\tctrl->use_inline_data = true;\n\n\tif (ctrl->ctrl.queue_count > 1) {\n\t\tret = nvme_rdma_configure_io_queues(ctrl, new);\n\t\tif (ret)\n\t\t\tgoto destroy_admin;\n\t}\n\n\tchanged = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_LIVE);\n\tif (!changed) {\n\t\t \n\t\tenum nvme_ctrl_state state = nvme_ctrl_state(&ctrl->ctrl);\n\n\t\tWARN_ON_ONCE(state != NVME_CTRL_DELETING &&\n\t\t\t     state != NVME_CTRL_DELETING_NOIO);\n\t\tWARN_ON_ONCE(new);\n\t\tret = -EINVAL;\n\t\tgoto destroy_io;\n\t}\n\n\tnvme_start_ctrl(&ctrl->ctrl);\n\treturn 0;\n\ndestroy_io:\n\tif (ctrl->ctrl.queue_count > 1) {\n\t\tnvme_quiesce_io_queues(&ctrl->ctrl);\n\t\tnvme_sync_io_queues(&ctrl->ctrl);\n\t\tnvme_rdma_stop_io_queues(ctrl);\n\t\tnvme_cancel_tagset(&ctrl->ctrl);\n\t\tif (new)\n\t\t\tnvme_remove_io_tag_set(&ctrl->ctrl);\n\t\tnvme_rdma_free_io_queues(ctrl);\n\t}\ndestroy_admin:\n\tnvme_quiesce_admin_queue(&ctrl->ctrl);\n\tblk_sync_queue(ctrl->ctrl.admin_q);\n\tnvme_rdma_stop_queue(&ctrl->queues[0]);\n\tnvme_cancel_admin_tagset(&ctrl->ctrl);\n\tif (new)\n\t\tnvme_remove_admin_tag_set(&ctrl->ctrl);\n\tnvme_rdma_destroy_admin_queue(ctrl);\n\treturn ret;\n}\n\nstatic void nvme_rdma_reconnect_ctrl_work(struct work_struct *work)\n{\n\tstruct nvme_rdma_ctrl *ctrl = container_of(to_delayed_work(work),\n\t\t\tstruct nvme_rdma_ctrl, reconnect_work);\n\n\t++ctrl->ctrl.nr_reconnects;\n\n\tif (nvme_rdma_setup_ctrl(ctrl, false))\n\t\tgoto requeue;\n\n\tdev_info(ctrl->ctrl.device, \"Successfully reconnected (%d attempts)\\n\",\n\t\t\tctrl->ctrl.nr_reconnects);\n\n\tctrl->ctrl.nr_reconnects = 0;\n\n\treturn;\n\nrequeue:\n\tdev_info(ctrl->ctrl.device, \"Failed reconnect attempt %d\\n\",\n\t\t\tctrl->ctrl.nr_reconnects);\n\tnvme_rdma_reconnect_or_remove(ctrl);\n}\n\nstatic void nvme_rdma_error_recovery_work(struct work_struct *work)\n{\n\tstruct nvme_rdma_ctrl *ctrl = container_of(work,\n\t\t\tstruct nvme_rdma_ctrl, err_work);\n\n\tnvme_stop_keep_alive(&ctrl->ctrl);\n\tflush_work(&ctrl->ctrl.async_event_work);\n\tnvme_rdma_teardown_io_queues(ctrl, false);\n\tnvme_unquiesce_io_queues(&ctrl->ctrl);\n\tnvme_rdma_teardown_admin_queue(ctrl, false);\n\tnvme_unquiesce_admin_queue(&ctrl->ctrl);\n\tnvme_auth_stop(&ctrl->ctrl);\n\n\tif (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING)) {\n\t\t \n\t\tenum nvme_ctrl_state state = nvme_ctrl_state(&ctrl->ctrl);\n\n\t\tWARN_ON_ONCE(state != NVME_CTRL_DELETING &&\n\t\t\t     state != NVME_CTRL_DELETING_NOIO);\n\t\treturn;\n\t}\n\n\tnvme_rdma_reconnect_or_remove(ctrl);\n}\n\nstatic void nvme_rdma_error_recovery(struct nvme_rdma_ctrl *ctrl)\n{\n\tif (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_RESETTING))\n\t\treturn;\n\n\tdev_warn(ctrl->ctrl.device, \"starting error recovery\\n\");\n\tqueue_work(nvme_reset_wq, &ctrl->err_work);\n}\n\nstatic void nvme_rdma_end_request(struct nvme_rdma_request *req)\n{\n\tstruct request *rq = blk_mq_rq_from_pdu(req);\n\n\tif (!refcount_dec_and_test(&req->ref))\n\t\treturn;\n\tif (!nvme_try_complete_req(rq, req->status, req->result))\n\t\tnvme_rdma_complete_rq(rq);\n}\n\nstatic void nvme_rdma_wr_error(struct ib_cq *cq, struct ib_wc *wc,\n\t\tconst char *op)\n{\n\tstruct nvme_rdma_queue *queue = wc->qp->qp_context;\n\tstruct nvme_rdma_ctrl *ctrl = queue->ctrl;\n\n\tif (nvme_ctrl_state(&ctrl->ctrl) == NVME_CTRL_LIVE)\n\t\tdev_info(ctrl->ctrl.device,\n\t\t\t     \"%s for CQE 0x%p failed with status %s (%d)\\n\",\n\t\t\t     op, wc->wr_cqe,\n\t\t\t     ib_wc_status_msg(wc->status), wc->status);\n\tnvme_rdma_error_recovery(ctrl);\n}\n\nstatic void nvme_rdma_memreg_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tif (unlikely(wc->status != IB_WC_SUCCESS))\n\t\tnvme_rdma_wr_error(cq, wc, \"MEMREG\");\n}\n\nstatic void nvme_rdma_inv_rkey_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct nvme_rdma_request *req =\n\t\tcontainer_of(wc->wr_cqe, struct nvme_rdma_request, reg_cqe);\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS))\n\t\tnvme_rdma_wr_error(cq, wc, \"LOCAL_INV\");\n\telse\n\t\tnvme_rdma_end_request(req);\n}\n\nstatic int nvme_rdma_inv_rkey(struct nvme_rdma_queue *queue,\n\t\tstruct nvme_rdma_request *req)\n{\n\tstruct ib_send_wr wr = {\n\t\t.opcode\t\t    = IB_WR_LOCAL_INV,\n\t\t.next\t\t    = NULL,\n\t\t.num_sge\t    = 0,\n\t\t.send_flags\t    = IB_SEND_SIGNALED,\n\t\t.ex.invalidate_rkey = req->mr->rkey,\n\t};\n\n\treq->reg_cqe.done = nvme_rdma_inv_rkey_done;\n\twr.wr_cqe = &req->reg_cqe;\n\n\treturn ib_post_send(queue->qp, &wr, NULL);\n}\n\nstatic void nvme_rdma_dma_unmap_req(struct ib_device *ibdev, struct request *rq)\n{\n\tstruct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);\n\n\tif (blk_integrity_rq(rq)) {\n\t\tib_dma_unmap_sg(ibdev, req->metadata_sgl->sg_table.sgl,\n\t\t\t\treq->metadata_sgl->nents, rq_dma_dir(rq));\n\t\tsg_free_table_chained(&req->metadata_sgl->sg_table,\n\t\t\t\t      NVME_INLINE_METADATA_SG_CNT);\n\t}\n\n\tib_dma_unmap_sg(ibdev, req->data_sgl.sg_table.sgl, req->data_sgl.nents,\n\t\t\trq_dma_dir(rq));\n\tsg_free_table_chained(&req->data_sgl.sg_table, NVME_INLINE_SG_CNT);\n}\n\nstatic void nvme_rdma_unmap_data(struct nvme_rdma_queue *queue,\n\t\tstruct request *rq)\n{\n\tstruct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);\n\tstruct nvme_rdma_device *dev = queue->device;\n\tstruct ib_device *ibdev = dev->dev;\n\tstruct list_head *pool = &queue->qp->rdma_mrs;\n\n\tif (!blk_rq_nr_phys_segments(rq))\n\t\treturn;\n\n\tif (req->use_sig_mr)\n\t\tpool = &queue->qp->sig_mrs;\n\n\tif (req->mr) {\n\t\tib_mr_pool_put(queue->qp, pool, req->mr);\n\t\treq->mr = NULL;\n\t}\n\n\tnvme_rdma_dma_unmap_req(ibdev, rq);\n}\n\nstatic int nvme_rdma_set_sg_null(struct nvme_command *c)\n{\n\tstruct nvme_keyed_sgl_desc *sg = &c->common.dptr.ksgl;\n\n\tsg->addr = 0;\n\tput_unaligned_le24(0, sg->length);\n\tput_unaligned_le32(0, sg->key);\n\tsg->type = NVME_KEY_SGL_FMT_DATA_DESC << 4;\n\treturn 0;\n}\n\nstatic int nvme_rdma_map_sg_inline(struct nvme_rdma_queue *queue,\n\t\tstruct nvme_rdma_request *req, struct nvme_command *c,\n\t\tint count)\n{\n\tstruct nvme_sgl_desc *sg = &c->common.dptr.sgl;\n\tstruct ib_sge *sge = &req->sge[1];\n\tstruct scatterlist *sgl;\n\tu32 len = 0;\n\tint i;\n\n\tfor_each_sg(req->data_sgl.sg_table.sgl, sgl, count, i) {\n\t\tsge->addr = sg_dma_address(sgl);\n\t\tsge->length = sg_dma_len(sgl);\n\t\tsge->lkey = queue->device->pd->local_dma_lkey;\n\t\tlen += sge->length;\n\t\tsge++;\n\t}\n\n\tsg->addr = cpu_to_le64(queue->ctrl->ctrl.icdoff);\n\tsg->length = cpu_to_le32(len);\n\tsg->type = (NVME_SGL_FMT_DATA_DESC << 4) | NVME_SGL_FMT_OFFSET;\n\n\treq->num_sge += count;\n\treturn 0;\n}\n\nstatic int nvme_rdma_map_sg_single(struct nvme_rdma_queue *queue,\n\t\tstruct nvme_rdma_request *req, struct nvme_command *c)\n{\n\tstruct nvme_keyed_sgl_desc *sg = &c->common.dptr.ksgl;\n\n\tsg->addr = cpu_to_le64(sg_dma_address(req->data_sgl.sg_table.sgl));\n\tput_unaligned_le24(sg_dma_len(req->data_sgl.sg_table.sgl), sg->length);\n\tput_unaligned_le32(queue->device->pd->unsafe_global_rkey, sg->key);\n\tsg->type = NVME_KEY_SGL_FMT_DATA_DESC << 4;\n\treturn 0;\n}\n\nstatic int nvme_rdma_map_sg_fr(struct nvme_rdma_queue *queue,\n\t\tstruct nvme_rdma_request *req, struct nvme_command *c,\n\t\tint count)\n{\n\tstruct nvme_keyed_sgl_desc *sg = &c->common.dptr.ksgl;\n\tint nr;\n\n\treq->mr = ib_mr_pool_get(queue->qp, &queue->qp->rdma_mrs);\n\tif (WARN_ON_ONCE(!req->mr))\n\t\treturn -EAGAIN;\n\n\t \n\tnr = ib_map_mr_sg(req->mr, req->data_sgl.sg_table.sgl, count, NULL,\n\t\t\t  SZ_4K);\n\tif (unlikely(nr < count)) {\n\t\tib_mr_pool_put(queue->qp, &queue->qp->rdma_mrs, req->mr);\n\t\treq->mr = NULL;\n\t\tif (nr < 0)\n\t\t\treturn nr;\n\t\treturn -EINVAL;\n\t}\n\n\tib_update_fast_reg_key(req->mr, ib_inc_rkey(req->mr->rkey));\n\n\treq->reg_cqe.done = nvme_rdma_memreg_done;\n\tmemset(&req->reg_wr, 0, sizeof(req->reg_wr));\n\treq->reg_wr.wr.opcode = IB_WR_REG_MR;\n\treq->reg_wr.wr.wr_cqe = &req->reg_cqe;\n\treq->reg_wr.wr.num_sge = 0;\n\treq->reg_wr.mr = req->mr;\n\treq->reg_wr.key = req->mr->rkey;\n\treq->reg_wr.access = IB_ACCESS_LOCAL_WRITE |\n\t\t\t     IB_ACCESS_REMOTE_READ |\n\t\t\t     IB_ACCESS_REMOTE_WRITE;\n\n\tsg->addr = cpu_to_le64(req->mr->iova);\n\tput_unaligned_le24(req->mr->length, sg->length);\n\tput_unaligned_le32(req->mr->rkey, sg->key);\n\tsg->type = (NVME_KEY_SGL_FMT_DATA_DESC << 4) |\n\t\t\tNVME_SGL_FMT_INVALIDATE;\n\n\treturn 0;\n}\n\nstatic void nvme_rdma_set_sig_domain(struct blk_integrity *bi,\n\t\tstruct nvme_command *cmd, struct ib_sig_domain *domain,\n\t\tu16 control, u8 pi_type)\n{\n\tdomain->sig_type = IB_SIG_TYPE_T10_DIF;\n\tdomain->sig.dif.bg_type = IB_T10DIF_CRC;\n\tdomain->sig.dif.pi_interval = 1 << bi->interval_exp;\n\tdomain->sig.dif.ref_tag = le32_to_cpu(cmd->rw.reftag);\n\tif (control & NVME_RW_PRINFO_PRCHK_REF)\n\t\tdomain->sig.dif.ref_remap = true;\n\n\tdomain->sig.dif.app_tag = le16_to_cpu(cmd->rw.apptag);\n\tdomain->sig.dif.apptag_check_mask = le16_to_cpu(cmd->rw.appmask);\n\tdomain->sig.dif.app_escape = true;\n\tif (pi_type == NVME_NS_DPS_PI_TYPE3)\n\t\tdomain->sig.dif.ref_escape = true;\n}\n\nstatic void nvme_rdma_set_sig_attrs(struct blk_integrity *bi,\n\t\tstruct nvme_command *cmd, struct ib_sig_attrs *sig_attrs,\n\t\tu8 pi_type)\n{\n\tu16 control = le16_to_cpu(cmd->rw.control);\n\n\tmemset(sig_attrs, 0, sizeof(*sig_attrs));\n\tif (control & NVME_RW_PRINFO_PRACT) {\n\t\t \n\t\tsig_attrs->mem.sig_type = IB_SIG_TYPE_NONE;\n\t\tnvme_rdma_set_sig_domain(bi, cmd, &sig_attrs->wire, control,\n\t\t\t\t\t pi_type);\n\t\t \n\t\tcontrol &= ~NVME_RW_PRINFO_PRACT;\n\t\tcmd->rw.control = cpu_to_le16(control);\n\t} else {\n\t\t \n\t\tnvme_rdma_set_sig_domain(bi, cmd, &sig_attrs->wire, control,\n\t\t\t\t\t pi_type);\n\t\tnvme_rdma_set_sig_domain(bi, cmd, &sig_attrs->mem, control,\n\t\t\t\t\t pi_type);\n\t}\n}\n\nstatic void nvme_rdma_set_prot_checks(struct nvme_command *cmd, u8 *mask)\n{\n\t*mask = 0;\n\tif (le16_to_cpu(cmd->rw.control) & NVME_RW_PRINFO_PRCHK_REF)\n\t\t*mask |= IB_SIG_CHECK_REFTAG;\n\tif (le16_to_cpu(cmd->rw.control) & NVME_RW_PRINFO_PRCHK_GUARD)\n\t\t*mask |= IB_SIG_CHECK_GUARD;\n}\n\nstatic void nvme_rdma_sig_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tif (unlikely(wc->status != IB_WC_SUCCESS))\n\t\tnvme_rdma_wr_error(cq, wc, \"SIG\");\n}\n\nstatic int nvme_rdma_map_sg_pi(struct nvme_rdma_queue *queue,\n\t\tstruct nvme_rdma_request *req, struct nvme_command *c,\n\t\tint count, int pi_count)\n{\n\tstruct nvme_rdma_sgl *sgl = &req->data_sgl;\n\tstruct ib_reg_wr *wr = &req->reg_wr;\n\tstruct request *rq = blk_mq_rq_from_pdu(req);\n\tstruct nvme_ns *ns = rq->q->queuedata;\n\tstruct bio *bio = rq->bio;\n\tstruct nvme_keyed_sgl_desc *sg = &c->common.dptr.ksgl;\n\tint nr;\n\n\treq->mr = ib_mr_pool_get(queue->qp, &queue->qp->sig_mrs);\n\tif (WARN_ON_ONCE(!req->mr))\n\t\treturn -EAGAIN;\n\n\tnr = ib_map_mr_sg_pi(req->mr, sgl->sg_table.sgl, count, NULL,\n\t\t\t     req->metadata_sgl->sg_table.sgl, pi_count, NULL,\n\t\t\t     SZ_4K);\n\tif (unlikely(nr))\n\t\tgoto mr_put;\n\n\tnvme_rdma_set_sig_attrs(blk_get_integrity(bio->bi_bdev->bd_disk), c,\n\t\t\t\treq->mr->sig_attrs, ns->pi_type);\n\tnvme_rdma_set_prot_checks(c, &req->mr->sig_attrs->check_mask);\n\n\tib_update_fast_reg_key(req->mr, ib_inc_rkey(req->mr->rkey));\n\n\treq->reg_cqe.done = nvme_rdma_sig_done;\n\tmemset(wr, 0, sizeof(*wr));\n\twr->wr.opcode = IB_WR_REG_MR_INTEGRITY;\n\twr->wr.wr_cqe = &req->reg_cqe;\n\twr->wr.num_sge = 0;\n\twr->wr.send_flags = 0;\n\twr->mr = req->mr;\n\twr->key = req->mr->rkey;\n\twr->access = IB_ACCESS_LOCAL_WRITE |\n\t\t     IB_ACCESS_REMOTE_READ |\n\t\t     IB_ACCESS_REMOTE_WRITE;\n\n\tsg->addr = cpu_to_le64(req->mr->iova);\n\tput_unaligned_le24(req->mr->length, sg->length);\n\tput_unaligned_le32(req->mr->rkey, sg->key);\n\tsg->type = NVME_KEY_SGL_FMT_DATA_DESC << 4;\n\n\treturn 0;\n\nmr_put:\n\tib_mr_pool_put(queue->qp, &queue->qp->sig_mrs, req->mr);\n\treq->mr = NULL;\n\tif (nr < 0)\n\t\treturn nr;\n\treturn -EINVAL;\n}\n\nstatic int nvme_rdma_dma_map_req(struct ib_device *ibdev, struct request *rq,\n\t\tint *count, int *pi_count)\n{\n\tstruct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);\n\tint ret;\n\n\treq->data_sgl.sg_table.sgl = (struct scatterlist *)(req + 1);\n\tret = sg_alloc_table_chained(&req->data_sgl.sg_table,\n\t\t\tblk_rq_nr_phys_segments(rq), req->data_sgl.sg_table.sgl,\n\t\t\tNVME_INLINE_SG_CNT);\n\tif (ret)\n\t\treturn -ENOMEM;\n\n\treq->data_sgl.nents = blk_rq_map_sg(rq->q, rq,\n\t\t\t\t\t    req->data_sgl.sg_table.sgl);\n\n\t*count = ib_dma_map_sg(ibdev, req->data_sgl.sg_table.sgl,\n\t\t\t       req->data_sgl.nents, rq_dma_dir(rq));\n\tif (unlikely(*count <= 0)) {\n\t\tret = -EIO;\n\t\tgoto out_free_table;\n\t}\n\n\tif (blk_integrity_rq(rq)) {\n\t\treq->metadata_sgl->sg_table.sgl =\n\t\t\t(struct scatterlist *)(req->metadata_sgl + 1);\n\t\tret = sg_alloc_table_chained(&req->metadata_sgl->sg_table,\n\t\t\t\tblk_rq_count_integrity_sg(rq->q, rq->bio),\n\t\t\t\treq->metadata_sgl->sg_table.sgl,\n\t\t\t\tNVME_INLINE_METADATA_SG_CNT);\n\t\tif (unlikely(ret)) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_unmap_sg;\n\t\t}\n\n\t\treq->metadata_sgl->nents = blk_rq_map_integrity_sg(rq->q,\n\t\t\t\trq->bio, req->metadata_sgl->sg_table.sgl);\n\t\t*pi_count = ib_dma_map_sg(ibdev,\n\t\t\t\t\t  req->metadata_sgl->sg_table.sgl,\n\t\t\t\t\t  req->metadata_sgl->nents,\n\t\t\t\t\t  rq_dma_dir(rq));\n\t\tif (unlikely(*pi_count <= 0)) {\n\t\t\tret = -EIO;\n\t\t\tgoto out_free_pi_table;\n\t\t}\n\t}\n\n\treturn 0;\n\nout_free_pi_table:\n\tsg_free_table_chained(&req->metadata_sgl->sg_table,\n\t\t\t      NVME_INLINE_METADATA_SG_CNT);\nout_unmap_sg:\n\tib_dma_unmap_sg(ibdev, req->data_sgl.sg_table.sgl, req->data_sgl.nents,\n\t\t\trq_dma_dir(rq));\nout_free_table:\n\tsg_free_table_chained(&req->data_sgl.sg_table, NVME_INLINE_SG_CNT);\n\treturn ret;\n}\n\nstatic int nvme_rdma_map_data(struct nvme_rdma_queue *queue,\n\t\tstruct request *rq, struct nvme_command *c)\n{\n\tstruct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);\n\tstruct nvme_rdma_device *dev = queue->device;\n\tstruct ib_device *ibdev = dev->dev;\n\tint pi_count = 0;\n\tint count, ret;\n\n\treq->num_sge = 1;\n\trefcount_set(&req->ref, 2);  \n\n\tc->common.flags |= NVME_CMD_SGL_METABUF;\n\n\tif (!blk_rq_nr_phys_segments(rq))\n\t\treturn nvme_rdma_set_sg_null(c);\n\n\tret = nvme_rdma_dma_map_req(ibdev, rq, &count, &pi_count);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\tif (req->use_sig_mr) {\n\t\tret = nvme_rdma_map_sg_pi(queue, req, c, count, pi_count);\n\t\tgoto out;\n\t}\n\n\tif (count <= dev->num_inline_segments) {\n\t\tif (rq_data_dir(rq) == WRITE && nvme_rdma_queue_idx(queue) &&\n\t\t    queue->ctrl->use_inline_data &&\n\t\t    blk_rq_payload_bytes(rq) <=\n\t\t\t\tnvme_rdma_inline_data_size(queue)) {\n\t\t\tret = nvme_rdma_map_sg_inline(queue, req, c, count);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (count == 1 && dev->pd->flags & IB_PD_UNSAFE_GLOBAL_RKEY) {\n\t\t\tret = nvme_rdma_map_sg_single(queue, req, c);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = nvme_rdma_map_sg_fr(queue, req, c, count);\nout:\n\tif (unlikely(ret))\n\t\tgoto out_dma_unmap_req;\n\n\treturn 0;\n\nout_dma_unmap_req:\n\tnvme_rdma_dma_unmap_req(ibdev, rq);\n\treturn ret;\n}\n\nstatic void nvme_rdma_send_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct nvme_rdma_qe *qe =\n\t\tcontainer_of(wc->wr_cqe, struct nvme_rdma_qe, cqe);\n\tstruct nvme_rdma_request *req =\n\t\tcontainer_of(qe, struct nvme_rdma_request, sqe);\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS))\n\t\tnvme_rdma_wr_error(cq, wc, \"SEND\");\n\telse\n\t\tnvme_rdma_end_request(req);\n}\n\nstatic int nvme_rdma_post_send(struct nvme_rdma_queue *queue,\n\t\tstruct nvme_rdma_qe *qe, struct ib_sge *sge, u32 num_sge,\n\t\tstruct ib_send_wr *first)\n{\n\tstruct ib_send_wr wr;\n\tint ret;\n\n\tsge->addr   = qe->dma;\n\tsge->length = sizeof(struct nvme_command);\n\tsge->lkey   = queue->device->pd->local_dma_lkey;\n\n\twr.next       = NULL;\n\twr.wr_cqe     = &qe->cqe;\n\twr.sg_list    = sge;\n\twr.num_sge    = num_sge;\n\twr.opcode     = IB_WR_SEND;\n\twr.send_flags = IB_SEND_SIGNALED;\n\n\tif (first)\n\t\tfirst->next = &wr;\n\telse\n\t\tfirst = &wr;\n\n\tret = ib_post_send(queue->qp, first, NULL);\n\tif (unlikely(ret)) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t     \"%s failed with error code %d\\n\", __func__, ret);\n\t}\n\treturn ret;\n}\n\nstatic int nvme_rdma_post_recv(struct nvme_rdma_queue *queue,\n\t\tstruct nvme_rdma_qe *qe)\n{\n\tstruct ib_recv_wr wr;\n\tstruct ib_sge list;\n\tint ret;\n\n\tlist.addr   = qe->dma;\n\tlist.length = sizeof(struct nvme_completion);\n\tlist.lkey   = queue->device->pd->local_dma_lkey;\n\n\tqe->cqe.done = nvme_rdma_recv_done;\n\n\twr.next     = NULL;\n\twr.wr_cqe   = &qe->cqe;\n\twr.sg_list  = &list;\n\twr.num_sge  = 1;\n\n\tret = ib_post_recv(queue->qp, &wr, NULL);\n\tif (unlikely(ret)) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"%s failed with error code %d\\n\", __func__, ret);\n\t}\n\treturn ret;\n}\n\nstatic struct blk_mq_tags *nvme_rdma_tagset(struct nvme_rdma_queue *queue)\n{\n\tu32 queue_idx = nvme_rdma_queue_idx(queue);\n\n\tif (queue_idx == 0)\n\t\treturn queue->ctrl->admin_tag_set.tags[queue_idx];\n\treturn queue->ctrl->tag_set.tags[queue_idx - 1];\n}\n\nstatic void nvme_rdma_async_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tif (unlikely(wc->status != IB_WC_SUCCESS))\n\t\tnvme_rdma_wr_error(cq, wc, \"ASYNC\");\n}\n\nstatic void nvme_rdma_submit_async_event(struct nvme_ctrl *arg)\n{\n\tstruct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(arg);\n\tstruct nvme_rdma_queue *queue = &ctrl->queues[0];\n\tstruct ib_device *dev = queue->device->dev;\n\tstruct nvme_rdma_qe *sqe = &ctrl->async_event_sqe;\n\tstruct nvme_command *cmd = sqe->data;\n\tstruct ib_sge sge;\n\tint ret;\n\n\tib_dma_sync_single_for_cpu(dev, sqe->dma, sizeof(*cmd), DMA_TO_DEVICE);\n\n\tmemset(cmd, 0, sizeof(*cmd));\n\tcmd->common.opcode = nvme_admin_async_event;\n\tcmd->common.command_id = NVME_AQ_BLK_MQ_DEPTH;\n\tcmd->common.flags |= NVME_CMD_SGL_METABUF;\n\tnvme_rdma_set_sg_null(cmd);\n\n\tsqe->cqe.done = nvme_rdma_async_done;\n\n\tib_dma_sync_single_for_device(dev, sqe->dma, sizeof(*cmd),\n\t\t\tDMA_TO_DEVICE);\n\n\tret = nvme_rdma_post_send(queue, sqe, &sge, 1, NULL);\n\tWARN_ON_ONCE(ret);\n}\n\nstatic void nvme_rdma_process_nvme_rsp(struct nvme_rdma_queue *queue,\n\t\tstruct nvme_completion *cqe, struct ib_wc *wc)\n{\n\tstruct request *rq;\n\tstruct nvme_rdma_request *req;\n\n\trq = nvme_find_rq(nvme_rdma_tagset(queue), cqe->command_id);\n\tif (!rq) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"got bad command_id %#x on QP %#x\\n\",\n\t\t\tcqe->command_id, queue->qp->qp_num);\n\t\tnvme_rdma_error_recovery(queue->ctrl);\n\t\treturn;\n\t}\n\treq = blk_mq_rq_to_pdu(rq);\n\n\treq->status = cqe->status;\n\treq->result = cqe->result;\n\n\tif (wc->wc_flags & IB_WC_WITH_INVALIDATE) {\n\t\tif (unlikely(!req->mr ||\n\t\t\t     wc->ex.invalidate_rkey != req->mr->rkey)) {\n\t\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\t\"Bogus remote invalidation for rkey %#x\\n\",\n\t\t\t\treq->mr ? req->mr->rkey : 0);\n\t\t\tnvme_rdma_error_recovery(queue->ctrl);\n\t\t}\n\t} else if (req->mr) {\n\t\tint ret;\n\n\t\tret = nvme_rdma_inv_rkey(queue, req);\n\t\tif (unlikely(ret < 0)) {\n\t\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\t\"Queueing INV WR for rkey %#x failed (%d)\\n\",\n\t\t\t\treq->mr->rkey, ret);\n\t\t\tnvme_rdma_error_recovery(queue->ctrl);\n\t\t}\n\t\t \n\t\treturn;\n\t}\n\n\tnvme_rdma_end_request(req);\n}\n\nstatic void nvme_rdma_recv_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct nvme_rdma_qe *qe =\n\t\tcontainer_of(wc->wr_cqe, struct nvme_rdma_qe, cqe);\n\tstruct nvme_rdma_queue *queue = wc->qp->qp_context;\n\tstruct ib_device *ibdev = queue->device->dev;\n\tstruct nvme_completion *cqe = qe->data;\n\tconst size_t len = sizeof(struct nvme_completion);\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS)) {\n\t\tnvme_rdma_wr_error(cq, wc, \"RECV\");\n\t\treturn;\n\t}\n\n\t \n\tif (unlikely(wc->byte_len < len)) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"Unexpected nvme completion length(%d)\\n\", wc->byte_len);\n\t\tnvme_rdma_error_recovery(queue->ctrl);\n\t\treturn;\n\t}\n\n\tib_dma_sync_single_for_cpu(ibdev, qe->dma, len, DMA_FROM_DEVICE);\n\t \n\tif (unlikely(nvme_is_aen_req(nvme_rdma_queue_idx(queue),\n\t\t\t\t     cqe->command_id)))\n\t\tnvme_complete_async_event(&queue->ctrl->ctrl, cqe->status,\n\t\t\t\t&cqe->result);\n\telse\n\t\tnvme_rdma_process_nvme_rsp(queue, cqe, wc);\n\tib_dma_sync_single_for_device(ibdev, qe->dma, len, DMA_FROM_DEVICE);\n\n\tnvme_rdma_post_recv(queue, qe);\n}\n\nstatic int nvme_rdma_conn_established(struct nvme_rdma_queue *queue)\n{\n\tint ret, i;\n\n\tfor (i = 0; i < queue->queue_size; i++) {\n\t\tret = nvme_rdma_post_recv(queue, &queue->rsp_ring[i]);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int nvme_rdma_conn_rejected(struct nvme_rdma_queue *queue,\n\t\tstruct rdma_cm_event *ev)\n{\n\tstruct rdma_cm_id *cm_id = queue->cm_id;\n\tint status = ev->status;\n\tconst char *rej_msg;\n\tconst struct nvme_rdma_cm_rej *rej_data;\n\tu8 rej_data_len;\n\n\trej_msg = rdma_reject_msg(cm_id, status);\n\trej_data = rdma_consumer_reject_data(cm_id, ev, &rej_data_len);\n\n\tif (rej_data && rej_data_len >= sizeof(u16)) {\n\t\tu16 sts = le16_to_cpu(rej_data->sts);\n\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t      \"Connect rejected: status %d (%s) nvme status %d (%s).\\n\",\n\t\t      status, rej_msg, sts, nvme_rdma_cm_msg(sts));\n\t} else {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"Connect rejected: status %d (%s).\\n\", status, rej_msg);\n\t}\n\n\treturn -ECONNRESET;\n}\n\nstatic int nvme_rdma_addr_resolved(struct nvme_rdma_queue *queue)\n{\n\tstruct nvme_ctrl *ctrl = &queue->ctrl->ctrl;\n\tint ret;\n\n\tret = nvme_rdma_create_queue_ib(queue);\n\tif (ret)\n\t\treturn ret;\n\n\tif (ctrl->opts->tos >= 0)\n\t\trdma_set_service_type(queue->cm_id, ctrl->opts->tos);\n\tret = rdma_resolve_route(queue->cm_id, NVME_RDMA_CM_TIMEOUT_MS);\n\tif (ret) {\n\t\tdev_err(ctrl->device, \"rdma_resolve_route failed (%d).\\n\",\n\t\t\tqueue->cm_error);\n\t\tgoto out_destroy_queue;\n\t}\n\n\treturn 0;\n\nout_destroy_queue:\n\tnvme_rdma_destroy_queue_ib(queue);\n\treturn ret;\n}\n\nstatic int nvme_rdma_route_resolved(struct nvme_rdma_queue *queue)\n{\n\tstruct nvme_rdma_ctrl *ctrl = queue->ctrl;\n\tstruct rdma_conn_param param = { };\n\tstruct nvme_rdma_cm_req priv = { };\n\tint ret;\n\n\tparam.qp_num = queue->qp->qp_num;\n\tparam.flow_control = 1;\n\n\tparam.responder_resources = queue->device->dev->attrs.max_qp_rd_atom;\n\t \n\tparam.retry_count = 7;\n\tparam.rnr_retry_count = 7;\n\tparam.private_data = &priv;\n\tparam.private_data_len = sizeof(priv);\n\n\tpriv.recfmt = cpu_to_le16(NVME_RDMA_CM_FMT_1_0);\n\tpriv.qid = cpu_to_le16(nvme_rdma_queue_idx(queue));\n\t \n\tif (priv.qid == 0) {\n\t\tpriv.hrqsize = cpu_to_le16(NVME_AQ_DEPTH);\n\t\tpriv.hsqsize = cpu_to_le16(NVME_AQ_DEPTH - 1);\n\t} else {\n\t\t \n\t\tpriv.hrqsize = cpu_to_le16(queue->queue_size);\n\t\tpriv.hsqsize = cpu_to_le16(queue->ctrl->ctrl.sqsize);\n\t}\n\n\tret = rdma_connect_locked(queue->cm_id, &param);\n\tif (ret) {\n\t\tdev_err(ctrl->ctrl.device,\n\t\t\t\"rdma_connect_locked failed (%d).\\n\", ret);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int nvme_rdma_cm_handler(struct rdma_cm_id *cm_id,\n\t\tstruct rdma_cm_event *ev)\n{\n\tstruct nvme_rdma_queue *queue = cm_id->context;\n\tint cm_error = 0;\n\n\tdev_dbg(queue->ctrl->ctrl.device, \"%s (%d): status %d id %p\\n\",\n\t\trdma_event_msg(ev->event), ev->event,\n\t\tev->status, cm_id);\n\n\tswitch (ev->event) {\n\tcase RDMA_CM_EVENT_ADDR_RESOLVED:\n\t\tcm_error = nvme_rdma_addr_resolved(queue);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_ROUTE_RESOLVED:\n\t\tcm_error = nvme_rdma_route_resolved(queue);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_ESTABLISHED:\n\t\tqueue->cm_error = nvme_rdma_conn_established(queue);\n\t\t \n\t\tcomplete(&queue->cm_done);\n\t\treturn 0;\n\tcase RDMA_CM_EVENT_REJECTED:\n\t\tcm_error = nvme_rdma_conn_rejected(queue, ev);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_ROUTE_ERROR:\n\tcase RDMA_CM_EVENT_CONNECT_ERROR:\n\tcase RDMA_CM_EVENT_UNREACHABLE:\n\tcase RDMA_CM_EVENT_ADDR_ERROR:\n\t\tdev_dbg(queue->ctrl->ctrl.device,\n\t\t\t\"CM error event %d\\n\", ev->event);\n\t\tcm_error = -ECONNRESET;\n\t\tbreak;\n\tcase RDMA_CM_EVENT_DISCONNECTED:\n\tcase RDMA_CM_EVENT_ADDR_CHANGE:\n\tcase RDMA_CM_EVENT_TIMEWAIT_EXIT:\n\t\tdev_dbg(queue->ctrl->ctrl.device,\n\t\t\t\"disconnect received - connection closed\\n\");\n\t\tnvme_rdma_error_recovery(queue->ctrl);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_DEVICE_REMOVAL:\n\t\t \n\t\tbreak;\n\tdefault:\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t\"Unexpected RDMA CM event (%d)\\n\", ev->event);\n\t\tnvme_rdma_error_recovery(queue->ctrl);\n\t\tbreak;\n\t}\n\n\tif (cm_error) {\n\t\tqueue->cm_error = cm_error;\n\t\tcomplete(&queue->cm_done);\n\t}\n\n\treturn 0;\n}\n\nstatic void nvme_rdma_complete_timed_out(struct request *rq)\n{\n\tstruct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);\n\tstruct nvme_rdma_queue *queue = req->queue;\n\n\tnvme_rdma_stop_queue(queue);\n\tnvmf_complete_timed_out_request(rq);\n}\n\nstatic enum blk_eh_timer_return nvme_rdma_timeout(struct request *rq)\n{\n\tstruct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);\n\tstruct nvme_rdma_queue *queue = req->queue;\n\tstruct nvme_rdma_ctrl *ctrl = queue->ctrl;\n\n\tdev_warn(ctrl->ctrl.device, \"I/O %d QID %d timeout\\n\",\n\t\t rq->tag, nvme_rdma_queue_idx(queue));\n\n\tif (nvme_ctrl_state(&ctrl->ctrl) != NVME_CTRL_LIVE) {\n\t\t \n\t\tnvme_rdma_complete_timed_out(rq);\n\t\treturn BLK_EH_DONE;\n\t}\n\n\t \n\tnvme_rdma_error_recovery(ctrl);\n\treturn BLK_EH_RESET_TIMER;\n}\n\nstatic blk_status_t nvme_rdma_queue_rq(struct blk_mq_hw_ctx *hctx,\n\t\tconst struct blk_mq_queue_data *bd)\n{\n\tstruct nvme_ns *ns = hctx->queue->queuedata;\n\tstruct nvme_rdma_queue *queue = hctx->driver_data;\n\tstruct request *rq = bd->rq;\n\tstruct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);\n\tstruct nvme_rdma_qe *sqe = &req->sqe;\n\tstruct nvme_command *c = nvme_req(rq)->cmd;\n\tstruct ib_device *dev;\n\tbool queue_ready = test_bit(NVME_RDMA_Q_LIVE, &queue->flags);\n\tblk_status_t ret;\n\tint err;\n\n\tWARN_ON_ONCE(rq->tag < 0);\n\n\tif (!nvme_check_ready(&queue->ctrl->ctrl, rq, queue_ready))\n\t\treturn nvme_fail_nonready_command(&queue->ctrl->ctrl, rq);\n\n\tdev = queue->device->dev;\n\n\treq->sqe.dma = ib_dma_map_single(dev, req->sqe.data,\n\t\t\t\t\t sizeof(struct nvme_command),\n\t\t\t\t\t DMA_TO_DEVICE);\n\terr = ib_dma_mapping_error(dev, req->sqe.dma);\n\tif (unlikely(err))\n\t\treturn BLK_STS_RESOURCE;\n\n\tib_dma_sync_single_for_cpu(dev, sqe->dma,\n\t\t\tsizeof(struct nvme_command), DMA_TO_DEVICE);\n\n\tret = nvme_setup_cmd(ns, rq);\n\tif (ret)\n\t\tgoto unmap_qe;\n\n\tnvme_start_request(rq);\n\n\tif (IS_ENABLED(CONFIG_BLK_DEV_INTEGRITY) &&\n\t    queue->pi_support &&\n\t    (c->common.opcode == nvme_cmd_write ||\n\t     c->common.opcode == nvme_cmd_read) &&\n\t    nvme_ns_has_pi(ns))\n\t\treq->use_sig_mr = true;\n\telse\n\t\treq->use_sig_mr = false;\n\n\terr = nvme_rdma_map_data(queue, rq, c);\n\tif (unlikely(err < 0)) {\n\t\tdev_err(queue->ctrl->ctrl.device,\n\t\t\t     \"Failed to map data (%d)\\n\", err);\n\t\tgoto err;\n\t}\n\n\tsqe->cqe.done = nvme_rdma_send_done;\n\n\tib_dma_sync_single_for_device(dev, sqe->dma,\n\t\t\tsizeof(struct nvme_command), DMA_TO_DEVICE);\n\n\terr = nvme_rdma_post_send(queue, sqe, req->sge, req->num_sge,\n\t\t\treq->mr ? &req->reg_wr.wr : NULL);\n\tif (unlikely(err))\n\t\tgoto err_unmap;\n\n\treturn BLK_STS_OK;\n\nerr_unmap:\n\tnvme_rdma_unmap_data(queue, rq);\nerr:\n\tif (err == -EIO)\n\t\tret = nvme_host_path_error(rq);\n\telse if (err == -ENOMEM || err == -EAGAIN)\n\t\tret = BLK_STS_RESOURCE;\n\telse\n\t\tret = BLK_STS_IOERR;\n\tnvme_cleanup_cmd(rq);\nunmap_qe:\n\tib_dma_unmap_single(dev, req->sqe.dma, sizeof(struct nvme_command),\n\t\t\t    DMA_TO_DEVICE);\n\treturn ret;\n}\n\nstatic int nvme_rdma_poll(struct blk_mq_hw_ctx *hctx, struct io_comp_batch *iob)\n{\n\tstruct nvme_rdma_queue *queue = hctx->driver_data;\n\n\treturn ib_process_cq_direct(queue->ib_cq, -1);\n}\n\nstatic void nvme_rdma_check_pi_status(struct nvme_rdma_request *req)\n{\n\tstruct request *rq = blk_mq_rq_from_pdu(req);\n\tstruct ib_mr_status mr_status;\n\tint ret;\n\n\tret = ib_check_mr_status(req->mr, IB_MR_CHECK_SIG_STATUS, &mr_status);\n\tif (ret) {\n\t\tpr_err(\"ib_check_mr_status failed, ret %d\\n\", ret);\n\t\tnvme_req(rq)->status = NVME_SC_INVALID_PI;\n\t\treturn;\n\t}\n\n\tif (mr_status.fail_status & IB_MR_CHECK_SIG_STATUS) {\n\t\tswitch (mr_status.sig_err.err_type) {\n\t\tcase IB_SIG_BAD_GUARD:\n\t\t\tnvme_req(rq)->status = NVME_SC_GUARD_CHECK;\n\t\t\tbreak;\n\t\tcase IB_SIG_BAD_REFTAG:\n\t\t\tnvme_req(rq)->status = NVME_SC_REFTAG_CHECK;\n\t\t\tbreak;\n\t\tcase IB_SIG_BAD_APPTAG:\n\t\t\tnvme_req(rq)->status = NVME_SC_APPTAG_CHECK;\n\t\t\tbreak;\n\t\t}\n\t\tpr_err(\"PI error found type %d expected 0x%x vs actual 0x%x\\n\",\n\t\t       mr_status.sig_err.err_type, mr_status.sig_err.expected,\n\t\t       mr_status.sig_err.actual);\n\t}\n}\n\nstatic void nvme_rdma_complete_rq(struct request *rq)\n{\n\tstruct nvme_rdma_request *req = blk_mq_rq_to_pdu(rq);\n\tstruct nvme_rdma_queue *queue = req->queue;\n\tstruct ib_device *ibdev = queue->device->dev;\n\n\tif (req->use_sig_mr)\n\t\tnvme_rdma_check_pi_status(req);\n\n\tnvme_rdma_unmap_data(queue, rq);\n\tib_dma_unmap_single(ibdev, req->sqe.dma, sizeof(struct nvme_command),\n\t\t\t    DMA_TO_DEVICE);\n\tnvme_complete_rq(rq);\n}\n\nstatic void nvme_rdma_map_queues(struct blk_mq_tag_set *set)\n{\n\tstruct nvme_rdma_ctrl *ctrl = to_rdma_ctrl(set->driver_data);\n\n\tnvmf_map_queues(set, &ctrl->ctrl, ctrl->io_queues);\n}\n\nstatic const struct blk_mq_ops nvme_rdma_mq_ops = {\n\t.queue_rq\t= nvme_rdma_queue_rq,\n\t.complete\t= nvme_rdma_complete_rq,\n\t.init_request\t= nvme_rdma_init_request,\n\t.exit_request\t= nvme_rdma_exit_request,\n\t.init_hctx\t= nvme_rdma_init_hctx,\n\t.timeout\t= nvme_rdma_timeout,\n\t.map_queues\t= nvme_rdma_map_queues,\n\t.poll\t\t= nvme_rdma_poll,\n};\n\nstatic const struct blk_mq_ops nvme_rdma_admin_mq_ops = {\n\t.queue_rq\t= nvme_rdma_queue_rq,\n\t.complete\t= nvme_rdma_complete_rq,\n\t.init_request\t= nvme_rdma_init_request,\n\t.exit_request\t= nvme_rdma_exit_request,\n\t.init_hctx\t= nvme_rdma_init_admin_hctx,\n\t.timeout\t= nvme_rdma_timeout,\n};\n\nstatic void nvme_rdma_shutdown_ctrl(struct nvme_rdma_ctrl *ctrl, bool shutdown)\n{\n\tnvme_rdma_teardown_io_queues(ctrl, shutdown);\n\tnvme_quiesce_admin_queue(&ctrl->ctrl);\n\tnvme_disable_ctrl(&ctrl->ctrl, shutdown);\n\tnvme_rdma_teardown_admin_queue(ctrl, shutdown);\n}\n\nstatic void nvme_rdma_delete_ctrl(struct nvme_ctrl *ctrl)\n{\n\tnvme_rdma_shutdown_ctrl(to_rdma_ctrl(ctrl), true);\n}\n\nstatic void nvme_rdma_reset_ctrl_work(struct work_struct *work)\n{\n\tstruct nvme_rdma_ctrl *ctrl =\n\t\tcontainer_of(work, struct nvme_rdma_ctrl, ctrl.reset_work);\n\n\tnvme_stop_ctrl(&ctrl->ctrl);\n\tnvme_rdma_shutdown_ctrl(ctrl, false);\n\n\tif (!nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING)) {\n\t\t \n\t\tWARN_ON_ONCE(1);\n\t\treturn;\n\t}\n\n\tif (nvme_rdma_setup_ctrl(ctrl, false))\n\t\tgoto out_fail;\n\n\treturn;\n\nout_fail:\n\t++ctrl->ctrl.nr_reconnects;\n\tnvme_rdma_reconnect_or_remove(ctrl);\n}\n\nstatic const struct nvme_ctrl_ops nvme_rdma_ctrl_ops = {\n\t.name\t\t\t= \"rdma\",\n\t.module\t\t\t= THIS_MODULE,\n\t.flags\t\t\t= NVME_F_FABRICS | NVME_F_METADATA_SUPPORTED,\n\t.reg_read32\t\t= nvmf_reg_read32,\n\t.reg_read64\t\t= nvmf_reg_read64,\n\t.reg_write32\t\t= nvmf_reg_write32,\n\t.free_ctrl\t\t= nvme_rdma_free_ctrl,\n\t.submit_async_event\t= nvme_rdma_submit_async_event,\n\t.delete_ctrl\t\t= nvme_rdma_delete_ctrl,\n\t.get_address\t\t= nvmf_get_address,\n\t.stop_ctrl\t\t= nvme_rdma_stop_ctrl,\n};\n\n \nstatic bool\nnvme_rdma_existing_controller(struct nvmf_ctrl_options *opts)\n{\n\tstruct nvme_rdma_ctrl *ctrl;\n\tbool found = false;\n\n\tmutex_lock(&nvme_rdma_ctrl_mutex);\n\tlist_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {\n\t\tfound = nvmf_ip_options_match(&ctrl->ctrl, opts);\n\t\tif (found)\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&nvme_rdma_ctrl_mutex);\n\n\treturn found;\n}\n\nstatic struct nvme_ctrl *nvme_rdma_create_ctrl(struct device *dev,\n\t\tstruct nvmf_ctrl_options *opts)\n{\n\tstruct nvme_rdma_ctrl *ctrl;\n\tint ret;\n\tbool changed;\n\n\tctrl = kzalloc(sizeof(*ctrl), GFP_KERNEL);\n\tif (!ctrl)\n\t\treturn ERR_PTR(-ENOMEM);\n\tctrl->ctrl.opts = opts;\n\tINIT_LIST_HEAD(&ctrl->list);\n\n\tif (!(opts->mask & NVMF_OPT_TRSVCID)) {\n\t\topts->trsvcid =\n\t\t\tkstrdup(__stringify(NVME_RDMA_IP_PORT), GFP_KERNEL);\n\t\tif (!opts->trsvcid) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_free_ctrl;\n\t\t}\n\t\topts->mask |= NVMF_OPT_TRSVCID;\n\t}\n\n\tret = inet_pton_with_scope(&init_net, AF_UNSPEC,\n\t\t\topts->traddr, opts->trsvcid, &ctrl->addr);\n\tif (ret) {\n\t\tpr_err(\"malformed address passed: %s:%s\\n\",\n\t\t\topts->traddr, opts->trsvcid);\n\t\tgoto out_free_ctrl;\n\t}\n\n\tif (opts->mask & NVMF_OPT_HOST_TRADDR) {\n\t\tret = inet_pton_with_scope(&init_net, AF_UNSPEC,\n\t\t\topts->host_traddr, NULL, &ctrl->src_addr);\n\t\tif (ret) {\n\t\t\tpr_err(\"malformed src address passed: %s\\n\",\n\t\t\t       opts->host_traddr);\n\t\t\tgoto out_free_ctrl;\n\t\t}\n\t}\n\n\tif (!opts->duplicate_connect && nvme_rdma_existing_controller(opts)) {\n\t\tret = -EALREADY;\n\t\tgoto out_free_ctrl;\n\t}\n\n\tINIT_DELAYED_WORK(&ctrl->reconnect_work,\n\t\t\tnvme_rdma_reconnect_ctrl_work);\n\tINIT_WORK(&ctrl->err_work, nvme_rdma_error_recovery_work);\n\tINIT_WORK(&ctrl->ctrl.reset_work, nvme_rdma_reset_ctrl_work);\n\n\tctrl->ctrl.queue_count = opts->nr_io_queues + opts->nr_write_queues +\n\t\t\t\topts->nr_poll_queues + 1;\n\tctrl->ctrl.sqsize = opts->queue_size - 1;\n\tctrl->ctrl.kato = opts->kato;\n\n\tret = -ENOMEM;\n\tctrl->queues = kcalloc(ctrl->ctrl.queue_count, sizeof(*ctrl->queues),\n\t\t\t\tGFP_KERNEL);\n\tif (!ctrl->queues)\n\t\tgoto out_free_ctrl;\n\n\tret = nvme_init_ctrl(&ctrl->ctrl, dev, &nvme_rdma_ctrl_ops,\n\t\t\t\t0  );\n\tif (ret)\n\t\tgoto out_kfree_queues;\n\n\tchanged = nvme_change_ctrl_state(&ctrl->ctrl, NVME_CTRL_CONNECTING);\n\tWARN_ON_ONCE(!changed);\n\n\tret = nvme_rdma_setup_ctrl(ctrl, true);\n\tif (ret)\n\t\tgoto out_uninit_ctrl;\n\n\tdev_info(ctrl->ctrl.device, \"new ctrl: NQN \\\"%s\\\", addr %pISpcs\\n\",\n\t\tnvmf_ctrl_subsysnqn(&ctrl->ctrl), &ctrl->addr);\n\n\tmutex_lock(&nvme_rdma_ctrl_mutex);\n\tlist_add_tail(&ctrl->list, &nvme_rdma_ctrl_list);\n\tmutex_unlock(&nvme_rdma_ctrl_mutex);\n\n\treturn &ctrl->ctrl;\n\nout_uninit_ctrl:\n\tnvme_uninit_ctrl(&ctrl->ctrl);\n\tnvme_put_ctrl(&ctrl->ctrl);\n\tif (ret > 0)\n\t\tret = -EIO;\n\treturn ERR_PTR(ret);\nout_kfree_queues:\n\tkfree(ctrl->queues);\nout_free_ctrl:\n\tkfree(ctrl);\n\treturn ERR_PTR(ret);\n}\n\nstatic struct nvmf_transport_ops nvme_rdma_transport = {\n\t.name\t\t= \"rdma\",\n\t.module\t\t= THIS_MODULE,\n\t.required_opts\t= NVMF_OPT_TRADDR,\n\t.allowed_opts\t= NVMF_OPT_TRSVCID | NVMF_OPT_RECONNECT_DELAY |\n\t\t\t  NVMF_OPT_HOST_TRADDR | NVMF_OPT_CTRL_LOSS_TMO |\n\t\t\t  NVMF_OPT_NR_WRITE_QUEUES | NVMF_OPT_NR_POLL_QUEUES |\n\t\t\t  NVMF_OPT_TOS,\n\t.create_ctrl\t= nvme_rdma_create_ctrl,\n};\n\nstatic void nvme_rdma_remove_one(struct ib_device *ib_device, void *client_data)\n{\n\tstruct nvme_rdma_ctrl *ctrl;\n\tstruct nvme_rdma_device *ndev;\n\tbool found = false;\n\n\tmutex_lock(&device_list_mutex);\n\tlist_for_each_entry(ndev, &device_list, entry) {\n\t\tif (ndev->dev == ib_device) {\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&device_list_mutex);\n\n\tif (!found)\n\t\treturn;\n\n\t \n\tmutex_lock(&nvme_rdma_ctrl_mutex);\n\tlist_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list) {\n\t\tif (ctrl->device->dev != ib_device)\n\t\t\tcontinue;\n\t\tnvme_delete_ctrl(&ctrl->ctrl);\n\t}\n\tmutex_unlock(&nvme_rdma_ctrl_mutex);\n\n\tflush_workqueue(nvme_delete_wq);\n}\n\nstatic struct ib_client nvme_rdma_ib_client = {\n\t.name   = \"nvme_rdma\",\n\t.remove = nvme_rdma_remove_one\n};\n\nstatic int __init nvme_rdma_init_module(void)\n{\n\tint ret;\n\n\tret = ib_register_client(&nvme_rdma_ib_client);\n\tif (ret)\n\t\treturn ret;\n\n\tret = nvmf_register_transport(&nvme_rdma_transport);\n\tif (ret)\n\t\tgoto err_unreg_client;\n\n\treturn 0;\n\nerr_unreg_client:\n\tib_unregister_client(&nvme_rdma_ib_client);\n\treturn ret;\n}\n\nstatic void __exit nvme_rdma_cleanup_module(void)\n{\n\tstruct nvme_rdma_ctrl *ctrl;\n\n\tnvmf_unregister_transport(&nvme_rdma_transport);\n\tib_unregister_client(&nvme_rdma_ib_client);\n\n\tmutex_lock(&nvme_rdma_ctrl_mutex);\n\tlist_for_each_entry(ctrl, &nvme_rdma_ctrl_list, list)\n\t\tnvme_delete_ctrl(&ctrl->ctrl);\n\tmutex_unlock(&nvme_rdma_ctrl_mutex);\n\tflush_workqueue(nvme_delete_wq);\n}\n\nmodule_init(nvme_rdma_init_module);\nmodule_exit(nvme_rdma_cleanup_module);\n\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}