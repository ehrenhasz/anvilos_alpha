{
  "module_name": "zns.c",
  "hash_id": "bed0d5c9f09876af13b35ec7e482f114414b21db2b0b4f991625585be041b764",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvme/host/zns.c",
  "human_readable_source": "\n \n\n#include <linux/blkdev.h>\n#include <linux/vmalloc.h>\n#include \"nvme.h\"\n\nint nvme_revalidate_zones(struct nvme_ns *ns)\n{\n\tstruct request_queue *q = ns->queue;\n\n\tblk_queue_chunk_sectors(q, ns->zsze);\n\tblk_queue_max_zone_append_sectors(q, ns->ctrl->max_zone_append);\n\n\treturn blk_revalidate_disk_zones(ns->disk, NULL);\n}\n\nstatic int nvme_set_max_append(struct nvme_ctrl *ctrl)\n{\n\tstruct nvme_command c = { };\n\tstruct nvme_id_ctrl_zns *id;\n\tint status;\n\n\tid = kzalloc(sizeof(*id), GFP_KERNEL);\n\tif (!id)\n\t\treturn -ENOMEM;\n\n\tc.identify.opcode = nvme_admin_identify;\n\tc.identify.cns = NVME_ID_CNS_CS_CTRL;\n\tc.identify.csi = NVME_CSI_ZNS;\n\n\tstatus = nvme_submit_sync_cmd(ctrl->admin_q, &c, id, sizeof(*id));\n\tif (status) {\n\t\tkfree(id);\n\t\treturn status;\n\t}\n\n\tif (id->zasl)\n\t\tctrl->max_zone_append = 1 << (id->zasl + 3);\n\telse\n\t\tctrl->max_zone_append = ctrl->max_hw_sectors;\n\tkfree(id);\n\treturn 0;\n}\n\nint nvme_update_zone_info(struct nvme_ns *ns, unsigned lbaf)\n{\n\tstruct nvme_effects_log *log = ns->head->effects;\n\tstruct request_queue *q = ns->queue;\n\tstruct nvme_command c = { };\n\tstruct nvme_id_ns_zns *id;\n\tint status;\n\n\t \n\tif ((le32_to_cpu(log->iocs[nvme_cmd_zone_append]) &\n\t\t\tNVME_CMD_EFFECTS_CSUPP)) {\n\t\tif (test_and_clear_bit(NVME_NS_FORCE_RO, &ns->flags))\n\t\t\tdev_warn(ns->ctrl->device,\n\t\t\t\t \"Zone Append supported for zoned namespace:%d. Remove read-only mode\\n\",\n\t\t\t\t ns->head->ns_id);\n\t} else {\n\t\tset_bit(NVME_NS_FORCE_RO, &ns->flags);\n\t\tdev_warn(ns->ctrl->device,\n\t\t\t \"Zone Append not supported for zoned namespace:%d. Forcing to read-only mode\\n\",\n\t\t\t ns->head->ns_id);\n\t}\n\n\t \n\tif (!ns->ctrl->max_zone_append) {\n\t\tstatus = nvme_set_max_append(ns->ctrl);\n\t\tif (status)\n\t\t\treturn status;\n\t}\n\n\tid = kzalloc(sizeof(*id), GFP_KERNEL);\n\tif (!id)\n\t\treturn -ENOMEM;\n\n\tc.identify.opcode = nvme_admin_identify;\n\tc.identify.nsid = cpu_to_le32(ns->head->ns_id);\n\tc.identify.cns = NVME_ID_CNS_CS_NS;\n\tc.identify.csi = NVME_CSI_ZNS;\n\n\tstatus = nvme_submit_sync_cmd(ns->ctrl->admin_q, &c, id, sizeof(*id));\n\tif (status)\n\t\tgoto free_data;\n\n\t \n\tif (id->zoc) {\n\t\tdev_warn(ns->ctrl->device,\n\t\t\t\"zone operations:%x not supported for namespace:%u\\n\",\n\t\t\tle16_to_cpu(id->zoc), ns->head->ns_id);\n\t\tstatus = -ENODEV;\n\t\tgoto free_data;\n\t}\n\n\tns->zsze = nvme_lba_to_sect(ns, le64_to_cpu(id->lbafe[lbaf].zsze));\n\tif (!is_power_of_2(ns->zsze)) {\n\t\tdev_warn(ns->ctrl->device,\n\t\t\t\"invalid zone size:%llu for namespace:%u\\n\",\n\t\t\tns->zsze, ns->head->ns_id);\n\t\tstatus = -ENODEV;\n\t\tgoto free_data;\n\t}\n\n\tdisk_set_zoned(ns->disk, BLK_ZONED_HM);\n\tblk_queue_flag_set(QUEUE_FLAG_ZONE_RESETALL, q);\n\tdisk_set_max_open_zones(ns->disk, le32_to_cpu(id->mor) + 1);\n\tdisk_set_max_active_zones(ns->disk, le32_to_cpu(id->mar) + 1);\nfree_data:\n\tkfree(id);\n\treturn status;\n}\n\nstatic void *nvme_zns_alloc_report_buffer(struct nvme_ns *ns,\n\t\t\t\t\t  unsigned int nr_zones, size_t *buflen)\n{\n\tstruct request_queue *q = ns->disk->queue;\n\tsize_t bufsize;\n\tvoid *buf;\n\n\tconst size_t min_bufsize = sizeof(struct nvme_zone_report) +\n\t\t\t\t   sizeof(struct nvme_zone_descriptor);\n\n\tnr_zones = min_t(unsigned int, nr_zones,\n\t\t\t get_capacity(ns->disk) >> ilog2(ns->zsze));\n\n\tbufsize = sizeof(struct nvme_zone_report) +\n\t\tnr_zones * sizeof(struct nvme_zone_descriptor);\n\tbufsize = min_t(size_t, bufsize,\n\t\t\tqueue_max_hw_sectors(q) << SECTOR_SHIFT);\n\tbufsize = min_t(size_t, bufsize, queue_max_segments(q) << PAGE_SHIFT);\n\n\twhile (bufsize >= min_bufsize) {\n\t\tbuf = __vmalloc(bufsize, GFP_KERNEL | __GFP_NORETRY);\n\t\tif (buf) {\n\t\t\t*buflen = bufsize;\n\t\t\treturn buf;\n\t\t}\n\t\tbufsize >>= 1;\n\t}\n\treturn NULL;\n}\n\nstatic int nvme_zone_parse_entry(struct nvme_ns *ns,\n\t\t\t\t struct nvme_zone_descriptor *entry,\n\t\t\t\t unsigned int idx, report_zones_cb cb,\n\t\t\t\t void *data)\n{\n\tstruct blk_zone zone = { };\n\n\tif ((entry->zt & 0xf) != NVME_ZONE_TYPE_SEQWRITE_REQ) {\n\t\tdev_err(ns->ctrl->device, \"invalid zone type %#x\\n\",\n\t\t\t\tentry->zt);\n\t\treturn -EINVAL;\n\t}\n\n\tzone.type = BLK_ZONE_TYPE_SEQWRITE_REQ;\n\tzone.cond = entry->zs >> 4;\n\tzone.len = ns->zsze;\n\tzone.capacity = nvme_lba_to_sect(ns, le64_to_cpu(entry->zcap));\n\tzone.start = nvme_lba_to_sect(ns, le64_to_cpu(entry->zslba));\n\tif (zone.cond == BLK_ZONE_COND_FULL)\n\t\tzone.wp = zone.start + zone.len;\n\telse\n\t\tzone.wp = nvme_lba_to_sect(ns, le64_to_cpu(entry->wp));\n\n\treturn cb(&zone, idx, data);\n}\n\nint nvme_ns_report_zones(struct nvme_ns *ns, sector_t sector,\n\t\tunsigned int nr_zones, report_zones_cb cb, void *data)\n{\n\tstruct nvme_zone_report *report;\n\tstruct nvme_command c = { };\n\tint ret, zone_idx = 0;\n\tunsigned int nz, i;\n\tsize_t buflen;\n\n\tif (ns->head->ids.csi != NVME_CSI_ZNS)\n\t\treturn -EINVAL;\n\n\treport = nvme_zns_alloc_report_buffer(ns, nr_zones, &buflen);\n\tif (!report)\n\t\treturn -ENOMEM;\n\n\tc.zmr.opcode = nvme_cmd_zone_mgmt_recv;\n\tc.zmr.nsid = cpu_to_le32(ns->head->ns_id);\n\tc.zmr.numd = cpu_to_le32(nvme_bytes_to_numd(buflen));\n\tc.zmr.zra = NVME_ZRA_ZONE_REPORT;\n\tc.zmr.zrasf = NVME_ZRASF_ZONE_REPORT_ALL;\n\tc.zmr.pr = NVME_REPORT_ZONE_PARTIAL;\n\n\tsector &= ~(ns->zsze - 1);\n\twhile (zone_idx < nr_zones && sector < get_capacity(ns->disk)) {\n\t\tmemset(report, 0, buflen);\n\n\t\tc.zmr.slba = cpu_to_le64(nvme_sect_to_lba(ns, sector));\n\t\tret = nvme_submit_sync_cmd(ns->queue, &c, report, buflen);\n\t\tif (ret) {\n\t\t\tif (ret > 0)\n\t\t\t\tret = -EIO;\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tnz = min((unsigned int)le64_to_cpu(report->nr_zones), nr_zones);\n\t\tif (!nz)\n\t\t\tbreak;\n\n\t\tfor (i = 0; i < nz && zone_idx < nr_zones; i++) {\n\t\t\tret = nvme_zone_parse_entry(ns, &report->entries[i],\n\t\t\t\t\t\t    zone_idx, cb, data);\n\t\t\tif (ret)\n\t\t\t\tgoto out_free;\n\t\t\tzone_idx++;\n\t\t}\n\n\t\tsector += ns->zsze * nz;\n\t}\n\n\tif (zone_idx > 0)\n\t\tret = zone_idx;\n\telse\n\t\tret = -EINVAL;\nout_free:\n\tkvfree(report);\n\treturn ret;\n}\n\nblk_status_t nvme_setup_zone_mgmt_send(struct nvme_ns *ns, struct request *req,\n\t\tstruct nvme_command *c, enum nvme_zone_mgmt_action action)\n{\n\tmemset(c, 0, sizeof(*c));\n\n\tc->zms.opcode = nvme_cmd_zone_mgmt_send;\n\tc->zms.nsid = cpu_to_le32(ns->head->ns_id);\n\tc->zms.slba = cpu_to_le64(nvme_sect_to_lba(ns, blk_rq_pos(req)));\n\tc->zms.zsa = action;\n\n\tif (req_op(req) == REQ_OP_ZONE_RESET_ALL)\n\t\tc->zms.select_all = 1;\n\n\treturn BLK_STS_OK;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}