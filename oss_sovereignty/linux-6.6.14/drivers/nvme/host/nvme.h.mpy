{
  "module_name": "nvme.h",
  "hash_id": "70b3e49076e004234d08428faf33b19feb75564691c2f89d5b0471f01267c686",
  "original_prompt": "Ingested from linux-6.6.14/drivers/nvme/host/nvme.h",
  "human_readable_source": " \n \n\n#ifndef _NVME_H\n#define _NVME_H\n\n#include <linux/nvme.h>\n#include <linux/cdev.h>\n#include <linux/pci.h>\n#include <linux/kref.h>\n#include <linux/blk-mq.h>\n#include <linux/sed-opal.h>\n#include <linux/fault-inject.h>\n#include <linux/rcupdate.h>\n#include <linux/wait.h>\n#include <linux/t10-pi.h>\n\n#include <trace/events/block.h>\n\nextern const struct pr_ops nvme_pr_ops;\n\nextern unsigned int nvme_io_timeout;\n#define NVME_IO_TIMEOUT\t(nvme_io_timeout * HZ)\n\nextern unsigned int admin_timeout;\n#define NVME_ADMIN_TIMEOUT\t(admin_timeout * HZ)\n\n#define NVME_DEFAULT_KATO\t5\n\n#ifdef CONFIG_ARCH_NO_SG_CHAIN\n#define  NVME_INLINE_SG_CNT  0\n#define  NVME_INLINE_METADATA_SG_CNT  0\n#else\n#define  NVME_INLINE_SG_CNT  2\n#define  NVME_INLINE_METADATA_SG_CNT  1\n#endif\n\n \n#define NVME_CTRL_PAGE_SHIFT\t12\n#define NVME_CTRL_PAGE_SIZE\t(1 << NVME_CTRL_PAGE_SHIFT)\n\nextern struct workqueue_struct *nvme_wq;\nextern struct workqueue_struct *nvme_reset_wq;\nextern struct workqueue_struct *nvme_delete_wq;\n\n \nenum nvme_quirks {\n\t \n\tNVME_QUIRK_STRIPE_SIZE\t\t\t= (1 << 0),\n\n\t \n\tNVME_QUIRK_IDENTIFY_CNS\t\t\t= (1 << 1),\n\n\t \n\tNVME_QUIRK_DEALLOCATE_ZEROES\t\t= (1 << 2),\n\n\t \n\tNVME_QUIRK_DELAY_BEFORE_CHK_RDY\t\t= (1 << 3),\n\n\t \n\tNVME_QUIRK_NO_APST\t\t\t= (1 << 4),\n\n\t \n\tNVME_QUIRK_NO_DEEPEST_PS\t\t= (1 << 5),\n\n\t \n\tNVME_QUIRK_MEDIUM_PRIO_SQ\t\t= (1 << 7),\n\n\t \n\tNVME_QUIRK_IGNORE_DEV_SUBNQN\t\t= (1 << 8),\n\n\t \n\tNVME_QUIRK_DISABLE_WRITE_ZEROES\t\t= (1 << 9),\n\n\t \n\tNVME_QUIRK_SIMPLE_SUSPEND\t\t= (1 << 10),\n\n\t \n\tNVME_QUIRK_SINGLE_VECTOR\t\t= (1 << 11),\n\n\t \n\tNVME_QUIRK_128_BYTES_SQES\t\t= (1 << 12),\n\n\t \n\tNVME_QUIRK_SHARED_TAGS                  = (1 << 13),\n\n\t \n\tNVME_QUIRK_NO_TEMP_THRESH_CHANGE\t= (1 << 14),\n\n\t \n\tNVME_QUIRK_NO_NS_DESC_LIST\t\t= (1 << 15),\n\n\t \n\tNVME_QUIRK_DMA_ADDRESS_BITS_48\t\t= (1 << 16),\n\n\t \n\tNVME_QUIRK_SKIP_CID_GEN\t\t\t= (1 << 17),\n\n\t \n\tNVME_QUIRK_BOGUS_NID\t\t\t= (1 << 18),\n\n\t \n\tNVME_QUIRK_NO_SECONDARY_TEMP_THRESH\t= (1 << 19),\n\n\t \n\tNVME_QUIRK_FORCE_NO_SIMPLE_SUSPEND\t= (1 << 20),\n};\n\n \nstruct nvme_request {\n\tstruct nvme_command\t*cmd;\n\tunion nvme_result\tresult;\n\tu8\t\t\tgenctr;\n\tu8\t\t\tretries;\n\tu8\t\t\tflags;\n\tu16\t\t\tstatus;\n#ifdef CONFIG_NVME_MULTIPATH\n\tunsigned long\t\tstart_time;\n#endif\n\tstruct nvme_ctrl\t*ctrl;\n};\n\n \n#define REQ_NVME_MPATH\t\tREQ_DRV\n\nenum {\n\tNVME_REQ_CANCELLED\t\t= (1 << 0),\n\tNVME_REQ_USERCMD\t\t= (1 << 1),\n\tNVME_MPATH_IO_STATS\t\t= (1 << 2),\n};\n\nstatic inline struct nvme_request *nvme_req(struct request *req)\n{\n\treturn blk_mq_rq_to_pdu(req);\n}\n\nstatic inline u16 nvme_req_qid(struct request *req)\n{\n\tif (!req->q->queuedata)\n\t\treturn 0;\n\n\treturn req->mq_hctx->queue_num + 1;\n}\n\n \n#define NVME_QUIRK_DELAY_AMOUNT\t\t2300\n\n \nenum nvme_ctrl_state {\n\tNVME_CTRL_NEW,\n\tNVME_CTRL_LIVE,\n\tNVME_CTRL_RESETTING,\n\tNVME_CTRL_CONNECTING,\n\tNVME_CTRL_DELETING,\n\tNVME_CTRL_DELETING_NOIO,\n\tNVME_CTRL_DEAD,\n};\n\nstruct nvme_fault_inject {\n#ifdef CONFIG_FAULT_INJECTION_DEBUG_FS\n\tstruct fault_attr attr;\n\tstruct dentry *parent;\n\tbool dont_retry;\t \n\tu16 status;\t\t \n#endif\n};\n\nenum nvme_ctrl_flags {\n\tNVME_CTRL_FAILFAST_EXPIRED\t= 0,\n\tNVME_CTRL_ADMIN_Q_STOPPED\t= 1,\n\tNVME_CTRL_STARTED_ONCE\t\t= 2,\n\tNVME_CTRL_STOPPED\t\t= 3,\n\tNVME_CTRL_SKIP_ID_CNS_CS\t= 4,\n\tNVME_CTRL_DIRTY_CAPABILITY\t= 5,\n\tNVME_CTRL_FROZEN\t\t= 6,\n};\n\nstruct nvme_ctrl {\n\tbool comp_seen;\n\tbool identified;\n\tenum nvme_ctrl_state state;\n\tspinlock_t lock;\n\tstruct mutex scan_lock;\n\tconst struct nvme_ctrl_ops *ops;\n\tstruct request_queue *admin_q;\n\tstruct request_queue *connect_q;\n\tstruct request_queue *fabrics_q;\n\tstruct device *dev;\n\tint instance;\n\tint numa_node;\n\tstruct blk_mq_tag_set *tagset;\n\tstruct blk_mq_tag_set *admin_tagset;\n\tstruct list_head namespaces;\n\tstruct rw_semaphore namespaces_rwsem;\n\tstruct device ctrl_device;\n\tstruct device *device;\t \n#ifdef CONFIG_NVME_HWMON\n\tstruct device *hwmon_device;\n#endif\n\tstruct cdev cdev;\n\tstruct work_struct reset_work;\n\tstruct work_struct delete_work;\n\twait_queue_head_t state_wq;\n\n\tstruct nvme_subsystem *subsys;\n\tstruct list_head subsys_entry;\n\n\tstruct opal_dev *opal_dev;\n\n\tchar name[12];\n\tu16 cntlid;\n\n\tu16 mtfa;\n\tu32 ctrl_config;\n\tu32 queue_count;\n\n\tu64 cap;\n\tu32 max_hw_sectors;\n\tu32 max_segments;\n\tu32 max_integrity_segments;\n\tu32 max_discard_sectors;\n\tu32 max_discard_segments;\n\tu32 max_zeroes_sectors;\n#ifdef CONFIG_BLK_DEV_ZONED\n\tu32 max_zone_append;\n#endif\n\tu16 crdt[3];\n\tu16 oncs;\n\tu32 dmrsl;\n\tu16 oacs;\n\tu16 sqsize;\n\tu32 max_namespaces;\n\tatomic_t abort_limit;\n\tu8 vwc;\n\tu32 vs;\n\tu32 sgls;\n\tu16 kas;\n\tu8 npss;\n\tu8 apsta;\n\tu16 wctemp;\n\tu16 cctemp;\n\tu32 oaes;\n\tu32 aen_result;\n\tu32 ctratt;\n\tunsigned int shutdown_timeout;\n\tunsigned int kato;\n\tbool subsystem;\n\tunsigned long quirks;\n\tstruct nvme_id_power_state psd[32];\n\tstruct nvme_effects_log *effects;\n\tstruct xarray cels;\n\tstruct work_struct scan_work;\n\tstruct work_struct async_event_work;\n\tstruct delayed_work ka_work;\n\tstruct delayed_work failfast_work;\n\tstruct nvme_command ka_cmd;\n\tunsigned long ka_last_check_time;\n\tstruct work_struct fw_act_work;\n\tunsigned long events;\n\n#ifdef CONFIG_NVME_MULTIPATH\n\t \n\tu8 anacap;\n\tu8 anatt;\n\tu32 anagrpmax;\n\tu32 nanagrpid;\n\tstruct mutex ana_lock;\n\tstruct nvme_ana_rsp_hdr *ana_log_buf;\n\tsize_t ana_log_size;\n\tstruct timer_list anatt_timer;\n\tstruct work_struct ana_work;\n#endif\n\n#ifdef CONFIG_NVME_AUTH\n\tstruct work_struct dhchap_auth_work;\n\tstruct mutex dhchap_auth_mutex;\n\tstruct nvme_dhchap_queue_context *dhchap_ctxs;\n\tstruct nvme_dhchap_key *host_key;\n\tstruct nvme_dhchap_key *ctrl_key;\n\tu16 transaction;\n#endif\n\n\t \n\tu64 ps_max_latency_us;\n\tbool apst_enabled;\n\n\t \n\tu16 hmmaxd;\n\tu32 hmpre;\n\tu32 hmmin;\n\tu32 hmminds;\n\n\t \n\tu32 ioccsz;\n\tu32 iorcsz;\n\tu16 icdoff;\n\tu16 maxcmd;\n\tint nr_reconnects;\n\tunsigned long flags;\n\tstruct nvmf_ctrl_options *opts;\n\n\tstruct page *discard_page;\n\tunsigned long discard_page_busy;\n\n\tstruct nvme_fault_inject fault_inject;\n\n\tenum nvme_ctrl_type cntrltype;\n\tenum nvme_dctype dctype;\n};\n\nstatic inline enum nvme_ctrl_state nvme_ctrl_state(struct nvme_ctrl *ctrl)\n{\n\treturn READ_ONCE(ctrl->state);\n}\n\nenum nvme_iopolicy {\n\tNVME_IOPOLICY_NUMA,\n\tNVME_IOPOLICY_RR,\n};\n\nstruct nvme_subsystem {\n\tint\t\t\tinstance;\n\tstruct device\t\tdev;\n\t \n\tstruct kref\t\tref;\n\tstruct list_head\tentry;\n\tstruct mutex\t\tlock;\n\tstruct list_head\tctrls;\n\tstruct list_head\tnsheads;\n\tchar\t\t\tsubnqn[NVMF_NQN_SIZE];\n\tchar\t\t\tserial[20];\n\tchar\t\t\tmodel[40];\n\tchar\t\t\tfirmware_rev[8];\n\tu8\t\t\tcmic;\n\tenum nvme_subsys_type\tsubtype;\n\tu16\t\t\tvendor_id;\n\tu16\t\t\tawupf;\t \n\tstruct ida\t\tns_ida;\n#ifdef CONFIG_NVME_MULTIPATH\n\tenum nvme_iopolicy\tiopolicy;\n#endif\n};\n\n \nstruct nvme_ns_ids {\n\tu8\teui64[8];\n\tu8\tnguid[16];\n\tuuid_t\tuuid;\n\tu8\tcsi;\n};\n\n \nstruct nvme_ns_head {\n\tstruct list_head\tlist;\n\tstruct srcu_struct      srcu;\n\tstruct nvme_subsystem\t*subsys;\n\tunsigned\t\tns_id;\n\tstruct nvme_ns_ids\tids;\n\tstruct list_head\tentry;\n\tstruct kref\t\tref;\n\tbool\t\t\tshared;\n\tint\t\t\tinstance;\n\tstruct nvme_effects_log *effects;\n\n\tstruct cdev\t\tcdev;\n\tstruct device\t\tcdev_device;\n\n\tstruct gendisk\t\t*disk;\n#ifdef CONFIG_NVME_MULTIPATH\n\tstruct bio_list\t\trequeue_list;\n\tspinlock_t\t\trequeue_lock;\n\tstruct work_struct\trequeue_work;\n\tstruct mutex\t\tlock;\n\tunsigned long\t\tflags;\n#define NVME_NSHEAD_DISK_LIVE\t0\n\tstruct nvme_ns __rcu\t*current_path[];\n#endif\n};\n\nstatic inline bool nvme_ns_head_multipath(struct nvme_ns_head *head)\n{\n\treturn IS_ENABLED(CONFIG_NVME_MULTIPATH) && head->disk;\n}\n\nenum nvme_ns_features {\n\tNVME_NS_EXT_LBAS = 1 << 0,  \n\tNVME_NS_METADATA_SUPPORTED = 1 << 1,  \n\tNVME_NS_DEAC,\t\t \n};\n\nstruct nvme_ns {\n\tstruct list_head list;\n\n\tstruct nvme_ctrl *ctrl;\n\tstruct request_queue *queue;\n\tstruct gendisk *disk;\n#ifdef CONFIG_NVME_MULTIPATH\n\tenum nvme_ana_state ana_state;\n\tu32 ana_grpid;\n#endif\n\tstruct list_head siblings;\n\tstruct kref kref;\n\tstruct nvme_ns_head *head;\n\n\tint lba_shift;\n\tu16 ms;\n\tu16 pi_size;\n\tu16 sgs;\n\tu32 sws;\n\tu8 pi_type;\n\tu8 guard_type;\n#ifdef CONFIG_BLK_DEV_ZONED\n\tu64 zsze;\n#endif\n\tunsigned long features;\n\tunsigned long flags;\n#define NVME_NS_REMOVING\t0\n#define NVME_NS_ANA_PENDING\t2\n#define NVME_NS_FORCE_RO\t3\n#define NVME_NS_READY\t\t4\n\n\tstruct cdev\t\tcdev;\n\tstruct device\t\tcdev_device;\n\n\tstruct nvme_fault_inject fault_inject;\n\n};\n\n \nstatic inline bool nvme_ns_has_pi(struct nvme_ns *ns)\n{\n\treturn ns->pi_type && ns->ms == ns->pi_size;\n}\n\nstruct nvme_ctrl_ops {\n\tconst char *name;\n\tstruct module *module;\n\tunsigned int flags;\n#define NVME_F_FABRICS\t\t\t(1 << 0)\n#define NVME_F_METADATA_SUPPORTED\t(1 << 1)\n#define NVME_F_BLOCKING\t\t\t(1 << 2)\n\n\tconst struct attribute_group **dev_attr_groups;\n\tint (*reg_read32)(struct nvme_ctrl *ctrl, u32 off, u32 *val);\n\tint (*reg_write32)(struct nvme_ctrl *ctrl, u32 off, u32 val);\n\tint (*reg_read64)(struct nvme_ctrl *ctrl, u32 off, u64 *val);\n\tvoid (*free_ctrl)(struct nvme_ctrl *ctrl);\n\tvoid (*submit_async_event)(struct nvme_ctrl *ctrl);\n\tvoid (*delete_ctrl)(struct nvme_ctrl *ctrl);\n\tvoid (*stop_ctrl)(struct nvme_ctrl *ctrl);\n\tint (*get_address)(struct nvme_ctrl *ctrl, char *buf, int size);\n\tvoid (*print_device_info)(struct nvme_ctrl *ctrl);\n\tbool (*supports_pci_p2pdma)(struct nvme_ctrl *ctrl);\n};\n\n \n#define nvme_genctr_mask(gen)\t\t\t(gen & 0xf)\n#define nvme_cid_install_genctr(gen)\t\t(nvme_genctr_mask(gen) << 12)\n#define nvme_genctr_from_cid(cid)\t\t((cid & 0xf000) >> 12)\n#define nvme_tag_from_cid(cid)\t\t\t(cid & 0xfff)\n\nstatic inline u16 nvme_cid(struct request *rq)\n{\n\treturn nvme_cid_install_genctr(nvme_req(rq)->genctr) | rq->tag;\n}\n\nstatic inline struct request *nvme_find_rq(struct blk_mq_tags *tags,\n\t\tu16 command_id)\n{\n\tu8 genctr = nvme_genctr_from_cid(command_id);\n\tu16 tag = nvme_tag_from_cid(command_id);\n\tstruct request *rq;\n\n\trq = blk_mq_tag_to_rq(tags, tag);\n\tif (unlikely(!rq)) {\n\t\tpr_err(\"could not locate request for tag %#x\\n\",\n\t\t\ttag);\n\t\treturn NULL;\n\t}\n\tif (unlikely(nvme_genctr_mask(nvme_req(rq)->genctr) != genctr)) {\n\t\tdev_err(nvme_req(rq)->ctrl->device,\n\t\t\t\"request %#x genctr mismatch (got %#x expected %#x)\\n\",\n\t\t\ttag, genctr, nvme_genctr_mask(nvme_req(rq)->genctr));\n\t\treturn NULL;\n\t}\n\treturn rq;\n}\n\nstatic inline struct request *nvme_cid_to_rq(struct blk_mq_tags *tags,\n                u16 command_id)\n{\n\treturn blk_mq_tag_to_rq(tags, nvme_tag_from_cid(command_id));\n}\n\n \nstatic inline int nvme_strlen(char *s, int len)\n{\n\twhile (s[len - 1] == ' ')\n\t\tlen--;\n\treturn len;\n}\n\nstatic inline void nvme_print_device_info(struct nvme_ctrl *ctrl)\n{\n\tstruct nvme_subsystem *subsys = ctrl->subsys;\n\n\tif (ctrl->ops->print_device_info) {\n\t\tctrl->ops->print_device_info(ctrl);\n\t\treturn;\n\t}\n\n\tdev_err(ctrl->device,\n\t\t\"VID:%04x model:%.*s firmware:%.*s\\n\", subsys->vendor_id,\n\t\tnvme_strlen(subsys->model, sizeof(subsys->model)),\n\t\tsubsys->model, nvme_strlen(subsys->firmware_rev,\n\t\t\t\t\t   sizeof(subsys->firmware_rev)),\n\t\tsubsys->firmware_rev);\n}\n\n#ifdef CONFIG_FAULT_INJECTION_DEBUG_FS\nvoid nvme_fault_inject_init(struct nvme_fault_inject *fault_inj,\n\t\t\t    const char *dev_name);\nvoid nvme_fault_inject_fini(struct nvme_fault_inject *fault_inject);\nvoid nvme_should_fail(struct request *req);\n#else\nstatic inline void nvme_fault_inject_init(struct nvme_fault_inject *fault_inj,\n\t\t\t\t\t  const char *dev_name)\n{\n}\nstatic inline void nvme_fault_inject_fini(struct nvme_fault_inject *fault_inj)\n{\n}\nstatic inline void nvme_should_fail(struct request *req) {}\n#endif\n\nbool nvme_wait_reset(struct nvme_ctrl *ctrl);\nint nvme_try_sched_reset(struct nvme_ctrl *ctrl);\n\nstatic inline int nvme_reset_subsystem(struct nvme_ctrl *ctrl)\n{\n\tint ret;\n\n\tif (!ctrl->subsystem)\n\t\treturn -ENOTTY;\n\tif (!nvme_wait_reset(ctrl))\n\t\treturn -EBUSY;\n\n\tret = ctrl->ops->reg_write32(ctrl, NVME_REG_NSSR, 0x4E564D65);\n\tif (ret)\n\t\treturn ret;\n\n\treturn nvme_try_sched_reset(ctrl);\n}\n\n \nstatic inline u64 nvme_sect_to_lba(struct nvme_ns *ns, sector_t sector)\n{\n\treturn sector >> (ns->lba_shift - SECTOR_SHIFT);\n}\n\n \nstatic inline sector_t nvme_lba_to_sect(struct nvme_ns *ns, u64 lba)\n{\n\treturn lba << (ns->lba_shift - SECTOR_SHIFT);\n}\n\n \nstatic inline u32 nvme_bytes_to_numd(size_t len)\n{\n\treturn (len >> 2) - 1;\n}\n\nstatic inline bool nvme_is_ana_error(u16 status)\n{\n\tswitch (status & 0x7ff) {\n\tcase NVME_SC_ANA_TRANSITION:\n\tcase NVME_SC_ANA_INACCESSIBLE:\n\tcase NVME_SC_ANA_PERSISTENT_LOSS:\n\t\treturn true;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic inline bool nvme_is_path_error(u16 status)\n{\n\t \n\treturn (status & 0x700) == 0x300;\n}\n\n \nstatic inline bool nvme_try_complete_req(struct request *req, __le16 status,\n\t\tunion nvme_result result)\n{\n\tstruct nvme_request *rq = nvme_req(req);\n\tstruct nvme_ctrl *ctrl = rq->ctrl;\n\n\tif (!(ctrl->quirks & NVME_QUIRK_SKIP_CID_GEN))\n\t\trq->genctr++;\n\n\trq->status = le16_to_cpu(status) >> 1;\n\trq->result = result;\n\t \n\tnvme_should_fail(req);\n\tif (unlikely(blk_should_fake_timeout(req->q)))\n\t\treturn true;\n\treturn blk_mq_complete_request_remote(req);\n}\n\nstatic inline void nvme_get_ctrl(struct nvme_ctrl *ctrl)\n{\n\tget_device(ctrl->device);\n}\n\nstatic inline void nvme_put_ctrl(struct nvme_ctrl *ctrl)\n{\n\tput_device(ctrl->device);\n}\n\nstatic inline bool nvme_is_aen_req(u16 qid, __u16 command_id)\n{\n\treturn !qid &&\n\t\tnvme_tag_from_cid(command_id) >= NVME_AQ_BLK_MQ_DEPTH;\n}\n\nvoid nvme_complete_rq(struct request *req);\nvoid nvme_complete_batch_req(struct request *req);\n\nstatic __always_inline void nvme_complete_batch(struct io_comp_batch *iob,\n\t\t\t\t\t\tvoid (*fn)(struct request *rq))\n{\n\tstruct request *req;\n\n\trq_list_for_each(&iob->req_list, req) {\n\t\tfn(req);\n\t\tnvme_complete_batch_req(req);\n\t}\n\tblk_mq_end_request_batch(iob);\n}\n\nblk_status_t nvme_host_path_error(struct request *req);\nbool nvme_cancel_request(struct request *req, void *data);\nvoid nvme_cancel_tagset(struct nvme_ctrl *ctrl);\nvoid nvme_cancel_admin_tagset(struct nvme_ctrl *ctrl);\nbool nvme_change_ctrl_state(struct nvme_ctrl *ctrl,\n\t\tenum nvme_ctrl_state new_state);\nint nvme_disable_ctrl(struct nvme_ctrl *ctrl, bool shutdown);\nint nvme_enable_ctrl(struct nvme_ctrl *ctrl);\nint nvme_init_ctrl(struct nvme_ctrl *ctrl, struct device *dev,\n\t\tconst struct nvme_ctrl_ops *ops, unsigned long quirks);\nvoid nvme_uninit_ctrl(struct nvme_ctrl *ctrl);\nvoid nvme_start_ctrl(struct nvme_ctrl *ctrl);\nvoid nvme_stop_ctrl(struct nvme_ctrl *ctrl);\nint nvme_init_ctrl_finish(struct nvme_ctrl *ctrl, bool was_suspended);\nint nvme_alloc_admin_tag_set(struct nvme_ctrl *ctrl, struct blk_mq_tag_set *set,\n\t\tconst struct blk_mq_ops *ops, unsigned int cmd_size);\nvoid nvme_remove_admin_tag_set(struct nvme_ctrl *ctrl);\nint nvme_alloc_io_tag_set(struct nvme_ctrl *ctrl, struct blk_mq_tag_set *set,\n\t\tconst struct blk_mq_ops *ops, unsigned int nr_maps,\n\t\tunsigned int cmd_size);\nvoid nvme_remove_io_tag_set(struct nvme_ctrl *ctrl);\n\nvoid nvme_remove_namespaces(struct nvme_ctrl *ctrl);\n\nvoid nvme_complete_async_event(struct nvme_ctrl *ctrl, __le16 status,\n\t\tvolatile union nvme_result *res);\n\nvoid nvme_quiesce_io_queues(struct nvme_ctrl *ctrl);\nvoid nvme_unquiesce_io_queues(struct nvme_ctrl *ctrl);\nvoid nvme_quiesce_admin_queue(struct nvme_ctrl *ctrl);\nvoid nvme_unquiesce_admin_queue(struct nvme_ctrl *ctrl);\nvoid nvme_mark_namespaces_dead(struct nvme_ctrl *ctrl);\nvoid nvme_sync_queues(struct nvme_ctrl *ctrl);\nvoid nvme_sync_io_queues(struct nvme_ctrl *ctrl);\nvoid nvme_unfreeze(struct nvme_ctrl *ctrl);\nvoid nvme_wait_freeze(struct nvme_ctrl *ctrl);\nint nvme_wait_freeze_timeout(struct nvme_ctrl *ctrl, long timeout);\nvoid nvme_start_freeze(struct nvme_ctrl *ctrl);\n\nstatic inline enum req_op nvme_req_op(struct nvme_command *cmd)\n{\n\treturn nvme_is_write(cmd) ? REQ_OP_DRV_OUT : REQ_OP_DRV_IN;\n}\n\n#define NVME_QID_ANY -1\nvoid nvme_init_request(struct request *req, struct nvme_command *cmd);\nvoid nvme_cleanup_cmd(struct request *req);\nblk_status_t nvme_setup_cmd(struct nvme_ns *ns, struct request *req);\nblk_status_t nvme_fail_nonready_command(struct nvme_ctrl *ctrl,\n\t\tstruct request *req);\nbool __nvme_check_ready(struct nvme_ctrl *ctrl, struct request *rq,\n\t\tbool queue_live);\n\nstatic inline bool nvme_check_ready(struct nvme_ctrl *ctrl, struct request *rq,\n\t\tbool queue_live)\n{\n\tif (likely(ctrl->state == NVME_CTRL_LIVE))\n\t\treturn true;\n\tif (ctrl->ops->flags & NVME_F_FABRICS &&\n\t    ctrl->state == NVME_CTRL_DELETING)\n\t\treturn queue_live;\n\treturn __nvme_check_ready(ctrl, rq, queue_live);\n}\n\n \nstatic inline bool nvme_is_unique_nsid(struct nvme_ctrl *ctrl,\n\t\tstruct nvme_ns_head *head)\n{\n\treturn head->shared ||\n\t\t(ctrl->oacs & NVME_CTRL_OACS_NS_MNGT_SUPP) ||\n\t\t(ctrl->subsys->cmic & NVME_CTRL_CMIC_ANA) ||\n\t\t(ctrl->ctratt & NVME_CTRL_CTRATT_NVM_SETS);\n}\n\nint nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,\n\t\tvoid *buf, unsigned bufflen);\nint __nvme_submit_sync_cmd(struct request_queue *q, struct nvme_command *cmd,\n\t\tunion nvme_result *result, void *buffer, unsigned bufflen,\n\t\tint qid, int at_head,\n\t\tblk_mq_req_flags_t flags);\nint nvme_set_features(struct nvme_ctrl *dev, unsigned int fid,\n\t\t      unsigned int dword11, void *buffer, size_t buflen,\n\t\t      u32 *result);\nint nvme_get_features(struct nvme_ctrl *dev, unsigned int fid,\n\t\t      unsigned int dword11, void *buffer, size_t buflen,\n\t\t      u32 *result);\nint nvme_set_queue_count(struct nvme_ctrl *ctrl, int *count);\nvoid nvme_stop_keep_alive(struct nvme_ctrl *ctrl);\nint nvme_reset_ctrl(struct nvme_ctrl *ctrl);\nint nvme_reset_ctrl_sync(struct nvme_ctrl *ctrl);\nint nvme_delete_ctrl(struct nvme_ctrl *ctrl);\nvoid nvme_queue_scan(struct nvme_ctrl *ctrl);\nint nvme_get_log(struct nvme_ctrl *ctrl, u32 nsid, u8 log_page, u8 lsp, u8 csi,\n\t\tvoid *log, size_t size, u64 offset);\nbool nvme_tryget_ns_head(struct nvme_ns_head *head);\nvoid nvme_put_ns_head(struct nvme_ns_head *head);\nint nvme_cdev_add(struct cdev *cdev, struct device *cdev_device,\n\t\tconst struct file_operations *fops, struct module *owner);\nvoid nvme_cdev_del(struct cdev *cdev, struct device *cdev_device);\nint nvme_ioctl(struct block_device *bdev, blk_mode_t mode,\n\t\tunsigned int cmd, unsigned long arg);\nlong nvme_ns_chr_ioctl(struct file *file, unsigned int cmd, unsigned long arg);\nint nvme_ns_head_ioctl(struct block_device *bdev, blk_mode_t mode,\n\t\tunsigned int cmd, unsigned long arg);\nlong nvme_ns_head_chr_ioctl(struct file *file, unsigned int cmd,\n\t\tunsigned long arg);\nlong nvme_dev_ioctl(struct file *file, unsigned int cmd,\n\t\tunsigned long arg);\nint nvme_ns_chr_uring_cmd_iopoll(struct io_uring_cmd *ioucmd,\n\t\tstruct io_comp_batch *iob, unsigned int poll_flags);\nint nvme_ns_chr_uring_cmd(struct io_uring_cmd *ioucmd,\n\t\tunsigned int issue_flags);\nint nvme_ns_head_chr_uring_cmd(struct io_uring_cmd *ioucmd,\n\t\tunsigned int issue_flags);\nint nvme_getgeo(struct block_device *bdev, struct hd_geometry *geo);\nint nvme_dev_uring_cmd(struct io_uring_cmd *ioucmd, unsigned int issue_flags);\n\nextern const struct attribute_group *nvme_ns_id_attr_groups[];\nextern const struct pr_ops nvme_pr_ops;\nextern const struct block_device_operations nvme_ns_head_ops;\nextern const struct attribute_group nvme_dev_attrs_group;\nextern const struct attribute_group *nvme_subsys_attrs_groups[];\nextern const struct attribute_group *nvme_dev_attr_groups[];\nextern const struct block_device_operations nvme_bdev_ops;\n\nvoid nvme_delete_ctrl_sync(struct nvme_ctrl *ctrl);\nstruct nvme_ns *nvme_find_path(struct nvme_ns_head *head);\n#ifdef CONFIG_NVME_MULTIPATH\nstatic inline bool nvme_ctrl_use_ana(struct nvme_ctrl *ctrl)\n{\n\treturn ctrl->ana_log_buf != NULL;\n}\n\nvoid nvme_mpath_unfreeze(struct nvme_subsystem *subsys);\nvoid nvme_mpath_wait_freeze(struct nvme_subsystem *subsys);\nvoid nvme_mpath_start_freeze(struct nvme_subsystem *subsys);\nvoid nvme_mpath_default_iopolicy(struct nvme_subsystem *subsys);\nvoid nvme_failover_req(struct request *req);\nvoid nvme_kick_requeue_lists(struct nvme_ctrl *ctrl);\nint nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl,struct nvme_ns_head *head);\nvoid nvme_mpath_add_disk(struct nvme_ns *ns, __le32 anagrpid);\nvoid nvme_mpath_remove_disk(struct nvme_ns_head *head);\nint nvme_mpath_init_identify(struct nvme_ctrl *ctrl, struct nvme_id_ctrl *id);\nvoid nvme_mpath_init_ctrl(struct nvme_ctrl *ctrl);\nvoid nvme_mpath_update(struct nvme_ctrl *ctrl);\nvoid nvme_mpath_uninit(struct nvme_ctrl *ctrl);\nvoid nvme_mpath_stop(struct nvme_ctrl *ctrl);\nbool nvme_mpath_clear_current_path(struct nvme_ns *ns);\nvoid nvme_mpath_revalidate_paths(struct nvme_ns *ns);\nvoid nvme_mpath_clear_ctrl_paths(struct nvme_ctrl *ctrl);\nvoid nvme_mpath_shutdown_disk(struct nvme_ns_head *head);\nvoid nvme_mpath_start_request(struct request *rq);\nvoid nvme_mpath_end_request(struct request *rq);\n\nstatic inline void nvme_trace_bio_complete(struct request *req)\n{\n\tstruct nvme_ns *ns = req->q->queuedata;\n\n\tif ((req->cmd_flags & REQ_NVME_MPATH) && req->bio)\n\t\ttrace_block_bio_complete(ns->head->disk->queue, req->bio);\n}\n\nextern bool multipath;\nextern struct device_attribute dev_attr_ana_grpid;\nextern struct device_attribute dev_attr_ana_state;\nextern struct device_attribute subsys_attr_iopolicy;\n\n#else\n#define multipath false\nstatic inline bool nvme_ctrl_use_ana(struct nvme_ctrl *ctrl)\n{\n\treturn false;\n}\nstatic inline void nvme_failover_req(struct request *req)\n{\n}\nstatic inline void nvme_kick_requeue_lists(struct nvme_ctrl *ctrl)\n{\n}\nstatic inline int nvme_mpath_alloc_disk(struct nvme_ctrl *ctrl,\n\t\tstruct nvme_ns_head *head)\n{\n\treturn 0;\n}\nstatic inline void nvme_mpath_add_disk(struct nvme_ns *ns, __le32 anagrpid)\n{\n}\nstatic inline void nvme_mpath_remove_disk(struct nvme_ns_head *head)\n{\n}\nstatic inline bool nvme_mpath_clear_current_path(struct nvme_ns *ns)\n{\n\treturn false;\n}\nstatic inline void nvme_mpath_revalidate_paths(struct nvme_ns *ns)\n{\n}\nstatic inline void nvme_mpath_clear_ctrl_paths(struct nvme_ctrl *ctrl)\n{\n}\nstatic inline void nvme_mpath_shutdown_disk(struct nvme_ns_head *head)\n{\n}\nstatic inline void nvme_trace_bio_complete(struct request *req)\n{\n}\nstatic inline void nvme_mpath_init_ctrl(struct nvme_ctrl *ctrl)\n{\n}\nstatic inline int nvme_mpath_init_identify(struct nvme_ctrl *ctrl,\n\t\tstruct nvme_id_ctrl *id)\n{\n\tif (ctrl->subsys->cmic & NVME_CTRL_CMIC_ANA)\n\t\tdev_warn(ctrl->device,\n\"Please enable CONFIG_NVME_MULTIPATH for full support of multi-port devices.\\n\");\n\treturn 0;\n}\nstatic inline void nvme_mpath_update(struct nvme_ctrl *ctrl)\n{\n}\nstatic inline void nvme_mpath_uninit(struct nvme_ctrl *ctrl)\n{\n}\nstatic inline void nvme_mpath_stop(struct nvme_ctrl *ctrl)\n{\n}\nstatic inline void nvme_mpath_unfreeze(struct nvme_subsystem *subsys)\n{\n}\nstatic inline void nvme_mpath_wait_freeze(struct nvme_subsystem *subsys)\n{\n}\nstatic inline void nvme_mpath_start_freeze(struct nvme_subsystem *subsys)\n{\n}\nstatic inline void nvme_mpath_default_iopolicy(struct nvme_subsystem *subsys)\n{\n}\nstatic inline void nvme_mpath_start_request(struct request *rq)\n{\n}\nstatic inline void nvme_mpath_end_request(struct request *rq)\n{\n}\n#endif  \n\nint nvme_revalidate_zones(struct nvme_ns *ns);\nint nvme_ns_report_zones(struct nvme_ns *ns, sector_t sector,\n\t\tunsigned int nr_zones, report_zones_cb cb, void *data);\n#ifdef CONFIG_BLK_DEV_ZONED\nint nvme_update_zone_info(struct nvme_ns *ns, unsigned lbaf);\nblk_status_t nvme_setup_zone_mgmt_send(struct nvme_ns *ns, struct request *req,\n\t\t\t\t       struct nvme_command *cmnd,\n\t\t\t\t       enum nvme_zone_mgmt_action action);\n#else\nstatic inline blk_status_t nvme_setup_zone_mgmt_send(struct nvme_ns *ns,\n\t\tstruct request *req, struct nvme_command *cmnd,\n\t\tenum nvme_zone_mgmt_action action)\n{\n\treturn BLK_STS_NOTSUPP;\n}\n\nstatic inline int nvme_update_zone_info(struct nvme_ns *ns, unsigned lbaf)\n{\n\tdev_warn(ns->ctrl->device,\n\t\t \"Please enable CONFIG_BLK_DEV_ZONED to support ZNS devices\\n\");\n\treturn -EPROTONOSUPPORT;\n}\n#endif\n\nstatic inline struct nvme_ns *nvme_get_ns_from_dev(struct device *dev)\n{\n\treturn dev_to_disk(dev)->private_data;\n}\n\n#ifdef CONFIG_NVME_HWMON\nint nvme_hwmon_init(struct nvme_ctrl *ctrl);\nvoid nvme_hwmon_exit(struct nvme_ctrl *ctrl);\n#else\nstatic inline int nvme_hwmon_init(struct nvme_ctrl *ctrl)\n{\n\treturn 0;\n}\n\nstatic inline void nvme_hwmon_exit(struct nvme_ctrl *ctrl)\n{\n}\n#endif\n\nstatic inline void nvme_start_request(struct request *rq)\n{\n\tif (rq->cmd_flags & REQ_NVME_MPATH)\n\t\tnvme_mpath_start_request(rq);\n\tblk_mq_start_request(rq);\n}\n\nstatic inline bool nvme_ctrl_sgl_supported(struct nvme_ctrl *ctrl)\n{\n\treturn ctrl->sgls & ((1 << 0) | (1 << 1));\n}\n\n#ifdef CONFIG_NVME_AUTH\nint __init nvme_init_auth(void);\nvoid __exit nvme_exit_auth(void);\nint nvme_auth_init_ctrl(struct nvme_ctrl *ctrl);\nvoid nvme_auth_stop(struct nvme_ctrl *ctrl);\nint nvme_auth_negotiate(struct nvme_ctrl *ctrl, int qid);\nint nvme_auth_wait(struct nvme_ctrl *ctrl, int qid);\nvoid nvme_auth_free(struct nvme_ctrl *ctrl);\n#else\nstatic inline int nvme_auth_init_ctrl(struct nvme_ctrl *ctrl)\n{\n\treturn 0;\n}\nstatic inline int __init nvme_init_auth(void)\n{\n\treturn 0;\n}\nstatic inline void __exit nvme_exit_auth(void)\n{\n}\nstatic inline void nvme_auth_stop(struct nvme_ctrl *ctrl) {};\nstatic inline int nvme_auth_negotiate(struct nvme_ctrl *ctrl, int qid)\n{\n\treturn -EPROTONOSUPPORT;\n}\nstatic inline int nvme_auth_wait(struct nvme_ctrl *ctrl, int qid)\n{\n\treturn NVME_SC_AUTH_REQUIRED;\n}\nstatic inline void nvme_auth_free(struct nvme_ctrl *ctrl) {};\n#endif\n\nu32 nvme_command_effects(struct nvme_ctrl *ctrl, struct nvme_ns *ns,\n\t\t\t u8 opcode);\nu32 nvme_passthru_start(struct nvme_ctrl *ctrl, struct nvme_ns *ns, u8 opcode);\nint nvme_execute_rq(struct request *rq, bool at_head);\nvoid nvme_passthru_end(struct nvme_ctrl *ctrl, struct nvme_ns *ns, u32 effects,\n\t\t       struct nvme_command *cmd, int status);\nstruct nvme_ctrl *nvme_ctrl_from_file(struct file *file);\nstruct nvme_ns *nvme_find_get_ns(struct nvme_ctrl *ctrl, unsigned nsid);\nvoid nvme_put_ns(struct nvme_ns *ns);\n\nstatic inline bool nvme_multi_css(struct nvme_ctrl *ctrl)\n{\n\treturn (ctrl->ctrl_config & NVME_CC_CSS_MASK) == NVME_CC_CSS_CSI;\n}\n\n#ifdef CONFIG_NVME_VERBOSE_ERRORS\nconst unsigned char *nvme_get_error_status_str(u16 status);\nconst unsigned char *nvme_get_opcode_str(u8 opcode);\nconst unsigned char *nvme_get_admin_opcode_str(u8 opcode);\nconst unsigned char *nvme_get_fabrics_opcode_str(u8 opcode);\n#else  \nstatic inline const unsigned char *nvme_get_error_status_str(u16 status)\n{\n\treturn \"I/O Error\";\n}\nstatic inline const unsigned char *nvme_get_opcode_str(u8 opcode)\n{\n\treturn \"I/O Cmd\";\n}\nstatic inline const unsigned char *nvme_get_admin_opcode_str(u8 opcode)\n{\n\treturn \"Admin Cmd\";\n}\n\nstatic inline const unsigned char *nvme_get_fabrics_opcode_str(u8 opcode)\n{\n\treturn \"Fabrics Cmd\";\n}\n#endif  \n\nstatic inline const unsigned char *nvme_opcode_str(int qid, u8 opcode, u8 fctype)\n{\n\tif (opcode == nvme_fabrics_command)\n\t\treturn nvme_get_fabrics_opcode_str(fctype);\n\treturn qid ? nvme_get_opcode_str(opcode) :\n\t\tnvme_get_admin_opcode_str(opcode);\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}