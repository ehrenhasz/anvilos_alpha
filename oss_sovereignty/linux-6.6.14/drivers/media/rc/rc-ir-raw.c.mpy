{
  "module_name": "rc-ir-raw.c",
  "hash_id": "6af775ae8f6afb350081d255cbb0aced8d7bb6034080e57d2d59f1302341635d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/media/rc/rc-ir-raw.c",
  "human_readable_source": "\n\n\n\n\n#include <linux/export.h>\n#include <linux/kthread.h>\n#include <linux/mutex.h>\n#include <linux/kmod.h>\n#include <linux/sched.h>\n#include \"rc-core-priv.h\"\n\n \nstatic LIST_HEAD(ir_raw_client_list);\n\n \nDEFINE_MUTEX(ir_raw_handler_lock);\nstatic LIST_HEAD(ir_raw_handler_list);\nstatic atomic64_t available_protocols = ATOMIC64_INIT(0);\n\nstatic int ir_raw_event_thread(void *data)\n{\n\tstruct ir_raw_event ev;\n\tstruct ir_raw_handler *handler;\n\tstruct ir_raw_event_ctrl *raw = data;\n\tstruct rc_dev *dev = raw->dev;\n\n\twhile (1) {\n\t\tmutex_lock(&ir_raw_handler_lock);\n\t\twhile (kfifo_out(&raw->kfifo, &ev, 1)) {\n\t\t\tif (is_timing_event(ev)) {\n\t\t\t\tif (ev.duration == 0)\n\t\t\t\t\tdev_warn_once(&dev->dev, \"nonsensical timing event of duration 0\");\n\t\t\t\tif (is_timing_event(raw->prev_ev) &&\n\t\t\t\t    !is_transition(&ev, &raw->prev_ev))\n\t\t\t\t\tdev_warn_once(&dev->dev, \"two consecutive events of type %s\",\n\t\t\t\t\t\t      TO_STR(ev.pulse));\n\t\t\t}\n\t\t\tlist_for_each_entry(handler, &ir_raw_handler_list, list)\n\t\t\t\tif (dev->enabled_protocols &\n\t\t\t\t    handler->protocols || !handler->protocols)\n\t\t\t\t\thandler->decode(dev, ev);\n\t\t\tlirc_raw_event(dev, ev);\n\t\t\traw->prev_ev = ev;\n\t\t}\n\t\tmutex_unlock(&ir_raw_handler_lock);\n\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\n\t\tif (kthread_should_stop()) {\n\t\t\t__set_current_state(TASK_RUNNING);\n\t\t\tbreak;\n\t\t} else if (!kfifo_is_empty(&raw->kfifo))\n\t\t\tset_current_state(TASK_RUNNING);\n\n\t\tschedule();\n\t}\n\n\treturn 0;\n}\n\n \nint ir_raw_event_store(struct rc_dev *dev, struct ir_raw_event *ev)\n{\n\tif (!dev->raw)\n\t\treturn -EINVAL;\n\n\tdev_dbg(&dev->dev, \"sample: (%05dus %s)\\n\",\n\t\tev->duration, TO_STR(ev->pulse));\n\n\tif (!kfifo_put(&dev->raw->kfifo, *ev)) {\n\t\tdev_err(&dev->dev, \"IR event FIFO is full!\\n\");\n\t\treturn -ENOSPC;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(ir_raw_event_store);\n\n \nint ir_raw_event_store_edge(struct rc_dev *dev, bool pulse)\n{\n\tktime_t\t\t\tnow;\n\tstruct ir_raw_event\tev = {};\n\n\tif (!dev->raw)\n\t\treturn -EINVAL;\n\n\tnow = ktime_get();\n\tev.duration = ktime_to_us(ktime_sub(now, dev->raw->last_event));\n\tev.pulse = !pulse;\n\n\treturn ir_raw_event_store_with_timeout(dev, &ev);\n}\nEXPORT_SYMBOL_GPL(ir_raw_event_store_edge);\n\n \nint ir_raw_event_store_with_timeout(struct rc_dev *dev, struct ir_raw_event *ev)\n{\n\tktime_t\t\tnow;\n\tint\t\trc = 0;\n\n\tif (!dev->raw)\n\t\treturn -EINVAL;\n\n\tnow = ktime_get();\n\n\tspin_lock(&dev->raw->edge_spinlock);\n\trc = ir_raw_event_store(dev, ev);\n\n\tdev->raw->last_event = now;\n\n\t \n\tif (!timer_pending(&dev->raw->edge_handle) ||\n\t    time_after(dev->raw->edge_handle.expires,\n\t\t       jiffies + msecs_to_jiffies(15))) {\n\t\tmod_timer(&dev->raw->edge_handle,\n\t\t\t  jiffies + msecs_to_jiffies(15));\n\t}\n\tspin_unlock(&dev->raw->edge_spinlock);\n\n\treturn rc;\n}\nEXPORT_SYMBOL_GPL(ir_raw_event_store_with_timeout);\n\n \nint ir_raw_event_store_with_filter(struct rc_dev *dev, struct ir_raw_event *ev)\n{\n\tif (!dev->raw)\n\t\treturn -EINVAL;\n\n\t \n\tif (dev->idle && !ev->pulse)\n\t\treturn 0;\n\telse if (dev->idle)\n\t\tir_raw_event_set_idle(dev, false);\n\n\tif (!dev->raw->this_ev.duration)\n\t\tdev->raw->this_ev = *ev;\n\telse if (ev->pulse == dev->raw->this_ev.pulse)\n\t\tdev->raw->this_ev.duration += ev->duration;\n\telse {\n\t\tir_raw_event_store(dev, &dev->raw->this_ev);\n\t\tdev->raw->this_ev = *ev;\n\t}\n\n\t \n\tif (!ev->pulse && dev->timeout &&\n\t    dev->raw->this_ev.duration >= dev->timeout)\n\t\tir_raw_event_set_idle(dev, true);\n\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(ir_raw_event_store_with_filter);\n\n \nvoid ir_raw_event_set_idle(struct rc_dev *dev, bool idle)\n{\n\tif (!dev->raw)\n\t\treturn;\n\n\tdev_dbg(&dev->dev, \"%s idle mode\\n\", idle ? \"enter\" : \"leave\");\n\n\tif (idle) {\n\t\tdev->raw->this_ev.timeout = true;\n\t\tir_raw_event_store(dev, &dev->raw->this_ev);\n\t\tdev->raw->this_ev = (struct ir_raw_event) {};\n\t}\n\n\tif (dev->s_idle)\n\t\tdev->s_idle(dev, idle);\n\n\tdev->idle = idle;\n}\nEXPORT_SYMBOL_GPL(ir_raw_event_set_idle);\n\n \nvoid ir_raw_event_handle(struct rc_dev *dev)\n{\n\tif (!dev->raw || !dev->raw->thread)\n\t\treturn;\n\n\twake_up_process(dev->raw->thread);\n}\nEXPORT_SYMBOL_GPL(ir_raw_event_handle);\n\n \nu64\nir_raw_get_allowed_protocols(void)\n{\n\treturn atomic64_read(&available_protocols);\n}\n\nstatic int change_protocol(struct rc_dev *dev, u64 *rc_proto)\n{\n\tstruct ir_raw_handler *handler;\n\tu32 timeout = 0;\n\n\tmutex_lock(&ir_raw_handler_lock);\n\tlist_for_each_entry(handler, &ir_raw_handler_list, list) {\n\t\tif (!(dev->enabled_protocols & handler->protocols) &&\n\t\t    (*rc_proto & handler->protocols) && handler->raw_register)\n\t\t\thandler->raw_register(dev);\n\n\t\tif ((dev->enabled_protocols & handler->protocols) &&\n\t\t    !(*rc_proto & handler->protocols) &&\n\t\t    handler->raw_unregister)\n\t\t\thandler->raw_unregister(dev);\n\t}\n\tmutex_unlock(&ir_raw_handler_lock);\n\n\tif (!dev->max_timeout)\n\t\treturn 0;\n\n\tmutex_lock(&ir_raw_handler_lock);\n\tlist_for_each_entry(handler, &ir_raw_handler_list, list) {\n\t\tif (handler->protocols & *rc_proto) {\n\t\t\tif (timeout < handler->min_timeout)\n\t\t\t\ttimeout = handler->min_timeout;\n\t\t}\n\t}\n\tmutex_unlock(&ir_raw_handler_lock);\n\n\tif (timeout == 0)\n\t\ttimeout = IR_DEFAULT_TIMEOUT;\n\telse\n\t\ttimeout += MS_TO_US(10);\n\n\tif (timeout < dev->min_timeout)\n\t\ttimeout = dev->min_timeout;\n\telse if (timeout > dev->max_timeout)\n\t\ttimeout = dev->max_timeout;\n\n\tif (dev->s_timeout)\n\t\tdev->s_timeout(dev, timeout);\n\telse\n\t\tdev->timeout = timeout;\n\n\treturn 0;\n}\n\nstatic void ir_raw_disable_protocols(struct rc_dev *dev, u64 protocols)\n{\n\tmutex_lock(&dev->lock);\n\tdev->enabled_protocols &= ~protocols;\n\tmutex_unlock(&dev->lock);\n}\n\n \nint ir_raw_gen_manchester(struct ir_raw_event **ev, unsigned int max,\n\t\t\t  const struct ir_raw_timings_manchester *timings,\n\t\t\t  unsigned int n, u64 data)\n{\n\tbool need_pulse;\n\tu64 i;\n\tint ret = -ENOBUFS;\n\n\ti = BIT_ULL(n - 1);\n\n\tif (timings->leader_pulse) {\n\t\tif (!max--)\n\t\t\treturn ret;\n\t\tinit_ir_raw_event_duration((*ev), 1, timings->leader_pulse);\n\t\tif (timings->leader_space) {\n\t\t\tif (!max--)\n\t\t\t\treturn ret;\n\t\t\tinit_ir_raw_event_duration(++(*ev), 0,\n\t\t\t\t\t\t   timings->leader_space);\n\t\t}\n\t} else {\n\t\t \n\t\t--(*ev);\n\t}\n\t \n\n\twhile (n && i > 0) {\n\t\tneed_pulse = !(data & i);\n\t\tif (timings->invert)\n\t\t\tneed_pulse = !need_pulse;\n\t\tif (need_pulse == !!(*ev)->pulse) {\n\t\t\t(*ev)->duration += timings->clock;\n\t\t} else {\n\t\t\tif (!max--)\n\t\t\t\tgoto nobufs;\n\t\t\tinit_ir_raw_event_duration(++(*ev), need_pulse,\n\t\t\t\t\t\t   timings->clock);\n\t\t}\n\n\t\tif (!max--)\n\t\t\tgoto nobufs;\n\t\tinit_ir_raw_event_duration(++(*ev), !need_pulse,\n\t\t\t\t\t   timings->clock);\n\t\ti >>= 1;\n\t}\n\n\tif (timings->trailer_space) {\n\t\tif (!(*ev)->pulse)\n\t\t\t(*ev)->duration += timings->trailer_space;\n\t\telse if (!max--)\n\t\t\tgoto nobufs;\n\t\telse\n\t\t\tinit_ir_raw_event_duration(++(*ev), 0,\n\t\t\t\t\t\t   timings->trailer_space);\n\t}\n\n\tret = 0;\nnobufs:\n\t \n\t++(*ev);\n\treturn ret;\n}\nEXPORT_SYMBOL(ir_raw_gen_manchester);\n\n \nint ir_raw_gen_pd(struct ir_raw_event **ev, unsigned int max,\n\t\t  const struct ir_raw_timings_pd *timings,\n\t\t  unsigned int n, u64 data)\n{\n\tint i;\n\tint ret;\n\tunsigned int space;\n\n\tif (timings->header_pulse) {\n\t\tret = ir_raw_gen_pulse_space(ev, &max, timings->header_pulse,\n\t\t\t\t\t     timings->header_space);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (timings->msb_first) {\n\t\tfor (i = n - 1; i >= 0; --i) {\n\t\t\tspace = timings->bit_space[(data >> i) & 1];\n\t\t\tret = ir_raw_gen_pulse_space(ev, &max,\n\t\t\t\t\t\t     timings->bit_pulse,\n\t\t\t\t\t\t     space);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < n; ++i, data >>= 1) {\n\t\t\tspace = timings->bit_space[data & 1];\n\t\t\tret = ir_raw_gen_pulse_space(ev, &max,\n\t\t\t\t\t\t     timings->bit_pulse,\n\t\t\t\t\t\t     space);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\tret = ir_raw_gen_pulse_space(ev, &max, timings->trailer_pulse,\n\t\t\t\t     timings->trailer_space);\n\treturn ret;\n}\nEXPORT_SYMBOL(ir_raw_gen_pd);\n\n \nint ir_raw_gen_pl(struct ir_raw_event **ev, unsigned int max,\n\t\t  const struct ir_raw_timings_pl *timings,\n\t\t  unsigned int n, u64 data)\n{\n\tint i;\n\tint ret = -ENOBUFS;\n\tunsigned int pulse;\n\n\tif (!max--)\n\t\treturn ret;\n\n\tinit_ir_raw_event_duration((*ev)++, 1, timings->header_pulse);\n\n\tif (timings->msb_first) {\n\t\tfor (i = n - 1; i >= 0; --i) {\n\t\t\tif (!max--)\n\t\t\t\treturn ret;\n\t\t\tinit_ir_raw_event_duration((*ev)++, 0,\n\t\t\t\t\t\t   timings->bit_space);\n\t\t\tif (!max--)\n\t\t\t\treturn ret;\n\t\t\tpulse = timings->bit_pulse[(data >> i) & 1];\n\t\t\tinit_ir_raw_event_duration((*ev)++, 1, pulse);\n\t\t}\n\t} else {\n\t\tfor (i = 0; i < n; ++i, data >>= 1) {\n\t\t\tif (!max--)\n\t\t\t\treturn ret;\n\t\t\tinit_ir_raw_event_duration((*ev)++, 0,\n\t\t\t\t\t\t   timings->bit_space);\n\t\t\tif (!max--)\n\t\t\t\treturn ret;\n\t\t\tpulse = timings->bit_pulse[data & 1];\n\t\t\tinit_ir_raw_event_duration((*ev)++, 1, pulse);\n\t\t}\n\t}\n\n\tif (!max--)\n\t\treturn ret;\n\n\tinit_ir_raw_event_duration((*ev)++, 0, timings->trailer_space);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(ir_raw_gen_pl);\n\n \nint ir_raw_encode_scancode(enum rc_proto protocol, u32 scancode,\n\t\t\t   struct ir_raw_event *events, unsigned int max)\n{\n\tstruct ir_raw_handler *handler;\n\tint ret = -EINVAL;\n\tu64 mask = 1ULL << protocol;\n\n\tir_raw_load_modules(&mask);\n\n\tmutex_lock(&ir_raw_handler_lock);\n\tlist_for_each_entry(handler, &ir_raw_handler_list, list) {\n\t\tif (handler->protocols & mask && handler->encode) {\n\t\t\tret = handler->encode(protocol, scancode, events, max);\n\t\t\tif (ret >= 0 || ret == -ENOBUFS)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&ir_raw_handler_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(ir_raw_encode_scancode);\n\n \nstatic void ir_raw_edge_handle(struct timer_list *t)\n{\n\tstruct ir_raw_event_ctrl *raw = from_timer(raw, t, edge_handle);\n\tstruct rc_dev *dev = raw->dev;\n\tunsigned long flags;\n\tktime_t interval;\n\n\tspin_lock_irqsave(&dev->raw->edge_spinlock, flags);\n\tinterval = ktime_sub(ktime_get(), dev->raw->last_event);\n\tif (ktime_to_us(interval) >= dev->timeout) {\n\t\tstruct ir_raw_event ev = {\n\t\t\t.timeout = true,\n\t\t\t.duration = ktime_to_us(interval)\n\t\t};\n\n\t\tir_raw_event_store(dev, &ev);\n\t} else {\n\t\tmod_timer(&dev->raw->edge_handle,\n\t\t\t  jiffies + usecs_to_jiffies(dev->timeout -\n\t\t\t\t\t\t     ktime_to_us(interval)));\n\t}\n\tspin_unlock_irqrestore(&dev->raw->edge_spinlock, flags);\n\n\tir_raw_event_handle(dev);\n}\n\n \nint ir_raw_encode_carrier(enum rc_proto protocol)\n{\n\tstruct ir_raw_handler *handler;\n\tint ret = -EINVAL;\n\tu64 mask = BIT_ULL(protocol);\n\n\tmutex_lock(&ir_raw_handler_lock);\n\tlist_for_each_entry(handler, &ir_raw_handler_list, list) {\n\t\tif (handler->protocols & mask && handler->encode) {\n\t\t\tret = handler->carrier;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&ir_raw_handler_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(ir_raw_encode_carrier);\n\n \nint ir_raw_event_prepare(struct rc_dev *dev)\n{\n\tif (!dev)\n\t\treturn -EINVAL;\n\n\tdev->raw = kzalloc(sizeof(*dev->raw), GFP_KERNEL);\n\tif (!dev->raw)\n\t\treturn -ENOMEM;\n\n\tdev->raw->dev = dev;\n\tdev->change_protocol = change_protocol;\n\tdev->idle = true;\n\tspin_lock_init(&dev->raw->edge_spinlock);\n\ttimer_setup(&dev->raw->edge_handle, ir_raw_edge_handle, 0);\n\tINIT_KFIFO(dev->raw->kfifo);\n\n\treturn 0;\n}\n\nint ir_raw_event_register(struct rc_dev *dev)\n{\n\tstruct task_struct *thread;\n\n\tthread = kthread_run(ir_raw_event_thread, dev->raw, \"rc%u\", dev->minor);\n\tif (IS_ERR(thread))\n\t\treturn PTR_ERR(thread);\n\n\tdev->raw->thread = thread;\n\n\tmutex_lock(&ir_raw_handler_lock);\n\tlist_add_tail(&dev->raw->list, &ir_raw_client_list);\n\tmutex_unlock(&ir_raw_handler_lock);\n\n\treturn 0;\n}\n\nvoid ir_raw_event_free(struct rc_dev *dev)\n{\n\tif (!dev)\n\t\treturn;\n\n\tkfree(dev->raw);\n\tdev->raw = NULL;\n}\n\nvoid ir_raw_event_unregister(struct rc_dev *dev)\n{\n\tstruct ir_raw_handler *handler;\n\n\tif (!dev || !dev->raw)\n\t\treturn;\n\n\tkthread_stop(dev->raw->thread);\n\tdel_timer_sync(&dev->raw->edge_handle);\n\n\tmutex_lock(&ir_raw_handler_lock);\n\tlist_del(&dev->raw->list);\n\tlist_for_each_entry(handler, &ir_raw_handler_list, list)\n\t\tif (handler->raw_unregister &&\n\t\t    (handler->protocols & dev->enabled_protocols))\n\t\t\thandler->raw_unregister(dev);\n\n\tlirc_bpf_free(dev);\n\n\tir_raw_event_free(dev);\n\n\t \n\tmutex_unlock(&ir_raw_handler_lock);\n}\n\n \n\nint ir_raw_handler_register(struct ir_raw_handler *ir_raw_handler)\n{\n\tmutex_lock(&ir_raw_handler_lock);\n\tlist_add_tail(&ir_raw_handler->list, &ir_raw_handler_list);\n\tatomic64_or(ir_raw_handler->protocols, &available_protocols);\n\tmutex_unlock(&ir_raw_handler_lock);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(ir_raw_handler_register);\n\nvoid ir_raw_handler_unregister(struct ir_raw_handler *ir_raw_handler)\n{\n\tstruct ir_raw_event_ctrl *raw;\n\tu64 protocols = ir_raw_handler->protocols;\n\n\tmutex_lock(&ir_raw_handler_lock);\n\tlist_del(&ir_raw_handler->list);\n\tlist_for_each_entry(raw, &ir_raw_client_list, list) {\n\t\tif (ir_raw_handler->raw_unregister &&\n\t\t    (raw->dev->enabled_protocols & protocols))\n\t\t\tir_raw_handler->raw_unregister(raw->dev);\n\t\tir_raw_disable_protocols(raw->dev, protocols);\n\t}\n\tatomic64_andnot(protocols, &available_protocols);\n\tmutex_unlock(&ir_raw_handler_lock);\n}\nEXPORT_SYMBOL(ir_raw_handler_unregister);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}