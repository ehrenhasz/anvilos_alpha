{
  "module_name": "hantro_g2_vp9_dec.c",
  "hash_id": "b8f19e10e57e31cf9d78a2036196c8e51faf7ef81fcbffc3abdcaea66a4b44eb",
  "original_prompt": "Ingested from linux-6.6.14/drivers/media/platform/verisilicon/hantro_g2_vp9_dec.c",
  "human_readable_source": "\n \n#include \"media/videobuf2-core.h\"\n#include \"media/videobuf2-dma-contig.h\"\n#include \"media/videobuf2-v4l2.h\"\n#include <linux/kernel.h>\n#include <linux/vmalloc.h>\n#include <media/v4l2-mem2mem.h>\n#include <media/v4l2-vp9.h>\n\n#include \"hantro.h\"\n#include \"hantro_vp9.h\"\n#include \"hantro_g2_regs.h\"\n\n#define G2_ALIGN 16\n\nenum hantro_ref_frames {\n\tINTRA_FRAME = 0,\n\tLAST_FRAME = 1,\n\tGOLDEN_FRAME = 2,\n\tALTREF_FRAME = 3,\n\tMAX_REF_FRAMES = 4\n};\n\nstatic int start_prepare_run(struct hantro_ctx *ctx, const struct v4l2_ctrl_vp9_frame **dec_params)\n{\n\tconst struct v4l2_ctrl_vp9_compressed_hdr *prob_updates;\n\tstruct hantro_vp9_dec_hw_ctx *vp9_ctx = &ctx->vp9_dec;\n\tstruct v4l2_ctrl *ctrl;\n\tunsigned int fctx_idx;\n\n\t \n\thantro_start_prepare_run(ctx);\n\n\tctrl = v4l2_ctrl_find(&ctx->ctrl_handler, V4L2_CID_STATELESS_VP9_FRAME);\n\tif (WARN_ON(!ctrl))\n\t\treturn -EINVAL;\n\t*dec_params = ctrl->p_cur.p;\n\n\tctrl = v4l2_ctrl_find(&ctx->ctrl_handler, V4L2_CID_STATELESS_VP9_COMPRESSED_HDR);\n\tif (WARN_ON(!ctrl))\n\t\treturn -EINVAL;\n\tprob_updates = ctrl->p_cur.p;\n\tvp9_ctx->cur.tx_mode = prob_updates->tx_mode;\n\n\t \n\tfctx_idx = v4l2_vp9_reset_frame_ctx(*dec_params, vp9_ctx->frame_context);\n\tvp9_ctx->cur.frame_context_idx = fctx_idx;\n\n\t \n\tvp9_ctx->probability_tables = vp9_ctx->frame_context[fctx_idx];\n\n\t \n\tv4l2_vp9_fw_update_probs(&vp9_ctx->probability_tables, prob_updates, *dec_params);\n\n\treturn 0;\n}\n\nstatic size_t chroma_offset(const struct hantro_ctx *ctx,\n\t\t\t    const struct v4l2_ctrl_vp9_frame *dec_params)\n{\n\tint bytes_per_pixel = dec_params->bit_depth == 8 ? 1 : 2;\n\n\treturn ctx->src_fmt.width * ctx->src_fmt.height * bytes_per_pixel;\n}\n\nstatic size_t mv_offset(const struct hantro_ctx *ctx,\n\t\t\tconst struct v4l2_ctrl_vp9_frame *dec_params)\n{\n\tsize_t cr_offset = chroma_offset(ctx, dec_params);\n\n\treturn ALIGN((cr_offset * 3) / 2, G2_ALIGN);\n}\n\nstatic struct hantro_decoded_buffer *\nget_ref_buf(struct hantro_ctx *ctx, struct vb2_v4l2_buffer *dst, u64 timestamp)\n{\n\tstruct v4l2_m2m_ctx *m2m_ctx = ctx->fh.m2m_ctx;\n\tstruct vb2_queue *cap_q = &m2m_ctx->cap_q_ctx.q;\n\tstruct vb2_buffer *buf;\n\n\t \n\tbuf = vb2_find_buffer(cap_q, timestamp);\n\tif (!buf)\n\t\tbuf = &dst->vb2_buf;\n\n\treturn vb2_to_hantro_decoded_buf(buf);\n}\n\nstatic void update_dec_buf_info(struct hantro_decoded_buffer *buf,\n\t\t\t\tconst struct v4l2_ctrl_vp9_frame *dec_params)\n{\n\tbuf->vp9.width = dec_params->frame_width_minus_1 + 1;\n\tbuf->vp9.height = dec_params->frame_height_minus_1 + 1;\n\tbuf->vp9.bit_depth = dec_params->bit_depth;\n}\n\nstatic void update_ctx_cur_info(struct hantro_vp9_dec_hw_ctx *vp9_ctx,\n\t\t\t\tstruct hantro_decoded_buffer *buf,\n\t\t\t\tconst struct v4l2_ctrl_vp9_frame *dec_params)\n{\n\tvp9_ctx->cur.valid = true;\n\tvp9_ctx->cur.reference_mode = dec_params->reference_mode;\n\tvp9_ctx->cur.interpolation_filter = dec_params->interpolation_filter;\n\tvp9_ctx->cur.flags = dec_params->flags;\n\tvp9_ctx->cur.timestamp = buf->base.vb.vb2_buf.timestamp;\n}\n\nstatic void config_output(struct hantro_ctx *ctx,\n\t\t\t  struct hantro_decoded_buffer *dst,\n\t\t\t  const struct v4l2_ctrl_vp9_frame *dec_params)\n{\n\tdma_addr_t luma_addr, chroma_addr, mv_addr;\n\n\thantro_reg_write(ctx->dev, &g2_out_dis, 0);\n\tif (!ctx->dev->variant->legacy_regs)\n\t\thantro_reg_write(ctx->dev, &g2_output_format, 0);\n\n\tluma_addr = hantro_get_dec_buf_addr(ctx, &dst->base.vb.vb2_buf);\n\thantro_write_addr(ctx->dev, G2_OUT_LUMA_ADDR, luma_addr);\n\n\tchroma_addr = luma_addr + chroma_offset(ctx, dec_params);\n\thantro_write_addr(ctx->dev, G2_OUT_CHROMA_ADDR, chroma_addr);\n\n\tmv_addr = luma_addr + mv_offset(ctx, dec_params);\n\thantro_write_addr(ctx->dev, G2_OUT_MV_ADDR, mv_addr);\n}\n\nstruct hantro_vp9_ref_reg {\n\tconst struct hantro_reg width;\n\tconst struct hantro_reg height;\n\tconst struct hantro_reg hor_scale;\n\tconst struct hantro_reg ver_scale;\n\tu32 y_base;\n\tu32 c_base;\n};\n\nstatic void config_ref(struct hantro_ctx *ctx,\n\t\t       struct hantro_decoded_buffer *dst,\n\t\t       const struct hantro_vp9_ref_reg *ref_reg,\n\t\t       const struct v4l2_ctrl_vp9_frame *dec_params,\n\t\t       u64 ref_ts)\n{\n\tstruct hantro_decoded_buffer *buf;\n\tdma_addr_t luma_addr, chroma_addr;\n\tu32 refw, refh;\n\n\tbuf = get_ref_buf(ctx, &dst->base.vb, ref_ts);\n\trefw = buf->vp9.width;\n\trefh = buf->vp9.height;\n\n\thantro_reg_write(ctx->dev, &ref_reg->width, refw);\n\thantro_reg_write(ctx->dev, &ref_reg->height, refh);\n\n\thantro_reg_write(ctx->dev, &ref_reg->hor_scale, (refw << 14) / dst->vp9.width);\n\thantro_reg_write(ctx->dev, &ref_reg->ver_scale, (refh << 14) / dst->vp9.height);\n\n\tluma_addr = hantro_get_dec_buf_addr(ctx, &buf->base.vb.vb2_buf);\n\thantro_write_addr(ctx->dev, ref_reg->y_base, luma_addr);\n\n\tchroma_addr = luma_addr + chroma_offset(ctx, dec_params);\n\thantro_write_addr(ctx->dev, ref_reg->c_base, chroma_addr);\n}\n\nstatic void config_ref_registers(struct hantro_ctx *ctx,\n\t\t\t\t const struct v4l2_ctrl_vp9_frame *dec_params,\n\t\t\t\t struct hantro_decoded_buffer *dst,\n\t\t\t\t struct hantro_decoded_buffer *mv_ref)\n{\n\tstatic const struct hantro_vp9_ref_reg ref_regs[] = {\n\t\t{\n\t\t\t \n\t\t\t.width = vp9_lref_width,\n\t\t\t.height = vp9_lref_height,\n\t\t\t.hor_scale = vp9_lref_hor_scale,\n\t\t\t.ver_scale = vp9_lref_ver_scale,\n\t\t\t.y_base = G2_REF_LUMA_ADDR(0),\n\t\t\t.c_base = G2_REF_CHROMA_ADDR(0),\n\t\t}, {\n\t\t\t \n\t\t\t.width = vp9_gref_width,\n\t\t\t.height = vp9_gref_height,\n\t\t\t.hor_scale = vp9_gref_hor_scale,\n\t\t\t.ver_scale = vp9_gref_ver_scale,\n\t\t\t.y_base = G2_REF_LUMA_ADDR(4),\n\t\t\t.c_base = G2_REF_CHROMA_ADDR(4),\n\t\t}, {\n\t\t\t \n\t\t\t.width = vp9_aref_width,\n\t\t\t.height = vp9_aref_height,\n\t\t\t.hor_scale = vp9_aref_hor_scale,\n\t\t\t.ver_scale = vp9_aref_ver_scale,\n\t\t\t.y_base = G2_REF_LUMA_ADDR(5),\n\t\t\t.c_base = G2_REF_CHROMA_ADDR(5),\n\t\t},\n\t};\n\tdma_addr_t mv_addr;\n\n\tconfig_ref(ctx, dst, &ref_regs[0], dec_params, dec_params->last_frame_ts);\n\tconfig_ref(ctx, dst, &ref_regs[1], dec_params, dec_params->golden_frame_ts);\n\tconfig_ref(ctx, dst, &ref_regs[2], dec_params, dec_params->alt_frame_ts);\n\n\tmv_addr = hantro_get_dec_buf_addr(ctx, &mv_ref->base.vb.vb2_buf) +\n\t\t  mv_offset(ctx, dec_params);\n\thantro_write_addr(ctx->dev, G2_REF_MV_ADDR(0), mv_addr);\n\n\thantro_reg_write(ctx->dev, &vp9_last_sign_bias,\n\t\t\t dec_params->ref_frame_sign_bias & V4L2_VP9_SIGN_BIAS_LAST ? 1 : 0);\n\n\thantro_reg_write(ctx->dev, &vp9_gref_sign_bias,\n\t\t\t dec_params->ref_frame_sign_bias & V4L2_VP9_SIGN_BIAS_GOLDEN ? 1 : 0);\n\n\thantro_reg_write(ctx->dev, &vp9_aref_sign_bias,\n\t\t\t dec_params->ref_frame_sign_bias & V4L2_VP9_SIGN_BIAS_ALT ? 1 : 0);\n}\n\nstatic void recompute_tile_info(unsigned short *tile_info, unsigned int tiles, unsigned int sbs)\n{\n\tint i;\n\tunsigned int accumulated = 0;\n\tunsigned int next_accumulated;\n\n\tfor (i = 1; i <= tiles; ++i) {\n\t\tnext_accumulated = i * sbs / tiles;\n\t\t*tile_info++ = next_accumulated - accumulated;\n\t\taccumulated = next_accumulated;\n\t}\n}\n\nstatic void\nrecompute_tile_rc_info(struct hantro_ctx *ctx,\n\t\t       unsigned int tile_r, unsigned int tile_c,\n\t\t       unsigned int sbs_r, unsigned int sbs_c)\n{\n\tstruct hantro_vp9_dec_hw_ctx *vp9_ctx = &ctx->vp9_dec;\n\n\trecompute_tile_info(vp9_ctx->tile_r_info, tile_r, sbs_r);\n\trecompute_tile_info(vp9_ctx->tile_c_info, tile_c, sbs_c);\n\n\tvp9_ctx->last_tile_r = tile_r;\n\tvp9_ctx->last_tile_c = tile_c;\n\tvp9_ctx->last_sbs_r = sbs_r;\n\tvp9_ctx->last_sbs_c = sbs_c;\n}\n\nstatic inline unsigned int first_tile_row(unsigned int tile_r, unsigned int sbs_r)\n{\n\tif (tile_r == sbs_r + 1)\n\t\treturn 1;\n\n\tif (tile_r == sbs_r + 2)\n\t\treturn 2;\n\n\treturn 0;\n}\n\nstatic void\nfill_tile_info(struct hantro_ctx *ctx,\n\t       unsigned int tile_r, unsigned int tile_c,\n\t       unsigned int sbs_r, unsigned int sbs_c,\n\t       unsigned short *tile_mem)\n{\n\tstruct hantro_vp9_dec_hw_ctx *vp9_ctx = &ctx->vp9_dec;\n\tunsigned int i, j;\n\tbool first = true;\n\n\tfor (i = first_tile_row(tile_r, sbs_r); i < tile_r; ++i) {\n\t\tunsigned short r_info = vp9_ctx->tile_r_info[i];\n\n\t\tif (first) {\n\t\t\tif (i > 0)\n\t\t\t\tr_info += vp9_ctx->tile_r_info[0];\n\t\t\tif (i == 2)\n\t\t\t\tr_info += vp9_ctx->tile_r_info[1];\n\t\t\tfirst = false;\n\t\t}\n\t\tfor (j = 0; j < tile_c; ++j) {\n\t\t\t*tile_mem++ = vp9_ctx->tile_c_info[j];\n\t\t\t*tile_mem++ = r_info;\n\t\t}\n\t}\n}\n\nstatic void\nconfig_tiles(struct hantro_ctx *ctx,\n\t     const struct v4l2_ctrl_vp9_frame *dec_params,\n\t     struct hantro_decoded_buffer *dst)\n{\n\tstruct hantro_vp9_dec_hw_ctx *vp9_ctx = &ctx->vp9_dec;\n\tstruct hantro_aux_buf *misc = &vp9_ctx->misc;\n\tstruct hantro_aux_buf *tile_edge = &vp9_ctx->tile_edge;\n\tdma_addr_t addr;\n\tunsigned short *tile_mem;\n\tunsigned int rows, cols;\n\n\taddr = misc->dma + vp9_ctx->tile_info_offset;\n\thantro_write_addr(ctx->dev, G2_TILE_SIZES_ADDR, addr);\n\n\ttile_mem = misc->cpu + vp9_ctx->tile_info_offset;\n\tif (dec_params->tile_cols_log2 || dec_params->tile_rows_log2) {\n\t\tunsigned int tile_r = (1 << dec_params->tile_rows_log2);\n\t\tunsigned int tile_c = (1 << dec_params->tile_cols_log2);\n\t\tunsigned int sbs_r = hantro_vp9_num_sbs(dst->vp9.height);\n\t\tunsigned int sbs_c = hantro_vp9_num_sbs(dst->vp9.width);\n\n\t\tif (tile_r != vp9_ctx->last_tile_r || tile_c != vp9_ctx->last_tile_c ||\n\t\t    sbs_r != vp9_ctx->last_sbs_r || sbs_c != vp9_ctx->last_sbs_c)\n\t\t\trecompute_tile_rc_info(ctx, tile_r, tile_c, sbs_r, sbs_c);\n\n\t\tfill_tile_info(ctx, tile_r, tile_c, sbs_r, sbs_c, tile_mem);\n\n\t\tcols = tile_c;\n\t\trows = tile_r;\n\t\thantro_reg_write(ctx->dev, &g2_tile_e, 1);\n\t} else {\n\t\ttile_mem[0] = hantro_vp9_num_sbs(dst->vp9.width);\n\t\ttile_mem[1] = hantro_vp9_num_sbs(dst->vp9.height);\n\n\t\tcols = 1;\n\t\trows = 1;\n\t\thantro_reg_write(ctx->dev, &g2_tile_e, 0);\n\t}\n\n\tif (ctx->dev->variant->legacy_regs) {\n\t\thantro_reg_write(ctx->dev, &g2_num_tile_cols_old, cols);\n\t\thantro_reg_write(ctx->dev, &g2_num_tile_rows_old, rows);\n\t} else {\n\t\thantro_reg_write(ctx->dev, &g2_num_tile_cols, cols);\n\t\thantro_reg_write(ctx->dev, &g2_num_tile_rows, rows);\n\t}\n\n\t \n\taddr = tile_edge->dma;\n\thantro_write_addr(ctx->dev, G2_TILE_FILTER_ADDR, addr);\n\n\taddr = tile_edge->dma + vp9_ctx->bsd_ctrl_offset;\n\thantro_write_addr(ctx->dev, G2_TILE_BSD_ADDR, addr);\n}\n\nstatic void\nupdate_feat_and_flag(struct hantro_vp9_dec_hw_ctx *vp9_ctx,\n\t\t     const struct v4l2_vp9_segmentation *seg,\n\t\t     unsigned int feature,\n\t\t     unsigned int segid)\n{\n\tu8 mask = V4L2_VP9_SEGMENT_FEATURE_ENABLED(feature);\n\n\tvp9_ctx->feature_data[segid][feature] = seg->feature_data[segid][feature];\n\tvp9_ctx->feature_enabled[segid] &= ~mask;\n\tvp9_ctx->feature_enabled[segid] |= (seg->feature_enabled[segid] & mask);\n}\n\nstatic inline s16 clip3(s16 x, s16 y, s16 z)\n{\n\treturn (z < x) ? x : (z > y) ? y : z;\n}\n\nstatic s16 feat_val_clip3(s16 feat_val, s16 feature_data, bool absolute, u8 clip)\n{\n\tif (absolute)\n\t\treturn feature_data;\n\n\treturn clip3(0, 255, feat_val + feature_data);\n}\n\nstatic void config_segment(struct hantro_ctx *ctx, const struct v4l2_ctrl_vp9_frame *dec_params)\n{\n\tstruct hantro_vp9_dec_hw_ctx *vp9_ctx = &ctx->vp9_dec;\n\tconst struct v4l2_vp9_segmentation *seg;\n\ts16 feat_val;\n\tunsigned char feat_id;\n\tunsigned int segid;\n\tbool segment_enabled, absolute, update_data;\n\n\tstatic const struct hantro_reg seg_regs[8][V4L2_VP9_SEG_LVL_MAX] = {\n\t\t{ vp9_quant_seg0, vp9_filt_level_seg0, vp9_refpic_seg0, vp9_skip_seg0 },\n\t\t{ vp9_quant_seg1, vp9_filt_level_seg1, vp9_refpic_seg1, vp9_skip_seg1 },\n\t\t{ vp9_quant_seg2, vp9_filt_level_seg2, vp9_refpic_seg2, vp9_skip_seg2 },\n\t\t{ vp9_quant_seg3, vp9_filt_level_seg3, vp9_refpic_seg3, vp9_skip_seg3 },\n\t\t{ vp9_quant_seg4, vp9_filt_level_seg4, vp9_refpic_seg4, vp9_skip_seg4 },\n\t\t{ vp9_quant_seg5, vp9_filt_level_seg5, vp9_refpic_seg5, vp9_skip_seg5 },\n\t\t{ vp9_quant_seg6, vp9_filt_level_seg6, vp9_refpic_seg6, vp9_skip_seg6 },\n\t\t{ vp9_quant_seg7, vp9_filt_level_seg7, vp9_refpic_seg7, vp9_skip_seg7 },\n\t};\n\n\tsegment_enabled = !!(dec_params->seg.flags & V4L2_VP9_SEGMENTATION_FLAG_ENABLED);\n\thantro_reg_write(ctx->dev, &vp9_segment_e, segment_enabled);\n\thantro_reg_write(ctx->dev, &vp9_segment_upd_e,\n\t\t\t !!(dec_params->seg.flags & V4L2_VP9_SEGMENTATION_FLAG_UPDATE_MAP));\n\thantro_reg_write(ctx->dev, &vp9_segment_temp_upd_e,\n\t\t\t !!(dec_params->seg.flags & V4L2_VP9_SEGMENTATION_FLAG_TEMPORAL_UPDATE));\n\n\tseg = &dec_params->seg;\n\tabsolute = !!(seg->flags & V4L2_VP9_SEGMENTATION_FLAG_ABS_OR_DELTA_UPDATE);\n\tupdate_data = !!(seg->flags & V4L2_VP9_SEGMENTATION_FLAG_UPDATE_DATA);\n\n\tfor (segid = 0; segid < 8; ++segid) {\n\t\t \n\t\tfeat_id = V4L2_VP9_SEG_LVL_ALT_Q;\n\t\tfeat_val = dec_params->quant.base_q_idx;\n\t\tif (segment_enabled) {\n\t\t\tif (update_data)\n\t\t\t\tupdate_feat_and_flag(vp9_ctx, seg, feat_id, segid);\n\t\t\tif (v4l2_vp9_seg_feat_enabled(vp9_ctx->feature_enabled, feat_id, segid))\n\t\t\t\tfeat_val = feat_val_clip3(feat_val,\n\t\t\t\t\t\t\t  vp9_ctx->feature_data[segid][feat_id],\n\t\t\t\t\t\t\t  absolute, 255);\n\t\t}\n\t\thantro_reg_write(ctx->dev, &seg_regs[segid][feat_id], feat_val);\n\n\t\t \n\t\tfeat_id = V4L2_VP9_SEG_LVL_ALT_L;\n\t\tfeat_val = dec_params->lf.level;\n\t\tif (segment_enabled) {\n\t\t\tif (update_data)\n\t\t\t\tupdate_feat_and_flag(vp9_ctx, seg, feat_id, segid);\n\t\t\tif (v4l2_vp9_seg_feat_enabled(vp9_ctx->feature_enabled, feat_id, segid))\n\t\t\t\tfeat_val = feat_val_clip3(feat_val,\n\t\t\t\t\t\t\t  vp9_ctx->feature_data[segid][feat_id],\n\t\t\t\t\t\t\t  absolute, 63);\n\t\t}\n\t\thantro_reg_write(ctx->dev, &seg_regs[segid][feat_id], feat_val);\n\n\t\t \n\t\tfeat_id = V4L2_VP9_SEG_LVL_REF_FRAME;\n\t\tfeat_val = 0;\n\t\tif (segment_enabled) {\n\t\t\tif (update_data)\n\t\t\t\tupdate_feat_and_flag(vp9_ctx, seg, feat_id, segid);\n\t\t\tif (!(dec_params->flags & V4L2_VP9_FRAME_FLAG_KEY_FRAME) &&\n\t\t\t    v4l2_vp9_seg_feat_enabled(vp9_ctx->feature_enabled, feat_id, segid))\n\t\t\t\tfeat_val = vp9_ctx->feature_data[segid][feat_id] + 1;\n\t\t}\n\t\thantro_reg_write(ctx->dev, &seg_regs[segid][feat_id], feat_val);\n\n\t\t \n\t\tfeat_id = V4L2_VP9_SEG_LVL_SKIP;\n\t\tfeat_val = 0;\n\t\tif (segment_enabled) {\n\t\t\tif (update_data)\n\t\t\t\tupdate_feat_and_flag(vp9_ctx, seg, feat_id, segid);\n\t\t\tfeat_val = v4l2_vp9_seg_feat_enabled(vp9_ctx->feature_enabled,\n\t\t\t\t\t\t\t     feat_id, segid) ? 1 : 0;\n\t\t}\n\t\thantro_reg_write(ctx->dev, &seg_regs[segid][feat_id], feat_val);\n\t}\n}\n\nstatic void config_loop_filter(struct hantro_ctx *ctx, const struct v4l2_ctrl_vp9_frame *dec_params)\n{\n\tbool d = dec_params->lf.flags & V4L2_VP9_LOOP_FILTER_FLAG_DELTA_ENABLED;\n\n\thantro_reg_write(ctx->dev, &vp9_filt_level, dec_params->lf.level);\n\thantro_reg_write(ctx->dev, &g2_out_filtering_dis, dec_params->lf.level == 0);\n\thantro_reg_write(ctx->dev, &vp9_filt_sharpness, dec_params->lf.sharpness);\n\n\thantro_reg_write(ctx->dev, &vp9_filt_ref_adj_0, d ? dec_params->lf.ref_deltas[0] : 0);\n\thantro_reg_write(ctx->dev, &vp9_filt_ref_adj_1, d ? dec_params->lf.ref_deltas[1] : 0);\n\thantro_reg_write(ctx->dev, &vp9_filt_ref_adj_2, d ? dec_params->lf.ref_deltas[2] : 0);\n\thantro_reg_write(ctx->dev, &vp9_filt_ref_adj_3, d ? dec_params->lf.ref_deltas[3] : 0);\n\thantro_reg_write(ctx->dev, &vp9_filt_mb_adj_0, d ? dec_params->lf.mode_deltas[0] : 0);\n\thantro_reg_write(ctx->dev, &vp9_filt_mb_adj_1, d ? dec_params->lf.mode_deltas[1] : 0);\n}\n\nstatic void config_picture_dimensions(struct hantro_ctx *ctx, struct hantro_decoded_buffer *dst)\n{\n\tu32 pic_w_4x4, pic_h_4x4;\n\n\thantro_reg_write(ctx->dev, &g2_pic_width_in_cbs, (dst->vp9.width + 7) / 8);\n\thantro_reg_write(ctx->dev, &g2_pic_height_in_cbs, (dst->vp9.height + 7) / 8);\n\tpic_w_4x4 = roundup(dst->vp9.width, 8) >> 2;\n\tpic_h_4x4 = roundup(dst->vp9.height, 8) >> 2;\n\thantro_reg_write(ctx->dev, &g2_pic_width_4x4, pic_w_4x4);\n\thantro_reg_write(ctx->dev, &g2_pic_height_4x4, pic_h_4x4);\n}\n\nstatic void\nconfig_bit_depth(struct hantro_ctx *ctx, const struct v4l2_ctrl_vp9_frame *dec_params)\n{\n\tif (ctx->dev->variant->legacy_regs) {\n\t\thantro_reg_write(ctx->dev, &g2_bit_depth_y, dec_params->bit_depth);\n\t\thantro_reg_write(ctx->dev, &g2_bit_depth_c, dec_params->bit_depth);\n\t\thantro_reg_write(ctx->dev, &g2_pix_shift, 0);\n\t} else {\n\t\thantro_reg_write(ctx->dev, &g2_bit_depth_y_minus8, dec_params->bit_depth - 8);\n\t\thantro_reg_write(ctx->dev, &g2_bit_depth_c_minus8, dec_params->bit_depth - 8);\n\t}\n}\n\nstatic inline bool is_lossless(const struct v4l2_vp9_quantization *quant)\n{\n\treturn quant->base_q_idx == 0 && quant->delta_q_uv_ac == 0 &&\n\t       quant->delta_q_uv_dc == 0 && quant->delta_q_y_dc == 0;\n}\n\nstatic void\nconfig_quant(struct hantro_ctx *ctx, const struct v4l2_ctrl_vp9_frame *dec_params)\n{\n\thantro_reg_write(ctx->dev, &vp9_qp_delta_y_dc, dec_params->quant.delta_q_y_dc);\n\thantro_reg_write(ctx->dev, &vp9_qp_delta_ch_dc, dec_params->quant.delta_q_uv_dc);\n\thantro_reg_write(ctx->dev, &vp9_qp_delta_ch_ac, dec_params->quant.delta_q_uv_ac);\n\thantro_reg_write(ctx->dev, &vp9_lossless_e, is_lossless(&dec_params->quant));\n}\n\nstatic u32\nhantro_interp_filter_from_v4l2(unsigned int interpolation_filter)\n{\n\tswitch (interpolation_filter) {\n\tcase V4L2_VP9_INTERP_FILTER_EIGHTTAP:\n\t\treturn 0x1;\n\tcase V4L2_VP9_INTERP_FILTER_EIGHTTAP_SMOOTH:\n\t\treturn 0;\n\tcase V4L2_VP9_INTERP_FILTER_EIGHTTAP_SHARP:\n\t\treturn 0x2;\n\tcase V4L2_VP9_INTERP_FILTER_BILINEAR:\n\t\treturn 0x3;\n\tcase V4L2_VP9_INTERP_FILTER_SWITCHABLE:\n\t\treturn 0x4;\n\t}\n\n\treturn 0;\n}\n\nstatic void\nconfig_others(struct hantro_ctx *ctx, const struct v4l2_ctrl_vp9_frame *dec_params,\n\t      bool intra_only, bool resolution_change)\n{\n\tstruct hantro_vp9_dec_hw_ctx *vp9_ctx = &ctx->vp9_dec;\n\n\thantro_reg_write(ctx->dev, &g2_idr_pic_e, intra_only);\n\n\thantro_reg_write(ctx->dev, &vp9_transform_mode, vp9_ctx->cur.tx_mode);\n\n\thantro_reg_write(ctx->dev, &vp9_mcomp_filt_type, intra_only ?\n\t\t0 : hantro_interp_filter_from_v4l2(dec_params->interpolation_filter));\n\n\thantro_reg_write(ctx->dev, &vp9_high_prec_mv_e,\n\t\t\t !!(dec_params->flags & V4L2_VP9_FRAME_FLAG_ALLOW_HIGH_PREC_MV));\n\n\thantro_reg_write(ctx->dev, &vp9_comp_pred_mode, dec_params->reference_mode);\n\n\thantro_reg_write(ctx->dev, &g2_tempor_mvp_e,\n\t\t\t !(dec_params->flags & V4L2_VP9_FRAME_FLAG_ERROR_RESILIENT) &&\n\t\t\t !(dec_params->flags & V4L2_VP9_FRAME_FLAG_KEY_FRAME) &&\n\t\t\t !(vp9_ctx->last.flags & V4L2_VP9_FRAME_FLAG_KEY_FRAME) &&\n\t\t\t !(dec_params->flags & V4L2_VP9_FRAME_FLAG_INTRA_ONLY) &&\n\t\t\t !resolution_change &&\n\t\t\t vp9_ctx->last.flags & V4L2_VP9_FRAME_FLAG_SHOW_FRAME\n\t);\n\n\thantro_reg_write(ctx->dev, &g2_write_mvs_e,\n\t\t\t !(dec_params->flags & V4L2_VP9_FRAME_FLAG_KEY_FRAME));\n}\n\nstatic void\nconfig_compound_reference(struct hantro_ctx *ctx,\n\t\t\t  const struct v4l2_ctrl_vp9_frame *dec_params)\n{\n\tu32 comp_fixed_ref, comp_var_ref[2];\n\tbool last_ref_frame_sign_bias;\n\tbool golden_ref_frame_sign_bias;\n\tbool alt_ref_frame_sign_bias;\n\tbool comp_ref_allowed = 0;\n\n\tcomp_fixed_ref = 0;\n\tcomp_var_ref[0] = 0;\n\tcomp_var_ref[1] = 0;\n\n\tlast_ref_frame_sign_bias = dec_params->ref_frame_sign_bias & V4L2_VP9_SIGN_BIAS_LAST;\n\tgolden_ref_frame_sign_bias = dec_params->ref_frame_sign_bias & V4L2_VP9_SIGN_BIAS_GOLDEN;\n\talt_ref_frame_sign_bias = dec_params->ref_frame_sign_bias & V4L2_VP9_SIGN_BIAS_ALT;\n\n\t \n\tcomp_ref_allowed |= golden_ref_frame_sign_bias != last_ref_frame_sign_bias;\n\tcomp_ref_allowed |= alt_ref_frame_sign_bias != last_ref_frame_sign_bias;\n\n\tif (comp_ref_allowed) {\n\t\tif (last_ref_frame_sign_bias ==\n\t\t    golden_ref_frame_sign_bias) {\n\t\t\tcomp_fixed_ref = ALTREF_FRAME;\n\t\t\tcomp_var_ref[0] = LAST_FRAME;\n\t\t\tcomp_var_ref[1] = GOLDEN_FRAME;\n\t\t} else if (last_ref_frame_sign_bias ==\n\t\t\t   alt_ref_frame_sign_bias) {\n\t\t\tcomp_fixed_ref = GOLDEN_FRAME;\n\t\t\tcomp_var_ref[0] = LAST_FRAME;\n\t\t\tcomp_var_ref[1] = ALTREF_FRAME;\n\t\t} else {\n\t\t\tcomp_fixed_ref = LAST_FRAME;\n\t\t\tcomp_var_ref[0] = GOLDEN_FRAME;\n\t\t\tcomp_var_ref[1] = ALTREF_FRAME;\n\t\t}\n\t}\n\n\thantro_reg_write(ctx->dev, &vp9_comp_pred_fixed_ref, comp_fixed_ref);\n\thantro_reg_write(ctx->dev, &vp9_comp_pred_var_ref0, comp_var_ref[0]);\n\thantro_reg_write(ctx->dev, &vp9_comp_pred_var_ref1, comp_var_ref[1]);\n}\n\n#define INNER_LOOP \\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tfor (m = 0; m < ARRAY_SIZE(adaptive->coef[0][0][0][0]); ++m) {\t\\\n\t\tmemcpy(adaptive->coef[i][j][k][l][m],\t\t\t\\\n\t\t       probs->coef[i][j][k][l][m],\t\t\t\\\n\t\t       sizeof(probs->coef[i][j][k][l][m]));\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\t\tadaptive->coef[i][j][k][l][m][3] = 0;\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n} while (0)\n\nstatic void config_probs(struct hantro_ctx *ctx, const struct v4l2_ctrl_vp9_frame *dec_params)\n{\n\tstruct hantro_vp9_dec_hw_ctx *vp9_ctx = &ctx->vp9_dec;\n\tstruct hantro_aux_buf *misc = &vp9_ctx->misc;\n\tstruct hantro_g2_all_probs *all_probs = misc->cpu;\n\tstruct hantro_g2_probs *adaptive;\n\tstruct hantro_g2_mv_probs *mv;\n\tconst struct v4l2_vp9_segmentation *seg = &dec_params->seg;\n\tconst struct v4l2_vp9_frame_context *probs = &vp9_ctx->probability_tables;\n\tint i, j, k, l, m;\n\n\tfor (i = 0; i < ARRAY_SIZE(all_probs->kf_y_mode_prob); ++i)\n\t\tfor (j = 0; j < ARRAY_SIZE(all_probs->kf_y_mode_prob[0]); ++j) {\n\t\t\tmemcpy(all_probs->kf_y_mode_prob[i][j],\n\t\t\t       v4l2_vp9_kf_y_mode_prob[i][j],\n\t\t\t       ARRAY_SIZE(all_probs->kf_y_mode_prob[i][j]));\n\n\t\t\tall_probs->kf_y_mode_prob_tail[i][j][0] =\n\t\t\t\tv4l2_vp9_kf_y_mode_prob[i][j][8];\n\t\t}\n\n\tmemcpy(all_probs->mb_segment_tree_probs, seg->tree_probs,\n\t       sizeof(all_probs->mb_segment_tree_probs));\n\n\tmemcpy(all_probs->segment_pred_probs, seg->pred_probs,\n\t       sizeof(all_probs->segment_pred_probs));\n\n\tfor (i = 0; i < ARRAY_SIZE(all_probs->kf_uv_mode_prob); ++i) {\n\t\tmemcpy(all_probs->kf_uv_mode_prob[i], v4l2_vp9_kf_uv_mode_prob[i],\n\t\t       ARRAY_SIZE(all_probs->kf_uv_mode_prob[i]));\n\n\t\tall_probs->kf_uv_mode_prob_tail[i][0] = v4l2_vp9_kf_uv_mode_prob[i][8];\n\t}\n\n\tadaptive = &all_probs->probs;\n\n\tfor (i = 0; i < ARRAY_SIZE(adaptive->inter_mode); ++i) {\n\t\tmemcpy(adaptive->inter_mode[i], probs->inter_mode[i],\n\t\t       ARRAY_SIZE(probs->inter_mode[i]));\n\n\t\tadaptive->inter_mode[i][3] = 0;\n\t}\n\n\tmemcpy(adaptive->is_inter, probs->is_inter, sizeof(adaptive->is_inter));\n\n\tfor (i = 0; i < ARRAY_SIZE(adaptive->uv_mode); ++i) {\n\t\tmemcpy(adaptive->uv_mode[i], probs->uv_mode[i],\n\t\t       sizeof(adaptive->uv_mode[i]));\n\t\tadaptive->uv_mode_tail[i][0] = probs->uv_mode[i][8];\n\t}\n\n\tmemcpy(adaptive->tx8, probs->tx8, sizeof(adaptive->tx8));\n\tmemcpy(adaptive->tx16, probs->tx16, sizeof(adaptive->tx16));\n\tmemcpy(adaptive->tx32, probs->tx32, sizeof(adaptive->tx32));\n\n\tfor (i = 0; i < ARRAY_SIZE(adaptive->y_mode); ++i) {\n\t\tmemcpy(adaptive->y_mode[i], probs->y_mode[i],\n\t\t       ARRAY_SIZE(adaptive->y_mode[i]));\n\n\t\tadaptive->y_mode_tail[i][0] = probs->y_mode[i][8];\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(adaptive->partition[0]); ++i) {\n\t\tmemcpy(adaptive->partition[0][i], v4l2_vp9_kf_partition_probs[i],\n\t\t       sizeof(v4l2_vp9_kf_partition_probs[i]));\n\n\t\tadaptive->partition[0][i][3] = 0;\n\t}\n\n\tfor (i = 0; i < ARRAY_SIZE(adaptive->partition[1]); ++i) {\n\t\tmemcpy(adaptive->partition[1][i], probs->partition[i],\n\t\t       sizeof(probs->partition[i]));\n\n\t\tadaptive->partition[1][i][3] = 0;\n\t}\n\n\tmemcpy(adaptive->interp_filter, probs->interp_filter,\n\t       sizeof(adaptive->interp_filter));\n\n\tmemcpy(adaptive->comp_mode, probs->comp_mode, sizeof(adaptive->comp_mode));\n\n\tmemcpy(adaptive->skip, probs->skip, sizeof(adaptive->skip));\n\n\tmv = &adaptive->mv;\n\n\tmemcpy(mv->joint, probs->mv.joint, sizeof(mv->joint));\n\tmemcpy(mv->sign, probs->mv.sign, sizeof(mv->sign));\n\tmemcpy(mv->class0_bit, probs->mv.class0_bit, sizeof(mv->class0_bit));\n\tmemcpy(mv->fr, probs->mv.fr, sizeof(mv->fr));\n\tmemcpy(mv->class0_hp, probs->mv.class0_hp, sizeof(mv->class0_hp));\n\tmemcpy(mv->hp, probs->mv.hp, sizeof(mv->hp));\n\tmemcpy(mv->classes, probs->mv.classes, sizeof(mv->classes));\n\tmemcpy(mv->class0_fr, probs->mv.class0_fr, sizeof(mv->class0_fr));\n\tmemcpy(mv->bits, probs->mv.bits, sizeof(mv->bits));\n\n\tmemcpy(adaptive->single_ref, probs->single_ref, sizeof(adaptive->single_ref));\n\n\tmemcpy(adaptive->comp_ref, probs->comp_ref, sizeof(adaptive->comp_ref));\n\n\tfor (i = 0; i < ARRAY_SIZE(adaptive->coef); ++i)\n\t\tfor (j = 0; j < ARRAY_SIZE(adaptive->coef[0]); ++j)\n\t\t\tfor (k = 0; k < ARRAY_SIZE(adaptive->coef[0][0]); ++k)\n\t\t\t\tfor (l = 0; l < ARRAY_SIZE(adaptive->coef[0][0][0]); ++l)\n\t\t\t\t\tINNER_LOOP;\n\n\thantro_write_addr(ctx->dev, G2_VP9_PROBS_ADDR, misc->dma);\n}\n\nstatic void config_counts(struct hantro_ctx *ctx)\n{\n\tstruct hantro_vp9_dec_hw_ctx *vp9_dec = &ctx->vp9_dec;\n\tstruct hantro_aux_buf *misc = &vp9_dec->misc;\n\tdma_addr_t addr = misc->dma + vp9_dec->ctx_counters_offset;\n\n\thantro_write_addr(ctx->dev, G2_VP9_CTX_COUNT_ADDR, addr);\n}\n\nstatic void config_seg_map(struct hantro_ctx *ctx,\n\t\t\t   const struct v4l2_ctrl_vp9_frame *dec_params,\n\t\t\t   bool intra_only, bool update_map)\n{\n\tstruct hantro_vp9_dec_hw_ctx *vp9_ctx = &ctx->vp9_dec;\n\tstruct hantro_aux_buf *segment_map = &vp9_ctx->segment_map;\n\tdma_addr_t addr;\n\n\tif (intra_only ||\n\t    (dec_params->flags & V4L2_VP9_FRAME_FLAG_ERROR_RESILIENT)) {\n\t\tmemset(segment_map->cpu, 0, segment_map->size);\n\t\tmemset(vp9_ctx->feature_data, 0, sizeof(vp9_ctx->feature_data));\n\t\tmemset(vp9_ctx->feature_enabled, 0, sizeof(vp9_ctx->feature_enabled));\n\t}\n\n\taddr = segment_map->dma + vp9_ctx->active_segment * vp9_ctx->segment_map_size;\n\thantro_write_addr(ctx->dev, G2_VP9_SEGMENT_READ_ADDR, addr);\n\n\taddr = segment_map->dma + (1 - vp9_ctx->active_segment) * vp9_ctx->segment_map_size;\n\thantro_write_addr(ctx->dev, G2_VP9_SEGMENT_WRITE_ADDR, addr);\n\n\tif (update_map)\n\t\tvp9_ctx->active_segment = 1 - vp9_ctx->active_segment;\n}\n\nstatic void\nconfig_source(struct hantro_ctx *ctx, const struct v4l2_ctrl_vp9_frame *dec_params,\n\t      struct vb2_v4l2_buffer *vb2_src)\n{\n\tdma_addr_t stream_base, tmp_addr;\n\tunsigned int headres_size;\n\tu32 src_len, start_bit, src_buf_len;\n\n\theadres_size = dec_params->uncompressed_header_size\n\t\t     + dec_params->compressed_header_size;\n\n\tstream_base = vb2_dma_contig_plane_dma_addr(&vb2_src->vb2_buf, 0);\n\n\ttmp_addr = stream_base + headres_size;\n\tif (ctx->dev->variant->legacy_regs)\n\t\thantro_write_addr(ctx->dev, G2_STREAM_ADDR, (tmp_addr & ~0xf));\n\telse\n\t\thantro_write_addr(ctx->dev, G2_STREAM_ADDR, stream_base);\n\n\tstart_bit = (tmp_addr & 0xf) * 8;\n\thantro_reg_write(ctx->dev, &g2_start_bit, start_bit);\n\n\tsrc_len = vb2_get_plane_payload(&vb2_src->vb2_buf, 0);\n\tsrc_len += start_bit / 8 - headres_size;\n\thantro_reg_write(ctx->dev, &g2_stream_len, src_len);\n\n\tif (!ctx->dev->variant->legacy_regs) {\n\t\ttmp_addr &= ~0xf;\n\t\thantro_reg_write(ctx->dev, &g2_strm_start_offset, tmp_addr - stream_base);\n\t\tsrc_buf_len = vb2_plane_size(&vb2_src->vb2_buf, 0);\n\t\thantro_reg_write(ctx->dev, &g2_strm_buffer_len, src_buf_len);\n\t}\n}\n\nstatic void\nconfig_registers(struct hantro_ctx *ctx, const struct v4l2_ctrl_vp9_frame *dec_params,\n\t\t struct vb2_v4l2_buffer *vb2_src, struct vb2_v4l2_buffer *vb2_dst)\n{\n\tstruct hantro_decoded_buffer *dst, *last, *mv_ref;\n\tstruct hantro_vp9_dec_hw_ctx *vp9_ctx = &ctx->vp9_dec;\n\tconst struct v4l2_vp9_segmentation *seg;\n\tbool intra_only, resolution_change;\n\n\t \n\tdst = vb2_to_hantro_decoded_buf(&vb2_dst->vb2_buf);\n\n\tif (vp9_ctx->last.valid)\n\t\tlast = get_ref_buf(ctx, &dst->base.vb, vp9_ctx->last.timestamp);\n\telse\n\t\tlast = dst;\n\n\tupdate_dec_buf_info(dst, dec_params);\n\tupdate_ctx_cur_info(vp9_ctx, dst, dec_params);\n\tseg = &dec_params->seg;\n\n\tintra_only = !!(dec_params->flags &\n\t\t\t(V4L2_VP9_FRAME_FLAG_KEY_FRAME |\n\t\t\tV4L2_VP9_FRAME_FLAG_INTRA_ONLY));\n\n\tif (!intra_only &&\n\t    !(dec_params->flags & V4L2_VP9_FRAME_FLAG_ERROR_RESILIENT) &&\n\t    vp9_ctx->last.valid)\n\t\tmv_ref = last;\n\telse\n\t\tmv_ref = dst;\n\n\tresolution_change = dst->vp9.width != last->vp9.width ||\n\t\t\t    dst->vp9.height != last->vp9.height;\n\n\t \n\thantro_reg_write(ctx->dev, &g2_mode, VP9_DEC_MODE);\n\tif (!ctx->dev->variant->legacy_regs) {\n\t\thantro_reg_write(ctx->dev, &g2_strm_swap, 0xf);\n\t\thantro_reg_write(ctx->dev, &g2_dirmv_swap, 0xf);\n\t\thantro_reg_write(ctx->dev, &g2_compress_swap, 0xf);\n\t\thantro_reg_write(ctx->dev, &g2_ref_compress_bypass, 1);\n\t} else {\n\t\thantro_reg_write(ctx->dev, &g2_strm_swap_old, 0x1f);\n\t\thantro_reg_write(ctx->dev, &g2_pic_swap, 0x10);\n\t\thantro_reg_write(ctx->dev, &g2_dirmv_swap_old, 0x10);\n\t\thantro_reg_write(ctx->dev, &g2_tab0_swap_old, 0x10);\n\t\thantro_reg_write(ctx->dev, &g2_tab1_swap_old, 0x10);\n\t\thantro_reg_write(ctx->dev, &g2_tab2_swap_old, 0x10);\n\t\thantro_reg_write(ctx->dev, &g2_tab3_swap_old, 0x10);\n\t\thantro_reg_write(ctx->dev, &g2_rscan_swap, 0x10);\n\t}\n\thantro_reg_write(ctx->dev, &g2_buswidth, BUS_WIDTH_128);\n\thantro_reg_write(ctx->dev, &g2_max_burst, 16);\n\thantro_reg_write(ctx->dev, &g2_apf_threshold, 8);\n\thantro_reg_write(ctx->dev, &g2_clk_gate_e, 1);\n\thantro_reg_write(ctx->dev, &g2_max_cb_size, 6);\n\thantro_reg_write(ctx->dev, &g2_min_cb_size, 3);\n\tif (ctx->dev->variant->double_buffer)\n\t\thantro_reg_write(ctx->dev, &g2_double_buffer_e, 1);\n\n\tconfig_output(ctx, dst, dec_params);\n\n\tif (!intra_only)\n\t\tconfig_ref_registers(ctx, dec_params, dst, mv_ref);\n\n\tconfig_tiles(ctx, dec_params, dst);\n\tconfig_segment(ctx, dec_params);\n\tconfig_loop_filter(ctx, dec_params);\n\tconfig_picture_dimensions(ctx, dst);\n\tconfig_bit_depth(ctx, dec_params);\n\tconfig_quant(ctx, dec_params);\n\tconfig_others(ctx, dec_params, intra_only, resolution_change);\n\tconfig_compound_reference(ctx, dec_params);\n\tconfig_probs(ctx, dec_params);\n\tconfig_counts(ctx);\n\tconfig_seg_map(ctx, dec_params, intra_only,\n\t\t       seg->flags & V4L2_VP9_SEGMENTATION_FLAG_UPDATE_MAP);\n\tconfig_source(ctx, dec_params, vb2_src);\n}\n\nint hantro_g2_vp9_dec_run(struct hantro_ctx *ctx)\n{\n\tconst struct v4l2_ctrl_vp9_frame *decode_params;\n\tstruct vb2_v4l2_buffer *src;\n\tstruct vb2_v4l2_buffer *dst;\n\tint ret;\n\n\thantro_g2_check_idle(ctx->dev);\n\n\tret = start_prepare_run(ctx, &decode_params);\n\tif (ret) {\n\t\thantro_end_prepare_run(ctx);\n\t\treturn ret;\n\t}\n\n\tsrc = hantro_get_src_buf(ctx);\n\tdst = hantro_get_dst_buf(ctx);\n\n\tconfig_registers(ctx, decode_params, src, dst);\n\n\thantro_end_prepare_run(ctx);\n\n\tvdpu_write(ctx->dev, G2_REG_INTERRUPT_DEC_E, G2_REG_INTERRUPT);\n\n\treturn 0;\n}\n\n#define copy_tx_and_skip(p1, p2)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tmemcpy((p1)->tx8, (p2)->tx8, sizeof((p1)->tx8));\t\\\n\tmemcpy((p1)->tx16, (p2)->tx16, sizeof((p1)->tx16));\t\\\n\tmemcpy((p1)->tx32, (p2)->tx32, sizeof((p1)->tx32));\t\\\n\tmemcpy((p1)->skip, (p2)->skip, sizeof((p1)->skip));\t\\\n} while (0)\n\nvoid hantro_g2_vp9_dec_done(struct hantro_ctx *ctx)\n{\n\tstruct hantro_vp9_dec_hw_ctx *vp9_ctx = &ctx->vp9_dec;\n\tunsigned int fctx_idx;\n\n\tif (!(vp9_ctx->cur.flags & V4L2_VP9_FRAME_FLAG_REFRESH_FRAME_CTX))\n\t\tgoto out_update_last;\n\n\tfctx_idx = vp9_ctx->cur.frame_context_idx;\n\n\tif (!(vp9_ctx->cur.flags & V4L2_VP9_FRAME_FLAG_PARALLEL_DEC_MODE)) {\n\t\t \n\t\tstruct v4l2_vp9_frame_context *probs = &vp9_ctx->probability_tables;\n\t\tbool frame_is_intra = vp9_ctx->cur.flags &\n\t\t    (V4L2_VP9_FRAME_FLAG_KEY_FRAME | V4L2_VP9_FRAME_FLAG_INTRA_ONLY);\n\t\tstruct tx_and_skip {\n\t\t\tu8 tx8[2][1];\n\t\t\tu8 tx16[2][2];\n\t\t\tu8 tx32[2][3];\n\t\t\tu8 skip[3];\n\t\t} _tx_skip, *tx_skip = &_tx_skip;\n\t\tstruct v4l2_vp9_frame_symbol_counts *counts;\n\t\tstruct symbol_counts *hantro_cnts;\n\t\tu32 tx16p[2][4];\n\t\tint i;\n\n\t\t \n\t\tif (frame_is_intra)\n\t\t\tcopy_tx_and_skip(tx_skip, probs);\n\n\t\t \n\t\t*probs = vp9_ctx->frame_context[fctx_idx];\n\n\t\t \n\t\tif (frame_is_intra)\n\t\t\tcopy_tx_and_skip(probs, tx_skip);\n\n\t\tcounts = &vp9_ctx->cnts;\n\t\thantro_cnts = vp9_ctx->misc.cpu + vp9_ctx->ctx_counters_offset;\n\t\tfor (i = 0; i < ARRAY_SIZE(tx16p); ++i) {\n\t\t\tmemcpy(tx16p[i],\n\t\t\t       hantro_cnts->tx16x16_count[i],\n\t\t\t       sizeof(hantro_cnts->tx16x16_count[0]));\n\t\t\ttx16p[i][3] = 0;\n\t\t}\n\t\tcounts->tx16p = &tx16p;\n\n\t\tv4l2_vp9_adapt_coef_probs(probs, counts,\n\t\t\t\t\t  !vp9_ctx->last.valid ||\n\t\t\t\t\t  vp9_ctx->last.flags & V4L2_VP9_FRAME_FLAG_KEY_FRAME,\n\t\t\t\t\t  frame_is_intra);\n\n\t\tif (!frame_is_intra) {\n\t\t\t \n\t\t\tu32 mv_mode[7][4];\n\n\t\t\tfor (i = 0; i < ARRAY_SIZE(mv_mode); ++i) {\n\t\t\t\tmv_mode[i][0] = hantro_cnts->inter_mode_counts[i][1][0];\n\t\t\t\tmv_mode[i][1] = hantro_cnts->inter_mode_counts[i][2][0];\n\t\t\t\tmv_mode[i][2] = hantro_cnts->inter_mode_counts[i][0][0];\n\t\t\t\tmv_mode[i][3] = hantro_cnts->inter_mode_counts[i][2][1];\n\t\t\t}\n\t\t\tcounts->mv_mode = &mv_mode;\n\t\t\tv4l2_vp9_adapt_noncoef_probs(&vp9_ctx->probability_tables, counts,\n\t\t\t\t\t\t     vp9_ctx->cur.reference_mode,\n\t\t\t\t\t\t     vp9_ctx->cur.interpolation_filter,\n\t\t\t\t\t\t     vp9_ctx->cur.tx_mode, vp9_ctx->cur.flags);\n\t\t}\n\t}\n\n\tvp9_ctx->frame_context[fctx_idx] = vp9_ctx->probability_tables;\n\nout_update_last:\n\tvp9_ctx->last = vp9_ctx->cur;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}