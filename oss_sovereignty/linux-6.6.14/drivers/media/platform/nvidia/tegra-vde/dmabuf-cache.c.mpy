{
  "module_name": "dmabuf-cache.c",
  "hash_id": "23021f0a7399c6492c610bf57f1f22be5e43084e870d9b7122bd23d0cf11671e",
  "original_prompt": "Ingested from linux-6.6.14/drivers/media/platform/nvidia/tegra-vde/dmabuf-cache.c",
  "human_readable_source": "\n \n\n#include <linux/dma-buf.h>\n#include <linux/iova.h>\n#include <linux/kernel.h>\n#include <linux/list.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/workqueue.h>\n#include <linux/module.h>\n\n#include \"vde.h\"\n\nMODULE_IMPORT_NS(DMA_BUF);\n\nstruct tegra_vde_cache_entry {\n\tenum dma_data_direction dma_dir;\n\tstruct dma_buf_attachment *a;\n\tstruct delayed_work dwork;\n\tstruct tegra_vde *vde;\n\tstruct list_head list;\n\tstruct sg_table *sgt;\n\tstruct iova *iova;\n\tunsigned int refcnt;\n};\n\nstatic void tegra_vde_release_entry(struct tegra_vde_cache_entry *entry)\n{\n\tstruct dma_buf *dmabuf = entry->a->dmabuf;\n\n\tWARN_ON_ONCE(entry->refcnt);\n\n\tif (entry->vde->domain)\n\t\ttegra_vde_iommu_unmap(entry->vde, entry->iova);\n\n\tdma_buf_unmap_attachment_unlocked(entry->a, entry->sgt, entry->dma_dir);\n\tdma_buf_detach(dmabuf, entry->a);\n\tdma_buf_put(dmabuf);\n\n\tlist_del(&entry->list);\n\tkfree(entry);\n}\n\nstatic void tegra_vde_delayed_unmap(struct work_struct *work)\n{\n\tstruct tegra_vde_cache_entry *entry;\n\tstruct tegra_vde *vde;\n\n\tentry = container_of(work, struct tegra_vde_cache_entry,\n\t\t\t     dwork.work);\n\tvde = entry->vde;\n\n\tmutex_lock(&vde->map_lock);\n\ttegra_vde_release_entry(entry);\n\tmutex_unlock(&vde->map_lock);\n}\n\nint tegra_vde_dmabuf_cache_map(struct tegra_vde *vde,\n\t\t\t       struct dma_buf *dmabuf,\n\t\t\t       enum dma_data_direction dma_dir,\n\t\t\t       struct dma_buf_attachment **ap,\n\t\t\t       dma_addr_t *addrp)\n{\n\tstruct dma_buf_attachment *attachment;\n\tstruct tegra_vde_cache_entry *entry;\n\tstruct device *dev = vde->dev;\n\tstruct sg_table *sgt;\n\tstruct iova *iova;\n\tint err;\n\n\tmutex_lock(&vde->map_lock);\n\n\tlist_for_each_entry(entry, &vde->map_list, list) {\n\t\tif (entry->a->dmabuf != dmabuf)\n\t\t\tcontinue;\n\n\t\tif (!cancel_delayed_work(&entry->dwork))\n\t\t\tcontinue;\n\n\t\tif (entry->dma_dir != dma_dir)\n\t\t\tentry->dma_dir = DMA_BIDIRECTIONAL;\n\n\t\tdma_buf_put(dmabuf);\n\n\t\tif (vde->domain)\n\t\t\t*addrp = iova_dma_addr(&vde->iova, entry->iova);\n\t\telse\n\t\t\t*addrp = sg_dma_address(entry->sgt->sgl);\n\n\t\tgoto ref;\n\t}\n\n\tattachment = dma_buf_attach(dmabuf, dev);\n\tif (IS_ERR(attachment)) {\n\t\tdev_err(dev, \"Failed to attach dmabuf\\n\");\n\t\terr = PTR_ERR(attachment);\n\t\tgoto err_unlock;\n\t}\n\n\tsgt = dma_buf_map_attachment_unlocked(attachment, dma_dir);\n\tif (IS_ERR(sgt)) {\n\t\tdev_err(dev, \"Failed to get dmabufs sg_table\\n\");\n\t\terr = PTR_ERR(sgt);\n\t\tgoto err_detach;\n\t}\n\n\tif (!vde->domain && sgt->nents > 1) {\n\t\tdev_err(dev, \"Sparse DMA region is unsupported, please enable IOMMU\\n\");\n\t\terr = -EINVAL;\n\t\tgoto err_unmap;\n\t}\n\n\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry) {\n\t\terr = -ENOMEM;\n\t\tgoto err_unmap;\n\t}\n\n\tif (vde->domain) {\n\t\terr = tegra_vde_iommu_map(vde, sgt, &iova, dmabuf->size);\n\t\tif (err)\n\t\t\tgoto err_free;\n\n\t\t*addrp = iova_dma_addr(&vde->iova, iova);\n\t} else {\n\t\t*addrp = sg_dma_address(sgt->sgl);\n\t\tiova = NULL;\n\t}\n\n\tINIT_DELAYED_WORK(&entry->dwork, tegra_vde_delayed_unmap);\n\tlist_add(&entry->list, &vde->map_list);\n\n\tentry->dma_dir = dma_dir;\n\tentry->iova = iova;\n\tentry->vde = vde;\n\tentry->sgt = sgt;\n\tentry->a = attachment;\nref:\n\tentry->refcnt++;\n\n\t*ap = entry->a;\n\n\tmutex_unlock(&vde->map_lock);\n\n\treturn 0;\n\nerr_free:\n\tkfree(entry);\nerr_unmap:\n\tdma_buf_unmap_attachment_unlocked(attachment, sgt, dma_dir);\nerr_detach:\n\tdma_buf_detach(dmabuf, attachment);\nerr_unlock:\n\tmutex_unlock(&vde->map_lock);\n\n\treturn err;\n}\n\nvoid tegra_vde_dmabuf_cache_unmap(struct tegra_vde *vde,\n\t\t\t\t  struct dma_buf_attachment *a,\n\t\t\t\t  bool release)\n{\n\tstruct tegra_vde_cache_entry *entry;\n\n\tmutex_lock(&vde->map_lock);\n\n\tlist_for_each_entry(entry, &vde->map_list, list) {\n\t\tif (entry->a != a)\n\t\t\tcontinue;\n\n\t\tWARN_ON_ONCE(!entry->refcnt);\n\n\t\tif (--entry->refcnt == 0) {\n\t\t\tif (release)\n\t\t\t\ttegra_vde_release_entry(entry);\n\t\t\telse\n\t\t\t\tschedule_delayed_work(&entry->dwork, 5 * HZ);\n\t\t}\n\t\tbreak;\n\t}\n\n\tmutex_unlock(&vde->map_lock);\n}\n\nvoid tegra_vde_dmabuf_cache_unmap_sync(struct tegra_vde *vde)\n{\n\tstruct tegra_vde_cache_entry *entry, *tmp;\n\n\tmutex_lock(&vde->map_lock);\n\n\tlist_for_each_entry_safe(entry, tmp, &vde->map_list, list) {\n\t\tif (entry->refcnt)\n\t\t\tcontinue;\n\n\t\tif (!cancel_delayed_work(&entry->dwork))\n\t\t\tcontinue;\n\n\t\ttegra_vde_release_entry(entry);\n\t}\n\n\tmutex_unlock(&vde->map_lock);\n}\n\nvoid tegra_vde_dmabuf_cache_unmap_all(struct tegra_vde *vde)\n{\n\tstruct tegra_vde_cache_entry *entry, *tmp;\n\n\tmutex_lock(&vde->map_lock);\n\n\twhile (!list_empty(&vde->map_list)) {\n\t\tlist_for_each_entry_safe(entry, tmp, &vde->map_list, list) {\n\t\t\tif (!cancel_delayed_work(&entry->dwork))\n\t\t\t\tcontinue;\n\n\t\t\ttegra_vde_release_entry(entry);\n\t\t}\n\n\t\tmutex_unlock(&vde->map_lock);\n\t\tschedule();\n\t\tmutex_lock(&vde->map_lock);\n\t}\n\n\tmutex_unlock(&vde->map_lock);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}