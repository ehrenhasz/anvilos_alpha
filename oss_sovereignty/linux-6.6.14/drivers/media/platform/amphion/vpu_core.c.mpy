{
  "module_name": "vpu_core.c",
  "hash_id": "43ff9a7be6b510807c108c07d9e6b8d8c68ab8af41b65dd063c9ad3202e2aa44",
  "original_prompt": "Ingested from linux-6.6.14/drivers/media/platform/amphion/vpu_core.c",
  "human_readable_source": "\n \n\n#include <linux/init.h>\n#include <linux/interconnect.h>\n#include <linux/ioctl.h>\n#include <linux/list.h>\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/of.h>\n#include <linux/of_address.h>\n#include <linux/platform_device.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n#include <linux/pm_runtime.h>\n#include <linux/pm_domain.h>\n#include <linux/firmware.h>\n#include <linux/vmalloc.h>\n#include \"vpu.h\"\n#include \"vpu_defs.h\"\n#include \"vpu_core.h\"\n#include \"vpu_mbox.h\"\n#include \"vpu_msgs.h\"\n#include \"vpu_rpc.h\"\n#include \"vpu_cmds.h\"\n\nvoid csr_writel(struct vpu_core *core, u32 reg, u32 val)\n{\n\twritel(val, core->base + reg);\n}\n\nu32 csr_readl(struct vpu_core *core, u32 reg)\n{\n\treturn readl(core->base + reg);\n}\n\nstatic int vpu_core_load_firmware(struct vpu_core *core)\n{\n\tconst struct firmware *pfw = NULL;\n\tint ret = 0;\n\n\tif (!core->fw.virt) {\n\t\tdev_err(core->dev, \"firmware buffer is not ready\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = request_firmware(&pfw, core->res->fwname, core->dev);\n\tdev_dbg(core->dev, \"request_firmware %s : %d\\n\", core->res->fwname, ret);\n\tif (ret) {\n\t\tdev_err(core->dev, \"request firmware %s failed, ret = %d\\n\",\n\t\t\tcore->res->fwname, ret);\n\t\treturn ret;\n\t}\n\n\tif (core->fw.length < pfw->size) {\n\t\tdev_err(core->dev, \"firmware buffer size want %zu, but %d\\n\",\n\t\t\tpfw->size, core->fw.length);\n\t\tret = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\tmemset(core->fw.virt, 0, core->fw.length);\n\tmemcpy(core->fw.virt, pfw->data, pfw->size);\n\tcore->fw.bytesused = pfw->size;\n\tret = vpu_iface_on_firmware_loaded(core);\nexit:\n\trelease_firmware(pfw);\n\tpfw = NULL;\n\n\treturn ret;\n}\n\nstatic int vpu_core_boot_done(struct vpu_core *core)\n{\n\tu32 fw_version;\n\n\tfw_version = vpu_iface_get_version(core);\n\tdev_info(core->dev, \"%s firmware version : %d.%d.%d\\n\",\n\t\t vpu_core_type_desc(core->type),\n\t\t (fw_version >> 16) & 0xff,\n\t\t (fw_version >> 8) & 0xff,\n\t\t fw_version & 0xff);\n\tcore->supported_instance_count = vpu_iface_get_max_instance_count(core);\n\tif (core->res->act_size) {\n\t\tu32 count = core->act.length / core->res->act_size;\n\n\t\tcore->supported_instance_count = min(core->supported_instance_count, count);\n\t}\n\tif (core->supported_instance_count >= BITS_PER_TYPE(core->instance_mask))\n\t\tcore->supported_instance_count = BITS_PER_TYPE(core->instance_mask);\n\tcore->fw_version = fw_version;\n\tvpu_core_set_state(core, VPU_CORE_ACTIVE);\n\n\treturn 0;\n}\n\nstatic int vpu_core_wait_boot_done(struct vpu_core *core)\n{\n\tint ret;\n\n\tret = wait_for_completion_timeout(&core->cmp, VPU_TIMEOUT);\n\tif (!ret) {\n\t\tdev_err(core->dev, \"boot timeout\\n\");\n\t\treturn -EINVAL;\n\t}\n\treturn vpu_core_boot_done(core);\n}\n\nstatic int vpu_core_boot(struct vpu_core *core, bool load)\n{\n\tint ret;\n\n\treinit_completion(&core->cmp);\n\tif (load) {\n\t\tret = vpu_core_load_firmware(core);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tvpu_iface_boot_core(core);\n\treturn vpu_core_wait_boot_done(core);\n}\n\nstatic int vpu_core_shutdown(struct vpu_core *core)\n{\n\treturn vpu_iface_shutdown_core(core);\n}\n\nstatic int vpu_core_restore(struct vpu_core *core)\n{\n\tint ret;\n\n\tret = vpu_core_sw_reset(core);\n\tif (ret)\n\t\treturn ret;\n\n\tvpu_core_boot_done(core);\n\treturn vpu_iface_restore_core(core);\n}\n\nstatic int __vpu_alloc_dma(struct device *dev, struct vpu_buffer *buf)\n{\n\tgfp_t gfp = GFP_KERNEL | GFP_DMA32;\n\n\tif (!buf->length)\n\t\treturn 0;\n\n\tbuf->virt = dma_alloc_coherent(dev, buf->length, &buf->phys, gfp);\n\tif (!buf->virt)\n\t\treturn -ENOMEM;\n\n\tbuf->dev = dev;\n\n\treturn 0;\n}\n\nvoid vpu_free_dma(struct vpu_buffer *buf)\n{\n\tif (!buf->virt || !buf->dev)\n\t\treturn;\n\n\tdma_free_coherent(buf->dev, buf->length, buf->virt, buf->phys);\n\tbuf->virt = NULL;\n\tbuf->phys = 0;\n\tbuf->length = 0;\n\tbuf->bytesused = 0;\n\tbuf->dev = NULL;\n}\n\nint vpu_alloc_dma(struct vpu_core *core, struct vpu_buffer *buf)\n{\n\treturn __vpu_alloc_dma(core->dev, buf);\n}\n\nvoid vpu_core_set_state(struct vpu_core *core, enum vpu_core_state state)\n{\n\tif (state != core->state)\n\t\tvpu_trace(core->dev, \"vpu core state change from %d to %d\\n\", core->state, state);\n\tcore->state = state;\n\tif (core->state == VPU_CORE_DEINIT)\n\t\tcore->hang_mask = 0;\n}\n\nstatic void vpu_core_update_state(struct vpu_core *core)\n{\n\tif (!vpu_iface_get_power_state(core)) {\n\t\tif (core->request_count)\n\t\t\tvpu_core_set_state(core, VPU_CORE_HANG);\n\t\telse\n\t\t\tvpu_core_set_state(core, VPU_CORE_DEINIT);\n\n\t} else if (core->state == VPU_CORE_ACTIVE && core->hang_mask) {\n\t\tvpu_core_set_state(core, VPU_CORE_HANG);\n\t}\n}\n\nstatic struct vpu_core *vpu_core_find_proper_by_type(struct vpu_dev *vpu, u32 type)\n{\n\tstruct vpu_core *core = NULL;\n\tint request_count = INT_MAX;\n\tstruct vpu_core *c;\n\n\tlist_for_each_entry(c, &vpu->cores, list) {\n\t\tdev_dbg(c->dev, \"instance_mask = 0x%lx, state = %d\\n\", c->instance_mask, c->state);\n\t\tif (c->type != type)\n\t\t\tcontinue;\n\t\tmutex_lock(&c->lock);\n\t\tvpu_core_update_state(c);\n\t\tmutex_unlock(&c->lock);\n\t\tif (c->state == VPU_CORE_DEINIT) {\n\t\t\tcore = c;\n\t\t\tbreak;\n\t\t}\n\t\tif (c->state != VPU_CORE_ACTIVE)\n\t\t\tcontinue;\n\t\tif (c->request_count < request_count) {\n\t\t\trequest_count = c->request_count;\n\t\t\tcore = c;\n\t\t}\n\t\tif (!request_count)\n\t\t\tbreak;\n\t}\n\n\treturn core;\n}\n\nstatic bool vpu_core_is_exist(struct vpu_dev *vpu, struct vpu_core *core)\n{\n\tstruct vpu_core *c;\n\n\tlist_for_each_entry(c, &vpu->cores, list) {\n\t\tif (c == core)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic void vpu_core_get_vpu(struct vpu_core *core)\n{\n\tcore->vpu->get_vpu(core->vpu);\n\tif (core->type == VPU_CORE_TYPE_ENC)\n\t\tcore->vpu->get_enc(core->vpu);\n\tif (core->type == VPU_CORE_TYPE_DEC)\n\t\tcore->vpu->get_dec(core->vpu);\n}\n\nstatic int vpu_core_register(struct device *dev, struct vpu_core *core)\n{\n\tstruct vpu_dev *vpu = dev_get_drvdata(dev);\n\tint ret = 0;\n\n\tdev_dbg(core->dev, \"register core %s\\n\", vpu_core_type_desc(core->type));\n\tif (vpu_core_is_exist(vpu, core))\n\t\treturn 0;\n\n\tcore->workqueue = alloc_ordered_workqueue(\"vpu\", WQ_MEM_RECLAIM);\n\tif (!core->workqueue) {\n\t\tdev_err(core->dev, \"fail to alloc workqueue\\n\");\n\t\treturn -ENOMEM;\n\t}\n\tINIT_WORK(&core->msg_work, vpu_msg_run_work);\n\tINIT_DELAYED_WORK(&core->msg_delayed_work, vpu_msg_delayed_work);\n\tcore->msg_buffer_size = roundup_pow_of_two(VPU_MSG_BUFFER_SIZE);\n\tcore->msg_buffer = vzalloc(core->msg_buffer_size);\n\tif (!core->msg_buffer) {\n\t\tdev_err(core->dev, \"failed allocate buffer for fifo\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\tret = kfifo_init(&core->msg_fifo, core->msg_buffer, core->msg_buffer_size);\n\tif (ret) {\n\t\tdev_err(core->dev, \"failed init kfifo\\n\");\n\t\tgoto error;\n\t}\n\n\tlist_add_tail(&core->list, &vpu->cores);\n\tvpu_core_get_vpu(core);\n\n\treturn 0;\nerror:\n\tif (core->msg_buffer) {\n\t\tvfree(core->msg_buffer);\n\t\tcore->msg_buffer = NULL;\n\t}\n\tif (core->workqueue) {\n\t\tdestroy_workqueue(core->workqueue);\n\t\tcore->workqueue = NULL;\n\t}\n\treturn ret;\n}\n\nstatic void vpu_core_put_vpu(struct vpu_core *core)\n{\n\tif (core->type == VPU_CORE_TYPE_ENC)\n\t\tcore->vpu->put_enc(core->vpu);\n\tif (core->type == VPU_CORE_TYPE_DEC)\n\t\tcore->vpu->put_dec(core->vpu);\n\tcore->vpu->put_vpu(core->vpu);\n}\n\nstatic int vpu_core_unregister(struct device *dev, struct vpu_core *core)\n{\n\tlist_del_init(&core->list);\n\n\tvpu_core_put_vpu(core);\n\tcore->vpu = NULL;\n\tvfree(core->msg_buffer);\n\tcore->msg_buffer = NULL;\n\n\tif (core->workqueue) {\n\t\tcancel_work_sync(&core->msg_work);\n\t\tcancel_delayed_work_sync(&core->msg_delayed_work);\n\t\tdestroy_workqueue(core->workqueue);\n\t\tcore->workqueue = NULL;\n\t}\n\n\treturn 0;\n}\n\nstatic int vpu_core_acquire_instance(struct vpu_core *core)\n{\n\tint id;\n\n\tid = ffz(core->instance_mask);\n\tif (id >= core->supported_instance_count)\n\t\treturn -EINVAL;\n\n\tset_bit(id, &core->instance_mask);\n\n\treturn id;\n}\n\nstatic void vpu_core_release_instance(struct vpu_core *core, int id)\n{\n\tif (id < 0 || id >= core->supported_instance_count)\n\t\treturn;\n\n\tclear_bit(id, &core->instance_mask);\n}\n\nstruct vpu_inst *vpu_inst_get(struct vpu_inst *inst)\n{\n\tif (!inst)\n\t\treturn NULL;\n\n\tatomic_inc(&inst->ref_count);\n\n\treturn inst;\n}\n\nvoid vpu_inst_put(struct vpu_inst *inst)\n{\n\tif (!inst)\n\t\treturn;\n\tif (atomic_dec_and_test(&inst->ref_count)) {\n\t\tif (inst->release)\n\t\t\tinst->release(inst);\n\t}\n}\n\nstruct vpu_core *vpu_request_core(struct vpu_dev *vpu, enum vpu_core_type type)\n{\n\tstruct vpu_core *core = NULL;\n\tint ret;\n\n\tmutex_lock(&vpu->lock);\n\n\tcore = vpu_core_find_proper_by_type(vpu, type);\n\tif (!core)\n\t\tgoto exit;\n\n\tmutex_lock(&core->lock);\n\tpm_runtime_resume_and_get(core->dev);\n\n\tif (core->state == VPU_CORE_DEINIT) {\n\t\tif (vpu_iface_get_power_state(core))\n\t\t\tret = vpu_core_restore(core);\n\t\telse\n\t\t\tret = vpu_core_boot(core, true);\n\t\tif (ret) {\n\t\t\tpm_runtime_put_sync(core->dev);\n\t\t\tmutex_unlock(&core->lock);\n\t\t\tcore = NULL;\n\t\t\tgoto exit;\n\t\t}\n\t}\n\n\tcore->request_count++;\n\n\tmutex_unlock(&core->lock);\nexit:\n\tmutex_unlock(&vpu->lock);\n\n\treturn core;\n}\n\nvoid vpu_release_core(struct vpu_core *core)\n{\n\tif (!core)\n\t\treturn;\n\n\tmutex_lock(&core->lock);\n\tpm_runtime_put_sync(core->dev);\n\tif (core->request_count)\n\t\tcore->request_count--;\n\tmutex_unlock(&core->lock);\n}\n\nint vpu_inst_register(struct vpu_inst *inst)\n{\n\tstruct vpu_dev *vpu;\n\tstruct vpu_core *core;\n\tint ret = 0;\n\n\tvpu = inst->vpu;\n\tcore = inst->core;\n\tif (!core) {\n\t\tcore = vpu_request_core(vpu, inst->type);\n\t\tif (!core) {\n\t\t\tdev_err(vpu->dev, \"there is no vpu core for %s\\n\",\n\t\t\t\tvpu_core_type_desc(inst->type));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tinst->core = core;\n\t\tinst->dev = get_device(core->dev);\n\t}\n\n\tmutex_lock(&core->lock);\n\tif (core->state != VPU_CORE_ACTIVE) {\n\t\tdev_err(core->dev, \"vpu core is not active, state = %d\\n\", core->state);\n\t\tret = -EINVAL;\n\t\tgoto exit;\n\t}\n\n\tif (inst->id >= 0 && inst->id < core->supported_instance_count)\n\t\tgoto exit;\n\n\tret = vpu_core_acquire_instance(core);\n\tif (ret < 0)\n\t\tgoto exit;\n\n\tvpu_trace(inst->dev, \"[%d] %p\\n\", ret, inst);\n\tinst->id = ret;\n\tlist_add_tail(&inst->list, &core->instances);\n\tret = 0;\n\tif (core->res->act_size) {\n\t\tinst->act.phys = core->act.phys + core->res->act_size * inst->id;\n\t\tinst->act.virt = core->act.virt + core->res->act_size * inst->id;\n\t\tinst->act.length = core->res->act_size;\n\t}\n\tvpu_inst_create_dbgfs_file(inst);\nexit:\n\tmutex_unlock(&core->lock);\n\n\tif (ret)\n\t\tdev_err(core->dev, \"register instance fail\\n\");\n\treturn ret;\n}\n\nint vpu_inst_unregister(struct vpu_inst *inst)\n{\n\tstruct vpu_core *core;\n\n\tif (!inst->core)\n\t\treturn 0;\n\n\tcore = inst->core;\n\tvpu_clear_request(inst);\n\tmutex_lock(&core->lock);\n\tif (inst->id >= 0 && inst->id < core->supported_instance_count) {\n\t\tvpu_inst_remove_dbgfs_file(inst);\n\t\tlist_del_init(&inst->list);\n\t\tvpu_core_release_instance(core, inst->id);\n\t\tinst->id = VPU_INST_NULL_ID;\n\t}\n\tvpu_core_update_state(core);\n\tif (core->state == VPU_CORE_HANG && !core->instance_mask) {\n\t\tint err;\n\n\t\tdev_info(core->dev, \"reset hang core\\n\");\n\t\tmutex_unlock(&core->lock);\n\t\terr = vpu_core_sw_reset(core);\n\t\tmutex_lock(&core->lock);\n\t\tif (!err) {\n\t\t\tvpu_core_set_state(core, VPU_CORE_ACTIVE);\n\t\t\tcore->hang_mask = 0;\n\t\t}\n\t}\n\tmutex_unlock(&core->lock);\n\n\treturn 0;\n}\n\nstruct vpu_inst *vpu_core_find_instance(struct vpu_core *core, u32 index)\n{\n\tstruct vpu_inst *inst = NULL;\n\tstruct vpu_inst *tmp;\n\n\tmutex_lock(&core->lock);\n\tif (index >= core->supported_instance_count || !test_bit(index, &core->instance_mask))\n\t\tgoto exit;\n\tlist_for_each_entry(tmp, &core->instances, list) {\n\t\tif (tmp->id == index) {\n\t\t\tinst = vpu_inst_get(tmp);\n\t\t\tbreak;\n\t\t}\n\t}\nexit:\n\tmutex_unlock(&core->lock);\n\n\treturn inst;\n}\n\nconst struct vpu_core_resources *vpu_get_resource(struct vpu_inst *inst)\n{\n\tstruct vpu_dev *vpu;\n\tstruct vpu_core *core = NULL;\n\tconst struct vpu_core_resources *res = NULL;\n\n\tif (!inst || !inst->vpu)\n\t\treturn NULL;\n\n\tif (inst->core && inst->core->res)\n\t\treturn inst->core->res;\n\n\tvpu = inst->vpu;\n\tmutex_lock(&vpu->lock);\n\tlist_for_each_entry(core, &vpu->cores, list) {\n\t\tif (core->type == inst->type) {\n\t\t\tres = core->res;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&vpu->lock);\n\n\treturn res;\n}\n\nstatic int vpu_core_parse_dt(struct vpu_core *core, struct device_node *np)\n{\n\tstruct device_node *node;\n\tstruct resource res;\n\tint ret;\n\n\tif (of_count_phandle_with_args(np, \"memory-region\", NULL) < 2) {\n\t\tdev_err(core->dev, \"need 2 memory-region for boot and rpc\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tnode = of_parse_phandle(np, \"memory-region\", 0);\n\tif (!node) {\n\t\tdev_err(core->dev, \"boot-region of_parse_phandle error\\n\");\n\t\treturn -ENODEV;\n\t}\n\tif (of_address_to_resource(node, 0, &res)) {\n\t\tdev_err(core->dev, \"boot-region of_address_to_resource error\\n\");\n\t\tof_node_put(node);\n\t\treturn -EINVAL;\n\t}\n\tcore->fw.phys = res.start;\n\tcore->fw.length = resource_size(&res);\n\n\tof_node_put(node);\n\n\tnode = of_parse_phandle(np, \"memory-region\", 1);\n\tif (!node) {\n\t\tdev_err(core->dev, \"rpc-region of_parse_phandle error\\n\");\n\t\treturn -ENODEV;\n\t}\n\tif (of_address_to_resource(node, 0, &res)) {\n\t\tdev_err(core->dev, \"rpc-region of_address_to_resource error\\n\");\n\t\tof_node_put(node);\n\t\treturn -EINVAL;\n\t}\n\tcore->rpc.phys = res.start;\n\tcore->rpc.length = resource_size(&res);\n\n\tif (core->rpc.length < core->res->rpc_size + core->res->fwlog_size) {\n\t\tdev_err(core->dev, \"the rpc-region <%pad, 0x%x> is not enough\\n\",\n\t\t\t&core->rpc.phys, core->rpc.length);\n\t\tof_node_put(node);\n\t\treturn -EINVAL;\n\t}\n\n\tcore->fw.virt = memremap(core->fw.phys, core->fw.length, MEMREMAP_WC);\n\tcore->rpc.virt = memremap(core->rpc.phys, core->rpc.length, MEMREMAP_WC);\n\tmemset(core->rpc.virt, 0, core->rpc.length);\n\n\tret = vpu_iface_check_memory_region(core, core->rpc.phys, core->rpc.length);\n\tif (ret != VPU_CORE_MEMORY_UNCACHED) {\n\t\tdev_err(core->dev, \"rpc region<%pad, 0x%x> isn't uncached\\n\",\n\t\t\t&core->rpc.phys, core->rpc.length);\n\t\tof_node_put(node);\n\t\treturn -EINVAL;\n\t}\n\n\tcore->log.phys = core->rpc.phys + core->res->rpc_size;\n\tcore->log.virt = core->rpc.virt + core->res->rpc_size;\n\tcore->log.length = core->res->fwlog_size;\n\tcore->act.phys = core->log.phys + core->log.length;\n\tcore->act.virt = core->log.virt + core->log.length;\n\tcore->act.length = core->rpc.length - core->res->rpc_size - core->log.length;\n\tcore->rpc.length = core->res->rpc_size;\n\n\tof_node_put(node);\n\n\treturn 0;\n}\n\nstatic int vpu_core_probe(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct vpu_core *core;\n\tstruct vpu_dev *vpu = dev_get_drvdata(dev->parent);\n\tstruct vpu_shared_addr *iface;\n\tu32 iface_data_size;\n\tint ret;\n\n\tdev_dbg(dev, \"probe\\n\");\n\tif (!vpu)\n\t\treturn -EINVAL;\n\tcore = devm_kzalloc(dev, sizeof(*core), GFP_KERNEL);\n\tif (!core)\n\t\treturn -ENOMEM;\n\n\tcore->pdev = pdev;\n\tcore->dev = dev;\n\tplatform_set_drvdata(pdev, core);\n\tcore->vpu = vpu;\n\tINIT_LIST_HEAD(&core->instances);\n\tmutex_init(&core->lock);\n\tmutex_init(&core->cmd_lock);\n\tinit_completion(&core->cmp);\n\tinit_waitqueue_head(&core->ack_wq);\n\tvpu_core_set_state(core, VPU_CORE_DEINIT);\n\n\tcore->res = of_device_get_match_data(dev);\n\tif (!core->res)\n\t\treturn -ENODEV;\n\n\tcore->type = core->res->type;\n\tcore->id = of_alias_get_id(dev->of_node, \"vpu-core\");\n\tif (core->id < 0) {\n\t\tdev_err(dev, \"can't get vpu core id\\n\");\n\t\treturn core->id;\n\t}\n\tdev_info(core->dev, \"[%d] = %s\\n\", core->id, vpu_core_type_desc(core->type));\n\tret = vpu_core_parse_dt(core, dev->of_node);\n\tif (ret)\n\t\treturn ret;\n\n\tcore->base = devm_platform_ioremap_resource(pdev, 0);\n\tif (IS_ERR(core->base))\n\t\treturn PTR_ERR(core->base);\n\n\tif (!vpu_iface_check_codec(core)) {\n\t\tdev_err(core->dev, \"is not supported\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = vpu_mbox_init(core);\n\tif (ret)\n\t\treturn ret;\n\n\tiface = devm_kzalloc(dev, sizeof(*iface), GFP_KERNEL);\n\tif (!iface)\n\t\treturn -ENOMEM;\n\n\tiface_data_size = vpu_iface_get_data_size(core);\n\tif (iface_data_size) {\n\t\tiface->priv = devm_kzalloc(dev, iface_data_size, GFP_KERNEL);\n\t\tif (!iface->priv)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tret = vpu_iface_init(core, iface, &core->rpc, core->fw.phys);\n\tif (ret) {\n\t\tdev_err(core->dev, \"init iface fail, ret = %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tvpu_iface_config_system(core, vpu->res->mreg_base, vpu->base);\n\tvpu_iface_set_log_buf(core, &core->log);\n\n\tpm_runtime_enable(dev);\n\tret = pm_runtime_resume_and_get(dev);\n\tif (ret) {\n\t\tpm_runtime_put_noidle(dev);\n\t\tpm_runtime_set_suspended(dev);\n\t\tgoto err_runtime_disable;\n\t}\n\n\tret = vpu_core_register(dev->parent, core);\n\tif (ret)\n\t\tgoto err_core_register;\n\tcore->parent = dev->parent;\n\n\tpm_runtime_put_sync(dev);\n\tvpu_core_create_dbgfs_file(core);\n\n\treturn 0;\n\nerr_core_register:\n\tpm_runtime_put_sync(dev);\nerr_runtime_disable:\n\tpm_runtime_disable(dev);\n\n\treturn ret;\n}\n\nstatic void vpu_core_remove(struct platform_device *pdev)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct vpu_core *core = platform_get_drvdata(pdev);\n\tint ret;\n\n\tvpu_core_remove_dbgfs_file(core);\n\tret = pm_runtime_resume_and_get(dev);\n\tWARN_ON(ret < 0);\n\n\tvpu_core_shutdown(core);\n\tpm_runtime_put_sync(dev);\n\tpm_runtime_disable(dev);\n\n\tvpu_core_unregister(core->parent, core);\n\tmemunmap(core->fw.virt);\n\tmemunmap(core->rpc.virt);\n\tmutex_destroy(&core->lock);\n\tmutex_destroy(&core->cmd_lock);\n}\n\nstatic int __maybe_unused vpu_core_runtime_resume(struct device *dev)\n{\n\tstruct vpu_core *core = dev_get_drvdata(dev);\n\n\treturn vpu_mbox_request(core);\n}\n\nstatic int __maybe_unused vpu_core_runtime_suspend(struct device *dev)\n{\n\tstruct vpu_core *core = dev_get_drvdata(dev);\n\n\tvpu_mbox_free(core);\n\treturn 0;\n}\n\nstatic void vpu_core_cancel_work(struct vpu_core *core)\n{\n\tstruct vpu_inst *inst = NULL;\n\n\tcancel_work_sync(&core->msg_work);\n\tcancel_delayed_work_sync(&core->msg_delayed_work);\n\n\tmutex_lock(&core->lock);\n\tlist_for_each_entry(inst, &core->instances, list)\n\t\tcancel_work_sync(&inst->msg_work);\n\tmutex_unlock(&core->lock);\n}\n\nstatic void vpu_core_resume_work(struct vpu_core *core)\n{\n\tstruct vpu_inst *inst = NULL;\n\tunsigned long delay = msecs_to_jiffies(10);\n\n\tqueue_work(core->workqueue, &core->msg_work);\n\tqueue_delayed_work(core->workqueue, &core->msg_delayed_work, delay);\n\n\tmutex_lock(&core->lock);\n\tlist_for_each_entry(inst, &core->instances, list)\n\t\tqueue_work(inst->workqueue, &inst->msg_work);\n\tmutex_unlock(&core->lock);\n}\n\nstatic int __maybe_unused vpu_core_resume(struct device *dev)\n{\n\tstruct vpu_core *core = dev_get_drvdata(dev);\n\tint ret = 0;\n\n\tmutex_lock(&core->lock);\n\tpm_runtime_resume_and_get(dev);\n\tvpu_core_get_vpu(core);\n\n\tif (core->request_count) {\n\t\tif (!vpu_iface_get_power_state(core))\n\t\t\tret = vpu_core_boot(core, false);\n\t\telse\n\t\t\tret = vpu_core_sw_reset(core);\n\t\tif (ret) {\n\t\t\tdev_err(core->dev, \"resume fail\\n\");\n\t\t\tvpu_core_set_state(core, VPU_CORE_HANG);\n\t\t}\n\t}\n\tvpu_core_update_state(core);\n\tpm_runtime_put_sync(dev);\n\tmutex_unlock(&core->lock);\n\n\tvpu_core_resume_work(core);\n\treturn ret;\n}\n\nstatic int __maybe_unused vpu_core_suspend(struct device *dev)\n{\n\tstruct vpu_core *core = dev_get_drvdata(dev);\n\tint ret = 0;\n\n\tmutex_lock(&core->lock);\n\tif (core->request_count)\n\t\tret = vpu_core_snapshot(core);\n\tmutex_unlock(&core->lock);\n\tif (ret)\n\t\treturn ret;\n\n\tvpu_core_cancel_work(core);\n\n\tmutex_lock(&core->lock);\n\tvpu_core_put_vpu(core);\n\tmutex_unlock(&core->lock);\n\treturn ret;\n}\n\nstatic const struct dev_pm_ops vpu_core_pm_ops = {\n\tSET_RUNTIME_PM_OPS(vpu_core_runtime_suspend, vpu_core_runtime_resume, NULL)\n\tSET_SYSTEM_SLEEP_PM_OPS(vpu_core_suspend, vpu_core_resume)\n};\n\nstatic struct vpu_core_resources imx8q_enc = {\n\t.type = VPU_CORE_TYPE_ENC,\n\t.fwname = \"amphion/vpu/vpu_fw_imx8_enc.bin\",\n\t.stride = 16,\n\t.max_width = 1920,\n\t.max_height = 1920,\n\t.min_width = 64,\n\t.min_height = 48,\n\t.step_width = 2,\n\t.step_height = 2,\n\t.rpc_size = 0x80000,\n\t.fwlog_size = 0x80000,\n\t.act_size = 0xc0000,\n};\n\nstatic struct vpu_core_resources imx8q_dec = {\n\t.type = VPU_CORE_TYPE_DEC,\n\t.fwname = \"amphion/vpu/vpu_fw_imx8_dec.bin\",\n\t.stride = 256,\n\t.max_width = 8188,\n\t.max_height = 8188,\n\t.min_width = 16,\n\t.min_height = 16,\n\t.step_width = 1,\n\t.step_height = 1,\n\t.rpc_size = 0x80000,\n\t.fwlog_size = 0x80000,\n};\n\nstatic const struct of_device_id vpu_core_dt_match[] = {\n\t{ .compatible = \"nxp,imx8q-vpu-encoder\", .data = &imx8q_enc },\n\t{ .compatible = \"nxp,imx8q-vpu-decoder\", .data = &imx8q_dec },\n\t{}\n};\nMODULE_DEVICE_TABLE(of, vpu_core_dt_match);\n\nstatic struct platform_driver amphion_vpu_core_driver = {\n\t.probe = vpu_core_probe,\n\t.remove_new = vpu_core_remove,\n\t.driver = {\n\t\t.name = \"amphion-vpu-core\",\n\t\t.of_match_table = vpu_core_dt_match,\n\t\t.pm = &vpu_core_pm_ops,\n\t},\n};\n\nint __init vpu_core_driver_init(void)\n{\n\treturn platform_driver_register(&amphion_vpu_core_driver);\n}\n\nvoid __exit vpu_core_driver_exit(void)\n{\n\tplatform_driver_unregister(&amphion_vpu_core_driver);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}