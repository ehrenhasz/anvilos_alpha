{
  "module_name": "ipu3-cio2.c",
  "hash_id": "05a68fdb1e3da3cd8c2106f3a747b35792048ef782ea3540b2cba41213a98893",
  "original_prompt": "Ingested from linux-6.6.14/drivers/media/pci/intel/ipu3/ipu3-cio2.c",
  "human_readable_source": "\n \n\n#include <linux/bitops.h>\n#include <linux/delay.h>\n#include <linux/interrupt.h>\n#include <linux/iopoll.h>\n#include <linux/mm.h>\n#include <linux/module.h>\n#include <linux/pci.h>\n#include <linux/pfn.h>\n#include <linux/pm_runtime.h>\n#include <linux/property.h>\n#include <linux/vmalloc.h>\n\n#include <media/ipu-bridge.h>\n#include <media/v4l2-ctrls.h>\n#include <media/v4l2-device.h>\n#include <media/v4l2-event.h>\n#include <media/v4l2-fwnode.h>\n#include <media/v4l2-ioctl.h>\n#include <media/videobuf2-dma-sg.h>\n\n#include \"ipu3-cio2.h\"\n\nstruct ipu3_cio2_fmt {\n\tu32 mbus_code;\n\tu32 fourcc;\n\tu8 mipicode;\n\tu8 bpp;\n};\n\n \nstatic const struct ipu3_cio2_fmt formats[] = {\n\t{\t \n\t\t.mbus_code\t= MEDIA_BUS_FMT_SGRBG10_1X10,\n\t\t.fourcc\t\t= V4L2_PIX_FMT_IPU3_SGRBG10,\n\t\t.mipicode\t= 0x2b,\n\t\t.bpp\t\t= 10,\n\t}, {\n\t\t.mbus_code\t= MEDIA_BUS_FMT_SGBRG10_1X10,\n\t\t.fourcc\t\t= V4L2_PIX_FMT_IPU3_SGBRG10,\n\t\t.mipicode\t= 0x2b,\n\t\t.bpp\t\t= 10,\n\t}, {\n\t\t.mbus_code\t= MEDIA_BUS_FMT_SBGGR10_1X10,\n\t\t.fourcc\t\t= V4L2_PIX_FMT_IPU3_SBGGR10,\n\t\t.mipicode\t= 0x2b,\n\t\t.bpp\t\t= 10,\n\t}, {\n\t\t.mbus_code\t= MEDIA_BUS_FMT_SRGGB10_1X10,\n\t\t.fourcc\t\t= V4L2_PIX_FMT_IPU3_SRGGB10,\n\t\t.mipicode\t= 0x2b,\n\t\t.bpp\t\t= 10,\n\t}, {\n\t\t.mbus_code\t= MEDIA_BUS_FMT_Y10_1X10,\n\t\t.fourcc\t\t= V4L2_PIX_FMT_IPU3_Y10,\n\t\t.mipicode\t= 0x2b,\n\t\t.bpp\t\t= 10,\n\t},\n};\n\n \nstatic const struct ipu3_cio2_fmt *cio2_find_format(const u32 *pixelformat,\n\t\t\t\t\t\t    const u32 *mbus_code)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ARRAY_SIZE(formats); i++) {\n\t\tif (pixelformat && *pixelformat != formats[i].fourcc)\n\t\t\tcontinue;\n\t\tif (mbus_code && *mbus_code != formats[i].mbus_code)\n\t\t\tcontinue;\n\n\t\treturn &formats[i];\n\t}\n\n\treturn NULL;\n}\n\nstatic inline u32 cio2_bytesperline(const unsigned int width)\n{\n\t \n\treturn DIV_ROUND_UP(width, 50) * 64;\n}\n\n \n\nstatic void cio2_fbpt_exit_dummy(struct cio2_device *cio2)\n{\n\tstruct device *dev = &cio2->pci_dev->dev;\n\n\tif (cio2->dummy_lop) {\n\t\tdma_free_coherent(dev, PAGE_SIZE, cio2->dummy_lop,\n\t\t\t\t  cio2->dummy_lop_bus_addr);\n\t\tcio2->dummy_lop = NULL;\n\t}\n\tif (cio2->dummy_page) {\n\t\tdma_free_coherent(dev, PAGE_SIZE, cio2->dummy_page,\n\t\t\t\t  cio2->dummy_page_bus_addr);\n\t\tcio2->dummy_page = NULL;\n\t}\n}\n\nstatic int cio2_fbpt_init_dummy(struct cio2_device *cio2)\n{\n\tstruct device *dev = &cio2->pci_dev->dev;\n\tunsigned int i;\n\n\tcio2->dummy_page = dma_alloc_coherent(dev, PAGE_SIZE,\n\t\t\t\t\t      &cio2->dummy_page_bus_addr,\n\t\t\t\t\t      GFP_KERNEL);\n\tcio2->dummy_lop = dma_alloc_coherent(dev, PAGE_SIZE,\n\t\t\t\t\t     &cio2->dummy_lop_bus_addr,\n\t\t\t\t\t     GFP_KERNEL);\n\tif (!cio2->dummy_page || !cio2->dummy_lop) {\n\t\tcio2_fbpt_exit_dummy(cio2);\n\t\treturn -ENOMEM;\n\t}\n\t \n\tfor (i = 0; i < CIO2_LOP_ENTRIES; i++)\n\t\tcio2->dummy_lop[i] = PFN_DOWN(cio2->dummy_page_bus_addr);\n\n\treturn 0;\n}\n\nstatic void cio2_fbpt_entry_enable(struct cio2_device *cio2,\n\t\t\t\t   struct cio2_fbpt_entry entry[CIO2_MAX_LOPS])\n{\n\t \n\tdma_wmb();\n\n\t \n\tentry[0].first_entry.ctrl = CIO2_FBPT_CTRL_VALID |\n\t\tCIO2_FBPT_CTRL_IOC | CIO2_FBPT_CTRL_IOS;\n}\n\n \nstatic void cio2_fbpt_entry_init_dummy(struct cio2_device *cio2,\n\t\t\t\t       struct cio2_fbpt_entry\n\t\t\t\t       entry[CIO2_MAX_LOPS])\n{\n\tunsigned int i;\n\n\tentry[0].first_entry.first_page_offset = 0;\n\tentry[1].second_entry.num_of_pages = CIO2_LOP_ENTRIES * CIO2_MAX_LOPS;\n\tentry[1].second_entry.last_page_available_bytes = PAGE_SIZE - 1;\n\n\tfor (i = 0; i < CIO2_MAX_LOPS; i++)\n\t\tentry[i].lop_page_addr = PFN_DOWN(cio2->dummy_lop_bus_addr);\n\n\tcio2_fbpt_entry_enable(cio2, entry);\n}\n\n \nstatic void cio2_fbpt_entry_init_buf(struct cio2_device *cio2,\n\t\t\t\t     struct cio2_buffer *b,\n\t\t\t\t     struct cio2_fbpt_entry\n\t\t\t\t     entry[CIO2_MAX_LOPS])\n{\n\tstruct vb2_buffer *vb = &b->vbb.vb2_buf;\n\tunsigned int length = vb->planes[0].length;\n\tint remaining, i;\n\n\tentry[0].first_entry.first_page_offset = b->offset;\n\tremaining = length + entry[0].first_entry.first_page_offset;\n\tentry[1].second_entry.num_of_pages = PFN_UP(remaining);\n\t \n\tremaining = offset_in_page(remaining) ?: PAGE_SIZE;\n\tentry[1].second_entry.last_page_available_bytes = remaining - 1;\n\t \n\tremaining = length;\n\ti = 0;\n\twhile (remaining > 0) {\n\t\tentry->lop_page_addr = PFN_DOWN(b->lop_bus_addr[i]);\n\t\tremaining -= CIO2_LOP_ENTRIES * PAGE_SIZE;\n\t\tentry++;\n\t\ti++;\n\t}\n\n\t \n\tentry->lop_page_addr = PFN_DOWN(cio2->dummy_lop_bus_addr);\n\n\tcio2_fbpt_entry_enable(cio2, entry);\n}\n\nstatic int cio2_fbpt_init(struct cio2_device *cio2, struct cio2_queue *q)\n{\n\tstruct device *dev = &cio2->pci_dev->dev;\n\n\tq->fbpt = dma_alloc_coherent(dev, CIO2_FBPT_SIZE, &q->fbpt_bus_addr,\n\t\t\t\t     GFP_KERNEL);\n\tif (!q->fbpt)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\nstatic void cio2_fbpt_exit(struct cio2_queue *q, struct device *dev)\n{\n\tdma_free_coherent(dev, CIO2_FBPT_SIZE, q->fbpt, q->fbpt_bus_addr);\n}\n\n \n\n \n\n \n#define LIMIT_SHIFT\t8\n\nstatic s32 cio2_rx_timing(s32 a, s32 b, s64 freq, int def)\n{\n\tconst u32 accinv = 16;  \n\tconst u32 uiinv = 500000000;  \n\ts32 r;\n\n\tfreq >>= LIMIT_SHIFT;\n\n\tif (WARN_ON(freq <= 0 || freq > S32_MAX))\n\t\treturn def;\n\t \n\tr = accinv * b * (uiinv >> LIMIT_SHIFT);\n\tr = r / (s32)freq;\n\t \n\tr += accinv * a;\n\n\treturn r;\n};\n\n \nstatic int cio2_csi2_calc_timing(struct cio2_device *cio2, struct cio2_queue *q,\n\t\t\t\t struct cio2_csi2_timing *timing,\n\t\t\t\t unsigned int bpp, unsigned int lanes)\n{\n\tstruct device *dev = &cio2->pci_dev->dev;\n\ts64 freq;\n\n\tif (!q->sensor)\n\t\treturn -ENODEV;\n\n\tfreq = v4l2_get_link_freq(q->sensor->ctrl_handler, bpp, lanes * 2);\n\tif (freq < 0) {\n\t\tdev_err(dev, \"error %lld, invalid link_freq\\n\", freq);\n\t\treturn freq;\n\t}\n\n\ttiming->clk_termen = cio2_rx_timing(CIO2_CSIRX_DLY_CNT_TERMEN_CLANE_A,\n\t\t\t\t\t    CIO2_CSIRX_DLY_CNT_TERMEN_CLANE_B,\n\t\t\t\t\t    freq,\n\t\t\t\t\t    CIO2_CSIRX_DLY_CNT_TERMEN_DEFAULT);\n\ttiming->clk_settle = cio2_rx_timing(CIO2_CSIRX_DLY_CNT_SETTLE_CLANE_A,\n\t\t\t\t\t    CIO2_CSIRX_DLY_CNT_SETTLE_CLANE_B,\n\t\t\t\t\t    freq,\n\t\t\t\t\t    CIO2_CSIRX_DLY_CNT_SETTLE_DEFAULT);\n\ttiming->dat_termen = cio2_rx_timing(CIO2_CSIRX_DLY_CNT_TERMEN_DLANE_A,\n\t\t\t\t\t    CIO2_CSIRX_DLY_CNT_TERMEN_DLANE_B,\n\t\t\t\t\t    freq,\n\t\t\t\t\t    CIO2_CSIRX_DLY_CNT_TERMEN_DEFAULT);\n\ttiming->dat_settle = cio2_rx_timing(CIO2_CSIRX_DLY_CNT_SETTLE_DLANE_A,\n\t\t\t\t\t    CIO2_CSIRX_DLY_CNT_SETTLE_DLANE_B,\n\t\t\t\t\t    freq,\n\t\t\t\t\t    CIO2_CSIRX_DLY_CNT_SETTLE_DEFAULT);\n\n\tdev_dbg(dev, \"freq ct value is %d\\n\", timing->clk_termen);\n\tdev_dbg(dev, \"freq cs value is %d\\n\", timing->clk_settle);\n\tdev_dbg(dev, \"freq dt value is %d\\n\", timing->dat_termen);\n\tdev_dbg(dev, \"freq ds value is %d\\n\", timing->dat_settle);\n\n\treturn 0;\n};\n\nstatic int cio2_hw_init(struct cio2_device *cio2, struct cio2_queue *q)\n{\n\tstatic const int NUM_VCS = 4;\n\tstatic const int SID;\t \n\tstatic const int ENTRY;\n\tstatic const int FBPT_WIDTH = DIV_ROUND_UP(CIO2_MAX_LOPS,\n\t\t\t\t\tCIO2_FBPT_SUBENTRY_UNIT);\n\tconst u32 num_buffers1 = CIO2_MAX_BUFFERS - 1;\n\tconst struct ipu3_cio2_fmt *fmt;\n\tvoid __iomem *const base = cio2->base;\n\tu8 lanes, csi2bus = q->csi2.port;\n\tu8 sensor_vc = SENSOR_VIR_CH_DFLT;\n\tstruct cio2_csi2_timing timing = { 0 };\n\tint i, r;\n\n\tfmt = cio2_find_format(NULL, &q->subdev_fmt.code);\n\tif (!fmt)\n\t\treturn -EINVAL;\n\n\tlanes = q->csi2.lanes;\n\n\tr = cio2_csi2_calc_timing(cio2, q, &timing, fmt->bpp, lanes);\n\tif (r)\n\t\treturn r;\n\n\twritel(timing.clk_termen, q->csi_rx_base +\n\t\tCIO2_REG_CSIRX_DLY_CNT_TERMEN(CIO2_CSIRX_DLY_CNT_CLANE_IDX));\n\twritel(timing.clk_settle, q->csi_rx_base +\n\t\tCIO2_REG_CSIRX_DLY_CNT_SETTLE(CIO2_CSIRX_DLY_CNT_CLANE_IDX));\n\n\tfor (i = 0; i < lanes; i++) {\n\t\twritel(timing.dat_termen, q->csi_rx_base +\n\t\t\tCIO2_REG_CSIRX_DLY_CNT_TERMEN(i));\n\t\twritel(timing.dat_settle, q->csi_rx_base +\n\t\t\tCIO2_REG_CSIRX_DLY_CNT_SETTLE(i));\n\t}\n\n\twritel(CIO2_PBM_WMCTRL1_MIN_2CK |\n\t       CIO2_PBM_WMCTRL1_MID1_2CK |\n\t       CIO2_PBM_WMCTRL1_MID2_2CK, base + CIO2_REG_PBM_WMCTRL1);\n\twritel(CIO2_PBM_WMCTRL2_HWM_2CK << CIO2_PBM_WMCTRL2_HWM_2CK_SHIFT |\n\t       CIO2_PBM_WMCTRL2_LWM_2CK << CIO2_PBM_WMCTRL2_LWM_2CK_SHIFT |\n\t       CIO2_PBM_WMCTRL2_OBFFWM_2CK <<\n\t       CIO2_PBM_WMCTRL2_OBFFWM_2CK_SHIFT |\n\t       CIO2_PBM_WMCTRL2_TRANSDYN << CIO2_PBM_WMCTRL2_TRANSDYN_SHIFT |\n\t       CIO2_PBM_WMCTRL2_OBFF_MEM_EN, base + CIO2_REG_PBM_WMCTRL2);\n\twritel(CIO2_PBM_ARB_CTRL_LANES_DIV <<\n\t       CIO2_PBM_ARB_CTRL_LANES_DIV_SHIFT |\n\t       CIO2_PBM_ARB_CTRL_LE_EN |\n\t       CIO2_PBM_ARB_CTRL_PLL_POST_SHTDN <<\n\t       CIO2_PBM_ARB_CTRL_PLL_POST_SHTDN_SHIFT |\n\t       CIO2_PBM_ARB_CTRL_PLL_AHD_WK_UP <<\n\t       CIO2_PBM_ARB_CTRL_PLL_AHD_WK_UP_SHIFT,\n\t       base + CIO2_REG_PBM_ARB_CTRL);\n\twritel(CIO2_CSIRX_STATUS_DLANE_HS_MASK,\n\t       q->csi_rx_base + CIO2_REG_CSIRX_STATUS_DLANE_HS);\n\twritel(CIO2_CSIRX_STATUS_DLANE_LP_MASK,\n\t       q->csi_rx_base + CIO2_REG_CSIRX_STATUS_DLANE_LP);\n\n\twritel(CIO2_FB_HPLL_FREQ, base + CIO2_REG_FB_HPLL_FREQ);\n\twritel(CIO2_ISCLK_RATIO, base + CIO2_REG_ISCLK_RATIO);\n\n\t \n\tfor (i = 0; i < NUM_VCS; i++)\n\t\twritel(1, q->csi_rx_base + CIO2_REG_MIPIBE_SP_LUT_ENTRY(i));\n\n\t \n\tfor (i = 0; i < 16; i++)\n\t\twritel(CIO2_MIPIBE_LP_LUT_ENTRY_DISREGARD,\n\t\t       q->csi_rx_base + CIO2_REG_MIPIBE_LP_LUT_ENTRY(i));\n\twritel(CIO2_MIPIBE_GLOBAL_LUT_DISREGARD,\n\t       q->csi_rx_base + CIO2_REG_MIPIBE_GLOBAL_LUT_DISREGARD);\n\n\twritel(CIO2_INT_EN_EXT_IE_MASK, base + CIO2_REG_INT_EN_EXT_IE);\n\twritel(CIO2_IRQCTRL_MASK, q->csi_rx_base + CIO2_REG_IRQCTRL_MASK);\n\twritel(CIO2_IRQCTRL_MASK, q->csi_rx_base + CIO2_REG_IRQCTRL_ENABLE);\n\twritel(0, q->csi_rx_base + CIO2_REG_IRQCTRL_EDGE);\n\twritel(0, q->csi_rx_base + CIO2_REG_IRQCTRL_LEVEL_NOT_PULSE);\n\twritel(CIO2_INT_EN_EXT_OE_MASK, base + CIO2_REG_INT_EN_EXT_OE);\n\n\twritel(CIO2_REG_INT_EN_IRQ | CIO2_INT_IOC(CIO2_DMA_CHAN) |\n\t       CIO2_REG_INT_EN_IOS(CIO2_DMA_CHAN),\n\t       base + CIO2_REG_INT_EN);\n\n\twritel((CIO2_PXM_PXF_FMT_CFG_BPP_10 | CIO2_PXM_PXF_FMT_CFG_PCK_64B)\n\t       << CIO2_PXM_PXF_FMT_CFG_SID0_SHIFT,\n\t       base + CIO2_REG_PXM_PXF_FMT_CFG0(csi2bus));\n\twritel(SID << CIO2_MIPIBE_LP_LUT_ENTRY_SID_SHIFT |\n\t       sensor_vc << CIO2_MIPIBE_LP_LUT_ENTRY_VC_SHIFT |\n\t       fmt->mipicode << CIO2_MIPIBE_LP_LUT_ENTRY_FORMAT_TYPE_SHIFT,\n\t       q->csi_rx_base + CIO2_REG_MIPIBE_LP_LUT_ENTRY(ENTRY));\n\twritel(0, q->csi_rx_base + CIO2_REG_MIPIBE_COMP_FORMAT(sensor_vc));\n\twritel(0, q->csi_rx_base + CIO2_REG_MIPIBE_FORCE_RAW8);\n\twritel(0, base + CIO2_REG_PXM_SID2BID0(csi2bus));\n\n\twritel(lanes, q->csi_rx_base + CIO2_REG_CSIRX_NOF_ENABLED_LANES);\n\twritel(CIO2_CGC_PRIM_TGE |\n\t       CIO2_CGC_SIDE_TGE |\n\t       CIO2_CGC_XOSC_TGE |\n\t       CIO2_CGC_D3I3_TGE |\n\t       CIO2_CGC_CSI2_INTERFRAME_TGE |\n\t       CIO2_CGC_CSI2_PORT_DCGE |\n\t       CIO2_CGC_SIDE_DCGE |\n\t       CIO2_CGC_PRIM_DCGE |\n\t       CIO2_CGC_ROSC_DCGE |\n\t       CIO2_CGC_XOSC_DCGE |\n\t       CIO2_CGC_CLKGATE_HOLDOFF << CIO2_CGC_CLKGATE_HOLDOFF_SHIFT |\n\t       CIO2_CGC_CSI_CLKGATE_HOLDOFF\n\t       << CIO2_CGC_CSI_CLKGATE_HOLDOFF_SHIFT, base + CIO2_REG_CGC);\n\twritel(CIO2_LTRCTRL_LTRDYNEN, base + CIO2_REG_LTRCTRL);\n\twritel(CIO2_LTRVAL0_VAL << CIO2_LTRVAL02_VAL_SHIFT |\n\t       CIO2_LTRVAL0_SCALE << CIO2_LTRVAL02_SCALE_SHIFT |\n\t       CIO2_LTRVAL1_VAL << CIO2_LTRVAL13_VAL_SHIFT |\n\t       CIO2_LTRVAL1_SCALE << CIO2_LTRVAL13_SCALE_SHIFT,\n\t       base + CIO2_REG_LTRVAL01);\n\twritel(CIO2_LTRVAL2_VAL << CIO2_LTRVAL02_VAL_SHIFT |\n\t       CIO2_LTRVAL2_SCALE << CIO2_LTRVAL02_SCALE_SHIFT |\n\t       CIO2_LTRVAL3_VAL << CIO2_LTRVAL13_VAL_SHIFT |\n\t       CIO2_LTRVAL3_SCALE << CIO2_LTRVAL13_SCALE_SHIFT,\n\t       base + CIO2_REG_LTRVAL23);\n\n\tfor (i = 0; i < CIO2_NUM_DMA_CHAN; i++) {\n\t\twritel(0, base + CIO2_REG_CDMABA(i));\n\t\twritel(0, base + CIO2_REG_CDMAC0(i));\n\t\twritel(0, base + CIO2_REG_CDMAC1(i));\n\t}\n\n\t \n\twritel(PFN_DOWN(q->fbpt_bus_addr), base + CIO2_REG_CDMABA(CIO2_DMA_CHAN));\n\n\twritel(num_buffers1 << CIO2_CDMAC0_FBPT_LEN_SHIFT |\n\t       FBPT_WIDTH << CIO2_CDMAC0_FBPT_WIDTH_SHIFT |\n\t       CIO2_CDMAC0_DMA_INTR_ON_FE |\n\t       CIO2_CDMAC0_FBPT_UPDATE_FIFO_FULL |\n\t       CIO2_CDMAC0_DMA_EN |\n\t       CIO2_CDMAC0_DMA_INTR_ON_FS |\n\t       CIO2_CDMAC0_DMA_HALTED, base + CIO2_REG_CDMAC0(CIO2_DMA_CHAN));\n\n\twritel(1 << CIO2_CDMAC1_LINENUMUPDATE_SHIFT,\n\t       base + CIO2_REG_CDMAC1(CIO2_DMA_CHAN));\n\n\twritel(0, base + CIO2_REG_PBM_FOPN_ABORT);\n\n\twritel(CIO2_PXM_FRF_CFG_CRC_TH << CIO2_PXM_FRF_CFG_CRC_TH_SHIFT |\n\t       CIO2_PXM_FRF_CFG_MSK_ECC_DPHY_NR |\n\t       CIO2_PXM_FRF_CFG_MSK_ECC_RE |\n\t       CIO2_PXM_FRF_CFG_MSK_ECC_DPHY_NE,\n\t       base + CIO2_REG_PXM_FRF_CFG(q->csi2.port));\n\n\t \n\twritel(CIO2_IRQCTRL_MASK, q->csi_rx_base + CIO2_REG_IRQCTRL_CLEAR);\n\twritel(~0, base + CIO2_REG_INT_STS_EXT_OE);\n\twritel(~0, base + CIO2_REG_INT_STS_EXT_IE);\n\twritel(~0, base + CIO2_REG_INT_STS);\n\n\t \n\twritel(1, q->csi_rx_base + CIO2_REG_MIPIBE_ENABLE);\n\twritel(1, q->csi_rx_base + CIO2_REG_CSIRX_ENABLE);\n\n\treturn 0;\n}\n\nstatic void cio2_hw_exit(struct cio2_device *cio2, struct cio2_queue *q)\n{\n\tstruct device *dev = &cio2->pci_dev->dev;\n\tvoid __iomem *const base = cio2->base;\n\tunsigned int i;\n\tu32 value;\n\tint ret;\n\n\t \n\twritel(0, q->csi_rx_base + CIO2_REG_IRQCTRL_MASK);\n\twritel(0, q->csi_rx_base + CIO2_REG_IRQCTRL_ENABLE);\n\twritel(0, q->csi_rx_base + CIO2_REG_CSIRX_ENABLE);\n\twritel(0, q->csi_rx_base + CIO2_REG_MIPIBE_ENABLE);\n\n\t \n\twritel(0, base + CIO2_REG_CDMAC0(CIO2_DMA_CHAN));\n\tret = readl_poll_timeout(base + CIO2_REG_CDMAC0(CIO2_DMA_CHAN),\n\t\t\t\t value, value & CIO2_CDMAC0_DMA_HALTED,\n\t\t\t\t 4000, 2000000);\n\tif (ret)\n\t\tdev_err(dev, \"DMA %i can not be halted\\n\", CIO2_DMA_CHAN);\n\n\tfor (i = 0; i < CIO2_NUM_PORTS; i++) {\n\t\twritel(readl(base + CIO2_REG_PXM_FRF_CFG(i)) |\n\t\t       CIO2_PXM_FRF_CFG_ABORT, base + CIO2_REG_PXM_FRF_CFG(i));\n\t\twritel(readl(base + CIO2_REG_PBM_FOPN_ABORT) |\n\t\t       CIO2_PBM_FOPN_ABORT(i), base + CIO2_REG_PBM_FOPN_ABORT);\n\t}\n}\n\nstatic void cio2_buffer_done(struct cio2_device *cio2, unsigned int dma_chan)\n{\n\tstruct device *dev = &cio2->pci_dev->dev;\n\tstruct cio2_queue *q = cio2->cur_queue;\n\tstruct cio2_fbpt_entry *entry;\n\tu64 ns = ktime_get_ns();\n\n\tif (dma_chan >= CIO2_QUEUES) {\n\t\tdev_err(dev, \"bad DMA channel %i\\n\", dma_chan);\n\t\treturn;\n\t}\n\n\tentry = &q->fbpt[q->bufs_first * CIO2_MAX_LOPS];\n\tif (entry->first_entry.ctrl & CIO2_FBPT_CTRL_VALID) {\n\t\tdev_warn(dev, \"no ready buffers found on DMA channel %u\\n\",\n\t\t\t dma_chan);\n\t\treturn;\n\t}\n\n\t \n\tdo {\n\t\tstruct cio2_buffer *b;\n\n\t\tb = q->bufs[q->bufs_first];\n\t\tif (b) {\n\t\t\tunsigned int received = entry[1].second_entry.num_of_bytes;\n\t\t\tunsigned long payload =\n\t\t\t\tvb2_get_plane_payload(&b->vbb.vb2_buf, 0);\n\n\t\t\tq->bufs[q->bufs_first] = NULL;\n\t\t\tatomic_dec(&q->bufs_queued);\n\t\t\tdev_dbg(dev, \"buffer %i done\\n\", b->vbb.vb2_buf.index);\n\n\t\t\tb->vbb.vb2_buf.timestamp = ns;\n\t\t\tb->vbb.field = V4L2_FIELD_NONE;\n\t\t\tb->vbb.sequence = atomic_read(&q->frame_sequence);\n\t\t\tif (payload != received)\n\t\t\t\tdev_warn(dev,\n\t\t\t\t\t \"payload length is %lu, received %u\\n\",\n\t\t\t\t\t payload, received);\n\t\t\tvb2_buffer_done(&b->vbb.vb2_buf, VB2_BUF_STATE_DONE);\n\t\t}\n\t\tatomic_inc(&q->frame_sequence);\n\t\tcio2_fbpt_entry_init_dummy(cio2, entry);\n\t\tq->bufs_first = (q->bufs_first + 1) % CIO2_MAX_BUFFERS;\n\t\tentry = &q->fbpt[q->bufs_first * CIO2_MAX_LOPS];\n\t} while (!(entry->first_entry.ctrl & CIO2_FBPT_CTRL_VALID));\n}\n\nstatic void cio2_queue_event_sof(struct cio2_device *cio2, struct cio2_queue *q)\n{\n\t \n\tstruct v4l2_event event = {\n\t\t.type = V4L2_EVENT_FRAME_SYNC,\n\t\t.u.frame_sync.frame_sequence = atomic_read(&q->frame_sequence),\n\t};\n\n\tv4l2_event_queue(q->subdev.devnode, &event);\n}\n\nstatic const char *const cio2_irq_errs[] = {\n\t\"single packet header error corrected\",\n\t\"multiple packet header errors detected\",\n\t\"payload checksum (CRC) error\",\n\t\"fifo overflow\",\n\t\"reserved short packet data type detected\",\n\t\"reserved long packet data type detected\",\n\t\"incomplete long packet detected\",\n\t\"frame sync error\",\n\t\"line sync error\",\n\t\"DPHY start of transmission error\",\n\t\"DPHY synchronization error\",\n\t\"escape mode error\",\n\t\"escape mode trigger event\",\n\t\"escape mode ultra-low power state for data lane(s)\",\n\t\"escape mode ultra-low power state exit for clock lane\",\n\t\"inter-frame short packet discarded\",\n\t\"inter-frame long packet discarded\",\n\t\"non-matching Long Packet stalled\",\n};\n\nstatic void cio2_irq_log_irq_errs(struct device *dev, u8 port, u32 status)\n{\n\tunsigned long csi2_status = status;\n\tunsigned int i;\n\n\tfor_each_set_bit(i, &csi2_status, ARRAY_SIZE(cio2_irq_errs))\n\t\tdev_err(dev, \"CSI-2 receiver port %i: %s\\n\",\n\t\t\tport, cio2_irq_errs[i]);\n\n\tif (fls_long(csi2_status) >= ARRAY_SIZE(cio2_irq_errs))\n\t\tdev_warn(dev, \"unknown CSI2 error 0x%lx on port %i\\n\",\n\t\t\t csi2_status, port);\n}\n\nstatic const char *const cio2_port_errs[] = {\n\t\"ECC recoverable\",\n\t\"DPHY not recoverable\",\n\t\"ECC not recoverable\",\n\t\"CRC error\",\n\t\"INTERFRAMEDATA\",\n\t\"PKT2SHORT\",\n\t\"PKT2LONG\",\n};\n\nstatic void cio2_irq_log_port_errs(struct device *dev, u8 port, u32 status)\n{\n\tunsigned long port_status = status;\n\tunsigned int i;\n\n\tfor_each_set_bit(i, &port_status, ARRAY_SIZE(cio2_port_errs))\n\t\tdev_err(dev, \"port %i error %s\\n\", port, cio2_port_errs[i]);\n}\n\nstatic void cio2_irq_handle_once(struct cio2_device *cio2, u32 int_status)\n{\n\tstruct device *dev = &cio2->pci_dev->dev;\n\tvoid __iomem *const base = cio2->base;\n\n\tif (int_status & CIO2_INT_IOOE) {\n\t\t \n\t\tu32 oe_status, oe_clear;\n\n\t\toe_clear = readl(base + CIO2_REG_INT_STS_EXT_OE);\n\t\toe_status = oe_clear;\n\n\t\tif (oe_status & CIO2_INT_EXT_OE_DMAOE_MASK) {\n\t\t\tdev_err(dev, \"DMA output error: 0x%x\\n\",\n\t\t\t\t(oe_status & CIO2_INT_EXT_OE_DMAOE_MASK)\n\t\t\t\t>> CIO2_INT_EXT_OE_DMAOE_SHIFT);\n\t\t\toe_status &= ~CIO2_INT_EXT_OE_DMAOE_MASK;\n\t\t}\n\t\tif (oe_status & CIO2_INT_EXT_OE_OES_MASK) {\n\t\t\tdev_err(dev, \"DMA output error on CSI2 buses: 0x%x\\n\",\n\t\t\t\t(oe_status & CIO2_INT_EXT_OE_OES_MASK)\n\t\t\t\t>> CIO2_INT_EXT_OE_OES_SHIFT);\n\t\t\toe_status &= ~CIO2_INT_EXT_OE_OES_MASK;\n\t\t}\n\t\twritel(oe_clear, base + CIO2_REG_INT_STS_EXT_OE);\n\t\tif (oe_status)\n\t\t\tdev_warn(dev, \"unknown interrupt 0x%x on OE\\n\",\n\t\t\t\t oe_status);\n\t\tint_status &= ~CIO2_INT_IOOE;\n\t}\n\n\tif (int_status & CIO2_INT_IOC_MASK) {\n\t\t \n\t\tu32 clr = 0;\n\t\tunsigned int d;\n\n\t\tfor (d = 0; d < CIO2_NUM_DMA_CHAN; d++)\n\t\t\tif (int_status & CIO2_INT_IOC(d)) {\n\t\t\t\tclr |= CIO2_INT_IOC(d);\n\t\t\t\tcio2_buffer_done(cio2, d);\n\t\t\t}\n\t\tint_status &= ~clr;\n\t}\n\n\tif (int_status & CIO2_INT_IOS_IOLN_MASK) {\n\t\t \n\t\tu32 clr = 0;\n\t\tunsigned int d;\n\n\t\tfor (d = 0; d < CIO2_NUM_DMA_CHAN; d++)\n\t\t\tif (int_status & CIO2_INT_IOS_IOLN(d)) {\n\t\t\t\tclr |= CIO2_INT_IOS_IOLN(d);\n\t\t\t\tif (d == CIO2_DMA_CHAN)\n\t\t\t\t\tcio2_queue_event_sof(cio2,\n\t\t\t\t\t\t\t     cio2->cur_queue);\n\t\t\t}\n\t\tint_status &= ~clr;\n\t}\n\n\tif (int_status & (CIO2_INT_IOIE | CIO2_INT_IOIRQ)) {\n\t\t \n\t\tunsigned int port;\n\t\tu32 ie_status;\n\n\t\tie_status = readl(base + CIO2_REG_INT_STS_EXT_IE);\n\n\t\tfor (port = 0; port < CIO2_NUM_PORTS; port++) {\n\t\t\tu32 port_status = (ie_status >> (port * 8)) & 0xff;\n\n\t\t\tcio2_irq_log_port_errs(dev, port, port_status);\n\n\t\t\tif (ie_status & CIO2_INT_EXT_IE_IRQ(port)) {\n\t\t\t\tvoid __iomem *csi_rx_base =\n\t\t\t\t\t\tbase + CIO2_REG_PIPE_BASE(port);\n\t\t\t\tu32 csi2_status;\n\n\t\t\t\tcsi2_status = readl(csi_rx_base +\n\t\t\t\t\t\tCIO2_REG_IRQCTRL_STATUS);\n\n\t\t\t\tcio2_irq_log_irq_errs(dev, port, csi2_status);\n\n\t\t\t\twritel(csi2_status,\n\t\t\t\t       csi_rx_base + CIO2_REG_IRQCTRL_CLEAR);\n\t\t\t}\n\t\t}\n\n\t\twritel(ie_status, base + CIO2_REG_INT_STS_EXT_IE);\n\n\t\tint_status &= ~(CIO2_INT_IOIE | CIO2_INT_IOIRQ);\n\t}\n\n\tif (int_status)\n\t\tdev_warn(dev, \"unknown interrupt 0x%x on INT\\n\", int_status);\n}\n\nstatic irqreturn_t cio2_irq(int irq, void *cio2_ptr)\n{\n\tstruct cio2_device *cio2 = cio2_ptr;\n\tvoid __iomem *const base = cio2->base;\n\tstruct device *dev = &cio2->pci_dev->dev;\n\tu32 int_status;\n\n\tint_status = readl(base + CIO2_REG_INT_STS);\n\tdev_dbg(dev, \"isr enter - interrupt status 0x%x\\n\", int_status);\n\tif (!int_status)\n\t\treturn IRQ_NONE;\n\n\tdo {\n\t\twritel(int_status, base + CIO2_REG_INT_STS);\n\t\tcio2_irq_handle_once(cio2, int_status);\n\t\tint_status = readl(base + CIO2_REG_INT_STS);\n\t\tif (int_status)\n\t\t\tdev_dbg(dev, \"pending status 0x%x\\n\", int_status);\n\t} while (int_status);\n\n\treturn IRQ_HANDLED;\n}\n\n \n\nstatic void cio2_vb2_return_all_buffers(struct cio2_queue *q,\n\t\t\t\t\tenum vb2_buffer_state state)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < CIO2_MAX_BUFFERS; i++) {\n\t\tif (q->bufs[i]) {\n\t\t\tatomic_dec(&q->bufs_queued);\n\t\t\tvb2_buffer_done(&q->bufs[i]->vbb.vb2_buf,\n\t\t\t\t\tstate);\n\t\t\tq->bufs[i] = NULL;\n\t\t}\n\t}\n}\n\nstatic int cio2_vb2_queue_setup(struct vb2_queue *vq,\n\t\t\t\tunsigned int *num_buffers,\n\t\t\t\tunsigned int *num_planes,\n\t\t\t\tunsigned int sizes[],\n\t\t\t\tstruct device *alloc_devs[])\n{\n\tstruct cio2_device *cio2 = vb2_get_drv_priv(vq);\n\tstruct device *dev = &cio2->pci_dev->dev;\n\tstruct cio2_queue *q = vb2q_to_cio2_queue(vq);\n\tunsigned int i;\n\n\tif (*num_planes && *num_planes < q->format.num_planes)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < q->format.num_planes; ++i) {\n\t\tif (*num_planes && sizes[i] < q->format.plane_fmt[i].sizeimage)\n\t\t\treturn -EINVAL;\n\t\tsizes[i] = q->format.plane_fmt[i].sizeimage;\n\t\talloc_devs[i] = dev;\n\t}\n\n\t*num_planes = q->format.num_planes;\n\t*num_buffers = clamp_val(*num_buffers, 1, CIO2_MAX_BUFFERS);\n\n\t \n\tfor (i = 0; i < CIO2_MAX_BUFFERS; i++) {\n\t\tq->bufs[i] = NULL;\n\t\tcio2_fbpt_entry_init_dummy(cio2, &q->fbpt[i * CIO2_MAX_LOPS]);\n\t}\n\tatomic_set(&q->bufs_queued, 0);\n\tq->bufs_first = 0;\n\tq->bufs_next = 0;\n\n\treturn 0;\n}\n\n \nstatic int cio2_vb2_buf_init(struct vb2_buffer *vb)\n{\n\tstruct cio2_device *cio2 = vb2_get_drv_priv(vb->vb2_queue);\n\tstruct device *dev = &cio2->pci_dev->dev;\n\tstruct cio2_buffer *b = to_cio2_buffer(vb);\n\tunsigned int pages = PFN_UP(vb->planes[0].length);\n\tunsigned int lops = DIV_ROUND_UP(pages + 1, CIO2_LOP_ENTRIES);\n\tstruct sg_table *sg;\n\tstruct sg_dma_page_iter sg_iter;\n\tunsigned int i, j;\n\n\tif (lops <= 0 || lops > CIO2_MAX_LOPS) {\n\t\tdev_err(dev, \"%s: bad buffer size (%i)\\n\", __func__,\n\t\t\tvb->planes[0].length);\n\t\treturn -ENOSPC;\t\t \n\t}\n\n\tmemset(b->lop, 0, sizeof(b->lop));\n\t \n\tfor (i = 0; i < lops; i++) {\n\t\tb->lop[i] = dma_alloc_coherent(dev, PAGE_SIZE,\n\t\t\t\t\t       &b->lop_bus_addr[i], GFP_KERNEL);\n\t\tif (!b->lop[i])\n\t\t\tgoto fail;\n\t}\n\n\t \n\tsg = vb2_dma_sg_plane_desc(vb, 0);\n\tif (!sg)\n\t\treturn -ENOMEM;\n\n\tif (sg->nents && sg->sgl)\n\t\tb->offset = sg->sgl->offset;\n\n\ti = j = 0;\n\tfor_each_sg_dma_page(sg->sgl, &sg_iter, sg->nents, 0) {\n\t\tif (!pages--)\n\t\t\tbreak;\n\t\tb->lop[i][j] = PFN_DOWN(sg_page_iter_dma_address(&sg_iter));\n\t\tj++;\n\t\tif (j == CIO2_LOP_ENTRIES) {\n\t\t\ti++;\n\t\t\tj = 0;\n\t\t}\n\t}\n\n\tb->lop[i][j] = PFN_DOWN(cio2->dummy_page_bus_addr);\n\treturn 0;\nfail:\n\twhile (i--)\n\t\tdma_free_coherent(dev, PAGE_SIZE, b->lop[i], b->lop_bus_addr[i]);\n\treturn -ENOMEM;\n}\n\n \nstatic void cio2_vb2_buf_queue(struct vb2_buffer *vb)\n{\n\tstruct cio2_device *cio2 = vb2_get_drv_priv(vb->vb2_queue);\n\tstruct device *dev = &cio2->pci_dev->dev;\n\tstruct cio2_queue *q =\n\t\tcontainer_of(vb->vb2_queue, struct cio2_queue, vbq);\n\tstruct cio2_buffer *b = to_cio2_buffer(vb);\n\tstruct cio2_fbpt_entry *entry;\n\tunsigned long flags;\n\tunsigned int i, j, next = q->bufs_next;\n\tint bufs_queued = atomic_inc_return(&q->bufs_queued);\n\tu32 fbpt_rp;\n\n\tdev_dbg(dev, \"queue buffer %d\\n\", vb->index);\n\n\t \n\tlocal_irq_save(flags);\n\n\tfbpt_rp = (readl(cio2->base + CIO2_REG_CDMARI(CIO2_DMA_CHAN))\n\t\t   >> CIO2_CDMARI_FBPT_RP_SHIFT)\n\t\t   & CIO2_CDMARI_FBPT_RP_MASK;\n\n\t \n\tfbpt_rp = (fbpt_rp + 1) % CIO2_MAX_BUFFERS;\n\n\tif (bufs_queued <= 1 || fbpt_rp == next)\n\t\t \n\t\tnext = (fbpt_rp + 1) % CIO2_MAX_BUFFERS;\n\n\tfor (i = 0; i < CIO2_MAX_BUFFERS; i++) {\n\t\t \n\t\tif (!q->bufs[next]) {\n\t\t\tq->bufs[next] = b;\n\t\t\tentry = &q->fbpt[next * CIO2_MAX_LOPS];\n\t\t\tcio2_fbpt_entry_init_buf(cio2, b, entry);\n\t\t\tlocal_irq_restore(flags);\n\t\t\tq->bufs_next = (next + 1) % CIO2_MAX_BUFFERS;\n\t\t\tfor (j = 0; j < vb->num_planes; j++)\n\t\t\t\tvb2_set_plane_payload(vb, j,\n\t\t\t\t\tq->format.plane_fmt[j].sizeimage);\n\t\t\treturn;\n\t\t}\n\n\t\tdev_dbg(dev, \"entry %i was full!\\n\", next);\n\t\tnext = (next + 1) % CIO2_MAX_BUFFERS;\n\t}\n\n\tlocal_irq_restore(flags);\n\tdev_err(dev, \"error: all cio2 entries were full!\\n\");\n\tatomic_dec(&q->bufs_queued);\n\tvb2_buffer_done(vb, VB2_BUF_STATE_ERROR);\n}\n\n \nstatic void cio2_vb2_buf_cleanup(struct vb2_buffer *vb)\n{\n\tstruct cio2_device *cio2 = vb2_get_drv_priv(vb->vb2_queue);\n\tstruct device *dev = &cio2->pci_dev->dev;\n\tstruct cio2_buffer *b = to_cio2_buffer(vb);\n\tunsigned int i;\n\n\t \n\tfor (i = 0; i < CIO2_MAX_LOPS; i++) {\n\t\tif (b->lop[i])\n\t\t\tdma_free_coherent(dev, PAGE_SIZE,\n\t\t\t\t\t  b->lop[i], b->lop_bus_addr[i]);\n\t}\n}\n\nstatic int cio2_vb2_start_streaming(struct vb2_queue *vq, unsigned int count)\n{\n\tstruct cio2_queue *q = vb2q_to_cio2_queue(vq);\n\tstruct cio2_device *cio2 = vb2_get_drv_priv(vq);\n\tstruct device *dev = &cio2->pci_dev->dev;\n\tint r;\n\n\tcio2->cur_queue = q;\n\tatomic_set(&q->frame_sequence, 0);\n\n\tr = pm_runtime_resume_and_get(dev);\n\tif (r < 0) {\n\t\tdev_info(dev, \"failed to set power %d\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = video_device_pipeline_start(&q->vdev, &q->pipe);\n\tif (r)\n\t\tgoto fail_pipeline;\n\n\tr = cio2_hw_init(cio2, q);\n\tif (r)\n\t\tgoto fail_hw;\n\n\t \n\tr = v4l2_subdev_call(q->sensor, video, s_stream, 1);\n\tif (r)\n\t\tgoto fail_csi2_subdev;\n\n\tcio2->streaming = true;\n\n\treturn 0;\n\nfail_csi2_subdev:\n\tcio2_hw_exit(cio2, q);\nfail_hw:\n\tvideo_device_pipeline_stop(&q->vdev);\nfail_pipeline:\n\tdev_dbg(dev, \"failed to start streaming (%d)\\n\", r);\n\tcio2_vb2_return_all_buffers(q, VB2_BUF_STATE_QUEUED);\n\tpm_runtime_put(dev);\n\n\treturn r;\n}\n\nstatic void cio2_vb2_stop_streaming(struct vb2_queue *vq)\n{\n\tstruct cio2_queue *q = vb2q_to_cio2_queue(vq);\n\tstruct cio2_device *cio2 = vb2_get_drv_priv(vq);\n\tstruct device *dev = &cio2->pci_dev->dev;\n\n\tif (v4l2_subdev_call(q->sensor, video, s_stream, 0))\n\t\tdev_err(dev, \"failed to stop sensor streaming\\n\");\n\n\tcio2_hw_exit(cio2, q);\n\tsynchronize_irq(cio2->pci_dev->irq);\n\tcio2_vb2_return_all_buffers(q, VB2_BUF_STATE_ERROR);\n\tvideo_device_pipeline_stop(&q->vdev);\n\tpm_runtime_put(dev);\n\tcio2->streaming = false;\n}\n\nstatic const struct vb2_ops cio2_vb2_ops = {\n\t.buf_init = cio2_vb2_buf_init,\n\t.buf_queue = cio2_vb2_buf_queue,\n\t.buf_cleanup = cio2_vb2_buf_cleanup,\n\t.queue_setup = cio2_vb2_queue_setup,\n\t.start_streaming = cio2_vb2_start_streaming,\n\t.stop_streaming = cio2_vb2_stop_streaming,\n\t.wait_prepare = vb2_ops_wait_prepare,\n\t.wait_finish = vb2_ops_wait_finish,\n};\n\n \n\nstatic int cio2_v4l2_querycap(struct file *file, void *fh,\n\t\t\t      struct v4l2_capability *cap)\n{\n\tstrscpy(cap->driver, CIO2_NAME, sizeof(cap->driver));\n\tstrscpy(cap->card, CIO2_DEVICE_NAME, sizeof(cap->card));\n\n\treturn 0;\n}\n\nstatic int cio2_v4l2_enum_fmt(struct file *file, void *fh,\n\t\t\t      struct v4l2_fmtdesc *f)\n{\n\tif (f->index >= ARRAY_SIZE(formats))\n\t\treturn -EINVAL;\n\n\tf->pixelformat = formats[f->index].fourcc;\n\n\treturn 0;\n}\n\n \nstatic int cio2_v4l2_g_fmt(struct file *file, void *fh, struct v4l2_format *f)\n{\n\tstruct cio2_queue *q = file_to_cio2_queue(file);\n\n\tf->fmt.pix_mp = q->format;\n\n\treturn 0;\n}\n\nstatic int cio2_v4l2_try_fmt(struct file *file, void *fh, struct v4l2_format *f)\n{\n\tconst struct ipu3_cio2_fmt *fmt;\n\tstruct v4l2_pix_format_mplane *mpix = &f->fmt.pix_mp;\n\n\tfmt = cio2_find_format(&mpix->pixelformat, NULL);\n\tif (!fmt)\n\t\tfmt = &formats[0];\n\n\t \n\tif (mpix->width > CIO2_IMAGE_MAX_WIDTH)\n\t\tmpix->width = CIO2_IMAGE_MAX_WIDTH;\n\tif (mpix->height > CIO2_IMAGE_MAX_HEIGHT)\n\t\tmpix->height = CIO2_IMAGE_MAX_HEIGHT;\n\n\tmpix->num_planes = 1;\n\tmpix->pixelformat = fmt->fourcc;\n\tmpix->colorspace = V4L2_COLORSPACE_RAW;\n\tmpix->field = V4L2_FIELD_NONE;\n\tmpix->plane_fmt[0].bytesperline = cio2_bytesperline(mpix->width);\n\tmpix->plane_fmt[0].sizeimage = mpix->plane_fmt[0].bytesperline *\n\t\t\t\t\t\t\tmpix->height;\n\n\t \n\tmpix->ycbcr_enc = V4L2_YCBCR_ENC_DEFAULT;\n\tmpix->quantization = V4L2_QUANTIZATION_DEFAULT;\n\tmpix->xfer_func = V4L2_XFER_FUNC_DEFAULT;\n\n\treturn 0;\n}\n\nstatic int cio2_v4l2_s_fmt(struct file *file, void *fh, struct v4l2_format *f)\n{\n\tstruct cio2_queue *q = file_to_cio2_queue(file);\n\n\tcio2_v4l2_try_fmt(file, fh, f);\n\tq->format = f->fmt.pix_mp;\n\n\treturn 0;\n}\n\nstatic int\ncio2_video_enum_input(struct file *file, void *fh, struct v4l2_input *input)\n{\n\tif (input->index > 0)\n\t\treturn -EINVAL;\n\n\tstrscpy(input->name, \"camera\", sizeof(input->name));\n\tinput->type = V4L2_INPUT_TYPE_CAMERA;\n\n\treturn 0;\n}\n\nstatic int\ncio2_video_g_input(struct file *file, void *fh, unsigned int *input)\n{\n\t*input = 0;\n\n\treturn 0;\n}\n\nstatic int\ncio2_video_s_input(struct file *file, void *fh, unsigned int input)\n{\n\treturn input == 0 ? 0 : -EINVAL;\n}\n\nstatic const struct v4l2_file_operations cio2_v4l2_fops = {\n\t.owner = THIS_MODULE,\n\t.unlocked_ioctl = video_ioctl2,\n\t.open = v4l2_fh_open,\n\t.release = vb2_fop_release,\n\t.poll = vb2_fop_poll,\n\t.mmap = vb2_fop_mmap,\n};\n\nstatic const struct v4l2_ioctl_ops cio2_v4l2_ioctl_ops = {\n\t.vidioc_querycap = cio2_v4l2_querycap,\n\t.vidioc_enum_fmt_vid_cap = cio2_v4l2_enum_fmt,\n\t.vidioc_g_fmt_vid_cap_mplane = cio2_v4l2_g_fmt,\n\t.vidioc_s_fmt_vid_cap_mplane = cio2_v4l2_s_fmt,\n\t.vidioc_try_fmt_vid_cap_mplane = cio2_v4l2_try_fmt,\n\t.vidioc_reqbufs = vb2_ioctl_reqbufs,\n\t.vidioc_create_bufs = vb2_ioctl_create_bufs,\n\t.vidioc_prepare_buf = vb2_ioctl_prepare_buf,\n\t.vidioc_querybuf = vb2_ioctl_querybuf,\n\t.vidioc_qbuf = vb2_ioctl_qbuf,\n\t.vidioc_dqbuf = vb2_ioctl_dqbuf,\n\t.vidioc_streamon = vb2_ioctl_streamon,\n\t.vidioc_streamoff = vb2_ioctl_streamoff,\n\t.vidioc_expbuf = vb2_ioctl_expbuf,\n\t.vidioc_enum_input = cio2_video_enum_input,\n\t.vidioc_g_input\t= cio2_video_g_input,\n\t.vidioc_s_input\t= cio2_video_s_input,\n};\n\nstatic int cio2_subdev_subscribe_event(struct v4l2_subdev *sd,\n\t\t\t\t       struct v4l2_fh *fh,\n\t\t\t\t       struct v4l2_event_subscription *sub)\n{\n\tif (sub->type != V4L2_EVENT_FRAME_SYNC)\n\t\treturn -EINVAL;\n\n\t \n\tif (sub->id != 0)\n\t\treturn -EINVAL;\n\n\treturn v4l2_event_subscribe(fh, sub, 0, NULL);\n}\n\nstatic int cio2_subdev_open(struct v4l2_subdev *sd, struct v4l2_subdev_fh *fh)\n{\n\tstruct v4l2_mbus_framefmt *format;\n\tconst struct v4l2_mbus_framefmt fmt_default = {\n\t\t.width = 1936,\n\t\t.height = 1096,\n\t\t.code = formats[0].mbus_code,\n\t\t.field = V4L2_FIELD_NONE,\n\t\t.colorspace = V4L2_COLORSPACE_RAW,\n\t\t.ycbcr_enc = V4L2_YCBCR_ENC_DEFAULT,\n\t\t.quantization = V4L2_QUANTIZATION_DEFAULT,\n\t\t.xfer_func = V4L2_XFER_FUNC_DEFAULT,\n\t};\n\n\t \n\tformat = v4l2_subdev_get_try_format(sd, fh->state, CIO2_PAD_SINK);\n\t*format = fmt_default;\n\n\t \n\tformat = v4l2_subdev_get_try_format(sd, fh->state, CIO2_PAD_SOURCE);\n\t*format = fmt_default;\n\n\treturn 0;\n}\n\n \nstatic int cio2_subdev_get_fmt(struct v4l2_subdev *sd,\n\t\t\t       struct v4l2_subdev_state *sd_state,\n\t\t\t       struct v4l2_subdev_format *fmt)\n{\n\tstruct cio2_queue *q = container_of(sd, struct cio2_queue, subdev);\n\n\tmutex_lock(&q->subdev_lock);\n\n\tif (fmt->which == V4L2_SUBDEV_FORMAT_TRY)\n\t\tfmt->format = *v4l2_subdev_get_try_format(sd, sd_state,\n\t\t\t\t\t\t\t  fmt->pad);\n\telse\n\t\tfmt->format = q->subdev_fmt;\n\n\tmutex_unlock(&q->subdev_lock);\n\n\treturn 0;\n}\n\n \nstatic int cio2_subdev_set_fmt(struct v4l2_subdev *sd,\n\t\t\t       struct v4l2_subdev_state *sd_state,\n\t\t\t       struct v4l2_subdev_format *fmt)\n{\n\tstruct cio2_queue *q = container_of(sd, struct cio2_queue, subdev);\n\tstruct v4l2_mbus_framefmt *mbus;\n\tu32 mbus_code = fmt->format.code;\n\tunsigned int i;\n\n\t \n\tif (fmt->pad == CIO2_PAD_SOURCE)\n\t\treturn cio2_subdev_get_fmt(sd, sd_state, fmt);\n\n\tif (fmt->which == V4L2_SUBDEV_FORMAT_TRY)\n\t\tmbus = v4l2_subdev_get_try_format(sd, sd_state, fmt->pad);\n\telse\n\t\tmbus = &q->subdev_fmt;\n\n\tfmt->format.code = formats[0].mbus_code;\n\n\tfor (i = 0; i < ARRAY_SIZE(formats); i++) {\n\t\tif (formats[i].mbus_code == mbus_code) {\n\t\t\tfmt->format.code = mbus_code;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfmt->format.width = min(fmt->format.width, CIO2_IMAGE_MAX_WIDTH);\n\tfmt->format.height = min(fmt->format.height, CIO2_IMAGE_MAX_HEIGHT);\n\tfmt->format.field = V4L2_FIELD_NONE;\n\n\tmutex_lock(&q->subdev_lock);\n\t*mbus = fmt->format;\n\tmutex_unlock(&q->subdev_lock);\n\n\treturn 0;\n}\n\nstatic int cio2_subdev_enum_mbus_code(struct v4l2_subdev *sd,\n\t\t\t\t      struct v4l2_subdev_state *sd_state,\n\t\t\t\t      struct v4l2_subdev_mbus_code_enum *code)\n{\n\tif (code->index >= ARRAY_SIZE(formats))\n\t\treturn -EINVAL;\n\n\tcode->code = formats[code->index].mbus_code;\n\treturn 0;\n}\n\nstatic int cio2_subdev_link_validate_get_format(struct media_pad *pad,\n\t\t\t\t\t\tstruct v4l2_subdev_format *fmt)\n{\n\tif (is_media_entity_v4l2_subdev(pad->entity)) {\n\t\tstruct v4l2_subdev *sd =\n\t\t\tmedia_entity_to_v4l2_subdev(pad->entity);\n\n\t\tmemset(fmt, 0, sizeof(*fmt));\n\t\tfmt->which = V4L2_SUBDEV_FORMAT_ACTIVE;\n\t\tfmt->pad = pad->index;\n\t\treturn v4l2_subdev_call(sd, pad, get_fmt, NULL, fmt);\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic int cio2_video_link_validate(struct media_link *link)\n{\n\tstruct media_entity *entity = link->sink->entity;\n\tstruct video_device *vd = media_entity_to_video_device(entity);\n\tstruct cio2_queue *q = container_of(vd, struct cio2_queue, vdev);\n\tstruct cio2_device *cio2 = video_get_drvdata(vd);\n\tstruct device *dev = &cio2->pci_dev->dev;\n\tstruct v4l2_subdev_format source_fmt;\n\tint ret;\n\n\tif (!media_pad_remote_pad_first(entity->pads)) {\n\t\tdev_info(dev, \"video node %s pad not connected\\n\", vd->name);\n\t\treturn -ENOTCONN;\n\t}\n\n\tret = cio2_subdev_link_validate_get_format(link->source, &source_fmt);\n\tif (ret < 0)\n\t\treturn 0;\n\n\tif (source_fmt.format.width != q->format.width ||\n\t    source_fmt.format.height != q->format.height) {\n\t\tdev_err(dev, \"Wrong width or height %ux%u (%ux%u expected)\\n\",\n\t\t\tq->format.width, q->format.height,\n\t\t\tsource_fmt.format.width, source_fmt.format.height);\n\t\treturn -EINVAL;\n\t}\n\n\tif (!cio2_find_format(&q->format.pixelformat, &source_fmt.format.code))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic const struct v4l2_subdev_core_ops cio2_subdev_core_ops = {\n\t.subscribe_event = cio2_subdev_subscribe_event,\n\t.unsubscribe_event = v4l2_event_subdev_unsubscribe,\n};\n\nstatic const struct v4l2_subdev_internal_ops cio2_subdev_internal_ops = {\n\t.open = cio2_subdev_open,\n};\n\nstatic const struct v4l2_subdev_pad_ops cio2_subdev_pad_ops = {\n\t.link_validate = v4l2_subdev_link_validate_default,\n\t.get_fmt = cio2_subdev_get_fmt,\n\t.set_fmt = cio2_subdev_set_fmt,\n\t.enum_mbus_code = cio2_subdev_enum_mbus_code,\n};\n\nstatic const struct v4l2_subdev_ops cio2_subdev_ops = {\n\t.core = &cio2_subdev_core_ops,\n\t.pad = &cio2_subdev_pad_ops,\n};\n\n \n\nstruct sensor_async_subdev {\n\tstruct v4l2_async_connection asd;\n\tstruct csi2_bus_info csi2;\n};\n\n#define to_sensor_asd(__asd)\t\\\n\tcontainer_of_const(__asd, struct sensor_async_subdev, asd)\n\n \nstatic int cio2_notifier_bound(struct v4l2_async_notifier *notifier,\n\t\t\t       struct v4l2_subdev *sd,\n\t\t\t       struct v4l2_async_connection *asd)\n{\n\tstruct cio2_device *cio2 = to_cio2_device(notifier);\n\tstruct sensor_async_subdev *s_asd = to_sensor_asd(asd);\n\tstruct cio2_queue *q;\n\tint ret;\n\n\tif (cio2->queue[s_asd->csi2.port].sensor)\n\t\treturn -EBUSY;\n\n\tret = ipu_bridge_instantiate_vcm(sd->dev);\n\tif (ret)\n\t\treturn ret;\n\n\tq = &cio2->queue[s_asd->csi2.port];\n\n\tq->csi2 = s_asd->csi2;\n\tq->sensor = sd;\n\tq->csi_rx_base = cio2->base + CIO2_REG_PIPE_BASE(q->csi2.port);\n\n\treturn 0;\n}\n\n \nstatic void cio2_notifier_unbind(struct v4l2_async_notifier *notifier,\n\t\t\t\t struct v4l2_subdev *sd,\n\t\t\t\t struct v4l2_async_connection *asd)\n{\n\tstruct cio2_device *cio2 = to_cio2_device(notifier);\n\tstruct sensor_async_subdev *s_asd = to_sensor_asd(asd);\n\n\tcio2->queue[s_asd->csi2.port].sensor = NULL;\n}\n\n \nstatic int cio2_notifier_complete(struct v4l2_async_notifier *notifier)\n{\n\tstruct cio2_device *cio2 = to_cio2_device(notifier);\n\tstruct device *dev = &cio2->pci_dev->dev;\n\tstruct sensor_async_subdev *s_asd;\n\tstruct v4l2_async_connection *asd;\n\tstruct cio2_queue *q;\n\tint ret;\n\n\tlist_for_each_entry(asd, &cio2->notifier.done_list, asc_entry) {\n\t\ts_asd = to_sensor_asd(asd);\n\t\tq = &cio2->queue[s_asd->csi2.port];\n\n\t\tret = media_entity_get_fwnode_pad(&q->sensor->entity,\n\t\t\t\t\t\t  s_asd->asd.match.fwnode,\n\t\t\t\t\t\t  MEDIA_PAD_FL_SOURCE);\n\t\tif (ret < 0) {\n\t\t\tdev_err(dev, \"no pad for endpoint %pfw (%d)\\n\",\n\t\t\t\ts_asd->asd.match.fwnode, ret);\n\t\t\treturn ret;\n\t\t}\n\n\t\tret = media_create_pad_link(&q->sensor->entity, ret,\n\t\t\t\t\t    &q->subdev.entity, CIO2_PAD_SINK,\n\t\t\t\t\t    0);\n\t\tif (ret) {\n\t\t\tdev_err(dev, \"failed to create link for %s (endpoint %pfw, error %d)\\n\",\n\t\t\t\tq->sensor->name, s_asd->asd.match.fwnode, ret);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\treturn v4l2_device_register_subdev_nodes(&cio2->v4l2_dev);\n}\n\nstatic const struct v4l2_async_notifier_operations cio2_async_ops = {\n\t.bound = cio2_notifier_bound,\n\t.unbind = cio2_notifier_unbind,\n\t.complete = cio2_notifier_complete,\n};\n\nstatic int cio2_parse_firmware(struct cio2_device *cio2)\n{\n\tstruct device *dev = &cio2->pci_dev->dev;\n\tunsigned int i;\n\tint ret;\n\n\tfor (i = 0; i < CIO2_NUM_PORTS; i++) {\n\t\tstruct v4l2_fwnode_endpoint vep = {\n\t\t\t.bus_type = V4L2_MBUS_CSI2_DPHY\n\t\t};\n\t\tstruct sensor_async_subdev *s_asd;\n\t\tstruct fwnode_handle *ep;\n\n\t\tep = fwnode_graph_get_endpoint_by_id(dev_fwnode(dev), i, 0,\n\t\t\t\t\t\tFWNODE_GRAPH_ENDPOINT_NEXT);\n\t\tif (!ep)\n\t\t\tcontinue;\n\n\t\tret = v4l2_fwnode_endpoint_parse(ep, &vep);\n\t\tif (ret)\n\t\t\tgoto err_parse;\n\n\t\ts_asd = v4l2_async_nf_add_fwnode_remote(&cio2->notifier, ep,\n\t\t\t\t\t\t\tstruct\n\t\t\t\t\t\t\tsensor_async_subdev);\n\t\tif (IS_ERR(s_asd)) {\n\t\t\tret = PTR_ERR(s_asd);\n\t\t\tgoto err_parse;\n\t\t}\n\n\t\ts_asd->csi2.port = vep.base.port;\n\t\ts_asd->csi2.lanes = vep.bus.mipi_csi2.num_data_lanes;\n\n\t\tfwnode_handle_put(ep);\n\n\t\tcontinue;\n\nerr_parse:\n\t\tfwnode_handle_put(ep);\n\t\treturn ret;\n\t}\n\n\t \n\tcio2->notifier.ops = &cio2_async_ops;\n\tret = v4l2_async_nf_register(&cio2->notifier);\n\tif (ret)\n\t\tdev_err(dev, \"failed to register async notifier : %d\\n\", ret);\n\n\treturn ret;\n}\n\n \nstatic const struct media_entity_operations cio2_media_ops = {\n\t.link_validate = v4l2_subdev_link_validate,\n};\n\nstatic const struct media_entity_operations cio2_video_entity_ops = {\n\t.link_validate = cio2_video_link_validate,\n};\n\nstatic int cio2_queue_init(struct cio2_device *cio2, struct cio2_queue *q)\n{\n\tstatic const u32 default_width = 1936;\n\tstatic const u32 default_height = 1096;\n\tconst struct ipu3_cio2_fmt dflt_fmt = formats[0];\n\tstruct device *dev = &cio2->pci_dev->dev;\n\tstruct video_device *vdev = &q->vdev;\n\tstruct vb2_queue *vbq = &q->vbq;\n\tstruct v4l2_subdev *subdev = &q->subdev;\n\tstruct v4l2_mbus_framefmt *fmt;\n\tint r;\n\n\t \n\tmutex_init(&q->lock);\n\tmutex_init(&q->subdev_lock);\n\n\t \n\tfmt = &q->subdev_fmt;\n\tfmt->width = default_width;\n\tfmt->height = default_height;\n\tfmt->code = dflt_fmt.mbus_code;\n\tfmt->field = V4L2_FIELD_NONE;\n\n\tq->format.width = default_width;\n\tq->format.height = default_height;\n\tq->format.pixelformat = dflt_fmt.fourcc;\n\tq->format.colorspace = V4L2_COLORSPACE_RAW;\n\tq->format.field = V4L2_FIELD_NONE;\n\tq->format.num_planes = 1;\n\tq->format.plane_fmt[0].bytesperline =\n\t\t\t\tcio2_bytesperline(q->format.width);\n\tq->format.plane_fmt[0].sizeimage = q->format.plane_fmt[0].bytesperline *\n\t\t\t\t\t\tq->format.height;\n\n\t \n\tr = cio2_fbpt_init(cio2, q);\n\tif (r)\n\t\tgoto fail_fbpt;\n\n\t \n\tq->subdev_pads[CIO2_PAD_SINK].flags = MEDIA_PAD_FL_SINK |\n\t\tMEDIA_PAD_FL_MUST_CONNECT;\n\tq->subdev_pads[CIO2_PAD_SOURCE].flags = MEDIA_PAD_FL_SOURCE;\n\tsubdev->entity.ops = &cio2_media_ops;\n\tsubdev->internal_ops = &cio2_subdev_internal_ops;\n\tr = media_entity_pads_init(&subdev->entity, CIO2_PADS, q->subdev_pads);\n\tif (r) {\n\t\tdev_err(dev, \"failed initialize subdev media entity (%d)\\n\", r);\n\t\tgoto fail_subdev_media_entity;\n\t}\n\n\tq->vdev_pad.flags = MEDIA_PAD_FL_SINK | MEDIA_PAD_FL_MUST_CONNECT;\n\tvdev->entity.ops = &cio2_video_entity_ops;\n\tr = media_entity_pads_init(&vdev->entity, 1, &q->vdev_pad);\n\tif (r) {\n\t\tdev_err(dev, \"failed initialize videodev media entity (%d)\\n\",\n\t\t\tr);\n\t\tgoto fail_vdev_media_entity;\n\t}\n\n\t \n\tv4l2_subdev_init(subdev, &cio2_subdev_ops);\n\tsubdev->flags = V4L2_SUBDEV_FL_HAS_DEVNODE | V4L2_SUBDEV_FL_HAS_EVENTS;\n\tsubdev->owner = THIS_MODULE;\n\tsnprintf(subdev->name, sizeof(subdev->name),\n\t\t CIO2_ENTITY_NAME \" %td\", q - cio2->queue);\n\tsubdev->entity.function = MEDIA_ENT_F_VID_IF_BRIDGE;\n\tv4l2_set_subdevdata(subdev, cio2);\n\tr = v4l2_device_register_subdev(&cio2->v4l2_dev, subdev);\n\tif (r) {\n\t\tdev_err(dev, \"failed initialize subdev (%d)\\n\", r);\n\t\tgoto fail_subdev;\n\t}\n\n\t \n\tvbq->type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;\n\tvbq->io_modes = VB2_USERPTR | VB2_MMAP | VB2_DMABUF;\n\tvbq->ops = &cio2_vb2_ops;\n\tvbq->mem_ops = &vb2_dma_sg_memops;\n\tvbq->buf_struct_size = sizeof(struct cio2_buffer);\n\tvbq->timestamp_flags = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC;\n\tvbq->min_buffers_needed = 1;\n\tvbq->drv_priv = cio2;\n\tvbq->lock = &q->lock;\n\tr = vb2_queue_init(vbq);\n\tif (r) {\n\t\tdev_err(dev, \"failed to initialize videobuf2 queue (%d)\\n\", r);\n\t\tgoto fail_subdev;\n\t}\n\n\t \n\tsnprintf(vdev->name, sizeof(vdev->name),\n\t\t \"%s %td\", CIO2_NAME, q - cio2->queue);\n\tvdev->release = video_device_release_empty;\n\tvdev->fops = &cio2_v4l2_fops;\n\tvdev->ioctl_ops = &cio2_v4l2_ioctl_ops;\n\tvdev->lock = &cio2->lock;\n\tvdev->v4l2_dev = &cio2->v4l2_dev;\n\tvdev->queue = &q->vbq;\n\tvdev->device_caps = V4L2_CAP_VIDEO_CAPTURE_MPLANE | V4L2_CAP_STREAMING;\n\tvideo_set_drvdata(vdev, cio2);\n\tr = video_register_device(vdev, VFL_TYPE_VIDEO, -1);\n\tif (r) {\n\t\tdev_err(dev, \"failed to register video device (%d)\\n\", r);\n\t\tgoto fail_vdev;\n\t}\n\n\t \n\tr = media_create_pad_link(\n\t\t&subdev->entity, CIO2_PAD_SOURCE, &vdev->entity, 0,\n\t\tMEDIA_LNK_FL_ENABLED | MEDIA_LNK_FL_IMMUTABLE);\n\tif (r)\n\t\tgoto fail_link;\n\n\treturn 0;\n\nfail_link:\n\tvb2_video_unregister_device(&q->vdev);\nfail_vdev:\n\tv4l2_device_unregister_subdev(subdev);\nfail_subdev:\n\tmedia_entity_cleanup(&vdev->entity);\nfail_vdev_media_entity:\n\tmedia_entity_cleanup(&subdev->entity);\nfail_subdev_media_entity:\n\tcio2_fbpt_exit(q, dev);\nfail_fbpt:\n\tmutex_destroy(&q->subdev_lock);\n\tmutex_destroy(&q->lock);\n\n\treturn r;\n}\n\nstatic void cio2_queue_exit(struct cio2_device *cio2, struct cio2_queue *q)\n{\n\tvb2_video_unregister_device(&q->vdev);\n\tmedia_entity_cleanup(&q->vdev.entity);\n\tv4l2_device_unregister_subdev(&q->subdev);\n\tmedia_entity_cleanup(&q->subdev.entity);\n\tcio2_fbpt_exit(q, &cio2->pci_dev->dev);\n\tmutex_destroy(&q->subdev_lock);\n\tmutex_destroy(&q->lock);\n}\n\nstatic int cio2_queues_init(struct cio2_device *cio2)\n{\n\tint i, r;\n\n\tfor (i = 0; i < CIO2_QUEUES; i++) {\n\t\tr = cio2_queue_init(cio2, &cio2->queue[i]);\n\t\tif (r)\n\t\t\tbreak;\n\t}\n\n\tif (i == CIO2_QUEUES)\n\t\treturn 0;\n\n\tfor (i--; i >= 0; i--)\n\t\tcio2_queue_exit(cio2, &cio2->queue[i]);\n\n\treturn r;\n}\n\nstatic void cio2_queues_exit(struct cio2_device *cio2)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < CIO2_QUEUES; i++)\n\t\tcio2_queue_exit(cio2, &cio2->queue[i]);\n}\n\nstatic int cio2_check_fwnode_graph(struct fwnode_handle *fwnode)\n{\n\tstruct fwnode_handle *endpoint;\n\n\tif (IS_ERR_OR_NULL(fwnode))\n\t\treturn -EINVAL;\n\n\tendpoint = fwnode_graph_get_next_endpoint(fwnode, NULL);\n\tif (endpoint) {\n\t\tfwnode_handle_put(endpoint);\n\t\treturn 0;\n\t}\n\n\treturn cio2_check_fwnode_graph(fwnode->secondary);\n}\n\n \n\nstatic int cio2_pci_probe(struct pci_dev *pci_dev,\n\t\t\t  const struct pci_device_id *id)\n{\n\tstruct device *dev = &pci_dev->dev;\n\tstruct fwnode_handle *fwnode = dev_fwnode(dev);\n\tstruct cio2_device *cio2;\n\tint r;\n\n\t \n\tr = cio2_check_fwnode_graph(fwnode);\n\tif (r) {\n\t\tif (fwnode && !IS_ERR_OR_NULL(fwnode->secondary)) {\n\t\t\tdev_err(dev, \"fwnode graph has no endpoints connected\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tr = ipu_bridge_init(dev, ipu_bridge_parse_ssdb);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tcio2 = devm_kzalloc(dev, sizeof(*cio2), GFP_KERNEL);\n\tif (!cio2)\n\t\treturn -ENOMEM;\n\tcio2->pci_dev = pci_dev;\n\n\tr = pcim_enable_device(pci_dev);\n\tif (r) {\n\t\tdev_err(dev, \"failed to enable device (%d)\\n\", r);\n\t\treturn r;\n\t}\n\n\tdev_info(dev, \"device 0x%x (rev: 0x%x)\\n\",\n\t\t pci_dev->device, pci_dev->revision);\n\n\tr = pcim_iomap_regions(pci_dev, 1 << CIO2_PCI_BAR, pci_name(pci_dev));\n\tif (r) {\n\t\tdev_err(dev, \"failed to remap I/O memory (%d)\\n\", r);\n\t\treturn -ENODEV;\n\t}\n\n\tcio2->base = pcim_iomap_table(pci_dev)[CIO2_PCI_BAR];\n\n\tpci_set_drvdata(pci_dev, cio2);\n\n\tpci_set_master(pci_dev);\n\n\tr = dma_set_mask(&pci_dev->dev, CIO2_DMA_MASK);\n\tif (r) {\n\t\tdev_err(dev, \"failed to set DMA mask (%d)\\n\", r);\n\t\treturn -ENODEV;\n\t}\n\n\tr = pci_enable_msi(pci_dev);\n\tif (r) {\n\t\tdev_err(dev, \"failed to enable MSI (%d)\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = cio2_fbpt_init_dummy(cio2);\n\tif (r)\n\t\treturn r;\n\n\tmutex_init(&cio2->lock);\n\n\tcio2->media_dev.dev = dev;\n\tstrscpy(cio2->media_dev.model, CIO2_DEVICE_NAME,\n\t\tsizeof(cio2->media_dev.model));\n\tcio2->media_dev.hw_revision = 0;\n\n\tmedia_device_init(&cio2->media_dev);\n\tr = media_device_register(&cio2->media_dev);\n\tif (r < 0)\n\t\tgoto fail_mutex_destroy;\n\n\tcio2->v4l2_dev.mdev = &cio2->media_dev;\n\tr = v4l2_device_register(dev, &cio2->v4l2_dev);\n\tif (r) {\n\t\tdev_err(dev, \"failed to register V4L2 device (%d)\\n\", r);\n\t\tgoto fail_media_device_unregister;\n\t}\n\n\tr = cio2_queues_init(cio2);\n\tif (r)\n\t\tgoto fail_v4l2_device_unregister;\n\n\tv4l2_async_nf_init(&cio2->notifier, &cio2->v4l2_dev);\n\n\t \n\tr = cio2_parse_firmware(cio2);\n\tif (r)\n\t\tgoto fail_clean_notifier;\n\n\tr = devm_request_irq(dev, pci_dev->irq, cio2_irq, IRQF_SHARED,\n\t\t\t     CIO2_NAME, cio2);\n\tif (r) {\n\t\tdev_err(dev, \"failed to request IRQ (%d)\\n\", r);\n\t\tgoto fail_clean_notifier;\n\t}\n\n\tpm_runtime_put_noidle(dev);\n\tpm_runtime_allow(dev);\n\n\treturn 0;\n\nfail_clean_notifier:\n\tv4l2_async_nf_unregister(&cio2->notifier);\n\tv4l2_async_nf_cleanup(&cio2->notifier);\n\tcio2_queues_exit(cio2);\nfail_v4l2_device_unregister:\n\tv4l2_device_unregister(&cio2->v4l2_dev);\nfail_media_device_unregister:\n\tmedia_device_unregister(&cio2->media_dev);\n\tmedia_device_cleanup(&cio2->media_dev);\nfail_mutex_destroy:\n\tmutex_destroy(&cio2->lock);\n\tcio2_fbpt_exit_dummy(cio2);\n\n\treturn r;\n}\n\nstatic void cio2_pci_remove(struct pci_dev *pci_dev)\n{\n\tstruct cio2_device *cio2 = pci_get_drvdata(pci_dev);\n\n\tmedia_device_unregister(&cio2->media_dev);\n\tv4l2_async_nf_unregister(&cio2->notifier);\n\tv4l2_async_nf_cleanup(&cio2->notifier);\n\tcio2_queues_exit(cio2);\n\tcio2_fbpt_exit_dummy(cio2);\n\tv4l2_device_unregister(&cio2->v4l2_dev);\n\tmedia_device_cleanup(&cio2->media_dev);\n\tmutex_destroy(&cio2->lock);\n\n\tpm_runtime_forbid(&pci_dev->dev);\n\tpm_runtime_get_noresume(&pci_dev->dev);\n}\n\nstatic int __maybe_unused cio2_runtime_suspend(struct device *dev)\n{\n\tstruct pci_dev *pci_dev = to_pci_dev(dev);\n\tstruct cio2_device *cio2 = pci_get_drvdata(pci_dev);\n\tvoid __iomem *const base = cio2->base;\n\tu16 pm;\n\n\twritel(CIO2_D0I3C_I3, base + CIO2_REG_D0I3C);\n\tdev_dbg(dev, \"cio2 runtime suspend.\\n\");\n\n\tpci_read_config_word(pci_dev, pci_dev->pm_cap + CIO2_PMCSR_OFFSET, &pm);\n\tpm = (pm >> CIO2_PMCSR_D0D3_SHIFT) << CIO2_PMCSR_D0D3_SHIFT;\n\tpm |= CIO2_PMCSR_D3;\n\tpci_write_config_word(pci_dev, pci_dev->pm_cap + CIO2_PMCSR_OFFSET, pm);\n\n\treturn 0;\n}\n\nstatic int __maybe_unused cio2_runtime_resume(struct device *dev)\n{\n\tstruct pci_dev *pci_dev = to_pci_dev(dev);\n\tstruct cio2_device *cio2 = pci_get_drvdata(pci_dev);\n\tvoid __iomem *const base = cio2->base;\n\tu16 pm;\n\n\twritel(CIO2_D0I3C_RR, base + CIO2_REG_D0I3C);\n\tdev_dbg(dev, \"cio2 runtime resume.\\n\");\n\n\tpci_read_config_word(pci_dev, pci_dev->pm_cap + CIO2_PMCSR_OFFSET, &pm);\n\tpm = (pm >> CIO2_PMCSR_D0D3_SHIFT) << CIO2_PMCSR_D0D3_SHIFT;\n\tpci_write_config_word(pci_dev, pci_dev->pm_cap + CIO2_PMCSR_OFFSET, pm);\n\n\treturn 0;\n}\n\n \nstatic void arrange(void *ptr, size_t elem_size, size_t elems, size_t start)\n{\n\tstruct {\n\t\tsize_t begin, end;\n\t} arr[2] = {\n\t\t{ 0, start - 1 },\n\t\t{ start, elems - 1 },\n\t};\n\n#define CHUNK_SIZE(a) ((a)->end - (a)->begin + 1)\n\n\t \n\twhile (CHUNK_SIZE(&arr[0]) && CHUNK_SIZE(&arr[1])) {\n\t\tsize_t size0, i;\n\n\t\t \n\t\tsize0 = min(CHUNK_SIZE(&arr[0]), CHUNK_SIZE(&arr[1]));\n\n\t\t \n\t\tfor (i = 0; i < size0; i++) {\n\t\t\tu8 *d = ptr + elem_size * (arr[1].begin + i);\n\t\t\tu8 *s = ptr + elem_size * (arr[0].begin + i);\n\t\t\tsize_t j;\n\n\t\t\tfor (j = 0; j < elem_size; j++)\n\t\t\t\tswap(d[j], s[j]);\n\t\t}\n\n\t\tif (CHUNK_SIZE(&arr[0]) > CHUNK_SIZE(&arr[1])) {\n\t\t\t \n\t\t\tarr[0].begin += size0;\n\t\t} else {\n\t\t\t \n\t\t\tarr[0].begin = arr[1].begin;\n\t\t\tarr[0].end = arr[1].begin + size0 - 1;\n\t\t\tarr[1].begin += size0;\n\t\t}\n\t}\n}\n\nstatic void cio2_fbpt_rearrange(struct cio2_device *cio2, struct cio2_queue *q)\n{\n\tunsigned int i, j;\n\n\tfor (i = 0, j = q->bufs_first; i < CIO2_MAX_BUFFERS;\n\t\ti++, j = (j + 1) % CIO2_MAX_BUFFERS)\n\t\tif (q->bufs[j])\n\t\t\tbreak;\n\n\tif (i == CIO2_MAX_BUFFERS)\n\t\treturn;\n\n\tif (j) {\n\t\tarrange(q->fbpt, sizeof(struct cio2_fbpt_entry) * CIO2_MAX_LOPS,\n\t\t\tCIO2_MAX_BUFFERS, j);\n\t\tarrange(q->bufs, sizeof(struct cio2_buffer *),\n\t\t\tCIO2_MAX_BUFFERS, j);\n\t}\n\n\t \n\tfor (i = 0; i < CIO2_MAX_BUFFERS; i++)\n\t\tcio2_fbpt_entry_enable(cio2, q->fbpt + i * CIO2_MAX_LOPS);\n}\n\nstatic int __maybe_unused cio2_suspend(struct device *dev)\n{\n\tstruct pci_dev *pci_dev = to_pci_dev(dev);\n\tstruct cio2_device *cio2 = pci_get_drvdata(pci_dev);\n\tstruct cio2_queue *q = cio2->cur_queue;\n\tint r;\n\n\tdev_dbg(dev, \"cio2 suspend\\n\");\n\tif (!cio2->streaming)\n\t\treturn 0;\n\n\t \n\tr = v4l2_subdev_call(q->sensor, video, s_stream, 0);\n\tif (r) {\n\t\tdev_err(dev, \"failed to stop sensor streaming\\n\");\n\t\treturn r;\n\t}\n\n\tcio2_hw_exit(cio2, q);\n\tsynchronize_irq(pci_dev->irq);\n\n\tpm_runtime_force_suspend(dev);\n\n\t \n\tcio2_fbpt_rearrange(cio2, q);\n\tq->bufs_first = 0;\n\tq->bufs_next = 0;\n\n\treturn 0;\n}\n\nstatic int __maybe_unused cio2_resume(struct device *dev)\n{\n\tstruct cio2_device *cio2 = dev_get_drvdata(dev);\n\tstruct cio2_queue *q = cio2->cur_queue;\n\tint r;\n\n\tdev_dbg(dev, \"cio2 resume\\n\");\n\tif (!cio2->streaming)\n\t\treturn 0;\n\t \n\tr = pm_runtime_force_resume(dev);\n\tif (r < 0) {\n\t\tdev_err(dev, \"failed to set power %d\\n\", r);\n\t\treturn r;\n\t}\n\n\tr = cio2_hw_init(cio2, q);\n\tif (r) {\n\t\tdev_err(dev, \"fail to init cio2 hw\\n\");\n\t\treturn r;\n\t}\n\n\tr = v4l2_subdev_call(q->sensor, video, s_stream, 1);\n\tif (r) {\n\t\tdev_err(dev, \"fail to start sensor streaming\\n\");\n\t\tcio2_hw_exit(cio2, q);\n\t}\n\n\treturn r;\n}\n\nstatic const struct dev_pm_ops cio2_pm_ops = {\n\tSET_RUNTIME_PM_OPS(&cio2_runtime_suspend, &cio2_runtime_resume, NULL)\n\tSET_SYSTEM_SLEEP_PM_OPS(&cio2_suspend, &cio2_resume)\n};\n\nstatic const struct pci_device_id cio2_pci_id_table[] = {\n\t{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, CIO2_PCI_ID) },\n\t{ }\n};\n\nMODULE_DEVICE_TABLE(pci, cio2_pci_id_table);\n\nstatic struct pci_driver cio2_pci_driver = {\n\t.name = CIO2_NAME,\n\t.id_table = cio2_pci_id_table,\n\t.probe = cio2_pci_probe,\n\t.remove = cio2_pci_remove,\n\t.driver = {\n\t\t.pm = &cio2_pm_ops,\n\t},\n};\n\nmodule_pci_driver(cio2_pci_driver);\n\nMODULE_AUTHOR(\"Tuukka Toivonen <tuukka.toivonen@intel.com>\");\nMODULE_AUTHOR(\"Tianshu Qiu <tian.shu.qiu@intel.com>\");\nMODULE_AUTHOR(\"Jian Xu Zheng\");\nMODULE_AUTHOR(\"Yuning Pu <yuning.pu@intel.com>\");\nMODULE_AUTHOR(\"Yong Zhi <yong.zhi@intel.com>\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_DESCRIPTION(\"IPU3 CIO2 driver\");\nMODULE_IMPORT_NS(INTEL_IPU_BRIDGE);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}