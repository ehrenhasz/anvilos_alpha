{
  "module_name": "videobuf2-dma-sg.c",
  "hash_id": "5fe7a8d0db63d93aee554e60bc7912d0168fa320aeefb3e91b06f864db23468c",
  "original_prompt": "Ingested from linux-6.6.14/drivers/media/common/videobuf2/videobuf2-dma-sg.c",
  "human_readable_source": " \n\n#include <linux/module.h>\n#include <linux/mm.h>\n#include <linux/refcount.h>\n#include <linux/scatterlist.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n\n#include <media/videobuf2-v4l2.h>\n#include <media/videobuf2-memops.h>\n#include <media/videobuf2-dma-sg.h>\n\nstatic int debug;\nmodule_param(debug, int, 0644);\n\n#define dprintk(level, fmt, arg...)\t\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\t\t\\\n\t\tif (debug >= level)\t\t\t\t\t\\\n\t\t\tprintk(KERN_DEBUG \"vb2-dma-sg: \" fmt, ## arg);\t\\\n\t} while (0)\n\nstruct vb2_dma_sg_buf {\n\tstruct device\t\t\t*dev;\n\tvoid\t\t\t\t*vaddr;\n\tstruct page\t\t\t**pages;\n\tstruct frame_vector\t\t*vec;\n\tint\t\t\t\toffset;\n\tenum dma_data_direction\t\tdma_dir;\n\tstruct sg_table\t\t\tsg_table;\n\t \n\tstruct sg_table\t\t\t*dma_sgt;\n\tsize_t\t\t\t\tsize;\n\tunsigned int\t\t\tnum_pages;\n\trefcount_t\t\t\trefcount;\n\tstruct vb2_vmarea_handler\thandler;\n\n\tstruct dma_buf_attachment\t*db_attach;\n\n\tstruct vb2_buffer\t\t*vb;\n};\n\nstatic void vb2_dma_sg_put(void *buf_priv);\n\nstatic int vb2_dma_sg_alloc_compacted(struct vb2_dma_sg_buf *buf,\n\t\tgfp_t gfp_flags)\n{\n\tunsigned int last_page = 0;\n\tunsigned long size = buf->size;\n\n\twhile (size > 0) {\n\t\tstruct page *pages;\n\t\tint order;\n\t\tint i;\n\n\t\torder = get_order(size);\n\t\t \n\t\tif ((PAGE_SIZE << order) > size)\n\t\t\torder--;\n\n\t\tpages = NULL;\n\t\twhile (!pages) {\n\t\t\tpages = alloc_pages(GFP_KERNEL | __GFP_ZERO |\n\t\t\t\t\t__GFP_NOWARN | gfp_flags, order);\n\t\t\tif (pages)\n\t\t\t\tbreak;\n\n\t\t\tif (order == 0) {\n\t\t\t\twhile (last_page--)\n\t\t\t\t\t__free_page(buf->pages[last_page]);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\torder--;\n\t\t}\n\n\t\tsplit_page(pages, order);\n\t\tfor (i = 0; i < (1 << order); i++)\n\t\t\tbuf->pages[last_page++] = &pages[i];\n\n\t\tsize -= PAGE_SIZE << order;\n\t}\n\n\treturn 0;\n}\n\nstatic void *vb2_dma_sg_alloc(struct vb2_buffer *vb, struct device *dev,\n\t\t\t      unsigned long size)\n{\n\tstruct vb2_dma_sg_buf *buf;\n\tstruct sg_table *sgt;\n\tint ret;\n\tint num_pages;\n\n\tif (WARN_ON(!dev) || WARN_ON(!size))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tbuf = kzalloc(sizeof *buf, GFP_KERNEL);\n\tif (!buf)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbuf->vaddr = NULL;\n\tbuf->dma_dir = vb->vb2_queue->dma_dir;\n\tbuf->offset = 0;\n\tbuf->size = size;\n\t \n\tbuf->num_pages = size >> PAGE_SHIFT;\n\tbuf->dma_sgt = &buf->sg_table;\n\n\t \n\tbuf->pages = kvcalloc(buf->num_pages, sizeof(struct page *), GFP_KERNEL);\n\tif (!buf->pages)\n\t\tgoto fail_pages_array_alloc;\n\n\tret = vb2_dma_sg_alloc_compacted(buf, vb->vb2_queue->gfp_flags);\n\tif (ret)\n\t\tgoto fail_pages_alloc;\n\n\tret = sg_alloc_table_from_pages(buf->dma_sgt, buf->pages,\n\t\t\tbuf->num_pages, 0, size, GFP_KERNEL);\n\tif (ret)\n\t\tgoto fail_table_alloc;\n\n\t \n\tbuf->dev = get_device(dev);\n\n\tsgt = &buf->sg_table;\n\t \n\tif (dma_map_sgtable(buf->dev, sgt, buf->dma_dir,\n\t\t\t    DMA_ATTR_SKIP_CPU_SYNC))\n\t\tgoto fail_map;\n\n\tbuf->handler.refcount = &buf->refcount;\n\tbuf->handler.put = vb2_dma_sg_put;\n\tbuf->handler.arg = buf;\n\tbuf->vb = vb;\n\n\trefcount_set(&buf->refcount, 1);\n\n\tdprintk(1, \"%s: Allocated buffer of %d pages\\n\",\n\t\t__func__, buf->num_pages);\n\treturn buf;\n\nfail_map:\n\tput_device(buf->dev);\n\tsg_free_table(buf->dma_sgt);\nfail_table_alloc:\n\tnum_pages = buf->num_pages;\n\twhile (num_pages--)\n\t\t__free_page(buf->pages[num_pages]);\nfail_pages_alloc:\n\tkvfree(buf->pages);\nfail_pages_array_alloc:\n\tkfree(buf);\n\treturn ERR_PTR(-ENOMEM);\n}\n\nstatic void vb2_dma_sg_put(void *buf_priv)\n{\n\tstruct vb2_dma_sg_buf *buf = buf_priv;\n\tstruct sg_table *sgt = &buf->sg_table;\n\tint i = buf->num_pages;\n\n\tif (refcount_dec_and_test(&buf->refcount)) {\n\t\tdprintk(1, \"%s: Freeing buffer of %d pages\\n\", __func__,\n\t\t\tbuf->num_pages);\n\t\tdma_unmap_sgtable(buf->dev, sgt, buf->dma_dir,\n\t\t\t\t  DMA_ATTR_SKIP_CPU_SYNC);\n\t\tif (buf->vaddr)\n\t\t\tvm_unmap_ram(buf->vaddr, buf->num_pages);\n\t\tsg_free_table(buf->dma_sgt);\n\t\twhile (--i >= 0)\n\t\t\t__free_page(buf->pages[i]);\n\t\tkvfree(buf->pages);\n\t\tput_device(buf->dev);\n\t\tkfree(buf);\n\t}\n}\n\nstatic void vb2_dma_sg_prepare(void *buf_priv)\n{\n\tstruct vb2_dma_sg_buf *buf = buf_priv;\n\tstruct sg_table *sgt = buf->dma_sgt;\n\n\tif (buf->vb->skip_cache_sync_on_prepare)\n\t\treturn;\n\n\tdma_sync_sgtable_for_device(buf->dev, sgt, buf->dma_dir);\n}\n\nstatic void vb2_dma_sg_finish(void *buf_priv)\n{\n\tstruct vb2_dma_sg_buf *buf = buf_priv;\n\tstruct sg_table *sgt = buf->dma_sgt;\n\n\tif (buf->vb->skip_cache_sync_on_finish)\n\t\treturn;\n\n\tdma_sync_sgtable_for_cpu(buf->dev, sgt, buf->dma_dir);\n}\n\nstatic void *vb2_dma_sg_get_userptr(struct vb2_buffer *vb, struct device *dev,\n\t\t\t\t    unsigned long vaddr, unsigned long size)\n{\n\tstruct vb2_dma_sg_buf *buf;\n\tstruct sg_table *sgt;\n\tstruct frame_vector *vec;\n\n\tif (WARN_ON(!dev))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tbuf = kzalloc(sizeof *buf, GFP_KERNEL);\n\tif (!buf)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbuf->vaddr = NULL;\n\tbuf->dev = dev;\n\tbuf->dma_dir = vb->vb2_queue->dma_dir;\n\tbuf->offset = vaddr & ~PAGE_MASK;\n\tbuf->size = size;\n\tbuf->dma_sgt = &buf->sg_table;\n\tbuf->vb = vb;\n\tvec = vb2_create_framevec(vaddr, size,\n\t\t\t\t  buf->dma_dir == DMA_FROM_DEVICE ||\n\t\t\t\t  buf->dma_dir == DMA_BIDIRECTIONAL);\n\tif (IS_ERR(vec))\n\t\tgoto userptr_fail_pfnvec;\n\tbuf->vec = vec;\n\n\tbuf->pages = frame_vector_pages(vec);\n\tif (IS_ERR(buf->pages))\n\t\tgoto userptr_fail_sgtable;\n\tbuf->num_pages = frame_vector_count(vec);\n\n\tif (sg_alloc_table_from_pages(buf->dma_sgt, buf->pages,\n\t\t\tbuf->num_pages, buf->offset, size, 0))\n\t\tgoto userptr_fail_sgtable;\n\n\tsgt = &buf->sg_table;\n\t \n\tif (dma_map_sgtable(buf->dev, sgt, buf->dma_dir,\n\t\t\t    DMA_ATTR_SKIP_CPU_SYNC))\n\t\tgoto userptr_fail_map;\n\n\treturn buf;\n\nuserptr_fail_map:\n\tsg_free_table(&buf->sg_table);\nuserptr_fail_sgtable:\n\tvb2_destroy_framevec(vec);\nuserptr_fail_pfnvec:\n\tkfree(buf);\n\treturn ERR_PTR(-ENOMEM);\n}\n\n \nstatic void vb2_dma_sg_put_userptr(void *buf_priv)\n{\n\tstruct vb2_dma_sg_buf *buf = buf_priv;\n\tstruct sg_table *sgt = &buf->sg_table;\n\tint i = buf->num_pages;\n\n\tdprintk(1, \"%s: Releasing userspace buffer of %d pages\\n\",\n\t       __func__, buf->num_pages);\n\tdma_unmap_sgtable(buf->dev, sgt, buf->dma_dir, DMA_ATTR_SKIP_CPU_SYNC);\n\tif (buf->vaddr)\n\t\tvm_unmap_ram(buf->vaddr, buf->num_pages);\n\tsg_free_table(buf->dma_sgt);\n\tif (buf->dma_dir == DMA_FROM_DEVICE ||\n\t    buf->dma_dir == DMA_BIDIRECTIONAL)\n\t\twhile (--i >= 0)\n\t\t\tset_page_dirty_lock(buf->pages[i]);\n\tvb2_destroy_framevec(buf->vec);\n\tkfree(buf);\n}\n\nstatic void *vb2_dma_sg_vaddr(struct vb2_buffer *vb, void *buf_priv)\n{\n\tstruct vb2_dma_sg_buf *buf = buf_priv;\n\tstruct iosys_map map;\n\tint ret;\n\n\tBUG_ON(!buf);\n\n\tif (!buf->vaddr) {\n\t\tif (buf->db_attach) {\n\t\t\tret = dma_buf_vmap_unlocked(buf->db_attach->dmabuf, &map);\n\t\t\tbuf->vaddr = ret ? NULL : map.vaddr;\n\t\t} else {\n\t\t\tbuf->vaddr = vm_map_ram(buf->pages, buf->num_pages, -1);\n\t\t}\n\t}\n\n\t \n\treturn buf->vaddr ? buf->vaddr + buf->offset : NULL;\n}\n\nstatic unsigned int vb2_dma_sg_num_users(void *buf_priv)\n{\n\tstruct vb2_dma_sg_buf *buf = buf_priv;\n\n\treturn refcount_read(&buf->refcount);\n}\n\nstatic int vb2_dma_sg_mmap(void *buf_priv, struct vm_area_struct *vma)\n{\n\tstruct vb2_dma_sg_buf *buf = buf_priv;\n\tint err;\n\n\tif (!buf) {\n\t\tprintk(KERN_ERR \"No memory to map\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = vm_map_pages(vma, buf->pages, buf->num_pages);\n\tif (err) {\n\t\tprintk(KERN_ERR \"Remapping memory, error: %d\\n\", err);\n\t\treturn err;\n\t}\n\n\t \n\tvma->vm_private_data\t= &buf->handler;\n\tvma->vm_ops\t\t= &vb2_common_vm_ops;\n\n\tvma->vm_ops->open(vma);\n\n\treturn 0;\n}\n\n \n \n \n\nstruct vb2_dma_sg_attachment {\n\tstruct sg_table sgt;\n\tenum dma_data_direction dma_dir;\n};\n\nstatic int vb2_dma_sg_dmabuf_ops_attach(struct dma_buf *dbuf,\n\tstruct dma_buf_attachment *dbuf_attach)\n{\n\tstruct vb2_dma_sg_attachment *attach;\n\tunsigned int i;\n\tstruct scatterlist *rd, *wr;\n\tstruct sg_table *sgt;\n\tstruct vb2_dma_sg_buf *buf = dbuf->priv;\n\tint ret;\n\n\tattach = kzalloc(sizeof(*attach), GFP_KERNEL);\n\tif (!attach)\n\t\treturn -ENOMEM;\n\n\tsgt = &attach->sgt;\n\t \n\tret = sg_alloc_table(sgt, buf->dma_sgt->orig_nents, GFP_KERNEL);\n\tif (ret) {\n\t\tkfree(attach);\n\t\treturn -ENOMEM;\n\t}\n\n\trd = buf->dma_sgt->sgl;\n\twr = sgt->sgl;\n\tfor (i = 0; i < sgt->orig_nents; ++i) {\n\t\tsg_set_page(wr, sg_page(rd), rd->length, rd->offset);\n\t\trd = sg_next(rd);\n\t\twr = sg_next(wr);\n\t}\n\n\tattach->dma_dir = DMA_NONE;\n\tdbuf_attach->priv = attach;\n\n\treturn 0;\n}\n\nstatic void vb2_dma_sg_dmabuf_ops_detach(struct dma_buf *dbuf,\n\tstruct dma_buf_attachment *db_attach)\n{\n\tstruct vb2_dma_sg_attachment *attach = db_attach->priv;\n\tstruct sg_table *sgt;\n\n\tif (!attach)\n\t\treturn;\n\n\tsgt = &attach->sgt;\n\n\t \n\tif (attach->dma_dir != DMA_NONE)\n\t\tdma_unmap_sgtable(db_attach->dev, sgt, attach->dma_dir, 0);\n\tsg_free_table(sgt);\n\tkfree(attach);\n\tdb_attach->priv = NULL;\n}\n\nstatic struct sg_table *vb2_dma_sg_dmabuf_ops_map(\n\tstruct dma_buf_attachment *db_attach, enum dma_data_direction dma_dir)\n{\n\tstruct vb2_dma_sg_attachment *attach = db_attach->priv;\n\tstruct sg_table *sgt;\n\n\tsgt = &attach->sgt;\n\t \n\tif (attach->dma_dir == dma_dir)\n\t\treturn sgt;\n\n\t \n\tif (attach->dma_dir != DMA_NONE) {\n\t\tdma_unmap_sgtable(db_attach->dev, sgt, attach->dma_dir, 0);\n\t\tattach->dma_dir = DMA_NONE;\n\t}\n\n\t \n\tif (dma_map_sgtable(db_attach->dev, sgt, dma_dir, 0)) {\n\t\tpr_err(\"failed to map scatterlist\\n\");\n\t\treturn ERR_PTR(-EIO);\n\t}\n\n\tattach->dma_dir = dma_dir;\n\n\treturn sgt;\n}\n\nstatic void vb2_dma_sg_dmabuf_ops_unmap(struct dma_buf_attachment *db_attach,\n\tstruct sg_table *sgt, enum dma_data_direction dma_dir)\n{\n\t \n}\n\nstatic void vb2_dma_sg_dmabuf_ops_release(struct dma_buf *dbuf)\n{\n\t \n\tvb2_dma_sg_put(dbuf->priv);\n}\n\nstatic int\nvb2_dma_sg_dmabuf_ops_begin_cpu_access(struct dma_buf *dbuf,\n\t\t\t\t       enum dma_data_direction direction)\n{\n\tstruct vb2_dma_sg_buf *buf = dbuf->priv;\n\tstruct sg_table *sgt = buf->dma_sgt;\n\n\tdma_sync_sg_for_cpu(buf->dev, sgt->sgl, sgt->nents, buf->dma_dir);\n\treturn 0;\n}\n\nstatic int\nvb2_dma_sg_dmabuf_ops_end_cpu_access(struct dma_buf *dbuf,\n\t\t\t\t     enum dma_data_direction direction)\n{\n\tstruct vb2_dma_sg_buf *buf = dbuf->priv;\n\tstruct sg_table *sgt = buf->dma_sgt;\n\n\tdma_sync_sg_for_device(buf->dev, sgt->sgl, sgt->nents, buf->dma_dir);\n\treturn 0;\n}\n\nstatic int vb2_dma_sg_dmabuf_ops_vmap(struct dma_buf *dbuf,\n\t\t\t\t      struct iosys_map *map)\n{\n\tstruct vb2_dma_sg_buf *buf = dbuf->priv;\n\n\tiosys_map_set_vaddr(map, buf->vaddr);\n\n\treturn 0;\n}\n\nstatic int vb2_dma_sg_dmabuf_ops_mmap(struct dma_buf *dbuf,\n\tstruct vm_area_struct *vma)\n{\n\treturn vb2_dma_sg_mmap(dbuf->priv, vma);\n}\n\nstatic const struct dma_buf_ops vb2_dma_sg_dmabuf_ops = {\n\t.attach = vb2_dma_sg_dmabuf_ops_attach,\n\t.detach = vb2_dma_sg_dmabuf_ops_detach,\n\t.map_dma_buf = vb2_dma_sg_dmabuf_ops_map,\n\t.unmap_dma_buf = vb2_dma_sg_dmabuf_ops_unmap,\n\t.begin_cpu_access = vb2_dma_sg_dmabuf_ops_begin_cpu_access,\n\t.end_cpu_access = vb2_dma_sg_dmabuf_ops_end_cpu_access,\n\t.vmap = vb2_dma_sg_dmabuf_ops_vmap,\n\t.mmap = vb2_dma_sg_dmabuf_ops_mmap,\n\t.release = vb2_dma_sg_dmabuf_ops_release,\n};\n\nstatic struct dma_buf *vb2_dma_sg_get_dmabuf(struct vb2_buffer *vb,\n\t\t\t\t\t     void *buf_priv,\n\t\t\t\t\t     unsigned long flags)\n{\n\tstruct vb2_dma_sg_buf *buf = buf_priv;\n\tstruct dma_buf *dbuf;\n\tDEFINE_DMA_BUF_EXPORT_INFO(exp_info);\n\n\texp_info.ops = &vb2_dma_sg_dmabuf_ops;\n\texp_info.size = buf->size;\n\texp_info.flags = flags;\n\texp_info.priv = buf;\n\n\tif (WARN_ON(!buf->dma_sgt))\n\t\treturn NULL;\n\n\tdbuf = dma_buf_export(&exp_info);\n\tif (IS_ERR(dbuf))\n\t\treturn NULL;\n\n\t \n\trefcount_inc(&buf->refcount);\n\n\treturn dbuf;\n}\n\n \n \n \n\nstatic int vb2_dma_sg_map_dmabuf(void *mem_priv)\n{\n\tstruct vb2_dma_sg_buf *buf = mem_priv;\n\tstruct sg_table *sgt;\n\n\tif (WARN_ON(!buf->db_attach)) {\n\t\tpr_err(\"trying to pin a non attached buffer\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (WARN_ON(buf->dma_sgt)) {\n\t\tpr_err(\"dmabuf buffer is already pinned\\n\");\n\t\treturn 0;\n\t}\n\n\t \n\tsgt = dma_buf_map_attachment_unlocked(buf->db_attach, buf->dma_dir);\n\tif (IS_ERR(sgt)) {\n\t\tpr_err(\"Error getting dmabuf scatterlist\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tbuf->dma_sgt = sgt;\n\tbuf->vaddr = NULL;\n\n\treturn 0;\n}\n\nstatic void vb2_dma_sg_unmap_dmabuf(void *mem_priv)\n{\n\tstruct vb2_dma_sg_buf *buf = mem_priv;\n\tstruct sg_table *sgt = buf->dma_sgt;\n\tstruct iosys_map map = IOSYS_MAP_INIT_VADDR(buf->vaddr);\n\n\tif (WARN_ON(!buf->db_attach)) {\n\t\tpr_err(\"trying to unpin a not attached buffer\\n\");\n\t\treturn;\n\t}\n\n\tif (WARN_ON(!sgt)) {\n\t\tpr_err(\"dmabuf buffer is already unpinned\\n\");\n\t\treturn;\n\t}\n\n\tif (buf->vaddr) {\n\t\tdma_buf_vunmap_unlocked(buf->db_attach->dmabuf, &map);\n\t\tbuf->vaddr = NULL;\n\t}\n\tdma_buf_unmap_attachment_unlocked(buf->db_attach, sgt, buf->dma_dir);\n\n\tbuf->dma_sgt = NULL;\n}\n\nstatic void vb2_dma_sg_detach_dmabuf(void *mem_priv)\n{\n\tstruct vb2_dma_sg_buf *buf = mem_priv;\n\n\t \n\tif (WARN_ON(buf->dma_sgt))\n\t\tvb2_dma_sg_unmap_dmabuf(buf);\n\n\t \n\tdma_buf_detach(buf->db_attach->dmabuf, buf->db_attach);\n\tkfree(buf);\n}\n\nstatic void *vb2_dma_sg_attach_dmabuf(struct vb2_buffer *vb, struct device *dev,\n\t\t\t\t      struct dma_buf *dbuf, unsigned long size)\n{\n\tstruct vb2_dma_sg_buf *buf;\n\tstruct dma_buf_attachment *dba;\n\n\tif (WARN_ON(!dev))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (dbuf->size < size)\n\t\treturn ERR_PTR(-EFAULT);\n\n\tbuf = kzalloc(sizeof(*buf), GFP_KERNEL);\n\tif (!buf)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tbuf->dev = dev;\n\t \n\tdba = dma_buf_attach(dbuf, buf->dev);\n\tif (IS_ERR(dba)) {\n\t\tpr_err(\"failed to attach dmabuf\\n\");\n\t\tkfree(buf);\n\t\treturn dba;\n\t}\n\n\tbuf->dma_dir = vb->vb2_queue->dma_dir;\n\tbuf->size = size;\n\tbuf->db_attach = dba;\n\tbuf->vb = vb;\n\n\treturn buf;\n}\n\nstatic void *vb2_dma_sg_cookie(struct vb2_buffer *vb, void *buf_priv)\n{\n\tstruct vb2_dma_sg_buf *buf = buf_priv;\n\n\treturn buf->dma_sgt;\n}\n\nconst struct vb2_mem_ops vb2_dma_sg_memops = {\n\t.alloc\t\t= vb2_dma_sg_alloc,\n\t.put\t\t= vb2_dma_sg_put,\n\t.get_userptr\t= vb2_dma_sg_get_userptr,\n\t.put_userptr\t= vb2_dma_sg_put_userptr,\n\t.prepare\t= vb2_dma_sg_prepare,\n\t.finish\t\t= vb2_dma_sg_finish,\n\t.vaddr\t\t= vb2_dma_sg_vaddr,\n\t.mmap\t\t= vb2_dma_sg_mmap,\n\t.num_users\t= vb2_dma_sg_num_users,\n\t.get_dmabuf\t= vb2_dma_sg_get_dmabuf,\n\t.map_dmabuf\t= vb2_dma_sg_map_dmabuf,\n\t.unmap_dmabuf\t= vb2_dma_sg_unmap_dmabuf,\n\t.attach_dmabuf\t= vb2_dma_sg_attach_dmabuf,\n\t.detach_dmabuf\t= vb2_dma_sg_detach_dmabuf,\n\t.cookie\t\t= vb2_dma_sg_cookie,\n};\nEXPORT_SYMBOL_GPL(vb2_dma_sg_memops);\n\nMODULE_DESCRIPTION(\"dma scatter/gather memory handling routines for videobuf2\");\nMODULE_AUTHOR(\"Andrzej Pietrasiewicz\");\nMODULE_LICENSE(\"GPL\");\nMODULE_IMPORT_NS(DMA_BUF);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}