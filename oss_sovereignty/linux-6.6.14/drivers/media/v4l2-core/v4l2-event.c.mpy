{
  "module_name": "v4l2-event.c",
  "hash_id": "46836babdc98b0345b92ae129b0e53c560feb1ad9f713c4224b3b6ac564bc70d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/media/v4l2-core/v4l2-event.c",
  "human_readable_source": "\n \n\n#include <media/v4l2-dev.h>\n#include <media/v4l2-fh.h>\n#include <media/v4l2-event.h>\n\n#include <linux/mm.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n\nstatic unsigned int sev_pos(const struct v4l2_subscribed_event *sev, unsigned int idx)\n{\n\tidx += sev->first;\n\treturn idx >= sev->elems ? idx - sev->elems : idx;\n}\n\nstatic int __v4l2_event_dequeue(struct v4l2_fh *fh, struct v4l2_event *event)\n{\n\tstruct v4l2_kevent *kev;\n\tstruct timespec64 ts;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&fh->vdev->fh_lock, flags);\n\n\tif (list_empty(&fh->available)) {\n\t\tspin_unlock_irqrestore(&fh->vdev->fh_lock, flags);\n\t\treturn -ENOENT;\n\t}\n\n\tWARN_ON(fh->navailable == 0);\n\n\tkev = list_first_entry(&fh->available, struct v4l2_kevent, list);\n\tlist_del(&kev->list);\n\tfh->navailable--;\n\n\tkev->event.pending = fh->navailable;\n\t*event = kev->event;\n\tts = ns_to_timespec64(kev->ts);\n\tevent->timestamp.tv_sec = ts.tv_sec;\n\tevent->timestamp.tv_nsec = ts.tv_nsec;\n\tkev->sev->first = sev_pos(kev->sev, 1);\n\tkev->sev->in_use--;\n\n\tspin_unlock_irqrestore(&fh->vdev->fh_lock, flags);\n\n\treturn 0;\n}\n\nint v4l2_event_dequeue(struct v4l2_fh *fh, struct v4l2_event *event,\n\t\t       int nonblocking)\n{\n\tint ret;\n\n\tif (nonblocking)\n\t\treturn __v4l2_event_dequeue(fh, event);\n\n\t \n\tif (fh->vdev->lock)\n\t\tmutex_unlock(fh->vdev->lock);\n\n\tdo {\n\t\tret = wait_event_interruptible(fh->wait,\n\t\t\t\t\t       fh->navailable != 0);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\n\t\tret = __v4l2_event_dequeue(fh, event);\n\t} while (ret == -ENOENT);\n\n\tif (fh->vdev->lock)\n\t\tmutex_lock(fh->vdev->lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(v4l2_event_dequeue);\n\n \nstatic struct v4l2_subscribed_event *v4l2_event_subscribed(\n\t\tstruct v4l2_fh *fh, u32 type, u32 id)\n{\n\tstruct v4l2_subscribed_event *sev;\n\n\tassert_spin_locked(&fh->vdev->fh_lock);\n\n\tlist_for_each_entry(sev, &fh->subscribed, list)\n\t\tif (sev->type == type && sev->id == id)\n\t\t\treturn sev;\n\n\treturn NULL;\n}\n\nstatic void __v4l2_event_queue_fh(struct v4l2_fh *fh,\n\t\t\t\t  const struct v4l2_event *ev, u64 ts)\n{\n\tstruct v4l2_subscribed_event *sev;\n\tstruct v4l2_kevent *kev;\n\tbool copy_payload = true;\n\n\t \n\tsev = v4l2_event_subscribed(fh, ev->type, ev->id);\n\tif (sev == NULL)\n\t\treturn;\n\n\t \n\tfh->sequence++;\n\n\t \n\tif (sev->in_use == sev->elems) {\n\t\t \n\t\tkev = sev->events + sev_pos(sev, 0);\n\t\tlist_del(&kev->list);\n\t\tsev->in_use--;\n\t\tsev->first = sev_pos(sev, 1);\n\t\tfh->navailable--;\n\t\tif (sev->elems == 1) {\n\t\t\tif (sev->ops && sev->ops->replace) {\n\t\t\t\tsev->ops->replace(&kev->event, ev);\n\t\t\t\tcopy_payload = false;\n\t\t\t}\n\t\t} else if (sev->ops && sev->ops->merge) {\n\t\t\tstruct v4l2_kevent *second_oldest =\n\t\t\t\tsev->events + sev_pos(sev, 0);\n\t\t\tsev->ops->merge(&kev->event, &second_oldest->event);\n\t\t}\n\t}\n\n\t \n\tkev = sev->events + sev_pos(sev, sev->in_use);\n\tkev->event.type = ev->type;\n\tif (copy_payload)\n\t\tkev->event.u = ev->u;\n\tkev->event.id = ev->id;\n\tkev->ts = ts;\n\tkev->event.sequence = fh->sequence;\n\tsev->in_use++;\n\tlist_add_tail(&kev->list, &fh->available);\n\n\tfh->navailable++;\n\n\twake_up_all(&fh->wait);\n}\n\nvoid v4l2_event_queue(struct video_device *vdev, const struct v4l2_event *ev)\n{\n\tstruct v4l2_fh *fh;\n\tunsigned long flags;\n\tu64 ts;\n\n\tif (vdev == NULL)\n\t\treturn;\n\n\tts = ktime_get_ns();\n\n\tspin_lock_irqsave(&vdev->fh_lock, flags);\n\n\tlist_for_each_entry(fh, &vdev->fh_list, list)\n\t\t__v4l2_event_queue_fh(fh, ev, ts);\n\n\tspin_unlock_irqrestore(&vdev->fh_lock, flags);\n}\nEXPORT_SYMBOL_GPL(v4l2_event_queue);\n\nvoid v4l2_event_queue_fh(struct v4l2_fh *fh, const struct v4l2_event *ev)\n{\n\tunsigned long flags;\n\tu64 ts = ktime_get_ns();\n\n\tspin_lock_irqsave(&fh->vdev->fh_lock, flags);\n\t__v4l2_event_queue_fh(fh, ev, ts);\n\tspin_unlock_irqrestore(&fh->vdev->fh_lock, flags);\n}\nEXPORT_SYMBOL_GPL(v4l2_event_queue_fh);\n\nint v4l2_event_pending(struct v4l2_fh *fh)\n{\n\treturn fh->navailable;\n}\nEXPORT_SYMBOL_GPL(v4l2_event_pending);\n\nvoid v4l2_event_wake_all(struct video_device *vdev)\n{\n\tstruct v4l2_fh *fh;\n\tunsigned long flags;\n\n\tif (!vdev)\n\t\treturn;\n\n\tspin_lock_irqsave(&vdev->fh_lock, flags);\n\n\tlist_for_each_entry(fh, &vdev->fh_list, list)\n\t\twake_up_all(&fh->wait);\n\n\tspin_unlock_irqrestore(&vdev->fh_lock, flags);\n}\nEXPORT_SYMBOL_GPL(v4l2_event_wake_all);\n\nstatic void __v4l2_event_unsubscribe(struct v4l2_subscribed_event *sev)\n{\n\tstruct v4l2_fh *fh = sev->fh;\n\tunsigned int i;\n\n\tlockdep_assert_held(&fh->subscribe_lock);\n\tassert_spin_locked(&fh->vdev->fh_lock);\n\n\t \n\tfor (i = 0; i < sev->in_use; i++) {\n\t\tlist_del(&sev->events[sev_pos(sev, i)].list);\n\t\tfh->navailable--;\n\t}\n\tlist_del(&sev->list);\n}\n\nint v4l2_event_subscribe(struct v4l2_fh *fh,\n\t\t\t const struct v4l2_event_subscription *sub, unsigned int elems,\n\t\t\t const struct v4l2_subscribed_event_ops *ops)\n{\n\tstruct v4l2_subscribed_event *sev, *found_ev;\n\tunsigned long flags;\n\tunsigned int i;\n\tint ret = 0;\n\n\tif (sub->type == V4L2_EVENT_ALL)\n\t\treturn -EINVAL;\n\n\tif (elems < 1)\n\t\telems = 1;\n\n\tsev = kvzalloc(struct_size(sev, events, elems), GFP_KERNEL);\n\tif (!sev)\n\t\treturn -ENOMEM;\n\tfor (i = 0; i < elems; i++)\n\t\tsev->events[i].sev = sev;\n\tsev->type = sub->type;\n\tsev->id = sub->id;\n\tsev->flags = sub->flags;\n\tsev->fh = fh;\n\tsev->ops = ops;\n\tsev->elems = elems;\n\n\tmutex_lock(&fh->subscribe_lock);\n\n\tspin_lock_irqsave(&fh->vdev->fh_lock, flags);\n\tfound_ev = v4l2_event_subscribed(fh, sub->type, sub->id);\n\tif (!found_ev)\n\t\tlist_add(&sev->list, &fh->subscribed);\n\tspin_unlock_irqrestore(&fh->vdev->fh_lock, flags);\n\n\tif (found_ev) {\n\t\t \n\t\tkvfree(sev);\n\t} else if (sev->ops && sev->ops->add) {\n\t\tret = sev->ops->add(sev, elems);\n\t\tif (ret) {\n\t\t\tspin_lock_irqsave(&fh->vdev->fh_lock, flags);\n\t\t\t__v4l2_event_unsubscribe(sev);\n\t\t\tspin_unlock_irqrestore(&fh->vdev->fh_lock, flags);\n\t\t\tkvfree(sev);\n\t\t}\n\t}\n\n\tmutex_unlock(&fh->subscribe_lock);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(v4l2_event_subscribe);\n\nvoid v4l2_event_unsubscribe_all(struct v4l2_fh *fh)\n{\n\tstruct v4l2_event_subscription sub;\n\tstruct v4l2_subscribed_event *sev;\n\tunsigned long flags;\n\n\tdo {\n\t\tsev = NULL;\n\n\t\tspin_lock_irqsave(&fh->vdev->fh_lock, flags);\n\t\tif (!list_empty(&fh->subscribed)) {\n\t\t\tsev = list_first_entry(&fh->subscribed,\n\t\t\t\t\tstruct v4l2_subscribed_event, list);\n\t\t\tsub.type = sev->type;\n\t\t\tsub.id = sev->id;\n\t\t}\n\t\tspin_unlock_irqrestore(&fh->vdev->fh_lock, flags);\n\t\tif (sev)\n\t\t\tv4l2_event_unsubscribe(fh, &sub);\n\t} while (sev);\n}\nEXPORT_SYMBOL_GPL(v4l2_event_unsubscribe_all);\n\nint v4l2_event_unsubscribe(struct v4l2_fh *fh,\n\t\t\t   const struct v4l2_event_subscription *sub)\n{\n\tstruct v4l2_subscribed_event *sev;\n\tunsigned long flags;\n\n\tif (sub->type == V4L2_EVENT_ALL) {\n\t\tv4l2_event_unsubscribe_all(fh);\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&fh->subscribe_lock);\n\n\tspin_lock_irqsave(&fh->vdev->fh_lock, flags);\n\n\tsev = v4l2_event_subscribed(fh, sub->type, sub->id);\n\tif (sev != NULL)\n\t\t__v4l2_event_unsubscribe(sev);\n\n\tspin_unlock_irqrestore(&fh->vdev->fh_lock, flags);\n\n\tif (sev && sev->ops && sev->ops->del)\n\t\tsev->ops->del(sev);\n\n\tmutex_unlock(&fh->subscribe_lock);\n\n\tkvfree(sev);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(v4l2_event_unsubscribe);\n\nint v4l2_event_subdev_unsubscribe(struct v4l2_subdev *sd, struct v4l2_fh *fh,\n\t\t\t\t  struct v4l2_event_subscription *sub)\n{\n\treturn v4l2_event_unsubscribe(fh, sub);\n}\nEXPORT_SYMBOL_GPL(v4l2_event_subdev_unsubscribe);\n\nstatic void v4l2_event_src_replace(struct v4l2_event *old,\n\t\t\t\tconst struct v4l2_event *new)\n{\n\tu32 old_changes = old->u.src_change.changes;\n\n\told->u.src_change = new->u.src_change;\n\told->u.src_change.changes |= old_changes;\n}\n\nstatic void v4l2_event_src_merge(const struct v4l2_event *old,\n\t\t\t\tstruct v4l2_event *new)\n{\n\tnew->u.src_change.changes |= old->u.src_change.changes;\n}\n\nstatic const struct v4l2_subscribed_event_ops v4l2_event_src_ch_ops = {\n\t.replace = v4l2_event_src_replace,\n\t.merge = v4l2_event_src_merge,\n};\n\nint v4l2_src_change_event_subscribe(struct v4l2_fh *fh,\n\t\t\t\tconst struct v4l2_event_subscription *sub)\n{\n\tif (sub->type == V4L2_EVENT_SOURCE_CHANGE)\n\t\treturn v4l2_event_subscribe(fh, sub, 0, &v4l2_event_src_ch_ops);\n\treturn -EINVAL;\n}\nEXPORT_SYMBOL_GPL(v4l2_src_change_event_subscribe);\n\nint v4l2_src_change_event_subdev_subscribe(struct v4l2_subdev *sd,\n\t\tstruct v4l2_fh *fh, struct v4l2_event_subscription *sub)\n{\n\treturn v4l2_src_change_event_subscribe(fh, sub);\n}\nEXPORT_SYMBOL_GPL(v4l2_src_change_event_subdev_subscribe);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}