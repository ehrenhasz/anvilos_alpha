{
  "module_name": "memdev.c",
  "hash_id": "ee85738bef9fc68261190d23c95d63907bdafa4ba8687b083b6ff315d7e1edd9",
  "original_prompt": "Ingested from linux-6.6.14/drivers/cxl/core/memdev.c",
  "human_readable_source": "\n \n\n#include <linux/io-64-nonatomic-lo-hi.h>\n#include <linux/firmware.h>\n#include <linux/device.h>\n#include <linux/slab.h>\n#include <linux/idr.h>\n#include <linux/pci.h>\n#include <cxlmem.h>\n#include \"trace.h\"\n#include \"core.h\"\n\nstatic DECLARE_RWSEM(cxl_memdev_rwsem);\n\n \n#define CXL_MEM_MAX_DEVS 65536\n\nstatic int cxl_mem_major;\nstatic DEFINE_IDA(cxl_memdev_ida);\n\nstatic void cxl_memdev_release(struct device *dev)\n{\n\tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n\n\tida_free(&cxl_memdev_ida, cxlmd->id);\n\tkfree(cxlmd);\n}\n\nstatic char *cxl_memdev_devnode(const struct device *dev, umode_t *mode, kuid_t *uid,\n\t\t\t\tkgid_t *gid)\n{\n\treturn kasprintf(GFP_KERNEL, \"cxl/%s\", dev_name(dev));\n}\n\nstatic ssize_t firmware_version_show(struct device *dev,\n\t\t\t\t     struct device_attribute *attr, char *buf)\n{\n\tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n\tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n\tstruct cxl_memdev_state *mds = to_cxl_memdev_state(cxlds);\n\n\tif (!mds)\n\t\treturn sysfs_emit(buf, \"\\n\");\n\treturn sysfs_emit(buf, \"%.16s\\n\", mds->firmware_version);\n}\nstatic DEVICE_ATTR_RO(firmware_version);\n\nstatic ssize_t payload_max_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n\tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n\tstruct cxl_memdev_state *mds = to_cxl_memdev_state(cxlds);\n\n\tif (!mds)\n\t\treturn sysfs_emit(buf, \"\\n\");\n\treturn sysfs_emit(buf, \"%zu\\n\", mds->payload_size);\n}\nstatic DEVICE_ATTR_RO(payload_max);\n\nstatic ssize_t label_storage_size_show(struct device *dev,\n\t\t\t\t       struct device_attribute *attr, char *buf)\n{\n\tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n\tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n\tstruct cxl_memdev_state *mds = to_cxl_memdev_state(cxlds);\n\n\tif (!mds)\n\t\treturn sysfs_emit(buf, \"\\n\");\n\treturn sysfs_emit(buf, \"%zu\\n\", mds->lsa_size);\n}\nstatic DEVICE_ATTR_RO(label_storage_size);\n\nstatic ssize_t ram_size_show(struct device *dev, struct device_attribute *attr,\n\t\t\t     char *buf)\n{\n\tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n\tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n\tunsigned long long len = resource_size(&cxlds->ram_res);\n\n\treturn sysfs_emit(buf, \"%#llx\\n\", len);\n}\n\nstatic struct device_attribute dev_attr_ram_size =\n\t__ATTR(size, 0444, ram_size_show, NULL);\n\nstatic ssize_t pmem_size_show(struct device *dev, struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n\tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n\tunsigned long long len = resource_size(&cxlds->pmem_res);\n\n\treturn sysfs_emit(buf, \"%#llx\\n\", len);\n}\n\nstatic struct device_attribute dev_attr_pmem_size =\n\t__ATTR(size, 0444, pmem_size_show, NULL);\n\nstatic ssize_t serial_show(struct device *dev, struct device_attribute *attr,\n\t\t\t   char *buf)\n{\n\tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n\tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n\n\treturn sysfs_emit(buf, \"%#llx\\n\", cxlds->serial);\n}\nstatic DEVICE_ATTR_RO(serial);\n\nstatic ssize_t numa_node_show(struct device *dev, struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\treturn sprintf(buf, \"%d\\n\", dev_to_node(dev));\n}\nstatic DEVICE_ATTR_RO(numa_node);\n\nstatic ssize_t security_state_show(struct device *dev,\n\t\t\t\t   struct device_attribute *attr,\n\t\t\t\t   char *buf)\n{\n\tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n\tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n\tstruct cxl_memdev_state *mds = to_cxl_memdev_state(cxlds);\n\tunsigned long state = mds->security.state;\n\tint rc = 0;\n\n\t \n\tmutex_lock(&mds->mbox_mutex);\n\tif (mds->security.sanitize_active)\n\t\trc = sysfs_emit(buf, \"sanitize\\n\");\n\tmutex_unlock(&mds->mbox_mutex);\n\tif (rc)\n\t\treturn rc;\n\n\tif (!(state & CXL_PMEM_SEC_STATE_USER_PASS_SET))\n\t\treturn sysfs_emit(buf, \"disabled\\n\");\n\tif (state & CXL_PMEM_SEC_STATE_FROZEN ||\n\t    state & CXL_PMEM_SEC_STATE_MASTER_PLIMIT ||\n\t    state & CXL_PMEM_SEC_STATE_USER_PLIMIT)\n\t\treturn sysfs_emit(buf, \"frozen\\n\");\n\tif (state & CXL_PMEM_SEC_STATE_LOCKED)\n\t\treturn sysfs_emit(buf, \"locked\\n\");\n\telse\n\t\treturn sysfs_emit(buf, \"unlocked\\n\");\n}\nstatic struct device_attribute dev_attr_security_state =\n\t__ATTR(state, 0444, security_state_show, NULL);\n\nstatic ssize_t security_sanitize_store(struct device *dev,\n\t\t\t\t       struct device_attribute *attr,\n\t\t\t\t       const char *buf, size_t len)\n{\n\tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n\tbool sanitize;\n\tssize_t rc;\n\n\tif (kstrtobool(buf, &sanitize) || !sanitize)\n\t\treturn -EINVAL;\n\n\trc = cxl_mem_sanitize(cxlmd, CXL_MBOX_OP_SANITIZE);\n\tif (rc)\n\t\treturn rc;\n\n\treturn len;\n}\nstatic struct device_attribute dev_attr_security_sanitize =\n\t__ATTR(sanitize, 0200, NULL, security_sanitize_store);\n\nstatic ssize_t security_erase_store(struct device *dev,\n\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t    const char *buf, size_t len)\n{\n\tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n\tssize_t rc;\n\tbool erase;\n\n\tif (kstrtobool(buf, &erase) || !erase)\n\t\treturn -EINVAL;\n\n\trc = cxl_mem_sanitize(cxlmd, CXL_MBOX_OP_SECURE_ERASE);\n\tif (rc)\n\t\treturn rc;\n\n\treturn len;\n}\nstatic struct device_attribute dev_attr_security_erase =\n\t__ATTR(erase, 0200, NULL, security_erase_store);\n\nstatic int cxl_get_poison_by_memdev(struct cxl_memdev *cxlmd)\n{\n\tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n\tu64 offset, length;\n\tint rc = 0;\n\n\t \n\tif (resource_size(&cxlds->pmem_res)) {\n\t\toffset = cxlds->pmem_res.start;\n\t\tlength = resource_size(&cxlds->pmem_res);\n\t\trc = cxl_mem_get_poison(cxlmd, offset, length, NULL);\n\t\tif (rc)\n\t\t\treturn rc;\n\t}\n\tif (resource_size(&cxlds->ram_res)) {\n\t\toffset = cxlds->ram_res.start;\n\t\tlength = resource_size(&cxlds->ram_res);\n\t\trc = cxl_mem_get_poison(cxlmd, offset, length, NULL);\n\t\t \n\t\tif (rc == -EFAULT)\n\t\t\trc = 0;\n\t}\n\treturn rc;\n}\n\nint cxl_trigger_poison_list(struct cxl_memdev *cxlmd)\n{\n\tstruct cxl_port *port;\n\tint rc;\n\n\tport = cxlmd->endpoint;\n\tif (!port || !is_cxl_endpoint(port))\n\t\treturn -EINVAL;\n\n\trc = down_read_interruptible(&cxl_region_rwsem);\n\tif (rc)\n\t\treturn rc;\n\n\trc = down_read_interruptible(&cxl_dpa_rwsem);\n\tif (rc) {\n\t\tup_read(&cxl_region_rwsem);\n\t\treturn rc;\n\t}\n\n\tif (cxl_num_decoders_committed(port) == 0) {\n\t\t \n\t\trc = cxl_get_poison_by_memdev(cxlmd);\n\t} else {\n\t\t \n\t\trc =  cxl_get_poison_by_endpoint(port);\n\t}\n\tup_read(&cxl_dpa_rwsem);\n\tup_read(&cxl_region_rwsem);\n\n\treturn rc;\n}\nEXPORT_SYMBOL_NS_GPL(cxl_trigger_poison_list, CXL);\n\nstruct cxl_dpa_to_region_context {\n\tstruct cxl_region *cxlr;\n\tu64 dpa;\n};\n\nstatic int __cxl_dpa_to_region(struct device *dev, void *arg)\n{\n\tstruct cxl_dpa_to_region_context *ctx = arg;\n\tstruct cxl_endpoint_decoder *cxled;\n\tu64 dpa = ctx->dpa;\n\n\tif (!is_endpoint_decoder(dev))\n\t\treturn 0;\n\n\tcxled = to_cxl_endpoint_decoder(dev);\n\tif (!cxled->dpa_res || !resource_size(cxled->dpa_res))\n\t\treturn 0;\n\n\tif (dpa > cxled->dpa_res->end || dpa < cxled->dpa_res->start)\n\t\treturn 0;\n\n\tdev_dbg(dev, \"dpa:0x%llx mapped in region:%s\\n\", dpa,\n\t\tdev_name(&cxled->cxld.region->dev));\n\n\tctx->cxlr = cxled->cxld.region;\n\n\treturn 1;\n}\n\nstatic struct cxl_region *cxl_dpa_to_region(struct cxl_memdev *cxlmd, u64 dpa)\n{\n\tstruct cxl_dpa_to_region_context ctx;\n\tstruct cxl_port *port;\n\n\tctx = (struct cxl_dpa_to_region_context) {\n\t\t.dpa = dpa,\n\t};\n\tport = cxlmd->endpoint;\n\tif (port && is_cxl_endpoint(port) && cxl_num_decoders_committed(port))\n\t\tdevice_for_each_child(&port->dev, &ctx, __cxl_dpa_to_region);\n\n\treturn ctx.cxlr;\n}\n\nstatic int cxl_validate_poison_dpa(struct cxl_memdev *cxlmd, u64 dpa)\n{\n\tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n\n\tif (!IS_ENABLED(CONFIG_DEBUG_FS))\n\t\treturn 0;\n\n\tif (!resource_size(&cxlds->dpa_res)) {\n\t\tdev_dbg(cxlds->dev, \"device has no dpa resource\\n\");\n\t\treturn -EINVAL;\n\t}\n\tif (dpa < cxlds->dpa_res.start || dpa > cxlds->dpa_res.end) {\n\t\tdev_dbg(cxlds->dev, \"dpa:0x%llx not in resource:%pR\\n\",\n\t\t\tdpa, &cxlds->dpa_res);\n\t\treturn -EINVAL;\n\t}\n\tif (!IS_ALIGNED(dpa, 64)) {\n\t\tdev_dbg(cxlds->dev, \"dpa:0x%llx is not 64-byte aligned\\n\", dpa);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint cxl_inject_poison(struct cxl_memdev *cxlmd, u64 dpa)\n{\n\tstruct cxl_memdev_state *mds = to_cxl_memdev_state(cxlmd->cxlds);\n\tstruct cxl_mbox_inject_poison inject;\n\tstruct cxl_poison_record record;\n\tstruct cxl_mbox_cmd mbox_cmd;\n\tstruct cxl_region *cxlr;\n\tint rc;\n\n\tif (!IS_ENABLED(CONFIG_DEBUG_FS))\n\t\treturn 0;\n\n\trc = down_read_interruptible(&cxl_region_rwsem);\n\tif (rc)\n\t\treturn rc;\n\n\trc = down_read_interruptible(&cxl_dpa_rwsem);\n\tif (rc) {\n\t\tup_read(&cxl_region_rwsem);\n\t\treturn rc;\n\t}\n\n\trc = cxl_validate_poison_dpa(cxlmd, dpa);\n\tif (rc)\n\t\tgoto out;\n\n\tinject.address = cpu_to_le64(dpa);\n\tmbox_cmd = (struct cxl_mbox_cmd) {\n\t\t.opcode = CXL_MBOX_OP_INJECT_POISON,\n\t\t.size_in = sizeof(inject),\n\t\t.payload_in = &inject,\n\t};\n\trc = cxl_internal_send_cmd(mds, &mbox_cmd);\n\tif (rc)\n\t\tgoto out;\n\n\tcxlr = cxl_dpa_to_region(cxlmd, dpa);\n\tif (cxlr)\n\t\tdev_warn_once(mds->cxlds.dev,\n\t\t\t      \"poison inject dpa:%#llx region: %s\\n\", dpa,\n\t\t\t      dev_name(&cxlr->dev));\n\n\trecord = (struct cxl_poison_record) {\n\t\t.address = cpu_to_le64(dpa),\n\t\t.length = cpu_to_le32(1),\n\t};\n\ttrace_cxl_poison(cxlmd, cxlr, &record, 0, 0, CXL_POISON_TRACE_INJECT);\nout:\n\tup_read(&cxl_dpa_rwsem);\n\tup_read(&cxl_region_rwsem);\n\n\treturn rc;\n}\nEXPORT_SYMBOL_NS_GPL(cxl_inject_poison, CXL);\n\nint cxl_clear_poison(struct cxl_memdev *cxlmd, u64 dpa)\n{\n\tstruct cxl_memdev_state *mds = to_cxl_memdev_state(cxlmd->cxlds);\n\tstruct cxl_mbox_clear_poison clear;\n\tstruct cxl_poison_record record;\n\tstruct cxl_mbox_cmd mbox_cmd;\n\tstruct cxl_region *cxlr;\n\tint rc;\n\n\tif (!IS_ENABLED(CONFIG_DEBUG_FS))\n\t\treturn 0;\n\n\trc = down_read_interruptible(&cxl_region_rwsem);\n\tif (rc)\n\t\treturn rc;\n\n\trc = down_read_interruptible(&cxl_dpa_rwsem);\n\tif (rc) {\n\t\tup_read(&cxl_region_rwsem);\n\t\treturn rc;\n\t}\n\n\trc = cxl_validate_poison_dpa(cxlmd, dpa);\n\tif (rc)\n\t\tgoto out;\n\n\t \n\tclear = (struct cxl_mbox_clear_poison) {\n\t\t.address = cpu_to_le64(dpa)\n\t};\n\n\tmbox_cmd = (struct cxl_mbox_cmd) {\n\t\t.opcode = CXL_MBOX_OP_CLEAR_POISON,\n\t\t.size_in = sizeof(clear),\n\t\t.payload_in = &clear,\n\t};\n\n\trc = cxl_internal_send_cmd(mds, &mbox_cmd);\n\tif (rc)\n\t\tgoto out;\n\n\tcxlr = cxl_dpa_to_region(cxlmd, dpa);\n\tif (cxlr)\n\t\tdev_warn_once(mds->cxlds.dev,\n\t\t\t      \"poison clear dpa:%#llx region: %s\\n\", dpa,\n\t\t\t      dev_name(&cxlr->dev));\n\n\trecord = (struct cxl_poison_record) {\n\t\t.address = cpu_to_le64(dpa),\n\t\t.length = cpu_to_le32(1),\n\t};\n\ttrace_cxl_poison(cxlmd, cxlr, &record, 0, 0, CXL_POISON_TRACE_CLEAR);\nout:\n\tup_read(&cxl_dpa_rwsem);\n\tup_read(&cxl_region_rwsem);\n\n\treturn rc;\n}\nEXPORT_SYMBOL_NS_GPL(cxl_clear_poison, CXL);\n\nstatic struct attribute *cxl_memdev_attributes[] = {\n\t&dev_attr_serial.attr,\n\t&dev_attr_firmware_version.attr,\n\t&dev_attr_payload_max.attr,\n\t&dev_attr_label_storage_size.attr,\n\t&dev_attr_numa_node.attr,\n\tNULL,\n};\n\nstatic struct attribute *cxl_memdev_pmem_attributes[] = {\n\t&dev_attr_pmem_size.attr,\n\tNULL,\n};\n\nstatic struct attribute *cxl_memdev_ram_attributes[] = {\n\t&dev_attr_ram_size.attr,\n\tNULL,\n};\n\nstatic struct attribute *cxl_memdev_security_attributes[] = {\n\t&dev_attr_security_state.attr,\n\t&dev_attr_security_sanitize.attr,\n\t&dev_attr_security_erase.attr,\n\tNULL,\n};\n\nstatic umode_t cxl_memdev_visible(struct kobject *kobj, struct attribute *a,\n\t\t\t\t  int n)\n{\n\tif (!IS_ENABLED(CONFIG_NUMA) && a == &dev_attr_numa_node.attr)\n\t\treturn 0;\n\treturn a->mode;\n}\n\nstatic struct attribute_group cxl_memdev_attribute_group = {\n\t.attrs = cxl_memdev_attributes,\n\t.is_visible = cxl_memdev_visible,\n};\n\nstatic struct attribute_group cxl_memdev_ram_attribute_group = {\n\t.name = \"ram\",\n\t.attrs = cxl_memdev_ram_attributes,\n};\n\nstatic struct attribute_group cxl_memdev_pmem_attribute_group = {\n\t.name = \"pmem\",\n\t.attrs = cxl_memdev_pmem_attributes,\n};\n\nstatic umode_t cxl_memdev_security_visible(struct kobject *kobj,\n\t\t\t\t\t   struct attribute *a, int n)\n{\n\tstruct device *dev = kobj_to_dev(kobj);\n\tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n\tstruct cxl_memdev_state *mds = to_cxl_memdev_state(cxlmd->cxlds);\n\n\tif (a == &dev_attr_security_sanitize.attr &&\n\t    !test_bit(CXL_SEC_ENABLED_SANITIZE, mds->security.enabled_cmds))\n\t\treturn 0;\n\n\tif (a == &dev_attr_security_erase.attr &&\n\t    !test_bit(CXL_SEC_ENABLED_SECURE_ERASE, mds->security.enabled_cmds))\n\t\treturn 0;\n\n\treturn a->mode;\n}\n\nstatic struct attribute_group cxl_memdev_security_attribute_group = {\n\t.name = \"security\",\n\t.attrs = cxl_memdev_security_attributes,\n\t.is_visible = cxl_memdev_security_visible,\n};\n\nstatic const struct attribute_group *cxl_memdev_attribute_groups[] = {\n\t&cxl_memdev_attribute_group,\n\t&cxl_memdev_ram_attribute_group,\n\t&cxl_memdev_pmem_attribute_group,\n\t&cxl_memdev_security_attribute_group,\n\tNULL,\n};\n\nstatic const struct device_type cxl_memdev_type = {\n\t.name = \"cxl_memdev\",\n\t.release = cxl_memdev_release,\n\t.devnode = cxl_memdev_devnode,\n\t.groups = cxl_memdev_attribute_groups,\n};\n\nbool is_cxl_memdev(const struct device *dev)\n{\n\treturn dev->type == &cxl_memdev_type;\n}\nEXPORT_SYMBOL_NS_GPL(is_cxl_memdev, CXL);\n\n \nvoid set_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n\t\t\t\tunsigned long *cmds)\n{\n\tdown_write(&cxl_memdev_rwsem);\n\tbitmap_or(mds->exclusive_cmds, mds->exclusive_cmds, cmds,\n\t\t  CXL_MEM_COMMAND_ID_MAX);\n\tup_write(&cxl_memdev_rwsem);\n}\nEXPORT_SYMBOL_NS_GPL(set_exclusive_cxl_commands, CXL);\n\n \nvoid clear_exclusive_cxl_commands(struct cxl_memdev_state *mds,\n\t\t\t\t  unsigned long *cmds)\n{\n\tdown_write(&cxl_memdev_rwsem);\n\tbitmap_andnot(mds->exclusive_cmds, mds->exclusive_cmds, cmds,\n\t\t      CXL_MEM_COMMAND_ID_MAX);\n\tup_write(&cxl_memdev_rwsem);\n}\nEXPORT_SYMBOL_NS_GPL(clear_exclusive_cxl_commands, CXL);\n\nstatic void cxl_memdev_shutdown(struct device *dev)\n{\n\tstruct cxl_memdev *cxlmd = to_cxl_memdev(dev);\n\n\tdown_write(&cxl_memdev_rwsem);\n\tcxlmd->cxlds = NULL;\n\tup_write(&cxl_memdev_rwsem);\n}\n\nstatic void cxl_memdev_unregister(void *_cxlmd)\n{\n\tstruct cxl_memdev *cxlmd = _cxlmd;\n\tstruct device *dev = &cxlmd->dev;\n\n\tcdev_device_del(&cxlmd->cdev, dev);\n\tcxl_memdev_shutdown(dev);\n\tput_device(dev);\n}\n\nstatic void detach_memdev(struct work_struct *work)\n{\n\tstruct cxl_memdev *cxlmd;\n\n\tcxlmd = container_of(work, typeof(*cxlmd), detach_work);\n\tdevice_release_driver(&cxlmd->dev);\n\tput_device(&cxlmd->dev);\n}\n\nstatic struct lock_class_key cxl_memdev_key;\n\nstatic struct cxl_memdev *cxl_memdev_alloc(struct cxl_dev_state *cxlds,\n\t\t\t\t\t   const struct file_operations *fops)\n{\n\tstruct cxl_memdev *cxlmd;\n\tstruct device *dev;\n\tstruct cdev *cdev;\n\tint rc;\n\n\tcxlmd = kzalloc(sizeof(*cxlmd), GFP_KERNEL);\n\tif (!cxlmd)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\trc = ida_alloc_max(&cxl_memdev_ida, CXL_MEM_MAX_DEVS - 1, GFP_KERNEL);\n\tif (rc < 0)\n\t\tgoto err;\n\tcxlmd->id = rc;\n\tcxlmd->depth = -1;\n\n\tdev = &cxlmd->dev;\n\tdevice_initialize(dev);\n\tlockdep_set_class(&dev->mutex, &cxl_memdev_key);\n\tdev->parent = cxlds->dev;\n\tdev->bus = &cxl_bus_type;\n\tdev->devt = MKDEV(cxl_mem_major, cxlmd->id);\n\tdev->type = &cxl_memdev_type;\n\tdevice_set_pm_not_required(dev);\n\tINIT_WORK(&cxlmd->detach_work, detach_memdev);\n\n\tcdev = &cxlmd->cdev;\n\tcdev_init(cdev, fops);\n\treturn cxlmd;\n\nerr:\n\tkfree(cxlmd);\n\treturn ERR_PTR(rc);\n}\n\nstatic long __cxl_memdev_ioctl(struct cxl_memdev *cxlmd, unsigned int cmd,\n\t\t\t       unsigned long arg)\n{\n\tswitch (cmd) {\n\tcase CXL_MEM_QUERY_COMMANDS:\n\t\treturn cxl_query_cmd(cxlmd, (void __user *)arg);\n\tcase CXL_MEM_SEND_COMMAND:\n\t\treturn cxl_send_cmd(cxlmd, (void __user *)arg);\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n}\n\nstatic long cxl_memdev_ioctl(struct file *file, unsigned int cmd,\n\t\t\t     unsigned long arg)\n{\n\tstruct cxl_memdev *cxlmd = file->private_data;\n\tstruct cxl_dev_state *cxlds;\n\tint rc = -ENXIO;\n\n\tdown_read(&cxl_memdev_rwsem);\n\tcxlds = cxlmd->cxlds;\n\tif (cxlds && cxlds->type == CXL_DEVTYPE_CLASSMEM)\n\t\trc = __cxl_memdev_ioctl(cxlmd, cmd, arg);\n\tup_read(&cxl_memdev_rwsem);\n\n\treturn rc;\n}\n\nstatic int cxl_memdev_open(struct inode *inode, struct file *file)\n{\n\tstruct cxl_memdev *cxlmd =\n\t\tcontainer_of(inode->i_cdev, typeof(*cxlmd), cdev);\n\n\tget_device(&cxlmd->dev);\n\tfile->private_data = cxlmd;\n\n\treturn 0;\n}\n\nstatic int cxl_memdev_release_file(struct inode *inode, struct file *file)\n{\n\tstruct cxl_memdev *cxlmd =\n\t\tcontainer_of(inode->i_cdev, typeof(*cxlmd), cdev);\n\n\tput_device(&cxlmd->dev);\n\n\treturn 0;\n}\n\n \nstatic int cxl_mem_get_fw_info(struct cxl_memdev_state *mds)\n{\n\tstruct cxl_mbox_get_fw_info info;\n\tstruct cxl_mbox_cmd mbox_cmd;\n\tint rc;\n\n\tmbox_cmd = (struct cxl_mbox_cmd) {\n\t\t.opcode = CXL_MBOX_OP_GET_FW_INFO,\n\t\t.size_out = sizeof(info),\n\t\t.payload_out = &info,\n\t};\n\n\trc = cxl_internal_send_cmd(mds, &mbox_cmd);\n\tif (rc < 0)\n\t\treturn rc;\n\n\tmds->fw.num_slots = info.num_slots;\n\tmds->fw.cur_slot = FIELD_GET(CXL_FW_INFO_SLOT_INFO_CUR_MASK,\n\t\t\t\t       info.slot_info);\n\n\treturn 0;\n}\n\n \nstatic int cxl_mem_activate_fw(struct cxl_memdev_state *mds, int slot)\n{\n\tstruct cxl_mbox_activate_fw activate;\n\tstruct cxl_mbox_cmd mbox_cmd;\n\n\tif (slot == 0 || slot > mds->fw.num_slots)\n\t\treturn -EINVAL;\n\n\tmbox_cmd = (struct cxl_mbox_cmd) {\n\t\t.opcode = CXL_MBOX_OP_ACTIVATE_FW,\n\t\t.size_in = sizeof(activate),\n\t\t.payload_in = &activate,\n\t};\n\n\t \n\tactivate.action = CXL_FW_ACTIVATE_OFFLINE;\n\tactivate.slot = slot;\n\n\treturn cxl_internal_send_cmd(mds, &mbox_cmd);\n}\n\n \nstatic int cxl_mem_abort_fw_xfer(struct cxl_memdev_state *mds)\n{\n\tstruct cxl_mbox_transfer_fw *transfer;\n\tstruct cxl_mbox_cmd mbox_cmd;\n\tint rc;\n\n\ttransfer = kzalloc(struct_size(transfer, data, 0), GFP_KERNEL);\n\tif (!transfer)\n\t\treturn -ENOMEM;\n\n\t \n\tmbox_cmd = (struct cxl_mbox_cmd) {\n\t\t.opcode = CXL_MBOX_OP_TRANSFER_FW,\n\t\t.size_in = sizeof(*transfer),\n\t\t.payload_in = transfer,\n\t\t.poll_interval_ms = 1000,\n\t\t.poll_count = 30,\n\t};\n\n\ttransfer->action = CXL_FW_TRANSFER_ACTION_ABORT;\n\n\trc = cxl_internal_send_cmd(mds, &mbox_cmd);\n\tkfree(transfer);\n\treturn rc;\n}\n\nstatic void cxl_fw_cleanup(struct fw_upload *fwl)\n{\n\tstruct cxl_memdev_state *mds = fwl->dd_handle;\n\n\tmds->fw.next_slot = 0;\n}\n\nstatic int cxl_fw_do_cancel(struct fw_upload *fwl)\n{\n\tstruct cxl_memdev_state *mds = fwl->dd_handle;\n\tstruct cxl_dev_state *cxlds = &mds->cxlds;\n\tstruct cxl_memdev *cxlmd = cxlds->cxlmd;\n\tint rc;\n\n\trc = cxl_mem_abort_fw_xfer(mds);\n\tif (rc < 0)\n\t\tdev_err(&cxlmd->dev, \"Error aborting FW transfer: %d\\n\", rc);\n\n\treturn FW_UPLOAD_ERR_CANCELED;\n}\n\nstatic enum fw_upload_err cxl_fw_prepare(struct fw_upload *fwl, const u8 *data,\n\t\t\t\t\t u32 size)\n{\n\tstruct cxl_memdev_state *mds = fwl->dd_handle;\n\tstruct cxl_mbox_transfer_fw *transfer;\n\n\tif (!size)\n\t\treturn FW_UPLOAD_ERR_INVALID_SIZE;\n\n\tmds->fw.oneshot = struct_size(transfer, data, size) <\n\t\t\t    mds->payload_size;\n\n\tif (cxl_mem_get_fw_info(mds))\n\t\treturn FW_UPLOAD_ERR_HW_ERROR;\n\n\t \n\tif (test_and_clear_bit(CXL_FW_CANCEL, mds->fw.state))\n\t\treturn FW_UPLOAD_ERR_CANCELED;\n\n\treturn FW_UPLOAD_ERR_NONE;\n}\n\nstatic enum fw_upload_err cxl_fw_write(struct fw_upload *fwl, const u8 *data,\n\t\t\t\t       u32 offset, u32 size, u32 *written)\n{\n\tstruct cxl_memdev_state *mds = fwl->dd_handle;\n\tstruct cxl_dev_state *cxlds = &mds->cxlds;\n\tstruct cxl_memdev *cxlmd = cxlds->cxlmd;\n\tstruct cxl_mbox_transfer_fw *transfer;\n\tstruct cxl_mbox_cmd mbox_cmd;\n\tu32 cur_size, remaining;\n\tsize_t size_in;\n\tint rc;\n\n\t*written = 0;\n\n\t \n\tif (!IS_ALIGNED(offset, CXL_FW_TRANSFER_ALIGNMENT)) {\n\t\tdev_err(&cxlmd->dev,\n\t\t\t\"misaligned offset for FW transfer slice (%u)\\n\",\n\t\t\toffset);\n\t\treturn FW_UPLOAD_ERR_RW_ERROR;\n\t}\n\n\t \n\tcur_size = min_t(size_t, size, mds->payload_size - sizeof(*transfer));\n\n\tremaining = size - cur_size;\n\tsize_in = struct_size(transfer, data, cur_size);\n\n\tif (test_and_clear_bit(CXL_FW_CANCEL, mds->fw.state))\n\t\treturn cxl_fw_do_cancel(fwl);\n\n\t \n\tmds->fw.next_slot = (mds->fw.cur_slot % mds->fw.num_slots) + 1;\n\n\t \n\ttransfer = kzalloc(size_in, GFP_KERNEL);\n\tif (!transfer)\n\t\treturn FW_UPLOAD_ERR_RW_ERROR;\n\n\ttransfer->offset = cpu_to_le32(offset / CXL_FW_TRANSFER_ALIGNMENT);\n\tmemcpy(transfer->data, data + offset, cur_size);\n\tif (mds->fw.oneshot) {\n\t\ttransfer->action = CXL_FW_TRANSFER_ACTION_FULL;\n\t\ttransfer->slot = mds->fw.next_slot;\n\t} else {\n\t\tif (offset == 0) {\n\t\t\ttransfer->action = CXL_FW_TRANSFER_ACTION_INITIATE;\n\t\t} else if (remaining == 0) {\n\t\t\ttransfer->action = CXL_FW_TRANSFER_ACTION_END;\n\t\t\ttransfer->slot = mds->fw.next_slot;\n\t\t} else {\n\t\t\ttransfer->action = CXL_FW_TRANSFER_ACTION_CONTINUE;\n\t\t}\n\t}\n\n\tmbox_cmd = (struct cxl_mbox_cmd) {\n\t\t.opcode = CXL_MBOX_OP_TRANSFER_FW,\n\t\t.size_in = size_in,\n\t\t.payload_in = transfer,\n\t\t.poll_interval_ms = 1000,\n\t\t.poll_count = 30,\n\t};\n\n\trc = cxl_internal_send_cmd(mds, &mbox_cmd);\n\tif (rc < 0) {\n\t\trc = FW_UPLOAD_ERR_RW_ERROR;\n\t\tgoto out_free;\n\t}\n\n\t*written = cur_size;\n\n\t \n\tif (mds->fw.oneshot || remaining == 0) {\n\t\tdev_dbg(&cxlmd->dev, \"Activating firmware slot: %d\\n\",\n\t\t\tmds->fw.next_slot);\n\t\trc = cxl_mem_activate_fw(mds, mds->fw.next_slot);\n\t\tif (rc < 0) {\n\t\t\tdev_err(&cxlmd->dev, \"Error activating firmware: %d\\n\",\n\t\t\t\trc);\n\t\t\trc = FW_UPLOAD_ERR_HW_ERROR;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\trc = FW_UPLOAD_ERR_NONE;\n\nout_free:\n\tkfree(transfer);\n\treturn rc;\n}\n\nstatic enum fw_upload_err cxl_fw_poll_complete(struct fw_upload *fwl)\n{\n\tstruct cxl_memdev_state *mds = fwl->dd_handle;\n\n\t \n\tif (test_and_clear_bit(CXL_FW_CANCEL, mds->fw.state))\n\t\treturn cxl_fw_do_cancel(fwl);\n\n\treturn FW_UPLOAD_ERR_NONE;\n}\n\nstatic void cxl_fw_cancel(struct fw_upload *fwl)\n{\n\tstruct cxl_memdev_state *mds = fwl->dd_handle;\n\n\tset_bit(CXL_FW_CANCEL, mds->fw.state);\n}\n\nstatic const struct fw_upload_ops cxl_memdev_fw_ops = {\n        .prepare = cxl_fw_prepare,\n        .write = cxl_fw_write,\n        .poll_complete = cxl_fw_poll_complete,\n        .cancel = cxl_fw_cancel,\n        .cleanup = cxl_fw_cleanup,\n};\n\nstatic void cxl_remove_fw_upload(void *fwl)\n{\n\tfirmware_upload_unregister(fwl);\n}\n\nint devm_cxl_setup_fw_upload(struct device *host, struct cxl_memdev_state *mds)\n{\n\tstruct cxl_dev_state *cxlds = &mds->cxlds;\n\tstruct device *dev = &cxlds->cxlmd->dev;\n\tstruct fw_upload *fwl;\n\n\tif (!test_bit(CXL_MEM_COMMAND_ID_GET_FW_INFO, mds->enabled_cmds))\n\t\treturn 0;\n\n\tfwl = firmware_upload_register(THIS_MODULE, dev, dev_name(dev),\n\t\t\t\t       &cxl_memdev_fw_ops, mds);\n\tif (IS_ERR(fwl))\n\t\treturn PTR_ERR(fwl);\n\treturn devm_add_action_or_reset(host, cxl_remove_fw_upload, fwl);\n}\nEXPORT_SYMBOL_NS_GPL(devm_cxl_setup_fw_upload, CXL);\n\nstatic const struct file_operations cxl_memdev_fops = {\n\t.owner = THIS_MODULE,\n\t.unlocked_ioctl = cxl_memdev_ioctl,\n\t.open = cxl_memdev_open,\n\t.release = cxl_memdev_release_file,\n\t.compat_ioctl = compat_ptr_ioctl,\n\t.llseek = noop_llseek,\n};\n\nstruct cxl_memdev *devm_cxl_add_memdev(struct device *host,\n\t\t\t\t       struct cxl_dev_state *cxlds)\n{\n\tstruct cxl_memdev *cxlmd;\n\tstruct device *dev;\n\tstruct cdev *cdev;\n\tint rc;\n\n\tcxlmd = cxl_memdev_alloc(cxlds, &cxl_memdev_fops);\n\tif (IS_ERR(cxlmd))\n\t\treturn cxlmd;\n\n\tdev = &cxlmd->dev;\n\trc = dev_set_name(dev, \"mem%d\", cxlmd->id);\n\tif (rc)\n\t\tgoto err;\n\n\t \n\tcxlmd->cxlds = cxlds;\n\tcxlds->cxlmd = cxlmd;\n\n\tcdev = &cxlmd->cdev;\n\trc = cdev_device_add(cdev, dev);\n\tif (rc)\n\t\tgoto err;\n\n\trc = devm_add_action_or_reset(host, cxl_memdev_unregister, cxlmd);\n\tif (rc)\n\t\treturn ERR_PTR(rc);\n\treturn cxlmd;\n\nerr:\n\t \n\tcxl_memdev_shutdown(dev);\n\tput_device(dev);\n\treturn ERR_PTR(rc);\n}\nEXPORT_SYMBOL_NS_GPL(devm_cxl_add_memdev, CXL);\n\nstatic void sanitize_teardown_notifier(void *data)\n{\n\tstruct cxl_memdev_state *mds = data;\n\tstruct kernfs_node *state;\n\n\t \n\tmutex_lock(&mds->mbox_mutex);\n\tstate = mds->security.sanitize_node;\n\tmds->security.sanitize_node = NULL;\n\tmutex_unlock(&mds->mbox_mutex);\n\n\tcancel_delayed_work_sync(&mds->security.poll_dwork);\n\tsysfs_put(state);\n}\n\nint devm_cxl_sanitize_setup_notifier(struct device *host,\n\t\t\t\t     struct cxl_memdev *cxlmd)\n{\n\tstruct cxl_dev_state *cxlds = cxlmd->cxlds;\n\tstruct cxl_memdev_state *mds = to_cxl_memdev_state(cxlds);\n\tstruct kernfs_node *sec;\n\n\tif (!test_bit(CXL_SEC_ENABLED_SANITIZE, mds->security.enabled_cmds))\n\t\treturn 0;\n\n\t \n\tsec = sysfs_get_dirent(cxlmd->dev.kobj.sd, \"security\");\n\tif (!sec)\n\t\treturn -ENOENT;\n\tmds->security.sanitize_node = sysfs_get_dirent(sec, \"state\");\n\tsysfs_put(sec);\n\tif (!mds->security.sanitize_node)\n\t\treturn -ENOENT;\n\n\treturn devm_add_action_or_reset(host, sanitize_teardown_notifier, mds);\n}\nEXPORT_SYMBOL_NS_GPL(devm_cxl_sanitize_setup_notifier, CXL);\n\n__init int cxl_memdev_init(void)\n{\n\tdev_t devt;\n\tint rc;\n\n\trc = alloc_chrdev_region(&devt, 0, CXL_MEM_MAX_DEVS, \"cxl\");\n\tif (rc)\n\t\treturn rc;\n\n\tcxl_mem_major = MAJOR(devt);\n\n\treturn 0;\n}\n\nvoid cxl_memdev_exit(void)\n{\n\tunregister_chrdev_region(MKDEV(cxl_mem_major, 0), CXL_MEM_MAX_DEVS);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}