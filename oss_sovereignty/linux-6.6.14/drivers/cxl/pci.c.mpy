{
  "module_name": "pci.c",
  "hash_id": "fb43df39e2cba55d3639cfd53b2829f00fc0bd1a5e4e77f302304efc7cc8b029",
  "original_prompt": "Ingested from linux-6.6.14/drivers/cxl/pci.c",
  "human_readable_source": "\n \n#include <linux/io-64-nonatomic-lo-hi.h>\n#include <linux/moduleparam.h>\n#include <linux/module.h>\n#include <linux/delay.h>\n#include <linux/sizes.h>\n#include <linux/mutex.h>\n#include <linux/list.h>\n#include <linux/pci.h>\n#include <linux/aer.h>\n#include <linux/io.h>\n#include \"cxlmem.h\"\n#include \"cxlpci.h\"\n#include \"cxl.h\"\n#include \"pmu.h\"\n\n \n\n#define cxl_doorbell_busy(cxlds)                                                \\\n\t(readl((cxlds)->regs.mbox + CXLDEV_MBOX_CTRL_OFFSET) &                  \\\n\t CXLDEV_MBOX_CTRL_DOORBELL)\n\n \n#define CXL_MAILBOX_TIMEOUT_MS (2 * HZ)\n\n \nstatic unsigned short mbox_ready_timeout = 60;\nmodule_param(mbox_ready_timeout, ushort, 0644);\nMODULE_PARM_DESC(mbox_ready_timeout, \"seconds to wait for mailbox ready\");\n\nstatic int cxl_pci_mbox_wait_for_doorbell(struct cxl_dev_state *cxlds)\n{\n\tconst unsigned long start = jiffies;\n\tunsigned long end = start;\n\n\twhile (cxl_doorbell_busy(cxlds)) {\n\t\tend = jiffies;\n\n\t\tif (time_after(end, start + CXL_MAILBOX_TIMEOUT_MS)) {\n\t\t\t \n\t\t\tif (!cxl_doorbell_busy(cxlds))\n\t\t\t\tbreak;\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\t\tcpu_relax();\n\t}\n\n\tdev_dbg(cxlds->dev, \"Doorbell wait took %dms\",\n\t\tjiffies_to_msecs(end) - jiffies_to_msecs(start));\n\treturn 0;\n}\n\n#define cxl_err(dev, status, msg)                                        \\\n\tdev_err_ratelimited(dev, msg \", device state %s%s\\n\",                  \\\n\t\t\t    status & CXLMDEV_DEV_FATAL ? \" fatal\" : \"\",        \\\n\t\t\t    status & CXLMDEV_FW_HALT ? \" firmware-halt\" : \"\")\n\n#define cxl_cmd_err(dev, cmd, status, msg)                               \\\n\tdev_err_ratelimited(dev, msg \" (opcode: %#x), device state %s%s\\n\",    \\\n\t\t\t    (cmd)->opcode,                                     \\\n\t\t\t    status & CXLMDEV_DEV_FATAL ? \" fatal\" : \"\",        \\\n\t\t\t    status & CXLMDEV_FW_HALT ? \" firmware-halt\" : \"\")\n\nstruct cxl_dev_id {\n\tstruct cxl_dev_state *cxlds;\n};\n\nstatic int cxl_request_irq(struct cxl_dev_state *cxlds, int irq,\n\t\t\t   irq_handler_t handler, irq_handler_t thread_fn)\n{\n\tstruct device *dev = cxlds->dev;\n\tstruct cxl_dev_id *dev_id;\n\n\t \n\tdev_id = devm_kzalloc(dev, sizeof(*dev_id), GFP_KERNEL);\n\tif (!dev_id)\n\t\treturn -ENOMEM;\n\tdev_id->cxlds = cxlds;\n\n\treturn devm_request_threaded_irq(dev, irq, handler, thread_fn,\n\t\t\t\t\t IRQF_SHARED | IRQF_ONESHOT,\n\t\t\t\t\t NULL, dev_id);\n}\n\nstatic bool cxl_mbox_background_complete(struct cxl_dev_state *cxlds)\n{\n\tu64 reg;\n\n\treg = readq(cxlds->regs.mbox + CXLDEV_MBOX_BG_CMD_STATUS_OFFSET);\n\treturn FIELD_GET(CXLDEV_MBOX_BG_CMD_COMMAND_PCT_MASK, reg) == 100;\n}\n\nstatic irqreturn_t cxl_pci_mbox_irq(int irq, void *id)\n{\n\tu64 reg;\n\tu16 opcode;\n\tstruct cxl_dev_id *dev_id = id;\n\tstruct cxl_dev_state *cxlds = dev_id->cxlds;\n\tstruct cxl_memdev_state *mds = to_cxl_memdev_state(cxlds);\n\n\tif (!cxl_mbox_background_complete(cxlds))\n\t\treturn IRQ_NONE;\n\n\treg = readq(cxlds->regs.mbox + CXLDEV_MBOX_BG_CMD_STATUS_OFFSET);\n\topcode = FIELD_GET(CXLDEV_MBOX_BG_CMD_COMMAND_OPCODE_MASK, reg);\n\tif (opcode == CXL_MBOX_OP_SANITIZE) {\n\t\tmutex_lock(&mds->mbox_mutex);\n\t\tif (mds->security.sanitize_node)\n\t\t\tmod_delayed_work(system_wq, &mds->security.poll_dwork, 0);\n\t\tmutex_unlock(&mds->mbox_mutex);\n\t} else {\n\t\t \n\t\trcuwait_wake_up(&mds->mbox_wait);\n\t}\n\n\treturn IRQ_HANDLED;\n}\n\n \nstatic void cxl_mbox_sanitize_work(struct work_struct *work)\n{\n\tstruct cxl_memdev_state *mds =\n\t\tcontainer_of(work, typeof(*mds), security.poll_dwork.work);\n\tstruct cxl_dev_state *cxlds = &mds->cxlds;\n\n\tmutex_lock(&mds->mbox_mutex);\n\tif (cxl_mbox_background_complete(cxlds)) {\n\t\tmds->security.poll_tmo_secs = 0;\n\t\tif (mds->security.sanitize_node)\n\t\t\tsysfs_notify_dirent(mds->security.sanitize_node);\n\t\tmds->security.sanitize_active = false;\n\n\t\tdev_dbg(cxlds->dev, \"Sanitization operation ended\\n\");\n\t} else {\n\t\tint timeout = mds->security.poll_tmo_secs + 10;\n\n\t\tmds->security.poll_tmo_secs = min(15 * 60, timeout);\n\t\tschedule_delayed_work(&mds->security.poll_dwork, timeout * HZ);\n\t}\n\tmutex_unlock(&mds->mbox_mutex);\n}\n\n \nstatic int __cxl_pci_mbox_send_cmd(struct cxl_memdev_state *mds,\n\t\t\t\t   struct cxl_mbox_cmd *mbox_cmd)\n{\n\tstruct cxl_dev_state *cxlds = &mds->cxlds;\n\tvoid __iomem *payload = cxlds->regs.mbox + CXLDEV_MBOX_PAYLOAD_OFFSET;\n\tstruct device *dev = cxlds->dev;\n\tu64 cmd_reg, status_reg;\n\tsize_t out_len;\n\tint rc;\n\n\tlockdep_assert_held(&mds->mbox_mutex);\n\n\t \n\n\t \n\tif (cxl_doorbell_busy(cxlds)) {\n\t\tu64 md_status =\n\t\t\treadq(cxlds->regs.memdev + CXLMDEV_STATUS_OFFSET);\n\n\t\tcxl_cmd_err(cxlds->dev, mbox_cmd, md_status,\n\t\t\t    \"mailbox queue busy\");\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tif (mds->security.poll_tmo_secs > 0) {\n\t\tif (mbox_cmd->opcode != CXL_MBOX_OP_GET_HEALTH_INFO)\n\t\t\treturn -EBUSY;\n\t}\n\n\tcmd_reg = FIELD_PREP(CXLDEV_MBOX_CMD_COMMAND_OPCODE_MASK,\n\t\t\t     mbox_cmd->opcode);\n\tif (mbox_cmd->size_in) {\n\t\tif (WARN_ON(!mbox_cmd->payload_in))\n\t\t\treturn -EINVAL;\n\n\t\tcmd_reg |= FIELD_PREP(CXLDEV_MBOX_CMD_PAYLOAD_LENGTH_MASK,\n\t\t\t\t      mbox_cmd->size_in);\n\t\tmemcpy_toio(payload, mbox_cmd->payload_in, mbox_cmd->size_in);\n\t}\n\n\t \n\twriteq(cmd_reg, cxlds->regs.mbox + CXLDEV_MBOX_CMD_OFFSET);\n\n\t \n\tdev_dbg(dev, \"Sending command: 0x%04x\\n\", mbox_cmd->opcode);\n\twritel(CXLDEV_MBOX_CTRL_DOORBELL,\n\t       cxlds->regs.mbox + CXLDEV_MBOX_CTRL_OFFSET);\n\n\t \n\trc = cxl_pci_mbox_wait_for_doorbell(cxlds);\n\tif (rc == -ETIMEDOUT) {\n\t\tu64 md_status = readq(cxlds->regs.memdev + CXLMDEV_STATUS_OFFSET);\n\n\t\tcxl_cmd_err(cxlds->dev, mbox_cmd, md_status, \"mailbox timeout\");\n\t\treturn rc;\n\t}\n\n\t \n\tstatus_reg = readq(cxlds->regs.mbox + CXLDEV_MBOX_STATUS_OFFSET);\n\tmbox_cmd->return_code =\n\t\tFIELD_GET(CXLDEV_MBOX_STATUS_RET_CODE_MASK, status_reg);\n\n\t \n\tif (mbox_cmd->return_code == CXL_MBOX_CMD_RC_BACKGROUND) {\n\t\tu64 bg_status_reg;\n\t\tint i, timeout;\n\n\t\t \n\t\tif (mbox_cmd->opcode == CXL_MBOX_OP_SANITIZE) {\n\t\t\tif (mds->security.sanitize_active)\n\t\t\t\treturn -EBUSY;\n\n\t\t\t \n\t\t\ttimeout = 1;\n\t\t\tmds->security.poll_tmo_secs = timeout;\n\t\t\tmds->security.sanitize_active = true;\n\t\t\tschedule_delayed_work(&mds->security.poll_dwork,\n\t\t\t\t\t      timeout * HZ);\n\t\t\tdev_dbg(dev, \"Sanitization operation started\\n\");\n\t\t\tgoto success;\n\t\t}\n\n\t\tdev_dbg(dev, \"Mailbox background operation (0x%04x) started\\n\",\n\t\t\tmbox_cmd->opcode);\n\n\t\ttimeout = mbox_cmd->poll_interval_ms;\n\t\tfor (i = 0; i < mbox_cmd->poll_count; i++) {\n\t\t\tif (rcuwait_wait_event_timeout(&mds->mbox_wait,\n\t\t\t\t       cxl_mbox_background_complete(cxlds),\n\t\t\t\t       TASK_UNINTERRUPTIBLE,\n\t\t\t\t       msecs_to_jiffies(timeout)) > 0)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (!cxl_mbox_background_complete(cxlds)) {\n\t\t\tdev_err(dev, \"timeout waiting for background (%d ms)\\n\",\n\t\t\t\ttimeout * mbox_cmd->poll_count);\n\t\t\treturn -ETIMEDOUT;\n\t\t}\n\n\t\tbg_status_reg = readq(cxlds->regs.mbox +\n\t\t\t\t      CXLDEV_MBOX_BG_CMD_STATUS_OFFSET);\n\t\tmbox_cmd->return_code =\n\t\t\tFIELD_GET(CXLDEV_MBOX_BG_CMD_COMMAND_RC_MASK,\n\t\t\t\t  bg_status_reg);\n\t\tdev_dbg(dev,\n\t\t\t\"Mailbox background operation (0x%04x) completed\\n\",\n\t\t\tmbox_cmd->opcode);\n\t}\n\n\tif (mbox_cmd->return_code != CXL_MBOX_CMD_RC_SUCCESS) {\n\t\tdev_dbg(dev, \"Mailbox operation had an error: %s\\n\",\n\t\t\tcxl_mbox_cmd_rc2str(mbox_cmd));\n\t\treturn 0;  \n\t}\n\nsuccess:\n\t \n\tcmd_reg = readq(cxlds->regs.mbox + CXLDEV_MBOX_CMD_OFFSET);\n\tout_len = FIELD_GET(CXLDEV_MBOX_CMD_PAYLOAD_LENGTH_MASK, cmd_reg);\n\n\t \n\tif (out_len && mbox_cmd->payload_out) {\n\t\t \n\t\tsize_t n;\n\n\t\tn = min3(mbox_cmd->size_out, mds->payload_size, out_len);\n\t\tmemcpy_fromio(mbox_cmd->payload_out, payload, n);\n\t\tmbox_cmd->size_out = n;\n\t} else {\n\t\tmbox_cmd->size_out = 0;\n\t}\n\n\treturn 0;\n}\n\nstatic int cxl_pci_mbox_send(struct cxl_memdev_state *mds,\n\t\t\t     struct cxl_mbox_cmd *cmd)\n{\n\tint rc;\n\n\tmutex_lock_io(&mds->mbox_mutex);\n\trc = __cxl_pci_mbox_send_cmd(mds, cmd);\n\tmutex_unlock(&mds->mbox_mutex);\n\n\treturn rc;\n}\n\nstatic int cxl_pci_setup_mailbox(struct cxl_memdev_state *mds)\n{\n\tstruct cxl_dev_state *cxlds = &mds->cxlds;\n\tconst int cap = readl(cxlds->regs.mbox + CXLDEV_MBOX_CAPS_OFFSET);\n\tstruct device *dev = cxlds->dev;\n\tunsigned long timeout;\n\tint irq, msgnum;\n\tu64 md_status;\n\tu32 ctrl;\n\n\ttimeout = jiffies + mbox_ready_timeout * HZ;\n\tdo {\n\t\tmd_status = readq(cxlds->regs.memdev + CXLMDEV_STATUS_OFFSET);\n\t\tif (md_status & CXLMDEV_MBOX_IF_READY)\n\t\t\tbreak;\n\t\tif (msleep_interruptible(100))\n\t\t\tbreak;\n\t} while (!time_after(jiffies, timeout));\n\n\tif (!(md_status & CXLMDEV_MBOX_IF_READY)) {\n\t\tcxl_err(dev, md_status, \"timeout awaiting mailbox ready\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\t \n\tif (cxl_pci_mbox_wait_for_doorbell(cxlds) != 0) {\n\t\tcxl_err(dev, md_status, \"timeout awaiting mailbox idle\");\n\t\treturn -ETIMEDOUT;\n\t}\n\n\tmds->mbox_send = cxl_pci_mbox_send;\n\tmds->payload_size =\n\t\t1 << FIELD_GET(CXLDEV_MBOX_CAP_PAYLOAD_SIZE_MASK, cap);\n\n\t \n\tmds->payload_size = min_t(size_t, mds->payload_size, SZ_1M);\n\tif (mds->payload_size < 256) {\n\t\tdev_err(dev, \"Mailbox is too small (%zub)\",\n\t\t\tmds->payload_size);\n\t\treturn -ENXIO;\n\t}\n\n\tdev_dbg(dev, \"Mailbox payload sized %zu\", mds->payload_size);\n\n\trcuwait_init(&mds->mbox_wait);\n\tINIT_DELAYED_WORK(&mds->security.poll_dwork, cxl_mbox_sanitize_work);\n\n\t \n\tif (!(cap & CXLDEV_MBOX_CAP_BG_CMD_IRQ))\n\t\treturn 0;\n\n\tmsgnum = FIELD_GET(CXLDEV_MBOX_CAP_IRQ_MSGNUM_MASK, cap);\n\tirq = pci_irq_vector(to_pci_dev(cxlds->dev), msgnum);\n\tif (irq < 0)\n\t\treturn 0;\n\n\tif (cxl_request_irq(cxlds, irq, NULL, cxl_pci_mbox_irq))\n\t\treturn 0;\n\n\tdev_dbg(cxlds->dev, \"Mailbox interrupts enabled\\n\");\n\t \n\tctrl = readl(cxlds->regs.mbox + CXLDEV_MBOX_CTRL_OFFSET);\n\tctrl |= CXLDEV_MBOX_CTRL_BG_CMD_IRQ;\n\twritel(ctrl, cxlds->regs.mbox + CXLDEV_MBOX_CTRL_OFFSET);\n\n\treturn 0;\n}\n\n \nstatic bool is_cxl_restricted(struct pci_dev *pdev)\n{\n\treturn pci_pcie_type(pdev) == PCI_EXP_TYPE_RC_END;\n}\n\nstatic int cxl_rcrb_get_comp_regs(struct pci_dev *pdev,\n\t\t\t\t  struct cxl_register_map *map)\n{\n\tstruct cxl_port *port;\n\tstruct cxl_dport *dport;\n\tresource_size_t component_reg_phys;\n\n\t*map = (struct cxl_register_map) {\n\t\t.host = &pdev->dev,\n\t\t.resource = CXL_RESOURCE_NONE,\n\t};\n\n\tport = cxl_pci_find_port(pdev, &dport);\n\tif (!port)\n\t\treturn -EPROBE_DEFER;\n\n\tcomponent_reg_phys = cxl_rcd_component_reg_phys(&pdev->dev, dport);\n\n\tput_device(&port->dev);\n\n\tif (component_reg_phys == CXL_RESOURCE_NONE)\n\t\treturn -ENXIO;\n\n\tmap->resource = component_reg_phys;\n\tmap->reg_type = CXL_REGLOC_RBI_COMPONENT;\n\tmap->max_size = CXL_COMPONENT_REG_BLOCK_SIZE;\n\n\treturn 0;\n}\n\nstatic int cxl_pci_setup_regs(struct pci_dev *pdev, enum cxl_regloc_type type,\n\t\t\t      struct cxl_register_map *map)\n{\n\tint rc;\n\n\trc = cxl_find_regblock(pdev, type, map);\n\n\t \n\tif (rc && type == CXL_REGLOC_RBI_COMPONENT && is_cxl_restricted(pdev))\n\t\trc = cxl_rcrb_get_comp_regs(pdev, map);\n\n\tif (rc)\n\t\treturn rc;\n\n\treturn cxl_setup_regs(map);\n}\n\nstatic int cxl_pci_ras_unmask(struct pci_dev *pdev)\n{\n\tstruct cxl_dev_state *cxlds = pci_get_drvdata(pdev);\n\tvoid __iomem *addr;\n\tu32 orig_val, val, mask;\n\tu16 cap;\n\tint rc;\n\n\tif (!cxlds->regs.ras) {\n\t\tdev_dbg(&pdev->dev, \"No RAS registers.\\n\");\n\t\treturn 0;\n\t}\n\n\t \n\tif (!pcie_aer_is_native(pdev))\n\t\treturn 0;\n\n\trc = pcie_capability_read_word(pdev, PCI_EXP_DEVCTL, &cap);\n\tif (rc)\n\t\treturn rc;\n\n\tif (cap & PCI_EXP_DEVCTL_URRE) {\n\t\taddr = cxlds->regs.ras + CXL_RAS_UNCORRECTABLE_MASK_OFFSET;\n\t\torig_val = readl(addr);\n\n\t\tmask = CXL_RAS_UNCORRECTABLE_MASK_MASK |\n\t\t       CXL_RAS_UNCORRECTABLE_MASK_F256B_MASK;\n\t\tval = orig_val & ~mask;\n\t\twritel(val, addr);\n\t\tdev_dbg(&pdev->dev,\n\t\t\t\"Uncorrectable RAS Errors Mask: %#x -> %#x\\n\",\n\t\t\torig_val, val);\n\t}\n\n\tif (cap & PCI_EXP_DEVCTL_CERE) {\n\t\taddr = cxlds->regs.ras + CXL_RAS_CORRECTABLE_MASK_OFFSET;\n\t\torig_val = readl(addr);\n\t\tval = orig_val & ~CXL_RAS_CORRECTABLE_MASK_MASK;\n\t\twritel(val, addr);\n\t\tdev_dbg(&pdev->dev, \"Correctable RAS Errors Mask: %#x -> %#x\\n\",\n\t\t\torig_val, val);\n\t}\n\n\treturn 0;\n}\n\nstatic void free_event_buf(void *buf)\n{\n\tkvfree(buf);\n}\n\n \nstatic int cxl_mem_alloc_event_buf(struct cxl_memdev_state *mds)\n{\n\tstruct cxl_get_event_payload *buf;\n\n\tbuf = kvmalloc(mds->payload_size, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\tmds->event.buf = buf;\n\n\treturn devm_add_action_or_reset(mds->cxlds.dev, free_event_buf, buf);\n}\n\nstatic int cxl_alloc_irq_vectors(struct pci_dev *pdev)\n{\n\tint nvecs;\n\n\t \n\tnvecs = pci_alloc_irq_vectors(pdev, 1, CXL_PCI_DEFAULT_MAX_VECTORS,\n\t\t\t\t      PCI_IRQ_MSIX | PCI_IRQ_MSI);\n\tif (nvecs < 1) {\n\t\tdev_dbg(&pdev->dev, \"Failed to alloc irq vectors: %d\\n\", nvecs);\n\t\treturn -ENXIO;\n\t}\n\treturn 0;\n}\n\nstatic irqreturn_t cxl_event_thread(int irq, void *id)\n{\n\tstruct cxl_dev_id *dev_id = id;\n\tstruct cxl_dev_state *cxlds = dev_id->cxlds;\n\tstruct cxl_memdev_state *mds = to_cxl_memdev_state(cxlds);\n\tu32 status;\n\n\tdo {\n\t\t \n\t\tstatus = readl(cxlds->regs.status + CXLDEV_DEV_EVENT_STATUS_OFFSET);\n\t\t \n\t\tstatus &= CXLDEV_EVENT_STATUS_ALL;\n\t\tif (!status)\n\t\t\tbreak;\n\t\tcxl_mem_get_event_records(mds, status);\n\t\tcond_resched();\n\t} while (status);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int cxl_event_req_irq(struct cxl_dev_state *cxlds, u8 setting)\n{\n\tstruct pci_dev *pdev = to_pci_dev(cxlds->dev);\n\tint irq;\n\n\tif (FIELD_GET(CXLDEV_EVENT_INT_MODE_MASK, setting) != CXL_INT_MSI_MSIX)\n\t\treturn -ENXIO;\n\n\tirq =  pci_irq_vector(pdev,\n\t\t\t      FIELD_GET(CXLDEV_EVENT_INT_MSGNUM_MASK, setting));\n\tif (irq < 0)\n\t\treturn irq;\n\n\treturn cxl_request_irq(cxlds, irq, NULL, cxl_event_thread);\n}\n\nstatic int cxl_event_get_int_policy(struct cxl_memdev_state *mds,\n\t\t\t\t    struct cxl_event_interrupt_policy *policy)\n{\n\tstruct cxl_mbox_cmd mbox_cmd = {\n\t\t.opcode = CXL_MBOX_OP_GET_EVT_INT_POLICY,\n\t\t.payload_out = policy,\n\t\t.size_out = sizeof(*policy),\n\t};\n\tint rc;\n\n\trc = cxl_internal_send_cmd(mds, &mbox_cmd);\n\tif (rc < 0)\n\t\tdev_err(mds->cxlds.dev,\n\t\t\t\"Failed to get event interrupt policy : %d\", rc);\n\n\treturn rc;\n}\n\nstatic int cxl_event_config_msgnums(struct cxl_memdev_state *mds,\n\t\t\t\t    struct cxl_event_interrupt_policy *policy)\n{\n\tstruct cxl_mbox_cmd mbox_cmd;\n\tint rc;\n\n\t*policy = (struct cxl_event_interrupt_policy) {\n\t\t.info_settings = CXL_INT_MSI_MSIX,\n\t\t.warn_settings = CXL_INT_MSI_MSIX,\n\t\t.failure_settings = CXL_INT_MSI_MSIX,\n\t\t.fatal_settings = CXL_INT_MSI_MSIX,\n\t};\n\n\tmbox_cmd = (struct cxl_mbox_cmd) {\n\t\t.opcode = CXL_MBOX_OP_SET_EVT_INT_POLICY,\n\t\t.payload_in = policy,\n\t\t.size_in = sizeof(*policy),\n\t};\n\n\trc = cxl_internal_send_cmd(mds, &mbox_cmd);\n\tif (rc < 0) {\n\t\tdev_err(mds->cxlds.dev, \"Failed to set event interrupt policy : %d\",\n\t\t\trc);\n\t\treturn rc;\n\t}\n\n\t \n\treturn cxl_event_get_int_policy(mds, policy);\n}\n\nstatic int cxl_event_irqsetup(struct cxl_memdev_state *mds)\n{\n\tstruct cxl_dev_state *cxlds = &mds->cxlds;\n\tstruct cxl_event_interrupt_policy policy;\n\tint rc;\n\n\trc = cxl_event_config_msgnums(mds, &policy);\n\tif (rc)\n\t\treturn rc;\n\n\trc = cxl_event_req_irq(cxlds, policy.info_settings);\n\tif (rc) {\n\t\tdev_err(cxlds->dev, \"Failed to get interrupt for event Info log\\n\");\n\t\treturn rc;\n\t}\n\n\trc = cxl_event_req_irq(cxlds, policy.warn_settings);\n\tif (rc) {\n\t\tdev_err(cxlds->dev, \"Failed to get interrupt for event Warn log\\n\");\n\t\treturn rc;\n\t}\n\n\trc = cxl_event_req_irq(cxlds, policy.failure_settings);\n\tif (rc) {\n\t\tdev_err(cxlds->dev, \"Failed to get interrupt for event Failure log\\n\");\n\t\treturn rc;\n\t}\n\n\trc = cxl_event_req_irq(cxlds, policy.fatal_settings);\n\tif (rc) {\n\t\tdev_err(cxlds->dev, \"Failed to get interrupt for event Fatal log\\n\");\n\t\treturn rc;\n\t}\n\n\treturn 0;\n}\n\nstatic bool cxl_event_int_is_fw(u8 setting)\n{\n\tu8 mode = FIELD_GET(CXLDEV_EVENT_INT_MODE_MASK, setting);\n\n\treturn mode == CXL_INT_FW;\n}\n\nstatic int cxl_event_config(struct pci_host_bridge *host_bridge,\n\t\t\t    struct cxl_memdev_state *mds)\n{\n\tstruct cxl_event_interrupt_policy policy;\n\tint rc;\n\n\t \n\tif (!host_bridge->native_cxl_error)\n\t\treturn 0;\n\n\trc = cxl_mem_alloc_event_buf(mds);\n\tif (rc)\n\t\treturn rc;\n\n\trc = cxl_event_get_int_policy(mds, &policy);\n\tif (rc)\n\t\treturn rc;\n\n\tif (cxl_event_int_is_fw(policy.info_settings) ||\n\t    cxl_event_int_is_fw(policy.warn_settings) ||\n\t    cxl_event_int_is_fw(policy.failure_settings) ||\n\t    cxl_event_int_is_fw(policy.fatal_settings)) {\n\t\tdev_err(mds->cxlds.dev,\n\t\t\t\"FW still in control of Event Logs despite _OSC settings\\n\");\n\t\treturn -EBUSY;\n\t}\n\n\trc = cxl_event_irqsetup(mds);\n\tif (rc)\n\t\treturn rc;\n\n\tcxl_mem_get_event_records(mds, CXLDEV_EVENT_STATUS_ALL);\n\n\treturn 0;\n}\n\nstatic int cxl_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct pci_host_bridge *host_bridge = pci_find_host_bridge(pdev->bus);\n\tstruct cxl_memdev_state *mds;\n\tstruct cxl_dev_state *cxlds;\n\tstruct cxl_register_map map;\n\tstruct cxl_memdev *cxlmd;\n\tint i, rc, pmu_count;\n\n\t \n\tBUILD_BUG_ON(offsetof(struct cxl_regs, memdev) !=\n\t\t     offsetof(struct cxl_regs, device_regs.memdev));\n\n\trc = pcim_enable_device(pdev);\n\tif (rc)\n\t\treturn rc;\n\tpci_set_master(pdev);\n\n\tmds = cxl_memdev_state_create(&pdev->dev);\n\tif (IS_ERR(mds))\n\t\treturn PTR_ERR(mds);\n\tcxlds = &mds->cxlds;\n\tpci_set_drvdata(pdev, cxlds);\n\n\tcxlds->rcd = is_cxl_restricted(pdev);\n\tcxlds->serial = pci_get_dsn(pdev);\n\tcxlds->cxl_dvsec = pci_find_dvsec_capability(\n\t\tpdev, PCI_DVSEC_VENDOR_ID_CXL, CXL_DVSEC_PCIE_DEVICE);\n\tif (!cxlds->cxl_dvsec)\n\t\tdev_warn(&pdev->dev,\n\t\t\t \"Device DVSEC not present, skip CXL.mem init\\n\");\n\n\trc = cxl_pci_setup_regs(pdev, CXL_REGLOC_RBI_MEMDEV, &map);\n\tif (rc)\n\t\treturn rc;\n\n\trc = cxl_map_device_regs(&map, &cxlds->regs.device_regs);\n\tif (rc)\n\t\treturn rc;\n\n\t \n\tcxlds->component_reg_phys = CXL_RESOURCE_NONE;\n\trc = cxl_pci_setup_regs(pdev, CXL_REGLOC_RBI_COMPONENT, &map);\n\tif (rc)\n\t\tdev_warn(&pdev->dev, \"No component registers (%d)\\n\", rc);\n\telse if (!map.component_map.ras.valid)\n\t\tdev_dbg(&pdev->dev, \"RAS registers not found\\n\");\n\n\tcxlds->component_reg_phys = map.resource;\n\n\trc = cxl_map_component_regs(&map, &cxlds->regs.component,\n\t\t\t\t    BIT(CXL_CM_CAP_CAP_ID_RAS));\n\tif (rc)\n\t\tdev_dbg(&pdev->dev, \"Failed to map RAS capability.\\n\");\n\n\trc = cxl_await_media_ready(cxlds);\n\tif (rc == 0)\n\t\tcxlds->media_ready = true;\n\telse\n\t\tdev_warn(&pdev->dev, \"Media not active (%d)\\n\", rc);\n\n\trc = cxl_alloc_irq_vectors(pdev);\n\tif (rc)\n\t\treturn rc;\n\n\trc = cxl_pci_setup_mailbox(mds);\n\tif (rc)\n\t\treturn rc;\n\n\trc = cxl_enumerate_cmds(mds);\n\tif (rc)\n\t\treturn rc;\n\n\trc = cxl_set_timestamp(mds);\n\tif (rc)\n\t\treturn rc;\n\n\trc = cxl_poison_state_init(mds);\n\tif (rc)\n\t\treturn rc;\n\n\trc = cxl_dev_state_identify(mds);\n\tif (rc)\n\t\treturn rc;\n\n\trc = cxl_mem_create_range_info(mds);\n\tif (rc)\n\t\treturn rc;\n\n\tcxlmd = devm_cxl_add_memdev(&pdev->dev, cxlds);\n\tif (IS_ERR(cxlmd))\n\t\treturn PTR_ERR(cxlmd);\n\n\trc = devm_cxl_setup_fw_upload(&pdev->dev, mds);\n\tif (rc)\n\t\treturn rc;\n\n\trc = devm_cxl_sanitize_setup_notifier(&pdev->dev, cxlmd);\n\tif (rc)\n\t\treturn rc;\n\n\tpmu_count = cxl_count_regblock(pdev, CXL_REGLOC_RBI_PMU);\n\tfor (i = 0; i < pmu_count; i++) {\n\t\tstruct cxl_pmu_regs pmu_regs;\n\n\t\trc = cxl_find_regblock_instance(pdev, CXL_REGLOC_RBI_PMU, &map, i);\n\t\tif (rc) {\n\t\t\tdev_dbg(&pdev->dev, \"Could not find PMU regblock\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\trc = cxl_map_pmu_regs(pdev, &pmu_regs, &map);\n\t\tif (rc) {\n\t\t\tdev_dbg(&pdev->dev, \"Could not map PMU regs\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\trc = devm_cxl_pmu_add(cxlds->dev, &pmu_regs, cxlmd->id, i, CXL_PMU_MEMDEV);\n\t\tif (rc) {\n\t\t\tdev_dbg(&pdev->dev, \"Could not add PMU instance\\n\");\n\t\t\tbreak;\n\t\t}\n\t}\n\n\trc = cxl_event_config(host_bridge, mds);\n\tif (rc)\n\t\treturn rc;\n\n\trc = cxl_pci_ras_unmask(pdev);\n\tif (rc)\n\t\tdev_dbg(&pdev->dev, \"No RAS reporting unmasked\\n\");\n\n\tpci_save_state(pdev);\n\n\treturn rc;\n}\n\nstatic const struct pci_device_id cxl_mem_pci_tbl[] = {\n\t \n\t{ PCI_DEVICE_CLASS((PCI_CLASS_MEMORY_CXL << 8 | CXL_MEMORY_PROGIF), ~0)},\n\t{   },\n};\nMODULE_DEVICE_TABLE(pci, cxl_mem_pci_tbl);\n\nstatic pci_ers_result_t cxl_slot_reset(struct pci_dev *pdev)\n{\n\tstruct cxl_dev_state *cxlds = pci_get_drvdata(pdev);\n\tstruct cxl_memdev *cxlmd = cxlds->cxlmd;\n\tstruct device *dev = &cxlmd->dev;\n\n\tdev_info(&pdev->dev, \"%s: restart CXL.mem after slot reset\\n\",\n\t\t dev_name(dev));\n\tpci_restore_state(pdev);\n\tif (device_attach(dev) <= 0)\n\t\treturn PCI_ERS_RESULT_DISCONNECT;\n\treturn PCI_ERS_RESULT_RECOVERED;\n}\n\nstatic void cxl_error_resume(struct pci_dev *pdev)\n{\n\tstruct cxl_dev_state *cxlds = pci_get_drvdata(pdev);\n\tstruct cxl_memdev *cxlmd = cxlds->cxlmd;\n\tstruct device *dev = &cxlmd->dev;\n\n\tdev_info(&pdev->dev, \"%s: error resume %s\\n\", dev_name(dev),\n\t\t dev->driver ? \"successful\" : \"failed\");\n}\n\nstatic const struct pci_error_handlers cxl_error_handlers = {\n\t.error_detected\t= cxl_error_detected,\n\t.slot_reset\t= cxl_slot_reset,\n\t.resume\t\t= cxl_error_resume,\n\t.cor_error_detected\t= cxl_cor_error_detected,\n};\n\nstatic struct pci_driver cxl_pci_driver = {\n\t.name\t\t\t= KBUILD_MODNAME,\n\t.id_table\t\t= cxl_mem_pci_tbl,\n\t.probe\t\t\t= cxl_pci_probe,\n\t.err_handler\t\t= &cxl_error_handlers,\n\t.driver\t= {\n\t\t.probe_type\t= PROBE_PREFER_ASYNCHRONOUS,\n\t},\n};\n\nMODULE_LICENSE(\"GPL v2\");\nmodule_pci_driver(cxl_pci_driver);\nMODULE_IMPORT_NS(CXL);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}