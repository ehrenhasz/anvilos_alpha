{
  "module_name": "vhost.c",
  "hash_id": "7b726e205f7be7ab612afacbced924843bad4444a2a202bec4cd2c9b2eee3ef8",
  "original_prompt": "Ingested from linux-6.6.14/drivers/vhost/vhost.c",
  "human_readable_source": "\n \n\n#include <linux/eventfd.h>\n#include <linux/vhost.h>\n#include <linux/uio.h>\n#include <linux/mm.h>\n#include <linux/miscdevice.h>\n#include <linux/mutex.h>\n#include <linux/poll.h>\n#include <linux/file.h>\n#include <linux/highmem.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/kthread.h>\n#include <linux/module.h>\n#include <linux/sort.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/vhost_task.h>\n#include <linux/interval_tree_generic.h>\n#include <linux/nospec.h>\n#include <linux/kcov.h>\n\n#include \"vhost.h\"\n\nstatic ushort max_mem_regions = 64;\nmodule_param(max_mem_regions, ushort, 0444);\nMODULE_PARM_DESC(max_mem_regions,\n\t\"Maximum number of memory regions in memory map. (default: 64)\");\nstatic int max_iotlb_entries = 2048;\nmodule_param(max_iotlb_entries, int, 0444);\nMODULE_PARM_DESC(max_iotlb_entries,\n\t\"Maximum number of iotlb entries. (default: 2048)\");\n\nenum {\n\tVHOST_MEMORY_F_LOG = 0x1,\n};\n\n#define vhost_used_event(vq) ((__virtio16 __user *)&vq->avail->ring[vq->num])\n#define vhost_avail_event(vq) ((__virtio16 __user *)&vq->used->ring[vq->num])\n\n#ifdef CONFIG_VHOST_CROSS_ENDIAN_LEGACY\nstatic void vhost_disable_cross_endian(struct vhost_virtqueue *vq)\n{\n\tvq->user_be = !virtio_legacy_is_little_endian();\n}\n\nstatic void vhost_enable_cross_endian_big(struct vhost_virtqueue *vq)\n{\n\tvq->user_be = true;\n}\n\nstatic void vhost_enable_cross_endian_little(struct vhost_virtqueue *vq)\n{\n\tvq->user_be = false;\n}\n\nstatic long vhost_set_vring_endian(struct vhost_virtqueue *vq, int __user *argp)\n{\n\tstruct vhost_vring_state s;\n\n\tif (vq->private_data)\n\t\treturn -EBUSY;\n\n\tif (copy_from_user(&s, argp, sizeof(s)))\n\t\treturn -EFAULT;\n\n\tif (s.num != VHOST_VRING_LITTLE_ENDIAN &&\n\t    s.num != VHOST_VRING_BIG_ENDIAN)\n\t\treturn -EINVAL;\n\n\tif (s.num == VHOST_VRING_BIG_ENDIAN)\n\t\tvhost_enable_cross_endian_big(vq);\n\telse\n\t\tvhost_enable_cross_endian_little(vq);\n\n\treturn 0;\n}\n\nstatic long vhost_get_vring_endian(struct vhost_virtqueue *vq, u32 idx,\n\t\t\t\t   int __user *argp)\n{\n\tstruct vhost_vring_state s = {\n\t\t.index = idx,\n\t\t.num = vq->user_be\n\t};\n\n\tif (copy_to_user(argp, &s, sizeof(s)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic void vhost_init_is_le(struct vhost_virtqueue *vq)\n{\n\t \n\tvq->is_le = vhost_has_feature(vq, VIRTIO_F_VERSION_1) || !vq->user_be;\n}\n#else\nstatic void vhost_disable_cross_endian(struct vhost_virtqueue *vq)\n{\n}\n\nstatic long vhost_set_vring_endian(struct vhost_virtqueue *vq, int __user *argp)\n{\n\treturn -ENOIOCTLCMD;\n}\n\nstatic long vhost_get_vring_endian(struct vhost_virtqueue *vq, u32 idx,\n\t\t\t\t   int __user *argp)\n{\n\treturn -ENOIOCTLCMD;\n}\n\nstatic void vhost_init_is_le(struct vhost_virtqueue *vq)\n{\n\tvq->is_le = vhost_has_feature(vq, VIRTIO_F_VERSION_1)\n\t\t|| virtio_legacy_is_little_endian();\n}\n#endif  \n\nstatic void vhost_reset_is_le(struct vhost_virtqueue *vq)\n{\n\tvhost_init_is_le(vq);\n}\n\nstruct vhost_flush_struct {\n\tstruct vhost_work work;\n\tstruct completion wait_event;\n};\n\nstatic void vhost_flush_work(struct vhost_work *work)\n{\n\tstruct vhost_flush_struct *s;\n\n\ts = container_of(work, struct vhost_flush_struct, work);\n\tcomplete(&s->wait_event);\n}\n\nstatic void vhost_poll_func(struct file *file, wait_queue_head_t *wqh,\n\t\t\t    poll_table *pt)\n{\n\tstruct vhost_poll *poll;\n\n\tpoll = container_of(pt, struct vhost_poll, table);\n\tpoll->wqh = wqh;\n\tadd_wait_queue(wqh, &poll->wait);\n}\n\nstatic int vhost_poll_wakeup(wait_queue_entry_t *wait, unsigned mode, int sync,\n\t\t\t     void *key)\n{\n\tstruct vhost_poll *poll = container_of(wait, struct vhost_poll, wait);\n\tstruct vhost_work *work = &poll->work;\n\n\tif (!(key_to_poll(key) & poll->mask))\n\t\treturn 0;\n\n\tif (!poll->dev->use_worker)\n\t\twork->fn(work);\n\telse\n\t\tvhost_poll_queue(poll);\n\n\treturn 0;\n}\n\nvoid vhost_work_init(struct vhost_work *work, vhost_work_fn_t fn)\n{\n\tclear_bit(VHOST_WORK_QUEUED, &work->flags);\n\twork->fn = fn;\n}\nEXPORT_SYMBOL_GPL(vhost_work_init);\n\n \nvoid vhost_poll_init(struct vhost_poll *poll, vhost_work_fn_t fn,\n\t\t     __poll_t mask, struct vhost_dev *dev,\n\t\t     struct vhost_virtqueue *vq)\n{\n\tinit_waitqueue_func_entry(&poll->wait, vhost_poll_wakeup);\n\tinit_poll_funcptr(&poll->table, vhost_poll_func);\n\tpoll->mask = mask;\n\tpoll->dev = dev;\n\tpoll->wqh = NULL;\n\tpoll->vq = vq;\n\n\tvhost_work_init(&poll->work, fn);\n}\nEXPORT_SYMBOL_GPL(vhost_poll_init);\n\n \nint vhost_poll_start(struct vhost_poll *poll, struct file *file)\n{\n\t__poll_t mask;\n\n\tif (poll->wqh)\n\t\treturn 0;\n\n\tmask = vfs_poll(file, &poll->table);\n\tif (mask)\n\t\tvhost_poll_wakeup(&poll->wait, 0, 0, poll_to_key(mask));\n\tif (mask & EPOLLERR) {\n\t\tvhost_poll_stop(poll);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(vhost_poll_start);\n\n \nvoid vhost_poll_stop(struct vhost_poll *poll)\n{\n\tif (poll->wqh) {\n\t\tremove_wait_queue(poll->wqh, &poll->wait);\n\t\tpoll->wqh = NULL;\n\t}\n}\nEXPORT_SYMBOL_GPL(vhost_poll_stop);\n\nstatic void vhost_worker_queue(struct vhost_worker *worker,\n\t\t\t       struct vhost_work *work)\n{\n\tif (!test_and_set_bit(VHOST_WORK_QUEUED, &work->flags)) {\n\t\t \n\t\tllist_add(&work->node, &worker->work_list);\n\t\tvhost_task_wake(worker->vtsk);\n\t}\n}\n\nbool vhost_vq_work_queue(struct vhost_virtqueue *vq, struct vhost_work *work)\n{\n\tstruct vhost_worker *worker;\n\tbool queued = false;\n\n\trcu_read_lock();\n\tworker = rcu_dereference(vq->worker);\n\tif (worker) {\n\t\tqueued = true;\n\t\tvhost_worker_queue(worker, work);\n\t}\n\trcu_read_unlock();\n\n\treturn queued;\n}\nEXPORT_SYMBOL_GPL(vhost_vq_work_queue);\n\nvoid vhost_vq_flush(struct vhost_virtqueue *vq)\n{\n\tstruct vhost_flush_struct flush;\n\n\tinit_completion(&flush.wait_event);\n\tvhost_work_init(&flush.work, vhost_flush_work);\n\n\tif (vhost_vq_work_queue(vq, &flush.work))\n\t\twait_for_completion(&flush.wait_event);\n}\nEXPORT_SYMBOL_GPL(vhost_vq_flush);\n\n \nstatic void vhost_worker_flush(struct vhost_worker *worker)\n{\n\tstruct vhost_flush_struct flush;\n\n\tinit_completion(&flush.wait_event);\n\tvhost_work_init(&flush.work, vhost_flush_work);\n\n\tvhost_worker_queue(worker, &flush.work);\n\twait_for_completion(&flush.wait_event);\n}\n\nvoid vhost_dev_flush(struct vhost_dev *dev)\n{\n\tstruct vhost_worker *worker;\n\tunsigned long i;\n\n\txa_for_each(&dev->worker_xa, i, worker) {\n\t\tmutex_lock(&worker->mutex);\n\t\tif (!worker->attachment_cnt) {\n\t\t\tmutex_unlock(&worker->mutex);\n\t\t\tcontinue;\n\t\t}\n\t\tvhost_worker_flush(worker);\n\t\tmutex_unlock(&worker->mutex);\n\t}\n}\nEXPORT_SYMBOL_GPL(vhost_dev_flush);\n\n \nbool vhost_vq_has_work(struct vhost_virtqueue *vq)\n{\n\tstruct vhost_worker *worker;\n\tbool has_work = false;\n\n\trcu_read_lock();\n\tworker = rcu_dereference(vq->worker);\n\tif (worker && !llist_empty(&worker->work_list))\n\t\thas_work = true;\n\trcu_read_unlock();\n\n\treturn has_work;\n}\nEXPORT_SYMBOL_GPL(vhost_vq_has_work);\n\nvoid vhost_poll_queue(struct vhost_poll *poll)\n{\n\tvhost_vq_work_queue(poll->vq, &poll->work);\n}\nEXPORT_SYMBOL_GPL(vhost_poll_queue);\n\nstatic void __vhost_vq_meta_reset(struct vhost_virtqueue *vq)\n{\n\tint j;\n\n\tfor (j = 0; j < VHOST_NUM_ADDRS; j++)\n\t\tvq->meta_iotlb[j] = NULL;\n}\n\nstatic void vhost_vq_meta_reset(struct vhost_dev *d)\n{\n\tint i;\n\n\tfor (i = 0; i < d->nvqs; ++i)\n\t\t__vhost_vq_meta_reset(d->vqs[i]);\n}\n\nstatic void vhost_vring_call_reset(struct vhost_vring_call *call_ctx)\n{\n\tcall_ctx->ctx = NULL;\n\tmemset(&call_ctx->producer, 0x0, sizeof(struct irq_bypass_producer));\n}\n\nbool vhost_vq_is_setup(struct vhost_virtqueue *vq)\n{\n\treturn vq->avail && vq->desc && vq->used && vhost_vq_access_ok(vq);\n}\nEXPORT_SYMBOL_GPL(vhost_vq_is_setup);\n\nstatic void vhost_vq_reset(struct vhost_dev *dev,\n\t\t\t   struct vhost_virtqueue *vq)\n{\n\tvq->num = 1;\n\tvq->desc = NULL;\n\tvq->avail = NULL;\n\tvq->used = NULL;\n\tvq->last_avail_idx = 0;\n\tvq->avail_idx = 0;\n\tvq->last_used_idx = 0;\n\tvq->signalled_used = 0;\n\tvq->signalled_used_valid = false;\n\tvq->used_flags = 0;\n\tvq->log_used = false;\n\tvq->log_addr = -1ull;\n\tvq->private_data = NULL;\n\tvq->acked_features = 0;\n\tvq->acked_backend_features = 0;\n\tvq->log_base = NULL;\n\tvq->error_ctx = NULL;\n\tvq->kick = NULL;\n\tvq->log_ctx = NULL;\n\tvhost_disable_cross_endian(vq);\n\tvhost_reset_is_le(vq);\n\tvq->busyloop_timeout = 0;\n\tvq->umem = NULL;\n\tvq->iotlb = NULL;\n\trcu_assign_pointer(vq->worker, NULL);\n\tvhost_vring_call_reset(&vq->call_ctx);\n\t__vhost_vq_meta_reset(vq);\n}\n\nstatic bool vhost_worker(void *data)\n{\n\tstruct vhost_worker *worker = data;\n\tstruct vhost_work *work, *work_next;\n\tstruct llist_node *node;\n\n\tnode = llist_del_all(&worker->work_list);\n\tif (node) {\n\t\t__set_current_state(TASK_RUNNING);\n\n\t\tnode = llist_reverse_order(node);\n\t\t \n\t\tsmp_wmb();\n\t\tllist_for_each_entry_safe(work, work_next, node, node) {\n\t\t\tclear_bit(VHOST_WORK_QUEUED, &work->flags);\n\t\t\tkcov_remote_start_common(worker->kcov_handle);\n\t\t\twork->fn(work);\n\t\t\tkcov_remote_stop();\n\t\t\tcond_resched();\n\t\t}\n\t}\n\n\treturn !!node;\n}\n\nstatic void vhost_vq_free_iovecs(struct vhost_virtqueue *vq)\n{\n\tkfree(vq->indirect);\n\tvq->indirect = NULL;\n\tkfree(vq->log);\n\tvq->log = NULL;\n\tkfree(vq->heads);\n\tvq->heads = NULL;\n}\n\n \nstatic long vhost_dev_alloc_iovecs(struct vhost_dev *dev)\n{\n\tstruct vhost_virtqueue *vq;\n\tint i;\n\n\tfor (i = 0; i < dev->nvqs; ++i) {\n\t\tvq = dev->vqs[i];\n\t\tvq->indirect = kmalloc_array(UIO_MAXIOV,\n\t\t\t\t\t     sizeof(*vq->indirect),\n\t\t\t\t\t     GFP_KERNEL);\n\t\tvq->log = kmalloc_array(dev->iov_limit, sizeof(*vq->log),\n\t\t\t\t\tGFP_KERNEL);\n\t\tvq->heads = kmalloc_array(dev->iov_limit, sizeof(*vq->heads),\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!vq->indirect || !vq->log || !vq->heads)\n\t\t\tgoto err_nomem;\n\t}\n\treturn 0;\n\nerr_nomem:\n\tfor (; i >= 0; --i)\n\t\tvhost_vq_free_iovecs(dev->vqs[i]);\n\treturn -ENOMEM;\n}\n\nstatic void vhost_dev_free_iovecs(struct vhost_dev *dev)\n{\n\tint i;\n\n\tfor (i = 0; i < dev->nvqs; ++i)\n\t\tvhost_vq_free_iovecs(dev->vqs[i]);\n}\n\nbool vhost_exceeds_weight(struct vhost_virtqueue *vq,\n\t\t\t  int pkts, int total_len)\n{\n\tstruct vhost_dev *dev = vq->dev;\n\n\tif ((dev->byte_weight && total_len >= dev->byte_weight) ||\n\t    pkts >= dev->weight) {\n\t\tvhost_poll_queue(&vq->poll);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\nEXPORT_SYMBOL_GPL(vhost_exceeds_weight);\n\nstatic size_t vhost_get_avail_size(struct vhost_virtqueue *vq,\n\t\t\t\t   unsigned int num)\n{\n\tsize_t event __maybe_unused =\n\t       vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;\n\n\treturn size_add(struct_size(vq->avail, ring, num), event);\n}\n\nstatic size_t vhost_get_used_size(struct vhost_virtqueue *vq,\n\t\t\t\t  unsigned int num)\n{\n\tsize_t event __maybe_unused =\n\t       vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;\n\n\treturn size_add(struct_size(vq->used, ring, num), event);\n}\n\nstatic size_t vhost_get_desc_size(struct vhost_virtqueue *vq,\n\t\t\t\t  unsigned int num)\n{\n\treturn sizeof(*vq->desc) * num;\n}\n\nvoid vhost_dev_init(struct vhost_dev *dev,\n\t\t    struct vhost_virtqueue **vqs, int nvqs,\n\t\t    int iov_limit, int weight, int byte_weight,\n\t\t    bool use_worker,\n\t\t    int (*msg_handler)(struct vhost_dev *dev, u32 asid,\n\t\t\t\t       struct vhost_iotlb_msg *msg))\n{\n\tstruct vhost_virtqueue *vq;\n\tint i;\n\n\tdev->vqs = vqs;\n\tdev->nvqs = nvqs;\n\tmutex_init(&dev->mutex);\n\tdev->log_ctx = NULL;\n\tdev->umem = NULL;\n\tdev->iotlb = NULL;\n\tdev->mm = NULL;\n\tdev->iov_limit = iov_limit;\n\tdev->weight = weight;\n\tdev->byte_weight = byte_weight;\n\tdev->use_worker = use_worker;\n\tdev->msg_handler = msg_handler;\n\tinit_waitqueue_head(&dev->wait);\n\tINIT_LIST_HEAD(&dev->read_list);\n\tINIT_LIST_HEAD(&dev->pending_list);\n\tspin_lock_init(&dev->iotlb_lock);\n\txa_init_flags(&dev->worker_xa, XA_FLAGS_ALLOC);\n\n\tfor (i = 0; i < dev->nvqs; ++i) {\n\t\tvq = dev->vqs[i];\n\t\tvq->log = NULL;\n\t\tvq->indirect = NULL;\n\t\tvq->heads = NULL;\n\t\tvq->dev = dev;\n\t\tmutex_init(&vq->mutex);\n\t\tvhost_vq_reset(dev, vq);\n\t\tif (vq->handle_kick)\n\t\t\tvhost_poll_init(&vq->poll, vq->handle_kick,\n\t\t\t\t\tEPOLLIN, dev, vq);\n\t}\n}\nEXPORT_SYMBOL_GPL(vhost_dev_init);\n\n \nlong vhost_dev_check_owner(struct vhost_dev *dev)\n{\n\t \n\treturn dev->mm == current->mm ? 0 : -EPERM;\n}\nEXPORT_SYMBOL_GPL(vhost_dev_check_owner);\n\n \nbool vhost_dev_has_owner(struct vhost_dev *dev)\n{\n\treturn dev->mm;\n}\nEXPORT_SYMBOL_GPL(vhost_dev_has_owner);\n\nstatic void vhost_attach_mm(struct vhost_dev *dev)\n{\n\t \n\tif (dev->use_worker) {\n\t\tdev->mm = get_task_mm(current);\n\t} else {\n\t\t \n\t\tdev->mm = current->mm;\n\t\tmmgrab(dev->mm);\n\t}\n}\n\nstatic void vhost_detach_mm(struct vhost_dev *dev)\n{\n\tif (!dev->mm)\n\t\treturn;\n\n\tif (dev->use_worker)\n\t\tmmput(dev->mm);\n\telse\n\t\tmmdrop(dev->mm);\n\n\tdev->mm = NULL;\n}\n\nstatic void vhost_worker_destroy(struct vhost_dev *dev,\n\t\t\t\t struct vhost_worker *worker)\n{\n\tif (!worker)\n\t\treturn;\n\n\tWARN_ON(!llist_empty(&worker->work_list));\n\txa_erase(&dev->worker_xa, worker->id);\n\tvhost_task_stop(worker->vtsk);\n\tkfree(worker);\n}\n\nstatic void vhost_workers_free(struct vhost_dev *dev)\n{\n\tstruct vhost_worker *worker;\n\tunsigned long i;\n\n\tif (!dev->use_worker)\n\t\treturn;\n\n\tfor (i = 0; i < dev->nvqs; i++)\n\t\trcu_assign_pointer(dev->vqs[i]->worker, NULL);\n\t \n\txa_for_each(&dev->worker_xa, i, worker)\n\t\tvhost_worker_destroy(dev, worker);\n\txa_destroy(&dev->worker_xa);\n}\n\nstatic struct vhost_worker *vhost_worker_create(struct vhost_dev *dev)\n{\n\tstruct vhost_worker *worker;\n\tstruct vhost_task *vtsk;\n\tchar name[TASK_COMM_LEN];\n\tint ret;\n\tu32 id;\n\n\tworker = kzalloc(sizeof(*worker), GFP_KERNEL_ACCOUNT);\n\tif (!worker)\n\t\treturn NULL;\n\n\tsnprintf(name, sizeof(name), \"vhost-%d\", current->pid);\n\n\tvtsk = vhost_task_create(vhost_worker, worker, name);\n\tif (!vtsk)\n\t\tgoto free_worker;\n\n\tmutex_init(&worker->mutex);\n\tinit_llist_head(&worker->work_list);\n\tworker->kcov_handle = kcov_common_handle();\n\tworker->vtsk = vtsk;\n\n\tvhost_task_start(vtsk);\n\n\tret = xa_alloc(&dev->worker_xa, &id, worker, xa_limit_32b, GFP_KERNEL);\n\tif (ret < 0)\n\t\tgoto stop_worker;\n\tworker->id = id;\n\n\treturn worker;\n\nstop_worker:\n\tvhost_task_stop(vtsk);\nfree_worker:\n\tkfree(worker);\n\treturn NULL;\n}\n\n \nstatic void __vhost_vq_attach_worker(struct vhost_virtqueue *vq,\n\t\t\t\t     struct vhost_worker *worker)\n{\n\tstruct vhost_worker *old_worker;\n\n\told_worker = rcu_dereference_check(vq->worker,\n\t\t\t\t\t   lockdep_is_held(&vq->dev->mutex));\n\n\tmutex_lock(&worker->mutex);\n\tworker->attachment_cnt++;\n\tmutex_unlock(&worker->mutex);\n\trcu_assign_pointer(vq->worker, worker);\n\n\tif (!old_worker)\n\t\treturn;\n\t \n\tmutex_lock(&old_worker->mutex);\n\told_worker->attachment_cnt--;\n\t \n\tmutex_lock(&vq->mutex);\n\tif (!vhost_vq_get_backend(vq) && !vq->kick) {\n\t\tmutex_unlock(&vq->mutex);\n\t\tmutex_unlock(&old_worker->mutex);\n\t\t \n\t\tWARN_ON(!old_worker->attachment_cnt &&\n\t\t\t!llist_empty(&old_worker->work_list));\n\t\treturn;\n\t}\n\tmutex_unlock(&vq->mutex);\n\n\t \n\tsynchronize_rcu();\n\t \n\tvhost_worker_flush(old_worker);\n\tmutex_unlock(&old_worker->mutex);\n}\n\n  \nstatic int vhost_vq_attach_worker(struct vhost_virtqueue *vq,\n\t\t\t\t  struct vhost_vring_worker *info)\n{\n\tunsigned long index = info->worker_id;\n\tstruct vhost_dev *dev = vq->dev;\n\tstruct vhost_worker *worker;\n\n\tif (!dev->use_worker)\n\t\treturn -EINVAL;\n\n\tworker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);\n\tif (!worker || worker->id != info->worker_id)\n\t\treturn -ENODEV;\n\n\t__vhost_vq_attach_worker(vq, worker);\n\treturn 0;\n}\n\n \nstatic int vhost_new_worker(struct vhost_dev *dev,\n\t\t\t    struct vhost_worker_state *info)\n{\n\tstruct vhost_worker *worker;\n\n\tworker = vhost_worker_create(dev);\n\tif (!worker)\n\t\treturn -ENOMEM;\n\n\tinfo->worker_id = worker->id;\n\treturn 0;\n}\n\n \nstatic int vhost_free_worker(struct vhost_dev *dev,\n\t\t\t     struct vhost_worker_state *info)\n{\n\tunsigned long index = info->worker_id;\n\tstruct vhost_worker *worker;\n\n\tworker = xa_find(&dev->worker_xa, &index, UINT_MAX, XA_PRESENT);\n\tif (!worker || worker->id != info->worker_id)\n\t\treturn -ENODEV;\n\n\tmutex_lock(&worker->mutex);\n\tif (worker->attachment_cnt) {\n\t\tmutex_unlock(&worker->mutex);\n\t\treturn -EBUSY;\n\t}\n\tmutex_unlock(&worker->mutex);\n\n\tvhost_worker_destroy(dev, worker);\n\treturn 0;\n}\n\nstatic int vhost_get_vq_from_user(struct vhost_dev *dev, void __user *argp,\n\t\t\t\t  struct vhost_virtqueue **vq, u32 *id)\n{\n\tu32 __user *idxp = argp;\n\tu32 idx;\n\tlong r;\n\n\tr = get_user(idx, idxp);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (idx >= dev->nvqs)\n\t\treturn -ENOBUFS;\n\n\tidx = array_index_nospec(idx, dev->nvqs);\n\n\t*vq = dev->vqs[idx];\n\t*id = idx;\n\treturn 0;\n}\n\n \nlong vhost_worker_ioctl(struct vhost_dev *dev, unsigned int ioctl,\n\t\t\tvoid __user *argp)\n{\n\tstruct vhost_vring_worker ring_worker;\n\tstruct vhost_worker_state state;\n\tstruct vhost_worker *worker;\n\tstruct vhost_virtqueue *vq;\n\tlong ret;\n\tu32 idx;\n\n\tif (!dev->use_worker)\n\t\treturn -EINVAL;\n\n\tif (!vhost_dev_has_owner(dev))\n\t\treturn -EINVAL;\n\n\tret = vhost_dev_check_owner(dev);\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (ioctl) {\n\t \n\tcase VHOST_NEW_WORKER:\n\t\tret = vhost_new_worker(dev, &state);\n\t\tif (!ret && copy_to_user(argp, &state, sizeof(state)))\n\t\t\tret = -EFAULT;\n\t\treturn ret;\n\tcase VHOST_FREE_WORKER:\n\t\tif (copy_from_user(&state, argp, sizeof(state)))\n\t\t\treturn -EFAULT;\n\t\treturn vhost_free_worker(dev, &state);\n\t \n\tcase VHOST_ATTACH_VRING_WORKER:\n\tcase VHOST_GET_VRING_WORKER:\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\n\tret = vhost_get_vq_from_user(dev, argp, &vq, &idx);\n\tif (ret)\n\t\treturn ret;\n\n\tswitch (ioctl) {\n\tcase VHOST_ATTACH_VRING_WORKER:\n\t\tif (copy_from_user(&ring_worker, argp, sizeof(ring_worker))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = vhost_vq_attach_worker(vq, &ring_worker);\n\t\tbreak;\n\tcase VHOST_GET_VRING_WORKER:\n\t\tworker = rcu_dereference_check(vq->worker,\n\t\t\t\t\t       lockdep_is_held(&dev->mutex));\n\t\tif (!worker) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tring_worker.index = idx;\n\t\tring_worker.worker_id = worker->id;\n\n\t\tif (copy_to_user(argp, &ring_worker, sizeof(ring_worker)))\n\t\t\tret = -EFAULT;\n\t\tbreak;\n\tdefault:\n\t\tret = -ENOIOCTLCMD;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(vhost_worker_ioctl);\n\n \nlong vhost_dev_set_owner(struct vhost_dev *dev)\n{\n\tstruct vhost_worker *worker;\n\tint err, i;\n\n\t \n\tif (vhost_dev_has_owner(dev)) {\n\t\terr = -EBUSY;\n\t\tgoto err_mm;\n\t}\n\n\tvhost_attach_mm(dev);\n\n\terr = vhost_dev_alloc_iovecs(dev);\n\tif (err)\n\t\tgoto err_iovecs;\n\n\tif (dev->use_worker) {\n\t\t \n\t\tworker = vhost_worker_create(dev);\n\t\tif (!worker) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto err_worker;\n\t\t}\n\n\t\tfor (i = 0; i < dev->nvqs; i++)\n\t\t\t__vhost_vq_attach_worker(dev->vqs[i], worker);\n\t}\n\n\treturn 0;\n\nerr_worker:\n\tvhost_dev_free_iovecs(dev);\nerr_iovecs:\n\tvhost_detach_mm(dev);\nerr_mm:\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(vhost_dev_set_owner);\n\nstatic struct vhost_iotlb *iotlb_alloc(void)\n{\n\treturn vhost_iotlb_alloc(max_iotlb_entries,\n\t\t\t\t VHOST_IOTLB_FLAG_RETIRE);\n}\n\nstruct vhost_iotlb *vhost_dev_reset_owner_prepare(void)\n{\n\treturn iotlb_alloc();\n}\nEXPORT_SYMBOL_GPL(vhost_dev_reset_owner_prepare);\n\n \nvoid vhost_dev_reset_owner(struct vhost_dev *dev, struct vhost_iotlb *umem)\n{\n\tint i;\n\n\tvhost_dev_cleanup(dev);\n\n\tdev->umem = umem;\n\t \n\tfor (i = 0; i < dev->nvqs; ++i)\n\t\tdev->vqs[i]->umem = umem;\n}\nEXPORT_SYMBOL_GPL(vhost_dev_reset_owner);\n\nvoid vhost_dev_stop(struct vhost_dev *dev)\n{\n\tint i;\n\n\tfor (i = 0; i < dev->nvqs; ++i) {\n\t\tif (dev->vqs[i]->kick && dev->vqs[i]->handle_kick)\n\t\t\tvhost_poll_stop(&dev->vqs[i]->poll);\n\t}\n\n\tvhost_dev_flush(dev);\n}\nEXPORT_SYMBOL_GPL(vhost_dev_stop);\n\nvoid vhost_clear_msg(struct vhost_dev *dev)\n{\n\tstruct vhost_msg_node *node, *n;\n\n\tspin_lock(&dev->iotlb_lock);\n\n\tlist_for_each_entry_safe(node, n, &dev->read_list, node) {\n\t\tlist_del(&node->node);\n\t\tkfree(node);\n\t}\n\n\tlist_for_each_entry_safe(node, n, &dev->pending_list, node) {\n\t\tlist_del(&node->node);\n\t\tkfree(node);\n\t}\n\n\tspin_unlock(&dev->iotlb_lock);\n}\nEXPORT_SYMBOL_GPL(vhost_clear_msg);\n\nvoid vhost_dev_cleanup(struct vhost_dev *dev)\n{\n\tint i;\n\n\tfor (i = 0; i < dev->nvqs; ++i) {\n\t\tif (dev->vqs[i]->error_ctx)\n\t\t\teventfd_ctx_put(dev->vqs[i]->error_ctx);\n\t\tif (dev->vqs[i]->kick)\n\t\t\tfput(dev->vqs[i]->kick);\n\t\tif (dev->vqs[i]->call_ctx.ctx)\n\t\t\teventfd_ctx_put(dev->vqs[i]->call_ctx.ctx);\n\t\tvhost_vq_reset(dev, dev->vqs[i]);\n\t}\n\tvhost_dev_free_iovecs(dev);\n\tif (dev->log_ctx)\n\t\teventfd_ctx_put(dev->log_ctx);\n\tdev->log_ctx = NULL;\n\t \n\tvhost_iotlb_free(dev->umem);\n\tdev->umem = NULL;\n\tvhost_iotlb_free(dev->iotlb);\n\tdev->iotlb = NULL;\n\tvhost_clear_msg(dev);\n\twake_up_interruptible_poll(&dev->wait, EPOLLIN | EPOLLRDNORM);\n\tvhost_workers_free(dev);\n\tvhost_detach_mm(dev);\n}\nEXPORT_SYMBOL_GPL(vhost_dev_cleanup);\n\nstatic bool log_access_ok(void __user *log_base, u64 addr, unsigned long sz)\n{\n\tu64 a = addr / VHOST_PAGE_SIZE / 8;\n\n\t \n\tif (a > ULONG_MAX - (unsigned long)log_base ||\n\t    a + (unsigned long)log_base > ULONG_MAX)\n\t\treturn false;\n\n\treturn access_ok(log_base + a,\n\t\t\t (sz + VHOST_PAGE_SIZE * 8 - 1) / VHOST_PAGE_SIZE / 8);\n}\n\n \nstatic bool vhost_overflow(u64 uaddr, u64 size)\n{\n\tif (uaddr > ULONG_MAX || size > ULONG_MAX)\n\t\treturn true;\n\n\tif (!size)\n\t\treturn false;\n\n\treturn uaddr > ULONG_MAX - size + 1;\n}\n\n \nstatic bool vq_memory_access_ok(void __user *log_base, struct vhost_iotlb *umem,\n\t\t\t\tint log_all)\n{\n\tstruct vhost_iotlb_map *map;\n\n\tif (!umem)\n\t\treturn false;\n\n\tlist_for_each_entry(map, &umem->list, link) {\n\t\tunsigned long a = map->addr;\n\n\t\tif (vhost_overflow(map->addr, map->size))\n\t\t\treturn false;\n\n\n\t\tif (!access_ok((void __user *)a, map->size))\n\t\t\treturn false;\n\t\telse if (log_all && !log_access_ok(log_base,\n\t\t\t\t\t\t   map->start,\n\t\t\t\t\t\t   map->size))\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic inline void __user *vhost_vq_meta_fetch(struct vhost_virtqueue *vq,\n\t\t\t\t\t       u64 addr, unsigned int size,\n\t\t\t\t\t       int type)\n{\n\tconst struct vhost_iotlb_map *map = vq->meta_iotlb[type];\n\n\tif (!map)\n\t\treturn NULL;\n\n\treturn (void __user *)(uintptr_t)(map->addr + addr - map->start);\n}\n\n \n \nstatic bool memory_access_ok(struct vhost_dev *d, struct vhost_iotlb *umem,\n\t\t\t     int log_all)\n{\n\tint i;\n\n\tfor (i = 0; i < d->nvqs; ++i) {\n\t\tbool ok;\n\t\tbool log;\n\n\t\tmutex_lock(&d->vqs[i]->mutex);\n\t\tlog = log_all || vhost_has_feature(d->vqs[i], VHOST_F_LOG_ALL);\n\t\t \n\t\tif (d->vqs[i]->private_data)\n\t\t\tok = vq_memory_access_ok(d->vqs[i]->log_base,\n\t\t\t\t\t\t umem, log);\n\t\telse\n\t\t\tok = true;\n\t\tmutex_unlock(&d->vqs[i]->mutex);\n\t\tif (!ok)\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,\n\t\t\t  struct iovec iov[], int iov_size, int access);\n\nstatic int vhost_copy_to_user(struct vhost_virtqueue *vq, void __user *to,\n\t\t\t      const void *from, unsigned size)\n{\n\tint ret;\n\n\tif (!vq->iotlb)\n\t\treturn __copy_to_user(to, from, size);\n\telse {\n\t\t \n\t\tstruct iov_iter t;\n\t\tvoid __user *uaddr = vhost_vq_meta_fetch(vq,\n\t\t\t\t     (u64)(uintptr_t)to, size,\n\t\t\t\t     VHOST_ADDR_USED);\n\n\t\tif (uaddr)\n\t\t\treturn __copy_to_user(uaddr, from, size);\n\n\t\tret = translate_desc(vq, (u64)(uintptr_t)to, size, vq->iotlb_iov,\n\t\t\t\t     ARRAY_SIZE(vq->iotlb_iov),\n\t\t\t\t     VHOST_ACCESS_WO);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tiov_iter_init(&t, ITER_DEST, vq->iotlb_iov, ret, size);\n\t\tret = copy_to_iter(from, size, &t);\n\t\tif (ret == size)\n\t\t\tret = 0;\n\t}\nout:\n\treturn ret;\n}\n\nstatic int vhost_copy_from_user(struct vhost_virtqueue *vq, void *to,\n\t\t\t\tvoid __user *from, unsigned size)\n{\n\tint ret;\n\n\tif (!vq->iotlb)\n\t\treturn __copy_from_user(to, from, size);\n\telse {\n\t\t \n\t\tvoid __user *uaddr = vhost_vq_meta_fetch(vq,\n\t\t\t\t     (u64)(uintptr_t)from, size,\n\t\t\t\t     VHOST_ADDR_DESC);\n\t\tstruct iov_iter f;\n\n\t\tif (uaddr)\n\t\t\treturn __copy_from_user(to, uaddr, size);\n\n\t\tret = translate_desc(vq, (u64)(uintptr_t)from, size, vq->iotlb_iov,\n\t\t\t\t     ARRAY_SIZE(vq->iotlb_iov),\n\t\t\t\t     VHOST_ACCESS_RO);\n\t\tif (ret < 0) {\n\t\t\tvq_err(vq, \"IOTLB translation failure: uaddr \"\n\t\t\t       \"%p size 0x%llx\\n\", from,\n\t\t\t       (unsigned long long) size);\n\t\t\tgoto out;\n\t\t}\n\t\tiov_iter_init(&f, ITER_SOURCE, vq->iotlb_iov, ret, size);\n\t\tret = copy_from_iter(to, size, &f);\n\t\tif (ret == size)\n\t\t\tret = 0;\n\t}\n\nout:\n\treturn ret;\n}\n\nstatic void __user *__vhost_get_user_slow(struct vhost_virtqueue *vq,\n\t\t\t\t\t  void __user *addr, unsigned int size,\n\t\t\t\t\t  int type)\n{\n\tint ret;\n\n\tret = translate_desc(vq, (u64)(uintptr_t)addr, size, vq->iotlb_iov,\n\t\t\t     ARRAY_SIZE(vq->iotlb_iov),\n\t\t\t     VHOST_ACCESS_RO);\n\tif (ret < 0) {\n\t\tvq_err(vq, \"IOTLB translation failure: uaddr \"\n\t\t\t\"%p size 0x%llx\\n\", addr,\n\t\t\t(unsigned long long) size);\n\t\treturn NULL;\n\t}\n\n\tif (ret != 1 || vq->iotlb_iov[0].iov_len != size) {\n\t\tvq_err(vq, \"Non atomic userspace memory access: uaddr \"\n\t\t\t\"%p size 0x%llx\\n\", addr,\n\t\t\t(unsigned long long) size);\n\t\treturn NULL;\n\t}\n\n\treturn vq->iotlb_iov[0].iov_base;\n}\n\n \nstatic inline void __user *__vhost_get_user(struct vhost_virtqueue *vq,\n\t\t\t\t\t    void __user *addr, unsigned int size,\n\t\t\t\t\t    int type)\n{\n\tvoid __user *uaddr = vhost_vq_meta_fetch(vq,\n\t\t\t     (u64)(uintptr_t)addr, size, type);\n\tif (uaddr)\n\t\treturn uaddr;\n\n\treturn __vhost_get_user_slow(vq, addr, size, type);\n}\n\n#define vhost_put_user(vq, x, ptr)\t\t\\\n({ \\\n\tint ret; \\\n\tif (!vq->iotlb) { \\\n\t\tret = __put_user(x, ptr); \\\n\t} else { \\\n\t\t__typeof__(ptr) to = \\\n\t\t\t(__typeof__(ptr)) __vhost_get_user(vq, ptr,\t\\\n\t\t\t\t\t  sizeof(*ptr), VHOST_ADDR_USED); \\\n\t\tif (to != NULL) \\\n\t\t\tret = __put_user(x, to); \\\n\t\telse \\\n\t\t\tret = -EFAULT;\t\\\n\t} \\\n\tret; \\\n})\n\nstatic inline int vhost_put_avail_event(struct vhost_virtqueue *vq)\n{\n\treturn vhost_put_user(vq, cpu_to_vhost16(vq, vq->avail_idx),\n\t\t\t      vhost_avail_event(vq));\n}\n\nstatic inline int vhost_put_used(struct vhost_virtqueue *vq,\n\t\t\t\t struct vring_used_elem *head, int idx,\n\t\t\t\t int count)\n{\n\treturn vhost_copy_to_user(vq, vq->used->ring + idx, head,\n\t\t\t\t  count * sizeof(*head));\n}\n\nstatic inline int vhost_put_used_flags(struct vhost_virtqueue *vq)\n\n{\n\treturn vhost_put_user(vq, cpu_to_vhost16(vq, vq->used_flags),\n\t\t\t      &vq->used->flags);\n}\n\nstatic inline int vhost_put_used_idx(struct vhost_virtqueue *vq)\n\n{\n\treturn vhost_put_user(vq, cpu_to_vhost16(vq, vq->last_used_idx),\n\t\t\t      &vq->used->idx);\n}\n\n#define vhost_get_user(vq, x, ptr, type)\t\t\\\n({ \\\n\tint ret; \\\n\tif (!vq->iotlb) { \\\n\t\tret = __get_user(x, ptr); \\\n\t} else { \\\n\t\t__typeof__(ptr) from = \\\n\t\t\t(__typeof__(ptr)) __vhost_get_user(vq, ptr, \\\n\t\t\t\t\t\t\t   sizeof(*ptr), \\\n\t\t\t\t\t\t\t   type); \\\n\t\tif (from != NULL) \\\n\t\t\tret = __get_user(x, from); \\\n\t\telse \\\n\t\t\tret = -EFAULT; \\\n\t} \\\n\tret; \\\n})\n\n#define vhost_get_avail(vq, x, ptr) \\\n\tvhost_get_user(vq, x, ptr, VHOST_ADDR_AVAIL)\n\n#define vhost_get_used(vq, x, ptr) \\\n\tvhost_get_user(vq, x, ptr, VHOST_ADDR_USED)\n\nstatic void vhost_dev_lock_vqs(struct vhost_dev *d)\n{\n\tint i = 0;\n\tfor (i = 0; i < d->nvqs; ++i)\n\t\tmutex_lock_nested(&d->vqs[i]->mutex, i);\n}\n\nstatic void vhost_dev_unlock_vqs(struct vhost_dev *d)\n{\n\tint i = 0;\n\tfor (i = 0; i < d->nvqs; ++i)\n\t\tmutex_unlock(&d->vqs[i]->mutex);\n}\n\nstatic inline int vhost_get_avail_idx(struct vhost_virtqueue *vq,\n\t\t\t\t      __virtio16 *idx)\n{\n\treturn vhost_get_avail(vq, *idx, &vq->avail->idx);\n}\n\nstatic inline int vhost_get_avail_head(struct vhost_virtqueue *vq,\n\t\t\t\t       __virtio16 *head, int idx)\n{\n\treturn vhost_get_avail(vq, *head,\n\t\t\t       &vq->avail->ring[idx & (vq->num - 1)]);\n}\n\nstatic inline int vhost_get_avail_flags(struct vhost_virtqueue *vq,\n\t\t\t\t\t__virtio16 *flags)\n{\n\treturn vhost_get_avail(vq, *flags, &vq->avail->flags);\n}\n\nstatic inline int vhost_get_used_event(struct vhost_virtqueue *vq,\n\t\t\t\t       __virtio16 *event)\n{\n\treturn vhost_get_avail(vq, *event, vhost_used_event(vq));\n}\n\nstatic inline int vhost_get_used_idx(struct vhost_virtqueue *vq,\n\t\t\t\t     __virtio16 *idx)\n{\n\treturn vhost_get_used(vq, *idx, &vq->used->idx);\n}\n\nstatic inline int vhost_get_desc(struct vhost_virtqueue *vq,\n\t\t\t\t struct vring_desc *desc, int idx)\n{\n\treturn vhost_copy_from_user(vq, desc, vq->desc + idx, sizeof(*desc));\n}\n\nstatic void vhost_iotlb_notify_vq(struct vhost_dev *d,\n\t\t\t\t  struct vhost_iotlb_msg *msg)\n{\n\tstruct vhost_msg_node *node, *n;\n\n\tspin_lock(&d->iotlb_lock);\n\n\tlist_for_each_entry_safe(node, n, &d->pending_list, node) {\n\t\tstruct vhost_iotlb_msg *vq_msg = &node->msg.iotlb;\n\t\tif (msg->iova <= vq_msg->iova &&\n\t\t    msg->iova + msg->size - 1 >= vq_msg->iova &&\n\t\t    vq_msg->type == VHOST_IOTLB_MISS) {\n\t\t\tvhost_poll_queue(&node->vq->poll);\n\t\t\tlist_del(&node->node);\n\t\t\tkfree(node);\n\t\t}\n\t}\n\n\tspin_unlock(&d->iotlb_lock);\n}\n\nstatic bool umem_access_ok(u64 uaddr, u64 size, int access)\n{\n\tunsigned long a = uaddr;\n\n\t \n\tif (vhost_overflow(uaddr, size))\n\t\treturn false;\n\n\tif ((access & VHOST_ACCESS_RO) &&\n\t    !access_ok((void __user *)a, size))\n\t\treturn false;\n\tif ((access & VHOST_ACCESS_WO) &&\n\t    !access_ok((void __user *)a, size))\n\t\treturn false;\n\treturn true;\n}\n\nstatic int vhost_process_iotlb_msg(struct vhost_dev *dev, u32 asid,\n\t\t\t\t   struct vhost_iotlb_msg *msg)\n{\n\tint ret = 0;\n\n\tif (asid != 0)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&dev->mutex);\n\tvhost_dev_lock_vqs(dev);\n\tswitch (msg->type) {\n\tcase VHOST_IOTLB_UPDATE:\n\t\tif (!dev->iotlb) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (!umem_access_ok(msg->uaddr, msg->size, msg->perm)) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tvhost_vq_meta_reset(dev);\n\t\tif (vhost_iotlb_add_range(dev->iotlb, msg->iova,\n\t\t\t\t\t  msg->iova + msg->size - 1,\n\t\t\t\t\t  msg->uaddr, msg->perm)) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\tvhost_iotlb_notify_vq(dev, msg);\n\t\tbreak;\n\tcase VHOST_IOTLB_INVALIDATE:\n\t\tif (!dev->iotlb) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tvhost_vq_meta_reset(dev);\n\t\tvhost_iotlb_del_range(dev->iotlb, msg->iova,\n\t\t\t\t      msg->iova + msg->size - 1);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tbreak;\n\t}\n\n\tvhost_dev_unlock_vqs(dev);\n\tmutex_unlock(&dev->mutex);\n\n\treturn ret;\n}\nssize_t vhost_chr_write_iter(struct vhost_dev *dev,\n\t\t\t     struct iov_iter *from)\n{\n\tstruct vhost_iotlb_msg msg;\n\tsize_t offset;\n\tint type, ret;\n\tu32 asid = 0;\n\n\tret = copy_from_iter(&type, sizeof(type), from);\n\tif (ret != sizeof(type)) {\n\t\tret = -EINVAL;\n\t\tgoto done;\n\t}\n\n\tswitch (type) {\n\tcase VHOST_IOTLB_MSG:\n\t\t \n\t\toffset = offsetof(struct vhost_msg, iotlb) - sizeof(int);\n\t\tbreak;\n\tcase VHOST_IOTLB_MSG_V2:\n\t\tif (vhost_backend_has_feature(dev->vqs[0],\n\t\t\t\t\t      VHOST_BACKEND_F_IOTLB_ASID)) {\n\t\t\tret = copy_from_iter(&asid, sizeof(asid), from);\n\t\t\tif (ret != sizeof(asid)) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto done;\n\t\t\t}\n\t\t\toffset = 0;\n\t\t} else\n\t\t\toffset = sizeof(__u32);\n\t\tbreak;\n\tdefault:\n\t\tret = -EINVAL;\n\t\tgoto done;\n\t}\n\n\tiov_iter_advance(from, offset);\n\tret = copy_from_iter(&msg, sizeof(msg), from);\n\tif (ret != sizeof(msg)) {\n\t\tret = -EINVAL;\n\t\tgoto done;\n\t}\n\n\tif (msg.type == VHOST_IOTLB_UPDATE && msg.size == 0) {\n\t\tret = -EINVAL;\n\t\tgoto done;\n\t}\n\n\tif (dev->msg_handler)\n\t\tret = dev->msg_handler(dev, asid, &msg);\n\telse\n\t\tret = vhost_process_iotlb_msg(dev, asid, &msg);\n\tif (ret) {\n\t\tret = -EFAULT;\n\t\tgoto done;\n\t}\n\n\tret = (type == VHOST_IOTLB_MSG) ? sizeof(struct vhost_msg) :\n\t      sizeof(struct vhost_msg_v2);\ndone:\n\treturn ret;\n}\nEXPORT_SYMBOL(vhost_chr_write_iter);\n\n__poll_t vhost_chr_poll(struct file *file, struct vhost_dev *dev,\n\t\t\t    poll_table *wait)\n{\n\t__poll_t mask = 0;\n\n\tpoll_wait(file, &dev->wait, wait);\n\n\tif (!list_empty(&dev->read_list))\n\t\tmask |= EPOLLIN | EPOLLRDNORM;\n\n\treturn mask;\n}\nEXPORT_SYMBOL(vhost_chr_poll);\n\nssize_t vhost_chr_read_iter(struct vhost_dev *dev, struct iov_iter *to,\n\t\t\t    int noblock)\n{\n\tDEFINE_WAIT(wait);\n\tstruct vhost_msg_node *node;\n\tssize_t ret = 0;\n\tunsigned size = sizeof(struct vhost_msg);\n\n\tif (iov_iter_count(to) < size)\n\t\treturn 0;\n\n\twhile (1) {\n\t\tif (!noblock)\n\t\t\tprepare_to_wait(&dev->wait, &wait,\n\t\t\t\t\tTASK_INTERRUPTIBLE);\n\n\t\tnode = vhost_dequeue_msg(dev, &dev->read_list);\n\t\tif (node)\n\t\t\tbreak;\n\t\tif (noblock) {\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tif (signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (!dev->iotlb) {\n\t\t\tret = -EBADFD;\n\t\t\tbreak;\n\t\t}\n\n\t\tschedule();\n\t}\n\n\tif (!noblock)\n\t\tfinish_wait(&dev->wait, &wait);\n\n\tif (node) {\n\t\tstruct vhost_iotlb_msg *msg;\n\t\tvoid *start = &node->msg;\n\n\t\tswitch (node->msg.type) {\n\t\tcase VHOST_IOTLB_MSG:\n\t\t\tsize = sizeof(node->msg);\n\t\t\tmsg = &node->msg.iotlb;\n\t\t\tbreak;\n\t\tcase VHOST_IOTLB_MSG_V2:\n\t\t\tsize = sizeof(node->msg_v2);\n\t\t\tmsg = &node->msg_v2.iotlb;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tBUG();\n\t\t\tbreak;\n\t\t}\n\n\t\tret = copy_to_iter(start, size, to);\n\t\tif (ret != size || msg->type != VHOST_IOTLB_MISS) {\n\t\t\tkfree(node);\n\t\t\treturn ret;\n\t\t}\n\t\tvhost_enqueue_msg(dev, &dev->pending_list, node);\n\t}\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(vhost_chr_read_iter);\n\nstatic int vhost_iotlb_miss(struct vhost_virtqueue *vq, u64 iova, int access)\n{\n\tstruct vhost_dev *dev = vq->dev;\n\tstruct vhost_msg_node *node;\n\tstruct vhost_iotlb_msg *msg;\n\tbool v2 = vhost_backend_has_feature(vq, VHOST_BACKEND_F_IOTLB_MSG_V2);\n\n\tnode = vhost_new_msg(vq, v2 ? VHOST_IOTLB_MSG_V2 : VHOST_IOTLB_MSG);\n\tif (!node)\n\t\treturn -ENOMEM;\n\n\tif (v2) {\n\t\tnode->msg_v2.type = VHOST_IOTLB_MSG_V2;\n\t\tmsg = &node->msg_v2.iotlb;\n\t} else {\n\t\tmsg = &node->msg.iotlb;\n\t}\n\n\tmsg->type = VHOST_IOTLB_MISS;\n\tmsg->iova = iova;\n\tmsg->perm = access;\n\n\tvhost_enqueue_msg(dev, &dev->read_list, node);\n\n\treturn 0;\n}\n\nstatic bool vq_access_ok(struct vhost_virtqueue *vq, unsigned int num,\n\t\t\t vring_desc_t __user *desc,\n\t\t\t vring_avail_t __user *avail,\n\t\t\t vring_used_t __user *used)\n\n{\n\t \n\tif (vq->iotlb)\n\t\treturn true;\n\n\treturn access_ok(desc, vhost_get_desc_size(vq, num)) &&\n\t       access_ok(avail, vhost_get_avail_size(vq, num)) &&\n\t       access_ok(used, vhost_get_used_size(vq, num));\n}\n\nstatic void vhost_vq_meta_update(struct vhost_virtqueue *vq,\n\t\t\t\t const struct vhost_iotlb_map *map,\n\t\t\t\t int type)\n{\n\tint access = (type == VHOST_ADDR_USED) ?\n\t\t     VHOST_ACCESS_WO : VHOST_ACCESS_RO;\n\n\tif (likely(map->perm & access))\n\t\tvq->meta_iotlb[type] = map;\n}\n\nstatic bool iotlb_access_ok(struct vhost_virtqueue *vq,\n\t\t\t    int access, u64 addr, u64 len, int type)\n{\n\tconst struct vhost_iotlb_map *map;\n\tstruct vhost_iotlb *umem = vq->iotlb;\n\tu64 s = 0, size, orig_addr = addr, last = addr + len - 1;\n\n\tif (vhost_vq_meta_fetch(vq, addr, len, type))\n\t\treturn true;\n\n\twhile (len > s) {\n\t\tmap = vhost_iotlb_itree_first(umem, addr, last);\n\t\tif (map == NULL || map->start > addr) {\n\t\t\tvhost_iotlb_miss(vq, addr, access);\n\t\t\treturn false;\n\t\t} else if (!(map->perm & access)) {\n\t\t\t \n\t\t\treturn false;\n\t\t}\n\n\t\tsize = map->size - addr + map->start;\n\n\t\tif (orig_addr == addr && size >= len)\n\t\t\tvhost_vq_meta_update(vq, map, type);\n\n\t\ts += size;\n\t\taddr += size;\n\t}\n\n\treturn true;\n}\n\nint vq_meta_prefetch(struct vhost_virtqueue *vq)\n{\n\tunsigned int num = vq->num;\n\n\tif (!vq->iotlb)\n\t\treturn 1;\n\n\treturn iotlb_access_ok(vq, VHOST_MAP_RO, (u64)(uintptr_t)vq->desc,\n\t\t\t       vhost_get_desc_size(vq, num), VHOST_ADDR_DESC) &&\n\t       iotlb_access_ok(vq, VHOST_MAP_RO, (u64)(uintptr_t)vq->avail,\n\t\t\t       vhost_get_avail_size(vq, num),\n\t\t\t       VHOST_ADDR_AVAIL) &&\n\t       iotlb_access_ok(vq, VHOST_MAP_WO, (u64)(uintptr_t)vq->used,\n\t\t\t       vhost_get_used_size(vq, num), VHOST_ADDR_USED);\n}\nEXPORT_SYMBOL_GPL(vq_meta_prefetch);\n\n \n \nbool vhost_log_access_ok(struct vhost_dev *dev)\n{\n\treturn memory_access_ok(dev, dev->umem, 1);\n}\nEXPORT_SYMBOL_GPL(vhost_log_access_ok);\n\nstatic bool vq_log_used_access_ok(struct vhost_virtqueue *vq,\n\t\t\t\t  void __user *log_base,\n\t\t\t\t  bool log_used,\n\t\t\t\t  u64 log_addr)\n{\n\t \n\tif (vq->iotlb)\n\t\treturn true;\n\n\treturn !log_used || log_access_ok(log_base, log_addr,\n\t\t\t\t\t  vhost_get_used_size(vq, vq->num));\n}\n\n \n \nstatic bool vq_log_access_ok(struct vhost_virtqueue *vq,\n\t\t\t     void __user *log_base)\n{\n\treturn vq_memory_access_ok(log_base, vq->umem,\n\t\t\t\t   vhost_has_feature(vq, VHOST_F_LOG_ALL)) &&\n\t\tvq_log_used_access_ok(vq, log_base, vq->log_used, vq->log_addr);\n}\n\n \n \nbool vhost_vq_access_ok(struct vhost_virtqueue *vq)\n{\n\tif (!vq_log_access_ok(vq, vq->log_base))\n\t\treturn false;\n\n\treturn vq_access_ok(vq, vq->num, vq->desc, vq->avail, vq->used);\n}\nEXPORT_SYMBOL_GPL(vhost_vq_access_ok);\n\nstatic long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)\n{\n\tstruct vhost_memory mem, *newmem;\n\tstruct vhost_memory_region *region;\n\tstruct vhost_iotlb *newumem, *oldumem;\n\tunsigned long size = offsetof(struct vhost_memory, regions);\n\tint i;\n\n\tif (copy_from_user(&mem, m, size))\n\t\treturn -EFAULT;\n\tif (mem.padding)\n\t\treturn -EOPNOTSUPP;\n\tif (mem.nregions > max_mem_regions)\n\t\treturn -E2BIG;\n\tnewmem = kvzalloc(struct_size(newmem, regions, mem.nregions),\n\t\t\tGFP_KERNEL);\n\tif (!newmem)\n\t\treturn -ENOMEM;\n\n\tmemcpy(newmem, &mem, size);\n\tif (copy_from_user(newmem->regions, m->regions,\n\t\t\t   flex_array_size(newmem, regions, mem.nregions))) {\n\t\tkvfree(newmem);\n\t\treturn -EFAULT;\n\t}\n\n\tnewumem = iotlb_alloc();\n\tif (!newumem) {\n\t\tkvfree(newmem);\n\t\treturn -ENOMEM;\n\t}\n\n\tfor (region = newmem->regions;\n\t     region < newmem->regions + mem.nregions;\n\t     region++) {\n\t\tif (vhost_iotlb_add_range(newumem,\n\t\t\t\t\t  region->guest_phys_addr,\n\t\t\t\t\t  region->guest_phys_addr +\n\t\t\t\t\t  region->memory_size - 1,\n\t\t\t\t\t  region->userspace_addr,\n\t\t\t\t\t  VHOST_MAP_RW))\n\t\t\tgoto err;\n\t}\n\n\tif (!memory_access_ok(d, newumem, 0))\n\t\tgoto err;\n\n\toldumem = d->umem;\n\td->umem = newumem;\n\n\t \n\tfor (i = 0; i < d->nvqs; ++i) {\n\t\tmutex_lock(&d->vqs[i]->mutex);\n\t\td->vqs[i]->umem = newumem;\n\t\tmutex_unlock(&d->vqs[i]->mutex);\n\t}\n\n\tkvfree(newmem);\n\tvhost_iotlb_free(oldumem);\n\treturn 0;\n\nerr:\n\tvhost_iotlb_free(newumem);\n\tkvfree(newmem);\n\treturn -EFAULT;\n}\n\nstatic long vhost_vring_set_num(struct vhost_dev *d,\n\t\t\t\tstruct vhost_virtqueue *vq,\n\t\t\t\tvoid __user *argp)\n{\n\tstruct vhost_vring_state s;\n\n\t \n\tif (vq->private_data)\n\t\treturn -EBUSY;\n\n\tif (copy_from_user(&s, argp, sizeof s))\n\t\treturn -EFAULT;\n\n\tif (!s.num || s.num > 0xffff || (s.num & (s.num - 1)))\n\t\treturn -EINVAL;\n\tvq->num = s.num;\n\n\treturn 0;\n}\n\nstatic long vhost_vring_set_addr(struct vhost_dev *d,\n\t\t\t\t struct vhost_virtqueue *vq,\n\t\t\t\t void __user *argp)\n{\n\tstruct vhost_vring_addr a;\n\n\tif (copy_from_user(&a, argp, sizeof a))\n\t\treturn -EFAULT;\n\tif (a.flags & ~(0x1 << VHOST_VRING_F_LOG))\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif ((u64)(unsigned long)a.desc_user_addr != a.desc_user_addr ||\n\t    (u64)(unsigned long)a.used_user_addr != a.used_user_addr ||\n\t    (u64)(unsigned long)a.avail_user_addr != a.avail_user_addr)\n\t\treturn -EFAULT;\n\n\t \n\tBUILD_BUG_ON(__alignof__ *vq->avail > VRING_AVAIL_ALIGN_SIZE);\n\tBUILD_BUG_ON(__alignof__ *vq->used > VRING_USED_ALIGN_SIZE);\n\tif ((a.avail_user_addr & (VRING_AVAIL_ALIGN_SIZE - 1)) ||\n\t    (a.used_user_addr & (VRING_USED_ALIGN_SIZE - 1)) ||\n\t    (a.log_guest_addr & (VRING_USED_ALIGN_SIZE - 1)))\n\t\treturn -EINVAL;\n\n\t \n\tif (vq->private_data) {\n\t\tif (!vq_access_ok(vq, vq->num,\n\t\t\t(void __user *)(unsigned long)a.desc_user_addr,\n\t\t\t(void __user *)(unsigned long)a.avail_user_addr,\n\t\t\t(void __user *)(unsigned long)a.used_user_addr))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (!vq_log_used_access_ok(vq, vq->log_base,\n\t\t\t\ta.flags & (0x1 << VHOST_VRING_F_LOG),\n\t\t\t\ta.log_guest_addr))\n\t\t\treturn -EINVAL;\n\t}\n\n\tvq->log_used = !!(a.flags & (0x1 << VHOST_VRING_F_LOG));\n\tvq->desc = (void __user *)(unsigned long)a.desc_user_addr;\n\tvq->avail = (void __user *)(unsigned long)a.avail_user_addr;\n\tvq->log_addr = a.log_guest_addr;\n\tvq->used = (void __user *)(unsigned long)a.used_user_addr;\n\n\treturn 0;\n}\n\nstatic long vhost_vring_set_num_addr(struct vhost_dev *d,\n\t\t\t\t     struct vhost_virtqueue *vq,\n\t\t\t\t     unsigned int ioctl,\n\t\t\t\t     void __user *argp)\n{\n\tlong r;\n\n\tmutex_lock(&vq->mutex);\n\n\tswitch (ioctl) {\n\tcase VHOST_SET_VRING_NUM:\n\t\tr = vhost_vring_set_num(d, vq, argp);\n\t\tbreak;\n\tcase VHOST_SET_VRING_ADDR:\n\t\tr = vhost_vring_set_addr(d, vq, argp);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tmutex_unlock(&vq->mutex);\n\n\treturn r;\n}\nlong vhost_vring_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)\n{\n\tstruct file *eventfp, *filep = NULL;\n\tbool pollstart = false, pollstop = false;\n\tstruct eventfd_ctx *ctx = NULL;\n\tstruct vhost_virtqueue *vq;\n\tstruct vhost_vring_state s;\n\tstruct vhost_vring_file f;\n\tu32 idx;\n\tlong r;\n\n\tr = vhost_get_vq_from_user(d, argp, &vq, &idx);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (ioctl == VHOST_SET_VRING_NUM ||\n\t    ioctl == VHOST_SET_VRING_ADDR) {\n\t\treturn vhost_vring_set_num_addr(d, vq, ioctl, argp);\n\t}\n\n\tmutex_lock(&vq->mutex);\n\n\tswitch (ioctl) {\n\tcase VHOST_SET_VRING_BASE:\n\t\t \n\t\tif (vq->private_data) {\n\t\t\tr = -EBUSY;\n\t\t\tbreak;\n\t\t}\n\t\tif (copy_from_user(&s, argp, sizeof s)) {\n\t\t\tr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif (vhost_has_feature(vq, VIRTIO_F_RING_PACKED)) {\n\t\t\tvq->last_avail_idx = s.num & 0xffff;\n\t\t\tvq->last_used_idx = (s.num >> 16) & 0xffff;\n\t\t} else {\n\t\t\tif (s.num > 0xffff) {\n\t\t\t\tr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tvq->last_avail_idx = s.num;\n\t\t}\n\t\t \n\t\tvq->avail_idx = vq->last_avail_idx;\n\t\tbreak;\n\tcase VHOST_GET_VRING_BASE:\n\t\ts.index = idx;\n\t\tif (vhost_has_feature(vq, VIRTIO_F_RING_PACKED))\n\t\t\ts.num = (u32)vq->last_avail_idx | ((u32)vq->last_used_idx << 16);\n\t\telse\n\t\t\ts.num = vq->last_avail_idx;\n\t\tif (copy_to_user(argp, &s, sizeof s))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\tcase VHOST_SET_VRING_KICK:\n\t\tif (copy_from_user(&f, argp, sizeof f)) {\n\t\t\tr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\teventfp = f.fd == VHOST_FILE_UNBIND ? NULL : eventfd_fget(f.fd);\n\t\tif (IS_ERR(eventfp)) {\n\t\t\tr = PTR_ERR(eventfp);\n\t\t\tbreak;\n\t\t}\n\t\tif (eventfp != vq->kick) {\n\t\t\tpollstop = (filep = vq->kick) != NULL;\n\t\t\tpollstart = (vq->kick = eventfp) != NULL;\n\t\t} else\n\t\t\tfilep = eventfp;\n\t\tbreak;\n\tcase VHOST_SET_VRING_CALL:\n\t\tif (copy_from_user(&f, argp, sizeof f)) {\n\t\t\tr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tctx = f.fd == VHOST_FILE_UNBIND ? NULL : eventfd_ctx_fdget(f.fd);\n\t\tif (IS_ERR(ctx)) {\n\t\t\tr = PTR_ERR(ctx);\n\t\t\tbreak;\n\t\t}\n\n\t\tswap(ctx, vq->call_ctx.ctx);\n\t\tbreak;\n\tcase VHOST_SET_VRING_ERR:\n\t\tif (copy_from_user(&f, argp, sizeof f)) {\n\t\t\tr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tctx = f.fd == VHOST_FILE_UNBIND ? NULL : eventfd_ctx_fdget(f.fd);\n\t\tif (IS_ERR(ctx)) {\n\t\t\tr = PTR_ERR(ctx);\n\t\t\tbreak;\n\t\t}\n\t\tswap(ctx, vq->error_ctx);\n\t\tbreak;\n\tcase VHOST_SET_VRING_ENDIAN:\n\t\tr = vhost_set_vring_endian(vq, argp);\n\t\tbreak;\n\tcase VHOST_GET_VRING_ENDIAN:\n\t\tr = vhost_get_vring_endian(vq, idx, argp);\n\t\tbreak;\n\tcase VHOST_SET_VRING_BUSYLOOP_TIMEOUT:\n\t\tif (copy_from_user(&s, argp, sizeof(s))) {\n\t\t\tr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tvq->busyloop_timeout = s.num;\n\t\tbreak;\n\tcase VHOST_GET_VRING_BUSYLOOP_TIMEOUT:\n\t\ts.index = idx;\n\t\ts.num = vq->busyloop_timeout;\n\t\tif (copy_to_user(argp, &s, sizeof(s)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\tdefault:\n\t\tr = -ENOIOCTLCMD;\n\t}\n\n\tif (pollstop && vq->handle_kick)\n\t\tvhost_poll_stop(&vq->poll);\n\n\tif (!IS_ERR_OR_NULL(ctx))\n\t\teventfd_ctx_put(ctx);\n\tif (filep)\n\t\tfput(filep);\n\n\tif (pollstart && vq->handle_kick)\n\t\tr = vhost_poll_start(&vq->poll, vq->kick);\n\n\tmutex_unlock(&vq->mutex);\n\n\tif (pollstop && vq->handle_kick)\n\t\tvhost_dev_flush(vq->poll.dev);\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(vhost_vring_ioctl);\n\nint vhost_init_device_iotlb(struct vhost_dev *d)\n{\n\tstruct vhost_iotlb *niotlb, *oiotlb;\n\tint i;\n\n\tniotlb = iotlb_alloc();\n\tif (!niotlb)\n\t\treturn -ENOMEM;\n\n\toiotlb = d->iotlb;\n\td->iotlb = niotlb;\n\n\tfor (i = 0; i < d->nvqs; ++i) {\n\t\tstruct vhost_virtqueue *vq = d->vqs[i];\n\n\t\tmutex_lock(&vq->mutex);\n\t\tvq->iotlb = niotlb;\n\t\t__vhost_vq_meta_reset(vq);\n\t\tmutex_unlock(&vq->mutex);\n\t}\n\n\tvhost_iotlb_free(oiotlb);\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(vhost_init_device_iotlb);\n\n \nlong vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)\n{\n\tstruct eventfd_ctx *ctx;\n\tu64 p;\n\tlong r;\n\tint i, fd;\n\n\t \n\tif (ioctl == VHOST_SET_OWNER) {\n\t\tr = vhost_dev_set_owner(d);\n\t\tgoto done;\n\t}\n\n\t \n\tr = vhost_dev_check_owner(d);\n\tif (r)\n\t\tgoto done;\n\n\tswitch (ioctl) {\n\tcase VHOST_SET_MEM_TABLE:\n\t\tr = vhost_set_memory(d, argp);\n\t\tbreak;\n\tcase VHOST_SET_LOG_BASE:\n\t\tif (copy_from_user(&p, argp, sizeof p)) {\n\t\t\tr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tif ((u64)(unsigned long)p != p) {\n\t\t\tr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tfor (i = 0; i < d->nvqs; ++i) {\n\t\t\tstruct vhost_virtqueue *vq;\n\t\t\tvoid __user *base = (void __user *)(unsigned long)p;\n\t\t\tvq = d->vqs[i];\n\t\t\tmutex_lock(&vq->mutex);\n\t\t\t \n\t\t\tif (vq->private_data && !vq_log_access_ok(vq, base))\n\t\t\t\tr = -EFAULT;\n\t\t\telse\n\t\t\t\tvq->log_base = base;\n\t\t\tmutex_unlock(&vq->mutex);\n\t\t}\n\t\tbreak;\n\tcase VHOST_SET_LOG_FD:\n\t\tr = get_user(fd, (int __user *)argp);\n\t\tif (r < 0)\n\t\t\tbreak;\n\t\tctx = fd == VHOST_FILE_UNBIND ? NULL : eventfd_ctx_fdget(fd);\n\t\tif (IS_ERR(ctx)) {\n\t\t\tr = PTR_ERR(ctx);\n\t\t\tbreak;\n\t\t}\n\t\tswap(ctx, d->log_ctx);\n\t\tfor (i = 0; i < d->nvqs; ++i) {\n\t\t\tmutex_lock(&d->vqs[i]->mutex);\n\t\t\td->vqs[i]->log_ctx = d->log_ctx;\n\t\t\tmutex_unlock(&d->vqs[i]->mutex);\n\t\t}\n\t\tif (ctx)\n\t\t\teventfd_ctx_put(ctx);\n\t\tbreak;\n\tdefault:\n\t\tr = -ENOIOCTLCMD;\n\t\tbreak;\n\t}\ndone:\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(vhost_dev_ioctl);\n\n \nstatic int set_bit_to_user(int nr, void __user *addr)\n{\n\tunsigned long log = (unsigned long)addr;\n\tstruct page *page;\n\tvoid *base;\n\tint bit = nr + (log % PAGE_SIZE) * 8;\n\tint r;\n\n\tr = pin_user_pages_fast(log, 1, FOLL_WRITE, &page);\n\tif (r < 0)\n\t\treturn r;\n\tBUG_ON(r != 1);\n\tbase = kmap_atomic(page);\n\tset_bit(bit, base);\n\tkunmap_atomic(base);\n\tunpin_user_pages_dirty_lock(&page, 1, true);\n\treturn 0;\n}\n\nstatic int log_write(void __user *log_base,\n\t\t     u64 write_address, u64 write_length)\n{\n\tu64 write_page = write_address / VHOST_PAGE_SIZE;\n\tint r;\n\n\tif (!write_length)\n\t\treturn 0;\n\twrite_length += write_address % VHOST_PAGE_SIZE;\n\tfor (;;) {\n\t\tu64 base = (u64)(unsigned long)log_base;\n\t\tu64 log = base + write_page / 8;\n\t\tint bit = write_page % 8;\n\t\tif ((u64)(unsigned long)log != log)\n\t\t\treturn -EFAULT;\n\t\tr = set_bit_to_user(bit, (void __user *)(unsigned long)log);\n\t\tif (r < 0)\n\t\t\treturn r;\n\t\tif (write_length <= VHOST_PAGE_SIZE)\n\t\t\tbreak;\n\t\twrite_length -= VHOST_PAGE_SIZE;\n\t\twrite_page += 1;\n\t}\n\treturn r;\n}\n\nstatic int log_write_hva(struct vhost_virtqueue *vq, u64 hva, u64 len)\n{\n\tstruct vhost_iotlb *umem = vq->umem;\n\tstruct vhost_iotlb_map *u;\n\tu64 start, end, l, min;\n\tint r;\n\tbool hit = false;\n\n\twhile (len) {\n\t\tmin = len;\n\t\t \n\t\tlist_for_each_entry(u, &umem->list, link) {\n\t\t\tif (u->addr > hva - 1 + len ||\n\t\t\t    u->addr - 1 + u->size < hva)\n\t\t\t\tcontinue;\n\t\t\tstart = max(u->addr, hva);\n\t\t\tend = min(u->addr - 1 + u->size, hva - 1 + len);\n\t\t\tl = end - start + 1;\n\t\t\tr = log_write(vq->log_base,\n\t\t\t\t      u->start + start - u->addr,\n\t\t\t\t      l);\n\t\t\tif (r < 0)\n\t\t\t\treturn r;\n\t\t\thit = true;\n\t\t\tmin = min(l, min);\n\t\t}\n\n\t\tif (!hit)\n\t\t\treturn -EFAULT;\n\n\t\tlen -= min;\n\t\thva += min;\n\t}\n\n\treturn 0;\n}\n\nstatic int log_used(struct vhost_virtqueue *vq, u64 used_offset, u64 len)\n{\n\tstruct iovec *iov = vq->log_iov;\n\tint i, ret;\n\n\tif (!vq->iotlb)\n\t\treturn log_write(vq->log_base, vq->log_addr + used_offset, len);\n\n\tret = translate_desc(vq, (uintptr_t)vq->used + used_offset,\n\t\t\t     len, iov, 64, VHOST_ACCESS_WO);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tfor (i = 0; i < ret; i++) {\n\t\tret = log_write_hva(vq,\t(uintptr_t)iov[i].iov_base,\n\t\t\t\t    iov[i].iov_len);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nint vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,\n\t\t    unsigned int log_num, u64 len, struct iovec *iov, int count)\n{\n\tint i, r;\n\n\t \n\tsmp_wmb();\n\n\tif (vq->iotlb) {\n\t\tfor (i = 0; i < count; i++) {\n\t\t\tr = log_write_hva(vq, (uintptr_t)iov[i].iov_base,\n\t\t\t\t\t  iov[i].iov_len);\n\t\t\tif (r < 0)\n\t\t\t\treturn r;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tfor (i = 0; i < log_num; ++i) {\n\t\tu64 l = min(log[i].len, len);\n\t\tr = log_write(vq->log_base, log[i].addr, l);\n\t\tif (r < 0)\n\t\t\treturn r;\n\t\tlen -= l;\n\t\tif (!len) {\n\t\t\tif (vq->log_ctx)\n\t\t\t\teventfd_signal(vq->log_ctx, 1);\n\t\t\treturn 0;\n\t\t}\n\t}\n\t \n\tBUG();\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(vhost_log_write);\n\nstatic int vhost_update_used_flags(struct vhost_virtqueue *vq)\n{\n\tvoid __user *used;\n\tif (vhost_put_used_flags(vq))\n\t\treturn -EFAULT;\n\tif (unlikely(vq->log_used)) {\n\t\t \n\t\tsmp_wmb();\n\t\t \n\t\tused = &vq->used->flags;\n\t\tlog_used(vq, (used - (void __user *)vq->used),\n\t\t\t sizeof vq->used->flags);\n\t\tif (vq->log_ctx)\n\t\t\teventfd_signal(vq->log_ctx, 1);\n\t}\n\treturn 0;\n}\n\nstatic int vhost_update_avail_event(struct vhost_virtqueue *vq)\n{\n\tif (vhost_put_avail_event(vq))\n\t\treturn -EFAULT;\n\tif (unlikely(vq->log_used)) {\n\t\tvoid __user *used;\n\t\t \n\t\tsmp_wmb();\n\t\t \n\t\tused = vhost_avail_event(vq);\n\t\tlog_used(vq, (used - (void __user *)vq->used),\n\t\t\t sizeof *vhost_avail_event(vq));\n\t\tif (vq->log_ctx)\n\t\t\teventfd_signal(vq->log_ctx, 1);\n\t}\n\treturn 0;\n}\n\nint vhost_vq_init_access(struct vhost_virtqueue *vq)\n{\n\t__virtio16 last_used_idx;\n\tint r;\n\tbool is_le = vq->is_le;\n\n\tif (!vq->private_data)\n\t\treturn 0;\n\n\tvhost_init_is_le(vq);\n\n\tr = vhost_update_used_flags(vq);\n\tif (r)\n\t\tgoto err;\n\tvq->signalled_used_valid = false;\n\tif (!vq->iotlb &&\n\t    !access_ok(&vq->used->idx, sizeof vq->used->idx)) {\n\t\tr = -EFAULT;\n\t\tgoto err;\n\t}\n\tr = vhost_get_used_idx(vq, &last_used_idx);\n\tif (r) {\n\t\tvq_err(vq, \"Can't access used idx at %p\\n\",\n\t\t       &vq->used->idx);\n\t\tgoto err;\n\t}\n\tvq->last_used_idx = vhost16_to_cpu(vq, last_used_idx);\n\treturn 0;\n\nerr:\n\tvq->is_le = is_le;\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(vhost_vq_init_access);\n\nstatic int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,\n\t\t\t  struct iovec iov[], int iov_size, int access)\n{\n\tconst struct vhost_iotlb_map *map;\n\tstruct vhost_dev *dev = vq->dev;\n\tstruct vhost_iotlb *umem = dev->iotlb ? dev->iotlb : dev->umem;\n\tstruct iovec *_iov;\n\tu64 s = 0, last = addr + len - 1;\n\tint ret = 0;\n\n\twhile ((u64)len > s) {\n\t\tu64 size;\n\t\tif (unlikely(ret >= iov_size)) {\n\t\t\tret = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\n\t\tmap = vhost_iotlb_itree_first(umem, addr, last);\n\t\tif (map == NULL || map->start > addr) {\n\t\t\tif (umem != dev->iotlb) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t} else if (!(map->perm & access)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\n\t\t_iov = iov + ret;\n\t\tsize = map->size - addr + map->start;\n\t\t_iov->iov_len = min((u64)len - s, size);\n\t\t_iov->iov_base = (void __user *)(unsigned long)\n\t\t\t\t (map->addr + addr - map->start);\n\t\ts += size;\n\t\taddr += size;\n\t\t++ret;\n\t}\n\n\tif (ret == -EAGAIN)\n\t\tvhost_iotlb_miss(vq, addr, access);\n\treturn ret;\n}\n\n \nstatic unsigned next_desc(struct vhost_virtqueue *vq, struct vring_desc *desc)\n{\n\tunsigned int next;\n\n\t \n\tif (!(desc->flags & cpu_to_vhost16(vq, VRING_DESC_F_NEXT)))\n\t\treturn -1U;\n\n\t \n\tnext = vhost16_to_cpu(vq, READ_ONCE(desc->next));\n\treturn next;\n}\n\nstatic int get_indirect(struct vhost_virtqueue *vq,\n\t\t\tstruct iovec iov[], unsigned int iov_size,\n\t\t\tunsigned int *out_num, unsigned int *in_num,\n\t\t\tstruct vhost_log *log, unsigned int *log_num,\n\t\t\tstruct vring_desc *indirect)\n{\n\tstruct vring_desc desc;\n\tunsigned int i = 0, count, found = 0;\n\tu32 len = vhost32_to_cpu(vq, indirect->len);\n\tstruct iov_iter from;\n\tint ret, access;\n\n\t \n\tif (unlikely(len % sizeof desc)) {\n\t\tvq_err(vq, \"Invalid length in indirect descriptor: \"\n\t\t       \"len 0x%llx not multiple of 0x%zx\\n\",\n\t\t       (unsigned long long)len,\n\t\t       sizeof desc);\n\t\treturn -EINVAL;\n\t}\n\n\tret = translate_desc(vq, vhost64_to_cpu(vq, indirect->addr), len, vq->indirect,\n\t\t\t     UIO_MAXIOV, VHOST_ACCESS_RO);\n\tif (unlikely(ret < 0)) {\n\t\tif (ret != -EAGAIN)\n\t\t\tvq_err(vq, \"Translation failure %d in indirect.\\n\", ret);\n\t\treturn ret;\n\t}\n\tiov_iter_init(&from, ITER_SOURCE, vq->indirect, ret, len);\n\tcount = len / sizeof desc;\n\t \n\tif (unlikely(count > USHRT_MAX + 1)) {\n\t\tvq_err(vq, \"Indirect buffer length too big: %d\\n\",\n\t\t       indirect->len);\n\t\treturn -E2BIG;\n\t}\n\n\tdo {\n\t\tunsigned iov_count = *in_num + *out_num;\n\t\tif (unlikely(++found > count)) {\n\t\t\tvq_err(vq, \"Loop detected: last one at %u \"\n\t\t\t       \"indirect size %u\\n\",\n\t\t\t       i, count);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (unlikely(!copy_from_iter_full(&desc, sizeof(desc), &from))) {\n\t\t\tvq_err(vq, \"Failed indirect descriptor: idx %d, %zx\\n\",\n\t\t\t       i, (size_t)vhost64_to_cpu(vq, indirect->addr) + i * sizeof desc);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (unlikely(desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_INDIRECT))) {\n\t\t\tvq_err(vq, \"Nested indirect descriptor: idx %d, %zx\\n\",\n\t\t\t       i, (size_t)vhost64_to_cpu(vq, indirect->addr) + i * sizeof desc);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_WRITE))\n\t\t\taccess = VHOST_ACCESS_WO;\n\t\telse\n\t\t\taccess = VHOST_ACCESS_RO;\n\n\t\tret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),\n\t\t\t\t     vhost32_to_cpu(vq, desc.len), iov + iov_count,\n\t\t\t\t     iov_size - iov_count, access);\n\t\tif (unlikely(ret < 0)) {\n\t\t\tif (ret != -EAGAIN)\n\t\t\t\tvq_err(vq, \"Translation failure %d indirect idx %d\\n\",\n\t\t\t\t\tret, i);\n\t\t\treturn ret;\n\t\t}\n\t\t \n\t\tif (access == VHOST_ACCESS_WO) {\n\t\t\t*in_num += ret;\n\t\t\tif (unlikely(log && ret)) {\n\t\t\t\tlog[*log_num].addr = vhost64_to_cpu(vq, desc.addr);\n\t\t\t\tlog[*log_num].len = vhost32_to_cpu(vq, desc.len);\n\t\t\t\t++*log_num;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tif (unlikely(*in_num)) {\n\t\t\t\tvq_err(vq, \"Indirect descriptor \"\n\t\t\t\t       \"has out after in: idx %d\\n\", i);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t*out_num += ret;\n\t\t}\n\t} while ((i = next_desc(vq, &desc)) != -1);\n\treturn 0;\n}\n\n \nint vhost_get_vq_desc(struct vhost_virtqueue *vq,\n\t\t      struct iovec iov[], unsigned int iov_size,\n\t\t      unsigned int *out_num, unsigned int *in_num,\n\t\t      struct vhost_log *log, unsigned int *log_num)\n{\n\tstruct vring_desc desc;\n\tunsigned int i, head, found = 0;\n\tu16 last_avail_idx;\n\t__virtio16 avail_idx;\n\t__virtio16 ring_head;\n\tint ret, access;\n\n\t \n\tlast_avail_idx = vq->last_avail_idx;\n\n\tif (vq->avail_idx == vq->last_avail_idx) {\n\t\tif (unlikely(vhost_get_avail_idx(vq, &avail_idx))) {\n\t\t\tvq_err(vq, \"Failed to access avail idx at %p\\n\",\n\t\t\t\t&vq->avail->idx);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tvq->avail_idx = vhost16_to_cpu(vq, avail_idx);\n\n\t\tif (unlikely((u16)(vq->avail_idx - last_avail_idx) > vq->num)) {\n\t\t\tvq_err(vq, \"Guest moved used index from %u to %u\",\n\t\t\t\tlast_avail_idx, vq->avail_idx);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\t \n\t\tif (vq->avail_idx == last_avail_idx)\n\t\t\treturn vq->num;\n\n\t\t \n\t\tsmp_rmb();\n\t}\n\n\t \n\tif (unlikely(vhost_get_avail_head(vq, &ring_head, last_avail_idx))) {\n\t\tvq_err(vq, \"Failed to read head: idx %d address %p\\n\",\n\t\t       last_avail_idx,\n\t\t       &vq->avail->ring[last_avail_idx % vq->num]);\n\t\treturn -EFAULT;\n\t}\n\n\thead = vhost16_to_cpu(vq, ring_head);\n\n\t \n\tif (unlikely(head >= vq->num)) {\n\t\tvq_err(vq, \"Guest says index %u > %u is available\",\n\t\t       head, vq->num);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\t*out_num = *in_num = 0;\n\tif (unlikely(log))\n\t\t*log_num = 0;\n\n\ti = head;\n\tdo {\n\t\tunsigned iov_count = *in_num + *out_num;\n\t\tif (unlikely(i >= vq->num)) {\n\t\t\tvq_err(vq, \"Desc index is %u > %u, head = %u\",\n\t\t\t       i, vq->num, head);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tif (unlikely(++found > vq->num)) {\n\t\t\tvq_err(vq, \"Loop detected: last one at %u \"\n\t\t\t       \"vq size %u head %u\\n\",\n\t\t\t       i, vq->num, head);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tret = vhost_get_desc(vq, &desc, i);\n\t\tif (unlikely(ret)) {\n\t\t\tvq_err(vq, \"Failed to get descriptor: idx %d addr %p\\n\",\n\t\t\t       i, vq->desc + i);\n\t\t\treturn -EFAULT;\n\t\t}\n\t\tif (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_INDIRECT)) {\n\t\t\tret = get_indirect(vq, iov, iov_size,\n\t\t\t\t\t   out_num, in_num,\n\t\t\t\t\t   log, log_num, &desc);\n\t\t\tif (unlikely(ret < 0)) {\n\t\t\t\tif (ret != -EAGAIN)\n\t\t\t\t\tvq_err(vq, \"Failure detected \"\n\t\t\t\t\t\t\"in indirect descriptor at idx %d\\n\", i);\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (desc.flags & cpu_to_vhost16(vq, VRING_DESC_F_WRITE))\n\t\t\taccess = VHOST_ACCESS_WO;\n\t\telse\n\t\t\taccess = VHOST_ACCESS_RO;\n\t\tret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),\n\t\t\t\t     vhost32_to_cpu(vq, desc.len), iov + iov_count,\n\t\t\t\t     iov_size - iov_count, access);\n\t\tif (unlikely(ret < 0)) {\n\t\t\tif (ret != -EAGAIN)\n\t\t\t\tvq_err(vq, \"Translation failure %d descriptor idx %d\\n\",\n\t\t\t\t\tret, i);\n\t\t\treturn ret;\n\t\t}\n\t\tif (access == VHOST_ACCESS_WO) {\n\t\t\t \n\t\t\t*in_num += ret;\n\t\t\tif (unlikely(log && ret)) {\n\t\t\t\tlog[*log_num].addr = vhost64_to_cpu(vq, desc.addr);\n\t\t\t\tlog[*log_num].len = vhost32_to_cpu(vq, desc.len);\n\t\t\t\t++*log_num;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tif (unlikely(*in_num)) {\n\t\t\t\tvq_err(vq, \"Descriptor has out after in: \"\n\t\t\t\t       \"idx %d\\n\", i);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t*out_num += ret;\n\t\t}\n\t} while ((i = next_desc(vq, &desc)) != -1);\n\n\t \n\tvq->last_avail_idx++;\n\n\t \n\tBUG_ON(!(vq->used_flags & VRING_USED_F_NO_NOTIFY));\n\treturn head;\n}\nEXPORT_SYMBOL_GPL(vhost_get_vq_desc);\n\n \nvoid vhost_discard_vq_desc(struct vhost_virtqueue *vq, int n)\n{\n\tvq->last_avail_idx -= n;\n}\nEXPORT_SYMBOL_GPL(vhost_discard_vq_desc);\n\n \nint vhost_add_used(struct vhost_virtqueue *vq, unsigned int head, int len)\n{\n\tstruct vring_used_elem heads = {\n\t\tcpu_to_vhost32(vq, head),\n\t\tcpu_to_vhost32(vq, len)\n\t};\n\n\treturn vhost_add_used_n(vq, &heads, 1);\n}\nEXPORT_SYMBOL_GPL(vhost_add_used);\n\nstatic int __vhost_add_used_n(struct vhost_virtqueue *vq,\n\t\t\t    struct vring_used_elem *heads,\n\t\t\t    unsigned count)\n{\n\tvring_used_elem_t __user *used;\n\tu16 old, new;\n\tint start;\n\n\tstart = vq->last_used_idx & (vq->num - 1);\n\tused = vq->used->ring + start;\n\tif (vhost_put_used(vq, heads, start, count)) {\n\t\tvq_err(vq, \"Failed to write used\");\n\t\treturn -EFAULT;\n\t}\n\tif (unlikely(vq->log_used)) {\n\t\t \n\t\tsmp_wmb();\n\t\t \n\t\tlog_used(vq, ((void __user *)used - (void __user *)vq->used),\n\t\t\t count * sizeof *used);\n\t}\n\told = vq->last_used_idx;\n\tnew = (vq->last_used_idx += count);\n\t \n\tif (unlikely((u16)(new - vq->signalled_used) < (u16)(new - old)))\n\t\tvq->signalled_used_valid = false;\n\treturn 0;\n}\n\n \nint vhost_add_used_n(struct vhost_virtqueue *vq, struct vring_used_elem *heads,\n\t\t     unsigned count)\n{\n\tint start, n, r;\n\n\tstart = vq->last_used_idx & (vq->num - 1);\n\tn = vq->num - start;\n\tif (n < count) {\n\t\tr = __vhost_add_used_n(vq, heads, n);\n\t\tif (r < 0)\n\t\t\treturn r;\n\t\theads += n;\n\t\tcount -= n;\n\t}\n\tr = __vhost_add_used_n(vq, heads, count);\n\n\t \n\tsmp_wmb();\n\tif (vhost_put_used_idx(vq)) {\n\t\tvq_err(vq, \"Failed to increment used idx\");\n\t\treturn -EFAULT;\n\t}\n\tif (unlikely(vq->log_used)) {\n\t\t \n\t\tsmp_wmb();\n\t\t \n\t\tlog_used(vq, offsetof(struct vring_used, idx),\n\t\t\t sizeof vq->used->idx);\n\t\tif (vq->log_ctx)\n\t\t\teventfd_signal(vq->log_ctx, 1);\n\t}\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(vhost_add_used_n);\n\nstatic bool vhost_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)\n{\n\t__u16 old, new;\n\t__virtio16 event;\n\tbool v;\n\t \n\tsmp_mb();\n\n\tif (vhost_has_feature(vq, VIRTIO_F_NOTIFY_ON_EMPTY) &&\n\t    unlikely(vq->avail_idx == vq->last_avail_idx))\n\t\treturn true;\n\n\tif (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {\n\t\t__virtio16 flags;\n\t\tif (vhost_get_avail_flags(vq, &flags)) {\n\t\t\tvq_err(vq, \"Failed to get flags\");\n\t\t\treturn true;\n\t\t}\n\t\treturn !(flags & cpu_to_vhost16(vq, VRING_AVAIL_F_NO_INTERRUPT));\n\t}\n\told = vq->signalled_used;\n\tv = vq->signalled_used_valid;\n\tnew = vq->signalled_used = vq->last_used_idx;\n\tvq->signalled_used_valid = true;\n\n\tif (unlikely(!v))\n\t\treturn true;\n\n\tif (vhost_get_used_event(vq, &event)) {\n\t\tvq_err(vq, \"Failed to get used event idx\");\n\t\treturn true;\n\t}\n\treturn vring_need_event(vhost16_to_cpu(vq, event), new, old);\n}\n\n \nvoid vhost_signal(struct vhost_dev *dev, struct vhost_virtqueue *vq)\n{\n\t \n\tif (vq->call_ctx.ctx && vhost_notify(dev, vq))\n\t\teventfd_signal(vq->call_ctx.ctx, 1);\n}\nEXPORT_SYMBOL_GPL(vhost_signal);\n\n \nvoid vhost_add_used_and_signal(struct vhost_dev *dev,\n\t\t\t       struct vhost_virtqueue *vq,\n\t\t\t       unsigned int head, int len)\n{\n\tvhost_add_used(vq, head, len);\n\tvhost_signal(dev, vq);\n}\nEXPORT_SYMBOL_GPL(vhost_add_used_and_signal);\n\n \nvoid vhost_add_used_and_signal_n(struct vhost_dev *dev,\n\t\t\t\t struct vhost_virtqueue *vq,\n\t\t\t\t struct vring_used_elem *heads, unsigned count)\n{\n\tvhost_add_used_n(vq, heads, count);\n\tvhost_signal(dev, vq);\n}\nEXPORT_SYMBOL_GPL(vhost_add_used_and_signal_n);\n\n \nbool vhost_vq_avail_empty(struct vhost_dev *dev, struct vhost_virtqueue *vq)\n{\n\t__virtio16 avail_idx;\n\tint r;\n\n\tif (vq->avail_idx != vq->last_avail_idx)\n\t\treturn false;\n\n\tr = vhost_get_avail_idx(vq, &avail_idx);\n\tif (unlikely(r))\n\t\treturn false;\n\tvq->avail_idx = vhost16_to_cpu(vq, avail_idx);\n\n\treturn vq->avail_idx == vq->last_avail_idx;\n}\nEXPORT_SYMBOL_GPL(vhost_vq_avail_empty);\n\n \nbool vhost_enable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)\n{\n\t__virtio16 avail_idx;\n\tint r;\n\n\tif (!(vq->used_flags & VRING_USED_F_NO_NOTIFY))\n\t\treturn false;\n\tvq->used_flags &= ~VRING_USED_F_NO_NOTIFY;\n\tif (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {\n\t\tr = vhost_update_used_flags(vq);\n\t\tif (r) {\n\t\t\tvq_err(vq, \"Failed to enable notification at %p: %d\\n\",\n\t\t\t       &vq->used->flags, r);\n\t\t\treturn false;\n\t\t}\n\t} else {\n\t\tr = vhost_update_avail_event(vq);\n\t\tif (r) {\n\t\t\tvq_err(vq, \"Failed to update avail event index at %p: %d\\n\",\n\t\t\t       vhost_avail_event(vq), r);\n\t\t\treturn false;\n\t\t}\n\t}\n\t \n\tsmp_mb();\n\tr = vhost_get_avail_idx(vq, &avail_idx);\n\tif (r) {\n\t\tvq_err(vq, \"Failed to check avail idx at %p: %d\\n\",\n\t\t       &vq->avail->idx, r);\n\t\treturn false;\n\t}\n\tvq->avail_idx = vhost16_to_cpu(vq, avail_idx);\n\n\treturn vq->avail_idx != vq->last_avail_idx;\n}\nEXPORT_SYMBOL_GPL(vhost_enable_notify);\n\n \nvoid vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)\n{\n\tint r;\n\n\tif (vq->used_flags & VRING_USED_F_NO_NOTIFY)\n\t\treturn;\n\tvq->used_flags |= VRING_USED_F_NO_NOTIFY;\n\tif (!vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX)) {\n\t\tr = vhost_update_used_flags(vq);\n\t\tif (r)\n\t\t\tvq_err(vq, \"Failed to disable notification at %p: %d\\n\",\n\t\t\t       &vq->used->flags, r);\n\t}\n}\nEXPORT_SYMBOL_GPL(vhost_disable_notify);\n\n \nstruct vhost_msg_node *vhost_new_msg(struct vhost_virtqueue *vq, int type)\n{\n\t \n\tstruct vhost_msg_node *node = kzalloc(sizeof(*node), GFP_KERNEL);\n\tif (!node)\n\t\treturn NULL;\n\n\tnode->vq = vq;\n\tnode->msg.type = type;\n\treturn node;\n}\nEXPORT_SYMBOL_GPL(vhost_new_msg);\n\nvoid vhost_enqueue_msg(struct vhost_dev *dev, struct list_head *head,\n\t\t       struct vhost_msg_node *node)\n{\n\tspin_lock(&dev->iotlb_lock);\n\tlist_add_tail(&node->node, head);\n\tspin_unlock(&dev->iotlb_lock);\n\n\twake_up_interruptible_poll(&dev->wait, EPOLLIN | EPOLLRDNORM);\n}\nEXPORT_SYMBOL_GPL(vhost_enqueue_msg);\n\nstruct vhost_msg_node *vhost_dequeue_msg(struct vhost_dev *dev,\n\t\t\t\t\t struct list_head *head)\n{\n\tstruct vhost_msg_node *node = NULL;\n\n\tspin_lock(&dev->iotlb_lock);\n\tif (!list_empty(head)) {\n\t\tnode = list_first_entry(head, struct vhost_msg_node,\n\t\t\t\t\tnode);\n\t\tlist_del(&node->node);\n\t}\n\tspin_unlock(&dev->iotlb_lock);\n\n\treturn node;\n}\nEXPORT_SYMBOL_GPL(vhost_dequeue_msg);\n\nvoid vhost_set_backend_features(struct vhost_dev *dev, u64 features)\n{\n\tstruct vhost_virtqueue *vq;\n\tint i;\n\n\tmutex_lock(&dev->mutex);\n\tfor (i = 0; i < dev->nvqs; ++i) {\n\t\tvq = dev->vqs[i];\n\t\tmutex_lock(&vq->mutex);\n\t\tvq->acked_backend_features = features;\n\t\tmutex_unlock(&vq->mutex);\n\t}\n\tmutex_unlock(&dev->mutex);\n}\nEXPORT_SYMBOL_GPL(vhost_set_backend_features);\n\nstatic int __init vhost_init(void)\n{\n\treturn 0;\n}\n\nstatic void __exit vhost_exit(void)\n{\n}\n\nmodule_init(vhost_init);\nmodule_exit(vhost_exit);\n\nMODULE_VERSION(\"0.0.1\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Michael S. Tsirkin\");\nMODULE_DESCRIPTION(\"Host kernel accelerator for virtio\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}