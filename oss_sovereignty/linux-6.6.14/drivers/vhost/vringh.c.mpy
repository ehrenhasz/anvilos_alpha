{
  "module_name": "vringh.c",
  "hash_id": "74da1476b3b687ce535dcaa3b8f7cb5635f23adb32a39cfac91a20ec00ac4d4d",
  "original_prompt": "Ingested from linux-6.6.14/drivers/vhost/vringh.c",
  "human_readable_source": "\n \n#include <linux/compiler.h>\n#include <linux/module.h>\n#include <linux/vringh.h>\n#include <linux/virtio_ring.h>\n#include <linux/kernel.h>\n#include <linux/ratelimit.h>\n#include <linux/uaccess.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n#if IS_REACHABLE(CONFIG_VHOST_IOTLB)\n#include <linux/bvec.h>\n#include <linux/highmem.h>\n#include <linux/vhost_iotlb.h>\n#endif\n#include <uapi/linux/virtio_config.h>\n\nstatic __printf(1,2) __cold void vringh_bad(const char *fmt, ...)\n{\n\tstatic DEFINE_RATELIMIT_STATE(vringh_rs,\n\t\t\t\t      DEFAULT_RATELIMIT_INTERVAL,\n\t\t\t\t      DEFAULT_RATELIMIT_BURST);\n\tif (__ratelimit(&vringh_rs)) {\n\t\tva_list ap;\n\t\tva_start(ap, fmt);\n\t\tprintk(KERN_NOTICE \"vringh:\");\n\t\tvprintk(fmt, ap);\n\t\tva_end(ap);\n\t}\n}\n\n \nstatic inline int __vringh_get_head(const struct vringh *vrh,\n\t\t\t\t    int (*getu16)(const struct vringh *vrh,\n\t\t\t\t\t\t  u16 *val, const __virtio16 *p),\n\t\t\t\t    u16 *last_avail_idx)\n{\n\tu16 avail_idx, i, head;\n\tint err;\n\n\terr = getu16(vrh, &avail_idx, &vrh->vring.avail->idx);\n\tif (err) {\n\t\tvringh_bad(\"Failed to access avail idx at %p\",\n\t\t\t   &vrh->vring.avail->idx);\n\t\treturn err;\n\t}\n\n\tif (*last_avail_idx == avail_idx)\n\t\treturn vrh->vring.num;\n\n\t \n\tvirtio_rmb(vrh->weak_barriers);\n\n\ti = *last_avail_idx & (vrh->vring.num - 1);\n\n\terr = getu16(vrh, &head, &vrh->vring.avail->ring[i]);\n\tif (err) {\n\t\tvringh_bad(\"Failed to read head: idx %d address %p\",\n\t\t\t   *last_avail_idx, &vrh->vring.avail->ring[i]);\n\t\treturn err;\n\t}\n\n\tif (head >= vrh->vring.num) {\n\t\tvringh_bad(\"Guest says index %u > %u is available\",\n\t\t\t   head, vrh->vring.num);\n\t\treturn -EINVAL;\n\t}\n\n\t(*last_avail_idx)++;\n\treturn head;\n}\n\n \nvoid vringh_kiov_advance(struct vringh_kiov *iov, size_t len)\n{\n\twhile (len && iov->i < iov->used) {\n\t\tsize_t partlen = min(iov->iov[iov->i].iov_len, len);\n\n\t\tiov->consumed += partlen;\n\t\tiov->iov[iov->i].iov_len -= partlen;\n\t\tiov->iov[iov->i].iov_base += partlen;\n\n\t\tif (!iov->iov[iov->i].iov_len) {\n\t\t\t \n\t\t\tiov->iov[iov->i].iov_len = iov->consumed;\n\t\t\tiov->iov[iov->i].iov_base -= iov->consumed;\n\n\t\t\tiov->consumed = 0;\n\t\t\tiov->i++;\n\t\t}\n\n\t\tlen -= partlen;\n\t}\n}\nEXPORT_SYMBOL(vringh_kiov_advance);\n\n \nstatic inline ssize_t vringh_iov_xfer(struct vringh *vrh,\n\t\t\t\t      struct vringh_kiov *iov,\n\t\t\t\t      void *ptr, size_t len,\n\t\t\t\t      int (*xfer)(const struct vringh *vrh,\n\t\t\t\t\t\t  void *addr, void *ptr,\n\t\t\t\t\t\t  size_t len))\n{\n\tint err, done = 0;\n\n\twhile (len && iov->i < iov->used) {\n\t\tsize_t partlen;\n\n\t\tpartlen = min(iov->iov[iov->i].iov_len, len);\n\t\terr = xfer(vrh, iov->iov[iov->i].iov_base, ptr, partlen);\n\t\tif (err)\n\t\t\treturn err;\n\t\tdone += partlen;\n\t\tlen -= partlen;\n\t\tptr += partlen;\n\t\tiov->consumed += partlen;\n\t\tiov->iov[iov->i].iov_len -= partlen;\n\t\tiov->iov[iov->i].iov_base += partlen;\n\n\t\tif (!iov->iov[iov->i].iov_len) {\n\t\t\t \n\t\t\tiov->iov[iov->i].iov_len = iov->consumed;\n\t\t\tiov->iov[iov->i].iov_base -= iov->consumed;\n\n\t\t\tiov->consumed = 0;\n\t\t\tiov->i++;\n\t\t}\n\t}\n\treturn done;\n}\n\n \nstatic inline bool range_check(struct vringh *vrh, u64 addr, size_t *len,\n\t\t\t       struct vringh_range *range,\n\t\t\t       bool (*getrange)(struct vringh *,\n\t\t\t\t\t\tu64, struct vringh_range *))\n{\n\tif (addr < range->start || addr > range->end_incl) {\n\t\tif (!getrange(vrh, addr, range))\n\t\t\treturn false;\n\t}\n\tBUG_ON(addr < range->start || addr > range->end_incl);\n\n\t \n\tif (unlikely(addr + *len == 0)) {\n\t\tif (range->end_incl == -1ULL)\n\t\t\treturn true;\n\t\tgoto truncate;\n\t}\n\n\t \n\tif (addr + *len < addr) {\n\t\tvringh_bad(\"Wrapping descriptor %zu@0x%llx\",\n\t\t\t   *len, (unsigned long long)addr);\n\t\treturn false;\n\t}\n\n\tif (unlikely(addr + *len - 1 > range->end_incl))\n\t\tgoto truncate;\n\treturn true;\n\ntruncate:\n\t*len = range->end_incl + 1 - addr;\n\treturn true;\n}\n\nstatic inline bool no_range_check(struct vringh *vrh, u64 addr, size_t *len,\n\t\t\t\t  struct vringh_range *range,\n\t\t\t\t  bool (*getrange)(struct vringh *,\n\t\t\t\t\t\t   u64, struct vringh_range *))\n{\n\treturn true;\n}\n\n \nstatic int move_to_indirect(const struct vringh *vrh,\n\t\t\t    int *up_next, u16 *i, void *addr,\n\t\t\t    const struct vring_desc *desc,\n\t\t\t    struct vring_desc **descs, int *desc_max)\n{\n\tu32 len;\n\n\t \n\tif (*up_next != -1) {\n\t\tvringh_bad(\"Multilevel indirect %u->%u\", *up_next, *i);\n\t\treturn -EINVAL;\n\t}\n\n\tlen = vringh32_to_cpu(vrh, desc->len);\n\tif (unlikely(len % sizeof(struct vring_desc))) {\n\t\tvringh_bad(\"Strange indirect len %u\", desc->len);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (desc->flags & cpu_to_vringh16(vrh, VRING_DESC_F_NEXT))\n\t\t*up_next = vringh16_to_cpu(vrh, desc->next);\n\telse\n\t\t*up_next = -2;\n\t*descs = addr;\n\t*desc_max = len / sizeof(struct vring_desc);\n\n\t \n\t*i = 0;\n\treturn 0;\n}\n\nstatic int resize_iovec(struct vringh_kiov *iov, gfp_t gfp)\n{\n\tstruct kvec *new;\n\tunsigned int flag, new_num = (iov->max_num & ~VRINGH_IOV_ALLOCATED) * 2;\n\n\tif (new_num < 8)\n\t\tnew_num = 8;\n\n\tflag = (iov->max_num & VRINGH_IOV_ALLOCATED);\n\tif (flag)\n\t\tnew = krealloc_array(iov->iov, new_num,\n\t\t\t\t     sizeof(struct iovec), gfp);\n\telse {\n\t\tnew = kmalloc_array(new_num, sizeof(struct iovec), gfp);\n\t\tif (new) {\n\t\t\tmemcpy(new, iov->iov,\n\t\t\t       iov->max_num * sizeof(struct iovec));\n\t\t\tflag = VRINGH_IOV_ALLOCATED;\n\t\t}\n\t}\n\tif (!new)\n\t\treturn -ENOMEM;\n\tiov->iov = new;\n\tiov->max_num = (new_num | flag);\n\treturn 0;\n}\n\nstatic u16 __cold return_from_indirect(const struct vringh *vrh, int *up_next,\n\t\t\t\t       struct vring_desc **descs, int *desc_max)\n{\n\tu16 i = *up_next;\n\n\t*up_next = -1;\n\t*descs = vrh->vring.desc;\n\t*desc_max = vrh->vring.num;\n\treturn i;\n}\n\nstatic int slow_copy(struct vringh *vrh, void *dst, const void *src,\n\t\t     bool (*rcheck)(struct vringh *vrh, u64 addr, size_t *len,\n\t\t\t\t    struct vringh_range *range,\n\t\t\t\t    bool (*getrange)(struct vringh *vrh,\n\t\t\t\t\t\t     u64,\n\t\t\t\t\t\t     struct vringh_range *)),\n\t\t     bool (*getrange)(struct vringh *vrh,\n\t\t\t\t      u64 addr,\n\t\t\t\t      struct vringh_range *r),\n\t\t     struct vringh_range *range,\n\t\t     int (*copy)(const struct vringh *vrh,\n\t\t\t\t void *dst, const void *src, size_t len))\n{\n\tsize_t part, len = sizeof(struct vring_desc);\n\n\tdo {\n\t\tu64 addr;\n\t\tint err;\n\n\t\tpart = len;\n\t\taddr = (u64)(unsigned long)src - range->offset;\n\n\t\tif (!rcheck(vrh, addr, &part, range, getrange))\n\t\t\treturn -EINVAL;\n\n\t\terr = copy(vrh, dst, src, part);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tdst += part;\n\t\tsrc += part;\n\t\tlen -= part;\n\t} while (len);\n\treturn 0;\n}\n\nstatic inline int\n__vringh_iov(struct vringh *vrh, u16 i,\n\t     struct vringh_kiov *riov,\n\t     struct vringh_kiov *wiov,\n\t     bool (*rcheck)(struct vringh *vrh, u64 addr, size_t *len,\n\t\t\t    struct vringh_range *range,\n\t\t\t    bool (*getrange)(struct vringh *, u64,\n\t\t\t\t\t     struct vringh_range *)),\n\t     bool (*getrange)(struct vringh *, u64, struct vringh_range *),\n\t     gfp_t gfp,\n\t     int (*copy)(const struct vringh *vrh,\n\t\t\t void *dst, const void *src, size_t len))\n{\n\tint err, count = 0, indirect_count = 0, up_next, desc_max;\n\tstruct vring_desc desc, *descs;\n\tstruct vringh_range range = { -1ULL, 0 }, slowrange;\n\tbool slow = false;\n\n\t \n\tdescs = vrh->vring.desc;\n\tdesc_max = vrh->vring.num;\n\tup_next = -1;\n\n\t \n\tif (WARN_ON(!riov && !wiov))\n\t\treturn -EINVAL;\n\n\tif (riov)\n\t\triov->i = riov->used = riov->consumed = 0;\n\tif (wiov)\n\t\twiov->i = wiov->used = wiov->consumed = 0;\n\n\tfor (;;) {\n\t\tvoid *addr;\n\t\tstruct vringh_kiov *iov;\n\t\tsize_t len;\n\n\t\tif (unlikely(slow))\n\t\t\terr = slow_copy(vrh, &desc, &descs[i], rcheck, getrange,\n\t\t\t\t\t&slowrange, copy);\n\t\telse\n\t\t\terr = copy(vrh, &desc, &descs[i], sizeof(desc));\n\t\tif (unlikely(err))\n\t\t\tgoto fail;\n\n\t\tif (unlikely(desc.flags &\n\t\t\t     cpu_to_vringh16(vrh, VRING_DESC_F_INDIRECT))) {\n\t\t\tu64 a = vringh64_to_cpu(vrh, desc.addr);\n\n\t\t\t \n\t\t\tlen = vringh32_to_cpu(vrh, desc.len);\n\t\t\tif (!rcheck(vrh, a, &len, &range, getrange)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto fail;\n\t\t\t}\n\n\t\t\tif (unlikely(len != vringh32_to_cpu(vrh, desc.len))) {\n\t\t\t\tslow = true;\n\t\t\t\t \n\t\t\t\tslowrange = range;\n\t\t\t}\n\n\t\t\taddr = (void *)(long)(a + range.offset);\n\t\t\terr = move_to_indirect(vrh, &up_next, &i, addr, &desc,\n\t\t\t\t\t       &descs, &desc_max);\n\t\t\tif (err)\n\t\t\t\tgoto fail;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (up_next == -1)\n\t\t\tcount++;\n\t\telse\n\t\t\tindirect_count++;\n\n\t\tif (count > vrh->vring.num || indirect_count > desc_max) {\n\t\t\tvringh_bad(\"Descriptor loop in %p\", descs);\n\t\t\terr = -ELOOP;\n\t\t\tgoto fail;\n\t\t}\n\n\t\tif (desc.flags & cpu_to_vringh16(vrh, VRING_DESC_F_WRITE))\n\t\t\tiov = wiov;\n\t\telse {\n\t\t\tiov = riov;\n\t\t\tif (unlikely(wiov && wiov->used)) {\n\t\t\t\tvringh_bad(\"Readable desc %p after writable\",\n\t\t\t\t\t   &descs[i]);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto fail;\n\t\t\t}\n\t\t}\n\n\t\tif (!iov) {\n\t\t\tvringh_bad(\"Unexpected %s desc\",\n\t\t\t\t   !wiov ? \"writable\" : \"readable\");\n\t\t\terr = -EPROTO;\n\t\t\tgoto fail;\n\t\t}\n\n\tagain:\n\t\t \n\t\tlen = vringh32_to_cpu(vrh, desc.len);\n\t\tif (!rcheck(vrh, vringh64_to_cpu(vrh, desc.addr), &len, &range,\n\t\t\t    getrange)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto fail;\n\t\t}\n\t\taddr = (void *)(unsigned long)(vringh64_to_cpu(vrh, desc.addr) +\n\t\t\t\t\t       range.offset);\n\n\t\tif (unlikely(iov->used == (iov->max_num & ~VRINGH_IOV_ALLOCATED))) {\n\t\t\terr = resize_iovec(iov, gfp);\n\t\t\tif (err)\n\t\t\t\tgoto fail;\n\t\t}\n\n\t\tiov->iov[iov->used].iov_base = addr;\n\t\tiov->iov[iov->used].iov_len = len;\n\t\tiov->used++;\n\n\t\tif (unlikely(len != vringh32_to_cpu(vrh, desc.len))) {\n\t\t\tdesc.len = cpu_to_vringh32(vrh,\n\t\t\t\t   vringh32_to_cpu(vrh, desc.len) - len);\n\t\t\tdesc.addr = cpu_to_vringh64(vrh,\n\t\t\t\t    vringh64_to_cpu(vrh, desc.addr) + len);\n\t\t\tgoto again;\n\t\t}\n\n\t\tif (desc.flags & cpu_to_vringh16(vrh, VRING_DESC_F_NEXT)) {\n\t\t\ti = vringh16_to_cpu(vrh, desc.next);\n\t\t} else {\n\t\t\t \n\t\t\tif (unlikely(up_next > 0)) {\n\t\t\t\ti = return_from_indirect(vrh, &up_next,\n\t\t\t\t\t\t\t &descs, &desc_max);\n\t\t\t\tslow = false;\n\t\t\t\tindirect_count = 0;\n\t\t\t} else\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (i >= desc_max) {\n\t\t\tvringh_bad(\"Chained index %u > %u\", i, desc_max);\n\t\t\terr = -EINVAL;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\n\treturn 0;\n\nfail:\n\treturn err;\n}\n\nstatic inline int __vringh_complete(struct vringh *vrh,\n\t\t\t\t    const struct vring_used_elem *used,\n\t\t\t\t    unsigned int num_used,\n\t\t\t\t    int (*putu16)(const struct vringh *vrh,\n\t\t\t\t\t\t  __virtio16 *p, u16 val),\n\t\t\t\t    int (*putused)(const struct vringh *vrh,\n\t\t\t\t\t\t   struct vring_used_elem *dst,\n\t\t\t\t\t\t   const struct vring_used_elem\n\t\t\t\t\t\t   *src, unsigned num))\n{\n\tstruct vring_used *used_ring;\n\tint err;\n\tu16 used_idx, off;\n\n\tused_ring = vrh->vring.used;\n\tused_idx = vrh->last_used_idx + vrh->completed;\n\n\toff = used_idx % vrh->vring.num;\n\n\t \n\tif (num_used > 1 && unlikely(off + num_used >= vrh->vring.num)) {\n\t\tu16 part = vrh->vring.num - off;\n\t\terr = putused(vrh, &used_ring->ring[off], used, part);\n\t\tif (!err)\n\t\t\terr = putused(vrh, &used_ring->ring[0], used + part,\n\t\t\t\t      num_used - part);\n\t} else\n\t\terr = putused(vrh, &used_ring->ring[off], used, num_used);\n\n\tif (err) {\n\t\tvringh_bad(\"Failed to write %u used entries %u at %p\",\n\t\t\t   num_used, off, &used_ring->ring[off]);\n\t\treturn err;\n\t}\n\n\t \n\tvirtio_wmb(vrh->weak_barriers);\n\n\terr = putu16(vrh, &vrh->vring.used->idx, used_idx + num_used);\n\tif (err) {\n\t\tvringh_bad(\"Failed to update used index at %p\",\n\t\t\t   &vrh->vring.used->idx);\n\t\treturn err;\n\t}\n\n\tvrh->completed += num_used;\n\treturn 0;\n}\n\n\nstatic inline int __vringh_need_notify(struct vringh *vrh,\n\t\t\t\t       int (*getu16)(const struct vringh *vrh,\n\t\t\t\t\t\t     u16 *val,\n\t\t\t\t\t\t     const __virtio16 *p))\n{\n\tbool notify;\n\tu16 used_event;\n\tint err;\n\n\t \n\tvirtio_mb(vrh->weak_barriers);\n\n\t \n\tif (!vrh->event_indices) {\n\t\tu16 flags;\n\t\terr = getu16(vrh, &flags, &vrh->vring.avail->flags);\n\t\tif (err) {\n\t\t\tvringh_bad(\"Failed to get flags at %p\",\n\t\t\t\t   &vrh->vring.avail->flags);\n\t\t\treturn err;\n\t\t}\n\t\treturn (!(flags & VRING_AVAIL_F_NO_INTERRUPT));\n\t}\n\n\t \n\terr = getu16(vrh, &used_event, &vring_used_event(&vrh->vring));\n\tif (err) {\n\t\tvringh_bad(\"Failed to get used event idx at %p\",\n\t\t\t   &vring_used_event(&vrh->vring));\n\t\treturn err;\n\t}\n\n\t \n\tif (unlikely(vrh->completed > 0xffff))\n\t\tnotify = true;\n\telse\n\t\tnotify = vring_need_event(used_event,\n\t\t\t\t\t  vrh->last_used_idx + vrh->completed,\n\t\t\t\t\t  vrh->last_used_idx);\n\n\tvrh->last_used_idx += vrh->completed;\n\tvrh->completed = 0;\n\treturn notify;\n}\n\nstatic inline bool __vringh_notify_enable(struct vringh *vrh,\n\t\t\t\t\t  int (*getu16)(const struct vringh *vrh,\n\t\t\t\t\t\t\tu16 *val, const __virtio16 *p),\n\t\t\t\t\t  int (*putu16)(const struct vringh *vrh,\n\t\t\t\t\t\t\t__virtio16 *p, u16 val))\n{\n\tu16 avail;\n\n\tif (!vrh->event_indices) {\n\t\t \n\t\tif (putu16(vrh, &vrh->vring.used->flags, 0) != 0) {\n\t\t\tvringh_bad(\"Clearing used flags %p\",\n\t\t\t\t   &vrh->vring.used->flags);\n\t\t\treturn true;\n\t\t}\n\t} else {\n\t\tif (putu16(vrh, &vring_avail_event(&vrh->vring),\n\t\t\t   vrh->last_avail_idx) != 0) {\n\t\t\tvringh_bad(\"Updating avail event index %p\",\n\t\t\t\t   &vring_avail_event(&vrh->vring));\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t \n\tvirtio_mb(vrh->weak_barriers);\n\n\tif (getu16(vrh, &avail, &vrh->vring.avail->idx) != 0) {\n\t\tvringh_bad(\"Failed to check avail idx at %p\",\n\t\t\t   &vrh->vring.avail->idx);\n\t\treturn true;\n\t}\n\n\t \n\treturn avail == vrh->last_avail_idx;\n}\n\nstatic inline void __vringh_notify_disable(struct vringh *vrh,\n\t\t\t\t\t   int (*putu16)(const struct vringh *vrh,\n\t\t\t\t\t\t\t __virtio16 *p, u16 val))\n{\n\tif (!vrh->event_indices) {\n\t\t \n\t\tif (putu16(vrh, &vrh->vring.used->flags,\n\t\t\t   VRING_USED_F_NO_NOTIFY)) {\n\t\t\tvringh_bad(\"Setting used flags %p\",\n\t\t\t\t   &vrh->vring.used->flags);\n\t\t}\n\t}\n}\n\n \nstatic inline int getu16_user(const struct vringh *vrh, u16 *val, const __virtio16 *p)\n{\n\t__virtio16 v = 0;\n\tint rc = get_user(v, (__force __virtio16 __user *)p);\n\t*val = vringh16_to_cpu(vrh, v);\n\treturn rc;\n}\n\nstatic inline int putu16_user(const struct vringh *vrh, __virtio16 *p, u16 val)\n{\n\t__virtio16 v = cpu_to_vringh16(vrh, val);\n\treturn put_user(v, (__force __virtio16 __user *)p);\n}\n\nstatic inline int copydesc_user(const struct vringh *vrh,\n\t\t\t\tvoid *dst, const void *src, size_t len)\n{\n\treturn copy_from_user(dst, (__force void __user *)src, len) ?\n\t\t-EFAULT : 0;\n}\n\nstatic inline int putused_user(const struct vringh *vrh,\n\t\t\t       struct vring_used_elem *dst,\n\t\t\t       const struct vring_used_elem *src,\n\t\t\t       unsigned int num)\n{\n\treturn copy_to_user((__force void __user *)dst, src,\n\t\t\t    sizeof(*dst) * num) ? -EFAULT : 0;\n}\n\nstatic inline int xfer_from_user(const struct vringh *vrh, void *src,\n\t\t\t\t void *dst, size_t len)\n{\n\treturn copy_from_user(dst, (__force void __user *)src, len) ?\n\t\t-EFAULT : 0;\n}\n\nstatic inline int xfer_to_user(const struct vringh *vrh,\n\t\t\t       void *dst, void *src, size_t len)\n{\n\treturn copy_to_user((__force void __user *)dst, src, len) ?\n\t\t-EFAULT : 0;\n}\n\n \nint vringh_init_user(struct vringh *vrh, u64 features,\n\t\t     unsigned int num, bool weak_barriers,\n\t\t     vring_desc_t __user *desc,\n\t\t     vring_avail_t __user *avail,\n\t\t     vring_used_t __user *used)\n{\n\t \n\tif (!num || num > 0xffff || (num & (num - 1))) {\n\t\tvringh_bad(\"Bad ring size %u\", num);\n\t\treturn -EINVAL;\n\t}\n\n\tvrh->little_endian = (features & (1ULL << VIRTIO_F_VERSION_1));\n\tvrh->event_indices = (features & (1 << VIRTIO_RING_F_EVENT_IDX));\n\tvrh->weak_barriers = weak_barriers;\n\tvrh->completed = 0;\n\tvrh->last_avail_idx = 0;\n\tvrh->last_used_idx = 0;\n\tvrh->vring.num = num;\n\t \n\tvrh->vring.desc = (__force struct vring_desc *)desc;\n\tvrh->vring.avail = (__force struct vring_avail *)avail;\n\tvrh->vring.used = (__force struct vring_used *)used;\n\treturn 0;\n}\nEXPORT_SYMBOL(vringh_init_user);\n\n \nint vringh_getdesc_user(struct vringh *vrh,\n\t\t\tstruct vringh_iov *riov,\n\t\t\tstruct vringh_iov *wiov,\n\t\t\tbool (*getrange)(struct vringh *vrh,\n\t\t\t\t\t u64 addr, struct vringh_range *r),\n\t\t\tu16 *head)\n{\n\tint err;\n\n\t*head = vrh->vring.num;\n\terr = __vringh_get_head(vrh, getu16_user, &vrh->last_avail_idx);\n\tif (err < 0)\n\t\treturn err;\n\n\t \n\tif (err == vrh->vring.num)\n\t\treturn 0;\n\n\t \n\tBUILD_BUG_ON(sizeof(struct vringh_kiov) != sizeof(struct vringh_iov));\n\tBUILD_BUG_ON(offsetof(struct vringh_kiov, iov) !=\n\t\t     offsetof(struct vringh_iov, iov));\n\tBUILD_BUG_ON(offsetof(struct vringh_kiov, i) !=\n\t\t     offsetof(struct vringh_iov, i));\n\tBUILD_BUG_ON(offsetof(struct vringh_kiov, used) !=\n\t\t     offsetof(struct vringh_iov, used));\n\tBUILD_BUG_ON(offsetof(struct vringh_kiov, max_num) !=\n\t\t     offsetof(struct vringh_iov, max_num));\n\tBUILD_BUG_ON(sizeof(struct iovec) != sizeof(struct kvec));\n\tBUILD_BUG_ON(offsetof(struct iovec, iov_base) !=\n\t\t     offsetof(struct kvec, iov_base));\n\tBUILD_BUG_ON(offsetof(struct iovec, iov_len) !=\n\t\t     offsetof(struct kvec, iov_len));\n\tBUILD_BUG_ON(sizeof(((struct iovec *)NULL)->iov_base)\n\t\t     != sizeof(((struct kvec *)NULL)->iov_base));\n\tBUILD_BUG_ON(sizeof(((struct iovec *)NULL)->iov_len)\n\t\t     != sizeof(((struct kvec *)NULL)->iov_len));\n\n\t*head = err;\n\terr = __vringh_iov(vrh, *head, (struct vringh_kiov *)riov,\n\t\t\t   (struct vringh_kiov *)wiov,\n\t\t\t   range_check, getrange, GFP_KERNEL, copydesc_user);\n\tif (err)\n\t\treturn err;\n\n\treturn 1;\n}\nEXPORT_SYMBOL(vringh_getdesc_user);\n\n \nssize_t vringh_iov_pull_user(struct vringh_iov *riov, void *dst, size_t len)\n{\n\treturn vringh_iov_xfer(NULL, (struct vringh_kiov *)riov,\n\t\t\t       dst, len, xfer_from_user);\n}\nEXPORT_SYMBOL(vringh_iov_pull_user);\n\n \nssize_t vringh_iov_push_user(struct vringh_iov *wiov,\n\t\t\t     const void *src, size_t len)\n{\n\treturn vringh_iov_xfer(NULL, (struct vringh_kiov *)wiov,\n\t\t\t       (void *)src, len, xfer_to_user);\n}\nEXPORT_SYMBOL(vringh_iov_push_user);\n\n \nvoid vringh_abandon_user(struct vringh *vrh, unsigned int num)\n{\n\t \n\tvrh->last_avail_idx -= num;\n}\nEXPORT_SYMBOL(vringh_abandon_user);\n\n \nint vringh_complete_user(struct vringh *vrh, u16 head, u32 len)\n{\n\tstruct vring_used_elem used;\n\n\tused.id = cpu_to_vringh32(vrh, head);\n\tused.len = cpu_to_vringh32(vrh, len);\n\treturn __vringh_complete(vrh, &used, 1, putu16_user, putused_user);\n}\nEXPORT_SYMBOL(vringh_complete_user);\n\n \nint vringh_complete_multi_user(struct vringh *vrh,\n\t\t\t       const struct vring_used_elem used[],\n\t\t\t       unsigned num_used)\n{\n\treturn __vringh_complete(vrh, used, num_used,\n\t\t\t\t putu16_user, putused_user);\n}\nEXPORT_SYMBOL(vringh_complete_multi_user);\n\n \nbool vringh_notify_enable_user(struct vringh *vrh)\n{\n\treturn __vringh_notify_enable(vrh, getu16_user, putu16_user);\n}\nEXPORT_SYMBOL(vringh_notify_enable_user);\n\n \nvoid vringh_notify_disable_user(struct vringh *vrh)\n{\n\t__vringh_notify_disable(vrh, putu16_user);\n}\nEXPORT_SYMBOL(vringh_notify_disable_user);\n\n \nint vringh_need_notify_user(struct vringh *vrh)\n{\n\treturn __vringh_need_notify(vrh, getu16_user);\n}\nEXPORT_SYMBOL(vringh_need_notify_user);\n\n \nstatic inline int getu16_kern(const struct vringh *vrh,\n\t\t\t      u16 *val, const __virtio16 *p)\n{\n\t*val = vringh16_to_cpu(vrh, READ_ONCE(*p));\n\treturn 0;\n}\n\nstatic inline int putu16_kern(const struct vringh *vrh, __virtio16 *p, u16 val)\n{\n\tWRITE_ONCE(*p, cpu_to_vringh16(vrh, val));\n\treturn 0;\n}\n\nstatic inline int copydesc_kern(const struct vringh *vrh,\n\t\t\t\tvoid *dst, const void *src, size_t len)\n{\n\tmemcpy(dst, src, len);\n\treturn 0;\n}\n\nstatic inline int putused_kern(const struct vringh *vrh,\n\t\t\t       struct vring_used_elem *dst,\n\t\t\t       const struct vring_used_elem *src,\n\t\t\t       unsigned int num)\n{\n\tmemcpy(dst, src, num * sizeof(*dst));\n\treturn 0;\n}\n\nstatic inline int xfer_kern(const struct vringh *vrh, void *src,\n\t\t\t    void *dst, size_t len)\n{\n\tmemcpy(dst, src, len);\n\treturn 0;\n}\n\nstatic inline int kern_xfer(const struct vringh *vrh, void *dst,\n\t\t\t    void *src, size_t len)\n{\n\tmemcpy(dst, src, len);\n\treturn 0;\n}\n\n \nint vringh_init_kern(struct vringh *vrh, u64 features,\n\t\t     unsigned int num, bool weak_barriers,\n\t\t     struct vring_desc *desc,\n\t\t     struct vring_avail *avail,\n\t\t     struct vring_used *used)\n{\n\t \n\tif (!num || num > 0xffff || (num & (num - 1))) {\n\t\tvringh_bad(\"Bad ring size %u\", num);\n\t\treturn -EINVAL;\n\t}\n\n\tvrh->little_endian = (features & (1ULL << VIRTIO_F_VERSION_1));\n\tvrh->event_indices = (features & (1 << VIRTIO_RING_F_EVENT_IDX));\n\tvrh->weak_barriers = weak_barriers;\n\tvrh->completed = 0;\n\tvrh->last_avail_idx = 0;\n\tvrh->last_used_idx = 0;\n\tvrh->vring.num = num;\n\tvrh->vring.desc = desc;\n\tvrh->vring.avail = avail;\n\tvrh->vring.used = used;\n\treturn 0;\n}\nEXPORT_SYMBOL(vringh_init_kern);\n\n \nint vringh_getdesc_kern(struct vringh *vrh,\n\t\t\tstruct vringh_kiov *riov,\n\t\t\tstruct vringh_kiov *wiov,\n\t\t\tu16 *head,\n\t\t\tgfp_t gfp)\n{\n\tint err;\n\n\terr = __vringh_get_head(vrh, getu16_kern, &vrh->last_avail_idx);\n\tif (err < 0)\n\t\treturn err;\n\n\t \n\tif (err == vrh->vring.num)\n\t\treturn 0;\n\n\t*head = err;\n\terr = __vringh_iov(vrh, *head, riov, wiov, no_range_check, NULL,\n\t\t\t   gfp, copydesc_kern);\n\tif (err)\n\t\treturn err;\n\n\treturn 1;\n}\nEXPORT_SYMBOL(vringh_getdesc_kern);\n\n \nssize_t vringh_iov_pull_kern(struct vringh_kiov *riov, void *dst, size_t len)\n{\n\treturn vringh_iov_xfer(NULL, riov, dst, len, xfer_kern);\n}\nEXPORT_SYMBOL(vringh_iov_pull_kern);\n\n \nssize_t vringh_iov_push_kern(struct vringh_kiov *wiov,\n\t\t\t     const void *src, size_t len)\n{\n\treturn vringh_iov_xfer(NULL, wiov, (void *)src, len, kern_xfer);\n}\nEXPORT_SYMBOL(vringh_iov_push_kern);\n\n \nvoid vringh_abandon_kern(struct vringh *vrh, unsigned int num)\n{\n\t \n\tvrh->last_avail_idx -= num;\n}\nEXPORT_SYMBOL(vringh_abandon_kern);\n\n \nint vringh_complete_kern(struct vringh *vrh, u16 head, u32 len)\n{\n\tstruct vring_used_elem used;\n\n\tused.id = cpu_to_vringh32(vrh, head);\n\tused.len = cpu_to_vringh32(vrh, len);\n\n\treturn __vringh_complete(vrh, &used, 1, putu16_kern, putused_kern);\n}\nEXPORT_SYMBOL(vringh_complete_kern);\n\n \nbool vringh_notify_enable_kern(struct vringh *vrh)\n{\n\treturn __vringh_notify_enable(vrh, getu16_kern, putu16_kern);\n}\nEXPORT_SYMBOL(vringh_notify_enable_kern);\n\n \nvoid vringh_notify_disable_kern(struct vringh *vrh)\n{\n\t__vringh_notify_disable(vrh, putu16_kern);\n}\nEXPORT_SYMBOL(vringh_notify_disable_kern);\n\n \nint vringh_need_notify_kern(struct vringh *vrh)\n{\n\treturn __vringh_need_notify(vrh, getu16_kern);\n}\nEXPORT_SYMBOL(vringh_need_notify_kern);\n\n#if IS_REACHABLE(CONFIG_VHOST_IOTLB)\n\nstruct iotlb_vec {\n\tunion {\n\t\tstruct iovec *iovec;\n\t\tstruct bio_vec *bvec;\n\t} iov;\n\tsize_t count;\n};\n\nstatic int iotlb_translate(const struct vringh *vrh,\n\t\t\t   u64 addr, u64 len, u64 *translated,\n\t\t\t   struct iotlb_vec *ivec, u32 perm)\n{\n\tstruct vhost_iotlb_map *map;\n\tstruct vhost_iotlb *iotlb = vrh->iotlb;\n\tint ret = 0;\n\tu64 s = 0, last = addr + len - 1;\n\n\tspin_lock(vrh->iotlb_lock);\n\n\twhile (len > s) {\n\t\tuintptr_t io_addr;\n\t\tsize_t io_len;\n\t\tu64 size;\n\n\t\tif (unlikely(ret >= ivec->count)) {\n\t\t\tret = -ENOBUFS;\n\t\t\tbreak;\n\t\t}\n\n\t\tmap = vhost_iotlb_itree_first(iotlb, addr, last);\n\t\tif (!map || map->start > addr) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t} else if (!(map->perm & perm)) {\n\t\t\tret = -EPERM;\n\t\t\tbreak;\n\t\t}\n\n\t\tsize = map->size - addr + map->start;\n\t\tio_len = min(len - s, size);\n\t\tio_addr = map->addr - map->start + addr;\n\n\t\tif (vrh->use_va) {\n\t\t\tstruct iovec *iovec = ivec->iov.iovec;\n\n\t\t\tiovec[ret].iov_len = io_len;\n\t\t\tiovec[ret].iov_base = (void __user *)io_addr;\n\t\t} else {\n\t\t\tu64 pfn = io_addr >> PAGE_SHIFT;\n\t\t\tstruct bio_vec *bvec = ivec->iov.bvec;\n\n\t\t\tbvec_set_page(&bvec[ret], pfn_to_page(pfn), io_len,\n\t\t\t\t      io_addr & (PAGE_SIZE - 1));\n\t\t}\n\n\t\ts += size;\n\t\taddr += size;\n\t\t++ret;\n\t}\n\n\tspin_unlock(vrh->iotlb_lock);\n\n\tif (translated)\n\t\t*translated = min(len, s);\n\n\treturn ret;\n}\n\n#define IOTLB_IOV_STRIDE 16\n\nstatic inline int copy_from_iotlb(const struct vringh *vrh, void *dst,\n\t\t\t\t  void *src, size_t len)\n{\n\tstruct iotlb_vec ivec;\n\tunion {\n\t\tstruct iovec iovec[IOTLB_IOV_STRIDE];\n\t\tstruct bio_vec bvec[IOTLB_IOV_STRIDE];\n\t} iov;\n\tu64 total_translated = 0;\n\n\tivec.iov.iovec = iov.iovec;\n\tivec.count = IOTLB_IOV_STRIDE;\n\n\twhile (total_translated < len) {\n\t\tstruct iov_iter iter;\n\t\tu64 translated;\n\t\tint ret;\n\n\t\tret = iotlb_translate(vrh, (u64)(uintptr_t)src,\n\t\t\t\t      len - total_translated, &translated,\n\t\t\t\t      &ivec, VHOST_MAP_RO);\n\t\tif (ret == -ENOBUFS)\n\t\t\tret = IOTLB_IOV_STRIDE;\n\t\telse if (ret < 0)\n\t\t\treturn ret;\n\n\t\tif (vrh->use_va) {\n\t\t\tiov_iter_init(&iter, ITER_SOURCE, ivec.iov.iovec, ret,\n\t\t\t\t      translated);\n\t\t} else {\n\t\t\tiov_iter_bvec(&iter, ITER_SOURCE, ivec.iov.bvec, ret,\n\t\t\t\t      translated);\n\t\t}\n\n\t\tret = copy_from_iter(dst, translated, &iter);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tsrc += translated;\n\t\tdst += translated;\n\t\ttotal_translated += translated;\n\t}\n\n\treturn total_translated;\n}\n\nstatic inline int copy_to_iotlb(const struct vringh *vrh, void *dst,\n\t\t\t\tvoid *src, size_t len)\n{\n\tstruct iotlb_vec ivec;\n\tunion {\n\t\tstruct iovec iovec[IOTLB_IOV_STRIDE];\n\t\tstruct bio_vec bvec[IOTLB_IOV_STRIDE];\n\t} iov;\n\tu64 total_translated = 0;\n\n\tivec.iov.iovec = iov.iovec;\n\tivec.count = IOTLB_IOV_STRIDE;\n\n\twhile (total_translated < len) {\n\t\tstruct iov_iter iter;\n\t\tu64 translated;\n\t\tint ret;\n\n\t\tret = iotlb_translate(vrh, (u64)(uintptr_t)dst,\n\t\t\t\t      len - total_translated, &translated,\n\t\t\t\t      &ivec, VHOST_MAP_WO);\n\t\tif (ret == -ENOBUFS)\n\t\t\tret = IOTLB_IOV_STRIDE;\n\t\telse if (ret < 0)\n\t\t\treturn ret;\n\n\t\tif (vrh->use_va) {\n\t\t\tiov_iter_init(&iter, ITER_DEST, ivec.iov.iovec, ret,\n\t\t\t\t      translated);\n\t\t} else {\n\t\t\tiov_iter_bvec(&iter, ITER_DEST, ivec.iov.bvec, ret,\n\t\t\t\t      translated);\n\t\t}\n\n\t\tret = copy_to_iter(src, translated, &iter);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tsrc += translated;\n\t\tdst += translated;\n\t\ttotal_translated += translated;\n\t}\n\n\treturn total_translated;\n}\n\nstatic inline int getu16_iotlb(const struct vringh *vrh,\n\t\t\t       u16 *val, const __virtio16 *p)\n{\n\tstruct iotlb_vec ivec;\n\tunion {\n\t\tstruct iovec iovec[1];\n\t\tstruct bio_vec bvec[1];\n\t} iov;\n\t__virtio16 tmp;\n\tint ret;\n\n\tivec.iov.iovec = iov.iovec;\n\tivec.count = 1;\n\n\t \n\tret = iotlb_translate(vrh, (u64)(uintptr_t)p, sizeof(*p),\n\t\t\t      NULL, &ivec, VHOST_MAP_RO);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (vrh->use_va) {\n\t\tret = __get_user(tmp, (__virtio16 __user *)ivec.iov.iovec[0].iov_base);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else {\n\t\tvoid *kaddr = kmap_local_page(ivec.iov.bvec[0].bv_page);\n\t\tvoid *from = kaddr + ivec.iov.bvec[0].bv_offset;\n\n\t\ttmp = READ_ONCE(*(__virtio16 *)from);\n\t\tkunmap_local(kaddr);\n\t}\n\n\t*val = vringh16_to_cpu(vrh, tmp);\n\n\treturn 0;\n}\n\nstatic inline int putu16_iotlb(const struct vringh *vrh,\n\t\t\t       __virtio16 *p, u16 val)\n{\n\tstruct iotlb_vec ivec;\n\tunion {\n\t\tstruct iovec iovec;\n\t\tstruct bio_vec bvec;\n\t} iov;\n\t__virtio16 tmp;\n\tint ret;\n\n\tivec.iov.iovec = &iov.iovec;\n\tivec.count = 1;\n\n\t \n\tret = iotlb_translate(vrh, (u64)(uintptr_t)p, sizeof(*p),\n\t\t\t      NULL, &ivec, VHOST_MAP_RO);\n\tif (ret < 0)\n\t\treturn ret;\n\n\ttmp = cpu_to_vringh16(vrh, val);\n\n\tif (vrh->use_va) {\n\t\tret = __put_user(tmp, (__virtio16 __user *)ivec.iov.iovec[0].iov_base);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else {\n\t\tvoid *kaddr = kmap_local_page(ivec.iov.bvec[0].bv_page);\n\t\tvoid *to = kaddr + ivec.iov.bvec[0].bv_offset;\n\n\t\tWRITE_ONCE(*(__virtio16 *)to, tmp);\n\t\tkunmap_local(kaddr);\n\t}\n\n\treturn 0;\n}\n\nstatic inline int copydesc_iotlb(const struct vringh *vrh,\n\t\t\t\t void *dst, const void *src, size_t len)\n{\n\tint ret;\n\n\tret = copy_from_iotlb(vrh, dst, (void *)src, len);\n\tif (ret != len)\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic inline int xfer_from_iotlb(const struct vringh *vrh, void *src,\n\t\t\t\t  void *dst, size_t len)\n{\n\tint ret;\n\n\tret = copy_from_iotlb(vrh, dst, src, len);\n\tif (ret != len)\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic inline int xfer_to_iotlb(const struct vringh *vrh,\n\t\t\t       void *dst, void *src, size_t len)\n{\n\tint ret;\n\n\tret = copy_to_iotlb(vrh, dst, src, len);\n\tif (ret != len)\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic inline int putused_iotlb(const struct vringh *vrh,\n\t\t\t\tstruct vring_used_elem *dst,\n\t\t\t\tconst struct vring_used_elem *src,\n\t\t\t\tunsigned int num)\n{\n\tint size = num * sizeof(*dst);\n\tint ret;\n\n\tret = copy_to_iotlb(vrh, dst, (void *)src, num * sizeof(*dst));\n\tif (ret != size)\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\n \nint vringh_init_iotlb(struct vringh *vrh, u64 features,\n\t\t      unsigned int num, bool weak_barriers,\n\t\t      struct vring_desc *desc,\n\t\t      struct vring_avail *avail,\n\t\t      struct vring_used *used)\n{\n\tvrh->use_va = false;\n\n\treturn vringh_init_kern(vrh, features, num, weak_barriers,\n\t\t\t\tdesc, avail, used);\n}\nEXPORT_SYMBOL(vringh_init_iotlb);\n\n \nint vringh_init_iotlb_va(struct vringh *vrh, u64 features,\n\t\t\t unsigned int num, bool weak_barriers,\n\t\t\t struct vring_desc *desc,\n\t\t\t struct vring_avail *avail,\n\t\t\t struct vring_used *used)\n{\n\tvrh->use_va = true;\n\n\treturn vringh_init_kern(vrh, features, num, weak_barriers,\n\t\t\t\tdesc, avail, used);\n}\nEXPORT_SYMBOL(vringh_init_iotlb_va);\n\n \nvoid vringh_set_iotlb(struct vringh *vrh, struct vhost_iotlb *iotlb,\n\t\t      spinlock_t *iotlb_lock)\n{\n\tvrh->iotlb = iotlb;\n\tvrh->iotlb_lock = iotlb_lock;\n}\nEXPORT_SYMBOL(vringh_set_iotlb);\n\n \nint vringh_getdesc_iotlb(struct vringh *vrh,\n\t\t\t struct vringh_kiov *riov,\n\t\t\t struct vringh_kiov *wiov,\n\t\t\t u16 *head,\n\t\t\t gfp_t gfp)\n{\n\tint err;\n\n\terr = __vringh_get_head(vrh, getu16_iotlb, &vrh->last_avail_idx);\n\tif (err < 0)\n\t\treturn err;\n\n\t \n\tif (err == vrh->vring.num)\n\t\treturn 0;\n\n\t*head = err;\n\terr = __vringh_iov(vrh, *head, riov, wiov, no_range_check, NULL,\n\t\t\t   gfp, copydesc_iotlb);\n\tif (err)\n\t\treturn err;\n\n\treturn 1;\n}\nEXPORT_SYMBOL(vringh_getdesc_iotlb);\n\n \nssize_t vringh_iov_pull_iotlb(struct vringh *vrh,\n\t\t\t      struct vringh_kiov *riov,\n\t\t\t      void *dst, size_t len)\n{\n\treturn vringh_iov_xfer(vrh, riov, dst, len, xfer_from_iotlb);\n}\nEXPORT_SYMBOL(vringh_iov_pull_iotlb);\n\n \nssize_t vringh_iov_push_iotlb(struct vringh *vrh,\n\t\t\t      struct vringh_kiov *wiov,\n\t\t\t      const void *src, size_t len)\n{\n\treturn vringh_iov_xfer(vrh, wiov, (void *)src, len, xfer_to_iotlb);\n}\nEXPORT_SYMBOL(vringh_iov_push_iotlb);\n\n \nvoid vringh_abandon_iotlb(struct vringh *vrh, unsigned int num)\n{\n\t \n\tvrh->last_avail_idx -= num;\n}\nEXPORT_SYMBOL(vringh_abandon_iotlb);\n\n \nint vringh_complete_iotlb(struct vringh *vrh, u16 head, u32 len)\n{\n\tstruct vring_used_elem used;\n\n\tused.id = cpu_to_vringh32(vrh, head);\n\tused.len = cpu_to_vringh32(vrh, len);\n\n\treturn __vringh_complete(vrh, &used, 1, putu16_iotlb, putused_iotlb);\n}\nEXPORT_SYMBOL(vringh_complete_iotlb);\n\n \nbool vringh_notify_enable_iotlb(struct vringh *vrh)\n{\n\treturn __vringh_notify_enable(vrh, getu16_iotlb, putu16_iotlb);\n}\nEXPORT_SYMBOL(vringh_notify_enable_iotlb);\n\n \nvoid vringh_notify_disable_iotlb(struct vringh *vrh)\n{\n\t__vringh_notify_disable(vrh, putu16_iotlb);\n}\nEXPORT_SYMBOL(vringh_notify_disable_iotlb);\n\n \nint vringh_need_notify_iotlb(struct vringh *vrh)\n{\n\treturn __vringh_need_notify(vrh, getu16_iotlb);\n}\nEXPORT_SYMBOL(vringh_need_notify_iotlb);\n\n#endif\n\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}