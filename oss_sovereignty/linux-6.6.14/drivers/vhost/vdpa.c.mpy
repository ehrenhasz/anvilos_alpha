{
  "module_name": "vdpa.c",
  "hash_id": "c44b5bbbea3e649a3cdae9eb7be74c2e42f2239a8db195b81e23ab4bb673e080",
  "original_prompt": "Ingested from linux-6.6.14/drivers/vhost/vdpa.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/module.h>\n#include <linux/cdev.h>\n#include <linux/device.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/iommu.h>\n#include <linux/uuid.h>\n#include <linux/vdpa.h>\n#include <linux/nospec.h>\n#include <linux/vhost.h>\n\n#include \"vhost.h\"\n\nenum {\n\tVHOST_VDPA_BACKEND_FEATURES =\n\t(1ULL << VHOST_BACKEND_F_IOTLB_MSG_V2) |\n\t(1ULL << VHOST_BACKEND_F_IOTLB_BATCH) |\n\t(1ULL << VHOST_BACKEND_F_IOTLB_ASID),\n};\n\n#define VHOST_VDPA_DEV_MAX (1U << MINORBITS)\n\n#define VHOST_VDPA_IOTLB_BUCKETS 16\n\nstruct vhost_vdpa_as {\n\tstruct hlist_node hash_link;\n\tstruct vhost_iotlb iotlb;\n\tu32 id;\n};\n\nstruct vhost_vdpa {\n\tstruct vhost_dev vdev;\n\tstruct iommu_domain *domain;\n\tstruct vhost_virtqueue *vqs;\n\tstruct completion completion;\n\tstruct vdpa_device *vdpa;\n\tstruct hlist_head as[VHOST_VDPA_IOTLB_BUCKETS];\n\tstruct device dev;\n\tstruct cdev cdev;\n\tatomic_t opened;\n\tu32 nvqs;\n\tint virtio_id;\n\tint minor;\n\tstruct eventfd_ctx *config_ctx;\n\tint in_batch;\n\tstruct vdpa_iova_range range;\n\tu32 batch_asid;\n};\n\nstatic DEFINE_IDA(vhost_vdpa_ida);\n\nstatic dev_t vhost_vdpa_major;\n\nstatic void vhost_vdpa_iotlb_unmap(struct vhost_vdpa *v,\n\t\t\t\t   struct vhost_iotlb *iotlb, u64 start,\n\t\t\t\t   u64 last, u32 asid);\n\nstatic inline u32 iotlb_to_asid(struct vhost_iotlb *iotlb)\n{\n\tstruct vhost_vdpa_as *as = container_of(iotlb, struct\n\t\t\t\t\t\tvhost_vdpa_as, iotlb);\n\treturn as->id;\n}\n\nstatic struct vhost_vdpa_as *asid_to_as(struct vhost_vdpa *v, u32 asid)\n{\n\tstruct hlist_head *head = &v->as[asid % VHOST_VDPA_IOTLB_BUCKETS];\n\tstruct vhost_vdpa_as *as;\n\n\thlist_for_each_entry(as, head, hash_link)\n\t\tif (as->id == asid)\n\t\t\treturn as;\n\n\treturn NULL;\n}\n\nstatic struct vhost_iotlb *asid_to_iotlb(struct vhost_vdpa *v, u32 asid)\n{\n\tstruct vhost_vdpa_as *as = asid_to_as(v, asid);\n\n\tif (!as)\n\t\treturn NULL;\n\n\treturn &as->iotlb;\n}\n\nstatic struct vhost_vdpa_as *vhost_vdpa_alloc_as(struct vhost_vdpa *v, u32 asid)\n{\n\tstruct hlist_head *head = &v->as[asid % VHOST_VDPA_IOTLB_BUCKETS];\n\tstruct vhost_vdpa_as *as;\n\n\tif (asid_to_as(v, asid))\n\t\treturn NULL;\n\n\tif (asid >= v->vdpa->nas)\n\t\treturn NULL;\n\n\tas = kmalloc(sizeof(*as), GFP_KERNEL);\n\tif (!as)\n\t\treturn NULL;\n\n\tvhost_iotlb_init(&as->iotlb, 0, 0);\n\tas->id = asid;\n\thlist_add_head(&as->hash_link, head);\n\n\treturn as;\n}\n\nstatic struct vhost_vdpa_as *vhost_vdpa_find_alloc_as(struct vhost_vdpa *v,\n\t\t\t\t\t\t      u32 asid)\n{\n\tstruct vhost_vdpa_as *as = asid_to_as(v, asid);\n\n\tif (as)\n\t\treturn as;\n\n\treturn vhost_vdpa_alloc_as(v, asid);\n}\n\nstatic int vhost_vdpa_remove_as(struct vhost_vdpa *v, u32 asid)\n{\n\tstruct vhost_vdpa_as *as = asid_to_as(v, asid);\n\n\tif (!as)\n\t\treturn -EINVAL;\n\n\thlist_del(&as->hash_link);\n\tvhost_vdpa_iotlb_unmap(v, &as->iotlb, 0ULL, 0ULL - 1, asid);\n\tkfree(as);\n\n\treturn 0;\n}\n\nstatic void handle_vq_kick(struct vhost_work *work)\n{\n\tstruct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,\n\t\t\t\t\t\t  poll.work);\n\tstruct vhost_vdpa *v = container_of(vq->dev, struct vhost_vdpa, vdev);\n\tconst struct vdpa_config_ops *ops = v->vdpa->config;\n\n\tops->kick_vq(v->vdpa, vq - v->vqs);\n}\n\nstatic irqreturn_t vhost_vdpa_virtqueue_cb(void *private)\n{\n\tstruct vhost_virtqueue *vq = private;\n\tstruct eventfd_ctx *call_ctx = vq->call_ctx.ctx;\n\n\tif (call_ctx)\n\t\teventfd_signal(call_ctx, 1);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic irqreturn_t vhost_vdpa_config_cb(void *private)\n{\n\tstruct vhost_vdpa *v = private;\n\tstruct eventfd_ctx *config_ctx = v->config_ctx;\n\n\tif (config_ctx)\n\t\teventfd_signal(config_ctx, 1);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic void vhost_vdpa_setup_vq_irq(struct vhost_vdpa *v, u16 qid)\n{\n\tstruct vhost_virtqueue *vq = &v->vqs[qid];\n\tconst struct vdpa_config_ops *ops = v->vdpa->config;\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tint ret, irq;\n\n\tif (!ops->get_vq_irq)\n\t\treturn;\n\n\tirq = ops->get_vq_irq(vdpa, qid);\n\tif (irq < 0)\n\t\treturn;\n\n\tirq_bypass_unregister_producer(&vq->call_ctx.producer);\n\tif (!vq->call_ctx.ctx)\n\t\treturn;\n\n\tvq->call_ctx.producer.token = vq->call_ctx.ctx;\n\tvq->call_ctx.producer.irq = irq;\n\tret = irq_bypass_register_producer(&vq->call_ctx.producer);\n\tif (unlikely(ret))\n\t\tdev_info(&v->dev, \"vq %u, irq bypass producer (token %p) registration fails, ret =  %d\\n\",\n\t\t\t qid, vq->call_ctx.producer.token, ret);\n}\n\nstatic void vhost_vdpa_unsetup_vq_irq(struct vhost_vdpa *v, u16 qid)\n{\n\tstruct vhost_virtqueue *vq = &v->vqs[qid];\n\n\tirq_bypass_unregister_producer(&vq->call_ctx.producer);\n}\n\nstatic int vhost_vdpa_reset(struct vhost_vdpa *v)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\n\tv->in_batch = 0;\n\n\treturn vdpa_reset(vdpa);\n}\n\nstatic long vhost_vdpa_bind_mm(struct vhost_vdpa *v)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\n\tif (!vdpa->use_va || !ops->bind_mm)\n\t\treturn 0;\n\n\treturn ops->bind_mm(vdpa, v->vdev.mm);\n}\n\nstatic void vhost_vdpa_unbind_mm(struct vhost_vdpa *v)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\n\tif (!vdpa->use_va || !ops->unbind_mm)\n\t\treturn;\n\n\tops->unbind_mm(vdpa);\n}\n\nstatic long vhost_vdpa_get_device_id(struct vhost_vdpa *v, u8 __user *argp)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tu32 device_id;\n\n\tdevice_id = ops->get_device_id(vdpa);\n\n\tif (copy_to_user(argp, &device_id, sizeof(device_id)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic long vhost_vdpa_get_status(struct vhost_vdpa *v, u8 __user *statusp)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tu8 status;\n\n\tstatus = ops->get_status(vdpa);\n\n\tif (copy_to_user(statusp, &status, sizeof(status)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic long vhost_vdpa_set_status(struct vhost_vdpa *v, u8 __user *statusp)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tu8 status, status_old;\n\tu32 nvqs = v->nvqs;\n\tint ret;\n\tu16 i;\n\n\tif (copy_from_user(&status, statusp, sizeof(status)))\n\t\treturn -EFAULT;\n\n\tstatus_old = ops->get_status(vdpa);\n\n\t \n\tif (status != 0 && (status_old & ~status) != 0)\n\t\treturn -EINVAL;\n\n\tif ((status_old & VIRTIO_CONFIG_S_DRIVER_OK) && !(status & VIRTIO_CONFIG_S_DRIVER_OK))\n\t\tfor (i = 0; i < nvqs; i++)\n\t\t\tvhost_vdpa_unsetup_vq_irq(v, i);\n\n\tif (status == 0) {\n\t\tret = vdpa_reset(vdpa);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else\n\t\tvdpa_set_status(vdpa, status);\n\n\tif ((status & VIRTIO_CONFIG_S_DRIVER_OK) && !(status_old & VIRTIO_CONFIG_S_DRIVER_OK))\n\t\tfor (i = 0; i < nvqs; i++)\n\t\t\tvhost_vdpa_setup_vq_irq(v, i);\n\n\treturn 0;\n}\n\nstatic int vhost_vdpa_config_validate(struct vhost_vdpa *v,\n\t\t\t\t      struct vhost_vdpa_config *c)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tsize_t size = vdpa->config->get_config_size(vdpa);\n\n\tif (c->len == 0 || c->off > size)\n\t\treturn -EINVAL;\n\n\tif (c->len > size - c->off)\n\t\treturn -E2BIG;\n\n\treturn 0;\n}\n\nstatic long vhost_vdpa_get_config(struct vhost_vdpa *v,\n\t\t\t\t  struct vhost_vdpa_config __user *c)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tstruct vhost_vdpa_config config;\n\tunsigned long size = offsetof(struct vhost_vdpa_config, buf);\n\tu8 *buf;\n\n\tif (copy_from_user(&config, c, size))\n\t\treturn -EFAULT;\n\tif (vhost_vdpa_config_validate(v, &config))\n\t\treturn -EINVAL;\n\tbuf = kvzalloc(config.len, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\tvdpa_get_config(vdpa, config.off, buf, config.len);\n\n\tif (copy_to_user(c->buf, buf, config.len)) {\n\t\tkvfree(buf);\n\t\treturn -EFAULT;\n\t}\n\n\tkvfree(buf);\n\treturn 0;\n}\n\nstatic long vhost_vdpa_set_config(struct vhost_vdpa *v,\n\t\t\t\t  struct vhost_vdpa_config __user *c)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tstruct vhost_vdpa_config config;\n\tunsigned long size = offsetof(struct vhost_vdpa_config, buf);\n\tu8 *buf;\n\n\tif (copy_from_user(&config, c, size))\n\t\treturn -EFAULT;\n\tif (vhost_vdpa_config_validate(v, &config))\n\t\treturn -EINVAL;\n\n\tbuf = vmemdup_user(c->buf, config.len);\n\tif (IS_ERR(buf))\n\t\treturn PTR_ERR(buf);\n\n\tvdpa_set_config(vdpa, config.off, buf, config.len);\n\n\tkvfree(buf);\n\treturn 0;\n}\n\nstatic bool vhost_vdpa_can_suspend(const struct vhost_vdpa *v)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\n\treturn ops->suspend;\n}\n\nstatic bool vhost_vdpa_can_resume(const struct vhost_vdpa *v)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\n\treturn ops->resume;\n}\n\nstatic long vhost_vdpa_get_features(struct vhost_vdpa *v, u64 __user *featurep)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tu64 features;\n\n\tfeatures = ops->get_device_features(vdpa);\n\n\tif (copy_to_user(featurep, &features, sizeof(features)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic u64 vhost_vdpa_get_backend_features(const struct vhost_vdpa *v)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\n\tif (!ops->get_backend_features)\n\t\treturn 0;\n\telse\n\t\treturn ops->get_backend_features(vdpa);\n}\n\nstatic long vhost_vdpa_set_features(struct vhost_vdpa *v, u64 __user *featurep)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tstruct vhost_dev *d = &v->vdev;\n\tu64 actual_features;\n\tu64 features;\n\tint i;\n\n\t \n\tif (ops->get_status(vdpa) & VIRTIO_CONFIG_S_FEATURES_OK)\n\t\treturn -EBUSY;\n\n\tif (copy_from_user(&features, featurep, sizeof(features)))\n\t\treturn -EFAULT;\n\n\tif (vdpa_set_features(vdpa, features))\n\t\treturn -EINVAL;\n\n\t \n\tactual_features = ops->get_driver_features(vdpa);\n\tfor (i = 0; i < d->nvqs; ++i) {\n\t\tstruct vhost_virtqueue *vq = d->vqs[i];\n\n\t\tmutex_lock(&vq->mutex);\n\t\tvq->acked_features = actual_features;\n\t\tmutex_unlock(&vq->mutex);\n\t}\n\n\treturn 0;\n}\n\nstatic long vhost_vdpa_get_vring_num(struct vhost_vdpa *v, u16 __user *argp)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tu16 num;\n\n\tnum = ops->get_vq_num_max(vdpa);\n\n\tif (copy_to_user(argp, &num, sizeof(num)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic void vhost_vdpa_config_put(struct vhost_vdpa *v)\n{\n\tif (v->config_ctx) {\n\t\teventfd_ctx_put(v->config_ctx);\n\t\tv->config_ctx = NULL;\n\t}\n}\n\nstatic long vhost_vdpa_set_config_call(struct vhost_vdpa *v, u32 __user *argp)\n{\n\tstruct vdpa_callback cb;\n\tint fd;\n\tstruct eventfd_ctx *ctx;\n\n\tcb.callback = vhost_vdpa_config_cb;\n\tcb.private = v;\n\tif (copy_from_user(&fd, argp, sizeof(fd)))\n\t\treturn  -EFAULT;\n\n\tctx = fd == VHOST_FILE_UNBIND ? NULL : eventfd_ctx_fdget(fd);\n\tswap(ctx, v->config_ctx);\n\n\tif (!IS_ERR_OR_NULL(ctx))\n\t\teventfd_ctx_put(ctx);\n\n\tif (IS_ERR(v->config_ctx)) {\n\t\tlong ret = PTR_ERR(v->config_ctx);\n\n\t\tv->config_ctx = NULL;\n\t\treturn ret;\n\t}\n\n\tv->vdpa->config->set_config_cb(v->vdpa, &cb);\n\n\treturn 0;\n}\n\nstatic long vhost_vdpa_get_iova_range(struct vhost_vdpa *v, u32 __user *argp)\n{\n\tstruct vhost_vdpa_iova_range range = {\n\t\t.first = v->range.first,\n\t\t.last = v->range.last,\n\t};\n\n\tif (copy_to_user(argp, &range, sizeof(range)))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nstatic long vhost_vdpa_get_config_size(struct vhost_vdpa *v, u32 __user *argp)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tu32 size;\n\n\tsize = ops->get_config_size(vdpa);\n\n\tif (copy_to_user(argp, &size, sizeof(size)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic long vhost_vdpa_get_vqs_count(struct vhost_vdpa *v, u32 __user *argp)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\n\tif (copy_to_user(argp, &vdpa->nvqs, sizeof(vdpa->nvqs)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\n \nstatic long vhost_vdpa_suspend(struct vhost_vdpa *v)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\n\tif (!ops->suspend)\n\t\treturn -EOPNOTSUPP;\n\n\treturn ops->suspend(vdpa);\n}\n\n \nstatic long vhost_vdpa_resume(struct vhost_vdpa *v)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\n\tif (!ops->resume)\n\t\treturn -EOPNOTSUPP;\n\n\treturn ops->resume(vdpa);\n}\n\nstatic long vhost_vdpa_vring_ioctl(struct vhost_vdpa *v, unsigned int cmd,\n\t\t\t\t   void __user *argp)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tstruct vdpa_vq_state vq_state;\n\tstruct vdpa_callback cb;\n\tstruct vhost_virtqueue *vq;\n\tstruct vhost_vring_state s;\n\tu32 idx;\n\tlong r;\n\n\tr = get_user(idx, (u32 __user *)argp);\n\tif (r < 0)\n\t\treturn r;\n\n\tif (idx >= v->nvqs)\n\t\treturn -ENOBUFS;\n\n\tidx = array_index_nospec(idx, v->nvqs);\n\tvq = &v->vqs[idx];\n\n\tswitch (cmd) {\n\tcase VHOST_VDPA_SET_VRING_ENABLE:\n\t\tif (copy_from_user(&s, argp, sizeof(s)))\n\t\t\treturn -EFAULT;\n\t\tops->set_vq_ready(vdpa, idx, s.num);\n\t\treturn 0;\n\tcase VHOST_VDPA_GET_VRING_GROUP:\n\t\tif (!ops->get_vq_group)\n\t\t\treturn -EOPNOTSUPP;\n\t\ts.index = idx;\n\t\ts.num = ops->get_vq_group(vdpa, idx);\n\t\tif (s.num >= vdpa->ngroups)\n\t\t\treturn -EIO;\n\t\telse if (copy_to_user(argp, &s, sizeof(s)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tcase VHOST_VDPA_SET_GROUP_ASID:\n\t\tif (copy_from_user(&s, argp, sizeof(s)))\n\t\t\treturn -EFAULT;\n\t\tif (s.num >= vdpa->nas)\n\t\t\treturn -EINVAL;\n\t\tif (!ops->set_group_asid)\n\t\t\treturn -EOPNOTSUPP;\n\t\treturn ops->set_group_asid(vdpa, idx, s.num);\n\tcase VHOST_GET_VRING_BASE:\n\t\tr = ops->get_vq_state(v->vdpa, idx, &vq_state);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tif (vhost_has_feature(vq, VIRTIO_F_RING_PACKED)) {\n\t\t\tvq->last_avail_idx = vq_state.packed.last_avail_idx |\n\t\t\t\t\t     (vq_state.packed.last_avail_counter << 15);\n\t\t\tvq->last_used_idx = vq_state.packed.last_used_idx |\n\t\t\t\t\t    (vq_state.packed.last_used_counter << 15);\n\t\t} else {\n\t\t\tvq->last_avail_idx = vq_state.split.avail_index;\n\t\t}\n\t\tbreak;\n\t}\n\n\tr = vhost_vring_ioctl(&v->vdev, cmd, argp);\n\tif (r)\n\t\treturn r;\n\n\tswitch (cmd) {\n\tcase VHOST_SET_VRING_ADDR:\n\t\tif (ops->set_vq_address(vdpa, idx,\n\t\t\t\t\t(u64)(uintptr_t)vq->desc,\n\t\t\t\t\t(u64)(uintptr_t)vq->avail,\n\t\t\t\t\t(u64)(uintptr_t)vq->used))\n\t\t\tr = -EINVAL;\n\t\tbreak;\n\n\tcase VHOST_SET_VRING_BASE:\n\t\tif (vhost_has_feature(vq, VIRTIO_F_RING_PACKED)) {\n\t\t\tvq_state.packed.last_avail_idx = vq->last_avail_idx & 0x7fff;\n\t\t\tvq_state.packed.last_avail_counter = !!(vq->last_avail_idx & 0x8000);\n\t\t\tvq_state.packed.last_used_idx = vq->last_used_idx & 0x7fff;\n\t\t\tvq_state.packed.last_used_counter = !!(vq->last_used_idx & 0x8000);\n\t\t} else {\n\t\t\tvq_state.split.avail_index = vq->last_avail_idx;\n\t\t}\n\t\tr = ops->set_vq_state(vdpa, idx, &vq_state);\n\t\tbreak;\n\n\tcase VHOST_SET_VRING_CALL:\n\t\tif (vq->call_ctx.ctx) {\n\t\t\tcb.callback = vhost_vdpa_virtqueue_cb;\n\t\t\tcb.private = vq;\n\t\t\tcb.trigger = vq->call_ctx.ctx;\n\t\t} else {\n\t\t\tcb.callback = NULL;\n\t\t\tcb.private = NULL;\n\t\t\tcb.trigger = NULL;\n\t\t}\n\t\tops->set_vq_cb(vdpa, idx, &cb);\n\t\tvhost_vdpa_setup_vq_irq(v, idx);\n\t\tbreak;\n\n\tcase VHOST_SET_VRING_NUM:\n\t\tops->set_vq_num(vdpa, idx, vq->num);\n\t\tbreak;\n\t}\n\n\treturn r;\n}\n\nstatic long vhost_vdpa_unlocked_ioctl(struct file *filep,\n\t\t\t\t      unsigned int cmd, unsigned long arg)\n{\n\tstruct vhost_vdpa *v = filep->private_data;\n\tstruct vhost_dev *d = &v->vdev;\n\tvoid __user *argp = (void __user *)arg;\n\tu64 __user *featurep = argp;\n\tu64 features;\n\tlong r = 0;\n\n\tif (cmd == VHOST_SET_BACKEND_FEATURES) {\n\t\tif (copy_from_user(&features, featurep, sizeof(features)))\n\t\t\treturn -EFAULT;\n\t\tif (features & ~(VHOST_VDPA_BACKEND_FEATURES |\n\t\t\t\t BIT_ULL(VHOST_BACKEND_F_SUSPEND) |\n\t\t\t\t BIT_ULL(VHOST_BACKEND_F_RESUME) |\n\t\t\t\t BIT_ULL(VHOST_BACKEND_F_ENABLE_AFTER_DRIVER_OK)))\n\t\t\treturn -EOPNOTSUPP;\n\t\tif ((features & BIT_ULL(VHOST_BACKEND_F_SUSPEND)) &&\n\t\t     !vhost_vdpa_can_suspend(v))\n\t\t\treturn -EOPNOTSUPP;\n\t\tif ((features & BIT_ULL(VHOST_BACKEND_F_RESUME)) &&\n\t\t     !vhost_vdpa_can_resume(v))\n\t\t\treturn -EOPNOTSUPP;\n\t\tvhost_set_backend_features(&v->vdev, features);\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&d->mutex);\n\n\tswitch (cmd) {\n\tcase VHOST_VDPA_GET_DEVICE_ID:\n\t\tr = vhost_vdpa_get_device_id(v, argp);\n\t\tbreak;\n\tcase VHOST_VDPA_GET_STATUS:\n\t\tr = vhost_vdpa_get_status(v, argp);\n\t\tbreak;\n\tcase VHOST_VDPA_SET_STATUS:\n\t\tr = vhost_vdpa_set_status(v, argp);\n\t\tbreak;\n\tcase VHOST_VDPA_GET_CONFIG:\n\t\tr = vhost_vdpa_get_config(v, argp);\n\t\tbreak;\n\tcase VHOST_VDPA_SET_CONFIG:\n\t\tr = vhost_vdpa_set_config(v, argp);\n\t\tbreak;\n\tcase VHOST_GET_FEATURES:\n\t\tr = vhost_vdpa_get_features(v, argp);\n\t\tbreak;\n\tcase VHOST_SET_FEATURES:\n\t\tr = vhost_vdpa_set_features(v, argp);\n\t\tbreak;\n\tcase VHOST_VDPA_GET_VRING_NUM:\n\t\tr = vhost_vdpa_get_vring_num(v, argp);\n\t\tbreak;\n\tcase VHOST_VDPA_GET_GROUP_NUM:\n\t\tif (copy_to_user(argp, &v->vdpa->ngroups,\n\t\t\t\t sizeof(v->vdpa->ngroups)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\tcase VHOST_VDPA_GET_AS_NUM:\n\t\tif (copy_to_user(argp, &v->vdpa->nas, sizeof(v->vdpa->nas)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\tcase VHOST_SET_LOG_BASE:\n\tcase VHOST_SET_LOG_FD:\n\t\tr = -ENOIOCTLCMD;\n\t\tbreak;\n\tcase VHOST_VDPA_SET_CONFIG_CALL:\n\t\tr = vhost_vdpa_set_config_call(v, argp);\n\t\tbreak;\n\tcase VHOST_GET_BACKEND_FEATURES:\n\t\tfeatures = VHOST_VDPA_BACKEND_FEATURES;\n\t\tif (vhost_vdpa_can_suspend(v))\n\t\t\tfeatures |= BIT_ULL(VHOST_BACKEND_F_SUSPEND);\n\t\tif (vhost_vdpa_can_resume(v))\n\t\t\tfeatures |= BIT_ULL(VHOST_BACKEND_F_RESUME);\n\t\tfeatures |= vhost_vdpa_get_backend_features(v);\n\t\tif (copy_to_user(featurep, &features, sizeof(features)))\n\t\t\tr = -EFAULT;\n\t\tbreak;\n\tcase VHOST_VDPA_GET_IOVA_RANGE:\n\t\tr = vhost_vdpa_get_iova_range(v, argp);\n\t\tbreak;\n\tcase VHOST_VDPA_GET_CONFIG_SIZE:\n\t\tr = vhost_vdpa_get_config_size(v, argp);\n\t\tbreak;\n\tcase VHOST_VDPA_GET_VQS_COUNT:\n\t\tr = vhost_vdpa_get_vqs_count(v, argp);\n\t\tbreak;\n\tcase VHOST_VDPA_SUSPEND:\n\t\tr = vhost_vdpa_suspend(v);\n\t\tbreak;\n\tcase VHOST_VDPA_RESUME:\n\t\tr = vhost_vdpa_resume(v);\n\t\tbreak;\n\tdefault:\n\t\tr = vhost_dev_ioctl(&v->vdev, cmd, argp);\n\t\tif (r == -ENOIOCTLCMD)\n\t\t\tr = vhost_vdpa_vring_ioctl(v, cmd, argp);\n\t\tbreak;\n\t}\n\n\tif (r)\n\t\tgoto out;\n\n\tswitch (cmd) {\n\tcase VHOST_SET_OWNER:\n\t\tr = vhost_vdpa_bind_mm(v);\n\t\tif (r)\n\t\t\tvhost_dev_reset_owner(d, NULL);\n\t\tbreak;\n\t}\nout:\n\tmutex_unlock(&d->mutex);\n\treturn r;\n}\nstatic void vhost_vdpa_general_unmap(struct vhost_vdpa *v,\n\t\t\t\t     struct vhost_iotlb_map *map, u32 asid)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tif (ops->dma_map) {\n\t\tops->dma_unmap(vdpa, asid, map->start, map->size);\n\t} else if (ops->set_map == NULL) {\n\t\tiommu_unmap(v->domain, map->start, map->size);\n\t}\n}\n\nstatic void vhost_vdpa_pa_unmap(struct vhost_vdpa *v, struct vhost_iotlb *iotlb,\n\t\t\t\tu64 start, u64 last, u32 asid)\n{\n\tstruct vhost_dev *dev = &v->vdev;\n\tstruct vhost_iotlb_map *map;\n\tstruct page *page;\n\tunsigned long pfn, pinned;\n\n\twhile ((map = vhost_iotlb_itree_first(iotlb, start, last)) != NULL) {\n\t\tpinned = PFN_DOWN(map->size);\n\t\tfor (pfn = PFN_DOWN(map->addr);\n\t\t     pinned > 0; pfn++, pinned--) {\n\t\t\tpage = pfn_to_page(pfn);\n\t\t\tif (map->perm & VHOST_ACCESS_WO)\n\t\t\t\tset_page_dirty_lock(page);\n\t\t\tunpin_user_page(page);\n\t\t}\n\t\tatomic64_sub(PFN_DOWN(map->size), &dev->mm->pinned_vm);\n\t\tvhost_vdpa_general_unmap(v, map, asid);\n\t\tvhost_iotlb_map_free(iotlb, map);\n\t}\n}\n\nstatic void vhost_vdpa_va_unmap(struct vhost_vdpa *v, struct vhost_iotlb *iotlb,\n\t\t\t\tu64 start, u64 last, u32 asid)\n{\n\tstruct vhost_iotlb_map *map;\n\tstruct vdpa_map_file *map_file;\n\n\twhile ((map = vhost_iotlb_itree_first(iotlb, start, last)) != NULL) {\n\t\tmap_file = (struct vdpa_map_file *)map->opaque;\n\t\tfput(map_file->file);\n\t\tkfree(map_file);\n\t\tvhost_vdpa_general_unmap(v, map, asid);\n\t\tvhost_iotlb_map_free(iotlb, map);\n\t}\n}\n\nstatic void vhost_vdpa_iotlb_unmap(struct vhost_vdpa *v,\n\t\t\t\t   struct vhost_iotlb *iotlb, u64 start,\n\t\t\t\t   u64 last, u32 asid)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\n\tif (vdpa->use_va)\n\t\treturn vhost_vdpa_va_unmap(v, iotlb, start, last, asid);\n\n\treturn vhost_vdpa_pa_unmap(v, iotlb, start, last, asid);\n}\n\nstatic int perm_to_iommu_flags(u32 perm)\n{\n\tint flags = 0;\n\n\tswitch (perm) {\n\tcase VHOST_ACCESS_WO:\n\t\tflags |= IOMMU_WRITE;\n\t\tbreak;\n\tcase VHOST_ACCESS_RO:\n\t\tflags |= IOMMU_READ;\n\t\tbreak;\n\tcase VHOST_ACCESS_RW:\n\t\tflags |= (IOMMU_WRITE | IOMMU_READ);\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"invalidate vhost IOTLB permission\\n\");\n\t\tbreak;\n\t}\n\n\treturn flags | IOMMU_CACHE;\n}\n\nstatic int vhost_vdpa_map(struct vhost_vdpa *v, struct vhost_iotlb *iotlb,\n\t\t\t  u64 iova, u64 size, u64 pa, u32 perm, void *opaque)\n{\n\tstruct vhost_dev *dev = &v->vdev;\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tu32 asid = iotlb_to_asid(iotlb);\n\tint r = 0;\n\n\tr = vhost_iotlb_add_range_ctx(iotlb, iova, iova + size - 1,\n\t\t\t\t      pa, perm, opaque);\n\tif (r)\n\t\treturn r;\n\n\tif (ops->dma_map) {\n\t\tr = ops->dma_map(vdpa, asid, iova, size, pa, perm, opaque);\n\t} else if (ops->set_map) {\n\t\tif (!v->in_batch)\n\t\t\tr = ops->set_map(vdpa, asid, iotlb);\n\t} else {\n\t\tr = iommu_map(v->domain, iova, pa, size,\n\t\t\t      perm_to_iommu_flags(perm), GFP_KERNEL);\n\t}\n\tif (r) {\n\t\tvhost_iotlb_del_range(iotlb, iova, iova + size - 1);\n\t\treturn r;\n\t}\n\n\tif (!vdpa->use_va)\n\t\tatomic64_add(PFN_DOWN(size), &dev->mm->pinned_vm);\n\n\treturn 0;\n}\n\nstatic void vhost_vdpa_unmap(struct vhost_vdpa *v,\n\t\t\t     struct vhost_iotlb *iotlb,\n\t\t\t     u64 iova, u64 size)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tu32 asid = iotlb_to_asid(iotlb);\n\n\tvhost_vdpa_iotlb_unmap(v, iotlb, iova, iova + size - 1, asid);\n\n\tif (ops->set_map) {\n\t\tif (!v->in_batch)\n\t\t\tops->set_map(vdpa, asid, iotlb);\n\t}\n\n}\n\nstatic int vhost_vdpa_va_map(struct vhost_vdpa *v,\n\t\t\t     struct vhost_iotlb *iotlb,\n\t\t\t     u64 iova, u64 size, u64 uaddr, u32 perm)\n{\n\tstruct vhost_dev *dev = &v->vdev;\n\tu64 offset, map_size, map_iova = iova;\n\tstruct vdpa_map_file *map_file;\n\tstruct vm_area_struct *vma;\n\tint ret = 0;\n\n\tmmap_read_lock(dev->mm);\n\n\twhile (size) {\n\t\tvma = find_vma(dev->mm, uaddr);\n\t\tif (!vma) {\n\t\t\tret = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tmap_size = min(size, vma->vm_end - uaddr);\n\t\tif (!(vma->vm_file && (vma->vm_flags & VM_SHARED) &&\n\t\t\t!(vma->vm_flags & (VM_IO | VM_PFNMAP))))\n\t\t\tgoto next;\n\n\t\tmap_file = kzalloc(sizeof(*map_file), GFP_KERNEL);\n\t\tif (!map_file) {\n\t\t\tret = -ENOMEM;\n\t\t\tbreak;\n\t\t}\n\t\toffset = (vma->vm_pgoff << PAGE_SHIFT) + uaddr - vma->vm_start;\n\t\tmap_file->offset = offset;\n\t\tmap_file->file = get_file(vma->vm_file);\n\t\tret = vhost_vdpa_map(v, iotlb, map_iova, map_size, uaddr,\n\t\t\t\t     perm, map_file);\n\t\tif (ret) {\n\t\t\tfput(map_file->file);\n\t\t\tkfree(map_file);\n\t\t\tbreak;\n\t\t}\nnext:\n\t\tsize -= map_size;\n\t\tuaddr += map_size;\n\t\tmap_iova += map_size;\n\t}\n\tif (ret)\n\t\tvhost_vdpa_unmap(v, iotlb, iova, map_iova - iova);\n\n\tmmap_read_unlock(dev->mm);\n\n\treturn ret;\n}\n\nstatic int vhost_vdpa_pa_map(struct vhost_vdpa *v,\n\t\t\t     struct vhost_iotlb *iotlb,\n\t\t\t     u64 iova, u64 size, u64 uaddr, u32 perm)\n{\n\tstruct vhost_dev *dev = &v->vdev;\n\tstruct page **page_list;\n\tunsigned long list_size = PAGE_SIZE / sizeof(struct page *);\n\tunsigned int gup_flags = FOLL_LONGTERM;\n\tunsigned long npages, cur_base, map_pfn, last_pfn = 0;\n\tunsigned long lock_limit, sz2pin, nchunks, i;\n\tu64 start = iova;\n\tlong pinned;\n\tint ret = 0;\n\n\t \n\tpage_list = (struct page **) __get_free_page(GFP_KERNEL);\n\tif (!page_list)\n\t\treturn -ENOMEM;\n\n\tif (perm & VHOST_ACCESS_WO)\n\t\tgup_flags |= FOLL_WRITE;\n\n\tnpages = PFN_UP(size + (iova & ~PAGE_MASK));\n\tif (!npages) {\n\t\tret = -EINVAL;\n\t\tgoto free;\n\t}\n\n\tmmap_read_lock(dev->mm);\n\n\tlock_limit = PFN_DOWN(rlimit(RLIMIT_MEMLOCK));\n\tif (npages + atomic64_read(&dev->mm->pinned_vm) > lock_limit) {\n\t\tret = -ENOMEM;\n\t\tgoto unlock;\n\t}\n\n\tcur_base = uaddr & PAGE_MASK;\n\tiova &= PAGE_MASK;\n\tnchunks = 0;\n\n\twhile (npages) {\n\t\tsz2pin = min_t(unsigned long, npages, list_size);\n\t\tpinned = pin_user_pages(cur_base, sz2pin,\n\t\t\t\t\tgup_flags, page_list);\n\t\tif (sz2pin != pinned) {\n\t\t\tif (pinned < 0) {\n\t\t\t\tret = pinned;\n\t\t\t} else {\n\t\t\t\tunpin_user_pages(page_list, pinned);\n\t\t\t\tret = -ENOMEM;\n\t\t\t}\n\t\t\tgoto out;\n\t\t}\n\t\tnchunks++;\n\n\t\tif (!last_pfn)\n\t\t\tmap_pfn = page_to_pfn(page_list[0]);\n\n\t\tfor (i = 0; i < pinned; i++) {\n\t\t\tunsigned long this_pfn = page_to_pfn(page_list[i]);\n\t\t\tu64 csize;\n\n\t\t\tif (last_pfn && (this_pfn != last_pfn + 1)) {\n\t\t\t\t \n\t\t\t\tcsize = PFN_PHYS(last_pfn - map_pfn + 1);\n\t\t\t\tret = vhost_vdpa_map(v, iotlb, iova, csize,\n\t\t\t\t\t\t     PFN_PHYS(map_pfn),\n\t\t\t\t\t\t     perm, NULL);\n\t\t\t\tif (ret) {\n\t\t\t\t\t \n\t\t\t\t\tunpin_user_pages(&page_list[i],\n\t\t\t\t\t\t\t pinned - i);\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\tmap_pfn = this_pfn;\n\t\t\t\tiova += csize;\n\t\t\t\tnchunks = 0;\n\t\t\t}\n\n\t\t\tlast_pfn = this_pfn;\n\t\t}\n\n\t\tcur_base += PFN_PHYS(pinned);\n\t\tnpages -= pinned;\n\t}\n\n\t \n\tret = vhost_vdpa_map(v, iotlb, iova, PFN_PHYS(last_pfn - map_pfn + 1),\n\t\t\t     PFN_PHYS(map_pfn), perm, NULL);\nout:\n\tif (ret) {\n\t\tif (nchunks) {\n\t\t\tunsigned long pfn;\n\n\t\t\t \n\t\t\tWARN_ON(!last_pfn);\n\t\t\tfor (pfn = map_pfn; pfn <= last_pfn; pfn++)\n\t\t\t\tunpin_user_page(pfn_to_page(pfn));\n\t\t}\n\t\tvhost_vdpa_unmap(v, iotlb, start, size);\n\t}\nunlock:\n\tmmap_read_unlock(dev->mm);\nfree:\n\tfree_page((unsigned long)page_list);\n\treturn ret;\n\n}\n\nstatic int vhost_vdpa_process_iotlb_update(struct vhost_vdpa *v,\n\t\t\t\t\t   struct vhost_iotlb *iotlb,\n\t\t\t\t\t   struct vhost_iotlb_msg *msg)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\n\tif (msg->iova < v->range.first || !msg->size ||\n\t    msg->iova > U64_MAX - msg->size + 1 ||\n\t    msg->iova + msg->size - 1 > v->range.last)\n\t\treturn -EINVAL;\n\n\tif (vhost_iotlb_itree_first(iotlb, msg->iova,\n\t\t\t\t    msg->iova + msg->size - 1))\n\t\treturn -EEXIST;\n\n\tif (vdpa->use_va)\n\t\treturn vhost_vdpa_va_map(v, iotlb, msg->iova, msg->size,\n\t\t\t\t\t msg->uaddr, msg->perm);\n\n\treturn vhost_vdpa_pa_map(v, iotlb, msg->iova, msg->size, msg->uaddr,\n\t\t\t\t msg->perm);\n}\n\nstatic int vhost_vdpa_process_iotlb_msg(struct vhost_dev *dev, u32 asid,\n\t\t\t\t\tstruct vhost_iotlb_msg *msg)\n{\n\tstruct vhost_vdpa *v = container_of(dev, struct vhost_vdpa, vdev);\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tstruct vhost_iotlb *iotlb = NULL;\n\tstruct vhost_vdpa_as *as = NULL;\n\tint r = 0;\n\n\tmutex_lock(&dev->mutex);\n\n\tr = vhost_dev_check_owner(dev);\n\tif (r)\n\t\tgoto unlock;\n\n\tif (msg->type == VHOST_IOTLB_UPDATE ||\n\t    msg->type == VHOST_IOTLB_BATCH_BEGIN) {\n\t\tas = vhost_vdpa_find_alloc_as(v, asid);\n\t\tif (!as) {\n\t\t\tdev_err(&v->dev, \"can't find and alloc asid %d\\n\",\n\t\t\t\tasid);\n\t\t\tr = -EINVAL;\n\t\t\tgoto unlock;\n\t\t}\n\t\tiotlb = &as->iotlb;\n\t} else\n\t\tiotlb = asid_to_iotlb(v, asid);\n\n\tif ((v->in_batch && v->batch_asid != asid) || !iotlb) {\n\t\tif (v->in_batch && v->batch_asid != asid) {\n\t\t\tdev_info(&v->dev, \"batch id %d asid %d\\n\",\n\t\t\t\t v->batch_asid, asid);\n\t\t}\n\t\tif (!iotlb)\n\t\t\tdev_err(&v->dev, \"no iotlb for asid %d\\n\", asid);\n\t\tr = -EINVAL;\n\t\tgoto unlock;\n\t}\n\n\tswitch (msg->type) {\n\tcase VHOST_IOTLB_UPDATE:\n\t\tr = vhost_vdpa_process_iotlb_update(v, iotlb, msg);\n\t\tbreak;\n\tcase VHOST_IOTLB_INVALIDATE:\n\t\tvhost_vdpa_unmap(v, iotlb, msg->iova, msg->size);\n\t\tbreak;\n\tcase VHOST_IOTLB_BATCH_BEGIN:\n\t\tv->batch_asid = asid;\n\t\tv->in_batch = true;\n\t\tbreak;\n\tcase VHOST_IOTLB_BATCH_END:\n\t\tif (v->in_batch && ops->set_map)\n\t\t\tops->set_map(vdpa, asid, iotlb);\n\t\tv->in_batch = false;\n\t\tbreak;\n\tdefault:\n\t\tr = -EINVAL;\n\t\tbreak;\n\t}\nunlock:\n\tmutex_unlock(&dev->mutex);\n\n\treturn r;\n}\n\nstatic ssize_t vhost_vdpa_chr_write_iter(struct kiocb *iocb,\n\t\t\t\t\t struct iov_iter *from)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct vhost_vdpa *v = file->private_data;\n\tstruct vhost_dev *dev = &v->vdev;\n\n\treturn vhost_chr_write_iter(dev, from);\n}\n\nstatic int vhost_vdpa_alloc_domain(struct vhost_vdpa *v)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tstruct device *dma_dev = vdpa_get_dma_dev(vdpa);\n\tconst struct bus_type *bus;\n\tint ret;\n\n\t \n\tif (ops->set_map || ops->dma_map)\n\t\treturn 0;\n\n\tbus = dma_dev->bus;\n\tif (!bus)\n\t\treturn -EFAULT;\n\n\tif (!device_iommu_capable(dma_dev, IOMMU_CAP_CACHE_COHERENCY)) {\n\t\tdev_warn_once(&v->dev,\n\t\t\t      \"Failed to allocate domain, device is not IOMMU cache coherent capable\\n\");\n\t\treturn -ENOTSUPP;\n\t}\n\n\tv->domain = iommu_domain_alloc(bus);\n\tif (!v->domain)\n\t\treturn -EIO;\n\n\tret = iommu_attach_device(v->domain, dma_dev);\n\tif (ret)\n\t\tgoto err_attach;\n\n\treturn 0;\n\nerr_attach:\n\tiommu_domain_free(v->domain);\n\tv->domain = NULL;\n\treturn ret;\n}\n\nstatic void vhost_vdpa_free_domain(struct vhost_vdpa *v)\n{\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tstruct device *dma_dev = vdpa_get_dma_dev(vdpa);\n\n\tif (v->domain) {\n\t\tiommu_detach_device(v->domain, dma_dev);\n\t\tiommu_domain_free(v->domain);\n\t}\n\n\tv->domain = NULL;\n}\n\nstatic void vhost_vdpa_set_iova_range(struct vhost_vdpa *v)\n{\n\tstruct vdpa_iova_range *range = &v->range;\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\n\tif (ops->get_iova_range) {\n\t\t*range = ops->get_iova_range(vdpa);\n\t} else if (v->domain && v->domain->geometry.force_aperture) {\n\t\trange->first = v->domain->geometry.aperture_start;\n\t\trange->last = v->domain->geometry.aperture_end;\n\t} else {\n\t\trange->first = 0;\n\t\trange->last = ULLONG_MAX;\n\t}\n}\n\nstatic void vhost_vdpa_cleanup(struct vhost_vdpa *v)\n{\n\tstruct vhost_vdpa_as *as;\n\tu32 asid;\n\n\tfor (asid = 0; asid < v->vdpa->nas; asid++) {\n\t\tas = asid_to_as(v, asid);\n\t\tif (as)\n\t\t\tvhost_vdpa_remove_as(v, asid);\n\t}\n\n\tvhost_vdpa_free_domain(v);\n\tvhost_dev_cleanup(&v->vdev);\n\tkfree(v->vdev.vqs);\n}\n\nstatic int vhost_vdpa_open(struct inode *inode, struct file *filep)\n{\n\tstruct vhost_vdpa *v;\n\tstruct vhost_dev *dev;\n\tstruct vhost_virtqueue **vqs;\n\tint r, opened;\n\tu32 i, nvqs;\n\n\tv = container_of(inode->i_cdev, struct vhost_vdpa, cdev);\n\n\topened = atomic_cmpxchg(&v->opened, 0, 1);\n\tif (opened)\n\t\treturn -EBUSY;\n\n\tnvqs = v->nvqs;\n\tr = vhost_vdpa_reset(v);\n\tif (r)\n\t\tgoto err;\n\n\tvqs = kmalloc_array(nvqs, sizeof(*vqs), GFP_KERNEL);\n\tif (!vqs) {\n\t\tr = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tdev = &v->vdev;\n\tfor (i = 0; i < nvqs; i++) {\n\t\tvqs[i] = &v->vqs[i];\n\t\tvqs[i]->handle_kick = handle_vq_kick;\n\t}\n\tvhost_dev_init(dev, vqs, nvqs, 0, 0, 0, false,\n\t\t       vhost_vdpa_process_iotlb_msg);\n\n\tr = vhost_vdpa_alloc_domain(v);\n\tif (r)\n\t\tgoto err_alloc_domain;\n\n\tvhost_vdpa_set_iova_range(v);\n\n\tfilep->private_data = v;\n\n\treturn 0;\n\nerr_alloc_domain:\n\tvhost_vdpa_cleanup(v);\nerr:\n\tatomic_dec(&v->opened);\n\treturn r;\n}\n\nstatic void vhost_vdpa_clean_irq(struct vhost_vdpa *v)\n{\n\tu32 i;\n\n\tfor (i = 0; i < v->nvqs; i++)\n\t\tvhost_vdpa_unsetup_vq_irq(v, i);\n}\n\nstatic int vhost_vdpa_release(struct inode *inode, struct file *filep)\n{\n\tstruct vhost_vdpa *v = filep->private_data;\n\tstruct vhost_dev *d = &v->vdev;\n\n\tmutex_lock(&d->mutex);\n\tfilep->private_data = NULL;\n\tvhost_vdpa_clean_irq(v);\n\tvhost_vdpa_reset(v);\n\tvhost_dev_stop(&v->vdev);\n\tvhost_vdpa_unbind_mm(v);\n\tvhost_vdpa_config_put(v);\n\tvhost_vdpa_cleanup(v);\n\tmutex_unlock(&d->mutex);\n\n\tatomic_dec(&v->opened);\n\tcomplete(&v->completion);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_MMU\nstatic vm_fault_t vhost_vdpa_fault(struct vm_fault *vmf)\n{\n\tstruct vhost_vdpa *v = vmf->vma->vm_file->private_data;\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tstruct vdpa_notification_area notify;\n\tstruct vm_area_struct *vma = vmf->vma;\n\tu16 index = vma->vm_pgoff;\n\n\tnotify = ops->get_vq_notification(vdpa, index);\n\n\tvma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);\n\tif (remap_pfn_range(vma, vmf->address & PAGE_MASK,\n\t\t\t    PFN_DOWN(notify.addr), PAGE_SIZE,\n\t\t\t    vma->vm_page_prot))\n\t\treturn VM_FAULT_SIGBUS;\n\n\treturn VM_FAULT_NOPAGE;\n}\n\nstatic const struct vm_operations_struct vhost_vdpa_vm_ops = {\n\t.fault = vhost_vdpa_fault,\n};\n\nstatic int vhost_vdpa_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tstruct vhost_vdpa *v = vma->vm_file->private_data;\n\tstruct vdpa_device *vdpa = v->vdpa;\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tstruct vdpa_notification_area notify;\n\tunsigned long index = vma->vm_pgoff;\n\n\tif (vma->vm_end - vma->vm_start != PAGE_SIZE)\n\t\treturn -EINVAL;\n\tif ((vma->vm_flags & VM_SHARED) == 0)\n\t\treturn -EINVAL;\n\tif (vma->vm_flags & VM_READ)\n\t\treturn -EINVAL;\n\tif (index > 65535)\n\t\treturn -EINVAL;\n\tif (!ops->get_vq_notification)\n\t\treturn -ENOTSUPP;\n\n\t \n\tnotify = ops->get_vq_notification(vdpa, index);\n\tif (notify.addr & (PAGE_SIZE - 1))\n\t\treturn -EINVAL;\n\tif (vma->vm_end - vma->vm_start != notify.size)\n\t\treturn -ENOTSUPP;\n\n\tvm_flags_set(vma, VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP);\n\tvma->vm_ops = &vhost_vdpa_vm_ops;\n\treturn 0;\n}\n#endif  \n\nstatic const struct file_operations vhost_vdpa_fops = {\n\t.owner\t\t= THIS_MODULE,\n\t.open\t\t= vhost_vdpa_open,\n\t.release\t= vhost_vdpa_release,\n\t.write_iter\t= vhost_vdpa_chr_write_iter,\n\t.unlocked_ioctl\t= vhost_vdpa_unlocked_ioctl,\n#ifdef CONFIG_MMU\n\t.mmap\t\t= vhost_vdpa_mmap,\n#endif  \n\t.compat_ioctl\t= compat_ptr_ioctl,\n};\n\nstatic void vhost_vdpa_release_dev(struct device *device)\n{\n\tstruct vhost_vdpa *v =\n\t       container_of(device, struct vhost_vdpa, dev);\n\n\tida_simple_remove(&vhost_vdpa_ida, v->minor);\n\tkfree(v->vqs);\n\tkfree(v);\n}\n\nstatic int vhost_vdpa_probe(struct vdpa_device *vdpa)\n{\n\tconst struct vdpa_config_ops *ops = vdpa->config;\n\tstruct vhost_vdpa *v;\n\tint minor;\n\tint i, r;\n\n\t \n\tif (!ops->set_map && !ops->dma_map &&\n\t    (vdpa->ngroups > 1 || vdpa->nas > 1))\n\t\treturn -EOPNOTSUPP;\n\n\tv = kzalloc(sizeof(*v), GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!v)\n\t\treturn -ENOMEM;\n\n\tminor = ida_simple_get(&vhost_vdpa_ida, 0,\n\t\t\t       VHOST_VDPA_DEV_MAX, GFP_KERNEL);\n\tif (minor < 0) {\n\t\tkfree(v);\n\t\treturn minor;\n\t}\n\n\tatomic_set(&v->opened, 0);\n\tv->minor = minor;\n\tv->vdpa = vdpa;\n\tv->nvqs = vdpa->nvqs;\n\tv->virtio_id = ops->get_device_id(vdpa);\n\n\tdevice_initialize(&v->dev);\n\tv->dev.release = vhost_vdpa_release_dev;\n\tv->dev.parent = &vdpa->dev;\n\tv->dev.devt = MKDEV(MAJOR(vhost_vdpa_major), minor);\n\tv->vqs = kmalloc_array(v->nvqs, sizeof(struct vhost_virtqueue),\n\t\t\t       GFP_KERNEL);\n\tif (!v->vqs) {\n\t\tr = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tr = dev_set_name(&v->dev, \"vhost-vdpa-%u\", minor);\n\tif (r)\n\t\tgoto err;\n\n\tcdev_init(&v->cdev, &vhost_vdpa_fops);\n\tv->cdev.owner = THIS_MODULE;\n\n\tr = cdev_device_add(&v->cdev, &v->dev);\n\tif (r)\n\t\tgoto err;\n\n\tinit_completion(&v->completion);\n\tvdpa_set_drvdata(vdpa, v);\n\n\tfor (i = 0; i < VHOST_VDPA_IOTLB_BUCKETS; i++)\n\t\tINIT_HLIST_HEAD(&v->as[i]);\n\n\treturn 0;\n\nerr:\n\tput_device(&v->dev);\n\treturn r;\n}\n\nstatic void vhost_vdpa_remove(struct vdpa_device *vdpa)\n{\n\tstruct vhost_vdpa *v = vdpa_get_drvdata(vdpa);\n\tint opened;\n\n\tcdev_device_del(&v->cdev, &v->dev);\n\n\tdo {\n\t\topened = atomic_cmpxchg(&v->opened, 0, 1);\n\t\tif (!opened)\n\t\t\tbreak;\n\t\twait_for_completion(&v->completion);\n\t} while (1);\n\n\tput_device(&v->dev);\n}\n\nstatic struct vdpa_driver vhost_vdpa_driver = {\n\t.driver = {\n\t\t.name\t= \"vhost_vdpa\",\n\t},\n\t.probe\t= vhost_vdpa_probe,\n\t.remove\t= vhost_vdpa_remove,\n};\n\nstatic int __init vhost_vdpa_init(void)\n{\n\tint r;\n\n\tr = alloc_chrdev_region(&vhost_vdpa_major, 0, VHOST_VDPA_DEV_MAX,\n\t\t\t\t\"vhost-vdpa\");\n\tif (r)\n\t\tgoto err_alloc_chrdev;\n\n\tr = vdpa_register_driver(&vhost_vdpa_driver);\n\tif (r)\n\t\tgoto err_vdpa_register_driver;\n\n\treturn 0;\n\nerr_vdpa_register_driver:\n\tunregister_chrdev_region(vhost_vdpa_major, VHOST_VDPA_DEV_MAX);\nerr_alloc_chrdev:\n\treturn r;\n}\nmodule_init(vhost_vdpa_init);\n\nstatic void __exit vhost_vdpa_exit(void)\n{\n\tvdpa_unregister_driver(&vhost_vdpa_driver);\n\tunregister_chrdev_region(vhost_vdpa_major, VHOST_VDPA_DEV_MAX);\n}\nmodule_exit(vhost_vdpa_exit);\n\nMODULE_VERSION(\"0.0.1\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Intel Corporation\");\nMODULE_DESCRIPTION(\"vDPA-based vhost backend for virtio\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}