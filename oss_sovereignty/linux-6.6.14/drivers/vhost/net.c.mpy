{
  "module_name": "net.c",
  "hash_id": "2afef86f248ebd838a29528080dc748630a7c847a7df64f63c4fd0379554e016",
  "original_prompt": "Ingested from linux-6.6.14/drivers/vhost/net.c",
  "human_readable_source": "\n \n\n#include <linux/compat.h>\n#include <linux/eventfd.h>\n#include <linux/vhost.h>\n#include <linux/virtio_net.h>\n#include <linux/miscdevice.h>\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/mutex.h>\n#include <linux/workqueue.h>\n#include <linux/file.h>\n#include <linux/slab.h>\n#include <linux/sched/clock.h>\n#include <linux/sched/signal.h>\n#include <linux/vmalloc.h>\n\n#include <linux/net.h>\n#include <linux/if_packet.h>\n#include <linux/if_arp.h>\n#include <linux/if_tun.h>\n#include <linux/if_macvlan.h>\n#include <linux/if_tap.h>\n#include <linux/if_vlan.h>\n#include <linux/skb_array.h>\n#include <linux/skbuff.h>\n\n#include <net/sock.h>\n#include <net/xdp.h>\n\n#include \"vhost.h\"\n\nstatic int experimental_zcopytx = 0;\nmodule_param(experimental_zcopytx, int, 0444);\nMODULE_PARM_DESC(experimental_zcopytx, \"Enable Zero Copy TX;\"\n\t\t                       \" 1 -Enable; 0 - Disable\");\n\n \n#define VHOST_NET_WEIGHT 0x80000\n\n \n#define VHOST_NET_PKT_WEIGHT 256\n\n \n#define VHOST_MAX_PEND 128\n#define VHOST_GOODCOPY_LEN 256\n\n \n \n#define VHOST_DMA_FAILED_LEN\t((__force __virtio32)3)\n \n#define VHOST_DMA_DONE_LEN\t((__force __virtio32)2)\n \n#define VHOST_DMA_IN_PROGRESS\t((__force __virtio32)1)\n \n#define VHOST_DMA_CLEAR_LEN\t((__force __virtio32)0)\n\n#define VHOST_DMA_IS_DONE(len) ((__force u32)(len) >= (__force u32)VHOST_DMA_DONE_LEN)\n\nenum {\n\tVHOST_NET_FEATURES = VHOST_FEATURES |\n\t\t\t (1ULL << VHOST_NET_F_VIRTIO_NET_HDR) |\n\t\t\t (1ULL << VIRTIO_NET_F_MRG_RXBUF) |\n\t\t\t (1ULL << VIRTIO_F_ACCESS_PLATFORM) |\n\t\t\t (1ULL << VIRTIO_F_RING_RESET)\n};\n\nenum {\n\tVHOST_NET_BACKEND_FEATURES = (1ULL << VHOST_BACKEND_F_IOTLB_MSG_V2)\n};\n\nenum {\n\tVHOST_NET_VQ_RX = 0,\n\tVHOST_NET_VQ_TX = 1,\n\tVHOST_NET_VQ_MAX = 2,\n};\n\nstruct vhost_net_ubuf_ref {\n\t \n\tatomic_t refcount;\n\twait_queue_head_t wait;\n\tstruct vhost_virtqueue *vq;\n};\n\n#define VHOST_NET_BATCH 64\nstruct vhost_net_buf {\n\tvoid **queue;\n\tint tail;\n\tint head;\n};\n\nstruct vhost_net_virtqueue {\n\tstruct vhost_virtqueue vq;\n\tsize_t vhost_hlen;\n\tsize_t sock_hlen;\n\t \n\t \n\tint upend_idx;\n\t \n\tint done_idx;\n\t \n\tint batched_xdp;\n\t \n\tstruct ubuf_info_msgzc *ubuf_info;\n\t \n\tstruct vhost_net_ubuf_ref *ubufs;\n\tstruct ptr_ring *rx_ring;\n\tstruct vhost_net_buf rxq;\n\t \n\tstruct xdp_buff *xdp;\n};\n\nstruct vhost_net {\n\tstruct vhost_dev dev;\n\tstruct vhost_net_virtqueue vqs[VHOST_NET_VQ_MAX];\n\tstruct vhost_poll poll[VHOST_NET_VQ_MAX];\n\t \n\tunsigned tx_packets;\n\t \n\tunsigned tx_zcopy_err;\n\t \n\tbool tx_flush;\n\t \n\tstruct page_frag page_frag;\n\t \n\tint refcnt_bias;\n};\n\nstatic unsigned vhost_net_zcopy_mask __read_mostly;\n\nstatic void *vhost_net_buf_get_ptr(struct vhost_net_buf *rxq)\n{\n\tif (rxq->tail != rxq->head)\n\t\treturn rxq->queue[rxq->head];\n\telse\n\t\treturn NULL;\n}\n\nstatic int vhost_net_buf_get_size(struct vhost_net_buf *rxq)\n{\n\treturn rxq->tail - rxq->head;\n}\n\nstatic int vhost_net_buf_is_empty(struct vhost_net_buf *rxq)\n{\n\treturn rxq->tail == rxq->head;\n}\n\nstatic void *vhost_net_buf_consume(struct vhost_net_buf *rxq)\n{\n\tvoid *ret = vhost_net_buf_get_ptr(rxq);\n\t++rxq->head;\n\treturn ret;\n}\n\nstatic int vhost_net_buf_produce(struct vhost_net_virtqueue *nvq)\n{\n\tstruct vhost_net_buf *rxq = &nvq->rxq;\n\n\trxq->head = 0;\n\trxq->tail = ptr_ring_consume_batched(nvq->rx_ring, rxq->queue,\n\t\t\t\t\t      VHOST_NET_BATCH);\n\treturn rxq->tail;\n}\n\nstatic void vhost_net_buf_unproduce(struct vhost_net_virtqueue *nvq)\n{\n\tstruct vhost_net_buf *rxq = &nvq->rxq;\n\n\tif (nvq->rx_ring && !vhost_net_buf_is_empty(rxq)) {\n\t\tptr_ring_unconsume(nvq->rx_ring, rxq->queue + rxq->head,\n\t\t\t\t   vhost_net_buf_get_size(rxq),\n\t\t\t\t   tun_ptr_free);\n\t\trxq->head = rxq->tail = 0;\n\t}\n}\n\nstatic int vhost_net_buf_peek_len(void *ptr)\n{\n\tif (tun_is_xdp_frame(ptr)) {\n\t\tstruct xdp_frame *xdpf = tun_ptr_to_xdp(ptr);\n\n\t\treturn xdpf->len;\n\t}\n\n\treturn __skb_array_len_with_tag(ptr);\n}\n\nstatic int vhost_net_buf_peek(struct vhost_net_virtqueue *nvq)\n{\n\tstruct vhost_net_buf *rxq = &nvq->rxq;\n\n\tif (!vhost_net_buf_is_empty(rxq))\n\t\tgoto out;\n\n\tif (!vhost_net_buf_produce(nvq))\n\t\treturn 0;\n\nout:\n\treturn vhost_net_buf_peek_len(vhost_net_buf_get_ptr(rxq));\n}\n\nstatic void vhost_net_buf_init(struct vhost_net_buf *rxq)\n{\n\trxq->head = rxq->tail = 0;\n}\n\nstatic void vhost_net_enable_zcopy(int vq)\n{\n\tvhost_net_zcopy_mask |= 0x1 << vq;\n}\n\nstatic struct vhost_net_ubuf_ref *\nvhost_net_ubuf_alloc(struct vhost_virtqueue *vq, bool zcopy)\n{\n\tstruct vhost_net_ubuf_ref *ubufs;\n\t \n\tif (!zcopy)\n\t\treturn NULL;\n\tubufs = kmalloc(sizeof(*ubufs), GFP_KERNEL);\n\tif (!ubufs)\n\t\treturn ERR_PTR(-ENOMEM);\n\tatomic_set(&ubufs->refcount, 1);\n\tinit_waitqueue_head(&ubufs->wait);\n\tubufs->vq = vq;\n\treturn ubufs;\n}\n\nstatic int vhost_net_ubuf_put(struct vhost_net_ubuf_ref *ubufs)\n{\n\tint r = atomic_sub_return(1, &ubufs->refcount);\n\tif (unlikely(!r))\n\t\twake_up(&ubufs->wait);\n\treturn r;\n}\n\nstatic void vhost_net_ubuf_put_and_wait(struct vhost_net_ubuf_ref *ubufs)\n{\n\tvhost_net_ubuf_put(ubufs);\n\twait_event(ubufs->wait, !atomic_read(&ubufs->refcount));\n}\n\nstatic void vhost_net_ubuf_put_wait_and_free(struct vhost_net_ubuf_ref *ubufs)\n{\n\tvhost_net_ubuf_put_and_wait(ubufs);\n\tkfree(ubufs);\n}\n\nstatic void vhost_net_clear_ubuf_info(struct vhost_net *n)\n{\n\tint i;\n\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; ++i) {\n\t\tkfree(n->vqs[i].ubuf_info);\n\t\tn->vqs[i].ubuf_info = NULL;\n\t}\n}\n\nstatic int vhost_net_set_ubuf_info(struct vhost_net *n)\n{\n\tbool zcopy;\n\tint i;\n\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; ++i) {\n\t\tzcopy = vhost_net_zcopy_mask & (0x1 << i);\n\t\tif (!zcopy)\n\t\t\tcontinue;\n\t\tn->vqs[i].ubuf_info =\n\t\t\tkmalloc_array(UIO_MAXIOV,\n\t\t\t\t      sizeof(*n->vqs[i].ubuf_info),\n\t\t\t\t      GFP_KERNEL);\n\t\tif  (!n->vqs[i].ubuf_info)\n\t\t\tgoto err;\n\t}\n\treturn 0;\n\nerr:\n\tvhost_net_clear_ubuf_info(n);\n\treturn -ENOMEM;\n}\n\nstatic void vhost_net_vq_reset(struct vhost_net *n)\n{\n\tint i;\n\n\tvhost_net_clear_ubuf_info(n);\n\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; i++) {\n\t\tn->vqs[i].done_idx = 0;\n\t\tn->vqs[i].upend_idx = 0;\n\t\tn->vqs[i].ubufs = NULL;\n\t\tn->vqs[i].vhost_hlen = 0;\n\t\tn->vqs[i].sock_hlen = 0;\n\t\tvhost_net_buf_init(&n->vqs[i].rxq);\n\t}\n\n}\n\nstatic void vhost_net_tx_packet(struct vhost_net *net)\n{\n\t++net->tx_packets;\n\tif (net->tx_packets < 1024)\n\t\treturn;\n\tnet->tx_packets = 0;\n\tnet->tx_zcopy_err = 0;\n}\n\nstatic void vhost_net_tx_err(struct vhost_net *net)\n{\n\t++net->tx_zcopy_err;\n}\n\nstatic bool vhost_net_tx_select_zcopy(struct vhost_net *net)\n{\n\t \n\treturn !net->tx_flush &&\n\t\tnet->tx_packets / 64 >= net->tx_zcopy_err;\n}\n\nstatic bool vhost_sock_zcopy(struct socket *sock)\n{\n\treturn unlikely(experimental_zcopytx) &&\n\t\tsock_flag(sock->sk, SOCK_ZEROCOPY);\n}\n\nstatic bool vhost_sock_xdp(struct socket *sock)\n{\n\treturn sock_flag(sock->sk, SOCK_XDP);\n}\n\n \nstatic void vhost_zerocopy_signal_used(struct vhost_net *net,\n\t\t\t\t       struct vhost_virtqueue *vq)\n{\n\tstruct vhost_net_virtqueue *nvq =\n\t\tcontainer_of(vq, struct vhost_net_virtqueue, vq);\n\tint i, add;\n\tint j = 0;\n\n\tfor (i = nvq->done_idx; i != nvq->upend_idx; i = (i + 1) % UIO_MAXIOV) {\n\t\tif (vq->heads[i].len == VHOST_DMA_FAILED_LEN)\n\t\t\tvhost_net_tx_err(net);\n\t\tif (VHOST_DMA_IS_DONE(vq->heads[i].len)) {\n\t\t\tvq->heads[i].len = VHOST_DMA_CLEAR_LEN;\n\t\t\t++j;\n\t\t} else\n\t\t\tbreak;\n\t}\n\twhile (j) {\n\t\tadd = min(UIO_MAXIOV - nvq->done_idx, j);\n\t\tvhost_add_used_and_signal_n(vq->dev, vq,\n\t\t\t\t\t    &vq->heads[nvq->done_idx], add);\n\t\tnvq->done_idx = (nvq->done_idx + add) % UIO_MAXIOV;\n\t\tj -= add;\n\t}\n}\n\nstatic void vhost_zerocopy_callback(struct sk_buff *skb,\n\t\t\t\t    struct ubuf_info *ubuf_base, bool success)\n{\n\tstruct ubuf_info_msgzc *ubuf = uarg_to_msgzc(ubuf_base);\n\tstruct vhost_net_ubuf_ref *ubufs = ubuf->ctx;\n\tstruct vhost_virtqueue *vq = ubufs->vq;\n\tint cnt;\n\n\trcu_read_lock_bh();\n\n\t \n\tvq->heads[ubuf->desc].len = success ?\n\t\tVHOST_DMA_DONE_LEN : VHOST_DMA_FAILED_LEN;\n\tcnt = vhost_net_ubuf_put(ubufs);\n\n\t \n\tif (cnt <= 1 || !(cnt % 16))\n\t\tvhost_poll_queue(&vq->poll);\n\n\trcu_read_unlock_bh();\n}\n\nstatic inline unsigned long busy_clock(void)\n{\n\treturn local_clock() >> 10;\n}\n\nstatic bool vhost_can_busy_poll(unsigned long endtime)\n{\n\treturn likely(!need_resched() && !time_after(busy_clock(), endtime) &&\n\t\t      !signal_pending(current));\n}\n\nstatic void vhost_net_disable_vq(struct vhost_net *n,\n\t\t\t\t struct vhost_virtqueue *vq)\n{\n\tstruct vhost_net_virtqueue *nvq =\n\t\tcontainer_of(vq, struct vhost_net_virtqueue, vq);\n\tstruct vhost_poll *poll = n->poll + (nvq - n->vqs);\n\tif (!vhost_vq_get_backend(vq))\n\t\treturn;\n\tvhost_poll_stop(poll);\n}\n\nstatic int vhost_net_enable_vq(struct vhost_net *n,\n\t\t\t\tstruct vhost_virtqueue *vq)\n{\n\tstruct vhost_net_virtqueue *nvq =\n\t\tcontainer_of(vq, struct vhost_net_virtqueue, vq);\n\tstruct vhost_poll *poll = n->poll + (nvq - n->vqs);\n\tstruct socket *sock;\n\n\tsock = vhost_vq_get_backend(vq);\n\tif (!sock)\n\t\treturn 0;\n\n\treturn vhost_poll_start(poll, sock->file);\n}\n\nstatic void vhost_net_signal_used(struct vhost_net_virtqueue *nvq)\n{\n\tstruct vhost_virtqueue *vq = &nvq->vq;\n\tstruct vhost_dev *dev = vq->dev;\n\n\tif (!nvq->done_idx)\n\t\treturn;\n\n\tvhost_add_used_and_signal_n(dev, vq, vq->heads, nvq->done_idx);\n\tnvq->done_idx = 0;\n}\n\nstatic void vhost_tx_batch(struct vhost_net *net,\n\t\t\t   struct vhost_net_virtqueue *nvq,\n\t\t\t   struct socket *sock,\n\t\t\t   struct msghdr *msghdr)\n{\n\tstruct tun_msg_ctl ctl = {\n\t\t.type = TUN_MSG_PTR,\n\t\t.num = nvq->batched_xdp,\n\t\t.ptr = nvq->xdp,\n\t};\n\tint i, err;\n\n\tif (nvq->batched_xdp == 0)\n\t\tgoto signal_used;\n\n\tmsghdr->msg_control = &ctl;\n\tmsghdr->msg_controllen = sizeof(ctl);\n\terr = sock->ops->sendmsg(sock, msghdr, 0);\n\tif (unlikely(err < 0)) {\n\t\tvq_err(&nvq->vq, \"Fail to batch sending packets\\n\");\n\n\t\t \n\t\tfor (i = 0; i < nvq->batched_xdp; ++i)\n\t\t\tput_page(virt_to_head_page(nvq->xdp[i].data));\n\t\tnvq->batched_xdp = 0;\n\t\tnvq->done_idx = 0;\n\t\treturn;\n\t}\n\nsignal_used:\n\tvhost_net_signal_used(nvq);\n\tnvq->batched_xdp = 0;\n}\n\nstatic int sock_has_rx_data(struct socket *sock)\n{\n\tif (unlikely(!sock))\n\t\treturn 0;\n\n\tif (sock->ops->peek_len)\n\t\treturn sock->ops->peek_len(sock);\n\n\treturn skb_queue_empty(&sock->sk->sk_receive_queue);\n}\n\nstatic void vhost_net_busy_poll_try_queue(struct vhost_net *net,\n\t\t\t\t\t  struct vhost_virtqueue *vq)\n{\n\tif (!vhost_vq_avail_empty(&net->dev, vq)) {\n\t\tvhost_poll_queue(&vq->poll);\n\t} else if (unlikely(vhost_enable_notify(&net->dev, vq))) {\n\t\tvhost_disable_notify(&net->dev, vq);\n\t\tvhost_poll_queue(&vq->poll);\n\t}\n}\n\nstatic void vhost_net_busy_poll(struct vhost_net *net,\n\t\t\t\tstruct vhost_virtqueue *rvq,\n\t\t\t\tstruct vhost_virtqueue *tvq,\n\t\t\t\tbool *busyloop_intr,\n\t\t\t\tbool poll_rx)\n{\n\tunsigned long busyloop_timeout;\n\tunsigned long endtime;\n\tstruct socket *sock;\n\tstruct vhost_virtqueue *vq = poll_rx ? tvq : rvq;\n\n\t \n\tif (!mutex_trylock(&vq->mutex))\n\t\treturn;\n\n\tvhost_disable_notify(&net->dev, vq);\n\tsock = vhost_vq_get_backend(rvq);\n\n\tbusyloop_timeout = poll_rx ? rvq->busyloop_timeout:\n\t\t\t\t     tvq->busyloop_timeout;\n\n\tpreempt_disable();\n\tendtime = busy_clock() + busyloop_timeout;\n\n\twhile (vhost_can_busy_poll(endtime)) {\n\t\tif (vhost_vq_has_work(vq)) {\n\t\t\t*busyloop_intr = true;\n\t\t\tbreak;\n\t\t}\n\n\t\tif ((sock_has_rx_data(sock) &&\n\t\t     !vhost_vq_avail_empty(&net->dev, rvq)) ||\n\t\t    !vhost_vq_avail_empty(&net->dev, tvq))\n\t\t\tbreak;\n\n\t\tcpu_relax();\n\t}\n\n\tpreempt_enable();\n\n\tif (poll_rx || sock_has_rx_data(sock))\n\t\tvhost_net_busy_poll_try_queue(net, vq);\n\telse if (!poll_rx)  \n\t\tvhost_enable_notify(&net->dev, rvq);\n\n\tmutex_unlock(&vq->mutex);\n}\n\nstatic int vhost_net_tx_get_vq_desc(struct vhost_net *net,\n\t\t\t\t    struct vhost_net_virtqueue *tnvq,\n\t\t\t\t    unsigned int *out_num, unsigned int *in_num,\n\t\t\t\t    struct msghdr *msghdr, bool *busyloop_intr)\n{\n\tstruct vhost_net_virtqueue *rnvq = &net->vqs[VHOST_NET_VQ_RX];\n\tstruct vhost_virtqueue *rvq = &rnvq->vq;\n\tstruct vhost_virtqueue *tvq = &tnvq->vq;\n\n\tint r = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov),\n\t\t\t\t  out_num, in_num, NULL, NULL);\n\n\tif (r == tvq->num && tvq->busyloop_timeout) {\n\t\t \n\t\tif (!vhost_sock_zcopy(vhost_vq_get_backend(tvq)))\n\t\t\tvhost_tx_batch(net, tnvq,\n\t\t\t\t       vhost_vq_get_backend(tvq),\n\t\t\t\t       msghdr);\n\n\t\tvhost_net_busy_poll(net, rvq, tvq, busyloop_intr, false);\n\n\t\tr = vhost_get_vq_desc(tvq, tvq->iov, ARRAY_SIZE(tvq->iov),\n\t\t\t\t      out_num, in_num, NULL, NULL);\n\t}\n\n\treturn r;\n}\n\nstatic bool vhost_exceeds_maxpend(struct vhost_net *net)\n{\n\tstruct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];\n\tstruct vhost_virtqueue *vq = &nvq->vq;\n\n\treturn (nvq->upend_idx + UIO_MAXIOV - nvq->done_idx) % UIO_MAXIOV >\n\t       min_t(unsigned int, VHOST_MAX_PEND, vq->num >> 2);\n}\n\nstatic size_t init_iov_iter(struct vhost_virtqueue *vq, struct iov_iter *iter,\n\t\t\t    size_t hdr_size, int out)\n{\n\t \n\tsize_t len = iov_length(vq->iov, out);\n\n\tiov_iter_init(iter, ITER_SOURCE, vq->iov, out, len);\n\tiov_iter_advance(iter, hdr_size);\n\n\treturn iov_iter_count(iter);\n}\n\nstatic int get_tx_bufs(struct vhost_net *net,\n\t\t       struct vhost_net_virtqueue *nvq,\n\t\t       struct msghdr *msg,\n\t\t       unsigned int *out, unsigned int *in,\n\t\t       size_t *len, bool *busyloop_intr)\n{\n\tstruct vhost_virtqueue *vq = &nvq->vq;\n\tint ret;\n\n\tret = vhost_net_tx_get_vq_desc(net, nvq, out, in, msg, busyloop_intr);\n\n\tif (ret < 0 || ret == vq->num)\n\t\treturn ret;\n\n\tif (*in) {\n\t\tvq_err(vq, \"Unexpected descriptor format for TX: out %d, int %d\\n\",\n\t\t\t*out, *in);\n\t\treturn -EFAULT;\n\t}\n\n\t \n\t*len = init_iov_iter(vq, &msg->msg_iter, nvq->vhost_hlen, *out);\n\tif (*len == 0) {\n\t\tvq_err(vq, \"Unexpected header len for TX: %zd expected %zd\\n\",\n\t\t\t*len, nvq->vhost_hlen);\n\t\treturn -EFAULT;\n\t}\n\n\treturn ret;\n}\n\nstatic bool tx_can_batch(struct vhost_virtqueue *vq, size_t total_len)\n{\n\treturn total_len < VHOST_NET_WEIGHT &&\n\t       !vhost_vq_avail_empty(vq->dev, vq);\n}\n\nstatic bool vhost_net_page_frag_refill(struct vhost_net *net, unsigned int sz,\n\t\t\t\t       struct page_frag *pfrag, gfp_t gfp)\n{\n\tif (pfrag->page) {\n\t\tif (pfrag->offset + sz <= pfrag->size)\n\t\t\treturn true;\n\t\t__page_frag_cache_drain(pfrag->page, net->refcnt_bias);\n\t}\n\n\tpfrag->offset = 0;\n\tnet->refcnt_bias = 0;\n\tif (SKB_FRAG_PAGE_ORDER) {\n\t\t \n\t\tpfrag->page = alloc_pages((gfp & ~__GFP_DIRECT_RECLAIM) |\n\t\t\t\t\t  __GFP_COMP | __GFP_NOWARN |\n\t\t\t\t\t  __GFP_NORETRY,\n\t\t\t\t\t  SKB_FRAG_PAGE_ORDER);\n\t\tif (likely(pfrag->page)) {\n\t\t\tpfrag->size = PAGE_SIZE << SKB_FRAG_PAGE_ORDER;\n\t\t\tgoto done;\n\t\t}\n\t}\n\tpfrag->page = alloc_page(gfp);\n\tif (likely(pfrag->page)) {\n\t\tpfrag->size = PAGE_SIZE;\n\t\tgoto done;\n\t}\n\treturn false;\n\ndone:\n\tnet->refcnt_bias = USHRT_MAX;\n\tpage_ref_add(pfrag->page, USHRT_MAX - 1);\n\treturn true;\n}\n\n#define VHOST_NET_RX_PAD (NET_IP_ALIGN + NET_SKB_PAD)\n\nstatic int vhost_net_build_xdp(struct vhost_net_virtqueue *nvq,\n\t\t\t       struct iov_iter *from)\n{\n\tstruct vhost_virtqueue *vq = &nvq->vq;\n\tstruct vhost_net *net = container_of(vq->dev, struct vhost_net,\n\t\t\t\t\t     dev);\n\tstruct socket *sock = vhost_vq_get_backend(vq);\n\tstruct page_frag *alloc_frag = &net->page_frag;\n\tstruct virtio_net_hdr *gso;\n\tstruct xdp_buff *xdp = &nvq->xdp[nvq->batched_xdp];\n\tstruct tun_xdp_hdr *hdr;\n\tsize_t len = iov_iter_count(from);\n\tint headroom = vhost_sock_xdp(sock) ? XDP_PACKET_HEADROOM : 0;\n\tint buflen = SKB_DATA_ALIGN(sizeof(struct skb_shared_info));\n\tint pad = SKB_DATA_ALIGN(VHOST_NET_RX_PAD + headroom + nvq->sock_hlen);\n\tint sock_hlen = nvq->sock_hlen;\n\tvoid *buf;\n\tint copied;\n\n\tif (unlikely(len < nvq->sock_hlen))\n\t\treturn -EFAULT;\n\n\tif (SKB_DATA_ALIGN(len + pad) +\n\t    SKB_DATA_ALIGN(sizeof(struct skb_shared_info)) > PAGE_SIZE)\n\t\treturn -ENOSPC;\n\n\tbuflen += SKB_DATA_ALIGN(len + pad);\n\talloc_frag->offset = ALIGN((u64)alloc_frag->offset, SMP_CACHE_BYTES);\n\tif (unlikely(!vhost_net_page_frag_refill(net, buflen,\n\t\t\t\t\t\t alloc_frag, GFP_KERNEL)))\n\t\treturn -ENOMEM;\n\n\tbuf = (char *)page_address(alloc_frag->page) + alloc_frag->offset;\n\tcopied = copy_page_from_iter(alloc_frag->page,\n\t\t\t\t     alloc_frag->offset +\n\t\t\t\t     offsetof(struct tun_xdp_hdr, gso),\n\t\t\t\t     sock_hlen, from);\n\tif (copied != sock_hlen)\n\t\treturn -EFAULT;\n\n\thdr = buf;\n\tgso = &hdr->gso;\n\n\tif ((gso->flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) &&\n\t    vhost16_to_cpu(vq, gso->csum_start) +\n\t    vhost16_to_cpu(vq, gso->csum_offset) + 2 >\n\t    vhost16_to_cpu(vq, gso->hdr_len)) {\n\t\tgso->hdr_len = cpu_to_vhost16(vq,\n\t\t\t       vhost16_to_cpu(vq, gso->csum_start) +\n\t\t\t       vhost16_to_cpu(vq, gso->csum_offset) + 2);\n\n\t\tif (vhost16_to_cpu(vq, gso->hdr_len) > len)\n\t\t\treturn -EINVAL;\n\t}\n\n\tlen -= sock_hlen;\n\tcopied = copy_page_from_iter(alloc_frag->page,\n\t\t\t\t     alloc_frag->offset + pad,\n\t\t\t\t     len, from);\n\tif (copied != len)\n\t\treturn -EFAULT;\n\n\txdp_init_buff(xdp, buflen, NULL);\n\txdp_prepare_buff(xdp, buf, pad, len, true);\n\thdr->buflen = buflen;\n\n\t--net->refcnt_bias;\n\talloc_frag->offset += buflen;\n\n\t++nvq->batched_xdp;\n\n\treturn 0;\n}\n\nstatic void handle_tx_copy(struct vhost_net *net, struct socket *sock)\n{\n\tstruct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];\n\tstruct vhost_virtqueue *vq = &nvq->vq;\n\tunsigned out, in;\n\tint head;\n\tstruct msghdr msg = {\n\t\t.msg_name = NULL,\n\t\t.msg_namelen = 0,\n\t\t.msg_control = NULL,\n\t\t.msg_controllen = 0,\n\t\t.msg_flags = MSG_DONTWAIT,\n\t};\n\tsize_t len, total_len = 0;\n\tint err;\n\tint sent_pkts = 0;\n\tbool sock_can_batch = (sock->sk->sk_sndbuf == INT_MAX);\n\n\tdo {\n\t\tbool busyloop_intr = false;\n\n\t\tif (nvq->done_idx == VHOST_NET_BATCH)\n\t\t\tvhost_tx_batch(net, nvq, sock, &msg);\n\n\t\thead = get_tx_bufs(net, nvq, &msg, &out, &in, &len,\n\t\t\t\t   &busyloop_intr);\n\t\t \n\t\tif (unlikely(head < 0))\n\t\t\tbreak;\n\t\t \n\t\tif (head == vq->num) {\n\t\t\tif (unlikely(busyloop_intr)) {\n\t\t\t\tvhost_poll_queue(&vq->poll);\n\t\t\t} else if (unlikely(vhost_enable_notify(&net->dev,\n\t\t\t\t\t\t\t\tvq))) {\n\t\t\t\tvhost_disable_notify(&net->dev, vq);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\ttotal_len += len;\n\n\t\t \n\t\tif (sock_can_batch) {\n\t\t\terr = vhost_net_build_xdp(nvq, &msg.msg_iter);\n\t\t\tif (!err) {\n\t\t\t\tgoto done;\n\t\t\t} else if (unlikely(err != -ENOSPC)) {\n\t\t\t\tvhost_tx_batch(net, nvq, sock, &msg);\n\t\t\t\tvhost_discard_vq_desc(vq, 1);\n\t\t\t\tvhost_net_enable_vq(net, vq);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tvhost_tx_batch(net, nvq, sock, &msg);\n\t\t\tmsg.msg_control = NULL;\n\t\t} else {\n\t\t\tif (tx_can_batch(vq, total_len))\n\t\t\t\tmsg.msg_flags |= MSG_MORE;\n\t\t\telse\n\t\t\t\tmsg.msg_flags &= ~MSG_MORE;\n\t\t}\n\n\t\terr = sock->ops->sendmsg(sock, &msg, len);\n\t\tif (unlikely(err < 0)) {\n\t\t\tif (err == -EAGAIN || err == -ENOMEM || err == -ENOBUFS) {\n\t\t\t\tvhost_discard_vq_desc(vq, 1);\n\t\t\t\tvhost_net_enable_vq(net, vq);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpr_debug(\"Fail to send packet: err %d\", err);\n\t\t} else if (unlikely(err != len))\n\t\t\tpr_debug(\"Truncated TX packet: len %d != %zd\\n\",\n\t\t\t\t err, len);\ndone:\n\t\tvq->heads[nvq->done_idx].id = cpu_to_vhost32(vq, head);\n\t\tvq->heads[nvq->done_idx].len = 0;\n\t\t++nvq->done_idx;\n\t} while (likely(!vhost_exceeds_weight(vq, ++sent_pkts, total_len)));\n\n\tvhost_tx_batch(net, nvq, sock, &msg);\n}\n\nstatic void handle_tx_zerocopy(struct vhost_net *net, struct socket *sock)\n{\n\tstruct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];\n\tstruct vhost_virtqueue *vq = &nvq->vq;\n\tunsigned out, in;\n\tint head;\n\tstruct msghdr msg = {\n\t\t.msg_name = NULL,\n\t\t.msg_namelen = 0,\n\t\t.msg_control = NULL,\n\t\t.msg_controllen = 0,\n\t\t.msg_flags = MSG_DONTWAIT,\n\t};\n\tstruct tun_msg_ctl ctl;\n\tsize_t len, total_len = 0;\n\tint err;\n\tstruct vhost_net_ubuf_ref *ubufs;\n\tstruct ubuf_info_msgzc *ubuf;\n\tbool zcopy_used;\n\tint sent_pkts = 0;\n\n\tdo {\n\t\tbool busyloop_intr;\n\n\t\t \n\t\tvhost_zerocopy_signal_used(net, vq);\n\n\t\tbusyloop_intr = false;\n\t\thead = get_tx_bufs(net, nvq, &msg, &out, &in, &len,\n\t\t\t\t   &busyloop_intr);\n\t\t \n\t\tif (unlikely(head < 0))\n\t\t\tbreak;\n\t\t \n\t\tif (head == vq->num) {\n\t\t\tif (unlikely(busyloop_intr)) {\n\t\t\t\tvhost_poll_queue(&vq->poll);\n\t\t\t} else if (unlikely(vhost_enable_notify(&net->dev, vq))) {\n\t\t\t\tvhost_disable_notify(&net->dev, vq);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tzcopy_used = len >= VHOST_GOODCOPY_LEN\n\t\t\t     && !vhost_exceeds_maxpend(net)\n\t\t\t     && vhost_net_tx_select_zcopy(net);\n\n\t\t \n\t\tif (zcopy_used) {\n\t\t\tubuf = nvq->ubuf_info + nvq->upend_idx;\n\t\t\tvq->heads[nvq->upend_idx].id = cpu_to_vhost32(vq, head);\n\t\t\tvq->heads[nvq->upend_idx].len = VHOST_DMA_IN_PROGRESS;\n\t\t\tubuf->ctx = nvq->ubufs;\n\t\t\tubuf->desc = nvq->upend_idx;\n\t\t\tubuf->ubuf.callback = vhost_zerocopy_callback;\n\t\t\tubuf->ubuf.flags = SKBFL_ZEROCOPY_FRAG;\n\t\t\trefcount_set(&ubuf->ubuf.refcnt, 1);\n\t\t\tmsg.msg_control = &ctl;\n\t\t\tctl.type = TUN_MSG_UBUF;\n\t\t\tctl.ptr = &ubuf->ubuf;\n\t\t\tmsg.msg_controllen = sizeof(ctl);\n\t\t\tubufs = nvq->ubufs;\n\t\t\tatomic_inc(&ubufs->refcount);\n\t\t\tnvq->upend_idx = (nvq->upend_idx + 1) % UIO_MAXIOV;\n\t\t} else {\n\t\t\tmsg.msg_control = NULL;\n\t\t\tubufs = NULL;\n\t\t}\n\t\ttotal_len += len;\n\t\tif (tx_can_batch(vq, total_len) &&\n\t\t    likely(!vhost_exceeds_maxpend(net))) {\n\t\t\tmsg.msg_flags |= MSG_MORE;\n\t\t} else {\n\t\t\tmsg.msg_flags &= ~MSG_MORE;\n\t\t}\n\n\t\terr = sock->ops->sendmsg(sock, &msg, len);\n\t\tif (unlikely(err < 0)) {\n\t\t\tbool retry = err == -EAGAIN || err == -ENOMEM || err == -ENOBUFS;\n\n\t\t\tif (zcopy_used) {\n\t\t\t\tif (vq->heads[ubuf->desc].len == VHOST_DMA_IN_PROGRESS)\n\t\t\t\t\tvhost_net_ubuf_put(ubufs);\n\t\t\t\tif (retry)\n\t\t\t\t\tnvq->upend_idx = ((unsigned)nvq->upend_idx - 1)\n\t\t\t\t\t\t% UIO_MAXIOV;\n\t\t\t\telse\n\t\t\t\t\tvq->heads[ubuf->desc].len = VHOST_DMA_DONE_LEN;\n\t\t\t}\n\t\t\tif (retry) {\n\t\t\t\tvhost_discard_vq_desc(vq, 1);\n\t\t\t\tvhost_net_enable_vq(net, vq);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tpr_debug(\"Fail to send packet: err %d\", err);\n\t\t} else if (unlikely(err != len))\n\t\t\tpr_debug(\"Truncated TX packet: \"\n\t\t\t\t \" len %d != %zd\\n\", err, len);\n\t\tif (!zcopy_used)\n\t\t\tvhost_add_used_and_signal(&net->dev, vq, head, 0);\n\t\telse\n\t\t\tvhost_zerocopy_signal_used(net, vq);\n\t\tvhost_net_tx_packet(net);\n\t} while (likely(!vhost_exceeds_weight(vq, ++sent_pkts, total_len)));\n}\n\n \nstatic void handle_tx(struct vhost_net *net)\n{\n\tstruct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_TX];\n\tstruct vhost_virtqueue *vq = &nvq->vq;\n\tstruct socket *sock;\n\n\tmutex_lock_nested(&vq->mutex, VHOST_NET_VQ_TX);\n\tsock = vhost_vq_get_backend(vq);\n\tif (!sock)\n\t\tgoto out;\n\n\tif (!vq_meta_prefetch(vq))\n\t\tgoto out;\n\n\tvhost_disable_notify(&net->dev, vq);\n\tvhost_net_disable_vq(net, vq);\n\n\tif (vhost_sock_zcopy(sock))\n\t\thandle_tx_zerocopy(net, sock);\n\telse\n\t\thandle_tx_copy(net, sock);\n\nout:\n\tmutex_unlock(&vq->mutex);\n}\n\nstatic int peek_head_len(struct vhost_net_virtqueue *rvq, struct sock *sk)\n{\n\tstruct sk_buff *head;\n\tint len = 0;\n\tunsigned long flags;\n\n\tif (rvq->rx_ring)\n\t\treturn vhost_net_buf_peek(rvq);\n\n\tspin_lock_irqsave(&sk->sk_receive_queue.lock, flags);\n\thead = skb_peek(&sk->sk_receive_queue);\n\tif (likely(head)) {\n\t\tlen = head->len;\n\t\tif (skb_vlan_tag_present(head))\n\t\t\tlen += VLAN_HLEN;\n\t}\n\n\tspin_unlock_irqrestore(&sk->sk_receive_queue.lock, flags);\n\treturn len;\n}\n\nstatic int vhost_net_rx_peek_head_len(struct vhost_net *net, struct sock *sk,\n\t\t\t\t      bool *busyloop_intr)\n{\n\tstruct vhost_net_virtqueue *rnvq = &net->vqs[VHOST_NET_VQ_RX];\n\tstruct vhost_net_virtqueue *tnvq = &net->vqs[VHOST_NET_VQ_TX];\n\tstruct vhost_virtqueue *rvq = &rnvq->vq;\n\tstruct vhost_virtqueue *tvq = &tnvq->vq;\n\tint len = peek_head_len(rnvq, sk);\n\n\tif (!len && rvq->busyloop_timeout) {\n\t\t \n\t\tvhost_net_signal_used(rnvq);\n\t\t \n\t\tvhost_net_busy_poll(net, rvq, tvq, busyloop_intr, true);\n\n\t\tlen = peek_head_len(rnvq, sk);\n\t}\n\n\treturn len;\n}\n\n \nstatic int get_rx_bufs(struct vhost_virtqueue *vq,\n\t\t       struct vring_used_elem *heads,\n\t\t       int datalen,\n\t\t       unsigned *iovcount,\n\t\t       struct vhost_log *log,\n\t\t       unsigned *log_num,\n\t\t       unsigned int quota)\n{\n\tunsigned int out, in;\n\tint seg = 0;\n\tint headcount = 0;\n\tunsigned d;\n\tint r, nlogs = 0;\n\t \n\tu32 len;\n\n\twhile (datalen > 0 && headcount < quota) {\n\t\tif (unlikely(seg >= UIO_MAXIOV)) {\n\t\t\tr = -ENOBUFS;\n\t\t\tgoto err;\n\t\t}\n\t\tr = vhost_get_vq_desc(vq, vq->iov + seg,\n\t\t\t\t      ARRAY_SIZE(vq->iov) - seg, &out,\n\t\t\t\t      &in, log, log_num);\n\t\tif (unlikely(r < 0))\n\t\t\tgoto err;\n\n\t\td = r;\n\t\tif (d == vq->num) {\n\t\t\tr = 0;\n\t\t\tgoto err;\n\t\t}\n\t\tif (unlikely(out || in <= 0)) {\n\t\t\tvq_err(vq, \"unexpected descriptor format for RX: \"\n\t\t\t\t\"out %d, in %d\\n\", out, in);\n\t\t\tr = -EINVAL;\n\t\t\tgoto err;\n\t\t}\n\t\tif (unlikely(log)) {\n\t\t\tnlogs += *log_num;\n\t\t\tlog += *log_num;\n\t\t}\n\t\theads[headcount].id = cpu_to_vhost32(vq, d);\n\t\tlen = iov_length(vq->iov + seg, in);\n\t\theads[headcount].len = cpu_to_vhost32(vq, len);\n\t\tdatalen -= len;\n\t\t++headcount;\n\t\tseg += in;\n\t}\n\theads[headcount - 1].len = cpu_to_vhost32(vq, len + datalen);\n\t*iovcount = seg;\n\tif (unlikely(log))\n\t\t*log_num = nlogs;\n\n\t \n\tif (unlikely(datalen > 0)) {\n\t\tr = UIO_MAXIOV + 1;\n\t\tgoto err;\n\t}\n\treturn headcount;\nerr:\n\tvhost_discard_vq_desc(vq, headcount);\n\treturn r;\n}\n\n \nstatic void handle_rx(struct vhost_net *net)\n{\n\tstruct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_RX];\n\tstruct vhost_virtqueue *vq = &nvq->vq;\n\tunsigned in, log;\n\tstruct vhost_log *vq_log;\n\tstruct msghdr msg = {\n\t\t.msg_name = NULL,\n\t\t.msg_namelen = 0,\n\t\t.msg_control = NULL,  \n\t\t.msg_controllen = 0,\n\t\t.msg_flags = MSG_DONTWAIT,\n\t};\n\tstruct virtio_net_hdr hdr = {\n\t\t.flags = 0,\n\t\t.gso_type = VIRTIO_NET_HDR_GSO_NONE\n\t};\n\tsize_t total_len = 0;\n\tint err, mergeable;\n\ts16 headcount;\n\tsize_t vhost_hlen, sock_hlen;\n\tsize_t vhost_len, sock_len;\n\tbool busyloop_intr = false;\n\tstruct socket *sock;\n\tstruct iov_iter fixup;\n\t__virtio16 num_buffers;\n\tint recv_pkts = 0;\n\n\tmutex_lock_nested(&vq->mutex, VHOST_NET_VQ_RX);\n\tsock = vhost_vq_get_backend(vq);\n\tif (!sock)\n\t\tgoto out;\n\n\tif (!vq_meta_prefetch(vq))\n\t\tgoto out;\n\n\tvhost_disable_notify(&net->dev, vq);\n\tvhost_net_disable_vq(net, vq);\n\n\tvhost_hlen = nvq->vhost_hlen;\n\tsock_hlen = nvq->sock_hlen;\n\n\tvq_log = unlikely(vhost_has_feature(vq, VHOST_F_LOG_ALL)) ?\n\t\tvq->log : NULL;\n\tmergeable = vhost_has_feature(vq, VIRTIO_NET_F_MRG_RXBUF);\n\n\tdo {\n\t\tsock_len = vhost_net_rx_peek_head_len(net, sock->sk,\n\t\t\t\t\t\t      &busyloop_intr);\n\t\tif (!sock_len)\n\t\t\tbreak;\n\t\tsock_len += sock_hlen;\n\t\tvhost_len = sock_len + vhost_hlen;\n\t\theadcount = get_rx_bufs(vq, vq->heads + nvq->done_idx,\n\t\t\t\t\tvhost_len, &in, vq_log, &log,\n\t\t\t\t\tlikely(mergeable) ? UIO_MAXIOV : 1);\n\t\t \n\t\tif (unlikely(headcount < 0))\n\t\t\tgoto out;\n\t\t \n\t\tif (!headcount) {\n\t\t\tif (unlikely(busyloop_intr)) {\n\t\t\t\tvhost_poll_queue(&vq->poll);\n\t\t\t} else if (unlikely(vhost_enable_notify(&net->dev, vq))) {\n\t\t\t\t \n\t\t\t\tvhost_disable_notify(&net->dev, vq);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t \n\t\t\tgoto out;\n\t\t}\n\t\tbusyloop_intr = false;\n\t\tif (nvq->rx_ring)\n\t\t\tmsg.msg_control = vhost_net_buf_consume(&nvq->rxq);\n\t\t \n\t\tif (unlikely(headcount > UIO_MAXIOV)) {\n\t\t\tiov_iter_init(&msg.msg_iter, ITER_DEST, vq->iov, 1, 1);\n\t\t\terr = sock->ops->recvmsg(sock, &msg,\n\t\t\t\t\t\t 1, MSG_DONTWAIT | MSG_TRUNC);\n\t\t\tpr_debug(\"Discarded rx packet: len %zd\\n\", sock_len);\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tiov_iter_init(&msg.msg_iter, ITER_DEST, vq->iov, in, vhost_len);\n\t\tfixup = msg.msg_iter;\n\t\tif (unlikely((vhost_hlen))) {\n\t\t\t \n\t\t\tiov_iter_advance(&msg.msg_iter, vhost_hlen);\n\t\t}\n\t\terr = sock->ops->recvmsg(sock, &msg,\n\t\t\t\t\t sock_len, MSG_DONTWAIT | MSG_TRUNC);\n\t\t \n\t\tif (unlikely(err != sock_len)) {\n\t\t\tpr_debug(\"Discarded rx packet: \"\n\t\t\t\t \" len %d, expected %zd\\n\", err, sock_len);\n\t\t\tvhost_discard_vq_desc(vq, headcount);\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tif (unlikely(vhost_hlen)) {\n\t\t\tif (copy_to_iter(&hdr, sizeof(hdr),\n\t\t\t\t\t &fixup) != sizeof(hdr)) {\n\t\t\t\tvq_err(vq, \"Unable to write vnet_hdr \"\n\t\t\t\t       \"at addr %p\\n\", vq->iov->iov_base);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tiov_iter_advance(&fixup, sizeof(hdr));\n\t\t}\n\t\t \n\n\t\tnum_buffers = cpu_to_vhost16(vq, headcount);\n\t\tif (likely(mergeable) &&\n\t\t    copy_to_iter(&num_buffers, sizeof num_buffers,\n\t\t\t\t &fixup) != sizeof num_buffers) {\n\t\t\tvq_err(vq, \"Failed num_buffers write\");\n\t\t\tvhost_discard_vq_desc(vq, headcount);\n\t\t\tgoto out;\n\t\t}\n\t\tnvq->done_idx += headcount;\n\t\tif (nvq->done_idx > VHOST_NET_BATCH)\n\t\t\tvhost_net_signal_used(nvq);\n\t\tif (unlikely(vq_log))\n\t\t\tvhost_log_write(vq, vq_log, log, vhost_len,\n\t\t\t\t\tvq->iov, in);\n\t\ttotal_len += vhost_len;\n\t} while (likely(!vhost_exceeds_weight(vq, ++recv_pkts, total_len)));\n\n\tif (unlikely(busyloop_intr))\n\t\tvhost_poll_queue(&vq->poll);\n\telse if (!sock_len)\n\t\tvhost_net_enable_vq(net, vq);\nout:\n\tvhost_net_signal_used(nvq);\n\tmutex_unlock(&vq->mutex);\n}\n\nstatic void handle_tx_kick(struct vhost_work *work)\n{\n\tstruct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,\n\t\t\t\t\t\t  poll.work);\n\tstruct vhost_net *net = container_of(vq->dev, struct vhost_net, dev);\n\n\thandle_tx(net);\n}\n\nstatic void handle_rx_kick(struct vhost_work *work)\n{\n\tstruct vhost_virtqueue *vq = container_of(work, struct vhost_virtqueue,\n\t\t\t\t\t\t  poll.work);\n\tstruct vhost_net *net = container_of(vq->dev, struct vhost_net, dev);\n\n\thandle_rx(net);\n}\n\nstatic void handle_tx_net(struct vhost_work *work)\n{\n\tstruct vhost_net *net = container_of(work, struct vhost_net,\n\t\t\t\t\t     poll[VHOST_NET_VQ_TX].work);\n\thandle_tx(net);\n}\n\nstatic void handle_rx_net(struct vhost_work *work)\n{\n\tstruct vhost_net *net = container_of(work, struct vhost_net,\n\t\t\t\t\t     poll[VHOST_NET_VQ_RX].work);\n\thandle_rx(net);\n}\n\nstatic int vhost_net_open(struct inode *inode, struct file *f)\n{\n\tstruct vhost_net *n;\n\tstruct vhost_dev *dev;\n\tstruct vhost_virtqueue **vqs;\n\tvoid **queue;\n\tstruct xdp_buff *xdp;\n\tint i;\n\n\tn = kvmalloc(sizeof *n, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!n)\n\t\treturn -ENOMEM;\n\tvqs = kmalloc_array(VHOST_NET_VQ_MAX, sizeof(*vqs), GFP_KERNEL);\n\tif (!vqs) {\n\t\tkvfree(n);\n\t\treturn -ENOMEM;\n\t}\n\n\tqueue = kmalloc_array(VHOST_NET_BATCH, sizeof(void *),\n\t\t\t      GFP_KERNEL);\n\tif (!queue) {\n\t\tkfree(vqs);\n\t\tkvfree(n);\n\t\treturn -ENOMEM;\n\t}\n\tn->vqs[VHOST_NET_VQ_RX].rxq.queue = queue;\n\n\txdp = kmalloc_array(VHOST_NET_BATCH, sizeof(*xdp), GFP_KERNEL);\n\tif (!xdp) {\n\t\tkfree(vqs);\n\t\tkvfree(n);\n\t\tkfree(queue);\n\t\treturn -ENOMEM;\n\t}\n\tn->vqs[VHOST_NET_VQ_TX].xdp = xdp;\n\n\tdev = &n->dev;\n\tvqs[VHOST_NET_VQ_TX] = &n->vqs[VHOST_NET_VQ_TX].vq;\n\tvqs[VHOST_NET_VQ_RX] = &n->vqs[VHOST_NET_VQ_RX].vq;\n\tn->vqs[VHOST_NET_VQ_TX].vq.handle_kick = handle_tx_kick;\n\tn->vqs[VHOST_NET_VQ_RX].vq.handle_kick = handle_rx_kick;\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; i++) {\n\t\tn->vqs[i].ubufs = NULL;\n\t\tn->vqs[i].ubuf_info = NULL;\n\t\tn->vqs[i].upend_idx = 0;\n\t\tn->vqs[i].done_idx = 0;\n\t\tn->vqs[i].batched_xdp = 0;\n\t\tn->vqs[i].vhost_hlen = 0;\n\t\tn->vqs[i].sock_hlen = 0;\n\t\tn->vqs[i].rx_ring = NULL;\n\t\tvhost_net_buf_init(&n->vqs[i].rxq);\n\t}\n\tvhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX,\n\t\t       UIO_MAXIOV + VHOST_NET_BATCH,\n\t\t       VHOST_NET_PKT_WEIGHT, VHOST_NET_WEIGHT, true,\n\t\t       NULL);\n\n\tvhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, EPOLLOUT, dev,\n\t\t\tvqs[VHOST_NET_VQ_TX]);\n\tvhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, EPOLLIN, dev,\n\t\t\tvqs[VHOST_NET_VQ_RX]);\n\n\tf->private_data = n;\n\tn->page_frag.page = NULL;\n\tn->refcnt_bias = 0;\n\n\treturn 0;\n}\n\nstatic struct socket *vhost_net_stop_vq(struct vhost_net *n,\n\t\t\t\t\tstruct vhost_virtqueue *vq)\n{\n\tstruct socket *sock;\n\tstruct vhost_net_virtqueue *nvq =\n\t\tcontainer_of(vq, struct vhost_net_virtqueue, vq);\n\n\tmutex_lock(&vq->mutex);\n\tsock = vhost_vq_get_backend(vq);\n\tvhost_net_disable_vq(n, vq);\n\tvhost_vq_set_backend(vq, NULL);\n\tvhost_net_buf_unproduce(nvq);\n\tnvq->rx_ring = NULL;\n\tmutex_unlock(&vq->mutex);\n\treturn sock;\n}\n\nstatic void vhost_net_stop(struct vhost_net *n, struct socket **tx_sock,\n\t\t\t   struct socket **rx_sock)\n{\n\t*tx_sock = vhost_net_stop_vq(n, &n->vqs[VHOST_NET_VQ_TX].vq);\n\t*rx_sock = vhost_net_stop_vq(n, &n->vqs[VHOST_NET_VQ_RX].vq);\n}\n\nstatic void vhost_net_flush(struct vhost_net *n)\n{\n\tvhost_dev_flush(&n->dev);\n\tif (n->vqs[VHOST_NET_VQ_TX].ubufs) {\n\t\tmutex_lock(&n->vqs[VHOST_NET_VQ_TX].vq.mutex);\n\t\tn->tx_flush = true;\n\t\tmutex_unlock(&n->vqs[VHOST_NET_VQ_TX].vq.mutex);\n\t\t \n\t\tvhost_net_ubuf_put_and_wait(n->vqs[VHOST_NET_VQ_TX].ubufs);\n\t\tmutex_lock(&n->vqs[VHOST_NET_VQ_TX].vq.mutex);\n\t\tn->tx_flush = false;\n\t\tatomic_set(&n->vqs[VHOST_NET_VQ_TX].ubufs->refcount, 1);\n\t\tmutex_unlock(&n->vqs[VHOST_NET_VQ_TX].vq.mutex);\n\t}\n}\n\nstatic int vhost_net_release(struct inode *inode, struct file *f)\n{\n\tstruct vhost_net *n = f->private_data;\n\tstruct socket *tx_sock;\n\tstruct socket *rx_sock;\n\n\tvhost_net_stop(n, &tx_sock, &rx_sock);\n\tvhost_net_flush(n);\n\tvhost_dev_stop(&n->dev);\n\tvhost_dev_cleanup(&n->dev);\n\tvhost_net_vq_reset(n);\n\tif (tx_sock)\n\t\tsockfd_put(tx_sock);\n\tif (rx_sock)\n\t\tsockfd_put(rx_sock);\n\t \n\tsynchronize_rcu();\n\t \n\tvhost_net_flush(n);\n\tkfree(n->vqs[VHOST_NET_VQ_RX].rxq.queue);\n\tkfree(n->vqs[VHOST_NET_VQ_TX].xdp);\n\tkfree(n->dev.vqs);\n\tif (n->page_frag.page)\n\t\t__page_frag_cache_drain(n->page_frag.page, n->refcnt_bias);\n\tkvfree(n);\n\treturn 0;\n}\n\nstatic struct socket *get_raw_socket(int fd)\n{\n\tint r;\n\tstruct socket *sock = sockfd_lookup(fd, &r);\n\n\tif (!sock)\n\t\treturn ERR_PTR(-ENOTSOCK);\n\n\t \n\tif (sock->sk->sk_type != SOCK_RAW) {\n\t\tr = -ESOCKTNOSUPPORT;\n\t\tgoto err;\n\t}\n\n\tif (sock->sk->sk_family != AF_PACKET) {\n\t\tr = -EPFNOSUPPORT;\n\t\tgoto err;\n\t}\n\treturn sock;\nerr:\n\tsockfd_put(sock);\n\treturn ERR_PTR(r);\n}\n\nstatic struct ptr_ring *get_tap_ptr_ring(struct file *file)\n{\n\tstruct ptr_ring *ring;\n\tring = tun_get_tx_ring(file);\n\tif (!IS_ERR(ring))\n\t\tgoto out;\n\tring = tap_get_ptr_ring(file);\n\tif (!IS_ERR(ring))\n\t\tgoto out;\n\tring = NULL;\nout:\n\treturn ring;\n}\n\nstatic struct socket *get_tap_socket(int fd)\n{\n\tstruct file *file = fget(fd);\n\tstruct socket *sock;\n\n\tif (!file)\n\t\treturn ERR_PTR(-EBADF);\n\tsock = tun_get_socket(file);\n\tif (!IS_ERR(sock))\n\t\treturn sock;\n\tsock = tap_get_socket(file);\n\tif (IS_ERR(sock))\n\t\tfput(file);\n\treturn sock;\n}\n\nstatic struct socket *get_socket(int fd)\n{\n\tstruct socket *sock;\n\n\t \n\tif (fd == -1)\n\t\treturn NULL;\n\tsock = get_raw_socket(fd);\n\tif (!IS_ERR(sock))\n\t\treturn sock;\n\tsock = get_tap_socket(fd);\n\tif (!IS_ERR(sock))\n\t\treturn sock;\n\treturn ERR_PTR(-ENOTSOCK);\n}\n\nstatic long vhost_net_set_backend(struct vhost_net *n, unsigned index, int fd)\n{\n\tstruct socket *sock, *oldsock;\n\tstruct vhost_virtqueue *vq;\n\tstruct vhost_net_virtqueue *nvq;\n\tstruct vhost_net_ubuf_ref *ubufs, *oldubufs = NULL;\n\tint r;\n\n\tmutex_lock(&n->dev.mutex);\n\tr = vhost_dev_check_owner(&n->dev);\n\tif (r)\n\t\tgoto err;\n\n\tif (index >= VHOST_NET_VQ_MAX) {\n\t\tr = -ENOBUFS;\n\t\tgoto err;\n\t}\n\tvq = &n->vqs[index].vq;\n\tnvq = &n->vqs[index];\n\tmutex_lock(&vq->mutex);\n\n\tif (fd == -1)\n\t\tvhost_clear_msg(&n->dev);\n\n\t \n\tif (!vhost_vq_access_ok(vq)) {\n\t\tr = -EFAULT;\n\t\tgoto err_vq;\n\t}\n\tsock = get_socket(fd);\n\tif (IS_ERR(sock)) {\n\t\tr = PTR_ERR(sock);\n\t\tgoto err_vq;\n\t}\n\n\t \n\toldsock = vhost_vq_get_backend(vq);\n\tif (sock != oldsock) {\n\t\tubufs = vhost_net_ubuf_alloc(vq,\n\t\t\t\t\t     sock && vhost_sock_zcopy(sock));\n\t\tif (IS_ERR(ubufs)) {\n\t\t\tr = PTR_ERR(ubufs);\n\t\t\tgoto err_ubufs;\n\t\t}\n\n\t\tvhost_net_disable_vq(n, vq);\n\t\tvhost_vq_set_backend(vq, sock);\n\t\tvhost_net_buf_unproduce(nvq);\n\t\tr = vhost_vq_init_access(vq);\n\t\tif (r)\n\t\t\tgoto err_used;\n\t\tr = vhost_net_enable_vq(n, vq);\n\t\tif (r)\n\t\t\tgoto err_used;\n\t\tif (index == VHOST_NET_VQ_RX) {\n\t\t\tif (sock)\n\t\t\t\tnvq->rx_ring = get_tap_ptr_ring(sock->file);\n\t\t\telse\n\t\t\t\tnvq->rx_ring = NULL;\n\t\t}\n\n\t\toldubufs = nvq->ubufs;\n\t\tnvq->ubufs = ubufs;\n\n\t\tn->tx_packets = 0;\n\t\tn->tx_zcopy_err = 0;\n\t\tn->tx_flush = false;\n\t}\n\n\tmutex_unlock(&vq->mutex);\n\n\tif (oldubufs) {\n\t\tvhost_net_ubuf_put_wait_and_free(oldubufs);\n\t\tmutex_lock(&vq->mutex);\n\t\tvhost_zerocopy_signal_used(n, vq);\n\t\tmutex_unlock(&vq->mutex);\n\t}\n\n\tif (oldsock) {\n\t\tvhost_dev_flush(&n->dev);\n\t\tsockfd_put(oldsock);\n\t}\n\n\tmutex_unlock(&n->dev.mutex);\n\treturn 0;\n\nerr_used:\n\tvhost_vq_set_backend(vq, oldsock);\n\tvhost_net_enable_vq(n, vq);\n\tif (ubufs)\n\t\tvhost_net_ubuf_put_wait_and_free(ubufs);\nerr_ubufs:\n\tif (sock)\n\t\tsockfd_put(sock);\nerr_vq:\n\tmutex_unlock(&vq->mutex);\nerr:\n\tmutex_unlock(&n->dev.mutex);\n\treturn r;\n}\n\nstatic long vhost_net_reset_owner(struct vhost_net *n)\n{\n\tstruct socket *tx_sock = NULL;\n\tstruct socket *rx_sock = NULL;\n\tlong err;\n\tstruct vhost_iotlb *umem;\n\n\tmutex_lock(&n->dev.mutex);\n\terr = vhost_dev_check_owner(&n->dev);\n\tif (err)\n\t\tgoto done;\n\tumem = vhost_dev_reset_owner_prepare();\n\tif (!umem) {\n\t\terr = -ENOMEM;\n\t\tgoto done;\n\t}\n\tvhost_net_stop(n, &tx_sock, &rx_sock);\n\tvhost_net_flush(n);\n\tvhost_dev_stop(&n->dev);\n\tvhost_dev_reset_owner(&n->dev, umem);\n\tvhost_net_vq_reset(n);\ndone:\n\tmutex_unlock(&n->dev.mutex);\n\tif (tx_sock)\n\t\tsockfd_put(tx_sock);\n\tif (rx_sock)\n\t\tsockfd_put(rx_sock);\n\treturn err;\n}\n\nstatic int vhost_net_set_features(struct vhost_net *n, u64 features)\n{\n\tsize_t vhost_hlen, sock_hlen, hdr_len;\n\tint i;\n\n\thdr_len = (features & ((1ULL << VIRTIO_NET_F_MRG_RXBUF) |\n\t\t\t       (1ULL << VIRTIO_F_VERSION_1))) ?\n\t\t\tsizeof(struct virtio_net_hdr_mrg_rxbuf) :\n\t\t\tsizeof(struct virtio_net_hdr);\n\tif (features & (1 << VHOST_NET_F_VIRTIO_NET_HDR)) {\n\t\t \n\t\tvhost_hlen = hdr_len;\n\t\tsock_hlen = 0;\n\t} else {\n\t\t \n\t\tvhost_hlen = 0;\n\t\tsock_hlen = hdr_len;\n\t}\n\tmutex_lock(&n->dev.mutex);\n\tif ((features & (1 << VHOST_F_LOG_ALL)) &&\n\t    !vhost_log_access_ok(&n->dev))\n\t\tgoto out_unlock;\n\n\tif ((features & (1ULL << VIRTIO_F_ACCESS_PLATFORM))) {\n\t\tif (vhost_init_device_iotlb(&n->dev))\n\t\t\tgoto out_unlock;\n\t}\n\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; ++i) {\n\t\tmutex_lock(&n->vqs[i].vq.mutex);\n\t\tn->vqs[i].vq.acked_features = features;\n\t\tn->vqs[i].vhost_hlen = vhost_hlen;\n\t\tn->vqs[i].sock_hlen = sock_hlen;\n\t\tmutex_unlock(&n->vqs[i].vq.mutex);\n\t}\n\tmutex_unlock(&n->dev.mutex);\n\treturn 0;\n\nout_unlock:\n\tmutex_unlock(&n->dev.mutex);\n\treturn -EFAULT;\n}\n\nstatic long vhost_net_set_owner(struct vhost_net *n)\n{\n\tint r;\n\n\tmutex_lock(&n->dev.mutex);\n\tif (vhost_dev_has_owner(&n->dev)) {\n\t\tr = -EBUSY;\n\t\tgoto out;\n\t}\n\tr = vhost_net_set_ubuf_info(n);\n\tif (r)\n\t\tgoto out;\n\tr = vhost_dev_set_owner(&n->dev);\n\tif (r)\n\t\tvhost_net_clear_ubuf_info(n);\n\tvhost_net_flush(n);\nout:\n\tmutex_unlock(&n->dev.mutex);\n\treturn r;\n}\n\nstatic long vhost_net_ioctl(struct file *f, unsigned int ioctl,\n\t\t\t    unsigned long arg)\n{\n\tstruct vhost_net *n = f->private_data;\n\tvoid __user *argp = (void __user *)arg;\n\tu64 __user *featurep = argp;\n\tstruct vhost_vring_file backend;\n\tu64 features;\n\tint r;\n\n\tswitch (ioctl) {\n\tcase VHOST_NET_SET_BACKEND:\n\t\tif (copy_from_user(&backend, argp, sizeof backend))\n\t\t\treturn -EFAULT;\n\t\treturn vhost_net_set_backend(n, backend.index, backend.fd);\n\tcase VHOST_GET_FEATURES:\n\t\tfeatures = VHOST_NET_FEATURES;\n\t\tif (copy_to_user(featurep, &features, sizeof features))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tcase VHOST_SET_FEATURES:\n\t\tif (copy_from_user(&features, featurep, sizeof features))\n\t\t\treturn -EFAULT;\n\t\tif (features & ~VHOST_NET_FEATURES)\n\t\t\treturn -EOPNOTSUPP;\n\t\treturn vhost_net_set_features(n, features);\n\tcase VHOST_GET_BACKEND_FEATURES:\n\t\tfeatures = VHOST_NET_BACKEND_FEATURES;\n\t\tif (copy_to_user(featurep, &features, sizeof(features)))\n\t\t\treturn -EFAULT;\n\t\treturn 0;\n\tcase VHOST_SET_BACKEND_FEATURES:\n\t\tif (copy_from_user(&features, featurep, sizeof(features)))\n\t\t\treturn -EFAULT;\n\t\tif (features & ~VHOST_NET_BACKEND_FEATURES)\n\t\t\treturn -EOPNOTSUPP;\n\t\tvhost_set_backend_features(&n->dev, features);\n\t\treturn 0;\n\tcase VHOST_RESET_OWNER:\n\t\treturn vhost_net_reset_owner(n);\n\tcase VHOST_SET_OWNER:\n\t\treturn vhost_net_set_owner(n);\n\tdefault:\n\t\tmutex_lock(&n->dev.mutex);\n\t\tr = vhost_dev_ioctl(&n->dev, ioctl, argp);\n\t\tif (r == -ENOIOCTLCMD)\n\t\t\tr = vhost_vring_ioctl(&n->dev, ioctl, argp);\n\t\telse\n\t\t\tvhost_net_flush(n);\n\t\tmutex_unlock(&n->dev.mutex);\n\t\treturn r;\n\t}\n}\n\nstatic ssize_t vhost_net_chr_read_iter(struct kiocb *iocb, struct iov_iter *to)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct vhost_net *n = file->private_data;\n\tstruct vhost_dev *dev = &n->dev;\n\tint noblock = file->f_flags & O_NONBLOCK;\n\n\treturn vhost_chr_read_iter(dev, to, noblock);\n}\n\nstatic ssize_t vhost_net_chr_write_iter(struct kiocb *iocb,\n\t\t\t\t\tstruct iov_iter *from)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct vhost_net *n = file->private_data;\n\tstruct vhost_dev *dev = &n->dev;\n\n\treturn vhost_chr_write_iter(dev, from);\n}\n\nstatic __poll_t vhost_net_chr_poll(struct file *file, poll_table *wait)\n{\n\tstruct vhost_net *n = file->private_data;\n\tstruct vhost_dev *dev = &n->dev;\n\n\treturn vhost_chr_poll(file, dev, wait);\n}\n\nstatic const struct file_operations vhost_net_fops = {\n\t.owner          = THIS_MODULE,\n\t.release        = vhost_net_release,\n\t.read_iter      = vhost_net_chr_read_iter,\n\t.write_iter     = vhost_net_chr_write_iter,\n\t.poll           = vhost_net_chr_poll,\n\t.unlocked_ioctl = vhost_net_ioctl,\n\t.compat_ioctl   = compat_ptr_ioctl,\n\t.open           = vhost_net_open,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic struct miscdevice vhost_net_misc = {\n\t.minor = VHOST_NET_MINOR,\n\t.name = \"vhost-net\",\n\t.fops = &vhost_net_fops,\n};\n\nstatic int __init vhost_net_init(void)\n{\n\tif (experimental_zcopytx)\n\t\tvhost_net_enable_zcopy(VHOST_NET_VQ_TX);\n\treturn misc_register(&vhost_net_misc);\n}\nmodule_init(vhost_net_init);\n\nstatic void __exit vhost_net_exit(void)\n{\n\tmisc_deregister(&vhost_net_misc);\n}\nmodule_exit(vhost_net_exit);\n\nMODULE_VERSION(\"0.0.1\");\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Michael S. Tsirkin\");\nMODULE_DESCRIPTION(\"Host kernel accelerator for virtio net\");\nMODULE_ALIAS_MISCDEV(VHOST_NET_MINOR);\nMODULE_ALIAS(\"devname:vhost-net\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}