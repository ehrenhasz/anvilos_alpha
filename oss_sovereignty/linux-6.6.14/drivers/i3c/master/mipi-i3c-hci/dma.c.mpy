{
  "module_name": "dma.c",
  "hash_id": "12cdc4291f8bda9250e83ddcc89f52f0d64b7e18b865e6f4e5e9532d3c55ad73",
  "original_prompt": "Ingested from linux-6.6.14/drivers/i3c/master/mipi-i3c-hci/dma.c",
  "human_readable_source": "\n \n\n#include <linux/bitfield.h>\n#include <linux/device.h>\n#include <linux/dma-mapping.h>\n#include <linux/errno.h>\n#include <linux/i3c/master.h>\n#include <linux/io.h>\n\n#include \"hci.h\"\n#include \"cmd.h\"\n#include \"ibi.h\"\n\n\n \n\n#define XFER_RINGS\t\t\t1\t \n#define XFER_RING_ENTRIES\t\t16\t \n\n#define IBI_RINGS\t\t\t1\t \n#define IBI_STATUS_RING_ENTRIES\t\t32\t \n#define IBI_CHUNK_CACHELINES\t\t1\t \n#define IBI_CHUNK_POOL_SIZE\t\t128\t \n\n \n\n#define rhs_reg_read(r)\t\treadl(hci->RHS_regs + (RHS_##r))\n#define rhs_reg_write(r, v)\twritel(v, hci->RHS_regs + (RHS_##r))\n\n#define RHS_CONTROL\t\t\t0x00\n#define PREAMBLE_SIZE\t\t\tGENMASK(31, 24)\t \n#define HEADER_SIZE\t\t\tGENMASK(23, 16)\t \n#define MAX_HEADER_COUNT_CAP\t\tGENMASK(7, 4)  \n#define MAX_HEADER_COUNT\t\tGENMASK(3, 0)  \n\n#define RHS_RHn_OFFSET(n)\t\t(0x04 + (n)*4)\n\n \n\n#define rh_reg_read(r)\t\treadl(rh->regs + (RH_##r))\n#define rh_reg_write(r, v)\twritel(v, rh->regs + (RH_##r))\n\n#define RH_CR_SETUP\t\t\t0x00\t \n#define CR_XFER_STRUCT_SIZE\t\tGENMASK(31, 24)\n#define CR_RESP_STRUCT_SIZE\t\tGENMASK(23, 16)\n#define CR_RING_SIZE\t\t\tGENMASK(8, 0)\n\n#define RH_IBI_SETUP\t\t\t0x04\n#define IBI_STATUS_STRUCT_SIZE\t\tGENMASK(31, 24)\n#define IBI_STATUS_RING_SIZE\t\tGENMASK(23, 16)\n#define IBI_DATA_CHUNK_SIZE\t\tGENMASK(12, 10)\n#define IBI_DATA_CHUNK_COUNT\t\tGENMASK(9, 0)\n\n#define RH_CHUNK_CONTROL\t\t\t0x08\n\n#define RH_INTR_STATUS\t\t\t0x10\n#define RH_INTR_STATUS_ENABLE\t\t0x14\n#define RH_INTR_SIGNAL_ENABLE\t\t0x18\n#define RH_INTR_FORCE\t\t\t0x1c\n#define INTR_IBI_READY\t\t\tBIT(12)\n#define INTR_TRANSFER_COMPLETION\tBIT(11)\n#define INTR_RING_OP\t\t\tBIT(10)\n#define INTR_TRANSFER_ERR\t\tBIT(9)\n#define INTR_WARN_INS_STOP_MODE\t\tBIT(7)\n#define INTR_IBI_RING_FULL\t\tBIT(6)\n#define INTR_TRANSFER_ABORT\t\tBIT(5)\n\n#define RH_RING_STATUS\t\t\t0x20\n#define RING_STATUS_LOCKED\t\tBIT(3)\n#define RING_STATUS_ABORTED\t\tBIT(2)\n#define RING_STATUS_RUNNING\t\tBIT(1)\n#define RING_STATUS_ENABLED\t\tBIT(0)\n\n#define RH_RING_CONTROL\t\t\t0x24\n#define RING_CTRL_ABORT\t\t\tBIT(2)\n#define RING_CTRL_RUN_STOP\t\tBIT(1)\n#define RING_CTRL_ENABLE\t\tBIT(0)\n\n#define RH_RING_OPERATION1\t\t0x28\n#define RING_OP1_IBI_DEQ_PTR\t\tGENMASK(23, 16)\n#define RING_OP1_CR_SW_DEQ_PTR\t\tGENMASK(15, 8)\n#define RING_OP1_CR_ENQ_PTR\t\tGENMASK(7, 0)\n\n#define RH_RING_OPERATION2\t\t0x2c\n#define RING_OP2_IBI_ENQ_PTR\t\tGENMASK(23, 16)\n#define RING_OP2_CR_DEQ_PTR\t\tGENMASK(7, 0)\n\n#define RH_CMD_RING_BASE_LO\t\t0x30\n#define RH_CMD_RING_BASE_HI\t\t0x34\n#define RH_RESP_RING_BASE_LO\t\t0x38\n#define RH_RESP_RING_BASE_HI\t\t0x3c\n#define RH_IBI_STATUS_RING_BASE_LO\t0x40\n#define RH_IBI_STATUS_RING_BASE_HI\t0x44\n#define RH_IBI_DATA_RING_BASE_LO\t0x48\n#define RH_IBI_DATA_RING_BASE_HI\t0x4c\n\n#define RH_CMD_RING_SG\t\t\t0x50\t \n#define RH_RESP_RING_SG\t\t\t0x54\n#define RH_IBI_STATUS_RING_SG\t\t0x58\n#define RH_IBI_DATA_RING_SG\t\t0x5c\n#define RING_SG_BLP\t\t\tBIT(31)\t \n#define RING_SG_LIST_SIZE\t\tGENMASK(15, 0)\n\n \n\n#define DATA_BUF_BLP\t\t\tBIT(31)\t \n#define DATA_BUF_IOC\t\t\tBIT(30)\t \n#define DATA_BUF_BLOCK_SIZE\t\tGENMASK(15, 0)\n\n\nstruct hci_rh_data {\n\tvoid __iomem *regs;\n\tvoid *xfer, *resp, *ibi_status, *ibi_data;\n\tdma_addr_t xfer_dma, resp_dma, ibi_status_dma, ibi_data_dma;\n\tunsigned int xfer_entries, ibi_status_entries, ibi_chunks_total;\n\tunsigned int xfer_struct_sz, resp_struct_sz, ibi_status_sz, ibi_chunk_sz;\n\tunsigned int done_ptr, ibi_chunk_ptr;\n\tstruct hci_xfer **src_xfers;\n\tspinlock_t lock;\n\tstruct completion op_done;\n};\n\nstruct hci_rings_data {\n\tunsigned int total;\n\tstruct hci_rh_data headers[];\n};\n\nstruct hci_dma_dev_ibi_data {\n\tstruct i3c_generic_ibi_pool *pool;\n\tunsigned int max_len;\n};\n\nstatic inline u32 lo32(dma_addr_t physaddr)\n{\n\treturn physaddr;\n}\n\nstatic inline u32 hi32(dma_addr_t physaddr)\n{\n\t \n\tif (sizeof(dma_addr_t) > 4) {\n\t\tu64 hi = physaddr;\n\t\treturn hi >> 32;\n\t}\n\treturn 0;\n}\n\nstatic void hci_dma_cleanup(struct i3c_hci *hci)\n{\n\tstruct hci_rings_data *rings = hci->io_data;\n\tstruct hci_rh_data *rh;\n\tunsigned int i;\n\n\tif (!rings)\n\t\treturn;\n\n\tfor (i = 0; i < rings->total; i++) {\n\t\trh = &rings->headers[i];\n\n\t\trh_reg_write(RING_CONTROL, 0);\n\t\trh_reg_write(CR_SETUP, 0);\n\t\trh_reg_write(IBI_SETUP, 0);\n\t\trh_reg_write(INTR_SIGNAL_ENABLE, 0);\n\n\t\tif (rh->xfer)\n\t\t\tdma_free_coherent(&hci->master.dev,\n\t\t\t\t\t  rh->xfer_struct_sz * rh->xfer_entries,\n\t\t\t\t\t  rh->xfer, rh->xfer_dma);\n\t\tif (rh->resp)\n\t\t\tdma_free_coherent(&hci->master.dev,\n\t\t\t\t\t  rh->resp_struct_sz * rh->xfer_entries,\n\t\t\t\t\t  rh->resp, rh->resp_dma);\n\t\tkfree(rh->src_xfers);\n\t\tif (rh->ibi_status)\n\t\t\tdma_free_coherent(&hci->master.dev,\n\t\t\t\t\t  rh->ibi_status_sz * rh->ibi_status_entries,\n\t\t\t\t\t  rh->ibi_status, rh->ibi_status_dma);\n\t\tif (rh->ibi_data_dma)\n\t\t\tdma_unmap_single(&hci->master.dev, rh->ibi_data_dma,\n\t\t\t\t\t rh->ibi_chunk_sz * rh->ibi_chunks_total,\n\t\t\t\t\t DMA_FROM_DEVICE);\n\t\tkfree(rh->ibi_data);\n\t}\n\n\trhs_reg_write(CONTROL, 0);\n\n\tkfree(rings);\n\thci->io_data = NULL;\n}\n\nstatic int hci_dma_init(struct i3c_hci *hci)\n{\n\tstruct hci_rings_data *rings;\n\tstruct hci_rh_data *rh;\n\tu32 regval;\n\tunsigned int i, nr_rings, xfers_sz, resps_sz;\n\tunsigned int ibi_status_ring_sz, ibi_data_ring_sz;\n\tint ret;\n\n\tregval = rhs_reg_read(CONTROL);\n\tnr_rings = FIELD_GET(MAX_HEADER_COUNT_CAP, regval);\n\tdev_info(&hci->master.dev, \"%d DMA rings available\\n\", nr_rings);\n\tif (unlikely(nr_rings > 8)) {\n\t\tdev_err(&hci->master.dev, \"number of rings should be <= 8\\n\");\n\t\tnr_rings = 8;\n\t}\n\tif (nr_rings > XFER_RINGS)\n\t\tnr_rings = XFER_RINGS;\n\trings = kzalloc(struct_size(rings, headers, nr_rings), GFP_KERNEL);\n\tif (!rings)\n\t\treturn -ENOMEM;\n\thci->io_data = rings;\n\trings->total = nr_rings;\n\n\tfor (i = 0; i < rings->total; i++) {\n\t\tu32 offset = rhs_reg_read(RHn_OFFSET(i));\n\n\t\tdev_info(&hci->master.dev, \"Ring %d at offset %#x\\n\", i, offset);\n\t\tret = -EINVAL;\n\t\tif (!offset)\n\t\t\tgoto err_out;\n\t\trh = &rings->headers[i];\n\t\trh->regs = hci->base_regs + offset;\n\t\tspin_lock_init(&rh->lock);\n\t\tinit_completion(&rh->op_done);\n\n\t\trh->xfer_entries = XFER_RING_ENTRIES;\n\n\t\tregval = rh_reg_read(CR_SETUP);\n\t\trh->xfer_struct_sz = FIELD_GET(CR_XFER_STRUCT_SIZE, regval);\n\t\trh->resp_struct_sz = FIELD_GET(CR_RESP_STRUCT_SIZE, regval);\n\t\tDBG(\"xfer_struct_sz = %d, resp_struct_sz = %d\",\n\t\t    rh->xfer_struct_sz, rh->resp_struct_sz);\n\t\txfers_sz = rh->xfer_struct_sz * rh->xfer_entries;\n\t\tresps_sz = rh->resp_struct_sz * rh->xfer_entries;\n\n\t\trh->xfer = dma_alloc_coherent(&hci->master.dev, xfers_sz,\n\t\t\t\t\t      &rh->xfer_dma, GFP_KERNEL);\n\t\trh->resp = dma_alloc_coherent(&hci->master.dev, resps_sz,\n\t\t\t\t\t      &rh->resp_dma, GFP_KERNEL);\n\t\trh->src_xfers =\n\t\t\tkmalloc_array(rh->xfer_entries, sizeof(*rh->src_xfers),\n\t\t\t\t      GFP_KERNEL);\n\t\tret = -ENOMEM;\n\t\tif (!rh->xfer || !rh->resp || !rh->src_xfers)\n\t\t\tgoto err_out;\n\n\t\trh_reg_write(CMD_RING_BASE_LO, lo32(rh->xfer_dma));\n\t\trh_reg_write(CMD_RING_BASE_HI, hi32(rh->xfer_dma));\n\t\trh_reg_write(RESP_RING_BASE_LO, lo32(rh->resp_dma));\n\t\trh_reg_write(RESP_RING_BASE_HI, hi32(rh->resp_dma));\n\n\t\tregval = FIELD_PREP(CR_RING_SIZE, rh->xfer_entries);\n\t\trh_reg_write(CR_SETUP, regval);\n\n\t\trh_reg_write(INTR_STATUS_ENABLE, 0xffffffff);\n\t\trh_reg_write(INTR_SIGNAL_ENABLE, INTR_IBI_READY |\n\t\t\t\t\t\t INTR_TRANSFER_COMPLETION |\n\t\t\t\t\t\t INTR_RING_OP |\n\t\t\t\t\t\t INTR_TRANSFER_ERR |\n\t\t\t\t\t\t INTR_WARN_INS_STOP_MODE |\n\t\t\t\t\t\t INTR_IBI_RING_FULL |\n\t\t\t\t\t\t INTR_TRANSFER_ABORT);\n\n\t\t \n\n\t\tif (i >= IBI_RINGS)\n\t\t\tgoto ring_ready;\n\n\t\tregval = rh_reg_read(IBI_SETUP);\n\t\trh->ibi_status_sz = FIELD_GET(IBI_STATUS_STRUCT_SIZE, regval);\n\t\trh->ibi_status_entries = IBI_STATUS_RING_ENTRIES;\n\t\trh->ibi_chunks_total = IBI_CHUNK_POOL_SIZE;\n\n\t\trh->ibi_chunk_sz = dma_get_cache_alignment();\n\t\trh->ibi_chunk_sz *= IBI_CHUNK_CACHELINES;\n\t\tBUG_ON(rh->ibi_chunk_sz > 256);\n\n\t\tibi_status_ring_sz = rh->ibi_status_sz * rh->ibi_status_entries;\n\t\tibi_data_ring_sz = rh->ibi_chunk_sz * rh->ibi_chunks_total;\n\n\t\trh->ibi_status =\n\t\t\tdma_alloc_coherent(&hci->master.dev, ibi_status_ring_sz,\n\t\t\t\t\t   &rh->ibi_status_dma, GFP_KERNEL);\n\t\trh->ibi_data = kmalloc(ibi_data_ring_sz, GFP_KERNEL);\n\t\tret = -ENOMEM;\n\t\tif (!rh->ibi_status || !rh->ibi_data)\n\t\t\tgoto err_out;\n\t\trh->ibi_data_dma =\n\t\t\tdma_map_single(&hci->master.dev, rh->ibi_data,\n\t\t\t\t       ibi_data_ring_sz, DMA_FROM_DEVICE);\n\t\tif (dma_mapping_error(&hci->master.dev, rh->ibi_data_dma)) {\n\t\t\trh->ibi_data_dma = 0;\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tregval = FIELD_PREP(IBI_STATUS_RING_SIZE,\n\t\t\t\t    rh->ibi_status_entries) |\n\t\t\t FIELD_PREP(IBI_DATA_CHUNK_SIZE,\n\t\t\t\t    ilog2(rh->ibi_chunk_sz) - 2) |\n\t\t\t FIELD_PREP(IBI_DATA_CHUNK_COUNT,\n\t\t\t\t    rh->ibi_chunks_total);\n\t\trh_reg_write(IBI_SETUP, regval);\n\n\t\tregval = rh_reg_read(INTR_SIGNAL_ENABLE);\n\t\tregval |= INTR_IBI_READY;\n\t\trh_reg_write(INTR_SIGNAL_ENABLE, regval);\n\nring_ready:\n\t\trh_reg_write(RING_CONTROL, RING_CTRL_ENABLE);\n\t}\n\n\tregval = FIELD_PREP(MAX_HEADER_COUNT, rings->total);\n\trhs_reg_write(CONTROL, regval);\n\treturn 0;\n\nerr_out:\n\thci_dma_cleanup(hci);\n\treturn ret;\n}\n\nstatic void hci_dma_unmap_xfer(struct i3c_hci *hci,\n\t\t\t       struct hci_xfer *xfer_list, unsigned int n)\n{\n\tstruct hci_xfer *xfer;\n\tunsigned int i;\n\n\tfor (i = 0; i < n; i++) {\n\t\txfer = xfer_list + i;\n\t\tdma_unmap_single(&hci->master.dev,\n\t\t\t\t xfer->data_dma, xfer->data_len,\n\t\t\t\t xfer->rnw ? DMA_FROM_DEVICE : DMA_TO_DEVICE);\n\t}\n}\n\nstatic int hci_dma_queue_xfer(struct i3c_hci *hci,\n\t\t\t      struct hci_xfer *xfer_list, int n)\n{\n\tstruct hci_rings_data *rings = hci->io_data;\n\tstruct hci_rh_data *rh;\n\tunsigned int i, ring, enqueue_ptr;\n\tu32 op1_val, op2_val;\n\n\t \n\tring = 0;\n\trh = &rings->headers[ring];\n\n\top1_val = rh_reg_read(RING_OPERATION1);\n\tenqueue_ptr = FIELD_GET(RING_OP1_CR_ENQ_PTR, op1_val);\n\tfor (i = 0; i < n; i++) {\n\t\tstruct hci_xfer *xfer = xfer_list + i;\n\t\tu32 *ring_data = rh->xfer + rh->xfer_struct_sz * enqueue_ptr;\n\n\t\t \n\t\t*ring_data++ = xfer->cmd_desc[0];\n\t\t*ring_data++ = xfer->cmd_desc[1];\n\t\tif (hci->cmd == &mipi_i3c_hci_cmd_v2) {\n\t\t\t*ring_data++ = xfer->cmd_desc[2];\n\t\t\t*ring_data++ = xfer->cmd_desc[3];\n\t\t}\n\n\t\t \n\t\tif (!xfer->data)\n\t\t\txfer->data_len = 0;\n\t\t*ring_data++ =\n\t\t\tFIELD_PREP(DATA_BUF_BLOCK_SIZE, xfer->data_len) |\n\t\t\t((i == n - 1) ? DATA_BUF_IOC : 0);\n\n\t\t \n\t\tif (xfer->data) {\n\t\t\txfer->data_dma =\n\t\t\t\tdma_map_single(&hci->master.dev,\n\t\t\t\t\t       xfer->data,\n\t\t\t\t\t       xfer->data_len,\n\t\t\t\t\t       xfer->rnw ?\n\t\t\t\t\t\t  DMA_FROM_DEVICE :\n\t\t\t\t\t\t  DMA_TO_DEVICE);\n\t\t\tif (dma_mapping_error(&hci->master.dev,\n\t\t\t\t\t      xfer->data_dma)) {\n\t\t\t\thci_dma_unmap_xfer(hci, xfer_list, i);\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t\t*ring_data++ = lo32(xfer->data_dma);\n\t\t\t*ring_data++ = hi32(xfer->data_dma);\n\t\t} else {\n\t\t\t*ring_data++ = 0;\n\t\t\t*ring_data++ = 0;\n\t\t}\n\n\t\t \n\t\trh->src_xfers[enqueue_ptr] = xfer;\n\t\t \n\t\txfer->ring_number = ring;\n\t\txfer->ring_entry = enqueue_ptr;\n\n\t\tenqueue_ptr = (enqueue_ptr + 1) % rh->xfer_entries;\n\n\t\t \n\t\top2_val = rh_reg_read(RING_OPERATION2);\n\t\tif (enqueue_ptr == FIELD_GET(RING_OP2_CR_DEQ_PTR, op2_val)) {\n\t\t\t \n\t\t\thci_dma_unmap_xfer(hci, xfer_list, i + 1);\n\t\t\treturn -EBUSY;\n\t\t}\n\t}\n\n\t \n\tspin_lock_irq(&rh->lock);\n\top1_val = rh_reg_read(RING_OPERATION1);\n\top1_val &= ~RING_OP1_CR_ENQ_PTR;\n\top1_val |= FIELD_PREP(RING_OP1_CR_ENQ_PTR, enqueue_ptr);\n\trh_reg_write(RING_OPERATION1, op1_val);\n\tspin_unlock_irq(&rh->lock);\n\n\treturn 0;\n}\n\nstatic bool hci_dma_dequeue_xfer(struct i3c_hci *hci,\n\t\t\t\t struct hci_xfer *xfer_list, int n)\n{\n\tstruct hci_rings_data *rings = hci->io_data;\n\tstruct hci_rh_data *rh = &rings->headers[xfer_list[0].ring_number];\n\tunsigned int i;\n\tbool did_unqueue = false;\n\n\t \n\trh_reg_write(RING_CONTROL, RING_CTRL_ABORT);\n\tif (wait_for_completion_timeout(&rh->op_done, HZ) == 0) {\n\t\t \n\t\tdev_crit(&hci->master.dev, \"unable to abort the ring\\n\");\n\t\tBUG();\n\t}\n\n\tfor (i = 0; i < n; i++) {\n\t\tstruct hci_xfer *xfer = xfer_list + i;\n\t\tint idx = xfer->ring_entry;\n\n\t\t \n\t\tif (idx >= 0) {\n\t\t\tu32 *ring_data = rh->xfer + rh->xfer_struct_sz * idx;\n\n\t\t\t \n\t\t\t*ring_data++ = FIELD_PREP(CMD_0_ATTR, 0x7);\n\t\t\t*ring_data++ = 0;\n\t\t\tif (hci->cmd == &mipi_i3c_hci_cmd_v2) {\n\t\t\t\t*ring_data++ = 0;\n\t\t\t\t*ring_data++ = 0;\n\t\t\t}\n\n\t\t\t \n\t\t\trh->src_xfers[idx] = NULL;\n\n\t\t\t \n\t\t\thci_dma_unmap_xfer(hci, xfer, 1);\n\n\t\t\tdid_unqueue = true;\n\t\t}\n\t}\n\n\t \n\trh_reg_write(RING_CONTROL, RING_CTRL_ENABLE);\n\n\treturn did_unqueue;\n}\n\nstatic void hci_dma_xfer_done(struct i3c_hci *hci, struct hci_rh_data *rh)\n{\n\tu32 op1_val, op2_val, resp, *ring_resp;\n\tunsigned int tid, done_ptr = rh->done_ptr;\n\tstruct hci_xfer *xfer;\n\n\tfor (;;) {\n\t\top2_val = rh_reg_read(RING_OPERATION2);\n\t\tif (done_ptr == FIELD_GET(RING_OP2_CR_DEQ_PTR, op2_val))\n\t\t\tbreak;\n\n\t\tring_resp = rh->resp + rh->resp_struct_sz * done_ptr;\n\t\tresp = *ring_resp;\n\t\ttid = RESP_TID(resp);\n\t\tDBG(\"resp = 0x%08x\", resp);\n\n\t\txfer = rh->src_xfers[done_ptr];\n\t\tif (!xfer) {\n\t\t\tDBG(\"orphaned ring entry\");\n\t\t} else {\n\t\t\thci_dma_unmap_xfer(hci, xfer, 1);\n\t\t\txfer->ring_entry = -1;\n\t\t\txfer->response = resp;\n\t\t\tif (tid != xfer->cmd_tid) {\n\t\t\t\tdev_err(&hci->master.dev,\n\t\t\t\t\t\"response tid=%d when expecting %d\\n\",\n\t\t\t\t\ttid, xfer->cmd_tid);\n\t\t\t\t \n\t\t\t}\n\t\t\tif (xfer->completion)\n\t\t\t\tcomplete(xfer->completion);\n\t\t}\n\n\t\tdone_ptr = (done_ptr + 1) % rh->xfer_entries;\n\t\trh->done_ptr = done_ptr;\n\t}\n\n\t \n\tspin_lock(&rh->lock);\n\top1_val = rh_reg_read(RING_OPERATION1);\n\top1_val &= ~RING_OP1_CR_SW_DEQ_PTR;\n\top1_val |= FIELD_PREP(RING_OP1_CR_SW_DEQ_PTR, done_ptr);\n\trh_reg_write(RING_OPERATION1, op1_val);\n\tspin_unlock(&rh->lock);\n}\n\nstatic int hci_dma_request_ibi(struct i3c_hci *hci, struct i3c_dev_desc *dev,\n\t\t\t       const struct i3c_ibi_setup *req)\n{\n\tstruct i3c_hci_dev_data *dev_data = i3c_dev_get_master_data(dev);\n\tstruct i3c_generic_ibi_pool *pool;\n\tstruct hci_dma_dev_ibi_data *dev_ibi;\n\n\tdev_ibi = kmalloc(sizeof(*dev_ibi), GFP_KERNEL);\n\tif (!dev_ibi)\n\t\treturn -ENOMEM;\n\tpool = i3c_generic_ibi_alloc_pool(dev, req);\n\tif (IS_ERR(pool)) {\n\t\tkfree(dev_ibi);\n\t\treturn PTR_ERR(pool);\n\t}\n\tdev_ibi->pool = pool;\n\tdev_ibi->max_len = req->max_payload_len;\n\tdev_data->ibi_data = dev_ibi;\n\treturn 0;\n}\n\nstatic void hci_dma_free_ibi(struct i3c_hci *hci, struct i3c_dev_desc *dev)\n{\n\tstruct i3c_hci_dev_data *dev_data = i3c_dev_get_master_data(dev);\n\tstruct hci_dma_dev_ibi_data *dev_ibi = dev_data->ibi_data;\n\n\tdev_data->ibi_data = NULL;\n\ti3c_generic_ibi_free_pool(dev_ibi->pool);\n\tkfree(dev_ibi);\n}\n\nstatic void hci_dma_recycle_ibi_slot(struct i3c_hci *hci,\n\t\t\t\t     struct i3c_dev_desc *dev,\n\t\t\t\t     struct i3c_ibi_slot *slot)\n{\n\tstruct i3c_hci_dev_data *dev_data = i3c_dev_get_master_data(dev);\n\tstruct hci_dma_dev_ibi_data *dev_ibi = dev_data->ibi_data;\n\n\ti3c_generic_ibi_recycle_slot(dev_ibi->pool, slot);\n}\n\nstatic void hci_dma_process_ibi(struct i3c_hci *hci, struct hci_rh_data *rh)\n{\n\tstruct i3c_dev_desc *dev;\n\tstruct i3c_hci_dev_data *dev_data;\n\tstruct hci_dma_dev_ibi_data *dev_ibi;\n\tstruct i3c_ibi_slot *slot;\n\tu32 op1_val, op2_val, ibi_status_error;\n\tunsigned int ptr, enq_ptr, deq_ptr;\n\tunsigned int ibi_size, ibi_chunks, ibi_data_offset, first_part;\n\tint ibi_addr, last_ptr;\n\tvoid *ring_ibi_data;\n\tdma_addr_t ring_ibi_data_dma;\n\n\top1_val = rh_reg_read(RING_OPERATION1);\n\tdeq_ptr = FIELD_GET(RING_OP1_IBI_DEQ_PTR, op1_val);\n\n\top2_val = rh_reg_read(RING_OPERATION2);\n\tenq_ptr = FIELD_GET(RING_OP2_IBI_ENQ_PTR, op2_val);\n\n\tibi_status_error = 0;\n\tibi_addr = -1;\n\tibi_chunks = 0;\n\tibi_size = 0;\n\tlast_ptr = -1;\n\n\t \n\tfor (ptr = deq_ptr; ptr != enq_ptr;\n\t     ptr = (ptr + 1) % rh->ibi_status_entries) {\n\t\tu32 ibi_status, *ring_ibi_status;\n\t\tunsigned int chunks;\n\n\t\tring_ibi_status = rh->ibi_status + rh->ibi_status_sz * ptr;\n\t\tibi_status = *ring_ibi_status;\n\t\tDBG(\"status = %#x\", ibi_status);\n\n\t\tif (ibi_status_error) {\n\t\t\t \n\t\t} else if (ibi_status & IBI_ERROR) {\n\t\t\tibi_status_error = ibi_status;\n\t\t} else if (ibi_addr ==  -1) {\n\t\t\tibi_addr = FIELD_GET(IBI_TARGET_ADDR, ibi_status);\n\t\t} else if (ibi_addr != FIELD_GET(IBI_TARGET_ADDR, ibi_status)) {\n\t\t\t \n\t\t\tibi_status_error = ibi_status;\n\t\t}\n\n\t\tchunks = FIELD_GET(IBI_CHUNKS, ibi_status);\n\t\tibi_chunks += chunks;\n\t\tif (!(ibi_status & IBI_LAST_STATUS)) {\n\t\t\tibi_size += chunks * rh->ibi_chunk_sz;\n\t\t} else {\n\t\t\tibi_size += FIELD_GET(IBI_DATA_LENGTH, ibi_status);\n\t\t\tlast_ptr = ptr;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\n\tif (last_ptr == -1) {\n\t\t \n\t\tDBG(\"no LAST_STATUS available (e=%d d=%d)\", enq_ptr, deq_ptr);\n\t\treturn;\n\t}\n\tdeq_ptr = last_ptr + 1;\n\tdeq_ptr %= rh->ibi_status_entries;\n\n\tif (ibi_status_error) {\n\t\tdev_err(&hci->master.dev, \"IBI error from %#x\\n\", ibi_addr);\n\t\tgoto done;\n\t}\n\n\t \n\tdev = i3c_hci_addr_to_dev(hci, ibi_addr);\n\tif (!dev) {\n\t\tdev_err(&hci->master.dev,\n\t\t\t\"IBI for unknown device %#x\\n\", ibi_addr);\n\t\tgoto done;\n\t}\n\n\tdev_data = i3c_dev_get_master_data(dev);\n\tdev_ibi = dev_data->ibi_data;\n\tif (ibi_size > dev_ibi->max_len) {\n\t\tdev_err(&hci->master.dev, \"IBI payload too big (%d > %d)\\n\",\n\t\t\tibi_size, dev_ibi->max_len);\n\t\tgoto done;\n\t}\n\n\t \n\tslot = i3c_generic_ibi_get_free_slot(dev_ibi->pool);\n\tif (!slot) {\n\t\tdev_err(&hci->master.dev, \"no free slot for IBI\\n\");\n\t\tgoto done;\n\t}\n\n\t \n\tibi_data_offset = rh->ibi_chunk_sz * rh->ibi_chunk_ptr;\n\tring_ibi_data = rh->ibi_data + ibi_data_offset;\n\tring_ibi_data_dma = rh->ibi_data_dma + ibi_data_offset;\n\tfirst_part = (rh->ibi_chunks_total - rh->ibi_chunk_ptr)\n\t\t\t* rh->ibi_chunk_sz;\n\tif (first_part > ibi_size)\n\t\tfirst_part = ibi_size;\n\tdma_sync_single_for_cpu(&hci->master.dev, ring_ibi_data_dma,\n\t\t\t\tfirst_part, DMA_FROM_DEVICE);\n\tmemcpy(slot->data, ring_ibi_data, first_part);\n\n\t \n\tif (ibi_size > first_part) {\n\t\t \n\t\tring_ibi_data = rh->ibi_data;\n\t\tring_ibi_data_dma = rh->ibi_data_dma;\n\t\tdma_sync_single_for_cpu(&hci->master.dev, ring_ibi_data_dma,\n\t\t\t\t\tibi_size - first_part, DMA_FROM_DEVICE);\n\t\tmemcpy(slot->data + first_part, ring_ibi_data,\n\t\t       ibi_size - first_part);\n\t}\n\n\t \n\tslot->dev = dev;\n\tslot->len = ibi_size;\n\ti3c_master_queue_ibi(dev, slot);\n\ndone:\n\t \n\tspin_lock(&rh->lock);\n\top1_val = rh_reg_read(RING_OPERATION1);\n\top1_val &= ~RING_OP1_IBI_DEQ_PTR;\n\top1_val |= FIELD_PREP(RING_OP1_IBI_DEQ_PTR, deq_ptr);\n\trh_reg_write(RING_OPERATION1, op1_val);\n\tspin_unlock(&rh->lock);\n\n\t \n\trh->ibi_chunk_ptr += ibi_chunks;\n\trh->ibi_chunk_ptr %= rh->ibi_chunks_total;\n\n\t \n\trh_reg_write(CHUNK_CONTROL, rh_reg_read(CHUNK_CONTROL) + ibi_chunks);\n}\n\nstatic bool hci_dma_irq_handler(struct i3c_hci *hci, unsigned int mask)\n{\n\tstruct hci_rings_data *rings = hci->io_data;\n\tunsigned int i;\n\tbool handled = false;\n\n\tfor (i = 0; mask && i < rings->total; i++) {\n\t\tstruct hci_rh_data *rh;\n\t\tu32 status;\n\n\t\tif (!(mask & BIT(i)))\n\t\t\tcontinue;\n\t\tmask &= ~BIT(i);\n\n\t\trh = &rings->headers[i];\n\t\tstatus = rh_reg_read(INTR_STATUS);\n\t\tDBG(\"rh%d status: %#x\", i, status);\n\t\tif (!status)\n\t\t\tcontinue;\n\t\trh_reg_write(INTR_STATUS, status);\n\n\t\tif (status & INTR_IBI_READY)\n\t\t\thci_dma_process_ibi(hci, rh);\n\t\tif (status & (INTR_TRANSFER_COMPLETION | INTR_TRANSFER_ERR))\n\t\t\thci_dma_xfer_done(hci, rh);\n\t\tif (status & INTR_RING_OP)\n\t\t\tcomplete(&rh->op_done);\n\n\t\tif (status & INTR_TRANSFER_ABORT)\n\t\t\tdev_notice_ratelimited(&hci->master.dev,\n\t\t\t\t\"ring %d: Transfer Aborted\\n\", i);\n\t\tif (status & INTR_WARN_INS_STOP_MODE)\n\t\t\tdev_warn_ratelimited(&hci->master.dev,\n\t\t\t\t\"ring %d: Inserted Stop on Mode Change\\n\", i);\n\t\tif (status & INTR_IBI_RING_FULL)\n\t\t\tdev_err_ratelimited(&hci->master.dev,\n\t\t\t\t\"ring %d: IBI Ring Full Condition\\n\", i);\n\n\t\thandled = true;\n\t}\n\n\treturn handled;\n}\n\nconst struct hci_io_ops mipi_i3c_hci_dma = {\n\t.init\t\t\t= hci_dma_init,\n\t.cleanup\t\t= hci_dma_cleanup,\n\t.queue_xfer\t\t= hci_dma_queue_xfer,\n\t.dequeue_xfer\t\t= hci_dma_dequeue_xfer,\n\t.irq_handler\t\t= hci_dma_irq_handler,\n\t.request_ibi\t\t= hci_dma_request_ibi,\n\t.free_ibi\t\t= hci_dma_free_ibi,\n\t.recycle_ibi_slot\t= hci_dma_recycle_ibi_slot,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}