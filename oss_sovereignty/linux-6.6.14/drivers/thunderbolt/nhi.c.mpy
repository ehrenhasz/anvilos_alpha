{
  "module_name": "nhi.c",
  "hash_id": "63242f03c2c772087044bfad153e3ed3fc29f5f4ddd0a0764c019a8273a7fcc2",
  "original_prompt": "Ingested from linux-6.6.14/drivers/thunderbolt/nhi.c",
  "human_readable_source": "\n \n\n#include <linux/pm_runtime.h>\n#include <linux/slab.h>\n#include <linux/errno.h>\n#include <linux/pci.h>\n#include <linux/dma-mapping.h>\n#include <linux/interrupt.h>\n#include <linux/iommu.h>\n#include <linux/module.h>\n#include <linux/delay.h>\n#include <linux/property.h>\n#include <linux/string_helpers.h>\n\n#include \"nhi.h\"\n#include \"nhi_regs.h\"\n#include \"tb.h\"\n\n#define RING_TYPE(ring) ((ring)->is_tx ? \"TX ring\" : \"RX ring\")\n\n#define RING_FIRST_USABLE_HOPID\t1\n \n#define RING_E2E_RESERVED_HOPID\tRING_FIRST_USABLE_HOPID\n \n#define MSIX_MIN_VECS\t\t6\n#define MSIX_MAX_VECS\t\t16\n\n#define NHI_MAILBOX_TIMEOUT\t500  \n\n \n#define QUIRK_AUTO_CLEAR_INT\tBIT(0)\n#define QUIRK_E2E\t\tBIT(1)\n\nstatic bool host_reset = true;\nmodule_param(host_reset, bool, 0444);\nMODULE_PARM_DESC(host_reset, \"reset USBv2 host router (default: true)\");\n\nstatic int ring_interrupt_index(const struct tb_ring *ring)\n{\n\tint bit = ring->hop;\n\tif (!ring->is_tx)\n\t\tbit += ring->nhi->hop_count;\n\treturn bit;\n}\n\nstatic void nhi_mask_interrupt(struct tb_nhi *nhi, int mask, int ring)\n{\n\tif (nhi->quirks & QUIRK_AUTO_CLEAR_INT) {\n\t\tu32 val;\n\n\t\tval = ioread32(nhi->iobase + REG_RING_INTERRUPT_BASE + ring);\n\t\tiowrite32(val & ~mask, nhi->iobase + REG_RING_INTERRUPT_BASE + ring);\n\t} else {\n\t\tiowrite32(mask, nhi->iobase + REG_RING_INTERRUPT_MASK_CLEAR_BASE + ring);\n\t}\n}\n\nstatic void nhi_clear_interrupt(struct tb_nhi *nhi, int ring)\n{\n\tif (nhi->quirks & QUIRK_AUTO_CLEAR_INT)\n\t\tioread32(nhi->iobase + REG_RING_NOTIFY_BASE + ring);\n\telse\n\t\tiowrite32(~0, nhi->iobase + REG_RING_INT_CLEAR + ring);\n}\n\n \nstatic void ring_interrupt_active(struct tb_ring *ring, bool active)\n{\n\tint index = ring_interrupt_index(ring) / 32 * 4;\n\tint reg = REG_RING_INTERRUPT_BASE + index;\n\tint interrupt_bit = ring_interrupt_index(ring) & 31;\n\tint mask = 1 << interrupt_bit;\n\tu32 old, new;\n\n\tif (ring->irq > 0) {\n\t\tu32 step, shift, ivr, misc;\n\t\tvoid __iomem *ivr_base;\n\t\tint auto_clear_bit;\n\t\tint index;\n\n\t\tif (ring->is_tx)\n\t\t\tindex = ring->hop;\n\t\telse\n\t\t\tindex = ring->hop + ring->nhi->hop_count;\n\n\t\t \n\t\tmisc = ioread32(ring->nhi->iobase + REG_DMA_MISC);\n\t\tif (ring->nhi->quirks & QUIRK_AUTO_CLEAR_INT)\n\t\t\tauto_clear_bit = REG_DMA_MISC_INT_AUTO_CLEAR;\n\t\telse\n\t\t\tauto_clear_bit = REG_DMA_MISC_DISABLE_AUTO_CLEAR;\n\t\tif (!(misc & auto_clear_bit))\n\t\t\tiowrite32(misc | auto_clear_bit,\n\t\t\t\t  ring->nhi->iobase + REG_DMA_MISC);\n\n\t\tivr_base = ring->nhi->iobase + REG_INT_VEC_ALLOC_BASE;\n\t\tstep = index / REG_INT_VEC_ALLOC_REGS * REG_INT_VEC_ALLOC_BITS;\n\t\tshift = index % REG_INT_VEC_ALLOC_REGS * REG_INT_VEC_ALLOC_BITS;\n\t\tivr = ioread32(ivr_base + step);\n\t\tivr &= ~(REG_INT_VEC_ALLOC_MASK << shift);\n\t\tif (active)\n\t\t\tivr |= ring->vector << shift;\n\t\tiowrite32(ivr, ivr_base + step);\n\t}\n\n\told = ioread32(ring->nhi->iobase + reg);\n\tif (active)\n\t\tnew = old | mask;\n\telse\n\t\tnew = old & ~mask;\n\n\tdev_dbg(&ring->nhi->pdev->dev,\n\t\t\"%s interrupt at register %#x bit %d (%#x -> %#x)\\n\",\n\t\tactive ? \"enabling\" : \"disabling\", reg, interrupt_bit, old, new);\n\n\tif (new == old)\n\t\tdev_WARN(&ring->nhi->pdev->dev,\n\t\t\t\t\t \"interrupt for %s %d is already %s\\n\",\n\t\t\t\t\t RING_TYPE(ring), ring->hop,\n\t\t\t\t\t active ? \"enabled\" : \"disabled\");\n\n\tif (active)\n\t\tiowrite32(new, ring->nhi->iobase + reg);\n\telse\n\t\tnhi_mask_interrupt(ring->nhi, mask, index);\n}\n\n \nstatic void nhi_disable_interrupts(struct tb_nhi *nhi)\n{\n\tint i = 0;\n\t \n\tfor (i = 0; i < RING_INTERRUPT_REG_COUNT(nhi); i++)\n\t\tnhi_mask_interrupt(nhi, ~0, 4 * i);\n\n\t \n\tfor (i = 0; i < RING_NOTIFY_REG_COUNT(nhi); i++)\n\t\tnhi_clear_interrupt(nhi, 4 * i);\n}\n\n \n\nstatic void __iomem *ring_desc_base(struct tb_ring *ring)\n{\n\tvoid __iomem *io = ring->nhi->iobase;\n\tio += ring->is_tx ? REG_TX_RING_BASE : REG_RX_RING_BASE;\n\tio += ring->hop * 16;\n\treturn io;\n}\n\nstatic void __iomem *ring_options_base(struct tb_ring *ring)\n{\n\tvoid __iomem *io = ring->nhi->iobase;\n\tio += ring->is_tx ? REG_TX_OPTIONS_BASE : REG_RX_OPTIONS_BASE;\n\tio += ring->hop * 32;\n\treturn io;\n}\n\nstatic void ring_iowrite_cons(struct tb_ring *ring, u16 cons)\n{\n\t \n\tiowrite32(cons, ring_desc_base(ring) + 8);\n}\n\nstatic void ring_iowrite_prod(struct tb_ring *ring, u16 prod)\n{\n\t \n\tiowrite32(prod << 16, ring_desc_base(ring) + 8);\n}\n\nstatic void ring_iowrite32desc(struct tb_ring *ring, u32 value, u32 offset)\n{\n\tiowrite32(value, ring_desc_base(ring) + offset);\n}\n\nstatic void ring_iowrite64desc(struct tb_ring *ring, u64 value, u32 offset)\n{\n\tiowrite32(value, ring_desc_base(ring) + offset);\n\tiowrite32(value >> 32, ring_desc_base(ring) + offset + 4);\n}\n\nstatic void ring_iowrite32options(struct tb_ring *ring, u32 value, u32 offset)\n{\n\tiowrite32(value, ring_options_base(ring) + offset);\n}\n\nstatic bool ring_full(struct tb_ring *ring)\n{\n\treturn ((ring->head + 1) % ring->size) == ring->tail;\n}\n\nstatic bool ring_empty(struct tb_ring *ring)\n{\n\treturn ring->head == ring->tail;\n}\n\n \nstatic void ring_write_descriptors(struct tb_ring *ring)\n{\n\tstruct ring_frame *frame, *n;\n\tstruct ring_desc *descriptor;\n\tlist_for_each_entry_safe(frame, n, &ring->queue, list) {\n\t\tif (ring_full(ring))\n\t\t\tbreak;\n\t\tlist_move_tail(&frame->list, &ring->in_flight);\n\t\tdescriptor = &ring->descriptors[ring->head];\n\t\tdescriptor->phys = frame->buffer_phy;\n\t\tdescriptor->time = 0;\n\t\tdescriptor->flags = RING_DESC_POSTED | RING_DESC_INTERRUPT;\n\t\tif (ring->is_tx) {\n\t\t\tdescriptor->length = frame->size;\n\t\t\tdescriptor->eof = frame->eof;\n\t\t\tdescriptor->sof = frame->sof;\n\t\t}\n\t\tring->head = (ring->head + 1) % ring->size;\n\t\tif (ring->is_tx)\n\t\t\tring_iowrite_prod(ring, ring->head);\n\t\telse\n\t\t\tring_iowrite_cons(ring, ring->head);\n\t}\n}\n\n \nstatic void ring_work(struct work_struct *work)\n{\n\tstruct tb_ring *ring = container_of(work, typeof(*ring), work);\n\tstruct ring_frame *frame;\n\tbool canceled = false;\n\tunsigned long flags;\n\tLIST_HEAD(done);\n\n\tspin_lock_irqsave(&ring->lock, flags);\n\n\tif (!ring->running) {\n\t\t \n\t\tlist_splice_tail_init(&ring->in_flight, &done);\n\t\tlist_splice_tail_init(&ring->queue, &done);\n\t\tcanceled = true;\n\t\tgoto invoke_callback;\n\t}\n\n\twhile (!ring_empty(ring)) {\n\t\tif (!(ring->descriptors[ring->tail].flags\n\t\t\t\t& RING_DESC_COMPLETED))\n\t\t\tbreak;\n\t\tframe = list_first_entry(&ring->in_flight, typeof(*frame),\n\t\t\t\t\t list);\n\t\tlist_move_tail(&frame->list, &done);\n\t\tif (!ring->is_tx) {\n\t\t\tframe->size = ring->descriptors[ring->tail].length;\n\t\t\tframe->eof = ring->descriptors[ring->tail].eof;\n\t\t\tframe->sof = ring->descriptors[ring->tail].sof;\n\t\t\tframe->flags = ring->descriptors[ring->tail].flags;\n\t\t}\n\t\tring->tail = (ring->tail + 1) % ring->size;\n\t}\n\tring_write_descriptors(ring);\n\ninvoke_callback:\n\t \n\tspin_unlock_irqrestore(&ring->lock, flags);\n\twhile (!list_empty(&done)) {\n\t\tframe = list_first_entry(&done, typeof(*frame), list);\n\t\t \n\t\tlist_del_init(&frame->list);\n\t\tif (frame->callback)\n\t\t\tframe->callback(ring, frame, canceled);\n\t}\n}\n\nint __tb_ring_enqueue(struct tb_ring *ring, struct ring_frame *frame)\n{\n\tunsigned long flags;\n\tint ret = 0;\n\n\tspin_lock_irqsave(&ring->lock, flags);\n\tif (ring->running) {\n\t\tlist_add_tail(&frame->list, &ring->queue);\n\t\tring_write_descriptors(ring);\n\t} else {\n\t\tret = -ESHUTDOWN;\n\t}\n\tspin_unlock_irqrestore(&ring->lock, flags);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(__tb_ring_enqueue);\n\n \nstruct ring_frame *tb_ring_poll(struct tb_ring *ring)\n{\n\tstruct ring_frame *frame = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ring->lock, flags);\n\tif (!ring->running)\n\t\tgoto unlock;\n\tif (ring_empty(ring))\n\t\tgoto unlock;\n\n\tif (ring->descriptors[ring->tail].flags & RING_DESC_COMPLETED) {\n\t\tframe = list_first_entry(&ring->in_flight, typeof(*frame),\n\t\t\t\t\t list);\n\t\tlist_del_init(&frame->list);\n\n\t\tif (!ring->is_tx) {\n\t\t\tframe->size = ring->descriptors[ring->tail].length;\n\t\t\tframe->eof = ring->descriptors[ring->tail].eof;\n\t\t\tframe->sof = ring->descriptors[ring->tail].sof;\n\t\t\tframe->flags = ring->descriptors[ring->tail].flags;\n\t\t}\n\n\t\tring->tail = (ring->tail + 1) % ring->size;\n\t}\n\nunlock:\n\tspin_unlock_irqrestore(&ring->lock, flags);\n\treturn frame;\n}\nEXPORT_SYMBOL_GPL(tb_ring_poll);\n\nstatic void __ring_interrupt_mask(struct tb_ring *ring, bool mask)\n{\n\tint idx = ring_interrupt_index(ring);\n\tint reg = REG_RING_INTERRUPT_BASE + idx / 32 * 4;\n\tint bit = idx % 32;\n\tu32 val;\n\n\tval = ioread32(ring->nhi->iobase + reg);\n\tif (mask)\n\t\tval &= ~BIT(bit);\n\telse\n\t\tval |= BIT(bit);\n\tiowrite32(val, ring->nhi->iobase + reg);\n}\n\n \nstatic void __ring_interrupt(struct tb_ring *ring)\n{\n\tif (!ring->running)\n\t\treturn;\n\n\tif (ring->start_poll) {\n\t\t__ring_interrupt_mask(ring, true);\n\t\tring->start_poll(ring->poll_data);\n\t} else {\n\t\tschedule_work(&ring->work);\n\t}\n}\n\n \nvoid tb_ring_poll_complete(struct tb_ring *ring)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ring->nhi->lock, flags);\n\tspin_lock(&ring->lock);\n\tif (ring->start_poll)\n\t\t__ring_interrupt_mask(ring, false);\n\tspin_unlock(&ring->lock);\n\tspin_unlock_irqrestore(&ring->nhi->lock, flags);\n}\nEXPORT_SYMBOL_GPL(tb_ring_poll_complete);\n\nstatic void ring_clear_msix(const struct tb_ring *ring)\n{\n\tint bit;\n\n\tif (ring->nhi->quirks & QUIRK_AUTO_CLEAR_INT)\n\t\treturn;\n\n\tbit = ring_interrupt_index(ring) & 31;\n\tif (ring->is_tx)\n\t\tiowrite32(BIT(bit), ring->nhi->iobase + REG_RING_INT_CLEAR);\n\telse\n\t\tiowrite32(BIT(bit), ring->nhi->iobase + REG_RING_INT_CLEAR +\n\t\t\t  4 * (ring->nhi->hop_count / 32));\n}\n\nstatic irqreturn_t ring_msix(int irq, void *data)\n{\n\tstruct tb_ring *ring = data;\n\n\tspin_lock(&ring->nhi->lock);\n\tring_clear_msix(ring);\n\tspin_lock(&ring->lock);\n\t__ring_interrupt(ring);\n\tspin_unlock(&ring->lock);\n\tspin_unlock(&ring->nhi->lock);\n\n\treturn IRQ_HANDLED;\n}\n\nstatic int ring_request_msix(struct tb_ring *ring, bool no_suspend)\n{\n\tstruct tb_nhi *nhi = ring->nhi;\n\tunsigned long irqflags;\n\tint ret;\n\n\tif (!nhi->pdev->msix_enabled)\n\t\treturn 0;\n\n\tret = ida_simple_get(&nhi->msix_ida, 0, MSIX_MAX_VECS, GFP_KERNEL);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tring->vector = ret;\n\n\tret = pci_irq_vector(ring->nhi->pdev, ring->vector);\n\tif (ret < 0)\n\t\tgoto err_ida_remove;\n\n\tring->irq = ret;\n\n\tirqflags = no_suspend ? IRQF_NO_SUSPEND : 0;\n\tret = request_irq(ring->irq, ring_msix, irqflags, \"thunderbolt\", ring);\n\tif (ret)\n\t\tgoto err_ida_remove;\n\n\treturn 0;\n\nerr_ida_remove:\n\tida_simple_remove(&nhi->msix_ida, ring->vector);\n\n\treturn ret;\n}\n\nstatic void ring_release_msix(struct tb_ring *ring)\n{\n\tif (ring->irq <= 0)\n\t\treturn;\n\n\tfree_irq(ring->irq, ring);\n\tida_simple_remove(&ring->nhi->msix_ida, ring->vector);\n\tring->vector = 0;\n\tring->irq = 0;\n}\n\nstatic int nhi_alloc_hop(struct tb_nhi *nhi, struct tb_ring *ring)\n{\n\tunsigned int start_hop = RING_FIRST_USABLE_HOPID;\n\tint ret = 0;\n\n\tif (nhi->quirks & QUIRK_E2E) {\n\t\tstart_hop = RING_FIRST_USABLE_HOPID + 1;\n\t\tif (ring->flags & RING_FLAG_E2E && !ring->is_tx) {\n\t\t\tdev_dbg(&nhi->pdev->dev, \"quirking E2E TX HopID %u -> %u\\n\",\n\t\t\t\tring->e2e_tx_hop, RING_E2E_RESERVED_HOPID);\n\t\t\tring->e2e_tx_hop = RING_E2E_RESERVED_HOPID;\n\t\t}\n\t}\n\n\tspin_lock_irq(&nhi->lock);\n\n\tif (ring->hop < 0) {\n\t\tunsigned int i;\n\n\t\t \n\t\tfor (i = start_hop; i < nhi->hop_count; i++) {\n\t\t\tif (ring->is_tx) {\n\t\t\t\tif (!nhi->tx_rings[i]) {\n\t\t\t\t\tring->hop = i;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (!nhi->rx_rings[i]) {\n\t\t\t\t\tring->hop = i;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\tif (ring->hop > 0 && ring->hop < start_hop) {\n\t\tdev_warn(&nhi->pdev->dev, \"invalid hop: %d\\n\", ring->hop);\n\t\tret = -EINVAL;\n\t\tgoto err_unlock;\n\t}\n\tif (ring->hop < 0 || ring->hop >= nhi->hop_count) {\n\t\tdev_warn(&nhi->pdev->dev, \"invalid hop: %d\\n\", ring->hop);\n\t\tret = -EINVAL;\n\t\tgoto err_unlock;\n\t}\n\tif (ring->is_tx && nhi->tx_rings[ring->hop]) {\n\t\tdev_warn(&nhi->pdev->dev, \"TX hop %d already allocated\\n\",\n\t\t\t ring->hop);\n\t\tret = -EBUSY;\n\t\tgoto err_unlock;\n\t}\n\tif (!ring->is_tx && nhi->rx_rings[ring->hop]) {\n\t\tdev_warn(&nhi->pdev->dev, \"RX hop %d already allocated\\n\",\n\t\t\t ring->hop);\n\t\tret = -EBUSY;\n\t\tgoto err_unlock;\n\t}\n\n\tif (ring->is_tx)\n\t\tnhi->tx_rings[ring->hop] = ring;\n\telse\n\t\tnhi->rx_rings[ring->hop] = ring;\n\nerr_unlock:\n\tspin_unlock_irq(&nhi->lock);\n\n\treturn ret;\n}\n\nstatic struct tb_ring *tb_ring_alloc(struct tb_nhi *nhi, u32 hop, int size,\n\t\t\t\t     bool transmit, unsigned int flags,\n\t\t\t\t     int e2e_tx_hop, u16 sof_mask, u16 eof_mask,\n\t\t\t\t     void (*start_poll)(void *),\n\t\t\t\t     void *poll_data)\n{\n\tstruct tb_ring *ring = NULL;\n\n\tdev_dbg(&nhi->pdev->dev, \"allocating %s ring %d of size %d\\n\",\n\t\ttransmit ? \"TX\" : \"RX\", hop, size);\n\n\tring = kzalloc(sizeof(*ring), GFP_KERNEL);\n\tif (!ring)\n\t\treturn NULL;\n\n\tspin_lock_init(&ring->lock);\n\tINIT_LIST_HEAD(&ring->queue);\n\tINIT_LIST_HEAD(&ring->in_flight);\n\tINIT_WORK(&ring->work, ring_work);\n\n\tring->nhi = nhi;\n\tring->hop = hop;\n\tring->is_tx = transmit;\n\tring->size = size;\n\tring->flags = flags;\n\tring->e2e_tx_hop = e2e_tx_hop;\n\tring->sof_mask = sof_mask;\n\tring->eof_mask = eof_mask;\n\tring->head = 0;\n\tring->tail = 0;\n\tring->running = false;\n\tring->start_poll = start_poll;\n\tring->poll_data = poll_data;\n\n\tring->descriptors = dma_alloc_coherent(&ring->nhi->pdev->dev,\n\t\t\tsize * sizeof(*ring->descriptors),\n\t\t\t&ring->descriptors_dma, GFP_KERNEL | __GFP_ZERO);\n\tif (!ring->descriptors)\n\t\tgoto err_free_ring;\n\n\tif (ring_request_msix(ring, flags & RING_FLAG_NO_SUSPEND))\n\t\tgoto err_free_descs;\n\n\tif (nhi_alloc_hop(nhi, ring))\n\t\tgoto err_release_msix;\n\n\treturn ring;\n\nerr_release_msix:\n\tring_release_msix(ring);\nerr_free_descs:\n\tdma_free_coherent(&ring->nhi->pdev->dev,\n\t\t\t  ring->size * sizeof(*ring->descriptors),\n\t\t\t  ring->descriptors, ring->descriptors_dma);\nerr_free_ring:\n\tkfree(ring);\n\n\treturn NULL;\n}\n\n \nstruct tb_ring *tb_ring_alloc_tx(struct tb_nhi *nhi, int hop, int size,\n\t\t\t\t unsigned int flags)\n{\n\treturn tb_ring_alloc(nhi, hop, size, true, flags, 0, 0, 0, NULL, NULL);\n}\nEXPORT_SYMBOL_GPL(tb_ring_alloc_tx);\n\n \nstruct tb_ring *tb_ring_alloc_rx(struct tb_nhi *nhi, int hop, int size,\n\t\t\t\t unsigned int flags, int e2e_tx_hop,\n\t\t\t\t u16 sof_mask, u16 eof_mask,\n\t\t\t\t void (*start_poll)(void *), void *poll_data)\n{\n\treturn tb_ring_alloc(nhi, hop, size, false, flags, e2e_tx_hop, sof_mask, eof_mask,\n\t\t\t     start_poll, poll_data);\n}\nEXPORT_SYMBOL_GPL(tb_ring_alloc_rx);\n\n \nvoid tb_ring_start(struct tb_ring *ring)\n{\n\tu16 frame_size;\n\tu32 flags;\n\n\tspin_lock_irq(&ring->nhi->lock);\n\tspin_lock(&ring->lock);\n\tif (ring->nhi->going_away)\n\t\tgoto err;\n\tif (ring->running) {\n\t\tdev_WARN(&ring->nhi->pdev->dev, \"ring already started\\n\");\n\t\tgoto err;\n\t}\n\tdev_dbg(&ring->nhi->pdev->dev, \"starting %s %d\\n\",\n\t\tRING_TYPE(ring), ring->hop);\n\n\tif (ring->flags & RING_FLAG_FRAME) {\n\t\t \n\t\tframe_size = 0;\n\t\tflags = RING_FLAG_ENABLE;\n\t} else {\n\t\tframe_size = TB_FRAME_SIZE;\n\t\tflags = RING_FLAG_ENABLE | RING_FLAG_RAW;\n\t}\n\n\tring_iowrite64desc(ring, ring->descriptors_dma, 0);\n\tif (ring->is_tx) {\n\t\tring_iowrite32desc(ring, ring->size, 12);\n\t\tring_iowrite32options(ring, 0, 4);  \n\t\tring_iowrite32options(ring, flags, 0);\n\t} else {\n\t\tu32 sof_eof_mask = ring->sof_mask << 16 | ring->eof_mask;\n\n\t\tring_iowrite32desc(ring, (frame_size << 16) | ring->size, 12);\n\t\tring_iowrite32options(ring, sof_eof_mask, 4);\n\t\tring_iowrite32options(ring, flags, 0);\n\t}\n\n\t \n\tif (ring->flags & RING_FLAG_E2E) {\n\t\tif (!ring->is_tx) {\n\t\t\tu32 hop;\n\n\t\t\thop = ring->e2e_tx_hop << REG_RX_OPTIONS_E2E_HOP_SHIFT;\n\t\t\thop &= REG_RX_OPTIONS_E2E_HOP_MASK;\n\t\t\tflags |= hop;\n\n\t\t\tdev_dbg(&ring->nhi->pdev->dev,\n\t\t\t\t\"enabling E2E for %s %d with TX HopID %d\\n\",\n\t\t\t\tRING_TYPE(ring), ring->hop, ring->e2e_tx_hop);\n\t\t} else {\n\t\t\tdev_dbg(&ring->nhi->pdev->dev, \"enabling E2E for %s %d\\n\",\n\t\t\t\tRING_TYPE(ring), ring->hop);\n\t\t}\n\n\t\tflags |= RING_FLAG_E2E_FLOW_CONTROL;\n\t\tring_iowrite32options(ring, flags, 0);\n\t}\n\n\tring_interrupt_active(ring, true);\n\tring->running = true;\nerr:\n\tspin_unlock(&ring->lock);\n\tspin_unlock_irq(&ring->nhi->lock);\n}\nEXPORT_SYMBOL_GPL(tb_ring_start);\n\n \nvoid tb_ring_stop(struct tb_ring *ring)\n{\n\tspin_lock_irq(&ring->nhi->lock);\n\tspin_lock(&ring->lock);\n\tdev_dbg(&ring->nhi->pdev->dev, \"stopping %s %d\\n\",\n\t\tRING_TYPE(ring), ring->hop);\n\tif (ring->nhi->going_away)\n\t\tgoto err;\n\tif (!ring->running) {\n\t\tdev_WARN(&ring->nhi->pdev->dev, \"%s %d already stopped\\n\",\n\t\t\t RING_TYPE(ring), ring->hop);\n\t\tgoto err;\n\t}\n\tring_interrupt_active(ring, false);\n\n\tring_iowrite32options(ring, 0, 0);\n\tring_iowrite64desc(ring, 0, 0);\n\tring_iowrite32desc(ring, 0, 8);\n\tring_iowrite32desc(ring, 0, 12);\n\tring->head = 0;\n\tring->tail = 0;\n\tring->running = false;\n\nerr:\n\tspin_unlock(&ring->lock);\n\tspin_unlock_irq(&ring->nhi->lock);\n\n\t \n\tschedule_work(&ring->work);\n\tflush_work(&ring->work);\n}\nEXPORT_SYMBOL_GPL(tb_ring_stop);\n\n \nvoid tb_ring_free(struct tb_ring *ring)\n{\n\tspin_lock_irq(&ring->nhi->lock);\n\t \n\tif (ring->is_tx)\n\t\tring->nhi->tx_rings[ring->hop] = NULL;\n\telse\n\t\tring->nhi->rx_rings[ring->hop] = NULL;\n\n\tif (ring->running) {\n\t\tdev_WARN(&ring->nhi->pdev->dev, \"%s %d still running\\n\",\n\t\t\t RING_TYPE(ring), ring->hop);\n\t}\n\tspin_unlock_irq(&ring->nhi->lock);\n\n\tring_release_msix(ring);\n\n\tdma_free_coherent(&ring->nhi->pdev->dev,\n\t\t\t  ring->size * sizeof(*ring->descriptors),\n\t\t\t  ring->descriptors, ring->descriptors_dma);\n\n\tring->descriptors = NULL;\n\tring->descriptors_dma = 0;\n\n\n\tdev_dbg(&ring->nhi->pdev->dev, \"freeing %s %d\\n\", RING_TYPE(ring),\n\t\tring->hop);\n\n\t \n\tflush_work(&ring->work);\n\tkfree(ring);\n}\nEXPORT_SYMBOL_GPL(tb_ring_free);\n\n \nint nhi_mailbox_cmd(struct tb_nhi *nhi, enum nhi_mailbox_cmd cmd, u32 data)\n{\n\tktime_t timeout;\n\tu32 val;\n\n\tiowrite32(data, nhi->iobase + REG_INMAIL_DATA);\n\n\tval = ioread32(nhi->iobase + REG_INMAIL_CMD);\n\tval &= ~(REG_INMAIL_CMD_MASK | REG_INMAIL_ERROR);\n\tval |= REG_INMAIL_OP_REQUEST | cmd;\n\tiowrite32(val, nhi->iobase + REG_INMAIL_CMD);\n\n\ttimeout = ktime_add_ms(ktime_get(), NHI_MAILBOX_TIMEOUT);\n\tdo {\n\t\tval = ioread32(nhi->iobase + REG_INMAIL_CMD);\n\t\tif (!(val & REG_INMAIL_OP_REQUEST))\n\t\t\tbreak;\n\t\tusleep_range(10, 20);\n\t} while (ktime_before(ktime_get(), timeout));\n\n\tif (val & REG_INMAIL_OP_REQUEST)\n\t\treturn -ETIMEDOUT;\n\tif (val & REG_INMAIL_ERROR)\n\t\treturn -EIO;\n\n\treturn 0;\n}\n\n \nenum nhi_fw_mode nhi_mailbox_mode(struct tb_nhi *nhi)\n{\n\tu32 val;\n\n\tval = ioread32(nhi->iobase + REG_OUTMAIL_CMD);\n\tval &= REG_OUTMAIL_CMD_OPMODE_MASK;\n\tval >>= REG_OUTMAIL_CMD_OPMODE_SHIFT;\n\n\treturn (enum nhi_fw_mode)val;\n}\n\nstatic void nhi_interrupt_work(struct work_struct *work)\n{\n\tstruct tb_nhi *nhi = container_of(work, typeof(*nhi), interrupt_work);\n\tint value = 0;  \n\tint bit;\n\tint hop = -1;\n\tint type = 0;  \n\tstruct tb_ring *ring;\n\n\tspin_lock_irq(&nhi->lock);\n\n\t \n\tfor (bit = 0; bit < 3 * nhi->hop_count; bit++) {\n\t\tif (bit % 32 == 0)\n\t\t\tvalue = ioread32(nhi->iobase\n\t\t\t\t\t + REG_RING_NOTIFY_BASE\n\t\t\t\t\t + 4 * (bit / 32));\n\t\tif (++hop == nhi->hop_count) {\n\t\t\thop = 0;\n\t\t\ttype++;\n\t\t}\n\t\tif ((value & (1 << (bit % 32))) == 0)\n\t\t\tcontinue;\n\t\tif (type == 2) {\n\t\t\tdev_warn(&nhi->pdev->dev,\n\t\t\t\t \"RX overflow for ring %d\\n\",\n\t\t\t\t hop);\n\t\t\tcontinue;\n\t\t}\n\t\tif (type == 0)\n\t\t\tring = nhi->tx_rings[hop];\n\t\telse\n\t\t\tring = nhi->rx_rings[hop];\n\t\tif (ring == NULL) {\n\t\t\tdev_warn(&nhi->pdev->dev,\n\t\t\t\t \"got interrupt for inactive %s ring %d\\n\",\n\t\t\t\t type ? \"RX\" : \"TX\",\n\t\t\t\t hop);\n\t\t\tcontinue;\n\t\t}\n\n\t\tspin_lock(&ring->lock);\n\t\t__ring_interrupt(ring);\n\t\tspin_unlock(&ring->lock);\n\t}\n\tspin_unlock_irq(&nhi->lock);\n}\n\nstatic irqreturn_t nhi_msi(int irq, void *data)\n{\n\tstruct tb_nhi *nhi = data;\n\tschedule_work(&nhi->interrupt_work);\n\treturn IRQ_HANDLED;\n}\n\nstatic int __nhi_suspend_noirq(struct device *dev, bool wakeup)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct tb *tb = pci_get_drvdata(pdev);\n\tstruct tb_nhi *nhi = tb->nhi;\n\tint ret;\n\n\tret = tb_domain_suspend_noirq(tb);\n\tif (ret)\n\t\treturn ret;\n\n\tif (nhi->ops && nhi->ops->suspend_noirq) {\n\t\tret = nhi->ops->suspend_noirq(tb->nhi, wakeup);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int nhi_suspend_noirq(struct device *dev)\n{\n\treturn __nhi_suspend_noirq(dev, device_may_wakeup(dev));\n}\n\nstatic int nhi_freeze_noirq(struct device *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct tb *tb = pci_get_drvdata(pdev);\n\n\treturn tb_domain_freeze_noirq(tb);\n}\n\nstatic int nhi_thaw_noirq(struct device *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct tb *tb = pci_get_drvdata(pdev);\n\n\treturn tb_domain_thaw_noirq(tb);\n}\n\nstatic bool nhi_wake_supported(struct pci_dev *pdev)\n{\n\tu8 val;\n\n\t \n\tif (device_property_read_u8(&pdev->dev, \"WAKE_SUPPORTED\", &val))\n\t\treturn !!val;\n\n\treturn true;\n}\n\nstatic int nhi_poweroff_noirq(struct device *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tbool wakeup;\n\n\twakeup = device_may_wakeup(dev) && nhi_wake_supported(pdev);\n\treturn __nhi_suspend_noirq(dev, wakeup);\n}\n\nstatic void nhi_enable_int_throttling(struct tb_nhi *nhi)\n{\n\t \n\tu32 throttle = DIV_ROUND_UP(128 * NSEC_PER_USEC, 256);\n\tunsigned int i;\n\n\t \n\tfor (i = 0; i < MSIX_MAX_VECS; i++) {\n\t\tu32 reg = REG_INT_THROTTLING_RATE + i * 4;\n\t\tiowrite32(throttle, nhi->iobase + reg);\n\t}\n}\n\nstatic int nhi_resume_noirq(struct device *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct tb *tb = pci_get_drvdata(pdev);\n\tstruct tb_nhi *nhi = tb->nhi;\n\tint ret;\n\n\t \n\tif (!pci_device_is_present(pdev)) {\n\t\tnhi->going_away = true;\n\t} else {\n\t\tif (nhi->ops && nhi->ops->resume_noirq) {\n\t\t\tret = nhi->ops->resume_noirq(nhi);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t\tnhi_enable_int_throttling(tb->nhi);\n\t}\n\n\treturn tb_domain_resume_noirq(tb);\n}\n\nstatic int nhi_suspend(struct device *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct tb *tb = pci_get_drvdata(pdev);\n\n\treturn tb_domain_suspend(tb);\n}\n\nstatic void nhi_complete(struct device *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct tb *tb = pci_get_drvdata(pdev);\n\n\t \n\tif (pm_runtime_suspended(&pdev->dev))\n\t\tpm_runtime_resume(&pdev->dev);\n\telse\n\t\ttb_domain_complete(tb);\n}\n\nstatic int nhi_runtime_suspend(struct device *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct tb *tb = pci_get_drvdata(pdev);\n\tstruct tb_nhi *nhi = tb->nhi;\n\tint ret;\n\n\tret = tb_domain_runtime_suspend(tb);\n\tif (ret)\n\t\treturn ret;\n\n\tif (nhi->ops && nhi->ops->runtime_suspend) {\n\t\tret = nhi->ops->runtime_suspend(tb->nhi);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic int nhi_runtime_resume(struct device *dev)\n{\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct tb *tb = pci_get_drvdata(pdev);\n\tstruct tb_nhi *nhi = tb->nhi;\n\tint ret;\n\n\tif (nhi->ops && nhi->ops->runtime_resume) {\n\t\tret = nhi->ops->runtime_resume(nhi);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tnhi_enable_int_throttling(nhi);\n\treturn tb_domain_runtime_resume(tb);\n}\n\nstatic void nhi_shutdown(struct tb_nhi *nhi)\n{\n\tint i;\n\n\tdev_dbg(&nhi->pdev->dev, \"shutdown\\n\");\n\n\tfor (i = 0; i < nhi->hop_count; i++) {\n\t\tif (nhi->tx_rings[i])\n\t\t\tdev_WARN(&nhi->pdev->dev,\n\t\t\t\t \"TX ring %d is still active\\n\", i);\n\t\tif (nhi->rx_rings[i])\n\t\t\tdev_WARN(&nhi->pdev->dev,\n\t\t\t\t \"RX ring %d is still active\\n\", i);\n\t}\n\tnhi_disable_interrupts(nhi);\n\t \n\tif (!nhi->pdev->msix_enabled) {\n\t\tdevm_free_irq(&nhi->pdev->dev, nhi->pdev->irq, nhi);\n\t\tflush_work(&nhi->interrupt_work);\n\t}\n\tida_destroy(&nhi->msix_ida);\n\n\tif (nhi->ops && nhi->ops->shutdown)\n\t\tnhi->ops->shutdown(nhi);\n}\n\nstatic void nhi_check_quirks(struct tb_nhi *nhi)\n{\n\tif (nhi->pdev->vendor == PCI_VENDOR_ID_INTEL) {\n\t\t \n\t\tnhi->quirks |= QUIRK_AUTO_CLEAR_INT;\n\n\t\tswitch (nhi->pdev->device) {\n\t\tcase PCI_DEVICE_ID_INTEL_FALCON_RIDGE_2C_NHI:\n\t\tcase PCI_DEVICE_ID_INTEL_FALCON_RIDGE_4C_NHI:\n\t\t\t \n\t\t\tnhi->quirks |= QUIRK_E2E;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int nhi_check_iommu_pdev(struct pci_dev *pdev, void *data)\n{\n\tif (!pdev->external_facing ||\n\t    !device_iommu_capable(&pdev->dev, IOMMU_CAP_PRE_BOOT_PROTECTION))\n\t\treturn 0;\n\t*(bool *)data = true;\n\treturn 1;  \n}\n\nstatic void nhi_check_iommu(struct tb_nhi *nhi)\n{\n\tstruct pci_bus *bus = nhi->pdev->bus;\n\tbool port_ok = false;\n\n\t \n\twhile (bus->parent)\n\t\tbus = bus->parent;\n\n\tpci_walk_bus(bus, nhi_check_iommu_pdev, &port_ok);\n\n\tnhi->iommu_dma_protection = port_ok;\n\tdev_dbg(&nhi->pdev->dev, \"IOMMU DMA protection is %s\\n\",\n\t\tstr_enabled_disabled(port_ok));\n}\n\nstatic void nhi_reset(struct tb_nhi *nhi)\n{\n\tktime_t timeout;\n\tu32 val;\n\n\tval = ioread32(nhi->iobase + REG_CAPS);\n\t \n\tif (FIELD_GET(REG_CAPS_VERSION_MASK, val) < REG_CAPS_VERSION_2)\n\t\treturn;\n\n\tif (!host_reset) {\n\t\tdev_dbg(&nhi->pdev->dev, \"skipping host router reset\\n\");\n\t\treturn;\n\t}\n\n\tiowrite32(REG_RESET_HRR, nhi->iobase + REG_RESET);\n\tmsleep(100);\n\n\ttimeout = ktime_add_ms(ktime_get(), 500);\n\tdo {\n\t\tval = ioread32(nhi->iobase + REG_RESET);\n\t\tif (!(val & REG_RESET_HRR)) {\n\t\t\tdev_warn(&nhi->pdev->dev, \"host router reset successful\\n\");\n\t\t\treturn;\n\t\t}\n\t\tusleep_range(10, 20);\n\t} while (ktime_before(ktime_get(), timeout));\n\n\tdev_warn(&nhi->pdev->dev, \"timeout resetting host router\\n\");\n}\n\nstatic int nhi_init_msi(struct tb_nhi *nhi)\n{\n\tstruct pci_dev *pdev = nhi->pdev;\n\tstruct device *dev = &pdev->dev;\n\tint res, irq, nvec;\n\n\t \n\tnhi_disable_interrupts(nhi);\n\n\tnhi_enable_int_throttling(nhi);\n\n\tida_init(&nhi->msix_ida);\n\n\t \n\tnvec = pci_alloc_irq_vectors(pdev, MSIX_MIN_VECS, MSIX_MAX_VECS,\n\t\t\t\t     PCI_IRQ_MSIX);\n\tif (nvec < 0) {\n\t\tnvec = pci_alloc_irq_vectors(pdev, 1, 1, PCI_IRQ_MSI);\n\t\tif (nvec < 0)\n\t\t\treturn nvec;\n\n\t\tINIT_WORK(&nhi->interrupt_work, nhi_interrupt_work);\n\n\t\tirq = pci_irq_vector(nhi->pdev, 0);\n\t\tif (irq < 0)\n\t\t\treturn irq;\n\n\t\tres = devm_request_irq(&pdev->dev, irq, nhi_msi,\n\t\t\t\t       IRQF_NO_SUSPEND, \"thunderbolt\", nhi);\n\t\tif (res)\n\t\t\treturn dev_err_probe(dev, res, \"request_irq failed, aborting\\n\");\n\t}\n\n\treturn 0;\n}\n\nstatic bool nhi_imr_valid(struct pci_dev *pdev)\n{\n\tu8 val;\n\n\tif (!device_property_read_u8(&pdev->dev, \"IMR_VALID\", &val))\n\t\treturn !!val;\n\n\treturn true;\n}\n\nstatic struct tb *nhi_select_cm(struct tb_nhi *nhi)\n{\n\tstruct tb *tb;\n\n\t \n\tif (tb_acpi_is_native())\n\t\treturn tb_probe(nhi);\n\n\t \n\ttb = icm_probe(nhi);\n\tif (!tb)\n\t\ttb = tb_probe(nhi);\n\n\treturn tb;\n}\n\nstatic int nhi_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct device *dev = &pdev->dev;\n\tstruct tb_nhi *nhi;\n\tstruct tb *tb;\n\tint res;\n\n\tif (!nhi_imr_valid(pdev))\n\t\treturn dev_err_probe(dev, -ENODEV, \"firmware image not valid, aborting\\n\");\n\n\tres = pcim_enable_device(pdev);\n\tif (res)\n\t\treturn dev_err_probe(dev, res, \"cannot enable PCI device, aborting\\n\");\n\n\tres = pcim_iomap_regions(pdev, 1 << 0, \"thunderbolt\");\n\tif (res)\n\t\treturn dev_err_probe(dev, res, \"cannot obtain PCI resources, aborting\\n\");\n\n\tnhi = devm_kzalloc(&pdev->dev, sizeof(*nhi), GFP_KERNEL);\n\tif (!nhi)\n\t\treturn -ENOMEM;\n\n\tnhi->pdev = pdev;\n\tnhi->ops = (const struct tb_nhi_ops *)id->driver_data;\n\t \n\tnhi->iobase = pcim_iomap_table(pdev)[0];\n\tnhi->hop_count = ioread32(nhi->iobase + REG_CAPS) & 0x3ff;\n\tdev_dbg(dev, \"total paths: %d\\n\", nhi->hop_count);\n\n\tnhi->tx_rings = devm_kcalloc(&pdev->dev, nhi->hop_count,\n\t\t\t\t     sizeof(*nhi->tx_rings), GFP_KERNEL);\n\tnhi->rx_rings = devm_kcalloc(&pdev->dev, nhi->hop_count,\n\t\t\t\t     sizeof(*nhi->rx_rings), GFP_KERNEL);\n\tif (!nhi->tx_rings || !nhi->rx_rings)\n\t\treturn -ENOMEM;\n\n\tnhi_check_quirks(nhi);\n\tnhi_check_iommu(nhi);\n\n\tnhi_reset(nhi);\n\n\tres = nhi_init_msi(nhi);\n\tif (res)\n\t\treturn dev_err_probe(dev, res, \"cannot enable MSI, aborting\\n\");\n\n\tspin_lock_init(&nhi->lock);\n\n\tres = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));\n\tif (res)\n\t\treturn dev_err_probe(dev, res, \"failed to set DMA mask\\n\");\n\n\tpci_set_master(pdev);\n\n\tif (nhi->ops && nhi->ops->init) {\n\t\tres = nhi->ops->init(nhi);\n\t\tif (res)\n\t\t\treturn res;\n\t}\n\n\ttb = nhi_select_cm(nhi);\n\tif (!tb)\n\t\treturn dev_err_probe(dev, -ENODEV,\n\t\t\t\"failed to determine connection manager, aborting\\n\");\n\n\tdev_dbg(dev, \"NHI initialized, starting thunderbolt\\n\");\n\n\tres = tb_domain_add(tb);\n\tif (res) {\n\t\t \n\t\ttb_domain_put(tb);\n\t\tnhi_shutdown(nhi);\n\t\treturn res;\n\t}\n\tpci_set_drvdata(pdev, tb);\n\n\tdevice_wakeup_enable(&pdev->dev);\n\n\tpm_runtime_allow(&pdev->dev);\n\tpm_runtime_set_autosuspend_delay(&pdev->dev, TB_AUTOSUSPEND_DELAY);\n\tpm_runtime_use_autosuspend(&pdev->dev);\n\tpm_runtime_put_autosuspend(&pdev->dev);\n\n\treturn 0;\n}\n\nstatic void nhi_remove(struct pci_dev *pdev)\n{\n\tstruct tb *tb = pci_get_drvdata(pdev);\n\tstruct tb_nhi *nhi = tb->nhi;\n\n\tpm_runtime_get_sync(&pdev->dev);\n\tpm_runtime_dont_use_autosuspend(&pdev->dev);\n\tpm_runtime_forbid(&pdev->dev);\n\n\ttb_domain_remove(tb);\n\tnhi_shutdown(nhi);\n}\n\n \nstatic const struct dev_pm_ops nhi_pm_ops = {\n\t.suspend_noirq = nhi_suspend_noirq,\n\t.resume_noirq = nhi_resume_noirq,\n\t.freeze_noirq = nhi_freeze_noirq,   \n\t.thaw_noirq = nhi_thaw_noirq,\n\t.restore_noirq = nhi_resume_noirq,\n\t.suspend = nhi_suspend,\n\t.poweroff_noirq = nhi_poweroff_noirq,\n\t.poweroff = nhi_suspend,\n\t.complete = nhi_complete,\n\t.runtime_suspend = nhi_runtime_suspend,\n\t.runtime_resume = nhi_runtime_resume,\n};\n\nstatic struct pci_device_id nhi_ids[] = {\n\t \n\t{\n\t\t.class = PCI_CLASS_SYSTEM_OTHER << 8, .class_mask = ~0,\n\t\t.vendor = PCI_VENDOR_ID_INTEL,\n\t\t.device = PCI_DEVICE_ID_INTEL_LIGHT_RIDGE,\n\t\t.subvendor = 0x2222, .subdevice = 0x1111,\n\t},\n\t{\n\t\t.class = PCI_CLASS_SYSTEM_OTHER << 8, .class_mask = ~0,\n\t\t.vendor = PCI_VENDOR_ID_INTEL,\n\t\t.device = PCI_DEVICE_ID_INTEL_CACTUS_RIDGE_4C,\n\t\t.subvendor = 0x2222, .subdevice = 0x1111,\n\t},\n\t{\n\t\t.class = PCI_CLASS_SYSTEM_OTHER << 8, .class_mask = ~0,\n\t\t.vendor = PCI_VENDOR_ID_INTEL,\n\t\t.device = PCI_DEVICE_ID_INTEL_FALCON_RIDGE_2C_NHI,\n\t\t.subvendor = PCI_ANY_ID, .subdevice = PCI_ANY_ID,\n\t},\n\t{\n\t\t.class = PCI_CLASS_SYSTEM_OTHER << 8, .class_mask = ~0,\n\t\t.vendor = PCI_VENDOR_ID_INTEL,\n\t\t.device = PCI_DEVICE_ID_INTEL_FALCON_RIDGE_4C_NHI,\n\t\t.subvendor = PCI_ANY_ID, .subdevice = PCI_ANY_ID,\n\t},\n\n\t \n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_ALPINE_RIDGE_2C_NHI) },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_ALPINE_RIDGE_4C_NHI) },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_ALPINE_RIDGE_USBONLY_NHI) },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_ALPINE_RIDGE_LP_NHI) },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_ALPINE_RIDGE_LP_USBONLY_NHI) },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_ALPINE_RIDGE_C_2C_NHI) },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_ALPINE_RIDGE_C_4C_NHI) },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_ALPINE_RIDGE_C_USBONLY_NHI) },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_TITAN_RIDGE_2C_NHI) },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_TITAN_RIDGE_4C_NHI) },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_ICL_NHI0),\n\t  .driver_data = (kernel_ulong_t)&icl_nhi_ops },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_ICL_NHI1),\n\t  .driver_data = (kernel_ulong_t)&icl_nhi_ops },\n\t \n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_TGL_NHI0),\n\t  .driver_data = (kernel_ulong_t)&icl_nhi_ops },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_TGL_NHI1),\n\t  .driver_data = (kernel_ulong_t)&icl_nhi_ops },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_TGL_H_NHI0),\n\t  .driver_data = (kernel_ulong_t)&icl_nhi_ops },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_TGL_H_NHI1),\n\t  .driver_data = (kernel_ulong_t)&icl_nhi_ops },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_ADL_NHI0),\n\t  .driver_data = (kernel_ulong_t)&icl_nhi_ops },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_ADL_NHI1),\n\t  .driver_data = (kernel_ulong_t)&icl_nhi_ops },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_RPL_NHI0),\n\t  .driver_data = (kernel_ulong_t)&icl_nhi_ops },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_RPL_NHI1),\n\t  .driver_data = (kernel_ulong_t)&icl_nhi_ops },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_MTL_M_NHI0),\n\t  .driver_data = (kernel_ulong_t)&icl_nhi_ops },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_MTL_P_NHI0),\n\t  .driver_data = (kernel_ulong_t)&icl_nhi_ops },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_MTL_P_NHI1),\n\t  .driver_data = (kernel_ulong_t)&icl_nhi_ops },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_BARLOW_RIDGE_HOST_80G_NHI) },\n\t{ PCI_VDEVICE(INTEL, PCI_DEVICE_ID_INTEL_BARLOW_RIDGE_HOST_40G_NHI) },\n\n\t \n\t{ PCI_DEVICE_CLASS(PCI_CLASS_SERIAL_USB_USB4, ~0) },\n\n\t{ 0,}\n};\n\nMODULE_DEVICE_TABLE(pci, nhi_ids);\nMODULE_DESCRIPTION(\"Thunderbolt/USB4 core driver\");\nMODULE_LICENSE(\"GPL\");\n\nstatic struct pci_driver nhi_driver = {\n\t.name = \"thunderbolt\",\n\t.id_table = nhi_ids,\n\t.probe = nhi_probe,\n\t.remove = nhi_remove,\n\t.shutdown = nhi_remove,\n\t.driver.pm = &nhi_pm_ops,\n};\n\nstatic int __init nhi_init(void)\n{\n\tint ret;\n\n\tret = tb_domain_init();\n\tif (ret)\n\t\treturn ret;\n\tret = pci_register_driver(&nhi_driver);\n\tif (ret)\n\t\ttb_domain_exit();\n\treturn ret;\n}\n\nstatic void __exit nhi_unload(void)\n{\n\tpci_unregister_driver(&nhi_driver);\n\ttb_domain_exit();\n}\n\nrootfs_initcall(nhi_init);\nmodule_exit(nhi_unload);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}