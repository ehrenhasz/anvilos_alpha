{
  "module_name": "xhci-ring.c",
  "hash_id": "58f4fd137740d8a64f72cb54224f215ae47220ea4725640952c9b8654617a2b3",
  "original_prompt": "Ingested from linux-6.6.14/drivers/usb/host/xhci-ring.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/scatterlist.h>\n#include <linux/slab.h>\n#include <linux/dma-mapping.h>\n#include \"xhci.h\"\n#include \"xhci-trace.h\"\n\nstatic int queue_command(struct xhci_hcd *xhci, struct xhci_command *cmd,\n\t\t\t u32 field1, u32 field2,\n\t\t\t u32 field3, u32 field4, bool command_must_succeed);\n\n \ndma_addr_t xhci_trb_virt_to_dma(struct xhci_segment *seg,\n\t\tunion xhci_trb *trb)\n{\n\tunsigned long segment_offset;\n\n\tif (!seg || !trb || trb < seg->trbs)\n\t\treturn 0;\n\t \n\tsegment_offset = trb - seg->trbs;\n\tif (segment_offset >= TRBS_PER_SEGMENT)\n\t\treturn 0;\n\treturn seg->dma + (segment_offset * sizeof(*trb));\n}\n\nstatic bool trb_is_noop(union xhci_trb *trb)\n{\n\treturn TRB_TYPE_NOOP_LE32(trb->generic.field[3]);\n}\n\nstatic bool trb_is_link(union xhci_trb *trb)\n{\n\treturn TRB_TYPE_LINK_LE32(trb->link.control);\n}\n\nstatic bool last_trb_on_seg(struct xhci_segment *seg, union xhci_trb *trb)\n{\n\treturn trb == &seg->trbs[TRBS_PER_SEGMENT - 1];\n}\n\nstatic bool last_trb_on_ring(struct xhci_ring *ring,\n\t\t\tstruct xhci_segment *seg, union xhci_trb *trb)\n{\n\treturn last_trb_on_seg(seg, trb) && (seg->next == ring->first_seg);\n}\n\nstatic bool link_trb_toggles_cycle(union xhci_trb *trb)\n{\n\treturn le32_to_cpu(trb->link.control) & LINK_TOGGLE;\n}\n\nstatic bool last_td_in_urb(struct xhci_td *td)\n{\n\tstruct urb_priv *urb_priv = td->urb->hcpriv;\n\n\treturn urb_priv->num_tds_done == urb_priv->num_tds;\n}\n\nstatic void inc_td_cnt(struct urb *urb)\n{\n\tstruct urb_priv *urb_priv = urb->hcpriv;\n\n\turb_priv->num_tds_done++;\n}\n\nstatic void trb_to_noop(union xhci_trb *trb, u32 noop_type)\n{\n\tif (trb_is_link(trb)) {\n\t\t \n\t\ttrb->link.control &= cpu_to_le32(~TRB_CHAIN);\n\t} else {\n\t\ttrb->generic.field[0] = 0;\n\t\ttrb->generic.field[1] = 0;\n\t\ttrb->generic.field[2] = 0;\n\t\t \n\t\ttrb->generic.field[3] &= cpu_to_le32(TRB_CYCLE);\n\t\ttrb->generic.field[3] |= cpu_to_le32(TRB_TYPE(noop_type));\n\t}\n}\n\n \nstatic void next_trb(struct xhci_hcd *xhci,\n\t\tstruct xhci_ring *ring,\n\t\tstruct xhci_segment **seg,\n\t\tunion xhci_trb **trb)\n{\n\tif (trb_is_link(*trb)) {\n\t\t*seg = (*seg)->next;\n\t\t*trb = ((*seg)->trbs);\n\t} else {\n\t\t(*trb)++;\n\t}\n}\n\n \nvoid inc_deq(struct xhci_hcd *xhci, struct xhci_ring *ring)\n{\n\tunsigned int link_trb_count = 0;\n\n\t \n\tif (ring->type == TYPE_EVENT) {\n\t\tif (!last_trb_on_seg(ring->deq_seg, ring->dequeue)) {\n\t\t\tring->dequeue++;\n\t\t\tgoto out;\n\t\t}\n\t\tif (last_trb_on_ring(ring, ring->deq_seg, ring->dequeue))\n\t\t\tring->cycle_state ^= 1;\n\t\tring->deq_seg = ring->deq_seg->next;\n\t\tring->dequeue = ring->deq_seg->trbs;\n\t\tgoto out;\n\t}\n\n\t \n\tif (!trb_is_link(ring->dequeue)) {\n\t\tif (last_trb_on_seg(ring->deq_seg, ring->dequeue))\n\t\t\txhci_warn(xhci, \"Missing link TRB at end of segment\\n\");\n\t\telse\n\t\t\tring->dequeue++;\n\t}\n\n\twhile (trb_is_link(ring->dequeue)) {\n\t\tring->deq_seg = ring->deq_seg->next;\n\t\tring->dequeue = ring->deq_seg->trbs;\n\n\t\tif (link_trb_count++ > ring->num_segs) {\n\t\t\txhci_warn(xhci, \"Ring is an endless link TRB loop\\n\");\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\ttrace_xhci_inc_deq(ring);\n\n\treturn;\n}\n\n \nstatic void inc_enq(struct xhci_hcd *xhci, struct xhci_ring *ring,\n\t\t\tbool more_trbs_coming)\n{\n\tu32 chain;\n\tunion xhci_trb *next;\n\tunsigned int link_trb_count = 0;\n\n\tchain = le32_to_cpu(ring->enqueue->generic.field[3]) & TRB_CHAIN;\n\n\tif (last_trb_on_seg(ring->enq_seg, ring->enqueue)) {\n\t\txhci_err(xhci, \"Tried to move enqueue past ring segment\\n\");\n\t\treturn;\n\t}\n\n\tnext = ++(ring->enqueue);\n\n\t \n\twhile (trb_is_link(next)) {\n\n\t\t \n\t\tif (!chain && !more_trbs_coming)\n\t\t\tbreak;\n\n\t\t \n\t\tif (!(ring->type == TYPE_ISOC &&\n\t\t      (xhci->quirks & XHCI_AMD_0x96_HOST)) &&\n\t\t    !xhci_link_trb_quirk(xhci)) {\n\t\t\tnext->link.control &= cpu_to_le32(~TRB_CHAIN);\n\t\t\tnext->link.control |= cpu_to_le32(chain);\n\t\t}\n\t\t \n\t\twmb();\n\t\tnext->link.control ^= cpu_to_le32(TRB_CYCLE);\n\n\t\t \n\t\tif (link_trb_toggles_cycle(next))\n\t\t\tring->cycle_state ^= 1;\n\n\t\tring->enq_seg = ring->enq_seg->next;\n\t\tring->enqueue = ring->enq_seg->trbs;\n\t\tnext = ring->enqueue;\n\n\t\tif (link_trb_count++ > ring->num_segs) {\n\t\t\txhci_warn(xhci, \"%s: Ring link TRB loop\\n\", __func__);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\ttrace_xhci_inc_enq(ring);\n}\n\n \nstatic unsigned int xhci_num_trbs_free(struct xhci_hcd *xhci, struct xhci_ring *ring)\n{\n\tstruct xhci_segment *enq_seg = ring->enq_seg;\n\tunion xhci_trb *enq = ring->enqueue;\n\tunion xhci_trb *last_on_seg;\n\tunsigned int free = 0;\n\tint i = 0;\n\n\t \n\tif (trb_is_link(enq)) {\n\t\tenq_seg = enq_seg->next;\n\t\tenq = enq_seg->trbs;\n\t}\n\n\t \n\tif (enq == ring->dequeue)\n\t\treturn ring->num_segs * (TRBS_PER_SEGMENT - 1);\n\n\tdo {\n\t\tif (ring->deq_seg == enq_seg && ring->dequeue >= enq)\n\t\t\treturn free + (ring->dequeue - enq);\n\t\tlast_on_seg = &enq_seg->trbs[TRBS_PER_SEGMENT - 1];\n\t\tfree += last_on_seg - enq;\n\t\tenq_seg = enq_seg->next;\n\t\tenq = enq_seg->trbs;\n\t} while (i++ <= ring->num_segs);\n\n\treturn free;\n}\n\n \n\nstatic unsigned int xhci_ring_expansion_needed(struct xhci_hcd *xhci, struct xhci_ring *ring,\n\t\t\t\t\t       unsigned int num_trbs)\n{\n\tstruct xhci_segment *seg;\n\tint trbs_past_seg;\n\tint enq_used;\n\tint new_segs;\n\n\tenq_used = ring->enqueue - ring->enq_seg->trbs;\n\n\t \n\ttrbs_past_seg = enq_used + num_trbs - (TRBS_PER_SEGMENT - 1);\n\n\tif (trbs_past_seg <= 0)\n\t\treturn 0;\n\n\t \n\tif (trb_is_link(ring->enqueue) && ring->enq_seg->next->trbs == ring->dequeue)\n\t\treturn 0;\n\n\tnew_segs = 1 + (trbs_past_seg / (TRBS_PER_SEGMENT - 1));\n\tseg = ring->enq_seg;\n\n\twhile (new_segs > 0) {\n\t\tseg = seg->next;\n\t\tif (seg == ring->deq_seg) {\n\t\t\txhci_dbg(xhci, \"Ring expansion by %d segments needed\\n\",\n\t\t\t\t new_segs);\n\t\t\txhci_dbg(xhci, \"Adding %d trbs moves enq %d trbs into deq seg\\n\",\n\t\t\t\t num_trbs, trbs_past_seg % TRBS_PER_SEGMENT);\n\t\t\treturn new_segs;\n\t\t}\n\t\tnew_segs--;\n\t}\n\n\treturn 0;\n}\n\n \nvoid xhci_ring_cmd_db(struct xhci_hcd *xhci)\n{\n\tif (!(xhci->cmd_ring_state & CMD_RING_STATE_RUNNING))\n\t\treturn;\n\n\txhci_dbg(xhci, \"// Ding dong!\\n\");\n\n\ttrace_xhci_ring_host_doorbell(0, DB_VALUE_HOST);\n\n\twritel(DB_VALUE_HOST, &xhci->dba->doorbell[0]);\n\t \n\treadl(&xhci->dba->doorbell[0]);\n}\n\nstatic bool xhci_mod_cmd_timer(struct xhci_hcd *xhci, unsigned long delay)\n{\n\treturn mod_delayed_work(system_wq, &xhci->cmd_timer, delay);\n}\n\nstatic struct xhci_command *xhci_next_queued_cmd(struct xhci_hcd *xhci)\n{\n\treturn list_first_entry_or_null(&xhci->cmd_list, struct xhci_command,\n\t\t\t\t\tcmd_list);\n}\n\n \nstatic void xhci_handle_stopped_cmd_ring(struct xhci_hcd *xhci,\n\t\t\t\t\t struct xhci_command *cur_cmd)\n{\n\tstruct xhci_command *i_cmd;\n\n\t \n\tlist_for_each_entry(i_cmd, &xhci->cmd_list, cmd_list) {\n\n\t\tif (i_cmd->status != COMP_COMMAND_ABORTED)\n\t\t\tcontinue;\n\n\t\ti_cmd->status = COMP_COMMAND_RING_STOPPED;\n\n\t\txhci_dbg(xhci, \"Turn aborted command %p to no-op\\n\",\n\t\t\t i_cmd->command_trb);\n\n\t\ttrb_to_noop(i_cmd->command_trb, TRB_CMD_NOOP);\n\n\t\t \n\t}\n\n\txhci->cmd_ring_state = CMD_RING_STATE_RUNNING;\n\n\t \n\tif ((xhci->cmd_ring->dequeue != xhci->cmd_ring->enqueue) &&\n\t    !(xhci->xhc_state & XHCI_STATE_DYING)) {\n\t\txhci->current_cmd = cur_cmd;\n\t\txhci_mod_cmd_timer(xhci, XHCI_CMD_DEFAULT_TIMEOUT);\n\t\txhci_ring_cmd_db(xhci);\n\t}\n}\n\n \nstatic int xhci_abort_cmd_ring(struct xhci_hcd *xhci, unsigned long flags)\n{\n\tstruct xhci_segment *new_seg\t= xhci->cmd_ring->deq_seg;\n\tunion xhci_trb *new_deq\t\t= xhci->cmd_ring->dequeue;\n\tu64 crcr;\n\tint ret;\n\n\txhci_dbg(xhci, \"Abort command ring\\n\");\n\n\treinit_completion(&xhci->cmd_ring_stop_completion);\n\n\t \n\tnext_trb(xhci, NULL, &new_seg, &new_deq);\n\tif (trb_is_link(new_deq))\n\t\tnext_trb(xhci, NULL, &new_seg, &new_deq);\n\n\tcrcr = xhci_trb_virt_to_dma(new_seg, new_deq);\n\txhci_write_64(xhci, crcr | CMD_RING_ABORT, &xhci->op_regs->cmd_ring);\n\n\t \n\tret = xhci_handshake(&xhci->op_regs->cmd_ring,\n\t\t\tCMD_RING_RUNNING, 0, 5 * 1000 * 1000);\n\tif (ret < 0) {\n\t\txhci_err(xhci, \"Abort failed to stop command ring: %d\\n\", ret);\n\t\txhci_halt(xhci);\n\t\txhci_hc_died(xhci);\n\t\treturn ret;\n\t}\n\t \n\tspin_unlock_irqrestore(&xhci->lock, flags);\n\tret = wait_for_completion_timeout(&xhci->cmd_ring_stop_completion,\n\t\t\t\t\t  msecs_to_jiffies(2000));\n\tspin_lock_irqsave(&xhci->lock, flags);\n\tif (!ret) {\n\t\txhci_dbg(xhci, \"No stop event for abort, ring start fail?\\n\");\n\t\txhci_cleanup_command_queue(xhci);\n\t} else {\n\t\txhci_handle_stopped_cmd_ring(xhci, xhci_next_queued_cmd(xhci));\n\t}\n\treturn 0;\n}\n\nvoid xhci_ring_ep_doorbell(struct xhci_hcd *xhci,\n\t\tunsigned int slot_id,\n\t\tunsigned int ep_index,\n\t\tunsigned int stream_id)\n{\n\t__le32 __iomem *db_addr = &xhci->dba->doorbell[slot_id];\n\tstruct xhci_virt_ep *ep = &xhci->devs[slot_id]->eps[ep_index];\n\tunsigned int ep_state = ep->ep_state;\n\n\t \n\tif ((ep_state & EP_STOP_CMD_PENDING) || (ep_state & SET_DEQ_PENDING) ||\n\t    (ep_state & EP_HALTED) || (ep_state & EP_CLEARING_TT))\n\t\treturn;\n\n\ttrace_xhci_ring_ep_doorbell(slot_id, DB_VALUE(ep_index, stream_id));\n\n\twritel(DB_VALUE(ep_index, stream_id), db_addr);\n\t \n\treadl(db_addr);\n}\n\n \nstatic void ring_doorbell_for_active_rings(struct xhci_hcd *xhci,\n\t\tunsigned int slot_id,\n\t\tunsigned int ep_index)\n{\n\tunsigned int stream_id;\n\tstruct xhci_virt_ep *ep;\n\n\tep = &xhci->devs[slot_id]->eps[ep_index];\n\n\t \n\tif (!(ep->ep_state & EP_HAS_STREAMS)) {\n\t\tif (ep->ring && !(list_empty(&ep->ring->td_list)))\n\t\t\txhci_ring_ep_doorbell(xhci, slot_id, ep_index, 0);\n\t\treturn;\n\t}\n\n\tfor (stream_id = 1; stream_id < ep->stream_info->num_streams;\n\t\t\tstream_id++) {\n\t\tstruct xhci_stream_info *stream_info = ep->stream_info;\n\t\tif (!list_empty(&stream_info->stream_rings[stream_id]->td_list))\n\t\t\txhci_ring_ep_doorbell(xhci, slot_id, ep_index,\n\t\t\t\t\t\tstream_id);\n\t}\n}\n\nvoid xhci_ring_doorbell_for_active_rings(struct xhci_hcd *xhci,\n\t\tunsigned int slot_id,\n\t\tunsigned int ep_index)\n{\n\tring_doorbell_for_active_rings(xhci, slot_id, ep_index);\n}\n\nstatic struct xhci_virt_ep *xhci_get_virt_ep(struct xhci_hcd *xhci,\n\t\t\t\t\t     unsigned int slot_id,\n\t\t\t\t\t     unsigned int ep_index)\n{\n\tif (slot_id == 0 || slot_id >= MAX_HC_SLOTS) {\n\t\txhci_warn(xhci, \"Invalid slot_id %u\\n\", slot_id);\n\t\treturn NULL;\n\t}\n\tif (ep_index >= EP_CTX_PER_DEV) {\n\t\txhci_warn(xhci, \"Invalid endpoint index %u\\n\", ep_index);\n\t\treturn NULL;\n\t}\n\tif (!xhci->devs[slot_id]) {\n\t\txhci_warn(xhci, \"No xhci virt device for slot_id %u\\n\", slot_id);\n\t\treturn NULL;\n\t}\n\n\treturn &xhci->devs[slot_id]->eps[ep_index];\n}\n\nstatic struct xhci_ring *xhci_virt_ep_to_ring(struct xhci_hcd *xhci,\n\t\t\t\t\t      struct xhci_virt_ep *ep,\n\t\t\t\t\t      unsigned int stream_id)\n{\n\t \n\tif (!(ep->ep_state & EP_HAS_STREAMS))\n\t\treturn ep->ring;\n\n\tif (!ep->stream_info)\n\t\treturn NULL;\n\n\tif (stream_id == 0 || stream_id >= ep->stream_info->num_streams) {\n\t\txhci_warn(xhci, \"Invalid stream_id %u request for slot_id %u ep_index %u\\n\",\n\t\t\t  stream_id, ep->vdev->slot_id, ep->ep_index);\n\t\treturn NULL;\n\t}\n\n\treturn ep->stream_info->stream_rings[stream_id];\n}\n\n \nstruct xhci_ring *xhci_triad_to_transfer_ring(struct xhci_hcd *xhci,\n\t\tunsigned int slot_id, unsigned int ep_index,\n\t\tunsigned int stream_id)\n{\n\tstruct xhci_virt_ep *ep;\n\n\tep = xhci_get_virt_ep(xhci, slot_id, ep_index);\n\tif (!ep)\n\t\treturn NULL;\n\n\treturn xhci_virt_ep_to_ring(xhci, ep, stream_id);\n}\n\n\n \nstatic u64 xhci_get_hw_deq(struct xhci_hcd *xhci, struct xhci_virt_device *vdev,\n\t\t\t   unsigned int ep_index, unsigned int stream_id)\n{\n\tstruct xhci_ep_ctx *ep_ctx;\n\tstruct xhci_stream_ctx *st_ctx;\n\tstruct xhci_virt_ep *ep;\n\n\tep = &vdev->eps[ep_index];\n\n\tif (ep->ep_state & EP_HAS_STREAMS) {\n\t\tst_ctx = &ep->stream_info->stream_ctx_array[stream_id];\n\t\treturn le64_to_cpu(st_ctx->stream_ring);\n\t}\n\tep_ctx = xhci_get_ep_ctx(xhci, vdev->out_ctx, ep_index);\n\treturn le64_to_cpu(ep_ctx->deq);\n}\n\nstatic int xhci_move_dequeue_past_td(struct xhci_hcd *xhci,\n\t\t\t\tunsigned int slot_id, unsigned int ep_index,\n\t\t\t\tunsigned int stream_id, struct xhci_td *td)\n{\n\tstruct xhci_virt_device *dev = xhci->devs[slot_id];\n\tstruct xhci_virt_ep *ep = &dev->eps[ep_index];\n\tstruct xhci_ring *ep_ring;\n\tstruct xhci_command *cmd;\n\tstruct xhci_segment *new_seg;\n\tunion xhci_trb *new_deq;\n\tint new_cycle;\n\tdma_addr_t addr;\n\tu64 hw_dequeue;\n\tbool cycle_found = false;\n\tbool td_last_trb_found = false;\n\tu32 trb_sct = 0;\n\tint ret;\n\n\tep_ring = xhci_triad_to_transfer_ring(xhci, slot_id,\n\t\t\tep_index, stream_id);\n\tif (!ep_ring) {\n\t\txhci_warn(xhci, \"WARN can't find new dequeue, invalid stream ID %u\\n\",\n\t\t\t  stream_id);\n\t\treturn -ENODEV;\n\t}\n\t \n\tif (!td) {\n\t\tif (list_empty(&ep_ring->td_list)) {\n\t\t\tnew_seg = ep_ring->enq_seg;\n\t\t\tnew_deq = ep_ring->enqueue;\n\t\t\tnew_cycle = ep_ring->cycle_state;\n\t\t\txhci_dbg(xhci, \"ep ring empty, Set new dequeue = enqueue\");\n\t\t\tgoto deq_found;\n\t\t} else {\n\t\t\txhci_warn(xhci, \"Can't find new dequeue state, missing td\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\thw_dequeue = xhci_get_hw_deq(xhci, dev, ep_index, stream_id);\n\tnew_seg = ep_ring->deq_seg;\n\tnew_deq = ep_ring->dequeue;\n\tnew_cycle = hw_dequeue & 0x1;\n\n\t \n\tdo {\n\t\tif (!cycle_found && xhci_trb_virt_to_dma(new_seg, new_deq)\n\t\t    == (dma_addr_t)(hw_dequeue & ~0xf)) {\n\t\t\tcycle_found = true;\n\t\t\tif (td_last_trb_found)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (new_deq == td->last_trb)\n\t\t\ttd_last_trb_found = true;\n\n\t\tif (cycle_found && trb_is_link(new_deq) &&\n\t\t    link_trb_toggles_cycle(new_deq))\n\t\t\tnew_cycle ^= 0x1;\n\n\t\tnext_trb(xhci, ep_ring, &new_seg, &new_deq);\n\n\t\t \n\t\tif (new_deq == ep->ring->dequeue) {\n\t\t\txhci_err(xhci, \"Error: Failed finding new dequeue state\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t} while (!cycle_found || !td_last_trb_found);\n\ndeq_found:\n\n\t \n\taddr = xhci_trb_virt_to_dma(new_seg, new_deq);\n\tif (addr == 0) {\n\t\txhci_warn(xhci, \"Can't find dma of new dequeue ptr\\n\");\n\t\txhci_warn(xhci, \"deq seg = %p, deq ptr = %p\\n\", new_seg, new_deq);\n\t\treturn -EINVAL;\n\t}\n\n\tif ((ep->ep_state & SET_DEQ_PENDING)) {\n\t\txhci_warn(xhci, \"Set TR Deq already pending, don't submit for 0x%pad\\n\",\n\t\t\t  &addr);\n\t\treturn -EBUSY;\n\t}\n\n\t \n\tcmd = xhci_alloc_command(xhci, false, GFP_ATOMIC);\n\tif (!cmd) {\n\t\txhci_warn(xhci, \"Can't alloc Set TR Deq cmd 0x%pad\\n\", &addr);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (stream_id)\n\t\ttrb_sct = SCT_FOR_TRB(SCT_PRI_TR);\n\tret = queue_command(xhci, cmd,\n\t\tlower_32_bits(addr) | trb_sct | new_cycle,\n\t\tupper_32_bits(addr),\n\t\tSTREAM_ID_FOR_TRB(stream_id), SLOT_ID_FOR_TRB(slot_id) |\n\t\tEP_ID_FOR_TRB(ep_index) | TRB_TYPE(TRB_SET_DEQ), false);\n\tif (ret < 0) {\n\t\txhci_free_command(xhci, cmd);\n\t\treturn ret;\n\t}\n\tep->queued_deq_seg = new_seg;\n\tep->queued_deq_ptr = new_deq;\n\n\txhci_dbg_trace(xhci, trace_xhci_dbg_cancel_urb,\n\t\t       \"Set TR Deq ptr 0x%llx, cycle %u\\n\", addr, new_cycle);\n\n\t \n\tep->ep_state |= SET_DEQ_PENDING;\n\txhci_ring_cmd_db(xhci);\n\treturn 0;\n}\n\n \nstatic void td_to_noop(struct xhci_hcd *xhci, struct xhci_ring *ep_ring,\n\t\t       struct xhci_td *td, bool flip_cycle)\n{\n\tstruct xhci_segment *seg\t= td->start_seg;\n\tunion xhci_trb *trb\t\t= td->first_trb;\n\n\twhile (1) {\n\t\ttrb_to_noop(trb, TRB_TR_NOOP);\n\n\t\t \n\t\tif (flip_cycle && trb != td->first_trb && trb != td->last_trb)\n\t\t\ttrb->generic.field[3] ^= cpu_to_le32(TRB_CYCLE);\n\n\t\tif (trb == td->last_trb)\n\t\t\tbreak;\n\n\t\tnext_trb(xhci, ep_ring, &seg, &trb);\n\t}\n}\n\n \nstatic void xhci_giveback_urb_in_irq(struct xhci_hcd *xhci,\n\t\t\t\t     struct xhci_td *cur_td, int status)\n{\n\tstruct urb\t*urb\t\t= cur_td->urb;\n\tstruct urb_priv\t*urb_priv\t= urb->hcpriv;\n\tstruct usb_hcd\t*hcd\t\t= bus_to_hcd(urb->dev->bus);\n\n\tif (usb_pipetype(urb->pipe) == PIPE_ISOCHRONOUS) {\n\t\txhci_to_hcd(xhci)->self.bandwidth_isoc_reqs--;\n\t\tif (xhci_to_hcd(xhci)->self.bandwidth_isoc_reqs\t== 0) {\n\t\t\tif (xhci->quirks & XHCI_AMD_PLL_FIX)\n\t\t\t\tusb_amd_quirk_pll_enable();\n\t\t}\n\t}\n\txhci_urb_free_priv(urb_priv);\n\tusb_hcd_unlink_urb_from_ep(hcd, urb);\n\ttrace_xhci_urb_giveback(urb);\n\tusb_hcd_giveback_urb(hcd, urb, status);\n}\n\nstatic void xhci_unmap_td_bounce_buffer(struct xhci_hcd *xhci,\n\t\tstruct xhci_ring *ring, struct xhci_td *td)\n{\n\tstruct device *dev = xhci_to_hcd(xhci)->self.sysdev;\n\tstruct xhci_segment *seg = td->bounce_seg;\n\tstruct urb *urb = td->urb;\n\tsize_t len;\n\n\tif (!ring || !seg || !urb)\n\t\treturn;\n\n\tif (usb_urb_dir_out(urb)) {\n\t\tdma_unmap_single(dev, seg->bounce_dma, ring->bounce_buf_len,\n\t\t\t\t DMA_TO_DEVICE);\n\t\treturn;\n\t}\n\n\tdma_unmap_single(dev, seg->bounce_dma, ring->bounce_buf_len,\n\t\t\t DMA_FROM_DEVICE);\n\t \n\tif (urb->num_sgs) {\n\t\tlen = sg_pcopy_from_buffer(urb->sg, urb->num_sgs, seg->bounce_buf,\n\t\t\t\t\t   seg->bounce_len, seg->bounce_offs);\n\t\tif (len != seg->bounce_len)\n\t\t\txhci_warn(xhci, \"WARN Wrong bounce buffer read length: %zu != %d\\n\",\n\t\t\t\t  len, seg->bounce_len);\n\t} else {\n\t\tmemcpy(urb->transfer_buffer + seg->bounce_offs, seg->bounce_buf,\n\t\t       seg->bounce_len);\n\t}\n\tseg->bounce_len = 0;\n\tseg->bounce_offs = 0;\n}\n\nstatic int xhci_td_cleanup(struct xhci_hcd *xhci, struct xhci_td *td,\n\t\t\t   struct xhci_ring *ep_ring, int status)\n{\n\tstruct urb *urb = NULL;\n\n\t \n\turb = td->urb;\n\n\t \n\txhci_unmap_td_bounce_buffer(xhci, ep_ring, td);\n\n\t \n\tif (urb->actual_length > urb->transfer_buffer_length) {\n\t\txhci_warn(xhci, \"URB req %u and actual %u transfer length mismatch\\n\",\n\t\t\t  urb->transfer_buffer_length, urb->actual_length);\n\t\turb->actual_length = 0;\n\t\tstatus = 0;\n\t}\n\t \n\tif (!list_empty(&td->td_list))\n\t\tlist_del_init(&td->td_list);\n\t \n\tif (!list_empty(&td->cancelled_td_list))\n\t\tlist_del_init(&td->cancelled_td_list);\n\n\tinc_td_cnt(urb);\n\t \n\tif (last_td_in_urb(td)) {\n\t\tif ((urb->actual_length != urb->transfer_buffer_length &&\n\t\t     (urb->transfer_flags & URB_SHORT_NOT_OK)) ||\n\t\t    (status != 0 && !usb_endpoint_xfer_isoc(&urb->ep->desc)))\n\t\t\txhci_dbg(xhci, \"Giveback URB %p, len = %d, expected = %d, status = %d\\n\",\n\t\t\t\t urb, urb->actual_length,\n\t\t\t\t urb->transfer_buffer_length, status);\n\n\t\t \n\t\tif (usb_pipetype(urb->pipe) == PIPE_ISOCHRONOUS)\n\t\t\tstatus = 0;\n\t\txhci_giveback_urb_in_irq(xhci, td, status);\n\t}\n\n\treturn 0;\n}\n\n\n \nstatic void xhci_giveback_invalidated_tds(struct xhci_virt_ep *ep)\n{\n\tstruct xhci_ring *ring;\n\tstruct xhci_td *td, *tmp_td;\n\n\tlist_for_each_entry_safe(td, tmp_td, &ep->cancelled_td_list,\n\t\t\t\t cancelled_td_list) {\n\n\t\tring = xhci_urb_to_transfer_ring(ep->xhci, td->urb);\n\n\t\tif (td->cancel_status == TD_CLEARED) {\n\t\t\txhci_dbg(ep->xhci, \"%s: Giveback cancelled URB %p TD\\n\",\n\t\t\t\t __func__, td->urb);\n\t\t\txhci_td_cleanup(ep->xhci, td, ring, td->status);\n\t\t} else {\n\t\t\txhci_dbg(ep->xhci, \"%s: Keep cancelled URB %p TD as cancel_status is %d\\n\",\n\t\t\t\t __func__, td->urb, td->cancel_status);\n\t\t}\n\t\tif (ep->xhci->xhc_state & XHCI_STATE_DYING)\n\t\t\treturn;\n\t}\n}\n\nstatic int xhci_reset_halted_ep(struct xhci_hcd *xhci, unsigned int slot_id,\n\t\t\t\tunsigned int ep_index, enum xhci_ep_reset_type reset_type)\n{\n\tstruct xhci_command *command;\n\tint ret = 0;\n\n\tcommand = xhci_alloc_command(xhci, false, GFP_ATOMIC);\n\tif (!command) {\n\t\tret = -ENOMEM;\n\t\tgoto done;\n\t}\n\n\txhci_dbg(xhci, \"%s-reset ep %u, slot %u\\n\",\n\t\t (reset_type == EP_HARD_RESET) ? \"Hard\" : \"Soft\",\n\t\t ep_index, slot_id);\n\n\tret = xhci_queue_reset_ep(xhci, command, slot_id, ep_index, reset_type);\ndone:\n\tif (ret)\n\t\txhci_err(xhci, \"ERROR queuing reset endpoint for slot %d ep_index %d, %d\\n\",\n\t\t\t slot_id, ep_index, ret);\n\treturn ret;\n}\n\nstatic int xhci_handle_halted_endpoint(struct xhci_hcd *xhci,\n\t\t\t\tstruct xhci_virt_ep *ep,\n\t\t\t\tstruct xhci_td *td,\n\t\t\t\tenum xhci_ep_reset_type reset_type)\n{\n\tunsigned int slot_id = ep->vdev->slot_id;\n\tint err;\n\n\t \n\tif (ep->vdev->flags & VDEV_PORT_ERROR)\n\t\treturn -ENODEV;\n\n\t \n\tif (reset_type == EP_HARD_RESET) {\n\t\tep->ep_state |= EP_HARD_CLEAR_TOGGLE;\n\t\tif (td && list_empty(&td->cancelled_td_list)) {\n\t\t\tlist_add_tail(&td->cancelled_td_list, &ep->cancelled_td_list);\n\t\t\ttd->cancel_status = TD_HALTED;\n\t\t}\n\t}\n\n\tif (ep->ep_state & EP_HALTED) {\n\t\txhci_dbg(xhci, \"Reset ep command for ep_index %d already pending\\n\",\n\t\t\t ep->ep_index);\n\t\treturn 0;\n\t}\n\n\terr = xhci_reset_halted_ep(xhci, slot_id, ep->ep_index, reset_type);\n\tif (err)\n\t\treturn err;\n\n\tep->ep_state |= EP_HALTED;\n\n\txhci_ring_cmd_db(xhci);\n\n\treturn 0;\n}\n\n \n\nstatic int xhci_invalidate_cancelled_tds(struct xhci_virt_ep *ep)\n{\n\tstruct xhci_hcd\t\t*xhci;\n\tstruct xhci_td\t\t*td = NULL;\n\tstruct xhci_td\t\t*tmp_td = NULL;\n\tstruct xhci_td\t\t*cached_td = NULL;\n\tstruct xhci_ring\t*ring;\n\tu64\t\t\thw_deq;\n\tunsigned int\t\tslot_id = ep->vdev->slot_id;\n\tint\t\t\terr;\n\n\txhci = ep->xhci;\n\n\tlist_for_each_entry_safe(td, tmp_td, &ep->cancelled_td_list, cancelled_td_list) {\n\t\txhci_dbg_trace(xhci, trace_xhci_dbg_cancel_urb,\n\t\t\t       \"Removing canceled TD starting at 0x%llx (dma) in stream %u URB %p\",\n\t\t\t       (unsigned long long)xhci_trb_virt_to_dma(\n\t\t\t\t       td->start_seg, td->first_trb),\n\t\t\t       td->urb->stream_id, td->urb);\n\t\tlist_del_init(&td->td_list);\n\t\tring = xhci_urb_to_transfer_ring(xhci, td->urb);\n\t\tif (!ring) {\n\t\t\txhci_warn(xhci, \"WARN Cancelled URB %p has invalid stream ID %u.\\n\",\n\t\t\t\t  td->urb, td->urb->stream_id);\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\thw_deq = xhci_get_hw_deq(xhci, ep->vdev, ep->ep_index,\n\t\t\t\t\t td->urb->stream_id);\n\t\thw_deq &= ~0xf;\n\n\t\tif (td->cancel_status == TD_HALTED ||\n\t\t    trb_in_td(xhci, td->start_seg, td->first_trb, td->last_trb, hw_deq, false)) {\n\t\t\tswitch (td->cancel_status) {\n\t\t\tcase TD_CLEARED:  \n\t\t\tcase TD_CLEARING_CACHE:  \n\t\t\t\tbreak;\n\t\t\tcase TD_DIRTY:  \n\t\t\tcase TD_HALTED:\n\t\t\t\ttd->cancel_status = TD_CLEARING_CACHE;\n\t\t\t\tif (cached_td)\n\t\t\t\t\t \n\t\t\t\t\txhci_dbg(xhci,\n\t\t\t\t\t\t \"Move dq past stream %u URB %p instead of stream %u URB %p\\n\",\n\t\t\t\t\t\t td->urb->stream_id, td->urb,\n\t\t\t\t\t\t cached_td->urb->stream_id, cached_td->urb);\n\t\t\t\tcached_td = td;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else {\n\t\t\ttd_to_noop(xhci, ring, td, false);\n\t\t\ttd->cancel_status = TD_CLEARED;\n\t\t}\n\t}\n\n\t \n\tif (!cached_td)\n\t\treturn 0;\n\n\terr = xhci_move_dequeue_past_td(xhci, slot_id, ep->ep_index,\n\t\t\t\t\tcached_td->urb->stream_id,\n\t\t\t\t\tcached_td);\n\tif (err) {\n\t\t \n\t\tlist_for_each_entry_safe(td, tmp_td, &ep->cancelled_td_list, cancelled_td_list) {\n\t\t\tif (td->cancel_status != TD_CLEARING_CACHE)\n\t\t\t\tcontinue;\n\t\t\txhci_dbg(xhci, \"Failed to clear cancelled cached URB %p, mark clear anyway\\n\",\n\t\t\t\t td->urb);\n\t\t\ttd_to_noop(xhci, ring, td, false);\n\t\t\ttd->cancel_status = TD_CLEARED;\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nstatic struct xhci_td *find_halted_td(struct xhci_virt_ep *ep)\n{\n\tstruct xhci_td\t*td;\n\tu64\t\thw_deq;\n\n\tif (!list_empty(&ep->ring->td_list)) {  \n\t\thw_deq = xhci_get_hw_deq(ep->xhci, ep->vdev, ep->ep_index, 0);\n\t\thw_deq &= ~0xf;\n\t\ttd = list_first_entry(&ep->ring->td_list, struct xhci_td, td_list);\n\t\tif (trb_in_td(ep->xhci, td->start_seg, td->first_trb,\n\t\t\t\ttd->last_trb, hw_deq, false))\n\t\t\treturn td;\n\t}\n\treturn NULL;\n}\n\n \nstatic void xhci_handle_cmd_stop_ep(struct xhci_hcd *xhci, int slot_id,\n\t\t\t\t    union xhci_trb *trb, u32 comp_code)\n{\n\tunsigned int ep_index;\n\tstruct xhci_virt_ep *ep;\n\tstruct xhci_ep_ctx *ep_ctx;\n\tstruct xhci_td *td = NULL;\n\tenum xhci_ep_reset_type reset_type;\n\tstruct xhci_command *command;\n\tint err;\n\n\tif (unlikely(TRB_TO_SUSPEND_PORT(le32_to_cpu(trb->generic.field[3])))) {\n\t\tif (!xhci->devs[slot_id])\n\t\t\txhci_warn(xhci, \"Stop endpoint command completion for disabled slot %u\\n\",\n\t\t\t\t  slot_id);\n\t\treturn;\n\t}\n\n\tep_index = TRB_TO_EP_INDEX(le32_to_cpu(trb->generic.field[3]));\n\tep = xhci_get_virt_ep(xhci, slot_id, ep_index);\n\tif (!ep)\n\t\treturn;\n\n\tep_ctx = xhci_get_ep_ctx(xhci, ep->vdev->out_ctx, ep_index);\n\n\ttrace_xhci_handle_cmd_stop_ep(ep_ctx);\n\n\tif (comp_code == COMP_CONTEXT_STATE_ERROR) {\n\t \n\t\tswitch (GET_EP_CTX_STATE(ep_ctx)) {\n\t\tcase EP_STATE_HALTED:\n\t\t\txhci_dbg(xhci, \"Stop ep completion raced with stall, reset ep\\n\");\n\t\t\tif (ep->ep_state & EP_HAS_STREAMS) {\n\t\t\t\treset_type = EP_SOFT_RESET;\n\t\t\t} else {\n\t\t\t\treset_type = EP_HARD_RESET;\n\t\t\t\ttd = find_halted_td(ep);\n\t\t\t\tif (td)\n\t\t\t\t\ttd->status = -EPROTO;\n\t\t\t}\n\t\t\t \n\t\t\terr = xhci_handle_halted_endpoint(xhci, ep, td, reset_type);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tep->ep_state &= ~EP_STOP_CMD_PENDING;\n\t\t\treturn;\n\t\tcase EP_STATE_RUNNING:\n\t\t\t \n\t\t\txhci_dbg(xhci, \"Stop ep completion ctx error, ep is running\\n\");\n\n\t\t\tcommand = xhci_alloc_command(xhci, false, GFP_ATOMIC);\n\t\t\tif (!command) {\n\t\t\t\tep->ep_state &= ~EP_STOP_CMD_PENDING;\n\t\t\t\treturn;\n\t\t\t}\n\t\t\txhci_queue_stop_endpoint(xhci, command, slot_id, ep_index, 0);\n\t\t\txhci_ring_cmd_db(xhci);\n\n\t\t\treturn;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\txhci_invalidate_cancelled_tds(ep);\n\tep->ep_state &= ~EP_STOP_CMD_PENDING;\n\n\t \n\txhci_giveback_invalidated_tds(ep);\n\tring_doorbell_for_active_rings(xhci, slot_id, ep_index);\n}\n\nstatic void xhci_kill_ring_urbs(struct xhci_hcd *xhci, struct xhci_ring *ring)\n{\n\tstruct xhci_td *cur_td;\n\tstruct xhci_td *tmp;\n\n\tlist_for_each_entry_safe(cur_td, tmp, &ring->td_list, td_list) {\n\t\tlist_del_init(&cur_td->td_list);\n\n\t\tif (!list_empty(&cur_td->cancelled_td_list))\n\t\t\tlist_del_init(&cur_td->cancelled_td_list);\n\n\t\txhci_unmap_td_bounce_buffer(xhci, ring, cur_td);\n\n\t\tinc_td_cnt(cur_td->urb);\n\t\tif (last_td_in_urb(cur_td))\n\t\t\txhci_giveback_urb_in_irq(xhci, cur_td, -ESHUTDOWN);\n\t}\n}\n\nstatic void xhci_kill_endpoint_urbs(struct xhci_hcd *xhci,\n\t\tint slot_id, int ep_index)\n{\n\tstruct xhci_td *cur_td;\n\tstruct xhci_td *tmp;\n\tstruct xhci_virt_ep *ep;\n\tstruct xhci_ring *ring;\n\n\tep = xhci_get_virt_ep(xhci, slot_id, ep_index);\n\tif (!ep)\n\t\treturn;\n\n\tif ((ep->ep_state & EP_HAS_STREAMS) ||\n\t\t\t(ep->ep_state & EP_GETTING_NO_STREAMS)) {\n\t\tint stream_id;\n\n\t\tfor (stream_id = 1; stream_id < ep->stream_info->num_streams;\n\t\t\t\tstream_id++) {\n\t\t\tring = ep->stream_info->stream_rings[stream_id];\n\t\t\tif (!ring)\n\t\t\t\tcontinue;\n\n\t\t\txhci_dbg_trace(xhci, trace_xhci_dbg_cancel_urb,\n\t\t\t\t\t\"Killing URBs for slot ID %u, ep index %u, stream %u\",\n\t\t\t\t\tslot_id, ep_index, stream_id);\n\t\t\txhci_kill_ring_urbs(xhci, ring);\n\t\t}\n\t} else {\n\t\tring = ep->ring;\n\t\tif (!ring)\n\t\t\treturn;\n\t\txhci_dbg_trace(xhci, trace_xhci_dbg_cancel_urb,\n\t\t\t\t\"Killing URBs for slot ID %u, ep index %u\",\n\t\t\t\tslot_id, ep_index);\n\t\txhci_kill_ring_urbs(xhci, ring);\n\t}\n\n\tlist_for_each_entry_safe(cur_td, tmp, &ep->cancelled_td_list,\n\t\t\tcancelled_td_list) {\n\t\tlist_del_init(&cur_td->cancelled_td_list);\n\t\tinc_td_cnt(cur_td->urb);\n\n\t\tif (last_td_in_urb(cur_td))\n\t\t\txhci_giveback_urb_in_irq(xhci, cur_td, -ESHUTDOWN);\n\t}\n}\n\n \nvoid xhci_hc_died(struct xhci_hcd *xhci)\n{\n\tint i, j;\n\n\tif (xhci->xhc_state & XHCI_STATE_DYING)\n\t\treturn;\n\n\txhci_err(xhci, \"xHCI host controller not responding, assume dead\\n\");\n\txhci->xhc_state |= XHCI_STATE_DYING;\n\n\txhci_cleanup_command_queue(xhci);\n\n\t \n\tfor (i = 0; i <= HCS_MAX_SLOTS(xhci->hcs_params1); i++) {\n\t\tif (!xhci->devs[i])\n\t\t\tcontinue;\n\t\tfor (j = 0; j < 31; j++)\n\t\t\txhci_kill_endpoint_urbs(xhci, i, j);\n\t}\n\n\t \n\tif (!(xhci->xhc_state & XHCI_STATE_REMOVING))\n\t\tusb_hc_died(xhci_to_hcd(xhci));\n}\n\nstatic void update_ring_for_set_deq_completion(struct xhci_hcd *xhci,\n\t\tstruct xhci_virt_device *dev,\n\t\tstruct xhci_ring *ep_ring,\n\t\tunsigned int ep_index)\n{\n\tunion xhci_trb *dequeue_temp;\n\n\tdequeue_temp = ep_ring->dequeue;\n\n\t \n\tif (trb_is_link(ep_ring->dequeue)) {\n\t\tep_ring->deq_seg = ep_ring->deq_seg->next;\n\t\tep_ring->dequeue = ep_ring->deq_seg->trbs;\n\t}\n\n\twhile (ep_ring->dequeue != dev->eps[ep_index].queued_deq_ptr) {\n\t\t \n\t\tep_ring->dequeue++;\n\t\tif (trb_is_link(ep_ring->dequeue)) {\n\t\t\tif (ep_ring->dequeue ==\n\t\t\t\t\tdev->eps[ep_index].queued_deq_ptr)\n\t\t\t\tbreak;\n\t\t\tep_ring->deq_seg = ep_ring->deq_seg->next;\n\t\t\tep_ring->dequeue = ep_ring->deq_seg->trbs;\n\t\t}\n\t\tif (ep_ring->dequeue == dequeue_temp) {\n\t\t\txhci_dbg(xhci, \"Unable to find new dequeue pointer\\n\");\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n \nstatic void xhci_handle_cmd_set_deq(struct xhci_hcd *xhci, int slot_id,\n\t\tunion xhci_trb *trb, u32 cmd_comp_code)\n{\n\tunsigned int ep_index;\n\tunsigned int stream_id;\n\tstruct xhci_ring *ep_ring;\n\tstruct xhci_virt_ep *ep;\n\tstruct xhci_ep_ctx *ep_ctx;\n\tstruct xhci_slot_ctx *slot_ctx;\n\tstruct xhci_td *td, *tmp_td;\n\n\tep_index = TRB_TO_EP_INDEX(le32_to_cpu(trb->generic.field[3]));\n\tstream_id = TRB_TO_STREAM_ID(le32_to_cpu(trb->generic.field[2]));\n\tep = xhci_get_virt_ep(xhci, slot_id, ep_index);\n\tif (!ep)\n\t\treturn;\n\n\tep_ring = xhci_virt_ep_to_ring(xhci, ep, stream_id);\n\tif (!ep_ring) {\n\t\txhci_warn(xhci, \"WARN Set TR deq ptr command for freed stream ID %u\\n\",\n\t\t\t\tstream_id);\n\t\t \n\t\tgoto cleanup;\n\t}\n\n\tep_ctx = xhci_get_ep_ctx(xhci, ep->vdev->out_ctx, ep_index);\n\tslot_ctx = xhci_get_slot_ctx(xhci, ep->vdev->out_ctx);\n\ttrace_xhci_handle_cmd_set_deq(slot_ctx);\n\ttrace_xhci_handle_cmd_set_deq_ep(ep_ctx);\n\n\tif (cmd_comp_code != COMP_SUCCESS) {\n\t\tunsigned int ep_state;\n\t\tunsigned int slot_state;\n\n\t\tswitch (cmd_comp_code) {\n\t\tcase COMP_TRB_ERROR:\n\t\t\txhci_warn(xhci, \"WARN Set TR Deq Ptr cmd invalid because of stream ID configuration\\n\");\n\t\t\tbreak;\n\t\tcase COMP_CONTEXT_STATE_ERROR:\n\t\t\txhci_warn(xhci, \"WARN Set TR Deq Ptr cmd failed due to incorrect slot or ep state.\\n\");\n\t\t\tep_state = GET_EP_CTX_STATE(ep_ctx);\n\t\t\tslot_state = le32_to_cpu(slot_ctx->dev_state);\n\t\t\tslot_state = GET_SLOT_STATE(slot_state);\n\t\t\txhci_dbg_trace(xhci, trace_xhci_dbg_cancel_urb,\n\t\t\t\t\t\"Slot state = %u, EP state = %u\",\n\t\t\t\t\tslot_state, ep_state);\n\t\t\tbreak;\n\t\tcase COMP_SLOT_NOT_ENABLED_ERROR:\n\t\t\txhci_warn(xhci, \"WARN Set TR Deq Ptr cmd failed because slot %u was not enabled.\\n\",\n\t\t\t\t\tslot_id);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\txhci_warn(xhci, \"WARN Set TR Deq Ptr cmd with unknown completion code of %u.\\n\",\n\t\t\t\t\tcmd_comp_code);\n\t\t\tbreak;\n\t\t}\n\t\t \n\t} else {\n\t\tu64 deq;\n\t\t \n\t\tif (ep->ep_state & EP_HAS_STREAMS) {\n\t\t\tstruct xhci_stream_ctx *ctx =\n\t\t\t\t&ep->stream_info->stream_ctx_array[stream_id];\n\t\t\tdeq = le64_to_cpu(ctx->stream_ring) & SCTX_DEQ_MASK;\n\t\t} else {\n\t\t\tdeq = le64_to_cpu(ep_ctx->deq) & ~EP_CTX_CYCLE_MASK;\n\t\t}\n\t\txhci_dbg_trace(xhci, trace_xhci_dbg_cancel_urb,\n\t\t\t\"Successful Set TR Deq Ptr cmd, deq = @%08llx\", deq);\n\t\tif (xhci_trb_virt_to_dma(ep->queued_deq_seg,\n\t\t\t\t\t ep->queued_deq_ptr) == deq) {\n\t\t\t \n\t\t\tupdate_ring_for_set_deq_completion(xhci, ep->vdev,\n\t\t\t\tep_ring, ep_index);\n\t\t} else {\n\t\t\txhci_warn(xhci, \"Mismatch between completed Set TR Deq Ptr command & xHCI internal state.\\n\");\n\t\t\txhci_warn(xhci, \"ep deq seg = %p, deq ptr = %p\\n\",\n\t\t\t\t  ep->queued_deq_seg, ep->queued_deq_ptr);\n\t\t}\n\t}\n\t \n\tlist_for_each_entry_safe(td, tmp_td, &ep->cancelled_td_list,\n\t\t\t\t cancelled_td_list) {\n\t\tep_ring = xhci_urb_to_transfer_ring(ep->xhci, td->urb);\n\t\tif (td->cancel_status == TD_CLEARING_CACHE) {\n\t\t\ttd->cancel_status = TD_CLEARED;\n\t\t\txhci_dbg(ep->xhci, \"%s: Giveback cancelled URB %p TD\\n\",\n\t\t\t\t __func__, td->urb);\n\t\t\txhci_td_cleanup(ep->xhci, td, ep_ring, td->status);\n\t\t} else {\n\t\t\txhci_dbg(ep->xhci, \"%s: Keep cancelled URB %p TD as cancel_status is %d\\n\",\n\t\t\t\t __func__, td->urb, td->cancel_status);\n\t\t}\n\t}\ncleanup:\n\tep->ep_state &= ~SET_DEQ_PENDING;\n\tep->queued_deq_seg = NULL;\n\tep->queued_deq_ptr = NULL;\n\t \n\tring_doorbell_for_active_rings(xhci, slot_id, ep_index);\n}\n\nstatic void xhci_handle_cmd_reset_ep(struct xhci_hcd *xhci, int slot_id,\n\t\tunion xhci_trb *trb, u32 cmd_comp_code)\n{\n\tstruct xhci_virt_ep *ep;\n\tstruct xhci_ep_ctx *ep_ctx;\n\tunsigned int ep_index;\n\n\tep_index = TRB_TO_EP_INDEX(le32_to_cpu(trb->generic.field[3]));\n\tep = xhci_get_virt_ep(xhci, slot_id, ep_index);\n\tif (!ep)\n\t\treturn;\n\n\tep_ctx = xhci_get_ep_ctx(xhci, ep->vdev->out_ctx, ep_index);\n\ttrace_xhci_handle_cmd_reset_ep(ep_ctx);\n\n\t \n\txhci_dbg_trace(xhci, trace_xhci_dbg_reset_ep,\n\t\t\"Ignoring reset ep completion code of %u\", cmd_comp_code);\n\n\t \n\txhci_invalidate_cancelled_tds(ep);\n\n\t \n\tep->ep_state &= ~EP_HALTED;\n\n\txhci_giveback_invalidated_tds(ep);\n\n\t \n\tif ((le32_to_cpu(trb->generic.field[3])) & TRB_TSP)\n\t\tring_doorbell_for_active_rings(xhci, slot_id, ep_index);\n}\n\nstatic void xhci_handle_cmd_enable_slot(struct xhci_hcd *xhci, int slot_id,\n\t\tstruct xhci_command *command, u32 cmd_comp_code)\n{\n\tif (cmd_comp_code == COMP_SUCCESS)\n\t\tcommand->slot_id = slot_id;\n\telse\n\t\tcommand->slot_id = 0;\n}\n\nstatic void xhci_handle_cmd_disable_slot(struct xhci_hcd *xhci, int slot_id)\n{\n\tstruct xhci_virt_device *virt_dev;\n\tstruct xhci_slot_ctx *slot_ctx;\n\n\tvirt_dev = xhci->devs[slot_id];\n\tif (!virt_dev)\n\t\treturn;\n\n\tslot_ctx = xhci_get_slot_ctx(xhci, virt_dev->out_ctx);\n\ttrace_xhci_handle_cmd_disable_slot(slot_ctx);\n\n\tif (xhci->quirks & XHCI_EP_LIMIT_QUIRK)\n\t\t \n\t\txhci_free_device_endpoint_resources(xhci, virt_dev, true);\n}\n\nstatic void xhci_handle_cmd_config_ep(struct xhci_hcd *xhci, int slot_id,\n\t\tu32 cmd_comp_code)\n{\n\tstruct xhci_virt_device *virt_dev;\n\tstruct xhci_input_control_ctx *ctrl_ctx;\n\tstruct xhci_ep_ctx *ep_ctx;\n\tunsigned int ep_index;\n\tu32 add_flags;\n\n\t \n\n\tvirt_dev = xhci->devs[slot_id];\n\tif (!virt_dev)\n\t\treturn;\n\tctrl_ctx = xhci_get_input_control_ctx(virt_dev->in_ctx);\n\tif (!ctrl_ctx) {\n\t\txhci_warn(xhci, \"Could not get input context, bad type.\\n\");\n\t\treturn;\n\t}\n\n\tadd_flags = le32_to_cpu(ctrl_ctx->add_flags);\n\n\t \n\tep_index = xhci_last_valid_endpoint(add_flags) - 1;\n\n\tep_ctx = xhci_get_ep_ctx(xhci, virt_dev->out_ctx, ep_index);\n\ttrace_xhci_handle_cmd_config_ep(ep_ctx);\n\n\treturn;\n}\n\nstatic void xhci_handle_cmd_addr_dev(struct xhci_hcd *xhci, int slot_id)\n{\n\tstruct xhci_virt_device *vdev;\n\tstruct xhci_slot_ctx *slot_ctx;\n\n\tvdev = xhci->devs[slot_id];\n\tif (!vdev)\n\t\treturn;\n\tslot_ctx = xhci_get_slot_ctx(xhci, vdev->out_ctx);\n\ttrace_xhci_handle_cmd_addr_dev(slot_ctx);\n}\n\nstatic void xhci_handle_cmd_reset_dev(struct xhci_hcd *xhci, int slot_id)\n{\n\tstruct xhci_virt_device *vdev;\n\tstruct xhci_slot_ctx *slot_ctx;\n\n\tvdev = xhci->devs[slot_id];\n\tif (!vdev) {\n\t\txhci_warn(xhci, \"Reset device command completion for disabled slot %u\\n\",\n\t\t\t  slot_id);\n\t\treturn;\n\t}\n\tslot_ctx = xhci_get_slot_ctx(xhci, vdev->out_ctx);\n\ttrace_xhci_handle_cmd_reset_dev(slot_ctx);\n\n\txhci_dbg(xhci, \"Completed reset device command.\\n\");\n}\n\nstatic void xhci_handle_cmd_nec_get_fw(struct xhci_hcd *xhci,\n\t\tstruct xhci_event_cmd *event)\n{\n\tif (!(xhci->quirks & XHCI_NEC_HOST)) {\n\t\txhci_warn(xhci, \"WARN NEC_GET_FW command on non-NEC host\\n\");\n\t\treturn;\n\t}\n\txhci_dbg_trace(xhci, trace_xhci_dbg_quirks,\n\t\t\t\"NEC firmware version %2x.%02x\",\n\t\t\tNEC_FW_MAJOR(le32_to_cpu(event->status)),\n\t\t\tNEC_FW_MINOR(le32_to_cpu(event->status)));\n}\n\nstatic void xhci_complete_del_and_free_cmd(struct xhci_command *cmd, u32 status)\n{\n\tlist_del(&cmd->cmd_list);\n\n\tif (cmd->completion) {\n\t\tcmd->status = status;\n\t\tcomplete(cmd->completion);\n\t} else {\n\t\tkfree(cmd);\n\t}\n}\n\nvoid xhci_cleanup_command_queue(struct xhci_hcd *xhci)\n{\n\tstruct xhci_command *cur_cmd, *tmp_cmd;\n\txhci->current_cmd = NULL;\n\tlist_for_each_entry_safe(cur_cmd, tmp_cmd, &xhci->cmd_list, cmd_list)\n\t\txhci_complete_del_and_free_cmd(cur_cmd, COMP_COMMAND_ABORTED);\n}\n\nvoid xhci_handle_command_timeout(struct work_struct *work)\n{\n\tstruct xhci_hcd\t*xhci;\n\tunsigned long\tflags;\n\tchar\t\tstr[XHCI_MSG_MAX];\n\tu64\t\thw_ring_state;\n\tu32\t\tcmd_field3;\n\tu32\t\tusbsts;\n\n\txhci = container_of(to_delayed_work(work), struct xhci_hcd, cmd_timer);\n\n\tspin_lock_irqsave(&xhci->lock, flags);\n\n\t \n\tif (!xhci->current_cmd || delayed_work_pending(&xhci->cmd_timer)) {\n\t\tspin_unlock_irqrestore(&xhci->lock, flags);\n\t\treturn;\n\t}\n\n\tcmd_field3 = le32_to_cpu(xhci->current_cmd->command_trb->generic.field[3]);\n\tusbsts = readl(&xhci->op_regs->status);\n\txhci_dbg(xhci, \"Command timeout, USBSTS:%s\\n\", xhci_decode_usbsts(str, usbsts));\n\n\t \n\tif (TRB_FIELD_TO_TYPE(cmd_field3) == TRB_STOP_RING) {\n\t\tstruct xhci_virt_ep\t*ep;\n\n\t\txhci_warn(xhci, \"xHCI host not responding to stop endpoint command\\n\");\n\n\t\tep = xhci_get_virt_ep(xhci, TRB_TO_SLOT_ID(cmd_field3),\n\t\t\t\t      TRB_TO_EP_INDEX(cmd_field3));\n\t\tif (ep)\n\t\t\tep->ep_state &= ~EP_STOP_CMD_PENDING;\n\n\t\txhci_halt(xhci);\n\t\txhci_hc_died(xhci);\n\t\tgoto time_out_completed;\n\t}\n\n\t \n\txhci->current_cmd->status = COMP_COMMAND_ABORTED;\n\n\t \n\thw_ring_state = xhci_read_64(xhci, &xhci->op_regs->cmd_ring);\n\tif (hw_ring_state == ~(u64)0) {\n\t\txhci_hc_died(xhci);\n\t\tgoto time_out_completed;\n\t}\n\n\tif ((xhci->cmd_ring_state & CMD_RING_STATE_RUNNING) &&\n\t    (hw_ring_state & CMD_RING_RUNNING))  {\n\t\t \n\t\txhci->cmd_ring_state = CMD_RING_STATE_ABORTED;\n\t\txhci_dbg(xhci, \"Command timeout\\n\");\n\t\txhci_abort_cmd_ring(xhci, flags);\n\t\tgoto time_out_completed;\n\t}\n\n\t \n\tif (xhci->xhc_state & XHCI_STATE_REMOVING) {\n\t\txhci_dbg(xhci, \"host removed, ring start fail?\\n\");\n\t\txhci_cleanup_command_queue(xhci);\n\n\t\tgoto time_out_completed;\n\t}\n\n\t \n\txhci_dbg(xhci, \"Command timeout on stopped ring\\n\");\n\txhci_handle_stopped_cmd_ring(xhci, xhci->current_cmd);\n\ntime_out_completed:\n\tspin_unlock_irqrestore(&xhci->lock, flags);\n\treturn;\n}\n\nstatic void handle_cmd_completion(struct xhci_hcd *xhci,\n\t\tstruct xhci_event_cmd *event)\n{\n\tunsigned int slot_id = TRB_TO_SLOT_ID(le32_to_cpu(event->flags));\n\tu64 cmd_dma;\n\tdma_addr_t cmd_dequeue_dma;\n\tu32 cmd_comp_code;\n\tunion xhci_trb *cmd_trb;\n\tstruct xhci_command *cmd;\n\tu32 cmd_type;\n\n\tif (slot_id >= MAX_HC_SLOTS) {\n\t\txhci_warn(xhci, \"Invalid slot_id %u\\n\", slot_id);\n\t\treturn;\n\t}\n\n\tcmd_dma = le64_to_cpu(event->cmd_trb);\n\tcmd_trb = xhci->cmd_ring->dequeue;\n\n\ttrace_xhci_handle_command(xhci->cmd_ring, &cmd_trb->generic);\n\n\tcmd_dequeue_dma = xhci_trb_virt_to_dma(xhci->cmd_ring->deq_seg,\n\t\t\tcmd_trb);\n\t \n\tif (!cmd_dequeue_dma || cmd_dma != (u64)cmd_dequeue_dma) {\n\t\txhci_warn(xhci,\n\t\t\t  \"ERROR mismatched command completion event\\n\");\n\t\treturn;\n\t}\n\n\tcmd = list_first_entry(&xhci->cmd_list, struct xhci_command, cmd_list);\n\n\tcancel_delayed_work(&xhci->cmd_timer);\n\n\tcmd_comp_code = GET_COMP_CODE(le32_to_cpu(event->status));\n\n\t \n\tif (cmd_comp_code == COMP_COMMAND_RING_STOPPED) {\n\t\tcomplete_all(&xhci->cmd_ring_stop_completion);\n\t\treturn;\n\t}\n\n\tif (cmd->command_trb != xhci->cmd_ring->dequeue) {\n\t\txhci_err(xhci,\n\t\t\t \"Command completion event does not match command\\n\");\n\t\treturn;\n\t}\n\n\t \n\tif (cmd_comp_code == COMP_COMMAND_ABORTED) {\n\t\txhci->cmd_ring_state = CMD_RING_STATE_STOPPED;\n\t\tif (cmd->status == COMP_COMMAND_ABORTED) {\n\t\t\tif (xhci->current_cmd == cmd)\n\t\t\t\txhci->current_cmd = NULL;\n\t\t\tgoto event_handled;\n\t\t}\n\t}\n\n\tcmd_type = TRB_FIELD_TO_TYPE(le32_to_cpu(cmd_trb->generic.field[3]));\n\tswitch (cmd_type) {\n\tcase TRB_ENABLE_SLOT:\n\t\txhci_handle_cmd_enable_slot(xhci, slot_id, cmd, cmd_comp_code);\n\t\tbreak;\n\tcase TRB_DISABLE_SLOT:\n\t\txhci_handle_cmd_disable_slot(xhci, slot_id);\n\t\tbreak;\n\tcase TRB_CONFIG_EP:\n\t\tif (!cmd->completion)\n\t\t\txhci_handle_cmd_config_ep(xhci, slot_id, cmd_comp_code);\n\t\tbreak;\n\tcase TRB_EVAL_CONTEXT:\n\t\tbreak;\n\tcase TRB_ADDR_DEV:\n\t\txhci_handle_cmd_addr_dev(xhci, slot_id);\n\t\tbreak;\n\tcase TRB_STOP_RING:\n\t\tWARN_ON(slot_id != TRB_TO_SLOT_ID(\n\t\t\t\tle32_to_cpu(cmd_trb->generic.field[3])));\n\t\tif (!cmd->completion)\n\t\t\txhci_handle_cmd_stop_ep(xhci, slot_id, cmd_trb,\n\t\t\t\t\t\tcmd_comp_code);\n\t\tbreak;\n\tcase TRB_SET_DEQ:\n\t\tWARN_ON(slot_id != TRB_TO_SLOT_ID(\n\t\t\t\tle32_to_cpu(cmd_trb->generic.field[3])));\n\t\txhci_handle_cmd_set_deq(xhci, slot_id, cmd_trb, cmd_comp_code);\n\t\tbreak;\n\tcase TRB_CMD_NOOP:\n\t\t \n\t\tif (cmd->status == COMP_COMMAND_RING_STOPPED)\n\t\t\tcmd_comp_code = COMP_COMMAND_RING_STOPPED;\n\t\tbreak;\n\tcase TRB_RESET_EP:\n\t\tWARN_ON(slot_id != TRB_TO_SLOT_ID(\n\t\t\t\tle32_to_cpu(cmd_trb->generic.field[3])));\n\t\txhci_handle_cmd_reset_ep(xhci, slot_id, cmd_trb, cmd_comp_code);\n\t\tbreak;\n\tcase TRB_RESET_DEV:\n\t\t \n\t\tslot_id = TRB_TO_SLOT_ID(\n\t\t\t\tle32_to_cpu(cmd_trb->generic.field[3]));\n\t\txhci_handle_cmd_reset_dev(xhci, slot_id);\n\t\tbreak;\n\tcase TRB_NEC_GET_FW:\n\t\txhci_handle_cmd_nec_get_fw(xhci, event);\n\t\tbreak;\n\tdefault:\n\t\t \n\t\txhci_info(xhci, \"INFO unknown command type %d\\n\", cmd_type);\n\t\tbreak;\n\t}\n\n\t \n\tif (!list_is_singular(&xhci->cmd_list)) {\n\t\txhci->current_cmd = list_first_entry(&cmd->cmd_list,\n\t\t\t\t\t\tstruct xhci_command, cmd_list);\n\t\txhci_mod_cmd_timer(xhci, XHCI_CMD_DEFAULT_TIMEOUT);\n\t} else if (xhci->current_cmd == cmd) {\n\t\txhci->current_cmd = NULL;\n\t}\n\nevent_handled:\n\txhci_complete_del_and_free_cmd(cmd, cmd_comp_code);\n\n\tinc_deq(xhci, xhci->cmd_ring);\n}\n\nstatic void handle_vendor_event(struct xhci_hcd *xhci,\n\t\t\t\tunion xhci_trb *event, u32 trb_type)\n{\n\txhci_dbg(xhci, \"Vendor specific event TRB type = %u\\n\", trb_type);\n\tif (trb_type == TRB_NEC_CMD_COMP && (xhci->quirks & XHCI_NEC_HOST))\n\t\thandle_cmd_completion(xhci, &event->event_cmd);\n}\n\nstatic void handle_device_notification(struct xhci_hcd *xhci,\n\t\tunion xhci_trb *event)\n{\n\tu32 slot_id;\n\tstruct usb_device *udev;\n\n\tslot_id = TRB_TO_SLOT_ID(le32_to_cpu(event->generic.field[3]));\n\tif (!xhci->devs[slot_id]) {\n\t\txhci_warn(xhci, \"Device Notification event for \"\n\t\t\t\t\"unused slot %u\\n\", slot_id);\n\t\treturn;\n\t}\n\n\txhci_dbg(xhci, \"Device Wake Notification event for slot ID %u\\n\",\n\t\t\tslot_id);\n\tudev = xhci->devs[slot_id]->udev;\n\tif (udev && udev->parent)\n\t\tusb_wakeup_notification(udev->parent, udev->portnum);\n}\n\n \nstatic void xhci_cavium_reset_phy_quirk(struct xhci_hcd *xhci)\n{\n\tstruct usb_hcd *hcd = xhci_to_hcd(xhci);\n\tu32 pll_lock_check;\n\tu32 retry_count = 4;\n\n\tdo {\n\t\t \n\t\twritel(0x6F, hcd->regs + 0x1048);\n\t\tudelay(10);\n\t\t \n\t\twritel(0x7F, hcd->regs + 0x1048);\n\t\tudelay(200);\n\t\tpll_lock_check = readl(hcd->regs + 0x1070);\n\t} while (!(pll_lock_check & 0x1) && --retry_count);\n}\n\nstatic void handle_port_status(struct xhci_hcd *xhci,\n\t\t\t       struct xhci_interrupter *ir,\n\t\t\t       union xhci_trb *event)\n{\n\tstruct usb_hcd *hcd;\n\tu32 port_id;\n\tu32 portsc, cmd_reg;\n\tint max_ports;\n\tint slot_id;\n\tunsigned int hcd_portnum;\n\tstruct xhci_bus_state *bus_state;\n\tbool bogus_port_status = false;\n\tstruct xhci_port *port;\n\n\t \n\tif (GET_COMP_CODE(le32_to_cpu(event->generic.field[2])) != COMP_SUCCESS)\n\t\txhci_warn(xhci,\n\t\t\t  \"WARN: xHC returned failed port status event\\n\");\n\n\tport_id = GET_PORT_ID(le32_to_cpu(event->generic.field[0]));\n\tmax_ports = HCS_MAX_PORTS(xhci->hcs_params1);\n\n\tif ((port_id <= 0) || (port_id > max_ports)) {\n\t\txhci_warn(xhci, \"Port change event with invalid port ID %d\\n\",\n\t\t\t  port_id);\n\t\tinc_deq(xhci, ir->event_ring);\n\t\treturn;\n\t}\n\n\tport = &xhci->hw_ports[port_id - 1];\n\tif (!port || !port->rhub || port->hcd_portnum == DUPLICATE_ENTRY) {\n\t\txhci_warn(xhci, \"Port change event, no port for port ID %u\\n\",\n\t\t\t  port_id);\n\t\tbogus_port_status = true;\n\t\tgoto cleanup;\n\t}\n\n\t \n\tif (port->rhub == &xhci->usb3_rhub && xhci->shared_hcd == NULL) {\n\t\txhci_dbg(xhci, \"ignore port event for removed USB3 hcd\\n\");\n\t\tbogus_port_status = true;\n\t\tgoto cleanup;\n\t}\n\n\thcd = port->rhub->hcd;\n\tbus_state = &port->rhub->bus_state;\n\thcd_portnum = port->hcd_portnum;\n\tportsc = readl(port->addr);\n\n\txhci_dbg(xhci, \"Port change event, %d-%d, id %d, portsc: 0x%x\\n\",\n\t\t hcd->self.busnum, hcd_portnum + 1, port_id, portsc);\n\n\ttrace_xhci_handle_port_status(hcd_portnum, portsc);\n\n\tif (hcd->state == HC_STATE_SUSPENDED) {\n\t\txhci_dbg(xhci, \"resume root hub\\n\");\n\t\tusb_hcd_resume_root_hub(hcd);\n\t}\n\n\tif (hcd->speed >= HCD_USB3 &&\n\t    (portsc & PORT_PLS_MASK) == XDEV_INACTIVE) {\n\t\tslot_id = xhci_find_slot_id_by_port(hcd, xhci, hcd_portnum + 1);\n\t\tif (slot_id && xhci->devs[slot_id])\n\t\t\txhci->devs[slot_id]->flags |= VDEV_PORT_ERROR;\n\t}\n\n\tif ((portsc & PORT_PLC) && (portsc & PORT_PLS_MASK) == XDEV_RESUME) {\n\t\txhci_dbg(xhci, \"port resume event for port %d\\n\", port_id);\n\n\t\tcmd_reg = readl(&xhci->op_regs->command);\n\t\tif (!(cmd_reg & CMD_RUN)) {\n\t\t\txhci_warn(xhci, \"xHC is not running.\\n\");\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\tif (DEV_SUPERSPEED_ANY(portsc)) {\n\t\t\txhci_dbg(xhci, \"remote wake SS port %d\\n\", port_id);\n\t\t\t \n\t\t\tbus_state->port_remote_wakeup |= 1 << hcd_portnum;\n\t\t\txhci_test_and_clear_bit(xhci, port, PORT_PLC);\n\t\t\tusb_hcd_start_port_resume(&hcd->self, hcd_portnum);\n\t\t\txhci_set_link_state(xhci, port, XDEV_U0);\n\t\t\t \n\t\t\tbogus_port_status = true;\n\t\t\tgoto cleanup;\n\t\t} else if (!test_bit(hcd_portnum, &bus_state->resuming_ports)) {\n\t\t\txhci_dbg(xhci, \"resume HS port %d\\n\", port_id);\n\t\t\tport->resume_timestamp = jiffies +\n\t\t\t\tmsecs_to_jiffies(USB_RESUME_TIMEOUT);\n\t\t\tset_bit(hcd_portnum, &bus_state->resuming_ports);\n\t\t\t \n\t\t\tset_bit(HCD_FLAG_POLL_RH, &hcd->flags);\n\t\t\tmod_timer(&hcd->rh_timer,\n\t\t\t\t  port->resume_timestamp);\n\t\t\tusb_hcd_start_port_resume(&hcd->self, hcd_portnum);\n\t\t\tbogus_port_status = true;\n\t\t}\n\t}\n\n\tif ((portsc & PORT_PLC) &&\n\t    DEV_SUPERSPEED_ANY(portsc) &&\n\t    ((portsc & PORT_PLS_MASK) == XDEV_U0 ||\n\t     (portsc & PORT_PLS_MASK) == XDEV_U1 ||\n\t     (portsc & PORT_PLS_MASK) == XDEV_U2)) {\n\t\txhci_dbg(xhci, \"resume SS port %d finished\\n\", port_id);\n\t\tcomplete(&port->u3exit_done);\n\t\t \n\t\tslot_id = xhci_find_slot_id_by_port(hcd, xhci, hcd_portnum + 1);\n\t\tif (slot_id && xhci->devs[slot_id])\n\t\t\txhci_ring_device(xhci, slot_id);\n\t\tif (bus_state->port_remote_wakeup & (1 << hcd_portnum)) {\n\t\t\txhci_test_and_clear_bit(xhci, port, PORT_PLC);\n\t\t\tusb_wakeup_notification(hcd->self.root_hub,\n\t\t\t\t\thcd_portnum + 1);\n\t\t\tbogus_port_status = true;\n\t\t\tgoto cleanup;\n\t\t}\n\t}\n\n\t \n\tif (hcd->speed < HCD_USB3 && port->rexit_active) {\n\t\tcomplete(&port->rexit_done);\n\t\tport->rexit_active = false;\n\t\tbogus_port_status = true;\n\t\tgoto cleanup;\n\t}\n\n\tif (hcd->speed < HCD_USB3) {\n\t\txhci_test_and_clear_bit(xhci, port, PORT_PLC);\n\t\tif ((xhci->quirks & XHCI_RESET_PLL_ON_DISCONNECT) &&\n\t\t    (portsc & PORT_CSC) && !(portsc & PORT_CONNECT))\n\t\t\txhci_cavium_reset_phy_quirk(xhci);\n\t}\n\ncleanup:\n\t \n\tinc_deq(xhci, ir->event_ring);\n\n\t \n\tif (bogus_port_status)\n\t\treturn;\n\n\t \n\txhci_dbg(xhci, \"%s: starting usb%d port polling.\\n\",\n\t\t __func__, hcd->self.busnum);\n\tset_bit(HCD_FLAG_POLL_RH, &hcd->flags);\n\tspin_unlock(&xhci->lock);\n\t \n\tusb_hcd_poll_rh_status(hcd);\n\tspin_lock(&xhci->lock);\n}\n\n \nstruct xhci_segment *trb_in_td(struct xhci_hcd *xhci,\n\t\tstruct xhci_segment *start_seg,\n\t\tunion xhci_trb\t*start_trb,\n\t\tunion xhci_trb\t*end_trb,\n\t\tdma_addr_t\tsuspect_dma,\n\t\tbool\t\tdebug)\n{\n\tdma_addr_t start_dma;\n\tdma_addr_t end_seg_dma;\n\tdma_addr_t end_trb_dma;\n\tstruct xhci_segment *cur_seg;\n\n\tstart_dma = xhci_trb_virt_to_dma(start_seg, start_trb);\n\tcur_seg = start_seg;\n\n\tdo {\n\t\tif (start_dma == 0)\n\t\t\treturn NULL;\n\t\t \n\t\tend_seg_dma = xhci_trb_virt_to_dma(cur_seg,\n\t\t\t\t&cur_seg->trbs[TRBS_PER_SEGMENT - 1]);\n\t\t \n\t\tend_trb_dma = xhci_trb_virt_to_dma(cur_seg, end_trb);\n\n\t\tif (debug)\n\t\t\txhci_warn(xhci,\n\t\t\t\t\"Looking for event-dma %016llx trb-start %016llx trb-end %016llx seg-start %016llx seg-end %016llx\\n\",\n\t\t\t\t(unsigned long long)suspect_dma,\n\t\t\t\t(unsigned long long)start_dma,\n\t\t\t\t(unsigned long long)end_trb_dma,\n\t\t\t\t(unsigned long long)cur_seg->dma,\n\t\t\t\t(unsigned long long)end_seg_dma);\n\n\t\tif (end_trb_dma > 0) {\n\t\t\t \n\t\t\tif (start_dma <= end_trb_dma) {\n\t\t\t\tif (suspect_dma >= start_dma && suspect_dma <= end_trb_dma)\n\t\t\t\t\treturn cur_seg;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tif ((suspect_dma >= start_dma &&\n\t\t\t\t\t\t\tsuspect_dma <= end_seg_dma) ||\n\t\t\t\t\t\t(suspect_dma >= cur_seg->dma &&\n\t\t\t\t\t\t suspect_dma <= end_trb_dma))\n\t\t\t\t\treturn cur_seg;\n\t\t\t}\n\t\t\treturn NULL;\n\t\t} else {\n\t\t\t \n\t\t\tif (suspect_dma >= start_dma && suspect_dma <= end_seg_dma)\n\t\t\t\treturn cur_seg;\n\t\t}\n\t\tcur_seg = cur_seg->next;\n\t\tstart_dma = xhci_trb_virt_to_dma(cur_seg, &cur_seg->trbs[0]);\n\t} while (cur_seg != start_seg);\n\n\treturn NULL;\n}\n\nstatic void xhci_clear_hub_tt_buffer(struct xhci_hcd *xhci, struct xhci_td *td,\n\t\tstruct xhci_virt_ep *ep)\n{\n\t \n\tif (td->urb->dev->tt && !usb_pipeint(td->urb->pipe) &&\n\t    (td->urb->dev->tt->hub != xhci_to_hcd(xhci)->self.root_hub) &&\n\t    !(ep->ep_state & EP_CLEARING_TT)) {\n\t\tep->ep_state |= EP_CLEARING_TT;\n\t\ttd->urb->ep->hcpriv = td->urb->dev;\n\t\tif (usb_hub_clear_tt_buffer(td->urb))\n\t\t\tep->ep_state &= ~EP_CLEARING_TT;\n\t}\n}\n\n \nstatic int xhci_requires_manual_halt_cleanup(struct xhci_hcd *xhci,\n\t\tstruct xhci_ep_ctx *ep_ctx,\n\t\tunsigned int trb_comp_code)\n{\n\t \n\tif (trb_comp_code == COMP_USB_TRANSACTION_ERROR ||\n\t\t\ttrb_comp_code == COMP_BABBLE_DETECTED_ERROR ||\n\t\t\ttrb_comp_code == COMP_SPLIT_TRANSACTION_ERROR)\n\t\t \n\t\tif (GET_EP_CTX_STATE(ep_ctx) == EP_STATE_HALTED)\n\t\t\treturn 1;\n\n\treturn 0;\n}\n\nint xhci_is_vendor_info_code(struct xhci_hcd *xhci, unsigned int trb_comp_code)\n{\n\tif (trb_comp_code >= 224 && trb_comp_code <= 255) {\n\t\t \n\t\txhci_dbg(xhci, \"Vendor defined info completion code %u\\n\",\n\t\t\t\ttrb_comp_code);\n\t\txhci_dbg(xhci, \"Treating code as success.\\n\");\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic int finish_td(struct xhci_hcd *xhci, struct xhci_virt_ep *ep,\n\t\t     struct xhci_ring *ep_ring, struct xhci_td *td,\n\t\t     u32 trb_comp_code)\n{\n\tstruct xhci_ep_ctx *ep_ctx;\n\n\tep_ctx = xhci_get_ep_ctx(xhci, ep->vdev->out_ctx, ep->ep_index);\n\n\tswitch (trb_comp_code) {\n\tcase COMP_STOPPED_LENGTH_INVALID:\n\tcase COMP_STOPPED_SHORT_PACKET:\n\tcase COMP_STOPPED:\n\t\t \n\t\treturn 0;\n\tcase COMP_USB_TRANSACTION_ERROR:\n\tcase COMP_BABBLE_DETECTED_ERROR:\n\tcase COMP_SPLIT_TRANSACTION_ERROR:\n\t\t \n\t\tif (GET_EP_CTX_STATE(ep_ctx) != EP_STATE_HALTED) {\n\t\t\t \n\t\t\tif ((ep->ep_state & EP_HALTED) &&\n\t\t\t    !list_empty(&td->cancelled_td_list)) {\n\t\t\t\txhci_dbg(xhci, \"Already resolving halted ep for 0x%llx\\n\",\n\t\t\t\t\t (unsigned long long)xhci_trb_virt_to_dma(\n\t\t\t\t\t\t td->start_seg, td->first_trb));\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t\t \n\t\txhci_clear_hub_tt_buffer(xhci, td, ep);\n\t\txhci_handle_halted_endpoint(xhci, ep, td, EP_HARD_RESET);\n\t\treturn 0;\n\tcase COMP_STALL_ERROR:\n\t\t \n\t\tif (ep->ep_index != 0)\n\t\t\txhci_clear_hub_tt_buffer(xhci, td, ep);\n\n\t\txhci_handle_halted_endpoint(xhci, ep, td, EP_HARD_RESET);\n\n\t\treturn 0;  \n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tep_ring->dequeue = td->last_trb;\n\tep_ring->deq_seg = td->last_trb_seg;\n\tinc_deq(xhci, ep_ring);\n\n\treturn xhci_td_cleanup(xhci, td, ep_ring, td->status);\n}\n\n \nstatic int sum_trb_lengths(struct xhci_hcd *xhci, struct xhci_ring *ring,\n\t\t\t   union xhci_trb *stop_trb)\n{\n\tu32 sum;\n\tunion xhci_trb *trb = ring->dequeue;\n\tstruct xhci_segment *seg = ring->deq_seg;\n\n\tfor (sum = 0; trb != stop_trb; next_trb(xhci, ring, &seg, &trb)) {\n\t\tif (!trb_is_noop(trb) && !trb_is_link(trb))\n\t\t\tsum += TRB_LEN(le32_to_cpu(trb->generic.field[2]));\n\t}\n\treturn sum;\n}\n\n \nstatic int process_ctrl_td(struct xhci_hcd *xhci, struct xhci_virt_ep *ep,\n\t\tstruct xhci_ring *ep_ring,  struct xhci_td *td,\n\t\t\t   union xhci_trb *ep_trb, struct xhci_transfer_event *event)\n{\n\tstruct xhci_ep_ctx *ep_ctx;\n\tu32 trb_comp_code;\n\tu32 remaining, requested;\n\tu32 trb_type;\n\n\ttrb_type = TRB_FIELD_TO_TYPE(le32_to_cpu(ep_trb->generic.field[3]));\n\tep_ctx = xhci_get_ep_ctx(xhci, ep->vdev->out_ctx, ep->ep_index);\n\ttrb_comp_code = GET_COMP_CODE(le32_to_cpu(event->transfer_len));\n\trequested = td->urb->transfer_buffer_length;\n\tremaining = EVENT_TRB_LEN(le32_to_cpu(event->transfer_len));\n\n\tswitch (trb_comp_code) {\n\tcase COMP_SUCCESS:\n\t\tif (trb_type != TRB_STATUS) {\n\t\t\txhci_warn(xhci, \"WARN: Success on ctrl %s TRB without IOC set?\\n\",\n\t\t\t\t  (trb_type == TRB_DATA) ? \"data\" : \"setup\");\n\t\t\ttd->status = -ESHUTDOWN;\n\t\t\tbreak;\n\t\t}\n\t\ttd->status = 0;\n\t\tbreak;\n\tcase COMP_SHORT_PACKET:\n\t\ttd->status = 0;\n\t\tbreak;\n\tcase COMP_STOPPED_SHORT_PACKET:\n\t\tif (trb_type == TRB_DATA || trb_type == TRB_NORMAL)\n\t\t\ttd->urb->actual_length = remaining;\n\t\telse\n\t\t\txhci_warn(xhci, \"WARN: Stopped Short Packet on ctrl setup or status TRB\\n\");\n\t\tgoto finish_td;\n\tcase COMP_STOPPED:\n\t\tswitch (trb_type) {\n\t\tcase TRB_SETUP:\n\t\t\ttd->urb->actual_length = 0;\n\t\t\tgoto finish_td;\n\t\tcase TRB_DATA:\n\t\tcase TRB_NORMAL:\n\t\t\ttd->urb->actual_length = requested - remaining;\n\t\t\tgoto finish_td;\n\t\tcase TRB_STATUS:\n\t\t\ttd->urb->actual_length = requested;\n\t\t\tgoto finish_td;\n\t\tdefault:\n\t\t\txhci_warn(xhci, \"WARN: unexpected TRB Type %d\\n\",\n\t\t\t\t  trb_type);\n\t\t\tgoto finish_td;\n\t\t}\n\tcase COMP_STOPPED_LENGTH_INVALID:\n\t\tgoto finish_td;\n\tdefault:\n\t\tif (!xhci_requires_manual_halt_cleanup(xhci,\n\t\t\t\t\t\t       ep_ctx, trb_comp_code))\n\t\t\tbreak;\n\t\txhci_dbg(xhci, \"TRB error %u, halted endpoint index = %u\\n\",\n\t\t\t trb_comp_code, ep->ep_index);\n\t\tfallthrough;\n\tcase COMP_STALL_ERROR:\n\t\t \n\t\tif (trb_type == TRB_DATA || trb_type == TRB_NORMAL)\n\t\t\ttd->urb->actual_length = requested - remaining;\n\t\telse if (!td->urb_length_set)\n\t\t\ttd->urb->actual_length = 0;\n\t\tgoto finish_td;\n\t}\n\n\t \n\tif (trb_type == TRB_SETUP)\n\t\tgoto finish_td;\n\n\t \n\tif (trb_type == TRB_DATA ||\n\t\ttrb_type == TRB_NORMAL) {\n\t\ttd->urb_length_set = true;\n\t\ttd->urb->actual_length = requested - remaining;\n\t\txhci_dbg(xhci, \"Waiting for status stage event\\n\");\n\t\treturn 0;\n\t}\n\n\t \n\tif (!td->urb_length_set)\n\t\ttd->urb->actual_length = requested;\n\nfinish_td:\n\treturn finish_td(xhci, ep, ep_ring, td, trb_comp_code);\n}\n\n \nstatic int process_isoc_td(struct xhci_hcd *xhci, struct xhci_virt_ep *ep,\n\t\tstruct xhci_ring *ep_ring, struct xhci_td *td,\n\t\tunion xhci_trb *ep_trb, struct xhci_transfer_event *event)\n{\n\tstruct urb_priv *urb_priv;\n\tint idx;\n\tstruct usb_iso_packet_descriptor *frame;\n\tu32 trb_comp_code;\n\tbool sum_trbs_for_length = false;\n\tu32 remaining, requested, ep_trb_len;\n\tint short_framestatus;\n\n\ttrb_comp_code = GET_COMP_CODE(le32_to_cpu(event->transfer_len));\n\turb_priv = td->urb->hcpriv;\n\tidx = urb_priv->num_tds_done;\n\tframe = &td->urb->iso_frame_desc[idx];\n\trequested = frame->length;\n\tremaining = EVENT_TRB_LEN(le32_to_cpu(event->transfer_len));\n\tep_trb_len = TRB_LEN(le32_to_cpu(ep_trb->generic.field[2]));\n\tshort_framestatus = td->urb->transfer_flags & URB_SHORT_NOT_OK ?\n\t\t-EREMOTEIO : 0;\n\n\t \n\tswitch (trb_comp_code) {\n\tcase COMP_SUCCESS:\n\t\tif (remaining) {\n\t\t\tframe->status = short_framestatus;\n\t\t\tif (xhci->quirks & XHCI_TRUST_TX_LENGTH)\n\t\t\t\tsum_trbs_for_length = true;\n\t\t\tbreak;\n\t\t}\n\t\tframe->status = 0;\n\t\tbreak;\n\tcase COMP_SHORT_PACKET:\n\t\tframe->status = short_framestatus;\n\t\tsum_trbs_for_length = true;\n\t\tbreak;\n\tcase COMP_BANDWIDTH_OVERRUN_ERROR:\n\t\tframe->status = -ECOMM;\n\t\tbreak;\n\tcase COMP_ISOCH_BUFFER_OVERRUN:\n\tcase COMP_BABBLE_DETECTED_ERROR:\n\t\tframe->status = -EOVERFLOW;\n\t\tbreak;\n\tcase COMP_INCOMPATIBLE_DEVICE_ERROR:\n\tcase COMP_STALL_ERROR:\n\t\tframe->status = -EPROTO;\n\t\tbreak;\n\tcase COMP_USB_TRANSACTION_ERROR:\n\t\tframe->status = -EPROTO;\n\t\tif (ep_trb != td->last_trb)\n\t\t\treturn 0;\n\t\tbreak;\n\tcase COMP_STOPPED:\n\t\tsum_trbs_for_length = true;\n\t\tbreak;\n\tcase COMP_STOPPED_SHORT_PACKET:\n\t\t \n\t\tframe->status = short_framestatus;\n\t\trequested = remaining;\n\t\tbreak;\n\tcase COMP_STOPPED_LENGTH_INVALID:\n\t\trequested = 0;\n\t\tremaining = 0;\n\t\tbreak;\n\tdefault:\n\t\tsum_trbs_for_length = true;\n\t\tframe->status = -1;\n\t\tbreak;\n\t}\n\n\tif (sum_trbs_for_length)\n\t\tframe->actual_length = sum_trb_lengths(xhci, ep->ring, ep_trb) +\n\t\t\tep_trb_len - remaining;\n\telse\n\t\tframe->actual_length = requested;\n\n\ttd->urb->actual_length += frame->actual_length;\n\n\treturn finish_td(xhci, ep, ep_ring, td, trb_comp_code);\n}\n\nstatic int skip_isoc_td(struct xhci_hcd *xhci, struct xhci_td *td,\n\t\t\tstruct xhci_virt_ep *ep, int status)\n{\n\tstruct urb_priv *urb_priv;\n\tstruct usb_iso_packet_descriptor *frame;\n\tint idx;\n\n\turb_priv = td->urb->hcpriv;\n\tidx = urb_priv->num_tds_done;\n\tframe = &td->urb->iso_frame_desc[idx];\n\n\t \n\tframe->status = -EXDEV;\n\n\t \n\tframe->actual_length = 0;\n\n\t \n\tep->ring->dequeue = td->last_trb;\n\tep->ring->deq_seg = td->last_trb_seg;\n\tinc_deq(xhci, ep->ring);\n\n\treturn xhci_td_cleanup(xhci, td, ep->ring, status);\n}\n\n \nstatic int process_bulk_intr_td(struct xhci_hcd *xhci, struct xhci_virt_ep *ep,\n\t\tstruct xhci_ring *ep_ring, struct xhci_td *td,\n\t\tunion xhci_trb *ep_trb, struct xhci_transfer_event *event)\n{\n\tstruct xhci_slot_ctx *slot_ctx;\n\tu32 trb_comp_code;\n\tu32 remaining, requested, ep_trb_len;\n\n\tslot_ctx = xhci_get_slot_ctx(xhci, ep->vdev->out_ctx);\n\ttrb_comp_code = GET_COMP_CODE(le32_to_cpu(event->transfer_len));\n\tremaining = EVENT_TRB_LEN(le32_to_cpu(event->transfer_len));\n\tep_trb_len = TRB_LEN(le32_to_cpu(ep_trb->generic.field[2]));\n\trequested = td->urb->transfer_buffer_length;\n\n\tswitch (trb_comp_code) {\n\tcase COMP_SUCCESS:\n\t\tep->err_count = 0;\n\t\t \n\t\tif (ep_trb != td->last_trb || remaining) {\n\t\t\txhci_warn(xhci, \"WARN Successful completion on short TX\\n\");\n\t\t\txhci_dbg(xhci, \"ep %#x - asked for %d bytes, %d bytes untransferred\\n\",\n\t\t\t\t td->urb->ep->desc.bEndpointAddress,\n\t\t\t\t requested, remaining);\n\t\t}\n\t\ttd->status = 0;\n\t\tbreak;\n\tcase COMP_SHORT_PACKET:\n\t\txhci_dbg(xhci, \"ep %#x - asked for %d bytes, %d bytes untransferred\\n\",\n\t\t\t td->urb->ep->desc.bEndpointAddress,\n\t\t\t requested, remaining);\n\t\ttd->status = 0;\n\t\tbreak;\n\tcase COMP_STOPPED_SHORT_PACKET:\n\t\ttd->urb->actual_length = remaining;\n\t\tgoto finish_td;\n\tcase COMP_STOPPED_LENGTH_INVALID:\n\t\t \n\t\tep_trb_len\t= 0;\n\t\tremaining\t= 0;\n\t\tbreak;\n\tcase COMP_USB_TRANSACTION_ERROR:\n\t\tif (xhci->quirks & XHCI_NO_SOFT_RETRY ||\n\t\t    (ep->err_count++ > MAX_SOFT_RETRY) ||\n\t\t    le32_to_cpu(slot_ctx->tt_info) & TT_SLOT)\n\t\t\tbreak;\n\n\t\ttd->status = 0;\n\n\t\txhci_handle_halted_endpoint(xhci, ep, td, EP_SOFT_RESET);\n\t\treturn 0;\n\tdefault:\n\t\t \n\t\tbreak;\n\t}\n\n\tif (ep_trb == td->last_trb)\n\t\ttd->urb->actual_length = requested - remaining;\n\telse\n\t\ttd->urb->actual_length =\n\t\t\tsum_trb_lengths(xhci, ep_ring, ep_trb) +\n\t\t\tep_trb_len - remaining;\nfinish_td:\n\tif (remaining > requested) {\n\t\txhci_warn(xhci, \"bad transfer trb length %d in event trb\\n\",\n\t\t\t  remaining);\n\t\ttd->urb->actual_length = 0;\n\t}\n\n\treturn finish_td(xhci, ep, ep_ring, td, trb_comp_code);\n}\n\n \nstatic int handle_tx_event(struct xhci_hcd *xhci,\n\t\t\t   struct xhci_interrupter *ir,\n\t\t\t   struct xhci_transfer_event *event)\n{\n\tstruct xhci_virt_ep *ep;\n\tstruct xhci_ring *ep_ring;\n\tunsigned int slot_id;\n\tint ep_index;\n\tstruct xhci_td *td = NULL;\n\tdma_addr_t ep_trb_dma;\n\tstruct xhci_segment *ep_seg;\n\tunion xhci_trb *ep_trb;\n\tint status = -EINPROGRESS;\n\tstruct xhci_ep_ctx *ep_ctx;\n\tu32 trb_comp_code;\n\tint td_num = 0;\n\tbool handling_skipped_tds = false;\n\n\tslot_id = TRB_TO_SLOT_ID(le32_to_cpu(event->flags));\n\tep_index = TRB_TO_EP_ID(le32_to_cpu(event->flags)) - 1;\n\ttrb_comp_code = GET_COMP_CODE(le32_to_cpu(event->transfer_len));\n\tep_trb_dma = le64_to_cpu(event->buffer);\n\n\tep = xhci_get_virt_ep(xhci, slot_id, ep_index);\n\tif (!ep) {\n\t\txhci_err(xhci, \"ERROR Invalid Transfer event\\n\");\n\t\tgoto err_out;\n\t}\n\n\tep_ring = xhci_dma_to_transfer_ring(ep, ep_trb_dma);\n\tep_ctx = xhci_get_ep_ctx(xhci, ep->vdev->out_ctx, ep_index);\n\n\tif (GET_EP_CTX_STATE(ep_ctx) == EP_STATE_DISABLED) {\n\t\txhci_err(xhci,\n\t\t\t \"ERROR Transfer event for disabled endpoint slot %u ep %u\\n\",\n\t\t\t  slot_id, ep_index);\n\t\tgoto err_out;\n\t}\n\n\t \n\tif (!ep_ring) {\n\t\tswitch (trb_comp_code) {\n\t\tcase COMP_STALL_ERROR:\n\t\tcase COMP_USB_TRANSACTION_ERROR:\n\t\tcase COMP_INVALID_STREAM_TYPE_ERROR:\n\t\tcase COMP_INVALID_STREAM_ID_ERROR:\n\t\t\txhci_dbg(xhci, \"Stream transaction error ep %u no id\\n\",\n\t\t\t\t ep_index);\n\t\t\tif (ep->err_count++ > MAX_SOFT_RETRY)\n\t\t\t\txhci_handle_halted_endpoint(xhci, ep, NULL,\n\t\t\t\t\t\t\t    EP_HARD_RESET);\n\t\t\telse\n\t\t\t\txhci_handle_halted_endpoint(xhci, ep, NULL,\n\t\t\t\t\t\t\t    EP_SOFT_RESET);\n\t\t\tgoto cleanup;\n\t\tcase COMP_RING_UNDERRUN:\n\t\tcase COMP_RING_OVERRUN:\n\t\tcase COMP_STOPPED_LENGTH_INVALID:\n\t\t\tgoto cleanup;\n\t\tdefault:\n\t\t\txhci_err(xhci, \"ERROR Transfer event for unknown stream ring slot %u ep %u\\n\",\n\t\t\t\t slot_id, ep_index);\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\t \n\tif (ep->skip)\n\t\ttd_num += list_count_nodes(&ep_ring->td_list);\n\n\t \n\tswitch (trb_comp_code) {\n\t \n\tcase COMP_SUCCESS:\n\t\tif (EVENT_TRB_LEN(le32_to_cpu(event->transfer_len)) == 0)\n\t\t\tbreak;\n\t\tif (xhci->quirks & XHCI_TRUST_TX_LENGTH ||\n\t\t    ep_ring->last_td_was_short)\n\t\t\ttrb_comp_code = COMP_SHORT_PACKET;\n\t\telse\n\t\t\txhci_warn_ratelimited(xhci,\n\t\t\t\t\t      \"WARN Successful completion on short TX for slot %u ep %u: needs XHCI_TRUST_TX_LENGTH quirk?\\n\",\n\t\t\t\t\t      slot_id, ep_index);\n\t\tbreak;\n\tcase COMP_SHORT_PACKET:\n\t\tbreak;\n\t \n\tcase COMP_STOPPED:\n\t\txhci_dbg(xhci, \"Stopped on Transfer TRB for slot %u ep %u\\n\",\n\t\t\t slot_id, ep_index);\n\t\tbreak;\n\tcase COMP_STOPPED_LENGTH_INVALID:\n\t\txhci_dbg(xhci,\n\t\t\t \"Stopped on No-op or Link TRB for slot %u ep %u\\n\",\n\t\t\t slot_id, ep_index);\n\t\tbreak;\n\tcase COMP_STOPPED_SHORT_PACKET:\n\t\txhci_dbg(xhci,\n\t\t\t \"Stopped with short packet transfer detected for slot %u ep %u\\n\",\n\t\t\t slot_id, ep_index);\n\t\tbreak;\n\t \n\tcase COMP_STALL_ERROR:\n\t\txhci_dbg(xhci, \"Stalled endpoint for slot %u ep %u\\n\", slot_id,\n\t\t\t ep_index);\n\t\tstatus = -EPIPE;\n\t\tbreak;\n\tcase COMP_SPLIT_TRANSACTION_ERROR:\n\t\txhci_dbg(xhci, \"Split transaction error for slot %u ep %u\\n\",\n\t\t\t slot_id, ep_index);\n\t\tstatus = -EPROTO;\n\t\tbreak;\n\tcase COMP_USB_TRANSACTION_ERROR:\n\t\txhci_dbg(xhci, \"Transfer error for slot %u ep %u on endpoint\\n\",\n\t\t\t slot_id, ep_index);\n\t\tstatus = -EPROTO;\n\t\tbreak;\n\tcase COMP_BABBLE_DETECTED_ERROR:\n\t\txhci_dbg(xhci, \"Babble error for slot %u ep %u on endpoint\\n\",\n\t\t\t slot_id, ep_index);\n\t\tstatus = -EOVERFLOW;\n\t\tbreak;\n\t \n\tcase COMP_TRB_ERROR:\n\t\txhci_warn(xhci,\n\t\t\t  \"WARN: TRB error for slot %u ep %u on endpoint\\n\",\n\t\t\t  slot_id, ep_index);\n\t\tstatus = -EILSEQ;\n\t\tbreak;\n\t \n\tcase COMP_DATA_BUFFER_ERROR:\n\t\txhci_warn(xhci,\n\t\t\t  \"WARN: HC couldn't access mem fast enough for slot %u ep %u\\n\",\n\t\t\t  slot_id, ep_index);\n\t\tstatus = -ENOSR;\n\t\tbreak;\n\tcase COMP_BANDWIDTH_OVERRUN_ERROR:\n\t\txhci_warn(xhci,\n\t\t\t  \"WARN: bandwidth overrun event for slot %u ep %u on endpoint\\n\",\n\t\t\t  slot_id, ep_index);\n\t\tbreak;\n\tcase COMP_ISOCH_BUFFER_OVERRUN:\n\t\txhci_warn(xhci,\n\t\t\t  \"WARN: buffer overrun event for slot %u ep %u on endpoint\",\n\t\t\t  slot_id, ep_index);\n\t\tbreak;\n\tcase COMP_RING_UNDERRUN:\n\t\t \n\t\txhci_dbg(xhci, \"underrun event on endpoint\\n\");\n\t\tif (!list_empty(&ep_ring->td_list))\n\t\t\txhci_dbg(xhci, \"Underrun Event for slot %d ep %d \"\n\t\t\t\t\t\"still with TDs queued?\\n\",\n\t\t\t\t TRB_TO_SLOT_ID(le32_to_cpu(event->flags)),\n\t\t\t\t ep_index);\n\t\tgoto cleanup;\n\tcase COMP_RING_OVERRUN:\n\t\txhci_dbg(xhci, \"overrun event on endpoint\\n\");\n\t\tif (!list_empty(&ep_ring->td_list))\n\t\t\txhci_dbg(xhci, \"Overrun Event for slot %d ep %d \"\n\t\t\t\t\t\"still with TDs queued?\\n\",\n\t\t\t\t TRB_TO_SLOT_ID(le32_to_cpu(event->flags)),\n\t\t\t\t ep_index);\n\t\tgoto cleanup;\n\tcase COMP_MISSED_SERVICE_ERROR:\n\t\t \n\t\tep->skip = true;\n\t\txhci_dbg(xhci,\n\t\t\t \"Miss service interval error for slot %u ep %u, set skip flag\\n\",\n\t\t\t slot_id, ep_index);\n\t\tgoto cleanup;\n\tcase COMP_NO_PING_RESPONSE_ERROR:\n\t\tep->skip = true;\n\t\txhci_dbg(xhci,\n\t\t\t \"No Ping response error for slot %u ep %u, Skip one Isoc TD\\n\",\n\t\t\t slot_id, ep_index);\n\t\tgoto cleanup;\n\n\tcase COMP_INCOMPATIBLE_DEVICE_ERROR:\n\t\t \n\t\txhci_warn(xhci,\n\t\t\t  \"WARN: detect an incompatible device for slot %u ep %u\",\n\t\t\t  slot_id, ep_index);\n\t\tstatus = -EPROTO;\n\t\tbreak;\n\tdefault:\n\t\tif (xhci_is_vendor_info_code(xhci, trb_comp_code)) {\n\t\t\tstatus = 0;\n\t\t\tbreak;\n\t\t}\n\t\txhci_warn(xhci,\n\t\t\t  \"ERROR Unknown event condition %u for slot %u ep %u , HC probably busted\\n\",\n\t\t\t  trb_comp_code, slot_id, ep_index);\n\t\tgoto cleanup;\n\t}\n\n\tdo {\n\t\t \n\t\tif (list_empty(&ep_ring->td_list)) {\n\t\t\t \n\n\t\t\tif (!(trb_comp_code == COMP_STOPPED ||\n\t\t\t      trb_comp_code == COMP_STOPPED_LENGTH_INVALID ||\n\t\t\t      ep_ring->last_td_was_short)) {\n\t\t\t\txhci_warn(xhci, \"WARN Event TRB for slot %d ep %d with no TDs queued?\\n\",\n\t\t\t\t\t\tTRB_TO_SLOT_ID(le32_to_cpu(event->flags)),\n\t\t\t\t\t\tep_index);\n\t\t\t}\n\t\t\tif (ep->skip) {\n\t\t\t\tep->skip = false;\n\t\t\t\txhci_dbg(xhci, \"td_list is empty while skip flag set. Clear skip flag for slot %u ep %u.\\n\",\n\t\t\t\t\t slot_id, ep_index);\n\t\t\t}\n\t\t\tif (trb_comp_code == COMP_STALL_ERROR ||\n\t\t\t    xhci_requires_manual_halt_cleanup(xhci, ep_ctx,\n\t\t\t\t\t\t\t      trb_comp_code)) {\n\t\t\t\txhci_handle_halted_endpoint(xhci, ep, NULL,\n\t\t\t\t\t\t\t    EP_HARD_RESET);\n\t\t\t}\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\t \n\t\tif (ep->skip && td_num == 0) {\n\t\t\tep->skip = false;\n\t\t\txhci_dbg(xhci, \"All tds on the ep_ring skipped. Clear skip flag for slot %u ep %u.\\n\",\n\t\t\t\t slot_id, ep_index);\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\ttd = list_first_entry(&ep_ring->td_list, struct xhci_td,\n\t\t\t\t      td_list);\n\t\tif (ep->skip)\n\t\t\ttd_num--;\n\n\t\t \n\t\tep_seg = trb_in_td(xhci, ep_ring->deq_seg, ep_ring->dequeue,\n\t\t\t\ttd->last_trb, ep_trb_dma, false);\n\n\t\t \n\t\tif (!ep_seg && (trb_comp_code == COMP_STOPPED ||\n\t\t\t   trb_comp_code == COMP_STOPPED_LENGTH_INVALID)) {\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\tif (!ep_seg) {\n\t\t\tif (!ep->skip ||\n\t\t\t    !usb_endpoint_xfer_isoc(&td->urb->ep->desc)) {\n\t\t\t\t \n\t\t\t\tif ((xhci->quirks & XHCI_SPURIOUS_SUCCESS) &&\n\t\t\t\t\t\tep_ring->last_td_was_short) {\n\t\t\t\t\tep_ring->last_td_was_short = false;\n\t\t\t\t\tgoto cleanup;\n\t\t\t\t}\n\t\t\t\t \n\t\t\t\txhci_err(xhci,\n\t\t\t\t\t\"ERROR Transfer event TRB DMA ptr not \"\n\t\t\t\t\t\"part of current TD ep_index %d \"\n\t\t\t\t\t\"comp_code %u\\n\", ep_index,\n\t\t\t\t\ttrb_comp_code);\n\t\t\t\ttrb_in_td(xhci, ep_ring->deq_seg,\n\t\t\t\t\t  ep_ring->dequeue, td->last_trb,\n\t\t\t\t\t  ep_trb_dma, true);\n\t\t\t\treturn -ESHUTDOWN;\n\t\t\t}\n\n\t\t\tskip_isoc_td(xhci, td, ep, status);\n\t\t\tgoto cleanup;\n\t\t}\n\t\tif (trb_comp_code == COMP_SHORT_PACKET)\n\t\t\tep_ring->last_td_was_short = true;\n\t\telse\n\t\t\tep_ring->last_td_was_short = false;\n\n\t\tif (ep->skip) {\n\t\t\txhci_dbg(xhci,\n\t\t\t\t \"Found td. Clear skip flag for slot %u ep %u.\\n\",\n\t\t\t\t slot_id, ep_index);\n\t\t\tep->skip = false;\n\t\t}\n\n\t\tep_trb = &ep_seg->trbs[(ep_trb_dma - ep_seg->dma) /\n\t\t\t\t\t\tsizeof(*ep_trb)];\n\n\t\ttrace_xhci_handle_transfer(ep_ring,\n\t\t\t\t(struct xhci_generic_trb *) ep_trb);\n\n\t\t \n\n\t\tif (trb_is_noop(ep_trb)) {\n\t\t\tif (trb_comp_code == COMP_STALL_ERROR ||\n\t\t\t    xhci_requires_manual_halt_cleanup(xhci, ep_ctx,\n\t\t\t\t\t\t\t      trb_comp_code))\n\t\t\t\txhci_handle_halted_endpoint(xhci, ep, td,\n\t\t\t\t\t\t\t    EP_HARD_RESET);\n\t\t\tgoto cleanup;\n\t\t}\n\n\t\ttd->status = status;\n\n\t\t \n\t\tif (usb_endpoint_xfer_control(&td->urb->ep->desc))\n\t\t\tprocess_ctrl_td(xhci, ep, ep_ring, td, ep_trb, event);\n\t\telse if (usb_endpoint_xfer_isoc(&td->urb->ep->desc))\n\t\t\tprocess_isoc_td(xhci, ep, ep_ring, td, ep_trb, event);\n\t\telse\n\t\t\tprocess_bulk_intr_td(xhci, ep, ep_ring, td, ep_trb, event);\ncleanup:\n\t\thandling_skipped_tds = ep->skip &&\n\t\t\ttrb_comp_code != COMP_MISSED_SERVICE_ERROR &&\n\t\t\ttrb_comp_code != COMP_NO_PING_RESPONSE_ERROR;\n\n\t\t \n\t\tif (!handling_skipped_tds)\n\t\t\tinc_deq(xhci, ir->event_ring);\n\n\t \n\t} while (handling_skipped_tds);\n\n\treturn 0;\n\nerr_out:\n\txhci_err(xhci, \"@%016llx %08x %08x %08x %08x\\n\",\n\t\t (unsigned long long) xhci_trb_virt_to_dma(\n\t\t\t ir->event_ring->deq_seg,\n\t\t\t ir->event_ring->dequeue),\n\t\t lower_32_bits(le64_to_cpu(event->buffer)),\n\t\t upper_32_bits(le64_to_cpu(event->buffer)),\n\t\t le32_to_cpu(event->transfer_len),\n\t\t le32_to_cpu(event->flags));\n\treturn -ENODEV;\n}\n\n \nstatic int xhci_handle_event(struct xhci_hcd *xhci, struct xhci_interrupter *ir)\n{\n\tunion xhci_trb *event;\n\tint update_ptrs = 1;\n\tu32 trb_type;\n\tint ret;\n\n\t \n\tif (!ir || !ir->event_ring || !ir->event_ring->dequeue) {\n\t\txhci_err(xhci, \"ERROR interrupter not ready\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tevent = ir->event_ring->dequeue;\n\t \n\tif ((le32_to_cpu(event->event_cmd.flags) & TRB_CYCLE) !=\n\t    ir->event_ring->cycle_state)\n\t\treturn 0;\n\n\ttrace_xhci_handle_event(ir->event_ring, &event->generic);\n\n\t \n\trmb();\n\ttrb_type = TRB_FIELD_TO_TYPE(le32_to_cpu(event->event_cmd.flags));\n\t \n\n\tswitch (trb_type) {\n\tcase TRB_COMPLETION:\n\t\thandle_cmd_completion(xhci, &event->event_cmd);\n\t\tbreak;\n\tcase TRB_PORT_STATUS:\n\t\thandle_port_status(xhci, ir, event);\n\t\tupdate_ptrs = 0;\n\t\tbreak;\n\tcase TRB_TRANSFER:\n\t\tret = handle_tx_event(xhci, ir, &event->trans_event);\n\t\tif (ret >= 0)\n\t\t\tupdate_ptrs = 0;\n\t\tbreak;\n\tcase TRB_DEV_NOTE:\n\t\thandle_device_notification(xhci, event);\n\t\tbreak;\n\tdefault:\n\t\tif (trb_type >= TRB_VENDOR_DEFINED_LOW)\n\t\t\thandle_vendor_event(xhci, event, trb_type);\n\t\telse\n\t\t\txhci_warn(xhci, \"ERROR unknown event type %d\\n\", trb_type);\n\t}\n\t \n\tif (xhci->xhc_state & XHCI_STATE_DYING) {\n\t\txhci_dbg(xhci, \"xHCI host dying, returning from \"\n\t\t\t\t\"event handler.\\n\");\n\t\treturn 0;\n\t}\n\n\tif (update_ptrs)\n\t\t \n\t\tinc_deq(xhci, ir->event_ring);\n\n\t \n\treturn 1;\n}\n\n \nstatic void xhci_update_erst_dequeue(struct xhci_hcd *xhci,\n\t\t\t\t     struct xhci_interrupter *ir,\n\t\t\t\t     union xhci_trb *event_ring_deq,\n\t\t\t\t     bool clear_ehb)\n{\n\tu64 temp_64;\n\tdma_addr_t deq;\n\n\ttemp_64 = xhci_read_64(xhci, &ir->ir_set->erst_dequeue);\n\t \n\tif (event_ring_deq != ir->event_ring->dequeue) {\n\t\tdeq = xhci_trb_virt_to_dma(ir->event_ring->deq_seg,\n\t\t\t\tir->event_ring->dequeue);\n\t\tif (deq == 0)\n\t\t\txhci_warn(xhci, \"WARN something wrong with SW event ring dequeue ptr\\n\");\n\t\t \n\t\tif ((temp_64 & (u64) ~ERST_PTR_MASK) ==\n\t\t\t\t((u64) deq & (u64) ~ERST_PTR_MASK))\n\t\t\treturn;\n\n\t\t \n\t\ttemp_64 &= ERST_DESI_MASK;\n\t\ttemp_64 |= ((u64) deq & (u64) ~ERST_PTR_MASK);\n\t}\n\n\t \n\tif (clear_ehb)\n\t\ttemp_64 |= ERST_EHB;\n\txhci_write_64(xhci, temp_64, &ir->ir_set->erst_dequeue);\n}\n\n \nirqreturn_t xhci_irq(struct usb_hcd *hcd)\n{\n\tstruct xhci_hcd *xhci = hcd_to_xhci(hcd);\n\tunion xhci_trb *event_ring_deq;\n\tstruct xhci_interrupter *ir;\n\tirqreturn_t ret = IRQ_NONE;\n\tu64 temp_64;\n\tu32 status;\n\tint event_loop = 0;\n\n\tspin_lock(&xhci->lock);\n\t \n\tstatus = readl(&xhci->op_regs->status);\n\tif (status == ~(u32)0) {\n\t\txhci_hc_died(xhci);\n\t\tret = IRQ_HANDLED;\n\t\tgoto out;\n\t}\n\n\tif (!(status & STS_EINT))\n\t\tgoto out;\n\n\tif (status & STS_HCE) {\n\t\txhci_warn(xhci, \"WARNING: Host Controller Error\\n\");\n\t\tgoto out;\n\t}\n\n\tif (status & STS_FATAL) {\n\t\txhci_warn(xhci, \"WARNING: Host System Error\\n\");\n\t\txhci_halt(xhci);\n\t\tret = IRQ_HANDLED;\n\t\tgoto out;\n\t}\n\n\t \n\tstatus |= STS_EINT;\n\twritel(status, &xhci->op_regs->status);\n\n\t \n\tir = xhci->interrupter;\n\tif (!hcd->msi_enabled) {\n\t\tu32 irq_pending;\n\t\tirq_pending = readl(&ir->ir_set->irq_pending);\n\t\tirq_pending |= IMAN_IP;\n\t\twritel(irq_pending, &ir->ir_set->irq_pending);\n\t}\n\n\tif (xhci->xhc_state & XHCI_STATE_DYING ||\n\t    xhci->xhc_state & XHCI_STATE_HALTED) {\n\t\txhci_dbg(xhci, \"xHCI dying, ignoring interrupt. \"\n\t\t\t\t\"Shouldn't IRQs be disabled?\\n\");\n\t\t \n\t\ttemp_64 = xhci_read_64(xhci, &ir->ir_set->erst_dequeue);\n\t\txhci_write_64(xhci, temp_64 | ERST_EHB,\n\t\t\t\t&ir->ir_set->erst_dequeue);\n\t\tret = IRQ_HANDLED;\n\t\tgoto out;\n\t}\n\n\tevent_ring_deq = ir->event_ring->dequeue;\n\t \n\twhile (xhci_handle_event(xhci, ir) > 0) {\n\t\tif (event_loop++ < TRBS_PER_SEGMENT / 2)\n\t\t\tcontinue;\n\t\txhci_update_erst_dequeue(xhci, ir, event_ring_deq, false);\n\t\tevent_ring_deq = ir->event_ring->dequeue;\n\n\t\t \n\t\tif (xhci->isoc_bei_interval > AVOID_BEI_INTERVAL_MIN)\n\t\t\txhci->isoc_bei_interval = xhci->isoc_bei_interval / 2;\n\n\t\tevent_loop = 0;\n\t}\n\n\txhci_update_erst_dequeue(xhci, ir, event_ring_deq, true);\n\tret = IRQ_HANDLED;\n\nout:\n\tspin_unlock(&xhci->lock);\n\n\treturn ret;\n}\n\nirqreturn_t xhci_msi_irq(int irq, void *hcd)\n{\n\treturn xhci_irq(hcd);\n}\nEXPORT_SYMBOL_GPL(xhci_msi_irq);\n\n \n\n \nstatic void queue_trb(struct xhci_hcd *xhci, struct xhci_ring *ring,\n\t\tbool more_trbs_coming,\n\t\tu32 field1, u32 field2, u32 field3, u32 field4)\n{\n\tstruct xhci_generic_trb *trb;\n\n\ttrb = &ring->enqueue->generic;\n\ttrb->field[0] = cpu_to_le32(field1);\n\ttrb->field[1] = cpu_to_le32(field2);\n\ttrb->field[2] = cpu_to_le32(field3);\n\t \n\twmb();\n\ttrb->field[3] = cpu_to_le32(field4);\n\n\ttrace_xhci_queue_trb(ring, trb);\n\n\tinc_enq(xhci, ring, more_trbs_coming);\n}\n\n \nstatic int prepare_ring(struct xhci_hcd *xhci, struct xhci_ring *ep_ring,\n\t\tu32 ep_state, unsigned int num_trbs, gfp_t mem_flags)\n{\n\tunsigned int link_trb_count = 0;\n\tunsigned int new_segs = 0;\n\n\t \n\tswitch (ep_state) {\n\tcase EP_STATE_DISABLED:\n\t\t \n\t\txhci_warn(xhci, \"WARN urb submitted to disabled ep\\n\");\n\t\treturn -ENOENT;\n\tcase EP_STATE_ERROR:\n\t\txhci_warn(xhci, \"WARN waiting for error on ep to be cleared\\n\");\n\t\t \n\t\t \n\t\treturn -EINVAL;\n\tcase EP_STATE_HALTED:\n\t\txhci_dbg(xhci, \"WARN halted endpoint, queueing URB anyway.\\n\");\n\t\tbreak;\n\tcase EP_STATE_STOPPED:\n\tcase EP_STATE_RUNNING:\n\t\tbreak;\n\tdefault:\n\t\txhci_err(xhci, \"ERROR unknown endpoint state for ep\\n\");\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\tif (ep_ring != xhci->cmd_ring) {\n\t\tnew_segs = xhci_ring_expansion_needed(xhci, ep_ring, num_trbs);\n\t} else if (xhci_num_trbs_free(xhci, ep_ring) <= num_trbs) {\n\t\txhci_err(xhci, \"Do not support expand command ring\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tif (new_segs) {\n\t\txhci_dbg_trace(xhci, trace_xhci_dbg_ring_expansion,\n\t\t\t\t\"ERROR no room on ep ring, try ring expansion\");\n\t\tif (xhci_ring_expansion(xhci, ep_ring, new_segs, mem_flags)) {\n\t\t\txhci_err(xhci, \"Ring expansion failed\\n\");\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\twhile (trb_is_link(ep_ring->enqueue)) {\n\t\t \n\t\tif (!xhci_link_trb_quirk(xhci) &&\n\t\t    !(ep_ring->type == TYPE_ISOC &&\n\t\t      (xhci->quirks & XHCI_AMD_0x96_HOST)))\n\t\t\tep_ring->enqueue->link.control &=\n\t\t\t\tcpu_to_le32(~TRB_CHAIN);\n\t\telse\n\t\t\tep_ring->enqueue->link.control |=\n\t\t\t\tcpu_to_le32(TRB_CHAIN);\n\n\t\twmb();\n\t\tep_ring->enqueue->link.control ^= cpu_to_le32(TRB_CYCLE);\n\n\t\t \n\t\tif (link_trb_toggles_cycle(ep_ring->enqueue))\n\t\t\tep_ring->cycle_state ^= 1;\n\n\t\tep_ring->enq_seg = ep_ring->enq_seg->next;\n\t\tep_ring->enqueue = ep_ring->enq_seg->trbs;\n\n\t\t \n\t\tif (link_trb_count++ > ep_ring->num_segs) {\n\t\t\txhci_warn(xhci, \"Ring is an endless link TRB loop\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (last_trb_on_seg(ep_ring->enq_seg, ep_ring->enqueue)) {\n\t\txhci_warn(xhci, \"Missing link TRB at end of ring segment\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int prepare_transfer(struct xhci_hcd *xhci,\n\t\tstruct xhci_virt_device *xdev,\n\t\tunsigned int ep_index,\n\t\tunsigned int stream_id,\n\t\tunsigned int num_trbs,\n\t\tstruct urb *urb,\n\t\tunsigned int td_index,\n\t\tgfp_t mem_flags)\n{\n\tint ret;\n\tstruct urb_priv *urb_priv;\n\tstruct xhci_td\t*td;\n\tstruct xhci_ring *ep_ring;\n\tstruct xhci_ep_ctx *ep_ctx = xhci_get_ep_ctx(xhci, xdev->out_ctx, ep_index);\n\n\tep_ring = xhci_triad_to_transfer_ring(xhci, xdev->slot_id, ep_index,\n\t\t\t\t\t      stream_id);\n\tif (!ep_ring) {\n\t\txhci_dbg(xhci, \"Can't prepare ring for bad stream ID %u\\n\",\n\t\t\t\tstream_id);\n\t\treturn -EINVAL;\n\t}\n\n\tret = prepare_ring(xhci, ep_ring, GET_EP_CTX_STATE(ep_ctx),\n\t\t\t   num_trbs, mem_flags);\n\tif (ret)\n\t\treturn ret;\n\n\turb_priv = urb->hcpriv;\n\ttd = &urb_priv->td[td_index];\n\n\tINIT_LIST_HEAD(&td->td_list);\n\tINIT_LIST_HEAD(&td->cancelled_td_list);\n\n\tif (td_index == 0) {\n\t\tret = usb_hcd_link_urb_to_ep(bus_to_hcd(urb->dev->bus), urb);\n\t\tif (unlikely(ret))\n\t\t\treturn ret;\n\t}\n\n\ttd->urb = urb;\n\t \n\tlist_add_tail(&td->td_list, &ep_ring->td_list);\n\ttd->start_seg = ep_ring->enq_seg;\n\ttd->first_trb = ep_ring->enqueue;\n\n\treturn 0;\n}\n\nunsigned int count_trbs(u64 addr, u64 len)\n{\n\tunsigned int num_trbs;\n\n\tnum_trbs = DIV_ROUND_UP(len + (addr & (TRB_MAX_BUFF_SIZE - 1)),\n\t\t\tTRB_MAX_BUFF_SIZE);\n\tif (num_trbs == 0)\n\t\tnum_trbs++;\n\n\treturn num_trbs;\n}\n\nstatic inline unsigned int count_trbs_needed(struct urb *urb)\n{\n\treturn count_trbs(urb->transfer_dma, urb->transfer_buffer_length);\n}\n\nstatic unsigned int count_sg_trbs_needed(struct urb *urb)\n{\n\tstruct scatterlist *sg;\n\tunsigned int i, len, full_len, num_trbs = 0;\n\n\tfull_len = urb->transfer_buffer_length;\n\n\tfor_each_sg(urb->sg, sg, urb->num_mapped_sgs, i) {\n\t\tlen = sg_dma_len(sg);\n\t\tnum_trbs += count_trbs(sg_dma_address(sg), len);\n\t\tlen = min_t(unsigned int, len, full_len);\n\t\tfull_len -= len;\n\t\tif (full_len == 0)\n\t\t\tbreak;\n\t}\n\n\treturn num_trbs;\n}\n\nstatic unsigned int count_isoc_trbs_needed(struct urb *urb, int i)\n{\n\tu64 addr, len;\n\n\taddr = (u64) (urb->transfer_dma + urb->iso_frame_desc[i].offset);\n\tlen = urb->iso_frame_desc[i].length;\n\n\treturn count_trbs(addr, len);\n}\n\nstatic void check_trb_math(struct urb *urb, int running_total)\n{\n\tif (unlikely(running_total != urb->transfer_buffer_length))\n\t\tdev_err(&urb->dev->dev, \"%s - ep %#x - Miscalculated tx length, \"\n\t\t\t\t\"queued %#x (%d), asked for %#x (%d)\\n\",\n\t\t\t\t__func__,\n\t\t\t\turb->ep->desc.bEndpointAddress,\n\t\t\t\trunning_total, running_total,\n\t\t\t\turb->transfer_buffer_length,\n\t\t\t\turb->transfer_buffer_length);\n}\n\nstatic void giveback_first_trb(struct xhci_hcd *xhci, int slot_id,\n\t\tunsigned int ep_index, unsigned int stream_id, int start_cycle,\n\t\tstruct xhci_generic_trb *start_trb)\n{\n\t \n\twmb();\n\tif (start_cycle)\n\t\tstart_trb->field[3] |= cpu_to_le32(start_cycle);\n\telse\n\t\tstart_trb->field[3] &= cpu_to_le32(~TRB_CYCLE);\n\txhci_ring_ep_doorbell(xhci, slot_id, ep_index, stream_id);\n}\n\nstatic void check_interval(struct xhci_hcd *xhci, struct urb *urb,\n\t\t\t\t\t\tstruct xhci_ep_ctx *ep_ctx)\n{\n\tint xhci_interval;\n\tint ep_interval;\n\n\txhci_interval = EP_INTERVAL_TO_UFRAMES(le32_to_cpu(ep_ctx->ep_info));\n\tep_interval = urb->interval;\n\n\t \n\tif (urb->dev->speed == USB_SPEED_LOW ||\n\t\t\turb->dev->speed == USB_SPEED_FULL)\n\t\tep_interval *= 8;\n\n\t \n\tif (xhci_interval != ep_interval) {\n\t\tdev_dbg_ratelimited(&urb->dev->dev,\n\t\t\t\t\"Driver uses different interval (%d microframe%s) than xHCI (%d microframe%s)\\n\",\n\t\t\t\tep_interval, ep_interval == 1 ? \"\" : \"s\",\n\t\t\t\txhci_interval, xhci_interval == 1 ? \"\" : \"s\");\n\t\turb->interval = xhci_interval;\n\t\t \n\t\tif (urb->dev->speed == USB_SPEED_LOW ||\n\t\t\t\turb->dev->speed == USB_SPEED_FULL)\n\t\t\turb->interval /= 8;\n\t}\n}\n\n \nint xhci_queue_intr_tx(struct xhci_hcd *xhci, gfp_t mem_flags,\n\t\tstruct urb *urb, int slot_id, unsigned int ep_index)\n{\n\tstruct xhci_ep_ctx *ep_ctx;\n\n\tep_ctx = xhci_get_ep_ctx(xhci, xhci->devs[slot_id]->out_ctx, ep_index);\n\tcheck_interval(xhci, urb, ep_ctx);\n\n\treturn xhci_queue_bulk_tx(xhci, mem_flags, urb, slot_id, ep_index);\n}\n\n \nstatic u32 xhci_td_remainder(struct xhci_hcd *xhci, int transferred,\n\t\t\t      int trb_buff_len, unsigned int td_total_len,\n\t\t\t      struct urb *urb, bool more_trbs_coming)\n{\n\tu32 maxp, total_packet_count;\n\n\t \n\tif (xhci->hci_version < 0x100 && !(xhci->quirks & XHCI_MTK_HOST))\n\t\treturn ((td_total_len - transferred) >> 10);\n\n\t \n\tif (!more_trbs_coming || (transferred == 0 && trb_buff_len == 0) ||\n\t    trb_buff_len == td_total_len)\n\t\treturn 0;\n\n\t \n\tif ((xhci->quirks & XHCI_MTK_HOST) && (xhci->hci_version < 0x100))\n\t\ttrb_buff_len = 0;\n\n\tmaxp = usb_endpoint_maxp(&urb->ep->desc);\n\ttotal_packet_count = DIV_ROUND_UP(td_total_len, maxp);\n\n\t \n\treturn (total_packet_count - ((transferred + trb_buff_len) / maxp));\n}\n\n\nstatic int xhci_align_td(struct xhci_hcd *xhci, struct urb *urb, u32 enqd_len,\n\t\t\t u32 *trb_buff_len, struct xhci_segment *seg)\n{\n\tstruct device *dev = xhci_to_hcd(xhci)->self.sysdev;\n\tunsigned int unalign;\n\tunsigned int max_pkt;\n\tu32 new_buff_len;\n\tsize_t len;\n\n\tmax_pkt = usb_endpoint_maxp(&urb->ep->desc);\n\tunalign = (enqd_len + *trb_buff_len) % max_pkt;\n\n\t \n\tif (unalign == 0)\n\t\treturn 0;\n\n\txhci_dbg(xhci, \"Unaligned %d bytes, buff len %d\\n\",\n\t\t unalign, *trb_buff_len);\n\n\t \n\tif (*trb_buff_len > unalign) {\n\t\t*trb_buff_len -= unalign;\n\t\txhci_dbg(xhci, \"split align, new buff len %d\\n\", *trb_buff_len);\n\t\treturn 0;\n\t}\n\n\t \n\tnew_buff_len = max_pkt - (enqd_len % max_pkt);\n\n\tif (new_buff_len > (urb->transfer_buffer_length - enqd_len))\n\t\tnew_buff_len = (urb->transfer_buffer_length - enqd_len);\n\n\t \n\tif (usb_urb_dir_out(urb)) {\n\t\tif (urb->num_sgs) {\n\t\t\tlen = sg_pcopy_to_buffer(urb->sg, urb->num_sgs,\n\t\t\t\t\t\t seg->bounce_buf, new_buff_len, enqd_len);\n\t\t\tif (len != new_buff_len)\n\t\t\t\txhci_warn(xhci, \"WARN Wrong bounce buffer write length: %zu != %d\\n\",\n\t\t\t\t\t  len, new_buff_len);\n\t\t} else {\n\t\t\tmemcpy(seg->bounce_buf, urb->transfer_buffer + enqd_len, new_buff_len);\n\t\t}\n\n\t\tseg->bounce_dma = dma_map_single(dev, seg->bounce_buf,\n\t\t\t\t\t\t max_pkt, DMA_TO_DEVICE);\n\t} else {\n\t\tseg->bounce_dma = dma_map_single(dev, seg->bounce_buf,\n\t\t\t\t\t\t max_pkt, DMA_FROM_DEVICE);\n\t}\n\n\tif (dma_mapping_error(dev, seg->bounce_dma)) {\n\t\t \n\t\txhci_warn(xhci, \"Failed mapping bounce buffer, not aligning\\n\");\n\t\treturn 0;\n\t}\n\t*trb_buff_len = new_buff_len;\n\tseg->bounce_len = new_buff_len;\n\tseg->bounce_offs = enqd_len;\n\n\txhci_dbg(xhci, \"Bounce align, new buff len %d\\n\", *trb_buff_len);\n\n\treturn 1;\n}\n\n \nint xhci_queue_bulk_tx(struct xhci_hcd *xhci, gfp_t mem_flags,\n\t\tstruct urb *urb, int slot_id, unsigned int ep_index)\n{\n\tstruct xhci_ring *ring;\n\tstruct urb_priv *urb_priv;\n\tstruct xhci_td *td;\n\tstruct xhci_generic_trb *start_trb;\n\tstruct scatterlist *sg = NULL;\n\tbool more_trbs_coming = true;\n\tbool need_zero_pkt = false;\n\tbool first_trb = true;\n\tunsigned int num_trbs;\n\tunsigned int start_cycle, num_sgs = 0;\n\tunsigned int enqd_len, block_len, trb_buff_len, full_len;\n\tint sent_len, ret;\n\tu32 field, length_field, remainder;\n\tu64 addr, send_addr;\n\n\tring = xhci_urb_to_transfer_ring(xhci, urb);\n\tif (!ring)\n\t\treturn -EINVAL;\n\n\tfull_len = urb->transfer_buffer_length;\n\t \n\tif (urb->num_sgs && !(urb->transfer_flags & URB_DMA_MAP_SINGLE)) {\n\t\tnum_sgs = urb->num_mapped_sgs;\n\t\tsg = urb->sg;\n\t\taddr = (u64) sg_dma_address(sg);\n\t\tblock_len = sg_dma_len(sg);\n\t\tnum_trbs = count_sg_trbs_needed(urb);\n\t} else {\n\t\tnum_trbs = count_trbs_needed(urb);\n\t\taddr = (u64) urb->transfer_dma;\n\t\tblock_len = full_len;\n\t}\n\tret = prepare_transfer(xhci, xhci->devs[slot_id],\n\t\t\tep_index, urb->stream_id,\n\t\t\tnum_trbs, urb, 0, mem_flags);\n\tif (unlikely(ret < 0))\n\t\treturn ret;\n\n\turb_priv = urb->hcpriv;\n\n\t \n\tif (urb->transfer_flags & URB_ZERO_PACKET && urb_priv->num_tds > 1)\n\t\tneed_zero_pkt = true;\n\n\ttd = &urb_priv->td[0];\n\n\t \n\tstart_trb = &ring->enqueue->generic;\n\tstart_cycle = ring->cycle_state;\n\tsend_addr = addr;\n\n\t \n\tfor (enqd_len = 0; first_trb || enqd_len < full_len;\n\t\t\tenqd_len += trb_buff_len) {\n\t\tfield = TRB_TYPE(TRB_NORMAL);\n\n\t\t \n\t\ttrb_buff_len = TRB_BUFF_LEN_UP_TO_BOUNDARY(addr);\n\t\ttrb_buff_len = min_t(unsigned int, trb_buff_len, block_len);\n\n\t\tif (enqd_len + trb_buff_len > full_len)\n\t\t\ttrb_buff_len = full_len - enqd_len;\n\n\t\t \n\t\tif (first_trb) {\n\t\t\tfirst_trb = false;\n\t\t\tif (start_cycle == 0)\n\t\t\t\tfield |= TRB_CYCLE;\n\t\t} else\n\t\t\tfield |= ring->cycle_state;\n\n\t\t \n\t\tif (enqd_len + trb_buff_len < full_len) {\n\t\t\tfield |= TRB_CHAIN;\n\t\t\tif (trb_is_link(ring->enqueue + 1)) {\n\t\t\t\tif (xhci_align_td(xhci, urb, enqd_len,\n\t\t\t\t\t\t  &trb_buff_len,\n\t\t\t\t\t\t  ring->enq_seg)) {\n\t\t\t\t\tsend_addr = ring->enq_seg->bounce_dma;\n\t\t\t\t\t \n\t\t\t\t\ttd->bounce_seg = ring->enq_seg;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif (enqd_len + trb_buff_len >= full_len) {\n\t\t\tfield &= ~TRB_CHAIN;\n\t\t\tfield |= TRB_IOC;\n\t\t\tmore_trbs_coming = false;\n\t\t\ttd->last_trb = ring->enqueue;\n\t\t\ttd->last_trb_seg = ring->enq_seg;\n\t\t\tif (xhci_urb_suitable_for_idt(urb)) {\n\t\t\t\tmemcpy(&send_addr, urb->transfer_buffer,\n\t\t\t\t       trb_buff_len);\n\t\t\t\tle64_to_cpus(&send_addr);\n\t\t\t\tfield |= TRB_IDT;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (usb_urb_dir_in(urb))\n\t\t\tfield |= TRB_ISP;\n\n\t\t \n\t\tremainder = xhci_td_remainder(xhci, enqd_len, trb_buff_len,\n\t\t\t\t\t      full_len, urb, more_trbs_coming);\n\n\t\tlength_field = TRB_LEN(trb_buff_len) |\n\t\t\tTRB_TD_SIZE(remainder) |\n\t\t\tTRB_INTR_TARGET(0);\n\n\t\tqueue_trb(xhci, ring, more_trbs_coming | need_zero_pkt,\n\t\t\t\tlower_32_bits(send_addr),\n\t\t\t\tupper_32_bits(send_addr),\n\t\t\t\tlength_field,\n\t\t\t\tfield);\n\t\ttd->num_trbs++;\n\t\taddr += trb_buff_len;\n\t\tsent_len = trb_buff_len;\n\n\t\twhile (sg && sent_len >= block_len) {\n\t\t\t \n\t\t\t--num_sgs;\n\t\t\tsent_len -= block_len;\n\t\t\tsg = sg_next(sg);\n\t\t\tif (num_sgs != 0 && sg) {\n\t\t\t\tblock_len = sg_dma_len(sg);\n\t\t\t\taddr = (u64) sg_dma_address(sg);\n\t\t\t\taddr += sent_len;\n\t\t\t}\n\t\t}\n\t\tblock_len -= sent_len;\n\t\tsend_addr = addr;\n\t}\n\n\tif (need_zero_pkt) {\n\t\tret = prepare_transfer(xhci, xhci->devs[slot_id],\n\t\t\t\t       ep_index, urb->stream_id,\n\t\t\t\t       1, urb, 1, mem_flags);\n\t\turb_priv->td[1].last_trb = ring->enqueue;\n\t\turb_priv->td[1].last_trb_seg = ring->enq_seg;\n\t\tfield = TRB_TYPE(TRB_NORMAL) | ring->cycle_state | TRB_IOC;\n\t\tqueue_trb(xhci, ring, 0, 0, 0, TRB_INTR_TARGET(0), field);\n\t\turb_priv->td[1].num_trbs++;\n\t}\n\n\tcheck_trb_math(urb, enqd_len);\n\tgiveback_first_trb(xhci, slot_id, ep_index, urb->stream_id,\n\t\t\tstart_cycle, start_trb);\n\treturn 0;\n}\n\n \nint xhci_queue_ctrl_tx(struct xhci_hcd *xhci, gfp_t mem_flags,\n\t\tstruct urb *urb, int slot_id, unsigned int ep_index)\n{\n\tstruct xhci_ring *ep_ring;\n\tint num_trbs;\n\tint ret;\n\tstruct usb_ctrlrequest *setup;\n\tstruct xhci_generic_trb *start_trb;\n\tint start_cycle;\n\tu32 field;\n\tstruct urb_priv *urb_priv;\n\tstruct xhci_td *td;\n\n\tep_ring = xhci_urb_to_transfer_ring(xhci, urb);\n\tif (!ep_ring)\n\t\treturn -EINVAL;\n\n\t \n\tif (!urb->setup_packet)\n\t\treturn -EINVAL;\n\n\t \n\tnum_trbs = 2;\n\t \n\tif (urb->transfer_buffer_length > 0)\n\t\tnum_trbs++;\n\tret = prepare_transfer(xhci, xhci->devs[slot_id],\n\t\t\tep_index, urb->stream_id,\n\t\t\tnum_trbs, urb, 0, mem_flags);\n\tif (ret < 0)\n\t\treturn ret;\n\n\turb_priv = urb->hcpriv;\n\ttd = &urb_priv->td[0];\n\ttd->num_trbs = num_trbs;\n\n\t \n\tstart_trb = &ep_ring->enqueue->generic;\n\tstart_cycle = ep_ring->cycle_state;\n\n\t \n\t \n\tsetup = (struct usb_ctrlrequest *) urb->setup_packet;\n\tfield = 0;\n\tfield |= TRB_IDT | TRB_TYPE(TRB_SETUP);\n\tif (start_cycle == 0)\n\t\tfield |= 0x1;\n\n\t \n\tif ((xhci->hci_version >= 0x100) || (xhci->quirks & XHCI_MTK_HOST)) {\n\t\tif (urb->transfer_buffer_length > 0) {\n\t\t\tif (setup->bRequestType & USB_DIR_IN)\n\t\t\t\tfield |= TRB_TX_TYPE(TRB_DATA_IN);\n\t\t\telse\n\t\t\t\tfield |= TRB_TX_TYPE(TRB_DATA_OUT);\n\t\t}\n\t}\n\n\tqueue_trb(xhci, ep_ring, true,\n\t\t  setup->bRequestType | setup->bRequest << 8 | le16_to_cpu(setup->wValue) << 16,\n\t\t  le16_to_cpu(setup->wIndex) | le16_to_cpu(setup->wLength) << 16,\n\t\t  TRB_LEN(8) | TRB_INTR_TARGET(0),\n\t\t   \n\t\t  field);\n\n\t \n\t \n\tif (usb_urb_dir_in(urb))\n\t\tfield = TRB_ISP | TRB_TYPE(TRB_DATA);\n\telse\n\t\tfield = TRB_TYPE(TRB_DATA);\n\n\tif (urb->transfer_buffer_length > 0) {\n\t\tu32 length_field, remainder;\n\t\tu64 addr;\n\n\t\tif (xhci_urb_suitable_for_idt(urb)) {\n\t\t\tmemcpy(&addr, urb->transfer_buffer,\n\t\t\t       urb->transfer_buffer_length);\n\t\t\tle64_to_cpus(&addr);\n\t\t\tfield |= TRB_IDT;\n\t\t} else {\n\t\t\taddr = (u64) urb->transfer_dma;\n\t\t}\n\n\t\tremainder = xhci_td_remainder(xhci, 0,\n\t\t\t\turb->transfer_buffer_length,\n\t\t\t\turb->transfer_buffer_length,\n\t\t\t\turb, 1);\n\t\tlength_field = TRB_LEN(urb->transfer_buffer_length) |\n\t\t\t\tTRB_TD_SIZE(remainder) |\n\t\t\t\tTRB_INTR_TARGET(0);\n\t\tif (setup->bRequestType & USB_DIR_IN)\n\t\t\tfield |= TRB_DIR_IN;\n\t\tqueue_trb(xhci, ep_ring, true,\n\t\t\t\tlower_32_bits(addr),\n\t\t\t\tupper_32_bits(addr),\n\t\t\t\tlength_field,\n\t\t\t\tfield | ep_ring->cycle_state);\n\t}\n\n\t \n\ttd->last_trb = ep_ring->enqueue;\n\ttd->last_trb_seg = ep_ring->enq_seg;\n\n\t \n\t \n\tif (urb->transfer_buffer_length > 0 && setup->bRequestType & USB_DIR_IN)\n\t\tfield = 0;\n\telse\n\t\tfield = TRB_DIR_IN;\n\tqueue_trb(xhci, ep_ring, false,\n\t\t\t0,\n\t\t\t0,\n\t\t\tTRB_INTR_TARGET(0),\n\t\t\t \n\t\t\tfield | TRB_IOC | TRB_TYPE(TRB_STATUS) | ep_ring->cycle_state);\n\n\tgiveback_first_trb(xhci, slot_id, ep_index, 0,\n\t\t\tstart_cycle, start_trb);\n\treturn 0;\n}\n\n \nstatic unsigned int xhci_get_burst_count(struct xhci_hcd *xhci,\n\t\tstruct urb *urb, unsigned int total_packet_count)\n{\n\tunsigned int max_burst;\n\n\tif (xhci->hci_version < 0x100 || urb->dev->speed < USB_SPEED_SUPER)\n\t\treturn 0;\n\n\tmax_burst = urb->ep->ss_ep_comp.bMaxBurst;\n\treturn DIV_ROUND_UP(total_packet_count, max_burst + 1) - 1;\n}\n\n \nstatic unsigned int xhci_get_last_burst_packet_count(struct xhci_hcd *xhci,\n\t\tstruct urb *urb, unsigned int total_packet_count)\n{\n\tunsigned int max_burst;\n\tunsigned int residue;\n\n\tif (xhci->hci_version < 0x100)\n\t\treturn 0;\n\n\tif (urb->dev->speed >= USB_SPEED_SUPER) {\n\t\t \n\t\tmax_burst = urb->ep->ss_ep_comp.bMaxBurst;\n\t\tresidue = total_packet_count % (max_burst + 1);\n\t\t \n\t\tif (residue == 0)\n\t\t\treturn max_burst;\n\t\treturn residue - 1;\n\t}\n\tif (total_packet_count == 0)\n\t\treturn 0;\n\treturn total_packet_count - 1;\n}\n\n \nstatic int xhci_get_isoc_frame_id(struct xhci_hcd *xhci,\n\t\tstruct urb *urb, int index)\n{\n\tint start_frame, ist, ret = 0;\n\tint start_frame_id, end_frame_id, current_frame_id;\n\n\tif (urb->dev->speed == USB_SPEED_LOW ||\n\t\t\turb->dev->speed == USB_SPEED_FULL)\n\t\tstart_frame = urb->start_frame + index * urb->interval;\n\telse\n\t\tstart_frame = (urb->start_frame + index * urb->interval) >> 3;\n\n\t \n\tist = HCS_IST(xhci->hcs_params2) & 0x7;\n\tif (HCS_IST(xhci->hcs_params2) & (1 << 3))\n\t\tist <<= 3;\n\n\t \n\tcurrent_frame_id = readl(&xhci->run_regs->microframe_index);\n\tstart_frame_id = roundup(current_frame_id + ist + 1, 8);\n\tend_frame_id = rounddown(current_frame_id + 895 * 8, 8);\n\n\tstart_frame &= 0x7ff;\n\tstart_frame_id = (start_frame_id >> 3) & 0x7ff;\n\tend_frame_id = (end_frame_id >> 3) & 0x7ff;\n\n\txhci_dbg(xhci, \"%s: index %d, reg 0x%x start_frame_id 0x%x, end_frame_id 0x%x, start_frame 0x%x\\n\",\n\t\t __func__, index, readl(&xhci->run_regs->microframe_index),\n\t\t start_frame_id, end_frame_id, start_frame);\n\n\tif (start_frame_id < end_frame_id) {\n\t\tif (start_frame > end_frame_id ||\n\t\t\t\tstart_frame < start_frame_id)\n\t\t\tret = -EINVAL;\n\t} else if (start_frame_id > end_frame_id) {\n\t\tif ((start_frame > end_frame_id &&\n\t\t\t\tstart_frame < start_frame_id))\n\t\t\tret = -EINVAL;\n\t} else {\n\t\t\tret = -EINVAL;\n\t}\n\n\tif (index == 0) {\n\t\tif (ret == -EINVAL || start_frame == start_frame_id) {\n\t\t\tstart_frame = start_frame_id + 1;\n\t\t\tif (urb->dev->speed == USB_SPEED_LOW ||\n\t\t\t\t\turb->dev->speed == USB_SPEED_FULL)\n\t\t\t\turb->start_frame = start_frame;\n\t\t\telse\n\t\t\t\turb->start_frame = start_frame << 3;\n\t\t\tret = 0;\n\t\t}\n\t}\n\n\tif (ret) {\n\t\txhci_warn(xhci, \"Frame ID %d (reg %d, index %d) beyond range (%d, %d)\\n\",\n\t\t\t\tstart_frame, current_frame_id, index,\n\t\t\t\tstart_frame_id, end_frame_id);\n\t\txhci_warn(xhci, \"Ignore frame ID field, use SIA bit instead\\n\");\n\t\treturn ret;\n\t}\n\n\treturn start_frame;\n}\n\n \nstatic bool trb_block_event_intr(struct xhci_hcd *xhci, int num_tds, int i)\n{\n\tif (xhci->hci_version < 0x100)\n\t\treturn false;\n\t \n\tif (i == num_tds - 1)\n\t\treturn false;\n\t \n\tif (i && xhci->quirks & XHCI_AVOID_BEI)\n\t\treturn !!(i % xhci->isoc_bei_interval);\n\n\treturn true;\n}\n\n \nstatic int xhci_queue_isoc_tx(struct xhci_hcd *xhci, gfp_t mem_flags,\n\t\tstruct urb *urb, int slot_id, unsigned int ep_index)\n{\n\tstruct xhci_ring *ep_ring;\n\tstruct urb_priv *urb_priv;\n\tstruct xhci_td *td;\n\tint num_tds, trbs_per_td;\n\tstruct xhci_generic_trb *start_trb;\n\tbool first_trb;\n\tint start_cycle;\n\tu32 field, length_field;\n\tint running_total, trb_buff_len, td_len, td_remain_len, ret;\n\tu64 start_addr, addr;\n\tint i, j;\n\tbool more_trbs_coming;\n\tstruct xhci_virt_ep *xep;\n\tint frame_id;\n\n\txep = &xhci->devs[slot_id]->eps[ep_index];\n\tep_ring = xhci->devs[slot_id]->eps[ep_index].ring;\n\n\tnum_tds = urb->number_of_packets;\n\tif (num_tds < 1) {\n\t\txhci_dbg(xhci, \"Isoc URB with zero packets?\\n\");\n\t\treturn -EINVAL;\n\t}\n\tstart_addr = (u64) urb->transfer_dma;\n\tstart_trb = &ep_ring->enqueue->generic;\n\tstart_cycle = ep_ring->cycle_state;\n\n\turb_priv = urb->hcpriv;\n\t \n\tfor (i = 0; i < num_tds; i++) {\n\t\tunsigned int total_pkt_count, max_pkt;\n\t\tunsigned int burst_count, last_burst_pkt_count;\n\t\tu32 sia_frame_id;\n\n\t\tfirst_trb = true;\n\t\trunning_total = 0;\n\t\taddr = start_addr + urb->iso_frame_desc[i].offset;\n\t\ttd_len = urb->iso_frame_desc[i].length;\n\t\ttd_remain_len = td_len;\n\t\tmax_pkt = usb_endpoint_maxp(&urb->ep->desc);\n\t\ttotal_pkt_count = DIV_ROUND_UP(td_len, max_pkt);\n\n\t\t \n\t\tif (total_pkt_count == 0)\n\t\t\ttotal_pkt_count++;\n\t\tburst_count = xhci_get_burst_count(xhci, urb, total_pkt_count);\n\t\tlast_burst_pkt_count = xhci_get_last_burst_packet_count(xhci,\n\t\t\t\t\t\t\turb, total_pkt_count);\n\n\t\ttrbs_per_td = count_isoc_trbs_needed(urb, i);\n\n\t\tret = prepare_transfer(xhci, xhci->devs[slot_id], ep_index,\n\t\t\t\turb->stream_id, trbs_per_td, urb, i, mem_flags);\n\t\tif (ret < 0) {\n\t\t\tif (i == 0)\n\t\t\t\treturn ret;\n\t\t\tgoto cleanup;\n\t\t}\n\t\ttd = &urb_priv->td[i];\n\t\ttd->num_trbs = trbs_per_td;\n\t\t \n\t\tsia_frame_id = TRB_SIA;\n\t\tif (!(urb->transfer_flags & URB_ISO_ASAP) &&\n\t\t    HCC_CFC(xhci->hcc_params)) {\n\t\t\tframe_id = xhci_get_isoc_frame_id(xhci, urb, i);\n\t\t\tif (frame_id >= 0)\n\t\t\t\tsia_frame_id = TRB_FRAME_ID(frame_id);\n\t\t}\n\t\t \n\t\tfield = TRB_TYPE(TRB_ISOC) |\n\t\t\tTRB_TLBPC(last_burst_pkt_count) |\n\t\t\tsia_frame_id |\n\t\t\t(i ? ep_ring->cycle_state : !start_cycle);\n\n\t\t \n\t\tif (!xep->use_extended_tbc)\n\t\t\tfield |= TRB_TBC(burst_count);\n\n\t\t \n\t\tfor (j = 0; j < trbs_per_td; j++) {\n\t\t\tu32 remainder = 0;\n\n\t\t\t \n\t\t\tif (!first_trb)\n\t\t\t\tfield = TRB_TYPE(TRB_NORMAL) |\n\t\t\t\t\tep_ring->cycle_state;\n\n\t\t\t \n\t\t\tif (usb_urb_dir_in(urb))\n\t\t\t\tfield |= TRB_ISP;\n\n\t\t\t \n\t\t\tif (j < trbs_per_td - 1) {\n\t\t\t\tmore_trbs_coming = true;\n\t\t\t\tfield |= TRB_CHAIN;\n\t\t\t} else {\n\t\t\t\tmore_trbs_coming = false;\n\t\t\t\ttd->last_trb = ep_ring->enqueue;\n\t\t\t\ttd->last_trb_seg = ep_ring->enq_seg;\n\t\t\t\tfield |= TRB_IOC;\n\t\t\t\tif (trb_block_event_intr(xhci, num_tds, i))\n\t\t\t\t\tfield |= TRB_BEI;\n\t\t\t}\n\t\t\t \n\t\t\ttrb_buff_len = TRB_BUFF_LEN_UP_TO_BOUNDARY(addr);\n\t\t\tif (trb_buff_len > td_remain_len)\n\t\t\t\ttrb_buff_len = td_remain_len;\n\n\t\t\t \n\t\t\tremainder = xhci_td_remainder(xhci, running_total,\n\t\t\t\t\t\t   trb_buff_len, td_len,\n\t\t\t\t\t\t   urb, more_trbs_coming);\n\n\t\t\tlength_field = TRB_LEN(trb_buff_len) |\n\t\t\t\tTRB_INTR_TARGET(0);\n\n\t\t\t \n\t\t\tif (first_trb && xep->use_extended_tbc)\n\t\t\t\tlength_field |= TRB_TD_SIZE_TBC(burst_count);\n\t\t\telse\n\t\t\t\tlength_field |= TRB_TD_SIZE(remainder);\n\t\t\tfirst_trb = false;\n\n\t\t\tqueue_trb(xhci, ep_ring, more_trbs_coming,\n\t\t\t\tlower_32_bits(addr),\n\t\t\t\tupper_32_bits(addr),\n\t\t\t\tlength_field,\n\t\t\t\tfield);\n\t\t\trunning_total += trb_buff_len;\n\n\t\t\taddr += trb_buff_len;\n\t\t\ttd_remain_len -= trb_buff_len;\n\t\t}\n\n\t\t \n\t\tif (running_total != td_len) {\n\t\t\txhci_err(xhci, \"ISOC TD length unmatch\\n\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t}\n\n\t \n\tif (HCC_CFC(xhci->hcc_params))\n\t\txep->next_frame_id = urb->start_frame + num_tds * urb->interval;\n\n\tif (xhci_to_hcd(xhci)->self.bandwidth_isoc_reqs == 0) {\n\t\tif (xhci->quirks & XHCI_AMD_PLL_FIX)\n\t\t\tusb_amd_quirk_pll_disable();\n\t}\n\txhci_to_hcd(xhci)->self.bandwidth_isoc_reqs++;\n\n\tgiveback_first_trb(xhci, slot_id, ep_index, urb->stream_id,\n\t\t\tstart_cycle, start_trb);\n\treturn 0;\ncleanup:\n\t \n\n\tfor (i--; i >= 0; i--)\n\t\tlist_del_init(&urb_priv->td[i].td_list);\n\n\t \n\turb_priv->td[0].last_trb = ep_ring->enqueue;\n\t \n\ttd_to_noop(xhci, ep_ring, &urb_priv->td[0], true);\n\n\t \n\tep_ring->enqueue = urb_priv->td[0].first_trb;\n\tep_ring->enq_seg = urb_priv->td[0].start_seg;\n\tep_ring->cycle_state = start_cycle;\n\tusb_hcd_unlink_urb_from_ep(bus_to_hcd(urb->dev->bus), urb);\n\treturn ret;\n}\n\n \nint xhci_queue_isoc_tx_prepare(struct xhci_hcd *xhci, gfp_t mem_flags,\n\t\tstruct urb *urb, int slot_id, unsigned int ep_index)\n{\n\tstruct xhci_virt_device *xdev;\n\tstruct xhci_ring *ep_ring;\n\tstruct xhci_ep_ctx *ep_ctx;\n\tint start_frame;\n\tint num_tds, num_trbs, i;\n\tint ret;\n\tstruct xhci_virt_ep *xep;\n\tint ist;\n\n\txdev = xhci->devs[slot_id];\n\txep = &xhci->devs[slot_id]->eps[ep_index];\n\tep_ring = xdev->eps[ep_index].ring;\n\tep_ctx = xhci_get_ep_ctx(xhci, xdev->out_ctx, ep_index);\n\n\tnum_trbs = 0;\n\tnum_tds = urb->number_of_packets;\n\tfor (i = 0; i < num_tds; i++)\n\t\tnum_trbs += count_isoc_trbs_needed(urb, i);\n\n\t \n\tret = prepare_ring(xhci, ep_ring, GET_EP_CTX_STATE(ep_ctx),\n\t\t\t   num_trbs, mem_flags);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tcheck_interval(xhci, urb, ep_ctx);\n\n\t \n\tif (HCC_CFC(xhci->hcc_params) && !list_empty(&ep_ring->td_list)) {\n\t\tif (GET_EP_CTX_STATE(ep_ctx) ==\tEP_STATE_RUNNING) {\n\t\t\turb->start_frame = xep->next_frame_id;\n\t\t\tgoto skip_start_over;\n\t\t}\n\t}\n\n\tstart_frame = readl(&xhci->run_regs->microframe_index);\n\tstart_frame &= 0x3fff;\n\t \n\tist = HCS_IST(xhci->hcs_params2) & 0x7;\n\tif (HCS_IST(xhci->hcs_params2) & (1 << 3))\n\t\tist <<= 3;\n\tstart_frame += ist + XHCI_CFC_DELAY;\n\tstart_frame = roundup(start_frame, 8);\n\n\t \n\tif (urb->dev->speed == USB_SPEED_LOW ||\n\t\t\turb->dev->speed == USB_SPEED_FULL) {\n\t\tstart_frame = roundup(start_frame, urb->interval << 3);\n\t\turb->start_frame = start_frame >> 3;\n\t} else {\n\t\tstart_frame = roundup(start_frame, urb->interval);\n\t\turb->start_frame = start_frame;\n\t}\n\nskip_start_over:\n\n\treturn xhci_queue_isoc_tx(xhci, mem_flags, urb, slot_id, ep_index);\n}\n\n \n\n \nstatic int queue_command(struct xhci_hcd *xhci, struct xhci_command *cmd,\n\t\t\t u32 field1, u32 field2,\n\t\t\t u32 field3, u32 field4, bool command_must_succeed)\n{\n\tint reserved_trbs = xhci->cmd_ring_reserved_trbs;\n\tint ret;\n\n\tif ((xhci->xhc_state & XHCI_STATE_DYING) ||\n\t\t(xhci->xhc_state & XHCI_STATE_HALTED)) {\n\t\txhci_dbg(xhci, \"xHCI dying or halted, can't queue_command\\n\");\n\t\treturn -ESHUTDOWN;\n\t}\n\n\tif (!command_must_succeed)\n\t\treserved_trbs++;\n\n\tret = prepare_ring(xhci, xhci->cmd_ring, EP_STATE_RUNNING,\n\t\t\treserved_trbs, GFP_ATOMIC);\n\tif (ret < 0) {\n\t\txhci_err(xhci, \"ERR: No room for command on command ring\\n\");\n\t\tif (command_must_succeed)\n\t\t\txhci_err(xhci, \"ERR: Reserved TRB counting for \"\n\t\t\t\t\t\"unfailable commands failed.\\n\");\n\t\treturn ret;\n\t}\n\n\tcmd->command_trb = xhci->cmd_ring->enqueue;\n\n\t \n\tif (list_empty(&xhci->cmd_list)) {\n\t\txhci->current_cmd = cmd;\n\t\txhci_mod_cmd_timer(xhci, XHCI_CMD_DEFAULT_TIMEOUT);\n\t}\n\n\tlist_add_tail(&cmd->cmd_list, &xhci->cmd_list);\n\n\tqueue_trb(xhci, xhci->cmd_ring, false, field1, field2, field3,\n\t\t\tfield4 | xhci->cmd_ring->cycle_state);\n\treturn 0;\n}\n\n \nint xhci_queue_slot_control(struct xhci_hcd *xhci, struct xhci_command *cmd,\n\t\tu32 trb_type, u32 slot_id)\n{\n\treturn queue_command(xhci, cmd, 0, 0, 0,\n\t\t\tTRB_TYPE(trb_type) | SLOT_ID_FOR_TRB(slot_id), false);\n}\n\n \nint xhci_queue_address_device(struct xhci_hcd *xhci, struct xhci_command *cmd,\n\t\tdma_addr_t in_ctx_ptr, u32 slot_id, enum xhci_setup_dev setup)\n{\n\treturn queue_command(xhci, cmd, lower_32_bits(in_ctx_ptr),\n\t\t\tupper_32_bits(in_ctx_ptr), 0,\n\t\t\tTRB_TYPE(TRB_ADDR_DEV) | SLOT_ID_FOR_TRB(slot_id)\n\t\t\t| (setup == SETUP_CONTEXT_ONLY ? TRB_BSR : 0), false);\n}\n\nint xhci_queue_vendor_command(struct xhci_hcd *xhci, struct xhci_command *cmd,\n\t\tu32 field1, u32 field2, u32 field3, u32 field4)\n{\n\treturn queue_command(xhci, cmd, field1, field2, field3, field4, false);\n}\n\n \nint xhci_queue_reset_device(struct xhci_hcd *xhci, struct xhci_command *cmd,\n\t\tu32 slot_id)\n{\n\treturn queue_command(xhci, cmd, 0, 0, 0,\n\t\t\tTRB_TYPE(TRB_RESET_DEV) | SLOT_ID_FOR_TRB(slot_id),\n\t\t\tfalse);\n}\n\n \nint xhci_queue_configure_endpoint(struct xhci_hcd *xhci,\n\t\tstruct xhci_command *cmd, dma_addr_t in_ctx_ptr,\n\t\tu32 slot_id, bool command_must_succeed)\n{\n\treturn queue_command(xhci, cmd, lower_32_bits(in_ctx_ptr),\n\t\t\tupper_32_bits(in_ctx_ptr), 0,\n\t\t\tTRB_TYPE(TRB_CONFIG_EP) | SLOT_ID_FOR_TRB(slot_id),\n\t\t\tcommand_must_succeed);\n}\n\n \nint xhci_queue_evaluate_context(struct xhci_hcd *xhci, struct xhci_command *cmd,\n\t\tdma_addr_t in_ctx_ptr, u32 slot_id, bool command_must_succeed)\n{\n\treturn queue_command(xhci, cmd, lower_32_bits(in_ctx_ptr),\n\t\t\tupper_32_bits(in_ctx_ptr), 0,\n\t\t\tTRB_TYPE(TRB_EVAL_CONTEXT) | SLOT_ID_FOR_TRB(slot_id),\n\t\t\tcommand_must_succeed);\n}\n\n \nint xhci_queue_stop_endpoint(struct xhci_hcd *xhci, struct xhci_command *cmd,\n\t\t\t     int slot_id, unsigned int ep_index, int suspend)\n{\n\tu32 trb_slot_id = SLOT_ID_FOR_TRB(slot_id);\n\tu32 trb_ep_index = EP_ID_FOR_TRB(ep_index);\n\tu32 type = TRB_TYPE(TRB_STOP_RING);\n\tu32 trb_suspend = SUSPEND_PORT_FOR_TRB(suspend);\n\n\treturn queue_command(xhci, cmd, 0, 0, 0,\n\t\t\ttrb_slot_id | trb_ep_index | type | trb_suspend, false);\n}\n\nint xhci_queue_reset_ep(struct xhci_hcd *xhci, struct xhci_command *cmd,\n\t\t\tint slot_id, unsigned int ep_index,\n\t\t\tenum xhci_ep_reset_type reset_type)\n{\n\tu32 trb_slot_id = SLOT_ID_FOR_TRB(slot_id);\n\tu32 trb_ep_index = EP_ID_FOR_TRB(ep_index);\n\tu32 type = TRB_TYPE(TRB_RESET_EP);\n\n\tif (reset_type == EP_SOFT_RESET)\n\t\ttype |= TRB_TSP;\n\n\treturn queue_command(xhci, cmd, 0, 0, 0,\n\t\t\ttrb_slot_id | trb_ep_index | type, false);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}