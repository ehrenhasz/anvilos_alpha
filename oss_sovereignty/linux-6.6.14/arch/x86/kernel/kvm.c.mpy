{
  "module_name": "kvm.c",
  "hash_id": "080aa59273b661579a5d33a362cb333bf8033036205c806bfa26d3223e93a058",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/kvm.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) \"kvm-guest: \" fmt\n\n#include <linux/context_tracking.h>\n#include <linux/init.h>\n#include <linux/irq.h>\n#include <linux/kernel.h>\n#include <linux/kvm_para.h>\n#include <linux/cpu.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/hardirq.h>\n#include <linux/notifier.h>\n#include <linux/reboot.h>\n#include <linux/hash.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/kprobes.h>\n#include <linux/nmi.h>\n#include <linux/swait.h>\n#include <linux/syscore_ops.h>\n#include <linux/cc_platform.h>\n#include <linux/efi.h>\n#include <asm/timer.h>\n#include <asm/cpu.h>\n#include <asm/traps.h>\n#include <asm/desc.h>\n#include <asm/tlbflush.h>\n#include <asm/apic.h>\n#include <asm/apicdef.h>\n#include <asm/hypervisor.h>\n#include <asm/tlb.h>\n#include <asm/cpuidle_haltpoll.h>\n#include <asm/ptrace.h>\n#include <asm/reboot.h>\n#include <asm/svm.h>\n#include <asm/e820/api.h>\n\nDEFINE_STATIC_KEY_FALSE(kvm_async_pf_enabled);\n\nstatic int kvmapf = 1;\n\nstatic int __init parse_no_kvmapf(char *arg)\n{\n        kvmapf = 0;\n        return 0;\n}\n\nearly_param(\"no-kvmapf\", parse_no_kvmapf);\n\nstatic int steal_acc = 1;\nstatic int __init parse_no_stealacc(char *arg)\n{\n        steal_acc = 0;\n        return 0;\n}\n\nearly_param(\"no-steal-acc\", parse_no_stealacc);\n\nstatic DEFINE_PER_CPU_DECRYPTED(struct kvm_vcpu_pv_apf_data, apf_reason) __aligned(64);\nDEFINE_PER_CPU_DECRYPTED(struct kvm_steal_time, steal_time) __aligned(64) __visible;\nstatic int has_steal_clock = 0;\n\nstatic int has_guest_poll = 0;\n \nstatic void kvm_io_delay(void)\n{\n}\n\n#define KVM_TASK_SLEEP_HASHBITS 8\n#define KVM_TASK_SLEEP_HASHSIZE (1<<KVM_TASK_SLEEP_HASHBITS)\n\nstruct kvm_task_sleep_node {\n\tstruct hlist_node link;\n\tstruct swait_queue_head wq;\n\tu32 token;\n\tint cpu;\n};\n\nstatic struct kvm_task_sleep_head {\n\traw_spinlock_t lock;\n\tstruct hlist_head list;\n} async_pf_sleepers[KVM_TASK_SLEEP_HASHSIZE];\n\nstatic struct kvm_task_sleep_node *_find_apf_task(struct kvm_task_sleep_head *b,\n\t\t\t\t\t\t  u32 token)\n{\n\tstruct hlist_node *p;\n\n\thlist_for_each(p, &b->list) {\n\t\tstruct kvm_task_sleep_node *n =\n\t\t\thlist_entry(p, typeof(*n), link);\n\t\tif (n->token == token)\n\t\t\treturn n;\n\t}\n\n\treturn NULL;\n}\n\nstatic bool kvm_async_pf_queue_task(u32 token, struct kvm_task_sleep_node *n)\n{\n\tu32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS);\n\tstruct kvm_task_sleep_head *b = &async_pf_sleepers[key];\n\tstruct kvm_task_sleep_node *e;\n\n\traw_spin_lock(&b->lock);\n\te = _find_apf_task(b, token);\n\tif (e) {\n\t\t \n\t\thlist_del(&e->link);\n\t\traw_spin_unlock(&b->lock);\n\t\tkfree(e);\n\t\treturn false;\n\t}\n\n\tn->token = token;\n\tn->cpu = smp_processor_id();\n\tinit_swait_queue_head(&n->wq);\n\thlist_add_head(&n->link, &b->list);\n\traw_spin_unlock(&b->lock);\n\treturn true;\n}\n\n \nvoid kvm_async_pf_task_wait_schedule(u32 token)\n{\n\tstruct kvm_task_sleep_node n;\n\tDECLARE_SWAITQUEUE(wait);\n\n\tlockdep_assert_irqs_disabled();\n\n\tif (!kvm_async_pf_queue_task(token, &n))\n\t\treturn;\n\n\tfor (;;) {\n\t\tprepare_to_swait_exclusive(&n.wq, &wait, TASK_UNINTERRUPTIBLE);\n\t\tif (hlist_unhashed(&n.link))\n\t\t\tbreak;\n\n\t\tlocal_irq_enable();\n\t\tschedule();\n\t\tlocal_irq_disable();\n\t}\n\tfinish_swait(&n.wq, &wait);\n}\nEXPORT_SYMBOL_GPL(kvm_async_pf_task_wait_schedule);\n\nstatic void apf_task_wake_one(struct kvm_task_sleep_node *n)\n{\n\thlist_del_init(&n->link);\n\tif (swq_has_sleeper(&n->wq))\n\t\tswake_up_one(&n->wq);\n}\n\nstatic void apf_task_wake_all(void)\n{\n\tint i;\n\n\tfor (i = 0; i < KVM_TASK_SLEEP_HASHSIZE; i++) {\n\t\tstruct kvm_task_sleep_head *b = &async_pf_sleepers[i];\n\t\tstruct kvm_task_sleep_node *n;\n\t\tstruct hlist_node *p, *next;\n\n\t\traw_spin_lock(&b->lock);\n\t\thlist_for_each_safe(p, next, &b->list) {\n\t\t\tn = hlist_entry(p, typeof(*n), link);\n\t\t\tif (n->cpu == smp_processor_id())\n\t\t\t\tapf_task_wake_one(n);\n\t\t}\n\t\traw_spin_unlock(&b->lock);\n\t}\n}\n\nvoid kvm_async_pf_task_wake(u32 token)\n{\n\tu32 key = hash_32(token, KVM_TASK_SLEEP_HASHBITS);\n\tstruct kvm_task_sleep_head *b = &async_pf_sleepers[key];\n\tstruct kvm_task_sleep_node *n, *dummy = NULL;\n\n\tif (token == ~0) {\n\t\tapf_task_wake_all();\n\t\treturn;\n\t}\n\nagain:\n\traw_spin_lock(&b->lock);\n\tn = _find_apf_task(b, token);\n\tif (!n) {\n\t\t \n\t\tif (!dummy) {\n\t\t\traw_spin_unlock(&b->lock);\n\t\t\tdummy = kzalloc(sizeof(*dummy), GFP_ATOMIC);\n\n\t\t\t \n\t\t\tif (!dummy)\n\t\t\t\tcpu_relax();\n\n\t\t\t \n\t\t\tgoto again;\n\t\t}\n\t\tdummy->token = token;\n\t\tdummy->cpu = smp_processor_id();\n\t\tinit_swait_queue_head(&dummy->wq);\n\t\thlist_add_head(&dummy->link, &b->list);\n\t\tdummy = NULL;\n\t} else {\n\t\tapf_task_wake_one(n);\n\t}\n\traw_spin_unlock(&b->lock);\n\n\t \n\tkfree(dummy);\n}\nEXPORT_SYMBOL_GPL(kvm_async_pf_task_wake);\n\nnoinstr u32 kvm_read_and_reset_apf_flags(void)\n{\n\tu32 flags = 0;\n\n\tif (__this_cpu_read(apf_reason.enabled)) {\n\t\tflags = __this_cpu_read(apf_reason.flags);\n\t\t__this_cpu_write(apf_reason.flags, 0);\n\t}\n\n\treturn flags;\n}\nEXPORT_SYMBOL_GPL(kvm_read_and_reset_apf_flags);\n\nnoinstr bool __kvm_handle_async_pf(struct pt_regs *regs, u32 token)\n{\n\tu32 flags = kvm_read_and_reset_apf_flags();\n\tirqentry_state_t state;\n\n\tif (!flags)\n\t\treturn false;\n\n\tstate = irqentry_enter(regs);\n\tinstrumentation_begin();\n\n\t \n\tif (unlikely(!(regs->flags & X86_EFLAGS_IF)))\n\t\tpanic(\"Host injected async #PF in interrupt disabled region\\n\");\n\n\tif (flags & KVM_PV_REASON_PAGE_NOT_PRESENT) {\n\t\tif (unlikely(!(user_mode(regs))))\n\t\t\tpanic(\"Host injected async #PF in kernel mode\\n\");\n\t\t \n\t\tkvm_async_pf_task_wait_schedule(token);\n\t} else {\n\t\tWARN_ONCE(1, \"Unexpected async PF flags: %x\\n\", flags);\n\t}\n\n\tinstrumentation_end();\n\tirqentry_exit(regs, state);\n\treturn true;\n}\n\nDEFINE_IDTENTRY_SYSVEC(sysvec_kvm_asyncpf_interrupt)\n{\n\tstruct pt_regs *old_regs = set_irq_regs(regs);\n\tu32 token;\n\n\tapic_eoi();\n\n\tinc_irq_stat(irq_hv_callback_count);\n\n\tif (__this_cpu_read(apf_reason.enabled)) {\n\t\ttoken = __this_cpu_read(apf_reason.token);\n\t\tkvm_async_pf_task_wake(token);\n\t\t__this_cpu_write(apf_reason.token, 0);\n\t\twrmsrl(MSR_KVM_ASYNC_PF_ACK, 1);\n\t}\n\n\tset_irq_regs(old_regs);\n}\n\nstatic void __init paravirt_ops_setup(void)\n{\n\tpv_info.name = \"KVM\";\n\n\tif (kvm_para_has_feature(KVM_FEATURE_NOP_IO_DELAY))\n\t\tpv_ops.cpu.io_delay = kvm_io_delay;\n\n#ifdef CONFIG_X86_IO_APIC\n\tno_timer_check = 1;\n#endif\n}\n\nstatic void kvm_register_steal_time(void)\n{\n\tint cpu = smp_processor_id();\n\tstruct kvm_steal_time *st = &per_cpu(steal_time, cpu);\n\n\tif (!has_steal_clock)\n\t\treturn;\n\n\twrmsrl(MSR_KVM_STEAL_TIME, (slow_virt_to_phys(st) | KVM_MSR_ENABLED));\n\tpr_debug(\"stealtime: cpu %d, msr %llx\\n\", cpu,\n\t\t(unsigned long long) slow_virt_to_phys(st));\n}\n\nstatic DEFINE_PER_CPU_DECRYPTED(unsigned long, kvm_apic_eoi) = KVM_PV_EOI_DISABLED;\n\nstatic notrace __maybe_unused void kvm_guest_apic_eoi_write(void)\n{\n\t \n\tif (__test_and_clear_bit(KVM_PV_EOI_BIT, this_cpu_ptr(&kvm_apic_eoi)))\n\t\treturn;\n\tapic_native_eoi();\n}\n\nstatic void kvm_guest_cpu_init(void)\n{\n\tif (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF_INT) && kvmapf) {\n\t\tu64 pa;\n\n\t\tWARN_ON_ONCE(!static_branch_likely(&kvm_async_pf_enabled));\n\n\t\tpa = slow_virt_to_phys(this_cpu_ptr(&apf_reason));\n\t\tpa |= KVM_ASYNC_PF_ENABLED | KVM_ASYNC_PF_DELIVERY_AS_INT;\n\n\t\tif (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF_VMEXIT))\n\t\t\tpa |= KVM_ASYNC_PF_DELIVERY_AS_PF_VMEXIT;\n\n\t\twrmsrl(MSR_KVM_ASYNC_PF_INT, HYPERVISOR_CALLBACK_VECTOR);\n\n\t\twrmsrl(MSR_KVM_ASYNC_PF_EN, pa);\n\t\t__this_cpu_write(apf_reason.enabled, 1);\n\t\tpr_debug(\"setup async PF for cpu %d\\n\", smp_processor_id());\n\t}\n\n\tif (kvm_para_has_feature(KVM_FEATURE_PV_EOI)) {\n\t\tunsigned long pa;\n\n\t\t \n\t\tBUILD_BUG_ON(__alignof__(kvm_apic_eoi) < 4);\n\t\t__this_cpu_write(kvm_apic_eoi, 0);\n\t\tpa = slow_virt_to_phys(this_cpu_ptr(&kvm_apic_eoi))\n\t\t\t| KVM_MSR_ENABLED;\n\t\twrmsrl(MSR_KVM_PV_EOI_EN, pa);\n\t}\n\n\tif (has_steal_clock)\n\t\tkvm_register_steal_time();\n}\n\nstatic void kvm_pv_disable_apf(void)\n{\n\tif (!__this_cpu_read(apf_reason.enabled))\n\t\treturn;\n\n\twrmsrl(MSR_KVM_ASYNC_PF_EN, 0);\n\t__this_cpu_write(apf_reason.enabled, 0);\n\n\tpr_debug(\"disable async PF for cpu %d\\n\", smp_processor_id());\n}\n\nstatic void kvm_disable_steal_time(void)\n{\n\tif (!has_steal_clock)\n\t\treturn;\n\n\twrmsr(MSR_KVM_STEAL_TIME, 0, 0);\n}\n\nstatic u64 kvm_steal_clock(int cpu)\n{\n\tu64 steal;\n\tstruct kvm_steal_time *src;\n\tint version;\n\n\tsrc = &per_cpu(steal_time, cpu);\n\tdo {\n\t\tversion = src->version;\n\t\tvirt_rmb();\n\t\tsteal = src->steal;\n\t\tvirt_rmb();\n\t} while ((version & 1) || (version != src->version));\n\n\treturn steal;\n}\n\nstatic inline void __set_percpu_decrypted(void *ptr, unsigned long size)\n{\n\tearly_set_memory_decrypted((unsigned long) ptr, size);\n}\n\n \nstatic void __init sev_map_percpu_data(void)\n{\n\tint cpu;\n\n\tif (!cc_platform_has(CC_ATTR_GUEST_MEM_ENCRYPT))\n\t\treturn;\n\n\tfor_each_possible_cpu(cpu) {\n\t\t__set_percpu_decrypted(&per_cpu(apf_reason, cpu), sizeof(apf_reason));\n\t\t__set_percpu_decrypted(&per_cpu(steal_time, cpu), sizeof(steal_time));\n\t\t__set_percpu_decrypted(&per_cpu(kvm_apic_eoi, cpu), sizeof(kvm_apic_eoi));\n\t}\n}\n\nstatic void kvm_guest_cpu_offline(bool shutdown)\n{\n\tkvm_disable_steal_time();\n\tif (kvm_para_has_feature(KVM_FEATURE_PV_EOI))\n\t\twrmsrl(MSR_KVM_PV_EOI_EN, 0);\n\tif (kvm_para_has_feature(KVM_FEATURE_MIGRATION_CONTROL))\n\t\twrmsrl(MSR_KVM_MIGRATION_CONTROL, 0);\n\tkvm_pv_disable_apf();\n\tif (!shutdown)\n\t\tapf_task_wake_all();\n\tkvmclock_disable();\n}\n\nstatic int kvm_cpu_online(unsigned int cpu)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tkvm_guest_cpu_init();\n\tlocal_irq_restore(flags);\n\treturn 0;\n}\n\n#ifdef CONFIG_SMP\n\nstatic DEFINE_PER_CPU(cpumask_var_t, __pv_cpu_mask);\n\nstatic bool pv_tlb_flush_supported(void)\n{\n\treturn (kvm_para_has_feature(KVM_FEATURE_PV_TLB_FLUSH) &&\n\t\t!kvm_para_has_hint(KVM_HINTS_REALTIME) &&\n\t\tkvm_para_has_feature(KVM_FEATURE_STEAL_TIME) &&\n\t\t!boot_cpu_has(X86_FEATURE_MWAIT) &&\n\t\t(num_possible_cpus() != 1));\n}\n\nstatic bool pv_ipi_supported(void)\n{\n\treturn (kvm_para_has_feature(KVM_FEATURE_PV_SEND_IPI) &&\n\t       (num_possible_cpus() != 1));\n}\n\nstatic bool pv_sched_yield_supported(void)\n{\n\treturn (kvm_para_has_feature(KVM_FEATURE_PV_SCHED_YIELD) &&\n\t\t!kvm_para_has_hint(KVM_HINTS_REALTIME) &&\n\t    kvm_para_has_feature(KVM_FEATURE_STEAL_TIME) &&\n\t    !boot_cpu_has(X86_FEATURE_MWAIT) &&\n\t    (num_possible_cpus() != 1));\n}\n\n#define KVM_IPI_CLUSTER_SIZE\t(2 * BITS_PER_LONG)\n\nstatic void __send_ipi_mask(const struct cpumask *mask, int vector)\n{\n\tunsigned long flags;\n\tint cpu, apic_id, icr;\n\tint min = 0, max = 0;\n#ifdef CONFIG_X86_64\n\t__uint128_t ipi_bitmap = 0;\n#else\n\tu64 ipi_bitmap = 0;\n#endif\n\tlong ret;\n\n\tif (cpumask_empty(mask))\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tswitch (vector) {\n\tdefault:\n\t\ticr = APIC_DM_FIXED | vector;\n\t\tbreak;\n\tcase NMI_VECTOR:\n\t\ticr = APIC_DM_NMI;\n\t\tbreak;\n\t}\n\n\tfor_each_cpu(cpu, mask) {\n\t\tapic_id = per_cpu(x86_cpu_to_apicid, cpu);\n\t\tif (!ipi_bitmap) {\n\t\t\tmin = max = apic_id;\n\t\t} else if (apic_id < min && max - apic_id < KVM_IPI_CLUSTER_SIZE) {\n\t\t\tipi_bitmap <<= min - apic_id;\n\t\t\tmin = apic_id;\n\t\t} else if (apic_id > min && apic_id < min + KVM_IPI_CLUSTER_SIZE) {\n\t\t\tmax = apic_id < max ? max : apic_id;\n\t\t} else {\n\t\t\tret = kvm_hypercall4(KVM_HC_SEND_IPI, (unsigned long)ipi_bitmap,\n\t\t\t\t(unsigned long)(ipi_bitmap >> BITS_PER_LONG), min, icr);\n\t\t\tWARN_ONCE(ret < 0, \"kvm-guest: failed to send PV IPI: %ld\",\n\t\t\t\t  ret);\n\t\t\tmin = max = apic_id;\n\t\t\tipi_bitmap = 0;\n\t\t}\n\t\t__set_bit(apic_id - min, (unsigned long *)&ipi_bitmap);\n\t}\n\n\tif (ipi_bitmap) {\n\t\tret = kvm_hypercall4(KVM_HC_SEND_IPI, (unsigned long)ipi_bitmap,\n\t\t\t(unsigned long)(ipi_bitmap >> BITS_PER_LONG), min, icr);\n\t\tWARN_ONCE(ret < 0, \"kvm-guest: failed to send PV IPI: %ld\",\n\t\t\t  ret);\n\t}\n\n\tlocal_irq_restore(flags);\n}\n\nstatic void kvm_send_ipi_mask(const struct cpumask *mask, int vector)\n{\n\t__send_ipi_mask(mask, vector);\n}\n\nstatic void kvm_send_ipi_mask_allbutself(const struct cpumask *mask, int vector)\n{\n\tunsigned int this_cpu = smp_processor_id();\n\tstruct cpumask *new_mask = this_cpu_cpumask_var_ptr(__pv_cpu_mask);\n\tconst struct cpumask *local_mask;\n\n\tcpumask_copy(new_mask, mask);\n\tcpumask_clear_cpu(this_cpu, new_mask);\n\tlocal_mask = new_mask;\n\t__send_ipi_mask(local_mask, vector);\n}\n\nstatic int __init setup_efi_kvm_sev_migration(void)\n{\n\tefi_char16_t efi_sev_live_migration_enabled[] = L\"SevLiveMigrationEnabled\";\n\tefi_guid_t efi_variable_guid = AMD_SEV_MEM_ENCRYPT_GUID;\n\tefi_status_t status;\n\tunsigned long size;\n\tbool enabled;\n\n\tif (!cc_platform_has(CC_ATTR_GUEST_MEM_ENCRYPT) ||\n\t    !kvm_para_has_feature(KVM_FEATURE_MIGRATION_CONTROL))\n\t\treturn 0;\n\n\tif (!efi_enabled(EFI_BOOT))\n\t\treturn 0;\n\n\tif (!efi_enabled(EFI_RUNTIME_SERVICES)) {\n\t\tpr_info(\"%s : EFI runtime services are not enabled\\n\", __func__);\n\t\treturn 0;\n\t}\n\n\tsize = sizeof(enabled);\n\n\t \n\tstatus = efi.get_variable(efi_sev_live_migration_enabled,\n\t\t\t\t  &efi_variable_guid, NULL, &size, &enabled);\n\n\tif (status == EFI_NOT_FOUND) {\n\t\tpr_info(\"%s : EFI live migration variable not found\\n\", __func__);\n\t\treturn 0;\n\t}\n\n\tif (status != EFI_SUCCESS) {\n\t\tpr_info(\"%s : EFI variable retrieval failed\\n\", __func__);\n\t\treturn 0;\n\t}\n\n\tif (enabled == 0) {\n\t\tpr_info(\"%s: live migration disabled in EFI\\n\", __func__);\n\t\treturn 0;\n\t}\n\n\tpr_info(\"%s : live migration enabled in EFI\\n\", __func__);\n\twrmsrl(MSR_KVM_MIGRATION_CONTROL, KVM_MIGRATION_READY);\n\n\treturn 1;\n}\n\nlate_initcall(setup_efi_kvm_sev_migration);\n\n \nstatic __init void kvm_setup_pv_ipi(void)\n{\n\tapic_update_callback(send_IPI_mask, kvm_send_ipi_mask);\n\tapic_update_callback(send_IPI_mask_allbutself, kvm_send_ipi_mask_allbutself);\n\tpr_info(\"setup PV IPIs\\n\");\n}\n\nstatic void kvm_smp_send_call_func_ipi(const struct cpumask *mask)\n{\n\tint cpu;\n\n\tnative_send_call_func_ipi(mask);\n\n\t \n\tfor_each_cpu(cpu, mask) {\n\t\tif (!idle_cpu(cpu) && vcpu_is_preempted(cpu)) {\n\t\t\tkvm_hypercall1(KVM_HC_SCHED_YIELD, per_cpu(x86_cpu_to_apicid, cpu));\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic void kvm_flush_tlb_multi(const struct cpumask *cpumask,\n\t\t\tconst struct flush_tlb_info *info)\n{\n\tu8 state;\n\tint cpu;\n\tstruct kvm_steal_time *src;\n\tstruct cpumask *flushmask = this_cpu_cpumask_var_ptr(__pv_cpu_mask);\n\n\tcpumask_copy(flushmask, cpumask);\n\t \n\tfor_each_cpu(cpu, flushmask) {\n\t\t \n\t\tsrc = &per_cpu(steal_time, cpu);\n\t\tstate = READ_ONCE(src->preempted);\n\t\tif ((state & KVM_VCPU_PREEMPTED)) {\n\t\t\tif (try_cmpxchg(&src->preempted, &state,\n\t\t\t\t\tstate | KVM_VCPU_FLUSH_TLB))\n\t\t\t\t__cpumask_clear_cpu(cpu, flushmask);\n\t\t}\n\t}\n\n\tnative_flush_tlb_multi(flushmask, info);\n}\n\nstatic __init int kvm_alloc_cpumask(void)\n{\n\tint cpu;\n\n\tif (!kvm_para_available() || nopv)\n\t\treturn 0;\n\n\tif (pv_tlb_flush_supported() || pv_ipi_supported())\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\tzalloc_cpumask_var_node(per_cpu_ptr(&__pv_cpu_mask, cpu),\n\t\t\t\tGFP_KERNEL, cpu_to_node(cpu));\n\t\t}\n\n\treturn 0;\n}\narch_initcall(kvm_alloc_cpumask);\n\nstatic void __init kvm_smp_prepare_boot_cpu(void)\n{\n\t \n\tsev_map_percpu_data();\n\n\tkvm_guest_cpu_init();\n\tnative_smp_prepare_boot_cpu();\n\tkvm_spinlock_init();\n}\n\nstatic int kvm_cpu_down_prepare(unsigned int cpu)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tkvm_guest_cpu_offline(false);\n\tlocal_irq_restore(flags);\n\treturn 0;\n}\n\n#endif\n\nstatic int kvm_suspend(void)\n{\n\tu64 val = 0;\n\n\tkvm_guest_cpu_offline(false);\n\n#ifdef CONFIG_ARCH_CPUIDLE_HALTPOLL\n\tif (kvm_para_has_feature(KVM_FEATURE_POLL_CONTROL))\n\t\trdmsrl(MSR_KVM_POLL_CONTROL, val);\n\thas_guest_poll = !(val & 1);\n#endif\n\treturn 0;\n}\n\nstatic void kvm_resume(void)\n{\n\tkvm_cpu_online(raw_smp_processor_id());\n\n#ifdef CONFIG_ARCH_CPUIDLE_HALTPOLL\n\tif (kvm_para_has_feature(KVM_FEATURE_POLL_CONTROL) && has_guest_poll)\n\t\twrmsrl(MSR_KVM_POLL_CONTROL, 0);\n#endif\n}\n\nstatic struct syscore_ops kvm_syscore_ops = {\n\t.suspend\t= kvm_suspend,\n\t.resume\t\t= kvm_resume,\n};\n\nstatic void kvm_pv_guest_cpu_reboot(void *unused)\n{\n\tkvm_guest_cpu_offline(true);\n}\n\nstatic int kvm_pv_reboot_notify(struct notifier_block *nb,\n\t\t\t\tunsigned long code, void *unused)\n{\n\tif (code == SYS_RESTART)\n\t\ton_each_cpu(kvm_pv_guest_cpu_reboot, NULL, 1);\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block kvm_pv_reboot_nb = {\n\t.notifier_call = kvm_pv_reboot_notify,\n};\n\n \n#ifdef CONFIG_KEXEC_CORE\nstatic void kvm_crash_shutdown(struct pt_regs *regs)\n{\n\tkvm_guest_cpu_offline(true);\n\tnative_machine_crash_shutdown(regs);\n}\n#endif\n\n#if defined(CONFIG_X86_32) || !defined(CONFIG_SMP)\nbool __kvm_vcpu_is_preempted(long cpu);\n\n__visible bool __kvm_vcpu_is_preempted(long cpu)\n{\n\tstruct kvm_steal_time *src = &per_cpu(steal_time, cpu);\n\n\treturn !!(src->preempted & KVM_VCPU_PREEMPTED);\n}\nPV_CALLEE_SAVE_REGS_THUNK(__kvm_vcpu_is_preempted);\n\n#else\n\n#include <asm/asm-offsets.h>\n\nextern bool __raw_callee_save___kvm_vcpu_is_preempted(long);\n\n \n#define PV_VCPU_PREEMPTED_ASM\t\t\t\t\t\t     \\\n \"movq   __per_cpu_offset(,%rdi,8), %rax\\n\\t\"\t\t\t\t     \\\n \"cmpb   $0, \" __stringify(KVM_STEAL_TIME_preempted) \"+steal_time(%rax)\\n\\t\" \\\n \"setne  %al\\n\\t\"\n\nDEFINE_PARAVIRT_ASM(__raw_callee_save___kvm_vcpu_is_preempted,\n\t\t    PV_VCPU_PREEMPTED_ASM, .text);\n#endif\n\nstatic void __init kvm_guest_init(void)\n{\n\tint i;\n\n\tparavirt_ops_setup();\n\tregister_reboot_notifier(&kvm_pv_reboot_nb);\n\tfor (i = 0; i < KVM_TASK_SLEEP_HASHSIZE; i++)\n\t\traw_spin_lock_init(&async_pf_sleepers[i].lock);\n\n\tif (kvm_para_has_feature(KVM_FEATURE_STEAL_TIME)) {\n\t\thas_steal_clock = 1;\n\t\tstatic_call_update(pv_steal_clock, kvm_steal_clock);\n\n\t\tpv_ops.lock.vcpu_is_preempted =\n\t\t\tPV_CALLEE_SAVE(__kvm_vcpu_is_preempted);\n\t}\n\n\tif (kvm_para_has_feature(KVM_FEATURE_PV_EOI))\n\t\tapic_update_callback(eoi, kvm_guest_apic_eoi_write);\n\n\tif (kvm_para_has_feature(KVM_FEATURE_ASYNC_PF_INT) && kvmapf) {\n\t\tstatic_branch_enable(&kvm_async_pf_enabled);\n\t\talloc_intr_gate(HYPERVISOR_CALLBACK_VECTOR, asm_sysvec_kvm_asyncpf_interrupt);\n\t}\n\n#ifdef CONFIG_SMP\n\tif (pv_tlb_flush_supported()) {\n\t\tpv_ops.mmu.flush_tlb_multi = kvm_flush_tlb_multi;\n\t\tpv_ops.mmu.tlb_remove_table = tlb_remove_table;\n\t\tpr_info(\"KVM setup pv remote TLB flush\\n\");\n\t}\n\n\tsmp_ops.smp_prepare_boot_cpu = kvm_smp_prepare_boot_cpu;\n\tif (pv_sched_yield_supported()) {\n\t\tsmp_ops.send_call_func_ipi = kvm_smp_send_call_func_ipi;\n\t\tpr_info(\"setup PV sched yield\\n\");\n\t}\n\tif (cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN, \"x86/kvm:online\",\n\t\t\t\t      kvm_cpu_online, kvm_cpu_down_prepare) < 0)\n\t\tpr_err(\"failed to install cpu hotplug callbacks\\n\");\n#else\n\tsev_map_percpu_data();\n\tkvm_guest_cpu_init();\n#endif\n\n#ifdef CONFIG_KEXEC_CORE\n\tmachine_ops.crash_shutdown = kvm_crash_shutdown;\n#endif\n\n\tregister_syscore_ops(&kvm_syscore_ops);\n\n\t \n\thardlockup_detector_disable();\n}\n\nstatic noinline uint32_t __kvm_cpuid_base(void)\n{\n\tif (boot_cpu_data.cpuid_level < 0)\n\t\treturn 0;\t \n\n\tif (boot_cpu_has(X86_FEATURE_HYPERVISOR))\n\t\treturn hypervisor_cpuid_base(KVM_SIGNATURE, 0);\n\n\treturn 0;\n}\n\nstatic inline uint32_t kvm_cpuid_base(void)\n{\n\tstatic int kvm_cpuid_base = -1;\n\n\tif (kvm_cpuid_base == -1)\n\t\tkvm_cpuid_base = __kvm_cpuid_base();\n\n\treturn kvm_cpuid_base;\n}\n\nbool kvm_para_available(void)\n{\n\treturn kvm_cpuid_base() != 0;\n}\nEXPORT_SYMBOL_GPL(kvm_para_available);\n\nunsigned int kvm_arch_para_features(void)\n{\n\treturn cpuid_eax(kvm_cpuid_base() | KVM_CPUID_FEATURES);\n}\n\nunsigned int kvm_arch_para_hints(void)\n{\n\treturn cpuid_edx(kvm_cpuid_base() | KVM_CPUID_FEATURES);\n}\nEXPORT_SYMBOL_GPL(kvm_arch_para_hints);\n\nstatic uint32_t __init kvm_detect(void)\n{\n\treturn kvm_cpuid_base();\n}\n\nstatic void __init kvm_apic_init(void)\n{\n#ifdef CONFIG_SMP\n\tif (pv_ipi_supported())\n\t\tkvm_setup_pv_ipi();\n#endif\n}\n\nstatic bool __init kvm_msi_ext_dest_id(void)\n{\n\treturn kvm_para_has_feature(KVM_FEATURE_MSI_EXT_DEST_ID);\n}\n\nstatic void kvm_sev_hc_page_enc_status(unsigned long pfn, int npages, bool enc)\n{\n\tkvm_sev_hypercall3(KVM_HC_MAP_GPA_RANGE, pfn << PAGE_SHIFT, npages,\n\t\t\t   KVM_MAP_GPA_RANGE_ENC_STAT(enc) | KVM_MAP_GPA_RANGE_PAGE_SZ_4K);\n}\n\nstatic void __init kvm_init_platform(void)\n{\n\tif (cc_platform_has(CC_ATTR_GUEST_MEM_ENCRYPT) &&\n\t    kvm_para_has_feature(KVM_FEATURE_MIGRATION_CONTROL)) {\n\t\tunsigned long nr_pages;\n\t\tint i;\n\n\t\tpv_ops.mmu.notify_page_enc_status_changed =\n\t\t\tkvm_sev_hc_page_enc_status;\n\n\t\t \n\n\t\tfor (i = 0; i < e820_table->nr_entries; i++) {\n\t\t\tstruct e820_entry *entry = &e820_table->entries[i];\n\n\t\t\tif (entry->type != E820_TYPE_RAM)\n\t\t\t\tcontinue;\n\n\t\t\tnr_pages = DIV_ROUND_UP(entry->size, PAGE_SIZE);\n\n\t\t\tkvm_sev_hypercall3(KVM_HC_MAP_GPA_RANGE, entry->addr,\n\t\t\t\t       nr_pages,\n\t\t\t\t       KVM_MAP_GPA_RANGE_ENCRYPTED | KVM_MAP_GPA_RANGE_PAGE_SZ_4K);\n\t\t}\n\n\t\t \n\t\tearly_set_mem_enc_dec_hypercall((unsigned long)__start_bss_decrypted,\n\t\t\t\t\t\t__end_bss_decrypted - __start_bss_decrypted, 0);\n\n\t\t \n\t\tif (!efi_enabled(EFI_BOOT))\n\t\t\twrmsrl(MSR_KVM_MIGRATION_CONTROL,\n\t\t\t       KVM_MIGRATION_READY);\n\t}\n\tkvmclock_init();\n\tx86_platform.apic_post_init = kvm_apic_init;\n}\n\n#if defined(CONFIG_AMD_MEM_ENCRYPT)\nstatic void kvm_sev_es_hcall_prepare(struct ghcb *ghcb, struct pt_regs *regs)\n{\n\t \n\tghcb_set_rbx(ghcb, regs->bx);\n\tghcb_set_rcx(ghcb, regs->cx);\n\tghcb_set_rdx(ghcb, regs->dx);\n\tghcb_set_rsi(ghcb, regs->si);\n}\n\nstatic bool kvm_sev_es_hcall_finish(struct ghcb *ghcb, struct pt_regs *regs)\n{\n\t \n\treturn true;\n}\n#endif\n\nconst __initconst struct hypervisor_x86 x86_hyper_kvm = {\n\t.name\t\t\t\t= \"KVM\",\n\t.detect\t\t\t\t= kvm_detect,\n\t.type\t\t\t\t= X86_HYPER_KVM,\n\t.init.guest_late_init\t\t= kvm_guest_init,\n\t.init.x2apic_available\t\t= kvm_para_available,\n\t.init.msi_ext_dest_id\t\t= kvm_msi_ext_dest_id,\n\t.init.init_platform\t\t= kvm_init_platform,\n#if defined(CONFIG_AMD_MEM_ENCRYPT)\n\t.runtime.sev_es_hcall_prepare\t= kvm_sev_es_hcall_prepare,\n\t.runtime.sev_es_hcall_finish\t= kvm_sev_es_hcall_finish,\n#endif\n};\n\nstatic __init int activate_jump_labels(void)\n{\n\tif (has_steal_clock) {\n\t\tstatic_key_slow_inc(&paravirt_steal_enabled);\n\t\tif (steal_acc)\n\t\t\tstatic_key_slow_inc(&paravirt_steal_rq_enabled);\n\t}\n\n\treturn 0;\n}\narch_initcall(activate_jump_labels);\n\n#ifdef CONFIG_PARAVIRT_SPINLOCKS\n\n \nstatic void kvm_kick_cpu(int cpu)\n{\n\tint apicid;\n\tunsigned long flags = 0;\n\n\tapicid = per_cpu(x86_cpu_to_apicid, cpu);\n\tkvm_hypercall2(KVM_HC_KICK_CPU, flags, apicid);\n}\n\n#include <asm/qspinlock.h>\n\nstatic void kvm_wait(u8 *ptr, u8 val)\n{\n\tif (in_nmi())\n\t\treturn;\n\n\t \n\tif (irqs_disabled()) {\n\t\tif (READ_ONCE(*ptr) == val)\n\t\t\thalt();\n\t} else {\n\t\tlocal_irq_disable();\n\n\t\t \n\t\tif (READ_ONCE(*ptr) == val)\n\t\t\tsafe_halt();\n\t\telse\n\t\t\tlocal_irq_enable();\n\t}\n}\n\n \nvoid __init kvm_spinlock_init(void)\n{\n\t \n\tif (!kvm_para_has_feature(KVM_FEATURE_PV_UNHALT)) {\n\t\tpr_info(\"PV spinlocks disabled, no host support\\n\");\n\t\treturn;\n\t}\n\n\t \n\tif (kvm_para_has_hint(KVM_HINTS_REALTIME)) {\n\t\tpr_info(\"PV spinlocks disabled with KVM_HINTS_REALTIME hints\\n\");\n\t\tgoto out;\n\t}\n\n\tif (num_possible_cpus() == 1) {\n\t\tpr_info(\"PV spinlocks disabled, single CPU\\n\");\n\t\tgoto out;\n\t}\n\n\tif (nopvspin) {\n\t\tpr_info(\"PV spinlocks disabled, forced by \\\"nopvspin\\\" parameter\\n\");\n\t\tgoto out;\n\t}\n\n\tpr_info(\"PV spinlocks enabled\\n\");\n\n\t__pv_init_lock_hash();\n\tpv_ops.lock.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;\n\tpv_ops.lock.queued_spin_unlock =\n\t\tPV_CALLEE_SAVE(__pv_queued_spin_unlock);\n\tpv_ops.lock.wait = kvm_wait;\n\tpv_ops.lock.kick = kvm_kick_cpu;\n\n\t \nout:\n\tstatic_branch_disable(&virt_spin_lock_key);\n}\n\n#endif\t \n\n#ifdef CONFIG_ARCH_CPUIDLE_HALTPOLL\n\nstatic void kvm_disable_host_haltpoll(void *i)\n{\n\twrmsrl(MSR_KVM_POLL_CONTROL, 0);\n}\n\nstatic void kvm_enable_host_haltpoll(void *i)\n{\n\twrmsrl(MSR_KVM_POLL_CONTROL, 1);\n}\n\nvoid arch_haltpoll_enable(unsigned int cpu)\n{\n\tif (!kvm_para_has_feature(KVM_FEATURE_POLL_CONTROL)) {\n\t\tpr_err_once(\"host does not support poll control\\n\");\n\t\tpr_err_once(\"host upgrade recommended\\n\");\n\t\treturn;\n\t}\n\n\t \n\tsmp_call_function_single(cpu, kvm_disable_host_haltpoll, NULL, 1);\n}\nEXPORT_SYMBOL_GPL(arch_haltpoll_enable);\n\nvoid arch_haltpoll_disable(unsigned int cpu)\n{\n\tif (!kvm_para_has_feature(KVM_FEATURE_POLL_CONTROL))\n\t\treturn;\n\n\t \n\tsmp_call_function_single(cpu, kvm_enable_host_haltpoll, NULL, 1);\n}\nEXPORT_SYMBOL_GPL(arch_haltpoll_disable);\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}