{
  "module_name": "smpboot.c",
  "hash_id": "c16bfc78234c0c2e25cb3c604f546bf31fd64b2fd78ad529c1534416c389c2ca",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/smpboot.c",
  "human_readable_source": "\n  \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/init.h>\n#include <linux/smp.h>\n#include <linux/export.h>\n#include <linux/sched.h>\n#include <linux/sched/topology.h>\n#include <linux/sched/hotplug.h>\n#include <linux/sched/task_stack.h>\n#include <linux/percpu.h>\n#include <linux/memblock.h>\n#include <linux/err.h>\n#include <linux/nmi.h>\n#include <linux/tboot.h>\n#include <linux/gfp.h>\n#include <linux/cpuidle.h>\n#include <linux/kexec.h>\n#include <linux/numa.h>\n#include <linux/pgtable.h>\n#include <linux/overflow.h>\n#include <linux/stackprotector.h>\n#include <linux/cpuhotplug.h>\n#include <linux/mc146818rtc.h>\n\n#include <asm/acpi.h>\n#include <asm/cacheinfo.h>\n#include <asm/desc.h>\n#include <asm/nmi.h>\n#include <asm/irq.h>\n#include <asm/realmode.h>\n#include <asm/cpu.h>\n#include <asm/numa.h>\n#include <asm/tlbflush.h>\n#include <asm/mtrr.h>\n#include <asm/mwait.h>\n#include <asm/apic.h>\n#include <asm/io_apic.h>\n#include <asm/fpu/api.h>\n#include <asm/setup.h>\n#include <asm/uv/uv.h>\n#include <asm/microcode.h>\n#include <asm/i8259.h>\n#include <asm/misc.h>\n#include <asm/qspinlock.h>\n#include <asm/intel-family.h>\n#include <asm/cpu_device_id.h>\n#include <asm/spec-ctrl.h>\n#include <asm/hw_irq.h>\n#include <asm/stackprotector.h>\n#include <asm/sev.h>\n\n \nDEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_sibling_map);\nEXPORT_PER_CPU_SYMBOL(cpu_sibling_map);\n\n \nDEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_core_map);\nEXPORT_PER_CPU_SYMBOL(cpu_core_map);\n\n \nDEFINE_PER_CPU_READ_MOSTLY(cpumask_var_t, cpu_die_map);\nEXPORT_PER_CPU_SYMBOL(cpu_die_map);\n\n \nDEFINE_PER_CPU_READ_MOSTLY(struct cpuinfo_x86, cpu_info);\nEXPORT_PER_CPU_SYMBOL(cpu_info);\n\n \nstruct cpumask __cpu_primary_thread_mask __read_mostly;\n\n \nstatic cpumask_var_t cpu_sibling_setup_mask;\n\nstruct mwait_cpu_dead {\n\tunsigned int\tcontrol;\n\tunsigned int\tstatus;\n};\n\n#define CPUDEAD_MWAIT_WAIT\t0xDEADBEEF\n#define CPUDEAD_MWAIT_KEXEC_HLT\t0x4A17DEAD\n\n \nstatic DEFINE_PER_CPU_ALIGNED(struct mwait_cpu_dead, mwait_cpu_dead);\n\n \nunsigned int __max_logical_packages __read_mostly;\nEXPORT_SYMBOL(__max_logical_packages);\nstatic unsigned int logical_packages __read_mostly;\nstatic unsigned int logical_die __read_mostly;\n\n \nint __read_mostly __max_smt_threads = 1;\n\n \nbool x86_topology_update;\n\nint arch_update_cpu_topology(void)\n{\n\tint retval = x86_topology_update;\n\n\tx86_topology_update = false;\n\treturn retval;\n}\n\nstatic unsigned int smpboot_warm_reset_vector_count;\n\nstatic inline void smpboot_setup_warm_reset_vector(unsigned long start_eip)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&rtc_lock, flags);\n\tif (!smpboot_warm_reset_vector_count++) {\n\t\tCMOS_WRITE(0xa, 0xf);\n\t\t*((volatile unsigned short *)phys_to_virt(TRAMPOLINE_PHYS_HIGH)) = start_eip >> 4;\n\t\t*((volatile unsigned short *)phys_to_virt(TRAMPOLINE_PHYS_LOW)) = start_eip & 0xf;\n\t}\n\tspin_unlock_irqrestore(&rtc_lock, flags);\n}\n\nstatic inline void smpboot_restore_warm_reset_vector(void)\n{\n\tunsigned long flags;\n\n\t \n\tspin_lock_irqsave(&rtc_lock, flags);\n\tif (!--smpboot_warm_reset_vector_count) {\n\t\tCMOS_WRITE(0, 0xf);\n\t\t*((volatile u32 *)phys_to_virt(TRAMPOLINE_PHYS_LOW)) = 0;\n\t}\n\tspin_unlock_irqrestore(&rtc_lock, flags);\n\n}\n\n \nstatic void ap_starting(void)\n{\n\tint cpuid = smp_processor_id();\n\n\t \n\tthis_cpu_write(mwait_cpu_dead.status, 0);\n\tthis_cpu_write(mwait_cpu_dead.control, 0);\n\n\t \n\tapic_ap_setup();\n\n\t \n\tsmp_store_cpu_info(cpuid);\n\n\t \n\tset_cpu_sibling_map(cpuid);\n\n\tap_init_aperfmperf();\n\n\tpr_debug(\"Stack at about %p\\n\", &cpuid);\n\n\twmb();\n\n\t \n\tnotify_cpu_starting(cpuid);\n}\n\nstatic void ap_calibrate_delay(void)\n{\n\t \n\tcalibrate_delay();\n\tcpu_data(smp_processor_id()).loops_per_jiffy = loops_per_jiffy;\n}\n\n \nstatic void notrace start_secondary(void *unused)\n{\n\t \n\tcr4_init();\n\n\t \n\tif (IS_ENABLED(CONFIG_X86_32)) {\n\t\t \n\t\tload_cr3(swapper_pg_dir);\n\t\t__flush_tlb_all();\n\t}\n\n\tcpu_init_exception_handling();\n\n\t \n\tif (IS_ENABLED(CONFIG_X86_64))\n\t\tload_ucode_ap();\n\n\t \n\tcpuhp_ap_sync_alive();\n\n\tcpu_init();\n\tfpu__init_cpu();\n\trcu_cpu_starting(raw_smp_processor_id());\n\tx86_cpuinit.early_percpu_clock_init();\n\n\tap_starting();\n\n\t \n\tcheck_tsc_sync_target();\n\n\t \n\tap_calibrate_delay();\n\n\tspeculative_store_bypass_ht_init();\n\n\t \n\tlock_vector_lock();\n\tset_cpu_online(smp_processor_id(), true);\n\tlapic_online();\n\tunlock_vector_lock();\n\tx86_platform.nmi_init();\n\n\t \n\tlocal_irq_enable();\n\n\tx86_cpuinit.setup_percpu_clockev();\n\n\twmb();\n\tcpu_startup_entry(CPUHP_AP_ONLINE_IDLE);\n}\n\n \nint topology_phys_to_logical_pkg(unsigned int phys_pkg)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct cpuinfo_x86 *c = &cpu_data(cpu);\n\n\t\tif (c->initialized && c->phys_proc_id == phys_pkg)\n\t\t\treturn c->logical_proc_id;\n\t}\n\treturn -1;\n}\nEXPORT_SYMBOL(topology_phys_to_logical_pkg);\n\n \nstatic int topology_phys_to_logical_die(unsigned int die_id, unsigned int cur_cpu)\n{\n\tint cpu, proc_id = cpu_data(cur_cpu).phys_proc_id;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct cpuinfo_x86 *c = &cpu_data(cpu);\n\n\t\tif (c->initialized && c->cpu_die_id == die_id &&\n\t\t    c->phys_proc_id == proc_id)\n\t\t\treturn c->logical_die_id;\n\t}\n\treturn -1;\n}\n\n \nint topology_update_package_map(unsigned int pkg, unsigned int cpu)\n{\n\tint new;\n\n\t \n\tnew = topology_phys_to_logical_pkg(pkg);\n\tif (new >= 0)\n\t\tgoto found;\n\n\tnew = logical_packages++;\n\tif (new != pkg) {\n\t\tpr_info(\"CPU %u Converting physical %u to logical package %u\\n\",\n\t\t\tcpu, pkg, new);\n\t}\nfound:\n\tcpu_data(cpu).logical_proc_id = new;\n\treturn 0;\n}\n \nint topology_update_die_map(unsigned int die, unsigned int cpu)\n{\n\tint new;\n\n\t \n\tnew = topology_phys_to_logical_die(die, cpu);\n\tif (new >= 0)\n\t\tgoto found;\n\n\tnew = logical_die++;\n\tif (new != die) {\n\t\tpr_info(\"CPU %u Converting physical %u to logical die %u\\n\",\n\t\t\tcpu, die, new);\n\t}\nfound:\n\tcpu_data(cpu).logical_die_id = new;\n\treturn 0;\n}\n\nstatic void __init smp_store_boot_cpu_info(void)\n{\n\tint id = 0;  \n\tstruct cpuinfo_x86 *c = &cpu_data(id);\n\n\t*c = boot_cpu_data;\n\tc->cpu_index = id;\n\ttopology_update_package_map(c->phys_proc_id, id);\n\ttopology_update_die_map(c->cpu_die_id, id);\n\tc->initialized = true;\n}\n\n \nvoid smp_store_cpu_info(int id)\n{\n\tstruct cpuinfo_x86 *c = &cpu_data(id);\n\n\t \n\tif (!c->initialized)\n\t\t*c = boot_cpu_data;\n\tc->cpu_index = id;\n\t \n\tidentify_secondary_cpu(c);\n\tc->initialized = true;\n}\n\nstatic bool\ntopology_same_node(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)\n{\n\tint cpu1 = c->cpu_index, cpu2 = o->cpu_index;\n\n\treturn (cpu_to_node(cpu1) == cpu_to_node(cpu2));\n}\n\nstatic bool\ntopology_sane(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o, const char *name)\n{\n\tint cpu1 = c->cpu_index, cpu2 = o->cpu_index;\n\n\treturn !WARN_ONCE(!topology_same_node(c, o),\n\t\t\"sched: CPU #%d's %s-sibling CPU #%d is not on the same node! \"\n\t\t\"[node: %d != %d]. Ignoring dependency.\\n\",\n\t\tcpu1, name, cpu2, cpu_to_node(cpu1), cpu_to_node(cpu2));\n}\n\n#define link_mask(mfunc, c1, c2)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tcpumask_set_cpu((c1), mfunc(c2));\t\t\t\t\\\n\tcpumask_set_cpu((c2), mfunc(c1));\t\t\t\t\\\n} while (0)\n\nstatic bool match_smt(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)\n{\n\tif (boot_cpu_has(X86_FEATURE_TOPOEXT)) {\n\t\tint cpu1 = c->cpu_index, cpu2 = o->cpu_index;\n\n\t\tif (c->phys_proc_id == o->phys_proc_id &&\n\t\t    c->cpu_die_id == o->cpu_die_id &&\n\t\t    per_cpu(cpu_llc_id, cpu1) == per_cpu(cpu_llc_id, cpu2)) {\n\t\t\tif (c->cpu_core_id == o->cpu_core_id)\n\t\t\t\treturn topology_sane(c, o, \"smt\");\n\n\t\t\tif ((c->cu_id != 0xff) &&\n\t\t\t    (o->cu_id != 0xff) &&\n\t\t\t    (c->cu_id == o->cu_id))\n\t\t\t\treturn topology_sane(c, o, \"smt\");\n\t\t}\n\n\t} else if (c->phys_proc_id == o->phys_proc_id &&\n\t\t   c->cpu_die_id == o->cpu_die_id &&\n\t\t   c->cpu_core_id == o->cpu_core_id) {\n\t\treturn topology_sane(c, o, \"smt\");\n\t}\n\n\treturn false;\n}\n\nstatic bool match_die(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)\n{\n\tif (c->phys_proc_id == o->phys_proc_id &&\n\t    c->cpu_die_id == o->cpu_die_id)\n\t\treturn true;\n\treturn false;\n}\n\nstatic bool match_l2c(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)\n{\n\tint cpu1 = c->cpu_index, cpu2 = o->cpu_index;\n\n\t \n\tif (per_cpu(cpu_l2c_id, cpu1) == BAD_APICID)\n\t\treturn match_smt(c, o);\n\n\t \n\tif (per_cpu(cpu_l2c_id, cpu1) != per_cpu(cpu_l2c_id, cpu2))\n\t\treturn false;\n\n\treturn topology_sane(c, o, \"l2c\");\n}\n\n \nstatic bool match_pkg(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)\n{\n\tif (c->phys_proc_id == o->phys_proc_id)\n\t\treturn true;\n\treturn false;\n}\n\n \n\nstatic const struct x86_cpu_id intel_cod_cpu[] = {\n\tX86_MATCH_INTEL_FAM6_MODEL(HASWELL_X, 0),\t \n\tX86_MATCH_INTEL_FAM6_MODEL(BROADWELL_X, 0),\t \n\tX86_MATCH_INTEL_FAM6_MODEL(ANY, 1),\t\t \n\t{}\n};\n\nstatic bool match_llc(struct cpuinfo_x86 *c, struct cpuinfo_x86 *o)\n{\n\tconst struct x86_cpu_id *id = x86_match_cpu(intel_cod_cpu);\n\tint cpu1 = c->cpu_index, cpu2 = o->cpu_index;\n\tbool intel_snc = id && id->driver_data;\n\n\t \n\tif (per_cpu(cpu_llc_id, cpu1) == BAD_APICID)\n\t\treturn false;\n\n\t \n\tif (per_cpu(cpu_llc_id, cpu1) != per_cpu(cpu_llc_id, cpu2))\n\t\treturn false;\n\n\t \n\tif (match_pkg(c, o) && !topology_same_node(c, o) && intel_snc)\n\t\treturn false;\n\n\treturn topology_sane(c, o, \"llc\");\n}\n\n\nstatic inline int x86_sched_itmt_flags(void)\n{\n\treturn sysctl_sched_itmt_enabled ? SD_ASYM_PACKING : 0;\n}\n\n#ifdef CONFIG_SCHED_MC\nstatic int x86_core_flags(void)\n{\n\treturn cpu_core_flags() | x86_sched_itmt_flags();\n}\n#endif\n#ifdef CONFIG_SCHED_SMT\nstatic int x86_smt_flags(void)\n{\n\treturn cpu_smt_flags();\n}\n#endif\n#ifdef CONFIG_SCHED_CLUSTER\nstatic int x86_cluster_flags(void)\n{\n\treturn cpu_cluster_flags() | x86_sched_itmt_flags();\n}\n#endif\n\nstatic int x86_die_flags(void)\n{\n\tif (cpu_feature_enabled(X86_FEATURE_HYBRID_CPU))\n\t       return x86_sched_itmt_flags();\n\n\treturn 0;\n}\n\n \nstatic bool x86_has_numa_in_package;\n\nstatic struct sched_domain_topology_level x86_topology[6];\n\nstatic void __init build_sched_topology(void)\n{\n\tint i = 0;\n\n#ifdef CONFIG_SCHED_SMT\n\tx86_topology[i++] = (struct sched_domain_topology_level){\n\t\tcpu_smt_mask, x86_smt_flags, SD_INIT_NAME(SMT)\n\t};\n#endif\n#ifdef CONFIG_SCHED_CLUSTER\n\tx86_topology[i++] = (struct sched_domain_topology_level){\n\t\tcpu_clustergroup_mask, x86_cluster_flags, SD_INIT_NAME(CLS)\n\t};\n#endif\n#ifdef CONFIG_SCHED_MC\n\tx86_topology[i++] = (struct sched_domain_topology_level){\n\t\tcpu_coregroup_mask, x86_core_flags, SD_INIT_NAME(MC)\n\t};\n#endif\n\t \n\tif (!x86_has_numa_in_package) {\n\t\tx86_topology[i++] = (struct sched_domain_topology_level){\n\t\t\tcpu_cpu_mask, x86_die_flags, SD_INIT_NAME(DIE)\n\t\t};\n\t}\n\n\t \n\tBUG_ON(i >= ARRAY_SIZE(x86_topology)-1);\n\n\tset_sched_topology(x86_topology);\n}\n\nvoid set_cpu_sibling_map(int cpu)\n{\n\tbool has_smt = smp_num_siblings > 1;\n\tbool has_mp = has_smt || boot_cpu_data.x86_max_cores > 1;\n\tstruct cpuinfo_x86 *c = &cpu_data(cpu);\n\tstruct cpuinfo_x86 *o;\n\tint i, threads;\n\n\tcpumask_set_cpu(cpu, cpu_sibling_setup_mask);\n\n\tif (!has_mp) {\n\t\tcpumask_set_cpu(cpu, topology_sibling_cpumask(cpu));\n\t\tcpumask_set_cpu(cpu, cpu_llc_shared_mask(cpu));\n\t\tcpumask_set_cpu(cpu, cpu_l2c_shared_mask(cpu));\n\t\tcpumask_set_cpu(cpu, topology_core_cpumask(cpu));\n\t\tcpumask_set_cpu(cpu, topology_die_cpumask(cpu));\n\t\tc->booted_cores = 1;\n\t\treturn;\n\t}\n\n\tfor_each_cpu(i, cpu_sibling_setup_mask) {\n\t\to = &cpu_data(i);\n\n\t\tif (match_pkg(c, o) && !topology_same_node(c, o))\n\t\t\tx86_has_numa_in_package = true;\n\n\t\tif ((i == cpu) || (has_smt && match_smt(c, o)))\n\t\t\tlink_mask(topology_sibling_cpumask, cpu, i);\n\n\t\tif ((i == cpu) || (has_mp && match_llc(c, o)))\n\t\t\tlink_mask(cpu_llc_shared_mask, cpu, i);\n\n\t\tif ((i == cpu) || (has_mp && match_l2c(c, o)))\n\t\t\tlink_mask(cpu_l2c_shared_mask, cpu, i);\n\n\t\tif ((i == cpu) || (has_mp && match_die(c, o)))\n\t\t\tlink_mask(topology_die_cpumask, cpu, i);\n\t}\n\n\tthreads = cpumask_weight(topology_sibling_cpumask(cpu));\n\tif (threads > __max_smt_threads)\n\t\t__max_smt_threads = threads;\n\n\tfor_each_cpu(i, topology_sibling_cpumask(cpu))\n\t\tcpu_data(i).smt_active = threads > 1;\n\n\t \n\tfor_each_cpu(i, cpu_sibling_setup_mask) {\n\t\to = &cpu_data(i);\n\n\t\tif ((i == cpu) || (has_mp && match_pkg(c, o))) {\n\t\t\tlink_mask(topology_core_cpumask, cpu, i);\n\n\t\t\t \n\t\t\tif (threads == 1) {\n\t\t\t\t \n\t\t\t\tif (cpumask_first(\n\t\t\t\t    topology_sibling_cpumask(i)) == i)\n\t\t\t\t\tc->booted_cores++;\n\t\t\t\t \n\t\t\t\tif (i != cpu)\n\t\t\t\t\tcpu_data(i).booted_cores++;\n\t\t\t} else if (i != cpu && !c->booted_cores)\n\t\t\t\tc->booted_cores = cpu_data(i).booted_cores;\n\t\t}\n\t}\n}\n\n \nconst struct cpumask *cpu_coregroup_mask(int cpu)\n{\n\treturn cpu_llc_shared_mask(cpu);\n}\n\nconst struct cpumask *cpu_clustergroup_mask(int cpu)\n{\n\treturn cpu_l2c_shared_mask(cpu);\n}\n\nstatic void impress_friends(void)\n{\n\tint cpu;\n\tunsigned long bogosum = 0;\n\t \n\tpr_debug(\"Before bogomips\\n\");\n\tfor_each_online_cpu(cpu)\n\t\tbogosum += cpu_data(cpu).loops_per_jiffy;\n\n\tpr_info(\"Total of %d processors activated (%lu.%02lu BogoMIPS)\\n\",\n\t\tnum_online_cpus(),\n\t\tbogosum/(500000/HZ),\n\t\t(bogosum/(5000/HZ))%100);\n\n\tpr_debug(\"Before bogocount - setting activated=1\\n\");\n}\n\n \n#define UDELAY_10MS_DEFAULT 10000\n\nstatic unsigned int init_udelay = UINT_MAX;\n\nstatic int __init cpu_init_udelay(char *str)\n{\n\tget_option(&str, &init_udelay);\n\n\treturn 0;\n}\nearly_param(\"cpu_init_udelay\", cpu_init_udelay);\n\nstatic void __init smp_quirk_init_udelay(void)\n{\n\t \n\tif (init_udelay != UINT_MAX)\n\t\treturn;\n\n\t \n\tif (((boot_cpu_data.x86_vendor == X86_VENDOR_INTEL) && (boot_cpu_data.x86 == 6)) ||\n\t    ((boot_cpu_data.x86_vendor == X86_VENDOR_HYGON) && (boot_cpu_data.x86 >= 0x18)) ||\n\t    ((boot_cpu_data.x86_vendor == X86_VENDOR_AMD) && (boot_cpu_data.x86 >= 0xF))) {\n\t\tinit_udelay = 0;\n\t\treturn;\n\t}\n\t \n\tinit_udelay = UDELAY_10MS_DEFAULT;\n}\n\n \nstatic void send_init_sequence(int phys_apicid)\n{\n\tint maxlvt = lapic_get_maxlvt();\n\n\t \n\tif (APIC_INTEGRATED(boot_cpu_apic_version)) {\n\t\t \n\t\tif (maxlvt > 3)\n\t\t\tapic_write(APIC_ESR, 0);\n\t\tapic_read(APIC_ESR);\n\t}\n\n\t \n\tapic_icr_write(APIC_INT_LEVELTRIG | APIC_INT_ASSERT | APIC_DM_INIT, phys_apicid);\n\tsafe_apic_wait_icr_idle();\n\n\tudelay(init_udelay);\n\n\t \n\tapic_icr_write(APIC_INT_LEVELTRIG | APIC_DM_INIT, phys_apicid);\n\tsafe_apic_wait_icr_idle();\n}\n\n \nstatic int wakeup_secondary_cpu_via_init(int phys_apicid, unsigned long start_eip)\n{\n\tunsigned long send_status = 0, accept_status = 0;\n\tint num_starts, j, maxlvt;\n\n\tpreempt_disable();\n\tmaxlvt = lapic_get_maxlvt();\n\tsend_init_sequence(phys_apicid);\n\n\tmb();\n\n\t \n\tif (APIC_INTEGRATED(boot_cpu_apic_version))\n\t\tnum_starts = 2;\n\telse\n\t\tnum_starts = 0;\n\n\t \n\tpr_debug(\"#startup loops: %d\\n\", num_starts);\n\n\tfor (j = 1; j <= num_starts; j++) {\n\t\tpr_debug(\"Sending STARTUP #%d\\n\", j);\n\t\tif (maxlvt > 3)\t\t \n\t\t\tapic_write(APIC_ESR, 0);\n\t\tapic_read(APIC_ESR);\n\t\tpr_debug(\"After apic_write\\n\");\n\n\t\t \n\n\t\t \n\t\t \n\t\t \n\t\tapic_icr_write(APIC_DM_STARTUP | (start_eip >> 12),\n\t\t\t       phys_apicid);\n\n\t\t \n\t\tif (init_udelay == 0)\n\t\t\tudelay(10);\n\t\telse\n\t\t\tudelay(300);\n\n\t\tpr_debug(\"Startup point 1\\n\");\n\n\t\tpr_debug(\"Waiting for send to finish...\\n\");\n\t\tsend_status = safe_apic_wait_icr_idle();\n\n\t\t \n\t\tif (init_udelay == 0)\n\t\t\tudelay(10);\n\t\telse\n\t\t\tudelay(200);\n\n\t\tif (maxlvt > 3)\t\t \n\t\t\tapic_write(APIC_ESR, 0);\n\t\taccept_status = (apic_read(APIC_ESR) & 0xEF);\n\t\tif (send_status || accept_status)\n\t\t\tbreak;\n\t}\n\tpr_debug(\"After Startup\\n\");\n\n\tif (send_status)\n\t\tpr_err(\"APIC never delivered???\\n\");\n\tif (accept_status)\n\t\tpr_err(\"APIC delivery error (%lx)\\n\", accept_status);\n\n\tpreempt_enable();\n\treturn (send_status | accept_status);\n}\n\n \nstatic void announce_cpu(int cpu, int apicid)\n{\n\tstatic int width, node_width, first = 1;\n\tstatic int current_node = NUMA_NO_NODE;\n\tint node = early_cpu_to_node(cpu);\n\n\tif (!width)\n\t\twidth = num_digits(num_possible_cpus()) + 1;  \n\n\tif (!node_width)\n\t\tnode_width = num_digits(num_possible_nodes()) + 1;  \n\n\tif (system_state < SYSTEM_RUNNING) {\n\t\tif (first)\n\t\t\tpr_info(\"x86: Booting SMP configuration:\\n\");\n\n\t\tif (node != current_node) {\n\t\t\tif (current_node > (-1))\n\t\t\t\tpr_cont(\"\\n\");\n\t\t\tcurrent_node = node;\n\n\t\t\tprintk(KERN_INFO \".... node %*s#%d, CPUs:  \",\n\t\t\t       node_width - num_digits(node), \" \", node);\n\t\t}\n\n\t\t \n\t\tif (first)\n\t\t\tpr_cont(\"%*s\", width + 1, \" \");\n\t\tfirst = 0;\n\n\t\tpr_cont(\"%*s#%d\", width - num_digits(cpu), \" \", cpu);\n\t} else\n\t\tpr_info(\"Booting Node %d Processor %d APIC 0x%x\\n\",\n\t\t\tnode, cpu, apicid);\n}\n\nint common_cpu_up(unsigned int cpu, struct task_struct *idle)\n{\n\tint ret;\n\n\t \n\talternatives_enable_smp();\n\n\tper_cpu(pcpu_hot.current_task, cpu) = idle;\n\tcpu_init_stack_canary(cpu, idle);\n\n\t \n\tret = irq_init_percpu_irqstack(cpu);\n\tif (ret)\n\t\treturn ret;\n\n#ifdef CONFIG_X86_32\n\t \n\tper_cpu(pcpu_hot.top_of_stack, cpu) = task_top_of_stack(idle);\n#endif\n\treturn 0;\n}\n\n \nstatic int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)\n{\n\tunsigned long start_ip = real_mode_header->trampoline_start;\n\tint ret;\n\n#ifdef CONFIG_X86_64\n\t \n\tif (apic->wakeup_secondary_cpu_64)\n\t\tstart_ip = real_mode_header->trampoline_start64;\n#endif\n\tidle->thread.sp = (unsigned long)task_pt_regs(idle);\n\tinitial_code = (unsigned long)start_secondary;\n\n\tif (IS_ENABLED(CONFIG_X86_32)) {\n\t\tearly_gdt_descr.address = (unsigned long)get_cpu_gdt_rw(cpu);\n\t\tinitial_stack  = idle->thread.sp;\n\t} else if (!(smpboot_control & STARTUP_PARALLEL_MASK)) {\n\t\tsmpboot_control = cpu;\n\t}\n\n\t \n\tinit_espfix_ap(cpu);\n\n\t \n\tannounce_cpu(cpu, apicid);\n\n\t \n\tif (x86_platform.legacy.warm_reset) {\n\n\t\tpr_debug(\"Setting warm reset code and vector.\\n\");\n\n\t\tsmpboot_setup_warm_reset_vector(start_ip);\n\t\t \n\t\tif (APIC_INTEGRATED(boot_cpu_apic_version)) {\n\t\t\tapic_write(APIC_ESR, 0);\n\t\t\tapic_read(APIC_ESR);\n\t\t}\n\t}\n\n\tsmp_mb();\n\n\t \n\tif (apic->wakeup_secondary_cpu_64)\n\t\tret = apic->wakeup_secondary_cpu_64(apicid, start_ip);\n\telse if (apic->wakeup_secondary_cpu)\n\t\tret = apic->wakeup_secondary_cpu(apicid, start_ip);\n\telse\n\t\tret = wakeup_secondary_cpu_via_init(apicid, start_ip);\n\n\t \n\tif (ret)\n\t\tarch_cpuhp_cleanup_kick_cpu(cpu);\n\treturn ret;\n}\n\nint native_kick_ap(unsigned int cpu, struct task_struct *tidle)\n{\n\tint apicid = apic->cpu_present_to_apicid(cpu);\n\tint err;\n\n\tlockdep_assert_irqs_enabled();\n\n\tpr_debug(\"++++++++++++++++++++=_---CPU UP  %u\\n\", cpu);\n\n\tif (apicid == BAD_APICID || !physid_isset(apicid, phys_cpu_present_map) ||\n\t    !apic_id_valid(apicid)) {\n\t\tpr_err(\"%s: bad cpu %d\\n\", __func__, cpu);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tmtrr_save_state();\n\n\t \n\tper_cpu(fpu_fpregs_owner_ctx, cpu) = NULL;\n\n\terr = common_cpu_up(cpu, tidle);\n\tif (err)\n\t\treturn err;\n\n\terr = do_boot_cpu(apicid, cpu, tidle);\n\tif (err)\n\t\tpr_err(\"do_boot_cpu failed(%d) to wakeup CPU#%u\\n\", err, cpu);\n\n\treturn err;\n}\n\nint arch_cpuhp_kick_ap_alive(unsigned int cpu, struct task_struct *tidle)\n{\n\treturn smp_ops.kick_ap_alive(cpu, tidle);\n}\n\nvoid arch_cpuhp_cleanup_kick_cpu(unsigned int cpu)\n{\n\t \n\tif (smp_ops.kick_ap_alive == native_kick_ap && x86_platform.legacy.warm_reset)\n\t\tsmpboot_restore_warm_reset_vector();\n}\n\nvoid arch_cpuhp_cleanup_dead_cpu(unsigned int cpu)\n{\n\tif (smp_ops.cleanup_dead_cpu)\n\t\tsmp_ops.cleanup_dead_cpu(cpu);\n\n\tif (system_state == SYSTEM_RUNNING)\n\t\tpr_info(\"CPU %u is now offline\\n\", cpu);\n}\n\nvoid arch_cpuhp_sync_state_poll(void)\n{\n\tif (smp_ops.poll_sync_state)\n\t\tsmp_ops.poll_sync_state();\n}\n\n \nvoid __init arch_disable_smp_support(void)\n{\n\tdisable_ioapic_support();\n}\n\n \nstatic __init void disable_smp(void)\n{\n\tpr_info(\"SMP disabled\\n\");\n\n\tdisable_ioapic_support();\n\n\tinit_cpu_present(cpumask_of(0));\n\tinit_cpu_possible(cpumask_of(0));\n\n\tif (smp_found_config)\n\t\tphysid_set_mask_of_physid(boot_cpu_physical_apicid, &phys_cpu_present_map);\n\telse\n\t\tphysid_set_mask_of_physid(0, &phys_cpu_present_map);\n\tcpumask_set_cpu(0, topology_sibling_cpumask(0));\n\tcpumask_set_cpu(0, topology_core_cpumask(0));\n\tcpumask_set_cpu(0, topology_die_cpumask(0));\n}\n\nstatic void __init smp_cpu_index_default(void)\n{\n\tint i;\n\tstruct cpuinfo_x86 *c;\n\n\tfor_each_possible_cpu(i) {\n\t\tc = &cpu_data(i);\n\t\t \n\t\tc->cpu_index = nr_cpu_ids;\n\t}\n}\n\nvoid __init smp_prepare_cpus_common(void)\n{\n\tunsigned int i;\n\n\tsmp_cpu_index_default();\n\n\t \n\tsmp_store_boot_cpu_info();  \n\tmb();\n\n\tfor_each_possible_cpu(i) {\n\t\tzalloc_cpumask_var(&per_cpu(cpu_sibling_map, i), GFP_KERNEL);\n\t\tzalloc_cpumask_var(&per_cpu(cpu_core_map, i), GFP_KERNEL);\n\t\tzalloc_cpumask_var(&per_cpu(cpu_die_map, i), GFP_KERNEL);\n\t\tzalloc_cpumask_var(&per_cpu(cpu_llc_shared_map, i), GFP_KERNEL);\n\t\tzalloc_cpumask_var(&per_cpu(cpu_l2c_shared_map, i), GFP_KERNEL);\n\t}\n\n\tset_cpu_sibling_map(0);\n}\n\n#ifdef CONFIG_X86_64\n \nbool __init arch_cpuhp_init_parallel_bringup(void)\n{\n\tif (!x86_cpuinit.parallel_bringup) {\n\t\tpr_info(\"Parallel CPU startup disabled by the platform\\n\");\n\t\treturn false;\n\t}\n\n\tsmpboot_control = STARTUP_READ_APICID;\n\tpr_debug(\"Parallel CPU startup enabled: 0x%08x\\n\", smpboot_control);\n\treturn true;\n}\n#endif\n\n \nvoid __init native_smp_prepare_cpus(unsigned int max_cpus)\n{\n\tsmp_prepare_cpus_common();\n\n\tswitch (apic_intr_mode) {\n\tcase APIC_PIC:\n\tcase APIC_VIRTUAL_WIRE_NO_CONFIG:\n\t\tdisable_smp();\n\t\treturn;\n\tcase APIC_SYMMETRIC_IO_NO_ROUTING:\n\t\tdisable_smp();\n\t\t \n\t\tx86_init.timers.setup_percpu_clockev();\n\t\treturn;\n\tcase APIC_VIRTUAL_WIRE:\n\tcase APIC_SYMMETRIC_IO:\n\t\tbreak;\n\t}\n\n\t \n\tx86_init.timers.setup_percpu_clockev();\n\n\tpr_info(\"CPU0: \");\n\tprint_cpu_info(&cpu_data(0));\n\n\tuv_system_init();\n\n\tsmp_quirk_init_udelay();\n\n\tspeculative_store_bypass_ht_init();\n\n\tsnp_set_wakeup_secondary_cpu();\n}\n\nvoid arch_thaw_secondary_cpus_begin(void)\n{\n\tset_cache_aps_delayed_init(true);\n}\n\nvoid arch_thaw_secondary_cpus_end(void)\n{\n\tcache_aps_init();\n}\n\n \nvoid __init native_smp_prepare_boot_cpu(void)\n{\n\tint me = smp_processor_id();\n\n\t \n\tif (!IS_ENABLED(CONFIG_SMP))\n\t\tswitch_gdt_and_percpu_base(me);\n\n\tnative_pv_lock_init();\n}\n\nvoid __init calculate_max_logical_packages(void)\n{\n\tint ncpus;\n\n\t \n\tncpus = cpu_data(0).booted_cores * topology_max_smt_threads();\n\t__max_logical_packages = DIV_ROUND_UP(total_cpus, ncpus);\n\tpr_info(\"Max logical packages: %u\\n\", __max_logical_packages);\n}\n\nvoid __init native_smp_cpus_done(unsigned int max_cpus)\n{\n\tpr_debug(\"Boot done\\n\");\n\n\tcalculate_max_logical_packages();\n\tbuild_sched_topology();\n\tnmi_selftest();\n\timpress_friends();\n\tcache_aps_init();\n}\n\nstatic int __initdata setup_possible_cpus = -1;\nstatic int __init _setup_possible_cpus(char *str)\n{\n\tget_option(&str, &setup_possible_cpus);\n\treturn 0;\n}\nearly_param(\"possible_cpus\", _setup_possible_cpus);\n\n\n \n__init void prefill_possible_map(void)\n{\n\tint i, possible;\n\n\ti = setup_max_cpus ?: 1;\n\tif (setup_possible_cpus == -1) {\n\t\tpossible = num_processors;\n#ifdef CONFIG_HOTPLUG_CPU\n\t\tif (setup_max_cpus)\n\t\t\tpossible += disabled_cpus;\n#else\n\t\tif (possible > i)\n\t\t\tpossible = i;\n#endif\n\t} else\n\t\tpossible = setup_possible_cpus;\n\n\ttotal_cpus = max_t(int, possible, num_processors + disabled_cpus);\n\n\t \n\tif (possible > nr_cpu_ids) {\n\t\tpr_warn(\"%d Processors exceeds NR_CPUS limit of %u\\n\",\n\t\t\tpossible, nr_cpu_ids);\n\t\tpossible = nr_cpu_ids;\n\t}\n\n#ifdef CONFIG_HOTPLUG_CPU\n\tif (!setup_max_cpus)\n#endif\n\tif (possible > i) {\n\t\tpr_warn(\"%d Processors exceeds max_cpus limit of %u\\n\",\n\t\t\tpossible, setup_max_cpus);\n\t\tpossible = i;\n\t}\n\n\tset_nr_cpu_ids(possible);\n\n\tpr_info(\"Allowing %d CPUs, %d hotplug CPUs\\n\",\n\t\tpossible, max_t(int, possible - num_processors, 0));\n\n\treset_cpu_possible_mask();\n\n\tfor (i = 0; i < possible; i++)\n\t\tset_cpu_possible(i, true);\n}\n\n \nvoid __init setup_cpu_local_masks(void)\n{\n\talloc_bootmem_cpumask_var(&cpu_sibling_setup_mask);\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\n\n \nstatic void recompute_smt_state(void)\n{\n\tint max_threads, cpu;\n\n\tmax_threads = 0;\n\tfor_each_online_cpu (cpu) {\n\t\tint threads = cpumask_weight(topology_sibling_cpumask(cpu));\n\n\t\tif (threads > max_threads)\n\t\t\tmax_threads = threads;\n\t}\n\t__max_smt_threads = max_threads;\n}\n\nstatic void remove_siblinginfo(int cpu)\n{\n\tint sibling;\n\tstruct cpuinfo_x86 *c = &cpu_data(cpu);\n\n\tfor_each_cpu(sibling, topology_core_cpumask(cpu)) {\n\t\tcpumask_clear_cpu(cpu, topology_core_cpumask(sibling));\n\t\t \n\t\tif (cpumask_weight(topology_sibling_cpumask(cpu)) == 1)\n\t\t\tcpu_data(sibling).booted_cores--;\n\t}\n\n\tfor_each_cpu(sibling, topology_die_cpumask(cpu))\n\t\tcpumask_clear_cpu(cpu, topology_die_cpumask(sibling));\n\n\tfor_each_cpu(sibling, topology_sibling_cpumask(cpu)) {\n\t\tcpumask_clear_cpu(cpu, topology_sibling_cpumask(sibling));\n\t\tif (cpumask_weight(topology_sibling_cpumask(sibling)) == 1)\n\t\t\tcpu_data(sibling).smt_active = false;\n\t}\n\n\tfor_each_cpu(sibling, cpu_llc_shared_mask(cpu))\n\t\tcpumask_clear_cpu(cpu, cpu_llc_shared_mask(sibling));\n\tfor_each_cpu(sibling, cpu_l2c_shared_mask(cpu))\n\t\tcpumask_clear_cpu(cpu, cpu_l2c_shared_mask(sibling));\n\tcpumask_clear(cpu_llc_shared_mask(cpu));\n\tcpumask_clear(cpu_l2c_shared_mask(cpu));\n\tcpumask_clear(topology_sibling_cpumask(cpu));\n\tcpumask_clear(topology_core_cpumask(cpu));\n\tcpumask_clear(topology_die_cpumask(cpu));\n\tc->cpu_core_id = 0;\n\tc->booted_cores = 0;\n\tcpumask_clear_cpu(cpu, cpu_sibling_setup_mask);\n\trecompute_smt_state();\n}\n\nstatic void remove_cpu_from_maps(int cpu)\n{\n\tset_cpu_online(cpu, false);\n\tnuma_remove_cpu(cpu);\n}\n\nvoid cpu_disable_common(void)\n{\n\tint cpu = smp_processor_id();\n\n\tremove_siblinginfo(cpu);\n\n\t \n\tlock_vector_lock();\n\tremove_cpu_from_maps(cpu);\n\tunlock_vector_lock();\n\tfixup_irqs();\n\tlapic_offline();\n}\n\nint native_cpu_disable(void)\n{\n\tint ret;\n\n\tret = lapic_can_unplug_cpu();\n\tif (ret)\n\t\treturn ret;\n\n\tcpu_disable_common();\n\n         \n\tapic_soft_disable();\n\n\treturn 0;\n}\n\nvoid play_dead_common(void)\n{\n\tidle_task_exit();\n\n\tcpuhp_ap_report_dead();\n\n\tlocal_irq_disable();\n}\n\n \nstatic inline void mwait_play_dead(void)\n{\n\tstruct mwait_cpu_dead *md = this_cpu_ptr(&mwait_cpu_dead);\n\tunsigned int eax, ebx, ecx, edx;\n\tunsigned int highest_cstate = 0;\n\tunsigned int highest_subcstate = 0;\n\tint i;\n\n\tif (boot_cpu_data.x86_vendor == X86_VENDOR_AMD ||\n\t    boot_cpu_data.x86_vendor == X86_VENDOR_HYGON)\n\t\treturn;\n\tif (!this_cpu_has(X86_FEATURE_MWAIT))\n\t\treturn;\n\tif (!this_cpu_has(X86_FEATURE_CLFLUSH))\n\t\treturn;\n\tif (__this_cpu_read(cpu_info.cpuid_level) < CPUID_MWAIT_LEAF)\n\t\treturn;\n\n\teax = CPUID_MWAIT_LEAF;\n\tecx = 0;\n\tnative_cpuid(&eax, &ebx, &ecx, &edx);\n\n\t \n\tif (!(ecx & CPUID5_ECX_EXTENSIONS_SUPPORTED)) {\n\t\teax = 0;\n\t} else {\n\t\tedx >>= MWAIT_SUBSTATE_SIZE;\n\t\tfor (i = 0; i < 7 && edx; i++, edx >>= MWAIT_SUBSTATE_SIZE) {\n\t\t\tif (edx & MWAIT_SUBSTATE_MASK) {\n\t\t\t\thighest_cstate = i;\n\t\t\t\thighest_subcstate = edx & MWAIT_SUBSTATE_MASK;\n\t\t\t}\n\t\t}\n\t\teax = (highest_cstate << MWAIT_SUBSTATE_SIZE) |\n\t\t\t(highest_subcstate - 1);\n\t}\n\n\t \n\tmd->status = CPUDEAD_MWAIT_WAIT;\n\tmd->control = CPUDEAD_MWAIT_WAIT;\n\n\twbinvd();\n\n\twhile (1) {\n\t\t \n\t\tmb();\n\t\tclflush(md);\n\t\tmb();\n\t\t__monitor(md, 0, 0);\n\t\tmb();\n\t\t__mwait(eax, 0);\n\n\t\tif (READ_ONCE(md->control) == CPUDEAD_MWAIT_KEXEC_HLT) {\n\t\t\t \n\t\t\tWRITE_ONCE(md->status, CPUDEAD_MWAIT_KEXEC_HLT);\n\t\t\twhile(1)\n\t\t\t\tnative_halt();\n\t\t}\n\t}\n}\n\n \nvoid smp_kick_mwait_play_dead(void)\n{\n\tu32 newstate = CPUDEAD_MWAIT_KEXEC_HLT;\n\tstruct mwait_cpu_dead *md;\n\tunsigned int cpu, i;\n\n\tfor_each_cpu_andnot(cpu, cpu_present_mask, cpu_online_mask) {\n\t\tmd = per_cpu_ptr(&mwait_cpu_dead, cpu);\n\n\t\t \n\t\tif (READ_ONCE(md->status) != CPUDEAD_MWAIT_WAIT)\n\t\t\tcontinue;\n\n\t\t \n\t\tfor (i = 0; READ_ONCE(md->status) != newstate && i < 1000; i++) {\n\t\t\t \n\t\t\tWRITE_ONCE(md->control, newstate);\n\t\t\tudelay(5);\n\t\t}\n\n\t\tif (READ_ONCE(md->status) != newstate)\n\t\t\tpr_err_once(\"CPU%u is stuck in mwait_play_dead()\\n\", cpu);\n\t}\n}\n\nvoid __noreturn hlt_play_dead(void)\n{\n\tif (__this_cpu_read(cpu_info.x86) >= 4)\n\t\twbinvd();\n\n\twhile (1)\n\t\tnative_halt();\n}\n\nvoid native_play_dead(void)\n{\n\tplay_dead_common();\n\ttboot_shutdown(TB_SHUTDOWN_WFS);\n\n\tmwait_play_dead();\n\tif (cpuidle_play_dead())\n\t\thlt_play_dead();\n}\n\n#else  \nint native_cpu_disable(void)\n{\n\treturn -ENOSYS;\n}\n\nvoid native_play_dead(void)\n{\n\tBUG();\n}\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}