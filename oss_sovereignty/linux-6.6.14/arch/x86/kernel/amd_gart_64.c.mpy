{
  "module_name": "amd_gart_64.c",
  "hash_id": "3e886323a2c36d50efbf9db604f224c0fc4d41f9cda272c1613034b07bbca29b",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/amd_gart_64.c",
  "human_readable_source": "\n \n\n#include <linux/types.h>\n#include <linux/ctype.h>\n#include <linux/agp_backend.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/sched.h>\n#include <linux/sched/debug.h>\n#include <linux/string.h>\n#include <linux/spinlock.h>\n#include <linux/pci.h>\n#include <linux/topology.h>\n#include <linux/interrupt.h>\n#include <linux/bitmap.h>\n#include <linux/kdebug.h>\n#include <linux/scatterlist.h>\n#include <linux/iommu-helper.h>\n#include <linux/syscore_ops.h>\n#include <linux/io.h>\n#include <linux/gfp.h>\n#include <linux/atomic.h>\n#include <linux/dma-direct.h>\n#include <linux/dma-map-ops.h>\n#include <asm/mtrr.h>\n#include <asm/proto.h>\n#include <asm/iommu.h>\n#include <asm/gart.h>\n#include <asm/set_memory.h>\n#include <asm/dma.h>\n#include <asm/amd_nb.h>\n#include <asm/x86_init.h>\n\nstatic unsigned long iommu_bus_base;\t \nstatic unsigned long iommu_size;\t \nstatic unsigned long iommu_pages;\t \n\nstatic u32 *iommu_gatt_base;\t\t \n\n \nstatic int iommu_fullflush = 1;\n\n \nstatic DEFINE_SPINLOCK(iommu_bitmap_lock);\n \nstatic unsigned long *iommu_gart_bitmap;\n\nstatic u32 gart_unmapped_entry;\n\n#define GPTE_VALID    1\n#define GPTE_COHERENT 2\n#define GPTE_ENCODE(x) \\\n\t(((x) & 0xfffff000) | (((x) >> 32) << 4) | GPTE_VALID | GPTE_COHERENT)\n#define GPTE_DECODE(x) (((x) & 0xfffff000) | (((u64)(x) & 0xff0) << 28))\n\n#ifdef CONFIG_AGP\n#define AGPEXTERN extern\n#else\n#define AGPEXTERN\n#endif\n\n \n#define GART_MAX_PHYS_ADDR\t(1ULL << 40)\n\n \nAGPEXTERN int agp_memory_reserved;\nAGPEXTERN __u32 *agp_gatt_table;\n\nstatic unsigned long next_bit;   \nstatic bool need_flush;\t\t \n\nstatic unsigned long alloc_iommu(struct device *dev, int size,\n\t\t\t\t unsigned long align_mask)\n{\n\tunsigned long offset, flags;\n\tunsigned long boundary_size;\n\tunsigned long base_index;\n\n\tbase_index = ALIGN(iommu_bus_base & dma_get_seg_boundary(dev),\n\t\t\t   PAGE_SIZE) >> PAGE_SHIFT;\n\tboundary_size = dma_get_seg_boundary_nr_pages(dev, PAGE_SHIFT);\n\n\tspin_lock_irqsave(&iommu_bitmap_lock, flags);\n\toffset = iommu_area_alloc(iommu_gart_bitmap, iommu_pages, next_bit,\n\t\t\t\t  size, base_index, boundary_size, align_mask);\n\tif (offset == -1) {\n\t\tneed_flush = true;\n\t\toffset = iommu_area_alloc(iommu_gart_bitmap, iommu_pages, 0,\n\t\t\t\t\t  size, base_index, boundary_size,\n\t\t\t\t\t  align_mask);\n\t}\n\tif (offset != -1) {\n\t\tnext_bit = offset+size;\n\t\tif (next_bit >= iommu_pages) {\n\t\t\tnext_bit = 0;\n\t\t\tneed_flush = true;\n\t\t}\n\t}\n\tif (iommu_fullflush)\n\t\tneed_flush = true;\n\tspin_unlock_irqrestore(&iommu_bitmap_lock, flags);\n\n\treturn offset;\n}\n\nstatic void free_iommu(unsigned long offset, int size)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&iommu_bitmap_lock, flags);\n\tbitmap_clear(iommu_gart_bitmap, offset, size);\n\tif (offset >= next_bit)\n\t\tnext_bit = offset + size;\n\tspin_unlock_irqrestore(&iommu_bitmap_lock, flags);\n}\n\n \nstatic void flush_gart(void)\n{\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&iommu_bitmap_lock, flags);\n\tif (need_flush) {\n\t\tamd_flush_garts();\n\t\tneed_flush = false;\n\t}\n\tspin_unlock_irqrestore(&iommu_bitmap_lock, flags);\n}\n\n#ifdef CONFIG_IOMMU_LEAK\n \nstatic void dump_leak(void)\n{\n\tstatic int dump;\n\n\tif (dump)\n\t\treturn;\n\tdump = 1;\n\n\tshow_stack(NULL, NULL, KERN_ERR);\n\tdebug_dma_dump_mappings(NULL);\n}\n#endif\n\nstatic void iommu_full(struct device *dev, size_t size, int dir)\n{\n\t \n\n\tdev_err(dev, \"PCI-DMA: Out of IOMMU space for %lu bytes\\n\", size);\n#ifdef CONFIG_IOMMU_LEAK\n\tdump_leak();\n#endif\n}\n\nstatic inline int\nneed_iommu(struct device *dev, unsigned long addr, size_t size)\n{\n\treturn force_iommu || !dma_capable(dev, addr, size, true);\n}\n\nstatic inline int\nnonforced_iommu(struct device *dev, unsigned long addr, size_t size)\n{\n\treturn !dma_capable(dev, addr, size, true);\n}\n\n \nstatic dma_addr_t dma_map_area(struct device *dev, dma_addr_t phys_mem,\n\t\t\t\tsize_t size, int dir, unsigned long align_mask)\n{\n\tunsigned long npages = iommu_num_pages(phys_mem, size, PAGE_SIZE);\n\tunsigned long iommu_page;\n\tint i;\n\n\tif (unlikely(phys_mem + size > GART_MAX_PHYS_ADDR))\n\t\treturn DMA_MAPPING_ERROR;\n\n\tiommu_page = alloc_iommu(dev, npages, align_mask);\n\tif (iommu_page == -1) {\n\t\tif (!nonforced_iommu(dev, phys_mem, size))\n\t\t\treturn phys_mem;\n\t\tif (panic_on_overflow)\n\t\t\tpanic(\"dma_map_area overflow %lu bytes\\n\", size);\n\t\tiommu_full(dev, size, dir);\n\t\treturn DMA_MAPPING_ERROR;\n\t}\n\n\tfor (i = 0; i < npages; i++) {\n\t\tiommu_gatt_base[iommu_page + i] = GPTE_ENCODE(phys_mem);\n\t\tphys_mem += PAGE_SIZE;\n\t}\n\treturn iommu_bus_base + iommu_page*PAGE_SIZE + (phys_mem & ~PAGE_MASK);\n}\n\n \nstatic dma_addr_t gart_map_page(struct device *dev, struct page *page,\n\t\t\t\tunsigned long offset, size_t size,\n\t\t\t\tenum dma_data_direction dir,\n\t\t\t\tunsigned long attrs)\n{\n\tunsigned long bus;\n\tphys_addr_t paddr = page_to_phys(page) + offset;\n\n\tif (!need_iommu(dev, paddr, size))\n\t\treturn paddr;\n\n\tbus = dma_map_area(dev, paddr, size, dir, 0);\n\tflush_gart();\n\n\treturn bus;\n}\n\n \nstatic void gart_unmap_page(struct device *dev, dma_addr_t dma_addr,\n\t\t\t    size_t size, enum dma_data_direction dir,\n\t\t\t    unsigned long attrs)\n{\n\tunsigned long iommu_page;\n\tint npages;\n\tint i;\n\n\tif (WARN_ON_ONCE(dma_addr == DMA_MAPPING_ERROR))\n\t\treturn;\n\n\t \n\tif (dma_addr < iommu_bus_base ||\n\t    dma_addr >= iommu_bus_base + iommu_size)\n\t\treturn;\n\n\tiommu_page = (dma_addr - iommu_bus_base)>>PAGE_SHIFT;\n\tnpages = iommu_num_pages(dma_addr, size, PAGE_SIZE);\n\tfor (i = 0; i < npages; i++) {\n\t\tiommu_gatt_base[iommu_page + i] = gart_unmapped_entry;\n\t}\n\tfree_iommu(iommu_page, npages);\n}\n\n \nstatic void gart_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\t\t  enum dma_data_direction dir, unsigned long attrs)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n\tfor_each_sg(sg, s, nents, i) {\n\t\tif (!s->dma_length || !s->length)\n\t\t\tbreak;\n\t\tgart_unmap_page(dev, s->dma_address, s->dma_length, dir, 0);\n\t}\n}\n\n \nstatic int dma_map_sg_nonforce(struct device *dev, struct scatterlist *sg,\n\t\t\t       int nents, int dir)\n{\n\tstruct scatterlist *s;\n\tint i;\n\n#ifdef CONFIG_IOMMU_DEBUG\n\tpr_debug(\"dma_map_sg overflow\\n\");\n#endif\n\n\tfor_each_sg(sg, s, nents, i) {\n\t\tunsigned long addr = sg_phys(s);\n\n\t\tif (nonforced_iommu(dev, addr, s->length)) {\n\t\t\taddr = dma_map_area(dev, addr, s->length, dir, 0);\n\t\t\tif (addr == DMA_MAPPING_ERROR) {\n\t\t\t\tif (i > 0)\n\t\t\t\t\tgart_unmap_sg(dev, sg, i, dir, 0);\n\t\t\t\tnents = 0;\n\t\t\t\tsg[0].dma_length = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\ts->dma_address = addr;\n\t\ts->dma_length = s->length;\n\t}\n\tflush_gart();\n\n\treturn nents;\n}\n\n \nstatic int __dma_map_cont(struct device *dev, struct scatterlist *start,\n\t\t\t  int nelems, struct scatterlist *sout,\n\t\t\t  unsigned long pages)\n{\n\tunsigned long iommu_start = alloc_iommu(dev, pages, 0);\n\tunsigned long iommu_page = iommu_start;\n\tstruct scatterlist *s;\n\tint i;\n\n\tif (iommu_start == -1)\n\t\treturn -ENOMEM;\n\n\tfor_each_sg(start, s, nelems, i) {\n\t\tunsigned long pages, addr;\n\t\tunsigned long phys_addr = s->dma_address;\n\n\t\tBUG_ON(s != start && s->offset);\n\t\tif (s == start) {\n\t\t\tsout->dma_address = iommu_bus_base;\n\t\t\tsout->dma_address += iommu_page*PAGE_SIZE + s->offset;\n\t\t\tsout->dma_length = s->length;\n\t\t} else {\n\t\t\tsout->dma_length += s->length;\n\t\t}\n\n\t\taddr = phys_addr;\n\t\tpages = iommu_num_pages(s->offset, s->length, PAGE_SIZE);\n\t\twhile (pages--) {\n\t\t\tiommu_gatt_base[iommu_page] = GPTE_ENCODE(addr);\n\t\t\taddr += PAGE_SIZE;\n\t\t\tiommu_page++;\n\t\t}\n\t}\n\tBUG_ON(iommu_page - iommu_start != pages);\n\n\treturn 0;\n}\n\nstatic inline int\ndma_map_cont(struct device *dev, struct scatterlist *start, int nelems,\n\t     struct scatterlist *sout, unsigned long pages, int need)\n{\n\tif (!need) {\n\t\tBUG_ON(nelems != 1);\n\t\tsout->dma_address = start->dma_address;\n\t\tsout->dma_length = start->length;\n\t\treturn 0;\n\t}\n\treturn __dma_map_cont(dev, start, nelems, sout, pages);\n}\n\n \nstatic int gart_map_sg(struct device *dev, struct scatterlist *sg, int nents,\n\t\t       enum dma_data_direction dir, unsigned long attrs)\n{\n\tstruct scatterlist *s, *ps, *start_sg, *sgmap;\n\tint need = 0, nextneed, i, out, start, ret;\n\tunsigned long pages = 0;\n\tunsigned int seg_size;\n\tunsigned int max_seg_size;\n\n\tif (nents == 0)\n\t\treturn -EINVAL;\n\n\tout\t\t= 0;\n\tstart\t\t= 0;\n\tstart_sg\t= sg;\n\tsgmap\t\t= sg;\n\tseg_size\t= 0;\n\tmax_seg_size\t= dma_get_max_seg_size(dev);\n\tps\t\t= NULL;  \n\n\tfor_each_sg(sg, s, nents, i) {\n\t\tdma_addr_t addr = sg_phys(s);\n\n\t\ts->dma_address = addr;\n\t\tBUG_ON(s->length == 0);\n\n\t\tnextneed = need_iommu(dev, addr, s->length);\n\n\t\t \n\t\tif (i > start) {\n\t\t\t \n\t\t\tif (!iommu_merge || !nextneed || !need || s->offset ||\n\t\t\t    (s->length + seg_size > max_seg_size) ||\n\t\t\t    (ps->offset + ps->length) % PAGE_SIZE) {\n\t\t\t\tret = dma_map_cont(dev, start_sg, i - start,\n\t\t\t\t\t\t   sgmap, pages, need);\n\t\t\t\tif (ret < 0)\n\t\t\t\t\tgoto error;\n\t\t\t\tout++;\n\n\t\t\t\tseg_size\t= 0;\n\t\t\t\tsgmap\t\t= sg_next(sgmap);\n\t\t\t\tpages\t\t= 0;\n\t\t\t\tstart\t\t= i;\n\t\t\t\tstart_sg\t= s;\n\t\t\t}\n\t\t}\n\n\t\tseg_size += s->length;\n\t\tneed = nextneed;\n\t\tpages += iommu_num_pages(s->offset, s->length, PAGE_SIZE);\n\t\tps = s;\n\t}\n\tret = dma_map_cont(dev, start_sg, i - start, sgmap, pages, need);\n\tif (ret < 0)\n\t\tgoto error;\n\tout++;\n\tflush_gart();\n\tif (out < nents) {\n\t\tsgmap = sg_next(sgmap);\n\t\tsgmap->dma_length = 0;\n\t}\n\treturn out;\n\nerror:\n\tflush_gart();\n\tgart_unmap_sg(dev, sg, out, dir, 0);\n\n\t \n\tif (force_iommu || iommu_merge) {\n\t\tout = dma_map_sg_nonforce(dev, sg, nents, dir);\n\t\tif (out > 0)\n\t\t\treturn out;\n\t}\n\tif (panic_on_overflow)\n\t\tpanic(\"dma_map_sg: overflow on %lu pages\\n\", pages);\n\n\tiommu_full(dev, pages << PAGE_SHIFT, dir);\n\treturn ret;\n}\n\n \nstatic void *\ngart_alloc_coherent(struct device *dev, size_t size, dma_addr_t *dma_addr,\n\t\t    gfp_t flag, unsigned long attrs)\n{\n\tvoid *vaddr;\n\n\tvaddr = dma_direct_alloc(dev, size, dma_addr, flag, attrs);\n\tif (!vaddr ||\n\t    !force_iommu || dev->coherent_dma_mask <= DMA_BIT_MASK(24))\n\t\treturn vaddr;\n\n\t*dma_addr = dma_map_area(dev, virt_to_phys(vaddr), size,\n\t\t\tDMA_BIDIRECTIONAL, (1UL << get_order(size)) - 1);\n\tflush_gart();\n\tif (unlikely(*dma_addr == DMA_MAPPING_ERROR))\n\t\tgoto out_free;\n\treturn vaddr;\nout_free:\n\tdma_direct_free(dev, size, vaddr, *dma_addr, attrs);\n\treturn NULL;\n}\n\n \nstatic void\ngart_free_coherent(struct device *dev, size_t size, void *vaddr,\n\t\t   dma_addr_t dma_addr, unsigned long attrs)\n{\n\tgart_unmap_page(dev, dma_addr, size, DMA_BIDIRECTIONAL, 0);\n\tdma_direct_free(dev, size, vaddr, dma_addr, attrs);\n}\n\nstatic int no_agp;\n\nstatic __init unsigned long check_iommu_size(unsigned long aper, u64 aper_size)\n{\n\tunsigned long a;\n\n\tif (!iommu_size) {\n\t\tiommu_size = aper_size;\n\t\tif (!no_agp)\n\t\t\tiommu_size /= 2;\n\t}\n\n\ta = aper + iommu_size;\n\tiommu_size -= round_up(a, PMD_SIZE) - a;\n\n\tif (iommu_size < 64*1024*1024) {\n\t\tpr_warn(\"PCI-DMA: Warning: Small IOMMU %luMB.\"\n\t\t\t\" Consider increasing the AGP aperture in BIOS\\n\",\n\t\t\tiommu_size >> 20);\n\t}\n\n\treturn iommu_size;\n}\n\nstatic __init unsigned read_aperture(struct pci_dev *dev, u32 *size)\n{\n\tunsigned aper_size = 0, aper_base_32, aper_order;\n\tu64 aper_base;\n\n\tpci_read_config_dword(dev, AMD64_GARTAPERTUREBASE, &aper_base_32);\n\tpci_read_config_dword(dev, AMD64_GARTAPERTURECTL, &aper_order);\n\taper_order = (aper_order >> 1) & 7;\n\n\taper_base = aper_base_32 & 0x7fff;\n\taper_base <<= 25;\n\n\taper_size = (32 * 1024 * 1024) << aper_order;\n\tif (aper_base + aper_size > 0x100000000UL || !aper_size)\n\t\taper_base = 0;\n\n\t*size = aper_size;\n\treturn aper_base;\n}\n\nstatic void enable_gart_translations(void)\n{\n\tint i;\n\n\tif (!amd_nb_has_feature(AMD_NB_GART))\n\t\treturn;\n\n\tfor (i = 0; i < amd_nb_num(); i++) {\n\t\tstruct pci_dev *dev = node_to_amd_nb(i)->misc;\n\n\t\tenable_gart_translation(dev, __pa(agp_gatt_table));\n\t}\n\n\t \n\tamd_flush_garts();\n}\n\n \nstatic bool fix_up_north_bridges;\nstatic u32 aperture_order;\nstatic u32 aperture_alloc;\n\nvoid set_up_gart_resume(u32 aper_order, u32 aper_alloc)\n{\n\tfix_up_north_bridges = true;\n\taperture_order = aper_order;\n\taperture_alloc = aper_alloc;\n}\n\nstatic void gart_fixup_northbridges(void)\n{\n\tint i;\n\n\tif (!fix_up_north_bridges)\n\t\treturn;\n\n\tif (!amd_nb_has_feature(AMD_NB_GART))\n\t\treturn;\n\n\tpr_info(\"PCI-DMA: Restoring GART aperture settings\\n\");\n\n\tfor (i = 0; i < amd_nb_num(); i++) {\n\t\tstruct pci_dev *dev = node_to_amd_nb(i)->misc;\n\n\t\t \n\t\tgart_set_size_and_enable(dev, aperture_order);\n\t\tpci_write_config_dword(dev, AMD64_GARTAPERTUREBASE, aperture_alloc >> 25);\n\t}\n}\n\nstatic void gart_resume(void)\n{\n\tpr_info(\"PCI-DMA: Resuming GART IOMMU\\n\");\n\n\tgart_fixup_northbridges();\n\n\tenable_gart_translations();\n}\n\nstatic struct syscore_ops gart_syscore_ops = {\n\t.resume\t\t= gart_resume,\n\n};\n\n \nstatic __init int init_amd_gatt(struct agp_kern_info *info)\n{\n\tunsigned aper_size, gatt_size, new_aper_size;\n\tunsigned aper_base, new_aper_base;\n\tstruct pci_dev *dev;\n\tvoid *gatt;\n\tint i;\n\n\tpr_info(\"PCI-DMA: Disabling AGP.\\n\");\n\n\taper_size = aper_base = info->aper_size = 0;\n\tdev = NULL;\n\tfor (i = 0; i < amd_nb_num(); i++) {\n\t\tdev = node_to_amd_nb(i)->misc;\n\t\tnew_aper_base = read_aperture(dev, &new_aper_size);\n\t\tif (!new_aper_base)\n\t\t\tgoto nommu;\n\n\t\tif (!aper_base) {\n\t\t\taper_size = new_aper_size;\n\t\t\taper_base = new_aper_base;\n\t\t}\n\t\tif (aper_size != new_aper_size || aper_base != new_aper_base)\n\t\t\tgoto nommu;\n\t}\n\tif (!aper_base)\n\t\tgoto nommu;\n\n\tinfo->aper_base = aper_base;\n\tinfo->aper_size = aper_size >> 20;\n\n\tgatt_size = (aper_size >> PAGE_SHIFT) * sizeof(u32);\n\tgatt = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\tget_order(gatt_size));\n\tif (!gatt)\n\t\tpanic(\"Cannot allocate GATT table\");\n\tif (set_memory_uc((unsigned long)gatt, gatt_size >> PAGE_SHIFT))\n\t\tpanic(\"Could not set GART PTEs to uncacheable pages\");\n\n\tagp_gatt_table = gatt;\n\n\tregister_syscore_ops(&gart_syscore_ops);\n\n\tflush_gart();\n\n\tpr_info(\"PCI-DMA: aperture base @ %x size %u KB\\n\",\n\t       aper_base, aper_size>>10);\n\n\treturn 0;\n\n nommu:\n\t \n\tpr_warn(\"PCI-DMA: More than 4GB of RAM and no IOMMU - falling back to iommu=soft.\\n\");\n\treturn -1;\n}\n\nstatic const struct dma_map_ops gart_dma_ops = {\n\t.map_sg\t\t\t\t= gart_map_sg,\n\t.unmap_sg\t\t\t= gart_unmap_sg,\n\t.map_page\t\t\t= gart_map_page,\n\t.unmap_page\t\t\t= gart_unmap_page,\n\t.alloc\t\t\t\t= gart_alloc_coherent,\n\t.free\t\t\t\t= gart_free_coherent,\n\t.mmap\t\t\t\t= dma_common_mmap,\n\t.get_sgtable\t\t\t= dma_common_get_sgtable,\n\t.dma_supported\t\t\t= dma_direct_supported,\n\t.get_required_mask\t\t= dma_direct_get_required_mask,\n\t.alloc_pages\t\t\t= dma_direct_alloc_pages,\n\t.free_pages\t\t\t= dma_direct_free_pages,\n};\n\nstatic void gart_iommu_shutdown(void)\n{\n\tstruct pci_dev *dev;\n\tint i;\n\n\t \n\tif (!no_agp)\n\t\treturn;\n\n\tif (!amd_nb_has_feature(AMD_NB_GART))\n\t\treturn;\n\n\tfor (i = 0; i < amd_nb_num(); i++) {\n\t\tu32 ctl;\n\n\t\tdev = node_to_amd_nb(i)->misc;\n\t\tpci_read_config_dword(dev, AMD64_GARTAPERTURECTL, &ctl);\n\n\t\tctl &= ~GARTEN;\n\n\t\tpci_write_config_dword(dev, AMD64_GARTAPERTURECTL, ctl);\n\t}\n}\n\nint __init gart_iommu_init(void)\n{\n\tstruct agp_kern_info info;\n\tunsigned long iommu_start;\n\tunsigned long aper_base, aper_size;\n\tunsigned long start_pfn, end_pfn;\n\tunsigned long scratch;\n\n\tif (!amd_nb_has_feature(AMD_NB_GART))\n\t\treturn 0;\n\n#ifndef CONFIG_AGP_AMD64\n\tno_agp = 1;\n#else\n\t \n\t \n\tno_agp = no_agp ||\n\t\t(agp_amd64_init() < 0) ||\n\t\t(agp_copy_info(agp_bridge, &info) < 0);\n#endif\n\n\tif (no_iommu ||\n\t    (!force_iommu && max_pfn <= MAX_DMA32_PFN) ||\n\t    !gart_iommu_aperture ||\n\t    (no_agp && init_amd_gatt(&info) < 0)) {\n\t\tif (max_pfn > MAX_DMA32_PFN) {\n\t\t\tpr_warn(\"More than 4GB of memory but GART IOMMU not available.\\n\");\n\t\t\tpr_warn(\"falling back to iommu=soft.\\n\");\n\t\t}\n\t\treturn 0;\n\t}\n\n\t \n\taper_size\t= info.aper_size << 20;\n\taper_base\t= info.aper_base;\n\tend_pfn\t\t= (aper_base>>PAGE_SHIFT) + (aper_size>>PAGE_SHIFT);\n\n\tstart_pfn = PFN_DOWN(aper_base);\n\tif (!pfn_range_is_mapped(start_pfn, end_pfn))\n\t\tinit_memory_mapping(start_pfn<<PAGE_SHIFT, end_pfn<<PAGE_SHIFT,\n\t\t\t\t    PAGE_KERNEL);\n\n\tpr_info(\"PCI-DMA: using GART IOMMU.\\n\");\n\tiommu_size = check_iommu_size(info.aper_base, aper_size);\n\tiommu_pages = iommu_size >> PAGE_SHIFT;\n\n\tiommu_gart_bitmap = (void *) __get_free_pages(GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\t\t      get_order(iommu_pages/8));\n\tif (!iommu_gart_bitmap)\n\t\tpanic(\"Cannot allocate iommu bitmap\\n\");\n\n\tpr_info(\"PCI-DMA: Reserving %luMB of IOMMU area in the AGP aperture\\n\",\n\t       iommu_size >> 20);\n\n\tagp_memory_reserved\t= iommu_size;\n\tiommu_start\t\t= aper_size - iommu_size;\n\tiommu_bus_base\t\t= info.aper_base + iommu_start;\n\tiommu_gatt_base\t\t= agp_gatt_table + (iommu_start>>PAGE_SHIFT);\n\n\t \n\tset_memory_np((unsigned long)__va(iommu_bus_base),\n\t\t\t\tiommu_size >> PAGE_SHIFT);\n\t \n\twbinvd();\n\n\t \n\tenable_gart_translations();\n\n\t \n\tscratch = get_zeroed_page(GFP_KERNEL);\n\tif (!scratch)\n\t\tpanic(\"Cannot allocate iommu scratch page\");\n\tgart_unmapped_entry = GPTE_ENCODE(__pa(scratch));\n\n\tflush_gart();\n\tdma_ops = &gart_dma_ops;\n\tx86_platform.iommu_shutdown = gart_iommu_shutdown;\n\tx86_swiotlb_enable = false;\n\n\treturn 0;\n}\n\nvoid __init gart_parse_options(char *p)\n{\n\tint arg;\n\n\tif (isdigit(*p) && get_option(&p, &arg))\n\t\tiommu_size = arg;\n\tif (!strncmp(p, \"fullflush\", 9))\n\t\tiommu_fullflush = 1;\n\tif (!strncmp(p, \"nofullflush\", 11))\n\t\tiommu_fullflush = 0;\n\tif (!strncmp(p, \"noagp\", 5))\n\t\tno_agp = 1;\n\tif (!strncmp(p, \"noaperture\", 10))\n\t\tfix_aperture = 0;\n\t \n\tif (!strncmp(p, \"force\", 5))\n\t\tgart_iommu_aperture_allowed = 1;\n\tif (!strncmp(p, \"allowed\", 7))\n\t\tgart_iommu_aperture_allowed = 1;\n\tif (!strncmp(p, \"memaper\", 7)) {\n\t\tfallback_aper_force = 1;\n\t\tp += 7;\n\t\tif (*p == '=') {\n\t\t\t++p;\n\t\t\tif (get_option(&p, &arg))\n\t\t\t\tfallback_aper_order = arg;\n\t\t}\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}