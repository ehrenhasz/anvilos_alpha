{
  "module_name": "callthunks.c",
  "hash_id": "7d1e06bea3ec1079342d9a19da1552372b1eca40d00d3ae263b4e527ec643520",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/callthunks.c",
  "human_readable_source": "\n\n#define pr_fmt(fmt) \"callthunks: \" fmt\n\n#include <linux/debugfs.h>\n#include <linux/kallsyms.h>\n#include <linux/memory.h>\n#include <linux/moduleloader.h>\n#include <linux/static_call.h>\n\n#include <asm/alternative.h>\n#include <asm/asm-offsets.h>\n#include <asm/cpu.h>\n#include <asm/ftrace.h>\n#include <asm/insn.h>\n#include <asm/kexec.h>\n#include <asm/nospec-branch.h>\n#include <asm/paravirt.h>\n#include <asm/sections.h>\n#include <asm/switch_to.h>\n#include <asm/sync_core.h>\n#include <asm/text-patching.h>\n#include <asm/xen/hypercall.h>\n\nstatic int __initdata_or_module debug_callthunks;\n\n#define prdbg(fmt, args...)\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\\\n\tif (debug_callthunks)\t\t\t\t\t\\\n\t\tprintk(KERN_DEBUG pr_fmt(fmt), ##args);\t\t\\\n} while(0)\n\nstatic int __init debug_thunks(char *str)\n{\n\tdebug_callthunks = 1;\n\treturn 1;\n}\n__setup(\"debug-callthunks\", debug_thunks);\n\n#ifdef CONFIG_CALL_THUNKS_DEBUG\nDEFINE_PER_CPU(u64, __x86_call_count);\nDEFINE_PER_CPU(u64, __x86_ret_count);\nDEFINE_PER_CPU(u64, __x86_stuffs_count);\nDEFINE_PER_CPU(u64, __x86_ctxsw_count);\nEXPORT_SYMBOL_GPL(__x86_ctxsw_count);\nEXPORT_SYMBOL_GPL(__x86_call_count);\n#endif\n\nextern s32 __call_sites[], __call_sites_end[];\n\nstruct thunk_desc {\n\tvoid\t\t*template;\n\tunsigned int\ttemplate_size;\n};\n\nstruct core_text {\n\tunsigned long\tbase;\n\tunsigned long\tend;\n\tconst char\t*name;\n};\n\nstatic bool thunks_initialized __ro_after_init;\n\nstatic const struct core_text builtin_coretext = {\n\t.base = (unsigned long)_text,\n\t.end  = (unsigned long)_etext,\n\t.name = \"builtin\",\n};\n\nasm (\n\t\".pushsection .rodata\t\t\t\t\\n\"\n\t\".global skl_call_thunk_template\t\t\\n\"\n\t\"skl_call_thunk_template:\t\t\t\\n\"\n\t\t__stringify(INCREMENT_CALL_DEPTH)\"\t\\n\"\n\t\".global skl_call_thunk_tail\t\t\t\\n\"\n\t\"skl_call_thunk_tail:\t\t\t\t\\n\"\n\t\".popsection\t\t\t\t\t\\n\"\n);\n\nextern u8 skl_call_thunk_template[];\nextern u8 skl_call_thunk_tail[];\n\n#define SKL_TMPL_SIZE \\\n\t((unsigned int)(skl_call_thunk_tail - skl_call_thunk_template))\n\nextern void error_entry(void);\nextern void xen_error_entry(void);\nextern void paranoid_entry(void);\n\nstatic inline bool within_coretext(const struct core_text *ct, void *addr)\n{\n\tunsigned long p = (unsigned long)addr;\n\n\treturn ct->base <= p && p < ct->end;\n}\n\nstatic inline bool within_module_coretext(void *addr)\n{\n\tbool ret = false;\n\n#ifdef CONFIG_MODULES\n\tstruct module *mod;\n\n\tpreempt_disable();\n\tmod = __module_address((unsigned long)addr);\n\tif (mod && within_module_core((unsigned long)addr, mod))\n\t\tret = true;\n\tpreempt_enable();\n#endif\n\treturn ret;\n}\n\nstatic bool is_coretext(const struct core_text *ct, void *addr)\n{\n\tif (ct && within_coretext(ct, addr))\n\t\treturn true;\n\tif (within_coretext(&builtin_coretext, addr))\n\t\treturn true;\n\treturn within_module_coretext(addr);\n}\n\nstatic bool skip_addr(void *dest)\n{\n\tif (dest == error_entry)\n\t\treturn true;\n\tif (dest == paranoid_entry)\n\t\treturn true;\n\tif (dest == xen_error_entry)\n\t\treturn true;\n\t \n\tif (dest == __switch_to_asm)\n\t\treturn true;\n\t \n\tif (dest == ret_from_fork)\n\t\treturn true;\n#if defined(CONFIG_HOTPLUG_CPU) && defined(CONFIG_AMD_MEM_ENCRYPT)\n\tif (dest == soft_restart_cpu)\n\t\treturn true;\n#endif\n#ifdef CONFIG_FUNCTION_TRACER\n\tif (dest == __fentry__)\n\t\treturn true;\n#endif\n#ifdef CONFIG_KEXEC_CORE\n\tif (dest >= (void *)relocate_kernel &&\n\t    dest < (void*)relocate_kernel + KEXEC_CONTROL_CODE_MAX_SIZE)\n\t\treturn true;\n#endif\n#ifdef CONFIG_XEN\n\tif (dest >= (void *)hypercall_page &&\n\t    dest < (void*)hypercall_page + PAGE_SIZE)\n\t\treturn true;\n#endif\n\treturn false;\n}\n\nstatic __init_or_module void *call_get_dest(void *addr)\n{\n\tstruct insn insn;\n\tvoid *dest;\n\tint ret;\n\n\tret = insn_decode_kernel(&insn, addr);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\t \n\tif (insn.opcode.bytes[0] != CALL_INSN_OPCODE)\n\t\treturn NULL;\n\n\tdest = addr + insn.length + insn.immediate.value;\n\tif (skip_addr(dest))\n\t\treturn NULL;\n\treturn dest;\n}\n\nstatic const u8 nops[] = {\n\t0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90,\n\t0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90,\n\t0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90,\n\t0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90,\n};\n\nstatic void *patch_dest(void *dest, bool direct)\n{\n\tunsigned int tsize = SKL_TMPL_SIZE;\n\tu8 *pad = dest - tsize;\n\n\t \n\tif (!bcmp(pad, skl_call_thunk_template, tsize))\n\t\treturn pad;\n\n\t \n\tif (bcmp(pad, nops, tsize)) {\n\t\tpr_warn_once(\"Invalid padding area for %pS\\n\", dest);\n\t\treturn NULL;\n\t}\n\n\tif (direct)\n\t\tmemcpy(pad, skl_call_thunk_template, tsize);\n\telse\n\t\ttext_poke_copy_locked(pad, skl_call_thunk_template, tsize, true);\n\treturn pad;\n}\n\nstatic __init_or_module void patch_call(void *addr, const struct core_text *ct)\n{\n\tvoid *pad, *dest;\n\tu8 bytes[8];\n\n\tif (!within_coretext(ct, addr))\n\t\treturn;\n\n\tdest = call_get_dest(addr);\n\tif (!dest || WARN_ON_ONCE(IS_ERR(dest)))\n\t\treturn;\n\n\tif (!is_coretext(ct, dest))\n\t\treturn;\n\n\tpad = patch_dest(dest, within_coretext(ct, dest));\n\tif (!pad)\n\t\treturn;\n\n\tprdbg(\"Patch call at: %pS %px to %pS %px -> %px \\n\", addr, addr,\n\t\tdest, dest, pad);\n\t__text_gen_insn(bytes, CALL_INSN_OPCODE, addr, pad, CALL_INSN_SIZE);\n\ttext_poke_early(addr, bytes, CALL_INSN_SIZE);\n}\n\nstatic __init_or_module void\npatch_call_sites(s32 *start, s32 *end, const struct core_text *ct)\n{\n\ts32 *s;\n\n\tfor (s = start; s < end; s++)\n\t\tpatch_call((void *)s + *s, ct);\n}\n\nstatic __init_or_module void\npatch_paravirt_call_sites(struct paravirt_patch_site *start,\n\t\t\t  struct paravirt_patch_site *end,\n\t\t\t  const struct core_text *ct)\n{\n\tstruct paravirt_patch_site *p;\n\n\tfor (p = start; p < end; p++)\n\t\tpatch_call(p->instr, ct);\n}\n\nstatic __init_or_module void\ncallthunks_setup(struct callthunk_sites *cs, const struct core_text *ct)\n{\n\tprdbg(\"Patching call sites %s\\n\", ct->name);\n\tpatch_call_sites(cs->call_start, cs->call_end, ct);\n\tpatch_paravirt_call_sites(cs->pv_start, cs->pv_end, ct);\n\tprdbg(\"Patching call sites done%s\\n\", ct->name);\n}\n\nvoid __init callthunks_patch_builtin_calls(void)\n{\n\tstruct callthunk_sites cs = {\n\t\t.call_start\t= __call_sites,\n\t\t.call_end\t= __call_sites_end,\n\t\t.pv_start\t= __parainstructions,\n\t\t.pv_end\t\t= __parainstructions_end\n\t};\n\n\tif (!cpu_feature_enabled(X86_FEATURE_CALL_DEPTH))\n\t\treturn;\n\n\tpr_info(\"Setting up call depth tracking\\n\");\n\tmutex_lock(&text_mutex);\n\tcallthunks_setup(&cs, &builtin_coretext);\n\tthunks_initialized = true;\n\tmutex_unlock(&text_mutex);\n}\n\nvoid *callthunks_translate_call_dest(void *dest)\n{\n\tvoid *target;\n\n\tlockdep_assert_held(&text_mutex);\n\n\tif (!thunks_initialized || skip_addr(dest))\n\t\treturn dest;\n\n\tif (!is_coretext(NULL, dest))\n\t\treturn dest;\n\n\ttarget = patch_dest(dest, false);\n\treturn target ? : dest;\n}\n\n#ifdef CONFIG_BPF_JIT\nstatic bool is_callthunk(void *addr)\n{\n\tunsigned int tmpl_size = SKL_TMPL_SIZE;\n\tvoid *tmpl = skl_call_thunk_template;\n\tunsigned long dest;\n\n\tdest = roundup((unsigned long)addr, CONFIG_FUNCTION_ALIGNMENT);\n\tif (!thunks_initialized || skip_addr((void *)dest))\n\t\treturn false;\n\n\treturn !bcmp((void *)(dest - tmpl_size), tmpl, tmpl_size);\n}\n\nint x86_call_depth_emit_accounting(u8 **pprog, void *func)\n{\n\tunsigned int tmpl_size = SKL_TMPL_SIZE;\n\tvoid *tmpl = skl_call_thunk_template;\n\n\tif (!thunks_initialized)\n\t\treturn 0;\n\n\t \n\tif (func && is_callthunk(func))\n\t\treturn 0;\n\n\tmemcpy(*pprog, tmpl, tmpl_size);\n\t*pprog += tmpl_size;\n\treturn tmpl_size;\n}\n#endif\n\n#ifdef CONFIG_MODULES\nvoid noinline callthunks_patch_module_calls(struct callthunk_sites *cs,\n\t\t\t\t\t    struct module *mod)\n{\n\tstruct core_text ct = {\n\t\t.base = (unsigned long)mod->mem[MOD_TEXT].base,\n\t\t.end  = (unsigned long)mod->mem[MOD_TEXT].base + mod->mem[MOD_TEXT].size,\n\t\t.name = mod->name,\n\t};\n\n\tif (!thunks_initialized)\n\t\treturn;\n\n\tmutex_lock(&text_mutex);\n\tcallthunks_setup(cs, &ct);\n\tmutex_unlock(&text_mutex);\n}\n#endif  \n\n#if defined(CONFIG_CALL_THUNKS_DEBUG) && defined(CONFIG_DEBUG_FS)\nstatic int callthunks_debug_show(struct seq_file *m, void *p)\n{\n\tunsigned long cpu = (unsigned long)m->private;\n\n\tseq_printf(m, \"C: %16llu R: %16llu S: %16llu X: %16llu\\n,\",\n\t\t   per_cpu(__x86_call_count, cpu),\n\t\t   per_cpu(__x86_ret_count, cpu),\n\t\t   per_cpu(__x86_stuffs_count, cpu),\n\t\t   per_cpu(__x86_ctxsw_count, cpu));\n\treturn 0;\n}\n\nstatic int callthunks_debug_open(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, callthunks_debug_show, inode->i_private);\n}\n\nstatic const struct file_operations dfs_ops = {\n\t.open\t\t= callthunks_debug_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= single_release,\n};\n\nstatic int __init callthunks_debugfs_init(void)\n{\n\tstruct dentry *dir;\n\tunsigned long cpu;\n\n\tdir = debugfs_create_dir(\"callthunks\", NULL);\n\tfor_each_possible_cpu(cpu) {\n\t\tvoid *arg = (void *)cpu;\n\t\tchar name [10];\n\n\t\tsprintf(name, \"cpu%lu\", cpu);\n\t\tdebugfs_create_file(name, 0644, dir, arg, &dfs_ops);\n\t}\n\treturn 0;\n}\n__initcall(callthunks_debugfs_init);\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}