{
  "module_name": "process.c",
  "hash_id": "6c9b7c5ce07dbbceb213d452085f115ce97aea3263df5dbb4019f7a822636eb8",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/process.c",
  "human_readable_source": "\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/errno.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/cpu.h>\n#include <linux/prctl.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/sched/idle.h>\n#include <linux/sched/debug.h>\n#include <linux/sched/task.h>\n#include <linux/sched/task_stack.h>\n#include <linux/init.h>\n#include <linux/export.h>\n#include <linux/pm.h>\n#include <linux/tick.h>\n#include <linux/random.h>\n#include <linux/user-return-notifier.h>\n#include <linux/dmi.h>\n#include <linux/utsname.h>\n#include <linux/stackprotector.h>\n#include <linux/cpuidle.h>\n#include <linux/acpi.h>\n#include <linux/elf-randomize.h>\n#include <linux/static_call.h>\n#include <trace/events/power.h>\n#include <linux/hw_breakpoint.h>\n#include <linux/entry-common.h>\n#include <asm/cpu.h>\n#include <asm/apic.h>\n#include <linux/uaccess.h>\n#include <asm/mwait.h>\n#include <asm/fpu/api.h>\n#include <asm/fpu/sched.h>\n#include <asm/fpu/xstate.h>\n#include <asm/debugreg.h>\n#include <asm/nmi.h>\n#include <asm/tlbflush.h>\n#include <asm/mce.h>\n#include <asm/vm86.h>\n#include <asm/switch_to.h>\n#include <asm/desc.h>\n#include <asm/prctl.h>\n#include <asm/spec-ctrl.h>\n#include <asm/io_bitmap.h>\n#include <asm/proto.h>\n#include <asm/frame.h>\n#include <asm/unwind.h>\n#include <asm/tdx.h>\n#include <asm/mmu_context.h>\n#include <asm/shstk.h>\n\n#include \"process.h\"\n\n \n__visible DEFINE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw) = {\n\t.x86_tss = {\n\t\t \n\t\t.sp0 = (1UL << (BITS_PER_LONG-1)) + 1,\n\n#ifdef CONFIG_X86_32\n\t\t.sp1 = TOP_OF_INIT_STACK,\n\n\t\t.ss0 = __KERNEL_DS,\n\t\t.ss1 = __KERNEL_CS,\n#endif\n\t\t.io_bitmap_base\t= IO_BITMAP_OFFSET_INVALID,\n\t },\n};\nEXPORT_PER_CPU_SYMBOL(cpu_tss_rw);\n\nDEFINE_PER_CPU(bool, __tss_limit_invalid);\nEXPORT_PER_CPU_SYMBOL_GPL(__tss_limit_invalid);\n\n \nint arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)\n{\n\tmemcpy(dst, src, arch_task_struct_size);\n#ifdef CONFIG_VM86\n\tdst->thread.vm86 = NULL;\n#endif\n\t \n\tdst->thread.fpu.fpstate = NULL;\n\n\treturn 0;\n}\n\n#ifdef CONFIG_X86_64\nvoid arch_release_task_struct(struct task_struct *tsk)\n{\n\tif (fpu_state_size_dynamic())\n\t\tfpstate_free(&tsk->thread.fpu);\n}\n#endif\n\n \nvoid exit_thread(struct task_struct *tsk)\n{\n\tstruct thread_struct *t = &tsk->thread;\n\tstruct fpu *fpu = &t->fpu;\n\n\tif (test_thread_flag(TIF_IO_BITMAP))\n\t\tio_bitmap_exit(tsk);\n\n\tfree_vm86(t);\n\n\tshstk_free(tsk);\n\tfpu__drop(fpu);\n}\n\nstatic int set_new_tls(struct task_struct *p, unsigned long tls)\n{\n\tstruct user_desc __user *utls = (struct user_desc __user *)tls;\n\n\tif (in_ia32_syscall())\n\t\treturn do_set_thread_area(p, -1, utls, 0);\n\telse\n\t\treturn do_set_thread_area_64(p, ARCH_SET_FS, tls);\n}\n\n__visible void ret_from_fork(struct task_struct *prev, struct pt_regs *regs,\n\t\t\t\t     int (*fn)(void *), void *fn_arg)\n{\n\tschedule_tail(prev);\n\n\t \n\tif (unlikely(fn)) {\n\t\tfn(fn_arg);\n\t\t \n\t\tregs->ax = 0;\n\t}\n\n\tsyscall_exit_to_user_mode(regs);\n}\n\nint copy_thread(struct task_struct *p, const struct kernel_clone_args *args)\n{\n\tunsigned long clone_flags = args->flags;\n\tunsigned long sp = args->stack;\n\tunsigned long tls = args->tls;\n\tstruct inactive_task_frame *frame;\n\tstruct fork_frame *fork_frame;\n\tstruct pt_regs *childregs;\n\tunsigned long new_ssp;\n\tint ret = 0;\n\n\tchildregs = task_pt_regs(p);\n\tfork_frame = container_of(childregs, struct fork_frame, regs);\n\tframe = &fork_frame->frame;\n\n\tframe->bp = encode_frame_pointer(childregs);\n\tframe->ret_addr = (unsigned long) ret_from_fork_asm;\n\tp->thread.sp = (unsigned long) fork_frame;\n\tp->thread.io_bitmap = NULL;\n\tp->thread.iopl_warn = 0;\n\tmemset(p->thread.ptrace_bps, 0, sizeof(p->thread.ptrace_bps));\n\n#ifdef CONFIG_X86_64\n\tcurrent_save_fsgs();\n\tp->thread.fsindex = current->thread.fsindex;\n\tp->thread.fsbase = current->thread.fsbase;\n\tp->thread.gsindex = current->thread.gsindex;\n\tp->thread.gsbase = current->thread.gsbase;\n\n\tsavesegment(es, p->thread.es);\n\tsavesegment(ds, p->thread.ds);\n\n\tif (p->mm && (clone_flags & (CLONE_VM | CLONE_VFORK)) == CLONE_VM)\n\t\tset_bit(MM_CONTEXT_LOCK_LAM, &p->mm->context.flags);\n#else\n\tp->thread.sp0 = (unsigned long) (childregs + 1);\n\tsavesegment(gs, p->thread.gs);\n\t \n\tframe->flags = X86_EFLAGS_FIXED;\n#endif\n\n\t \n\tnew_ssp = shstk_alloc_thread_stack(p, clone_flags, args->stack_size);\n\tif (IS_ERR_VALUE(new_ssp))\n\t\treturn PTR_ERR((void *)new_ssp);\n\n\tfpu_clone(p, clone_flags, args->fn, new_ssp);\n\n\t \n\tif (unlikely(p->flags & PF_KTHREAD)) {\n\t\tp->thread.pkru = pkru_get_init_value();\n\t\tmemset(childregs, 0, sizeof(struct pt_regs));\n\t\tkthread_frame_init(frame, args->fn, args->fn_arg);\n\t\treturn 0;\n\t}\n\n\t \n\tp->thread.pkru = read_pkru();\n\n\tframe->bx = 0;\n\t*childregs = *current_pt_regs();\n\tchildregs->ax = 0;\n\tif (sp)\n\t\tchildregs->sp = sp;\n\n\tif (unlikely(args->fn)) {\n\t\t \n\t\tchildregs->sp = 0;\n\t\tchildregs->ip = 0;\n\t\tkthread_frame_init(frame, args->fn, args->fn_arg);\n\t\treturn 0;\n\t}\n\n\t \n\tif (clone_flags & CLONE_SETTLS)\n\t\tret = set_new_tls(p, tls);\n\n\tif (!ret && unlikely(test_tsk_thread_flag(current, TIF_IO_BITMAP)))\n\t\tio_bitmap_share(p);\n\n\treturn ret;\n}\n\nstatic void pkru_flush_thread(void)\n{\n\t \n\tpkru_write_default();\n}\n\nvoid flush_thread(void)\n{\n\tstruct task_struct *tsk = current;\n\n\tflush_ptrace_hw_breakpoint(tsk);\n\tmemset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));\n\n\tfpu_flush_thread();\n\tpkru_flush_thread();\n}\n\nvoid disable_TSC(void)\n{\n\tpreempt_disable();\n\tif (!test_and_set_thread_flag(TIF_NOTSC))\n\t\t \n\t\tcr4_set_bits(X86_CR4_TSD);\n\tpreempt_enable();\n}\n\nstatic void enable_TSC(void)\n{\n\tpreempt_disable();\n\tif (test_and_clear_thread_flag(TIF_NOTSC))\n\t\t \n\t\tcr4_clear_bits(X86_CR4_TSD);\n\tpreempt_enable();\n}\n\nint get_tsc_mode(unsigned long adr)\n{\n\tunsigned int val;\n\n\tif (test_thread_flag(TIF_NOTSC))\n\t\tval = PR_TSC_SIGSEGV;\n\telse\n\t\tval = PR_TSC_ENABLE;\n\n\treturn put_user(val, (unsigned int __user *)adr);\n}\n\nint set_tsc_mode(unsigned int val)\n{\n\tif (val == PR_TSC_SIGSEGV)\n\t\tdisable_TSC();\n\telse if (val == PR_TSC_ENABLE)\n\t\tenable_TSC();\n\telse\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nDEFINE_PER_CPU(u64, msr_misc_features_shadow);\n\nstatic void set_cpuid_faulting(bool on)\n{\n\tu64 msrval;\n\n\tmsrval = this_cpu_read(msr_misc_features_shadow);\n\tmsrval &= ~MSR_MISC_FEATURES_ENABLES_CPUID_FAULT;\n\tmsrval |= (on << MSR_MISC_FEATURES_ENABLES_CPUID_FAULT_BIT);\n\tthis_cpu_write(msr_misc_features_shadow, msrval);\n\twrmsrl(MSR_MISC_FEATURES_ENABLES, msrval);\n}\n\nstatic void disable_cpuid(void)\n{\n\tpreempt_disable();\n\tif (!test_and_set_thread_flag(TIF_NOCPUID)) {\n\t\t \n\t\tset_cpuid_faulting(true);\n\t}\n\tpreempt_enable();\n}\n\nstatic void enable_cpuid(void)\n{\n\tpreempt_disable();\n\tif (test_and_clear_thread_flag(TIF_NOCPUID)) {\n\t\t \n\t\tset_cpuid_faulting(false);\n\t}\n\tpreempt_enable();\n}\n\nstatic int get_cpuid_mode(void)\n{\n\treturn !test_thread_flag(TIF_NOCPUID);\n}\n\nstatic int set_cpuid_mode(unsigned long cpuid_enabled)\n{\n\tif (!boot_cpu_has(X86_FEATURE_CPUID_FAULT))\n\t\treturn -ENODEV;\n\n\tif (cpuid_enabled)\n\t\tenable_cpuid();\n\telse\n\t\tdisable_cpuid();\n\n\treturn 0;\n}\n\n \nvoid arch_setup_new_exec(void)\n{\n\t \n\tif (test_thread_flag(TIF_NOCPUID))\n\t\tenable_cpuid();\n\n\t \n\tif (test_thread_flag(TIF_SSBD) &&\n\t    task_spec_ssb_noexec(current)) {\n\t\tclear_thread_flag(TIF_SSBD);\n\t\ttask_clear_spec_ssb_disable(current);\n\t\ttask_clear_spec_ssb_noexec(current);\n\t\tspeculation_ctrl_update(read_thread_flags());\n\t}\n\n\tmm_reset_untag_mask(current->mm);\n}\n\n#ifdef CONFIG_X86_IOPL_IOPERM\nstatic inline void switch_to_bitmap(unsigned long tifp)\n{\n\t \n\tif (tifp & _TIF_IO_BITMAP)\n\t\ttss_invalidate_io_bitmap();\n}\n\nstatic void tss_copy_io_bitmap(struct tss_struct *tss, struct io_bitmap *iobm)\n{\n\t \n\tmemcpy(tss->io_bitmap.bitmap, iobm->bitmap,\n\t       max(tss->io_bitmap.prev_max, iobm->max));\n\n\t \n\ttss->io_bitmap.prev_max = iobm->max;\n\ttss->io_bitmap.prev_sequence = iobm->sequence;\n}\n\n \nvoid native_tss_update_io_bitmap(void)\n{\n\tstruct tss_struct *tss = this_cpu_ptr(&cpu_tss_rw);\n\tstruct thread_struct *t = &current->thread;\n\tu16 *base = &tss->x86_tss.io_bitmap_base;\n\n\tif (!test_thread_flag(TIF_IO_BITMAP)) {\n\t\tnative_tss_invalidate_io_bitmap();\n\t\treturn;\n\t}\n\n\tif (IS_ENABLED(CONFIG_X86_IOPL_IOPERM) && t->iopl_emul == 3) {\n\t\t*base = IO_BITMAP_OFFSET_VALID_ALL;\n\t} else {\n\t\tstruct io_bitmap *iobm = t->io_bitmap;\n\n\t\t \n\t\tif (tss->io_bitmap.prev_sequence != iobm->sequence)\n\t\t\ttss_copy_io_bitmap(tss, iobm);\n\n\t\t \n\t\t*base = IO_BITMAP_OFFSET_VALID_MAP;\n\t}\n\n\t \n\trefresh_tss_limit();\n}\n#else  \nstatic inline void switch_to_bitmap(unsigned long tifp) { }\n#endif\n\n#ifdef CONFIG_SMP\n\nstruct ssb_state {\n\tstruct ssb_state\t*shared_state;\n\traw_spinlock_t\t\tlock;\n\tunsigned int\t\tdisable_state;\n\tunsigned long\t\tlocal_state;\n};\n\n#define LSTATE_SSB\t0\n\nstatic DEFINE_PER_CPU(struct ssb_state, ssb_state);\n\nvoid speculative_store_bypass_ht_init(void)\n{\n\tstruct ssb_state *st = this_cpu_ptr(&ssb_state);\n\tunsigned int this_cpu = smp_processor_id();\n\tunsigned int cpu;\n\n\tst->local_state = 0;\n\n\t \n\tif (st->shared_state)\n\t\treturn;\n\n\traw_spin_lock_init(&st->lock);\n\n\t \n\tfor_each_cpu(cpu, topology_sibling_cpumask(this_cpu)) {\n\t\tif (cpu == this_cpu)\n\t\t\tcontinue;\n\n\t\tif (!per_cpu(ssb_state, cpu).shared_state)\n\t\t\tcontinue;\n\n\t\t \n\t\tst->shared_state = per_cpu(ssb_state, cpu).shared_state;\n\t\treturn;\n\t}\n\n\t \n\tst->shared_state = st;\n}\n\n \nstatic __always_inline void amd_set_core_ssb_state(unsigned long tifn)\n{\n\tstruct ssb_state *st = this_cpu_ptr(&ssb_state);\n\tu64 msr = x86_amd_ls_cfg_base;\n\n\tif (!static_cpu_has(X86_FEATURE_ZEN)) {\n\t\tmsr |= ssbd_tif_to_amd_ls_cfg(tifn);\n\t\twrmsrl(MSR_AMD64_LS_CFG, msr);\n\t\treturn;\n\t}\n\n\tif (tifn & _TIF_SSBD) {\n\t\t \n\t\tif (__test_and_set_bit(LSTATE_SSB, &st->local_state))\n\t\t\treturn;\n\n\t\tmsr |= x86_amd_ls_cfg_ssbd_mask;\n\n\t\traw_spin_lock(&st->shared_state->lock);\n\t\t \n\t\tif (!st->shared_state->disable_state)\n\t\t\twrmsrl(MSR_AMD64_LS_CFG, msr);\n\t\tst->shared_state->disable_state++;\n\t\traw_spin_unlock(&st->shared_state->lock);\n\t} else {\n\t\tif (!__test_and_clear_bit(LSTATE_SSB, &st->local_state))\n\t\t\treturn;\n\n\t\traw_spin_lock(&st->shared_state->lock);\n\t\tst->shared_state->disable_state--;\n\t\tif (!st->shared_state->disable_state)\n\t\t\twrmsrl(MSR_AMD64_LS_CFG, msr);\n\t\traw_spin_unlock(&st->shared_state->lock);\n\t}\n}\n#else\nstatic __always_inline void amd_set_core_ssb_state(unsigned long tifn)\n{\n\tu64 msr = x86_amd_ls_cfg_base | ssbd_tif_to_amd_ls_cfg(tifn);\n\n\twrmsrl(MSR_AMD64_LS_CFG, msr);\n}\n#endif\n\nstatic __always_inline void amd_set_ssb_virt_state(unsigned long tifn)\n{\n\t \n\twrmsrl(MSR_AMD64_VIRT_SPEC_CTRL, ssbd_tif_to_spec_ctrl(tifn));\n}\n\n \nstatic __always_inline void __speculation_ctrl_update(unsigned long tifp,\n\t\t\t\t\t\t      unsigned long tifn)\n{\n\tunsigned long tif_diff = tifp ^ tifn;\n\tu64 msr = x86_spec_ctrl_base;\n\tbool updmsr = false;\n\n\tlockdep_assert_irqs_disabled();\n\n\t \n\tif (static_cpu_has(X86_FEATURE_VIRT_SSBD)) {\n\t\tif (tif_diff & _TIF_SSBD)\n\t\t\tamd_set_ssb_virt_state(tifn);\n\t} else if (static_cpu_has(X86_FEATURE_LS_CFG_SSBD)) {\n\t\tif (tif_diff & _TIF_SSBD)\n\t\t\tamd_set_core_ssb_state(tifn);\n\t} else if (static_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD) ||\n\t\t   static_cpu_has(X86_FEATURE_AMD_SSBD)) {\n\t\tupdmsr |= !!(tif_diff & _TIF_SSBD);\n\t\tmsr |= ssbd_tif_to_spec_ctrl(tifn);\n\t}\n\n\t \n\tif (IS_ENABLED(CONFIG_SMP) &&\n\t    static_branch_unlikely(&switch_to_cond_stibp)) {\n\t\tupdmsr |= !!(tif_diff & _TIF_SPEC_IB);\n\t\tmsr |= stibp_tif_to_spec_ctrl(tifn);\n\t}\n\n\tif (updmsr)\n\t\tupdate_spec_ctrl_cond(msr);\n}\n\nstatic unsigned long speculation_ctrl_update_tif(struct task_struct *tsk)\n{\n\tif (test_and_clear_tsk_thread_flag(tsk, TIF_SPEC_FORCE_UPDATE)) {\n\t\tif (task_spec_ssb_disable(tsk))\n\t\t\tset_tsk_thread_flag(tsk, TIF_SSBD);\n\t\telse\n\t\t\tclear_tsk_thread_flag(tsk, TIF_SSBD);\n\n\t\tif (task_spec_ib_disable(tsk))\n\t\t\tset_tsk_thread_flag(tsk, TIF_SPEC_IB);\n\t\telse\n\t\t\tclear_tsk_thread_flag(tsk, TIF_SPEC_IB);\n\t}\n\t \n\treturn read_task_thread_flags(tsk);\n}\n\nvoid speculation_ctrl_update(unsigned long tif)\n{\n\tunsigned long flags;\n\n\t \n\tlocal_irq_save(flags);\n\t__speculation_ctrl_update(~tif, tif);\n\tlocal_irq_restore(flags);\n}\n\n \nvoid speculation_ctrl_update_current(void)\n{\n\tpreempt_disable();\n\tspeculation_ctrl_update(speculation_ctrl_update_tif(current));\n\tpreempt_enable();\n}\n\nstatic inline void cr4_toggle_bits_irqsoff(unsigned long mask)\n{\n\tunsigned long newval, cr4 = this_cpu_read(cpu_tlbstate.cr4);\n\n\tnewval = cr4 ^ mask;\n\tif (newval != cr4) {\n\t\tthis_cpu_write(cpu_tlbstate.cr4, newval);\n\t\t__write_cr4(newval);\n\t}\n}\n\nvoid __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p)\n{\n\tunsigned long tifp, tifn;\n\n\ttifn = read_task_thread_flags(next_p);\n\ttifp = read_task_thread_flags(prev_p);\n\n\tswitch_to_bitmap(tifp);\n\n\tpropagate_user_return_notify(prev_p, next_p);\n\n\tif ((tifp & _TIF_BLOCKSTEP || tifn & _TIF_BLOCKSTEP) &&\n\t    arch_has_block_step()) {\n\t\tunsigned long debugctl, msk;\n\n\t\trdmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);\n\t\tdebugctl &= ~DEBUGCTLMSR_BTF;\n\t\tmsk = tifn & _TIF_BLOCKSTEP;\n\t\tdebugctl |= (msk >> TIF_BLOCKSTEP) << DEBUGCTLMSR_BTF_SHIFT;\n\t\twrmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);\n\t}\n\n\tif ((tifp ^ tifn) & _TIF_NOTSC)\n\t\tcr4_toggle_bits_irqsoff(X86_CR4_TSD);\n\n\tif ((tifp ^ tifn) & _TIF_NOCPUID)\n\t\tset_cpuid_faulting(!!(tifn & _TIF_NOCPUID));\n\n\tif (likely(!((tifp | tifn) & _TIF_SPEC_FORCE_UPDATE))) {\n\t\t__speculation_ctrl_update(tifp, tifn);\n\t} else {\n\t\tspeculation_ctrl_update_tif(prev_p);\n\t\ttifn = speculation_ctrl_update_tif(next_p);\n\n\t\t \n\t\t__speculation_ctrl_update(~tifn, tifn);\n\t}\n}\n\n \nunsigned long boot_option_idle_override = IDLE_NO_OVERRIDE;\nEXPORT_SYMBOL(boot_option_idle_override);\n\n \nvoid __cpuidle default_idle(void)\n{\n\traw_safe_halt();\n\traw_local_irq_disable();\n}\n#if defined(CONFIG_APM_MODULE) || defined(CONFIG_HALTPOLL_CPUIDLE_MODULE)\nEXPORT_SYMBOL(default_idle);\n#endif\n\nDEFINE_STATIC_CALL_NULL(x86_idle, default_idle);\n\nstatic bool x86_idle_set(void)\n{\n\treturn !!static_call_query(x86_idle);\n}\n\n#ifndef CONFIG_SMP\nstatic inline void __noreturn play_dead(void)\n{\n\tBUG();\n}\n#endif\n\nvoid arch_cpu_idle_enter(void)\n{\n\ttsc_verify_tsc_adjust(false);\n\tlocal_touch_nmi();\n}\n\nvoid __noreturn arch_cpu_idle_dead(void)\n{\n\tplay_dead();\n}\n\n \nvoid __cpuidle arch_cpu_idle(void)\n{\n\tstatic_call(x86_idle)();\n}\nEXPORT_SYMBOL_GPL(arch_cpu_idle);\n\n#ifdef CONFIG_XEN\nbool xen_set_default_idle(void)\n{\n\tbool ret = x86_idle_set();\n\n\tstatic_call_update(x86_idle, default_idle);\n\n\treturn ret;\n}\n#endif\n\nstruct cpumask cpus_stop_mask;\n\nvoid __noreturn stop_this_cpu(void *dummy)\n{\n\tstruct cpuinfo_x86 *c = this_cpu_ptr(&cpu_info);\n\tunsigned int cpu = smp_processor_id();\n\n\tlocal_irq_disable();\n\n\t \n\tset_cpu_online(cpu, false);\n\tdisable_local_APIC();\n\tmcheck_cpu_clear(c);\n\n\t \n\tif (c->extended_cpuid_level >= 0x8000001f && (cpuid_eax(0x8000001f) & BIT(0)))\n\t\tnative_wbinvd();\n\n\t \n\tcpumask_clear_cpu(cpu, &cpus_stop_mask);\n\n\tfor (;;) {\n\t\t \n\t\tnative_halt();\n\t}\n}\n\n \nstatic void amd_e400_idle(void)\n{\n\t \n\tif (!boot_cpu_has_bug(X86_BUG_AMD_APIC_C1E)) {\n\t\tdefault_idle();\n\t\treturn;\n\t}\n\n\ttick_broadcast_enter();\n\n\tdefault_idle();\n\n\ttick_broadcast_exit();\n}\n\n \nstatic int prefer_mwait_c1_over_halt(const struct cpuinfo_x86 *c)\n{\n\tu32 eax, ebx, ecx, edx;\n\n\t \n\tif (boot_option_idle_override == IDLE_NOMWAIT)\n\t\treturn 0;\n\n\t \n\tif (!cpu_has(c, X86_FEATURE_MWAIT))\n\t\treturn 0;\n\n\t \n\tif (boot_cpu_has_bug(X86_BUG_MONITOR))\n\t\treturn 0;\n\n\tcpuid(CPUID_MWAIT_LEAF, &eax, &ebx, &ecx, &edx);\n\n\t \n\tif (!(ecx & CPUID5_ECX_EXTENSIONS_SUPPORTED))\n\t\treturn 1;\n\n\t \n\treturn (edx & MWAIT_C1_SUBSTATE_MASK);\n}\n\n \nstatic __cpuidle void mwait_idle(void)\n{\n\tif (!current_set_polling_and_test()) {\n\t\tif (this_cpu_has(X86_BUG_CLFLUSH_MONITOR)) {\n\t\t\tmb();  \n\t\t\tclflush((void *)&current_thread_info()->flags);\n\t\t\tmb();  \n\t\t}\n\n\t\t__monitor((void *)&current_thread_info()->flags, 0, 0);\n\t\tif (!need_resched()) {\n\t\t\t__sti_mwait(0, 0);\n\t\t\traw_local_irq_disable();\n\t\t}\n\t}\n\t__current_clr_polling();\n}\n\nvoid select_idle_routine(const struct cpuinfo_x86 *c)\n{\n#ifdef CONFIG_SMP\n\tif (boot_option_idle_override == IDLE_POLL && smp_num_siblings > 1)\n\t\tpr_warn_once(\"WARNING: polling idle and HT enabled, performance may degrade\\n\");\n#endif\n\tif (x86_idle_set() || boot_option_idle_override == IDLE_POLL)\n\t\treturn;\n\n\tif (boot_cpu_has_bug(X86_BUG_AMD_E400)) {\n\t\tpr_info(\"using AMD E400 aware idle routine\\n\");\n\t\tstatic_call_update(x86_idle, amd_e400_idle);\n\t} else if (prefer_mwait_c1_over_halt(c)) {\n\t\tpr_info(\"using mwait in idle threads\\n\");\n\t\tstatic_call_update(x86_idle, mwait_idle);\n\t} else if (cpu_feature_enabled(X86_FEATURE_TDX_GUEST)) {\n\t\tpr_info(\"using TDX aware idle routine\\n\");\n\t\tstatic_call_update(x86_idle, tdx_safe_halt);\n\t} else\n\t\tstatic_call_update(x86_idle, default_idle);\n}\n\nvoid amd_e400_c1e_apic_setup(void)\n{\n\tif (boot_cpu_has_bug(X86_BUG_AMD_APIC_C1E)) {\n\t\tpr_info(\"Switch to broadcast mode on CPU%d\\n\", smp_processor_id());\n\t\tlocal_irq_disable();\n\t\ttick_broadcast_force();\n\t\tlocal_irq_enable();\n\t}\n}\n\nvoid __init arch_post_acpi_subsys_init(void)\n{\n\tu32 lo, hi;\n\n\tif (!boot_cpu_has_bug(X86_BUG_AMD_E400))\n\t\treturn;\n\n\t \n\trdmsr(MSR_K8_INT_PENDING_MSG, lo, hi);\n\tif (!(lo & K8_INTP_C1E_ACTIVE_MASK))\n\t\treturn;\n\n\tboot_cpu_set_bug(X86_BUG_AMD_APIC_C1E);\n\n\tif (!boot_cpu_has(X86_FEATURE_NONSTOP_TSC))\n\t\tmark_tsc_unstable(\"TSC halt in AMD C1E\");\n\tpr_info(\"System has AMD C1E enabled\\n\");\n}\n\nstatic int __init idle_setup(char *str)\n{\n\tif (!str)\n\t\treturn -EINVAL;\n\n\tif (!strcmp(str, \"poll\")) {\n\t\tpr_info(\"using polling idle threads\\n\");\n\t\tboot_option_idle_override = IDLE_POLL;\n\t\tcpu_idle_poll_ctrl(true);\n\t} else if (!strcmp(str, \"halt\")) {\n\t\t \n\t\tstatic_call_update(x86_idle, default_idle);\n\t\tboot_option_idle_override = IDLE_HALT;\n\t} else if (!strcmp(str, \"nomwait\")) {\n\t\t \n\t\tboot_option_idle_override = IDLE_NOMWAIT;\n\t} else\n\t\treturn -1;\n\n\treturn 0;\n}\nearly_param(\"idle\", idle_setup);\n\nunsigned long arch_align_stack(unsigned long sp)\n{\n\tif (!(current->personality & ADDR_NO_RANDOMIZE) && randomize_va_space)\n\t\tsp -= get_random_u32_below(8192);\n\treturn sp & ~0xf;\n}\n\nunsigned long arch_randomize_brk(struct mm_struct *mm)\n{\n\treturn randomize_page(mm->brk, 0x02000000);\n}\n\n \nunsigned long __get_wchan(struct task_struct *p)\n{\n\tstruct unwind_state state;\n\tunsigned long addr = 0;\n\n\tif (!try_get_task_stack(p))\n\t\treturn 0;\n\n\tfor (unwind_start(&state, p, NULL, NULL); !unwind_done(&state);\n\t     unwind_next_frame(&state)) {\n\t\taddr = unwind_get_return_address(&state);\n\t\tif (!addr)\n\t\t\tbreak;\n\t\tif (in_sched_functions(addr))\n\t\t\tcontinue;\n\t\tbreak;\n\t}\n\n\tput_task_stack(p);\n\n\treturn addr;\n}\n\nlong do_arch_prctl_common(int option, unsigned long arg2)\n{\n\tswitch (option) {\n\tcase ARCH_GET_CPUID:\n\t\treturn get_cpuid_mode();\n\tcase ARCH_SET_CPUID:\n\t\treturn set_cpuid_mode(arg2);\n\tcase ARCH_GET_XCOMP_SUPP:\n\tcase ARCH_GET_XCOMP_PERM:\n\tcase ARCH_REQ_XCOMP_PERM:\n\tcase ARCH_GET_XCOMP_GUEST_PERM:\n\tcase ARCH_REQ_XCOMP_GUEST_PERM:\n\t\treturn fpu_xstate_prctl(option, arg2);\n\t}\n\n\treturn -EINVAL;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}