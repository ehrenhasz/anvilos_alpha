{
  "module_name": "opt.c",
  "hash_id": "280968fcbd1657f7c8ce9d799e14b2be49fd87b62bcddd5031c94f7e223a1dae",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/kprobes/opt.c",
  "human_readable_source": "\n \n#include <linux/kprobes.h>\n#include <linux/perf_event.h>\n#include <linux/ptrace.h>\n#include <linux/string.h>\n#include <linux/slab.h>\n#include <linux/hardirq.h>\n#include <linux/preempt.h>\n#include <linux/extable.h>\n#include <linux/kdebug.h>\n#include <linux/kallsyms.h>\n#include <linux/kgdb.h>\n#include <linux/ftrace.h>\n#include <linux/objtool.h>\n#include <linux/pgtable.h>\n#include <linux/static_call.h>\n\n#include <asm/text-patching.h>\n#include <asm/cacheflush.h>\n#include <asm/desc.h>\n#include <linux/uaccess.h>\n#include <asm/alternative.h>\n#include <asm/insn.h>\n#include <asm/debugreg.h>\n#include <asm/set_memory.h>\n#include <asm/sections.h>\n#include <asm/nospec-branch.h>\n\n#include \"common.h\"\n\nunsigned long __recover_optprobed_insn(kprobe_opcode_t *buf, unsigned long addr)\n{\n\tstruct optimized_kprobe *op;\n\tstruct kprobe *kp;\n\tlong offs;\n\tint i;\n\n\tfor (i = 0; i < JMP32_INSN_SIZE; i++) {\n\t\tkp = get_kprobe((void *)addr - i);\n\t\t \n\t\tif (kp && kprobe_optimized(kp)) {\n\t\t\top = container_of(kp, struct optimized_kprobe, kp);\n\t\t\t \n\t\t\tif (list_empty(&op->list) || optprobe_queued_unopt(op))\n\t\t\t\tgoto found;\n\t\t}\n\t}\n\n\treturn addr;\nfound:\n\t \n\tif (copy_from_kernel_nofault(buf, (void *)addr,\n\t\tMAX_INSN_SIZE * sizeof(kprobe_opcode_t)))\n\t\treturn 0UL;\n\n\tif (addr == (unsigned long)kp->addr) {\n\t\tbuf[0] = kp->opcode;\n\t\tmemcpy(buf + 1, op->optinsn.copied_insn, DISP32_SIZE);\n\t} else {\n\t\toffs = addr - (unsigned long)kp->addr - 1;\n\t\tmemcpy(buf, op->optinsn.copied_insn + offs, DISP32_SIZE - offs);\n\t}\n\n\treturn (unsigned long)buf;\n}\n\nstatic void synthesize_clac(kprobe_opcode_t *addr)\n{\n\t \n\tif (!boot_cpu_has(X86_FEATURE_SMAP))\n\t\treturn;\n\n\t \n\taddr[0] = 0x0f;\n\taddr[1] = 0x01;\n\taddr[2] = 0xca;\n}\n\n \nstatic void synthesize_set_arg1(kprobe_opcode_t *addr, unsigned long val)\n{\n#ifdef CONFIG_X86_64\n\t*addr++ = 0x48;\n\t*addr++ = 0xbf;\n#else\n\t*addr++ = 0xb8;\n#endif\n\t*(unsigned long *)addr = val;\n}\n\nasm (\n\t\t\t\".pushsection .rodata\\n\"\n\t\t\t\"optprobe_template_func:\\n\"\n\t\t\t\".global optprobe_template_entry\\n\"\n\t\t\t\"optprobe_template_entry:\\n\"\n#ifdef CONFIG_X86_64\n\t\t\t\"       pushq $\" __stringify(__KERNEL_DS) \"\\n\"\n\t\t\t \n\t\t\t\"\tpushq %rsp\\n\"\n\t\t\t\"\tpushfq\\n\"\n\t\t\t\".global optprobe_template_clac\\n\"\n\t\t\t\"optprobe_template_clac:\\n\"\n\t\t\tASM_NOP3\n\t\t\tSAVE_REGS_STRING\n\t\t\t\"\tmovq %rsp, %rsi\\n\"\n\t\t\t\".global optprobe_template_val\\n\"\n\t\t\t\"optprobe_template_val:\\n\"\n\t\t\tASM_NOP5\n\t\t\tASM_NOP5\n\t\t\t\".global optprobe_template_call\\n\"\n\t\t\t\"optprobe_template_call:\\n\"\n\t\t\tASM_NOP5\n\t\t\t \n\t\t\t\"\tmovq 18*8(%rsp), %rdx\\n\"\n\t\t\t\"\tmovq %rdx, 20*8(%rsp)\\n\"\n\t\t\tRESTORE_REGS_STRING\n\t\t\t \n\t\t\t\"\taddq $16, %rsp\\n\"\n\t\t\t \n\t\t\t\"\tpopfq\\n\"\n#else  \n\t\t\t\"\tpushl %ss\\n\"\n\t\t\t \n\t\t\t\"\tpushl %esp\\n\"\n\t\t\t\"\tpushfl\\n\"\n\t\t\t\".global optprobe_template_clac\\n\"\n\t\t\t\"optprobe_template_clac:\\n\"\n\t\t\tASM_NOP3\n\t\t\tSAVE_REGS_STRING\n\t\t\t\"\tmovl %esp, %edx\\n\"\n\t\t\t\".global optprobe_template_val\\n\"\n\t\t\t\"optprobe_template_val:\\n\"\n\t\t\tASM_NOP5\n\t\t\t\".global optprobe_template_call\\n\"\n\t\t\t\"optprobe_template_call:\\n\"\n\t\t\tASM_NOP5\n\t\t\t \n\t\t\t\"\tmovl 14*4(%esp), %edx\\n\"\n\t\t\t\"\tmovl %edx, 16*4(%esp)\\n\"\n\t\t\tRESTORE_REGS_STRING\n\t\t\t \n\t\t\t\"\taddl $8, %esp\\n\"\n\t\t\t \n\t\t\t\"\tpopfl\\n\"\n#endif\n\t\t\t\".global optprobe_template_end\\n\"\n\t\t\t\"optprobe_template_end:\\n\"\n\t\t\t\".popsection\\n\");\n\nvoid optprobe_template_func(void);\nSTACK_FRAME_NON_STANDARD(optprobe_template_func);\n\n#define TMPL_CLAC_IDX \\\n\t((long)optprobe_template_clac - (long)optprobe_template_entry)\n#define TMPL_MOVE_IDX \\\n\t((long)optprobe_template_val - (long)optprobe_template_entry)\n#define TMPL_CALL_IDX \\\n\t((long)optprobe_template_call - (long)optprobe_template_entry)\n#define TMPL_END_IDX \\\n\t((long)optprobe_template_end - (long)optprobe_template_entry)\n\n \nstatic void\noptimized_callback(struct optimized_kprobe *op, struct pt_regs *regs)\n{\n\t \n\tif (kprobe_disabled(&op->kp))\n\t\treturn;\n\n\tpreempt_disable();\n\tif (kprobe_running()) {\n\t\tkprobes_inc_nmissed_count(&op->kp);\n\t} else {\n\t\tstruct kprobe_ctlblk *kcb = get_kprobe_ctlblk();\n\t\t \n\t\tregs->sp += sizeof(long);\n\t\t \n\t\tregs->cs = __KERNEL_CS;\n#ifdef CONFIG_X86_32\n\t\tregs->gs = 0;\n#endif\n\t\tregs->ip = (unsigned long)op->kp.addr + INT3_INSN_SIZE;\n\t\tregs->orig_ax = ~0UL;\n\n\t\t__this_cpu_write(current_kprobe, &op->kp);\n\t\tkcb->kprobe_status = KPROBE_HIT_ACTIVE;\n\t\topt_pre_handler(&op->kp, regs);\n\t\t__this_cpu_write(current_kprobe, NULL);\n\t}\n\tpreempt_enable();\n}\nNOKPROBE_SYMBOL(optimized_callback);\n\nstatic int copy_optimized_instructions(u8 *dest, u8 *src, u8 *real)\n{\n\tstruct insn insn;\n\tint len = 0, ret;\n\n\twhile (len < JMP32_INSN_SIZE) {\n\t\tret = __copy_instruction(dest + len, src + len, real + len, &insn);\n\t\tif (!ret || !can_boost(&insn, src + len))\n\t\t\treturn -EINVAL;\n\t\tlen += ret;\n\t}\n\t \n\tif (ftrace_text_reserved(src, src + len - 1) ||\n\t    alternatives_text_reserved(src, src + len - 1) ||\n\t    jump_label_text_reserved(src, src + len - 1) ||\n\t    static_call_text_reserved(src, src + len - 1))\n\t\treturn -EBUSY;\n\n\treturn len;\n}\n\n \nstatic int insn_is_indirect_jump(struct insn *insn)\n{\n\treturn ((insn->opcode.bytes[0] == 0xff &&\n\t\t(X86_MODRM_REG(insn->modrm.value) & 6) == 4) ||  \n\t\tinsn->opcode.bytes[0] == 0xea);\t \n}\n\n \nstatic int insn_jump_into_range(struct insn *insn, unsigned long start, int len)\n{\n\tunsigned long target = 0;\n\n\tswitch (insn->opcode.bytes[0]) {\n\tcase 0xe0:\t \n\tcase 0xe1:\t \n\tcase 0xe2:\t \n\tcase 0xe3:\t \n\tcase 0xe9:\t \n\tcase 0xeb:\t \n\t\tbreak;\n\tcase 0x0f:\n\t\tif ((insn->opcode.bytes[1] & 0xf0) == 0x80)  \n\t\t\tbreak;\n\t\treturn 0;\n\tdefault:\n\t\tif ((insn->opcode.bytes[0] & 0xf0) == 0x70)  \n\t\t\tbreak;\n\t\treturn 0;\n\t}\n\ttarget = (unsigned long)insn->next_byte + insn->immediate.value;\n\n\treturn (start <= target && target <= start + len);\n}\n\n \nstatic int can_optimize(unsigned long paddr)\n{\n\tunsigned long addr, size = 0, offset = 0;\n\tstruct insn insn;\n\tkprobe_opcode_t buf[MAX_INSN_SIZE];\n\n\t \n\tif (!kallsyms_lookup_size_offset(paddr, &size, &offset))\n\t\treturn 0;\n\n\t \n\tif (((paddr >= (unsigned long)__entry_text_start) &&\n\t     (paddr <  (unsigned long)__entry_text_end)))\n\t\treturn 0;\n\n\t \n\tif (size - offset < JMP32_INSN_SIZE)\n\t\treturn 0;\n\n\t \n\taddr = paddr - offset;\n\twhile (addr < paddr - offset + size) {  \n\t\tunsigned long recovered_insn;\n\t\tint ret;\n\n\t\tif (search_exception_tables(addr))\n\t\t\t \n\t\t\treturn 0;\n\t\trecovered_insn = recover_probed_instruction(buf, addr);\n\t\tif (!recovered_insn)\n\t\t\treturn 0;\n\n\t\tret = insn_decode_kernel(&insn, (void *)recovered_insn);\n\t\tif (ret < 0)\n\t\t\treturn 0;\n#ifdef CONFIG_KGDB\n\t\t \n\t\tif (insn.opcode.bytes[0] == INT3_INSN_OPCODE &&\n\t\t    kgdb_has_hit_break(addr))\n\t\t\treturn 0;\n#endif\n\t\t \n\t\tinsn.kaddr = (void *)addr;\n\t\tinsn.next_byte = (void *)(addr + insn.length);\n\t\t \n\t\tif (!IS_ENABLED(CONFIG_RETPOLINE) &&\n\t\t    !IS_ENABLED(CONFIG_X86_KERNEL_IBT) &&\n\t\t    insn_is_indirect_jump(&insn))\n\t\t\treturn 0;\n\t\tif (insn_jump_into_range(&insn, paddr + INT3_INSN_SIZE,\n\t\t\t\t\t DISP32_SIZE))\n\t\t\treturn 0;\n\t\taddr += insn.length;\n\t}\n\n\treturn 1;\n}\n\n \nint arch_check_optimized_kprobe(struct optimized_kprobe *op)\n{\n\tint i;\n\tstruct kprobe *p;\n\n\tfor (i = 1; i < op->optinsn.size; i++) {\n\t\tp = get_kprobe(op->kp.addr + i);\n\t\tif (p && !kprobe_disarmed(p))\n\t\t\treturn -EEXIST;\n\t}\n\n\treturn 0;\n}\n\n \nint arch_within_optimized_kprobe(struct optimized_kprobe *op,\n\t\t\t\t kprobe_opcode_t *addr)\n{\n\treturn (op->kp.addr <= addr &&\n\t\top->kp.addr + op->optinsn.size > addr);\n}\n\n \nstatic\nvoid __arch_remove_optimized_kprobe(struct optimized_kprobe *op, int dirty)\n{\n\tu8 *slot = op->optinsn.insn;\n\tif (slot) {\n\t\tint len = TMPL_END_IDX + op->optinsn.size + JMP32_INSN_SIZE;\n\n\t\t \n\t\tif (dirty)\n\t\t\tperf_event_text_poke(slot, slot, len, NULL, 0);\n\n\t\tfree_optinsn_slot(slot, dirty);\n\t\top->optinsn.insn = NULL;\n\t\top->optinsn.size = 0;\n\t}\n}\n\nvoid arch_remove_optimized_kprobe(struct optimized_kprobe *op)\n{\n\t__arch_remove_optimized_kprobe(op, 1);\n}\n\n \nint arch_prepare_optimized_kprobe(struct optimized_kprobe *op,\n\t\t\t\t  struct kprobe *__unused)\n{\n\tu8 *buf = NULL, *slot;\n\tint ret, len;\n\tlong rel;\n\n\tif (!can_optimize((unsigned long)op->kp.addr))\n\t\treturn -EILSEQ;\n\n\tbuf = kzalloc(MAX_OPTINSN_SIZE, GFP_KERNEL);\n\tif (!buf)\n\t\treturn -ENOMEM;\n\n\top->optinsn.insn = slot = get_optinsn_slot();\n\tif (!slot) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t \n\trel = (long)slot - (long)op->kp.addr + JMP32_INSN_SIZE;\n\tif (abs(rel) > 0x7fffffff) {\n\t\tret = -ERANGE;\n\t\tgoto err;\n\t}\n\n\t \n\tmemcpy(buf, optprobe_template_entry, TMPL_END_IDX);\n\n\t \n\tret = copy_optimized_instructions(buf + TMPL_END_IDX, op->kp.addr,\n\t\t\t\t\t  slot + TMPL_END_IDX);\n\tif (ret < 0)\n\t\tgoto err;\n\top->optinsn.size = ret;\n\tlen = TMPL_END_IDX + op->optinsn.size;\n\n\tsynthesize_clac(buf + TMPL_CLAC_IDX);\n\n\t \n\tsynthesize_set_arg1(buf + TMPL_MOVE_IDX, (unsigned long)op);\n\n\t \n\tsynthesize_relcall(buf + TMPL_CALL_IDX,\n\t\t\t   slot + TMPL_CALL_IDX, optimized_callback);\n\n\t \n\tsynthesize_reljump(buf + len, slot + len,\n\t\t\t   (u8 *)op->kp.addr + op->optinsn.size);\n\tlen += JMP32_INSN_SIZE;\n\n\t \n\n\t \n\tperf_event_text_poke(slot, NULL, 0, buf, len);\n\ttext_poke(slot, buf, len);\n\n\tret = 0;\nout:\n\tkfree(buf);\n\treturn ret;\n\nerr:\n\t__arch_remove_optimized_kprobe(op, 0);\n\tgoto out;\n}\n\n \nvoid arch_optimize_kprobes(struct list_head *oplist)\n{\n\tstruct optimized_kprobe *op, *tmp;\n\tu8 insn_buff[JMP32_INSN_SIZE];\n\n\tlist_for_each_entry_safe(op, tmp, oplist, list) {\n\t\ts32 rel = (s32)((long)op->optinsn.insn -\n\t\t\t((long)op->kp.addr + JMP32_INSN_SIZE));\n\n\t\tWARN_ON(kprobe_disabled(&op->kp));\n\n\t\t \n\t\tmemcpy(op->optinsn.copied_insn, op->kp.addr + INT3_INSN_SIZE,\n\t\t       DISP32_SIZE);\n\n\t\tinsn_buff[0] = JMP32_INSN_OPCODE;\n\t\t*(s32 *)(&insn_buff[1]) = rel;\n\n\t\ttext_poke_bp(op->kp.addr, insn_buff, JMP32_INSN_SIZE, NULL);\n\n\t\tlist_del_init(&op->list);\n\t}\n}\n\n \nvoid arch_unoptimize_kprobe(struct optimized_kprobe *op)\n{\n\tu8 new[JMP32_INSN_SIZE] = { INT3_INSN_OPCODE, };\n\tu8 old[JMP32_INSN_SIZE];\n\tu8 *addr = op->kp.addr;\n\n\tmemcpy(old, op->kp.addr, JMP32_INSN_SIZE);\n\tmemcpy(new + INT3_INSN_SIZE,\n\t       op->optinsn.copied_insn,\n\t       JMP32_INSN_SIZE - INT3_INSN_SIZE);\n\n\ttext_poke(addr, new, INT3_INSN_SIZE);\n\ttext_poke_sync();\n\ttext_poke(addr + INT3_INSN_SIZE,\n\t\t  new + INT3_INSN_SIZE,\n\t\t  JMP32_INSN_SIZE - INT3_INSN_SIZE);\n\ttext_poke_sync();\n\n\tperf_event_text_poke(op->kp.addr, old, JMP32_INSN_SIZE, new, JMP32_INSN_SIZE);\n}\n\n \nextern void arch_unoptimize_kprobes(struct list_head *oplist,\n\t\t\t\t    struct list_head *done_list)\n{\n\tstruct optimized_kprobe *op, *tmp;\n\n\tlist_for_each_entry_safe(op, tmp, oplist, list) {\n\t\tarch_unoptimize_kprobe(op);\n\t\tlist_move(&op->list, done_list);\n\t}\n}\n\nint setup_detour_execution(struct kprobe *p, struct pt_regs *regs, int reenter)\n{\n\tstruct optimized_kprobe *op;\n\n\tif (p->flags & KPROBE_FLAG_OPTIMIZED) {\n\t\t \n\t\top = container_of(p, struct optimized_kprobe, kp);\n\t\t \n\t\tregs->ip = (unsigned long)op->optinsn.insn + TMPL_END_IDX;\n\t\tif (!reenter)\n\t\t\treset_current_kprobe();\n\t\treturn 1;\n\t}\n\treturn 0;\n}\nNOKPROBE_SYMBOL(setup_detour_execution);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}