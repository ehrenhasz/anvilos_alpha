{
  "module_name": "ldt.c",
  "hash_id": "97f19be4b93da686081f88682c4abeb9a2b536cac8fcd2b57a948ebed567192c",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/ldt.c",
  "human_readable_source": "\n \n\n#include <linux/errno.h>\n#include <linux/gfp.h>\n#include <linux/sched.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/smp.h>\n#include <linux/syscalls.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/uaccess.h>\n\n#include <asm/ldt.h>\n#include <asm/tlb.h>\n#include <asm/desc.h>\n#include <asm/mmu_context.h>\n#include <asm/pgtable_areas.h>\n\n#include <xen/xen.h>\n\n \n#define LDT_SLOT_STRIDE (LDT_ENTRIES * LDT_ENTRY_SIZE)\n\nstatic inline void *ldt_slot_va(int slot)\n{\n\treturn (void *)(LDT_BASE_ADDR + LDT_SLOT_STRIDE * slot);\n}\n\nvoid load_mm_ldt(struct mm_struct *mm)\n{\n\tstruct ldt_struct *ldt;\n\n\t \n\tldt = READ_ONCE(mm->context.ldt);\n\n\t \n\n\tif (unlikely(ldt)) {\n\t\tif (static_cpu_has(X86_FEATURE_PTI)) {\n\t\t\tif (WARN_ON_ONCE((unsigned long)ldt->slot > 1)) {\n\t\t\t\t \n\t\t\t\tclear_LDT();\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t \n\t\t\tset_ldt(ldt_slot_va(ldt->slot), ldt->nr_entries);\n\t\t} else {\n\t\t\tset_ldt(ldt->entries, ldt->nr_entries);\n\t\t}\n\t} else {\n\t\tclear_LDT();\n\t}\n}\n\nvoid switch_ldt(struct mm_struct *prev, struct mm_struct *next)\n{\n\t \n\tif (unlikely((unsigned long)prev->context.ldt |\n\t\t     (unsigned long)next->context.ldt))\n\t\tload_mm_ldt(next);\n\n\tDEBUG_LOCKS_WARN_ON(preemptible());\n}\n\nstatic void refresh_ldt_segments(void)\n{\n#ifdef CONFIG_X86_64\n\tunsigned short sel;\n\n\t \n\tsavesegment(ds, sel);\n\tif ((sel & SEGMENT_TI_MASK) == SEGMENT_LDT)\n\t\tloadsegment(ds, sel);\n\n\tsavesegment(es, sel);\n\tif ((sel & SEGMENT_TI_MASK) == SEGMENT_LDT)\n\t\tloadsegment(es, sel);\n#endif\n}\n\n \nstatic void flush_ldt(void *__mm)\n{\n\tstruct mm_struct *mm = __mm;\n\n\tif (this_cpu_read(cpu_tlbstate.loaded_mm) != mm)\n\t\treturn;\n\n\tload_mm_ldt(mm);\n\n\trefresh_ldt_segments();\n}\n\n \nstatic struct ldt_struct *alloc_ldt_struct(unsigned int num_entries)\n{\n\tstruct ldt_struct *new_ldt;\n\tunsigned int alloc_size;\n\n\tif (num_entries > LDT_ENTRIES)\n\t\treturn NULL;\n\n\tnew_ldt = kmalloc(sizeof(struct ldt_struct), GFP_KERNEL_ACCOUNT);\n\tif (!new_ldt)\n\t\treturn NULL;\n\n\tBUILD_BUG_ON(LDT_ENTRY_SIZE != sizeof(struct desc_struct));\n\talloc_size = num_entries * LDT_ENTRY_SIZE;\n\n\t \n\tif (alloc_size > PAGE_SIZE)\n\t\tnew_ldt->entries = __vmalloc(alloc_size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);\n\telse\n\t\tnew_ldt->entries = (void *)get_zeroed_page(GFP_KERNEL_ACCOUNT);\n\n\tif (!new_ldt->entries) {\n\t\tkfree(new_ldt);\n\t\treturn NULL;\n\t}\n\n\t \n\tnew_ldt->slot = -1;\n\n\tnew_ldt->nr_entries = num_entries;\n\treturn new_ldt;\n}\n\n#ifdef CONFIG_PAGE_TABLE_ISOLATION\n\nstatic void do_sanity_check(struct mm_struct *mm,\n\t\t\t    bool had_kernel_mapping,\n\t\t\t    bool had_user_mapping)\n{\n\tif (mm->context.ldt) {\n\t\t \n\t\tWARN_ON(!had_kernel_mapping);\n\t\tif (boot_cpu_has(X86_FEATURE_PTI))\n\t\t\tWARN_ON(!had_user_mapping);\n\t} else {\n\t\t \n\t\tWARN_ON(had_kernel_mapping);\n\t\tif (boot_cpu_has(X86_FEATURE_PTI))\n\t\t\tWARN_ON(had_user_mapping);\n\t}\n}\n\n#ifdef CONFIG_X86_PAE\n\nstatic pmd_t *pgd_to_pmd_walk(pgd_t *pgd, unsigned long va)\n{\n\tp4d_t *p4d;\n\tpud_t *pud;\n\n\tif (pgd->pgd == 0)\n\t\treturn NULL;\n\n\tp4d = p4d_offset(pgd, va);\n\tif (p4d_none(*p4d))\n\t\treturn NULL;\n\n\tpud = pud_offset(p4d, va);\n\tif (pud_none(*pud))\n\t\treturn NULL;\n\n\treturn pmd_offset(pud, va);\n}\n\nstatic void map_ldt_struct_to_user(struct mm_struct *mm)\n{\n\tpgd_t *k_pgd = pgd_offset(mm, LDT_BASE_ADDR);\n\tpgd_t *u_pgd = kernel_to_user_pgdp(k_pgd);\n\tpmd_t *k_pmd, *u_pmd;\n\n\tk_pmd = pgd_to_pmd_walk(k_pgd, LDT_BASE_ADDR);\n\tu_pmd = pgd_to_pmd_walk(u_pgd, LDT_BASE_ADDR);\n\n\tif (boot_cpu_has(X86_FEATURE_PTI) && !mm->context.ldt)\n\t\tset_pmd(u_pmd, *k_pmd);\n}\n\nstatic void sanity_check_ldt_mapping(struct mm_struct *mm)\n{\n\tpgd_t *k_pgd = pgd_offset(mm, LDT_BASE_ADDR);\n\tpgd_t *u_pgd = kernel_to_user_pgdp(k_pgd);\n\tbool had_kernel, had_user;\n\tpmd_t *k_pmd, *u_pmd;\n\n\tk_pmd      = pgd_to_pmd_walk(k_pgd, LDT_BASE_ADDR);\n\tu_pmd      = pgd_to_pmd_walk(u_pgd, LDT_BASE_ADDR);\n\thad_kernel = (k_pmd->pmd != 0);\n\thad_user   = (u_pmd->pmd != 0);\n\n\tdo_sanity_check(mm, had_kernel, had_user);\n}\n\n#else  \n\nstatic void map_ldt_struct_to_user(struct mm_struct *mm)\n{\n\tpgd_t *pgd = pgd_offset(mm, LDT_BASE_ADDR);\n\n\tif (boot_cpu_has(X86_FEATURE_PTI) && !mm->context.ldt)\n\t\tset_pgd(kernel_to_user_pgdp(pgd), *pgd);\n}\n\nstatic void sanity_check_ldt_mapping(struct mm_struct *mm)\n{\n\tpgd_t *pgd = pgd_offset(mm, LDT_BASE_ADDR);\n\tbool had_kernel = (pgd->pgd != 0);\n\tbool had_user   = (kernel_to_user_pgdp(pgd)->pgd != 0);\n\n\tdo_sanity_check(mm, had_kernel, had_user);\n}\n\n#endif  \n\n \nstatic int\nmap_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt, int slot)\n{\n\tunsigned long va;\n\tbool is_vmalloc;\n\tspinlock_t *ptl;\n\tint i, nr_pages;\n\n\tif (!boot_cpu_has(X86_FEATURE_PTI))\n\t\treturn 0;\n\n\t \n\tWARN_ON(ldt->slot != -1);\n\n\t \n\tsanity_check_ldt_mapping(mm);\n\n\tis_vmalloc = is_vmalloc_addr(ldt->entries);\n\n\tnr_pages = DIV_ROUND_UP(ldt->nr_entries * LDT_ENTRY_SIZE, PAGE_SIZE);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tunsigned long offset = i << PAGE_SHIFT;\n\t\tconst void *src = (char *)ldt->entries + offset;\n\t\tunsigned long pfn;\n\t\tpgprot_t pte_prot;\n\t\tpte_t pte, *ptep;\n\n\t\tva = (unsigned long)ldt_slot_va(slot) + offset;\n\t\tpfn = is_vmalloc ? vmalloc_to_pfn(src) :\n\t\t\tpage_to_pfn(virt_to_page(src));\n\t\t \n\t\tptep = get_locked_pte(mm, va, &ptl);\n\t\tif (!ptep)\n\t\t\treturn -ENOMEM;\n\t\t \n\t\tpte_prot = __pgprot(__PAGE_KERNEL_RO & ~_PAGE_GLOBAL);\n\t\t \n\t\tpgprot_val(pte_prot) &= __supported_pte_mask;\n\t\tpte = pfn_pte(pfn, pte_prot);\n\t\tset_pte_at(mm, va, ptep, pte);\n\t\tpte_unmap_unlock(ptep, ptl);\n\t}\n\n\t \n\tmap_ldt_struct_to_user(mm);\n\n\tldt->slot = slot;\n\treturn 0;\n}\n\nstatic void unmap_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt)\n{\n\tunsigned long va;\n\tint i, nr_pages;\n\n\tif (!ldt)\n\t\treturn;\n\n\t \n\tif (!boot_cpu_has(X86_FEATURE_PTI))\n\t\treturn;\n\n\tnr_pages = DIV_ROUND_UP(ldt->nr_entries * LDT_ENTRY_SIZE, PAGE_SIZE);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tunsigned long offset = i << PAGE_SHIFT;\n\t\tspinlock_t *ptl;\n\t\tpte_t *ptep;\n\n\t\tva = (unsigned long)ldt_slot_va(ldt->slot) + offset;\n\t\tptep = get_locked_pte(mm, va, &ptl);\n\t\tif (!WARN_ON_ONCE(!ptep)) {\n\t\t\tpte_clear(mm, va, ptep);\n\t\t\tpte_unmap_unlock(ptep, ptl);\n\t\t}\n\t}\n\n\tva = (unsigned long)ldt_slot_va(ldt->slot);\n\tflush_tlb_mm_range(mm, va, va + nr_pages * PAGE_SIZE, PAGE_SHIFT, false);\n}\n\n#else  \n\nstatic int\nmap_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt, int slot)\n{\n\treturn 0;\n}\n\nstatic void unmap_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt)\n{\n}\n#endif  \n\nstatic void free_ldt_pgtables(struct mm_struct *mm)\n{\n#ifdef CONFIG_PAGE_TABLE_ISOLATION\n\tstruct mmu_gather tlb;\n\tunsigned long start = LDT_BASE_ADDR;\n\tunsigned long end = LDT_END_ADDR;\n\n\tif (!boot_cpu_has(X86_FEATURE_PTI))\n\t\treturn;\n\n\t \n\ttlb_gather_mmu_fullmm(&tlb, mm);\n\tfree_pgd_range(&tlb, start, end, start, end);\n\ttlb_finish_mmu(&tlb);\n#endif\n}\n\n \nstatic void finalize_ldt_struct(struct ldt_struct *ldt)\n{\n\tparavirt_alloc_ldt(ldt->entries, ldt->nr_entries);\n}\n\nstatic void install_ldt(struct mm_struct *mm, struct ldt_struct *ldt)\n{\n\tmutex_lock(&mm->context.lock);\n\n\t \n\tsmp_store_release(&mm->context.ldt, ldt);\n\n\t \n\ton_each_cpu_mask(mm_cpumask(mm), flush_ldt, mm, true);\n\n\tmutex_unlock(&mm->context.lock);\n}\n\nstatic void free_ldt_struct(struct ldt_struct *ldt)\n{\n\tif (likely(!ldt))\n\t\treturn;\n\n\tparavirt_free_ldt(ldt->entries, ldt->nr_entries);\n\tif (ldt->nr_entries * LDT_ENTRY_SIZE > PAGE_SIZE)\n\t\tvfree_atomic(ldt->entries);\n\telse\n\t\tfree_page((unsigned long)ldt->entries);\n\tkfree(ldt);\n}\n\n \nint ldt_dup_context(struct mm_struct *old_mm, struct mm_struct *mm)\n{\n\tstruct ldt_struct *new_ldt;\n\tint retval = 0;\n\n\tif (!old_mm)\n\t\treturn 0;\n\n\tmutex_lock(&old_mm->context.lock);\n\tif (!old_mm->context.ldt)\n\t\tgoto out_unlock;\n\n\tnew_ldt = alloc_ldt_struct(old_mm->context.ldt->nr_entries);\n\tif (!new_ldt) {\n\t\tretval = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\tmemcpy(new_ldt->entries, old_mm->context.ldt->entries,\n\t       new_ldt->nr_entries * LDT_ENTRY_SIZE);\n\tfinalize_ldt_struct(new_ldt);\n\n\tretval = map_ldt_struct(mm, new_ldt, 0);\n\tif (retval) {\n\t\tfree_ldt_pgtables(mm);\n\t\tfree_ldt_struct(new_ldt);\n\t\tgoto out_unlock;\n\t}\n\tmm->context.ldt = new_ldt;\n\nout_unlock:\n\tmutex_unlock(&old_mm->context.lock);\n\treturn retval;\n}\n\n \nvoid destroy_context_ldt(struct mm_struct *mm)\n{\n\tfree_ldt_struct(mm->context.ldt);\n\tmm->context.ldt = NULL;\n}\n\nvoid ldt_arch_exit_mmap(struct mm_struct *mm)\n{\n\tfree_ldt_pgtables(mm);\n}\n\nstatic int read_ldt(void __user *ptr, unsigned long bytecount)\n{\n\tstruct mm_struct *mm = current->mm;\n\tunsigned long entries_size;\n\tint retval;\n\n\tdown_read(&mm->context.ldt_usr_sem);\n\n\tif (!mm->context.ldt) {\n\t\tretval = 0;\n\t\tgoto out_unlock;\n\t}\n\n\tif (bytecount > LDT_ENTRY_SIZE * LDT_ENTRIES)\n\t\tbytecount = LDT_ENTRY_SIZE * LDT_ENTRIES;\n\n\tentries_size = mm->context.ldt->nr_entries * LDT_ENTRY_SIZE;\n\tif (entries_size > bytecount)\n\t\tentries_size = bytecount;\n\n\tif (copy_to_user(ptr, mm->context.ldt->entries, entries_size)) {\n\t\tretval = -EFAULT;\n\t\tgoto out_unlock;\n\t}\n\n\tif (entries_size != bytecount) {\n\t\t \n\t\tif (clear_user(ptr + entries_size, bytecount - entries_size)) {\n\t\t\tretval = -EFAULT;\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\tretval = bytecount;\n\nout_unlock:\n\tup_read(&mm->context.ldt_usr_sem);\n\treturn retval;\n}\n\nstatic int read_default_ldt(void __user *ptr, unsigned long bytecount)\n{\n\t \n#ifdef CONFIG_X86_32\n\tunsigned long size = 5 * sizeof(struct desc_struct);\n#else\n\tunsigned long size = 128;\n#endif\n\tif (bytecount > size)\n\t\tbytecount = size;\n\tif (clear_user(ptr, bytecount))\n\t\treturn -EFAULT;\n\treturn bytecount;\n}\n\nstatic bool allow_16bit_segments(void)\n{\n\tif (!IS_ENABLED(CONFIG_X86_16BIT))\n\t\treturn false;\n\n#ifdef CONFIG_XEN_PV\n\t \n\tif (xen_pv_domain()) {\n\t\tpr_info_once(\"Warning: 16-bit segments do not work correctly in a Xen PV guest\\n\");\n\t\treturn false;\n\t}\n#endif\n\n\treturn true;\n}\n\nstatic int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct ldt_struct *new_ldt, *old_ldt;\n\tunsigned int old_nr_entries, new_nr_entries;\n\tstruct user_desc ldt_info;\n\tstruct desc_struct ldt;\n\tint error;\n\n\terror = -EINVAL;\n\tif (bytecount != sizeof(ldt_info))\n\t\tgoto out;\n\terror = -EFAULT;\n\tif (copy_from_user(&ldt_info, ptr, sizeof(ldt_info)))\n\t\tgoto out;\n\n\terror = -EINVAL;\n\tif (ldt_info.entry_number >= LDT_ENTRIES)\n\t\tgoto out;\n\tif (ldt_info.contents == 3) {\n\t\tif (oldmode)\n\t\t\tgoto out;\n\t\tif (ldt_info.seg_not_present == 0)\n\t\t\tgoto out;\n\t}\n\n\tif ((oldmode && !ldt_info.base_addr && !ldt_info.limit) ||\n\t    LDT_empty(&ldt_info)) {\n\t\t \n\t\tmemset(&ldt, 0, sizeof(ldt));\n\t} else {\n\t\tif (!ldt_info.seg_32bit && !allow_16bit_segments()) {\n\t\t\terror = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tfill_ldt(&ldt, &ldt_info);\n\t\tif (oldmode)\n\t\t\tldt.avl = 0;\n\t}\n\n\tif (down_write_killable(&mm->context.ldt_usr_sem))\n\t\treturn -EINTR;\n\n\told_ldt       = mm->context.ldt;\n\told_nr_entries = old_ldt ? old_ldt->nr_entries : 0;\n\tnew_nr_entries = max(ldt_info.entry_number + 1, old_nr_entries);\n\n\terror = -ENOMEM;\n\tnew_ldt = alloc_ldt_struct(new_nr_entries);\n\tif (!new_ldt)\n\t\tgoto out_unlock;\n\n\tif (old_ldt)\n\t\tmemcpy(new_ldt->entries, old_ldt->entries, old_nr_entries * LDT_ENTRY_SIZE);\n\n\tnew_ldt->entries[ldt_info.entry_number] = ldt;\n\tfinalize_ldt_struct(new_ldt);\n\n\t \n\terror = map_ldt_struct(mm, new_ldt, old_ldt ? !old_ldt->slot : 0);\n\tif (error) {\n\t\t \n\t\tif (!WARN_ON_ONCE(old_ldt))\n\t\t\tfree_ldt_pgtables(mm);\n\t\tfree_ldt_struct(new_ldt);\n\t\tgoto out_unlock;\n\t}\n\n\tinstall_ldt(mm, new_ldt);\n\tunmap_ldt_struct(mm, old_ldt);\n\tfree_ldt_struct(old_ldt);\n\terror = 0;\n\nout_unlock:\n\tup_write(&mm->context.ldt_usr_sem);\nout:\n\treturn error;\n}\n\nSYSCALL_DEFINE3(modify_ldt, int , func , void __user * , ptr ,\n\t\tunsigned long , bytecount)\n{\n\tint ret = -ENOSYS;\n\n\tswitch (func) {\n\tcase 0:\n\t\tret = read_ldt(ptr, bytecount);\n\t\tbreak;\n\tcase 1:\n\t\tret = write_ldt(ptr, bytecount, 1);\n\t\tbreak;\n\tcase 2:\n\t\tret = read_default_ldt(ptr, bytecount);\n\t\tbreak;\n\tcase 0x11:\n\t\tret = write_ldt(ptr, bytecount, 0);\n\t\tbreak;\n\t}\n\t \n\treturn (unsigned int)ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}