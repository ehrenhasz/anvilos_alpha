{
  "module_name": "nmi.c",
  "hash_id": "b54437f6bf490b89205c339be753b2482d776ccbdb2ecce6398138709bb5057a",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/nmi.c",
  "human_readable_source": "\n \n\n \n#include <linux/spinlock.h>\n#include <linux/kprobes.h>\n#include <linux/kdebug.h>\n#include <linux/sched/debug.h>\n#include <linux/nmi.h>\n#include <linux/debugfs.h>\n#include <linux/delay.h>\n#include <linux/hardirq.h>\n#include <linux/ratelimit.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/atomic.h>\n#include <linux/sched/clock.h>\n\n#include <asm/cpu_entry_area.h>\n#include <asm/traps.h>\n#include <asm/mach_traps.h>\n#include <asm/nmi.h>\n#include <asm/x86_init.h>\n#include <asm/reboot.h>\n#include <asm/cache.h>\n#include <asm/nospec-branch.h>\n#include <asm/sev.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/nmi.h>\n\nstruct nmi_desc {\n\traw_spinlock_t lock;\n\tstruct list_head head;\n};\n\nstatic struct nmi_desc nmi_desc[NMI_MAX] = \n{\n\t{\n\t\t.lock = __RAW_SPIN_LOCK_UNLOCKED(&nmi_desc[0].lock),\n\t\t.head = LIST_HEAD_INIT(nmi_desc[0].head),\n\t},\n\t{\n\t\t.lock = __RAW_SPIN_LOCK_UNLOCKED(&nmi_desc[1].lock),\n\t\t.head = LIST_HEAD_INIT(nmi_desc[1].head),\n\t},\n\t{\n\t\t.lock = __RAW_SPIN_LOCK_UNLOCKED(&nmi_desc[2].lock),\n\t\t.head = LIST_HEAD_INIT(nmi_desc[2].head),\n\t},\n\t{\n\t\t.lock = __RAW_SPIN_LOCK_UNLOCKED(&nmi_desc[3].lock),\n\t\t.head = LIST_HEAD_INIT(nmi_desc[3].head),\n\t},\n\n};\n\nstruct nmi_stats {\n\tunsigned int normal;\n\tunsigned int unknown;\n\tunsigned int external;\n\tunsigned int swallow;\n\tunsigned long recv_jiffies;\n\tunsigned long idt_seq;\n\tunsigned long idt_nmi_seq;\n\tunsigned long idt_ignored;\n\tatomic_long_t idt_calls;\n\tunsigned long idt_seq_snap;\n\tunsigned long idt_nmi_seq_snap;\n\tunsigned long idt_ignored_snap;\n\tlong idt_calls_snap;\n};\n\nstatic DEFINE_PER_CPU(struct nmi_stats, nmi_stats);\n\nstatic int ignore_nmis __read_mostly;\n\nint unknown_nmi_panic;\n \nstatic DEFINE_RAW_SPINLOCK(nmi_reason_lock);\n\nstatic int __init setup_unknown_nmi_panic(char *str)\n{\n\tunknown_nmi_panic = 1;\n\treturn 1;\n}\n__setup(\"unknown_nmi_panic\", setup_unknown_nmi_panic);\n\n#define nmi_to_desc(type) (&nmi_desc[type])\n\nstatic u64 nmi_longest_ns = 1 * NSEC_PER_MSEC;\n\nstatic int __init nmi_warning_debugfs(void)\n{\n\tdebugfs_create_u64(\"nmi_longest_ns\", 0644,\n\t\t\tarch_debugfs_dir, &nmi_longest_ns);\n\treturn 0;\n}\nfs_initcall(nmi_warning_debugfs);\n\nstatic void nmi_check_duration(struct nmiaction *action, u64 duration)\n{\n\tint remainder_ns, decimal_msecs;\n\n\tif (duration < nmi_longest_ns || duration < action->max_duration)\n\t\treturn;\n\n\taction->max_duration = duration;\n\n\tremainder_ns = do_div(duration, (1000 * 1000));\n\tdecimal_msecs = remainder_ns / 1000;\n\n\tprintk_ratelimited(KERN_INFO\n\t\t\"INFO: NMI handler (%ps) took too long to run: %lld.%03d msecs\\n\",\n\t\taction->handler, duration, decimal_msecs);\n}\n\nstatic int nmi_handle(unsigned int type, struct pt_regs *regs)\n{\n\tstruct nmi_desc *desc = nmi_to_desc(type);\n\tstruct nmiaction *a;\n\tint handled=0;\n\n\trcu_read_lock();\n\n\t \n\tlist_for_each_entry_rcu(a, &desc->head, list) {\n\t\tint thishandled;\n\t\tu64 delta;\n\n\t\tdelta = sched_clock();\n\t\tthishandled = a->handler(type, regs);\n\t\thandled += thishandled;\n\t\tdelta = sched_clock() - delta;\n\t\ttrace_nmi_handler(a->handler, (int)delta, thishandled);\n\n\t\tnmi_check_duration(a, delta);\n\t}\n\n\trcu_read_unlock();\n\n\t \n\treturn handled;\n}\nNOKPROBE_SYMBOL(nmi_handle);\n\nint __register_nmi_handler(unsigned int type, struct nmiaction *action)\n{\n\tstruct nmi_desc *desc = nmi_to_desc(type);\n\tunsigned long flags;\n\n\tif (WARN_ON_ONCE(!action->handler || !list_empty(&action->list)))\n\t\treturn -EINVAL;\n\n\traw_spin_lock_irqsave(&desc->lock, flags);\n\n\t \n\tWARN_ON_ONCE(type == NMI_SERR && !list_empty(&desc->head));\n\tWARN_ON_ONCE(type == NMI_IO_CHECK && !list_empty(&desc->head));\n\n\t \n\tif (action->flags & NMI_FLAG_FIRST)\n\t\tlist_add_rcu(&action->list, &desc->head);\n\telse\n\t\tlist_add_tail_rcu(&action->list, &desc->head);\n\n\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\treturn 0;\n}\nEXPORT_SYMBOL(__register_nmi_handler);\n\nvoid unregister_nmi_handler(unsigned int type, const char *name)\n{\n\tstruct nmi_desc *desc = nmi_to_desc(type);\n\tstruct nmiaction *n, *found = NULL;\n\tunsigned long flags;\n\n\traw_spin_lock_irqsave(&desc->lock, flags);\n\n\tlist_for_each_entry_rcu(n, &desc->head, list) {\n\t\t \n\t\tif (!strcmp(n->name, name)) {\n\t\t\tWARN(in_nmi(),\n\t\t\t\t\"Trying to free NMI (%s) from NMI context!\\n\", n->name);\n\t\t\tlist_del_rcu(&n->list);\n\t\t\tfound = n;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\traw_spin_unlock_irqrestore(&desc->lock, flags);\n\tif (found) {\n\t\tsynchronize_rcu();\n\t\tINIT_LIST_HEAD(&found->list);\n\t}\n}\nEXPORT_SYMBOL_GPL(unregister_nmi_handler);\n\nstatic void\npci_serr_error(unsigned char reason, struct pt_regs *regs)\n{\n\t \n\tif (nmi_handle(NMI_SERR, regs))\n\t\treturn;\n\n\tpr_emerg(\"NMI: PCI system error (SERR) for reason %02x on CPU %d.\\n\",\n\t\t reason, smp_processor_id());\n\n\tif (panic_on_unrecovered_nmi)\n\t\tnmi_panic(regs, \"NMI: Not continuing\");\n\n\tpr_emerg(\"Dazed and confused, but trying to continue\\n\");\n\n\t \n\treason = (reason & NMI_REASON_CLEAR_MASK) | NMI_REASON_CLEAR_SERR;\n\toutb(reason, NMI_REASON_PORT);\n}\nNOKPROBE_SYMBOL(pci_serr_error);\n\nstatic void\nio_check_error(unsigned char reason, struct pt_regs *regs)\n{\n\tunsigned long i;\n\n\t \n\tif (nmi_handle(NMI_IO_CHECK, regs))\n\t\treturn;\n\n\tpr_emerg(\n\t\"NMI: IOCK error (debug interrupt?) for reason %02x on CPU %d.\\n\",\n\t\t reason, smp_processor_id());\n\tshow_regs(regs);\n\n\tif (panic_on_io_nmi) {\n\t\tnmi_panic(regs, \"NMI IOCK error: Not continuing\");\n\n\t\t \n\t\treturn;\n\t}\n\n\t \n\treason = (reason & NMI_REASON_CLEAR_MASK) | NMI_REASON_CLEAR_IOCHK;\n\toutb(reason, NMI_REASON_PORT);\n\n\ti = 20000;\n\twhile (--i) {\n\t\ttouch_nmi_watchdog();\n\t\tudelay(100);\n\t}\n\n\treason &= ~NMI_REASON_CLEAR_IOCHK;\n\toutb(reason, NMI_REASON_PORT);\n}\nNOKPROBE_SYMBOL(io_check_error);\n\nstatic void\nunknown_nmi_error(unsigned char reason, struct pt_regs *regs)\n{\n\tint handled;\n\n\t \n\thandled = nmi_handle(NMI_UNKNOWN, regs);\n\tif (handled) {\n\t\t__this_cpu_add(nmi_stats.unknown, handled);\n\t\treturn;\n\t}\n\n\t__this_cpu_add(nmi_stats.unknown, 1);\n\n\tpr_emerg(\"Uhhuh. NMI received for unknown reason %02x on CPU %d.\\n\",\n\t\t reason, smp_processor_id());\n\n\tif (unknown_nmi_panic || panic_on_unrecovered_nmi)\n\t\tnmi_panic(regs, \"NMI: Not continuing\");\n\n\tpr_emerg(\"Dazed and confused, but trying to continue\\n\");\n}\nNOKPROBE_SYMBOL(unknown_nmi_error);\n\nstatic DEFINE_PER_CPU(bool, swallow_nmi);\nstatic DEFINE_PER_CPU(unsigned long, last_nmi_rip);\n\nstatic noinstr void default_do_nmi(struct pt_regs *regs)\n{\n\tunsigned char reason = 0;\n\tint handled;\n\tbool b2b = false;\n\n\t \n\n\t \n\tif (regs->ip == __this_cpu_read(last_nmi_rip))\n\t\tb2b = true;\n\telse\n\t\t__this_cpu_write(swallow_nmi, false);\n\n\t__this_cpu_write(last_nmi_rip, regs->ip);\n\n\tinstrumentation_begin();\n\n\thandled = nmi_handle(NMI_LOCAL, regs);\n\t__this_cpu_add(nmi_stats.normal, handled);\n\tif (handled) {\n\t\t \n\t\tif (handled > 1)\n\t\t\t__this_cpu_write(swallow_nmi, true);\n\t\tgoto out;\n\t}\n\n\t \n\twhile (!raw_spin_trylock(&nmi_reason_lock)) {\n\t\trun_crash_ipi_callback(regs);\n\t\tcpu_relax();\n\t}\n\n\treason = x86_platform.get_nmi_reason();\n\n\tif (reason & NMI_REASON_MASK) {\n\t\tif (reason & NMI_REASON_SERR)\n\t\t\tpci_serr_error(reason, regs);\n\t\telse if (reason & NMI_REASON_IOCHK)\n\t\t\tio_check_error(reason, regs);\n#ifdef CONFIG_X86_32\n\t\t \n\t\treassert_nmi();\n#endif\n\t\t__this_cpu_add(nmi_stats.external, 1);\n\t\traw_spin_unlock(&nmi_reason_lock);\n\t\tgoto out;\n\t}\n\traw_spin_unlock(&nmi_reason_lock);\n\n\t \n\tif (b2b && __this_cpu_read(swallow_nmi))\n\t\t__this_cpu_add(nmi_stats.swallow, 1);\n\telse\n\t\tunknown_nmi_error(reason, regs);\n\nout:\n\tinstrumentation_end();\n}\n\n \nenum nmi_states {\n\tNMI_NOT_RUNNING = 0,\n\tNMI_EXECUTING,\n\tNMI_LATCHED,\n};\nstatic DEFINE_PER_CPU(enum nmi_states, nmi_state);\nstatic DEFINE_PER_CPU(unsigned long, nmi_cr2);\nstatic DEFINE_PER_CPU(unsigned long, nmi_dr7);\n\nDEFINE_IDTENTRY_RAW(exc_nmi)\n{\n\tirqentry_state_t irq_state;\n\tstruct nmi_stats *nsp = this_cpu_ptr(&nmi_stats);\n\n\t \n\tsev_es_nmi_complete();\n\tif (IS_ENABLED(CONFIG_NMI_CHECK_CPU))\n\t\traw_atomic_long_inc(&nsp->idt_calls);\n\n\tif (IS_ENABLED(CONFIG_SMP) && arch_cpu_is_offline(smp_processor_id()))\n\t\treturn;\n\n\tif (this_cpu_read(nmi_state) != NMI_NOT_RUNNING) {\n\t\tthis_cpu_write(nmi_state, NMI_LATCHED);\n\t\treturn;\n\t}\n\tthis_cpu_write(nmi_state, NMI_EXECUTING);\n\tthis_cpu_write(nmi_cr2, read_cr2());\n\nnmi_restart:\n\tif (IS_ENABLED(CONFIG_NMI_CHECK_CPU)) {\n\t\tWRITE_ONCE(nsp->idt_seq, nsp->idt_seq + 1);\n\t\tWARN_ON_ONCE(!(nsp->idt_seq & 0x1));\n\t\tWRITE_ONCE(nsp->recv_jiffies, jiffies);\n\t}\n\n\t \n\tsev_es_ist_enter(regs);\n\n\tthis_cpu_write(nmi_dr7, local_db_save());\n\n\tirq_state = irqentry_nmi_enter(regs);\n\n\tinc_irq_stat(__nmi_count);\n\n\tif (IS_ENABLED(CONFIG_NMI_CHECK_CPU) && ignore_nmis) {\n\t\tWRITE_ONCE(nsp->idt_ignored, nsp->idt_ignored + 1);\n\t} else if (!ignore_nmis) {\n\t\tif (IS_ENABLED(CONFIG_NMI_CHECK_CPU)) {\n\t\t\tWRITE_ONCE(nsp->idt_nmi_seq, nsp->idt_nmi_seq + 1);\n\t\t\tWARN_ON_ONCE(!(nsp->idt_nmi_seq & 0x1));\n\t\t}\n\t\tdefault_do_nmi(regs);\n\t\tif (IS_ENABLED(CONFIG_NMI_CHECK_CPU)) {\n\t\t\tWRITE_ONCE(nsp->idt_nmi_seq, nsp->idt_nmi_seq + 1);\n\t\t\tWARN_ON_ONCE(nsp->idt_nmi_seq & 0x1);\n\t\t}\n\t}\n\n\tirqentry_nmi_exit(regs, irq_state);\n\n\tlocal_db_restore(this_cpu_read(nmi_dr7));\n\n\tsev_es_ist_exit();\n\n\tif (unlikely(this_cpu_read(nmi_cr2) != read_cr2()))\n\t\twrite_cr2(this_cpu_read(nmi_cr2));\n\tif (IS_ENABLED(CONFIG_NMI_CHECK_CPU)) {\n\t\tWRITE_ONCE(nsp->idt_seq, nsp->idt_seq + 1);\n\t\tWARN_ON_ONCE(nsp->idt_seq & 0x1);\n\t\tWRITE_ONCE(nsp->recv_jiffies, jiffies);\n\t}\n\tif (this_cpu_dec_return(nmi_state))\n\t\tgoto nmi_restart;\n\n\tif (user_mode(regs))\n\t\tmds_user_clear_cpu_buffers();\n}\n\n#if IS_ENABLED(CONFIG_KVM_INTEL)\nDEFINE_IDTENTRY_RAW(exc_nmi_kvm_vmx)\n{\n\texc_nmi(regs);\n}\n#if IS_MODULE(CONFIG_KVM_INTEL)\nEXPORT_SYMBOL_GPL(asm_exc_nmi_kvm_vmx);\n#endif\n#endif\n\n#ifdef CONFIG_NMI_CHECK_CPU\n\nstatic char *nmi_check_stall_msg[] = {\n \n \n \n \n \n \n \n  \"NMIs are not reaching exc_nmi() handler\",\n  \"exc_nmi() handler is ignoring NMIs\",\n  \"CPU is offline and NMIs are not reaching exc_nmi() handler\",\n  \"CPU is offline and exc_nmi() handler is legitimately ignoring NMIs\",\n  \"CPU is in exc_nmi() handler and no further NMIs are reaching handler\",\n  \"CPU is in exc_nmi() handler which is legitimately ignoring NMIs\",\n  \"CPU is offline in exc_nmi() handler and no more NMIs are reaching exc_nmi() handler\",\n  \"CPU is offline in exc_nmi() handler which is legitimately ignoring NMIs\",\n};\n\nvoid nmi_backtrace_stall_snap(const struct cpumask *btp)\n{\n\tint cpu;\n\tstruct nmi_stats *nsp;\n\n\tfor_each_cpu(cpu, btp) {\n\t\tnsp = per_cpu_ptr(&nmi_stats, cpu);\n\t\tnsp->idt_seq_snap = READ_ONCE(nsp->idt_seq);\n\t\tnsp->idt_nmi_seq_snap = READ_ONCE(nsp->idt_nmi_seq);\n\t\tnsp->idt_ignored_snap = READ_ONCE(nsp->idt_ignored);\n\t\tnsp->idt_calls_snap = atomic_long_read(&nsp->idt_calls);\n\t}\n}\n\nvoid nmi_backtrace_stall_check(const struct cpumask *btp)\n{\n\tint cpu;\n\tint idx;\n\tunsigned long nmi_seq;\n\tunsigned long j = jiffies;\n\tchar *modp;\n\tchar *msgp;\n\tchar *msghp;\n\tstruct nmi_stats *nsp;\n\n\tfor_each_cpu(cpu, btp) {\n\t\tnsp = per_cpu_ptr(&nmi_stats, cpu);\n\t\tmodp = \"\";\n\t\tmsghp = \"\";\n\t\tnmi_seq = READ_ONCE(nsp->idt_nmi_seq);\n\t\tif (nsp->idt_nmi_seq_snap + 1 == nmi_seq && (nmi_seq & 0x1)) {\n\t\t\tmsgp = \"CPU entered NMI handler function, but has not exited\";\n\t\t} else if ((nsp->idt_nmi_seq_snap & 0x1) != (nmi_seq & 0x1)) {\n\t\t\tmsgp = \"CPU is handling NMIs\";\n\t\t} else {\n\t\t\tidx = ((nsp->idt_seq_snap & 0x1) << 2) |\n\t\t\t      (cpu_is_offline(cpu) << 1) |\n\t\t\t      (nsp->idt_calls_snap != atomic_long_read(&nsp->idt_calls));\n\t\t\tmsgp = nmi_check_stall_msg[idx];\n\t\t\tif (nsp->idt_ignored_snap != READ_ONCE(nsp->idt_ignored) && (idx & 0x1))\n\t\t\t\tmodp = \", but OK because ignore_nmis was set\";\n\t\t\tif (nmi_seq & ~0x1)\n\t\t\t\tmsghp = \" (CPU currently in NMI handler function)\";\n\t\t\telse if (nsp->idt_nmi_seq_snap + 1 == nmi_seq)\n\t\t\t\tmsghp = \" (CPU exited one NMI handler function)\";\n\t\t}\n\t\tpr_alert(\"%s: CPU %d: %s%s%s, last activity: %lu jiffies ago.\\n\",\n\t\t\t __func__, cpu, msgp, modp, msghp, j - READ_ONCE(nsp->recv_jiffies));\n\t}\n}\n\n#endif\n\nvoid stop_nmi(void)\n{\n\tignore_nmis++;\n}\n\nvoid restart_nmi(void)\n{\n\tignore_nmis--;\n}\n\n \nvoid local_touch_nmi(void)\n{\n\t__this_cpu_write(last_nmi_rip, 0);\n}\nEXPORT_SYMBOL_GPL(local_touch_nmi);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}