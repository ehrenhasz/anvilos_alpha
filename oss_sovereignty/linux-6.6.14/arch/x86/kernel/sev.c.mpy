{
  "module_name": "sev.c",
  "hash_id": "81e5a09661aeb5ad87de87b15030b2c5ce239e19c4117b1274b38e7fc8197502",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/sev.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)\t\"SEV: \" fmt\n\n#include <linux/sched/debug.h>\t \n#include <linux/percpu-defs.h>\n#include <linux/cc_platform.h>\n#include <linux/printk.h>\n#include <linux/mm_types.h>\n#include <linux/set_memory.h>\n#include <linux/memblock.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\n#include <linux/cpumask.h>\n#include <linux/efi.h>\n#include <linux/platform_device.h>\n#include <linux/io.h>\n#include <linux/psp-sev.h>\n#include <uapi/linux/sev-guest.h>\n\n#include <asm/cpu_entry_area.h>\n#include <asm/stacktrace.h>\n#include <asm/sev.h>\n#include <asm/insn-eval.h>\n#include <asm/fpu/xcr.h>\n#include <asm/processor.h>\n#include <asm/realmode.h>\n#include <asm/setup.h>\n#include <asm/traps.h>\n#include <asm/svm.h>\n#include <asm/smp.h>\n#include <asm/cpu.h>\n#include <asm/apic.h>\n#include <asm/cpuid.h>\n#include <asm/cmdline.h>\n\n#define DR7_RESET_VALUE        0x400\n\n \n#define AP_INIT_CS_LIMIT\t\t0xffff\n#define AP_INIT_DS_LIMIT\t\t0xffff\n#define AP_INIT_LDTR_LIMIT\t\t0xffff\n#define AP_INIT_GDTR_LIMIT\t\t0xffff\n#define AP_INIT_IDTR_LIMIT\t\t0xffff\n#define AP_INIT_TR_LIMIT\t\t0xffff\n#define AP_INIT_RFLAGS_DEFAULT\t\t0x2\n#define AP_INIT_DR6_DEFAULT\t\t0xffff0ff0\n#define AP_INIT_GPAT_DEFAULT\t\t0x0007040600070406ULL\n#define AP_INIT_XCR0_DEFAULT\t\t0x1\n#define AP_INIT_X87_FTW_DEFAULT\t\t0x5555\n#define AP_INIT_X87_FCW_DEFAULT\t\t0x0040\n#define AP_INIT_CR0_DEFAULT\t\t0x60000010\n#define AP_INIT_MXCSR_DEFAULT\t\t0x1f80\n\n \nstatic struct ghcb boot_ghcb_page __bss_decrypted __aligned(PAGE_SIZE);\n\n \nstatic struct ghcb *boot_ghcb __section(\".data\");\n\n \nstatic u64 sev_hv_features __ro_after_init;\n\n \nstruct sev_es_runtime_data {\n\tstruct ghcb ghcb_page;\n\n\t \n\tstruct ghcb backup_ghcb;\n\n\t \n\tbool ghcb_active;\n\tbool backup_ghcb_active;\n\n\t \n\tunsigned long dr7;\n};\n\nstruct ghcb_state {\n\tstruct ghcb *ghcb;\n};\n\nstatic DEFINE_PER_CPU(struct sev_es_runtime_data*, runtime_data);\nstatic DEFINE_PER_CPU(struct sev_es_save_area *, sev_vmsa);\n\nstruct sev_config {\n\t__u64 debug\t\t: 1,\n\n\t       \n\t      ghcbs_initialized\t: 1,\n\n\t      __reserved\t: 62;\n};\n\nstatic struct sev_config sev_cfg __read_mostly;\n\nstatic __always_inline bool on_vc_stack(struct pt_regs *regs)\n{\n\tunsigned long sp = regs->sp;\n\n\t \n\tif (user_mode(regs))\n\t\treturn false;\n\n\t \n\tif (ip_within_syscall_gap(regs))\n\t\treturn false;\n\n\treturn ((sp >= __this_cpu_ist_bottom_va(VC)) && (sp < __this_cpu_ist_top_va(VC)));\n}\n\n \nvoid noinstr __sev_es_ist_enter(struct pt_regs *regs)\n{\n\tunsigned long old_ist, new_ist;\n\n\t \n\tnew_ist = old_ist = __this_cpu_read(cpu_tss_rw.x86_tss.ist[IST_INDEX_VC]);\n\n\t \n\tif (on_vc_stack(regs))\n\t\tnew_ist = regs->sp;\n\n\t \n\tnew_ist -= sizeof(old_ist);\n\t*(unsigned long *)new_ist = old_ist;\n\n\t \n\tthis_cpu_write(cpu_tss_rw.x86_tss.ist[IST_INDEX_VC], new_ist);\n}\n\nvoid noinstr __sev_es_ist_exit(void)\n{\n\tunsigned long ist;\n\n\t \n\tist = __this_cpu_read(cpu_tss_rw.x86_tss.ist[IST_INDEX_VC]);\n\n\tif (WARN_ON(ist == __this_cpu_ist_top_va(VC)))\n\t\treturn;\n\n\t \n\tthis_cpu_write(cpu_tss_rw.x86_tss.ist[IST_INDEX_VC], *(unsigned long *)ist);\n}\n\n \nstatic noinstr struct ghcb *__sev_get_ghcb(struct ghcb_state *state)\n{\n\tstruct sev_es_runtime_data *data;\n\tstruct ghcb *ghcb;\n\n\tWARN_ON(!irqs_disabled());\n\n\tdata = this_cpu_read(runtime_data);\n\tghcb = &data->ghcb_page;\n\n\tif (unlikely(data->ghcb_active)) {\n\t\t \n\n\t\tif (unlikely(data->backup_ghcb_active)) {\n\t\t\t \n\t\t\tdata->ghcb_active        = false;\n\t\t\tdata->backup_ghcb_active = false;\n\n\t\t\tinstrumentation_begin();\n\t\t\tpanic(\"Unable to handle #VC exception! GHCB and Backup GHCB are already in use\");\n\t\t\tinstrumentation_end();\n\t\t}\n\n\t\t \n\t\tdata->backup_ghcb_active = true;\n\n\t\tstate->ghcb = &data->backup_ghcb;\n\n\t\t \n\t\t*state->ghcb = *ghcb;\n\t} else {\n\t\tstate->ghcb = NULL;\n\t\tdata->ghcb_active = true;\n\t}\n\n\treturn ghcb;\n}\n\nstatic inline u64 sev_es_rd_ghcb_msr(void)\n{\n\treturn __rdmsr(MSR_AMD64_SEV_ES_GHCB);\n}\n\nstatic __always_inline void sev_es_wr_ghcb_msr(u64 val)\n{\n\tu32 low, high;\n\n\tlow  = (u32)(val);\n\thigh = (u32)(val >> 32);\n\n\tnative_wrmsr(MSR_AMD64_SEV_ES_GHCB, low, high);\n}\n\nstatic int vc_fetch_insn_kernel(struct es_em_ctxt *ctxt,\n\t\t\t\tunsigned char *buffer)\n{\n\treturn copy_from_kernel_nofault(buffer, (unsigned char *)ctxt->regs->ip, MAX_INSN_SIZE);\n}\n\nstatic enum es_result __vc_decode_user_insn(struct es_em_ctxt *ctxt)\n{\n\tchar buffer[MAX_INSN_SIZE];\n\tint insn_bytes;\n\n\tinsn_bytes = insn_fetch_from_user_inatomic(ctxt->regs, buffer);\n\tif (insn_bytes == 0) {\n\t\t \n\t\tctxt->fi.vector     = X86_TRAP_PF;\n\t\tctxt->fi.error_code = X86_PF_INSTR | X86_PF_USER;\n\t\tctxt->fi.cr2        = ctxt->regs->ip;\n\t\treturn ES_EXCEPTION;\n\t} else if (insn_bytes == -EINVAL) {\n\t\t \n\t\tctxt->fi.vector     = X86_TRAP_GP;\n\t\tctxt->fi.error_code = 0;\n\t\tctxt->fi.cr2        = 0;\n\t\treturn ES_EXCEPTION;\n\t}\n\n\tif (!insn_decode_from_regs(&ctxt->insn, ctxt->regs, buffer, insn_bytes))\n\t\treturn ES_DECODE_FAILED;\n\n\tif (ctxt->insn.immediate.got)\n\t\treturn ES_OK;\n\telse\n\t\treturn ES_DECODE_FAILED;\n}\n\nstatic enum es_result __vc_decode_kern_insn(struct es_em_ctxt *ctxt)\n{\n\tchar buffer[MAX_INSN_SIZE];\n\tint res, ret;\n\n\tres = vc_fetch_insn_kernel(ctxt, buffer);\n\tif (res) {\n\t\tctxt->fi.vector     = X86_TRAP_PF;\n\t\tctxt->fi.error_code = X86_PF_INSTR;\n\t\tctxt->fi.cr2        = ctxt->regs->ip;\n\t\treturn ES_EXCEPTION;\n\t}\n\n\tret = insn_decode(&ctxt->insn, buffer, MAX_INSN_SIZE, INSN_MODE_64);\n\tif (ret < 0)\n\t\treturn ES_DECODE_FAILED;\n\telse\n\t\treturn ES_OK;\n}\n\nstatic enum es_result vc_decode_insn(struct es_em_ctxt *ctxt)\n{\n\tif (user_mode(ctxt->regs))\n\t\treturn __vc_decode_user_insn(ctxt);\n\telse\n\t\treturn __vc_decode_kern_insn(ctxt);\n}\n\nstatic enum es_result vc_write_mem(struct es_em_ctxt *ctxt,\n\t\t\t\t   char *dst, char *buf, size_t size)\n{\n\tunsigned long error_code = X86_PF_PROT | X86_PF_WRITE;\n\n\t \n\tswitch (size) {\n\tcase 1: {\n\t\tu8 d1;\n\t\tu8 __user *target = (u8 __user *)dst;\n\n\t\tmemcpy(&d1, buf, 1);\n\t\tif (__put_user(d1, target))\n\t\t\tgoto fault;\n\t\tbreak;\n\t}\n\tcase 2: {\n\t\tu16 d2;\n\t\tu16 __user *target = (u16 __user *)dst;\n\n\t\tmemcpy(&d2, buf, 2);\n\t\tif (__put_user(d2, target))\n\t\t\tgoto fault;\n\t\tbreak;\n\t}\n\tcase 4: {\n\t\tu32 d4;\n\t\tu32 __user *target = (u32 __user *)dst;\n\n\t\tmemcpy(&d4, buf, 4);\n\t\tif (__put_user(d4, target))\n\t\t\tgoto fault;\n\t\tbreak;\n\t}\n\tcase 8: {\n\t\tu64 d8;\n\t\tu64 __user *target = (u64 __user *)dst;\n\n\t\tmemcpy(&d8, buf, 8);\n\t\tif (__put_user(d8, target))\n\t\t\tgoto fault;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tWARN_ONCE(1, \"%s: Invalid size: %zu\\n\", __func__, size);\n\t\treturn ES_UNSUPPORTED;\n\t}\n\n\treturn ES_OK;\n\nfault:\n\tif (user_mode(ctxt->regs))\n\t\terror_code |= X86_PF_USER;\n\n\tctxt->fi.vector = X86_TRAP_PF;\n\tctxt->fi.error_code = error_code;\n\tctxt->fi.cr2 = (unsigned long)dst;\n\n\treturn ES_EXCEPTION;\n}\n\nstatic enum es_result vc_read_mem(struct es_em_ctxt *ctxt,\n\t\t\t\t  char *src, char *buf, size_t size)\n{\n\tunsigned long error_code = X86_PF_PROT;\n\n\t \n\tswitch (size) {\n\tcase 1: {\n\t\tu8 d1;\n\t\tu8 __user *s = (u8 __user *)src;\n\n\t\tif (__get_user(d1, s))\n\t\t\tgoto fault;\n\t\tmemcpy(buf, &d1, 1);\n\t\tbreak;\n\t}\n\tcase 2: {\n\t\tu16 d2;\n\t\tu16 __user *s = (u16 __user *)src;\n\n\t\tif (__get_user(d2, s))\n\t\t\tgoto fault;\n\t\tmemcpy(buf, &d2, 2);\n\t\tbreak;\n\t}\n\tcase 4: {\n\t\tu32 d4;\n\t\tu32 __user *s = (u32 __user *)src;\n\n\t\tif (__get_user(d4, s))\n\t\t\tgoto fault;\n\t\tmemcpy(buf, &d4, 4);\n\t\tbreak;\n\t}\n\tcase 8: {\n\t\tu64 d8;\n\t\tu64 __user *s = (u64 __user *)src;\n\t\tif (__get_user(d8, s))\n\t\t\tgoto fault;\n\t\tmemcpy(buf, &d8, 8);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tWARN_ONCE(1, \"%s: Invalid size: %zu\\n\", __func__, size);\n\t\treturn ES_UNSUPPORTED;\n\t}\n\n\treturn ES_OK;\n\nfault:\n\tif (user_mode(ctxt->regs))\n\t\terror_code |= X86_PF_USER;\n\n\tctxt->fi.vector = X86_TRAP_PF;\n\tctxt->fi.error_code = error_code;\n\tctxt->fi.cr2 = (unsigned long)src;\n\n\treturn ES_EXCEPTION;\n}\n\nstatic enum es_result vc_slow_virt_to_phys(struct ghcb *ghcb, struct es_em_ctxt *ctxt,\n\t\t\t\t\t   unsigned long vaddr, phys_addr_t *paddr)\n{\n\tunsigned long va = (unsigned long)vaddr;\n\tunsigned int level;\n\tphys_addr_t pa;\n\tpgd_t *pgd;\n\tpte_t *pte;\n\n\tpgd = __va(read_cr3_pa());\n\tpgd = &pgd[pgd_index(va)];\n\tpte = lookup_address_in_pgd(pgd, va, &level);\n\tif (!pte) {\n\t\tctxt->fi.vector     = X86_TRAP_PF;\n\t\tctxt->fi.cr2        = vaddr;\n\t\tctxt->fi.error_code = 0;\n\n\t\tif (user_mode(ctxt->regs))\n\t\t\tctxt->fi.error_code |= X86_PF_USER;\n\n\t\treturn ES_EXCEPTION;\n\t}\n\n\tif (WARN_ON_ONCE(pte_val(*pte) & _PAGE_ENC))\n\t\t \n\t\treturn ES_UNSUPPORTED;\n\n\tpa = (phys_addr_t)pte_pfn(*pte) << PAGE_SHIFT;\n\tpa |= va & ~page_level_mask(level);\n\n\t*paddr = pa;\n\n\treturn ES_OK;\n}\n\nstatic enum es_result vc_ioio_check(struct es_em_ctxt *ctxt, u16 port, size_t size)\n{\n\tBUG_ON(size > 4);\n\n\tif (user_mode(ctxt->regs)) {\n\t\tstruct thread_struct *t = &current->thread;\n\t\tstruct io_bitmap *iobm = t->io_bitmap;\n\t\tsize_t idx;\n\n\t\tif (!iobm)\n\t\t\tgoto fault;\n\n\t\tfor (idx = port; idx < port + size; ++idx) {\n\t\t\tif (test_bit(idx, iobm->bitmap))\n\t\t\t\tgoto fault;\n\t\t}\n\t}\n\n\treturn ES_OK;\n\nfault:\n\tctxt->fi.vector = X86_TRAP_GP;\n\tctxt->fi.error_code = 0;\n\n\treturn ES_EXCEPTION;\n}\n\n \n#include \"sev-shared.c\"\n\nstatic noinstr void __sev_put_ghcb(struct ghcb_state *state)\n{\n\tstruct sev_es_runtime_data *data;\n\tstruct ghcb *ghcb;\n\n\tWARN_ON(!irqs_disabled());\n\n\tdata = this_cpu_read(runtime_data);\n\tghcb = &data->ghcb_page;\n\n\tif (state->ghcb) {\n\t\t \n\t\t*ghcb = *state->ghcb;\n\t\tdata->backup_ghcb_active = false;\n\t\tstate->ghcb = NULL;\n\t} else {\n\t\t \n\t\tvc_ghcb_invalidate(ghcb);\n\t\tdata->ghcb_active = false;\n\t}\n}\n\nvoid noinstr __sev_es_nmi_complete(void)\n{\n\tstruct ghcb_state state;\n\tstruct ghcb *ghcb;\n\n\tghcb = __sev_get_ghcb(&state);\n\n\tvc_ghcb_invalidate(ghcb);\n\tghcb_set_sw_exit_code(ghcb, SVM_VMGEXIT_NMI_COMPLETE);\n\tghcb_set_sw_exit_info_1(ghcb, 0);\n\tghcb_set_sw_exit_info_2(ghcb, 0);\n\n\tsev_es_wr_ghcb_msr(__pa_nodebug(ghcb));\n\tVMGEXIT();\n\n\t__sev_put_ghcb(&state);\n}\n\nstatic u64 __init get_secrets_page(void)\n{\n\tu64 pa_data = boot_params.cc_blob_address;\n\tstruct cc_blob_sev_info info;\n\tvoid *map;\n\n\t \n\tif (!pa_data)\n\t\treturn 0;\n\n\tmap = early_memremap(pa_data, sizeof(info));\n\tif (!map) {\n\t\tpr_err(\"Unable to locate SNP secrets page: failed to map the Confidential Computing blob.\\n\");\n\t\treturn 0;\n\t}\n\tmemcpy(&info, map, sizeof(info));\n\tearly_memunmap(map, sizeof(info));\n\n\t \n\tif (!info.secrets_phys || info.secrets_len != PAGE_SIZE)\n\t\treturn 0;\n\n\treturn info.secrets_phys;\n}\n\nstatic u64 __init get_snp_jump_table_addr(void)\n{\n\tstruct snp_secrets_page_layout *layout;\n\tvoid __iomem *mem;\n\tu64 pa, addr;\n\n\tpa = get_secrets_page();\n\tif (!pa)\n\t\treturn 0;\n\n\tmem = ioremap_encrypted(pa, PAGE_SIZE);\n\tif (!mem) {\n\t\tpr_err(\"Unable to locate AP jump table address: failed to map the SNP secrets page.\\n\");\n\t\treturn 0;\n\t}\n\n\tlayout = (__force struct snp_secrets_page_layout *)mem;\n\n\taddr = layout->os_area.ap_jump_table_pa;\n\tiounmap(mem);\n\n\treturn addr;\n}\n\nstatic u64 __init get_jump_table_addr(void)\n{\n\tstruct ghcb_state state;\n\tunsigned long flags;\n\tstruct ghcb *ghcb;\n\tu64 ret = 0;\n\n\tif (cc_platform_has(CC_ATTR_GUEST_SEV_SNP))\n\t\treturn get_snp_jump_table_addr();\n\n\tlocal_irq_save(flags);\n\n\tghcb = __sev_get_ghcb(&state);\n\n\tvc_ghcb_invalidate(ghcb);\n\tghcb_set_sw_exit_code(ghcb, SVM_VMGEXIT_AP_JUMP_TABLE);\n\tghcb_set_sw_exit_info_1(ghcb, SVM_VMGEXIT_GET_AP_JUMP_TABLE);\n\tghcb_set_sw_exit_info_2(ghcb, 0);\n\n\tsev_es_wr_ghcb_msr(__pa(ghcb));\n\tVMGEXIT();\n\n\tif (ghcb_sw_exit_info_1_is_valid(ghcb) &&\n\t    ghcb_sw_exit_info_2_is_valid(ghcb))\n\t\tret = ghcb->save.sw_exit_info_2;\n\n\t__sev_put_ghcb(&state);\n\n\tlocal_irq_restore(flags);\n\n\treturn ret;\n}\n\nstatic void early_set_pages_state(unsigned long vaddr, unsigned long paddr,\n\t\t\t\t  unsigned long npages, enum psc_op op)\n{\n\tunsigned long paddr_end;\n\tu64 val;\n\tint ret;\n\n\tvaddr = vaddr & PAGE_MASK;\n\n\tpaddr = paddr & PAGE_MASK;\n\tpaddr_end = paddr + (npages << PAGE_SHIFT);\n\n\twhile (paddr < paddr_end) {\n\t\tif (op == SNP_PAGE_STATE_SHARED) {\n\t\t\t \n\t\t\tret = pvalidate(vaddr, RMP_PG_SIZE_4K, false);\n\t\t\tif (WARN(ret, \"Failed to validate address 0x%lx ret %d\", paddr, ret))\n\t\t\t\tgoto e_term;\n\t\t}\n\n\t\t \n\t\tsev_es_wr_ghcb_msr(GHCB_MSR_PSC_REQ_GFN(paddr >> PAGE_SHIFT, op));\n\t\tVMGEXIT();\n\n\t\tval = sev_es_rd_ghcb_msr();\n\n\t\tif (WARN(GHCB_RESP_CODE(val) != GHCB_MSR_PSC_RESP,\n\t\t\t \"Wrong PSC response code: 0x%x\\n\",\n\t\t\t (unsigned int)GHCB_RESP_CODE(val)))\n\t\t\tgoto e_term;\n\n\t\tif (WARN(GHCB_MSR_PSC_RESP_VAL(val),\n\t\t\t \"Failed to change page state to '%s' paddr 0x%lx error 0x%llx\\n\",\n\t\t\t op == SNP_PAGE_STATE_PRIVATE ? \"private\" : \"shared\",\n\t\t\t paddr, GHCB_MSR_PSC_RESP_VAL(val)))\n\t\t\tgoto e_term;\n\n\t\tif (op == SNP_PAGE_STATE_PRIVATE) {\n\t\t\t \n\t\t\tret = pvalidate(vaddr, RMP_PG_SIZE_4K, true);\n\t\t\tif (WARN(ret, \"Failed to validate address 0x%lx ret %d\", paddr, ret))\n\t\t\t\tgoto e_term;\n\t\t}\n\n\t\tvaddr += PAGE_SIZE;\n\t\tpaddr += PAGE_SIZE;\n\t}\n\n\treturn;\n\ne_term:\n\tsev_es_terminate(SEV_TERM_SET_LINUX, GHCB_TERM_PSC);\n}\n\nvoid __init early_snp_set_memory_private(unsigned long vaddr, unsigned long paddr,\n\t\t\t\t\t unsigned long npages)\n{\n\t \n\tif (!(sev_status & MSR_AMD64_SEV_SNP_ENABLED))\n\t\treturn;\n\n\t  \n\tearly_set_pages_state(vaddr, paddr, npages, SNP_PAGE_STATE_PRIVATE);\n}\n\nvoid __init early_snp_set_memory_shared(unsigned long vaddr, unsigned long paddr,\n\t\t\t\t\tunsigned long npages)\n{\n\t \n\tif (!(sev_status & MSR_AMD64_SEV_SNP_ENABLED))\n\t\treturn;\n\n\t  \n\tearly_set_pages_state(vaddr, paddr, npages, SNP_PAGE_STATE_SHARED);\n}\n\nvoid __init snp_prep_memory(unsigned long paddr, unsigned int sz, enum psc_op op)\n{\n\tunsigned long vaddr, npages;\n\n\tvaddr = (unsigned long)__va(paddr);\n\tnpages = PAGE_ALIGN(sz) >> PAGE_SHIFT;\n\n\tif (op == SNP_PAGE_STATE_PRIVATE)\n\t\tearly_snp_set_memory_private(vaddr, paddr, npages);\n\telse if (op == SNP_PAGE_STATE_SHARED)\n\t\tearly_snp_set_memory_shared(vaddr, paddr, npages);\n\telse\n\t\tWARN(1, \"invalid memory op %d\\n\", op);\n}\n\nstatic unsigned long __set_pages_state(struct snp_psc_desc *data, unsigned long vaddr,\n\t\t\t\t       unsigned long vaddr_end, int op)\n{\n\tstruct ghcb_state state;\n\tbool use_large_entry;\n\tstruct psc_hdr *hdr;\n\tstruct psc_entry *e;\n\tunsigned long flags;\n\tunsigned long pfn;\n\tstruct ghcb *ghcb;\n\tint i;\n\n\thdr = &data->hdr;\n\te = data->entries;\n\n\tmemset(data, 0, sizeof(*data));\n\ti = 0;\n\n\twhile (vaddr < vaddr_end && i < ARRAY_SIZE(data->entries)) {\n\t\thdr->end_entry = i;\n\n\t\tif (is_vmalloc_addr((void *)vaddr)) {\n\t\t\tpfn = vmalloc_to_pfn((void *)vaddr);\n\t\t\tuse_large_entry = false;\n\t\t} else {\n\t\t\tpfn = __pa(vaddr) >> PAGE_SHIFT;\n\t\t\tuse_large_entry = true;\n\t\t}\n\n\t\te->gfn = pfn;\n\t\te->operation = op;\n\n\t\tif (use_large_entry && IS_ALIGNED(vaddr, PMD_SIZE) &&\n\t\t    (vaddr_end - vaddr) >= PMD_SIZE) {\n\t\t\te->pagesize = RMP_PG_SIZE_2M;\n\t\t\tvaddr += PMD_SIZE;\n\t\t} else {\n\t\t\te->pagesize = RMP_PG_SIZE_4K;\n\t\t\tvaddr += PAGE_SIZE;\n\t\t}\n\n\t\te++;\n\t\ti++;\n\t}\n\n\t \n\tif (op == SNP_PAGE_STATE_SHARED)\n\t\tpvalidate_pages(data);\n\n\tlocal_irq_save(flags);\n\n\tif (sev_cfg.ghcbs_initialized)\n\t\tghcb = __sev_get_ghcb(&state);\n\telse\n\t\tghcb = boot_ghcb;\n\n\t \n\tif (!ghcb || vmgexit_psc(ghcb, data))\n\t\tsev_es_terminate(SEV_TERM_SET_LINUX, GHCB_TERM_PSC);\n\n\tif (sev_cfg.ghcbs_initialized)\n\t\t__sev_put_ghcb(&state);\n\n\tlocal_irq_restore(flags);\n\n\t \n\tif (op == SNP_PAGE_STATE_PRIVATE)\n\t\tpvalidate_pages(data);\n\n\treturn vaddr;\n}\n\nstatic void set_pages_state(unsigned long vaddr, unsigned long npages, int op)\n{\n\tstruct snp_psc_desc desc;\n\tunsigned long vaddr_end;\n\n\t \n\tif (!boot_ghcb)\n\t\treturn early_set_pages_state(vaddr, __pa(vaddr), npages, op);\n\n\tvaddr = vaddr & PAGE_MASK;\n\tvaddr_end = vaddr + (npages << PAGE_SHIFT);\n\n\twhile (vaddr < vaddr_end)\n\t\tvaddr = __set_pages_state(&desc, vaddr, vaddr_end, op);\n}\n\nvoid snp_set_memory_shared(unsigned long vaddr, unsigned long npages)\n{\n\tif (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))\n\t\treturn;\n\n\tset_pages_state(vaddr, npages, SNP_PAGE_STATE_SHARED);\n}\n\nvoid snp_set_memory_private(unsigned long vaddr, unsigned long npages)\n{\n\tif (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))\n\t\treturn;\n\n\tset_pages_state(vaddr, npages, SNP_PAGE_STATE_PRIVATE);\n}\n\nvoid snp_accept_memory(phys_addr_t start, phys_addr_t end)\n{\n\tunsigned long vaddr, npages;\n\n\tif (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))\n\t\treturn;\n\n\tvaddr = (unsigned long)__va(start);\n\tnpages = (end - start) >> PAGE_SHIFT;\n\n\tset_pages_state(vaddr, npages, SNP_PAGE_STATE_PRIVATE);\n}\n\nstatic int snp_set_vmsa(void *va, bool vmsa)\n{\n\tu64 attrs;\n\n\t \n\tattrs = 1;\n\tif (vmsa)\n\t\tattrs |= RMPADJUST_VMSA_PAGE_BIT;\n\n\treturn rmpadjust((unsigned long)va, RMP_PG_SIZE_4K, attrs);\n}\n\n#define __ATTR_BASE\t\t(SVM_SELECTOR_P_MASK | SVM_SELECTOR_S_MASK)\n#define INIT_CS_ATTRIBS\t\t(__ATTR_BASE | SVM_SELECTOR_READ_MASK | SVM_SELECTOR_CODE_MASK)\n#define INIT_DS_ATTRIBS\t\t(__ATTR_BASE | SVM_SELECTOR_WRITE_MASK)\n\n#define INIT_LDTR_ATTRIBS\t(SVM_SELECTOR_P_MASK | 2)\n#define INIT_TR_ATTRIBS\t\t(SVM_SELECTOR_P_MASK | 3)\n\nstatic void *snp_alloc_vmsa_page(void)\n{\n\tstruct page *p;\n\n\t \n\tp = alloc_pages(GFP_KERNEL_ACCOUNT | __GFP_ZERO, 1);\n\tif (!p)\n\t\treturn NULL;\n\n\tsplit_page(p, 1);\n\n\t \n\t__free_page(p);\n\n\treturn page_address(p + 1);\n}\n\nstatic void snp_cleanup_vmsa(struct sev_es_save_area *vmsa)\n{\n\tint err;\n\n\terr = snp_set_vmsa(vmsa, false);\n\tif (err)\n\t\tpr_err(\"clear VMSA page failed (%u), leaking page\\n\", err);\n\telse\n\t\tfree_page((unsigned long)vmsa);\n}\n\nstatic int wakeup_cpu_via_vmgexit(int apic_id, unsigned long start_ip)\n{\n\tstruct sev_es_save_area *cur_vmsa, *vmsa;\n\tstruct ghcb_state state;\n\tunsigned long flags;\n\tstruct ghcb *ghcb;\n\tu8 sipi_vector;\n\tint cpu, ret;\n\tu64 cr4;\n\n\t \n\tif (!(sev_hv_features & GHCB_HV_FT_SNP_AP_CREATION))\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tif (WARN_ONCE(start_ip != real_mode_header->trampoline_start,\n\t\t      \"Unsupported SNP start_ip: %lx\\n\", start_ip))\n\t\treturn -EINVAL;\n\n\t \n\tstart_ip = real_mode_header->sev_es_trampoline_start;\n\n\t \n\tfor_each_present_cpu(cpu) {\n\t\tif (arch_match_cpu_phys_id(cpu, apic_id))\n\t\t\tbreak;\n\t}\n\tif (cpu >= nr_cpu_ids)\n\t\treturn -EINVAL;\n\n\tcur_vmsa = per_cpu(sev_vmsa, cpu);\n\n\t \n\tvmsa = (struct sev_es_save_area *)snp_alloc_vmsa_page();\n\tif (!vmsa)\n\t\treturn -ENOMEM;\n\n\t \n\tcr4 = native_read_cr4() & X86_CR4_MCE;\n\n\t \n\tsipi_vector\t\t= (start_ip >> 12);\n\tvmsa->cs.base\t\t= sipi_vector << 12;\n\tvmsa->cs.limit\t\t= AP_INIT_CS_LIMIT;\n\tvmsa->cs.attrib\t\t= INIT_CS_ATTRIBS;\n\tvmsa->cs.selector\t= sipi_vector << 8;\n\n\t \n\tvmsa->rip\t\t= start_ip & 0xfff;\n\n\t \n\tvmsa->ds.limit\t\t= AP_INIT_DS_LIMIT;\n\tvmsa->ds.attrib\t\t= INIT_DS_ATTRIBS;\n\tvmsa->es\t\t= vmsa->ds;\n\tvmsa->fs\t\t= vmsa->ds;\n\tvmsa->gs\t\t= vmsa->ds;\n\tvmsa->ss\t\t= vmsa->ds;\n\n\tvmsa->gdtr.limit\t= AP_INIT_GDTR_LIMIT;\n\tvmsa->ldtr.limit\t= AP_INIT_LDTR_LIMIT;\n\tvmsa->ldtr.attrib\t= INIT_LDTR_ATTRIBS;\n\tvmsa->idtr.limit\t= AP_INIT_IDTR_LIMIT;\n\tvmsa->tr.limit\t\t= AP_INIT_TR_LIMIT;\n\tvmsa->tr.attrib\t\t= INIT_TR_ATTRIBS;\n\n\tvmsa->cr4\t\t= cr4;\n\tvmsa->cr0\t\t= AP_INIT_CR0_DEFAULT;\n\tvmsa->dr7\t\t= DR7_RESET_VALUE;\n\tvmsa->dr6\t\t= AP_INIT_DR6_DEFAULT;\n\tvmsa->rflags\t\t= AP_INIT_RFLAGS_DEFAULT;\n\tvmsa->g_pat\t\t= AP_INIT_GPAT_DEFAULT;\n\tvmsa->xcr0\t\t= AP_INIT_XCR0_DEFAULT;\n\tvmsa->mxcsr\t\t= AP_INIT_MXCSR_DEFAULT;\n\tvmsa->x87_ftw\t\t= AP_INIT_X87_FTW_DEFAULT;\n\tvmsa->x87_fcw\t\t= AP_INIT_X87_FCW_DEFAULT;\n\n\t \n\tvmsa->efer\t\t= EFER_SVME;\n\n\t \n\tvmsa->vmpl\t\t= 0;\n\tvmsa->sev_features\t= sev_status >> 2;\n\n\t \n\tret = snp_set_vmsa(vmsa, true);\n\tif (ret) {\n\t\tpr_err(\"set VMSA page failed (%u)\\n\", ret);\n\t\tfree_page((unsigned long)vmsa);\n\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tlocal_irq_save(flags);\n\n\tghcb = __sev_get_ghcb(&state);\n\n\tvc_ghcb_invalidate(ghcb);\n\tghcb_set_rax(ghcb, vmsa->sev_features);\n\tghcb_set_sw_exit_code(ghcb, SVM_VMGEXIT_AP_CREATION);\n\tghcb_set_sw_exit_info_1(ghcb, ((u64)apic_id << 32) | SVM_VMGEXIT_AP_CREATE);\n\tghcb_set_sw_exit_info_2(ghcb, __pa(vmsa));\n\n\tsev_es_wr_ghcb_msr(__pa(ghcb));\n\tVMGEXIT();\n\n\tif (!ghcb_sw_exit_info_1_is_valid(ghcb) ||\n\t    lower_32_bits(ghcb->save.sw_exit_info_1)) {\n\t\tpr_err(\"SNP AP Creation error\\n\");\n\t\tret = -EINVAL;\n\t}\n\n\t__sev_put_ghcb(&state);\n\n\tlocal_irq_restore(flags);\n\n\t \n\tif (ret) {\n\t\tsnp_cleanup_vmsa(vmsa);\n\t\tvmsa = NULL;\n\t}\n\n\t \n\tif (cur_vmsa)\n\t\tsnp_cleanup_vmsa(cur_vmsa);\n\n\t \n\tper_cpu(sev_vmsa, cpu) = vmsa;\n\n\treturn ret;\n}\n\nvoid __init snp_set_wakeup_secondary_cpu(void)\n{\n\tif (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))\n\t\treturn;\n\n\t \n\tapic_update_callback(wakeup_secondary_cpu, wakeup_cpu_via_vmgexit);\n}\n\nint __init sev_es_setup_ap_jump_table(struct real_mode_header *rmh)\n{\n\tu16 startup_cs, startup_ip;\n\tphys_addr_t jump_table_pa;\n\tu64 jump_table_addr;\n\tu16 __iomem *jump_table;\n\n\tjump_table_addr = get_jump_table_addr();\n\n\t \n\tif (!jump_table_addr)\n\t\treturn 0;\n\n\t \n\tif (jump_table_addr & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\n\tjump_table_pa = jump_table_addr & PAGE_MASK;\n\n\tstartup_cs = (u16)(rmh->trampoline_start >> 4);\n\tstartup_ip = (u16)(rmh->sev_es_trampoline_start -\n\t\t\t   rmh->trampoline_start);\n\n\tjump_table = ioremap_encrypted(jump_table_pa, PAGE_SIZE);\n\tif (!jump_table)\n\t\treturn -EIO;\n\n\twritew(startup_ip, &jump_table[0]);\n\twritew(startup_cs, &jump_table[1]);\n\n\tiounmap(jump_table);\n\n\treturn 0;\n}\n\n \nint __init sev_es_efi_map_ghcbs(pgd_t *pgd)\n{\n\tstruct sev_es_runtime_data *data;\n\tunsigned long address, pflags;\n\tint cpu;\n\tu64 pfn;\n\n\tif (!cc_platform_has(CC_ATTR_GUEST_STATE_ENCRYPT))\n\t\treturn 0;\n\n\tpflags = _PAGE_NX | _PAGE_RW;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tdata = per_cpu(runtime_data, cpu);\n\n\t\taddress = __pa(&data->ghcb_page);\n\t\tpfn = address >> PAGE_SHIFT;\n\n\t\tif (kernel_map_pages_in_pgd(pgd, pfn, address, 1, pflags))\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic enum es_result vc_handle_msr(struct ghcb *ghcb, struct es_em_ctxt *ctxt)\n{\n\tstruct pt_regs *regs = ctxt->regs;\n\tenum es_result ret;\n\tu64 exit_info_1;\n\n\t \n\texit_info_1 = (ctxt->insn.opcode.bytes[1] == 0x30) ? 1 : 0;\n\n\tghcb_set_rcx(ghcb, regs->cx);\n\tif (exit_info_1) {\n\t\tghcb_set_rax(ghcb, regs->ax);\n\t\tghcb_set_rdx(ghcb, regs->dx);\n\t}\n\n\tret = sev_es_ghcb_hv_call(ghcb, ctxt, SVM_EXIT_MSR, exit_info_1, 0);\n\n\tif ((ret == ES_OK) && (!exit_info_1)) {\n\t\tregs->ax = ghcb->save.rax;\n\t\tregs->dx = ghcb->save.rdx;\n\t}\n\n\treturn ret;\n}\n\nstatic void snp_register_per_cpu_ghcb(void)\n{\n\tstruct sev_es_runtime_data *data;\n\tstruct ghcb *ghcb;\n\n\tdata = this_cpu_read(runtime_data);\n\tghcb = &data->ghcb_page;\n\n\tsnp_register_ghcb_early(__pa(ghcb));\n}\n\nvoid setup_ghcb(void)\n{\n\tif (!cc_platform_has(CC_ATTR_GUEST_STATE_ENCRYPT))\n\t\treturn;\n\n\t \n\tif (initial_vc_handler == (unsigned long)kernel_exc_vmm_communication) {\n\t\tif (cc_platform_has(CC_ATTR_GUEST_SEV_SNP))\n\t\t\tsnp_register_per_cpu_ghcb();\n\n\t\tsev_cfg.ghcbs_initialized = true;\n\n\t\treturn;\n\t}\n\n\t \n\tif (!sev_es_negotiate_protocol())\n\t\tsev_es_terminate(SEV_TERM_SET_GEN, GHCB_SEV_ES_GEN_REQ);\n\n\t \n\tmemset(&boot_ghcb_page, 0, PAGE_SIZE);\n\n\t \n\tboot_ghcb = &boot_ghcb_page;\n\n\t \n\tif (cc_platform_has(CC_ATTR_GUEST_SEV_SNP))\n\t\tsnp_register_ghcb_early(__pa(&boot_ghcb_page));\n}\n\n#ifdef CONFIG_HOTPLUG_CPU\nstatic void sev_es_ap_hlt_loop(void)\n{\n\tstruct ghcb_state state;\n\tstruct ghcb *ghcb;\n\n\tghcb = __sev_get_ghcb(&state);\n\n\twhile (true) {\n\t\tvc_ghcb_invalidate(ghcb);\n\t\tghcb_set_sw_exit_code(ghcb, SVM_VMGEXIT_AP_HLT_LOOP);\n\t\tghcb_set_sw_exit_info_1(ghcb, 0);\n\t\tghcb_set_sw_exit_info_2(ghcb, 0);\n\n\t\tsev_es_wr_ghcb_msr(__pa(ghcb));\n\t\tVMGEXIT();\n\n\t\t \n\t\tif (ghcb_sw_exit_info_2_is_valid(ghcb) &&\n\t\t    ghcb->save.sw_exit_info_2)\n\t\t\tbreak;\n\t}\n\n\t__sev_put_ghcb(&state);\n}\n\n \nstatic void sev_es_play_dead(void)\n{\n\tplay_dead_common();\n\n\t \n\n\tsev_es_ap_hlt_loop();\n\n\t \n\tsoft_restart_cpu();\n}\n#else   \n#define sev_es_play_dead\tnative_play_dead\n#endif  \n\n#ifdef CONFIG_SMP\nstatic void __init sev_es_setup_play_dead(void)\n{\n\tsmp_ops.play_dead = sev_es_play_dead;\n}\n#else\nstatic inline void sev_es_setup_play_dead(void) { }\n#endif\n\nstatic void __init alloc_runtime_data(int cpu)\n{\n\tstruct sev_es_runtime_data *data;\n\n\tdata = memblock_alloc(sizeof(*data), PAGE_SIZE);\n\tif (!data)\n\t\tpanic(\"Can't allocate SEV-ES runtime data\");\n\n\tper_cpu(runtime_data, cpu) = data;\n}\n\nstatic void __init init_ghcb(int cpu)\n{\n\tstruct sev_es_runtime_data *data;\n\tint err;\n\n\tdata = per_cpu(runtime_data, cpu);\n\n\terr = early_set_memory_decrypted((unsigned long)&data->ghcb_page,\n\t\t\t\t\t sizeof(data->ghcb_page));\n\tif (err)\n\t\tpanic(\"Can't map GHCBs unencrypted\");\n\n\tmemset(&data->ghcb_page, 0, sizeof(data->ghcb_page));\n\n\tdata->ghcb_active = false;\n\tdata->backup_ghcb_active = false;\n}\n\nvoid __init sev_es_init_vc_handling(void)\n{\n\tint cpu;\n\n\tBUILD_BUG_ON(offsetof(struct sev_es_runtime_data, ghcb_page) % PAGE_SIZE);\n\n\tif (!cc_platform_has(CC_ATTR_GUEST_STATE_ENCRYPT))\n\t\treturn;\n\n\tif (!sev_es_check_cpu_features())\n\t\tpanic(\"SEV-ES CPU Features missing\");\n\n\t \n\tif (cc_platform_has(CC_ATTR_GUEST_SEV_SNP)) {\n\t\tsev_hv_features = get_hv_features();\n\n\t\tif (!(sev_hv_features & GHCB_HV_FT_SNP))\n\t\t\tsev_es_terminate(SEV_TERM_SET_GEN, GHCB_SNP_UNSUPPORTED);\n\t}\n\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\talloc_runtime_data(cpu);\n\t\tinit_ghcb(cpu);\n\t}\n\n\tsev_es_setup_play_dead();\n\n\t \n\tinitial_vc_handler = (unsigned long)kernel_exc_vmm_communication;\n}\n\nstatic void __init vc_early_forward_exception(struct es_em_ctxt *ctxt)\n{\n\tint trapnr = ctxt->fi.vector;\n\n\tif (trapnr == X86_TRAP_PF)\n\t\tnative_write_cr2(ctxt->fi.cr2);\n\n\tctxt->regs->orig_ax = ctxt->fi.error_code;\n\tdo_early_exception(ctxt->regs, trapnr);\n}\n\nstatic long *vc_insn_get_rm(struct es_em_ctxt *ctxt)\n{\n\tlong *reg_array;\n\tint offset;\n\n\treg_array = (long *)ctxt->regs;\n\toffset    = insn_get_modrm_rm_off(&ctxt->insn, ctxt->regs);\n\n\tif (offset < 0)\n\t\treturn NULL;\n\n\toffset /= sizeof(long);\n\n\treturn reg_array + offset;\n}\nstatic enum es_result vc_do_mmio(struct ghcb *ghcb, struct es_em_ctxt *ctxt,\n\t\t\t\t unsigned int bytes, bool read)\n{\n\tu64 exit_code, exit_info_1, exit_info_2;\n\tunsigned long ghcb_pa = __pa(ghcb);\n\tenum es_result res;\n\tphys_addr_t paddr;\n\tvoid __user *ref;\n\n\tref = insn_get_addr_ref(&ctxt->insn, ctxt->regs);\n\tif (ref == (void __user *)-1L)\n\t\treturn ES_UNSUPPORTED;\n\n\texit_code = read ? SVM_VMGEXIT_MMIO_READ : SVM_VMGEXIT_MMIO_WRITE;\n\n\tres = vc_slow_virt_to_phys(ghcb, ctxt, (unsigned long)ref, &paddr);\n\tif (res != ES_OK) {\n\t\tif (res == ES_EXCEPTION && !read)\n\t\t\tctxt->fi.error_code |= X86_PF_WRITE;\n\n\t\treturn res;\n\t}\n\n\texit_info_1 = paddr;\n\t \n\texit_info_2 = bytes;\n\n\tghcb_set_sw_scratch(ghcb, ghcb_pa + offsetof(struct ghcb, shared_buffer));\n\n\treturn sev_es_ghcb_hv_call(ghcb, ctxt, exit_code, exit_info_1, exit_info_2);\n}\n\n \nstatic enum es_result vc_handle_mmio_movs(struct es_em_ctxt *ctxt,\n\t\t\t\t\t  unsigned int bytes)\n{\n\tunsigned long ds_base, es_base;\n\tunsigned char *src, *dst;\n\tunsigned char buffer[8];\n\tenum es_result ret;\n\tbool rep;\n\tint off;\n\n\tds_base = insn_get_seg_base(ctxt->regs, INAT_SEG_REG_DS);\n\tes_base = insn_get_seg_base(ctxt->regs, INAT_SEG_REG_ES);\n\n\tif (ds_base == -1L || es_base == -1L) {\n\t\tctxt->fi.vector = X86_TRAP_GP;\n\t\tctxt->fi.error_code = 0;\n\t\treturn ES_EXCEPTION;\n\t}\n\n\tsrc = ds_base + (unsigned char *)ctxt->regs->si;\n\tdst = es_base + (unsigned char *)ctxt->regs->di;\n\n\tret = vc_read_mem(ctxt, src, buffer, bytes);\n\tif (ret != ES_OK)\n\t\treturn ret;\n\n\tret = vc_write_mem(ctxt, dst, buffer, bytes);\n\tif (ret != ES_OK)\n\t\treturn ret;\n\n\tif (ctxt->regs->flags & X86_EFLAGS_DF)\n\t\toff = -bytes;\n\telse\n\t\toff =  bytes;\n\n\tctxt->regs->si += off;\n\tctxt->regs->di += off;\n\n\trep = insn_has_rep_prefix(&ctxt->insn);\n\tif (rep)\n\t\tctxt->regs->cx -= 1;\n\n\tif (!rep || ctxt->regs->cx == 0)\n\t\treturn ES_OK;\n\telse\n\t\treturn ES_RETRY;\n}\n\nstatic enum es_result vc_handle_mmio(struct ghcb *ghcb, struct es_em_ctxt *ctxt)\n{\n\tstruct insn *insn = &ctxt->insn;\n\tenum insn_mmio_type mmio;\n\tunsigned int bytes = 0;\n\tenum es_result ret;\n\tu8 sign_byte;\n\tlong *reg_data;\n\n\tmmio = insn_decode_mmio(insn, &bytes);\n\tif (mmio == INSN_MMIO_DECODE_FAILED)\n\t\treturn ES_DECODE_FAILED;\n\n\tif (mmio != INSN_MMIO_WRITE_IMM && mmio != INSN_MMIO_MOVS) {\n\t\treg_data = insn_get_modrm_reg_ptr(insn, ctxt->regs);\n\t\tif (!reg_data)\n\t\t\treturn ES_DECODE_FAILED;\n\t}\n\n\tif (user_mode(ctxt->regs))\n\t\treturn ES_UNSUPPORTED;\n\n\tswitch (mmio) {\n\tcase INSN_MMIO_WRITE:\n\t\tmemcpy(ghcb->shared_buffer, reg_data, bytes);\n\t\tret = vc_do_mmio(ghcb, ctxt, bytes, false);\n\t\tbreak;\n\tcase INSN_MMIO_WRITE_IMM:\n\t\tmemcpy(ghcb->shared_buffer, insn->immediate1.bytes, bytes);\n\t\tret = vc_do_mmio(ghcb, ctxt, bytes, false);\n\t\tbreak;\n\tcase INSN_MMIO_READ:\n\t\tret = vc_do_mmio(ghcb, ctxt, bytes, true);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\t \n\t\tif (bytes == 4)\n\t\t\t*reg_data = 0;\n\n\t\tmemcpy(reg_data, ghcb->shared_buffer, bytes);\n\t\tbreak;\n\tcase INSN_MMIO_READ_ZERO_EXTEND:\n\t\tret = vc_do_mmio(ghcb, ctxt, bytes, true);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\t \n\t\tmemset(reg_data, 0, insn->opnd_bytes);\n\t\tmemcpy(reg_data, ghcb->shared_buffer, bytes);\n\t\tbreak;\n\tcase INSN_MMIO_READ_SIGN_EXTEND:\n\t\tret = vc_do_mmio(ghcb, ctxt, bytes, true);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tif (bytes == 1) {\n\t\t\tu8 *val = (u8 *)ghcb->shared_buffer;\n\n\t\t\tsign_byte = (*val & 0x80) ? 0xff : 0x00;\n\t\t} else {\n\t\t\tu16 *val = (u16 *)ghcb->shared_buffer;\n\n\t\t\tsign_byte = (*val & 0x8000) ? 0xff : 0x00;\n\t\t}\n\n\t\t \n\t\tmemset(reg_data, sign_byte, insn->opnd_bytes);\n\t\tmemcpy(reg_data, ghcb->shared_buffer, bytes);\n\t\tbreak;\n\tcase INSN_MMIO_MOVS:\n\t\tret = vc_handle_mmio_movs(ctxt, bytes);\n\t\tbreak;\n\tdefault:\n\t\tret = ES_UNSUPPORTED;\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic enum es_result vc_handle_dr7_write(struct ghcb *ghcb,\n\t\t\t\t\t  struct es_em_ctxt *ctxt)\n{\n\tstruct sev_es_runtime_data *data = this_cpu_read(runtime_data);\n\tlong val, *reg = vc_insn_get_rm(ctxt);\n\tenum es_result ret;\n\n\tif (sev_status & MSR_AMD64_SNP_DEBUG_SWAP)\n\t\treturn ES_VMM_ERROR;\n\n\tif (!reg)\n\t\treturn ES_DECODE_FAILED;\n\n\tval = *reg;\n\n\t \n\tif (val >> 32) {\n\t\tctxt->fi.vector = X86_TRAP_GP;\n\t\tctxt->fi.error_code = 0;\n\t\treturn ES_EXCEPTION;\n\t}\n\n\t \n\tval = (val & 0xffff23ffL) | BIT(10);\n\n\t \n\tif (!data && (val & ~DR7_RESET_VALUE))\n\t\treturn ES_UNSUPPORTED;\n\n\t \n\tghcb_set_rax(ghcb, val);\n\tret = sev_es_ghcb_hv_call(ghcb, ctxt, SVM_EXIT_WRITE_DR7, 0, 0);\n\tif (ret != ES_OK)\n\t\treturn ret;\n\n\tif (data)\n\t\tdata->dr7 = val;\n\n\treturn ES_OK;\n}\n\nstatic enum es_result vc_handle_dr7_read(struct ghcb *ghcb,\n\t\t\t\t\t struct es_em_ctxt *ctxt)\n{\n\tstruct sev_es_runtime_data *data = this_cpu_read(runtime_data);\n\tlong *reg = vc_insn_get_rm(ctxt);\n\n\tif (sev_status & MSR_AMD64_SNP_DEBUG_SWAP)\n\t\treturn ES_VMM_ERROR;\n\n\tif (!reg)\n\t\treturn ES_DECODE_FAILED;\n\n\tif (data)\n\t\t*reg = data->dr7;\n\telse\n\t\t*reg = DR7_RESET_VALUE;\n\n\treturn ES_OK;\n}\n\nstatic enum es_result vc_handle_wbinvd(struct ghcb *ghcb,\n\t\t\t\t       struct es_em_ctxt *ctxt)\n{\n\treturn sev_es_ghcb_hv_call(ghcb, ctxt, SVM_EXIT_WBINVD, 0, 0);\n}\n\nstatic enum es_result vc_handle_rdpmc(struct ghcb *ghcb, struct es_em_ctxt *ctxt)\n{\n\tenum es_result ret;\n\n\tghcb_set_rcx(ghcb, ctxt->regs->cx);\n\n\tret = sev_es_ghcb_hv_call(ghcb, ctxt, SVM_EXIT_RDPMC, 0, 0);\n\tif (ret != ES_OK)\n\t\treturn ret;\n\n\tif (!(ghcb_rax_is_valid(ghcb) && ghcb_rdx_is_valid(ghcb)))\n\t\treturn ES_VMM_ERROR;\n\n\tctxt->regs->ax = ghcb->save.rax;\n\tctxt->regs->dx = ghcb->save.rdx;\n\n\treturn ES_OK;\n}\n\nstatic enum es_result vc_handle_monitor(struct ghcb *ghcb,\n\t\t\t\t\tstruct es_em_ctxt *ctxt)\n{\n\t \n\treturn ES_OK;\n}\n\nstatic enum es_result vc_handle_mwait(struct ghcb *ghcb,\n\t\t\t\t      struct es_em_ctxt *ctxt)\n{\n\t \n\treturn ES_OK;\n}\n\nstatic enum es_result vc_handle_vmmcall(struct ghcb *ghcb,\n\t\t\t\t\tstruct es_em_ctxt *ctxt)\n{\n\tenum es_result ret;\n\n\tghcb_set_rax(ghcb, ctxt->regs->ax);\n\tghcb_set_cpl(ghcb, user_mode(ctxt->regs) ? 3 : 0);\n\n\tif (x86_platform.hyper.sev_es_hcall_prepare)\n\t\tx86_platform.hyper.sev_es_hcall_prepare(ghcb, ctxt->regs);\n\n\tret = sev_es_ghcb_hv_call(ghcb, ctxt, SVM_EXIT_VMMCALL, 0, 0);\n\tif (ret != ES_OK)\n\t\treturn ret;\n\n\tif (!ghcb_rax_is_valid(ghcb))\n\t\treturn ES_VMM_ERROR;\n\n\tctxt->regs->ax = ghcb->save.rax;\n\n\t \n\tif (x86_platform.hyper.sev_es_hcall_finish &&\n\t    !x86_platform.hyper.sev_es_hcall_finish(ghcb, ctxt->regs))\n\t\treturn ES_VMM_ERROR;\n\n\treturn ES_OK;\n}\n\nstatic enum es_result vc_handle_trap_ac(struct ghcb *ghcb,\n\t\t\t\t\tstruct es_em_ctxt *ctxt)\n{\n\t \n\tctxt->fi.vector = X86_TRAP_AC;\n\tctxt->fi.error_code = 0;\n\treturn ES_EXCEPTION;\n}\n\nstatic enum es_result vc_handle_exitcode(struct es_em_ctxt *ctxt,\n\t\t\t\t\t struct ghcb *ghcb,\n\t\t\t\t\t unsigned long exit_code)\n{\n\tenum es_result result;\n\n\tswitch (exit_code) {\n\tcase SVM_EXIT_READ_DR7:\n\t\tresult = vc_handle_dr7_read(ghcb, ctxt);\n\t\tbreak;\n\tcase SVM_EXIT_WRITE_DR7:\n\t\tresult = vc_handle_dr7_write(ghcb, ctxt);\n\t\tbreak;\n\tcase SVM_EXIT_EXCP_BASE + X86_TRAP_AC:\n\t\tresult = vc_handle_trap_ac(ghcb, ctxt);\n\t\tbreak;\n\tcase SVM_EXIT_RDTSC:\n\tcase SVM_EXIT_RDTSCP:\n\t\tresult = vc_handle_rdtsc(ghcb, ctxt, exit_code);\n\t\tbreak;\n\tcase SVM_EXIT_RDPMC:\n\t\tresult = vc_handle_rdpmc(ghcb, ctxt);\n\t\tbreak;\n\tcase SVM_EXIT_INVD:\n\t\tpr_err_ratelimited(\"#VC exception for INVD??? Seriously???\\n\");\n\t\tresult = ES_UNSUPPORTED;\n\t\tbreak;\n\tcase SVM_EXIT_CPUID:\n\t\tresult = vc_handle_cpuid(ghcb, ctxt);\n\t\tbreak;\n\tcase SVM_EXIT_IOIO:\n\t\tresult = vc_handle_ioio(ghcb, ctxt);\n\t\tbreak;\n\tcase SVM_EXIT_MSR:\n\t\tresult = vc_handle_msr(ghcb, ctxt);\n\t\tbreak;\n\tcase SVM_EXIT_VMMCALL:\n\t\tresult = vc_handle_vmmcall(ghcb, ctxt);\n\t\tbreak;\n\tcase SVM_EXIT_WBINVD:\n\t\tresult = vc_handle_wbinvd(ghcb, ctxt);\n\t\tbreak;\n\tcase SVM_EXIT_MONITOR:\n\t\tresult = vc_handle_monitor(ghcb, ctxt);\n\t\tbreak;\n\tcase SVM_EXIT_MWAIT:\n\t\tresult = vc_handle_mwait(ghcb, ctxt);\n\t\tbreak;\n\tcase SVM_EXIT_NPF:\n\t\tresult = vc_handle_mmio(ghcb, ctxt);\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tresult = ES_UNSUPPORTED;\n\t}\n\n\treturn result;\n}\n\nstatic __always_inline void vc_forward_exception(struct es_em_ctxt *ctxt)\n{\n\tlong error_code = ctxt->fi.error_code;\n\tint trapnr = ctxt->fi.vector;\n\n\tctxt->regs->orig_ax = ctxt->fi.error_code;\n\n\tswitch (trapnr) {\n\tcase X86_TRAP_GP:\n\t\texc_general_protection(ctxt->regs, error_code);\n\t\tbreak;\n\tcase X86_TRAP_UD:\n\t\texc_invalid_op(ctxt->regs);\n\t\tbreak;\n\tcase X86_TRAP_PF:\n\t\twrite_cr2(ctxt->fi.cr2);\n\t\texc_page_fault(ctxt->regs, error_code);\n\t\tbreak;\n\tcase X86_TRAP_AC:\n\t\texc_alignment_check(ctxt->regs, error_code);\n\t\tbreak;\n\tdefault:\n\t\tpr_emerg(\"Unsupported exception in #VC instruction emulation - can't continue\\n\");\n\t\tBUG();\n\t}\n}\n\nstatic __always_inline bool is_vc2_stack(unsigned long sp)\n{\n\treturn (sp >= __this_cpu_ist_bottom_va(VC2) && sp < __this_cpu_ist_top_va(VC2));\n}\n\nstatic __always_inline bool vc_from_invalid_context(struct pt_regs *regs)\n{\n\tunsigned long sp, prev_sp;\n\n\tsp      = (unsigned long)regs;\n\tprev_sp = regs->sp;\n\n\t \n\treturn is_vc2_stack(sp) && !is_vc2_stack(prev_sp);\n}\n\nstatic bool vc_raw_handle_exception(struct pt_regs *regs, unsigned long error_code)\n{\n\tstruct ghcb_state state;\n\tstruct es_em_ctxt ctxt;\n\tenum es_result result;\n\tstruct ghcb *ghcb;\n\tbool ret = true;\n\n\tghcb = __sev_get_ghcb(&state);\n\n\tvc_ghcb_invalidate(ghcb);\n\tresult = vc_init_em_ctxt(&ctxt, regs, error_code);\n\n\tif (result == ES_OK)\n\t\tresult = vc_handle_exitcode(&ctxt, ghcb, error_code);\n\n\t__sev_put_ghcb(&state);\n\n\t \n\tswitch (result) {\n\tcase ES_OK:\n\t\tvc_finish_insn(&ctxt);\n\t\tbreak;\n\tcase ES_UNSUPPORTED:\n\t\tpr_err_ratelimited(\"Unsupported exit-code 0x%02lx in #VC exception (IP: 0x%lx)\\n\",\n\t\t\t\t   error_code, regs->ip);\n\t\tret = false;\n\t\tbreak;\n\tcase ES_VMM_ERROR:\n\t\tpr_err_ratelimited(\"Failure in communication with VMM (exit-code 0x%02lx IP: 0x%lx)\\n\",\n\t\t\t\t   error_code, regs->ip);\n\t\tret = false;\n\t\tbreak;\n\tcase ES_DECODE_FAILED:\n\t\tpr_err_ratelimited(\"Failed to decode instruction (exit-code 0x%02lx IP: 0x%lx)\\n\",\n\t\t\t\t   error_code, regs->ip);\n\t\tret = false;\n\t\tbreak;\n\tcase ES_EXCEPTION:\n\t\tvc_forward_exception(&ctxt);\n\t\tbreak;\n\tcase ES_RETRY:\n\t\t \n\t\tbreak;\n\tdefault:\n\t\tpr_emerg(\"Unknown result in %s():%d\\n\", __func__, result);\n\t\t \n\t\tBUG();\n\t}\n\n\treturn ret;\n}\n\nstatic __always_inline bool vc_is_db(unsigned long error_code)\n{\n\treturn error_code == SVM_EXIT_EXCP_BASE + X86_TRAP_DB;\n}\n\n \nDEFINE_IDTENTRY_VC_KERNEL(exc_vmm_communication)\n{\n\tirqentry_state_t irq_state;\n\n\t \n\tif (unlikely(vc_from_invalid_context(regs))) {\n\t\tinstrumentation_begin();\n\t\tpanic(\"Can't handle #VC exception from unsupported context\\n\");\n\t\tinstrumentation_end();\n\t}\n\n\t \n\tif (vc_is_db(error_code)) {\n\t\texc_debug(regs);\n\t\treturn;\n\t}\n\n\tirq_state = irqentry_nmi_enter(regs);\n\n\tinstrumentation_begin();\n\n\tif (!vc_raw_handle_exception(regs, error_code)) {\n\t\t \n\t\tshow_regs(regs);\n\n\t\t \n\t\tsev_es_terminate(SEV_TERM_SET_GEN, GHCB_SEV_ES_GEN_REQ);\n\n\t\t \n\t\tpanic(\"Returned from Terminate-Request to Hypervisor\\n\");\n\t}\n\n\tinstrumentation_end();\n\tirqentry_nmi_exit(regs, irq_state);\n}\n\n \nDEFINE_IDTENTRY_VC_USER(exc_vmm_communication)\n{\n\t \n\tif (vc_is_db(error_code)) {\n\t\tnoist_exc_debug(regs);\n\t\treturn;\n\t}\n\n\tirqentry_enter_from_user_mode(regs);\n\tinstrumentation_begin();\n\n\tif (!vc_raw_handle_exception(regs, error_code)) {\n\t\t \n\t\tforce_sig_fault(SIGBUS, BUS_OBJERR, (void __user *)0);\n\t}\n\n\tinstrumentation_end();\n\tirqentry_exit_to_user_mode(regs);\n}\n\nbool __init handle_vc_boot_ghcb(struct pt_regs *regs)\n{\n\tunsigned long exit_code = regs->orig_ax;\n\tstruct es_em_ctxt ctxt;\n\tenum es_result result;\n\n\tvc_ghcb_invalidate(boot_ghcb);\n\n\tresult = vc_init_em_ctxt(&ctxt, regs, exit_code);\n\tif (result == ES_OK)\n\t\tresult = vc_handle_exitcode(&ctxt, boot_ghcb, exit_code);\n\n\t \n\tswitch (result) {\n\tcase ES_OK:\n\t\tvc_finish_insn(&ctxt);\n\t\tbreak;\n\tcase ES_UNSUPPORTED:\n\t\tearly_printk(\"PANIC: Unsupported exit-code 0x%02lx in early #VC exception (IP: 0x%lx)\\n\",\n\t\t\t\texit_code, regs->ip);\n\t\tgoto fail;\n\tcase ES_VMM_ERROR:\n\t\tearly_printk(\"PANIC: Failure in communication with VMM (exit-code 0x%02lx IP: 0x%lx)\\n\",\n\t\t\t\texit_code, regs->ip);\n\t\tgoto fail;\n\tcase ES_DECODE_FAILED:\n\t\tearly_printk(\"PANIC: Failed to decode instruction (exit-code 0x%02lx IP: 0x%lx)\\n\",\n\t\t\t\texit_code, regs->ip);\n\t\tgoto fail;\n\tcase ES_EXCEPTION:\n\t\tvc_early_forward_exception(&ctxt);\n\t\tbreak;\n\tcase ES_RETRY:\n\t\t \n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\treturn true;\n\nfail:\n\tshow_regs(regs);\n\n\tsev_es_terminate(SEV_TERM_SET_GEN, GHCB_SEV_ES_GEN_REQ);\n}\n\n \nstatic __init struct cc_blob_sev_info *find_cc_blob(struct boot_params *bp)\n{\n\tstruct cc_blob_sev_info *cc_info;\n\n\t \n\tif (bp->cc_blob_address) {\n\t\tcc_info = (struct cc_blob_sev_info *)(unsigned long)bp->cc_blob_address;\n\t\tgoto found_cc_info;\n\t}\n\n\t \n\tcc_info = find_cc_blob_setup_data(bp);\n\tif (!cc_info)\n\t\treturn NULL;\n\nfound_cc_info:\n\tif (cc_info->magic != CC_BLOB_SEV_HDR_MAGIC)\n\t\tsnp_abort();\n\n\treturn cc_info;\n}\n\nbool __init snp_init(struct boot_params *bp)\n{\n\tstruct cc_blob_sev_info *cc_info;\n\n\tif (!bp)\n\t\treturn false;\n\n\tcc_info = find_cc_blob(bp);\n\tif (!cc_info)\n\t\treturn false;\n\n\tsetup_cpuid_table(cc_info);\n\n\t \n\tbp->cc_blob_address = (u32)(unsigned long)cc_info;\n\n\treturn true;\n}\n\nvoid __init __noreturn snp_abort(void)\n{\n\tsev_es_terminate(SEV_TERM_SET_GEN, GHCB_SNP_UNSUPPORTED);\n}\n\nstatic void dump_cpuid_table(void)\n{\n\tconst struct snp_cpuid_table *cpuid_table = snp_cpuid_get_table();\n\tint i = 0;\n\n\tpr_info(\"count=%d reserved=0x%x reserved2=0x%llx\\n\",\n\t\tcpuid_table->count, cpuid_table->__reserved1, cpuid_table->__reserved2);\n\n\tfor (i = 0; i < SNP_CPUID_COUNT_MAX; i++) {\n\t\tconst struct snp_cpuid_fn *fn = &cpuid_table->fn[i];\n\n\t\tpr_info(\"index=%3d fn=0x%08x subfn=0x%08x: eax=0x%08x ebx=0x%08x ecx=0x%08x edx=0x%08x xcr0_in=0x%016llx xss_in=0x%016llx reserved=0x%016llx\\n\",\n\t\t\ti, fn->eax_in, fn->ecx_in, fn->eax, fn->ebx, fn->ecx,\n\t\t\tfn->edx, fn->xcr0_in, fn->xss_in, fn->__reserved);\n\t}\n}\n\n \nstatic int __init report_cpuid_table(void)\n{\n\tconst struct snp_cpuid_table *cpuid_table = snp_cpuid_get_table();\n\n\tif (!cpuid_table->count)\n\t\treturn 0;\n\n\tpr_info(\"Using SNP CPUID table, %d entries present.\\n\",\n\t\tcpuid_table->count);\n\n\tif (sev_cfg.debug)\n\t\tdump_cpuid_table();\n\n\treturn 0;\n}\narch_initcall(report_cpuid_table);\n\nstatic int __init init_sev_config(char *str)\n{\n\tchar *s;\n\n\twhile ((s = strsep(&str, \",\"))) {\n\t\tif (!strcmp(s, \"debug\")) {\n\t\t\tsev_cfg.debug = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\tpr_info(\"SEV command-line option '%s' was not recognized\\n\", s);\n\t}\n\n\treturn 1;\n}\n__setup(\"sev=\", init_sev_config);\n\nint snp_issue_guest_request(u64 exit_code, struct snp_req_data *input, struct snp_guest_request_ioctl *rio)\n{\n\tstruct ghcb_state state;\n\tstruct es_em_ctxt ctxt;\n\tunsigned long flags;\n\tstruct ghcb *ghcb;\n\tint ret;\n\n\trio->exitinfo2 = SEV_RET_NO_FW_CALL;\n\n\t \n\tlocal_irq_save(flags);\n\n\tghcb = __sev_get_ghcb(&state);\n\tif (!ghcb) {\n\t\tret = -EIO;\n\t\tgoto e_restore_irq;\n\t}\n\n\tvc_ghcb_invalidate(ghcb);\n\n\tif (exit_code == SVM_VMGEXIT_EXT_GUEST_REQUEST) {\n\t\tghcb_set_rax(ghcb, input->data_gpa);\n\t\tghcb_set_rbx(ghcb, input->data_npages);\n\t}\n\n\tret = sev_es_ghcb_hv_call(ghcb, &ctxt, exit_code, input->req_gpa, input->resp_gpa);\n\tif (ret)\n\t\tgoto e_put;\n\n\trio->exitinfo2 = ghcb->save.sw_exit_info_2;\n\tswitch (rio->exitinfo2) {\n\tcase 0:\n\t\tbreak;\n\n\tcase SNP_GUEST_VMM_ERR(SNP_GUEST_VMM_ERR_BUSY):\n\t\tret = -EAGAIN;\n\t\tbreak;\n\n\tcase SNP_GUEST_VMM_ERR(SNP_GUEST_VMM_ERR_INVALID_LEN):\n\t\t \n\t\tif (exit_code == SVM_VMGEXIT_EXT_GUEST_REQUEST) {\n\t\t\tinput->data_npages = ghcb_get_rbx(ghcb);\n\t\t\tret = -ENOSPC;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tdefault:\n\t\tret = -EIO;\n\t\tbreak;\n\t}\n\ne_put:\n\t__sev_put_ghcb(&state);\ne_restore_irq:\n\tlocal_irq_restore(flags);\n\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(snp_issue_guest_request);\n\nstatic struct platform_device sev_guest_device = {\n\t.name\t\t= \"sev-guest\",\n\t.id\t\t= -1,\n};\n\nstatic int __init snp_init_platform_device(void)\n{\n\tstruct sev_guest_platform_data data;\n\tu64 gpa;\n\n\tif (!cc_platform_has(CC_ATTR_GUEST_SEV_SNP))\n\t\treturn -ENODEV;\n\n\tgpa = get_secrets_page();\n\tif (!gpa)\n\t\treturn -ENODEV;\n\n\tdata.secrets_gpa = gpa;\n\tif (platform_device_add_data(&sev_guest_device, &data, sizeof(data)))\n\t\treturn -ENODEV;\n\n\tif (platform_device_register(&sev_guest_device))\n\t\treturn -ENODEV;\n\n\tpr_info(\"SNP guest platform device initialized.\\n\");\n\treturn 0;\n}\ndevice_initcall(snp_init_platform_device);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}