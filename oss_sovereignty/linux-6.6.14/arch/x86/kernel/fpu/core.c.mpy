{
  "module_name": "core.c",
  "hash_id": "adce857c36e15c4916071c5c3355cf3e3e4fbbdead7202d66611d39596ccd39f",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/fpu/core.c",
  "human_readable_source": "\n \n#include <asm/fpu/api.h>\n#include <asm/fpu/regset.h>\n#include <asm/fpu/sched.h>\n#include <asm/fpu/signal.h>\n#include <asm/fpu/types.h>\n#include <asm/traps.h>\n#include <asm/irq_regs.h>\n\n#include <uapi/asm/kvm.h>\n\n#include <linux/hardirq.h>\n#include <linux/pkeys.h>\n#include <linux/vmalloc.h>\n\n#include \"context.h\"\n#include \"internal.h\"\n#include \"legacy.h\"\n#include \"xstate.h\"\n\n#define CREATE_TRACE_POINTS\n#include <asm/trace/fpu.h>\n\n#ifdef CONFIG_X86_64\nDEFINE_STATIC_KEY_FALSE(__fpu_state_size_dynamic);\nDEFINE_PER_CPU(u64, xfd_state);\n#endif\n\n \nstruct fpu_state_config\tfpu_kernel_cfg __ro_after_init;\nstruct fpu_state_config fpu_user_cfg __ro_after_init;\n\n \nstruct fpstate init_fpstate __ro_after_init;\n\n \nstatic DEFINE_PER_CPU(bool, in_kernel_fpu);\n\n \nDEFINE_PER_CPU(struct fpu *, fpu_fpregs_owner_ctx);\n\n \nbool irq_fpu_usable(void)\n{\n\tif (WARN_ON_ONCE(in_nmi()))\n\t\treturn false;\n\n\t \n\tif (this_cpu_read(in_kernel_fpu))\n\t\treturn false;\n\n\t \n\tif (!in_hardirq())\n\t\treturn true;\n\n\t \n\treturn !softirq_count();\n}\nEXPORT_SYMBOL(irq_fpu_usable);\n\n \nstatic void update_avx_timestamp(struct fpu *fpu)\n{\n\n#define AVX512_TRACKING_MASK\t(XFEATURE_MASK_ZMM_Hi256 | XFEATURE_MASK_Hi16_ZMM)\n\n\tif (fpu->fpstate->regs.xsave.header.xfeatures & AVX512_TRACKING_MASK)\n\t\tfpu->avx512_timestamp = jiffies;\n}\n\n \nvoid save_fpregs_to_fpstate(struct fpu *fpu)\n{\n\tif (likely(use_xsave())) {\n\t\tos_xsave(fpu->fpstate);\n\t\tupdate_avx_timestamp(fpu);\n\t\treturn;\n\t}\n\n\tif (likely(use_fxsr())) {\n\t\tfxsave(&fpu->fpstate->regs.fxsave);\n\t\treturn;\n\t}\n\n\t \n\tasm volatile(\"fnsave %[fp]; fwait\" : [fp] \"=m\" (fpu->fpstate->regs.fsave));\n\tfrstor(&fpu->fpstate->regs.fsave);\n}\n\nvoid restore_fpregs_from_fpstate(struct fpstate *fpstate, u64 mask)\n{\n\t \n\tif (unlikely(static_cpu_has_bug(X86_BUG_FXSAVE_LEAK))) {\n\t\tasm volatile(\n\t\t\t\"fnclex\\n\\t\"\n\t\t\t\"emms\\n\\t\"\n\t\t\t\"fildl %P[addr]\"\t \n\t\t\t: : [addr] \"m\" (fpstate));\n\t}\n\n\tif (use_xsave()) {\n\t\t \n\t\txfd_update_state(fpstate);\n\n\t\t \n\t\tmask = fpu_kernel_cfg.max_features & mask;\n\n\t\tos_xrstor(fpstate, mask);\n\t} else {\n\t\tif (use_fxsr())\n\t\t\tfxrstor(&fpstate->regs.fxsave);\n\t\telse\n\t\t\tfrstor(&fpstate->regs.fsave);\n\t}\n}\n\nvoid fpu_reset_from_exception_fixup(void)\n{\n\trestore_fpregs_from_fpstate(&init_fpstate, XFEATURE_MASK_FPSTATE);\n}\n\n#if IS_ENABLED(CONFIG_KVM)\nstatic void __fpstate_reset(struct fpstate *fpstate, u64 xfd);\n\nstatic void fpu_init_guest_permissions(struct fpu_guest *gfpu)\n{\n\tstruct fpu_state_perm *fpuperm;\n\tu64 perm;\n\n\tif (!IS_ENABLED(CONFIG_X86_64))\n\t\treturn;\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tfpuperm = &current->group_leader->thread.fpu.guest_perm;\n\tperm = fpuperm->__state_perm;\n\n\t \n\tWRITE_ONCE(fpuperm->__state_perm, perm | FPU_GUEST_PERM_LOCKED);\n\n\tspin_unlock_irq(&current->sighand->siglock);\n\n\tgfpu->perm = perm & ~FPU_GUEST_PERM_LOCKED;\n}\n\nbool fpu_alloc_guest_fpstate(struct fpu_guest *gfpu)\n{\n\tstruct fpstate *fpstate;\n\tunsigned int size;\n\n\tsize = fpu_user_cfg.default_size + ALIGN(offsetof(struct fpstate, regs), 64);\n\tfpstate = vzalloc(size);\n\tif (!fpstate)\n\t\treturn false;\n\n\t \n\t__fpstate_reset(fpstate, 0);\n\tfpstate_init_user(fpstate);\n\tfpstate->is_valloc\t= true;\n\tfpstate->is_guest\t= true;\n\n\tgfpu->fpstate\t\t= fpstate;\n\tgfpu->xfeatures\t\t= fpu_user_cfg.default_features;\n\tgfpu->perm\t\t= fpu_user_cfg.default_features;\n\n\t \n\tgfpu->uabi_size\t\t= sizeof(struct kvm_xsave);\n\tif (WARN_ON_ONCE(fpu_user_cfg.default_size > gfpu->uabi_size))\n\t\tgfpu->uabi_size = fpu_user_cfg.default_size;\n\n\tfpu_init_guest_permissions(gfpu);\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(fpu_alloc_guest_fpstate);\n\nvoid fpu_free_guest_fpstate(struct fpu_guest *gfpu)\n{\n\tstruct fpstate *fps = gfpu->fpstate;\n\n\tif (!fps)\n\t\treturn;\n\n\tif (WARN_ON_ONCE(!fps->is_valloc || !fps->is_guest || fps->in_use))\n\t\treturn;\n\n\tgfpu->fpstate = NULL;\n\tvfree(fps);\n}\nEXPORT_SYMBOL_GPL(fpu_free_guest_fpstate);\n\n \nint fpu_enable_guest_xfd_features(struct fpu_guest *guest_fpu, u64 xfeatures)\n{\n\tlockdep_assert_preemption_enabled();\n\n\t \n\txfeatures &= ~guest_fpu->xfeatures;\n\tif (!xfeatures)\n\t\treturn 0;\n\n\treturn __xfd_enable_feature(xfeatures, guest_fpu);\n}\nEXPORT_SYMBOL_GPL(fpu_enable_guest_xfd_features);\n\n#ifdef CONFIG_X86_64\nvoid fpu_update_guest_xfd(struct fpu_guest *guest_fpu, u64 xfd)\n{\n\tfpregs_lock();\n\tguest_fpu->fpstate->xfd = xfd;\n\tif (guest_fpu->fpstate->in_use)\n\t\txfd_update_state(guest_fpu->fpstate);\n\tfpregs_unlock();\n}\nEXPORT_SYMBOL_GPL(fpu_update_guest_xfd);\n\n \nvoid fpu_sync_guest_vmexit_xfd_state(void)\n{\n\tstruct fpstate *fps = current->thread.fpu.fpstate;\n\n\tlockdep_assert_irqs_disabled();\n\tif (fpu_state_size_dynamic()) {\n\t\trdmsrl(MSR_IA32_XFD, fps->xfd);\n\t\t__this_cpu_write(xfd_state, fps->xfd);\n\t}\n}\nEXPORT_SYMBOL_GPL(fpu_sync_guest_vmexit_xfd_state);\n#endif  \n\nint fpu_swap_kvm_fpstate(struct fpu_guest *guest_fpu, bool enter_guest)\n{\n\tstruct fpstate *guest_fps = guest_fpu->fpstate;\n\tstruct fpu *fpu = &current->thread.fpu;\n\tstruct fpstate *cur_fps = fpu->fpstate;\n\n\tfpregs_lock();\n\tif (!cur_fps->is_confidential && !test_thread_flag(TIF_NEED_FPU_LOAD))\n\t\tsave_fpregs_to_fpstate(fpu);\n\n\t \n\tif (enter_guest) {\n\t\tfpu->__task_fpstate = cur_fps;\n\t\tfpu->fpstate = guest_fps;\n\t\tguest_fps->in_use = true;\n\t} else {\n\t\tguest_fps->in_use = false;\n\t\tfpu->fpstate = fpu->__task_fpstate;\n\t\tfpu->__task_fpstate = NULL;\n\t}\n\n\tcur_fps = fpu->fpstate;\n\n\tif (!cur_fps->is_confidential) {\n\t\t \n\t\trestore_fpregs_from_fpstate(cur_fps, XFEATURE_MASK_FPSTATE);\n\t} else {\n\t\t \n\t\txfd_update_state(cur_fps);\n\t}\n\n\tfpregs_mark_activate();\n\tfpregs_unlock();\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(fpu_swap_kvm_fpstate);\n\nvoid fpu_copy_guest_fpstate_to_uabi(struct fpu_guest *gfpu, void *buf,\n\t\t\t\t    unsigned int size, u64 xfeatures, u32 pkru)\n{\n\tstruct fpstate *kstate = gfpu->fpstate;\n\tunion fpregs_state *ustate = buf;\n\tstruct membuf mb = { .p = buf, .left = size };\n\n\tif (cpu_feature_enabled(X86_FEATURE_XSAVE)) {\n\t\t__copy_xstate_to_uabi_buf(mb, kstate, xfeatures, pkru,\n\t\t\t\t\t  XSTATE_COPY_XSAVE);\n\t} else {\n\t\tmemcpy(&ustate->fxsave, &kstate->regs.fxsave,\n\t\t       sizeof(ustate->fxsave));\n\t\t \n\t\tustate->xsave.header.xfeatures = XFEATURE_MASK_FPSSE;\n\t}\n}\nEXPORT_SYMBOL_GPL(fpu_copy_guest_fpstate_to_uabi);\n\nint fpu_copy_uabi_to_guest_fpstate(struct fpu_guest *gfpu, const void *buf,\n\t\t\t\t   u64 xcr0, u32 *vpkru)\n{\n\tstruct fpstate *kstate = gfpu->fpstate;\n\tconst union fpregs_state *ustate = buf;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_XSAVE)) {\n\t\tif (ustate->xsave.header.xfeatures & ~XFEATURE_MASK_FPSSE)\n\t\t\treturn -EINVAL;\n\t\tif (ustate->fxsave.mxcsr & ~mxcsr_feature_mask)\n\t\t\treturn -EINVAL;\n\t\tmemcpy(&kstate->regs.fxsave, &ustate->fxsave, sizeof(ustate->fxsave));\n\t\treturn 0;\n\t}\n\n\tif (ustate->xsave.header.xfeatures & ~xcr0)\n\t\treturn -EINVAL;\n\n\t \n\tif (!(ustate->xsave.header.xfeatures & XFEATURE_MASK_PKRU))\n\t\tvpkru = NULL;\n\n\treturn copy_uabi_from_kernel_to_xstate(kstate, ustate, vpkru);\n}\nEXPORT_SYMBOL_GPL(fpu_copy_uabi_to_guest_fpstate);\n#endif  \n\nvoid kernel_fpu_begin_mask(unsigned int kfpu_mask)\n{\n\tpreempt_disable();\n\n\tWARN_ON_FPU(!irq_fpu_usable());\n\tWARN_ON_FPU(this_cpu_read(in_kernel_fpu));\n\n\tthis_cpu_write(in_kernel_fpu, true);\n\n\tif (!(current->flags & (PF_KTHREAD | PF_USER_WORKER)) &&\n\t    !test_thread_flag(TIF_NEED_FPU_LOAD)) {\n\t\tset_thread_flag(TIF_NEED_FPU_LOAD);\n\t\tsave_fpregs_to_fpstate(&current->thread.fpu);\n\t}\n\t__cpu_invalidate_fpregs_state();\n\n\t \n\tif (likely(kfpu_mask & KFPU_MXCSR) && boot_cpu_has(X86_FEATURE_XMM))\n\t\tldmxcsr(MXCSR_DEFAULT);\n\n\tif (unlikely(kfpu_mask & KFPU_387) && boot_cpu_has(X86_FEATURE_FPU))\n\t\tasm volatile (\"fninit\");\n}\nEXPORT_SYMBOL_GPL(kernel_fpu_begin_mask);\n\nvoid kernel_fpu_end(void)\n{\n\tWARN_ON_FPU(!this_cpu_read(in_kernel_fpu));\n\n\tthis_cpu_write(in_kernel_fpu, false);\n\tpreempt_enable();\n}\nEXPORT_SYMBOL_GPL(kernel_fpu_end);\n\n \nvoid fpu_sync_fpstate(struct fpu *fpu)\n{\n\tWARN_ON_FPU(fpu != &current->thread.fpu);\n\n\tfpregs_lock();\n\ttrace_x86_fpu_before_save(fpu);\n\n\tif (!test_thread_flag(TIF_NEED_FPU_LOAD))\n\t\tsave_fpregs_to_fpstate(fpu);\n\n\ttrace_x86_fpu_after_save(fpu);\n\tfpregs_unlock();\n}\n\nstatic inline unsigned int init_fpstate_copy_size(void)\n{\n\tif (!use_xsave())\n\t\treturn fpu_kernel_cfg.default_size;\n\n\t \n\treturn sizeof(init_fpstate.regs.xsave);\n}\n\nstatic inline void fpstate_init_fxstate(struct fpstate *fpstate)\n{\n\tfpstate->regs.fxsave.cwd = 0x37f;\n\tfpstate->regs.fxsave.mxcsr = MXCSR_DEFAULT;\n}\n\n \nstatic inline void fpstate_init_fstate(struct fpstate *fpstate)\n{\n\tfpstate->regs.fsave.cwd = 0xffff037fu;\n\tfpstate->regs.fsave.swd = 0xffff0000u;\n\tfpstate->regs.fsave.twd = 0xffffffffu;\n\tfpstate->regs.fsave.fos = 0xffff0000u;\n}\n\n \nvoid fpstate_init_user(struct fpstate *fpstate)\n{\n\tif (!cpu_feature_enabled(X86_FEATURE_FPU)) {\n\t\tfpstate_init_soft(&fpstate->regs.soft);\n\t\treturn;\n\t}\n\n\txstate_init_xcomp_bv(&fpstate->regs.xsave, fpstate->xfeatures);\n\n\tif (cpu_feature_enabled(X86_FEATURE_FXSR))\n\t\tfpstate_init_fxstate(fpstate);\n\telse\n\t\tfpstate_init_fstate(fpstate);\n}\n\nstatic void __fpstate_reset(struct fpstate *fpstate, u64 xfd)\n{\n\t \n\tfpstate->size\t\t= fpu_kernel_cfg.default_size;\n\tfpstate->user_size\t= fpu_user_cfg.default_size;\n\tfpstate->xfeatures\t= fpu_kernel_cfg.default_features;\n\tfpstate->user_xfeatures\t= fpu_user_cfg.default_features;\n\tfpstate->xfd\t\t= xfd;\n}\n\nvoid fpstate_reset(struct fpu *fpu)\n{\n\t \n\tfpu->fpstate = &fpu->__fpstate;\n\t__fpstate_reset(fpu->fpstate, init_fpstate.xfd);\n\n\t \n\tfpu->perm.__state_perm\t\t= fpu_kernel_cfg.default_features;\n\tfpu->perm.__state_size\t\t= fpu_kernel_cfg.default_size;\n\tfpu->perm.__user_state_size\t= fpu_user_cfg.default_size;\n\t \n\tfpu->guest_perm = fpu->perm;\n}\n\nstatic inline void fpu_inherit_perms(struct fpu *dst_fpu)\n{\n\tif (fpu_state_size_dynamic()) {\n\t\tstruct fpu *src_fpu = &current->group_leader->thread.fpu;\n\n\t\tspin_lock_irq(&current->sighand->siglock);\n\t\t \n\t\tdst_fpu->perm = src_fpu->perm;\n\t\tdst_fpu->guest_perm = src_fpu->guest_perm;\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t}\n}\n\n \nstatic int update_fpu_shstk(struct task_struct *dst, unsigned long ssp)\n{\n#ifdef CONFIG_X86_USER_SHADOW_STACK\n\tstruct cet_user_state *xstate;\n\n\t \n\tif (!ssp)\n\t\treturn 0;\n\n\txstate = get_xsave_addr(&dst->thread.fpu.fpstate->regs.xsave,\n\t\t\t\tXFEATURE_CET_USER);\n\n\t \n\tif (WARN_ON_ONCE(!xstate))\n\t\treturn 1;\n\n\txstate->user_ssp = (u64)ssp;\n#endif\n\treturn 0;\n}\n\n \nint fpu_clone(struct task_struct *dst, unsigned long clone_flags, bool minimal,\n\t      unsigned long ssp)\n{\n\tstruct fpu *src_fpu = &current->thread.fpu;\n\tstruct fpu *dst_fpu = &dst->thread.fpu;\n\n\t \n\tdst_fpu->last_cpu = -1;\n\n\tfpstate_reset(dst_fpu);\n\n\tif (!cpu_feature_enabled(X86_FEATURE_FPU))\n\t\treturn 0;\n\n\t \n\tset_tsk_thread_flag(dst, TIF_NEED_FPU_LOAD);\n\n\t \n\tif (minimal) {\n\t\t \n\t\tmemcpy(&dst_fpu->fpstate->regs, &init_fpstate.regs,\n\t\t       init_fpstate_copy_size());\n\t\treturn 0;\n\t}\n\n\t \n\tBUILD_BUG_ON(XFEATURE_MASK_USER_DYNAMIC != XFEATURE_MASK_XTILE_DATA);\n\n\t \n\tfpregs_lock();\n\tif (test_thread_flag(TIF_NEED_FPU_LOAD))\n\t\tfpregs_restore_userregs();\n\tsave_fpregs_to_fpstate(dst_fpu);\n\tfpregs_unlock();\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfpu_inherit_perms(dst_fpu);\n\n\t \n\tif (use_xsave())\n\t\tdst_fpu->fpstate->regs.xsave.header.xfeatures &= ~XFEATURE_MASK_PASID;\n\n\t \n\tif (update_fpu_shstk(dst, ssp))\n\t\treturn 1;\n\n\ttrace_x86_fpu_copy_src(src_fpu);\n\ttrace_x86_fpu_copy_dst(dst_fpu);\n\n\treturn 0;\n}\n\n \nvoid fpu_thread_struct_whitelist(unsigned long *offset, unsigned long *size)\n{\n\t*offset = offsetof(struct thread_struct, fpu.__fpstate.regs);\n\t*size = fpu_kernel_cfg.default_size;\n}\n\n \nvoid fpu__drop(struct fpu *fpu)\n{\n\tpreempt_disable();\n\n\tif (fpu == &current->thread.fpu) {\n\t\t \n\t\tasm volatile(\"1: fwait\\n\"\n\t\t\t     \"2:\\n\"\n\t\t\t     _ASM_EXTABLE(1b, 2b));\n\t\tfpregs_deactivate(fpu);\n\t}\n\n\ttrace_x86_fpu_dropped(fpu);\n\n\tpreempt_enable();\n}\n\n \nstatic inline void restore_fpregs_from_init_fpstate(u64 features_mask)\n{\n\tif (use_xsave())\n\t\tos_xrstor(&init_fpstate, features_mask);\n\telse if (use_fxsr())\n\t\tfxrstor(&init_fpstate.regs.fxsave);\n\telse\n\t\tfrstor(&init_fpstate.regs.fsave);\n\n\tpkru_write_default();\n}\n\n \nstatic void fpu_reset_fpregs(void)\n{\n\tstruct fpu *fpu = &current->thread.fpu;\n\n\tfpregs_lock();\n\t__fpu_invalidate_fpregs_state(fpu);\n\t \n\tmemcpy(&fpu->fpstate->regs, &init_fpstate.regs, init_fpstate_copy_size());\n\tset_thread_flag(TIF_NEED_FPU_LOAD);\n\tfpregs_unlock();\n}\n\n \nvoid fpu__clear_user_states(struct fpu *fpu)\n{\n\tWARN_ON_FPU(fpu != &current->thread.fpu);\n\n\tfpregs_lock();\n\tif (!cpu_feature_enabled(X86_FEATURE_FPU)) {\n\t\tfpu_reset_fpregs();\n\t\tfpregs_unlock();\n\t\treturn;\n\t}\n\n\t \n\tif (xfeatures_mask_supervisor() &&\n\t    !fpregs_state_valid(fpu, smp_processor_id()))\n\t\tos_xrstor_supervisor(fpu->fpstate);\n\n\t \n\trestore_fpregs_from_init_fpstate(XFEATURE_MASK_USER_RESTORE);\n\n\t \n\tfpregs_mark_activate();\n\tfpregs_unlock();\n}\n\nvoid fpu_flush_thread(void)\n{\n\tfpstate_reset(&current->thread.fpu);\n\tfpu_reset_fpregs();\n}\n \nvoid switch_fpu_return(void)\n{\n\tif (!static_cpu_has(X86_FEATURE_FPU))\n\t\treturn;\n\n\tfpregs_restore_userregs();\n}\nEXPORT_SYMBOL_GPL(switch_fpu_return);\n\nvoid fpregs_lock_and_load(void)\n{\n\t \n\tWARN_ON_ONCE(!irq_fpu_usable());\n\tWARN_ON_ONCE(current->flags & PF_KTHREAD);\n\n\tfpregs_lock();\n\n\tfpregs_assert_state_consistent();\n\n\tif (test_thread_flag(TIF_NEED_FPU_LOAD))\n\t\tfpregs_restore_userregs();\n}\n\n#ifdef CONFIG_X86_DEBUG_FPU\n \nvoid fpregs_assert_state_consistent(void)\n{\n\tstruct fpu *fpu = &current->thread.fpu;\n\n\tif (test_thread_flag(TIF_NEED_FPU_LOAD))\n\t\treturn;\n\n\tWARN_ON_FPU(!fpregs_state_valid(fpu, smp_processor_id()));\n}\nEXPORT_SYMBOL_GPL(fpregs_assert_state_consistent);\n#endif\n\nvoid fpregs_mark_activate(void)\n{\n\tstruct fpu *fpu = &current->thread.fpu;\n\n\tfpregs_activate(fpu);\n\tfpu->last_cpu = smp_processor_id();\n\tclear_thread_flag(TIF_NEED_FPU_LOAD);\n}\n\n \n\nint fpu__exception_code(struct fpu *fpu, int trap_nr)\n{\n\tint err;\n\n\tif (trap_nr == X86_TRAP_MF) {\n\t\tunsigned short cwd, swd;\n\t\t \n\t\tif (boot_cpu_has(X86_FEATURE_FXSR)) {\n\t\t\tcwd = fpu->fpstate->regs.fxsave.cwd;\n\t\t\tswd = fpu->fpstate->regs.fxsave.swd;\n\t\t} else {\n\t\t\tcwd = (unsigned short)fpu->fpstate->regs.fsave.cwd;\n\t\t\tswd = (unsigned short)fpu->fpstate->regs.fsave.swd;\n\t\t}\n\n\t\terr = swd & ~cwd;\n\t} else {\n\t\t \n\t\tunsigned short mxcsr = MXCSR_DEFAULT;\n\n\t\tif (boot_cpu_has(X86_FEATURE_XMM))\n\t\t\tmxcsr = fpu->fpstate->regs.fxsave.mxcsr;\n\n\t\terr = ~(mxcsr >> 7) & mxcsr;\n\t}\n\n\tif (err & 0x001) {\t \n\t\t \n\t\treturn FPE_FLTINV;\n\t} else if (err & 0x004) {  \n\t\treturn FPE_FLTDIV;\n\t} else if (err & 0x008) {  \n\t\treturn FPE_FLTOVF;\n\t} else if (err & 0x012) {  \n\t\treturn FPE_FLTUND;\n\t} else if (err & 0x020) {  \n\t\treturn FPE_FLTRES;\n\t}\n\n\t \n\treturn 0;\n}\n\n \nnoinstr void fpu_idle_fpregs(void)\n{\n\t \n\tif (cpu_feature_enabled(X86_FEATURE_AMX_TILE) &&\n\t    (xfeatures_in_use() & XFEATURE_MASK_XTILE)) {\n\t\ttile_release();\n\t\t__this_cpu_write(fpu_fpregs_owner_ctx, NULL);\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}