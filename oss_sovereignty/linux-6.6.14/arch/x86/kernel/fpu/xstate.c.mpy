{
  "module_name": "xstate.c",
  "hash_id": "19ed23989e60b71d41608057f8e0db479795e9bb48609c402f4067c87d6a33c2",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/fpu/xstate.c",
  "human_readable_source": "\n \n#include <linux/bitops.h>\n#include <linux/compat.h>\n#include <linux/cpu.h>\n#include <linux/mman.h>\n#include <linux/nospec.h>\n#include <linux/pkeys.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/vmalloc.h>\n\n#include <asm/fpu/api.h>\n#include <asm/fpu/regset.h>\n#include <asm/fpu/signal.h>\n#include <asm/fpu/xcr.h>\n\n#include <asm/tlbflush.h>\n#include <asm/prctl.h>\n#include <asm/elf.h>\n\n#include \"context.h\"\n#include \"internal.h\"\n#include \"legacy.h\"\n#include \"xstate.h\"\n\n#define for_each_extended_xfeature(bit, mask)\t\t\t\t\\\n\t(bit) = FIRST_EXTENDED_XFEATURE;\t\t\t\t\\\n\tfor_each_set_bit_from(bit, (unsigned long *)&(mask), 8 * sizeof(mask))\n\n \nstatic const char *xfeature_names[] =\n{\n\t\"x87 floating point registers\",\n\t\"SSE registers\",\n\t\"AVX registers\",\n\t\"MPX bounds registers\",\n\t\"MPX CSR\",\n\t\"AVX-512 opmask\",\n\t\"AVX-512 Hi256\",\n\t\"AVX-512 ZMM_Hi256\",\n\t\"Processor Trace (unused)\",\n\t\"Protection Keys User registers\",\n\t\"PASID state\",\n\t\"Control-flow User registers\",\n\t\"Control-flow Kernel registers (unused)\",\n\t\"unknown xstate feature\",\n\t\"unknown xstate feature\",\n\t\"unknown xstate feature\",\n\t\"unknown xstate feature\",\n\t\"AMX Tile config\",\n\t\"AMX Tile data\",\n\t\"unknown xstate feature\",\n};\n\nstatic unsigned short xsave_cpuid_features[] __initdata = {\n\t[XFEATURE_FP]\t\t\t\t= X86_FEATURE_FPU,\n\t[XFEATURE_SSE]\t\t\t\t= X86_FEATURE_XMM,\n\t[XFEATURE_YMM]\t\t\t\t= X86_FEATURE_AVX,\n\t[XFEATURE_BNDREGS]\t\t\t= X86_FEATURE_MPX,\n\t[XFEATURE_BNDCSR]\t\t\t= X86_FEATURE_MPX,\n\t[XFEATURE_OPMASK]\t\t\t= X86_FEATURE_AVX512F,\n\t[XFEATURE_ZMM_Hi256]\t\t\t= X86_FEATURE_AVX512F,\n\t[XFEATURE_Hi16_ZMM]\t\t\t= X86_FEATURE_AVX512F,\n\t[XFEATURE_PT_UNIMPLEMENTED_SO_FAR]\t= X86_FEATURE_INTEL_PT,\n\t[XFEATURE_PKRU]\t\t\t\t= X86_FEATURE_OSPKE,\n\t[XFEATURE_PASID]\t\t\t= X86_FEATURE_ENQCMD,\n\t[XFEATURE_CET_USER]\t\t\t= X86_FEATURE_SHSTK,\n\t[XFEATURE_XTILE_CFG]\t\t\t= X86_FEATURE_AMX_TILE,\n\t[XFEATURE_XTILE_DATA]\t\t\t= X86_FEATURE_AMX_TILE,\n};\n\nstatic unsigned int xstate_offsets[XFEATURE_MAX] __ro_after_init =\n\t{ [ 0 ... XFEATURE_MAX - 1] = -1};\nstatic unsigned int xstate_sizes[XFEATURE_MAX] __ro_after_init =\n\t{ [ 0 ... XFEATURE_MAX - 1] = -1};\nstatic unsigned int xstate_flags[XFEATURE_MAX] __ro_after_init;\n\n#define XSTATE_FLAG_SUPERVISOR\tBIT(0)\n#define XSTATE_FLAG_ALIGNED64\tBIT(1)\n\n \nint cpu_has_xfeatures(u64 xfeatures_needed, const char **feature_name)\n{\n\tu64 xfeatures_missing = xfeatures_needed & ~fpu_kernel_cfg.max_features;\n\n\tif (unlikely(feature_name)) {\n\t\tlong xfeature_idx, max_idx;\n\t\tu64 xfeatures_print;\n\t\t \n\t\tif (xfeatures_missing)\n\t\t\txfeatures_print = xfeatures_missing;\n\t\telse\n\t\t\txfeatures_print = xfeatures_needed;\n\n\t\txfeature_idx = fls64(xfeatures_print)-1;\n\t\tmax_idx = ARRAY_SIZE(xfeature_names)-1;\n\t\txfeature_idx = min(xfeature_idx, max_idx);\n\n\t\t*feature_name = xfeature_names[xfeature_idx];\n\t}\n\n\tif (xfeatures_missing)\n\t\treturn 0;\n\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(cpu_has_xfeatures);\n\nstatic bool xfeature_is_aligned64(int xfeature_nr)\n{\n\treturn xstate_flags[xfeature_nr] & XSTATE_FLAG_ALIGNED64;\n}\n\nstatic bool xfeature_is_supervisor(int xfeature_nr)\n{\n\treturn xstate_flags[xfeature_nr] & XSTATE_FLAG_SUPERVISOR;\n}\n\nstatic unsigned int xfeature_get_offset(u64 xcomp_bv, int xfeature)\n{\n\tunsigned int offs, i;\n\n\t \n\tif (!cpu_feature_enabled(X86_FEATURE_XCOMPACTED) ||\n\t    xfeature <= XFEATURE_SSE)\n\t\treturn xstate_offsets[xfeature];\n\n\t \n\toffs = FXSAVE_SIZE + XSAVE_HDR_SIZE;\n\tfor_each_extended_xfeature(i, xcomp_bv) {\n\t\tif (xfeature_is_aligned64(i))\n\t\t\toffs = ALIGN(offs, 64);\n\t\tif (i == xfeature)\n\t\t\tbreak;\n\t\toffs += xstate_sizes[i];\n\t}\n\treturn offs;\n}\n\n \nvoid fpu__init_cpu_xstate(void)\n{\n\tif (!boot_cpu_has(X86_FEATURE_XSAVE) || !fpu_kernel_cfg.max_features)\n\t\treturn;\n\n\tcr4_set_bits(X86_CR4_OSXSAVE);\n\n\t \n\tif (cpu_feature_enabled(X86_FEATURE_XFD))\n\t\twrmsrl(MSR_IA32_XFD, init_fpstate.xfd);\n\n\t \n\txsetbv(XCR_XFEATURE_ENABLED_MASK, fpu_user_cfg.max_features);\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_XSAVES)) {\n\t\twrmsrl(MSR_IA32_XSS, xfeatures_mask_supervisor() |\n\t\t\t\t     xfeatures_mask_independent());\n\t}\n}\n\nstatic bool xfeature_enabled(enum xfeature xfeature)\n{\n\treturn fpu_kernel_cfg.max_features & BIT_ULL(xfeature);\n}\n\n \nstatic void __init setup_xstate_cache(void)\n{\n\tu32 eax, ebx, ecx, edx, i;\n\t \n\tunsigned int last_good_offset = offsetof(struct xregs_state,\n\t\t\t\t\t\t extended_state_area);\n\t \n\txstate_offsets[XFEATURE_FP]\t= 0;\n\txstate_sizes[XFEATURE_FP]\t= offsetof(struct fxregs_state,\n\t\t\t\t\t\t   xmm_space);\n\n\txstate_offsets[XFEATURE_SSE]\t= xstate_sizes[XFEATURE_FP];\n\txstate_sizes[XFEATURE_SSE]\t= sizeof_field(struct fxregs_state,\n\t\t\t\t\t\t       xmm_space);\n\n\tfor_each_extended_xfeature(i, fpu_kernel_cfg.max_features) {\n\t\tcpuid_count(XSTATE_CPUID, i, &eax, &ebx, &ecx, &edx);\n\n\t\txstate_sizes[i] = eax;\n\t\txstate_flags[i] = ecx;\n\n\t\t \n\t\tif (xfeature_is_supervisor(i))\n\t\t\tcontinue;\n\n\t\txstate_offsets[i] = ebx;\n\n\t\t \n\t\tWARN_ONCE(last_good_offset > xstate_offsets[i],\n\t\t\t  \"x86/fpu: misordered xstate at %d\\n\", last_good_offset);\n\n\t\tlast_good_offset = xstate_offsets[i];\n\t}\n}\n\nstatic void __init print_xstate_feature(u64 xstate_mask)\n{\n\tconst char *feature_name;\n\n\tif (cpu_has_xfeatures(xstate_mask, &feature_name))\n\t\tpr_info(\"x86/fpu: Supporting XSAVE feature 0x%03Lx: '%s'\\n\", xstate_mask, feature_name);\n}\n\n \nstatic void __init print_xstate_features(void)\n{\n\tprint_xstate_feature(XFEATURE_MASK_FP);\n\tprint_xstate_feature(XFEATURE_MASK_SSE);\n\tprint_xstate_feature(XFEATURE_MASK_YMM);\n\tprint_xstate_feature(XFEATURE_MASK_BNDREGS);\n\tprint_xstate_feature(XFEATURE_MASK_BNDCSR);\n\tprint_xstate_feature(XFEATURE_MASK_OPMASK);\n\tprint_xstate_feature(XFEATURE_MASK_ZMM_Hi256);\n\tprint_xstate_feature(XFEATURE_MASK_Hi16_ZMM);\n\tprint_xstate_feature(XFEATURE_MASK_PKRU);\n\tprint_xstate_feature(XFEATURE_MASK_PASID);\n\tprint_xstate_feature(XFEATURE_MASK_CET_USER);\n\tprint_xstate_feature(XFEATURE_MASK_XTILE_CFG);\n\tprint_xstate_feature(XFEATURE_MASK_XTILE_DATA);\n}\n\n \n#define CHECK_XFEATURE(nr) do {\t\t\\\n\tWARN_ON(nr < FIRST_EXTENDED_XFEATURE);\t\\\n\tWARN_ON(nr >= XFEATURE_MAX);\t\\\n} while (0)\n\n \nstatic void __init print_xstate_offset_size(void)\n{\n\tint i;\n\n\tfor_each_extended_xfeature(i, fpu_kernel_cfg.max_features) {\n\t\tpr_info(\"x86/fpu: xstate_offset[%d]: %4d, xstate_sizes[%d]: %4d\\n\",\n\t\t\ti, xfeature_get_offset(fpu_kernel_cfg.max_features, i),\n\t\t\ti, xstate_sizes[i]);\n\t}\n}\n\n \nstatic __init void os_xrstor_booting(struct xregs_state *xstate)\n{\n\tu64 mask = fpu_kernel_cfg.max_features & XFEATURE_MASK_FPSTATE;\n\tu32 lmask = mask;\n\tu32 hmask = mask >> 32;\n\tint err;\n\n\tif (cpu_feature_enabled(X86_FEATURE_XSAVES))\n\t\tXSTATE_OP(XRSTORS, xstate, lmask, hmask, err);\n\telse\n\t\tXSTATE_OP(XRSTOR, xstate, lmask, hmask, err);\n\n\t \n\tWARN_ON_FPU(err);\n}\n\n \n#define XFEATURES_INIT_FPSTATE_HANDLED\t\t\\\n\t(XFEATURE_MASK_FP |\t\t\t\\\n\t XFEATURE_MASK_SSE |\t\t\t\\\n\t XFEATURE_MASK_YMM |\t\t\t\\\n\t XFEATURE_MASK_OPMASK |\t\t\t\\\n\t XFEATURE_MASK_ZMM_Hi256 |\t\t\\\n\t XFEATURE_MASK_Hi16_ZMM\t |\t\t\\\n\t XFEATURE_MASK_PKRU |\t\t\t\\\n\t XFEATURE_MASK_BNDREGS |\t\t\\\n\t XFEATURE_MASK_BNDCSR |\t\t\t\\\n\t XFEATURE_MASK_PASID |\t\t\t\\\n\t XFEATURE_MASK_CET_USER |\t\t\\\n\t XFEATURE_MASK_XTILE)\n\n \nstatic void __init setup_init_fpu_buf(void)\n{\n\tBUILD_BUG_ON((XFEATURE_MASK_USER_SUPPORTED |\n\t\t      XFEATURE_MASK_SUPERVISOR_SUPPORTED) !=\n\t\t     XFEATURES_INIT_FPSTATE_HANDLED);\n\n\tif (!boot_cpu_has(X86_FEATURE_XSAVE))\n\t\treturn;\n\n\tprint_xstate_features();\n\n\txstate_init_xcomp_bv(&init_fpstate.regs.xsave, init_fpstate.xfeatures);\n\n\t \n\tos_xrstor_booting(&init_fpstate.regs.xsave);\n\n\t \n\tfxsave(&init_fpstate.regs.fxsave);\n}\n\nint xfeature_size(int xfeature_nr)\n{\n\tu32 eax, ebx, ecx, edx;\n\n\tCHECK_XFEATURE(xfeature_nr);\n\tcpuid_count(XSTATE_CPUID, xfeature_nr, &eax, &ebx, &ecx, &edx);\n\treturn eax;\n}\n\n \nstatic int validate_user_xstate_header(const struct xstate_header *hdr,\n\t\t\t\t       struct fpstate *fpstate)\n{\n\t \n\tif (hdr->xfeatures & ~fpstate->user_xfeatures)\n\t\treturn -EINVAL;\n\n\t \n\tif (hdr->xcomp_bv)\n\t\treturn -EINVAL;\n\n\t \n\tBUILD_BUG_ON(sizeof(hdr->reserved) != 48);\n\n\t \n\tif (memchr_inv(hdr->reserved, 0, sizeof(hdr->reserved)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic void __init __xstate_dump_leaves(void)\n{\n\tint i;\n\tu32 eax, ebx, ecx, edx;\n\tstatic int should_dump = 1;\n\n\tif (!should_dump)\n\t\treturn;\n\tshould_dump = 0;\n\t \n\tfor (i = 0; i < XFEATURE_MAX + 10; i++) {\n\t\tcpuid_count(XSTATE_CPUID, i, &eax, &ebx, &ecx, &edx);\n\t\tpr_warn(\"CPUID[%02x, %02x]: eax=%08x ebx=%08x ecx=%08x edx=%08x\\n\",\n\t\t\tXSTATE_CPUID, i, eax, ebx, ecx, edx);\n\t}\n}\n\n#define XSTATE_WARN_ON(x, fmt, ...) do {\t\t\t\t\t\\\n\tif (WARN_ONCE(x, \"XSAVE consistency problem: \" fmt, ##__VA_ARGS__)) {\t\\\n\t\t__xstate_dump_leaves();\t\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\t\\\n} while (0)\n\n#define XCHECK_SZ(sz, nr, __struct) ({\t\t\t\t\t\\\n\tif (WARN_ONCE(sz != sizeof(__struct),\t\t\t\t\\\n\t    \"[%s]: struct is %zu bytes, cpu state %d bytes\\n\",\t\t\\\n\t    xfeature_names[nr], sizeof(__struct), sz)) {\t\t\\\n\t\t__xstate_dump_leaves();\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\ttrue;\t\t\t\t\t\t\t\t\\\n})\n\n\n \nstatic int __init check_xtile_data_against_struct(int size)\n{\n\tu32 max_palid, palid, state_size;\n\tu32 eax, ebx, ecx, edx;\n\tu16 max_tile;\n\n\t \n\tcpuid_count(TILE_CPUID, 0, &max_palid, &ebx, &ecx, &edx);\n\n\t \n\tfor (palid = 1, max_tile = 0; palid <= max_palid; palid++) {\n\t\tu16 tile_size, max;\n\n\t\t \n\t\tcpuid_count(TILE_CPUID, palid, &eax, &ebx, &edx, &edx);\n\t\ttile_size = eax >> 16;\n\t\tmax = ebx >> 16;\n\n\t\tif (tile_size != sizeof(struct xtile_data)) {\n\t\t\tpr_err(\"%s: struct is %zu bytes, cpu xtile %d bytes\\n\",\n\t\t\t       __stringify(XFEATURE_XTILE_DATA),\n\t\t\t       sizeof(struct xtile_data), tile_size);\n\t\t\t__xstate_dump_leaves();\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (max > max_tile)\n\t\t\tmax_tile = max;\n\t}\n\n\tstate_size = sizeof(struct xtile_data) * max_tile;\n\tif (size != state_size) {\n\t\tpr_err(\"%s: calculated size is %u bytes, cpu state %d bytes\\n\",\n\t\t       __stringify(XFEATURE_XTILE_DATA), state_size, size);\n\t\t__xstate_dump_leaves();\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n \nstatic bool __init check_xstate_against_struct(int nr)\n{\n\t \n\tint sz = xfeature_size(nr);\n\n\t \n\tswitch (nr) {\n\tcase XFEATURE_YMM:\t  return XCHECK_SZ(sz, nr, struct ymmh_struct);\n\tcase XFEATURE_BNDREGS:\t  return XCHECK_SZ(sz, nr, struct mpx_bndreg_state);\n\tcase XFEATURE_BNDCSR:\t  return XCHECK_SZ(sz, nr, struct mpx_bndcsr_state);\n\tcase XFEATURE_OPMASK:\t  return XCHECK_SZ(sz, nr, struct avx_512_opmask_state);\n\tcase XFEATURE_ZMM_Hi256:  return XCHECK_SZ(sz, nr, struct avx_512_zmm_uppers_state);\n\tcase XFEATURE_Hi16_ZMM:\t  return XCHECK_SZ(sz, nr, struct avx_512_hi16_state);\n\tcase XFEATURE_PKRU:\t  return XCHECK_SZ(sz, nr, struct pkru_state);\n\tcase XFEATURE_PASID:\t  return XCHECK_SZ(sz, nr, struct ia32_pasid_state);\n\tcase XFEATURE_XTILE_CFG:  return XCHECK_SZ(sz, nr, struct xtile_cfg);\n\tcase XFEATURE_CET_USER:\t  return XCHECK_SZ(sz, nr, struct cet_user_state);\n\tcase XFEATURE_XTILE_DATA: check_xtile_data_against_struct(sz); return true;\n\tdefault:\n\t\tXSTATE_WARN_ON(1, \"No structure for xstate: %d\\n\", nr);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic unsigned int xstate_calculate_size(u64 xfeatures, bool compacted)\n{\n\tunsigned int topmost = fls64(xfeatures) -  1;\n\tunsigned int offset = xstate_offsets[topmost];\n\n\tif (topmost <= XFEATURE_SSE)\n\t\treturn sizeof(struct xregs_state);\n\n\tif (compacted)\n\t\toffset = xfeature_get_offset(xfeatures, topmost);\n\treturn offset + xstate_sizes[topmost];\n}\n\n \nstatic bool __init paranoid_xstate_size_valid(unsigned int kernel_size)\n{\n\tbool compacted = cpu_feature_enabled(X86_FEATURE_XCOMPACTED);\n\tbool xsaves = cpu_feature_enabled(X86_FEATURE_XSAVES);\n\tunsigned int size = FXSAVE_SIZE + XSAVE_HDR_SIZE;\n\tint i;\n\n\tfor_each_extended_xfeature(i, fpu_kernel_cfg.max_features) {\n\t\tif (!check_xstate_against_struct(i))\n\t\t\treturn false;\n\t\t \n\t\tif (!xsaves && xfeature_is_supervisor(i)) {\n\t\t\tXSTATE_WARN_ON(1, \"Got supervisor feature %d, but XSAVES not advertised\\n\", i);\n\t\t\treturn false;\n\t\t}\n\t}\n\tsize = xstate_calculate_size(fpu_kernel_cfg.max_features, compacted);\n\tXSTATE_WARN_ON(size != kernel_size,\n\t\t       \"size %u != kernel_size %u\\n\", size, kernel_size);\n\treturn size == kernel_size;\n}\n\n \nstatic unsigned int __init get_compacted_size(void)\n{\n\tunsigned int eax, ebx, ecx, edx;\n\t \n\tcpuid_count(XSTATE_CPUID, 1, &eax, &ebx, &ecx, &edx);\n\treturn ebx;\n}\n\n \nstatic unsigned int __init get_xsave_compacted_size(void)\n{\n\tu64 mask = xfeatures_mask_independent();\n\tunsigned int size;\n\n\tif (!mask)\n\t\treturn get_compacted_size();\n\n\t \n\twrmsrl(MSR_IA32_XSS, xfeatures_mask_supervisor());\n\n\t \n\tsize = get_compacted_size();\n\n\t \n\twrmsrl(MSR_IA32_XSS, xfeatures_mask_supervisor() | mask);\n\n\treturn size;\n}\n\nstatic unsigned int __init get_xsave_size_user(void)\n{\n\tunsigned int eax, ebx, ecx, edx;\n\t \n\tcpuid_count(XSTATE_CPUID, 0, &eax, &ebx, &ecx, &edx);\n\treturn ebx;\n}\n\nstatic int __init init_xstate_size(void)\n{\n\t \n\tunsigned int user_size, kernel_size, kernel_default_size;\n\tbool compacted = cpu_feature_enabled(X86_FEATURE_XCOMPACTED);\n\n\t \n\tuser_size = get_xsave_size_user();\n\n\t \n\tif (compacted)\n\t\tkernel_size = get_xsave_compacted_size();\n\telse\n\t\tkernel_size = user_size;\n\n\tkernel_default_size =\n\t\txstate_calculate_size(fpu_kernel_cfg.default_features, compacted);\n\n\tif (!paranoid_xstate_size_valid(kernel_size))\n\t\treturn -EINVAL;\n\n\tfpu_kernel_cfg.max_size = kernel_size;\n\tfpu_user_cfg.max_size = user_size;\n\n\tfpu_kernel_cfg.default_size = kernel_default_size;\n\tfpu_user_cfg.default_size =\n\t\txstate_calculate_size(fpu_user_cfg.default_features, false);\n\n\treturn 0;\n}\n\n \nstatic void __init fpu__init_disable_system_xstate(unsigned int legacy_size)\n{\n\tfpu_kernel_cfg.max_features = 0;\n\tcr4_clear_bits(X86_CR4_OSXSAVE);\n\tsetup_clear_cpu_cap(X86_FEATURE_XSAVE);\n\n\t \n\tfpu_kernel_cfg.max_size = legacy_size;\n\tfpu_kernel_cfg.default_size = legacy_size;\n\tfpu_user_cfg.max_size = legacy_size;\n\tfpu_user_cfg.default_size = legacy_size;\n\n\t \n\tinit_fpstate.xfd = 0;\n\n\tfpstate_reset(&current->thread.fpu);\n}\n\n \nvoid __init fpu__init_system_xstate(unsigned int legacy_size)\n{\n\tunsigned int eax, ebx, ecx, edx;\n\tu64 xfeatures;\n\tint err;\n\tint i;\n\n\tif (!boot_cpu_has(X86_FEATURE_FPU)) {\n\t\tpr_info(\"x86/fpu: No FPU detected\\n\");\n\t\treturn;\n\t}\n\n\tif (!boot_cpu_has(X86_FEATURE_XSAVE)) {\n\t\tpr_info(\"x86/fpu: x87 FPU will use %s\\n\",\n\t\t\tboot_cpu_has(X86_FEATURE_FXSR) ? \"FXSAVE\" : \"FSAVE\");\n\t\treturn;\n\t}\n\n\tif (boot_cpu_data.cpuid_level < XSTATE_CPUID) {\n\t\tWARN_ON_FPU(1);\n\t\treturn;\n\t}\n\n\t \n\tcpuid_count(XSTATE_CPUID, 0, &eax, &ebx, &ecx, &edx);\n\tfpu_kernel_cfg.max_features = eax + ((u64)edx << 32);\n\n\t \n\tcpuid_count(XSTATE_CPUID, 1, &eax, &ebx, &ecx, &edx);\n\tfpu_kernel_cfg.max_features |= ecx + ((u64)edx << 32);\n\n\tif ((fpu_kernel_cfg.max_features & XFEATURE_MASK_FPSSE) != XFEATURE_MASK_FPSSE) {\n\t\t \n\t\tpr_err(\"x86/fpu: FP/SSE not present amongst the CPU's xstate features: 0x%llx.\\n\",\n\t\t       fpu_kernel_cfg.max_features);\n\t\tgoto out_disable;\n\t}\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(xsave_cpuid_features); i++) {\n\t\tunsigned short cid = xsave_cpuid_features[i];\n\n\t\t \n\t\tif ((i != XFEATURE_FP && !cid) || !boot_cpu_has(cid))\n\t\t\tfpu_kernel_cfg.max_features &= ~BIT_ULL(i);\n\t}\n\n\tif (!cpu_feature_enabled(X86_FEATURE_XFD))\n\t\tfpu_kernel_cfg.max_features &= ~XFEATURE_MASK_USER_DYNAMIC;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_XSAVES))\n\t\tfpu_kernel_cfg.max_features &= XFEATURE_MASK_USER_SUPPORTED;\n\telse\n\t\tfpu_kernel_cfg.max_features &= XFEATURE_MASK_USER_SUPPORTED |\n\t\t\t\t\tXFEATURE_MASK_SUPERVISOR_SUPPORTED;\n\n\tfpu_user_cfg.max_features = fpu_kernel_cfg.max_features;\n\tfpu_user_cfg.max_features &= XFEATURE_MASK_USER_SUPPORTED;\n\n\t \n\tfpu_kernel_cfg.default_features = fpu_kernel_cfg.max_features;\n\tfpu_kernel_cfg.default_features &= ~XFEATURE_MASK_USER_DYNAMIC;\n\n\tfpu_user_cfg.default_features = fpu_user_cfg.max_features;\n\tfpu_user_cfg.default_features &= ~XFEATURE_MASK_USER_DYNAMIC;\n\n\t \n\txfeatures = fpu_kernel_cfg.max_features;\n\n\t \n\tinit_fpstate.xfd = fpu_user_cfg.max_features & XFEATURE_MASK_USER_DYNAMIC;\n\n\t \n\tif (cpu_feature_enabled(X86_FEATURE_XSAVEC) ||\n\t    cpu_feature_enabled(X86_FEATURE_XSAVES))\n\t\tsetup_force_cpu_cap(X86_FEATURE_XCOMPACTED);\n\n\t \n\tfpu__init_cpu_xstate();\n\n\t \n\tsetup_xstate_cache();\n\n\terr = init_xstate_size();\n\tif (err)\n\t\tgoto out_disable;\n\n\t \n\tfpstate_reset(&current->thread.fpu);\n\n\t \n\tupdate_regset_xstate_info(fpu_user_cfg.max_size,\n\t\t\t\t  fpu_user_cfg.max_features);\n\n\t \n\tinit_fpstate.size\t\t= fpu_kernel_cfg.default_size;\n\tinit_fpstate.xfeatures\t\t= fpu_kernel_cfg.default_features;\n\n\tif (init_fpstate.size > sizeof(init_fpstate.regs)) {\n\t\tpr_warn(\"x86/fpu: init_fpstate buffer too small (%zu < %d), disabling XSAVE\\n\",\n\t\t\tsizeof(init_fpstate.regs), init_fpstate.size);\n\t\tgoto out_disable;\n\t}\n\n\tsetup_init_fpu_buf();\n\n\t \n\tif (xfeatures != fpu_kernel_cfg.max_features) {\n\t\tpr_err(\"x86/fpu: xfeatures modified from 0x%016llx to 0x%016llx during init, disabling XSAVE\\n\",\n\t\t       xfeatures, fpu_kernel_cfg.max_features);\n\t\tgoto out_disable;\n\t}\n\n\t \n\tsetup_force_cpu_cap(X86_FEATURE_OSXSAVE);\n\n\tprint_xstate_offset_size();\n\tpr_info(\"x86/fpu: Enabled xstate features 0x%llx, context size is %d bytes, using '%s' format.\\n\",\n\t\tfpu_kernel_cfg.max_features,\n\t\tfpu_kernel_cfg.max_size,\n\t\tboot_cpu_has(X86_FEATURE_XCOMPACTED) ? \"compacted\" : \"standard\");\n\treturn;\n\nout_disable:\n\t \n\tfpu__init_disable_system_xstate(legacy_size);\n}\n\n \nvoid fpu__resume_cpu(void)\n{\n\t \n\tif (cpu_feature_enabled(X86_FEATURE_XSAVE))\n\t\txsetbv(XCR_XFEATURE_ENABLED_MASK, fpu_user_cfg.max_features);\n\n\t \n\tif (cpu_feature_enabled(X86_FEATURE_XSAVES)) {\n\t\twrmsrl(MSR_IA32_XSS, xfeatures_mask_supervisor()  |\n\t\t\t\t     xfeatures_mask_independent());\n\t}\n\n\tif (fpu_state_size_dynamic())\n\t\twrmsrl(MSR_IA32_XFD, current->thread.fpu.fpstate->xfd);\n}\n\n \nstatic void *__raw_xsave_addr(struct xregs_state *xsave, int xfeature_nr)\n{\n\tu64 xcomp_bv = xsave->header.xcomp_bv;\n\n\tif (WARN_ON_ONCE(!xfeature_enabled(xfeature_nr)))\n\t\treturn NULL;\n\n\tif (cpu_feature_enabled(X86_FEATURE_XCOMPACTED)) {\n\t\tif (WARN_ON_ONCE(!(xcomp_bv & BIT_ULL(xfeature_nr))))\n\t\t\treturn NULL;\n\t}\n\n\treturn (void *)xsave + xfeature_get_offset(xcomp_bv, xfeature_nr);\n}\n\n \nvoid *get_xsave_addr(struct xregs_state *xsave, int xfeature_nr)\n{\n\t \n\tif (!boot_cpu_has(X86_FEATURE_XSAVE))\n\t\treturn NULL;\n\n\t \n\tif (WARN_ON_ONCE(!xfeature_enabled(xfeature_nr)))\n\t\treturn NULL;\n\n\t \n\tif (!(xsave->header.xfeatures & BIT_ULL(xfeature_nr)))\n\t\treturn NULL;\n\n\treturn __raw_xsave_addr(xsave, xfeature_nr);\n}\n\n#ifdef CONFIG_ARCH_HAS_PKEYS\n\n \nint arch_set_user_pkey_access(struct task_struct *tsk, int pkey,\n\t\t\t      unsigned long init_val)\n{\n\tu32 old_pkru, new_pkru_bits = 0;\n\tint pkey_shift;\n\n\t \n\tif (!cpu_feature_enabled(X86_FEATURE_OSPKE))\n\t\treturn -EINVAL;\n\n\t \n\tif (WARN_ON_ONCE(pkey >= arch_max_pkey()))\n\t\treturn -EINVAL;\n\n\t \n\tif (init_val & PKEY_DISABLE_ACCESS)\n\t\tnew_pkru_bits |= PKRU_AD_BIT;\n\tif (init_val & PKEY_DISABLE_WRITE)\n\t\tnew_pkru_bits |= PKRU_WD_BIT;\n\n\t \n\tpkey_shift = pkey * PKRU_BITS_PER_PKEY;\n\tnew_pkru_bits <<= pkey_shift;\n\n\t \n\told_pkru = read_pkru();\n\told_pkru &= ~((PKRU_AD_BIT|PKRU_WD_BIT) << pkey_shift);\n\n\t \n\twrite_pkru(old_pkru | new_pkru_bits);\n\n\treturn 0;\n}\n#endif  \n\nstatic void copy_feature(bool from_xstate, struct membuf *to, void *xstate,\n\t\t\t void *init_xstate, unsigned int size)\n{\n\tmembuf_write(to, from_xstate ? xstate : init_xstate, size);\n}\n\n \nvoid __copy_xstate_to_uabi_buf(struct membuf to, struct fpstate *fpstate,\n\t\t\t       u64 xfeatures, u32 pkru_val,\n\t\t\t       enum xstate_copy_mode copy_mode)\n{\n\tconst unsigned int off_mxcsr = offsetof(struct fxregs_state, mxcsr);\n\tstruct xregs_state *xinit = &init_fpstate.regs.xsave;\n\tstruct xregs_state *xsave = &fpstate->regs.xsave;\n\tstruct xstate_header header;\n\tunsigned int zerofrom;\n\tu64 mask;\n\tint i;\n\n\tmemset(&header, 0, sizeof(header));\n\theader.xfeatures = xsave->header.xfeatures;\n\n\t \n\tswitch (copy_mode) {\n\tcase XSTATE_COPY_FP:\n\t\theader.xfeatures &= XFEATURE_MASK_FP;\n\t\tbreak;\n\n\tcase XSTATE_COPY_FX:\n\t\theader.xfeatures &= XFEATURE_MASK_FP | XFEATURE_MASK_SSE;\n\t\tbreak;\n\n\tcase XSTATE_COPY_XSAVE:\n\t\theader.xfeatures &= fpstate->user_xfeatures & xfeatures;\n\t\tbreak;\n\t}\n\n\t \n\tcopy_feature(header.xfeatures & XFEATURE_MASK_FP, &to, &xsave->i387,\n\t\t     &xinit->i387, off_mxcsr);\n\n\t \n\tcopy_feature(header.xfeatures & (XFEATURE_MASK_SSE | XFEATURE_MASK_YMM),\n\t\t     &to, &xsave->i387.mxcsr, &xinit->i387.mxcsr,\n\t\t     MXCSR_AND_FLAGS_SIZE);\n\n\t \n\tcopy_feature(header.xfeatures & XFEATURE_MASK_FP,\n\t\t     &to, &xsave->i387.st_space, &xinit->i387.st_space,\n\t\t     sizeof(xsave->i387.st_space));\n\n\t \n\tcopy_feature(header.xfeatures & XFEATURE_MASK_SSE,\n\t\t     &to, &xsave->i387.xmm_space, &xinit->i387.xmm_space,\n\t\t     sizeof(xsave->i387.xmm_space));\n\n\tif (copy_mode != XSTATE_COPY_XSAVE)\n\t\tgoto out;\n\n\t \n\tmembuf_zero(&to, sizeof(xsave->i387.padding));\n\n\t \n\tmembuf_write(&to, xstate_fx_sw_bytes, sizeof(xsave->i387.sw_reserved));\n\n\t \n\tmembuf_write(&to, &header, sizeof(header));\n\n\tzerofrom = offsetof(struct xregs_state, extended_state_area);\n\n\t \n\tmask = header.xfeatures;\n\n\tfor_each_extended_xfeature(i, mask) {\n\t\t \n\t\tif (zerofrom < xstate_offsets[i])\n\t\t\tmembuf_zero(&to, xstate_offsets[i] - zerofrom);\n\n\t\tif (i == XFEATURE_PKRU) {\n\t\t\tstruct pkru_state pkru = {0};\n\t\t\t \n\t\t\tpkru.pkru = pkru_val;\n\t\t\tmembuf_write(&to, &pkru, sizeof(pkru));\n\t\t} else {\n\t\t\tmembuf_write(&to,\n\t\t\t\t     __raw_xsave_addr(xsave, i),\n\t\t\t\t     xstate_sizes[i]);\n\t\t}\n\t\t \n\t\tzerofrom = xstate_offsets[i] + xstate_sizes[i];\n\t}\n\nout:\n\tif (to.left)\n\t\tmembuf_zero(&to, to.left);\n}\n\n \nvoid copy_xstate_to_uabi_buf(struct membuf to, struct task_struct *tsk,\n\t\t\t     enum xstate_copy_mode copy_mode)\n{\n\t__copy_xstate_to_uabi_buf(to, tsk->thread.fpu.fpstate,\n\t\t\t\t  tsk->thread.fpu.fpstate->user_xfeatures,\n\t\t\t\t  tsk->thread.pkru, copy_mode);\n}\n\nstatic int copy_from_buffer(void *dst, unsigned int offset, unsigned int size,\n\t\t\t    const void *kbuf, const void __user *ubuf)\n{\n\tif (kbuf) {\n\t\tmemcpy(dst, kbuf + offset, size);\n\t} else {\n\t\tif (copy_from_user(dst, ubuf + offset, size))\n\t\t\treturn -EFAULT;\n\t}\n\treturn 0;\n}\n\n\n \nstatic int copy_uabi_to_xstate(struct fpstate *fpstate, const void *kbuf,\n\t\t\t       const void __user *ubuf, u32 *pkru)\n{\n\tstruct xregs_state *xsave = &fpstate->regs.xsave;\n\tunsigned int offset, size;\n\tstruct xstate_header hdr;\n\tu64 mask;\n\tint i;\n\n\toffset = offsetof(struct xregs_state, header);\n\tif (copy_from_buffer(&hdr, offset, sizeof(hdr), kbuf, ubuf))\n\t\treturn -EFAULT;\n\n\tif (validate_user_xstate_header(&hdr, fpstate))\n\t\treturn -EINVAL;\n\n\t \n\tmask = XFEATURE_MASK_FP | XFEATURE_MASK_SSE | XFEATURE_MASK_YMM;\n\tif (hdr.xfeatures & mask) {\n\t\tu32 mxcsr[2];\n\n\t\toffset = offsetof(struct fxregs_state, mxcsr);\n\t\tif (copy_from_buffer(mxcsr, offset, sizeof(mxcsr), kbuf, ubuf))\n\t\t\treturn -EFAULT;\n\n\t\t \n\t\tif (mxcsr[0] & ~mxcsr_feature_mask)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (!(hdr.xfeatures & XFEATURE_MASK_FP)) {\n\t\t\txsave->i387.mxcsr = mxcsr[0];\n\t\t\txsave->i387.mxcsr_mask = mxcsr[1];\n\t\t}\n\t}\n\n\tfor (i = 0; i < XFEATURE_MAX; i++) {\n\t\tmask = BIT_ULL(i);\n\n\t\tif (hdr.xfeatures & mask) {\n\t\t\tvoid *dst = __raw_xsave_addr(xsave, i);\n\n\t\t\toffset = xstate_offsets[i];\n\t\t\tsize = xstate_sizes[i];\n\n\t\t\tif (copy_from_buffer(dst, offset, size, kbuf, ubuf))\n\t\t\t\treturn -EFAULT;\n\t\t}\n\t}\n\n\tif (hdr.xfeatures & XFEATURE_MASK_PKRU) {\n\t\tstruct pkru_state *xpkru;\n\n\t\txpkru = __raw_xsave_addr(xsave, XFEATURE_PKRU);\n\t\t*pkru = xpkru->pkru;\n\t} else {\n\t\t \n\t\tif (pkru)\n\t\t\t*pkru = 0;\n\t}\n\n\t \n\txsave->header.xfeatures &= XFEATURE_MASK_SUPERVISOR_ALL;\n\n\t \n\txsave->header.xfeatures |= hdr.xfeatures;\n\n\treturn 0;\n}\n\n \nint copy_uabi_from_kernel_to_xstate(struct fpstate *fpstate, const void *kbuf, u32 *pkru)\n{\n\treturn copy_uabi_to_xstate(fpstate, kbuf, NULL, pkru);\n}\n\n \nint copy_sigframe_from_user_to_xstate(struct task_struct *tsk,\n\t\t\t\t      const void __user *ubuf)\n{\n\treturn copy_uabi_to_xstate(tsk->thread.fpu.fpstate, NULL, ubuf, &tsk->thread.pkru);\n}\n\nstatic bool validate_independent_components(u64 mask)\n{\n\tu64 xchk;\n\n\tif (WARN_ON_FPU(!cpu_feature_enabled(X86_FEATURE_XSAVES)))\n\t\treturn false;\n\n\txchk = ~xfeatures_mask_independent();\n\n\tif (WARN_ON_ONCE(!mask || mask & xchk))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nvoid xsaves(struct xregs_state *xstate, u64 mask)\n{\n\tint err;\n\n\tif (!validate_independent_components(mask))\n\t\treturn;\n\n\tXSTATE_OP(XSAVES, xstate, (u32)mask, (u32)(mask >> 32), err);\n\tWARN_ON_ONCE(err);\n}\n\n \nvoid xrstors(struct xregs_state *xstate, u64 mask)\n{\n\tint err;\n\n\tif (!validate_independent_components(mask))\n\t\treturn;\n\n\tXSTATE_OP(XRSTORS, xstate, (u32)mask, (u32)(mask >> 32), err);\n\tWARN_ON_ONCE(err);\n}\n\n#if IS_ENABLED(CONFIG_KVM)\nvoid fpstate_clear_xstate_component(struct fpstate *fps, unsigned int xfeature)\n{\n\tvoid *addr = get_xsave_addr(&fps->regs.xsave, xfeature);\n\n\tif (addr)\n\t\tmemset(addr, 0, xstate_sizes[xfeature]);\n}\nEXPORT_SYMBOL_GPL(fpstate_clear_xstate_component);\n#endif\n\n#ifdef CONFIG_X86_64\n\n#ifdef CONFIG_X86_DEBUG_FPU\n \nstatic bool xstate_op_valid(struct fpstate *fpstate, u64 mask, bool rstor)\n{\n\tu64 xfd = __this_cpu_read(xfd_state);\n\n\tif (fpstate->xfd == xfd)\n\t\treturn true;\n\n\t  \n\tif (fpstate->xfd == current->thread.fpu.fpstate->xfd)\n\t\treturn false;\n\n\t \n\tif (fpstate == &init_fpstate)\n\t\treturn rstor;\n\n\t \n\n\t \n\tmask &= ~xfd;\n\n\t \n\tmask &= ~fpstate->xfeatures;\n\n\t \n\treturn !mask;\n}\n\nvoid xfd_validate_state(struct fpstate *fpstate, u64 mask, bool rstor)\n{\n\tWARN_ON_ONCE(!xstate_op_valid(fpstate, mask, rstor));\n}\n#endif  \n\nstatic int __init xfd_update_static_branch(void)\n{\n\t \n\tif (init_fpstate.xfd)\n\t\tstatic_branch_enable(&__fpu_state_size_dynamic);\n\treturn 0;\n}\narch_initcall(xfd_update_static_branch)\n\nvoid fpstate_free(struct fpu *fpu)\n{\n\tif (fpu->fpstate && fpu->fpstate != &fpu->__fpstate)\n\t\tvfree(fpu->fpstate);\n}\n\n \nstatic int fpstate_realloc(u64 xfeatures, unsigned int ksize,\n\t\t\t   unsigned int usize, struct fpu_guest *guest_fpu)\n{\n\tstruct fpu *fpu = &current->thread.fpu;\n\tstruct fpstate *curfps, *newfps = NULL;\n\tunsigned int fpsize;\n\tbool in_use;\n\n\tfpsize = ksize + ALIGN(offsetof(struct fpstate, regs), 64);\n\n\tnewfps = vzalloc(fpsize);\n\tif (!newfps)\n\t\treturn -ENOMEM;\n\tnewfps->size = ksize;\n\tnewfps->user_size = usize;\n\tnewfps->is_valloc = true;\n\n\t \n\tcurfps = guest_fpu ? guest_fpu->fpstate : fpu->fpstate;\n\n\t \n\tin_use = fpu->fpstate == curfps;\n\n\tif (guest_fpu) {\n\t\tnewfps->is_guest = true;\n\t\tnewfps->is_confidential = curfps->is_confidential;\n\t\tnewfps->in_use = curfps->in_use;\n\t\tguest_fpu->xfeatures |= xfeatures;\n\t\tguest_fpu->uabi_size = usize;\n\t}\n\n\tfpregs_lock();\n\t \n\tif (in_use && test_thread_flag(TIF_NEED_FPU_LOAD))\n\t\tfpregs_restore_userregs();\n\n\tnewfps->xfeatures = curfps->xfeatures | xfeatures;\n\tnewfps->user_xfeatures = curfps->user_xfeatures | xfeatures;\n\tnewfps->xfd = curfps->xfd & ~xfeatures;\n\n\t \n\txstate_init_xcomp_bv(&newfps->regs.xsave, newfps->xfeatures);\n\n\tif (guest_fpu) {\n\t\tguest_fpu->fpstate = newfps;\n\t\t \n\t\tif (in_use)\n\t\t\tfpu->fpstate = newfps;\n\t} else {\n\t\tfpu->fpstate = newfps;\n\t}\n\n\tif (in_use)\n\t\txfd_update_state(fpu->fpstate);\n\tfpregs_unlock();\n\n\t \n\tif (curfps && curfps->is_valloc)\n\t\tvfree(curfps);\n\n\treturn 0;\n}\n\nstatic int validate_sigaltstack(unsigned int usize)\n{\n\tstruct task_struct *thread, *leader = current->group_leader;\n\tunsigned long framesize = get_sigframe_size();\n\n\tlockdep_assert_held(&current->sighand->siglock);\n\n\t \n\tframesize -= fpu_user_cfg.max_size;\n\tframesize += usize;\n\tfor_each_thread(leader, thread) {\n\t\tif (thread->sas_ss_size && thread->sas_ss_size < framesize)\n\t\t\treturn -ENOSPC;\n\t}\n\treturn 0;\n}\n\nstatic int __xstate_request_perm(u64 permitted, u64 requested, bool guest)\n{\n\t \n\tbool compacted = cpu_feature_enabled(X86_FEATURE_XCOMPACTED);\n\tstruct fpu *fpu = &current->group_leader->thread.fpu;\n\tstruct fpu_state_perm *perm;\n\tunsigned int ksize, usize;\n\tu64 mask;\n\tint ret = 0;\n\n\t \n\tif ((permitted & requested) == requested)\n\t\treturn 0;\n\n\t \n\tmask = permitted | requested;\n\t \n\tif (!guest)\n\t\tmask |= xfeatures_mask_supervisor();\n\tksize = xstate_calculate_size(mask, compacted);\n\n\t \n\tmask &= XFEATURE_MASK_USER_SUPPORTED;\n\tusize = xstate_calculate_size(mask, false);\n\n\tif (!guest) {\n\t\tret = validate_sigaltstack(usize);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tperm = guest ? &fpu->guest_perm : &fpu->perm;\n\t \n\tWRITE_ONCE(perm->__state_perm, mask);\n\t \n\tperm->__state_size = ksize;\n\tperm->__user_state_size = usize;\n\treturn ret;\n}\n\n \nstatic const u64 xstate_prctl_req[XFEATURE_MAX] = {\n\t[XFEATURE_XTILE_DATA] = XFEATURE_MASK_XTILE_DATA,\n};\n\nstatic int xstate_request_perm(unsigned long idx, bool guest)\n{\n\tu64 permitted, requested;\n\tint ret;\n\n\tif (idx >= XFEATURE_MAX)\n\t\treturn -EINVAL;\n\n\t \n\tidx = array_index_nospec(idx, ARRAY_SIZE(xstate_prctl_req));\n\trequested = xstate_prctl_req[idx];\n\tif (!requested)\n\t\treturn -EOPNOTSUPP;\n\n\tif ((fpu_user_cfg.max_features & requested) != requested)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tpermitted = xstate_get_group_perm(guest);\n\tif ((permitted & requested) == requested)\n\t\treturn 0;\n\n\t \n\tspin_lock_irq(&current->sighand->siglock);\n\tpermitted = xstate_get_group_perm(guest);\n\n\t \n\tif (guest && (permitted & FPU_GUEST_PERM_LOCKED))\n\t\tret = -EBUSY;\n\telse\n\t\tret = __xstate_request_perm(permitted, requested, guest);\n\tspin_unlock_irq(&current->sighand->siglock);\n\treturn ret;\n}\n\nint __xfd_enable_feature(u64 xfd_err, struct fpu_guest *guest_fpu)\n{\n\tu64 xfd_event = xfd_err & XFEATURE_MASK_USER_DYNAMIC;\n\tstruct fpu_state_perm *perm;\n\tunsigned int ksize, usize;\n\tstruct fpu *fpu;\n\n\tif (!xfd_event) {\n\t\tif (!guest_fpu)\n\t\t\tpr_err_once(\"XFD: Invalid xfd error: %016llx\\n\", xfd_err);\n\t\treturn 0;\n\t}\n\n\t \n\tspin_lock_irq(&current->sighand->siglock);\n\n\t \n\tif ((xstate_get_group_perm(!!guest_fpu) & xfd_event) != xfd_event) {\n\t\tspin_unlock_irq(&current->sighand->siglock);\n\t\treturn -EPERM;\n\t}\n\n\tfpu = &current->group_leader->thread.fpu;\n\tperm = guest_fpu ? &fpu->guest_perm : &fpu->perm;\n\tksize = perm->__state_size;\n\tusize = perm->__user_state_size;\n\n\t \n\tspin_unlock_irq(&current->sighand->siglock);\n\n\t \n\tif (fpstate_realloc(xfd_event, ksize, usize, guest_fpu))\n\t\treturn -EFAULT;\n\treturn 0;\n}\n\nint xfd_enable_feature(u64 xfd_err)\n{\n\treturn __xfd_enable_feature(xfd_err, NULL);\n}\n\n#else  \nstatic inline int xstate_request_perm(unsigned long idx, bool guest)\n{\n\treturn -EPERM;\n}\n#endif   \n\nu64 xstate_get_guest_group_perm(void)\n{\n\treturn xstate_get_group_perm(true);\n}\nEXPORT_SYMBOL_GPL(xstate_get_guest_group_perm);\n\n \nlong fpu_xstate_prctl(int option, unsigned long arg2)\n{\n\tu64 __user *uptr = (u64 __user *)arg2;\n\tu64 permitted, supported;\n\tunsigned long idx = arg2;\n\tbool guest = false;\n\n\tswitch (option) {\n\tcase ARCH_GET_XCOMP_SUPP:\n\t\tsupported = fpu_user_cfg.max_features |\tfpu_user_cfg.legacy_features;\n\t\treturn put_user(supported, uptr);\n\n\tcase ARCH_GET_XCOMP_PERM:\n\t\t \n\t\tpermitted = xstate_get_host_group_perm();\n\t\tpermitted &= XFEATURE_MASK_USER_SUPPORTED;\n\t\treturn put_user(permitted, uptr);\n\n\tcase ARCH_GET_XCOMP_GUEST_PERM:\n\t\tpermitted = xstate_get_guest_group_perm();\n\t\tpermitted &= XFEATURE_MASK_USER_SUPPORTED;\n\t\treturn put_user(permitted, uptr);\n\n\tcase ARCH_REQ_XCOMP_GUEST_PERM:\n\t\tguest = true;\n\t\tfallthrough;\n\n\tcase ARCH_REQ_XCOMP_PERM:\n\t\tif (!IS_ENABLED(CONFIG_X86_64))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\treturn xstate_request_perm(idx, guest);\n\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n}\n\n#ifdef CONFIG_PROC_PID_ARCH_STATUS\n \nstatic void avx512_status(struct seq_file *m, struct task_struct *task)\n{\n\tunsigned long timestamp = READ_ONCE(task->thread.fpu.avx512_timestamp);\n\tlong delta;\n\n\tif (!timestamp) {\n\t\t \n\t\tdelta = -1;\n\t} else {\n\t\tdelta = (long)(jiffies - timestamp);\n\t\t \n\t\tif (delta < 0)\n\t\t\tdelta = LONG_MAX;\n\t\tdelta = jiffies_to_msecs(delta);\n\t}\n\n\tseq_put_decimal_ll(m, \"AVX512_elapsed_ms:\\t\", delta);\n\tseq_putc(m, '\\n');\n}\n\n \nint proc_pid_arch_status(struct seq_file *m, struct pid_namespace *ns,\n\t\t\tstruct pid *pid, struct task_struct *task)\n{\n\t \n\tif (cpu_feature_enabled(X86_FEATURE_AVX512F))\n\t\tavx512_status(m, task);\n\n\treturn 0;\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}