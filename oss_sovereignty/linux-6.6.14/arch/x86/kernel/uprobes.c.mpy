{
  "module_name": "uprobes.c",
  "hash_id": "f0f2eb48b93951bafa04740477398445ed923ecea1455967314841526ce58afa",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/uprobes.c",
  "human_readable_source": "\n \n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/ptrace.h>\n#include <linux/uprobes.h>\n#include <linux/uaccess.h>\n\n#include <linux/kdebug.h>\n#include <asm/processor.h>\n#include <asm/insn.h>\n#include <asm/mmu_context.h>\n\n \n\n \n#define UPROBE_FIX_IP\t\t0x01\n\n \n#define UPROBE_FIX_CALL\t\t0x02\n\n \n#define UPROBE_FIX_SETF\t\t0x04\n\n#define UPROBE_FIX_RIP_SI\t0x08\n#define UPROBE_FIX_RIP_DI\t0x10\n#define UPROBE_FIX_RIP_BX\t0x20\n#define UPROBE_FIX_RIP_MASK\t\\\n\t(UPROBE_FIX_RIP_SI | UPROBE_FIX_RIP_DI | UPROBE_FIX_RIP_BX)\n\n#define\tUPROBE_TRAP_NR\t\tUINT_MAX\n\n \n#define OPCODE1(insn)\t\t((insn)->opcode.bytes[0])\n#define OPCODE2(insn)\t\t((insn)->opcode.bytes[1])\n#define OPCODE3(insn)\t\t((insn)->opcode.bytes[2])\n#define MODRM_REG(insn)\t\tX86_MODRM_REG((insn)->modrm.value)\n\n#define W(row, b0, b1, b2, b3, b4, b5, b6, b7, b8, b9, ba, bb, bc, bd, be, bf)\\\n\t(((b0##UL << 0x0)|(b1##UL << 0x1)|(b2##UL << 0x2)|(b3##UL << 0x3) |   \\\n\t  (b4##UL << 0x4)|(b5##UL << 0x5)|(b6##UL << 0x6)|(b7##UL << 0x7) |   \\\n\t  (b8##UL << 0x8)|(b9##UL << 0x9)|(ba##UL << 0xa)|(bb##UL << 0xb) |   \\\n\t  (bc##UL << 0xc)|(bd##UL << 0xd)|(be##UL << 0xe)|(bf##UL << 0xf))    \\\n\t << (row % 32))\n\n \n#if defined(CONFIG_X86_32) || defined(CONFIG_IA32_EMULATION)\nstatic volatile u32 good_insns_32[256 / 32] = {\n\t \n\t \n\tW(0x00, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1) |  \n\tW(0x10, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0) ,  \n\tW(0x20, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) |  \n\tW(0x30, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) ,  \n\tW(0x40, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) |  \n\tW(0x50, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) ,  \n\tW(0x60, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0) |  \n\tW(0x70, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) ,  \n\tW(0x80, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) |  \n\tW(0x90, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) ,  \n\tW(0xa0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) |  \n\tW(0xb0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) ,  \n\tW(0xc0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0) |  \n\tW(0xd0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) ,  \n\tW(0xe0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0) |  \n\tW(0xf0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1)    \n\t \n\t \n};\n#else\n#define good_insns_32\tNULL\n#endif\n\n \n#if defined(CONFIG_X86_64)\nstatic volatile u32 good_insns_64[256 / 32] = {\n\t \n\t \n\tW(0x00, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1) |  \n\tW(0x10, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0) ,  \n\tW(0x20, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0) |  \n\tW(0x30, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0) ,  \n\tW(0x40, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) |  \n\tW(0x50, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) ,  \n\tW(0x60, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0) |  \n\tW(0x70, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) ,  \n\tW(0x80, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) |  \n\tW(0x90, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1) ,  \n\tW(0xa0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) |  \n\tW(0xb0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) ,  \n\tW(0xc0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0) |  \n\tW(0xd0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1) ,  \n\tW(0xe0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0) |  \n\tW(0xf0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1)    \n\t \n\t \n};\n#else\n#define good_insns_64\tNULL\n#endif\n\n \nstatic volatile u32 good_2byte_insns[256 / 32] = {\n\t \n\t \n\tW(0x00, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1) |  \n\tW(0x10, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) ,  \n\tW(0x20, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) |  \n\tW(0x30, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1) ,  \n\tW(0x40, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) |  \n\tW(0x50, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) ,  \n\tW(0x60, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) |  \n\tW(0x70, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1) ,  \n\tW(0x80, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) |  \n\tW(0x90, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) ,  \n\tW(0xa0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1) |  \n\tW(0xb0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) ,  \n\tW(0xc0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) |  \n\tW(0xd0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) ,  \n\tW(0xe0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1) |  \n\tW(0xf0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)    \n\t \n\t \n};\n#undef W\n\n \n\n \n\nstatic bool is_prefix_bad(struct insn *insn)\n{\n\tinsn_byte_t p;\n\tint i;\n\n\tfor_each_insn_prefix(insn, i, p) {\n\t\tinsn_attr_t attr;\n\n\t\tattr = inat_get_opcode_attribute(p);\n\t\tswitch (attr) {\n\t\tcase INAT_MAKE_PREFIX(INAT_PFX_ES):\n\t\tcase INAT_MAKE_PREFIX(INAT_PFX_CS):\n\t\tcase INAT_MAKE_PREFIX(INAT_PFX_DS):\n\t\tcase INAT_MAKE_PREFIX(INAT_PFX_SS):\n\t\tcase INAT_MAKE_PREFIX(INAT_PFX_LOCK):\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic int uprobe_init_insn(struct arch_uprobe *auprobe, struct insn *insn, bool x86_64)\n{\n\tenum insn_mode m = x86_64 ? INSN_MODE_64 : INSN_MODE_32;\n\tu32 volatile *good_insns;\n\tint ret;\n\n\tret = insn_decode(insn, auprobe->insn, sizeof(auprobe->insn), m);\n\tif (ret < 0)\n\t\treturn -ENOEXEC;\n\n\tif (is_prefix_bad(insn))\n\t\treturn -ENOTSUPP;\n\n\t \n\tif (insn_masking_exception(insn))\n\t\treturn -ENOTSUPP;\n\n\tif (x86_64)\n\t\tgood_insns = good_insns_64;\n\telse\n\t\tgood_insns = good_insns_32;\n\n\tif (test_bit(OPCODE1(insn), (unsigned long *)good_insns))\n\t\treturn 0;\n\n\tif (insn->opcode.nbytes == 2) {\n\t\tif (test_bit(OPCODE2(insn), (unsigned long *)good_2byte_insns))\n\t\t\treturn 0;\n\t}\n\n\treturn -ENOTSUPP;\n}\n\n#ifdef CONFIG_X86_64\n \nstatic void riprel_analyze(struct arch_uprobe *auprobe, struct insn *insn)\n{\n\tu8 *cursor;\n\tu8 reg;\n\tu8 reg2;\n\n\tif (!insn_rip_relative(insn))\n\t\treturn;\n\n\t \n\tif (insn->rex_prefix.nbytes) {\n\t\tcursor = auprobe->insn + insn_offset_rex_prefix(insn);\n\t\t \n\t\t*cursor &= 0xfe;\n\t}\n\t \n\tif (insn->vex_prefix.nbytes >= 3) {\n\t\t \n\t\tcursor = auprobe->insn + insn_offset_vex_prefix(insn) + 1;\n\t\t*cursor |= 0x60;\n\t}\n\n\t \n\n\treg = MODRM_REG(insn);\t \n\treg2 = 0xff;\t\t \n\tif (insn->vex_prefix.nbytes)\n\t\treg2 = insn->vex_prefix.bytes[2];\n\t \n\treg2 = ((reg2 >> 3) & 0x7) ^ 0x7;\n\t \n\tif (reg != 6 && reg2 != 6) {\n\t\treg2 = 6;\n\t\tauprobe->defparam.fixups |= UPROBE_FIX_RIP_SI;\n\t} else if (reg != 7 && reg2 != 7) {\n\t\treg2 = 7;\n\t\tauprobe->defparam.fixups |= UPROBE_FIX_RIP_DI;\n\t\t \n\t} else {\n\t\treg2 = 3;\n\t\tauprobe->defparam.fixups |= UPROBE_FIX_RIP_BX;\n\t}\n\t \n\tcursor = auprobe->insn + insn_offset_modrm(insn);\n\t \n\t*cursor = 0x80 | (reg << 3) | reg2;\n}\n\nstatic inline unsigned long *\nscratch_reg(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n\tif (auprobe->defparam.fixups & UPROBE_FIX_RIP_SI)\n\t\treturn &regs->si;\n\tif (auprobe->defparam.fixups & UPROBE_FIX_RIP_DI)\n\t\treturn &regs->di;\n\treturn &regs->bx;\n}\n\n \nstatic void riprel_pre_xol(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n\tif (auprobe->defparam.fixups & UPROBE_FIX_RIP_MASK) {\n\t\tstruct uprobe_task *utask = current->utask;\n\t\tunsigned long *sr = scratch_reg(auprobe, regs);\n\n\t\tutask->autask.saved_scratch_register = *sr;\n\t\t*sr = utask->vaddr + auprobe->defparam.ilen;\n\t}\n}\n\nstatic void riprel_post_xol(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n\tif (auprobe->defparam.fixups & UPROBE_FIX_RIP_MASK) {\n\t\tstruct uprobe_task *utask = current->utask;\n\t\tunsigned long *sr = scratch_reg(auprobe, regs);\n\n\t\t*sr = utask->autask.saved_scratch_register;\n\t}\n}\n#else  \n \nstatic void riprel_analyze(struct arch_uprobe *auprobe, struct insn *insn)\n{\n}\nstatic void riprel_pre_xol(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n}\nstatic void riprel_post_xol(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n}\n#endif  \n\nstruct uprobe_xol_ops {\n\tbool\t(*emulate)(struct arch_uprobe *, struct pt_regs *);\n\tint\t(*pre_xol)(struct arch_uprobe *, struct pt_regs *);\n\tint\t(*post_xol)(struct arch_uprobe *, struct pt_regs *);\n\tvoid\t(*abort)(struct arch_uprobe *, struct pt_regs *);\n};\n\nstatic inline int sizeof_long(struct pt_regs *regs)\n{\n\t \n\treturn user_64bit_mode(regs) ? 8 : 4;\n}\n\nstatic int default_pre_xol_op(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n\triprel_pre_xol(auprobe, regs);\n\treturn 0;\n}\n\nstatic int emulate_push_stack(struct pt_regs *regs, unsigned long val)\n{\n\tunsigned long new_sp = regs->sp - sizeof_long(regs);\n\n\tif (copy_to_user((void __user *)new_sp, &val, sizeof_long(regs)))\n\t\treturn -EFAULT;\n\n\tregs->sp = new_sp;\n\treturn 0;\n}\n\n \nstatic int default_post_xol_op(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n\tstruct uprobe_task *utask = current->utask;\n\n\triprel_post_xol(auprobe, regs);\n\tif (auprobe->defparam.fixups & UPROBE_FIX_IP) {\n\t\tlong correction = utask->vaddr - utask->xol_vaddr;\n\t\tregs->ip += correction;\n\t} else if (auprobe->defparam.fixups & UPROBE_FIX_CALL) {\n\t\tregs->sp += sizeof_long(regs);  \n\t\tif (emulate_push_stack(regs, utask->vaddr + auprobe->defparam.ilen))\n\t\t\treturn -ERESTART;\n\t}\n\t \n\tif (auprobe->defparam.fixups & UPROBE_FIX_SETF)\n\t\tutask->autask.saved_tf = true;\n\n\treturn 0;\n}\n\nstatic void default_abort_op(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n\triprel_post_xol(auprobe, regs);\n}\n\nstatic const struct uprobe_xol_ops default_xol_ops = {\n\t.pre_xol  = default_pre_xol_op,\n\t.post_xol = default_post_xol_op,\n\t.abort\t  = default_abort_op,\n};\n\nstatic bool branch_is_call(struct arch_uprobe *auprobe)\n{\n\treturn auprobe->branch.opc1 == 0xe8;\n}\n\n#define CASE_COND\t\t\t\t\t\\\n\tCOND(70, 71, XF(OF))\t\t\t\t\\\n\tCOND(72, 73, XF(CF))\t\t\t\t\\\n\tCOND(74, 75, XF(ZF))\t\t\t\t\\\n\tCOND(78, 79, XF(SF))\t\t\t\t\\\n\tCOND(7a, 7b, XF(PF))\t\t\t\t\\\n\tCOND(76, 77, XF(CF) || XF(ZF))\t\t\t\\\n\tCOND(7c, 7d, XF(SF) != XF(OF))\t\t\t\\\n\tCOND(7e, 7f, XF(ZF) || XF(SF) != XF(OF))\n\n#define COND(op_y, op_n, expr)\t\t\t\t\\\n\tcase 0x ## op_y: DO((expr) != 0)\t\t\\\n\tcase 0x ## op_n: DO((expr) == 0)\n\n#define XF(xf)\t(!!(flags & X86_EFLAGS_ ## xf))\n\nstatic bool is_cond_jmp_opcode(u8 opcode)\n{\n\tswitch (opcode) {\n\t#define DO(expr)\t\\\n\t\treturn true;\n\tCASE_COND\n\t#undef\tDO\n\n\tdefault:\n\t\treturn false;\n\t}\n}\n\nstatic bool check_jmp_cond(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n\tunsigned long flags = regs->flags;\n\n\tswitch (auprobe->branch.opc1) {\n\t#define DO(expr)\t\\\n\t\treturn expr;\n\tCASE_COND\n\t#undef\tDO\n\n\tdefault:\t \n\t\treturn true;\n\t}\n}\n\n#undef\tXF\n#undef\tCOND\n#undef\tCASE_COND\n\nstatic bool branch_emulate_op(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n\tunsigned long new_ip = regs->ip += auprobe->branch.ilen;\n\tunsigned long offs = (long)auprobe->branch.offs;\n\n\tif (branch_is_call(auprobe)) {\n\t\t \n\t\tif (emulate_push_stack(regs, new_ip))\n\t\t\treturn false;\n\t} else if (!check_jmp_cond(auprobe, regs)) {\n\t\toffs = 0;\n\t}\n\n\tregs->ip = new_ip + offs;\n\treturn true;\n}\n\nstatic bool push_emulate_op(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n\tunsigned long *src_ptr = (void *)regs + auprobe->push.reg_offset;\n\n\tif (emulate_push_stack(regs, *src_ptr))\n\t\treturn false;\n\tregs->ip += auprobe->push.ilen;\n\treturn true;\n}\n\nstatic int branch_post_xol_op(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n\tBUG_ON(!branch_is_call(auprobe));\n\t \n\tregs->sp += sizeof_long(regs);\n\treturn -ERESTART;\n}\n\nstatic void branch_clear_offset(struct arch_uprobe *auprobe, struct insn *insn)\n{\n\t \n\tmemset(auprobe->insn + insn_offset_immediate(insn),\n\t\t0, insn->immediate.nbytes);\n}\n\nstatic const struct uprobe_xol_ops branch_xol_ops = {\n\t.emulate  = branch_emulate_op,\n\t.post_xol = branch_post_xol_op,\n};\n\nstatic const struct uprobe_xol_ops push_xol_ops = {\n\t.emulate  = push_emulate_op,\n};\n\n \nstatic int branch_setup_xol_ops(struct arch_uprobe *auprobe, struct insn *insn)\n{\n\tu8 opc1 = OPCODE1(insn);\n\tinsn_byte_t p;\n\tint i;\n\n\tswitch (opc1) {\n\tcase 0xeb:\t \n\tcase 0xe9:\t \n\t\tbreak;\n\tcase 0x90:\t \n\t\tgoto setup;\n\n\tcase 0xe8:\t \n\t\tbranch_clear_offset(auprobe, insn);\n\t\tbreak;\n\n\tcase 0x0f:\n\t\tif (insn->opcode.nbytes != 2)\n\t\t\treturn -ENOSYS;\n\t\t \n\t\topc1 = OPCODE2(insn) - 0x10;\n\t\tfallthrough;\n\tdefault:\n\t\tif (!is_cond_jmp_opcode(opc1))\n\t\t\treturn -ENOSYS;\n\t}\n\n\t \n\tfor_each_insn_prefix(insn, i, p) {\n\t\tif (p == 0x66)\n\t\t\treturn -ENOTSUPP;\n\t}\n\nsetup:\n\tauprobe->branch.opc1 = opc1;\n\tauprobe->branch.ilen = insn->length;\n\tauprobe->branch.offs = insn->immediate.value;\n\n\tauprobe->ops = &branch_xol_ops;\n\treturn 0;\n}\n\n \nstatic int push_setup_xol_ops(struct arch_uprobe *auprobe, struct insn *insn)\n{\n\tu8 opc1 = OPCODE1(insn), reg_offset = 0;\n\n\tif (opc1 < 0x50 || opc1 > 0x57)\n\t\treturn -ENOSYS;\n\n\tif (insn->length > 2)\n\t\treturn -ENOSYS;\n\tif (insn->length == 2) {\n\t\t \n#ifdef CONFIG_X86_64\n\t\tif (insn->rex_prefix.nbytes != 1 ||\n\t\t    insn->rex_prefix.bytes[0] != 0x41)\n\t\t\treturn -ENOSYS;\n\n\t\tswitch (opc1) {\n\t\tcase 0x50:\n\t\t\treg_offset = offsetof(struct pt_regs, r8);\n\t\t\tbreak;\n\t\tcase 0x51:\n\t\t\treg_offset = offsetof(struct pt_regs, r9);\n\t\t\tbreak;\n\t\tcase 0x52:\n\t\t\treg_offset = offsetof(struct pt_regs, r10);\n\t\t\tbreak;\n\t\tcase 0x53:\n\t\t\treg_offset = offsetof(struct pt_regs, r11);\n\t\t\tbreak;\n\t\tcase 0x54:\n\t\t\treg_offset = offsetof(struct pt_regs, r12);\n\t\t\tbreak;\n\t\tcase 0x55:\n\t\t\treg_offset = offsetof(struct pt_regs, r13);\n\t\t\tbreak;\n\t\tcase 0x56:\n\t\t\treg_offset = offsetof(struct pt_regs, r14);\n\t\t\tbreak;\n\t\tcase 0x57:\n\t\t\treg_offset = offsetof(struct pt_regs, r15);\n\t\t\tbreak;\n\t\t}\n#else\n\t\treturn -ENOSYS;\n#endif\n\t} else {\n\t\tswitch (opc1) {\n\t\tcase 0x50:\n\t\t\treg_offset = offsetof(struct pt_regs, ax);\n\t\t\tbreak;\n\t\tcase 0x51:\n\t\t\treg_offset = offsetof(struct pt_regs, cx);\n\t\t\tbreak;\n\t\tcase 0x52:\n\t\t\treg_offset = offsetof(struct pt_regs, dx);\n\t\t\tbreak;\n\t\tcase 0x53:\n\t\t\treg_offset = offsetof(struct pt_regs, bx);\n\t\t\tbreak;\n\t\tcase 0x54:\n\t\t\treg_offset = offsetof(struct pt_regs, sp);\n\t\t\tbreak;\n\t\tcase 0x55:\n\t\t\treg_offset = offsetof(struct pt_regs, bp);\n\t\t\tbreak;\n\t\tcase 0x56:\n\t\t\treg_offset = offsetof(struct pt_regs, si);\n\t\t\tbreak;\n\t\tcase 0x57:\n\t\t\treg_offset = offsetof(struct pt_regs, di);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tauprobe->push.reg_offset = reg_offset;\n\tauprobe->push.ilen = insn->length;\n\tauprobe->ops = &push_xol_ops;\n\treturn 0;\n}\n\n \nint arch_uprobe_analyze_insn(struct arch_uprobe *auprobe, struct mm_struct *mm, unsigned long addr)\n{\n\tstruct insn insn;\n\tu8 fix_ip_or_call = UPROBE_FIX_IP;\n\tint ret;\n\n\tret = uprobe_init_insn(auprobe, &insn, is_64bit_mm(mm));\n\tif (ret)\n\t\treturn ret;\n\n\tret = branch_setup_xol_ops(auprobe, &insn);\n\tif (ret != -ENOSYS)\n\t\treturn ret;\n\n\tret = push_setup_xol_ops(auprobe, &insn);\n\tif (ret != -ENOSYS)\n\t\treturn ret;\n\n\t \n\tswitch (OPCODE1(&insn)) {\n\tcase 0x9d:\t\t \n\t\tauprobe->defparam.fixups |= UPROBE_FIX_SETF;\n\t\tbreak;\n\tcase 0xc3:\t\t \n\tcase 0xcb:\n\tcase 0xc2:\n\tcase 0xca:\n\tcase 0xea:\t\t \n\t\tfix_ip_or_call = 0;\n\t\tbreak;\n\tcase 0x9a:\t\t \n\t\tfix_ip_or_call = UPROBE_FIX_CALL;\n\t\tbreak;\n\tcase 0xff:\n\t\tswitch (MODRM_REG(&insn)) {\n\t\tcase 2: case 3:\t\t\t \n\t\t\tfix_ip_or_call = UPROBE_FIX_CALL;\n\t\t\tbreak;\n\t\tcase 4: case 5:\t\t\t \n\t\t\tfix_ip_or_call = 0;\n\t\t\tbreak;\n\t\t}\n\t\tfallthrough;\n\tdefault:\n\t\triprel_analyze(auprobe, &insn);\n\t}\n\n\tauprobe->defparam.ilen = insn.length;\n\tauprobe->defparam.fixups |= fix_ip_or_call;\n\n\tauprobe->ops = &default_xol_ops;\n\treturn 0;\n}\n\n \nint arch_uprobe_pre_xol(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n\tstruct uprobe_task *utask = current->utask;\n\n\tif (auprobe->ops->pre_xol) {\n\t\tint err = auprobe->ops->pre_xol(auprobe, regs);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tregs->ip = utask->xol_vaddr;\n\tutask->autask.saved_trap_nr = current->thread.trap_nr;\n\tcurrent->thread.trap_nr = UPROBE_TRAP_NR;\n\n\tutask->autask.saved_tf = !!(regs->flags & X86_EFLAGS_TF);\n\tregs->flags |= X86_EFLAGS_TF;\n\tif (test_tsk_thread_flag(current, TIF_BLOCKSTEP))\n\t\tset_task_blockstep(current, false);\n\n\treturn 0;\n}\n\n \nbool arch_uprobe_xol_was_trapped(struct task_struct *t)\n{\n\tif (t->thread.trap_nr != UPROBE_TRAP_NR)\n\t\treturn true;\n\n\treturn false;\n}\n\n \nint arch_uprobe_post_xol(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n\tstruct uprobe_task *utask = current->utask;\n\tbool send_sigtrap = utask->autask.saved_tf;\n\tint err = 0;\n\n\tWARN_ON_ONCE(current->thread.trap_nr != UPROBE_TRAP_NR);\n\tcurrent->thread.trap_nr = utask->autask.saved_trap_nr;\n\n\tif (auprobe->ops->post_xol) {\n\t\terr = auprobe->ops->post_xol(auprobe, regs);\n\t\tif (err) {\n\t\t\t \n\t\t\tregs->ip = utask->vaddr;\n\t\t\tif (err == -ERESTART)\n\t\t\t\terr = 0;\n\t\t\tsend_sigtrap = false;\n\t\t}\n\t}\n\t \n\tif (send_sigtrap)\n\t\tsend_sig(SIGTRAP, current, 0);\n\n\tif (!utask->autask.saved_tf)\n\t\tregs->flags &= ~X86_EFLAGS_TF;\n\n\treturn err;\n}\n\n \nint arch_uprobe_exception_notify(struct notifier_block *self, unsigned long val, void *data)\n{\n\tstruct die_args *args = data;\n\tstruct pt_regs *regs = args->regs;\n\tint ret = NOTIFY_DONE;\n\n\t \n\tif (regs && !user_mode(regs))\n\t\treturn NOTIFY_DONE;\n\n\tswitch (val) {\n\tcase DIE_INT3:\n\t\tif (uprobe_pre_sstep_notifier(regs))\n\t\t\tret = NOTIFY_STOP;\n\n\t\tbreak;\n\n\tcase DIE_DEBUG:\n\t\tif (uprobe_post_sstep_notifier(regs))\n\t\t\tret = NOTIFY_STOP;\n\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\n \nvoid arch_uprobe_abort_xol(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n\tstruct uprobe_task *utask = current->utask;\n\n\tif (auprobe->ops->abort)\n\t\tauprobe->ops->abort(auprobe, regs);\n\n\tcurrent->thread.trap_nr = utask->autask.saved_trap_nr;\n\tregs->ip = utask->vaddr;\n\t \n\tif (!utask->autask.saved_tf)\n\t\tregs->flags &= ~X86_EFLAGS_TF;\n}\n\nstatic bool __skip_sstep(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n\tif (auprobe->ops->emulate)\n\t\treturn auprobe->ops->emulate(auprobe, regs);\n\treturn false;\n}\n\nbool arch_uprobe_skip_sstep(struct arch_uprobe *auprobe, struct pt_regs *regs)\n{\n\tbool ret = __skip_sstep(auprobe, regs);\n\tif (ret && (regs->flags & X86_EFLAGS_TF))\n\t\tsend_sig(SIGTRAP, current, 0);\n\treturn ret;\n}\n\nunsigned long\narch_uretprobe_hijack_return_addr(unsigned long trampoline_vaddr, struct pt_regs *regs)\n{\n\tint rasize = sizeof_long(regs), nleft;\n\tunsigned long orig_ret_vaddr = 0;  \n\n\tif (copy_from_user(&orig_ret_vaddr, (void __user *)regs->sp, rasize))\n\t\treturn -1;\n\n\t \n\tif (orig_ret_vaddr == trampoline_vaddr)\n\t\treturn orig_ret_vaddr;\n\n\tnleft = copy_to_user((void __user *)regs->sp, &trampoline_vaddr, rasize);\n\tif (likely(!nleft))\n\t\treturn orig_ret_vaddr;\n\n\tif (nleft != rasize) {\n\t\tpr_err(\"return address clobbered: pid=%d, %%sp=%#lx, %%ip=%#lx\\n\",\n\t\t       current->pid, regs->sp, regs->ip);\n\n\t\tforce_sig(SIGSEGV);\n\t}\n\n\treturn -1;\n}\n\nbool arch_uretprobe_is_alive(struct return_instance *ret, enum rp_check ctx,\n\t\t\t\tstruct pt_regs *regs)\n{\n\tif (ctx == RP_CHECK_CALL)  \n\t\treturn regs->sp < ret->stack;\n\telse\n\t\treturn regs->sp <= ret->stack;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}