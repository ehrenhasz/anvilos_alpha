{
  "module_name": "tsc.c",
  "hash_id": "ecef172a0f2c97c1bacddb5fd745a9a33b1104b4b68ff5c3f62a4c770e8f669c",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/tsc.c",
  "human_readable_source": "\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/sched/clock.h>\n#include <linux/init.h>\n#include <linux/export.h>\n#include <linux/timer.h>\n#include <linux/acpi_pmtmr.h>\n#include <linux/cpufreq.h>\n#include <linux/delay.h>\n#include <linux/clocksource.h>\n#include <linux/percpu.h>\n#include <linux/timex.h>\n#include <linux/static_key.h>\n#include <linux/static_call.h>\n\n#include <asm/hpet.h>\n#include <asm/timer.h>\n#include <asm/vgtod.h>\n#include <asm/time.h>\n#include <asm/delay.h>\n#include <asm/hypervisor.h>\n#include <asm/nmi.h>\n#include <asm/x86_init.h>\n#include <asm/geode.h>\n#include <asm/apic.h>\n#include <asm/intel-family.h>\n#include <asm/i8259.h>\n#include <asm/uv/uv.h>\n\nunsigned int __read_mostly cpu_khz;\t \nEXPORT_SYMBOL(cpu_khz);\n\nunsigned int __read_mostly tsc_khz;\nEXPORT_SYMBOL(tsc_khz);\n\n#define KHZ\t1000\n\n \nstatic int __read_mostly tsc_unstable;\nstatic unsigned int __initdata tsc_early_khz;\n\nstatic DEFINE_STATIC_KEY_FALSE(__use_tsc);\n\nint tsc_clocksource_reliable;\n\nstatic int __read_mostly tsc_force_recalibrate;\n\nstatic u32 art_to_tsc_numerator;\nstatic u32 art_to_tsc_denominator;\nstatic u64 art_to_tsc_offset;\nstatic struct clocksource *art_related_clocksource;\n\nstruct cyc2ns {\n\tstruct cyc2ns_data data[2];\t \n\tseqcount_latch_t   seq;\t\t \n\n};  \n\nstatic DEFINE_PER_CPU_ALIGNED(struct cyc2ns, cyc2ns);\n\nstatic int __init tsc_early_khz_setup(char *buf)\n{\n\treturn kstrtouint(buf, 0, &tsc_early_khz);\n}\nearly_param(\"tsc_early_khz\", tsc_early_khz_setup);\n\n__always_inline void __cyc2ns_read(struct cyc2ns_data *data)\n{\n\tint seq, idx;\n\n\tdo {\n\t\tseq = this_cpu_read(cyc2ns.seq.seqcount.sequence);\n\t\tidx = seq & 1;\n\n\t\tdata->cyc2ns_offset = this_cpu_read(cyc2ns.data[idx].cyc2ns_offset);\n\t\tdata->cyc2ns_mul    = this_cpu_read(cyc2ns.data[idx].cyc2ns_mul);\n\t\tdata->cyc2ns_shift  = this_cpu_read(cyc2ns.data[idx].cyc2ns_shift);\n\n\t} while (unlikely(seq != this_cpu_read(cyc2ns.seq.seqcount.sequence)));\n}\n\n__always_inline void cyc2ns_read_begin(struct cyc2ns_data *data)\n{\n\tpreempt_disable_notrace();\n\t__cyc2ns_read(data);\n}\n\n__always_inline void cyc2ns_read_end(void)\n{\n\tpreempt_enable_notrace();\n}\n\n \n\nstatic __always_inline unsigned long long __cycles_2_ns(unsigned long long cyc)\n{\n\tstruct cyc2ns_data data;\n\tunsigned long long ns;\n\n\t__cyc2ns_read(&data);\n\n\tns = data.cyc2ns_offset;\n\tns += mul_u64_u32_shr(cyc, data.cyc2ns_mul, data.cyc2ns_shift);\n\n\treturn ns;\n}\n\nstatic __always_inline unsigned long long cycles_2_ns(unsigned long long cyc)\n{\n\tunsigned long long ns;\n\tpreempt_disable_notrace();\n\tns = __cycles_2_ns(cyc);\n\tpreempt_enable_notrace();\n\treturn ns;\n}\n\nstatic void __set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long tsc_now)\n{\n\tunsigned long long ns_now;\n\tstruct cyc2ns_data data;\n\tstruct cyc2ns *c2n;\n\n\tns_now = cycles_2_ns(tsc_now);\n\n\t \n\tclocks_calc_mult_shift(&data.cyc2ns_mul, &data.cyc2ns_shift, khz,\n\t\t\t       NSEC_PER_MSEC, 0);\n\n\t \n\tif (data.cyc2ns_shift == 32) {\n\t\tdata.cyc2ns_shift = 31;\n\t\tdata.cyc2ns_mul >>= 1;\n\t}\n\n\tdata.cyc2ns_offset = ns_now -\n\t\tmul_u64_u32_shr(tsc_now, data.cyc2ns_mul, data.cyc2ns_shift);\n\n\tc2n = per_cpu_ptr(&cyc2ns, cpu);\n\n\traw_write_seqcount_latch(&c2n->seq);\n\tc2n->data[0] = data;\n\traw_write_seqcount_latch(&c2n->seq);\n\tc2n->data[1] = data;\n}\n\nstatic void set_cyc2ns_scale(unsigned long khz, int cpu, unsigned long long tsc_now)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tsched_clock_idle_sleep_event();\n\n\tif (khz)\n\t\t__set_cyc2ns_scale(khz, cpu, tsc_now);\n\n\tsched_clock_idle_wakeup_event();\n\tlocal_irq_restore(flags);\n}\n\n \nstatic void __init cyc2ns_init_boot_cpu(void)\n{\n\tstruct cyc2ns *c2n = this_cpu_ptr(&cyc2ns);\n\n\tseqcount_latch_init(&c2n->seq);\n\t__set_cyc2ns_scale(tsc_khz, smp_processor_id(), rdtsc());\n}\n\n \nstatic void __init cyc2ns_init_secondary_cpus(void)\n{\n\tunsigned int cpu, this_cpu = smp_processor_id();\n\tstruct cyc2ns *c2n = this_cpu_ptr(&cyc2ns);\n\tstruct cyc2ns_data *data = c2n->data;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (cpu != this_cpu) {\n\t\t\tseqcount_latch_init(&c2n->seq);\n\t\t\tc2n = per_cpu_ptr(&cyc2ns, cpu);\n\t\t\tc2n->data[0] = data[0];\n\t\t\tc2n->data[1] = data[1];\n\t\t}\n\t}\n}\n\n \nnoinstr u64 native_sched_clock(void)\n{\n\tif (static_branch_likely(&__use_tsc)) {\n\t\tu64 tsc_now = rdtsc();\n\n\t\t \n\t\treturn __cycles_2_ns(tsc_now);\n\t}\n\n\t \n\n\t \n\treturn (jiffies_64 - INITIAL_JIFFIES) * (1000000000 / HZ);\n}\n\n \nu64 native_sched_clock_from_tsc(u64 tsc)\n{\n\treturn cycles_2_ns(tsc);\n}\n\n \n#ifdef CONFIG_PARAVIRT\nnoinstr u64 sched_clock_noinstr(void)\n{\n\treturn paravirt_sched_clock();\n}\n\nbool using_native_sched_clock(void)\n{\n\treturn static_call_query(pv_sched_clock) == native_sched_clock;\n}\n#else\nu64 sched_clock_noinstr(void) __attribute__((alias(\"native_sched_clock\")));\n\nbool using_native_sched_clock(void) { return true; }\n#endif\n\nnotrace u64 sched_clock(void)\n{\n\tu64 now;\n\tpreempt_disable_notrace();\n\tnow = sched_clock_noinstr();\n\tpreempt_enable_notrace();\n\treturn now;\n}\n\nint check_tsc_unstable(void)\n{\n\treturn tsc_unstable;\n}\nEXPORT_SYMBOL_GPL(check_tsc_unstable);\n\n#ifdef CONFIG_X86_TSC\nint __init notsc_setup(char *str)\n{\n\tmark_tsc_unstable(\"boot parameter notsc\");\n\treturn 1;\n}\n#else\n \nint __init notsc_setup(char *str)\n{\n\tsetup_clear_cpu_cap(X86_FEATURE_TSC);\n\treturn 1;\n}\n#endif\n\n__setup(\"notsc\", notsc_setup);\n\nstatic int no_sched_irq_time;\nstatic int no_tsc_watchdog;\nstatic int tsc_as_watchdog;\n\nstatic int __init tsc_setup(char *str)\n{\n\tif (!strcmp(str, \"reliable\"))\n\t\ttsc_clocksource_reliable = 1;\n\tif (!strncmp(str, \"noirqtime\", 9))\n\t\tno_sched_irq_time = 1;\n\tif (!strcmp(str, \"unstable\"))\n\t\tmark_tsc_unstable(\"boot parameter\");\n\tif (!strcmp(str, \"nowatchdog\")) {\n\t\tno_tsc_watchdog = 1;\n\t\tif (tsc_as_watchdog)\n\t\t\tpr_alert(\"%s: Overriding earlier tsc=watchdog with tsc=nowatchdog\\n\",\n\t\t\t\t __func__);\n\t\ttsc_as_watchdog = 0;\n\t}\n\tif (!strcmp(str, \"recalibrate\"))\n\t\ttsc_force_recalibrate = 1;\n\tif (!strcmp(str, \"watchdog\")) {\n\t\tif (no_tsc_watchdog)\n\t\t\tpr_alert(\"%s: tsc=watchdog overridden by earlier tsc=nowatchdog\\n\",\n\t\t\t\t __func__);\n\t\telse\n\t\t\ttsc_as_watchdog = 1;\n\t}\n\treturn 1;\n}\n\n__setup(\"tsc=\", tsc_setup);\n\n#define MAX_RETRIES\t\t5\n#define TSC_DEFAULT_THRESHOLD\t0x20000\n\n \nstatic u64 tsc_read_refs(u64 *p, int hpet)\n{\n\tu64 t1, t2;\n\tu64 thresh = tsc_khz ? tsc_khz >> 5 : TSC_DEFAULT_THRESHOLD;\n\tint i;\n\n\tfor (i = 0; i < MAX_RETRIES; i++) {\n\t\tt1 = get_cycles();\n\t\tif (hpet)\n\t\t\t*p = hpet_readl(HPET_COUNTER) & 0xFFFFFFFF;\n\t\telse\n\t\t\t*p = acpi_pm_read_early();\n\t\tt2 = get_cycles();\n\t\tif ((t2 - t1) < thresh)\n\t\t\treturn t2;\n\t}\n\treturn ULLONG_MAX;\n}\n\n \nstatic unsigned long calc_hpet_ref(u64 deltatsc, u64 hpet1, u64 hpet2)\n{\n\tu64 tmp;\n\n\tif (hpet2 < hpet1)\n\t\thpet2 += 0x100000000ULL;\n\thpet2 -= hpet1;\n\ttmp = ((u64)hpet2 * hpet_readl(HPET_PERIOD));\n\tdo_div(tmp, 1000000);\n\tdeltatsc = div64_u64(deltatsc, tmp);\n\n\treturn (unsigned long) deltatsc;\n}\n\n \nstatic unsigned long calc_pmtimer_ref(u64 deltatsc, u64 pm1, u64 pm2)\n{\n\tu64 tmp;\n\n\tif (!pm1 && !pm2)\n\t\treturn ULONG_MAX;\n\n\tif (pm2 < pm1)\n\t\tpm2 += (u64)ACPI_PM_OVRRUN;\n\tpm2 -= pm1;\n\ttmp = pm2 * 1000000000LL;\n\tdo_div(tmp, PMTMR_TICKS_PER_SEC);\n\tdo_div(deltatsc, tmp);\n\n\treturn (unsigned long) deltatsc;\n}\n\n#define CAL_MS\t\t10\n#define CAL_LATCH\t(PIT_TICK_RATE / (1000 / CAL_MS))\n#define CAL_PIT_LOOPS\t1000\n\n#define CAL2_MS\t\t50\n#define CAL2_LATCH\t(PIT_TICK_RATE / (1000 / CAL2_MS))\n#define CAL2_PIT_LOOPS\t5000\n\n\n \nstatic unsigned long pit_calibrate_tsc(u32 latch, unsigned long ms, int loopmin)\n{\n\tu64 tsc, t1, t2, delta;\n\tunsigned long tscmin, tscmax;\n\tint pitcnt;\n\n\tif (!has_legacy_pic()) {\n\t\t \n\t\tudelay(10 * USEC_PER_MSEC);\n\t\tudelay(10 * USEC_PER_MSEC);\n\t\tudelay(10 * USEC_PER_MSEC);\n\t\tudelay(10 * USEC_PER_MSEC);\n\t\tudelay(10 * USEC_PER_MSEC);\n\t\treturn ULONG_MAX;\n\t}\n\n\t \n\toutb((inb(0x61) & ~0x02) | 0x01, 0x61);\n\n\t \n\toutb(0xb0, 0x43);\n\toutb(latch & 0xff, 0x42);\n\toutb(latch >> 8, 0x42);\n\n\ttsc = t1 = t2 = get_cycles();\n\n\tpitcnt = 0;\n\ttscmax = 0;\n\ttscmin = ULONG_MAX;\n\twhile ((inb(0x61) & 0x20) == 0) {\n\t\tt2 = get_cycles();\n\t\tdelta = t2 - tsc;\n\t\ttsc = t2;\n\t\tif ((unsigned long) delta < tscmin)\n\t\t\ttscmin = (unsigned int) delta;\n\t\tif ((unsigned long) delta > tscmax)\n\t\t\ttscmax = (unsigned int) delta;\n\t\tpitcnt++;\n\t}\n\n\t \n\tif (pitcnt < loopmin || tscmax > 10 * tscmin)\n\t\treturn ULONG_MAX;\n\n\t \n\tdelta = t2 - t1;\n\tdo_div(delta, ms);\n\treturn delta;\n}\n\n \nstatic inline int pit_verify_msb(unsigned char val)\n{\n\t \n\tinb(0x42);\n\treturn inb(0x42) == val;\n}\n\nstatic inline int pit_expect_msb(unsigned char val, u64 *tscp, unsigned long *deltap)\n{\n\tint count;\n\tu64 tsc = 0, prev_tsc = 0;\n\n\tfor (count = 0; count < 50000; count++) {\n\t\tif (!pit_verify_msb(val))\n\t\t\tbreak;\n\t\tprev_tsc = tsc;\n\t\ttsc = get_cycles();\n\t}\n\t*deltap = get_cycles() - prev_tsc;\n\t*tscp = tsc;\n\n\t \n\treturn count > 5;\n}\n\n \n#define MAX_QUICK_PIT_MS 50\n#define MAX_QUICK_PIT_ITERATIONS (MAX_QUICK_PIT_MS * PIT_TICK_RATE / 1000 / 256)\n\nstatic unsigned long quick_pit_calibrate(void)\n{\n\tint i;\n\tu64 tsc, delta;\n\tunsigned long d1, d2;\n\n\tif (!has_legacy_pic())\n\t\treturn 0;\n\n\t \n\toutb((inb(0x61) & ~0x02) | 0x01, 0x61);\n\n\t \n\toutb(0xb0, 0x43);\n\n\t \n\toutb(0xff, 0x42);\n\toutb(0xff, 0x42);\n\n\t \n\tpit_verify_msb(0);\n\n\tif (pit_expect_msb(0xff, &tsc, &d1)) {\n\t\tfor (i = 1; i <= MAX_QUICK_PIT_ITERATIONS; i++) {\n\t\t\tif (!pit_expect_msb(0xff-i, &delta, &d2))\n\t\t\t\tbreak;\n\n\t\t\tdelta -= tsc;\n\n\t\t\t \n\t\t\tif (i == 1 &&\n\t\t\t    d1 + d2 >= (delta * MAX_QUICK_PIT_ITERATIONS) >> 11)\n\t\t\t\treturn 0;\n\n\t\t\t \n\t\t\tif (d1+d2 >= delta >> 11)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (!pit_verify_msb(0xfe - i))\n\t\t\t\tbreak;\n\t\t\tgoto success;\n\t\t}\n\t}\n\tpr_info(\"Fast TSC calibration failed\\n\");\n\treturn 0;\n\nsuccess:\n\t \n\tdelta *= PIT_TICK_RATE;\n\tdo_div(delta, i*256*1000);\n\tpr_info(\"Fast TSC calibration using PIT\\n\");\n\treturn delta;\n}\n\n \nunsigned long native_calibrate_tsc(void)\n{\n\tunsigned int eax_denominator, ebx_numerator, ecx_hz, edx;\n\tunsigned int crystal_khz;\n\n\tif (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL)\n\t\treturn 0;\n\n\tif (boot_cpu_data.cpuid_level < 0x15)\n\t\treturn 0;\n\n\teax_denominator = ebx_numerator = ecx_hz = edx = 0;\n\n\t \n\tcpuid(0x15, &eax_denominator, &ebx_numerator, &ecx_hz, &edx);\n\n\tif (ebx_numerator == 0 || eax_denominator == 0)\n\t\treturn 0;\n\n\tcrystal_khz = ecx_hz / 1000;\n\n\t \n\tif (crystal_khz == 0 &&\n\t\t\tboot_cpu_data.x86_model == INTEL_FAM6_ATOM_GOLDMONT_D)\n\t\tcrystal_khz = 25000;\n\n\t \n\tif (crystal_khz != 0)\n\t\tsetup_force_cpu_cap(X86_FEATURE_TSC_KNOWN_FREQ);\n\n\t \n\tif (crystal_khz == 0 && boot_cpu_data.cpuid_level >= 0x16) {\n\t\tunsigned int eax_base_mhz, ebx, ecx, edx;\n\n\t\tcpuid(0x16, &eax_base_mhz, &ebx, &ecx, &edx);\n\t\tcrystal_khz = eax_base_mhz * 1000 *\n\t\t\teax_denominator / ebx_numerator;\n\t}\n\n\tif (crystal_khz == 0)\n\t\treturn 0;\n\n\t \n\tif (boot_cpu_data.x86_model == INTEL_FAM6_ATOM_GOLDMONT)\n\t\tsetup_force_cpu_cap(X86_FEATURE_TSC_RELIABLE);\n\n#ifdef CONFIG_X86_LOCAL_APIC\n\t \n\tlapic_timer_period = crystal_khz * 1000 / HZ;\n#endif\n\n\treturn crystal_khz * ebx_numerator / eax_denominator;\n}\n\nstatic unsigned long cpu_khz_from_cpuid(void)\n{\n\tunsigned int eax_base_mhz, ebx_max_mhz, ecx_bus_mhz, edx;\n\n\tif (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL)\n\t\treturn 0;\n\n\tif (boot_cpu_data.cpuid_level < 0x16)\n\t\treturn 0;\n\n\teax_base_mhz = ebx_max_mhz = ecx_bus_mhz = edx = 0;\n\n\tcpuid(0x16, &eax_base_mhz, &ebx_max_mhz, &ecx_bus_mhz, &edx);\n\n\treturn eax_base_mhz * 1000;\n}\n\n \nstatic unsigned long pit_hpet_ptimer_calibrate_cpu(void)\n{\n\tu64 tsc1, tsc2, delta, ref1, ref2;\n\tunsigned long tsc_pit_min = ULONG_MAX, tsc_ref_min = ULONG_MAX;\n\tunsigned long flags, latch, ms;\n\tint hpet = is_hpet_enabled(), i, loopmin;\n\n\t \n\n\t \n\tlatch = CAL_LATCH;\n\tms = CAL_MS;\n\tloopmin = CAL_PIT_LOOPS;\n\n\tfor (i = 0; i < 3; i++) {\n\t\tunsigned long tsc_pit_khz;\n\n\t\t \n\t\tlocal_irq_save(flags);\n\t\ttsc1 = tsc_read_refs(&ref1, hpet);\n\t\ttsc_pit_khz = pit_calibrate_tsc(latch, ms, loopmin);\n\t\ttsc2 = tsc_read_refs(&ref2, hpet);\n\t\tlocal_irq_restore(flags);\n\n\t\t \n\t\ttsc_pit_min = min(tsc_pit_min, tsc_pit_khz);\n\n\t\t \n\t\tif (ref1 == ref2)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (tsc1 == ULLONG_MAX || tsc2 == ULLONG_MAX)\n\t\t\tcontinue;\n\n\t\ttsc2 = (tsc2 - tsc1) * 1000000LL;\n\t\tif (hpet)\n\t\t\ttsc2 = calc_hpet_ref(tsc2, ref1, ref2);\n\t\telse\n\t\t\ttsc2 = calc_pmtimer_ref(tsc2, ref1, ref2);\n\n\t\ttsc_ref_min = min(tsc_ref_min, (unsigned long) tsc2);\n\n\t\t \n\t\tdelta = ((u64) tsc_pit_min) * 100;\n\t\tdo_div(delta, tsc_ref_min);\n\n\t\t \n\t\tif (delta >= 90 && delta <= 110) {\n\t\t\tpr_info(\"PIT calibration matches %s. %d loops\\n\",\n\t\t\t\thpet ? \"HPET\" : \"PMTIMER\", i + 1);\n\t\t\treturn tsc_ref_min;\n\t\t}\n\n\t\t \n\t\tif (i == 1 && tsc_pit_min == ULONG_MAX) {\n\t\t\tlatch = CAL2_LATCH;\n\t\t\tms = CAL2_MS;\n\t\t\tloopmin = CAL2_PIT_LOOPS;\n\t\t}\n\t}\n\n\t \n\tif (tsc_pit_min == ULONG_MAX) {\n\t\t \n\t\tpr_warn(\"Unable to calibrate against PIT\\n\");\n\n\t\t \n\t\tif (!hpet && !ref1 && !ref2) {\n\t\t\tpr_notice(\"No reference (HPET/PMTIMER) available\\n\");\n\t\t\treturn 0;\n\t\t}\n\n\t\t \n\t\tif (tsc_ref_min == ULONG_MAX) {\n\t\t\tpr_warn(\"HPET/PMTIMER calibration failed\\n\");\n\t\t\treturn 0;\n\t\t}\n\n\t\t \n\t\tpr_info(\"using %s reference calibration\\n\",\n\t\t\thpet ? \"HPET\" : \"PMTIMER\");\n\n\t\treturn tsc_ref_min;\n\t}\n\n\t \n\tif (!hpet && !ref1 && !ref2) {\n\t\tpr_info(\"Using PIT calibration value\\n\");\n\t\treturn tsc_pit_min;\n\t}\n\n\t \n\tif (tsc_ref_min == ULONG_MAX) {\n\t\tpr_warn(\"HPET/PMTIMER calibration failed. Using PIT calibration.\\n\");\n\t\treturn tsc_pit_min;\n\t}\n\n\t \n\tpr_warn(\"PIT calibration deviates from %s: %lu %lu\\n\",\n\t\thpet ? \"HPET\" : \"PMTIMER\", tsc_pit_min, tsc_ref_min);\n\tpr_info(\"Using PIT calibration value\\n\");\n\treturn tsc_pit_min;\n}\n\n \nunsigned long native_calibrate_cpu_early(void)\n{\n\tunsigned long flags, fast_calibrate = cpu_khz_from_cpuid();\n\n\tif (!fast_calibrate)\n\t\tfast_calibrate = cpu_khz_from_msr();\n\tif (!fast_calibrate) {\n\t\tlocal_irq_save(flags);\n\t\tfast_calibrate = quick_pit_calibrate();\n\t\tlocal_irq_restore(flags);\n\t}\n\treturn fast_calibrate;\n}\n\n\n \nstatic unsigned long native_calibrate_cpu(void)\n{\n\tunsigned long tsc_freq = native_calibrate_cpu_early();\n\n\tif (!tsc_freq)\n\t\ttsc_freq = pit_hpet_ptimer_calibrate_cpu();\n\n\treturn tsc_freq;\n}\n\nvoid recalibrate_cpu_khz(void)\n{\n#ifndef CONFIG_SMP\n\tunsigned long cpu_khz_old = cpu_khz;\n\n\tif (!boot_cpu_has(X86_FEATURE_TSC))\n\t\treturn;\n\n\tcpu_khz = x86_platform.calibrate_cpu();\n\ttsc_khz = x86_platform.calibrate_tsc();\n\tif (tsc_khz == 0)\n\t\ttsc_khz = cpu_khz;\n\telse if (abs(cpu_khz - tsc_khz) * 10 > tsc_khz)\n\t\tcpu_khz = tsc_khz;\n\tcpu_data(0).loops_per_jiffy = cpufreq_scale(cpu_data(0).loops_per_jiffy,\n\t\t\t\t\t\t    cpu_khz_old, cpu_khz);\n#endif\n}\nEXPORT_SYMBOL_GPL(recalibrate_cpu_khz);\n\n\nstatic unsigned long long cyc2ns_suspend;\n\nvoid tsc_save_sched_clock_state(void)\n{\n\tif (!sched_clock_stable())\n\t\treturn;\n\n\tcyc2ns_suspend = sched_clock();\n}\n\n \nvoid tsc_restore_sched_clock_state(void)\n{\n\tunsigned long long offset;\n\tunsigned long flags;\n\tint cpu;\n\n\tif (!sched_clock_stable())\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\t \n\n\tthis_cpu_write(cyc2ns.data[0].cyc2ns_offset, 0);\n\tthis_cpu_write(cyc2ns.data[1].cyc2ns_offset, 0);\n\n\toffset = cyc2ns_suspend - sched_clock();\n\n\tfor_each_possible_cpu(cpu) {\n\t\tper_cpu(cyc2ns.data[0].cyc2ns_offset, cpu) = offset;\n\t\tper_cpu(cyc2ns.data[1].cyc2ns_offset, cpu) = offset;\n\t}\n\n\tlocal_irq_restore(flags);\n}\n\n#ifdef CONFIG_CPU_FREQ\n \n\nstatic unsigned int  ref_freq;\nstatic unsigned long loops_per_jiffy_ref;\nstatic unsigned long tsc_khz_ref;\n\nstatic int time_cpufreq_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t\tvoid *data)\n{\n\tstruct cpufreq_freqs *freq = data;\n\n\tif (num_online_cpus() > 1) {\n\t\tmark_tsc_unstable(\"cpufreq changes on SMP\");\n\t\treturn 0;\n\t}\n\n\tif (!ref_freq) {\n\t\tref_freq = freq->old;\n\t\tloops_per_jiffy_ref = boot_cpu_data.loops_per_jiffy;\n\t\ttsc_khz_ref = tsc_khz;\n\t}\n\n\tif ((val == CPUFREQ_PRECHANGE  && freq->old < freq->new) ||\n\t    (val == CPUFREQ_POSTCHANGE && freq->old > freq->new)) {\n\t\tboot_cpu_data.loops_per_jiffy =\n\t\t\tcpufreq_scale(loops_per_jiffy_ref, ref_freq, freq->new);\n\n\t\ttsc_khz = cpufreq_scale(tsc_khz_ref, ref_freq, freq->new);\n\t\tif (!(freq->flags & CPUFREQ_CONST_LOOPS))\n\t\t\tmark_tsc_unstable(\"cpufreq changes\");\n\n\t\tset_cyc2ns_scale(tsc_khz, freq->policy->cpu, rdtsc());\n\t}\n\n\treturn 0;\n}\n\nstatic struct notifier_block time_cpufreq_notifier_block = {\n\t.notifier_call  = time_cpufreq_notifier\n};\n\nstatic int __init cpufreq_register_tsc_scaling(void)\n{\n\tif (!boot_cpu_has(X86_FEATURE_TSC))\n\t\treturn 0;\n\tif (boot_cpu_has(X86_FEATURE_CONSTANT_TSC))\n\t\treturn 0;\n\tcpufreq_register_notifier(&time_cpufreq_notifier_block,\n\t\t\t\tCPUFREQ_TRANSITION_NOTIFIER);\n\treturn 0;\n}\n\ncore_initcall(cpufreq_register_tsc_scaling);\n\n#endif  \n\n#define ART_CPUID_LEAF (0x15)\n#define ART_MIN_DENOMINATOR (1)\n\n\n \nstatic void __init detect_art(void)\n{\n\tunsigned int unused[2];\n\n\tif (boot_cpu_data.cpuid_level < ART_CPUID_LEAF)\n\t\treturn;\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_HYPERVISOR) ||\n\t    !boot_cpu_has(X86_FEATURE_NONSTOP_TSC) ||\n\t    !boot_cpu_has(X86_FEATURE_TSC_ADJUST) ||\n\t    tsc_async_resets)\n\t\treturn;\n\n\tcpuid(ART_CPUID_LEAF, &art_to_tsc_denominator,\n\t      &art_to_tsc_numerator, unused, unused+1);\n\n\tif (art_to_tsc_denominator < ART_MIN_DENOMINATOR)\n\t\treturn;\n\n\trdmsrl(MSR_IA32_TSC_ADJUST, art_to_tsc_offset);\n\n\t \n\tsetup_force_cpu_cap(X86_FEATURE_ART);\n}\n\n\n \n\nstatic void tsc_resume(struct clocksource *cs)\n{\n\ttsc_verify_tsc_adjust(true);\n}\n\n \nstatic u64 read_tsc(struct clocksource *cs)\n{\n\treturn (u64)rdtsc_ordered();\n}\n\nstatic void tsc_cs_mark_unstable(struct clocksource *cs)\n{\n\tif (tsc_unstable)\n\t\treturn;\n\n\ttsc_unstable = 1;\n\tif (using_native_sched_clock())\n\t\tclear_sched_clock_stable();\n\tdisable_sched_clock_irqtime();\n\tpr_info(\"Marking TSC unstable due to clocksource watchdog\\n\");\n}\n\nstatic void tsc_cs_tick_stable(struct clocksource *cs)\n{\n\tif (tsc_unstable)\n\t\treturn;\n\n\tif (using_native_sched_clock())\n\t\tsched_clock_tick_stable();\n}\n\nstatic int tsc_cs_enable(struct clocksource *cs)\n{\n\tvclocks_set_used(VDSO_CLOCKMODE_TSC);\n\treturn 0;\n}\n\n \nstatic struct clocksource clocksource_tsc_early = {\n\t.name\t\t\t= \"tsc-early\",\n\t.rating\t\t\t= 299,\n\t.uncertainty_margin\t= 32 * NSEC_PER_MSEC,\n\t.read\t\t\t= read_tsc,\n\t.mask\t\t\t= CLOCKSOURCE_MASK(64),\n\t.flags\t\t\t= CLOCK_SOURCE_IS_CONTINUOUS |\n\t\t\t\t  CLOCK_SOURCE_MUST_VERIFY,\n\t.vdso_clock_mode\t= VDSO_CLOCKMODE_TSC,\n\t.enable\t\t\t= tsc_cs_enable,\n\t.resume\t\t\t= tsc_resume,\n\t.mark_unstable\t\t= tsc_cs_mark_unstable,\n\t.tick_stable\t\t= tsc_cs_tick_stable,\n\t.list\t\t\t= LIST_HEAD_INIT(clocksource_tsc_early.list),\n};\n\n \nstatic struct clocksource clocksource_tsc = {\n\t.name\t\t\t= \"tsc\",\n\t.rating\t\t\t= 300,\n\t.read\t\t\t= read_tsc,\n\t.mask\t\t\t= CLOCKSOURCE_MASK(64),\n\t.flags\t\t\t= CLOCK_SOURCE_IS_CONTINUOUS |\n\t\t\t\t  CLOCK_SOURCE_VALID_FOR_HRES |\n\t\t\t\t  CLOCK_SOURCE_MUST_VERIFY |\n\t\t\t\t  CLOCK_SOURCE_VERIFY_PERCPU,\n\t.vdso_clock_mode\t= VDSO_CLOCKMODE_TSC,\n\t.enable\t\t\t= tsc_cs_enable,\n\t.resume\t\t\t= tsc_resume,\n\t.mark_unstable\t\t= tsc_cs_mark_unstable,\n\t.tick_stable\t\t= tsc_cs_tick_stable,\n\t.list\t\t\t= LIST_HEAD_INIT(clocksource_tsc.list),\n};\n\nvoid mark_tsc_unstable(char *reason)\n{\n\tif (tsc_unstable)\n\t\treturn;\n\n\ttsc_unstable = 1;\n\tif (using_native_sched_clock())\n\t\tclear_sched_clock_stable();\n\tdisable_sched_clock_irqtime();\n\tpr_info(\"Marking TSC unstable due to %s\\n\", reason);\n\n\tclocksource_mark_unstable(&clocksource_tsc_early);\n\tclocksource_mark_unstable(&clocksource_tsc);\n}\n\nEXPORT_SYMBOL_GPL(mark_tsc_unstable);\n\nstatic void __init tsc_disable_clocksource_watchdog(void)\n{\n\tclocksource_tsc_early.flags &= ~CLOCK_SOURCE_MUST_VERIFY;\n\tclocksource_tsc.flags &= ~CLOCK_SOURCE_MUST_VERIFY;\n}\n\nbool tsc_clocksource_watchdog_disabled(void)\n{\n\treturn !(clocksource_tsc.flags & CLOCK_SOURCE_MUST_VERIFY) &&\n\t       tsc_as_watchdog && !no_tsc_watchdog;\n}\n\nstatic void __init check_system_tsc_reliable(void)\n{\n#if defined(CONFIG_MGEODEGX1) || defined(CONFIG_MGEODE_LX) || defined(CONFIG_X86_GENERIC)\n\tif (is_geode_lx()) {\n\t\t \n#define RTSC_SUSP 0x100\n\t\tunsigned long res_low, res_high;\n\n\t\trdmsr_safe(MSR_GEODE_BUSCONT_CONF0, &res_low, &res_high);\n\t\t \n\t\tif (res_low & RTSC_SUSP)\n\t\t\ttsc_clocksource_reliable = 1;\n\t}\n#endif\n\tif (boot_cpu_has(X86_FEATURE_TSC_RELIABLE))\n\t\ttsc_clocksource_reliable = 1;\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_CONSTANT_TSC) &&\n\t    boot_cpu_has(X86_FEATURE_NONSTOP_TSC) &&\n\t    boot_cpu_has(X86_FEATURE_TSC_ADJUST) &&\n\t    nr_online_nodes <= 4)\n\t\ttsc_disable_clocksource_watchdog();\n}\n\n \nint unsynchronized_tsc(void)\n{\n\tif (!boot_cpu_has(X86_FEATURE_TSC) || tsc_unstable)\n\t\treturn 1;\n\n#ifdef CONFIG_SMP\n\tif (apic_is_clustered_box())\n\t\treturn 1;\n#endif\n\n\tif (boot_cpu_has(X86_FEATURE_CONSTANT_TSC))\n\t\treturn 0;\n\n\tif (tsc_clocksource_reliable)\n\t\treturn 0;\n\t \n\tif (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL) {\n\t\t \n\t\tif (num_possible_cpus() > 1)\n\t\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n \nstruct system_counterval_t convert_art_to_tsc(u64 art)\n{\n\tu64 tmp, res, rem;\n\n\trem = do_div(art, art_to_tsc_denominator);\n\n\tres = art * art_to_tsc_numerator;\n\ttmp = rem * art_to_tsc_numerator;\n\n\tdo_div(tmp, art_to_tsc_denominator);\n\tres += tmp + art_to_tsc_offset;\n\n\treturn (struct system_counterval_t) {.cs = art_related_clocksource,\n\t\t\t.cycles = res};\n}\nEXPORT_SYMBOL(convert_art_to_tsc);\n\n \n\nstruct system_counterval_t convert_art_ns_to_tsc(u64 art_ns)\n{\n\tu64 tmp, res, rem;\n\n\trem = do_div(art_ns, USEC_PER_SEC);\n\n\tres = art_ns * tsc_khz;\n\ttmp = rem * tsc_khz;\n\n\tdo_div(tmp, USEC_PER_SEC);\n\tres += tmp;\n\n\treturn (struct system_counterval_t) { .cs = art_related_clocksource,\n\t\t\t\t\t      .cycles = res};\n}\nEXPORT_SYMBOL(convert_art_ns_to_tsc);\n\n\nstatic void tsc_refine_calibration_work(struct work_struct *work);\nstatic DECLARE_DELAYED_WORK(tsc_irqwork, tsc_refine_calibration_work);\n \nstatic void tsc_refine_calibration_work(struct work_struct *work)\n{\n\tstatic u64 tsc_start = ULLONG_MAX, ref_start;\n\tstatic int hpet;\n\tu64 tsc_stop, ref_stop, delta;\n\tunsigned long freq;\n\tint cpu;\n\n\t \n\tif (tsc_unstable)\n\t\tgoto unreg;\n\n\t \n\tif (tsc_start == ULLONG_MAX) {\nrestart:\n\t\t \n\t\thpet = is_hpet_enabled();\n\t\ttsc_start = tsc_read_refs(&ref_start, hpet);\n\t\tschedule_delayed_work(&tsc_irqwork, HZ);\n\t\treturn;\n\t}\n\n\ttsc_stop = tsc_read_refs(&ref_stop, hpet);\n\n\t \n\tif (ref_start == ref_stop)\n\t\tgoto out;\n\n\t \n\tif (tsc_stop == ULLONG_MAX)\n\t\tgoto restart;\n\n\tdelta = tsc_stop - tsc_start;\n\tdelta *= 1000000LL;\n\tif (hpet)\n\t\tfreq = calc_hpet_ref(delta, ref_start, ref_stop);\n\telse\n\t\tfreq = calc_pmtimer_ref(delta, ref_start, ref_stop);\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_TSC_KNOWN_FREQ)) {\n\n\t\t \n\t\tif (abs(tsc_khz - freq) > (tsc_khz >> 11)) {\n\t\t\tpr_warn(\"Warning: TSC freq calibrated by CPUID/MSR differs from what is calibrated by HW timer, please check with vendor!!\\n\");\n\t\t\tpr_info(\"Previous calibrated TSC freq:\\t %lu.%03lu MHz\\n\",\n\t\t\t\t(unsigned long)tsc_khz / 1000,\n\t\t\t\t(unsigned long)tsc_khz % 1000);\n\t\t}\n\n\t\tpr_info(\"TSC freq recalibrated by [%s]:\\t %lu.%03lu MHz\\n\",\n\t\t\thpet ? \"HPET\" : \"PM_TIMER\",\n\t\t\t(unsigned long)freq / 1000,\n\t\t\t(unsigned long)freq % 1000);\n\n\t\treturn;\n\t}\n\n\t \n\tif (abs(tsc_khz - freq) > tsc_khz/100)\n\t\tgoto out;\n\n\ttsc_khz = freq;\n\tpr_info(\"Refined TSC clocksource calibration: %lu.%03lu MHz\\n\",\n\t\t(unsigned long)tsc_khz / 1000,\n\t\t(unsigned long)tsc_khz % 1000);\n\n\t \n\tlapic_update_tsc_freq();\n\n\t \n\tfor_each_possible_cpu(cpu)\n\t\tset_cyc2ns_scale(tsc_khz, cpu, tsc_stop);\n\nout:\n\tif (tsc_unstable)\n\t\tgoto unreg;\n\n\tif (boot_cpu_has(X86_FEATURE_ART))\n\t\tart_related_clocksource = &clocksource_tsc;\n\tclocksource_register_khz(&clocksource_tsc, tsc_khz);\nunreg:\n\tclocksource_unregister(&clocksource_tsc_early);\n}\n\n\nstatic int __init init_tsc_clocksource(void)\n{\n\tif (!boot_cpu_has(X86_FEATURE_TSC) || !tsc_khz)\n\t\treturn 0;\n\n\tif (tsc_unstable) {\n\t\tclocksource_unregister(&clocksource_tsc_early);\n\t\treturn 0;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_NONSTOP_TSC_S3))\n\t\tclocksource_tsc.flags |= CLOCK_SOURCE_SUSPEND_NONSTOP;\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_TSC_KNOWN_FREQ)) {\n\t\tif (boot_cpu_has(X86_FEATURE_ART))\n\t\t\tart_related_clocksource = &clocksource_tsc;\n\t\tclocksource_register_khz(&clocksource_tsc, tsc_khz);\n\t\tclocksource_unregister(&clocksource_tsc_early);\n\n\t\tif (!tsc_force_recalibrate)\n\t\t\treturn 0;\n\t}\n\n\tschedule_delayed_work(&tsc_irqwork, 0);\n\treturn 0;\n}\n \ndevice_initcall(init_tsc_clocksource);\n\nstatic bool __init determine_cpu_tsc_frequencies(bool early)\n{\n\t \n\tWARN_ON(cpu_khz || tsc_khz);\n\n\tif (early) {\n\t\tcpu_khz = x86_platform.calibrate_cpu();\n\t\tif (tsc_early_khz)\n\t\t\ttsc_khz = tsc_early_khz;\n\t\telse\n\t\t\ttsc_khz = x86_platform.calibrate_tsc();\n\t} else {\n\t\t \n\t\tWARN_ON(x86_platform.calibrate_cpu != native_calibrate_cpu);\n\t\tcpu_khz = pit_hpet_ptimer_calibrate_cpu();\n\t}\n\n\t \n\tif (tsc_khz == 0)\n\t\ttsc_khz = cpu_khz;\n\telse if (abs(cpu_khz - tsc_khz) * 10 > tsc_khz)\n\t\tcpu_khz = tsc_khz;\n\n\tif (tsc_khz == 0)\n\t\treturn false;\n\n\tpr_info(\"Detected %lu.%03lu MHz processor\\n\",\n\t\t(unsigned long)cpu_khz / KHZ,\n\t\t(unsigned long)cpu_khz % KHZ);\n\n\tif (cpu_khz != tsc_khz) {\n\t\tpr_info(\"Detected %lu.%03lu MHz TSC\",\n\t\t\t(unsigned long)tsc_khz / KHZ,\n\t\t\t(unsigned long)tsc_khz % KHZ);\n\t}\n\treturn true;\n}\n\nstatic unsigned long __init get_loops_per_jiffy(void)\n{\n\tu64 lpj = (u64)tsc_khz * KHZ;\n\n\tdo_div(lpj, HZ);\n\treturn lpj;\n}\n\nstatic void __init tsc_enable_sched_clock(void)\n{\n\tloops_per_jiffy = get_loops_per_jiffy();\n\tuse_tsc_delay();\n\n\t \n\ttsc_store_and_check_tsc_adjust(true);\n\tcyc2ns_init_boot_cpu();\n\tstatic_branch_enable(&__use_tsc);\n}\n\nvoid __init tsc_early_init(void)\n{\n\tif (!boot_cpu_has(X86_FEATURE_TSC))\n\t\treturn;\n\t \n\tif (is_early_uv_system())\n\t\treturn;\n\tif (!determine_cpu_tsc_frequencies(true))\n\t\treturn;\n\ttsc_enable_sched_clock();\n}\n\nvoid __init tsc_init(void)\n{\n\tif (!cpu_feature_enabled(X86_FEATURE_TSC)) {\n\t\tsetup_clear_cpu_cap(X86_FEATURE_TSC_DEADLINE_TIMER);\n\t\treturn;\n\t}\n\n\t \n\tif (x86_platform.calibrate_cpu == native_calibrate_cpu_early)\n\t\tx86_platform.calibrate_cpu = native_calibrate_cpu;\n\n\tif (!tsc_khz) {\n\t\t \n\t\tif (!determine_cpu_tsc_frequencies(false)) {\n\t\t\tmark_tsc_unstable(\"could not calculate TSC khz\");\n\t\t\tsetup_clear_cpu_cap(X86_FEATURE_TSC_DEADLINE_TIMER);\n\t\t\treturn;\n\t\t}\n\t\ttsc_enable_sched_clock();\n\t}\n\n\tcyc2ns_init_secondary_cpus();\n\n\tif (!no_sched_irq_time)\n\t\tenable_sched_clock_irqtime();\n\n\tlpj_fine = get_loops_per_jiffy();\n\n\tcheck_system_tsc_reliable();\n\n\tif (unsynchronized_tsc()) {\n\t\tmark_tsc_unstable(\"TSCs unsynchronized\");\n\t\treturn;\n\t}\n\n\tif (tsc_clocksource_reliable || no_tsc_watchdog)\n\t\ttsc_disable_clocksource_watchdog();\n\n\tclocksource_register_khz(&clocksource_tsc_early, tsc_khz);\n\tdetect_art();\n}\n\n#ifdef CONFIG_SMP\n \nunsigned long calibrate_delay_is_known(void)\n{\n\tint sibling, cpu = smp_processor_id();\n\tint constant_tsc = cpu_has(&cpu_data(cpu), X86_FEATURE_CONSTANT_TSC);\n\tconst struct cpumask *mask = topology_core_cpumask(cpu);\n\n\t \n\tif (constant_tsc && !tsc_unstable)\n\t\treturn cpu_data(0).loops_per_jiffy;\n\n\t \n\tif (!constant_tsc || !mask)\n\t\treturn 0;\n\n\tsibling = cpumask_any_but(mask, cpu);\n\tif (sibling < nr_cpu_ids)\n\t\treturn cpu_data(sibling).loops_per_jiffy;\n\treturn 0;\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}