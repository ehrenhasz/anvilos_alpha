{
  "module_name": "main.c",
  "hash_id": "47d4dbaf17091e6b196229fdf1c134728bbecbfd687ef42db3b1944c9cfa6bbc",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/cpu/sgx/main.c",
  "human_readable_source": "\n \n\n#include <linux/file.h>\n#include <linux/freezer.h>\n#include <linux/highmem.h>\n#include <linux/kthread.h>\n#include <linux/miscdevice.h>\n#include <linux/node.h>\n#include <linux/pagemap.h>\n#include <linux/ratelimit.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/signal.h>\n#include <linux/slab.h>\n#include <linux/sysfs.h>\n#include <asm/sgx.h>\n#include \"driver.h\"\n#include \"encl.h\"\n#include \"encls.h\"\n\nstruct sgx_epc_section sgx_epc_sections[SGX_MAX_EPC_SECTIONS];\nstatic int sgx_nr_epc_sections;\nstatic struct task_struct *ksgxd_tsk;\nstatic DECLARE_WAIT_QUEUE_HEAD(ksgxd_waitq);\nstatic DEFINE_XARRAY(sgx_epc_address_space);\n\n \nstatic LIST_HEAD(sgx_active_page_list);\nstatic DEFINE_SPINLOCK(sgx_reclaimer_lock);\n\nstatic atomic_long_t sgx_nr_free_pages = ATOMIC_LONG_INIT(0);\n\n \nstatic nodemask_t sgx_numa_mask;\n\n \nstatic struct sgx_numa_node *sgx_numa_nodes;\n\nstatic LIST_HEAD(sgx_dirty_page_list);\n\n \nstatic unsigned long __sgx_sanitize_pages(struct list_head *dirty_page_list)\n{\n\tunsigned long left_dirty = 0;\n\tstruct sgx_epc_page *page;\n\tLIST_HEAD(dirty);\n\tint ret;\n\n\t \n\twhile (!list_empty(dirty_page_list)) {\n\t\tif (kthread_should_stop())\n\t\t\treturn 0;\n\n\t\tpage = list_first_entry(dirty_page_list, struct sgx_epc_page, list);\n\n\t\t \n\t\tif (page->poison) {\n\t\t\tstruct sgx_epc_section *section = &sgx_epc_sections[page->section];\n\t\t\tstruct sgx_numa_node *node = section->node;\n\n\t\t\tspin_lock(&node->lock);\n\t\t\tlist_move(&page->list, &node->sgx_poison_page_list);\n\t\t\tspin_unlock(&node->lock);\n\n\t\t\tcontinue;\n\t\t}\n\n\t\tret = __eremove(sgx_get_epc_virt_addr(page));\n\t\tif (!ret) {\n\t\t\t \n\t\t\tlist_del(&page->list);\n\t\t\tsgx_free_epc_page(page);\n\t\t} else {\n\t\t\t \n\t\t\tlist_move_tail(&page->list, &dirty);\n\t\t\tleft_dirty++;\n\t\t}\n\n\t\tcond_resched();\n\t}\n\n\tlist_splice(&dirty, dirty_page_list);\n\treturn left_dirty;\n}\n\nstatic bool sgx_reclaimer_age(struct sgx_epc_page *epc_page)\n{\n\tstruct sgx_encl_page *page = epc_page->owner;\n\tstruct sgx_encl *encl = page->encl;\n\tstruct sgx_encl_mm *encl_mm;\n\tbool ret = true;\n\tint idx;\n\n\tidx = srcu_read_lock(&encl->srcu);\n\n\tlist_for_each_entry_rcu(encl_mm, &encl->mm_list, list) {\n\t\tif (!mmget_not_zero(encl_mm->mm))\n\t\t\tcontinue;\n\n\t\tmmap_read_lock(encl_mm->mm);\n\t\tret = !sgx_encl_test_and_clear_young(encl_mm->mm, page);\n\t\tmmap_read_unlock(encl_mm->mm);\n\n\t\tmmput_async(encl_mm->mm);\n\n\t\tif (!ret)\n\t\t\tbreak;\n\t}\n\n\tsrcu_read_unlock(&encl->srcu, idx);\n\n\tif (!ret)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void sgx_reclaimer_block(struct sgx_epc_page *epc_page)\n{\n\tstruct sgx_encl_page *page = epc_page->owner;\n\tunsigned long addr = page->desc & PAGE_MASK;\n\tstruct sgx_encl *encl = page->encl;\n\tint ret;\n\n\tsgx_zap_enclave_ptes(encl, addr);\n\n\tmutex_lock(&encl->lock);\n\n\tret = __eblock(sgx_get_epc_virt_addr(epc_page));\n\tif (encls_failed(ret))\n\t\tENCLS_WARN(ret, \"EBLOCK\");\n\n\tmutex_unlock(&encl->lock);\n}\n\nstatic int __sgx_encl_ewb(struct sgx_epc_page *epc_page, void *va_slot,\n\t\t\t  struct sgx_backing *backing)\n{\n\tstruct sgx_pageinfo pginfo;\n\tint ret;\n\n\tpginfo.addr = 0;\n\tpginfo.secs = 0;\n\n\tpginfo.contents = (unsigned long)kmap_local_page(backing->contents);\n\tpginfo.metadata = (unsigned long)kmap_local_page(backing->pcmd) +\n\t\t\t  backing->pcmd_offset;\n\n\tret = __ewb(&pginfo, sgx_get_epc_virt_addr(epc_page), va_slot);\n\tset_page_dirty(backing->pcmd);\n\tset_page_dirty(backing->contents);\n\n\tkunmap_local((void *)(unsigned long)(pginfo.metadata -\n\t\t\t\t\t      backing->pcmd_offset));\n\tkunmap_local((void *)(unsigned long)pginfo.contents);\n\n\treturn ret;\n}\n\nvoid sgx_ipi_cb(void *info)\n{\n}\n\n \nstatic void sgx_encl_ewb(struct sgx_epc_page *epc_page,\n\t\t\t struct sgx_backing *backing)\n{\n\tstruct sgx_encl_page *encl_page = epc_page->owner;\n\tstruct sgx_encl *encl = encl_page->encl;\n\tstruct sgx_va_page *va_page;\n\tunsigned int va_offset;\n\tvoid *va_slot;\n\tint ret;\n\n\tencl_page->desc &= ~SGX_ENCL_PAGE_BEING_RECLAIMED;\n\n\tva_page = list_first_entry(&encl->va_pages, struct sgx_va_page,\n\t\t\t\t   list);\n\tva_offset = sgx_alloc_va_slot(va_page);\n\tva_slot = sgx_get_epc_virt_addr(va_page->epc_page) + va_offset;\n\tif (sgx_va_page_full(va_page))\n\t\tlist_move_tail(&va_page->list, &encl->va_pages);\n\n\tret = __sgx_encl_ewb(epc_page, va_slot, backing);\n\tif (ret == SGX_NOT_TRACKED) {\n\t\tret = __etrack(sgx_get_epc_virt_addr(encl->secs.epc_page));\n\t\tif (ret) {\n\t\t\tif (encls_failed(ret))\n\t\t\t\tENCLS_WARN(ret, \"ETRACK\");\n\t\t}\n\n\t\tret = __sgx_encl_ewb(epc_page, va_slot, backing);\n\t\tif (ret == SGX_NOT_TRACKED) {\n\t\t\t \n\t\t\ton_each_cpu_mask(sgx_encl_cpumask(encl),\n\t\t\t\t\t sgx_ipi_cb, NULL, 1);\n\t\t\tret = __sgx_encl_ewb(epc_page, va_slot, backing);\n\t\t}\n\t}\n\n\tif (ret) {\n\t\tif (encls_failed(ret))\n\t\t\tENCLS_WARN(ret, \"EWB\");\n\n\t\tsgx_free_va_slot(va_page, va_offset);\n\t} else {\n\t\tencl_page->desc |= va_offset;\n\t\tencl_page->va_page = va_page;\n\t}\n}\n\nstatic void sgx_reclaimer_write(struct sgx_epc_page *epc_page,\n\t\t\t\tstruct sgx_backing *backing)\n{\n\tstruct sgx_encl_page *encl_page = epc_page->owner;\n\tstruct sgx_encl *encl = encl_page->encl;\n\tstruct sgx_backing secs_backing;\n\tint ret;\n\n\tmutex_lock(&encl->lock);\n\n\tsgx_encl_ewb(epc_page, backing);\n\tencl_page->epc_page = NULL;\n\tencl->secs_child_cnt--;\n\tsgx_encl_put_backing(backing);\n\n\tif (!encl->secs_child_cnt && test_bit(SGX_ENCL_INITIALIZED, &encl->flags)) {\n\t\tret = sgx_encl_alloc_backing(encl, PFN_DOWN(encl->size),\n\t\t\t\t\t   &secs_backing);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tsgx_encl_ewb(encl->secs.epc_page, &secs_backing);\n\n\t\tsgx_encl_free_epc_page(encl->secs.epc_page);\n\t\tencl->secs.epc_page = NULL;\n\n\t\tsgx_encl_put_backing(&secs_backing);\n\t}\n\nout:\n\tmutex_unlock(&encl->lock);\n}\n\n \nstatic void sgx_reclaim_pages(void)\n{\n\tstruct sgx_epc_page *chunk[SGX_NR_TO_SCAN];\n\tstruct sgx_backing backing[SGX_NR_TO_SCAN];\n\tstruct sgx_encl_page *encl_page;\n\tstruct sgx_epc_page *epc_page;\n\tpgoff_t page_index;\n\tint cnt = 0;\n\tint ret;\n\tint i;\n\n\tspin_lock(&sgx_reclaimer_lock);\n\tfor (i = 0; i < SGX_NR_TO_SCAN; i++) {\n\t\tif (list_empty(&sgx_active_page_list))\n\t\t\tbreak;\n\n\t\tepc_page = list_first_entry(&sgx_active_page_list,\n\t\t\t\t\t    struct sgx_epc_page, list);\n\t\tlist_del_init(&epc_page->list);\n\t\tencl_page = epc_page->owner;\n\n\t\tif (kref_get_unless_zero(&encl_page->encl->refcount) != 0)\n\t\t\tchunk[cnt++] = epc_page;\n\t\telse\n\t\t\t \n\t\t\tepc_page->flags &= ~SGX_EPC_PAGE_RECLAIMER_TRACKED;\n\t}\n\tspin_unlock(&sgx_reclaimer_lock);\n\n\tfor (i = 0; i < cnt; i++) {\n\t\tepc_page = chunk[i];\n\t\tencl_page = epc_page->owner;\n\n\t\tif (!sgx_reclaimer_age(epc_page))\n\t\t\tgoto skip;\n\n\t\tpage_index = PFN_DOWN(encl_page->desc - encl_page->encl->base);\n\n\t\tmutex_lock(&encl_page->encl->lock);\n\t\tret = sgx_encl_alloc_backing(encl_page->encl, page_index, &backing[i]);\n\t\tif (ret) {\n\t\t\tmutex_unlock(&encl_page->encl->lock);\n\t\t\tgoto skip;\n\t\t}\n\n\t\tencl_page->desc |= SGX_ENCL_PAGE_BEING_RECLAIMED;\n\t\tmutex_unlock(&encl_page->encl->lock);\n\t\tcontinue;\n\nskip:\n\t\tspin_lock(&sgx_reclaimer_lock);\n\t\tlist_add_tail(&epc_page->list, &sgx_active_page_list);\n\t\tspin_unlock(&sgx_reclaimer_lock);\n\n\t\tkref_put(&encl_page->encl->refcount, sgx_encl_release);\n\n\t\tchunk[i] = NULL;\n\t}\n\n\tfor (i = 0; i < cnt; i++) {\n\t\tepc_page = chunk[i];\n\t\tif (epc_page)\n\t\t\tsgx_reclaimer_block(epc_page);\n\t}\n\n\tfor (i = 0; i < cnt; i++) {\n\t\tepc_page = chunk[i];\n\t\tif (!epc_page)\n\t\t\tcontinue;\n\n\t\tencl_page = epc_page->owner;\n\t\tsgx_reclaimer_write(epc_page, &backing[i]);\n\n\t\tkref_put(&encl_page->encl->refcount, sgx_encl_release);\n\t\tepc_page->flags &= ~SGX_EPC_PAGE_RECLAIMER_TRACKED;\n\n\t\tsgx_free_epc_page(epc_page);\n\t}\n}\n\nstatic bool sgx_should_reclaim(unsigned long watermark)\n{\n\treturn atomic_long_read(&sgx_nr_free_pages) < watermark &&\n\t       !list_empty(&sgx_active_page_list);\n}\n\n \nvoid sgx_reclaim_direct(void)\n{\n\tif (sgx_should_reclaim(SGX_NR_LOW_PAGES))\n\t\tsgx_reclaim_pages();\n}\n\nstatic int ksgxd(void *p)\n{\n\tset_freezable();\n\n\t \n\t__sgx_sanitize_pages(&sgx_dirty_page_list);\n\tWARN_ON(__sgx_sanitize_pages(&sgx_dirty_page_list));\n\n\twhile (!kthread_should_stop()) {\n\t\tif (try_to_freeze())\n\t\t\tcontinue;\n\n\t\twait_event_freezable(ksgxd_waitq,\n\t\t\t\t     kthread_should_stop() ||\n\t\t\t\t     sgx_should_reclaim(SGX_NR_HIGH_PAGES));\n\n\t\tif (sgx_should_reclaim(SGX_NR_HIGH_PAGES))\n\t\t\tsgx_reclaim_pages();\n\n\t\tcond_resched();\n\t}\n\n\treturn 0;\n}\n\nstatic bool __init sgx_page_reclaimer_init(void)\n{\n\tstruct task_struct *tsk;\n\n\ttsk = kthread_run(ksgxd, NULL, \"ksgxd\");\n\tif (IS_ERR(tsk))\n\t\treturn false;\n\n\tksgxd_tsk = tsk;\n\n\treturn true;\n}\n\nbool current_is_ksgxd(void)\n{\n\treturn current == ksgxd_tsk;\n}\n\nstatic struct sgx_epc_page *__sgx_alloc_epc_page_from_node(int nid)\n{\n\tstruct sgx_numa_node *node = &sgx_numa_nodes[nid];\n\tstruct sgx_epc_page *page = NULL;\n\n\tspin_lock(&node->lock);\n\n\tif (list_empty(&node->free_page_list)) {\n\t\tspin_unlock(&node->lock);\n\t\treturn NULL;\n\t}\n\n\tpage = list_first_entry(&node->free_page_list, struct sgx_epc_page, list);\n\tlist_del_init(&page->list);\n\tpage->flags = 0;\n\n\tspin_unlock(&node->lock);\n\tatomic_long_dec(&sgx_nr_free_pages);\n\n\treturn page;\n}\n\n \nstruct sgx_epc_page *__sgx_alloc_epc_page(void)\n{\n\tstruct sgx_epc_page *page;\n\tint nid_of_current = numa_node_id();\n\tint nid = nid_of_current;\n\n\tif (node_isset(nid_of_current, sgx_numa_mask)) {\n\t\tpage = __sgx_alloc_epc_page_from_node(nid_of_current);\n\t\tif (page)\n\t\t\treturn page;\n\t}\n\n\t \n\twhile (true) {\n\t\tnid = next_node_in(nid, sgx_numa_mask);\n\t\tif (nid == nid_of_current)\n\t\t\tbreak;\n\n\t\tpage = __sgx_alloc_epc_page_from_node(nid);\n\t\tif (page)\n\t\t\treturn page;\n\t}\n\n\treturn ERR_PTR(-ENOMEM);\n}\n\n \nvoid sgx_mark_page_reclaimable(struct sgx_epc_page *page)\n{\n\tspin_lock(&sgx_reclaimer_lock);\n\tpage->flags |= SGX_EPC_PAGE_RECLAIMER_TRACKED;\n\tlist_add_tail(&page->list, &sgx_active_page_list);\n\tspin_unlock(&sgx_reclaimer_lock);\n}\n\n \nint sgx_unmark_page_reclaimable(struct sgx_epc_page *page)\n{\n\tspin_lock(&sgx_reclaimer_lock);\n\tif (page->flags & SGX_EPC_PAGE_RECLAIMER_TRACKED) {\n\t\t \n\t\tif (list_empty(&page->list)) {\n\t\t\tspin_unlock(&sgx_reclaimer_lock);\n\t\t\treturn -EBUSY;\n\t\t}\n\n\t\tlist_del(&page->list);\n\t\tpage->flags &= ~SGX_EPC_PAGE_RECLAIMER_TRACKED;\n\t}\n\tspin_unlock(&sgx_reclaimer_lock);\n\n\treturn 0;\n}\n\n \nstruct sgx_epc_page *sgx_alloc_epc_page(void *owner, bool reclaim)\n{\n\tstruct sgx_epc_page *page;\n\n\tfor ( ; ; ) {\n\t\tpage = __sgx_alloc_epc_page();\n\t\tif (!IS_ERR(page)) {\n\t\t\tpage->owner = owner;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (list_empty(&sgx_active_page_list))\n\t\t\treturn ERR_PTR(-ENOMEM);\n\n\t\tif (!reclaim) {\n\t\t\tpage = ERR_PTR(-EBUSY);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (signal_pending(current)) {\n\t\t\tpage = ERR_PTR(-ERESTARTSYS);\n\t\t\tbreak;\n\t\t}\n\n\t\tsgx_reclaim_pages();\n\t\tcond_resched();\n\t}\n\n\tif (sgx_should_reclaim(SGX_NR_LOW_PAGES))\n\t\twake_up(&ksgxd_waitq);\n\n\treturn page;\n}\n\n \nvoid sgx_free_epc_page(struct sgx_epc_page *page)\n{\n\tstruct sgx_epc_section *section = &sgx_epc_sections[page->section];\n\tstruct sgx_numa_node *node = section->node;\n\n\tspin_lock(&node->lock);\n\n\tpage->owner = NULL;\n\tif (page->poison)\n\t\tlist_add(&page->list, &node->sgx_poison_page_list);\n\telse\n\t\tlist_add_tail(&page->list, &node->free_page_list);\n\tpage->flags = SGX_EPC_PAGE_IS_FREE;\n\n\tspin_unlock(&node->lock);\n\tatomic_long_inc(&sgx_nr_free_pages);\n}\n\nstatic bool __init sgx_setup_epc_section(u64 phys_addr, u64 size,\n\t\t\t\t\t unsigned long index,\n\t\t\t\t\t struct sgx_epc_section *section)\n{\n\tunsigned long nr_pages = size >> PAGE_SHIFT;\n\tunsigned long i;\n\n\tsection->virt_addr = memremap(phys_addr, size, MEMREMAP_WB);\n\tif (!section->virt_addr)\n\t\treturn false;\n\n\tsection->pages = vmalloc(nr_pages * sizeof(struct sgx_epc_page));\n\tif (!section->pages) {\n\t\tmemunmap(section->virt_addr);\n\t\treturn false;\n\t}\n\n\tsection->phys_addr = phys_addr;\n\txa_store_range(&sgx_epc_address_space, section->phys_addr,\n\t\t       phys_addr + size - 1, section, GFP_KERNEL);\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tsection->pages[i].section = index;\n\t\tsection->pages[i].flags = 0;\n\t\tsection->pages[i].owner = NULL;\n\t\tsection->pages[i].poison = 0;\n\t\tlist_add_tail(&section->pages[i].list, &sgx_dirty_page_list);\n\t}\n\n\treturn true;\n}\n\nbool arch_is_platform_page(u64 paddr)\n{\n\treturn !!xa_load(&sgx_epc_address_space, paddr);\n}\nEXPORT_SYMBOL_GPL(arch_is_platform_page);\n\nstatic struct sgx_epc_page *sgx_paddr_to_page(u64 paddr)\n{\n\tstruct sgx_epc_section *section;\n\n\tsection = xa_load(&sgx_epc_address_space, paddr);\n\tif (!section)\n\t\treturn NULL;\n\n\treturn &section->pages[PFN_DOWN(paddr - section->phys_addr)];\n}\n\n \nint arch_memory_failure(unsigned long pfn, int flags)\n{\n\tstruct sgx_epc_page *page = sgx_paddr_to_page(pfn << PAGE_SHIFT);\n\tstruct sgx_epc_section *section;\n\tstruct sgx_numa_node *node;\n\n\t \n\tif (!page)\n\t\treturn -ENXIO;\n\n\t \n\tif (flags & MF_ACTION_REQUIRED)\n\t\tforce_sig(SIGBUS);\n\n\tsection = &sgx_epc_sections[page->section];\n\tnode = section->node;\n\n\tspin_lock(&node->lock);\n\n\t \n\tif (page->poison)\n\t\tgoto out;\n\n\tpage->poison = 1;\n\n\t \n\tif (page->flags & SGX_EPC_PAGE_IS_FREE) {\n\t\tlist_move(&page->list, &node->sgx_poison_page_list);\n\t\tgoto out;\n\t}\n\n\t \nout:\n\tspin_unlock(&node->lock);\n\treturn 0;\n}\n\n \nstatic inline u64 __init sgx_calc_section_metric(u64 low, u64 high)\n{\n\treturn (low & GENMASK_ULL(31, 12)) +\n\t       ((high & GENMASK_ULL(19, 0)) << 32);\n}\n\n#ifdef CONFIG_NUMA\nstatic ssize_t sgx_total_bytes_show(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn sysfs_emit(buf, \"%lu\\n\", sgx_numa_nodes[dev->id].size);\n}\nstatic DEVICE_ATTR_RO(sgx_total_bytes);\n\nstatic umode_t arch_node_attr_is_visible(struct kobject *kobj,\n\t\tstruct attribute *attr, int idx)\n{\n\t \n\tif (nodes_empty(sgx_numa_mask))\n\t\treturn 0;\n\n\treturn attr->mode;\n}\n\nstatic struct attribute *arch_node_dev_attrs[] = {\n\t&dev_attr_sgx_total_bytes.attr,\n\tNULL,\n};\n\nconst struct attribute_group arch_node_dev_group = {\n\t.name = \"x86\",\n\t.attrs = arch_node_dev_attrs,\n\t.is_visible = arch_node_attr_is_visible,\n};\n\nstatic void __init arch_update_sysfs_visibility(int nid)\n{\n\tstruct node *node = node_devices[nid];\n\tint ret;\n\n\tret = sysfs_update_group(&node->dev.kobj, &arch_node_dev_group);\n\n\tif (ret)\n\t\tpr_err(\"sysfs update failed (%d), files may be invisible\", ret);\n}\n#else  \nstatic void __init arch_update_sysfs_visibility(int nid) {}\n#endif\n\nstatic bool __init sgx_page_cache_init(void)\n{\n\tu32 eax, ebx, ecx, edx, type;\n\tu64 pa, size;\n\tint nid;\n\tint i;\n\n\tsgx_numa_nodes = kmalloc_array(num_possible_nodes(), sizeof(*sgx_numa_nodes), GFP_KERNEL);\n\tif (!sgx_numa_nodes)\n\t\treturn false;\n\n\tfor (i = 0; i < ARRAY_SIZE(sgx_epc_sections); i++) {\n\t\tcpuid_count(SGX_CPUID, i + SGX_CPUID_EPC, &eax, &ebx, &ecx, &edx);\n\n\t\ttype = eax & SGX_CPUID_EPC_MASK;\n\t\tif (type == SGX_CPUID_EPC_INVALID)\n\t\t\tbreak;\n\n\t\tif (type != SGX_CPUID_EPC_SECTION) {\n\t\t\tpr_err_once(\"Unknown EPC section type: %u\\n\", type);\n\t\t\tbreak;\n\t\t}\n\n\t\tpa   = sgx_calc_section_metric(eax, ebx);\n\t\tsize = sgx_calc_section_metric(ecx, edx);\n\n\t\tpr_info(\"EPC section 0x%llx-0x%llx\\n\", pa, pa + size - 1);\n\n\t\tif (!sgx_setup_epc_section(pa, size, i, &sgx_epc_sections[i])) {\n\t\t\tpr_err(\"No free memory for an EPC section\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t\tnid = numa_map_to_online_node(phys_to_target_node(pa));\n\t\tif (nid == NUMA_NO_NODE) {\n\t\t\t \n\t\t\tpr_warn(FW_BUG \"Unable to map EPC section to online node. Fallback to the NUMA node 0.\\n\");\n\t\t\tnid = 0;\n\t\t}\n\n\t\tif (!node_isset(nid, sgx_numa_mask)) {\n\t\t\tspin_lock_init(&sgx_numa_nodes[nid].lock);\n\t\t\tINIT_LIST_HEAD(&sgx_numa_nodes[nid].free_page_list);\n\t\t\tINIT_LIST_HEAD(&sgx_numa_nodes[nid].sgx_poison_page_list);\n\t\t\tnode_set(nid, sgx_numa_mask);\n\t\t\tsgx_numa_nodes[nid].size = 0;\n\n\t\t\t \n\t\t\tarch_update_sysfs_visibility(nid);\n\t\t}\n\n\t\tsgx_epc_sections[i].node =  &sgx_numa_nodes[nid];\n\t\tsgx_numa_nodes[nid].size += size;\n\n\t\tsgx_nr_epc_sections++;\n\t}\n\n\tif (!sgx_nr_epc_sections) {\n\t\tpr_err(\"There are zero EPC sections.\\n\");\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nvoid sgx_update_lepubkeyhash(u64 *lepubkeyhash)\n{\n\tint i;\n\n\tWARN_ON_ONCE(preemptible());\n\n\tfor (i = 0; i < 4; i++)\n\t\twrmsrl(MSR_IA32_SGXLEPUBKEYHASH0 + i, lepubkeyhash[i]);\n}\n\nconst struct file_operations sgx_provision_fops = {\n\t.owner\t\t\t= THIS_MODULE,\n};\n\nstatic struct miscdevice sgx_dev_provision = {\n\t.minor = MISC_DYNAMIC_MINOR,\n\t.name = \"sgx_provision\",\n\t.nodename = \"sgx_provision\",\n\t.fops = &sgx_provision_fops,\n};\n\n \nint sgx_set_attribute(unsigned long *allowed_attributes,\n\t\t      unsigned int attribute_fd)\n{\n\tstruct fd f = fdget(attribute_fd);\n\n\tif (!f.file)\n\t\treturn -EINVAL;\n\n\tif (f.file->f_op != &sgx_provision_fops) {\n\t\tfdput(f);\n\t\treturn -EINVAL;\n\t}\n\n\t*allowed_attributes |= SGX_ATTR_PROVISIONKEY;\n\n\tfdput(f);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(sgx_set_attribute);\n\nstatic int __init sgx_init(void)\n{\n\tint ret;\n\tint i;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_SGX))\n\t\treturn -ENODEV;\n\n\tif (!sgx_page_cache_init())\n\t\treturn -ENOMEM;\n\n\tif (!sgx_page_reclaimer_init()) {\n\t\tret = -ENOMEM;\n\t\tgoto err_page_cache;\n\t}\n\n\tret = misc_register(&sgx_dev_provision);\n\tif (ret)\n\t\tgoto err_kthread;\n\n\t \n\tret = sgx_drv_init();\n\n\tif (sgx_vepc_init() && ret)\n\t\tgoto err_provision;\n\n\treturn 0;\n\nerr_provision:\n\tmisc_deregister(&sgx_dev_provision);\n\nerr_kthread:\n\tkthread_stop(ksgxd_tsk);\n\nerr_page_cache:\n\tfor (i = 0; i < sgx_nr_epc_sections; i++) {\n\t\tvfree(sgx_epc_sections[i].pages);\n\t\tmemunmap(sgx_epc_sections[i].virt_addr);\n\t}\n\n\treturn ret;\n}\n\ndevice_initcall(sgx_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}