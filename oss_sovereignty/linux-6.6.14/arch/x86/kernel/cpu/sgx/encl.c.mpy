{
  "module_name": "encl.c",
  "hash_id": "bf4970959aab308838ec06f08c866ad3dfe25b708f472b74ea16fa8198111994",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/cpu/sgx/encl.c",
  "human_readable_source": "\n \n\n#include <linux/lockdep.h>\n#include <linux/mm.h>\n#include <linux/mman.h>\n#include <linux/shmem_fs.h>\n#include <linux/suspend.h>\n#include <linux/sched/mm.h>\n#include <asm/sgx.h>\n#include \"encl.h\"\n#include \"encls.h\"\n#include \"sgx.h\"\n\nstatic int sgx_encl_lookup_backing(struct sgx_encl *encl, unsigned long page_index,\n\t\t\t    struct sgx_backing *backing);\n\n#define PCMDS_PER_PAGE (PAGE_SIZE / sizeof(struct sgx_pcmd))\n \n#define PCMD_FIRST_MASK GENMASK(4, 0)\n\n \nstatic int reclaimer_writing_to_pcmd(struct sgx_encl *encl,\n\t\t\t\t     unsigned long start_addr)\n{\n\tint reclaimed = 0;\n\tint i;\n\n\t \n\tBUILD_BUG_ON(PCMDS_PER_PAGE != 32);\n\n\tfor (i = 0; i < PCMDS_PER_PAGE; i++) {\n\t\tstruct sgx_encl_page *entry;\n\t\tunsigned long addr;\n\n\t\taddr = start_addr + i * PAGE_SIZE;\n\n\t\t \n\t\tif (addr == encl->base + encl->size)\n\t\t\tbreak;\n\n\t\tentry = xa_load(&encl->page_array, PFN_DOWN(addr));\n\t\tif (!entry)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (entry->epc_page &&\n\t\t    (entry->desc & SGX_ENCL_PAGE_BEING_RECLAIMED)) {\n\t\t\treclaimed = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn reclaimed;\n}\n\n \nstatic inline pgoff_t sgx_encl_get_backing_page_pcmd_offset(struct sgx_encl *encl,\n\t\t\t\t\t\t\t    unsigned long page_index)\n{\n\tpgoff_t epc_end_off = encl->size + sizeof(struct sgx_secs);\n\n\treturn epc_end_off + page_index * sizeof(struct sgx_pcmd);\n}\n\n \nstatic inline void sgx_encl_truncate_backing_page(struct sgx_encl *encl, unsigned long page_index)\n{\n\tstruct inode *inode = file_inode(encl->backing);\n\n\tshmem_truncate_range(inode, PFN_PHYS(page_index), PFN_PHYS(page_index) + PAGE_SIZE - 1);\n}\n\n \nstatic int __sgx_encl_eldu(struct sgx_encl_page *encl_page,\n\t\t\t   struct sgx_epc_page *epc_page,\n\t\t\t   struct sgx_epc_page *secs_page)\n{\n\tunsigned long va_offset = encl_page->desc & SGX_ENCL_PAGE_VA_OFFSET_MASK;\n\tstruct sgx_encl *encl = encl_page->encl;\n\tpgoff_t page_index, page_pcmd_off;\n\tunsigned long pcmd_first_page;\n\tstruct sgx_pageinfo pginfo;\n\tstruct sgx_backing b;\n\tbool pcmd_page_empty;\n\tu8 *pcmd_page;\n\tint ret;\n\n\tif (secs_page)\n\t\tpage_index = PFN_DOWN(encl_page->desc - encl_page->encl->base);\n\telse\n\t\tpage_index = PFN_DOWN(encl->size);\n\n\t \n\tpcmd_first_page = PFN_PHYS(page_index & ~PCMD_FIRST_MASK) + encl->base;\n\n\tpage_pcmd_off = sgx_encl_get_backing_page_pcmd_offset(encl, page_index);\n\n\tret = sgx_encl_lookup_backing(encl, page_index, &b);\n\tif (ret)\n\t\treturn ret;\n\n\tpginfo.addr = encl_page->desc & PAGE_MASK;\n\tpginfo.contents = (unsigned long)kmap_local_page(b.contents);\n\tpcmd_page = kmap_local_page(b.pcmd);\n\tpginfo.metadata = (unsigned long)pcmd_page + b.pcmd_offset;\n\n\tif (secs_page)\n\t\tpginfo.secs = (u64)sgx_get_epc_virt_addr(secs_page);\n\telse\n\t\tpginfo.secs = 0;\n\n\tret = __eldu(&pginfo, sgx_get_epc_virt_addr(epc_page),\n\t\t     sgx_get_epc_virt_addr(encl_page->va_page->epc_page) + va_offset);\n\tif (ret) {\n\t\tif (encls_failed(ret))\n\t\t\tENCLS_WARN(ret, \"ELDU\");\n\n\t\tret = -EFAULT;\n\t}\n\n\tmemset(pcmd_page + b.pcmd_offset, 0, sizeof(struct sgx_pcmd));\n\tset_page_dirty(b.pcmd);\n\n\t \n\tpcmd_page_empty = !memchr_inv(pcmd_page, 0, PAGE_SIZE);\n\n\tkunmap_local(pcmd_page);\n\tkunmap_local((void *)(unsigned long)pginfo.contents);\n\n\tget_page(b.pcmd);\n\tsgx_encl_put_backing(&b);\n\n\tsgx_encl_truncate_backing_page(encl, page_index);\n\n\tif (pcmd_page_empty && !reclaimer_writing_to_pcmd(encl, pcmd_first_page)) {\n\t\tsgx_encl_truncate_backing_page(encl, PFN_DOWN(page_pcmd_off));\n\t\tpcmd_page = kmap_local_page(b.pcmd);\n\t\tif (memchr_inv(pcmd_page, 0, PAGE_SIZE))\n\t\t\tpr_warn(\"PCMD page not empty after truncate.\\n\");\n\t\tkunmap_local(pcmd_page);\n\t}\n\n\tput_page(b.pcmd);\n\n\treturn ret;\n}\n\nstatic struct sgx_epc_page *sgx_encl_eldu(struct sgx_encl_page *encl_page,\n\t\t\t\t\t  struct sgx_epc_page *secs_page)\n{\n\n\tunsigned long va_offset = encl_page->desc & SGX_ENCL_PAGE_VA_OFFSET_MASK;\n\tstruct sgx_encl *encl = encl_page->encl;\n\tstruct sgx_epc_page *epc_page;\n\tint ret;\n\n\tepc_page = sgx_alloc_epc_page(encl_page, false);\n\tif (IS_ERR(epc_page))\n\t\treturn epc_page;\n\n\tret = __sgx_encl_eldu(encl_page, epc_page, secs_page);\n\tif (ret) {\n\t\tsgx_encl_free_epc_page(epc_page);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tsgx_free_va_slot(encl_page->va_page, va_offset);\n\tlist_move(&encl_page->va_page->list, &encl->va_pages);\n\tencl_page->desc &= ~SGX_ENCL_PAGE_VA_OFFSET_MASK;\n\tencl_page->epc_page = epc_page;\n\n\treturn epc_page;\n}\n\n \nstatic struct sgx_epc_page *sgx_encl_load_secs(struct sgx_encl *encl)\n{\n\tstruct sgx_epc_page *epc_page = encl->secs.epc_page;\n\n\tif (!epc_page)\n\t\tepc_page = sgx_encl_eldu(&encl->secs, NULL);\n\n\treturn epc_page;\n}\n\nstatic struct sgx_encl_page *__sgx_encl_load_page(struct sgx_encl *encl,\n\t\t\t\t\t\t  struct sgx_encl_page *entry)\n{\n\tstruct sgx_epc_page *epc_page;\n\n\t \n\tif (entry->epc_page) {\n\t\tif (entry->desc & SGX_ENCL_PAGE_BEING_RECLAIMED)\n\t\t\treturn ERR_PTR(-EBUSY);\n\n\t\treturn entry;\n\t}\n\n\tepc_page = sgx_encl_load_secs(encl);\n\tif (IS_ERR(epc_page))\n\t\treturn ERR_CAST(epc_page);\n\n\tepc_page = sgx_encl_eldu(entry, encl->secs.epc_page);\n\tif (IS_ERR(epc_page))\n\t\treturn ERR_CAST(epc_page);\n\n\tencl->secs_child_cnt++;\n\tsgx_mark_page_reclaimable(entry->epc_page);\n\n\treturn entry;\n}\n\nstatic struct sgx_encl_page *sgx_encl_load_page_in_vma(struct sgx_encl *encl,\n\t\t\t\t\t\t       unsigned long addr,\n\t\t\t\t\t\t       unsigned long vm_flags)\n{\n\tunsigned long vm_prot_bits = vm_flags & VM_ACCESS_FLAGS;\n\tstruct sgx_encl_page *entry;\n\n\tentry = xa_load(&encl->page_array, PFN_DOWN(addr));\n\tif (!entry)\n\t\treturn ERR_PTR(-EFAULT);\n\n\t \n\tif ((entry->vm_max_prot_bits & vm_prot_bits) != vm_prot_bits)\n\t\treturn ERR_PTR(-EFAULT);\n\n\treturn __sgx_encl_load_page(encl, entry);\n}\n\nstruct sgx_encl_page *sgx_encl_load_page(struct sgx_encl *encl,\n\t\t\t\t\t unsigned long addr)\n{\n\tstruct sgx_encl_page *entry;\n\n\tentry = xa_load(&encl->page_array, PFN_DOWN(addr));\n\tif (!entry)\n\t\treturn ERR_PTR(-EFAULT);\n\n\treturn __sgx_encl_load_page(encl, entry);\n}\n\n \nstatic vm_fault_t sgx_encl_eaug_page(struct vm_area_struct *vma,\n\t\t\t\t     struct sgx_encl *encl, unsigned long addr)\n{\n\tvm_fault_t vmret = VM_FAULT_SIGBUS;\n\tstruct sgx_pageinfo pginfo = {0};\n\tstruct sgx_encl_page *encl_page;\n\tstruct sgx_epc_page *epc_page;\n\tstruct sgx_va_page *va_page;\n\tunsigned long phys_addr;\n\tu64 secinfo_flags;\n\tint ret;\n\n\tif (!test_bit(SGX_ENCL_INITIALIZED, &encl->flags))\n\t\treturn VM_FAULT_SIGBUS;\n\n\t \n\tsecinfo_flags = SGX_SECINFO_R | SGX_SECINFO_W | SGX_SECINFO_X;\n\tencl_page = sgx_encl_page_alloc(encl, addr - encl->base, secinfo_flags);\n\tif (IS_ERR(encl_page))\n\t\treturn VM_FAULT_OOM;\n\n\tmutex_lock(&encl->lock);\n\n\tepc_page = sgx_encl_load_secs(encl);\n\tif (IS_ERR(epc_page)) {\n\t\tif (PTR_ERR(epc_page) == -EBUSY)\n\t\t\tvmret = VM_FAULT_NOPAGE;\n\t\tgoto err_out_unlock;\n\t}\n\n\tepc_page = sgx_alloc_epc_page(encl_page, false);\n\tif (IS_ERR(epc_page)) {\n\t\tif (PTR_ERR(epc_page) == -EBUSY)\n\t\t\tvmret =  VM_FAULT_NOPAGE;\n\t\tgoto err_out_unlock;\n\t}\n\n\tva_page = sgx_encl_grow(encl, false);\n\tif (IS_ERR(va_page)) {\n\t\tif (PTR_ERR(va_page) == -EBUSY)\n\t\t\tvmret = VM_FAULT_NOPAGE;\n\t\tgoto err_out_epc;\n\t}\n\n\tif (va_page)\n\t\tlist_add(&va_page->list, &encl->va_pages);\n\n\tret = xa_insert(&encl->page_array, PFN_DOWN(encl_page->desc),\n\t\t\tencl_page, GFP_KERNEL);\n\t \n\tif (ret)\n\t\tgoto err_out_shrink;\n\n\tpginfo.secs = (unsigned long)sgx_get_epc_virt_addr(encl->secs.epc_page);\n\tpginfo.addr = encl_page->desc & PAGE_MASK;\n\tpginfo.metadata = 0;\n\n\tret = __eaug(&pginfo, sgx_get_epc_virt_addr(epc_page));\n\tif (ret)\n\t\tgoto err_out;\n\n\tencl_page->encl = encl;\n\tencl_page->epc_page = epc_page;\n\tencl_page->type = SGX_PAGE_TYPE_REG;\n\tencl->secs_child_cnt++;\n\n\tsgx_mark_page_reclaimable(encl_page->epc_page);\n\n\tphys_addr = sgx_get_epc_phys_addr(epc_page);\n\t \n\tvmret = vmf_insert_pfn(vma, addr, PFN_DOWN(phys_addr));\n\tif (vmret != VM_FAULT_NOPAGE) {\n\t\tmutex_unlock(&encl->lock);\n\t\treturn VM_FAULT_SIGBUS;\n\t}\n\tmutex_unlock(&encl->lock);\n\treturn VM_FAULT_NOPAGE;\n\nerr_out:\n\txa_erase(&encl->page_array, PFN_DOWN(encl_page->desc));\n\nerr_out_shrink:\n\tsgx_encl_shrink(encl, va_page);\nerr_out_epc:\n\tsgx_encl_free_epc_page(epc_page);\nerr_out_unlock:\n\tmutex_unlock(&encl->lock);\n\tkfree(encl_page);\n\n\treturn vmret;\n}\n\nstatic vm_fault_t sgx_vma_fault(struct vm_fault *vmf)\n{\n\tunsigned long addr = (unsigned long)vmf->address;\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct sgx_encl_page *entry;\n\tunsigned long phys_addr;\n\tstruct sgx_encl *encl;\n\tvm_fault_t ret;\n\n\tencl = vma->vm_private_data;\n\n\t \n\tif (unlikely(!encl))\n\t\treturn VM_FAULT_SIGBUS;\n\n\t \n\tif (cpu_feature_enabled(X86_FEATURE_SGX2) &&\n\t    (!xa_load(&encl->page_array, PFN_DOWN(addr))))\n\t\treturn sgx_encl_eaug_page(vma, encl, addr);\n\n\tmutex_lock(&encl->lock);\n\n\tentry = sgx_encl_load_page_in_vma(encl, addr, vma->vm_flags);\n\tif (IS_ERR(entry)) {\n\t\tmutex_unlock(&encl->lock);\n\n\t\tif (PTR_ERR(entry) == -EBUSY)\n\t\t\treturn VM_FAULT_NOPAGE;\n\n\t\treturn VM_FAULT_SIGBUS;\n\t}\n\n\tphys_addr = sgx_get_epc_phys_addr(entry->epc_page);\n\n\tret = vmf_insert_pfn(vma, addr, PFN_DOWN(phys_addr));\n\tif (ret != VM_FAULT_NOPAGE) {\n\t\tmutex_unlock(&encl->lock);\n\n\t\treturn VM_FAULT_SIGBUS;\n\t}\n\n\tsgx_encl_test_and_clear_young(vma->vm_mm, entry);\n\tmutex_unlock(&encl->lock);\n\n\treturn VM_FAULT_NOPAGE;\n}\n\nstatic void sgx_vma_open(struct vm_area_struct *vma)\n{\n\tstruct sgx_encl *encl = vma->vm_private_data;\n\n\t \n\tif (unlikely(!encl))\n\t\treturn;\n\n\tif (sgx_encl_mm_add(encl, vma->vm_mm))\n\t\tvma->vm_private_data = NULL;\n}\n\n\n \nint sgx_encl_may_map(struct sgx_encl *encl, unsigned long start,\n\t\t     unsigned long end, unsigned long vm_flags)\n{\n\tunsigned long vm_prot_bits = vm_flags & VM_ACCESS_FLAGS;\n\tstruct sgx_encl_page *page;\n\tunsigned long count = 0;\n\tint ret = 0;\n\n\tXA_STATE(xas, &encl->page_array, PFN_DOWN(start));\n\n\t \n\tif (test_bit(SGX_ENCL_INITIALIZED, &encl->flags) &&\n\t    (start < encl->base || end > encl->base + encl->size))\n\t\treturn -EACCES;\n\n\t \n\tif (current->personality & READ_IMPLIES_EXEC)\n\t\treturn -EACCES;\n\n\tmutex_lock(&encl->lock);\n\txas_lock(&xas);\n\txas_for_each(&xas, page, PFN_DOWN(end - 1)) {\n\t\tif (~page->vm_max_prot_bits & vm_prot_bits) {\n\t\t\tret = -EACCES;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (!(++count % XA_CHECK_SCHED)) {\n\t\t\txas_pause(&xas);\n\t\t\txas_unlock(&xas);\n\t\t\tmutex_unlock(&encl->lock);\n\n\t\t\tcond_resched();\n\n\t\t\tmutex_lock(&encl->lock);\n\t\t\txas_lock(&xas);\n\t\t}\n\t}\n\txas_unlock(&xas);\n\tmutex_unlock(&encl->lock);\n\n\treturn ret;\n}\n\nstatic int sgx_vma_mprotect(struct vm_area_struct *vma, unsigned long start,\n\t\t\t    unsigned long end, unsigned long newflags)\n{\n\treturn sgx_encl_may_map(vma->vm_private_data, start, end, newflags);\n}\n\nstatic int sgx_encl_debug_read(struct sgx_encl *encl, struct sgx_encl_page *page,\n\t\t\t       unsigned long addr, void *data)\n{\n\tunsigned long offset = addr & ~PAGE_MASK;\n\tint ret;\n\n\n\tret = __edbgrd(sgx_get_epc_virt_addr(page->epc_page) + offset, data);\n\tif (ret)\n\t\treturn -EIO;\n\n\treturn 0;\n}\n\nstatic int sgx_encl_debug_write(struct sgx_encl *encl, struct sgx_encl_page *page,\n\t\t\t\tunsigned long addr, void *data)\n{\n\tunsigned long offset = addr & ~PAGE_MASK;\n\tint ret;\n\n\tret = __edbgwr(sgx_get_epc_virt_addr(page->epc_page) + offset, data);\n\tif (ret)\n\t\treturn -EIO;\n\n\treturn 0;\n}\n\n \nstatic struct sgx_encl_page *sgx_encl_reserve_page(struct sgx_encl *encl,\n\t\t\t\t\t\t   unsigned long addr,\n\t\t\t\t\t\t   unsigned long vm_flags)\n{\n\tstruct sgx_encl_page *entry;\n\n\tfor ( ; ; ) {\n\t\tmutex_lock(&encl->lock);\n\n\t\tentry = sgx_encl_load_page_in_vma(encl, addr, vm_flags);\n\t\tif (PTR_ERR(entry) != -EBUSY)\n\t\t\tbreak;\n\n\t\tmutex_unlock(&encl->lock);\n\t}\n\n\tif (IS_ERR(entry))\n\t\tmutex_unlock(&encl->lock);\n\n\treturn entry;\n}\n\nstatic int sgx_vma_access(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t  void *buf, int len, int write)\n{\n\tstruct sgx_encl *encl = vma->vm_private_data;\n\tstruct sgx_encl_page *entry = NULL;\n\tchar data[sizeof(unsigned long)];\n\tunsigned long align;\n\tint offset;\n\tint cnt;\n\tint ret = 0;\n\tint i;\n\n\t \n\tif (!encl)\n\t\treturn -EFAULT;\n\n\tif (!test_bit(SGX_ENCL_DEBUG, &encl->flags))\n\t\treturn -EFAULT;\n\n\tfor (i = 0; i < len; i += cnt) {\n\t\tentry = sgx_encl_reserve_page(encl, (addr + i) & PAGE_MASK,\n\t\t\t\t\t      vma->vm_flags);\n\t\tif (IS_ERR(entry)) {\n\t\t\tret = PTR_ERR(entry);\n\t\t\tbreak;\n\t\t}\n\n\t\talign = ALIGN_DOWN(addr + i, sizeof(unsigned long));\n\t\toffset = (addr + i) & (sizeof(unsigned long) - 1);\n\t\tcnt = sizeof(unsigned long) - offset;\n\t\tcnt = min(cnt, len - i);\n\n\t\tret = sgx_encl_debug_read(encl, entry, align, data);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif (write) {\n\t\t\tmemcpy(data + offset, buf + i, cnt);\n\t\t\tret = sgx_encl_debug_write(encl, entry, align, data);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t} else {\n\t\t\tmemcpy(buf + i, data + offset, cnt);\n\t\t}\n\nout:\n\t\tmutex_unlock(&encl->lock);\n\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\treturn ret < 0 ? ret : i;\n}\n\nconst struct vm_operations_struct sgx_vm_ops = {\n\t.fault = sgx_vma_fault,\n\t.mprotect = sgx_vma_mprotect,\n\t.open = sgx_vma_open,\n\t.access = sgx_vma_access,\n};\n\n \nvoid sgx_encl_release(struct kref *ref)\n{\n\tstruct sgx_encl *encl = container_of(ref, struct sgx_encl, refcount);\n\tunsigned long max_page_index = PFN_DOWN(encl->base + encl->size - 1);\n\tstruct sgx_va_page *va_page;\n\tstruct sgx_encl_page *entry;\n\tunsigned long count = 0;\n\n\tXA_STATE(xas, &encl->page_array, PFN_DOWN(encl->base));\n\n\txas_lock(&xas);\n\txas_for_each(&xas, entry, max_page_index) {\n\t\tif (entry->epc_page) {\n\t\t\t \n\t\t\tif (sgx_unmark_page_reclaimable(entry->epc_page))\n\t\t\t\tcontinue;\n\n\t\t\tsgx_encl_free_epc_page(entry->epc_page);\n\t\t\tencl->secs_child_cnt--;\n\t\t\tentry->epc_page = NULL;\n\t\t}\n\n\t\tkfree(entry);\n\t\t \n\t\tif (!(++count % XA_CHECK_SCHED)) {\n\t\t\txas_pause(&xas);\n\t\t\txas_unlock(&xas);\n\n\t\t\tcond_resched();\n\n\t\t\txas_lock(&xas);\n\t\t}\n\t}\n\txas_unlock(&xas);\n\n\txa_destroy(&encl->page_array);\n\n\tif (!encl->secs_child_cnt && encl->secs.epc_page) {\n\t\tsgx_encl_free_epc_page(encl->secs.epc_page);\n\t\tencl->secs.epc_page = NULL;\n\t}\n\n\twhile (!list_empty(&encl->va_pages)) {\n\t\tva_page = list_first_entry(&encl->va_pages, struct sgx_va_page,\n\t\t\t\t\t   list);\n\t\tlist_del(&va_page->list);\n\t\tsgx_encl_free_epc_page(va_page->epc_page);\n\t\tkfree(va_page);\n\t}\n\n\tif (encl->backing)\n\t\tfput(encl->backing);\n\n\tcleanup_srcu_struct(&encl->srcu);\n\n\tWARN_ON_ONCE(!list_empty(&encl->mm_list));\n\n\t \n\tWARN_ON_ONCE(encl->secs_child_cnt);\n\tWARN_ON_ONCE(encl->secs.epc_page);\n\n\tkfree(encl);\n}\n\n \nstatic void sgx_mmu_notifier_release(struct mmu_notifier *mn,\n\t\t\t\t     struct mm_struct *mm)\n{\n\tstruct sgx_encl_mm *encl_mm = container_of(mn, struct sgx_encl_mm, mmu_notifier);\n\tstruct sgx_encl_mm *tmp = NULL;\n\tbool found = false;\n\n\t \n\tspin_lock(&encl_mm->encl->mm_lock);\n\tlist_for_each_entry(tmp, &encl_mm->encl->mm_list, list) {\n\t\tif (tmp == encl_mm) {\n\t\t\tlist_del_rcu(&encl_mm->list);\n\t\t\tfound = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&encl_mm->encl->mm_lock);\n\n\tif (found) {\n\t\tsynchronize_srcu(&encl_mm->encl->srcu);\n\t\tmmu_notifier_put(mn);\n\t}\n}\n\nstatic void sgx_mmu_notifier_free(struct mmu_notifier *mn)\n{\n\tstruct sgx_encl_mm *encl_mm = container_of(mn, struct sgx_encl_mm, mmu_notifier);\n\n\t \n\tkref_put(&encl_mm->encl->refcount, sgx_encl_release);\n\n\tkfree(encl_mm);\n}\n\nstatic const struct mmu_notifier_ops sgx_mmu_notifier_ops = {\n\t.release\t\t= sgx_mmu_notifier_release,\n\t.free_notifier\t\t= sgx_mmu_notifier_free,\n};\n\nstatic struct sgx_encl_mm *sgx_encl_find_mm(struct sgx_encl *encl,\n\t\t\t\t\t    struct mm_struct *mm)\n{\n\tstruct sgx_encl_mm *encl_mm = NULL;\n\tstruct sgx_encl_mm *tmp;\n\tint idx;\n\n\tidx = srcu_read_lock(&encl->srcu);\n\n\tlist_for_each_entry_rcu(tmp, &encl->mm_list, list) {\n\t\tif (tmp->mm == mm) {\n\t\t\tencl_mm = tmp;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tsrcu_read_unlock(&encl->srcu, idx);\n\n\treturn encl_mm;\n}\n\nint sgx_encl_mm_add(struct sgx_encl *encl, struct mm_struct *mm)\n{\n\tstruct sgx_encl_mm *encl_mm;\n\tint ret;\n\n\t \n\tmmap_assert_write_locked(mm);\n\n\t \n\tif (sgx_encl_find_mm(encl, mm))\n\t\treturn 0;\n\n\tencl_mm = kzalloc(sizeof(*encl_mm), GFP_KERNEL);\n\tif (!encl_mm)\n\t\treturn -ENOMEM;\n\n\t \n\tkref_get(&encl->refcount);\n\tencl_mm->encl = encl;\n\tencl_mm->mm = mm;\n\tencl_mm->mmu_notifier.ops = &sgx_mmu_notifier_ops;\n\n\tret = __mmu_notifier_register(&encl_mm->mmu_notifier, mm);\n\tif (ret) {\n\t\tkfree(encl_mm);\n\t\treturn ret;\n\t}\n\n\tspin_lock(&encl->mm_lock);\n\tlist_add_rcu(&encl_mm->list, &encl->mm_list);\n\t \n\tsmp_wmb();\n\tencl->mm_list_version++;\n\tspin_unlock(&encl->mm_lock);\n\n\treturn 0;\n}\n\n \nconst cpumask_t *sgx_encl_cpumask(struct sgx_encl *encl)\n{\n\tcpumask_t *cpumask = &encl->cpumask;\n\tstruct sgx_encl_mm *encl_mm;\n\tint idx;\n\n\tcpumask_clear(cpumask);\n\n\tidx = srcu_read_lock(&encl->srcu);\n\n\tlist_for_each_entry_rcu(encl_mm, &encl->mm_list, list) {\n\t\tif (!mmget_not_zero(encl_mm->mm))\n\t\t\tcontinue;\n\n\t\tcpumask_or(cpumask, cpumask, mm_cpumask(encl_mm->mm));\n\n\t\tmmput_async(encl_mm->mm);\n\t}\n\n\tsrcu_read_unlock(&encl->srcu, idx);\n\n\treturn cpumask;\n}\n\nstatic struct page *sgx_encl_get_backing_page(struct sgx_encl *encl,\n\t\t\t\t\t      pgoff_t index)\n{\n\tstruct address_space *mapping = encl->backing->f_mapping;\n\tgfp_t gfpmask = mapping_gfp_mask(mapping);\n\n\treturn shmem_read_mapping_page_gfp(mapping, index, gfpmask);\n}\n\n \nstatic int __sgx_encl_get_backing(struct sgx_encl *encl, unsigned long page_index,\n\t\t\t struct sgx_backing *backing)\n{\n\tpgoff_t page_pcmd_off = sgx_encl_get_backing_page_pcmd_offset(encl, page_index);\n\tstruct page *contents;\n\tstruct page *pcmd;\n\n\tcontents = sgx_encl_get_backing_page(encl, page_index);\n\tif (IS_ERR(contents))\n\t\treturn PTR_ERR(contents);\n\n\tpcmd = sgx_encl_get_backing_page(encl, PFN_DOWN(page_pcmd_off));\n\tif (IS_ERR(pcmd)) {\n\t\tput_page(contents);\n\t\treturn PTR_ERR(pcmd);\n\t}\n\n\tbacking->contents = contents;\n\tbacking->pcmd = pcmd;\n\tbacking->pcmd_offset = page_pcmd_off & (PAGE_SIZE - 1);\n\n\treturn 0;\n}\n\n \nstatic struct mem_cgroup *sgx_encl_get_mem_cgroup(struct sgx_encl *encl)\n{\n\tstruct mem_cgroup *memcg = NULL;\n\tstruct sgx_encl_mm *encl_mm;\n\tint idx;\n\n\t \n\tif (!current_is_ksgxd())\n\t\treturn get_mem_cgroup_from_mm(current->mm);\n\n\t \n\tidx = srcu_read_lock(&encl->srcu);\n\n\tlist_for_each_entry_rcu(encl_mm, &encl->mm_list, list) {\n\t\tif (!mmget_not_zero(encl_mm->mm))\n\t\t\tcontinue;\n\n\t\tmemcg = get_mem_cgroup_from_mm(encl_mm->mm);\n\n\t\tmmput_async(encl_mm->mm);\n\n\t\tbreak;\n\t}\n\n\tsrcu_read_unlock(&encl->srcu, idx);\n\n\t \n\tif (!memcg)\n\t\treturn get_mem_cgroup_from_mm(NULL);\n\n\treturn memcg;\n}\n\n \nint sgx_encl_alloc_backing(struct sgx_encl *encl, unsigned long page_index,\n\t\t\t   struct sgx_backing *backing)\n{\n\tstruct mem_cgroup *encl_memcg = sgx_encl_get_mem_cgroup(encl);\n\tstruct mem_cgroup *memcg = set_active_memcg(encl_memcg);\n\tint ret;\n\n\tret = __sgx_encl_get_backing(encl, page_index, backing);\n\n\tset_active_memcg(memcg);\n\tmem_cgroup_put(encl_memcg);\n\n\treturn ret;\n}\n\n \nstatic int sgx_encl_lookup_backing(struct sgx_encl *encl, unsigned long page_index,\n\t\t\t   struct sgx_backing *backing)\n{\n\treturn __sgx_encl_get_backing(encl, page_index, backing);\n}\n\n \nvoid sgx_encl_put_backing(struct sgx_backing *backing)\n{\n\tput_page(backing->pcmd);\n\tput_page(backing->contents);\n}\n\nstatic int sgx_encl_test_and_clear_young_cb(pte_t *ptep, unsigned long addr,\n\t\t\t\t\t    void *data)\n{\n\tpte_t pte;\n\tint ret;\n\n\tret = pte_young(*ptep);\n\tif (ret) {\n\t\tpte = pte_mkold(*ptep);\n\t\tset_pte_at((struct mm_struct *)data, addr, ptep, pte);\n\t}\n\n\treturn ret;\n}\n\n \nint sgx_encl_test_and_clear_young(struct mm_struct *mm,\n\t\t\t\t  struct sgx_encl_page *page)\n{\n\tunsigned long addr = page->desc & PAGE_MASK;\n\tstruct sgx_encl *encl = page->encl;\n\tstruct vm_area_struct *vma;\n\tint ret;\n\n\tret = sgx_encl_find(mm, addr, &vma);\n\tif (ret)\n\t\treturn 0;\n\n\tif (encl != vma->vm_private_data)\n\t\treturn 0;\n\n\tret = apply_to_page_range(vma->vm_mm, addr, PAGE_SIZE,\n\t\t\t\t  sgx_encl_test_and_clear_young_cb, vma->vm_mm);\n\tif (ret < 0)\n\t\treturn 0;\n\n\treturn ret;\n}\n\nstruct sgx_encl_page *sgx_encl_page_alloc(struct sgx_encl *encl,\n\t\t\t\t\t  unsigned long offset,\n\t\t\t\t\t  u64 secinfo_flags)\n{\n\tstruct sgx_encl_page *encl_page;\n\tunsigned long prot;\n\n\tencl_page = kzalloc(sizeof(*encl_page), GFP_KERNEL);\n\tif (!encl_page)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tencl_page->desc = encl->base + offset;\n\tencl_page->encl = encl;\n\n\tprot = _calc_vm_trans(secinfo_flags, SGX_SECINFO_R, PROT_READ)  |\n\t       _calc_vm_trans(secinfo_flags, SGX_SECINFO_W, PROT_WRITE) |\n\t       _calc_vm_trans(secinfo_flags, SGX_SECINFO_X, PROT_EXEC);\n\n\t \n\tif ((secinfo_flags & SGX_SECINFO_PAGE_TYPE_MASK) == SGX_SECINFO_TCS)\n\t\tprot |= PROT_READ | PROT_WRITE;\n\n\t \n\tencl_page->vm_max_prot_bits = calc_vm_prot_bits(prot, 0);\n\n\treturn encl_page;\n}\n\n \nvoid sgx_zap_enclave_ptes(struct sgx_encl *encl, unsigned long addr)\n{\n\tunsigned long mm_list_version;\n\tstruct sgx_encl_mm *encl_mm;\n\tstruct vm_area_struct *vma;\n\tint idx, ret;\n\n\tdo {\n\t\tmm_list_version = encl->mm_list_version;\n\n\t\t \n\t\tsmp_rmb();\n\n\t\tidx = srcu_read_lock(&encl->srcu);\n\n\t\tlist_for_each_entry_rcu(encl_mm, &encl->mm_list, list) {\n\t\t\tif (!mmget_not_zero(encl_mm->mm))\n\t\t\t\tcontinue;\n\n\t\t\tmmap_read_lock(encl_mm->mm);\n\n\t\t\tret = sgx_encl_find(encl_mm->mm, addr, &vma);\n\t\t\tif (!ret && encl == vma->vm_private_data)\n\t\t\t\tzap_vma_ptes(vma, addr, PAGE_SIZE);\n\n\t\t\tmmap_read_unlock(encl_mm->mm);\n\n\t\t\tmmput_async(encl_mm->mm);\n\t\t}\n\n\t\tsrcu_read_unlock(&encl->srcu, idx);\n\t} while (unlikely(encl->mm_list_version != mm_list_version));\n}\n\n \nstruct sgx_epc_page *sgx_alloc_va_page(bool reclaim)\n{\n\tstruct sgx_epc_page *epc_page;\n\tint ret;\n\n\tepc_page = sgx_alloc_epc_page(NULL, reclaim);\n\tif (IS_ERR(epc_page))\n\t\treturn ERR_CAST(epc_page);\n\n\tret = __epa(sgx_get_epc_virt_addr(epc_page));\n\tif (ret) {\n\t\tWARN_ONCE(1, \"EPA returned %d (0x%x)\", ret, ret);\n\t\tsgx_encl_free_epc_page(epc_page);\n\t\treturn ERR_PTR(-EFAULT);\n\t}\n\n\treturn epc_page;\n}\n\n \nunsigned int sgx_alloc_va_slot(struct sgx_va_page *va_page)\n{\n\tint slot = find_first_zero_bit(va_page->slots, SGX_VA_SLOT_COUNT);\n\n\tif (slot < SGX_VA_SLOT_COUNT)\n\t\tset_bit(slot, va_page->slots);\n\n\treturn slot << 3;\n}\n\n \nvoid sgx_free_va_slot(struct sgx_va_page *va_page, unsigned int offset)\n{\n\tclear_bit(offset >> 3, va_page->slots);\n}\n\n \nbool sgx_va_page_full(struct sgx_va_page *va_page)\n{\n\tint slot = find_first_zero_bit(va_page->slots, SGX_VA_SLOT_COUNT);\n\n\treturn slot == SGX_VA_SLOT_COUNT;\n}\n\n \nvoid sgx_encl_free_epc_page(struct sgx_epc_page *page)\n{\n\tint ret;\n\n\tWARN_ON_ONCE(page->flags & SGX_EPC_PAGE_RECLAIMER_TRACKED);\n\n\tret = __eremove(sgx_get_epc_virt_addr(page));\n\tif (WARN_ONCE(ret, EREMOVE_ERROR_MESSAGE, ret, ret))\n\t\treturn;\n\n\tsgx_free_epc_page(page);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}