{
  "module_name": "aperfmperf.c",
  "hash_id": "03794eb531be10266833a0f62d343b75d48a2cd793306b4d8aadf2a04796e4fc",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/cpu/aperfmperf.c",
  "human_readable_source": "\n \n#include <linux/cpufreq.h>\n#include <linux/delay.h>\n#include <linux/ktime.h>\n#include <linux/math64.h>\n#include <linux/percpu.h>\n#include <linux/rcupdate.h>\n#include <linux/sched/isolation.h>\n#include <linux/sched/topology.h>\n#include <linux/smp.h>\n#include <linux/syscore_ops.h>\n\n#include <asm/cpu.h>\n#include <asm/cpu_device_id.h>\n#include <asm/intel-family.h>\n\n#include \"cpu.h\"\n\nstruct aperfmperf {\n\tseqcount_t\tseq;\n\tunsigned long\tlast_update;\n\tu64\t\tacnt;\n\tu64\t\tmcnt;\n\tu64\t\taperf;\n\tu64\t\tmperf;\n};\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct aperfmperf, cpu_samples) = {\n\t.seq = SEQCNT_ZERO(cpu_samples.seq)\n};\n\nstatic void init_counter_refs(void)\n{\n\tu64 aperf, mperf;\n\n\trdmsrl(MSR_IA32_APERF, aperf);\n\trdmsrl(MSR_IA32_MPERF, mperf);\n\n\tthis_cpu_write(cpu_samples.aperf, aperf);\n\tthis_cpu_write(cpu_samples.mperf, mperf);\n}\n\n#if defined(CONFIG_X86_64) && defined(CONFIG_SMP)\n \n\nDEFINE_STATIC_KEY_FALSE(arch_scale_freq_key);\n\nstatic u64 arch_turbo_freq_ratio = SCHED_CAPACITY_SCALE;\nstatic u64 arch_max_freq_ratio = SCHED_CAPACITY_SCALE;\n\nvoid arch_set_max_freq_ratio(bool turbo_disabled)\n{\n\tarch_max_freq_ratio = turbo_disabled ? SCHED_CAPACITY_SCALE :\n\t\t\t\t\tarch_turbo_freq_ratio;\n}\nEXPORT_SYMBOL_GPL(arch_set_max_freq_ratio);\n\nstatic bool __init turbo_disabled(void)\n{\n\tu64 misc_en;\n\tint err;\n\n\terr = rdmsrl_safe(MSR_IA32_MISC_ENABLE, &misc_en);\n\tif (err)\n\t\treturn false;\n\n\treturn (misc_en & MSR_IA32_MISC_ENABLE_TURBO_DISABLE);\n}\n\nstatic bool __init slv_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq)\n{\n\tint err;\n\n\terr = rdmsrl_safe(MSR_ATOM_CORE_RATIOS, base_freq);\n\tif (err)\n\t\treturn false;\n\n\terr = rdmsrl_safe(MSR_ATOM_CORE_TURBO_RATIOS, turbo_freq);\n\tif (err)\n\t\treturn false;\n\n\t*base_freq = (*base_freq >> 16) & 0x3F;      \n\t*turbo_freq = *turbo_freq & 0x3F;            \n\n\treturn true;\n}\n\n#define X86_MATCH(model)\t\t\t\t\t\\\n\tX86_MATCH_VENDOR_FAM_MODEL_FEATURE(INTEL, 6,\t\t\\\n\t\tINTEL_FAM6_##model, X86_FEATURE_APERFMPERF, NULL)\n\nstatic const struct x86_cpu_id has_knl_turbo_ratio_limits[] __initconst = {\n\tX86_MATCH(XEON_PHI_KNL),\n\tX86_MATCH(XEON_PHI_KNM),\n\t{}\n};\n\nstatic const struct x86_cpu_id has_skx_turbo_ratio_limits[] __initconst = {\n\tX86_MATCH(SKYLAKE_X),\n\t{}\n};\n\nstatic const struct x86_cpu_id has_glm_turbo_ratio_limits[] __initconst = {\n\tX86_MATCH(ATOM_GOLDMONT),\n\tX86_MATCH(ATOM_GOLDMONT_D),\n\tX86_MATCH(ATOM_GOLDMONT_PLUS),\n\t{}\n};\n\nstatic bool __init knl_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq,\n\t\t\t\t\t  int num_delta_fratio)\n{\n\tint fratio, delta_fratio, found;\n\tint err, i;\n\tu64 msr;\n\n\terr = rdmsrl_safe(MSR_PLATFORM_INFO, base_freq);\n\tif (err)\n\t\treturn false;\n\n\t*base_freq = (*base_freq >> 8) & 0xFF;\t     \n\n\terr = rdmsrl_safe(MSR_TURBO_RATIO_LIMIT, &msr);\n\tif (err)\n\t\treturn false;\n\n\tfratio = (msr >> 8) & 0xFF;\n\ti = 16;\n\tfound = 0;\n\tdo {\n\t\tif (found >= num_delta_fratio) {\n\t\t\t*turbo_freq = fratio;\n\t\t\treturn true;\n\t\t}\n\n\t\tdelta_fratio = (msr >> (i + 5)) & 0x7;\n\n\t\tif (delta_fratio) {\n\t\t\tfound += 1;\n\t\t\tfratio -= delta_fratio;\n\t\t}\n\n\t\ti += 8;\n\t} while (i < 64);\n\n\treturn true;\n}\n\nstatic bool __init skx_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq, int size)\n{\n\tu64 ratios, counts;\n\tu32 group_size;\n\tint err, i;\n\n\terr = rdmsrl_safe(MSR_PLATFORM_INFO, base_freq);\n\tif (err)\n\t\treturn false;\n\n\t*base_freq = (*base_freq >> 8) & 0xFF;       \n\n\terr = rdmsrl_safe(MSR_TURBO_RATIO_LIMIT, &ratios);\n\tif (err)\n\t\treturn false;\n\n\terr = rdmsrl_safe(MSR_TURBO_RATIO_LIMIT1, &counts);\n\tif (err)\n\t\treturn false;\n\n\tfor (i = 0; i < 64; i += 8) {\n\t\tgroup_size = (counts >> i) & 0xFF;\n\t\tif (group_size >= size) {\n\t\t\t*turbo_freq = (ratios >> i) & 0xFF;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic bool __init core_set_max_freq_ratio(u64 *base_freq, u64 *turbo_freq)\n{\n\tu64 msr;\n\tint err;\n\n\terr = rdmsrl_safe(MSR_PLATFORM_INFO, base_freq);\n\tif (err)\n\t\treturn false;\n\n\terr = rdmsrl_safe(MSR_TURBO_RATIO_LIMIT, &msr);\n\tif (err)\n\t\treturn false;\n\n\t*base_freq = (*base_freq >> 8) & 0xFF;     \n\t*turbo_freq = (msr >> 24) & 0xFF;          \n\n\t \n\tif (!*turbo_freq)\n\t\t*turbo_freq = msr & 0xFF;          \n\n\treturn true;\n}\n\nstatic bool __init intel_set_max_freq_ratio(void)\n{\n\tu64 base_freq, turbo_freq;\n\tu64 turbo_ratio;\n\n\tif (slv_set_max_freq_ratio(&base_freq, &turbo_freq))\n\t\tgoto out;\n\n\tif (x86_match_cpu(has_glm_turbo_ratio_limits) &&\n\t    skx_set_max_freq_ratio(&base_freq, &turbo_freq, 1))\n\t\tgoto out;\n\n\tif (x86_match_cpu(has_knl_turbo_ratio_limits) &&\n\t    knl_set_max_freq_ratio(&base_freq, &turbo_freq, 1))\n\t\tgoto out;\n\n\tif (x86_match_cpu(has_skx_turbo_ratio_limits) &&\n\t    skx_set_max_freq_ratio(&base_freq, &turbo_freq, 4))\n\t\tgoto out;\n\n\tif (core_set_max_freq_ratio(&base_freq, &turbo_freq))\n\t\tgoto out;\n\n\treturn false;\n\nout:\n\t \n\tif (!base_freq || !turbo_freq) {\n\t\tpr_debug(\"Couldn't determine cpu base or turbo frequency, necessary for scale-invariant accounting.\\n\");\n\t\treturn false;\n\t}\n\n\tturbo_ratio = div_u64(turbo_freq * SCHED_CAPACITY_SCALE, base_freq);\n\tif (!turbo_ratio) {\n\t\tpr_debug(\"Non-zero turbo and base frequencies led to a 0 ratio.\\n\");\n\t\treturn false;\n\t}\n\n\tarch_turbo_freq_ratio = turbo_ratio;\n\tarch_set_max_freq_ratio(turbo_disabled());\n\n\treturn true;\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic struct syscore_ops freq_invariance_syscore_ops = {\n\t.resume = init_counter_refs,\n};\n\nstatic void register_freq_invariance_syscore_ops(void)\n{\n\tregister_syscore_ops(&freq_invariance_syscore_ops);\n}\n#else\nstatic inline void register_freq_invariance_syscore_ops(void) {}\n#endif\n\nstatic void freq_invariance_enable(void)\n{\n\tif (static_branch_unlikely(&arch_scale_freq_key)) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn;\n\t}\n\tstatic_branch_enable(&arch_scale_freq_key);\n\tregister_freq_invariance_syscore_ops();\n\tpr_info(\"Estimated ratio of average max frequency by base frequency (times 1024): %llu\\n\", arch_max_freq_ratio);\n}\n\nvoid freq_invariance_set_perf_ratio(u64 ratio, bool turbo_disabled)\n{\n\tarch_turbo_freq_ratio = ratio;\n\tarch_set_max_freq_ratio(turbo_disabled);\n\tfreq_invariance_enable();\n}\n\nstatic void __init bp_init_freq_invariance(void)\n{\n\tif (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL)\n\t\treturn;\n\n\tif (intel_set_max_freq_ratio())\n\t\tfreq_invariance_enable();\n}\n\nstatic void disable_freq_invariance_workfn(struct work_struct *work)\n{\n\tint cpu;\n\n\tstatic_branch_disable(&arch_scale_freq_key);\n\n\t \n\tfor_each_possible_cpu(cpu)\n\t\tper_cpu(arch_freq_scale, cpu) = SCHED_CAPACITY_SCALE;\n}\n\nstatic DECLARE_WORK(disable_freq_invariance_work,\n\t\t    disable_freq_invariance_workfn);\n\nDEFINE_PER_CPU(unsigned long, arch_freq_scale) = SCHED_CAPACITY_SCALE;\n\nstatic void scale_freq_tick(u64 acnt, u64 mcnt)\n{\n\tu64 freq_scale;\n\n\tif (!arch_scale_freq_invariant())\n\t\treturn;\n\n\tif (check_shl_overflow(acnt, 2*SCHED_CAPACITY_SHIFT, &acnt))\n\t\tgoto error;\n\n\tif (check_mul_overflow(mcnt, arch_max_freq_ratio, &mcnt) || !mcnt)\n\t\tgoto error;\n\n\tfreq_scale = div64_u64(acnt, mcnt);\n\tif (!freq_scale)\n\t\tgoto error;\n\n\tif (freq_scale > SCHED_CAPACITY_SCALE)\n\t\tfreq_scale = SCHED_CAPACITY_SCALE;\n\n\tthis_cpu_write(arch_freq_scale, freq_scale);\n\treturn;\n\nerror:\n\tpr_warn(\"Scheduler frequency invariance went wobbly, disabling!\\n\");\n\tschedule_work(&disable_freq_invariance_work);\n}\n#else\nstatic inline void bp_init_freq_invariance(void) { }\nstatic inline void scale_freq_tick(u64 acnt, u64 mcnt) { }\n#endif  \n\nvoid arch_scale_freq_tick(void)\n{\n\tstruct aperfmperf *s = this_cpu_ptr(&cpu_samples);\n\tu64 acnt, mcnt, aperf, mperf;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_APERFMPERF))\n\t\treturn;\n\n\trdmsrl(MSR_IA32_APERF, aperf);\n\trdmsrl(MSR_IA32_MPERF, mperf);\n\tacnt = aperf - s->aperf;\n\tmcnt = mperf - s->mperf;\n\n\ts->aperf = aperf;\n\ts->mperf = mperf;\n\n\traw_write_seqcount_begin(&s->seq);\n\ts->last_update = jiffies;\n\ts->acnt = acnt;\n\ts->mcnt = mcnt;\n\traw_write_seqcount_end(&s->seq);\n\n\tscale_freq_tick(acnt, mcnt);\n}\n\n \n#define MAX_SAMPLE_AGE\t((unsigned long)HZ / 50)\n\nunsigned int arch_freq_get_on_cpu(int cpu)\n{\n\tstruct aperfmperf *s = per_cpu_ptr(&cpu_samples, cpu);\n\tunsigned int seq, freq;\n\tunsigned long last;\n\tu64 acnt, mcnt;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_APERFMPERF))\n\t\tgoto fallback;\n\n\tdo {\n\t\tseq = raw_read_seqcount_begin(&s->seq);\n\t\tlast = s->last_update;\n\t\tacnt = s->acnt;\n\t\tmcnt = s->mcnt;\n\t} while (read_seqcount_retry(&s->seq, seq));\n\n\t \n\tif (!mcnt || (jiffies - last) > MAX_SAMPLE_AGE)\n\t\tgoto fallback;\n\n\treturn div64_u64((cpu_khz * acnt), mcnt);\n\nfallback:\n\tfreq = cpufreq_quick_get(cpu);\n\treturn freq ? freq : cpu_khz;\n}\n\nstatic int __init bp_init_aperfmperf(void)\n{\n\tif (!cpu_feature_enabled(X86_FEATURE_APERFMPERF))\n\t\treturn 0;\n\n\tinit_counter_refs();\n\tbp_init_freq_invariance();\n\treturn 0;\n}\nearly_initcall(bp_init_aperfmperf);\n\nvoid ap_init_aperfmperf(void)\n{\n\tif (cpu_feature_enabled(X86_FEATURE_APERFMPERF))\n\t\tinit_counter_refs();\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}