{
  "module_name": "bugs.c",
  "hash_id": "a8130fb71ca51a39e9c4721bc260023426943f13e91f9f18384d475bc2bb27ef",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/cpu/bugs.c",
  "human_readable_source": "\n \n#include <linux/init.h>\n#include <linux/cpu.h>\n#include <linux/module.h>\n#include <linux/nospec.h>\n#include <linux/prctl.h>\n#include <linux/sched/smt.h>\n#include <linux/pgtable.h>\n#include <linux/bpf.h>\n\n#include <asm/spec-ctrl.h>\n#include <asm/cmdline.h>\n#include <asm/bugs.h>\n#include <asm/processor.h>\n#include <asm/processor-flags.h>\n#include <asm/fpu/api.h>\n#include <asm/msr.h>\n#include <asm/vmx.h>\n#include <asm/paravirt.h>\n#include <asm/intel-family.h>\n#include <asm/e820/api.h>\n#include <asm/hypervisor.h>\n#include <asm/tlbflush.h>\n#include <asm/cpu.h>\n\n#include \"cpu.h\"\n\nstatic void __init spectre_v1_select_mitigation(void);\nstatic void __init spectre_v2_select_mitigation(void);\nstatic void __init retbleed_select_mitigation(void);\nstatic void __init spectre_v2_user_select_mitigation(void);\nstatic void __init ssb_select_mitigation(void);\nstatic void __init l1tf_select_mitigation(void);\nstatic void __init mds_select_mitigation(void);\nstatic void __init md_clear_update_mitigation(void);\nstatic void __init md_clear_select_mitigation(void);\nstatic void __init taa_select_mitigation(void);\nstatic void __init mmio_select_mitigation(void);\nstatic void __init srbds_select_mitigation(void);\nstatic void __init l1d_flush_select_mitigation(void);\nstatic void __init srso_select_mitigation(void);\nstatic void __init gds_select_mitigation(void);\n\n \nu64 x86_spec_ctrl_base;\nEXPORT_SYMBOL_GPL(x86_spec_ctrl_base);\n\n \nDEFINE_PER_CPU(u64, x86_spec_ctrl_current);\nEXPORT_SYMBOL_GPL(x86_spec_ctrl_current);\n\nu64 x86_pred_cmd __ro_after_init = PRED_CMD_IBPB;\nEXPORT_SYMBOL_GPL(x86_pred_cmd);\n\nstatic DEFINE_MUTEX(spec_ctrl_mutex);\n\nvoid (*x86_return_thunk)(void) __ro_after_init = &__x86_return_thunk;\n\n \nstatic void update_spec_ctrl(u64 val)\n{\n\tthis_cpu_write(x86_spec_ctrl_current, val);\n\twrmsrl(MSR_IA32_SPEC_CTRL, val);\n}\n\n \nvoid update_spec_ctrl_cond(u64 val)\n{\n\tif (this_cpu_read(x86_spec_ctrl_current) == val)\n\t\treturn;\n\n\tthis_cpu_write(x86_spec_ctrl_current, val);\n\n\t \n\tif (!cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS))\n\t\twrmsrl(MSR_IA32_SPEC_CTRL, val);\n}\n\nnoinstr u64 spec_ctrl_current(void)\n{\n\treturn this_cpu_read(x86_spec_ctrl_current);\n}\nEXPORT_SYMBOL_GPL(spec_ctrl_current);\n\n \nu64 __ro_after_init x86_amd_ls_cfg_base;\nu64 __ro_after_init x86_amd_ls_cfg_ssbd_mask;\n\n \nDEFINE_STATIC_KEY_FALSE(switch_to_cond_stibp);\n \nDEFINE_STATIC_KEY_FALSE(switch_mm_cond_ibpb);\n \nDEFINE_STATIC_KEY_FALSE(switch_mm_always_ibpb);\n\n \nDEFINE_STATIC_KEY_FALSE(mds_user_clear);\nEXPORT_SYMBOL_GPL(mds_user_clear);\n \nDEFINE_STATIC_KEY_FALSE(mds_idle_clear);\nEXPORT_SYMBOL_GPL(mds_idle_clear);\n\n \nDEFINE_STATIC_KEY_FALSE(switch_mm_cond_l1d_flush);\n\n \nDEFINE_STATIC_KEY_FALSE(mmio_stale_data_clear);\nEXPORT_SYMBOL_GPL(mmio_stale_data_clear);\n\nvoid __init cpu_select_mitigations(void)\n{\n\t \n\tif (cpu_feature_enabled(X86_FEATURE_MSR_SPEC_CTRL)) {\n\t\trdmsrl(MSR_IA32_SPEC_CTRL, x86_spec_ctrl_base);\n\n\t\t \n\t\tx86_spec_ctrl_base &= ~SPEC_CTRL_MITIGATIONS_MASK;\n\t}\n\n\t \n\tspectre_v1_select_mitigation();\n\tspectre_v2_select_mitigation();\n\t \n\tretbleed_select_mitigation();\n\t \n\tspectre_v2_user_select_mitigation();\n\tssb_select_mitigation();\n\tl1tf_select_mitigation();\n\tmd_clear_select_mitigation();\n\tsrbds_select_mitigation();\n\tl1d_flush_select_mitigation();\n\n\t \n\tsrso_select_mitigation();\n\tgds_select_mitigation();\n}\n\n \nvoid\nx86_virt_spec_ctrl(u64 guest_virt_spec_ctrl, bool setguest)\n{\n\tu64 guestval, hostval;\n\tstruct thread_info *ti = current_thread_info();\n\n\t \n\tif (!static_cpu_has(X86_FEATURE_LS_CFG_SSBD) &&\n\t    !static_cpu_has(X86_FEATURE_VIRT_SSBD))\n\t\treturn;\n\n\t \n\tif (static_cpu_has(X86_FEATURE_SPEC_STORE_BYPASS_DISABLE))\n\t\thostval = SPEC_CTRL_SSBD;\n\telse\n\t\thostval = ssbd_tif_to_spec_ctrl(ti->flags);\n\n\t \n\tguestval = guest_virt_spec_ctrl & SPEC_CTRL_SSBD;\n\n\tif (hostval != guestval) {\n\t\tunsigned long tif;\n\n\t\ttif = setguest ? ssbd_spec_ctrl_to_tif(guestval) :\n\t\t\t\t ssbd_spec_ctrl_to_tif(hostval);\n\n\t\tspeculation_ctrl_update(tif);\n\t}\n}\nEXPORT_SYMBOL_GPL(x86_virt_spec_ctrl);\n\nstatic void x86_amd_ssb_disable(void)\n{\n\tu64 msrval = x86_amd_ls_cfg_base | x86_amd_ls_cfg_ssbd_mask;\n\n\tif (boot_cpu_has(X86_FEATURE_VIRT_SSBD))\n\t\twrmsrl(MSR_AMD64_VIRT_SPEC_CTRL, SPEC_CTRL_SSBD);\n\telse if (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD))\n\t\twrmsrl(MSR_AMD64_LS_CFG, msrval);\n}\n\n#undef pr_fmt\n#define pr_fmt(fmt)\t\"MDS: \" fmt\n\n \nstatic enum mds_mitigations mds_mitigation __ro_after_init = MDS_MITIGATION_FULL;\nstatic bool mds_nosmt __ro_after_init = false;\n\nstatic const char * const mds_strings[] = {\n\t[MDS_MITIGATION_OFF]\t= \"Vulnerable\",\n\t[MDS_MITIGATION_FULL]\t= \"Mitigation: Clear CPU buffers\",\n\t[MDS_MITIGATION_VMWERV]\t= \"Vulnerable: Clear CPU buffers attempted, no microcode\",\n};\n\nstatic void __init mds_select_mitigation(void)\n{\n\tif (!boot_cpu_has_bug(X86_BUG_MDS) || cpu_mitigations_off()) {\n\t\tmds_mitigation = MDS_MITIGATION_OFF;\n\t\treturn;\n\t}\n\n\tif (mds_mitigation == MDS_MITIGATION_FULL) {\n\t\tif (!boot_cpu_has(X86_FEATURE_MD_CLEAR))\n\t\t\tmds_mitigation = MDS_MITIGATION_VMWERV;\n\n\t\tstatic_branch_enable(&mds_user_clear);\n\n\t\tif (!boot_cpu_has(X86_BUG_MSBDS_ONLY) &&\n\t\t    (mds_nosmt || cpu_mitigations_auto_nosmt()))\n\t\t\tcpu_smt_disable(false);\n\t}\n}\n\nstatic int __init mds_cmdline(char *str)\n{\n\tif (!boot_cpu_has_bug(X86_BUG_MDS))\n\t\treturn 0;\n\n\tif (!str)\n\t\treturn -EINVAL;\n\n\tif (!strcmp(str, \"off\"))\n\t\tmds_mitigation = MDS_MITIGATION_OFF;\n\telse if (!strcmp(str, \"full\"))\n\t\tmds_mitigation = MDS_MITIGATION_FULL;\n\telse if (!strcmp(str, \"full,nosmt\")) {\n\t\tmds_mitigation = MDS_MITIGATION_FULL;\n\t\tmds_nosmt = true;\n\t}\n\n\treturn 0;\n}\nearly_param(\"mds\", mds_cmdline);\n\n#undef pr_fmt\n#define pr_fmt(fmt)\t\"TAA: \" fmt\n\nenum taa_mitigations {\n\tTAA_MITIGATION_OFF,\n\tTAA_MITIGATION_UCODE_NEEDED,\n\tTAA_MITIGATION_VERW,\n\tTAA_MITIGATION_TSX_DISABLED,\n};\n\n \nstatic enum taa_mitigations taa_mitigation __ro_after_init = TAA_MITIGATION_VERW;\nstatic bool taa_nosmt __ro_after_init;\n\nstatic const char * const taa_strings[] = {\n\t[TAA_MITIGATION_OFF]\t\t= \"Vulnerable\",\n\t[TAA_MITIGATION_UCODE_NEEDED]\t= \"Vulnerable: Clear CPU buffers attempted, no microcode\",\n\t[TAA_MITIGATION_VERW]\t\t= \"Mitigation: Clear CPU buffers\",\n\t[TAA_MITIGATION_TSX_DISABLED]\t= \"Mitigation: TSX disabled\",\n};\n\nstatic void __init taa_select_mitigation(void)\n{\n\tu64 ia32_cap;\n\n\tif (!boot_cpu_has_bug(X86_BUG_TAA)) {\n\t\ttaa_mitigation = TAA_MITIGATION_OFF;\n\t\treturn;\n\t}\n\n\t \n\tif (!boot_cpu_has(X86_FEATURE_RTM)) {\n\t\ttaa_mitigation = TAA_MITIGATION_TSX_DISABLED;\n\t\treturn;\n\t}\n\n\tif (cpu_mitigations_off()) {\n\t\ttaa_mitigation = TAA_MITIGATION_OFF;\n\t\treturn;\n\t}\n\n\t \n\tif (taa_mitigation == TAA_MITIGATION_OFF &&\n\t    mds_mitigation == MDS_MITIGATION_OFF)\n\t\treturn;\n\n\tif (boot_cpu_has(X86_FEATURE_MD_CLEAR))\n\t\ttaa_mitigation = TAA_MITIGATION_VERW;\n\telse\n\t\ttaa_mitigation = TAA_MITIGATION_UCODE_NEEDED;\n\n\t \n\tia32_cap = x86_read_arch_cap_msr();\n\tif ( (ia32_cap & ARCH_CAP_MDS_NO) &&\n\t    !(ia32_cap & ARCH_CAP_TSX_CTRL_MSR))\n\t\ttaa_mitigation = TAA_MITIGATION_UCODE_NEEDED;\n\n\t \n\tstatic_branch_enable(&mds_user_clear);\n\n\tif (taa_nosmt || cpu_mitigations_auto_nosmt())\n\t\tcpu_smt_disable(false);\n}\n\nstatic int __init tsx_async_abort_parse_cmdline(char *str)\n{\n\tif (!boot_cpu_has_bug(X86_BUG_TAA))\n\t\treturn 0;\n\n\tif (!str)\n\t\treturn -EINVAL;\n\n\tif (!strcmp(str, \"off\")) {\n\t\ttaa_mitigation = TAA_MITIGATION_OFF;\n\t} else if (!strcmp(str, \"full\")) {\n\t\ttaa_mitigation = TAA_MITIGATION_VERW;\n\t} else if (!strcmp(str, \"full,nosmt\")) {\n\t\ttaa_mitigation = TAA_MITIGATION_VERW;\n\t\ttaa_nosmt = true;\n\t}\n\n\treturn 0;\n}\nearly_param(\"tsx_async_abort\", tsx_async_abort_parse_cmdline);\n\n#undef pr_fmt\n#define pr_fmt(fmt)\t\"MMIO Stale Data: \" fmt\n\nenum mmio_mitigations {\n\tMMIO_MITIGATION_OFF,\n\tMMIO_MITIGATION_UCODE_NEEDED,\n\tMMIO_MITIGATION_VERW,\n};\n\n \nstatic enum mmio_mitigations mmio_mitigation __ro_after_init = MMIO_MITIGATION_VERW;\nstatic bool mmio_nosmt __ro_after_init = false;\n\nstatic const char * const mmio_strings[] = {\n\t[MMIO_MITIGATION_OFF]\t\t= \"Vulnerable\",\n\t[MMIO_MITIGATION_UCODE_NEEDED]\t= \"Vulnerable: Clear CPU buffers attempted, no microcode\",\n\t[MMIO_MITIGATION_VERW]\t\t= \"Mitigation: Clear CPU buffers\",\n};\n\nstatic void __init mmio_select_mitigation(void)\n{\n\tu64 ia32_cap;\n\n\tif (!boot_cpu_has_bug(X86_BUG_MMIO_STALE_DATA) ||\n\t     boot_cpu_has_bug(X86_BUG_MMIO_UNKNOWN) ||\n\t     cpu_mitigations_off()) {\n\t\tmmio_mitigation = MMIO_MITIGATION_OFF;\n\t\treturn;\n\t}\n\n\tif (mmio_mitigation == MMIO_MITIGATION_OFF)\n\t\treturn;\n\n\tia32_cap = x86_read_arch_cap_msr();\n\n\t \n\tif (boot_cpu_has_bug(X86_BUG_MDS) || (boot_cpu_has_bug(X86_BUG_TAA) &&\n\t\t\t\t\t      boot_cpu_has(X86_FEATURE_RTM)))\n\t\tstatic_branch_enable(&mds_user_clear);\n\telse\n\t\tstatic_branch_enable(&mmio_stale_data_clear);\n\n\t \n\tif (!(ia32_cap & ARCH_CAP_FBSDP_NO))\n\t\tstatic_branch_enable(&mds_idle_clear);\n\n\t \n\tif ((ia32_cap & ARCH_CAP_FB_CLEAR) ||\n\t    (boot_cpu_has(X86_FEATURE_MD_CLEAR) &&\n\t     boot_cpu_has(X86_FEATURE_FLUSH_L1D) &&\n\t     !(ia32_cap & ARCH_CAP_MDS_NO)))\n\t\tmmio_mitigation = MMIO_MITIGATION_VERW;\n\telse\n\t\tmmio_mitigation = MMIO_MITIGATION_UCODE_NEEDED;\n\n\tif (mmio_nosmt || cpu_mitigations_auto_nosmt())\n\t\tcpu_smt_disable(false);\n}\n\nstatic int __init mmio_stale_data_parse_cmdline(char *str)\n{\n\tif (!boot_cpu_has_bug(X86_BUG_MMIO_STALE_DATA))\n\t\treturn 0;\n\n\tif (!str)\n\t\treturn -EINVAL;\n\n\tif (!strcmp(str, \"off\")) {\n\t\tmmio_mitigation = MMIO_MITIGATION_OFF;\n\t} else if (!strcmp(str, \"full\")) {\n\t\tmmio_mitigation = MMIO_MITIGATION_VERW;\n\t} else if (!strcmp(str, \"full,nosmt\")) {\n\t\tmmio_mitigation = MMIO_MITIGATION_VERW;\n\t\tmmio_nosmt = true;\n\t}\n\n\treturn 0;\n}\nearly_param(\"mmio_stale_data\", mmio_stale_data_parse_cmdline);\n\n#undef pr_fmt\n#define pr_fmt(fmt)     \"\" fmt\n\nstatic void __init md_clear_update_mitigation(void)\n{\n\tif (cpu_mitigations_off())\n\t\treturn;\n\n\tif (!static_key_enabled(&mds_user_clear))\n\t\tgoto out;\n\n\t \n\tif (mds_mitigation == MDS_MITIGATION_OFF &&\n\t    boot_cpu_has_bug(X86_BUG_MDS)) {\n\t\tmds_mitigation = MDS_MITIGATION_FULL;\n\t\tmds_select_mitigation();\n\t}\n\tif (taa_mitigation == TAA_MITIGATION_OFF &&\n\t    boot_cpu_has_bug(X86_BUG_TAA)) {\n\t\ttaa_mitigation = TAA_MITIGATION_VERW;\n\t\ttaa_select_mitigation();\n\t}\n\tif (mmio_mitigation == MMIO_MITIGATION_OFF &&\n\t    boot_cpu_has_bug(X86_BUG_MMIO_STALE_DATA)) {\n\t\tmmio_mitigation = MMIO_MITIGATION_VERW;\n\t\tmmio_select_mitigation();\n\t}\nout:\n\tif (boot_cpu_has_bug(X86_BUG_MDS))\n\t\tpr_info(\"MDS: %s\\n\", mds_strings[mds_mitigation]);\n\tif (boot_cpu_has_bug(X86_BUG_TAA))\n\t\tpr_info(\"TAA: %s\\n\", taa_strings[taa_mitigation]);\n\tif (boot_cpu_has_bug(X86_BUG_MMIO_STALE_DATA))\n\t\tpr_info(\"MMIO Stale Data: %s\\n\", mmio_strings[mmio_mitigation]);\n\telse if (boot_cpu_has_bug(X86_BUG_MMIO_UNKNOWN))\n\t\tpr_info(\"MMIO Stale Data: Unknown: No mitigations\\n\");\n}\n\nstatic void __init md_clear_select_mitigation(void)\n{\n\tmds_select_mitigation();\n\ttaa_select_mitigation();\n\tmmio_select_mitigation();\n\n\t \n\tmd_clear_update_mitigation();\n}\n\n#undef pr_fmt\n#define pr_fmt(fmt)\t\"SRBDS: \" fmt\n\nenum srbds_mitigations {\n\tSRBDS_MITIGATION_OFF,\n\tSRBDS_MITIGATION_UCODE_NEEDED,\n\tSRBDS_MITIGATION_FULL,\n\tSRBDS_MITIGATION_TSX_OFF,\n\tSRBDS_MITIGATION_HYPERVISOR,\n};\n\nstatic enum srbds_mitigations srbds_mitigation __ro_after_init = SRBDS_MITIGATION_FULL;\n\nstatic const char * const srbds_strings[] = {\n\t[SRBDS_MITIGATION_OFF]\t\t= \"Vulnerable\",\n\t[SRBDS_MITIGATION_UCODE_NEEDED]\t= \"Vulnerable: No microcode\",\n\t[SRBDS_MITIGATION_FULL]\t\t= \"Mitigation: Microcode\",\n\t[SRBDS_MITIGATION_TSX_OFF]\t= \"Mitigation: TSX disabled\",\n\t[SRBDS_MITIGATION_HYPERVISOR]\t= \"Unknown: Dependent on hypervisor status\",\n};\n\nstatic bool srbds_off;\n\nvoid update_srbds_msr(void)\n{\n\tu64 mcu_ctrl;\n\n\tif (!boot_cpu_has_bug(X86_BUG_SRBDS))\n\t\treturn;\n\n\tif (boot_cpu_has(X86_FEATURE_HYPERVISOR))\n\t\treturn;\n\n\tif (srbds_mitigation == SRBDS_MITIGATION_UCODE_NEEDED)\n\t\treturn;\n\n\t \n\tif (!boot_cpu_has(X86_FEATURE_SRBDS_CTRL))\n\t\treturn;\n\n\trdmsrl(MSR_IA32_MCU_OPT_CTRL, mcu_ctrl);\n\n\tswitch (srbds_mitigation) {\n\tcase SRBDS_MITIGATION_OFF:\n\tcase SRBDS_MITIGATION_TSX_OFF:\n\t\tmcu_ctrl |= RNGDS_MITG_DIS;\n\t\tbreak;\n\tcase SRBDS_MITIGATION_FULL:\n\t\tmcu_ctrl &= ~RNGDS_MITG_DIS;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\twrmsrl(MSR_IA32_MCU_OPT_CTRL, mcu_ctrl);\n}\n\nstatic void __init srbds_select_mitigation(void)\n{\n\tu64 ia32_cap;\n\n\tif (!boot_cpu_has_bug(X86_BUG_SRBDS))\n\t\treturn;\n\n\t \n\tia32_cap = x86_read_arch_cap_msr();\n\tif ((ia32_cap & ARCH_CAP_MDS_NO) && !boot_cpu_has(X86_FEATURE_RTM) &&\n\t    !boot_cpu_has_bug(X86_BUG_MMIO_STALE_DATA))\n\t\tsrbds_mitigation = SRBDS_MITIGATION_TSX_OFF;\n\telse if (boot_cpu_has(X86_FEATURE_HYPERVISOR))\n\t\tsrbds_mitigation = SRBDS_MITIGATION_HYPERVISOR;\n\telse if (!boot_cpu_has(X86_FEATURE_SRBDS_CTRL))\n\t\tsrbds_mitigation = SRBDS_MITIGATION_UCODE_NEEDED;\n\telse if (cpu_mitigations_off() || srbds_off)\n\t\tsrbds_mitigation = SRBDS_MITIGATION_OFF;\n\n\tupdate_srbds_msr();\n\tpr_info(\"%s\\n\", srbds_strings[srbds_mitigation]);\n}\n\nstatic int __init srbds_parse_cmdline(char *str)\n{\n\tif (!str)\n\t\treturn -EINVAL;\n\n\tif (!boot_cpu_has_bug(X86_BUG_SRBDS))\n\t\treturn 0;\n\n\tsrbds_off = !strcmp(str, \"off\");\n\treturn 0;\n}\nearly_param(\"srbds\", srbds_parse_cmdline);\n\n#undef pr_fmt\n#define pr_fmt(fmt)     \"L1D Flush : \" fmt\n\nenum l1d_flush_mitigations {\n\tL1D_FLUSH_OFF = 0,\n\tL1D_FLUSH_ON,\n};\n\nstatic enum l1d_flush_mitigations l1d_flush_mitigation __initdata = L1D_FLUSH_OFF;\n\nstatic void __init l1d_flush_select_mitigation(void)\n{\n\tif (!l1d_flush_mitigation || !boot_cpu_has(X86_FEATURE_FLUSH_L1D))\n\t\treturn;\n\n\tstatic_branch_enable(&switch_mm_cond_l1d_flush);\n\tpr_info(\"Conditional flush on switch_mm() enabled\\n\");\n}\n\nstatic int __init l1d_flush_parse_cmdline(char *str)\n{\n\tif (!strcmp(str, \"on\"))\n\t\tl1d_flush_mitigation = L1D_FLUSH_ON;\n\n\treturn 0;\n}\nearly_param(\"l1d_flush\", l1d_flush_parse_cmdline);\n\n#undef pr_fmt\n#define pr_fmt(fmt)\t\"GDS: \" fmt\n\nenum gds_mitigations {\n\tGDS_MITIGATION_OFF,\n\tGDS_MITIGATION_UCODE_NEEDED,\n\tGDS_MITIGATION_FORCE,\n\tGDS_MITIGATION_FULL,\n\tGDS_MITIGATION_FULL_LOCKED,\n\tGDS_MITIGATION_HYPERVISOR,\n};\n\n#if IS_ENABLED(CONFIG_GDS_FORCE_MITIGATION)\nstatic enum gds_mitigations gds_mitigation __ro_after_init = GDS_MITIGATION_FORCE;\n#else\nstatic enum gds_mitigations gds_mitigation __ro_after_init = GDS_MITIGATION_FULL;\n#endif\n\nstatic const char * const gds_strings[] = {\n\t[GDS_MITIGATION_OFF]\t\t= \"Vulnerable\",\n\t[GDS_MITIGATION_UCODE_NEEDED]\t= \"Vulnerable: No microcode\",\n\t[GDS_MITIGATION_FORCE]\t\t= \"Mitigation: AVX disabled, no microcode\",\n\t[GDS_MITIGATION_FULL]\t\t= \"Mitigation: Microcode\",\n\t[GDS_MITIGATION_FULL_LOCKED]\t= \"Mitigation: Microcode (locked)\",\n\t[GDS_MITIGATION_HYPERVISOR]\t= \"Unknown: Dependent on hypervisor status\",\n};\n\nbool gds_ucode_mitigated(void)\n{\n\treturn (gds_mitigation == GDS_MITIGATION_FULL ||\n\t\tgds_mitigation == GDS_MITIGATION_FULL_LOCKED);\n}\nEXPORT_SYMBOL_GPL(gds_ucode_mitigated);\n\nvoid update_gds_msr(void)\n{\n\tu64 mcu_ctrl_after;\n\tu64 mcu_ctrl;\n\n\tswitch (gds_mitigation) {\n\tcase GDS_MITIGATION_OFF:\n\t\trdmsrl(MSR_IA32_MCU_OPT_CTRL, mcu_ctrl);\n\t\tmcu_ctrl |= GDS_MITG_DIS;\n\t\tbreak;\n\tcase GDS_MITIGATION_FULL_LOCKED:\n\t\t \n\tcase GDS_MITIGATION_FULL:\n\t\trdmsrl(MSR_IA32_MCU_OPT_CTRL, mcu_ctrl);\n\t\tmcu_ctrl &= ~GDS_MITG_DIS;\n\t\tbreak;\n\tcase GDS_MITIGATION_FORCE:\n\tcase GDS_MITIGATION_UCODE_NEEDED:\n\tcase GDS_MITIGATION_HYPERVISOR:\n\t\treturn;\n\t};\n\n\twrmsrl(MSR_IA32_MCU_OPT_CTRL, mcu_ctrl);\n\n\t \n\trdmsrl(MSR_IA32_MCU_OPT_CTRL, mcu_ctrl_after);\n\tWARN_ON_ONCE(mcu_ctrl != mcu_ctrl_after);\n}\n\nstatic void __init gds_select_mitigation(void)\n{\n\tu64 mcu_ctrl;\n\n\tif (!boot_cpu_has_bug(X86_BUG_GDS))\n\t\treturn;\n\n\tif (boot_cpu_has(X86_FEATURE_HYPERVISOR)) {\n\t\tgds_mitigation = GDS_MITIGATION_HYPERVISOR;\n\t\tgoto out;\n\t}\n\n\tif (cpu_mitigations_off())\n\t\tgds_mitigation = GDS_MITIGATION_OFF;\n\t \n\n\t \n\tif (!(x86_read_arch_cap_msr() & ARCH_CAP_GDS_CTRL)) {\n\t\tif (gds_mitigation == GDS_MITIGATION_FORCE) {\n\t\t\t \n\t\t\tsetup_clear_cpu_cap(X86_FEATURE_AVX);\n\t\t\tpr_warn(\"Microcode update needed! Disabling AVX as mitigation.\\n\");\n\t\t} else {\n\t\t\tgds_mitigation = GDS_MITIGATION_UCODE_NEEDED;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t \n\tif (gds_mitigation == GDS_MITIGATION_FORCE)\n\t\tgds_mitigation = GDS_MITIGATION_FULL;\n\n\trdmsrl(MSR_IA32_MCU_OPT_CTRL, mcu_ctrl);\n\tif (mcu_ctrl & GDS_MITG_LOCKED) {\n\t\tif (gds_mitigation == GDS_MITIGATION_OFF)\n\t\t\tpr_warn(\"Mitigation locked. Disable failed.\\n\");\n\n\t\t \n\t\tgds_mitigation = GDS_MITIGATION_FULL_LOCKED;\n\t}\n\n\tupdate_gds_msr();\nout:\n\tpr_info(\"%s\\n\", gds_strings[gds_mitigation]);\n}\n\nstatic int __init gds_parse_cmdline(char *str)\n{\n\tif (!str)\n\t\treturn -EINVAL;\n\n\tif (!boot_cpu_has_bug(X86_BUG_GDS))\n\t\treturn 0;\n\n\tif (!strcmp(str, \"off\"))\n\t\tgds_mitigation = GDS_MITIGATION_OFF;\n\telse if (!strcmp(str, \"force\"))\n\t\tgds_mitigation = GDS_MITIGATION_FORCE;\n\n\treturn 0;\n}\nearly_param(\"gather_data_sampling\", gds_parse_cmdline);\n\n#undef pr_fmt\n#define pr_fmt(fmt)     \"Spectre V1 : \" fmt\n\nenum spectre_v1_mitigation {\n\tSPECTRE_V1_MITIGATION_NONE,\n\tSPECTRE_V1_MITIGATION_AUTO,\n};\n\nstatic enum spectre_v1_mitigation spectre_v1_mitigation __ro_after_init =\n\tSPECTRE_V1_MITIGATION_AUTO;\n\nstatic const char * const spectre_v1_strings[] = {\n\t[SPECTRE_V1_MITIGATION_NONE] = \"Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\",\n\t[SPECTRE_V1_MITIGATION_AUTO] = \"Mitigation: usercopy/swapgs barriers and __user pointer sanitization\",\n};\n\n \nstatic bool smap_works_speculatively(void)\n{\n\tif (!boot_cpu_has(X86_FEATURE_SMAP))\n\t\treturn false;\n\n\t \n\tif (boot_cpu_has(X86_BUG_CPU_MELTDOWN))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void __init spectre_v1_select_mitigation(void)\n{\n\tif (!boot_cpu_has_bug(X86_BUG_SPECTRE_V1) || cpu_mitigations_off()) {\n\t\tspectre_v1_mitigation = SPECTRE_V1_MITIGATION_NONE;\n\t\treturn;\n\t}\n\n\tif (spectre_v1_mitigation == SPECTRE_V1_MITIGATION_AUTO) {\n\t\t \n\t\tif (boot_cpu_has(X86_FEATURE_FSGSBASE) ||\n\t\t    !smap_works_speculatively()) {\n\t\t\t \n\t\t\tif (boot_cpu_has_bug(X86_BUG_SWAPGS) &&\n\t\t\t    !boot_cpu_has(X86_FEATURE_PTI))\n\t\t\t\tsetup_force_cpu_cap(X86_FEATURE_FENCE_SWAPGS_USER);\n\n\t\t\t \n\t\t\tsetup_force_cpu_cap(X86_FEATURE_FENCE_SWAPGS_KERNEL);\n\t\t}\n\t}\n\n\tpr_info(\"%s\\n\", spectre_v1_strings[spectre_v1_mitigation]);\n}\n\nstatic int __init nospectre_v1_cmdline(char *str)\n{\n\tspectre_v1_mitigation = SPECTRE_V1_MITIGATION_NONE;\n\treturn 0;\n}\nearly_param(\"nospectre_v1\", nospectre_v1_cmdline);\n\nenum spectre_v2_mitigation spectre_v2_enabled __ro_after_init = SPECTRE_V2_NONE;\n\n#undef pr_fmt\n#define pr_fmt(fmt)     \"RETBleed: \" fmt\n\nenum retbleed_mitigation {\n\tRETBLEED_MITIGATION_NONE,\n\tRETBLEED_MITIGATION_UNRET,\n\tRETBLEED_MITIGATION_IBPB,\n\tRETBLEED_MITIGATION_IBRS,\n\tRETBLEED_MITIGATION_EIBRS,\n\tRETBLEED_MITIGATION_STUFF,\n};\n\nenum retbleed_mitigation_cmd {\n\tRETBLEED_CMD_OFF,\n\tRETBLEED_CMD_AUTO,\n\tRETBLEED_CMD_UNRET,\n\tRETBLEED_CMD_IBPB,\n\tRETBLEED_CMD_STUFF,\n};\n\nstatic const char * const retbleed_strings[] = {\n\t[RETBLEED_MITIGATION_NONE]\t= \"Vulnerable\",\n\t[RETBLEED_MITIGATION_UNRET]\t= \"Mitigation: untrained return thunk\",\n\t[RETBLEED_MITIGATION_IBPB]\t= \"Mitigation: IBPB\",\n\t[RETBLEED_MITIGATION_IBRS]\t= \"Mitigation: IBRS\",\n\t[RETBLEED_MITIGATION_EIBRS]\t= \"Mitigation: Enhanced IBRS\",\n\t[RETBLEED_MITIGATION_STUFF]\t= \"Mitigation: Stuffing\",\n};\n\nstatic enum retbleed_mitigation retbleed_mitigation __ro_after_init =\n\tRETBLEED_MITIGATION_NONE;\nstatic enum retbleed_mitigation_cmd retbleed_cmd __ro_after_init =\n\tRETBLEED_CMD_AUTO;\n\nstatic int __ro_after_init retbleed_nosmt = false;\n\nstatic int __init retbleed_parse_cmdline(char *str)\n{\n\tif (!str)\n\t\treturn -EINVAL;\n\n\twhile (str) {\n\t\tchar *next = strchr(str, ',');\n\t\tif (next) {\n\t\t\t*next = 0;\n\t\t\tnext++;\n\t\t}\n\n\t\tif (!strcmp(str, \"off\")) {\n\t\t\tretbleed_cmd = RETBLEED_CMD_OFF;\n\t\t} else if (!strcmp(str, \"auto\")) {\n\t\t\tretbleed_cmd = RETBLEED_CMD_AUTO;\n\t\t} else if (!strcmp(str, \"unret\")) {\n\t\t\tretbleed_cmd = RETBLEED_CMD_UNRET;\n\t\t} else if (!strcmp(str, \"ibpb\")) {\n\t\t\tretbleed_cmd = RETBLEED_CMD_IBPB;\n\t\t} else if (!strcmp(str, \"stuff\")) {\n\t\t\tretbleed_cmd = RETBLEED_CMD_STUFF;\n\t\t} else if (!strcmp(str, \"nosmt\")) {\n\t\t\tretbleed_nosmt = true;\n\t\t} else if (!strcmp(str, \"force\")) {\n\t\t\tsetup_force_cpu_bug(X86_BUG_RETBLEED);\n\t\t} else {\n\t\t\tpr_err(\"Ignoring unknown retbleed option (%s).\", str);\n\t\t}\n\n\t\tstr = next;\n\t}\n\n\treturn 0;\n}\nearly_param(\"retbleed\", retbleed_parse_cmdline);\n\n#define RETBLEED_UNTRAIN_MSG \"WARNING: BTB untrained return thunk mitigation is only effective on AMD/Hygon!\\n\"\n#define RETBLEED_INTEL_MSG \"WARNING: Spectre v2 mitigation leaves CPU vulnerable to RETBleed attacks, data leaks possible!\\n\"\n\nstatic void __init retbleed_select_mitigation(void)\n{\n\tbool mitigate_smt = false;\n\n\tif (!boot_cpu_has_bug(X86_BUG_RETBLEED) || cpu_mitigations_off())\n\t\treturn;\n\n\tswitch (retbleed_cmd) {\n\tcase RETBLEED_CMD_OFF:\n\t\treturn;\n\n\tcase RETBLEED_CMD_UNRET:\n\t\tif (IS_ENABLED(CONFIG_CPU_UNRET_ENTRY)) {\n\t\t\tretbleed_mitigation = RETBLEED_MITIGATION_UNRET;\n\t\t} else {\n\t\t\tpr_err(\"WARNING: kernel not compiled with CPU_UNRET_ENTRY.\\n\");\n\t\t\tgoto do_cmd_auto;\n\t\t}\n\t\tbreak;\n\n\tcase RETBLEED_CMD_IBPB:\n\t\tif (!boot_cpu_has(X86_FEATURE_IBPB)) {\n\t\t\tpr_err(\"WARNING: CPU does not support IBPB.\\n\");\n\t\t\tgoto do_cmd_auto;\n\t\t} else if (IS_ENABLED(CONFIG_CPU_IBPB_ENTRY)) {\n\t\t\tretbleed_mitigation = RETBLEED_MITIGATION_IBPB;\n\t\t} else {\n\t\t\tpr_err(\"WARNING: kernel not compiled with CPU_IBPB_ENTRY.\\n\");\n\t\t\tgoto do_cmd_auto;\n\t\t}\n\t\tbreak;\n\n\tcase RETBLEED_CMD_STUFF:\n\t\tif (IS_ENABLED(CONFIG_CALL_DEPTH_TRACKING) &&\n\t\t    spectre_v2_enabled == SPECTRE_V2_RETPOLINE) {\n\t\t\tretbleed_mitigation = RETBLEED_MITIGATION_STUFF;\n\n\t\t} else {\n\t\t\tif (IS_ENABLED(CONFIG_CALL_DEPTH_TRACKING))\n\t\t\t\tpr_err(\"WARNING: retbleed=stuff depends on spectre_v2=retpoline\\n\");\n\t\t\telse\n\t\t\t\tpr_err(\"WARNING: kernel not compiled with CALL_DEPTH_TRACKING.\\n\");\n\n\t\t\tgoto do_cmd_auto;\n\t\t}\n\t\tbreak;\n\ndo_cmd_auto:\n\tcase RETBLEED_CMD_AUTO:\n\tdefault:\n\t\tif (boot_cpu_data.x86_vendor == X86_VENDOR_AMD ||\n\t\t    boot_cpu_data.x86_vendor == X86_VENDOR_HYGON) {\n\t\t\tif (IS_ENABLED(CONFIG_CPU_UNRET_ENTRY))\n\t\t\t\tretbleed_mitigation = RETBLEED_MITIGATION_UNRET;\n\t\t\telse if (IS_ENABLED(CONFIG_CPU_IBPB_ENTRY) && boot_cpu_has(X86_FEATURE_IBPB))\n\t\t\t\tretbleed_mitigation = RETBLEED_MITIGATION_IBPB;\n\t\t}\n\n\t\t \n\n\t\tbreak;\n\t}\n\n\tswitch (retbleed_mitigation) {\n\tcase RETBLEED_MITIGATION_UNRET:\n\t\tsetup_force_cpu_cap(X86_FEATURE_RETHUNK);\n\t\tsetup_force_cpu_cap(X86_FEATURE_UNRET);\n\n\t\tif (IS_ENABLED(CONFIG_RETHUNK))\n\t\t\tx86_return_thunk = retbleed_return_thunk;\n\n\t\tif (boot_cpu_data.x86_vendor != X86_VENDOR_AMD &&\n\t\t    boot_cpu_data.x86_vendor != X86_VENDOR_HYGON)\n\t\t\tpr_err(RETBLEED_UNTRAIN_MSG);\n\n\t\tmitigate_smt = true;\n\t\tbreak;\n\n\tcase RETBLEED_MITIGATION_IBPB:\n\t\tsetup_force_cpu_cap(X86_FEATURE_ENTRY_IBPB);\n\t\tsetup_force_cpu_cap(X86_FEATURE_IBPB_ON_VMEXIT);\n\t\tmitigate_smt = true;\n\t\tbreak;\n\n\tcase RETBLEED_MITIGATION_STUFF:\n\t\tsetup_force_cpu_cap(X86_FEATURE_RETHUNK);\n\t\tsetup_force_cpu_cap(X86_FEATURE_CALL_DEPTH);\n\t\tx86_set_skl_return_thunk();\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (mitigate_smt && !boot_cpu_has(X86_FEATURE_STIBP) &&\n\t    (retbleed_nosmt || cpu_mitigations_auto_nosmt()))\n\t\tcpu_smt_disable(false);\n\n\t \n\tif (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL) {\n\t\tswitch (spectre_v2_enabled) {\n\t\tcase SPECTRE_V2_IBRS:\n\t\t\tretbleed_mitigation = RETBLEED_MITIGATION_IBRS;\n\t\t\tbreak;\n\t\tcase SPECTRE_V2_EIBRS:\n\t\tcase SPECTRE_V2_EIBRS_RETPOLINE:\n\t\tcase SPECTRE_V2_EIBRS_LFENCE:\n\t\t\tretbleed_mitigation = RETBLEED_MITIGATION_EIBRS;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tif (retbleed_mitigation != RETBLEED_MITIGATION_STUFF)\n\t\t\t\tpr_err(RETBLEED_INTEL_MSG);\n\t\t}\n\t}\n\n\tpr_info(\"%s\\n\", retbleed_strings[retbleed_mitigation]);\n}\n\n#undef pr_fmt\n#define pr_fmt(fmt)     \"Spectre V2 : \" fmt\n\nstatic enum spectre_v2_user_mitigation spectre_v2_user_stibp __ro_after_init =\n\tSPECTRE_V2_USER_NONE;\nstatic enum spectre_v2_user_mitigation spectre_v2_user_ibpb __ro_after_init =\n\tSPECTRE_V2_USER_NONE;\n\n#ifdef CONFIG_RETPOLINE\nstatic bool spectre_v2_bad_module;\n\nbool retpoline_module_ok(bool has_retpoline)\n{\n\tif (spectre_v2_enabled == SPECTRE_V2_NONE || has_retpoline)\n\t\treturn true;\n\n\tpr_err(\"System may be vulnerable to spectre v2\\n\");\n\tspectre_v2_bad_module = true;\n\treturn false;\n}\n\nstatic inline const char *spectre_v2_module_string(void)\n{\n\treturn spectre_v2_bad_module ? \" - vulnerable module loaded\" : \"\";\n}\n#else\nstatic inline const char *spectre_v2_module_string(void) { return \"\"; }\n#endif\n\n#define SPECTRE_V2_LFENCE_MSG \"WARNING: LFENCE mitigation is not recommended for this CPU, data leaks possible!\\n\"\n#define SPECTRE_V2_EIBRS_EBPF_MSG \"WARNING: Unprivileged eBPF is enabled with eIBRS on, data leaks possible via Spectre v2 BHB attacks!\\n\"\n#define SPECTRE_V2_EIBRS_LFENCE_EBPF_SMT_MSG \"WARNING: Unprivileged eBPF is enabled with eIBRS+LFENCE mitigation and SMT, data leaks possible via Spectre v2 BHB attacks!\\n\"\n#define SPECTRE_V2_IBRS_PERF_MSG \"WARNING: IBRS mitigation selected on Enhanced IBRS CPU, this may cause unnecessary performance loss\\n\"\n\n#ifdef CONFIG_BPF_SYSCALL\nvoid unpriv_ebpf_notify(int new_state)\n{\n\tif (new_state)\n\t\treturn;\n\n\t \n\n\tswitch (spectre_v2_enabled) {\n\tcase SPECTRE_V2_EIBRS:\n\t\tpr_err(SPECTRE_V2_EIBRS_EBPF_MSG);\n\t\tbreak;\n\tcase SPECTRE_V2_EIBRS_LFENCE:\n\t\tif (sched_smt_active())\n\t\t\tpr_err(SPECTRE_V2_EIBRS_LFENCE_EBPF_SMT_MSG);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n#endif\n\nstatic inline bool match_option(const char *arg, int arglen, const char *opt)\n{\n\tint len = strlen(opt);\n\n\treturn len == arglen && !strncmp(arg, opt, len);\n}\n\n \nenum spectre_v2_mitigation_cmd {\n\tSPECTRE_V2_CMD_NONE,\n\tSPECTRE_V2_CMD_AUTO,\n\tSPECTRE_V2_CMD_FORCE,\n\tSPECTRE_V2_CMD_RETPOLINE,\n\tSPECTRE_V2_CMD_RETPOLINE_GENERIC,\n\tSPECTRE_V2_CMD_RETPOLINE_LFENCE,\n\tSPECTRE_V2_CMD_EIBRS,\n\tSPECTRE_V2_CMD_EIBRS_RETPOLINE,\n\tSPECTRE_V2_CMD_EIBRS_LFENCE,\n\tSPECTRE_V2_CMD_IBRS,\n};\n\nenum spectre_v2_user_cmd {\n\tSPECTRE_V2_USER_CMD_NONE,\n\tSPECTRE_V2_USER_CMD_AUTO,\n\tSPECTRE_V2_USER_CMD_FORCE,\n\tSPECTRE_V2_USER_CMD_PRCTL,\n\tSPECTRE_V2_USER_CMD_PRCTL_IBPB,\n\tSPECTRE_V2_USER_CMD_SECCOMP,\n\tSPECTRE_V2_USER_CMD_SECCOMP_IBPB,\n};\n\nstatic const char * const spectre_v2_user_strings[] = {\n\t[SPECTRE_V2_USER_NONE]\t\t\t= \"User space: Vulnerable\",\n\t[SPECTRE_V2_USER_STRICT]\t\t= \"User space: Mitigation: STIBP protection\",\n\t[SPECTRE_V2_USER_STRICT_PREFERRED]\t= \"User space: Mitigation: STIBP always-on protection\",\n\t[SPECTRE_V2_USER_PRCTL]\t\t\t= \"User space: Mitigation: STIBP via prctl\",\n\t[SPECTRE_V2_USER_SECCOMP]\t\t= \"User space: Mitigation: STIBP via seccomp and prctl\",\n};\n\nstatic const struct {\n\tconst char\t\t\t*option;\n\tenum spectre_v2_user_cmd\tcmd;\n\tbool\t\t\t\tsecure;\n} v2_user_options[] __initconst = {\n\t{ \"auto\",\t\tSPECTRE_V2_USER_CMD_AUTO,\t\tfalse },\n\t{ \"off\",\t\tSPECTRE_V2_USER_CMD_NONE,\t\tfalse },\n\t{ \"on\",\t\t\tSPECTRE_V2_USER_CMD_FORCE,\t\ttrue  },\n\t{ \"prctl\",\t\tSPECTRE_V2_USER_CMD_PRCTL,\t\tfalse },\n\t{ \"prctl,ibpb\",\t\tSPECTRE_V2_USER_CMD_PRCTL_IBPB,\t\tfalse },\n\t{ \"seccomp\",\t\tSPECTRE_V2_USER_CMD_SECCOMP,\t\tfalse },\n\t{ \"seccomp,ibpb\",\tSPECTRE_V2_USER_CMD_SECCOMP_IBPB,\tfalse },\n};\n\nstatic void __init spec_v2_user_print_cond(const char *reason, bool secure)\n{\n\tif (boot_cpu_has_bug(X86_BUG_SPECTRE_V2) != secure)\n\t\tpr_info(\"spectre_v2_user=%s forced on command line.\\n\", reason);\n}\n\nstatic __ro_after_init enum spectre_v2_mitigation_cmd spectre_v2_cmd;\n\nstatic enum spectre_v2_user_cmd __init\nspectre_v2_parse_user_cmdline(void)\n{\n\tchar arg[20];\n\tint ret, i;\n\n\tswitch (spectre_v2_cmd) {\n\tcase SPECTRE_V2_CMD_NONE:\n\t\treturn SPECTRE_V2_USER_CMD_NONE;\n\tcase SPECTRE_V2_CMD_FORCE:\n\t\treturn SPECTRE_V2_USER_CMD_FORCE;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tret = cmdline_find_option(boot_command_line, \"spectre_v2_user\",\n\t\t\t\t  arg, sizeof(arg));\n\tif (ret < 0)\n\t\treturn SPECTRE_V2_USER_CMD_AUTO;\n\n\tfor (i = 0; i < ARRAY_SIZE(v2_user_options); i++) {\n\t\tif (match_option(arg, ret, v2_user_options[i].option)) {\n\t\t\tspec_v2_user_print_cond(v2_user_options[i].option,\n\t\t\t\t\t\tv2_user_options[i].secure);\n\t\t\treturn v2_user_options[i].cmd;\n\t\t}\n\t}\n\n\tpr_err(\"Unknown user space protection option (%s). Switching to AUTO select\\n\", arg);\n\treturn SPECTRE_V2_USER_CMD_AUTO;\n}\n\nstatic inline bool spectre_v2_in_ibrs_mode(enum spectre_v2_mitigation mode)\n{\n\treturn spectre_v2_in_eibrs_mode(mode) || mode == SPECTRE_V2_IBRS;\n}\n\nstatic void __init\nspectre_v2_user_select_mitigation(void)\n{\n\tenum spectre_v2_user_mitigation mode = SPECTRE_V2_USER_NONE;\n\tbool smt_possible = IS_ENABLED(CONFIG_SMP);\n\tenum spectre_v2_user_cmd cmd;\n\n\tif (!boot_cpu_has(X86_FEATURE_IBPB) && !boot_cpu_has(X86_FEATURE_STIBP))\n\t\treturn;\n\n\tif (cpu_smt_control == CPU_SMT_FORCE_DISABLED ||\n\t    cpu_smt_control == CPU_SMT_NOT_SUPPORTED)\n\t\tsmt_possible = false;\n\n\tcmd = spectre_v2_parse_user_cmdline();\n\tswitch (cmd) {\n\tcase SPECTRE_V2_USER_CMD_NONE:\n\t\tgoto set_mode;\n\tcase SPECTRE_V2_USER_CMD_FORCE:\n\t\tmode = SPECTRE_V2_USER_STRICT;\n\t\tbreak;\n\tcase SPECTRE_V2_USER_CMD_AUTO:\n\tcase SPECTRE_V2_USER_CMD_PRCTL:\n\tcase SPECTRE_V2_USER_CMD_PRCTL_IBPB:\n\t\tmode = SPECTRE_V2_USER_PRCTL;\n\t\tbreak;\n\tcase SPECTRE_V2_USER_CMD_SECCOMP:\n\tcase SPECTRE_V2_USER_CMD_SECCOMP_IBPB:\n\t\tif (IS_ENABLED(CONFIG_SECCOMP))\n\t\t\tmode = SPECTRE_V2_USER_SECCOMP;\n\t\telse\n\t\t\tmode = SPECTRE_V2_USER_PRCTL;\n\t\tbreak;\n\t}\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_IBPB)) {\n\t\tsetup_force_cpu_cap(X86_FEATURE_USE_IBPB);\n\n\t\tspectre_v2_user_ibpb = mode;\n\t\tswitch (cmd) {\n\t\tcase SPECTRE_V2_USER_CMD_FORCE:\n\t\tcase SPECTRE_V2_USER_CMD_PRCTL_IBPB:\n\t\tcase SPECTRE_V2_USER_CMD_SECCOMP_IBPB:\n\t\t\tstatic_branch_enable(&switch_mm_always_ibpb);\n\t\t\tspectre_v2_user_ibpb = SPECTRE_V2_USER_STRICT;\n\t\t\tbreak;\n\t\tcase SPECTRE_V2_USER_CMD_PRCTL:\n\t\tcase SPECTRE_V2_USER_CMD_AUTO:\n\t\tcase SPECTRE_V2_USER_CMD_SECCOMP:\n\t\t\tstatic_branch_enable(&switch_mm_cond_ibpb);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tpr_info(\"mitigation: Enabling %s Indirect Branch Prediction Barrier\\n\",\n\t\t\tstatic_key_enabled(&switch_mm_always_ibpb) ?\n\t\t\t\"always-on\" : \"conditional\");\n\t}\n\n\t \n\tif (!boot_cpu_has(X86_FEATURE_STIBP) ||\n\t    !smt_possible ||\n\t    (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&\n\t     !boot_cpu_has(X86_FEATURE_AUTOIBRS)))\n\t\treturn;\n\n\t \n\tif (mode != SPECTRE_V2_USER_STRICT &&\n\t    boot_cpu_has(X86_FEATURE_AMD_STIBP_ALWAYS_ON))\n\t\tmode = SPECTRE_V2_USER_STRICT_PREFERRED;\n\n\tif (retbleed_mitigation == RETBLEED_MITIGATION_UNRET ||\n\t    retbleed_mitigation == RETBLEED_MITIGATION_IBPB) {\n\t\tif (mode != SPECTRE_V2_USER_STRICT &&\n\t\t    mode != SPECTRE_V2_USER_STRICT_PREFERRED)\n\t\t\tpr_info(\"Selecting STIBP always-on mode to complement retbleed mitigation\\n\");\n\t\tmode = SPECTRE_V2_USER_STRICT_PREFERRED;\n\t}\n\n\tspectre_v2_user_stibp = mode;\n\nset_mode:\n\tpr_info(\"%s\\n\", spectre_v2_user_strings[mode]);\n}\n\nstatic const char * const spectre_v2_strings[] = {\n\t[SPECTRE_V2_NONE]\t\t\t= \"Vulnerable\",\n\t[SPECTRE_V2_RETPOLINE]\t\t\t= \"Mitigation: Retpolines\",\n\t[SPECTRE_V2_LFENCE]\t\t\t= \"Mitigation: LFENCE\",\n\t[SPECTRE_V2_EIBRS]\t\t\t= \"Mitigation: Enhanced / Automatic IBRS\",\n\t[SPECTRE_V2_EIBRS_LFENCE]\t\t= \"Mitigation: Enhanced / Automatic IBRS + LFENCE\",\n\t[SPECTRE_V2_EIBRS_RETPOLINE]\t\t= \"Mitigation: Enhanced / Automatic IBRS + Retpolines\",\n\t[SPECTRE_V2_IBRS]\t\t\t= \"Mitigation: IBRS\",\n};\n\nstatic const struct {\n\tconst char *option;\n\tenum spectre_v2_mitigation_cmd cmd;\n\tbool secure;\n} mitigation_options[] __initconst = {\n\t{ \"off\",\t\tSPECTRE_V2_CMD_NONE,\t\t  false },\n\t{ \"on\",\t\t\tSPECTRE_V2_CMD_FORCE,\t\t  true  },\n\t{ \"retpoline\",\t\tSPECTRE_V2_CMD_RETPOLINE,\t  false },\n\t{ \"retpoline,amd\",\tSPECTRE_V2_CMD_RETPOLINE_LFENCE,  false },\n\t{ \"retpoline,lfence\",\tSPECTRE_V2_CMD_RETPOLINE_LFENCE,  false },\n\t{ \"retpoline,generic\",\tSPECTRE_V2_CMD_RETPOLINE_GENERIC, false },\n\t{ \"eibrs\",\t\tSPECTRE_V2_CMD_EIBRS,\t\t  false },\n\t{ \"eibrs,lfence\",\tSPECTRE_V2_CMD_EIBRS_LFENCE,\t  false },\n\t{ \"eibrs,retpoline\",\tSPECTRE_V2_CMD_EIBRS_RETPOLINE,\t  false },\n\t{ \"auto\",\t\tSPECTRE_V2_CMD_AUTO,\t\t  false },\n\t{ \"ibrs\",\t\tSPECTRE_V2_CMD_IBRS,              false },\n};\n\nstatic void __init spec_v2_print_cond(const char *reason, bool secure)\n{\n\tif (boot_cpu_has_bug(X86_BUG_SPECTRE_V2) != secure)\n\t\tpr_info(\"%s selected on command line.\\n\", reason);\n}\n\nstatic enum spectre_v2_mitigation_cmd __init spectre_v2_parse_cmdline(void)\n{\n\tenum spectre_v2_mitigation_cmd cmd = SPECTRE_V2_CMD_AUTO;\n\tchar arg[20];\n\tint ret, i;\n\n\tif (cmdline_find_option_bool(boot_command_line, \"nospectre_v2\") ||\n\t    cpu_mitigations_off())\n\t\treturn SPECTRE_V2_CMD_NONE;\n\n\tret = cmdline_find_option(boot_command_line, \"spectre_v2\", arg, sizeof(arg));\n\tif (ret < 0)\n\t\treturn SPECTRE_V2_CMD_AUTO;\n\n\tfor (i = 0; i < ARRAY_SIZE(mitigation_options); i++) {\n\t\tif (!match_option(arg, ret, mitigation_options[i].option))\n\t\t\tcontinue;\n\t\tcmd = mitigation_options[i].cmd;\n\t\tbreak;\n\t}\n\n\tif (i >= ARRAY_SIZE(mitigation_options)) {\n\t\tpr_err(\"unknown option (%s). Switching to AUTO select\\n\", arg);\n\t\treturn SPECTRE_V2_CMD_AUTO;\n\t}\n\n\tif ((cmd == SPECTRE_V2_CMD_RETPOLINE ||\n\t     cmd == SPECTRE_V2_CMD_RETPOLINE_LFENCE ||\n\t     cmd == SPECTRE_V2_CMD_RETPOLINE_GENERIC ||\n\t     cmd == SPECTRE_V2_CMD_EIBRS_LFENCE ||\n\t     cmd == SPECTRE_V2_CMD_EIBRS_RETPOLINE) &&\n\t    !IS_ENABLED(CONFIG_RETPOLINE)) {\n\t\tpr_err(\"%s selected but not compiled in. Switching to AUTO select\\n\",\n\t\t       mitigation_options[i].option);\n\t\treturn SPECTRE_V2_CMD_AUTO;\n\t}\n\n\tif ((cmd == SPECTRE_V2_CMD_EIBRS ||\n\t     cmd == SPECTRE_V2_CMD_EIBRS_LFENCE ||\n\t     cmd == SPECTRE_V2_CMD_EIBRS_RETPOLINE) &&\n\t    !boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {\n\t\tpr_err(\"%s selected but CPU doesn't have Enhanced or Automatic IBRS. Switching to AUTO select\\n\",\n\t\t       mitigation_options[i].option);\n\t\treturn SPECTRE_V2_CMD_AUTO;\n\t}\n\n\tif ((cmd == SPECTRE_V2_CMD_RETPOLINE_LFENCE ||\n\t     cmd == SPECTRE_V2_CMD_EIBRS_LFENCE) &&\n\t    !boot_cpu_has(X86_FEATURE_LFENCE_RDTSC)) {\n\t\tpr_err(\"%s selected, but CPU doesn't have a serializing LFENCE. Switching to AUTO select\\n\",\n\t\t       mitigation_options[i].option);\n\t\treturn SPECTRE_V2_CMD_AUTO;\n\t}\n\n\tif (cmd == SPECTRE_V2_CMD_IBRS && !IS_ENABLED(CONFIG_CPU_IBRS_ENTRY)) {\n\t\tpr_err(\"%s selected but not compiled in. Switching to AUTO select\\n\",\n\t\t       mitigation_options[i].option);\n\t\treturn SPECTRE_V2_CMD_AUTO;\n\t}\n\n\tif (cmd == SPECTRE_V2_CMD_IBRS && boot_cpu_data.x86_vendor != X86_VENDOR_INTEL) {\n\t\tpr_err(\"%s selected but not Intel CPU. Switching to AUTO select\\n\",\n\t\t       mitigation_options[i].option);\n\t\treturn SPECTRE_V2_CMD_AUTO;\n\t}\n\n\tif (cmd == SPECTRE_V2_CMD_IBRS && !boot_cpu_has(X86_FEATURE_IBRS)) {\n\t\tpr_err(\"%s selected but CPU doesn't have IBRS. Switching to AUTO select\\n\",\n\t\t       mitigation_options[i].option);\n\t\treturn SPECTRE_V2_CMD_AUTO;\n\t}\n\n\tif (cmd == SPECTRE_V2_CMD_IBRS && cpu_feature_enabled(X86_FEATURE_XENPV)) {\n\t\tpr_err(\"%s selected but running as XenPV guest. Switching to AUTO select\\n\",\n\t\t       mitigation_options[i].option);\n\t\treturn SPECTRE_V2_CMD_AUTO;\n\t}\n\n\tspec_v2_print_cond(mitigation_options[i].option,\n\t\t\t   mitigation_options[i].secure);\n\treturn cmd;\n}\n\nstatic enum spectre_v2_mitigation __init spectre_v2_select_retpoline(void)\n{\n\tif (!IS_ENABLED(CONFIG_RETPOLINE)) {\n\t\tpr_err(\"Kernel not compiled with retpoline; no mitigation available!\");\n\t\treturn SPECTRE_V2_NONE;\n\t}\n\n\treturn SPECTRE_V2_RETPOLINE;\n}\n\n \nstatic void __init spec_ctrl_disable_kernel_rrsba(void)\n{\n\tu64 ia32_cap;\n\n\tif (!boot_cpu_has(X86_FEATURE_RRSBA_CTRL))\n\t\treturn;\n\n\tia32_cap = x86_read_arch_cap_msr();\n\n\tif (ia32_cap & ARCH_CAP_RRSBA) {\n\t\tx86_spec_ctrl_base |= SPEC_CTRL_RRSBA_DIS_S;\n\t\tupdate_spec_ctrl(x86_spec_ctrl_base);\n\t}\n}\n\nstatic void __init spectre_v2_determine_rsb_fill_type_at_vmexit(enum spectre_v2_mitigation mode)\n{\n\t \n\tswitch (mode) {\n\tcase SPECTRE_V2_NONE:\n\t\treturn;\n\n\tcase SPECTRE_V2_EIBRS_LFENCE:\n\tcase SPECTRE_V2_EIBRS:\n\t\tif (boot_cpu_has_bug(X86_BUG_EIBRS_PBRSB)) {\n\t\t\tsetup_force_cpu_cap(X86_FEATURE_RSB_VMEXIT_LITE);\n\t\t\tpr_info(\"Spectre v2 / PBRSB-eIBRS: Retire a single CALL on VMEXIT\\n\");\n\t\t}\n\t\treturn;\n\n\tcase SPECTRE_V2_EIBRS_RETPOLINE:\n\tcase SPECTRE_V2_RETPOLINE:\n\tcase SPECTRE_V2_LFENCE:\n\tcase SPECTRE_V2_IBRS:\n\t\tsetup_force_cpu_cap(X86_FEATURE_RSB_VMEXIT);\n\t\tpr_info(\"Spectre v2 / SpectreRSB : Filling RSB on VMEXIT\\n\");\n\t\treturn;\n\t}\n\n\tpr_warn_once(\"Unknown Spectre v2 mode, disabling RSB mitigation at VM exit\");\n\tdump_stack();\n}\n\nstatic void __init spectre_v2_select_mitigation(void)\n{\n\tenum spectre_v2_mitigation_cmd cmd = spectre_v2_parse_cmdline();\n\tenum spectre_v2_mitigation mode = SPECTRE_V2_NONE;\n\n\t \n\tif (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2) &&\n\t    (cmd == SPECTRE_V2_CMD_NONE || cmd == SPECTRE_V2_CMD_AUTO))\n\t\treturn;\n\n\tswitch (cmd) {\n\tcase SPECTRE_V2_CMD_NONE:\n\t\treturn;\n\n\tcase SPECTRE_V2_CMD_FORCE:\n\tcase SPECTRE_V2_CMD_AUTO:\n\t\tif (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED)) {\n\t\t\tmode = SPECTRE_V2_EIBRS;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (IS_ENABLED(CONFIG_CPU_IBRS_ENTRY) &&\n\t\t    boot_cpu_has_bug(X86_BUG_RETBLEED) &&\n\t\t    retbleed_cmd != RETBLEED_CMD_OFF &&\n\t\t    retbleed_cmd != RETBLEED_CMD_STUFF &&\n\t\t    boot_cpu_has(X86_FEATURE_IBRS) &&\n\t\t    boot_cpu_data.x86_vendor == X86_VENDOR_INTEL) {\n\t\t\tmode = SPECTRE_V2_IBRS;\n\t\t\tbreak;\n\t\t}\n\n\t\tmode = spectre_v2_select_retpoline();\n\t\tbreak;\n\n\tcase SPECTRE_V2_CMD_RETPOLINE_LFENCE:\n\t\tpr_err(SPECTRE_V2_LFENCE_MSG);\n\t\tmode = SPECTRE_V2_LFENCE;\n\t\tbreak;\n\n\tcase SPECTRE_V2_CMD_RETPOLINE_GENERIC:\n\t\tmode = SPECTRE_V2_RETPOLINE;\n\t\tbreak;\n\n\tcase SPECTRE_V2_CMD_RETPOLINE:\n\t\tmode = spectre_v2_select_retpoline();\n\t\tbreak;\n\n\tcase SPECTRE_V2_CMD_IBRS:\n\t\tmode = SPECTRE_V2_IBRS;\n\t\tbreak;\n\n\tcase SPECTRE_V2_CMD_EIBRS:\n\t\tmode = SPECTRE_V2_EIBRS;\n\t\tbreak;\n\n\tcase SPECTRE_V2_CMD_EIBRS_LFENCE:\n\t\tmode = SPECTRE_V2_EIBRS_LFENCE;\n\t\tbreak;\n\n\tcase SPECTRE_V2_CMD_EIBRS_RETPOLINE:\n\t\tmode = SPECTRE_V2_EIBRS_RETPOLINE;\n\t\tbreak;\n\t}\n\n\tif (mode == SPECTRE_V2_EIBRS && unprivileged_ebpf_enabled())\n\t\tpr_err(SPECTRE_V2_EIBRS_EBPF_MSG);\n\n\tif (spectre_v2_in_ibrs_mode(mode)) {\n\t\tif (boot_cpu_has(X86_FEATURE_AUTOIBRS)) {\n\t\t\tmsr_set_bit(MSR_EFER, _EFER_AUTOIBRS);\n\t\t} else {\n\t\t\tx86_spec_ctrl_base |= SPEC_CTRL_IBRS;\n\t\t\tupdate_spec_ctrl(x86_spec_ctrl_base);\n\t\t}\n\t}\n\n\tswitch (mode) {\n\tcase SPECTRE_V2_NONE:\n\tcase SPECTRE_V2_EIBRS:\n\t\tbreak;\n\n\tcase SPECTRE_V2_IBRS:\n\t\tsetup_force_cpu_cap(X86_FEATURE_KERNEL_IBRS);\n\t\tif (boot_cpu_has(X86_FEATURE_IBRS_ENHANCED))\n\t\t\tpr_warn(SPECTRE_V2_IBRS_PERF_MSG);\n\t\tbreak;\n\n\tcase SPECTRE_V2_LFENCE:\n\tcase SPECTRE_V2_EIBRS_LFENCE:\n\t\tsetup_force_cpu_cap(X86_FEATURE_RETPOLINE_LFENCE);\n\t\tfallthrough;\n\n\tcase SPECTRE_V2_RETPOLINE:\n\tcase SPECTRE_V2_EIBRS_RETPOLINE:\n\t\tsetup_force_cpu_cap(X86_FEATURE_RETPOLINE);\n\t\tbreak;\n\t}\n\n\t \n\tif (mode == SPECTRE_V2_EIBRS_LFENCE ||\n\t    mode == SPECTRE_V2_EIBRS_RETPOLINE ||\n\t    mode == SPECTRE_V2_RETPOLINE)\n\t\tspec_ctrl_disable_kernel_rrsba();\n\n\tspectre_v2_enabled = mode;\n\tpr_info(\"%s\\n\", spectre_v2_strings[mode]);\n\n\t \n\tsetup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);\n\tpr_info(\"Spectre v2 / SpectreRSB mitigation: Filling RSB on context switch\\n\");\n\n\tspectre_v2_determine_rsb_fill_type_at_vmexit(mode);\n\n\t \n\tif (boot_cpu_has_bug(X86_BUG_RETBLEED) &&\n\t    boot_cpu_has(X86_FEATURE_IBPB) &&\n\t    (boot_cpu_data.x86_vendor == X86_VENDOR_AMD ||\n\t     boot_cpu_data.x86_vendor == X86_VENDOR_HYGON)) {\n\n\t\tif (retbleed_cmd != RETBLEED_CMD_IBPB) {\n\t\t\tsetup_force_cpu_cap(X86_FEATURE_USE_IBPB_FW);\n\t\t\tpr_info(\"Enabling Speculation Barrier for firmware calls\\n\");\n\t\t}\n\n\t} else if (boot_cpu_has(X86_FEATURE_IBRS) && !spectre_v2_in_ibrs_mode(mode)) {\n\t\tsetup_force_cpu_cap(X86_FEATURE_USE_IBRS_FW);\n\t\tpr_info(\"Enabling Restricted Speculation for firmware calls\\n\");\n\t}\n\n\t \n\tspectre_v2_cmd = cmd;\n}\n\nstatic void update_stibp_msr(void * __unused)\n{\n\tu64 val = spec_ctrl_current() | (x86_spec_ctrl_base & SPEC_CTRL_STIBP);\n\tupdate_spec_ctrl(val);\n}\n\n \nstatic void update_stibp_strict(void)\n{\n\tu64 mask = x86_spec_ctrl_base & ~SPEC_CTRL_STIBP;\n\n\tif (sched_smt_active())\n\t\tmask |= SPEC_CTRL_STIBP;\n\n\tif (mask == x86_spec_ctrl_base)\n\t\treturn;\n\n\tpr_info(\"Update user space SMT mitigation: STIBP %s\\n\",\n\t\tmask & SPEC_CTRL_STIBP ? \"always-on\" : \"off\");\n\tx86_spec_ctrl_base = mask;\n\ton_each_cpu(update_stibp_msr, NULL, 1);\n}\n\n \nstatic void update_indir_branch_cond(void)\n{\n\tif (sched_smt_active())\n\t\tstatic_branch_enable(&switch_to_cond_stibp);\n\telse\n\t\tstatic_branch_disable(&switch_to_cond_stibp);\n}\n\n#undef pr_fmt\n#define pr_fmt(fmt) fmt\n\n \nstatic void update_mds_branch_idle(void)\n{\n\tu64 ia32_cap = x86_read_arch_cap_msr();\n\n\t \n\tif (!boot_cpu_has_bug(X86_BUG_MSBDS_ONLY))\n\t\treturn;\n\n\tif (sched_smt_active()) {\n\t\tstatic_branch_enable(&mds_idle_clear);\n\t} else if (mmio_mitigation == MMIO_MITIGATION_OFF ||\n\t\t   (ia32_cap & ARCH_CAP_FBSDP_NO)) {\n\t\tstatic_branch_disable(&mds_idle_clear);\n\t}\n}\n\n#define MDS_MSG_SMT \"MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.\\n\"\n#define TAA_MSG_SMT \"TAA CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/tsx_async_abort.html for more details.\\n\"\n#define MMIO_MSG_SMT \"MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.\\n\"\n\nvoid cpu_bugs_smt_update(void)\n{\n\tmutex_lock(&spec_ctrl_mutex);\n\n\tif (sched_smt_active() && unprivileged_ebpf_enabled() &&\n\t    spectre_v2_enabled == SPECTRE_V2_EIBRS_LFENCE)\n\t\tpr_warn_once(SPECTRE_V2_EIBRS_LFENCE_EBPF_SMT_MSG);\n\n\tswitch (spectre_v2_user_stibp) {\n\tcase SPECTRE_V2_USER_NONE:\n\t\tbreak;\n\tcase SPECTRE_V2_USER_STRICT:\n\tcase SPECTRE_V2_USER_STRICT_PREFERRED:\n\t\tupdate_stibp_strict();\n\t\tbreak;\n\tcase SPECTRE_V2_USER_PRCTL:\n\tcase SPECTRE_V2_USER_SECCOMP:\n\t\tupdate_indir_branch_cond();\n\t\tbreak;\n\t}\n\n\tswitch (mds_mitigation) {\n\tcase MDS_MITIGATION_FULL:\n\tcase MDS_MITIGATION_VMWERV:\n\t\tif (sched_smt_active() && !boot_cpu_has(X86_BUG_MSBDS_ONLY))\n\t\t\tpr_warn_once(MDS_MSG_SMT);\n\t\tupdate_mds_branch_idle();\n\t\tbreak;\n\tcase MDS_MITIGATION_OFF:\n\t\tbreak;\n\t}\n\n\tswitch (taa_mitigation) {\n\tcase TAA_MITIGATION_VERW:\n\tcase TAA_MITIGATION_UCODE_NEEDED:\n\t\tif (sched_smt_active())\n\t\t\tpr_warn_once(TAA_MSG_SMT);\n\t\tbreak;\n\tcase TAA_MITIGATION_TSX_DISABLED:\n\tcase TAA_MITIGATION_OFF:\n\t\tbreak;\n\t}\n\n\tswitch (mmio_mitigation) {\n\tcase MMIO_MITIGATION_VERW:\n\tcase MMIO_MITIGATION_UCODE_NEEDED:\n\t\tif (sched_smt_active())\n\t\t\tpr_warn_once(MMIO_MSG_SMT);\n\t\tbreak;\n\tcase MMIO_MITIGATION_OFF:\n\t\tbreak;\n\t}\n\n\tmutex_unlock(&spec_ctrl_mutex);\n}\n\n#undef pr_fmt\n#define pr_fmt(fmt)\t\"Speculative Store Bypass: \" fmt\n\nstatic enum ssb_mitigation ssb_mode __ro_after_init = SPEC_STORE_BYPASS_NONE;\n\n \nenum ssb_mitigation_cmd {\n\tSPEC_STORE_BYPASS_CMD_NONE,\n\tSPEC_STORE_BYPASS_CMD_AUTO,\n\tSPEC_STORE_BYPASS_CMD_ON,\n\tSPEC_STORE_BYPASS_CMD_PRCTL,\n\tSPEC_STORE_BYPASS_CMD_SECCOMP,\n};\n\nstatic const char * const ssb_strings[] = {\n\t[SPEC_STORE_BYPASS_NONE]\t= \"Vulnerable\",\n\t[SPEC_STORE_BYPASS_DISABLE]\t= \"Mitigation: Speculative Store Bypass disabled\",\n\t[SPEC_STORE_BYPASS_PRCTL]\t= \"Mitigation: Speculative Store Bypass disabled via prctl\",\n\t[SPEC_STORE_BYPASS_SECCOMP]\t= \"Mitigation: Speculative Store Bypass disabled via prctl and seccomp\",\n};\n\nstatic const struct {\n\tconst char *option;\n\tenum ssb_mitigation_cmd cmd;\n} ssb_mitigation_options[]  __initconst = {\n\t{ \"auto\",\tSPEC_STORE_BYPASS_CMD_AUTO },     \n\t{ \"on\",\t\tSPEC_STORE_BYPASS_CMD_ON },       \n\t{ \"off\",\tSPEC_STORE_BYPASS_CMD_NONE },     \n\t{ \"prctl\",\tSPEC_STORE_BYPASS_CMD_PRCTL },    \n\t{ \"seccomp\",\tSPEC_STORE_BYPASS_CMD_SECCOMP },  \n};\n\nstatic enum ssb_mitigation_cmd __init ssb_parse_cmdline(void)\n{\n\tenum ssb_mitigation_cmd cmd = SPEC_STORE_BYPASS_CMD_AUTO;\n\tchar arg[20];\n\tint ret, i;\n\n\tif (cmdline_find_option_bool(boot_command_line, \"nospec_store_bypass_disable\") ||\n\t    cpu_mitigations_off()) {\n\t\treturn SPEC_STORE_BYPASS_CMD_NONE;\n\t} else {\n\t\tret = cmdline_find_option(boot_command_line, \"spec_store_bypass_disable\",\n\t\t\t\t\t  arg, sizeof(arg));\n\t\tif (ret < 0)\n\t\t\treturn SPEC_STORE_BYPASS_CMD_AUTO;\n\n\t\tfor (i = 0; i < ARRAY_SIZE(ssb_mitigation_options); i++) {\n\t\t\tif (!match_option(arg, ret, ssb_mitigation_options[i].option))\n\t\t\t\tcontinue;\n\n\t\t\tcmd = ssb_mitigation_options[i].cmd;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (i >= ARRAY_SIZE(ssb_mitigation_options)) {\n\t\t\tpr_err(\"unknown option (%s). Switching to AUTO select\\n\", arg);\n\t\t\treturn SPEC_STORE_BYPASS_CMD_AUTO;\n\t\t}\n\t}\n\n\treturn cmd;\n}\n\nstatic enum ssb_mitigation __init __ssb_select_mitigation(void)\n{\n\tenum ssb_mitigation mode = SPEC_STORE_BYPASS_NONE;\n\tenum ssb_mitigation_cmd cmd;\n\n\tif (!boot_cpu_has(X86_FEATURE_SSBD))\n\t\treturn mode;\n\n\tcmd = ssb_parse_cmdline();\n\tif (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS) &&\n\t    (cmd == SPEC_STORE_BYPASS_CMD_NONE ||\n\t     cmd == SPEC_STORE_BYPASS_CMD_AUTO))\n\t\treturn mode;\n\n\tswitch (cmd) {\n\tcase SPEC_STORE_BYPASS_CMD_SECCOMP:\n\t\t \n\t\tif (IS_ENABLED(CONFIG_SECCOMP))\n\t\t\tmode = SPEC_STORE_BYPASS_SECCOMP;\n\t\telse\n\t\t\tmode = SPEC_STORE_BYPASS_PRCTL;\n\t\tbreak;\n\tcase SPEC_STORE_BYPASS_CMD_ON:\n\t\tmode = SPEC_STORE_BYPASS_DISABLE;\n\t\tbreak;\n\tcase SPEC_STORE_BYPASS_CMD_AUTO:\n\tcase SPEC_STORE_BYPASS_CMD_PRCTL:\n\t\tmode = SPEC_STORE_BYPASS_PRCTL;\n\t\tbreak;\n\tcase SPEC_STORE_BYPASS_CMD_NONE:\n\t\tbreak;\n\t}\n\n\t \n\tif (mode == SPEC_STORE_BYPASS_DISABLE) {\n\t\tsetup_force_cpu_cap(X86_FEATURE_SPEC_STORE_BYPASS_DISABLE);\n\t\t \n\t\tif (!static_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD) &&\n\t\t    !static_cpu_has(X86_FEATURE_AMD_SSBD)) {\n\t\t\tx86_amd_ssb_disable();\n\t\t} else {\n\t\t\tx86_spec_ctrl_base |= SPEC_CTRL_SSBD;\n\t\t\tupdate_spec_ctrl(x86_spec_ctrl_base);\n\t\t}\n\t}\n\n\treturn mode;\n}\n\nstatic void ssb_select_mitigation(void)\n{\n\tssb_mode = __ssb_select_mitigation();\n\n\tif (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))\n\t\tpr_info(\"%s\\n\", ssb_strings[ssb_mode]);\n}\n\n#undef pr_fmt\n#define pr_fmt(fmt)     \"Speculation prctl: \" fmt\n\nstatic void task_update_spec_tif(struct task_struct *tsk)\n{\n\t \n\tset_tsk_thread_flag(tsk, TIF_SPEC_FORCE_UPDATE);\n\n\t \n\tif (tsk == current)\n\t\tspeculation_ctrl_update_current();\n}\n\nstatic int l1d_flush_prctl_set(struct task_struct *task, unsigned long ctrl)\n{\n\n\tif (!static_branch_unlikely(&switch_mm_cond_l1d_flush))\n\t\treturn -EPERM;\n\n\tswitch (ctrl) {\n\tcase PR_SPEC_ENABLE:\n\t\tset_ti_thread_flag(&task->thread_info, TIF_SPEC_L1D_FLUSH);\n\t\treturn 0;\n\tcase PR_SPEC_DISABLE:\n\t\tclear_ti_thread_flag(&task->thread_info, TIF_SPEC_L1D_FLUSH);\n\t\treturn 0;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n}\n\nstatic int ssb_prctl_set(struct task_struct *task, unsigned long ctrl)\n{\n\tif (ssb_mode != SPEC_STORE_BYPASS_PRCTL &&\n\t    ssb_mode != SPEC_STORE_BYPASS_SECCOMP)\n\t\treturn -ENXIO;\n\n\tswitch (ctrl) {\n\tcase PR_SPEC_ENABLE:\n\t\t \n\t\tif (task_spec_ssb_force_disable(task))\n\t\t\treturn -EPERM;\n\t\ttask_clear_spec_ssb_disable(task);\n\t\ttask_clear_spec_ssb_noexec(task);\n\t\ttask_update_spec_tif(task);\n\t\tbreak;\n\tcase PR_SPEC_DISABLE:\n\t\ttask_set_spec_ssb_disable(task);\n\t\ttask_clear_spec_ssb_noexec(task);\n\t\ttask_update_spec_tif(task);\n\t\tbreak;\n\tcase PR_SPEC_FORCE_DISABLE:\n\t\ttask_set_spec_ssb_disable(task);\n\t\ttask_set_spec_ssb_force_disable(task);\n\t\ttask_clear_spec_ssb_noexec(task);\n\t\ttask_update_spec_tif(task);\n\t\tbreak;\n\tcase PR_SPEC_DISABLE_NOEXEC:\n\t\tif (task_spec_ssb_force_disable(task))\n\t\t\treturn -EPERM;\n\t\ttask_set_spec_ssb_disable(task);\n\t\ttask_set_spec_ssb_noexec(task);\n\t\ttask_update_spec_tif(task);\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\treturn 0;\n}\n\nstatic bool is_spec_ib_user_controlled(void)\n{\n\treturn spectre_v2_user_ibpb == SPECTRE_V2_USER_PRCTL ||\n\t\tspectre_v2_user_ibpb == SPECTRE_V2_USER_SECCOMP ||\n\t\tspectre_v2_user_stibp == SPECTRE_V2_USER_PRCTL ||\n\t\tspectre_v2_user_stibp == SPECTRE_V2_USER_SECCOMP;\n}\n\nstatic int ib_prctl_set(struct task_struct *task, unsigned long ctrl)\n{\n\tswitch (ctrl) {\n\tcase PR_SPEC_ENABLE:\n\t\tif (spectre_v2_user_ibpb == SPECTRE_V2_USER_NONE &&\n\t\t    spectre_v2_user_stibp == SPECTRE_V2_USER_NONE)\n\t\t\treturn 0;\n\n\t\t \n\t\tif (!is_spec_ib_user_controlled() ||\n\t\t    task_spec_ib_force_disable(task))\n\t\t\treturn -EPERM;\n\n\t\ttask_clear_spec_ib_disable(task);\n\t\ttask_update_spec_tif(task);\n\t\tbreak;\n\tcase PR_SPEC_DISABLE:\n\tcase PR_SPEC_FORCE_DISABLE:\n\t\t \n\t\tif (spectre_v2_user_ibpb == SPECTRE_V2_USER_NONE &&\n\t\t    spectre_v2_user_stibp == SPECTRE_V2_USER_NONE)\n\t\t\treturn -EPERM;\n\n\t\tif (!is_spec_ib_user_controlled())\n\t\t\treturn 0;\n\n\t\ttask_set_spec_ib_disable(task);\n\t\tif (ctrl == PR_SPEC_FORCE_DISABLE)\n\t\t\ttask_set_spec_ib_force_disable(task);\n\t\ttask_update_spec_tif(task);\n\t\tif (task == current)\n\t\t\tindirect_branch_prediction_barrier();\n\t\tbreak;\n\tdefault:\n\t\treturn -ERANGE;\n\t}\n\treturn 0;\n}\n\nint arch_prctl_spec_ctrl_set(struct task_struct *task, unsigned long which,\n\t\t\t     unsigned long ctrl)\n{\n\tswitch (which) {\n\tcase PR_SPEC_STORE_BYPASS:\n\t\treturn ssb_prctl_set(task, ctrl);\n\tcase PR_SPEC_INDIRECT_BRANCH:\n\t\treturn ib_prctl_set(task, ctrl);\n\tcase PR_SPEC_L1D_FLUSH:\n\t\treturn l1d_flush_prctl_set(task, ctrl);\n\tdefault:\n\t\treturn -ENODEV;\n\t}\n}\n\n#ifdef CONFIG_SECCOMP\nvoid arch_seccomp_spec_mitigate(struct task_struct *task)\n{\n\tif (ssb_mode == SPEC_STORE_BYPASS_SECCOMP)\n\t\tssb_prctl_set(task, PR_SPEC_FORCE_DISABLE);\n\tif (spectre_v2_user_ibpb == SPECTRE_V2_USER_SECCOMP ||\n\t    spectre_v2_user_stibp == SPECTRE_V2_USER_SECCOMP)\n\t\tib_prctl_set(task, PR_SPEC_FORCE_DISABLE);\n}\n#endif\n\nstatic int l1d_flush_prctl_get(struct task_struct *task)\n{\n\tif (!static_branch_unlikely(&switch_mm_cond_l1d_flush))\n\t\treturn PR_SPEC_FORCE_DISABLE;\n\n\tif (test_ti_thread_flag(&task->thread_info, TIF_SPEC_L1D_FLUSH))\n\t\treturn PR_SPEC_PRCTL | PR_SPEC_ENABLE;\n\telse\n\t\treturn PR_SPEC_PRCTL | PR_SPEC_DISABLE;\n}\n\nstatic int ssb_prctl_get(struct task_struct *task)\n{\n\tswitch (ssb_mode) {\n\tcase SPEC_STORE_BYPASS_DISABLE:\n\t\treturn PR_SPEC_DISABLE;\n\tcase SPEC_STORE_BYPASS_SECCOMP:\n\tcase SPEC_STORE_BYPASS_PRCTL:\n\t\tif (task_spec_ssb_force_disable(task))\n\t\t\treturn PR_SPEC_PRCTL | PR_SPEC_FORCE_DISABLE;\n\t\tif (task_spec_ssb_noexec(task))\n\t\t\treturn PR_SPEC_PRCTL | PR_SPEC_DISABLE_NOEXEC;\n\t\tif (task_spec_ssb_disable(task))\n\t\t\treturn PR_SPEC_PRCTL | PR_SPEC_DISABLE;\n\t\treturn PR_SPEC_PRCTL | PR_SPEC_ENABLE;\n\tdefault:\n\t\tif (boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))\n\t\t\treturn PR_SPEC_ENABLE;\n\t\treturn PR_SPEC_NOT_AFFECTED;\n\t}\n}\n\nstatic int ib_prctl_get(struct task_struct *task)\n{\n\tif (!boot_cpu_has_bug(X86_BUG_SPECTRE_V2))\n\t\treturn PR_SPEC_NOT_AFFECTED;\n\n\tif (spectre_v2_user_ibpb == SPECTRE_V2_USER_NONE &&\n\t    spectre_v2_user_stibp == SPECTRE_V2_USER_NONE)\n\t\treturn PR_SPEC_ENABLE;\n\telse if (is_spec_ib_user_controlled()) {\n\t\tif (task_spec_ib_force_disable(task))\n\t\t\treturn PR_SPEC_PRCTL | PR_SPEC_FORCE_DISABLE;\n\t\tif (task_spec_ib_disable(task))\n\t\t\treturn PR_SPEC_PRCTL | PR_SPEC_DISABLE;\n\t\treturn PR_SPEC_PRCTL | PR_SPEC_ENABLE;\n\t} else if (spectre_v2_user_ibpb == SPECTRE_V2_USER_STRICT ||\n\t    spectre_v2_user_stibp == SPECTRE_V2_USER_STRICT ||\n\t    spectre_v2_user_stibp == SPECTRE_V2_USER_STRICT_PREFERRED)\n\t\treturn PR_SPEC_DISABLE;\n\telse\n\t\treturn PR_SPEC_NOT_AFFECTED;\n}\n\nint arch_prctl_spec_ctrl_get(struct task_struct *task, unsigned long which)\n{\n\tswitch (which) {\n\tcase PR_SPEC_STORE_BYPASS:\n\t\treturn ssb_prctl_get(task);\n\tcase PR_SPEC_INDIRECT_BRANCH:\n\t\treturn ib_prctl_get(task);\n\tcase PR_SPEC_L1D_FLUSH:\n\t\treturn l1d_flush_prctl_get(task);\n\tdefault:\n\t\treturn -ENODEV;\n\t}\n}\n\nvoid x86_spec_ctrl_setup_ap(void)\n{\n\tif (boot_cpu_has(X86_FEATURE_MSR_SPEC_CTRL))\n\t\tupdate_spec_ctrl(x86_spec_ctrl_base);\n\n\tif (ssb_mode == SPEC_STORE_BYPASS_DISABLE)\n\t\tx86_amd_ssb_disable();\n}\n\nbool itlb_multihit_kvm_mitigation;\nEXPORT_SYMBOL_GPL(itlb_multihit_kvm_mitigation);\n\n#undef pr_fmt\n#define pr_fmt(fmt)\t\"L1TF: \" fmt\n\n \nenum l1tf_mitigations l1tf_mitigation __ro_after_init = L1TF_MITIGATION_FLUSH;\n#if IS_ENABLED(CONFIG_KVM_INTEL)\nEXPORT_SYMBOL_GPL(l1tf_mitigation);\n#endif\nenum vmx_l1d_flush_state l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;\nEXPORT_SYMBOL_GPL(l1tf_vmx_mitigation);\n\n \nstatic void override_cache_bits(struct cpuinfo_x86 *c)\n{\n\tif (c->x86 != 6)\n\t\treturn;\n\n\tswitch (c->x86_model) {\n\tcase INTEL_FAM6_NEHALEM:\n\tcase INTEL_FAM6_WESTMERE:\n\tcase INTEL_FAM6_SANDYBRIDGE:\n\tcase INTEL_FAM6_IVYBRIDGE:\n\tcase INTEL_FAM6_HASWELL:\n\tcase INTEL_FAM6_HASWELL_L:\n\tcase INTEL_FAM6_HASWELL_G:\n\tcase INTEL_FAM6_BROADWELL:\n\tcase INTEL_FAM6_BROADWELL_G:\n\tcase INTEL_FAM6_SKYLAKE_L:\n\tcase INTEL_FAM6_SKYLAKE:\n\tcase INTEL_FAM6_KABYLAKE_L:\n\tcase INTEL_FAM6_KABYLAKE:\n\t\tif (c->x86_cache_bits < 44)\n\t\t\tc->x86_cache_bits = 44;\n\t\tbreak;\n\t}\n}\n\nstatic void __init l1tf_select_mitigation(void)\n{\n\tu64 half_pa;\n\n\tif (!boot_cpu_has_bug(X86_BUG_L1TF))\n\t\treturn;\n\n\tif (cpu_mitigations_off())\n\t\tl1tf_mitigation = L1TF_MITIGATION_OFF;\n\telse if (cpu_mitigations_auto_nosmt())\n\t\tl1tf_mitigation = L1TF_MITIGATION_FLUSH_NOSMT;\n\n\toverride_cache_bits(&boot_cpu_data);\n\n\tswitch (l1tf_mitigation) {\n\tcase L1TF_MITIGATION_OFF:\n\tcase L1TF_MITIGATION_FLUSH_NOWARN:\n\tcase L1TF_MITIGATION_FLUSH:\n\t\tbreak;\n\tcase L1TF_MITIGATION_FLUSH_NOSMT:\n\tcase L1TF_MITIGATION_FULL:\n\t\tcpu_smt_disable(false);\n\t\tbreak;\n\tcase L1TF_MITIGATION_FULL_FORCE:\n\t\tcpu_smt_disable(true);\n\t\tbreak;\n\t}\n\n#if CONFIG_PGTABLE_LEVELS == 2\n\tpr_warn(\"Kernel not compiled for PAE. No mitigation for L1TF\\n\");\n\treturn;\n#endif\n\n\thalf_pa = (u64)l1tf_pfn_limit() << PAGE_SHIFT;\n\tif (l1tf_mitigation != L1TF_MITIGATION_OFF &&\n\t\t\te820__mapped_any(half_pa, ULLONG_MAX - half_pa, E820_TYPE_RAM)) {\n\t\tpr_warn(\"System has more than MAX_PA/2 memory. L1TF mitigation not effective.\\n\");\n\t\tpr_info(\"You may make it effective by booting the kernel with mem=%llu parameter.\\n\",\n\t\t\t\thalf_pa);\n\t\tpr_info(\"However, doing so will make a part of your RAM unusable.\\n\");\n\t\tpr_info(\"Reading https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/l1tf.html might help you decide.\\n\");\n\t\treturn;\n\t}\n\n\tsetup_force_cpu_cap(X86_FEATURE_L1TF_PTEINV);\n}\n\nstatic int __init l1tf_cmdline(char *str)\n{\n\tif (!boot_cpu_has_bug(X86_BUG_L1TF))\n\t\treturn 0;\n\n\tif (!str)\n\t\treturn -EINVAL;\n\n\tif (!strcmp(str, \"off\"))\n\t\tl1tf_mitigation = L1TF_MITIGATION_OFF;\n\telse if (!strcmp(str, \"flush,nowarn\"))\n\t\tl1tf_mitigation = L1TF_MITIGATION_FLUSH_NOWARN;\n\telse if (!strcmp(str, \"flush\"))\n\t\tl1tf_mitigation = L1TF_MITIGATION_FLUSH;\n\telse if (!strcmp(str, \"flush,nosmt\"))\n\t\tl1tf_mitigation = L1TF_MITIGATION_FLUSH_NOSMT;\n\telse if (!strcmp(str, \"full\"))\n\t\tl1tf_mitigation = L1TF_MITIGATION_FULL;\n\telse if (!strcmp(str, \"full,force\"))\n\t\tl1tf_mitigation = L1TF_MITIGATION_FULL_FORCE;\n\n\treturn 0;\n}\nearly_param(\"l1tf\", l1tf_cmdline);\n\n#undef pr_fmt\n#define pr_fmt(fmt)\t\"Speculative Return Stack Overflow: \" fmt\n\nenum srso_mitigation {\n\tSRSO_MITIGATION_NONE,\n\tSRSO_MITIGATION_UCODE_NEEDED,\n\tSRSO_MITIGATION_SAFE_RET_UCODE_NEEDED,\n\tSRSO_MITIGATION_MICROCODE,\n\tSRSO_MITIGATION_SAFE_RET,\n\tSRSO_MITIGATION_IBPB,\n\tSRSO_MITIGATION_IBPB_ON_VMEXIT,\n};\n\nenum srso_mitigation_cmd {\n\tSRSO_CMD_OFF,\n\tSRSO_CMD_MICROCODE,\n\tSRSO_CMD_SAFE_RET,\n\tSRSO_CMD_IBPB,\n\tSRSO_CMD_IBPB_ON_VMEXIT,\n};\n\nstatic const char * const srso_strings[] = {\n\t[SRSO_MITIGATION_NONE]\t\t\t= \"Vulnerable\",\n\t[SRSO_MITIGATION_UCODE_NEEDED]\t\t= \"Vulnerable: No microcode\",\n\t[SRSO_MITIGATION_SAFE_RET_UCODE_NEEDED]\t= \"Vulnerable: Safe RET, no microcode\",\n\t[SRSO_MITIGATION_MICROCODE]\t\t= \"Vulnerable: Microcode, no safe RET\",\n\t[SRSO_MITIGATION_SAFE_RET]\t\t= \"Mitigation: Safe RET\",\n\t[SRSO_MITIGATION_IBPB]\t\t\t= \"Mitigation: IBPB\",\n\t[SRSO_MITIGATION_IBPB_ON_VMEXIT]\t= \"Mitigation: IBPB on VMEXIT only\"\n};\n\nstatic enum srso_mitigation srso_mitigation __ro_after_init = SRSO_MITIGATION_NONE;\nstatic enum srso_mitigation_cmd srso_cmd __ro_after_init = SRSO_CMD_SAFE_RET;\n\nstatic int __init srso_parse_cmdline(char *str)\n{\n\tif (!str)\n\t\treturn -EINVAL;\n\n\tif (!strcmp(str, \"off\"))\n\t\tsrso_cmd = SRSO_CMD_OFF;\n\telse if (!strcmp(str, \"microcode\"))\n\t\tsrso_cmd = SRSO_CMD_MICROCODE;\n\telse if (!strcmp(str, \"safe-ret\"))\n\t\tsrso_cmd = SRSO_CMD_SAFE_RET;\n\telse if (!strcmp(str, \"ibpb\"))\n\t\tsrso_cmd = SRSO_CMD_IBPB;\n\telse if (!strcmp(str, \"ibpb-vmexit\"))\n\t\tsrso_cmd = SRSO_CMD_IBPB_ON_VMEXIT;\n\telse\n\t\tpr_err(\"Ignoring unknown SRSO option (%s).\", str);\n\n\treturn 0;\n}\nearly_param(\"spec_rstack_overflow\", srso_parse_cmdline);\n\n#define SRSO_NOTICE \"WARNING: See https://kernel.org/doc/html/latest/admin-guide/hw-vuln/srso.html for mitigation options.\"\n\nstatic void __init srso_select_mitigation(void)\n{\n\tbool has_microcode = boot_cpu_has(X86_FEATURE_IBPB_BRTYPE);\n\n\tif (!boot_cpu_has_bug(X86_BUG_SRSO) || cpu_mitigations_off())\n\t\tgoto pred_cmd;\n\n\tif (has_microcode) {\n\t\t \n\t\tif (boot_cpu_data.x86 < 0x19 && !cpu_smt_possible()) {\n\t\t\tsetup_force_cpu_cap(X86_FEATURE_SRSO_NO);\n\t\t\treturn;\n\t\t}\n\n\t\tif (retbleed_mitigation == RETBLEED_MITIGATION_IBPB) {\n\t\t\tsrso_mitigation = SRSO_MITIGATION_IBPB;\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tpr_warn(\"IBPB-extending microcode not applied!\\n\");\n\t\tpr_warn(SRSO_NOTICE);\n\n\t\t \n\t\tsrso_mitigation = SRSO_MITIGATION_UCODE_NEEDED;\n\t}\n\n\tswitch (srso_cmd) {\n\tcase SRSO_CMD_OFF:\n\t\tgoto pred_cmd;\n\n\tcase SRSO_CMD_MICROCODE:\n\t\tif (has_microcode) {\n\t\t\tsrso_mitigation = SRSO_MITIGATION_MICROCODE;\n\t\t\tpr_warn(SRSO_NOTICE);\n\t\t}\n\t\tbreak;\n\n\tcase SRSO_CMD_SAFE_RET:\n\t\tif (IS_ENABLED(CONFIG_CPU_SRSO)) {\n\t\t\t \n\t\t\tsetup_force_cpu_cap(X86_FEATURE_RETHUNK);\n\t\t\tsetup_force_cpu_cap(X86_FEATURE_UNRET);\n\n\t\t\tif (boot_cpu_data.x86 == 0x19) {\n\t\t\t\tsetup_force_cpu_cap(X86_FEATURE_SRSO_ALIAS);\n\t\t\t\tx86_return_thunk = srso_alias_return_thunk;\n\t\t\t} else {\n\t\t\t\tsetup_force_cpu_cap(X86_FEATURE_SRSO);\n\t\t\t\tx86_return_thunk = srso_return_thunk;\n\t\t\t}\n\t\t\tif (has_microcode)\n\t\t\t\tsrso_mitigation = SRSO_MITIGATION_SAFE_RET;\n\t\t\telse\n\t\t\t\tsrso_mitigation = SRSO_MITIGATION_SAFE_RET_UCODE_NEEDED;\n\t\t} else {\n\t\t\tpr_err(\"WARNING: kernel not compiled with CPU_SRSO.\\n\");\n\t\t\tgoto pred_cmd;\n\t\t}\n\t\tbreak;\n\n\tcase SRSO_CMD_IBPB:\n\t\tif (IS_ENABLED(CONFIG_CPU_IBPB_ENTRY)) {\n\t\t\tif (has_microcode) {\n\t\t\t\tsetup_force_cpu_cap(X86_FEATURE_ENTRY_IBPB);\n\t\t\t\tsrso_mitigation = SRSO_MITIGATION_IBPB;\n\t\t\t}\n\t\t} else {\n\t\t\tpr_err(\"WARNING: kernel not compiled with CPU_IBPB_ENTRY.\\n\");\n\t\t\tgoto pred_cmd;\n\t\t}\n\t\tbreak;\n\n\tcase SRSO_CMD_IBPB_ON_VMEXIT:\n\t\tif (IS_ENABLED(CONFIG_CPU_SRSO)) {\n\t\t\tif (!boot_cpu_has(X86_FEATURE_ENTRY_IBPB) && has_microcode) {\n\t\t\t\tsetup_force_cpu_cap(X86_FEATURE_IBPB_ON_VMEXIT);\n\t\t\t\tsrso_mitigation = SRSO_MITIGATION_IBPB_ON_VMEXIT;\n\t\t\t}\n\t\t} else {\n\t\t\tpr_err(\"WARNING: kernel not compiled with CPU_SRSO.\\n\");\n\t\t\tgoto pred_cmd;\n                }\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\nout:\n\tpr_info(\"%s\\n\", srso_strings[srso_mitigation]);\n\npred_cmd:\n\tif ((!boot_cpu_has_bug(X86_BUG_SRSO) || srso_cmd == SRSO_CMD_OFF) &&\n\t     boot_cpu_has(X86_FEATURE_SBPB))\n\t\tx86_pred_cmd = PRED_CMD_SBPB;\n}\n\n#undef pr_fmt\n#define pr_fmt(fmt) fmt\n\n#ifdef CONFIG_SYSFS\n\n#define L1TF_DEFAULT_MSG \"Mitigation: PTE Inversion\"\n\n#if IS_ENABLED(CONFIG_KVM_INTEL)\nstatic const char * const l1tf_vmx_states[] = {\n\t[VMENTER_L1D_FLUSH_AUTO]\t\t= \"auto\",\n\t[VMENTER_L1D_FLUSH_NEVER]\t\t= \"vulnerable\",\n\t[VMENTER_L1D_FLUSH_COND]\t\t= \"conditional cache flushes\",\n\t[VMENTER_L1D_FLUSH_ALWAYS]\t\t= \"cache flushes\",\n\t[VMENTER_L1D_FLUSH_EPT_DISABLED]\t= \"EPT disabled\",\n\t[VMENTER_L1D_FLUSH_NOT_REQUIRED]\t= \"flush not necessary\"\n};\n\nstatic ssize_t l1tf_show_state(char *buf)\n{\n\tif (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO)\n\t\treturn sysfs_emit(buf, \"%s\\n\", L1TF_DEFAULT_MSG);\n\n\tif (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_EPT_DISABLED ||\n\t    (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER &&\n\t     sched_smt_active())) {\n\t\treturn sysfs_emit(buf, \"%s; VMX: %s\\n\", L1TF_DEFAULT_MSG,\n\t\t\t\t  l1tf_vmx_states[l1tf_vmx_mitigation]);\n\t}\n\n\treturn sysfs_emit(buf, \"%s; VMX: %s, SMT %s\\n\", L1TF_DEFAULT_MSG,\n\t\t\t  l1tf_vmx_states[l1tf_vmx_mitigation],\n\t\t\t  sched_smt_active() ? \"vulnerable\" : \"disabled\");\n}\n\nstatic ssize_t itlb_multihit_show_state(char *buf)\n{\n\tif (!boot_cpu_has(X86_FEATURE_MSR_IA32_FEAT_CTL) ||\n\t    !boot_cpu_has(X86_FEATURE_VMX))\n\t\treturn sysfs_emit(buf, \"KVM: Mitigation: VMX unsupported\\n\");\n\telse if (!(cr4_read_shadow() & X86_CR4_VMXE))\n\t\treturn sysfs_emit(buf, \"KVM: Mitigation: VMX disabled\\n\");\n\telse if (itlb_multihit_kvm_mitigation)\n\t\treturn sysfs_emit(buf, \"KVM: Mitigation: Split huge pages\\n\");\n\telse\n\t\treturn sysfs_emit(buf, \"KVM: Vulnerable\\n\");\n}\n#else\nstatic ssize_t l1tf_show_state(char *buf)\n{\n\treturn sysfs_emit(buf, \"%s\\n\", L1TF_DEFAULT_MSG);\n}\n\nstatic ssize_t itlb_multihit_show_state(char *buf)\n{\n\treturn sysfs_emit(buf, \"Processor vulnerable\\n\");\n}\n#endif\n\nstatic ssize_t mds_show_state(char *buf)\n{\n\tif (boot_cpu_has(X86_FEATURE_HYPERVISOR)) {\n\t\treturn sysfs_emit(buf, \"%s; SMT Host state unknown\\n\",\n\t\t\t\t  mds_strings[mds_mitigation]);\n\t}\n\n\tif (boot_cpu_has(X86_BUG_MSBDS_ONLY)) {\n\t\treturn sysfs_emit(buf, \"%s; SMT %s\\n\", mds_strings[mds_mitigation],\n\t\t\t\t  (mds_mitigation == MDS_MITIGATION_OFF ? \"vulnerable\" :\n\t\t\t\t   sched_smt_active() ? \"mitigated\" : \"disabled\"));\n\t}\n\n\treturn sysfs_emit(buf, \"%s; SMT %s\\n\", mds_strings[mds_mitigation],\n\t\t\t  sched_smt_active() ? \"vulnerable\" : \"disabled\");\n}\n\nstatic ssize_t tsx_async_abort_show_state(char *buf)\n{\n\tif ((taa_mitigation == TAA_MITIGATION_TSX_DISABLED) ||\n\t    (taa_mitigation == TAA_MITIGATION_OFF))\n\t\treturn sysfs_emit(buf, \"%s\\n\", taa_strings[taa_mitigation]);\n\n\tif (boot_cpu_has(X86_FEATURE_HYPERVISOR)) {\n\t\treturn sysfs_emit(buf, \"%s; SMT Host state unknown\\n\",\n\t\t\t\t  taa_strings[taa_mitigation]);\n\t}\n\n\treturn sysfs_emit(buf, \"%s; SMT %s\\n\", taa_strings[taa_mitigation],\n\t\t\t  sched_smt_active() ? \"vulnerable\" : \"disabled\");\n}\n\nstatic ssize_t mmio_stale_data_show_state(char *buf)\n{\n\tif (boot_cpu_has_bug(X86_BUG_MMIO_UNKNOWN))\n\t\treturn sysfs_emit(buf, \"Unknown: No mitigations\\n\");\n\n\tif (mmio_mitigation == MMIO_MITIGATION_OFF)\n\t\treturn sysfs_emit(buf, \"%s\\n\", mmio_strings[mmio_mitigation]);\n\n\tif (boot_cpu_has(X86_FEATURE_HYPERVISOR)) {\n\t\treturn sysfs_emit(buf, \"%s; SMT Host state unknown\\n\",\n\t\t\t\t  mmio_strings[mmio_mitigation]);\n\t}\n\n\treturn sysfs_emit(buf, \"%s; SMT %s\\n\", mmio_strings[mmio_mitigation],\n\t\t\t  sched_smt_active() ? \"vulnerable\" : \"disabled\");\n}\n\nstatic char *stibp_state(void)\n{\n\tif (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&\n\t    !boot_cpu_has(X86_FEATURE_AUTOIBRS))\n\t\treturn \"\";\n\n\tswitch (spectre_v2_user_stibp) {\n\tcase SPECTRE_V2_USER_NONE:\n\t\treturn \", STIBP: disabled\";\n\tcase SPECTRE_V2_USER_STRICT:\n\t\treturn \", STIBP: forced\";\n\tcase SPECTRE_V2_USER_STRICT_PREFERRED:\n\t\treturn \", STIBP: always-on\";\n\tcase SPECTRE_V2_USER_PRCTL:\n\tcase SPECTRE_V2_USER_SECCOMP:\n\t\tif (static_key_enabled(&switch_to_cond_stibp))\n\t\t\treturn \", STIBP: conditional\";\n\t}\n\treturn \"\";\n}\n\nstatic char *ibpb_state(void)\n{\n\tif (boot_cpu_has(X86_FEATURE_IBPB)) {\n\t\tif (static_key_enabled(&switch_mm_always_ibpb))\n\t\t\treturn \", IBPB: always-on\";\n\t\tif (static_key_enabled(&switch_mm_cond_ibpb))\n\t\t\treturn \", IBPB: conditional\";\n\t\treturn \", IBPB: disabled\";\n\t}\n\treturn \"\";\n}\n\nstatic char *pbrsb_eibrs_state(void)\n{\n\tif (boot_cpu_has_bug(X86_BUG_EIBRS_PBRSB)) {\n\t\tif (boot_cpu_has(X86_FEATURE_RSB_VMEXIT_LITE) ||\n\t\t    boot_cpu_has(X86_FEATURE_RSB_VMEXIT))\n\t\t\treturn \", PBRSB-eIBRS: SW sequence\";\n\t\telse\n\t\t\treturn \", PBRSB-eIBRS: Vulnerable\";\n\t} else {\n\t\treturn \", PBRSB-eIBRS: Not affected\";\n\t}\n}\n\nstatic ssize_t spectre_v2_show_state(char *buf)\n{\n\tif (spectre_v2_enabled == SPECTRE_V2_LFENCE)\n\t\treturn sysfs_emit(buf, \"Vulnerable: LFENCE\\n\");\n\n\tif (spectre_v2_enabled == SPECTRE_V2_EIBRS && unprivileged_ebpf_enabled())\n\t\treturn sysfs_emit(buf, \"Vulnerable: eIBRS with unprivileged eBPF\\n\");\n\n\tif (sched_smt_active() && unprivileged_ebpf_enabled() &&\n\t    spectre_v2_enabled == SPECTRE_V2_EIBRS_LFENCE)\n\t\treturn sysfs_emit(buf, \"Vulnerable: eIBRS+LFENCE with unprivileged eBPF and SMT\\n\");\n\n\treturn sysfs_emit(buf, \"%s%s%s%s%s%s%s\\n\",\n\t\t\t  spectre_v2_strings[spectre_v2_enabled],\n\t\t\t  ibpb_state(),\n\t\t\t  boot_cpu_has(X86_FEATURE_USE_IBRS_FW) ? \", IBRS_FW\" : \"\",\n\t\t\t  stibp_state(),\n\t\t\t  boot_cpu_has(X86_FEATURE_RSB_CTXSW) ? \", RSB filling\" : \"\",\n\t\t\t  pbrsb_eibrs_state(),\n\t\t\t  spectre_v2_module_string());\n}\n\nstatic ssize_t srbds_show_state(char *buf)\n{\n\treturn sysfs_emit(buf, \"%s\\n\", srbds_strings[srbds_mitigation]);\n}\n\nstatic ssize_t retbleed_show_state(char *buf)\n{\n\tif (retbleed_mitigation == RETBLEED_MITIGATION_UNRET ||\n\t    retbleed_mitigation == RETBLEED_MITIGATION_IBPB) {\n\t\tif (boot_cpu_data.x86_vendor != X86_VENDOR_AMD &&\n\t\t    boot_cpu_data.x86_vendor != X86_VENDOR_HYGON)\n\t\t\treturn sysfs_emit(buf, \"Vulnerable: untrained return thunk / IBPB on non-AMD based uarch\\n\");\n\n\t\treturn sysfs_emit(buf, \"%s; SMT %s\\n\", retbleed_strings[retbleed_mitigation],\n\t\t\t\t  !sched_smt_active() ? \"disabled\" :\n\t\t\t\t  spectre_v2_user_stibp == SPECTRE_V2_USER_STRICT ||\n\t\t\t\t  spectre_v2_user_stibp == SPECTRE_V2_USER_STRICT_PREFERRED ?\n\t\t\t\t  \"enabled with STIBP protection\" : \"vulnerable\");\n\t}\n\n\treturn sysfs_emit(buf, \"%s\\n\", retbleed_strings[retbleed_mitigation]);\n}\n\nstatic ssize_t srso_show_state(char *buf)\n{\n\tif (boot_cpu_has(X86_FEATURE_SRSO_NO))\n\t\treturn sysfs_emit(buf, \"Mitigation: SMT disabled\\n\");\n\n\treturn sysfs_emit(buf, \"%s\\n\", srso_strings[srso_mitigation]);\n}\n\nstatic ssize_t gds_show_state(char *buf)\n{\n\treturn sysfs_emit(buf, \"%s\\n\", gds_strings[gds_mitigation]);\n}\n\nstatic ssize_t cpu_show_common(struct device *dev, struct device_attribute *attr,\n\t\t\t       char *buf, unsigned int bug)\n{\n\tif (!boot_cpu_has_bug(bug))\n\t\treturn sysfs_emit(buf, \"Not affected\\n\");\n\n\tswitch (bug) {\n\tcase X86_BUG_CPU_MELTDOWN:\n\t\tif (boot_cpu_has(X86_FEATURE_PTI))\n\t\t\treturn sysfs_emit(buf, \"Mitigation: PTI\\n\");\n\n\t\tif (hypervisor_is_type(X86_HYPER_XEN_PV))\n\t\t\treturn sysfs_emit(buf, \"Unknown (XEN PV detected, hypervisor mitigation required)\\n\");\n\n\t\tbreak;\n\n\tcase X86_BUG_SPECTRE_V1:\n\t\treturn sysfs_emit(buf, \"%s\\n\", spectre_v1_strings[spectre_v1_mitigation]);\n\n\tcase X86_BUG_SPECTRE_V2:\n\t\treturn spectre_v2_show_state(buf);\n\n\tcase X86_BUG_SPEC_STORE_BYPASS:\n\t\treturn sysfs_emit(buf, \"%s\\n\", ssb_strings[ssb_mode]);\n\n\tcase X86_BUG_L1TF:\n\t\tif (boot_cpu_has(X86_FEATURE_L1TF_PTEINV))\n\t\t\treturn l1tf_show_state(buf);\n\t\tbreak;\n\n\tcase X86_BUG_MDS:\n\t\treturn mds_show_state(buf);\n\n\tcase X86_BUG_TAA:\n\t\treturn tsx_async_abort_show_state(buf);\n\n\tcase X86_BUG_ITLB_MULTIHIT:\n\t\treturn itlb_multihit_show_state(buf);\n\n\tcase X86_BUG_SRBDS:\n\t\treturn srbds_show_state(buf);\n\n\tcase X86_BUG_MMIO_STALE_DATA:\n\tcase X86_BUG_MMIO_UNKNOWN:\n\t\treturn mmio_stale_data_show_state(buf);\n\n\tcase X86_BUG_RETBLEED:\n\t\treturn retbleed_show_state(buf);\n\n\tcase X86_BUG_SRSO:\n\t\treturn srso_show_state(buf);\n\n\tcase X86_BUG_GDS:\n\t\treturn gds_show_state(buf);\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn sysfs_emit(buf, \"Vulnerable\\n\");\n}\n\nssize_t cpu_show_meltdown(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_CPU_MELTDOWN);\n}\n\nssize_t cpu_show_spectre_v1(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_SPECTRE_V1);\n}\n\nssize_t cpu_show_spectre_v2(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_SPECTRE_V2);\n}\n\nssize_t cpu_show_spec_store_bypass(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_SPEC_STORE_BYPASS);\n}\n\nssize_t cpu_show_l1tf(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_L1TF);\n}\n\nssize_t cpu_show_mds(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_MDS);\n}\n\nssize_t cpu_show_tsx_async_abort(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_TAA);\n}\n\nssize_t cpu_show_itlb_multihit(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_ITLB_MULTIHIT);\n}\n\nssize_t cpu_show_srbds(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_SRBDS);\n}\n\nssize_t cpu_show_mmio_stale_data(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tif (boot_cpu_has_bug(X86_BUG_MMIO_UNKNOWN))\n\t\treturn cpu_show_common(dev, attr, buf, X86_BUG_MMIO_UNKNOWN);\n\telse\n\t\treturn cpu_show_common(dev, attr, buf, X86_BUG_MMIO_STALE_DATA);\n}\n\nssize_t cpu_show_retbleed(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_RETBLEED);\n}\n\nssize_t cpu_show_spec_rstack_overflow(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_SRSO);\n}\n\nssize_t cpu_show_gds(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\treturn cpu_show_common(dev, attr, buf, X86_BUG_GDS);\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}