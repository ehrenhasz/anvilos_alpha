{
  "module_name": "core.c",
  "hash_id": "ad416228958b9b2df44b9986df2a21ed0404bacfdc06aa0ebda4c01b02a9e1be",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/cpu/mce/core.c",
  "human_readable_source": "\n \n\n#include <linux/thread_info.h>\n#include <linux/capability.h>\n#include <linux/miscdevice.h>\n#include <linux/ratelimit.h>\n#include <linux/rcupdate.h>\n#include <linux/kobject.h>\n#include <linux/uaccess.h>\n#include <linux/kdebug.h>\n#include <linux/kernel.h>\n#include <linux/percpu.h>\n#include <linux/string.h>\n#include <linux/device.h>\n#include <linux/syscore_ops.h>\n#include <linux/delay.h>\n#include <linux/ctype.h>\n#include <linux/sched.h>\n#include <linux/sysfs.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/kmod.h>\n#include <linux/poll.h>\n#include <linux/nmi.h>\n#include <linux/cpu.h>\n#include <linux/ras.h>\n#include <linux/smp.h>\n#include <linux/fs.h>\n#include <linux/mm.h>\n#include <linux/debugfs.h>\n#include <linux/irq_work.h>\n#include <linux/export.h>\n#include <linux/set_memory.h>\n#include <linux/sync_core.h>\n#include <linux/task_work.h>\n#include <linux/hardirq.h>\n\n#include <asm/intel-family.h>\n#include <asm/processor.h>\n#include <asm/traps.h>\n#include <asm/tlbflush.h>\n#include <asm/mce.h>\n#include <asm/msr.h>\n#include <asm/reboot.h>\n\n#include \"internal.h\"\n\n \nstatic DEFINE_MUTEX(mce_sysfs_mutex);\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/mce.h>\n\n#define SPINUNIT\t\t100\t \n\nDEFINE_PER_CPU(unsigned, mce_exception_count);\n\nDEFINE_PER_CPU_READ_MOSTLY(unsigned int, mce_num_banks);\n\nDEFINE_PER_CPU_READ_MOSTLY(struct mce_bank[MAX_NR_BANKS], mce_banks_array);\n\n#define ATTR_LEN               16\n \nstruct mce_bank_dev {\n\tstruct device_attribute\tattr;\t\t\t \n\tchar\t\t\tattrname[ATTR_LEN];\t \n\tu8\t\t\tbank;\t\t\t \n};\nstatic struct mce_bank_dev mce_bank_devs[MAX_NR_BANKS];\n\nstruct mce_vendor_flags mce_flags __read_mostly;\n\nstruct mca_config mca_cfg __read_mostly = {\n\t.bootlog  = -1,\n\t.monarch_timeout = -1\n};\n\nstatic DEFINE_PER_CPU(struct mce, mces_seen);\nstatic unsigned long mce_need_notify;\n\n \nDEFINE_PER_CPU(mce_banks_t, mce_poll_banks) = {\n\t[0 ... BITS_TO_LONGS(MAX_NR_BANKS)-1] = ~0UL\n};\n\n \nmce_banks_t mce_banks_ce_disabled;\n\nstatic struct work_struct mce_work;\nstatic struct irq_work mce_irq_work;\n\n \nBLOCKING_NOTIFIER_HEAD(x86_mce_decoder_chain);\n\n \nvoid mce_setup(struct mce *m)\n{\n\tmemset(m, 0, sizeof(struct mce));\n\tm->cpu = m->extcpu = smp_processor_id();\n\t \n\tm->time = __ktime_get_real_seconds();\n\tm->cpuvendor = boot_cpu_data.x86_vendor;\n\tm->cpuid = cpuid_eax(1);\n\tm->socketid = cpu_data(m->extcpu).phys_proc_id;\n\tm->apicid = cpu_data(m->extcpu).initial_apicid;\n\tm->mcgcap = __rdmsr(MSR_IA32_MCG_CAP);\n\tm->ppin = cpu_data(m->extcpu).ppin;\n\tm->microcode = boot_cpu_data.microcode;\n}\n\nDEFINE_PER_CPU(struct mce, injectm);\nEXPORT_PER_CPU_SYMBOL_GPL(injectm);\n\nvoid mce_log(struct mce *m)\n{\n\tif (!mce_gen_pool_add(m))\n\t\tirq_work_queue(&mce_irq_work);\n}\nEXPORT_SYMBOL_GPL(mce_log);\n\nvoid mce_register_decode_chain(struct notifier_block *nb)\n{\n\tif (WARN_ON(nb->priority < MCE_PRIO_LOWEST ||\n\t\t    nb->priority > MCE_PRIO_HIGHEST))\n\t\treturn;\n\n\tblocking_notifier_chain_register(&x86_mce_decoder_chain, nb);\n}\nEXPORT_SYMBOL_GPL(mce_register_decode_chain);\n\nvoid mce_unregister_decode_chain(struct notifier_block *nb)\n{\n\tblocking_notifier_chain_unregister(&x86_mce_decoder_chain, nb);\n}\nEXPORT_SYMBOL_GPL(mce_unregister_decode_chain);\n\nstatic void __print_mce(struct mce *m)\n{\n\tpr_emerg(HW_ERR \"CPU %d: Machine Check%s: %Lx Bank %d: %016Lx\\n\",\n\t\t m->extcpu,\n\t\t (m->mcgstatus & MCG_STATUS_MCIP ? \" Exception\" : \"\"),\n\t\t m->mcgstatus, m->bank, m->status);\n\n\tif (m->ip) {\n\t\tpr_emerg(HW_ERR \"RIP%s %02x:<%016Lx> \",\n\t\t\t!(m->mcgstatus & MCG_STATUS_EIPV) ? \" !INEXACT!\" : \"\",\n\t\t\tm->cs, m->ip);\n\n\t\tif (m->cs == __KERNEL_CS)\n\t\t\tpr_cont(\"{%pS}\", (void *)(unsigned long)m->ip);\n\t\tpr_cont(\"\\n\");\n\t}\n\n\tpr_emerg(HW_ERR \"TSC %llx \", m->tsc);\n\tif (m->addr)\n\t\tpr_cont(\"ADDR %llx \", m->addr);\n\tif (m->misc)\n\t\tpr_cont(\"MISC %llx \", m->misc);\n\tif (m->ppin)\n\t\tpr_cont(\"PPIN %llx \", m->ppin);\n\n\tif (mce_flags.smca) {\n\t\tif (m->synd)\n\t\t\tpr_cont(\"SYND %llx \", m->synd);\n\t\tif (m->ipid)\n\t\t\tpr_cont(\"IPID %llx \", m->ipid);\n\t}\n\n\tpr_cont(\"\\n\");\n\n\t \n\tpr_emerg(HW_ERR \"PROCESSOR %u:%x TIME %llu SOCKET %u APIC %x microcode %x\\n\",\n\t\tm->cpuvendor, m->cpuid, m->time, m->socketid, m->apicid,\n\t\tm->microcode);\n}\n\nstatic void print_mce(struct mce *m)\n{\n\t__print_mce(m);\n\n\tif (m->cpuvendor != X86_VENDOR_AMD && m->cpuvendor != X86_VENDOR_HYGON)\n\t\tpr_emerg_ratelimited(HW_ERR \"Run the above through 'mcelog --ascii'\\n\");\n}\n\n#define PANIC_TIMEOUT 5  \n\nstatic atomic_t mce_panicked;\n\nstatic int fake_panic;\nstatic atomic_t mce_fake_panicked;\n\n \nstatic void wait_for_panic(void)\n{\n\tlong timeout = PANIC_TIMEOUT*USEC_PER_SEC;\n\n\tpreempt_disable();\n\tlocal_irq_enable();\n\twhile (timeout-- > 0)\n\t\tudelay(1);\n\tif (panic_timeout == 0)\n\t\tpanic_timeout = mca_cfg.panic_timeout;\n\tpanic(\"Panicing machine check CPU died\");\n}\n\nstatic noinstr void mce_panic(const char *msg, struct mce *final, char *exp)\n{\n\tstruct llist_node *pending;\n\tstruct mce_evt_llist *l;\n\tint apei_err = 0;\n\n\t \n\tinstrumentation_begin();\n\n\tif (!fake_panic) {\n\t\t \n\t\tif (atomic_inc_return(&mce_panicked) > 1)\n\t\t\twait_for_panic();\n\t\tbarrier();\n\n\t\tbust_spinlocks(1);\n\t\tconsole_verbose();\n\t} else {\n\t\t \n\t\tif (atomic_inc_return(&mce_fake_panicked) > 1)\n\t\t\tgoto out;\n\t}\n\tpending = mce_gen_pool_prepare_records();\n\t \n\tllist_for_each_entry(l, pending, llnode) {\n\t\tstruct mce *m = &l->mce;\n\t\tif (!(m->status & MCI_STATUS_UC)) {\n\t\t\tprint_mce(m);\n\t\t\tif (!apei_err)\n\t\t\t\tapei_err = apei_write_mce(m);\n\t\t}\n\t}\n\t \n\tllist_for_each_entry(l, pending, llnode) {\n\t\tstruct mce *m = &l->mce;\n\t\tif (!(m->status & MCI_STATUS_UC))\n\t\t\tcontinue;\n\t\tif (!final || mce_cmp(m, final)) {\n\t\t\tprint_mce(m);\n\t\t\tif (!apei_err)\n\t\t\t\tapei_err = apei_write_mce(m);\n\t\t}\n\t}\n\tif (final) {\n\t\tprint_mce(final);\n\t\tif (!apei_err)\n\t\t\tapei_err = apei_write_mce(final);\n\t}\n\tif (exp)\n\t\tpr_emerg(HW_ERR \"Machine check: %s\\n\", exp);\n\tif (!fake_panic) {\n\t\tif (panic_timeout == 0)\n\t\t\tpanic_timeout = mca_cfg.panic_timeout;\n\t\tpanic(msg);\n\t} else\n\t\tpr_emerg(HW_ERR \"Fake kernel panic: %s\\n\", msg);\n\nout:\n\tinstrumentation_end();\n}\n\n \n\nstatic int msr_to_offset(u32 msr)\n{\n\tunsigned bank = __this_cpu_read(injectm.bank);\n\n\tif (msr == mca_cfg.rip_msr)\n\t\treturn offsetof(struct mce, ip);\n\tif (msr == mca_msr_reg(bank, MCA_STATUS))\n\t\treturn offsetof(struct mce, status);\n\tif (msr == mca_msr_reg(bank, MCA_ADDR))\n\t\treturn offsetof(struct mce, addr);\n\tif (msr == mca_msr_reg(bank, MCA_MISC))\n\t\treturn offsetof(struct mce, misc);\n\tif (msr == MSR_IA32_MCG_STATUS)\n\t\treturn offsetof(struct mce, mcgstatus);\n\treturn -1;\n}\n\nvoid ex_handler_msr_mce(struct pt_regs *regs, bool wrmsr)\n{\n\tif (wrmsr) {\n\t\tpr_emerg(\"MSR access error: WRMSR to 0x%x (tried to write 0x%08x%08x) at rIP: 0x%lx (%pS)\\n\",\n\t\t\t (unsigned int)regs->cx, (unsigned int)regs->dx, (unsigned int)regs->ax,\n\t\t\t regs->ip, (void *)regs->ip);\n\t} else {\n\t\tpr_emerg(\"MSR access error: RDMSR from 0x%x at rIP: 0x%lx (%pS)\\n\",\n\t\t\t (unsigned int)regs->cx, regs->ip, (void *)regs->ip);\n\t}\n\n\tshow_stack_regs(regs);\n\n\tpanic(\"MCA architectural violation!\\n\");\n\n\twhile (true)\n\t\tcpu_relax();\n}\n\n \nnoinstr u64 mce_rdmsrl(u32 msr)\n{\n\tDECLARE_ARGS(val, low, high);\n\n\tif (__this_cpu_read(injectm.finished)) {\n\t\tint offset;\n\t\tu64 ret;\n\n\t\tinstrumentation_begin();\n\n\t\toffset = msr_to_offset(msr);\n\t\tif (offset < 0)\n\t\t\tret = 0;\n\t\telse\n\t\t\tret = *(u64 *)((char *)this_cpu_ptr(&injectm) + offset);\n\n\t\tinstrumentation_end();\n\n\t\treturn ret;\n\t}\n\n\t \n\tasm volatile(\"1: rdmsr\\n\"\n\t\t     \"2:\\n\"\n\t\t     _ASM_EXTABLE_TYPE(1b, 2b, EX_TYPE_RDMSR_IN_MCE)\n\t\t     : EAX_EDX_RET(val, low, high) : \"c\" (msr));\n\n\n\treturn EAX_EDX_VAL(val, low, high);\n}\n\nstatic noinstr void mce_wrmsrl(u32 msr, u64 v)\n{\n\tu32 low, high;\n\n\tif (__this_cpu_read(injectm.finished)) {\n\t\tint offset;\n\n\t\tinstrumentation_begin();\n\n\t\toffset = msr_to_offset(msr);\n\t\tif (offset >= 0)\n\t\t\t*(u64 *)((char *)this_cpu_ptr(&injectm) + offset) = v;\n\n\t\tinstrumentation_end();\n\n\t\treturn;\n\t}\n\n\tlow  = (u32)v;\n\thigh = (u32)(v >> 32);\n\n\t \n\tasm volatile(\"1: wrmsr\\n\"\n\t\t     \"2:\\n\"\n\t\t     _ASM_EXTABLE_TYPE(1b, 2b, EX_TYPE_WRMSR_IN_MCE)\n\t\t     : : \"c\" (msr), \"a\"(low), \"d\" (high) : \"memory\");\n}\n\n \nstatic noinstr void mce_gather_info(struct mce *m, struct pt_regs *regs)\n{\n\t \n\tinstrumentation_begin();\n\tmce_setup(m);\n\tinstrumentation_end();\n\n\tm->mcgstatus = mce_rdmsrl(MSR_IA32_MCG_STATUS);\n\tif (regs) {\n\t\t \n\t\tif (m->mcgstatus & (MCG_STATUS_RIPV|MCG_STATUS_EIPV)) {\n\t\t\tm->ip = regs->ip;\n\t\t\tm->cs = regs->cs;\n\n\t\t\t \n\t\t\tif (v8086_mode(regs))\n\t\t\t\tm->cs |= 3;\n\t\t}\n\t\t \n\t\tif (mca_cfg.rip_msr)\n\t\t\tm->ip = mce_rdmsrl(mca_cfg.rip_msr);\n\t}\n}\n\nint mce_available(struct cpuinfo_x86 *c)\n{\n\tif (mca_cfg.disabled)\n\t\treturn 0;\n\treturn cpu_has(c, X86_FEATURE_MCE) && cpu_has(c, X86_FEATURE_MCA);\n}\n\nstatic void mce_schedule_work(void)\n{\n\tif (!mce_gen_pool_empty())\n\t\tschedule_work(&mce_work);\n}\n\nstatic void mce_irq_work_cb(struct irq_work *entry)\n{\n\tmce_schedule_work();\n}\n\n \nint mce_usable_address(struct mce *m)\n{\n\tif (!(m->status & MCI_STATUS_ADDRV))\n\t\treturn 0;\n\n\t \n\tif (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL &&\n\t    boot_cpu_data.x86_vendor != X86_VENDOR_ZHAOXIN)\n\t\treturn 1;\n\n\tif (!(m->status & MCI_STATUS_MISCV))\n\t\treturn 0;\n\n\tif (MCI_MISC_ADDR_LSB(m->misc) > PAGE_SHIFT)\n\t\treturn 0;\n\n\tif (MCI_MISC_ADDR_MODE(m->misc) != MCI_MISC_ADDR_PHYS)\n\t\treturn 0;\n\n\treturn 1;\n}\nEXPORT_SYMBOL_GPL(mce_usable_address);\n\nbool mce_is_memory_error(struct mce *m)\n{\n\tswitch (m->cpuvendor) {\n\tcase X86_VENDOR_AMD:\n\tcase X86_VENDOR_HYGON:\n\t\treturn amd_mce_is_memory_error(m);\n\n\tcase X86_VENDOR_INTEL:\n\tcase X86_VENDOR_ZHAOXIN:\n\t\t \n\t\treturn (m->status & 0xef80) == BIT(7) ||\n\t\t       (m->status & 0xef00) == BIT(8) ||\n\t\t       (m->status & 0xeffc) == 0xc;\n\n\tdefault:\n\t\treturn false;\n\t}\n}\nEXPORT_SYMBOL_GPL(mce_is_memory_error);\n\nstatic bool whole_page(struct mce *m)\n{\n\tif (!mca_cfg.ser || !(m->status & MCI_STATUS_MISCV))\n\t\treturn true;\n\n\treturn MCI_MISC_ADDR_LSB(m->misc) >= PAGE_SHIFT;\n}\n\nbool mce_is_correctable(struct mce *m)\n{\n\tif (m->cpuvendor == X86_VENDOR_AMD && m->status & MCI_STATUS_DEFERRED)\n\t\treturn false;\n\n\tif (m->cpuvendor == X86_VENDOR_HYGON && m->status & MCI_STATUS_DEFERRED)\n\t\treturn false;\n\n\tif (m->status & MCI_STATUS_UC)\n\t\treturn false;\n\n\treturn true;\n}\nEXPORT_SYMBOL_GPL(mce_is_correctable);\n\nstatic int mce_early_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t      void *data)\n{\n\tstruct mce *m = (struct mce *)data;\n\n\tif (!m)\n\t\treturn NOTIFY_DONE;\n\n\t \n\ttrace_mce_record(m);\n\n\tset_bit(0, &mce_need_notify);\n\n\tmce_notify_irq();\n\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block early_nb = {\n\t.notifier_call\t= mce_early_notifier,\n\t.priority\t= MCE_PRIO_EARLY,\n};\n\nstatic int uc_decode_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t      void *data)\n{\n\tstruct mce *mce = (struct mce *)data;\n\tunsigned long pfn;\n\n\tif (!mce || !mce_usable_address(mce))\n\t\treturn NOTIFY_DONE;\n\n\tif (mce->severity != MCE_AO_SEVERITY &&\n\t    mce->severity != MCE_DEFERRED_SEVERITY)\n\t\treturn NOTIFY_DONE;\n\n\tpfn = (mce->addr & MCI_ADDR_PHYSADDR) >> PAGE_SHIFT;\n\tif (!memory_failure(pfn, 0)) {\n\t\tset_mce_nospec(pfn);\n\t\tmce->kflags |= MCE_HANDLED_UC;\n\t}\n\n\treturn NOTIFY_OK;\n}\n\nstatic struct notifier_block mce_uc_nb = {\n\t.notifier_call\t= uc_decode_notifier,\n\t.priority\t= MCE_PRIO_UC,\n};\n\nstatic int mce_default_notifier(struct notifier_block *nb, unsigned long val,\n\t\t\t\tvoid *data)\n{\n\tstruct mce *m = (struct mce *)data;\n\n\tif (!m)\n\t\treturn NOTIFY_DONE;\n\n\tif (mca_cfg.print_all || !m->kflags)\n\t\t__print_mce(m);\n\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block mce_default_nb = {\n\t.notifier_call\t= mce_default_notifier,\n\t \n\t.priority\t= MCE_PRIO_LOWEST,\n};\n\n \nstatic noinstr void mce_read_aux(struct mce *m, int i)\n{\n\tif (m->status & MCI_STATUS_MISCV)\n\t\tm->misc = mce_rdmsrl(mca_msr_reg(i, MCA_MISC));\n\n\tif (m->status & MCI_STATUS_ADDRV) {\n\t\tm->addr = mce_rdmsrl(mca_msr_reg(i, MCA_ADDR));\n\n\t\t \n\t\tif (mca_cfg.ser && (m->status & MCI_STATUS_MISCV)) {\n\t\t\tu8 shift = MCI_MISC_ADDR_LSB(m->misc);\n\t\t\tm->addr >>= shift;\n\t\t\tm->addr <<= shift;\n\t\t}\n\n\t\tsmca_extract_err_addr(m);\n\t}\n\n\tif (mce_flags.smca) {\n\t\tm->ipid = mce_rdmsrl(MSR_AMD64_SMCA_MCx_IPID(i));\n\n\t\tif (m->status & MCI_STATUS_SYNDV)\n\t\t\tm->synd = mce_rdmsrl(MSR_AMD64_SMCA_MCx_SYND(i));\n\t}\n}\n\nDEFINE_PER_CPU(unsigned, mce_poll_count);\n\n \nbool machine_check_poll(enum mcp_flags flags, mce_banks_t *b)\n{\n\tstruct mce_bank *mce_banks = this_cpu_ptr(mce_banks_array);\n\tbool error_seen = false;\n\tstruct mce m;\n\tint i;\n\n\tthis_cpu_inc(mce_poll_count);\n\n\tmce_gather_info(&m, NULL);\n\n\tif (flags & MCP_TIMESTAMP)\n\t\tm.tsc = rdtsc();\n\n\tfor (i = 0; i < this_cpu_read(mce_num_banks); i++) {\n\t\tif (!mce_banks[i].ctl || !test_bit(i, *b))\n\t\t\tcontinue;\n\n\t\tm.misc = 0;\n\t\tm.addr = 0;\n\t\tm.bank = i;\n\n\t\tbarrier();\n\t\tm.status = mce_rdmsrl(mca_msr_reg(i, MCA_STATUS));\n\n\t\t \n\t\tif (!(m.status & MCI_STATUS_VAL))\n\t\t\tcontinue;\n\n\t\t \n\t\tif ((flags & MCP_UC) || !(m.status & MCI_STATUS_UC))\n\t\t\tgoto log_it;\n\n\t\t \n\t\tif (!mca_cfg.ser) {\n\t\t\tif (m.status & MCI_STATUS_UC)\n\t\t\t\tcontinue;\n\t\t\tgoto log_it;\n\t\t}\n\n\t\t \n\t\tif (!(m.status & MCI_STATUS_EN))\n\t\t\tgoto log_it;\n\n\t\t \n\t\tif (!(m.status & MCI_STATUS_PCC) && !(m.status & MCI_STATUS_S))\n\t\t\tgoto log_it;\n\n\t\t \n\t\tcontinue;\n\nlog_it:\n\t\terror_seen = true;\n\n\t\tif (flags & MCP_DONTLOG)\n\t\t\tgoto clear_it;\n\n\t\tmce_read_aux(&m, i);\n\t\tm.severity = mce_severity(&m, NULL, NULL, false);\n\t\t \n\n\t\tif (mca_cfg.dont_log_ce && !mce_usable_address(&m))\n\t\t\tgoto clear_it;\n\n\t\tif (flags & MCP_QUEUE_LOG)\n\t\t\tmce_gen_pool_add(&m);\n\t\telse\n\t\t\tmce_log(&m);\n\nclear_it:\n\t\t \n\t\tmce_wrmsrl(mca_msr_reg(i, MCA_STATUS), 0);\n\t}\n\n\t \n\n\tsync_core();\n\n\treturn error_seen;\n}\nEXPORT_SYMBOL_GPL(machine_check_poll);\n\n \nstatic __always_inline void\nquirk_sandybridge_ifu(int bank, struct mce *m, struct pt_regs *regs)\n{\n\tif (bank != 0)\n\t\treturn;\n\tif ((m->mcgstatus & (MCG_STATUS_EIPV|MCG_STATUS_RIPV)) != 0)\n\t\treturn;\n\tif ((m->status & (MCI_STATUS_OVER|MCI_STATUS_UC|\n\t\t          MCI_STATUS_EN|MCI_STATUS_MISCV|MCI_STATUS_ADDRV|\n\t\t\t  MCI_STATUS_PCC|MCI_STATUS_S|MCI_STATUS_AR|\n\t\t\t  MCACOD)) !=\n\t\t\t (MCI_STATUS_UC|MCI_STATUS_EN|\n\t\t\t  MCI_STATUS_MISCV|MCI_STATUS_ADDRV|MCI_STATUS_S|\n\t\t\t  MCI_STATUS_AR|MCACOD_INSTR))\n\t\treturn;\n\n\tm->mcgstatus |= MCG_STATUS_EIPV;\n\tm->ip = regs->ip;\n\tm->cs = regs->cs;\n}\n\n \nstatic noinstr bool quirk_skylake_repmov(void)\n{\n\tu64 mcgstatus   = mce_rdmsrl(MSR_IA32_MCG_STATUS);\n\tu64 misc_enable = mce_rdmsrl(MSR_IA32_MISC_ENABLE);\n\tu64 mc1_status;\n\n\t \n\tif (!(mcgstatus & MCG_STATUS_LMCES) ||\n\t    !(misc_enable & MSR_IA32_MISC_ENABLE_FAST_STRING))\n\t\treturn false;\n\n\tmc1_status = mce_rdmsrl(MSR_IA32_MCx_STATUS(1));\n\n\t \n\tif ((mc1_status &\n\t     (MCI_STATUS_VAL | MCI_STATUS_OVER | MCI_STATUS_UC | MCI_STATUS_EN |\n\t      MCI_STATUS_ADDRV | MCI_STATUS_MISCV | MCI_STATUS_PCC |\n\t      MCI_STATUS_AR | MCI_STATUS_S)) ==\n\t     (MCI_STATUS_VAL |                   MCI_STATUS_UC | MCI_STATUS_EN |\n\t      MCI_STATUS_ADDRV | MCI_STATUS_MISCV |\n\t      MCI_STATUS_AR | MCI_STATUS_S)) {\n\t\tmisc_enable &= ~MSR_IA32_MISC_ENABLE_FAST_STRING;\n\t\tmce_wrmsrl(MSR_IA32_MISC_ENABLE, misc_enable);\n\t\tmce_wrmsrl(MSR_IA32_MCx_STATUS(1), 0);\n\n\t\tinstrumentation_begin();\n\t\tpr_err_once(\"Erratum detected, disable fast string copy instructions.\\n\");\n\t\tinstrumentation_end();\n\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic __always_inline void quirk_zen_ifu(int bank, struct mce *m, struct pt_regs *regs)\n{\n\tif (bank != 1)\n\t\treturn;\n\tif (!(m->status & MCI_STATUS_POISON))\n\t\treturn;\n\n\tm->cs = regs->cs;\n}\n\n \nstatic __always_inline int mce_no_way_out(struct mce *m, char **msg, unsigned long *validp,\n\t\t\t\t\t  struct pt_regs *regs)\n{\n\tchar *tmp = *msg;\n\tint i;\n\n\tfor (i = 0; i < this_cpu_read(mce_num_banks); i++) {\n\t\tm->status = mce_rdmsrl(mca_msr_reg(i, MCA_STATUS));\n\t\tif (!(m->status & MCI_STATUS_VAL))\n\t\t\tcontinue;\n\n\t\tarch___set_bit(i, validp);\n\t\tif (mce_flags.snb_ifu_quirk)\n\t\t\tquirk_sandybridge_ifu(i, m, regs);\n\n\t\tif (mce_flags.zen_ifu_quirk)\n\t\t\tquirk_zen_ifu(i, m, regs);\n\n\t\tm->bank = i;\n\t\tif (mce_severity(m, regs, &tmp, true) >= MCE_PANIC_SEVERITY) {\n\t\t\tmce_read_aux(m, i);\n\t\t\t*msg = tmp;\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\n \nstatic atomic_t mce_executing;\n\n \nstatic atomic_t mce_callin;\n\n \nstatic cpumask_t mce_missing_cpus = CPU_MASK_ALL;\n\n \nstatic noinstr int mce_timed_out(u64 *t, const char *msg)\n{\n\tint ret = 0;\n\n\t \n\tinstrumentation_begin();\n\n\t \n\trmb();\n\tif (atomic_read(&mce_panicked))\n\t\twait_for_panic();\n\tif (!mca_cfg.monarch_timeout)\n\t\tgoto out;\n\tif ((s64)*t < SPINUNIT) {\n\t\tif (cpumask_and(&mce_missing_cpus, cpu_online_mask, &mce_missing_cpus))\n\t\t\tpr_emerg(\"CPUs not responding to MCE broadcast (may include false positives): %*pbl\\n\",\n\t\t\t\t cpumask_pr_args(&mce_missing_cpus));\n\t\tmce_panic(msg, NULL, NULL);\n\n\t\tret = 1;\n\t\tgoto out;\n\t}\n\t*t -= SPINUNIT;\n\nout:\n\ttouch_nmi_watchdog();\n\n\tinstrumentation_end();\n\n\treturn ret;\n}\n\n \nstatic void mce_reign(void)\n{\n\tint cpu;\n\tstruct mce *m = NULL;\n\tint global_worst = 0;\n\tchar *msg = NULL;\n\n\t \n\tfor_each_possible_cpu(cpu) {\n\t\tstruct mce *mtmp = &per_cpu(mces_seen, cpu);\n\n\t\tif (mtmp->severity > global_worst) {\n\t\t\tglobal_worst = mtmp->severity;\n\t\t\tm = &per_cpu(mces_seen, cpu);\n\t\t}\n\t}\n\n\t \n\tif (m && global_worst >= MCE_PANIC_SEVERITY) {\n\t\t \n\t\tmce_severity(m, NULL, &msg, true);\n\t\tmce_panic(\"Fatal machine check\", m, msg);\n\t}\n\n\t \n\n\t \n\tif (global_worst <= MCE_KEEP_SEVERITY)\n\t\tmce_panic(\"Fatal machine check from unknown source\", NULL, NULL);\n\n\t \n\tfor_each_possible_cpu(cpu)\n\t\tmemset(&per_cpu(mces_seen, cpu), 0, sizeof(struct mce));\n}\n\nstatic atomic_t global_nwo;\n\n \nstatic noinstr int mce_start(int *no_way_out)\n{\n\tu64 timeout = (u64)mca_cfg.monarch_timeout * NSEC_PER_USEC;\n\tint order, ret = -1;\n\n\tif (!timeout)\n\t\treturn ret;\n\n\traw_atomic_add(*no_way_out, &global_nwo);\n\t \n\torder = raw_atomic_inc_return(&mce_callin);\n\tarch_cpumask_clear_cpu(smp_processor_id(), &mce_missing_cpus);\n\n\t \n\tinstrumentation_begin();\n\n\t \n\twhile (raw_atomic_read(&mce_callin) != num_online_cpus()) {\n\t\tif (mce_timed_out(&timeout,\n\t\t\t\t  \"Timeout: Not all CPUs entered broadcast exception handler\")) {\n\t\t\traw_atomic_set(&global_nwo, 0);\n\t\t\tgoto out;\n\t\t}\n\t\tndelay(SPINUNIT);\n\t}\n\n\t \n\tsmp_rmb();\n\n\tif (order == 1) {\n\t\t \n\t\traw_atomic_set(&mce_executing, 1);\n\t} else {\n\t\t \n\t\twhile (raw_atomic_read(&mce_executing) < order) {\n\t\t\tif (mce_timed_out(&timeout,\n\t\t\t\t\t  \"Timeout: Subject CPUs unable to finish machine check processing\")) {\n\t\t\t\traw_atomic_set(&global_nwo, 0);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tndelay(SPINUNIT);\n\t\t}\n\t}\n\n\t \n\t*no_way_out = raw_atomic_read(&global_nwo);\n\n\tret = order;\n\nout:\n\tinstrumentation_end();\n\n\treturn ret;\n}\n\n \nstatic noinstr int mce_end(int order)\n{\n\tu64 timeout = (u64)mca_cfg.monarch_timeout * NSEC_PER_USEC;\n\tint ret = -1;\n\n\t \n\tinstrumentation_begin();\n\n\tif (!timeout)\n\t\tgoto reset;\n\tif (order < 0)\n\t\tgoto reset;\n\n\t \n\tatomic_inc(&mce_executing);\n\n\tif (order == 1) {\n\t\t \n\t\twhile (atomic_read(&mce_executing) <= num_online_cpus()) {\n\t\t\tif (mce_timed_out(&timeout,\n\t\t\t\t\t  \"Timeout: Monarch CPU unable to finish machine check processing\"))\n\t\t\t\tgoto reset;\n\t\t\tndelay(SPINUNIT);\n\t\t}\n\n\t\tmce_reign();\n\t\tbarrier();\n\t\tret = 0;\n\t} else {\n\t\t \n\t\twhile (atomic_read(&mce_executing) != 0) {\n\t\t\tif (mce_timed_out(&timeout,\n\t\t\t\t\t  \"Timeout: Monarch CPU did not finish machine check processing\"))\n\t\t\t\tgoto reset;\n\t\t\tndelay(SPINUNIT);\n\t\t}\n\n\t\t \n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\t \nreset:\n\tatomic_set(&global_nwo, 0);\n\tatomic_set(&mce_callin, 0);\n\tcpumask_setall(&mce_missing_cpus);\n\tbarrier();\n\n\t \n\tatomic_set(&mce_executing, 0);\n\nout:\n\tinstrumentation_end();\n\n\treturn ret;\n}\n\nstatic __always_inline void mce_clear_state(unsigned long *toclear)\n{\n\tint i;\n\n\tfor (i = 0; i < this_cpu_read(mce_num_banks); i++) {\n\t\tif (arch_test_bit(i, toclear))\n\t\t\tmce_wrmsrl(mca_msr_reg(i, MCA_STATUS), 0);\n\t}\n}\n\n \nstatic noinstr bool mce_check_crashing_cpu(void)\n{\n\tunsigned int cpu = smp_processor_id();\n\n\tif (arch_cpu_is_offline(cpu) ||\n\t    (crashing_cpu != -1 && crashing_cpu != cpu)) {\n\t\tu64 mcgstatus;\n\n\t\tmcgstatus = __rdmsr(MSR_IA32_MCG_STATUS);\n\n\t\tif (boot_cpu_data.x86_vendor == X86_VENDOR_ZHAOXIN) {\n\t\t\tif (mcgstatus & MCG_STATUS_LMCES)\n\t\t\t\treturn false;\n\t\t}\n\n\t\tif (mcgstatus & MCG_STATUS_RIPV) {\n\t\t\t__wrmsr(MSR_IA32_MCG_STATUS, 0, 0);\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\nstatic __always_inline int\n__mc_scan_banks(struct mce *m, struct pt_regs *regs, struct mce *final,\n\t\tunsigned long *toclear, unsigned long *valid_banks, int no_way_out,\n\t\tint *worst)\n{\n\tstruct mce_bank *mce_banks = this_cpu_ptr(mce_banks_array);\n\tstruct mca_config *cfg = &mca_cfg;\n\tint severity, i, taint = 0;\n\n\tfor (i = 0; i < this_cpu_read(mce_num_banks); i++) {\n\t\tarch___clear_bit(i, toclear);\n\t\tif (!arch_test_bit(i, valid_banks))\n\t\t\tcontinue;\n\n\t\tif (!mce_banks[i].ctl)\n\t\t\tcontinue;\n\n\t\tm->misc = 0;\n\t\tm->addr = 0;\n\t\tm->bank = i;\n\n\t\tm->status = mce_rdmsrl(mca_msr_reg(i, MCA_STATUS));\n\t\tif (!(m->status & MCI_STATUS_VAL))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!(m->status & (cfg->ser ? MCI_STATUS_S : MCI_STATUS_UC)) &&\n\t\t\t!no_way_out)\n\t\t\tcontinue;\n\n\t\t \n\t\ttaint++;\n\n\t\tseverity = mce_severity(m, regs, NULL, true);\n\n\t\t \n\t\tif ((severity == MCE_KEEP_SEVERITY ||\n\t\t     severity == MCE_UCNA_SEVERITY) && !no_way_out)\n\t\t\tcontinue;\n\n\t\tarch___set_bit(i, toclear);\n\n\t\t \n\t\tif (severity == MCE_NO_SEVERITY)\n\t\t\tcontinue;\n\n\t\tmce_read_aux(m, i);\n\n\t\t \n\t\tm->severity = severity;\n\n\t\t \n\t\tinstrumentation_begin();\n\t\tmce_log(m);\n\t\tinstrumentation_end();\n\n\t\tif (severity > *worst) {\n\t\t\t*final = *m;\n\t\t\t*worst = severity;\n\t\t}\n\t}\n\n\t \n\t*m = *final;\n\n\treturn taint;\n}\n\nstatic void kill_me_now(struct callback_head *ch)\n{\n\tstruct task_struct *p = container_of(ch, struct task_struct, mce_kill_me);\n\n\tp->mce_count = 0;\n\tforce_sig(SIGBUS);\n}\n\nstatic void kill_me_maybe(struct callback_head *cb)\n{\n\tstruct task_struct *p = container_of(cb, struct task_struct, mce_kill_me);\n\tint flags = MF_ACTION_REQUIRED;\n\tunsigned long pfn;\n\tint ret;\n\n\tp->mce_count = 0;\n\tpr_err(\"Uncorrected hardware memory error in user-access at %llx\", p->mce_addr);\n\n\tif (!p->mce_ripv)\n\t\tflags |= MF_MUST_KILL;\n\n\tpfn = (p->mce_addr & MCI_ADDR_PHYSADDR) >> PAGE_SHIFT;\n\tret = memory_failure(pfn, flags);\n\tif (!ret) {\n\t\tset_mce_nospec(pfn);\n\t\tsync_core();\n\t\treturn;\n\t}\n\n\t \n\tif (ret == -EHWPOISON || ret == -EOPNOTSUPP)\n\t\treturn;\n\n\tpr_err(\"Memory error not recovered\");\n\tkill_me_now(cb);\n}\n\nstatic void kill_me_never(struct callback_head *cb)\n{\n\tstruct task_struct *p = container_of(cb, struct task_struct, mce_kill_me);\n\tunsigned long pfn;\n\n\tp->mce_count = 0;\n\tpr_err(\"Kernel accessed poison in user space at %llx\\n\", p->mce_addr);\n\tpfn = (p->mce_addr & MCI_ADDR_PHYSADDR) >> PAGE_SHIFT;\n\tif (!memory_failure(pfn, 0))\n\t\tset_mce_nospec(pfn);\n}\n\nstatic void queue_task_work(struct mce *m, char *msg, void (*func)(struct callback_head *))\n{\n\tint count = ++current->mce_count;\n\n\t \n\tif (count == 1) {\n\t\tcurrent->mce_addr = m->addr;\n\t\tcurrent->mce_kflags = m->kflags;\n\t\tcurrent->mce_ripv = !!(m->mcgstatus & MCG_STATUS_RIPV);\n\t\tcurrent->mce_whole_page = whole_page(m);\n\t\tcurrent->mce_kill_me.func = func;\n\t}\n\n\t \n\tif (count > 10)\n\t\tmce_panic(\"Too many consecutive machine checks while accessing user data\", m, msg);\n\n\t \n\tif (count > 1 && (current->mce_addr >> PAGE_SHIFT) != (m->addr >> PAGE_SHIFT))\n\t\tmce_panic(\"Consecutive machine checks to different user pages\", m, msg);\n\n\t \n\tif (count > 1)\n\t\treturn;\n\n\ttask_work_add(current, &current->mce_kill_me, TWA_RESUME);\n}\n\n \nstatic noinstr void unexpected_machine_check(struct pt_regs *regs)\n{\n\tinstrumentation_begin();\n\tpr_err(\"CPU#%d: Unexpected int18 (Machine Check)\\n\",\n\t       smp_processor_id());\n\tinstrumentation_end();\n}\n\n \nnoinstr void do_machine_check(struct pt_regs *regs)\n{\n\tint worst = 0, order, no_way_out, kill_current_task, lmce, taint = 0;\n\tDECLARE_BITMAP(valid_banks, MAX_NR_BANKS) = { 0 };\n\tDECLARE_BITMAP(toclear, MAX_NR_BANKS) = { 0 };\n\tstruct mce m, *final;\n\tchar *msg = NULL;\n\n\tif (unlikely(mce_flags.p5))\n\t\treturn pentium_machine_check(regs);\n\telse if (unlikely(mce_flags.winchip))\n\t\treturn winchip_machine_check(regs);\n\telse if (unlikely(!mca_cfg.initialized))\n\t\treturn unexpected_machine_check(regs);\n\n\tif (mce_flags.skx_repmov_quirk && quirk_skylake_repmov())\n\t\tgoto clear;\n\n\t \n\torder = -1;\n\n\t \n\tno_way_out = 0;\n\n\t \n\tkill_current_task = 0;\n\n\t \n\tlmce = 1;\n\n\tthis_cpu_inc(mce_exception_count);\n\n\tmce_gather_info(&m, regs);\n\tm.tsc = rdtsc();\n\n\tfinal = this_cpu_ptr(&mces_seen);\n\t*final = m;\n\n\tno_way_out = mce_no_way_out(&m, &msg, valid_banks, regs);\n\n\tbarrier();\n\n\t \n\tif (!(m.mcgstatus & MCG_STATUS_RIPV))\n\t\tkill_current_task = 1;\n\t \n\tif (m.cpuvendor == X86_VENDOR_INTEL ||\n\t    m.cpuvendor == X86_VENDOR_ZHAOXIN)\n\t\tlmce = m.mcgstatus & MCG_STATUS_LMCES;\n\n\t \n\tif (lmce) {\n\t\tif (no_way_out)\n\t\t\tmce_panic(\"Fatal local machine check\", &m, msg);\n\t} else {\n\t\torder = mce_start(&no_way_out);\n\t}\n\n\ttaint = __mc_scan_banks(&m, regs, final, toclear, valid_banks, no_way_out, &worst);\n\n\tif (!no_way_out)\n\t\tmce_clear_state(toclear);\n\n\t \n\tif (!lmce) {\n\t\tif (mce_end(order) < 0) {\n\t\t\tif (!no_way_out)\n\t\t\t\tno_way_out = worst >= MCE_PANIC_SEVERITY;\n\n\t\t\tif (no_way_out)\n\t\t\t\tmce_panic(\"Fatal machine check on current CPU\", &m, msg);\n\t\t}\n\t} else {\n\t\t \n\t\tif (worst >= MCE_PANIC_SEVERITY) {\n\t\t\tmce_severity(&m, regs, &msg, true);\n\t\t\tmce_panic(\"Local fatal machine check!\", &m, msg);\n\t\t}\n\t}\n\n\t \n\tinstrumentation_begin();\n\n\tif (taint)\n\t\tadd_taint(TAINT_MACHINE_CHECK, LOCKDEP_NOW_UNRELIABLE);\n\n\tif (worst != MCE_AR_SEVERITY && !kill_current_task)\n\t\tgoto out;\n\n\t \n\tif ((m.cs & 3) == 3) {\n\t\t \n\t\tBUG_ON(!on_thread_stack() || !user_mode(regs));\n\n\t\tif (!mce_usable_address(&m))\n\t\t\tqueue_task_work(&m, msg, kill_me_now);\n\t\telse\n\t\t\tqueue_task_work(&m, msg, kill_me_maybe);\n\n\t} else {\n\t\t \n\t\tif (m.kflags & MCE_IN_KERNEL_RECOV) {\n\t\t\tif (!fixup_exception(regs, X86_TRAP_MC, 0, 0))\n\t\t\t\tmce_panic(\"Failed kernel mode recovery\", &m, msg);\n\t\t}\n\n\t\tif (m.kflags & MCE_IN_KERNEL_COPYIN)\n\t\t\tqueue_task_work(&m, msg, kill_me_never);\n\t}\n\nout:\n\tinstrumentation_end();\n\nclear:\n\tmce_wrmsrl(MSR_IA32_MCG_STATUS, 0);\n}\nEXPORT_SYMBOL_GPL(do_machine_check);\n\n#ifndef CONFIG_MEMORY_FAILURE\nint memory_failure(unsigned long pfn, int flags)\n{\n\t \n\tBUG_ON(flags & MF_ACTION_REQUIRED);\n\tpr_err(\"Uncorrected memory error in page 0x%lx ignored\\n\"\n\t       \"Rebuild kernel with CONFIG_MEMORY_FAILURE=y for smarter handling\\n\",\n\t       pfn);\n\n\treturn 0;\n}\n#endif\n\n \nstatic unsigned long check_interval = INITIAL_CHECK_INTERVAL;\n\nstatic DEFINE_PER_CPU(unsigned long, mce_next_interval);  \nstatic DEFINE_PER_CPU(struct timer_list, mce_timer);\n\nstatic unsigned long mce_adjust_timer_default(unsigned long interval)\n{\n\treturn interval;\n}\n\nstatic unsigned long (*mce_adjust_timer)(unsigned long interval) = mce_adjust_timer_default;\n\nstatic void __start_timer(struct timer_list *t, unsigned long interval)\n{\n\tunsigned long when = jiffies + interval;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\n\tif (!timer_pending(t) || time_before(when, t->expires))\n\t\tmod_timer(t, round_jiffies(when));\n\n\tlocal_irq_restore(flags);\n}\n\nstatic void mc_poll_banks_default(void)\n{\n\tmachine_check_poll(0, this_cpu_ptr(&mce_poll_banks));\n}\n\nvoid (*mc_poll_banks)(void) = mc_poll_banks_default;\n\nstatic void mce_timer_fn(struct timer_list *t)\n{\n\tstruct timer_list *cpu_t = this_cpu_ptr(&mce_timer);\n\tunsigned long iv;\n\n\tWARN_ON(cpu_t != t);\n\n\tiv = __this_cpu_read(mce_next_interval);\n\n\tif (mce_available(this_cpu_ptr(&cpu_info))) {\n\t\tmc_poll_banks();\n\n\t\tif (mce_intel_cmci_poll()) {\n\t\t\tiv = mce_adjust_timer(iv);\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\t \n\tif (mce_notify_irq())\n\t\tiv = max(iv / 2, (unsigned long) HZ/100);\n\telse\n\t\tiv = min(iv * 2, round_jiffies_relative(check_interval * HZ));\n\ndone:\n\t__this_cpu_write(mce_next_interval, iv);\n\t__start_timer(t, iv);\n}\n\n \nvoid mce_timer_kick(unsigned long interval)\n{\n\tstruct timer_list *t = this_cpu_ptr(&mce_timer);\n\tunsigned long iv = __this_cpu_read(mce_next_interval);\n\n\t__start_timer(t, interval);\n\n\tif (interval < iv)\n\t\t__this_cpu_write(mce_next_interval, interval);\n}\n\n \nstatic void mce_timer_delete_all(void)\n{\n\tint cpu;\n\n\tfor_each_online_cpu(cpu)\n\t\tdel_timer_sync(&per_cpu(mce_timer, cpu));\n}\n\n \nint mce_notify_irq(void)\n{\n\t \n\tstatic DEFINE_RATELIMIT_STATE(ratelimit, 60*HZ, 2);\n\n\tif (test_and_clear_bit(0, &mce_need_notify)) {\n\t\tmce_work_trigger();\n\n\t\tif (__ratelimit(&ratelimit))\n\t\t\tpr_info(HW_ERR \"Machine check events logged\\n\");\n\n\t\treturn 1;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(mce_notify_irq);\n\nstatic void __mcheck_cpu_mce_banks_init(void)\n{\n\tstruct mce_bank *mce_banks = this_cpu_ptr(mce_banks_array);\n\tu8 n_banks = this_cpu_read(mce_num_banks);\n\tint i;\n\n\tfor (i = 0; i < n_banks; i++) {\n\t\tstruct mce_bank *b = &mce_banks[i];\n\n\t\t \n\t\tb->ctl = -1ULL;\n\t\tb->init = true;\n\t}\n}\n\n \nstatic void __mcheck_cpu_cap_init(void)\n{\n\tu64 cap;\n\tu8 b;\n\n\trdmsrl(MSR_IA32_MCG_CAP, cap);\n\n\tb = cap & MCG_BANKCNT_MASK;\n\n\tif (b > MAX_NR_BANKS) {\n\t\tpr_warn(\"CPU%d: Using only %u machine check banks out of %u\\n\",\n\t\t\tsmp_processor_id(), MAX_NR_BANKS, b);\n\t\tb = MAX_NR_BANKS;\n\t}\n\n\tthis_cpu_write(mce_num_banks, b);\n\n\t__mcheck_cpu_mce_banks_init();\n\n\t \n\tif ((cap & MCG_EXT_P) && MCG_EXT_CNT(cap) >= 9)\n\t\tmca_cfg.rip_msr = MSR_IA32_MCG_EIP;\n\n\tif (cap & MCG_SER_P)\n\t\tmca_cfg.ser = 1;\n}\n\nstatic void __mcheck_cpu_init_generic(void)\n{\n\tenum mcp_flags m_fl = 0;\n\tmce_banks_t all_banks;\n\tu64 cap;\n\n\tif (!mca_cfg.bootlog)\n\t\tm_fl = MCP_DONTLOG;\n\n\t \n\tbitmap_fill(all_banks, MAX_NR_BANKS);\n\tmachine_check_poll(MCP_UC | MCP_QUEUE_LOG | m_fl, &all_banks);\n\n\tcr4_set_bits(X86_CR4_MCE);\n\n\trdmsrl(MSR_IA32_MCG_CAP, cap);\n\tif (cap & MCG_CTL_P)\n\t\twrmsr(MSR_IA32_MCG_CTL, 0xffffffff, 0xffffffff);\n}\n\nstatic void __mcheck_cpu_init_clear_banks(void)\n{\n\tstruct mce_bank *mce_banks = this_cpu_ptr(mce_banks_array);\n\tint i;\n\n\tfor (i = 0; i < this_cpu_read(mce_num_banks); i++) {\n\t\tstruct mce_bank *b = &mce_banks[i];\n\n\t\tif (!b->init)\n\t\t\tcontinue;\n\t\twrmsrl(mca_msr_reg(i, MCA_CTL), b->ctl);\n\t\twrmsrl(mca_msr_reg(i, MCA_STATUS), 0);\n\t}\n}\n\n \nstatic void __mcheck_cpu_check_banks(void)\n{\n\tstruct mce_bank *mce_banks = this_cpu_ptr(mce_banks_array);\n\tu64 msrval;\n\tint i;\n\n\tfor (i = 0; i < this_cpu_read(mce_num_banks); i++) {\n\t\tstruct mce_bank *b = &mce_banks[i];\n\n\t\tif (!b->init)\n\t\t\tcontinue;\n\n\t\trdmsrl(mca_msr_reg(i, MCA_CTL), msrval);\n\t\tb->init = !!msrval;\n\t}\n}\n\n \nstatic int __mcheck_cpu_apply_quirks(struct cpuinfo_x86 *c)\n{\n\tstruct mce_bank *mce_banks = this_cpu_ptr(mce_banks_array);\n\tstruct mca_config *cfg = &mca_cfg;\n\n\tif (c->x86_vendor == X86_VENDOR_UNKNOWN) {\n\t\tpr_info(\"unknown CPU type - not enabling MCE support\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t \n\tif (c->x86_vendor == X86_VENDOR_AMD) {\n\t\tif (c->x86 == 15 && this_cpu_read(mce_num_banks) > 4) {\n\t\t\t \n\t\t\tclear_bit(10, (unsigned long *)&mce_banks[4].ctl);\n\t\t}\n\t\tif (c->x86 < 0x11 && cfg->bootlog < 0) {\n\t\t\t \n\t\t\tcfg->bootlog = 0;\n\t\t}\n\t\t \n\t\tif (c->x86 == 6 && this_cpu_read(mce_num_banks) > 0)\n\t\t\tmce_banks[0].ctl = 0;\n\n\t\t \n\t\tif (c->x86 == 0x15 && c->x86_model <= 0xf)\n\t\t\tmce_flags.overflow_recov = 1;\n\n\t\tif (c->x86 >= 0x17 && c->x86 <= 0x1A)\n\t\t\tmce_flags.zen_ifu_quirk = 1;\n\n\t}\n\n\tif (c->x86_vendor == X86_VENDOR_INTEL) {\n\t\t \n\n\t\tif (c->x86 == 6 && c->x86_model < 0x1A && this_cpu_read(mce_num_banks) > 0)\n\t\t\tmce_banks[0].init = false;\n\n\t\t \n\t\tif ((c->x86 > 6 || (c->x86 == 6 && c->x86_model >= 0xe)) &&\n\t\t\tcfg->monarch_timeout < 0)\n\t\t\tcfg->monarch_timeout = USEC_PER_SEC;\n\n\t\t \n\t\tif (c->x86 == 6 && c->x86_model <= 13 && cfg->bootlog < 0)\n\t\t\tcfg->bootlog = 0;\n\n\t\tif (c->x86 == 6 && c->x86_model == 45)\n\t\t\tmce_flags.snb_ifu_quirk = 1;\n\n\t\t \n\t\tif (c->x86 == 6 && c->x86_model == INTEL_FAM6_SKYLAKE_X)\n\t\t\tmce_flags.skx_repmov_quirk = 1;\n\t}\n\n\tif (c->x86_vendor == X86_VENDOR_ZHAOXIN) {\n\t\t \n\t\tif (c->x86 > 6 || (c->x86_model == 0x19 || c->x86_model == 0x1f)) {\n\t\t\tif (cfg->monarch_timeout < 0)\n\t\t\t\tcfg->monarch_timeout = USEC_PER_SEC;\n\t\t}\n\t}\n\n\tif (cfg->monarch_timeout < 0)\n\t\tcfg->monarch_timeout = 0;\n\tif (cfg->bootlog != 0)\n\t\tcfg->panic_timeout = 30;\n\n\treturn 0;\n}\n\nstatic int __mcheck_cpu_ancient_init(struct cpuinfo_x86 *c)\n{\n\tif (c->x86 != 5)\n\t\treturn 0;\n\n\tswitch (c->x86_vendor) {\n\tcase X86_VENDOR_INTEL:\n\t\tintel_p5_mcheck_init(c);\n\t\tmce_flags.p5 = 1;\n\t\treturn 1;\n\tcase X86_VENDOR_CENTAUR:\n\t\twinchip_mcheck_init(c);\n\t\tmce_flags.winchip = 1;\n\t\treturn 1;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void __mcheck_cpu_init_early(struct cpuinfo_x86 *c)\n{\n\tif (c->x86_vendor == X86_VENDOR_AMD || c->x86_vendor == X86_VENDOR_HYGON) {\n\t\tmce_flags.overflow_recov = !!cpu_has(c, X86_FEATURE_OVERFLOW_RECOV);\n\t\tmce_flags.succor\t = !!cpu_has(c, X86_FEATURE_SUCCOR);\n\t\tmce_flags.smca\t\t = !!cpu_has(c, X86_FEATURE_SMCA);\n\t\tmce_flags.amd_threshold\t = 1;\n\t}\n}\n\nstatic void mce_centaur_feature_init(struct cpuinfo_x86 *c)\n{\n\tstruct mca_config *cfg = &mca_cfg;\n\n\t  \n\tif ((c->x86 == 6 && c->x86_model == 0xf && c->x86_stepping >= 0xe) ||\n\t     c->x86 > 6) {\n\t\tif (cfg->monarch_timeout < 0)\n\t\t\tcfg->monarch_timeout = USEC_PER_SEC;\n\t}\n}\n\nstatic void mce_zhaoxin_feature_init(struct cpuinfo_x86 *c)\n{\n\tstruct mce_bank *mce_banks = this_cpu_ptr(mce_banks_array);\n\n\t \n\tif ((c->x86 == 7 && c->x86_model == 0x1b) ||\n\t    (c->x86_model == 0x19 || c->x86_model == 0x1f)) {\n\t\tif (this_cpu_read(mce_num_banks) > 8)\n\t\t\tmce_banks[8].ctl = 0;\n\t}\n\n\tintel_init_cmci();\n\tintel_init_lmce();\n\tmce_adjust_timer = cmci_intel_adjust_timer;\n}\n\nstatic void mce_zhaoxin_feature_clear(struct cpuinfo_x86 *c)\n{\n\tintel_clear_lmce();\n}\n\nstatic void __mcheck_cpu_init_vendor(struct cpuinfo_x86 *c)\n{\n\tswitch (c->x86_vendor) {\n\tcase X86_VENDOR_INTEL:\n\t\tmce_intel_feature_init(c);\n\t\tmce_adjust_timer = cmci_intel_adjust_timer;\n\t\tbreak;\n\n\tcase X86_VENDOR_AMD: {\n\t\tmce_amd_feature_init(c);\n\t\tbreak;\n\t\t}\n\n\tcase X86_VENDOR_HYGON:\n\t\tmce_hygon_feature_init(c);\n\t\tbreak;\n\n\tcase X86_VENDOR_CENTAUR:\n\t\tmce_centaur_feature_init(c);\n\t\tbreak;\n\n\tcase X86_VENDOR_ZHAOXIN:\n\t\tmce_zhaoxin_feature_init(c);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void __mcheck_cpu_clear_vendor(struct cpuinfo_x86 *c)\n{\n\tswitch (c->x86_vendor) {\n\tcase X86_VENDOR_INTEL:\n\t\tmce_intel_feature_clear(c);\n\t\tbreak;\n\n\tcase X86_VENDOR_ZHAOXIN:\n\t\tmce_zhaoxin_feature_clear(c);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void mce_start_timer(struct timer_list *t)\n{\n\tunsigned long iv = check_interval * HZ;\n\n\tif (mca_cfg.ignore_ce || !iv)\n\t\treturn;\n\n\tthis_cpu_write(mce_next_interval, iv);\n\t__start_timer(t, iv);\n}\n\nstatic void __mcheck_cpu_setup_timer(void)\n{\n\tstruct timer_list *t = this_cpu_ptr(&mce_timer);\n\n\ttimer_setup(t, mce_timer_fn, TIMER_PINNED);\n}\n\nstatic void __mcheck_cpu_init_timer(void)\n{\n\tstruct timer_list *t = this_cpu_ptr(&mce_timer);\n\n\ttimer_setup(t, mce_timer_fn, TIMER_PINNED);\n\tmce_start_timer(t);\n}\n\nbool filter_mce(struct mce *m)\n{\n\tif (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)\n\t\treturn amd_filter_mce(m);\n\tif (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL)\n\t\treturn intel_filter_mce(m);\n\n\treturn false;\n}\n\nstatic __always_inline void exc_machine_check_kernel(struct pt_regs *regs)\n{\n\tirqentry_state_t irq_state;\n\n\tWARN_ON_ONCE(user_mode(regs));\n\n\t \n\tif (mca_cfg.initialized && mce_check_crashing_cpu())\n\t\treturn;\n\n\tirq_state = irqentry_nmi_enter(regs);\n\n\tdo_machine_check(regs);\n\n\tirqentry_nmi_exit(regs, irq_state);\n}\n\nstatic __always_inline void exc_machine_check_user(struct pt_regs *regs)\n{\n\tirqentry_enter_from_user_mode(regs);\n\n\tdo_machine_check(regs);\n\n\tirqentry_exit_to_user_mode(regs);\n}\n\n#ifdef CONFIG_X86_64\n \nDEFINE_IDTENTRY_MCE(exc_machine_check)\n{\n\tunsigned long dr7;\n\n\tdr7 = local_db_save();\n\texc_machine_check_kernel(regs);\n\tlocal_db_restore(dr7);\n}\n\n \nDEFINE_IDTENTRY_MCE_USER(exc_machine_check)\n{\n\tunsigned long dr7;\n\n\tdr7 = local_db_save();\n\texc_machine_check_user(regs);\n\tlocal_db_restore(dr7);\n}\n#else\n \nDEFINE_IDTENTRY_RAW(exc_machine_check)\n{\n\tunsigned long dr7;\n\n\tdr7 = local_db_save();\n\tif (user_mode(regs))\n\t\texc_machine_check_user(regs);\n\telse\n\t\texc_machine_check_kernel(regs);\n\tlocal_db_restore(dr7);\n}\n#endif\n\n \nvoid mcheck_cpu_init(struct cpuinfo_x86 *c)\n{\n\tif (mca_cfg.disabled)\n\t\treturn;\n\n\tif (__mcheck_cpu_ancient_init(c))\n\t\treturn;\n\n\tif (!mce_available(c))\n\t\treturn;\n\n\t__mcheck_cpu_cap_init();\n\n\tif (__mcheck_cpu_apply_quirks(c) < 0) {\n\t\tmca_cfg.disabled = 1;\n\t\treturn;\n\t}\n\n\tif (mce_gen_pool_init()) {\n\t\tmca_cfg.disabled = 1;\n\t\tpr_emerg(\"Couldn't allocate MCE records pool!\\n\");\n\t\treturn;\n\t}\n\n\tmca_cfg.initialized = 1;\n\n\t__mcheck_cpu_init_early(c);\n\t__mcheck_cpu_init_generic();\n\t__mcheck_cpu_init_vendor(c);\n\t__mcheck_cpu_init_clear_banks();\n\t__mcheck_cpu_check_banks();\n\t__mcheck_cpu_setup_timer();\n}\n\n \nvoid mcheck_cpu_clear(struct cpuinfo_x86 *c)\n{\n\tif (mca_cfg.disabled)\n\t\treturn;\n\n\tif (!mce_available(c))\n\t\treturn;\n\n\t \n\t__mcheck_cpu_clear_vendor(c);\n\n}\n\nstatic void __mce_disable_bank(void *arg)\n{\n\tint bank = *((int *)arg);\n\t__clear_bit(bank, this_cpu_ptr(mce_poll_banks));\n\tcmci_disable_bank(bank);\n}\n\nvoid mce_disable_bank(int bank)\n{\n\tif (bank >= this_cpu_read(mce_num_banks)) {\n\t\tpr_warn(FW_BUG\n\t\t\t\"Ignoring request to disable invalid MCA bank %d.\\n\",\n\t\t\tbank);\n\t\treturn;\n\t}\n\tset_bit(bank, mce_banks_ce_disabled);\n\ton_each_cpu(__mce_disable_bank, &bank, 1);\n}\n\n \nstatic int __init mcheck_enable(char *str)\n{\n\tstruct mca_config *cfg = &mca_cfg;\n\n\tif (*str == 0) {\n\t\tenable_p5_mce();\n\t\treturn 1;\n\t}\n\tif (*str == '=')\n\t\tstr++;\n\tif (!strcmp(str, \"off\"))\n\t\tcfg->disabled = 1;\n\telse if (!strcmp(str, \"no_cmci\"))\n\t\tcfg->cmci_disabled = true;\n\telse if (!strcmp(str, \"no_lmce\"))\n\t\tcfg->lmce_disabled = 1;\n\telse if (!strcmp(str, \"dont_log_ce\"))\n\t\tcfg->dont_log_ce = true;\n\telse if (!strcmp(str, \"print_all\"))\n\t\tcfg->print_all = true;\n\telse if (!strcmp(str, \"ignore_ce\"))\n\t\tcfg->ignore_ce = true;\n\telse if (!strcmp(str, \"bootlog\") || !strcmp(str, \"nobootlog\"))\n\t\tcfg->bootlog = (str[0] == 'b');\n\telse if (!strcmp(str, \"bios_cmci_threshold\"))\n\t\tcfg->bios_cmci_threshold = 1;\n\telse if (!strcmp(str, \"recovery\"))\n\t\tcfg->recovery = 1;\n\telse if (isdigit(str[0]))\n\t\tget_option(&str, &(cfg->monarch_timeout));\n\telse {\n\t\tpr_info(\"mce argument %s ignored. Please use /sys\\n\", str);\n\t\treturn 0;\n\t}\n\treturn 1;\n}\n__setup(\"mce\", mcheck_enable);\n\nint __init mcheck_init(void)\n{\n\tmce_register_decode_chain(&early_nb);\n\tmce_register_decode_chain(&mce_uc_nb);\n\tmce_register_decode_chain(&mce_default_nb);\n\n\tINIT_WORK(&mce_work, mce_gen_pool_process);\n\tinit_irq_work(&mce_irq_work, mce_irq_work_cb);\n\n\treturn 0;\n}\n\n \n\n \nstatic void mce_disable_error_reporting(void)\n{\n\tstruct mce_bank *mce_banks = this_cpu_ptr(mce_banks_array);\n\tint i;\n\n\tfor (i = 0; i < this_cpu_read(mce_num_banks); i++) {\n\t\tstruct mce_bank *b = &mce_banks[i];\n\n\t\tif (b->init)\n\t\t\twrmsrl(mca_msr_reg(i, MCA_CTL), 0);\n\t}\n\treturn;\n}\n\nstatic void vendor_disable_error_reporting(void)\n{\n\t \n\tif (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL ||\n\t    boot_cpu_data.x86_vendor == X86_VENDOR_HYGON ||\n\t    boot_cpu_data.x86_vendor == X86_VENDOR_AMD ||\n\t    boot_cpu_data.x86_vendor == X86_VENDOR_ZHAOXIN)\n\t\treturn;\n\n\tmce_disable_error_reporting();\n}\n\nstatic int mce_syscore_suspend(void)\n{\n\tvendor_disable_error_reporting();\n\treturn 0;\n}\n\nstatic void mce_syscore_shutdown(void)\n{\n\tvendor_disable_error_reporting();\n}\n\n \nstatic void mce_syscore_resume(void)\n{\n\t__mcheck_cpu_init_generic();\n\t__mcheck_cpu_init_vendor(raw_cpu_ptr(&cpu_info));\n\t__mcheck_cpu_init_clear_banks();\n}\n\nstatic struct syscore_ops mce_syscore_ops = {\n\t.suspend\t= mce_syscore_suspend,\n\t.shutdown\t= mce_syscore_shutdown,\n\t.resume\t\t= mce_syscore_resume,\n};\n\n \n\nstatic void mce_cpu_restart(void *data)\n{\n\tif (!mce_available(raw_cpu_ptr(&cpu_info)))\n\t\treturn;\n\t__mcheck_cpu_init_generic();\n\t__mcheck_cpu_init_clear_banks();\n\t__mcheck_cpu_init_timer();\n}\n\n \nstatic void mce_restart(void)\n{\n\tmce_timer_delete_all();\n\ton_each_cpu(mce_cpu_restart, NULL, 1);\n\tmce_schedule_work();\n}\n\n \nstatic void mce_disable_cmci(void *data)\n{\n\tif (!mce_available(raw_cpu_ptr(&cpu_info)))\n\t\treturn;\n\tcmci_clear();\n}\n\nstatic void mce_enable_ce(void *all)\n{\n\tif (!mce_available(raw_cpu_ptr(&cpu_info)))\n\t\treturn;\n\tcmci_reenable();\n\tcmci_recheck();\n\tif (all)\n\t\t__mcheck_cpu_init_timer();\n}\n\nstatic struct bus_type mce_subsys = {\n\t.name\t\t= \"machinecheck\",\n\t.dev_name\t= \"machinecheck\",\n};\n\nDEFINE_PER_CPU(struct device *, mce_device);\n\nstatic inline struct mce_bank_dev *attr_to_bank(struct device_attribute *attr)\n{\n\treturn container_of(attr, struct mce_bank_dev, attr);\n}\n\nstatic ssize_t show_bank(struct device *s, struct device_attribute *attr,\n\t\t\t char *buf)\n{\n\tu8 bank = attr_to_bank(attr)->bank;\n\tstruct mce_bank *b;\n\n\tif (bank >= per_cpu(mce_num_banks, s->id))\n\t\treturn -EINVAL;\n\n\tb = &per_cpu(mce_banks_array, s->id)[bank];\n\n\tif (!b->init)\n\t\treturn -ENODEV;\n\n\treturn sprintf(buf, \"%llx\\n\", b->ctl);\n}\n\nstatic ssize_t set_bank(struct device *s, struct device_attribute *attr,\n\t\t\tconst char *buf, size_t size)\n{\n\tu8 bank = attr_to_bank(attr)->bank;\n\tstruct mce_bank *b;\n\tu64 new;\n\n\tif (kstrtou64(buf, 0, &new) < 0)\n\t\treturn -EINVAL;\n\n\tif (bank >= per_cpu(mce_num_banks, s->id))\n\t\treturn -EINVAL;\n\n\tb = &per_cpu(mce_banks_array, s->id)[bank];\n\n\tif (!b->init)\n\t\treturn -ENODEV;\n\n\tb->ctl = new;\n\tmce_restart();\n\n\treturn size;\n}\n\nstatic ssize_t set_ignore_ce(struct device *s,\n\t\t\t     struct device_attribute *attr,\n\t\t\t     const char *buf, size_t size)\n{\n\tu64 new;\n\n\tif (kstrtou64(buf, 0, &new) < 0)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&mce_sysfs_mutex);\n\tif (mca_cfg.ignore_ce ^ !!new) {\n\t\tif (new) {\n\t\t\t \n\t\t\tmce_timer_delete_all();\n\t\t\ton_each_cpu(mce_disable_cmci, NULL, 1);\n\t\t\tmca_cfg.ignore_ce = true;\n\t\t} else {\n\t\t\t \n\t\t\tmca_cfg.ignore_ce = false;\n\t\t\ton_each_cpu(mce_enable_ce, (void *)1, 1);\n\t\t}\n\t}\n\tmutex_unlock(&mce_sysfs_mutex);\n\n\treturn size;\n}\n\nstatic ssize_t set_cmci_disabled(struct device *s,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t const char *buf, size_t size)\n{\n\tu64 new;\n\n\tif (kstrtou64(buf, 0, &new) < 0)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&mce_sysfs_mutex);\n\tif (mca_cfg.cmci_disabled ^ !!new) {\n\t\tif (new) {\n\t\t\t \n\t\t\ton_each_cpu(mce_disable_cmci, NULL, 1);\n\t\t\tmca_cfg.cmci_disabled = true;\n\t\t} else {\n\t\t\t \n\t\t\tmca_cfg.cmci_disabled = false;\n\t\t\ton_each_cpu(mce_enable_ce, NULL, 1);\n\t\t}\n\t}\n\tmutex_unlock(&mce_sysfs_mutex);\n\n\treturn size;\n}\n\nstatic ssize_t store_int_with_restart(struct device *s,\n\t\t\t\t      struct device_attribute *attr,\n\t\t\t\t      const char *buf, size_t size)\n{\n\tunsigned long old_check_interval = check_interval;\n\tssize_t ret = device_store_ulong(s, attr, buf, size);\n\n\tif (check_interval == old_check_interval)\n\t\treturn ret;\n\n\tmutex_lock(&mce_sysfs_mutex);\n\tmce_restart();\n\tmutex_unlock(&mce_sysfs_mutex);\n\n\treturn ret;\n}\n\nstatic DEVICE_INT_ATTR(monarch_timeout, 0644, mca_cfg.monarch_timeout);\nstatic DEVICE_BOOL_ATTR(dont_log_ce, 0644, mca_cfg.dont_log_ce);\nstatic DEVICE_BOOL_ATTR(print_all, 0644, mca_cfg.print_all);\n\nstatic struct dev_ext_attribute dev_attr_check_interval = {\n\t__ATTR(check_interval, 0644, device_show_int, store_int_with_restart),\n\t&check_interval\n};\n\nstatic struct dev_ext_attribute dev_attr_ignore_ce = {\n\t__ATTR(ignore_ce, 0644, device_show_bool, set_ignore_ce),\n\t&mca_cfg.ignore_ce\n};\n\nstatic struct dev_ext_attribute dev_attr_cmci_disabled = {\n\t__ATTR(cmci_disabled, 0644, device_show_bool, set_cmci_disabled),\n\t&mca_cfg.cmci_disabled\n};\n\nstatic struct device_attribute *mce_device_attrs[] = {\n\t&dev_attr_check_interval.attr,\n#ifdef CONFIG_X86_MCELOG_LEGACY\n\t&dev_attr_trigger,\n#endif\n\t&dev_attr_monarch_timeout.attr,\n\t&dev_attr_dont_log_ce.attr,\n\t&dev_attr_print_all.attr,\n\t&dev_attr_ignore_ce.attr,\n\t&dev_attr_cmci_disabled.attr,\n\tNULL\n};\n\nstatic cpumask_var_t mce_device_initialized;\n\nstatic void mce_device_release(struct device *dev)\n{\n\tkfree(dev);\n}\n\n \nstatic int mce_device_create(unsigned int cpu)\n{\n\tstruct device *dev;\n\tint err;\n\tint i, j;\n\n\tif (!mce_available(&boot_cpu_data))\n\t\treturn -EIO;\n\n\tdev = per_cpu(mce_device, cpu);\n\tif (dev)\n\t\treturn 0;\n\n\tdev = kzalloc(sizeof(*dev), GFP_KERNEL);\n\tif (!dev)\n\t\treturn -ENOMEM;\n\tdev->id  = cpu;\n\tdev->bus = &mce_subsys;\n\tdev->release = &mce_device_release;\n\n\terr = device_register(dev);\n\tif (err) {\n\t\tput_device(dev);\n\t\treturn err;\n\t}\n\n\tfor (i = 0; mce_device_attrs[i]; i++) {\n\t\terr = device_create_file(dev, mce_device_attrs[i]);\n\t\tif (err)\n\t\t\tgoto error;\n\t}\n\tfor (j = 0; j < per_cpu(mce_num_banks, cpu); j++) {\n\t\terr = device_create_file(dev, &mce_bank_devs[j].attr);\n\t\tif (err)\n\t\t\tgoto error2;\n\t}\n\tcpumask_set_cpu(cpu, mce_device_initialized);\n\tper_cpu(mce_device, cpu) = dev;\n\n\treturn 0;\nerror2:\n\twhile (--j >= 0)\n\t\tdevice_remove_file(dev, &mce_bank_devs[j].attr);\nerror:\n\twhile (--i >= 0)\n\t\tdevice_remove_file(dev, mce_device_attrs[i]);\n\n\tdevice_unregister(dev);\n\n\treturn err;\n}\n\nstatic void mce_device_remove(unsigned int cpu)\n{\n\tstruct device *dev = per_cpu(mce_device, cpu);\n\tint i;\n\n\tif (!cpumask_test_cpu(cpu, mce_device_initialized))\n\t\treturn;\n\n\tfor (i = 0; mce_device_attrs[i]; i++)\n\t\tdevice_remove_file(dev, mce_device_attrs[i]);\n\n\tfor (i = 0; i < per_cpu(mce_num_banks, cpu); i++)\n\t\tdevice_remove_file(dev, &mce_bank_devs[i].attr);\n\n\tdevice_unregister(dev);\n\tcpumask_clear_cpu(cpu, mce_device_initialized);\n\tper_cpu(mce_device, cpu) = NULL;\n}\n\n \nstatic void mce_disable_cpu(void)\n{\n\tif (!mce_available(raw_cpu_ptr(&cpu_info)))\n\t\treturn;\n\n\tif (!cpuhp_tasks_frozen)\n\t\tcmci_clear();\n\n\tvendor_disable_error_reporting();\n}\n\nstatic void mce_reenable_cpu(void)\n{\n\tstruct mce_bank *mce_banks = this_cpu_ptr(mce_banks_array);\n\tint i;\n\n\tif (!mce_available(raw_cpu_ptr(&cpu_info)))\n\t\treturn;\n\n\tif (!cpuhp_tasks_frozen)\n\t\tcmci_reenable();\n\tfor (i = 0; i < this_cpu_read(mce_num_banks); i++) {\n\t\tstruct mce_bank *b = &mce_banks[i];\n\n\t\tif (b->init)\n\t\t\twrmsrl(mca_msr_reg(i, MCA_CTL), b->ctl);\n\t}\n}\n\nstatic int mce_cpu_dead(unsigned int cpu)\n{\n\tmce_intel_hcpu_update(cpu);\n\n\t \n\tif (!cpuhp_tasks_frozen)\n\t\tcmci_rediscover();\n\treturn 0;\n}\n\nstatic int mce_cpu_online(unsigned int cpu)\n{\n\tstruct timer_list *t = this_cpu_ptr(&mce_timer);\n\tint ret;\n\n\tmce_device_create(cpu);\n\n\tret = mce_threshold_create_device(cpu);\n\tif (ret) {\n\t\tmce_device_remove(cpu);\n\t\treturn ret;\n\t}\n\tmce_reenable_cpu();\n\tmce_start_timer(t);\n\treturn 0;\n}\n\nstatic int mce_cpu_pre_down(unsigned int cpu)\n{\n\tstruct timer_list *t = this_cpu_ptr(&mce_timer);\n\n\tmce_disable_cpu();\n\tdel_timer_sync(t);\n\tmce_threshold_remove_device(cpu);\n\tmce_device_remove(cpu);\n\treturn 0;\n}\n\nstatic __init void mce_init_banks(void)\n{\n\tint i;\n\n\tfor (i = 0; i < MAX_NR_BANKS; i++) {\n\t\tstruct mce_bank_dev *b = &mce_bank_devs[i];\n\t\tstruct device_attribute *a = &b->attr;\n\n\t\tb->bank = i;\n\n\t\tsysfs_attr_init(&a->attr);\n\t\ta->attr.name\t= b->attrname;\n\t\tsnprintf(b->attrname, ATTR_LEN, \"bank%d\", i);\n\n\t\ta->attr.mode\t= 0644;\n\t\ta->show\t\t= show_bank;\n\t\ta->store\t= set_bank;\n\t}\n}\n\n \nstatic __init int mcheck_init_device(void)\n{\n\tint err;\n\n\t \n\tMAYBE_BUILD_BUG_ON(__VIRTUAL_MASK_SHIFT >= 63);\n\n\tif (!mce_available(&boot_cpu_data)) {\n\t\terr = -EIO;\n\t\tgoto err_out;\n\t}\n\n\tif (!zalloc_cpumask_var(&mce_device_initialized, GFP_KERNEL)) {\n\t\terr = -ENOMEM;\n\t\tgoto err_out;\n\t}\n\n\tmce_init_banks();\n\n\terr = subsys_system_register(&mce_subsys, NULL);\n\tif (err)\n\t\tgoto err_out_mem;\n\n\terr = cpuhp_setup_state(CPUHP_X86_MCE_DEAD, \"x86/mce:dead\", NULL,\n\t\t\t\tmce_cpu_dead);\n\tif (err)\n\t\tgoto err_out_mem;\n\n\t \n\terr = cpuhp_setup_state(CPUHP_AP_ONLINE_DYN, \"x86/mce:online\",\n\t\t\t\tmce_cpu_online, mce_cpu_pre_down);\n\tif (err < 0)\n\t\tgoto err_out_online;\n\n\tregister_syscore_ops(&mce_syscore_ops);\n\n\treturn 0;\n\nerr_out_online:\n\tcpuhp_remove_state(CPUHP_X86_MCE_DEAD);\n\nerr_out_mem:\n\tfree_cpumask_var(mce_device_initialized);\n\nerr_out:\n\tpr_err(\"Unable to init MCE device (rc: %d)\\n\", err);\n\n\treturn err;\n}\ndevice_initcall_sync(mcheck_init_device);\n\n \nstatic int __init mcheck_disable(char *str)\n{\n\tmca_cfg.disabled = 1;\n\treturn 1;\n}\n__setup(\"nomce\", mcheck_disable);\n\n#ifdef CONFIG_DEBUG_FS\nstruct dentry *mce_get_debugfs_dir(void)\n{\n\tstatic struct dentry *dmce;\n\n\tif (!dmce)\n\t\tdmce = debugfs_create_dir(\"mce\", NULL);\n\n\treturn dmce;\n}\n\nstatic void mce_reset(void)\n{\n\tatomic_set(&mce_fake_panicked, 0);\n\tatomic_set(&mce_executing, 0);\n\tatomic_set(&mce_callin, 0);\n\tatomic_set(&global_nwo, 0);\n\tcpumask_setall(&mce_missing_cpus);\n}\n\nstatic int fake_panic_get(void *data, u64 *val)\n{\n\t*val = fake_panic;\n\treturn 0;\n}\n\nstatic int fake_panic_set(void *data, u64 val)\n{\n\tmce_reset();\n\tfake_panic = val;\n\treturn 0;\n}\n\nDEFINE_DEBUGFS_ATTRIBUTE(fake_panic_fops, fake_panic_get, fake_panic_set,\n\t\t\t \"%llu\\n\");\n\nstatic void __init mcheck_debugfs_init(void)\n{\n\tstruct dentry *dmce;\n\n\tdmce = mce_get_debugfs_dir();\n\tdebugfs_create_file_unsafe(\"fake_panic\", 0444, dmce, NULL,\n\t\t\t\t   &fake_panic_fops);\n}\n#else\nstatic void __init mcheck_debugfs_init(void) { }\n#endif\n\nstatic int __init mcheck_late_init(void)\n{\n\tif (mca_cfg.recovery)\n\t\tenable_copy_mc_fragile();\n\n\tmcheck_debugfs_init();\n\n\t \n\tmce_schedule_work();\n\n\treturn 0;\n}\nlate_initcall(mcheck_late_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}