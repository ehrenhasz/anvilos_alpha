{
  "module_name": "amd.c",
  "hash_id": "96765fed03bd8cb5165ec610c601fa66778f99bc4778a9ffdedee71dfe3c9064",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/cpu/amd.c",
  "human_readable_source": "\n#include <linux/export.h>\n#include <linux/bitops.h>\n#include <linux/elf.h>\n#include <linux/mm.h>\n\n#include <linux/io.h>\n#include <linux/sched.h>\n#include <linux/sched/clock.h>\n#include <linux/random.h>\n#include <linux/topology.h>\n#include <asm/processor.h>\n#include <asm/apic.h>\n#include <asm/cacheinfo.h>\n#include <asm/cpu.h>\n#include <asm/spec-ctrl.h>\n#include <asm/smp.h>\n#include <asm/numa.h>\n#include <asm/pci-direct.h>\n#include <asm/delay.h>\n#include <asm/debugreg.h>\n#include <asm/resctrl.h>\n\n#ifdef CONFIG_X86_64\n# include <asm/mmconfig.h>\n#endif\n\n#include \"cpu.h\"\n\n \nstatic u32 nodes_per_socket = 1;\n\n \n\n#define AMD_LEGACY_ERRATUM(...)\t\t{ -1, __VA_ARGS__, 0 }\n#define AMD_OSVW_ERRATUM(osvw_id, ...)\t{ osvw_id, __VA_ARGS__, 0 }\n#define AMD_MODEL_RANGE(f, m_start, s_start, m_end, s_end) \\\n\t((f << 24) | (m_start << 16) | (s_start << 12) | (m_end << 4) | (s_end))\n#define AMD_MODEL_RANGE_FAMILY(range)\t(((range) >> 24) & 0xff)\n#define AMD_MODEL_RANGE_START(range)\t(((range) >> 12) & 0xfff)\n#define AMD_MODEL_RANGE_END(range)\t((range) & 0xfff)\n\nstatic const int amd_erratum_400[] =\n\tAMD_OSVW_ERRATUM(1, AMD_MODEL_RANGE(0xf, 0x41, 0x2, 0xff, 0xf),\n\t\t\t    AMD_MODEL_RANGE(0x10, 0x2, 0x1, 0xff, 0xf));\n\nstatic const int amd_erratum_383[] =\n\tAMD_OSVW_ERRATUM(3, AMD_MODEL_RANGE(0x10, 0, 0, 0xff, 0xf));\n\n \nstatic const int amd_erratum_1054[] =\n\tAMD_LEGACY_ERRATUM(AMD_MODEL_RANGE(0x17, 0, 0, 0x2f, 0xf));\n\nstatic const int amd_zenbleed[] =\n\tAMD_LEGACY_ERRATUM(AMD_MODEL_RANGE(0x17, 0x30, 0x0, 0x4f, 0xf),\n\t\t\t   AMD_MODEL_RANGE(0x17, 0x60, 0x0, 0x7f, 0xf),\n\t\t\t   AMD_MODEL_RANGE(0x17, 0x90, 0x0, 0x91, 0xf),\n\t\t\t   AMD_MODEL_RANGE(0x17, 0xa0, 0x0, 0xaf, 0xf));\n\nstatic const int amd_div0[] =\n\tAMD_LEGACY_ERRATUM(AMD_MODEL_RANGE(0x17, 0x00, 0x0, 0x2f, 0xf),\n\t\t\t   AMD_MODEL_RANGE(0x17, 0x50, 0x0, 0x5f, 0xf));\n\nstatic const int amd_erratum_1485[] =\n\tAMD_LEGACY_ERRATUM(AMD_MODEL_RANGE(0x19, 0x10, 0x0, 0x1f, 0xf),\n\t\t\t   AMD_MODEL_RANGE(0x19, 0x60, 0x0, 0xaf, 0xf));\n\nstatic bool cpu_has_amd_erratum(struct cpuinfo_x86 *cpu, const int *erratum)\n{\n\tint osvw_id = *erratum++;\n\tu32 range;\n\tu32 ms;\n\n\tif (osvw_id >= 0 && osvw_id < 65536 &&\n\t    cpu_has(cpu, X86_FEATURE_OSVW)) {\n\t\tu64 osvw_len;\n\n\t\trdmsrl(MSR_AMD64_OSVW_ID_LENGTH, osvw_len);\n\t\tif (osvw_id < osvw_len) {\n\t\t\tu64 osvw_bits;\n\n\t\t\trdmsrl(MSR_AMD64_OSVW_STATUS + (osvw_id >> 6),\n\t\t\t    osvw_bits);\n\t\t\treturn osvw_bits & (1ULL << (osvw_id & 0x3f));\n\t\t}\n\t}\n\n\t \n\tms = (cpu->x86_model << 4) | cpu->x86_stepping;\n\twhile ((range = *erratum++))\n\t\tif ((cpu->x86 == AMD_MODEL_RANGE_FAMILY(range)) &&\n\t\t    (ms >= AMD_MODEL_RANGE_START(range)) &&\n\t\t    (ms <= AMD_MODEL_RANGE_END(range)))\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline int rdmsrl_amd_safe(unsigned msr, unsigned long long *p)\n{\n\tu32 gprs[8] = { 0 };\n\tint err;\n\n\tWARN_ONCE((boot_cpu_data.x86 != 0xf),\n\t\t  \"%s should only be used on K8!\\n\", __func__);\n\n\tgprs[1] = msr;\n\tgprs[7] = 0x9c5a203a;\n\n\terr = rdmsr_safe_regs(gprs);\n\n\t*p = gprs[0] | ((u64)gprs[2] << 32);\n\n\treturn err;\n}\n\nstatic inline int wrmsrl_amd_safe(unsigned msr, unsigned long long val)\n{\n\tu32 gprs[8] = { 0 };\n\n\tWARN_ONCE((boot_cpu_data.x86 != 0xf),\n\t\t  \"%s should only be used on K8!\\n\", __func__);\n\n\tgprs[0] = (u32)val;\n\tgprs[1] = msr;\n\tgprs[2] = val >> 32;\n\tgprs[7] = 0x9c5a203a;\n\n\treturn wrmsr_safe_regs(gprs);\n}\n\n \n\n#ifdef CONFIG_X86_32\nextern __visible void vide(void);\n__asm__(\".text\\n\"\n\t\".globl vide\\n\"\n\t\".type vide, @function\\n\"\n\t\".align 4\\n\"\n\t\"vide: ret\\n\");\n#endif\n\nstatic void init_amd_k5(struct cpuinfo_x86 *c)\n{\n#ifdef CONFIG_X86_32\n \n#define CBAR\t\t(0xfffc)  \n#define CBAR_ENB\t(0x80000000)\n#define CBAR_KEY\t(0X000000CB)\n\tif (c->x86_model == 9 || c->x86_model == 10) {\n\t\tif (inl(CBAR) & CBAR_ENB)\n\t\t\toutl(0 | CBAR_KEY, CBAR);\n\t}\n#endif\n}\n\nstatic void init_amd_k6(struct cpuinfo_x86 *c)\n{\n#ifdef CONFIG_X86_32\n\tu32 l, h;\n\tint mbytes = get_num_physpages() >> (20-PAGE_SHIFT);\n\n\tif (c->x86_model < 6) {\n\t\t \n\t\tif (c->x86_model == 0) {\n\t\t\tclear_cpu_cap(c, X86_FEATURE_APIC);\n\t\t\tset_cpu_cap(c, X86_FEATURE_PGE);\n\t\t}\n\t\treturn;\n\t}\n\n\tif (c->x86_model == 6 && c->x86_stepping == 1) {\n\t\tconst int K6_BUG_LOOP = 1000000;\n\t\tint n;\n\t\tvoid (*f_vide)(void);\n\t\tu64 d, d2;\n\n\t\tpr_info(\"AMD K6 stepping B detected - \");\n\n\t\t \n\n\t\tn = K6_BUG_LOOP;\n\t\tf_vide = vide;\n\t\tOPTIMIZER_HIDE_VAR(f_vide);\n\t\td = rdtsc();\n\t\twhile (n--)\n\t\t\tf_vide();\n\t\td2 = rdtsc();\n\t\td = d2-d;\n\n\t\tif (d > 20*K6_BUG_LOOP)\n\t\t\tpr_cont(\"system stability may be impaired when more than 32 MB are used.\\n\");\n\t\telse\n\t\t\tpr_cont(\"probably OK (after B9730xxxx).\\n\");\n\t}\n\n\t \n\tif (c->x86_model < 8 ||\n\t   (c->x86_model == 8 && c->x86_stepping < 8)) {\n\t\t \n\t\tif (mbytes > 508)\n\t\t\tmbytes = 508;\n\n\t\trdmsr(MSR_K6_WHCR, l, h);\n\t\tif ((l&0x0000FFFF) == 0) {\n\t\t\tunsigned long flags;\n\t\t\tl = (1<<0)|((mbytes/4)<<1);\n\t\t\tlocal_irq_save(flags);\n\t\t\twbinvd();\n\t\t\twrmsr(MSR_K6_WHCR, l, h);\n\t\t\tlocal_irq_restore(flags);\n\t\t\tpr_info(\"Enabling old style K6 write allocation for %d Mb\\n\",\n\t\t\t\tmbytes);\n\t\t}\n\t\treturn;\n\t}\n\n\tif ((c->x86_model == 8 && c->x86_stepping > 7) ||\n\t     c->x86_model == 9 || c->x86_model == 13) {\n\t\t \n\n\t\tif (mbytes > 4092)\n\t\t\tmbytes = 4092;\n\n\t\trdmsr(MSR_K6_WHCR, l, h);\n\t\tif ((l&0xFFFF0000) == 0) {\n\t\t\tunsigned long flags;\n\t\t\tl = ((mbytes>>2)<<22)|(1<<16);\n\t\t\tlocal_irq_save(flags);\n\t\t\twbinvd();\n\t\t\twrmsr(MSR_K6_WHCR, l, h);\n\t\t\tlocal_irq_restore(flags);\n\t\t\tpr_info(\"Enabling new style K6 write allocation for %d Mb\\n\",\n\t\t\t\tmbytes);\n\t\t}\n\n\t\treturn;\n\t}\n\n\tif (c->x86_model == 10) {\n\t\t \n\t\t \n\t\treturn;\n\t}\n#endif\n}\n\nstatic void init_amd_k7(struct cpuinfo_x86 *c)\n{\n#ifdef CONFIG_X86_32\n\tu32 l, h;\n\n\t \n\tif (c->x86_model >= 6 && c->x86_model <= 10) {\n\t\tif (!cpu_has(c, X86_FEATURE_XMM)) {\n\t\t\tpr_info(\"Enabling disabled K7/SSE Support.\\n\");\n\t\t\tmsr_clear_bit(MSR_K7_HWCR, 15);\n\t\t\tset_cpu_cap(c, X86_FEATURE_XMM);\n\t\t}\n\t}\n\n\t \n\tif ((c->x86_model == 8 && c->x86_stepping >= 1) || (c->x86_model > 8)) {\n\t\trdmsr(MSR_K7_CLK_CTL, l, h);\n\t\tif ((l & 0xfff00000) != 0x20000000) {\n\t\t\tpr_info(\"CPU: CLK_CTL MSR was %x. Reprogramming to %x\\n\",\n\t\t\t\tl, ((l & 0x000fffff)|0x20000000));\n\t\t\twrmsr(MSR_K7_CLK_CTL, (l & 0x000fffff)|0x20000000, h);\n\t\t}\n\t}\n\n\t \n\tif (!c->cpu_index)\n\t\treturn;\n\n\t \n\t \n\tif ((c->x86_model == 6) && ((c->x86_stepping == 0) ||\n\t    (c->x86_stepping == 1)))\n\t\treturn;\n\n\t \n\tif ((c->x86_model == 7) && (c->x86_stepping == 0))\n\t\treturn;\n\n\t \n\tif (((c->x86_model == 6) && (c->x86_stepping >= 2)) ||\n\t    ((c->x86_model == 7) && (c->x86_stepping >= 1)) ||\n\t     (c->x86_model > 7))\n\t\tif (cpu_has(c, X86_FEATURE_MP))\n\t\t\treturn;\n\n\t \n\n\t \n\tWARN_ONCE(1, \"WARNING: This combination of AMD\"\n\t\t\" processors is not suitable for SMP.\\n\");\n\tadd_taint(TAINT_CPU_OUT_OF_SPEC, LOCKDEP_NOW_UNRELIABLE);\n#endif\n}\n\n#ifdef CONFIG_NUMA\n \nstatic int nearby_node(int apicid)\n{\n\tint i, node;\n\n\tfor (i = apicid - 1; i >= 0; i--) {\n\t\tnode = __apicid_to_node[i];\n\t\tif (node != NUMA_NO_NODE && node_online(node))\n\t\t\treturn node;\n\t}\n\tfor (i = apicid + 1; i < MAX_LOCAL_APIC; i++) {\n\t\tnode = __apicid_to_node[i];\n\t\tif (node != NUMA_NO_NODE && node_online(node))\n\t\t\treturn node;\n\t}\n\treturn first_node(node_online_map);  \n}\n#endif\n\n \nstatic void legacy_fixup_core_id(struct cpuinfo_x86 *c)\n{\n\tu32 cus_per_node;\n\n\tif (c->x86 >= 0x17)\n\t\treturn;\n\n\tcus_per_node = c->x86_max_cores / nodes_per_socket;\n\tc->cpu_core_id %= cus_per_node;\n}\n\n \nstatic void amd_get_topology(struct cpuinfo_x86 *c)\n{\n\tint cpu = smp_processor_id();\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_TOPOEXT)) {\n\t\tint err;\n\t\tu32 eax, ebx, ecx, edx;\n\n\t\tcpuid(0x8000001e, &eax, &ebx, &ecx, &edx);\n\n\t\tc->cpu_die_id  = ecx & 0xff;\n\n\t\tif (c->x86 == 0x15)\n\t\t\tc->cu_id = ebx & 0xff;\n\n\t\tif (c->x86 >= 0x17) {\n\t\t\tc->cpu_core_id = ebx & 0xff;\n\n\t\t\tif (smp_num_siblings > 1)\n\t\t\t\tc->x86_max_cores /= smp_num_siblings;\n\t\t}\n\n\t\t \n\t\terr = detect_extended_topology(c);\n\t\tif (!err)\n\t\t\tc->x86_coreid_bits = get_count_order(c->x86_max_cores);\n\n\t\tcacheinfo_amd_init_llc_id(c, cpu);\n\n\t} else if (cpu_has(c, X86_FEATURE_NODEID_MSR)) {\n\t\tu64 value;\n\n\t\trdmsrl(MSR_FAM10H_NODE_ID, value);\n\t\tc->cpu_die_id = value & 7;\n\n\t\tper_cpu(cpu_llc_id, cpu) = c->cpu_die_id;\n\t} else\n\t\treturn;\n\n\tif (nodes_per_socket > 1) {\n\t\tset_cpu_cap(c, X86_FEATURE_AMD_DCM);\n\t\tlegacy_fixup_core_id(c);\n\t}\n}\n\n \nstatic void amd_detect_cmp(struct cpuinfo_x86 *c)\n{\n\tunsigned bits;\n\tint cpu = smp_processor_id();\n\n\tbits = c->x86_coreid_bits;\n\t \n\tc->cpu_core_id = c->initial_apicid & ((1 << bits)-1);\n\t \n\tc->phys_proc_id = c->initial_apicid >> bits;\n\t \n\tper_cpu(cpu_llc_id, cpu) = c->cpu_die_id = c->phys_proc_id;\n}\n\nu32 amd_get_nodes_per_socket(void)\n{\n\treturn nodes_per_socket;\n}\nEXPORT_SYMBOL_GPL(amd_get_nodes_per_socket);\n\nstatic void srat_detect_node(struct cpuinfo_x86 *c)\n{\n#ifdef CONFIG_NUMA\n\tint cpu = smp_processor_id();\n\tint node;\n\tunsigned apicid = c->apicid;\n\n\tnode = numa_cpu_node(cpu);\n\tif (node == NUMA_NO_NODE)\n\t\tnode = get_llc_id(cpu);\n\n\t \n\tif (x86_cpuinit.fixup_cpu_id)\n\t\tx86_cpuinit.fixup_cpu_id(c, node);\n\n\tif (!node_online(node)) {\n\t\t \n\t\tint ht_nodeid = c->initial_apicid;\n\n\t\tif (__apicid_to_node[ht_nodeid] != NUMA_NO_NODE)\n\t\t\tnode = __apicid_to_node[ht_nodeid];\n\t\t \n\t\tif (!node_online(node))\n\t\t\tnode = nearby_node(apicid);\n\t}\n\tnuma_set_node(cpu, node);\n#endif\n}\n\nstatic void early_init_amd_mc(struct cpuinfo_x86 *c)\n{\n#ifdef CONFIG_SMP\n\tunsigned bits, ecx;\n\n\t \n\tif (c->extended_cpuid_level < 0x80000008)\n\t\treturn;\n\n\tecx = cpuid_ecx(0x80000008);\n\n\tc->x86_max_cores = (ecx & 0xff) + 1;\n\n\t \n\tbits = (ecx >> 12) & 0xF;\n\n\t \n\tif (bits == 0) {\n\t\twhile ((1 << bits) < c->x86_max_cores)\n\t\t\tbits++;\n\t}\n\n\tc->x86_coreid_bits = bits;\n#endif\n}\n\nstatic void bsp_init_amd(struct cpuinfo_x86 *c)\n{\n\tif (cpu_has(c, X86_FEATURE_CONSTANT_TSC)) {\n\n\t\tif (c->x86 > 0x10 ||\n\t\t    (c->x86 == 0x10 && c->x86_model >= 0x2)) {\n\t\t\tu64 val;\n\n\t\t\trdmsrl(MSR_K7_HWCR, val);\n\t\t\tif (!(val & BIT(24)))\n\t\t\t\tpr_warn(FW_BUG \"TSC doesn't count with P0 frequency!\\n\");\n\t\t}\n\t}\n\n\tif (c->x86 == 0x15) {\n\t\tunsigned long upperbit;\n\t\tu32 cpuid, assoc;\n\n\t\tcpuid\t = cpuid_edx(0x80000005);\n\t\tassoc\t = cpuid >> 16 & 0xff;\n\t\tupperbit = ((cpuid >> 24) << 10) / assoc;\n\n\t\tva_align.mask\t  = (upperbit - 1) & PAGE_MASK;\n\t\tva_align.flags    = ALIGN_VA_32 | ALIGN_VA_64;\n\n\t\t \n\t\tva_align.bits = get_random_u32() & va_align.mask;\n\t}\n\n\tif (cpu_has(c, X86_FEATURE_MWAITX))\n\t\tuse_mwaitx_delay();\n\n\tif (boot_cpu_has(X86_FEATURE_TOPOEXT)) {\n\t\tu32 ecx;\n\n\t\tecx = cpuid_ecx(0x8000001e);\n\t\t__max_die_per_package = nodes_per_socket = ((ecx >> 8) & 7) + 1;\n\t} else if (boot_cpu_has(X86_FEATURE_NODEID_MSR)) {\n\t\tu64 value;\n\n\t\trdmsrl(MSR_FAM10H_NODE_ID, value);\n\t\t__max_die_per_package = nodes_per_socket = ((value >> 3) & 7) + 1;\n\t}\n\n\tif (!boot_cpu_has(X86_FEATURE_AMD_SSBD) &&\n\t    !boot_cpu_has(X86_FEATURE_VIRT_SSBD) &&\n\t    c->x86 >= 0x15 && c->x86 <= 0x17) {\n\t\tunsigned int bit;\n\n\t\tswitch (c->x86) {\n\t\tcase 0x15: bit = 54; break;\n\t\tcase 0x16: bit = 33; break;\n\t\tcase 0x17: bit = 10; break;\n\t\tdefault: return;\n\t\t}\n\t\t \n\t\tif (!rdmsrl_safe(MSR_AMD64_LS_CFG, &x86_amd_ls_cfg_base)) {\n\t\t\tsetup_force_cpu_cap(X86_FEATURE_LS_CFG_SSBD);\n\t\t\tsetup_force_cpu_cap(X86_FEATURE_SSBD);\n\t\t\tx86_amd_ls_cfg_ssbd_mask = 1ULL << bit;\n\t\t}\n\t}\n\n\tresctrl_cpu_detect(c);\n}\n\nstatic void early_detect_mem_encrypt(struct cpuinfo_x86 *c)\n{\n\tu64 msr;\n\n\t \n\tif (cpu_has(c, X86_FEATURE_SME) || cpu_has(c, X86_FEATURE_SEV)) {\n\t\t \n\t\trdmsrl(MSR_AMD64_SYSCFG, msr);\n\t\tif (!(msr & MSR_AMD64_SYSCFG_MEM_ENCRYPT))\n\t\t\tgoto clear_all;\n\n\t\t \n\t\tc->x86_phys_bits -= (cpuid_ebx(0x8000001f) >> 6) & 0x3f;\n\n\t\tif (IS_ENABLED(CONFIG_X86_32))\n\t\t\tgoto clear_all;\n\n\t\tif (!sme_me_mask)\n\t\t\tsetup_clear_cpu_cap(X86_FEATURE_SME);\n\n\t\trdmsrl(MSR_K7_HWCR, msr);\n\t\tif (!(msr & MSR_K7_HWCR_SMMLOCK))\n\t\t\tgoto clear_sev;\n\n\t\treturn;\n\nclear_all:\n\t\tsetup_clear_cpu_cap(X86_FEATURE_SME);\nclear_sev:\n\t\tsetup_clear_cpu_cap(X86_FEATURE_SEV);\n\t\tsetup_clear_cpu_cap(X86_FEATURE_SEV_ES);\n\t}\n}\n\nstatic void early_init_amd(struct cpuinfo_x86 *c)\n{\n\tu64 value;\n\tu32 dummy;\n\n\tearly_init_amd_mc(c);\n\n\tif (c->x86 >= 0xf)\n\t\tset_cpu_cap(c, X86_FEATURE_K8);\n\n\trdmsr_safe(MSR_AMD64_PATCH_LEVEL, &c->microcode, &dummy);\n\n\t \n\tif (c->x86_power & (1 << 8)) {\n\t\tset_cpu_cap(c, X86_FEATURE_CONSTANT_TSC);\n\t\tset_cpu_cap(c, X86_FEATURE_NONSTOP_TSC);\n\t}\n\n\t \n\tif (c->x86_power & BIT(12))\n\t\tset_cpu_cap(c, X86_FEATURE_ACC_POWER);\n\n\t \n\tif (c->x86_power & BIT(14))\n\t\tset_cpu_cap(c, X86_FEATURE_RAPL);\n\n#ifdef CONFIG_X86_64\n\tset_cpu_cap(c, X86_FEATURE_SYSCALL32);\n#else\n\t \n\tif (c->x86 == 5)\n\t\tif (c->x86_model == 13 || c->x86_model == 9 ||\n\t\t    (c->x86_model == 8 && c->x86_stepping >= 8))\n\t\t\tset_cpu_cap(c, X86_FEATURE_K6_MTRR);\n#endif\n#if defined(CONFIG_X86_LOCAL_APIC) && defined(CONFIG_PCI)\n\t \n\tif (boot_cpu_has(X86_FEATURE_APIC)) {\n\t\tif (c->x86 > 0x16)\n\t\t\tset_cpu_cap(c, X86_FEATURE_EXTD_APICID);\n\t\telse if (c->x86 >= 0xf) {\n\t\t\t \n\t\t\tunsigned int val;\n\n\t\t\tval = read_pci_config(0, 24, 0, 0x68);\n\t\t\tif ((val >> 17 & 0x3) == 0x3)\n\t\t\t\tset_cpu_cap(c, X86_FEATURE_EXTD_APICID);\n\t\t}\n\t}\n#endif\n\n\t \n\tset_cpu_cap(c, X86_FEATURE_VMMCALL);\n\n\t \n\tif (c->x86 == 0x16 && c->x86_model <= 0xf)\n\t\tmsr_set_bit(MSR_AMD64_LS_CFG, 15);\n\n\t \n\tif (cpu_has_amd_erratum(c, amd_erratum_400))\n\t\tset_cpu_bug(c, X86_BUG_AMD_E400);\n\n\tearly_detect_mem_encrypt(c);\n\n\t \n\tif (c->x86 == 0x15 &&\n\t    (c->x86_model >= 0x10 && c->x86_model <= 0x6f) &&\n\t    !cpu_has(c, X86_FEATURE_TOPOEXT)) {\n\n\t\tif (msr_set_bit(0xc0011005, 54) > 0) {\n\t\t\trdmsrl(0xc0011005, value);\n\t\t\tif (value & BIT_64(54)) {\n\t\t\t\tset_cpu_cap(c, X86_FEATURE_TOPOEXT);\n\t\t\t\tpr_info_once(FW_INFO \"CPU: Re-enabling disabled Topology Extensions Support.\\n\");\n\t\t\t}\n\t\t}\n\t}\n\n\tif (cpu_has(c, X86_FEATURE_TOPOEXT))\n\t\tsmp_num_siblings = ((cpuid_ebx(0x8000001e) >> 8) & 0xff) + 1;\n\n\tif (!cpu_has(c, X86_FEATURE_HYPERVISOR) && !cpu_has(c, X86_FEATURE_IBPB_BRTYPE)) {\n\t\tif (c->x86 == 0x17 && boot_cpu_has(X86_FEATURE_AMD_IBPB))\n\t\t\tsetup_force_cpu_cap(X86_FEATURE_IBPB_BRTYPE);\n\t\telse if (c->x86 >= 0x19 && !wrmsrl_safe(MSR_IA32_PRED_CMD, PRED_CMD_SBPB)) {\n\t\t\tsetup_force_cpu_cap(X86_FEATURE_IBPB_BRTYPE);\n\t\t\tsetup_force_cpu_cap(X86_FEATURE_SBPB);\n\t\t}\n\t}\n}\n\nstatic void init_amd_k8(struct cpuinfo_x86 *c)\n{\n\tu32 level;\n\tu64 value;\n\n\t \n\tlevel = cpuid_eax(1);\n\tif ((level >= 0x0f48 && level < 0x0f50) || level >= 0x0f58)\n\t\tset_cpu_cap(c, X86_FEATURE_REP_GOOD);\n\n\t \n\tif (c->x86_model < 0x14 && cpu_has(c, X86_FEATURE_LAHF_LM)) {\n\t\tclear_cpu_cap(c, X86_FEATURE_LAHF_LM);\n\t\tif (!rdmsrl_amd_safe(0xc001100d, &value)) {\n\t\t\tvalue &= ~BIT_64(32);\n\t\t\twrmsrl_amd_safe(0xc001100d, value);\n\t\t}\n\t}\n\n\tif (!c->x86_model_id[0])\n\t\tstrcpy(c->x86_model_id, \"Hammer\");\n\n#ifdef CONFIG_SMP\n\t \n\tmsr_set_bit(MSR_K7_HWCR, 6);\n#endif\n\tset_cpu_bug(c, X86_BUG_SWAPGS_FENCE);\n}\n\nstatic void init_amd_gh(struct cpuinfo_x86 *c)\n{\n#ifdef CONFIG_MMCONF_FAM10H\n\t \n\tif (c == &boot_cpu_data)\n\t\tcheck_enable_amd_mmconf_dmi();\n\n\tfam10h_check_enable_mmcfg();\n#endif\n\n\t \n\tmsr_set_bit(MSR_AMD64_MCx_MASK(4), 10);\n\n\t \n\tmsr_clear_bit(MSR_AMD64_BU_CFG2, 24);\n\n\tif (cpu_has_amd_erratum(c, amd_erratum_383))\n\t\tset_cpu_bug(c, X86_BUG_AMD_TLB_MMATCH);\n}\n\nstatic void init_amd_ln(struct cpuinfo_x86 *c)\n{\n\t \n\tmsr_set_bit(MSR_AMD64_DE_CFG, 31);\n}\n\nstatic bool rdrand_force;\n\nstatic int __init rdrand_cmdline(char *str)\n{\n\tif (!str)\n\t\treturn -EINVAL;\n\n\tif (!strcmp(str, \"force\"))\n\t\trdrand_force = true;\n\telse\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\nearly_param(\"rdrand\", rdrand_cmdline);\n\nstatic void clear_rdrand_cpuid_bit(struct cpuinfo_x86 *c)\n{\n\t \n\tif (!IS_ENABLED(CONFIG_PM_SLEEP))\n\t\treturn;\n\n\t \n\tif (!(cpuid_ecx(1) & BIT(30)) || rdrand_force)\n\t\treturn;\n\n\tmsr_clear_bit(MSR_AMD64_CPUID_FN_1, 62);\n\n\t \n\tif (cpuid_ecx(1) & BIT(30)) {\n\t\tpr_info_once(\"BIOS may not properly restore RDRAND after suspend, but hypervisor does not support hiding RDRAND via CPUID.\\n\");\n\t\treturn;\n\t}\n\n\tclear_cpu_cap(c, X86_FEATURE_RDRAND);\n\tpr_info_once(\"BIOS may not properly restore RDRAND after suspend, hiding RDRAND via CPUID. Use rdrand=force to reenable.\\n\");\n}\n\nstatic void init_amd_jg(struct cpuinfo_x86 *c)\n{\n\t \n\tclear_rdrand_cpuid_bit(c);\n}\n\nstatic void init_amd_bd(struct cpuinfo_x86 *c)\n{\n\tu64 value;\n\n\t \n\tif ((c->x86_model >= 0x02) && (c->x86_model < 0x20)) {\n\t\tif (!rdmsrl_safe(MSR_F15H_IC_CFG, &value) && !(value & 0x1E)) {\n\t\t\tvalue |= 0x1E;\n\t\t\twrmsrl_safe(MSR_F15H_IC_CFG, value);\n\t\t}\n\t}\n\n\t \n\tclear_rdrand_cpuid_bit(c);\n}\n\nvoid init_spectral_chicken(struct cpuinfo_x86 *c)\n{\n#ifdef CONFIG_CPU_UNRET_ENTRY\n\tu64 value;\n\n\t \n\tif (!cpu_has(c, X86_FEATURE_HYPERVISOR) && cpu_has(c, X86_FEATURE_AMD_STIBP)) {\n\t\tif (!rdmsrl_safe(MSR_ZEN2_SPECTRAL_CHICKEN, &value)) {\n\t\t\tvalue |= MSR_ZEN2_SPECTRAL_CHICKEN_BIT;\n\t\t\twrmsrl_safe(MSR_ZEN2_SPECTRAL_CHICKEN, value);\n\t\t}\n\t}\n#endif\n\t \n\tclear_cpu_cap(c, X86_FEATURE_XSAVES);\n}\n\nstatic void init_amd_zn(struct cpuinfo_x86 *c)\n{\n\tset_cpu_cap(c, X86_FEATURE_ZEN);\n\n#ifdef CONFIG_NUMA\n\tnode_reclaim_distance = 32;\n#endif\n\n\t \n\tif (!cpu_has(c, X86_FEATURE_HYPERVISOR)) {\n\n\t\t \n\t\tif (!cpu_has(c, X86_FEATURE_CPB))\n\t\t\tset_cpu_cap(c, X86_FEATURE_CPB);\n\n\t\t \n\t\tif (c->x86 == 0x19 && !cpu_has(c, X86_FEATURE_BTC_NO))\n\t\t\tset_cpu_cap(c, X86_FEATURE_BTC_NO);\n\t}\n}\n\nstatic bool cpu_has_zenbleed_microcode(void)\n{\n\tu32 good_rev = 0;\n\n\tswitch (boot_cpu_data.x86_model) {\n\tcase 0x30 ... 0x3f: good_rev = 0x0830107a; break;\n\tcase 0x60 ... 0x67: good_rev = 0x0860010b; break;\n\tcase 0x68 ... 0x6f: good_rev = 0x08608105; break;\n\tcase 0x70 ... 0x7f: good_rev = 0x08701032; break;\n\tcase 0xa0 ... 0xaf: good_rev = 0x08a00008; break;\n\n\tdefault:\n\t\treturn false;\n\t\tbreak;\n\t}\n\n\tif (boot_cpu_data.microcode < good_rev)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic void zenbleed_check(struct cpuinfo_x86 *c)\n{\n\tif (!cpu_has_amd_erratum(c, amd_zenbleed))\n\t\treturn;\n\n\tif (cpu_has(c, X86_FEATURE_HYPERVISOR))\n\t\treturn;\n\n\tif (!cpu_has(c, X86_FEATURE_AVX))\n\t\treturn;\n\n\tif (!cpu_has_zenbleed_microcode()) {\n\t\tpr_notice_once(\"Zenbleed: please update your microcode for the most optimal fix\\n\");\n\t\tmsr_set_bit(MSR_AMD64_DE_CFG, MSR_AMD64_DE_CFG_ZEN2_FP_BACKUP_FIX_BIT);\n\t} else {\n\t\tmsr_clear_bit(MSR_AMD64_DE_CFG, MSR_AMD64_DE_CFG_ZEN2_FP_BACKUP_FIX_BIT);\n\t}\n}\n\nstatic void init_amd(struct cpuinfo_x86 *c)\n{\n\tearly_init_amd(c);\n\n\t \n\tclear_cpu_cap(c, 0*32+31);\n\n\tif (c->x86 >= 0x10)\n\t\tset_cpu_cap(c, X86_FEATURE_REP_GOOD);\n\n\t \n\tif (cpu_has(c, X86_FEATURE_FSRM))\n\t\tset_cpu_cap(c, X86_FEATURE_FSRS);\n\n\t \n\tc->apicid = read_apic_id();\n\n\t \n\tif (c->x86 < 6)\n\t\tclear_cpu_cap(c, X86_FEATURE_MCE);\n\n\tswitch (c->x86) {\n\tcase 4:    init_amd_k5(c); break;\n\tcase 5:    init_amd_k6(c); break;\n\tcase 6:\t   init_amd_k7(c); break;\n\tcase 0xf:  init_amd_k8(c); break;\n\tcase 0x10: init_amd_gh(c); break;\n\tcase 0x12: init_amd_ln(c); break;\n\tcase 0x15: init_amd_bd(c); break;\n\tcase 0x16: init_amd_jg(c); break;\n\tcase 0x17: init_spectral_chicken(c);\n\t\t   fallthrough;\n\tcase 0x19: init_amd_zn(c); break;\n\t}\n\n\t \n\tif ((c->x86 >= 6) && (!cpu_has(c, X86_FEATURE_XSAVEERPTR)))\n\t\tset_cpu_bug(c, X86_BUG_FXSAVE_LEAK);\n\n\tcpu_detect_cache_sizes(c);\n\n\tamd_detect_cmp(c);\n\tamd_get_topology(c);\n\tsrat_detect_node(c);\n\n\tinit_amd_cacheinfo(c);\n\n\tif (!cpu_has(c, X86_FEATURE_LFENCE_RDTSC) && cpu_has(c, X86_FEATURE_XMM2)) {\n\t\t \n\t\tmsr_set_bit(MSR_AMD64_DE_CFG,\n\t\t\t    MSR_AMD64_DE_CFG_LFENCE_SERIALIZE_BIT);\n\n\t\t \n\t\tset_cpu_cap(c, X86_FEATURE_LFENCE_RDTSC);\n\t}\n\n\t \n\tif (c->x86 > 0x11)\n\t\tset_cpu_cap(c, X86_FEATURE_ARAT);\n\n\t \n\tif (!cpu_has(c, X86_FEATURE_3DNOWPREFETCH))\n\t\tif (cpu_has(c, X86_FEATURE_3DNOW) || cpu_has(c, X86_FEATURE_LM))\n\t\t\tset_cpu_cap(c, X86_FEATURE_3DNOWPREFETCH);\n\n\t \n\tif (!cpu_feature_enabled(X86_FEATURE_XENPV))\n\t\tset_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);\n\n\t \n\tif (cpu_has(c, X86_FEATURE_IRPERF) &&\n\t    !cpu_has_amd_erratum(c, amd_erratum_1054))\n\t\tmsr_set_bit(MSR_K7_HWCR, MSR_K7_HWCR_IRPERF_EN_BIT);\n\n\tcheck_null_seg_clears_base(c);\n\n\t \n\tif (spectre_v2_in_eibrs_mode(spectre_v2_enabled) &&\n\t    cpu_has(c, X86_FEATURE_AUTOIBRS))\n\t\tWARN_ON_ONCE(msr_set_bit(MSR_EFER, _EFER_AUTOIBRS));\n\n\tzenbleed_check(c);\n\n\tif (cpu_has_amd_erratum(c, amd_div0)) {\n\t\tpr_notice_once(\"AMD Zen1 DIV0 bug detected. Disable SMT for full protection.\\n\");\n\t\tsetup_force_cpu_bug(X86_BUG_DIV0);\n\t}\n\n\tif (!cpu_has(c, X86_FEATURE_HYPERVISOR) &&\n\t     cpu_has_amd_erratum(c, amd_erratum_1485))\n\t\tmsr_set_bit(MSR_ZEN4_BP_CFG, MSR_ZEN4_BP_CFG_SHARED_BTB_FIX_BIT);\n}\n\n#ifdef CONFIG_X86_32\nstatic unsigned int amd_size_cache(struct cpuinfo_x86 *c, unsigned int size)\n{\n\t \n\tif (c->x86 == 6) {\n\t\t \n\t\tif (c->x86_model == 3 && c->x86_stepping == 0)\n\t\t\tsize = 64;\n\t\t \n\t\tif (c->x86_model == 4 &&\n\t\t\t(c->x86_stepping == 0 || c->x86_stepping == 1))\n\t\t\tsize = 256;\n\t}\n\treturn size;\n}\n#endif\n\nstatic void cpu_detect_tlb_amd(struct cpuinfo_x86 *c)\n{\n\tu32 ebx, eax, ecx, edx;\n\tu16 mask = 0xfff;\n\n\tif (c->x86 < 0xf)\n\t\treturn;\n\n\tif (c->extended_cpuid_level < 0x80000006)\n\t\treturn;\n\n\tcpuid(0x80000006, &eax, &ebx, &ecx, &edx);\n\n\ttlb_lld_4k[ENTRIES] = (ebx >> 16) & mask;\n\ttlb_lli_4k[ENTRIES] = ebx & mask;\n\n\t \n\tif (c->x86 == 0xf) {\n\t\tcpuid(0x80000005, &eax, &ebx, &ecx, &edx);\n\t\tmask = 0xff;\n\t}\n\n\t \n\tif (!((eax >> 16) & mask))\n\t\ttlb_lld_2m[ENTRIES] = (cpuid_eax(0x80000005) >> 16) & 0xff;\n\telse\n\t\ttlb_lld_2m[ENTRIES] = (eax >> 16) & mask;\n\n\t \n\ttlb_lld_4m[ENTRIES] = tlb_lld_2m[ENTRIES] >> 1;\n\n\t \n\tif (!(eax & mask)) {\n\t\t \n\t\tif (c->x86 == 0x15 && c->x86_model <= 0x1f) {\n\t\t\ttlb_lli_2m[ENTRIES] = 1024;\n\t\t} else {\n\t\t\tcpuid(0x80000005, &eax, &ebx, &ecx, &edx);\n\t\t\ttlb_lli_2m[ENTRIES] = eax & 0xff;\n\t\t}\n\t} else\n\t\ttlb_lli_2m[ENTRIES] = eax & mask;\n\n\ttlb_lli_4m[ENTRIES] = tlb_lli_2m[ENTRIES] >> 1;\n}\n\nstatic const struct cpu_dev amd_cpu_dev = {\n\t.c_vendor\t= \"AMD\",\n\t.c_ident\t= { \"AuthenticAMD\" },\n#ifdef CONFIG_X86_32\n\t.legacy_models = {\n\t\t{ .family = 4, .model_names =\n\t\t  {\n\t\t\t  [3] = \"486 DX/2\",\n\t\t\t  [7] = \"486 DX/2-WB\",\n\t\t\t  [8] = \"486 DX/4\",\n\t\t\t  [9] = \"486 DX/4-WB\",\n\t\t\t  [14] = \"Am5x86-WT\",\n\t\t\t  [15] = \"Am5x86-WB\"\n\t\t  }\n\t\t},\n\t},\n\t.legacy_cache_size = amd_size_cache,\n#endif\n\t.c_early_init   = early_init_amd,\n\t.c_detect_tlb\t= cpu_detect_tlb_amd,\n\t.c_bsp_init\t= bsp_init_amd,\n\t.c_init\t\t= init_amd,\n\t.c_x86_vendor\t= X86_VENDOR_AMD,\n};\n\ncpu_dev_register(amd_cpu_dev);\n\nstatic DEFINE_PER_CPU_READ_MOSTLY(unsigned long[4], amd_dr_addr_mask);\n\nstatic unsigned int amd_msr_dr_addr_masks[] = {\n\tMSR_F16H_DR0_ADDR_MASK,\n\tMSR_F16H_DR1_ADDR_MASK,\n\tMSR_F16H_DR1_ADDR_MASK + 1,\n\tMSR_F16H_DR1_ADDR_MASK + 2\n};\n\nvoid amd_set_dr_addr_mask(unsigned long mask, unsigned int dr)\n{\n\tint cpu = smp_processor_id();\n\n\tif (!cpu_feature_enabled(X86_FEATURE_BPEXT))\n\t\treturn;\n\n\tif (WARN_ON_ONCE(dr >= ARRAY_SIZE(amd_msr_dr_addr_masks)))\n\t\treturn;\n\n\tif (per_cpu(amd_dr_addr_mask, cpu)[dr] == mask)\n\t\treturn;\n\n\twrmsr(amd_msr_dr_addr_masks[dr], mask, 0);\n\tper_cpu(amd_dr_addr_mask, cpu)[dr] = mask;\n}\n\nunsigned long amd_get_dr_addr_mask(unsigned int dr)\n{\n\tif (!cpu_feature_enabled(X86_FEATURE_BPEXT))\n\t\treturn 0;\n\n\tif (WARN_ON_ONCE(dr >= ARRAY_SIZE(amd_msr_dr_addr_masks)))\n\t\treturn 0;\n\n\treturn per_cpu(amd_dr_addr_mask[dr], smp_processor_id());\n}\nEXPORT_SYMBOL_GPL(amd_get_dr_addr_mask);\n\nu32 amd_get_highest_perf(void)\n{\n\tstruct cpuinfo_x86 *c = &boot_cpu_data;\n\n\tif (c->x86 == 0x17 && ((c->x86_model >= 0x30 && c->x86_model < 0x40) ||\n\t\t\t       (c->x86_model >= 0x70 && c->x86_model < 0x80)))\n\t\treturn 166;\n\n\tif (c->x86 == 0x19 && ((c->x86_model >= 0x20 && c->x86_model < 0x30) ||\n\t\t\t       (c->x86_model >= 0x40 && c->x86_model < 0x70)))\n\t\treturn 166;\n\n\treturn 255;\n}\nEXPORT_SYMBOL_GPL(amd_get_highest_perf);\n\nstatic void zenbleed_check_cpu(void *unused)\n{\n\tstruct cpuinfo_x86 *c = &cpu_data(smp_processor_id());\n\n\tzenbleed_check(c);\n}\n\nvoid amd_check_microcode(void)\n{\n\tif (boot_cpu_data.x86_vendor != X86_VENDOR_AMD)\n\t\treturn;\n\n\ton_each_cpu(zenbleed_check_cpu, NULL, 1);\n}\n\n \nvoid noinstr amd_clear_divider(void)\n{\n\tasm volatile(ALTERNATIVE(\"\", \"div %2\\n\\t\", X86_BUG_DIV0)\n\t\t     :: \"a\" (0), \"d\" (0), \"r\" (1));\n}\nEXPORT_SYMBOL_GPL(amd_clear_divider);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}