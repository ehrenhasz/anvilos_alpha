{
  "module_name": "monitor.c",
  "hash_id": "268cf96f7bf35054f9aaa56a32edaf71c2f8faae57a8c18d96839f5445fca54b",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/cpu/resctrl/monitor.c",
  "human_readable_source": "\n \n\n#include <linux/module.h>\n#include <linux/sizes.h>\n#include <linux/slab.h>\n\n#include <asm/cpu_device_id.h>\n#include <asm/resctrl.h>\n\n#include \"internal.h\"\n\nstruct rmid_entry {\n\tu32\t\t\t\trmid;\n\tint\t\t\t\tbusy;\n\tstruct list_head\t\tlist;\n};\n\n \nstatic LIST_HEAD(rmid_free_lru);\n\n \nstatic unsigned int rmid_limbo_count;\n\n \nstatic struct rmid_entry\t*rmid_ptrs;\n\n \nbool rdt_mon_capable;\n\n \nunsigned int rdt_mon_features;\n\n \nunsigned int resctrl_rmid_realloc_threshold;\n\n \nunsigned int resctrl_rmid_realloc_limit;\n\n#define CF(cf)\t((unsigned long)(1048576 * (cf) + 0.5))\n\n \nstatic const struct mbm_correction_factor_table {\n\tu32 rmidthreshold;\n\tu64 cf;\n} mbm_cf_table[] __initconst = {\n\t{7,\tCF(1.000000)},\n\t{15,\tCF(1.000000)},\n\t{15,\tCF(0.969650)},\n\t{31,\tCF(1.000000)},\n\t{31,\tCF(1.066667)},\n\t{31,\tCF(0.969650)},\n\t{47,\tCF(1.142857)},\n\t{63,\tCF(1.000000)},\n\t{63,\tCF(1.185115)},\n\t{63,\tCF(1.066553)},\n\t{79,\tCF(1.454545)},\n\t{95,\tCF(1.000000)},\n\t{95,\tCF(1.230769)},\n\t{95,\tCF(1.142857)},\n\t{95,\tCF(1.066667)},\n\t{127,\tCF(1.000000)},\n\t{127,\tCF(1.254863)},\n\t{127,\tCF(1.185255)},\n\t{151,\tCF(1.000000)},\n\t{127,\tCF(1.066667)},\n\t{167,\tCF(1.000000)},\n\t{159,\tCF(1.454334)},\n\t{183,\tCF(1.000000)},\n\t{127,\tCF(0.969744)},\n\t{191,\tCF(1.280246)},\n\t{191,\tCF(1.230921)},\n\t{215,\tCF(1.000000)},\n\t{191,\tCF(1.143118)},\n};\n\nstatic u32 mbm_cf_rmidthreshold __read_mostly = UINT_MAX;\nstatic u64 mbm_cf __read_mostly;\n\nstatic inline u64 get_corrected_mbm_count(u32 rmid, unsigned long val)\n{\n\t \n\tif (rmid > mbm_cf_rmidthreshold)\n\t\tval = (val * mbm_cf) >> 20;\n\n\treturn val;\n}\n\nstatic inline struct rmid_entry *__rmid_entry(u32 rmid)\n{\n\tstruct rmid_entry *entry;\n\n\tentry = &rmid_ptrs[rmid];\n\tWARN_ON(entry->rmid != rmid);\n\n\treturn entry;\n}\n\nstatic int __rmid_read(u32 rmid, enum resctrl_event_id eventid, u64 *val)\n{\n\tu64 msr_val;\n\n\t \n\twrmsr(MSR_IA32_QM_EVTSEL, eventid, rmid);\n\trdmsrl(MSR_IA32_QM_CTR, msr_val);\n\n\tif (msr_val & RMID_VAL_ERROR)\n\t\treturn -EIO;\n\tif (msr_val & RMID_VAL_UNAVAIL)\n\t\treturn -EINVAL;\n\n\t*val = msr_val;\n\treturn 0;\n}\n\nstatic struct arch_mbm_state *get_arch_mbm_state(struct rdt_hw_domain *hw_dom,\n\t\t\t\t\t\t u32 rmid,\n\t\t\t\t\t\t enum resctrl_event_id eventid)\n{\n\tswitch (eventid) {\n\tcase QOS_L3_OCCUP_EVENT_ID:\n\t\treturn NULL;\n\tcase QOS_L3_MBM_TOTAL_EVENT_ID:\n\t\treturn &hw_dom->arch_mbm_total[rmid];\n\tcase QOS_L3_MBM_LOCAL_EVENT_ID:\n\t\treturn &hw_dom->arch_mbm_local[rmid];\n\t}\n\n\t \n\tWARN_ON_ONCE(1);\n\n\treturn NULL;\n}\n\nvoid resctrl_arch_reset_rmid(struct rdt_resource *r, struct rdt_domain *d,\n\t\t\t     u32 rmid, enum resctrl_event_id eventid)\n{\n\tstruct rdt_hw_domain *hw_dom = resctrl_to_arch_dom(d);\n\tstruct arch_mbm_state *am;\n\n\tam = get_arch_mbm_state(hw_dom, rmid, eventid);\n\tif (am) {\n\t\tmemset(am, 0, sizeof(*am));\n\n\t\t \n\t\t__rmid_read(rmid, eventid, &am->prev_msr);\n\t}\n}\n\n \nvoid resctrl_arch_reset_rmid_all(struct rdt_resource *r, struct rdt_domain *d)\n{\n\tstruct rdt_hw_domain *hw_dom = resctrl_to_arch_dom(d);\n\n\tif (is_mbm_total_enabled())\n\t\tmemset(hw_dom->arch_mbm_total, 0,\n\t\t       sizeof(*hw_dom->arch_mbm_total) * r->num_rmid);\n\n\tif (is_mbm_local_enabled())\n\t\tmemset(hw_dom->arch_mbm_local, 0,\n\t\t       sizeof(*hw_dom->arch_mbm_local) * r->num_rmid);\n}\n\nstatic u64 mbm_overflow_count(u64 prev_msr, u64 cur_msr, unsigned int width)\n{\n\tu64 shift = 64 - width, chunks;\n\n\tchunks = (cur_msr << shift) - (prev_msr << shift);\n\treturn chunks >> shift;\n}\n\nint resctrl_arch_rmid_read(struct rdt_resource *r, struct rdt_domain *d,\n\t\t\t   u32 rmid, enum resctrl_event_id eventid, u64 *val)\n{\n\tstruct rdt_hw_resource *hw_res = resctrl_to_arch_res(r);\n\tstruct rdt_hw_domain *hw_dom = resctrl_to_arch_dom(d);\n\tstruct arch_mbm_state *am;\n\tu64 msr_val, chunks;\n\tint ret;\n\n\tif (!cpumask_test_cpu(smp_processor_id(), &d->cpu_mask))\n\t\treturn -EINVAL;\n\n\tret = __rmid_read(rmid, eventid, &msr_val);\n\tif (ret)\n\t\treturn ret;\n\n\tam = get_arch_mbm_state(hw_dom, rmid, eventid);\n\tif (am) {\n\t\tam->chunks += mbm_overflow_count(am->prev_msr, msr_val,\n\t\t\t\t\t\t hw_res->mbm_width);\n\t\tchunks = get_corrected_mbm_count(rmid, am->chunks);\n\t\tam->prev_msr = msr_val;\n\t} else {\n\t\tchunks = msr_val;\n\t}\n\n\t*val = chunks * hw_res->mon_scale;\n\n\treturn 0;\n}\n\n \nvoid __check_limbo(struct rdt_domain *d, bool force_free)\n{\n\tstruct rdt_resource *r = &rdt_resources_all[RDT_RESOURCE_L3].r_resctrl;\n\tstruct rmid_entry *entry;\n\tu32 crmid = 1, nrmid;\n\tbool rmid_dirty;\n\tu64 val = 0;\n\n\t \n\tfor (;;) {\n\t\tnrmid = find_next_bit(d->rmid_busy_llc, r->num_rmid, crmid);\n\t\tif (nrmid >= r->num_rmid)\n\t\t\tbreak;\n\n\t\tentry = __rmid_entry(nrmid);\n\n\t\tif (resctrl_arch_rmid_read(r, d, entry->rmid,\n\t\t\t\t\t   QOS_L3_OCCUP_EVENT_ID, &val)) {\n\t\t\trmid_dirty = true;\n\t\t} else {\n\t\t\trmid_dirty = (val >= resctrl_rmid_realloc_threshold);\n\t\t}\n\n\t\tif (force_free || !rmid_dirty) {\n\t\t\tclear_bit(entry->rmid, d->rmid_busy_llc);\n\t\t\tif (!--entry->busy) {\n\t\t\t\trmid_limbo_count--;\n\t\t\t\tlist_add_tail(&entry->list, &rmid_free_lru);\n\t\t\t}\n\t\t}\n\t\tcrmid = nrmid + 1;\n\t}\n}\n\nbool has_busy_rmid(struct rdt_resource *r, struct rdt_domain *d)\n{\n\treturn find_first_bit(d->rmid_busy_llc, r->num_rmid) != r->num_rmid;\n}\n\n \nint alloc_rmid(void)\n{\n\tstruct rmid_entry *entry;\n\n\tlockdep_assert_held(&rdtgroup_mutex);\n\n\tif (list_empty(&rmid_free_lru))\n\t\treturn rmid_limbo_count ? -EBUSY : -ENOSPC;\n\n\tentry = list_first_entry(&rmid_free_lru,\n\t\t\t\t struct rmid_entry, list);\n\tlist_del(&entry->list);\n\n\treturn entry->rmid;\n}\n\nstatic void add_rmid_to_limbo(struct rmid_entry *entry)\n{\n\tstruct rdt_resource *r = &rdt_resources_all[RDT_RESOURCE_L3].r_resctrl;\n\tstruct rdt_domain *d;\n\tint cpu, err;\n\tu64 val = 0;\n\n\tentry->busy = 0;\n\tcpu = get_cpu();\n\tlist_for_each_entry(d, &r->domains, list) {\n\t\tif (cpumask_test_cpu(cpu, &d->cpu_mask)) {\n\t\t\terr = resctrl_arch_rmid_read(r, d, entry->rmid,\n\t\t\t\t\t\t     QOS_L3_OCCUP_EVENT_ID,\n\t\t\t\t\t\t     &val);\n\t\t\tif (err || val <= resctrl_rmid_realloc_threshold)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!has_busy_rmid(r, d))\n\t\t\tcqm_setup_limbo_handler(d, CQM_LIMBOCHECK_INTERVAL);\n\t\tset_bit(entry->rmid, d->rmid_busy_llc);\n\t\tentry->busy++;\n\t}\n\tput_cpu();\n\n\tif (entry->busy)\n\t\trmid_limbo_count++;\n\telse\n\t\tlist_add_tail(&entry->list, &rmid_free_lru);\n}\n\nvoid free_rmid(u32 rmid)\n{\n\tstruct rmid_entry *entry;\n\n\tif (!rmid)\n\t\treturn;\n\n\tlockdep_assert_held(&rdtgroup_mutex);\n\n\tentry = __rmid_entry(rmid);\n\n\tif (is_llc_occupancy_enabled())\n\t\tadd_rmid_to_limbo(entry);\n\telse\n\t\tlist_add_tail(&entry->list, &rmid_free_lru);\n}\n\nstatic struct mbm_state *get_mbm_state(struct rdt_domain *d, u32 rmid,\n\t\t\t\t       enum resctrl_event_id evtid)\n{\n\tswitch (evtid) {\n\tcase QOS_L3_MBM_TOTAL_EVENT_ID:\n\t\treturn &d->mbm_total[rmid];\n\tcase QOS_L3_MBM_LOCAL_EVENT_ID:\n\t\treturn &d->mbm_local[rmid];\n\tdefault:\n\t\treturn NULL;\n\t}\n}\n\nstatic int __mon_event_count(u32 rmid, struct rmid_read *rr)\n{\n\tstruct mbm_state *m;\n\tu64 tval = 0;\n\n\tif (rr->first) {\n\t\tresctrl_arch_reset_rmid(rr->r, rr->d, rmid, rr->evtid);\n\t\tm = get_mbm_state(rr->d, rmid, rr->evtid);\n\t\tif (m)\n\t\t\tmemset(m, 0, sizeof(struct mbm_state));\n\t\treturn 0;\n\t}\n\n\trr->err = resctrl_arch_rmid_read(rr->r, rr->d, rmid, rr->evtid, &tval);\n\tif (rr->err)\n\t\treturn rr->err;\n\n\trr->val += tval;\n\n\treturn 0;\n}\n\n \nstatic void mbm_bw_count(u32 rmid, struct rmid_read *rr)\n{\n\tstruct mbm_state *m = &rr->d->mbm_local[rmid];\n\tu64 cur_bw, bytes, cur_bytes;\n\n\tcur_bytes = rr->val;\n\tbytes = cur_bytes - m->prev_bw_bytes;\n\tm->prev_bw_bytes = cur_bytes;\n\n\tcur_bw = bytes / SZ_1M;\n\n\tif (m->delta_comp)\n\t\tm->delta_bw = abs(cur_bw - m->prev_bw);\n\tm->delta_comp = false;\n\tm->prev_bw = cur_bw;\n}\n\n \nvoid mon_event_count(void *info)\n{\n\tstruct rdtgroup *rdtgrp, *entry;\n\tstruct rmid_read *rr = info;\n\tstruct list_head *head;\n\tint ret;\n\n\trdtgrp = rr->rgrp;\n\n\tret = __mon_event_count(rdtgrp->mon.rmid, rr);\n\n\t \n\thead = &rdtgrp->mon.crdtgrp_list;\n\n\tif (rdtgrp->type == RDTCTRL_GROUP) {\n\t\tlist_for_each_entry(entry, head, mon.crdtgrp_list) {\n\t\t\tif (__mon_event_count(entry->mon.rmid, rr) == 0)\n\t\t\t\tret = 0;\n\t\t}\n\t}\n\n\t \n\tif (ret == 0)\n\t\trr->err = 0;\n}\n\n \nstatic void update_mba_bw(struct rdtgroup *rgrp, struct rdt_domain *dom_mbm)\n{\n\tu32 closid, rmid, cur_msr_val, new_msr_val;\n\tstruct mbm_state *pmbm_data, *cmbm_data;\n\tu32 cur_bw, delta_bw, user_bw;\n\tstruct rdt_resource *r_mba;\n\tstruct rdt_domain *dom_mba;\n\tstruct list_head *head;\n\tstruct rdtgroup *entry;\n\n\tif (!is_mbm_local_enabled())\n\t\treturn;\n\n\tr_mba = &rdt_resources_all[RDT_RESOURCE_MBA].r_resctrl;\n\n\tclosid = rgrp->closid;\n\trmid = rgrp->mon.rmid;\n\tpmbm_data = &dom_mbm->mbm_local[rmid];\n\n\tdom_mba = get_domain_from_cpu(smp_processor_id(), r_mba);\n\tif (!dom_mba) {\n\t\tpr_warn_once(\"Failure to get domain for MBA update\\n\");\n\t\treturn;\n\t}\n\n\tcur_bw = pmbm_data->prev_bw;\n\tuser_bw = dom_mba->mbps_val[closid];\n\tdelta_bw = pmbm_data->delta_bw;\n\n\t \n\tcur_msr_val = resctrl_arch_get_config(r_mba, dom_mba, closid, CDP_NONE);\n\n\t \n\thead = &rgrp->mon.crdtgrp_list;\n\tlist_for_each_entry(entry, head, mon.crdtgrp_list) {\n\t\tcmbm_data = &dom_mbm->mbm_local[entry->mon.rmid];\n\t\tcur_bw += cmbm_data->prev_bw;\n\t\tdelta_bw += cmbm_data->delta_bw;\n\t}\n\n\t \n\tif (cur_msr_val > r_mba->membw.min_bw && user_bw < cur_bw) {\n\t\tnew_msr_val = cur_msr_val - r_mba->membw.bw_gran;\n\t} else if (cur_msr_val < MAX_MBA_BW &&\n\t\t   (user_bw > (cur_bw + delta_bw))) {\n\t\tnew_msr_val = cur_msr_val + r_mba->membw.bw_gran;\n\t} else {\n\t\treturn;\n\t}\n\n\tresctrl_arch_update_one(r_mba, dom_mba, closid, CDP_NONE, new_msr_val);\n\n\t \n\tpmbm_data->delta_comp = true;\n\tlist_for_each_entry(entry, head, mon.crdtgrp_list) {\n\t\tcmbm_data = &dom_mbm->mbm_local[entry->mon.rmid];\n\t\tcmbm_data->delta_comp = true;\n\t}\n}\n\nstatic void mbm_update(struct rdt_resource *r, struct rdt_domain *d, int rmid)\n{\n\tstruct rmid_read rr;\n\n\trr.first = false;\n\trr.r = r;\n\trr.d = d;\n\n\t \n\tif (is_mbm_total_enabled()) {\n\t\trr.evtid = QOS_L3_MBM_TOTAL_EVENT_ID;\n\t\trr.val = 0;\n\t\t__mon_event_count(rmid, &rr);\n\t}\n\tif (is_mbm_local_enabled()) {\n\t\trr.evtid = QOS_L3_MBM_LOCAL_EVENT_ID;\n\t\trr.val = 0;\n\t\t__mon_event_count(rmid, &rr);\n\n\t\t \n\t\tif (is_mba_sc(NULL))\n\t\t\tmbm_bw_count(rmid, &rr);\n\t}\n}\n\n \nvoid cqm_handle_limbo(struct work_struct *work)\n{\n\tunsigned long delay = msecs_to_jiffies(CQM_LIMBOCHECK_INTERVAL);\n\tint cpu = smp_processor_id();\n\tstruct rdt_resource *r;\n\tstruct rdt_domain *d;\n\n\tmutex_lock(&rdtgroup_mutex);\n\n\tr = &rdt_resources_all[RDT_RESOURCE_L3].r_resctrl;\n\td = container_of(work, struct rdt_domain, cqm_limbo.work);\n\n\t__check_limbo(d, false);\n\n\tif (has_busy_rmid(r, d))\n\t\tschedule_delayed_work_on(cpu, &d->cqm_limbo, delay);\n\n\tmutex_unlock(&rdtgroup_mutex);\n}\n\nvoid cqm_setup_limbo_handler(struct rdt_domain *dom, unsigned long delay_ms)\n{\n\tunsigned long delay = msecs_to_jiffies(delay_ms);\n\tint cpu;\n\n\tcpu = cpumask_any(&dom->cpu_mask);\n\tdom->cqm_work_cpu = cpu;\n\n\tschedule_delayed_work_on(cpu, &dom->cqm_limbo, delay);\n}\n\nvoid mbm_handle_overflow(struct work_struct *work)\n{\n\tunsigned long delay = msecs_to_jiffies(MBM_OVERFLOW_INTERVAL);\n\tstruct rdtgroup *prgrp, *crgrp;\n\tint cpu = smp_processor_id();\n\tstruct list_head *head;\n\tstruct rdt_resource *r;\n\tstruct rdt_domain *d;\n\n\tmutex_lock(&rdtgroup_mutex);\n\n\tif (!static_branch_likely(&rdt_mon_enable_key))\n\t\tgoto out_unlock;\n\n\tr = &rdt_resources_all[RDT_RESOURCE_L3].r_resctrl;\n\td = container_of(work, struct rdt_domain, mbm_over.work);\n\n\tlist_for_each_entry(prgrp, &rdt_all_groups, rdtgroup_list) {\n\t\tmbm_update(r, d, prgrp->mon.rmid);\n\n\t\thead = &prgrp->mon.crdtgrp_list;\n\t\tlist_for_each_entry(crgrp, head, mon.crdtgrp_list)\n\t\t\tmbm_update(r, d, crgrp->mon.rmid);\n\n\t\tif (is_mba_sc(NULL))\n\t\t\tupdate_mba_bw(prgrp, d);\n\t}\n\n\tschedule_delayed_work_on(cpu, &d->mbm_over, delay);\n\nout_unlock:\n\tmutex_unlock(&rdtgroup_mutex);\n}\n\nvoid mbm_setup_overflow_handler(struct rdt_domain *dom, unsigned long delay_ms)\n{\n\tunsigned long delay = msecs_to_jiffies(delay_ms);\n\tint cpu;\n\n\tif (!static_branch_likely(&rdt_mon_enable_key))\n\t\treturn;\n\tcpu = cpumask_any(&dom->cpu_mask);\n\tdom->mbm_work_cpu = cpu;\n\tschedule_delayed_work_on(cpu, &dom->mbm_over, delay);\n}\n\nstatic int dom_data_init(struct rdt_resource *r)\n{\n\tstruct rmid_entry *entry = NULL;\n\tint i, nr_rmids;\n\n\tnr_rmids = r->num_rmid;\n\trmid_ptrs = kcalloc(nr_rmids, sizeof(struct rmid_entry), GFP_KERNEL);\n\tif (!rmid_ptrs)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < nr_rmids; i++) {\n\t\tentry = &rmid_ptrs[i];\n\t\tINIT_LIST_HEAD(&entry->list);\n\n\t\tentry->rmid = i;\n\t\tlist_add_tail(&entry->list, &rmid_free_lru);\n\t}\n\n\t \n\tentry = __rmid_entry(0);\n\tlist_del(&entry->list);\n\n\treturn 0;\n}\n\nstatic struct mon_evt llc_occupancy_event = {\n\t.name\t\t= \"llc_occupancy\",\n\t.evtid\t\t= QOS_L3_OCCUP_EVENT_ID,\n};\n\nstatic struct mon_evt mbm_total_event = {\n\t.name\t\t= \"mbm_total_bytes\",\n\t.evtid\t\t= QOS_L3_MBM_TOTAL_EVENT_ID,\n};\n\nstatic struct mon_evt mbm_local_event = {\n\t.name\t\t= \"mbm_local_bytes\",\n\t.evtid\t\t= QOS_L3_MBM_LOCAL_EVENT_ID,\n};\n\n \nstatic void l3_mon_evt_init(struct rdt_resource *r)\n{\n\tINIT_LIST_HEAD(&r->evt_list);\n\n\tif (is_llc_occupancy_enabled())\n\t\tlist_add_tail(&llc_occupancy_event.list, &r->evt_list);\n\tif (is_mbm_total_enabled())\n\t\tlist_add_tail(&mbm_total_event.list, &r->evt_list);\n\tif (is_mbm_local_enabled())\n\t\tlist_add_tail(&mbm_local_event.list, &r->evt_list);\n}\n\nint __init rdt_get_mon_l3_config(struct rdt_resource *r)\n{\n\tunsigned int mbm_offset = boot_cpu_data.x86_cache_mbm_width_offset;\n\tstruct rdt_hw_resource *hw_res = resctrl_to_arch_res(r);\n\tunsigned int threshold;\n\tint ret;\n\n\tresctrl_rmid_realloc_limit = boot_cpu_data.x86_cache_size * 1024;\n\thw_res->mon_scale = boot_cpu_data.x86_cache_occ_scale;\n\tr->num_rmid = boot_cpu_data.x86_cache_max_rmid + 1;\n\thw_res->mbm_width = MBM_CNTR_WIDTH_BASE;\n\n\tif (mbm_offset > 0 && mbm_offset <= MBM_CNTR_WIDTH_OFFSET_MAX)\n\t\thw_res->mbm_width += mbm_offset;\n\telse if (mbm_offset > MBM_CNTR_WIDTH_OFFSET_MAX)\n\t\tpr_warn(\"Ignoring impossible MBM counter offset\\n\");\n\n\t \n\tthreshold = resctrl_rmid_realloc_limit / r->num_rmid;\n\n\t \n\tresctrl_rmid_realloc_threshold = resctrl_arch_round_mon_val(threshold);\n\n\tret = dom_data_init(r);\n\tif (ret)\n\t\treturn ret;\n\n\tif (rdt_cpu_has(X86_FEATURE_BMEC)) {\n\t\tif (rdt_cpu_has(X86_FEATURE_CQM_MBM_TOTAL)) {\n\t\t\tmbm_total_event.configurable = true;\n\t\t\tmbm_config_rftype_init(\"mbm_total_bytes_config\");\n\t\t}\n\t\tif (rdt_cpu_has(X86_FEATURE_CQM_MBM_LOCAL)) {\n\t\t\tmbm_local_event.configurable = true;\n\t\t\tmbm_config_rftype_init(\"mbm_local_bytes_config\");\n\t\t}\n\t}\n\n\tl3_mon_evt_init(r);\n\n\tr->mon_capable = true;\n\n\treturn 0;\n}\n\nvoid __init intel_rdt_mbm_apply_quirk(void)\n{\n\tint cf_index;\n\n\tcf_index = (boot_cpu_data.x86_cache_max_rmid + 1) / 8 - 1;\n\tif (cf_index >= ARRAY_SIZE(mbm_cf_table)) {\n\t\tpr_info(\"No MBM correction factor available\\n\");\n\t\treturn;\n\t}\n\n\tmbm_cf_rmidthreshold = mbm_cf_table[cf_index].rmidthreshold;\n\tmbm_cf = mbm_cf_table[cf_index].cf;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}