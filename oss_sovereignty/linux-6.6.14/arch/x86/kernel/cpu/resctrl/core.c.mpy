{
  "module_name": "core.c",
  "hash_id": "a5346c47a4cb504ff27ea1dfa37a654ca412e2ffd0e33ab72c73c1263a26f17a",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/cpu/resctrl/core.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)\t\"resctrl: \" fmt\n\n#include <linux/slab.h>\n#include <linux/err.h>\n#include <linux/cacheinfo.h>\n#include <linux/cpuhotplug.h>\n\n#include <asm/intel-family.h>\n#include <asm/resctrl.h>\n#include \"internal.h\"\n\n \nDEFINE_MUTEX(rdtgroup_mutex);\n\n \nDEFINE_PER_CPU(struct resctrl_pqr_state, pqr_state);\n\n \nint max_name_width, max_data_width;\n\n \nbool rdt_alloc_capable;\n\nstatic void\nmba_wrmsr_intel(struct rdt_domain *d, struct msr_param *m,\n\t\tstruct rdt_resource *r);\nstatic void\ncat_wrmsr(struct rdt_domain *d, struct msr_param *m, struct rdt_resource *r);\nstatic void\nmba_wrmsr_amd(struct rdt_domain *d, struct msr_param *m,\n\t      struct rdt_resource *r);\n\n#define domain_init(id) LIST_HEAD_INIT(rdt_resources_all[id].r_resctrl.domains)\n\nstruct rdt_hw_resource rdt_resources_all[] = {\n\t[RDT_RESOURCE_L3] =\n\t{\n\t\t.r_resctrl = {\n\t\t\t.rid\t\t\t= RDT_RESOURCE_L3,\n\t\t\t.name\t\t\t= \"L3\",\n\t\t\t.cache_level\t\t= 3,\n\t\t\t.domains\t\t= domain_init(RDT_RESOURCE_L3),\n\t\t\t.parse_ctrlval\t\t= parse_cbm,\n\t\t\t.format_str\t\t= \"%d=%0*x\",\n\t\t\t.fflags\t\t\t= RFTYPE_RES_CACHE,\n\t\t},\n\t\t.msr_base\t\t= MSR_IA32_L3_CBM_BASE,\n\t\t.msr_update\t\t= cat_wrmsr,\n\t},\n\t[RDT_RESOURCE_L2] =\n\t{\n\t\t.r_resctrl = {\n\t\t\t.rid\t\t\t= RDT_RESOURCE_L2,\n\t\t\t.name\t\t\t= \"L2\",\n\t\t\t.cache_level\t\t= 2,\n\t\t\t.domains\t\t= domain_init(RDT_RESOURCE_L2),\n\t\t\t.parse_ctrlval\t\t= parse_cbm,\n\t\t\t.format_str\t\t= \"%d=%0*x\",\n\t\t\t.fflags\t\t\t= RFTYPE_RES_CACHE,\n\t\t},\n\t\t.msr_base\t\t= MSR_IA32_L2_CBM_BASE,\n\t\t.msr_update\t\t= cat_wrmsr,\n\t},\n\t[RDT_RESOURCE_MBA] =\n\t{\n\t\t.r_resctrl = {\n\t\t\t.rid\t\t\t= RDT_RESOURCE_MBA,\n\t\t\t.name\t\t\t= \"MB\",\n\t\t\t.cache_level\t\t= 3,\n\t\t\t.domains\t\t= domain_init(RDT_RESOURCE_MBA),\n\t\t\t.parse_ctrlval\t\t= parse_bw,\n\t\t\t.format_str\t\t= \"%d=%*u\",\n\t\t\t.fflags\t\t\t= RFTYPE_RES_MB,\n\t\t},\n\t},\n\t[RDT_RESOURCE_SMBA] =\n\t{\n\t\t.r_resctrl = {\n\t\t\t.rid\t\t\t= RDT_RESOURCE_SMBA,\n\t\t\t.name\t\t\t= \"SMBA\",\n\t\t\t.cache_level\t\t= 3,\n\t\t\t.domains\t\t= domain_init(RDT_RESOURCE_SMBA),\n\t\t\t.parse_ctrlval\t\t= parse_bw,\n\t\t\t.format_str\t\t= \"%d=%*u\",\n\t\t\t.fflags\t\t\t= RFTYPE_RES_MB,\n\t\t},\n\t},\n};\n\n \nstatic inline void cache_alloc_hsw_probe(void)\n{\n\tstruct rdt_hw_resource *hw_res = &rdt_resources_all[RDT_RESOURCE_L3];\n\tstruct rdt_resource *r  = &hw_res->r_resctrl;\n\tu32 l, h, max_cbm = BIT_MASK(20) - 1;\n\n\tif (wrmsr_safe(MSR_IA32_L3_CBM_BASE, max_cbm, 0))\n\t\treturn;\n\n\trdmsr(MSR_IA32_L3_CBM_BASE, l, h);\n\n\t \n\tif (l != max_cbm)\n\t\treturn;\n\n\thw_res->num_closid = 4;\n\tr->default_ctrl = max_cbm;\n\tr->cache.cbm_len = 20;\n\tr->cache.shareable_bits = 0xc0000;\n\tr->cache.min_cbm_bits = 2;\n\tr->alloc_capable = true;\n\n\trdt_alloc_capable = true;\n}\n\nbool is_mba_sc(struct rdt_resource *r)\n{\n\tif (!r)\n\t\treturn rdt_resources_all[RDT_RESOURCE_MBA].r_resctrl.membw.mba_sc;\n\n\t \n\tif (r->rid != RDT_RESOURCE_MBA)\n\t\treturn false;\n\n\treturn r->membw.mba_sc;\n}\n\n \nstatic inline bool rdt_get_mb_table(struct rdt_resource *r)\n{\n\t \n\tpr_info(\"MBA b/w map not implemented for cpu:%d, model:%d\",\n\t\tboot_cpu_data.x86, boot_cpu_data.x86_model);\n\n\treturn false;\n}\n\nstatic bool __get_mem_config_intel(struct rdt_resource *r)\n{\n\tstruct rdt_hw_resource *hw_res = resctrl_to_arch_res(r);\n\tunion cpuid_0x10_3_eax eax;\n\tunion cpuid_0x10_x_edx edx;\n\tu32 ebx, ecx, max_delay;\n\n\tcpuid_count(0x00000010, 3, &eax.full, &ebx, &ecx, &edx.full);\n\thw_res->num_closid = edx.split.cos_max + 1;\n\tmax_delay = eax.split.max_delay + 1;\n\tr->default_ctrl = MAX_MBA_BW;\n\tr->membw.arch_needs_linear = true;\n\tif (ecx & MBA_IS_LINEAR) {\n\t\tr->membw.delay_linear = true;\n\t\tr->membw.min_bw = MAX_MBA_BW - max_delay;\n\t\tr->membw.bw_gran = MAX_MBA_BW - max_delay;\n\t} else {\n\t\tif (!rdt_get_mb_table(r))\n\t\t\treturn false;\n\t\tr->membw.arch_needs_linear = false;\n\t}\n\tr->data_width = 3;\n\n\tif (boot_cpu_has(X86_FEATURE_PER_THREAD_MBA))\n\t\tr->membw.throttle_mode = THREAD_THROTTLE_PER_THREAD;\n\telse\n\t\tr->membw.throttle_mode = THREAD_THROTTLE_MAX;\n\tthread_throttle_mode_init();\n\n\tr->alloc_capable = true;\n\n\treturn true;\n}\n\nstatic bool __rdt_get_mem_config_amd(struct rdt_resource *r)\n{\n\tstruct rdt_hw_resource *hw_res = resctrl_to_arch_res(r);\n\tunion cpuid_0x10_3_eax eax;\n\tunion cpuid_0x10_x_edx edx;\n\tu32 ebx, ecx, subleaf;\n\n\t \n\tsubleaf = (r->rid == RDT_RESOURCE_SMBA) ? 2 :  1;\n\n\tcpuid_count(0x80000020, subleaf, &eax.full, &ebx, &ecx, &edx.full);\n\thw_res->num_closid = edx.split.cos_max + 1;\n\tr->default_ctrl = MAX_MBA_BW_AMD;\n\n\t \n\tr->membw.delay_linear = false;\n\tr->membw.arch_needs_linear = false;\n\n\t \n\tr->membw.throttle_mode = THREAD_THROTTLE_UNDEFINED;\n\tr->membw.min_bw = 0;\n\tr->membw.bw_gran = 1;\n\t \n\tr->data_width = 4;\n\n\tr->alloc_capable = true;\n\n\treturn true;\n}\n\nstatic void rdt_get_cache_alloc_cfg(int idx, struct rdt_resource *r)\n{\n\tstruct rdt_hw_resource *hw_res = resctrl_to_arch_res(r);\n\tunion cpuid_0x10_1_eax eax;\n\tunion cpuid_0x10_x_edx edx;\n\tu32 ebx, ecx;\n\n\tcpuid_count(0x00000010, idx, &eax.full, &ebx, &ecx, &edx.full);\n\thw_res->num_closid = edx.split.cos_max + 1;\n\tr->cache.cbm_len = eax.split.cbm_len + 1;\n\tr->default_ctrl = BIT_MASK(eax.split.cbm_len + 1) - 1;\n\tr->cache.shareable_bits = ebx & r->default_ctrl;\n\tr->data_width = (r->cache.cbm_len + 3) / 4;\n\tr->alloc_capable = true;\n}\n\nstatic void rdt_get_cdp_config(int level)\n{\n\t \n\trdt_resources_all[level].cdp_enabled = false;\n\trdt_resources_all[level].r_resctrl.cdp_capable = true;\n}\n\nstatic void rdt_get_cdp_l3_config(void)\n{\n\trdt_get_cdp_config(RDT_RESOURCE_L3);\n}\n\nstatic void rdt_get_cdp_l2_config(void)\n{\n\trdt_get_cdp_config(RDT_RESOURCE_L2);\n}\n\nstatic void\nmba_wrmsr_amd(struct rdt_domain *d, struct msr_param *m, struct rdt_resource *r)\n{\n\tunsigned int i;\n\tstruct rdt_hw_domain *hw_dom = resctrl_to_arch_dom(d);\n\tstruct rdt_hw_resource *hw_res = resctrl_to_arch_res(r);\n\n\tfor (i = m->low; i < m->high; i++)\n\t\twrmsrl(hw_res->msr_base + i, hw_dom->ctrl_val[i]);\n}\n\n \nstatic u32 delay_bw_map(unsigned long bw, struct rdt_resource *r)\n{\n\tif (r->membw.delay_linear)\n\t\treturn MAX_MBA_BW - bw;\n\n\tpr_warn_once(\"Non Linear delay-bw map not supported but queried\\n\");\n\treturn r->default_ctrl;\n}\n\nstatic void\nmba_wrmsr_intel(struct rdt_domain *d, struct msr_param *m,\n\t\tstruct rdt_resource *r)\n{\n\tunsigned int i;\n\tstruct rdt_hw_domain *hw_dom = resctrl_to_arch_dom(d);\n\tstruct rdt_hw_resource *hw_res = resctrl_to_arch_res(r);\n\n\t \n\tfor (i = m->low; i < m->high; i++)\n\t\twrmsrl(hw_res->msr_base + i, delay_bw_map(hw_dom->ctrl_val[i], r));\n}\n\nstatic void\ncat_wrmsr(struct rdt_domain *d, struct msr_param *m, struct rdt_resource *r)\n{\n\tunsigned int i;\n\tstruct rdt_hw_domain *hw_dom = resctrl_to_arch_dom(d);\n\tstruct rdt_hw_resource *hw_res = resctrl_to_arch_res(r);\n\n\tfor (i = m->low; i < m->high; i++)\n\t\twrmsrl(hw_res->msr_base + i, hw_dom->ctrl_val[i]);\n}\n\nstruct rdt_domain *get_domain_from_cpu(int cpu, struct rdt_resource *r)\n{\n\tstruct rdt_domain *d;\n\n\tlist_for_each_entry(d, &r->domains, list) {\n\t\t \n\t\tif (cpumask_test_cpu(cpu, &d->cpu_mask))\n\t\t\treturn d;\n\t}\n\n\treturn NULL;\n}\n\nu32 resctrl_arch_get_num_closid(struct rdt_resource *r)\n{\n\treturn resctrl_to_arch_res(r)->num_closid;\n}\n\nvoid rdt_ctrl_update(void *arg)\n{\n\tstruct msr_param *m = arg;\n\tstruct rdt_hw_resource *hw_res = resctrl_to_arch_res(m->res);\n\tstruct rdt_resource *r = m->res;\n\tint cpu = smp_processor_id();\n\tstruct rdt_domain *d;\n\n\td = get_domain_from_cpu(cpu, r);\n\tif (d) {\n\t\thw_res->msr_update(d, m, r);\n\t\treturn;\n\t}\n\tpr_warn_once(\"cpu %d not found in any domain for resource %s\\n\",\n\t\t     cpu, r->name);\n}\n\n \nstruct rdt_domain *rdt_find_domain(struct rdt_resource *r, int id,\n\t\t\t\t   struct list_head **pos)\n{\n\tstruct rdt_domain *d;\n\tstruct list_head *l;\n\n\tif (id < 0)\n\t\treturn ERR_PTR(-ENODEV);\n\n\tlist_for_each(l, &r->domains) {\n\t\td = list_entry(l, struct rdt_domain, list);\n\t\t \n\t\tif (id == d->id)\n\t\t\treturn d;\n\t\t \n\t\tif (id < d->id)\n\t\t\tbreak;\n\t}\n\n\tif (pos)\n\t\t*pos = l;\n\n\treturn NULL;\n}\n\nstatic void setup_default_ctrlval(struct rdt_resource *r, u32 *dc)\n{\n\tstruct rdt_hw_resource *hw_res = resctrl_to_arch_res(r);\n\tint i;\n\n\t \n\tfor (i = 0; i < hw_res->num_closid; i++, dc++)\n\t\t*dc = r->default_ctrl;\n}\n\nstatic void domain_free(struct rdt_hw_domain *hw_dom)\n{\n\tkfree(hw_dom->arch_mbm_total);\n\tkfree(hw_dom->arch_mbm_local);\n\tkfree(hw_dom->ctrl_val);\n\tkfree(hw_dom);\n}\n\nstatic int domain_setup_ctrlval(struct rdt_resource *r, struct rdt_domain *d)\n{\n\tstruct rdt_hw_resource *hw_res = resctrl_to_arch_res(r);\n\tstruct rdt_hw_domain *hw_dom = resctrl_to_arch_dom(d);\n\tstruct msr_param m;\n\tu32 *dc;\n\n\tdc = kmalloc_array(hw_res->num_closid, sizeof(*hw_dom->ctrl_val),\n\t\t\t   GFP_KERNEL);\n\tif (!dc)\n\t\treturn -ENOMEM;\n\n\thw_dom->ctrl_val = dc;\n\tsetup_default_ctrlval(r, dc);\n\n\tm.low = 0;\n\tm.high = hw_res->num_closid;\n\thw_res->msr_update(d, &m, r);\n\treturn 0;\n}\n\n \nstatic int arch_domain_mbm_alloc(u32 num_rmid, struct rdt_hw_domain *hw_dom)\n{\n\tsize_t tsize;\n\n\tif (is_mbm_total_enabled()) {\n\t\ttsize = sizeof(*hw_dom->arch_mbm_total);\n\t\thw_dom->arch_mbm_total = kcalloc(num_rmid, tsize, GFP_KERNEL);\n\t\tif (!hw_dom->arch_mbm_total)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (is_mbm_local_enabled()) {\n\t\ttsize = sizeof(*hw_dom->arch_mbm_local);\n\t\thw_dom->arch_mbm_local = kcalloc(num_rmid, tsize, GFP_KERNEL);\n\t\tif (!hw_dom->arch_mbm_local) {\n\t\t\tkfree(hw_dom->arch_mbm_total);\n\t\t\thw_dom->arch_mbm_total = NULL;\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic void domain_add_cpu(int cpu, struct rdt_resource *r)\n{\n\tint id = get_cpu_cacheinfo_id(cpu, r->cache_level);\n\tstruct list_head *add_pos = NULL;\n\tstruct rdt_hw_domain *hw_dom;\n\tstruct rdt_domain *d;\n\tint err;\n\n\td = rdt_find_domain(r, id, &add_pos);\n\tif (IS_ERR(d)) {\n\t\tpr_warn(\"Couldn't find cache id for CPU %d\\n\", cpu);\n\t\treturn;\n\t}\n\n\tif (d) {\n\t\tcpumask_set_cpu(cpu, &d->cpu_mask);\n\t\tif (r->cache.arch_has_per_cpu_cfg)\n\t\t\trdt_domain_reconfigure_cdp(r);\n\t\treturn;\n\t}\n\n\thw_dom = kzalloc_node(sizeof(*hw_dom), GFP_KERNEL, cpu_to_node(cpu));\n\tif (!hw_dom)\n\t\treturn;\n\n\td = &hw_dom->d_resctrl;\n\td->id = id;\n\tcpumask_set_cpu(cpu, &d->cpu_mask);\n\n\trdt_domain_reconfigure_cdp(r);\n\n\tif (r->alloc_capable && domain_setup_ctrlval(r, d)) {\n\t\tdomain_free(hw_dom);\n\t\treturn;\n\t}\n\n\tif (r->mon_capable && arch_domain_mbm_alloc(r->num_rmid, hw_dom)) {\n\t\tdomain_free(hw_dom);\n\t\treturn;\n\t}\n\n\tlist_add_tail(&d->list, add_pos);\n\n\terr = resctrl_online_domain(r, d);\n\tif (err) {\n\t\tlist_del(&d->list);\n\t\tdomain_free(hw_dom);\n\t}\n}\n\nstatic void domain_remove_cpu(int cpu, struct rdt_resource *r)\n{\n\tint id = get_cpu_cacheinfo_id(cpu, r->cache_level);\n\tstruct rdt_hw_domain *hw_dom;\n\tstruct rdt_domain *d;\n\n\td = rdt_find_domain(r, id, NULL);\n\tif (IS_ERR_OR_NULL(d)) {\n\t\tpr_warn(\"Couldn't find cache id for CPU %d\\n\", cpu);\n\t\treturn;\n\t}\n\thw_dom = resctrl_to_arch_dom(d);\n\n\tcpumask_clear_cpu(cpu, &d->cpu_mask);\n\tif (cpumask_empty(&d->cpu_mask)) {\n\t\tresctrl_offline_domain(r, d);\n\t\tlist_del(&d->list);\n\n\t\t \n\t\tif (d->plr)\n\t\t\td->plr->d = NULL;\n\t\tdomain_free(hw_dom);\n\n\t\treturn;\n\t}\n\n\tif (r == &rdt_resources_all[RDT_RESOURCE_L3].r_resctrl) {\n\t\tif (is_mbm_enabled() && cpu == d->mbm_work_cpu) {\n\t\t\tcancel_delayed_work(&d->mbm_over);\n\t\t\tmbm_setup_overflow_handler(d, 0);\n\t\t}\n\t\tif (is_llc_occupancy_enabled() && cpu == d->cqm_work_cpu &&\n\t\t    has_busy_rmid(r, d)) {\n\t\t\tcancel_delayed_work(&d->cqm_limbo);\n\t\t\tcqm_setup_limbo_handler(d, 0);\n\t\t}\n\t}\n}\n\nstatic void clear_closid_rmid(int cpu)\n{\n\tstruct resctrl_pqr_state *state = this_cpu_ptr(&pqr_state);\n\n\tstate->default_closid = 0;\n\tstate->default_rmid = 0;\n\tstate->cur_closid = 0;\n\tstate->cur_rmid = 0;\n\twrmsr(MSR_IA32_PQR_ASSOC, 0, 0);\n}\n\nstatic int resctrl_online_cpu(unsigned int cpu)\n{\n\tstruct rdt_resource *r;\n\n\tmutex_lock(&rdtgroup_mutex);\n\tfor_each_capable_rdt_resource(r)\n\t\tdomain_add_cpu(cpu, r);\n\t \n\tcpumask_set_cpu(cpu, &rdtgroup_default.cpu_mask);\n\tclear_closid_rmid(cpu);\n\tmutex_unlock(&rdtgroup_mutex);\n\n\treturn 0;\n}\n\nstatic void clear_childcpus(struct rdtgroup *r, unsigned int cpu)\n{\n\tstruct rdtgroup *cr;\n\n\tlist_for_each_entry(cr, &r->mon.crdtgrp_list, mon.crdtgrp_list) {\n\t\tif (cpumask_test_and_clear_cpu(cpu, &cr->cpu_mask)) {\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int resctrl_offline_cpu(unsigned int cpu)\n{\n\tstruct rdtgroup *rdtgrp;\n\tstruct rdt_resource *r;\n\n\tmutex_lock(&rdtgroup_mutex);\n\tfor_each_capable_rdt_resource(r)\n\t\tdomain_remove_cpu(cpu, r);\n\tlist_for_each_entry(rdtgrp, &rdt_all_groups, rdtgroup_list) {\n\t\tif (cpumask_test_and_clear_cpu(cpu, &rdtgrp->cpu_mask)) {\n\t\t\tclear_childcpus(rdtgrp, cpu);\n\t\t\tbreak;\n\t\t}\n\t}\n\tclear_closid_rmid(cpu);\n\tmutex_unlock(&rdtgroup_mutex);\n\n\treturn 0;\n}\n\n \nstatic __init void rdt_init_padding(void)\n{\n\tstruct rdt_resource *r;\n\n\tfor_each_alloc_capable_rdt_resource(r) {\n\t\tif (r->data_width > max_data_width)\n\t\t\tmax_data_width = r->data_width;\n\t}\n}\n\nenum {\n\tRDT_FLAG_CMT,\n\tRDT_FLAG_MBM_TOTAL,\n\tRDT_FLAG_MBM_LOCAL,\n\tRDT_FLAG_L3_CAT,\n\tRDT_FLAG_L3_CDP,\n\tRDT_FLAG_L2_CAT,\n\tRDT_FLAG_L2_CDP,\n\tRDT_FLAG_MBA,\n\tRDT_FLAG_SMBA,\n\tRDT_FLAG_BMEC,\n};\n\n#define RDT_OPT(idx, n, f)\t\\\n[idx] = {\t\t\t\\\n\t.name = n,\t\t\\\n\t.flag = f\t\t\\\n}\n\nstruct rdt_options {\n\tchar\t*name;\n\tint\tflag;\n\tbool\tforce_off, force_on;\n};\n\nstatic struct rdt_options rdt_options[]  __initdata = {\n\tRDT_OPT(RDT_FLAG_CMT,\t    \"cmt\",\tX86_FEATURE_CQM_OCCUP_LLC),\n\tRDT_OPT(RDT_FLAG_MBM_TOTAL, \"mbmtotal\", X86_FEATURE_CQM_MBM_TOTAL),\n\tRDT_OPT(RDT_FLAG_MBM_LOCAL, \"mbmlocal\", X86_FEATURE_CQM_MBM_LOCAL),\n\tRDT_OPT(RDT_FLAG_L3_CAT,    \"l3cat\",\tX86_FEATURE_CAT_L3),\n\tRDT_OPT(RDT_FLAG_L3_CDP,    \"l3cdp\",\tX86_FEATURE_CDP_L3),\n\tRDT_OPT(RDT_FLAG_L2_CAT,    \"l2cat\",\tX86_FEATURE_CAT_L2),\n\tRDT_OPT(RDT_FLAG_L2_CDP,    \"l2cdp\",\tX86_FEATURE_CDP_L2),\n\tRDT_OPT(RDT_FLAG_MBA,\t    \"mba\",\tX86_FEATURE_MBA),\n\tRDT_OPT(RDT_FLAG_SMBA,\t    \"smba\",\tX86_FEATURE_SMBA),\n\tRDT_OPT(RDT_FLAG_BMEC,\t    \"bmec\",\tX86_FEATURE_BMEC),\n};\n#define NUM_RDT_OPTIONS ARRAY_SIZE(rdt_options)\n\nstatic int __init set_rdt_options(char *str)\n{\n\tstruct rdt_options *o;\n\tbool force_off;\n\tchar *tok;\n\n\tif (*str == '=')\n\t\tstr++;\n\twhile ((tok = strsep(&str, \",\")) != NULL) {\n\t\tforce_off = *tok == '!';\n\t\tif (force_off)\n\t\t\ttok++;\n\t\tfor (o = rdt_options; o < &rdt_options[NUM_RDT_OPTIONS]; o++) {\n\t\t\tif (strcmp(tok, o->name) == 0) {\n\t\t\t\tif (force_off)\n\t\t\t\t\to->force_off = true;\n\t\t\t\telse\n\t\t\t\t\to->force_on = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\treturn 1;\n}\n__setup(\"rdt\", set_rdt_options);\n\nbool __init rdt_cpu_has(int flag)\n{\n\tbool ret = boot_cpu_has(flag);\n\tstruct rdt_options *o;\n\n\tif (!ret)\n\t\treturn ret;\n\n\tfor (o = rdt_options; o < &rdt_options[NUM_RDT_OPTIONS]; o++) {\n\t\tif (flag == o->flag) {\n\t\t\tif (o->force_off)\n\t\t\t\tret = false;\n\t\t\tif (o->force_on)\n\t\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic __init bool get_mem_config(void)\n{\n\tstruct rdt_hw_resource *hw_res = &rdt_resources_all[RDT_RESOURCE_MBA];\n\n\tif (!rdt_cpu_has(X86_FEATURE_MBA))\n\t\treturn false;\n\n\tif (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL)\n\t\treturn __get_mem_config_intel(&hw_res->r_resctrl);\n\telse if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)\n\t\treturn __rdt_get_mem_config_amd(&hw_res->r_resctrl);\n\n\treturn false;\n}\n\nstatic __init bool get_slow_mem_config(void)\n{\n\tstruct rdt_hw_resource *hw_res = &rdt_resources_all[RDT_RESOURCE_SMBA];\n\n\tif (!rdt_cpu_has(X86_FEATURE_SMBA))\n\t\treturn false;\n\n\tif (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)\n\t\treturn __rdt_get_mem_config_amd(&hw_res->r_resctrl);\n\n\treturn false;\n}\n\nstatic __init bool get_rdt_alloc_resources(void)\n{\n\tstruct rdt_resource *r;\n\tbool ret = false;\n\n\tif (rdt_alloc_capable)\n\t\treturn true;\n\n\tif (!boot_cpu_has(X86_FEATURE_RDT_A))\n\t\treturn false;\n\n\tif (rdt_cpu_has(X86_FEATURE_CAT_L3)) {\n\t\tr = &rdt_resources_all[RDT_RESOURCE_L3].r_resctrl;\n\t\trdt_get_cache_alloc_cfg(1, r);\n\t\tif (rdt_cpu_has(X86_FEATURE_CDP_L3))\n\t\t\trdt_get_cdp_l3_config();\n\t\tret = true;\n\t}\n\tif (rdt_cpu_has(X86_FEATURE_CAT_L2)) {\n\t\t \n\t\tr = &rdt_resources_all[RDT_RESOURCE_L2].r_resctrl;\n\t\trdt_get_cache_alloc_cfg(2, r);\n\t\tif (rdt_cpu_has(X86_FEATURE_CDP_L2))\n\t\t\trdt_get_cdp_l2_config();\n\t\tret = true;\n\t}\n\n\tif (get_mem_config())\n\t\tret = true;\n\n\tif (get_slow_mem_config())\n\t\tret = true;\n\n\treturn ret;\n}\n\nstatic __init bool get_rdt_mon_resources(void)\n{\n\tstruct rdt_resource *r = &rdt_resources_all[RDT_RESOURCE_L3].r_resctrl;\n\n\tif (rdt_cpu_has(X86_FEATURE_CQM_OCCUP_LLC))\n\t\trdt_mon_features |= (1 << QOS_L3_OCCUP_EVENT_ID);\n\tif (rdt_cpu_has(X86_FEATURE_CQM_MBM_TOTAL))\n\t\trdt_mon_features |= (1 << QOS_L3_MBM_TOTAL_EVENT_ID);\n\tif (rdt_cpu_has(X86_FEATURE_CQM_MBM_LOCAL))\n\t\trdt_mon_features |= (1 << QOS_L3_MBM_LOCAL_EVENT_ID);\n\n\tif (!rdt_mon_features)\n\t\treturn false;\n\n\treturn !rdt_get_mon_l3_config(r);\n}\n\nstatic __init void __check_quirks_intel(void)\n{\n\tswitch (boot_cpu_data.x86_model) {\n\tcase INTEL_FAM6_HASWELL_X:\n\t\tif (!rdt_options[RDT_FLAG_L3_CAT].force_off)\n\t\t\tcache_alloc_hsw_probe();\n\t\tbreak;\n\tcase INTEL_FAM6_SKYLAKE_X:\n\t\tif (boot_cpu_data.x86_stepping <= 4)\n\t\t\tset_rdt_options(\"!cmt,!mbmtotal,!mbmlocal,!l3cat\");\n\t\telse\n\t\t\tset_rdt_options(\"!l3cat\");\n\t\tfallthrough;\n\tcase INTEL_FAM6_BROADWELL_X:\n\t\tintel_rdt_mbm_apply_quirk();\n\t\tbreak;\n\t}\n}\n\nstatic __init void check_quirks(void)\n{\n\tif (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL)\n\t\t__check_quirks_intel();\n}\n\nstatic __init bool get_rdt_resources(void)\n{\n\trdt_alloc_capable = get_rdt_alloc_resources();\n\trdt_mon_capable = get_rdt_mon_resources();\n\n\treturn (rdt_mon_capable || rdt_alloc_capable);\n}\n\nstatic __init void rdt_init_res_defs_intel(void)\n{\n\tstruct rdt_hw_resource *hw_res;\n\tstruct rdt_resource *r;\n\n\tfor_each_rdt_resource(r) {\n\t\thw_res = resctrl_to_arch_res(r);\n\n\t\tif (r->rid == RDT_RESOURCE_L3 ||\n\t\t    r->rid == RDT_RESOURCE_L2) {\n\t\t\tr->cache.arch_has_sparse_bitmaps = false;\n\t\t\tr->cache.arch_has_per_cpu_cfg = false;\n\t\t\tr->cache.min_cbm_bits = 1;\n\t\t} else if (r->rid == RDT_RESOURCE_MBA) {\n\t\t\thw_res->msr_base = MSR_IA32_MBA_THRTL_BASE;\n\t\t\thw_res->msr_update = mba_wrmsr_intel;\n\t\t}\n\t}\n}\n\nstatic __init void rdt_init_res_defs_amd(void)\n{\n\tstruct rdt_hw_resource *hw_res;\n\tstruct rdt_resource *r;\n\n\tfor_each_rdt_resource(r) {\n\t\thw_res = resctrl_to_arch_res(r);\n\n\t\tif (r->rid == RDT_RESOURCE_L3 ||\n\t\t    r->rid == RDT_RESOURCE_L2) {\n\t\t\tr->cache.arch_has_sparse_bitmaps = true;\n\t\t\tr->cache.arch_has_per_cpu_cfg = true;\n\t\t\tr->cache.min_cbm_bits = 0;\n\t\t} else if (r->rid == RDT_RESOURCE_MBA) {\n\t\t\thw_res->msr_base = MSR_IA32_MBA_BW_BASE;\n\t\t\thw_res->msr_update = mba_wrmsr_amd;\n\t\t} else if (r->rid == RDT_RESOURCE_SMBA) {\n\t\t\thw_res->msr_base = MSR_IA32_SMBA_BW_BASE;\n\t\t\thw_res->msr_update = mba_wrmsr_amd;\n\t\t}\n\t}\n}\n\nstatic __init void rdt_init_res_defs(void)\n{\n\tif (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL)\n\t\trdt_init_res_defs_intel();\n\telse if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)\n\t\trdt_init_res_defs_amd();\n}\n\nstatic enum cpuhp_state rdt_online;\n\n \nvoid resctrl_cpu_detect(struct cpuinfo_x86 *c)\n{\n\tif (!cpu_has(c, X86_FEATURE_CQM_LLC)) {\n\t\tc->x86_cache_max_rmid  = -1;\n\t\tc->x86_cache_occ_scale = -1;\n\t\tc->x86_cache_mbm_width_offset = -1;\n\t\treturn;\n\t}\n\n\t \n\tc->x86_cache_max_rmid = cpuid_ebx(0xf);\n\n\tif (cpu_has(c, X86_FEATURE_CQM_OCCUP_LLC) ||\n\t    cpu_has(c, X86_FEATURE_CQM_MBM_TOTAL) ||\n\t    cpu_has(c, X86_FEATURE_CQM_MBM_LOCAL)) {\n\t\tu32 eax, ebx, ecx, edx;\n\n\t\t \n\t\tcpuid_count(0xf, 1, &eax, &ebx, &ecx, &edx);\n\n\t\tc->x86_cache_max_rmid  = ecx;\n\t\tc->x86_cache_occ_scale = ebx;\n\t\tc->x86_cache_mbm_width_offset = eax & 0xff;\n\n\t\tif (c->x86_vendor == X86_VENDOR_AMD && !c->x86_cache_mbm_width_offset)\n\t\t\tc->x86_cache_mbm_width_offset = MBM_CNTR_WIDTH_OFFSET_AMD;\n\t}\n}\n\nstatic int __init resctrl_late_init(void)\n{\n\tstruct rdt_resource *r;\n\tint state, ret;\n\n\t \n\trdt_init_res_defs();\n\n\tcheck_quirks();\n\n\tif (!get_rdt_resources())\n\t\treturn -ENODEV;\n\n\trdt_init_padding();\n\n\tstate = cpuhp_setup_state(CPUHP_AP_ONLINE_DYN,\n\t\t\t\t  \"x86/resctrl/cat:online:\",\n\t\t\t\t  resctrl_online_cpu, resctrl_offline_cpu);\n\tif (state < 0)\n\t\treturn state;\n\n\tret = rdtgroup_init();\n\tif (ret) {\n\t\tcpuhp_remove_state(state);\n\t\treturn ret;\n\t}\n\trdt_online = state;\n\n\tfor_each_alloc_capable_rdt_resource(r)\n\t\tpr_info(\"%s allocation detected\\n\", r->name);\n\n\tfor_each_mon_capable_rdt_resource(r)\n\t\tpr_info(\"%s monitoring detected\\n\", r->name);\n\n\treturn 0;\n}\n\nlate_initcall(resctrl_late_init);\n\nstatic void __exit resctrl_exit(void)\n{\n\tcpuhp_remove_state(rdt_online);\n\trdtgroup_exit();\n}\n\n__exitcall(resctrl_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}