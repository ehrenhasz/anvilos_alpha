{
  "module_name": "rdtgroup.c",
  "hash_id": "95e4c6b5e6885470a61a14b9ca51b600283c9a8efe69c4ecfc6f99386d30de10",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/cpu/resctrl/rdtgroup.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)\tKBUILD_MODNAME \": \" fmt\n\n#include <linux/cacheinfo.h>\n#include <linux/cpu.h>\n#include <linux/debugfs.h>\n#include <linux/fs.h>\n#include <linux/fs_parser.h>\n#include <linux/sysfs.h>\n#include <linux/kernfs.h>\n#include <linux/seq_buf.h>\n#include <linux/seq_file.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/task.h>\n#include <linux/slab.h>\n#include <linux/task_work.h>\n#include <linux/user_namespace.h>\n\n#include <uapi/linux/magic.h>\n\n#include <asm/resctrl.h>\n#include \"internal.h\"\n\nDEFINE_STATIC_KEY_FALSE(rdt_enable_key);\nDEFINE_STATIC_KEY_FALSE(rdt_mon_enable_key);\nDEFINE_STATIC_KEY_FALSE(rdt_alloc_enable_key);\nstatic struct kernfs_root *rdt_root;\nstruct rdtgroup rdtgroup_default;\nLIST_HEAD(rdt_all_groups);\n\n \nLIST_HEAD(resctrl_schema_all);\n\n \nstatic struct kernfs_node *kn_info;\n\n \nstatic struct kernfs_node *kn_mongrp;\n\n \nstatic struct kernfs_node *kn_mondata;\n\nstatic struct seq_buf last_cmd_status;\nstatic char last_cmd_status_buf[512];\n\nstruct dentry *debugfs_resctrl;\n\nvoid rdt_last_cmd_clear(void)\n{\n\tlockdep_assert_held(&rdtgroup_mutex);\n\tseq_buf_clear(&last_cmd_status);\n}\n\nvoid rdt_last_cmd_puts(const char *s)\n{\n\tlockdep_assert_held(&rdtgroup_mutex);\n\tseq_buf_puts(&last_cmd_status, s);\n}\n\nvoid rdt_last_cmd_printf(const char *fmt, ...)\n{\n\tva_list ap;\n\n\tva_start(ap, fmt);\n\tlockdep_assert_held(&rdtgroup_mutex);\n\tseq_buf_vprintf(&last_cmd_status, fmt, ap);\n\tva_end(ap);\n}\n\nvoid rdt_staged_configs_clear(void)\n{\n\tstruct rdt_resource *r;\n\tstruct rdt_domain *dom;\n\n\tlockdep_assert_held(&rdtgroup_mutex);\n\n\tfor_each_alloc_capable_rdt_resource(r) {\n\t\tlist_for_each_entry(dom, &r->domains, list)\n\t\t\tmemset(dom->staged_config, 0, sizeof(dom->staged_config));\n\t}\n}\n\n \nstatic int closid_free_map;\nstatic int closid_free_map_len;\n\nint closids_supported(void)\n{\n\treturn closid_free_map_len;\n}\n\nstatic void closid_init(void)\n{\n\tstruct resctrl_schema *s;\n\tu32 rdt_min_closid = 32;\n\n\t \n\tlist_for_each_entry(s, &resctrl_schema_all, list)\n\t\trdt_min_closid = min(rdt_min_closid, s->num_closid);\n\n\tclosid_free_map = BIT_MASK(rdt_min_closid) - 1;\n\n\t \n\tclosid_free_map &= ~1;\n\tclosid_free_map_len = rdt_min_closid;\n}\n\nstatic int closid_alloc(void)\n{\n\tu32 closid = ffs(closid_free_map);\n\n\tif (closid == 0)\n\t\treturn -ENOSPC;\n\tclosid--;\n\tclosid_free_map &= ~(1 << closid);\n\n\treturn closid;\n}\n\nvoid closid_free(int closid)\n{\n\tclosid_free_map |= 1 << closid;\n}\n\n \nstatic bool closid_allocated(unsigned int closid)\n{\n\treturn (closid_free_map & (1 << closid)) == 0;\n}\n\n \nenum rdtgrp_mode rdtgroup_mode_by_closid(int closid)\n{\n\tstruct rdtgroup *rdtgrp;\n\n\tlist_for_each_entry(rdtgrp, &rdt_all_groups, rdtgroup_list) {\n\t\tif (rdtgrp->closid == closid)\n\t\t\treturn rdtgrp->mode;\n\t}\n\n\treturn RDT_NUM_MODES;\n}\n\nstatic const char * const rdt_mode_str[] = {\n\t[RDT_MODE_SHAREABLE]\t\t= \"shareable\",\n\t[RDT_MODE_EXCLUSIVE]\t\t= \"exclusive\",\n\t[RDT_MODE_PSEUDO_LOCKSETUP]\t= \"pseudo-locksetup\",\n\t[RDT_MODE_PSEUDO_LOCKED]\t= \"pseudo-locked\",\n};\n\n \nstatic const char *rdtgroup_mode_str(enum rdtgrp_mode mode)\n{\n\tif (mode < RDT_MODE_SHAREABLE || mode >= RDT_NUM_MODES)\n\t\treturn \"unknown\";\n\n\treturn rdt_mode_str[mode];\n}\n\n \nstatic int rdtgroup_kn_set_ugid(struct kernfs_node *kn)\n{\n\tstruct iattr iattr = { .ia_valid = ATTR_UID | ATTR_GID,\n\t\t\t\t.ia_uid = current_fsuid(),\n\t\t\t\t.ia_gid = current_fsgid(), };\n\n\tif (uid_eq(iattr.ia_uid, GLOBAL_ROOT_UID) &&\n\t    gid_eq(iattr.ia_gid, GLOBAL_ROOT_GID))\n\t\treturn 0;\n\n\treturn kernfs_setattr(kn, &iattr);\n}\n\nstatic int rdtgroup_add_file(struct kernfs_node *parent_kn, struct rftype *rft)\n{\n\tstruct kernfs_node *kn;\n\tint ret;\n\n\tkn = __kernfs_create_file(parent_kn, rft->name, rft->mode,\n\t\t\t\t  GLOBAL_ROOT_UID, GLOBAL_ROOT_GID,\n\t\t\t\t  0, rft->kf_ops, rft, NULL, NULL);\n\tif (IS_ERR(kn))\n\t\treturn PTR_ERR(kn);\n\n\tret = rdtgroup_kn_set_ugid(kn);\n\tif (ret) {\n\t\tkernfs_remove(kn);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int rdtgroup_seqfile_show(struct seq_file *m, void *arg)\n{\n\tstruct kernfs_open_file *of = m->private;\n\tstruct rftype *rft = of->kn->priv;\n\n\tif (rft->seq_show)\n\t\treturn rft->seq_show(of, m, arg);\n\treturn 0;\n}\n\nstatic ssize_t rdtgroup_file_write(struct kernfs_open_file *of, char *buf,\n\t\t\t\t   size_t nbytes, loff_t off)\n{\n\tstruct rftype *rft = of->kn->priv;\n\n\tif (rft->write)\n\t\treturn rft->write(of, buf, nbytes, off);\n\n\treturn -EINVAL;\n}\n\nstatic const struct kernfs_ops rdtgroup_kf_single_ops = {\n\t.atomic_write_len\t= PAGE_SIZE,\n\t.write\t\t\t= rdtgroup_file_write,\n\t.seq_show\t\t= rdtgroup_seqfile_show,\n};\n\nstatic const struct kernfs_ops kf_mondata_ops = {\n\t.atomic_write_len\t= PAGE_SIZE,\n\t.seq_show\t\t= rdtgroup_mondata_show,\n};\n\nstatic bool is_cpu_list(struct kernfs_open_file *of)\n{\n\tstruct rftype *rft = of->kn->priv;\n\n\treturn rft->flags & RFTYPE_FLAGS_CPUS_LIST;\n}\n\nstatic int rdtgroup_cpus_show(struct kernfs_open_file *of,\n\t\t\t      struct seq_file *s, void *v)\n{\n\tstruct rdtgroup *rdtgrp;\n\tstruct cpumask *mask;\n\tint ret = 0;\n\n\trdtgrp = rdtgroup_kn_lock_live(of->kn);\n\n\tif (rdtgrp) {\n\t\tif (rdtgrp->mode == RDT_MODE_PSEUDO_LOCKED) {\n\t\t\tif (!rdtgrp->plr->d) {\n\t\t\t\trdt_last_cmd_clear();\n\t\t\t\trdt_last_cmd_puts(\"Cache domain offline\\n\");\n\t\t\t\tret = -ENODEV;\n\t\t\t} else {\n\t\t\t\tmask = &rdtgrp->plr->d->cpu_mask;\n\t\t\t\tseq_printf(s, is_cpu_list(of) ?\n\t\t\t\t\t   \"%*pbl\\n\" : \"%*pb\\n\",\n\t\t\t\t\t   cpumask_pr_args(mask));\n\t\t\t}\n\t\t} else {\n\t\t\tseq_printf(s, is_cpu_list(of) ? \"%*pbl\\n\" : \"%*pb\\n\",\n\t\t\t\t   cpumask_pr_args(&rdtgrp->cpu_mask));\n\t\t}\n\t} else {\n\t\tret = -ENOENT;\n\t}\n\trdtgroup_kn_unlock(of->kn);\n\n\treturn ret;\n}\n\n \nstatic void update_cpu_closid_rmid(void *info)\n{\n\tstruct rdtgroup *r = info;\n\n\tif (r) {\n\t\tthis_cpu_write(pqr_state.default_closid, r->closid);\n\t\tthis_cpu_write(pqr_state.default_rmid, r->mon.rmid);\n\t}\n\n\t \n\tresctrl_sched_in(current);\n}\n\n \nstatic void\nupdate_closid_rmid(const struct cpumask *cpu_mask, struct rdtgroup *r)\n{\n\ton_each_cpu_mask(cpu_mask, update_cpu_closid_rmid, r, 1);\n}\n\nstatic int cpus_mon_write(struct rdtgroup *rdtgrp, cpumask_var_t newmask,\n\t\t\t  cpumask_var_t tmpmask)\n{\n\tstruct rdtgroup *prgrp = rdtgrp->mon.parent, *crgrp;\n\tstruct list_head *head;\n\n\t \n\tcpumask_andnot(tmpmask, newmask, &prgrp->cpu_mask);\n\tif (!cpumask_empty(tmpmask)) {\n\t\trdt_last_cmd_puts(\"Can only add CPUs to mongroup that belong to parent\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tcpumask_andnot(tmpmask, &rdtgrp->cpu_mask, newmask);\n\tif (!cpumask_empty(tmpmask)) {\n\t\t \n\t\tcpumask_or(&prgrp->cpu_mask, &prgrp->cpu_mask, tmpmask);\n\t\tupdate_closid_rmid(tmpmask, prgrp);\n\t}\n\n\t \n\tcpumask_andnot(tmpmask, newmask, &rdtgrp->cpu_mask);\n\tif (!cpumask_empty(tmpmask)) {\n\t\thead = &prgrp->mon.crdtgrp_list;\n\t\tlist_for_each_entry(crgrp, head, mon.crdtgrp_list) {\n\t\t\tif (crgrp == rdtgrp)\n\t\t\t\tcontinue;\n\t\t\tcpumask_andnot(&crgrp->cpu_mask, &crgrp->cpu_mask,\n\t\t\t\t       tmpmask);\n\t\t}\n\t\tupdate_closid_rmid(tmpmask, rdtgrp);\n\t}\n\n\t \n\tcpumask_copy(&rdtgrp->cpu_mask, newmask);\n\n\treturn 0;\n}\n\nstatic void cpumask_rdtgrp_clear(struct rdtgroup *r, struct cpumask *m)\n{\n\tstruct rdtgroup *crgrp;\n\n\tcpumask_andnot(&r->cpu_mask, &r->cpu_mask, m);\n\t \n\tlist_for_each_entry(crgrp, &r->mon.crdtgrp_list, mon.crdtgrp_list)\n\t\tcpumask_and(&crgrp->cpu_mask, &r->cpu_mask, &crgrp->cpu_mask);\n}\n\nstatic int cpus_ctrl_write(struct rdtgroup *rdtgrp, cpumask_var_t newmask,\n\t\t\t   cpumask_var_t tmpmask, cpumask_var_t tmpmask1)\n{\n\tstruct rdtgroup *r, *crgrp;\n\tstruct list_head *head;\n\n\t \n\tcpumask_andnot(tmpmask, &rdtgrp->cpu_mask, newmask);\n\tif (!cpumask_empty(tmpmask)) {\n\t\t \n\t\tif (rdtgrp == &rdtgroup_default) {\n\t\t\trdt_last_cmd_puts(\"Can't drop CPUs from default group\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\t \n\t\tcpumask_or(&rdtgroup_default.cpu_mask,\n\t\t\t   &rdtgroup_default.cpu_mask, tmpmask);\n\t\tupdate_closid_rmid(tmpmask, &rdtgroup_default);\n\t}\n\n\t \n\tcpumask_andnot(tmpmask, newmask, &rdtgrp->cpu_mask);\n\tif (!cpumask_empty(tmpmask)) {\n\t\tlist_for_each_entry(r, &rdt_all_groups, rdtgroup_list) {\n\t\t\tif (r == rdtgrp)\n\t\t\t\tcontinue;\n\t\t\tcpumask_and(tmpmask1, &r->cpu_mask, tmpmask);\n\t\t\tif (!cpumask_empty(tmpmask1))\n\t\t\t\tcpumask_rdtgrp_clear(r, tmpmask1);\n\t\t}\n\t\tupdate_closid_rmid(tmpmask, rdtgrp);\n\t}\n\n\t \n\tcpumask_copy(&rdtgrp->cpu_mask, newmask);\n\n\t \n\thead = &rdtgrp->mon.crdtgrp_list;\n\tlist_for_each_entry(crgrp, head, mon.crdtgrp_list) {\n\t\tcpumask_and(tmpmask, &rdtgrp->cpu_mask, &crgrp->cpu_mask);\n\t\tupdate_closid_rmid(tmpmask, rdtgrp);\n\t\tcpumask_clear(&crgrp->cpu_mask);\n\t}\n\n\treturn 0;\n}\n\nstatic ssize_t rdtgroup_cpus_write(struct kernfs_open_file *of,\n\t\t\t\t   char *buf, size_t nbytes, loff_t off)\n{\n\tcpumask_var_t tmpmask, newmask, tmpmask1;\n\tstruct rdtgroup *rdtgrp;\n\tint ret;\n\n\tif (!buf)\n\t\treturn -EINVAL;\n\n\tif (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\tif (!zalloc_cpumask_var(&newmask, GFP_KERNEL)) {\n\t\tfree_cpumask_var(tmpmask);\n\t\treturn -ENOMEM;\n\t}\n\tif (!zalloc_cpumask_var(&tmpmask1, GFP_KERNEL)) {\n\t\tfree_cpumask_var(tmpmask);\n\t\tfree_cpumask_var(newmask);\n\t\treturn -ENOMEM;\n\t}\n\n\trdtgrp = rdtgroup_kn_lock_live(of->kn);\n\tif (!rdtgrp) {\n\t\tret = -ENOENT;\n\t\tgoto unlock;\n\t}\n\n\tif (rdtgrp->mode == RDT_MODE_PSEUDO_LOCKED ||\n\t    rdtgrp->mode == RDT_MODE_PSEUDO_LOCKSETUP) {\n\t\tret = -EINVAL;\n\t\trdt_last_cmd_puts(\"Pseudo-locking in progress\\n\");\n\t\tgoto unlock;\n\t}\n\n\tif (is_cpu_list(of))\n\t\tret = cpulist_parse(buf, newmask);\n\telse\n\t\tret = cpumask_parse(buf, newmask);\n\n\tif (ret) {\n\t\trdt_last_cmd_puts(\"Bad CPU list/mask\\n\");\n\t\tgoto unlock;\n\t}\n\n\t \n\tcpumask_andnot(tmpmask, newmask, cpu_online_mask);\n\tif (!cpumask_empty(tmpmask)) {\n\t\tret = -EINVAL;\n\t\trdt_last_cmd_puts(\"Can only assign online CPUs\\n\");\n\t\tgoto unlock;\n\t}\n\n\tif (rdtgrp->type == RDTCTRL_GROUP)\n\t\tret = cpus_ctrl_write(rdtgrp, newmask, tmpmask, tmpmask1);\n\telse if (rdtgrp->type == RDTMON_GROUP)\n\t\tret = cpus_mon_write(rdtgrp, newmask, tmpmask);\n\telse\n\t\tret = -EINVAL;\n\nunlock:\n\trdtgroup_kn_unlock(of->kn);\n\tfree_cpumask_var(tmpmask);\n\tfree_cpumask_var(newmask);\n\tfree_cpumask_var(tmpmask1);\n\n\treturn ret ?: nbytes;\n}\n\n \nstatic void rdtgroup_remove(struct rdtgroup *rdtgrp)\n{\n\tkernfs_put(rdtgrp->kn);\n\tkfree(rdtgrp);\n}\n\nstatic void _update_task_closid_rmid(void *task)\n{\n\t \n\tif (task == current)\n\t\tresctrl_sched_in(task);\n}\n\nstatic void update_task_closid_rmid(struct task_struct *t)\n{\n\tif (IS_ENABLED(CONFIG_SMP) && task_curr(t))\n\t\tsmp_call_function_single(task_cpu(t), _update_task_closid_rmid, t, 1);\n\telse\n\t\t_update_task_closid_rmid(t);\n}\n\nstatic int __rdtgroup_move_task(struct task_struct *tsk,\n\t\t\t\tstruct rdtgroup *rdtgrp)\n{\n\t \n\tif ((rdtgrp->type == RDTCTRL_GROUP && tsk->closid == rdtgrp->closid &&\n\t     tsk->rmid == rdtgrp->mon.rmid) ||\n\t    (rdtgrp->type == RDTMON_GROUP && tsk->rmid == rdtgrp->mon.rmid &&\n\t     tsk->closid == rdtgrp->mon.parent->closid))\n\t\treturn 0;\n\n\t \n\n\tif (rdtgrp->type == RDTCTRL_GROUP) {\n\t\tWRITE_ONCE(tsk->closid, rdtgrp->closid);\n\t\tWRITE_ONCE(tsk->rmid, rdtgrp->mon.rmid);\n\t} else if (rdtgrp->type == RDTMON_GROUP) {\n\t\tif (rdtgrp->mon.parent->closid == tsk->closid) {\n\t\t\tWRITE_ONCE(tsk->rmid, rdtgrp->mon.rmid);\n\t\t} else {\n\t\t\trdt_last_cmd_puts(\"Can't move task to different control group\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\t \n\tsmp_mb();\n\n\t \n\tupdate_task_closid_rmid(tsk);\n\n\treturn 0;\n}\n\nstatic bool is_closid_match(struct task_struct *t, struct rdtgroup *r)\n{\n\treturn (rdt_alloc_capable &&\n\t       (r->type == RDTCTRL_GROUP) && (t->closid == r->closid));\n}\n\nstatic bool is_rmid_match(struct task_struct *t, struct rdtgroup *r)\n{\n\treturn (rdt_mon_capable &&\n\t       (r->type == RDTMON_GROUP) && (t->rmid == r->mon.rmid));\n}\n\n \nint rdtgroup_tasks_assigned(struct rdtgroup *r)\n{\n\tstruct task_struct *p, *t;\n\tint ret = 0;\n\n\tlockdep_assert_held(&rdtgroup_mutex);\n\n\trcu_read_lock();\n\tfor_each_process_thread(p, t) {\n\t\tif (is_closid_match(t, r) || is_rmid_match(t, r)) {\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t}\n\trcu_read_unlock();\n\n\treturn ret;\n}\n\nstatic int rdtgroup_task_write_permission(struct task_struct *task,\n\t\t\t\t\t  struct kernfs_open_file *of)\n{\n\tconst struct cred *tcred = get_task_cred(task);\n\tconst struct cred *cred = current_cred();\n\tint ret = 0;\n\n\t \n\tif (!uid_eq(cred->euid, GLOBAL_ROOT_UID) &&\n\t    !uid_eq(cred->euid, tcred->uid) &&\n\t    !uid_eq(cred->euid, tcred->suid)) {\n\t\trdt_last_cmd_printf(\"No permission to move task %d\\n\", task->pid);\n\t\tret = -EPERM;\n\t}\n\n\tput_cred(tcred);\n\treturn ret;\n}\n\nstatic int rdtgroup_move_task(pid_t pid, struct rdtgroup *rdtgrp,\n\t\t\t      struct kernfs_open_file *of)\n{\n\tstruct task_struct *tsk;\n\tint ret;\n\n\trcu_read_lock();\n\tif (pid) {\n\t\ttsk = find_task_by_vpid(pid);\n\t\tif (!tsk) {\n\t\t\trcu_read_unlock();\n\t\t\trdt_last_cmd_printf(\"No task %d\\n\", pid);\n\t\t\treturn -ESRCH;\n\t\t}\n\t} else {\n\t\ttsk = current;\n\t}\n\n\tget_task_struct(tsk);\n\trcu_read_unlock();\n\n\tret = rdtgroup_task_write_permission(tsk, of);\n\tif (!ret)\n\t\tret = __rdtgroup_move_task(tsk, rdtgrp);\n\n\tput_task_struct(tsk);\n\treturn ret;\n}\n\nstatic ssize_t rdtgroup_tasks_write(struct kernfs_open_file *of,\n\t\t\t\t    char *buf, size_t nbytes, loff_t off)\n{\n\tstruct rdtgroup *rdtgrp;\n\tint ret = 0;\n\tpid_t pid;\n\n\tif (kstrtoint(strstrip(buf), 0, &pid) || pid < 0)\n\t\treturn -EINVAL;\n\trdtgrp = rdtgroup_kn_lock_live(of->kn);\n\tif (!rdtgrp) {\n\t\trdtgroup_kn_unlock(of->kn);\n\t\treturn -ENOENT;\n\t}\n\trdt_last_cmd_clear();\n\n\tif (rdtgrp->mode == RDT_MODE_PSEUDO_LOCKED ||\n\t    rdtgrp->mode == RDT_MODE_PSEUDO_LOCKSETUP) {\n\t\tret = -EINVAL;\n\t\trdt_last_cmd_puts(\"Pseudo-locking in progress\\n\");\n\t\tgoto unlock;\n\t}\n\n\tret = rdtgroup_move_task(pid, rdtgrp, of);\n\nunlock:\n\trdtgroup_kn_unlock(of->kn);\n\n\treturn ret ?: nbytes;\n}\n\nstatic void show_rdt_tasks(struct rdtgroup *r, struct seq_file *s)\n{\n\tstruct task_struct *p, *t;\n\tpid_t pid;\n\n\trcu_read_lock();\n\tfor_each_process_thread(p, t) {\n\t\tif (is_closid_match(t, r) || is_rmid_match(t, r)) {\n\t\t\tpid = task_pid_vnr(t);\n\t\t\tif (pid)\n\t\t\t\tseq_printf(s, \"%d\\n\", pid);\n\t\t}\n\t}\n\trcu_read_unlock();\n}\n\nstatic int rdtgroup_tasks_show(struct kernfs_open_file *of,\n\t\t\t       struct seq_file *s, void *v)\n{\n\tstruct rdtgroup *rdtgrp;\n\tint ret = 0;\n\n\trdtgrp = rdtgroup_kn_lock_live(of->kn);\n\tif (rdtgrp)\n\t\tshow_rdt_tasks(rdtgrp, s);\n\telse\n\t\tret = -ENOENT;\n\trdtgroup_kn_unlock(of->kn);\n\n\treturn ret;\n}\n\n#ifdef CONFIG_PROC_CPU_RESCTRL\n\n \nint proc_resctrl_show(struct seq_file *s, struct pid_namespace *ns,\n\t\t      struct pid *pid, struct task_struct *tsk)\n{\n\tstruct rdtgroup *rdtg;\n\tint ret = 0;\n\n\tmutex_lock(&rdtgroup_mutex);\n\n\t \n\tif (!static_branch_unlikely(&rdt_enable_key)) {\n\t\tseq_puts(s, \"res:\\nmon:\\n\");\n\t\tgoto unlock;\n\t}\n\n\tlist_for_each_entry(rdtg, &rdt_all_groups, rdtgroup_list) {\n\t\tstruct rdtgroup *crg;\n\n\t\t \n\t\tif (rdtg->mode != RDT_MODE_SHAREABLE &&\n\t\t    rdtg->mode != RDT_MODE_EXCLUSIVE)\n\t\t\tcontinue;\n\n\t\tif (rdtg->closid != tsk->closid)\n\t\t\tcontinue;\n\n\t\tseq_printf(s, \"res:%s%s\\n\", (rdtg == &rdtgroup_default) ? \"/\" : \"\",\n\t\t\t   rdtg->kn->name);\n\t\tseq_puts(s, \"mon:\");\n\t\tlist_for_each_entry(crg, &rdtg->mon.crdtgrp_list,\n\t\t\t\t    mon.crdtgrp_list) {\n\t\t\tif (tsk->rmid != crg->mon.rmid)\n\t\t\t\tcontinue;\n\t\t\tseq_printf(s, \"%s\", crg->kn->name);\n\t\t\tbreak;\n\t\t}\n\t\tseq_putc(s, '\\n');\n\t\tgoto unlock;\n\t}\n\t \n\tret = -ENOENT;\nunlock:\n\tmutex_unlock(&rdtgroup_mutex);\n\n\treturn ret;\n}\n#endif\n\nstatic int rdt_last_cmd_status_show(struct kernfs_open_file *of,\n\t\t\t\t    struct seq_file *seq, void *v)\n{\n\tint len;\n\n\tmutex_lock(&rdtgroup_mutex);\n\tlen = seq_buf_used(&last_cmd_status);\n\tif (len)\n\t\tseq_printf(seq, \"%.*s\", len, last_cmd_status_buf);\n\telse\n\t\tseq_puts(seq, \"ok\\n\");\n\tmutex_unlock(&rdtgroup_mutex);\n\treturn 0;\n}\n\nstatic int rdt_num_closids_show(struct kernfs_open_file *of,\n\t\t\t\tstruct seq_file *seq, void *v)\n{\n\tstruct resctrl_schema *s = of->kn->parent->priv;\n\n\tseq_printf(seq, \"%u\\n\", s->num_closid);\n\treturn 0;\n}\n\nstatic int rdt_default_ctrl_show(struct kernfs_open_file *of,\n\t\t\t     struct seq_file *seq, void *v)\n{\n\tstruct resctrl_schema *s = of->kn->parent->priv;\n\tstruct rdt_resource *r = s->res;\n\n\tseq_printf(seq, \"%x\\n\", r->default_ctrl);\n\treturn 0;\n}\n\nstatic int rdt_min_cbm_bits_show(struct kernfs_open_file *of,\n\t\t\t     struct seq_file *seq, void *v)\n{\n\tstruct resctrl_schema *s = of->kn->parent->priv;\n\tstruct rdt_resource *r = s->res;\n\n\tseq_printf(seq, \"%u\\n\", r->cache.min_cbm_bits);\n\treturn 0;\n}\n\nstatic int rdt_shareable_bits_show(struct kernfs_open_file *of,\n\t\t\t\t   struct seq_file *seq, void *v)\n{\n\tstruct resctrl_schema *s = of->kn->parent->priv;\n\tstruct rdt_resource *r = s->res;\n\n\tseq_printf(seq, \"%x\\n\", r->cache.shareable_bits);\n\treturn 0;\n}\n\n \nstatic int rdt_bit_usage_show(struct kernfs_open_file *of,\n\t\t\t      struct seq_file *seq, void *v)\n{\n\tstruct resctrl_schema *s = of->kn->parent->priv;\n\t \n\tunsigned long sw_shareable = 0, hw_shareable = 0;\n\tunsigned long exclusive = 0, pseudo_locked = 0;\n\tstruct rdt_resource *r = s->res;\n\tstruct rdt_domain *dom;\n\tint i, hwb, swb, excl, psl;\n\tenum rdtgrp_mode mode;\n\tbool sep = false;\n\tu32 ctrl_val;\n\n\tmutex_lock(&rdtgroup_mutex);\n\thw_shareable = r->cache.shareable_bits;\n\tlist_for_each_entry(dom, &r->domains, list) {\n\t\tif (sep)\n\t\t\tseq_putc(seq, ';');\n\t\tsw_shareable = 0;\n\t\texclusive = 0;\n\t\tseq_printf(seq, \"%d=\", dom->id);\n\t\tfor (i = 0; i < closids_supported(); i++) {\n\t\t\tif (!closid_allocated(i))\n\t\t\t\tcontinue;\n\t\t\tctrl_val = resctrl_arch_get_config(r, dom, i,\n\t\t\t\t\t\t\t   s->conf_type);\n\t\t\tmode = rdtgroup_mode_by_closid(i);\n\t\t\tswitch (mode) {\n\t\t\tcase RDT_MODE_SHAREABLE:\n\t\t\t\tsw_shareable |= ctrl_val;\n\t\t\t\tbreak;\n\t\t\tcase RDT_MODE_EXCLUSIVE:\n\t\t\t\texclusive |= ctrl_val;\n\t\t\t\tbreak;\n\t\t\tcase RDT_MODE_PSEUDO_LOCKSETUP:\n\t\t\t \n\t\t\t\tbreak;\n\t\t\tcase RDT_MODE_PSEUDO_LOCKED:\n\t\t\tcase RDT_NUM_MODES:\n\t\t\t\tWARN(1,\n\t\t\t\t     \"invalid mode for closid %d\\n\", i);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tfor (i = r->cache.cbm_len - 1; i >= 0; i--) {\n\t\t\tpseudo_locked = dom->plr ? dom->plr->cbm : 0;\n\t\t\thwb = test_bit(i, &hw_shareable);\n\t\t\tswb = test_bit(i, &sw_shareable);\n\t\t\texcl = test_bit(i, &exclusive);\n\t\t\tpsl = test_bit(i, &pseudo_locked);\n\t\t\tif (hwb && swb)\n\t\t\t\tseq_putc(seq, 'X');\n\t\t\telse if (hwb && !swb)\n\t\t\t\tseq_putc(seq, 'H');\n\t\t\telse if (!hwb && swb)\n\t\t\t\tseq_putc(seq, 'S');\n\t\t\telse if (excl)\n\t\t\t\tseq_putc(seq, 'E');\n\t\t\telse if (psl)\n\t\t\t\tseq_putc(seq, 'P');\n\t\t\telse  \n\t\t\t\tseq_putc(seq, '0');\n\t\t}\n\t\tsep = true;\n\t}\n\tseq_putc(seq, '\\n');\n\tmutex_unlock(&rdtgroup_mutex);\n\treturn 0;\n}\n\nstatic int rdt_min_bw_show(struct kernfs_open_file *of,\n\t\t\t     struct seq_file *seq, void *v)\n{\n\tstruct resctrl_schema *s = of->kn->parent->priv;\n\tstruct rdt_resource *r = s->res;\n\n\tseq_printf(seq, \"%u\\n\", r->membw.min_bw);\n\treturn 0;\n}\n\nstatic int rdt_num_rmids_show(struct kernfs_open_file *of,\n\t\t\t      struct seq_file *seq, void *v)\n{\n\tstruct rdt_resource *r = of->kn->parent->priv;\n\n\tseq_printf(seq, \"%d\\n\", r->num_rmid);\n\n\treturn 0;\n}\n\nstatic int rdt_mon_features_show(struct kernfs_open_file *of,\n\t\t\t\t struct seq_file *seq, void *v)\n{\n\tstruct rdt_resource *r = of->kn->parent->priv;\n\tstruct mon_evt *mevt;\n\n\tlist_for_each_entry(mevt, &r->evt_list, list) {\n\t\tseq_printf(seq, \"%s\\n\", mevt->name);\n\t\tif (mevt->configurable)\n\t\t\tseq_printf(seq, \"%s_config\\n\", mevt->name);\n\t}\n\n\treturn 0;\n}\n\nstatic int rdt_bw_gran_show(struct kernfs_open_file *of,\n\t\t\t     struct seq_file *seq, void *v)\n{\n\tstruct resctrl_schema *s = of->kn->parent->priv;\n\tstruct rdt_resource *r = s->res;\n\n\tseq_printf(seq, \"%u\\n\", r->membw.bw_gran);\n\treturn 0;\n}\n\nstatic int rdt_delay_linear_show(struct kernfs_open_file *of,\n\t\t\t     struct seq_file *seq, void *v)\n{\n\tstruct resctrl_schema *s = of->kn->parent->priv;\n\tstruct rdt_resource *r = s->res;\n\n\tseq_printf(seq, \"%u\\n\", r->membw.delay_linear);\n\treturn 0;\n}\n\nstatic int max_threshold_occ_show(struct kernfs_open_file *of,\n\t\t\t\t  struct seq_file *seq, void *v)\n{\n\tseq_printf(seq, \"%u\\n\", resctrl_rmid_realloc_threshold);\n\n\treturn 0;\n}\n\nstatic int rdt_thread_throttle_mode_show(struct kernfs_open_file *of,\n\t\t\t\t\t struct seq_file *seq, void *v)\n{\n\tstruct resctrl_schema *s = of->kn->parent->priv;\n\tstruct rdt_resource *r = s->res;\n\n\tif (r->membw.throttle_mode == THREAD_THROTTLE_PER_THREAD)\n\t\tseq_puts(seq, \"per-thread\\n\");\n\telse\n\t\tseq_puts(seq, \"max\\n\");\n\n\treturn 0;\n}\n\nstatic ssize_t max_threshold_occ_write(struct kernfs_open_file *of,\n\t\t\t\t       char *buf, size_t nbytes, loff_t off)\n{\n\tunsigned int bytes;\n\tint ret;\n\n\tret = kstrtouint(buf, 0, &bytes);\n\tif (ret)\n\t\treturn ret;\n\n\tif (bytes > resctrl_rmid_realloc_limit)\n\t\treturn -EINVAL;\n\n\tresctrl_rmid_realloc_threshold = resctrl_arch_round_mon_val(bytes);\n\n\treturn nbytes;\n}\n\n \nstatic int rdtgroup_mode_show(struct kernfs_open_file *of,\n\t\t\t      struct seq_file *s, void *v)\n{\n\tstruct rdtgroup *rdtgrp;\n\n\trdtgrp = rdtgroup_kn_lock_live(of->kn);\n\tif (!rdtgrp) {\n\t\trdtgroup_kn_unlock(of->kn);\n\t\treturn -ENOENT;\n\t}\n\n\tseq_printf(s, \"%s\\n\", rdtgroup_mode_str(rdtgrp->mode));\n\n\trdtgroup_kn_unlock(of->kn);\n\treturn 0;\n}\n\nstatic enum resctrl_conf_type resctrl_peer_type(enum resctrl_conf_type my_type)\n{\n\tswitch (my_type) {\n\tcase CDP_CODE:\n\t\treturn CDP_DATA;\n\tcase CDP_DATA:\n\t\treturn CDP_CODE;\n\tdefault:\n\tcase CDP_NONE:\n\t\treturn CDP_NONE;\n\t}\n}\n\n \nstatic bool __rdtgroup_cbm_overlaps(struct rdt_resource *r, struct rdt_domain *d,\n\t\t\t\t    unsigned long cbm, int closid,\n\t\t\t\t    enum resctrl_conf_type type, bool exclusive)\n{\n\tenum rdtgrp_mode mode;\n\tunsigned long ctrl_b;\n\tint i;\n\n\t \n\tif (!exclusive) {\n\t\tctrl_b = r->cache.shareable_bits;\n\t\tif (bitmap_intersects(&cbm, &ctrl_b, r->cache.cbm_len))\n\t\t\treturn true;\n\t}\n\n\t \n\tfor (i = 0; i < closids_supported(); i++) {\n\t\tctrl_b = resctrl_arch_get_config(r, d, i, type);\n\t\tmode = rdtgroup_mode_by_closid(i);\n\t\tif (closid_allocated(i) && i != closid &&\n\t\t    mode != RDT_MODE_PSEUDO_LOCKSETUP) {\n\t\t\tif (bitmap_intersects(&cbm, &ctrl_b, r->cache.cbm_len)) {\n\t\t\t\tif (exclusive) {\n\t\t\t\t\tif (mode == RDT_MODE_EXCLUSIVE)\n\t\t\t\t\t\treturn true;\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn false;\n}\n\n \nbool rdtgroup_cbm_overlaps(struct resctrl_schema *s, struct rdt_domain *d,\n\t\t\t   unsigned long cbm, int closid, bool exclusive)\n{\n\tenum resctrl_conf_type peer_type = resctrl_peer_type(s->conf_type);\n\tstruct rdt_resource *r = s->res;\n\n\tif (__rdtgroup_cbm_overlaps(r, d, cbm, closid, s->conf_type,\n\t\t\t\t    exclusive))\n\t\treturn true;\n\n\tif (!resctrl_arch_get_cdp_enabled(r->rid))\n\t\treturn false;\n\treturn  __rdtgroup_cbm_overlaps(r, d, cbm, closid, peer_type, exclusive);\n}\n\n \nstatic bool rdtgroup_mode_test_exclusive(struct rdtgroup *rdtgrp)\n{\n\tint closid = rdtgrp->closid;\n\tstruct resctrl_schema *s;\n\tstruct rdt_resource *r;\n\tbool has_cache = false;\n\tstruct rdt_domain *d;\n\tu32 ctrl;\n\n\tlist_for_each_entry(s, &resctrl_schema_all, list) {\n\t\tr = s->res;\n\t\tif (r->rid == RDT_RESOURCE_MBA || r->rid == RDT_RESOURCE_SMBA)\n\t\t\tcontinue;\n\t\thas_cache = true;\n\t\tlist_for_each_entry(d, &r->domains, list) {\n\t\t\tctrl = resctrl_arch_get_config(r, d, closid,\n\t\t\t\t\t\t       s->conf_type);\n\t\t\tif (rdtgroup_cbm_overlaps(s, d, ctrl, closid, false)) {\n\t\t\t\trdt_last_cmd_puts(\"Schemata overlaps\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (!has_cache) {\n\t\trdt_last_cmd_puts(\"Cannot be exclusive without CAT/CDP\\n\");\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nstatic ssize_t rdtgroup_mode_write(struct kernfs_open_file *of,\n\t\t\t\t   char *buf, size_t nbytes, loff_t off)\n{\n\tstruct rdtgroup *rdtgrp;\n\tenum rdtgrp_mode mode;\n\tint ret = 0;\n\n\t \n\tif (nbytes == 0 || buf[nbytes - 1] != '\\n')\n\t\treturn -EINVAL;\n\tbuf[nbytes - 1] = '\\0';\n\n\trdtgrp = rdtgroup_kn_lock_live(of->kn);\n\tif (!rdtgrp) {\n\t\trdtgroup_kn_unlock(of->kn);\n\t\treturn -ENOENT;\n\t}\n\n\trdt_last_cmd_clear();\n\n\tmode = rdtgrp->mode;\n\n\tif ((!strcmp(buf, \"shareable\") && mode == RDT_MODE_SHAREABLE) ||\n\t    (!strcmp(buf, \"exclusive\") && mode == RDT_MODE_EXCLUSIVE) ||\n\t    (!strcmp(buf, \"pseudo-locksetup\") &&\n\t     mode == RDT_MODE_PSEUDO_LOCKSETUP) ||\n\t    (!strcmp(buf, \"pseudo-locked\") && mode == RDT_MODE_PSEUDO_LOCKED))\n\t\tgoto out;\n\n\tif (mode == RDT_MODE_PSEUDO_LOCKED) {\n\t\trdt_last_cmd_puts(\"Cannot change pseudo-locked group\\n\");\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (!strcmp(buf, \"shareable\")) {\n\t\tif (rdtgrp->mode == RDT_MODE_PSEUDO_LOCKSETUP) {\n\t\t\tret = rdtgroup_locksetup_exit(rdtgrp);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t\trdtgrp->mode = RDT_MODE_SHAREABLE;\n\t} else if (!strcmp(buf, \"exclusive\")) {\n\t\tif (!rdtgroup_mode_test_exclusive(rdtgrp)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tif (rdtgrp->mode == RDT_MODE_PSEUDO_LOCKSETUP) {\n\t\t\tret = rdtgroup_locksetup_exit(rdtgrp);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\t\trdtgrp->mode = RDT_MODE_EXCLUSIVE;\n\t} else if (!strcmp(buf, \"pseudo-locksetup\")) {\n\t\tret = rdtgroup_locksetup_enter(rdtgrp);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\trdtgrp->mode = RDT_MODE_PSEUDO_LOCKSETUP;\n\t} else {\n\t\trdt_last_cmd_puts(\"Unknown or unsupported mode\\n\");\n\t\tret = -EINVAL;\n\t}\n\nout:\n\trdtgroup_kn_unlock(of->kn);\n\treturn ret ?: nbytes;\n}\n\n \nunsigned int rdtgroup_cbm_to_size(struct rdt_resource *r,\n\t\t\t\t  struct rdt_domain *d, unsigned long cbm)\n{\n\tstruct cpu_cacheinfo *ci;\n\tunsigned int size = 0;\n\tint num_b, i;\n\n\tnum_b = bitmap_weight(&cbm, r->cache.cbm_len);\n\tci = get_cpu_cacheinfo(cpumask_any(&d->cpu_mask));\n\tfor (i = 0; i < ci->num_leaves; i++) {\n\t\tif (ci->info_list[i].level == r->cache_level) {\n\t\t\tsize = ci->info_list[i].size / r->cache.cbm_len * num_b;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn size;\n}\n\n \nstatic int rdtgroup_size_show(struct kernfs_open_file *of,\n\t\t\t      struct seq_file *s, void *v)\n{\n\tstruct resctrl_schema *schema;\n\tenum resctrl_conf_type type;\n\tstruct rdtgroup *rdtgrp;\n\tstruct rdt_resource *r;\n\tstruct rdt_domain *d;\n\tunsigned int size;\n\tint ret = 0;\n\tu32 closid;\n\tbool sep;\n\tu32 ctrl;\n\n\trdtgrp = rdtgroup_kn_lock_live(of->kn);\n\tif (!rdtgrp) {\n\t\trdtgroup_kn_unlock(of->kn);\n\t\treturn -ENOENT;\n\t}\n\n\tif (rdtgrp->mode == RDT_MODE_PSEUDO_LOCKED) {\n\t\tif (!rdtgrp->plr->d) {\n\t\t\trdt_last_cmd_clear();\n\t\t\trdt_last_cmd_puts(\"Cache domain offline\\n\");\n\t\t\tret = -ENODEV;\n\t\t} else {\n\t\t\tseq_printf(s, \"%*s:\", max_name_width,\n\t\t\t\t   rdtgrp->plr->s->name);\n\t\t\tsize = rdtgroup_cbm_to_size(rdtgrp->plr->s->res,\n\t\t\t\t\t\t    rdtgrp->plr->d,\n\t\t\t\t\t\t    rdtgrp->plr->cbm);\n\t\t\tseq_printf(s, \"%d=%u\\n\", rdtgrp->plr->d->id, size);\n\t\t}\n\t\tgoto out;\n\t}\n\n\tclosid = rdtgrp->closid;\n\n\tlist_for_each_entry(schema, &resctrl_schema_all, list) {\n\t\tr = schema->res;\n\t\ttype = schema->conf_type;\n\t\tsep = false;\n\t\tseq_printf(s, \"%*s:\", max_name_width, schema->name);\n\t\tlist_for_each_entry(d, &r->domains, list) {\n\t\t\tif (sep)\n\t\t\t\tseq_putc(s, ';');\n\t\t\tif (rdtgrp->mode == RDT_MODE_PSEUDO_LOCKSETUP) {\n\t\t\t\tsize = 0;\n\t\t\t} else {\n\t\t\t\tif (is_mba_sc(r))\n\t\t\t\t\tctrl = d->mbps_val[closid];\n\t\t\t\telse\n\t\t\t\t\tctrl = resctrl_arch_get_config(r, d,\n\t\t\t\t\t\t\t\t       closid,\n\t\t\t\t\t\t\t\t       type);\n\t\t\t\tif (r->rid == RDT_RESOURCE_MBA ||\n\t\t\t\t    r->rid == RDT_RESOURCE_SMBA)\n\t\t\t\t\tsize = ctrl;\n\t\t\t\telse\n\t\t\t\t\tsize = rdtgroup_cbm_to_size(r, d, ctrl);\n\t\t\t}\n\t\t\tseq_printf(s, \"%d=%u\", d->id, size);\n\t\t\tsep = true;\n\t\t}\n\t\tseq_putc(s, '\\n');\n\t}\n\nout:\n\trdtgroup_kn_unlock(of->kn);\n\n\treturn ret;\n}\n\nstruct mon_config_info {\n\tu32 evtid;\n\tu32 mon_config;\n};\n\n#define INVALID_CONFIG_INDEX   UINT_MAX\n\n \nstatic inline unsigned int mon_event_config_index_get(u32 evtid)\n{\n\tswitch (evtid) {\n\tcase QOS_L3_MBM_TOTAL_EVENT_ID:\n\t\treturn 0;\n\tcase QOS_L3_MBM_LOCAL_EVENT_ID:\n\t\treturn 1;\n\tdefault:\n\t\t \n\t\treturn INVALID_CONFIG_INDEX;\n\t}\n}\n\nstatic void mon_event_config_read(void *info)\n{\n\tstruct mon_config_info *mon_info = info;\n\tunsigned int index;\n\tu64 msrval;\n\n\tindex = mon_event_config_index_get(mon_info->evtid);\n\tif (index == INVALID_CONFIG_INDEX) {\n\t\tpr_warn_once(\"Invalid event id %d\\n\", mon_info->evtid);\n\t\treturn;\n\t}\n\trdmsrl(MSR_IA32_EVT_CFG_BASE + index, msrval);\n\n\t \n\tmon_info->mon_config = msrval & MAX_EVT_CONFIG_BITS;\n}\n\nstatic void mondata_config_read(struct rdt_domain *d, struct mon_config_info *mon_info)\n{\n\tsmp_call_function_any(&d->cpu_mask, mon_event_config_read, mon_info, 1);\n}\n\nstatic int mbm_config_show(struct seq_file *s, struct rdt_resource *r, u32 evtid)\n{\n\tstruct mon_config_info mon_info = {0};\n\tstruct rdt_domain *dom;\n\tbool sep = false;\n\n\tmutex_lock(&rdtgroup_mutex);\n\n\tlist_for_each_entry(dom, &r->domains, list) {\n\t\tif (sep)\n\t\t\tseq_puts(s, \";\");\n\n\t\tmemset(&mon_info, 0, sizeof(struct mon_config_info));\n\t\tmon_info.evtid = evtid;\n\t\tmondata_config_read(dom, &mon_info);\n\n\t\tseq_printf(s, \"%d=0x%02x\", dom->id, mon_info.mon_config);\n\t\tsep = true;\n\t}\n\tseq_puts(s, \"\\n\");\n\n\tmutex_unlock(&rdtgroup_mutex);\n\n\treturn 0;\n}\n\nstatic int mbm_total_bytes_config_show(struct kernfs_open_file *of,\n\t\t\t\t       struct seq_file *seq, void *v)\n{\n\tstruct rdt_resource *r = of->kn->parent->priv;\n\n\tmbm_config_show(seq, r, QOS_L3_MBM_TOTAL_EVENT_ID);\n\n\treturn 0;\n}\n\nstatic int mbm_local_bytes_config_show(struct kernfs_open_file *of,\n\t\t\t\t       struct seq_file *seq, void *v)\n{\n\tstruct rdt_resource *r = of->kn->parent->priv;\n\n\tmbm_config_show(seq, r, QOS_L3_MBM_LOCAL_EVENT_ID);\n\n\treturn 0;\n}\n\nstatic void mon_event_config_write(void *info)\n{\n\tstruct mon_config_info *mon_info = info;\n\tunsigned int index;\n\n\tindex = mon_event_config_index_get(mon_info->evtid);\n\tif (index == INVALID_CONFIG_INDEX) {\n\t\tpr_warn_once(\"Invalid event id %d\\n\", mon_info->evtid);\n\t\treturn;\n\t}\n\twrmsr(MSR_IA32_EVT_CFG_BASE + index, mon_info->mon_config, 0);\n}\n\nstatic int mbm_config_write_domain(struct rdt_resource *r,\n\t\t\t\t   struct rdt_domain *d, u32 evtid, u32 val)\n{\n\tstruct mon_config_info mon_info = {0};\n\tint ret = 0;\n\n\t \n\tif (val > MAX_EVT_CONFIG_BITS) {\n\t\trdt_last_cmd_puts(\"Invalid event configuration\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tmon_info.evtid = evtid;\n\tmondata_config_read(d, &mon_info);\n\tif (mon_info.mon_config == val)\n\t\tgoto out;\n\n\tmon_info.mon_config = val;\n\n\t \n\tsmp_call_function_any(&d->cpu_mask, mon_event_config_write,\n\t\t\t      &mon_info, 1);\n\n\t \n\tresctrl_arch_reset_rmid_all(r, d);\n\nout:\n\treturn ret;\n}\n\nstatic int mon_config_write(struct rdt_resource *r, char *tok, u32 evtid)\n{\n\tchar *dom_str = NULL, *id_str;\n\tunsigned long dom_id, val;\n\tstruct rdt_domain *d;\n\tint ret = 0;\n\nnext:\n\tif (!tok || tok[0] == '\\0')\n\t\treturn 0;\n\n\t \n\tdom_str = strim(strsep(&tok, \";\"));\n\tid_str = strsep(&dom_str, \"=\");\n\n\tif (!id_str || kstrtoul(id_str, 10, &dom_id)) {\n\t\trdt_last_cmd_puts(\"Missing '=' or non-numeric domain id\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!dom_str || kstrtoul(dom_str, 16, &val)) {\n\t\trdt_last_cmd_puts(\"Non-numeric event configuration value\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tlist_for_each_entry(d, &r->domains, list) {\n\t\tif (d->id == dom_id) {\n\t\t\tret = mbm_config_write_domain(r, d, evtid, val);\n\t\t\tif (ret)\n\t\t\t\treturn -EINVAL;\n\t\t\tgoto next;\n\t\t}\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic ssize_t mbm_total_bytes_config_write(struct kernfs_open_file *of,\n\t\t\t\t\t    char *buf, size_t nbytes,\n\t\t\t\t\t    loff_t off)\n{\n\tstruct rdt_resource *r = of->kn->parent->priv;\n\tint ret;\n\n\t \n\tif (nbytes == 0 || buf[nbytes - 1] != '\\n')\n\t\treturn -EINVAL;\n\n\tmutex_lock(&rdtgroup_mutex);\n\n\trdt_last_cmd_clear();\n\n\tbuf[nbytes - 1] = '\\0';\n\n\tret = mon_config_write(r, buf, QOS_L3_MBM_TOTAL_EVENT_ID);\n\n\tmutex_unlock(&rdtgroup_mutex);\n\n\treturn ret ?: nbytes;\n}\n\nstatic ssize_t mbm_local_bytes_config_write(struct kernfs_open_file *of,\n\t\t\t\t\t    char *buf, size_t nbytes,\n\t\t\t\t\t    loff_t off)\n{\n\tstruct rdt_resource *r = of->kn->parent->priv;\n\tint ret;\n\n\t \n\tif (nbytes == 0 || buf[nbytes - 1] != '\\n')\n\t\treturn -EINVAL;\n\n\tmutex_lock(&rdtgroup_mutex);\n\n\trdt_last_cmd_clear();\n\n\tbuf[nbytes - 1] = '\\0';\n\n\tret = mon_config_write(r, buf, QOS_L3_MBM_LOCAL_EVENT_ID);\n\n\tmutex_unlock(&rdtgroup_mutex);\n\n\treturn ret ?: nbytes;\n}\n\n \nstatic struct rftype res_common_files[] = {\n\t{\n\t\t.name\t\t= \"last_cmd_status\",\n\t\t.mode\t\t= 0444,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.seq_show\t= rdt_last_cmd_status_show,\n\t\t.fflags\t\t= RF_TOP_INFO,\n\t},\n\t{\n\t\t.name\t\t= \"num_closids\",\n\t\t.mode\t\t= 0444,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.seq_show\t= rdt_num_closids_show,\n\t\t.fflags\t\t= RF_CTRL_INFO,\n\t},\n\t{\n\t\t.name\t\t= \"mon_features\",\n\t\t.mode\t\t= 0444,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.seq_show\t= rdt_mon_features_show,\n\t\t.fflags\t\t= RF_MON_INFO,\n\t},\n\t{\n\t\t.name\t\t= \"num_rmids\",\n\t\t.mode\t\t= 0444,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.seq_show\t= rdt_num_rmids_show,\n\t\t.fflags\t\t= RF_MON_INFO,\n\t},\n\t{\n\t\t.name\t\t= \"cbm_mask\",\n\t\t.mode\t\t= 0444,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.seq_show\t= rdt_default_ctrl_show,\n\t\t.fflags\t\t= RF_CTRL_INFO | RFTYPE_RES_CACHE,\n\t},\n\t{\n\t\t.name\t\t= \"min_cbm_bits\",\n\t\t.mode\t\t= 0444,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.seq_show\t= rdt_min_cbm_bits_show,\n\t\t.fflags\t\t= RF_CTRL_INFO | RFTYPE_RES_CACHE,\n\t},\n\t{\n\t\t.name\t\t= \"shareable_bits\",\n\t\t.mode\t\t= 0444,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.seq_show\t= rdt_shareable_bits_show,\n\t\t.fflags\t\t= RF_CTRL_INFO | RFTYPE_RES_CACHE,\n\t},\n\t{\n\t\t.name\t\t= \"bit_usage\",\n\t\t.mode\t\t= 0444,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.seq_show\t= rdt_bit_usage_show,\n\t\t.fflags\t\t= RF_CTRL_INFO | RFTYPE_RES_CACHE,\n\t},\n\t{\n\t\t.name\t\t= \"min_bandwidth\",\n\t\t.mode\t\t= 0444,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.seq_show\t= rdt_min_bw_show,\n\t\t.fflags\t\t= RF_CTRL_INFO | RFTYPE_RES_MB,\n\t},\n\t{\n\t\t.name\t\t= \"bandwidth_gran\",\n\t\t.mode\t\t= 0444,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.seq_show\t= rdt_bw_gran_show,\n\t\t.fflags\t\t= RF_CTRL_INFO | RFTYPE_RES_MB,\n\t},\n\t{\n\t\t.name\t\t= \"delay_linear\",\n\t\t.mode\t\t= 0444,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.seq_show\t= rdt_delay_linear_show,\n\t\t.fflags\t\t= RF_CTRL_INFO | RFTYPE_RES_MB,\n\t},\n\t \n\t{\n\t\t.name\t\t= \"thread_throttle_mode\",\n\t\t.mode\t\t= 0444,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.seq_show\t= rdt_thread_throttle_mode_show,\n\t},\n\t{\n\t\t.name\t\t= \"max_threshold_occupancy\",\n\t\t.mode\t\t= 0644,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.write\t\t= max_threshold_occ_write,\n\t\t.seq_show\t= max_threshold_occ_show,\n\t\t.fflags\t\t= RF_MON_INFO | RFTYPE_RES_CACHE,\n\t},\n\t{\n\t\t.name\t\t= \"mbm_total_bytes_config\",\n\t\t.mode\t\t= 0644,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.seq_show\t= mbm_total_bytes_config_show,\n\t\t.write\t\t= mbm_total_bytes_config_write,\n\t},\n\t{\n\t\t.name\t\t= \"mbm_local_bytes_config\",\n\t\t.mode\t\t= 0644,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.seq_show\t= mbm_local_bytes_config_show,\n\t\t.write\t\t= mbm_local_bytes_config_write,\n\t},\n\t{\n\t\t.name\t\t= \"cpus\",\n\t\t.mode\t\t= 0644,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.write\t\t= rdtgroup_cpus_write,\n\t\t.seq_show\t= rdtgroup_cpus_show,\n\t\t.fflags\t\t= RFTYPE_BASE,\n\t},\n\t{\n\t\t.name\t\t= \"cpus_list\",\n\t\t.mode\t\t= 0644,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.write\t\t= rdtgroup_cpus_write,\n\t\t.seq_show\t= rdtgroup_cpus_show,\n\t\t.flags\t\t= RFTYPE_FLAGS_CPUS_LIST,\n\t\t.fflags\t\t= RFTYPE_BASE,\n\t},\n\t{\n\t\t.name\t\t= \"tasks\",\n\t\t.mode\t\t= 0644,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.write\t\t= rdtgroup_tasks_write,\n\t\t.seq_show\t= rdtgroup_tasks_show,\n\t\t.fflags\t\t= RFTYPE_BASE,\n\t},\n\t{\n\t\t.name\t\t= \"schemata\",\n\t\t.mode\t\t= 0644,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.write\t\t= rdtgroup_schemata_write,\n\t\t.seq_show\t= rdtgroup_schemata_show,\n\t\t.fflags\t\t= RF_CTRL_BASE,\n\t},\n\t{\n\t\t.name\t\t= \"mode\",\n\t\t.mode\t\t= 0644,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.write\t\t= rdtgroup_mode_write,\n\t\t.seq_show\t= rdtgroup_mode_show,\n\t\t.fflags\t\t= RF_CTRL_BASE,\n\t},\n\t{\n\t\t.name\t\t= \"size\",\n\t\t.mode\t\t= 0444,\n\t\t.kf_ops\t\t= &rdtgroup_kf_single_ops,\n\t\t.seq_show\t= rdtgroup_size_show,\n\t\t.fflags\t\t= RF_CTRL_BASE,\n\t},\n\n};\n\nstatic int rdtgroup_add_files(struct kernfs_node *kn, unsigned long fflags)\n{\n\tstruct rftype *rfts, *rft;\n\tint ret, len;\n\n\trfts = res_common_files;\n\tlen = ARRAY_SIZE(res_common_files);\n\n\tlockdep_assert_held(&rdtgroup_mutex);\n\n\tfor (rft = rfts; rft < rfts + len; rft++) {\n\t\tif (rft->fflags && ((fflags & rft->fflags) == rft->fflags)) {\n\t\t\tret = rdtgroup_add_file(kn, rft);\n\t\t\tif (ret)\n\t\t\t\tgoto error;\n\t\t}\n\t}\n\n\treturn 0;\nerror:\n\tpr_warn(\"Failed to add %s, err=%d\\n\", rft->name, ret);\n\twhile (--rft >= rfts) {\n\t\tif ((fflags & rft->fflags) == rft->fflags)\n\t\t\tkernfs_remove_by_name(kn, rft->name);\n\t}\n\treturn ret;\n}\n\nstatic struct rftype *rdtgroup_get_rftype_by_name(const char *name)\n{\n\tstruct rftype *rfts, *rft;\n\tint len;\n\n\trfts = res_common_files;\n\tlen = ARRAY_SIZE(res_common_files);\n\n\tfor (rft = rfts; rft < rfts + len; rft++) {\n\t\tif (!strcmp(rft->name, name))\n\t\t\treturn rft;\n\t}\n\n\treturn NULL;\n}\n\nvoid __init thread_throttle_mode_init(void)\n{\n\tstruct rftype *rft;\n\n\trft = rdtgroup_get_rftype_by_name(\"thread_throttle_mode\");\n\tif (!rft)\n\t\treturn;\n\n\trft->fflags = RF_CTRL_INFO | RFTYPE_RES_MB;\n}\n\nvoid __init mbm_config_rftype_init(const char *config)\n{\n\tstruct rftype *rft;\n\n\trft = rdtgroup_get_rftype_by_name(config);\n\tif (rft)\n\t\trft->fflags = RF_MON_INFO | RFTYPE_RES_CACHE;\n}\n\n \nint rdtgroup_kn_mode_restrict(struct rdtgroup *r, const char *name)\n{\n\tstruct iattr iattr = {.ia_valid = ATTR_MODE,};\n\tstruct kernfs_node *kn;\n\tint ret = 0;\n\n\tkn = kernfs_find_and_get_ns(r->kn, name, NULL);\n\tif (!kn)\n\t\treturn -ENOENT;\n\n\tswitch (kernfs_type(kn)) {\n\tcase KERNFS_DIR:\n\t\tiattr.ia_mode = S_IFDIR;\n\t\tbreak;\n\tcase KERNFS_FILE:\n\t\tiattr.ia_mode = S_IFREG;\n\t\tbreak;\n\tcase KERNFS_LINK:\n\t\tiattr.ia_mode = S_IFLNK;\n\t\tbreak;\n\t}\n\n\tret = kernfs_setattr(kn, &iattr);\n\tkernfs_put(kn);\n\treturn ret;\n}\n\n \nint rdtgroup_kn_mode_restore(struct rdtgroup *r, const char *name,\n\t\t\t     umode_t mask)\n{\n\tstruct iattr iattr = {.ia_valid = ATTR_MODE,};\n\tstruct kernfs_node *kn, *parent;\n\tstruct rftype *rfts, *rft;\n\tint ret, len;\n\n\trfts = res_common_files;\n\tlen = ARRAY_SIZE(res_common_files);\n\n\tfor (rft = rfts; rft < rfts + len; rft++) {\n\t\tif (!strcmp(rft->name, name))\n\t\t\tiattr.ia_mode = rft->mode & mask;\n\t}\n\n\tkn = kernfs_find_and_get_ns(r->kn, name, NULL);\n\tif (!kn)\n\t\treturn -ENOENT;\n\n\tswitch (kernfs_type(kn)) {\n\tcase KERNFS_DIR:\n\t\tparent = kernfs_get_parent(kn);\n\t\tif (parent) {\n\t\t\tiattr.ia_mode |= parent->mode;\n\t\t\tkernfs_put(parent);\n\t\t}\n\t\tiattr.ia_mode |= S_IFDIR;\n\t\tbreak;\n\tcase KERNFS_FILE:\n\t\tiattr.ia_mode |= S_IFREG;\n\t\tbreak;\n\tcase KERNFS_LINK:\n\t\tiattr.ia_mode |= S_IFLNK;\n\t\tbreak;\n\t}\n\n\tret = kernfs_setattr(kn, &iattr);\n\tkernfs_put(kn);\n\treturn ret;\n}\n\nstatic int rdtgroup_mkdir_info_resdir(void *priv, char *name,\n\t\t\t\t      unsigned long fflags)\n{\n\tstruct kernfs_node *kn_subdir;\n\tint ret;\n\n\tkn_subdir = kernfs_create_dir(kn_info, name,\n\t\t\t\t      kn_info->mode, priv);\n\tif (IS_ERR(kn_subdir))\n\t\treturn PTR_ERR(kn_subdir);\n\n\tret = rdtgroup_kn_set_ugid(kn_subdir);\n\tif (ret)\n\t\treturn ret;\n\n\tret = rdtgroup_add_files(kn_subdir, fflags);\n\tif (!ret)\n\t\tkernfs_activate(kn_subdir);\n\n\treturn ret;\n}\n\nstatic int rdtgroup_create_info_dir(struct kernfs_node *parent_kn)\n{\n\tstruct resctrl_schema *s;\n\tstruct rdt_resource *r;\n\tunsigned long fflags;\n\tchar name[32];\n\tint ret;\n\n\t \n\tkn_info = kernfs_create_dir(parent_kn, \"info\", parent_kn->mode, NULL);\n\tif (IS_ERR(kn_info))\n\t\treturn PTR_ERR(kn_info);\n\n\tret = rdtgroup_add_files(kn_info, RF_TOP_INFO);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\t \n\tlist_for_each_entry(s, &resctrl_schema_all, list) {\n\t\tr = s->res;\n\t\tfflags =  r->fflags | RF_CTRL_INFO;\n\t\tret = rdtgroup_mkdir_info_resdir(s, s->name, fflags);\n\t\tif (ret)\n\t\t\tgoto out_destroy;\n\t}\n\n\tfor_each_mon_capable_rdt_resource(r) {\n\t\tfflags =  r->fflags | RF_MON_INFO;\n\t\tsprintf(name, \"%s_MON\", r->name);\n\t\tret = rdtgroup_mkdir_info_resdir(r, name, fflags);\n\t\tif (ret)\n\t\t\tgoto out_destroy;\n\t}\n\n\tret = rdtgroup_kn_set_ugid(kn_info);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tkernfs_activate(kn_info);\n\n\treturn 0;\n\nout_destroy:\n\tkernfs_remove(kn_info);\n\treturn ret;\n}\n\nstatic int\nmongroup_create_dir(struct kernfs_node *parent_kn, struct rdtgroup *prgrp,\n\t\t    char *name, struct kernfs_node **dest_kn)\n{\n\tstruct kernfs_node *kn;\n\tint ret;\n\n\t \n\tkn = kernfs_create_dir(parent_kn, name, parent_kn->mode, prgrp);\n\tif (IS_ERR(kn))\n\t\treturn PTR_ERR(kn);\n\n\tif (dest_kn)\n\t\t*dest_kn = kn;\n\n\tret = rdtgroup_kn_set_ugid(kn);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tkernfs_activate(kn);\n\n\treturn 0;\n\nout_destroy:\n\tkernfs_remove(kn);\n\treturn ret;\n}\n\nstatic void l3_qos_cfg_update(void *arg)\n{\n\tbool *enable = arg;\n\n\twrmsrl(MSR_IA32_L3_QOS_CFG, *enable ? L3_QOS_CDP_ENABLE : 0ULL);\n}\n\nstatic void l2_qos_cfg_update(void *arg)\n{\n\tbool *enable = arg;\n\n\twrmsrl(MSR_IA32_L2_QOS_CFG, *enable ? L2_QOS_CDP_ENABLE : 0ULL);\n}\n\nstatic inline bool is_mba_linear(void)\n{\n\treturn rdt_resources_all[RDT_RESOURCE_MBA].r_resctrl.membw.delay_linear;\n}\n\nstatic int set_cache_qos_cfg(int level, bool enable)\n{\n\tvoid (*update)(void *arg);\n\tstruct rdt_resource *r_l;\n\tcpumask_var_t cpu_mask;\n\tstruct rdt_domain *d;\n\tint cpu;\n\n\tif (level == RDT_RESOURCE_L3)\n\t\tupdate = l3_qos_cfg_update;\n\telse if (level == RDT_RESOURCE_L2)\n\t\tupdate = l2_qos_cfg_update;\n\telse\n\t\treturn -EINVAL;\n\n\tif (!zalloc_cpumask_var(&cpu_mask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tr_l = &rdt_resources_all[level].r_resctrl;\n\tlist_for_each_entry(d, &r_l->domains, list) {\n\t\tif (r_l->cache.arch_has_per_cpu_cfg)\n\t\t\t \n\t\t\tfor_each_cpu(cpu, &d->cpu_mask)\n\t\t\t\tcpumask_set_cpu(cpu, cpu_mask);\n\t\telse\n\t\t\t \n\t\t\tcpumask_set_cpu(cpumask_any(&d->cpu_mask), cpu_mask);\n\t}\n\n\t \n\ton_each_cpu_mask(cpu_mask, update, &enable, 1);\n\n\tfree_cpumask_var(cpu_mask);\n\n\treturn 0;\n}\n\n \nvoid rdt_domain_reconfigure_cdp(struct rdt_resource *r)\n{\n\tstruct rdt_hw_resource *hw_res = resctrl_to_arch_res(r);\n\n\tif (!r->cdp_capable)\n\t\treturn;\n\n\tif (r->rid == RDT_RESOURCE_L2)\n\t\tl2_qos_cfg_update(&hw_res->cdp_enabled);\n\n\tif (r->rid == RDT_RESOURCE_L3)\n\t\tl3_qos_cfg_update(&hw_res->cdp_enabled);\n}\n\nstatic int mba_sc_domain_allocate(struct rdt_resource *r, struct rdt_domain *d)\n{\n\tu32 num_closid = resctrl_arch_get_num_closid(r);\n\tint cpu = cpumask_any(&d->cpu_mask);\n\tint i;\n\n\td->mbps_val = kcalloc_node(num_closid, sizeof(*d->mbps_val),\n\t\t\t\t   GFP_KERNEL, cpu_to_node(cpu));\n\tif (!d->mbps_val)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < num_closid; i++)\n\t\td->mbps_val[i] = MBA_MAX_MBPS;\n\n\treturn 0;\n}\n\nstatic void mba_sc_domain_destroy(struct rdt_resource *r,\n\t\t\t\t  struct rdt_domain *d)\n{\n\tkfree(d->mbps_val);\n\td->mbps_val = NULL;\n}\n\n \nstatic bool supports_mba_mbps(void)\n{\n\tstruct rdt_resource *r = &rdt_resources_all[RDT_RESOURCE_MBA].r_resctrl;\n\n\treturn (is_mbm_local_enabled() &&\n\t\tr->alloc_capable && is_mba_linear());\n}\n\n \nstatic int set_mba_sc(bool mba_sc)\n{\n\tstruct rdt_resource *r = &rdt_resources_all[RDT_RESOURCE_MBA].r_resctrl;\n\tu32 num_closid = resctrl_arch_get_num_closid(r);\n\tstruct rdt_domain *d;\n\tint i;\n\n\tif (!supports_mba_mbps() || mba_sc == is_mba_sc(r))\n\t\treturn -EINVAL;\n\n\tr->membw.mba_sc = mba_sc;\n\n\tlist_for_each_entry(d, &r->domains, list) {\n\t\tfor (i = 0; i < num_closid; i++)\n\t\t\td->mbps_val[i] = MBA_MAX_MBPS;\n\t}\n\n\treturn 0;\n}\n\nstatic int cdp_enable(int level)\n{\n\tstruct rdt_resource *r_l = &rdt_resources_all[level].r_resctrl;\n\tint ret;\n\n\tif (!r_l->alloc_capable)\n\t\treturn -EINVAL;\n\n\tret = set_cache_qos_cfg(level, true);\n\tif (!ret)\n\t\trdt_resources_all[level].cdp_enabled = true;\n\n\treturn ret;\n}\n\nstatic void cdp_disable(int level)\n{\n\tstruct rdt_hw_resource *r_hw = &rdt_resources_all[level];\n\n\tif (r_hw->cdp_enabled) {\n\t\tset_cache_qos_cfg(level, false);\n\t\tr_hw->cdp_enabled = false;\n\t}\n}\n\nint resctrl_arch_set_cdp_enabled(enum resctrl_res_level l, bool enable)\n{\n\tstruct rdt_hw_resource *hw_res = &rdt_resources_all[l];\n\n\tif (!hw_res->r_resctrl.cdp_capable)\n\t\treturn -EINVAL;\n\n\tif (enable)\n\t\treturn cdp_enable(l);\n\n\tcdp_disable(l);\n\n\treturn 0;\n}\n\nstatic void cdp_disable_all(void)\n{\n\tif (resctrl_arch_get_cdp_enabled(RDT_RESOURCE_L3))\n\t\tresctrl_arch_set_cdp_enabled(RDT_RESOURCE_L3, false);\n\tif (resctrl_arch_get_cdp_enabled(RDT_RESOURCE_L2))\n\t\tresctrl_arch_set_cdp_enabled(RDT_RESOURCE_L2, false);\n}\n\n \nstatic struct rdtgroup *kernfs_to_rdtgroup(struct kernfs_node *kn)\n{\n\tif (kernfs_type(kn) == KERNFS_DIR) {\n\t\t \n\t\tif (kn == kn_info || kn->parent == kn_info)\n\t\t\treturn NULL;\n\t\telse\n\t\t\treturn kn->priv;\n\t} else {\n\t\treturn kn->parent->priv;\n\t}\n}\n\nstatic void rdtgroup_kn_get(struct rdtgroup *rdtgrp, struct kernfs_node *kn)\n{\n\tatomic_inc(&rdtgrp->waitcount);\n\tkernfs_break_active_protection(kn);\n}\n\nstatic void rdtgroup_kn_put(struct rdtgroup *rdtgrp, struct kernfs_node *kn)\n{\n\tif (atomic_dec_and_test(&rdtgrp->waitcount) &&\n\t    (rdtgrp->flags & RDT_DELETED)) {\n\t\tif (rdtgrp->mode == RDT_MODE_PSEUDO_LOCKSETUP ||\n\t\t    rdtgrp->mode == RDT_MODE_PSEUDO_LOCKED)\n\t\t\trdtgroup_pseudo_lock_remove(rdtgrp);\n\t\tkernfs_unbreak_active_protection(kn);\n\t\trdtgroup_remove(rdtgrp);\n\t} else {\n\t\tkernfs_unbreak_active_protection(kn);\n\t}\n}\n\nstruct rdtgroup *rdtgroup_kn_lock_live(struct kernfs_node *kn)\n{\n\tstruct rdtgroup *rdtgrp = kernfs_to_rdtgroup(kn);\n\n\tif (!rdtgrp)\n\t\treturn NULL;\n\n\trdtgroup_kn_get(rdtgrp, kn);\n\n\tmutex_lock(&rdtgroup_mutex);\n\n\t \n\tif (rdtgrp->flags & RDT_DELETED)\n\t\treturn NULL;\n\n\treturn rdtgrp;\n}\n\nvoid rdtgroup_kn_unlock(struct kernfs_node *kn)\n{\n\tstruct rdtgroup *rdtgrp = kernfs_to_rdtgroup(kn);\n\n\tif (!rdtgrp)\n\t\treturn;\n\n\tmutex_unlock(&rdtgroup_mutex);\n\trdtgroup_kn_put(rdtgrp, kn);\n}\n\nstatic int mkdir_mondata_all(struct kernfs_node *parent_kn,\n\t\t\t     struct rdtgroup *prgrp,\n\t\t\t     struct kernfs_node **mon_data_kn);\n\nstatic int rdt_enable_ctx(struct rdt_fs_context *ctx)\n{\n\tint ret = 0;\n\n\tif (ctx->enable_cdpl2)\n\t\tret = resctrl_arch_set_cdp_enabled(RDT_RESOURCE_L2, true);\n\n\tif (!ret && ctx->enable_cdpl3)\n\t\tret = resctrl_arch_set_cdp_enabled(RDT_RESOURCE_L3, true);\n\n\tif (!ret && ctx->enable_mba_mbps)\n\t\tret = set_mba_sc(true);\n\n\treturn ret;\n}\n\nstatic int schemata_list_add(struct rdt_resource *r, enum resctrl_conf_type type)\n{\n\tstruct resctrl_schema *s;\n\tconst char *suffix = \"\";\n\tint ret, cl;\n\n\ts = kzalloc(sizeof(*s), GFP_KERNEL);\n\tif (!s)\n\t\treturn -ENOMEM;\n\n\ts->res = r;\n\ts->num_closid = resctrl_arch_get_num_closid(r);\n\tif (resctrl_arch_get_cdp_enabled(r->rid))\n\t\ts->num_closid /= 2;\n\n\ts->conf_type = type;\n\tswitch (type) {\n\tcase CDP_CODE:\n\t\tsuffix = \"CODE\";\n\t\tbreak;\n\tcase CDP_DATA:\n\t\tsuffix = \"DATA\";\n\t\tbreak;\n\tcase CDP_NONE:\n\t\tsuffix = \"\";\n\t\tbreak;\n\t}\n\n\tret = snprintf(s->name, sizeof(s->name), \"%s%s\", r->name, suffix);\n\tif (ret >= sizeof(s->name)) {\n\t\tkfree(s);\n\t\treturn -EINVAL;\n\t}\n\n\tcl = strlen(s->name);\n\n\t \n\tif (r->cdp_capable && !resctrl_arch_get_cdp_enabled(r->rid))\n\t\tcl += 4;\n\n\tif (cl > max_name_width)\n\t\tmax_name_width = cl;\n\n\tINIT_LIST_HEAD(&s->list);\n\tlist_add(&s->list, &resctrl_schema_all);\n\n\treturn 0;\n}\n\nstatic int schemata_list_create(void)\n{\n\tstruct rdt_resource *r;\n\tint ret = 0;\n\n\tfor_each_alloc_capable_rdt_resource(r) {\n\t\tif (resctrl_arch_get_cdp_enabled(r->rid)) {\n\t\t\tret = schemata_list_add(r, CDP_CODE);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\n\t\t\tret = schemata_list_add(r, CDP_DATA);\n\t\t} else {\n\t\t\tret = schemata_list_add(r, CDP_NONE);\n\t\t}\n\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic void schemata_list_destroy(void)\n{\n\tstruct resctrl_schema *s, *tmp;\n\n\tlist_for_each_entry_safe(s, tmp, &resctrl_schema_all, list) {\n\t\tlist_del(&s->list);\n\t\tkfree(s);\n\t}\n}\n\nstatic int rdt_get_tree(struct fs_context *fc)\n{\n\tstruct rdt_fs_context *ctx = rdt_fc2context(fc);\n\tstruct rdt_domain *dom;\n\tstruct rdt_resource *r;\n\tint ret;\n\n\tcpus_read_lock();\n\tmutex_lock(&rdtgroup_mutex);\n\t \n\tif (static_branch_unlikely(&rdt_enable_key)) {\n\t\tret = -EBUSY;\n\t\tgoto out;\n\t}\n\n\tret = rdt_enable_ctx(ctx);\n\tif (ret < 0)\n\t\tgoto out_cdp;\n\n\tret = schemata_list_create();\n\tif (ret) {\n\t\tschemata_list_destroy();\n\t\tgoto out_mba;\n\t}\n\n\tclosid_init();\n\n\tret = rdtgroup_create_info_dir(rdtgroup_default.kn);\n\tif (ret < 0)\n\t\tgoto out_schemata_free;\n\n\tif (rdt_mon_capable) {\n\t\tret = mongroup_create_dir(rdtgroup_default.kn,\n\t\t\t\t\t  &rdtgroup_default, \"mon_groups\",\n\t\t\t\t\t  &kn_mongrp);\n\t\tif (ret < 0)\n\t\t\tgoto out_info;\n\n\t\tret = mkdir_mondata_all(rdtgroup_default.kn,\n\t\t\t\t\t&rdtgroup_default, &kn_mondata);\n\t\tif (ret < 0)\n\t\t\tgoto out_mongrp;\n\t\trdtgroup_default.mon.mon_data_kn = kn_mondata;\n\t}\n\n\tret = rdt_pseudo_lock_init();\n\tif (ret)\n\t\tgoto out_mondata;\n\n\tret = kernfs_get_tree(fc);\n\tif (ret < 0)\n\t\tgoto out_psl;\n\n\tif (rdt_alloc_capable)\n\t\tstatic_branch_enable_cpuslocked(&rdt_alloc_enable_key);\n\tif (rdt_mon_capable)\n\t\tstatic_branch_enable_cpuslocked(&rdt_mon_enable_key);\n\n\tif (rdt_alloc_capable || rdt_mon_capable)\n\t\tstatic_branch_enable_cpuslocked(&rdt_enable_key);\n\n\tif (is_mbm_enabled()) {\n\t\tr = &rdt_resources_all[RDT_RESOURCE_L3].r_resctrl;\n\t\tlist_for_each_entry(dom, &r->domains, list)\n\t\t\tmbm_setup_overflow_handler(dom, MBM_OVERFLOW_INTERVAL);\n\t}\n\n\tgoto out;\n\nout_psl:\n\trdt_pseudo_lock_release();\nout_mondata:\n\tif (rdt_mon_capable)\n\t\tkernfs_remove(kn_mondata);\nout_mongrp:\n\tif (rdt_mon_capable)\n\t\tkernfs_remove(kn_mongrp);\nout_info:\n\tkernfs_remove(kn_info);\nout_schemata_free:\n\tschemata_list_destroy();\nout_mba:\n\tif (ctx->enable_mba_mbps)\n\t\tset_mba_sc(false);\nout_cdp:\n\tcdp_disable_all();\nout:\n\trdt_last_cmd_clear();\n\tmutex_unlock(&rdtgroup_mutex);\n\tcpus_read_unlock();\n\treturn ret;\n}\n\nenum rdt_param {\n\tOpt_cdp,\n\tOpt_cdpl2,\n\tOpt_mba_mbps,\n\tnr__rdt_params\n};\n\nstatic const struct fs_parameter_spec rdt_fs_parameters[] = {\n\tfsparam_flag(\"cdp\",\t\tOpt_cdp),\n\tfsparam_flag(\"cdpl2\",\t\tOpt_cdpl2),\n\tfsparam_flag(\"mba_MBps\",\tOpt_mba_mbps),\n\t{}\n};\n\nstatic int rdt_parse_param(struct fs_context *fc, struct fs_parameter *param)\n{\n\tstruct rdt_fs_context *ctx = rdt_fc2context(fc);\n\tstruct fs_parse_result result;\n\tint opt;\n\n\topt = fs_parse(fc, rdt_fs_parameters, param, &result);\n\tif (opt < 0)\n\t\treturn opt;\n\n\tswitch (opt) {\n\tcase Opt_cdp:\n\t\tctx->enable_cdpl3 = true;\n\t\treturn 0;\n\tcase Opt_cdpl2:\n\t\tctx->enable_cdpl2 = true;\n\t\treturn 0;\n\tcase Opt_mba_mbps:\n\t\tif (!supports_mba_mbps())\n\t\t\treturn -EINVAL;\n\t\tctx->enable_mba_mbps = true;\n\t\treturn 0;\n\t}\n\n\treturn -EINVAL;\n}\n\nstatic void rdt_fs_context_free(struct fs_context *fc)\n{\n\tstruct rdt_fs_context *ctx = rdt_fc2context(fc);\n\n\tkernfs_free_fs_context(fc);\n\tkfree(ctx);\n}\n\nstatic const struct fs_context_operations rdt_fs_context_ops = {\n\t.free\t\t= rdt_fs_context_free,\n\t.parse_param\t= rdt_parse_param,\n\t.get_tree\t= rdt_get_tree,\n};\n\nstatic int rdt_init_fs_context(struct fs_context *fc)\n{\n\tstruct rdt_fs_context *ctx;\n\n\tctx = kzalloc(sizeof(struct rdt_fs_context), GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\tctx->kfc.root = rdt_root;\n\tctx->kfc.magic = RDTGROUP_SUPER_MAGIC;\n\tfc->fs_private = &ctx->kfc;\n\tfc->ops = &rdt_fs_context_ops;\n\tput_user_ns(fc->user_ns);\n\tfc->user_ns = get_user_ns(&init_user_ns);\n\tfc->global = true;\n\treturn 0;\n}\n\nstatic int reset_all_ctrls(struct rdt_resource *r)\n{\n\tstruct rdt_hw_resource *hw_res = resctrl_to_arch_res(r);\n\tstruct rdt_hw_domain *hw_dom;\n\tstruct msr_param msr_param;\n\tcpumask_var_t cpu_mask;\n\tstruct rdt_domain *d;\n\tint i;\n\n\tif (!zalloc_cpumask_var(&cpu_mask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\tmsr_param.res = r;\n\tmsr_param.low = 0;\n\tmsr_param.high = hw_res->num_closid;\n\n\t \n\tlist_for_each_entry(d, &r->domains, list) {\n\t\thw_dom = resctrl_to_arch_dom(d);\n\t\tcpumask_set_cpu(cpumask_any(&d->cpu_mask), cpu_mask);\n\n\t\tfor (i = 0; i < hw_res->num_closid; i++)\n\t\t\thw_dom->ctrl_val[i] = r->default_ctrl;\n\t}\n\n\t \n\ton_each_cpu_mask(cpu_mask, rdt_ctrl_update, &msr_param, 1);\n\n\tfree_cpumask_var(cpu_mask);\n\n\treturn 0;\n}\n\n \nstatic void rdt_move_group_tasks(struct rdtgroup *from, struct rdtgroup *to,\n\t\t\t\t struct cpumask *mask)\n{\n\tstruct task_struct *p, *t;\n\n\tread_lock(&tasklist_lock);\n\tfor_each_process_thread(p, t) {\n\t\tif (!from || is_closid_match(t, from) ||\n\t\t    is_rmid_match(t, from)) {\n\t\t\tWRITE_ONCE(t->closid, to->closid);\n\t\t\tWRITE_ONCE(t->rmid, to->mon.rmid);\n\n\t\t\t \n\t\t\tsmp_mb();\n\n\t\t\t \n\t\t\tif (IS_ENABLED(CONFIG_SMP) && mask && task_curr(t))\n\t\t\t\tcpumask_set_cpu(task_cpu(t), mask);\n\t\t}\n\t}\n\tread_unlock(&tasklist_lock);\n}\n\nstatic void free_all_child_rdtgrp(struct rdtgroup *rdtgrp)\n{\n\tstruct rdtgroup *sentry, *stmp;\n\tstruct list_head *head;\n\n\thead = &rdtgrp->mon.crdtgrp_list;\n\tlist_for_each_entry_safe(sentry, stmp, head, mon.crdtgrp_list) {\n\t\tfree_rmid(sentry->mon.rmid);\n\t\tlist_del(&sentry->mon.crdtgrp_list);\n\n\t\tif (atomic_read(&sentry->waitcount) != 0)\n\t\t\tsentry->flags = RDT_DELETED;\n\t\telse\n\t\t\trdtgroup_remove(sentry);\n\t}\n}\n\n \nstatic void rmdir_all_sub(void)\n{\n\tstruct rdtgroup *rdtgrp, *tmp;\n\n\t \n\trdt_move_group_tasks(NULL, &rdtgroup_default, NULL);\n\n\tlist_for_each_entry_safe(rdtgrp, tmp, &rdt_all_groups, rdtgroup_list) {\n\t\t \n\t\tfree_all_child_rdtgrp(rdtgrp);\n\n\t\t \n\t\tif (rdtgrp == &rdtgroup_default)\n\t\t\tcontinue;\n\n\t\tif (rdtgrp->mode == RDT_MODE_PSEUDO_LOCKSETUP ||\n\t\t    rdtgrp->mode == RDT_MODE_PSEUDO_LOCKED)\n\t\t\trdtgroup_pseudo_lock_remove(rdtgrp);\n\n\t\t \n\t\tcpumask_or(&rdtgroup_default.cpu_mask,\n\t\t\t   &rdtgroup_default.cpu_mask, &rdtgrp->cpu_mask);\n\n\t\tfree_rmid(rdtgrp->mon.rmid);\n\n\t\tkernfs_remove(rdtgrp->kn);\n\t\tlist_del(&rdtgrp->rdtgroup_list);\n\n\t\tif (atomic_read(&rdtgrp->waitcount) != 0)\n\t\t\trdtgrp->flags = RDT_DELETED;\n\t\telse\n\t\t\trdtgroup_remove(rdtgrp);\n\t}\n\t \n\tupdate_closid_rmid(cpu_online_mask, &rdtgroup_default);\n\n\tkernfs_remove(kn_info);\n\tkernfs_remove(kn_mongrp);\n\tkernfs_remove(kn_mondata);\n}\n\nstatic void rdt_kill_sb(struct super_block *sb)\n{\n\tstruct rdt_resource *r;\n\n\tcpus_read_lock();\n\tmutex_lock(&rdtgroup_mutex);\n\n\tset_mba_sc(false);\n\n\t \n\tfor_each_alloc_capable_rdt_resource(r)\n\t\treset_all_ctrls(r);\n\tcdp_disable_all();\n\trmdir_all_sub();\n\trdt_pseudo_lock_release();\n\trdtgroup_default.mode = RDT_MODE_SHAREABLE;\n\tschemata_list_destroy();\n\tstatic_branch_disable_cpuslocked(&rdt_alloc_enable_key);\n\tstatic_branch_disable_cpuslocked(&rdt_mon_enable_key);\n\tstatic_branch_disable_cpuslocked(&rdt_enable_key);\n\tkernfs_kill_sb(sb);\n\tmutex_unlock(&rdtgroup_mutex);\n\tcpus_read_unlock();\n}\n\nstatic struct file_system_type rdt_fs_type = {\n\t.name\t\t\t= \"resctrl\",\n\t.init_fs_context\t= rdt_init_fs_context,\n\t.parameters\t\t= rdt_fs_parameters,\n\t.kill_sb\t\t= rdt_kill_sb,\n};\n\nstatic int mon_addfile(struct kernfs_node *parent_kn, const char *name,\n\t\t       void *priv)\n{\n\tstruct kernfs_node *kn;\n\tint ret = 0;\n\n\tkn = __kernfs_create_file(parent_kn, name, 0444,\n\t\t\t\t  GLOBAL_ROOT_UID, GLOBAL_ROOT_GID, 0,\n\t\t\t\t  &kf_mondata_ops, priv, NULL, NULL);\n\tif (IS_ERR(kn))\n\t\treturn PTR_ERR(kn);\n\n\tret = rdtgroup_kn_set_ugid(kn);\n\tif (ret) {\n\t\tkernfs_remove(kn);\n\t\treturn ret;\n\t}\n\n\treturn ret;\n}\n\n \nstatic void rmdir_mondata_subdir_allrdtgrp(struct rdt_resource *r,\n\t\t\t\t\t   unsigned int dom_id)\n{\n\tstruct rdtgroup *prgrp, *crgrp;\n\tchar name[32];\n\n\tlist_for_each_entry(prgrp, &rdt_all_groups, rdtgroup_list) {\n\t\tsprintf(name, \"mon_%s_%02d\", r->name, dom_id);\n\t\tkernfs_remove_by_name(prgrp->mon.mon_data_kn, name);\n\n\t\tlist_for_each_entry(crgrp, &prgrp->mon.crdtgrp_list, mon.crdtgrp_list)\n\t\t\tkernfs_remove_by_name(crgrp->mon.mon_data_kn, name);\n\t}\n}\n\nstatic int mkdir_mondata_subdir(struct kernfs_node *parent_kn,\n\t\t\t\tstruct rdt_domain *d,\n\t\t\t\tstruct rdt_resource *r, struct rdtgroup *prgrp)\n{\n\tunion mon_data_bits priv;\n\tstruct kernfs_node *kn;\n\tstruct mon_evt *mevt;\n\tstruct rmid_read rr;\n\tchar name[32];\n\tint ret;\n\n\tsprintf(name, \"mon_%s_%02d\", r->name, d->id);\n\t \n\tkn = kernfs_create_dir(parent_kn, name, parent_kn->mode, prgrp);\n\tif (IS_ERR(kn))\n\t\treturn PTR_ERR(kn);\n\n\tret = rdtgroup_kn_set_ugid(kn);\n\tif (ret)\n\t\tgoto out_destroy;\n\n\tif (WARN_ON(list_empty(&r->evt_list))) {\n\t\tret = -EPERM;\n\t\tgoto out_destroy;\n\t}\n\n\tpriv.u.rid = r->rid;\n\tpriv.u.domid = d->id;\n\tlist_for_each_entry(mevt, &r->evt_list, list) {\n\t\tpriv.u.evtid = mevt->evtid;\n\t\tret = mon_addfile(kn, mevt->name, priv.priv);\n\t\tif (ret)\n\t\t\tgoto out_destroy;\n\n\t\tif (is_mbm_event(mevt->evtid))\n\t\t\tmon_event_read(&rr, r, d, prgrp, mevt->evtid, true);\n\t}\n\tkernfs_activate(kn);\n\treturn 0;\n\nout_destroy:\n\tkernfs_remove(kn);\n\treturn ret;\n}\n\n \nstatic void mkdir_mondata_subdir_allrdtgrp(struct rdt_resource *r,\n\t\t\t\t\t   struct rdt_domain *d)\n{\n\tstruct kernfs_node *parent_kn;\n\tstruct rdtgroup *prgrp, *crgrp;\n\tstruct list_head *head;\n\n\tlist_for_each_entry(prgrp, &rdt_all_groups, rdtgroup_list) {\n\t\tparent_kn = prgrp->mon.mon_data_kn;\n\t\tmkdir_mondata_subdir(parent_kn, d, r, prgrp);\n\n\t\thead = &prgrp->mon.crdtgrp_list;\n\t\tlist_for_each_entry(crgrp, head, mon.crdtgrp_list) {\n\t\t\tparent_kn = crgrp->mon.mon_data_kn;\n\t\t\tmkdir_mondata_subdir(parent_kn, d, r, crgrp);\n\t\t}\n\t}\n}\n\nstatic int mkdir_mondata_subdir_alldom(struct kernfs_node *parent_kn,\n\t\t\t\t       struct rdt_resource *r,\n\t\t\t\t       struct rdtgroup *prgrp)\n{\n\tstruct rdt_domain *dom;\n\tint ret;\n\n\tlist_for_each_entry(dom, &r->domains, list) {\n\t\tret = mkdir_mondata_subdir(parent_kn, dom, r, prgrp);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int mkdir_mondata_all(struct kernfs_node *parent_kn,\n\t\t\t     struct rdtgroup *prgrp,\n\t\t\t     struct kernfs_node **dest_kn)\n{\n\tstruct rdt_resource *r;\n\tstruct kernfs_node *kn;\n\tint ret;\n\n\t \n\tret = mongroup_create_dir(parent_kn, prgrp, \"mon_data\", &kn);\n\tif (ret)\n\t\treturn ret;\n\n\tif (dest_kn)\n\t\t*dest_kn = kn;\n\n\t \n\tfor_each_mon_capable_rdt_resource(r) {\n\t\tret = mkdir_mondata_subdir_alldom(kn, r, prgrp);\n\t\tif (ret)\n\t\t\tgoto out_destroy;\n\t}\n\n\treturn 0;\n\nout_destroy:\n\tkernfs_remove(kn);\n\treturn ret;\n}\n\n \nstatic u32 cbm_ensure_valid(u32 _val, struct rdt_resource *r)\n{\n\tunsigned int cbm_len = r->cache.cbm_len;\n\tunsigned long first_bit, zero_bit;\n\tunsigned long val = _val;\n\n\tif (!val)\n\t\treturn 0;\n\n\tfirst_bit = find_first_bit(&val, cbm_len);\n\tzero_bit = find_next_zero_bit(&val, cbm_len, first_bit);\n\n\t \n\tbitmap_clear(&val, zero_bit, cbm_len - zero_bit);\n\treturn (u32)val;\n}\n\n \nstatic int __init_one_rdt_domain(struct rdt_domain *d, struct resctrl_schema *s,\n\t\t\t\t u32 closid)\n{\n\tenum resctrl_conf_type peer_type = resctrl_peer_type(s->conf_type);\n\tenum resctrl_conf_type t = s->conf_type;\n\tstruct resctrl_staged_config *cfg;\n\tstruct rdt_resource *r = s->res;\n\tu32 used_b = 0, unused_b = 0;\n\tunsigned long tmp_cbm;\n\tenum rdtgrp_mode mode;\n\tu32 peer_ctl, ctrl_val;\n\tint i;\n\n\tcfg = &d->staged_config[t];\n\tcfg->have_new_ctrl = false;\n\tcfg->new_ctrl = r->cache.shareable_bits;\n\tused_b = r->cache.shareable_bits;\n\tfor (i = 0; i < closids_supported(); i++) {\n\t\tif (closid_allocated(i) && i != closid) {\n\t\t\tmode = rdtgroup_mode_by_closid(i);\n\t\t\tif (mode == RDT_MODE_PSEUDO_LOCKSETUP)\n\t\t\t\t \n\t\t\t\tcontinue;\n\t\t\t \n\t\t\tif (resctrl_arch_get_cdp_enabled(r->rid))\n\t\t\t\tpeer_ctl = resctrl_arch_get_config(r, d, i,\n\t\t\t\t\t\t\t\t   peer_type);\n\t\t\telse\n\t\t\t\tpeer_ctl = 0;\n\t\t\tctrl_val = resctrl_arch_get_config(r, d, i,\n\t\t\t\t\t\t\t   s->conf_type);\n\t\t\tused_b |= ctrl_val | peer_ctl;\n\t\t\tif (mode == RDT_MODE_SHAREABLE)\n\t\t\t\tcfg->new_ctrl |= ctrl_val | peer_ctl;\n\t\t}\n\t}\n\tif (d->plr && d->plr->cbm > 0)\n\t\tused_b |= d->plr->cbm;\n\tunused_b = used_b ^ (BIT_MASK(r->cache.cbm_len) - 1);\n\tunused_b &= BIT_MASK(r->cache.cbm_len) - 1;\n\tcfg->new_ctrl |= unused_b;\n\t \n\tcfg->new_ctrl = cbm_ensure_valid(cfg->new_ctrl, r);\n\t \n\ttmp_cbm = cfg->new_ctrl;\n\tif (bitmap_weight(&tmp_cbm, r->cache.cbm_len) < r->cache.min_cbm_bits) {\n\t\trdt_last_cmd_printf(\"No space on %s:%d\\n\", s->name, d->id);\n\t\treturn -ENOSPC;\n\t}\n\tcfg->have_new_ctrl = true;\n\n\treturn 0;\n}\n\n \nstatic int rdtgroup_init_cat(struct resctrl_schema *s, u32 closid)\n{\n\tstruct rdt_domain *d;\n\tint ret;\n\n\tlist_for_each_entry(d, &s->res->domains, list) {\n\t\tret = __init_one_rdt_domain(d, s, closid);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void rdtgroup_init_mba(struct rdt_resource *r, u32 closid)\n{\n\tstruct resctrl_staged_config *cfg;\n\tstruct rdt_domain *d;\n\n\tlist_for_each_entry(d, &r->domains, list) {\n\t\tif (is_mba_sc(r)) {\n\t\t\td->mbps_val[closid] = MBA_MAX_MBPS;\n\t\t\tcontinue;\n\t\t}\n\n\t\tcfg = &d->staged_config[CDP_NONE];\n\t\tcfg->new_ctrl = r->default_ctrl;\n\t\tcfg->have_new_ctrl = true;\n\t}\n}\n\n \nstatic int rdtgroup_init_alloc(struct rdtgroup *rdtgrp)\n{\n\tstruct resctrl_schema *s;\n\tstruct rdt_resource *r;\n\tint ret = 0;\n\n\trdt_staged_configs_clear();\n\n\tlist_for_each_entry(s, &resctrl_schema_all, list) {\n\t\tr = s->res;\n\t\tif (r->rid == RDT_RESOURCE_MBA ||\n\t\t    r->rid == RDT_RESOURCE_SMBA) {\n\t\t\trdtgroup_init_mba(r, rdtgrp->closid);\n\t\t\tif (is_mba_sc(r))\n\t\t\t\tcontinue;\n\t\t} else {\n\t\t\tret = rdtgroup_init_cat(s, rdtgrp->closid);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tret = resctrl_arch_update_domains(r, rdtgrp->closid);\n\t\tif (ret < 0) {\n\t\t\trdt_last_cmd_puts(\"Failed to initialize allocations\\n\");\n\t\t\tgoto out;\n\t\t}\n\n\t}\n\n\trdtgrp->mode = RDT_MODE_SHAREABLE;\n\nout:\n\trdt_staged_configs_clear();\n\treturn ret;\n}\n\nstatic int mkdir_rdt_prepare(struct kernfs_node *parent_kn,\n\t\t\t     const char *name, umode_t mode,\n\t\t\t     enum rdt_group_type rtype, struct rdtgroup **r)\n{\n\tstruct rdtgroup *prdtgrp, *rdtgrp;\n\tstruct kernfs_node *kn;\n\tuint files = 0;\n\tint ret;\n\n\tprdtgrp = rdtgroup_kn_lock_live(parent_kn);\n\tif (!prdtgrp) {\n\t\tret = -ENODEV;\n\t\tgoto out_unlock;\n\t}\n\n\tif (rtype == RDTMON_GROUP &&\n\t    (prdtgrp->mode == RDT_MODE_PSEUDO_LOCKSETUP ||\n\t     prdtgrp->mode == RDT_MODE_PSEUDO_LOCKED)) {\n\t\tret = -EINVAL;\n\t\trdt_last_cmd_puts(\"Pseudo-locking in progress\\n\");\n\t\tgoto out_unlock;\n\t}\n\n\t \n\trdtgrp = kzalloc(sizeof(*rdtgrp), GFP_KERNEL);\n\tif (!rdtgrp) {\n\t\tret = -ENOSPC;\n\t\trdt_last_cmd_puts(\"Kernel out of memory\\n\");\n\t\tgoto out_unlock;\n\t}\n\t*r = rdtgrp;\n\trdtgrp->mon.parent = prdtgrp;\n\trdtgrp->type = rtype;\n\tINIT_LIST_HEAD(&rdtgrp->mon.crdtgrp_list);\n\n\t \n\tkn = kernfs_create_dir(parent_kn, name, mode, rdtgrp);\n\tif (IS_ERR(kn)) {\n\t\tret = PTR_ERR(kn);\n\t\trdt_last_cmd_puts(\"kernfs create error\\n\");\n\t\tgoto out_free_rgrp;\n\t}\n\trdtgrp->kn = kn;\n\n\t \n\tkernfs_get(kn);\n\n\tret = rdtgroup_kn_set_ugid(kn);\n\tif (ret) {\n\t\trdt_last_cmd_puts(\"kernfs perm error\\n\");\n\t\tgoto out_destroy;\n\t}\n\n\tfiles = RFTYPE_BASE | BIT(RF_CTRLSHIFT + rtype);\n\tret = rdtgroup_add_files(kn, files);\n\tif (ret) {\n\t\trdt_last_cmd_puts(\"kernfs fill error\\n\");\n\t\tgoto out_destroy;\n\t}\n\n\tif (rdt_mon_capable) {\n\t\tret = alloc_rmid();\n\t\tif (ret < 0) {\n\t\t\trdt_last_cmd_puts(\"Out of RMIDs\\n\");\n\t\t\tgoto out_destroy;\n\t\t}\n\t\trdtgrp->mon.rmid = ret;\n\n\t\tret = mkdir_mondata_all(kn, rdtgrp, &rdtgrp->mon.mon_data_kn);\n\t\tif (ret) {\n\t\t\trdt_last_cmd_puts(\"kernfs subdir error\\n\");\n\t\t\tgoto out_idfree;\n\t\t}\n\t}\n\tkernfs_activate(kn);\n\n\t \n\treturn 0;\n\nout_idfree:\n\tfree_rmid(rdtgrp->mon.rmid);\nout_destroy:\n\tkernfs_put(rdtgrp->kn);\n\tkernfs_remove(rdtgrp->kn);\nout_free_rgrp:\n\tkfree(rdtgrp);\nout_unlock:\n\trdtgroup_kn_unlock(parent_kn);\n\treturn ret;\n}\n\nstatic void mkdir_rdt_prepare_clean(struct rdtgroup *rgrp)\n{\n\tkernfs_remove(rgrp->kn);\n\tfree_rmid(rgrp->mon.rmid);\n\trdtgroup_remove(rgrp);\n}\n\n \nstatic int rdtgroup_mkdir_mon(struct kernfs_node *parent_kn,\n\t\t\t      const char *name, umode_t mode)\n{\n\tstruct rdtgroup *rdtgrp, *prgrp;\n\tint ret;\n\n\tret = mkdir_rdt_prepare(parent_kn, name, mode, RDTMON_GROUP, &rdtgrp);\n\tif (ret)\n\t\treturn ret;\n\n\tprgrp = rdtgrp->mon.parent;\n\trdtgrp->closid = prgrp->closid;\n\n\t \n\tlist_add_tail(&rdtgrp->mon.crdtgrp_list, &prgrp->mon.crdtgrp_list);\n\n\trdtgroup_kn_unlock(parent_kn);\n\treturn ret;\n}\n\n \nstatic int rdtgroup_mkdir_ctrl_mon(struct kernfs_node *parent_kn,\n\t\t\t\t   const char *name, umode_t mode)\n{\n\tstruct rdtgroup *rdtgrp;\n\tstruct kernfs_node *kn;\n\tu32 closid;\n\tint ret;\n\n\tret = mkdir_rdt_prepare(parent_kn, name, mode, RDTCTRL_GROUP, &rdtgrp);\n\tif (ret)\n\t\treturn ret;\n\n\tkn = rdtgrp->kn;\n\tret = closid_alloc();\n\tif (ret < 0) {\n\t\trdt_last_cmd_puts(\"Out of CLOSIDs\\n\");\n\t\tgoto out_common_fail;\n\t}\n\tclosid = ret;\n\tret = 0;\n\n\trdtgrp->closid = closid;\n\tret = rdtgroup_init_alloc(rdtgrp);\n\tif (ret < 0)\n\t\tgoto out_id_free;\n\n\tlist_add(&rdtgrp->rdtgroup_list, &rdt_all_groups);\n\n\tif (rdt_mon_capable) {\n\t\t \n\t\tret = mongroup_create_dir(kn, rdtgrp, \"mon_groups\", NULL);\n\t\tif (ret) {\n\t\t\trdt_last_cmd_puts(\"kernfs subdir error\\n\");\n\t\t\tgoto out_del_list;\n\t\t}\n\t}\n\n\tgoto out_unlock;\n\nout_del_list:\n\tlist_del(&rdtgrp->rdtgroup_list);\nout_id_free:\n\tclosid_free(closid);\nout_common_fail:\n\tmkdir_rdt_prepare_clean(rdtgrp);\nout_unlock:\n\trdtgroup_kn_unlock(parent_kn);\n\treturn ret;\n}\n\n \nstatic bool is_mon_groups(struct kernfs_node *kn, const char *name)\n{\n\treturn (!strcmp(kn->name, \"mon_groups\") &&\n\t\tstrcmp(name, \"mon_groups\"));\n}\n\nstatic int rdtgroup_mkdir(struct kernfs_node *parent_kn, const char *name,\n\t\t\t  umode_t mode)\n{\n\t \n\tif (strchr(name, '\\n'))\n\t\treturn -EINVAL;\n\n\t \n\tif (rdt_alloc_capable && parent_kn == rdtgroup_default.kn)\n\t\treturn rdtgroup_mkdir_ctrl_mon(parent_kn, name, mode);\n\n\t \n\tif (rdt_mon_capable && is_mon_groups(parent_kn, name))\n\t\treturn rdtgroup_mkdir_mon(parent_kn, name, mode);\n\n\treturn -EPERM;\n}\n\nstatic int rdtgroup_rmdir_mon(struct rdtgroup *rdtgrp, cpumask_var_t tmpmask)\n{\n\tstruct rdtgroup *prdtgrp = rdtgrp->mon.parent;\n\tint cpu;\n\n\t \n\trdt_move_group_tasks(rdtgrp, prdtgrp, tmpmask);\n\n\t \n\tfor_each_cpu(cpu, &rdtgrp->cpu_mask)\n\t\tper_cpu(pqr_state.default_rmid, cpu) = prdtgrp->mon.rmid;\n\t \n\tcpumask_or(tmpmask, tmpmask, &rdtgrp->cpu_mask);\n\tupdate_closid_rmid(tmpmask, NULL);\n\n\trdtgrp->flags = RDT_DELETED;\n\tfree_rmid(rdtgrp->mon.rmid);\n\n\t \n\tWARN_ON(list_empty(&prdtgrp->mon.crdtgrp_list));\n\tlist_del(&rdtgrp->mon.crdtgrp_list);\n\n\tkernfs_remove(rdtgrp->kn);\n\n\treturn 0;\n}\n\nstatic int rdtgroup_ctrl_remove(struct rdtgroup *rdtgrp)\n{\n\trdtgrp->flags = RDT_DELETED;\n\tlist_del(&rdtgrp->rdtgroup_list);\n\n\tkernfs_remove(rdtgrp->kn);\n\treturn 0;\n}\n\nstatic int rdtgroup_rmdir_ctrl(struct rdtgroup *rdtgrp, cpumask_var_t tmpmask)\n{\n\tint cpu;\n\n\t \n\trdt_move_group_tasks(rdtgrp, &rdtgroup_default, tmpmask);\n\n\t \n\tcpumask_or(&rdtgroup_default.cpu_mask,\n\t\t   &rdtgroup_default.cpu_mask, &rdtgrp->cpu_mask);\n\n\t \n\tfor_each_cpu(cpu, &rdtgrp->cpu_mask) {\n\t\tper_cpu(pqr_state.default_closid, cpu) = rdtgroup_default.closid;\n\t\tper_cpu(pqr_state.default_rmid, cpu) = rdtgroup_default.mon.rmid;\n\t}\n\n\t \n\tcpumask_or(tmpmask, tmpmask, &rdtgrp->cpu_mask);\n\tupdate_closid_rmid(tmpmask, NULL);\n\n\tclosid_free(rdtgrp->closid);\n\tfree_rmid(rdtgrp->mon.rmid);\n\n\trdtgroup_ctrl_remove(rdtgrp);\n\n\t \n\tfree_all_child_rdtgrp(rdtgrp);\n\n\treturn 0;\n}\n\nstatic int rdtgroup_rmdir(struct kernfs_node *kn)\n{\n\tstruct kernfs_node *parent_kn = kn->parent;\n\tstruct rdtgroup *rdtgrp;\n\tcpumask_var_t tmpmask;\n\tint ret = 0;\n\n\tif (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL))\n\t\treturn -ENOMEM;\n\n\trdtgrp = rdtgroup_kn_lock_live(kn);\n\tif (!rdtgrp) {\n\t\tret = -EPERM;\n\t\tgoto out;\n\t}\n\n\t \n\tif (rdtgrp->type == RDTCTRL_GROUP && parent_kn == rdtgroup_default.kn &&\n\t    rdtgrp != &rdtgroup_default) {\n\t\tif (rdtgrp->mode == RDT_MODE_PSEUDO_LOCKSETUP ||\n\t\t    rdtgrp->mode == RDT_MODE_PSEUDO_LOCKED) {\n\t\t\tret = rdtgroup_ctrl_remove(rdtgrp);\n\t\t} else {\n\t\t\tret = rdtgroup_rmdir_ctrl(rdtgrp, tmpmask);\n\t\t}\n\t} else if (rdtgrp->type == RDTMON_GROUP &&\n\t\t is_mon_groups(parent_kn, kn->name)) {\n\t\tret = rdtgroup_rmdir_mon(rdtgrp, tmpmask);\n\t} else {\n\t\tret = -EPERM;\n\t}\n\nout:\n\trdtgroup_kn_unlock(kn);\n\tfree_cpumask_var(tmpmask);\n\treturn ret;\n}\n\n \nstatic void mongrp_reparent(struct rdtgroup *rdtgrp,\n\t\t\t    struct rdtgroup *new_prdtgrp,\n\t\t\t    cpumask_var_t cpus)\n{\n\tstruct rdtgroup *prdtgrp = rdtgrp->mon.parent;\n\n\tWARN_ON(rdtgrp->type != RDTMON_GROUP);\n\tWARN_ON(new_prdtgrp->type != RDTCTRL_GROUP);\n\n\t \n\tif (prdtgrp == new_prdtgrp)\n\t\treturn;\n\n\tWARN_ON(list_empty(&prdtgrp->mon.crdtgrp_list));\n\tlist_move_tail(&rdtgrp->mon.crdtgrp_list,\n\t\t       &new_prdtgrp->mon.crdtgrp_list);\n\n\trdtgrp->mon.parent = new_prdtgrp;\n\trdtgrp->closid = new_prdtgrp->closid;\n\n\t \n\trdt_move_group_tasks(rdtgrp, rdtgrp, cpus);\n\n\tupdate_closid_rmid(cpus, NULL);\n}\n\nstatic int rdtgroup_rename(struct kernfs_node *kn,\n\t\t\t   struct kernfs_node *new_parent, const char *new_name)\n{\n\tstruct rdtgroup *new_prdtgrp;\n\tstruct rdtgroup *rdtgrp;\n\tcpumask_var_t tmpmask;\n\tint ret;\n\n\trdtgrp = kernfs_to_rdtgroup(kn);\n\tnew_prdtgrp = kernfs_to_rdtgroup(new_parent);\n\tif (!rdtgrp || !new_prdtgrp)\n\t\treturn -ENOENT;\n\n\t \n\trdtgroup_kn_get(rdtgrp, kn);\n\trdtgroup_kn_get(new_prdtgrp, new_parent);\n\n\tmutex_lock(&rdtgroup_mutex);\n\n\trdt_last_cmd_clear();\n\n\t \n\tif (kernfs_type(kn) != KERNFS_DIR ||\n\t    kernfs_type(new_parent) != KERNFS_DIR) {\n\t\trdt_last_cmd_puts(\"Source and destination must be directories\");\n\t\tret = -EPERM;\n\t\tgoto out;\n\t}\n\n\tif ((rdtgrp->flags & RDT_DELETED) || (new_prdtgrp->flags & RDT_DELETED)) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tif (rdtgrp->type != RDTMON_GROUP || !kn->parent ||\n\t    !is_mon_groups(kn->parent, kn->name)) {\n\t\trdt_last_cmd_puts(\"Source must be a MON group\\n\");\n\t\tret = -EPERM;\n\t\tgoto out;\n\t}\n\n\tif (!is_mon_groups(new_parent, new_name)) {\n\t\trdt_last_cmd_puts(\"Destination must be a mon_groups subdirectory\\n\");\n\t\tret = -EPERM;\n\t\tgoto out;\n\t}\n\n\t \n\tif (!cpumask_empty(&rdtgrp->cpu_mask) &&\n\t    rdtgrp->mon.parent != new_prdtgrp) {\n\t\trdt_last_cmd_puts(\"Cannot move a MON group that monitors CPUs\\n\");\n\t\tret = -EPERM;\n\t\tgoto out;\n\t}\n\n\t \n\tif (!zalloc_cpumask_var(&tmpmask, GFP_KERNEL)) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t \n\tret = kernfs_rename(kn, new_parent, new_name);\n\tif (!ret)\n\t\tmongrp_reparent(rdtgrp, new_prdtgrp, tmpmask);\n\n\tfree_cpumask_var(tmpmask);\n\nout:\n\tmutex_unlock(&rdtgroup_mutex);\n\trdtgroup_kn_put(rdtgrp, kn);\n\trdtgroup_kn_put(new_prdtgrp, new_parent);\n\treturn ret;\n}\n\nstatic int rdtgroup_show_options(struct seq_file *seq, struct kernfs_root *kf)\n{\n\tif (resctrl_arch_get_cdp_enabled(RDT_RESOURCE_L3))\n\t\tseq_puts(seq, \",cdp\");\n\n\tif (resctrl_arch_get_cdp_enabled(RDT_RESOURCE_L2))\n\t\tseq_puts(seq, \",cdpl2\");\n\n\tif (is_mba_sc(&rdt_resources_all[RDT_RESOURCE_MBA].r_resctrl))\n\t\tseq_puts(seq, \",mba_MBps\");\n\n\treturn 0;\n}\n\nstatic struct kernfs_syscall_ops rdtgroup_kf_syscall_ops = {\n\t.mkdir\t\t= rdtgroup_mkdir,\n\t.rmdir\t\t= rdtgroup_rmdir,\n\t.rename\t\t= rdtgroup_rename,\n\t.show_options\t= rdtgroup_show_options,\n};\n\nstatic int __init rdtgroup_setup_root(void)\n{\n\tint ret;\n\n\trdt_root = kernfs_create_root(&rdtgroup_kf_syscall_ops,\n\t\t\t\t      KERNFS_ROOT_CREATE_DEACTIVATED |\n\t\t\t\t      KERNFS_ROOT_EXTRA_OPEN_PERM_CHECK,\n\t\t\t\t      &rdtgroup_default);\n\tif (IS_ERR(rdt_root))\n\t\treturn PTR_ERR(rdt_root);\n\n\tmutex_lock(&rdtgroup_mutex);\n\n\trdtgroup_default.closid = 0;\n\trdtgroup_default.mon.rmid = 0;\n\trdtgroup_default.type = RDTCTRL_GROUP;\n\tINIT_LIST_HEAD(&rdtgroup_default.mon.crdtgrp_list);\n\n\tlist_add(&rdtgroup_default.rdtgroup_list, &rdt_all_groups);\n\n\tret = rdtgroup_add_files(kernfs_root_to_node(rdt_root), RF_CTRL_BASE);\n\tif (ret) {\n\t\tkernfs_destroy_root(rdt_root);\n\t\tgoto out;\n\t}\n\n\trdtgroup_default.kn = kernfs_root_to_node(rdt_root);\n\tkernfs_activate(rdtgroup_default.kn);\n\nout:\n\tmutex_unlock(&rdtgroup_mutex);\n\n\treturn ret;\n}\n\nstatic void domain_destroy_mon_state(struct rdt_domain *d)\n{\n\tbitmap_free(d->rmid_busy_llc);\n\tkfree(d->mbm_total);\n\tkfree(d->mbm_local);\n}\n\nvoid resctrl_offline_domain(struct rdt_resource *r, struct rdt_domain *d)\n{\n\tlockdep_assert_held(&rdtgroup_mutex);\n\n\tif (supports_mba_mbps() && r->rid == RDT_RESOURCE_MBA)\n\t\tmba_sc_domain_destroy(r, d);\n\n\tif (!r->mon_capable)\n\t\treturn;\n\n\t \n\tif (static_branch_unlikely(&rdt_mon_enable_key))\n\t\trmdir_mondata_subdir_allrdtgrp(r, d->id);\n\n\tif (is_mbm_enabled())\n\t\tcancel_delayed_work(&d->mbm_over);\n\tif (is_llc_occupancy_enabled() && has_busy_rmid(r, d)) {\n\t\t \n\t\t__check_limbo(d, true);\n\t\tcancel_delayed_work(&d->cqm_limbo);\n\t}\n\n\tdomain_destroy_mon_state(d);\n}\n\nstatic int domain_setup_mon_state(struct rdt_resource *r, struct rdt_domain *d)\n{\n\tsize_t tsize;\n\n\tif (is_llc_occupancy_enabled()) {\n\t\td->rmid_busy_llc = bitmap_zalloc(r->num_rmid, GFP_KERNEL);\n\t\tif (!d->rmid_busy_llc)\n\t\t\treturn -ENOMEM;\n\t}\n\tif (is_mbm_total_enabled()) {\n\t\ttsize = sizeof(*d->mbm_total);\n\t\td->mbm_total = kcalloc(r->num_rmid, tsize, GFP_KERNEL);\n\t\tif (!d->mbm_total) {\n\t\t\tbitmap_free(d->rmid_busy_llc);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\tif (is_mbm_local_enabled()) {\n\t\ttsize = sizeof(*d->mbm_local);\n\t\td->mbm_local = kcalloc(r->num_rmid, tsize, GFP_KERNEL);\n\t\tif (!d->mbm_local) {\n\t\t\tbitmap_free(d->rmid_busy_llc);\n\t\t\tkfree(d->mbm_total);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint resctrl_online_domain(struct rdt_resource *r, struct rdt_domain *d)\n{\n\tint err;\n\n\tlockdep_assert_held(&rdtgroup_mutex);\n\n\tif (supports_mba_mbps() && r->rid == RDT_RESOURCE_MBA)\n\t\t \n\t\treturn mba_sc_domain_allocate(r, d);\n\n\tif (!r->mon_capable)\n\t\treturn 0;\n\n\terr = domain_setup_mon_state(r, d);\n\tif (err)\n\t\treturn err;\n\n\tif (is_mbm_enabled()) {\n\t\tINIT_DELAYED_WORK(&d->mbm_over, mbm_handle_overflow);\n\t\tmbm_setup_overflow_handler(d, MBM_OVERFLOW_INTERVAL);\n\t}\n\n\tif (is_llc_occupancy_enabled())\n\t\tINIT_DELAYED_WORK(&d->cqm_limbo, cqm_handle_limbo);\n\n\t \n\tif (static_branch_unlikely(&rdt_mon_enable_key))\n\t\tmkdir_mondata_subdir_allrdtgrp(r, d);\n\n\treturn 0;\n}\n\n \nint __init rdtgroup_init(void)\n{\n\tint ret = 0;\n\n\tseq_buf_init(&last_cmd_status, last_cmd_status_buf,\n\t\t     sizeof(last_cmd_status_buf));\n\n\tret = rdtgroup_setup_root();\n\tif (ret)\n\t\treturn ret;\n\n\tret = sysfs_create_mount_point(fs_kobj, \"resctrl\");\n\tif (ret)\n\t\tgoto cleanup_root;\n\n\tret = register_filesystem(&rdt_fs_type);\n\tif (ret)\n\t\tgoto cleanup_mountpoint;\n\n\t \n\tdebugfs_resctrl = debugfs_create_dir(\"resctrl\", NULL);\n\n\treturn 0;\n\ncleanup_mountpoint:\n\tsysfs_remove_mount_point(fs_kobj, \"resctrl\");\ncleanup_root:\n\tkernfs_destroy_root(rdt_root);\n\n\treturn ret;\n}\n\nvoid __exit rdtgroup_exit(void)\n{\n\tdebugfs_remove_recursive(debugfs_resctrl);\n\tunregister_filesystem(&rdt_fs_type);\n\tsysfs_remove_mount_point(fs_kobj, \"resctrl\");\n\tkernfs_destroy_root(rdt_root);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}