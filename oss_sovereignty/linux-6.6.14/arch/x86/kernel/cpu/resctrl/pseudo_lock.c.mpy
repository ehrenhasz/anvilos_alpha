{
  "module_name": "pseudo_lock.c",
  "hash_id": "5cc4f89abca7e98049ff3efd836dc898e01d0c637b27c391ac8dc0f15179c531",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kernel/cpu/resctrl/pseudo_lock.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)\tKBUILD_MODNAME \": \" fmt\n\n#include <linux/cacheinfo.h>\n#include <linux/cpu.h>\n#include <linux/cpumask.h>\n#include <linux/debugfs.h>\n#include <linux/kthread.h>\n#include <linux/mman.h>\n#include <linux/perf_event.h>\n#include <linux/pm_qos.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n\n#include <asm/cacheflush.h>\n#include <asm/intel-family.h>\n#include <asm/resctrl.h>\n#include <asm/perf_event.h>\n\n#include \"../../events/perf_event.h\"  \n#include \"internal.h\"\n\n#define CREATE_TRACE_POINTS\n#include \"pseudo_lock_event.h\"\n\n \nstatic u64 prefetch_disable_bits;\n\n \nstatic unsigned int pseudo_lock_major;\nstatic unsigned long pseudo_lock_minor_avail = GENMASK(MINORBITS, 0);\n\nstatic char *pseudo_lock_devnode(const struct device *dev, umode_t *mode)\n{\n\tconst struct rdtgroup *rdtgrp;\n\n\trdtgrp = dev_get_drvdata(dev);\n\tif (mode)\n\t\t*mode = 0600;\n\treturn kasprintf(GFP_KERNEL, \"pseudo_lock/%s\", rdtgrp->kn->name);\n}\n\nstatic const struct class pseudo_lock_class = {\n\t.name = \"pseudo_lock\",\n\t.devnode = pseudo_lock_devnode,\n};\n\n \nstatic u64 get_prefetch_disable_bits(void)\n{\n\tif (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL ||\n\t    boot_cpu_data.x86 != 6)\n\t\treturn 0;\n\n\tswitch (boot_cpu_data.x86_model) {\n\tcase INTEL_FAM6_BROADWELL_X:\n\t\t \n\t\treturn 0xF;\n\tcase INTEL_FAM6_ATOM_GOLDMONT:\n\tcase INTEL_FAM6_ATOM_GOLDMONT_PLUS:\n\t\t \n\t\treturn 0x5;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int pseudo_lock_minor_get(unsigned int *minor)\n{\n\tunsigned long first_bit;\n\n\tfirst_bit = find_first_bit(&pseudo_lock_minor_avail, MINORBITS);\n\n\tif (first_bit == MINORBITS)\n\t\treturn -ENOSPC;\n\n\t__clear_bit(first_bit, &pseudo_lock_minor_avail);\n\t*minor = first_bit;\n\n\treturn 0;\n}\n\n \nstatic void pseudo_lock_minor_release(unsigned int minor)\n{\n\t__set_bit(minor, &pseudo_lock_minor_avail);\n}\n\n \nstatic struct rdtgroup *region_find_by_minor(unsigned int minor)\n{\n\tstruct rdtgroup *rdtgrp, *rdtgrp_match = NULL;\n\n\tlist_for_each_entry(rdtgrp, &rdt_all_groups, rdtgroup_list) {\n\t\tif (rdtgrp->plr && rdtgrp->plr->minor == minor) {\n\t\t\trdtgrp_match = rdtgrp;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn rdtgrp_match;\n}\n\n \nstruct pseudo_lock_pm_req {\n\tstruct list_head list;\n\tstruct dev_pm_qos_request req;\n};\n\nstatic void pseudo_lock_cstates_relax(struct pseudo_lock_region *plr)\n{\n\tstruct pseudo_lock_pm_req *pm_req, *next;\n\n\tlist_for_each_entry_safe(pm_req, next, &plr->pm_reqs, list) {\n\t\tdev_pm_qos_remove_request(&pm_req->req);\n\t\tlist_del(&pm_req->list);\n\t\tkfree(pm_req);\n\t}\n}\n\n \nstatic int pseudo_lock_cstates_constrain(struct pseudo_lock_region *plr)\n{\n\tstruct pseudo_lock_pm_req *pm_req;\n\tint cpu;\n\tint ret;\n\n\tfor_each_cpu(cpu, &plr->d->cpu_mask) {\n\t\tpm_req = kzalloc(sizeof(*pm_req), GFP_KERNEL);\n\t\tif (!pm_req) {\n\t\t\trdt_last_cmd_puts(\"Failure to allocate memory for PM QoS\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_err;\n\t\t}\n\t\tret = dev_pm_qos_add_request(get_cpu_device(cpu),\n\t\t\t\t\t     &pm_req->req,\n\t\t\t\t\t     DEV_PM_QOS_RESUME_LATENCY,\n\t\t\t\t\t     30);\n\t\tif (ret < 0) {\n\t\t\trdt_last_cmd_printf(\"Failed to add latency req CPU%d\\n\",\n\t\t\t\t\t    cpu);\n\t\t\tkfree(pm_req);\n\t\t\tret = -1;\n\t\t\tgoto out_err;\n\t\t}\n\t\tlist_add(&pm_req->list, &plr->pm_reqs);\n\t}\n\n\treturn 0;\n\nout_err:\n\tpseudo_lock_cstates_relax(plr);\n\treturn ret;\n}\n\n \nstatic void pseudo_lock_region_clear(struct pseudo_lock_region *plr)\n{\n\tplr->size = 0;\n\tplr->line_size = 0;\n\tkfree(plr->kmem);\n\tplr->kmem = NULL;\n\tplr->s = NULL;\n\tif (plr->d)\n\t\tplr->d->plr = NULL;\n\tplr->d = NULL;\n\tplr->cbm = 0;\n\tplr->debugfs_dir = NULL;\n}\n\n \nstatic int pseudo_lock_region_init(struct pseudo_lock_region *plr)\n{\n\tstruct cpu_cacheinfo *ci;\n\tint ret;\n\tint i;\n\n\t \n\tplr->cpu = cpumask_first(&plr->d->cpu_mask);\n\n\tif (!cpu_online(plr->cpu)) {\n\t\trdt_last_cmd_printf(\"CPU %u associated with cache not online\\n\",\n\t\t\t\t    plr->cpu);\n\t\tret = -ENODEV;\n\t\tgoto out_region;\n\t}\n\n\tci = get_cpu_cacheinfo(plr->cpu);\n\n\tplr->size = rdtgroup_cbm_to_size(plr->s->res, plr->d, plr->cbm);\n\n\tfor (i = 0; i < ci->num_leaves; i++) {\n\t\tif (ci->info_list[i].level == plr->s->res->cache_level) {\n\t\t\tplr->line_size = ci->info_list[i].coherency_line_size;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\tret = -1;\n\trdt_last_cmd_puts(\"Unable to determine cache line size\\n\");\nout_region:\n\tpseudo_lock_region_clear(plr);\n\treturn ret;\n}\n\n \nstatic int pseudo_lock_init(struct rdtgroup *rdtgrp)\n{\n\tstruct pseudo_lock_region *plr;\n\n\tplr = kzalloc(sizeof(*plr), GFP_KERNEL);\n\tif (!plr)\n\t\treturn -ENOMEM;\n\n\tinit_waitqueue_head(&plr->lock_thread_wq);\n\tINIT_LIST_HEAD(&plr->pm_reqs);\n\trdtgrp->plr = plr;\n\treturn 0;\n}\n\n \nstatic int pseudo_lock_region_alloc(struct pseudo_lock_region *plr)\n{\n\tint ret;\n\n\tret = pseudo_lock_region_init(plr);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tif (plr->size > KMALLOC_MAX_SIZE) {\n\t\trdt_last_cmd_puts(\"Requested region exceeds maximum size\\n\");\n\t\tret = -E2BIG;\n\t\tgoto out_region;\n\t}\n\n\tplr->kmem = kzalloc(plr->size, GFP_KERNEL);\n\tif (!plr->kmem) {\n\t\trdt_last_cmd_puts(\"Unable to allocate memory\\n\");\n\t\tret = -ENOMEM;\n\t\tgoto out_region;\n\t}\n\n\tret = 0;\n\tgoto out;\nout_region:\n\tpseudo_lock_region_clear(plr);\nout:\n\treturn ret;\n}\n\n \nstatic void pseudo_lock_free(struct rdtgroup *rdtgrp)\n{\n\tpseudo_lock_region_clear(rdtgrp->plr);\n\tkfree(rdtgrp->plr);\n\trdtgrp->plr = NULL;\n}\n\n \nstatic int pseudo_lock_fn(void *_rdtgrp)\n{\n\tstruct rdtgroup *rdtgrp = _rdtgrp;\n\tstruct pseudo_lock_region *plr = rdtgrp->plr;\n\tu32 rmid_p, closid_p;\n\tunsigned long i;\n\tu64 saved_msr;\n#ifdef CONFIG_KASAN\n\t \n\tunsigned int line_size;\n\tunsigned int size;\n\tvoid *mem_r;\n#else\n\tregister unsigned int line_size asm(\"esi\");\n\tregister unsigned int size asm(\"edi\");\n\tregister void *mem_r asm(_ASM_BX);\n#endif  \n\n\t \n\tnative_wbinvd();\n\n\t \n\tlocal_irq_disable();\n\n\t \n\tsaved_msr = __rdmsr(MSR_MISC_FEATURE_CONTROL);\n\t__wrmsr(MSR_MISC_FEATURE_CONTROL, prefetch_disable_bits, 0x0);\n\tclosid_p = this_cpu_read(pqr_state.cur_closid);\n\trmid_p = this_cpu_read(pqr_state.cur_rmid);\n\tmem_r = plr->kmem;\n\tsize = plr->size;\n\tline_size = plr->line_size;\n\t \n\t__wrmsr(MSR_IA32_PQR_ASSOC, rmid_p, rdtgrp->closid);\n\t \n\tfor (i = 0; i < size; i += PAGE_SIZE) {\n\t\t \n\t\trmb();\n\t\tasm volatile(\"mov (%0,%1,1), %%eax\\n\\t\"\n\t\t\t:\n\t\t\t: \"r\" (mem_r), \"r\" (i)\n\t\t\t: \"%eax\", \"memory\");\n\t}\n\tfor (i = 0; i < size; i += line_size) {\n\t\t \n\t\trmb();\n\t\tasm volatile(\"mov (%0,%1,1), %%eax\\n\\t\"\n\t\t\t:\n\t\t\t: \"r\" (mem_r), \"r\" (i)\n\t\t\t: \"%eax\", \"memory\");\n\t}\n\t \n\t__wrmsr(MSR_IA32_PQR_ASSOC, rmid_p, closid_p);\n\n\t \n\twrmsrl(MSR_MISC_FEATURE_CONTROL, saved_msr);\n\tlocal_irq_enable();\n\n\tplr->thread_done = 1;\n\twake_up_interruptible(&plr->lock_thread_wq);\n\treturn 0;\n}\n\n \nstatic int rdtgroup_monitor_in_progress(struct rdtgroup *rdtgrp)\n{\n\treturn !list_empty(&rdtgrp->mon.crdtgrp_list);\n}\n\n \nstatic int rdtgroup_locksetup_user_restrict(struct rdtgroup *rdtgrp)\n{\n\tint ret;\n\n\tret = rdtgroup_kn_mode_restrict(rdtgrp, \"tasks\");\n\tif (ret)\n\t\treturn ret;\n\n\tret = rdtgroup_kn_mode_restrict(rdtgrp, \"cpus\");\n\tif (ret)\n\t\tgoto err_tasks;\n\n\tret = rdtgroup_kn_mode_restrict(rdtgrp, \"cpus_list\");\n\tif (ret)\n\t\tgoto err_cpus;\n\n\tif (rdt_mon_capable) {\n\t\tret = rdtgroup_kn_mode_restrict(rdtgrp, \"mon_groups\");\n\t\tif (ret)\n\t\t\tgoto err_cpus_list;\n\t}\n\n\tret = 0;\n\tgoto out;\n\nerr_cpus_list:\n\trdtgroup_kn_mode_restore(rdtgrp, \"cpus_list\", 0777);\nerr_cpus:\n\trdtgroup_kn_mode_restore(rdtgrp, \"cpus\", 0777);\nerr_tasks:\n\trdtgroup_kn_mode_restore(rdtgrp, \"tasks\", 0777);\nout:\n\treturn ret;\n}\n\n \nstatic int rdtgroup_locksetup_user_restore(struct rdtgroup *rdtgrp)\n{\n\tint ret;\n\n\tret = rdtgroup_kn_mode_restore(rdtgrp, \"tasks\", 0777);\n\tif (ret)\n\t\treturn ret;\n\n\tret = rdtgroup_kn_mode_restore(rdtgrp, \"cpus\", 0777);\n\tif (ret)\n\t\tgoto err_tasks;\n\n\tret = rdtgroup_kn_mode_restore(rdtgrp, \"cpus_list\", 0777);\n\tif (ret)\n\t\tgoto err_cpus;\n\n\tif (rdt_mon_capable) {\n\t\tret = rdtgroup_kn_mode_restore(rdtgrp, \"mon_groups\", 0777);\n\t\tif (ret)\n\t\t\tgoto err_cpus_list;\n\t}\n\n\tret = 0;\n\tgoto out;\n\nerr_cpus_list:\n\trdtgroup_kn_mode_restrict(rdtgrp, \"cpus_list\");\nerr_cpus:\n\trdtgroup_kn_mode_restrict(rdtgrp, \"cpus\");\nerr_tasks:\n\trdtgroup_kn_mode_restrict(rdtgrp, \"tasks\");\nout:\n\treturn ret;\n}\n\n \nint rdtgroup_locksetup_enter(struct rdtgroup *rdtgrp)\n{\n\tint ret;\n\n\t \n\tif (rdtgrp == &rdtgroup_default) {\n\t\trdt_last_cmd_puts(\"Cannot pseudo-lock default group\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (resctrl_arch_get_cdp_enabled(RDT_RESOURCE_L3) ||\n\t    resctrl_arch_get_cdp_enabled(RDT_RESOURCE_L2)) {\n\t\trdt_last_cmd_puts(\"CDP enabled\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tprefetch_disable_bits = get_prefetch_disable_bits();\n\tif (prefetch_disable_bits == 0) {\n\t\trdt_last_cmd_puts(\"Pseudo-locking not supported\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (rdtgroup_monitor_in_progress(rdtgrp)) {\n\t\trdt_last_cmd_puts(\"Monitoring in progress\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (rdtgroup_tasks_assigned(rdtgrp)) {\n\t\trdt_last_cmd_puts(\"Tasks assigned to resource group\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!cpumask_empty(&rdtgrp->cpu_mask)) {\n\t\trdt_last_cmd_puts(\"CPUs assigned to resource group\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (rdtgroup_locksetup_user_restrict(rdtgrp)) {\n\t\trdt_last_cmd_puts(\"Unable to modify resctrl permissions\\n\");\n\t\treturn -EIO;\n\t}\n\n\tret = pseudo_lock_init(rdtgrp);\n\tif (ret) {\n\t\trdt_last_cmd_puts(\"Unable to init pseudo-lock region\\n\");\n\t\tgoto out_release;\n\t}\n\n\t \n\tfree_rmid(rdtgrp->mon.rmid);\n\n\tret = 0;\n\tgoto out;\n\nout_release:\n\trdtgroup_locksetup_user_restore(rdtgrp);\nout:\n\treturn ret;\n}\n\n \nint rdtgroup_locksetup_exit(struct rdtgroup *rdtgrp)\n{\n\tint ret;\n\n\tif (rdt_mon_capable) {\n\t\tret = alloc_rmid();\n\t\tif (ret < 0) {\n\t\t\trdt_last_cmd_puts(\"Out of RMIDs\\n\");\n\t\t\treturn ret;\n\t\t}\n\t\trdtgrp->mon.rmid = ret;\n\t}\n\n\tret = rdtgroup_locksetup_user_restore(rdtgrp);\n\tif (ret) {\n\t\tfree_rmid(rdtgrp->mon.rmid);\n\t\treturn ret;\n\t}\n\n\tpseudo_lock_free(rdtgrp);\n\treturn 0;\n}\n\n \nbool rdtgroup_cbm_overlaps_pseudo_locked(struct rdt_domain *d, unsigned long cbm)\n{\n\tunsigned int cbm_len;\n\tunsigned long cbm_b;\n\n\tif (d->plr) {\n\t\tcbm_len = d->plr->s->res->cache.cbm_len;\n\t\tcbm_b = d->plr->cbm;\n\t\tif (bitmap_intersects(&cbm, &cbm_b, cbm_len))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nbool rdtgroup_pseudo_locked_in_hierarchy(struct rdt_domain *d)\n{\n\tcpumask_var_t cpu_with_psl;\n\tstruct rdt_resource *r;\n\tstruct rdt_domain *d_i;\n\tbool ret = false;\n\n\tif (!zalloc_cpumask_var(&cpu_with_psl, GFP_KERNEL))\n\t\treturn true;\n\n\t \n\tfor_each_alloc_capable_rdt_resource(r) {\n\t\tlist_for_each_entry(d_i, &r->domains, list) {\n\t\t\tif (d_i->plr)\n\t\t\t\tcpumask_or(cpu_with_psl, cpu_with_psl,\n\t\t\t\t\t   &d_i->cpu_mask);\n\t\t}\n\t}\n\n\t \n\tif (cpumask_intersects(&d->cpu_mask, cpu_with_psl))\n\t\tret = true;\n\n\tfree_cpumask_var(cpu_with_psl);\n\treturn ret;\n}\n\n \nstatic int measure_cycles_lat_fn(void *_plr)\n{\n\tstruct pseudo_lock_region *plr = _plr;\n\tu32 saved_low, saved_high;\n\tunsigned long i;\n\tu64 start, end;\n\tvoid *mem_r;\n\n\tlocal_irq_disable();\n\t \n\trdmsr(MSR_MISC_FEATURE_CONTROL, saved_low, saved_high);\n\twrmsr(MSR_MISC_FEATURE_CONTROL, prefetch_disable_bits, 0x0);\n\tmem_r = READ_ONCE(plr->kmem);\n\t \n\tstart = rdtsc_ordered();\n\tfor (i = 0; i < plr->size; i += 32) {\n\t\tstart = rdtsc_ordered();\n\t\tasm volatile(\"mov (%0,%1,1), %%eax\\n\\t\"\n\t\t\t     :\n\t\t\t     : \"r\" (mem_r), \"r\" (i)\n\t\t\t     : \"%eax\", \"memory\");\n\t\tend = rdtsc_ordered();\n\t\ttrace_pseudo_lock_mem_latency((u32)(end - start));\n\t}\n\twrmsr(MSR_MISC_FEATURE_CONTROL, saved_low, saved_high);\n\tlocal_irq_enable();\n\tplr->thread_done = 1;\n\twake_up_interruptible(&plr->lock_thread_wq);\n\treturn 0;\n}\n\n \nstatic struct perf_event_attr perf_miss_attr = {\n\t.type\t\t= PERF_TYPE_RAW,\n\t.size\t\t= sizeof(struct perf_event_attr),\n\t.pinned\t\t= 1,\n\t.disabled\t= 0,\n\t.exclude_user\t= 1,\n};\n\nstatic struct perf_event_attr perf_hit_attr = {\n\t.type\t\t= PERF_TYPE_RAW,\n\t.size\t\t= sizeof(struct perf_event_attr),\n\t.pinned\t\t= 1,\n\t.disabled\t= 0,\n\t.exclude_user\t= 1,\n};\n\nstruct residency_counts {\n\tu64 miss_before, hits_before;\n\tu64 miss_after,  hits_after;\n};\n\nstatic int measure_residency_fn(struct perf_event_attr *miss_attr,\n\t\t\t\tstruct perf_event_attr *hit_attr,\n\t\t\t\tstruct pseudo_lock_region *plr,\n\t\t\t\tstruct residency_counts *counts)\n{\n\tu64 hits_before = 0, hits_after = 0, miss_before = 0, miss_after = 0;\n\tstruct perf_event *miss_event, *hit_event;\n\tint hit_pmcnum, miss_pmcnum;\n\tu32 saved_low, saved_high;\n\tunsigned int line_size;\n\tunsigned int size;\n\tunsigned long i;\n\tvoid *mem_r;\n\tu64 tmp;\n\n\tmiss_event = perf_event_create_kernel_counter(miss_attr, plr->cpu,\n\t\t\t\t\t\t      NULL, NULL, NULL);\n\tif (IS_ERR(miss_event))\n\t\tgoto out;\n\n\thit_event = perf_event_create_kernel_counter(hit_attr, plr->cpu,\n\t\t\t\t\t\t     NULL, NULL, NULL);\n\tif (IS_ERR(hit_event))\n\t\tgoto out_miss;\n\n\tlocal_irq_disable();\n\t \n\tif (perf_event_read_local(miss_event, &tmp, NULL, NULL)) {\n\t\tlocal_irq_enable();\n\t\tgoto out_hit;\n\t}\n\tif (perf_event_read_local(hit_event, &tmp, NULL, NULL)) {\n\t\tlocal_irq_enable();\n\t\tgoto out_hit;\n\t}\n\n\t \n\trdmsr(MSR_MISC_FEATURE_CONTROL, saved_low, saved_high);\n\twrmsr(MSR_MISC_FEATURE_CONTROL, prefetch_disable_bits, 0x0);\n\n\t \n\t \n\tmiss_pmcnum = x86_perf_rdpmc_index(miss_event);\n\thit_pmcnum = x86_perf_rdpmc_index(hit_event);\n\tline_size = READ_ONCE(plr->line_size);\n\tmem_r = READ_ONCE(plr->kmem);\n\tsize = READ_ONCE(plr->size);\n\n\t \n\trdpmcl(hit_pmcnum, hits_before);\n\trdpmcl(miss_pmcnum, miss_before);\n\t \n\trmb();\n\trdpmcl(hit_pmcnum, hits_before);\n\trdpmcl(miss_pmcnum, miss_before);\n\t \n\trmb();\n\tfor (i = 0; i < size; i += line_size) {\n\t\t \n\t\trmb();\n\t\tasm volatile(\"mov (%0,%1,1), %%eax\\n\\t\"\n\t\t\t     :\n\t\t\t     : \"r\" (mem_r), \"r\" (i)\n\t\t\t     : \"%eax\", \"memory\");\n\t}\n\t \n\trmb();\n\trdpmcl(hit_pmcnum, hits_after);\n\trdpmcl(miss_pmcnum, miss_after);\n\t \n\trmb();\n\t \n\twrmsr(MSR_MISC_FEATURE_CONTROL, saved_low, saved_high);\n\tlocal_irq_enable();\nout_hit:\n\tperf_event_release_kernel(hit_event);\nout_miss:\n\tperf_event_release_kernel(miss_event);\nout:\n\t \n\tcounts->miss_before = miss_before;\n\tcounts->hits_before = hits_before;\n\tcounts->miss_after  = miss_after;\n\tcounts->hits_after  = hits_after;\n\treturn 0;\n}\n\nstatic int measure_l2_residency(void *_plr)\n{\n\tstruct pseudo_lock_region *plr = _plr;\n\tstruct residency_counts counts = {0};\n\n\t \n\tswitch (boot_cpu_data.x86_model) {\n\tcase INTEL_FAM6_ATOM_GOLDMONT:\n\tcase INTEL_FAM6_ATOM_GOLDMONT_PLUS:\n\t\tperf_miss_attr.config = X86_CONFIG(.event = 0xd1,\n\t\t\t\t\t\t   .umask = 0x10);\n\t\tperf_hit_attr.config = X86_CONFIG(.event = 0xd1,\n\t\t\t\t\t\t  .umask = 0x2);\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tmeasure_residency_fn(&perf_miss_attr, &perf_hit_attr, plr, &counts);\n\t \n\ttrace_pseudo_lock_l2(counts.hits_after - counts.hits_before,\n\t\t\t     counts.miss_after - counts.miss_before);\nout:\n\tplr->thread_done = 1;\n\twake_up_interruptible(&plr->lock_thread_wq);\n\treturn 0;\n}\n\nstatic int measure_l3_residency(void *_plr)\n{\n\tstruct pseudo_lock_region *plr = _plr;\n\tstruct residency_counts counts = {0};\n\n\t \n\n\tswitch (boot_cpu_data.x86_model) {\n\tcase INTEL_FAM6_BROADWELL_X:\n\t\t \n\t\tperf_hit_attr.config = X86_CONFIG(.event = 0x2e,\n\t\t\t\t\t\t  .umask = 0x4f);\n\t\tperf_miss_attr.config = X86_CONFIG(.event = 0x2e,\n\t\t\t\t\t\t   .umask = 0x41);\n\t\tbreak;\n\tdefault:\n\t\tgoto out;\n\t}\n\n\tmeasure_residency_fn(&perf_miss_attr, &perf_hit_attr, plr, &counts);\n\t \n\n\tcounts.miss_after -= counts.miss_before;\n\tif (boot_cpu_data.x86_model == INTEL_FAM6_BROADWELL_X) {\n\t\t \n\t\t \n\t\tcounts.hits_after -= counts.hits_before;\n\t\t \n\t\tcounts.hits_after -= min(counts.miss_after, counts.hits_after);\n\t} else {\n\t\tcounts.hits_after -= counts.hits_before;\n\t}\n\n\ttrace_pseudo_lock_l3(counts.hits_after, counts.miss_after);\nout:\n\tplr->thread_done = 1;\n\twake_up_interruptible(&plr->lock_thread_wq);\n\treturn 0;\n}\n\n \nstatic int pseudo_lock_measure_cycles(struct rdtgroup *rdtgrp, int sel)\n{\n\tstruct pseudo_lock_region *plr = rdtgrp->plr;\n\tstruct task_struct *thread;\n\tunsigned int cpu;\n\tint ret = -1;\n\n\tcpus_read_lock();\n\tmutex_lock(&rdtgroup_mutex);\n\n\tif (rdtgrp->flags & RDT_DELETED) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tif (!plr->d) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tplr->thread_done = 0;\n\tcpu = cpumask_first(&plr->d->cpu_mask);\n\tif (!cpu_online(cpu)) {\n\t\tret = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tplr->cpu = cpu;\n\n\tif (sel == 1)\n\t\tthread = kthread_create_on_node(measure_cycles_lat_fn, plr,\n\t\t\t\t\t\tcpu_to_node(cpu),\n\t\t\t\t\t\t\"pseudo_lock_measure/%u\",\n\t\t\t\t\t\tcpu);\n\telse if (sel == 2)\n\t\tthread = kthread_create_on_node(measure_l2_residency, plr,\n\t\t\t\t\t\tcpu_to_node(cpu),\n\t\t\t\t\t\t\"pseudo_lock_measure/%u\",\n\t\t\t\t\t\tcpu);\n\telse if (sel == 3)\n\t\tthread = kthread_create_on_node(measure_l3_residency, plr,\n\t\t\t\t\t\tcpu_to_node(cpu),\n\t\t\t\t\t\t\"pseudo_lock_measure/%u\",\n\t\t\t\t\t\tcpu);\n\telse\n\t\tgoto out;\n\n\tif (IS_ERR(thread)) {\n\t\tret = PTR_ERR(thread);\n\t\tgoto out;\n\t}\n\tkthread_bind(thread, cpu);\n\twake_up_process(thread);\n\n\tret = wait_event_interruptible(plr->lock_thread_wq,\n\t\t\t\t       plr->thread_done == 1);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tret = 0;\n\nout:\n\tmutex_unlock(&rdtgroup_mutex);\n\tcpus_read_unlock();\n\treturn ret;\n}\n\nstatic ssize_t pseudo_lock_measure_trigger(struct file *file,\n\t\t\t\t\t   const char __user *user_buf,\n\t\t\t\t\t   size_t count, loff_t *ppos)\n{\n\tstruct rdtgroup *rdtgrp = file->private_data;\n\tsize_t buf_size;\n\tchar buf[32];\n\tint ret;\n\tint sel;\n\n\tbuf_size = min(count, (sizeof(buf) - 1));\n\tif (copy_from_user(buf, user_buf, buf_size))\n\t\treturn -EFAULT;\n\n\tbuf[buf_size] = '\\0';\n\tret = kstrtoint(buf, 10, &sel);\n\tif (ret == 0) {\n\t\tif (sel != 1 && sel != 2 && sel != 3)\n\t\t\treturn -EINVAL;\n\t\tret = debugfs_file_get(file->f_path.dentry);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = pseudo_lock_measure_cycles(rdtgrp, sel);\n\t\tif (ret == 0)\n\t\t\tret = count;\n\t\tdebugfs_file_put(file->f_path.dentry);\n\t}\n\n\treturn ret;\n}\n\nstatic const struct file_operations pseudo_measure_fops = {\n\t.write = pseudo_lock_measure_trigger,\n\t.open = simple_open,\n\t.llseek = default_llseek,\n};\n\n \nint rdtgroup_pseudo_lock_create(struct rdtgroup *rdtgrp)\n{\n\tstruct pseudo_lock_region *plr = rdtgrp->plr;\n\tstruct task_struct *thread;\n\tunsigned int new_minor;\n\tstruct device *dev;\n\tint ret;\n\n\tret = pseudo_lock_region_alloc(plr);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = pseudo_lock_cstates_constrain(plr);\n\tif (ret < 0) {\n\t\tret = -EINVAL;\n\t\tgoto out_region;\n\t}\n\n\tplr->thread_done = 0;\n\n\tthread = kthread_create_on_node(pseudo_lock_fn, rdtgrp,\n\t\t\t\t\tcpu_to_node(plr->cpu),\n\t\t\t\t\t\"pseudo_lock/%u\", plr->cpu);\n\tif (IS_ERR(thread)) {\n\t\tret = PTR_ERR(thread);\n\t\trdt_last_cmd_printf(\"Locking thread returned error %d\\n\", ret);\n\t\tgoto out_cstates;\n\t}\n\n\tkthread_bind(thread, plr->cpu);\n\twake_up_process(thread);\n\n\tret = wait_event_interruptible(plr->lock_thread_wq,\n\t\t\t\t       plr->thread_done == 1);\n\tif (ret < 0) {\n\t\t \n\t\trdt_last_cmd_puts(\"Locking thread interrupted\\n\");\n\t\tgoto out_cstates;\n\t}\n\n\tret = pseudo_lock_minor_get(&new_minor);\n\tif (ret < 0) {\n\t\trdt_last_cmd_puts(\"Unable to obtain a new minor number\\n\");\n\t\tgoto out_cstates;\n\t}\n\n\t \n\tmutex_unlock(&rdtgroup_mutex);\n\n\tif (!IS_ERR_OR_NULL(debugfs_resctrl)) {\n\t\tplr->debugfs_dir = debugfs_create_dir(rdtgrp->kn->name,\n\t\t\t\t\t\t      debugfs_resctrl);\n\t\tif (!IS_ERR_OR_NULL(plr->debugfs_dir))\n\t\t\tdebugfs_create_file(\"pseudo_lock_measure\", 0200,\n\t\t\t\t\t    plr->debugfs_dir, rdtgrp,\n\t\t\t\t\t    &pseudo_measure_fops);\n\t}\n\n\tdev = device_create(&pseudo_lock_class, NULL,\n\t\t\t    MKDEV(pseudo_lock_major, new_minor),\n\t\t\t    rdtgrp, \"%s\", rdtgrp->kn->name);\n\n\tmutex_lock(&rdtgroup_mutex);\n\n\tif (IS_ERR(dev)) {\n\t\tret = PTR_ERR(dev);\n\t\trdt_last_cmd_printf(\"Failed to create character device: %d\\n\",\n\t\t\t\t    ret);\n\t\tgoto out_debugfs;\n\t}\n\n\t \n\tif (rdtgrp->flags & RDT_DELETED) {\n\t\tret = -ENODEV;\n\t\tgoto out_device;\n\t}\n\n\tplr->minor = new_minor;\n\n\trdtgrp->mode = RDT_MODE_PSEUDO_LOCKED;\n\tclosid_free(rdtgrp->closid);\n\trdtgroup_kn_mode_restore(rdtgrp, \"cpus\", 0444);\n\trdtgroup_kn_mode_restore(rdtgrp, \"cpus_list\", 0444);\n\n\tret = 0;\n\tgoto out;\n\nout_device:\n\tdevice_destroy(&pseudo_lock_class, MKDEV(pseudo_lock_major, new_minor));\nout_debugfs:\n\tdebugfs_remove_recursive(plr->debugfs_dir);\n\tpseudo_lock_minor_release(new_minor);\nout_cstates:\n\tpseudo_lock_cstates_relax(plr);\nout_region:\n\tpseudo_lock_region_clear(plr);\nout:\n\treturn ret;\n}\n\n \nvoid rdtgroup_pseudo_lock_remove(struct rdtgroup *rdtgrp)\n{\n\tstruct pseudo_lock_region *plr = rdtgrp->plr;\n\n\tif (rdtgrp->mode == RDT_MODE_PSEUDO_LOCKSETUP) {\n\t\t \n\t\tclosid_free(rdtgrp->closid);\n\t\tgoto free;\n\t}\n\n\tpseudo_lock_cstates_relax(plr);\n\tdebugfs_remove_recursive(rdtgrp->plr->debugfs_dir);\n\tdevice_destroy(&pseudo_lock_class, MKDEV(pseudo_lock_major, plr->minor));\n\tpseudo_lock_minor_release(plr->minor);\n\nfree:\n\tpseudo_lock_free(rdtgrp);\n}\n\nstatic int pseudo_lock_dev_open(struct inode *inode, struct file *filp)\n{\n\tstruct rdtgroup *rdtgrp;\n\n\tmutex_lock(&rdtgroup_mutex);\n\n\trdtgrp = region_find_by_minor(iminor(inode));\n\tif (!rdtgrp) {\n\t\tmutex_unlock(&rdtgroup_mutex);\n\t\treturn -ENODEV;\n\t}\n\n\tfilp->private_data = rdtgrp;\n\tatomic_inc(&rdtgrp->waitcount);\n\t \n\tfilp->f_mode &= ~(FMODE_LSEEK | FMODE_PREAD | FMODE_PWRITE);\n\n\tmutex_unlock(&rdtgroup_mutex);\n\n\treturn 0;\n}\n\nstatic int pseudo_lock_dev_release(struct inode *inode, struct file *filp)\n{\n\tstruct rdtgroup *rdtgrp;\n\n\tmutex_lock(&rdtgroup_mutex);\n\trdtgrp = filp->private_data;\n\tWARN_ON(!rdtgrp);\n\tif (!rdtgrp) {\n\t\tmutex_unlock(&rdtgroup_mutex);\n\t\treturn -ENODEV;\n\t}\n\tfilp->private_data = NULL;\n\tatomic_dec(&rdtgrp->waitcount);\n\tmutex_unlock(&rdtgroup_mutex);\n\treturn 0;\n}\n\nstatic int pseudo_lock_dev_mremap(struct vm_area_struct *area)\n{\n\t \n\treturn -EINVAL;\n}\n\nstatic const struct vm_operations_struct pseudo_mmap_ops = {\n\t.mremap = pseudo_lock_dev_mremap,\n};\n\nstatic int pseudo_lock_dev_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\tunsigned long vsize = vma->vm_end - vma->vm_start;\n\tunsigned long off = vma->vm_pgoff << PAGE_SHIFT;\n\tstruct pseudo_lock_region *plr;\n\tstruct rdtgroup *rdtgrp;\n\tunsigned long physical;\n\tunsigned long psize;\n\n\tmutex_lock(&rdtgroup_mutex);\n\n\trdtgrp = filp->private_data;\n\tWARN_ON(!rdtgrp);\n\tif (!rdtgrp) {\n\t\tmutex_unlock(&rdtgroup_mutex);\n\t\treturn -ENODEV;\n\t}\n\n\tplr = rdtgrp->plr;\n\n\tif (!plr->d) {\n\t\tmutex_unlock(&rdtgroup_mutex);\n\t\treturn -ENODEV;\n\t}\n\n\t \n\tif (!cpumask_subset(current->cpus_ptr, &plr->d->cpu_mask)) {\n\t\tmutex_unlock(&rdtgroup_mutex);\n\t\treturn -EINVAL;\n\t}\n\n\tphysical = __pa(plr->kmem) >> PAGE_SHIFT;\n\tpsize = plr->size - off;\n\n\tif (off > plr->size) {\n\t\tmutex_unlock(&rdtgroup_mutex);\n\t\treturn -ENOSPC;\n\t}\n\n\t \n\tif (!(vma->vm_flags & VM_SHARED)) {\n\t\tmutex_unlock(&rdtgroup_mutex);\n\t\treturn -EINVAL;\n\t}\n\n\tif (vsize > psize) {\n\t\tmutex_unlock(&rdtgroup_mutex);\n\t\treturn -ENOSPC;\n\t}\n\n\tmemset(plr->kmem + off, 0, vsize);\n\n\tif (remap_pfn_range(vma, vma->vm_start, physical + vma->vm_pgoff,\n\t\t\t    vsize, vma->vm_page_prot)) {\n\t\tmutex_unlock(&rdtgroup_mutex);\n\t\treturn -EAGAIN;\n\t}\n\tvma->vm_ops = &pseudo_mmap_ops;\n\tmutex_unlock(&rdtgroup_mutex);\n\treturn 0;\n}\n\nstatic const struct file_operations pseudo_lock_dev_fops = {\n\t.owner =\tTHIS_MODULE,\n\t.llseek =\tno_llseek,\n\t.read =\t\tNULL,\n\t.write =\tNULL,\n\t.open =\t\tpseudo_lock_dev_open,\n\t.release =\tpseudo_lock_dev_release,\n\t.mmap =\t\tpseudo_lock_dev_mmap,\n};\n\nint rdt_pseudo_lock_init(void)\n{\n\tint ret;\n\n\tret = register_chrdev(0, \"pseudo_lock\", &pseudo_lock_dev_fops);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tpseudo_lock_major = ret;\n\n\tret = class_register(&pseudo_lock_class);\n\tif (ret) {\n\t\tunregister_chrdev(pseudo_lock_major, \"pseudo_lock\");\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nvoid rdt_pseudo_lock_release(void)\n{\n\tclass_unregister(&pseudo_lock_class);\n\tunregister_chrdev(pseudo_lock_major, \"pseudo_lock\");\n\tpseudo_lock_major = 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}