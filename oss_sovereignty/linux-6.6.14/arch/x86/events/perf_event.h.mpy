{
  "module_name": "perf_event.h",
  "hash_id": "a958c47741611d783d7a559cf9583446632cbca8d7435b34a53dc44519fb8a5b",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/events/perf_event.h",
  "human_readable_source": " \n\n#include <linux/perf_event.h>\n\n#include <asm/fpu/xstate.h>\n#include <asm/intel_ds.h>\n#include <asm/cpu.h>\n\n \n\n \nenum extra_reg_type {\n\tEXTRA_REG_NONE\t\t= -1,  \n\n\tEXTRA_REG_RSP_0\t\t= 0,   \n\tEXTRA_REG_RSP_1\t\t= 1,   \n\tEXTRA_REG_LBR\t\t= 2,   \n\tEXTRA_REG_LDLAT\t\t= 3,   \n\tEXTRA_REG_FE\t\t= 4,   \n\tEXTRA_REG_SNOOP_0\t= 5,   \n\tEXTRA_REG_SNOOP_1\t= 6,   \n\n\tEXTRA_REG_MAX\t\t       \n};\n\nstruct event_constraint {\n\tunion {\n\t\tunsigned long\tidxmsk[BITS_TO_LONGS(X86_PMC_IDX_MAX)];\n\t\tu64\t\tidxmsk64;\n\t};\n\tu64\t\tcode;\n\tu64\t\tcmask;\n\tint\t\tweight;\n\tint\t\toverlap;\n\tint\t\tflags;\n\tunsigned int\tsize;\n};\n\nstatic inline bool constraint_match(struct event_constraint *c, u64 ecode)\n{\n\treturn ((ecode & c->cmask) - c->code) <= (u64)c->size;\n}\n\n#define PERF_ARCH(name, val)\t\\\n\tPERF_X86_EVENT_##name = val,\n\n \nenum {\n#include \"perf_event_flags.h\"\n};\n\n#undef PERF_ARCH\n\n#define PERF_ARCH(name, val)\t\t\t\t\t\t\\\n\tstatic_assert((PERF_X86_EVENT_##name & PERF_EVENT_FLAG_ARCH) ==\t\\\n\t\t      PERF_X86_EVENT_##name);\n\n#include \"perf_event_flags.h\"\n\n#undef PERF_ARCH\n\nstatic inline bool is_topdown_count(struct perf_event *event)\n{\n\treturn event->hw.flags & PERF_X86_EVENT_TOPDOWN;\n}\n\nstatic inline bool is_metric_event(struct perf_event *event)\n{\n\tu64 config = event->attr.config;\n\n\treturn ((config & ARCH_PERFMON_EVENTSEL_EVENT) == 0) &&\n\t\t((config & INTEL_ARCH_EVENT_MASK) >= INTEL_TD_METRIC_RETIRING)  &&\n\t\t((config & INTEL_ARCH_EVENT_MASK) <= INTEL_TD_METRIC_MAX);\n}\n\nstatic inline bool is_slots_event(struct perf_event *event)\n{\n\treturn (event->attr.config & INTEL_ARCH_EVENT_MASK) == INTEL_TD_SLOTS;\n}\n\nstatic inline bool is_topdown_event(struct perf_event *event)\n{\n\treturn is_metric_event(event) || is_slots_event(event);\n}\n\nstruct amd_nb {\n\tint nb_id;   \n\tint refcnt;  \n\tstruct perf_event *owners[X86_PMC_IDX_MAX];\n\tstruct event_constraint event_constraints[X86_PMC_IDX_MAX];\n};\n\n#define PEBS_COUNTER_MASK\t((1ULL << MAX_PEBS_EVENTS) - 1)\n#define PEBS_PMI_AFTER_EACH_RECORD BIT_ULL(60)\n#define PEBS_OUTPUT_OFFSET\t61\n#define PEBS_OUTPUT_MASK\t(3ull << PEBS_OUTPUT_OFFSET)\n#define PEBS_OUTPUT_PT\t\t(1ull << PEBS_OUTPUT_OFFSET)\n#define PEBS_VIA_PT_MASK\t(PEBS_OUTPUT_PT | PEBS_PMI_AFTER_EACH_RECORD)\n\n \n#define LARGE_PEBS_FLAGS \\\n\t(PERF_SAMPLE_IP | PERF_SAMPLE_TID | PERF_SAMPLE_ADDR | \\\n\tPERF_SAMPLE_ID | PERF_SAMPLE_CPU | PERF_SAMPLE_STREAM_ID | \\\n\tPERF_SAMPLE_DATA_SRC | PERF_SAMPLE_IDENTIFIER | \\\n\tPERF_SAMPLE_TRANSACTION | PERF_SAMPLE_PHYS_ADDR | \\\n\tPERF_SAMPLE_REGS_INTR | PERF_SAMPLE_REGS_USER | \\\n\tPERF_SAMPLE_PERIOD | PERF_SAMPLE_CODE_PAGE_SIZE | \\\n\tPERF_SAMPLE_WEIGHT_TYPE)\n\n#define PEBS_GP_REGS\t\t\t\\\n\t((1ULL << PERF_REG_X86_AX)    | \\\n\t (1ULL << PERF_REG_X86_BX)    | \\\n\t (1ULL << PERF_REG_X86_CX)    | \\\n\t (1ULL << PERF_REG_X86_DX)    | \\\n\t (1ULL << PERF_REG_X86_DI)    | \\\n\t (1ULL << PERF_REG_X86_SI)    | \\\n\t (1ULL << PERF_REG_X86_SP)    | \\\n\t (1ULL << PERF_REG_X86_BP)    | \\\n\t (1ULL << PERF_REG_X86_IP)    | \\\n\t (1ULL << PERF_REG_X86_FLAGS) | \\\n\t (1ULL << PERF_REG_X86_R8)    | \\\n\t (1ULL << PERF_REG_X86_R9)    | \\\n\t (1ULL << PERF_REG_X86_R10)   | \\\n\t (1ULL << PERF_REG_X86_R11)   | \\\n\t (1ULL << PERF_REG_X86_R12)   | \\\n\t (1ULL << PERF_REG_X86_R13)   | \\\n\t (1ULL << PERF_REG_X86_R14)   | \\\n\t (1ULL << PERF_REG_X86_R15))\n\n \nstruct er_account {\n\traw_spinlock_t      lock;\t \n\tu64                 config;\t \n\tu64                 reg;\t \n\tatomic_t            ref;\t \n};\n\n \nstruct intel_shared_regs {\n\tstruct er_account       regs[EXTRA_REG_MAX];\n\tint                     refcnt;\t\t \n\tunsigned                core_id;\t \n};\n\nenum intel_excl_state_type {\n\tINTEL_EXCL_UNUSED    = 0,  \n\tINTEL_EXCL_SHARED    = 1,  \n\tINTEL_EXCL_EXCLUSIVE = 2,  \n};\n\nstruct intel_excl_states {\n\tenum intel_excl_state_type state[X86_PMC_IDX_MAX];\n\tbool sched_started;  \n};\n\nstruct intel_excl_cntrs {\n\traw_spinlock_t\tlock;\n\n\tstruct intel_excl_states states[2];\n\n\tunion {\n\t\tu16\thas_exclusive[2];\n\t\tu32\texclusive_present;\n\t};\n\n\tint\t\trefcnt;\t\t \n\tunsigned\tcore_id;\t \n};\n\nstruct x86_perf_task_context;\n#define MAX_LBR_ENTRIES\t\t32\n\nenum {\n\tLBR_FORMAT_32\t\t= 0x00,\n\tLBR_FORMAT_LIP\t\t= 0x01,\n\tLBR_FORMAT_EIP\t\t= 0x02,\n\tLBR_FORMAT_EIP_FLAGS\t= 0x03,\n\tLBR_FORMAT_EIP_FLAGS2\t= 0x04,\n\tLBR_FORMAT_INFO\t\t= 0x05,\n\tLBR_FORMAT_TIME\t\t= 0x06,\n\tLBR_FORMAT_INFO2\t= 0x07,\n\tLBR_FORMAT_MAX_KNOWN    = LBR_FORMAT_INFO2,\n};\n\nenum {\n\tX86_PERF_KFREE_SHARED = 0,\n\tX86_PERF_KFREE_EXCL   = 1,\n\tX86_PERF_KFREE_MAX\n};\n\nstruct cpu_hw_events {\n\t \n\tstruct perf_event\t*events[X86_PMC_IDX_MAX];  \n\tunsigned long\t\tactive_mask[BITS_TO_LONGS(X86_PMC_IDX_MAX)];\n\tunsigned long\t\tdirty[BITS_TO_LONGS(X86_PMC_IDX_MAX)];\n\tint\t\t\tenabled;\n\n\tint\t\t\tn_events;  \n\tint\t\t\tn_added;   \n\tint\t\t\tn_txn;     \n\tint\t\t\tn_txn_pair;\n\tint\t\t\tn_txn_metric;\n\tint\t\t\tassign[X86_PMC_IDX_MAX];  \n\tu64\t\t\ttags[X86_PMC_IDX_MAX];\n\n\tstruct perf_event\t*event_list[X86_PMC_IDX_MAX];  \n\tstruct event_constraint\t*event_constraint[X86_PMC_IDX_MAX];\n\n\tint\t\t\tn_excl;  \n\n\tunsigned int\t\ttxn_flags;\n\tint\t\t\tis_fake;\n\n\t \n\tstruct debug_store\t*ds;\n\tvoid\t\t\t*ds_pebs_vaddr;\n\tvoid\t\t\t*ds_bts_vaddr;\n\tu64\t\t\tpebs_enabled;\n\tint\t\t\tn_pebs;\n\tint\t\t\tn_large_pebs;\n\tint\t\t\tn_pebs_via_pt;\n\tint\t\t\tpebs_output;\n\n\t \n\tu64\t\t\tpebs_data_cfg;\n\tu64\t\t\tactive_pebs_data_cfg;\n\tint\t\t\tpebs_record_size;\n\n\t \n\tu64\t\t\tfixed_ctrl_val;\n\tu64\t\t\tactive_fixed_ctrl_val;\n\n\t \n\tint\t\t\t\tlbr_users;\n\tint\t\t\t\tlbr_pebs_users;\n\tstruct perf_branch_stack\tlbr_stack;\n\tstruct perf_branch_entry\tlbr_entries[MAX_LBR_ENTRIES];\n\tunion {\n\t\tstruct er_account\t\t*lbr_sel;\n\t\tstruct er_account\t\t*lbr_ctl;\n\t};\n\tu64\t\t\t\tbr_sel;\n\tvoid\t\t\t\t*last_task_ctx;\n\tint\t\t\t\tlast_log_id;\n\tint\t\t\t\tlbr_select;\n\tvoid\t\t\t\t*lbr_xsave;\n\n\t \n\tu64\t\t\t\tintel_ctrl_guest_mask;\n\tu64\t\t\t\tintel_ctrl_host_mask;\n\tstruct perf_guest_switch_msr\tguest_switch_msrs[X86_PMC_IDX_MAX];\n\n\t \n\tu64\t\t\t\tintel_cp_status;\n\n\t \n\tstruct intel_shared_regs\t*shared_regs;\n\t \n\tstruct event_constraint *constraint_list;  \n\tstruct intel_excl_cntrs\t\t*excl_cntrs;\n\tint excl_thread_id;  \n\n\t \n\tu64\t\t\t\ttfa_shadow;\n\n\t \n\t \n\tint\t\t\t\tn_metric;\n\n\t \n\tstruct amd_nb\t\t\t*amd_nb;\n\tint\t\t\t\tbrs_active;  \n\n\t \n\tu64\t\t\t\tperf_ctr_virt_mask;\n\tint\t\t\t\tn_pair;  \n\n\tvoid\t\t\t\t*kfree_on_online[X86_PERF_KFREE_MAX];\n\n\tstruct pmu\t\t\t*pmu;\n};\n\n#define __EVENT_CONSTRAINT_RANGE(c, e, n, m, w, o, f) {\t\\\n\t{ .idxmsk64 = (n) },\t\t\\\n\t.code = (c),\t\t\t\\\n\t.size = (e) - (c),\t\t\\\n\t.cmask = (m),\t\t\t\\\n\t.weight = (w),\t\t\t\\\n\t.overlap = (o),\t\t\t\\\n\t.flags = f,\t\t\t\\\n}\n\n#define __EVENT_CONSTRAINT(c, n, m, w, o, f) \\\n\t__EVENT_CONSTRAINT_RANGE(c, c, n, m, w, o, f)\n\n#define EVENT_CONSTRAINT(c, n, m)\t\\\n\t__EVENT_CONSTRAINT(c, n, m, HWEIGHT(n), 0, 0)\n\n \n#define EVENT_CONSTRAINT_RANGE(c, e, n, m) \\\n\t__EVENT_CONSTRAINT_RANGE(c, e, n, m, HWEIGHT(n), 0, 0)\n\n#define INTEL_EXCLEVT_CONSTRAINT(c, n)\t\\\n\t__EVENT_CONSTRAINT(c, n, ARCH_PERFMON_EVENTSEL_EVENT, HWEIGHT(n),\\\n\t\t\t   0, PERF_X86_EVENT_EXCL)\n\n \n#define EVENT_CONSTRAINT_OVERLAP(c, n, m)\t\\\n\t__EVENT_CONSTRAINT(c, n, m, HWEIGHT(n), 1, 0)\n\n \n#define INTEL_EVENT_CONSTRAINT(c, n)\t\\\n\tEVENT_CONSTRAINT(c, n, ARCH_PERFMON_EVENTSEL_EVENT)\n\n \n#define INTEL_EVENT_CONSTRAINT_RANGE(c, e, n)\t\t\t\\\n\tEVENT_CONSTRAINT_RANGE(c, e, n, ARCH_PERFMON_EVENTSEL_EVENT)\n\n \n#define FIXED_EVENT_FLAGS (X86_RAW_EVENT_MASK|HSW_IN_TX|HSW_IN_TX_CHECKPOINTED)\n#define FIXED_EVENT_CONSTRAINT(c, n)\t\\\n\tEVENT_CONSTRAINT(c, (1ULL << (32+n)), FIXED_EVENT_FLAGS)\n\n \n\n#define METRIC_EVENT_CONSTRAINT(c, n)\t\t\t\t\t\\\n\tEVENT_CONSTRAINT(c, (1ULL << (INTEL_PMC_IDX_METRIC_BASE + n)),\t\\\n\t\t\t INTEL_ARCH_EVENT_MASK)\n\n \n#define INTEL_UEVENT_CONSTRAINT(c, n)\t\\\n\tEVENT_CONSTRAINT(c, n, INTEL_ARCH_EVENT_MASK)\n\n \n#define INTEL_UBIT_EVENT_CONSTRAINT(c, n)\t\\\n\tEVENT_CONSTRAINT(c, n, ARCH_PERFMON_EVENTSEL_EVENT|(c))\n\n \n#define INTEL_FLAGS_UEVENT_CONSTRAINT(c, n)\t\\\n\tEVENT_CONSTRAINT(c, n, INTEL_ARCH_EVENT_MASK|X86_ALL_EVENT_FLAGS)\n\n#define INTEL_EXCLUEVT_CONSTRAINT(c, n)\t\\\n\t__EVENT_CONSTRAINT(c, n, INTEL_ARCH_EVENT_MASK, \\\n\t\t\t   HWEIGHT(n), 0, PERF_X86_EVENT_EXCL)\n\n#define INTEL_PLD_CONSTRAINT(c, n)\t\\\n\t__EVENT_CONSTRAINT(c, n, INTEL_ARCH_EVENT_MASK|X86_ALL_EVENT_FLAGS, \\\n\t\t\t   HWEIGHT(n), 0, PERF_X86_EVENT_PEBS_LDLAT)\n\n#define INTEL_PSD_CONSTRAINT(c, n)\t\\\n\t__EVENT_CONSTRAINT(c, n, INTEL_ARCH_EVENT_MASK|X86_ALL_EVENT_FLAGS, \\\n\t\t\t   HWEIGHT(n), 0, PERF_X86_EVENT_PEBS_STLAT)\n\n#define INTEL_PST_CONSTRAINT(c, n)\t\\\n\t__EVENT_CONSTRAINT(c, n, INTEL_ARCH_EVENT_MASK|X86_ALL_EVENT_FLAGS, \\\n\t\t\t  HWEIGHT(n), 0, PERF_X86_EVENT_PEBS_ST)\n\n#define INTEL_HYBRID_LAT_CONSTRAINT(c, n)\t\\\n\t__EVENT_CONSTRAINT(c, n, INTEL_ARCH_EVENT_MASK|X86_ALL_EVENT_FLAGS, \\\n\t\t\t  HWEIGHT(n), 0, PERF_X86_EVENT_PEBS_LAT_HYBRID)\n\n \n#define INTEL_FLAGS_EVENT_CONSTRAINT(c, n) \\\n\tEVENT_CONSTRAINT(c, n, ARCH_PERFMON_EVENTSEL_EVENT|X86_ALL_EVENT_FLAGS)\n\n#define INTEL_FLAGS_EVENT_CONSTRAINT_RANGE(c, e, n)\t\t\t\\\n\tEVENT_CONSTRAINT_RANGE(c, e, n, ARCH_PERFMON_EVENTSEL_EVENT|X86_ALL_EVENT_FLAGS)\n\n \n#define INTEL_ALL_EVENT_CONSTRAINT(code, n)\t\\\n\tEVENT_CONSTRAINT(code, n, X86_ALL_EVENT_FLAGS)\n\n \n#define INTEL_FLAGS_EVENT_CONSTRAINT_DATALA_ST(code, n) \\\n\t__EVENT_CONSTRAINT(code, n, \t\t\t\\\n\t\t\t  ARCH_PERFMON_EVENTSEL_EVENT|X86_ALL_EVENT_FLAGS, \\\n\t\t\t  HWEIGHT(n), 0, PERF_X86_EVENT_PEBS_ST_HSW)\n\n \n#define INTEL_FLAGS_EVENT_CONSTRAINT_DATALA_LD(code, n) \\\n\t__EVENT_CONSTRAINT(code, n,\t\t\t\\\n\t\t\t  ARCH_PERFMON_EVENTSEL_EVENT|X86_ALL_EVENT_FLAGS, \\\n\t\t\t  HWEIGHT(n), 0, PERF_X86_EVENT_PEBS_LD_HSW)\n\n#define INTEL_FLAGS_EVENT_CONSTRAINT_DATALA_LD_RANGE(code, end, n) \\\n\t__EVENT_CONSTRAINT_RANGE(code, end, n,\t\t\t\t\\\n\t\t\t  ARCH_PERFMON_EVENTSEL_EVENT|X86_ALL_EVENT_FLAGS, \\\n\t\t\t  HWEIGHT(n), 0, PERF_X86_EVENT_PEBS_LD_HSW)\n\n#define INTEL_FLAGS_EVENT_CONSTRAINT_DATALA_XLD(code, n) \\\n\t__EVENT_CONSTRAINT(code, n,\t\t\t\\\n\t\t\t  ARCH_PERFMON_EVENTSEL_EVENT|X86_ALL_EVENT_FLAGS, \\\n\t\t\t  HWEIGHT(n), 0, \\\n\t\t\t  PERF_X86_EVENT_PEBS_LD_HSW|PERF_X86_EVENT_EXCL)\n\n \n#define INTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_ST(code, n) \\\n\t__EVENT_CONSTRAINT(code, n, \t\t\t\\\n\t\t\t  INTEL_ARCH_EVENT_MASK|X86_ALL_EVENT_FLAGS, \\\n\t\t\t  HWEIGHT(n), 0, PERF_X86_EVENT_PEBS_ST_HSW)\n\n#define INTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_XST(code, n) \\\n\t__EVENT_CONSTRAINT(code, n,\t\t\t\\\n\t\t\t  INTEL_ARCH_EVENT_MASK|X86_ALL_EVENT_FLAGS, \\\n\t\t\t  HWEIGHT(n), 0, \\\n\t\t\t  PERF_X86_EVENT_PEBS_ST_HSW|PERF_X86_EVENT_EXCL)\n\n \n#define INTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(code, n) \\\n\t__EVENT_CONSTRAINT(code, n, \t\t\t\\\n\t\t\t  INTEL_ARCH_EVENT_MASK|X86_ALL_EVENT_FLAGS, \\\n\t\t\t  HWEIGHT(n), 0, PERF_X86_EVENT_PEBS_LD_HSW)\n\n#define INTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_XLD(code, n) \\\n\t__EVENT_CONSTRAINT(code, n,\t\t\t\\\n\t\t\t  INTEL_ARCH_EVENT_MASK|X86_ALL_EVENT_FLAGS, \\\n\t\t\t  HWEIGHT(n), 0, \\\n\t\t\t  PERF_X86_EVENT_PEBS_LD_HSW|PERF_X86_EVENT_EXCL)\n\n \n#define INTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_NA(code, n) \\\n\t__EVENT_CONSTRAINT(code, n, \t\t\t\\\n\t\t\t  INTEL_ARCH_EVENT_MASK|X86_ALL_EVENT_FLAGS, \\\n\t\t\t  HWEIGHT(n), 0, PERF_X86_EVENT_PEBS_NA_HSW)\n\n\n \n#define EVENT_CONSTRAINT_END { .weight = -1 }\n\n \n#define for_each_event_constraint(e, c)\t\\\n\tfor ((e) = (c); (e)->weight != -1; (e)++)\n\n \nstruct extra_reg {\n\tunsigned int\t\tevent;\n\tunsigned int\t\tmsr;\n\tu64\t\t\tconfig_mask;\n\tu64\t\t\tvalid_mask;\n\tint\t\t\tidx;   \n\tbool\t\t\textra_msr_access;\n};\n\n#define EVENT_EXTRA_REG(e, ms, m, vm, i) {\t\\\n\t.event = (e),\t\t\t\\\n\t.msr = (ms),\t\t\t\\\n\t.config_mask = (m),\t\t\\\n\t.valid_mask = (vm),\t\t\\\n\t.idx = EXTRA_REG_##i,\t\t\\\n\t.extra_msr_access = true,\t\\\n\t}\n\n#define INTEL_EVENT_EXTRA_REG(event, msr, vm, idx)\t\\\n\tEVENT_EXTRA_REG(event, msr, ARCH_PERFMON_EVENTSEL_EVENT, vm, idx)\n\n#define INTEL_UEVENT_EXTRA_REG(event, msr, vm, idx) \\\n\tEVENT_EXTRA_REG(event, msr, ARCH_PERFMON_EVENTSEL_EVENT | \\\n\t\t\tARCH_PERFMON_EVENTSEL_UMASK, vm, idx)\n\n#define INTEL_UEVENT_PEBS_LDLAT_EXTRA_REG(c) \\\n\tINTEL_UEVENT_EXTRA_REG(c, \\\n\t\t\t       MSR_PEBS_LD_LAT_THRESHOLD, \\\n\t\t\t       0xffff, \\\n\t\t\t       LDLAT)\n\n#define EVENT_EXTRA_END EVENT_EXTRA_REG(0, 0, 0, 0, RSP_0)\n\nunion perf_capabilities {\n\tstruct {\n\t\tu64\tlbr_format:6;\n\t\tu64\tpebs_trap:1;\n\t\tu64\tpebs_arch_reg:1;\n\t\tu64\tpebs_format:4;\n\t\tu64\tsmm_freeze:1;\n\t\t \n\t\tu64\tfull_width_write:1;\n\t\tu64     pebs_baseline:1;\n\t\tu64\tperf_metrics:1;\n\t\tu64\tpebs_output_pt_available:1;\n\t\tu64\tpebs_timing_info:1;\n\t\tu64\tanythread_deprecated:1;\n\t};\n\tu64\tcapabilities;\n};\n\nstruct x86_pmu_quirk {\n\tstruct x86_pmu_quirk *next;\n\tvoid (*func)(void);\n};\n\nunion x86_pmu_config {\n\tstruct {\n\t\tu64 event:8,\n\t\t    umask:8,\n\t\t    usr:1,\n\t\t    os:1,\n\t\t    edge:1,\n\t\t    pc:1,\n\t\t    interrupt:1,\n\t\t    __reserved1:1,\n\t\t    en:1,\n\t\t    inv:1,\n\t\t    cmask:8,\n\t\t    event2:4,\n\t\t    __reserved2:4,\n\t\t    go:1,\n\t\t    ho:1;\n\t} bits;\n\tu64 value;\n};\n\n#define X86_CONFIG(args...) ((union x86_pmu_config){.bits = {args}}).value\n\nenum {\n\tx86_lbr_exclusive_lbr,\n\tx86_lbr_exclusive_bts,\n\tx86_lbr_exclusive_pt,\n\tx86_lbr_exclusive_max,\n};\n\n#define PERF_PEBS_DATA_SOURCE_MAX\t0x10\n#define PERF_PEBS_DATA_SOURCE_MASK\t(PERF_PEBS_DATA_SOURCE_MAX - 1)\n\nstruct x86_hybrid_pmu {\n\tstruct pmu\t\t\tpmu;\n\tconst char\t\t\t*name;\n\tu8\t\t\t\tcpu_type;\n\tcpumask_t\t\t\tsupported_cpus;\n\tunion perf_capabilities\t\tintel_cap;\n\tu64\t\t\t\tintel_ctrl;\n\tint\t\t\t\tmax_pebs_events;\n\tint\t\t\t\tnum_counters;\n\tint\t\t\t\tnum_counters_fixed;\n\tstruct event_constraint\t\tunconstrained;\n\n\tu64\t\t\t\thw_cache_event_ids\n\t\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX];\n\tu64\t\t\t\thw_cache_extra_regs\n\t\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX];\n\tstruct event_constraint\t\t*event_constraints;\n\tstruct event_constraint\t\t*pebs_constraints;\n\tstruct extra_reg\t\t*extra_regs;\n\n\tunsigned int\t\t\tlate_ack\t:1,\n\t\t\t\t\tmid_ack\t\t:1,\n\t\t\t\t\tenabled_ack\t:1;\n\n\tu64\t\t\t\tpebs_data_source[PERF_PEBS_DATA_SOURCE_MAX];\n};\n\nstatic __always_inline struct x86_hybrid_pmu *hybrid_pmu(struct pmu *pmu)\n{\n\treturn container_of(pmu, struct x86_hybrid_pmu, pmu);\n}\n\nextern struct static_key_false perf_is_hybrid;\n#define is_hybrid()\t\tstatic_branch_unlikely(&perf_is_hybrid)\n\n#define hybrid(_pmu, _field)\t\t\t\t\\\n(*({\t\t\t\t\t\t\t\\\n\ttypeof(&x86_pmu._field) __Fp = &x86_pmu._field;\t\\\n\t\t\t\t\t\t\t\\\n\tif (is_hybrid() && (_pmu))\t\t\t\\\n\t\t__Fp = &hybrid_pmu(_pmu)->_field;\t\\\n\t\t\t\t\t\t\t\\\n\t__Fp;\t\t\t\t\t\t\\\n}))\n\n#define hybrid_var(_pmu, _var)\t\t\t\t\\\n(*({\t\t\t\t\t\t\t\\\n\ttypeof(&_var) __Fp = &_var;\t\t\t\\\n\t\t\t\t\t\t\t\\\n\tif (is_hybrid() && (_pmu))\t\t\t\\\n\t\t__Fp = &hybrid_pmu(_pmu)->_var;\t\t\\\n\t\t\t\t\t\t\t\\\n\t__Fp;\t\t\t\t\t\t\\\n}))\n\n#define hybrid_bit(_pmu, _field)\t\t\t\\\n({\t\t\t\t\t\t\t\\\n\tbool __Fp = x86_pmu._field;\t\t\t\\\n\t\t\t\t\t\t\t\\\n\tif (is_hybrid() && (_pmu))\t\t\t\\\n\t\t__Fp = hybrid_pmu(_pmu)->_field;\t\\\n\t\t\t\t\t\t\t\\\n\t__Fp;\t\t\t\t\t\t\\\n})\n\nenum hybrid_pmu_type {\n\thybrid_big\t\t= 0x40,\n\thybrid_small\t\t= 0x20,\n\n\thybrid_big_small\t= hybrid_big | hybrid_small,\n};\n\n#define X86_HYBRID_PMU_ATOM_IDX\t\t0\n#define X86_HYBRID_PMU_CORE_IDX\t\t1\n\n#define X86_HYBRID_NUM_PMUS\t\t2\n\n \nstruct x86_pmu {\n\t \n\tconst char\t*name;\n\tint\t\tversion;\n\tint\t\t(*handle_irq)(struct pt_regs *);\n\tvoid\t\t(*disable_all)(void);\n\tvoid\t\t(*enable_all)(int added);\n\tvoid\t\t(*enable)(struct perf_event *);\n\tvoid\t\t(*disable)(struct perf_event *);\n\tvoid\t\t(*assign)(struct perf_event *event, int idx);\n\tvoid\t\t(*add)(struct perf_event *);\n\tvoid\t\t(*del)(struct perf_event *);\n\tvoid\t\t(*read)(struct perf_event *event);\n\tint\t\t(*set_period)(struct perf_event *event);\n\tu64\t\t(*update)(struct perf_event *event);\n\tint\t\t(*hw_config)(struct perf_event *event);\n\tint\t\t(*schedule_events)(struct cpu_hw_events *cpuc, int n, int *assign);\n\tunsigned\teventsel;\n\tunsigned\tperfctr;\n\tint\t\t(*addr_offset)(int index, bool eventsel);\n\tint\t\t(*rdpmc_index)(int index);\n\tu64\t\t(*event_map)(int);\n\tint\t\tmax_events;\n\tint\t\tnum_counters;\n\tint\t\tnum_counters_fixed;\n\tint\t\tcntval_bits;\n\tu64\t\tcntval_mask;\n\tunion {\n\t\t\tunsigned long events_maskl;\n\t\t\tunsigned long events_mask[BITS_TO_LONGS(ARCH_PERFMON_EVENTS_COUNT)];\n\t};\n\tint\t\tevents_mask_len;\n\tint\t\tapic;\n\tu64\t\tmax_period;\n\tstruct event_constraint *\n\t\t\t(*get_event_constraints)(struct cpu_hw_events *cpuc,\n\t\t\t\t\t\t int idx,\n\t\t\t\t\t\t struct perf_event *event);\n\n\tvoid\t\t(*put_event_constraints)(struct cpu_hw_events *cpuc,\n\t\t\t\t\t\t struct perf_event *event);\n\n\tvoid\t\t(*start_scheduling)(struct cpu_hw_events *cpuc);\n\n\tvoid\t\t(*commit_scheduling)(struct cpu_hw_events *cpuc, int idx, int cntr);\n\n\tvoid\t\t(*stop_scheduling)(struct cpu_hw_events *cpuc);\n\n\tstruct event_constraint *event_constraints;\n\tstruct x86_pmu_quirk *quirks;\n\tvoid\t\t(*limit_period)(struct perf_event *event, s64 *l);\n\n\t \n\tunsigned int\tlate_ack\t\t:1,\n\t\t\tmid_ack\t\t\t:1,\n\t\t\tenabled_ack\t\t:1;\n\t \n\tint\t\tattr_rdpmc_broken;\n\tint\t\tattr_rdpmc;\n\tstruct attribute **format_attrs;\n\n\tssize_t\t\t(*events_sysfs_show)(char *page, u64 config);\n\tconst struct attribute_group **attr_update;\n\n\tunsigned long\tattr_freeze_on_smi;\n\n\t \n\tint\t\t(*cpu_prepare)(int cpu);\n\tvoid\t\t(*cpu_starting)(int cpu);\n\tvoid\t\t(*cpu_dying)(int cpu);\n\tvoid\t\t(*cpu_dead)(int cpu);\n\n\tvoid\t\t(*check_microcode)(void);\n\tvoid\t\t(*sched_task)(struct perf_event_pmu_context *pmu_ctx,\n\t\t\t\t      bool sched_in);\n\n\t \n\tu64\t\t\tintel_ctrl;\n\tunion perf_capabilities intel_cap;\n\n\t \n\tunsigned int\tbts\t\t\t:1,\n\t\t\tbts_active\t\t:1,\n\t\t\tpebs\t\t\t:1,\n\t\t\tpebs_active\t\t:1,\n\t\t\tpebs_broken\t\t:1,\n\t\t\tpebs_prec_dist\t\t:1,\n\t\t\tpebs_no_tlb\t\t:1,\n\t\t\tpebs_no_isolation\t:1,\n\t\t\tpebs_block\t\t:1,\n\t\t\tpebs_ept\t\t:1;\n\tint\t\tpebs_record_size;\n\tint\t\tpebs_buffer_size;\n\tint\t\tmax_pebs_events;\n\tvoid\t\t(*drain_pebs)(struct pt_regs *regs, struct perf_sample_data *data);\n\tstruct event_constraint *pebs_constraints;\n\tvoid\t\t(*pebs_aliases)(struct perf_event *event);\n\tu64\t\t(*pebs_latency_data)(struct perf_event *event, u64 status);\n\tunsigned long\tlarge_pebs_flags;\n\tu64\t\trtm_abort_event;\n\tu64\t\tpebs_capable;\n\n\t \n\tunsigned int\tlbr_tos, lbr_from, lbr_to,\n\t\t\tlbr_info, lbr_nr;\t    \n\tunion {\n\t\tu64\tlbr_sel_mask;\t\t    \n\t\tu64\tlbr_ctl_mask;\t\t    \n\t};\n\tunion {\n\t\tconst int\t*lbr_sel_map;\t    \n\t\tint\t\t*lbr_ctl_map;\t    \n\t};\n\tbool\t\tlbr_double_abort;\t    \n\tbool\t\tlbr_pt_coexist;\t\t    \n\n\tunsigned int\tlbr_has_info:1;\n\tunsigned int\tlbr_has_tsx:1;\n\tunsigned int\tlbr_from_flags:1;\n\tunsigned int\tlbr_to_cycles:1;\n\n\t \n\tunsigned int\tlbr_depth_mask:8;\n\tunsigned int\tlbr_deep_c_reset:1;\n\tunsigned int\tlbr_lip:1;\n\tunsigned int\tlbr_cpl:1;\n\tunsigned int\tlbr_filter:1;\n\tunsigned int\tlbr_call_stack:1;\n\tunsigned int\tlbr_mispred:1;\n\tunsigned int\tlbr_timed_lbr:1;\n\tunsigned int\tlbr_br_type:1;\n\n\tvoid\t\t(*lbr_reset)(void);\n\tvoid\t\t(*lbr_read)(struct cpu_hw_events *cpuc);\n\tvoid\t\t(*lbr_save)(void *ctx);\n\tvoid\t\t(*lbr_restore)(void *ctx);\n\n\t \n\tatomic_t\tlbr_exclusive[x86_lbr_exclusive_max];\n\n\t \n\tint\t\tnum_topdown_events;\n\n\t \n\tvoid\t\t(*swap_task_ctx)(struct perf_event_pmu_context *prev_epc,\n\t\t\t\t\t struct perf_event_pmu_context *next_epc);\n\n\t \n\tunsigned int\tamd_nb_constraints : 1;\n\tu64\t\tperf_ctr_pair_en;\n\n\t \n\tstruct extra_reg *extra_regs;\n\tunsigned int flags;\n\n\t \n\tstruct perf_guest_switch_msr *(*guest_get_msrs)(int *nr, void *data);\n\n\t \n\tint (*check_period) (struct perf_event *event, u64 period);\n\n\tint (*aux_output_match) (struct perf_event *event);\n\n\tvoid (*filter)(struct pmu *pmu, int cpu, bool *ret);\n\t \n\tint\t\t\t\tnum_hybrid_pmus;\n\tstruct x86_hybrid_pmu\t\t*hybrid_pmu;\n\tu8 (*get_hybrid_cpu_type)\t(void);\n};\n\nstruct x86_perf_task_context_opt {\n\tint lbr_callstack_users;\n\tint lbr_stack_state;\n\tint log_id;\n};\n\nstruct x86_perf_task_context {\n\tu64 lbr_sel;\n\tint tos;\n\tint valid_lbrs;\n\tstruct x86_perf_task_context_opt opt;\n\tstruct lbr_entry lbr[MAX_LBR_ENTRIES];\n};\n\nstruct x86_perf_task_context_arch_lbr {\n\tstruct x86_perf_task_context_opt opt;\n\tstruct lbr_entry entries[];\n};\n\n \nstruct x86_perf_task_context_arch_lbr_xsave {\n\tstruct x86_perf_task_context_opt\t\topt;\n\n\tunion {\n\t\tstruct xregs_state\t\t\txsave;\n\t\tstruct {\n\t\t\tstruct fxregs_state\t\ti387;\n\t\t\tstruct xstate_header\t\theader;\n\t\t\tstruct arch_lbr_state\t\tlbr;\n\t\t} __attribute__ ((packed, aligned (XSAVE_ALIGNMENT)));\n\t};\n};\n\n#define x86_add_quirk(func_)\t\t\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tstatic struct x86_pmu_quirk __quirk __initdata = {\t\t\\\n\t\t.func = func_,\t\t\t\t\t\t\\\n\t};\t\t\t\t\t\t\t\t\\\n\t__quirk.next = x86_pmu.quirks;\t\t\t\t\t\\\n\tx86_pmu.quirks = &__quirk;\t\t\t\t\t\\\n} while (0)\n\n \n#define PMU_FL_NO_HT_SHARING\t0x1  \n#define PMU_FL_HAS_RSP_1\t0x2  \n#define PMU_FL_EXCL_CNTRS\t0x4  \n#define PMU_FL_EXCL_ENABLED\t0x8  \n#define PMU_FL_PEBS_ALL\t\t0x10  \n#define PMU_FL_TFA\t\t0x20  \n#define PMU_FL_PAIR\t\t0x40  \n#define PMU_FL_INSTR_LATENCY\t0x80  \n#define PMU_FL_MEM_LOADS_AUX\t0x100  \n#define PMU_FL_RETIRE_LATENCY\t0x200  \n\n#define EVENT_VAR(_id)  event_attr_##_id\n#define EVENT_PTR(_id) &event_attr_##_id.attr.attr\n\n#define EVENT_ATTR(_name, _id)\t\t\t\t\t\t\\\nstatic struct perf_pmu_events_attr EVENT_VAR(_id) = {\t\t\t\\\n\t.attr\t\t= __ATTR(_name, 0444, events_sysfs_show, NULL),\t\\\n\t.id\t\t= PERF_COUNT_HW_##_id,\t\t\t\t\\\n\t.event_str\t= NULL,\t\t\t\t\t\t\\\n};\n\n#define EVENT_ATTR_STR(_name, v, str)\t\t\t\t\t\\\nstatic struct perf_pmu_events_attr event_attr_##v = {\t\t\t\\\n\t.attr\t\t= __ATTR(_name, 0444, events_sysfs_show, NULL),\t\\\n\t.id\t\t= 0,\t\t\t\t\t\t\\\n\t.event_str\t= str,\t\t\t\t\t\t\\\n};\n\n#define EVENT_ATTR_STR_HT(_name, v, noht, ht)\t\t\t\t\\\nstatic struct perf_pmu_events_ht_attr event_attr_##v = {\t\t\\\n\t.attr\t\t= __ATTR(_name, 0444, events_ht_sysfs_show, NULL),\\\n\t.id\t\t= 0,\t\t\t\t\t\t\\\n\t.event_str_noht\t= noht,\t\t\t\t\t\t\\\n\t.event_str_ht\t= ht,\t\t\t\t\t\t\\\n}\n\n#define EVENT_ATTR_STR_HYBRID(_name, v, str, _pmu)\t\t\t\\\nstatic struct perf_pmu_events_hybrid_attr event_attr_##v = {\t\t\\\n\t.attr\t\t= __ATTR(_name, 0444, events_hybrid_sysfs_show, NULL),\\\n\t.id\t\t= 0,\t\t\t\t\t\t\\\n\t.event_str\t= str,\t\t\t\t\t\t\\\n\t.pmu_type\t= _pmu,\t\t\t\t\t\t\\\n}\n\n#define FORMAT_HYBRID_PTR(_id) (&format_attr_hybrid_##_id.attr.attr)\n\n#define FORMAT_ATTR_HYBRID(_name, _pmu)\t\t\t\t\t\\\nstatic struct perf_pmu_format_hybrid_attr format_attr_hybrid_##_name = {\\\n\t.attr\t\t= __ATTR_RO(_name),\t\t\t\t\\\n\t.pmu_type\t= _pmu,\t\t\t\t\t\t\\\n}\n\nstruct pmu *x86_get_pmu(unsigned int cpu);\nextern struct x86_pmu x86_pmu __read_mostly;\n\nDECLARE_STATIC_CALL(x86_pmu_set_period, *x86_pmu.set_period);\nDECLARE_STATIC_CALL(x86_pmu_update,     *x86_pmu.update);\n\nstatic __always_inline struct x86_perf_task_context_opt *task_context_opt(void *ctx)\n{\n\tif (static_cpu_has(X86_FEATURE_ARCH_LBR))\n\t\treturn &((struct x86_perf_task_context_arch_lbr *)ctx)->opt;\n\n\treturn &((struct x86_perf_task_context *)ctx)->opt;\n}\n\nstatic inline bool x86_pmu_has_lbr_callstack(void)\n{\n\treturn  x86_pmu.lbr_sel_map &&\n\t\tx86_pmu.lbr_sel_map[PERF_SAMPLE_BRANCH_CALL_STACK_SHIFT] > 0;\n}\n\nDECLARE_PER_CPU(struct cpu_hw_events, cpu_hw_events);\nDECLARE_PER_CPU(u64 [X86_PMC_IDX_MAX], pmc_prev_left);\n\nint x86_perf_event_set_period(struct perf_event *event);\n\n \n\n#define C(x) PERF_COUNT_HW_CACHE_##x\n\nextern u64 __read_mostly hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX];\nextern u64 __read_mostly hw_cache_extra_regs\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX];\n\nu64 x86_perf_event_update(struct perf_event *event);\n\nstatic inline unsigned int x86_pmu_config_addr(int index)\n{\n\treturn x86_pmu.eventsel + (x86_pmu.addr_offset ?\n\t\t\t\t   x86_pmu.addr_offset(index, true) : index);\n}\n\nstatic inline unsigned int x86_pmu_event_addr(int index)\n{\n\treturn x86_pmu.perfctr + (x86_pmu.addr_offset ?\n\t\t\t\t  x86_pmu.addr_offset(index, false) : index);\n}\n\nstatic inline int x86_pmu_rdpmc_index(int index)\n{\n\treturn x86_pmu.rdpmc_index ? x86_pmu.rdpmc_index(index) : index;\n}\n\nbool check_hw_exists(struct pmu *pmu, int num_counters,\n\t\t     int num_counters_fixed);\n\nint x86_add_exclusive(unsigned int what);\n\nvoid x86_del_exclusive(unsigned int what);\n\nint x86_reserve_hardware(void);\n\nvoid x86_release_hardware(void);\n\nint x86_pmu_max_precise(void);\n\nvoid hw_perf_lbr_event_destroy(struct perf_event *event);\n\nint x86_setup_perfctr(struct perf_event *event);\n\nint x86_pmu_hw_config(struct perf_event *event);\n\nvoid x86_pmu_disable_all(void);\n\nstatic inline bool has_amd_brs(struct hw_perf_event *hwc)\n{\n\treturn hwc->flags & PERF_X86_EVENT_AMD_BRS;\n}\n\nstatic inline bool is_counter_pair(struct hw_perf_event *hwc)\n{\n\treturn hwc->flags & PERF_X86_EVENT_PAIR;\n}\n\nstatic inline void __x86_pmu_enable_event(struct hw_perf_event *hwc,\n\t\t\t\t\t  u64 enable_mask)\n{\n\tu64 disable_mask = __this_cpu_read(cpu_hw_events.perf_ctr_virt_mask);\n\n\tif (hwc->extra_reg.reg)\n\t\twrmsrl(hwc->extra_reg.reg, hwc->extra_reg.config);\n\n\t \n\tif (is_counter_pair(hwc))\n\t\twrmsrl(x86_pmu_config_addr(hwc->idx + 1), x86_pmu.perf_ctr_pair_en);\n\n\twrmsrl(hwc->config_base, (hwc->config | enable_mask) & ~disable_mask);\n}\n\nvoid x86_pmu_enable_all(int added);\n\nint perf_assign_events(struct event_constraint **constraints, int n,\n\t\t\tint wmin, int wmax, int gpmax, int *assign);\nint x86_schedule_events(struct cpu_hw_events *cpuc, int n, int *assign);\n\nvoid x86_pmu_stop(struct perf_event *event, int flags);\n\nstatic inline void x86_pmu_disable_event(struct perf_event *event)\n{\n\tu64 disable_mask = __this_cpu_read(cpu_hw_events.perf_ctr_virt_mask);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\twrmsrl(hwc->config_base, hwc->config & ~disable_mask);\n\n\tif (is_counter_pair(hwc))\n\t\twrmsrl(x86_pmu_config_addr(hwc->idx + 1), 0);\n}\n\nvoid x86_pmu_enable_event(struct perf_event *event);\n\nint x86_pmu_handle_irq(struct pt_regs *regs);\n\nvoid x86_pmu_show_pmu_cap(int num_counters, int num_counters_fixed,\n\t\t\t  u64 intel_ctrl);\n\nextern struct event_constraint emptyconstraint;\n\nextern struct event_constraint unconstrained;\n\nstatic inline bool kernel_ip(unsigned long ip)\n{\n#ifdef CONFIG_X86_32\n\treturn ip > PAGE_OFFSET;\n#else\n\treturn (long)ip < 0;\n#endif\n}\n\n \nstatic inline void set_linear_ip(struct pt_regs *regs, unsigned long ip)\n{\n\tregs->cs = kernel_ip(ip) ? __KERNEL_CS : __USER_CS;\n\tif (regs->flags & X86_VM_MASK)\n\t\tregs->flags ^= (PERF_EFLAGS_VM | X86_VM_MASK);\n\tregs->ip = ip;\n}\n\n \nenum {\n\tX86_BR_NONE\t\t= 0,       \n\n\tX86_BR_USER\t\t= 1 << 0,  \n\tX86_BR_KERNEL\t\t= 1 << 1,  \n\n\tX86_BR_CALL\t\t= 1 << 2,  \n\tX86_BR_RET\t\t= 1 << 3,  \n\tX86_BR_SYSCALL\t\t= 1 << 4,  \n\tX86_BR_SYSRET\t\t= 1 << 5,  \n\tX86_BR_INT\t\t= 1 << 6,  \n\tX86_BR_IRET\t\t= 1 << 7,  \n\tX86_BR_JCC\t\t= 1 << 8,  \n\tX86_BR_JMP\t\t= 1 << 9,  \n\tX86_BR_IRQ\t\t= 1 << 10, \n\tX86_BR_IND_CALL\t\t= 1 << 11, \n\tX86_BR_ABORT\t\t= 1 << 12, \n\tX86_BR_IN_TX\t\t= 1 << 13, \n\tX86_BR_NO_TX\t\t= 1 << 14, \n\tX86_BR_ZERO_CALL\t= 1 << 15, \n\tX86_BR_CALL_STACK\t= 1 << 16, \n\tX86_BR_IND_JMP\t\t= 1 << 17, \n\n\tX86_BR_TYPE_SAVE\t= 1 << 18, \n\n};\n\n#define X86_BR_PLM (X86_BR_USER | X86_BR_KERNEL)\n#define X86_BR_ANYTX (X86_BR_NO_TX | X86_BR_IN_TX)\n\n#define X86_BR_ANY       \\\n\t(X86_BR_CALL    |\\\n\t X86_BR_RET     |\\\n\t X86_BR_SYSCALL |\\\n\t X86_BR_SYSRET  |\\\n\t X86_BR_INT     |\\\n\t X86_BR_IRET    |\\\n\t X86_BR_JCC     |\\\n\t X86_BR_JMP\t |\\\n\t X86_BR_IRQ\t |\\\n\t X86_BR_ABORT\t |\\\n\t X86_BR_IND_CALL |\\\n\t X86_BR_IND_JMP  |\\\n\t X86_BR_ZERO_CALL)\n\n#define X86_BR_ALL (X86_BR_PLM | X86_BR_ANY)\n\n#define X86_BR_ANY_CALL\t\t \\\n\t(X86_BR_CALL\t\t|\\\n\t X86_BR_IND_CALL\t|\\\n\t X86_BR_ZERO_CALL\t|\\\n\t X86_BR_SYSCALL\t\t|\\\n\t X86_BR_IRQ\t\t|\\\n\t X86_BR_INT)\n\nint common_branch_type(int type);\nint branch_type(unsigned long from, unsigned long to, int abort);\nint branch_type_fused(unsigned long from, unsigned long to, int abort,\n\t\t      int *offset);\n\nssize_t x86_event_sysfs_show(char *page, u64 config, u64 event);\nssize_t intel_event_sysfs_show(char *page, u64 config);\n\nssize_t events_sysfs_show(struct device *dev, struct device_attribute *attr,\n\t\t\t  char *page);\nssize_t events_ht_sysfs_show(struct device *dev, struct device_attribute *attr,\n\t\t\t  char *page);\nssize_t events_hybrid_sysfs_show(struct device *dev,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t char *page);\n\nstatic inline bool fixed_counter_disabled(int i, struct pmu *pmu)\n{\n\tu64 intel_ctrl = hybrid(pmu, intel_ctrl);\n\n\treturn !(intel_ctrl >> (i + INTEL_PMC_IDX_FIXED));\n}\n\n#ifdef CONFIG_CPU_SUP_AMD\n\nint amd_pmu_init(void);\n\nint amd_pmu_lbr_init(void);\nvoid amd_pmu_lbr_reset(void);\nvoid amd_pmu_lbr_read(void);\nvoid amd_pmu_lbr_add(struct perf_event *event);\nvoid amd_pmu_lbr_del(struct perf_event *event);\nvoid amd_pmu_lbr_sched_task(struct perf_event_pmu_context *pmu_ctx, bool sched_in);\nvoid amd_pmu_lbr_enable_all(void);\nvoid amd_pmu_lbr_disable_all(void);\nint amd_pmu_lbr_hw_config(struct perf_event *event);\n\n#ifdef CONFIG_PERF_EVENTS_AMD_BRS\n\n#define AMD_FAM19H_BRS_EVENT 0xc4  \n\nint amd_brs_init(void);\nvoid amd_brs_disable(void);\nvoid amd_brs_enable(void);\nvoid amd_brs_enable_all(void);\nvoid amd_brs_disable_all(void);\nvoid amd_brs_drain(void);\nvoid amd_brs_lopwr_init(void);\nint amd_brs_hw_config(struct perf_event *event);\nvoid amd_brs_reset(void);\n\nstatic inline void amd_pmu_brs_add(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tperf_sched_cb_inc(event->pmu);\n\tcpuc->lbr_users++;\n\t \n}\n\nstatic inline void amd_pmu_brs_del(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tcpuc->lbr_users--;\n\tWARN_ON_ONCE(cpuc->lbr_users < 0);\n\n\tperf_sched_cb_dec(event->pmu);\n}\n\nvoid amd_pmu_brs_sched_task(struct perf_event_pmu_context *pmu_ctx, bool sched_in);\n#else\nstatic inline int amd_brs_init(void)\n{\n\treturn 0;\n}\nstatic inline void amd_brs_disable(void) {}\nstatic inline void amd_brs_enable(void) {}\nstatic inline void amd_brs_drain(void) {}\nstatic inline void amd_brs_lopwr_init(void) {}\nstatic inline void amd_brs_disable_all(void) {}\nstatic inline int amd_brs_hw_config(struct perf_event *event)\n{\n\treturn 0;\n}\nstatic inline void amd_brs_reset(void) {}\n\nstatic inline void amd_pmu_brs_add(struct perf_event *event)\n{\n}\n\nstatic inline void amd_pmu_brs_del(struct perf_event *event)\n{\n}\n\nstatic inline void amd_pmu_brs_sched_task(struct perf_event_pmu_context *pmu_ctx, bool sched_in)\n{\n}\n\nstatic inline void amd_brs_enable_all(void)\n{\n}\n\n#endif\n\n#else  \n\nstatic inline int amd_pmu_init(void)\n{\n\treturn 0;\n}\n\nstatic inline int amd_brs_init(void)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic inline void amd_brs_drain(void)\n{\n}\n\nstatic inline void amd_brs_enable_all(void)\n{\n}\n\nstatic inline void amd_brs_disable_all(void)\n{\n}\n#endif  \n\nstatic inline int is_pebs_pt(struct perf_event *event)\n{\n\treturn !!(event->hw.flags & PERF_X86_EVENT_PEBS_VIA_PT);\n}\n\n#ifdef CONFIG_CPU_SUP_INTEL\n\nstatic inline bool intel_pmu_has_bts_period(struct perf_event *event, u64 period)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tunsigned int hw_event, bts_event;\n\n\tif (event->attr.freq)\n\t\treturn false;\n\n\thw_event = hwc->config & INTEL_ARCH_EVENT_MASK;\n\tbts_event = x86_pmu.event_map(PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\n\n\treturn hw_event == bts_event && period == 1;\n}\n\nstatic inline bool intel_pmu_has_bts(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\treturn intel_pmu_has_bts_period(event, hwc->sample_period);\n}\n\nstatic __always_inline void __intel_pmu_pebs_disable_all(void)\n{\n\twrmsrl(MSR_IA32_PEBS_ENABLE, 0);\n}\n\nstatic __always_inline void __intel_pmu_arch_lbr_disable(void)\n{\n\twrmsrl(MSR_ARCH_LBR_CTL, 0);\n}\n\nstatic __always_inline void __intel_pmu_lbr_disable(void)\n{\n\tu64 debugctl;\n\n\trdmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);\n\tdebugctl &= ~(DEBUGCTLMSR_LBR | DEBUGCTLMSR_FREEZE_LBRS_ON_PMI);\n\twrmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);\n}\n\nint intel_pmu_save_and_restart(struct perf_event *event);\n\nstruct event_constraint *\nx86_get_event_constraints(struct cpu_hw_events *cpuc, int idx,\n\t\t\t  struct perf_event *event);\n\nextern int intel_cpuc_prepare(struct cpu_hw_events *cpuc, int cpu);\nextern void intel_cpuc_finish(struct cpu_hw_events *cpuc);\n\nint intel_pmu_init(void);\n\nvoid init_debug_store_on_cpu(int cpu);\n\nvoid fini_debug_store_on_cpu(int cpu);\n\nvoid release_ds_buffers(void);\n\nvoid reserve_ds_buffers(void);\n\nvoid release_lbr_buffers(void);\n\nvoid reserve_lbr_buffers(void);\n\nextern struct event_constraint bts_constraint;\nextern struct event_constraint vlbr_constraint;\n\nvoid intel_pmu_enable_bts(u64 config);\n\nvoid intel_pmu_disable_bts(void);\n\nint intel_pmu_drain_bts_buffer(void);\n\nu64 adl_latency_data_small(struct perf_event *event, u64 status);\n\nu64 mtl_latency_data_small(struct perf_event *event, u64 status);\n\nextern struct event_constraint intel_core2_pebs_event_constraints[];\n\nextern struct event_constraint intel_atom_pebs_event_constraints[];\n\nextern struct event_constraint intel_slm_pebs_event_constraints[];\n\nextern struct event_constraint intel_glm_pebs_event_constraints[];\n\nextern struct event_constraint intel_glp_pebs_event_constraints[];\n\nextern struct event_constraint intel_grt_pebs_event_constraints[];\n\nextern struct event_constraint intel_nehalem_pebs_event_constraints[];\n\nextern struct event_constraint intel_westmere_pebs_event_constraints[];\n\nextern struct event_constraint intel_snb_pebs_event_constraints[];\n\nextern struct event_constraint intel_ivb_pebs_event_constraints[];\n\nextern struct event_constraint intel_hsw_pebs_event_constraints[];\n\nextern struct event_constraint intel_bdw_pebs_event_constraints[];\n\nextern struct event_constraint intel_skl_pebs_event_constraints[];\n\nextern struct event_constraint intel_icl_pebs_event_constraints[];\n\nextern struct event_constraint intel_spr_pebs_event_constraints[];\n\nstruct event_constraint *intel_pebs_constraints(struct perf_event *event);\n\nvoid intel_pmu_pebs_add(struct perf_event *event);\n\nvoid intel_pmu_pebs_del(struct perf_event *event);\n\nvoid intel_pmu_pebs_enable(struct perf_event *event);\n\nvoid intel_pmu_pebs_disable(struct perf_event *event);\n\nvoid intel_pmu_pebs_enable_all(void);\n\nvoid intel_pmu_pebs_disable_all(void);\n\nvoid intel_pmu_pebs_sched_task(struct perf_event_pmu_context *pmu_ctx, bool sched_in);\n\nvoid intel_pmu_auto_reload_read(struct perf_event *event);\n\nvoid intel_pmu_store_pebs_lbrs(struct lbr_entry *lbr);\n\nvoid intel_ds_init(void);\n\nvoid intel_pmu_lbr_swap_task_ctx(struct perf_event_pmu_context *prev_epc,\n\t\t\t\t struct perf_event_pmu_context *next_epc);\n\nvoid intel_pmu_lbr_sched_task(struct perf_event_pmu_context *pmu_ctx, bool sched_in);\n\nu64 lbr_from_signext_quirk_wr(u64 val);\n\nvoid intel_pmu_lbr_reset(void);\n\nvoid intel_pmu_lbr_reset_32(void);\n\nvoid intel_pmu_lbr_reset_64(void);\n\nvoid intel_pmu_lbr_add(struct perf_event *event);\n\nvoid intel_pmu_lbr_del(struct perf_event *event);\n\nvoid intel_pmu_lbr_enable_all(bool pmi);\n\nvoid intel_pmu_lbr_disable_all(void);\n\nvoid intel_pmu_lbr_read(void);\n\nvoid intel_pmu_lbr_read_32(struct cpu_hw_events *cpuc);\n\nvoid intel_pmu_lbr_read_64(struct cpu_hw_events *cpuc);\n\nvoid intel_pmu_lbr_save(void *ctx);\n\nvoid intel_pmu_lbr_restore(void *ctx);\n\nvoid intel_pmu_lbr_init_core(void);\n\nvoid intel_pmu_lbr_init_nhm(void);\n\nvoid intel_pmu_lbr_init_atom(void);\n\nvoid intel_pmu_lbr_init_slm(void);\n\nvoid intel_pmu_lbr_init_snb(void);\n\nvoid intel_pmu_lbr_init_hsw(void);\n\nvoid intel_pmu_lbr_init_skl(void);\n\nvoid intel_pmu_lbr_init_knl(void);\n\nvoid intel_pmu_lbr_init(void);\n\nvoid intel_pmu_arch_lbr_init(void);\n\nvoid intel_pmu_pebs_data_source_nhm(void);\n\nvoid intel_pmu_pebs_data_source_skl(bool pmem);\n\nvoid intel_pmu_pebs_data_source_adl(void);\n\nvoid intel_pmu_pebs_data_source_grt(void);\n\nvoid intel_pmu_pebs_data_source_mtl(void);\n\nvoid intel_pmu_pebs_data_source_cmt(void);\n\nint intel_pmu_setup_lbr_filter(struct perf_event *event);\n\nvoid intel_pt_interrupt(void);\n\nint intel_bts_interrupt(void);\n\nvoid intel_bts_enable_local(void);\n\nvoid intel_bts_disable_local(void);\n\nint p4_pmu_init(void);\n\nint p6_pmu_init(void);\n\nint knc_pmu_init(void);\n\nstatic inline int is_ht_workaround_enabled(void)\n{\n\treturn !!(x86_pmu.flags & PMU_FL_EXCL_ENABLED);\n}\n\n#else  \n\nstatic inline void reserve_ds_buffers(void)\n{\n}\n\nstatic inline void release_ds_buffers(void)\n{\n}\n\nstatic inline void release_lbr_buffers(void)\n{\n}\n\nstatic inline void reserve_lbr_buffers(void)\n{\n}\n\nstatic inline int intel_pmu_init(void)\n{\n\treturn 0;\n}\n\nstatic inline int intel_cpuc_prepare(struct cpu_hw_events *cpuc, int cpu)\n{\n\treturn 0;\n}\n\nstatic inline void intel_cpuc_finish(struct cpu_hw_events *cpuc)\n{\n}\n\nstatic inline int is_ht_workaround_enabled(void)\n{\n\treturn 0;\n}\n#endif  \n\n#if ((defined CONFIG_CPU_SUP_CENTAUR) || (defined CONFIG_CPU_SUP_ZHAOXIN))\nint zhaoxin_pmu_init(void);\n#else\nstatic inline int zhaoxin_pmu_init(void)\n{\n\treturn 0;\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}