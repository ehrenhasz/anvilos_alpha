{
  "module_name": "pt.c",
  "hash_id": "1f76d78490551fe67d4ccd22ca3041a71b2ba592d2e61b994576a1badc7e5a8b",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/events/intel/pt.c",
  "human_readable_source": "\n \n\n#undef DEBUG\n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/types.h>\n#include <linux/bits.h>\n#include <linux/limits.h>\n#include <linux/slab.h>\n#include <linux/device.h>\n\n#include <asm/perf_event.h>\n#include <asm/insn.h>\n#include <asm/io.h>\n#include <asm/intel_pt.h>\n#include <asm/intel-family.h>\n\n#include \"../perf_event.h\"\n#include \"pt.h\"\n\nstatic DEFINE_PER_CPU(struct pt, pt_ctx);\n\nstatic struct pt_pmu pt_pmu;\n\n \n#define PT_CAP(_n, _l, _r, _m)\t\t\t\t\t\t\\\n\t[PT_CAP_ ## _n] = { .name = __stringify(_n), .leaf = _l,\t\\\n\t\t\t    .reg = _r, .mask = _m }\n\nstatic struct pt_cap_desc {\n\tconst char\t*name;\n\tu32\t\tleaf;\n\tu8\t\treg;\n\tu32\t\tmask;\n} pt_caps[] = {\n\tPT_CAP(max_subleaf,\t\t0, CPUID_EAX, 0xffffffff),\n\tPT_CAP(cr3_filtering,\t\t0, CPUID_EBX, BIT(0)),\n\tPT_CAP(psb_cyc,\t\t\t0, CPUID_EBX, BIT(1)),\n\tPT_CAP(ip_filtering,\t\t0, CPUID_EBX, BIT(2)),\n\tPT_CAP(mtc,\t\t\t0, CPUID_EBX, BIT(3)),\n\tPT_CAP(ptwrite,\t\t\t0, CPUID_EBX, BIT(4)),\n\tPT_CAP(power_event_trace,\t0, CPUID_EBX, BIT(5)),\n\tPT_CAP(event_trace,\t\t0, CPUID_EBX, BIT(7)),\n\tPT_CAP(tnt_disable,\t\t0, CPUID_EBX, BIT(8)),\n\tPT_CAP(topa_output,\t\t0, CPUID_ECX, BIT(0)),\n\tPT_CAP(topa_multiple_entries,\t0, CPUID_ECX, BIT(1)),\n\tPT_CAP(single_range_output,\t0, CPUID_ECX, BIT(2)),\n\tPT_CAP(output_subsys,\t\t0, CPUID_ECX, BIT(3)),\n\tPT_CAP(payloads_lip,\t\t0, CPUID_ECX, BIT(31)),\n\tPT_CAP(num_address_ranges,\t1, CPUID_EAX, 0x7),\n\tPT_CAP(mtc_periods,\t\t1, CPUID_EAX, 0xffff0000),\n\tPT_CAP(cycle_thresholds,\t1, CPUID_EBX, 0xffff),\n\tPT_CAP(psb_periods,\t\t1, CPUID_EBX, 0xffff0000),\n};\n\nu32 intel_pt_validate_cap(u32 *caps, enum pt_capabilities capability)\n{\n\tstruct pt_cap_desc *cd = &pt_caps[capability];\n\tu32 c = caps[cd->leaf * PT_CPUID_REGS_NUM + cd->reg];\n\tunsigned int shift = __ffs(cd->mask);\n\n\treturn (c & cd->mask) >> shift;\n}\nEXPORT_SYMBOL_GPL(intel_pt_validate_cap);\n\nu32 intel_pt_validate_hw_cap(enum pt_capabilities cap)\n{\n\treturn intel_pt_validate_cap(pt_pmu.caps, cap);\n}\nEXPORT_SYMBOL_GPL(intel_pt_validate_hw_cap);\n\nstatic ssize_t pt_cap_show(struct device *cdev,\n\t\t\t   struct device_attribute *attr,\n\t\t\t   char *buf)\n{\n\tstruct dev_ext_attribute *ea =\n\t\tcontainer_of(attr, struct dev_ext_attribute, attr);\n\tenum pt_capabilities cap = (long)ea->var;\n\n\treturn snprintf(buf, PAGE_SIZE, \"%x\\n\", intel_pt_validate_hw_cap(cap));\n}\n\nstatic struct attribute_group pt_cap_group __ro_after_init = {\n\t.name\t= \"caps\",\n};\n\nPMU_FORMAT_ATTR(pt,\t\t\"config:0\"\t);\nPMU_FORMAT_ATTR(cyc,\t\t\"config:1\"\t);\nPMU_FORMAT_ATTR(pwr_evt,\t\"config:4\"\t);\nPMU_FORMAT_ATTR(fup_on_ptw,\t\"config:5\"\t);\nPMU_FORMAT_ATTR(mtc,\t\t\"config:9\"\t);\nPMU_FORMAT_ATTR(tsc,\t\t\"config:10\"\t);\nPMU_FORMAT_ATTR(noretcomp,\t\"config:11\"\t);\nPMU_FORMAT_ATTR(ptw,\t\t\"config:12\"\t);\nPMU_FORMAT_ATTR(branch,\t\t\"config:13\"\t);\nPMU_FORMAT_ATTR(event,\t\t\"config:31\"\t);\nPMU_FORMAT_ATTR(notnt,\t\t\"config:55\"\t);\nPMU_FORMAT_ATTR(mtc_period,\t\"config:14-17\"\t);\nPMU_FORMAT_ATTR(cyc_thresh,\t\"config:19-22\"\t);\nPMU_FORMAT_ATTR(psb_period,\t\"config:24-27\"\t);\n\nstatic struct attribute *pt_formats_attr[] = {\n\t&format_attr_pt.attr,\n\t&format_attr_cyc.attr,\n\t&format_attr_pwr_evt.attr,\n\t&format_attr_event.attr,\n\t&format_attr_notnt.attr,\n\t&format_attr_fup_on_ptw.attr,\n\t&format_attr_mtc.attr,\n\t&format_attr_tsc.attr,\n\t&format_attr_noretcomp.attr,\n\t&format_attr_ptw.attr,\n\t&format_attr_branch.attr,\n\t&format_attr_mtc_period.attr,\n\t&format_attr_cyc_thresh.attr,\n\t&format_attr_psb_period.attr,\n\tNULL,\n};\n\nstatic struct attribute_group pt_format_group = {\n\t.name\t= \"format\",\n\t.attrs\t= pt_formats_attr,\n};\n\nstatic ssize_t\npt_timing_attr_show(struct device *dev, struct device_attribute *attr,\n\t\t    char *page)\n{\n\tstruct perf_pmu_events_attr *pmu_attr =\n\t\tcontainer_of(attr, struct perf_pmu_events_attr, attr);\n\n\tswitch (pmu_attr->id) {\n\tcase 0:\n\t\treturn sprintf(page, \"%lu\\n\", pt_pmu.max_nonturbo_ratio);\n\tcase 1:\n\t\treturn sprintf(page, \"%u:%u\\n\",\n\t\t\t       pt_pmu.tsc_art_num,\n\t\t\t       pt_pmu.tsc_art_den);\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn -EINVAL;\n}\n\nPMU_EVENT_ATTR(max_nonturbo_ratio, timing_attr_max_nonturbo_ratio, 0,\n\t       pt_timing_attr_show);\nPMU_EVENT_ATTR(tsc_art_ratio, timing_attr_tsc_art_ratio, 1,\n\t       pt_timing_attr_show);\n\nstatic struct attribute *pt_timing_attr[] = {\n\t&timing_attr_max_nonturbo_ratio.attr.attr,\n\t&timing_attr_tsc_art_ratio.attr.attr,\n\tNULL,\n};\n\nstatic struct attribute_group pt_timing_group = {\n\t.attrs\t= pt_timing_attr,\n};\n\nstatic const struct attribute_group *pt_attr_groups[] = {\n\t&pt_cap_group,\n\t&pt_format_group,\n\t&pt_timing_group,\n\tNULL,\n};\n\nstatic int __init pt_pmu_hw_init(void)\n{\n\tstruct dev_ext_attribute *de_attrs;\n\tstruct attribute **attrs;\n\tsize_t size;\n\tu64 reg;\n\tint ret;\n\tlong i;\n\n\trdmsrl(MSR_PLATFORM_INFO, reg);\n\tpt_pmu.max_nonturbo_ratio = (reg & 0xff00) >> 8;\n\n\t \n\tif (boot_cpu_data.cpuid_level >= CPUID_TSC_LEAF) {\n\t\tu32 eax, ebx, ecx, edx;\n\n\t\tcpuid(CPUID_TSC_LEAF, &eax, &ebx, &ecx, &edx);\n\n\t\tpt_pmu.tsc_art_num = ebx;\n\t\tpt_pmu.tsc_art_den = eax;\n\t}\n\n\t \n\tswitch (boot_cpu_data.x86_model) {\n\tcase INTEL_FAM6_BROADWELL:\n\tcase INTEL_FAM6_BROADWELL_D:\n\tcase INTEL_FAM6_BROADWELL_G:\n\tcase INTEL_FAM6_BROADWELL_X:\n\t\t \n\t\tpt_pmu.branch_en_always_on = true;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_VMX)) {\n\t\t \n\t\trdmsrl(MSR_IA32_VMX_MISC, reg);\n\t\tif (reg & BIT(14))\n\t\t\tpt_pmu.vmx = true;\n\t}\n\n\tfor (i = 0; i < PT_CPUID_LEAVES; i++) {\n\t\tcpuid_count(20, i,\n\t\t\t    &pt_pmu.caps[CPUID_EAX + i*PT_CPUID_REGS_NUM],\n\t\t\t    &pt_pmu.caps[CPUID_EBX + i*PT_CPUID_REGS_NUM],\n\t\t\t    &pt_pmu.caps[CPUID_ECX + i*PT_CPUID_REGS_NUM],\n\t\t\t    &pt_pmu.caps[CPUID_EDX + i*PT_CPUID_REGS_NUM]);\n\t}\n\n\tret = -ENOMEM;\n\tsize = sizeof(struct attribute *) * (ARRAY_SIZE(pt_caps)+1);\n\tattrs = kzalloc(size, GFP_KERNEL);\n\tif (!attrs)\n\t\tgoto fail;\n\n\tsize = sizeof(struct dev_ext_attribute) * (ARRAY_SIZE(pt_caps)+1);\n\tde_attrs = kzalloc(size, GFP_KERNEL);\n\tif (!de_attrs)\n\t\tgoto fail;\n\n\tfor (i = 0; i < ARRAY_SIZE(pt_caps); i++) {\n\t\tstruct dev_ext_attribute *de_attr = de_attrs + i;\n\n\t\tde_attr->attr.attr.name = pt_caps[i].name;\n\n\t\tsysfs_attr_init(&de_attr->attr.attr);\n\n\t\tde_attr->attr.attr.mode\t\t= S_IRUGO;\n\t\tde_attr->attr.show\t\t= pt_cap_show;\n\t\tde_attr->var\t\t\t= (void *)i;\n\n\t\tattrs[i] = &de_attr->attr.attr;\n\t}\n\n\tpt_cap_group.attrs = attrs;\n\n\treturn 0;\n\nfail:\n\tkfree(attrs);\n\n\treturn ret;\n}\n\n#define RTIT_CTL_CYC_PSB (RTIT_CTL_CYCLEACC\t| \\\n\t\t\t  RTIT_CTL_CYC_THRESH\t| \\\n\t\t\t  RTIT_CTL_PSB_FREQ)\n\n#define RTIT_CTL_MTC\t(RTIT_CTL_MTC_EN\t| \\\n\t\t\t RTIT_CTL_MTC_RANGE)\n\n#define RTIT_CTL_PTW\t(RTIT_CTL_PTW_EN\t| \\\n\t\t\t RTIT_CTL_FUP_ON_PTW)\n\n \n#define RTIT_CTL_PASSTHROUGH RTIT_CTL_TRACEEN\n\n#define PT_CONFIG_MASK (RTIT_CTL_TRACEEN\t| \\\n\t\t\tRTIT_CTL_TSC_EN\t\t| \\\n\t\t\tRTIT_CTL_DISRETC\t| \\\n\t\t\tRTIT_CTL_BRANCH_EN\t| \\\n\t\t\tRTIT_CTL_CYC_PSB\t| \\\n\t\t\tRTIT_CTL_MTC\t\t| \\\n\t\t\tRTIT_CTL_PWR_EVT_EN\t| \\\n\t\t\tRTIT_CTL_EVENT_EN\t| \\\n\t\t\tRTIT_CTL_NOTNT\t\t| \\\n\t\t\tRTIT_CTL_FUP_ON_PTW\t| \\\n\t\t\tRTIT_CTL_PTW_EN)\n\nstatic bool pt_event_valid(struct perf_event *event)\n{\n\tu64 config = event->attr.config;\n\tu64 allowed, requested;\n\n\tif ((config & PT_CONFIG_MASK) != config)\n\t\treturn false;\n\n\tif (config & RTIT_CTL_CYC_PSB) {\n\t\tif (!intel_pt_validate_hw_cap(PT_CAP_psb_cyc))\n\t\t\treturn false;\n\n\t\tallowed = intel_pt_validate_hw_cap(PT_CAP_psb_periods);\n\t\trequested = (config & RTIT_CTL_PSB_FREQ) >>\n\t\t\tRTIT_CTL_PSB_FREQ_OFFSET;\n\t\tif (requested && (!(allowed & BIT(requested))))\n\t\t\treturn false;\n\n\t\tallowed = intel_pt_validate_hw_cap(PT_CAP_cycle_thresholds);\n\t\trequested = (config & RTIT_CTL_CYC_THRESH) >>\n\t\t\tRTIT_CTL_CYC_THRESH_OFFSET;\n\t\tif (requested && (!(allowed & BIT(requested))))\n\t\t\treturn false;\n\t}\n\n\tif (config & RTIT_CTL_MTC) {\n\t\t \n\t\tif (!intel_pt_validate_hw_cap(PT_CAP_mtc))\n\t\t\treturn false;\n\n\t\tallowed = intel_pt_validate_hw_cap(PT_CAP_mtc_periods);\n\t\tif (!allowed)\n\t\t\treturn false;\n\n\t\trequested = (config & RTIT_CTL_MTC_RANGE) >>\n\t\t\tRTIT_CTL_MTC_RANGE_OFFSET;\n\n\t\tif (!(allowed & BIT(requested)))\n\t\t\treturn false;\n\t}\n\n\tif (config & RTIT_CTL_PWR_EVT_EN &&\n\t    !intel_pt_validate_hw_cap(PT_CAP_power_event_trace))\n\t\treturn false;\n\n\tif (config & RTIT_CTL_EVENT_EN &&\n\t    !intel_pt_validate_hw_cap(PT_CAP_event_trace))\n\t\treturn false;\n\n\tif (config & RTIT_CTL_NOTNT &&\n\t    !intel_pt_validate_hw_cap(PT_CAP_tnt_disable))\n\t\treturn false;\n\n\tif (config & RTIT_CTL_PTW) {\n\t\tif (!intel_pt_validate_hw_cap(PT_CAP_ptwrite))\n\t\t\treturn false;\n\n\t\t \n\t\tif ((config & RTIT_CTL_FUP_ON_PTW) &&\n\t\t    !(config & RTIT_CTL_PTW_EN))\n\t\t\treturn false;\n\t}\n\n\t \n\tif (config & RTIT_CTL_PASSTHROUGH) {\n\t\t \n\t\tif (pt_pmu.branch_en_always_on &&\n\t\t    !(config & RTIT_CTL_BRANCH_EN))\n\t\t\treturn false;\n\t} else {\n\t\t \n\t\tif (config & RTIT_CTL_BRANCH_EN)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \n\nstatic void pt_config_start(struct perf_event *event)\n{\n\tstruct pt *pt = this_cpu_ptr(&pt_ctx);\n\tu64 ctl = event->hw.config;\n\n\tctl |= RTIT_CTL_TRACEEN;\n\tif (READ_ONCE(pt->vmx_on))\n\t\tperf_aux_output_flag(&pt->handle, PERF_AUX_FLAG_PARTIAL);\n\telse\n\t\twrmsrl(MSR_IA32_RTIT_CTL, ctl);\n\n\tWRITE_ONCE(event->hw.config, ctl);\n}\n\n \nstatic const struct pt_address_range {\n\tunsigned long\tmsr_a;\n\tunsigned long\tmsr_b;\n\tunsigned int\treg_off;\n} pt_address_ranges[] = {\n\t{\n\t\t.msr_a\t = MSR_IA32_RTIT_ADDR0_A,\n\t\t.msr_b\t = MSR_IA32_RTIT_ADDR0_B,\n\t\t.reg_off = RTIT_CTL_ADDR0_OFFSET,\n\t},\n\t{\n\t\t.msr_a\t = MSR_IA32_RTIT_ADDR1_A,\n\t\t.msr_b\t = MSR_IA32_RTIT_ADDR1_B,\n\t\t.reg_off = RTIT_CTL_ADDR1_OFFSET,\n\t},\n\t{\n\t\t.msr_a\t = MSR_IA32_RTIT_ADDR2_A,\n\t\t.msr_b\t = MSR_IA32_RTIT_ADDR2_B,\n\t\t.reg_off = RTIT_CTL_ADDR2_OFFSET,\n\t},\n\t{\n\t\t.msr_a\t = MSR_IA32_RTIT_ADDR3_A,\n\t\t.msr_b\t = MSR_IA32_RTIT_ADDR3_B,\n\t\t.reg_off = RTIT_CTL_ADDR3_OFFSET,\n\t}\n};\n\nstatic u64 pt_config_filters(struct perf_event *event)\n{\n\tstruct pt_filters *filters = event->hw.addr_filters;\n\tstruct pt *pt = this_cpu_ptr(&pt_ctx);\n\tunsigned int range = 0;\n\tu64 rtit_ctl = 0;\n\n\tif (!filters)\n\t\treturn 0;\n\n\tperf_event_addr_filters_sync(event);\n\n\tfor (range = 0; range < filters->nr_filters; range++) {\n\t\tstruct pt_filter *filter = &filters->filter[range];\n\n\t\t \n\n\t\t \n\t\tif (pt->filters.filter[range].msr_a != filter->msr_a) {\n\t\t\twrmsrl(pt_address_ranges[range].msr_a, filter->msr_a);\n\t\t\tpt->filters.filter[range].msr_a = filter->msr_a;\n\t\t}\n\n\t\tif (pt->filters.filter[range].msr_b != filter->msr_b) {\n\t\t\twrmsrl(pt_address_ranges[range].msr_b, filter->msr_b);\n\t\t\tpt->filters.filter[range].msr_b = filter->msr_b;\n\t\t}\n\n\t\trtit_ctl |= (u64)filter->config << pt_address_ranges[range].reg_off;\n\t}\n\n\treturn rtit_ctl;\n}\n\nstatic void pt_config(struct perf_event *event)\n{\n\tstruct pt *pt = this_cpu_ptr(&pt_ctx);\n\tstruct pt_buffer *buf = perf_get_aux(&pt->handle);\n\tu64 reg;\n\n\t \n\tif (!event->hw.config) {\n\t\tperf_event_itrace_started(event);\n\t\twrmsrl(MSR_IA32_RTIT_STATUS, 0);\n\t}\n\n\treg = pt_config_filters(event);\n\treg |= RTIT_CTL_TRACEEN;\n\tif (!buf->single)\n\t\treg |= RTIT_CTL_TOPA;\n\n\t \n\tif (event->attr.config & BIT(0)) {\n\t\treg |= event->attr.config & RTIT_CTL_BRANCH_EN;\n\t} else {\n\t\treg |= RTIT_CTL_BRANCH_EN;\n\t}\n\n\tif (!event->attr.exclude_kernel)\n\t\treg |= RTIT_CTL_OS;\n\tif (!event->attr.exclude_user)\n\t\treg |= RTIT_CTL_USR;\n\n\treg |= (event->attr.config & PT_CONFIG_MASK);\n\n\tevent->hw.config = reg;\n\tpt_config_start(event);\n}\n\nstatic void pt_config_stop(struct perf_event *event)\n{\n\tstruct pt *pt = this_cpu_ptr(&pt_ctx);\n\tu64 ctl = READ_ONCE(event->hw.config);\n\n\t \n\tif (!(ctl & RTIT_CTL_TRACEEN))\n\t\treturn;\n\n\tctl &= ~RTIT_CTL_TRACEEN;\n\tif (!READ_ONCE(pt->vmx_on))\n\t\twrmsrl(MSR_IA32_RTIT_CTL, ctl);\n\n\tWRITE_ONCE(event->hw.config, ctl);\n\n\t \n\twmb();\n}\n\n \nstruct topa {\n\tstruct list_head\tlist;\n\tu64\t\t\toffset;\n\tsize_t\t\t\tsize;\n\tint\t\t\tlast;\n\tunsigned int\t\tz_count;\n};\n\n \n\n#define TENTS_PER_PAGE\t\\\n\t((PAGE_SIZE - sizeof(struct topa)) / sizeof(struct topa_entry))\n\n \nstruct topa_page {\n\tstruct topa_entry\ttable[TENTS_PER_PAGE];\n\tstruct topa\t\ttopa;\n};\n\nstatic inline struct topa_page *topa_to_page(struct topa *topa)\n{\n\treturn container_of(topa, struct topa_page, topa);\n}\n\nstatic inline struct topa_page *topa_entry_to_page(struct topa_entry *te)\n{\n\treturn (struct topa_page *)((unsigned long)te & PAGE_MASK);\n}\n\nstatic inline phys_addr_t topa_pfn(struct topa *topa)\n{\n\treturn PFN_DOWN(virt_to_phys(topa_to_page(topa)));\n}\n\n \n#define TOPA_ENTRY(t, i)\t\t\t\t\\\n\t((i) == -1\t\t\t\t\t\\\n\t\t? &topa_to_page(t)->table[(t)->last]\t\\\n\t\t: &topa_to_page(t)->table[(i)])\n#define TOPA_ENTRY_SIZE(t, i) (sizes(TOPA_ENTRY((t), (i))->size))\n#define TOPA_ENTRY_PAGES(t, i) (1 << TOPA_ENTRY((t), (i))->size)\n\nstatic void pt_config_buffer(struct pt_buffer *buf)\n{\n\tstruct pt *pt = this_cpu_ptr(&pt_ctx);\n\tu64 reg, mask;\n\tvoid *base;\n\n\tif (buf->single) {\n\t\tbase = buf->data_pages[0];\n\t\tmask = (buf->nr_pages * PAGE_SIZE - 1) >> 7;\n\t} else {\n\t\tbase = topa_to_page(buf->cur)->table;\n\t\tmask = (u64)buf->cur_idx;\n\t}\n\n\treg = virt_to_phys(base);\n\tif (pt->output_base != reg) {\n\t\tpt->output_base = reg;\n\t\twrmsrl(MSR_IA32_RTIT_OUTPUT_BASE, reg);\n\t}\n\n\treg = 0x7f | (mask << 7) | ((u64)buf->output_off << 32);\n\tif (pt->output_mask != reg) {\n\t\tpt->output_mask = reg;\n\t\twrmsrl(MSR_IA32_RTIT_OUTPUT_MASK, reg);\n\t}\n}\n\n \nstatic struct topa *topa_alloc(int cpu, gfp_t gfp)\n{\n\tint node = cpu_to_node(cpu);\n\tstruct topa_page *tp;\n\tstruct page *p;\n\n\tp = alloc_pages_node(node, gfp | __GFP_ZERO, 0);\n\tif (!p)\n\t\treturn NULL;\n\n\ttp = page_address(p);\n\ttp->topa.last = 0;\n\n\t \n\tif (!intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries)) {\n\t\tTOPA_ENTRY(&tp->topa, 1)->base = page_to_phys(p) >> TOPA_SHIFT;\n\t\tTOPA_ENTRY(&tp->topa, 1)->end = 1;\n\t}\n\n\treturn &tp->topa;\n}\n\n \nstatic void topa_free(struct topa *topa)\n{\n\tfree_page((unsigned long)topa);\n}\n\n \nstatic void topa_insert_table(struct pt_buffer *buf, struct topa *topa)\n{\n\tstruct topa *last = buf->last;\n\n\tlist_add_tail(&topa->list, &buf->tables);\n\n\tif (!buf->first) {\n\t\tbuf->first = buf->last = buf->cur = topa;\n\t\treturn;\n\t}\n\n\ttopa->offset = last->offset + last->size;\n\tbuf->last = topa;\n\n\tif (!intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries))\n\t\treturn;\n\n\tBUG_ON(last->last != TENTS_PER_PAGE - 1);\n\n\tTOPA_ENTRY(last, -1)->base = topa_pfn(topa);\n\tTOPA_ENTRY(last, -1)->end = 1;\n}\n\n \nstatic bool topa_table_full(struct topa *topa)\n{\n\t \n\tif (!intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries))\n\t\treturn !!topa->last;\n\n\treturn topa->last == TENTS_PER_PAGE - 1;\n}\n\n \nstatic int topa_insert_pages(struct pt_buffer *buf, int cpu, gfp_t gfp)\n{\n\tstruct topa *topa = buf->last;\n\tint order = 0;\n\tstruct page *p;\n\n\tp = virt_to_page(buf->data_pages[buf->nr_pages]);\n\tif (PagePrivate(p))\n\t\torder = page_private(p);\n\n\tif (topa_table_full(topa)) {\n\t\ttopa = topa_alloc(cpu, gfp);\n\t\tif (!topa)\n\t\t\treturn -ENOMEM;\n\n\t\ttopa_insert_table(buf, topa);\n\t}\n\n\tif (topa->z_count == topa->last - 1) {\n\t\tif (order == TOPA_ENTRY(topa, topa->last - 1)->size)\n\t\t\ttopa->z_count++;\n\t}\n\n\tTOPA_ENTRY(topa, -1)->base = page_to_phys(p) >> TOPA_SHIFT;\n\tTOPA_ENTRY(topa, -1)->size = order;\n\tif (!buf->snapshot &&\n\t    !intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries)) {\n\t\tTOPA_ENTRY(topa, -1)->intr = 1;\n\t\tTOPA_ENTRY(topa, -1)->stop = 1;\n\t}\n\n\ttopa->last++;\n\ttopa->size += sizes(order);\n\n\tbuf->nr_pages += 1ul << order;\n\n\treturn 0;\n}\n\n \nstatic void pt_topa_dump(struct pt_buffer *buf)\n{\n\tstruct topa *topa;\n\n\tlist_for_each_entry(topa, &buf->tables, list) {\n\t\tstruct topa_page *tp = topa_to_page(topa);\n\t\tint i;\n\n\t\tpr_debug(\"# table @%p, off %llx size %zx\\n\", tp->table,\n\t\t\t topa->offset, topa->size);\n\t\tfor (i = 0; i < TENTS_PER_PAGE; i++) {\n\t\t\tpr_debug(\"# entry @%p (%lx sz %u %c%c%c) raw=%16llx\\n\",\n\t\t\t\t &tp->table[i],\n\t\t\t\t (unsigned long)tp->table[i].base << TOPA_SHIFT,\n\t\t\t\t sizes(tp->table[i].size),\n\t\t\t\t tp->table[i].end ?  'E' : ' ',\n\t\t\t\t tp->table[i].intr ? 'I' : ' ',\n\t\t\t\t tp->table[i].stop ? 'S' : ' ',\n\t\t\t\t *(u64 *)&tp->table[i]);\n\t\t\tif ((intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries) &&\n\t\t\t     tp->table[i].stop) ||\n\t\t\t    tp->table[i].end)\n\t\t\t\tbreak;\n\t\t\tif (!i && topa->z_count)\n\t\t\t\ti += topa->z_count;\n\t\t}\n\t}\n}\n\n \nstatic void pt_buffer_advance(struct pt_buffer *buf)\n{\n\tbuf->output_off = 0;\n\tbuf->cur_idx++;\n\n\tif (buf->cur_idx == buf->cur->last) {\n\t\tif (buf->cur == buf->last)\n\t\t\tbuf->cur = buf->first;\n\t\telse\n\t\t\tbuf->cur = list_entry(buf->cur->list.next, struct topa,\n\t\t\t\t\t      list);\n\t\tbuf->cur_idx = 0;\n\t}\n}\n\n \nstatic void pt_update_head(struct pt *pt)\n{\n\tstruct pt_buffer *buf = perf_get_aux(&pt->handle);\n\tu64 topa_idx, base, old;\n\n\tif (buf->single) {\n\t\tlocal_set(&buf->data_size, buf->output_off);\n\t\treturn;\n\t}\n\n\t \n\tbase = buf->cur->offset + buf->output_off;\n\n\t \n\tfor (topa_idx = 0; topa_idx < buf->cur_idx; topa_idx++)\n\t\tbase += TOPA_ENTRY_SIZE(buf->cur, topa_idx);\n\n\tif (buf->snapshot) {\n\t\tlocal_set(&buf->data_size, base);\n\t} else {\n\t\told = (local64_xchg(&buf->head, base) &\n\t\t       ((buf->nr_pages << PAGE_SHIFT) - 1));\n\t\tif (base < old)\n\t\t\tbase += buf->nr_pages << PAGE_SHIFT;\n\n\t\tlocal_add(base - old, &buf->data_size);\n\t}\n}\n\n \nstatic void *pt_buffer_region(struct pt_buffer *buf)\n{\n\treturn phys_to_virt(TOPA_ENTRY(buf->cur, buf->cur_idx)->base << TOPA_SHIFT);\n}\n\n \nstatic size_t pt_buffer_region_size(struct pt_buffer *buf)\n{\n\treturn TOPA_ENTRY_SIZE(buf->cur, buf->cur_idx);\n}\n\n \nstatic void pt_handle_status(struct pt *pt)\n{\n\tstruct pt_buffer *buf = perf_get_aux(&pt->handle);\n\tint advance = 0;\n\tu64 status;\n\n\trdmsrl(MSR_IA32_RTIT_STATUS, status);\n\n\tif (status & RTIT_STATUS_ERROR) {\n\t\tpr_err_ratelimited(\"ToPA ERROR encountered, trying to recover\\n\");\n\t\tpt_topa_dump(buf);\n\t\tstatus &= ~RTIT_STATUS_ERROR;\n\t}\n\n\tif (status & RTIT_STATUS_STOPPED) {\n\t\tstatus &= ~RTIT_STATUS_STOPPED;\n\n\t\t \n\t\tif (!buf->single &&\n\t\t    (!intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries) ||\n\t\t     buf->output_off == pt_buffer_region_size(buf))) {\n\t\t\tperf_aux_output_flag(&pt->handle,\n\t\t\t                     PERF_AUX_FLAG_TRUNCATED);\n\t\t\tadvance++;\n\t\t}\n\t}\n\n\t \n\tif (!intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries) &&\n\t    !buf->snapshot &&\n\t    pt_buffer_region_size(buf) - buf->output_off <= TOPA_PMI_MARGIN) {\n\t\tvoid *head = pt_buffer_region(buf);\n\n\t\t \n\t\tmemset(head + buf->output_off, 0,\n\t\t       pt_buffer_region_size(buf) -\n\t\t       buf->output_off);\n\t\tadvance++;\n\t}\n\n\tif (advance)\n\t\tpt_buffer_advance(buf);\n\n\twrmsrl(MSR_IA32_RTIT_STATUS, status);\n}\n\n \nstatic void pt_read_offset(struct pt_buffer *buf)\n{\n\tstruct pt *pt = this_cpu_ptr(&pt_ctx);\n\tstruct topa_page *tp;\n\n\tif (!buf->single) {\n\t\trdmsrl(MSR_IA32_RTIT_OUTPUT_BASE, pt->output_base);\n\t\ttp = phys_to_virt(pt->output_base);\n\t\tbuf->cur = &tp->topa;\n\t}\n\n\trdmsrl(MSR_IA32_RTIT_OUTPUT_MASK, pt->output_mask);\n\t \n\tbuf->output_off = pt->output_mask >> 32;\n\t \n\tif (!buf->single)\n\t\tbuf->cur_idx = (pt->output_mask & 0xffffff80) >> 7;\n}\n\nstatic struct topa_entry *\npt_topa_entry_for_page(struct pt_buffer *buf, unsigned int pg)\n{\n\tstruct topa_page *tp;\n\tstruct topa *topa;\n\tunsigned int idx, cur_pg = 0, z_pg = 0, start_idx = 0;\n\n\t \n\tif (WARN_ON_ONCE(pg >= buf->nr_pages))\n\t\treturn NULL;\n\n\t \n\tlist_for_each_entry(topa, &buf->tables, list) {\n\t\tif (topa->offset + topa->size > pg << PAGE_SHIFT)\n\t\t\tgoto found;\n\t}\n\n\t \n\tWARN_ON_ONCE(1);\n\n\treturn NULL;\n\nfound:\n\t \n\tif (WARN_ON_ONCE(topa->last == -1))\n\t\treturn NULL;\n\n\ttp = topa_to_page(topa);\n\tcur_pg = PFN_DOWN(topa->offset);\n\tif (topa->z_count) {\n\t\tz_pg = TOPA_ENTRY_PAGES(topa, 0) * (topa->z_count + 1);\n\t\tstart_idx = topa->z_count + 1;\n\t}\n\n\t \n\tif (pg >= cur_pg && pg < cur_pg + z_pg) {\n\t\tidx = (pg - cur_pg) / TOPA_ENTRY_PAGES(topa, 0);\n\t\treturn &tp->table[idx];\n\t}\n\n\t \n\tfor (idx = start_idx, cur_pg += z_pg; idx < topa->last; idx++) {\n\t\tif (cur_pg + TOPA_ENTRY_PAGES(topa, idx) > pg)\n\t\t\treturn &tp->table[idx];\n\n\t\tcur_pg += TOPA_ENTRY_PAGES(topa, idx);\n\t}\n\n\t \n\tWARN_ON_ONCE(1);\n\n\treturn NULL;\n}\n\nstatic struct topa_entry *\npt_topa_prev_entry(struct pt_buffer *buf, struct topa_entry *te)\n{\n\tunsigned long table = (unsigned long)te & ~(PAGE_SIZE - 1);\n\tstruct topa_page *tp;\n\tstruct topa *topa;\n\n\ttp = (struct topa_page *)table;\n\tif (tp->table != te)\n\t\treturn --te;\n\n\ttopa = &tp->topa;\n\tif (topa == buf->first)\n\t\ttopa = buf->last;\n\telse\n\t\ttopa = list_prev_entry(topa, list);\n\n\ttp = topa_to_page(topa);\n\n\treturn &tp->table[topa->last - 1];\n}\n\n \nstatic int pt_buffer_reset_markers(struct pt_buffer *buf,\n\t\t\t\t   struct perf_output_handle *handle)\n\n{\n\tunsigned long head = local64_read(&buf->head);\n\tunsigned long idx, npages, wakeup;\n\n\tif (buf->single)\n\t\treturn 0;\n\n\t \n\tif (buf->output_off + handle->size + 1 < pt_buffer_region_size(buf)) {\n\t\tperf_aux_output_flag(handle, PERF_AUX_FLAG_TRUNCATED);\n\t\treturn -EINVAL;\n\t}\n\n\n\t \n\tif (!intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries))\n\t\treturn 0;\n\n\t \n\tif (buf->stop_te) {\n\t\tbuf->stop_te->stop = 0;\n\t\tbuf->stop_te->intr = 0;\n\t}\n\n\tif (buf->intr_te)\n\t\tbuf->intr_te->intr = 0;\n\n\t \n\tnpages = handle->size >> PAGE_SHIFT;\n\n\t \n\tif (!offset_in_page(head + handle->size + 1))\n\t\tnpages++;\n\n\tidx = (head >> PAGE_SHIFT) + npages;\n\tidx &= buf->nr_pages - 1;\n\n\tif (idx != buf->stop_pos) {\n\t\tbuf->stop_pos = idx;\n\t\tbuf->stop_te = pt_topa_entry_for_page(buf, idx);\n\t\tbuf->stop_te = pt_topa_prev_entry(buf, buf->stop_te);\n\t}\n\n\twakeup = handle->wakeup >> PAGE_SHIFT;\n\n\t \n\tidx = (head >> PAGE_SHIFT) + npages - 1;\n\tif (idx > wakeup)\n\t\tidx = wakeup;\n\n\tidx &= buf->nr_pages - 1;\n\tif (idx != buf->intr_pos) {\n\t\tbuf->intr_pos = idx;\n\t\tbuf->intr_te = pt_topa_entry_for_page(buf, idx);\n\t\tbuf->intr_te = pt_topa_prev_entry(buf, buf->intr_te);\n\t}\n\n\tbuf->stop_te->stop = 1;\n\tbuf->stop_te->intr = 1;\n\tbuf->intr_te->intr = 1;\n\n\treturn 0;\n}\n\n \nstatic void pt_buffer_reset_offsets(struct pt_buffer *buf, unsigned long head)\n{\n\tstruct topa_page *cur_tp;\n\tstruct topa_entry *te;\n\tint pg;\n\n\tif (buf->snapshot)\n\t\thead &= (buf->nr_pages << PAGE_SHIFT) - 1;\n\n\tif (!buf->single) {\n\t\tpg = (head >> PAGE_SHIFT) & (buf->nr_pages - 1);\n\t\tte = pt_topa_entry_for_page(buf, pg);\n\n\t\tcur_tp = topa_entry_to_page(te);\n\t\tbuf->cur = &cur_tp->topa;\n\t\tbuf->cur_idx = te - TOPA_ENTRY(buf->cur, 0);\n\t\tbuf->output_off = head & (pt_buffer_region_size(buf) - 1);\n\t} else {\n\t\tbuf->output_off = head;\n\t}\n\n\tlocal64_set(&buf->head, head);\n\tlocal_set(&buf->data_size, 0);\n}\n\n \nstatic void pt_buffer_fini_topa(struct pt_buffer *buf)\n{\n\tstruct topa *topa, *iter;\n\n\tif (buf->single)\n\t\treturn;\n\n\tlist_for_each_entry_safe(topa, iter, &buf->tables, list) {\n\t\t \n\t\ttopa_free(topa);\n\t}\n}\n\n \nstatic int pt_buffer_init_topa(struct pt_buffer *buf, int cpu,\n\t\t\t       unsigned long nr_pages, gfp_t gfp)\n{\n\tstruct topa *topa;\n\tint err;\n\n\ttopa = topa_alloc(cpu, gfp);\n\tif (!topa)\n\t\treturn -ENOMEM;\n\n\ttopa_insert_table(buf, topa);\n\n\twhile (buf->nr_pages < nr_pages) {\n\t\terr = topa_insert_pages(buf, cpu, gfp);\n\t\tif (err) {\n\t\t\tpt_buffer_fini_topa(buf);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\t \n\tif (intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries)) {\n\t\tTOPA_ENTRY(buf->last, -1)->base = topa_pfn(buf->first);\n\t\tTOPA_ENTRY(buf->last, -1)->end = 1;\n\t}\n\n\tpt_topa_dump(buf);\n\treturn 0;\n}\n\nstatic int pt_buffer_try_single(struct pt_buffer *buf, int nr_pages)\n{\n\tstruct page *p = virt_to_page(buf->data_pages[0]);\n\tint ret = -ENOTSUPP, order = 0;\n\n\t \n\tif (!buf->snapshot)\n\t\tgoto out;\n\n\tif (!intel_pt_validate_hw_cap(PT_CAP_single_range_output))\n\t\tgoto out;\n\n\tif (PagePrivate(p))\n\t\torder = page_private(p);\n\n\tif (1 << order != nr_pages)\n\t\tgoto out;\n\n\t \n\tif (nr_pages > 1)\n\t\tgoto out;\n\n\tbuf->single = true;\n\tbuf->nr_pages = nr_pages;\n\tret = 0;\nout:\n\treturn ret;\n}\n\n \nstatic void *\npt_buffer_setup_aux(struct perf_event *event, void **pages,\n\t\t    int nr_pages, bool snapshot)\n{\n\tstruct pt_buffer *buf;\n\tint node, ret, cpu = event->cpu;\n\n\tif (!nr_pages)\n\t\treturn NULL;\n\n\t \n\tif (event->attr.aux_sample_size && !snapshot)\n\t\treturn NULL;\n\n\tif (cpu == -1)\n\t\tcpu = raw_smp_processor_id();\n\tnode = cpu_to_node(cpu);\n\n\tbuf = kzalloc_node(sizeof(struct pt_buffer), GFP_KERNEL, node);\n\tif (!buf)\n\t\treturn NULL;\n\n\tbuf->snapshot = snapshot;\n\tbuf->data_pages = pages;\n\tbuf->stop_pos = -1;\n\tbuf->intr_pos = -1;\n\n\tINIT_LIST_HEAD(&buf->tables);\n\n\tret = pt_buffer_try_single(buf, nr_pages);\n\tif (!ret)\n\t\treturn buf;\n\n\tret = pt_buffer_init_topa(buf, cpu, nr_pages, GFP_KERNEL);\n\tif (ret) {\n\t\tkfree(buf);\n\t\treturn NULL;\n\t}\n\n\treturn buf;\n}\n\n \nstatic void pt_buffer_free_aux(void *data)\n{\n\tstruct pt_buffer *buf = data;\n\n\tpt_buffer_fini_topa(buf);\n\tkfree(buf);\n}\n\nstatic int pt_addr_filters_init(struct perf_event *event)\n{\n\tstruct pt_filters *filters;\n\tint node = event->cpu == -1 ? -1 : cpu_to_node(event->cpu);\n\n\tif (!intel_pt_validate_hw_cap(PT_CAP_num_address_ranges))\n\t\treturn 0;\n\n\tfilters = kzalloc_node(sizeof(struct pt_filters), GFP_KERNEL, node);\n\tif (!filters)\n\t\treturn -ENOMEM;\n\n\tif (event->parent)\n\t\tmemcpy(filters, event->parent->hw.addr_filters,\n\t\t       sizeof(*filters));\n\n\tevent->hw.addr_filters = filters;\n\n\treturn 0;\n}\n\nstatic void pt_addr_filters_fini(struct perf_event *event)\n{\n\tkfree(event->hw.addr_filters);\n\tevent->hw.addr_filters = NULL;\n}\n\n#ifdef CONFIG_X86_64\n \nstatic u64 clamp_to_ge_canonical_addr(u64 vaddr, u8 vaddr_bits)\n{\n\treturn __is_canonical_address(vaddr, vaddr_bits) ?\n\t       vaddr :\n\t       -BIT_ULL(vaddr_bits - 1);\n}\n\n \nstatic u64 clamp_to_le_canonical_addr(u64 vaddr, u8 vaddr_bits)\n{\n\treturn __is_canonical_address(vaddr, vaddr_bits) ?\n\t       vaddr :\n\t       BIT_ULL(vaddr_bits - 1) - 1;\n}\n#else\n#define clamp_to_ge_canonical_addr(x, y) (x)\n#define clamp_to_le_canonical_addr(x, y) (x)\n#endif\n\nstatic int pt_event_addr_filters_validate(struct list_head *filters)\n{\n\tstruct perf_addr_filter *filter;\n\tint range = 0;\n\n\tlist_for_each_entry(filter, filters, entry) {\n\t\t \n\t\tif (!filter->size ||\n\t\t    filter->action == PERF_ADDR_FILTER_ACTION_START)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (++range > intel_pt_validate_hw_cap(PT_CAP_num_address_ranges))\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\n\treturn 0;\n}\n\nstatic void pt_event_addr_filters_sync(struct perf_event *event)\n{\n\tstruct perf_addr_filters_head *head = perf_event_addr_filters(event);\n\tunsigned long msr_a, msr_b;\n\tstruct perf_addr_filter_range *fr = event->addr_filter_ranges;\n\tstruct pt_filters *filters = event->hw.addr_filters;\n\tstruct perf_addr_filter *filter;\n\tint range = 0;\n\n\tif (!filters)\n\t\treturn;\n\n\tlist_for_each_entry(filter, &head->list, entry) {\n\t\tif (filter->path.dentry && !fr[range].start) {\n\t\t\tmsr_a = msr_b = 0;\n\t\t} else {\n\t\t\tunsigned long n = fr[range].size - 1;\n\t\t\tunsigned long a = fr[range].start;\n\t\t\tunsigned long b;\n\n\t\t\tif (a > ULONG_MAX - n)\n\t\t\t\tb = ULONG_MAX;\n\t\t\telse\n\t\t\t\tb = a + n;\n\t\t\t \n\t\t\tmsr_a = clamp_to_ge_canonical_addr(a, boot_cpu_data.x86_virt_bits);\n\t\t\tmsr_b = clamp_to_le_canonical_addr(b, boot_cpu_data.x86_virt_bits);\n\t\t\tif (msr_b < msr_a)\n\t\t\t\tmsr_a = msr_b = 0;\n\t\t}\n\n\t\tfilters->filter[range].msr_a  = msr_a;\n\t\tfilters->filter[range].msr_b  = msr_b;\n\t\tif (filter->action == PERF_ADDR_FILTER_ACTION_FILTER)\n\t\t\tfilters->filter[range].config = 1;\n\t\telse\n\t\t\tfilters->filter[range].config = 2;\n\t\trange++;\n\t}\n\n\tfilters->nr_filters = range;\n}\n\n \nvoid intel_pt_interrupt(void)\n{\n\tstruct pt *pt = this_cpu_ptr(&pt_ctx);\n\tstruct pt_buffer *buf;\n\tstruct perf_event *event = pt->handle.event;\n\n\t \n\tif (!READ_ONCE(pt->handle_nmi))\n\t\treturn;\n\n\tif (!event)\n\t\treturn;\n\n\tpt_config_stop(event);\n\n\tbuf = perf_get_aux(&pt->handle);\n\tif (!buf)\n\t\treturn;\n\n\tpt_read_offset(buf);\n\n\tpt_handle_status(pt);\n\n\tpt_update_head(pt);\n\n\tperf_aux_output_end(&pt->handle, local_xchg(&buf->data_size, 0));\n\n\tif (!event->hw.state) {\n\t\tint ret;\n\n\t\tbuf = perf_aux_output_begin(&pt->handle, event);\n\t\tif (!buf) {\n\t\t\tevent->hw.state = PERF_HES_STOPPED;\n\t\t\treturn;\n\t\t}\n\n\t\tpt_buffer_reset_offsets(buf, pt->handle.head);\n\t\t \n\t\tret = pt_buffer_reset_markers(buf, &pt->handle);\n\t\tif (ret) {\n\t\t\tperf_aux_output_end(&pt->handle, 0);\n\t\t\treturn;\n\t\t}\n\n\t\tpt_config_buffer(buf);\n\t\tpt_config_start(event);\n\t}\n}\n\nvoid intel_pt_handle_vmx(int on)\n{\n\tstruct pt *pt = this_cpu_ptr(&pt_ctx);\n\tstruct perf_event *event;\n\tunsigned long flags;\n\n\t \n\tif (pt_pmu.vmx)\n\t\treturn;\n\n\t \n\tlocal_irq_save(flags);\n\tWRITE_ONCE(pt->vmx_on, on);\n\n\t \n\tevent = pt->handle.event;\n\tif (event)\n\t\tperf_aux_output_flag(&pt->handle,\n\t\t                     PERF_AUX_FLAG_PARTIAL);\n\n\t \n\tif (!on && event)\n\t\twrmsrl(MSR_IA32_RTIT_CTL, event->hw.config);\n\n\tlocal_irq_restore(flags);\n}\nEXPORT_SYMBOL_GPL(intel_pt_handle_vmx);\n\n \n\nstatic void pt_event_start(struct perf_event *event, int mode)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct pt *pt = this_cpu_ptr(&pt_ctx);\n\tstruct pt_buffer *buf;\n\n\tbuf = perf_aux_output_begin(&pt->handle, event);\n\tif (!buf)\n\t\tgoto fail_stop;\n\n\tpt_buffer_reset_offsets(buf, pt->handle.head);\n\tif (!buf->snapshot) {\n\t\tif (pt_buffer_reset_markers(buf, &pt->handle))\n\t\t\tgoto fail_end_stop;\n\t}\n\n\tWRITE_ONCE(pt->handle_nmi, 1);\n\thwc->state = 0;\n\n\tpt_config_buffer(buf);\n\tpt_config(event);\n\n\treturn;\n\nfail_end_stop:\n\tperf_aux_output_end(&pt->handle, 0);\nfail_stop:\n\thwc->state = PERF_HES_STOPPED;\n}\n\nstatic void pt_event_stop(struct perf_event *event, int mode)\n{\n\tstruct pt *pt = this_cpu_ptr(&pt_ctx);\n\n\t \n\tWRITE_ONCE(pt->handle_nmi, 0);\n\n\tpt_config_stop(event);\n\n\tif (event->hw.state == PERF_HES_STOPPED)\n\t\treturn;\n\n\tevent->hw.state = PERF_HES_STOPPED;\n\n\tif (mode & PERF_EF_UPDATE) {\n\t\tstruct pt_buffer *buf = perf_get_aux(&pt->handle);\n\n\t\tif (!buf)\n\t\t\treturn;\n\n\t\tif (WARN_ON_ONCE(pt->handle.event != event))\n\t\t\treturn;\n\n\t\tpt_read_offset(buf);\n\n\t\tpt_handle_status(pt);\n\n\t\tpt_update_head(pt);\n\n\t\tif (buf->snapshot)\n\t\t\tpt->handle.head =\n\t\t\t\tlocal_xchg(&buf->data_size,\n\t\t\t\t\t   buf->nr_pages << PAGE_SHIFT);\n\t\tperf_aux_output_end(&pt->handle, local_xchg(&buf->data_size, 0));\n\t}\n}\n\nstatic long pt_event_snapshot_aux(struct perf_event *event,\n\t\t\t\t  struct perf_output_handle *handle,\n\t\t\t\t  unsigned long size)\n{\n\tstruct pt *pt = this_cpu_ptr(&pt_ctx);\n\tstruct pt_buffer *buf = perf_get_aux(&pt->handle);\n\tunsigned long from = 0, to;\n\tlong ret;\n\n\tif (WARN_ON_ONCE(!buf))\n\t\treturn 0;\n\n\t \n\tif (WARN_ON_ONCE(!buf->snapshot))\n\t\treturn 0;\n\n\t \n\tif (READ_ONCE(pt->handle_nmi))\n\t\tpt_config_stop(event);\n\n\tpt_read_offset(buf);\n\tpt_update_head(pt);\n\n\tto = local_read(&buf->data_size);\n\tif (to < size)\n\t\tfrom = buf->nr_pages << PAGE_SHIFT;\n\tfrom += to - size;\n\n\tret = perf_output_copy_aux(&pt->handle, handle, from, to);\n\n\t \n\tif (pt->handle_nmi)\n\t\tpt_config_start(event);\n\n\treturn ret;\n}\n\nstatic void pt_event_del(struct perf_event *event, int mode)\n{\n\tpt_event_stop(event, PERF_EF_UPDATE);\n}\n\nstatic int pt_event_add(struct perf_event *event, int mode)\n{\n\tstruct pt *pt = this_cpu_ptr(&pt_ctx);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint ret = -EBUSY;\n\n\tif (pt->handle.event)\n\t\tgoto fail;\n\n\tif (mode & PERF_EF_START) {\n\t\tpt_event_start(event, 0);\n\t\tret = -EINVAL;\n\t\tif (hwc->state == PERF_HES_STOPPED)\n\t\t\tgoto fail;\n\t} else {\n\t\thwc->state = PERF_HES_STOPPED;\n\t}\n\n\tret = 0;\nfail:\n\n\treturn ret;\n}\n\nstatic void pt_event_read(struct perf_event *event)\n{\n}\n\nstatic void pt_event_destroy(struct perf_event *event)\n{\n\tpt_addr_filters_fini(event);\n\tx86_del_exclusive(x86_lbr_exclusive_pt);\n}\n\nstatic int pt_event_init(struct perf_event *event)\n{\n\tif (event->attr.type != pt_pmu.pmu.type)\n\t\treturn -ENOENT;\n\n\tif (!pt_event_valid(event))\n\t\treturn -EINVAL;\n\n\tif (x86_add_exclusive(x86_lbr_exclusive_pt))\n\t\treturn -EBUSY;\n\n\tif (pt_addr_filters_init(event)) {\n\t\tx86_del_exclusive(x86_lbr_exclusive_pt);\n\t\treturn -ENOMEM;\n\t}\n\n\tevent->destroy = pt_event_destroy;\n\n\treturn 0;\n}\n\nvoid cpu_emergency_stop_pt(void)\n{\n\tstruct pt *pt = this_cpu_ptr(&pt_ctx);\n\n\tif (pt->handle.event)\n\t\tpt_event_stop(pt->handle.event, PERF_EF_UPDATE);\n}\n\nint is_intel_pt_event(struct perf_event *event)\n{\n\treturn event->pmu == &pt_pmu.pmu;\n}\n\nstatic __init int pt_init(void)\n{\n\tint ret, cpu, prior_warn = 0;\n\n\tBUILD_BUG_ON(sizeof(struct topa) > PAGE_SIZE);\n\n\tif (!boot_cpu_has(X86_FEATURE_INTEL_PT))\n\t\treturn -ENODEV;\n\n\tcpus_read_lock();\n\tfor_each_online_cpu(cpu) {\n\t\tu64 ctl;\n\n\t\tret = rdmsrl_safe_on_cpu(cpu, MSR_IA32_RTIT_CTL, &ctl);\n\t\tif (!ret && (ctl & RTIT_CTL_TRACEEN))\n\t\t\tprior_warn++;\n\t}\n\tcpus_read_unlock();\n\n\tif (prior_warn) {\n\t\tx86_add_exclusive(x86_lbr_exclusive_pt);\n\t\tpr_warn(\"PT is enabled at boot time, doing nothing\\n\");\n\n\t\treturn -EBUSY;\n\t}\n\n\tret = pt_pmu_hw_init();\n\tif (ret)\n\t\treturn ret;\n\n\tif (!intel_pt_validate_hw_cap(PT_CAP_topa_output)) {\n\t\tpr_warn(\"ToPA output is not supported on this CPU\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (!intel_pt_validate_hw_cap(PT_CAP_topa_multiple_entries))\n\t\tpt_pmu.pmu.capabilities = PERF_PMU_CAP_AUX_NO_SG;\n\n\tpt_pmu.pmu.capabilities\t|= PERF_PMU_CAP_EXCLUSIVE | PERF_PMU_CAP_ITRACE;\n\tpt_pmu.pmu.attr_groups\t\t = pt_attr_groups;\n\tpt_pmu.pmu.task_ctx_nr\t\t = perf_sw_context;\n\tpt_pmu.pmu.event_init\t\t = pt_event_init;\n\tpt_pmu.pmu.add\t\t\t = pt_event_add;\n\tpt_pmu.pmu.del\t\t\t = pt_event_del;\n\tpt_pmu.pmu.start\t\t = pt_event_start;\n\tpt_pmu.pmu.stop\t\t\t = pt_event_stop;\n\tpt_pmu.pmu.snapshot_aux\t\t = pt_event_snapshot_aux;\n\tpt_pmu.pmu.read\t\t\t = pt_event_read;\n\tpt_pmu.pmu.setup_aux\t\t = pt_buffer_setup_aux;\n\tpt_pmu.pmu.free_aux\t\t = pt_buffer_free_aux;\n\tpt_pmu.pmu.addr_filters_sync     = pt_event_addr_filters_sync;\n\tpt_pmu.pmu.addr_filters_validate = pt_event_addr_filters_validate;\n\tpt_pmu.pmu.nr_addr_filters       =\n\t\tintel_pt_validate_hw_cap(PT_CAP_num_address_ranges);\n\n\tret = perf_pmu_register(&pt_pmu.pmu, \"intel_pt\", -1);\n\n\treturn ret;\n}\narch_initcall(pt_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}