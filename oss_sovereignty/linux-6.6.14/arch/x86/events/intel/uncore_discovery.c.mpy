{
  "module_name": "uncore_discovery.c",
  "hash_id": "877eed327d7363a772f7c3a387b384a8b94863554356b0ceaa64ac6f29dfb9c5",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/events/intel/uncore_discovery.c",
  "human_readable_source": " \n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include \"uncore.h\"\n#include \"uncore_discovery.h\"\n\nstatic struct rb_root discovery_tables = RB_ROOT;\nstatic int num_discovered_types[UNCORE_ACCESS_MAX];\n\nstatic bool has_generic_discovery_table(void)\n{\n\tstruct pci_dev *dev;\n\tint dvsec;\n\n\tdev = pci_get_device(PCI_VENDOR_ID_INTEL, UNCORE_DISCOVERY_TABLE_DEVICE, NULL);\n\tif (!dev)\n\t\treturn false;\n\n\t \n\tdvsec = pci_find_next_ext_capability(dev, 0, UNCORE_EXT_CAP_ID_DISCOVERY);\n\tpci_dev_put(dev);\n\tif (dvsec)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int logical_die_id;\n\nstatic int get_device_die_id(struct pci_dev *dev)\n{\n\tint node = pcibus_to_node(dev->bus);\n\n\t \n\tif (node < 0)\n\t\treturn logical_die_id++;\n\n\treturn uncore_device_to_die(dev);\n}\n\n#define __node_2_type(cur)\t\\\n\trb_entry((cur), struct intel_uncore_discovery_type, node)\n\nstatic inline int __type_cmp(const void *key, const struct rb_node *b)\n{\n\tstruct intel_uncore_discovery_type *type_b = __node_2_type(b);\n\tconst u16 *type_id = key;\n\n\tif (type_b->type > *type_id)\n\t\treturn -1;\n\telse if (type_b->type < *type_id)\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic inline struct intel_uncore_discovery_type *\nsearch_uncore_discovery_type(u16 type_id)\n{\n\tstruct rb_node *node = rb_find(&type_id, &discovery_tables, __type_cmp);\n\n\treturn (node) ? __node_2_type(node) : NULL;\n}\n\nstatic inline bool __type_less(struct rb_node *a, const struct rb_node *b)\n{\n\treturn (__node_2_type(a)->type < __node_2_type(b)->type);\n}\n\nstatic struct intel_uncore_discovery_type *\nadd_uncore_discovery_type(struct uncore_unit_discovery *unit)\n{\n\tstruct intel_uncore_discovery_type *type;\n\n\tif (unit->access_type >= UNCORE_ACCESS_MAX) {\n\t\tpr_warn(\"Unsupported access type %d\\n\", unit->access_type);\n\t\treturn NULL;\n\t}\n\n\ttype = kzalloc(sizeof(struct intel_uncore_discovery_type), GFP_KERNEL);\n\tif (!type)\n\t\treturn NULL;\n\n\ttype->box_ctrl_die = kcalloc(__uncore_max_dies, sizeof(u64), GFP_KERNEL);\n\tif (!type->box_ctrl_die)\n\t\tgoto free_type;\n\n\ttype->access_type = unit->access_type;\n\tnum_discovered_types[type->access_type]++;\n\ttype->type = unit->box_type;\n\n\trb_add(&type->node, &discovery_tables, __type_less);\n\n\treturn type;\n\nfree_type:\n\tkfree(type);\n\n\treturn NULL;\n\n}\n\nstatic struct intel_uncore_discovery_type *\nget_uncore_discovery_type(struct uncore_unit_discovery *unit)\n{\n\tstruct intel_uncore_discovery_type *type;\n\n\ttype = search_uncore_discovery_type(unit->box_type);\n\tif (type)\n\t\treturn type;\n\n\treturn add_uncore_discovery_type(unit);\n}\n\nstatic void\nuncore_insert_box_info(struct uncore_unit_discovery *unit,\n\t\t       int die, bool parsed)\n{\n\tstruct intel_uncore_discovery_type *type;\n\tunsigned int *box_offset, *ids;\n\tint i;\n\n\tif (!unit->ctl || !unit->ctl_offset || !unit->ctr_offset) {\n\t\tpr_info(\"Invalid address is detected for uncore type %d box %d, \"\n\t\t\t\"Disable the uncore unit.\\n\",\n\t\t\tunit->box_type, unit->box_id);\n\t\treturn;\n\t}\n\n\tif (parsed) {\n\t\ttype = search_uncore_discovery_type(unit->box_type);\n\t\tif (!type) {\n\t\t\tpr_info(\"A spurious uncore type %d is detected, \"\n\t\t\t\t\"Disable the uncore type.\\n\",\n\t\t\t\tunit->box_type);\n\t\t\treturn;\n\t\t}\n\t\t \n\t\tif (!type->box_ctrl_die[die])\n\t\t\ttype->box_ctrl_die[die] = unit->ctl;\n\t\treturn;\n\t}\n\n\ttype = get_uncore_discovery_type(unit);\n\tif (!type)\n\t\treturn;\n\n\tbox_offset = kcalloc(type->num_boxes + 1, sizeof(unsigned int), GFP_KERNEL);\n\tif (!box_offset)\n\t\treturn;\n\n\tids = kcalloc(type->num_boxes + 1, sizeof(unsigned int), GFP_KERNEL);\n\tif (!ids)\n\t\tgoto free_box_offset;\n\n\t \n\tif (!type->num_boxes) {\n\t\ttype->box_ctrl = unit->ctl;\n\t\ttype->box_ctrl_die[die] = unit->ctl;\n\t\ttype->num_counters = unit->num_regs;\n\t\ttype->counter_width = unit->bit_width;\n\t\ttype->ctl_offset = unit->ctl_offset;\n\t\ttype->ctr_offset = unit->ctr_offset;\n\t\t*ids = unit->box_id;\n\t\tgoto end;\n\t}\n\n\tfor (i = 0; i < type->num_boxes; i++) {\n\t\tids[i] = type->ids[i];\n\t\tbox_offset[i] = type->box_offset[i];\n\n\t\tif (unit->box_id == ids[i]) {\n\t\t\tpr_info(\"Duplicate uncore type %d box ID %d is detected, \"\n\t\t\t\t\"Drop the duplicate uncore unit.\\n\",\n\t\t\t\tunit->box_type, unit->box_id);\n\t\t\tgoto free_ids;\n\t\t}\n\t}\n\tids[i] = unit->box_id;\n\tbox_offset[i] = unit->ctl - type->box_ctrl;\n\tkfree(type->ids);\n\tkfree(type->box_offset);\nend:\n\ttype->ids = ids;\n\ttype->box_offset = box_offset;\n\ttype->num_boxes++;\n\treturn;\n\nfree_ids:\n\tkfree(ids);\n\nfree_box_offset:\n\tkfree(box_offset);\n\n}\n\nstatic bool\nuncore_ignore_unit(struct uncore_unit_discovery *unit, int *ignore)\n{\n\tint i;\n\n\tif (!ignore)\n\t\treturn false;\n\n\tfor (i = 0; ignore[i] != UNCORE_IGNORE_END ; i++) {\n\t\tif (unit->box_type == ignore[i])\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic int parse_discovery_table(struct pci_dev *dev, int die,\n\t\t\t\t u32 bar_offset, bool *parsed,\n\t\t\t\t int *ignore)\n{\n\tstruct uncore_global_discovery global;\n\tstruct uncore_unit_discovery unit;\n\tvoid __iomem *io_addr;\n\tresource_size_t addr;\n\tunsigned long size;\n\tu32 val;\n\tint i;\n\n\tpci_read_config_dword(dev, bar_offset, &val);\n\n\tif (val & ~PCI_BASE_ADDRESS_MEM_MASK & ~PCI_BASE_ADDRESS_MEM_TYPE_64)\n\t\treturn -EINVAL;\n\n\taddr = (resource_size_t)(val & PCI_BASE_ADDRESS_MEM_MASK);\n#ifdef CONFIG_PHYS_ADDR_T_64BIT\n\tif ((val & PCI_BASE_ADDRESS_MEM_TYPE_MASK) == PCI_BASE_ADDRESS_MEM_TYPE_64) {\n\t\tu32 val2;\n\n\t\tpci_read_config_dword(dev, bar_offset + 4, &val2);\n\t\taddr |= ((resource_size_t)val2) << 32;\n\t}\n#endif\n\tsize = UNCORE_DISCOVERY_GLOBAL_MAP_SIZE;\n\tio_addr = ioremap(addr, size);\n\tif (!io_addr)\n\t\treturn -ENOMEM;\n\n\t \n\tmemcpy_fromio(&global, io_addr, sizeof(struct uncore_global_discovery));\n\tif (uncore_discovery_invalid_unit(global)) {\n\t\tpr_info(\"Invalid Global Discovery State: 0x%llx 0x%llx 0x%llx\\n\",\n\t\t\tglobal.table1, global.ctl, global.table3);\n\t\tiounmap(io_addr);\n\t\treturn -EINVAL;\n\t}\n\tiounmap(io_addr);\n\n\tsize = (1 + global.max_units) * global.stride * 8;\n\tio_addr = ioremap(addr, size);\n\tif (!io_addr)\n\t\treturn -ENOMEM;\n\n\t \n\tfor (i = 0; i < global.max_units; i++) {\n\t\tmemcpy_fromio(&unit, io_addr + (i + 1) * (global.stride * 8),\n\t\t\t      sizeof(struct uncore_unit_discovery));\n\n\t\tif (uncore_discovery_invalid_unit(unit))\n\t\t\tcontinue;\n\n\t\tif (unit.access_type >= UNCORE_ACCESS_MAX)\n\t\t\tcontinue;\n\n\t\tif (uncore_ignore_unit(&unit, ignore))\n\t\t\tcontinue;\n\n\t\tuncore_insert_box_info(&unit, die, *parsed);\n\t}\n\n\t*parsed = true;\n\tiounmap(io_addr);\n\treturn 0;\n}\n\nbool intel_uncore_has_discovery_tables(int *ignore)\n{\n\tu32 device, val, entry_id, bar_offset;\n\tint die, dvsec = 0, ret = true;\n\tstruct pci_dev *dev = NULL;\n\tbool parsed = false;\n\n\tif (has_generic_discovery_table())\n\t\tdevice = UNCORE_DISCOVERY_TABLE_DEVICE;\n\telse\n\t\tdevice = PCI_ANY_ID;\n\n\t \n\twhile ((dev = pci_get_device(PCI_VENDOR_ID_INTEL, device, dev)) != NULL) {\n\t\twhile ((dvsec = pci_find_next_ext_capability(dev, dvsec, UNCORE_EXT_CAP_ID_DISCOVERY))) {\n\t\t\tpci_read_config_dword(dev, dvsec + UNCORE_DISCOVERY_DVSEC_OFFSET, &val);\n\t\t\tentry_id = val & UNCORE_DISCOVERY_DVSEC_ID_MASK;\n\t\t\tif (entry_id != UNCORE_DISCOVERY_DVSEC_ID_PMON)\n\t\t\t\tcontinue;\n\n\t\t\tpci_read_config_dword(dev, dvsec + UNCORE_DISCOVERY_DVSEC2_OFFSET, &val);\n\n\t\t\tif (val & ~UNCORE_DISCOVERY_DVSEC2_BIR_MASK) {\n\t\t\t\tret = false;\n\t\t\t\tgoto err;\n\t\t\t}\n\t\t\tbar_offset = UNCORE_DISCOVERY_BIR_BASE +\n\t\t\t\t     (val & UNCORE_DISCOVERY_DVSEC2_BIR_MASK) * UNCORE_DISCOVERY_BIR_STEP;\n\n\t\t\tdie = get_device_die_id(dev);\n\t\t\tif (die < 0)\n\t\t\t\tcontinue;\n\n\t\t\tparse_discovery_table(dev, die, bar_offset, &parsed, ignore);\n\t\t}\n\t}\n\n\t \n\tif (!parsed)\n\t\tret = false;\nerr:\n\tpci_dev_put(dev);\n\n\treturn ret;\n}\n\nvoid intel_uncore_clear_discovery_tables(void)\n{\n\tstruct intel_uncore_discovery_type *type, *next;\n\n\trbtree_postorder_for_each_entry_safe(type, next, &discovery_tables, node) {\n\t\tkfree(type->box_ctrl_die);\n\t\tkfree(type);\n\t}\n}\n\nDEFINE_UNCORE_FORMAT_ATTR(event, event, \"config:0-7\");\nDEFINE_UNCORE_FORMAT_ATTR(umask, umask, \"config:8-15\");\nDEFINE_UNCORE_FORMAT_ATTR(edge, edge, \"config:18\");\nDEFINE_UNCORE_FORMAT_ATTR(inv, inv, \"config:23\");\nDEFINE_UNCORE_FORMAT_ATTR(thresh, thresh, \"config:24-31\");\n\nstatic struct attribute *generic_uncore_formats_attr[] = {\n\t&format_attr_event.attr,\n\t&format_attr_umask.attr,\n\t&format_attr_edge.attr,\n\t&format_attr_inv.attr,\n\t&format_attr_thresh.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group generic_uncore_format_group = {\n\t.name = \"format\",\n\t.attrs = generic_uncore_formats_attr,\n};\n\nvoid intel_generic_uncore_msr_init_box(struct intel_uncore_box *box)\n{\n\twrmsrl(uncore_msr_box_ctl(box), GENERIC_PMON_BOX_CTL_INT);\n}\n\nvoid intel_generic_uncore_msr_disable_box(struct intel_uncore_box *box)\n{\n\twrmsrl(uncore_msr_box_ctl(box), GENERIC_PMON_BOX_CTL_FRZ);\n}\n\nvoid intel_generic_uncore_msr_enable_box(struct intel_uncore_box *box)\n{\n\twrmsrl(uncore_msr_box_ctl(box), 0);\n}\n\nstatic void intel_generic_uncore_msr_enable_event(struct intel_uncore_box *box,\n\t\t\t\t\t    struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\twrmsrl(hwc->config_base, hwc->config);\n}\n\nstatic void intel_generic_uncore_msr_disable_event(struct intel_uncore_box *box,\n\t\t\t\t\t     struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\twrmsrl(hwc->config_base, 0);\n}\n\nstatic struct intel_uncore_ops generic_uncore_msr_ops = {\n\t.init_box\t\t= intel_generic_uncore_msr_init_box,\n\t.disable_box\t\t= intel_generic_uncore_msr_disable_box,\n\t.enable_box\t\t= intel_generic_uncore_msr_enable_box,\n\t.disable_event\t\t= intel_generic_uncore_msr_disable_event,\n\t.enable_event\t\t= intel_generic_uncore_msr_enable_event,\n\t.read_counter\t\t= uncore_msr_read_counter,\n};\n\nvoid intel_generic_uncore_pci_init_box(struct intel_uncore_box *box)\n{\n\tstruct pci_dev *pdev = box->pci_dev;\n\tint box_ctl = uncore_pci_box_ctl(box);\n\n\t__set_bit(UNCORE_BOX_FLAG_CTL_OFFS8, &box->flags);\n\tpci_write_config_dword(pdev, box_ctl, GENERIC_PMON_BOX_CTL_INT);\n}\n\nvoid intel_generic_uncore_pci_disable_box(struct intel_uncore_box *box)\n{\n\tstruct pci_dev *pdev = box->pci_dev;\n\tint box_ctl = uncore_pci_box_ctl(box);\n\n\tpci_write_config_dword(pdev, box_ctl, GENERIC_PMON_BOX_CTL_FRZ);\n}\n\nvoid intel_generic_uncore_pci_enable_box(struct intel_uncore_box *box)\n{\n\tstruct pci_dev *pdev = box->pci_dev;\n\tint box_ctl = uncore_pci_box_ctl(box);\n\n\tpci_write_config_dword(pdev, box_ctl, 0);\n}\n\nstatic void intel_generic_uncore_pci_enable_event(struct intel_uncore_box *box,\n\t\t\t\t\t    struct perf_event *event)\n{\n\tstruct pci_dev *pdev = box->pci_dev;\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tpci_write_config_dword(pdev, hwc->config_base, hwc->config);\n}\n\nvoid intel_generic_uncore_pci_disable_event(struct intel_uncore_box *box,\n\t\t\t\t\t    struct perf_event *event)\n{\n\tstruct pci_dev *pdev = box->pci_dev;\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tpci_write_config_dword(pdev, hwc->config_base, 0);\n}\n\nu64 intel_generic_uncore_pci_read_counter(struct intel_uncore_box *box,\n\t\t\t\t\t  struct perf_event *event)\n{\n\tstruct pci_dev *pdev = box->pci_dev;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 count = 0;\n\n\tpci_read_config_dword(pdev, hwc->event_base, (u32 *)&count);\n\tpci_read_config_dword(pdev, hwc->event_base + 4, (u32 *)&count + 1);\n\n\treturn count;\n}\n\nstatic struct intel_uncore_ops generic_uncore_pci_ops = {\n\t.init_box\t= intel_generic_uncore_pci_init_box,\n\t.disable_box\t= intel_generic_uncore_pci_disable_box,\n\t.enable_box\t= intel_generic_uncore_pci_enable_box,\n\t.disable_event\t= intel_generic_uncore_pci_disable_event,\n\t.enable_event\t= intel_generic_uncore_pci_enable_event,\n\t.read_counter\t= intel_generic_uncore_pci_read_counter,\n};\n\n#define UNCORE_GENERIC_MMIO_SIZE\t\t0x4000\n\nstatic u64 generic_uncore_mmio_box_ctl(struct intel_uncore_box *box)\n{\n\tstruct intel_uncore_type *type = box->pmu->type;\n\n\tif (!type->box_ctls || !type->box_ctls[box->dieid] || !type->mmio_offsets)\n\t\treturn 0;\n\n\treturn type->box_ctls[box->dieid] + type->mmio_offsets[box->pmu->pmu_idx];\n}\n\nvoid intel_generic_uncore_mmio_init_box(struct intel_uncore_box *box)\n{\n\tu64 box_ctl = generic_uncore_mmio_box_ctl(box);\n\tstruct intel_uncore_type *type = box->pmu->type;\n\tresource_size_t addr;\n\n\tif (!box_ctl) {\n\t\tpr_warn(\"Uncore type %d box %d: Invalid box control address.\\n\",\n\t\t\ttype->type_id, type->box_ids[box->pmu->pmu_idx]);\n\t\treturn;\n\t}\n\n\taddr = box_ctl;\n\tbox->io_addr = ioremap(addr, UNCORE_GENERIC_MMIO_SIZE);\n\tif (!box->io_addr) {\n\t\tpr_warn(\"Uncore type %d box %d: ioremap error for 0x%llx.\\n\",\n\t\t\ttype->type_id, type->box_ids[box->pmu->pmu_idx],\n\t\t\t(unsigned long long)addr);\n\t\treturn;\n\t}\n\n\twritel(GENERIC_PMON_BOX_CTL_INT, box->io_addr);\n}\n\nvoid intel_generic_uncore_mmio_disable_box(struct intel_uncore_box *box)\n{\n\tif (!box->io_addr)\n\t\treturn;\n\n\twritel(GENERIC_PMON_BOX_CTL_FRZ, box->io_addr);\n}\n\nvoid intel_generic_uncore_mmio_enable_box(struct intel_uncore_box *box)\n{\n\tif (!box->io_addr)\n\t\treturn;\n\n\twritel(0, box->io_addr);\n}\n\nvoid intel_generic_uncore_mmio_enable_event(struct intel_uncore_box *box,\n\t\t\t\t\t    struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (!box->io_addr)\n\t\treturn;\n\n\twritel(hwc->config, box->io_addr + hwc->config_base);\n}\n\nvoid intel_generic_uncore_mmio_disable_event(struct intel_uncore_box *box,\n\t\t\t\t\t     struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (!box->io_addr)\n\t\treturn;\n\n\twritel(0, box->io_addr + hwc->config_base);\n}\n\nstatic struct intel_uncore_ops generic_uncore_mmio_ops = {\n\t.init_box\t= intel_generic_uncore_mmio_init_box,\n\t.exit_box\t= uncore_mmio_exit_box,\n\t.disable_box\t= intel_generic_uncore_mmio_disable_box,\n\t.enable_box\t= intel_generic_uncore_mmio_enable_box,\n\t.disable_event\t= intel_generic_uncore_mmio_disable_event,\n\t.enable_event\t= intel_generic_uncore_mmio_enable_event,\n\t.read_counter\t= uncore_mmio_read_counter,\n};\n\nstatic bool uncore_update_uncore_type(enum uncore_access_type type_id,\n\t\t\t\t      struct intel_uncore_type *uncore,\n\t\t\t\t      struct intel_uncore_discovery_type *type)\n{\n\tuncore->type_id = type->type;\n\tuncore->num_boxes = type->num_boxes;\n\tuncore->num_counters = type->num_counters;\n\tuncore->perf_ctr_bits = type->counter_width;\n\tuncore->box_ids = type->ids;\n\n\tswitch (type_id) {\n\tcase UNCORE_ACCESS_MSR:\n\t\tuncore->ops = &generic_uncore_msr_ops;\n\t\tuncore->perf_ctr = (unsigned int)type->box_ctrl + type->ctr_offset;\n\t\tuncore->event_ctl = (unsigned int)type->box_ctrl + type->ctl_offset;\n\t\tuncore->box_ctl = (unsigned int)type->box_ctrl;\n\t\tuncore->msr_offsets = type->box_offset;\n\t\tbreak;\n\tcase UNCORE_ACCESS_PCI:\n\t\tuncore->ops = &generic_uncore_pci_ops;\n\t\tuncore->perf_ctr = (unsigned int)UNCORE_DISCOVERY_PCI_BOX_CTRL(type->box_ctrl) + type->ctr_offset;\n\t\tuncore->event_ctl = (unsigned int)UNCORE_DISCOVERY_PCI_BOX_CTRL(type->box_ctrl) + type->ctl_offset;\n\t\tuncore->box_ctl = (unsigned int)UNCORE_DISCOVERY_PCI_BOX_CTRL(type->box_ctrl);\n\t\tuncore->box_ctls = type->box_ctrl_die;\n\t\tuncore->pci_offsets = type->box_offset;\n\t\tbreak;\n\tcase UNCORE_ACCESS_MMIO:\n\t\tuncore->ops = &generic_uncore_mmio_ops;\n\t\tuncore->perf_ctr = (unsigned int)type->ctr_offset;\n\t\tuncore->event_ctl = (unsigned int)type->ctl_offset;\n\t\tuncore->box_ctl = (unsigned int)type->box_ctrl;\n\t\tuncore->box_ctls = type->box_ctrl_die;\n\t\tuncore->mmio_offsets = type->box_offset;\n\t\tuncore->mmio_map_size = UNCORE_GENERIC_MMIO_SIZE;\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstruct intel_uncore_type **\nintel_uncore_generic_init_uncores(enum uncore_access_type type_id, int num_extra)\n{\n\tstruct intel_uncore_discovery_type *type;\n\tstruct intel_uncore_type **uncores;\n\tstruct intel_uncore_type *uncore;\n\tstruct rb_node *node;\n\tint i = 0;\n\n\tuncores = kcalloc(num_discovered_types[type_id] + num_extra + 1,\n\t\t\t  sizeof(struct intel_uncore_type *), GFP_KERNEL);\n\tif (!uncores)\n\t\treturn empty_uncore;\n\n\tfor (node = rb_first(&discovery_tables); node; node = rb_next(node)) {\n\t\ttype = rb_entry(node, struct intel_uncore_discovery_type, node);\n\t\tif (type->access_type != type_id)\n\t\t\tcontinue;\n\n\t\tuncore = kzalloc(sizeof(struct intel_uncore_type), GFP_KERNEL);\n\t\tif (!uncore)\n\t\t\tbreak;\n\n\t\tuncore->event_mask = GENERIC_PMON_RAW_EVENT_MASK;\n\t\tuncore->format_group = &generic_uncore_format_group;\n\n\t\tif (!uncore_update_uncore_type(type_id, uncore, type)) {\n\t\t\tkfree(uncore);\n\t\t\tcontinue;\n\t\t}\n\t\tuncores[i++] = uncore;\n\t}\n\n\treturn uncores;\n}\n\nvoid intel_uncore_generic_uncore_cpu_init(void)\n{\n\tuncore_msr_uncores = intel_uncore_generic_init_uncores(UNCORE_ACCESS_MSR, 0);\n}\n\nint intel_uncore_generic_uncore_pci_init(void)\n{\n\tuncore_pci_uncores = intel_uncore_generic_init_uncores(UNCORE_ACCESS_PCI, 0);\n\n\treturn 0;\n}\n\nvoid intel_uncore_generic_uncore_mmio_init(void)\n{\n\tuncore_mmio_uncores = intel_uncore_generic_init_uncores(UNCORE_ACCESS_MMIO, 0);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}