{
  "module_name": "uncore.h",
  "hash_id": "f57641a14f6c0701c1f8502c0ce9c25614cae5bc098abff590bf2581322fa0ec",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/events/intel/uncore.h",
  "human_readable_source": " \n#include <linux/slab.h>\n#include <linux/pci.h>\n#include <asm/apicdef.h>\n#include <asm/intel-family.h>\n#include <linux/io-64-nonatomic-lo-hi.h>\n\n#include <linux/perf_event.h>\n#include \"../perf_event.h\"\n\n#define UNCORE_PMU_NAME_LEN\t\t32\n#define UNCORE_PMU_HRTIMER_INTERVAL\t(60LL * NSEC_PER_SEC)\n#define UNCORE_SNB_IMC_HRTIMER_INTERVAL (5ULL * NSEC_PER_SEC)\n\n#define UNCORE_FIXED_EVENT\t\t0xff\n#define UNCORE_PMC_IDX_MAX_GENERIC\t8\n#define UNCORE_PMC_IDX_MAX_FIXED\t1\n#define UNCORE_PMC_IDX_MAX_FREERUNNING\t1\n#define UNCORE_PMC_IDX_FIXED\t\tUNCORE_PMC_IDX_MAX_GENERIC\n#define UNCORE_PMC_IDX_FREERUNNING\t(UNCORE_PMC_IDX_FIXED + \\\n\t\t\t\t\tUNCORE_PMC_IDX_MAX_FIXED)\n#define UNCORE_PMC_IDX_MAX\t\t(UNCORE_PMC_IDX_FREERUNNING + \\\n\t\t\t\t\tUNCORE_PMC_IDX_MAX_FREERUNNING)\n\n#define UNCORE_PCI_DEV_FULL_DATA(dev, func, type, idx)\t\\\n\t\t((dev << 24) | (func << 16) | (type << 8) | idx)\n#define UNCORE_PCI_DEV_DATA(type, idx)\t((type << 8) | idx)\n#define UNCORE_PCI_DEV_DEV(data)\t((data >> 24) & 0xff)\n#define UNCORE_PCI_DEV_FUNC(data)\t((data >> 16) & 0xff)\n#define UNCORE_PCI_DEV_TYPE(data)\t((data >> 8) & 0xff)\n#define UNCORE_PCI_DEV_IDX(data)\t(data & 0xff)\n#define UNCORE_EXTRA_PCI_DEV\t\t0xff\n#define UNCORE_EXTRA_PCI_DEV_MAX\t4\n\n#define UNCORE_EVENT_CONSTRAINT(c, n) EVENT_CONSTRAINT(c, n, 0xff)\n\n#define UNCORE_IGNORE_END\t\t-1\n\nstruct pci_extra_dev {\n\tstruct pci_dev *dev[UNCORE_EXTRA_PCI_DEV_MAX];\n};\n\nstruct intel_uncore_ops;\nstruct intel_uncore_pmu;\nstruct intel_uncore_box;\nstruct uncore_event_desc;\nstruct freerunning_counters;\nstruct intel_uncore_topology;\n\nstruct intel_uncore_type {\n\tconst char *name;\n\tint num_counters;\n\tint num_boxes;\n\tint perf_ctr_bits;\n\tint fixed_ctr_bits;\n\tint num_freerunning_types;\n\tint type_id;\n\tunsigned perf_ctr;\n\tunsigned event_ctl;\n\tunsigned event_mask;\n\tunsigned event_mask_ext;\n\tunsigned fixed_ctr;\n\tunsigned fixed_ctl;\n\tunsigned box_ctl;\n\tu64 *box_ctls;\t \n\tunion {\n\t\tunsigned msr_offset;\n\t\tunsigned mmio_offset;\n\t};\n\tunsigned mmio_map_size;\n\tunsigned num_shared_regs:8;\n\tunsigned single_fixed:1;\n\tunsigned pair_ctr_ctl:1;\n\tunion {\n\t\tunsigned *msr_offsets;\n\t\tunsigned *pci_offsets;\n\t\tunsigned *mmio_offsets;\n\t};\n\tunsigned *box_ids;\n\tstruct event_constraint unconstrainted;\n\tstruct event_constraint *constraints;\n\tstruct intel_uncore_pmu *pmus;\n\tstruct intel_uncore_ops *ops;\n\tstruct uncore_event_desc *event_descs;\n\tstruct freerunning_counters *freerunning;\n\tconst struct attribute_group *attr_groups[4];\n\tconst struct attribute_group **attr_update;\n\tstruct pmu *pmu;  \n\t \n\tstruct intel_uncore_topology **topology;\n\t \n\tint (*get_topology)(struct intel_uncore_type *type);\n\tvoid (*set_mapping)(struct intel_uncore_type *type);\n\tvoid (*cleanup_mapping)(struct intel_uncore_type *type);\n};\n\n#define pmu_group attr_groups[0]\n#define format_group attr_groups[1]\n#define events_group attr_groups[2]\n\nstruct intel_uncore_ops {\n\tvoid (*init_box)(struct intel_uncore_box *);\n\tvoid (*exit_box)(struct intel_uncore_box *);\n\tvoid (*disable_box)(struct intel_uncore_box *);\n\tvoid (*enable_box)(struct intel_uncore_box *);\n\tvoid (*disable_event)(struct intel_uncore_box *, struct perf_event *);\n\tvoid (*enable_event)(struct intel_uncore_box *, struct perf_event *);\n\tu64 (*read_counter)(struct intel_uncore_box *, struct perf_event *);\n\tint (*hw_config)(struct intel_uncore_box *, struct perf_event *);\n\tstruct event_constraint *(*get_constraint)(struct intel_uncore_box *,\n\t\t\t\t\t\t   struct perf_event *);\n\tvoid (*put_constraint)(struct intel_uncore_box *, struct perf_event *);\n};\n\nstruct intel_uncore_pmu {\n\tstruct pmu\t\t\tpmu;\n\tchar\t\t\t\tname[UNCORE_PMU_NAME_LEN];\n\tint\t\t\t\tpmu_idx;\n\tint\t\t\t\tfunc_id;\n\tbool\t\t\t\tregistered;\n\tatomic_t\t\t\tactiveboxes;\n\tstruct intel_uncore_type\t*type;\n\tstruct intel_uncore_box\t\t**boxes;\n};\n\nstruct intel_uncore_extra_reg {\n\traw_spinlock_t lock;\n\tu64 config, config1, config2;\n\tatomic_t ref;\n};\n\nstruct intel_uncore_box {\n\tint dieid;\t \n\tint n_active;\t \n\tint n_events;\n\tint cpu;\t \n\tunsigned long flags;\n\tatomic_t refcnt;\n\tstruct perf_event *events[UNCORE_PMC_IDX_MAX];\n\tstruct perf_event *event_list[UNCORE_PMC_IDX_MAX];\n\tstruct event_constraint *event_constraint[UNCORE_PMC_IDX_MAX];\n\tunsigned long active_mask[BITS_TO_LONGS(UNCORE_PMC_IDX_MAX)];\n\tu64 tags[UNCORE_PMC_IDX_MAX];\n\tstruct pci_dev *pci_dev;\n\tstruct intel_uncore_pmu *pmu;\n\tu64 hrtimer_duration;  \n\tstruct hrtimer hrtimer;\n\tstruct list_head list;\n\tstruct list_head active_list;\n\tvoid __iomem *io_addr;\n\tstruct intel_uncore_extra_reg shared_regs[];\n};\n\n \n#define CFL_UNC_CBO_7_PERFEVTSEL0\t\t0xf70\n#define CFL_UNC_CBO_7_PER_CTR0\t\t\t0xf76\n\n#define UNCORE_BOX_FLAG_INITIATED\t\t0\n \n#define UNCORE_BOX_FLAG_CTL_OFFS8\t\t1\n \n#define UNCORE_BOX_FLAG_CFL8_CBOX_MSR_OFFS\t2\n\nstruct uncore_event_desc {\n\tstruct device_attribute attr;\n\tconst char *config;\n};\n\nstruct freerunning_counters {\n\tunsigned int counter_base;\n\tunsigned int counter_offset;\n\tunsigned int box_offset;\n\tunsigned int num_counters;\n\tunsigned int bits;\n\tunsigned *box_offsets;\n};\n\nstruct uncore_iio_topology {\n\tint pci_bus_no;\n\tint segment;\n};\n\nstruct uncore_upi_topology {\n\tint die_to;\n\tint pmu_idx_to;\n\tint enabled;\n};\n\nstruct intel_uncore_topology {\n\tint pmu_idx;\n\tunion {\n\t\tvoid *untyped;\n\t\tstruct uncore_iio_topology *iio;\n\t\tstruct uncore_upi_topology *upi;\n\t};\n};\n\nstruct pci2phy_map {\n\tstruct list_head list;\n\tint segment;\n\tint pbus_to_dieid[256];\n};\n\nstruct pci2phy_map *__find_pci2phy_map(int segment);\nint uncore_pcibus_to_dieid(struct pci_bus *bus);\nint uncore_die_to_segment(int die);\nint uncore_device_to_die(struct pci_dev *dev);\n\nssize_t uncore_event_show(struct device *dev,\n\t\t\t  struct device_attribute *attr, char *buf);\n\nstatic inline struct intel_uncore_pmu *dev_to_uncore_pmu(struct device *dev)\n{\n\treturn container_of(dev_get_drvdata(dev), struct intel_uncore_pmu, pmu);\n}\n\n#define to_device_attribute(n)\tcontainer_of(n, struct device_attribute, attr)\n#define to_dev_ext_attribute(n)\tcontainer_of(n, struct dev_ext_attribute, attr)\n#define attr_to_ext_attr(n)\tto_dev_ext_attribute(to_device_attribute(n))\n\nextern int __uncore_max_dies;\n#define uncore_max_dies()\t(__uncore_max_dies)\n\n#define INTEL_UNCORE_EVENT_DESC(_name, _config)\t\t\t\\\n{\t\t\t\t\t\t\t\t\\\n\t.attr\t= __ATTR(_name, 0444, uncore_event_show, NULL),\t\\\n\t.config\t= _config,\t\t\t\t\t\\\n}\n\n#define DEFINE_UNCORE_FORMAT_ATTR(_var, _name, _format)\t\t\t\\\nstatic ssize_t __uncore_##_var##_show(struct device *dev,\t\t\\\n\t\t\t\tstruct device_attribute *attr,\t\t\\\n\t\t\t\tchar *page)\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(_format) >= PAGE_SIZE);\t\t\t\\\n\treturn sprintf(page, _format \"\\n\");\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic struct device_attribute format_attr_##_var =\t\t\t\\\n\t__ATTR(_name, 0444, __uncore_##_var##_show, NULL)\n\nstatic inline bool uncore_pmc_fixed(int idx)\n{\n\treturn idx == UNCORE_PMC_IDX_FIXED;\n}\n\nstatic inline bool uncore_pmc_freerunning(int idx)\n{\n\treturn idx == UNCORE_PMC_IDX_FREERUNNING;\n}\n\nstatic inline bool uncore_mmio_is_valid_offset(struct intel_uncore_box *box,\n\t\t\t\t\t       unsigned long offset)\n{\n\tif (offset < box->pmu->type->mmio_map_size)\n\t\treturn true;\n\n\tpr_warn_once(\"perf uncore: Invalid offset 0x%lx exceeds mapped area of %s.\\n\",\n\t\t     offset, box->pmu->type->name);\n\n\treturn false;\n}\n\nstatic inline\nunsigned int uncore_mmio_box_ctl(struct intel_uncore_box *box)\n{\n\treturn box->pmu->type->box_ctl +\n\t       box->pmu->type->mmio_offset * box->pmu->pmu_idx;\n}\n\nstatic inline unsigned uncore_pci_box_ctl(struct intel_uncore_box *box)\n{\n\treturn box->pmu->type->box_ctl;\n}\n\nstatic inline unsigned uncore_pci_fixed_ctl(struct intel_uncore_box *box)\n{\n\treturn box->pmu->type->fixed_ctl;\n}\n\nstatic inline unsigned uncore_pci_fixed_ctr(struct intel_uncore_box *box)\n{\n\treturn box->pmu->type->fixed_ctr;\n}\n\nstatic inline\nunsigned uncore_pci_event_ctl(struct intel_uncore_box *box, int idx)\n{\n\tif (test_bit(UNCORE_BOX_FLAG_CTL_OFFS8, &box->flags))\n\t\treturn idx * 8 + box->pmu->type->event_ctl;\n\n\treturn idx * 4 + box->pmu->type->event_ctl;\n}\n\nstatic inline\nunsigned uncore_pci_perf_ctr(struct intel_uncore_box *box, int idx)\n{\n\treturn idx * 8 + box->pmu->type->perf_ctr;\n}\n\nstatic inline unsigned uncore_msr_box_offset(struct intel_uncore_box *box)\n{\n\tstruct intel_uncore_pmu *pmu = box->pmu;\n\treturn pmu->type->msr_offsets ?\n\t\tpmu->type->msr_offsets[pmu->pmu_idx] :\n\t\tpmu->type->msr_offset * pmu->pmu_idx;\n}\n\nstatic inline unsigned uncore_msr_box_ctl(struct intel_uncore_box *box)\n{\n\tif (!box->pmu->type->box_ctl)\n\t\treturn 0;\n\treturn box->pmu->type->box_ctl + uncore_msr_box_offset(box);\n}\n\nstatic inline unsigned uncore_msr_fixed_ctl(struct intel_uncore_box *box)\n{\n\tif (!box->pmu->type->fixed_ctl)\n\t\treturn 0;\n\treturn box->pmu->type->fixed_ctl + uncore_msr_box_offset(box);\n}\n\nstatic inline unsigned uncore_msr_fixed_ctr(struct intel_uncore_box *box)\n{\n\treturn box->pmu->type->fixed_ctr + uncore_msr_box_offset(box);\n}\n\n\n \nstatic inline unsigned int uncore_freerunning_idx(u64 config)\n{\n\treturn ((config >> 8) & 0xf);\n}\n\n#define UNCORE_FREERUNNING_UMASK_START\t\t0x10\n\nstatic inline unsigned int uncore_freerunning_type(u64 config)\n{\n\treturn ((((config >> 8) - UNCORE_FREERUNNING_UMASK_START) >> 4) & 0xf);\n}\n\nstatic inline\nunsigned int uncore_freerunning_counter(struct intel_uncore_box *box,\n\t\t\t\t\tstruct perf_event *event)\n{\n\tunsigned int type = uncore_freerunning_type(event->hw.config);\n\tunsigned int idx = uncore_freerunning_idx(event->hw.config);\n\tstruct intel_uncore_pmu *pmu = box->pmu;\n\n\treturn pmu->type->freerunning[type].counter_base +\n\t       pmu->type->freerunning[type].counter_offset * idx +\n\t       (pmu->type->freerunning[type].box_offsets ?\n\t        pmu->type->freerunning[type].box_offsets[pmu->pmu_idx] :\n\t        pmu->type->freerunning[type].box_offset * pmu->pmu_idx);\n}\n\nstatic inline\nunsigned uncore_msr_event_ctl(struct intel_uncore_box *box, int idx)\n{\n\tif (test_bit(UNCORE_BOX_FLAG_CFL8_CBOX_MSR_OFFS, &box->flags)) {\n\t\treturn CFL_UNC_CBO_7_PERFEVTSEL0 +\n\t\t       (box->pmu->type->pair_ctr_ctl ? 2 * idx : idx);\n\t} else {\n\t\treturn box->pmu->type->event_ctl +\n\t\t       (box->pmu->type->pair_ctr_ctl ? 2 * idx : idx) +\n\t\t       uncore_msr_box_offset(box);\n\t}\n}\n\nstatic inline\nunsigned uncore_msr_perf_ctr(struct intel_uncore_box *box, int idx)\n{\n\tif (test_bit(UNCORE_BOX_FLAG_CFL8_CBOX_MSR_OFFS, &box->flags)) {\n\t\treturn CFL_UNC_CBO_7_PER_CTR0 +\n\t\t       (box->pmu->type->pair_ctr_ctl ? 2 * idx : idx);\n\t} else {\n\t\treturn box->pmu->type->perf_ctr +\n\t\t       (box->pmu->type->pair_ctr_ctl ? 2 * idx : idx) +\n\t\t       uncore_msr_box_offset(box);\n\t}\n}\n\nstatic inline\nunsigned uncore_fixed_ctl(struct intel_uncore_box *box)\n{\n\tif (box->pci_dev || box->io_addr)\n\t\treturn uncore_pci_fixed_ctl(box);\n\telse\n\t\treturn uncore_msr_fixed_ctl(box);\n}\n\nstatic inline\nunsigned uncore_fixed_ctr(struct intel_uncore_box *box)\n{\n\tif (box->pci_dev || box->io_addr)\n\t\treturn uncore_pci_fixed_ctr(box);\n\telse\n\t\treturn uncore_msr_fixed_ctr(box);\n}\n\nstatic inline\nunsigned uncore_event_ctl(struct intel_uncore_box *box, int idx)\n{\n\tif (box->pci_dev || box->io_addr)\n\t\treturn uncore_pci_event_ctl(box, idx);\n\telse\n\t\treturn uncore_msr_event_ctl(box, idx);\n}\n\nstatic inline\nunsigned uncore_perf_ctr(struct intel_uncore_box *box, int idx)\n{\n\tif (box->pci_dev || box->io_addr)\n\t\treturn uncore_pci_perf_ctr(box, idx);\n\telse\n\t\treturn uncore_msr_perf_ctr(box, idx);\n}\n\nstatic inline int uncore_perf_ctr_bits(struct intel_uncore_box *box)\n{\n\treturn box->pmu->type->perf_ctr_bits;\n}\n\nstatic inline int uncore_fixed_ctr_bits(struct intel_uncore_box *box)\n{\n\treturn box->pmu->type->fixed_ctr_bits;\n}\n\nstatic inline\nunsigned int uncore_freerunning_bits(struct intel_uncore_box *box,\n\t\t\t\t     struct perf_event *event)\n{\n\tunsigned int type = uncore_freerunning_type(event->hw.config);\n\n\treturn box->pmu->type->freerunning[type].bits;\n}\n\nstatic inline int uncore_num_freerunning(struct intel_uncore_box *box,\n\t\t\t\t\t struct perf_event *event)\n{\n\tunsigned int type = uncore_freerunning_type(event->hw.config);\n\n\treturn box->pmu->type->freerunning[type].num_counters;\n}\n\nstatic inline int uncore_num_freerunning_types(struct intel_uncore_box *box,\n\t\t\t\t\t       struct perf_event *event)\n{\n\treturn box->pmu->type->num_freerunning_types;\n}\n\nstatic inline bool check_valid_freerunning_event(struct intel_uncore_box *box,\n\t\t\t\t\t\t struct perf_event *event)\n{\n\tunsigned int type = uncore_freerunning_type(event->hw.config);\n\tunsigned int idx = uncore_freerunning_idx(event->hw.config);\n\n\treturn (type < uncore_num_freerunning_types(box, event)) &&\n\t       (idx < uncore_num_freerunning(box, event));\n}\n\nstatic inline int uncore_num_counters(struct intel_uncore_box *box)\n{\n\treturn box->pmu->type->num_counters;\n}\n\nstatic inline bool is_freerunning_event(struct perf_event *event)\n{\n\tu64 cfg = event->attr.config;\n\n\treturn ((cfg & UNCORE_FIXED_EVENT) == UNCORE_FIXED_EVENT) &&\n\t       (((cfg >> 8) & 0xff) >= UNCORE_FREERUNNING_UMASK_START);\n}\n\n \nstatic inline int uncore_freerunning_hw_config(struct intel_uncore_box *box,\n\t\t\t\t\t       struct perf_event *event)\n{\n\tif (is_freerunning_event(event))\n\t\treturn 0;\n\n\treturn -EINVAL;\n}\n\nstatic inline void uncore_disable_event(struct intel_uncore_box *box,\n\t\t\t\tstruct perf_event *event)\n{\n\tbox->pmu->type->ops->disable_event(box, event);\n}\n\nstatic inline void uncore_enable_event(struct intel_uncore_box *box,\n\t\t\t\tstruct perf_event *event)\n{\n\tbox->pmu->type->ops->enable_event(box, event);\n}\n\nstatic inline u64 uncore_read_counter(struct intel_uncore_box *box,\n\t\t\t\tstruct perf_event *event)\n{\n\treturn box->pmu->type->ops->read_counter(box, event);\n}\n\nstatic inline void uncore_box_init(struct intel_uncore_box *box)\n{\n\tif (!test_and_set_bit(UNCORE_BOX_FLAG_INITIATED, &box->flags)) {\n\t\tif (box->pmu->type->ops->init_box)\n\t\t\tbox->pmu->type->ops->init_box(box);\n\t}\n}\n\nstatic inline void uncore_box_exit(struct intel_uncore_box *box)\n{\n\tif (test_and_clear_bit(UNCORE_BOX_FLAG_INITIATED, &box->flags)) {\n\t\tif (box->pmu->type->ops->exit_box)\n\t\t\tbox->pmu->type->ops->exit_box(box);\n\t}\n}\n\nstatic inline bool uncore_box_is_fake(struct intel_uncore_box *box)\n{\n\treturn (box->dieid < 0);\n}\n\nstatic inline struct intel_uncore_pmu *uncore_event_to_pmu(struct perf_event *event)\n{\n\treturn container_of(event->pmu, struct intel_uncore_pmu, pmu);\n}\n\nstatic inline struct intel_uncore_box *uncore_event_to_box(struct perf_event *event)\n{\n\treturn event->pmu_private;\n}\n\nstruct intel_uncore_box *uncore_pmu_to_box(struct intel_uncore_pmu *pmu, int cpu);\nu64 uncore_msr_read_counter(struct intel_uncore_box *box, struct perf_event *event);\nvoid uncore_mmio_exit_box(struct intel_uncore_box *box);\nu64 uncore_mmio_read_counter(struct intel_uncore_box *box,\n\t\t\t     struct perf_event *event);\nvoid uncore_pmu_start_hrtimer(struct intel_uncore_box *box);\nvoid uncore_pmu_cancel_hrtimer(struct intel_uncore_box *box);\nvoid uncore_pmu_event_start(struct perf_event *event, int flags);\nvoid uncore_pmu_event_stop(struct perf_event *event, int flags);\nint uncore_pmu_event_add(struct perf_event *event, int flags);\nvoid uncore_pmu_event_del(struct perf_event *event, int flags);\nvoid uncore_pmu_event_read(struct perf_event *event);\nvoid uncore_perf_event_update(struct intel_uncore_box *box, struct perf_event *event);\nstruct event_constraint *\nuncore_get_constraint(struct intel_uncore_box *box, struct perf_event *event);\nvoid uncore_put_constraint(struct intel_uncore_box *box, struct perf_event *event);\nu64 uncore_shared_reg_config(struct intel_uncore_box *box, int idx);\nvoid uncore_get_alias_name(char *pmu_name, struct intel_uncore_pmu *pmu);\n\nextern struct intel_uncore_type *empty_uncore[];\nextern struct intel_uncore_type **uncore_msr_uncores;\nextern struct intel_uncore_type **uncore_pci_uncores;\nextern struct intel_uncore_type **uncore_mmio_uncores;\nextern struct pci_driver *uncore_pci_driver;\nextern struct pci_driver *uncore_pci_sub_driver;\nextern raw_spinlock_t pci2phy_map_lock;\nextern struct list_head pci2phy_map_head;\nextern struct pci_extra_dev *uncore_extra_pci_dev;\nextern struct event_constraint uncore_constraint_empty;\nextern int spr_uncore_units_ignore[];\n\n \nint snb_uncore_pci_init(void);\nint ivb_uncore_pci_init(void);\nint hsw_uncore_pci_init(void);\nint bdw_uncore_pci_init(void);\nint skl_uncore_pci_init(void);\nvoid snb_uncore_cpu_init(void);\nvoid nhm_uncore_cpu_init(void);\nvoid skl_uncore_cpu_init(void);\nvoid icl_uncore_cpu_init(void);\nvoid tgl_uncore_cpu_init(void);\nvoid adl_uncore_cpu_init(void);\nvoid mtl_uncore_cpu_init(void);\nvoid tgl_uncore_mmio_init(void);\nvoid tgl_l_uncore_mmio_init(void);\nvoid adl_uncore_mmio_init(void);\nint snb_pci2phy_map_init(int devid);\n\n \nint snbep_uncore_pci_init(void);\nvoid snbep_uncore_cpu_init(void);\nint ivbep_uncore_pci_init(void);\nvoid ivbep_uncore_cpu_init(void);\nint hswep_uncore_pci_init(void);\nvoid hswep_uncore_cpu_init(void);\nint bdx_uncore_pci_init(void);\nvoid bdx_uncore_cpu_init(void);\nint knl_uncore_pci_init(void);\nvoid knl_uncore_cpu_init(void);\nint skx_uncore_pci_init(void);\nvoid skx_uncore_cpu_init(void);\nint snr_uncore_pci_init(void);\nvoid snr_uncore_cpu_init(void);\nvoid snr_uncore_mmio_init(void);\nint icx_uncore_pci_init(void);\nvoid icx_uncore_cpu_init(void);\nvoid icx_uncore_mmio_init(void);\nint spr_uncore_pci_init(void);\nvoid spr_uncore_cpu_init(void);\nvoid spr_uncore_mmio_init(void);\n\n \nvoid nhmex_uncore_cpu_init(void);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}