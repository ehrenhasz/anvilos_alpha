{
  "module_name": "uncore.c",
  "hash_id": "d8cb69661bdcc6cef1802a7e6dbdc13c62cfd433e61e04c5fb5395fe587d2c1a",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/events/intel/uncore.c",
  "human_readable_source": "\n#include <linux/module.h>\n\n#include <asm/cpu_device_id.h>\n#include <asm/intel-family.h>\n#include \"uncore.h\"\n#include \"uncore_discovery.h\"\n\nstatic bool uncore_no_discover;\nmodule_param(uncore_no_discover, bool, 0);\nMODULE_PARM_DESC(uncore_no_discover, \"Don't enable the Intel uncore PerfMon discovery mechanism \"\n\t\t\t\t     \"(default: enable the discovery mechanism).\");\nstruct intel_uncore_type *empty_uncore[] = { NULL, };\nstruct intel_uncore_type **uncore_msr_uncores = empty_uncore;\nstruct intel_uncore_type **uncore_pci_uncores = empty_uncore;\nstruct intel_uncore_type **uncore_mmio_uncores = empty_uncore;\n\nstatic bool pcidrv_registered;\nstruct pci_driver *uncore_pci_driver;\n \nstruct pci_driver *uncore_pci_sub_driver;\n \nDEFINE_RAW_SPINLOCK(pci2phy_map_lock);\nstruct list_head pci2phy_map_head = LIST_HEAD_INIT(pci2phy_map_head);\nstruct pci_extra_dev *uncore_extra_pci_dev;\nint __uncore_max_dies;\n\n \nstatic cpumask_t uncore_cpu_mask;\n\n \nstatic struct event_constraint uncore_constraint_fixed =\n\tEVENT_CONSTRAINT(~0ULL, 1 << UNCORE_PMC_IDX_FIXED, ~0ULL);\nstruct event_constraint uncore_constraint_empty =\n\tEVENT_CONSTRAINT(0, 0, 0);\n\nMODULE_LICENSE(\"GPL\");\n\nint uncore_pcibus_to_dieid(struct pci_bus *bus)\n{\n\tstruct pci2phy_map *map;\n\tint die_id = -1;\n\n\traw_spin_lock(&pci2phy_map_lock);\n\tlist_for_each_entry(map, &pci2phy_map_head, list) {\n\t\tif (map->segment == pci_domain_nr(bus)) {\n\t\t\tdie_id = map->pbus_to_dieid[bus->number];\n\t\t\tbreak;\n\t\t}\n\t}\n\traw_spin_unlock(&pci2phy_map_lock);\n\n\treturn die_id;\n}\n\nint uncore_die_to_segment(int die)\n{\n\tstruct pci_bus *bus = NULL;\n\n\t \n\twhile ((bus = pci_find_next_bus(bus)) &&\n\t       (die != uncore_pcibus_to_dieid(bus)))\n\t\t;\n\n\treturn bus ? pci_domain_nr(bus) : -EINVAL;\n}\n\nint uncore_device_to_die(struct pci_dev *dev)\n{\n\tint node = pcibus_to_node(dev->bus);\n\tint cpu;\n\n\tfor_each_cpu(cpu, cpumask_of_pcibus(dev->bus)) {\n\t\tstruct cpuinfo_x86 *c = &cpu_data(cpu);\n\n\t\tif (c->initialized && cpu_to_node(cpu) == node)\n\t\t\treturn c->logical_die_id;\n\t}\n\n\treturn -1;\n}\n\nstatic void uncore_free_pcibus_map(void)\n{\n\tstruct pci2phy_map *map, *tmp;\n\n\tlist_for_each_entry_safe(map, tmp, &pci2phy_map_head, list) {\n\t\tlist_del(&map->list);\n\t\tkfree(map);\n\t}\n}\n\nstruct pci2phy_map *__find_pci2phy_map(int segment)\n{\n\tstruct pci2phy_map *map, *alloc = NULL;\n\tint i;\n\n\tlockdep_assert_held(&pci2phy_map_lock);\n\nlookup:\n\tlist_for_each_entry(map, &pci2phy_map_head, list) {\n\t\tif (map->segment == segment)\n\t\t\tgoto end;\n\t}\n\n\tif (!alloc) {\n\t\traw_spin_unlock(&pci2phy_map_lock);\n\t\talloc = kmalloc(sizeof(struct pci2phy_map), GFP_KERNEL);\n\t\traw_spin_lock(&pci2phy_map_lock);\n\n\t\tif (!alloc)\n\t\t\treturn NULL;\n\n\t\tgoto lookup;\n\t}\n\n\tmap = alloc;\n\talloc = NULL;\n\tmap->segment = segment;\n\tfor (i = 0; i < 256; i++)\n\t\tmap->pbus_to_dieid[i] = -1;\n\tlist_add_tail(&map->list, &pci2phy_map_head);\n\nend:\n\tkfree(alloc);\n\treturn map;\n}\n\nssize_t uncore_event_show(struct device *dev,\n\t\t\t  struct device_attribute *attr, char *buf)\n{\n\tstruct uncore_event_desc *event =\n\t\tcontainer_of(attr, struct uncore_event_desc, attr);\n\treturn sprintf(buf, \"%s\", event->config);\n}\n\nstruct intel_uncore_box *uncore_pmu_to_box(struct intel_uncore_pmu *pmu, int cpu)\n{\n\tunsigned int dieid = topology_logical_die_id(cpu);\n\n\t \n\treturn dieid < uncore_max_dies() ? pmu->boxes[dieid] : NULL;\n}\n\nu64 uncore_msr_read_counter(struct intel_uncore_box *box, struct perf_event *event)\n{\n\tu64 count;\n\n\trdmsrl(event->hw.event_base, count);\n\n\treturn count;\n}\n\nvoid uncore_mmio_exit_box(struct intel_uncore_box *box)\n{\n\tif (box->io_addr)\n\t\tiounmap(box->io_addr);\n}\n\nu64 uncore_mmio_read_counter(struct intel_uncore_box *box,\n\t\t\t     struct perf_event *event)\n{\n\tif (!box->io_addr)\n\t\treturn 0;\n\n\tif (!uncore_mmio_is_valid_offset(box, event->hw.event_base))\n\t\treturn 0;\n\n\treturn readq(box->io_addr + event->hw.event_base);\n}\n\n \nstruct event_constraint *\nuncore_get_constraint(struct intel_uncore_box *box, struct perf_event *event)\n{\n\tstruct intel_uncore_extra_reg *er;\n\tstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\n\tstruct hw_perf_event_extra *reg2 = &event->hw.branch_reg;\n\tunsigned long flags;\n\tbool ok = false;\n\n\t \n\tif (reg1->idx == EXTRA_REG_NONE ||\n\t    (!uncore_box_is_fake(box) && reg1->alloc))\n\t\treturn NULL;\n\n\ter = &box->shared_regs[reg1->idx];\n\traw_spin_lock_irqsave(&er->lock, flags);\n\tif (!atomic_read(&er->ref) ||\n\t    (er->config1 == reg1->config && er->config2 == reg2->config)) {\n\t\tatomic_inc(&er->ref);\n\t\ter->config1 = reg1->config;\n\t\ter->config2 = reg2->config;\n\t\tok = true;\n\t}\n\traw_spin_unlock_irqrestore(&er->lock, flags);\n\n\tif (ok) {\n\t\tif (!uncore_box_is_fake(box))\n\t\t\treg1->alloc = 1;\n\t\treturn NULL;\n\t}\n\n\treturn &uncore_constraint_empty;\n}\n\nvoid uncore_put_constraint(struct intel_uncore_box *box, struct perf_event *event)\n{\n\tstruct intel_uncore_extra_reg *er;\n\tstruct hw_perf_event_extra *reg1 = &event->hw.extra_reg;\n\n\t \n\tif (uncore_box_is_fake(box) || !reg1->alloc)\n\t\treturn;\n\n\ter = &box->shared_regs[reg1->idx];\n\tatomic_dec(&er->ref);\n\treg1->alloc = 0;\n}\n\nu64 uncore_shared_reg_config(struct intel_uncore_box *box, int idx)\n{\n\tstruct intel_uncore_extra_reg *er;\n\tunsigned long flags;\n\tu64 config;\n\n\ter = &box->shared_regs[idx];\n\n\traw_spin_lock_irqsave(&er->lock, flags);\n\tconfig = er->config;\n\traw_spin_unlock_irqrestore(&er->lock, flags);\n\n\treturn config;\n}\n\nstatic void uncore_assign_hw_event(struct intel_uncore_box *box,\n\t\t\t\t   struct perf_event *event, int idx)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\thwc->idx = idx;\n\thwc->last_tag = ++box->tags[idx];\n\n\tif (uncore_pmc_fixed(hwc->idx)) {\n\t\thwc->event_base = uncore_fixed_ctr(box);\n\t\thwc->config_base = uncore_fixed_ctl(box);\n\t\treturn;\n\t}\n\n\thwc->config_base = uncore_event_ctl(box, hwc->idx);\n\thwc->event_base  = uncore_perf_ctr(box, hwc->idx);\n}\n\nvoid uncore_perf_event_update(struct intel_uncore_box *box, struct perf_event *event)\n{\n\tu64 prev_count, new_count, delta;\n\tint shift;\n\n\tif (uncore_pmc_freerunning(event->hw.idx))\n\t\tshift = 64 - uncore_freerunning_bits(box, event);\n\telse if (uncore_pmc_fixed(event->hw.idx))\n\t\tshift = 64 - uncore_fixed_ctr_bits(box);\n\telse\n\t\tshift = 64 - uncore_perf_ctr_bits(box);\n\n\t \nagain:\n\tprev_count = local64_read(&event->hw.prev_count);\n\tnew_count = uncore_read_counter(box, event);\n\tif (local64_xchg(&event->hw.prev_count, new_count) != prev_count)\n\t\tgoto again;\n\n\tdelta = (new_count << shift) - (prev_count << shift);\n\tdelta >>= shift;\n\n\tlocal64_add(delta, &event->count);\n}\n\n \nstatic enum hrtimer_restart uncore_pmu_hrtimer(struct hrtimer *hrtimer)\n{\n\tstruct intel_uncore_box *box;\n\tstruct perf_event *event;\n\tunsigned long flags;\n\tint bit;\n\n\tbox = container_of(hrtimer, struct intel_uncore_box, hrtimer);\n\tif (!box->n_active || box->cpu != smp_processor_id())\n\t\treturn HRTIMER_NORESTART;\n\t \n\tlocal_irq_save(flags);\n\n\t \n\tlist_for_each_entry(event, &box->active_list, active_entry) {\n\t\tuncore_perf_event_update(box, event);\n\t}\n\n\tfor_each_set_bit(bit, box->active_mask, UNCORE_PMC_IDX_MAX)\n\t\tuncore_perf_event_update(box, box->events[bit]);\n\n\tlocal_irq_restore(flags);\n\n\thrtimer_forward_now(hrtimer, ns_to_ktime(box->hrtimer_duration));\n\treturn HRTIMER_RESTART;\n}\n\nvoid uncore_pmu_start_hrtimer(struct intel_uncore_box *box)\n{\n\thrtimer_start(&box->hrtimer, ns_to_ktime(box->hrtimer_duration),\n\t\t      HRTIMER_MODE_REL_PINNED);\n}\n\nvoid uncore_pmu_cancel_hrtimer(struct intel_uncore_box *box)\n{\n\thrtimer_cancel(&box->hrtimer);\n}\n\nstatic void uncore_pmu_init_hrtimer(struct intel_uncore_box *box)\n{\n\thrtimer_init(&box->hrtimer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);\n\tbox->hrtimer.function = uncore_pmu_hrtimer;\n}\n\nstatic struct intel_uncore_box *uncore_alloc_box(struct intel_uncore_type *type,\n\t\t\t\t\t\t int node)\n{\n\tint i, size, numshared = type->num_shared_regs ;\n\tstruct intel_uncore_box *box;\n\n\tsize = sizeof(*box) + numshared * sizeof(struct intel_uncore_extra_reg);\n\n\tbox = kzalloc_node(size, GFP_KERNEL, node);\n\tif (!box)\n\t\treturn NULL;\n\n\tfor (i = 0; i < numshared; i++)\n\t\traw_spin_lock_init(&box->shared_regs[i].lock);\n\n\tuncore_pmu_init_hrtimer(box);\n\tbox->cpu = -1;\n\tbox->dieid = -1;\n\n\t \n\tbox->hrtimer_duration = UNCORE_PMU_HRTIMER_INTERVAL;\n\n\tINIT_LIST_HEAD(&box->active_list);\n\n\treturn box;\n}\n\n \nstatic int uncore_pmu_event_init(struct perf_event *event);\n\nstatic bool is_box_event(struct intel_uncore_box *box, struct perf_event *event)\n{\n\treturn &box->pmu->pmu == event->pmu;\n}\n\nstatic int\nuncore_collect_events(struct intel_uncore_box *box, struct perf_event *leader,\n\t\t      bool dogrp)\n{\n\tstruct perf_event *event;\n\tint n, max_count;\n\n\tmax_count = box->pmu->type->num_counters;\n\tif (box->pmu->type->fixed_ctl)\n\t\tmax_count++;\n\n\tif (box->n_events >= max_count)\n\t\treturn -EINVAL;\n\n\tn = box->n_events;\n\n\tif (is_box_event(box, leader)) {\n\t\tbox->event_list[n] = leader;\n\t\tn++;\n\t}\n\n\tif (!dogrp)\n\t\treturn n;\n\n\tfor_each_sibling_event(event, leader) {\n\t\tif (!is_box_event(box, event) ||\n\t\t    event->state <= PERF_EVENT_STATE_OFF)\n\t\t\tcontinue;\n\n\t\tif (n >= max_count)\n\t\t\treturn -EINVAL;\n\n\t\tbox->event_list[n] = event;\n\t\tn++;\n\t}\n\treturn n;\n}\n\nstatic struct event_constraint *\nuncore_get_event_constraint(struct intel_uncore_box *box, struct perf_event *event)\n{\n\tstruct intel_uncore_type *type = box->pmu->type;\n\tstruct event_constraint *c;\n\n\tif (type->ops->get_constraint) {\n\t\tc = type->ops->get_constraint(box, event);\n\t\tif (c)\n\t\t\treturn c;\n\t}\n\n\tif (event->attr.config == UNCORE_FIXED_EVENT)\n\t\treturn &uncore_constraint_fixed;\n\n\tif (type->constraints) {\n\t\tfor_each_event_constraint(c, type->constraints) {\n\t\t\tif ((event->hw.config & c->cmask) == c->code)\n\t\t\t\treturn c;\n\t\t}\n\t}\n\n\treturn &type->unconstrainted;\n}\n\nstatic void uncore_put_event_constraint(struct intel_uncore_box *box,\n\t\t\t\t\tstruct perf_event *event)\n{\n\tif (box->pmu->type->ops->put_constraint)\n\t\tbox->pmu->type->ops->put_constraint(box, event);\n}\n\nstatic int uncore_assign_events(struct intel_uncore_box *box, int assign[], int n)\n{\n\tunsigned long used_mask[BITS_TO_LONGS(UNCORE_PMC_IDX_MAX)];\n\tstruct event_constraint *c;\n\tint i, wmin, wmax, ret = 0;\n\tstruct hw_perf_event *hwc;\n\n\tbitmap_zero(used_mask, UNCORE_PMC_IDX_MAX);\n\n\tfor (i = 0, wmin = UNCORE_PMC_IDX_MAX, wmax = 0; i < n; i++) {\n\t\tc = uncore_get_event_constraint(box, box->event_list[i]);\n\t\tbox->event_constraint[i] = c;\n\t\twmin = min(wmin, c->weight);\n\t\twmax = max(wmax, c->weight);\n\t}\n\n\t \n\tfor (i = 0; i < n; i++) {\n\t\thwc = &box->event_list[i]->hw;\n\t\tc = box->event_constraint[i];\n\n\t\t \n\t\tif (hwc->idx == -1)\n\t\t\tbreak;\n\n\t\t \n\t\tif (!test_bit(hwc->idx, c->idxmsk))\n\t\t\tbreak;\n\n\t\t \n\t\tif (test_bit(hwc->idx, used_mask))\n\t\t\tbreak;\n\n\t\t__set_bit(hwc->idx, used_mask);\n\t\tif (assign)\n\t\t\tassign[i] = hwc->idx;\n\t}\n\t \n\tif (i != n)\n\t\tret = perf_assign_events(box->event_constraint, n,\n\t\t\t\t\t wmin, wmax, n, assign);\n\n\tif (!assign || ret) {\n\t\tfor (i = 0; i < n; i++)\n\t\t\tuncore_put_event_constraint(box, box->event_list[i]);\n\t}\n\treturn ret ? -EINVAL : 0;\n}\n\nvoid uncore_pmu_event_start(struct perf_event *event, int flags)\n{\n\tstruct intel_uncore_box *box = uncore_event_to_box(event);\n\tint idx = event->hw.idx;\n\n\tif (WARN_ON_ONCE(idx == -1 || idx >= UNCORE_PMC_IDX_MAX))\n\t\treturn;\n\n\t \n\tif (uncore_pmc_freerunning(event->hw.idx)) {\n\t\tlist_add_tail(&event->active_entry, &box->active_list);\n\t\tlocal64_set(&event->hw.prev_count,\n\t\t\t    uncore_read_counter(box, event));\n\t\tif (box->n_active++ == 0)\n\t\t\tuncore_pmu_start_hrtimer(box);\n\t\treturn;\n\t}\n\n\tif (WARN_ON_ONCE(!(event->hw.state & PERF_HES_STOPPED)))\n\t\treturn;\n\n\tevent->hw.state = 0;\n\tbox->events[idx] = event;\n\tbox->n_active++;\n\t__set_bit(idx, box->active_mask);\n\n\tlocal64_set(&event->hw.prev_count, uncore_read_counter(box, event));\n\tuncore_enable_event(box, event);\n\n\tif (box->n_active == 1)\n\t\tuncore_pmu_start_hrtimer(box);\n}\n\nvoid uncore_pmu_event_stop(struct perf_event *event, int flags)\n{\n\tstruct intel_uncore_box *box = uncore_event_to_box(event);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t \n\tif (uncore_pmc_freerunning(hwc->idx)) {\n\t\tlist_del(&event->active_entry);\n\t\tif (--box->n_active == 0)\n\t\t\tuncore_pmu_cancel_hrtimer(box);\n\t\tuncore_perf_event_update(box, event);\n\t\treturn;\n\t}\n\n\tif (__test_and_clear_bit(hwc->idx, box->active_mask)) {\n\t\tuncore_disable_event(box, event);\n\t\tbox->n_active--;\n\t\tbox->events[hwc->idx] = NULL;\n\t\tWARN_ON_ONCE(hwc->state & PERF_HES_STOPPED);\n\t\thwc->state |= PERF_HES_STOPPED;\n\n\t\tif (box->n_active == 0)\n\t\t\tuncore_pmu_cancel_hrtimer(box);\n\t}\n\n\tif ((flags & PERF_EF_UPDATE) && !(hwc->state & PERF_HES_UPTODATE)) {\n\t\t \n\t\tuncore_perf_event_update(box, event);\n\t\thwc->state |= PERF_HES_UPTODATE;\n\t}\n}\n\nint uncore_pmu_event_add(struct perf_event *event, int flags)\n{\n\tstruct intel_uncore_box *box = uncore_event_to_box(event);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint assign[UNCORE_PMC_IDX_MAX];\n\tint i, n, ret;\n\n\tif (!box)\n\t\treturn -ENODEV;\n\n\t \n\tif (uncore_pmc_freerunning(hwc->idx)) {\n\t\tif (flags & PERF_EF_START)\n\t\t\tuncore_pmu_event_start(event, 0);\n\t\treturn 0;\n\t}\n\n\tret = n = uncore_collect_events(box, event, false);\n\tif (ret < 0)\n\t\treturn ret;\n\n\thwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\n\tif (!(flags & PERF_EF_START))\n\t\thwc->state |= PERF_HES_ARCH;\n\n\tret = uncore_assign_events(box, assign, n);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tfor (i = 0; i < box->n_events; i++) {\n\t\tevent = box->event_list[i];\n\t\thwc = &event->hw;\n\n\t\tif (hwc->idx == assign[i] &&\n\t\t\thwc->last_tag == box->tags[assign[i]])\n\t\t\tcontinue;\n\t\t \n\t\tif (hwc->state & PERF_HES_STOPPED)\n\t\t\thwc->state |= PERF_HES_ARCH;\n\n\t\tuncore_pmu_event_stop(event, PERF_EF_UPDATE);\n\t}\n\n\t \n\tfor (i = 0; i < n; i++) {\n\t\tevent = box->event_list[i];\n\t\thwc = &event->hw;\n\n\t\tif (hwc->idx != assign[i] ||\n\t\t\thwc->last_tag != box->tags[assign[i]])\n\t\t\tuncore_assign_hw_event(box, event, assign[i]);\n\t\telse if (i < box->n_events)\n\t\t\tcontinue;\n\n\t\tif (hwc->state & PERF_HES_ARCH)\n\t\t\tcontinue;\n\n\t\tuncore_pmu_event_start(event, 0);\n\t}\n\tbox->n_events = n;\n\n\treturn 0;\n}\n\nvoid uncore_pmu_event_del(struct perf_event *event, int flags)\n{\n\tstruct intel_uncore_box *box = uncore_event_to_box(event);\n\tint i;\n\n\tuncore_pmu_event_stop(event, PERF_EF_UPDATE);\n\n\t \n\tif (uncore_pmc_freerunning(event->hw.idx))\n\t\treturn;\n\n\tfor (i = 0; i < box->n_events; i++) {\n\t\tif (event == box->event_list[i]) {\n\t\t\tuncore_put_event_constraint(box, event);\n\n\t\t\tfor (++i; i < box->n_events; i++)\n\t\t\t\tbox->event_list[i - 1] = box->event_list[i];\n\n\t\t\t--box->n_events;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tevent->hw.idx = -1;\n\tevent->hw.last_tag = ~0ULL;\n}\n\nvoid uncore_pmu_event_read(struct perf_event *event)\n{\n\tstruct intel_uncore_box *box = uncore_event_to_box(event);\n\tuncore_perf_event_update(box, event);\n}\n\n \nstatic int uncore_validate_group(struct intel_uncore_pmu *pmu,\n\t\t\t\tstruct perf_event *event)\n{\n\tstruct perf_event *leader = event->group_leader;\n\tstruct intel_uncore_box *fake_box;\n\tint ret = -EINVAL, n;\n\n\t \n\tif (uncore_pmc_freerunning(event->hw.idx))\n\t\treturn 0;\n\n\tfake_box = uncore_alloc_box(pmu->type, NUMA_NO_NODE);\n\tif (!fake_box)\n\t\treturn -ENOMEM;\n\n\tfake_box->pmu = pmu;\n\t \n\tn = uncore_collect_events(fake_box, leader, true);\n\tif (n < 0)\n\t\tgoto out;\n\n\tfake_box->n_events = n;\n\tn = uncore_collect_events(fake_box, event, false);\n\tif (n < 0)\n\t\tgoto out;\n\n\tfake_box->n_events = n;\n\n\tret = uncore_assign_events(fake_box, NULL, n);\nout:\n\tkfree(fake_box);\n\treturn ret;\n}\n\nstatic int uncore_pmu_event_init(struct perf_event *event)\n{\n\tstruct intel_uncore_pmu *pmu;\n\tstruct intel_uncore_box *box;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint ret;\n\n\tif (event->attr.type != event->pmu->type)\n\t\treturn -ENOENT;\n\n\tpmu = uncore_event_to_pmu(event);\n\t \n\tif (pmu->func_id < 0)\n\t\treturn -ENOENT;\n\n\t \n\tif (hwc->sample_period)\n\t\treturn -EINVAL;\n\n\t \n\tif (event->cpu < 0)\n\t\treturn -EINVAL;\n\tbox = uncore_pmu_to_box(pmu, event->cpu);\n\tif (!box || box->cpu < 0)\n\t\treturn -EINVAL;\n\tevent->cpu = box->cpu;\n\tevent->pmu_private = box;\n\n\tevent->event_caps |= PERF_EV_CAP_READ_ACTIVE_PKG;\n\n\tevent->hw.idx = -1;\n\tevent->hw.last_tag = ~0ULL;\n\tevent->hw.extra_reg.idx = EXTRA_REG_NONE;\n\tevent->hw.branch_reg.idx = EXTRA_REG_NONE;\n\n\tif (event->attr.config == UNCORE_FIXED_EVENT) {\n\t\t \n\t\tif (!pmu->type->fixed_ctl)\n\t\t\treturn -EINVAL;\n\t\t \n\t\tif (pmu->type->single_fixed && pmu->pmu_idx > 0)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\thwc->config = 0ULL;\n\t} else if (is_freerunning_event(event)) {\n\t\thwc->config = event->attr.config;\n\t\tif (!check_valid_freerunning_event(box, event))\n\t\t\treturn -EINVAL;\n\t\tevent->hw.idx = UNCORE_PMC_IDX_FREERUNNING;\n\t\t \n\t\tevent->hw.event_base = uncore_freerunning_counter(box, event);\n\t} else {\n\t\thwc->config = event->attr.config &\n\t\t\t      (pmu->type->event_mask | ((u64)pmu->type->event_mask_ext << 32));\n\t\tif (pmu->type->ops->hw_config) {\n\t\t\tret = pmu->type->ops->hw_config(box, event);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\tif (event->group_leader != event)\n\t\tret = uncore_validate_group(pmu, event);\n\telse\n\t\tret = 0;\n\n\treturn ret;\n}\n\nstatic void uncore_pmu_enable(struct pmu *pmu)\n{\n\tstruct intel_uncore_pmu *uncore_pmu;\n\tstruct intel_uncore_box *box;\n\n\tuncore_pmu = container_of(pmu, struct intel_uncore_pmu, pmu);\n\n\tbox = uncore_pmu_to_box(uncore_pmu, smp_processor_id());\n\tif (!box)\n\t\treturn;\n\n\tif (uncore_pmu->type->ops->enable_box)\n\t\tuncore_pmu->type->ops->enable_box(box);\n}\n\nstatic void uncore_pmu_disable(struct pmu *pmu)\n{\n\tstruct intel_uncore_pmu *uncore_pmu;\n\tstruct intel_uncore_box *box;\n\n\tuncore_pmu = container_of(pmu, struct intel_uncore_pmu, pmu);\n\n\tbox = uncore_pmu_to_box(uncore_pmu, smp_processor_id());\n\tif (!box)\n\t\treturn;\n\n\tif (uncore_pmu->type->ops->disable_box)\n\t\tuncore_pmu->type->ops->disable_box(box);\n}\n\nstatic ssize_t uncore_get_attr_cpumask(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\treturn cpumap_print_to_pagebuf(true, buf, &uncore_cpu_mask);\n}\n\nstatic DEVICE_ATTR(cpumask, S_IRUGO, uncore_get_attr_cpumask, NULL);\n\nstatic struct attribute *uncore_pmu_attrs[] = {\n\t&dev_attr_cpumask.attr,\n\tNULL,\n};\n\nstatic const struct attribute_group uncore_pmu_attr_group = {\n\t.attrs = uncore_pmu_attrs,\n};\n\nstatic inline int uncore_get_box_id(struct intel_uncore_type *type,\n\t\t\t\t    struct intel_uncore_pmu *pmu)\n{\n\treturn type->box_ids ? type->box_ids[pmu->pmu_idx] : pmu->pmu_idx;\n}\n\nvoid uncore_get_alias_name(char *pmu_name, struct intel_uncore_pmu *pmu)\n{\n\tstruct intel_uncore_type *type = pmu->type;\n\n\tif (type->num_boxes == 1)\n\t\tsprintf(pmu_name, \"uncore_type_%u\", type->type_id);\n\telse {\n\t\tsprintf(pmu_name, \"uncore_type_%u_%d\",\n\t\t\ttype->type_id, uncore_get_box_id(type, pmu));\n\t}\n}\n\nstatic void uncore_get_pmu_name(struct intel_uncore_pmu *pmu)\n{\n\tstruct intel_uncore_type *type = pmu->type;\n\n\t \n\tif (!type->name) {\n\t\tuncore_get_alias_name(pmu->name, pmu);\n\t\treturn;\n\t}\n\n\tif (type->num_boxes == 1) {\n\t\tif (strlen(type->name) > 0)\n\t\t\tsprintf(pmu->name, \"uncore_%s\", type->name);\n\t\telse\n\t\t\tsprintf(pmu->name, \"uncore\");\n\t} else {\n\t\t \n\t\tsprintf(pmu->name, \"uncore_%s_%d\", type->name,\n\t\t\tuncore_get_box_id(type, pmu));\n\t}\n}\n\nstatic int uncore_pmu_register(struct intel_uncore_pmu *pmu)\n{\n\tint ret;\n\n\tif (!pmu->type->pmu) {\n\t\tpmu->pmu = (struct pmu) {\n\t\t\t.attr_groups\t= pmu->type->attr_groups,\n\t\t\t.task_ctx_nr\t= perf_invalid_context,\n\t\t\t.pmu_enable\t= uncore_pmu_enable,\n\t\t\t.pmu_disable\t= uncore_pmu_disable,\n\t\t\t.event_init\t= uncore_pmu_event_init,\n\t\t\t.add\t\t= uncore_pmu_event_add,\n\t\t\t.del\t\t= uncore_pmu_event_del,\n\t\t\t.start\t\t= uncore_pmu_event_start,\n\t\t\t.stop\t\t= uncore_pmu_event_stop,\n\t\t\t.read\t\t= uncore_pmu_event_read,\n\t\t\t.module\t\t= THIS_MODULE,\n\t\t\t.capabilities\t= PERF_PMU_CAP_NO_EXCLUDE,\n\t\t\t.attr_update\t= pmu->type->attr_update,\n\t\t};\n\t} else {\n\t\tpmu->pmu = *pmu->type->pmu;\n\t\tpmu->pmu.attr_groups = pmu->type->attr_groups;\n\t\tpmu->pmu.attr_update = pmu->type->attr_update;\n\t}\n\n\tuncore_get_pmu_name(pmu);\n\n\tret = perf_pmu_register(&pmu->pmu, pmu->name, -1);\n\tif (!ret)\n\t\tpmu->registered = true;\n\treturn ret;\n}\n\nstatic void uncore_pmu_unregister(struct intel_uncore_pmu *pmu)\n{\n\tif (!pmu->registered)\n\t\treturn;\n\tperf_pmu_unregister(&pmu->pmu);\n\tpmu->registered = false;\n}\n\nstatic void uncore_free_boxes(struct intel_uncore_pmu *pmu)\n{\n\tint die;\n\n\tfor (die = 0; die < uncore_max_dies(); die++)\n\t\tkfree(pmu->boxes[die]);\n\tkfree(pmu->boxes);\n}\n\nstatic void uncore_type_exit(struct intel_uncore_type *type)\n{\n\tstruct intel_uncore_pmu *pmu = type->pmus;\n\tint i;\n\n\tif (type->cleanup_mapping)\n\t\ttype->cleanup_mapping(type);\n\n\tif (pmu) {\n\t\tfor (i = 0; i < type->num_boxes; i++, pmu++) {\n\t\t\tuncore_pmu_unregister(pmu);\n\t\t\tuncore_free_boxes(pmu);\n\t\t}\n\t\tkfree(type->pmus);\n\t\ttype->pmus = NULL;\n\t}\n\tif (type->box_ids) {\n\t\tkfree(type->box_ids);\n\t\ttype->box_ids = NULL;\n\t}\n\tkfree(type->events_group);\n\ttype->events_group = NULL;\n}\n\nstatic void uncore_types_exit(struct intel_uncore_type **types)\n{\n\tfor (; *types; types++)\n\t\tuncore_type_exit(*types);\n}\n\nstatic int __init uncore_type_init(struct intel_uncore_type *type, bool setid)\n{\n\tstruct intel_uncore_pmu *pmus;\n\tsize_t size;\n\tint i, j;\n\n\tpmus = kcalloc(type->num_boxes, sizeof(*pmus), GFP_KERNEL);\n\tif (!pmus)\n\t\treturn -ENOMEM;\n\n\tsize = uncore_max_dies() * sizeof(struct intel_uncore_box *);\n\n\tfor (i = 0; i < type->num_boxes; i++) {\n\t\tpmus[i].func_id\t= setid ? i : -1;\n\t\tpmus[i].pmu_idx\t= i;\n\t\tpmus[i].type\t= type;\n\t\tpmus[i].boxes\t= kzalloc(size, GFP_KERNEL);\n\t\tif (!pmus[i].boxes)\n\t\t\tgoto err;\n\t}\n\n\ttype->pmus = pmus;\n\ttype->unconstrainted = (struct event_constraint)\n\t\t__EVENT_CONSTRAINT(0, (1ULL << type->num_counters) - 1,\n\t\t\t\t0, type->num_counters, 0, 0);\n\n\tif (type->event_descs) {\n\t\tstruct {\n\t\t\tstruct attribute_group group;\n\t\t\tstruct attribute *attrs[];\n\t\t} *attr_group;\n\t\tfor (i = 0; type->event_descs[i].attr.attr.name; i++);\n\n\t\tattr_group = kzalloc(struct_size(attr_group, attrs, i + 1),\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!attr_group)\n\t\t\tgoto err;\n\n\t\tattr_group->group.name = \"events\";\n\t\tattr_group->group.attrs = attr_group->attrs;\n\n\t\tfor (j = 0; j < i; j++)\n\t\t\tattr_group->attrs[j] = &type->event_descs[j].attr.attr;\n\n\t\ttype->events_group = &attr_group->group;\n\t}\n\n\ttype->pmu_group = &uncore_pmu_attr_group;\n\n\tif (type->set_mapping)\n\t\ttype->set_mapping(type);\n\n\treturn 0;\n\nerr:\n\tfor (i = 0; i < type->num_boxes; i++)\n\t\tkfree(pmus[i].boxes);\n\tkfree(pmus);\n\n\treturn -ENOMEM;\n}\n\nstatic int __init\nuncore_types_init(struct intel_uncore_type **types, bool setid)\n{\n\tint ret;\n\n\tfor (; *types; types++) {\n\t\tret = uncore_type_init(*types, setid);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\n\n \nstatic int uncore_pci_get_dev_die_info(struct pci_dev *pdev, int *die)\n{\n\t*die = uncore_pcibus_to_dieid(pdev->bus);\n\tif (*die < 0)\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic struct intel_uncore_pmu *\nuncore_pci_find_dev_pmu_from_types(struct pci_dev *pdev)\n{\n\tstruct intel_uncore_type **types = uncore_pci_uncores;\n\tstruct intel_uncore_type *type;\n\tu64 box_ctl;\n\tint i, die;\n\n\tfor (; *types; types++) {\n\t\ttype = *types;\n\t\tfor (die = 0; die < __uncore_max_dies; die++) {\n\t\t\tfor (i = 0; i < type->num_boxes; i++) {\n\t\t\t\tif (!type->box_ctls[die])\n\t\t\t\t\tcontinue;\n\t\t\t\tbox_ctl = type->box_ctls[die] + type->pci_offsets[i];\n\t\t\t\tif (pdev->devfn == UNCORE_DISCOVERY_PCI_DEVFN(box_ctl) &&\n\t\t\t\t    pdev->bus->number == UNCORE_DISCOVERY_PCI_BUS(box_ctl) &&\n\t\t\t\t    pci_domain_nr(pdev->bus) == UNCORE_DISCOVERY_PCI_DOMAIN(box_ctl))\n\t\t\t\t\treturn &type->pmus[i];\n\t\t\t}\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\n \nstatic struct intel_uncore_pmu *\nuncore_pci_find_dev_pmu(struct pci_dev *pdev, const struct pci_device_id *ids)\n{\n\tstruct intel_uncore_pmu *pmu = NULL;\n\tstruct intel_uncore_type *type;\n\tkernel_ulong_t data;\n\tunsigned int devfn;\n\n\tif (!ids)\n\t\treturn uncore_pci_find_dev_pmu_from_types(pdev);\n\n\twhile (ids && ids->vendor) {\n\t\tif ((ids->vendor == pdev->vendor) &&\n\t\t    (ids->device == pdev->device)) {\n\t\t\tdata = ids->driver_data;\n\t\t\tdevfn = PCI_DEVFN(UNCORE_PCI_DEV_DEV(data),\n\t\t\t\t\t  UNCORE_PCI_DEV_FUNC(data));\n\t\t\tif (devfn == pdev->devfn) {\n\t\t\t\ttype = uncore_pci_uncores[UNCORE_PCI_DEV_TYPE(data)];\n\t\t\t\tpmu = &type->pmus[UNCORE_PCI_DEV_IDX(data)];\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tids++;\n\t}\n\treturn pmu;\n}\n\n \nstatic int uncore_pci_pmu_register(struct pci_dev *pdev,\n\t\t\t\t   struct intel_uncore_type *type,\n\t\t\t\t   struct intel_uncore_pmu *pmu,\n\t\t\t\t   int die)\n{\n\tstruct intel_uncore_box *box;\n\tint ret;\n\n\tif (WARN_ON_ONCE(pmu->boxes[die] != NULL))\n\t\treturn -EINVAL;\n\n\tbox = uncore_alloc_box(type, NUMA_NO_NODE);\n\tif (!box)\n\t\treturn -ENOMEM;\n\n\tif (pmu->func_id < 0)\n\t\tpmu->func_id = pdev->devfn;\n\telse\n\t\tWARN_ON_ONCE(pmu->func_id != pdev->devfn);\n\n\tatomic_inc(&box->refcnt);\n\tbox->dieid = die;\n\tbox->pci_dev = pdev;\n\tbox->pmu = pmu;\n\tuncore_box_init(box);\n\n\tpmu->boxes[die] = box;\n\tif (atomic_inc_return(&pmu->activeboxes) > 1)\n\t\treturn 0;\n\n\t \n\tret = uncore_pmu_register(pmu);\n\tif (ret) {\n\t\tpmu->boxes[die] = NULL;\n\t\tuncore_box_exit(box);\n\t\tkfree(box);\n\t}\n\treturn ret;\n}\n\n \nstatic int uncore_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)\n{\n\tstruct intel_uncore_type *type;\n\tstruct intel_uncore_pmu *pmu = NULL;\n\tint die, ret;\n\n\tret = uncore_pci_get_dev_die_info(pdev, &die);\n\tif (ret)\n\t\treturn ret;\n\n\tif (UNCORE_PCI_DEV_TYPE(id->driver_data) == UNCORE_EXTRA_PCI_DEV) {\n\t\tint idx = UNCORE_PCI_DEV_IDX(id->driver_data);\n\n\t\tuncore_extra_pci_dev[die].dev[idx] = pdev;\n\t\tpci_set_drvdata(pdev, NULL);\n\t\treturn 0;\n\t}\n\n\ttype = uncore_pci_uncores[UNCORE_PCI_DEV_TYPE(id->driver_data)];\n\n\t \n\tif (id->driver_data & ~0xffff) {\n\t\tstruct pci_driver *pci_drv = to_pci_driver(pdev->dev.driver);\n\n\t\tpmu = uncore_pci_find_dev_pmu(pdev, pci_drv->id_table);\n\t\tif (pmu == NULL)\n\t\t\treturn -ENODEV;\n\t} else {\n\t\t \n\t\tpmu = &type->pmus[UNCORE_PCI_DEV_IDX(id->driver_data)];\n\t}\n\n\tret = uncore_pci_pmu_register(pdev, type, pmu, die);\n\n\tpci_set_drvdata(pdev, pmu->boxes[die]);\n\n\treturn ret;\n}\n\n \nstatic void uncore_pci_pmu_unregister(struct intel_uncore_pmu *pmu, int die)\n{\n\tstruct intel_uncore_box *box = pmu->boxes[die];\n\n\tpmu->boxes[die] = NULL;\n\tif (atomic_dec_return(&pmu->activeboxes) == 0)\n\t\tuncore_pmu_unregister(pmu);\n\tuncore_box_exit(box);\n\tkfree(box);\n}\n\nstatic void uncore_pci_remove(struct pci_dev *pdev)\n{\n\tstruct intel_uncore_box *box;\n\tstruct intel_uncore_pmu *pmu;\n\tint i, die;\n\n\tif (uncore_pci_get_dev_die_info(pdev, &die))\n\t\treturn;\n\n\tbox = pci_get_drvdata(pdev);\n\tif (!box) {\n\t\tfor (i = 0; i < UNCORE_EXTRA_PCI_DEV_MAX; i++) {\n\t\t\tif (uncore_extra_pci_dev[die].dev[i] == pdev) {\n\t\t\t\tuncore_extra_pci_dev[die].dev[i] = NULL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tWARN_ON_ONCE(i >= UNCORE_EXTRA_PCI_DEV_MAX);\n\t\treturn;\n\t}\n\n\tpmu = box->pmu;\n\n\tpci_set_drvdata(pdev, NULL);\n\n\tuncore_pci_pmu_unregister(pmu, die);\n}\n\nstatic int uncore_bus_notify(struct notifier_block *nb,\n\t\t\t     unsigned long action, void *data,\n\t\t\t     const struct pci_device_id *ids)\n{\n\tstruct device *dev = data;\n\tstruct pci_dev *pdev = to_pci_dev(dev);\n\tstruct intel_uncore_pmu *pmu;\n\tint die;\n\n\t \n\tif (action != BUS_NOTIFY_DEL_DEVICE)\n\t\treturn NOTIFY_DONE;\n\n\tpmu = uncore_pci_find_dev_pmu(pdev, ids);\n\tif (!pmu)\n\t\treturn NOTIFY_DONE;\n\n\tif (uncore_pci_get_dev_die_info(pdev, &die))\n\t\treturn NOTIFY_DONE;\n\n\tuncore_pci_pmu_unregister(pmu, die);\n\n\treturn NOTIFY_OK;\n}\n\nstatic int uncore_pci_sub_bus_notify(struct notifier_block *nb,\n\t\t\t\t     unsigned long action, void *data)\n{\n\treturn uncore_bus_notify(nb, action, data,\n\t\t\t\t uncore_pci_sub_driver->id_table);\n}\n\nstatic struct notifier_block uncore_pci_sub_notifier = {\n\t.notifier_call = uncore_pci_sub_bus_notify,\n};\n\nstatic void uncore_pci_sub_driver_init(void)\n{\n\tconst struct pci_device_id *ids = uncore_pci_sub_driver->id_table;\n\tstruct intel_uncore_type *type;\n\tstruct intel_uncore_pmu *pmu;\n\tstruct pci_dev *pci_sub_dev;\n\tbool notify = false;\n\tunsigned int devfn;\n\tint die;\n\n\twhile (ids && ids->vendor) {\n\t\tpci_sub_dev = NULL;\n\t\ttype = uncore_pci_uncores[UNCORE_PCI_DEV_TYPE(ids->driver_data)];\n\t\t \n\t\twhile ((pci_sub_dev = pci_get_device(PCI_VENDOR_ID_INTEL,\n\t\t\t\t\t\t     ids->device, pci_sub_dev))) {\n\t\t\tdevfn = PCI_DEVFN(UNCORE_PCI_DEV_DEV(ids->driver_data),\n\t\t\t\t\t  UNCORE_PCI_DEV_FUNC(ids->driver_data));\n\t\t\tif (devfn != pci_sub_dev->devfn)\n\t\t\t\tcontinue;\n\n\t\t\tpmu = &type->pmus[UNCORE_PCI_DEV_IDX(ids->driver_data)];\n\t\t\tif (!pmu)\n\t\t\t\tcontinue;\n\n\t\t\tif (uncore_pci_get_dev_die_info(pci_sub_dev, &die))\n\t\t\t\tcontinue;\n\n\t\t\tif (!uncore_pci_pmu_register(pci_sub_dev, type, pmu,\n\t\t\t\t\t\t     die))\n\t\t\t\tnotify = true;\n\t\t}\n\t\tids++;\n\t}\n\n\tif (notify && bus_register_notifier(&pci_bus_type, &uncore_pci_sub_notifier))\n\t\tnotify = false;\n\n\tif (!notify)\n\t\tuncore_pci_sub_driver = NULL;\n}\n\nstatic int uncore_pci_bus_notify(struct notifier_block *nb,\n\t\t\t\t     unsigned long action, void *data)\n{\n\treturn uncore_bus_notify(nb, action, data, NULL);\n}\n\nstatic struct notifier_block uncore_pci_notifier = {\n\t.notifier_call = uncore_pci_bus_notify,\n};\n\n\nstatic void uncore_pci_pmus_register(void)\n{\n\tstruct intel_uncore_type **types = uncore_pci_uncores;\n\tstruct intel_uncore_type *type;\n\tstruct intel_uncore_pmu *pmu;\n\tstruct pci_dev *pdev;\n\tu64 box_ctl;\n\tint i, die;\n\n\tfor (; *types; types++) {\n\t\ttype = *types;\n\t\tfor (die = 0; die < __uncore_max_dies; die++) {\n\t\t\tfor (i = 0; i < type->num_boxes; i++) {\n\t\t\t\tif (!type->box_ctls[die])\n\t\t\t\t\tcontinue;\n\t\t\t\tbox_ctl = type->box_ctls[die] + type->pci_offsets[i];\n\t\t\t\tpdev = pci_get_domain_bus_and_slot(UNCORE_DISCOVERY_PCI_DOMAIN(box_ctl),\n\t\t\t\t\t\t\t\t   UNCORE_DISCOVERY_PCI_BUS(box_ctl),\n\t\t\t\t\t\t\t\t   UNCORE_DISCOVERY_PCI_DEVFN(box_ctl));\n\t\t\t\tif (!pdev)\n\t\t\t\t\tcontinue;\n\t\t\t\tpmu = &type->pmus[i];\n\n\t\t\t\tuncore_pci_pmu_register(pdev, type, pmu, die);\n\t\t\t}\n\t\t}\n\t}\n\n\tbus_register_notifier(&pci_bus_type, &uncore_pci_notifier);\n}\n\nstatic int __init uncore_pci_init(void)\n{\n\tsize_t size;\n\tint ret;\n\n\tsize = uncore_max_dies() * sizeof(struct pci_extra_dev);\n\tuncore_extra_pci_dev = kzalloc(size, GFP_KERNEL);\n\tif (!uncore_extra_pci_dev) {\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\tret = uncore_types_init(uncore_pci_uncores, false);\n\tif (ret)\n\t\tgoto errtype;\n\n\tif (uncore_pci_driver) {\n\t\tuncore_pci_driver->probe = uncore_pci_probe;\n\t\tuncore_pci_driver->remove = uncore_pci_remove;\n\n\t\tret = pci_register_driver(uncore_pci_driver);\n\t\tif (ret)\n\t\t\tgoto errtype;\n\t} else\n\t\tuncore_pci_pmus_register();\n\n\tif (uncore_pci_sub_driver)\n\t\tuncore_pci_sub_driver_init();\n\n\tpcidrv_registered = true;\n\treturn 0;\n\nerrtype:\n\tuncore_types_exit(uncore_pci_uncores);\n\tkfree(uncore_extra_pci_dev);\n\tuncore_extra_pci_dev = NULL;\n\tuncore_free_pcibus_map();\nerr:\n\tuncore_pci_uncores = empty_uncore;\n\treturn ret;\n}\n\nstatic void uncore_pci_exit(void)\n{\n\tif (pcidrv_registered) {\n\t\tpcidrv_registered = false;\n\t\tif (uncore_pci_sub_driver)\n\t\t\tbus_unregister_notifier(&pci_bus_type, &uncore_pci_sub_notifier);\n\t\tif (uncore_pci_driver)\n\t\t\tpci_unregister_driver(uncore_pci_driver);\n\t\telse\n\t\t\tbus_unregister_notifier(&pci_bus_type, &uncore_pci_notifier);\n\t\tuncore_types_exit(uncore_pci_uncores);\n\t\tkfree(uncore_extra_pci_dev);\n\t\tuncore_free_pcibus_map();\n\t}\n}\n\nstatic void uncore_change_type_ctx(struct intel_uncore_type *type, int old_cpu,\n\t\t\t\t   int new_cpu)\n{\n\tstruct intel_uncore_pmu *pmu = type->pmus;\n\tstruct intel_uncore_box *box;\n\tint i, die;\n\n\tdie = topology_logical_die_id(old_cpu < 0 ? new_cpu : old_cpu);\n\tfor (i = 0; i < type->num_boxes; i++, pmu++) {\n\t\tbox = pmu->boxes[die];\n\t\tif (!box)\n\t\t\tcontinue;\n\n\t\tif (old_cpu < 0) {\n\t\t\tWARN_ON_ONCE(box->cpu != -1);\n\t\t\tbox->cpu = new_cpu;\n\t\t\tcontinue;\n\t\t}\n\n\t\tWARN_ON_ONCE(box->cpu != old_cpu);\n\t\tbox->cpu = -1;\n\t\tif (new_cpu < 0)\n\t\t\tcontinue;\n\n\t\tuncore_pmu_cancel_hrtimer(box);\n\t\tperf_pmu_migrate_context(&pmu->pmu, old_cpu, new_cpu);\n\t\tbox->cpu = new_cpu;\n\t}\n}\n\nstatic void uncore_change_context(struct intel_uncore_type **uncores,\n\t\t\t\t  int old_cpu, int new_cpu)\n{\n\tfor (; *uncores; uncores++)\n\t\tuncore_change_type_ctx(*uncores, old_cpu, new_cpu);\n}\n\nstatic void uncore_box_unref(struct intel_uncore_type **types, int id)\n{\n\tstruct intel_uncore_type *type;\n\tstruct intel_uncore_pmu *pmu;\n\tstruct intel_uncore_box *box;\n\tint i;\n\n\tfor (; *types; types++) {\n\t\ttype = *types;\n\t\tpmu = type->pmus;\n\t\tfor (i = 0; i < type->num_boxes; i++, pmu++) {\n\t\t\tbox = pmu->boxes[id];\n\t\t\tif (box && atomic_dec_return(&box->refcnt) == 0)\n\t\t\t\tuncore_box_exit(box);\n\t\t}\n\t}\n}\n\nstatic int uncore_event_cpu_offline(unsigned int cpu)\n{\n\tint die, target;\n\n\t \n\tif (!cpumask_test_and_clear_cpu(cpu, &uncore_cpu_mask))\n\t\tgoto unref;\n\t \n\ttarget = cpumask_any_but(topology_die_cpumask(cpu), cpu);\n\n\t \n\tif (target < nr_cpu_ids)\n\t\tcpumask_set_cpu(target, &uncore_cpu_mask);\n\telse\n\t\ttarget = -1;\n\n\tuncore_change_context(uncore_msr_uncores, cpu, target);\n\tuncore_change_context(uncore_mmio_uncores, cpu, target);\n\tuncore_change_context(uncore_pci_uncores, cpu, target);\n\nunref:\n\t \n\tdie = topology_logical_die_id(cpu);\n\tuncore_box_unref(uncore_msr_uncores, die);\n\tuncore_box_unref(uncore_mmio_uncores, die);\n\treturn 0;\n}\n\nstatic int allocate_boxes(struct intel_uncore_type **types,\n\t\t\t unsigned int die, unsigned int cpu)\n{\n\tstruct intel_uncore_box *box, *tmp;\n\tstruct intel_uncore_type *type;\n\tstruct intel_uncore_pmu *pmu;\n\tLIST_HEAD(allocated);\n\tint i;\n\n\t \n\tfor (; *types; types++) {\n\t\ttype = *types;\n\t\tpmu = type->pmus;\n\t\tfor (i = 0; i < type->num_boxes; i++, pmu++) {\n\t\t\tif (pmu->boxes[die])\n\t\t\t\tcontinue;\n\t\t\tbox = uncore_alloc_box(type, cpu_to_node(cpu));\n\t\t\tif (!box)\n\t\t\t\tgoto cleanup;\n\t\t\tbox->pmu = pmu;\n\t\t\tbox->dieid = die;\n\t\t\tlist_add(&box->active_list, &allocated);\n\t\t}\n\t}\n\t \n\tlist_for_each_entry_safe(box, tmp, &allocated, active_list) {\n\t\tlist_del_init(&box->active_list);\n\t\tbox->pmu->boxes[die] = box;\n\t}\n\treturn 0;\n\ncleanup:\n\tlist_for_each_entry_safe(box, tmp, &allocated, active_list) {\n\t\tlist_del_init(&box->active_list);\n\t\tkfree(box);\n\t}\n\treturn -ENOMEM;\n}\n\nstatic int uncore_box_ref(struct intel_uncore_type **types,\n\t\t\t  int id, unsigned int cpu)\n{\n\tstruct intel_uncore_type *type;\n\tstruct intel_uncore_pmu *pmu;\n\tstruct intel_uncore_box *box;\n\tint i, ret;\n\n\tret = allocate_boxes(types, id, cpu);\n\tif (ret)\n\t\treturn ret;\n\n\tfor (; *types; types++) {\n\t\ttype = *types;\n\t\tpmu = type->pmus;\n\t\tfor (i = 0; i < type->num_boxes; i++, pmu++) {\n\t\t\tbox = pmu->boxes[id];\n\t\t\tif (box && atomic_inc_return(&box->refcnt) == 1)\n\t\t\t\tuncore_box_init(box);\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic int uncore_event_cpu_online(unsigned int cpu)\n{\n\tint die, target, msr_ret, mmio_ret;\n\n\tdie = topology_logical_die_id(cpu);\n\tmsr_ret = uncore_box_ref(uncore_msr_uncores, die, cpu);\n\tmmio_ret = uncore_box_ref(uncore_mmio_uncores, die, cpu);\n\tif (msr_ret && mmio_ret)\n\t\treturn -ENOMEM;\n\n\t \n\ttarget = cpumask_any_and(&uncore_cpu_mask, topology_die_cpumask(cpu));\n\tif (target < nr_cpu_ids)\n\t\treturn 0;\n\n\tcpumask_set_cpu(cpu, &uncore_cpu_mask);\n\n\tif (!msr_ret)\n\t\tuncore_change_context(uncore_msr_uncores, -1, cpu);\n\tif (!mmio_ret)\n\t\tuncore_change_context(uncore_mmio_uncores, -1, cpu);\n\tuncore_change_context(uncore_pci_uncores, -1, cpu);\n\treturn 0;\n}\n\nstatic int __init type_pmu_register(struct intel_uncore_type *type)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < type->num_boxes; i++) {\n\t\tret = uncore_pmu_register(&type->pmus[i]);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic int __init uncore_msr_pmus_register(void)\n{\n\tstruct intel_uncore_type **types = uncore_msr_uncores;\n\tint ret;\n\n\tfor (; *types; types++) {\n\t\tret = type_pmu_register(*types);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic int __init uncore_cpu_init(void)\n{\n\tint ret;\n\n\tret = uncore_types_init(uncore_msr_uncores, true);\n\tif (ret)\n\t\tgoto err;\n\n\tret = uncore_msr_pmus_register();\n\tif (ret)\n\t\tgoto err;\n\treturn 0;\nerr:\n\tuncore_types_exit(uncore_msr_uncores);\n\tuncore_msr_uncores = empty_uncore;\n\treturn ret;\n}\n\nstatic int __init uncore_mmio_init(void)\n{\n\tstruct intel_uncore_type **types = uncore_mmio_uncores;\n\tint ret;\n\n\tret = uncore_types_init(types, true);\n\tif (ret)\n\t\tgoto err;\n\n\tfor (; *types; types++) {\n\t\tret = type_pmu_register(*types);\n\t\tif (ret)\n\t\t\tgoto err;\n\t}\n\treturn 0;\nerr:\n\tuncore_types_exit(uncore_mmio_uncores);\n\tuncore_mmio_uncores = empty_uncore;\n\treturn ret;\n}\n\nstruct intel_uncore_init_fun {\n\tvoid\t(*cpu_init)(void);\n\tint\t(*pci_init)(void);\n\tvoid\t(*mmio_init)(void);\n\t \n\tbool\tuse_discovery;\n\t \n\tint\t*uncore_units_ignore;\n};\n\nstatic const struct intel_uncore_init_fun nhm_uncore_init __initconst = {\n\t.cpu_init = nhm_uncore_cpu_init,\n};\n\nstatic const struct intel_uncore_init_fun snb_uncore_init __initconst = {\n\t.cpu_init = snb_uncore_cpu_init,\n\t.pci_init = snb_uncore_pci_init,\n};\n\nstatic const struct intel_uncore_init_fun ivb_uncore_init __initconst = {\n\t.cpu_init = snb_uncore_cpu_init,\n\t.pci_init = ivb_uncore_pci_init,\n};\n\nstatic const struct intel_uncore_init_fun hsw_uncore_init __initconst = {\n\t.cpu_init = snb_uncore_cpu_init,\n\t.pci_init = hsw_uncore_pci_init,\n};\n\nstatic const struct intel_uncore_init_fun bdw_uncore_init __initconst = {\n\t.cpu_init = snb_uncore_cpu_init,\n\t.pci_init = bdw_uncore_pci_init,\n};\n\nstatic const struct intel_uncore_init_fun snbep_uncore_init __initconst = {\n\t.cpu_init = snbep_uncore_cpu_init,\n\t.pci_init = snbep_uncore_pci_init,\n};\n\nstatic const struct intel_uncore_init_fun nhmex_uncore_init __initconst = {\n\t.cpu_init = nhmex_uncore_cpu_init,\n};\n\nstatic const struct intel_uncore_init_fun ivbep_uncore_init __initconst = {\n\t.cpu_init = ivbep_uncore_cpu_init,\n\t.pci_init = ivbep_uncore_pci_init,\n};\n\nstatic const struct intel_uncore_init_fun hswep_uncore_init __initconst = {\n\t.cpu_init = hswep_uncore_cpu_init,\n\t.pci_init = hswep_uncore_pci_init,\n};\n\nstatic const struct intel_uncore_init_fun bdx_uncore_init __initconst = {\n\t.cpu_init = bdx_uncore_cpu_init,\n\t.pci_init = bdx_uncore_pci_init,\n};\n\nstatic const struct intel_uncore_init_fun knl_uncore_init __initconst = {\n\t.cpu_init = knl_uncore_cpu_init,\n\t.pci_init = knl_uncore_pci_init,\n};\n\nstatic const struct intel_uncore_init_fun skl_uncore_init __initconst = {\n\t.cpu_init = skl_uncore_cpu_init,\n\t.pci_init = skl_uncore_pci_init,\n};\n\nstatic const struct intel_uncore_init_fun skx_uncore_init __initconst = {\n\t.cpu_init = skx_uncore_cpu_init,\n\t.pci_init = skx_uncore_pci_init,\n};\n\nstatic const struct intel_uncore_init_fun icl_uncore_init __initconst = {\n\t.cpu_init = icl_uncore_cpu_init,\n\t.pci_init = skl_uncore_pci_init,\n};\n\nstatic const struct intel_uncore_init_fun tgl_uncore_init __initconst = {\n\t.cpu_init = tgl_uncore_cpu_init,\n\t.mmio_init = tgl_uncore_mmio_init,\n};\n\nstatic const struct intel_uncore_init_fun tgl_l_uncore_init __initconst = {\n\t.cpu_init = tgl_uncore_cpu_init,\n\t.mmio_init = tgl_l_uncore_mmio_init,\n};\n\nstatic const struct intel_uncore_init_fun rkl_uncore_init __initconst = {\n\t.cpu_init = tgl_uncore_cpu_init,\n\t.pci_init = skl_uncore_pci_init,\n};\n\nstatic const struct intel_uncore_init_fun adl_uncore_init __initconst = {\n\t.cpu_init = adl_uncore_cpu_init,\n\t.mmio_init = adl_uncore_mmio_init,\n};\n\nstatic const struct intel_uncore_init_fun mtl_uncore_init __initconst = {\n\t.cpu_init = mtl_uncore_cpu_init,\n\t.mmio_init = adl_uncore_mmio_init,\n};\n\nstatic const struct intel_uncore_init_fun icx_uncore_init __initconst = {\n\t.cpu_init = icx_uncore_cpu_init,\n\t.pci_init = icx_uncore_pci_init,\n\t.mmio_init = icx_uncore_mmio_init,\n};\n\nstatic const struct intel_uncore_init_fun snr_uncore_init __initconst = {\n\t.cpu_init = snr_uncore_cpu_init,\n\t.pci_init = snr_uncore_pci_init,\n\t.mmio_init = snr_uncore_mmio_init,\n};\n\nstatic const struct intel_uncore_init_fun spr_uncore_init __initconst = {\n\t.cpu_init = spr_uncore_cpu_init,\n\t.pci_init = spr_uncore_pci_init,\n\t.mmio_init = spr_uncore_mmio_init,\n\t.use_discovery = true,\n\t.uncore_units_ignore = spr_uncore_units_ignore,\n};\n\nstatic const struct intel_uncore_init_fun generic_uncore_init __initconst = {\n\t.cpu_init = intel_uncore_generic_uncore_cpu_init,\n\t.pci_init = intel_uncore_generic_uncore_pci_init,\n\t.mmio_init = intel_uncore_generic_uncore_mmio_init,\n};\n\nstatic const struct x86_cpu_id intel_uncore_match[] __initconst = {\n\tX86_MATCH_INTEL_FAM6_MODEL(NEHALEM_EP,\t\t&nhm_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(NEHALEM,\t\t&nhm_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(WESTMERE,\t\t&nhm_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(WESTMERE_EP,\t\t&nhm_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(SANDYBRIDGE,\t\t&snb_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(IVYBRIDGE,\t\t&ivb_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(HASWELL,\t\t&hsw_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(HASWELL_L,\t\t&hsw_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(HASWELL_G,\t\t&hsw_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(BROADWELL,\t\t&bdw_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(BROADWELL_G,\t\t&bdw_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(SANDYBRIDGE_X,\t&snbep_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(NEHALEM_EX,\t\t&nhmex_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(WESTMERE_EX,\t\t&nhmex_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(IVYBRIDGE_X,\t\t&ivbep_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(HASWELL_X,\t\t&hswep_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(BROADWELL_X,\t\t&bdx_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(BROADWELL_D,\t\t&bdx_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(XEON_PHI_KNL,\t&knl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(XEON_PHI_KNM,\t&knl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(SKYLAKE,\t\t&skl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(SKYLAKE_L,\t\t&skl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(SKYLAKE_X,\t\t&skx_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(KABYLAKE_L,\t\t&skl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(KABYLAKE,\t\t&skl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(COMETLAKE_L,\t\t&skl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(COMETLAKE,\t\t&skl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(ICELAKE_L,\t\t&icl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(ICELAKE_NNPI,\t&icl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(ICELAKE,\t\t&icl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(ICELAKE_D,\t\t&icx_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(ICELAKE_X,\t\t&icx_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(TIGERLAKE_L,\t\t&tgl_l_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(TIGERLAKE,\t\t&tgl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(ROCKETLAKE,\t\t&rkl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(ALDERLAKE,\t\t&adl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(ALDERLAKE_L,\t\t&adl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(RAPTORLAKE,\t\t&adl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(RAPTORLAKE_P,\t&adl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(RAPTORLAKE_S,\t&adl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(METEORLAKE,\t\t&mtl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(METEORLAKE_L,\t&mtl_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(SAPPHIRERAPIDS_X,\t&spr_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(EMERALDRAPIDS_X,\t&spr_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(ATOM_TREMONT_D,\t&snr_uncore_init),\n\tX86_MATCH_INTEL_FAM6_MODEL(ATOM_GRACEMONT,\t&adl_uncore_init),\n\t{},\n};\nMODULE_DEVICE_TABLE(x86cpu, intel_uncore_match);\n\nstatic int __init intel_uncore_init(void)\n{\n\tconst struct x86_cpu_id *id;\n\tstruct intel_uncore_init_fun *uncore_init;\n\tint pret = 0, cret = 0, mret = 0, ret;\n\n\tif (boot_cpu_has(X86_FEATURE_HYPERVISOR))\n\t\treturn -ENODEV;\n\n\t__uncore_max_dies =\n\t\ttopology_max_packages() * topology_max_die_per_package();\n\n\tid = x86_match_cpu(intel_uncore_match);\n\tif (!id) {\n\t\tif (!uncore_no_discover && intel_uncore_has_discovery_tables(NULL))\n\t\t\tuncore_init = (struct intel_uncore_init_fun *)&generic_uncore_init;\n\t\telse\n\t\t\treturn -ENODEV;\n\t} else {\n\t\tuncore_init = (struct intel_uncore_init_fun *)id->driver_data;\n\t\tif (uncore_no_discover && uncore_init->use_discovery)\n\t\t\treturn -ENODEV;\n\t\tif (uncore_init->use_discovery &&\n\t\t    !intel_uncore_has_discovery_tables(uncore_init->uncore_units_ignore))\n\t\t\treturn -ENODEV;\n\t}\n\n\tif (uncore_init->pci_init) {\n\t\tpret = uncore_init->pci_init();\n\t\tif (!pret)\n\t\t\tpret = uncore_pci_init();\n\t}\n\n\tif (uncore_init->cpu_init) {\n\t\tuncore_init->cpu_init();\n\t\tcret = uncore_cpu_init();\n\t}\n\n\tif (uncore_init->mmio_init) {\n\t\tuncore_init->mmio_init();\n\t\tmret = uncore_mmio_init();\n\t}\n\n\tif (cret && pret && mret) {\n\t\tret = -ENODEV;\n\t\tgoto free_discovery;\n\t}\n\n\t \n\tret = cpuhp_setup_state(CPUHP_AP_PERF_X86_UNCORE_ONLINE,\n\t\t\t\t\"perf/x86/intel/uncore:online\",\n\t\t\t\tuncore_event_cpu_online,\n\t\t\t\tuncore_event_cpu_offline);\n\tif (ret)\n\t\tgoto err;\n\treturn 0;\n\nerr:\n\tuncore_types_exit(uncore_msr_uncores);\n\tuncore_types_exit(uncore_mmio_uncores);\n\tuncore_pci_exit();\nfree_discovery:\n\tintel_uncore_clear_discovery_tables();\n\treturn ret;\n}\nmodule_init(intel_uncore_init);\n\nstatic void __exit intel_uncore_exit(void)\n{\n\tcpuhp_remove_state(CPUHP_AP_PERF_X86_UNCORE_ONLINE);\n\tuncore_types_exit(uncore_msr_uncores);\n\tuncore_types_exit(uncore_mmio_uncores);\n\tuncore_pci_exit();\n\tintel_uncore_clear_discovery_tables();\n}\nmodule_exit(intel_uncore_exit);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}