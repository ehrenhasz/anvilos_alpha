{
  "module_name": "lbr.c",
  "hash_id": "a1a7854ccafbc65c0baa8948327ce4d26f1b8087136831dbfbb86b4c1b76fbab",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/events/intel/lbr.c",
  "human_readable_source": "\n#include <linux/perf_event.h>\n#include <linux/types.h>\n\n#include <asm/perf_event.h>\n#include <asm/msr.h>\n\n#include \"../perf_event.h\"\n\n \n#define LBR_KERNEL_BIT\t\t0  \n#define LBR_USER_BIT\t\t1  \n#define LBR_JCC_BIT\t\t2  \n#define LBR_REL_CALL_BIT\t3  \n#define LBR_IND_CALL_BIT\t4  \n#define LBR_RETURN_BIT\t\t5  \n#define LBR_IND_JMP_BIT\t\t6  \n#define LBR_REL_JMP_BIT\t\t7  \n#define LBR_FAR_BIT\t\t8  \n#define LBR_CALL_STACK_BIT\t9  \n\n \n#define LBR_NO_INFO_BIT\t       63  \n\n#define LBR_KERNEL\t(1 << LBR_KERNEL_BIT)\n#define LBR_USER\t(1 << LBR_USER_BIT)\n#define LBR_JCC\t\t(1 << LBR_JCC_BIT)\n#define LBR_REL_CALL\t(1 << LBR_REL_CALL_BIT)\n#define LBR_IND_CALL\t(1 << LBR_IND_CALL_BIT)\n#define LBR_RETURN\t(1 << LBR_RETURN_BIT)\n#define LBR_REL_JMP\t(1 << LBR_REL_JMP_BIT)\n#define LBR_IND_JMP\t(1 << LBR_IND_JMP_BIT)\n#define LBR_FAR\t\t(1 << LBR_FAR_BIT)\n#define LBR_CALL_STACK\t(1 << LBR_CALL_STACK_BIT)\n#define LBR_NO_INFO\t(1ULL << LBR_NO_INFO_BIT)\n\n#define LBR_PLM (LBR_KERNEL | LBR_USER)\n\n#define LBR_SEL_MASK\t0x3ff\t \n#define LBR_NOT_SUPP\t-1\t \n#define LBR_IGN\t\t0\t \n\n#define LBR_ANY\t\t \\\n\t(LBR_JCC\t|\\\n\t LBR_REL_CALL\t|\\\n\t LBR_IND_CALL\t|\\\n\t LBR_RETURN\t|\\\n\t LBR_REL_JMP\t|\\\n\t LBR_IND_JMP\t|\\\n\t LBR_FAR)\n\n#define LBR_FROM_FLAG_MISPRED\tBIT_ULL(63)\n#define LBR_FROM_FLAG_IN_TX\tBIT_ULL(62)\n#define LBR_FROM_FLAG_ABORT\tBIT_ULL(61)\n\n#define LBR_FROM_SIGNEXT_2MSB\t(BIT_ULL(60) | BIT_ULL(59))\n\n \n#define ARCH_LBR_KERNEL_BIT\t\t1   \n#define ARCH_LBR_USER_BIT\t\t2   \n#define ARCH_LBR_CALL_STACK_BIT\t\t3   \n#define ARCH_LBR_JCC_BIT\t\t16  \n#define ARCH_LBR_REL_JMP_BIT\t\t17  \n#define ARCH_LBR_IND_JMP_BIT\t\t18  \n#define ARCH_LBR_REL_CALL_BIT\t\t19  \n#define ARCH_LBR_IND_CALL_BIT\t\t20  \n#define ARCH_LBR_RETURN_BIT\t\t21  \n#define ARCH_LBR_OTHER_BRANCH_BIT\t22  \n\n#define ARCH_LBR_KERNEL\t\t\t(1ULL << ARCH_LBR_KERNEL_BIT)\n#define ARCH_LBR_USER\t\t\t(1ULL << ARCH_LBR_USER_BIT)\n#define ARCH_LBR_CALL_STACK\t\t(1ULL << ARCH_LBR_CALL_STACK_BIT)\n#define ARCH_LBR_JCC\t\t\t(1ULL << ARCH_LBR_JCC_BIT)\n#define ARCH_LBR_REL_JMP\t\t(1ULL << ARCH_LBR_REL_JMP_BIT)\n#define ARCH_LBR_IND_JMP\t\t(1ULL << ARCH_LBR_IND_JMP_BIT)\n#define ARCH_LBR_REL_CALL\t\t(1ULL << ARCH_LBR_REL_CALL_BIT)\n#define ARCH_LBR_IND_CALL\t\t(1ULL << ARCH_LBR_IND_CALL_BIT)\n#define ARCH_LBR_RETURN\t\t\t(1ULL << ARCH_LBR_RETURN_BIT)\n#define ARCH_LBR_OTHER_BRANCH\t\t(1ULL << ARCH_LBR_OTHER_BRANCH_BIT)\n\n#define ARCH_LBR_ANY\t\t\t \\\n\t(ARCH_LBR_JCC\t\t\t|\\\n\t ARCH_LBR_REL_JMP\t\t|\\\n\t ARCH_LBR_IND_JMP\t\t|\\\n\t ARCH_LBR_REL_CALL\t\t|\\\n\t ARCH_LBR_IND_CALL\t\t|\\\n\t ARCH_LBR_RETURN\t\t|\\\n\t ARCH_LBR_OTHER_BRANCH)\n\n#define ARCH_LBR_CTL_MASK\t\t\t0x7f000e\n\nstatic void intel_pmu_lbr_filter(struct cpu_hw_events *cpuc);\n\nstatic __always_inline bool is_lbr_call_stack_bit_set(u64 config)\n{\n\tif (static_cpu_has(X86_FEATURE_ARCH_LBR))\n\t\treturn !!(config & ARCH_LBR_CALL_STACK);\n\n\treturn !!(config & LBR_CALL_STACK);\n}\n\n \n\nstatic void __intel_pmu_lbr_enable(bool pmi)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tu64 debugctl, lbr_select = 0, orig_debugctl;\n\n\t \n\tif (pmi && x86_pmu.version >= 4)\n\t\treturn;\n\n\t \n\tif (cpuc->lbr_sel)\n\t\tlbr_select = cpuc->lbr_sel->config & x86_pmu.lbr_sel_mask;\n\tif (!static_cpu_has(X86_FEATURE_ARCH_LBR) && !pmi && cpuc->lbr_sel)\n\t\twrmsrl(MSR_LBR_SELECT, lbr_select);\n\n\trdmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);\n\torig_debugctl = debugctl;\n\n\tif (!static_cpu_has(X86_FEATURE_ARCH_LBR))\n\t\tdebugctl |= DEBUGCTLMSR_LBR;\n\t \n\tif (is_lbr_call_stack_bit_set(lbr_select))\n\t\tdebugctl &= ~DEBUGCTLMSR_FREEZE_LBRS_ON_PMI;\n\telse\n\t\tdebugctl |= DEBUGCTLMSR_FREEZE_LBRS_ON_PMI;\n\n\tif (orig_debugctl != debugctl)\n\t\twrmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);\n\n\tif (static_cpu_has(X86_FEATURE_ARCH_LBR))\n\t\twrmsrl(MSR_ARCH_LBR_CTL, lbr_select | ARCH_LBR_CTL_LBREN);\n}\n\nvoid intel_pmu_lbr_reset_32(void)\n{\n\tint i;\n\n\tfor (i = 0; i < x86_pmu.lbr_nr; i++)\n\t\twrmsrl(x86_pmu.lbr_from + i, 0);\n}\n\nvoid intel_pmu_lbr_reset_64(void)\n{\n\tint i;\n\n\tfor (i = 0; i < x86_pmu.lbr_nr; i++) {\n\t\twrmsrl(x86_pmu.lbr_from + i, 0);\n\t\twrmsrl(x86_pmu.lbr_to   + i, 0);\n\t\tif (x86_pmu.lbr_has_info)\n\t\t\twrmsrl(x86_pmu.lbr_info + i, 0);\n\t}\n}\n\nstatic void intel_pmu_arch_lbr_reset(void)\n{\n\t \n\twrmsrl(MSR_ARCH_LBR_DEPTH, x86_pmu.lbr_nr);\n}\n\nvoid intel_pmu_lbr_reset(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tif (!x86_pmu.lbr_nr)\n\t\treturn;\n\n\tx86_pmu.lbr_reset();\n\n\tcpuc->last_task_ctx = NULL;\n\tcpuc->last_log_id = 0;\n\tif (!static_cpu_has(X86_FEATURE_ARCH_LBR) && cpuc->lbr_select)\n\t\twrmsrl(MSR_LBR_SELECT, 0);\n}\n\n \nstatic inline u64 intel_pmu_lbr_tos(void)\n{\n\tu64 tos;\n\n\trdmsrl(x86_pmu.lbr_tos, tos);\n\treturn tos;\n}\n\nenum {\n\tLBR_NONE,\n\tLBR_VALID,\n};\n\n \nstatic inline bool lbr_from_signext_quirk_needed(void)\n{\n\tbool tsx_support = boot_cpu_has(X86_FEATURE_HLE) ||\n\t\t\t   boot_cpu_has(X86_FEATURE_RTM);\n\n\treturn !tsx_support;\n}\n\nstatic DEFINE_STATIC_KEY_FALSE(lbr_from_quirk_key);\n\n \ninline u64 lbr_from_signext_quirk_wr(u64 val)\n{\n\tif (static_branch_unlikely(&lbr_from_quirk_key)) {\n\t\t \n\t\tval |= (LBR_FROM_SIGNEXT_2MSB & val) << 2;\n\t}\n\treturn val;\n}\n\n \nstatic u64 lbr_from_signext_quirk_rd(u64 val)\n{\n\tif (static_branch_unlikely(&lbr_from_quirk_key)) {\n\t\t \n\t\tval &= ~(LBR_FROM_FLAG_IN_TX | LBR_FROM_FLAG_ABORT);\n\t}\n\treturn val;\n}\n\nstatic __always_inline void wrlbr_from(unsigned int idx, u64 val)\n{\n\tval = lbr_from_signext_quirk_wr(val);\n\twrmsrl(x86_pmu.lbr_from + idx, val);\n}\n\nstatic __always_inline void wrlbr_to(unsigned int idx, u64 val)\n{\n\twrmsrl(x86_pmu.lbr_to + idx, val);\n}\n\nstatic __always_inline void wrlbr_info(unsigned int idx, u64 val)\n{\n\twrmsrl(x86_pmu.lbr_info + idx, val);\n}\n\nstatic __always_inline u64 rdlbr_from(unsigned int idx, struct lbr_entry *lbr)\n{\n\tu64 val;\n\n\tif (lbr)\n\t\treturn lbr->from;\n\n\trdmsrl(x86_pmu.lbr_from + idx, val);\n\n\treturn lbr_from_signext_quirk_rd(val);\n}\n\nstatic __always_inline u64 rdlbr_to(unsigned int idx, struct lbr_entry *lbr)\n{\n\tu64 val;\n\n\tif (lbr)\n\t\treturn lbr->to;\n\n\trdmsrl(x86_pmu.lbr_to + idx, val);\n\n\treturn val;\n}\n\nstatic __always_inline u64 rdlbr_info(unsigned int idx, struct lbr_entry *lbr)\n{\n\tu64 val;\n\n\tif (lbr)\n\t\treturn lbr->info;\n\n\trdmsrl(x86_pmu.lbr_info + idx, val);\n\n\treturn val;\n}\n\nstatic inline void\nwrlbr_all(struct lbr_entry *lbr, unsigned int idx, bool need_info)\n{\n\twrlbr_from(idx, lbr->from);\n\twrlbr_to(idx, lbr->to);\n\tif (need_info)\n\t\twrlbr_info(idx, lbr->info);\n}\n\nstatic inline bool\nrdlbr_all(struct lbr_entry *lbr, unsigned int idx, bool need_info)\n{\n\tu64 from = rdlbr_from(idx, NULL);\n\n\t \n\tif (!from)\n\t\treturn false;\n\n\tlbr->from = from;\n\tlbr->to = rdlbr_to(idx, NULL);\n\tif (need_info)\n\t\tlbr->info = rdlbr_info(idx, NULL);\n\n\treturn true;\n}\n\nvoid intel_pmu_lbr_restore(void *ctx)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct x86_perf_task_context *task_ctx = ctx;\n\tbool need_info = x86_pmu.lbr_has_info;\n\tu64 tos = task_ctx->tos;\n\tunsigned lbr_idx, mask;\n\tint i;\n\n\tmask = x86_pmu.lbr_nr - 1;\n\tfor (i = 0; i < task_ctx->valid_lbrs; i++) {\n\t\tlbr_idx = (tos - i) & mask;\n\t\twrlbr_all(&task_ctx->lbr[i], lbr_idx, need_info);\n\t}\n\n\tfor (; i < x86_pmu.lbr_nr; i++) {\n\t\tlbr_idx = (tos - i) & mask;\n\t\twrlbr_from(lbr_idx, 0);\n\t\twrlbr_to(lbr_idx, 0);\n\t\tif (need_info)\n\t\t\twrlbr_info(lbr_idx, 0);\n\t}\n\n\twrmsrl(x86_pmu.lbr_tos, tos);\n\n\tif (cpuc->lbr_select)\n\t\twrmsrl(MSR_LBR_SELECT, task_ctx->lbr_sel);\n}\n\nstatic void intel_pmu_arch_lbr_restore(void *ctx)\n{\n\tstruct x86_perf_task_context_arch_lbr *task_ctx = ctx;\n\tstruct lbr_entry *entries = task_ctx->entries;\n\tint i;\n\n\t \n\tif (!entries[x86_pmu.lbr_nr - 1].from)\n\t\tintel_pmu_arch_lbr_reset();\n\n\tfor (i = 0; i < x86_pmu.lbr_nr; i++) {\n\t\tif (!entries[i].from)\n\t\t\tbreak;\n\t\twrlbr_all(&entries[i], i, true);\n\t}\n}\n\n \nstatic void intel_pmu_arch_lbr_xrstors(void *ctx)\n{\n\tstruct x86_perf_task_context_arch_lbr_xsave *task_ctx = ctx;\n\n\txrstors(&task_ctx->xsave, XFEATURE_MASK_LBR);\n}\n\nstatic __always_inline bool lbr_is_reset_in_cstate(void *ctx)\n{\n\tif (static_cpu_has(X86_FEATURE_ARCH_LBR))\n\t\treturn x86_pmu.lbr_deep_c_reset && !rdlbr_from(0, NULL);\n\n\treturn !rdlbr_from(((struct x86_perf_task_context *)ctx)->tos, NULL);\n}\n\nstatic void __intel_pmu_lbr_restore(void *ctx)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tif (task_context_opt(ctx)->lbr_callstack_users == 0 ||\n\t    task_context_opt(ctx)->lbr_stack_state == LBR_NONE) {\n\t\tintel_pmu_lbr_reset();\n\t\treturn;\n\t}\n\n\t \n\tif ((ctx == cpuc->last_task_ctx) &&\n\t    (task_context_opt(ctx)->log_id == cpuc->last_log_id) &&\n\t    !lbr_is_reset_in_cstate(ctx)) {\n\t\ttask_context_opt(ctx)->lbr_stack_state = LBR_NONE;\n\t\treturn;\n\t}\n\n\tx86_pmu.lbr_restore(ctx);\n\n\ttask_context_opt(ctx)->lbr_stack_state = LBR_NONE;\n}\n\nvoid intel_pmu_lbr_save(void *ctx)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct x86_perf_task_context *task_ctx = ctx;\n\tbool need_info = x86_pmu.lbr_has_info;\n\tunsigned lbr_idx, mask;\n\tu64 tos;\n\tint i;\n\n\tmask = x86_pmu.lbr_nr - 1;\n\ttos = intel_pmu_lbr_tos();\n\tfor (i = 0; i < x86_pmu.lbr_nr; i++) {\n\t\tlbr_idx = (tos - i) & mask;\n\t\tif (!rdlbr_all(&task_ctx->lbr[i], lbr_idx, need_info))\n\t\t\tbreak;\n\t}\n\ttask_ctx->valid_lbrs = i;\n\ttask_ctx->tos = tos;\n\n\tif (cpuc->lbr_select)\n\t\trdmsrl(MSR_LBR_SELECT, task_ctx->lbr_sel);\n}\n\nstatic void intel_pmu_arch_lbr_save(void *ctx)\n{\n\tstruct x86_perf_task_context_arch_lbr *task_ctx = ctx;\n\tstruct lbr_entry *entries = task_ctx->entries;\n\tint i;\n\n\tfor (i = 0; i < x86_pmu.lbr_nr; i++) {\n\t\tif (!rdlbr_all(&entries[i], i, true))\n\t\t\tbreak;\n\t}\n\n\t \n\tif (i < x86_pmu.lbr_nr)\n\t\tentries[x86_pmu.lbr_nr - 1].from = 0;\n}\n\n \nstatic void intel_pmu_arch_lbr_xsaves(void *ctx)\n{\n\tstruct x86_perf_task_context_arch_lbr_xsave *task_ctx = ctx;\n\n\txsaves(&task_ctx->xsave, XFEATURE_MASK_LBR);\n}\n\nstatic void __intel_pmu_lbr_save(void *ctx)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tif (task_context_opt(ctx)->lbr_callstack_users == 0) {\n\t\ttask_context_opt(ctx)->lbr_stack_state = LBR_NONE;\n\t\treturn;\n\t}\n\n\tx86_pmu.lbr_save(ctx);\n\n\ttask_context_opt(ctx)->lbr_stack_state = LBR_VALID;\n\n\tcpuc->last_task_ctx = ctx;\n\tcpuc->last_log_id = ++task_context_opt(ctx)->log_id;\n}\n\nvoid intel_pmu_lbr_swap_task_ctx(struct perf_event_pmu_context *prev_epc,\n\t\t\t\t struct perf_event_pmu_context *next_epc)\n{\n\tvoid *prev_ctx_data, *next_ctx_data;\n\n\tswap(prev_epc->task_ctx_data, next_epc->task_ctx_data);\n\n\t \n\n\tprev_ctx_data = next_epc->task_ctx_data;\n\tnext_ctx_data = prev_epc->task_ctx_data;\n\n\tif (!prev_ctx_data || !next_ctx_data)\n\t\treturn;\n\n\tswap(task_context_opt(prev_ctx_data)->lbr_callstack_users,\n\t     task_context_opt(next_ctx_data)->lbr_callstack_users);\n}\n\nvoid intel_pmu_lbr_sched_task(struct perf_event_pmu_context *pmu_ctx, bool sched_in)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tvoid *task_ctx;\n\n\tif (!cpuc->lbr_users)\n\t\treturn;\n\n\t \n\ttask_ctx = pmu_ctx ? pmu_ctx->task_ctx_data : NULL;\n\tif (task_ctx) {\n\t\tif (sched_in)\n\t\t\t__intel_pmu_lbr_restore(task_ctx);\n\t\telse\n\t\t\t__intel_pmu_lbr_save(task_ctx);\n\t\treturn;\n\t}\n\n\t \n\tif (sched_in)\n\t\tintel_pmu_lbr_reset();\n}\n\nstatic inline bool branch_user_callstack(unsigned br_sel)\n{\n\treturn (br_sel & X86_BR_USER) && (br_sel & X86_BR_CALL_STACK);\n}\n\nvoid intel_pmu_lbr_add(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tif (!x86_pmu.lbr_nr)\n\t\treturn;\n\n\tif (event->hw.flags & PERF_X86_EVENT_LBR_SELECT)\n\t\tcpuc->lbr_select = 1;\n\n\tcpuc->br_sel = event->hw.branch_reg.reg;\n\n\tif (branch_user_callstack(cpuc->br_sel) && event->pmu_ctx->task_ctx_data)\n\t\ttask_context_opt(event->pmu_ctx->task_ctx_data)->lbr_callstack_users++;\n\n\t \n\tif (x86_pmu.intel_cap.pebs_baseline && event->attr.precise_ip > 0)\n\t\tcpuc->lbr_pebs_users++;\n\tperf_sched_cb_inc(event->pmu);\n\tif (!cpuc->lbr_users++ && !event->total_time_running)\n\t\tintel_pmu_lbr_reset();\n}\n\nvoid release_lbr_buffers(void)\n{\n\tstruct kmem_cache *kmem_cache;\n\tstruct cpu_hw_events *cpuc;\n\tint cpu;\n\n\tif (!static_cpu_has(X86_FEATURE_ARCH_LBR))\n\t\treturn;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcpuc = per_cpu_ptr(&cpu_hw_events, cpu);\n\t\tkmem_cache = x86_get_pmu(cpu)->task_ctx_cache;\n\t\tif (kmem_cache && cpuc->lbr_xsave) {\n\t\t\tkmem_cache_free(kmem_cache, cpuc->lbr_xsave);\n\t\t\tcpuc->lbr_xsave = NULL;\n\t\t}\n\t}\n}\n\nvoid reserve_lbr_buffers(void)\n{\n\tstruct kmem_cache *kmem_cache;\n\tstruct cpu_hw_events *cpuc;\n\tint cpu;\n\n\tif (!static_cpu_has(X86_FEATURE_ARCH_LBR))\n\t\treturn;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tcpuc = per_cpu_ptr(&cpu_hw_events, cpu);\n\t\tkmem_cache = x86_get_pmu(cpu)->task_ctx_cache;\n\t\tif (!kmem_cache || cpuc->lbr_xsave)\n\t\t\tcontinue;\n\n\t\tcpuc->lbr_xsave = kmem_cache_alloc_node(kmem_cache,\n\t\t\t\t\t\t\tGFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\t\t\tcpu_to_node(cpu));\n\t}\n}\n\nvoid intel_pmu_lbr_del(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tif (!x86_pmu.lbr_nr)\n\t\treturn;\n\n\tif (branch_user_callstack(cpuc->br_sel) &&\n\t    event->pmu_ctx->task_ctx_data)\n\t\ttask_context_opt(event->pmu_ctx->task_ctx_data)->lbr_callstack_users--;\n\n\tif (event->hw.flags & PERF_X86_EVENT_LBR_SELECT)\n\t\tcpuc->lbr_select = 0;\n\n\tif (x86_pmu.intel_cap.pebs_baseline && event->attr.precise_ip > 0)\n\t\tcpuc->lbr_pebs_users--;\n\tcpuc->lbr_users--;\n\tWARN_ON_ONCE(cpuc->lbr_users < 0);\n\tWARN_ON_ONCE(cpuc->lbr_pebs_users < 0);\n\tperf_sched_cb_dec(event->pmu);\n}\n\nstatic inline bool vlbr_exclude_host(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\treturn test_bit(INTEL_PMC_IDX_FIXED_VLBR,\n\t\t(unsigned long *)&cpuc->intel_ctrl_guest_mask);\n}\n\nvoid intel_pmu_lbr_enable_all(bool pmi)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tif (cpuc->lbr_users && !vlbr_exclude_host())\n\t\t__intel_pmu_lbr_enable(pmi);\n}\n\nvoid intel_pmu_lbr_disable_all(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tif (cpuc->lbr_users && !vlbr_exclude_host()) {\n\t\tif (static_cpu_has(X86_FEATURE_ARCH_LBR))\n\t\t\treturn __intel_pmu_arch_lbr_disable();\n\n\t\t__intel_pmu_lbr_disable();\n\t}\n}\n\nvoid intel_pmu_lbr_read_32(struct cpu_hw_events *cpuc)\n{\n\tunsigned long mask = x86_pmu.lbr_nr - 1;\n\tstruct perf_branch_entry *br = cpuc->lbr_entries;\n\tu64 tos = intel_pmu_lbr_tos();\n\tint i;\n\n\tfor (i = 0; i < x86_pmu.lbr_nr; i++) {\n\t\tunsigned long lbr_idx = (tos - i) & mask;\n\t\tunion {\n\t\t\tstruct {\n\t\t\t\tu32 from;\n\t\t\t\tu32 to;\n\t\t\t};\n\t\t\tu64     lbr;\n\t\t} msr_lastbranch;\n\n\t\trdmsrl(x86_pmu.lbr_from + lbr_idx, msr_lastbranch.lbr);\n\n\t\tperf_clear_branch_entry_bitfields(br);\n\n\t\tbr->from\t= msr_lastbranch.from;\n\t\tbr->to\t\t= msr_lastbranch.to;\n\t\tbr++;\n\t}\n\tcpuc->lbr_stack.nr = i;\n\tcpuc->lbr_stack.hw_idx = tos;\n}\n\n \nvoid intel_pmu_lbr_read_64(struct cpu_hw_events *cpuc)\n{\n\tbool need_info = false, call_stack = false;\n\tunsigned long mask = x86_pmu.lbr_nr - 1;\n\tstruct perf_branch_entry *br = cpuc->lbr_entries;\n\tu64 tos = intel_pmu_lbr_tos();\n\tint i;\n\tint out = 0;\n\tint num = x86_pmu.lbr_nr;\n\n\tif (cpuc->lbr_sel) {\n\t\tneed_info = !(cpuc->lbr_sel->config & LBR_NO_INFO);\n\t\tif (cpuc->lbr_sel->config & LBR_CALL_STACK)\n\t\t\tcall_stack = true;\n\t}\n\n\tfor (i = 0; i < num; i++) {\n\t\tunsigned long lbr_idx = (tos - i) & mask;\n\t\tu64 from, to, mis = 0, pred = 0, in_tx = 0, abort = 0;\n\t\tu16 cycles = 0;\n\n\t\tfrom = rdlbr_from(lbr_idx, NULL);\n\t\tto   = rdlbr_to(lbr_idx, NULL);\n\n\t\t \n\t\tif (call_stack && !from)\n\t\t\tbreak;\n\n\t\tif (x86_pmu.lbr_has_info) {\n\t\t\tif (need_info) {\n\t\t\t\tu64 info;\n\n\t\t\t\tinfo = rdlbr_info(lbr_idx, NULL);\n\t\t\t\tmis = !!(info & LBR_INFO_MISPRED);\n\t\t\t\tpred = !mis;\n\t\t\t\tcycles = (info & LBR_INFO_CYCLES);\n\t\t\t\tif (x86_pmu.lbr_has_tsx) {\n\t\t\t\t\tin_tx = !!(info & LBR_INFO_IN_TX);\n\t\t\t\t\tabort = !!(info & LBR_INFO_ABORT);\n\t\t\t\t}\n\t\t\t}\n\t\t} else {\n\t\t\tint skip = 0;\n\n\t\t\tif (x86_pmu.lbr_from_flags) {\n\t\t\t\tmis = !!(from & LBR_FROM_FLAG_MISPRED);\n\t\t\t\tpred = !mis;\n\t\t\t\tskip = 1;\n\t\t\t}\n\t\t\tif (x86_pmu.lbr_has_tsx) {\n\t\t\t\tin_tx = !!(from & LBR_FROM_FLAG_IN_TX);\n\t\t\t\tabort = !!(from & LBR_FROM_FLAG_ABORT);\n\t\t\t\tskip = 3;\n\t\t\t}\n\t\t\tfrom = (u64)((((s64)from) << skip) >> skip);\n\n\t\t\tif (x86_pmu.lbr_to_cycles) {\n\t\t\t\tcycles = ((to >> 48) & LBR_INFO_CYCLES);\n\t\t\t\tto = (u64)((((s64)to) << 16) >> 16);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (abort && x86_pmu.lbr_double_abort && out > 0)\n\t\t\tout--;\n\n\t\tperf_clear_branch_entry_bitfields(br+out);\n\t\tbr[out].from\t = from;\n\t\tbr[out].to\t = to;\n\t\tbr[out].mispred\t = mis;\n\t\tbr[out].predicted = pred;\n\t\tbr[out].in_tx\t = in_tx;\n\t\tbr[out].abort\t = abort;\n\t\tbr[out].cycles\t = cycles;\n\t\tout++;\n\t}\n\tcpuc->lbr_stack.nr = out;\n\tcpuc->lbr_stack.hw_idx = tos;\n}\n\nstatic DEFINE_STATIC_KEY_FALSE(x86_lbr_mispred);\nstatic DEFINE_STATIC_KEY_FALSE(x86_lbr_cycles);\nstatic DEFINE_STATIC_KEY_FALSE(x86_lbr_type);\n\nstatic __always_inline int get_lbr_br_type(u64 info)\n{\n\tint type = 0;\n\n\tif (static_branch_likely(&x86_lbr_type))\n\t\ttype = (info & LBR_INFO_BR_TYPE) >> LBR_INFO_BR_TYPE_OFFSET;\n\n\treturn type;\n}\n\nstatic __always_inline bool get_lbr_mispred(u64 info)\n{\n\tbool mispred = 0;\n\n\tif (static_branch_likely(&x86_lbr_mispred))\n\t\tmispred = !!(info & LBR_INFO_MISPRED);\n\n\treturn mispred;\n}\n\nstatic __always_inline u16 get_lbr_cycles(u64 info)\n{\n\tu16 cycles = info & LBR_INFO_CYCLES;\n\n\tif (static_cpu_has(X86_FEATURE_ARCH_LBR) &&\n\t    (!static_branch_likely(&x86_lbr_cycles) ||\n\t     !(info & LBR_INFO_CYC_CNT_VALID)))\n\t\tcycles = 0;\n\n\treturn cycles;\n}\n\nstatic void intel_pmu_store_lbr(struct cpu_hw_events *cpuc,\n\t\t\t\tstruct lbr_entry *entries)\n{\n\tstruct perf_branch_entry *e;\n\tstruct lbr_entry *lbr;\n\tu64 from, to, info;\n\tint i;\n\n\tfor (i = 0; i < x86_pmu.lbr_nr; i++) {\n\t\tlbr = entries ? &entries[i] : NULL;\n\t\te = &cpuc->lbr_entries[i];\n\n\t\tfrom = rdlbr_from(i, lbr);\n\t\t \n\t\tif (!from)\n\t\t\tbreak;\n\n\t\tto = rdlbr_to(i, lbr);\n\t\tinfo = rdlbr_info(i, lbr);\n\n\t\tperf_clear_branch_entry_bitfields(e);\n\n\t\te->from\t\t= from;\n\t\te->to\t\t= to;\n\t\te->mispred\t= get_lbr_mispred(info);\n\t\te->predicted\t= !e->mispred;\n\t\te->in_tx\t= !!(info & LBR_INFO_IN_TX);\n\t\te->abort\t= !!(info & LBR_INFO_ABORT);\n\t\te->cycles\t= get_lbr_cycles(info);\n\t\te->type\t\t= get_lbr_br_type(info);\n\t}\n\n\tcpuc->lbr_stack.nr = i;\n}\n\nstatic void intel_pmu_arch_lbr_read(struct cpu_hw_events *cpuc)\n{\n\tintel_pmu_store_lbr(cpuc, NULL);\n}\n\nstatic void intel_pmu_arch_lbr_read_xsave(struct cpu_hw_events *cpuc)\n{\n\tstruct x86_perf_task_context_arch_lbr_xsave *xsave = cpuc->lbr_xsave;\n\n\tif (!xsave) {\n\t\tintel_pmu_store_lbr(cpuc, NULL);\n\t\treturn;\n\t}\n\txsaves(&xsave->xsave, XFEATURE_MASK_LBR);\n\n\tintel_pmu_store_lbr(cpuc, xsave->lbr.entries);\n}\n\nvoid intel_pmu_lbr_read(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\t \n\tif (!cpuc->lbr_users || vlbr_exclude_host() ||\n\t    cpuc->lbr_users == cpuc->lbr_pebs_users)\n\t\treturn;\n\n\tx86_pmu.lbr_read(cpuc);\n\n\tintel_pmu_lbr_filter(cpuc);\n}\n\n \nstatic int intel_pmu_setup_sw_lbr_filter(struct perf_event *event)\n{\n\tu64 br_type = event->attr.branch_sample_type;\n\tint mask = 0;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_USER)\n\t\tmask |= X86_BR_USER;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_KERNEL)\n\t\tmask |= X86_BR_KERNEL;\n\n\t \n\n\tif (br_type & PERF_SAMPLE_BRANCH_ANY)\n\t\tmask |= X86_BR_ANY;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_ANY_CALL)\n\t\tmask |= X86_BR_ANY_CALL;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_ANY_RETURN)\n\t\tmask |= X86_BR_RET | X86_BR_IRET | X86_BR_SYSRET;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_IND_CALL)\n\t\tmask |= X86_BR_IND_CALL;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_ABORT_TX)\n\t\tmask |= X86_BR_ABORT;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_IN_TX)\n\t\tmask |= X86_BR_IN_TX;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_NO_TX)\n\t\tmask |= X86_BR_NO_TX;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_COND)\n\t\tmask |= X86_BR_JCC;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_CALL_STACK) {\n\t\tif (!x86_pmu_has_lbr_callstack())\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (mask & ~(X86_BR_USER | X86_BR_KERNEL))\n\t\t\treturn -EINVAL;\n\t\tmask |= X86_BR_CALL | X86_BR_IND_CALL | X86_BR_RET |\n\t\t\tX86_BR_CALL_STACK;\n\t}\n\n\tif (br_type & PERF_SAMPLE_BRANCH_IND_JUMP)\n\t\tmask |= X86_BR_IND_JMP;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_CALL)\n\t\tmask |= X86_BR_CALL | X86_BR_ZERO_CALL;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_TYPE_SAVE)\n\t\tmask |= X86_BR_TYPE_SAVE;\n\n\t \n\tevent->hw.branch_reg.reg = mask;\n\treturn 0;\n}\n\n \nstatic int intel_pmu_setup_hw_lbr_filter(struct perf_event *event)\n{\n\tstruct hw_perf_event_extra *reg;\n\tu64 br_type = event->attr.branch_sample_type;\n\tu64 mask = 0, v;\n\tint i;\n\n\tfor (i = 0; i < PERF_SAMPLE_BRANCH_MAX_SHIFT; i++) {\n\t\tif (!(br_type & (1ULL << i)))\n\t\t\tcontinue;\n\n\t\tv = x86_pmu.lbr_sel_map[i];\n\t\tif (v == LBR_NOT_SUPP)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (v != LBR_IGN)\n\t\t\tmask |= v;\n\t}\n\n\treg = &event->hw.branch_reg;\n\treg->idx = EXTRA_REG_LBR;\n\n\tif (static_cpu_has(X86_FEATURE_ARCH_LBR)) {\n\t\treg->config = mask;\n\n\t\t \n\t\treg->reg |= X86_BR_TYPE_SAVE;\n\t\treturn 0;\n\t}\n\n\t \n\treg->config = mask ^ (x86_pmu.lbr_sel_mask & ~LBR_CALL_STACK);\n\n\tif ((br_type & PERF_SAMPLE_BRANCH_NO_CYCLES) &&\n\t    (br_type & PERF_SAMPLE_BRANCH_NO_FLAGS) &&\n\t    x86_pmu.lbr_has_info)\n\t\treg->config |= LBR_NO_INFO;\n\n\treturn 0;\n}\n\nint intel_pmu_setup_lbr_filter(struct perf_event *event)\n{\n\tint ret = 0;\n\n\t \n\tif (!x86_pmu.lbr_nr)\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tret = intel_pmu_setup_sw_lbr_filter(event);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (x86_pmu.lbr_sel_map)\n\t\tret = intel_pmu_setup_hw_lbr_filter(event);\n\n\treturn ret;\n}\n\nenum {\n\tARCH_LBR_BR_TYPE_JCC\t\t\t= 0,\n\tARCH_LBR_BR_TYPE_NEAR_IND_JMP\t\t= 1,\n\tARCH_LBR_BR_TYPE_NEAR_REL_JMP\t\t= 2,\n\tARCH_LBR_BR_TYPE_NEAR_IND_CALL\t\t= 3,\n\tARCH_LBR_BR_TYPE_NEAR_REL_CALL\t\t= 4,\n\tARCH_LBR_BR_TYPE_NEAR_RET\t\t= 5,\n\tARCH_LBR_BR_TYPE_KNOWN_MAX\t\t= ARCH_LBR_BR_TYPE_NEAR_RET,\n\n\tARCH_LBR_BR_TYPE_MAP_MAX\t\t= 16,\n};\n\nstatic const int arch_lbr_br_type_map[ARCH_LBR_BR_TYPE_MAP_MAX] = {\n\t[ARCH_LBR_BR_TYPE_JCC]\t\t\t= X86_BR_JCC,\n\t[ARCH_LBR_BR_TYPE_NEAR_IND_JMP]\t\t= X86_BR_IND_JMP,\n\t[ARCH_LBR_BR_TYPE_NEAR_REL_JMP]\t\t= X86_BR_JMP,\n\t[ARCH_LBR_BR_TYPE_NEAR_IND_CALL]\t= X86_BR_IND_CALL,\n\t[ARCH_LBR_BR_TYPE_NEAR_REL_CALL]\t= X86_BR_CALL,\n\t[ARCH_LBR_BR_TYPE_NEAR_RET]\t\t= X86_BR_RET,\n};\n\n \nstatic void\nintel_pmu_lbr_filter(struct cpu_hw_events *cpuc)\n{\n\tu64 from, to;\n\tint br_sel = cpuc->br_sel;\n\tint i, j, type, to_plm;\n\tbool compress = false;\n\n\t \n\tif (((br_sel & X86_BR_ALL) == X86_BR_ALL) &&\n\t    ((br_sel & X86_BR_TYPE_SAVE) != X86_BR_TYPE_SAVE))\n\t\treturn;\n\n\tfor (i = 0; i < cpuc->lbr_stack.nr; i++) {\n\n\t\tfrom = cpuc->lbr_entries[i].from;\n\t\tto = cpuc->lbr_entries[i].to;\n\t\ttype = cpuc->lbr_entries[i].type;\n\n\t\t \n\t\tif (static_cpu_has(X86_FEATURE_ARCH_LBR) &&\n\t\t    type <= ARCH_LBR_BR_TYPE_KNOWN_MAX) {\n\t\t\tto_plm = kernel_ip(to) ? X86_BR_KERNEL : X86_BR_USER;\n\t\t\ttype = arch_lbr_br_type_map[type] | to_plm;\n\t\t} else\n\t\t\ttype = branch_type(from, to, cpuc->lbr_entries[i].abort);\n\t\tif (type != X86_BR_NONE && (br_sel & X86_BR_ANYTX)) {\n\t\t\tif (cpuc->lbr_entries[i].in_tx)\n\t\t\t\ttype |= X86_BR_IN_TX;\n\t\t\telse\n\t\t\t\ttype |= X86_BR_NO_TX;\n\t\t}\n\n\t\t \n\t\tif (type == X86_BR_NONE || (br_sel & type) != type) {\n\t\t\tcpuc->lbr_entries[i].from = 0;\n\t\t\tcompress = true;\n\t\t}\n\n\t\tif ((br_sel & X86_BR_TYPE_SAVE) == X86_BR_TYPE_SAVE)\n\t\t\tcpuc->lbr_entries[i].type = common_branch_type(type);\n\t}\n\n\tif (!compress)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < cpuc->lbr_stack.nr; ) {\n\t\tif (!cpuc->lbr_entries[i].from) {\n\t\t\tj = i;\n\t\t\twhile (++j < cpuc->lbr_stack.nr)\n\t\t\t\tcpuc->lbr_entries[j-1] = cpuc->lbr_entries[j];\n\t\t\tcpuc->lbr_stack.nr--;\n\t\t\tif (!cpuc->lbr_entries[i].from)\n\t\t\t\tcontinue;\n\t\t}\n\t\ti++;\n\t}\n}\n\nvoid intel_pmu_store_pebs_lbrs(struct lbr_entry *lbr)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\t \n\tif (static_cpu_has(X86_FEATURE_ARCH_LBR) ||\n\t    (cpuc->n_pebs == cpuc->n_large_pebs))\n\t\tcpuc->lbr_stack.hw_idx = -1ULL;\n\telse\n\t\tcpuc->lbr_stack.hw_idx = intel_pmu_lbr_tos();\n\n\tintel_pmu_store_lbr(cpuc, lbr);\n\tintel_pmu_lbr_filter(cpuc);\n}\n\n \nstatic const int nhm_lbr_sel_map[PERF_SAMPLE_BRANCH_MAX_SHIFT] = {\n\t[PERF_SAMPLE_BRANCH_ANY_SHIFT]\t\t= LBR_ANY,\n\t[PERF_SAMPLE_BRANCH_USER_SHIFT]\t\t= LBR_USER,\n\t[PERF_SAMPLE_BRANCH_KERNEL_SHIFT]\t= LBR_KERNEL,\n\t[PERF_SAMPLE_BRANCH_HV_SHIFT]\t\t= LBR_IGN,\n\t[PERF_SAMPLE_BRANCH_ANY_RETURN_SHIFT]\t= LBR_RETURN | LBR_REL_JMP\n\t\t\t\t\t\t| LBR_IND_JMP | LBR_FAR,\n\t \n\t[PERF_SAMPLE_BRANCH_ANY_CALL_SHIFT] =\n\t LBR_REL_CALL | LBR_IND_CALL | LBR_REL_JMP | LBR_IND_JMP | LBR_FAR,\n\t \n\t[PERF_SAMPLE_BRANCH_IND_CALL_SHIFT] = LBR_IND_CALL | LBR_IND_JMP,\n\t[PERF_SAMPLE_BRANCH_COND_SHIFT]     = LBR_JCC,\n\t[PERF_SAMPLE_BRANCH_IND_JUMP_SHIFT] = LBR_IND_JMP,\n};\n\nstatic const int snb_lbr_sel_map[PERF_SAMPLE_BRANCH_MAX_SHIFT] = {\n\t[PERF_SAMPLE_BRANCH_ANY_SHIFT]\t\t= LBR_ANY,\n\t[PERF_SAMPLE_BRANCH_USER_SHIFT]\t\t= LBR_USER,\n\t[PERF_SAMPLE_BRANCH_KERNEL_SHIFT]\t= LBR_KERNEL,\n\t[PERF_SAMPLE_BRANCH_HV_SHIFT]\t\t= LBR_IGN,\n\t[PERF_SAMPLE_BRANCH_ANY_RETURN_SHIFT]\t= LBR_RETURN | LBR_FAR,\n\t[PERF_SAMPLE_BRANCH_ANY_CALL_SHIFT]\t= LBR_REL_CALL | LBR_IND_CALL\n\t\t\t\t\t\t| LBR_FAR,\n\t[PERF_SAMPLE_BRANCH_IND_CALL_SHIFT]\t= LBR_IND_CALL,\n\t[PERF_SAMPLE_BRANCH_COND_SHIFT]\t\t= LBR_JCC,\n\t[PERF_SAMPLE_BRANCH_IND_JUMP_SHIFT]\t= LBR_IND_JMP,\n\t[PERF_SAMPLE_BRANCH_CALL_SHIFT]\t\t= LBR_REL_CALL,\n};\n\nstatic const int hsw_lbr_sel_map[PERF_SAMPLE_BRANCH_MAX_SHIFT] = {\n\t[PERF_SAMPLE_BRANCH_ANY_SHIFT]\t\t= LBR_ANY,\n\t[PERF_SAMPLE_BRANCH_USER_SHIFT]\t\t= LBR_USER,\n\t[PERF_SAMPLE_BRANCH_KERNEL_SHIFT]\t= LBR_KERNEL,\n\t[PERF_SAMPLE_BRANCH_HV_SHIFT]\t\t= LBR_IGN,\n\t[PERF_SAMPLE_BRANCH_ANY_RETURN_SHIFT]\t= LBR_RETURN | LBR_FAR,\n\t[PERF_SAMPLE_BRANCH_ANY_CALL_SHIFT]\t= LBR_REL_CALL | LBR_IND_CALL\n\t\t\t\t\t\t| LBR_FAR,\n\t[PERF_SAMPLE_BRANCH_IND_CALL_SHIFT]\t= LBR_IND_CALL,\n\t[PERF_SAMPLE_BRANCH_COND_SHIFT]\t\t= LBR_JCC,\n\t[PERF_SAMPLE_BRANCH_CALL_STACK_SHIFT]\t= LBR_REL_CALL | LBR_IND_CALL\n\t\t\t\t\t\t| LBR_RETURN | LBR_CALL_STACK,\n\t[PERF_SAMPLE_BRANCH_IND_JUMP_SHIFT]\t= LBR_IND_JMP,\n\t[PERF_SAMPLE_BRANCH_CALL_SHIFT]\t\t= LBR_REL_CALL,\n};\n\nstatic int arch_lbr_ctl_map[PERF_SAMPLE_BRANCH_MAX_SHIFT] = {\n\t[PERF_SAMPLE_BRANCH_ANY_SHIFT]\t\t= ARCH_LBR_ANY,\n\t[PERF_SAMPLE_BRANCH_USER_SHIFT]\t\t= ARCH_LBR_USER,\n\t[PERF_SAMPLE_BRANCH_KERNEL_SHIFT]\t= ARCH_LBR_KERNEL,\n\t[PERF_SAMPLE_BRANCH_HV_SHIFT]\t\t= LBR_IGN,\n\t[PERF_SAMPLE_BRANCH_ANY_RETURN_SHIFT]\t= ARCH_LBR_RETURN |\n\t\t\t\t\t\t  ARCH_LBR_OTHER_BRANCH,\n\t[PERF_SAMPLE_BRANCH_ANY_CALL_SHIFT]     = ARCH_LBR_REL_CALL |\n\t\t\t\t\t\t  ARCH_LBR_IND_CALL |\n\t\t\t\t\t\t  ARCH_LBR_OTHER_BRANCH,\n\t[PERF_SAMPLE_BRANCH_IND_CALL_SHIFT]     = ARCH_LBR_IND_CALL,\n\t[PERF_SAMPLE_BRANCH_COND_SHIFT]         = ARCH_LBR_JCC,\n\t[PERF_SAMPLE_BRANCH_CALL_STACK_SHIFT]   = ARCH_LBR_REL_CALL |\n\t\t\t\t\t\t  ARCH_LBR_IND_CALL |\n\t\t\t\t\t\t  ARCH_LBR_RETURN |\n\t\t\t\t\t\t  ARCH_LBR_CALL_STACK,\n\t[PERF_SAMPLE_BRANCH_IND_JUMP_SHIFT]\t= ARCH_LBR_IND_JMP,\n\t[PERF_SAMPLE_BRANCH_CALL_SHIFT]\t\t= ARCH_LBR_REL_CALL,\n};\n\n \nvoid __init intel_pmu_lbr_init_core(void)\n{\n\tx86_pmu.lbr_nr     = 4;\n\tx86_pmu.lbr_tos    = MSR_LBR_TOS;\n\tx86_pmu.lbr_from   = MSR_LBR_CORE_FROM;\n\tx86_pmu.lbr_to     = MSR_LBR_CORE_TO;\n\n\t \n}\n\n \nvoid __init intel_pmu_lbr_init_nhm(void)\n{\n\tx86_pmu.lbr_nr     = 16;\n\tx86_pmu.lbr_tos    = MSR_LBR_TOS;\n\tx86_pmu.lbr_from   = MSR_LBR_NHM_FROM;\n\tx86_pmu.lbr_to     = MSR_LBR_NHM_TO;\n\n\tx86_pmu.lbr_sel_mask = LBR_SEL_MASK;\n\tx86_pmu.lbr_sel_map  = nhm_lbr_sel_map;\n\n\t \n}\n\n \nvoid __init intel_pmu_lbr_init_snb(void)\n{\n\tx86_pmu.lbr_nr\t = 16;\n\tx86_pmu.lbr_tos\t = MSR_LBR_TOS;\n\tx86_pmu.lbr_from = MSR_LBR_NHM_FROM;\n\tx86_pmu.lbr_to   = MSR_LBR_NHM_TO;\n\n\tx86_pmu.lbr_sel_mask = LBR_SEL_MASK;\n\tx86_pmu.lbr_sel_map  = snb_lbr_sel_map;\n\n\t \n}\n\nstatic inline struct kmem_cache *\ncreate_lbr_kmem_cache(size_t size, size_t align)\n{\n\treturn kmem_cache_create(\"x86_lbr\", size, align, 0, NULL);\n}\n\n \nvoid intel_pmu_lbr_init_hsw(void)\n{\n\tsize_t size = sizeof(struct x86_perf_task_context);\n\n\tx86_pmu.lbr_nr\t = 16;\n\tx86_pmu.lbr_tos\t = MSR_LBR_TOS;\n\tx86_pmu.lbr_from = MSR_LBR_NHM_FROM;\n\tx86_pmu.lbr_to   = MSR_LBR_NHM_TO;\n\n\tx86_pmu.lbr_sel_mask = LBR_SEL_MASK;\n\tx86_pmu.lbr_sel_map  = hsw_lbr_sel_map;\n\n\tx86_get_pmu(smp_processor_id())->task_ctx_cache = create_lbr_kmem_cache(size, 0);\n}\n\n \n__init void intel_pmu_lbr_init_skl(void)\n{\n\tsize_t size = sizeof(struct x86_perf_task_context);\n\n\tx86_pmu.lbr_nr\t = 32;\n\tx86_pmu.lbr_tos\t = MSR_LBR_TOS;\n\tx86_pmu.lbr_from = MSR_LBR_NHM_FROM;\n\tx86_pmu.lbr_to   = MSR_LBR_NHM_TO;\n\tx86_pmu.lbr_info = MSR_LBR_INFO_0;\n\n\tx86_pmu.lbr_sel_mask = LBR_SEL_MASK;\n\tx86_pmu.lbr_sel_map  = hsw_lbr_sel_map;\n\n\tx86_get_pmu(smp_processor_id())->task_ctx_cache = create_lbr_kmem_cache(size, 0);\n\n\t \n}\n\n \nvoid __init intel_pmu_lbr_init_atom(void)\n{\n\t \n\tif (boot_cpu_data.x86_model == 28\n\t    && boot_cpu_data.x86_stepping < 10) {\n\t\tpr_cont(\"LBR disabled due to erratum\");\n\t\treturn;\n\t}\n\n\tx86_pmu.lbr_nr\t   = 8;\n\tx86_pmu.lbr_tos    = MSR_LBR_TOS;\n\tx86_pmu.lbr_from   = MSR_LBR_CORE_FROM;\n\tx86_pmu.lbr_to     = MSR_LBR_CORE_TO;\n\n\t \n}\n\n \nvoid __init intel_pmu_lbr_init_slm(void)\n{\n\tx86_pmu.lbr_nr\t   = 8;\n\tx86_pmu.lbr_tos    = MSR_LBR_TOS;\n\tx86_pmu.lbr_from   = MSR_LBR_CORE_FROM;\n\tx86_pmu.lbr_to     = MSR_LBR_CORE_TO;\n\n\tx86_pmu.lbr_sel_mask = LBR_SEL_MASK;\n\tx86_pmu.lbr_sel_map  = nhm_lbr_sel_map;\n\n\t \n\tpr_cont(\"8-deep LBR, \");\n}\n\n \nvoid intel_pmu_lbr_init_knl(void)\n{\n\tx86_pmu.lbr_nr\t   = 8;\n\tx86_pmu.lbr_tos    = MSR_LBR_TOS;\n\tx86_pmu.lbr_from   = MSR_LBR_NHM_FROM;\n\tx86_pmu.lbr_to     = MSR_LBR_NHM_TO;\n\n\tx86_pmu.lbr_sel_mask = LBR_SEL_MASK;\n\tx86_pmu.lbr_sel_map  = snb_lbr_sel_map;\n\n\t \n\tif (x86_pmu.intel_cap.lbr_format == LBR_FORMAT_LIP)\n\t\tx86_pmu.intel_cap.lbr_format = LBR_FORMAT_EIP_FLAGS;\n}\n\nvoid intel_pmu_lbr_init(void)\n{\n\tswitch (x86_pmu.intel_cap.lbr_format) {\n\tcase LBR_FORMAT_EIP_FLAGS2:\n\t\tx86_pmu.lbr_has_tsx = 1;\n\t\tx86_pmu.lbr_from_flags = 1;\n\t\tif (lbr_from_signext_quirk_needed())\n\t\t\tstatic_branch_enable(&lbr_from_quirk_key);\n\t\tbreak;\n\n\tcase LBR_FORMAT_EIP_FLAGS:\n\t\tx86_pmu.lbr_from_flags = 1;\n\t\tbreak;\n\n\tcase LBR_FORMAT_INFO:\n\t\tx86_pmu.lbr_has_tsx = 1;\n\t\tfallthrough;\n\tcase LBR_FORMAT_INFO2:\n\t\tx86_pmu.lbr_has_info = 1;\n\t\tbreak;\n\n\tcase LBR_FORMAT_TIME:\n\t\tx86_pmu.lbr_from_flags = 1;\n\t\tx86_pmu.lbr_to_cycles = 1;\n\t\tbreak;\n\t}\n\n\tif (x86_pmu.lbr_has_info) {\n\t\t \n\t\tstatic_branch_enable(&x86_lbr_mispred);\n\t\tstatic_branch_enable(&x86_lbr_cycles);\n\t}\n}\n\n \nstatic inline unsigned int get_lbr_state_size(void)\n{\n\treturn sizeof(struct arch_lbr_state) +\n\t       x86_pmu.lbr_nr * sizeof(struct lbr_entry);\n}\n\nstatic bool is_arch_lbr_xsave_available(void)\n{\n\tif (!boot_cpu_has(X86_FEATURE_XSAVES))\n\t\treturn false;\n\n\t \n\tif (xfeature_size(XFEATURE_LBR) == 0)\n\t\treturn false;\n\n\tif (WARN_ON(xfeature_size(XFEATURE_LBR) != get_lbr_state_size()))\n\t\treturn false;\n\n\treturn true;\n}\n\nvoid __init intel_pmu_arch_lbr_init(void)\n{\n\tstruct pmu *pmu = x86_get_pmu(smp_processor_id());\n\tunion cpuid28_eax eax;\n\tunion cpuid28_ebx ebx;\n\tunion cpuid28_ecx ecx;\n\tunsigned int unused_edx;\n\tbool arch_lbr_xsave;\n\tsize_t size;\n\tu64 lbr_nr;\n\n\t \n\tcpuid(28, &eax.full, &ebx.full, &ecx.full, &unused_edx);\n\n\tlbr_nr = fls(eax.split.lbr_depth_mask) * 8;\n\tif (!lbr_nr)\n\t\tgoto clear_arch_lbr;\n\n\t \n\tif (wrmsrl_safe(MSR_ARCH_LBR_DEPTH, lbr_nr))\n\t\tgoto clear_arch_lbr;\n\n\tx86_pmu.lbr_depth_mask = eax.split.lbr_depth_mask;\n\tx86_pmu.lbr_deep_c_reset = eax.split.lbr_deep_c_reset;\n\tx86_pmu.lbr_lip = eax.split.lbr_lip;\n\tx86_pmu.lbr_cpl = ebx.split.lbr_cpl;\n\tx86_pmu.lbr_filter = ebx.split.lbr_filter;\n\tx86_pmu.lbr_call_stack = ebx.split.lbr_call_stack;\n\tx86_pmu.lbr_mispred = ecx.split.lbr_mispred;\n\tx86_pmu.lbr_timed_lbr = ecx.split.lbr_timed_lbr;\n\tx86_pmu.lbr_br_type = ecx.split.lbr_br_type;\n\tx86_pmu.lbr_nr = lbr_nr;\n\n\tif (x86_pmu.lbr_mispred)\n\t\tstatic_branch_enable(&x86_lbr_mispred);\n\tif (x86_pmu.lbr_timed_lbr)\n\t\tstatic_branch_enable(&x86_lbr_cycles);\n\tif (x86_pmu.lbr_br_type)\n\t\tstatic_branch_enable(&x86_lbr_type);\n\n\tarch_lbr_xsave = is_arch_lbr_xsave_available();\n\tif (arch_lbr_xsave) {\n\t\tsize = sizeof(struct x86_perf_task_context_arch_lbr_xsave) +\n\t\t       get_lbr_state_size();\n\t\tpmu->task_ctx_cache = create_lbr_kmem_cache(size,\n\t\t\t\t\t\t\t    XSAVE_ALIGNMENT);\n\t}\n\n\tif (!pmu->task_ctx_cache) {\n\t\tarch_lbr_xsave = false;\n\n\t\tsize = sizeof(struct x86_perf_task_context_arch_lbr) +\n\t\t       lbr_nr * sizeof(struct lbr_entry);\n\t\tpmu->task_ctx_cache = create_lbr_kmem_cache(size, 0);\n\t}\n\n\tx86_pmu.lbr_from = MSR_ARCH_LBR_FROM_0;\n\tx86_pmu.lbr_to = MSR_ARCH_LBR_TO_0;\n\tx86_pmu.lbr_info = MSR_ARCH_LBR_INFO_0;\n\n\t \n\tif (!x86_pmu.lbr_cpl ||\n\t    !x86_pmu.lbr_filter ||\n\t    !x86_pmu.lbr_call_stack)\n\t\tarch_lbr_ctl_map[PERF_SAMPLE_BRANCH_CALL_STACK_SHIFT] = LBR_NOT_SUPP;\n\n\tif (!x86_pmu.lbr_cpl) {\n\t\tarch_lbr_ctl_map[PERF_SAMPLE_BRANCH_USER_SHIFT] = LBR_NOT_SUPP;\n\t\tarch_lbr_ctl_map[PERF_SAMPLE_BRANCH_KERNEL_SHIFT] = LBR_NOT_SUPP;\n\t} else if (!x86_pmu.lbr_filter) {\n\t\tarch_lbr_ctl_map[PERF_SAMPLE_BRANCH_ANY_SHIFT] = LBR_NOT_SUPP;\n\t\tarch_lbr_ctl_map[PERF_SAMPLE_BRANCH_ANY_RETURN_SHIFT] = LBR_NOT_SUPP;\n\t\tarch_lbr_ctl_map[PERF_SAMPLE_BRANCH_ANY_CALL_SHIFT] = LBR_NOT_SUPP;\n\t\tarch_lbr_ctl_map[PERF_SAMPLE_BRANCH_IND_CALL_SHIFT] = LBR_NOT_SUPP;\n\t\tarch_lbr_ctl_map[PERF_SAMPLE_BRANCH_COND_SHIFT] = LBR_NOT_SUPP;\n\t\tarch_lbr_ctl_map[PERF_SAMPLE_BRANCH_IND_JUMP_SHIFT] = LBR_NOT_SUPP;\n\t\tarch_lbr_ctl_map[PERF_SAMPLE_BRANCH_CALL_SHIFT] = LBR_NOT_SUPP;\n\t}\n\n\tx86_pmu.lbr_ctl_mask = ARCH_LBR_CTL_MASK;\n\tx86_pmu.lbr_ctl_map  = arch_lbr_ctl_map;\n\n\tif (!x86_pmu.lbr_cpl && !x86_pmu.lbr_filter)\n\t\tx86_pmu.lbr_ctl_map = NULL;\n\n\tx86_pmu.lbr_reset = intel_pmu_arch_lbr_reset;\n\tif (arch_lbr_xsave) {\n\t\tx86_pmu.lbr_save = intel_pmu_arch_lbr_xsaves;\n\t\tx86_pmu.lbr_restore = intel_pmu_arch_lbr_xrstors;\n\t\tx86_pmu.lbr_read = intel_pmu_arch_lbr_read_xsave;\n\t\tpr_cont(\"XSAVE \");\n\t} else {\n\t\tx86_pmu.lbr_save = intel_pmu_arch_lbr_save;\n\t\tx86_pmu.lbr_restore = intel_pmu_arch_lbr_restore;\n\t\tx86_pmu.lbr_read = intel_pmu_arch_lbr_read;\n\t}\n\n\tpr_cont(\"Architectural LBR, \");\n\n\treturn;\n\nclear_arch_lbr:\n\tsetup_clear_cpu_cap(X86_FEATURE_ARCH_LBR);\n}\n\n \nvoid x86_perf_get_lbr(struct x86_pmu_lbr *lbr)\n{\n\tlbr->nr = x86_pmu.lbr_nr;\n\tlbr->from = x86_pmu.lbr_from;\n\tlbr->to = x86_pmu.lbr_to;\n\tlbr->info = x86_pmu.lbr_info;\n}\nEXPORT_SYMBOL_GPL(x86_perf_get_lbr);\n\nstruct event_constraint vlbr_constraint =\n\t__EVENT_CONSTRAINT(INTEL_FIXED_VLBR_EVENT, (1ULL << INTEL_PMC_IDX_FIXED_VLBR),\n\t\t\t  FIXED_EVENT_FLAGS, 1, 0, PERF_X86_EVENT_LBR_SELECT);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}