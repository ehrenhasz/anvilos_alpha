{
  "module_name": "core.c",
  "hash_id": "bac427dca596b3c8fd5b64ed351b5c7e3ecdb4a31646445898499575fb2ff213",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/events/intel/core.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/stddef.h>\n#include <linux/types.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/export.h>\n#include <linux/nmi.h>\n#include <linux/kvm_host.h>\n\n#include <asm/cpufeature.h>\n#include <asm/hardirq.h>\n#include <asm/intel-family.h>\n#include <asm/intel_pt.h>\n#include <asm/apic.h>\n#include <asm/cpu_device_id.h>\n\n#include \"../perf_event.h\"\n\n \nstatic u64 intel_perfmon_event_map[PERF_COUNT_HW_MAX] __read_mostly =\n{\n\t[PERF_COUNT_HW_CPU_CYCLES]\t\t= 0x003c,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t\t= 0x00c0,\n\t[PERF_COUNT_HW_CACHE_REFERENCES]\t= 0x4f2e,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t\t= 0x412e,\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS]\t= 0x00c4,\n\t[PERF_COUNT_HW_BRANCH_MISSES]\t\t= 0x00c5,\n\t[PERF_COUNT_HW_BUS_CYCLES]\t\t= 0x013c,\n\t[PERF_COUNT_HW_REF_CPU_CYCLES]\t\t= 0x0300,  \n};\n\nstatic struct event_constraint intel_core_event_constraints[] __read_mostly =\n{\n\tINTEL_EVENT_CONSTRAINT(0x11, 0x2),  \n\tINTEL_EVENT_CONSTRAINT(0x12, 0x2),  \n\tINTEL_EVENT_CONSTRAINT(0x13, 0x2),  \n\tINTEL_EVENT_CONSTRAINT(0x14, 0x1),  \n\tINTEL_EVENT_CONSTRAINT(0x19, 0x2),  \n\tINTEL_EVENT_CONSTRAINT(0xc1, 0x1),  \n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_core2_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0),  \n\tFIXED_EVENT_CONSTRAINT(0x003c, 1),  \n\tFIXED_EVENT_CONSTRAINT(0x0300, 2),  \n\tINTEL_EVENT_CONSTRAINT(0x10, 0x1),  \n\tINTEL_EVENT_CONSTRAINT(0x11, 0x2),  \n\tINTEL_EVENT_CONSTRAINT(0x12, 0x2),  \n\tINTEL_EVENT_CONSTRAINT(0x13, 0x2),  \n\tINTEL_EVENT_CONSTRAINT(0x14, 0x1),  \n\tINTEL_EVENT_CONSTRAINT(0x18, 0x1),  \n\tINTEL_EVENT_CONSTRAINT(0x19, 0x2),  \n\tINTEL_EVENT_CONSTRAINT(0xa1, 0x1),  \n\tINTEL_EVENT_CONSTRAINT(0xc9, 0x1),  \n\tINTEL_EVENT_CONSTRAINT(0xcb, 0x1),  \n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_nehalem_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0),  \n\tFIXED_EVENT_CONSTRAINT(0x003c, 1),  \n\tFIXED_EVENT_CONSTRAINT(0x0300, 2),  \n\tINTEL_EVENT_CONSTRAINT(0x40, 0x3),  \n\tINTEL_EVENT_CONSTRAINT(0x41, 0x3),  \n\tINTEL_EVENT_CONSTRAINT(0x42, 0x3),  \n\tINTEL_EVENT_CONSTRAINT(0x43, 0x3),  \n\tINTEL_EVENT_CONSTRAINT(0x48, 0x3),  \n\tINTEL_EVENT_CONSTRAINT(0x4e, 0x3),  \n\tINTEL_EVENT_CONSTRAINT(0x51, 0x3),  \n\tINTEL_EVENT_CONSTRAINT(0x63, 0x3),  \n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct extra_reg intel_nehalem_extra_regs[] __read_mostly =\n{\n\t \n\tINTEL_UEVENT_EXTRA_REG(0x01b7, MSR_OFFCORE_RSP_0, 0xffff, RSP_0),\n\tINTEL_UEVENT_PEBS_LDLAT_EXTRA_REG(0x100b),\n\tEVENT_EXTRA_END\n};\n\nstatic struct event_constraint intel_westmere_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0),  \n\tFIXED_EVENT_CONSTRAINT(0x003c, 1),  \n\tFIXED_EVENT_CONSTRAINT(0x0300, 2),  \n\tINTEL_EVENT_CONSTRAINT(0x51, 0x3),  \n\tINTEL_EVENT_CONSTRAINT(0x60, 0x1),  \n\tINTEL_EVENT_CONSTRAINT(0x63, 0x3),  \n\tINTEL_EVENT_CONSTRAINT(0xb3, 0x1),  \n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_snb_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0),  \n\tFIXED_EVENT_CONSTRAINT(0x003c, 1),  \n\tFIXED_EVENT_CONSTRAINT(0x0300, 2),  \n\tINTEL_UEVENT_CONSTRAINT(0x04a3, 0xf),  \n\tINTEL_UEVENT_CONSTRAINT(0x05a3, 0xf),  \n\tINTEL_UEVENT_CONSTRAINT(0x02a3, 0x4),  \n\tINTEL_UEVENT_CONSTRAINT(0x06a3, 0x4),  \n\tINTEL_EVENT_CONSTRAINT(0x48, 0x4),  \n\tINTEL_UEVENT_CONSTRAINT(0x01c0, 0x2),  \n\tINTEL_EVENT_CONSTRAINT(0xcd, 0x8),  \n\tINTEL_UEVENT_CONSTRAINT(0x04a3, 0xf),  \n\tINTEL_UEVENT_CONSTRAINT(0x02a3, 0x4),  \n\n\t \n\tINTEL_EXCLEVT_CONSTRAINT(0xd0, 0xf),  \n\tINTEL_EXCLEVT_CONSTRAINT(0xd1, 0xf),  \n\tINTEL_EXCLEVT_CONSTRAINT(0xd2, 0xf),  \n\tINTEL_EXCLEVT_CONSTRAINT(0xd3, 0xf),  \n\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_ivb_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0),  \n\tFIXED_EVENT_CONSTRAINT(0x003c, 1),  \n\tFIXED_EVENT_CONSTRAINT(0x0300, 2),  \n\tINTEL_UEVENT_CONSTRAINT(0x0148, 0x4),  \n\tINTEL_UEVENT_CONSTRAINT(0x0279, 0xf),  \n\tINTEL_UEVENT_CONSTRAINT(0x019c, 0xf),  \n\tINTEL_UEVENT_CONSTRAINT(0x02a3, 0xf),  \n\tINTEL_UEVENT_CONSTRAINT(0x04a3, 0xf),  \n\tINTEL_UEVENT_CONSTRAINT(0x05a3, 0xf),  \n\tINTEL_UEVENT_CONSTRAINT(0x06a3, 0xf),  \n\tINTEL_UEVENT_CONSTRAINT(0x08a3, 0x4),  \n\tINTEL_UEVENT_CONSTRAINT(0x0ca3, 0x4),  \n\tINTEL_UEVENT_CONSTRAINT(0x01c0, 0x2),  \n\n\t \n\tINTEL_EXCLEVT_CONSTRAINT(0xd0, 0xf),  \n\tINTEL_EXCLEVT_CONSTRAINT(0xd1, 0xf),  \n\tINTEL_EXCLEVT_CONSTRAINT(0xd2, 0xf),  \n\tINTEL_EXCLEVT_CONSTRAINT(0xd3, 0xf),  \n\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct extra_reg intel_westmere_extra_regs[] __read_mostly =\n{\n\t \n\tINTEL_UEVENT_EXTRA_REG(0x01b7, MSR_OFFCORE_RSP_0, 0xffff, RSP_0),\n\tINTEL_UEVENT_EXTRA_REG(0x01bb, MSR_OFFCORE_RSP_1, 0xffff, RSP_1),\n\tINTEL_UEVENT_PEBS_LDLAT_EXTRA_REG(0x100b),\n\tEVENT_EXTRA_END\n};\n\nstatic struct event_constraint intel_v1_event_constraints[] __read_mostly =\n{\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_gen_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0),  \n\tFIXED_EVENT_CONSTRAINT(0x003c, 1),  \n\tFIXED_EVENT_CONSTRAINT(0x0300, 2),  \n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_v5_gen_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0),  \n\tFIXED_EVENT_CONSTRAINT(0x003c, 1),  \n\tFIXED_EVENT_CONSTRAINT(0x0300, 2),  \n\tFIXED_EVENT_CONSTRAINT(0x0400, 3),  \n\tFIXED_EVENT_CONSTRAINT(0x0500, 4),\n\tFIXED_EVENT_CONSTRAINT(0x0600, 5),\n\tFIXED_EVENT_CONSTRAINT(0x0700, 6),\n\tFIXED_EVENT_CONSTRAINT(0x0800, 7),\n\tFIXED_EVENT_CONSTRAINT(0x0900, 8),\n\tFIXED_EVENT_CONSTRAINT(0x0a00, 9),\n\tFIXED_EVENT_CONSTRAINT(0x0b00, 10),\n\tFIXED_EVENT_CONSTRAINT(0x0c00, 11),\n\tFIXED_EVENT_CONSTRAINT(0x0d00, 12),\n\tFIXED_EVENT_CONSTRAINT(0x0e00, 13),\n\tFIXED_EVENT_CONSTRAINT(0x0f00, 14),\n\tFIXED_EVENT_CONSTRAINT(0x1000, 15),\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_slm_event_constraints[] __read_mostly =\n{\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0),  \n\tFIXED_EVENT_CONSTRAINT(0x003c, 1),  \n\tFIXED_EVENT_CONSTRAINT(0x0300, 2),  \n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_skl_event_constraints[] = {\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0),\t \n\tFIXED_EVENT_CONSTRAINT(0x003c, 1),\t \n\tFIXED_EVENT_CONSTRAINT(0x0300, 2),\t \n\tINTEL_UEVENT_CONSTRAINT(0x1c0, 0x2),\t \n\n\t \n\tINTEL_EVENT_CONSTRAINT(0xd0, 0xf),\t \n\tINTEL_EVENT_CONSTRAINT(0xd1, 0xf),\t \n\tINTEL_EVENT_CONSTRAINT(0xd2, 0xf),\t \n\tINTEL_EVENT_CONSTRAINT(0xcd, 0xf),\t \n\tINTEL_EVENT_CONSTRAINT(0xc6, 0xf),\t \n\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct extra_reg intel_knl_extra_regs[] __read_mostly = {\n\tINTEL_UEVENT_EXTRA_REG(0x01b7, MSR_OFFCORE_RSP_0, 0x799ffbb6e7ull, RSP_0),\n\tINTEL_UEVENT_EXTRA_REG(0x02b7, MSR_OFFCORE_RSP_1, 0x399ffbffe7ull, RSP_1),\n\tEVENT_EXTRA_END\n};\n\nstatic struct extra_reg intel_snb_extra_regs[] __read_mostly = {\n\t \n\tINTEL_UEVENT_EXTRA_REG(0x01b7, MSR_OFFCORE_RSP_0, 0x3f807f8fffull, RSP_0),\n\tINTEL_UEVENT_EXTRA_REG(0x01bb, MSR_OFFCORE_RSP_1, 0x3f807f8fffull, RSP_1),\n\tINTEL_UEVENT_PEBS_LDLAT_EXTRA_REG(0x01cd),\n\tEVENT_EXTRA_END\n};\n\nstatic struct extra_reg intel_snbep_extra_regs[] __read_mostly = {\n\t \n\tINTEL_UEVENT_EXTRA_REG(0x01b7, MSR_OFFCORE_RSP_0, 0x3fffff8fffull, RSP_0),\n\tINTEL_UEVENT_EXTRA_REG(0x01bb, MSR_OFFCORE_RSP_1, 0x3fffff8fffull, RSP_1),\n\tINTEL_UEVENT_PEBS_LDLAT_EXTRA_REG(0x01cd),\n\tEVENT_EXTRA_END\n};\n\nstatic struct extra_reg intel_skl_extra_regs[] __read_mostly = {\n\tINTEL_UEVENT_EXTRA_REG(0x01b7, MSR_OFFCORE_RSP_0, 0x3fffff8fffull, RSP_0),\n\tINTEL_UEVENT_EXTRA_REG(0x01bb, MSR_OFFCORE_RSP_1, 0x3fffff8fffull, RSP_1),\n\tINTEL_UEVENT_PEBS_LDLAT_EXTRA_REG(0x01cd),\n\t \n\tINTEL_UEVENT_EXTRA_REG(0x01c6, MSR_PEBS_FRONTEND, 0x7fff17, FE),\n\tEVENT_EXTRA_END\n};\n\nstatic struct event_constraint intel_icl_event_constraints[] = {\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0),\t \n\tFIXED_EVENT_CONSTRAINT(0x01c0, 0),\t \n\tFIXED_EVENT_CONSTRAINT(0x0100, 0),\t \n\tFIXED_EVENT_CONSTRAINT(0x003c, 1),\t \n\tFIXED_EVENT_CONSTRAINT(0x0300, 2),\t \n\tFIXED_EVENT_CONSTRAINT(0x0400, 3),\t \n\tMETRIC_EVENT_CONSTRAINT(INTEL_TD_METRIC_RETIRING, 0),\n\tMETRIC_EVENT_CONSTRAINT(INTEL_TD_METRIC_BAD_SPEC, 1),\n\tMETRIC_EVENT_CONSTRAINT(INTEL_TD_METRIC_FE_BOUND, 2),\n\tMETRIC_EVENT_CONSTRAINT(INTEL_TD_METRIC_BE_BOUND, 3),\n\tINTEL_EVENT_CONSTRAINT_RANGE(0x03, 0x0a, 0xf),\n\tINTEL_EVENT_CONSTRAINT_RANGE(0x1f, 0x28, 0xf),\n\tINTEL_EVENT_CONSTRAINT(0x32, 0xf),\t \n\tINTEL_EVENT_CONSTRAINT_RANGE(0x48, 0x56, 0xf),\n\tINTEL_EVENT_CONSTRAINT_RANGE(0x60, 0x8b, 0xf),\n\tINTEL_UEVENT_CONSTRAINT(0x04a3, 0xff),   \n\tINTEL_UEVENT_CONSTRAINT(0x10a3, 0xff),   \n\tINTEL_UEVENT_CONSTRAINT(0x14a3, 0xff),   \n\tINTEL_EVENT_CONSTRAINT(0xa3, 0xf),       \n\tINTEL_EVENT_CONSTRAINT_RANGE(0xa8, 0xb0, 0xf),\n\tINTEL_EVENT_CONSTRAINT_RANGE(0xb7, 0xbd, 0xf),\n\tINTEL_EVENT_CONSTRAINT_RANGE(0xd0, 0xe6, 0xf),\n\tINTEL_EVENT_CONSTRAINT(0xef, 0xf),\n\tINTEL_EVENT_CONSTRAINT_RANGE(0xf0, 0xf4, 0xf),\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct extra_reg intel_icl_extra_regs[] __read_mostly = {\n\tINTEL_UEVENT_EXTRA_REG(0x01b7, MSR_OFFCORE_RSP_0, 0x3fffffbfffull, RSP_0),\n\tINTEL_UEVENT_EXTRA_REG(0x01bb, MSR_OFFCORE_RSP_1, 0x3fffffbfffull, RSP_1),\n\tINTEL_UEVENT_PEBS_LDLAT_EXTRA_REG(0x01cd),\n\tINTEL_UEVENT_EXTRA_REG(0x01c6, MSR_PEBS_FRONTEND, 0x7fff17, FE),\n\tEVENT_EXTRA_END\n};\n\nstatic struct extra_reg intel_spr_extra_regs[] __read_mostly = {\n\tINTEL_UEVENT_EXTRA_REG(0x012a, MSR_OFFCORE_RSP_0, 0x3fffffffffull, RSP_0),\n\tINTEL_UEVENT_EXTRA_REG(0x012b, MSR_OFFCORE_RSP_1, 0x3fffffffffull, RSP_1),\n\tINTEL_UEVENT_PEBS_LDLAT_EXTRA_REG(0x01cd),\n\tINTEL_UEVENT_EXTRA_REG(0x01c6, MSR_PEBS_FRONTEND, 0x7fff1f, FE),\n\tINTEL_UEVENT_EXTRA_REG(0x40ad, MSR_PEBS_FRONTEND, 0x7, FE),\n\tINTEL_UEVENT_EXTRA_REG(0x04c2, MSR_PEBS_FRONTEND, 0x8, FE),\n\tEVENT_EXTRA_END\n};\n\nstatic struct event_constraint intel_spr_event_constraints[] = {\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0),\t \n\tFIXED_EVENT_CONSTRAINT(0x0100, 0),\t \n\tFIXED_EVENT_CONSTRAINT(0x003c, 1),\t \n\tFIXED_EVENT_CONSTRAINT(0x0300, 2),\t \n\tFIXED_EVENT_CONSTRAINT(0x0400, 3),\t \n\tMETRIC_EVENT_CONSTRAINT(INTEL_TD_METRIC_RETIRING, 0),\n\tMETRIC_EVENT_CONSTRAINT(INTEL_TD_METRIC_BAD_SPEC, 1),\n\tMETRIC_EVENT_CONSTRAINT(INTEL_TD_METRIC_FE_BOUND, 2),\n\tMETRIC_EVENT_CONSTRAINT(INTEL_TD_METRIC_BE_BOUND, 3),\n\tMETRIC_EVENT_CONSTRAINT(INTEL_TD_METRIC_HEAVY_OPS, 4),\n\tMETRIC_EVENT_CONSTRAINT(INTEL_TD_METRIC_BR_MISPREDICT, 5),\n\tMETRIC_EVENT_CONSTRAINT(INTEL_TD_METRIC_FETCH_LAT, 6),\n\tMETRIC_EVENT_CONSTRAINT(INTEL_TD_METRIC_MEM_BOUND, 7),\n\n\tINTEL_EVENT_CONSTRAINT(0x2e, 0xff),\n\tINTEL_EVENT_CONSTRAINT(0x3c, 0xff),\n\t \n\tINTEL_EVENT_CONSTRAINT_RANGE(0x01, 0x8f, 0xf),\n\n\tINTEL_UEVENT_CONSTRAINT(0x01a3, 0xf),\n\tINTEL_UEVENT_CONSTRAINT(0x02a3, 0xf),\n\tINTEL_UEVENT_CONSTRAINT(0x08a3, 0xf),\n\tINTEL_UEVENT_CONSTRAINT(0x04a4, 0x1),\n\tINTEL_UEVENT_CONSTRAINT(0x08a4, 0x1),\n\tINTEL_UEVENT_CONSTRAINT(0x02cd, 0x1),\n\tINTEL_EVENT_CONSTRAINT(0xce, 0x1),\n\tINTEL_EVENT_CONSTRAINT_RANGE(0xd0, 0xdf, 0xf),\n\t \n\tINTEL_EVENT_CONSTRAINT_RANGE(0x90, 0xfe, 0xff),\n\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct extra_reg intel_gnr_extra_regs[] __read_mostly = {\n\tINTEL_UEVENT_EXTRA_REG(0x012a, MSR_OFFCORE_RSP_0, 0x3fffffffffull, RSP_0),\n\tINTEL_UEVENT_EXTRA_REG(0x012b, MSR_OFFCORE_RSP_1, 0x3fffffffffull, RSP_1),\n\tINTEL_UEVENT_PEBS_LDLAT_EXTRA_REG(0x01cd),\n\tINTEL_UEVENT_EXTRA_REG(0x02c6, MSR_PEBS_FRONTEND, 0x9, FE),\n\tINTEL_UEVENT_EXTRA_REG(0x03c6, MSR_PEBS_FRONTEND, 0x7fff1f, FE),\n\tINTEL_UEVENT_EXTRA_REG(0x40ad, MSR_PEBS_FRONTEND, 0x7, FE),\n\tINTEL_UEVENT_EXTRA_REG(0x04c2, MSR_PEBS_FRONTEND, 0x8, FE),\n\tEVENT_EXTRA_END\n};\n\nEVENT_ATTR_STR(mem-loads,\tmem_ld_nhm,\t\"event=0x0b,umask=0x10,ldlat=3\");\nEVENT_ATTR_STR(mem-loads,\tmem_ld_snb,\t\"event=0xcd,umask=0x1,ldlat=3\");\nEVENT_ATTR_STR(mem-stores,\tmem_st_snb,\t\"event=0xcd,umask=0x2\");\n\nstatic struct attribute *nhm_mem_events_attrs[] = {\n\tEVENT_PTR(mem_ld_nhm),\n\tNULL,\n};\n\n \n\nEVENT_ATTR_STR_HT(topdown-total-slots, td_total_slots,\n\t\"event=0x3c,umask=0x0\",\t\t\t \n\t\"event=0x3c,umask=0x0,any=1\");\t\t \nEVENT_ATTR_STR_HT(topdown-total-slots.scale, td_total_slots_scale, \"4\", \"2\");\nEVENT_ATTR_STR(topdown-slots-issued, td_slots_issued,\n\t\"event=0xe,umask=0x1\");\t\t\t \nEVENT_ATTR_STR(topdown-slots-retired, td_slots_retired,\n\t\"event=0xc2,umask=0x2\");\t\t \nEVENT_ATTR_STR(topdown-fetch-bubbles, td_fetch_bubbles,\n\t\"event=0x9c,umask=0x1\");\t\t \nEVENT_ATTR_STR_HT(topdown-recovery-bubbles, td_recovery_bubbles,\n\t\"event=0xd,umask=0x3,cmask=1\",\t\t \n\t\"event=0xd,umask=0x3,cmask=1,any=1\");\t \nEVENT_ATTR_STR_HT(topdown-recovery-bubbles.scale, td_recovery_bubbles_scale,\n\t\"4\", \"2\");\n\nEVENT_ATTR_STR(slots,\t\t\tslots,\t\t\t\"event=0x00,umask=0x4\");\nEVENT_ATTR_STR(topdown-retiring,\ttd_retiring,\t\t\"event=0x00,umask=0x80\");\nEVENT_ATTR_STR(topdown-bad-spec,\ttd_bad_spec,\t\t\"event=0x00,umask=0x81\");\nEVENT_ATTR_STR(topdown-fe-bound,\ttd_fe_bound,\t\t\"event=0x00,umask=0x82\");\nEVENT_ATTR_STR(topdown-be-bound,\ttd_be_bound,\t\t\"event=0x00,umask=0x83\");\nEVENT_ATTR_STR(topdown-heavy-ops,\ttd_heavy_ops,\t\t\"event=0x00,umask=0x84\");\nEVENT_ATTR_STR(topdown-br-mispredict,\ttd_br_mispredict,\t\"event=0x00,umask=0x85\");\nEVENT_ATTR_STR(topdown-fetch-lat,\ttd_fetch_lat,\t\t\"event=0x00,umask=0x86\");\nEVENT_ATTR_STR(topdown-mem-bound,\ttd_mem_bound,\t\t\"event=0x00,umask=0x87\");\n\nstatic struct attribute *snb_events_attrs[] = {\n\tEVENT_PTR(td_slots_issued),\n\tEVENT_PTR(td_slots_retired),\n\tEVENT_PTR(td_fetch_bubbles),\n\tEVENT_PTR(td_total_slots),\n\tEVENT_PTR(td_total_slots_scale),\n\tEVENT_PTR(td_recovery_bubbles),\n\tEVENT_PTR(td_recovery_bubbles_scale),\n\tNULL,\n};\n\nstatic struct attribute *snb_mem_events_attrs[] = {\n\tEVENT_PTR(mem_ld_snb),\n\tEVENT_PTR(mem_st_snb),\n\tNULL,\n};\n\nstatic struct event_constraint intel_hsw_event_constraints[] = {\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0),  \n\tFIXED_EVENT_CONSTRAINT(0x003c, 1),  \n\tFIXED_EVENT_CONSTRAINT(0x0300, 2),  \n\tINTEL_UEVENT_CONSTRAINT(0x148, 0x4),\t \n\tINTEL_UEVENT_CONSTRAINT(0x01c0, 0x2),  \n\tINTEL_EVENT_CONSTRAINT(0xcd, 0x8),  \n\t \n\tINTEL_UEVENT_CONSTRAINT(0x08a3, 0x4),\n\t \n\tINTEL_UEVENT_CONSTRAINT(0x0ca3, 0x4),\n\t \n\tINTEL_UEVENT_CONSTRAINT(0x04a3, 0xf),\n\n\t \n\tINTEL_EXCLEVT_CONSTRAINT(0xd0, 0xf),  \n\tINTEL_EXCLEVT_CONSTRAINT(0xd1, 0xf),  \n\tINTEL_EXCLEVT_CONSTRAINT(0xd2, 0xf),  \n\tINTEL_EXCLEVT_CONSTRAINT(0xd3, 0xf),  \n\n\tEVENT_CONSTRAINT_END\n};\n\nstatic struct event_constraint intel_bdw_event_constraints[] = {\n\tFIXED_EVENT_CONSTRAINT(0x00c0, 0),\t \n\tFIXED_EVENT_CONSTRAINT(0x003c, 1),\t \n\tFIXED_EVENT_CONSTRAINT(0x0300, 2),\t \n\tINTEL_UEVENT_CONSTRAINT(0x148, 0x4),\t \n\tINTEL_UBIT_EVENT_CONSTRAINT(0x8a3, 0x4),\t \n\t \n\tINTEL_EVENT_CONSTRAINT(0xd0, 0xf),\t \n\tINTEL_EVENT_CONSTRAINT(0xd1, 0xf),\t \n\tINTEL_EVENT_CONSTRAINT(0xd2, 0xf),\t \n\tINTEL_EVENT_CONSTRAINT(0xcd, 0xf),\t \n\tEVENT_CONSTRAINT_END\n};\n\nstatic u64 intel_pmu_event_map(int hw_event)\n{\n\treturn intel_perfmon_event_map[hw_event];\n}\n\nstatic __initconst const u64 spr_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x81d0,\n\t\t[ C(RESULT_MISS)   ] = 0xe124,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x82d0,\n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_MISS)   ] = 0xe424,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x12a,\n\t\t[ C(RESULT_MISS)   ] = 0x12a,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x12a,\n\t\t[ C(RESULT_MISS)   ] = 0x12a,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x81d0,\n\t\t[ C(RESULT_MISS)   ] = 0xe12,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x82d0,\n\t\t[ C(RESULT_MISS)   ] = 0xe13,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = 0xe11,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x4c4,\n\t\t[ C(RESULT_MISS)   ] = 0x4c5,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(NODE) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x12a,\n\t\t[ C(RESULT_MISS)   ] = 0x12a,\n\t},\n },\n};\n\nstatic __initconst const u64 spr_hw_cache_extra_regs\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x10001,\n\t\t[ C(RESULT_MISS)   ] = 0x3fbfc00001,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x3f3ffc0002,\n\t\t[ C(RESULT_MISS)   ] = 0x3f3fc00002,\n\t},\n },\n [ C(NODE) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x10c000001,\n\t\t[ C(RESULT_MISS)   ] = 0x3fb3000001,\n\t},\n },\n};\n\n \n\n#define SKL_DEMAND_DATA_RD\t\tBIT_ULL(0)\n#define SKL_DEMAND_RFO\t\t\tBIT_ULL(1)\n#define SKL_ANY_RESPONSE\t\tBIT_ULL(16)\n#define SKL_SUPPLIER_NONE\t\tBIT_ULL(17)\n#define SKL_L3_MISS_LOCAL_DRAM\t\tBIT_ULL(26)\n#define SKL_L3_MISS_REMOTE_HOP0_DRAM\tBIT_ULL(27)\n#define SKL_L3_MISS_REMOTE_HOP1_DRAM\tBIT_ULL(28)\n#define SKL_L3_MISS_REMOTE_HOP2P_DRAM\tBIT_ULL(29)\n#define SKL_L3_MISS\t\t\t(SKL_L3_MISS_LOCAL_DRAM| \\\n\t\t\t\t\t SKL_L3_MISS_REMOTE_HOP0_DRAM| \\\n\t\t\t\t\t SKL_L3_MISS_REMOTE_HOP1_DRAM| \\\n\t\t\t\t\t SKL_L3_MISS_REMOTE_HOP2P_DRAM)\n#define SKL_SPL_HIT\t\t\tBIT_ULL(30)\n#define SKL_SNOOP_NONE\t\t\tBIT_ULL(31)\n#define SKL_SNOOP_NOT_NEEDED\t\tBIT_ULL(32)\n#define SKL_SNOOP_MISS\t\t\tBIT_ULL(33)\n#define SKL_SNOOP_HIT_NO_FWD\t\tBIT_ULL(34)\n#define SKL_SNOOP_HIT_WITH_FWD\t\tBIT_ULL(35)\n#define SKL_SNOOP_HITM\t\t\tBIT_ULL(36)\n#define SKL_SNOOP_NON_DRAM\t\tBIT_ULL(37)\n#define SKL_ANY_SNOOP\t\t\t(SKL_SPL_HIT|SKL_SNOOP_NONE| \\\n\t\t\t\t\t SKL_SNOOP_NOT_NEEDED|SKL_SNOOP_MISS| \\\n\t\t\t\t\t SKL_SNOOP_HIT_NO_FWD|SKL_SNOOP_HIT_WITH_FWD| \\\n\t\t\t\t\t SKL_SNOOP_HITM|SKL_SNOOP_NON_DRAM)\n#define SKL_DEMAND_READ\t\t\tSKL_DEMAND_DATA_RD\n#define SKL_SNOOP_DRAM\t\t\t(SKL_SNOOP_NONE| \\\n\t\t\t\t\t SKL_SNOOP_NOT_NEEDED|SKL_SNOOP_MISS| \\\n\t\t\t\t\t SKL_SNOOP_HIT_NO_FWD|SKL_SNOOP_HIT_WITH_FWD| \\\n\t\t\t\t\t SKL_SNOOP_HITM|SKL_SPL_HIT)\n#define SKL_DEMAND_WRITE\t\tSKL_DEMAND_RFO\n#define SKL_LLC_ACCESS\t\t\tSKL_ANY_RESPONSE\n#define SKL_L3_MISS_REMOTE\t\t(SKL_L3_MISS_REMOTE_HOP0_DRAM| \\\n\t\t\t\t\t SKL_L3_MISS_REMOTE_HOP1_DRAM| \\\n\t\t\t\t\t SKL_L3_MISS_REMOTE_HOP2P_DRAM)\n\nstatic __initconst const u64 skl_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x81d0,\t \n\t\t[ C(RESULT_MISS)   ] = 0x151,\t \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x82d0,\t \n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x283,\t \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x1b7,\t \n\t\t[ C(RESULT_MISS)   ] = 0x1b7,\t \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x1b7,\t \n\t\t[ C(RESULT_MISS)   ] = 0x1b7,\t \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x81d0,\t \n\t\t[ C(RESULT_MISS)   ] = 0xe08,\t \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x82d0,\t \n\t\t[ C(RESULT_MISS)   ] = 0xe49,\t \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x2085,\t \n\t\t[ C(RESULT_MISS)   ] = 0xe85,\t \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0xc4,\t \n\t\t[ C(RESULT_MISS)   ] = 0xc5,\t \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(NODE) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x1b7,\t \n\t\t[ C(RESULT_MISS)   ] = 0x1b7,\t \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x1b7,\t \n\t\t[ C(RESULT_MISS)   ] = 0x1b7,\t \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n};\n\nstatic __initconst const u64 skl_hw_cache_extra_regs\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = SKL_DEMAND_READ|\n\t\t\t\t       SKL_LLC_ACCESS|SKL_ANY_SNOOP,\n\t\t[ C(RESULT_MISS)   ] = SKL_DEMAND_READ|\n\t\t\t\t       SKL_L3_MISS|SKL_ANY_SNOOP|\n\t\t\t\t       SKL_SUPPLIER_NONE,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = SKL_DEMAND_WRITE|\n\t\t\t\t       SKL_LLC_ACCESS|SKL_ANY_SNOOP,\n\t\t[ C(RESULT_MISS)   ] = SKL_DEMAND_WRITE|\n\t\t\t\t       SKL_L3_MISS|SKL_ANY_SNOOP|\n\t\t\t\t       SKL_SUPPLIER_NONE,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(NODE) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = SKL_DEMAND_READ|\n\t\t\t\t       SKL_L3_MISS_LOCAL_DRAM|SKL_SNOOP_DRAM,\n\t\t[ C(RESULT_MISS)   ] = SKL_DEMAND_READ|\n\t\t\t\t       SKL_L3_MISS_REMOTE|SKL_SNOOP_DRAM,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = SKL_DEMAND_WRITE|\n\t\t\t\t       SKL_L3_MISS_LOCAL_DRAM|SKL_SNOOP_DRAM,\n\t\t[ C(RESULT_MISS)   ] = SKL_DEMAND_WRITE|\n\t\t\t\t       SKL_L3_MISS_REMOTE|SKL_SNOOP_DRAM,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n};\n\n#define SNB_DMND_DATA_RD\t(1ULL << 0)\n#define SNB_DMND_RFO\t\t(1ULL << 1)\n#define SNB_DMND_IFETCH\t\t(1ULL << 2)\n#define SNB_DMND_WB\t\t(1ULL << 3)\n#define SNB_PF_DATA_RD\t\t(1ULL << 4)\n#define SNB_PF_RFO\t\t(1ULL << 5)\n#define SNB_PF_IFETCH\t\t(1ULL << 6)\n#define SNB_LLC_DATA_RD\t\t(1ULL << 7)\n#define SNB_LLC_RFO\t\t(1ULL << 8)\n#define SNB_LLC_IFETCH\t\t(1ULL << 9)\n#define SNB_BUS_LOCKS\t\t(1ULL << 10)\n#define SNB_STRM_ST\t\t(1ULL << 11)\n#define SNB_OTHER\t\t(1ULL << 15)\n#define SNB_RESP_ANY\t\t(1ULL << 16)\n#define SNB_NO_SUPP\t\t(1ULL << 17)\n#define SNB_LLC_HITM\t\t(1ULL << 18)\n#define SNB_LLC_HITE\t\t(1ULL << 19)\n#define SNB_LLC_HITS\t\t(1ULL << 20)\n#define SNB_LLC_HITF\t\t(1ULL << 21)\n#define SNB_LOCAL\t\t(1ULL << 22)\n#define SNB_REMOTE\t\t(0xffULL << 23)\n#define SNB_SNP_NONE\t\t(1ULL << 31)\n#define SNB_SNP_NOT_NEEDED\t(1ULL << 32)\n#define SNB_SNP_MISS\t\t(1ULL << 33)\n#define SNB_NO_FWD\t\t(1ULL << 34)\n#define SNB_SNP_FWD\t\t(1ULL << 35)\n#define SNB_HITM\t\t(1ULL << 36)\n#define SNB_NON_DRAM\t\t(1ULL << 37)\n\n#define SNB_DMND_READ\t\t(SNB_DMND_DATA_RD|SNB_LLC_DATA_RD)\n#define SNB_DMND_WRITE\t\t(SNB_DMND_RFO|SNB_LLC_RFO)\n#define SNB_DMND_PREFETCH\t(SNB_PF_DATA_RD|SNB_PF_RFO)\n\n#define SNB_SNP_ANY\t\t(SNB_SNP_NONE|SNB_SNP_NOT_NEEDED| \\\n\t\t\t\t SNB_SNP_MISS|SNB_NO_FWD|SNB_SNP_FWD| \\\n\t\t\t\t SNB_HITM)\n\n#define SNB_DRAM_ANY\t\t(SNB_LOCAL|SNB_REMOTE|SNB_SNP_ANY)\n#define SNB_DRAM_REMOTE\t\t(SNB_REMOTE|SNB_SNP_ANY)\n\n#define SNB_L3_ACCESS\t\tSNB_RESP_ANY\n#define SNB_L3_MISS\t\t(SNB_DRAM_ANY|SNB_NON_DRAM)\n\nstatic __initconst const u64 snb_hw_cache_extra_regs\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = SNB_DMND_READ|SNB_L3_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = SNB_DMND_READ|SNB_L3_MISS,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = SNB_DMND_WRITE|SNB_L3_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = SNB_DMND_WRITE|SNB_L3_MISS,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = SNB_DMND_PREFETCH|SNB_L3_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = SNB_DMND_PREFETCH|SNB_L3_MISS,\n\t},\n },\n [ C(NODE) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = SNB_DMND_READ|SNB_DRAM_ANY,\n\t\t[ C(RESULT_MISS)   ] = SNB_DMND_READ|SNB_DRAM_REMOTE,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = SNB_DMND_WRITE|SNB_DRAM_ANY,\n\t\t[ C(RESULT_MISS)   ] = SNB_DMND_WRITE|SNB_DRAM_REMOTE,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = SNB_DMND_PREFETCH|SNB_DRAM_ANY,\n\t\t[ C(RESULT_MISS)   ] = SNB_DMND_PREFETCH|SNB_DRAM_REMOTE,\n\t},\n },\n};\n\nstatic __initconst const u64 snb_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0xf1d0,  \n\t\t[ C(RESULT_MISS)   ] = 0x0151,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0xf2d0,  \n\t\t[ C(RESULT_MISS)   ] = 0x0851,  \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x024e,  \n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0280,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t \n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t \n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t \n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t \n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t \n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t \n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x81d0,  \n\t\t[ C(RESULT_MISS)   ] = 0x0108,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x82d0,  \n\t\t[ C(RESULT_MISS)   ] = 0x0149,  \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x1085,  \n\t\t[ C(RESULT_MISS)   ] = 0x0185,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c4,  \n\t\t[ C(RESULT_MISS)   ] = 0x00c5,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(NODE) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n },\n\n};\n\n \n\n#define HSW_DEMAND_DATA_RD\t\tBIT_ULL(0)\n#define HSW_DEMAND_RFO\t\t\tBIT_ULL(1)\n#define HSW_ANY_RESPONSE\t\tBIT_ULL(16)\n#define HSW_SUPPLIER_NONE\t\tBIT_ULL(17)\n#define HSW_L3_MISS_LOCAL_DRAM\t\tBIT_ULL(22)\n#define HSW_L3_MISS_REMOTE_HOP0\t\tBIT_ULL(27)\n#define HSW_L3_MISS_REMOTE_HOP1\t\tBIT_ULL(28)\n#define HSW_L3_MISS_REMOTE_HOP2P\tBIT_ULL(29)\n#define HSW_L3_MISS\t\t\t(HSW_L3_MISS_LOCAL_DRAM| \\\n\t\t\t\t\t HSW_L3_MISS_REMOTE_HOP0|HSW_L3_MISS_REMOTE_HOP1| \\\n\t\t\t\t\t HSW_L3_MISS_REMOTE_HOP2P)\n#define HSW_SNOOP_NONE\t\t\tBIT_ULL(31)\n#define HSW_SNOOP_NOT_NEEDED\t\tBIT_ULL(32)\n#define HSW_SNOOP_MISS\t\t\tBIT_ULL(33)\n#define HSW_SNOOP_HIT_NO_FWD\t\tBIT_ULL(34)\n#define HSW_SNOOP_HIT_WITH_FWD\t\tBIT_ULL(35)\n#define HSW_SNOOP_HITM\t\t\tBIT_ULL(36)\n#define HSW_SNOOP_NON_DRAM\t\tBIT_ULL(37)\n#define HSW_ANY_SNOOP\t\t\t(HSW_SNOOP_NONE| \\\n\t\t\t\t\t HSW_SNOOP_NOT_NEEDED|HSW_SNOOP_MISS| \\\n\t\t\t\t\t HSW_SNOOP_HIT_NO_FWD|HSW_SNOOP_HIT_WITH_FWD| \\\n\t\t\t\t\t HSW_SNOOP_HITM|HSW_SNOOP_NON_DRAM)\n#define HSW_SNOOP_DRAM\t\t\t(HSW_ANY_SNOOP & ~HSW_SNOOP_NON_DRAM)\n#define HSW_DEMAND_READ\t\t\tHSW_DEMAND_DATA_RD\n#define HSW_DEMAND_WRITE\t\tHSW_DEMAND_RFO\n#define HSW_L3_MISS_REMOTE\t\t(HSW_L3_MISS_REMOTE_HOP0|\\\n\t\t\t\t\t HSW_L3_MISS_REMOTE_HOP1|HSW_L3_MISS_REMOTE_HOP2P)\n#define HSW_LLC_ACCESS\t\t\tHSW_ANY_RESPONSE\n\n#define BDW_L3_MISS_LOCAL\t\tBIT(26)\n#define BDW_L3_MISS\t\t\t(BDW_L3_MISS_LOCAL| \\\n\t\t\t\t\t HSW_L3_MISS_REMOTE_HOP0|HSW_L3_MISS_REMOTE_HOP1| \\\n\t\t\t\t\t HSW_L3_MISS_REMOTE_HOP2P)\n\n\nstatic __initconst const u64 hsw_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x81d0,\t \n\t\t[ C(RESULT_MISS)   ] = 0x151,\t \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x82d0,\t \n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x280,\t \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x1b7,\t \n\t\t[ C(RESULT_MISS)   ] = 0x1b7,\t \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x1b7,\t \n\t\t[ C(RESULT_MISS)   ] = 0x1b7,\t \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x81d0,\t \n\t\t[ C(RESULT_MISS)   ] = 0x108,\t \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x82d0,\t \n\t\t[ C(RESULT_MISS)   ] = 0x149,\t \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x6085,\t \n\t\t[ C(RESULT_MISS)   ] = 0x185,\t \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0xc4,\t \n\t\t[ C(RESULT_MISS)   ] = 0xc5,\t \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(NODE) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x1b7,\t \n\t\t[ C(RESULT_MISS)   ] = 0x1b7,\t \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x1b7,\t \n\t\t[ C(RESULT_MISS)   ] = 0x1b7,\t \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n};\n\nstatic __initconst const u64 hsw_hw_cache_extra_regs\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = HSW_DEMAND_READ|\n\t\t\t\t       HSW_LLC_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = HSW_DEMAND_READ|\n\t\t\t\t       HSW_L3_MISS|HSW_ANY_SNOOP,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = HSW_DEMAND_WRITE|\n\t\t\t\t       HSW_LLC_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = HSW_DEMAND_WRITE|\n\t\t\t\t       HSW_L3_MISS|HSW_ANY_SNOOP,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(NODE) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = HSW_DEMAND_READ|\n\t\t\t\t       HSW_L3_MISS_LOCAL_DRAM|\n\t\t\t\t       HSW_SNOOP_DRAM,\n\t\t[ C(RESULT_MISS)   ] = HSW_DEMAND_READ|\n\t\t\t\t       HSW_L3_MISS_REMOTE|\n\t\t\t\t       HSW_SNOOP_DRAM,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = HSW_DEMAND_WRITE|\n\t\t\t\t       HSW_L3_MISS_LOCAL_DRAM|\n\t\t\t\t       HSW_SNOOP_DRAM,\n\t\t[ C(RESULT_MISS)   ] = HSW_DEMAND_WRITE|\n\t\t\t\t       HSW_L3_MISS_REMOTE|\n\t\t\t\t       HSW_SNOOP_DRAM,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n};\n\nstatic __initconst const u64 westmere_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x010b,  \n\t\t[ C(RESULT_MISS)   ] = 0x0151,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x020b,  \n\t\t[ C(RESULT_MISS)   ] = 0x0251,  \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x014e,  \n\t\t[ C(RESULT_MISS)   ] = 0x024e,  \n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0380,  \n\t\t[ C(RESULT_MISS)   ] = 0x0280,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t \n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t \n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t \n\t[ C(OP_WRITE) ] = {\n\t\t \n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t \n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t \n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t \n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x010b,  \n\t\t[ C(RESULT_MISS)   ] = 0x0108,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x020b,  \n\t\t[ C(RESULT_MISS)   ] = 0x010c,  \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x01c0,  \n\t\t[ C(RESULT_MISS)   ] = 0x0185,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c4,  \n\t\t[ C(RESULT_MISS)   ] = 0x03e8,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(NODE) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n },\n};\n\n \n\n#define NHM_DMND_DATA_RD\t(1 << 0)\n#define NHM_DMND_RFO\t\t(1 << 1)\n#define NHM_DMND_IFETCH\t\t(1 << 2)\n#define NHM_DMND_WB\t\t(1 << 3)\n#define NHM_PF_DATA_RD\t\t(1 << 4)\n#define NHM_PF_DATA_RFO\t\t(1 << 5)\n#define NHM_PF_IFETCH\t\t(1 << 6)\n#define NHM_OFFCORE_OTHER\t(1 << 7)\n#define NHM_UNCORE_HIT\t\t(1 << 8)\n#define NHM_OTHER_CORE_HIT_SNP\t(1 << 9)\n#define NHM_OTHER_CORE_HITM\t(1 << 10)\n        \t\t\t \n#define NHM_REMOTE_CACHE_FWD\t(1 << 12)\n#define NHM_REMOTE_DRAM\t\t(1 << 13)\n#define NHM_LOCAL_DRAM\t\t(1 << 14)\n#define NHM_NON_DRAM\t\t(1 << 15)\n\n#define NHM_LOCAL\t\t(NHM_LOCAL_DRAM|NHM_REMOTE_CACHE_FWD)\n#define NHM_REMOTE\t\t(NHM_REMOTE_DRAM)\n\n#define NHM_DMND_READ\t\t(NHM_DMND_DATA_RD)\n#define NHM_DMND_WRITE\t\t(NHM_DMND_RFO|NHM_DMND_WB)\n#define NHM_DMND_PREFETCH\t(NHM_PF_DATA_RD|NHM_PF_DATA_RFO)\n\n#define NHM_L3_HIT\t(NHM_UNCORE_HIT|NHM_OTHER_CORE_HIT_SNP|NHM_OTHER_CORE_HITM)\n#define NHM_L3_MISS\t(NHM_NON_DRAM|NHM_LOCAL_DRAM|NHM_REMOTE_DRAM|NHM_REMOTE_CACHE_FWD)\n#define NHM_L3_ACCESS\t(NHM_L3_HIT|NHM_L3_MISS)\n\nstatic __initconst const u64 nehalem_hw_cache_extra_regs\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = NHM_DMND_READ|NHM_L3_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = NHM_DMND_READ|NHM_L3_MISS,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = NHM_DMND_WRITE|NHM_L3_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = NHM_DMND_WRITE|NHM_L3_MISS,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = NHM_DMND_PREFETCH|NHM_L3_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = NHM_DMND_PREFETCH|NHM_L3_MISS,\n\t},\n },\n [ C(NODE) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = NHM_DMND_READ|NHM_LOCAL|NHM_REMOTE,\n\t\t[ C(RESULT_MISS)   ] = NHM_DMND_READ|NHM_REMOTE,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = NHM_DMND_WRITE|NHM_LOCAL|NHM_REMOTE,\n\t\t[ C(RESULT_MISS)   ] = NHM_DMND_WRITE|NHM_REMOTE,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = NHM_DMND_PREFETCH|NHM_LOCAL|NHM_REMOTE,\n\t\t[ C(RESULT_MISS)   ] = NHM_DMND_PREFETCH|NHM_REMOTE,\n\t},\n },\n};\n\nstatic __initconst const u64 nehalem_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x010b,  \n\t\t[ C(RESULT_MISS)   ] = 0x0151,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x020b,  \n\t\t[ C(RESULT_MISS)   ] = 0x0251,  \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x014e,  \n\t\t[ C(RESULT_MISS)   ] = 0x024e,  \n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0380,  \n\t\t[ C(RESULT_MISS)   ] = 0x0280,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t \n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t \n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t \n\t[ C(OP_WRITE) ] = {\n\t\t \n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t \n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t \n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t \n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f40,  \n\t\t[ C(RESULT_MISS)   ] = 0x0108,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f41,  \n\t\t[ C(RESULT_MISS)   ] = 0x010c,  \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0x0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x01c0,  \n\t\t[ C(RESULT_MISS)   ] = 0x20c8,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c4,  \n\t\t[ C(RESULT_MISS)   ] = 0x03e8,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(NODE) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n },\n};\n\nstatic __initconst const u64 core2_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f40,  \n\t\t[ C(RESULT_MISS)   ] = 0x0140,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f41,  \n\t\t[ C(RESULT_MISS)   ] = 0x0141,  \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x104e,  \n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0080,  \n\t\t[ C(RESULT_MISS)   ] = 0x0081,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x4f29,  \n\t\t[ C(RESULT_MISS)   ] = 0x4129,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x4f2A,  \n\t\t[ C(RESULT_MISS)   ] = 0x412A,  \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f40,  \n\t\t[ C(RESULT_MISS)   ] = 0x0208,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0f41,  \n\t\t[ C(RESULT_MISS)   ] = 0x0808,  \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c0,  \n\t\t[ C(RESULT_MISS)   ] = 0x1282,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c4,  \n\t\t[ C(RESULT_MISS)   ] = 0x00c5,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n};\n\nstatic __initconst const u64 atom_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x2140,  \n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x2240,  \n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0380,  \n\t\t[ C(RESULT_MISS)   ] = 0x0280,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x4f29,  \n\t\t[ C(RESULT_MISS)   ] = 0x4129,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x4f2A,  \n\t\t[ C(RESULT_MISS)   ] = 0x412A,  \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x2140,  \n\t\t[ C(RESULT_MISS)   ] = 0x0508,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x2240,  \n\t\t[ C(RESULT_MISS)   ] = 0x0608,  \n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c0,  \n\t\t[ C(RESULT_MISS)   ] = 0x0282,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c4,  \n\t\t[ C(RESULT_MISS)   ] = 0x00c5,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n};\n\nEVENT_ATTR_STR(topdown-total-slots, td_total_slots_slm, \"event=0x3c\");\nEVENT_ATTR_STR(topdown-total-slots.scale, td_total_slots_scale_slm, \"2\");\n \nEVENT_ATTR_STR(topdown-fetch-bubbles, td_fetch_bubbles_slm,\n\t       \"event=0xca,umask=0x50\");\nEVENT_ATTR_STR(topdown-fetch-bubbles.scale, td_fetch_bubbles_scale_slm, \"2\");\n \nEVENT_ATTR_STR(topdown-slots-issued, td_slots_issued_slm,\n\t       \"event=0xc2,umask=0x10\");\n \nEVENT_ATTR_STR(topdown-slots-retired, td_slots_retired_slm,\n\t       \"event=0xc2,umask=0x10\");\n\nstatic struct attribute *slm_events_attrs[] = {\n\tEVENT_PTR(td_total_slots_slm),\n\tEVENT_PTR(td_total_slots_scale_slm),\n\tEVENT_PTR(td_fetch_bubbles_slm),\n\tEVENT_PTR(td_fetch_bubbles_scale_slm),\n\tEVENT_PTR(td_slots_issued_slm),\n\tEVENT_PTR(td_slots_retired_slm),\n\tNULL\n};\n\nstatic struct extra_reg intel_slm_extra_regs[] __read_mostly =\n{\n\t \n\tINTEL_UEVENT_EXTRA_REG(0x01b7, MSR_OFFCORE_RSP_0, 0x768005ffffull, RSP_0),\n\tINTEL_UEVENT_EXTRA_REG(0x02b7, MSR_OFFCORE_RSP_1, 0x368005ffffull, RSP_1),\n\tEVENT_EXTRA_END\n};\n\n#define SLM_DMND_READ\t\tSNB_DMND_DATA_RD\n#define SLM_DMND_WRITE\t\tSNB_DMND_RFO\n#define SLM_DMND_PREFETCH\t(SNB_PF_DATA_RD|SNB_PF_RFO)\n\n#define SLM_SNP_ANY\t\t(SNB_SNP_NONE|SNB_SNP_MISS|SNB_NO_FWD|SNB_HITM)\n#define SLM_LLC_ACCESS\t\tSNB_RESP_ANY\n#define SLM_LLC_MISS\t\t(SLM_SNP_ANY|SNB_NON_DRAM)\n\nstatic __initconst const u64 slm_hw_cache_extra_regs\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = SLM_DMND_READ|SLM_LLC_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = SLM_DMND_WRITE|SLM_LLC_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = SLM_DMND_WRITE|SLM_LLC_MISS,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = SLM_DMND_PREFETCH|SLM_LLC_ACCESS,\n\t\t[ C(RESULT_MISS)   ] = SLM_DMND_PREFETCH|SLM_LLC_MISS,\n\t},\n },\n};\n\nstatic __initconst const u64 slm_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0x0104,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0380,  \n\t\t[ C(RESULT_MISS)   ] = 0x0280,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t \n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t \n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t \n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t \n\t\t[ C(RESULT_ACCESS) ] = 0x01b7,\n\t\t \n\t\t[ C(RESULT_MISS)   ] = 0x01b7,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0x0804,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c0,  \n\t\t[ C(RESULT_MISS)   ] = 0x40205,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c4,  \n\t\t[ C(RESULT_MISS)   ] = 0x00c5,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n};\n\nEVENT_ATTR_STR(topdown-total-slots, td_total_slots_glm, \"event=0x3c\");\nEVENT_ATTR_STR(topdown-total-slots.scale, td_total_slots_scale_glm, \"3\");\n \nEVENT_ATTR_STR(topdown-fetch-bubbles, td_fetch_bubbles_glm, \"event=0x9c\");\n \nEVENT_ATTR_STR(topdown-recovery-bubbles, td_recovery_bubbles_glm, \"event=0xca,umask=0x02\");\n \nEVENT_ATTR_STR(topdown-slots-retired, td_slots_retired_glm, \"event=0xc2\");\n \nEVENT_ATTR_STR(topdown-slots-issued, td_slots_issued_glm, \"event=0x0e\");\n\nstatic struct attribute *glm_events_attrs[] = {\n\tEVENT_PTR(td_total_slots_glm),\n\tEVENT_PTR(td_total_slots_scale_glm),\n\tEVENT_PTR(td_fetch_bubbles_glm),\n\tEVENT_PTR(td_recovery_bubbles_glm),\n\tEVENT_PTR(td_slots_issued_glm),\n\tEVENT_PTR(td_slots_retired_glm),\n\tNULL\n};\n\nstatic struct extra_reg intel_glm_extra_regs[] __read_mostly = {\n\t \n\tINTEL_UEVENT_EXTRA_REG(0x01b7, MSR_OFFCORE_RSP_0, 0x760005ffbfull, RSP_0),\n\tINTEL_UEVENT_EXTRA_REG(0x02b7, MSR_OFFCORE_RSP_1, 0x360005ffbfull, RSP_1),\n\tEVENT_EXTRA_END\n};\n\n#define GLM_DEMAND_DATA_RD\t\tBIT_ULL(0)\n#define GLM_DEMAND_RFO\t\t\tBIT_ULL(1)\n#define GLM_ANY_RESPONSE\t\tBIT_ULL(16)\n#define GLM_SNP_NONE_OR_MISS\t\tBIT_ULL(33)\n#define GLM_DEMAND_READ\t\t\tGLM_DEMAND_DATA_RD\n#define GLM_DEMAND_WRITE\t\tGLM_DEMAND_RFO\n#define GLM_DEMAND_PREFETCH\t\t(SNB_PF_DATA_RD|SNB_PF_RFO)\n#define GLM_LLC_ACCESS\t\t\tGLM_ANY_RESPONSE\n#define GLM_SNP_ANY\t\t\t(GLM_SNP_NONE_OR_MISS|SNB_NO_FWD|SNB_HITM)\n#define GLM_LLC_MISS\t\t\t(GLM_SNP_ANY|SNB_NON_DRAM)\n\nstatic __initconst const u64 glm_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(L1D)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x81d0,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x0,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x82d0,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x0,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x0,\n\t\t\t[C(RESULT_MISS)]\t= 0x0,\n\t\t},\n\t},\n\t[C(L1I)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x0380,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x0280,\t \n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= -1,\n\t\t\t[C(RESULT_MISS)]\t= -1,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x0,\n\t\t\t[C(RESULT_MISS)]\t= 0x0,\n\t\t},\n\t},\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x1b7,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x1b7,\t \n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x1b7,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x1b7,\t \n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x1b7,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x1b7,\t \n\t\t},\n\t},\n\t[C(DTLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x81d0,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x0,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x82d0,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x0,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x0,\n\t\t\t[C(RESULT_MISS)]\t= 0x0,\n\t\t},\n\t},\n\t[C(ITLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x00c0,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x0481,\t \n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= -1,\n\t\t\t[C(RESULT_MISS)]\t= -1,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= -1,\n\t\t\t[C(RESULT_MISS)]\t= -1,\n\t\t},\n\t},\n\t[C(BPU)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x00c4,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x00c5,\t \n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= -1,\n\t\t\t[C(RESULT_MISS)]\t= -1,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= -1,\n\t\t\t[C(RESULT_MISS)]\t= -1,\n\t\t},\n\t},\n};\n\nstatic __initconst const u64 glm_hw_cache_extra_regs\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= GLM_DEMAND_READ|\n\t\t\t\t\t\t  GLM_LLC_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= GLM_DEMAND_READ|\n\t\t\t\t\t\t  GLM_LLC_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= GLM_DEMAND_WRITE|\n\t\t\t\t\t\t  GLM_LLC_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= GLM_DEMAND_WRITE|\n\t\t\t\t\t\t  GLM_LLC_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= GLM_DEMAND_PREFETCH|\n\t\t\t\t\t\t  GLM_LLC_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= GLM_DEMAND_PREFETCH|\n\t\t\t\t\t\t  GLM_LLC_MISS,\n\t\t},\n\t},\n};\n\nstatic __initconst const u64 glp_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(L1D)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x81d0,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x0,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x82d0,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x0,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x0,\n\t\t\t[C(RESULT_MISS)]\t= 0x0,\n\t\t},\n\t},\n\t[C(L1I)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x0380,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x0280,\t \n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= -1,\n\t\t\t[C(RESULT_MISS)]\t= -1,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x0,\n\t\t\t[C(RESULT_MISS)]\t= 0x0,\n\t\t},\n\t},\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x1b7,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x1b7,\t \n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x1b7,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x1b7,\t \n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x0,\n\t\t\t[C(RESULT_MISS)]\t= 0x0,\n\t\t},\n\t},\n\t[C(DTLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x81d0,\t \n\t\t\t[C(RESULT_MISS)]\t= 0xe08,\t \n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x82d0,\t \n\t\t\t[C(RESULT_MISS)]\t= 0xe49,\t \n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x0,\n\t\t\t[C(RESULT_MISS)]\t= 0x0,\n\t\t},\n\t},\n\t[C(ITLB)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x00c0,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x0481,\t \n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= -1,\n\t\t\t[C(RESULT_MISS)]\t= -1,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= -1,\n\t\t\t[C(RESULT_MISS)]\t= -1,\n\t\t},\n\t},\n\t[C(BPU)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x00c4,\t \n\t\t\t[C(RESULT_MISS)]\t= 0x00c5,\t \n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= -1,\n\t\t\t[C(RESULT_MISS)]\t= -1,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= -1,\n\t\t\t[C(RESULT_MISS)]\t= -1,\n\t\t},\n\t},\n};\n\nstatic __initconst const u64 glp_hw_cache_extra_regs\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= GLM_DEMAND_READ|\n\t\t\t\t\t\t  GLM_LLC_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= GLM_DEMAND_READ|\n\t\t\t\t\t\t  GLM_LLC_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= GLM_DEMAND_WRITE|\n\t\t\t\t\t\t  GLM_LLC_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= GLM_DEMAND_WRITE|\n\t\t\t\t\t\t  GLM_LLC_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x0,\n\t\t\t[C(RESULT_MISS)]\t= 0x0,\n\t\t},\n\t},\n};\n\n#define TNT_LOCAL_DRAM\t\t\tBIT_ULL(26)\n#define TNT_DEMAND_READ\t\t\tGLM_DEMAND_DATA_RD\n#define TNT_DEMAND_WRITE\t\tGLM_DEMAND_RFO\n#define TNT_LLC_ACCESS\t\t\tGLM_ANY_RESPONSE\n#define TNT_SNP_ANY\t\t\t(SNB_SNP_NOT_NEEDED|SNB_SNP_MISS| \\\n\t\t\t\t\t SNB_NO_FWD|SNB_SNP_FWD|SNB_HITM)\n#define TNT_LLC_MISS\t\t\t(TNT_SNP_ANY|SNB_NON_DRAM|TNT_LOCAL_DRAM)\n\nstatic __initconst const u64 tnt_hw_cache_extra_regs\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= TNT_DEMAND_READ|\n\t\t\t\t\t\t  TNT_LLC_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= TNT_DEMAND_READ|\n\t\t\t\t\t\t  TNT_LLC_MISS,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= TNT_DEMAND_WRITE|\n\t\t\t\t\t\t  TNT_LLC_ACCESS,\n\t\t\t[C(RESULT_MISS)]\t= TNT_DEMAND_WRITE|\n\t\t\t\t\t\t  TNT_LLC_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)]\t= 0x0,\n\t\t\t[C(RESULT_MISS)]\t= 0x0,\n\t\t},\n\t},\n};\n\nEVENT_ATTR_STR(topdown-fe-bound,       td_fe_bound_tnt,        \"event=0x71,umask=0x0\");\nEVENT_ATTR_STR(topdown-retiring,       td_retiring_tnt,        \"event=0xc2,umask=0x0\");\nEVENT_ATTR_STR(topdown-bad-spec,       td_bad_spec_tnt,        \"event=0x73,umask=0x6\");\nEVENT_ATTR_STR(topdown-be-bound,       td_be_bound_tnt,        \"event=0x74,umask=0x0\");\n\nstatic struct attribute *tnt_events_attrs[] = {\n\tEVENT_PTR(td_fe_bound_tnt),\n\tEVENT_PTR(td_retiring_tnt),\n\tEVENT_PTR(td_bad_spec_tnt),\n\tEVENT_PTR(td_be_bound_tnt),\n\tNULL,\n};\n\nstatic struct extra_reg intel_tnt_extra_regs[] __read_mostly = {\n\t \n\tINTEL_UEVENT_EXTRA_REG(0x01b7, MSR_OFFCORE_RSP_0, 0x800ff0ffffff9fffull, RSP_0),\n\tINTEL_UEVENT_EXTRA_REG(0x02b7, MSR_OFFCORE_RSP_1, 0xff0ffffff9fffull, RSP_1),\n\tEVENT_EXTRA_END\n};\n\nEVENT_ATTR_STR(mem-loads,\tmem_ld_grt,\t\"event=0xd0,umask=0x5,ldlat=3\");\nEVENT_ATTR_STR(mem-stores,\tmem_st_grt,\t\"event=0xd0,umask=0x6\");\n\nstatic struct attribute *grt_mem_attrs[] = {\n\tEVENT_PTR(mem_ld_grt),\n\tEVENT_PTR(mem_st_grt),\n\tNULL\n};\n\nstatic struct extra_reg intel_grt_extra_regs[] __read_mostly = {\n\t \n\tINTEL_UEVENT_EXTRA_REG(0x01b7, MSR_OFFCORE_RSP_0, 0x3fffffffffull, RSP_0),\n\tINTEL_UEVENT_EXTRA_REG(0x02b7, MSR_OFFCORE_RSP_1, 0x3fffffffffull, RSP_1),\n\tINTEL_UEVENT_PEBS_LDLAT_EXTRA_REG(0x5d0),\n\tEVENT_EXTRA_END\n};\n\nEVENT_ATTR_STR(topdown-retiring,       td_retiring_cmt,        \"event=0x72,umask=0x0\");\nEVENT_ATTR_STR(topdown-bad-spec,       td_bad_spec_cmt,        \"event=0x73,umask=0x0\");\n\nstatic struct attribute *cmt_events_attrs[] = {\n\tEVENT_PTR(td_fe_bound_tnt),\n\tEVENT_PTR(td_retiring_cmt),\n\tEVENT_PTR(td_bad_spec_cmt),\n\tEVENT_PTR(td_be_bound_tnt),\n\tNULL\n};\n\nstatic struct extra_reg intel_cmt_extra_regs[] __read_mostly = {\n\t \n\tINTEL_UEVENT_EXTRA_REG(0x01b7, MSR_OFFCORE_RSP_0, 0x800ff3ffffffffffull, RSP_0),\n\tINTEL_UEVENT_EXTRA_REG(0x02b7, MSR_OFFCORE_RSP_1, 0xff3ffffffffffull, RSP_1),\n\tINTEL_UEVENT_PEBS_LDLAT_EXTRA_REG(0x5d0),\n\tINTEL_UEVENT_EXTRA_REG(0x0127, MSR_SNOOP_RSP_0, 0xffffffffffffffffull, SNOOP_0),\n\tINTEL_UEVENT_EXTRA_REG(0x0227, MSR_SNOOP_RSP_1, 0xffffffffffffffffull, SNOOP_1),\n\tEVENT_EXTRA_END\n};\n\n#define KNL_OT_L2_HITE\t\tBIT_ULL(19)  \n#define KNL_OT_L2_HITF\t\tBIT_ULL(20)  \n#define KNL_MCDRAM_LOCAL\tBIT_ULL(21)\n#define KNL_MCDRAM_FAR\t\tBIT_ULL(22)\n#define KNL_DDR_LOCAL\t\tBIT_ULL(23)\n#define KNL_DDR_FAR\t\tBIT_ULL(24)\n#define KNL_DRAM_ANY\t\t(KNL_MCDRAM_LOCAL | KNL_MCDRAM_FAR | \\\n\t\t\t\t    KNL_DDR_LOCAL | KNL_DDR_FAR)\n#define KNL_L2_READ\t\tSLM_DMND_READ\n#define KNL_L2_WRITE\t\tSLM_DMND_WRITE\n#define KNL_L2_PREFETCH\t\tSLM_DMND_PREFETCH\n#define KNL_L2_ACCESS\t\tSLM_LLC_ACCESS\n#define KNL_L2_MISS\t\t(KNL_OT_L2_HITE | KNL_OT_L2_HITF | \\\n\t\t\t\t   KNL_DRAM_ANY | SNB_SNP_ANY | \\\n\t\t\t\t\t\t  SNB_NON_DRAM)\n\nstatic __initconst const u64 knl_hw_cache_extra_regs\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n\t[C(LL)] = {\n\t\t[C(OP_READ)] = {\n\t\t\t[C(RESULT_ACCESS)] = KNL_L2_READ | KNL_L2_ACCESS,\n\t\t\t[C(RESULT_MISS)]   = 0,\n\t\t},\n\t\t[C(OP_WRITE)] = {\n\t\t\t[C(RESULT_ACCESS)] = KNL_L2_WRITE | KNL_L2_ACCESS,\n\t\t\t[C(RESULT_MISS)]   = KNL_L2_WRITE | KNL_L2_MISS,\n\t\t},\n\t\t[C(OP_PREFETCH)] = {\n\t\t\t[C(RESULT_ACCESS)] = KNL_L2_PREFETCH | KNL_L2_ACCESS,\n\t\t\t[C(RESULT_MISS)]   = KNL_L2_PREFETCH | KNL_L2_MISS,\n\t\t},\n\t},\n};\n\n \nstatic __always_inline void __intel_pmu_disable_all(bool bts)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\twrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, 0);\n\n\tif (bts && test_bit(INTEL_PMC_IDX_FIXED_BTS, cpuc->active_mask))\n\t\tintel_pmu_disable_bts();\n}\n\nstatic __always_inline void intel_pmu_disable_all(void)\n{\n\t__intel_pmu_disable_all(true);\n\tintel_pmu_pebs_disable_all();\n\tintel_pmu_lbr_disable_all();\n}\n\nstatic void __intel_pmu_enable_all(int added, bool pmi)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tu64 intel_ctrl = hybrid(cpuc->pmu, intel_ctrl);\n\n\tintel_pmu_lbr_enable_all(pmi);\n\n\tif (cpuc->fixed_ctrl_val != cpuc->active_fixed_ctrl_val) {\n\t\twrmsrl(MSR_ARCH_PERFMON_FIXED_CTR_CTRL, cpuc->fixed_ctrl_val);\n\t\tcpuc->active_fixed_ctrl_val = cpuc->fixed_ctrl_val;\n\t}\n\n\twrmsrl(MSR_CORE_PERF_GLOBAL_CTRL,\n\t       intel_ctrl & ~cpuc->intel_ctrl_guest_mask);\n\n\tif (test_bit(INTEL_PMC_IDX_FIXED_BTS, cpuc->active_mask)) {\n\t\tstruct perf_event *event =\n\t\t\tcpuc->events[INTEL_PMC_IDX_FIXED_BTS];\n\n\t\tif (WARN_ON_ONCE(!event))\n\t\t\treturn;\n\n\t\tintel_pmu_enable_bts(event->hw.config);\n\t}\n}\n\nstatic void intel_pmu_enable_all(int added)\n{\n\tintel_pmu_pebs_enable_all();\n\t__intel_pmu_enable_all(added, false);\n}\n\nstatic noinline int\n__intel_pmu_snapshot_branch_stack(struct perf_branch_entry *entries,\n\t\t\t\t  unsigned int cnt, unsigned long flags)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tintel_pmu_lbr_read();\n\tcnt = min_t(unsigned int, cnt, x86_pmu.lbr_nr);\n\n\tmemcpy(entries, cpuc->lbr_entries, sizeof(struct perf_branch_entry) * cnt);\n\tintel_pmu_enable_all(0);\n\tlocal_irq_restore(flags);\n\treturn cnt;\n}\n\nstatic int\nintel_pmu_snapshot_branch_stack(struct perf_branch_entry *entries, unsigned int cnt)\n{\n\tunsigned long flags;\n\n\t \n\tlocal_irq_save(flags);\n\t__intel_pmu_disable_all(false);  \n\t__intel_pmu_lbr_disable();\n\t \n\treturn __intel_pmu_snapshot_branch_stack(entries, cnt, flags);\n}\n\nstatic int\nintel_pmu_snapshot_arch_branch_stack(struct perf_branch_entry *entries, unsigned int cnt)\n{\n\tunsigned long flags;\n\n\t \n\tlocal_irq_save(flags);\n\t__intel_pmu_disable_all(false);  \n\t__intel_pmu_arch_lbr_disable();\n\t \n\treturn __intel_pmu_snapshot_branch_stack(entries, cnt, flags);\n}\n\n \nstatic void intel_pmu_nhm_workaround(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstatic const unsigned long nhm_magic[4] = {\n\t\t0x4300B5,\n\t\t0x4300D2,\n\t\t0x4300B1,\n\t\t0x4300B1\n\t};\n\tstruct perf_event *event;\n\tint i;\n\n\t \n\n\t \n\n\t \n\tfor (i = 0; i < 4; i++) {\n\t\tevent = cpuc->events[i];\n\t\tif (event)\n\t\t\tstatic_call(x86_pmu_update)(event);\n\t}\n\n\tfor (i = 0; i < 4; i++) {\n\t\twrmsrl(MSR_ARCH_PERFMON_EVENTSEL0 + i, nhm_magic[i]);\n\t\twrmsrl(MSR_ARCH_PERFMON_PERFCTR0 + i, 0x0);\n\t}\n\n\twrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, 0xf);\n\twrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, 0x0);\n\n\tfor (i = 0; i < 4; i++) {\n\t\tevent = cpuc->events[i];\n\n\t\tif (event) {\n\t\t\tstatic_call(x86_pmu_set_period)(event);\n\t\t\t__x86_pmu_enable_event(&event->hw,\n\t\t\t\t\tARCH_PERFMON_EVENTSEL_ENABLE);\n\t\t} else\n\t\t\twrmsrl(MSR_ARCH_PERFMON_EVENTSEL0 + i, 0x0);\n\t}\n}\n\nstatic void intel_pmu_nhm_enable_all(int added)\n{\n\tif (added)\n\t\tintel_pmu_nhm_workaround();\n\tintel_pmu_enable_all(added);\n}\n\nstatic void intel_set_tfa(struct cpu_hw_events *cpuc, bool on)\n{\n\tu64 val = on ? MSR_TFA_RTM_FORCE_ABORT : 0;\n\n\tif (cpuc->tfa_shadow != val) {\n\t\tcpuc->tfa_shadow = val;\n\t\twrmsrl(MSR_TSX_FORCE_ABORT, val);\n\t}\n}\n\nstatic void intel_tfa_commit_scheduling(struct cpu_hw_events *cpuc, int idx, int cntr)\n{\n\t \n\tif (cntr == 3)\n\t\tintel_set_tfa(cpuc, true);\n}\n\nstatic void intel_tfa_pmu_enable_all(int added)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\t \n\tif (!test_bit(3, cpuc->active_mask))\n\t\tintel_set_tfa(cpuc, false);\n\n\tintel_pmu_enable_all(added);\n}\n\nstatic inline u64 intel_pmu_get_status(void)\n{\n\tu64 status;\n\n\trdmsrl(MSR_CORE_PERF_GLOBAL_STATUS, status);\n\n\treturn status;\n}\n\nstatic inline void intel_pmu_ack_status(u64 ack)\n{\n\twrmsrl(MSR_CORE_PERF_GLOBAL_OVF_CTRL, ack);\n}\n\nstatic inline bool event_is_checkpointed(struct perf_event *event)\n{\n\treturn unlikely(event->hw.config & HSW_IN_TX_CHECKPOINTED) != 0;\n}\n\nstatic inline void intel_set_masks(struct perf_event *event, int idx)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tif (event->attr.exclude_host)\n\t\t__set_bit(idx, (unsigned long *)&cpuc->intel_ctrl_guest_mask);\n\tif (event->attr.exclude_guest)\n\t\t__set_bit(idx, (unsigned long *)&cpuc->intel_ctrl_host_mask);\n\tif (event_is_checkpointed(event))\n\t\t__set_bit(idx, (unsigned long *)&cpuc->intel_cp_status);\n}\n\nstatic inline void intel_clear_masks(struct perf_event *event, int idx)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\t__clear_bit(idx, (unsigned long *)&cpuc->intel_ctrl_guest_mask);\n\t__clear_bit(idx, (unsigned long *)&cpuc->intel_ctrl_host_mask);\n\t__clear_bit(idx, (unsigned long *)&cpuc->intel_cp_status);\n}\n\nstatic void intel_pmu_disable_fixed(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\tu64 mask;\n\n\tif (is_topdown_idx(idx)) {\n\t\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\t\t \n\t\tif (*(u64 *)cpuc->active_mask & INTEL_PMC_OTHER_TOPDOWN_BITS(idx))\n\t\t\treturn;\n\t\tidx = INTEL_PMC_IDX_FIXED_SLOTS;\n\t}\n\n\tintel_clear_masks(event, idx);\n\n\tmask = intel_fixed_bits_by_idx(idx - INTEL_PMC_IDX_FIXED, INTEL_FIXED_BITS_MASK);\n\tcpuc->fixed_ctrl_val &= ~mask;\n}\n\nstatic void intel_pmu_disable_event(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\n\tswitch (idx) {\n\tcase 0 ... INTEL_PMC_IDX_FIXED - 1:\n\t\tintel_clear_masks(event, idx);\n\t\tx86_pmu_disable_event(event);\n\t\tbreak;\n\tcase INTEL_PMC_IDX_FIXED ... INTEL_PMC_IDX_FIXED_BTS - 1:\n\tcase INTEL_PMC_IDX_METRIC_BASE ... INTEL_PMC_IDX_METRIC_END:\n\t\tintel_pmu_disable_fixed(event);\n\t\tbreak;\n\tcase INTEL_PMC_IDX_FIXED_BTS:\n\t\tintel_pmu_disable_bts();\n\t\tintel_pmu_drain_bts_buffer();\n\t\treturn;\n\tcase INTEL_PMC_IDX_FIXED_VLBR:\n\t\tintel_clear_masks(event, idx);\n\t\tbreak;\n\tdefault:\n\t\tintel_clear_masks(event, idx);\n\t\tpr_warn(\"Failed to disable the event with invalid index %d\\n\",\n\t\t\tidx);\n\t\treturn;\n\t}\n\n\t \n\tif (unlikely(event->attr.precise_ip))\n\t\tintel_pmu_pebs_disable(event);\n}\n\nstatic void intel_pmu_assign_event(struct perf_event *event, int idx)\n{\n\tif (is_pebs_pt(event))\n\t\tperf_report_aux_output_id(event, idx);\n}\n\nstatic void intel_pmu_del_event(struct perf_event *event)\n{\n\tif (needs_branch_stack(event))\n\t\tintel_pmu_lbr_del(event);\n\tif (event->attr.precise_ip)\n\t\tintel_pmu_pebs_del(event);\n}\n\nstatic int icl_set_topdown_event_period(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\ts64 left = local64_read(&hwc->period_left);\n\n\t \n\tif (left == x86_pmu.max_period) {\n\t\twrmsrl(MSR_CORE_PERF_FIXED_CTR3, 0);\n\t\twrmsrl(MSR_PERF_METRICS, 0);\n\t\thwc->saved_slots = 0;\n\t\thwc->saved_metric = 0;\n\t}\n\n\tif ((hwc->saved_slots) && is_slots_event(event)) {\n\t\twrmsrl(MSR_CORE_PERF_FIXED_CTR3, hwc->saved_slots);\n\t\twrmsrl(MSR_PERF_METRICS, hwc->saved_metric);\n\t}\n\n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}\n\nstatic int adl_set_topdown_event_period(struct perf_event *event)\n{\n\tstruct x86_hybrid_pmu *pmu = hybrid_pmu(event->pmu);\n\n\tif (pmu->cpu_type != hybrid_big)\n\t\treturn 0;\n\n\treturn icl_set_topdown_event_period(event);\n}\n\nDEFINE_STATIC_CALL(intel_pmu_set_topdown_event_period, x86_perf_event_set_period);\n\nstatic inline u64 icl_get_metrics_event_value(u64 metric, u64 slots, int idx)\n{\n\tu32 val;\n\n\t \n\tval = (metric >> ((idx - INTEL_PMC_IDX_METRIC_BASE) * 8)) & 0xff;\n\treturn  mul_u64_u32_div(slots, val, 0xff);\n}\n\nstatic u64 icl_get_topdown_value(struct perf_event *event,\n\t\t\t\t       u64 slots, u64 metrics)\n{\n\tint idx = event->hw.idx;\n\tu64 delta;\n\n\tif (is_metric_idx(idx))\n\t\tdelta = icl_get_metrics_event_value(metrics, slots, idx);\n\telse\n\t\tdelta = slots;\n\n\treturn delta;\n}\n\nstatic void __icl_update_topdown_event(struct perf_event *event,\n\t\t\t\t       u64 slots, u64 metrics,\n\t\t\t\t       u64 last_slots, u64 last_metrics)\n{\n\tu64 delta, last = 0;\n\n\tdelta = icl_get_topdown_value(event, slots, metrics);\n\tif (last_slots)\n\t\tlast = icl_get_topdown_value(event, last_slots, last_metrics);\n\n\t \n\tif (delta > last) {\n\t\tdelta -= last;\n\t\tlocal64_add(delta, &event->count);\n\t}\n}\n\nstatic void update_saved_topdown_regs(struct perf_event *event, u64 slots,\n\t\t\t\t      u64 metrics, int metric_end)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct perf_event *other;\n\tint idx;\n\n\tevent->hw.saved_slots = slots;\n\tevent->hw.saved_metric = metrics;\n\n\tfor_each_set_bit(idx, cpuc->active_mask, metric_end + 1) {\n\t\tif (!is_topdown_idx(idx))\n\t\t\tcontinue;\n\t\tother = cpuc->events[idx];\n\t\tother->hw.saved_slots = slots;\n\t\tother->hw.saved_metric = metrics;\n\t}\n}\n\n \n\nstatic u64 intel_update_topdown_event(struct perf_event *event, int metric_end)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct perf_event *other;\n\tu64 slots, metrics;\n\tbool reset = true;\n\tint idx;\n\n\t \n\trdpmcl((3 | INTEL_PMC_FIXED_RDPMC_BASE), slots);\n\tif (!slots)\n\t\treturn 0;\n\n\t \n\trdpmcl(INTEL_PMC_FIXED_RDPMC_METRICS, metrics);\n\n\tfor_each_set_bit(idx, cpuc->active_mask, metric_end + 1) {\n\t\tif (!is_topdown_idx(idx))\n\t\t\tcontinue;\n\t\tother = cpuc->events[idx];\n\t\t__icl_update_topdown_event(other, slots, metrics,\n\t\t\t\t\t   event ? event->hw.saved_slots : 0,\n\t\t\t\t\t   event ? event->hw.saved_metric : 0);\n\t}\n\n\t \n\tif (event && !test_bit(event->hw.idx, cpuc->active_mask)) {\n\t\t__icl_update_topdown_event(event, slots, metrics,\n\t\t\t\t\t   event->hw.saved_slots,\n\t\t\t\t\t   event->hw.saved_metric);\n\n\t\t \n\t\tupdate_saved_topdown_regs(event, slots, metrics, metric_end);\n\t\treset = false;\n\t}\n\n\tif (reset) {\n\t\t \n\t\twrmsrl(MSR_CORE_PERF_FIXED_CTR3, 0);\n\t\twrmsrl(MSR_PERF_METRICS, 0);\n\t\tif (event)\n\t\t\tupdate_saved_topdown_regs(event, 0, 0, metric_end);\n\t}\n\n\treturn slots;\n}\n\nstatic u64 icl_update_topdown_event(struct perf_event *event)\n{\n\treturn intel_update_topdown_event(event, INTEL_PMC_IDX_METRIC_BASE +\n\t\t\t\t\t\t x86_pmu.num_topdown_events - 1);\n}\n\nstatic u64 adl_update_topdown_event(struct perf_event *event)\n{\n\tstruct x86_hybrid_pmu *pmu = hybrid_pmu(event->pmu);\n\n\tif (pmu->cpu_type != hybrid_big)\n\t\treturn 0;\n\n\treturn icl_update_topdown_event(event);\n}\n\nDEFINE_STATIC_CALL(intel_pmu_update_topdown_event, x86_perf_event_update);\n\nstatic void intel_pmu_read_topdown_event(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\t \n\tif ((cpuc->txn_flags & PERF_PMU_TXN_READ) &&\n\t    !is_slots_event(event))\n\t\treturn;\n\n\tperf_pmu_disable(event->pmu);\n\tstatic_call(intel_pmu_update_topdown_event)(event);\n\tperf_pmu_enable(event->pmu);\n}\n\nstatic void intel_pmu_read_event(struct perf_event *event)\n{\n\tif (event->hw.flags & PERF_X86_EVENT_AUTO_RELOAD)\n\t\tintel_pmu_auto_reload_read(event);\n\telse if (is_topdown_count(event))\n\t\tintel_pmu_read_topdown_event(event);\n\telse\n\t\tx86_perf_event_update(event);\n}\n\nstatic void intel_pmu_enable_fixed(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 mask, bits = 0;\n\tint idx = hwc->idx;\n\n\tif (is_topdown_idx(idx)) {\n\t\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\t\t \n\t\tif (*(u64 *)cpuc->active_mask & INTEL_PMC_OTHER_TOPDOWN_BITS(idx))\n\t\t\treturn;\n\n\t\tidx = INTEL_PMC_IDX_FIXED_SLOTS;\n\t}\n\n\tintel_set_masks(event, idx);\n\n\t \n\tif (!event->attr.precise_ip)\n\t\tbits |= INTEL_FIXED_0_ENABLE_PMI;\n\tif (hwc->config & ARCH_PERFMON_EVENTSEL_USR)\n\t\tbits |= INTEL_FIXED_0_USER;\n\tif (hwc->config & ARCH_PERFMON_EVENTSEL_OS)\n\t\tbits |= INTEL_FIXED_0_KERNEL;\n\n\t \n\tif (x86_pmu.version > 2 && hwc->config & ARCH_PERFMON_EVENTSEL_ANY)\n\t\tbits |= INTEL_FIXED_0_ANYTHREAD;\n\n\tidx -= INTEL_PMC_IDX_FIXED;\n\tbits = intel_fixed_bits_by_idx(idx, bits);\n\tmask = intel_fixed_bits_by_idx(idx, INTEL_FIXED_BITS_MASK);\n\n\tif (x86_pmu.intel_cap.pebs_baseline && event->attr.precise_ip) {\n\t\tbits |= intel_fixed_bits_by_idx(idx, ICL_FIXED_0_ADAPTIVE);\n\t\tmask |= intel_fixed_bits_by_idx(idx, ICL_FIXED_0_ADAPTIVE);\n\t}\n\n\tcpuc->fixed_ctrl_val &= ~mask;\n\tcpuc->fixed_ctrl_val |= bits;\n}\n\nstatic void intel_pmu_enable_event(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx = hwc->idx;\n\n\tif (unlikely(event->attr.precise_ip))\n\t\tintel_pmu_pebs_enable(event);\n\n\tswitch (idx) {\n\tcase 0 ... INTEL_PMC_IDX_FIXED - 1:\n\t\tintel_set_masks(event, idx);\n\t\t__x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);\n\t\tbreak;\n\tcase INTEL_PMC_IDX_FIXED ... INTEL_PMC_IDX_FIXED_BTS - 1:\n\tcase INTEL_PMC_IDX_METRIC_BASE ... INTEL_PMC_IDX_METRIC_END:\n\t\tintel_pmu_enable_fixed(event);\n\t\tbreak;\n\tcase INTEL_PMC_IDX_FIXED_BTS:\n\t\tif (!__this_cpu_read(cpu_hw_events.enabled))\n\t\t\treturn;\n\t\tintel_pmu_enable_bts(hwc->config);\n\t\tbreak;\n\tcase INTEL_PMC_IDX_FIXED_VLBR:\n\t\tintel_set_masks(event, idx);\n\t\tbreak;\n\tdefault:\n\t\tpr_warn(\"Failed to enable the event with invalid index %d\\n\",\n\t\t\tidx);\n\t}\n}\n\nstatic void intel_pmu_add_event(struct perf_event *event)\n{\n\tif (event->attr.precise_ip)\n\t\tintel_pmu_pebs_add(event);\n\tif (needs_branch_stack(event))\n\t\tintel_pmu_lbr_add(event);\n}\n\n \nint intel_pmu_save_and_restart(struct perf_event *event)\n{\n\tstatic_call(x86_pmu_update)(event);\n\t \n\tif (unlikely(event_is_checkpointed(event))) {\n\t\t \n\t\twrmsrl(event->hw.event_base, 0);\n\t\tlocal64_set(&event->hw.prev_count, 0);\n\t}\n\treturn static_call(x86_pmu_set_period)(event);\n}\n\nstatic int intel_pmu_set_period(struct perf_event *event)\n{\n\tif (unlikely(is_topdown_count(event)))\n\t\treturn static_call(intel_pmu_set_topdown_event_period)(event);\n\n\treturn x86_perf_event_set_period(event);\n}\n\nstatic u64 intel_pmu_update(struct perf_event *event)\n{\n\tif (unlikely(is_topdown_count(event)))\n\t\treturn static_call(intel_pmu_update_topdown_event)(event);\n\n\treturn x86_perf_event_update(event);\n}\n\nstatic void intel_pmu_reset(void)\n{\n\tstruct debug_store *ds = __this_cpu_read(cpu_hw_events.ds);\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tint num_counters_fixed = hybrid(cpuc->pmu, num_counters_fixed);\n\tint num_counters = hybrid(cpuc->pmu, num_counters);\n\tunsigned long flags;\n\tint idx;\n\n\tif (!num_counters)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tpr_info(\"clearing PMU state on CPU#%d\\n\", smp_processor_id());\n\n\tfor (idx = 0; idx < num_counters; idx++) {\n\t\twrmsrl_safe(x86_pmu_config_addr(idx), 0ull);\n\t\twrmsrl_safe(x86_pmu_event_addr(idx),  0ull);\n\t}\n\tfor (idx = 0; idx < num_counters_fixed; idx++) {\n\t\tif (fixed_counter_disabled(idx, cpuc->pmu))\n\t\t\tcontinue;\n\t\twrmsrl_safe(MSR_ARCH_PERFMON_FIXED_CTR0 + idx, 0ull);\n\t}\n\n\tif (ds)\n\t\tds->bts_index = ds->bts_buffer_base;\n\n\t \n\tif (x86_pmu.version >= 2) {\n\t\tintel_pmu_ack_status(intel_pmu_get_status());\n\t\twrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, 0);\n\t}\n\n\t \n\tif (x86_pmu.lbr_nr) {\n\t\tupdate_debugctlmsr(get_debugctlmsr() &\n\t\t\t~(DEBUGCTLMSR_FREEZE_LBRS_ON_PMI|DEBUGCTLMSR_LBR));\n\t}\n\n\tlocal_irq_restore(flags);\n}\n\n \nstatic void x86_pmu_handle_guest_pebs(struct pt_regs *regs,\n\t\t\t\t      struct perf_sample_data *data)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tu64 guest_pebs_idxs = cpuc->pebs_enabled & ~cpuc->intel_ctrl_host_mask;\n\tstruct perf_event *event = NULL;\n\tint bit;\n\n\tif (!unlikely(perf_guest_state()))\n\t\treturn;\n\n\tif (!x86_pmu.pebs_ept || !x86_pmu.pebs_active ||\n\t    !guest_pebs_idxs)\n\t\treturn;\n\n\tfor_each_set_bit(bit, (unsigned long *)&guest_pebs_idxs,\n\t\t\t INTEL_PMC_IDX_FIXED + x86_pmu.num_counters_fixed) {\n\t\tevent = cpuc->events[bit];\n\t\tif (!event->attr.precise_ip)\n\t\t\tcontinue;\n\n\t\tperf_sample_data_init(data, 0, event->hw.last_period);\n\t\tif (perf_event_overflow(event, data, regs))\n\t\t\tx86_pmu_stop(event, 0);\n\n\t\t \n\t\tbreak;\n\t}\n}\n\nstatic int handle_pmi_common(struct pt_regs *regs, u64 status)\n{\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tint bit;\n\tint handled = 0;\n\tu64 intel_ctrl = hybrid(cpuc->pmu, intel_ctrl);\n\n\tinc_irq_stat(apic_perf_irqs);\n\n\t \n\tstatus &= ~(GLOBAL_STATUS_COND_CHG |\n\t\t    GLOBAL_STATUS_ASIF |\n\t\t    GLOBAL_STATUS_LBRS_FROZEN);\n\tif (!status)\n\t\treturn 0;\n\t \n\tstatus &= ~(cpuc->pebs_enabled & x86_pmu.pebs_capable);\n\n\t \n\tif (__test_and_clear_bit(GLOBAL_STATUS_BUFFER_OVF_BIT, (unsigned long *)&status)) {\n\t\tu64 pebs_enabled = cpuc->pebs_enabled;\n\n\t\thandled++;\n\t\tx86_pmu_handle_guest_pebs(regs, &data);\n\t\tx86_pmu.drain_pebs(regs, &data);\n\t\tstatus &= intel_ctrl | GLOBAL_STATUS_TRACE_TOPAPMI;\n\n\t\t \n\t\tif (pebs_enabled != cpuc->pebs_enabled)\n\t\t\twrmsrl(MSR_IA32_PEBS_ENABLE, cpuc->pebs_enabled);\n\t}\n\n\t \n\tif (__test_and_clear_bit(GLOBAL_STATUS_TRACE_TOPAPMI_BIT, (unsigned long *)&status)) {\n\t\thandled++;\n\t\tif (!perf_guest_handle_intel_pt_intr())\n\t\t\tintel_pt_interrupt();\n\t}\n\n\t \n\tif (__test_and_clear_bit(GLOBAL_STATUS_PERF_METRICS_OVF_BIT, (unsigned long *)&status)) {\n\t\thandled++;\n\t\tstatic_call(intel_pmu_update_topdown_event)(NULL);\n\t}\n\n\t \n\tstatus |= cpuc->intel_cp_status;\n\n\tfor_each_set_bit(bit, (unsigned long *)&status, X86_PMC_IDX_MAX) {\n\t\tstruct perf_event *event = cpuc->events[bit];\n\n\t\thandled++;\n\n\t\tif (!test_bit(bit, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\tif (!intel_pmu_save_and_restart(event))\n\t\t\tcontinue;\n\n\t\tperf_sample_data_init(&data, 0, event->hw.last_period);\n\n\t\tif (has_branch_stack(event))\n\t\t\tperf_sample_save_brstack(&data, event, &cpuc->lbr_stack);\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tx86_pmu_stop(event, 0);\n\t}\n\n\treturn handled;\n}\n\n \nstatic int intel_pmu_handle_irq(struct pt_regs *regs)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tbool late_ack = hybrid_bit(cpuc->pmu, late_ack);\n\tbool mid_ack = hybrid_bit(cpuc->pmu, mid_ack);\n\tint loops;\n\tu64 status;\n\tint handled;\n\tint pmu_enabled;\n\n\t \n\tpmu_enabled = cpuc->enabled;\n\t \n\tif (!late_ack && !mid_ack)\n\t\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n\tintel_bts_disable_local();\n\tcpuc->enabled = 0;\n\t__intel_pmu_disable_all(true);\n\thandled = intel_pmu_drain_bts_buffer();\n\thandled += intel_bts_interrupt();\n\tstatus = intel_pmu_get_status();\n\tif (!status)\n\t\tgoto done;\n\n\tloops = 0;\nagain:\n\tintel_pmu_lbr_read();\n\tintel_pmu_ack_status(status);\n\tif (++loops > 100) {\n\t\tstatic bool warned;\n\n\t\tif (!warned) {\n\t\t\tWARN(1, \"perfevents: irq loop stuck!\\n\");\n\t\t\tperf_event_print_debug();\n\t\t\twarned = true;\n\t\t}\n\t\tintel_pmu_reset();\n\t\tgoto done;\n\t}\n\n\thandled += handle_pmi_common(regs, status);\n\n\t \n\tstatus = intel_pmu_get_status();\n\tif (status)\n\t\tgoto again;\n\ndone:\n\tif (mid_ack)\n\t\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n\t \n\tcpuc->enabled = pmu_enabled;\n\tif (pmu_enabled)\n\t\t__intel_pmu_enable_all(0, true);\n\tintel_bts_enable_local();\n\n\t \n\tif (late_ack)\n\t\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n\treturn handled;\n}\n\nstatic struct event_constraint *\nintel_bts_constraints(struct perf_event *event)\n{\n\tif (unlikely(intel_pmu_has_bts(event)))\n\t\treturn &bts_constraint;\n\n\treturn NULL;\n}\n\n \nstatic struct event_constraint *\nintel_vlbr_constraints(struct perf_event *event)\n{\n\tstruct event_constraint *c = &vlbr_constraint;\n\n\tif (unlikely(constraint_match(c, event->hw.config))) {\n\t\tevent->hw.flags |= c->flags;\n\t\treturn c;\n\t}\n\n\treturn NULL;\n}\n\nstatic int intel_alt_er(struct cpu_hw_events *cpuc,\n\t\t\tint idx, u64 config)\n{\n\tstruct extra_reg *extra_regs = hybrid(cpuc->pmu, extra_regs);\n\tint alt_idx = idx;\n\n\tif (!(x86_pmu.flags & PMU_FL_HAS_RSP_1))\n\t\treturn idx;\n\n\tif (idx == EXTRA_REG_RSP_0)\n\t\talt_idx = EXTRA_REG_RSP_1;\n\n\tif (idx == EXTRA_REG_RSP_1)\n\t\talt_idx = EXTRA_REG_RSP_0;\n\n\tif (config & ~extra_regs[alt_idx].valid_mask)\n\t\treturn idx;\n\n\treturn alt_idx;\n}\n\nstatic void intel_fixup_er(struct perf_event *event, int idx)\n{\n\tstruct extra_reg *extra_regs = hybrid(event->pmu, extra_regs);\n\tevent->hw.extra_reg.idx = idx;\n\n\tif (idx == EXTRA_REG_RSP_0) {\n\t\tevent->hw.config &= ~INTEL_ARCH_EVENT_MASK;\n\t\tevent->hw.config |= extra_regs[EXTRA_REG_RSP_0].event;\n\t\tevent->hw.extra_reg.reg = MSR_OFFCORE_RSP_0;\n\t} else if (idx == EXTRA_REG_RSP_1) {\n\t\tevent->hw.config &= ~INTEL_ARCH_EVENT_MASK;\n\t\tevent->hw.config |= extra_regs[EXTRA_REG_RSP_1].event;\n\t\tevent->hw.extra_reg.reg = MSR_OFFCORE_RSP_1;\n\t}\n}\n\n \nstatic struct event_constraint *\n__intel_shared_reg_get_constraints(struct cpu_hw_events *cpuc,\n\t\t\t\t   struct perf_event *event,\n\t\t\t\t   struct hw_perf_event_extra *reg)\n{\n\tstruct event_constraint *c = &emptyconstraint;\n\tstruct er_account *era;\n\tunsigned long flags;\n\tint idx = reg->idx;\n\n\t \n\tif (reg->alloc && !cpuc->is_fake)\n\t\treturn NULL;  \n\nagain:\n\tera = &cpuc->shared_regs->regs[idx];\n\t \n\traw_spin_lock_irqsave(&era->lock, flags);\n\n\tif (!atomic_read(&era->ref) || era->config == reg->config) {\n\n\t\t \n\t\tif (!cpuc->is_fake) {\n\t\t\tif (idx != reg->idx)\n\t\t\t\tintel_fixup_er(event, idx);\n\n\t\t\t \n\t\t\treg->alloc = 1;\n\t\t}\n\n\t\t \n\t\tera->config = reg->config;\n\t\tera->reg = reg->reg;\n\n\t\t \n\t\tatomic_inc(&era->ref);\n\n\t\t \n\t\tc = NULL;\n\t} else {\n\t\tidx = intel_alt_er(cpuc, idx, reg->config);\n\t\tif (idx != reg->idx) {\n\t\t\traw_spin_unlock_irqrestore(&era->lock, flags);\n\t\t\tgoto again;\n\t\t}\n\t}\n\traw_spin_unlock_irqrestore(&era->lock, flags);\n\n\treturn c;\n}\n\nstatic void\n__intel_shared_reg_put_constraints(struct cpu_hw_events *cpuc,\n\t\t\t\t   struct hw_perf_event_extra *reg)\n{\n\tstruct er_account *era;\n\n\t \n\tif (!reg->alloc || cpuc->is_fake)\n\t\treturn;\n\n\tera = &cpuc->shared_regs->regs[reg->idx];\n\n\t \n\tatomic_dec(&era->ref);\n\n\t \n\treg->alloc = 0;\n}\n\nstatic struct event_constraint *\nintel_shared_regs_constraints(struct cpu_hw_events *cpuc,\n\t\t\t      struct perf_event *event)\n{\n\tstruct event_constraint *c = NULL, *d;\n\tstruct hw_perf_event_extra *xreg, *breg;\n\n\txreg = &event->hw.extra_reg;\n\tif (xreg->idx != EXTRA_REG_NONE) {\n\t\tc = __intel_shared_reg_get_constraints(cpuc, event, xreg);\n\t\tif (c == &emptyconstraint)\n\t\t\treturn c;\n\t}\n\tbreg = &event->hw.branch_reg;\n\tif (breg->idx != EXTRA_REG_NONE) {\n\t\td = __intel_shared_reg_get_constraints(cpuc, event, breg);\n\t\tif (d == &emptyconstraint) {\n\t\t\t__intel_shared_reg_put_constraints(cpuc, xreg);\n\t\t\tc = d;\n\t\t}\n\t}\n\treturn c;\n}\n\nstruct event_constraint *\nx86_get_event_constraints(struct cpu_hw_events *cpuc, int idx,\n\t\t\t  struct perf_event *event)\n{\n\tstruct event_constraint *event_constraints = hybrid(cpuc->pmu, event_constraints);\n\tstruct event_constraint *c;\n\n\tif (event_constraints) {\n\t\tfor_each_event_constraint(c, event_constraints) {\n\t\t\tif (constraint_match(c, event->hw.config)) {\n\t\t\t\tevent->hw.flags |= c->flags;\n\t\t\t\treturn c;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn &hybrid_var(cpuc->pmu, unconstrained);\n}\n\nstatic struct event_constraint *\n__intel_get_event_constraints(struct cpu_hw_events *cpuc, int idx,\n\t\t\t    struct perf_event *event)\n{\n\tstruct event_constraint *c;\n\n\tc = intel_vlbr_constraints(event);\n\tif (c)\n\t\treturn c;\n\n\tc = intel_bts_constraints(event);\n\tif (c)\n\t\treturn c;\n\n\tc = intel_shared_regs_constraints(cpuc, event);\n\tif (c)\n\t\treturn c;\n\n\tc = intel_pebs_constraints(event);\n\tif (c)\n\t\treturn c;\n\n\treturn x86_get_event_constraints(cpuc, idx, event);\n}\n\nstatic void\nintel_start_scheduling(struct cpu_hw_events *cpuc)\n{\n\tstruct intel_excl_cntrs *excl_cntrs = cpuc->excl_cntrs;\n\tstruct intel_excl_states *xl;\n\tint tid = cpuc->excl_thread_id;\n\n\t \n\tif (cpuc->is_fake || !is_ht_workaround_enabled())\n\t\treturn;\n\n\t \n\tif (WARN_ON_ONCE(!excl_cntrs))\n\t\treturn;\n\n\txl = &excl_cntrs->states[tid];\n\n\txl->sched_started = true;\n\t \n\traw_spin_lock(&excl_cntrs->lock);\n}\n\nstatic void intel_commit_scheduling(struct cpu_hw_events *cpuc, int idx, int cntr)\n{\n\tstruct intel_excl_cntrs *excl_cntrs = cpuc->excl_cntrs;\n\tstruct event_constraint *c = cpuc->event_constraint[idx];\n\tstruct intel_excl_states *xl;\n\tint tid = cpuc->excl_thread_id;\n\n\tif (cpuc->is_fake || !is_ht_workaround_enabled())\n\t\treturn;\n\n\tif (WARN_ON_ONCE(!excl_cntrs))\n\t\treturn;\n\n\tif (!(c->flags & PERF_X86_EVENT_DYNAMIC))\n\t\treturn;\n\n\txl = &excl_cntrs->states[tid];\n\n\tlockdep_assert_held(&excl_cntrs->lock);\n\n\tif (c->flags & PERF_X86_EVENT_EXCL)\n\t\txl->state[cntr] = INTEL_EXCL_EXCLUSIVE;\n\telse\n\t\txl->state[cntr] = INTEL_EXCL_SHARED;\n}\n\nstatic void\nintel_stop_scheduling(struct cpu_hw_events *cpuc)\n{\n\tstruct intel_excl_cntrs *excl_cntrs = cpuc->excl_cntrs;\n\tstruct intel_excl_states *xl;\n\tint tid = cpuc->excl_thread_id;\n\n\t \n\tif (cpuc->is_fake || !is_ht_workaround_enabled())\n\t\treturn;\n\t \n\tif (WARN_ON_ONCE(!excl_cntrs))\n\t\treturn;\n\n\txl = &excl_cntrs->states[tid];\n\n\txl->sched_started = false;\n\t \n\traw_spin_unlock(&excl_cntrs->lock);\n}\n\nstatic struct event_constraint *\ndyn_constraint(struct cpu_hw_events *cpuc, struct event_constraint *c, int idx)\n{\n\tWARN_ON_ONCE(!cpuc->constraint_list);\n\n\tif (!(c->flags & PERF_X86_EVENT_DYNAMIC)) {\n\t\tstruct event_constraint *cx;\n\n\t\t \n\t\tcx = &cpuc->constraint_list[idx];\n\n\t\t \n\t\t*cx = *c;\n\n\t\t \n\t\tcx->flags |= PERF_X86_EVENT_DYNAMIC;\n\t\tc = cx;\n\t}\n\n\treturn c;\n}\n\nstatic struct event_constraint *\nintel_get_excl_constraints(struct cpu_hw_events *cpuc, struct perf_event *event,\n\t\t\t   int idx, struct event_constraint *c)\n{\n\tstruct intel_excl_cntrs *excl_cntrs = cpuc->excl_cntrs;\n\tstruct intel_excl_states *xlo;\n\tint tid = cpuc->excl_thread_id;\n\tint is_excl, i, w;\n\n\t \n\tif (cpuc->is_fake || !is_ht_workaround_enabled())\n\t\treturn c;\n\n\t \n\tif (WARN_ON_ONCE(!excl_cntrs))\n\t\treturn c;\n\n\t \n\tc = dyn_constraint(cpuc, c, idx);\n\n\t \n\n\t \n\txlo = &excl_cntrs->states[tid ^ 1];\n\n\t \n\tis_excl = c->flags & PERF_X86_EVENT_EXCL;\n\tif (is_excl && !(event->hw.flags & PERF_X86_EVENT_EXCL_ACCT)) {\n\t\tevent->hw.flags |= PERF_X86_EVENT_EXCL_ACCT;\n\t\tif (!cpuc->n_excl++)\n\t\t\tWRITE_ONCE(excl_cntrs->has_exclusive[tid], 1);\n\t}\n\n\t \n\tw = c->weight;\n\tfor_each_set_bit(i, c->idxmsk, X86_PMC_IDX_MAX) {\n\t\t \n\t\tif (xlo->state[i] == INTEL_EXCL_EXCLUSIVE) {\n\t\t\t__clear_bit(i, c->idxmsk);\n\t\t\tw--;\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tif (is_excl && xlo->state[i] == INTEL_EXCL_SHARED) {\n\t\t\t__clear_bit(i, c->idxmsk);\n\t\t\tw--;\n\t\t\tcontinue;\n\t\t}\n\t}\n\n\t \n\tif (!w)\n\t\tc = &emptyconstraint;\n\n\tc->weight = w;\n\n\treturn c;\n}\n\nstatic struct event_constraint *\nintel_get_event_constraints(struct cpu_hw_events *cpuc, int idx,\n\t\t\t    struct perf_event *event)\n{\n\tstruct event_constraint *c1, *c2;\n\n\tc1 = cpuc->event_constraint[idx];\n\n\t \n\tc2 = __intel_get_event_constraints(cpuc, idx, event);\n\tif (c1) {\n\t        WARN_ON_ONCE(!(c1->flags & PERF_X86_EVENT_DYNAMIC));\n\t\tbitmap_copy(c1->idxmsk, c2->idxmsk, X86_PMC_IDX_MAX);\n\t\tc1->weight = c2->weight;\n\t\tc2 = c1;\n\t}\n\n\tif (cpuc->excl_cntrs)\n\t\treturn intel_get_excl_constraints(cpuc, event, idx, c2);\n\n\treturn c2;\n}\n\nstatic void intel_put_excl_constraints(struct cpu_hw_events *cpuc,\n\t\tstruct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct intel_excl_cntrs *excl_cntrs = cpuc->excl_cntrs;\n\tint tid = cpuc->excl_thread_id;\n\tstruct intel_excl_states *xl;\n\n\t \n\tif (cpuc->is_fake)\n\t\treturn;\n\n\tif (WARN_ON_ONCE(!excl_cntrs))\n\t\treturn;\n\n\tif (hwc->flags & PERF_X86_EVENT_EXCL_ACCT) {\n\t\thwc->flags &= ~PERF_X86_EVENT_EXCL_ACCT;\n\t\tif (!--cpuc->n_excl)\n\t\t\tWRITE_ONCE(excl_cntrs->has_exclusive[tid], 0);\n\t}\n\n\t \n\tif (hwc->idx >= 0) {\n\t\txl = &excl_cntrs->states[tid];\n\n\t\t \n\t\tif (!xl->sched_started)\n\t\t\traw_spin_lock(&excl_cntrs->lock);\n\n\t\txl->state[hwc->idx] = INTEL_EXCL_UNUSED;\n\n\t\tif (!xl->sched_started)\n\t\t\traw_spin_unlock(&excl_cntrs->lock);\n\t}\n}\n\nstatic void\nintel_put_shared_regs_event_constraints(struct cpu_hw_events *cpuc,\n\t\t\t\t\tstruct perf_event *event)\n{\n\tstruct hw_perf_event_extra *reg;\n\n\treg = &event->hw.extra_reg;\n\tif (reg->idx != EXTRA_REG_NONE)\n\t\t__intel_shared_reg_put_constraints(cpuc, reg);\n\n\treg = &event->hw.branch_reg;\n\tif (reg->idx != EXTRA_REG_NONE)\n\t\t__intel_shared_reg_put_constraints(cpuc, reg);\n}\n\nstatic void intel_put_event_constraints(struct cpu_hw_events *cpuc,\n\t\t\t\t\tstruct perf_event *event)\n{\n\tintel_put_shared_regs_event_constraints(cpuc, event);\n\n\t \n\tif (cpuc->excl_cntrs)\n\t\tintel_put_excl_constraints(cpuc, event);\n}\n\nstatic void intel_pebs_aliases_core2(struct perf_event *event)\n{\n\tif ((event->hw.config & X86_RAW_EVENT_MASK) == 0x003c) {\n\t\t \n\t\tu64 alt_config = X86_CONFIG(.event=0xc0, .inv=1, .cmask=16);\n\n\t\talt_config |= (event->hw.config & ~X86_RAW_EVENT_MASK);\n\t\tevent->hw.config = alt_config;\n\t}\n}\n\nstatic void intel_pebs_aliases_snb(struct perf_event *event)\n{\n\tif ((event->hw.config & X86_RAW_EVENT_MASK) == 0x003c) {\n\t\t \n\t\tu64 alt_config = X86_CONFIG(.event=0xc2, .umask=0x01, .inv=1, .cmask=16);\n\n\t\talt_config |= (event->hw.config & ~X86_RAW_EVENT_MASK);\n\t\tevent->hw.config = alt_config;\n\t}\n}\n\nstatic void intel_pebs_aliases_precdist(struct perf_event *event)\n{\n\tif ((event->hw.config & X86_RAW_EVENT_MASK) == 0x003c) {\n\t\t \n\t\tu64 alt_config = X86_CONFIG(.event=0xc0, .umask=0x01, .inv=1, .cmask=16);\n\n\t\talt_config |= (event->hw.config & ~X86_RAW_EVENT_MASK);\n\t\tevent->hw.config = alt_config;\n\t}\n}\n\nstatic void intel_pebs_aliases_ivb(struct perf_event *event)\n{\n\tif (event->attr.precise_ip < 3)\n\t\treturn intel_pebs_aliases_snb(event);\n\treturn intel_pebs_aliases_precdist(event);\n}\n\nstatic void intel_pebs_aliases_skl(struct perf_event *event)\n{\n\tif (event->attr.precise_ip < 3)\n\t\treturn intel_pebs_aliases_core2(event);\n\treturn intel_pebs_aliases_precdist(event);\n}\n\nstatic unsigned long intel_pmu_large_pebs_flags(struct perf_event *event)\n{\n\tunsigned long flags = x86_pmu.large_pebs_flags;\n\n\tif (event->attr.use_clockid)\n\t\tflags &= ~PERF_SAMPLE_TIME;\n\tif (!event->attr.exclude_kernel)\n\t\tflags &= ~PERF_SAMPLE_REGS_USER;\n\tif (event->attr.sample_regs_user & ~PEBS_GP_REGS)\n\t\tflags &= ~(PERF_SAMPLE_REGS_USER | PERF_SAMPLE_REGS_INTR);\n\treturn flags;\n}\n\nstatic int intel_pmu_bts_config(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\n\tif (unlikely(intel_pmu_has_bts(event))) {\n\t\t \n\t\tif (!x86_pmu.bts_active)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\t \n\t\tif (!attr->exclude_kernel)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\t \n\t\tif (attr->precise_ip)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\t \n\t\tif (x86_add_exclusive(x86_lbr_exclusive_lbr))\n\t\t\treturn -EBUSY;\n\n\t\tevent->destroy = hw_perf_lbr_event_destroy;\n\t}\n\n\treturn 0;\n}\n\nstatic int core_pmu_hw_config(struct perf_event *event)\n{\n\tint ret = x86_pmu_hw_config(event);\n\n\tif (ret)\n\t\treturn ret;\n\n\treturn intel_pmu_bts_config(event);\n}\n\n#define INTEL_TD_METRIC_AVAILABLE_MAX\t(INTEL_TD_METRIC_RETIRING + \\\n\t\t\t\t\t ((x86_pmu.num_topdown_events - 1) << 8))\n\nstatic bool is_available_metric_event(struct perf_event *event)\n{\n\treturn is_metric_event(event) &&\n\t\tevent->attr.config <= INTEL_TD_METRIC_AVAILABLE_MAX;\n}\n\nstatic inline bool is_mem_loads_event(struct perf_event *event)\n{\n\treturn (event->attr.config & INTEL_ARCH_EVENT_MASK) == X86_CONFIG(.event=0xcd, .umask=0x01);\n}\n\nstatic inline bool is_mem_loads_aux_event(struct perf_event *event)\n{\n\treturn (event->attr.config & INTEL_ARCH_EVENT_MASK) == X86_CONFIG(.event=0x03, .umask=0x82);\n}\n\nstatic inline bool require_mem_loads_aux_event(struct perf_event *event)\n{\n\tif (!(x86_pmu.flags & PMU_FL_MEM_LOADS_AUX))\n\t\treturn false;\n\n\tif (is_hybrid())\n\t\treturn hybrid_pmu(event->pmu)->cpu_type == hybrid_big;\n\n\treturn true;\n}\n\nstatic inline bool intel_pmu_has_cap(struct perf_event *event, int idx)\n{\n\tunion perf_capabilities *intel_cap = &hybrid(event->pmu, intel_cap);\n\n\treturn test_bit(idx, (unsigned long *)&intel_cap->capabilities);\n}\n\nstatic int intel_pmu_hw_config(struct perf_event *event)\n{\n\tint ret = x86_pmu_hw_config(event);\n\n\tif (ret)\n\t\treturn ret;\n\n\tret = intel_pmu_bts_config(event);\n\tif (ret)\n\t\treturn ret;\n\n\tif (event->attr.precise_ip) {\n\t\tif ((event->attr.config & INTEL_ARCH_EVENT_MASK) == INTEL_FIXED_VLBR_EVENT)\n\t\t\treturn -EINVAL;\n\n\t\tif (!(event->attr.freq || (event->attr.wakeup_events && !event->attr.watermark))) {\n\t\t\tevent->hw.flags |= PERF_X86_EVENT_AUTO_RELOAD;\n\t\t\tif (!(event->attr.sample_type &\n\t\t\t      ~intel_pmu_large_pebs_flags(event))) {\n\t\t\t\tevent->hw.flags |= PERF_X86_EVENT_LARGE_PEBS;\n\t\t\t\tevent->attach_state |= PERF_ATTACH_SCHED_CB;\n\t\t\t}\n\t\t}\n\t\tif (x86_pmu.pebs_aliases)\n\t\t\tx86_pmu.pebs_aliases(event);\n\t}\n\n\tif (needs_branch_stack(event)) {\n\t\tret = intel_pmu_setup_lbr_filter(event);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tevent->attach_state |= PERF_ATTACH_SCHED_CB;\n\n\t\t \n\t\tif (!unlikely(intel_pmu_has_bts(event))) {\n\t\t\t \n\t\t\tif (x86_add_exclusive(x86_lbr_exclusive_lbr))\n\t\t\t\treturn -EBUSY;\n\n\t\t\tevent->destroy = hw_perf_lbr_event_destroy;\n\t\t}\n\t}\n\n\tif (event->attr.aux_output) {\n\t\tif (!event->attr.precise_ip)\n\t\t\treturn -EINVAL;\n\n\t\tevent->hw.flags |= PERF_X86_EVENT_PEBS_VIA_PT;\n\t}\n\n\tif ((event->attr.type == PERF_TYPE_HARDWARE) ||\n\t    (event->attr.type == PERF_TYPE_HW_CACHE))\n\t\treturn 0;\n\n\t \n\tif (intel_pmu_has_cap(event, PERF_CAP_METRICS_IDX) && is_topdown_event(event)) {\n\t\tif (event->attr.config1 || event->attr.config2)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (event->attr.config & X86_ALL_EVENT_FLAGS)\n\t\t\treturn -EINVAL;\n\n\t\tif (is_available_metric_event(event)) {\n\t\t\tstruct perf_event *leader = event->group_leader;\n\n\t\t\t \n\t\t\tif (is_sampling_event(event))\n\t\t\t\treturn -EINVAL;\n\n\t\t\t \n\t\t\tif (!is_slots_event(leader))\n\t\t\t\treturn -EINVAL;\n\n\t\t\t \n\t\t\tif (is_sampling_event(leader))\n\t\t\t\treturn -EINVAL;\n\n\t\t\tevent->event_caps |= PERF_EV_CAP_SIBLING;\n\t\t\t \n\t\t\tleader->hw.flags |= PERF_X86_EVENT_TOPDOWN;\n\t\t\tevent->hw.flags  |= PERF_X86_EVENT_TOPDOWN;\n\t\t}\n\t}\n\n\t \n\tif (require_mem_loads_aux_event(event) &&\n\t    (event->attr.sample_type & PERF_SAMPLE_DATA_SRC) &&\n\t    is_mem_loads_event(event)) {\n\t\tstruct perf_event *leader = event->group_leader;\n\t\tstruct perf_event *sibling = NULL;\n\n\t\t \n\t\tif (leader == event)\n\t\t\treturn -ENODATA;\n\n\t\tif (!is_mem_loads_aux_event(leader)) {\n\t\t\tfor_each_sibling_event(sibling, leader) {\n\t\t\t\tif (is_mem_loads_aux_event(sibling))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (list_entry_is_head(sibling, &leader->sibling_list, sibling_list))\n\t\t\t\treturn -ENODATA;\n\t\t}\n\t}\n\n\tif (!(event->attr.config & ARCH_PERFMON_EVENTSEL_ANY))\n\t\treturn 0;\n\n\tif (x86_pmu.version < 3)\n\t\treturn -EINVAL;\n\n\tret = perf_allow_cpu(&event->attr);\n\tif (ret)\n\t\treturn ret;\n\n\tevent->hw.config |= ARCH_PERFMON_EVENTSEL_ANY;\n\n\treturn 0;\n}\n\n \nstatic struct perf_guest_switch_msr *intel_guest_get_msrs(int *nr, void *data)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct perf_guest_switch_msr *arr = cpuc->guest_switch_msrs;\n\tstruct kvm_pmu *kvm_pmu = (struct kvm_pmu *)data;\n\tu64 intel_ctrl = hybrid(cpuc->pmu, intel_ctrl);\n\tu64 pebs_mask = cpuc->pebs_enabled & x86_pmu.pebs_capable;\n\tint global_ctrl, pebs_enable;\n\n\t \n\t*nr = 0;\n\tglobal_ctrl = (*nr)++;\n\tarr[global_ctrl] = (struct perf_guest_switch_msr){\n\t\t.msr = MSR_CORE_PERF_GLOBAL_CTRL,\n\t\t.host = intel_ctrl & ~cpuc->intel_ctrl_guest_mask,\n\t\t.guest = intel_ctrl & ~cpuc->intel_ctrl_host_mask & ~pebs_mask,\n\t};\n\n\tif (!x86_pmu.pebs)\n\t\treturn arr;\n\n\t \n\tif (x86_pmu.pebs_no_isolation) {\n\t\tarr[(*nr)++] = (struct perf_guest_switch_msr){\n\t\t\t.msr = MSR_IA32_PEBS_ENABLE,\n\t\t\t.host = cpuc->pebs_enabled,\n\t\t\t.guest = 0,\n\t\t};\n\t\treturn arr;\n\t}\n\n\tif (!kvm_pmu || !x86_pmu.pebs_ept)\n\t\treturn arr;\n\n\tarr[(*nr)++] = (struct perf_guest_switch_msr){\n\t\t.msr = MSR_IA32_DS_AREA,\n\t\t.host = (unsigned long)cpuc->ds,\n\t\t.guest = kvm_pmu->ds_area,\n\t};\n\n\tif (x86_pmu.intel_cap.pebs_baseline) {\n\t\tarr[(*nr)++] = (struct perf_guest_switch_msr){\n\t\t\t.msr = MSR_PEBS_DATA_CFG,\n\t\t\t.host = cpuc->active_pebs_data_cfg,\n\t\t\t.guest = kvm_pmu->pebs_data_cfg,\n\t\t};\n\t}\n\n\tpebs_enable = (*nr)++;\n\tarr[pebs_enable] = (struct perf_guest_switch_msr){\n\t\t.msr = MSR_IA32_PEBS_ENABLE,\n\t\t.host = cpuc->pebs_enabled & ~cpuc->intel_ctrl_guest_mask,\n\t\t.guest = pebs_mask & ~cpuc->intel_ctrl_host_mask,\n\t};\n\n\tif (arr[pebs_enable].host) {\n\t\t \n\t\tarr[pebs_enable].guest = 0;\n\t} else {\n\t\t \n\t\tarr[pebs_enable].guest &= ~kvm_pmu->host_cross_mapped_mask;\n\t\tarr[global_ctrl].guest &= ~kvm_pmu->host_cross_mapped_mask;\n\t\t \n\t\tarr[global_ctrl].guest |= arr[pebs_enable].guest;\n\t}\n\n\treturn arr;\n}\n\nstatic struct perf_guest_switch_msr *core_guest_get_msrs(int *nr, void *data)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct perf_guest_switch_msr *arr = cpuc->guest_switch_msrs;\n\tint idx;\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++)  {\n\t\tstruct perf_event *event = cpuc->events[idx];\n\n\t\tarr[idx].msr = x86_pmu_config_addr(idx);\n\t\tarr[idx].host = arr[idx].guest = 0;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\tarr[idx].host = arr[idx].guest =\n\t\t\tevent->hw.config | ARCH_PERFMON_EVENTSEL_ENABLE;\n\n\t\tif (event->attr.exclude_host)\n\t\t\tarr[idx].host &= ~ARCH_PERFMON_EVENTSEL_ENABLE;\n\t\telse if (event->attr.exclude_guest)\n\t\t\tarr[idx].guest &= ~ARCH_PERFMON_EVENTSEL_ENABLE;\n\t}\n\n\t*nr = x86_pmu.num_counters;\n\treturn arr;\n}\n\nstatic void core_pmu_enable_event(struct perf_event *event)\n{\n\tif (!event->attr.exclude_host)\n\t\tx86_pmu_enable_event(event);\n}\n\nstatic void core_pmu_enable_all(int added)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tint idx;\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tstruct hw_perf_event *hwc = &cpuc->events[idx]->hw;\n\n\t\tif (!test_bit(idx, cpuc->active_mask) ||\n\t\t\t\tcpuc->events[idx]->attr.exclude_host)\n\t\t\tcontinue;\n\n\t\t__x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);\n\t}\n}\n\nstatic int hsw_hw_config(struct perf_event *event)\n{\n\tint ret = intel_pmu_hw_config(event);\n\n\tif (ret)\n\t\treturn ret;\n\tif (!boot_cpu_has(X86_FEATURE_RTM) && !boot_cpu_has(X86_FEATURE_HLE))\n\t\treturn 0;\n\tevent->hw.config |= event->attr.config & (HSW_IN_TX|HSW_IN_TX_CHECKPOINTED);\n\n\t \n\tif ((event->hw.config & (HSW_IN_TX|HSW_IN_TX_CHECKPOINTED)) &&\n\t     ((event->hw.config & ARCH_PERFMON_EVENTSEL_ANY) ||\n\t      event->attr.precise_ip > 0))\n\t\treturn -EOPNOTSUPP;\n\n\tif (event_is_checkpointed(event)) {\n\t\t \n\t\tif (event->attr.sample_period > 0 &&\n\t\t    event->attr.sample_period < 0x7fffffff)\n\t\t\treturn -EOPNOTSUPP;\n\t}\n\treturn 0;\n}\n\nstatic struct event_constraint counter0_constraint =\n\t\t\tINTEL_ALL_EVENT_CONSTRAINT(0, 0x1);\n\nstatic struct event_constraint counter1_constraint =\n\t\t\tINTEL_ALL_EVENT_CONSTRAINT(0, 0x2);\n\nstatic struct event_constraint counter0_1_constraint =\n\t\t\tINTEL_ALL_EVENT_CONSTRAINT(0, 0x3);\n\nstatic struct event_constraint counter2_constraint =\n\t\t\tEVENT_CONSTRAINT(0, 0x4, 0);\n\nstatic struct event_constraint fixed0_constraint =\n\t\t\tFIXED_EVENT_CONSTRAINT(0x00c0, 0);\n\nstatic struct event_constraint fixed0_counter0_constraint =\n\t\t\tINTEL_ALL_EVENT_CONSTRAINT(0, 0x100000001ULL);\n\nstatic struct event_constraint fixed0_counter0_1_constraint =\n\t\t\tINTEL_ALL_EVENT_CONSTRAINT(0, 0x100000003ULL);\n\nstatic struct event_constraint counters_1_7_constraint =\n\t\t\tINTEL_ALL_EVENT_CONSTRAINT(0, 0xfeULL);\n\nstatic struct event_constraint *\nhsw_get_event_constraints(struct cpu_hw_events *cpuc, int idx,\n\t\t\t  struct perf_event *event)\n{\n\tstruct event_constraint *c;\n\n\tc = intel_get_event_constraints(cpuc, idx, event);\n\n\t \n\tif (event->hw.config & HSW_IN_TX_CHECKPOINTED) {\n\t\tif (c->idxmsk64 & (1U << 2))\n\t\t\treturn &counter2_constraint;\n\t\treturn &emptyconstraint;\n\t}\n\n\treturn c;\n}\n\nstatic struct event_constraint *\nicl_get_event_constraints(struct cpu_hw_events *cpuc, int idx,\n\t\t\t  struct perf_event *event)\n{\n\t \n\tif ((event->attr.precise_ip == 3) &&\n\t    constraint_match(&fixed0_constraint, event->hw.config))\n\t\treturn &fixed0_constraint;\n\n\treturn hsw_get_event_constraints(cpuc, idx, event);\n}\n\nstatic struct event_constraint *\nspr_get_event_constraints(struct cpu_hw_events *cpuc, int idx,\n\t\t\t  struct perf_event *event)\n{\n\tstruct event_constraint *c;\n\n\tc = icl_get_event_constraints(cpuc, idx, event);\n\n\t \n\tif ((event->attr.precise_ip == 3) &&\n\t    !constraint_match(&fixed0_constraint, event->hw.config)) {\n\t\tif (c->idxmsk64 & BIT_ULL(0))\n\t\t\treturn &counter0_constraint;\n\n\t\treturn &emptyconstraint;\n\t}\n\n\treturn c;\n}\n\nstatic struct event_constraint *\nglp_get_event_constraints(struct cpu_hw_events *cpuc, int idx,\n\t\t\t  struct perf_event *event)\n{\n\tstruct event_constraint *c;\n\n\t \n\tif (event->attr.precise_ip == 3)\n\t\treturn &counter0_constraint;\n\n\tc = intel_get_event_constraints(cpuc, idx, event);\n\n\treturn c;\n}\n\nstatic struct event_constraint *\ntnt_get_event_constraints(struct cpu_hw_events *cpuc, int idx,\n\t\t\t  struct perf_event *event)\n{\n\tstruct event_constraint *c;\n\n\tc = intel_get_event_constraints(cpuc, idx, event);\n\n\t \n\tif (event->attr.precise_ip == 3) {\n\t\t \n\t\tif (constraint_match(&fixed0_constraint, event->hw.config))\n\t\t\treturn &fixed0_counter0_constraint;\n\n\t\treturn &counter0_constraint;\n\t}\n\n\treturn c;\n}\n\nstatic bool allow_tsx_force_abort = true;\n\nstatic struct event_constraint *\ntfa_get_event_constraints(struct cpu_hw_events *cpuc, int idx,\n\t\t\t  struct perf_event *event)\n{\n\tstruct event_constraint *c = hsw_get_event_constraints(cpuc, idx, event);\n\n\t \n\tif (!allow_tsx_force_abort && test_bit(3, c->idxmsk)) {\n\t\tc = dyn_constraint(cpuc, c, idx);\n\t\tc->idxmsk64 &= ~(1ULL << 3);\n\t\tc->weight--;\n\t}\n\n\treturn c;\n}\n\nstatic struct event_constraint *\nadl_get_event_constraints(struct cpu_hw_events *cpuc, int idx,\n\t\t\t  struct perf_event *event)\n{\n\tstruct x86_hybrid_pmu *pmu = hybrid_pmu(event->pmu);\n\n\tif (pmu->cpu_type == hybrid_big)\n\t\treturn spr_get_event_constraints(cpuc, idx, event);\n\telse if (pmu->cpu_type == hybrid_small)\n\t\treturn tnt_get_event_constraints(cpuc, idx, event);\n\n\tWARN_ON(1);\n\treturn &emptyconstraint;\n}\n\nstatic struct event_constraint *\ncmt_get_event_constraints(struct cpu_hw_events *cpuc, int idx,\n\t\t\t  struct perf_event *event)\n{\n\tstruct event_constraint *c;\n\n\tc = intel_get_event_constraints(cpuc, idx, event);\n\n\t \n\tif (event->attr.precise_ip == 3) {\n\t\t \n\t\tif (constraint_match(&fixed0_constraint, event->hw.config))\n\t\t\treturn &fixed0_counter0_1_constraint;\n\n\t\tswitch (c->idxmsk64 & 0x3ull) {\n\t\tcase 0x1:\n\t\t\treturn &counter0_constraint;\n\t\tcase 0x2:\n\t\t\treturn &counter1_constraint;\n\t\tcase 0x3:\n\t\t\treturn &counter0_1_constraint;\n\t\t}\n\t\treturn &emptyconstraint;\n\t}\n\n\treturn c;\n}\n\nstatic struct event_constraint *\nrwc_get_event_constraints(struct cpu_hw_events *cpuc, int idx,\n\t\t\t  struct perf_event *event)\n{\n\tstruct event_constraint *c;\n\n\tc = spr_get_event_constraints(cpuc, idx, event);\n\n\t \n\tif (event->attr.precise_ip &&\n\t    (event->attr.sample_type & PERF_SAMPLE_WEIGHT_TYPE) &&\n\t    constraint_match(&fixed0_constraint, event->hw.config)) {\n\t\t \n\t\tif (event->attr.precise_ip == 3)\n\t\t\treturn &emptyconstraint;\n\t\treturn &counters_1_7_constraint;\n\t}\n\n\treturn c;\n}\n\nstatic struct event_constraint *\nmtl_get_event_constraints(struct cpu_hw_events *cpuc, int idx,\n\t\t\t  struct perf_event *event)\n{\n\tstruct x86_hybrid_pmu *pmu = hybrid_pmu(event->pmu);\n\n\tif (pmu->cpu_type == hybrid_big)\n\t\treturn rwc_get_event_constraints(cpuc, idx, event);\n\tif (pmu->cpu_type == hybrid_small)\n\t\treturn cmt_get_event_constraints(cpuc, idx, event);\n\n\tWARN_ON(1);\n\treturn &emptyconstraint;\n}\n\nstatic int adl_hw_config(struct perf_event *event)\n{\n\tstruct x86_hybrid_pmu *pmu = hybrid_pmu(event->pmu);\n\n\tif (pmu->cpu_type == hybrid_big)\n\t\treturn hsw_hw_config(event);\n\telse if (pmu->cpu_type == hybrid_small)\n\t\treturn intel_pmu_hw_config(event);\n\n\tWARN_ON(1);\n\treturn -EOPNOTSUPP;\n}\n\nstatic u8 adl_get_hybrid_cpu_type(void)\n{\n\treturn hybrid_big;\n}\n\n \nstatic void bdw_limit_period(struct perf_event *event, s64 *left)\n{\n\tif ((event->hw.config & INTEL_ARCH_EVENT_MASK) ==\n\t\t\tX86_CONFIG(.event=0xc0, .umask=0x01)) {\n\t\tif (*left < 128)\n\t\t\t*left = 128;\n\t\t*left &= ~0x3fULL;\n\t}\n}\n\nstatic void nhm_limit_period(struct perf_event *event, s64 *left)\n{\n\t*left = max(*left, 32LL);\n}\n\nstatic void spr_limit_period(struct perf_event *event, s64 *left)\n{\n\tif (event->attr.precise_ip == 3)\n\t\t*left = max(*left, 128LL);\n}\n\nPMU_FORMAT_ATTR(event,\t\"config:0-7\"\t);\nPMU_FORMAT_ATTR(umask,\t\"config:8-15\"\t);\nPMU_FORMAT_ATTR(edge,\t\"config:18\"\t);\nPMU_FORMAT_ATTR(pc,\t\"config:19\"\t);\nPMU_FORMAT_ATTR(any,\t\"config:21\"\t);  \nPMU_FORMAT_ATTR(inv,\t\"config:23\"\t);\nPMU_FORMAT_ATTR(cmask,\t\"config:24-31\"\t);\nPMU_FORMAT_ATTR(in_tx,  \"config:32\");\nPMU_FORMAT_ATTR(in_tx_cp, \"config:33\");\n\nstatic struct attribute *intel_arch_formats_attr[] = {\n\t&format_attr_event.attr,\n\t&format_attr_umask.attr,\n\t&format_attr_edge.attr,\n\t&format_attr_pc.attr,\n\t&format_attr_inv.attr,\n\t&format_attr_cmask.attr,\n\tNULL,\n};\n\nssize_t intel_event_sysfs_show(char *page, u64 config)\n{\n\tu64 event = (config & ARCH_PERFMON_EVENTSEL_EVENT);\n\n\treturn x86_event_sysfs_show(page, config, event);\n}\n\nstatic struct intel_shared_regs *allocate_shared_regs(int cpu)\n{\n\tstruct intel_shared_regs *regs;\n\tint i;\n\n\tregs = kzalloc_node(sizeof(struct intel_shared_regs),\n\t\t\t    GFP_KERNEL, cpu_to_node(cpu));\n\tif (regs) {\n\t\t \n\t\tfor (i = 0; i < EXTRA_REG_MAX; i++)\n\t\t\traw_spin_lock_init(&regs->regs[i].lock);\n\n\t\tregs->core_id = -1;\n\t}\n\treturn regs;\n}\n\nstatic struct intel_excl_cntrs *allocate_excl_cntrs(int cpu)\n{\n\tstruct intel_excl_cntrs *c;\n\n\tc = kzalloc_node(sizeof(struct intel_excl_cntrs),\n\t\t\t GFP_KERNEL, cpu_to_node(cpu));\n\tif (c) {\n\t\traw_spin_lock_init(&c->lock);\n\t\tc->core_id = -1;\n\t}\n\treturn c;\n}\n\n\nint intel_cpuc_prepare(struct cpu_hw_events *cpuc, int cpu)\n{\n\tcpuc->pebs_record_size = x86_pmu.pebs_record_size;\n\n\tif (is_hybrid() || x86_pmu.extra_regs || x86_pmu.lbr_sel_map) {\n\t\tcpuc->shared_regs = allocate_shared_regs(cpu);\n\t\tif (!cpuc->shared_regs)\n\t\t\tgoto err;\n\t}\n\n\tif (x86_pmu.flags & (PMU_FL_EXCL_CNTRS | PMU_FL_TFA)) {\n\t\tsize_t sz = X86_PMC_IDX_MAX * sizeof(struct event_constraint);\n\n\t\tcpuc->constraint_list = kzalloc_node(sz, GFP_KERNEL, cpu_to_node(cpu));\n\t\tif (!cpuc->constraint_list)\n\t\t\tgoto err_shared_regs;\n\t}\n\n\tif (x86_pmu.flags & PMU_FL_EXCL_CNTRS) {\n\t\tcpuc->excl_cntrs = allocate_excl_cntrs(cpu);\n\t\tif (!cpuc->excl_cntrs)\n\t\t\tgoto err_constraint_list;\n\n\t\tcpuc->excl_thread_id = 0;\n\t}\n\n\treturn 0;\n\nerr_constraint_list:\n\tkfree(cpuc->constraint_list);\n\tcpuc->constraint_list = NULL;\n\nerr_shared_regs:\n\tkfree(cpuc->shared_regs);\n\tcpuc->shared_regs = NULL;\n\nerr:\n\treturn -ENOMEM;\n}\n\nstatic int intel_pmu_cpu_prepare(int cpu)\n{\n\treturn intel_cpuc_prepare(&per_cpu(cpu_hw_events, cpu), cpu);\n}\n\nstatic void flip_smm_bit(void *data)\n{\n\tunsigned long set = *(unsigned long *)data;\n\n\tif (set > 0) {\n\t\tmsr_set_bit(MSR_IA32_DEBUGCTLMSR,\n\t\t\t    DEBUGCTLMSR_FREEZE_IN_SMM_BIT);\n\t} else {\n\t\tmsr_clear_bit(MSR_IA32_DEBUGCTLMSR,\n\t\t\t      DEBUGCTLMSR_FREEZE_IN_SMM_BIT);\n\t}\n}\n\nstatic void intel_pmu_check_num_counters(int *num_counters,\n\t\t\t\t\t int *num_counters_fixed,\n\t\t\t\t\t u64 *intel_ctrl, u64 fixed_mask);\n\nstatic void update_pmu_cap(struct x86_hybrid_pmu *pmu)\n{\n\tunsigned int sub_bitmaps = cpuid_eax(ARCH_PERFMON_EXT_LEAF);\n\tunsigned int eax, ebx, ecx, edx;\n\n\tif (sub_bitmaps & ARCH_PERFMON_NUM_COUNTER_LEAF_BIT) {\n\t\tcpuid_count(ARCH_PERFMON_EXT_LEAF, ARCH_PERFMON_NUM_COUNTER_LEAF,\n\t\t\t    &eax, &ebx, &ecx, &edx);\n\t\tpmu->num_counters = fls(eax);\n\t\tpmu->num_counters_fixed = fls(ebx);\n\t\tintel_pmu_check_num_counters(&pmu->num_counters, &pmu->num_counters_fixed,\n\t\t\t\t\t     &pmu->intel_ctrl, ebx);\n\t}\n}\n\nstatic bool init_hybrid_pmu(int cpu)\n{\n\tstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\n\tu8 cpu_type = get_this_hybrid_cpu_type();\n\tstruct x86_hybrid_pmu *pmu = NULL;\n\tint i;\n\n\tif (!cpu_type && x86_pmu.get_hybrid_cpu_type)\n\t\tcpu_type = x86_pmu.get_hybrid_cpu_type();\n\n\tfor (i = 0; i < x86_pmu.num_hybrid_pmus; i++) {\n\t\tif (x86_pmu.hybrid_pmu[i].cpu_type == cpu_type) {\n\t\t\tpmu = &x86_pmu.hybrid_pmu[i];\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (WARN_ON_ONCE(!pmu || (pmu->pmu.type == -1))) {\n\t\tcpuc->pmu = NULL;\n\t\treturn false;\n\t}\n\n\t \n\tif (!cpumask_empty(&pmu->supported_cpus))\n\t\tgoto end;\n\n\tif (this_cpu_has(X86_FEATURE_ARCH_PERFMON_EXT))\n\t\tupdate_pmu_cap(pmu);\n\n\tif (!check_hw_exists(&pmu->pmu, pmu->num_counters, pmu->num_counters_fixed))\n\t\treturn false;\n\n\tpr_info(\"%s PMU driver: \", pmu->name);\n\n\tif (pmu->intel_cap.pebs_output_pt_available)\n\t\tpr_cont(\"PEBS-via-PT \");\n\n\tpr_cont(\"\\n\");\n\n\tx86_pmu_show_pmu_cap(pmu->num_counters, pmu->num_counters_fixed,\n\t\t\t     pmu->intel_ctrl);\n\nend:\n\tcpumask_set_cpu(cpu, &pmu->supported_cpus);\n\tcpuc->pmu = &pmu->pmu;\n\n\treturn true;\n}\n\nstatic void intel_pmu_cpu_starting(int cpu)\n{\n\tstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\n\tint core_id = topology_core_id(cpu);\n\tint i;\n\n\tif (is_hybrid() && !init_hybrid_pmu(cpu))\n\t\treturn;\n\n\tinit_debug_store_on_cpu(cpu);\n\t \n\tintel_pmu_lbr_reset();\n\n\tcpuc->lbr_sel = NULL;\n\n\tif (x86_pmu.flags & PMU_FL_TFA) {\n\t\tWARN_ON_ONCE(cpuc->tfa_shadow);\n\t\tcpuc->tfa_shadow = ~0ULL;\n\t\tintel_set_tfa(cpuc, false);\n\t}\n\n\tif (x86_pmu.version > 1)\n\t\tflip_smm_bit(&x86_pmu.attr_freeze_on_smi);\n\n\t \n\tif (!is_hybrid() && x86_pmu.intel_cap.perf_metrics) {\n\t\tunion perf_capabilities perf_cap;\n\n\t\trdmsrl(MSR_IA32_PERF_CAPABILITIES, perf_cap.capabilities);\n\t\tif (!perf_cap.perf_metrics) {\n\t\t\tx86_pmu.intel_cap.perf_metrics = 0;\n\t\t\tx86_pmu.intel_ctrl &= ~(1ULL << GLOBAL_CTRL_EN_PERF_METRICS);\n\t\t}\n\t}\n\n\tif (!cpuc->shared_regs)\n\t\treturn;\n\n\tif (!(x86_pmu.flags & PMU_FL_NO_HT_SHARING)) {\n\t\tfor_each_cpu(i, topology_sibling_cpumask(cpu)) {\n\t\t\tstruct intel_shared_regs *pc;\n\n\t\t\tpc = per_cpu(cpu_hw_events, i).shared_regs;\n\t\t\tif (pc && pc->core_id == core_id) {\n\t\t\t\tcpuc->kfree_on_online[0] = cpuc->shared_regs;\n\t\t\t\tcpuc->shared_regs = pc;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tcpuc->shared_regs->core_id = core_id;\n\t\tcpuc->shared_regs->refcnt++;\n\t}\n\n\tif (x86_pmu.lbr_sel_map)\n\t\tcpuc->lbr_sel = &cpuc->shared_regs->regs[EXTRA_REG_LBR];\n\n\tif (x86_pmu.flags & PMU_FL_EXCL_CNTRS) {\n\t\tfor_each_cpu(i, topology_sibling_cpumask(cpu)) {\n\t\t\tstruct cpu_hw_events *sibling;\n\t\t\tstruct intel_excl_cntrs *c;\n\n\t\t\tsibling = &per_cpu(cpu_hw_events, i);\n\t\t\tc = sibling->excl_cntrs;\n\t\t\tif (c && c->core_id == core_id) {\n\t\t\t\tcpuc->kfree_on_online[1] = cpuc->excl_cntrs;\n\t\t\t\tcpuc->excl_cntrs = c;\n\t\t\t\tif (!sibling->excl_thread_id)\n\t\t\t\t\tcpuc->excl_thread_id = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tcpuc->excl_cntrs->core_id = core_id;\n\t\tcpuc->excl_cntrs->refcnt++;\n\t}\n}\n\nstatic void free_excl_cntrs(struct cpu_hw_events *cpuc)\n{\n\tstruct intel_excl_cntrs *c;\n\n\tc = cpuc->excl_cntrs;\n\tif (c) {\n\t\tif (c->core_id == -1 || --c->refcnt == 0)\n\t\t\tkfree(c);\n\t\tcpuc->excl_cntrs = NULL;\n\t}\n\n\tkfree(cpuc->constraint_list);\n\tcpuc->constraint_list = NULL;\n}\n\nstatic void intel_pmu_cpu_dying(int cpu)\n{\n\tfini_debug_store_on_cpu(cpu);\n}\n\nvoid intel_cpuc_finish(struct cpu_hw_events *cpuc)\n{\n\tstruct intel_shared_regs *pc;\n\n\tpc = cpuc->shared_regs;\n\tif (pc) {\n\t\tif (pc->core_id == -1 || --pc->refcnt == 0)\n\t\t\tkfree(pc);\n\t\tcpuc->shared_regs = NULL;\n\t}\n\n\tfree_excl_cntrs(cpuc);\n}\n\nstatic void intel_pmu_cpu_dead(int cpu)\n{\n\tstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\n\n\tintel_cpuc_finish(cpuc);\n\n\tif (is_hybrid() && cpuc->pmu)\n\t\tcpumask_clear_cpu(cpu, &hybrid_pmu(cpuc->pmu)->supported_cpus);\n}\n\nstatic void intel_pmu_sched_task(struct perf_event_pmu_context *pmu_ctx,\n\t\t\t\t bool sched_in)\n{\n\tintel_pmu_pebs_sched_task(pmu_ctx, sched_in);\n\tintel_pmu_lbr_sched_task(pmu_ctx, sched_in);\n}\n\nstatic void intel_pmu_swap_task_ctx(struct perf_event_pmu_context *prev_epc,\n\t\t\t\t    struct perf_event_pmu_context *next_epc)\n{\n\tintel_pmu_lbr_swap_task_ctx(prev_epc, next_epc);\n}\n\nstatic int intel_pmu_check_period(struct perf_event *event, u64 value)\n{\n\treturn intel_pmu_has_bts_period(event, value) ? -EINVAL : 0;\n}\n\nstatic void intel_aux_output_init(void)\n{\n\t \n\tif (x86_pmu.intel_cap.pebs_output_pt_available)\n\t\tx86_pmu.assign = intel_pmu_assign_event;\n}\n\nstatic int intel_pmu_aux_output_match(struct perf_event *event)\n{\n\t \n\tif (!x86_pmu.intel_cap.pebs_output_pt_available)\n\t\treturn 0;\n\n\treturn is_intel_pt_event(event);\n}\n\nstatic void intel_pmu_filter(struct pmu *pmu, int cpu, bool *ret)\n{\n\tstruct x86_hybrid_pmu *hpmu = hybrid_pmu(pmu);\n\n\t*ret = !cpumask_test_cpu(cpu, &hpmu->supported_cpus);\n}\n\nPMU_FORMAT_ATTR(offcore_rsp, \"config1:0-63\");\n\nPMU_FORMAT_ATTR(ldlat, \"config1:0-15\");\n\nPMU_FORMAT_ATTR(frontend, \"config1:0-23\");\n\nPMU_FORMAT_ATTR(snoop_rsp, \"config1:0-63\");\n\nstatic struct attribute *intel_arch3_formats_attr[] = {\n\t&format_attr_event.attr,\n\t&format_attr_umask.attr,\n\t&format_attr_edge.attr,\n\t&format_attr_pc.attr,\n\t&format_attr_any.attr,\n\t&format_attr_inv.attr,\n\t&format_attr_cmask.attr,\n\tNULL,\n};\n\nstatic struct attribute *hsw_format_attr[] = {\n\t&format_attr_in_tx.attr,\n\t&format_attr_in_tx_cp.attr,\n\t&format_attr_offcore_rsp.attr,\n\t&format_attr_ldlat.attr,\n\tNULL\n};\n\nstatic struct attribute *nhm_format_attr[] = {\n\t&format_attr_offcore_rsp.attr,\n\t&format_attr_ldlat.attr,\n\tNULL\n};\n\nstatic struct attribute *slm_format_attr[] = {\n\t&format_attr_offcore_rsp.attr,\n\tNULL\n};\n\nstatic struct attribute *cmt_format_attr[] = {\n\t&format_attr_offcore_rsp.attr,\n\t&format_attr_ldlat.attr,\n\t&format_attr_snoop_rsp.attr,\n\tNULL\n};\n\nstatic struct attribute *skl_format_attr[] = {\n\t&format_attr_frontend.attr,\n\tNULL,\n};\n\nstatic __initconst const struct x86_pmu core_pmu = {\n\t.name\t\t\t= \"core\",\n\t.handle_irq\t\t= x86_pmu_handle_irq,\n\t.disable_all\t\t= x86_pmu_disable_all,\n\t.enable_all\t\t= core_pmu_enable_all,\n\t.enable\t\t\t= core_pmu_enable_event,\n\t.disable\t\t= x86_pmu_disable_event,\n\t.hw_config\t\t= core_pmu_hw_config,\n\t.schedule_events\t= x86_schedule_events,\n\t.eventsel\t\t= MSR_ARCH_PERFMON_EVENTSEL0,\n\t.perfctr\t\t= MSR_ARCH_PERFMON_PERFCTR0,\n\t.event_map\t\t= intel_pmu_event_map,\n\t.max_events\t\t= ARRAY_SIZE(intel_perfmon_event_map),\n\t.apic\t\t\t= 1,\n\t.large_pebs_flags\t= LARGE_PEBS_FLAGS,\n\n\t \n\t.max_period\t\t= (1ULL<<31) - 1,\n\t.get_event_constraints\t= intel_get_event_constraints,\n\t.put_event_constraints\t= intel_put_event_constraints,\n\t.event_constraints\t= intel_core_event_constraints,\n\t.guest_get_msrs\t\t= core_guest_get_msrs,\n\t.format_attrs\t\t= intel_arch_formats_attr,\n\t.events_sysfs_show\t= intel_event_sysfs_show,\n\n\t \n\t.cpu_prepare\t\t= intel_pmu_cpu_prepare,\n\t.cpu_starting\t\t= intel_pmu_cpu_starting,\n\t.cpu_dying\t\t= intel_pmu_cpu_dying,\n\t.cpu_dead\t\t= intel_pmu_cpu_dead,\n\n\t.check_period\t\t= intel_pmu_check_period,\n\n\t.lbr_reset\t\t= intel_pmu_lbr_reset_64,\n\t.lbr_read\t\t= intel_pmu_lbr_read_64,\n\t.lbr_save\t\t= intel_pmu_lbr_save,\n\t.lbr_restore\t\t= intel_pmu_lbr_restore,\n};\n\nstatic __initconst const struct x86_pmu intel_pmu = {\n\t.name\t\t\t= \"Intel\",\n\t.handle_irq\t\t= intel_pmu_handle_irq,\n\t.disable_all\t\t= intel_pmu_disable_all,\n\t.enable_all\t\t= intel_pmu_enable_all,\n\t.enable\t\t\t= intel_pmu_enable_event,\n\t.disable\t\t= intel_pmu_disable_event,\n\t.add\t\t\t= intel_pmu_add_event,\n\t.del\t\t\t= intel_pmu_del_event,\n\t.read\t\t\t= intel_pmu_read_event,\n\t.set_period\t\t= intel_pmu_set_period,\n\t.update\t\t\t= intel_pmu_update,\n\t.hw_config\t\t= intel_pmu_hw_config,\n\t.schedule_events\t= x86_schedule_events,\n\t.eventsel\t\t= MSR_ARCH_PERFMON_EVENTSEL0,\n\t.perfctr\t\t= MSR_ARCH_PERFMON_PERFCTR0,\n\t.event_map\t\t= intel_pmu_event_map,\n\t.max_events\t\t= ARRAY_SIZE(intel_perfmon_event_map),\n\t.apic\t\t\t= 1,\n\t.large_pebs_flags\t= LARGE_PEBS_FLAGS,\n\t \n\t.max_period\t\t= (1ULL << 31) - 1,\n\t.get_event_constraints\t= intel_get_event_constraints,\n\t.put_event_constraints\t= intel_put_event_constraints,\n\t.pebs_aliases\t\t= intel_pebs_aliases_core2,\n\n\t.format_attrs\t\t= intel_arch3_formats_attr,\n\t.events_sysfs_show\t= intel_event_sysfs_show,\n\n\t.cpu_prepare\t\t= intel_pmu_cpu_prepare,\n\t.cpu_starting\t\t= intel_pmu_cpu_starting,\n\t.cpu_dying\t\t= intel_pmu_cpu_dying,\n\t.cpu_dead\t\t= intel_pmu_cpu_dead,\n\n\t.guest_get_msrs\t\t= intel_guest_get_msrs,\n\t.sched_task\t\t= intel_pmu_sched_task,\n\t.swap_task_ctx\t\t= intel_pmu_swap_task_ctx,\n\n\t.check_period\t\t= intel_pmu_check_period,\n\n\t.aux_output_match\t= intel_pmu_aux_output_match,\n\n\t.lbr_reset\t\t= intel_pmu_lbr_reset_64,\n\t.lbr_read\t\t= intel_pmu_lbr_read_64,\n\t.lbr_save\t\t= intel_pmu_lbr_save,\n\t.lbr_restore\t\t= intel_pmu_lbr_restore,\n\n\t \n\t.attr_freeze_on_smi\t= 1,\n};\n\nstatic __init void intel_clovertown_quirk(void)\n{\n\t \n\tpr_warn(\"PEBS disabled due to CPU errata\\n\");\n\tx86_pmu.pebs = 0;\n\tx86_pmu.pebs_constraints = NULL;\n}\n\nstatic const struct x86_cpu_desc isolation_ucodes[] = {\n\tINTEL_CPU_DESC(INTEL_FAM6_HASWELL,\t\t 3, 0x0000001f),\n\tINTEL_CPU_DESC(INTEL_FAM6_HASWELL_L,\t\t 1, 0x0000001e),\n\tINTEL_CPU_DESC(INTEL_FAM6_HASWELL_G,\t\t 1, 0x00000015),\n\tINTEL_CPU_DESC(INTEL_FAM6_HASWELL_X,\t\t 2, 0x00000037),\n\tINTEL_CPU_DESC(INTEL_FAM6_HASWELL_X,\t\t 4, 0x0000000a),\n\tINTEL_CPU_DESC(INTEL_FAM6_BROADWELL,\t\t 4, 0x00000023),\n\tINTEL_CPU_DESC(INTEL_FAM6_BROADWELL_G,\t\t 1, 0x00000014),\n\tINTEL_CPU_DESC(INTEL_FAM6_BROADWELL_D,\t\t 2, 0x00000010),\n\tINTEL_CPU_DESC(INTEL_FAM6_BROADWELL_D,\t\t 3, 0x07000009),\n\tINTEL_CPU_DESC(INTEL_FAM6_BROADWELL_D,\t\t 4, 0x0f000009),\n\tINTEL_CPU_DESC(INTEL_FAM6_BROADWELL_D,\t\t 5, 0x0e000002),\n\tINTEL_CPU_DESC(INTEL_FAM6_BROADWELL_X,\t\t 1, 0x0b000014),\n\tINTEL_CPU_DESC(INTEL_FAM6_SKYLAKE_X,\t\t 3, 0x00000021),\n\tINTEL_CPU_DESC(INTEL_FAM6_SKYLAKE_X,\t\t 4, 0x00000000),\n\tINTEL_CPU_DESC(INTEL_FAM6_SKYLAKE_X,\t\t 5, 0x00000000),\n\tINTEL_CPU_DESC(INTEL_FAM6_SKYLAKE_X,\t\t 6, 0x00000000),\n\tINTEL_CPU_DESC(INTEL_FAM6_SKYLAKE_X,\t\t 7, 0x00000000),\n\tINTEL_CPU_DESC(INTEL_FAM6_SKYLAKE_X,\t\t11, 0x00000000),\n\tINTEL_CPU_DESC(INTEL_FAM6_SKYLAKE_L,\t\t 3, 0x0000007c),\n\tINTEL_CPU_DESC(INTEL_FAM6_SKYLAKE,\t\t 3, 0x0000007c),\n\tINTEL_CPU_DESC(INTEL_FAM6_KABYLAKE,\t\t 9, 0x0000004e),\n\tINTEL_CPU_DESC(INTEL_FAM6_KABYLAKE_L,\t\t 9, 0x0000004e),\n\tINTEL_CPU_DESC(INTEL_FAM6_KABYLAKE_L,\t\t10, 0x0000004e),\n\tINTEL_CPU_DESC(INTEL_FAM6_KABYLAKE_L,\t\t11, 0x0000004e),\n\tINTEL_CPU_DESC(INTEL_FAM6_KABYLAKE_L,\t\t12, 0x0000004e),\n\tINTEL_CPU_DESC(INTEL_FAM6_KABYLAKE,\t\t10, 0x0000004e),\n\tINTEL_CPU_DESC(INTEL_FAM6_KABYLAKE,\t\t11, 0x0000004e),\n\tINTEL_CPU_DESC(INTEL_FAM6_KABYLAKE,\t\t12, 0x0000004e),\n\tINTEL_CPU_DESC(INTEL_FAM6_KABYLAKE,\t\t13, 0x0000004e),\n\t{}\n};\n\nstatic void intel_check_pebs_isolation(void)\n{\n\tx86_pmu.pebs_no_isolation = !x86_cpu_has_min_microcode_rev(isolation_ucodes);\n}\n\nstatic __init void intel_pebs_isolation_quirk(void)\n{\n\tWARN_ON_ONCE(x86_pmu.check_microcode);\n\tx86_pmu.check_microcode = intel_check_pebs_isolation;\n\tintel_check_pebs_isolation();\n}\n\nstatic const struct x86_cpu_desc pebs_ucodes[] = {\n\tINTEL_CPU_DESC(INTEL_FAM6_SANDYBRIDGE,\t\t7, 0x00000028),\n\tINTEL_CPU_DESC(INTEL_FAM6_SANDYBRIDGE_X,\t6, 0x00000618),\n\tINTEL_CPU_DESC(INTEL_FAM6_SANDYBRIDGE_X,\t7, 0x0000070c),\n\t{}\n};\n\nstatic bool intel_snb_pebs_broken(void)\n{\n\treturn !x86_cpu_has_min_microcode_rev(pebs_ucodes);\n}\n\nstatic void intel_snb_check_microcode(void)\n{\n\tif (intel_snb_pebs_broken() == x86_pmu.pebs_broken)\n\t\treturn;\n\n\t \n\tif (x86_pmu.pebs_broken) {\n\t\tpr_info(\"PEBS enabled due to microcode update\\n\");\n\t\tx86_pmu.pebs_broken = 0;\n\t} else {\n\t\tpr_info(\"PEBS disabled due to CPU errata, please upgrade microcode\\n\");\n\t\tx86_pmu.pebs_broken = 1;\n\t}\n}\n\nstatic bool is_lbr_from(unsigned long msr)\n{\n\tunsigned long lbr_from_nr = x86_pmu.lbr_from + x86_pmu.lbr_nr;\n\n\treturn x86_pmu.lbr_from <= msr && msr < lbr_from_nr;\n}\n\n \nstatic bool check_msr(unsigned long msr, u64 mask)\n{\n\tu64 val_old, val_new, val_tmp;\n\n\t \n\tif (!boot_cpu_has(X86_FEATURE_HYPERVISOR))\n\t\treturn true;\n\n\t \n\tif (rdmsrl_safe(msr, &val_old))\n\t\treturn false;\n\n\t \n\tval_tmp = val_old ^ mask;\n\n\tif (is_lbr_from(msr))\n\t\tval_tmp = lbr_from_signext_quirk_wr(val_tmp);\n\n\tif (wrmsrl_safe(msr, val_tmp) ||\n\t    rdmsrl_safe(msr, &val_new))\n\t\treturn false;\n\n\t \n\tif (val_new != val_tmp)\n\t\treturn false;\n\n\tif (is_lbr_from(msr))\n\t\tval_old = lbr_from_signext_quirk_wr(val_old);\n\n\t \n\twrmsrl(msr, val_old);\n\n\treturn true;\n}\n\nstatic __init void intel_sandybridge_quirk(void)\n{\n\tx86_pmu.check_microcode = intel_snb_check_microcode;\n\tcpus_read_lock();\n\tintel_snb_check_microcode();\n\tcpus_read_unlock();\n}\n\nstatic const struct { int id; char *name; } intel_arch_events_map[] __initconst = {\n\t{ PERF_COUNT_HW_CPU_CYCLES, \"cpu cycles\" },\n\t{ PERF_COUNT_HW_INSTRUCTIONS, \"instructions\" },\n\t{ PERF_COUNT_HW_BUS_CYCLES, \"bus cycles\" },\n\t{ PERF_COUNT_HW_CACHE_REFERENCES, \"cache references\" },\n\t{ PERF_COUNT_HW_CACHE_MISSES, \"cache misses\" },\n\t{ PERF_COUNT_HW_BRANCH_INSTRUCTIONS, \"branch instructions\" },\n\t{ PERF_COUNT_HW_BRANCH_MISSES, \"branch misses\" },\n};\n\nstatic __init void intel_arch_events_quirk(void)\n{\n\tint bit;\n\n\t \n\tfor_each_set_bit(bit, x86_pmu.events_mask, ARRAY_SIZE(intel_arch_events_map)) {\n\t\tintel_perfmon_event_map[intel_arch_events_map[bit].id] = 0;\n\t\tpr_warn(\"CPUID marked event: \\'%s\\' unavailable\\n\",\n\t\t\tintel_arch_events_map[bit].name);\n\t}\n}\n\nstatic __init void intel_nehalem_quirk(void)\n{\n\tunion cpuid10_ebx ebx;\n\n\tebx.full = x86_pmu.events_maskl;\n\tif (ebx.split.no_branch_misses_retired) {\n\t\t \n\t\tintel_perfmon_event_map[PERF_COUNT_HW_BRANCH_MISSES] = 0x7f89;\n\t\tebx.split.no_branch_misses_retired = 0;\n\t\tx86_pmu.events_maskl = ebx.full;\n\t\tpr_info(\"CPU erratum AAJ80 worked around\\n\");\n\t}\n}\n\n \nstatic __init void intel_ht_bug(void)\n{\n\tx86_pmu.flags |= PMU_FL_EXCL_CNTRS | PMU_FL_EXCL_ENABLED;\n\n\tx86_pmu.start_scheduling = intel_start_scheduling;\n\tx86_pmu.commit_scheduling = intel_commit_scheduling;\n\tx86_pmu.stop_scheduling = intel_stop_scheduling;\n}\n\nEVENT_ATTR_STR(mem-loads,\tmem_ld_hsw,\t\"event=0xcd,umask=0x1,ldlat=3\");\nEVENT_ATTR_STR(mem-stores,\tmem_st_hsw,\t\"event=0xd0,umask=0x82\")\n\n \nEVENT_ATTR_STR(tx-start,\ttx_start,\t\"event=0xc9,umask=0x1\");\nEVENT_ATTR_STR(tx-commit,\ttx_commit,\t\"event=0xc9,umask=0x2\");\nEVENT_ATTR_STR(tx-abort,\ttx_abort,\t\"event=0xc9,umask=0x4\");\nEVENT_ATTR_STR(tx-capacity,\ttx_capacity,\t\"event=0x54,umask=0x2\");\nEVENT_ATTR_STR(tx-conflict,\ttx_conflict,\t\"event=0x54,umask=0x1\");\nEVENT_ATTR_STR(el-start,\tel_start,\t\"event=0xc8,umask=0x1\");\nEVENT_ATTR_STR(el-commit,\tel_commit,\t\"event=0xc8,umask=0x2\");\nEVENT_ATTR_STR(el-abort,\tel_abort,\t\"event=0xc8,umask=0x4\");\nEVENT_ATTR_STR(el-capacity,\tel_capacity,\t\"event=0x54,umask=0x2\");\nEVENT_ATTR_STR(el-conflict,\tel_conflict,\t\"event=0x54,umask=0x1\");\nEVENT_ATTR_STR(cycles-t,\tcycles_t,\t\"event=0x3c,in_tx=1\");\nEVENT_ATTR_STR(cycles-ct,\tcycles_ct,\t\"event=0x3c,in_tx=1,in_tx_cp=1\");\n\nstatic struct attribute *hsw_events_attrs[] = {\n\tEVENT_PTR(td_slots_issued),\n\tEVENT_PTR(td_slots_retired),\n\tEVENT_PTR(td_fetch_bubbles),\n\tEVENT_PTR(td_total_slots),\n\tEVENT_PTR(td_total_slots_scale),\n\tEVENT_PTR(td_recovery_bubbles),\n\tEVENT_PTR(td_recovery_bubbles_scale),\n\tNULL\n};\n\nstatic struct attribute *hsw_mem_events_attrs[] = {\n\tEVENT_PTR(mem_ld_hsw),\n\tEVENT_PTR(mem_st_hsw),\n\tNULL,\n};\n\nstatic struct attribute *hsw_tsx_events_attrs[] = {\n\tEVENT_PTR(tx_start),\n\tEVENT_PTR(tx_commit),\n\tEVENT_PTR(tx_abort),\n\tEVENT_PTR(tx_capacity),\n\tEVENT_PTR(tx_conflict),\n\tEVENT_PTR(el_start),\n\tEVENT_PTR(el_commit),\n\tEVENT_PTR(el_abort),\n\tEVENT_PTR(el_capacity),\n\tEVENT_PTR(el_conflict),\n\tEVENT_PTR(cycles_t),\n\tEVENT_PTR(cycles_ct),\n\tNULL\n};\n\nEVENT_ATTR_STR(tx-capacity-read,  tx_capacity_read,  \"event=0x54,umask=0x80\");\nEVENT_ATTR_STR(tx-capacity-write, tx_capacity_write, \"event=0x54,umask=0x2\");\nEVENT_ATTR_STR(el-capacity-read,  el_capacity_read,  \"event=0x54,umask=0x80\");\nEVENT_ATTR_STR(el-capacity-write, el_capacity_write, \"event=0x54,umask=0x2\");\n\nstatic struct attribute *icl_events_attrs[] = {\n\tEVENT_PTR(mem_ld_hsw),\n\tEVENT_PTR(mem_st_hsw),\n\tNULL,\n};\n\nstatic struct attribute *icl_td_events_attrs[] = {\n\tEVENT_PTR(slots),\n\tEVENT_PTR(td_retiring),\n\tEVENT_PTR(td_bad_spec),\n\tEVENT_PTR(td_fe_bound),\n\tEVENT_PTR(td_be_bound),\n\tNULL,\n};\n\nstatic struct attribute *icl_tsx_events_attrs[] = {\n\tEVENT_PTR(tx_start),\n\tEVENT_PTR(tx_abort),\n\tEVENT_PTR(tx_commit),\n\tEVENT_PTR(tx_capacity_read),\n\tEVENT_PTR(tx_capacity_write),\n\tEVENT_PTR(tx_conflict),\n\tEVENT_PTR(el_start),\n\tEVENT_PTR(el_abort),\n\tEVENT_PTR(el_commit),\n\tEVENT_PTR(el_capacity_read),\n\tEVENT_PTR(el_capacity_write),\n\tEVENT_PTR(el_conflict),\n\tEVENT_PTR(cycles_t),\n\tEVENT_PTR(cycles_ct),\n\tNULL,\n};\n\n\nEVENT_ATTR_STR(mem-stores,\tmem_st_spr,\t\"event=0xcd,umask=0x2\");\nEVENT_ATTR_STR(mem-loads-aux,\tmem_ld_aux,\t\"event=0x03,umask=0x82\");\n\nstatic struct attribute *spr_events_attrs[] = {\n\tEVENT_PTR(mem_ld_hsw),\n\tEVENT_PTR(mem_st_spr),\n\tEVENT_PTR(mem_ld_aux),\n\tNULL,\n};\n\nstatic struct attribute *spr_td_events_attrs[] = {\n\tEVENT_PTR(slots),\n\tEVENT_PTR(td_retiring),\n\tEVENT_PTR(td_bad_spec),\n\tEVENT_PTR(td_fe_bound),\n\tEVENT_PTR(td_be_bound),\n\tEVENT_PTR(td_heavy_ops),\n\tEVENT_PTR(td_br_mispredict),\n\tEVENT_PTR(td_fetch_lat),\n\tEVENT_PTR(td_mem_bound),\n\tNULL,\n};\n\nstatic struct attribute *spr_tsx_events_attrs[] = {\n\tEVENT_PTR(tx_start),\n\tEVENT_PTR(tx_abort),\n\tEVENT_PTR(tx_commit),\n\tEVENT_PTR(tx_capacity_read),\n\tEVENT_PTR(tx_capacity_write),\n\tEVENT_PTR(tx_conflict),\n\tEVENT_PTR(cycles_t),\n\tEVENT_PTR(cycles_ct),\n\tNULL,\n};\n\nstatic ssize_t freeze_on_smi_show(struct device *cdev,\n\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t  char *buf)\n{\n\treturn sprintf(buf, \"%lu\\n\", x86_pmu.attr_freeze_on_smi);\n}\n\nstatic DEFINE_MUTEX(freeze_on_smi_mutex);\n\nstatic ssize_t freeze_on_smi_store(struct device *cdev,\n\t\t\t\t   struct device_attribute *attr,\n\t\t\t\t   const char *buf, size_t count)\n{\n\tunsigned long val;\n\tssize_t ret;\n\n\tret = kstrtoul(buf, 0, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (val > 1)\n\t\treturn -EINVAL;\n\n\tmutex_lock(&freeze_on_smi_mutex);\n\n\tif (x86_pmu.attr_freeze_on_smi == val)\n\t\tgoto done;\n\n\tx86_pmu.attr_freeze_on_smi = val;\n\n\tcpus_read_lock();\n\ton_each_cpu(flip_smm_bit, &val, 1);\n\tcpus_read_unlock();\ndone:\n\tmutex_unlock(&freeze_on_smi_mutex);\n\n\treturn count;\n}\n\nstatic void update_tfa_sched(void *ignored)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\t \n\tif (test_bit(3, cpuc->active_mask))\n\t\tperf_pmu_resched(x86_get_pmu(smp_processor_id()));\n}\n\nstatic ssize_t show_sysctl_tfa(struct device *cdev,\n\t\t\t      struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\treturn snprintf(buf, 40, \"%d\\n\", allow_tsx_force_abort);\n}\n\nstatic ssize_t set_sysctl_tfa(struct device *cdev,\n\t\t\t      struct device_attribute *attr,\n\t\t\t      const char *buf, size_t count)\n{\n\tbool val;\n\tssize_t ret;\n\n\tret = kstrtobool(buf, &val);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (val == allow_tsx_force_abort)\n\t\treturn count;\n\n\tallow_tsx_force_abort = val;\n\n\tcpus_read_lock();\n\ton_each_cpu(update_tfa_sched, NULL, 1);\n\tcpus_read_unlock();\n\n\treturn count;\n}\n\n\nstatic DEVICE_ATTR_RW(freeze_on_smi);\n\nstatic ssize_t branches_show(struct device *cdev,\n\t\t\t     struct device_attribute *attr,\n\t\t\t     char *buf)\n{\n\treturn snprintf(buf, PAGE_SIZE, \"%d\\n\", x86_pmu.lbr_nr);\n}\n\nstatic DEVICE_ATTR_RO(branches);\n\nstatic struct attribute *lbr_attrs[] = {\n\t&dev_attr_branches.attr,\n\tNULL\n};\n\nstatic char pmu_name_str[30];\n\nstatic ssize_t pmu_name_show(struct device *cdev,\n\t\t\t     struct device_attribute *attr,\n\t\t\t     char *buf)\n{\n\treturn snprintf(buf, PAGE_SIZE, \"%s\\n\", pmu_name_str);\n}\n\nstatic DEVICE_ATTR_RO(pmu_name);\n\nstatic struct attribute *intel_pmu_caps_attrs[] = {\n       &dev_attr_pmu_name.attr,\n       NULL\n};\n\nstatic DEVICE_ATTR(allow_tsx_force_abort, 0644,\n\t\t   show_sysctl_tfa,\n\t\t   set_sysctl_tfa);\n\nstatic struct attribute *intel_pmu_attrs[] = {\n\t&dev_attr_freeze_on_smi.attr,\n\t&dev_attr_allow_tsx_force_abort.attr,\n\tNULL,\n};\n\nstatic umode_t\ntsx_is_visible(struct kobject *kobj, struct attribute *attr, int i)\n{\n\treturn boot_cpu_has(X86_FEATURE_RTM) ? attr->mode : 0;\n}\n\nstatic umode_t\npebs_is_visible(struct kobject *kobj, struct attribute *attr, int i)\n{\n\treturn x86_pmu.pebs ? attr->mode : 0;\n}\n\nstatic umode_t\nmem_is_visible(struct kobject *kobj, struct attribute *attr, int i)\n{\n\tif (attr == &event_attr_mem_ld_aux.attr.attr)\n\t\treturn x86_pmu.flags & PMU_FL_MEM_LOADS_AUX ? attr->mode : 0;\n\n\treturn pebs_is_visible(kobj, attr, i);\n}\n\nstatic umode_t\nlbr_is_visible(struct kobject *kobj, struct attribute *attr, int i)\n{\n\treturn x86_pmu.lbr_nr ? attr->mode : 0;\n}\n\nstatic umode_t\nexra_is_visible(struct kobject *kobj, struct attribute *attr, int i)\n{\n\treturn x86_pmu.version >= 2 ? attr->mode : 0;\n}\n\nstatic umode_t\ndefault_is_visible(struct kobject *kobj, struct attribute *attr, int i)\n{\n\tif (attr == &dev_attr_allow_tsx_force_abort.attr)\n\t\treturn x86_pmu.flags & PMU_FL_TFA ? attr->mode : 0;\n\n\treturn attr->mode;\n}\n\nstatic struct attribute_group group_events_td  = {\n\t.name = \"events\",\n};\n\nstatic struct attribute_group group_events_mem = {\n\t.name       = \"events\",\n\t.is_visible = mem_is_visible,\n};\n\nstatic struct attribute_group group_events_tsx = {\n\t.name       = \"events\",\n\t.is_visible = tsx_is_visible,\n};\n\nstatic struct attribute_group group_caps_gen = {\n\t.name  = \"caps\",\n\t.attrs = intel_pmu_caps_attrs,\n};\n\nstatic struct attribute_group group_caps_lbr = {\n\t.name       = \"caps\",\n\t.attrs\t    = lbr_attrs,\n\t.is_visible = lbr_is_visible,\n};\n\nstatic struct attribute_group group_format_extra = {\n\t.name       = \"format\",\n\t.is_visible = exra_is_visible,\n};\n\nstatic struct attribute_group group_format_extra_skl = {\n\t.name       = \"format\",\n\t.is_visible = exra_is_visible,\n};\n\nstatic struct attribute_group group_default = {\n\t.attrs      = intel_pmu_attrs,\n\t.is_visible = default_is_visible,\n};\n\nstatic const struct attribute_group *attr_update[] = {\n\t&group_events_td,\n\t&group_events_mem,\n\t&group_events_tsx,\n\t&group_caps_gen,\n\t&group_caps_lbr,\n\t&group_format_extra,\n\t&group_format_extra_skl,\n\t&group_default,\n\tNULL,\n};\n\nEVENT_ATTR_STR_HYBRID(slots,                 slots_adl,        \"event=0x00,umask=0x4\",                       hybrid_big);\nEVENT_ATTR_STR_HYBRID(topdown-retiring,      td_retiring_adl,  \"event=0xc2,umask=0x0;event=0x00,umask=0x80\", hybrid_big_small);\nEVENT_ATTR_STR_HYBRID(topdown-bad-spec,      td_bad_spec_adl,  \"event=0x73,umask=0x0;event=0x00,umask=0x81\", hybrid_big_small);\nEVENT_ATTR_STR_HYBRID(topdown-fe-bound,      td_fe_bound_adl,  \"event=0x71,umask=0x0;event=0x00,umask=0x82\", hybrid_big_small);\nEVENT_ATTR_STR_HYBRID(topdown-be-bound,      td_be_bound_adl,  \"event=0x74,umask=0x0;event=0x00,umask=0x83\", hybrid_big_small);\nEVENT_ATTR_STR_HYBRID(topdown-heavy-ops,     td_heavy_ops_adl, \"event=0x00,umask=0x84\",                      hybrid_big);\nEVENT_ATTR_STR_HYBRID(topdown-br-mispredict, td_br_mis_adl,    \"event=0x00,umask=0x85\",                      hybrid_big);\nEVENT_ATTR_STR_HYBRID(topdown-fetch-lat,     td_fetch_lat_adl, \"event=0x00,umask=0x86\",                      hybrid_big);\nEVENT_ATTR_STR_HYBRID(topdown-mem-bound,     td_mem_bound_adl, \"event=0x00,umask=0x87\",                      hybrid_big);\n\nstatic struct attribute *adl_hybrid_events_attrs[] = {\n\tEVENT_PTR(slots_adl),\n\tEVENT_PTR(td_retiring_adl),\n\tEVENT_PTR(td_bad_spec_adl),\n\tEVENT_PTR(td_fe_bound_adl),\n\tEVENT_PTR(td_be_bound_adl),\n\tEVENT_PTR(td_heavy_ops_adl),\n\tEVENT_PTR(td_br_mis_adl),\n\tEVENT_PTR(td_fetch_lat_adl),\n\tEVENT_PTR(td_mem_bound_adl),\n\tNULL,\n};\n\n \nEVENT_ATTR_STR_HYBRID(mem-loads,     mem_ld_adl,     \"event=0xd0,umask=0x5,ldlat=3;event=0xcd,umask=0x1,ldlat=3\", hybrid_big_small);\nEVENT_ATTR_STR_HYBRID(mem-stores,    mem_st_adl,     \"event=0xd0,umask=0x6;event=0xcd,umask=0x2\",                 hybrid_big_small);\nEVENT_ATTR_STR_HYBRID(mem-loads-aux, mem_ld_aux_adl, \"event=0x03,umask=0x82\",                                     hybrid_big);\n\nstatic struct attribute *adl_hybrid_mem_attrs[] = {\n\tEVENT_PTR(mem_ld_adl),\n\tEVENT_PTR(mem_st_adl),\n\tEVENT_PTR(mem_ld_aux_adl),\n\tNULL,\n};\n\nstatic struct attribute *mtl_hybrid_mem_attrs[] = {\n\tEVENT_PTR(mem_ld_adl),\n\tEVENT_PTR(mem_st_adl),\n\tNULL\n};\n\nEVENT_ATTR_STR_HYBRID(tx-start,          tx_start_adl,          \"event=0xc9,umask=0x1\",          hybrid_big);\nEVENT_ATTR_STR_HYBRID(tx-commit,         tx_commit_adl,         \"event=0xc9,umask=0x2\",          hybrid_big);\nEVENT_ATTR_STR_HYBRID(tx-abort,          tx_abort_adl,          \"event=0xc9,umask=0x4\",          hybrid_big);\nEVENT_ATTR_STR_HYBRID(tx-conflict,       tx_conflict_adl,       \"event=0x54,umask=0x1\",          hybrid_big);\nEVENT_ATTR_STR_HYBRID(cycles-t,          cycles_t_adl,          \"event=0x3c,in_tx=1\",            hybrid_big);\nEVENT_ATTR_STR_HYBRID(cycles-ct,         cycles_ct_adl,         \"event=0x3c,in_tx=1,in_tx_cp=1\", hybrid_big);\nEVENT_ATTR_STR_HYBRID(tx-capacity-read,  tx_capacity_read_adl,  \"event=0x54,umask=0x80\",         hybrid_big);\nEVENT_ATTR_STR_HYBRID(tx-capacity-write, tx_capacity_write_adl, \"event=0x54,umask=0x2\",          hybrid_big);\n\nstatic struct attribute *adl_hybrid_tsx_attrs[] = {\n\tEVENT_PTR(tx_start_adl),\n\tEVENT_PTR(tx_abort_adl),\n\tEVENT_PTR(tx_commit_adl),\n\tEVENT_PTR(tx_capacity_read_adl),\n\tEVENT_PTR(tx_capacity_write_adl),\n\tEVENT_PTR(tx_conflict_adl),\n\tEVENT_PTR(cycles_t_adl),\n\tEVENT_PTR(cycles_ct_adl),\n\tNULL,\n};\n\nFORMAT_ATTR_HYBRID(in_tx,       hybrid_big);\nFORMAT_ATTR_HYBRID(in_tx_cp,    hybrid_big);\nFORMAT_ATTR_HYBRID(offcore_rsp, hybrid_big_small);\nFORMAT_ATTR_HYBRID(ldlat,       hybrid_big_small);\nFORMAT_ATTR_HYBRID(frontend,    hybrid_big);\n\n#define ADL_HYBRID_RTM_FORMAT_ATTR\t\\\n\tFORMAT_HYBRID_PTR(in_tx),\t\\\n\tFORMAT_HYBRID_PTR(in_tx_cp)\n\n#define ADL_HYBRID_FORMAT_ATTR\t\t\\\n\tFORMAT_HYBRID_PTR(offcore_rsp),\t\\\n\tFORMAT_HYBRID_PTR(ldlat),\t\\\n\tFORMAT_HYBRID_PTR(frontend)\n\nstatic struct attribute *adl_hybrid_extra_attr_rtm[] = {\n\tADL_HYBRID_RTM_FORMAT_ATTR,\n\tADL_HYBRID_FORMAT_ATTR,\n\tNULL\n};\n\nstatic struct attribute *adl_hybrid_extra_attr[] = {\n\tADL_HYBRID_FORMAT_ATTR,\n\tNULL\n};\n\nFORMAT_ATTR_HYBRID(snoop_rsp,\thybrid_small);\n\nstatic struct attribute *mtl_hybrid_extra_attr_rtm[] = {\n\tADL_HYBRID_RTM_FORMAT_ATTR,\n\tADL_HYBRID_FORMAT_ATTR,\n\tFORMAT_HYBRID_PTR(snoop_rsp),\n\tNULL\n};\n\nstatic struct attribute *mtl_hybrid_extra_attr[] = {\n\tADL_HYBRID_FORMAT_ATTR,\n\tFORMAT_HYBRID_PTR(snoop_rsp),\n\tNULL\n};\n\nstatic bool is_attr_for_this_pmu(struct kobject *kobj, struct attribute *attr)\n{\n\tstruct device *dev = kobj_to_dev(kobj);\n\tstruct x86_hybrid_pmu *pmu =\n\t\tcontainer_of(dev_get_drvdata(dev), struct x86_hybrid_pmu, pmu);\n\tstruct perf_pmu_events_hybrid_attr *pmu_attr =\n\t\tcontainer_of(attr, struct perf_pmu_events_hybrid_attr, attr.attr);\n\n\treturn pmu->cpu_type & pmu_attr->pmu_type;\n}\n\nstatic umode_t hybrid_events_is_visible(struct kobject *kobj,\n\t\t\t\t\tstruct attribute *attr, int i)\n{\n\treturn is_attr_for_this_pmu(kobj, attr) ? attr->mode : 0;\n}\n\nstatic inline int hybrid_find_supported_cpu(struct x86_hybrid_pmu *pmu)\n{\n\tint cpu = cpumask_first(&pmu->supported_cpus);\n\n\treturn (cpu >= nr_cpu_ids) ? -1 : cpu;\n}\n\nstatic umode_t hybrid_tsx_is_visible(struct kobject *kobj,\n\t\t\t\t     struct attribute *attr, int i)\n{\n\tstruct device *dev = kobj_to_dev(kobj);\n\tstruct x86_hybrid_pmu *pmu =\n\t\t container_of(dev_get_drvdata(dev), struct x86_hybrid_pmu, pmu);\n\tint cpu = hybrid_find_supported_cpu(pmu);\n\n\treturn (cpu >= 0) && is_attr_for_this_pmu(kobj, attr) && cpu_has(&cpu_data(cpu), X86_FEATURE_RTM) ? attr->mode : 0;\n}\n\nstatic umode_t hybrid_format_is_visible(struct kobject *kobj,\n\t\t\t\t\tstruct attribute *attr, int i)\n{\n\tstruct device *dev = kobj_to_dev(kobj);\n\tstruct x86_hybrid_pmu *pmu =\n\t\tcontainer_of(dev_get_drvdata(dev), struct x86_hybrid_pmu, pmu);\n\tstruct perf_pmu_format_hybrid_attr *pmu_attr =\n\t\tcontainer_of(attr, struct perf_pmu_format_hybrid_attr, attr.attr);\n\tint cpu = hybrid_find_supported_cpu(pmu);\n\n\treturn (cpu >= 0) && (pmu->cpu_type & pmu_attr->pmu_type) ? attr->mode : 0;\n}\n\nstatic struct attribute_group hybrid_group_events_td  = {\n\t.name\t\t= \"events\",\n\t.is_visible\t= hybrid_events_is_visible,\n};\n\nstatic struct attribute_group hybrid_group_events_mem = {\n\t.name\t\t= \"events\",\n\t.is_visible\t= hybrid_events_is_visible,\n};\n\nstatic struct attribute_group hybrid_group_events_tsx = {\n\t.name\t\t= \"events\",\n\t.is_visible\t= hybrid_tsx_is_visible,\n};\n\nstatic struct attribute_group hybrid_group_format_extra = {\n\t.name\t\t= \"format\",\n\t.is_visible\t= hybrid_format_is_visible,\n};\n\nstatic ssize_t intel_hybrid_get_attr_cpus(struct device *dev,\n\t\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t\t  char *buf)\n{\n\tstruct x86_hybrid_pmu *pmu =\n\t\tcontainer_of(dev_get_drvdata(dev), struct x86_hybrid_pmu, pmu);\n\n\treturn cpumap_print_to_pagebuf(true, buf, &pmu->supported_cpus);\n}\n\nstatic DEVICE_ATTR(cpus, S_IRUGO, intel_hybrid_get_attr_cpus, NULL);\nstatic struct attribute *intel_hybrid_cpus_attrs[] = {\n\t&dev_attr_cpus.attr,\n\tNULL,\n};\n\nstatic struct attribute_group hybrid_group_cpus = {\n\t.attrs\t\t= intel_hybrid_cpus_attrs,\n};\n\nstatic const struct attribute_group *hybrid_attr_update[] = {\n\t&hybrid_group_events_td,\n\t&hybrid_group_events_mem,\n\t&hybrid_group_events_tsx,\n\t&group_caps_gen,\n\t&group_caps_lbr,\n\t&hybrid_group_format_extra,\n\t&group_default,\n\t&hybrid_group_cpus,\n\tNULL,\n};\n\nstatic struct attribute *empty_attrs;\n\nstatic void intel_pmu_check_num_counters(int *num_counters,\n\t\t\t\t\t int *num_counters_fixed,\n\t\t\t\t\t u64 *intel_ctrl, u64 fixed_mask)\n{\n\tif (*num_counters > INTEL_PMC_MAX_GENERIC) {\n\t\tWARN(1, KERN_ERR \"hw perf events %d > max(%d), clipping!\",\n\t\t     *num_counters, INTEL_PMC_MAX_GENERIC);\n\t\t*num_counters = INTEL_PMC_MAX_GENERIC;\n\t}\n\t*intel_ctrl = (1ULL << *num_counters) - 1;\n\n\tif (*num_counters_fixed > INTEL_PMC_MAX_FIXED) {\n\t\tWARN(1, KERN_ERR \"hw perf events fixed %d > max(%d), clipping!\",\n\t\t     *num_counters_fixed, INTEL_PMC_MAX_FIXED);\n\t\t*num_counters_fixed = INTEL_PMC_MAX_FIXED;\n\t}\n\n\t*intel_ctrl |= fixed_mask << INTEL_PMC_IDX_FIXED;\n}\n\nstatic void intel_pmu_check_event_constraints(struct event_constraint *event_constraints,\n\t\t\t\t\t      int num_counters,\n\t\t\t\t\t      int num_counters_fixed,\n\t\t\t\t\t      u64 intel_ctrl)\n{\n\tstruct event_constraint *c;\n\n\tif (!event_constraints)\n\t\treturn;\n\n\t \n\tfor_each_event_constraint(c, event_constraints) {\n\t\t \n\t\tif (c->idxmsk64 & INTEL_PMC_MSK_TOPDOWN) {\n\t\t\t \n\t\t\tif (!(INTEL_PMC_MSK_FIXED_SLOTS & intel_ctrl))\n\t\t\t\tc->idxmsk64 = 0;\n\t\t\tc->weight = hweight64(c->idxmsk64);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (c->cmask == FIXED_EVENT_FLAGS) {\n\t\t\t \n\t\t\tc->idxmsk64 &= intel_ctrl;\n\n\t\t\t \n\t\t\tif (!use_fixed_pseudo_encoding(c->code))\n\t\t\t\tc->idxmsk64 |= (1ULL << num_counters) - 1;\n\t\t}\n\t\tc->idxmsk64 &=\n\t\t\t~(~0ULL << (INTEL_PMC_IDX_FIXED + num_counters_fixed));\n\t\tc->weight = hweight64(c->idxmsk64);\n\t}\n}\n\nstatic void intel_pmu_check_extra_regs(struct extra_reg *extra_regs)\n{\n\tstruct extra_reg *er;\n\n\t \n\tif (!extra_regs)\n\t\treturn;\n\n\tfor (er = extra_regs; er->msr; er++) {\n\t\ter->extra_msr_access = check_msr(er->msr, 0x11UL);\n\t\t \n\t\tif ((er->idx == EXTRA_REG_LBR) && !er->extra_msr_access)\n\t\t\tx86_pmu.lbr_sel_map = NULL;\n\t}\n}\n\nstatic void intel_pmu_check_hybrid_pmus(u64 fixed_mask)\n{\n\tstruct x86_hybrid_pmu *pmu;\n\tint i;\n\n\tfor (i = 0; i < x86_pmu.num_hybrid_pmus; i++) {\n\t\tpmu = &x86_pmu.hybrid_pmu[i];\n\n\t\tintel_pmu_check_num_counters(&pmu->num_counters,\n\t\t\t\t\t     &pmu->num_counters_fixed,\n\t\t\t\t\t     &pmu->intel_ctrl,\n\t\t\t\t\t     fixed_mask);\n\n\t\tif (pmu->intel_cap.perf_metrics) {\n\t\t\tpmu->intel_ctrl |= 1ULL << GLOBAL_CTRL_EN_PERF_METRICS;\n\t\t\tpmu->intel_ctrl |= INTEL_PMC_MSK_FIXED_SLOTS;\n\t\t}\n\n\t\tif (pmu->intel_cap.pebs_output_pt_available)\n\t\t\tpmu->pmu.capabilities |= PERF_PMU_CAP_AUX_OUTPUT;\n\n\t\tintel_pmu_check_event_constraints(pmu->event_constraints,\n\t\t\t\t\t\t  pmu->num_counters,\n\t\t\t\t\t\t  pmu->num_counters_fixed,\n\t\t\t\t\t\t  pmu->intel_ctrl);\n\n\t\tintel_pmu_check_extra_regs(pmu->extra_regs);\n\t}\n}\n\nstatic __always_inline bool is_mtl(u8 x86_model)\n{\n\treturn (x86_model == INTEL_FAM6_METEORLAKE) ||\n\t       (x86_model == INTEL_FAM6_METEORLAKE_L);\n}\n\n__init int intel_pmu_init(void)\n{\n\tstruct attribute **extra_skl_attr = &empty_attrs;\n\tstruct attribute **extra_attr = &empty_attrs;\n\tstruct attribute **td_attr    = &empty_attrs;\n\tstruct attribute **mem_attr   = &empty_attrs;\n\tstruct attribute **tsx_attr   = &empty_attrs;\n\tunion cpuid10_edx edx;\n\tunion cpuid10_eax eax;\n\tunion cpuid10_ebx ebx;\n\tunsigned int fixed_mask;\n\tbool pmem = false;\n\tint version, i;\n\tchar *name;\n\tstruct x86_hybrid_pmu *pmu;\n\n\tif (!cpu_has(&boot_cpu_data, X86_FEATURE_ARCH_PERFMON)) {\n\t\tswitch (boot_cpu_data.x86) {\n\t\tcase 0x6:\n\t\t\treturn p6_pmu_init();\n\t\tcase 0xb:\n\t\t\treturn knc_pmu_init();\n\t\tcase 0xf:\n\t\t\treturn p4_pmu_init();\n\t\t}\n\t\treturn -ENODEV;\n\t}\n\n\t \n\tcpuid(10, &eax.full, &ebx.full, &fixed_mask, &edx.full);\n\tif (eax.split.mask_length < ARCH_PERFMON_EVENTS_COUNT)\n\t\treturn -ENODEV;\n\n\tversion = eax.split.version_id;\n\tif (version < 2)\n\t\tx86_pmu = core_pmu;\n\telse\n\t\tx86_pmu = intel_pmu;\n\n\tx86_pmu.version\t\t\t= version;\n\tx86_pmu.num_counters\t\t= eax.split.num_counters;\n\tx86_pmu.cntval_bits\t\t= eax.split.bit_width;\n\tx86_pmu.cntval_mask\t\t= (1ULL << eax.split.bit_width) - 1;\n\n\tx86_pmu.events_maskl\t\t= ebx.full;\n\tx86_pmu.events_mask_len\t\t= eax.split.mask_length;\n\n\tx86_pmu.max_pebs_events\t\t= min_t(unsigned, MAX_PEBS_EVENTS, x86_pmu.num_counters);\n\tx86_pmu.pebs_capable\t\t= PEBS_COUNTER_MASK;\n\n\t \n\tif (version > 1 && version < 5) {\n\t\tint assume = 3 * !boot_cpu_has(X86_FEATURE_HYPERVISOR);\n\n\t\tx86_pmu.num_counters_fixed =\n\t\t\tmax((int)edx.split.num_counters_fixed, assume);\n\n\t\tfixed_mask = (1L << x86_pmu.num_counters_fixed) - 1;\n\t} else if (version >= 5)\n\t\tx86_pmu.num_counters_fixed = fls(fixed_mask);\n\n\tif (boot_cpu_has(X86_FEATURE_PDCM)) {\n\t\tu64 capabilities;\n\n\t\trdmsrl(MSR_IA32_PERF_CAPABILITIES, capabilities);\n\t\tx86_pmu.intel_cap.capabilities = capabilities;\n\t}\n\n\tif (x86_pmu.intel_cap.lbr_format == LBR_FORMAT_32) {\n\t\tx86_pmu.lbr_reset = intel_pmu_lbr_reset_32;\n\t\tx86_pmu.lbr_read = intel_pmu_lbr_read_32;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_ARCH_LBR))\n\t\tintel_pmu_arch_lbr_init();\n\n\tintel_ds_init();\n\n\tx86_add_quirk(intel_arch_events_quirk);  \n\n\tif (version >= 5) {\n\t\tx86_pmu.intel_cap.anythread_deprecated = edx.split.anythread_deprecated;\n\t\tif (x86_pmu.intel_cap.anythread_deprecated)\n\t\t\tpr_cont(\" AnyThread deprecated, \");\n\t}\n\n\t \n\tswitch (boot_cpu_data.x86_model) {\n\tcase INTEL_FAM6_CORE_YONAH:\n\t\tpr_cont(\"Core events, \");\n\t\tname = \"core\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_CORE2_MEROM:\n\t\tx86_add_quirk(intel_clovertown_quirk);\n\t\tfallthrough;\n\n\tcase INTEL_FAM6_CORE2_MEROM_L:\n\tcase INTEL_FAM6_CORE2_PENRYN:\n\tcase INTEL_FAM6_CORE2_DUNNINGTON:\n\t\tmemcpy(hw_cache_event_ids, core2_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\n\t\tintel_pmu_lbr_init_core();\n\n\t\tx86_pmu.event_constraints = intel_core2_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_core2_pebs_event_constraints;\n\t\tpr_cont(\"Core2 events, \");\n\t\tname = \"core2\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_NEHALEM:\n\tcase INTEL_FAM6_NEHALEM_EP:\n\tcase INTEL_FAM6_NEHALEM_EX:\n\t\tmemcpy(hw_cache_event_ids, nehalem_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, nehalem_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_nhm();\n\n\t\tx86_pmu.event_constraints = intel_nehalem_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_nehalem_pebs_event_constraints;\n\t\tx86_pmu.enable_all = intel_pmu_nhm_enable_all;\n\t\tx86_pmu.extra_regs = intel_nehalem_extra_regs;\n\t\tx86_pmu.limit_period = nhm_limit_period;\n\n\t\tmem_attr = nhm_mem_events_attrs;\n\n\t\t \n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] =\n\t\t\tX86_CONFIG(.event=0x0e, .umask=0x01, .inv=1, .cmask=1);\n\t\t \n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] =\n\t\t\tX86_CONFIG(.event=0xb1, .umask=0x3f, .inv=1, .cmask=1);\n\n\t\tintel_pmu_pebs_data_source_nhm();\n\t\tx86_add_quirk(intel_nehalem_quirk);\n\t\tx86_pmu.pebs_no_tlb = 1;\n\t\textra_attr = nhm_format_attr;\n\n\t\tpr_cont(\"Nehalem events, \");\n\t\tname = \"nehalem\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_ATOM_BONNELL:\n\tcase INTEL_FAM6_ATOM_BONNELL_MID:\n\tcase INTEL_FAM6_ATOM_SALTWELL:\n\tcase INTEL_FAM6_ATOM_SALTWELL_MID:\n\tcase INTEL_FAM6_ATOM_SALTWELL_TABLET:\n\t\tmemcpy(hw_cache_event_ids, atom_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\n\t\tintel_pmu_lbr_init_atom();\n\n\t\tx86_pmu.event_constraints = intel_gen_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_atom_pebs_event_constraints;\n\t\tx86_pmu.pebs_aliases = intel_pebs_aliases_core2;\n\t\tpr_cont(\"Atom events, \");\n\t\tname = \"bonnell\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_ATOM_SILVERMONT:\n\tcase INTEL_FAM6_ATOM_SILVERMONT_D:\n\tcase INTEL_FAM6_ATOM_SILVERMONT_MID:\n\tcase INTEL_FAM6_ATOM_AIRMONT:\n\tcase INTEL_FAM6_ATOM_AIRMONT_MID:\n\t\tmemcpy(hw_cache_event_ids, slm_hw_cache_event_ids,\n\t\t\tsizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, slm_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_slm();\n\n\t\tx86_pmu.event_constraints = intel_slm_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_slm_pebs_event_constraints;\n\t\tx86_pmu.extra_regs = intel_slm_extra_regs;\n\t\tx86_pmu.flags |= PMU_FL_HAS_RSP_1;\n\t\ttd_attr = slm_events_attrs;\n\t\textra_attr = slm_format_attr;\n\t\tpr_cont(\"Silvermont events, \");\n\t\tname = \"silvermont\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_ATOM_GOLDMONT:\n\tcase INTEL_FAM6_ATOM_GOLDMONT_D:\n\t\tmemcpy(hw_cache_event_ids, glm_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, glm_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_skl();\n\n\t\tx86_pmu.event_constraints = intel_slm_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_glm_pebs_event_constraints;\n\t\tx86_pmu.extra_regs = intel_glm_extra_regs;\n\t\t \n\t\tx86_pmu.pebs_aliases = NULL;\n\t\tx86_pmu.pebs_prec_dist = true;\n\t\tx86_pmu.lbr_pt_coexist = true;\n\t\tx86_pmu.flags |= PMU_FL_HAS_RSP_1;\n\t\ttd_attr = glm_events_attrs;\n\t\textra_attr = slm_format_attr;\n\t\tpr_cont(\"Goldmont events, \");\n\t\tname = \"goldmont\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_ATOM_GOLDMONT_PLUS:\n\t\tmemcpy(hw_cache_event_ids, glp_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, glp_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_skl();\n\n\t\tx86_pmu.event_constraints = intel_slm_event_constraints;\n\t\tx86_pmu.extra_regs = intel_glm_extra_regs;\n\t\t \n\t\tx86_pmu.pebs_aliases = NULL;\n\t\tx86_pmu.pebs_prec_dist = true;\n\t\tx86_pmu.lbr_pt_coexist = true;\n\t\tx86_pmu.pebs_capable = ~0ULL;\n\t\tx86_pmu.flags |= PMU_FL_HAS_RSP_1;\n\t\tx86_pmu.flags |= PMU_FL_PEBS_ALL;\n\t\tx86_pmu.get_event_constraints = glp_get_event_constraints;\n\t\ttd_attr = glm_events_attrs;\n\t\t \n\t\tevent_attr_td_total_slots_scale_glm.event_str = \"4\";\n\t\textra_attr = slm_format_attr;\n\t\tpr_cont(\"Goldmont plus events, \");\n\t\tname = \"goldmont_plus\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_ATOM_TREMONT_D:\n\tcase INTEL_FAM6_ATOM_TREMONT:\n\tcase INTEL_FAM6_ATOM_TREMONT_L:\n\t\tx86_pmu.late_ack = true;\n\t\tmemcpy(hw_cache_event_ids, glp_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, tnt_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\t\thw_cache_event_ids[C(ITLB)][C(OP_READ)][C(RESULT_ACCESS)] = -1;\n\n\t\tintel_pmu_lbr_init_skl();\n\n\t\tx86_pmu.event_constraints = intel_slm_event_constraints;\n\t\tx86_pmu.extra_regs = intel_tnt_extra_regs;\n\t\t \n\t\tx86_pmu.pebs_aliases = NULL;\n\t\tx86_pmu.pebs_prec_dist = true;\n\t\tx86_pmu.lbr_pt_coexist = true;\n\t\tx86_pmu.flags |= PMU_FL_HAS_RSP_1;\n\t\tx86_pmu.get_event_constraints = tnt_get_event_constraints;\n\t\ttd_attr = tnt_events_attrs;\n\t\textra_attr = slm_format_attr;\n\t\tpr_cont(\"Tremont events, \");\n\t\tname = \"Tremont\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_ATOM_GRACEMONT:\n\t\tx86_pmu.mid_ack = true;\n\t\tmemcpy(hw_cache_event_ids, glp_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, tnt_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\t\thw_cache_event_ids[C(ITLB)][C(OP_READ)][C(RESULT_ACCESS)] = -1;\n\n\t\tx86_pmu.event_constraints = intel_slm_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_grt_pebs_event_constraints;\n\t\tx86_pmu.extra_regs = intel_grt_extra_regs;\n\n\t\tx86_pmu.pebs_aliases = NULL;\n\t\tx86_pmu.pebs_prec_dist = true;\n\t\tx86_pmu.pebs_block = true;\n\t\tx86_pmu.lbr_pt_coexist = true;\n\t\tx86_pmu.flags |= PMU_FL_HAS_RSP_1;\n\t\tx86_pmu.flags |= PMU_FL_INSTR_LATENCY;\n\n\t\tintel_pmu_pebs_data_source_grt();\n\t\tx86_pmu.pebs_latency_data = adl_latency_data_small;\n\t\tx86_pmu.get_event_constraints = tnt_get_event_constraints;\n\t\tx86_pmu.limit_period = spr_limit_period;\n\t\ttd_attr = tnt_events_attrs;\n\t\tmem_attr = grt_mem_attrs;\n\t\textra_attr = nhm_format_attr;\n\t\tpr_cont(\"Gracemont events, \");\n\t\tname = \"gracemont\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_ATOM_CRESTMONT:\n\tcase INTEL_FAM6_ATOM_CRESTMONT_X:\n\t\tx86_pmu.mid_ack = true;\n\t\tmemcpy(hw_cache_event_ids, glp_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, tnt_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\t\thw_cache_event_ids[C(ITLB)][C(OP_READ)][C(RESULT_ACCESS)] = -1;\n\n\t\tx86_pmu.event_constraints = intel_slm_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_grt_pebs_event_constraints;\n\t\tx86_pmu.extra_regs = intel_cmt_extra_regs;\n\n\t\tx86_pmu.pebs_aliases = NULL;\n\t\tx86_pmu.pebs_prec_dist = true;\n\t\tx86_pmu.lbr_pt_coexist = true;\n\t\tx86_pmu.pebs_block = true;\n\t\tx86_pmu.flags |= PMU_FL_HAS_RSP_1;\n\t\tx86_pmu.flags |= PMU_FL_INSTR_LATENCY;\n\n\t\tintel_pmu_pebs_data_source_cmt();\n\t\tx86_pmu.pebs_latency_data = mtl_latency_data_small;\n\t\tx86_pmu.get_event_constraints = cmt_get_event_constraints;\n\t\tx86_pmu.limit_period = spr_limit_period;\n\t\ttd_attr = cmt_events_attrs;\n\t\tmem_attr = grt_mem_attrs;\n\t\textra_attr = cmt_format_attr;\n\t\tpr_cont(\"Crestmont events, \");\n\t\tname = \"crestmont\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_WESTMERE:\n\tcase INTEL_FAM6_WESTMERE_EP:\n\tcase INTEL_FAM6_WESTMERE_EX:\n\t\tmemcpy(hw_cache_event_ids, westmere_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, nehalem_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_nhm();\n\n\t\tx86_pmu.event_constraints = intel_westmere_event_constraints;\n\t\tx86_pmu.enable_all = intel_pmu_nhm_enable_all;\n\t\tx86_pmu.pebs_constraints = intel_westmere_pebs_event_constraints;\n\t\tx86_pmu.extra_regs = intel_westmere_extra_regs;\n\t\tx86_pmu.flags |= PMU_FL_HAS_RSP_1;\n\n\t\tmem_attr = nhm_mem_events_attrs;\n\n\t\t \n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] =\n\t\t\tX86_CONFIG(.event=0x0e, .umask=0x01, .inv=1, .cmask=1);\n\t\t \n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] =\n\t\t\tX86_CONFIG(.event=0xb1, .umask=0x3f, .inv=1, .cmask=1);\n\n\t\tintel_pmu_pebs_data_source_nhm();\n\t\textra_attr = nhm_format_attr;\n\t\tpr_cont(\"Westmere events, \");\n\t\tname = \"westmere\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_SANDYBRIDGE:\n\tcase INTEL_FAM6_SANDYBRIDGE_X:\n\t\tx86_add_quirk(intel_sandybridge_quirk);\n\t\tx86_add_quirk(intel_ht_bug);\n\t\tmemcpy(hw_cache_event_ids, snb_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, snb_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_snb();\n\n\t\tx86_pmu.event_constraints = intel_snb_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_snb_pebs_event_constraints;\n\t\tx86_pmu.pebs_aliases = intel_pebs_aliases_snb;\n\t\tif (boot_cpu_data.x86_model == INTEL_FAM6_SANDYBRIDGE_X)\n\t\t\tx86_pmu.extra_regs = intel_snbep_extra_regs;\n\t\telse\n\t\t\tx86_pmu.extra_regs = intel_snb_extra_regs;\n\n\n\t\t \n\t\tx86_pmu.flags |= PMU_FL_HAS_RSP_1;\n\t\tx86_pmu.flags |= PMU_FL_NO_HT_SHARING;\n\n\t\ttd_attr  = snb_events_attrs;\n\t\tmem_attr = snb_mem_events_attrs;\n\n\t\t \n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] =\n\t\t\tX86_CONFIG(.event=0x0e, .umask=0x01, .inv=1, .cmask=1);\n\t\t \n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_BACKEND] =\n\t\t\tX86_CONFIG(.event=0xb1, .umask=0x01, .inv=1, .cmask=1);\n\n\t\textra_attr = nhm_format_attr;\n\n\t\tpr_cont(\"SandyBridge events, \");\n\t\tname = \"sandybridge\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_IVYBRIDGE:\n\tcase INTEL_FAM6_IVYBRIDGE_X:\n\t\tx86_add_quirk(intel_ht_bug);\n\t\tmemcpy(hw_cache_event_ids, snb_hw_cache_event_ids,\n\t\t       sizeof(hw_cache_event_ids));\n\t\t \n\t\thw_cache_event_ids[C(DTLB)][C(OP_READ)][C(RESULT_MISS)] = 0x8108;  \n\n\t\tmemcpy(hw_cache_extra_regs, snb_hw_cache_extra_regs,\n\t\t       sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_snb();\n\n\t\tx86_pmu.event_constraints = intel_ivb_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_ivb_pebs_event_constraints;\n\t\tx86_pmu.pebs_aliases = intel_pebs_aliases_ivb;\n\t\tx86_pmu.pebs_prec_dist = true;\n\t\tif (boot_cpu_data.x86_model == INTEL_FAM6_IVYBRIDGE_X)\n\t\t\tx86_pmu.extra_regs = intel_snbep_extra_regs;\n\t\telse\n\t\t\tx86_pmu.extra_regs = intel_snb_extra_regs;\n\t\t \n\t\tx86_pmu.flags |= PMU_FL_HAS_RSP_1;\n\t\tx86_pmu.flags |= PMU_FL_NO_HT_SHARING;\n\n\t\ttd_attr  = snb_events_attrs;\n\t\tmem_attr = snb_mem_events_attrs;\n\n\t\t \n\t\tintel_perfmon_event_map[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND] =\n\t\t\tX86_CONFIG(.event=0x0e, .umask=0x01, .inv=1, .cmask=1);\n\n\t\textra_attr = nhm_format_attr;\n\n\t\tpr_cont(\"IvyBridge events, \");\n\t\tname = \"ivybridge\";\n\t\tbreak;\n\n\n\tcase INTEL_FAM6_HASWELL:\n\tcase INTEL_FAM6_HASWELL_X:\n\tcase INTEL_FAM6_HASWELL_L:\n\tcase INTEL_FAM6_HASWELL_G:\n\t\tx86_add_quirk(intel_ht_bug);\n\t\tx86_add_quirk(intel_pebs_isolation_quirk);\n\t\tx86_pmu.late_ack = true;\n\t\tmemcpy(hw_cache_event_ids, hsw_hw_cache_event_ids, sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, hsw_hw_cache_extra_regs, sizeof(hw_cache_extra_regs));\n\n\t\tintel_pmu_lbr_init_hsw();\n\n\t\tx86_pmu.event_constraints = intel_hsw_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_hsw_pebs_event_constraints;\n\t\tx86_pmu.extra_regs = intel_snbep_extra_regs;\n\t\tx86_pmu.pebs_aliases = intel_pebs_aliases_ivb;\n\t\tx86_pmu.pebs_prec_dist = true;\n\t\t \n\t\tx86_pmu.flags |= PMU_FL_HAS_RSP_1;\n\t\tx86_pmu.flags |= PMU_FL_NO_HT_SHARING;\n\n\t\tx86_pmu.hw_config = hsw_hw_config;\n\t\tx86_pmu.get_event_constraints = hsw_get_event_constraints;\n\t\tx86_pmu.lbr_double_abort = true;\n\t\textra_attr = boot_cpu_has(X86_FEATURE_RTM) ?\n\t\t\thsw_format_attr : nhm_format_attr;\n\t\ttd_attr  = hsw_events_attrs;\n\t\tmem_attr = hsw_mem_events_attrs;\n\t\ttsx_attr = hsw_tsx_events_attrs;\n\t\tpr_cont(\"Haswell events, \");\n\t\tname = \"haswell\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_BROADWELL:\n\tcase INTEL_FAM6_BROADWELL_D:\n\tcase INTEL_FAM6_BROADWELL_G:\n\tcase INTEL_FAM6_BROADWELL_X:\n\t\tx86_add_quirk(intel_pebs_isolation_quirk);\n\t\tx86_pmu.late_ack = true;\n\t\tmemcpy(hw_cache_event_ids, hsw_hw_cache_event_ids, sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, hsw_hw_cache_extra_regs, sizeof(hw_cache_extra_regs));\n\n\t\t \n\t\thw_cache_extra_regs[C(LL)][C(OP_READ)][C(RESULT_MISS)] = HSW_DEMAND_READ |\n\t\t\t\t\t\t\t\t\t BDW_L3_MISS|HSW_SNOOP_DRAM;\n\t\thw_cache_extra_regs[C(LL)][C(OP_WRITE)][C(RESULT_MISS)] = HSW_DEMAND_WRITE|BDW_L3_MISS|\n\t\t\t\t\t\t\t\t\t  HSW_SNOOP_DRAM;\n\t\thw_cache_extra_regs[C(NODE)][C(OP_READ)][C(RESULT_ACCESS)] = HSW_DEMAND_READ|\n\t\t\t\t\t\t\t\t\t     BDW_L3_MISS_LOCAL|HSW_SNOOP_DRAM;\n\t\thw_cache_extra_regs[C(NODE)][C(OP_WRITE)][C(RESULT_ACCESS)] = HSW_DEMAND_WRITE|\n\t\t\t\t\t\t\t\t\t      BDW_L3_MISS_LOCAL|HSW_SNOOP_DRAM;\n\n\t\tintel_pmu_lbr_init_hsw();\n\n\t\tx86_pmu.event_constraints = intel_bdw_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_bdw_pebs_event_constraints;\n\t\tx86_pmu.extra_regs = intel_snbep_extra_regs;\n\t\tx86_pmu.pebs_aliases = intel_pebs_aliases_ivb;\n\t\tx86_pmu.pebs_prec_dist = true;\n\t\t \n\t\tx86_pmu.flags |= PMU_FL_HAS_RSP_1;\n\t\tx86_pmu.flags |= PMU_FL_NO_HT_SHARING;\n\n\t\tx86_pmu.hw_config = hsw_hw_config;\n\t\tx86_pmu.get_event_constraints = hsw_get_event_constraints;\n\t\tx86_pmu.limit_period = bdw_limit_period;\n\t\textra_attr = boot_cpu_has(X86_FEATURE_RTM) ?\n\t\t\thsw_format_attr : nhm_format_attr;\n\t\ttd_attr  = hsw_events_attrs;\n\t\tmem_attr = hsw_mem_events_attrs;\n\t\ttsx_attr = hsw_tsx_events_attrs;\n\t\tpr_cont(\"Broadwell events, \");\n\t\tname = \"broadwell\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_XEON_PHI_KNL:\n\tcase INTEL_FAM6_XEON_PHI_KNM:\n\t\tmemcpy(hw_cache_event_ids,\n\t\t       slm_hw_cache_event_ids, sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs,\n\t\t       knl_hw_cache_extra_regs, sizeof(hw_cache_extra_regs));\n\t\tintel_pmu_lbr_init_knl();\n\n\t\tx86_pmu.event_constraints = intel_slm_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_slm_pebs_event_constraints;\n\t\tx86_pmu.extra_regs = intel_knl_extra_regs;\n\n\t\t \n\t\tx86_pmu.flags |= PMU_FL_HAS_RSP_1;\n\t\tx86_pmu.flags |= PMU_FL_NO_HT_SHARING;\n\t\textra_attr = slm_format_attr;\n\t\tpr_cont(\"Knights Landing/Mill events, \");\n\t\tname = \"knights-landing\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_SKYLAKE_X:\n\t\tpmem = true;\n\t\tfallthrough;\n\tcase INTEL_FAM6_SKYLAKE_L:\n\tcase INTEL_FAM6_SKYLAKE:\n\tcase INTEL_FAM6_KABYLAKE_L:\n\tcase INTEL_FAM6_KABYLAKE:\n\tcase INTEL_FAM6_COMETLAKE_L:\n\tcase INTEL_FAM6_COMETLAKE:\n\t\tx86_add_quirk(intel_pebs_isolation_quirk);\n\t\tx86_pmu.late_ack = true;\n\t\tmemcpy(hw_cache_event_ids, skl_hw_cache_event_ids, sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, skl_hw_cache_extra_regs, sizeof(hw_cache_extra_regs));\n\t\tintel_pmu_lbr_init_skl();\n\n\t\t \n\t\tevent_attr_td_recovery_bubbles.event_str_noht =\n\t\t\t\"event=0xd,umask=0x1,cmask=1\";\n\t\tevent_attr_td_recovery_bubbles.event_str_ht =\n\t\t\t\"event=0xd,umask=0x1,cmask=1,any=1\";\n\n\t\tx86_pmu.event_constraints = intel_skl_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_skl_pebs_event_constraints;\n\t\tx86_pmu.extra_regs = intel_skl_extra_regs;\n\t\tx86_pmu.pebs_aliases = intel_pebs_aliases_skl;\n\t\tx86_pmu.pebs_prec_dist = true;\n\t\t \n\t\tx86_pmu.flags |= PMU_FL_HAS_RSP_1;\n\t\tx86_pmu.flags |= PMU_FL_NO_HT_SHARING;\n\n\t\tx86_pmu.hw_config = hsw_hw_config;\n\t\tx86_pmu.get_event_constraints = hsw_get_event_constraints;\n\t\textra_attr = boot_cpu_has(X86_FEATURE_RTM) ?\n\t\t\thsw_format_attr : nhm_format_attr;\n\t\textra_skl_attr = skl_format_attr;\n\t\ttd_attr  = hsw_events_attrs;\n\t\tmem_attr = hsw_mem_events_attrs;\n\t\ttsx_attr = hsw_tsx_events_attrs;\n\t\tintel_pmu_pebs_data_source_skl(pmem);\n\n\t\t \n\t\tif (boot_cpu_has(X86_FEATURE_TSX_FORCE_ABORT) &&\n\t\t   !boot_cpu_has(X86_FEATURE_RTM_ALWAYS_ABORT)) {\n\t\t\tx86_pmu.flags |= PMU_FL_TFA;\n\t\t\tx86_pmu.get_event_constraints = tfa_get_event_constraints;\n\t\t\tx86_pmu.enable_all = intel_tfa_pmu_enable_all;\n\t\t\tx86_pmu.commit_scheduling = intel_tfa_commit_scheduling;\n\t\t}\n\n\t\tpr_cont(\"Skylake events, \");\n\t\tname = \"skylake\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_ICELAKE_X:\n\tcase INTEL_FAM6_ICELAKE_D:\n\t\tx86_pmu.pebs_ept = 1;\n\t\tpmem = true;\n\t\tfallthrough;\n\tcase INTEL_FAM6_ICELAKE_L:\n\tcase INTEL_FAM6_ICELAKE:\n\tcase INTEL_FAM6_TIGERLAKE_L:\n\tcase INTEL_FAM6_TIGERLAKE:\n\tcase INTEL_FAM6_ROCKETLAKE:\n\t\tx86_pmu.late_ack = true;\n\t\tmemcpy(hw_cache_event_ids, skl_hw_cache_event_ids, sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, skl_hw_cache_extra_regs, sizeof(hw_cache_extra_regs));\n\t\thw_cache_event_ids[C(ITLB)][C(OP_READ)][C(RESULT_ACCESS)] = -1;\n\t\tintel_pmu_lbr_init_skl();\n\n\t\tx86_pmu.event_constraints = intel_icl_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_icl_pebs_event_constraints;\n\t\tx86_pmu.extra_regs = intel_icl_extra_regs;\n\t\tx86_pmu.pebs_aliases = NULL;\n\t\tx86_pmu.pebs_prec_dist = true;\n\t\tx86_pmu.flags |= PMU_FL_HAS_RSP_1;\n\t\tx86_pmu.flags |= PMU_FL_NO_HT_SHARING;\n\n\t\tx86_pmu.hw_config = hsw_hw_config;\n\t\tx86_pmu.get_event_constraints = icl_get_event_constraints;\n\t\textra_attr = boot_cpu_has(X86_FEATURE_RTM) ?\n\t\t\thsw_format_attr : nhm_format_attr;\n\t\textra_skl_attr = skl_format_attr;\n\t\tmem_attr = icl_events_attrs;\n\t\ttd_attr = icl_td_events_attrs;\n\t\ttsx_attr = icl_tsx_events_attrs;\n\t\tx86_pmu.rtm_abort_event = X86_CONFIG(.event=0xc9, .umask=0x04);\n\t\tx86_pmu.lbr_pt_coexist = true;\n\t\tintel_pmu_pebs_data_source_skl(pmem);\n\t\tx86_pmu.num_topdown_events = 4;\n\t\tstatic_call_update(intel_pmu_update_topdown_event,\n\t\t\t\t   &icl_update_topdown_event);\n\t\tstatic_call_update(intel_pmu_set_topdown_event_period,\n\t\t\t\t   &icl_set_topdown_event_period);\n\t\tpr_cont(\"Icelake events, \");\n\t\tname = \"icelake\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_SAPPHIRERAPIDS_X:\n\tcase INTEL_FAM6_EMERALDRAPIDS_X:\n\t\tx86_pmu.flags |= PMU_FL_MEM_LOADS_AUX;\n\t\tx86_pmu.extra_regs = intel_spr_extra_regs;\n\t\tfallthrough;\n\tcase INTEL_FAM6_GRANITERAPIDS_X:\n\tcase INTEL_FAM6_GRANITERAPIDS_D:\n\t\tpmem = true;\n\t\tx86_pmu.late_ack = true;\n\t\tmemcpy(hw_cache_event_ids, spr_hw_cache_event_ids, sizeof(hw_cache_event_ids));\n\t\tmemcpy(hw_cache_extra_regs, spr_hw_cache_extra_regs, sizeof(hw_cache_extra_regs));\n\n\t\tx86_pmu.event_constraints = intel_spr_event_constraints;\n\t\tx86_pmu.pebs_constraints = intel_spr_pebs_event_constraints;\n\t\tif (!x86_pmu.extra_regs)\n\t\t\tx86_pmu.extra_regs = intel_gnr_extra_regs;\n\t\tx86_pmu.limit_period = spr_limit_period;\n\t\tx86_pmu.pebs_ept = 1;\n\t\tx86_pmu.pebs_aliases = NULL;\n\t\tx86_pmu.pebs_prec_dist = true;\n\t\tx86_pmu.pebs_block = true;\n\t\tx86_pmu.flags |= PMU_FL_HAS_RSP_1;\n\t\tx86_pmu.flags |= PMU_FL_NO_HT_SHARING;\n\t\tx86_pmu.flags |= PMU_FL_INSTR_LATENCY;\n\n\t\tx86_pmu.hw_config = hsw_hw_config;\n\t\tx86_pmu.get_event_constraints = spr_get_event_constraints;\n\t\textra_attr = boot_cpu_has(X86_FEATURE_RTM) ?\n\t\t\thsw_format_attr : nhm_format_attr;\n\t\textra_skl_attr = skl_format_attr;\n\t\tmem_attr = spr_events_attrs;\n\t\ttd_attr = spr_td_events_attrs;\n\t\ttsx_attr = spr_tsx_events_attrs;\n\t\tx86_pmu.rtm_abort_event = X86_CONFIG(.event=0xc9, .umask=0x04);\n\t\tx86_pmu.lbr_pt_coexist = true;\n\t\tintel_pmu_pebs_data_source_skl(pmem);\n\t\tx86_pmu.num_topdown_events = 8;\n\t\tstatic_call_update(intel_pmu_update_topdown_event,\n\t\t\t\t   &icl_update_topdown_event);\n\t\tstatic_call_update(intel_pmu_set_topdown_event_period,\n\t\t\t\t   &icl_set_topdown_event_period);\n\t\tpr_cont(\"Sapphire Rapids events, \");\n\t\tname = \"sapphire_rapids\";\n\t\tbreak;\n\n\tcase INTEL_FAM6_ALDERLAKE:\n\tcase INTEL_FAM6_ALDERLAKE_L:\n\tcase INTEL_FAM6_RAPTORLAKE:\n\tcase INTEL_FAM6_RAPTORLAKE_P:\n\tcase INTEL_FAM6_RAPTORLAKE_S:\n\tcase INTEL_FAM6_METEORLAKE:\n\tcase INTEL_FAM6_METEORLAKE_L:\n\t\t \n\t\tx86_pmu.hybrid_pmu = kcalloc(X86_HYBRID_NUM_PMUS,\n\t\t\t\t\t     sizeof(struct x86_hybrid_pmu),\n\t\t\t\t\t     GFP_KERNEL);\n\t\tif (!x86_pmu.hybrid_pmu)\n\t\t\treturn -ENOMEM;\n\t\tstatic_branch_enable(&perf_is_hybrid);\n\t\tx86_pmu.num_hybrid_pmus = X86_HYBRID_NUM_PMUS;\n\n\t\tx86_pmu.pebs_aliases = NULL;\n\t\tx86_pmu.pebs_prec_dist = true;\n\t\tx86_pmu.pebs_block = true;\n\t\tx86_pmu.flags |= PMU_FL_HAS_RSP_1;\n\t\tx86_pmu.flags |= PMU_FL_NO_HT_SHARING;\n\t\tx86_pmu.flags |= PMU_FL_INSTR_LATENCY;\n\t\tx86_pmu.lbr_pt_coexist = true;\n\t\tx86_pmu.pebs_latency_data = adl_latency_data_small;\n\t\tx86_pmu.num_topdown_events = 8;\n\t\tstatic_call_update(intel_pmu_update_topdown_event,\n\t\t\t\t   &adl_update_topdown_event);\n\t\tstatic_call_update(intel_pmu_set_topdown_event_period,\n\t\t\t\t   &adl_set_topdown_event_period);\n\n\t\tx86_pmu.filter = intel_pmu_filter;\n\t\tx86_pmu.get_event_constraints = adl_get_event_constraints;\n\t\tx86_pmu.hw_config = adl_hw_config;\n\t\tx86_pmu.limit_period = spr_limit_period;\n\t\tx86_pmu.get_hybrid_cpu_type = adl_get_hybrid_cpu_type;\n\t\t \n\t\tx86_pmu.rtm_abort_event = X86_CONFIG(.event=0xc9, .umask=0x04);\n\n\t\ttd_attr = adl_hybrid_events_attrs;\n\t\tmem_attr = adl_hybrid_mem_attrs;\n\t\ttsx_attr = adl_hybrid_tsx_attrs;\n\t\textra_attr = boot_cpu_has(X86_FEATURE_RTM) ?\n\t\t\tadl_hybrid_extra_attr_rtm : adl_hybrid_extra_attr;\n\n\t\t \n\t\tpmu = &x86_pmu.hybrid_pmu[X86_HYBRID_PMU_CORE_IDX];\n\t\tpmu->name = \"cpu_core\";\n\t\tpmu->cpu_type = hybrid_big;\n\t\tpmu->late_ack = true;\n\t\tif (cpu_feature_enabled(X86_FEATURE_HYBRID_CPU)) {\n\t\t\tpmu->num_counters = x86_pmu.num_counters + 2;\n\t\t\tpmu->num_counters_fixed = x86_pmu.num_counters_fixed + 1;\n\t\t} else {\n\t\t\tpmu->num_counters = x86_pmu.num_counters;\n\t\t\tpmu->num_counters_fixed = x86_pmu.num_counters_fixed;\n\t\t}\n\n\t\t \n\t\tif ((pmu->num_counters > 8) || (pmu->num_counters_fixed > 4)) {\n\t\t\tpmu->num_counters = x86_pmu.num_counters;\n\t\t\tpmu->num_counters_fixed = x86_pmu.num_counters_fixed;\n\t\t}\n\n\t\tpmu->max_pebs_events = min_t(unsigned, MAX_PEBS_EVENTS, pmu->num_counters);\n\t\tpmu->unconstrained = (struct event_constraint)\n\t\t\t\t\t__EVENT_CONSTRAINT(0, (1ULL << pmu->num_counters) - 1,\n\t\t\t\t\t\t\t   0, pmu->num_counters, 0, 0);\n\t\tpmu->intel_cap.capabilities = x86_pmu.intel_cap.capabilities;\n\t\tpmu->intel_cap.perf_metrics = 1;\n\t\tpmu->intel_cap.pebs_output_pt_available = 0;\n\n\t\tmemcpy(pmu->hw_cache_event_ids, spr_hw_cache_event_ids, sizeof(pmu->hw_cache_event_ids));\n\t\tmemcpy(pmu->hw_cache_extra_regs, spr_hw_cache_extra_regs, sizeof(pmu->hw_cache_extra_regs));\n\t\tpmu->event_constraints = intel_spr_event_constraints;\n\t\tpmu->pebs_constraints = intel_spr_pebs_event_constraints;\n\t\tpmu->extra_regs = intel_spr_extra_regs;\n\n\t\t \n\t\tpmu = &x86_pmu.hybrid_pmu[X86_HYBRID_PMU_ATOM_IDX];\n\t\tpmu->name = \"cpu_atom\";\n\t\tpmu->cpu_type = hybrid_small;\n\t\tpmu->mid_ack = true;\n\t\tpmu->num_counters = x86_pmu.num_counters;\n\t\tpmu->num_counters_fixed = x86_pmu.num_counters_fixed;\n\t\tpmu->max_pebs_events = x86_pmu.max_pebs_events;\n\t\tpmu->unconstrained = (struct event_constraint)\n\t\t\t\t\t__EVENT_CONSTRAINT(0, (1ULL << pmu->num_counters) - 1,\n\t\t\t\t\t\t\t   0, pmu->num_counters, 0, 0);\n\t\tpmu->intel_cap.capabilities = x86_pmu.intel_cap.capabilities;\n\t\tpmu->intel_cap.perf_metrics = 0;\n\t\tpmu->intel_cap.pebs_output_pt_available = 1;\n\n\t\tmemcpy(pmu->hw_cache_event_ids, glp_hw_cache_event_ids, sizeof(pmu->hw_cache_event_ids));\n\t\tmemcpy(pmu->hw_cache_extra_regs, tnt_hw_cache_extra_regs, sizeof(pmu->hw_cache_extra_regs));\n\t\tpmu->hw_cache_event_ids[C(ITLB)][C(OP_READ)][C(RESULT_ACCESS)] = -1;\n\t\tpmu->event_constraints = intel_slm_event_constraints;\n\t\tpmu->pebs_constraints = intel_grt_pebs_event_constraints;\n\t\tpmu->extra_regs = intel_grt_extra_regs;\n\t\tif (is_mtl(boot_cpu_data.x86_model)) {\n\t\t\tx86_pmu.hybrid_pmu[X86_HYBRID_PMU_CORE_IDX].extra_regs = intel_gnr_extra_regs;\n\t\t\tx86_pmu.pebs_latency_data = mtl_latency_data_small;\n\t\t\textra_attr = boot_cpu_has(X86_FEATURE_RTM) ?\n\t\t\t\tmtl_hybrid_extra_attr_rtm : mtl_hybrid_extra_attr;\n\t\t\tmem_attr = mtl_hybrid_mem_attrs;\n\t\t\tintel_pmu_pebs_data_source_mtl();\n\t\t\tx86_pmu.get_event_constraints = mtl_get_event_constraints;\n\t\t\tpmu->extra_regs = intel_cmt_extra_regs;\n\t\t\tpr_cont(\"Meteorlake Hybrid events, \");\n\t\t\tname = \"meteorlake_hybrid\";\n\t\t} else {\n\t\t\tx86_pmu.flags |= PMU_FL_MEM_LOADS_AUX;\n\t\t\tintel_pmu_pebs_data_source_adl();\n\t\t\tpr_cont(\"Alderlake Hybrid events, \");\n\t\t\tname = \"alderlake_hybrid\";\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tswitch (x86_pmu.version) {\n\t\tcase 1:\n\t\t\tx86_pmu.event_constraints = intel_v1_event_constraints;\n\t\t\tpr_cont(\"generic architected perfmon v1, \");\n\t\t\tname = \"generic_arch_v1\";\n\t\t\tbreak;\n\t\tcase 2:\n\t\tcase 3:\n\t\tcase 4:\n\t\t\t \n\t\t\tx86_pmu.event_constraints = intel_gen_event_constraints;\n\t\t\tpr_cont(\"generic architected perfmon, \");\n\t\t\tname = \"generic_arch_v2+\";\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\tif (x86_pmu.num_counters_fixed > INTEL_PMC_MAX_FIXED)\n\t\t\t\tx86_pmu.num_counters_fixed = INTEL_PMC_MAX_FIXED;\n\t\t\tintel_v5_gen_event_constraints[x86_pmu.num_counters_fixed].weight = -1;\n\t\t\tx86_pmu.event_constraints = intel_v5_gen_event_constraints;\n\t\t\tpr_cont(\"generic architected perfmon, \");\n\t\t\tname = \"generic_arch_v5+\";\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tsnprintf(pmu_name_str, sizeof(pmu_name_str), \"%s\", name);\n\n\tif (!is_hybrid()) {\n\t\tgroup_events_td.attrs  = td_attr;\n\t\tgroup_events_mem.attrs = mem_attr;\n\t\tgroup_events_tsx.attrs = tsx_attr;\n\t\tgroup_format_extra.attrs = extra_attr;\n\t\tgroup_format_extra_skl.attrs = extra_skl_attr;\n\n\t\tx86_pmu.attr_update = attr_update;\n\t} else {\n\t\thybrid_group_events_td.attrs  = td_attr;\n\t\thybrid_group_events_mem.attrs = mem_attr;\n\t\thybrid_group_events_tsx.attrs = tsx_attr;\n\t\thybrid_group_format_extra.attrs = extra_attr;\n\n\t\tx86_pmu.attr_update = hybrid_attr_update;\n\t}\n\n\tintel_pmu_check_num_counters(&x86_pmu.num_counters,\n\t\t\t\t     &x86_pmu.num_counters_fixed,\n\t\t\t\t     &x86_pmu.intel_ctrl,\n\t\t\t\t     (u64)fixed_mask);\n\n\t \n\tif (x86_pmu.intel_cap.anythread_deprecated)\n\t\tx86_pmu.format_attrs = intel_arch_formats_attr;\n\n\tintel_pmu_check_event_constraints(x86_pmu.event_constraints,\n\t\t\t\t\t  x86_pmu.num_counters,\n\t\t\t\t\t  x86_pmu.num_counters_fixed,\n\t\t\t\t\t  x86_pmu.intel_ctrl);\n\t \n\tif (x86_pmu.lbr_tos && !check_msr(x86_pmu.lbr_tos, 0x3UL))\n\t\tx86_pmu.lbr_nr = 0;\n\tfor (i = 0; i < x86_pmu.lbr_nr; i++) {\n\t\tif (!(check_msr(x86_pmu.lbr_from + i, 0xffffUL) &&\n\t\t      check_msr(x86_pmu.lbr_to + i, 0xffffUL)))\n\t\t\tx86_pmu.lbr_nr = 0;\n\t}\n\n\tif (x86_pmu.lbr_nr) {\n\t\tintel_pmu_lbr_init();\n\n\t\tpr_cont(\"%d-deep LBR, \", x86_pmu.lbr_nr);\n\n\t\t \n\t\tif (x86_pmu.disable_all == intel_pmu_disable_all) {\n\t\t\tif (boot_cpu_has(X86_FEATURE_ARCH_LBR)) {\n\t\t\t\tstatic_call_update(perf_snapshot_branch_stack,\n\t\t\t\t\t\t   intel_pmu_snapshot_arch_branch_stack);\n\t\t\t} else {\n\t\t\t\tstatic_call_update(perf_snapshot_branch_stack,\n\t\t\t\t\t\t   intel_pmu_snapshot_branch_stack);\n\t\t\t}\n\t\t}\n\t}\n\n\tintel_pmu_check_extra_regs(x86_pmu.extra_regs);\n\n\t \n\tif (x86_pmu.intel_cap.full_width_write) {\n\t\tx86_pmu.max_period = x86_pmu.cntval_mask >> 1;\n\t\tx86_pmu.perfctr = MSR_IA32_PMC0;\n\t\tpr_cont(\"full-width counters, \");\n\t}\n\n\tif (!is_hybrid() && x86_pmu.intel_cap.perf_metrics)\n\t\tx86_pmu.intel_ctrl |= 1ULL << GLOBAL_CTRL_EN_PERF_METRICS;\n\n\tif (is_hybrid())\n\t\tintel_pmu_check_hybrid_pmus((u64)fixed_mask);\n\n\tif (x86_pmu.intel_cap.pebs_timing_info)\n\t\tx86_pmu.flags |= PMU_FL_RETIRE_LATENCY;\n\n\tintel_aux_output_init();\n\n\treturn 0;\n}\n\n \nstatic __init int fixup_ht_bug(void)\n{\n\tint c;\n\t \n\tif (!(x86_pmu.flags & PMU_FL_EXCL_ENABLED))\n\t\treturn 0;\n\n\tif (topology_max_smt_threads() > 1) {\n\t\tpr_info(\"PMU erratum BJ122, BV98, HSD29 worked around, HT is on\\n\");\n\t\treturn 0;\n\t}\n\n\tcpus_read_lock();\n\n\thardlockup_detector_perf_stop();\n\n\tx86_pmu.flags &= ~(PMU_FL_EXCL_CNTRS | PMU_FL_EXCL_ENABLED);\n\n\tx86_pmu.start_scheduling = NULL;\n\tx86_pmu.commit_scheduling = NULL;\n\tx86_pmu.stop_scheduling = NULL;\n\n\thardlockup_detector_perf_restart();\n\n\tfor_each_online_cpu(c)\n\t\tfree_excl_cntrs(&per_cpu(cpu_hw_events, c));\n\n\tcpus_read_unlock();\n\tpr_info(\"PMU erratum BJ122, BV98, HSD29 workaround disabled, HT off\\n\");\n\treturn 0;\n}\nsubsys_initcall(fixup_ht_bug)\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}