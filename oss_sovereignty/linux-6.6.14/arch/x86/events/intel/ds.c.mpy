{
  "module_name": "ds.c",
  "hash_id": "ccf9193c5e6691eac2a3ccc84fb133adbc4d34fe557fbb30196b354381582429",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/events/intel/ds.c",
  "human_readable_source": "\n#include <linux/bitops.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/sched/clock.h>\n\n#include <asm/cpu_entry_area.h>\n#include <asm/perf_event.h>\n#include <asm/tlbflush.h>\n#include <asm/insn.h>\n#include <asm/io.h>\n#include <asm/timer.h>\n\n#include \"../perf_event.h\"\n\n \nDEFINE_PER_CPU_PAGE_ALIGNED(struct debug_store, cpu_debug_store);\n\n \n#define BTS_RECORD_SIZE\t\t24\n\n#define PEBS_FIXUP_SIZE\t\tPAGE_SIZE\n\n \n\nunion intel_x86_pebs_dse {\n\tu64 val;\n\tstruct {\n\t\tunsigned int ld_dse:4;\n\t\tunsigned int ld_stlb_miss:1;\n\t\tunsigned int ld_locked:1;\n\t\tunsigned int ld_data_blk:1;\n\t\tunsigned int ld_addr_blk:1;\n\t\tunsigned int ld_reserved:24;\n\t};\n\tstruct {\n\t\tunsigned int st_l1d_hit:1;\n\t\tunsigned int st_reserved1:3;\n\t\tunsigned int st_stlb_miss:1;\n\t\tunsigned int st_locked:1;\n\t\tunsigned int st_reserved2:26;\n\t};\n\tstruct {\n\t\tunsigned int st_lat_dse:4;\n\t\tunsigned int st_lat_stlb_miss:1;\n\t\tunsigned int st_lat_locked:1;\n\t\tunsigned int ld_reserved3:26;\n\t};\n\tstruct {\n\t\tunsigned int mtl_dse:5;\n\t\tunsigned int mtl_locked:1;\n\t\tunsigned int mtl_stlb_miss:1;\n\t\tunsigned int mtl_fwd_blk:1;\n\t\tunsigned int ld_reserved4:24;\n\t};\n};\n\n\n \n#define P(a, b) PERF_MEM_S(a, b)\n#define OP_LH (P(OP, LOAD) | P(LVL, HIT))\n#define LEVEL(x) P(LVLNUM, x)\n#define REM P(REMOTE, REMOTE)\n#define SNOOP_NONE_MISS (P(SNOOP, NONE) | P(SNOOP, MISS))\n\n \nstatic u64 pebs_data_source[] = {\n\tP(OP, LOAD) | P(LVL, MISS) | LEVEL(L3) | P(SNOOP, NA), \n\tOP_LH | P(LVL, L1)  | LEVEL(L1) | P(SNOOP, NONE),   \n\tOP_LH | P(LVL, LFB) | LEVEL(LFB) | P(SNOOP, NONE),  \n\tOP_LH | P(LVL, L2)  | LEVEL(L2) | P(SNOOP, NONE),   \n\tOP_LH | P(LVL, L3)  | LEVEL(L3) | P(SNOOP, NONE),   \n\tOP_LH | P(LVL, L3)  | LEVEL(L3) | P(SNOOP, MISS),   \n\tOP_LH | P(LVL, L3)  | LEVEL(L3) | P(SNOOP, HIT),    \n\tOP_LH | P(LVL, L3)  | LEVEL(L3) | P(SNOOP, HITM),   \n\tOP_LH | P(LVL, REM_CCE1) | REM | LEVEL(L3) | P(SNOOP, HIT),   \n\tOP_LH | P(LVL, REM_CCE1) | REM | LEVEL(L3) | P(SNOOP, HITM),  \n\tOP_LH | P(LVL, LOC_RAM)  | LEVEL(RAM) | P(SNOOP, HIT),        \n\tOP_LH | P(LVL, REM_RAM1) | REM | LEVEL(L3) | P(SNOOP, HIT),   \n\tOP_LH | P(LVL, LOC_RAM)  | LEVEL(RAM) | SNOOP_NONE_MISS,      \n\tOP_LH | P(LVL, REM_RAM1) | LEVEL(RAM) | REM | SNOOP_NONE_MISS,  \n\tOP_LH | P(LVL, IO)  | LEVEL(NA) | P(SNOOP, NONE),  \n\tOP_LH | P(LVL, UNC) | LEVEL(NA) | P(SNOOP, NONE),  \n};\n\n \nvoid __init intel_pmu_pebs_data_source_nhm(void)\n{\n\tpebs_data_source[0x05] = OP_LH | P(LVL, L3) | LEVEL(L3) | P(SNOOP, HIT);\n\tpebs_data_source[0x06] = OP_LH | P(LVL, L3) | LEVEL(L3) | P(SNOOP, HITM);\n\tpebs_data_source[0x07] = OP_LH | P(LVL, L3) | LEVEL(L3) | P(SNOOP, HITM);\n}\n\nstatic void __init __intel_pmu_pebs_data_source_skl(bool pmem, u64 *data_source)\n{\n\tu64 pmem_or_l4 = pmem ? LEVEL(PMEM) : LEVEL(L4);\n\n\tdata_source[0x08] = OP_LH | pmem_or_l4 | P(SNOOP, HIT);\n\tdata_source[0x09] = OP_LH | pmem_or_l4 | REM | P(SNOOP, HIT);\n\tdata_source[0x0b] = OP_LH | LEVEL(RAM) | REM | P(SNOOP, NONE);\n\tdata_source[0x0c] = OP_LH | LEVEL(ANY_CACHE) | REM | P(SNOOPX, FWD);\n\tdata_source[0x0d] = OP_LH | LEVEL(ANY_CACHE) | REM | P(SNOOP, HITM);\n}\n\nvoid __init intel_pmu_pebs_data_source_skl(bool pmem)\n{\n\t__intel_pmu_pebs_data_source_skl(pmem, pebs_data_source);\n}\n\nstatic void __init __intel_pmu_pebs_data_source_grt(u64 *data_source)\n{\n\tdata_source[0x05] = OP_LH | P(LVL, L3) | LEVEL(L3) | P(SNOOP, HIT);\n\tdata_source[0x06] = OP_LH | P(LVL, L3) | LEVEL(L3) | P(SNOOP, HITM);\n\tdata_source[0x08] = OP_LH | P(LVL, L3) | LEVEL(L3) | P(SNOOPX, FWD);\n}\n\nvoid __init intel_pmu_pebs_data_source_grt(void)\n{\n\t__intel_pmu_pebs_data_source_grt(pebs_data_source);\n}\n\nvoid __init intel_pmu_pebs_data_source_adl(void)\n{\n\tu64 *data_source;\n\n\tdata_source = x86_pmu.hybrid_pmu[X86_HYBRID_PMU_CORE_IDX].pebs_data_source;\n\tmemcpy(data_source, pebs_data_source, sizeof(pebs_data_source));\n\t__intel_pmu_pebs_data_source_skl(false, data_source);\n\n\tdata_source = x86_pmu.hybrid_pmu[X86_HYBRID_PMU_ATOM_IDX].pebs_data_source;\n\tmemcpy(data_source, pebs_data_source, sizeof(pebs_data_source));\n\t__intel_pmu_pebs_data_source_grt(data_source);\n}\n\nstatic void __init __intel_pmu_pebs_data_source_cmt(u64 *data_source)\n{\n\tdata_source[0x07] = OP_LH | P(LVL, L3) | LEVEL(L3) | P(SNOOPX, FWD);\n\tdata_source[0x08] = OP_LH | P(LVL, L3) | LEVEL(L3) | P(SNOOP, HITM);\n\tdata_source[0x0a] = OP_LH | P(LVL, LOC_RAM)  | LEVEL(RAM) | P(SNOOP, NONE);\n\tdata_source[0x0b] = OP_LH | LEVEL(RAM) | REM | P(SNOOP, NONE);\n\tdata_source[0x0c] = OP_LH | LEVEL(RAM) | REM | P(SNOOPX, FWD);\n\tdata_source[0x0d] = OP_LH | LEVEL(RAM) | REM | P(SNOOP, HITM);\n}\n\nvoid __init intel_pmu_pebs_data_source_mtl(void)\n{\n\tu64 *data_source;\n\n\tdata_source = x86_pmu.hybrid_pmu[X86_HYBRID_PMU_CORE_IDX].pebs_data_source;\n\tmemcpy(data_source, pebs_data_source, sizeof(pebs_data_source));\n\t__intel_pmu_pebs_data_source_skl(false, data_source);\n\n\tdata_source = x86_pmu.hybrid_pmu[X86_HYBRID_PMU_ATOM_IDX].pebs_data_source;\n\tmemcpy(data_source, pebs_data_source, sizeof(pebs_data_source));\n\t__intel_pmu_pebs_data_source_cmt(data_source);\n}\n\nvoid __init intel_pmu_pebs_data_source_cmt(void)\n{\n\t__intel_pmu_pebs_data_source_cmt(pebs_data_source);\n}\n\nstatic u64 precise_store_data(u64 status)\n{\n\tunion intel_x86_pebs_dse dse;\n\tu64 val = P(OP, STORE) | P(SNOOP, NA) | P(LVL, L1) | P(TLB, L2);\n\n\tdse.val = status;\n\n\t \n\tif (dse.st_stlb_miss)\n\t\tval |= P(TLB, MISS);\n\telse\n\t\tval |= P(TLB, HIT);\n\n\t \n\tif (dse.st_l1d_hit)\n\t\tval |= P(LVL, HIT);\n\telse\n\t\tval |= P(LVL, MISS);\n\n\t \n\tif (dse.st_locked)\n\t\tval |= P(LOCK, LOCKED);\n\n\treturn val;\n}\n\nstatic u64 precise_datala_hsw(struct perf_event *event, u64 status)\n{\n\tunion perf_mem_data_src dse;\n\n\tdse.val = PERF_MEM_NA;\n\n\tif (event->hw.flags & PERF_X86_EVENT_PEBS_ST_HSW)\n\t\tdse.mem_op = PERF_MEM_OP_STORE;\n\telse if (event->hw.flags & PERF_X86_EVENT_PEBS_LD_HSW)\n\t\tdse.mem_op = PERF_MEM_OP_LOAD;\n\n\t \n\tif (event->hw.flags & PERF_X86_EVENT_PEBS_ST_HSW) {\n\t\tif (status & 1)\n\t\t\tdse.mem_lvl = PERF_MEM_LVL_L1 | PERF_MEM_LVL_HIT;\n\t\telse\n\t\t\tdse.mem_lvl = PERF_MEM_LVL_L1 | PERF_MEM_LVL_MISS;\n\t}\n\treturn dse.val;\n}\n\nstatic inline void pebs_set_tlb_lock(u64 *val, bool tlb, bool lock)\n{\n\t \n\tif (tlb)\n\t\t*val |= P(TLB, MISS) | P(TLB, L2);\n\telse\n\t\t*val |= P(TLB, HIT) | P(TLB, L1) | P(TLB, L2);\n\n\t \n\tif (lock)\n\t\t*val |= P(LOCK, LOCKED);\n}\n\n \nstatic u64 __adl_latency_data_small(struct perf_event *event, u64 status,\n\t\t\t\t     u8 dse, bool tlb, bool lock, bool blk)\n{\n\tu64 val;\n\n\tWARN_ON_ONCE(hybrid_pmu(event->pmu)->cpu_type == hybrid_big);\n\n\tdse &= PERF_PEBS_DATA_SOURCE_MASK;\n\tval = hybrid_var(event->pmu, pebs_data_source)[dse];\n\n\tpebs_set_tlb_lock(&val, tlb, lock);\n\n\tif (blk)\n\t\tval |= P(BLK, DATA);\n\telse\n\t\tval |= P(BLK, NA);\n\n\treturn val;\n}\n\nu64 adl_latency_data_small(struct perf_event *event, u64 status)\n{\n\tunion intel_x86_pebs_dse dse;\n\n\tdse.val = status;\n\n\treturn __adl_latency_data_small(event, status, dse.ld_dse,\n\t\t\t\t\tdse.ld_locked, dse.ld_stlb_miss,\n\t\t\t\t\tdse.ld_data_blk);\n}\n\n \nu64 mtl_latency_data_small(struct perf_event *event, u64 status)\n{\n\tunion intel_x86_pebs_dse dse;\n\n\tdse.val = status;\n\n\treturn __adl_latency_data_small(event, status, dse.mtl_dse,\n\t\t\t\t\tdse.mtl_stlb_miss, dse.mtl_locked,\n\t\t\t\t\tdse.mtl_fwd_blk);\n}\n\nstatic u64 load_latency_data(struct perf_event *event, u64 status)\n{\n\tunion intel_x86_pebs_dse dse;\n\tu64 val;\n\n\tdse.val = status;\n\n\t \n\tval = hybrid_var(event->pmu, pebs_data_source)[dse.ld_dse];\n\n\t \n\tif (x86_pmu.pebs_no_tlb) {\n\t\tval |= P(TLB, NA) | P(LOCK, NA);\n\t\treturn val;\n\t}\n\n\tpebs_set_tlb_lock(&val, dse.ld_stlb_miss, dse.ld_locked);\n\n\t \n\tif (!x86_pmu.pebs_block) {\n\t\tval |= P(BLK, NA);\n\t\treturn val;\n\t}\n\t \n\tif (dse.ld_data_blk)\n\t\tval |= P(BLK, DATA);\n\n\t \n\tif (dse.ld_addr_blk)\n\t\tval |= P(BLK, ADDR);\n\n\tif (!dse.ld_data_blk && !dse.ld_addr_blk)\n\t\tval |= P(BLK, NA);\n\n\treturn val;\n}\n\nstatic u64 store_latency_data(struct perf_event *event, u64 status)\n{\n\tunion intel_x86_pebs_dse dse;\n\tunion perf_mem_data_src src;\n\tu64 val;\n\n\tdse.val = status;\n\n\t \n\tval = hybrid_var(event->pmu, pebs_data_source)[dse.st_lat_dse];\n\n\tpebs_set_tlb_lock(&val, dse.st_lat_stlb_miss, dse.st_lat_locked);\n\n\tval |= P(BLK, NA);\n\n\t \n\tsrc.val = val;\n\tsrc.mem_op = P(OP,STORE);\n\n\treturn src.val;\n}\n\nstruct pebs_record_core {\n\tu64 flags, ip;\n\tu64 ax, bx, cx, dx;\n\tu64 si, di, bp, sp;\n\tu64 r8,  r9,  r10, r11;\n\tu64 r12, r13, r14, r15;\n};\n\nstruct pebs_record_nhm {\n\tu64 flags, ip;\n\tu64 ax, bx, cx, dx;\n\tu64 si, di, bp, sp;\n\tu64 r8,  r9,  r10, r11;\n\tu64 r12, r13, r14, r15;\n\tu64 status, dla, dse, lat;\n};\n\n \nstruct pebs_record_hsw {\n\tu64 flags, ip;\n\tu64 ax, bx, cx, dx;\n\tu64 si, di, bp, sp;\n\tu64 r8,  r9,  r10, r11;\n\tu64 r12, r13, r14, r15;\n\tu64 status, dla, dse, lat;\n\tu64 real_ip, tsx_tuning;\n};\n\nunion hsw_tsx_tuning {\n\tstruct {\n\t\tu32 cycles_last_block     : 32,\n\t\t    hle_abort\t\t  : 1,\n\t\t    rtm_abort\t\t  : 1,\n\t\t    instruction_abort     : 1,\n\t\t    non_instruction_abort : 1,\n\t\t    retry\t\t  : 1,\n\t\t    data_conflict\t  : 1,\n\t\t    capacity_writes\t  : 1,\n\t\t    capacity_reads\t  : 1;\n\t};\n\tu64\t    value;\n};\n\n#define PEBS_HSW_TSX_FLAGS\t0xff00000000ULL\n\n \n\nstruct pebs_record_skl {\n\tu64 flags, ip;\n\tu64 ax, bx, cx, dx;\n\tu64 si, di, bp, sp;\n\tu64 r8,  r9,  r10, r11;\n\tu64 r12, r13, r14, r15;\n\tu64 status, dla, dse, lat;\n\tu64 real_ip, tsx_tuning;\n\tu64 tsc;\n};\n\nvoid init_debug_store_on_cpu(int cpu)\n{\n\tstruct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;\n\n\tif (!ds)\n\t\treturn;\n\n\twrmsr_on_cpu(cpu, MSR_IA32_DS_AREA,\n\t\t     (u32)((u64)(unsigned long)ds),\n\t\t     (u32)((u64)(unsigned long)ds >> 32));\n}\n\nvoid fini_debug_store_on_cpu(int cpu)\n{\n\tif (!per_cpu(cpu_hw_events, cpu).ds)\n\t\treturn;\n\n\twrmsr_on_cpu(cpu, MSR_IA32_DS_AREA, 0, 0);\n}\n\nstatic DEFINE_PER_CPU(void *, insn_buffer);\n\nstatic void ds_update_cea(void *cea, void *addr, size_t size, pgprot_t prot)\n{\n\tunsigned long start = (unsigned long)cea;\n\tphys_addr_t pa;\n\tsize_t msz = 0;\n\n\tpa = virt_to_phys(addr);\n\n\tpreempt_disable();\n\tfor (; msz < size; msz += PAGE_SIZE, pa += PAGE_SIZE, cea += PAGE_SIZE)\n\t\tcea_set_pte(cea, pa, prot);\n\n\t \n\tflush_tlb_kernel_range(start, start + size);\n\tpreempt_enable();\n}\n\nstatic void ds_clear_cea(void *cea, size_t size)\n{\n\tunsigned long start = (unsigned long)cea;\n\tsize_t msz = 0;\n\n\tpreempt_disable();\n\tfor (; msz < size; msz += PAGE_SIZE, cea += PAGE_SIZE)\n\t\tcea_set_pte(cea, 0, PAGE_NONE);\n\n\tflush_tlb_kernel_range(start, start + size);\n\tpreempt_enable();\n}\n\nstatic void *dsalloc_pages(size_t size, gfp_t flags, int cpu)\n{\n\tunsigned int order = get_order(size);\n\tint node = cpu_to_node(cpu);\n\tstruct page *page;\n\n\tpage = __alloc_pages_node(node, flags | __GFP_ZERO, order);\n\treturn page ? page_address(page) : NULL;\n}\n\nstatic void dsfree_pages(const void *buffer, size_t size)\n{\n\tif (buffer)\n\t\tfree_pages((unsigned long)buffer, get_order(size));\n}\n\nstatic int alloc_pebs_buffer(int cpu)\n{\n\tstruct cpu_hw_events *hwev = per_cpu_ptr(&cpu_hw_events, cpu);\n\tstruct debug_store *ds = hwev->ds;\n\tsize_t bsiz = x86_pmu.pebs_buffer_size;\n\tint max, node = cpu_to_node(cpu);\n\tvoid *buffer, *insn_buff, *cea;\n\n\tif (!x86_pmu.pebs)\n\t\treturn 0;\n\n\tbuffer = dsalloc_pages(bsiz, GFP_KERNEL, cpu);\n\tif (unlikely(!buffer))\n\t\treturn -ENOMEM;\n\n\t \n\tif (x86_pmu.intel_cap.pebs_format < 2) {\n\t\tinsn_buff = kzalloc_node(PEBS_FIXUP_SIZE, GFP_KERNEL, node);\n\t\tif (!insn_buff) {\n\t\t\tdsfree_pages(buffer, bsiz);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tper_cpu(insn_buffer, cpu) = insn_buff;\n\t}\n\thwev->ds_pebs_vaddr = buffer;\n\t \n\tcea = &get_cpu_entry_area(cpu)->cpu_debug_buffers.pebs_buffer;\n\tds->pebs_buffer_base = (unsigned long) cea;\n\tds_update_cea(cea, buffer, bsiz, PAGE_KERNEL);\n\tds->pebs_index = ds->pebs_buffer_base;\n\tmax = x86_pmu.pebs_record_size * (bsiz / x86_pmu.pebs_record_size);\n\tds->pebs_absolute_maximum = ds->pebs_buffer_base + max;\n\treturn 0;\n}\n\nstatic void release_pebs_buffer(int cpu)\n{\n\tstruct cpu_hw_events *hwev = per_cpu_ptr(&cpu_hw_events, cpu);\n\tvoid *cea;\n\n\tif (!x86_pmu.pebs)\n\t\treturn;\n\n\tkfree(per_cpu(insn_buffer, cpu));\n\tper_cpu(insn_buffer, cpu) = NULL;\n\n\t \n\tcea = &get_cpu_entry_area(cpu)->cpu_debug_buffers.pebs_buffer;\n\tds_clear_cea(cea, x86_pmu.pebs_buffer_size);\n\tdsfree_pages(hwev->ds_pebs_vaddr, x86_pmu.pebs_buffer_size);\n\thwev->ds_pebs_vaddr = NULL;\n}\n\nstatic int alloc_bts_buffer(int cpu)\n{\n\tstruct cpu_hw_events *hwev = per_cpu_ptr(&cpu_hw_events, cpu);\n\tstruct debug_store *ds = hwev->ds;\n\tvoid *buffer, *cea;\n\tint max;\n\n\tif (!x86_pmu.bts)\n\t\treturn 0;\n\n\tbuffer = dsalloc_pages(BTS_BUFFER_SIZE, GFP_KERNEL | __GFP_NOWARN, cpu);\n\tif (unlikely(!buffer)) {\n\t\tWARN_ONCE(1, \"%s: BTS buffer allocation failure\\n\", __func__);\n\t\treturn -ENOMEM;\n\t}\n\thwev->ds_bts_vaddr = buffer;\n\t \n\tcea = &get_cpu_entry_area(cpu)->cpu_debug_buffers.bts_buffer;\n\tds->bts_buffer_base = (unsigned long) cea;\n\tds_update_cea(cea, buffer, BTS_BUFFER_SIZE, PAGE_KERNEL);\n\tds->bts_index = ds->bts_buffer_base;\n\tmax = BTS_BUFFER_SIZE / BTS_RECORD_SIZE;\n\tds->bts_absolute_maximum = ds->bts_buffer_base +\n\t\t\t\t\tmax * BTS_RECORD_SIZE;\n\tds->bts_interrupt_threshold = ds->bts_absolute_maximum -\n\t\t\t\t\t(max / 16) * BTS_RECORD_SIZE;\n\treturn 0;\n}\n\nstatic void release_bts_buffer(int cpu)\n{\n\tstruct cpu_hw_events *hwev = per_cpu_ptr(&cpu_hw_events, cpu);\n\tvoid *cea;\n\n\tif (!x86_pmu.bts)\n\t\treturn;\n\n\t \n\tcea = &get_cpu_entry_area(cpu)->cpu_debug_buffers.bts_buffer;\n\tds_clear_cea(cea, BTS_BUFFER_SIZE);\n\tdsfree_pages(hwev->ds_bts_vaddr, BTS_BUFFER_SIZE);\n\thwev->ds_bts_vaddr = NULL;\n}\n\nstatic int alloc_ds_buffer(int cpu)\n{\n\tstruct debug_store *ds = &get_cpu_entry_area(cpu)->cpu_debug_store;\n\n\tmemset(ds, 0, sizeof(*ds));\n\tper_cpu(cpu_hw_events, cpu).ds = ds;\n\treturn 0;\n}\n\nstatic void release_ds_buffer(int cpu)\n{\n\tper_cpu(cpu_hw_events, cpu).ds = NULL;\n}\n\nvoid release_ds_buffers(void)\n{\n\tint cpu;\n\n\tif (!x86_pmu.bts && !x86_pmu.pebs)\n\t\treturn;\n\n\tfor_each_possible_cpu(cpu)\n\t\trelease_ds_buffer(cpu);\n\n\tfor_each_possible_cpu(cpu) {\n\t\t \n\t\tfini_debug_store_on_cpu(cpu);\n\t}\n\n\tfor_each_possible_cpu(cpu) {\n\t\trelease_pebs_buffer(cpu);\n\t\trelease_bts_buffer(cpu);\n\t}\n}\n\nvoid reserve_ds_buffers(void)\n{\n\tint bts_err = 0, pebs_err = 0;\n\tint cpu;\n\n\tx86_pmu.bts_active = 0;\n\tx86_pmu.pebs_active = 0;\n\n\tif (!x86_pmu.bts && !x86_pmu.pebs)\n\t\treturn;\n\n\tif (!x86_pmu.bts)\n\t\tbts_err = 1;\n\n\tif (!x86_pmu.pebs)\n\t\tpebs_err = 1;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tif (alloc_ds_buffer(cpu)) {\n\t\t\tbts_err = 1;\n\t\t\tpebs_err = 1;\n\t\t}\n\n\t\tif (!bts_err && alloc_bts_buffer(cpu))\n\t\t\tbts_err = 1;\n\n\t\tif (!pebs_err && alloc_pebs_buffer(cpu))\n\t\t\tpebs_err = 1;\n\n\t\tif (bts_err && pebs_err)\n\t\t\tbreak;\n\t}\n\n\tif (bts_err) {\n\t\tfor_each_possible_cpu(cpu)\n\t\t\trelease_bts_buffer(cpu);\n\t}\n\n\tif (pebs_err) {\n\t\tfor_each_possible_cpu(cpu)\n\t\t\trelease_pebs_buffer(cpu);\n\t}\n\n\tif (bts_err && pebs_err) {\n\t\tfor_each_possible_cpu(cpu)\n\t\t\trelease_ds_buffer(cpu);\n\t} else {\n\t\tif (x86_pmu.bts && !bts_err)\n\t\t\tx86_pmu.bts_active = 1;\n\n\t\tif (x86_pmu.pebs && !pebs_err)\n\t\t\tx86_pmu.pebs_active = 1;\n\n\t\tfor_each_possible_cpu(cpu) {\n\t\t\t \n\t\t\tinit_debug_store_on_cpu(cpu);\n\t\t}\n\t}\n}\n\n \n\nstruct event_constraint bts_constraint =\n\tEVENT_CONSTRAINT(0, 1ULL << INTEL_PMC_IDX_FIXED_BTS, 0);\n\nvoid intel_pmu_enable_bts(u64 config)\n{\n\tunsigned long debugctlmsr;\n\n\tdebugctlmsr = get_debugctlmsr();\n\n\tdebugctlmsr |= DEBUGCTLMSR_TR;\n\tdebugctlmsr |= DEBUGCTLMSR_BTS;\n\tif (config & ARCH_PERFMON_EVENTSEL_INT)\n\t\tdebugctlmsr |= DEBUGCTLMSR_BTINT;\n\n\tif (!(config & ARCH_PERFMON_EVENTSEL_OS))\n\t\tdebugctlmsr |= DEBUGCTLMSR_BTS_OFF_OS;\n\n\tif (!(config & ARCH_PERFMON_EVENTSEL_USR))\n\t\tdebugctlmsr |= DEBUGCTLMSR_BTS_OFF_USR;\n\n\tupdate_debugctlmsr(debugctlmsr);\n}\n\nvoid intel_pmu_disable_bts(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tunsigned long debugctlmsr;\n\n\tif (!cpuc->ds)\n\t\treturn;\n\n\tdebugctlmsr = get_debugctlmsr();\n\n\tdebugctlmsr &=\n\t\t~(DEBUGCTLMSR_TR | DEBUGCTLMSR_BTS | DEBUGCTLMSR_BTINT |\n\t\t  DEBUGCTLMSR_BTS_OFF_OS | DEBUGCTLMSR_BTS_OFF_USR);\n\n\tupdate_debugctlmsr(debugctlmsr);\n}\n\nint intel_pmu_drain_bts_buffer(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct debug_store *ds = cpuc->ds;\n\tstruct bts_record {\n\t\tu64\tfrom;\n\t\tu64\tto;\n\t\tu64\tflags;\n\t};\n\tstruct perf_event *event = cpuc->events[INTEL_PMC_IDX_FIXED_BTS];\n\tstruct bts_record *at, *base, *top;\n\tstruct perf_output_handle handle;\n\tstruct perf_event_header header;\n\tstruct perf_sample_data data;\n\tunsigned long skip = 0;\n\tstruct pt_regs regs;\n\n\tif (!event)\n\t\treturn 0;\n\n\tif (!x86_pmu.bts_active)\n\t\treturn 0;\n\n\tbase = (struct bts_record *)(unsigned long)ds->bts_buffer_base;\n\ttop  = (struct bts_record *)(unsigned long)ds->bts_index;\n\n\tif (top <= base)\n\t\treturn 0;\n\n\tmemset(&regs, 0, sizeof(regs));\n\n\tds->bts_index = ds->bts_buffer_base;\n\n\tperf_sample_data_init(&data, 0, event->hw.last_period);\n\n\t \n\tfor (at = base; at < top; at++) {\n\t\t \n\t\tif (event->attr.exclude_kernel &&\n\t\t    (kernel_ip(at->from) || kernel_ip(at->to)))\n\t\t\tskip++;\n\t}\n\n\t \n\trcu_read_lock();\n\tperf_prepare_sample(&data, event, &regs);\n\tperf_prepare_header(&header, &data, event, &regs);\n\n\tif (perf_output_begin(&handle, &data, event,\n\t\t\t      header.size * (top - base - skip)))\n\t\tgoto unlock;\n\n\tfor (at = base; at < top; at++) {\n\t\t \n\t\tif (event->attr.exclude_kernel &&\n\t\t    (kernel_ip(at->from) || kernel_ip(at->to)))\n\t\t\tcontinue;\n\n\t\tdata.ip\t\t= at->from;\n\t\tdata.addr\t= at->to;\n\n\t\tperf_output_sample(&handle, &header, &data, event);\n\t}\n\n\tperf_output_end(&handle);\n\n\t \n\tevent->hw.interrupts++;\n\tevent->pending_kill = POLL_IN;\nunlock:\n\trcu_read_unlock();\n\treturn 1;\n}\n\nstatic inline void intel_pmu_drain_pebs_buffer(void)\n{\n\tstruct perf_sample_data data;\n\n\tx86_pmu.drain_pebs(NULL, &data);\n}\n\n \nstruct event_constraint intel_core2_pebs_event_constraints[] = {\n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x00c0, 0x1),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0xfec1, 0x1),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x00c5, 0x1),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x1fc7, 0x1),  \n\tINTEL_FLAGS_EVENT_CONSTRAINT(0xcb, 0x1),     \n\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x108000c0, 0x01),\n\tEVENT_CONSTRAINT_END\n};\n\nstruct event_constraint intel_atom_pebs_event_constraints[] = {\n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x00c0, 0x1),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x00c5, 0x1),  \n\tINTEL_FLAGS_EVENT_CONSTRAINT(0xcb, 0x1),     \n\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x108000c0, 0x01),\n\t \n\tINTEL_ALL_EVENT_CONSTRAINT(0, 0x1),\n\tEVENT_CONSTRAINT_END\n};\n\nstruct event_constraint intel_slm_pebs_event_constraints[] = {\n\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x108000c0, 0x1),\n\t \n\tINTEL_ALL_EVENT_CONSTRAINT(0, 0x1),\n\tEVENT_CONSTRAINT_END\n};\n\nstruct event_constraint intel_glm_pebs_event_constraints[] = {\n\t \n\tINTEL_ALL_EVENT_CONSTRAINT(0, 0x1),\n\tEVENT_CONSTRAINT_END\n};\n\nstruct event_constraint intel_grt_pebs_event_constraints[] = {\n\t \n\tINTEL_HYBRID_LAT_CONSTRAINT(0x5d0, 0x3),\n\tINTEL_HYBRID_LAT_CONSTRAINT(0x6d0, 0xf),\n\tEVENT_CONSTRAINT_END\n};\n\nstruct event_constraint intel_nehalem_pebs_event_constraints[] = {\n\tINTEL_PLD_CONSTRAINT(0x100b, 0xf),       \n\tINTEL_FLAGS_EVENT_CONSTRAINT(0x0f, 0xf),     \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x010c, 0xf),  \n\tINTEL_FLAGS_EVENT_CONSTRAINT(0xc0, 0xf),     \n\tINTEL_EVENT_CONSTRAINT(0xc2, 0xf),     \n\tINTEL_FLAGS_EVENT_CONSTRAINT(0xc4, 0xf),     \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x02c5, 0xf),  \n\tINTEL_FLAGS_EVENT_CONSTRAINT(0xc7, 0xf),     \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x20c8, 0xf),  \n\tINTEL_FLAGS_EVENT_CONSTRAINT(0xcb, 0xf),     \n\tINTEL_FLAGS_EVENT_CONSTRAINT(0xf7, 0xf),     \n\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x108000c0, 0x0f),\n\tEVENT_CONSTRAINT_END\n};\n\nstruct event_constraint intel_westmere_pebs_event_constraints[] = {\n\tINTEL_PLD_CONSTRAINT(0x100b, 0xf),       \n\tINTEL_FLAGS_EVENT_CONSTRAINT(0x0f, 0xf),     \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x010c, 0xf),  \n\tINTEL_FLAGS_EVENT_CONSTRAINT(0xc0, 0xf),     \n\tINTEL_EVENT_CONSTRAINT(0xc2, 0xf),     \n\tINTEL_FLAGS_EVENT_CONSTRAINT(0xc4, 0xf),     \n\tINTEL_FLAGS_EVENT_CONSTRAINT(0xc5, 0xf),     \n\tINTEL_FLAGS_EVENT_CONSTRAINT(0xc7, 0xf),     \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x20c8, 0xf),  \n\tINTEL_FLAGS_EVENT_CONSTRAINT(0xcb, 0xf),     \n\tINTEL_FLAGS_EVENT_CONSTRAINT(0xf7, 0xf),     \n\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x108000c0, 0x0f),\n\tEVENT_CONSTRAINT_END\n};\n\nstruct event_constraint intel_snb_pebs_event_constraints[] = {\n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x01c0, 0x2),  \n\tINTEL_PLD_CONSTRAINT(0x01cd, 0x8),     \n\tINTEL_PST_CONSTRAINT(0x02cd, 0x8),     \n\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x108001c2, 0xf),\n        INTEL_EXCLEVT_CONSTRAINT(0xd0, 0xf),     \n        INTEL_EXCLEVT_CONSTRAINT(0xd1, 0xf),     \n        INTEL_EXCLEVT_CONSTRAINT(0xd2, 0xf),     \n        INTEL_EXCLEVT_CONSTRAINT(0xd3, 0xf),     \n\t \n\tINTEL_ALL_EVENT_CONSTRAINT(0, 0xf),\n\tEVENT_CONSTRAINT_END\n};\n\nstruct event_constraint intel_ivb_pebs_event_constraints[] = {\n        INTEL_FLAGS_UEVENT_CONSTRAINT(0x01c0, 0x2),  \n        INTEL_PLD_CONSTRAINT(0x01cd, 0x8),     \n\tINTEL_PST_CONSTRAINT(0x02cd, 0x8),     \n\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x108001c2, 0xf),\n\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x108001c0, 0x2),\n\tINTEL_EXCLEVT_CONSTRAINT(0xd0, 0xf),     \n\tINTEL_EXCLEVT_CONSTRAINT(0xd1, 0xf),     \n\tINTEL_EXCLEVT_CONSTRAINT(0xd2, 0xf),     \n\tINTEL_EXCLEVT_CONSTRAINT(0xd3, 0xf),     \n\t \n\tINTEL_ALL_EVENT_CONSTRAINT(0, 0xf),\n        EVENT_CONSTRAINT_END\n};\n\nstruct event_constraint intel_hsw_pebs_event_constraints[] = {\n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x01c0, 0x2),  \n\tINTEL_PLD_CONSTRAINT(0x01cd, 0xf),     \n\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x108001c2, 0xf),\n\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x108001c0, 0x2),\n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_NA(0x01c2, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_XLD(0x11d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_XLD(0x21d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_XLD(0x41d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_XLD(0x81d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_XST(0x12d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_XST(0x42d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_XST(0x82d0, 0xf),  \n\tINTEL_FLAGS_EVENT_CONSTRAINT_DATALA_XLD(0xd1, 0xf),     \n\tINTEL_FLAGS_EVENT_CONSTRAINT_DATALA_XLD(0xd2, 0xf),     \n\tINTEL_FLAGS_EVENT_CONSTRAINT_DATALA_XLD(0xd3, 0xf),     \n\t \n\tINTEL_ALL_EVENT_CONSTRAINT(0, 0xf),\n\tEVENT_CONSTRAINT_END\n};\n\nstruct event_constraint intel_bdw_pebs_event_constraints[] = {\n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x01c0, 0x2),  \n\tINTEL_PLD_CONSTRAINT(0x01cd, 0xf),     \n\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x108001c2, 0xf),\n\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x108001c0, 0x2),\n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_NA(0x01c2, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(0x11d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(0x21d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(0x41d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(0x81d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_ST(0x12d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_ST(0x42d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_ST(0x82d0, 0xf),  \n\tINTEL_FLAGS_EVENT_CONSTRAINT_DATALA_LD(0xd1, 0xf),     \n\tINTEL_FLAGS_EVENT_CONSTRAINT_DATALA_LD(0xd2, 0xf),     \n\tINTEL_FLAGS_EVENT_CONSTRAINT_DATALA_LD(0xd3, 0xf),     \n\t \n\tINTEL_ALL_EVENT_CONSTRAINT(0, 0xf),\n\tEVENT_CONSTRAINT_END\n};\n\n\nstruct event_constraint intel_skl_pebs_event_constraints[] = {\n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x1c0, 0x2),\t \n\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x108001c0, 0x2),\n\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x108000c0, 0x0f),\n\tINTEL_PLD_CONSTRAINT(0x1cd, 0xf),\t\t       \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(0x11d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_ST(0x12d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(0x21d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_ST(0x22d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(0x41d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_ST(0x42d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(0x81d0, 0xf),  \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_ST(0x82d0, 0xf),  \n\tINTEL_FLAGS_EVENT_CONSTRAINT_DATALA_LD(0xd1, 0xf),     \n\tINTEL_FLAGS_EVENT_CONSTRAINT_DATALA_LD(0xd2, 0xf),     \n\tINTEL_FLAGS_EVENT_CONSTRAINT_DATALA_LD(0xd3, 0xf),     \n\t \n\tINTEL_ALL_EVENT_CONSTRAINT(0, 0xf),\n\tEVENT_CONSTRAINT_END\n};\n\nstruct event_constraint intel_icl_pebs_event_constraints[] = {\n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x01c0, 0x100000000ULL),\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x0100, 0x100000000ULL),\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x0400, 0x800000000ULL),\t \n\n\tINTEL_PLD_CONSTRAINT(0x1cd, 0xff),\t\t\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(0x11d0, 0xf),\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_ST(0x12d0, 0xf),\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(0x21d0, 0xf),\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(0x41d0, 0xf),\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_ST(0x42d0, 0xf),\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(0x81d0, 0xf),\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_ST(0x82d0, 0xf),\t \n\n\tINTEL_FLAGS_EVENT_CONSTRAINT_DATALA_LD_RANGE(0xd1, 0xd4, 0xf),  \n\n\tINTEL_FLAGS_EVENT_CONSTRAINT(0xd0, 0xf),\t\t \n\n\t \n\n\tEVENT_CONSTRAINT_END\n};\n\nstruct event_constraint intel_spr_pebs_event_constraints[] = {\n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x100, 0x100000000ULL),\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT(0x0400, 0x800000000ULL),\n\n\tINTEL_FLAGS_EVENT_CONSTRAINT(0xc0, 0xfe),\n\tINTEL_PLD_CONSTRAINT(0x1cd, 0xfe),\n\tINTEL_PSD_CONSTRAINT(0x2cd, 0x1),\n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(0x11d0, 0xf),\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_ST(0x12d0, 0xf),\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(0x21d0, 0xf),\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(0x41d0, 0xf),\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_ST(0x42d0, 0xf),\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_LD(0x81d0, 0xf),\t \n\tINTEL_FLAGS_UEVENT_CONSTRAINT_DATALA_ST(0x82d0, 0xf),\t \n\n\tINTEL_FLAGS_EVENT_CONSTRAINT_DATALA_LD_RANGE(0xd1, 0xd4, 0xf),\n\n\tINTEL_FLAGS_EVENT_CONSTRAINT(0xd0, 0xf),\n\n\t \n\n\tEVENT_CONSTRAINT_END\n};\n\nstruct event_constraint *intel_pebs_constraints(struct perf_event *event)\n{\n\tstruct event_constraint *pebs_constraints = hybrid(event->pmu, pebs_constraints);\n\tstruct event_constraint *c;\n\n\tif (!event->attr.precise_ip)\n\t\treturn NULL;\n\n\tif (pebs_constraints) {\n\t\tfor_each_event_constraint(c, pebs_constraints) {\n\t\t\tif (constraint_match(c, event->hw.config)) {\n\t\t\t\tevent->hw.flags |= c->flags;\n\t\t\t\treturn c;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (x86_pmu.flags & PMU_FL_PEBS_ALL)\n\t\treturn NULL;\n\n\treturn &emptyconstraint;\n}\n\n \nstatic inline bool pebs_needs_sched_cb(struct cpu_hw_events *cpuc)\n{\n\tif (cpuc->n_pebs == cpuc->n_pebs_via_pt)\n\t\treturn false;\n\n\treturn cpuc->n_pebs && (cpuc->n_pebs == cpuc->n_large_pebs);\n}\n\nvoid intel_pmu_pebs_sched_task(struct perf_event_pmu_context *pmu_ctx, bool sched_in)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tif (!sched_in && pebs_needs_sched_cb(cpuc))\n\t\tintel_pmu_drain_pebs_buffer();\n}\n\nstatic inline void pebs_update_threshold(struct cpu_hw_events *cpuc)\n{\n\tstruct debug_store *ds = cpuc->ds;\n\tint max_pebs_events = hybrid(cpuc->pmu, max_pebs_events);\n\tint num_counters_fixed = hybrid(cpuc->pmu, num_counters_fixed);\n\tu64 threshold;\n\tint reserved;\n\n\tif (cpuc->n_pebs_via_pt)\n\t\treturn;\n\n\tif (x86_pmu.flags & PMU_FL_PEBS_ALL)\n\t\treserved = max_pebs_events + num_counters_fixed;\n\telse\n\t\treserved = max_pebs_events;\n\n\tif (cpuc->n_pebs == cpuc->n_large_pebs) {\n\t\tthreshold = ds->pebs_absolute_maximum -\n\t\t\treserved * cpuc->pebs_record_size;\n\t} else {\n\t\tthreshold = ds->pebs_buffer_base + cpuc->pebs_record_size;\n\t}\n\n\tds->pebs_interrupt_threshold = threshold;\n}\n\nstatic void adaptive_pebs_record_size_update(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tu64 pebs_data_cfg = cpuc->pebs_data_cfg;\n\tint sz = sizeof(struct pebs_basic);\n\n\tif (pebs_data_cfg & PEBS_DATACFG_MEMINFO)\n\t\tsz += sizeof(struct pebs_meminfo);\n\tif (pebs_data_cfg & PEBS_DATACFG_GP)\n\t\tsz += sizeof(struct pebs_gprs);\n\tif (pebs_data_cfg & PEBS_DATACFG_XMMS)\n\t\tsz += sizeof(struct pebs_xmm);\n\tif (pebs_data_cfg & PEBS_DATACFG_LBRS)\n\t\tsz += x86_pmu.lbr_nr * sizeof(struct lbr_entry);\n\n\tcpuc->pebs_record_size = sz;\n}\n\n#define PERF_PEBS_MEMINFO_TYPE\t(PERF_SAMPLE_ADDR | PERF_SAMPLE_DATA_SRC |   \\\n\t\t\t\tPERF_SAMPLE_PHYS_ADDR |\t\t\t     \\\n\t\t\t\tPERF_SAMPLE_WEIGHT_TYPE |\t\t     \\\n\t\t\t\tPERF_SAMPLE_TRANSACTION |\t\t     \\\n\t\t\t\tPERF_SAMPLE_DATA_PAGE_SIZE)\n\nstatic u64 pebs_update_adaptive_cfg(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\tu64 sample_type = attr->sample_type;\n\tu64 pebs_data_cfg = 0;\n\tbool gprs, tsx_weight;\n\n\tif (!(sample_type & ~(PERF_SAMPLE_IP|PERF_SAMPLE_TIME)) &&\n\t    attr->precise_ip > 1)\n\t\treturn pebs_data_cfg;\n\n\tif (sample_type & PERF_PEBS_MEMINFO_TYPE)\n\t\tpebs_data_cfg |= PEBS_DATACFG_MEMINFO;\n\n\t \n\tgprs = (sample_type & PERF_SAMPLE_REGS_INTR) &&\n\t       (attr->sample_regs_intr & PEBS_GP_REGS);\n\n\ttsx_weight = (sample_type & PERF_SAMPLE_WEIGHT_TYPE) &&\n\t\t     ((attr->config & INTEL_ARCH_EVENT_MASK) ==\n\t\t      x86_pmu.rtm_abort_event);\n\n\tif (gprs || (attr->precise_ip < 2) || tsx_weight)\n\t\tpebs_data_cfg |= PEBS_DATACFG_GP;\n\n\tif ((sample_type & PERF_SAMPLE_REGS_INTR) &&\n\t    (attr->sample_regs_intr & PERF_REG_EXTENDED_MASK))\n\t\tpebs_data_cfg |= PEBS_DATACFG_XMMS;\n\n\tif (sample_type & PERF_SAMPLE_BRANCH_STACK) {\n\t\t \n\t\tpebs_data_cfg |= PEBS_DATACFG_LBRS |\n\t\t\t((x86_pmu.lbr_nr-1) << PEBS_DATACFG_LBR_SHIFT);\n\t}\n\n\treturn pebs_data_cfg;\n}\n\nstatic void\npebs_update_state(bool needed_cb, struct cpu_hw_events *cpuc,\n\t\t  struct perf_event *event, bool add)\n{\n\tstruct pmu *pmu = event->pmu;\n\n\t \n\tif (cpuc->n_pebs == 1)\n\t\tcpuc->pebs_data_cfg = PEBS_UPDATE_DS_SW;\n\n\tif (needed_cb != pebs_needs_sched_cb(cpuc)) {\n\t\tif (!needed_cb)\n\t\t\tperf_sched_cb_inc(pmu);\n\t\telse\n\t\t\tperf_sched_cb_dec(pmu);\n\n\t\tcpuc->pebs_data_cfg |= PEBS_UPDATE_DS_SW;\n\t}\n\n\t \n\tif (x86_pmu.intel_cap.pebs_baseline && add) {\n\t\tu64 pebs_data_cfg;\n\n\t\tpebs_data_cfg = pebs_update_adaptive_cfg(event);\n\t\t \n\t\tif (pebs_data_cfg & ~cpuc->pebs_data_cfg)\n\t\t\tcpuc->pebs_data_cfg |= pebs_data_cfg | PEBS_UPDATE_DS_SW;\n\t}\n}\n\nvoid intel_pmu_pebs_add(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tbool needed_cb = pebs_needs_sched_cb(cpuc);\n\n\tcpuc->n_pebs++;\n\tif (hwc->flags & PERF_X86_EVENT_LARGE_PEBS)\n\t\tcpuc->n_large_pebs++;\n\tif (hwc->flags & PERF_X86_EVENT_PEBS_VIA_PT)\n\t\tcpuc->n_pebs_via_pt++;\n\n\tpebs_update_state(needed_cb, cpuc, event, true);\n}\n\nstatic void intel_pmu_pebs_via_pt_disable(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tif (!is_pebs_pt(event))\n\t\treturn;\n\n\tif (!(cpuc->pebs_enabled & ~PEBS_VIA_PT_MASK))\n\t\tcpuc->pebs_enabled &= ~PEBS_VIA_PT_MASK;\n}\n\nstatic void intel_pmu_pebs_via_pt_enable(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct debug_store *ds = cpuc->ds;\n\tu64 value = ds->pebs_event_reset[hwc->idx];\n\tu32 base = MSR_RELOAD_PMC0;\n\tunsigned int idx = hwc->idx;\n\n\tif (!is_pebs_pt(event))\n\t\treturn;\n\n\tif (!(event->hw.flags & PERF_X86_EVENT_LARGE_PEBS))\n\t\tcpuc->pebs_enabled |= PEBS_PMI_AFTER_EACH_RECORD;\n\n\tcpuc->pebs_enabled |= PEBS_OUTPUT_PT;\n\n\tif (hwc->idx >= INTEL_PMC_IDX_FIXED) {\n\t\tbase = MSR_RELOAD_FIXED_CTR0;\n\t\tidx = hwc->idx - INTEL_PMC_IDX_FIXED;\n\t\tif (x86_pmu.intel_cap.pebs_format < 5)\n\t\t\tvalue = ds->pebs_event_reset[MAX_PEBS_EVENTS_FMT4 + idx];\n\t\telse\n\t\t\tvalue = ds->pebs_event_reset[MAX_PEBS_EVENTS + idx];\n\t}\n\twrmsrl(base + idx, value);\n}\n\nstatic inline void intel_pmu_drain_large_pebs(struct cpu_hw_events *cpuc)\n{\n\tif (cpuc->n_pebs == cpuc->n_large_pebs &&\n\t    cpuc->n_pebs != cpuc->n_pebs_via_pt)\n\t\tintel_pmu_drain_pebs_buffer();\n}\n\nvoid intel_pmu_pebs_enable(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tu64 pebs_data_cfg = cpuc->pebs_data_cfg & ~PEBS_UPDATE_DS_SW;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct debug_store *ds = cpuc->ds;\n\tunsigned int idx = hwc->idx;\n\n\thwc->config &= ~ARCH_PERFMON_EVENTSEL_INT;\n\n\tcpuc->pebs_enabled |= 1ULL << hwc->idx;\n\n\tif ((event->hw.flags & PERF_X86_EVENT_PEBS_LDLAT) && (x86_pmu.version < 5))\n\t\tcpuc->pebs_enabled |= 1ULL << (hwc->idx + 32);\n\telse if (event->hw.flags & PERF_X86_EVENT_PEBS_ST)\n\t\tcpuc->pebs_enabled |= 1ULL << 63;\n\n\tif (x86_pmu.intel_cap.pebs_baseline) {\n\t\thwc->config |= ICL_EVENTSEL_ADAPTIVE;\n\t\tif (pebs_data_cfg != cpuc->active_pebs_data_cfg) {\n\t\t\t \n\t\t\tintel_pmu_drain_large_pebs(cpuc);\n\t\t\tadaptive_pebs_record_size_update();\n\t\t\twrmsrl(MSR_PEBS_DATA_CFG, pebs_data_cfg);\n\t\t\tcpuc->active_pebs_data_cfg = pebs_data_cfg;\n\t\t}\n\t}\n\tif (cpuc->pebs_data_cfg & PEBS_UPDATE_DS_SW) {\n\t\tcpuc->pebs_data_cfg = pebs_data_cfg;\n\t\tpebs_update_threshold(cpuc);\n\t}\n\n\tif (idx >= INTEL_PMC_IDX_FIXED) {\n\t\tif (x86_pmu.intel_cap.pebs_format < 5)\n\t\t\tidx = MAX_PEBS_EVENTS_FMT4 + (idx - INTEL_PMC_IDX_FIXED);\n\t\telse\n\t\t\tidx = MAX_PEBS_EVENTS + (idx - INTEL_PMC_IDX_FIXED);\n\t}\n\n\t \n\tif (hwc->flags & PERF_X86_EVENT_AUTO_RELOAD) {\n\t\tds->pebs_event_reset[idx] =\n\t\t\t(u64)(-hwc->sample_period) & x86_pmu.cntval_mask;\n\t} else {\n\t\tds->pebs_event_reset[idx] = 0;\n\t}\n\n\tintel_pmu_pebs_via_pt_enable(event);\n}\n\nvoid intel_pmu_pebs_del(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tbool needed_cb = pebs_needs_sched_cb(cpuc);\n\n\tcpuc->n_pebs--;\n\tif (hwc->flags & PERF_X86_EVENT_LARGE_PEBS)\n\t\tcpuc->n_large_pebs--;\n\tif (hwc->flags & PERF_X86_EVENT_PEBS_VIA_PT)\n\t\tcpuc->n_pebs_via_pt--;\n\n\tpebs_update_state(needed_cb, cpuc, event, false);\n}\n\nvoid intel_pmu_pebs_disable(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tintel_pmu_drain_large_pebs(cpuc);\n\n\tcpuc->pebs_enabled &= ~(1ULL << hwc->idx);\n\n\tif ((event->hw.flags & PERF_X86_EVENT_PEBS_LDLAT) &&\n\t    (x86_pmu.version < 5))\n\t\tcpuc->pebs_enabled &= ~(1ULL << (hwc->idx + 32));\n\telse if (event->hw.flags & PERF_X86_EVENT_PEBS_ST)\n\t\tcpuc->pebs_enabled &= ~(1ULL << 63);\n\n\tintel_pmu_pebs_via_pt_disable(event);\n\n\tif (cpuc->enabled)\n\t\twrmsrl(MSR_IA32_PEBS_ENABLE, cpuc->pebs_enabled);\n\n\thwc->config |= ARCH_PERFMON_EVENTSEL_INT;\n}\n\nvoid intel_pmu_pebs_enable_all(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tif (cpuc->pebs_enabled)\n\t\twrmsrl(MSR_IA32_PEBS_ENABLE, cpuc->pebs_enabled);\n}\n\nvoid intel_pmu_pebs_disable_all(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tif (cpuc->pebs_enabled)\n\t\t__intel_pmu_pebs_disable_all();\n}\n\nstatic int intel_pmu_pebs_fixup_ip(struct pt_regs *regs)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tunsigned long from = cpuc->lbr_entries[0].from;\n\tunsigned long old_to, to = cpuc->lbr_entries[0].to;\n\tunsigned long ip = regs->ip;\n\tint is_64bit = 0;\n\tvoid *kaddr;\n\tint size;\n\n\t \n\tif (!x86_pmu.intel_cap.pebs_trap)\n\t\treturn 1;\n\n\t \n\tif (!cpuc->lbr_stack.nr || !from || !to)\n\t\treturn 0;\n\n\t \n\tif (kernel_ip(ip) != kernel_ip(to))\n\t\treturn 0;\n\n\t \n\tif ((ip - to) > PEBS_FIXUP_SIZE)\n\t\treturn 0;\n\n\t \n\tif (ip == to) {\n\t\tset_linear_ip(regs, from);\n\t\treturn 1;\n\t}\n\n\tsize = ip - to;\n\tif (!kernel_ip(ip)) {\n\t\tint bytes;\n\t\tu8 *buf = this_cpu_read(insn_buffer);\n\n\t\t \n\t\tbytes = copy_from_user_nmi(buf, (void __user *)to, size);\n\t\tif (bytes != 0)\n\t\t\treturn 0;\n\n\t\tkaddr = buf;\n\t} else {\n\t\tkaddr = (void *)to;\n\t}\n\n\tdo {\n\t\tstruct insn insn;\n\n\t\told_to = to;\n\n#ifdef CONFIG_X86_64\n\t\tis_64bit = kernel_ip(to) || any_64bit_mode(regs);\n#endif\n\t\tinsn_init(&insn, kaddr, size, is_64bit);\n\n\t\t \n\t\tif (insn_get_length(&insn))\n\t\t\tbreak;\n\n\t\tto += insn.length;\n\t\tkaddr += insn.length;\n\t\tsize -= insn.length;\n\t} while (to < ip);\n\n\tif (to == ip) {\n\t\tset_linear_ip(regs, old_to);\n\t\treturn 1;\n\t}\n\n\t \n\treturn 0;\n}\n\nstatic inline u64 intel_get_tsx_weight(u64 tsx_tuning)\n{\n\tif (tsx_tuning) {\n\t\tunion hsw_tsx_tuning tsx = { .value = tsx_tuning };\n\t\treturn tsx.cycles_last_block;\n\t}\n\treturn 0;\n}\n\nstatic inline u64 intel_get_tsx_transaction(u64 tsx_tuning, u64 ax)\n{\n\tu64 txn = (tsx_tuning & PEBS_HSW_TSX_FLAGS) >> 32;\n\n\t \n\tif ((txn & PERF_TXN_TRANSACTION) && (ax & 1))\n\t\ttxn |= ((ax >> 24) & 0xff) << PERF_TXN_ABORT_SHIFT;\n\treturn txn;\n}\n\nstatic inline u64 get_pebs_status(void *n)\n{\n\tif (x86_pmu.intel_cap.pebs_format < 4)\n\t\treturn ((struct pebs_record_nhm *)n)->status;\n\treturn ((struct pebs_basic *)n)->applicable_counters;\n}\n\n#define PERF_X86_EVENT_PEBS_HSW_PREC \\\n\t\t(PERF_X86_EVENT_PEBS_ST_HSW | \\\n\t\t PERF_X86_EVENT_PEBS_LD_HSW | \\\n\t\t PERF_X86_EVENT_PEBS_NA_HSW)\n\nstatic u64 get_data_src(struct perf_event *event, u64 aux)\n{\n\tu64 val = PERF_MEM_NA;\n\tint fl = event->hw.flags;\n\tbool fst = fl & (PERF_X86_EVENT_PEBS_ST | PERF_X86_EVENT_PEBS_HSW_PREC);\n\n\tif (fl & PERF_X86_EVENT_PEBS_LDLAT)\n\t\tval = load_latency_data(event, aux);\n\telse if (fl & PERF_X86_EVENT_PEBS_STLAT)\n\t\tval = store_latency_data(event, aux);\n\telse if (fl & PERF_X86_EVENT_PEBS_LAT_HYBRID)\n\t\tval = x86_pmu.pebs_latency_data(event, aux);\n\telse if (fst && (fl & PERF_X86_EVENT_PEBS_HSW_PREC))\n\t\tval = precise_datala_hsw(event, aux);\n\telse if (fst)\n\t\tval = precise_store_data(aux);\n\treturn val;\n}\n\nstatic void setup_pebs_time(struct perf_event *event,\n\t\t\t    struct perf_sample_data *data,\n\t\t\t    u64 tsc)\n{\n\t \n\tif (event->attr.use_clockid != 0)\n\t\treturn;\n\n\t \n\tif (!using_native_sched_clock() || !sched_clock_stable())\n\t\treturn;\n\n\tdata->time = native_sched_clock_from_tsc(tsc) + __sched_clock_offset;\n\tdata->sample_flags |= PERF_SAMPLE_TIME;\n}\n\n#define PERF_SAMPLE_ADDR_TYPE\t(PERF_SAMPLE_ADDR |\t\t\\\n\t\t\t\t PERF_SAMPLE_PHYS_ADDR |\t\\\n\t\t\t\t PERF_SAMPLE_DATA_PAGE_SIZE)\n\nstatic void setup_pebs_fixed_sample_data(struct perf_event *event,\n\t\t\t\t   struct pt_regs *iregs, void *__pebs,\n\t\t\t\t   struct perf_sample_data *data,\n\t\t\t\t   struct pt_regs *regs)\n{\n\t \n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct pebs_record_skl *pebs = __pebs;\n\tu64 sample_type;\n\tint fll;\n\n\tif (pebs == NULL)\n\t\treturn;\n\n\tsample_type = event->attr.sample_type;\n\tfll = event->hw.flags & PERF_X86_EVENT_PEBS_LDLAT;\n\n\tperf_sample_data_init(data, 0, event->hw.last_period);\n\n\tdata->period = event->hw.last_period;\n\n\t \n\tif (fll && (sample_type & PERF_SAMPLE_WEIGHT_TYPE)) {\n\t\tdata->weight.full = pebs->lat;\n\t\tdata->sample_flags |= PERF_SAMPLE_WEIGHT_TYPE;\n\t}\n\n\t \n\tif (sample_type & PERF_SAMPLE_DATA_SRC) {\n\t\tdata->data_src.val = get_data_src(event, pebs->dse);\n\t\tdata->sample_flags |= PERF_SAMPLE_DATA_SRC;\n\t}\n\n\t \n\tif (sample_type & PERF_SAMPLE_CALLCHAIN)\n\t\tperf_sample_save_callchain(data, event, iregs);\n\n\t \n\t*regs = *iregs;\n\n\t \n\tregs->flags = pebs->flags & ~PERF_EFLAGS_EXACT;\n\n\tif (sample_type & PERF_SAMPLE_REGS_INTR) {\n\t\tregs->ax = pebs->ax;\n\t\tregs->bx = pebs->bx;\n\t\tregs->cx = pebs->cx;\n\t\tregs->dx = pebs->dx;\n\t\tregs->si = pebs->si;\n\t\tregs->di = pebs->di;\n\n\t\tregs->bp = pebs->bp;\n\t\tregs->sp = pebs->sp;\n\n#ifndef CONFIG_X86_32\n\t\tregs->r8 = pebs->r8;\n\t\tregs->r9 = pebs->r9;\n\t\tregs->r10 = pebs->r10;\n\t\tregs->r11 = pebs->r11;\n\t\tregs->r12 = pebs->r12;\n\t\tregs->r13 = pebs->r13;\n\t\tregs->r14 = pebs->r14;\n\t\tregs->r15 = pebs->r15;\n#endif\n\t}\n\n\tif (event->attr.precise_ip > 1) {\n\t\t \n\t\tif (x86_pmu.intel_cap.pebs_format >= 2) {\n\t\t\tset_linear_ip(regs, pebs->real_ip);\n\t\t\tregs->flags |= PERF_EFLAGS_EXACT;\n\t\t} else {\n\t\t\t \n\t\t\tset_linear_ip(regs, pebs->ip);\n\n\t\t\t \n\t\t\tif (intel_pmu_pebs_fixup_ip(regs))\n\t\t\t\tregs->flags |= PERF_EFLAGS_EXACT;\n\t\t}\n\t} else {\n\t\t \n\t\tset_linear_ip(regs, pebs->ip);\n\t}\n\n\n\tif ((sample_type & PERF_SAMPLE_ADDR_TYPE) &&\n\t    x86_pmu.intel_cap.pebs_format >= 1) {\n\t\tdata->addr = pebs->dla;\n\t\tdata->sample_flags |= PERF_SAMPLE_ADDR;\n\t}\n\n\tif (x86_pmu.intel_cap.pebs_format >= 2) {\n\t\t \n\t\tif ((sample_type & PERF_SAMPLE_WEIGHT_TYPE) && !fll) {\n\t\t\tdata->weight.full = intel_get_tsx_weight(pebs->tsx_tuning);\n\t\t\tdata->sample_flags |= PERF_SAMPLE_WEIGHT_TYPE;\n\t\t}\n\t\tif (sample_type & PERF_SAMPLE_TRANSACTION) {\n\t\t\tdata->txn = intel_get_tsx_transaction(pebs->tsx_tuning,\n\t\t\t\t\t\t\t      pebs->ax);\n\t\t\tdata->sample_flags |= PERF_SAMPLE_TRANSACTION;\n\t\t}\n\t}\n\n\t \n\tif (x86_pmu.intel_cap.pebs_format >= 3)\n\t\tsetup_pebs_time(event, data, pebs->tsc);\n\n\tif (has_branch_stack(event))\n\t\tperf_sample_save_brstack(data, event, &cpuc->lbr_stack);\n}\n\nstatic void adaptive_pebs_save_regs(struct pt_regs *regs,\n\t\t\t\t    struct pebs_gprs *gprs)\n{\n\tregs->ax = gprs->ax;\n\tregs->bx = gprs->bx;\n\tregs->cx = gprs->cx;\n\tregs->dx = gprs->dx;\n\tregs->si = gprs->si;\n\tregs->di = gprs->di;\n\tregs->bp = gprs->bp;\n\tregs->sp = gprs->sp;\n#ifndef CONFIG_X86_32\n\tregs->r8 = gprs->r8;\n\tregs->r9 = gprs->r9;\n\tregs->r10 = gprs->r10;\n\tregs->r11 = gprs->r11;\n\tregs->r12 = gprs->r12;\n\tregs->r13 = gprs->r13;\n\tregs->r14 = gprs->r14;\n\tregs->r15 = gprs->r15;\n#endif\n}\n\n#define PEBS_LATENCY_MASK\t\t\t0xffff\n#define PEBS_CACHE_LATENCY_OFFSET\t\t32\n#define PEBS_RETIRE_LATENCY_OFFSET\t\t32\n\n \n\nstatic void setup_pebs_adaptive_sample_data(struct perf_event *event,\n\t\t\t\t\t    struct pt_regs *iregs, void *__pebs,\n\t\t\t\t\t    struct perf_sample_data *data,\n\t\t\t\t\t    struct pt_regs *regs)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct pebs_basic *basic = __pebs;\n\tvoid *next_record = basic + 1;\n\tu64 sample_type;\n\tu64 format_size;\n\tstruct pebs_meminfo *meminfo = NULL;\n\tstruct pebs_gprs *gprs = NULL;\n\tstruct x86_perf_regs *perf_regs;\n\n\tif (basic == NULL)\n\t\treturn;\n\n\tperf_regs = container_of(regs, struct x86_perf_regs, regs);\n\tperf_regs->xmm_regs = NULL;\n\n\tsample_type = event->attr.sample_type;\n\tformat_size = basic->format_size;\n\tperf_sample_data_init(data, 0, event->hw.last_period);\n\tdata->period = event->hw.last_period;\n\n\tsetup_pebs_time(event, data, basic->tsc);\n\n\t \n\tif (sample_type & PERF_SAMPLE_CALLCHAIN)\n\t\tperf_sample_save_callchain(data, event, iregs);\n\n\t*regs = *iregs;\n\t \n\tset_linear_ip(regs, basic->ip);\n\tregs->flags = PERF_EFLAGS_EXACT;\n\n\tif ((sample_type & PERF_SAMPLE_WEIGHT_STRUCT) && (x86_pmu.flags & PMU_FL_RETIRE_LATENCY))\n\t\tdata->weight.var3_w = format_size >> PEBS_RETIRE_LATENCY_OFFSET & PEBS_LATENCY_MASK;\n\n\t \n\tif (format_size & PEBS_DATACFG_MEMINFO) {\n\t\tmeminfo = next_record;\n\t\tnext_record = meminfo + 1;\n\t}\n\n\tif (format_size & PEBS_DATACFG_GP) {\n\t\tgprs = next_record;\n\t\tnext_record = gprs + 1;\n\n\t\tif (event->attr.precise_ip < 2) {\n\t\t\tset_linear_ip(regs, gprs->ip);\n\t\t\tregs->flags &= ~PERF_EFLAGS_EXACT;\n\t\t}\n\n\t\tif (sample_type & PERF_SAMPLE_REGS_INTR)\n\t\t\tadaptive_pebs_save_regs(regs, gprs);\n\t}\n\n\tif (format_size & PEBS_DATACFG_MEMINFO) {\n\t\tif (sample_type & PERF_SAMPLE_WEIGHT_TYPE) {\n\t\t\tu64 weight = meminfo->latency;\n\n\t\t\tif (x86_pmu.flags & PMU_FL_INSTR_LATENCY) {\n\t\t\t\tdata->weight.var2_w = weight & PEBS_LATENCY_MASK;\n\t\t\t\tweight >>= PEBS_CACHE_LATENCY_OFFSET;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (sample_type & PERF_SAMPLE_WEIGHT) {\n\t\t\t\tdata->weight.full = weight ?:\n\t\t\t\t\tintel_get_tsx_weight(meminfo->tsx_tuning);\n\t\t\t} else {\n\t\t\t\tdata->weight.var1_dw = (u32)(weight & PEBS_LATENCY_MASK) ?:\n\t\t\t\t\tintel_get_tsx_weight(meminfo->tsx_tuning);\n\t\t\t}\n\t\t\tdata->sample_flags |= PERF_SAMPLE_WEIGHT_TYPE;\n\t\t}\n\n\t\tif (sample_type & PERF_SAMPLE_DATA_SRC) {\n\t\t\tdata->data_src.val = get_data_src(event, meminfo->aux);\n\t\t\tdata->sample_flags |= PERF_SAMPLE_DATA_SRC;\n\t\t}\n\n\t\tif (sample_type & PERF_SAMPLE_ADDR_TYPE) {\n\t\t\tdata->addr = meminfo->address;\n\t\t\tdata->sample_flags |= PERF_SAMPLE_ADDR;\n\t\t}\n\n\t\tif (sample_type & PERF_SAMPLE_TRANSACTION) {\n\t\t\tdata->txn = intel_get_tsx_transaction(meminfo->tsx_tuning,\n\t\t\t\t\t\t\t  gprs ? gprs->ax : 0);\n\t\t\tdata->sample_flags |= PERF_SAMPLE_TRANSACTION;\n\t\t}\n\t}\n\n\tif (format_size & PEBS_DATACFG_XMMS) {\n\t\tstruct pebs_xmm *xmm = next_record;\n\n\t\tnext_record = xmm + 1;\n\t\tperf_regs->xmm_regs = xmm->xmm;\n\t}\n\n\tif (format_size & PEBS_DATACFG_LBRS) {\n\t\tstruct lbr_entry *lbr = next_record;\n\t\tint num_lbr = ((format_size >> PEBS_DATACFG_LBR_SHIFT)\n\t\t\t\t\t& 0xff) + 1;\n\t\tnext_record = next_record + num_lbr * sizeof(struct lbr_entry);\n\n\t\tif (has_branch_stack(event)) {\n\t\t\tintel_pmu_store_pebs_lbrs(lbr);\n\t\t\tperf_sample_save_brstack(data, event, &cpuc->lbr_stack);\n\t\t}\n\t}\n\n\tWARN_ONCE(next_record != __pebs + (format_size >> 48),\n\t\t\t\"PEBS record size %llu, expected %llu, config %llx\\n\",\n\t\t\tformat_size >> 48,\n\t\t\t(u64)(next_record - __pebs),\n\t\t\tbasic->format_size);\n}\n\nstatic inline void *\nget_next_pebs_record_by_bit(void *base, void *top, int bit)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tvoid *at;\n\tu64 pebs_status;\n\n\t \n\tif (x86_pmu.intel_cap.pebs_format < 1)\n\t\treturn base;\n\n\tif (base == NULL)\n\t\treturn NULL;\n\n\tfor (at = base; at < top; at += cpuc->pebs_record_size) {\n\t\tunsigned long status = get_pebs_status(at);\n\n\t\tif (test_bit(bit, (unsigned long *)&status)) {\n\t\t\t \n\t\t\tif (x86_pmu.intel_cap.pebs_format >= 3)\n\t\t\t\treturn at;\n\n\t\t\tif (status == (1 << bit))\n\t\t\t\treturn at;\n\n\t\t\t \n\t\t\tpebs_status = status & cpuc->pebs_enabled;\n\t\t\tpebs_status &= PEBS_COUNTER_MASK;\n\t\t\tif (pebs_status == (1 << bit))\n\t\t\t\treturn at;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nvoid intel_pmu_auto_reload_read(struct perf_event *event)\n{\n\tWARN_ON(!(event->hw.flags & PERF_X86_EVENT_AUTO_RELOAD));\n\n\tperf_pmu_disable(event->pmu);\n\tintel_pmu_drain_pebs_buffer();\n\tperf_pmu_enable(event->pmu);\n}\n\n \nstatic int\nintel_pmu_save_and_restart_reload(struct perf_event *event, int count)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint shift = 64 - x86_pmu.cntval_bits;\n\tu64 period = hwc->sample_period;\n\tu64 prev_raw_count, new_raw_count;\n\ts64 new, old;\n\n\tWARN_ON(!period);\n\n\t \n\tWARN_ON(this_cpu_read(cpu_hw_events.enabled));\n\n\tprev_raw_count = local64_read(&hwc->prev_count);\n\trdpmcl(hwc->event_base_rdpmc, new_raw_count);\n\tlocal64_set(&hwc->prev_count, new_raw_count);\n\n\t \n\tnew = ((s64)(new_raw_count << shift) >> shift);\n\told = ((s64)(prev_raw_count << shift) >> shift);\n\tlocal64_add(new - old + count * period, &event->count);\n\n\tlocal64_set(&hwc->period_left, -new);\n\n\tperf_event_update_userpage(event);\n\n\treturn 0;\n}\n\nstatic __always_inline void\n__intel_pmu_pebs_event(struct perf_event *event,\n\t\t       struct pt_regs *iregs,\n\t\t       struct perf_sample_data *data,\n\t\t       void *base, void *top,\n\t\t       int bit, int count,\n\t\t       void (*setup_sample)(struct perf_event *,\n\t\t\t\t\t    struct pt_regs *,\n\t\t\t\t\t    void *,\n\t\t\t\t\t    struct perf_sample_data *,\n\t\t\t\t\t    struct pt_regs *))\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct x86_perf_regs perf_regs;\n\tstruct pt_regs *regs = &perf_regs.regs;\n\tvoid *at = get_next_pebs_record_by_bit(base, top, bit);\n\tstatic struct pt_regs dummy_iregs;\n\n\tif (hwc->flags & PERF_X86_EVENT_AUTO_RELOAD) {\n\t\t \n\t\tintel_pmu_save_and_restart_reload(event, count);\n\t} else if (!intel_pmu_save_and_restart(event))\n\t\treturn;\n\n\tif (!iregs)\n\t\tiregs = &dummy_iregs;\n\n\twhile (count > 1) {\n\t\tsetup_sample(event, iregs, at, data, regs);\n\t\tperf_event_output(event, data, regs);\n\t\tat += cpuc->pebs_record_size;\n\t\tat = get_next_pebs_record_by_bit(at, top, bit);\n\t\tcount--;\n\t}\n\n\tsetup_sample(event, iregs, at, data, regs);\n\tif (iregs == &dummy_iregs) {\n\t\t \n\t\tperf_event_output(event, data, regs);\n\t} else {\n\t\t \n\t\tif (perf_event_overflow(event, data, regs))\n\t\t\tx86_pmu_stop(event, 0);\n\t}\n}\n\nstatic void intel_pmu_drain_pebs_core(struct pt_regs *iregs, struct perf_sample_data *data)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct debug_store *ds = cpuc->ds;\n\tstruct perf_event *event = cpuc->events[0];  \n\tstruct pebs_record_core *at, *top;\n\tint n;\n\n\tif (!x86_pmu.pebs_active)\n\t\treturn;\n\n\tat  = (struct pebs_record_core *)(unsigned long)ds->pebs_buffer_base;\n\ttop = (struct pebs_record_core *)(unsigned long)ds->pebs_index;\n\n\t \n\tds->pebs_index = ds->pebs_buffer_base;\n\n\tif (!test_bit(0, cpuc->active_mask))\n\t\treturn;\n\n\tWARN_ON_ONCE(!event);\n\n\tif (!event->attr.precise_ip)\n\t\treturn;\n\n\tn = top - at;\n\tif (n <= 0) {\n\t\tif (event->hw.flags & PERF_X86_EVENT_AUTO_RELOAD)\n\t\t\tintel_pmu_save_and_restart_reload(event, 0);\n\t\treturn;\n\t}\n\n\t__intel_pmu_pebs_event(event, iregs, data, at, top, 0, n,\n\t\t\t       setup_pebs_fixed_sample_data);\n}\n\nstatic void intel_pmu_pebs_event_update_no_drain(struct cpu_hw_events *cpuc, int size)\n{\n\tstruct perf_event *event;\n\tint bit;\n\n\t \n\tfor_each_set_bit(bit, (unsigned long *)&cpuc->pebs_enabled, size) {\n\t\tevent = cpuc->events[bit];\n\t\tif (event->hw.flags & PERF_X86_EVENT_AUTO_RELOAD)\n\t\t\tintel_pmu_save_and_restart_reload(event, 0);\n\t}\n}\n\nstatic void intel_pmu_drain_pebs_nhm(struct pt_regs *iregs, struct perf_sample_data *data)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct debug_store *ds = cpuc->ds;\n\tstruct perf_event *event;\n\tvoid *base, *at, *top;\n\tshort counts[INTEL_PMC_IDX_FIXED + MAX_FIXED_PEBS_EVENTS] = {};\n\tshort error[INTEL_PMC_IDX_FIXED + MAX_FIXED_PEBS_EVENTS] = {};\n\tint bit, i, size;\n\tu64 mask;\n\n\tif (!x86_pmu.pebs_active)\n\t\treturn;\n\n\tbase = (struct pebs_record_nhm *)(unsigned long)ds->pebs_buffer_base;\n\ttop = (struct pebs_record_nhm *)(unsigned long)ds->pebs_index;\n\n\tds->pebs_index = ds->pebs_buffer_base;\n\n\tmask = (1ULL << x86_pmu.max_pebs_events) - 1;\n\tsize = x86_pmu.max_pebs_events;\n\tif (x86_pmu.flags & PMU_FL_PEBS_ALL) {\n\t\tmask |= ((1ULL << x86_pmu.num_counters_fixed) - 1) << INTEL_PMC_IDX_FIXED;\n\t\tsize = INTEL_PMC_IDX_FIXED + x86_pmu.num_counters_fixed;\n\t}\n\n\tif (unlikely(base >= top)) {\n\t\tintel_pmu_pebs_event_update_no_drain(cpuc, size);\n\t\treturn;\n\t}\n\n\tfor (at = base; at < top; at += x86_pmu.pebs_record_size) {\n\t\tstruct pebs_record_nhm *p = at;\n\t\tu64 pebs_status;\n\n\t\tpebs_status = p->status & cpuc->pebs_enabled;\n\t\tpebs_status &= mask;\n\n\t\t \n\t\tif (x86_pmu.intel_cap.pebs_format >= 3) {\n\t\t\tfor_each_set_bit(bit, (unsigned long *)&pebs_status, size)\n\t\t\t\tcounts[bit]++;\n\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!pebs_status && cpuc->pebs_enabled &&\n\t\t\t!(cpuc->pebs_enabled & (cpuc->pebs_enabled-1)))\n\t\t\tpebs_status = p->status = cpuc->pebs_enabled;\n\n\t\tbit = find_first_bit((unsigned long *)&pebs_status,\n\t\t\t\t\tx86_pmu.max_pebs_events);\n\t\tif (bit >= x86_pmu.max_pebs_events)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (pebs_status != (1ULL << bit)) {\n\t\t\tfor_each_set_bit(i, (unsigned long *)&pebs_status, size)\n\t\t\t\terror[i]++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tcounts[bit]++;\n\t}\n\n\tfor_each_set_bit(bit, (unsigned long *)&mask, size) {\n\t\tif ((counts[bit] == 0) && (error[bit] == 0))\n\t\t\tcontinue;\n\n\t\tevent = cpuc->events[bit];\n\t\tif (WARN_ON_ONCE(!event))\n\t\t\tcontinue;\n\n\t\tif (WARN_ON_ONCE(!event->attr.precise_ip))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (error[bit]) {\n\t\t\tperf_log_lost_samples(event, error[bit]);\n\n\t\t\tif (iregs && perf_event_account_interrupt(event))\n\t\t\t\tx86_pmu_stop(event, 0);\n\t\t}\n\n\t\tif (counts[bit]) {\n\t\t\t__intel_pmu_pebs_event(event, iregs, data, base,\n\t\t\t\t\t       top, bit, counts[bit],\n\t\t\t\t\t       setup_pebs_fixed_sample_data);\n\t\t}\n\t}\n}\n\nstatic void intel_pmu_drain_pebs_icl(struct pt_regs *iregs, struct perf_sample_data *data)\n{\n\tshort counts[INTEL_PMC_IDX_FIXED + MAX_FIXED_PEBS_EVENTS] = {};\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tint max_pebs_events = hybrid(cpuc->pmu, max_pebs_events);\n\tint num_counters_fixed = hybrid(cpuc->pmu, num_counters_fixed);\n\tstruct debug_store *ds = cpuc->ds;\n\tstruct perf_event *event;\n\tvoid *base, *at, *top;\n\tint bit, size;\n\tu64 mask;\n\n\tif (!x86_pmu.pebs_active)\n\t\treturn;\n\n\tbase = (struct pebs_basic *)(unsigned long)ds->pebs_buffer_base;\n\ttop = (struct pebs_basic *)(unsigned long)ds->pebs_index;\n\n\tds->pebs_index = ds->pebs_buffer_base;\n\n\tmask = ((1ULL << max_pebs_events) - 1) |\n\t       (((1ULL << num_counters_fixed) - 1) << INTEL_PMC_IDX_FIXED);\n\tsize = INTEL_PMC_IDX_FIXED + num_counters_fixed;\n\n\tif (unlikely(base >= top)) {\n\t\tintel_pmu_pebs_event_update_no_drain(cpuc, size);\n\t\treturn;\n\t}\n\n\tfor (at = base; at < top; at += cpuc->pebs_record_size) {\n\t\tu64 pebs_status;\n\n\t\tpebs_status = get_pebs_status(at) & cpuc->pebs_enabled;\n\t\tpebs_status &= mask;\n\n\t\tfor_each_set_bit(bit, (unsigned long *)&pebs_status, size)\n\t\t\tcounts[bit]++;\n\t}\n\n\tfor_each_set_bit(bit, (unsigned long *)&mask, size) {\n\t\tif (counts[bit] == 0)\n\t\t\tcontinue;\n\n\t\tevent = cpuc->events[bit];\n\t\tif (WARN_ON_ONCE(!event))\n\t\t\tcontinue;\n\n\t\tif (WARN_ON_ONCE(!event->attr.precise_ip))\n\t\t\tcontinue;\n\n\t\t__intel_pmu_pebs_event(event, iregs, data, base,\n\t\t\t\t       top, bit, counts[bit],\n\t\t\t\t       setup_pebs_adaptive_sample_data);\n\t}\n}\n\n \n\nvoid __init intel_ds_init(void)\n{\n\t \n\tif (!boot_cpu_has(X86_FEATURE_DTES64))\n\t\treturn;\n\n\tx86_pmu.bts  = boot_cpu_has(X86_FEATURE_BTS);\n\tx86_pmu.pebs = boot_cpu_has(X86_FEATURE_PEBS);\n\tx86_pmu.pebs_buffer_size = PEBS_BUFFER_SIZE;\n\tif (x86_pmu.version <= 4)\n\t\tx86_pmu.pebs_no_isolation = 1;\n\n\tif (x86_pmu.pebs) {\n\t\tchar pebs_type = x86_pmu.intel_cap.pebs_trap ?  '+' : '-';\n\t\tchar *pebs_qual = \"\";\n\t\tint format = x86_pmu.intel_cap.pebs_format;\n\n\t\tif (format < 4)\n\t\t\tx86_pmu.intel_cap.pebs_baseline = 0;\n\n\t\tswitch (format) {\n\t\tcase 0:\n\t\t\tpr_cont(\"PEBS fmt0%c, \", pebs_type);\n\t\t\tx86_pmu.pebs_record_size = sizeof(struct pebs_record_core);\n\t\t\t \n\t\t\tx86_pmu.pebs_buffer_size = PAGE_SIZE;\n\t\t\tx86_pmu.drain_pebs = intel_pmu_drain_pebs_core;\n\t\t\tbreak;\n\n\t\tcase 1:\n\t\t\tpr_cont(\"PEBS fmt1%c, \", pebs_type);\n\t\t\tx86_pmu.pebs_record_size = sizeof(struct pebs_record_nhm);\n\t\t\tx86_pmu.drain_pebs = intel_pmu_drain_pebs_nhm;\n\t\t\tbreak;\n\n\t\tcase 2:\n\t\t\tpr_cont(\"PEBS fmt2%c, \", pebs_type);\n\t\t\tx86_pmu.pebs_record_size = sizeof(struct pebs_record_hsw);\n\t\t\tx86_pmu.drain_pebs = intel_pmu_drain_pebs_nhm;\n\t\t\tbreak;\n\n\t\tcase 3:\n\t\t\tpr_cont(\"PEBS fmt3%c, \", pebs_type);\n\t\t\tx86_pmu.pebs_record_size =\n\t\t\t\t\t\tsizeof(struct pebs_record_skl);\n\t\t\tx86_pmu.drain_pebs = intel_pmu_drain_pebs_nhm;\n\t\t\tx86_pmu.large_pebs_flags |= PERF_SAMPLE_TIME;\n\t\t\tbreak;\n\n\t\tcase 5:\n\t\t\tx86_pmu.pebs_ept = 1;\n\t\t\tfallthrough;\n\t\tcase 4:\n\t\t\tx86_pmu.drain_pebs = intel_pmu_drain_pebs_icl;\n\t\t\tx86_pmu.pebs_record_size = sizeof(struct pebs_basic);\n\t\t\tif (x86_pmu.intel_cap.pebs_baseline) {\n\t\t\t\tx86_pmu.large_pebs_flags |=\n\t\t\t\t\tPERF_SAMPLE_BRANCH_STACK |\n\t\t\t\t\tPERF_SAMPLE_TIME;\n\t\t\t\tx86_pmu.flags |= PMU_FL_PEBS_ALL;\n\t\t\t\tx86_pmu.pebs_capable = ~0ULL;\n\t\t\t\tpebs_qual = \"-baseline\";\n\t\t\t\tx86_get_pmu(smp_processor_id())->capabilities |= PERF_PMU_CAP_EXTENDED_REGS;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tx86_pmu.large_pebs_flags &=\n\t\t\t\t\t~(PERF_SAMPLE_ADDR |\n\t\t\t\t\t  PERF_SAMPLE_TIME |\n\t\t\t\t\t  PERF_SAMPLE_DATA_SRC |\n\t\t\t\t\t  PERF_SAMPLE_TRANSACTION |\n\t\t\t\t\t  PERF_SAMPLE_REGS_USER |\n\t\t\t\t\t  PERF_SAMPLE_REGS_INTR);\n\t\t\t}\n\t\t\tpr_cont(\"PEBS fmt4%c%s, \", pebs_type, pebs_qual);\n\n\t\t\tif (!is_hybrid() && x86_pmu.intel_cap.pebs_output_pt_available) {\n\t\t\t\tpr_cont(\"PEBS-via-PT, \");\n\t\t\t\tx86_get_pmu(smp_processor_id())->capabilities |= PERF_PMU_CAP_AUX_OUTPUT;\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tpr_cont(\"no PEBS fmt%d%c, \", format, pebs_type);\n\t\t\tx86_pmu.pebs = 0;\n\t\t}\n\t}\n}\n\nvoid perf_restore_debug_store(void)\n{\n\tstruct debug_store *ds = __this_cpu_read(cpu_hw_events.ds);\n\n\tif (!x86_pmu.bts && !x86_pmu.pebs)\n\t\treturn;\n\n\twrmsrl(MSR_IA32_DS_AREA, (unsigned long)ds);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}