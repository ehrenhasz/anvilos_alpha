{
  "module_name": "lbr.c",
  "hash_id": "0ca3b1d149e9bc340c7255ebab442d56d7bf53d1aee3b2ecc9fda4e10e0979ce",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/events/amd/lbr.c",
  "human_readable_source": "\n#include <linux/perf_event.h>\n#include <asm/perf_event.h>\n\n#include \"../perf_event.h\"\n\n \n#define LBR_SELECT_MASK\t\t0x1ff\n\n \n#define LBR_SELECT_KERNEL\t\t0\t \n#define LBR_SELECT_USER\t\t\t1\t \n#define LBR_SELECT_JCC\t\t\t2\t \n#define LBR_SELECT_CALL_NEAR_REL\t3\t \n#define LBR_SELECT_CALL_NEAR_IND\t4\t \n#define LBR_SELECT_RET_NEAR\t\t5\t \n#define LBR_SELECT_JMP_NEAR_IND\t\t6\t \n#define LBR_SELECT_JMP_NEAR_REL\t\t7\t \n#define LBR_SELECT_FAR_BRANCH\t\t8\t \n\n#define LBR_KERNEL\tBIT(LBR_SELECT_KERNEL)\n#define LBR_USER\tBIT(LBR_SELECT_USER)\n#define LBR_JCC\t\tBIT(LBR_SELECT_JCC)\n#define LBR_REL_CALL\tBIT(LBR_SELECT_CALL_NEAR_REL)\n#define LBR_IND_CALL\tBIT(LBR_SELECT_CALL_NEAR_IND)\n#define LBR_RETURN\tBIT(LBR_SELECT_RET_NEAR)\n#define LBR_REL_JMP\tBIT(LBR_SELECT_JMP_NEAR_REL)\n#define LBR_IND_JMP\tBIT(LBR_SELECT_JMP_NEAR_IND)\n#define LBR_FAR\t\tBIT(LBR_SELECT_FAR_BRANCH)\n#define LBR_NOT_SUPP\t-1\t \n#define LBR_IGNORE\t0\n\n#define LBR_ANY\t\t\\\n\t(LBR_JCC | LBR_REL_CALL | LBR_IND_CALL | LBR_RETURN |\t\\\n\t LBR_REL_JMP | LBR_IND_JMP | LBR_FAR)\n\nstruct branch_entry {\n\tunion {\n\t\tstruct {\n\t\t\tu64\tip:58;\n\t\t\tu64\tip_sign_ext:5;\n\t\t\tu64\tmispredict:1;\n\t\t} split;\n\t\tu64\t\tfull;\n\t} from;\n\n\tunion {\n\t\tstruct {\n\t\t\tu64\tip:58;\n\t\t\tu64\tip_sign_ext:3;\n\t\t\tu64\treserved:1;\n\t\t\tu64\tspec:1;\n\t\t\tu64\tvalid:1;\n\t\t} split;\n\t\tu64\t\tfull;\n\t} to;\n};\n\nstatic __always_inline void amd_pmu_lbr_set_from(unsigned int idx, u64 val)\n{\n\twrmsrl(MSR_AMD_SAMP_BR_FROM + idx * 2, val);\n}\n\nstatic __always_inline void amd_pmu_lbr_set_to(unsigned int idx, u64 val)\n{\n\twrmsrl(MSR_AMD_SAMP_BR_FROM + idx * 2 + 1, val);\n}\n\nstatic __always_inline u64 amd_pmu_lbr_get_from(unsigned int idx)\n{\n\tu64 val;\n\n\trdmsrl(MSR_AMD_SAMP_BR_FROM + idx * 2, val);\n\n\treturn val;\n}\n\nstatic __always_inline u64 amd_pmu_lbr_get_to(unsigned int idx)\n{\n\tu64 val;\n\n\trdmsrl(MSR_AMD_SAMP_BR_FROM + idx * 2 + 1, val);\n\n\treturn val;\n}\n\nstatic __always_inline u64 sign_ext_branch_ip(u64 ip)\n{\n\tu32 shift = 64 - boot_cpu_data.x86_virt_bits;\n\n\treturn (u64)(((s64)ip << shift) >> shift);\n}\n\nstatic void amd_pmu_lbr_filter(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tint br_sel = cpuc->br_sel, offset, type, i, j;\n\tbool compress = false;\n\tbool fused_only = false;\n\tu64 from, to;\n\n\t \n\tif (((br_sel & X86_BR_ALL) == X86_BR_ALL) &&\n\t    ((br_sel & X86_BR_TYPE_SAVE) != X86_BR_TYPE_SAVE))\n\t\tfused_only = true;\n\n\tfor (i = 0; i < cpuc->lbr_stack.nr; i++) {\n\t\tfrom = cpuc->lbr_entries[i].from;\n\t\tto = cpuc->lbr_entries[i].to;\n\t\ttype = branch_type_fused(from, to, 0, &offset);\n\n\t\t \n\t\tif (offset) {\n\t\t\tcpuc->lbr_entries[i].from += offset;\n\t\t\tif (fused_only)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (type == X86_BR_NONE || (br_sel & type) != type) {\n\t\t\tcpuc->lbr_entries[i].from = 0;\t \n\t\t\tcompress = true;\n\t\t}\n\n\t\tif ((br_sel & X86_BR_TYPE_SAVE) == X86_BR_TYPE_SAVE)\n\t\t\tcpuc->lbr_entries[i].type = common_branch_type(type);\n\t}\n\n\tif (!compress)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < cpuc->lbr_stack.nr; ) {\n\t\tif (!cpuc->lbr_entries[i].from) {\n\t\t\tj = i;\n\t\t\twhile (++j < cpuc->lbr_stack.nr)\n\t\t\t\tcpuc->lbr_entries[j - 1] = cpuc->lbr_entries[j];\n\t\t\tcpuc->lbr_stack.nr--;\n\t\t\tif (!cpuc->lbr_entries[i].from)\n\t\t\t\tcontinue;\n\t\t}\n\t\ti++;\n\t}\n}\n\nstatic const int lbr_spec_map[PERF_BR_SPEC_MAX] = {\n\tPERF_BR_SPEC_NA,\n\tPERF_BR_SPEC_WRONG_PATH,\n\tPERF_BR_NON_SPEC_CORRECT_PATH,\n\tPERF_BR_SPEC_CORRECT_PATH,\n};\n\nvoid amd_pmu_lbr_read(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct perf_branch_entry *br = cpuc->lbr_entries;\n\tstruct branch_entry entry;\n\tint out = 0, idx, i;\n\n\tif (!cpuc->lbr_users)\n\t\treturn;\n\n\tfor (i = 0; i < x86_pmu.lbr_nr; i++) {\n\t\tentry.from.full\t= amd_pmu_lbr_get_from(i);\n\t\tentry.to.full\t= amd_pmu_lbr_get_to(i);\n\n\t\t \n\t\tif (!entry.to.split.valid && !entry.to.split.spec)\n\t\t\tcontinue;\n\n\t\tperf_clear_branch_entry_bitfields(br + out);\n\n\t\tbr[out].from\t= sign_ext_branch_ip(entry.from.split.ip);\n\t\tbr[out].to\t= sign_ext_branch_ip(entry.to.split.ip);\n\t\tbr[out].mispred\t= entry.from.split.mispredict;\n\t\tbr[out].predicted = !br[out].mispred;\n\n\t\t \n\t\tidx = (entry.to.split.valid << 1) | entry.to.split.spec;\n\t\tbr[out].spec = lbr_spec_map[idx];\n\t\tout++;\n\t}\n\n\tcpuc->lbr_stack.nr = out;\n\n\t \n\tcpuc->lbr_stack.hw_idx = 0;\n\n\t \n\tamd_pmu_lbr_filter();\n}\n\nstatic const int lbr_select_map[PERF_SAMPLE_BRANCH_MAX_SHIFT] = {\n\t[PERF_SAMPLE_BRANCH_USER_SHIFT]\t\t= LBR_USER,\n\t[PERF_SAMPLE_BRANCH_KERNEL_SHIFT]\t= LBR_KERNEL,\n\t[PERF_SAMPLE_BRANCH_HV_SHIFT]\t\t= LBR_IGNORE,\n\n\t[PERF_SAMPLE_BRANCH_ANY_SHIFT]\t\t= LBR_ANY,\n\t[PERF_SAMPLE_BRANCH_ANY_CALL_SHIFT]\t= LBR_REL_CALL | LBR_IND_CALL | LBR_FAR,\n\t[PERF_SAMPLE_BRANCH_ANY_RETURN_SHIFT]\t= LBR_RETURN | LBR_FAR,\n\t[PERF_SAMPLE_BRANCH_IND_CALL_SHIFT]\t= LBR_IND_CALL,\n\t[PERF_SAMPLE_BRANCH_ABORT_TX_SHIFT]\t= LBR_NOT_SUPP,\n\t[PERF_SAMPLE_BRANCH_IN_TX_SHIFT]\t= LBR_NOT_SUPP,\n\t[PERF_SAMPLE_BRANCH_NO_TX_SHIFT]\t= LBR_NOT_SUPP,\n\t[PERF_SAMPLE_BRANCH_COND_SHIFT]\t\t= LBR_JCC,\n\n\t[PERF_SAMPLE_BRANCH_CALL_STACK_SHIFT]\t= LBR_NOT_SUPP,\n\t[PERF_SAMPLE_BRANCH_IND_JUMP_SHIFT]\t= LBR_IND_JMP,\n\t[PERF_SAMPLE_BRANCH_CALL_SHIFT]\t\t= LBR_REL_CALL,\n\n\t[PERF_SAMPLE_BRANCH_NO_FLAGS_SHIFT]\t= LBR_NOT_SUPP,\n\t[PERF_SAMPLE_BRANCH_NO_CYCLES_SHIFT]\t= LBR_NOT_SUPP,\n};\n\nstatic int amd_pmu_lbr_setup_filter(struct perf_event *event)\n{\n\tstruct hw_perf_event_extra *reg = &event->hw.branch_reg;\n\tu64 br_type = event->attr.branch_sample_type;\n\tu64 mask = 0, v;\n\tint i;\n\n\t \n\tif (!x86_pmu.lbr_nr)\n\t\treturn -EOPNOTSUPP;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_USER)\n\t\tmask |= X86_BR_USER;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_KERNEL)\n\t\tmask |= X86_BR_KERNEL;\n\n\t \n\n\tif (br_type & PERF_SAMPLE_BRANCH_ANY)\n\t\tmask |= X86_BR_ANY;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_ANY_CALL)\n\t\tmask |= X86_BR_ANY_CALL;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_ANY_RETURN)\n\t\tmask |= X86_BR_RET | X86_BR_IRET | X86_BR_SYSRET;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_IND_CALL)\n\t\tmask |= X86_BR_IND_CALL;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_COND)\n\t\tmask |= X86_BR_JCC;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_IND_JUMP)\n\t\tmask |= X86_BR_IND_JMP;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_CALL)\n\t\tmask |= X86_BR_CALL | X86_BR_ZERO_CALL;\n\n\tif (br_type & PERF_SAMPLE_BRANCH_TYPE_SAVE)\n\t\tmask |= X86_BR_TYPE_SAVE;\n\n\treg->reg = mask;\n\tmask = 0;\n\n\tfor (i = 0; i < PERF_SAMPLE_BRANCH_MAX_SHIFT; i++) {\n\t\tif (!(br_type & BIT_ULL(i)))\n\t\t\tcontinue;\n\n\t\tv = lbr_select_map[i];\n\t\tif (v == LBR_NOT_SUPP)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tif (v != LBR_IGNORE)\n\t\t\tmask |= v;\n\t}\n\n\t \n\treg->config = mask ^ LBR_SELECT_MASK;\n\n\treturn 0;\n}\n\nint amd_pmu_lbr_hw_config(struct perf_event *event)\n{\n\tint ret = 0;\n\n\t \n\tif (!is_sampling_event(event))\n\t\treturn -EINVAL;\n\n\tret = amd_pmu_lbr_setup_filter(event);\n\tif (!ret)\n\t\tevent->attach_state |= PERF_ATTACH_SCHED_CB;\n\n\treturn ret;\n}\n\nvoid amd_pmu_lbr_reset(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tint i;\n\n\tif (!x86_pmu.lbr_nr)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < x86_pmu.lbr_nr; i++) {\n\t\tamd_pmu_lbr_set_from(i, 0);\n\t\tamd_pmu_lbr_set_to(i, 0);\n\t}\n\n\tcpuc->last_task_ctx = NULL;\n\tcpuc->last_log_id = 0;\n\twrmsrl(MSR_AMD64_LBR_SELECT, 0);\n}\n\nvoid amd_pmu_lbr_add(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct hw_perf_event_extra *reg = &event->hw.branch_reg;\n\n\tif (!x86_pmu.lbr_nr)\n\t\treturn;\n\n\tif (has_branch_stack(event)) {\n\t\tcpuc->lbr_select = 1;\n\t\tcpuc->lbr_sel->config = reg->config;\n\t\tcpuc->br_sel = reg->reg;\n\t}\n\n\tperf_sched_cb_inc(event->pmu);\n\n\tif (!cpuc->lbr_users++ && !event->total_time_running)\n\t\tamd_pmu_lbr_reset();\n}\n\nvoid amd_pmu_lbr_del(struct perf_event *event)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tif (!x86_pmu.lbr_nr)\n\t\treturn;\n\n\tif (has_branch_stack(event))\n\t\tcpuc->lbr_select = 0;\n\n\tcpuc->lbr_users--;\n\tWARN_ON_ONCE(cpuc->lbr_users < 0);\n\tperf_sched_cb_dec(event->pmu);\n}\n\nvoid amd_pmu_lbr_sched_task(struct perf_event_pmu_context *pmu_ctx, bool sched_in)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\t \n\tif (cpuc->lbr_users && sched_in)\n\t\tamd_pmu_lbr_reset();\n}\n\nvoid amd_pmu_lbr_enable_all(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tu64 lbr_select, dbg_ctl, dbg_extn_cfg;\n\n\tif (!cpuc->lbr_users || !x86_pmu.lbr_nr)\n\t\treturn;\n\n\t \n\tif (cpuc->lbr_select) {\n\t\tlbr_select = cpuc->lbr_sel->config & LBR_SELECT_MASK;\n\t\twrmsrl(MSR_AMD64_LBR_SELECT, lbr_select);\n\t}\n\n\trdmsrl(MSR_IA32_DEBUGCTLMSR, dbg_ctl);\n\trdmsrl(MSR_AMD_DBG_EXTN_CFG, dbg_extn_cfg);\n\n\twrmsrl(MSR_IA32_DEBUGCTLMSR, dbg_ctl | DEBUGCTLMSR_FREEZE_LBRS_ON_PMI);\n\twrmsrl(MSR_AMD_DBG_EXTN_CFG, dbg_extn_cfg | DBG_EXTN_CFG_LBRV2EN);\n}\n\nvoid amd_pmu_lbr_disable_all(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tu64 dbg_ctl, dbg_extn_cfg;\n\n\tif (!cpuc->lbr_users || !x86_pmu.lbr_nr)\n\t\treturn;\n\n\trdmsrl(MSR_AMD_DBG_EXTN_CFG, dbg_extn_cfg);\n\trdmsrl(MSR_IA32_DEBUGCTLMSR, dbg_ctl);\n\n\twrmsrl(MSR_AMD_DBG_EXTN_CFG, dbg_extn_cfg & ~DBG_EXTN_CFG_LBRV2EN);\n\twrmsrl(MSR_IA32_DEBUGCTLMSR, dbg_ctl & ~DEBUGCTLMSR_FREEZE_LBRS_ON_PMI);\n}\n\n__init int amd_pmu_lbr_init(void)\n{\n\tunion cpuid_0x80000022_ebx ebx;\n\n\tif (x86_pmu.version < 2 || !boot_cpu_has(X86_FEATURE_AMD_LBR_V2))\n\t\treturn -EOPNOTSUPP;\n\n\t \n\tebx.full = cpuid_ebx(EXT_PERFMON_DEBUG_FEATURES);\n\tx86_pmu.lbr_nr = ebx.split.lbr_v2_stack_sz;\n\n\tpr_cont(\"%d-deep LBR, \", x86_pmu.lbr_nr);\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}