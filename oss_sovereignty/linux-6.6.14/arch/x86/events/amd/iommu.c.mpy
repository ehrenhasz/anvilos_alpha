{
  "module_name": "iommu.c",
  "hash_id": "1b898422c0ac4556b675f1a702a9503e02b80e4e0ad8455fc5be613bac267285",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/events/amd/iommu.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt)\t\"perf/amd_iommu: \" fmt\n\n#include <linux/perf_event.h>\n#include <linux/init.h>\n#include <linux/cpumask.h>\n#include <linux/slab.h>\n#include <linux/amd-iommu.h>\n\n#include \"../perf_event.h\"\n#include \"iommu.h\"\n\n \n#define GET_CSOURCE(x)     ((x)->conf & 0xFFULL)\n#define GET_DEVID(x)       (((x)->conf >> 8)  & 0xFFFFULL)\n#define GET_DOMID(x)       (((x)->conf >> 24) & 0xFFFFULL)\n#define GET_PASID(x)       (((x)->conf >> 40) & 0xFFFFFULL)\n\n \n#define GET_DEVID_MASK(x)  ((x)->conf1  & 0xFFFFULL)\n#define GET_DOMID_MASK(x)  (((x)->conf1 >> 16) & 0xFFFFULL)\n#define GET_PASID_MASK(x)  (((x)->conf1 >> 32) & 0xFFFFFULL)\n\n#define IOMMU_NAME_SIZE 16\n\nstruct perf_amd_iommu {\n\tstruct list_head list;\n\tstruct pmu pmu;\n\tstruct amd_iommu *iommu;\n\tchar name[IOMMU_NAME_SIZE];\n\tu8 max_banks;\n\tu8 max_counters;\n\tu64 cntr_assign_mask;\n\traw_spinlock_t lock;\n};\n\nstatic LIST_HEAD(perf_amd_iommu_list);\n\n \nPMU_FORMAT_ATTR(csource,    \"config:0-7\");\nPMU_FORMAT_ATTR(devid,      \"config:8-23\");\nPMU_FORMAT_ATTR(domid,      \"config:24-39\");\nPMU_FORMAT_ATTR(pasid,      \"config:40-59\");\nPMU_FORMAT_ATTR(devid_mask, \"config1:0-15\");\nPMU_FORMAT_ATTR(domid_mask, \"config1:16-31\");\nPMU_FORMAT_ATTR(pasid_mask, \"config1:32-51\");\n\nstatic struct attribute *iommu_format_attrs[] = {\n\t&format_attr_csource.attr,\n\t&format_attr_devid.attr,\n\t&format_attr_pasid.attr,\n\t&format_attr_domid.attr,\n\t&format_attr_devid_mask.attr,\n\t&format_attr_pasid_mask.attr,\n\t&format_attr_domid_mask.attr,\n\tNULL,\n};\n\nstatic struct attribute_group amd_iommu_format_group = {\n\t.name = \"format\",\n\t.attrs = iommu_format_attrs,\n};\n\n \nstatic struct attribute_group amd_iommu_events_group = {\n\t.name = \"events\",\n};\n\nstruct amd_iommu_event_desc {\n\tstruct device_attribute attr;\n\tconst char *event;\n};\n\nstatic ssize_t _iommu_event_show(struct device *dev,\n\t\t\t\tstruct device_attribute *attr, char *buf)\n{\n\tstruct amd_iommu_event_desc *event =\n\t\tcontainer_of(attr, struct amd_iommu_event_desc, attr);\n\treturn sprintf(buf, \"%s\\n\", event->event);\n}\n\n#define AMD_IOMMU_EVENT_DESC(_name, _event)\t\t\t\\\n{\t\t\t\t\t\t\t\t\\\n\t.attr  = __ATTR(_name, 0444, _iommu_event_show, NULL),\t\\\n\t.event = _event,\t\t\t\t\t\\\n}\n\nstatic struct amd_iommu_event_desc amd_iommu_v2_event_descs[] = {\n\tAMD_IOMMU_EVENT_DESC(mem_pass_untrans,        \"csource=0x01\"),\n\tAMD_IOMMU_EVENT_DESC(mem_pass_pretrans,       \"csource=0x02\"),\n\tAMD_IOMMU_EVENT_DESC(mem_pass_excl,           \"csource=0x03\"),\n\tAMD_IOMMU_EVENT_DESC(mem_target_abort,        \"csource=0x04\"),\n\tAMD_IOMMU_EVENT_DESC(mem_trans_total,         \"csource=0x05\"),\n\tAMD_IOMMU_EVENT_DESC(mem_iommu_tlb_pte_hit,   \"csource=0x06\"),\n\tAMD_IOMMU_EVENT_DESC(mem_iommu_tlb_pte_mis,   \"csource=0x07\"),\n\tAMD_IOMMU_EVENT_DESC(mem_iommu_tlb_pde_hit,   \"csource=0x08\"),\n\tAMD_IOMMU_EVENT_DESC(mem_iommu_tlb_pde_mis,   \"csource=0x09\"),\n\tAMD_IOMMU_EVENT_DESC(mem_dte_hit,             \"csource=0x0a\"),\n\tAMD_IOMMU_EVENT_DESC(mem_dte_mis,             \"csource=0x0b\"),\n\tAMD_IOMMU_EVENT_DESC(page_tbl_read_tot,       \"csource=0x0c\"),\n\tAMD_IOMMU_EVENT_DESC(page_tbl_read_nst,       \"csource=0x0d\"),\n\tAMD_IOMMU_EVENT_DESC(page_tbl_read_gst,       \"csource=0x0e\"),\n\tAMD_IOMMU_EVENT_DESC(int_dte_hit,             \"csource=0x0f\"),\n\tAMD_IOMMU_EVENT_DESC(int_dte_mis,             \"csource=0x10\"),\n\tAMD_IOMMU_EVENT_DESC(cmd_processed,           \"csource=0x11\"),\n\tAMD_IOMMU_EVENT_DESC(cmd_processed_inv,       \"csource=0x12\"),\n\tAMD_IOMMU_EVENT_DESC(tlb_inv,                 \"csource=0x13\"),\n\tAMD_IOMMU_EVENT_DESC(ign_rd_wr_mmio_1ff8h,    \"csource=0x14\"),\n\tAMD_IOMMU_EVENT_DESC(vapic_int_non_guest,     \"csource=0x15\"),\n\tAMD_IOMMU_EVENT_DESC(vapic_int_guest,         \"csource=0x16\"),\n\tAMD_IOMMU_EVENT_DESC(smi_recv,                \"csource=0x17\"),\n\tAMD_IOMMU_EVENT_DESC(smi_blk,                 \"csource=0x18\"),\n\t{   },\n};\n\n \nstatic cpumask_t iommu_cpumask;\n\nstatic ssize_t _iommu_cpumask_show(struct device *dev,\n\t\t\t\t   struct device_attribute *attr,\n\t\t\t\t   char *buf)\n{\n\treturn cpumap_print_to_pagebuf(true, buf, &iommu_cpumask);\n}\nstatic DEVICE_ATTR(cpumask, S_IRUGO, _iommu_cpumask_show, NULL);\n\nstatic struct attribute *iommu_cpumask_attrs[] = {\n\t&dev_attr_cpumask.attr,\n\tNULL,\n};\n\nstatic struct attribute_group amd_iommu_cpumask_group = {\n\t.attrs = iommu_cpumask_attrs,\n};\n\n \n\nstatic int get_next_avail_iommu_bnk_cntr(struct perf_event *event)\n{\n\tstruct perf_amd_iommu *piommu = container_of(event->pmu, struct perf_amd_iommu, pmu);\n\tint max_cntrs = piommu->max_counters;\n\tint max_banks = piommu->max_banks;\n\tu32 shift, bank, cntr;\n\tunsigned long flags;\n\tint retval;\n\n\traw_spin_lock_irqsave(&piommu->lock, flags);\n\n\tfor (bank = 0; bank < max_banks; bank++) {\n\t\tfor (cntr = 0; cntr < max_cntrs; cntr++) {\n\t\t\tshift = bank + (bank*3) + cntr;\n\t\t\tif (piommu->cntr_assign_mask & BIT_ULL(shift)) {\n\t\t\t\tcontinue;\n\t\t\t} else {\n\t\t\t\tpiommu->cntr_assign_mask |= BIT_ULL(shift);\n\t\t\t\tevent->hw.iommu_bank = bank;\n\t\t\t\tevent->hw.iommu_cntr = cntr;\n\t\t\t\tretval = 0;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\tretval = -ENOSPC;\nout:\n\traw_spin_unlock_irqrestore(&piommu->lock, flags);\n\treturn retval;\n}\n\nstatic int clear_avail_iommu_bnk_cntr(struct perf_amd_iommu *perf_iommu,\n\t\t\t\t\tu8 bank, u8 cntr)\n{\n\tunsigned long flags;\n\tint max_banks, max_cntrs;\n\tint shift = 0;\n\n\tmax_banks = perf_iommu->max_banks;\n\tmax_cntrs = perf_iommu->max_counters;\n\n\tif ((bank > max_banks) || (cntr > max_cntrs))\n\t\treturn -EINVAL;\n\n\tshift = bank + cntr + (bank*3);\n\n\traw_spin_lock_irqsave(&perf_iommu->lock, flags);\n\tperf_iommu->cntr_assign_mask &= ~(1ULL<<shift);\n\traw_spin_unlock_irqrestore(&perf_iommu->lock, flags);\n\n\treturn 0;\n}\n\nstatic int perf_iommu_event_init(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t \n\tif (event->attr.type != event->pmu->type)\n\t\treturn -ENOENT;\n\n\t \n\tif (is_sampling_event(event) || event->attach_state & PERF_ATTACH_TASK)\n\t\treturn -EINVAL;\n\n\tif (event->cpu < 0)\n\t\treturn -EINVAL;\n\n\t \n\thwc->conf  = event->attr.config;\n\thwc->conf1 = event->attr.config1;\n\n\treturn 0;\n}\n\nstatic inline struct amd_iommu *perf_event_2_iommu(struct perf_event *ev)\n{\n\treturn (container_of(ev->pmu, struct perf_amd_iommu, pmu))->iommu;\n}\n\nstatic void perf_iommu_enable_event(struct perf_event *ev)\n{\n\tstruct amd_iommu *iommu = perf_event_2_iommu(ev);\n\tstruct hw_perf_event *hwc = &ev->hw;\n\tu8 bank = hwc->iommu_bank;\n\tu8 cntr = hwc->iommu_cntr;\n\tu64 reg = 0ULL;\n\n\treg = GET_CSOURCE(hwc);\n\tamd_iommu_pc_set_reg(iommu, bank, cntr, IOMMU_PC_COUNTER_SRC_REG, &reg);\n\n\treg = GET_DEVID_MASK(hwc);\n\treg = GET_DEVID(hwc) | (reg << 32);\n\tif (reg)\n\t\treg |= BIT(31);\n\tamd_iommu_pc_set_reg(iommu, bank, cntr, IOMMU_PC_DEVID_MATCH_REG, &reg);\n\n\treg = GET_PASID_MASK(hwc);\n\treg = GET_PASID(hwc) | (reg << 32);\n\tif (reg)\n\t\treg |= BIT(31);\n\tamd_iommu_pc_set_reg(iommu, bank, cntr, IOMMU_PC_PASID_MATCH_REG, &reg);\n\n\treg = GET_DOMID_MASK(hwc);\n\treg = GET_DOMID(hwc) | (reg << 32);\n\tif (reg)\n\t\treg |= BIT(31);\n\tamd_iommu_pc_set_reg(iommu, bank, cntr, IOMMU_PC_DOMID_MATCH_REG, &reg);\n}\n\nstatic void perf_iommu_disable_event(struct perf_event *event)\n{\n\tstruct amd_iommu *iommu = perf_event_2_iommu(event);\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 reg = 0ULL;\n\n\tamd_iommu_pc_set_reg(iommu, hwc->iommu_bank, hwc->iommu_cntr,\n\t\t\t     IOMMU_PC_COUNTER_SRC_REG, &reg);\n}\n\nstatic void perf_iommu_start(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (WARN_ON_ONCE(!(hwc->state & PERF_HES_STOPPED)))\n\t\treturn;\n\n\tWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\n\thwc->state = 0;\n\n\t \n\tperf_iommu_enable_event(event);\n\n\tif (flags & PERF_EF_RELOAD) {\n\t\tu64 count = 0;\n\t\tstruct amd_iommu *iommu = perf_event_2_iommu(event);\n\n\t\t \n\t\tamd_iommu_pc_set_reg(iommu, hwc->iommu_bank, hwc->iommu_cntr,\n\t\t\t\t     IOMMU_PC_COUNTER_REG, &count);\n\t}\n\n\tperf_event_update_userpage(event);\n}\n\nstatic void perf_iommu_read(struct perf_event *event)\n{\n\tu64 count;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct amd_iommu *iommu = perf_event_2_iommu(event);\n\n\tif (amd_iommu_pc_get_reg(iommu, hwc->iommu_bank, hwc->iommu_cntr,\n\t\t\t\t IOMMU_PC_COUNTER_REG, &count))\n\t\treturn;\n\n\t \n\tcount &= GENMASK_ULL(47, 0);\n\n\t \n\tlocal64_add(count, &event->count);\n}\n\nstatic void perf_iommu_stop(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (hwc->state & PERF_HES_UPTODATE)\n\t\treturn;\n\n\t \n\tperf_iommu_read(event);\n\thwc->state |= PERF_HES_UPTODATE;\n\n\tperf_iommu_disable_event(event);\n\tWARN_ON_ONCE(hwc->state & PERF_HES_STOPPED);\n\thwc->state |= PERF_HES_STOPPED;\n}\n\nstatic int perf_iommu_add(struct perf_event *event, int flags)\n{\n\tint retval;\n\n\tevent->hw.state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\n\n\t \n\tretval = get_next_avail_iommu_bnk_cntr(event);\n\tif (retval)\n\t\treturn retval;\n\n\tif (flags & PERF_EF_START)\n\t\tperf_iommu_start(event, PERF_EF_RELOAD);\n\n\treturn 0;\n}\n\nstatic void perf_iommu_del(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct perf_amd_iommu *perf_iommu =\n\t\t\tcontainer_of(event->pmu, struct perf_amd_iommu, pmu);\n\n\tperf_iommu_stop(event, PERF_EF_UPDATE);\n\n\t \n\tclear_avail_iommu_bnk_cntr(perf_iommu,\n\t\t\t\t   hwc->iommu_bank, hwc->iommu_cntr);\n\n\tperf_event_update_userpage(event);\n}\n\nstatic __init int _init_events_attrs(void)\n{\n\tint i = 0, j;\n\tstruct attribute **attrs;\n\n\twhile (amd_iommu_v2_event_descs[i].attr.attr.name)\n\t\ti++;\n\n\tattrs = kcalloc(i + 1, sizeof(*attrs), GFP_KERNEL);\n\tif (!attrs)\n\t\treturn -ENOMEM;\n\n\tfor (j = 0; j < i; j++)\n\t\tattrs[j] = &amd_iommu_v2_event_descs[j].attr.attr;\n\n\tamd_iommu_events_group.attrs = attrs;\n\treturn 0;\n}\n\nstatic const struct attribute_group *amd_iommu_attr_groups[] = {\n\t&amd_iommu_format_group,\n\t&amd_iommu_cpumask_group,\n\t&amd_iommu_events_group,\n\tNULL,\n};\n\nstatic const struct pmu iommu_pmu __initconst = {\n\t.event_init\t= perf_iommu_event_init,\n\t.add\t\t= perf_iommu_add,\n\t.del\t\t= perf_iommu_del,\n\t.start\t\t= perf_iommu_start,\n\t.stop\t\t= perf_iommu_stop,\n\t.read\t\t= perf_iommu_read,\n\t.task_ctx_nr\t= perf_invalid_context,\n\t.attr_groups\t= amd_iommu_attr_groups,\n\t.capabilities\t= PERF_PMU_CAP_NO_EXCLUDE,\n};\n\nstatic __init int init_one_iommu(unsigned int idx)\n{\n\tstruct perf_amd_iommu *perf_iommu;\n\tint ret;\n\n\tperf_iommu = kzalloc(sizeof(struct perf_amd_iommu), GFP_KERNEL);\n\tif (!perf_iommu)\n\t\treturn -ENOMEM;\n\n\traw_spin_lock_init(&perf_iommu->lock);\n\n\tperf_iommu->pmu          = iommu_pmu;\n\tperf_iommu->iommu        = get_amd_iommu(idx);\n\tperf_iommu->max_banks    = amd_iommu_pc_get_max_banks(idx);\n\tperf_iommu->max_counters = amd_iommu_pc_get_max_counters(idx);\n\n\tif (!perf_iommu->iommu ||\n\t    !perf_iommu->max_banks ||\n\t    !perf_iommu->max_counters) {\n\t\tkfree(perf_iommu);\n\t\treturn -EINVAL;\n\t}\n\n\tsnprintf(perf_iommu->name, IOMMU_NAME_SIZE, \"amd_iommu_%u\", idx);\n\n\tret = perf_pmu_register(&perf_iommu->pmu, perf_iommu->name, -1);\n\tif (!ret) {\n\t\tpr_info(\"Detected AMD IOMMU #%d (%d banks, %d counters/bank).\\n\",\n\t\t\tidx, perf_iommu->max_banks, perf_iommu->max_counters);\n\t\tlist_add_tail(&perf_iommu->list, &perf_amd_iommu_list);\n\t} else {\n\t\tpr_warn(\"Error initializing IOMMU %d.\\n\", idx);\n\t\tkfree(perf_iommu);\n\t}\n\treturn ret;\n}\n\nstatic __init int amd_iommu_pc_init(void)\n{\n\tunsigned int i, cnt = 0;\n\tint ret;\n\n\t \n\tif (!amd_iommu_pc_supported())\n\t\treturn -ENODEV;\n\n\tret = _init_events_attrs();\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tfor (i = 0; i < amd_iommu_get_num_iommus(); i++) {\n\t\tret = init_one_iommu(i);\n\t\tif (!ret)\n\t\t\tcnt++;\n\t}\n\n\tif (!cnt) {\n\t\tkfree(amd_iommu_events_group.attrs);\n\t\treturn -ENODEV;\n\t}\n\n\t \n\tcpumask_set_cpu(0, &iommu_cpumask);\n\treturn 0;\n}\n\ndevice_initcall(amd_iommu_pc_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}