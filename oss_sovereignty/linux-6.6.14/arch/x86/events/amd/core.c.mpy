{
  "module_name": "core.c",
  "hash_id": "3755180c971711f0ce673b26d2abed91d5ccd83d6d9a4b1730195a6d3f4bd88f",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/events/amd/core.c",
  "human_readable_source": "\n#include <linux/perf_event.h>\n#include <linux/jump_label.h>\n#include <linux/export.h>\n#include <linux/types.h>\n#include <linux/init.h>\n#include <linux/slab.h>\n#include <linux/delay.h>\n#include <linux/jiffies.h>\n#include <asm/apicdef.h>\n#include <asm/apic.h>\n#include <asm/nmi.h>\n\n#include \"../perf_event.h\"\n\nstatic DEFINE_PER_CPU(unsigned long, perf_nmi_tstamp);\nstatic unsigned long perf_nmi_window;\n\n \n#define AMD_MERGE_EVENT ((0xFULL << 32) | 0xFFULL)\n#define AMD_MERGE_EVENT_ENABLE (AMD_MERGE_EVENT | ARCH_PERFMON_EVENTSEL_ENABLE)\n\n \nstatic u64 amd_pmu_global_cntr_mask __read_mostly;\n\nstatic __initconst const u64 amd_hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] =\n{\n [ C(L1D) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0040,  \n\t\t[ C(RESULT_MISS)   ] = 0x0141,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0267,  \n\t\t[ C(RESULT_MISS)   ] = 0x0167,  \n\t},\n },\n [ C(L1I ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0080,  \n\t\t[ C(RESULT_MISS)   ] = 0x0081,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x014B,  \n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(LL  ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x037D,  \n\t\t[ C(RESULT_MISS)   ] = 0x037E,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x017F,  \n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(DTLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0040,  \n\t\t[ C(RESULT_MISS)   ] = 0x0746,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0,\n\t\t[ C(RESULT_MISS)   ] = 0,\n\t},\n },\n [ C(ITLB) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x0080,  \n\t\t[ C(RESULT_MISS)   ] = 0x0385,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(BPU ) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0x00c2,  \n\t\t[ C(RESULT_MISS)   ] = 0x00c3,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n [ C(NODE) ] = {\n\t[ C(OP_READ) ] = {\n\t\t[ C(RESULT_ACCESS) ] = 0xb8e9,  \n\t\t[ C(RESULT_MISS)   ] = 0x98e9,  \n\t},\n\t[ C(OP_WRITE) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n\t[ C(OP_PREFETCH) ] = {\n\t\t[ C(RESULT_ACCESS) ] = -1,\n\t\t[ C(RESULT_MISS)   ] = -1,\n\t},\n },\n};\n\nstatic __initconst const u64 amd_hw_cache_event_ids_f17h\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX] = {\n[C(L1D)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = 0x0040,  \n\t\t[C(RESULT_MISS)]   = 0xc860,  \n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = 0,\n\t\t[C(RESULT_MISS)]   = 0,\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = 0xff5a,  \n\t\t[C(RESULT_MISS)]   = 0,\n\t},\n},\n[C(L1I)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = 0x0080,  \n\t\t[C(RESULT_MISS)]   = 0x0081,  \n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = -1,\n\t\t[C(RESULT_MISS)]   = -1,\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = 0,\n\t\t[C(RESULT_MISS)]   = 0,\n\t},\n},\n[C(LL)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = 0,\n\t\t[C(RESULT_MISS)]   = 0,\n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = 0,\n\t\t[C(RESULT_MISS)]   = 0,\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = 0,\n\t\t[C(RESULT_MISS)]   = 0,\n\t},\n},\n[C(DTLB)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = 0xff45,  \n\t\t[C(RESULT_MISS)]   = 0xf045,  \n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = 0,\n\t\t[C(RESULT_MISS)]   = 0,\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = 0,\n\t\t[C(RESULT_MISS)]   = 0,\n\t},\n},\n[C(ITLB)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = 0x0084,  \n\t\t[C(RESULT_MISS)]   = 0xff85,  \n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = -1,\n\t\t[C(RESULT_MISS)]   = -1,\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = -1,\n\t\t[C(RESULT_MISS)]   = -1,\n\t},\n},\n[C(BPU)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = 0x00c2,  \n\t\t[C(RESULT_MISS)]   = 0x00c3,  \n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = -1,\n\t\t[C(RESULT_MISS)]   = -1,\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = -1,\n\t\t[C(RESULT_MISS)]   = -1,\n\t},\n},\n[C(NODE)] = {\n\t[C(OP_READ)] = {\n\t\t[C(RESULT_ACCESS)] = 0,\n\t\t[C(RESULT_MISS)]   = 0,\n\t},\n\t[C(OP_WRITE)] = {\n\t\t[C(RESULT_ACCESS)] = -1,\n\t\t[C(RESULT_MISS)]   = -1,\n\t},\n\t[C(OP_PREFETCH)] = {\n\t\t[C(RESULT_ACCESS)] = -1,\n\t\t[C(RESULT_MISS)]   = -1,\n\t},\n},\n};\n\n \nstatic const u64 amd_perfmon_event_map[PERF_COUNT_HW_MAX] =\n{\n\t[PERF_COUNT_HW_CPU_CYCLES]\t\t= 0x0076,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t\t= 0x00c0,\n\t[PERF_COUNT_HW_CACHE_REFERENCES]\t= 0x077d,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t\t= 0x077e,\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS]\t= 0x00c2,\n\t[PERF_COUNT_HW_BRANCH_MISSES]\t\t= 0x00c3,\n\t[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND]\t= 0x00d0,  \n\t[PERF_COUNT_HW_STALLED_CYCLES_BACKEND]\t= 0x00d1,  \n};\n\n \nstatic const u64 amd_f17h_perfmon_event_map[PERF_COUNT_HW_MAX] =\n{\n\t[PERF_COUNT_HW_CPU_CYCLES]\t\t= 0x0076,\n\t[PERF_COUNT_HW_INSTRUCTIONS]\t\t= 0x00c0,\n\t[PERF_COUNT_HW_CACHE_REFERENCES]\t= 0xff60,\n\t[PERF_COUNT_HW_CACHE_MISSES]\t\t= 0x0964,\n\t[PERF_COUNT_HW_BRANCH_INSTRUCTIONS]\t= 0x00c2,\n\t[PERF_COUNT_HW_BRANCH_MISSES]\t\t= 0x00c3,\n\t[PERF_COUNT_HW_STALLED_CYCLES_FRONTEND]\t= 0x0287,\n\t[PERF_COUNT_HW_STALLED_CYCLES_BACKEND]\t= 0x0187,\n};\n\nstatic u64 amd_pmu_event_map(int hw_event)\n{\n\tif (boot_cpu_data.x86 >= 0x17)\n\t\treturn amd_f17h_perfmon_event_map[hw_event];\n\n\treturn amd_perfmon_event_map[hw_event];\n}\n\n \nstatic unsigned int event_offsets[X86_PMC_IDX_MAX] __read_mostly;\nstatic unsigned int count_offsets[X86_PMC_IDX_MAX] __read_mostly;\n\n \nstatic inline int amd_pmu_addr_offset(int index, bool eventsel)\n{\n\tint offset;\n\n\tif (!index)\n\t\treturn index;\n\n\tif (eventsel)\n\t\toffset = event_offsets[index];\n\telse\n\t\toffset = count_offsets[index];\n\n\tif (offset)\n\t\treturn offset;\n\n\tif (!boot_cpu_has(X86_FEATURE_PERFCTR_CORE))\n\t\toffset = index;\n\telse\n\t\toffset = index << 1;\n\n\tif (eventsel)\n\t\tevent_offsets[index] = offset;\n\telse\n\t\tcount_offsets[index] = offset;\n\n\treturn offset;\n}\n\n \nstatic inline unsigned int amd_get_event_code(struct hw_perf_event *hwc)\n{\n\treturn ((hwc->config >> 24) & 0x0f00) | (hwc->config & 0x00ff);\n}\n\nstatic inline bool amd_is_pair_event_code(struct hw_perf_event *hwc)\n{\n\tif (!(x86_pmu.flags & PMU_FL_PAIR))\n\t\treturn false;\n\n\tswitch (amd_get_event_code(hwc)) {\n\tcase 0x003:\treturn true;\t \n\tdefault:\treturn false;\n\t}\n}\n\nDEFINE_STATIC_CALL_RET0(amd_pmu_branch_hw_config, *x86_pmu.hw_config);\n\nstatic int amd_core_hw_config(struct perf_event *event)\n{\n\tif (event->attr.exclude_host && event->attr.exclude_guest)\n\t\t \n\t\tevent->hw.config &= ~(ARCH_PERFMON_EVENTSEL_USR |\n\t\t\t\t      ARCH_PERFMON_EVENTSEL_OS);\n\telse if (event->attr.exclude_host)\n\t\tevent->hw.config |= AMD64_EVENTSEL_GUESTONLY;\n\telse if (event->attr.exclude_guest)\n\t\tevent->hw.config |= AMD64_EVENTSEL_HOSTONLY;\n\n\tif ((x86_pmu.flags & PMU_FL_PAIR) && amd_is_pair_event_code(&event->hw))\n\t\tevent->hw.flags |= PERF_X86_EVENT_PAIR;\n\n\tif (has_branch_stack(event))\n\t\treturn static_call(amd_pmu_branch_hw_config)(event);\n\n\treturn 0;\n}\n\nstatic inline int amd_is_nb_event(struct hw_perf_event *hwc)\n{\n\treturn (hwc->config & 0xe0) == 0xe0;\n}\n\nstatic inline int amd_has_nb(struct cpu_hw_events *cpuc)\n{\n\tstruct amd_nb *nb = cpuc->amd_nb;\n\n\treturn nb && nb->nb_id != -1;\n}\n\nstatic int amd_pmu_hw_config(struct perf_event *event)\n{\n\tint ret;\n\n\t \n\tif (event->attr.precise_ip && get_ibs_caps())\n\t\treturn forward_event_to_ibs(event);\n\n\tif (has_branch_stack(event) && !x86_pmu.lbr_nr)\n\t\treturn -EOPNOTSUPP;\n\n\tret = x86_pmu_hw_config(event);\n\tif (ret)\n\t\treturn ret;\n\n\tif (event->attr.type == PERF_TYPE_RAW)\n\t\tevent->hw.config |= event->attr.config & AMD64_RAW_EVENT_MASK;\n\n\treturn amd_core_hw_config(event);\n}\n\nstatic void __amd_put_nb_event_constraints(struct cpu_hw_events *cpuc,\n\t\t\t\t\t   struct perf_event *event)\n{\n\tstruct amd_nb *nb = cpuc->amd_nb;\n\tint i;\n\n\t \n\tfor (i = 0; i < x86_pmu.num_counters; i++) {\n\t\tif (cmpxchg(nb->owners + i, event, NULL) == event)\n\t\t\tbreak;\n\t}\n}\n\n  \nstatic struct event_constraint *\n__amd_get_nb_event_constraints(struct cpu_hw_events *cpuc, struct perf_event *event,\n\t\t\t       struct event_constraint *c)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct amd_nb *nb = cpuc->amd_nb;\n\tstruct perf_event *old;\n\tint idx, new = -1;\n\n\tif (!c)\n\t\tc = &unconstrained;\n\n\tif (cpuc->is_fake)\n\t\treturn c;\n\n\t \n\tfor_each_set_bit(idx, c->idxmsk, x86_pmu.num_counters) {\n\t\tif (new == -1 || hwc->idx == idx)\n\t\t\t \n\t\t\told = cmpxchg(nb->owners + idx, NULL, event);\n\t\telse if (nb->owners[idx] == event)\n\t\t\t \n\t\t\told = event;\n\t\telse\n\t\t\tcontinue;\n\n\t\tif (old && old != event)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (new != -1)\n\t\t\tcmpxchg(nb->owners + new, event, NULL);\n\t\tnew = idx;\n\n\t\t \n\t\tif (old == event)\n\t\t\tbreak;\n\t}\n\n\tif (new == -1)\n\t\treturn &emptyconstraint;\n\n\treturn &nb->event_constraints[new];\n}\n\nstatic struct amd_nb *amd_alloc_nb(int cpu)\n{\n\tstruct amd_nb *nb;\n\tint i;\n\n\tnb = kzalloc_node(sizeof(struct amd_nb), GFP_KERNEL, cpu_to_node(cpu));\n\tif (!nb)\n\t\treturn NULL;\n\n\tnb->nb_id = -1;\n\n\t \n\tfor (i = 0; i < x86_pmu.num_counters; i++) {\n\t\t__set_bit(i, nb->event_constraints[i].idxmsk);\n\t\tnb->event_constraints[i].weight = 1;\n\t}\n\treturn nb;\n}\n\ntypedef void (amd_pmu_branch_reset_t)(void);\nDEFINE_STATIC_CALL_NULL(amd_pmu_branch_reset, amd_pmu_branch_reset_t);\n\nstatic void amd_pmu_cpu_reset(int cpu)\n{\n\tif (x86_pmu.lbr_nr)\n\t\tstatic_call(amd_pmu_branch_reset)();\n\n\tif (x86_pmu.version < 2)\n\t\treturn;\n\n\t \n\twrmsrl(MSR_AMD64_PERF_CNTR_GLOBAL_CTL, 0);\n\n\t \n\twrmsrl(MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR,\n\t       GLOBAL_STATUS_LBRS_FROZEN | amd_pmu_global_cntr_mask);\n}\n\nstatic int amd_pmu_cpu_prepare(int cpu)\n{\n\tstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\n\n\tcpuc->lbr_sel = kzalloc_node(sizeof(struct er_account), GFP_KERNEL,\n\t\t\t\t     cpu_to_node(cpu));\n\tif (!cpuc->lbr_sel)\n\t\treturn -ENOMEM;\n\n\tWARN_ON_ONCE(cpuc->amd_nb);\n\n\tif (!x86_pmu.amd_nb_constraints)\n\t\treturn 0;\n\n\tcpuc->amd_nb = amd_alloc_nb(cpu);\n\tif (cpuc->amd_nb)\n\t\treturn 0;\n\n\tkfree(cpuc->lbr_sel);\n\tcpuc->lbr_sel = NULL;\n\n\treturn -ENOMEM;\n}\n\nstatic void amd_pmu_cpu_starting(int cpu)\n{\n\tstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\n\tvoid **onln = &cpuc->kfree_on_online[X86_PERF_KFREE_SHARED];\n\tstruct amd_nb *nb;\n\tint i, nb_id;\n\n\tcpuc->perf_ctr_virt_mask = AMD64_EVENTSEL_HOSTONLY;\n\tamd_pmu_cpu_reset(cpu);\n\n\tif (!x86_pmu.amd_nb_constraints)\n\t\treturn;\n\n\tnb_id = topology_die_id(cpu);\n\tWARN_ON_ONCE(nb_id == BAD_APICID);\n\n\tfor_each_online_cpu(i) {\n\t\tnb = per_cpu(cpu_hw_events, i).amd_nb;\n\t\tif (WARN_ON_ONCE(!nb))\n\t\t\tcontinue;\n\n\t\tif (nb->nb_id == nb_id) {\n\t\t\t*onln = cpuc->amd_nb;\n\t\t\tcpuc->amd_nb = nb;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tcpuc->amd_nb->nb_id = nb_id;\n\tcpuc->amd_nb->refcnt++;\n}\n\nstatic void amd_pmu_cpu_dead(int cpu)\n{\n\tstruct cpu_hw_events *cpuhw = &per_cpu(cpu_hw_events, cpu);\n\n\tkfree(cpuhw->lbr_sel);\n\tcpuhw->lbr_sel = NULL;\n\tamd_pmu_cpu_reset(cpu);\n\n\tif (!x86_pmu.amd_nb_constraints)\n\t\treturn;\n\n\tif (cpuhw->amd_nb) {\n\t\tstruct amd_nb *nb = cpuhw->amd_nb;\n\n\t\tif (nb->nb_id == -1 || --nb->refcnt == 0)\n\t\t\tkfree(nb);\n\n\t\tcpuhw->amd_nb = NULL;\n\t}\n}\n\nstatic inline void amd_pmu_set_global_ctl(u64 ctl)\n{\n\twrmsrl(MSR_AMD64_PERF_CNTR_GLOBAL_CTL, ctl);\n}\n\nstatic inline u64 amd_pmu_get_global_status(void)\n{\n\tu64 status;\n\n\t \n\trdmsrl(MSR_AMD64_PERF_CNTR_GLOBAL_STATUS, status);\n\n\treturn status;\n}\n\nstatic inline void amd_pmu_ack_global_status(u64 status)\n{\n\t \n\n\twrmsrl(MSR_AMD64_PERF_CNTR_GLOBAL_STATUS_CLR, status);\n}\n\nstatic bool amd_pmu_test_overflow_topbit(int idx)\n{\n\tu64 counter;\n\n\trdmsrl(x86_pmu_event_addr(idx), counter);\n\n\treturn !(counter & BIT_ULL(x86_pmu.cntval_bits - 1));\n}\n\nstatic bool amd_pmu_test_overflow_status(int idx)\n{\n\treturn amd_pmu_get_global_status() & BIT_ULL(idx);\n}\n\nDEFINE_STATIC_CALL(amd_pmu_test_overflow, amd_pmu_test_overflow_topbit);\n\n \n#define OVERFLOW_WAIT_COUNT\t50\n\nstatic void amd_pmu_wait_on_overflow(int idx)\n{\n\tunsigned int i;\n\n\t \n\tfor (i = 0; i < OVERFLOW_WAIT_COUNT; i++) {\n\t\tif (!static_call(amd_pmu_test_overflow)(idx))\n\t\t\tbreak;\n\n\t\t \n\t\tudelay(1);\n\t}\n}\n\nstatic void amd_pmu_check_overflow(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tint idx;\n\n\t \n\tif (in_nmi())\n\t\treturn;\n\n\t \n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\tamd_pmu_wait_on_overflow(idx);\n\t}\n}\n\nstatic void amd_pmu_enable_event(struct perf_event *event)\n{\n\tx86_pmu_enable_event(event);\n}\n\nstatic void amd_pmu_enable_all(int added)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tint idx;\n\n\tamd_brs_enable_all();\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\t \n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\tamd_pmu_enable_event(cpuc->events[idx]);\n\t}\n}\n\nstatic void amd_pmu_v2_enable_event(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t \n\t__x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);\n}\n\nstatic __always_inline void amd_pmu_core_enable_all(void)\n{\n\tamd_pmu_set_global_ctl(amd_pmu_global_cntr_mask);\n}\n\nstatic void amd_pmu_v2_enable_all(int added)\n{\n\tamd_pmu_lbr_enable_all();\n\tamd_pmu_core_enable_all();\n}\n\nstatic void amd_pmu_disable_event(struct perf_event *event)\n{\n\tx86_pmu_disable_event(event);\n\n\t \n\tif (in_nmi())\n\t\treturn;\n\n\tamd_pmu_wait_on_overflow(event->hw.idx);\n}\n\nstatic void amd_pmu_disable_all(void)\n{\n\tamd_brs_disable_all();\n\tx86_pmu_disable_all();\n\tamd_pmu_check_overflow();\n}\n\nstatic __always_inline void amd_pmu_core_disable_all(void)\n{\n\tamd_pmu_set_global_ctl(0);\n}\n\nstatic void amd_pmu_v2_disable_all(void)\n{\n\tamd_pmu_core_disable_all();\n\tamd_pmu_lbr_disable_all();\n\tamd_pmu_check_overflow();\n}\n\nDEFINE_STATIC_CALL_NULL(amd_pmu_branch_add, *x86_pmu.add);\n\nstatic void amd_pmu_add_event(struct perf_event *event)\n{\n\tif (needs_branch_stack(event))\n\t\tstatic_call(amd_pmu_branch_add)(event);\n}\n\nDEFINE_STATIC_CALL_NULL(amd_pmu_branch_del, *x86_pmu.del);\n\nstatic void amd_pmu_del_event(struct perf_event *event)\n{\n\tif (needs_branch_stack(event))\n\t\tstatic_call(amd_pmu_branch_del)(event);\n}\n\n \nstatic inline int amd_pmu_adjust_nmi_window(int handled)\n{\n\t \n\tif (handled) {\n\t\tthis_cpu_write(perf_nmi_tstamp, jiffies + perf_nmi_window);\n\n\t\treturn handled;\n\t}\n\n\tif (time_after(jiffies, this_cpu_read(perf_nmi_tstamp)))\n\t\treturn NMI_DONE;\n\n\treturn NMI_HANDLED;\n}\n\nstatic int amd_pmu_handle_irq(struct pt_regs *regs)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tint handled;\n\tint pmu_enabled;\n\n\t \n\tpmu_enabled = cpuc->enabled;\n\tcpuc->enabled = 0;\n\n\tamd_brs_disable_all();\n\n\t \n\tif (cpuc->lbr_users)\n\t\tamd_brs_drain();\n\n\t \n\thandled = x86_pmu_handle_irq(regs);\n\n\tcpuc->enabled = pmu_enabled;\n\tif (pmu_enabled)\n\t\tamd_brs_enable_all();\n\n\treturn amd_pmu_adjust_nmi_window(handled);\n}\n\nstatic int amd_pmu_v2_handle_irq(struct pt_regs *regs)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct perf_sample_data data;\n\tstruct hw_perf_event *hwc;\n\tstruct perf_event *event;\n\tint handled = 0, idx;\n\tu64 reserved, status, mask;\n\tbool pmu_enabled;\n\n\t \n\tpmu_enabled = cpuc->enabled;\n\tcpuc->enabled = 0;\n\n\t \n\tamd_pmu_core_disable_all();\n\n\tstatus = amd_pmu_get_global_status();\n\n\t \n\tif (!status)\n\t\tgoto done;\n\n\t \n\tif (status & GLOBAL_STATUS_LBRS_FROZEN) {\n\t\tamd_pmu_lbr_read();\n\t\tstatus &= ~GLOBAL_STATUS_LBRS_FROZEN;\n\t}\n\n\treserved = status & ~amd_pmu_global_cntr_mask;\n\tif (reserved)\n\t\tpr_warn_once(\"Reserved PerfCntrGlobalStatus bits are set (0x%llx), please consider updating microcode\\n\",\n\t\t\t     reserved);\n\n\t \n\tstatus &= amd_pmu_global_cntr_mask;\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\tevent = cpuc->events[idx];\n\t\thwc = &event->hw;\n\t\tx86_perf_event_update(event);\n\t\tmask = BIT_ULL(idx);\n\n\t\tif (!(status & mask))\n\t\t\tcontinue;\n\n\t\t \n\t\thandled++;\n\t\tstatus &= ~mask;\n\t\tperf_sample_data_init(&data, 0, hwc->last_period);\n\n\t\tif (!x86_perf_event_set_period(event))\n\t\t\tcontinue;\n\n\t\tif (has_branch_stack(event))\n\t\t\tperf_sample_save_brstack(&data, event, &cpuc->lbr_stack);\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tx86_pmu_stop(event, 0);\n\t}\n\n\t \n\tWARN_ON(status > 0);\n\n\t \n\tamd_pmu_ack_global_status(~status);\n\n\t \n\tinc_irq_stat(apic_perf_irqs);\n\ndone:\n\tcpuc->enabled = pmu_enabled;\n\n\t \n\tif (pmu_enabled)\n\t\tamd_pmu_core_enable_all();\n\n\treturn amd_pmu_adjust_nmi_window(handled);\n}\n\nstatic struct event_constraint *\namd_get_event_constraints(struct cpu_hw_events *cpuc, int idx,\n\t\t\t  struct perf_event *event)\n{\n\t \n\tif (!(amd_has_nb(cpuc) && amd_is_nb_event(&event->hw)))\n\t\treturn &unconstrained;\n\n\treturn __amd_get_nb_event_constraints(cpuc, event, NULL);\n}\n\nstatic void amd_put_event_constraints(struct cpu_hw_events *cpuc,\n\t\t\t\t      struct perf_event *event)\n{\n\tif (amd_has_nb(cpuc) && amd_is_nb_event(&event->hw))\n\t\t__amd_put_nb_event_constraints(cpuc, event);\n}\n\nPMU_FORMAT_ATTR(event,\t\"config:0-7,32-35\");\nPMU_FORMAT_ATTR(umask,\t\"config:8-15\"\t);\nPMU_FORMAT_ATTR(edge,\t\"config:18\"\t);\nPMU_FORMAT_ATTR(inv,\t\"config:23\"\t);\nPMU_FORMAT_ATTR(cmask,\t\"config:24-31\"\t);\n\nstatic struct attribute *amd_format_attr[] = {\n\t&format_attr_event.attr,\n\t&format_attr_umask.attr,\n\t&format_attr_edge.attr,\n\t&format_attr_inv.attr,\n\t&format_attr_cmask.attr,\n\tNULL,\n};\n\n \n\n#define AMD_EVENT_TYPE_MASK\t0x000000F0ULL\n\n#define AMD_EVENT_FP\t\t0x00000000ULL ... 0x00000010ULL\n#define AMD_EVENT_LS\t\t0x00000020ULL ... 0x00000030ULL\n#define AMD_EVENT_DC\t\t0x00000040ULL ... 0x00000050ULL\n#define AMD_EVENT_CU\t\t0x00000060ULL ... 0x00000070ULL\n#define AMD_EVENT_IC_DE\t\t0x00000080ULL ... 0x00000090ULL\n#define AMD_EVENT_EX_LS\t\t0x000000C0ULL\n#define AMD_EVENT_DE\t\t0x000000D0ULL\n#define AMD_EVENT_NB\t\t0x000000E0ULL ... 0x000000F0ULL\n\n \n\nstatic struct event_constraint amd_f15_PMC0  = EVENT_CONSTRAINT(0, 0x01, 0);\nstatic struct event_constraint amd_f15_PMC20 = EVENT_CONSTRAINT(0, 0x07, 0);\nstatic struct event_constraint amd_f15_PMC3  = EVENT_CONSTRAINT(0, 0x08, 0);\nstatic struct event_constraint amd_f15_PMC30 = EVENT_CONSTRAINT_OVERLAP(0, 0x09, 0);\nstatic struct event_constraint amd_f15_PMC50 = EVENT_CONSTRAINT(0, 0x3F, 0);\nstatic struct event_constraint amd_f15_PMC53 = EVENT_CONSTRAINT(0, 0x38, 0);\n\nstatic struct event_constraint *\namd_get_event_constraints_f15h(struct cpu_hw_events *cpuc, int idx,\n\t\t\t       struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tunsigned int event_code = amd_get_event_code(hwc);\n\n\tswitch (event_code & AMD_EVENT_TYPE_MASK) {\n\tcase AMD_EVENT_FP:\n\t\tswitch (event_code) {\n\t\tcase 0x000:\n\t\t\tif (!(hwc->config & 0x0000F000ULL))\n\t\t\t\tbreak;\n\t\t\tif (!(hwc->config & 0x00000F00ULL))\n\t\t\t\tbreak;\n\t\t\treturn &amd_f15_PMC3;\n\t\tcase 0x004:\n\t\t\tif (hweight_long(hwc->config & ARCH_PERFMON_EVENTSEL_UMASK) <= 1)\n\t\t\t\tbreak;\n\t\t\treturn &amd_f15_PMC3;\n\t\tcase 0x003:\n\t\tcase 0x00B:\n\t\tcase 0x00D:\n\t\t\treturn &amd_f15_PMC3;\n\t\t}\n\t\treturn &amd_f15_PMC53;\n\tcase AMD_EVENT_LS:\n\tcase AMD_EVENT_DC:\n\tcase AMD_EVENT_EX_LS:\n\t\tswitch (event_code) {\n\t\tcase 0x023:\n\t\tcase 0x043:\n\t\tcase 0x045:\n\t\tcase 0x046:\n\t\tcase 0x054:\n\t\tcase 0x055:\n\t\t\treturn &amd_f15_PMC20;\n\t\tcase 0x02D:\n\t\t\treturn &amd_f15_PMC3;\n\t\tcase 0x02E:\n\t\t\treturn &amd_f15_PMC30;\n\t\tcase 0x031:\n\t\t\tif (hweight_long(hwc->config & ARCH_PERFMON_EVENTSEL_UMASK) <= 1)\n\t\t\t\treturn &amd_f15_PMC20;\n\t\t\treturn &emptyconstraint;\n\t\tcase 0x1C0:\n\t\t\treturn &amd_f15_PMC53;\n\t\tdefault:\n\t\t\treturn &amd_f15_PMC50;\n\t\t}\n\tcase AMD_EVENT_CU:\n\tcase AMD_EVENT_IC_DE:\n\tcase AMD_EVENT_DE:\n\t\tswitch (event_code) {\n\t\tcase 0x08F:\n\t\tcase 0x187:\n\t\tcase 0x188:\n\t\t\treturn &amd_f15_PMC0;\n\t\tcase 0x0DB ... 0x0DF:\n\t\tcase 0x1D6:\n\t\tcase 0x1D8:\n\t\t\treturn &amd_f15_PMC50;\n\t\tdefault:\n\t\t\treturn &amd_f15_PMC20;\n\t\t}\n\tcase AMD_EVENT_NB:\n\t\t \n\t\treturn &emptyconstraint;\n\tdefault:\n\t\treturn &emptyconstraint;\n\t}\n}\n\nstatic struct event_constraint pair_constraint;\n\nstatic struct event_constraint *\namd_get_event_constraints_f17h(struct cpu_hw_events *cpuc, int idx,\n\t\t\t       struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (amd_is_pair_event_code(hwc))\n\t\treturn &pair_constraint;\n\n\treturn &unconstrained;\n}\n\nstatic void amd_put_event_constraints_f17h(struct cpu_hw_events *cpuc,\n\t\t\t\t\t   struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (is_counter_pair(hwc))\n\t\t--cpuc->n_pair;\n}\n\n \nstatic struct event_constraint amd_fam19h_brs_cntr0_constraint =\n\tEVENT_CONSTRAINT(0, 0x1, AMD64_RAW_EVENT_MASK);\n\nstatic struct event_constraint amd_fam19h_brs_pair_cntr0_constraint =\n\t__EVENT_CONSTRAINT(0, 0x1, AMD64_RAW_EVENT_MASK, 1, 0, PERF_X86_EVENT_PAIR);\n\nstatic struct event_constraint *\namd_get_event_constraints_f19h(struct cpu_hw_events *cpuc, int idx,\n\t\t\t  struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tbool has_brs = has_amd_brs(hwc);\n\n\t \n\tif (amd_is_pair_event_code(hwc)) {\n\t\treturn has_brs ? &amd_fam19h_brs_pair_cntr0_constraint\n\t\t\t       : &pair_constraint;\n\t}\n\n\tif (has_brs)\n\t\treturn &amd_fam19h_brs_cntr0_constraint;\n\n\treturn &unconstrained;\n}\n\n\nstatic ssize_t amd_event_sysfs_show(char *page, u64 config)\n{\n\tu64 event = (config & ARCH_PERFMON_EVENTSEL_EVENT) |\n\t\t    (config & AMD64_EVENTSEL_EVENT) >> 24;\n\n\treturn x86_event_sysfs_show(page, config, event);\n}\n\nstatic void amd_pmu_limit_period(struct perf_event *event, s64 *left)\n{\n\t \n\tif (has_branch_stack(event) && *left > x86_pmu.lbr_nr)\n\t\t*left -= x86_pmu.lbr_nr;\n}\n\nstatic __initconst const struct x86_pmu amd_pmu = {\n\t.name\t\t\t= \"AMD\",\n\t.handle_irq\t\t= amd_pmu_handle_irq,\n\t.disable_all\t\t= amd_pmu_disable_all,\n\t.enable_all\t\t= amd_pmu_enable_all,\n\t.enable\t\t\t= amd_pmu_enable_event,\n\t.disable\t\t= amd_pmu_disable_event,\n\t.hw_config\t\t= amd_pmu_hw_config,\n\t.schedule_events\t= x86_schedule_events,\n\t.eventsel\t\t= MSR_K7_EVNTSEL0,\n\t.perfctr\t\t= MSR_K7_PERFCTR0,\n\t.addr_offset            = amd_pmu_addr_offset,\n\t.event_map\t\t= amd_pmu_event_map,\n\t.max_events\t\t= ARRAY_SIZE(amd_perfmon_event_map),\n\t.num_counters\t\t= AMD64_NUM_COUNTERS,\n\t.add\t\t\t= amd_pmu_add_event,\n\t.del\t\t\t= amd_pmu_del_event,\n\t.cntval_bits\t\t= 48,\n\t.cntval_mask\t\t= (1ULL << 48) - 1,\n\t.apic\t\t\t= 1,\n\t \n\t.max_period\t\t= (1ULL << 47) - 1,\n\t.get_event_constraints\t= amd_get_event_constraints,\n\t.put_event_constraints\t= amd_put_event_constraints,\n\n\t.format_attrs\t\t= amd_format_attr,\n\t.events_sysfs_show\t= amd_event_sysfs_show,\n\n\t.cpu_prepare\t\t= amd_pmu_cpu_prepare,\n\t.cpu_starting\t\t= amd_pmu_cpu_starting,\n\t.cpu_dead\t\t= amd_pmu_cpu_dead,\n\n\t.amd_nb_constraints\t= 1,\n};\n\nstatic ssize_t branches_show(struct device *cdev,\n\t\t\t      struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\treturn snprintf(buf, PAGE_SIZE, \"%d\\n\", x86_pmu.lbr_nr);\n}\n\nstatic DEVICE_ATTR_RO(branches);\n\nstatic struct attribute *amd_pmu_branches_attrs[] = {\n\t&dev_attr_branches.attr,\n\tNULL,\n};\n\nstatic umode_t\namd_branches_is_visible(struct kobject *kobj, struct attribute *attr, int i)\n{\n\treturn x86_pmu.lbr_nr ? attr->mode : 0;\n}\n\nstatic struct attribute_group group_caps_amd_branches = {\n\t.name  = \"caps\",\n\t.attrs = amd_pmu_branches_attrs,\n\t.is_visible = amd_branches_is_visible,\n};\n\n#ifdef CONFIG_PERF_EVENTS_AMD_BRS\n\nEVENT_ATTR_STR(branch-brs, amd_branch_brs,\n\t       \"event=\" __stringify(AMD_FAM19H_BRS_EVENT)\"\\n\");\n\nstatic struct attribute *amd_brs_events_attrs[] = {\n\tEVENT_PTR(amd_branch_brs),\n\tNULL,\n};\n\nstatic umode_t\namd_brs_is_visible(struct kobject *kobj, struct attribute *attr, int i)\n{\n\treturn static_cpu_has(X86_FEATURE_BRS) && x86_pmu.lbr_nr ?\n\t       attr->mode : 0;\n}\n\nstatic struct attribute_group group_events_amd_brs = {\n\t.name       = \"events\",\n\t.attrs      = amd_brs_events_attrs,\n\t.is_visible = amd_brs_is_visible,\n};\n\n#endif\t \n\nstatic const struct attribute_group *amd_attr_update[] = {\n\t&group_caps_amd_branches,\n#ifdef CONFIG_PERF_EVENTS_AMD_BRS\n\t&group_events_amd_brs,\n#endif\n\tNULL,\n};\n\nstatic int __init amd_core_pmu_init(void)\n{\n\tunion cpuid_0x80000022_ebx ebx;\n\tu64 even_ctr_mask = 0ULL;\n\tint i;\n\n\tif (!boot_cpu_has(X86_FEATURE_PERFCTR_CORE))\n\t\treturn 0;\n\n\t \n\tperf_nmi_window = msecs_to_jiffies(100);\n\n\t \n\tx86_pmu.eventsel\t= MSR_F15H_PERF_CTL;\n\tx86_pmu.perfctr\t\t= MSR_F15H_PERF_CTR;\n\tx86_pmu.num_counters\t= AMD64_NUM_COUNTERS_CORE;\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_PERFMON_V2)) {\n\t\tebx.full = cpuid_ebx(EXT_PERFMON_DEBUG_FEATURES);\n\n\t\t \n\t\tx86_pmu.version = 2;\n\n\t\t \n\t\tx86_pmu.num_counters = ebx.split.num_core_pmc;\n\n\t\tamd_pmu_global_cntr_mask = (1ULL << x86_pmu.num_counters) - 1;\n\n\t\t \n\t\tx86_pmu.enable_all = amd_pmu_v2_enable_all;\n\t\tx86_pmu.disable_all = amd_pmu_v2_disable_all;\n\t\tx86_pmu.enable = amd_pmu_v2_enable_event;\n\t\tx86_pmu.handle_irq = amd_pmu_v2_handle_irq;\n\t\tstatic_call_update(amd_pmu_test_overflow, amd_pmu_test_overflow_status);\n\t}\n\n\t \n\tx86_pmu.amd_nb_constraints = 0;\n\n\tif (boot_cpu_data.x86 == 0x15) {\n\t\tpr_cont(\"Fam15h \");\n\t\tx86_pmu.get_event_constraints = amd_get_event_constraints_f15h;\n\t}\n\tif (boot_cpu_data.x86 >= 0x17) {\n\t\tpr_cont(\"Fam17h+ \");\n\t\t \n\t\tfor (i = 0; i < x86_pmu.num_counters - 1; i += 2)\n\t\t\teven_ctr_mask |= BIT_ULL(i);\n\n\t\tpair_constraint = (struct event_constraint)\n\t\t\t\t    __EVENT_CONSTRAINT(0, even_ctr_mask, 0,\n\t\t\t\t    x86_pmu.num_counters / 2, 0,\n\t\t\t\t    PERF_X86_EVENT_PAIR);\n\n\t\tx86_pmu.get_event_constraints = amd_get_event_constraints_f17h;\n\t\tx86_pmu.put_event_constraints = amd_put_event_constraints_f17h;\n\t\tx86_pmu.perf_ctr_pair_en = AMD_MERGE_EVENT_ENABLE;\n\t\tx86_pmu.flags |= PMU_FL_PAIR;\n\t}\n\n\t \n\tif (!amd_pmu_lbr_init()) {\n\t\t \n\t\tx86_pmu.sched_task = amd_pmu_lbr_sched_task;\n\t\tstatic_call_update(amd_pmu_branch_hw_config, amd_pmu_lbr_hw_config);\n\t\tstatic_call_update(amd_pmu_branch_reset, amd_pmu_lbr_reset);\n\t\tstatic_call_update(amd_pmu_branch_add, amd_pmu_lbr_add);\n\t\tstatic_call_update(amd_pmu_branch_del, amd_pmu_lbr_del);\n\t} else if (!amd_brs_init()) {\n\t\t \n\t\tx86_pmu.get_event_constraints = amd_get_event_constraints_f19h;\n\t\tx86_pmu.sched_task = amd_pmu_brs_sched_task;\n\t\tx86_pmu.limit_period = amd_pmu_limit_period;\n\n\t\tstatic_call_update(amd_pmu_branch_hw_config, amd_brs_hw_config);\n\t\tstatic_call_update(amd_pmu_branch_reset, amd_brs_reset);\n\t\tstatic_call_update(amd_pmu_branch_add, amd_pmu_brs_add);\n\t\tstatic_call_update(amd_pmu_branch_del, amd_pmu_brs_del);\n\n\t\t \n\n\t\t \n\t\tamd_brs_lopwr_init();\n\t}\n\n\tx86_pmu.attr_update = amd_attr_update;\n\n\tpr_cont(\"core perfctr, \");\n\treturn 0;\n}\n\n__init int amd_pmu_init(void)\n{\n\tint ret;\n\n\t \n\tif (boot_cpu_data.x86 < 6)\n\t\treturn -ENODEV;\n\n\tx86_pmu = amd_pmu;\n\n\tret = amd_core_pmu_init();\n\tif (ret)\n\t\treturn ret;\n\n\tif (num_possible_cpus() == 1) {\n\t\t \n\t\tx86_pmu.amd_nb_constraints = 0;\n\t}\n\n\tif (boot_cpu_data.x86 >= 0x17)\n\t\tmemcpy(hw_cache_event_ids, amd_hw_cache_event_ids_f17h, sizeof(hw_cache_event_ids));\n\telse\n\t\tmemcpy(hw_cache_event_ids, amd_hw_cache_event_ids, sizeof(hw_cache_event_ids));\n\n\treturn 0;\n}\n\nstatic inline void amd_pmu_reload_virt(void)\n{\n\tif (x86_pmu.version >= 2) {\n\t\t \n\t\tamd_pmu_v2_disable_all();\n\t\tamd_pmu_enable_all(0);\n\t\tamd_pmu_v2_enable_all(0);\n\t\treturn;\n\t}\n\n\tamd_pmu_disable_all();\n\tamd_pmu_enable_all(0);\n}\n\nvoid amd_pmu_enable_virt(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tcpuc->perf_ctr_virt_mask = 0;\n\n\t \n\tamd_pmu_reload_virt();\n}\nEXPORT_SYMBOL_GPL(amd_pmu_enable_virt);\n\nvoid amd_pmu_disable_virt(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\t \n\tcpuc->perf_ctr_virt_mask = AMD64_EVENTSEL_HOSTONLY;\n\n\t \n\tamd_pmu_reload_virt();\n}\nEXPORT_SYMBOL_GPL(amd_pmu_disable_virt);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}