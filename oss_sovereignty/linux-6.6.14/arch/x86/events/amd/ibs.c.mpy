{
  "module_name": "ibs.c",
  "hash_id": "92e755083c78e5a4cef8691e3ad340244fa3b16dd3a07c7d55ac9d64f3cd2444",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/events/amd/ibs.c",
  "human_readable_source": " \n\n#include <linux/perf_event.h>\n#include <linux/init.h>\n#include <linux/export.h>\n#include <linux/pci.h>\n#include <linux/ptrace.h>\n#include <linux/syscore_ops.h>\n#include <linux/sched/clock.h>\n\n#include <asm/apic.h>\n\n#include \"../perf_event.h\"\n\nstatic u32 ibs_caps;\n\n#if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_CPU_SUP_AMD)\n\n#include <linux/kprobes.h>\n#include <linux/hardirq.h>\n\n#include <asm/nmi.h>\n#include <asm/amd-ibs.h>\n\n#define IBS_FETCH_CONFIG_MASK\t(IBS_FETCH_RAND_EN | IBS_FETCH_MAX_CNT)\n#define IBS_OP_CONFIG_MASK\tIBS_OP_MAX_CNT\n\n\n \n\nenum ibs_states {\n\tIBS_ENABLED\t= 0,\n\tIBS_STARTED\t= 1,\n\tIBS_STOPPING\t= 2,\n\tIBS_STOPPED\t= 3,\n\n\tIBS_MAX_STATES,\n};\n\nstruct cpu_perf_ibs {\n\tstruct perf_event\t*event;\n\tunsigned long\t\tstate[BITS_TO_LONGS(IBS_MAX_STATES)];\n};\n\nstruct perf_ibs {\n\tstruct pmu\t\t\tpmu;\n\tunsigned int\t\t\tmsr;\n\tu64\t\t\t\tconfig_mask;\n\tu64\t\t\t\tcnt_mask;\n\tu64\t\t\t\tenable_mask;\n\tu64\t\t\t\tvalid_mask;\n\tu64\t\t\t\tmax_period;\n\tunsigned long\t\t\toffset_mask[1];\n\tint\t\t\t\toffset_max;\n\tunsigned int\t\t\tfetch_count_reset_broken : 1;\n\tunsigned int\t\t\tfetch_ignore_if_zero_rip : 1;\n\tstruct cpu_perf_ibs __percpu\t*pcpu;\n\n\tu64\t\t\t\t(*get_count)(u64 config);\n};\n\nstatic int\nperf_event_set_period(struct hw_perf_event *hwc, u64 min, u64 max, u64 *hw_period)\n{\n\ts64 left = local64_read(&hwc->period_left);\n\ts64 period = hwc->sample_period;\n\tint overflow = 0;\n\n\t \n\tif (unlikely(left <= -period)) {\n\t\tleft = period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\toverflow = 1;\n\t}\n\n\tif (unlikely(left < (s64)min)) {\n\t\tleft += period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\toverflow = 1;\n\t}\n\n\t \n\tif (left > max) {\n\t\tleft -= max;\n\t\tif (left > max)\n\t\t\tleft = max;\n\t\telse if (left < min)\n\t\t\tleft = min;\n\t}\n\n\t*hw_period = (u64)left;\n\n\treturn overflow;\n}\n\nstatic  int\nperf_event_try_update(struct perf_event *event, u64 new_raw_count, int width)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint shift = 64 - width;\n\tu64 prev_raw_count;\n\tu64 delta;\n\n\t \n\tprev_raw_count = local64_read(&hwc->prev_count);\n\tif (!local64_try_cmpxchg(&hwc->prev_count,\n\t\t\t\t &prev_raw_count, new_raw_count))\n\t\treturn 0;\n\n\t \n\tdelta = (new_raw_count << shift) - (prev_raw_count << shift);\n\tdelta >>= shift;\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &hwc->period_left);\n\n\treturn 1;\n}\n\nstatic struct perf_ibs perf_ibs_fetch;\nstatic struct perf_ibs perf_ibs_op;\n\nstatic struct perf_ibs *get_ibs_pmu(int type)\n{\n\tif (perf_ibs_fetch.pmu.type == type)\n\t\treturn &perf_ibs_fetch;\n\tif (perf_ibs_op.pmu.type == type)\n\t\treturn &perf_ibs_op;\n\treturn NULL;\n}\n\n \nstatic int core_pmu_ibs_config(struct perf_event *event, u64 *config)\n{\n\tswitch (event->attr.type) {\n\tcase PERF_TYPE_HARDWARE:\n\t\tswitch (event->attr.config) {\n\t\tcase PERF_COUNT_HW_CPU_CYCLES:\n\t\t\t*config = 0;\n\t\t\treturn 0;\n\t\t}\n\t\tbreak;\n\tcase PERF_TYPE_RAW:\n\t\tswitch (event->attr.config) {\n\t\tcase 0x0076:\n\t\t\t*config = 0;\n\t\t\treturn 0;\n\t\tcase 0x00C1:\n\t\t\t*config = IBS_OP_CNT_CTL;\n\t\t\treturn 0;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOENT;\n\t}\n\n\treturn -EOPNOTSUPP;\n}\n\n \nint forward_event_to_ibs(struct perf_event *event)\n{\n\tu64 config = 0;\n\n\tif (!event->attr.precise_ip || event->attr.precise_ip > 2)\n\t\treturn -EOPNOTSUPP;\n\n\tif (!core_pmu_ibs_config(event, &config)) {\n\t\tevent->attr.type = perf_ibs_op.pmu.type;\n\t\tevent->attr.config = config;\n\t}\n\treturn -ENOENT;\n}\n\n \nstatic int validate_group(struct perf_event *event)\n{\n\tstruct perf_event *sibling;\n\n\tif (event->group_leader == event)\n\t\treturn 0;\n\n\tif (event->group_leader->pmu == event->pmu)\n\t\treturn -EINVAL;\n\n\tfor_each_sibling_event(sibling, event->group_leader) {\n\t\tif (sibling->pmu == event->pmu)\n\t\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic int perf_ibs_init(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct perf_ibs *perf_ibs;\n\tu64 max_cnt, config;\n\tint ret;\n\n\tperf_ibs = get_ibs_pmu(event->attr.type);\n\tif (!perf_ibs)\n\t\treturn -ENOENT;\n\n\tconfig = event->attr.config;\n\n\tif (event->pmu != &perf_ibs->pmu)\n\t\treturn -ENOENT;\n\n\tif (config & ~perf_ibs->config_mask)\n\t\treturn -EINVAL;\n\n\tret = validate_group(event);\n\tif (ret)\n\t\treturn ret;\n\n\tif (hwc->sample_period) {\n\t\tif (config & perf_ibs->cnt_mask)\n\t\t\t \n\t\t\treturn -EINVAL;\n\t\tif (!event->attr.sample_freq && hwc->sample_period & 0x0f)\n\t\t\t \n\t\t\treturn -EINVAL;\n\t\thwc->sample_period &= ~0x0FULL;\n\t\tif (!hwc->sample_period)\n\t\t\thwc->sample_period = 0x10;\n\t} else {\n\t\tmax_cnt = config & perf_ibs->cnt_mask;\n\t\tconfig &= ~perf_ibs->cnt_mask;\n\t\tevent->attr.sample_period = max_cnt << 4;\n\t\thwc->sample_period = event->attr.sample_period;\n\t}\n\n\tif (!hwc->sample_period)\n\t\treturn -EINVAL;\n\n\t \n\thwc->last_period = hwc->sample_period;\n\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\n\thwc->config_base = perf_ibs->msr;\n\thwc->config = config;\n\n\treturn 0;\n}\n\nstatic int perf_ibs_set_period(struct perf_ibs *perf_ibs,\n\t\t\t       struct hw_perf_event *hwc, u64 *period)\n{\n\tint overflow;\n\n\t \n\toverflow = perf_event_set_period(hwc, 1<<4, perf_ibs->max_period, period);\n\tlocal64_set(&hwc->prev_count, 0);\n\n\treturn overflow;\n}\n\nstatic u64 get_ibs_fetch_count(u64 config)\n{\n\tunion ibs_fetch_ctl fetch_ctl = (union ibs_fetch_ctl)config;\n\n\treturn fetch_ctl.fetch_cnt << 4;\n}\n\nstatic u64 get_ibs_op_count(u64 config)\n{\n\tunion ibs_op_ctl op_ctl = (union ibs_op_ctl)config;\n\tu64 count = 0;\n\n\t \n\tif (op_ctl.op_val) {\n\t\tcount = op_ctl.opmaxcnt << 4;\n\t\tif (ibs_caps & IBS_CAPS_OPCNTEXT)\n\t\t\tcount += op_ctl.opmaxcnt_ext << 20;\n\t} else if (ibs_caps & IBS_CAPS_RDWROPCNT) {\n\t\tcount = op_ctl.opcurcnt;\n\t}\n\n\treturn count;\n}\n\nstatic void\nperf_ibs_event_update(struct perf_ibs *perf_ibs, struct perf_event *event,\n\t\t      u64 *config)\n{\n\tu64 count = perf_ibs->get_count(*config);\n\n\t \n\twhile (!perf_event_try_update(event, count, 64)) {\n\t\trdmsrl(event->hw.config_base, *config);\n\t\tcount = perf_ibs->get_count(*config);\n\t}\n}\n\nstatic inline void perf_ibs_enable_event(struct perf_ibs *perf_ibs,\n\t\t\t\t\t struct hw_perf_event *hwc, u64 config)\n{\n\tu64 tmp = hwc->config | config;\n\n\tif (perf_ibs->fetch_count_reset_broken)\n\t\twrmsrl(hwc->config_base, tmp & ~perf_ibs->enable_mask);\n\n\twrmsrl(hwc->config_base, tmp | perf_ibs->enable_mask);\n}\n\n \nstatic inline void perf_ibs_disable_event(struct perf_ibs *perf_ibs,\n\t\t\t\t\t  struct hw_perf_event *hwc, u64 config)\n{\n\tconfig &= ~perf_ibs->cnt_mask;\n\tif (boot_cpu_data.x86 == 0x10)\n\t\twrmsrl(hwc->config_base, config);\n\tconfig &= ~perf_ibs->enable_mask;\n\twrmsrl(hwc->config_base, config);\n}\n\n \nstatic void perf_ibs_start(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct perf_ibs *perf_ibs = container_of(event->pmu, struct perf_ibs, pmu);\n\tstruct cpu_perf_ibs *pcpu = this_cpu_ptr(perf_ibs->pcpu);\n\tu64 period, config = 0;\n\n\tif (WARN_ON_ONCE(!(hwc->state & PERF_HES_STOPPED)))\n\t\treturn;\n\n\tWARN_ON_ONCE(!(hwc->state & PERF_HES_UPTODATE));\n\thwc->state = 0;\n\n\tperf_ibs_set_period(perf_ibs, hwc, &period);\n\tif (perf_ibs == &perf_ibs_op && (ibs_caps & IBS_CAPS_OPCNTEXT)) {\n\t\tconfig |= period & IBS_OP_MAX_CNT_EXT_MASK;\n\t\tperiod &= ~IBS_OP_MAX_CNT_EXT_MASK;\n\t}\n\tconfig |= period >> 4;\n\n\t \n\tset_bit(IBS_STARTED,    pcpu->state);\n\tclear_bit(IBS_STOPPING, pcpu->state);\n\tperf_ibs_enable_event(perf_ibs, hwc, config);\n\n\tperf_event_update_userpage(event);\n}\n\nstatic void perf_ibs_stop(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tstruct perf_ibs *perf_ibs = container_of(event->pmu, struct perf_ibs, pmu);\n\tstruct cpu_perf_ibs *pcpu = this_cpu_ptr(perf_ibs->pcpu);\n\tu64 config;\n\tint stopping;\n\n\tif (test_and_set_bit(IBS_STOPPING, pcpu->state))\n\t\treturn;\n\n\tstopping = test_bit(IBS_STARTED, pcpu->state);\n\n\tif (!stopping && (hwc->state & PERF_HES_UPTODATE))\n\t\treturn;\n\n\trdmsrl(hwc->config_base, config);\n\n\tif (stopping) {\n\t\t \n\t\tset_bit(IBS_STOPPED, pcpu->state);\n\t\tperf_ibs_disable_event(perf_ibs, hwc, config);\n\t\t \n\t\tclear_bit(IBS_STARTED, pcpu->state);\n\t\tWARN_ON_ONCE(hwc->state & PERF_HES_STOPPED);\n\t\thwc->state |= PERF_HES_STOPPED;\n\t}\n\n\tif (hwc->state & PERF_HES_UPTODATE)\n\t\treturn;\n\n\t \n\tconfig &= ~perf_ibs->valid_mask;\n\n\tperf_ibs_event_update(perf_ibs, event, &config);\n\thwc->state |= PERF_HES_UPTODATE;\n}\n\nstatic int perf_ibs_add(struct perf_event *event, int flags)\n{\n\tstruct perf_ibs *perf_ibs = container_of(event->pmu, struct perf_ibs, pmu);\n\tstruct cpu_perf_ibs *pcpu = this_cpu_ptr(perf_ibs->pcpu);\n\n\tif (test_and_set_bit(IBS_ENABLED, pcpu->state))\n\t\treturn -ENOSPC;\n\n\tevent->hw.state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\n\n\tpcpu->event = event;\n\n\tif (flags & PERF_EF_START)\n\t\tperf_ibs_start(event, PERF_EF_RELOAD);\n\n\treturn 0;\n}\n\nstatic void perf_ibs_del(struct perf_event *event, int flags)\n{\n\tstruct perf_ibs *perf_ibs = container_of(event->pmu, struct perf_ibs, pmu);\n\tstruct cpu_perf_ibs *pcpu = this_cpu_ptr(perf_ibs->pcpu);\n\n\tif (!test_and_clear_bit(IBS_ENABLED, pcpu->state))\n\t\treturn;\n\n\tperf_ibs_stop(event, PERF_EF_UPDATE);\n\n\tpcpu->event = NULL;\n\n\tperf_event_update_userpage(event);\n}\n\nstatic void perf_ibs_read(struct perf_event *event) { }\n\n \nstatic struct attribute *attrs_empty[] = {\n\tNULL,\n};\n\nstatic struct attribute_group empty_format_group = {\n\t.name = \"format\",\n\t.attrs = attrs_empty,\n};\n\nstatic struct attribute_group empty_caps_group = {\n\t.name = \"caps\",\n\t.attrs = attrs_empty,\n};\n\nstatic const struct attribute_group *empty_attr_groups[] = {\n\t&empty_format_group,\n\t&empty_caps_group,\n\tNULL,\n};\n\nPMU_FORMAT_ATTR(rand_en,\t\"config:57\");\nPMU_FORMAT_ATTR(cnt_ctl,\t\"config:19\");\nPMU_EVENT_ATTR_STRING(l3missonly, fetch_l3missonly, \"config:59\");\nPMU_EVENT_ATTR_STRING(l3missonly, op_l3missonly, \"config:16\");\nPMU_EVENT_ATTR_STRING(zen4_ibs_extensions, zen4_ibs_extensions, \"1\");\n\nstatic umode_t\nzen4_ibs_extensions_is_visible(struct kobject *kobj, struct attribute *attr, int i)\n{\n\treturn ibs_caps & IBS_CAPS_ZEN4 ? attr->mode : 0;\n}\n\nstatic struct attribute *rand_en_attrs[] = {\n\t&format_attr_rand_en.attr,\n\tNULL,\n};\n\nstatic struct attribute *fetch_l3missonly_attrs[] = {\n\t&fetch_l3missonly.attr.attr,\n\tNULL,\n};\n\nstatic struct attribute *zen4_ibs_extensions_attrs[] = {\n\t&zen4_ibs_extensions.attr.attr,\n\tNULL,\n};\n\nstatic struct attribute_group group_rand_en = {\n\t.name = \"format\",\n\t.attrs = rand_en_attrs,\n};\n\nstatic struct attribute_group group_fetch_l3missonly = {\n\t.name = \"format\",\n\t.attrs = fetch_l3missonly_attrs,\n\t.is_visible = zen4_ibs_extensions_is_visible,\n};\n\nstatic struct attribute_group group_zen4_ibs_extensions = {\n\t.name = \"caps\",\n\t.attrs = zen4_ibs_extensions_attrs,\n\t.is_visible = zen4_ibs_extensions_is_visible,\n};\n\nstatic const struct attribute_group *fetch_attr_groups[] = {\n\t&group_rand_en,\n\t&empty_caps_group,\n\tNULL,\n};\n\nstatic const struct attribute_group *fetch_attr_update[] = {\n\t&group_fetch_l3missonly,\n\t&group_zen4_ibs_extensions,\n\tNULL,\n};\n\nstatic umode_t\ncnt_ctl_is_visible(struct kobject *kobj, struct attribute *attr, int i)\n{\n\treturn ibs_caps & IBS_CAPS_OPCNT ? attr->mode : 0;\n}\n\nstatic struct attribute *cnt_ctl_attrs[] = {\n\t&format_attr_cnt_ctl.attr,\n\tNULL,\n};\n\nstatic struct attribute *op_l3missonly_attrs[] = {\n\t&op_l3missonly.attr.attr,\n\tNULL,\n};\n\nstatic struct attribute_group group_cnt_ctl = {\n\t.name = \"format\",\n\t.attrs = cnt_ctl_attrs,\n\t.is_visible = cnt_ctl_is_visible,\n};\n\nstatic struct attribute_group group_op_l3missonly = {\n\t.name = \"format\",\n\t.attrs = op_l3missonly_attrs,\n\t.is_visible = zen4_ibs_extensions_is_visible,\n};\n\nstatic const struct attribute_group *op_attr_update[] = {\n\t&group_cnt_ctl,\n\t&group_op_l3missonly,\n\t&group_zen4_ibs_extensions,\n\tNULL,\n};\n\nstatic struct perf_ibs perf_ibs_fetch = {\n\t.pmu = {\n\t\t.task_ctx_nr\t= perf_hw_context,\n\n\t\t.event_init\t= perf_ibs_init,\n\t\t.add\t\t= perf_ibs_add,\n\t\t.del\t\t= perf_ibs_del,\n\t\t.start\t\t= perf_ibs_start,\n\t\t.stop\t\t= perf_ibs_stop,\n\t\t.read\t\t= perf_ibs_read,\n\t\t.capabilities\t= PERF_PMU_CAP_NO_EXCLUDE,\n\t},\n\t.msr\t\t\t= MSR_AMD64_IBSFETCHCTL,\n\t.config_mask\t\t= IBS_FETCH_CONFIG_MASK,\n\t.cnt_mask\t\t= IBS_FETCH_MAX_CNT,\n\t.enable_mask\t\t= IBS_FETCH_ENABLE,\n\t.valid_mask\t\t= IBS_FETCH_VAL,\n\t.max_period\t\t= IBS_FETCH_MAX_CNT << 4,\n\t.offset_mask\t\t= { MSR_AMD64_IBSFETCH_REG_MASK },\n\t.offset_max\t\t= MSR_AMD64_IBSFETCH_REG_COUNT,\n\n\t.get_count\t\t= get_ibs_fetch_count,\n};\n\nstatic struct perf_ibs perf_ibs_op = {\n\t.pmu = {\n\t\t.task_ctx_nr\t= perf_hw_context,\n\n\t\t.event_init\t= perf_ibs_init,\n\t\t.add\t\t= perf_ibs_add,\n\t\t.del\t\t= perf_ibs_del,\n\t\t.start\t\t= perf_ibs_start,\n\t\t.stop\t\t= perf_ibs_stop,\n\t\t.read\t\t= perf_ibs_read,\n\t\t.capabilities\t= PERF_PMU_CAP_NO_EXCLUDE,\n\t},\n\t.msr\t\t\t= MSR_AMD64_IBSOPCTL,\n\t.config_mask\t\t= IBS_OP_CONFIG_MASK,\n\t.cnt_mask\t\t= IBS_OP_MAX_CNT | IBS_OP_CUR_CNT |\n\t\t\t\t  IBS_OP_CUR_CNT_RAND,\n\t.enable_mask\t\t= IBS_OP_ENABLE,\n\t.valid_mask\t\t= IBS_OP_VAL,\n\t.max_period\t\t= IBS_OP_MAX_CNT << 4,\n\t.offset_mask\t\t= { MSR_AMD64_IBSOP_REG_MASK },\n\t.offset_max\t\t= MSR_AMD64_IBSOP_REG_COUNT,\n\n\t.get_count\t\t= get_ibs_op_count,\n};\n\nstatic void perf_ibs_get_mem_op(union ibs_op_data3 *op_data3,\n\t\t\t\tstruct perf_sample_data *data)\n{\n\tunion perf_mem_data_src *data_src = &data->data_src;\n\n\tdata_src->mem_op = PERF_MEM_OP_NA;\n\n\tif (op_data3->ld_op)\n\t\tdata_src->mem_op = PERF_MEM_OP_LOAD;\n\telse if (op_data3->st_op)\n\t\tdata_src->mem_op = PERF_MEM_OP_STORE;\n}\n\n \nstatic u8 perf_ibs_data_src(union ibs_op_data2 *op_data2)\n{\n\tif (ibs_caps & IBS_CAPS_ZEN4)\n\t\treturn (op_data2->data_src_hi << 3) | op_data2->data_src_lo;\n\n\treturn op_data2->data_src_lo;\n}\n\n#define\tL(x)\t\t(PERF_MEM_S(LVL, x) | PERF_MEM_S(LVL, HIT))\n#define\tLN(x)\t\tPERF_MEM_S(LVLNUM, x)\n#define\tREM\t\tPERF_MEM_S(REMOTE, REMOTE)\n#define\tHOPS(x)\t\tPERF_MEM_S(HOPS, x)\n\nstatic u64 g_data_src[8] = {\n\t[IBS_DATA_SRC_LOC_CACHE]\t  = L(L3) | L(REM_CCE1) | LN(ANY_CACHE) | HOPS(0),\n\t[IBS_DATA_SRC_DRAM]\t\t  = L(LOC_RAM) | LN(RAM),\n\t[IBS_DATA_SRC_REM_CACHE]\t  = L(REM_CCE2) | LN(ANY_CACHE) | REM | HOPS(1),\n\t[IBS_DATA_SRC_IO]\t\t  = L(IO) | LN(IO),\n};\n\n#define RMT_NODE_BITS\t\t\t(1 << IBS_DATA_SRC_DRAM)\n#define RMT_NODE_APPLICABLE(x)\t\t(RMT_NODE_BITS & (1 << x))\n\nstatic u64 g_zen4_data_src[32] = {\n\t[IBS_DATA_SRC_EXT_LOC_CACHE]\t  = L(L3) | LN(L3),\n\t[IBS_DATA_SRC_EXT_NEAR_CCX_CACHE] = L(REM_CCE1) | LN(ANY_CACHE) | REM | HOPS(0),\n\t[IBS_DATA_SRC_EXT_DRAM]\t\t  = L(LOC_RAM) | LN(RAM),\n\t[IBS_DATA_SRC_EXT_FAR_CCX_CACHE]  = L(REM_CCE2) | LN(ANY_CACHE) | REM | HOPS(1),\n\t[IBS_DATA_SRC_EXT_PMEM]\t\t  = LN(PMEM),\n\t[IBS_DATA_SRC_EXT_IO]\t\t  = L(IO) | LN(IO),\n\t[IBS_DATA_SRC_EXT_EXT_MEM]\t  = LN(CXL),\n};\n\n#define ZEN4_RMT_NODE_BITS\t\t((1 << IBS_DATA_SRC_EXT_DRAM) | \\\n\t\t\t\t\t (1 << IBS_DATA_SRC_EXT_PMEM) | \\\n\t\t\t\t\t (1 << IBS_DATA_SRC_EXT_EXT_MEM))\n#define ZEN4_RMT_NODE_APPLICABLE(x)\t(ZEN4_RMT_NODE_BITS & (1 << x))\n\nstatic __u64 perf_ibs_get_mem_lvl(union ibs_op_data2 *op_data2,\n\t\t\t\t  union ibs_op_data3 *op_data3,\n\t\t\t\t  struct perf_sample_data *data)\n{\n\tunion perf_mem_data_src *data_src = &data->data_src;\n\tu8 ibs_data_src = perf_ibs_data_src(op_data2);\n\n\tdata_src->mem_lvl = 0;\n\tdata_src->mem_lvl_num = 0;\n\n\t \n\tif (op_data3->dc_uc_mem_acc && ibs_data_src != IBS_DATA_SRC_EXT_IO)\n\t\treturn L(UNC) | LN(UNC);\n\n\t \n\tif (op_data3->dc_miss == 0)\n\t\treturn L(L1) | LN(L1);\n\n\t \n\tif (op_data3->l2_miss == 0) {\n\t\t \n\t\tif (boot_cpu_data.x86 != 0x19 || boot_cpu_data.x86_model > 0xF ||\n\t\t    !(op_data3->sw_pf || op_data3->dc_miss_no_mab_alloc))\n\t\t\treturn L(L2) | LN(L2);\n\t}\n\n\t \n\tif (data_src->mem_op != PERF_MEM_OP_LOAD)\n\t\tgoto check_mab;\n\n\tif (ibs_caps & IBS_CAPS_ZEN4) {\n\t\tu64 val = g_zen4_data_src[ibs_data_src];\n\n\t\tif (!val)\n\t\t\tgoto check_mab;\n\n\t\t \n\t\tif (op_data2->rmt_node && ZEN4_RMT_NODE_APPLICABLE(ibs_data_src)) {\n\t\t\tif (ibs_data_src == IBS_DATA_SRC_EXT_DRAM)\n\t\t\t\tval = L(REM_RAM1) | LN(RAM) | REM | HOPS(1);\n\t\t\telse\n\t\t\t\tval |= REM | HOPS(1);\n\t\t}\n\n\t\treturn val;\n\t} else {\n\t\tu64 val = g_data_src[ibs_data_src];\n\n\t\tif (!val)\n\t\t\tgoto check_mab;\n\n\t\t \n\t\tif (op_data2->rmt_node && RMT_NODE_APPLICABLE(ibs_data_src)) {\n\t\t\tif (ibs_data_src == IBS_DATA_SRC_DRAM)\n\t\t\t\tval = L(REM_RAM1) | LN(RAM) | REM | HOPS(1);\n\t\t\telse\n\t\t\t\tval |= REM | HOPS(1);\n\t\t}\n\n\t\treturn val;\n\t}\n\ncheck_mab:\n\t \n\tif (op_data3->dc_miss_no_mab_alloc)\n\t\treturn L(LFB) | LN(LFB);\n\n\t \n\treturn PERF_MEM_S(LVL, NA) | LN(NA);\n}\n\nstatic bool perf_ibs_cache_hit_st_valid(void)\n{\n\t \n\tstatic int cache_hit_st_valid;\n\n\tif (unlikely(!cache_hit_st_valid)) {\n\t\tif (boot_cpu_data.x86 == 0x19 &&\n\t\t    (boot_cpu_data.x86_model <= 0xF ||\n\t\t    (boot_cpu_data.x86_model >= 0x20 &&\n\t\t     boot_cpu_data.x86_model <= 0x5F))) {\n\t\t\tcache_hit_st_valid = -1;\n\t\t} else {\n\t\t\tcache_hit_st_valid = 1;\n\t\t}\n\t}\n\n\treturn cache_hit_st_valid == 1;\n}\n\nstatic void perf_ibs_get_mem_snoop(union ibs_op_data2 *op_data2,\n\t\t\t\t   struct perf_sample_data *data)\n{\n\tunion perf_mem_data_src *data_src = &data->data_src;\n\tu8 ibs_data_src;\n\n\tdata_src->mem_snoop = PERF_MEM_SNOOP_NA;\n\n\tif (!perf_ibs_cache_hit_st_valid() ||\n\t    data_src->mem_op != PERF_MEM_OP_LOAD ||\n\t    data_src->mem_lvl & PERF_MEM_LVL_L1 ||\n\t    data_src->mem_lvl & PERF_MEM_LVL_L2 ||\n\t    op_data2->cache_hit_st)\n\t\treturn;\n\n\tibs_data_src = perf_ibs_data_src(op_data2);\n\n\tif (ibs_caps & IBS_CAPS_ZEN4) {\n\t\tif (ibs_data_src == IBS_DATA_SRC_EXT_LOC_CACHE ||\n\t\t    ibs_data_src == IBS_DATA_SRC_EXT_NEAR_CCX_CACHE ||\n\t\t    ibs_data_src == IBS_DATA_SRC_EXT_FAR_CCX_CACHE)\n\t\t\tdata_src->mem_snoop = PERF_MEM_SNOOP_HITM;\n\t} else if (ibs_data_src == IBS_DATA_SRC_LOC_CACHE) {\n\t\tdata_src->mem_snoop = PERF_MEM_SNOOP_HITM;\n\t}\n}\n\nstatic void perf_ibs_get_tlb_lvl(union ibs_op_data3 *op_data3,\n\t\t\t\t struct perf_sample_data *data)\n{\n\tunion perf_mem_data_src *data_src = &data->data_src;\n\n\tdata_src->mem_dtlb = PERF_MEM_TLB_NA;\n\n\tif (!op_data3->dc_lin_addr_valid)\n\t\treturn;\n\n\tif (!op_data3->dc_l1tlb_miss) {\n\t\tdata_src->mem_dtlb = PERF_MEM_TLB_L1 | PERF_MEM_TLB_HIT;\n\t\treturn;\n\t}\n\n\tif (!op_data3->dc_l2tlb_miss) {\n\t\tdata_src->mem_dtlb = PERF_MEM_TLB_L2 | PERF_MEM_TLB_HIT;\n\t\treturn;\n\t}\n\n\tdata_src->mem_dtlb = PERF_MEM_TLB_L2 | PERF_MEM_TLB_MISS;\n}\n\nstatic void perf_ibs_get_mem_lock(union ibs_op_data3 *op_data3,\n\t\t\t\t  struct perf_sample_data *data)\n{\n\tunion perf_mem_data_src *data_src = &data->data_src;\n\n\tdata_src->mem_lock = PERF_MEM_LOCK_NA;\n\n\tif (op_data3->dc_locked_op)\n\t\tdata_src->mem_lock = PERF_MEM_LOCK_LOCKED;\n}\n\n#define ibs_op_msr_idx(msr)\t(msr - MSR_AMD64_IBSOPCTL)\n\nstatic void perf_ibs_get_data_src(struct perf_ibs_data *ibs_data,\n\t\t\t\t  struct perf_sample_data *data,\n\t\t\t\t  union ibs_op_data2 *op_data2,\n\t\t\t\t  union ibs_op_data3 *op_data3)\n{\n\tunion perf_mem_data_src *data_src = &data->data_src;\n\n\tdata_src->val |= perf_ibs_get_mem_lvl(op_data2, op_data3, data);\n\tperf_ibs_get_mem_snoop(op_data2, data);\n\tperf_ibs_get_tlb_lvl(op_data3, data);\n\tperf_ibs_get_mem_lock(op_data3, data);\n}\n\nstatic __u64 perf_ibs_get_op_data2(struct perf_ibs_data *ibs_data,\n\t\t\t\t   union ibs_op_data3 *op_data3)\n{\n\t__u64 val = ibs_data->regs[ibs_op_msr_idx(MSR_AMD64_IBSOPDATA2)];\n\n\t \n\tif (boot_cpu_data.x86 == 0x19 && boot_cpu_data.x86_model <= 0xF &&\n\t    (op_data3->sw_pf || op_data3->dc_miss_no_mab_alloc)) {\n\t\t \n\t\tval = 0;\n\t}\n\treturn val;\n}\n\nstatic void perf_ibs_parse_ld_st_data(__u64 sample_type,\n\t\t\t\t      struct perf_ibs_data *ibs_data,\n\t\t\t\t      struct perf_sample_data *data)\n{\n\tunion ibs_op_data3 op_data3;\n\tunion ibs_op_data2 op_data2;\n\tunion ibs_op_data op_data;\n\n\tdata->data_src.val = PERF_MEM_NA;\n\top_data3.val = ibs_data->regs[ibs_op_msr_idx(MSR_AMD64_IBSOPDATA3)];\n\n\tperf_ibs_get_mem_op(&op_data3, data);\n\tif (data->data_src.mem_op != PERF_MEM_OP_LOAD &&\n\t    data->data_src.mem_op != PERF_MEM_OP_STORE)\n\t\treturn;\n\n\top_data2.val = perf_ibs_get_op_data2(ibs_data, &op_data3);\n\n\tif (sample_type & PERF_SAMPLE_DATA_SRC) {\n\t\tperf_ibs_get_data_src(ibs_data, data, &op_data2, &op_data3);\n\t\tdata->sample_flags |= PERF_SAMPLE_DATA_SRC;\n\t}\n\n\tif (sample_type & PERF_SAMPLE_WEIGHT_TYPE && op_data3.dc_miss &&\n\t    data->data_src.mem_op == PERF_MEM_OP_LOAD) {\n\t\top_data.val = ibs_data->regs[ibs_op_msr_idx(MSR_AMD64_IBSOPDATA)];\n\n\t\tif (sample_type & PERF_SAMPLE_WEIGHT_STRUCT) {\n\t\t\tdata->weight.var1_dw = op_data3.dc_miss_lat;\n\t\t\tdata->weight.var2_w = op_data.tag_to_ret_ctr;\n\t\t} else if (sample_type & PERF_SAMPLE_WEIGHT) {\n\t\t\tdata->weight.full = op_data3.dc_miss_lat;\n\t\t}\n\t\tdata->sample_flags |= PERF_SAMPLE_WEIGHT_TYPE;\n\t}\n\n\tif (sample_type & PERF_SAMPLE_ADDR && op_data3.dc_lin_addr_valid) {\n\t\tdata->addr = ibs_data->regs[ibs_op_msr_idx(MSR_AMD64_IBSDCLINAD)];\n\t\tdata->sample_flags |= PERF_SAMPLE_ADDR;\n\t}\n\n\tif (sample_type & PERF_SAMPLE_PHYS_ADDR && op_data3.dc_phy_addr_valid) {\n\t\tdata->phys_addr = ibs_data->regs[ibs_op_msr_idx(MSR_AMD64_IBSDCPHYSAD)];\n\t\tdata->sample_flags |= PERF_SAMPLE_PHYS_ADDR;\n\t}\n}\n\nstatic int perf_ibs_get_offset_max(struct perf_ibs *perf_ibs, u64 sample_type,\n\t\t\t\t   int check_rip)\n{\n\tif (sample_type & PERF_SAMPLE_RAW ||\n\t    (perf_ibs == &perf_ibs_op &&\n\t     (sample_type & PERF_SAMPLE_DATA_SRC ||\n\t      sample_type & PERF_SAMPLE_WEIGHT_TYPE ||\n\t      sample_type & PERF_SAMPLE_ADDR ||\n\t      sample_type & PERF_SAMPLE_PHYS_ADDR)))\n\t\treturn perf_ibs->offset_max;\n\telse if (check_rip)\n\t\treturn 3;\n\treturn 1;\n}\n\nstatic int perf_ibs_handle_irq(struct perf_ibs *perf_ibs, struct pt_regs *iregs)\n{\n\tstruct cpu_perf_ibs *pcpu = this_cpu_ptr(perf_ibs->pcpu);\n\tstruct perf_event *event = pcpu->event;\n\tstruct hw_perf_event *hwc;\n\tstruct perf_sample_data data;\n\tstruct perf_raw_record raw;\n\tstruct pt_regs regs;\n\tstruct perf_ibs_data ibs_data;\n\tint offset, size, check_rip, offset_max, throttle = 0;\n\tunsigned int msr;\n\tu64 *buf, *config, period, new_config = 0;\n\n\tif (!test_bit(IBS_STARTED, pcpu->state)) {\nfail:\n\t\t \n\t\tif (test_and_clear_bit(IBS_STOPPED, pcpu->state))\n\t\t\treturn 1;\n\n\t\treturn 0;\n\t}\n\n\tif (WARN_ON_ONCE(!event))\n\t\tgoto fail;\n\n\thwc = &event->hw;\n\tmsr = hwc->config_base;\n\tbuf = ibs_data.regs;\n\trdmsrl(msr, *buf);\n\tif (!(*buf++ & perf_ibs->valid_mask))\n\t\tgoto fail;\n\n\tconfig = &ibs_data.regs[0];\n\tperf_ibs_event_update(perf_ibs, event, config);\n\tperf_sample_data_init(&data, 0, hwc->last_period);\n\tif (!perf_ibs_set_period(perf_ibs, hwc, &period))\n\t\tgoto out;\t \n\n\tibs_data.caps = ibs_caps;\n\tsize = 1;\n\toffset = 1;\n\tcheck_rip = (perf_ibs == &perf_ibs_op && (ibs_caps & IBS_CAPS_RIPINVALIDCHK));\n\n\toffset_max = perf_ibs_get_offset_max(perf_ibs, event->attr.sample_type, check_rip);\n\n\tdo {\n\t\trdmsrl(msr + offset, *buf++);\n\t\tsize++;\n\t\toffset = find_next_bit(perf_ibs->offset_mask,\n\t\t\t\t       perf_ibs->offset_max,\n\t\t\t\t       offset + 1);\n\t} while (offset < offset_max);\n\t \n\tif (event->attr.sample_type & PERF_SAMPLE_RAW) {\n\t\tif (perf_ibs == &perf_ibs_op) {\n\t\t\tif (ibs_caps & IBS_CAPS_BRNTRGT) {\n\t\t\t\trdmsrl(MSR_AMD64_IBSBRTARGET, *buf++);\n\t\t\t\tsize++;\n\t\t\t}\n\t\t\tif (ibs_caps & IBS_CAPS_OPDATA4) {\n\t\t\t\trdmsrl(MSR_AMD64_IBSOPDATA4, *buf++);\n\t\t\t\tsize++;\n\t\t\t}\n\t\t}\n\t\tif (perf_ibs == &perf_ibs_fetch && (ibs_caps & IBS_CAPS_FETCHCTLEXTD)) {\n\t\t\trdmsrl(MSR_AMD64_ICIBSEXTDCTL, *buf++);\n\t\t\tsize++;\n\t\t}\n\t}\n\tibs_data.size = sizeof(u64) * size;\n\n\tregs = *iregs;\n\tif (check_rip && (ibs_data.regs[2] & IBS_RIP_INVALID)) {\n\t\tregs.flags &= ~PERF_EFLAGS_EXACT;\n\t} else {\n\t\t \n\t\tif (perf_ibs->fetch_ignore_if_zero_rip && !(ibs_data.regs[1]))\n\t\t\tgoto out;\n\n\t\tset_linear_ip(&regs, ibs_data.regs[1]);\n\t\tregs.flags |= PERF_EFLAGS_EXACT;\n\t}\n\n\tif (event->attr.sample_type & PERF_SAMPLE_RAW) {\n\t\traw = (struct perf_raw_record){\n\t\t\t.frag = {\n\t\t\t\t.size = sizeof(u32) + ibs_data.size,\n\t\t\t\t.data = ibs_data.data,\n\t\t\t},\n\t\t};\n\t\tperf_sample_save_raw_data(&data, &raw);\n\t}\n\n\tif (perf_ibs == &perf_ibs_op)\n\t\tperf_ibs_parse_ld_st_data(event->attr.sample_type, &ibs_data, &data);\n\n\t \n\tif (event->attr.sample_type & PERF_SAMPLE_CALLCHAIN)\n\t\tperf_sample_save_callchain(&data, event, iregs);\n\n\tthrottle = perf_event_overflow(event, &data, &regs);\nout:\n\tif (throttle) {\n\t\tperf_ibs_stop(event, 0);\n\t} else {\n\t\tif (perf_ibs == &perf_ibs_op) {\n\t\t\tif (ibs_caps & IBS_CAPS_OPCNTEXT) {\n\t\t\t\tnew_config = period & IBS_OP_MAX_CNT_EXT_MASK;\n\t\t\t\tperiod &= ~IBS_OP_MAX_CNT_EXT_MASK;\n\t\t\t}\n\t\t\tif ((ibs_caps & IBS_CAPS_RDWROPCNT) && (*config & IBS_OP_CNT_CTL))\n\t\t\t\tnew_config |= *config & IBS_OP_CUR_CNT_RAND;\n\t\t}\n\t\tnew_config |= period >> 4;\n\n\t\tperf_ibs_enable_event(perf_ibs, hwc, new_config);\n\t}\n\n\tperf_event_update_userpage(event);\n\n\treturn 1;\n}\n\nstatic int\nperf_ibs_nmi_handler(unsigned int cmd, struct pt_regs *regs)\n{\n\tu64 stamp = sched_clock();\n\tint handled = 0;\n\n\thandled += perf_ibs_handle_irq(&perf_ibs_fetch, regs);\n\thandled += perf_ibs_handle_irq(&perf_ibs_op, regs);\n\n\tif (handled)\n\t\tinc_irq_stat(apic_perf_irqs);\n\n\tperf_sample_event_took(sched_clock() - stamp);\n\n\treturn handled;\n}\nNOKPROBE_SYMBOL(perf_ibs_nmi_handler);\n\nstatic __init int perf_ibs_pmu_init(struct perf_ibs *perf_ibs, char *name)\n{\n\tstruct cpu_perf_ibs __percpu *pcpu;\n\tint ret;\n\n\tpcpu = alloc_percpu(struct cpu_perf_ibs);\n\tif (!pcpu)\n\t\treturn -ENOMEM;\n\n\tperf_ibs->pcpu = pcpu;\n\n\tret = perf_pmu_register(&perf_ibs->pmu, name, -1);\n\tif (ret) {\n\t\tperf_ibs->pcpu = NULL;\n\t\tfree_percpu(pcpu);\n\t}\n\n\treturn ret;\n}\n\nstatic __init int perf_ibs_fetch_init(void)\n{\n\t \n\tif (boot_cpu_data.x86 >= 0x16 && boot_cpu_data.x86 <= 0x18)\n\t\tperf_ibs_fetch.fetch_count_reset_broken = 1;\n\n\tif (boot_cpu_data.x86 == 0x19 && boot_cpu_data.x86_model < 0x10)\n\t\tperf_ibs_fetch.fetch_ignore_if_zero_rip = 1;\n\n\tif (ibs_caps & IBS_CAPS_ZEN4)\n\t\tperf_ibs_fetch.config_mask |= IBS_FETCH_L3MISSONLY;\n\n\tperf_ibs_fetch.pmu.attr_groups = fetch_attr_groups;\n\tperf_ibs_fetch.pmu.attr_update = fetch_attr_update;\n\n\treturn perf_ibs_pmu_init(&perf_ibs_fetch, \"ibs_fetch\");\n}\n\nstatic __init int perf_ibs_op_init(void)\n{\n\tif (ibs_caps & IBS_CAPS_OPCNT)\n\t\tperf_ibs_op.config_mask |= IBS_OP_CNT_CTL;\n\n\tif (ibs_caps & IBS_CAPS_OPCNTEXT) {\n\t\tperf_ibs_op.max_period  |= IBS_OP_MAX_CNT_EXT_MASK;\n\t\tperf_ibs_op.config_mask\t|= IBS_OP_MAX_CNT_EXT_MASK;\n\t\tperf_ibs_op.cnt_mask    |= IBS_OP_MAX_CNT_EXT_MASK;\n\t}\n\n\tif (ibs_caps & IBS_CAPS_ZEN4)\n\t\tperf_ibs_op.config_mask |= IBS_OP_L3MISSONLY;\n\n\tperf_ibs_op.pmu.attr_groups = empty_attr_groups;\n\tperf_ibs_op.pmu.attr_update = op_attr_update;\n\n\treturn perf_ibs_pmu_init(&perf_ibs_op, \"ibs_op\");\n}\n\nstatic __init int perf_event_ibs_init(void)\n{\n\tint ret;\n\n\tret = perf_ibs_fetch_init();\n\tif (ret)\n\t\treturn ret;\n\n\tret = perf_ibs_op_init();\n\tif (ret)\n\t\tgoto err_op;\n\n\tret = register_nmi_handler(NMI_LOCAL, perf_ibs_nmi_handler, 0, \"perf_ibs\");\n\tif (ret)\n\t\tgoto err_nmi;\n\n\tpr_info(\"perf: AMD IBS detected (0x%08x)\\n\", ibs_caps);\n\treturn 0;\n\nerr_nmi:\n\tperf_pmu_unregister(&perf_ibs_op.pmu);\n\tfree_percpu(perf_ibs_op.pcpu);\n\tperf_ibs_op.pcpu = NULL;\nerr_op:\n\tperf_pmu_unregister(&perf_ibs_fetch.pmu);\n\tfree_percpu(perf_ibs_fetch.pcpu);\n\tperf_ibs_fetch.pcpu = NULL;\n\n\treturn ret;\n}\n\n#else  \n\nstatic __init int perf_event_ibs_init(void)\n{\n\treturn 0;\n}\n\n#endif\n\n \n\nstatic __init u32 __get_ibs_caps(void)\n{\n\tu32 caps;\n\tunsigned int max_level;\n\n\tif (!boot_cpu_has(X86_FEATURE_IBS))\n\t\treturn 0;\n\n\t \n\tmax_level = cpuid_eax(0x80000000);\n\tif (max_level < IBS_CPUID_FEATURES)\n\t\treturn IBS_CAPS_DEFAULT;\n\n\tcaps = cpuid_eax(IBS_CPUID_FEATURES);\n\tif (!(caps & IBS_CAPS_AVAIL))\n\t\t \n\t\treturn IBS_CAPS_DEFAULT;\n\n\treturn caps;\n}\n\nu32 get_ibs_caps(void)\n{\n\treturn ibs_caps;\n}\n\nEXPORT_SYMBOL(get_ibs_caps);\n\nstatic inline int get_eilvt(int offset)\n{\n\treturn !setup_APIC_eilvt(offset, 0, APIC_EILVT_MSG_NMI, 1);\n}\n\nstatic inline int put_eilvt(int offset)\n{\n\treturn !setup_APIC_eilvt(offset, 0, 0, 1);\n}\n\n \nstatic inline int ibs_eilvt_valid(void)\n{\n\tint offset;\n\tu64 val;\n\tint valid = 0;\n\n\tpreempt_disable();\n\n\trdmsrl(MSR_AMD64_IBSCTL, val);\n\toffset = val & IBSCTL_LVT_OFFSET_MASK;\n\n\tif (!(val & IBSCTL_LVT_OFFSET_VALID)) {\n\t\tpr_err(FW_BUG \"cpu %d, invalid IBS interrupt offset %d (MSR%08X=0x%016llx)\\n\",\n\t\t       smp_processor_id(), offset, MSR_AMD64_IBSCTL, val);\n\t\tgoto out;\n\t}\n\n\tif (!get_eilvt(offset)) {\n\t\tpr_err(FW_BUG \"cpu %d, IBS interrupt offset %d not available (MSR%08X=0x%016llx)\\n\",\n\t\t       smp_processor_id(), offset, MSR_AMD64_IBSCTL, val);\n\t\tgoto out;\n\t}\n\n\tvalid = 1;\nout:\n\tpreempt_enable();\n\n\treturn valid;\n}\n\nstatic int setup_ibs_ctl(int ibs_eilvt_off)\n{\n\tstruct pci_dev *cpu_cfg;\n\tint nodes;\n\tu32 value = 0;\n\n\tnodes = 0;\n\tcpu_cfg = NULL;\n\tdo {\n\t\tcpu_cfg = pci_get_device(PCI_VENDOR_ID_AMD,\n\t\t\t\t\t PCI_DEVICE_ID_AMD_10H_NB_MISC,\n\t\t\t\t\t cpu_cfg);\n\t\tif (!cpu_cfg)\n\t\t\tbreak;\n\t\t++nodes;\n\t\tpci_write_config_dword(cpu_cfg, IBSCTL, ibs_eilvt_off\n\t\t\t\t       | IBSCTL_LVT_OFFSET_VALID);\n\t\tpci_read_config_dword(cpu_cfg, IBSCTL, &value);\n\t\tif (value != (ibs_eilvt_off | IBSCTL_LVT_OFFSET_VALID)) {\n\t\t\tpci_dev_put(cpu_cfg);\n\t\t\tpr_debug(\"Failed to setup IBS LVT offset, IBSCTL = 0x%08x\\n\",\n\t\t\t\t value);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} while (1);\n\n\tif (!nodes) {\n\t\tpr_debug(\"No CPU node configured for IBS\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void force_ibs_eilvt_setup(void)\n{\n\tint offset;\n\tint ret;\n\n\tpreempt_disable();\n\t \n\tfor (offset = 1; offset < APIC_EILVT_NR_MAX; offset++) {\n\t\tif (get_eilvt(offset))\n\t\t\tbreak;\n\t}\n\tpreempt_enable();\n\n\tif (offset == APIC_EILVT_NR_MAX) {\n\t\tpr_debug(\"No EILVT entry available\\n\");\n\t\treturn;\n\t}\n\n\tret = setup_ibs_ctl(offset);\n\tif (ret)\n\t\tgoto out;\n\n\tif (!ibs_eilvt_valid())\n\t\tgoto out;\n\n\tpr_info(\"LVT offset %d assigned\\n\", offset);\n\n\treturn;\nout:\n\tpreempt_disable();\n\tput_eilvt(offset);\n\tpreempt_enable();\n\treturn;\n}\n\nstatic void ibs_eilvt_setup(void)\n{\n\t \n\tif (boot_cpu_data.x86 == 0x10)\n\t\tforce_ibs_eilvt_setup();\n}\n\nstatic inline int get_ibs_lvt_offset(void)\n{\n\tu64 val;\n\n\trdmsrl(MSR_AMD64_IBSCTL, val);\n\tif (!(val & IBSCTL_LVT_OFFSET_VALID))\n\t\treturn -EINVAL;\n\n\treturn val & IBSCTL_LVT_OFFSET_MASK;\n}\n\nstatic void setup_APIC_ibs(void)\n{\n\tint offset;\n\n\toffset = get_ibs_lvt_offset();\n\tif (offset < 0)\n\t\tgoto failed;\n\n\tif (!setup_APIC_eilvt(offset, 0, APIC_EILVT_MSG_NMI, 0))\n\t\treturn;\nfailed:\n\tpr_warn(\"perf: IBS APIC setup failed on cpu #%d\\n\",\n\t\tsmp_processor_id());\n}\n\nstatic void clear_APIC_ibs(void)\n{\n\tint offset;\n\n\toffset = get_ibs_lvt_offset();\n\tif (offset >= 0)\n\t\tsetup_APIC_eilvt(offset, 0, APIC_EILVT_MSG_FIX, 1);\n}\n\nstatic int x86_pmu_amd_ibs_starting_cpu(unsigned int cpu)\n{\n\tsetup_APIC_ibs();\n\treturn 0;\n}\n\n#ifdef CONFIG_PM\n\nstatic int perf_ibs_suspend(void)\n{\n\tclear_APIC_ibs();\n\treturn 0;\n}\n\nstatic void perf_ibs_resume(void)\n{\n\tibs_eilvt_setup();\n\tsetup_APIC_ibs();\n}\n\nstatic struct syscore_ops perf_ibs_syscore_ops = {\n\t.resume\t\t= perf_ibs_resume,\n\t.suspend\t= perf_ibs_suspend,\n};\n\nstatic void perf_ibs_pm_init(void)\n{\n\tregister_syscore_ops(&perf_ibs_syscore_ops);\n}\n\n#else\n\nstatic inline void perf_ibs_pm_init(void) { }\n\n#endif\n\nstatic int x86_pmu_amd_ibs_dying_cpu(unsigned int cpu)\n{\n\tclear_APIC_ibs();\n\treturn 0;\n}\n\nstatic __init int amd_ibs_init(void)\n{\n\tu32 caps;\n\n\tcaps = __get_ibs_caps();\n\tif (!caps)\n\t\treturn -ENODEV;\t \n\n\tibs_eilvt_setup();\n\n\tif (!ibs_eilvt_valid())\n\t\treturn -EINVAL;\n\n\tperf_ibs_pm_init();\n\n\tibs_caps = caps;\n\t \n\tsmp_mb();\n\t \n\tcpuhp_setup_state(CPUHP_AP_PERF_X86_AMD_IBS_STARTING,\n\t\t\t  \"perf/x86/amd/ibs:starting\",\n\t\t\t  x86_pmu_amd_ibs_starting_cpu,\n\t\t\t  x86_pmu_amd_ibs_dying_cpu);\n\n\treturn perf_event_ibs_init();\n}\n\n \ndevice_initcall(amd_ibs_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}