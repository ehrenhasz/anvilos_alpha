{
  "module_name": "uncore.c",
  "hash_id": "37919184c3ee0185fbf0f6ed01dae6931d1d0042df9369123df0cd86ace9ef3f",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/events/amd/uncore.c",
  "human_readable_source": "\n \n\n#include <linux/perf_event.h>\n#include <linux/percpu.h>\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/cpu.h>\n#include <linux/cpumask.h>\n#include <linux/cpufeature.h>\n#include <linux/smp.h>\n\n#include <asm/perf_event.h>\n#include <asm/msr.h>\n\n#define NUM_COUNTERS_NB\t\t4\n#define NUM_COUNTERS_L2\t\t4\n#define NUM_COUNTERS_L3\t\t6\n\n#define RDPMC_BASE_NB\t\t6\n#define RDPMC_BASE_LLC\t\t10\n\n#define COUNTER_SHIFT\t\t16\n\n#undef pr_fmt\n#define pr_fmt(fmt)\t\"amd_uncore: \" fmt\n\nstatic int pmu_version;\nstatic int num_counters_llc;\nstatic int num_counters_nb;\nstatic bool l3_mask;\n\nstatic HLIST_HEAD(uncore_unused_list);\n\nstruct amd_uncore {\n\tint id;\n\tint refcnt;\n\tint cpu;\n\tint num_counters;\n\tint rdpmc_base;\n\tu32 msr_base;\n\tcpumask_t *active_mask;\n\tstruct pmu *pmu;\n\tstruct perf_event **events;\n\tstruct hlist_node node;\n};\n\nstatic struct amd_uncore * __percpu *amd_uncore_nb;\nstatic struct amd_uncore * __percpu *amd_uncore_llc;\n\nstatic struct pmu amd_nb_pmu;\nstatic struct pmu amd_llc_pmu;\n\nstatic cpumask_t amd_nb_active_mask;\nstatic cpumask_t amd_llc_active_mask;\n\nstatic bool is_nb_event(struct perf_event *event)\n{\n\treturn event->pmu->type == amd_nb_pmu.type;\n}\n\nstatic bool is_llc_event(struct perf_event *event)\n{\n\treturn event->pmu->type == amd_llc_pmu.type;\n}\n\nstatic struct amd_uncore *event_to_amd_uncore(struct perf_event *event)\n{\n\tif (is_nb_event(event) && amd_uncore_nb)\n\t\treturn *per_cpu_ptr(amd_uncore_nb, event->cpu);\n\telse if (is_llc_event(event) && amd_uncore_llc)\n\t\treturn *per_cpu_ptr(amd_uncore_llc, event->cpu);\n\n\treturn NULL;\n}\n\nstatic void amd_uncore_read(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 prev, new;\n\ts64 delta;\n\n\t \n\n\tprev = local64_read(&hwc->prev_count);\n\trdpmcl(hwc->event_base_rdpmc, new);\n\tlocal64_set(&hwc->prev_count, new);\n\tdelta = (new << COUNTER_SHIFT) - (prev << COUNTER_SHIFT);\n\tdelta >>= COUNTER_SHIFT;\n\tlocal64_add(delta, &event->count);\n}\n\nstatic void amd_uncore_start(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (flags & PERF_EF_RELOAD)\n\t\twrmsrl(hwc->event_base, (u64)local64_read(&hwc->prev_count));\n\n\thwc->state = 0;\n\twrmsrl(hwc->config_base, (hwc->config | ARCH_PERFMON_EVENTSEL_ENABLE));\n\tperf_event_update_userpage(event);\n}\n\nstatic void amd_uncore_stop(struct perf_event *event, int flags)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\twrmsrl(hwc->config_base, hwc->config);\n\thwc->state |= PERF_HES_STOPPED;\n\n\tif ((flags & PERF_EF_UPDATE) && !(hwc->state & PERF_HES_UPTODATE)) {\n\t\tamd_uncore_read(event);\n\t\thwc->state |= PERF_HES_UPTODATE;\n\t}\n}\n\nstatic int amd_uncore_add(struct perf_event *event, int flags)\n{\n\tint i;\n\tstruct amd_uncore *uncore = event_to_amd_uncore(event);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\t \n\tif (hwc->idx != -1 && uncore->events[hwc->idx] == event)\n\t\tgoto out;\n\n\tfor (i = 0; i < uncore->num_counters; i++) {\n\t\tif (uncore->events[i] == event) {\n\t\t\thwc->idx = i;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\thwc->idx = -1;\n\tfor (i = 0; i < uncore->num_counters; i++) {\n\t\tif (cmpxchg(&uncore->events[i], NULL, event) == NULL) {\n\t\t\thwc->idx = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\nout:\n\tif (hwc->idx == -1)\n\t\treturn -EBUSY;\n\n\thwc->config_base = uncore->msr_base + (2 * hwc->idx);\n\thwc->event_base = uncore->msr_base + 1 + (2 * hwc->idx);\n\thwc->event_base_rdpmc = uncore->rdpmc_base + hwc->idx;\n\thwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\n\n\t \n\tif (is_nb_event(event) && hwc->idx >= NUM_COUNTERS_NB)\n\t\thwc->event_base_rdpmc += NUM_COUNTERS_L3;\n\n\tif (flags & PERF_EF_START)\n\t\tamd_uncore_start(event, PERF_EF_RELOAD);\n\n\treturn 0;\n}\n\nstatic void amd_uncore_del(struct perf_event *event, int flags)\n{\n\tint i;\n\tstruct amd_uncore *uncore = event_to_amd_uncore(event);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tamd_uncore_stop(event, PERF_EF_UPDATE);\n\n\tfor (i = 0; i < uncore->num_counters; i++) {\n\t\tif (cmpxchg(&uncore->events[i], event, NULL) == event)\n\t\t\tbreak;\n\t}\n\n\thwc->idx = -1;\n}\n\n \nstatic u64 l3_thread_slice_mask(u64 config)\n{\n\tif (boot_cpu_data.x86 <= 0x18)\n\t\treturn ((config & AMD64_L3_SLICE_MASK) ? : AMD64_L3_SLICE_MASK) |\n\t\t       ((config & AMD64_L3_THREAD_MASK) ? : AMD64_L3_THREAD_MASK);\n\n\t \n\tif (!(config & AMD64_L3_F19H_THREAD_MASK))\n\t\treturn AMD64_L3_F19H_THREAD_MASK | AMD64_L3_EN_ALL_SLICES |\n\t\t       AMD64_L3_EN_ALL_CORES;\n\n\treturn config & (AMD64_L3_F19H_THREAD_MASK | AMD64_L3_SLICEID_MASK |\n\t\t\t AMD64_L3_EN_ALL_CORES | AMD64_L3_EN_ALL_SLICES |\n\t\t\t AMD64_L3_COREID_MASK);\n}\n\nstatic int amd_uncore_event_init(struct perf_event *event)\n{\n\tstruct amd_uncore *uncore;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 event_mask = AMD64_RAW_EVENT_MASK_NB;\n\n\tif (event->attr.type != event->pmu->type)\n\t\treturn -ENOENT;\n\n\tif (pmu_version >= 2 && is_nb_event(event))\n\t\tevent_mask = AMD64_PERFMON_V2_RAW_EVENT_MASK_NB;\n\n\t \n\thwc->config = event->attr.config & event_mask;\n\thwc->idx = -1;\n\n\tif (event->cpu < 0)\n\t\treturn -EINVAL;\n\n\t \n\tif (l3_mask && is_llc_event(event))\n\t\thwc->config |= l3_thread_slice_mask(event->attr.config);\n\n\tuncore = event_to_amd_uncore(event);\n\tif (!uncore)\n\t\treturn -ENODEV;\n\n\t \n\tevent->cpu = uncore->cpu;\n\n\treturn 0;\n}\n\nstatic umode_t\namd_f17h_uncore_is_visible(struct kobject *kobj, struct attribute *attr, int i)\n{\n\treturn boot_cpu_data.x86 >= 0x17 && boot_cpu_data.x86 < 0x19 ?\n\t       attr->mode : 0;\n}\n\nstatic umode_t\namd_f19h_uncore_is_visible(struct kobject *kobj, struct attribute *attr, int i)\n{\n\treturn boot_cpu_data.x86 >= 0x19 ? attr->mode : 0;\n}\n\nstatic ssize_t amd_uncore_attr_show_cpumask(struct device *dev,\n\t\t\t\t\t    struct device_attribute *attr,\n\t\t\t\t\t    char *buf)\n{\n\tcpumask_t *active_mask;\n\tstruct pmu *pmu = dev_get_drvdata(dev);\n\n\tif (pmu->type == amd_nb_pmu.type)\n\t\tactive_mask = &amd_nb_active_mask;\n\telse if (pmu->type == amd_llc_pmu.type)\n\t\tactive_mask = &amd_llc_active_mask;\n\telse\n\t\treturn 0;\n\n\treturn cpumap_print_to_pagebuf(true, buf, active_mask);\n}\nstatic DEVICE_ATTR(cpumask, S_IRUGO, amd_uncore_attr_show_cpumask, NULL);\n\nstatic struct attribute *amd_uncore_attrs[] = {\n\t&dev_attr_cpumask.attr,\n\tNULL,\n};\n\nstatic struct attribute_group amd_uncore_attr_group = {\n\t.attrs = amd_uncore_attrs,\n};\n\n#define DEFINE_UNCORE_FORMAT_ATTR(_var, _name, _format)\t\t\t\\\nstatic ssize_t __uncore_##_var##_show(struct device *dev,\t\t\\\n\t\t\t\tstruct device_attribute *attr,\t\t\\\n\t\t\t\tchar *page)\t\t\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tBUILD_BUG_ON(sizeof(_format) >= PAGE_SIZE);\t\t\t\\\n\treturn sprintf(page, _format \"\\n\");\t\t\t\t\\\n}\t\t\t\t\t\t\t\t\t\\\nstatic struct device_attribute format_attr_##_var =\t\t\t\\\n\t__ATTR(_name, 0444, __uncore_##_var##_show, NULL)\n\nDEFINE_UNCORE_FORMAT_ATTR(event12,\tevent,\t\t\"config:0-7,32-35\");\nDEFINE_UNCORE_FORMAT_ATTR(event14,\tevent,\t\t\"config:0-7,32-35,59-60\");  \nDEFINE_UNCORE_FORMAT_ATTR(event14v2,\tevent,\t\t\"config:0-7,32-37\");\t    \nDEFINE_UNCORE_FORMAT_ATTR(event8,\tevent,\t\t\"config:0-7\");\t\t    \nDEFINE_UNCORE_FORMAT_ATTR(umask8,\tumask,\t\t\"config:8-15\");\nDEFINE_UNCORE_FORMAT_ATTR(umask12,\tumask,\t\t\"config:8-15,24-27\");\t    \nDEFINE_UNCORE_FORMAT_ATTR(coreid,\tcoreid,\t\t\"config:42-44\");\t    \nDEFINE_UNCORE_FORMAT_ATTR(slicemask,\tslicemask,\t\"config:48-51\");\t    \nDEFINE_UNCORE_FORMAT_ATTR(threadmask8,\tthreadmask,\t\"config:56-63\");\t    \nDEFINE_UNCORE_FORMAT_ATTR(threadmask2,\tthreadmask,\t\"config:56-57\");\t    \nDEFINE_UNCORE_FORMAT_ATTR(enallslices,\tenallslices,\t\"config:46\");\t\t    \nDEFINE_UNCORE_FORMAT_ATTR(enallcores,\tenallcores,\t\"config:47\");\t\t    \nDEFINE_UNCORE_FORMAT_ATTR(sliceid,\tsliceid,\t\"config:48-50\");\t    \n\n \nstatic struct attribute *amd_uncore_df_format_attr[] = {\n\t&format_attr_event12.attr,\t \n\t&format_attr_umask8.attr,\t \n\tNULL,\n};\n\n \nstatic struct attribute *amd_uncore_l3_format_attr[] = {\n\t&format_attr_event12.attr,\t \n\t&format_attr_umask8.attr,\t \n\tNULL,\t\t\t\t \n\tNULL,\n};\n\n \nstatic struct attribute *amd_f17h_uncore_l3_format_attr[] = {\n\t&format_attr_slicemask.attr,\t \n\tNULL,\n};\n\n \nstatic struct attribute *amd_f19h_uncore_l3_format_attr[] = {\n\t&format_attr_coreid.attr,\t \n\t&format_attr_enallslices.attr,\t \n\t&format_attr_enallcores.attr,\t \n\t&format_attr_sliceid.attr,\t \n\tNULL,\n};\n\nstatic struct attribute_group amd_uncore_df_format_group = {\n\t.name = \"format\",\n\t.attrs = amd_uncore_df_format_attr,\n};\n\nstatic struct attribute_group amd_uncore_l3_format_group = {\n\t.name = \"format\",\n\t.attrs = amd_uncore_l3_format_attr,\n};\n\nstatic struct attribute_group amd_f17h_uncore_l3_format_group = {\n\t.name = \"format\",\n\t.attrs = amd_f17h_uncore_l3_format_attr,\n\t.is_visible = amd_f17h_uncore_is_visible,\n};\n\nstatic struct attribute_group amd_f19h_uncore_l3_format_group = {\n\t.name = \"format\",\n\t.attrs = amd_f19h_uncore_l3_format_attr,\n\t.is_visible = amd_f19h_uncore_is_visible,\n};\n\nstatic const struct attribute_group *amd_uncore_df_attr_groups[] = {\n\t&amd_uncore_attr_group,\n\t&amd_uncore_df_format_group,\n\tNULL,\n};\n\nstatic const struct attribute_group *amd_uncore_l3_attr_groups[] = {\n\t&amd_uncore_attr_group,\n\t&amd_uncore_l3_format_group,\n\tNULL,\n};\n\nstatic const struct attribute_group *amd_uncore_l3_attr_update[] = {\n\t&amd_f17h_uncore_l3_format_group,\n\t&amd_f19h_uncore_l3_format_group,\n\tNULL,\n};\n\nstatic struct pmu amd_nb_pmu = {\n\t.task_ctx_nr\t= perf_invalid_context,\n\t.attr_groups\t= amd_uncore_df_attr_groups,\n\t.name\t\t= \"amd_nb\",\n\t.event_init\t= amd_uncore_event_init,\n\t.add\t\t= amd_uncore_add,\n\t.del\t\t= amd_uncore_del,\n\t.start\t\t= amd_uncore_start,\n\t.stop\t\t= amd_uncore_stop,\n\t.read\t\t= amd_uncore_read,\n\t.capabilities\t= PERF_PMU_CAP_NO_EXCLUDE | PERF_PMU_CAP_NO_INTERRUPT,\n\t.module\t\t= THIS_MODULE,\n};\n\nstatic struct pmu amd_llc_pmu = {\n\t.task_ctx_nr\t= perf_invalid_context,\n\t.attr_groups\t= amd_uncore_l3_attr_groups,\n\t.attr_update\t= amd_uncore_l3_attr_update,\n\t.name\t\t= \"amd_l2\",\n\t.event_init\t= amd_uncore_event_init,\n\t.add\t\t= amd_uncore_add,\n\t.del\t\t= amd_uncore_del,\n\t.start\t\t= amd_uncore_start,\n\t.stop\t\t= amd_uncore_stop,\n\t.read\t\t= amd_uncore_read,\n\t.capabilities\t= PERF_PMU_CAP_NO_EXCLUDE | PERF_PMU_CAP_NO_INTERRUPT,\n\t.module\t\t= THIS_MODULE,\n};\n\nstatic struct amd_uncore *amd_uncore_alloc(unsigned int cpu)\n{\n\treturn kzalloc_node(sizeof(struct amd_uncore), GFP_KERNEL,\n\t\t\tcpu_to_node(cpu));\n}\n\nstatic inline struct perf_event **\namd_uncore_events_alloc(unsigned int num, unsigned int cpu)\n{\n\treturn kzalloc_node(sizeof(struct perf_event *) * num, GFP_KERNEL,\n\t\t\t    cpu_to_node(cpu));\n}\n\nstatic int amd_uncore_cpu_up_prepare(unsigned int cpu)\n{\n\tstruct amd_uncore *uncore_nb = NULL, *uncore_llc = NULL;\n\n\tif (amd_uncore_nb) {\n\t\t*per_cpu_ptr(amd_uncore_nb, cpu) = NULL;\n\t\tuncore_nb = amd_uncore_alloc(cpu);\n\t\tif (!uncore_nb)\n\t\t\tgoto fail;\n\t\tuncore_nb->cpu = cpu;\n\t\tuncore_nb->num_counters = num_counters_nb;\n\t\tuncore_nb->rdpmc_base = RDPMC_BASE_NB;\n\t\tuncore_nb->msr_base = MSR_F15H_NB_PERF_CTL;\n\t\tuncore_nb->active_mask = &amd_nb_active_mask;\n\t\tuncore_nb->pmu = &amd_nb_pmu;\n\t\tuncore_nb->events = amd_uncore_events_alloc(num_counters_nb, cpu);\n\t\tif (!uncore_nb->events)\n\t\t\tgoto fail;\n\t\tuncore_nb->id = -1;\n\t\t*per_cpu_ptr(amd_uncore_nb, cpu) = uncore_nb;\n\t}\n\n\tif (amd_uncore_llc) {\n\t\t*per_cpu_ptr(amd_uncore_llc, cpu) = NULL;\n\t\tuncore_llc = amd_uncore_alloc(cpu);\n\t\tif (!uncore_llc)\n\t\t\tgoto fail;\n\t\tuncore_llc->cpu = cpu;\n\t\tuncore_llc->num_counters = num_counters_llc;\n\t\tuncore_llc->rdpmc_base = RDPMC_BASE_LLC;\n\t\tuncore_llc->msr_base = MSR_F16H_L2I_PERF_CTL;\n\t\tuncore_llc->active_mask = &amd_llc_active_mask;\n\t\tuncore_llc->pmu = &amd_llc_pmu;\n\t\tuncore_llc->events = amd_uncore_events_alloc(num_counters_llc, cpu);\n\t\tif (!uncore_llc->events)\n\t\t\tgoto fail;\n\t\tuncore_llc->id = -1;\n\t\t*per_cpu_ptr(amd_uncore_llc, cpu) = uncore_llc;\n\t}\n\n\treturn 0;\n\nfail:\n\tif (uncore_nb) {\n\t\tkfree(uncore_nb->events);\n\t\tkfree(uncore_nb);\n\t}\n\n\tif (uncore_llc) {\n\t\tkfree(uncore_llc->events);\n\t\tkfree(uncore_llc);\n\t}\n\n\treturn -ENOMEM;\n}\n\nstatic struct amd_uncore *\namd_uncore_find_online_sibling(struct amd_uncore *this,\n\t\t\t       struct amd_uncore * __percpu *uncores)\n{\n\tunsigned int cpu;\n\tstruct amd_uncore *that;\n\n\tfor_each_online_cpu(cpu) {\n\t\tthat = *per_cpu_ptr(uncores, cpu);\n\n\t\tif (!that)\n\t\t\tcontinue;\n\n\t\tif (this == that)\n\t\t\tcontinue;\n\n\t\tif (this->id == that->id) {\n\t\t\thlist_add_head(&this->node, &uncore_unused_list);\n\t\t\tthis = that;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tthis->refcnt++;\n\treturn this;\n}\n\nstatic int amd_uncore_cpu_starting(unsigned int cpu)\n{\n\tunsigned int eax, ebx, ecx, edx;\n\tstruct amd_uncore *uncore;\n\n\tif (amd_uncore_nb) {\n\t\tuncore = *per_cpu_ptr(amd_uncore_nb, cpu);\n\t\tcpuid(0x8000001e, &eax, &ebx, &ecx, &edx);\n\t\tuncore->id = ecx & 0xff;\n\n\t\tuncore = amd_uncore_find_online_sibling(uncore, amd_uncore_nb);\n\t\t*per_cpu_ptr(amd_uncore_nb, cpu) = uncore;\n\t}\n\n\tif (amd_uncore_llc) {\n\t\tuncore = *per_cpu_ptr(amd_uncore_llc, cpu);\n\t\tuncore->id = get_llc_id(cpu);\n\n\t\tuncore = amd_uncore_find_online_sibling(uncore, amd_uncore_llc);\n\t\t*per_cpu_ptr(amd_uncore_llc, cpu) = uncore;\n\t}\n\n\treturn 0;\n}\n\nstatic void uncore_clean_online(void)\n{\n\tstruct amd_uncore *uncore;\n\tstruct hlist_node *n;\n\n\thlist_for_each_entry_safe(uncore, n, &uncore_unused_list, node) {\n\t\thlist_del(&uncore->node);\n\t\tkfree(uncore->events);\n\t\tkfree(uncore);\n\t}\n}\n\nstatic void uncore_online(unsigned int cpu,\n\t\t\t  struct amd_uncore * __percpu *uncores)\n{\n\tstruct amd_uncore *uncore = *per_cpu_ptr(uncores, cpu);\n\n\tuncore_clean_online();\n\n\tif (cpu == uncore->cpu)\n\t\tcpumask_set_cpu(cpu, uncore->active_mask);\n}\n\nstatic int amd_uncore_cpu_online(unsigned int cpu)\n{\n\tif (amd_uncore_nb)\n\t\tuncore_online(cpu, amd_uncore_nb);\n\n\tif (amd_uncore_llc)\n\t\tuncore_online(cpu, amd_uncore_llc);\n\n\treturn 0;\n}\n\nstatic void uncore_down_prepare(unsigned int cpu,\n\t\t\t\tstruct amd_uncore * __percpu *uncores)\n{\n\tunsigned int i;\n\tstruct amd_uncore *this = *per_cpu_ptr(uncores, cpu);\n\n\tif (this->cpu != cpu)\n\t\treturn;\n\n\t \n\tfor_each_online_cpu(i) {\n\t\tstruct amd_uncore *that = *per_cpu_ptr(uncores, i);\n\n\t\tif (cpu == i)\n\t\t\tcontinue;\n\n\t\tif (this == that) {\n\t\t\tperf_pmu_migrate_context(this->pmu, cpu, i);\n\t\t\tcpumask_clear_cpu(cpu, that->active_mask);\n\t\t\tcpumask_set_cpu(i, that->active_mask);\n\t\t\tthat->cpu = i;\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\nstatic int amd_uncore_cpu_down_prepare(unsigned int cpu)\n{\n\tif (amd_uncore_nb)\n\t\tuncore_down_prepare(cpu, amd_uncore_nb);\n\n\tif (amd_uncore_llc)\n\t\tuncore_down_prepare(cpu, amd_uncore_llc);\n\n\treturn 0;\n}\n\nstatic void uncore_dead(unsigned int cpu, struct amd_uncore * __percpu *uncores)\n{\n\tstruct amd_uncore *uncore = *per_cpu_ptr(uncores, cpu);\n\n\tif (cpu == uncore->cpu)\n\t\tcpumask_clear_cpu(cpu, uncore->active_mask);\n\n\tif (!--uncore->refcnt) {\n\t\tkfree(uncore->events);\n\t\tkfree(uncore);\n\t}\n\n\t*per_cpu_ptr(uncores, cpu) = NULL;\n}\n\nstatic int amd_uncore_cpu_dead(unsigned int cpu)\n{\n\tif (amd_uncore_nb)\n\t\tuncore_dead(cpu, amd_uncore_nb);\n\n\tif (amd_uncore_llc)\n\t\tuncore_dead(cpu, amd_uncore_llc);\n\n\treturn 0;\n}\n\nstatic int __init amd_uncore_init(void)\n{\n\tstruct attribute **df_attr = amd_uncore_df_format_attr;\n\tstruct attribute **l3_attr = amd_uncore_l3_format_attr;\n\tunion cpuid_0x80000022_ebx ebx;\n\tint ret = -ENODEV;\n\n\tif (boot_cpu_data.x86_vendor != X86_VENDOR_AMD &&\n\t    boot_cpu_data.x86_vendor != X86_VENDOR_HYGON)\n\t\treturn -ENODEV;\n\n\tif (!boot_cpu_has(X86_FEATURE_TOPOEXT))\n\t\treturn -ENODEV;\n\n\tif (boot_cpu_has(X86_FEATURE_PERFMON_V2))\n\t\tpmu_version = 2;\n\n\tnum_counters_nb\t= NUM_COUNTERS_NB;\n\tnum_counters_llc = NUM_COUNTERS_L2;\n\tif (boot_cpu_data.x86 >= 0x17) {\n\t\t \n\t\tnum_counters_llc\t  = NUM_COUNTERS_L3;\n\t\tamd_nb_pmu.name\t\t  = \"amd_df\";\n\t\tamd_llc_pmu.name\t  = \"amd_l3\";\n\t\tl3_mask\t\t\t  = true;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_PERFCTR_NB)) {\n\t\tif (pmu_version >= 2) {\n\t\t\t*df_attr++ = &format_attr_event14v2.attr;\n\t\t\t*df_attr++ = &format_attr_umask12.attr;\n\t\t} else if (boot_cpu_data.x86 >= 0x17) {\n\t\t\t*df_attr = &format_attr_event14.attr;\n\t\t}\n\n\t\tamd_uncore_nb = alloc_percpu(struct amd_uncore *);\n\t\tif (!amd_uncore_nb) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto fail_nb;\n\t\t}\n\t\tret = perf_pmu_register(&amd_nb_pmu, amd_nb_pmu.name, -1);\n\t\tif (ret)\n\t\t\tgoto fail_nb;\n\n\t\tif (pmu_version >= 2) {\n\t\t\tebx.full = cpuid_ebx(EXT_PERFMON_DEBUG_FEATURES);\n\t\t\tnum_counters_nb = ebx.split.num_df_pmc;\n\t\t}\n\n\t\tpr_info(\"%d %s %s counters detected\\n\", num_counters_nb,\n\t\t\tboot_cpu_data.x86_vendor == X86_VENDOR_HYGON ?  \"HYGON\" : \"\",\n\t\t\tamd_nb_pmu.name);\n\n\t\tret = 0;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_PERFCTR_LLC)) {\n\t\tif (boot_cpu_data.x86 >= 0x19) {\n\t\t\t*l3_attr++ = &format_attr_event8.attr;\n\t\t\t*l3_attr++ = &format_attr_umask8.attr;\n\t\t\t*l3_attr++ = &format_attr_threadmask2.attr;\n\t\t} else if (boot_cpu_data.x86 >= 0x17) {\n\t\t\t*l3_attr++ = &format_attr_event8.attr;\n\t\t\t*l3_attr++ = &format_attr_umask8.attr;\n\t\t\t*l3_attr++ = &format_attr_threadmask8.attr;\n\t\t}\n\n\t\tamd_uncore_llc = alloc_percpu(struct amd_uncore *);\n\t\tif (!amd_uncore_llc) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto fail_llc;\n\t\t}\n\t\tret = perf_pmu_register(&amd_llc_pmu, amd_llc_pmu.name, -1);\n\t\tif (ret)\n\t\t\tgoto fail_llc;\n\n\t\tpr_info(\"%d %s %s counters detected\\n\", num_counters_llc,\n\t\t\tboot_cpu_data.x86_vendor == X86_VENDOR_HYGON ?  \"HYGON\" : \"\",\n\t\t\tamd_llc_pmu.name);\n\t\tret = 0;\n\t}\n\n\t \n\tif (cpuhp_setup_state(CPUHP_PERF_X86_AMD_UNCORE_PREP,\n\t\t\t      \"perf/x86/amd/uncore:prepare\",\n\t\t\t      amd_uncore_cpu_up_prepare, amd_uncore_cpu_dead))\n\t\tgoto fail_llc;\n\n\tif (cpuhp_setup_state(CPUHP_AP_PERF_X86_AMD_UNCORE_STARTING,\n\t\t\t      \"perf/x86/amd/uncore:starting\",\n\t\t\t      amd_uncore_cpu_starting, NULL))\n\t\tgoto fail_prep;\n\tif (cpuhp_setup_state(CPUHP_AP_PERF_X86_AMD_UNCORE_ONLINE,\n\t\t\t      \"perf/x86/amd/uncore:online\",\n\t\t\t      amd_uncore_cpu_online,\n\t\t\t      amd_uncore_cpu_down_prepare))\n\t\tgoto fail_start;\n\treturn 0;\n\nfail_start:\n\tcpuhp_remove_state(CPUHP_AP_PERF_X86_AMD_UNCORE_STARTING);\nfail_prep:\n\tcpuhp_remove_state(CPUHP_PERF_X86_AMD_UNCORE_PREP);\nfail_llc:\n\tif (boot_cpu_has(X86_FEATURE_PERFCTR_NB))\n\t\tperf_pmu_unregister(&amd_nb_pmu);\n\tfree_percpu(amd_uncore_llc);\nfail_nb:\n\tfree_percpu(amd_uncore_nb);\n\n\treturn ret;\n}\n\nstatic void __exit amd_uncore_exit(void)\n{\n\tcpuhp_remove_state(CPUHP_AP_PERF_X86_AMD_UNCORE_ONLINE);\n\tcpuhp_remove_state(CPUHP_AP_PERF_X86_AMD_UNCORE_STARTING);\n\tcpuhp_remove_state(CPUHP_PERF_X86_AMD_UNCORE_PREP);\n\n\tif (boot_cpu_has(X86_FEATURE_PERFCTR_LLC)) {\n\t\tperf_pmu_unregister(&amd_llc_pmu);\n\t\tfree_percpu(amd_uncore_llc);\n\t\tamd_uncore_llc = NULL;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_PERFCTR_NB)) {\n\t\tperf_pmu_unregister(&amd_nb_pmu);\n\t\tfree_percpu(amd_uncore_nb);\n\t\tamd_uncore_nb = NULL;\n\t}\n}\n\nmodule_init(amd_uncore_init);\nmodule_exit(amd_uncore_exit);\n\nMODULE_DESCRIPTION(\"AMD Uncore Driver\");\nMODULE_LICENSE(\"GPL v2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}