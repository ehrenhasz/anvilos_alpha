{
  "module_name": "core.c",
  "hash_id": "211b08dbfa3131d226e71ad9819b67b9e613d7e0bcd157d4b4dd13c671b2e2f1",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/events/core.c",
  "human_readable_source": " \n\n#include <linux/perf_event.h>\n#include <linux/capability.h>\n#include <linux/notifier.h>\n#include <linux/hardirq.h>\n#include <linux/kprobes.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/kdebug.h>\n#include <linux/sched/mm.h>\n#include <linux/sched/clock.h>\n#include <linux/uaccess.h>\n#include <linux/slab.h>\n#include <linux/cpu.h>\n#include <linux/bitops.h>\n#include <linux/device.h>\n#include <linux/nospec.h>\n#include <linux/static_call.h>\n\n#include <asm/apic.h>\n#include <asm/stacktrace.h>\n#include <asm/nmi.h>\n#include <asm/smp.h>\n#include <asm/alternative.h>\n#include <asm/mmu_context.h>\n#include <asm/tlbflush.h>\n#include <asm/timer.h>\n#include <asm/desc.h>\n#include <asm/ldt.h>\n#include <asm/unwind.h>\n\n#include \"perf_event.h\"\n\nstruct x86_pmu x86_pmu __read_mostly;\nstatic struct pmu pmu;\n\nDEFINE_PER_CPU(struct cpu_hw_events, cpu_hw_events) = {\n\t.enabled = 1,\n\t.pmu = &pmu,\n};\n\nDEFINE_STATIC_KEY_FALSE(rdpmc_never_available_key);\nDEFINE_STATIC_KEY_FALSE(rdpmc_always_available_key);\nDEFINE_STATIC_KEY_FALSE(perf_is_hybrid);\n\n \nDEFINE_STATIC_CALL_NULL(x86_pmu_handle_irq,  *x86_pmu.handle_irq);\nDEFINE_STATIC_CALL_NULL(x86_pmu_disable_all, *x86_pmu.disable_all);\nDEFINE_STATIC_CALL_NULL(x86_pmu_enable_all,  *x86_pmu.enable_all);\nDEFINE_STATIC_CALL_NULL(x86_pmu_enable,\t     *x86_pmu.enable);\nDEFINE_STATIC_CALL_NULL(x86_pmu_disable,     *x86_pmu.disable);\n\nDEFINE_STATIC_CALL_NULL(x86_pmu_assign, *x86_pmu.assign);\n\nDEFINE_STATIC_CALL_NULL(x86_pmu_add,  *x86_pmu.add);\nDEFINE_STATIC_CALL_NULL(x86_pmu_del,  *x86_pmu.del);\nDEFINE_STATIC_CALL_NULL(x86_pmu_read, *x86_pmu.read);\n\nDEFINE_STATIC_CALL_NULL(x86_pmu_set_period,   *x86_pmu.set_period);\nDEFINE_STATIC_CALL_NULL(x86_pmu_update,       *x86_pmu.update);\nDEFINE_STATIC_CALL_NULL(x86_pmu_limit_period, *x86_pmu.limit_period);\n\nDEFINE_STATIC_CALL_NULL(x86_pmu_schedule_events,       *x86_pmu.schedule_events);\nDEFINE_STATIC_CALL_NULL(x86_pmu_get_event_constraints, *x86_pmu.get_event_constraints);\nDEFINE_STATIC_CALL_NULL(x86_pmu_put_event_constraints, *x86_pmu.put_event_constraints);\n\nDEFINE_STATIC_CALL_NULL(x86_pmu_start_scheduling,  *x86_pmu.start_scheduling);\nDEFINE_STATIC_CALL_NULL(x86_pmu_commit_scheduling, *x86_pmu.commit_scheduling);\nDEFINE_STATIC_CALL_NULL(x86_pmu_stop_scheduling,   *x86_pmu.stop_scheduling);\n\nDEFINE_STATIC_CALL_NULL(x86_pmu_sched_task,    *x86_pmu.sched_task);\nDEFINE_STATIC_CALL_NULL(x86_pmu_swap_task_ctx, *x86_pmu.swap_task_ctx);\n\nDEFINE_STATIC_CALL_NULL(x86_pmu_drain_pebs,   *x86_pmu.drain_pebs);\nDEFINE_STATIC_CALL_NULL(x86_pmu_pebs_aliases, *x86_pmu.pebs_aliases);\n\nDEFINE_STATIC_CALL_NULL(x86_pmu_filter, *x86_pmu.filter);\n\n \nDEFINE_STATIC_CALL_RET0(x86_pmu_guest_get_msrs, *x86_pmu.guest_get_msrs);\n\nu64 __read_mostly hw_cache_event_ids\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX];\nu64 __read_mostly hw_cache_extra_regs\n\t\t\t\t[PERF_COUNT_HW_CACHE_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_OP_MAX]\n\t\t\t\t[PERF_COUNT_HW_CACHE_RESULT_MAX];\n\n \nu64 x86_perf_event_update(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint shift = 64 - x86_pmu.cntval_bits;\n\tu64 prev_raw_count, new_raw_count;\n\tu64 delta;\n\n\tif (unlikely(!hwc->event_base))\n\t\treturn 0;\n\n\t \n\tprev_raw_count = local64_read(&hwc->prev_count);\n\tdo {\n\t\trdpmcl(hwc->event_base_rdpmc, new_raw_count);\n\t} while (!local64_try_cmpxchg(&hwc->prev_count,\n\t\t\t\t      &prev_raw_count, new_raw_count));\n\n\t \n\tdelta = (new_raw_count << shift) - (prev_raw_count << shift);\n\tdelta >>= shift;\n\n\tlocal64_add(delta, &event->count);\n\tlocal64_sub(delta, &hwc->period_left);\n\n\treturn new_raw_count;\n}\n\n \nstatic int x86_pmu_extra_regs(u64 config, struct perf_event *event)\n{\n\tstruct extra_reg *extra_regs = hybrid(event->pmu, extra_regs);\n\tstruct hw_perf_event_extra *reg;\n\tstruct extra_reg *er;\n\n\treg = &event->hw.extra_reg;\n\n\tif (!extra_regs)\n\t\treturn 0;\n\n\tfor (er = extra_regs; er->msr; er++) {\n\t\tif (er->event != (config & er->config_mask))\n\t\t\tcontinue;\n\t\tif (event->attr.config1 & ~er->valid_mask)\n\t\t\treturn -EINVAL;\n\t\t \n\t\tif (!er->extra_msr_access)\n\t\t\treturn -ENXIO;\n\n\t\treg->idx = er->idx;\n\t\treg->config = event->attr.config1;\n\t\treg->reg = er->msr;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic atomic_t active_events;\nstatic atomic_t pmc_refcount;\nstatic DEFINE_MUTEX(pmc_reserve_mutex);\n\n#ifdef CONFIG_X86_LOCAL_APIC\n\nstatic inline int get_possible_num_counters(void)\n{\n\tint i, num_counters = x86_pmu.num_counters;\n\n\tif (!is_hybrid())\n\t\treturn num_counters;\n\n\tfor (i = 0; i < x86_pmu.num_hybrid_pmus; i++)\n\t\tnum_counters = max_t(int, num_counters, x86_pmu.hybrid_pmu[i].num_counters);\n\n\treturn num_counters;\n}\n\nstatic bool reserve_pmc_hardware(void)\n{\n\tint i, num_counters = get_possible_num_counters();\n\n\tfor (i = 0; i < num_counters; i++) {\n\t\tif (!reserve_perfctr_nmi(x86_pmu_event_addr(i)))\n\t\t\tgoto perfctr_fail;\n\t}\n\n\tfor (i = 0; i < num_counters; i++) {\n\t\tif (!reserve_evntsel_nmi(x86_pmu_config_addr(i)))\n\t\t\tgoto eventsel_fail;\n\t}\n\n\treturn true;\n\neventsel_fail:\n\tfor (i--; i >= 0; i--)\n\t\trelease_evntsel_nmi(x86_pmu_config_addr(i));\n\n\ti = num_counters;\n\nperfctr_fail:\n\tfor (i--; i >= 0; i--)\n\t\trelease_perfctr_nmi(x86_pmu_event_addr(i));\n\n\treturn false;\n}\n\nstatic void release_pmc_hardware(void)\n{\n\tint i, num_counters = get_possible_num_counters();\n\n\tfor (i = 0; i < num_counters; i++) {\n\t\trelease_perfctr_nmi(x86_pmu_event_addr(i));\n\t\trelease_evntsel_nmi(x86_pmu_config_addr(i));\n\t}\n}\n\n#else\n\nstatic bool reserve_pmc_hardware(void) { return true; }\nstatic void release_pmc_hardware(void) {}\n\n#endif\n\nbool check_hw_exists(struct pmu *pmu, int num_counters, int num_counters_fixed)\n{\n\tu64 val, val_fail = -1, val_new= ~0;\n\tint i, reg, reg_fail = -1, ret = 0;\n\tint bios_fail = 0;\n\tint reg_safe = -1;\n\n\t \n\tfor (i = 0; i < num_counters; i++) {\n\t\treg = x86_pmu_config_addr(i);\n\t\tret = rdmsrl_safe(reg, &val);\n\t\tif (ret)\n\t\t\tgoto msr_fail;\n\t\tif (val & ARCH_PERFMON_EVENTSEL_ENABLE) {\n\t\t\tbios_fail = 1;\n\t\t\tval_fail = val;\n\t\t\treg_fail = reg;\n\t\t} else {\n\t\t\treg_safe = i;\n\t\t}\n\t}\n\n\tif (num_counters_fixed) {\n\t\treg = MSR_ARCH_PERFMON_FIXED_CTR_CTRL;\n\t\tret = rdmsrl_safe(reg, &val);\n\t\tif (ret)\n\t\t\tgoto msr_fail;\n\t\tfor (i = 0; i < num_counters_fixed; i++) {\n\t\t\tif (fixed_counter_disabled(i, pmu))\n\t\t\t\tcontinue;\n\t\t\tif (val & (0x03ULL << i*4)) {\n\t\t\t\tbios_fail = 1;\n\t\t\t\tval_fail = val;\n\t\t\t\treg_fail = reg;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\n\tif (reg_safe == -1) {\n\t\treg = reg_safe;\n\t\tgoto msr_fail;\n\t}\n\n\t \n\treg = x86_pmu_event_addr(reg_safe);\n\tif (rdmsrl_safe(reg, &val))\n\t\tgoto msr_fail;\n\tval ^= 0xffffUL;\n\tret = wrmsrl_safe(reg, val);\n\tret |= rdmsrl_safe(reg, &val_new);\n\tif (ret || val != val_new)\n\t\tgoto msr_fail;\n\n\t \n\tif (bios_fail) {\n\t\tpr_cont(\"Broken BIOS detected, complain to your hardware vendor.\\n\");\n\t\tpr_err(FW_BUG \"the BIOS has corrupted hw-PMU resources (MSR %x is %Lx)\\n\",\n\t\t\t      reg_fail, val_fail);\n\t}\n\n\treturn true;\n\nmsr_fail:\n\tif (boot_cpu_has(X86_FEATURE_HYPERVISOR)) {\n\t\tpr_cont(\"PMU not available due to virtualization, using software events only.\\n\");\n\t} else {\n\t\tpr_cont(\"Broken PMU hardware detected, using software events only.\\n\");\n\t\tpr_err(\"Failed to access perfctr msr (MSR %x is %Lx)\\n\",\n\t\t       reg, val_new);\n\t}\n\n\treturn false;\n}\n\nstatic void hw_perf_event_destroy(struct perf_event *event)\n{\n\tx86_release_hardware();\n\tatomic_dec(&active_events);\n}\n\nvoid hw_perf_lbr_event_destroy(struct perf_event *event)\n{\n\thw_perf_event_destroy(event);\n\n\t \n\tx86_del_exclusive(x86_lbr_exclusive_lbr);\n}\n\nstatic inline int x86_pmu_initialized(void)\n{\n\treturn x86_pmu.handle_irq != NULL;\n}\n\nstatic inline int\nset_ext_hw_attr(struct hw_perf_event *hwc, struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\tunsigned int cache_type, cache_op, cache_result;\n\tu64 config, val;\n\n\tconfig = attr->config;\n\n\tcache_type = (config >> 0) & 0xff;\n\tif (cache_type >= PERF_COUNT_HW_CACHE_MAX)\n\t\treturn -EINVAL;\n\tcache_type = array_index_nospec(cache_type, PERF_COUNT_HW_CACHE_MAX);\n\n\tcache_op = (config >>  8) & 0xff;\n\tif (cache_op >= PERF_COUNT_HW_CACHE_OP_MAX)\n\t\treturn -EINVAL;\n\tcache_op = array_index_nospec(cache_op, PERF_COUNT_HW_CACHE_OP_MAX);\n\n\tcache_result = (config >> 16) & 0xff;\n\tif (cache_result >= PERF_COUNT_HW_CACHE_RESULT_MAX)\n\t\treturn -EINVAL;\n\tcache_result = array_index_nospec(cache_result, PERF_COUNT_HW_CACHE_RESULT_MAX);\n\n\tval = hybrid_var(event->pmu, hw_cache_event_ids)[cache_type][cache_op][cache_result];\n\tif (val == 0)\n\t\treturn -ENOENT;\n\n\tif (val == -1)\n\t\treturn -EINVAL;\n\n\thwc->config |= val;\n\tattr->config1 = hybrid_var(event->pmu, hw_cache_extra_regs)[cache_type][cache_op][cache_result];\n\treturn x86_pmu_extra_regs(val, event);\n}\n\nint x86_reserve_hardware(void)\n{\n\tint err = 0;\n\n\tif (!atomic_inc_not_zero(&pmc_refcount)) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tif (atomic_read(&pmc_refcount) == 0) {\n\t\t\tif (!reserve_pmc_hardware()) {\n\t\t\t\terr = -EBUSY;\n\t\t\t} else {\n\t\t\t\treserve_ds_buffers();\n\t\t\t\treserve_lbr_buffers();\n\t\t\t}\n\t\t}\n\t\tif (!err)\n\t\t\tatomic_inc(&pmc_refcount);\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n\n\treturn err;\n}\n\nvoid x86_release_hardware(void)\n{\n\tif (atomic_dec_and_mutex_lock(&pmc_refcount, &pmc_reserve_mutex)) {\n\t\trelease_pmc_hardware();\n\t\trelease_ds_buffers();\n\t\trelease_lbr_buffers();\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n}\n\n \nint x86_add_exclusive(unsigned int what)\n{\n\tint i;\n\n\t \n\tif (x86_pmu.lbr_pt_coexist && what == x86_lbr_exclusive_pt)\n\t\tgoto out;\n\n\tif (!atomic_inc_not_zero(&x86_pmu.lbr_exclusive[what])) {\n\t\tmutex_lock(&pmc_reserve_mutex);\n\t\tfor (i = 0; i < ARRAY_SIZE(x86_pmu.lbr_exclusive); i++) {\n\t\t\tif (i != what && atomic_read(&x86_pmu.lbr_exclusive[i]))\n\t\t\t\tgoto fail_unlock;\n\t\t}\n\t\tatomic_inc(&x86_pmu.lbr_exclusive[what]);\n\t\tmutex_unlock(&pmc_reserve_mutex);\n\t}\n\nout:\n\tatomic_inc(&active_events);\n\treturn 0;\n\nfail_unlock:\n\tmutex_unlock(&pmc_reserve_mutex);\n\treturn -EBUSY;\n}\n\nvoid x86_del_exclusive(unsigned int what)\n{\n\tatomic_dec(&active_events);\n\n\t \n\tif (x86_pmu.lbr_pt_coexist && what == x86_lbr_exclusive_pt)\n\t\treturn;\n\n\tatomic_dec(&x86_pmu.lbr_exclusive[what]);\n}\n\nint x86_setup_perfctr(struct perf_event *event)\n{\n\tstruct perf_event_attr *attr = &event->attr;\n\tstruct hw_perf_event *hwc = &event->hw;\n\tu64 config;\n\n\tif (!is_sampling_event(event)) {\n\t\thwc->sample_period = x86_pmu.max_period;\n\t\thwc->last_period = hwc->sample_period;\n\t\tlocal64_set(&hwc->period_left, hwc->sample_period);\n\t}\n\n\tif (attr->type == event->pmu->type)\n\t\treturn x86_pmu_extra_regs(event->attr.config, event);\n\n\tif (attr->type == PERF_TYPE_HW_CACHE)\n\t\treturn set_ext_hw_attr(hwc, event);\n\n\tif (attr->config >= x86_pmu.max_events)\n\t\treturn -EINVAL;\n\n\tattr->config = array_index_nospec((unsigned long)attr->config, x86_pmu.max_events);\n\n\t \n\tconfig = x86_pmu.event_map(attr->config);\n\n\tif (config == 0)\n\t\treturn -ENOENT;\n\n\tif (config == -1LL)\n\t\treturn -EINVAL;\n\n\thwc->config |= config;\n\n\treturn 0;\n}\n\n \nstatic inline int precise_br_compat(struct perf_event *event)\n{\n\tu64 m = event->attr.branch_sample_type;\n\tu64 b = 0;\n\n\t \n\tif (!(m & PERF_SAMPLE_BRANCH_ANY))\n\t\treturn 0;\n\n\tm &= PERF_SAMPLE_BRANCH_KERNEL | PERF_SAMPLE_BRANCH_USER;\n\n\tif (!event->attr.exclude_user)\n\t\tb |= PERF_SAMPLE_BRANCH_USER;\n\n\tif (!event->attr.exclude_kernel)\n\t\tb |= PERF_SAMPLE_BRANCH_KERNEL;\n\n\t \n\n\treturn m == b;\n}\n\nint x86_pmu_max_precise(void)\n{\n\tint precise = 0;\n\n\t \n\tif (x86_pmu.pebs_active && !x86_pmu.pebs_broken) {\n\t\tprecise++;\n\n\t\t \n\t\tif (x86_pmu.lbr_nr || x86_pmu.intel_cap.pebs_format >= 2)\n\t\t\tprecise++;\n\n\t\tif (x86_pmu.pebs_prec_dist)\n\t\t\tprecise++;\n\t}\n\treturn precise;\n}\n\nint x86_pmu_hw_config(struct perf_event *event)\n{\n\tif (event->attr.precise_ip) {\n\t\tint precise = x86_pmu_max_precise();\n\n\t\tif (event->attr.precise_ip > precise)\n\t\t\treturn -EOPNOTSUPP;\n\n\t\t \n\t\tif (!is_sampling_event(event))\n\t\t\treturn -EINVAL;\n\t}\n\t \n\tif (event->attr.precise_ip > 1 && x86_pmu.intel_cap.pebs_format < 2) {\n\t\tu64 *br_type = &event->attr.branch_sample_type;\n\n\t\tif (has_branch_stack(event)) {\n\t\t\tif (!precise_br_compat(event))\n\t\t\t\treturn -EOPNOTSUPP;\n\n\t\t\t \n\n\t\t} else {\n\t\t\t \n\t\t\t*br_type = PERF_SAMPLE_BRANCH_ANY;\n\n\t\t\tif (!event->attr.exclude_user)\n\t\t\t\t*br_type |= PERF_SAMPLE_BRANCH_USER;\n\n\t\t\tif (!event->attr.exclude_kernel)\n\t\t\t\t*br_type |= PERF_SAMPLE_BRANCH_KERNEL;\n\t\t}\n\t}\n\n\tif (event->attr.branch_sample_type & PERF_SAMPLE_BRANCH_CALL_STACK)\n\t\tevent->attach_state |= PERF_ATTACH_TASK_DATA;\n\n\t \n\tevent->hw.config = ARCH_PERFMON_EVENTSEL_INT;\n\n\t \n\tif (!event->attr.exclude_user)\n\t\tevent->hw.config |= ARCH_PERFMON_EVENTSEL_USR;\n\tif (!event->attr.exclude_kernel)\n\t\tevent->hw.config |= ARCH_PERFMON_EVENTSEL_OS;\n\n\tif (event->attr.type == event->pmu->type)\n\t\tevent->hw.config |= event->attr.config & X86_RAW_EVENT_MASK;\n\n\tif (event->attr.sample_period && x86_pmu.limit_period) {\n\t\ts64 left = event->attr.sample_period;\n\t\tx86_pmu.limit_period(event, &left);\n\t\tif (left > event->attr.sample_period)\n\t\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (unlikely(event->attr.sample_regs_user & PERF_REG_EXTENDED_MASK))\n\t\treturn -EINVAL;\n\t \n\tif (unlikely(event->attr.sample_regs_intr & PERF_REG_EXTENDED_MASK)) {\n\t\tif (!(event->pmu->capabilities & PERF_PMU_CAP_EXTENDED_REGS))\n\t\t\treturn -EINVAL;\n\n\t\tif (!event->attr.precise_ip)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn x86_setup_perfctr(event);\n}\n\n \nstatic int __x86_pmu_event_init(struct perf_event *event)\n{\n\tint err;\n\n\tif (!x86_pmu_initialized())\n\t\treturn -ENODEV;\n\n\terr = x86_reserve_hardware();\n\tif (err)\n\t\treturn err;\n\n\tatomic_inc(&active_events);\n\tevent->destroy = hw_perf_event_destroy;\n\n\tevent->hw.idx = -1;\n\tevent->hw.last_cpu = -1;\n\tevent->hw.last_tag = ~0ULL;\n\n\t \n\tevent->hw.extra_reg.idx = EXTRA_REG_NONE;\n\tevent->hw.branch_reg.idx = EXTRA_REG_NONE;\n\n\treturn x86_pmu.hw_config(event);\n}\n\nvoid x86_pmu_disable_all(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tint idx;\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tstruct hw_perf_event *hwc = &cpuc->events[idx]->hw;\n\t\tu64 val;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\t\trdmsrl(x86_pmu_config_addr(idx), val);\n\t\tif (!(val & ARCH_PERFMON_EVENTSEL_ENABLE))\n\t\t\tcontinue;\n\t\tval &= ~ARCH_PERFMON_EVENTSEL_ENABLE;\n\t\twrmsrl(x86_pmu_config_addr(idx), val);\n\t\tif (is_counter_pair(hwc))\n\t\t\twrmsrl(x86_pmu_config_addr(idx + 1), 0);\n\t}\n}\n\nstruct perf_guest_switch_msr *perf_guest_get_msrs(int *nr, void *data)\n{\n\treturn static_call(x86_pmu_guest_get_msrs)(nr, data);\n}\nEXPORT_SYMBOL_GPL(perf_guest_get_msrs);\n\n \nstatic void x86_pmu_disable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tif (!x86_pmu_initialized())\n\t\treturn;\n\n\tif (!cpuc->enabled)\n\t\treturn;\n\n\tcpuc->n_added = 0;\n\tcpuc->enabled = 0;\n\tbarrier();\n\n\tstatic_call(x86_pmu_disable_all)();\n}\n\nvoid x86_pmu_enable_all(int added)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tint idx;\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tstruct hw_perf_event *hwc = &cpuc->events[idx]->hw;\n\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\t__x86_pmu_enable_event(hwc, ARCH_PERFMON_EVENTSEL_ENABLE);\n\t}\n}\n\nstatic inline int is_x86_event(struct perf_event *event)\n{\n\tint i;\n\n\tif (!is_hybrid())\n\t\treturn event->pmu == &pmu;\n\n\tfor (i = 0; i < x86_pmu.num_hybrid_pmus; i++) {\n\t\tif (event->pmu == &x86_pmu.hybrid_pmu[i].pmu)\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstruct pmu *x86_get_pmu(unsigned int cpu)\n{\n\tstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\n\n\t \n\tif (WARN_ON_ONCE(!cpuc->pmu))\n\t\treturn &pmu;\n\n\treturn cpuc->pmu;\n}\n \nstruct sched_state {\n\tint\tweight;\n\tint\tevent;\t\t \n\tint\tcounter;\t \n\tint\tunassigned;\t \n\tint\tnr_gp;\t\t \n\tu64\tused;\n};\n\n \n#define\tSCHED_STATES_MAX\t2\n\nstruct perf_sched {\n\tint\t\t\tmax_weight;\n\tint\t\t\tmax_events;\n\tint\t\t\tmax_gp;\n\tint\t\t\tsaved_states;\n\tstruct event_constraint\t**constraints;\n\tstruct sched_state\tstate;\n\tstruct sched_state\tsaved[SCHED_STATES_MAX];\n};\n\n \nstatic void perf_sched_init(struct perf_sched *sched, struct event_constraint **constraints,\n\t\t\t    int num, int wmin, int wmax, int gpmax)\n{\n\tint idx;\n\n\tmemset(sched, 0, sizeof(*sched));\n\tsched->max_events\t= num;\n\tsched->max_weight\t= wmax;\n\tsched->max_gp\t\t= gpmax;\n\tsched->constraints\t= constraints;\n\n\tfor (idx = 0; idx < num; idx++) {\n\t\tif (constraints[idx]->weight == wmin)\n\t\t\tbreak;\n\t}\n\n\tsched->state.event\t= idx;\t\t \n\tsched->state.weight\t= wmin;\n\tsched->state.unassigned\t= num;\n}\n\nstatic void perf_sched_save_state(struct perf_sched *sched)\n{\n\tif (WARN_ON_ONCE(sched->saved_states >= SCHED_STATES_MAX))\n\t\treturn;\n\n\tsched->saved[sched->saved_states] = sched->state;\n\tsched->saved_states++;\n}\n\nstatic bool perf_sched_restore_state(struct perf_sched *sched)\n{\n\tif (!sched->saved_states)\n\t\treturn false;\n\n\tsched->saved_states--;\n\tsched->state = sched->saved[sched->saved_states];\n\n\t \n\t \n\tsched->state.used &= ~BIT_ULL(sched->state.counter);\n\n\t \n\tsched->state.counter++;\n\n\treturn true;\n}\n\n \nstatic bool __perf_sched_find_counter(struct perf_sched *sched)\n{\n\tstruct event_constraint *c;\n\tint idx;\n\n\tif (!sched->state.unassigned)\n\t\treturn false;\n\n\tif (sched->state.event >= sched->max_events)\n\t\treturn false;\n\n\tc = sched->constraints[sched->state.event];\n\t \n\tif (c->idxmsk64 & (~0ULL << INTEL_PMC_IDX_FIXED)) {\n\t\tidx = INTEL_PMC_IDX_FIXED;\n\t\tfor_each_set_bit_from(idx, c->idxmsk, X86_PMC_IDX_MAX) {\n\t\t\tu64 mask = BIT_ULL(idx);\n\n\t\t\tif (sched->state.used & mask)\n\t\t\t\tcontinue;\n\n\t\t\tsched->state.used |= mask;\n\t\t\tgoto done;\n\t\t}\n\t}\n\n\t \n\tidx = sched->state.counter;\n\tfor_each_set_bit_from(idx, c->idxmsk, INTEL_PMC_IDX_FIXED) {\n\t\tu64 mask = BIT_ULL(idx);\n\n\t\tif (c->flags & PERF_X86_EVENT_PAIR)\n\t\t\tmask |= mask << 1;\n\n\t\tif (sched->state.used & mask)\n\t\t\tcontinue;\n\n\t\tif (sched->state.nr_gp++ >= sched->max_gp)\n\t\t\treturn false;\n\n\t\tsched->state.used |= mask;\n\t\tgoto done;\n\t}\n\n\treturn false;\n\ndone:\n\tsched->state.counter = idx;\n\n\tif (c->overlap)\n\t\tperf_sched_save_state(sched);\n\n\treturn true;\n}\n\nstatic bool perf_sched_find_counter(struct perf_sched *sched)\n{\n\twhile (!__perf_sched_find_counter(sched)) {\n\t\tif (!perf_sched_restore_state(sched))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nstatic bool perf_sched_next_event(struct perf_sched *sched)\n{\n\tstruct event_constraint *c;\n\n\tif (!sched->state.unassigned || !--sched->state.unassigned)\n\t\treturn false;\n\n\tdo {\n\t\t \n\t\tsched->state.event++;\n\t\tif (sched->state.event >= sched->max_events) {\n\t\t\t \n\t\t\tsched->state.event = 0;\n\t\t\tsched->state.weight++;\n\t\t\tif (sched->state.weight > sched->max_weight)\n\t\t\t\treturn false;\n\t\t}\n\t\tc = sched->constraints[sched->state.event];\n\t} while (c->weight != sched->state.weight);\n\n\tsched->state.counter = 0;\t \n\n\treturn true;\n}\n\n \nint perf_assign_events(struct event_constraint **constraints, int n,\n\t\t\tint wmin, int wmax, int gpmax, int *assign)\n{\n\tstruct perf_sched sched;\n\n\tperf_sched_init(&sched, constraints, n, wmin, wmax, gpmax);\n\n\tdo {\n\t\tif (!perf_sched_find_counter(&sched))\n\t\t\tbreak;\t \n\t\tif (assign)\n\t\t\tassign[sched.state.event] = sched.state.counter;\n\t} while (perf_sched_next_event(&sched));\n\n\treturn sched.state.unassigned;\n}\nEXPORT_SYMBOL_GPL(perf_assign_events);\n\nint x86_schedule_events(struct cpu_hw_events *cpuc, int n, int *assign)\n{\n\tint num_counters = hybrid(cpuc->pmu, num_counters);\n\tstruct event_constraint *c;\n\tstruct perf_event *e;\n\tint n0, i, wmin, wmax, unsched = 0;\n\tstruct hw_perf_event *hwc;\n\tu64 used_mask = 0;\n\n\t \n\tn0 = cpuc->n_events;\n\tif (cpuc->txn_flags & PERF_PMU_TXN_ADD)\n\t\tn0 -= cpuc->n_txn;\n\n\tstatic_call_cond(x86_pmu_start_scheduling)(cpuc);\n\n\tfor (i = 0, wmin = X86_PMC_IDX_MAX, wmax = 0; i < n; i++) {\n\t\tc = cpuc->event_constraint[i];\n\n\t\t \n\t\tWARN_ON_ONCE((c && i >= n0) || (!c && i < n0));\n\n\t\t \n\t\tif (!c || (c->flags & PERF_X86_EVENT_DYNAMIC)) {\n\t\t\tc = static_call(x86_pmu_get_event_constraints)(cpuc, i, cpuc->event_list[i]);\n\t\t\tcpuc->event_constraint[i] = c;\n\t\t}\n\n\t\twmin = min(wmin, c->weight);\n\t\twmax = max(wmax, c->weight);\n\t}\n\n\t \n\tfor (i = 0; i < n; i++) {\n\t\tu64 mask;\n\n\t\thwc = &cpuc->event_list[i]->hw;\n\t\tc = cpuc->event_constraint[i];\n\n\t\t \n\t\tif (hwc->idx == -1)\n\t\t\tbreak;\n\n\t\t \n\t\tif (!test_bit(hwc->idx, c->idxmsk))\n\t\t\tbreak;\n\n\t\tmask = BIT_ULL(hwc->idx);\n\t\tif (is_counter_pair(hwc))\n\t\t\tmask |= mask << 1;\n\n\t\t \n\t\tif (used_mask & mask)\n\t\t\tbreak;\n\n\t\tused_mask |= mask;\n\n\t\tif (assign)\n\t\t\tassign[i] = hwc->idx;\n\t}\n\n\t \n\tif (i != n) {\n\t\tint gpmax = num_counters;\n\n\t\t \n\t\tif (is_ht_workaround_enabled() && !cpuc->is_fake &&\n\t\t    READ_ONCE(cpuc->excl_cntrs->exclusive_present))\n\t\t\tgpmax /= 2;\n\n\t\t \n\t\tif (x86_pmu.flags & PMU_FL_PAIR) {\n\t\t\tgpmax = num_counters - cpuc->n_pair;\n\t\t\tWARN_ON(gpmax <= 0);\n\t\t}\n\n\t\tunsched = perf_assign_events(cpuc->event_constraint, n, wmin,\n\t\t\t\t\t     wmax, gpmax, assign);\n\t}\n\n\t \n\tif (!unsched && assign) {\n\t\tfor (i = 0; i < n; i++)\n\t\t\tstatic_call_cond(x86_pmu_commit_scheduling)(cpuc, i, assign[i]);\n\t} else {\n\t\tfor (i = n0; i < n; i++) {\n\t\t\te = cpuc->event_list[i];\n\n\t\t\t \n\t\t\tstatic_call_cond(x86_pmu_put_event_constraints)(cpuc, e);\n\n\t\t\tcpuc->event_constraint[i] = NULL;\n\t\t}\n\t}\n\n\tstatic_call_cond(x86_pmu_stop_scheduling)(cpuc);\n\n\treturn unsched ? -EINVAL : 0;\n}\n\nstatic int add_nr_metric_event(struct cpu_hw_events *cpuc,\n\t\t\t       struct perf_event *event)\n{\n\tif (is_metric_event(event)) {\n\t\tif (cpuc->n_metric == INTEL_TD_METRIC_NUM)\n\t\t\treturn -EINVAL;\n\t\tcpuc->n_metric++;\n\t\tcpuc->n_txn_metric++;\n\t}\n\n\treturn 0;\n}\n\nstatic void del_nr_metric_event(struct cpu_hw_events *cpuc,\n\t\t\t\tstruct perf_event *event)\n{\n\tif (is_metric_event(event))\n\t\tcpuc->n_metric--;\n}\n\nstatic int collect_event(struct cpu_hw_events *cpuc, struct perf_event *event,\n\t\t\t int max_count, int n)\n{\n\tunion perf_capabilities intel_cap = hybrid(cpuc->pmu, intel_cap);\n\n\tif (intel_cap.perf_metrics && add_nr_metric_event(cpuc, event))\n\t\treturn -EINVAL;\n\n\tif (n >= max_count + cpuc->n_metric)\n\t\treturn -EINVAL;\n\n\tcpuc->event_list[n] = event;\n\tif (is_counter_pair(&event->hw)) {\n\t\tcpuc->n_pair++;\n\t\tcpuc->n_txn_pair++;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int collect_events(struct cpu_hw_events *cpuc, struct perf_event *leader, bool dogrp)\n{\n\tint num_counters = hybrid(cpuc->pmu, num_counters);\n\tint num_counters_fixed = hybrid(cpuc->pmu, num_counters_fixed);\n\tstruct perf_event *event;\n\tint n, max_count;\n\n\tmax_count = num_counters + num_counters_fixed;\n\n\t \n\tn = cpuc->n_events;\n\tif (!cpuc->n_events)\n\t\tcpuc->pebs_output = 0;\n\n\tif (!cpuc->is_fake && leader->attr.precise_ip) {\n\t\t \n\t\tif (is_pebs_pt(leader) && !leader->aux_event)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (cpuc->pebs_output &&\n\t\t    cpuc->pebs_output != is_pebs_pt(leader) + 1)\n\t\t\treturn -EINVAL;\n\n\t\tcpuc->pebs_output = is_pebs_pt(leader) + 1;\n\t}\n\n\tif (is_x86_event(leader)) {\n\t\tif (collect_event(cpuc, leader, max_count, n))\n\t\t\treturn -EINVAL;\n\t\tn++;\n\t}\n\n\tif (!dogrp)\n\t\treturn n;\n\n\tfor_each_sibling_event(event, leader) {\n\t\tif (!is_x86_event(event) || event->state <= PERF_EVENT_STATE_OFF)\n\t\t\tcontinue;\n\n\t\tif (collect_event(cpuc, event, max_count, n))\n\t\t\treturn -EINVAL;\n\n\t\tn++;\n\t}\n\treturn n;\n}\n\nstatic inline void x86_assign_hw_event(struct perf_event *event,\n\t\t\t\tstruct cpu_hw_events *cpuc, int i)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\tint idx;\n\n\tidx = hwc->idx = cpuc->assign[i];\n\thwc->last_cpu = smp_processor_id();\n\thwc->last_tag = ++cpuc->tags[i];\n\n\tstatic_call_cond(x86_pmu_assign)(event, idx);\n\n\tswitch (hwc->idx) {\n\tcase INTEL_PMC_IDX_FIXED_BTS:\n\tcase INTEL_PMC_IDX_FIXED_VLBR:\n\t\thwc->config_base = 0;\n\t\thwc->event_base\t= 0;\n\t\tbreak;\n\n\tcase INTEL_PMC_IDX_METRIC_BASE ... INTEL_PMC_IDX_METRIC_END:\n\t\t \n\t\tidx = INTEL_PMC_IDX_FIXED_SLOTS;\n\t\tfallthrough;\n\tcase INTEL_PMC_IDX_FIXED ... INTEL_PMC_IDX_FIXED_BTS-1:\n\t\thwc->config_base = MSR_ARCH_PERFMON_FIXED_CTR_CTRL;\n\t\thwc->event_base = MSR_ARCH_PERFMON_FIXED_CTR0 +\n\t\t\t\t(idx - INTEL_PMC_IDX_FIXED);\n\t\thwc->event_base_rdpmc = (idx - INTEL_PMC_IDX_FIXED) |\n\t\t\t\t\tINTEL_PMC_FIXED_RDPMC_BASE;\n\t\tbreak;\n\n\tdefault:\n\t\thwc->config_base = x86_pmu_config_addr(hwc->idx);\n\t\thwc->event_base  = x86_pmu_event_addr(hwc->idx);\n\t\thwc->event_base_rdpmc = x86_pmu_rdpmc_index(hwc->idx);\n\t\tbreak;\n\t}\n}\n\n \nint x86_perf_rdpmc_index(struct perf_event *event)\n{\n\tlockdep_assert_irqs_disabled();\n\n\treturn event->hw.event_base_rdpmc;\n}\n\nstatic inline int match_prev_assignment(struct hw_perf_event *hwc,\n\t\t\t\t\tstruct cpu_hw_events *cpuc,\n\t\t\t\t\tint i)\n{\n\treturn hwc->idx == cpuc->assign[i] &&\n\t\thwc->last_cpu == smp_processor_id() &&\n\t\thwc->last_tag == cpuc->tags[i];\n}\n\nstatic void x86_pmu_start(struct perf_event *event, int flags);\n\nstatic void x86_pmu_enable(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct perf_event *event;\n\tstruct hw_perf_event *hwc;\n\tint i, added = cpuc->n_added;\n\n\tif (!x86_pmu_initialized())\n\t\treturn;\n\n\tif (cpuc->enabled)\n\t\treturn;\n\n\tif (cpuc->n_added) {\n\t\tint n_running = cpuc->n_events - cpuc->n_added;\n\t\t \n\t\tfor (i = 0; i < n_running; i++) {\n\t\t\tevent = cpuc->event_list[i];\n\t\t\thwc = &event->hw;\n\n\t\t\t \n\t\t\tif (hwc->idx == -1 ||\n\t\t\t    match_prev_assignment(hwc, cpuc, i))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (hwc->state & PERF_HES_STOPPED)\n\t\t\t\thwc->state |= PERF_HES_ARCH;\n\n\t\t\tx86_pmu_stop(event, PERF_EF_UPDATE);\n\t\t}\n\n\t\t \n\t\tfor (i = 0; i < cpuc->n_events; i++) {\n\t\t\tevent = cpuc->event_list[i];\n\t\t\thwc = &event->hw;\n\n\t\t\tif (!match_prev_assignment(hwc, cpuc, i))\n\t\t\t\tx86_assign_hw_event(event, cpuc, i);\n\t\t\telse if (i < n_running)\n\t\t\t\tcontinue;\n\n\t\t\tif (hwc->state & PERF_HES_ARCH)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tx86_pmu_start(event, PERF_EF_RELOAD);\n\t\t}\n\t\tcpuc->n_added = 0;\n\t\tperf_events_lapic_init();\n\t}\n\n\tcpuc->enabled = 1;\n\tbarrier();\n\n\tstatic_call(x86_pmu_enable_all)(added);\n}\n\nDEFINE_PER_CPU(u64 [X86_PMC_IDX_MAX], pmc_prev_left);\n\n \nint x86_perf_event_set_period(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\ts64 left = local64_read(&hwc->period_left);\n\ts64 period = hwc->sample_period;\n\tint ret = 0, idx = hwc->idx;\n\n\tif (unlikely(!hwc->event_base))\n\t\treturn 0;\n\n\t \n\tif (unlikely(left <= -period)) {\n\t\tleft = period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\n\tif (unlikely(left <= 0)) {\n\t\tleft += period;\n\t\tlocal64_set(&hwc->period_left, left);\n\t\thwc->last_period = period;\n\t\tret = 1;\n\t}\n\t \n\tif (unlikely(left < 2))\n\t\tleft = 2;\n\n\tif (left > x86_pmu.max_period)\n\t\tleft = x86_pmu.max_period;\n\n\tstatic_call_cond(x86_pmu_limit_period)(event, &left);\n\n\tthis_cpu_write(pmc_prev_left[idx], left);\n\n\t \n\tlocal64_set(&hwc->prev_count, (u64)-left);\n\n\twrmsrl(hwc->event_base, (u64)(-left) & x86_pmu.cntval_mask);\n\n\t \n\tif (is_counter_pair(hwc))\n\t\twrmsrl(x86_pmu_event_addr(idx + 1), 0xffff);\n\n\tperf_event_update_userpage(event);\n\n\treturn ret;\n}\n\nvoid x86_pmu_enable_event(struct perf_event *event)\n{\n\tif (__this_cpu_read(cpu_hw_events.enabled))\n\t\t__x86_pmu_enable_event(&event->hw,\n\t\t\t\t       ARCH_PERFMON_EVENTSEL_ENABLE);\n}\n\n \nstatic int x86_pmu_add(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct hw_perf_event *hwc;\n\tint assign[X86_PMC_IDX_MAX];\n\tint n, n0, ret;\n\n\thwc = &event->hw;\n\n\tn0 = cpuc->n_events;\n\tret = n = collect_events(cpuc, event, false);\n\tif (ret < 0)\n\t\tgoto out;\n\n\thwc->state = PERF_HES_UPTODATE | PERF_HES_STOPPED;\n\tif (!(flags & PERF_EF_START))\n\t\thwc->state |= PERF_HES_ARCH;\n\n\t \n\tif (cpuc->txn_flags & PERF_PMU_TXN_ADD)\n\t\tgoto done_collect;\n\n\tret = static_call(x86_pmu_schedule_events)(cpuc, n, assign);\n\tif (ret)\n\t\tgoto out;\n\t \n\tmemcpy(cpuc->assign, assign, n*sizeof(int));\n\ndone_collect:\n\t \n\tcpuc->n_events = n;\n\tcpuc->n_added += n - n0;\n\tcpuc->n_txn += n - n0;\n\n\t \n\tstatic_call_cond(x86_pmu_add)(event);\n\n\tret = 0;\nout:\n\treturn ret;\n}\n\nstatic void x86_pmu_start(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tint idx = event->hw.idx;\n\n\tif (WARN_ON_ONCE(!(event->hw.state & PERF_HES_STOPPED)))\n\t\treturn;\n\n\tif (WARN_ON_ONCE(idx == -1))\n\t\treturn;\n\n\tif (flags & PERF_EF_RELOAD) {\n\t\tWARN_ON_ONCE(!(event->hw.state & PERF_HES_UPTODATE));\n\t\tstatic_call(x86_pmu_set_period)(event);\n\t}\n\n\tevent->hw.state = 0;\n\n\tcpuc->events[idx] = event;\n\t__set_bit(idx, cpuc->active_mask);\n\tstatic_call(x86_pmu_enable)(event);\n\tperf_event_update_userpage(event);\n}\n\nvoid perf_event_print_debug(void)\n{\n\tu64 ctrl, status, overflow, pmc_ctrl, pmc_count, prev_left, fixed;\n\tu64 pebs, debugctl;\n\tint cpu = smp_processor_id();\n\tstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\n\tint num_counters = hybrid(cpuc->pmu, num_counters);\n\tint num_counters_fixed = hybrid(cpuc->pmu, num_counters_fixed);\n\tstruct event_constraint *pebs_constraints = hybrid(cpuc->pmu, pebs_constraints);\n\tunsigned long flags;\n\tint idx;\n\n\tif (!num_counters)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tif (x86_pmu.version >= 2) {\n\t\trdmsrl(MSR_CORE_PERF_GLOBAL_CTRL, ctrl);\n\t\trdmsrl(MSR_CORE_PERF_GLOBAL_STATUS, status);\n\t\trdmsrl(MSR_CORE_PERF_GLOBAL_OVF_CTRL, overflow);\n\t\trdmsrl(MSR_ARCH_PERFMON_FIXED_CTR_CTRL, fixed);\n\n\t\tpr_info(\"\\n\");\n\t\tpr_info(\"CPU#%d: ctrl:       %016llx\\n\", cpu, ctrl);\n\t\tpr_info(\"CPU#%d: status:     %016llx\\n\", cpu, status);\n\t\tpr_info(\"CPU#%d: overflow:   %016llx\\n\", cpu, overflow);\n\t\tpr_info(\"CPU#%d: fixed:      %016llx\\n\", cpu, fixed);\n\t\tif (pebs_constraints) {\n\t\t\trdmsrl(MSR_IA32_PEBS_ENABLE, pebs);\n\t\t\tpr_info(\"CPU#%d: pebs:       %016llx\\n\", cpu, pebs);\n\t\t}\n\t\tif (x86_pmu.lbr_nr) {\n\t\t\trdmsrl(MSR_IA32_DEBUGCTLMSR, debugctl);\n\t\t\tpr_info(\"CPU#%d: debugctl:   %016llx\\n\", cpu, debugctl);\n\t\t}\n\t}\n\tpr_info(\"CPU#%d: active:     %016llx\\n\", cpu, *(u64 *)cpuc->active_mask);\n\n\tfor (idx = 0; idx < num_counters; idx++) {\n\t\trdmsrl(x86_pmu_config_addr(idx), pmc_ctrl);\n\t\trdmsrl(x86_pmu_event_addr(idx), pmc_count);\n\n\t\tprev_left = per_cpu(pmc_prev_left[idx], cpu);\n\n\t\tpr_info(\"CPU#%d:   gen-PMC%d ctrl:  %016llx\\n\",\n\t\t\tcpu, idx, pmc_ctrl);\n\t\tpr_info(\"CPU#%d:   gen-PMC%d count: %016llx\\n\",\n\t\t\tcpu, idx, pmc_count);\n\t\tpr_info(\"CPU#%d:   gen-PMC%d left:  %016llx\\n\",\n\t\t\tcpu, idx, prev_left);\n\t}\n\tfor (idx = 0; idx < num_counters_fixed; idx++) {\n\t\tif (fixed_counter_disabled(idx, cpuc->pmu))\n\t\t\tcontinue;\n\t\trdmsrl(MSR_ARCH_PERFMON_FIXED_CTR0 + idx, pmc_count);\n\n\t\tpr_info(\"CPU#%d: fixed-PMC%d count: %016llx\\n\",\n\t\t\tcpu, idx, pmc_count);\n\t}\n\tlocal_irq_restore(flags);\n}\n\nvoid x86_pmu_stop(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (test_bit(hwc->idx, cpuc->active_mask)) {\n\t\tstatic_call(x86_pmu_disable)(event);\n\t\t__clear_bit(hwc->idx, cpuc->active_mask);\n\t\tcpuc->events[hwc->idx] = NULL;\n\t\tWARN_ON_ONCE(hwc->state & PERF_HES_STOPPED);\n\t\thwc->state |= PERF_HES_STOPPED;\n\t}\n\n\tif ((flags & PERF_EF_UPDATE) && !(hwc->state & PERF_HES_UPTODATE)) {\n\t\t \n\t\tstatic_call(x86_pmu_update)(event);\n\t\thwc->state |= PERF_HES_UPTODATE;\n\t}\n}\n\nstatic void x86_pmu_del(struct perf_event *event, int flags)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tunion perf_capabilities intel_cap = hybrid(cpuc->pmu, intel_cap);\n\tint i;\n\n\t \n\tif (cpuc->txn_flags & PERF_PMU_TXN_ADD)\n\t\tgoto do_del;\n\n\t__set_bit(event->hw.idx, cpuc->dirty);\n\n\t \n\tx86_pmu_stop(event, PERF_EF_UPDATE);\n\n\tfor (i = 0; i < cpuc->n_events; i++) {\n\t\tif (event == cpuc->event_list[i])\n\t\t\tbreak;\n\t}\n\n\tif (WARN_ON_ONCE(i == cpuc->n_events))  \n\t\treturn;\n\n\t \n\tif (i >= cpuc->n_events - cpuc->n_added)\n\t\t--cpuc->n_added;\n\n\tstatic_call_cond(x86_pmu_put_event_constraints)(cpuc, event);\n\n\t \n\twhile (++i < cpuc->n_events) {\n\t\tcpuc->event_list[i-1] = cpuc->event_list[i];\n\t\tcpuc->event_constraint[i-1] = cpuc->event_constraint[i];\n\t}\n\tcpuc->event_constraint[i-1] = NULL;\n\t--cpuc->n_events;\n\tif (intel_cap.perf_metrics)\n\t\tdel_nr_metric_event(cpuc, event);\n\n\tperf_event_update_userpage(event);\n\ndo_del:\n\n\t \n\tstatic_call_cond(x86_pmu_del)(event);\n}\n\nint x86_pmu_handle_irq(struct pt_regs *regs)\n{\n\tstruct perf_sample_data data;\n\tstruct cpu_hw_events *cpuc;\n\tstruct perf_event *event;\n\tint idx, handled = 0;\n\tu64 val;\n\n\tcpuc = this_cpu_ptr(&cpu_hw_events);\n\n\t \n\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n\n\tfor (idx = 0; idx < x86_pmu.num_counters; idx++) {\n\t\tif (!test_bit(idx, cpuc->active_mask))\n\t\t\tcontinue;\n\n\t\tevent = cpuc->events[idx];\n\n\t\tval = static_call(x86_pmu_update)(event);\n\t\tif (val & (1ULL << (x86_pmu.cntval_bits - 1)))\n\t\t\tcontinue;\n\n\t\t \n\t\thandled++;\n\n\t\tif (!static_call(x86_pmu_set_period)(event))\n\t\t\tcontinue;\n\n\t\tperf_sample_data_init(&data, 0, event->hw.last_period);\n\n\t\tif (has_branch_stack(event))\n\t\t\tperf_sample_save_brstack(&data, event, &cpuc->lbr_stack);\n\n\t\tif (perf_event_overflow(event, &data, regs))\n\t\t\tx86_pmu_stop(event, 0);\n\t}\n\n\tif (handled)\n\t\tinc_irq_stat(apic_perf_irqs);\n\n\treturn handled;\n}\n\nvoid perf_events_lapic_init(void)\n{\n\tif (!x86_pmu.apic || !x86_pmu_initialized())\n\t\treturn;\n\n\t \n\tapic_write(APIC_LVTPC, APIC_DM_NMI);\n}\n\nstatic int\nperf_event_nmi_handler(unsigned int cmd, struct pt_regs *regs)\n{\n\tu64 start_clock;\n\tu64 finish_clock;\n\tint ret;\n\n\t \n\tif (!atomic_read(&active_events))\n\t\treturn NMI_DONE;\n\n\tstart_clock = sched_clock();\n\tret = static_call(x86_pmu_handle_irq)(regs);\n\tfinish_clock = sched_clock();\n\n\tperf_sample_event_took(finish_clock - start_clock);\n\n\treturn ret;\n}\nNOKPROBE_SYMBOL(perf_event_nmi_handler);\n\nstruct event_constraint emptyconstraint;\nstruct event_constraint unconstrained;\n\nstatic int x86_pmu_prepare_cpu(unsigned int cpu)\n{\n\tstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\n\tint i;\n\n\tfor (i = 0 ; i < X86_PERF_KFREE_MAX; i++)\n\t\tcpuc->kfree_on_online[i] = NULL;\n\tif (x86_pmu.cpu_prepare)\n\t\treturn x86_pmu.cpu_prepare(cpu);\n\treturn 0;\n}\n\nstatic int x86_pmu_dead_cpu(unsigned int cpu)\n{\n\tif (x86_pmu.cpu_dead)\n\t\tx86_pmu.cpu_dead(cpu);\n\treturn 0;\n}\n\nstatic int x86_pmu_online_cpu(unsigned int cpu)\n{\n\tstruct cpu_hw_events *cpuc = &per_cpu(cpu_hw_events, cpu);\n\tint i;\n\n\tfor (i = 0 ; i < X86_PERF_KFREE_MAX; i++) {\n\t\tkfree(cpuc->kfree_on_online[i]);\n\t\tcpuc->kfree_on_online[i] = NULL;\n\t}\n\treturn 0;\n}\n\nstatic int x86_pmu_starting_cpu(unsigned int cpu)\n{\n\tif (x86_pmu.cpu_starting)\n\t\tx86_pmu.cpu_starting(cpu);\n\treturn 0;\n}\n\nstatic int x86_pmu_dying_cpu(unsigned int cpu)\n{\n\tif (x86_pmu.cpu_dying)\n\t\tx86_pmu.cpu_dying(cpu);\n\treturn 0;\n}\n\nstatic void __init pmu_check_apic(void)\n{\n\tif (boot_cpu_has(X86_FEATURE_APIC))\n\t\treturn;\n\n\tx86_pmu.apic = 0;\n\tpr_info(\"no APIC, boot with the \\\"lapic\\\" boot parameter to force-enable it.\\n\");\n\tpr_info(\"no hardware sampling interrupt available.\\n\");\n\n\t \n\tpmu.capabilities |= PERF_PMU_CAP_NO_INTERRUPT;\n\n}\n\nstatic struct attribute_group x86_pmu_format_group __ro_after_init = {\n\t.name = \"format\",\n\t.attrs = NULL,\n};\n\nssize_t events_sysfs_show(struct device *dev, struct device_attribute *attr, char *page)\n{\n\tstruct perf_pmu_events_attr *pmu_attr =\n\t\tcontainer_of(attr, struct perf_pmu_events_attr, attr);\n\tu64 config = 0;\n\n\tif (pmu_attr->id < x86_pmu.max_events)\n\t\tconfig = x86_pmu.event_map(pmu_attr->id);\n\n\t \n\tif (pmu_attr->event_str)\n\t\treturn sprintf(page, \"%s\\n\", pmu_attr->event_str);\n\n\treturn x86_pmu.events_sysfs_show(page, config);\n}\nEXPORT_SYMBOL_GPL(events_sysfs_show);\n\nssize_t events_ht_sysfs_show(struct device *dev, struct device_attribute *attr,\n\t\t\t  char *page)\n{\n\tstruct perf_pmu_events_ht_attr *pmu_attr =\n\t\tcontainer_of(attr, struct perf_pmu_events_ht_attr, attr);\n\n\t \n\treturn sprintf(page, \"%s\",\n\t\t\ttopology_max_smt_threads() > 1 ?\n\t\t\tpmu_attr->event_str_ht :\n\t\t\tpmu_attr->event_str_noht);\n}\n\nssize_t events_hybrid_sysfs_show(struct device *dev,\n\t\t\t\t struct device_attribute *attr,\n\t\t\t\t char *page)\n{\n\tstruct perf_pmu_events_hybrid_attr *pmu_attr =\n\t\tcontainer_of(attr, struct perf_pmu_events_hybrid_attr, attr);\n\tstruct x86_hybrid_pmu *pmu;\n\tconst char *str, *next_str;\n\tint i;\n\n\tif (hweight64(pmu_attr->pmu_type) == 1)\n\t\treturn sprintf(page, \"%s\", pmu_attr->event_str);\n\n\t \n\tpmu = container_of(dev_get_drvdata(dev), struct x86_hybrid_pmu, pmu);\n\n\tstr = pmu_attr->event_str;\n\tfor (i = 0; i < x86_pmu.num_hybrid_pmus; i++) {\n\t\tif (!(x86_pmu.hybrid_pmu[i].cpu_type & pmu_attr->pmu_type))\n\t\t\tcontinue;\n\t\tif (x86_pmu.hybrid_pmu[i].cpu_type & pmu->cpu_type) {\n\t\t\tnext_str = strchr(str, ';');\n\t\t\tif (next_str)\n\t\t\t\treturn snprintf(page, next_str - str + 1, \"%s\", str);\n\t\t\telse\n\t\t\t\treturn sprintf(page, \"%s\", str);\n\t\t}\n\t\tstr = strchr(str, ';');\n\t\tstr++;\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(events_hybrid_sysfs_show);\n\nEVENT_ATTR(cpu-cycles,\t\t\tCPU_CYCLES\t\t);\nEVENT_ATTR(instructions,\t\tINSTRUCTIONS\t\t);\nEVENT_ATTR(cache-references,\t\tCACHE_REFERENCES\t);\nEVENT_ATTR(cache-misses, \t\tCACHE_MISSES\t\t);\nEVENT_ATTR(branch-instructions,\t\tBRANCH_INSTRUCTIONS\t);\nEVENT_ATTR(branch-misses,\t\tBRANCH_MISSES\t\t);\nEVENT_ATTR(bus-cycles,\t\t\tBUS_CYCLES\t\t);\nEVENT_ATTR(stalled-cycles-frontend,\tSTALLED_CYCLES_FRONTEND\t);\nEVENT_ATTR(stalled-cycles-backend,\tSTALLED_CYCLES_BACKEND\t);\nEVENT_ATTR(ref-cycles,\t\t\tREF_CPU_CYCLES\t\t);\n\nstatic struct attribute *empty_attrs;\n\nstatic struct attribute *events_attr[] = {\n\tEVENT_PTR(CPU_CYCLES),\n\tEVENT_PTR(INSTRUCTIONS),\n\tEVENT_PTR(CACHE_REFERENCES),\n\tEVENT_PTR(CACHE_MISSES),\n\tEVENT_PTR(BRANCH_INSTRUCTIONS),\n\tEVENT_PTR(BRANCH_MISSES),\n\tEVENT_PTR(BUS_CYCLES),\n\tEVENT_PTR(STALLED_CYCLES_FRONTEND),\n\tEVENT_PTR(STALLED_CYCLES_BACKEND),\n\tEVENT_PTR(REF_CPU_CYCLES),\n\tNULL,\n};\n\n \nstatic umode_t\nis_visible(struct kobject *kobj, struct attribute *attr, int idx)\n{\n\tstruct perf_pmu_events_attr *pmu_attr;\n\n\tif (idx >= x86_pmu.max_events)\n\t\treturn 0;\n\n\tpmu_attr = container_of(attr, struct perf_pmu_events_attr, attr.attr);\n\t \n\treturn pmu_attr->event_str || x86_pmu.event_map(idx) ? attr->mode : 0;\n}\n\nstatic struct attribute_group x86_pmu_events_group __ro_after_init = {\n\t.name = \"events\",\n\t.attrs = events_attr,\n\t.is_visible = is_visible,\n};\n\nssize_t x86_event_sysfs_show(char *page, u64 config, u64 event)\n{\n\tu64 umask  = (config & ARCH_PERFMON_EVENTSEL_UMASK) >> 8;\n\tu64 cmask  = (config & ARCH_PERFMON_EVENTSEL_CMASK) >> 24;\n\tbool edge  = (config & ARCH_PERFMON_EVENTSEL_EDGE);\n\tbool pc    = (config & ARCH_PERFMON_EVENTSEL_PIN_CONTROL);\n\tbool any   = (config & ARCH_PERFMON_EVENTSEL_ANY);\n\tbool inv   = (config & ARCH_PERFMON_EVENTSEL_INV);\n\tssize_t ret;\n\n\t \n\tret = sprintf(page, \"event=0x%02llx\", event);\n\n\tif (umask)\n\t\tret += sprintf(page + ret, \",umask=0x%02llx\", umask);\n\n\tif (edge)\n\t\tret += sprintf(page + ret, \",edge\");\n\n\tif (pc)\n\t\tret += sprintf(page + ret, \",pc\");\n\n\tif (any)\n\t\tret += sprintf(page + ret, \",any\");\n\n\tif (inv)\n\t\tret += sprintf(page + ret, \",inv\");\n\n\tif (cmask)\n\t\tret += sprintf(page + ret, \",cmask=0x%02llx\", cmask);\n\n\tret += sprintf(page + ret, \"\\n\");\n\n\treturn ret;\n}\n\nstatic struct attribute_group x86_pmu_attr_group;\nstatic struct attribute_group x86_pmu_caps_group;\n\nstatic void x86_pmu_static_call_update(void)\n{\n\tstatic_call_update(x86_pmu_handle_irq, x86_pmu.handle_irq);\n\tstatic_call_update(x86_pmu_disable_all, x86_pmu.disable_all);\n\tstatic_call_update(x86_pmu_enable_all, x86_pmu.enable_all);\n\tstatic_call_update(x86_pmu_enable, x86_pmu.enable);\n\tstatic_call_update(x86_pmu_disable, x86_pmu.disable);\n\n\tstatic_call_update(x86_pmu_assign, x86_pmu.assign);\n\n\tstatic_call_update(x86_pmu_add, x86_pmu.add);\n\tstatic_call_update(x86_pmu_del, x86_pmu.del);\n\tstatic_call_update(x86_pmu_read, x86_pmu.read);\n\n\tstatic_call_update(x86_pmu_set_period, x86_pmu.set_period);\n\tstatic_call_update(x86_pmu_update, x86_pmu.update);\n\tstatic_call_update(x86_pmu_limit_period, x86_pmu.limit_period);\n\n\tstatic_call_update(x86_pmu_schedule_events, x86_pmu.schedule_events);\n\tstatic_call_update(x86_pmu_get_event_constraints, x86_pmu.get_event_constraints);\n\tstatic_call_update(x86_pmu_put_event_constraints, x86_pmu.put_event_constraints);\n\n\tstatic_call_update(x86_pmu_start_scheduling, x86_pmu.start_scheduling);\n\tstatic_call_update(x86_pmu_commit_scheduling, x86_pmu.commit_scheduling);\n\tstatic_call_update(x86_pmu_stop_scheduling, x86_pmu.stop_scheduling);\n\n\tstatic_call_update(x86_pmu_sched_task, x86_pmu.sched_task);\n\tstatic_call_update(x86_pmu_swap_task_ctx, x86_pmu.swap_task_ctx);\n\n\tstatic_call_update(x86_pmu_drain_pebs, x86_pmu.drain_pebs);\n\tstatic_call_update(x86_pmu_pebs_aliases, x86_pmu.pebs_aliases);\n\n\tstatic_call_update(x86_pmu_guest_get_msrs, x86_pmu.guest_get_msrs);\n\tstatic_call_update(x86_pmu_filter, x86_pmu.filter);\n}\n\nstatic void _x86_pmu_read(struct perf_event *event)\n{\n\tstatic_call(x86_pmu_update)(event);\n}\n\nvoid x86_pmu_show_pmu_cap(int num_counters, int num_counters_fixed,\n\t\t\t  u64 intel_ctrl)\n{\n\tpr_info(\"... version:                %d\\n\",     x86_pmu.version);\n\tpr_info(\"... bit width:              %d\\n\",     x86_pmu.cntval_bits);\n\tpr_info(\"... generic registers:      %d\\n\",     num_counters);\n\tpr_info(\"... value mask:             %016Lx\\n\", x86_pmu.cntval_mask);\n\tpr_info(\"... max period:             %016Lx\\n\", x86_pmu.max_period);\n\tpr_info(\"... fixed-purpose events:   %lu\\n\",\n\t\t\thweight64((((1ULL << num_counters_fixed) - 1)\n\t\t\t\t\t<< INTEL_PMC_IDX_FIXED) & intel_ctrl));\n\tpr_info(\"... event mask:             %016Lx\\n\", intel_ctrl);\n}\n\nstatic int __init init_hw_perf_events(void)\n{\n\tstruct x86_pmu_quirk *quirk;\n\tint err;\n\n\tpr_info(\"Performance Events: \");\n\n\tswitch (boot_cpu_data.x86_vendor) {\n\tcase X86_VENDOR_INTEL:\n\t\terr = intel_pmu_init();\n\t\tbreak;\n\tcase X86_VENDOR_AMD:\n\t\terr = amd_pmu_init();\n\t\tbreak;\n\tcase X86_VENDOR_HYGON:\n\t\terr = amd_pmu_init();\n\t\tx86_pmu.name = \"HYGON\";\n\t\tbreak;\n\tcase X86_VENDOR_ZHAOXIN:\n\tcase X86_VENDOR_CENTAUR:\n\t\terr = zhaoxin_pmu_init();\n\t\tbreak;\n\tdefault:\n\t\terr = -ENOTSUPP;\n\t}\n\tif (err != 0) {\n\t\tpr_cont(\"no PMU driver, software events only.\\n\");\n\t\terr = 0;\n\t\tgoto out_bad_pmu;\n\t}\n\n\tpmu_check_apic();\n\n\t \n\tif (!check_hw_exists(&pmu, x86_pmu.num_counters, x86_pmu.num_counters_fixed))\n\t\tgoto out_bad_pmu;\n\n\tpr_cont(\"%s PMU driver.\\n\", x86_pmu.name);\n\n\tx86_pmu.attr_rdpmc = 1;  \n\n\tfor (quirk = x86_pmu.quirks; quirk; quirk = quirk->next)\n\t\tquirk->func();\n\n\tif (!x86_pmu.intel_ctrl)\n\t\tx86_pmu.intel_ctrl = (1 << x86_pmu.num_counters) - 1;\n\n\tperf_events_lapic_init();\n\tregister_nmi_handler(NMI_LOCAL, perf_event_nmi_handler, 0, \"PMI\");\n\n\tunconstrained = (struct event_constraint)\n\t\t__EVENT_CONSTRAINT(0, (1ULL << x86_pmu.num_counters) - 1,\n\t\t\t\t   0, x86_pmu.num_counters, 0, 0);\n\n\tx86_pmu_format_group.attrs = x86_pmu.format_attrs;\n\n\tif (!x86_pmu.events_sysfs_show)\n\t\tx86_pmu_events_group.attrs = &empty_attrs;\n\n\tpmu.attr_update = x86_pmu.attr_update;\n\n\tif (!is_hybrid()) {\n\t\tx86_pmu_show_pmu_cap(x86_pmu.num_counters,\n\t\t\t\t     x86_pmu.num_counters_fixed,\n\t\t\t\t     x86_pmu.intel_ctrl);\n\t}\n\n\tif (!x86_pmu.read)\n\t\tx86_pmu.read = _x86_pmu_read;\n\n\tif (!x86_pmu.guest_get_msrs)\n\t\tx86_pmu.guest_get_msrs = (void *)&__static_call_return0;\n\n\tif (!x86_pmu.set_period)\n\t\tx86_pmu.set_period = x86_perf_event_set_period;\n\n\tif (!x86_pmu.update)\n\t\tx86_pmu.update = x86_perf_event_update;\n\n\tx86_pmu_static_call_update();\n\n\t \n\terr = cpuhp_setup_state(CPUHP_PERF_X86_PREPARE, \"perf/x86:prepare\",\n\t\t\t\tx86_pmu_prepare_cpu, x86_pmu_dead_cpu);\n\tif (err)\n\t\treturn err;\n\n\terr = cpuhp_setup_state(CPUHP_AP_PERF_X86_STARTING,\n\t\t\t\t\"perf/x86:starting\", x86_pmu_starting_cpu,\n\t\t\t\tx86_pmu_dying_cpu);\n\tif (err)\n\t\tgoto out;\n\n\terr = cpuhp_setup_state(CPUHP_AP_PERF_X86_ONLINE, \"perf/x86:online\",\n\t\t\t\tx86_pmu_online_cpu, NULL);\n\tif (err)\n\t\tgoto out1;\n\n\tif (!is_hybrid()) {\n\t\terr = perf_pmu_register(&pmu, \"cpu\", PERF_TYPE_RAW);\n\t\tif (err)\n\t\t\tgoto out2;\n\t} else {\n\t\tstruct x86_hybrid_pmu *hybrid_pmu;\n\t\tint i, j;\n\n\t\tfor (i = 0; i < x86_pmu.num_hybrid_pmus; i++) {\n\t\t\thybrid_pmu = &x86_pmu.hybrid_pmu[i];\n\n\t\t\thybrid_pmu->pmu = pmu;\n\t\t\thybrid_pmu->pmu.type = -1;\n\t\t\thybrid_pmu->pmu.attr_update = x86_pmu.attr_update;\n\t\t\thybrid_pmu->pmu.capabilities |= PERF_PMU_CAP_EXTENDED_HW_TYPE;\n\n\t\t\terr = perf_pmu_register(&hybrid_pmu->pmu, hybrid_pmu->name,\n\t\t\t\t\t\t(hybrid_pmu->cpu_type == hybrid_big) ? PERF_TYPE_RAW : -1);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (i < x86_pmu.num_hybrid_pmus) {\n\t\t\tfor (j = 0; j < i; j++)\n\t\t\t\tperf_pmu_unregister(&x86_pmu.hybrid_pmu[j].pmu);\n\t\t\tpr_warn(\"Failed to register hybrid PMUs\\n\");\n\t\t\tkfree(x86_pmu.hybrid_pmu);\n\t\t\tx86_pmu.hybrid_pmu = NULL;\n\t\t\tx86_pmu.num_hybrid_pmus = 0;\n\t\t\tgoto out2;\n\t\t}\n\t}\n\n\treturn 0;\n\nout2:\n\tcpuhp_remove_state(CPUHP_AP_PERF_X86_ONLINE);\nout1:\n\tcpuhp_remove_state(CPUHP_AP_PERF_X86_STARTING);\nout:\n\tcpuhp_remove_state(CPUHP_PERF_X86_PREPARE);\nout_bad_pmu:\n\tmemset(&x86_pmu, 0, sizeof(x86_pmu));\n\treturn err;\n}\nearly_initcall(init_hw_perf_events);\n\nstatic void x86_pmu_read(struct perf_event *event)\n{\n\tstatic_call(x86_pmu_read)(event);\n}\n\n \nstatic void x86_pmu_start_txn(struct pmu *pmu, unsigned int txn_flags)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tWARN_ON_ONCE(cpuc->txn_flags);\t\t \n\n\tcpuc->txn_flags = txn_flags;\n\tif (txn_flags & ~PERF_PMU_TXN_ADD)\n\t\treturn;\n\n\tperf_pmu_disable(pmu);\n\t__this_cpu_write(cpu_hw_events.n_txn, 0);\n\t__this_cpu_write(cpu_hw_events.n_txn_pair, 0);\n\t__this_cpu_write(cpu_hw_events.n_txn_metric, 0);\n}\n\n \nstatic void x86_pmu_cancel_txn(struct pmu *pmu)\n{\n\tunsigned int txn_flags;\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\n\tWARN_ON_ONCE(!cpuc->txn_flags);\t \n\n\ttxn_flags = cpuc->txn_flags;\n\tcpuc->txn_flags = 0;\n\tif (txn_flags & ~PERF_PMU_TXN_ADD)\n\t\treturn;\n\n\t \n\t__this_cpu_sub(cpu_hw_events.n_added, __this_cpu_read(cpu_hw_events.n_txn));\n\t__this_cpu_sub(cpu_hw_events.n_events, __this_cpu_read(cpu_hw_events.n_txn));\n\t__this_cpu_sub(cpu_hw_events.n_pair, __this_cpu_read(cpu_hw_events.n_txn_pair));\n\t__this_cpu_sub(cpu_hw_events.n_metric, __this_cpu_read(cpu_hw_events.n_txn_metric));\n\tperf_pmu_enable(pmu);\n}\n\n \nstatic int x86_pmu_commit_txn(struct pmu *pmu)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tint assign[X86_PMC_IDX_MAX];\n\tint n, ret;\n\n\tWARN_ON_ONCE(!cpuc->txn_flags);\t \n\n\tif (cpuc->txn_flags & ~PERF_PMU_TXN_ADD) {\n\t\tcpuc->txn_flags = 0;\n\t\treturn 0;\n\t}\n\n\tn = cpuc->n_events;\n\n\tif (!x86_pmu_initialized())\n\t\treturn -EAGAIN;\n\n\tret = static_call(x86_pmu_schedule_events)(cpuc, n, assign);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tmemcpy(cpuc->assign, assign, n*sizeof(int));\n\n\tcpuc->txn_flags = 0;\n\tperf_pmu_enable(pmu);\n\treturn 0;\n}\n \nstatic void free_fake_cpuc(struct cpu_hw_events *cpuc)\n{\n\tintel_cpuc_finish(cpuc);\n\tkfree(cpuc);\n}\n\nstatic struct cpu_hw_events *allocate_fake_cpuc(struct pmu *event_pmu)\n{\n\tstruct cpu_hw_events *cpuc;\n\tint cpu;\n\n\tcpuc = kzalloc(sizeof(*cpuc), GFP_KERNEL);\n\tif (!cpuc)\n\t\treturn ERR_PTR(-ENOMEM);\n\tcpuc->is_fake = 1;\n\n\tif (is_hybrid()) {\n\t\tstruct x86_hybrid_pmu *h_pmu;\n\n\t\th_pmu = hybrid_pmu(event_pmu);\n\t\tif (cpumask_empty(&h_pmu->supported_cpus))\n\t\t\tgoto error;\n\t\tcpu = cpumask_first(&h_pmu->supported_cpus);\n\t} else\n\t\tcpu = raw_smp_processor_id();\n\tcpuc->pmu = event_pmu;\n\n\tif (intel_cpuc_prepare(cpuc, cpu))\n\t\tgoto error;\n\n\treturn cpuc;\nerror:\n\tfree_fake_cpuc(cpuc);\n\treturn ERR_PTR(-ENOMEM);\n}\n\n \nstatic int validate_event(struct perf_event *event)\n{\n\tstruct cpu_hw_events *fake_cpuc;\n\tstruct event_constraint *c;\n\tint ret = 0;\n\n\tfake_cpuc = allocate_fake_cpuc(event->pmu);\n\tif (IS_ERR(fake_cpuc))\n\t\treturn PTR_ERR(fake_cpuc);\n\n\tc = x86_pmu.get_event_constraints(fake_cpuc, 0, event);\n\n\tif (!c || !c->weight)\n\t\tret = -EINVAL;\n\n\tif (x86_pmu.put_event_constraints)\n\t\tx86_pmu.put_event_constraints(fake_cpuc, event);\n\n\tfree_fake_cpuc(fake_cpuc);\n\n\treturn ret;\n}\n\n \nstatic int validate_group(struct perf_event *event)\n{\n\tstruct perf_event *leader = event->group_leader;\n\tstruct cpu_hw_events *fake_cpuc;\n\tint ret = -EINVAL, n;\n\n\t \n\tif (is_hybrid()) {\n\t\tstruct perf_event *sibling;\n\t\tstruct pmu *pmu = NULL;\n\n\t\tif (is_x86_event(leader))\n\t\t\tpmu = leader->pmu;\n\n\t\tfor_each_sibling_event(sibling, leader) {\n\t\t\tif (!is_x86_event(sibling))\n\t\t\t\tcontinue;\n\t\t\tif (!pmu)\n\t\t\t\tpmu = sibling->pmu;\n\t\t\telse if (pmu != sibling->pmu)\n\t\t\t\treturn ret;\n\t\t}\n\t}\n\n\tfake_cpuc = allocate_fake_cpuc(event->pmu);\n\tif (IS_ERR(fake_cpuc))\n\t\treturn PTR_ERR(fake_cpuc);\n\t \n\tn = collect_events(fake_cpuc, leader, true);\n\tif (n < 0)\n\t\tgoto out;\n\n\tfake_cpuc->n_events = n;\n\tn = collect_events(fake_cpuc, event, false);\n\tif (n < 0)\n\t\tgoto out;\n\n\tfake_cpuc->n_events = 0;\n\tret = x86_pmu.schedule_events(fake_cpuc, n, NULL);\n\nout:\n\tfree_fake_cpuc(fake_cpuc);\n\treturn ret;\n}\n\nstatic int x86_pmu_event_init(struct perf_event *event)\n{\n\tstruct x86_hybrid_pmu *pmu = NULL;\n\tint err;\n\n\tif ((event->attr.type != event->pmu->type) &&\n\t    (event->attr.type != PERF_TYPE_HARDWARE) &&\n\t    (event->attr.type != PERF_TYPE_HW_CACHE))\n\t\treturn -ENOENT;\n\n\tif (is_hybrid() && (event->cpu != -1)) {\n\t\tpmu = hybrid_pmu(event->pmu);\n\t\tif (!cpumask_test_cpu(event->cpu, &pmu->supported_cpus))\n\t\t\treturn -ENOENT;\n\t}\n\n\terr = __x86_pmu_event_init(event);\n\tif (!err) {\n\t\tif (event->group_leader != event)\n\t\t\terr = validate_group(event);\n\t\telse\n\t\t\terr = validate_event(event);\n\t}\n\tif (err) {\n\t\tif (event->destroy)\n\t\t\tevent->destroy(event);\n\t\tevent->destroy = NULL;\n\t}\n\n\tif (READ_ONCE(x86_pmu.attr_rdpmc) &&\n\t    !(event->hw.flags & PERF_X86_EVENT_LARGE_PEBS))\n\t\tevent->hw.flags |= PERF_EVENT_FLAG_USER_READ_CNT;\n\n\treturn err;\n}\n\nvoid perf_clear_dirty_counters(void)\n{\n\tstruct cpu_hw_events *cpuc = this_cpu_ptr(&cpu_hw_events);\n\tint i;\n\n\t  \n\tfor (i = 0; i < cpuc->n_events; i++)\n\t\t__clear_bit(cpuc->assign[i], cpuc->dirty);\n\n\tif (bitmap_empty(cpuc->dirty, X86_PMC_IDX_MAX))\n\t\treturn;\n\n\tfor_each_set_bit(i, cpuc->dirty, X86_PMC_IDX_MAX) {\n\t\tif (i >= INTEL_PMC_IDX_FIXED) {\n\t\t\t \n\t\t\tif ((i - INTEL_PMC_IDX_FIXED) >= hybrid(cpuc->pmu, num_counters_fixed))\n\t\t\t\tcontinue;\n\n\t\t\twrmsrl(MSR_ARCH_PERFMON_FIXED_CTR0 + (i - INTEL_PMC_IDX_FIXED), 0);\n\t\t} else {\n\t\t\twrmsrl(x86_pmu_event_addr(i), 0);\n\t\t}\n\t}\n\n\tbitmap_zero(cpuc->dirty, X86_PMC_IDX_MAX);\n}\n\nstatic void x86_pmu_event_mapped(struct perf_event *event, struct mm_struct *mm)\n{\n\tif (!(event->hw.flags & PERF_EVENT_FLAG_USER_READ_CNT))\n\t\treturn;\n\n\t \n\tmmap_assert_write_locked(mm);\n\n\tif (atomic_inc_return(&mm->context.perf_rdpmc_allowed) == 1)\n\t\ton_each_cpu_mask(mm_cpumask(mm), cr4_update_pce, NULL, 1);\n}\n\nstatic void x86_pmu_event_unmapped(struct perf_event *event, struct mm_struct *mm)\n{\n\tif (!(event->hw.flags & PERF_EVENT_FLAG_USER_READ_CNT))\n\t\treturn;\n\n\tif (atomic_dec_and_test(&mm->context.perf_rdpmc_allowed))\n\t\ton_each_cpu_mask(mm_cpumask(mm), cr4_update_pce, NULL, 1);\n}\n\nstatic int x86_pmu_event_idx(struct perf_event *event)\n{\n\tstruct hw_perf_event *hwc = &event->hw;\n\n\tif (!(hwc->flags & PERF_EVENT_FLAG_USER_READ_CNT))\n\t\treturn 0;\n\n\tif (is_metric_idx(hwc->idx))\n\t\treturn INTEL_PMC_FIXED_RDPMC_METRICS + 1;\n\telse\n\t\treturn hwc->event_base_rdpmc + 1;\n}\n\nstatic ssize_t get_attr_rdpmc(struct device *cdev,\n\t\t\t      struct device_attribute *attr,\n\t\t\t      char *buf)\n{\n\treturn snprintf(buf, 40, \"%d\\n\", x86_pmu.attr_rdpmc);\n}\n\nstatic ssize_t set_attr_rdpmc(struct device *cdev,\n\t\t\t      struct device_attribute *attr,\n\t\t\t      const char *buf, size_t count)\n{\n\tunsigned long val;\n\tssize_t ret;\n\n\tret = kstrtoul(buf, 0, &val);\n\tif (ret)\n\t\treturn ret;\n\n\tif (val > 2)\n\t\treturn -EINVAL;\n\n\tif (x86_pmu.attr_rdpmc_broken)\n\t\treturn -ENOTSUPP;\n\n\tif (val != x86_pmu.attr_rdpmc) {\n\t\t \n\t\tif (val == 0)\n\t\t\tstatic_branch_inc(&rdpmc_never_available_key);\n\t\telse if (x86_pmu.attr_rdpmc == 0)\n\t\t\tstatic_branch_dec(&rdpmc_never_available_key);\n\n\t\tif (val == 2)\n\t\t\tstatic_branch_inc(&rdpmc_always_available_key);\n\t\telse if (x86_pmu.attr_rdpmc == 2)\n\t\t\tstatic_branch_dec(&rdpmc_always_available_key);\n\n\t\ton_each_cpu(cr4_update_pce, NULL, 1);\n\t\tx86_pmu.attr_rdpmc = val;\n\t}\n\n\treturn count;\n}\n\nstatic DEVICE_ATTR(rdpmc, S_IRUSR | S_IWUSR, get_attr_rdpmc, set_attr_rdpmc);\n\nstatic struct attribute *x86_pmu_attrs[] = {\n\t&dev_attr_rdpmc.attr,\n\tNULL,\n};\n\nstatic struct attribute_group x86_pmu_attr_group __ro_after_init = {\n\t.attrs = x86_pmu_attrs,\n};\n\nstatic ssize_t max_precise_show(struct device *cdev,\n\t\t\t\t  struct device_attribute *attr,\n\t\t\t\t  char *buf)\n{\n\treturn snprintf(buf, PAGE_SIZE, \"%d\\n\", x86_pmu_max_precise());\n}\n\nstatic DEVICE_ATTR_RO(max_precise);\n\nstatic struct attribute *x86_pmu_caps_attrs[] = {\n\t&dev_attr_max_precise.attr,\n\tNULL\n};\n\nstatic struct attribute_group x86_pmu_caps_group __ro_after_init = {\n\t.name = \"caps\",\n\t.attrs = x86_pmu_caps_attrs,\n};\n\nstatic const struct attribute_group *x86_pmu_attr_groups[] = {\n\t&x86_pmu_attr_group,\n\t&x86_pmu_format_group,\n\t&x86_pmu_events_group,\n\t&x86_pmu_caps_group,\n\tNULL,\n};\n\nstatic void x86_pmu_sched_task(struct perf_event_pmu_context *pmu_ctx, bool sched_in)\n{\n\tstatic_call_cond(x86_pmu_sched_task)(pmu_ctx, sched_in);\n}\n\nstatic void x86_pmu_swap_task_ctx(struct perf_event_pmu_context *prev_epc,\n\t\t\t\t  struct perf_event_pmu_context *next_epc)\n{\n\tstatic_call_cond(x86_pmu_swap_task_ctx)(prev_epc, next_epc);\n}\n\nvoid perf_check_microcode(void)\n{\n\tif (x86_pmu.check_microcode)\n\t\tx86_pmu.check_microcode();\n}\n\nstatic int x86_pmu_check_period(struct perf_event *event, u64 value)\n{\n\tif (x86_pmu.check_period && x86_pmu.check_period(event, value))\n\t\treturn -EINVAL;\n\n\tif (value && x86_pmu.limit_period) {\n\t\ts64 left = value;\n\t\tx86_pmu.limit_period(event, &left);\n\t\tif (left > value)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int x86_pmu_aux_output_match(struct perf_event *event)\n{\n\tif (!(pmu.capabilities & PERF_PMU_CAP_AUX_OUTPUT))\n\t\treturn 0;\n\n\tif (x86_pmu.aux_output_match)\n\t\treturn x86_pmu.aux_output_match(event);\n\n\treturn 0;\n}\n\nstatic bool x86_pmu_filter(struct pmu *pmu, int cpu)\n{\n\tbool ret = false;\n\n\tstatic_call_cond(x86_pmu_filter)(pmu, cpu, &ret);\n\n\treturn ret;\n}\n\nstatic struct pmu pmu = {\n\t.pmu_enable\t\t= x86_pmu_enable,\n\t.pmu_disable\t\t= x86_pmu_disable,\n\n\t.attr_groups\t\t= x86_pmu_attr_groups,\n\n\t.event_init\t\t= x86_pmu_event_init,\n\n\t.event_mapped\t\t= x86_pmu_event_mapped,\n\t.event_unmapped\t\t= x86_pmu_event_unmapped,\n\n\t.add\t\t\t= x86_pmu_add,\n\t.del\t\t\t= x86_pmu_del,\n\t.start\t\t\t= x86_pmu_start,\n\t.stop\t\t\t= x86_pmu_stop,\n\t.read\t\t\t= x86_pmu_read,\n\n\t.start_txn\t\t= x86_pmu_start_txn,\n\t.cancel_txn\t\t= x86_pmu_cancel_txn,\n\t.commit_txn\t\t= x86_pmu_commit_txn,\n\n\t.event_idx\t\t= x86_pmu_event_idx,\n\t.sched_task\t\t= x86_pmu_sched_task,\n\t.swap_task_ctx\t\t= x86_pmu_swap_task_ctx,\n\t.check_period\t\t= x86_pmu_check_period,\n\n\t.aux_output_match\t= x86_pmu_aux_output_match,\n\n\t.filter\t\t\t= x86_pmu_filter,\n};\n\nvoid arch_perf_update_userpage(struct perf_event *event,\n\t\t\t       struct perf_event_mmap_page *userpg, u64 now)\n{\n\tstruct cyc2ns_data data;\n\tu64 offset;\n\n\tuserpg->cap_user_time = 0;\n\tuserpg->cap_user_time_zero = 0;\n\tuserpg->cap_user_rdpmc =\n\t\t!!(event->hw.flags & PERF_EVENT_FLAG_USER_READ_CNT);\n\tuserpg->pmc_width = x86_pmu.cntval_bits;\n\n\tif (!using_native_sched_clock() || !sched_clock_stable())\n\t\treturn;\n\n\tcyc2ns_read_begin(&data);\n\n\toffset = data.cyc2ns_offset + __sched_clock_offset;\n\n\t \n\tuserpg->cap_user_time = 1;\n\tuserpg->time_mult = data.cyc2ns_mul;\n\tuserpg->time_shift = data.cyc2ns_shift;\n\tuserpg->time_offset = offset - now;\n\n\t \n\tif (!event->attr.use_clockid) {\n\t\tuserpg->cap_user_time_zero = 1;\n\t\tuserpg->time_zero = offset;\n\t}\n\n\tcyc2ns_read_end();\n}\n\n \nstatic bool perf_hw_regs(struct pt_regs *regs)\n{\n\treturn regs->flags & X86_EFLAGS_FIXED;\n}\n\nvoid\nperf_callchain_kernel(struct perf_callchain_entry_ctx *entry, struct pt_regs *regs)\n{\n\tstruct unwind_state state;\n\tunsigned long addr;\n\n\tif (perf_guest_state()) {\n\t\t \n\t\treturn;\n\t}\n\n\tif (perf_callchain_store(entry, regs->ip))\n\t\treturn;\n\n\tif (perf_hw_regs(regs))\n\t\tunwind_start(&state, current, regs, NULL);\n\telse\n\t\tunwind_start(&state, current, NULL, (void *)regs->sp);\n\n\tfor (; !unwind_done(&state); unwind_next_frame(&state)) {\n\t\taddr = unwind_get_return_address(&state);\n\t\tif (!addr || perf_callchain_store(entry, addr))\n\t\t\treturn;\n\t}\n}\n\nstatic inline int\nvalid_user_frame(const void __user *fp, unsigned long size)\n{\n\treturn __access_ok(fp, size);\n}\n\nstatic unsigned long get_segment_base(unsigned int segment)\n{\n\tstruct desc_struct *desc;\n\tunsigned int idx = segment >> 3;\n\n\tif ((segment & SEGMENT_TI_MASK) == SEGMENT_LDT) {\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n\t\tstruct ldt_struct *ldt;\n\n\t\t \n\t\tldt = READ_ONCE(current->active_mm->context.ldt);\n\t\tif (!ldt || idx >= ldt->nr_entries)\n\t\t\treturn 0;\n\n\t\tdesc = &ldt->entries[idx];\n#else\n\t\treturn 0;\n#endif\n\t} else {\n\t\tif (idx >= GDT_ENTRIES)\n\t\t\treturn 0;\n\n\t\tdesc = raw_cpu_ptr(gdt_page.gdt) + idx;\n\t}\n\n\treturn get_desc_base(desc);\n}\n\n#ifdef CONFIG_IA32_EMULATION\n\n#include <linux/compat.h>\n\nstatic inline int\nperf_callchain_user32(struct pt_regs *regs, struct perf_callchain_entry_ctx *entry)\n{\n\t \n\tunsigned long ss_base, cs_base;\n\tstruct stack_frame_ia32 frame;\n\tconst struct stack_frame_ia32 __user *fp;\n\n\tif (user_64bit_mode(regs))\n\t\treturn 0;\n\n\tcs_base = get_segment_base(regs->cs);\n\tss_base = get_segment_base(regs->ss);\n\n\tfp = compat_ptr(ss_base + regs->bp);\n\tpagefault_disable();\n\twhile (entry->nr < entry->max_stack) {\n\t\tif (!valid_user_frame(fp, sizeof(frame)))\n\t\t\tbreak;\n\n\t\tif (__get_user(frame.next_frame, &fp->next_frame))\n\t\t\tbreak;\n\t\tif (__get_user(frame.return_address, &fp->return_address))\n\t\t\tbreak;\n\n\t\tperf_callchain_store(entry, cs_base + frame.return_address);\n\t\tfp = compat_ptr(ss_base + frame.next_frame);\n\t}\n\tpagefault_enable();\n\treturn 1;\n}\n#else\nstatic inline int\nperf_callchain_user32(struct pt_regs *regs, struct perf_callchain_entry_ctx *entry)\n{\n    return 0;\n}\n#endif\n\nvoid\nperf_callchain_user(struct perf_callchain_entry_ctx *entry, struct pt_regs *regs)\n{\n\tstruct stack_frame frame;\n\tconst struct stack_frame __user *fp;\n\n\tif (perf_guest_state()) {\n\t\t \n\t\treturn;\n\t}\n\n\t \n\tif (regs->flags & (X86_VM_MASK | PERF_EFLAGS_VM))\n\t\treturn;\n\n\tfp = (void __user *)regs->bp;\n\n\tperf_callchain_store(entry, regs->ip);\n\n\tif (!nmi_uaccess_okay())\n\t\treturn;\n\n\tif (perf_callchain_user32(regs, entry))\n\t\treturn;\n\n\tpagefault_disable();\n\twhile (entry->nr < entry->max_stack) {\n\t\tif (!valid_user_frame(fp, sizeof(frame)))\n\t\t\tbreak;\n\n\t\tif (__get_user(frame.next_frame, &fp->next_frame))\n\t\t\tbreak;\n\t\tif (__get_user(frame.return_address, &fp->return_address))\n\t\t\tbreak;\n\n\t\tperf_callchain_store(entry, frame.return_address);\n\t\tfp = (void __user *)frame.next_frame;\n\t}\n\tpagefault_enable();\n}\n\n \nstatic unsigned long code_segment_base(struct pt_regs *regs)\n{\n\t \n\n#ifdef CONFIG_X86_32\n\t \n\tif (regs->flags & X86_VM_MASK)\n\t\treturn 0x10 * regs->cs;\n\n\tif (user_mode(regs) && regs->cs != __USER_CS)\n\t\treturn get_segment_base(regs->cs);\n#else\n\tif (user_mode(regs) && !user_64bit_mode(regs) &&\n\t    regs->cs != __USER32_CS)\n\t\treturn get_segment_base(regs->cs);\n#endif\n\treturn 0;\n}\n\nunsigned long perf_instruction_pointer(struct pt_regs *regs)\n{\n\tif (perf_guest_state())\n\t\treturn perf_guest_get_ip();\n\n\treturn regs->ip + code_segment_base(regs);\n}\n\nunsigned long perf_misc_flags(struct pt_regs *regs)\n{\n\tunsigned int guest_state = perf_guest_state();\n\tint misc = 0;\n\n\tif (guest_state) {\n\t\tif (guest_state & PERF_GUEST_USER)\n\t\t\tmisc |= PERF_RECORD_MISC_GUEST_USER;\n\t\telse\n\t\t\tmisc |= PERF_RECORD_MISC_GUEST_KERNEL;\n\t} else {\n\t\tif (user_mode(regs))\n\t\t\tmisc |= PERF_RECORD_MISC_USER;\n\t\telse\n\t\t\tmisc |= PERF_RECORD_MISC_KERNEL;\n\t}\n\n\tif (regs->flags & PERF_EFLAGS_EXACT)\n\t\tmisc |= PERF_RECORD_MISC_EXACT_IP;\n\n\treturn misc;\n}\n\nvoid perf_get_x86_pmu_capability(struct x86_pmu_capability *cap)\n{\n\t \n\tif (WARN_ON_ONCE(cpu_feature_enabled(X86_FEATURE_HYBRID_CPU)) ||\n\t    !x86_pmu_initialized()) {\n\t\tmemset(cap, 0, sizeof(*cap));\n\t\treturn;\n\t}\n\n\t \n\tcap->version\t\t= x86_pmu.version;\n\tcap->num_counters_gp\t= x86_pmu.num_counters;\n\tcap->num_counters_fixed\t= x86_pmu.num_counters_fixed;\n\tcap->bit_width_gp\t= x86_pmu.cntval_bits;\n\tcap->bit_width_fixed\t= x86_pmu.cntval_bits;\n\tcap->events_mask\t= (unsigned int)x86_pmu.events_maskl;\n\tcap->events_mask_len\t= x86_pmu.events_mask_len;\n\tcap->pebs_ept\t\t= x86_pmu.pebs_ept;\n}\nEXPORT_SYMBOL_GPL(perf_get_x86_pmu_capability);\n\nu64 perf_get_hw_event_config(int hw_event)\n{\n\tint max = x86_pmu.max_events;\n\n\tif (hw_event < max)\n\t\treturn x86_pmu.event_map(array_index_nospec(hw_event, max));\n\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(perf_get_hw_event_config);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}