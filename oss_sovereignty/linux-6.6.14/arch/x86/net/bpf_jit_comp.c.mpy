{
  "module_name": "bpf_jit_comp.c",
  "hash_id": "aecb9b91a071abcb81c81c86dfd9e420e439a8475ea8e84ede2b0e82f1e5b171",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/net/bpf_jit_comp.c",
  "human_readable_source": "\n \n#include <linux/netdevice.h>\n#include <linux/filter.h>\n#include <linux/if_vlan.h>\n#include <linux/bpf.h>\n#include <linux/memory.h>\n#include <linux/sort.h>\n#include <asm/extable.h>\n#include <asm/ftrace.h>\n#include <asm/set_memory.h>\n#include <asm/nospec-branch.h>\n#include <asm/text-patching.h>\n\nstatic u8 *emit_code(u8 *ptr, u32 bytes, unsigned int len)\n{\n\tif (len == 1)\n\t\t*ptr = bytes;\n\telse if (len == 2)\n\t\t*(u16 *)ptr = bytes;\n\telse {\n\t\t*(u32 *)ptr = bytes;\n\t\tbarrier();\n\t}\n\treturn ptr + len;\n}\n\n#define EMIT(bytes, len) \\\n\tdo { prog = emit_code(prog, bytes, len); } while (0)\n\n#define EMIT1(b1)\t\tEMIT(b1, 1)\n#define EMIT2(b1, b2)\t\tEMIT((b1) + ((b2) << 8), 2)\n#define EMIT3(b1, b2, b3)\tEMIT((b1) + ((b2) << 8) + ((b3) << 16), 3)\n#define EMIT4(b1, b2, b3, b4)   EMIT((b1) + ((b2) << 8) + ((b3) << 16) + ((b4) << 24), 4)\n\n#define EMIT1_off32(b1, off) \\\n\tdo { EMIT1(b1); EMIT(off, 4); } while (0)\n#define EMIT2_off32(b1, b2, off) \\\n\tdo { EMIT2(b1, b2); EMIT(off, 4); } while (0)\n#define EMIT3_off32(b1, b2, b3, off) \\\n\tdo { EMIT3(b1, b2, b3); EMIT(off, 4); } while (0)\n#define EMIT4_off32(b1, b2, b3, b4, off) \\\n\tdo { EMIT4(b1, b2, b3, b4); EMIT(off, 4); } while (0)\n\n#ifdef CONFIG_X86_KERNEL_IBT\n#define EMIT_ENDBR()\tEMIT(gen_endbr(), 4)\n#else\n#define EMIT_ENDBR()\n#endif\n\nstatic bool is_imm8(int value)\n{\n\treturn value <= 127 && value >= -128;\n}\n\nstatic bool is_simm32(s64 value)\n{\n\treturn value == (s64)(s32)value;\n}\n\nstatic bool is_uimm32(u64 value)\n{\n\treturn value == (u64)(u32)value;\n}\n\n \n#define EMIT_mov(DST, SRC)\t\t\t\t\t\t\t\t \\\n\tdo {\t\t\t\t\t\t\t\t\t\t \\\n\t\tif (DST != SRC)\t\t\t\t\t\t\t\t \\\n\t\t\tEMIT3(add_2mod(0x48, DST, SRC), 0x89, add_2reg(0xC0, DST, SRC)); \\\n\t} while (0)\n\nstatic int bpf_size_to_x86_bytes(int bpf_size)\n{\n\tif (bpf_size == BPF_W)\n\t\treturn 4;\n\telse if (bpf_size == BPF_H)\n\t\treturn 2;\n\telse if (bpf_size == BPF_B)\n\t\treturn 1;\n\telse if (bpf_size == BPF_DW)\n\t\treturn 4;  \n\telse\n\t\treturn 0;\n}\n\n \n#define X86_JB  0x72\n#define X86_JAE 0x73\n#define X86_JE  0x74\n#define X86_JNE 0x75\n#define X86_JBE 0x76\n#define X86_JA  0x77\n#define X86_JL  0x7C\n#define X86_JGE 0x7D\n#define X86_JLE 0x7E\n#define X86_JG  0x7F\n\n \n#define AUX_REG (MAX_BPF_JIT_REG + 1)\n#define X86_REG_R9 (MAX_BPF_JIT_REG + 2)\n\n \nstatic const int reg2hex[] = {\n\t[BPF_REG_0] = 0,   \n\t[BPF_REG_1] = 7,   \n\t[BPF_REG_2] = 6,   \n\t[BPF_REG_3] = 2,   \n\t[BPF_REG_4] = 1,   \n\t[BPF_REG_5] = 0,   \n\t[BPF_REG_6] = 3,   \n\t[BPF_REG_7] = 5,   \n\t[BPF_REG_8] = 6,   \n\t[BPF_REG_9] = 7,   \n\t[BPF_REG_FP] = 5,  \n\t[BPF_REG_AX] = 2,  \n\t[AUX_REG] = 3,     \n\t[X86_REG_R9] = 1,  \n};\n\nstatic const int reg2pt_regs[] = {\n\t[BPF_REG_0] = offsetof(struct pt_regs, ax),\n\t[BPF_REG_1] = offsetof(struct pt_regs, di),\n\t[BPF_REG_2] = offsetof(struct pt_regs, si),\n\t[BPF_REG_3] = offsetof(struct pt_regs, dx),\n\t[BPF_REG_4] = offsetof(struct pt_regs, cx),\n\t[BPF_REG_5] = offsetof(struct pt_regs, r8),\n\t[BPF_REG_6] = offsetof(struct pt_regs, bx),\n\t[BPF_REG_7] = offsetof(struct pt_regs, r13),\n\t[BPF_REG_8] = offsetof(struct pt_regs, r14),\n\t[BPF_REG_9] = offsetof(struct pt_regs, r15),\n};\n\n \nstatic bool is_ereg(u32 reg)\n{\n\treturn (1 << reg) & (BIT(BPF_REG_5) |\n\t\t\t     BIT(AUX_REG) |\n\t\t\t     BIT(BPF_REG_7) |\n\t\t\t     BIT(BPF_REG_8) |\n\t\t\t     BIT(BPF_REG_9) |\n\t\t\t     BIT(X86_REG_R9) |\n\t\t\t     BIT(BPF_REG_AX));\n}\n\n \nstatic bool is_ereg_8l(u32 reg)\n{\n\treturn is_ereg(reg) ||\n\t    (1 << reg) & (BIT(BPF_REG_1) |\n\t\t\t  BIT(BPF_REG_2) |\n\t\t\t  BIT(BPF_REG_FP));\n}\n\nstatic bool is_axreg(u32 reg)\n{\n\treturn reg == BPF_REG_0;\n}\n\n \nstatic u8 add_1mod(u8 byte, u32 reg)\n{\n\tif (is_ereg(reg))\n\t\tbyte |= 1;\n\treturn byte;\n}\n\nstatic u8 add_2mod(u8 byte, u32 r1, u32 r2)\n{\n\tif (is_ereg(r1))\n\t\tbyte |= 1;\n\tif (is_ereg(r2))\n\t\tbyte |= 4;\n\treturn byte;\n}\n\n \nstatic u8 add_1reg(u8 byte, u32 dst_reg)\n{\n\treturn byte + reg2hex[dst_reg];\n}\n\n \nstatic u8 add_2reg(u8 byte, u32 dst_reg, u32 src_reg)\n{\n\treturn byte + reg2hex[dst_reg] + (reg2hex[src_reg] << 3);\n}\n\n \nstatic u8 simple_alu_opcodes[] = {\n\t[BPF_ADD] = 0x01,\n\t[BPF_SUB] = 0x29,\n\t[BPF_AND] = 0x21,\n\t[BPF_OR] = 0x09,\n\t[BPF_XOR] = 0x31,\n\t[BPF_LSH] = 0xE0,\n\t[BPF_RSH] = 0xE8,\n\t[BPF_ARSH] = 0xF8,\n};\n\nstatic void jit_fill_hole(void *area, unsigned int size)\n{\n\t \n\tmemset(area, 0xcc, size);\n}\n\nint bpf_arch_text_invalidate(void *dst, size_t len)\n{\n\treturn IS_ERR_OR_NULL(text_poke_set(dst, 0xcc, len));\n}\n\nstruct jit_context {\n\tint cleanup_addr;  \n\n\t \n\tint tail_call_direct_label;\n\tint tail_call_indirect_label;\n};\n\n \n#define BPF_MAX_INSN_SIZE\t128\n#define BPF_INSN_SAFETY\t\t64\n\n \n#define X86_PATCH_SIZE\t\t5\n \n#define X86_TAIL_CALL_OFFSET\t(11 + ENDBR_INSN_SIZE)\n\nstatic void push_callee_regs(u8 **pprog, bool *callee_regs_used)\n{\n\tu8 *prog = *pprog;\n\n\tif (callee_regs_used[0])\n\t\tEMIT1(0x53);          \n\tif (callee_regs_used[1])\n\t\tEMIT2(0x41, 0x55);    \n\tif (callee_regs_used[2])\n\t\tEMIT2(0x41, 0x56);    \n\tif (callee_regs_used[3])\n\t\tEMIT2(0x41, 0x57);    \n\t*pprog = prog;\n}\n\nstatic void pop_callee_regs(u8 **pprog, bool *callee_regs_used)\n{\n\tu8 *prog = *pprog;\n\n\tif (callee_regs_used[3])\n\t\tEMIT2(0x41, 0x5F);    \n\tif (callee_regs_used[2])\n\t\tEMIT2(0x41, 0x5E);    \n\tif (callee_regs_used[1])\n\t\tEMIT2(0x41, 0x5D);    \n\tif (callee_regs_used[0])\n\t\tEMIT1(0x5B);          \n\t*pprog = prog;\n}\n\n \nstatic void emit_prologue(u8 **pprog, u32 stack_depth, bool ebpf_from_cbpf,\n\t\t\t  bool tail_call_reachable, bool is_subprog)\n{\n\tu8 *prog = *pprog;\n\n\t \n\tEMIT_ENDBR();\n\tmemcpy(prog, x86_nops[5], X86_PATCH_SIZE);\n\tprog += X86_PATCH_SIZE;\n\tif (!ebpf_from_cbpf) {\n\t\tif (tail_call_reachable && !is_subprog)\n\t\t\tEMIT2(0x31, 0xC0);  \n\t\telse\n\t\t\tEMIT2(0x66, 0x90);  \n\t}\n\tEMIT1(0x55);              \n\tEMIT3(0x48, 0x89, 0xE5);  \n\n\t \n\tEMIT_ENDBR();\n\n\t \n\tif (stack_depth)\n\t\tEMIT3_off32(0x48, 0x81, 0xEC, round_up(stack_depth, 8));\n\tif (tail_call_reachable)\n\t\tEMIT1(0x50);          \n\t*pprog = prog;\n}\n\nstatic int emit_patch(u8 **pprog, void *func, void *ip, u8 opcode)\n{\n\tu8 *prog = *pprog;\n\ts64 offset;\n\n\toffset = func - (ip + X86_PATCH_SIZE);\n\tif (!is_simm32(offset)) {\n\t\tpr_err(\"Target call %p is out of range\\n\", func);\n\t\treturn -ERANGE;\n\t}\n\tEMIT1_off32(opcode, offset);\n\t*pprog = prog;\n\treturn 0;\n}\n\nstatic int emit_call(u8 **pprog, void *func, void *ip)\n{\n\treturn emit_patch(pprog, func, ip, 0xE8);\n}\n\nstatic int emit_rsb_call(u8 **pprog, void *func, void *ip)\n{\n\tOPTIMIZER_HIDE_VAR(func);\n\tx86_call_depth_emit_accounting(pprog, func);\n\treturn emit_patch(pprog, func, ip, 0xE8);\n}\n\nstatic int emit_jump(u8 **pprog, void *func, void *ip)\n{\n\treturn emit_patch(pprog, func, ip, 0xE9);\n}\n\nstatic int __bpf_arch_text_poke(void *ip, enum bpf_text_poke_type t,\n\t\t\t\tvoid *old_addr, void *new_addr)\n{\n\tconst u8 *nop_insn = x86_nops[5];\n\tu8 old_insn[X86_PATCH_SIZE];\n\tu8 new_insn[X86_PATCH_SIZE];\n\tu8 *prog;\n\tint ret;\n\n\tmemcpy(old_insn, nop_insn, X86_PATCH_SIZE);\n\tif (old_addr) {\n\t\tprog = old_insn;\n\t\tret = t == BPF_MOD_CALL ?\n\t\t      emit_call(&prog, old_addr, ip) :\n\t\t      emit_jump(&prog, old_addr, ip);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tmemcpy(new_insn, nop_insn, X86_PATCH_SIZE);\n\tif (new_addr) {\n\t\tprog = new_insn;\n\t\tret = t == BPF_MOD_CALL ?\n\t\t      emit_call(&prog, new_addr, ip) :\n\t\t      emit_jump(&prog, new_addr, ip);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tret = -EBUSY;\n\tmutex_lock(&text_mutex);\n\tif (memcmp(ip, old_insn, X86_PATCH_SIZE))\n\t\tgoto out;\n\tret = 1;\n\tif (memcmp(ip, new_insn, X86_PATCH_SIZE)) {\n\t\ttext_poke_bp(ip, new_insn, X86_PATCH_SIZE, NULL);\n\t\tret = 0;\n\t}\nout:\n\tmutex_unlock(&text_mutex);\n\treturn ret;\n}\n\nint bpf_arch_text_poke(void *ip, enum bpf_text_poke_type t,\n\t\t       void *old_addr, void *new_addr)\n{\n\tif (!is_kernel_text((long)ip) &&\n\t    !is_bpf_text_address((long)ip))\n\t\t \n\t\treturn -EINVAL;\n\n\t \n\tif (is_endbr(*(u32 *)ip))\n\t\tip += ENDBR_INSN_SIZE;\n\n\treturn __bpf_arch_text_poke(ip, t, old_addr, new_addr);\n}\n\n#define EMIT_LFENCE()\tEMIT3(0x0F, 0xAE, 0xE8)\n\nstatic void emit_indirect_jump(u8 **pprog, int reg, u8 *ip)\n{\n\tu8 *prog = *pprog;\n\n\tif (cpu_feature_enabled(X86_FEATURE_RETPOLINE_LFENCE)) {\n\t\tEMIT_LFENCE();\n\t\tEMIT2(0xFF, 0xE0 + reg);\n\t} else if (cpu_feature_enabled(X86_FEATURE_RETPOLINE)) {\n\t\tOPTIMIZER_HIDE_VAR(reg);\n\t\tif (cpu_feature_enabled(X86_FEATURE_CALL_DEPTH))\n\t\t\temit_jump(&prog, &__x86_indirect_jump_thunk_array[reg], ip);\n\t\telse\n\t\t\temit_jump(&prog, &__x86_indirect_thunk_array[reg], ip);\n\t} else {\n\t\tEMIT2(0xFF, 0xE0 + reg);\t \n\t\tif (IS_ENABLED(CONFIG_RETPOLINE) || IS_ENABLED(CONFIG_SLS))\n\t\t\tEMIT1(0xCC);\t\t \n\t}\n\n\t*pprog = prog;\n}\n\nstatic void emit_return(u8 **pprog, u8 *ip)\n{\n\tu8 *prog = *pprog;\n\n\tif (cpu_feature_enabled(X86_FEATURE_RETHUNK)) {\n\t\temit_jump(&prog, x86_return_thunk, ip);\n\t} else {\n\t\tEMIT1(0xC3);\t\t \n\t\tif (IS_ENABLED(CONFIG_SLS))\n\t\t\tEMIT1(0xCC);\t \n\t}\n\n\t*pprog = prog;\n}\n\n \nstatic void emit_bpf_tail_call_indirect(u8 **pprog, bool *callee_regs_used,\n\t\t\t\t\tu32 stack_depth, u8 *ip,\n\t\t\t\t\tstruct jit_context *ctx)\n{\n\tint tcc_off = -4 - round_up(stack_depth, 8);\n\tu8 *prog = *pprog, *start = *pprog;\n\tint offset;\n\n\t \n\n\t \n\tEMIT2(0x89, 0xD2);                         \n\tEMIT3(0x39, 0x56,                          \n\t      offsetof(struct bpf_array, map.max_entries));\n\n\toffset = ctx->tail_call_indirect_label - (prog + 2 - start);\n\tEMIT2(X86_JBE, offset);                    \n\n\t \n\tEMIT2_off32(0x8B, 0x85, tcc_off);          \n\tEMIT3(0x83, 0xF8, MAX_TAIL_CALL_CNT);      \n\n\toffset = ctx->tail_call_indirect_label - (prog + 2 - start);\n\tEMIT2(X86_JAE, offset);                    \n\tEMIT3(0x83, 0xC0, 0x01);                   \n\tEMIT2_off32(0x89, 0x85, tcc_off);          \n\n\t \n\tEMIT4_off32(0x48, 0x8B, 0x8C, 0xD6,        \n\t\t    offsetof(struct bpf_array, ptrs));\n\n\t \n\tEMIT3(0x48, 0x85, 0xC9);                   \n\n\toffset = ctx->tail_call_indirect_label - (prog + 2 - start);\n\tEMIT2(X86_JE, offset);                     \n\n\tpop_callee_regs(&prog, callee_regs_used);\n\n\tEMIT1(0x58);                               \n\tif (stack_depth)\n\t\tEMIT3_off32(0x48, 0x81, 0xC4,      \n\t\t\t    round_up(stack_depth, 8));\n\n\t \n\tEMIT4(0x48, 0x8B, 0x49,                    \n\t      offsetof(struct bpf_prog, bpf_func));\n\tEMIT4(0x48, 0x83, 0xC1,                    \n\t      X86_TAIL_CALL_OFFSET);\n\t \n\temit_indirect_jump(&prog, 1  , ip + (prog - start));\n\n\t \n\tctx->tail_call_indirect_label = prog - start;\n\t*pprog = prog;\n}\n\nstatic void emit_bpf_tail_call_direct(struct bpf_jit_poke_descriptor *poke,\n\t\t\t\t      u8 **pprog, u8 *ip,\n\t\t\t\t      bool *callee_regs_used, u32 stack_depth,\n\t\t\t\t      struct jit_context *ctx)\n{\n\tint tcc_off = -4 - round_up(stack_depth, 8);\n\tu8 *prog = *pprog, *start = *pprog;\n\tint offset;\n\n\t \n\tEMIT2_off32(0x8B, 0x85, tcc_off);              \n\tEMIT3(0x83, 0xF8, MAX_TAIL_CALL_CNT);          \n\n\toffset = ctx->tail_call_direct_label - (prog + 2 - start);\n\tEMIT2(X86_JAE, offset);                        \n\tEMIT3(0x83, 0xC0, 0x01);                       \n\tEMIT2_off32(0x89, 0x85, tcc_off);              \n\n\tpoke->tailcall_bypass = ip + (prog - start);\n\tpoke->adj_off = X86_TAIL_CALL_OFFSET;\n\tpoke->tailcall_target = ip + ctx->tail_call_direct_label - X86_PATCH_SIZE;\n\tpoke->bypass_addr = (u8 *)poke->tailcall_target + X86_PATCH_SIZE;\n\n\temit_jump(&prog, (u8 *)poke->tailcall_target + X86_PATCH_SIZE,\n\t\t  poke->tailcall_bypass);\n\n\tpop_callee_regs(&prog, callee_regs_used);\n\tEMIT1(0x58);                                   \n\tif (stack_depth)\n\t\tEMIT3_off32(0x48, 0x81, 0xC4, round_up(stack_depth, 8));\n\n\tmemcpy(prog, x86_nops[5], X86_PATCH_SIZE);\n\tprog += X86_PATCH_SIZE;\n\n\t \n\tctx->tail_call_direct_label = prog - start;\n\n\t*pprog = prog;\n}\n\nstatic void bpf_tail_call_direct_fixup(struct bpf_prog *prog)\n{\n\tstruct bpf_jit_poke_descriptor *poke;\n\tstruct bpf_array *array;\n\tstruct bpf_prog *target;\n\tint i, ret;\n\n\tfor (i = 0; i < prog->aux->size_poke_tab; i++) {\n\t\tpoke = &prog->aux->poke_tab[i];\n\t\tif (poke->aux && poke->aux != prog->aux)\n\t\t\tcontinue;\n\n\t\tWARN_ON_ONCE(READ_ONCE(poke->tailcall_target_stable));\n\n\t\tif (poke->reason != BPF_POKE_REASON_TAIL_CALL)\n\t\t\tcontinue;\n\n\t\tarray = container_of(poke->tail_call.map, struct bpf_array, map);\n\t\tmutex_lock(&array->aux->poke_mutex);\n\t\ttarget = array->ptrs[poke->tail_call.key];\n\t\tif (target) {\n\t\t\tret = __bpf_arch_text_poke(poke->tailcall_target,\n\t\t\t\t\t\t   BPF_MOD_JUMP, NULL,\n\t\t\t\t\t\t   (u8 *)target->bpf_func +\n\t\t\t\t\t\t   poke->adj_off);\n\t\t\tBUG_ON(ret < 0);\n\t\t\tret = __bpf_arch_text_poke(poke->tailcall_bypass,\n\t\t\t\t\t\t   BPF_MOD_JUMP,\n\t\t\t\t\t\t   (u8 *)poke->tailcall_target +\n\t\t\t\t\t\t   X86_PATCH_SIZE, NULL);\n\t\t\tBUG_ON(ret < 0);\n\t\t}\n\t\tWRITE_ONCE(poke->tailcall_target_stable, true);\n\t\tmutex_unlock(&array->aux->poke_mutex);\n\t}\n}\n\nstatic void emit_mov_imm32(u8 **pprog, bool sign_propagate,\n\t\t\t   u32 dst_reg, const u32 imm32)\n{\n\tu8 *prog = *pprog;\n\tu8 b1, b2, b3;\n\n\t \n\tif (sign_propagate && (s32)imm32 < 0) {\n\t\t \n\t\tb1 = add_1mod(0x48, dst_reg);\n\t\tb2 = 0xC7;\n\t\tb3 = 0xC0;\n\t\tEMIT3_off32(b1, b2, add_1reg(b3, dst_reg), imm32);\n\t\tgoto done;\n\t}\n\n\t \n\tif (imm32 == 0) {\n\t\tif (is_ereg(dst_reg))\n\t\t\tEMIT1(add_2mod(0x40, dst_reg, dst_reg));\n\t\tb2 = 0x31;  \n\t\tb3 = 0xC0;\n\t\tEMIT2(b2, add_2reg(b3, dst_reg, dst_reg));\n\t\tgoto done;\n\t}\n\n\t \n\tif (is_ereg(dst_reg))\n\t\tEMIT1(add_1mod(0x40, dst_reg));\n\tEMIT1_off32(add_1reg(0xB8, dst_reg), imm32);\ndone:\n\t*pprog = prog;\n}\n\nstatic void emit_mov_imm64(u8 **pprog, u32 dst_reg,\n\t\t\t   const u32 imm32_hi, const u32 imm32_lo)\n{\n\tu8 *prog = *pprog;\n\n\tif (is_uimm32(((u64)imm32_hi << 32) | (u32)imm32_lo)) {\n\t\t \n\t\temit_mov_imm32(&prog, false, dst_reg, imm32_lo);\n\t} else {\n\t\t \n\t\tEMIT2(add_1mod(0x48, dst_reg), add_1reg(0xB8, dst_reg));\n\t\tEMIT(imm32_lo, 4);\n\t\tEMIT(imm32_hi, 4);\n\t}\n\n\t*pprog = prog;\n}\n\nstatic void emit_mov_reg(u8 **pprog, bool is64, u32 dst_reg, u32 src_reg)\n{\n\tu8 *prog = *pprog;\n\n\tif (is64) {\n\t\t \n\t\tEMIT_mov(dst_reg, src_reg);\n\t} else {\n\t\t \n\t\tif (is_ereg(dst_reg) || is_ereg(src_reg))\n\t\t\tEMIT1(add_2mod(0x40, dst_reg, src_reg));\n\t\tEMIT2(0x89, add_2reg(0xC0, dst_reg, src_reg));\n\t}\n\n\t*pprog = prog;\n}\n\nstatic void emit_movsx_reg(u8 **pprog, int num_bits, bool is64, u32 dst_reg,\n\t\t\t   u32 src_reg)\n{\n\tu8 *prog = *pprog;\n\n\tif (is64) {\n\t\t \n\t\tif (num_bits == 8)\n\t\t\tEMIT4(add_2mod(0x48, src_reg, dst_reg), 0x0f, 0xbe,\n\t\t\t      add_2reg(0xC0, src_reg, dst_reg));\n\t\telse if (num_bits == 16)\n\t\t\tEMIT4(add_2mod(0x48, src_reg, dst_reg), 0x0f, 0xbf,\n\t\t\t      add_2reg(0xC0, src_reg, dst_reg));\n\t\telse if (num_bits == 32)\n\t\t\tEMIT3(add_2mod(0x48, src_reg, dst_reg), 0x63,\n\t\t\t      add_2reg(0xC0, src_reg, dst_reg));\n\t} else {\n\t\t \n\t\tif (num_bits == 8) {\n\t\t\tEMIT4(add_2mod(0x40, src_reg, dst_reg), 0x0f, 0xbe,\n\t\t\t      add_2reg(0xC0, src_reg, dst_reg));\n\t\t} else if (num_bits == 16) {\n\t\t\tif (is_ereg(dst_reg) || is_ereg(src_reg))\n\t\t\t\tEMIT1(add_2mod(0x40, src_reg, dst_reg));\n\t\t\tEMIT3(add_2mod(0x0f, src_reg, dst_reg), 0xbf,\n\t\t\t      add_2reg(0xC0, src_reg, dst_reg));\n\t\t}\n\t}\n\n\t*pprog = prog;\n}\n\n \nstatic void emit_insn_suffix(u8 **pprog, u32 ptr_reg, u32 val_reg, int off)\n{\n\tu8 *prog = *pprog;\n\n\tif (is_imm8(off)) {\n\t\t \n\t\tEMIT2(add_2reg(0x40, ptr_reg, val_reg), off);\n\t} else {\n\t\t \n\t\tEMIT1_off32(add_2reg(0x80, ptr_reg, val_reg), off);\n\t}\n\t*pprog = prog;\n}\n\n \nstatic void maybe_emit_mod(u8 **pprog, u32 dst_reg, u32 src_reg, bool is64)\n{\n\tu8 *prog = *pprog;\n\n\tif (is64)\n\t\tEMIT1(add_2mod(0x48, dst_reg, src_reg));\n\telse if (is_ereg(dst_reg) || is_ereg(src_reg))\n\t\tEMIT1(add_2mod(0x40, dst_reg, src_reg));\n\t*pprog = prog;\n}\n\n \nstatic void maybe_emit_1mod(u8 **pprog, u32 reg, bool is64)\n{\n\tu8 *prog = *pprog;\n\n\tif (is64)\n\t\tEMIT1(add_1mod(0x48, reg));\n\telse if (is_ereg(reg))\n\t\tEMIT1(add_1mod(0x40, reg));\n\t*pprog = prog;\n}\n\n \nstatic void emit_ldx(u8 **pprog, u32 size, u32 dst_reg, u32 src_reg, int off)\n{\n\tu8 *prog = *pprog;\n\n\tswitch (size) {\n\tcase BPF_B:\n\t\t \n\t\tEMIT3(add_2mod(0x48, src_reg, dst_reg), 0x0F, 0xB6);\n\t\tbreak;\n\tcase BPF_H:\n\t\t \n\t\tEMIT3(add_2mod(0x48, src_reg, dst_reg), 0x0F, 0xB7);\n\t\tbreak;\n\tcase BPF_W:\n\t\t \n\t\tif (is_ereg(dst_reg) || is_ereg(src_reg))\n\t\t\tEMIT2(add_2mod(0x40, src_reg, dst_reg), 0x8B);\n\t\telse\n\t\t\tEMIT1(0x8B);\n\t\tbreak;\n\tcase BPF_DW:\n\t\t \n\t\tEMIT2(add_2mod(0x48, src_reg, dst_reg), 0x8B);\n\t\tbreak;\n\t}\n\temit_insn_suffix(&prog, src_reg, dst_reg, off);\n\t*pprog = prog;\n}\n\n \nstatic void emit_ldsx(u8 **pprog, u32 size, u32 dst_reg, u32 src_reg, int off)\n{\n\tu8 *prog = *pprog;\n\n\tswitch (size) {\n\tcase BPF_B:\n\t\t \n\t\tEMIT3(add_2mod(0x48, src_reg, dst_reg), 0x0F, 0xBE);\n\t\tbreak;\n\tcase BPF_H:\n\t\t \n\t\tEMIT3(add_2mod(0x48, src_reg, dst_reg), 0x0F, 0xBF);\n\t\tbreak;\n\tcase BPF_W:\n\t\t \n\t\tEMIT2(add_2mod(0x48, src_reg, dst_reg), 0x63);\n\t\tbreak;\n\t}\n\temit_insn_suffix(&prog, src_reg, dst_reg, off);\n\t*pprog = prog;\n}\n\n \nstatic void emit_stx(u8 **pprog, u32 size, u32 dst_reg, u32 src_reg, int off)\n{\n\tu8 *prog = *pprog;\n\n\tswitch (size) {\n\tcase BPF_B:\n\t\t \n\t\tif (is_ereg(dst_reg) || is_ereg_8l(src_reg))\n\t\t\t \n\t\t\tEMIT2(add_2mod(0x40, dst_reg, src_reg), 0x88);\n\t\telse\n\t\t\tEMIT1(0x88);\n\t\tbreak;\n\tcase BPF_H:\n\t\tif (is_ereg(dst_reg) || is_ereg(src_reg))\n\t\t\tEMIT3(0x66, add_2mod(0x40, dst_reg, src_reg), 0x89);\n\t\telse\n\t\t\tEMIT2(0x66, 0x89);\n\t\tbreak;\n\tcase BPF_W:\n\t\tif (is_ereg(dst_reg) || is_ereg(src_reg))\n\t\t\tEMIT2(add_2mod(0x40, dst_reg, src_reg), 0x89);\n\t\telse\n\t\t\tEMIT1(0x89);\n\t\tbreak;\n\tcase BPF_DW:\n\t\tEMIT2(add_2mod(0x48, dst_reg, src_reg), 0x89);\n\t\tbreak;\n\t}\n\temit_insn_suffix(&prog, dst_reg, src_reg, off);\n\t*pprog = prog;\n}\n\nstatic int emit_atomic(u8 **pprog, u8 atomic_op,\n\t\t       u32 dst_reg, u32 src_reg, s16 off, u8 bpf_size)\n{\n\tu8 *prog = *pprog;\n\n\tEMIT1(0xF0);  \n\n\tmaybe_emit_mod(&prog, dst_reg, src_reg, bpf_size == BPF_DW);\n\n\t \n\tswitch (atomic_op) {\n\tcase BPF_ADD:\n\tcase BPF_AND:\n\tcase BPF_OR:\n\tcase BPF_XOR:\n\t\t \n\t\tEMIT1(simple_alu_opcodes[atomic_op]);\n\t\tbreak;\n\tcase BPF_ADD | BPF_FETCH:\n\t\t \n\t\tEMIT2(0x0F, 0xC1);\n\t\tbreak;\n\tcase BPF_XCHG:\n\t\t \n\t\tEMIT1(0x87);\n\t\tbreak;\n\tcase BPF_CMPXCHG:\n\t\t \n\t\tEMIT2(0x0F, 0xB1);\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"bpf_jit: unknown atomic opcode %02x\\n\", atomic_op);\n\t\treturn -EFAULT;\n\t}\n\n\temit_insn_suffix(&prog, dst_reg, src_reg, off);\n\n\t*pprog = prog;\n\treturn 0;\n}\n\nbool ex_handler_bpf(const struct exception_table_entry *x, struct pt_regs *regs)\n{\n\tu32 reg = x->fixup >> 8;\n\n\t \n\t*(unsigned long *)((void *)regs + reg) = 0;\n\tregs->ip += x->fixup & 0xff;\n\treturn true;\n}\n\nstatic void detect_reg_usage(struct bpf_insn *insn, int insn_cnt,\n\t\t\t     bool *regs_used, bool *tail_call_seen)\n{\n\tint i;\n\n\tfor (i = 1; i <= insn_cnt; i++, insn++) {\n\t\tif (insn->code == (BPF_JMP | BPF_TAIL_CALL))\n\t\t\t*tail_call_seen = true;\n\t\tif (insn->dst_reg == BPF_REG_6 || insn->src_reg == BPF_REG_6)\n\t\t\tregs_used[0] = true;\n\t\tif (insn->dst_reg == BPF_REG_7 || insn->src_reg == BPF_REG_7)\n\t\t\tregs_used[1] = true;\n\t\tif (insn->dst_reg == BPF_REG_8 || insn->src_reg == BPF_REG_8)\n\t\t\tregs_used[2] = true;\n\t\tif (insn->dst_reg == BPF_REG_9 || insn->src_reg == BPF_REG_9)\n\t\t\tregs_used[3] = true;\n\t}\n}\n\nstatic void emit_nops(u8 **pprog, int len)\n{\n\tu8 *prog = *pprog;\n\tint i, noplen;\n\n\twhile (len > 0) {\n\t\tnoplen = len;\n\n\t\tif (noplen > ASM_NOP_MAX)\n\t\t\tnoplen = ASM_NOP_MAX;\n\n\t\tfor (i = 0; i < noplen; i++)\n\t\t\tEMIT1(x86_nops[noplen][i]);\n\t\tlen -= noplen;\n\t}\n\n\t*pprog = prog;\n}\n\n \nstatic void emit_3vex(u8 **pprog, bool r, bool x, bool b, u8 m,\n\t\t      bool w, u8 src_reg2, bool l, u8 pp)\n{\n\tu8 *prog = *pprog;\n\tconst u8 b0 = 0xc4;  \n\tu8 b1, b2;\n\tu8 vvvv = reg2hex[src_reg2];\n\n\t \n\tif (is_ereg(src_reg2))\n\t\tvvvv |= 1 << 3;\n\n\t \n\tb1 = (!r << 7) | (!x << 6) | (!b << 5) | (m & 0x1f);\n\t \n\tb2 = (w << 7) | ((~vvvv & 0xf) << 3) | (l << 2) | (pp & 3);\n\n\tEMIT3(b0, b1, b2);\n\t*pprog = prog;\n}\n\n \nstatic void emit_shiftx(u8 **pprog, u32 dst_reg, u8 src_reg, bool is64, u8 op)\n{\n\tu8 *prog = *pprog;\n\tbool r = is_ereg(dst_reg);\n\tu8 m = 2;  \n\n\temit_3vex(&prog, r, false, r, m, is64, src_reg, false, op);\n\tEMIT2(0xf7, add_2reg(0xC0, dst_reg, dst_reg));\n\t*pprog = prog;\n}\n\n#define INSN_SZ_DIFF (((addrs[i] - addrs[i - 1]) - (prog - temp)))\n\n \n#define RESTORE_TAIL_CALL_CNT(stack)\t\t\t\t\\\n\tEMIT3_off32(0x48, 0x8B, 0x85, -round_up(stack, 8) - 8)\n\nstatic int do_jit(struct bpf_prog *bpf_prog, int *addrs, u8 *image, u8 *rw_image,\n\t\t  int oldproglen, struct jit_context *ctx, bool jmp_padding)\n{\n\tbool tail_call_reachable = bpf_prog->aux->tail_call_reachable;\n\tstruct bpf_insn *insn = bpf_prog->insnsi;\n\tbool callee_regs_used[4] = {};\n\tint insn_cnt = bpf_prog->len;\n\tbool tail_call_seen = false;\n\tbool seen_exit = false;\n\tu8 temp[BPF_MAX_INSN_SIZE + BPF_INSN_SAFETY];\n\tint i, excnt = 0;\n\tint ilen, proglen = 0;\n\tu8 *prog = temp;\n\tint err;\n\n\tdetect_reg_usage(insn, insn_cnt, callee_regs_used,\n\t\t\t &tail_call_seen);\n\n\t \n\ttail_call_reachable |= tail_call_seen;\n\n\temit_prologue(&prog, bpf_prog->aux->stack_depth,\n\t\t      bpf_prog_was_classic(bpf_prog), tail_call_reachable,\n\t\t      bpf_prog->aux->func_idx != 0);\n\tpush_callee_regs(&prog, callee_regs_used);\n\n\tilen = prog - temp;\n\tif (rw_image)\n\t\tmemcpy(rw_image + proglen, temp, ilen);\n\tproglen += ilen;\n\taddrs[0] = proglen;\n\tprog = temp;\n\n\tfor (i = 1; i <= insn_cnt; i++, insn++) {\n\t\tconst s32 imm32 = insn->imm;\n\t\tu32 dst_reg = insn->dst_reg;\n\t\tu32 src_reg = insn->src_reg;\n\t\tu8 b2 = 0, b3 = 0;\n\t\tu8 *start_of_ldx;\n\t\ts64 jmp_offset;\n\t\ts16 insn_off;\n\t\tu8 jmp_cond;\n\t\tu8 *func;\n\t\tint nops;\n\n\t\tswitch (insn->code) {\n\t\t\t \n\t\tcase BPF_ALU | BPF_ADD | BPF_X:\n\t\tcase BPF_ALU | BPF_SUB | BPF_X:\n\t\tcase BPF_ALU | BPF_AND | BPF_X:\n\t\tcase BPF_ALU | BPF_OR | BPF_X:\n\t\tcase BPF_ALU | BPF_XOR | BPF_X:\n\t\tcase BPF_ALU64 | BPF_ADD | BPF_X:\n\t\tcase BPF_ALU64 | BPF_SUB | BPF_X:\n\t\tcase BPF_ALU64 | BPF_AND | BPF_X:\n\t\tcase BPF_ALU64 | BPF_OR | BPF_X:\n\t\tcase BPF_ALU64 | BPF_XOR | BPF_X:\n\t\t\tmaybe_emit_mod(&prog, dst_reg, src_reg,\n\t\t\t\t       BPF_CLASS(insn->code) == BPF_ALU64);\n\t\t\tb2 = simple_alu_opcodes[BPF_OP(insn->code)];\n\t\t\tEMIT2(b2, add_2reg(0xC0, dst_reg, src_reg));\n\t\t\tbreak;\n\n\t\tcase BPF_ALU64 | BPF_MOV | BPF_X:\n\t\tcase BPF_ALU | BPF_MOV | BPF_X:\n\t\t\tif (insn->off == 0)\n\t\t\t\temit_mov_reg(&prog,\n\t\t\t\t\t     BPF_CLASS(insn->code) == BPF_ALU64,\n\t\t\t\t\t     dst_reg, src_reg);\n\t\t\telse\n\t\t\t\temit_movsx_reg(&prog, insn->off,\n\t\t\t\t\t       BPF_CLASS(insn->code) == BPF_ALU64,\n\t\t\t\t\t       dst_reg, src_reg);\n\t\t\tbreak;\n\n\t\t\t \n\t\tcase BPF_ALU | BPF_NEG:\n\t\tcase BPF_ALU64 | BPF_NEG:\n\t\t\tmaybe_emit_1mod(&prog, dst_reg,\n\t\t\t\t\tBPF_CLASS(insn->code) == BPF_ALU64);\n\t\t\tEMIT2(0xF7, add_1reg(0xD8, dst_reg));\n\t\t\tbreak;\n\n\t\tcase BPF_ALU | BPF_ADD | BPF_K:\n\t\tcase BPF_ALU | BPF_SUB | BPF_K:\n\t\tcase BPF_ALU | BPF_AND | BPF_K:\n\t\tcase BPF_ALU | BPF_OR | BPF_K:\n\t\tcase BPF_ALU | BPF_XOR | BPF_K:\n\t\tcase BPF_ALU64 | BPF_ADD | BPF_K:\n\t\tcase BPF_ALU64 | BPF_SUB | BPF_K:\n\t\tcase BPF_ALU64 | BPF_AND | BPF_K:\n\t\tcase BPF_ALU64 | BPF_OR | BPF_K:\n\t\tcase BPF_ALU64 | BPF_XOR | BPF_K:\n\t\t\tmaybe_emit_1mod(&prog, dst_reg,\n\t\t\t\t\tBPF_CLASS(insn->code) == BPF_ALU64);\n\n\t\t\t \n\t\t\tswitch (BPF_OP(insn->code)) {\n\t\t\tcase BPF_ADD:\n\t\t\t\tb3 = 0xC0;\n\t\t\t\tb2 = 0x05;\n\t\t\t\tbreak;\n\t\t\tcase BPF_SUB:\n\t\t\t\tb3 = 0xE8;\n\t\t\t\tb2 = 0x2D;\n\t\t\t\tbreak;\n\t\t\tcase BPF_AND:\n\t\t\t\tb3 = 0xE0;\n\t\t\t\tb2 = 0x25;\n\t\t\t\tbreak;\n\t\t\tcase BPF_OR:\n\t\t\t\tb3 = 0xC8;\n\t\t\t\tb2 = 0x0D;\n\t\t\t\tbreak;\n\t\t\tcase BPF_XOR:\n\t\t\t\tb3 = 0xF0;\n\t\t\t\tb2 = 0x35;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (is_imm8(imm32))\n\t\t\t\tEMIT3(0x83, add_1reg(b3, dst_reg), imm32);\n\t\t\telse if (is_axreg(dst_reg))\n\t\t\t\tEMIT1_off32(b2, imm32);\n\t\t\telse\n\t\t\t\tEMIT2_off32(0x81, add_1reg(b3, dst_reg), imm32);\n\t\t\tbreak;\n\n\t\tcase BPF_ALU64 | BPF_MOV | BPF_K:\n\t\tcase BPF_ALU | BPF_MOV | BPF_K:\n\t\t\temit_mov_imm32(&prog, BPF_CLASS(insn->code) == BPF_ALU64,\n\t\t\t\t       dst_reg, imm32);\n\t\t\tbreak;\n\n\t\tcase BPF_LD | BPF_IMM | BPF_DW:\n\t\t\temit_mov_imm64(&prog, dst_reg, insn[1].imm, insn[0].imm);\n\t\t\tinsn++;\n\t\t\ti++;\n\t\t\tbreak;\n\n\t\t\t \n\t\tcase BPF_ALU | BPF_MOD | BPF_X:\n\t\tcase BPF_ALU | BPF_DIV | BPF_X:\n\t\tcase BPF_ALU | BPF_MOD | BPF_K:\n\t\tcase BPF_ALU | BPF_DIV | BPF_K:\n\t\tcase BPF_ALU64 | BPF_MOD | BPF_X:\n\t\tcase BPF_ALU64 | BPF_DIV | BPF_X:\n\t\tcase BPF_ALU64 | BPF_MOD | BPF_K:\n\t\tcase BPF_ALU64 | BPF_DIV | BPF_K: {\n\t\t\tbool is64 = BPF_CLASS(insn->code) == BPF_ALU64;\n\n\t\t\tif (dst_reg != BPF_REG_0)\n\t\t\t\tEMIT1(0x50);  \n\t\t\tif (dst_reg != BPF_REG_3)\n\t\t\t\tEMIT1(0x52);  \n\n\t\t\tif (BPF_SRC(insn->code) == BPF_X) {\n\t\t\t\tif (src_reg == BPF_REG_0 ||\n\t\t\t\t    src_reg == BPF_REG_3) {\n\t\t\t\t\t \n\t\t\t\t\tEMIT_mov(AUX_REG, src_reg);\n\t\t\t\t\tsrc_reg = AUX_REG;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tEMIT3_off32(0x49, 0xC7, 0xC3, imm32);\n\t\t\t\tsrc_reg = AUX_REG;\n\t\t\t}\n\n\t\t\tif (dst_reg != BPF_REG_0)\n\t\t\t\t \n\t\t\t\temit_mov_reg(&prog, is64, BPF_REG_0, dst_reg);\n\n\t\t\tif (insn->off == 0) {\n\t\t\t\t \n\t\t\t\tEMIT2(0x31, 0xd2);\n\n\t\t\t\t \n\t\t\t\tmaybe_emit_1mod(&prog, src_reg, is64);\n\t\t\t\tEMIT2(0xF7, add_1reg(0xF0, src_reg));\n\t\t\t} else {\n\t\t\t\tif (BPF_CLASS(insn->code) == BPF_ALU)\n\t\t\t\t\tEMIT1(0x99);  \n\t\t\t\telse\n\t\t\t\t\tEMIT2(0x48, 0x99);  \n\n\t\t\t\t \n\t\t\t\tmaybe_emit_1mod(&prog, src_reg, is64);\n\t\t\t\tEMIT2(0xF7, add_1reg(0xF8, src_reg));\n\t\t\t}\n\n\t\t\tif (BPF_OP(insn->code) == BPF_MOD &&\n\t\t\t    dst_reg != BPF_REG_3)\n\t\t\t\t \n\t\t\t\temit_mov_reg(&prog, is64, dst_reg, BPF_REG_3);\n\t\t\telse if (BPF_OP(insn->code) == BPF_DIV &&\n\t\t\t\t dst_reg != BPF_REG_0)\n\t\t\t\t \n\t\t\t\temit_mov_reg(&prog, is64, dst_reg, BPF_REG_0);\n\n\t\t\tif (dst_reg != BPF_REG_3)\n\t\t\t\tEMIT1(0x5A);  \n\t\t\tif (dst_reg != BPF_REG_0)\n\t\t\t\tEMIT1(0x58);  \n\t\t\tbreak;\n\t\t}\n\n\t\tcase BPF_ALU | BPF_MUL | BPF_K:\n\t\tcase BPF_ALU64 | BPF_MUL | BPF_K:\n\t\t\tmaybe_emit_mod(&prog, dst_reg, dst_reg,\n\t\t\t\t       BPF_CLASS(insn->code) == BPF_ALU64);\n\n\t\t\tif (is_imm8(imm32))\n\t\t\t\t \n\t\t\t\tEMIT3(0x6B, add_2reg(0xC0, dst_reg, dst_reg),\n\t\t\t\t      imm32);\n\t\t\telse\n\t\t\t\t \n\t\t\t\tEMIT2_off32(0x69,\n\t\t\t\t\t    add_2reg(0xC0, dst_reg, dst_reg),\n\t\t\t\t\t    imm32);\n\t\t\tbreak;\n\n\t\tcase BPF_ALU | BPF_MUL | BPF_X:\n\t\tcase BPF_ALU64 | BPF_MUL | BPF_X:\n\t\t\tmaybe_emit_mod(&prog, src_reg, dst_reg,\n\t\t\t\t       BPF_CLASS(insn->code) == BPF_ALU64);\n\n\t\t\t \n\t\t\tEMIT3(0x0F, 0xAF, add_2reg(0xC0, src_reg, dst_reg));\n\t\t\tbreak;\n\n\t\t\t \n\t\tcase BPF_ALU | BPF_LSH | BPF_K:\n\t\tcase BPF_ALU | BPF_RSH | BPF_K:\n\t\tcase BPF_ALU | BPF_ARSH | BPF_K:\n\t\tcase BPF_ALU64 | BPF_LSH | BPF_K:\n\t\tcase BPF_ALU64 | BPF_RSH | BPF_K:\n\t\tcase BPF_ALU64 | BPF_ARSH | BPF_K:\n\t\t\tmaybe_emit_1mod(&prog, dst_reg,\n\t\t\t\t\tBPF_CLASS(insn->code) == BPF_ALU64);\n\n\t\t\tb3 = simple_alu_opcodes[BPF_OP(insn->code)];\n\t\t\tif (imm32 == 1)\n\t\t\t\tEMIT2(0xD1, add_1reg(b3, dst_reg));\n\t\t\telse\n\t\t\t\tEMIT3(0xC1, add_1reg(b3, dst_reg), imm32);\n\t\t\tbreak;\n\n\t\tcase BPF_ALU | BPF_LSH | BPF_X:\n\t\tcase BPF_ALU | BPF_RSH | BPF_X:\n\t\tcase BPF_ALU | BPF_ARSH | BPF_X:\n\t\tcase BPF_ALU64 | BPF_LSH | BPF_X:\n\t\tcase BPF_ALU64 | BPF_RSH | BPF_X:\n\t\tcase BPF_ALU64 | BPF_ARSH | BPF_X:\n\t\t\t \n\t\t\tif (boot_cpu_has(X86_FEATURE_BMI2) && src_reg != BPF_REG_4) {\n\t\t\t\t \n\t\t\t\tbool w = (BPF_CLASS(insn->code) == BPF_ALU64);\n\t\t\t\tu8 op;\n\n\t\t\t\tswitch (BPF_OP(insn->code)) {\n\t\t\t\tcase BPF_LSH:\n\t\t\t\t\top = 1;  \n\t\t\t\t\tbreak;\n\t\t\t\tcase BPF_RSH:\n\t\t\t\t\top = 3;  \n\t\t\t\t\tbreak;\n\t\t\t\tcase BPF_ARSH:\n\t\t\t\t\top = 2;  \n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\temit_shiftx(&prog, dst_reg, src_reg, w, op);\n\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (src_reg != BPF_REG_4) {  \n\t\t\t\t \n\t\t\t\tif (dst_reg == BPF_REG_4) {\n\t\t\t\t\t \n\t\t\t\t\tEMIT_mov(AUX_REG, dst_reg);\n\t\t\t\t\tdst_reg = AUX_REG;\n\t\t\t\t} else {\n\t\t\t\t\tEMIT1(0x51);  \n\t\t\t\t}\n\t\t\t\t \n\t\t\t\tEMIT_mov(BPF_REG_4, src_reg);\n\t\t\t}\n\n\t\t\t \n\t\t\tmaybe_emit_1mod(&prog, dst_reg,\n\t\t\t\t\tBPF_CLASS(insn->code) == BPF_ALU64);\n\n\t\t\tb3 = simple_alu_opcodes[BPF_OP(insn->code)];\n\t\t\tEMIT2(0xD3, add_1reg(b3, dst_reg));\n\n\t\t\tif (src_reg != BPF_REG_4) {\n\t\t\t\tif (insn->dst_reg == BPF_REG_4)\n\t\t\t\t\t \n\t\t\t\t\tEMIT_mov(insn->dst_reg, AUX_REG);\n\t\t\t\telse\n\t\t\t\t\tEMIT1(0x59);  \n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tcase BPF_ALU | BPF_END | BPF_FROM_BE:\n\t\tcase BPF_ALU64 | BPF_END | BPF_FROM_LE:\n\t\t\tswitch (imm32) {\n\t\t\tcase 16:\n\t\t\t\t \n\t\t\t\tEMIT1(0x66);\n\t\t\t\tif (is_ereg(dst_reg))\n\t\t\t\t\tEMIT1(0x41);\n\t\t\t\tEMIT3(0xC1, add_1reg(0xC8, dst_reg), 8);\n\n\t\t\t\t \n\t\t\t\tif (is_ereg(dst_reg))\n\t\t\t\t\tEMIT3(0x45, 0x0F, 0xB7);\n\t\t\t\telse\n\t\t\t\t\tEMIT2(0x0F, 0xB7);\n\t\t\t\tEMIT1(add_2reg(0xC0, dst_reg, dst_reg));\n\t\t\t\tbreak;\n\t\t\tcase 32:\n\t\t\t\t \n\t\t\t\tif (is_ereg(dst_reg))\n\t\t\t\t\tEMIT2(0x41, 0x0F);\n\t\t\t\telse\n\t\t\t\t\tEMIT1(0x0F);\n\t\t\t\tEMIT1(add_1reg(0xC8, dst_reg));\n\t\t\t\tbreak;\n\t\t\tcase 64:\n\t\t\t\t \n\t\t\t\tEMIT3(add_1mod(0x48, dst_reg), 0x0F,\n\t\t\t\t      add_1reg(0xC8, dst_reg));\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase BPF_ALU | BPF_END | BPF_FROM_LE:\n\t\t\tswitch (imm32) {\n\t\t\tcase 16:\n\t\t\t\t \n\t\t\t\tif (is_ereg(dst_reg))\n\t\t\t\t\tEMIT3(0x45, 0x0F, 0xB7);\n\t\t\t\telse\n\t\t\t\t\tEMIT2(0x0F, 0xB7);\n\t\t\t\tEMIT1(add_2reg(0xC0, dst_reg, dst_reg));\n\t\t\t\tbreak;\n\t\t\tcase 32:\n\t\t\t\t \n\t\t\t\tif (is_ereg(dst_reg))\n\t\t\t\t\tEMIT1(0x45);\n\t\t\t\tEMIT2(0x89, add_2reg(0xC0, dst_reg, dst_reg));\n\t\t\t\tbreak;\n\t\t\tcase 64:\n\t\t\t\t \n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\t\t \n\t\tcase BPF_ST | BPF_NOSPEC:\n\t\t\tEMIT_LFENCE();\n\t\t\tbreak;\n\n\t\t\t \n\t\tcase BPF_ST | BPF_MEM | BPF_B:\n\t\t\tif (is_ereg(dst_reg))\n\t\t\t\tEMIT2(0x41, 0xC6);\n\t\t\telse\n\t\t\t\tEMIT1(0xC6);\n\t\t\tgoto st;\n\t\tcase BPF_ST | BPF_MEM | BPF_H:\n\t\t\tif (is_ereg(dst_reg))\n\t\t\t\tEMIT3(0x66, 0x41, 0xC7);\n\t\t\telse\n\t\t\t\tEMIT2(0x66, 0xC7);\n\t\t\tgoto st;\n\t\tcase BPF_ST | BPF_MEM | BPF_W:\n\t\t\tif (is_ereg(dst_reg))\n\t\t\t\tEMIT2(0x41, 0xC7);\n\t\t\telse\n\t\t\t\tEMIT1(0xC7);\n\t\t\tgoto st;\n\t\tcase BPF_ST | BPF_MEM | BPF_DW:\n\t\t\tEMIT2(add_1mod(0x48, dst_reg), 0xC7);\n\nst:\t\t\tif (is_imm8(insn->off))\n\t\t\t\tEMIT2(add_1reg(0x40, dst_reg), insn->off);\n\t\t\telse\n\t\t\t\tEMIT1_off32(add_1reg(0x80, dst_reg), insn->off);\n\n\t\t\tEMIT(imm32, bpf_size_to_x86_bytes(BPF_SIZE(insn->code)));\n\t\t\tbreak;\n\n\t\t\t \n\t\tcase BPF_STX | BPF_MEM | BPF_B:\n\t\tcase BPF_STX | BPF_MEM | BPF_H:\n\t\tcase BPF_STX | BPF_MEM | BPF_W:\n\t\tcase BPF_STX | BPF_MEM | BPF_DW:\n\t\t\temit_stx(&prog, BPF_SIZE(insn->code), dst_reg, src_reg, insn->off);\n\t\t\tbreak;\n\n\t\t\t \n\t\tcase BPF_LDX | BPF_MEM | BPF_B:\n\t\tcase BPF_LDX | BPF_PROBE_MEM | BPF_B:\n\t\tcase BPF_LDX | BPF_MEM | BPF_H:\n\t\tcase BPF_LDX | BPF_PROBE_MEM | BPF_H:\n\t\tcase BPF_LDX | BPF_MEM | BPF_W:\n\t\tcase BPF_LDX | BPF_PROBE_MEM | BPF_W:\n\t\tcase BPF_LDX | BPF_MEM | BPF_DW:\n\t\tcase BPF_LDX | BPF_PROBE_MEM | BPF_DW:\n\t\t\t \n\t\tcase BPF_LDX | BPF_MEMSX | BPF_B:\n\t\tcase BPF_LDX | BPF_MEMSX | BPF_H:\n\t\tcase BPF_LDX | BPF_MEMSX | BPF_W:\n\t\tcase BPF_LDX | BPF_PROBE_MEMSX | BPF_B:\n\t\tcase BPF_LDX | BPF_PROBE_MEMSX | BPF_H:\n\t\tcase BPF_LDX | BPF_PROBE_MEMSX | BPF_W:\n\t\t\tinsn_off = insn->off;\n\n\t\t\tif (BPF_MODE(insn->code) == BPF_PROBE_MEM ||\n\t\t\t    BPF_MODE(insn->code) == BPF_PROBE_MEMSX) {\n\t\t\t\t \n\n\t\t\t\tu64 limit = TASK_SIZE_MAX + PAGE_SIZE;\n\t\t\t\tu8 *end_of_jmp;\n\n\t\t\t\t \n\t\t\t\tinsn_off = 0;\n\n\t\t\t\t \n\t\t\t\tEMIT2(add_1mod(0x48, AUX_REG), add_1reg(0xB8, AUX_REG));\n\t\t\t\tEMIT((u32)limit, 4);\n\t\t\t\tEMIT(limit >> 32, 4);\n\n\t\t\t\tif (insn->off) {\n\t\t\t\t\t \n\t\t\t\t\tmaybe_emit_1mod(&prog, src_reg, true);\n\t\t\t\t\tEMIT2_off32(0x81, add_1reg(0xC0, src_reg), insn->off);\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\tmaybe_emit_mod(&prog, src_reg, AUX_REG, true);\n\t\t\t\tEMIT2(0x39, add_2reg(0xC0, src_reg, AUX_REG));\n\n\t\t\t\t \n\t\t\t\tEMIT2(X86_JAE, 0);\n\t\t\t\tend_of_jmp = prog;\n\n\t\t\t\t \n\t\t\t\temit_mov_imm32(&prog, false, dst_reg, 0);\n\t\t\t\t \n\t\t\t\tEMIT2(0xEB, 0);\n\n\t\t\t\t \n\t\t\t\tstart_of_ldx = prog;\n\t\t\t\tend_of_jmp[-1] = start_of_ldx - end_of_jmp;\n\t\t\t}\n\t\t\tif (BPF_MODE(insn->code) == BPF_PROBE_MEMSX ||\n\t\t\t    BPF_MODE(insn->code) == BPF_MEMSX)\n\t\t\t\temit_ldsx(&prog, BPF_SIZE(insn->code), dst_reg, src_reg, insn_off);\n\t\t\telse\n\t\t\t\temit_ldx(&prog, BPF_SIZE(insn->code), dst_reg, src_reg, insn_off);\n\t\t\tif (BPF_MODE(insn->code) == BPF_PROBE_MEM ||\n\t\t\t    BPF_MODE(insn->code) == BPF_PROBE_MEMSX) {\n\t\t\t\tstruct exception_table_entry *ex;\n\t\t\t\tu8 *_insn = image + proglen + (start_of_ldx - temp);\n\t\t\t\ts64 delta;\n\n\t\t\t\t \n\t\t\t\tstart_of_ldx[-1] = prog - start_of_ldx;\n\n\t\t\t\tif (insn->off && src_reg != dst_reg) {\n\t\t\t\t\t \n\t\t\t\t\tmaybe_emit_1mod(&prog, src_reg, true);\n\t\t\t\t\tEMIT2_off32(0x81, add_1reg(0xE8, src_reg), insn->off);\n\t\t\t\t}\n\n\t\t\t\tif (!bpf_prog->aux->extable)\n\t\t\t\t\tbreak;\n\n\t\t\t\tif (excnt >= bpf_prog->aux->num_exentries) {\n\t\t\t\t\tpr_err(\"ex gen bug\\n\");\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t\tex = &bpf_prog->aux->extable[excnt++];\n\n\t\t\t\tdelta = _insn - (u8 *)&ex->insn;\n\t\t\t\tif (!is_simm32(delta)) {\n\t\t\t\t\tpr_err(\"extable->insn doesn't fit into 32-bit\\n\");\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t\t \n\t\t\t\tex = (void *)rw_image + ((void *)ex - (void *)image);\n\n\t\t\t\tex->insn = delta;\n\n\t\t\t\tex->data = EX_TYPE_BPF;\n\n\t\t\t\tif (dst_reg > BPF_REG_9) {\n\t\t\t\t\tpr_err(\"verifier error\\n\");\n\t\t\t\t\treturn -EFAULT;\n\t\t\t\t}\n\t\t\t\t \n\t\t\t\tex->fixup = (prog - start_of_ldx) | (reg2pt_regs[dst_reg] << 8);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase BPF_STX | BPF_ATOMIC | BPF_W:\n\t\tcase BPF_STX | BPF_ATOMIC | BPF_DW:\n\t\t\tif (insn->imm == (BPF_AND | BPF_FETCH) ||\n\t\t\t    insn->imm == (BPF_OR | BPF_FETCH) ||\n\t\t\t    insn->imm == (BPF_XOR | BPF_FETCH)) {\n\t\t\t\tbool is64 = BPF_SIZE(insn->code) == BPF_DW;\n\t\t\t\tu32 real_src_reg = src_reg;\n\t\t\t\tu32 real_dst_reg = dst_reg;\n\t\t\t\tu8 *branch_target;\n\n\t\t\t\t \n\n\t\t\t\t \n\t\t\t\temit_mov_reg(&prog, true, BPF_REG_AX, BPF_REG_0);\n\t\t\t\tif (src_reg == BPF_REG_0)\n\t\t\t\t\treal_src_reg = BPF_REG_AX;\n\t\t\t\tif (dst_reg == BPF_REG_0)\n\t\t\t\t\treal_dst_reg = BPF_REG_AX;\n\n\t\t\t\tbranch_target = prog;\n\t\t\t\t \n\t\t\t\temit_ldx(&prog, BPF_SIZE(insn->code),\n\t\t\t\t\t BPF_REG_0, real_dst_reg, insn->off);\n\t\t\t\t \n\t\t\t\temit_mov_reg(&prog, is64, AUX_REG, BPF_REG_0);\n\t\t\t\tmaybe_emit_mod(&prog, AUX_REG, real_src_reg, is64);\n\t\t\t\tEMIT2(simple_alu_opcodes[BPF_OP(insn->imm)],\n\t\t\t\t      add_2reg(0xC0, AUX_REG, real_src_reg));\n\t\t\t\t \n\t\t\t\terr = emit_atomic(&prog, BPF_CMPXCHG,\n\t\t\t\t\t\t  real_dst_reg, AUX_REG,\n\t\t\t\t\t\t  insn->off,\n\t\t\t\t\t\t  BPF_SIZE(insn->code));\n\t\t\t\tif (WARN_ON(err))\n\t\t\t\t\treturn err;\n\t\t\t\t \n\t\t\t\tEMIT2(X86_JNE, -(prog - branch_target) - 2);\n\t\t\t\t \n\t\t\t\temit_mov_reg(&prog, is64, real_src_reg, BPF_REG_0);\n\t\t\t\t \n\t\t\t\temit_mov_reg(&prog, true, BPF_REG_0, BPF_REG_AX);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\terr = emit_atomic(&prog, insn->imm, dst_reg, src_reg,\n\t\t\t\t\t  insn->off, BPF_SIZE(insn->code));\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tbreak;\n\n\t\t\t \n\t\tcase BPF_JMP | BPF_CALL: {\n\t\t\tint offs;\n\n\t\t\tfunc = (u8 *) __bpf_call_base + imm32;\n\t\t\tif (tail_call_reachable) {\n\t\t\t\tRESTORE_TAIL_CALL_CNT(bpf_prog->aux->stack_depth);\n\t\t\t\tif (!imm32)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\toffs = 7 + x86_call_depth_emit_accounting(&prog, func);\n\t\t\t} else {\n\t\t\t\tif (!imm32)\n\t\t\t\t\treturn -EINVAL;\n\t\t\t\toffs = x86_call_depth_emit_accounting(&prog, func);\n\t\t\t}\n\t\t\tif (emit_call(&prog, func, image + addrs[i - 1] + offs))\n\t\t\t\treturn -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tcase BPF_JMP | BPF_TAIL_CALL:\n\t\t\tif (imm32)\n\t\t\t\temit_bpf_tail_call_direct(&bpf_prog->aux->poke_tab[imm32 - 1],\n\t\t\t\t\t\t\t  &prog, image + addrs[i - 1],\n\t\t\t\t\t\t\t  callee_regs_used,\n\t\t\t\t\t\t\t  bpf_prog->aux->stack_depth,\n\t\t\t\t\t\t\t  ctx);\n\t\t\telse\n\t\t\t\temit_bpf_tail_call_indirect(&prog,\n\t\t\t\t\t\t\t    callee_regs_used,\n\t\t\t\t\t\t\t    bpf_prog->aux->stack_depth,\n\t\t\t\t\t\t\t    image + addrs[i - 1],\n\t\t\t\t\t\t\t    ctx);\n\t\t\tbreak;\n\n\t\t\t \n\t\tcase BPF_JMP | BPF_JEQ | BPF_X:\n\t\tcase BPF_JMP | BPF_JNE | BPF_X:\n\t\tcase BPF_JMP | BPF_JGT | BPF_X:\n\t\tcase BPF_JMP | BPF_JLT | BPF_X:\n\t\tcase BPF_JMP | BPF_JGE | BPF_X:\n\t\tcase BPF_JMP | BPF_JLE | BPF_X:\n\t\tcase BPF_JMP | BPF_JSGT | BPF_X:\n\t\tcase BPF_JMP | BPF_JSLT | BPF_X:\n\t\tcase BPF_JMP | BPF_JSGE | BPF_X:\n\t\tcase BPF_JMP | BPF_JSLE | BPF_X:\n\t\tcase BPF_JMP32 | BPF_JEQ | BPF_X:\n\t\tcase BPF_JMP32 | BPF_JNE | BPF_X:\n\t\tcase BPF_JMP32 | BPF_JGT | BPF_X:\n\t\tcase BPF_JMP32 | BPF_JLT | BPF_X:\n\t\tcase BPF_JMP32 | BPF_JGE | BPF_X:\n\t\tcase BPF_JMP32 | BPF_JLE | BPF_X:\n\t\tcase BPF_JMP32 | BPF_JSGT | BPF_X:\n\t\tcase BPF_JMP32 | BPF_JSLT | BPF_X:\n\t\tcase BPF_JMP32 | BPF_JSGE | BPF_X:\n\t\tcase BPF_JMP32 | BPF_JSLE | BPF_X:\n\t\t\t \n\t\t\tmaybe_emit_mod(&prog, dst_reg, src_reg,\n\t\t\t\t       BPF_CLASS(insn->code) == BPF_JMP);\n\t\t\tEMIT2(0x39, add_2reg(0xC0, dst_reg, src_reg));\n\t\t\tgoto emit_cond_jmp;\n\n\t\tcase BPF_JMP | BPF_JSET | BPF_X:\n\t\tcase BPF_JMP32 | BPF_JSET | BPF_X:\n\t\t\t \n\t\t\tmaybe_emit_mod(&prog, dst_reg, src_reg,\n\t\t\t\t       BPF_CLASS(insn->code) == BPF_JMP);\n\t\t\tEMIT2(0x85, add_2reg(0xC0, dst_reg, src_reg));\n\t\t\tgoto emit_cond_jmp;\n\n\t\tcase BPF_JMP | BPF_JSET | BPF_K:\n\t\tcase BPF_JMP32 | BPF_JSET | BPF_K:\n\t\t\t \n\t\t\tmaybe_emit_1mod(&prog, dst_reg,\n\t\t\t\t\tBPF_CLASS(insn->code) == BPF_JMP);\n\t\t\tEMIT2_off32(0xF7, add_1reg(0xC0, dst_reg), imm32);\n\t\t\tgoto emit_cond_jmp;\n\n\t\tcase BPF_JMP | BPF_JEQ | BPF_K:\n\t\tcase BPF_JMP | BPF_JNE | BPF_K:\n\t\tcase BPF_JMP | BPF_JGT | BPF_K:\n\t\tcase BPF_JMP | BPF_JLT | BPF_K:\n\t\tcase BPF_JMP | BPF_JGE | BPF_K:\n\t\tcase BPF_JMP | BPF_JLE | BPF_K:\n\t\tcase BPF_JMP | BPF_JSGT | BPF_K:\n\t\tcase BPF_JMP | BPF_JSLT | BPF_K:\n\t\tcase BPF_JMP | BPF_JSGE | BPF_K:\n\t\tcase BPF_JMP | BPF_JSLE | BPF_K:\n\t\tcase BPF_JMP32 | BPF_JEQ | BPF_K:\n\t\tcase BPF_JMP32 | BPF_JNE | BPF_K:\n\t\tcase BPF_JMP32 | BPF_JGT | BPF_K:\n\t\tcase BPF_JMP32 | BPF_JLT | BPF_K:\n\t\tcase BPF_JMP32 | BPF_JGE | BPF_K:\n\t\tcase BPF_JMP32 | BPF_JLE | BPF_K:\n\t\tcase BPF_JMP32 | BPF_JSGT | BPF_K:\n\t\tcase BPF_JMP32 | BPF_JSLT | BPF_K:\n\t\tcase BPF_JMP32 | BPF_JSGE | BPF_K:\n\t\tcase BPF_JMP32 | BPF_JSLE | BPF_K:\n\t\t\t \n\t\t\tif (imm32 == 0) {\n\t\t\t\tmaybe_emit_mod(&prog, dst_reg, dst_reg,\n\t\t\t\t\t       BPF_CLASS(insn->code) == BPF_JMP);\n\t\t\t\tEMIT2(0x85, add_2reg(0xC0, dst_reg, dst_reg));\n\t\t\t\tgoto emit_cond_jmp;\n\t\t\t}\n\n\t\t\t \n\t\t\tmaybe_emit_1mod(&prog, dst_reg,\n\t\t\t\t\tBPF_CLASS(insn->code) == BPF_JMP);\n\n\t\t\tif (is_imm8(imm32))\n\t\t\t\tEMIT3(0x83, add_1reg(0xF8, dst_reg), imm32);\n\t\t\telse\n\t\t\t\tEMIT2_off32(0x81, add_1reg(0xF8, dst_reg), imm32);\n\nemit_cond_jmp:\t\t \n\t\t\tswitch (BPF_OP(insn->code)) {\n\t\t\tcase BPF_JEQ:\n\t\t\t\tjmp_cond = X86_JE;\n\t\t\t\tbreak;\n\t\t\tcase BPF_JSET:\n\t\t\tcase BPF_JNE:\n\t\t\t\tjmp_cond = X86_JNE;\n\t\t\t\tbreak;\n\t\t\tcase BPF_JGT:\n\t\t\t\t \n\t\t\t\tjmp_cond = X86_JA;\n\t\t\t\tbreak;\n\t\t\tcase BPF_JLT:\n\t\t\t\t \n\t\t\t\tjmp_cond = X86_JB;\n\t\t\t\tbreak;\n\t\t\tcase BPF_JGE:\n\t\t\t\t \n\t\t\t\tjmp_cond = X86_JAE;\n\t\t\t\tbreak;\n\t\t\tcase BPF_JLE:\n\t\t\t\t \n\t\t\t\tjmp_cond = X86_JBE;\n\t\t\t\tbreak;\n\t\t\tcase BPF_JSGT:\n\t\t\t\t \n\t\t\t\tjmp_cond = X86_JG;\n\t\t\t\tbreak;\n\t\t\tcase BPF_JSLT:\n\t\t\t\t \n\t\t\t\tjmp_cond = X86_JL;\n\t\t\t\tbreak;\n\t\t\tcase BPF_JSGE:\n\t\t\t\t \n\t\t\t\tjmp_cond = X86_JGE;\n\t\t\t\tbreak;\n\t\t\tcase BPF_JSLE:\n\t\t\t\t \n\t\t\t\tjmp_cond = X86_JLE;\n\t\t\t\tbreak;\n\t\t\tdefault:  \n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\tjmp_offset = addrs[i + insn->off] - addrs[i];\n\t\t\tif (is_imm8(jmp_offset)) {\n\t\t\t\tif (jmp_padding) {\n\t\t\t\t\t \n\t\t\t\t\tnops = INSN_SZ_DIFF - 2;\n\t\t\t\t\tif (nops != 0 && nops != 4) {\n\t\t\t\t\t\tpr_err(\"unexpected jmp_cond padding: %d bytes\\n\",\n\t\t\t\t\t\t       nops);\n\t\t\t\t\t\treturn -EFAULT;\n\t\t\t\t\t}\n\t\t\t\t\temit_nops(&prog, nops);\n\t\t\t\t}\n\t\t\t\tEMIT2(jmp_cond, jmp_offset);\n\t\t\t} else if (is_simm32(jmp_offset)) {\n\t\t\t\tEMIT2_off32(0x0F, jmp_cond + 0x10, jmp_offset);\n\t\t\t} else {\n\t\t\t\tpr_err(\"cond_jmp gen bug %llx\\n\", jmp_offset);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\n\t\t\tbreak;\n\n\t\tcase BPF_JMP | BPF_JA:\n\t\tcase BPF_JMP32 | BPF_JA:\n\t\t\tif (BPF_CLASS(insn->code) == BPF_JMP) {\n\t\t\t\tif (insn->off == -1)\n\t\t\t\t\t \n\t\t\t\t\tjmp_offset = -2;\n\t\t\t\telse\n\t\t\t\t\tjmp_offset = addrs[i + insn->off] - addrs[i];\n\t\t\t} else {\n\t\t\t\tif (insn->imm == -1)\n\t\t\t\t\tjmp_offset = -2;\n\t\t\t\telse\n\t\t\t\t\tjmp_offset = addrs[i + insn->imm] - addrs[i];\n\t\t\t}\n\n\t\t\tif (!jmp_offset) {\n\t\t\t\t \n\t\t\t\tif (jmp_padding) {\n\t\t\t\t\t \n\t\t\t\t\tnops = INSN_SZ_DIFF;\n\t\t\t\t\tif (nops != 0 && nops != 2 && nops != 5) {\n\t\t\t\t\t\tpr_err(\"unexpected nop jump padding: %d bytes\\n\",\n\t\t\t\t\t\t       nops);\n\t\t\t\t\t\treturn -EFAULT;\n\t\t\t\t\t}\n\t\t\t\t\temit_nops(&prog, nops);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\nemit_jmp:\n\t\t\tif (is_imm8(jmp_offset)) {\n\t\t\t\tif (jmp_padding) {\n\t\t\t\t\t \n\t\t\t\t\tnops = INSN_SZ_DIFF - 2;\n\t\t\t\t\tif (nops != 0 && nops != 3) {\n\t\t\t\t\t\tpr_err(\"unexpected jump padding: %d bytes\\n\",\n\t\t\t\t\t\t       nops);\n\t\t\t\t\t\treturn -EFAULT;\n\t\t\t\t\t}\n\t\t\t\t\temit_nops(&prog, INSN_SZ_DIFF - 2);\n\t\t\t\t}\n\t\t\t\tEMIT2(0xEB, jmp_offset);\n\t\t\t} else if (is_simm32(jmp_offset)) {\n\t\t\t\tEMIT1_off32(0xE9, jmp_offset);\n\t\t\t} else {\n\t\t\t\tpr_err(\"jmp gen bug %llx\\n\", jmp_offset);\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase BPF_JMP | BPF_EXIT:\n\t\t\tif (seen_exit) {\n\t\t\t\tjmp_offset = ctx->cleanup_addr - addrs[i];\n\t\t\t\tgoto emit_jmp;\n\t\t\t}\n\t\t\tseen_exit = true;\n\t\t\t \n\t\t\tctx->cleanup_addr = proglen;\n\t\t\tpop_callee_regs(&prog, callee_regs_used);\n\t\t\tEMIT1(0xC9);          \n\t\t\temit_return(&prog, image + addrs[i - 1] + (prog - temp));\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\t \n\t\t\tpr_err(\"bpf_jit: unknown opcode %02x\\n\", insn->code);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tilen = prog - temp;\n\t\tif (ilen > BPF_MAX_INSN_SIZE) {\n\t\t\tpr_err(\"bpf_jit: fatal insn size error\\n\");\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tif (image) {\n\t\t\t \n\t\t\tif (unlikely(proglen + ilen > oldproglen ||\n\t\t\t\t     proglen + ilen != addrs[i])) {\n\t\t\t\tpr_err(\"bpf_jit: fatal error\\n\");\n\t\t\t\treturn -EFAULT;\n\t\t\t}\n\t\t\tmemcpy(rw_image + proglen, temp, ilen);\n\t\t}\n\t\tproglen += ilen;\n\t\taddrs[i] = proglen;\n\t\tprog = temp;\n\t}\n\n\tif (image && excnt != bpf_prog->aux->num_exentries) {\n\t\tpr_err(\"extable is not populated\\n\");\n\t\treturn -EFAULT;\n\t}\n\treturn proglen;\n}\n\nstatic void clean_stack_garbage(const struct btf_func_model *m,\n\t\t\t\tu8 **pprog, int nr_stack_slots,\n\t\t\t\tint stack_size)\n{\n\tint arg_size, off;\n\tu8 *prog;\n\n\t \n\tif (nr_stack_slots != 1)\n\t\treturn;\n\n\t \n\targ_size = m->arg_size[m->nr_args - 1];\n\tif (arg_size <= 4) {\n\t\toff = -(stack_size - 4);\n\t\tprog = *pprog;\n\t\t \n\t\tif (!is_imm8(off))\n\t\t\tEMIT2_off32(0xC7, 0x85, off);\n\t\telse\n\t\t\tEMIT3(0xC7, 0x45, off);\n\t\tEMIT(0, 4);\n\t\t*pprog = prog;\n\t}\n}\n\n \nstatic int get_nr_used_regs(const struct btf_func_model *m)\n{\n\tint i, arg_regs, nr_used_regs = 0;\n\n\tfor (i = 0; i < min_t(int, m->nr_args, MAX_BPF_FUNC_ARGS); i++) {\n\t\targ_regs = (m->arg_size[i] + 7) / 8;\n\t\tif (nr_used_regs + arg_regs <= 6)\n\t\t\tnr_used_regs += arg_regs;\n\n\t\tif (nr_used_regs >= 6)\n\t\t\tbreak;\n\t}\n\n\treturn nr_used_regs;\n}\n\nstatic void save_args(const struct btf_func_model *m, u8 **prog,\n\t\t      int stack_size, bool for_call_origin)\n{\n\tint arg_regs, first_off = 0, nr_regs = 0, nr_stack_slots = 0;\n\tint i, j;\n\n\t \n\tfor (i = 0; i < min_t(int, m->nr_args, MAX_BPF_FUNC_ARGS); i++) {\n\t\targ_regs = (m->arg_size[i] + 7) / 8;\n\n\t\t \n\t\tif (nr_regs + arg_regs > 6) {\n\t\t\t \n\t\t\tfor (j = 0; j < arg_regs; j++) {\n\t\t\t\temit_ldx(prog, BPF_DW, BPF_REG_0, BPF_REG_FP,\n\t\t\t\t\t nr_stack_slots * 8 + 0x18);\n\t\t\t\temit_stx(prog, BPF_DW, BPF_REG_FP, BPF_REG_0,\n\t\t\t\t\t -stack_size);\n\n\t\t\t\tif (!nr_stack_slots)\n\t\t\t\t\tfirst_off = stack_size;\n\t\t\t\tstack_size -= 8;\n\t\t\t\tnr_stack_slots++;\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tif (for_call_origin) {\n\t\t\t\tnr_regs += arg_regs;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tfor (j = 0; j < arg_regs; j++) {\n\t\t\t\temit_stx(prog, BPF_DW, BPF_REG_FP,\n\t\t\t\t\t nr_regs == 5 ? X86_REG_R9 : BPF_REG_1 + nr_regs,\n\t\t\t\t\t -stack_size);\n\t\t\t\tstack_size -= 8;\n\t\t\t\tnr_regs++;\n\t\t\t}\n\t\t}\n\t}\n\n\tclean_stack_garbage(m, prog, nr_stack_slots, first_off);\n}\n\nstatic void restore_regs(const struct btf_func_model *m, u8 **prog,\n\t\t\t int stack_size)\n{\n\tint i, j, arg_regs, nr_regs = 0;\n\n\t \n\tfor (i = 0; i < min_t(int, m->nr_args, MAX_BPF_FUNC_ARGS); i++) {\n\t\targ_regs = (m->arg_size[i] + 7) / 8;\n\t\tif (nr_regs + arg_regs <= 6) {\n\t\t\tfor (j = 0; j < arg_regs; j++) {\n\t\t\t\temit_ldx(prog, BPF_DW,\n\t\t\t\t\t nr_regs == 5 ? X86_REG_R9 : BPF_REG_1 + nr_regs,\n\t\t\t\t\t BPF_REG_FP,\n\t\t\t\t\t -stack_size);\n\t\t\t\tstack_size -= 8;\n\t\t\t\tnr_regs++;\n\t\t\t}\n\t\t} else {\n\t\t\tstack_size -= 8 * arg_regs;\n\t\t}\n\n\t\tif (nr_regs >= 6)\n\t\t\tbreak;\n\t}\n}\n\nstatic int invoke_bpf_prog(const struct btf_func_model *m, u8 **pprog,\n\t\t\t   struct bpf_tramp_link *l, int stack_size,\n\t\t\t   int run_ctx_off, bool save_ret)\n{\n\tu8 *prog = *pprog;\n\tu8 *jmp_insn;\n\tint ctx_cookie_off = offsetof(struct bpf_tramp_run_ctx, bpf_cookie);\n\tstruct bpf_prog *p = l->link.prog;\n\tu64 cookie = l->cookie;\n\n\t \n\temit_mov_imm64(&prog, BPF_REG_1, (long) cookie >> 32, (u32) (long) cookie);\n\n\t \n\temit_stx(&prog, BPF_DW, BPF_REG_FP, BPF_REG_1, -run_ctx_off + ctx_cookie_off);\n\n\t \n\temit_mov_imm64(&prog, BPF_REG_1, (long) p >> 32, (u32) (long) p);\n\t \n\tif (!is_imm8(-run_ctx_off))\n\t\tEMIT3_off32(0x48, 0x8D, 0xB5, -run_ctx_off);\n\telse\n\t\tEMIT4(0x48, 0x8D, 0x75, -run_ctx_off);\n\n\tif (emit_rsb_call(&prog, bpf_trampoline_enter(p), prog))\n\t\treturn -EINVAL;\n\t \n\temit_mov_reg(&prog, true, BPF_REG_6, BPF_REG_0);\n\n\t \n\tEMIT3(0x48, 0x85, 0xC0);   \n\t \n\tjmp_insn = prog;\n\temit_nops(&prog, 2);\n\n\t \n\tif (!is_imm8(-stack_size))\n\t\tEMIT3_off32(0x48, 0x8D, 0xBD, -stack_size);\n\telse\n\t\tEMIT4(0x48, 0x8D, 0x7D, -stack_size);\n\t \n\tif (!p->jited)\n\t\temit_mov_imm64(&prog, BPF_REG_2,\n\t\t\t       (long) p->insnsi >> 32,\n\t\t\t       (u32) (long) p->insnsi);\n\t \n\tif (emit_rsb_call(&prog, p->bpf_func, prog))\n\t\treturn -EINVAL;\n\n\t \n\tif (save_ret)\n\t\temit_stx(&prog, BPF_DW, BPF_REG_FP, BPF_REG_0, -8);\n\n\t \n\tjmp_insn[0] = X86_JE;\n\tjmp_insn[1] = prog - jmp_insn - 2;\n\n\t \n\temit_mov_imm64(&prog, BPF_REG_1, (long) p >> 32, (u32) (long) p);\n\t \n\temit_mov_reg(&prog, true, BPF_REG_2, BPF_REG_6);\n\t \n\tif (!is_imm8(-run_ctx_off))\n\t\tEMIT3_off32(0x48, 0x8D, 0x95, -run_ctx_off);\n\telse\n\t\tEMIT4(0x48, 0x8D, 0x55, -run_ctx_off);\n\tif (emit_rsb_call(&prog, bpf_trampoline_exit(p), prog))\n\t\treturn -EINVAL;\n\n\t*pprog = prog;\n\treturn 0;\n}\n\nstatic void emit_align(u8 **pprog, u32 align)\n{\n\tu8 *target, *prog = *pprog;\n\n\ttarget = PTR_ALIGN(prog, align);\n\tif (target != prog)\n\t\temit_nops(&prog, target - prog);\n\n\t*pprog = prog;\n}\n\nstatic int emit_cond_near_jump(u8 **pprog, void *func, void *ip, u8 jmp_cond)\n{\n\tu8 *prog = *pprog;\n\ts64 offset;\n\n\toffset = func - (ip + 2 + 4);\n\tif (!is_simm32(offset)) {\n\t\tpr_err(\"Target %p is out of range\\n\", func);\n\t\treturn -EINVAL;\n\t}\n\tEMIT2_off32(0x0F, jmp_cond + 0x10, offset);\n\t*pprog = prog;\n\treturn 0;\n}\n\nstatic int invoke_bpf(const struct btf_func_model *m, u8 **pprog,\n\t\t      struct bpf_tramp_links *tl, int stack_size,\n\t\t      int run_ctx_off, bool save_ret)\n{\n\tint i;\n\tu8 *prog = *pprog;\n\n\tfor (i = 0; i < tl->nr_links; i++) {\n\t\tif (invoke_bpf_prog(m, &prog, tl->links[i], stack_size,\n\t\t\t\t    run_ctx_off, save_ret))\n\t\t\treturn -EINVAL;\n\t}\n\t*pprog = prog;\n\treturn 0;\n}\n\nstatic int invoke_bpf_mod_ret(const struct btf_func_model *m, u8 **pprog,\n\t\t\t      struct bpf_tramp_links *tl, int stack_size,\n\t\t\t      int run_ctx_off, u8 **branches)\n{\n\tu8 *prog = *pprog;\n\tint i;\n\n\t \n\temit_mov_imm32(&prog, false, BPF_REG_0, 0);\n\temit_stx(&prog, BPF_DW, BPF_REG_FP, BPF_REG_0, -8);\n\tfor (i = 0; i < tl->nr_links; i++) {\n\t\tif (invoke_bpf_prog(m, &prog, tl->links[i], stack_size, run_ctx_off, true))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\t \n\t\tEMIT4(0x48, 0x83, 0x7d, 0xf8); EMIT1(0x00);\n\n\t\t \n\t\tbranches[i] = prog;\n\t\temit_nops(&prog, 4 + 2);\n\t}\n\n\t*pprog = prog;\n\treturn 0;\n}\n\n \nint arch_prepare_bpf_trampoline(struct bpf_tramp_image *im, void *image, void *image_end,\n\t\t\t\tconst struct btf_func_model *m, u32 flags,\n\t\t\t\tstruct bpf_tramp_links *tlinks,\n\t\t\t\tvoid *func_addr)\n{\n\tint i, ret, nr_regs = m->nr_args, stack_size = 0;\n\tint regs_off, nregs_off, ip_off, run_ctx_off, arg_stack_off, rbx_off;\n\tstruct bpf_tramp_links *fentry = &tlinks[BPF_TRAMP_FENTRY];\n\tstruct bpf_tramp_links *fexit = &tlinks[BPF_TRAMP_FEXIT];\n\tstruct bpf_tramp_links *fmod_ret = &tlinks[BPF_TRAMP_MODIFY_RETURN];\n\tvoid *orig_call = func_addr;\n\tu8 **branches = NULL;\n\tu8 *prog;\n\tbool save_ret;\n\n\t \n\tfor (i = 0; i < m->nr_args; i++)\n\t\tif (m->arg_flags[i] & BTF_FMODEL_STRUCT_ARG)\n\t\t\tnr_regs += (m->arg_size[i] + 7) / 8 - 1;\n\n\t \n\tif (nr_regs > MAX_BPF_FUNC_ARGS)\n\t\treturn -ENOTSUPP;\n\n\t \n\n\t \n\tsave_ret = flags & (BPF_TRAMP_F_CALL_ORIG | BPF_TRAMP_F_RET_FENTRY_RET);\n\tif (save_ret)\n\t\tstack_size += 8;\n\n\tstack_size += nr_regs * 8;\n\tregs_off = stack_size;\n\n\t \n\tstack_size += 8;\n\tnregs_off = stack_size;\n\n\tif (flags & BPF_TRAMP_F_IP_ARG)\n\t\tstack_size += 8;  \n\n\tip_off = stack_size;\n\n\tstack_size += 8;\n\trbx_off = stack_size;\n\n\tstack_size += (sizeof(struct bpf_tramp_run_ctx) + 7) & ~0x7;\n\trun_ctx_off = stack_size;\n\n\tif (nr_regs > 6 && (flags & BPF_TRAMP_F_CALL_ORIG)) {\n\t\t \n\t\tstack_size += (nr_regs - get_nr_used_regs(m)) * 8;\n\t\t \n\t\tstack_size += (stack_size % 16) ? 0 : 8;\n\t}\n\n\targ_stack_off = stack_size;\n\n\tif (flags & BPF_TRAMP_F_SKIP_FRAME) {\n\t\t \n\t\tif (is_endbr(*(u32 *)orig_call))\n\t\t\torig_call += ENDBR_INSN_SIZE;\n\t\torig_call += X86_PATCH_SIZE;\n\t}\n\n\tprog = image;\n\n\tEMIT_ENDBR();\n\t \n\tx86_call_depth_emit_accounting(&prog, NULL);\n\tEMIT1(0x55);\t\t  \n\tEMIT3(0x48, 0x89, 0xE5);  \n\tif (!is_imm8(stack_size))\n\t\t \n\t\tEMIT3_off32(0x48, 0x81, 0xEC, stack_size);\n\telse\n\t\t \n\t\tEMIT4(0x48, 0x83, 0xEC, stack_size);\n\tif (flags & BPF_TRAMP_F_TAIL_CALL_CTX)\n\t\tEMIT1(0x50);\t\t \n\t \n\temit_stx(&prog, BPF_DW, BPF_REG_FP, BPF_REG_6, -rbx_off);\n\n\t \n\temit_mov_imm64(&prog, BPF_REG_0, 0, (u32) nr_regs);\n\temit_stx(&prog, BPF_DW, BPF_REG_FP, BPF_REG_0, -nregs_off);\n\n\tif (flags & BPF_TRAMP_F_IP_ARG) {\n\t\t \n\t\temit_mov_imm64(&prog, BPF_REG_0, (long) func_addr >> 32, (u32) (long) func_addr);\n\t\temit_stx(&prog, BPF_DW, BPF_REG_FP, BPF_REG_0, -ip_off);\n\t}\n\n\tsave_args(m, &prog, regs_off, false);\n\n\tif (flags & BPF_TRAMP_F_CALL_ORIG) {\n\t\t \n\t\temit_mov_imm64(&prog, BPF_REG_1, (long) im >> 32, (u32) (long) im);\n\t\tif (emit_rsb_call(&prog, __bpf_tramp_enter, prog)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t}\n\n\tif (fentry->nr_links)\n\t\tif (invoke_bpf(m, &prog, fentry, regs_off, run_ctx_off,\n\t\t\t       flags & BPF_TRAMP_F_RET_FENTRY_RET))\n\t\t\treturn -EINVAL;\n\n\tif (fmod_ret->nr_links) {\n\t\tbranches = kcalloc(fmod_ret->nr_links, sizeof(u8 *),\n\t\t\t\t   GFP_KERNEL);\n\t\tif (!branches)\n\t\t\treturn -ENOMEM;\n\n\t\tif (invoke_bpf_mod_ret(m, &prog, fmod_ret, regs_off,\n\t\t\t\t       run_ctx_off, branches)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t}\n\n\tif (flags & BPF_TRAMP_F_CALL_ORIG) {\n\t\trestore_regs(m, &prog, regs_off);\n\t\tsave_args(m, &prog, arg_stack_off, true);\n\n\t\tif (flags & BPF_TRAMP_F_TAIL_CALL_CTX)\n\t\t\t \n\t\t\tRESTORE_TAIL_CALL_CNT(stack_size);\n\n\t\tif (flags & BPF_TRAMP_F_ORIG_STACK) {\n\t\t\temit_ldx(&prog, BPF_DW, BPF_REG_6, BPF_REG_FP, 8);\n\t\t\tEMIT2(0xff, 0xd3);  \n\t\t} else {\n\t\t\t \n\t\t\tif (emit_rsb_call(&prog, orig_call, prog)) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto cleanup;\n\t\t\t}\n\t\t}\n\t\t \n\t\temit_stx(&prog, BPF_DW, BPF_REG_FP, BPF_REG_0, -8);\n\t\tim->ip_after_call = prog;\n\t\tmemcpy(prog, x86_nops[5], X86_PATCH_SIZE);\n\t\tprog += X86_PATCH_SIZE;\n\t}\n\n\tif (fmod_ret->nr_links) {\n\t\t \n\t\temit_align(&prog, 16);\n\t\t \n\t\tfor (i = 0; i < fmod_ret->nr_links; i++)\n\t\t\temit_cond_near_jump(&branches[i], prog, branches[i],\n\t\t\t\t\t    X86_JNE);\n\t}\n\n\tif (fexit->nr_links)\n\t\tif (invoke_bpf(m, &prog, fexit, regs_off, run_ctx_off, false)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\n\tif (flags & BPF_TRAMP_F_RESTORE_REGS)\n\t\trestore_regs(m, &prog, regs_off);\n\n\t \n\tif (flags & BPF_TRAMP_F_CALL_ORIG) {\n\t\tim->ip_epilogue = prog;\n\t\t \n\t\temit_mov_imm64(&prog, BPF_REG_1, (long) im >> 32, (u32) (long) im);\n\t\tif (emit_rsb_call(&prog, __bpf_tramp_exit, prog)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto cleanup;\n\t\t}\n\t} else if (flags & BPF_TRAMP_F_TAIL_CALL_CTX)\n\t\t \n\t\tRESTORE_TAIL_CALL_CNT(stack_size);\n\n\t \n\tif (save_ret)\n\t\temit_ldx(&prog, BPF_DW, BPF_REG_0, BPF_REG_FP, -8);\n\n\temit_ldx(&prog, BPF_DW, BPF_REG_6, BPF_REG_FP, -rbx_off);\n\tEMIT1(0xC9);  \n\tif (flags & BPF_TRAMP_F_SKIP_FRAME)\n\t\t \n\t\tEMIT4(0x48, 0x83, 0xC4, 8);  \n\temit_return(&prog, prog);\n\t \n\tif (WARN_ON_ONCE(prog > (u8 *)image_end - BPF_INSN_SAFETY)) {\n\t\tret = -EFAULT;\n\t\tgoto cleanup;\n\t}\n\tret = prog - (u8 *)image;\n\ncleanup:\n\tkfree(branches);\n\treturn ret;\n}\n\nstatic int emit_bpf_dispatcher(u8 **pprog, int a, int b, s64 *progs, u8 *image, u8 *buf)\n{\n\tu8 *jg_reloc, *prog = *pprog;\n\tint pivot, err, jg_bytes = 1;\n\ts64 jg_offset;\n\n\tif (a == b) {\n\t\t \n\t\tEMIT1(add_1mod(0x48, BPF_REG_3));\t \n\t\tif (!is_simm32(progs[a]))\n\t\t\treturn -1;\n\t\tEMIT2_off32(0x81, add_1reg(0xF8, BPF_REG_3),\n\t\t\t    progs[a]);\n\t\terr = emit_cond_near_jump(&prog,\t \n\t\t\t\t\t  (void *)progs[a], image + (prog - buf),\n\t\t\t\t\t  X86_JE);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\temit_indirect_jump(&prog, 2  , image + (prog - buf));\n\n\t\t*pprog = prog;\n\t\treturn 0;\n\t}\n\n\t \n\tpivot = (b - a) / 2;\n\tEMIT1(add_1mod(0x48, BPF_REG_3));\t\t \n\tif (!is_simm32(progs[a + pivot]))\n\t\treturn -1;\n\tEMIT2_off32(0x81, add_1reg(0xF8, BPF_REG_3), progs[a + pivot]);\n\n\tif (pivot > 2) {\t\t\t\t \n\t\t \n\t\tjg_bytes = 4;\n\t\tEMIT2_off32(0x0F, X86_JG + 0x10, 0);\n\t} else {\n\t\tEMIT2(X86_JG, 0);\n\t}\n\tjg_reloc = prog;\n\n\terr = emit_bpf_dispatcher(&prog, a, a + pivot,\t \n\t\t\t\t  progs, image, buf);\n\tif (err)\n\t\treturn err;\n\n\t \n\temit_align(&prog, 16);\n\tjg_offset = prog - jg_reloc;\n\temit_code(jg_reloc - jg_bytes, jg_offset, jg_bytes);\n\n\terr = emit_bpf_dispatcher(&prog, a + pivot + 1,\t \n\t\t\t\t  b, progs, image, buf);\n\tif (err)\n\t\treturn err;\n\n\t*pprog = prog;\n\treturn 0;\n}\n\nstatic int cmp_ips(const void *a, const void *b)\n{\n\tconst s64 *ipa = a;\n\tconst s64 *ipb = b;\n\n\tif (*ipa > *ipb)\n\t\treturn 1;\n\tif (*ipa < *ipb)\n\t\treturn -1;\n\treturn 0;\n}\n\nint arch_prepare_bpf_dispatcher(void *image, void *buf, s64 *funcs, int num_funcs)\n{\n\tu8 *prog = buf;\n\n\tsort(funcs, num_funcs, sizeof(funcs[0]), cmp_ips, NULL);\n\treturn emit_bpf_dispatcher(&prog, 0, num_funcs - 1, funcs, image, buf);\n}\n\nstruct x64_jit_data {\n\tstruct bpf_binary_header *rw_header;\n\tstruct bpf_binary_header *header;\n\tint *addrs;\n\tu8 *image;\n\tint proglen;\n\tstruct jit_context ctx;\n};\n\n#define MAX_PASSES 20\n#define PADDING_PASSES (MAX_PASSES - 5)\n\nstruct bpf_prog *bpf_int_jit_compile(struct bpf_prog *prog)\n{\n\tstruct bpf_binary_header *rw_header = NULL;\n\tstruct bpf_binary_header *header = NULL;\n\tstruct bpf_prog *tmp, *orig_prog = prog;\n\tstruct x64_jit_data *jit_data;\n\tint proglen, oldproglen = 0;\n\tstruct jit_context ctx = {};\n\tbool tmp_blinded = false;\n\tbool extra_pass = false;\n\tbool padding = false;\n\tu8 *rw_image = NULL;\n\tu8 *image = NULL;\n\tint *addrs;\n\tint pass;\n\tint i;\n\n\tif (!prog->jit_requested)\n\t\treturn orig_prog;\n\n\ttmp = bpf_jit_blind_constants(prog);\n\t \n\tif (IS_ERR(tmp))\n\t\treturn orig_prog;\n\tif (tmp != prog) {\n\t\ttmp_blinded = true;\n\t\tprog = tmp;\n\t}\n\n\tjit_data = prog->aux->jit_data;\n\tif (!jit_data) {\n\t\tjit_data = kzalloc(sizeof(*jit_data), GFP_KERNEL);\n\t\tif (!jit_data) {\n\t\t\tprog = orig_prog;\n\t\t\tgoto out;\n\t\t}\n\t\tprog->aux->jit_data = jit_data;\n\t}\n\taddrs = jit_data->addrs;\n\tif (addrs) {\n\t\tctx = jit_data->ctx;\n\t\toldproglen = jit_data->proglen;\n\t\timage = jit_data->image;\n\t\theader = jit_data->header;\n\t\trw_header = jit_data->rw_header;\n\t\trw_image = (void *)rw_header + ((void *)image - (void *)header);\n\t\textra_pass = true;\n\t\tpadding = true;\n\t\tgoto skip_init_addrs;\n\t}\n\taddrs = kvmalloc_array(prog->len + 1, sizeof(*addrs), GFP_KERNEL);\n\tif (!addrs) {\n\t\tprog = orig_prog;\n\t\tgoto out_addrs;\n\t}\n\n\t \n\tfor (proglen = 0, i = 0; i <= prog->len; i++) {\n\t\tproglen += 64;\n\t\taddrs[i] = proglen;\n\t}\n\tctx.cleanup_addr = proglen;\nskip_init_addrs:\n\n\t \n\tfor (pass = 0; pass < MAX_PASSES || image; pass++) {\n\t\tif (!padding && pass >= PADDING_PASSES)\n\t\t\tpadding = true;\n\t\tproglen = do_jit(prog, addrs, image, rw_image, oldproglen, &ctx, padding);\n\t\tif (proglen <= 0) {\nout_image:\n\t\t\timage = NULL;\n\t\t\tif (header) {\n\t\t\t\tbpf_arch_text_copy(&header->size, &rw_header->size,\n\t\t\t\t\t\t   sizeof(rw_header->size));\n\t\t\t\tbpf_jit_binary_pack_free(header, rw_header);\n\t\t\t}\n\t\t\t \n\t\t\tprog = orig_prog;\n\t\t\tif (extra_pass) {\n\t\t\t\tprog->bpf_func = NULL;\n\t\t\t\tprog->jited = 0;\n\t\t\t\tprog->jited_len = 0;\n\t\t\t}\n\t\t\tgoto out_addrs;\n\t\t}\n\t\tif (image) {\n\t\t\tif (proglen != oldproglen) {\n\t\t\t\tpr_err(\"bpf_jit: proglen=%d != oldproglen=%d\\n\",\n\t\t\t\t       proglen, oldproglen);\n\t\t\t\tgoto out_image;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tif (proglen == oldproglen) {\n\t\t\t \n\t\t\tu32 align = __alignof__(struct exception_table_entry);\n\t\t\tu32 extable_size = prog->aux->num_exentries *\n\t\t\t\tsizeof(struct exception_table_entry);\n\n\t\t\t \n\t\t\theader = bpf_jit_binary_pack_alloc(roundup(proglen, align) + extable_size,\n\t\t\t\t\t\t\t   &image, align, &rw_header, &rw_image,\n\t\t\t\t\t\t\t   jit_fill_hole);\n\t\t\tif (!header) {\n\t\t\t\tprog = orig_prog;\n\t\t\t\tgoto out_addrs;\n\t\t\t}\n\t\t\tprog->aux->extable = (void *) image + roundup(proglen, align);\n\t\t}\n\t\toldproglen = proglen;\n\t\tcond_resched();\n\t}\n\n\tif (bpf_jit_enable > 1)\n\t\tbpf_jit_dump(prog->len, proglen, pass + 1, rw_image);\n\n\tif (image) {\n\t\tif (!prog->is_func || extra_pass) {\n\t\t\t \n\t\t\tif (WARN_ON(bpf_jit_binary_pack_finalize(prog, header, rw_header))) {\n\t\t\t\t \n\t\t\t\theader = NULL;\n\t\t\t\tgoto out_image;\n\t\t\t}\n\n\t\t\tbpf_tail_call_direct_fixup(prog);\n\t\t} else {\n\t\t\tjit_data->addrs = addrs;\n\t\t\tjit_data->ctx = ctx;\n\t\t\tjit_data->proglen = proglen;\n\t\t\tjit_data->image = image;\n\t\t\tjit_data->header = header;\n\t\t\tjit_data->rw_header = rw_header;\n\t\t}\n\t\tprog->bpf_func = (void *)image;\n\t\tprog->jited = 1;\n\t\tprog->jited_len = proglen;\n\t} else {\n\t\tprog = orig_prog;\n\t}\n\n\tif (!image || !prog->is_func || extra_pass) {\n\t\tif (image)\n\t\t\tbpf_prog_fill_jited_linfo(prog, addrs + 1);\nout_addrs:\n\t\tkvfree(addrs);\n\t\tkfree(jit_data);\n\t\tprog->aux->jit_data = NULL;\n\t}\nout:\n\tif (tmp_blinded)\n\t\tbpf_jit_prog_release_other(prog, prog == orig_prog ?\n\t\t\t\t\t   tmp : orig_prog);\n\treturn prog;\n}\n\nbool bpf_jit_supports_kfunc_call(void)\n{\n\treturn true;\n}\n\nvoid *bpf_arch_text_copy(void *dst, void *src, size_t len)\n{\n\tif (text_poke_copy(dst, src, len) == NULL)\n\t\treturn ERR_PTR(-EINVAL);\n\treturn dst;\n}\n\n \nbool bpf_jit_supports_subprog_tailcalls(void)\n{\n\treturn true;\n}\n\nvoid bpf_jit_free(struct bpf_prog *prog)\n{\n\tif (prog->jited) {\n\t\tstruct x64_jit_data *jit_data = prog->aux->jit_data;\n\t\tstruct bpf_binary_header *hdr;\n\n\t\t \n\t\tif (jit_data) {\n\t\t\tbpf_jit_binary_pack_finalize(prog, jit_data->header,\n\t\t\t\t\t\t     jit_data->rw_header);\n\t\t\tkvfree(jit_data->addrs);\n\t\t\tkfree(jit_data);\n\t\t}\n\t\thdr = bpf_jit_binary_pack_hdr(prog);\n\t\tbpf_jit_binary_pack_free(hdr, NULL);\n\t\tWARN_ON_ONCE(!bpf_prog_kallsyms_verify_off(prog));\n\t}\n\n\tbpf_prog_unlock_free(prog);\n}\n\nvoid bpf_arch_poke_desc_update(struct bpf_jit_poke_descriptor *poke,\n\t\t\t       struct bpf_prog *new, struct bpf_prog *old)\n{\n\tu8 *old_addr, *new_addr, *old_bypass_addr;\n\tint ret;\n\n\told_bypass_addr = old ? NULL : poke->bypass_addr;\n\told_addr = old ? (u8 *)old->bpf_func + poke->adj_off : NULL;\n\tnew_addr = new ? (u8 *)new->bpf_func + poke->adj_off : NULL;\n\n\t \n\tif (new) {\n\t\tret = __bpf_arch_text_poke(poke->tailcall_target,\n\t\t\t\t\t   BPF_MOD_JUMP,\n\t\t\t\t\t   old_addr, new_addr);\n\t\tBUG_ON(ret < 0);\n\t\tif (!old) {\n\t\t\tret = __bpf_arch_text_poke(poke->tailcall_bypass,\n\t\t\t\t\t\t   BPF_MOD_JUMP,\n\t\t\t\t\t\t   poke->bypass_addr,\n\t\t\t\t\t\t   NULL);\n\t\t\tBUG_ON(ret < 0);\n\t\t}\n\t} else {\n\t\tret = __bpf_arch_text_poke(poke->tailcall_bypass,\n\t\t\t\t\t   BPF_MOD_JUMP,\n\t\t\t\t\t   old_bypass_addr,\n\t\t\t\t\t   poke->bypass_addr);\n\t\tBUG_ON(ret < 0);\n\t\t \n\t\tif (!ret)\n\t\t\tsynchronize_rcu();\n\t\tret = __bpf_arch_text_poke(poke->tailcall_target,\n\t\t\t\t\t   BPF_MOD_JUMP,\n\t\t\t\t\t   old_addr, NULL);\n\t\tBUG_ON(ret < 0);\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}