{
  "module_name": "chacha-avx2-x86_64.S",
  "hash_id": "7eac0033730a2297994818e9222666208612306f0920e49f2ca3b5d418409782",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/crypto/chacha-avx2-x86_64.S",
  "human_readable_source": " \n \n\n#include <linux/linkage.h>\n\n.section\t.rodata.cst32.ROT8, \"aM\", @progbits, 32\n.align 32\nROT8:\t.octa 0x0e0d0c0f0a09080b0605040702010003\n\t.octa 0x0e0d0c0f0a09080b0605040702010003\n\n.section\t.rodata.cst32.ROT16, \"aM\", @progbits, 32\n.align 32\nROT16:\t.octa 0x0d0c0f0e09080b0a0504070601000302\n\t.octa 0x0d0c0f0e09080b0a0504070601000302\n\n.section\t.rodata.cst32.CTRINC, \"aM\", @progbits, 32\n.align 32\nCTRINC:\t.octa 0x00000003000000020000000100000000\n\t.octa 0x00000007000000060000000500000004\n\n.section\t.rodata.cst32.CTR2BL, \"aM\", @progbits, 32\n.align 32\nCTR2BL:\t.octa 0x00000000000000000000000000000000\n\t.octa 0x00000000000000000000000000000001\n\n.section\t.rodata.cst32.CTR4BL, \"aM\", @progbits, 32\n.align 32\nCTR4BL:\t.octa 0x00000000000000000000000000000002\n\t.octa 0x00000000000000000000000000000003\n\n.text\n\nSYM_FUNC_START(chacha_2block_xor_avx2)\n\t# %rdi: Input state matrix, s\n\t# %rsi: up to 2 data blocks output, o\n\t# %rdx: up to 2 data blocks input, i\n\t# %rcx: input/output length in bytes\n\t# %r8d: nrounds\n\n\t# This function encrypts two ChaCha blocks by loading the state\n\t# matrix twice across four AVX registers. It performs matrix operations\n\t# on four words in each matrix in parallel, but requires shuffling to\n\t# rearrange the words after each round.\n\n\tvzeroupper\n\n\t# x0..3[0-2] = s0..3\n\tvbroadcasti128\t0x00(%rdi),%ymm0\n\tvbroadcasti128\t0x10(%rdi),%ymm1\n\tvbroadcasti128\t0x20(%rdi),%ymm2\n\tvbroadcasti128\t0x30(%rdi),%ymm3\n\n\tvpaddd\t\tCTR2BL(%rip),%ymm3,%ymm3\n\n\tvmovdqa\t\t%ymm0,%ymm8\n\tvmovdqa\t\t%ymm1,%ymm9\n\tvmovdqa\t\t%ymm2,%ymm10\n\tvmovdqa\t\t%ymm3,%ymm11\n\n\tvmovdqa\t\tROT8(%rip),%ymm4\n\tvmovdqa\t\tROT16(%rip),%ymm5\n\n\tmov\t\t%rcx,%rax\n\n.Ldoubleround:\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 16)\n\tvpaddd\t\t%ymm1,%ymm0,%ymm0\n\tvpxor\t\t%ymm0,%ymm3,%ymm3\n\tvpshufb\t\t%ymm5,%ymm3,%ymm3\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 12)\n\tvpaddd\t\t%ymm3,%ymm2,%ymm2\n\tvpxor\t\t%ymm2,%ymm1,%ymm1\n\tvmovdqa\t\t%ymm1,%ymm6\n\tvpslld\t\t$12,%ymm6,%ymm6\n\tvpsrld\t\t$20,%ymm1,%ymm1\n\tvpor\t\t%ymm6,%ymm1,%ymm1\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 8)\n\tvpaddd\t\t%ymm1,%ymm0,%ymm0\n\tvpxor\t\t%ymm0,%ymm3,%ymm3\n\tvpshufb\t\t%ymm4,%ymm3,%ymm3\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 7)\n\tvpaddd\t\t%ymm3,%ymm2,%ymm2\n\tvpxor\t\t%ymm2,%ymm1,%ymm1\n\tvmovdqa\t\t%ymm1,%ymm7\n\tvpslld\t\t$7,%ymm7,%ymm7\n\tvpsrld\t\t$25,%ymm1,%ymm1\n\tvpor\t\t%ymm7,%ymm1,%ymm1\n\n\t# x1 = shuffle32(x1, MASK(0, 3, 2, 1))\n\tvpshufd\t\t$0x39,%ymm1,%ymm1\n\t# x2 = shuffle32(x2, MASK(1, 0, 3, 2))\n\tvpshufd\t\t$0x4e,%ymm2,%ymm2\n\t# x3 = shuffle32(x3, MASK(2, 1, 0, 3))\n\tvpshufd\t\t$0x93,%ymm3,%ymm3\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 16)\n\tvpaddd\t\t%ymm1,%ymm0,%ymm0\n\tvpxor\t\t%ymm0,%ymm3,%ymm3\n\tvpshufb\t\t%ymm5,%ymm3,%ymm3\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 12)\n\tvpaddd\t\t%ymm3,%ymm2,%ymm2\n\tvpxor\t\t%ymm2,%ymm1,%ymm1\n\tvmovdqa\t\t%ymm1,%ymm6\n\tvpslld\t\t$12,%ymm6,%ymm6\n\tvpsrld\t\t$20,%ymm1,%ymm1\n\tvpor\t\t%ymm6,%ymm1,%ymm1\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 8)\n\tvpaddd\t\t%ymm1,%ymm0,%ymm0\n\tvpxor\t\t%ymm0,%ymm3,%ymm3\n\tvpshufb\t\t%ymm4,%ymm3,%ymm3\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 7)\n\tvpaddd\t\t%ymm3,%ymm2,%ymm2\n\tvpxor\t\t%ymm2,%ymm1,%ymm1\n\tvmovdqa\t\t%ymm1,%ymm7\n\tvpslld\t\t$7,%ymm7,%ymm7\n\tvpsrld\t\t$25,%ymm1,%ymm1\n\tvpor\t\t%ymm7,%ymm1,%ymm1\n\n\t# x1 = shuffle32(x1, MASK(2, 1, 0, 3))\n\tvpshufd\t\t$0x93,%ymm1,%ymm1\n\t# x2 = shuffle32(x2, MASK(1, 0, 3, 2))\n\tvpshufd\t\t$0x4e,%ymm2,%ymm2\n\t# x3 = shuffle32(x3, MASK(0, 3, 2, 1))\n\tvpshufd\t\t$0x39,%ymm3,%ymm3\n\n\tsub\t\t$2,%r8d\n\tjnz\t\t.Ldoubleround\n\n\t# o0 = i0 ^ (x0 + s0)\n\tvpaddd\t\t%ymm8,%ymm0,%ymm7\n\tcmp\t\t$0x10,%rax\n\tjl\t\t.Lxorpart2\n\tvpxor\t\t0x00(%rdx),%xmm7,%xmm6\n\tvmovdqu\t\t%xmm6,0x00(%rsi)\n\tvextracti128\t$1,%ymm7,%xmm0\n\t# o1 = i1 ^ (x1 + s1)\n\tvpaddd\t\t%ymm9,%ymm1,%ymm7\n\tcmp\t\t$0x20,%rax\n\tjl\t\t.Lxorpart2\n\tvpxor\t\t0x10(%rdx),%xmm7,%xmm6\n\tvmovdqu\t\t%xmm6,0x10(%rsi)\n\tvextracti128\t$1,%ymm7,%xmm1\n\t# o2 = i2 ^ (x2 + s2)\n\tvpaddd\t\t%ymm10,%ymm2,%ymm7\n\tcmp\t\t$0x30,%rax\n\tjl\t\t.Lxorpart2\n\tvpxor\t\t0x20(%rdx),%xmm7,%xmm6\n\tvmovdqu\t\t%xmm6,0x20(%rsi)\n\tvextracti128\t$1,%ymm7,%xmm2\n\t# o3 = i3 ^ (x3 + s3)\n\tvpaddd\t\t%ymm11,%ymm3,%ymm7\n\tcmp\t\t$0x40,%rax\n\tjl\t\t.Lxorpart2\n\tvpxor\t\t0x30(%rdx),%xmm7,%xmm6\n\tvmovdqu\t\t%xmm6,0x30(%rsi)\n\tvextracti128\t$1,%ymm7,%xmm3\n\n\t# xor and write second block\n\tvmovdqa\t\t%xmm0,%xmm7\n\tcmp\t\t$0x50,%rax\n\tjl\t\t.Lxorpart2\n\tvpxor\t\t0x40(%rdx),%xmm7,%xmm6\n\tvmovdqu\t\t%xmm6,0x40(%rsi)\n\n\tvmovdqa\t\t%xmm1,%xmm7\n\tcmp\t\t$0x60,%rax\n\tjl\t\t.Lxorpart2\n\tvpxor\t\t0x50(%rdx),%xmm7,%xmm6\n\tvmovdqu\t\t%xmm6,0x50(%rsi)\n\n\tvmovdqa\t\t%xmm2,%xmm7\n\tcmp\t\t$0x70,%rax\n\tjl\t\t.Lxorpart2\n\tvpxor\t\t0x60(%rdx),%xmm7,%xmm6\n\tvmovdqu\t\t%xmm6,0x60(%rsi)\n\n\tvmovdqa\t\t%xmm3,%xmm7\n\tcmp\t\t$0x80,%rax\n\tjl\t\t.Lxorpart2\n\tvpxor\t\t0x70(%rdx),%xmm7,%xmm6\n\tvmovdqu\t\t%xmm6,0x70(%rsi)\n\n.Ldone2:\n\tvzeroupper\n\tRET\n\n.Lxorpart2:\n\t# xor remaining bytes from partial register into output\n\tmov\t\t%rax,%r9\n\tand\t\t$0x0f,%r9\n\tjz\t\t.Ldone2\n\tand\t\t$~0x0f,%rax\n\n\tmov\t\t%rsi,%r11\n\n\tlea\t\t8(%rsp),%r10\n\tsub\t\t$0x10,%rsp\n\tand\t\t$~31,%rsp\n\n\tlea\t\t(%rdx,%rax),%rsi\n\tmov\t\t%rsp,%rdi\n\tmov\t\t%r9,%rcx\n\trep movsb\n\n\tvpxor\t\t0x00(%rsp),%xmm7,%xmm7\n\tvmovdqa\t\t%xmm7,0x00(%rsp)\n\n\tmov\t\t%rsp,%rsi\n\tlea\t\t(%r11,%rax),%rdi\n\tmov\t\t%r9,%rcx\n\trep movsb\n\n\tlea\t\t-8(%r10),%rsp\n\tjmp\t\t.Ldone2\n\nSYM_FUNC_END(chacha_2block_xor_avx2)\n\nSYM_FUNC_START(chacha_4block_xor_avx2)\n\t# %rdi: Input state matrix, s\n\t# %rsi: up to 4 data blocks output, o\n\t# %rdx: up to 4 data blocks input, i\n\t# %rcx: input/output length in bytes\n\t# %r8d: nrounds\n\n\t# This function encrypts four ChaCha blocks by loading the state\n\t# matrix four times across eight AVX registers. It performs matrix\n\t# operations on four words in two matrices in parallel, sequentially\n\t# to the operations on the four words of the other two matrices. The\n\t# required word shuffling has a rather high latency, we can do the\n\t# arithmetic on two matrix-pairs without much slowdown.\n\n\tvzeroupper\n\n\t# x0..3[0-4] = s0..3\n\tvbroadcasti128\t0x00(%rdi),%ymm0\n\tvbroadcasti128\t0x10(%rdi),%ymm1\n\tvbroadcasti128\t0x20(%rdi),%ymm2\n\tvbroadcasti128\t0x30(%rdi),%ymm3\n\n\tvmovdqa\t\t%ymm0,%ymm4\n\tvmovdqa\t\t%ymm1,%ymm5\n\tvmovdqa\t\t%ymm2,%ymm6\n\tvmovdqa\t\t%ymm3,%ymm7\n\n\tvpaddd\t\tCTR2BL(%rip),%ymm3,%ymm3\n\tvpaddd\t\tCTR4BL(%rip),%ymm7,%ymm7\n\n\tvmovdqa\t\t%ymm0,%ymm11\n\tvmovdqa\t\t%ymm1,%ymm12\n\tvmovdqa\t\t%ymm2,%ymm13\n\tvmovdqa\t\t%ymm3,%ymm14\n\tvmovdqa\t\t%ymm7,%ymm15\n\n\tvmovdqa\t\tROT8(%rip),%ymm8\n\tvmovdqa\t\tROT16(%rip),%ymm9\n\n\tmov\t\t%rcx,%rax\n\n.Ldoubleround4:\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 16)\n\tvpaddd\t\t%ymm1,%ymm0,%ymm0\n\tvpxor\t\t%ymm0,%ymm3,%ymm3\n\tvpshufb\t\t%ymm9,%ymm3,%ymm3\n\n\tvpaddd\t\t%ymm5,%ymm4,%ymm4\n\tvpxor\t\t%ymm4,%ymm7,%ymm7\n\tvpshufb\t\t%ymm9,%ymm7,%ymm7\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 12)\n\tvpaddd\t\t%ymm3,%ymm2,%ymm2\n\tvpxor\t\t%ymm2,%ymm1,%ymm1\n\tvmovdqa\t\t%ymm1,%ymm10\n\tvpslld\t\t$12,%ymm10,%ymm10\n\tvpsrld\t\t$20,%ymm1,%ymm1\n\tvpor\t\t%ymm10,%ymm1,%ymm1\n\n\tvpaddd\t\t%ymm7,%ymm6,%ymm6\n\tvpxor\t\t%ymm6,%ymm5,%ymm5\n\tvmovdqa\t\t%ymm5,%ymm10\n\tvpslld\t\t$12,%ymm10,%ymm10\n\tvpsrld\t\t$20,%ymm5,%ymm5\n\tvpor\t\t%ymm10,%ymm5,%ymm5\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 8)\n\tvpaddd\t\t%ymm1,%ymm0,%ymm0\n\tvpxor\t\t%ymm0,%ymm3,%ymm3\n\tvpshufb\t\t%ymm8,%ymm3,%ymm3\n\n\tvpaddd\t\t%ymm5,%ymm4,%ymm4\n\tvpxor\t\t%ymm4,%ymm7,%ymm7\n\tvpshufb\t\t%ymm8,%ymm7,%ymm7\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 7)\n\tvpaddd\t\t%ymm3,%ymm2,%ymm2\n\tvpxor\t\t%ymm2,%ymm1,%ymm1\n\tvmovdqa\t\t%ymm1,%ymm10\n\tvpslld\t\t$7,%ymm10,%ymm10\n\tvpsrld\t\t$25,%ymm1,%ymm1\n\tvpor\t\t%ymm10,%ymm1,%ymm1\n\n\tvpaddd\t\t%ymm7,%ymm6,%ymm6\n\tvpxor\t\t%ymm6,%ymm5,%ymm5\n\tvmovdqa\t\t%ymm5,%ymm10\n\tvpslld\t\t$7,%ymm10,%ymm10\n\tvpsrld\t\t$25,%ymm5,%ymm5\n\tvpor\t\t%ymm10,%ymm5,%ymm5\n\n\t# x1 = shuffle32(x1, MASK(0, 3, 2, 1))\n\tvpshufd\t\t$0x39,%ymm1,%ymm1\n\tvpshufd\t\t$0x39,%ymm5,%ymm5\n\t# x2 = shuffle32(x2, MASK(1, 0, 3, 2))\n\tvpshufd\t\t$0x4e,%ymm2,%ymm2\n\tvpshufd\t\t$0x4e,%ymm6,%ymm6\n\t# x3 = shuffle32(x3, MASK(2, 1, 0, 3))\n\tvpshufd\t\t$0x93,%ymm3,%ymm3\n\tvpshufd\t\t$0x93,%ymm7,%ymm7\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 16)\n\tvpaddd\t\t%ymm1,%ymm0,%ymm0\n\tvpxor\t\t%ymm0,%ymm3,%ymm3\n\tvpshufb\t\t%ymm9,%ymm3,%ymm3\n\n\tvpaddd\t\t%ymm5,%ymm4,%ymm4\n\tvpxor\t\t%ymm4,%ymm7,%ymm7\n\tvpshufb\t\t%ymm9,%ymm7,%ymm7\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 12)\n\tvpaddd\t\t%ymm3,%ymm2,%ymm2\n\tvpxor\t\t%ymm2,%ymm1,%ymm1\n\tvmovdqa\t\t%ymm1,%ymm10\n\tvpslld\t\t$12,%ymm10,%ymm10\n\tvpsrld\t\t$20,%ymm1,%ymm1\n\tvpor\t\t%ymm10,%ymm1,%ymm1\n\n\tvpaddd\t\t%ymm7,%ymm6,%ymm6\n\tvpxor\t\t%ymm6,%ymm5,%ymm5\n\tvmovdqa\t\t%ymm5,%ymm10\n\tvpslld\t\t$12,%ymm10,%ymm10\n\tvpsrld\t\t$20,%ymm5,%ymm5\n\tvpor\t\t%ymm10,%ymm5,%ymm5\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 8)\n\tvpaddd\t\t%ymm1,%ymm0,%ymm0\n\tvpxor\t\t%ymm0,%ymm3,%ymm3\n\tvpshufb\t\t%ymm8,%ymm3,%ymm3\n\n\tvpaddd\t\t%ymm5,%ymm4,%ymm4\n\tvpxor\t\t%ymm4,%ymm7,%ymm7\n\tvpshufb\t\t%ymm8,%ymm7,%ymm7\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 7)\n\tvpaddd\t\t%ymm3,%ymm2,%ymm2\n\tvpxor\t\t%ymm2,%ymm1,%ymm1\n\tvmovdqa\t\t%ymm1,%ymm10\n\tvpslld\t\t$7,%ymm10,%ymm10\n\tvpsrld\t\t$25,%ymm1,%ymm1\n\tvpor\t\t%ymm10,%ymm1,%ymm1\n\n\tvpaddd\t\t%ymm7,%ymm6,%ymm6\n\tvpxor\t\t%ymm6,%ymm5,%ymm5\n\tvmovdqa\t\t%ymm5,%ymm10\n\tvpslld\t\t$7,%ymm10,%ymm10\n\tvpsrld\t\t$25,%ymm5,%ymm5\n\tvpor\t\t%ymm10,%ymm5,%ymm5\n\n\t# x1 = shuffle32(x1, MASK(2, 1, 0, 3))\n\tvpshufd\t\t$0x93,%ymm1,%ymm1\n\tvpshufd\t\t$0x93,%ymm5,%ymm5\n\t# x2 = shuffle32(x2, MASK(1, 0, 3, 2))\n\tvpshufd\t\t$0x4e,%ymm2,%ymm2\n\tvpshufd\t\t$0x4e,%ymm6,%ymm6\n\t# x3 = shuffle32(x3, MASK(0, 3, 2, 1))\n\tvpshufd\t\t$0x39,%ymm3,%ymm3\n\tvpshufd\t\t$0x39,%ymm7,%ymm7\n\n\tsub\t\t$2,%r8d\n\tjnz\t\t.Ldoubleround4\n\n\t# o0 = i0 ^ (x0 + s0), first block\n\tvpaddd\t\t%ymm11,%ymm0,%ymm10\n\tcmp\t\t$0x10,%rax\n\tjl\t\t.Lxorpart4\n\tvpxor\t\t0x00(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x00(%rsi)\n\tvextracti128\t$1,%ymm10,%xmm0\n\t# o1 = i1 ^ (x1 + s1), first block\n\tvpaddd\t\t%ymm12,%ymm1,%ymm10\n\tcmp\t\t$0x20,%rax\n\tjl\t\t.Lxorpart4\n\tvpxor\t\t0x10(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x10(%rsi)\n\tvextracti128\t$1,%ymm10,%xmm1\n\t# o2 = i2 ^ (x2 + s2), first block\n\tvpaddd\t\t%ymm13,%ymm2,%ymm10\n\tcmp\t\t$0x30,%rax\n\tjl\t\t.Lxorpart4\n\tvpxor\t\t0x20(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x20(%rsi)\n\tvextracti128\t$1,%ymm10,%xmm2\n\t# o3 = i3 ^ (x3 + s3), first block\n\tvpaddd\t\t%ymm14,%ymm3,%ymm10\n\tcmp\t\t$0x40,%rax\n\tjl\t\t.Lxorpart4\n\tvpxor\t\t0x30(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x30(%rsi)\n\tvextracti128\t$1,%ymm10,%xmm3\n\n\t# xor and write second block\n\tvmovdqa\t\t%xmm0,%xmm10\n\tcmp\t\t$0x50,%rax\n\tjl\t\t.Lxorpart4\n\tvpxor\t\t0x40(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x40(%rsi)\n\n\tvmovdqa\t\t%xmm1,%xmm10\n\tcmp\t\t$0x60,%rax\n\tjl\t\t.Lxorpart4\n\tvpxor\t\t0x50(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x50(%rsi)\n\n\tvmovdqa\t\t%xmm2,%xmm10\n\tcmp\t\t$0x70,%rax\n\tjl\t\t.Lxorpart4\n\tvpxor\t\t0x60(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x60(%rsi)\n\n\tvmovdqa\t\t%xmm3,%xmm10\n\tcmp\t\t$0x80,%rax\n\tjl\t\t.Lxorpart4\n\tvpxor\t\t0x70(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x70(%rsi)\n\n\t# o0 = i0 ^ (x0 + s0), third block\n\tvpaddd\t\t%ymm11,%ymm4,%ymm10\n\tcmp\t\t$0x90,%rax\n\tjl\t\t.Lxorpart4\n\tvpxor\t\t0x80(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x80(%rsi)\n\tvextracti128\t$1,%ymm10,%xmm4\n\t# o1 = i1 ^ (x1 + s1), third block\n\tvpaddd\t\t%ymm12,%ymm5,%ymm10\n\tcmp\t\t$0xa0,%rax\n\tjl\t\t.Lxorpart4\n\tvpxor\t\t0x90(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x90(%rsi)\n\tvextracti128\t$1,%ymm10,%xmm5\n\t# o2 = i2 ^ (x2 + s2), third block\n\tvpaddd\t\t%ymm13,%ymm6,%ymm10\n\tcmp\t\t$0xb0,%rax\n\tjl\t\t.Lxorpart4\n\tvpxor\t\t0xa0(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0xa0(%rsi)\n\tvextracti128\t$1,%ymm10,%xmm6\n\t# o3 = i3 ^ (x3 + s3), third block\n\tvpaddd\t\t%ymm15,%ymm7,%ymm10\n\tcmp\t\t$0xc0,%rax\n\tjl\t\t.Lxorpart4\n\tvpxor\t\t0xb0(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0xb0(%rsi)\n\tvextracti128\t$1,%ymm10,%xmm7\n\n\t# xor and write fourth block\n\tvmovdqa\t\t%xmm4,%xmm10\n\tcmp\t\t$0xd0,%rax\n\tjl\t\t.Lxorpart4\n\tvpxor\t\t0xc0(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0xc0(%rsi)\n\n\tvmovdqa\t\t%xmm5,%xmm10\n\tcmp\t\t$0xe0,%rax\n\tjl\t\t.Lxorpart4\n\tvpxor\t\t0xd0(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0xd0(%rsi)\n\n\tvmovdqa\t\t%xmm6,%xmm10\n\tcmp\t\t$0xf0,%rax\n\tjl\t\t.Lxorpart4\n\tvpxor\t\t0xe0(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0xe0(%rsi)\n\n\tvmovdqa\t\t%xmm7,%xmm10\n\tcmp\t\t$0x100,%rax\n\tjl\t\t.Lxorpart4\n\tvpxor\t\t0xf0(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0xf0(%rsi)\n\n.Ldone4:\n\tvzeroupper\n\tRET\n\n.Lxorpart4:\n\t# xor remaining bytes from partial register into output\n\tmov\t\t%rax,%r9\n\tand\t\t$0x0f,%r9\n\tjz\t\t.Ldone4\n\tand\t\t$~0x0f,%rax\n\n\tmov\t\t%rsi,%r11\n\n\tlea\t\t8(%rsp),%r10\n\tsub\t\t$0x10,%rsp\n\tand\t\t$~31,%rsp\n\n\tlea\t\t(%rdx,%rax),%rsi\n\tmov\t\t%rsp,%rdi\n\tmov\t\t%r9,%rcx\n\trep movsb\n\n\tvpxor\t\t0x00(%rsp),%xmm10,%xmm10\n\tvmovdqa\t\t%xmm10,0x00(%rsp)\n\n\tmov\t\t%rsp,%rsi\n\tlea\t\t(%r11,%rax),%rdi\n\tmov\t\t%r9,%rcx\n\trep movsb\n\n\tlea\t\t-8(%r10),%rsp\n\tjmp\t\t.Ldone4\n\nSYM_FUNC_END(chacha_4block_xor_avx2)\n\nSYM_FUNC_START(chacha_8block_xor_avx2)\n\t# %rdi: Input state matrix, s\n\t# %rsi: up to 8 data blocks output, o\n\t# %rdx: up to 8 data blocks input, i\n\t# %rcx: input/output length in bytes\n\t# %r8d: nrounds\n\n\t# This function encrypts eight consecutive ChaCha blocks by loading\n\t# the state matrix in AVX registers eight times. As we need some\n\t# scratch registers, we save the first four registers on the stack. The\n\t# algorithm performs each operation on the corresponding word of each\n\t# state matrix, hence requires no word shuffling. For final XORing step\n\t# we transpose the matrix by interleaving 32-, 64- and then 128-bit\n\t# words, which allows us to do XOR in AVX registers. 8/16-bit word\n\t# rotation is done with the slightly better performing byte shuffling,\n\t# 7/12-bit word rotation uses traditional shift+OR.\n\n\tvzeroupper\n\t# 4 * 32 byte stack, 32-byte aligned\n\tlea\t\t8(%rsp),%r10\n\tand\t\t$~31, %rsp\n\tsub\t\t$0x80, %rsp\n\tmov\t\t%rcx,%rax\n\n\t# x0..15[0-7] = s[0..15]\n\tvpbroadcastd\t0x00(%rdi),%ymm0\n\tvpbroadcastd\t0x04(%rdi),%ymm1\n\tvpbroadcastd\t0x08(%rdi),%ymm2\n\tvpbroadcastd\t0x0c(%rdi),%ymm3\n\tvpbroadcastd\t0x10(%rdi),%ymm4\n\tvpbroadcastd\t0x14(%rdi),%ymm5\n\tvpbroadcastd\t0x18(%rdi),%ymm6\n\tvpbroadcastd\t0x1c(%rdi),%ymm7\n\tvpbroadcastd\t0x20(%rdi),%ymm8\n\tvpbroadcastd\t0x24(%rdi),%ymm9\n\tvpbroadcastd\t0x28(%rdi),%ymm10\n\tvpbroadcastd\t0x2c(%rdi),%ymm11\n\tvpbroadcastd\t0x30(%rdi),%ymm12\n\tvpbroadcastd\t0x34(%rdi),%ymm13\n\tvpbroadcastd\t0x38(%rdi),%ymm14\n\tvpbroadcastd\t0x3c(%rdi),%ymm15\n\t# x0..3 on stack\n\tvmovdqa\t\t%ymm0,0x00(%rsp)\n\tvmovdqa\t\t%ymm1,0x20(%rsp)\n\tvmovdqa\t\t%ymm2,0x40(%rsp)\n\tvmovdqa\t\t%ymm3,0x60(%rsp)\n\n\tvmovdqa\t\tCTRINC(%rip),%ymm1\n\tvmovdqa\t\tROT8(%rip),%ymm2\n\tvmovdqa\t\tROT16(%rip),%ymm3\n\n\t# x12 += counter values 0-3\n\tvpaddd\t\t%ymm1,%ymm12,%ymm12\n\n.Ldoubleround8:\n\t# x0 += x4, x12 = rotl32(x12 ^ x0, 16)\n\tvpaddd\t\t0x00(%rsp),%ymm4,%ymm0\n\tvmovdqa\t\t%ymm0,0x00(%rsp)\n\tvpxor\t\t%ymm0,%ymm12,%ymm12\n\tvpshufb\t\t%ymm3,%ymm12,%ymm12\n\t# x1 += x5, x13 = rotl32(x13 ^ x1, 16)\n\tvpaddd\t\t0x20(%rsp),%ymm5,%ymm0\n\tvmovdqa\t\t%ymm0,0x20(%rsp)\n\tvpxor\t\t%ymm0,%ymm13,%ymm13\n\tvpshufb\t\t%ymm3,%ymm13,%ymm13\n\t# x2 += x6, x14 = rotl32(x14 ^ x2, 16)\n\tvpaddd\t\t0x40(%rsp),%ymm6,%ymm0\n\tvmovdqa\t\t%ymm0,0x40(%rsp)\n\tvpxor\t\t%ymm0,%ymm14,%ymm14\n\tvpshufb\t\t%ymm3,%ymm14,%ymm14\n\t# x3 += x7, x15 = rotl32(x15 ^ x3, 16)\n\tvpaddd\t\t0x60(%rsp),%ymm7,%ymm0\n\tvmovdqa\t\t%ymm0,0x60(%rsp)\n\tvpxor\t\t%ymm0,%ymm15,%ymm15\n\tvpshufb\t\t%ymm3,%ymm15,%ymm15\n\n\t# x8 += x12, x4 = rotl32(x4 ^ x8, 12)\n\tvpaddd\t\t%ymm12,%ymm8,%ymm8\n\tvpxor\t\t%ymm8,%ymm4,%ymm4\n\tvpslld\t\t$12,%ymm4,%ymm0\n\tvpsrld\t\t$20,%ymm4,%ymm4\n\tvpor\t\t%ymm0,%ymm4,%ymm4\n\t# x9 += x13, x5 = rotl32(x5 ^ x9, 12)\n\tvpaddd\t\t%ymm13,%ymm9,%ymm9\n\tvpxor\t\t%ymm9,%ymm5,%ymm5\n\tvpslld\t\t$12,%ymm5,%ymm0\n\tvpsrld\t\t$20,%ymm5,%ymm5\n\tvpor\t\t%ymm0,%ymm5,%ymm5\n\t# x10 += x14, x6 = rotl32(x6 ^ x10, 12)\n\tvpaddd\t\t%ymm14,%ymm10,%ymm10\n\tvpxor\t\t%ymm10,%ymm6,%ymm6\n\tvpslld\t\t$12,%ymm6,%ymm0\n\tvpsrld\t\t$20,%ymm6,%ymm6\n\tvpor\t\t%ymm0,%ymm6,%ymm6\n\t# x11 += x15, x7 = rotl32(x7 ^ x11, 12)\n\tvpaddd\t\t%ymm15,%ymm11,%ymm11\n\tvpxor\t\t%ymm11,%ymm7,%ymm7\n\tvpslld\t\t$12,%ymm7,%ymm0\n\tvpsrld\t\t$20,%ymm7,%ymm7\n\tvpor\t\t%ymm0,%ymm7,%ymm7\n\n\t# x0 += x4, x12 = rotl32(x12 ^ x0, 8)\n\tvpaddd\t\t0x00(%rsp),%ymm4,%ymm0\n\tvmovdqa\t\t%ymm0,0x00(%rsp)\n\tvpxor\t\t%ymm0,%ymm12,%ymm12\n\tvpshufb\t\t%ymm2,%ymm12,%ymm12\n\t# x1 += x5, x13 = rotl32(x13 ^ x1, 8)\n\tvpaddd\t\t0x20(%rsp),%ymm5,%ymm0\n\tvmovdqa\t\t%ymm0,0x20(%rsp)\n\tvpxor\t\t%ymm0,%ymm13,%ymm13\n\tvpshufb\t\t%ymm2,%ymm13,%ymm13\n\t# x2 += x6, x14 = rotl32(x14 ^ x2, 8)\n\tvpaddd\t\t0x40(%rsp),%ymm6,%ymm0\n\tvmovdqa\t\t%ymm0,0x40(%rsp)\n\tvpxor\t\t%ymm0,%ymm14,%ymm14\n\tvpshufb\t\t%ymm2,%ymm14,%ymm14\n\t# x3 += x7, x15 = rotl32(x15 ^ x3, 8)\n\tvpaddd\t\t0x60(%rsp),%ymm7,%ymm0\n\tvmovdqa\t\t%ymm0,0x60(%rsp)\n\tvpxor\t\t%ymm0,%ymm15,%ymm15\n\tvpshufb\t\t%ymm2,%ymm15,%ymm15\n\n\t# x8 += x12, x4 = rotl32(x4 ^ x8, 7)\n\tvpaddd\t\t%ymm12,%ymm8,%ymm8\n\tvpxor\t\t%ymm8,%ymm4,%ymm4\n\tvpslld\t\t$7,%ymm4,%ymm0\n\tvpsrld\t\t$25,%ymm4,%ymm4\n\tvpor\t\t%ymm0,%ymm4,%ymm4\n\t# x9 += x13, x5 = rotl32(x5 ^ x9, 7)\n\tvpaddd\t\t%ymm13,%ymm9,%ymm9\n\tvpxor\t\t%ymm9,%ymm5,%ymm5\n\tvpslld\t\t$7,%ymm5,%ymm0\n\tvpsrld\t\t$25,%ymm5,%ymm5\n\tvpor\t\t%ymm0,%ymm5,%ymm5\n\t# x10 += x14, x6 = rotl32(x6 ^ x10, 7)\n\tvpaddd\t\t%ymm14,%ymm10,%ymm10\n\tvpxor\t\t%ymm10,%ymm6,%ymm6\n\tvpslld\t\t$7,%ymm6,%ymm0\n\tvpsrld\t\t$25,%ymm6,%ymm6\n\tvpor\t\t%ymm0,%ymm6,%ymm6\n\t# x11 += x15, x7 = rotl32(x7 ^ x11, 7)\n\tvpaddd\t\t%ymm15,%ymm11,%ymm11\n\tvpxor\t\t%ymm11,%ymm7,%ymm7\n\tvpslld\t\t$7,%ymm7,%ymm0\n\tvpsrld\t\t$25,%ymm7,%ymm7\n\tvpor\t\t%ymm0,%ymm7,%ymm7\n\n\t# x0 += x5, x15 = rotl32(x15 ^ x0, 16)\n\tvpaddd\t\t0x00(%rsp),%ymm5,%ymm0\n\tvmovdqa\t\t%ymm0,0x00(%rsp)\n\tvpxor\t\t%ymm0,%ymm15,%ymm15\n\tvpshufb\t\t%ymm3,%ymm15,%ymm15\n\t# x1 += x6, x12 = rotl32(x12 ^ x1, 16)%ymm0\n\tvpaddd\t\t0x20(%rsp),%ymm6,%ymm0\n\tvmovdqa\t\t%ymm0,0x20(%rsp)\n\tvpxor\t\t%ymm0,%ymm12,%ymm12\n\tvpshufb\t\t%ymm3,%ymm12,%ymm12\n\t# x2 += x7, x13 = rotl32(x13 ^ x2, 16)\n\tvpaddd\t\t0x40(%rsp),%ymm7,%ymm0\n\tvmovdqa\t\t%ymm0,0x40(%rsp)\n\tvpxor\t\t%ymm0,%ymm13,%ymm13\n\tvpshufb\t\t%ymm3,%ymm13,%ymm13\n\t# x3 += x4, x14 = rotl32(x14 ^ x3, 16)\n\tvpaddd\t\t0x60(%rsp),%ymm4,%ymm0\n\tvmovdqa\t\t%ymm0,0x60(%rsp)\n\tvpxor\t\t%ymm0,%ymm14,%ymm14\n\tvpshufb\t\t%ymm3,%ymm14,%ymm14\n\n\t# x10 += x15, x5 = rotl32(x5 ^ x10, 12)\n\tvpaddd\t\t%ymm15,%ymm10,%ymm10\n\tvpxor\t\t%ymm10,%ymm5,%ymm5\n\tvpslld\t\t$12,%ymm5,%ymm0\n\tvpsrld\t\t$20,%ymm5,%ymm5\n\tvpor\t\t%ymm0,%ymm5,%ymm5\n\t# x11 += x12, x6 = rotl32(x6 ^ x11, 12)\n\tvpaddd\t\t%ymm12,%ymm11,%ymm11\n\tvpxor\t\t%ymm11,%ymm6,%ymm6\n\tvpslld\t\t$12,%ymm6,%ymm0\n\tvpsrld\t\t$20,%ymm6,%ymm6\n\tvpor\t\t%ymm0,%ymm6,%ymm6\n\t# x8 += x13, x7 = rotl32(x7 ^ x8, 12)\n\tvpaddd\t\t%ymm13,%ymm8,%ymm8\n\tvpxor\t\t%ymm8,%ymm7,%ymm7\n\tvpslld\t\t$12,%ymm7,%ymm0\n\tvpsrld\t\t$20,%ymm7,%ymm7\n\tvpor\t\t%ymm0,%ymm7,%ymm7\n\t# x9 += x14, x4 = rotl32(x4 ^ x9, 12)\n\tvpaddd\t\t%ymm14,%ymm9,%ymm9\n\tvpxor\t\t%ymm9,%ymm4,%ymm4\n\tvpslld\t\t$12,%ymm4,%ymm0\n\tvpsrld\t\t$20,%ymm4,%ymm4\n\tvpor\t\t%ymm0,%ymm4,%ymm4\n\n\t# x0 += x5, x15 = rotl32(x15 ^ x0, 8)\n\tvpaddd\t\t0x00(%rsp),%ymm5,%ymm0\n\tvmovdqa\t\t%ymm0,0x00(%rsp)\n\tvpxor\t\t%ymm0,%ymm15,%ymm15\n\tvpshufb\t\t%ymm2,%ymm15,%ymm15\n\t# x1 += x6, x12 = rotl32(x12 ^ x1, 8)\n\tvpaddd\t\t0x20(%rsp),%ymm6,%ymm0\n\tvmovdqa\t\t%ymm0,0x20(%rsp)\n\tvpxor\t\t%ymm0,%ymm12,%ymm12\n\tvpshufb\t\t%ymm2,%ymm12,%ymm12\n\t# x2 += x7, x13 = rotl32(x13 ^ x2, 8)\n\tvpaddd\t\t0x40(%rsp),%ymm7,%ymm0\n\tvmovdqa\t\t%ymm0,0x40(%rsp)\n\tvpxor\t\t%ymm0,%ymm13,%ymm13\n\tvpshufb\t\t%ymm2,%ymm13,%ymm13\n\t# x3 += x4, x14 = rotl32(x14 ^ x3, 8)\n\tvpaddd\t\t0x60(%rsp),%ymm4,%ymm0\n\tvmovdqa\t\t%ymm0,0x60(%rsp)\n\tvpxor\t\t%ymm0,%ymm14,%ymm14\n\tvpshufb\t\t%ymm2,%ymm14,%ymm14\n\n\t# x10 += x15, x5 = rotl32(x5 ^ x10, 7)\n\tvpaddd\t\t%ymm15,%ymm10,%ymm10\n\tvpxor\t\t%ymm10,%ymm5,%ymm5\n\tvpslld\t\t$7,%ymm5,%ymm0\n\tvpsrld\t\t$25,%ymm5,%ymm5\n\tvpor\t\t%ymm0,%ymm5,%ymm5\n\t# x11 += x12, x6 = rotl32(x6 ^ x11, 7)\n\tvpaddd\t\t%ymm12,%ymm11,%ymm11\n\tvpxor\t\t%ymm11,%ymm6,%ymm6\n\tvpslld\t\t$7,%ymm6,%ymm0\n\tvpsrld\t\t$25,%ymm6,%ymm6\n\tvpor\t\t%ymm0,%ymm6,%ymm6\n\t# x8 += x13, x7 = rotl32(x7 ^ x8, 7)\n\tvpaddd\t\t%ymm13,%ymm8,%ymm8\n\tvpxor\t\t%ymm8,%ymm7,%ymm7\n\tvpslld\t\t$7,%ymm7,%ymm0\n\tvpsrld\t\t$25,%ymm7,%ymm7\n\tvpor\t\t%ymm0,%ymm7,%ymm7\n\t# x9 += x14, x4 = rotl32(x4 ^ x9, 7)\n\tvpaddd\t\t%ymm14,%ymm9,%ymm9\n\tvpxor\t\t%ymm9,%ymm4,%ymm4\n\tvpslld\t\t$7,%ymm4,%ymm0\n\tvpsrld\t\t$25,%ymm4,%ymm4\n\tvpor\t\t%ymm0,%ymm4,%ymm4\n\n\tsub\t\t$2,%r8d\n\tjnz\t\t.Ldoubleround8\n\n\t# x0..15[0-3] += s[0..15]\n\tvpbroadcastd\t0x00(%rdi),%ymm0\n\tvpaddd\t\t0x00(%rsp),%ymm0,%ymm0\n\tvmovdqa\t\t%ymm0,0x00(%rsp)\n\tvpbroadcastd\t0x04(%rdi),%ymm0\n\tvpaddd\t\t0x20(%rsp),%ymm0,%ymm0\n\tvmovdqa\t\t%ymm0,0x20(%rsp)\n\tvpbroadcastd\t0x08(%rdi),%ymm0\n\tvpaddd\t\t0x40(%rsp),%ymm0,%ymm0\n\tvmovdqa\t\t%ymm0,0x40(%rsp)\n\tvpbroadcastd\t0x0c(%rdi),%ymm0\n\tvpaddd\t\t0x60(%rsp),%ymm0,%ymm0\n\tvmovdqa\t\t%ymm0,0x60(%rsp)\n\tvpbroadcastd\t0x10(%rdi),%ymm0\n\tvpaddd\t\t%ymm0,%ymm4,%ymm4\n\tvpbroadcastd\t0x14(%rdi),%ymm0\n\tvpaddd\t\t%ymm0,%ymm5,%ymm5\n\tvpbroadcastd\t0x18(%rdi),%ymm0\n\tvpaddd\t\t%ymm0,%ymm6,%ymm6\n\tvpbroadcastd\t0x1c(%rdi),%ymm0\n\tvpaddd\t\t%ymm0,%ymm7,%ymm7\n\tvpbroadcastd\t0x20(%rdi),%ymm0\n\tvpaddd\t\t%ymm0,%ymm8,%ymm8\n\tvpbroadcastd\t0x24(%rdi),%ymm0\n\tvpaddd\t\t%ymm0,%ymm9,%ymm9\n\tvpbroadcastd\t0x28(%rdi),%ymm0\n\tvpaddd\t\t%ymm0,%ymm10,%ymm10\n\tvpbroadcastd\t0x2c(%rdi),%ymm0\n\tvpaddd\t\t%ymm0,%ymm11,%ymm11\n\tvpbroadcastd\t0x30(%rdi),%ymm0\n\tvpaddd\t\t%ymm0,%ymm12,%ymm12\n\tvpbroadcastd\t0x34(%rdi),%ymm0\n\tvpaddd\t\t%ymm0,%ymm13,%ymm13\n\tvpbroadcastd\t0x38(%rdi),%ymm0\n\tvpaddd\t\t%ymm0,%ymm14,%ymm14\n\tvpbroadcastd\t0x3c(%rdi),%ymm0\n\tvpaddd\t\t%ymm0,%ymm15,%ymm15\n\n\t# x12 += counter values 0-3\n\tvpaddd\t\t%ymm1,%ymm12,%ymm12\n\n\t# interleave 32-bit words in state n, n+1\n\tvmovdqa\t\t0x00(%rsp),%ymm0\n\tvmovdqa\t\t0x20(%rsp),%ymm1\n\tvpunpckldq\t%ymm1,%ymm0,%ymm2\n\tvpunpckhdq\t%ymm1,%ymm0,%ymm1\n\tvmovdqa\t\t%ymm2,0x00(%rsp)\n\tvmovdqa\t\t%ymm1,0x20(%rsp)\n\tvmovdqa\t\t0x40(%rsp),%ymm0\n\tvmovdqa\t\t0x60(%rsp),%ymm1\n\tvpunpckldq\t%ymm1,%ymm0,%ymm2\n\tvpunpckhdq\t%ymm1,%ymm0,%ymm1\n\tvmovdqa\t\t%ymm2,0x40(%rsp)\n\tvmovdqa\t\t%ymm1,0x60(%rsp)\n\tvmovdqa\t\t%ymm4,%ymm0\n\tvpunpckldq\t%ymm5,%ymm0,%ymm4\n\tvpunpckhdq\t%ymm5,%ymm0,%ymm5\n\tvmovdqa\t\t%ymm6,%ymm0\n\tvpunpckldq\t%ymm7,%ymm0,%ymm6\n\tvpunpckhdq\t%ymm7,%ymm0,%ymm7\n\tvmovdqa\t\t%ymm8,%ymm0\n\tvpunpckldq\t%ymm9,%ymm0,%ymm8\n\tvpunpckhdq\t%ymm9,%ymm0,%ymm9\n\tvmovdqa\t\t%ymm10,%ymm0\n\tvpunpckldq\t%ymm11,%ymm0,%ymm10\n\tvpunpckhdq\t%ymm11,%ymm0,%ymm11\n\tvmovdqa\t\t%ymm12,%ymm0\n\tvpunpckldq\t%ymm13,%ymm0,%ymm12\n\tvpunpckhdq\t%ymm13,%ymm0,%ymm13\n\tvmovdqa\t\t%ymm14,%ymm0\n\tvpunpckldq\t%ymm15,%ymm0,%ymm14\n\tvpunpckhdq\t%ymm15,%ymm0,%ymm15\n\n\t# interleave 64-bit words in state n, n+2\n\tvmovdqa\t\t0x00(%rsp),%ymm0\n\tvmovdqa\t\t0x40(%rsp),%ymm2\n\tvpunpcklqdq\t%ymm2,%ymm0,%ymm1\n\tvpunpckhqdq\t%ymm2,%ymm0,%ymm2\n\tvmovdqa\t\t%ymm1,0x00(%rsp)\n\tvmovdqa\t\t%ymm2,0x40(%rsp)\n\tvmovdqa\t\t0x20(%rsp),%ymm0\n\tvmovdqa\t\t0x60(%rsp),%ymm2\n\tvpunpcklqdq\t%ymm2,%ymm0,%ymm1\n\tvpunpckhqdq\t%ymm2,%ymm0,%ymm2\n\tvmovdqa\t\t%ymm1,0x20(%rsp)\n\tvmovdqa\t\t%ymm2,0x60(%rsp)\n\tvmovdqa\t\t%ymm4,%ymm0\n\tvpunpcklqdq\t%ymm6,%ymm0,%ymm4\n\tvpunpckhqdq\t%ymm6,%ymm0,%ymm6\n\tvmovdqa\t\t%ymm5,%ymm0\n\tvpunpcklqdq\t%ymm7,%ymm0,%ymm5\n\tvpunpckhqdq\t%ymm7,%ymm0,%ymm7\n\tvmovdqa\t\t%ymm8,%ymm0\n\tvpunpcklqdq\t%ymm10,%ymm0,%ymm8\n\tvpunpckhqdq\t%ymm10,%ymm0,%ymm10\n\tvmovdqa\t\t%ymm9,%ymm0\n\tvpunpcklqdq\t%ymm11,%ymm0,%ymm9\n\tvpunpckhqdq\t%ymm11,%ymm0,%ymm11\n\tvmovdqa\t\t%ymm12,%ymm0\n\tvpunpcklqdq\t%ymm14,%ymm0,%ymm12\n\tvpunpckhqdq\t%ymm14,%ymm0,%ymm14\n\tvmovdqa\t\t%ymm13,%ymm0\n\tvpunpcklqdq\t%ymm15,%ymm0,%ymm13\n\tvpunpckhqdq\t%ymm15,%ymm0,%ymm15\n\n\t# interleave 128-bit words in state n, n+4\n\t# xor/write first four blocks\n\tvmovdqa\t\t0x00(%rsp),%ymm1\n\tvperm2i128\t$0x20,%ymm4,%ymm1,%ymm0\n\tcmp\t\t$0x0020,%rax\n\tjl\t\t.Lxorpart8\n\tvpxor\t\t0x0000(%rdx),%ymm0,%ymm0\n\tvmovdqu\t\t%ymm0,0x0000(%rsi)\n\tvperm2i128\t$0x31,%ymm4,%ymm1,%ymm4\n\n\tvperm2i128\t$0x20,%ymm12,%ymm8,%ymm0\n\tcmp\t\t$0x0040,%rax\n\tjl\t\t.Lxorpart8\n\tvpxor\t\t0x0020(%rdx),%ymm0,%ymm0\n\tvmovdqu\t\t%ymm0,0x0020(%rsi)\n\tvperm2i128\t$0x31,%ymm12,%ymm8,%ymm12\n\n\tvmovdqa\t\t0x40(%rsp),%ymm1\n\tvperm2i128\t$0x20,%ymm6,%ymm1,%ymm0\n\tcmp\t\t$0x0060,%rax\n\tjl\t\t.Lxorpart8\n\tvpxor\t\t0x0040(%rdx),%ymm0,%ymm0\n\tvmovdqu\t\t%ymm0,0x0040(%rsi)\n\tvperm2i128\t$0x31,%ymm6,%ymm1,%ymm6\n\n\tvperm2i128\t$0x20,%ymm14,%ymm10,%ymm0\n\tcmp\t\t$0x0080,%rax\n\tjl\t\t.Lxorpart8\n\tvpxor\t\t0x0060(%rdx),%ymm0,%ymm0\n\tvmovdqu\t\t%ymm0,0x0060(%rsi)\n\tvperm2i128\t$0x31,%ymm14,%ymm10,%ymm14\n\n\tvmovdqa\t\t0x20(%rsp),%ymm1\n\tvperm2i128\t$0x20,%ymm5,%ymm1,%ymm0\n\tcmp\t\t$0x00a0,%rax\n\tjl\t\t.Lxorpart8\n\tvpxor\t\t0x0080(%rdx),%ymm0,%ymm0\n\tvmovdqu\t\t%ymm0,0x0080(%rsi)\n\tvperm2i128\t$0x31,%ymm5,%ymm1,%ymm5\n\n\tvperm2i128\t$0x20,%ymm13,%ymm9,%ymm0\n\tcmp\t\t$0x00c0,%rax\n\tjl\t\t.Lxorpart8\n\tvpxor\t\t0x00a0(%rdx),%ymm0,%ymm0\n\tvmovdqu\t\t%ymm0,0x00a0(%rsi)\n\tvperm2i128\t$0x31,%ymm13,%ymm9,%ymm13\n\n\tvmovdqa\t\t0x60(%rsp),%ymm1\n\tvperm2i128\t$0x20,%ymm7,%ymm1,%ymm0\n\tcmp\t\t$0x00e0,%rax\n\tjl\t\t.Lxorpart8\n\tvpxor\t\t0x00c0(%rdx),%ymm0,%ymm0\n\tvmovdqu\t\t%ymm0,0x00c0(%rsi)\n\tvperm2i128\t$0x31,%ymm7,%ymm1,%ymm7\n\n\tvperm2i128\t$0x20,%ymm15,%ymm11,%ymm0\n\tcmp\t\t$0x0100,%rax\n\tjl\t\t.Lxorpart8\n\tvpxor\t\t0x00e0(%rdx),%ymm0,%ymm0\n\tvmovdqu\t\t%ymm0,0x00e0(%rsi)\n\tvperm2i128\t$0x31,%ymm15,%ymm11,%ymm15\n\n\t# xor remaining blocks, write to output\n\tvmovdqa\t\t%ymm4,%ymm0\n\tcmp\t\t$0x0120,%rax\n\tjl\t\t.Lxorpart8\n\tvpxor\t\t0x0100(%rdx),%ymm0,%ymm0\n\tvmovdqu\t\t%ymm0,0x0100(%rsi)\n\n\tvmovdqa\t\t%ymm12,%ymm0\n\tcmp\t\t$0x0140,%rax\n\tjl\t\t.Lxorpart8\n\tvpxor\t\t0x0120(%rdx),%ymm0,%ymm0\n\tvmovdqu\t\t%ymm0,0x0120(%rsi)\n\n\tvmovdqa\t\t%ymm6,%ymm0\n\tcmp\t\t$0x0160,%rax\n\tjl\t\t.Lxorpart8\n\tvpxor\t\t0x0140(%rdx),%ymm0,%ymm0\n\tvmovdqu\t\t%ymm0,0x0140(%rsi)\n\n\tvmovdqa\t\t%ymm14,%ymm0\n\tcmp\t\t$0x0180,%rax\n\tjl\t\t.Lxorpart8\n\tvpxor\t\t0x0160(%rdx),%ymm0,%ymm0\n\tvmovdqu\t\t%ymm0,0x0160(%rsi)\n\n\tvmovdqa\t\t%ymm5,%ymm0\n\tcmp\t\t$0x01a0,%rax\n\tjl\t\t.Lxorpart8\n\tvpxor\t\t0x0180(%rdx),%ymm0,%ymm0\n\tvmovdqu\t\t%ymm0,0x0180(%rsi)\n\n\tvmovdqa\t\t%ymm13,%ymm0\n\tcmp\t\t$0x01c0,%rax\n\tjl\t\t.Lxorpart8\n\tvpxor\t\t0x01a0(%rdx),%ymm0,%ymm0\n\tvmovdqu\t\t%ymm0,0x01a0(%rsi)\n\n\tvmovdqa\t\t%ymm7,%ymm0\n\tcmp\t\t$0x01e0,%rax\n\tjl\t\t.Lxorpart8\n\tvpxor\t\t0x01c0(%rdx),%ymm0,%ymm0\n\tvmovdqu\t\t%ymm0,0x01c0(%rsi)\n\n\tvmovdqa\t\t%ymm15,%ymm0\n\tcmp\t\t$0x0200,%rax\n\tjl\t\t.Lxorpart8\n\tvpxor\t\t0x01e0(%rdx),%ymm0,%ymm0\n\tvmovdqu\t\t%ymm0,0x01e0(%rsi)\n\n.Ldone8:\n\tvzeroupper\n\tlea\t\t-8(%r10),%rsp\n\tRET\n\n.Lxorpart8:\n\t# xor remaining bytes from partial register into output\n\tmov\t\t%rax,%r9\n\tand\t\t$0x1f,%r9\n\tjz\t\t.Ldone8\n\tand\t\t$~0x1f,%rax\n\n\tmov\t\t%rsi,%r11\n\n\tlea\t\t(%rdx,%rax),%rsi\n\tmov\t\t%rsp,%rdi\n\tmov\t\t%r9,%rcx\n\trep movsb\n\n\tvpxor\t\t0x00(%rsp),%ymm0,%ymm0\n\tvmovdqa\t\t%ymm0,0x00(%rsp)\n\n\tmov\t\t%rsp,%rsi\n\tlea\t\t(%r11,%rax),%rdi\n\tmov\t\t%r9,%rcx\n\trep movsb\n\n\tjmp\t\t.Ldone8\n\nSYM_FUNC_END(chacha_8block_xor_avx2)\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}