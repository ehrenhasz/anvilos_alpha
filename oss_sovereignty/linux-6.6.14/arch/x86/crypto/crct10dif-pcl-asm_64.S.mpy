{
  "module_name": "crct10dif-pcl-asm_64.S",
  "hash_id": "3f4076d28aa55c806dc9a232eb3a15ed6dfdbbc7f54ed8a3a12c773b6bba7e35",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/crypto/crct10dif-pcl-asm_64.S",
  "human_readable_source": "########################################################################\n# Implement fast CRC-T10DIF computation with SSE and PCLMULQDQ instructions\n#\n# Copyright (c) 2013, Intel Corporation\n#\n# Authors:\n#     Erdinc Ozturk <erdinc.ozturk@intel.com>\n#     Vinodh Gopal <vinodh.gopal@intel.com>\n#     James Guilford <james.guilford@intel.com>\n#     Tim Chen <tim.c.chen@linux.intel.com>\n#\n# This software is available to you under a choice of one of two\n# licenses.  You may choose to be licensed under the terms of the GNU\n# General Public License (GPL) Version 2, available from the file\n# COPYING in the main directory of this source tree, or the\n# OpenIB.org BSD license below:\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n# * Redistributions of source code must retain the above copyright\n#   notice, this list of conditions and the following disclaimer.\n#\n# * Redistributions in binary form must reproduce the above copyright\n#   notice, this list of conditions and the following disclaimer in the\n#   documentation and/or other materials provided with the\n#   distribution.\n#\n# * Neither the name of the Intel Corporation nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n#\n#\n# THIS SOFTWARE IS PROVIDED BY INTEL CORPORATION \"\"AS IS\"\" AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL INTEL CORPORATION OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n# LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n# NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n#\n#       Reference paper titled \"Fast CRC Computation for Generic\n#\tPolynomials Using PCLMULQDQ Instruction\"\n#       URL: http: \n#  /white-papers/fast-crc-computation-generic-polynomials-pclmulqdq-paper.pdf\n#\n\n#include <linux/linkage.h>\n\n.text\n\n#define\t\tinit_crc\t%edi\n#define\t\tbuf\t\t%rsi\n#define\t\tlen\t\t%rdx\n\n#define\t\tFOLD_CONSTS\t%xmm10\n#define\t\tBSWAP_MASK\t%xmm11\n\n# Fold reg1, reg2 into the next 32 data bytes, storing the result back into\n# reg1, reg2.\n.macro\tfold_32_bytes\toffset, reg1, reg2\n\tmovdqu\t\\offset(buf), %xmm9\n\tmovdqu\t\\offset+16(buf), %xmm12\n\tpshufb\tBSWAP_MASK, %xmm9\n\tpshufb\tBSWAP_MASK, %xmm12\n\tmovdqa\t\\reg1, %xmm8\n\tmovdqa\t\\reg2, %xmm13\n\tpclmulqdq\t$0x00, FOLD_CONSTS, \\reg1\n\tpclmulqdq\t$0x11, FOLD_CONSTS, %xmm8\n\tpclmulqdq\t$0x00, FOLD_CONSTS, \\reg2\n\tpclmulqdq\t$0x11, FOLD_CONSTS, %xmm13\n\tpxor\t%xmm9 , \\reg1\n\txorps\t%xmm8 , \\reg1\n\tpxor\t%xmm12, \\reg2\n\txorps\t%xmm13, \\reg2\n.endm\n\n# Fold src_reg into dst_reg.\n.macro\tfold_16_bytes\tsrc_reg, dst_reg\n\tmovdqa\t\\src_reg, %xmm8\n\tpclmulqdq\t$0x11, FOLD_CONSTS, \\src_reg\n\tpclmulqdq\t$0x00, FOLD_CONSTS, %xmm8\n\tpxor\t%xmm8, \\dst_reg\n\txorps\t\\src_reg, \\dst_reg\n.endm\n\n#\n# u16 crc_t10dif_pcl(u16 init_crc, const *u8 buf, size_t len);\n#\n# Assumes len >= 16.\n#\nSYM_FUNC_START(crc_t10dif_pcl)\n\n\tmovdqa\t.Lbswap_mask(%rip), BSWAP_MASK\n\n\t# For sizes less than 256 bytes, we can't fold 128 bytes at a time.\n\tcmp\t$256, len\n\tjl\t.Lless_than_256_bytes\n\n\t# Load the first 128 data bytes.  Byte swapping is necessary to make the\n\t# bit order match the polynomial coefficient order.\n\tmovdqu\t16*0(buf), %xmm0\n\tmovdqu\t16*1(buf), %xmm1\n\tmovdqu\t16*2(buf), %xmm2\n\tmovdqu\t16*3(buf), %xmm3\n\tmovdqu\t16*4(buf), %xmm4\n\tmovdqu\t16*5(buf), %xmm5\n\tmovdqu\t16*6(buf), %xmm6\n\tmovdqu\t16*7(buf), %xmm7\n\tadd\t$128, buf\n\tpshufb\tBSWAP_MASK, %xmm0\n\tpshufb\tBSWAP_MASK, %xmm1\n\tpshufb\tBSWAP_MASK, %xmm2\n\tpshufb\tBSWAP_MASK, %xmm3\n\tpshufb\tBSWAP_MASK, %xmm4\n\tpshufb\tBSWAP_MASK, %xmm5\n\tpshufb\tBSWAP_MASK, %xmm6\n\tpshufb\tBSWAP_MASK, %xmm7\n\n\t# XOR the first 16 data *bits* with the initial CRC value.\n\tpxor\t%xmm8, %xmm8\n\tpinsrw\t$7, init_crc, %xmm8\n\tpxor\t%xmm8, %xmm0\n\n\tmovdqa\t.Lfold_across_128_bytes_consts(%rip), FOLD_CONSTS\n\n\t# Subtract 128 for the 128 data bytes just consumed.  Subtract another\n\t# 128 to simplify the termination condition of the following loop.\n\tsub\t$256, len\n\n\t# While >= 128 data bytes remain (not counting xmm0-7), fold the 128\n\t# bytes xmm0-7 into them, storing the result back into xmm0-7.\n.Lfold_128_bytes_loop:\n\tfold_32_bytes\t0, %xmm0, %xmm1\n\tfold_32_bytes\t32, %xmm2, %xmm3\n\tfold_32_bytes\t64, %xmm4, %xmm5\n\tfold_32_bytes\t96, %xmm6, %xmm7\n\tadd\t$128, buf\n\tsub\t$128, len\n\tjge\t.Lfold_128_bytes_loop\n\n\t# Now fold the 112 bytes in xmm0-xmm6 into the 16 bytes in xmm7.\n\n\t# Fold across 64 bytes.\n\tmovdqa\t.Lfold_across_64_bytes_consts(%rip), FOLD_CONSTS\n\tfold_16_bytes\t%xmm0, %xmm4\n\tfold_16_bytes\t%xmm1, %xmm5\n\tfold_16_bytes\t%xmm2, %xmm6\n\tfold_16_bytes\t%xmm3, %xmm7\n\t# Fold across 32 bytes.\n\tmovdqa\t.Lfold_across_32_bytes_consts(%rip), FOLD_CONSTS\n\tfold_16_bytes\t%xmm4, %xmm6\n\tfold_16_bytes\t%xmm5, %xmm7\n\t# Fold across 16 bytes.\n\tmovdqa\t.Lfold_across_16_bytes_consts(%rip), FOLD_CONSTS\n\tfold_16_bytes\t%xmm6, %xmm7\n\n\t# Add 128 to get the correct number of data bytes remaining in 0...127\n\t# (not counting xmm7), following the previous extra subtraction by 128.\n\t# Then subtract 16 to simplify the termination condition of the\n\t# following loop.\n\tadd\t$128-16, len\n\n\t# While >= 16 data bytes remain (not counting xmm7), fold the 16 bytes\n\t# xmm7 into them, storing the result back into xmm7.\n\tjl\t.Lfold_16_bytes_loop_done\n.Lfold_16_bytes_loop:\n\tmovdqa\t%xmm7, %xmm8\n\tpclmulqdq\t$0x11, FOLD_CONSTS, %xmm7\n\tpclmulqdq\t$0x00, FOLD_CONSTS, %xmm8\n\tpxor\t%xmm8, %xmm7\n\tmovdqu\t(buf), %xmm0\n\tpshufb\tBSWAP_MASK, %xmm0\n\tpxor\t%xmm0 , %xmm7\n\tadd\t$16, buf\n\tsub\t$16, len\n\tjge\t.Lfold_16_bytes_loop\n\n.Lfold_16_bytes_loop_done:\n\t# Add 16 to get the correct number of data bytes remaining in 0...15\n\t# (not counting xmm7), following the previous extra subtraction by 16.\n\tadd\t$16, len\n\tje\t.Lreduce_final_16_bytes\n\n.Lhandle_partial_segment:\n\t# Reduce the last '16 + len' bytes where 1 <= len <= 15 and the first 16\n\t# bytes are in xmm7 and the rest are the remaining data in 'buf'.  To do\n\t# this without needing a fold constant for each possible 'len', redivide\n\t# the bytes into a first chunk of 'len' bytes and a second chunk of 16\n\t# bytes, then fold the first chunk into the second.\n\n\tmovdqa\t%xmm7, %xmm2\n\n\t# xmm1 = last 16 original data bytes\n\tmovdqu\t-16(buf, len), %xmm1\n\tpshufb\tBSWAP_MASK, %xmm1\n\n\t# xmm2 = high order part of second chunk: xmm7 left-shifted by 'len' bytes.\n\tlea\t.Lbyteshift_table+16(%rip), %rax\n\tsub\tlen, %rax\n\tmovdqu\t(%rax), %xmm0\n\tpshufb\t%xmm0, %xmm2\n\n\t# xmm7 = first chunk: xmm7 right-shifted by '16-len' bytes.\n\tpxor\t.Lmask1(%rip), %xmm0\n\tpshufb\t%xmm0, %xmm7\n\n\t# xmm1 = second chunk: 'len' bytes from xmm1 (low-order bytes),\n\t# then '16-len' bytes from xmm2 (high-order bytes).\n\tpblendvb\t%xmm2, %xmm1\t#xmm0 is implicit\n\n\t# Fold the first chunk into the second chunk, storing the result in xmm7.\n\tmovdqa\t%xmm7, %xmm8\n\tpclmulqdq\t$0x11, FOLD_CONSTS, %xmm7\n\tpclmulqdq\t$0x00, FOLD_CONSTS, %xmm8\n\tpxor\t%xmm8, %xmm7\n\tpxor\t%xmm1, %xmm7\n\n.Lreduce_final_16_bytes:\n\t# Reduce the 128-bit value M(x), stored in xmm7, to the final 16-bit CRC\n\n\t# Load 'x^48 * (x^48 mod G(x))' and 'x^48 * (x^80 mod G(x))'.\n\tmovdqa\t.Lfinal_fold_consts(%rip), FOLD_CONSTS\n\n\t# Fold the high 64 bits into the low 64 bits, while also multiplying by\n\t# x^64.  This produces a 128-bit value congruent to x^64 * M(x) and\n\t# whose low 48 bits are 0.\n\tmovdqa\t%xmm7, %xmm0\n\tpclmulqdq\t$0x11, FOLD_CONSTS, %xmm7 # high bits * x^48 * (x^80 mod G(x))\n\tpslldq\t$8, %xmm0\n\tpxor\t%xmm0, %xmm7\t\t\t  # + low bits * x^64\n\n\t# Fold the high 32 bits into the low 96 bits.  This produces a 96-bit\n\t# value congruent to x^64 * M(x) and whose low 48 bits are 0.\n\tmovdqa\t%xmm7, %xmm0\n\tpand\t.Lmask2(%rip), %xmm0\t\t  # zero high 32 bits\n\tpsrldq\t$12, %xmm7\t\t\t  # extract high 32 bits\n\tpclmulqdq\t$0x00, FOLD_CONSTS, %xmm7 # high 32 bits * x^48 * (x^48 mod G(x))\n\tpxor\t%xmm0, %xmm7\t\t\t  # + low bits\n\n\t# Load G(x) and floor(x^48 / G(x)).\n\tmovdqa\t.Lbarrett_reduction_consts(%rip), FOLD_CONSTS\n\n\t# Use Barrett reduction to compute the final CRC value.\n\tmovdqa\t%xmm7, %xmm0\n\tpclmulqdq\t$0x11, FOLD_CONSTS, %xmm7 # high 32 bits * floor(x^48 / G(x))\n\tpsrlq\t$32, %xmm7\t\t\t  # /= x^32\n\tpclmulqdq\t$0x00, FOLD_CONSTS, %xmm7 # *= G(x)\n\tpsrlq\t$48, %xmm0\n\tpxor\t%xmm7, %xmm0\t\t     # + low 16 nonzero bits\n\t# Final CRC value (x^16 * M(x)) mod G(x) is in low 16 bits of xmm0.\n\n\tpextrw\t$0, %xmm0, %eax\n\tRET\n\n.align 16\n.Lless_than_256_bytes:\n\t# Checksumming a buffer of length 16...255 bytes\n\n\t# Load the first 16 data bytes.\n\tmovdqu\t(buf), %xmm7\n\tpshufb\tBSWAP_MASK, %xmm7\n\tadd\t$16, buf\n\n\t# XOR the first 16 data *bits* with the initial CRC value.\n\tpxor\t%xmm0, %xmm0\n\tpinsrw\t$7, init_crc, %xmm0\n\tpxor\t%xmm0, %xmm7\n\n\tmovdqa\t.Lfold_across_16_bytes_consts(%rip), FOLD_CONSTS\n\tcmp\t$16, len\n\tje\t.Lreduce_final_16_bytes\t\t# len == 16\n\tsub\t$32, len\n\tjge\t.Lfold_16_bytes_loop\t\t# 32 <= len <= 255\n\tadd\t$16, len\n\tjmp\t.Lhandle_partial_segment\t# 17 <= len <= 31\nSYM_FUNC_END(crc_t10dif_pcl)\n\n.section\t.rodata, \"a\", @progbits\n.align 16\n\n# Fold constants precomputed from the polynomial 0x18bb7\n# G(x) = x^16 + x^15 + x^11 + x^9 + x^8 + x^7 + x^5 + x^4 + x^2 + x^1 + x^0\n.Lfold_across_128_bytes_consts:\n\t.quad\t\t0x0000000000006123\t# x^(8*128)\tmod G(x)\n\t.quad\t\t0x0000000000002295\t# x^(8*128+64)\tmod G(x)\n.Lfold_across_64_bytes_consts:\n\t.quad\t\t0x0000000000001069\t# x^(4*128)\tmod G(x)\n\t.quad\t\t0x000000000000dd31\t# x^(4*128+64)\tmod G(x)\n.Lfold_across_32_bytes_consts:\n\t.quad\t\t0x000000000000857d\t# x^(2*128)\tmod G(x)\n\t.quad\t\t0x0000000000007acc\t# x^(2*128+64)\tmod G(x)\n.Lfold_across_16_bytes_consts:\n\t.quad\t\t0x000000000000a010\t# x^(1*128)\tmod G(x)\n\t.quad\t\t0x0000000000001faa\t# x^(1*128+64)\tmod G(x)\n.Lfinal_fold_consts:\n\t.quad\t\t0x1368000000000000\t# x^48 * (x^48 mod G(x))\n\t.quad\t\t0x2d56000000000000\t# x^48 * (x^80 mod G(x))\n.Lbarrett_reduction_consts:\n\t.quad\t\t0x0000000000018bb7\t# G(x)\n\t.quad\t\t0x00000001f65a57f8\t# floor(x^48 / G(x))\n\n.section\t.rodata.cst16.mask1, \"aM\", @progbits, 16\n.align 16\n.Lmask1:\n\t.octa\t0x80808080808080808080808080808080\n\n.section\t.rodata.cst16.mask2, \"aM\", @progbits, 16\n.align 16\n.Lmask2:\n\t.octa\t0x00000000FFFFFFFFFFFFFFFFFFFFFFFF\n\n.section\t.rodata.cst16.bswap_mask, \"aM\", @progbits, 16\n.align 16\n.Lbswap_mask:\n\t.octa\t0x000102030405060708090A0B0C0D0E0F\n\n.section\t.rodata.cst32.byteshift_table, \"aM\", @progbits, 32\n.align 16\n# For 1 <= len <= 15, the 16-byte vector beginning at &byteshift_table[16 - len]\n# is the index vector to shift left by 'len' bytes, and is also {0x80, ...,\n# 0x80} XOR the index vector to shift right by '16 - len' bytes.\n.Lbyteshift_table:\n\t.byte\t\t 0x0, 0x81, 0x82, 0x83, 0x84, 0x85, 0x86, 0x87\n\t.byte\t\t0x88, 0x89, 0x8a, 0x8b, 0x8c, 0x8d, 0x8e, 0x8f\n\t.byte\t\t 0x0,  0x1,  0x2,  0x3,  0x4,  0x5,  0x6,  0x7\n\t.byte\t\t 0x8,  0x9,  0xa,  0xb,  0xc,  0xd,  0xe , 0x0\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}