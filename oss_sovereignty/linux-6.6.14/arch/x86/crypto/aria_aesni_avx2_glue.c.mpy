{
  "module_name": "aria_aesni_avx2_glue.c",
  "hash_id": "cf13fb2db26947f8e4c68ff47c2f8ff8e7104a83c131a9b0118289d9aef30003",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/crypto/aria_aesni_avx2_glue.c",
  "human_readable_source": " \n \n\n#include <crypto/algapi.h>\n#include <crypto/internal/simd.h>\n#include <crypto/aria.h>\n#include <linux/crypto.h>\n#include <linux/err.h>\n#include <linux/module.h>\n#include <linux/types.h>\n\n#include \"ecb_cbc_helpers.h\"\n#include \"aria-avx.h\"\n\nasmlinkage void aria_aesni_avx2_encrypt_32way(const void *ctx, u8 *dst,\n\t\t\t\t\t      const u8 *src);\nEXPORT_SYMBOL_GPL(aria_aesni_avx2_encrypt_32way);\nasmlinkage void aria_aesni_avx2_decrypt_32way(const void *ctx, u8 *dst,\n\t\t\t\t\t      const u8 *src);\nEXPORT_SYMBOL_GPL(aria_aesni_avx2_decrypt_32way);\nasmlinkage void aria_aesni_avx2_ctr_crypt_32way(const void *ctx, u8 *dst,\n\t\t\t\t\t\tconst u8 *src,\n\t\t\t\t\t\tu8 *keystream, u8 *iv);\nEXPORT_SYMBOL_GPL(aria_aesni_avx2_ctr_crypt_32way);\n#ifdef CONFIG_AS_GFNI\nasmlinkage void aria_aesni_avx2_gfni_encrypt_32way(const void *ctx, u8 *dst,\n\t\t\t\t\t\t   const u8 *src);\nEXPORT_SYMBOL_GPL(aria_aesni_avx2_gfni_encrypt_32way);\nasmlinkage void aria_aesni_avx2_gfni_decrypt_32way(const void *ctx, u8 *dst,\n\t\t\t\t\t\t   const u8 *src);\nEXPORT_SYMBOL_GPL(aria_aesni_avx2_gfni_decrypt_32way);\nasmlinkage void aria_aesni_avx2_gfni_ctr_crypt_32way(const void *ctx, u8 *dst,\n\t\t\t\t\t\t     const u8 *src,\n\t\t\t\t\t\t     u8 *keystream, u8 *iv);\nEXPORT_SYMBOL_GPL(aria_aesni_avx2_gfni_ctr_crypt_32way);\n#endif  \n\nstatic struct aria_avx_ops aria_ops;\n\nstruct aria_avx2_request_ctx {\n\tu8 keystream[ARIA_AESNI_AVX2_PARALLEL_BLOCK_SIZE];\n};\n\nstatic int ecb_do_encrypt(struct skcipher_request *req, const u32 *rkey)\n{\n\tECB_WALK_START(req, ARIA_BLOCK_SIZE, ARIA_AESNI_PARALLEL_BLOCKS);\n\tECB_BLOCK(ARIA_AESNI_AVX2_PARALLEL_BLOCKS, aria_ops.aria_encrypt_32way);\n\tECB_BLOCK(ARIA_AESNI_PARALLEL_BLOCKS, aria_ops.aria_encrypt_16way);\n\tECB_BLOCK(1, aria_encrypt);\n\tECB_WALK_END();\n}\n\nstatic int ecb_do_decrypt(struct skcipher_request *req, const u32 *rkey)\n{\n\tECB_WALK_START(req, ARIA_BLOCK_SIZE, ARIA_AESNI_PARALLEL_BLOCKS);\n\tECB_BLOCK(ARIA_AESNI_AVX2_PARALLEL_BLOCKS, aria_ops.aria_decrypt_32way);\n\tECB_BLOCK(ARIA_AESNI_PARALLEL_BLOCKS, aria_ops.aria_decrypt_16way);\n\tECB_BLOCK(1, aria_decrypt);\n\tECB_WALK_END();\n}\n\nstatic int aria_avx2_ecb_encrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct aria_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\treturn ecb_do_encrypt(req, ctx->enc_key[0]);\n}\n\nstatic int aria_avx2_ecb_decrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct aria_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\treturn ecb_do_decrypt(req, ctx->dec_key[0]);\n}\n\nstatic int aria_avx2_set_key(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t    unsigned int keylen)\n{\n\treturn aria_set_key(&tfm->base, key, keylen);\n}\n\nstatic int aria_avx2_ctr_encrypt(struct skcipher_request *req)\n{\n\tstruct aria_avx2_request_ctx *req_ctx = skcipher_request_ctx(req);\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct aria_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes) > 0) {\n\t\tconst u8 *src = walk.src.virt.addr;\n\t\tu8 *dst = walk.dst.virt.addr;\n\n\t\twhile (nbytes >= ARIA_AESNI_AVX2_PARALLEL_BLOCK_SIZE) {\n\t\t\tkernel_fpu_begin();\n\t\t\taria_ops.aria_ctr_crypt_32way(ctx, dst, src,\n\t\t\t\t\t\t      &req_ctx->keystream[0],\n\t\t\t\t\t\t      walk.iv);\n\t\t\tkernel_fpu_end();\n\t\t\tdst += ARIA_AESNI_AVX2_PARALLEL_BLOCK_SIZE;\n\t\t\tsrc += ARIA_AESNI_AVX2_PARALLEL_BLOCK_SIZE;\n\t\t\tnbytes -= ARIA_AESNI_AVX2_PARALLEL_BLOCK_SIZE;\n\t\t}\n\n\t\twhile (nbytes >= ARIA_AESNI_PARALLEL_BLOCK_SIZE) {\n\t\t\tkernel_fpu_begin();\n\t\t\taria_ops.aria_ctr_crypt_16way(ctx, dst, src,\n\t\t\t\t\t\t      &req_ctx->keystream[0],\n\t\t\t\t\t\t      walk.iv);\n\t\t\tkernel_fpu_end();\n\t\t\tdst += ARIA_AESNI_PARALLEL_BLOCK_SIZE;\n\t\t\tsrc += ARIA_AESNI_PARALLEL_BLOCK_SIZE;\n\t\t\tnbytes -= ARIA_AESNI_PARALLEL_BLOCK_SIZE;\n\t\t}\n\n\t\twhile (nbytes >= ARIA_BLOCK_SIZE) {\n\t\t\tmemcpy(&req_ctx->keystream[0], walk.iv, ARIA_BLOCK_SIZE);\n\t\t\tcrypto_inc(walk.iv, ARIA_BLOCK_SIZE);\n\n\t\t\taria_encrypt(ctx, &req_ctx->keystream[0],\n\t\t\t\t     &req_ctx->keystream[0]);\n\n\t\t\tcrypto_xor_cpy(dst, src, &req_ctx->keystream[0],\n\t\t\t\t       ARIA_BLOCK_SIZE);\n\t\t\tdst += ARIA_BLOCK_SIZE;\n\t\t\tsrc += ARIA_BLOCK_SIZE;\n\t\t\tnbytes -= ARIA_BLOCK_SIZE;\n\t\t}\n\n\t\tif (walk.nbytes == walk.total && nbytes > 0) {\n\t\t\tmemcpy(&req_ctx->keystream[0], walk.iv,\n\t\t\t       ARIA_BLOCK_SIZE);\n\t\t\tcrypto_inc(walk.iv, ARIA_BLOCK_SIZE);\n\n\t\t\taria_encrypt(ctx, &req_ctx->keystream[0],\n\t\t\t\t     &req_ctx->keystream[0]);\n\n\t\t\tcrypto_xor_cpy(dst, src, &req_ctx->keystream[0],\n\t\t\t\t       nbytes);\n\t\t\tdst += nbytes;\n\t\t\tsrc += nbytes;\n\t\t\tnbytes = 0;\n\t\t}\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\n\treturn err;\n}\n\nstatic int aria_avx2_init_tfm(struct crypto_skcipher *tfm)\n{\n\tcrypto_skcipher_set_reqsize(tfm, sizeof(struct aria_avx2_request_ctx));\n\n\treturn 0;\n}\n\nstatic struct skcipher_alg aria_algs[] = {\n\t{\n\t\t.base.cra_name\t\t= \"__ecb(aria)\",\n\t\t.base.cra_driver_name\t= \"__ecb-aria-avx2\",\n\t\t.base.cra_priority\t= 500,\n\t\t.base.cra_flags\t\t= CRYPTO_ALG_INTERNAL,\n\t\t.base.cra_blocksize\t= ARIA_BLOCK_SIZE,\n\t\t.base.cra_ctxsize\t= sizeof(struct aria_ctx),\n\t\t.base.cra_module\t= THIS_MODULE,\n\t\t.min_keysize\t\t= ARIA_MIN_KEY_SIZE,\n\t\t.max_keysize\t\t= ARIA_MAX_KEY_SIZE,\n\t\t.setkey\t\t\t= aria_avx2_set_key,\n\t\t.encrypt\t\t= aria_avx2_ecb_encrypt,\n\t\t.decrypt\t\t= aria_avx2_ecb_decrypt,\n\t}, {\n\t\t.base.cra_name\t\t= \"__ctr(aria)\",\n\t\t.base.cra_driver_name\t= \"__ctr-aria-avx2\",\n\t\t.base.cra_priority\t= 500,\n\t\t.base.cra_flags\t\t= CRYPTO_ALG_INTERNAL |\n\t\t\t\t\t  CRYPTO_ALG_SKCIPHER_REQSIZE_LARGE,\n\t\t.base.cra_blocksize\t= 1,\n\t\t.base.cra_ctxsize\t= sizeof(struct aria_ctx),\n\t\t.base.cra_module\t= THIS_MODULE,\n\t\t.min_keysize\t\t= ARIA_MIN_KEY_SIZE,\n\t\t.max_keysize\t\t= ARIA_MAX_KEY_SIZE,\n\t\t.ivsize\t\t\t= ARIA_BLOCK_SIZE,\n\t\t.chunksize\t\t= ARIA_BLOCK_SIZE,\n\t\t.setkey\t\t\t= aria_avx2_set_key,\n\t\t.encrypt\t\t= aria_avx2_ctr_encrypt,\n\t\t.decrypt\t\t= aria_avx2_ctr_encrypt,\n\t\t.init                   = aria_avx2_init_tfm,\n\t}\n};\n\nstatic struct simd_skcipher_alg *aria_simd_algs[ARRAY_SIZE(aria_algs)];\n\nstatic int __init aria_avx2_init(void)\n{\n\tconst char *feature_name;\n\n\tif (!boot_cpu_has(X86_FEATURE_AVX) ||\n\t    !boot_cpu_has(X86_FEATURE_AVX2) ||\n\t    !boot_cpu_has(X86_FEATURE_AES) ||\n\t    !boot_cpu_has(X86_FEATURE_OSXSAVE)) {\n\t\tpr_info(\"AVX2 or AES-NI instructions are not detected.\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (!cpu_has_xfeatures(XFEATURE_MASK_SSE | XFEATURE_MASK_YMM,\n\t\t\t\t&feature_name)) {\n\t\tpr_info(\"CPU feature '%s' is not supported.\\n\", feature_name);\n\t\treturn -ENODEV;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_GFNI) && IS_ENABLED(CONFIG_AS_GFNI)) {\n\t\taria_ops.aria_encrypt_16way = aria_aesni_avx_gfni_encrypt_16way;\n\t\taria_ops.aria_decrypt_16way = aria_aesni_avx_gfni_decrypt_16way;\n\t\taria_ops.aria_ctr_crypt_16way = aria_aesni_avx_gfni_ctr_crypt_16way;\n\t\taria_ops.aria_encrypt_32way = aria_aesni_avx2_gfni_encrypt_32way;\n\t\taria_ops.aria_decrypt_32way = aria_aesni_avx2_gfni_decrypt_32way;\n\t\taria_ops.aria_ctr_crypt_32way = aria_aesni_avx2_gfni_ctr_crypt_32way;\n\t} else {\n\t\taria_ops.aria_encrypt_16way = aria_aesni_avx_encrypt_16way;\n\t\taria_ops.aria_decrypt_16way = aria_aesni_avx_decrypt_16way;\n\t\taria_ops.aria_ctr_crypt_16way = aria_aesni_avx_ctr_crypt_16way;\n\t\taria_ops.aria_encrypt_32way = aria_aesni_avx2_encrypt_32way;\n\t\taria_ops.aria_decrypt_32way = aria_aesni_avx2_decrypt_32way;\n\t\taria_ops.aria_ctr_crypt_32way = aria_aesni_avx2_ctr_crypt_32way;\n\t}\n\n\treturn simd_register_skciphers_compat(aria_algs,\n\t\t\t\t\t      ARRAY_SIZE(aria_algs),\n\t\t\t\t\t      aria_simd_algs);\n}\n\nstatic void __exit aria_avx2_exit(void)\n{\n\tsimd_unregister_skciphers(aria_algs, ARRAY_SIZE(aria_algs),\n\t\t\t\t  aria_simd_algs);\n}\n\nmodule_init(aria_avx2_init);\nmodule_exit(aria_avx2_exit);\n\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Taehee Yoo <ap420073@gmail.com>\");\nMODULE_DESCRIPTION(\"ARIA Cipher Algorithm, AVX2/AES-NI/GFNI optimized\");\nMODULE_ALIAS_CRYPTO(\"aria\");\nMODULE_ALIAS_CRYPTO(\"aria-aesni-avx2\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}