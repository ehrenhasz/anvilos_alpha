{
  "module_name": "aesni-intel_asm.S",
  "hash_id": "96e337aea25238f421887884d4b839a3941efd5bb6c3002f818923eb9511f29f",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/crypto/aesni-intel_asm.S",
  "human_readable_source": " \n \n\n#include <linux/linkage.h>\n#include <asm/frame.h>\n#include <asm/nospec-branch.h>\n\n \n#define MOVADQ\tmovaps\n#define MOVUDQ\tmovups\n\n#ifdef __x86_64__\n\n# constants in mergeable sections, linker can reorder and merge\n.section\t.rodata.cst16.POLY, \"aM\", @progbits, 16\n.align 16\nPOLY:   .octa 0xC2000000000000000000000000000001\n.section\t.rodata.cst16.TWOONE, \"aM\", @progbits, 16\n.align 16\nTWOONE: .octa 0x00000001000000000000000000000001\n\n.section\t.rodata.cst16.SHUF_MASK, \"aM\", @progbits, 16\n.align 16\nSHUF_MASK:  .octa 0x000102030405060708090A0B0C0D0E0F\n.section\t.rodata.cst16.MASK1, \"aM\", @progbits, 16\n.align 16\nMASK1:      .octa 0x0000000000000000ffffffffffffffff\n.section\t.rodata.cst16.MASK2, \"aM\", @progbits, 16\n.align 16\nMASK2:      .octa 0xffffffffffffffff0000000000000000\n.section\t.rodata.cst16.ONE, \"aM\", @progbits, 16\n.align 16\nONE:        .octa 0x00000000000000000000000000000001\n.section\t.rodata.cst16.F_MIN_MASK, \"aM\", @progbits, 16\n.align 16\nF_MIN_MASK: .octa 0xf1f2f3f4f5f6f7f8f9fafbfcfdfeff0\n.section\t.rodata.cst16.dec, \"aM\", @progbits, 16\n.align 16\ndec:        .octa 0x1\n.section\t.rodata.cst16.enc, \"aM\", @progbits, 16\n.align 16\nenc:        .octa 0x2\n\n# order of these constants should not change.\n# more specifically, ALL_F should follow SHIFT_MASK,\n# and zero should follow ALL_F\n.section\t.rodata, \"a\", @progbits\n.align 16\nSHIFT_MASK: .octa 0x0f0e0d0c0b0a09080706050403020100\nALL_F:      .octa 0xffffffffffffffffffffffffffffffff\n            .octa 0x00000000000000000000000000000000\n\n.text\n\n\n#define\tSTACK_OFFSET    8*3\n\n#define AadHash 16*0\n#define AadLen 16*1\n#define InLen (16*1)+8\n#define PBlockEncKey 16*2\n#define OrigIV 16*3\n#define CurCount 16*4\n#define PBlockLen 16*5\n#define\tHashKey\t\t16*6\t \n#define\tHashKey_2\t16*7\t \n#define\tHashKey_3\t16*8\t \n#define\tHashKey_4\t16*9\t \n#define\tHashKey_k\t16*10\t \n\t\t\t\t \n\t\t\t\t \n#define\tHashKey_2_k\t16*11\t \n\t\t\t\t \n\t\t\t\t \n#define\tHashKey_3_k\t16*12\t \n\t\t\t\t \n\t\t\t\t \n#define\tHashKey_4_k\t16*13\t \n\t\t\t\t \n\t\t\t\t \n\n#define arg1 rdi\n#define arg2 rsi\n#define arg3 rdx\n#define arg4 rcx\n#define arg5 r8\n#define arg6 r9\n#define arg7 STACK_OFFSET+8(%rsp)\n#define arg8 STACK_OFFSET+16(%rsp)\n#define arg9 STACK_OFFSET+24(%rsp)\n#define arg10 STACK_OFFSET+32(%rsp)\n#define arg11 STACK_OFFSET+40(%rsp)\n#define keysize 2*15*16(%arg1)\n#endif\n\n\n#define STATE1\t%xmm0\n#define STATE2\t%xmm4\n#define STATE3\t%xmm5\n#define STATE4\t%xmm6\n#define STATE\tSTATE1\n#define IN1\t%xmm1\n#define IN2\t%xmm7\n#define IN3\t%xmm8\n#define IN4\t%xmm9\n#define IN\tIN1\n#define KEY\t%xmm2\n#define IV\t%xmm3\n\n#define BSWAP_MASK %xmm10\n#define CTR\t%xmm11\n#define INC\t%xmm12\n\n#define GF128MUL_MASK %xmm7\n\n#ifdef __x86_64__\n#define AREG\t%rax\n#define KEYP\t%rdi\n#define OUTP\t%rsi\n#define UKEYP\tOUTP\n#define INP\t%rdx\n#define LEN\t%rcx\n#define IVP\t%r8\n#define KLEN\t%r9d\n#define T1\t%r10\n#define TKEYP\tT1\n#define T2\t%r11\n#define TCTR_LOW T2\n#else\n#define AREG\t%eax\n#define KEYP\t%edi\n#define OUTP\tAREG\n#define UKEYP\tOUTP\n#define INP\t%edx\n#define LEN\t%esi\n#define IVP\t%ebp\n#define KLEN\t%ebx\n#define T1\t%ecx\n#define TKEYP\tT1\n#endif\n\n.macro FUNC_SAVE\n\tpush\t%r12\n\tpush\t%r13\n\tpush\t%r14\n#\n# states of %xmm registers %xmm6:%xmm15 not saved\n# all %xmm registers are clobbered\n#\n.endm\n\n\n.macro FUNC_RESTORE\n\tpop\t%r14\n\tpop\t%r13\n\tpop\t%r12\n.endm\n\n# Precompute hashkeys.\n# Input: Hash subkey.\n# Output: HashKeys stored in gcm_context_data.  Only needs to be called\n# once per key.\n# clobbers r12, and tmp xmm registers.\n.macro PRECOMPUTE SUBKEY TMP1 TMP2 TMP3 TMP4 TMP5 TMP6 TMP7\n\tmov\t\\SUBKEY, %r12\n\tmovdqu\t(%r12), \\TMP3\n\tmovdqa\tSHUF_MASK(%rip), \\TMP2\n\tpshufb\t\\TMP2, \\TMP3\n\n\t# precompute HashKey<<1 mod poly from the HashKey (required for GHASH)\n\n\tmovdqa\t\\TMP3, \\TMP2\n\tpsllq\t$1, \\TMP3\n\tpsrlq\t$63, \\TMP2\n\tmovdqa\t\\TMP2, \\TMP1\n\tpslldq\t$8, \\TMP2\n\tpsrldq\t$8, \\TMP1\n\tpor\t\\TMP2, \\TMP3\n\n\t# reduce HashKey<<1\n\n\tpshufd\t$0x24, \\TMP1, \\TMP2\n\tpcmpeqd TWOONE(%rip), \\TMP2\n\tpand\tPOLY(%rip), \\TMP2\n\tpxor\t\\TMP2, \\TMP3\n\tmovdqu\t\\TMP3, HashKey(%arg2)\n\n\tmovdqa\t   \\TMP3, \\TMP5\n\tpshufd\t   $78, \\TMP3, \\TMP1\n\tpxor\t   \\TMP3, \\TMP1\n\tmovdqu\t   \\TMP1, HashKey_k(%arg2)\n\n\tGHASH_MUL  \\TMP5, \\TMP3, \\TMP1, \\TMP2, \\TMP4, \\TMP6, \\TMP7\n# TMP5 = HashKey^2<<1 (mod poly)\n\tmovdqu\t   \\TMP5, HashKey_2(%arg2)\n# HashKey_2 = HashKey^2<<1 (mod poly)\n\tpshufd\t   $78, \\TMP5, \\TMP1\n\tpxor\t   \\TMP5, \\TMP1\n\tmovdqu\t   \\TMP1, HashKey_2_k(%arg2)\n\n\tGHASH_MUL  \\TMP5, \\TMP3, \\TMP1, \\TMP2, \\TMP4, \\TMP6, \\TMP7\n# TMP5 = HashKey^3<<1 (mod poly)\n\tmovdqu\t   \\TMP5, HashKey_3(%arg2)\n\tpshufd\t   $78, \\TMP5, \\TMP1\n\tpxor\t   \\TMP5, \\TMP1\n\tmovdqu\t   \\TMP1, HashKey_3_k(%arg2)\n\n\tGHASH_MUL  \\TMP5, \\TMP3, \\TMP1, \\TMP2, \\TMP4, \\TMP6, \\TMP7\n# TMP5 = HashKey^3<<1 (mod poly)\n\tmovdqu\t   \\TMP5, HashKey_4(%arg2)\n\tpshufd\t   $78, \\TMP5, \\TMP1\n\tpxor\t   \\TMP5, \\TMP1\n\tmovdqu\t   \\TMP1, HashKey_4_k(%arg2)\n.endm\n\n# GCM_INIT initializes a gcm_context struct to prepare for encoding/decoding.\n# Clobbers rax, r10-r13 and xmm0-xmm6, %xmm13\n.macro GCM_INIT Iv SUBKEY AAD AADLEN\n\tmov \\AADLEN, %r11\n\tmov %r11, AadLen(%arg2) # ctx_data.aad_length = aad_length\n\txor %r11d, %r11d\n\tmov %r11, InLen(%arg2) # ctx_data.in_length = 0\n\tmov %r11, PBlockLen(%arg2) # ctx_data.partial_block_length = 0\n\tmov %r11, PBlockEncKey(%arg2) # ctx_data.partial_block_enc_key = 0\n\tmov \\Iv, %rax\n\tmovdqu (%rax), %xmm0\n\tmovdqu %xmm0, OrigIV(%arg2) # ctx_data.orig_IV = iv\n\n\tmovdqa  SHUF_MASK(%rip), %xmm2\n\tpshufb %xmm2, %xmm0\n\tmovdqu %xmm0, CurCount(%arg2) # ctx_data.current_counter = iv\n\n\tPRECOMPUTE \\SUBKEY, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7\n\tmovdqu HashKey(%arg2), %xmm13\n\n\tCALC_AAD_HASH %xmm13, \\AAD, \\AADLEN, %xmm0, %xmm1, %xmm2, %xmm3, \\\n\t%xmm4, %xmm5, %xmm6\n.endm\n\n# GCM_ENC_DEC Encodes/Decodes given data. Assumes that the passed gcm_context\n# struct has been initialized by GCM_INIT.\n# Requires the input data be at least 1 byte long because of READ_PARTIAL_BLOCK\n# Clobbers rax, r10-r13, and xmm0-xmm15\n.macro GCM_ENC_DEC operation\n\tmovdqu AadHash(%arg2), %xmm8\n\tmovdqu HashKey(%arg2), %xmm13\n\tadd %arg5, InLen(%arg2)\n\n\txor %r11d, %r11d # initialise the data pointer offset as zero\n\tPARTIAL_BLOCK %arg3 %arg4 %arg5 %r11 %xmm8 \\operation\n\n\tsub %r11, %arg5\t\t# sub partial block data used\n\tmov %arg5, %r13\t\t# save the number of bytes\n\n\tand $-16, %r13\t\t# %r13 = %r13 - (%r13 mod 16)\n\tmov %r13, %r12\n\t# Encrypt/Decrypt first few blocks\n\n\tand\t$(3<<4), %r12\n\tjz\t.L_initial_num_blocks_is_0_\\@\n\tcmp\t$(2<<4), %r12\n\tjb\t.L_initial_num_blocks_is_1_\\@\n\tje\t.L_initial_num_blocks_is_2_\\@\n.L_initial_num_blocks_is_3_\\@:\n\tINITIAL_BLOCKS_ENC_DEC\t%xmm9, %xmm10, %xmm13, %xmm11, %xmm12, %xmm0, \\\n%xmm1, %xmm2, %xmm3, %xmm4, %xmm8, %xmm5, %xmm6, 5, 678, \\operation\n\tsub\t$48, %r13\n\tjmp\t.L_initial_blocks_\\@\n.L_initial_num_blocks_is_2_\\@:\n\tINITIAL_BLOCKS_ENC_DEC\t%xmm9, %xmm10, %xmm13, %xmm11, %xmm12, %xmm0, \\\n%xmm1, %xmm2, %xmm3, %xmm4, %xmm8, %xmm5, %xmm6, 6, 78, \\operation\n\tsub\t$32, %r13\n\tjmp\t.L_initial_blocks_\\@\n.L_initial_num_blocks_is_1_\\@:\n\tINITIAL_BLOCKS_ENC_DEC\t%xmm9, %xmm10, %xmm13, %xmm11, %xmm12, %xmm0, \\\n%xmm1, %xmm2, %xmm3, %xmm4, %xmm8, %xmm5, %xmm6, 7, 8, \\operation\n\tsub\t$16, %r13\n\tjmp\t.L_initial_blocks_\\@\n.L_initial_num_blocks_is_0_\\@:\n\tINITIAL_BLOCKS_ENC_DEC\t%xmm9, %xmm10, %xmm13, %xmm11, %xmm12, %xmm0, \\\n%xmm1, %xmm2, %xmm3, %xmm4, %xmm8, %xmm5, %xmm6, 8, 0, \\operation\n.L_initial_blocks_\\@:\n\n\t# Main loop - Encrypt/Decrypt remaining blocks\n\n\ttest\t%r13, %r13\n\tje\t.L_zero_cipher_left_\\@\n\tsub\t$64, %r13\n\tje\t.L_four_cipher_left_\\@\n.L_crypt_by_4_\\@:\n\tGHASH_4_ENCRYPT_4_PARALLEL_\\operation\t%xmm9, %xmm10, %xmm11, %xmm12, \\\n\t%xmm13, %xmm14, %xmm0, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, \\\n\t%xmm7, %xmm8, enc\n\tadd\t$64, %r11\n\tsub\t$64, %r13\n\tjne\t.L_crypt_by_4_\\@\n.L_four_cipher_left_\\@:\n\tGHASH_LAST_4\t%xmm9, %xmm10, %xmm11, %xmm12, %xmm13, %xmm14, \\\n%xmm15, %xmm1, %xmm2, %xmm3, %xmm4, %xmm8\n.L_zero_cipher_left_\\@:\n\tmovdqu %xmm8, AadHash(%arg2)\n\tmovdqu %xmm0, CurCount(%arg2)\n\n\tmov\t%arg5, %r13\n\tand\t$15, %r13\t\t\t# %r13 = arg5 (mod 16)\n\tje\t.L_multiple_of_16_bytes_\\@\n\n\tmov %r13, PBlockLen(%arg2)\n\n\t# Handle the last <16 Byte block separately\n\tpaddd ONE(%rip), %xmm0                # INCR CNT to get Yn\n\tmovdqu %xmm0, CurCount(%arg2)\n\tmovdqa SHUF_MASK(%rip), %xmm10\n\tpshufb %xmm10, %xmm0\n\n\tENCRYPT_SINGLE_BLOCK\t%xmm0, %xmm1        # Encrypt(K, Yn)\n\tmovdqu %xmm0, PBlockEncKey(%arg2)\n\n\tcmp\t$16, %arg5\n\tjge\t.L_large_enough_update_\\@\n\n\tlea (%arg4,%r11,1), %r10\n\tmov %r13, %r12\n\tREAD_PARTIAL_BLOCK %r10 %r12 %xmm2 %xmm1\n\tjmp\t.L_data_read_\\@\n\n.L_large_enough_update_\\@:\n\tsub\t$16, %r11\n\tadd\t%r13, %r11\n\n\t# receive the last <16 Byte block\n\tmovdqu\t(%arg4, %r11, 1), %xmm1\n\n\tsub\t%r13, %r11\n\tadd\t$16, %r11\n\n\tlea\tSHIFT_MASK+16(%rip), %r12\n\t# adjust the shuffle mask pointer to be able to shift 16-r13 bytes\n\t# (r13 is the number of bytes in plaintext mod 16)\n\tsub\t%r13, %r12\n\t# get the appropriate shuffle mask\n\tmovdqu\t(%r12), %xmm2\n\t# shift right 16-r13 bytes\n\tpshufb  %xmm2, %xmm1\n\n.L_data_read_\\@:\n\tlea ALL_F+16(%rip), %r12\n\tsub %r13, %r12\n\n.ifc \\operation, dec\n\tmovdqa  %xmm1, %xmm2\n.endif\n\tpxor\t%xmm1, %xmm0            # XOR Encrypt(K, Yn)\n\tmovdqu\t(%r12), %xmm1\n\t# get the appropriate mask to mask out top 16-r13 bytes of xmm0\n\tpand\t%xmm1, %xmm0            # mask out top 16-r13 bytes of xmm0\n.ifc \\operation, dec\n\tpand    %xmm1, %xmm2\n\tmovdqa SHUF_MASK(%rip), %xmm10\n\tpshufb %xmm10 ,%xmm2\n\n\tpxor %xmm2, %xmm8\n.else\n\tmovdqa SHUF_MASK(%rip), %xmm10\n\tpshufb %xmm10,%xmm0\n\n\tpxor\t%xmm0, %xmm8\n.endif\n\n\tmovdqu %xmm8, AadHash(%arg2)\n.ifc \\operation, enc\n\t# GHASH computation for the last <16 byte block\n\tmovdqa SHUF_MASK(%rip), %xmm10\n\t# shuffle xmm0 back to output as ciphertext\n\tpshufb %xmm10, %xmm0\n.endif\n\n\t# Output %r13 bytes\n\tmovq %xmm0, %rax\n\tcmp $8, %r13\n\tjle .L_less_than_8_bytes_left_\\@\n\tmov %rax, (%arg3 , %r11, 1)\n\tadd $8, %r11\n\tpsrldq $8, %xmm0\n\tmovq %xmm0, %rax\n\tsub $8, %r13\n.L_less_than_8_bytes_left_\\@:\n\tmov %al,  (%arg3, %r11, 1)\n\tadd $1, %r11\n\tshr $8, %rax\n\tsub $1, %r13\n\tjne .L_less_than_8_bytes_left_\\@\n.L_multiple_of_16_bytes_\\@:\n.endm\n\n# GCM_COMPLETE Finishes update of tag of last partial block\n# Output: Authorization Tag (AUTH_TAG)\n# Clobbers rax, r10-r12, and xmm0, xmm1, xmm5-xmm15\n.macro GCM_COMPLETE AUTHTAG AUTHTAGLEN\n\tmovdqu AadHash(%arg2), %xmm8\n\tmovdqu HashKey(%arg2), %xmm13\n\n\tmov PBlockLen(%arg2), %r12\n\n\ttest %r12, %r12\n\tje .L_partial_done\\@\n\n\tGHASH_MUL %xmm8, %xmm13, %xmm9, %xmm10, %xmm11, %xmm5, %xmm6\n\n.L_partial_done\\@:\n\tmov AadLen(%arg2), %r12  # %r13 = aadLen (number of bytes)\n\tshl\t$3, %r12\t\t  # convert into number of bits\n\tmovd\t%r12d, %xmm15\t\t  # len(A) in %xmm15\n\tmov InLen(%arg2), %r12\n\tshl     $3, %r12                  # len(C) in bits (*128)\n\tmovq    %r12, %xmm1\n\n\tpslldq\t$8, %xmm15\t\t  # %xmm15 = len(A)||0x0000000000000000\n\tpxor\t%xmm1, %xmm15\t\t  # %xmm15 = len(A)||len(C)\n\tpxor\t%xmm15, %xmm8\n\tGHASH_MUL\t%xmm8, %xmm13, %xmm9, %xmm10, %xmm11, %xmm5, %xmm6\n\t# final GHASH computation\n\tmovdqa SHUF_MASK(%rip), %xmm10\n\tpshufb %xmm10, %xmm8\n\n\tmovdqu OrigIV(%arg2), %xmm0       # %xmm0 = Y0\n\tENCRYPT_SINGLE_BLOCK\t%xmm0,  %xmm1\t  # E(K, Y0)\n\tpxor\t%xmm8, %xmm0\n.L_return_T_\\@:\n\tmov\t\\AUTHTAG, %r10                     # %r10 = authTag\n\tmov\t\\AUTHTAGLEN, %r11                    # %r11 = auth_tag_len\n\tcmp\t$16, %r11\n\tje\t.L_T_16_\\@\n\tcmp\t$8, %r11\n\tjl\t.L_T_4_\\@\n.L_T_8_\\@:\n\tmovq\t%xmm0, %rax\n\tmov\t%rax, (%r10)\n\tadd\t$8, %r10\n\tsub\t$8, %r11\n\tpsrldq\t$8, %xmm0\n\ttest\t%r11, %r11\n\tje\t.L_return_T_done_\\@\n.L_T_4_\\@:\n\tmovd\t%xmm0, %eax\n\tmov\t%eax, (%r10)\n\tadd\t$4, %r10\n\tsub\t$4, %r11\n\tpsrldq\t$4, %xmm0\n\ttest\t%r11, %r11\n\tje\t.L_return_T_done_\\@\n.L_T_123_\\@:\n\tmovd\t%xmm0, %eax\n\tcmp\t$2, %r11\n\tjl\t.L_T_1_\\@\n\tmov\t%ax, (%r10)\n\tcmp\t$2, %r11\n\tje\t.L_return_T_done_\\@\n\tadd\t$2, %r10\n\tsar\t$16, %eax\n.L_T_1_\\@:\n\tmov\t%al, (%r10)\n\tjmp\t.L_return_T_done_\\@\n.L_T_16_\\@:\n\tmovdqu\t%xmm0, (%r10)\n.L_return_T_done_\\@:\n.endm\n\n#ifdef __x86_64__\n \n.macro GHASH_MUL GH HK TMP1 TMP2 TMP3 TMP4 TMP5\n\tmovdqa\t  \\GH, \\TMP1\n\tpshufd\t  $78, \\GH, \\TMP2\n\tpshufd\t  $78, \\HK, \\TMP3\n\tpxor\t  \\GH, \\TMP2            # TMP2 = a1+a0\n\tpxor\t  \\HK, \\TMP3            # TMP3 = b1+b0\n\tpclmulqdq $0x11, \\HK, \\TMP1     # TMP1 = a1*b1\n\tpclmulqdq $0x00, \\HK, \\GH       # GH = a0*b0\n\tpclmulqdq $0x00, \\TMP3, \\TMP2   # TMP2 = (a0+a1)*(b1+b0)\n\tpxor\t  \\GH, \\TMP2\n\tpxor\t  \\TMP1, \\TMP2          # TMP2 = (a0*b0)+(a1*b0)\n\tmovdqa\t  \\TMP2, \\TMP3\n\tpslldq\t  $8, \\TMP3             # left shift TMP3 2 DWs\n\tpsrldq\t  $8, \\TMP2             # right shift TMP2 2 DWs\n\tpxor\t  \\TMP3, \\GH\n\tpxor\t  \\TMP2, \\TMP1          # TMP2:GH holds the result of GH*HK\n\n        # first phase of the reduction\n\n\tmovdqa    \\GH, \\TMP2\n\tmovdqa    \\GH, \\TMP3\n\tmovdqa    \\GH, \\TMP4            # copy GH into TMP2,TMP3 and TMP4\n\t\t\t\t\t# in in order to perform\n\t\t\t\t\t# independent shifts\n\tpslld     $31, \\TMP2            # packed right shift <<31\n\tpslld     $30, \\TMP3            # packed right shift <<30\n\tpslld     $25, \\TMP4            # packed right shift <<25\n\tpxor      \\TMP3, \\TMP2          # xor the shifted versions\n\tpxor      \\TMP4, \\TMP2\n\tmovdqa    \\TMP2, \\TMP5\n\tpsrldq    $4, \\TMP5             # right shift TMP5 1 DW\n\tpslldq    $12, \\TMP2            # left shift TMP2 3 DWs\n\tpxor      \\TMP2, \\GH\n\n        # second phase of the reduction\n\n\tmovdqa    \\GH,\\TMP2             # copy GH into TMP2,TMP3 and TMP4\n\t\t\t\t\t# in in order to perform\n\t\t\t\t\t# independent shifts\n\tmovdqa    \\GH,\\TMP3\n\tmovdqa    \\GH,\\TMP4\n\tpsrld     $1,\\TMP2              # packed left shift >>1\n\tpsrld     $2,\\TMP3              # packed left shift >>2\n\tpsrld     $7,\\TMP4              # packed left shift >>7\n\tpxor      \\TMP3,\\TMP2\t\t# xor the shifted versions\n\tpxor      \\TMP4,\\TMP2\n\tpxor      \\TMP5, \\TMP2\n\tpxor      \\TMP2, \\GH\n\tpxor      \\TMP1, \\GH            # result is in TMP1\n.endm\n\n# Reads DLEN bytes starting at DPTR and stores in XMMDst\n# where 0 < DLEN < 16\n# Clobbers %rax, DLEN and XMM1\n.macro READ_PARTIAL_BLOCK DPTR DLEN XMM1 XMMDst\n        cmp $8, \\DLEN\n        jl .L_read_lt8_\\@\n        mov (\\DPTR), %rax\n        movq %rax, \\XMMDst\n        sub $8, \\DLEN\n        jz .L_done_read_partial_block_\\@\n\txor %eax, %eax\n.L_read_next_byte_\\@:\n        shl $8, %rax\n        mov 7(\\DPTR, \\DLEN, 1), %al\n        dec \\DLEN\n        jnz .L_read_next_byte_\\@\n        movq %rax, \\XMM1\n\tpslldq $8, \\XMM1\n        por \\XMM1, \\XMMDst\n\tjmp .L_done_read_partial_block_\\@\n.L_read_lt8_\\@:\n\txor %eax, %eax\n.L_read_next_byte_lt8_\\@:\n        shl $8, %rax\n        mov -1(\\DPTR, \\DLEN, 1), %al\n        dec \\DLEN\n        jnz .L_read_next_byte_lt8_\\@\n        movq %rax, \\XMMDst\n.L_done_read_partial_block_\\@:\n.endm\n\n# CALC_AAD_HASH: Calculates the hash of the data which will not be encrypted.\n# clobbers r10-11, xmm14\n.macro CALC_AAD_HASH HASHKEY AAD AADLEN TMP1 TMP2 TMP3 TMP4 TMP5 \\\n\tTMP6 TMP7\n\tMOVADQ\t   SHUF_MASK(%rip), %xmm14\n\tmov\t   \\AAD, %r10\t\t# %r10 = AAD\n\tmov\t   \\AADLEN, %r11\t\t# %r11 = aadLen\n\tpxor\t   \\TMP7, \\TMP7\n\tpxor\t   \\TMP6, \\TMP6\n\n\tcmp\t   $16, %r11\n\tjl\t   .L_get_AAD_rest\\@\n.L_get_AAD_blocks\\@:\n\tmovdqu\t   (%r10), \\TMP7\n\tpshufb\t   %xmm14, \\TMP7 # byte-reflect the AAD data\n\tpxor\t   \\TMP7, \\TMP6\n\tGHASH_MUL  \\TMP6, \\HASHKEY, \\TMP1, \\TMP2, \\TMP3, \\TMP4, \\TMP5\n\tadd\t   $16, %r10\n\tsub\t   $16, %r11\n\tcmp\t   $16, %r11\n\tjge\t   .L_get_AAD_blocks\\@\n\n\tmovdqu\t   \\TMP6, \\TMP7\n\n\t \n.L_get_AAD_rest\\@:\n\ttest\t   %r11, %r11\n\tje\t   .L_get_AAD_done\\@\n\n\tREAD_PARTIAL_BLOCK %r10, %r11, \\TMP1, \\TMP7\n\tpshufb\t   %xmm14, \\TMP7 # byte-reflect the AAD data\n\tpxor\t   \\TMP6, \\TMP7\n\tGHASH_MUL  \\TMP7, \\HASHKEY, \\TMP1, \\TMP2, \\TMP3, \\TMP4, \\TMP5\n\tmovdqu \\TMP7, \\TMP6\n\n.L_get_AAD_done\\@:\n\tmovdqu \\TMP6, AadHash(%arg2)\n.endm\n\n# PARTIAL_BLOCK: Handles encryption/decryption and the tag partial blocks\n# between update calls.\n# Requires the input data be at least 1 byte long due to READ_PARTIAL_BLOCK\n# Outputs encrypted bytes, and updates hash and partial info in gcm_data_context\n# Clobbers rax, r10, r12, r13, xmm0-6, xmm9-13\n.macro PARTIAL_BLOCK CYPH_PLAIN_OUT PLAIN_CYPH_IN PLAIN_CYPH_LEN DATA_OFFSET \\\n\tAAD_HASH operation\n\tmov \tPBlockLen(%arg2), %r13\n\ttest\t%r13, %r13\n\tje\t.L_partial_block_done_\\@\t# Leave Macro if no partial blocks\n\t# Read in input data without over reading\n\tcmp\t$16, \\PLAIN_CYPH_LEN\n\tjl\t.L_fewer_than_16_bytes_\\@\n\tmovups\t(\\PLAIN_CYPH_IN), %xmm1\t# If more than 16 bytes, just fill xmm\n\tjmp\t.L_data_read_\\@\n\n.L_fewer_than_16_bytes_\\@:\n\tlea\t(\\PLAIN_CYPH_IN, \\DATA_OFFSET, 1), %r10\n\tmov\t\\PLAIN_CYPH_LEN, %r12\n\tREAD_PARTIAL_BLOCK %r10 %r12 %xmm0 %xmm1\n\n\tmov PBlockLen(%arg2), %r13\n\n.L_data_read_\\@:\t\t\t\t# Finished reading in data\n\n\tmovdqu\tPBlockEncKey(%arg2), %xmm9\n\tmovdqu\tHashKey(%arg2), %xmm13\n\n\tlea\tSHIFT_MASK(%rip), %r12\n\n\t# adjust the shuffle mask pointer to be able to shift r13 bytes\n\t# r16-r13 is the number of bytes in plaintext mod 16)\n\tadd\t%r13, %r12\n\tmovdqu\t(%r12), %xmm2\t\t# get the appropriate shuffle mask\n\tpshufb\t%xmm2, %xmm9\t\t# shift right r13 bytes\n\n.ifc \\operation, dec\n\tmovdqa\t%xmm1, %xmm3\n\tpxor\t%xmm1, %xmm9\t\t# Cyphertext XOR E(K, Yn)\n\n\tmov\t\\PLAIN_CYPH_LEN, %r10\n\tadd\t%r13, %r10\n\t# Set r10 to be the amount of data left in CYPH_PLAIN_IN after filling\n\tsub\t$16, %r10\n\t# Determine if if partial block is not being filled and\n\t# shift mask accordingly\n\tjge\t.L_no_extra_mask_1_\\@\n\tsub\t%r10, %r12\n.L_no_extra_mask_1_\\@:\n\n\tmovdqu\tALL_F-SHIFT_MASK(%r12), %xmm1\n\t# get the appropriate mask to mask out bottom r13 bytes of xmm9\n\tpand\t%xmm1, %xmm9\t\t# mask out bottom r13 bytes of xmm9\n\n\tpand\t%xmm1, %xmm3\n\tmovdqa\tSHUF_MASK(%rip), %xmm10\n\tpshufb\t%xmm10, %xmm3\n\tpshufb\t%xmm2, %xmm3\n\tpxor\t%xmm3, \\AAD_HASH\n\n\ttest\t%r10, %r10\n\tjl\t.L_partial_incomplete_1_\\@\n\n\t# GHASH computation for the last <16 Byte block\n\tGHASH_MUL \\AAD_HASH, %xmm13, %xmm0, %xmm10, %xmm11, %xmm5, %xmm6\n\txor\t%eax, %eax\n\n\tmov\t%rax, PBlockLen(%arg2)\n\tjmp\t.L_dec_done_\\@\n.L_partial_incomplete_1_\\@:\n\tadd\t\\PLAIN_CYPH_LEN, PBlockLen(%arg2)\n.L_dec_done_\\@:\n\tmovdqu\t\\AAD_HASH, AadHash(%arg2)\n.else\n\tpxor\t%xmm1, %xmm9\t\t\t# Plaintext XOR E(K, Yn)\n\n\tmov\t\\PLAIN_CYPH_LEN, %r10\n\tadd\t%r13, %r10\n\t# Set r10 to be the amount of data left in CYPH_PLAIN_IN after filling\n\tsub\t$16, %r10\n\t# Determine if if partial block is not being filled and\n\t# shift mask accordingly\n\tjge\t.L_no_extra_mask_2_\\@\n\tsub\t%r10, %r12\n.L_no_extra_mask_2_\\@:\n\n\tmovdqu\tALL_F-SHIFT_MASK(%r12), %xmm1\n\t# get the appropriate mask to mask out bottom r13 bytes of xmm9\n\tpand\t%xmm1, %xmm9\n\n\tmovdqa\tSHUF_MASK(%rip), %xmm1\n\tpshufb\t%xmm1, %xmm9\n\tpshufb\t%xmm2, %xmm9\n\tpxor\t%xmm9, \\AAD_HASH\n\n\ttest\t%r10, %r10\n\tjl\t.L_partial_incomplete_2_\\@\n\n\t# GHASH computation for the last <16 Byte block\n\tGHASH_MUL \\AAD_HASH, %xmm13, %xmm0, %xmm10, %xmm11, %xmm5, %xmm6\n\txor\t%eax, %eax\n\n\tmov\t%rax, PBlockLen(%arg2)\n\tjmp\t.L_encode_done_\\@\n.L_partial_incomplete_2_\\@:\n\tadd\t\\PLAIN_CYPH_LEN, PBlockLen(%arg2)\n.L_encode_done_\\@:\n\tmovdqu\t\\AAD_HASH, AadHash(%arg2)\n\n\tmovdqa\tSHUF_MASK(%rip), %xmm10\n\t# shuffle xmm9 back to output as ciphertext\n\tpshufb\t%xmm10, %xmm9\n\tpshufb\t%xmm2, %xmm9\n.endif\n\t# output encrypted Bytes\n\ttest\t%r10, %r10\n\tjl\t.L_partial_fill_\\@\n\tmov\t%r13, %r12\n\tmov\t$16, %r13\n\t# Set r13 to be the number of bytes to write out\n\tsub\t%r12, %r13\n\tjmp\t.L_count_set_\\@\n.L_partial_fill_\\@:\n\tmov\t\\PLAIN_CYPH_LEN, %r13\n.L_count_set_\\@:\n\tmovdqa\t%xmm9, %xmm0\n\tmovq\t%xmm0, %rax\n\tcmp\t$8, %r13\n\tjle\t.L_less_than_8_bytes_left_\\@\n\n\tmov\t%rax, (\\CYPH_PLAIN_OUT, \\DATA_OFFSET, 1)\n\tadd\t$8, \\DATA_OFFSET\n\tpsrldq\t$8, %xmm0\n\tmovq\t%xmm0, %rax\n\tsub\t$8, %r13\n.L_less_than_8_bytes_left_\\@:\n\tmovb\t%al, (\\CYPH_PLAIN_OUT, \\DATA_OFFSET, 1)\n\tadd\t$1, \\DATA_OFFSET\n\tshr\t$8, %rax\n\tsub\t$1, %r13\n\tjne\t.L_less_than_8_bytes_left_\\@\n.L_partial_block_done_\\@:\n.endm # PARTIAL_BLOCK\n\n \n\n\n.macro INITIAL_BLOCKS_ENC_DEC TMP1 TMP2 TMP3 TMP4 TMP5 XMM0 XMM1 \\\n\tXMM2 XMM3 XMM4 XMMDst TMP6 TMP7 i i_seq operation\n\tMOVADQ\t\tSHUF_MASK(%rip), %xmm14\n\n\tmovdqu AadHash(%arg2), %xmm\\i\t\t    # XMM0 = Y0\n\n\t# start AES for num_initial_blocks blocks\n\n\tmovdqu CurCount(%arg2), \\XMM0                # XMM0 = Y0\n\n.if (\\i == 5) || (\\i == 6) || (\\i == 7)\n\n\tMOVADQ\t\tONE(%RIP),\\TMP1\n\tMOVADQ\t\t0(%arg1),\\TMP2\n.irpc index, \\i_seq\n\tpaddd\t\t\\TMP1, \\XMM0                 # INCR Y0\n.ifc \\operation, dec\n        movdqa     \\XMM0, %xmm\\index\n.else\n\tMOVADQ\t\t\\XMM0, %xmm\\index\n.endif\n\tpshufb\t%xmm14, %xmm\\index      # perform a 16 byte swap\n\tpxor\t\t\\TMP2, %xmm\\index\n.endr\n\tlea\t0x10(%arg1),%r10\n\tmov\tkeysize,%eax\n\tshr\t$2,%eax\t\t\t\t# 128->4, 192->6, 256->8\n\tadd\t$5,%eax\t\t\t      # 128->9, 192->11, 256->13\n\n.Laes_loop_initial_\\@:\n\tMOVADQ\t(%r10),\\TMP1\n.irpc\tindex, \\i_seq\n\taesenc\t\\TMP1, %xmm\\index\n.endr\n\tadd\t$16,%r10\n\tsub\t$1,%eax\n\tjnz\t.Laes_loop_initial_\\@\n\n\tMOVADQ\t(%r10), \\TMP1\n.irpc index, \\i_seq\n\taesenclast \\TMP1, %xmm\\index         # Last Round\n.endr\n.irpc index, \\i_seq\n\tmovdqu\t   (%arg4 , %r11, 1), \\TMP1\n\tpxor\t   \\TMP1, %xmm\\index\n\tmovdqu\t   %xmm\\index, (%arg3 , %r11, 1)\n\t# write back plaintext/ciphertext for num_initial_blocks\n\tadd\t   $16, %r11\n\n.ifc \\operation, dec\n\tmovdqa     \\TMP1, %xmm\\index\n.endif\n\tpshufb\t   %xmm14, %xmm\\index\n\n\t\t# prepare plaintext/ciphertext for GHASH computation\n.endr\n.endif\n\n        # apply GHASH on num_initial_blocks blocks\n\n.if \\i == 5\n        pxor       %xmm5, %xmm6\n\tGHASH_MUL  %xmm6, \\TMP3, \\TMP1, \\TMP2, \\TMP4, \\TMP5, \\XMM1\n        pxor       %xmm6, %xmm7\n\tGHASH_MUL  %xmm7, \\TMP3, \\TMP1, \\TMP2, \\TMP4, \\TMP5, \\XMM1\n        pxor       %xmm7, %xmm8\n\tGHASH_MUL  %xmm8, \\TMP3, \\TMP1, \\TMP2, \\TMP4, \\TMP5, \\XMM1\n.elseif \\i == 6\n        pxor       %xmm6, %xmm7\n\tGHASH_MUL  %xmm7, \\TMP3, \\TMP1, \\TMP2, \\TMP4, \\TMP5, \\XMM1\n        pxor       %xmm7, %xmm8\n\tGHASH_MUL  %xmm8, \\TMP3, \\TMP1, \\TMP2, \\TMP4, \\TMP5, \\XMM1\n.elseif \\i == 7\n        pxor       %xmm7, %xmm8\n\tGHASH_MUL  %xmm8, \\TMP3, \\TMP1, \\TMP2, \\TMP4, \\TMP5, \\XMM1\n.endif\n\tcmp\t   $64, %r13\n\tjl\t.L_initial_blocks_done\\@\n\t# no need for precomputed values\n \n\tMOVADQ\t   ONE(%RIP),\\TMP1\n\tpaddd\t   \\TMP1, \\XMM0              # INCR Y0\n\tMOVADQ\t   \\XMM0, \\XMM1\n\tpshufb  %xmm14, \\XMM1        # perform a 16 byte swap\n\n\tpaddd\t   \\TMP1, \\XMM0              # INCR Y0\n\tMOVADQ\t   \\XMM0, \\XMM2\n\tpshufb  %xmm14, \\XMM2        # perform a 16 byte swap\n\n\tpaddd\t   \\TMP1, \\XMM0              # INCR Y0\n\tMOVADQ\t   \\XMM0, \\XMM3\n\tpshufb %xmm14, \\XMM3        # perform a 16 byte swap\n\n\tpaddd\t   \\TMP1, \\XMM0              # INCR Y0\n\tMOVADQ\t   \\XMM0, \\XMM4\n\tpshufb %xmm14, \\XMM4        # perform a 16 byte swap\n\n\tMOVADQ\t   0(%arg1),\\TMP1\n\tpxor\t   \\TMP1, \\XMM1\n\tpxor\t   \\TMP1, \\XMM2\n\tpxor\t   \\TMP1, \\XMM3\n\tpxor\t   \\TMP1, \\XMM4\n.irpc index, 1234 # do 4 rounds\n\tmovaps 0x10*\\index(%arg1), \\TMP1\n\taesenc\t   \\TMP1, \\XMM1\n\taesenc\t   \\TMP1, \\XMM2\n\taesenc\t   \\TMP1, \\XMM3\n\taesenc\t   \\TMP1, \\XMM4\n.endr\n.irpc index, 56789 # do next 5 rounds\n\tmovaps 0x10*\\index(%arg1), \\TMP1\n\taesenc\t   \\TMP1, \\XMM1\n\taesenc\t   \\TMP1, \\XMM2\n\taesenc\t   \\TMP1, \\XMM3\n\taesenc\t   \\TMP1, \\XMM4\n.endr\n\tlea\t   0xa0(%arg1),%r10\n\tmov\t   keysize,%eax\n\tshr\t   $2,%eax\t\t\t# 128->4, 192->6, 256->8\n\tsub\t   $4,%eax\t\t\t# 128->0, 192->2, 256->4\n\tjz\t   .Laes_loop_pre_done\\@\n\n.Laes_loop_pre_\\@:\n\tMOVADQ\t   (%r10),\\TMP2\n.irpc\tindex, 1234\n\taesenc\t   \\TMP2, %xmm\\index\n.endr\n\tadd\t   $16,%r10\n\tsub\t   $1,%eax\n\tjnz\t   .Laes_loop_pre_\\@\n\n.Laes_loop_pre_done\\@:\n\tMOVADQ\t   (%r10), \\TMP2\n\taesenclast \\TMP2, \\XMM1\n\taesenclast \\TMP2, \\XMM2\n\taesenclast \\TMP2, \\XMM3\n\taesenclast \\TMP2, \\XMM4\n\tmovdqu\t   16*0(%arg4 , %r11 , 1), \\TMP1\n\tpxor\t   \\TMP1, \\XMM1\n.ifc \\operation, dec\n\tmovdqu     \\XMM1, 16*0(%arg3 , %r11 , 1)\n\tmovdqa     \\TMP1, \\XMM1\n.endif\n\tmovdqu\t   16*1(%arg4 , %r11 , 1), \\TMP1\n\tpxor\t   \\TMP1, \\XMM2\n.ifc \\operation, dec\n\tmovdqu     \\XMM2, 16*1(%arg3 , %r11 , 1)\n\tmovdqa     \\TMP1, \\XMM2\n.endif\n\tmovdqu\t   16*2(%arg4 , %r11 , 1), \\TMP1\n\tpxor\t   \\TMP1, \\XMM3\n.ifc \\operation, dec\n\tmovdqu     \\XMM3, 16*2(%arg3 , %r11 , 1)\n\tmovdqa     \\TMP1, \\XMM3\n.endif\n\tmovdqu\t   16*3(%arg4 , %r11 , 1), \\TMP1\n\tpxor\t   \\TMP1, \\XMM4\n.ifc \\operation, dec\n\tmovdqu     \\XMM4, 16*3(%arg3 , %r11 , 1)\n\tmovdqa     \\TMP1, \\XMM4\n.else\n\tmovdqu     \\XMM1, 16*0(%arg3 , %r11 , 1)\n\tmovdqu     \\XMM2, 16*1(%arg3 , %r11 , 1)\n\tmovdqu     \\XMM3, 16*2(%arg3 , %r11 , 1)\n\tmovdqu     \\XMM4, 16*3(%arg3 , %r11 , 1)\n.endif\n\n\tadd\t   $64, %r11\n\tpshufb %xmm14, \\XMM1 # perform a 16 byte swap\n\tpxor\t   \\XMMDst, \\XMM1\n# combine GHASHed value with the corresponding ciphertext\n\tpshufb %xmm14, \\XMM2 # perform a 16 byte swap\n\tpshufb %xmm14, \\XMM3 # perform a 16 byte swap\n\tpshufb %xmm14, \\XMM4 # perform a 16 byte swap\n\n.L_initial_blocks_done\\@:\n\n.endm\n\n \n.macro GHASH_4_ENCRYPT_4_PARALLEL_enc TMP1 TMP2 TMP3 TMP4 TMP5 \\\nTMP6 XMM0 XMM1 XMM2 XMM3 XMM4 XMM5 XMM6 XMM7 XMM8 operation\n\n\tmovdqa\t  \\XMM1, \\XMM5\n\tmovdqa\t  \\XMM2, \\XMM6\n\tmovdqa\t  \\XMM3, \\XMM7\n\tmovdqa\t  \\XMM4, \\XMM8\n\n        movdqa    SHUF_MASK(%rip), %xmm15\n        # multiply TMP5 * HashKey using karatsuba\n\n\tmovdqa\t  \\XMM5, \\TMP4\n\tpshufd\t  $78, \\XMM5, \\TMP6\n\tpxor\t  \\XMM5, \\TMP6\n\tpaddd     ONE(%rip), \\XMM0\t\t# INCR CNT\n\tmovdqu\t  HashKey_4(%arg2), \\TMP5\n\tpclmulqdq $0x11, \\TMP5, \\TMP4           # TMP4 = a1*b1\n\tmovdqa    \\XMM0, \\XMM1\n\tpaddd     ONE(%rip), \\XMM0\t\t# INCR CNT\n\tmovdqa    \\XMM0, \\XMM2\n\tpaddd     ONE(%rip), \\XMM0\t\t# INCR CNT\n\tmovdqa    \\XMM0, \\XMM3\n\tpaddd     ONE(%rip), \\XMM0\t\t# INCR CNT\n\tmovdqa    \\XMM0, \\XMM4\n\tpshufb %xmm15, \\XMM1\t# perform a 16 byte swap\n\tpclmulqdq $0x00, \\TMP5, \\XMM5           # XMM5 = a0*b0\n\tpshufb %xmm15, \\XMM2\t# perform a 16 byte swap\n\tpshufb %xmm15, \\XMM3\t# perform a 16 byte swap\n\tpshufb %xmm15, \\XMM4\t# perform a 16 byte swap\n\n\tpxor\t  (%arg1), \\XMM1\n\tpxor\t  (%arg1), \\XMM2\n\tpxor\t  (%arg1), \\XMM3\n\tpxor\t  (%arg1), \\XMM4\n\tmovdqu\t  HashKey_4_k(%arg2), \\TMP5\n\tpclmulqdq $0x00, \\TMP5, \\TMP6       # TMP6 = (a1+a0)*(b1+b0)\n\tmovaps 0x10(%arg1), \\TMP1\n\taesenc\t  \\TMP1, \\XMM1              # Round 1\n\taesenc\t  \\TMP1, \\XMM2\n\taesenc\t  \\TMP1, \\XMM3\n\taesenc\t  \\TMP1, \\XMM4\n\tmovaps 0x20(%arg1), \\TMP1\n\taesenc\t  \\TMP1, \\XMM1              # Round 2\n\taesenc\t  \\TMP1, \\XMM2\n\taesenc\t  \\TMP1, \\XMM3\n\taesenc\t  \\TMP1, \\XMM4\n\tmovdqa\t  \\XMM6, \\TMP1\n\tpshufd\t  $78, \\XMM6, \\TMP2\n\tpxor\t  \\XMM6, \\TMP2\n\tmovdqu\t  HashKey_3(%arg2), \\TMP5\n\tpclmulqdq $0x11, \\TMP5, \\TMP1       # TMP1 = a1 * b1\n\tmovaps 0x30(%arg1), \\TMP3\n\taesenc    \\TMP3, \\XMM1              # Round 3\n\taesenc    \\TMP3, \\XMM2\n\taesenc    \\TMP3, \\XMM3\n\taesenc    \\TMP3, \\XMM4\n\tpclmulqdq $0x00, \\TMP5, \\XMM6       # XMM6 = a0*b0\n\tmovaps 0x40(%arg1), \\TMP3\n\taesenc\t  \\TMP3, \\XMM1              # Round 4\n\taesenc\t  \\TMP3, \\XMM2\n\taesenc\t  \\TMP3, \\XMM3\n\taesenc\t  \\TMP3, \\XMM4\n\tmovdqu\t  HashKey_3_k(%arg2), \\TMP5\n\tpclmulqdq $0x00, \\TMP5, \\TMP2       # TMP2 = (a1+a0)*(b1+b0)\n\tmovaps 0x50(%arg1), \\TMP3\n\taesenc\t  \\TMP3, \\XMM1              # Round 5\n\taesenc\t  \\TMP3, \\XMM2\n\taesenc\t  \\TMP3, \\XMM3\n\taesenc\t  \\TMP3, \\XMM4\n\tpxor\t  \\TMP1, \\TMP4\n# accumulate the results in TMP4:XMM5, TMP6 holds the middle part\n\tpxor\t  \\XMM6, \\XMM5\n\tpxor\t  \\TMP2, \\TMP6\n\tmovdqa\t  \\XMM7, \\TMP1\n\tpshufd\t  $78, \\XMM7, \\TMP2\n\tpxor\t  \\XMM7, \\TMP2\n\tmovdqu\t  HashKey_2(%arg2), \\TMP5\n\n        # Multiply TMP5 * HashKey using karatsuba\n\n\tpclmulqdq $0x11, \\TMP5, \\TMP1       # TMP1 = a1*b1\n\tmovaps 0x60(%arg1), \\TMP3\n\taesenc\t  \\TMP3, \\XMM1              # Round 6\n\taesenc\t  \\TMP3, \\XMM2\n\taesenc\t  \\TMP3, \\XMM3\n\taesenc\t  \\TMP3, \\XMM4\n\tpclmulqdq $0x00, \\TMP5, \\XMM7       # XMM7 = a0*b0\n\tmovaps 0x70(%arg1), \\TMP3\n\taesenc\t  \\TMP3, \\XMM1              # Round 7\n\taesenc\t  \\TMP3, \\XMM2\n\taesenc\t  \\TMP3, \\XMM3\n\taesenc\t  \\TMP3, \\XMM4\n\tmovdqu\t  HashKey_2_k(%arg2), \\TMP5\n\tpclmulqdq $0x00, \\TMP5, \\TMP2       # TMP2 = (a1+a0)*(b1+b0)\n\tmovaps 0x80(%arg1), \\TMP3\n\taesenc\t  \\TMP3, \\XMM1              # Round 8\n\taesenc\t  \\TMP3, \\XMM2\n\taesenc\t  \\TMP3, \\XMM3\n\taesenc\t  \\TMP3, \\XMM4\n\tpxor\t  \\TMP1, \\TMP4\n# accumulate the results in TMP4:XMM5, TMP6 holds the middle part\n\tpxor\t  \\XMM7, \\XMM5\n\tpxor\t  \\TMP2, \\TMP6\n\n        # Multiply XMM8 * HashKey\n        # XMM8 and TMP5 hold the values for the two operands\n\n\tmovdqa\t  \\XMM8, \\TMP1\n\tpshufd\t  $78, \\XMM8, \\TMP2\n\tpxor\t  \\XMM8, \\TMP2\n\tmovdqu\t  HashKey(%arg2), \\TMP5\n\tpclmulqdq $0x11, \\TMP5, \\TMP1      # TMP1 = a1*b1\n\tmovaps 0x90(%arg1), \\TMP3\n\taesenc\t  \\TMP3, \\XMM1             # Round 9\n\taesenc\t  \\TMP3, \\XMM2\n\taesenc\t  \\TMP3, \\XMM3\n\taesenc\t  \\TMP3, \\XMM4\n\tpclmulqdq $0x00, \\TMP5, \\XMM8      # XMM8 = a0*b0\n\tlea\t  0xa0(%arg1),%r10\n\tmov\t  keysize,%eax\n\tshr\t  $2,%eax\t\t\t# 128->4, 192->6, 256->8\n\tsub\t  $4,%eax\t\t\t# 128->0, 192->2, 256->4\n\tjz\t  .Laes_loop_par_enc_done\\@\n\n.Laes_loop_par_enc\\@:\n\tMOVADQ\t  (%r10),\\TMP3\n.irpc\tindex, 1234\n\taesenc\t  \\TMP3, %xmm\\index\n.endr\n\tadd\t  $16,%r10\n\tsub\t  $1,%eax\n\tjnz\t  .Laes_loop_par_enc\\@\n\n.Laes_loop_par_enc_done\\@:\n\tMOVADQ\t  (%r10), \\TMP3\n\taesenclast \\TMP3, \\XMM1           # Round 10\n\taesenclast \\TMP3, \\XMM2\n\taesenclast \\TMP3, \\XMM3\n\taesenclast \\TMP3, \\XMM4\n\tmovdqu    HashKey_k(%arg2), \\TMP5\n\tpclmulqdq $0x00, \\TMP5, \\TMP2          # TMP2 = (a1+a0)*(b1+b0)\n\tmovdqu\t  (%arg4,%r11,1), \\TMP3\n\tpxor\t  \\TMP3, \\XMM1                 # Ciphertext/Plaintext XOR EK\n\tmovdqu\t  16(%arg4,%r11,1), \\TMP3\n\tpxor\t  \\TMP3, \\XMM2                 # Ciphertext/Plaintext XOR EK\n\tmovdqu\t  32(%arg4,%r11,1), \\TMP3\n\tpxor\t  \\TMP3, \\XMM3                 # Ciphertext/Plaintext XOR EK\n\tmovdqu\t  48(%arg4,%r11,1), \\TMP3\n\tpxor\t  \\TMP3, \\XMM4                 # Ciphertext/Plaintext XOR EK\n        movdqu    \\XMM1, (%arg3,%r11,1)        # Write to the ciphertext buffer\n        movdqu    \\XMM2, 16(%arg3,%r11,1)      # Write to the ciphertext buffer\n        movdqu    \\XMM3, 32(%arg3,%r11,1)      # Write to the ciphertext buffer\n        movdqu    \\XMM4, 48(%arg3,%r11,1)      # Write to the ciphertext buffer\n\tpshufb %xmm15, \\XMM1        # perform a 16 byte swap\n\tpshufb %xmm15, \\XMM2\t# perform a 16 byte swap\n\tpshufb %xmm15, \\XMM3\t# perform a 16 byte swap\n\tpshufb %xmm15, \\XMM4\t# perform a 16 byte swap\n\n\tpxor\t  \\TMP4, \\TMP1\n\tpxor\t  \\XMM8, \\XMM5\n\tpxor\t  \\TMP6, \\TMP2\n\tpxor\t  \\TMP1, \\TMP2\n\tpxor\t  \\XMM5, \\TMP2\n\tmovdqa\t  \\TMP2, \\TMP3\n\tpslldq\t  $8, \\TMP3                    # left shift TMP3 2 DWs\n\tpsrldq\t  $8, \\TMP2                    # right shift TMP2 2 DWs\n\tpxor\t  \\TMP3, \\XMM5\n\tpxor\t  \\TMP2, \\TMP1\t  # accumulate the results in TMP1:XMM5\n\n        # first phase of reduction\n\n\tmovdqa    \\XMM5, \\TMP2\n\tmovdqa    \\XMM5, \\TMP3\n\tmovdqa    \\XMM5, \\TMP4\n# move XMM5 into TMP2, TMP3, TMP4 in order to perform shifts independently\n\tpslld     $31, \\TMP2                   # packed right shift << 31\n\tpslld     $30, \\TMP3                   # packed right shift << 30\n\tpslld     $25, \\TMP4                   # packed right shift << 25\n\tpxor      \\TMP3, \\TMP2\t               # xor the shifted versions\n\tpxor      \\TMP4, \\TMP2\n\tmovdqa    \\TMP2, \\TMP5\n\tpsrldq    $4, \\TMP5                    # right shift T5 1 DW\n\tpslldq    $12, \\TMP2                   # left shift T2 3 DWs\n\tpxor      \\TMP2, \\XMM5\n\n        # second phase of reduction\n\n\tmovdqa    \\XMM5,\\TMP2 # make 3 copies of XMM5 into TMP2, TMP3, TMP4\n\tmovdqa    \\XMM5,\\TMP3\n\tmovdqa    \\XMM5,\\TMP4\n\tpsrld     $1, \\TMP2                    # packed left shift >>1\n\tpsrld     $2, \\TMP3                    # packed left shift >>2\n\tpsrld     $7, \\TMP4                    # packed left shift >>7\n\tpxor      \\TMP3,\\TMP2\t\t       # xor the shifted versions\n\tpxor      \\TMP4,\\TMP2\n\tpxor      \\TMP5, \\TMP2\n\tpxor      \\TMP2, \\XMM5\n\tpxor      \\TMP1, \\XMM5                 # result is in TMP1\n\n\tpxor\t  \\XMM5, \\XMM1\n.endm\n\n \n.macro GHASH_4_ENCRYPT_4_PARALLEL_dec TMP1 TMP2 TMP3 TMP4 TMP5 \\\nTMP6 XMM0 XMM1 XMM2 XMM3 XMM4 XMM5 XMM6 XMM7 XMM8 operation\n\n\tmovdqa\t  \\XMM1, \\XMM5\n\tmovdqa\t  \\XMM2, \\XMM6\n\tmovdqa\t  \\XMM3, \\XMM7\n\tmovdqa\t  \\XMM4, \\XMM8\n\n        movdqa    SHUF_MASK(%rip), %xmm15\n        # multiply TMP5 * HashKey using karatsuba\n\n\tmovdqa\t  \\XMM5, \\TMP4\n\tpshufd\t  $78, \\XMM5, \\TMP6\n\tpxor\t  \\XMM5, \\TMP6\n\tpaddd     ONE(%rip), \\XMM0\t\t# INCR CNT\n\tmovdqu\t  HashKey_4(%arg2), \\TMP5\n\tpclmulqdq $0x11, \\TMP5, \\TMP4           # TMP4 = a1*b1\n\tmovdqa    \\XMM0, \\XMM1\n\tpaddd     ONE(%rip), \\XMM0\t\t# INCR CNT\n\tmovdqa    \\XMM0, \\XMM2\n\tpaddd     ONE(%rip), \\XMM0\t\t# INCR CNT\n\tmovdqa    \\XMM0, \\XMM3\n\tpaddd     ONE(%rip), \\XMM0\t\t# INCR CNT\n\tmovdqa    \\XMM0, \\XMM4\n\tpshufb %xmm15, \\XMM1\t# perform a 16 byte swap\n\tpclmulqdq $0x00, \\TMP5, \\XMM5           # XMM5 = a0*b0\n\tpshufb %xmm15, \\XMM2\t# perform a 16 byte swap\n\tpshufb %xmm15, \\XMM3\t# perform a 16 byte swap\n\tpshufb %xmm15, \\XMM4\t# perform a 16 byte swap\n\n\tpxor\t  (%arg1), \\XMM1\n\tpxor\t  (%arg1), \\XMM2\n\tpxor\t  (%arg1), \\XMM3\n\tpxor\t  (%arg1), \\XMM4\n\tmovdqu\t  HashKey_4_k(%arg2), \\TMP5\n\tpclmulqdq $0x00, \\TMP5, \\TMP6       # TMP6 = (a1+a0)*(b1+b0)\n\tmovaps 0x10(%arg1), \\TMP1\n\taesenc\t  \\TMP1, \\XMM1              # Round 1\n\taesenc\t  \\TMP1, \\XMM2\n\taesenc\t  \\TMP1, \\XMM3\n\taesenc\t  \\TMP1, \\XMM4\n\tmovaps 0x20(%arg1), \\TMP1\n\taesenc\t  \\TMP1, \\XMM1              # Round 2\n\taesenc\t  \\TMP1, \\XMM2\n\taesenc\t  \\TMP1, \\XMM3\n\taesenc\t  \\TMP1, \\XMM4\n\tmovdqa\t  \\XMM6, \\TMP1\n\tpshufd\t  $78, \\XMM6, \\TMP2\n\tpxor\t  \\XMM6, \\TMP2\n\tmovdqu\t  HashKey_3(%arg2), \\TMP5\n\tpclmulqdq $0x11, \\TMP5, \\TMP1       # TMP1 = a1 * b1\n\tmovaps 0x30(%arg1), \\TMP3\n\taesenc    \\TMP3, \\XMM1              # Round 3\n\taesenc    \\TMP3, \\XMM2\n\taesenc    \\TMP3, \\XMM3\n\taesenc    \\TMP3, \\XMM4\n\tpclmulqdq $0x00, \\TMP5, \\XMM6       # XMM6 = a0*b0\n\tmovaps 0x40(%arg1), \\TMP3\n\taesenc\t  \\TMP3, \\XMM1              # Round 4\n\taesenc\t  \\TMP3, \\XMM2\n\taesenc\t  \\TMP3, \\XMM3\n\taesenc\t  \\TMP3, \\XMM4\n\tmovdqu\t  HashKey_3_k(%arg2), \\TMP5\n\tpclmulqdq $0x00, \\TMP5, \\TMP2       # TMP2 = (a1+a0)*(b1+b0)\n\tmovaps 0x50(%arg1), \\TMP3\n\taesenc\t  \\TMP3, \\XMM1              # Round 5\n\taesenc\t  \\TMP3, \\XMM2\n\taesenc\t  \\TMP3, \\XMM3\n\taesenc\t  \\TMP3, \\XMM4\n\tpxor\t  \\TMP1, \\TMP4\n# accumulate the results in TMP4:XMM5, TMP6 holds the middle part\n\tpxor\t  \\XMM6, \\XMM5\n\tpxor\t  \\TMP2, \\TMP6\n\tmovdqa\t  \\XMM7, \\TMP1\n\tpshufd\t  $78, \\XMM7, \\TMP2\n\tpxor\t  \\XMM7, \\TMP2\n\tmovdqu\t  HashKey_2(%arg2), \\TMP5\n\n        # Multiply TMP5 * HashKey using karatsuba\n\n\tpclmulqdq $0x11, \\TMP5, \\TMP1       # TMP1 = a1*b1\n\tmovaps 0x60(%arg1), \\TMP3\n\taesenc\t  \\TMP3, \\XMM1              # Round 6\n\taesenc\t  \\TMP3, \\XMM2\n\taesenc\t  \\TMP3, \\XMM3\n\taesenc\t  \\TMP3, \\XMM4\n\tpclmulqdq $0x00, \\TMP5, \\XMM7       # XMM7 = a0*b0\n\tmovaps 0x70(%arg1), \\TMP3\n\taesenc\t  \\TMP3, \\XMM1              # Round 7\n\taesenc\t  \\TMP3, \\XMM2\n\taesenc\t  \\TMP3, \\XMM3\n\taesenc\t  \\TMP3, \\XMM4\n\tmovdqu\t  HashKey_2_k(%arg2), \\TMP5\n\tpclmulqdq $0x00, \\TMP5, \\TMP2       # TMP2 = (a1+a0)*(b1+b0)\n\tmovaps 0x80(%arg1), \\TMP3\n\taesenc\t  \\TMP3, \\XMM1              # Round 8\n\taesenc\t  \\TMP3, \\XMM2\n\taesenc\t  \\TMP3, \\XMM3\n\taesenc\t  \\TMP3, \\XMM4\n\tpxor\t  \\TMP1, \\TMP4\n# accumulate the results in TMP4:XMM5, TMP6 holds the middle part\n\tpxor\t  \\XMM7, \\XMM5\n\tpxor\t  \\TMP2, \\TMP6\n\n        # Multiply XMM8 * HashKey\n        # XMM8 and TMP5 hold the values for the two operands\n\n\tmovdqa\t  \\XMM8, \\TMP1\n\tpshufd\t  $78, \\XMM8, \\TMP2\n\tpxor\t  \\XMM8, \\TMP2\n\tmovdqu\t  HashKey(%arg2), \\TMP5\n\tpclmulqdq $0x11, \\TMP5, \\TMP1      # TMP1 = a1*b1\n\tmovaps 0x90(%arg1), \\TMP3\n\taesenc\t  \\TMP3, \\XMM1             # Round 9\n\taesenc\t  \\TMP3, \\XMM2\n\taesenc\t  \\TMP3, \\XMM3\n\taesenc\t  \\TMP3, \\XMM4\n\tpclmulqdq $0x00, \\TMP5, \\XMM8      # XMM8 = a0*b0\n\tlea\t  0xa0(%arg1),%r10\n\tmov\t  keysize,%eax\n\tshr\t  $2,%eax\t\t        # 128->4, 192->6, 256->8\n\tsub\t  $4,%eax\t\t\t# 128->0, 192->2, 256->4\n\tjz\t  .Laes_loop_par_dec_done\\@\n\n.Laes_loop_par_dec\\@:\n\tMOVADQ\t  (%r10),\\TMP3\n.irpc\tindex, 1234\n\taesenc\t  \\TMP3, %xmm\\index\n.endr\n\tadd\t  $16,%r10\n\tsub\t  $1,%eax\n\tjnz\t  .Laes_loop_par_dec\\@\n\n.Laes_loop_par_dec_done\\@:\n\tMOVADQ\t  (%r10), \\TMP3\n\taesenclast \\TMP3, \\XMM1           # last round\n\taesenclast \\TMP3, \\XMM2\n\taesenclast \\TMP3, \\XMM3\n\taesenclast \\TMP3, \\XMM4\n\tmovdqu    HashKey_k(%arg2), \\TMP5\n\tpclmulqdq $0x00, \\TMP5, \\TMP2          # TMP2 = (a1+a0)*(b1+b0)\n\tmovdqu\t  (%arg4,%r11,1), \\TMP3\n\tpxor\t  \\TMP3, \\XMM1                 # Ciphertext/Plaintext XOR EK\n\tmovdqu\t  \\XMM1, (%arg3,%r11,1)        # Write to plaintext buffer\n\tmovdqa    \\TMP3, \\XMM1\n\tmovdqu\t  16(%arg4,%r11,1), \\TMP3\n\tpxor\t  \\TMP3, \\XMM2                 # Ciphertext/Plaintext XOR EK\n\tmovdqu\t  \\XMM2, 16(%arg3,%r11,1)      # Write to plaintext buffer\n\tmovdqa    \\TMP3, \\XMM2\n\tmovdqu\t  32(%arg4,%r11,1), \\TMP3\n\tpxor\t  \\TMP3, \\XMM3                 # Ciphertext/Plaintext XOR EK\n\tmovdqu\t  \\XMM3, 32(%arg3,%r11,1)      # Write to plaintext buffer\n\tmovdqa    \\TMP3, \\XMM3\n\tmovdqu\t  48(%arg4,%r11,1), \\TMP3\n\tpxor\t  \\TMP3, \\XMM4                 # Ciphertext/Plaintext XOR EK\n\tmovdqu\t  \\XMM4, 48(%arg3,%r11,1)      # Write to plaintext buffer\n\tmovdqa    \\TMP3, \\XMM4\n\tpshufb %xmm15, \\XMM1        # perform a 16 byte swap\n\tpshufb %xmm15, \\XMM2\t# perform a 16 byte swap\n\tpshufb %xmm15, \\XMM3\t# perform a 16 byte swap\n\tpshufb %xmm15, \\XMM4\t# perform a 16 byte swap\n\n\tpxor\t  \\TMP4, \\TMP1\n\tpxor\t  \\XMM8, \\XMM5\n\tpxor\t  \\TMP6, \\TMP2\n\tpxor\t  \\TMP1, \\TMP2\n\tpxor\t  \\XMM5, \\TMP2\n\tmovdqa\t  \\TMP2, \\TMP3\n\tpslldq\t  $8, \\TMP3                    # left shift TMP3 2 DWs\n\tpsrldq\t  $8, \\TMP2                    # right shift TMP2 2 DWs\n\tpxor\t  \\TMP3, \\XMM5\n\tpxor\t  \\TMP2, \\TMP1\t  # accumulate the results in TMP1:XMM5\n\n        # first phase of reduction\n\n\tmovdqa    \\XMM5, \\TMP2\n\tmovdqa    \\XMM5, \\TMP3\n\tmovdqa    \\XMM5, \\TMP4\n# move XMM5 into TMP2, TMP3, TMP4 in order to perform shifts independently\n\tpslld     $31, \\TMP2                   # packed right shift << 31\n\tpslld     $30, \\TMP3                   # packed right shift << 30\n\tpslld     $25, \\TMP4                   # packed right shift << 25\n\tpxor      \\TMP3, \\TMP2\t               # xor the shifted versions\n\tpxor      \\TMP4, \\TMP2\n\tmovdqa    \\TMP2, \\TMP5\n\tpsrldq    $4, \\TMP5                    # right shift T5 1 DW\n\tpslldq    $12, \\TMP2                   # left shift T2 3 DWs\n\tpxor      \\TMP2, \\XMM5\n\n        # second phase of reduction\n\n\tmovdqa    \\XMM5,\\TMP2 # make 3 copies of XMM5 into TMP2, TMP3, TMP4\n\tmovdqa    \\XMM5,\\TMP3\n\tmovdqa    \\XMM5,\\TMP4\n\tpsrld     $1, \\TMP2                    # packed left shift >>1\n\tpsrld     $2, \\TMP3                    # packed left shift >>2\n\tpsrld     $7, \\TMP4                    # packed left shift >>7\n\tpxor      \\TMP3,\\TMP2\t\t       # xor the shifted versions\n\tpxor      \\TMP4,\\TMP2\n\tpxor      \\TMP5, \\TMP2\n\tpxor      \\TMP2, \\XMM5\n\tpxor      \\TMP1, \\XMM5                 # result is in TMP1\n\n\tpxor\t  \\XMM5, \\XMM1\n.endm\n\n \n.macro\tGHASH_LAST_4 TMP1 TMP2 TMP3 TMP4 TMP5 TMP6 \\\nTMP7 XMM1 XMM2 XMM3 XMM4 XMMDst\n\n        # Multiply TMP6 * HashKey (using Karatsuba)\n\n\tmovdqa\t  \\XMM1, \\TMP6\n\tpshufd\t  $78, \\XMM1, \\TMP2\n\tpxor\t  \\XMM1, \\TMP2\n\tmovdqu\t  HashKey_4(%arg2), \\TMP5\n\tpclmulqdq $0x11, \\TMP5, \\TMP6       # TMP6 = a1*b1\n\tpclmulqdq $0x00, \\TMP5, \\XMM1       # XMM1 = a0*b0\n\tmovdqu\t  HashKey_4_k(%arg2), \\TMP4\n\tpclmulqdq $0x00, \\TMP4, \\TMP2       # TMP2 = (a1+a0)*(b1+b0)\n\tmovdqa\t  \\XMM1, \\XMMDst\n\tmovdqa\t  \\TMP2, \\XMM1              # result in TMP6, XMMDst, XMM1\n\n        # Multiply TMP1 * HashKey (using Karatsuba)\n\n\tmovdqa\t  \\XMM2, \\TMP1\n\tpshufd\t  $78, \\XMM2, \\TMP2\n\tpxor\t  \\XMM2, \\TMP2\n\tmovdqu\t  HashKey_3(%arg2), \\TMP5\n\tpclmulqdq $0x11, \\TMP5, \\TMP1       # TMP1 = a1*b1\n\tpclmulqdq $0x00, \\TMP5, \\XMM2       # XMM2 = a0*b0\n\tmovdqu\t  HashKey_3_k(%arg2), \\TMP4\n\tpclmulqdq $0x00, \\TMP4, \\TMP2       # TMP2 = (a1+a0)*(b1+b0)\n\tpxor\t  \\TMP1, \\TMP6\n\tpxor\t  \\XMM2, \\XMMDst\n\tpxor\t  \\TMP2, \\XMM1\n# results accumulated in TMP6, XMMDst, XMM1\n\n        # Multiply TMP1 * HashKey (using Karatsuba)\n\n\tmovdqa\t  \\XMM3, \\TMP1\n\tpshufd\t  $78, \\XMM3, \\TMP2\n\tpxor\t  \\XMM3, \\TMP2\n\tmovdqu\t  HashKey_2(%arg2), \\TMP5\n\tpclmulqdq $0x11, \\TMP5, \\TMP1       # TMP1 = a1*b1\n\tpclmulqdq $0x00, \\TMP5, \\XMM3       # XMM3 = a0*b0\n\tmovdqu\t  HashKey_2_k(%arg2), \\TMP4\n\tpclmulqdq $0x00, \\TMP4, \\TMP2       # TMP2 = (a1+a0)*(b1+b0)\n\tpxor\t  \\TMP1, \\TMP6\n\tpxor\t  \\XMM3, \\XMMDst\n\tpxor\t  \\TMP2, \\XMM1   # results accumulated in TMP6, XMMDst, XMM1\n\n        # Multiply TMP1 * HashKey (using Karatsuba)\n\tmovdqa\t  \\XMM4, \\TMP1\n\tpshufd\t  $78, \\XMM4, \\TMP2\n\tpxor\t  \\XMM4, \\TMP2\n\tmovdqu\t  HashKey(%arg2), \\TMP5\n\tpclmulqdq $0x11, \\TMP5, \\TMP1\t    # TMP1 = a1*b1\n\tpclmulqdq $0x00, \\TMP5, \\XMM4       # XMM4 = a0*b0\n\tmovdqu\t  HashKey_k(%arg2), \\TMP4\n\tpclmulqdq $0x00, \\TMP4, \\TMP2       # TMP2 = (a1+a0)*(b1+b0)\n\tpxor\t  \\TMP1, \\TMP6\n\tpxor\t  \\XMM4, \\XMMDst\n\tpxor\t  \\XMM1, \\TMP2\n\tpxor\t  \\TMP6, \\TMP2\n\tpxor\t  \\XMMDst, \\TMP2\n\t# middle section of the temp results combined as in karatsuba algorithm\n\tmovdqa\t  \\TMP2, \\TMP4\n\tpslldq\t  $8, \\TMP4                 # left shift TMP4 2 DWs\n\tpsrldq\t  $8, \\TMP2                 # right shift TMP2 2 DWs\n\tpxor\t  \\TMP4, \\XMMDst\n\tpxor\t  \\TMP2, \\TMP6\n# TMP6:XMMDst holds the result of the accumulated carry-less multiplications\n\t# first phase of the reduction\n\tmovdqa    \\XMMDst, \\TMP2\n\tmovdqa    \\XMMDst, \\TMP3\n\tmovdqa    \\XMMDst, \\TMP4\n# move XMMDst into TMP2, TMP3, TMP4 in order to perform 3 shifts independently\n\tpslld     $31, \\TMP2                # packed right shifting << 31\n\tpslld     $30, \\TMP3                # packed right shifting << 30\n\tpslld     $25, \\TMP4                # packed right shifting << 25\n\tpxor      \\TMP3, \\TMP2              # xor the shifted versions\n\tpxor      \\TMP4, \\TMP2\n\tmovdqa    \\TMP2, \\TMP7\n\tpsrldq    $4, \\TMP7                 # right shift TMP7 1 DW\n\tpslldq    $12, \\TMP2                # left shift TMP2 3 DWs\n\tpxor      \\TMP2, \\XMMDst\n\n        # second phase of the reduction\n\tmovdqa    \\XMMDst, \\TMP2\n\t# make 3 copies of XMMDst for doing 3 shift operations\n\tmovdqa    \\XMMDst, \\TMP3\n\tmovdqa    \\XMMDst, \\TMP4\n\tpsrld     $1, \\TMP2                 # packed left shift >> 1\n\tpsrld     $2, \\TMP3                 # packed left shift >> 2\n\tpsrld     $7, \\TMP4                 # packed left shift >> 7\n\tpxor      \\TMP3, \\TMP2              # xor the shifted versions\n\tpxor      \\TMP4, \\TMP2\n\tpxor      \\TMP7, \\TMP2\n\tpxor      \\TMP2, \\XMMDst\n\tpxor      \\TMP6, \\XMMDst            # reduced result is in XMMDst\n.endm\n\n\n \n\n.macro ENCRYPT_SINGLE_BLOCK XMM0 TMP1\n\n\tpxor\t\t(%arg1), \\XMM0\n\tmov\t\tkeysize,%eax\n\tshr\t\t$2,%eax\t\t\t# 128->4, 192->6, 256->8\n\tadd\t\t$5,%eax\t\t\t# 128->9, 192->11, 256->13\n\tlea\t\t16(%arg1), %r10\t  # get first expanded key address\n\n_esb_loop_\\@:\n\tMOVADQ\t\t(%r10),\\TMP1\n\taesenc\t\t\\TMP1,\\XMM0\n\tadd\t\t$16,%r10\n\tsub\t\t$1,%eax\n\tjnz\t\t_esb_loop_\\@\n\n\tMOVADQ\t\t(%r10),\\TMP1\n\taesenclast\t\\TMP1,\\XMM0\n.endm\n \nSYM_FUNC_START(aesni_gcm_dec)\n\tFUNC_SAVE\n\n\tGCM_INIT %arg6, arg7, arg8, arg9\n\tGCM_ENC_DEC dec\n\tGCM_COMPLETE arg10, arg11\n\tFUNC_RESTORE\n\tRET\nSYM_FUNC_END(aesni_gcm_dec)\n\n\n \nSYM_FUNC_START(aesni_gcm_enc)\n\tFUNC_SAVE\n\n\tGCM_INIT %arg6, arg7, arg8, arg9\n\tGCM_ENC_DEC enc\n\n\tGCM_COMPLETE arg10, arg11\n\tFUNC_RESTORE\n\tRET\nSYM_FUNC_END(aesni_gcm_enc)\n\n \nSYM_FUNC_START(aesni_gcm_init)\n\tFUNC_SAVE\n\tGCM_INIT %arg3, %arg4,%arg5, %arg6\n\tFUNC_RESTORE\n\tRET\nSYM_FUNC_END(aesni_gcm_init)\n\n \nSYM_FUNC_START(aesni_gcm_enc_update)\n\tFUNC_SAVE\n\tGCM_ENC_DEC enc\n\tFUNC_RESTORE\n\tRET\nSYM_FUNC_END(aesni_gcm_enc_update)\n\n \nSYM_FUNC_START(aesni_gcm_dec_update)\n\tFUNC_SAVE\n\tGCM_ENC_DEC dec\n\tFUNC_RESTORE\n\tRET\nSYM_FUNC_END(aesni_gcm_dec_update)\n\n \nSYM_FUNC_START(aesni_gcm_finalize)\n\tFUNC_SAVE\n\tGCM_COMPLETE %arg3 %arg4\n\tFUNC_RESTORE\n\tRET\nSYM_FUNC_END(aesni_gcm_finalize)\n\n#endif\n\nSYM_FUNC_START_LOCAL(_key_expansion_256a)\n\tpshufd $0b11111111, %xmm1, %xmm1\n\tshufps $0b00010000, %xmm0, %xmm4\n\tpxor %xmm4, %xmm0\n\tshufps $0b10001100, %xmm0, %xmm4\n\tpxor %xmm4, %xmm0\n\tpxor %xmm1, %xmm0\n\tmovaps %xmm0, (TKEYP)\n\tadd $0x10, TKEYP\n\tRET\nSYM_FUNC_END(_key_expansion_256a)\nSYM_FUNC_ALIAS_LOCAL(_key_expansion_128, _key_expansion_256a)\n\nSYM_FUNC_START_LOCAL(_key_expansion_192a)\n\tpshufd $0b01010101, %xmm1, %xmm1\n\tshufps $0b00010000, %xmm0, %xmm4\n\tpxor %xmm4, %xmm0\n\tshufps $0b10001100, %xmm0, %xmm4\n\tpxor %xmm4, %xmm0\n\tpxor %xmm1, %xmm0\n\n\tmovaps %xmm2, %xmm5\n\tmovaps %xmm2, %xmm6\n\tpslldq $4, %xmm5\n\tpshufd $0b11111111, %xmm0, %xmm3\n\tpxor %xmm3, %xmm2\n\tpxor %xmm5, %xmm2\n\n\tmovaps %xmm0, %xmm1\n\tshufps $0b01000100, %xmm0, %xmm6\n\tmovaps %xmm6, (TKEYP)\n\tshufps $0b01001110, %xmm2, %xmm1\n\tmovaps %xmm1, 0x10(TKEYP)\n\tadd $0x20, TKEYP\n\tRET\nSYM_FUNC_END(_key_expansion_192a)\n\nSYM_FUNC_START_LOCAL(_key_expansion_192b)\n\tpshufd $0b01010101, %xmm1, %xmm1\n\tshufps $0b00010000, %xmm0, %xmm4\n\tpxor %xmm4, %xmm0\n\tshufps $0b10001100, %xmm0, %xmm4\n\tpxor %xmm4, %xmm0\n\tpxor %xmm1, %xmm0\n\n\tmovaps %xmm2, %xmm5\n\tpslldq $4, %xmm5\n\tpshufd $0b11111111, %xmm0, %xmm3\n\tpxor %xmm3, %xmm2\n\tpxor %xmm5, %xmm2\n\n\tmovaps %xmm0, (TKEYP)\n\tadd $0x10, TKEYP\n\tRET\nSYM_FUNC_END(_key_expansion_192b)\n\nSYM_FUNC_START_LOCAL(_key_expansion_256b)\n\tpshufd $0b10101010, %xmm1, %xmm1\n\tshufps $0b00010000, %xmm2, %xmm4\n\tpxor %xmm4, %xmm2\n\tshufps $0b10001100, %xmm2, %xmm4\n\tpxor %xmm4, %xmm2\n\tpxor %xmm1, %xmm2\n\tmovaps %xmm2, (TKEYP)\n\tadd $0x10, TKEYP\n\tRET\nSYM_FUNC_END(_key_expansion_256b)\n\n \nSYM_FUNC_START(aesni_set_key)\n\tFRAME_BEGIN\n#ifndef __x86_64__\n\tpushl KEYP\n\tmovl (FRAME_OFFSET+8)(%esp), KEYP\t# ctx\n\tmovl (FRAME_OFFSET+12)(%esp), UKEYP\t# in_key\n\tmovl (FRAME_OFFSET+16)(%esp), %edx\t# key_len\n#endif\n\tmovups (UKEYP), %xmm0\t\t# user key (first 16 bytes)\n\tmovaps %xmm0, (KEYP)\n\tlea 0x10(KEYP), TKEYP\t\t# key addr\n\tmovl %edx, 480(KEYP)\n\tpxor %xmm4, %xmm4\t\t# xmm4 is assumed 0 in _key_expansion_x\n\tcmp $24, %dl\n\tjb .Lenc_key128\n\tje .Lenc_key192\n\tmovups 0x10(UKEYP), %xmm2\t# other user key\n\tmovaps %xmm2, (TKEYP)\n\tadd $0x10, TKEYP\n\taeskeygenassist $0x1, %xmm2, %xmm1\t# round 1\n\tcall _key_expansion_256a\n\taeskeygenassist $0x1, %xmm0, %xmm1\n\tcall _key_expansion_256b\n\taeskeygenassist $0x2, %xmm2, %xmm1\t# round 2\n\tcall _key_expansion_256a\n\taeskeygenassist $0x2, %xmm0, %xmm1\n\tcall _key_expansion_256b\n\taeskeygenassist $0x4, %xmm2, %xmm1\t# round 3\n\tcall _key_expansion_256a\n\taeskeygenassist $0x4, %xmm0, %xmm1\n\tcall _key_expansion_256b\n\taeskeygenassist $0x8, %xmm2, %xmm1\t# round 4\n\tcall _key_expansion_256a\n\taeskeygenassist $0x8, %xmm0, %xmm1\n\tcall _key_expansion_256b\n\taeskeygenassist $0x10, %xmm2, %xmm1\t# round 5\n\tcall _key_expansion_256a\n\taeskeygenassist $0x10, %xmm0, %xmm1\n\tcall _key_expansion_256b\n\taeskeygenassist $0x20, %xmm2, %xmm1\t# round 6\n\tcall _key_expansion_256a\n\taeskeygenassist $0x20, %xmm0, %xmm1\n\tcall _key_expansion_256b\n\taeskeygenassist $0x40, %xmm2, %xmm1\t# round 7\n\tcall _key_expansion_256a\n\tjmp .Ldec_key\n.Lenc_key192:\n\tmovq 0x10(UKEYP), %xmm2\t\t# other user key\n\taeskeygenassist $0x1, %xmm2, %xmm1\t# round 1\n\tcall _key_expansion_192a\n\taeskeygenassist $0x2, %xmm2, %xmm1\t# round 2\n\tcall _key_expansion_192b\n\taeskeygenassist $0x4, %xmm2, %xmm1\t# round 3\n\tcall _key_expansion_192a\n\taeskeygenassist $0x8, %xmm2, %xmm1\t# round 4\n\tcall _key_expansion_192b\n\taeskeygenassist $0x10, %xmm2, %xmm1\t# round 5\n\tcall _key_expansion_192a\n\taeskeygenassist $0x20, %xmm2, %xmm1\t# round 6\n\tcall _key_expansion_192b\n\taeskeygenassist $0x40, %xmm2, %xmm1\t# round 7\n\tcall _key_expansion_192a\n\taeskeygenassist $0x80, %xmm2, %xmm1\t# round 8\n\tcall _key_expansion_192b\n\tjmp .Ldec_key\n.Lenc_key128:\n\taeskeygenassist $0x1, %xmm0, %xmm1\t# round 1\n\tcall _key_expansion_128\n\taeskeygenassist $0x2, %xmm0, %xmm1\t# round 2\n\tcall _key_expansion_128\n\taeskeygenassist $0x4, %xmm0, %xmm1\t# round 3\n\tcall _key_expansion_128\n\taeskeygenassist $0x8, %xmm0, %xmm1\t# round 4\n\tcall _key_expansion_128\n\taeskeygenassist $0x10, %xmm0, %xmm1\t# round 5\n\tcall _key_expansion_128\n\taeskeygenassist $0x20, %xmm0, %xmm1\t# round 6\n\tcall _key_expansion_128\n\taeskeygenassist $0x40, %xmm0, %xmm1\t# round 7\n\tcall _key_expansion_128\n\taeskeygenassist $0x80, %xmm0, %xmm1\t# round 8\n\tcall _key_expansion_128\n\taeskeygenassist $0x1b, %xmm0, %xmm1\t# round 9\n\tcall _key_expansion_128\n\taeskeygenassist $0x36, %xmm0, %xmm1\t# round 10\n\tcall _key_expansion_128\n.Ldec_key:\n\tsub $0x10, TKEYP\n\tmovaps (KEYP), %xmm0\n\tmovaps (TKEYP), %xmm1\n\tmovaps %xmm0, 240(TKEYP)\n\tmovaps %xmm1, 240(KEYP)\n\tadd $0x10, KEYP\n\tlea 240-16(TKEYP), UKEYP\n.align 4\n.Ldec_key_loop:\n\tmovaps (KEYP), %xmm0\n\taesimc %xmm0, %xmm1\n\tmovaps %xmm1, (UKEYP)\n\tadd $0x10, KEYP\n\tsub $0x10, UKEYP\n\tcmp TKEYP, KEYP\n\tjb .Ldec_key_loop\n\txor AREG, AREG\n#ifndef __x86_64__\n\tpopl KEYP\n#endif\n\tFRAME_END\n\tRET\nSYM_FUNC_END(aesni_set_key)\n\n \nSYM_FUNC_START(aesni_enc)\n\tFRAME_BEGIN\n#ifndef __x86_64__\n\tpushl KEYP\n\tpushl KLEN\n\tmovl (FRAME_OFFSET+12)(%esp), KEYP\t# ctx\n\tmovl (FRAME_OFFSET+16)(%esp), OUTP\t# dst\n\tmovl (FRAME_OFFSET+20)(%esp), INP\t# src\n#endif\n\tmovl 480(KEYP), KLEN\t\t# key length\n\tmovups (INP), STATE\t\t# input\n\tcall _aesni_enc1\n\tmovups STATE, (OUTP)\t\t# output\n#ifndef __x86_64__\n\tpopl KLEN\n\tpopl KEYP\n#endif\n\tFRAME_END\n\tRET\nSYM_FUNC_END(aesni_enc)\n\n \nSYM_FUNC_START_LOCAL(_aesni_enc1)\n\tmovaps (KEYP), KEY\t\t# key\n\tmov KEYP, TKEYP\n\tpxor KEY, STATE\t\t# round 0\n\tadd $0x30, TKEYP\n\tcmp $24, KLEN\n\tjb .Lenc128\n\tlea 0x20(TKEYP), TKEYP\n\tje .Lenc192\n\tadd $0x20, TKEYP\n\tmovaps -0x60(TKEYP), KEY\n\taesenc KEY, STATE\n\tmovaps -0x50(TKEYP), KEY\n\taesenc KEY, STATE\n.align 4\n.Lenc192:\n\tmovaps -0x40(TKEYP), KEY\n\taesenc KEY, STATE\n\tmovaps -0x30(TKEYP), KEY\n\taesenc KEY, STATE\n.align 4\n.Lenc128:\n\tmovaps -0x20(TKEYP), KEY\n\taesenc KEY, STATE\n\tmovaps -0x10(TKEYP), KEY\n\taesenc KEY, STATE\n\tmovaps (TKEYP), KEY\n\taesenc KEY, STATE\n\tmovaps 0x10(TKEYP), KEY\n\taesenc KEY, STATE\n\tmovaps 0x20(TKEYP), KEY\n\taesenc KEY, STATE\n\tmovaps 0x30(TKEYP), KEY\n\taesenc KEY, STATE\n\tmovaps 0x40(TKEYP), KEY\n\taesenc KEY, STATE\n\tmovaps 0x50(TKEYP), KEY\n\taesenc KEY, STATE\n\tmovaps 0x60(TKEYP), KEY\n\taesenc KEY, STATE\n\tmovaps 0x70(TKEYP), KEY\n\taesenclast KEY, STATE\n\tRET\nSYM_FUNC_END(_aesni_enc1)\n\n \nSYM_FUNC_START_LOCAL(_aesni_enc4)\n\tmovaps (KEYP), KEY\t\t# key\n\tmov KEYP, TKEYP\n\tpxor KEY, STATE1\t\t# round 0\n\tpxor KEY, STATE2\n\tpxor KEY, STATE3\n\tpxor KEY, STATE4\n\tadd $0x30, TKEYP\n\tcmp $24, KLEN\n\tjb .L4enc128\n\tlea 0x20(TKEYP), TKEYP\n\tje .L4enc192\n\tadd $0x20, TKEYP\n\tmovaps -0x60(TKEYP), KEY\n\taesenc KEY, STATE1\n\taesenc KEY, STATE2\n\taesenc KEY, STATE3\n\taesenc KEY, STATE4\n\tmovaps -0x50(TKEYP), KEY\n\taesenc KEY, STATE1\n\taesenc KEY, STATE2\n\taesenc KEY, STATE3\n\taesenc KEY, STATE4\n#.align 4\n.L4enc192:\n\tmovaps -0x40(TKEYP), KEY\n\taesenc KEY, STATE1\n\taesenc KEY, STATE2\n\taesenc KEY, STATE3\n\taesenc KEY, STATE4\n\tmovaps -0x30(TKEYP), KEY\n\taesenc KEY, STATE1\n\taesenc KEY, STATE2\n\taesenc KEY, STATE3\n\taesenc KEY, STATE4\n#.align 4\n.L4enc128:\n\tmovaps -0x20(TKEYP), KEY\n\taesenc KEY, STATE1\n\taesenc KEY, STATE2\n\taesenc KEY, STATE3\n\taesenc KEY, STATE4\n\tmovaps -0x10(TKEYP), KEY\n\taesenc KEY, STATE1\n\taesenc KEY, STATE2\n\taesenc KEY, STATE3\n\taesenc KEY, STATE4\n\tmovaps (TKEYP), KEY\n\taesenc KEY, STATE1\n\taesenc KEY, STATE2\n\taesenc KEY, STATE3\n\taesenc KEY, STATE4\n\tmovaps 0x10(TKEYP), KEY\n\taesenc KEY, STATE1\n\taesenc KEY, STATE2\n\taesenc KEY, STATE3\n\taesenc KEY, STATE4\n\tmovaps 0x20(TKEYP), KEY\n\taesenc KEY, STATE1\n\taesenc KEY, STATE2\n\taesenc KEY, STATE3\n\taesenc KEY, STATE4\n\tmovaps 0x30(TKEYP), KEY\n\taesenc KEY, STATE1\n\taesenc KEY, STATE2\n\taesenc KEY, STATE3\n\taesenc KEY, STATE4\n\tmovaps 0x40(TKEYP), KEY\n\taesenc KEY, STATE1\n\taesenc KEY, STATE2\n\taesenc KEY, STATE3\n\taesenc KEY, STATE4\n\tmovaps 0x50(TKEYP), KEY\n\taesenc KEY, STATE1\n\taesenc KEY, STATE2\n\taesenc KEY, STATE3\n\taesenc KEY, STATE4\n\tmovaps 0x60(TKEYP), KEY\n\taesenc KEY, STATE1\n\taesenc KEY, STATE2\n\taesenc KEY, STATE3\n\taesenc KEY, STATE4\n\tmovaps 0x70(TKEYP), KEY\n\taesenclast KEY, STATE1\t\t# last round\n\taesenclast KEY, STATE2\n\taesenclast KEY, STATE3\n\taesenclast KEY, STATE4\n\tRET\nSYM_FUNC_END(_aesni_enc4)\n\n \nSYM_FUNC_START(aesni_dec)\n\tFRAME_BEGIN\n#ifndef __x86_64__\n\tpushl KEYP\n\tpushl KLEN\n\tmovl (FRAME_OFFSET+12)(%esp), KEYP\t# ctx\n\tmovl (FRAME_OFFSET+16)(%esp), OUTP\t# dst\n\tmovl (FRAME_OFFSET+20)(%esp), INP\t# src\n#endif\n\tmov 480(KEYP), KLEN\t\t# key length\n\tadd $240, KEYP\n\tmovups (INP), STATE\t\t# input\n\tcall _aesni_dec1\n\tmovups STATE, (OUTP)\t\t#output\n#ifndef __x86_64__\n\tpopl KLEN\n\tpopl KEYP\n#endif\n\tFRAME_END\n\tRET\nSYM_FUNC_END(aesni_dec)\n\n \nSYM_FUNC_START_LOCAL(_aesni_dec1)\n\tmovaps (KEYP), KEY\t\t# key\n\tmov KEYP, TKEYP\n\tpxor KEY, STATE\t\t# round 0\n\tadd $0x30, TKEYP\n\tcmp $24, KLEN\n\tjb .Ldec128\n\tlea 0x20(TKEYP), TKEYP\n\tje .Ldec192\n\tadd $0x20, TKEYP\n\tmovaps -0x60(TKEYP), KEY\n\taesdec KEY, STATE\n\tmovaps -0x50(TKEYP), KEY\n\taesdec KEY, STATE\n.align 4\n.Ldec192:\n\tmovaps -0x40(TKEYP), KEY\n\taesdec KEY, STATE\n\tmovaps -0x30(TKEYP), KEY\n\taesdec KEY, STATE\n.align 4\n.Ldec128:\n\tmovaps -0x20(TKEYP), KEY\n\taesdec KEY, STATE\n\tmovaps -0x10(TKEYP), KEY\n\taesdec KEY, STATE\n\tmovaps (TKEYP), KEY\n\taesdec KEY, STATE\n\tmovaps 0x10(TKEYP), KEY\n\taesdec KEY, STATE\n\tmovaps 0x20(TKEYP), KEY\n\taesdec KEY, STATE\n\tmovaps 0x30(TKEYP), KEY\n\taesdec KEY, STATE\n\tmovaps 0x40(TKEYP), KEY\n\taesdec KEY, STATE\n\tmovaps 0x50(TKEYP), KEY\n\taesdec KEY, STATE\n\tmovaps 0x60(TKEYP), KEY\n\taesdec KEY, STATE\n\tmovaps 0x70(TKEYP), KEY\n\taesdeclast KEY, STATE\n\tRET\nSYM_FUNC_END(_aesni_dec1)\n\n \nSYM_FUNC_START_LOCAL(_aesni_dec4)\n\tmovaps (KEYP), KEY\t\t# key\n\tmov KEYP, TKEYP\n\tpxor KEY, STATE1\t\t# round 0\n\tpxor KEY, STATE2\n\tpxor KEY, STATE3\n\tpxor KEY, STATE4\n\tadd $0x30, TKEYP\n\tcmp $24, KLEN\n\tjb .L4dec128\n\tlea 0x20(TKEYP), TKEYP\n\tje .L4dec192\n\tadd $0x20, TKEYP\n\tmovaps -0x60(TKEYP), KEY\n\taesdec KEY, STATE1\n\taesdec KEY, STATE2\n\taesdec KEY, STATE3\n\taesdec KEY, STATE4\n\tmovaps -0x50(TKEYP), KEY\n\taesdec KEY, STATE1\n\taesdec KEY, STATE2\n\taesdec KEY, STATE3\n\taesdec KEY, STATE4\n.align 4\n.L4dec192:\n\tmovaps -0x40(TKEYP), KEY\n\taesdec KEY, STATE1\n\taesdec KEY, STATE2\n\taesdec KEY, STATE3\n\taesdec KEY, STATE4\n\tmovaps -0x30(TKEYP), KEY\n\taesdec KEY, STATE1\n\taesdec KEY, STATE2\n\taesdec KEY, STATE3\n\taesdec KEY, STATE4\n.align 4\n.L4dec128:\n\tmovaps -0x20(TKEYP), KEY\n\taesdec KEY, STATE1\n\taesdec KEY, STATE2\n\taesdec KEY, STATE3\n\taesdec KEY, STATE4\n\tmovaps -0x10(TKEYP), KEY\n\taesdec KEY, STATE1\n\taesdec KEY, STATE2\n\taesdec KEY, STATE3\n\taesdec KEY, STATE4\n\tmovaps (TKEYP), KEY\n\taesdec KEY, STATE1\n\taesdec KEY, STATE2\n\taesdec KEY, STATE3\n\taesdec KEY, STATE4\n\tmovaps 0x10(TKEYP), KEY\n\taesdec KEY, STATE1\n\taesdec KEY, STATE2\n\taesdec KEY, STATE3\n\taesdec KEY, STATE4\n\tmovaps 0x20(TKEYP), KEY\n\taesdec KEY, STATE1\n\taesdec KEY, STATE2\n\taesdec KEY, STATE3\n\taesdec KEY, STATE4\n\tmovaps 0x30(TKEYP), KEY\n\taesdec KEY, STATE1\n\taesdec KEY, STATE2\n\taesdec KEY, STATE3\n\taesdec KEY, STATE4\n\tmovaps 0x40(TKEYP), KEY\n\taesdec KEY, STATE1\n\taesdec KEY, STATE2\n\taesdec KEY, STATE3\n\taesdec KEY, STATE4\n\tmovaps 0x50(TKEYP), KEY\n\taesdec KEY, STATE1\n\taesdec KEY, STATE2\n\taesdec KEY, STATE3\n\taesdec KEY, STATE4\n\tmovaps 0x60(TKEYP), KEY\n\taesdec KEY, STATE1\n\taesdec KEY, STATE2\n\taesdec KEY, STATE3\n\taesdec KEY, STATE4\n\tmovaps 0x70(TKEYP), KEY\n\taesdeclast KEY, STATE1\t\t# last round\n\taesdeclast KEY, STATE2\n\taesdeclast KEY, STATE3\n\taesdeclast KEY, STATE4\n\tRET\nSYM_FUNC_END(_aesni_dec4)\n\n \nSYM_FUNC_START(aesni_ecb_enc)\n\tFRAME_BEGIN\n#ifndef __x86_64__\n\tpushl LEN\n\tpushl KEYP\n\tpushl KLEN\n\tmovl (FRAME_OFFSET+16)(%esp), KEYP\t# ctx\n\tmovl (FRAME_OFFSET+20)(%esp), OUTP\t# dst\n\tmovl (FRAME_OFFSET+24)(%esp), INP\t# src\n\tmovl (FRAME_OFFSET+28)(%esp), LEN\t# len\n#endif\n\ttest LEN, LEN\t\t# check length\n\tjz .Lecb_enc_ret\n\tmov 480(KEYP), KLEN\n\tcmp $16, LEN\n\tjb .Lecb_enc_ret\n\tcmp $64, LEN\n\tjb .Lecb_enc_loop1\n.align 4\n.Lecb_enc_loop4:\n\tmovups (INP), STATE1\n\tmovups 0x10(INP), STATE2\n\tmovups 0x20(INP), STATE3\n\tmovups 0x30(INP), STATE4\n\tcall _aesni_enc4\n\tmovups STATE1, (OUTP)\n\tmovups STATE2, 0x10(OUTP)\n\tmovups STATE3, 0x20(OUTP)\n\tmovups STATE4, 0x30(OUTP)\n\tsub $64, LEN\n\tadd $64, INP\n\tadd $64, OUTP\n\tcmp $64, LEN\n\tjge .Lecb_enc_loop4\n\tcmp $16, LEN\n\tjb .Lecb_enc_ret\n.align 4\n.Lecb_enc_loop1:\n\tmovups (INP), STATE1\n\tcall _aesni_enc1\n\tmovups STATE1, (OUTP)\n\tsub $16, LEN\n\tadd $16, INP\n\tadd $16, OUTP\n\tcmp $16, LEN\n\tjge .Lecb_enc_loop1\n.Lecb_enc_ret:\n#ifndef __x86_64__\n\tpopl KLEN\n\tpopl KEYP\n\tpopl LEN\n#endif\n\tFRAME_END\n\tRET\nSYM_FUNC_END(aesni_ecb_enc)\n\n \nSYM_FUNC_START(aesni_ecb_dec)\n\tFRAME_BEGIN\n#ifndef __x86_64__\n\tpushl LEN\n\tpushl KEYP\n\tpushl KLEN\n\tmovl (FRAME_OFFSET+16)(%esp), KEYP\t# ctx\n\tmovl (FRAME_OFFSET+20)(%esp), OUTP\t# dst\n\tmovl (FRAME_OFFSET+24)(%esp), INP\t# src\n\tmovl (FRAME_OFFSET+28)(%esp), LEN\t# len\n#endif\n\ttest LEN, LEN\n\tjz .Lecb_dec_ret\n\tmov 480(KEYP), KLEN\n\tadd $240, KEYP\n\tcmp $16, LEN\n\tjb .Lecb_dec_ret\n\tcmp $64, LEN\n\tjb .Lecb_dec_loop1\n.align 4\n.Lecb_dec_loop4:\n\tmovups (INP), STATE1\n\tmovups 0x10(INP), STATE2\n\tmovups 0x20(INP), STATE3\n\tmovups 0x30(INP), STATE4\n\tcall _aesni_dec4\n\tmovups STATE1, (OUTP)\n\tmovups STATE2, 0x10(OUTP)\n\tmovups STATE3, 0x20(OUTP)\n\tmovups STATE4, 0x30(OUTP)\n\tsub $64, LEN\n\tadd $64, INP\n\tadd $64, OUTP\n\tcmp $64, LEN\n\tjge .Lecb_dec_loop4\n\tcmp $16, LEN\n\tjb .Lecb_dec_ret\n.align 4\n.Lecb_dec_loop1:\n\tmovups (INP), STATE1\n\tcall _aesni_dec1\n\tmovups STATE1, (OUTP)\n\tsub $16, LEN\n\tadd $16, INP\n\tadd $16, OUTP\n\tcmp $16, LEN\n\tjge .Lecb_dec_loop1\n.Lecb_dec_ret:\n#ifndef __x86_64__\n\tpopl KLEN\n\tpopl KEYP\n\tpopl LEN\n#endif\n\tFRAME_END\n\tRET\nSYM_FUNC_END(aesni_ecb_dec)\n\n \nSYM_FUNC_START(aesni_cbc_enc)\n\tFRAME_BEGIN\n#ifndef __x86_64__\n\tpushl IVP\n\tpushl LEN\n\tpushl KEYP\n\tpushl KLEN\n\tmovl (FRAME_OFFSET+20)(%esp), KEYP\t# ctx\n\tmovl (FRAME_OFFSET+24)(%esp), OUTP\t# dst\n\tmovl (FRAME_OFFSET+28)(%esp), INP\t# src\n\tmovl (FRAME_OFFSET+32)(%esp), LEN\t# len\n\tmovl (FRAME_OFFSET+36)(%esp), IVP\t# iv\n#endif\n\tcmp $16, LEN\n\tjb .Lcbc_enc_ret\n\tmov 480(KEYP), KLEN\n\tmovups (IVP), STATE\t# load iv as initial state\n.align 4\n.Lcbc_enc_loop:\n\tmovups (INP), IN\t# load input\n\tpxor IN, STATE\n\tcall _aesni_enc1\n\tmovups STATE, (OUTP)\t# store output\n\tsub $16, LEN\n\tadd $16, INP\n\tadd $16, OUTP\n\tcmp $16, LEN\n\tjge .Lcbc_enc_loop\n\tmovups STATE, (IVP)\n.Lcbc_enc_ret:\n#ifndef __x86_64__\n\tpopl KLEN\n\tpopl KEYP\n\tpopl LEN\n\tpopl IVP\n#endif\n\tFRAME_END\n\tRET\nSYM_FUNC_END(aesni_cbc_enc)\n\n \nSYM_FUNC_START(aesni_cbc_dec)\n\tFRAME_BEGIN\n#ifndef __x86_64__\n\tpushl IVP\n\tpushl LEN\n\tpushl KEYP\n\tpushl KLEN\n\tmovl (FRAME_OFFSET+20)(%esp), KEYP\t# ctx\n\tmovl (FRAME_OFFSET+24)(%esp), OUTP\t# dst\n\tmovl (FRAME_OFFSET+28)(%esp), INP\t# src\n\tmovl (FRAME_OFFSET+32)(%esp), LEN\t# len\n\tmovl (FRAME_OFFSET+36)(%esp), IVP\t# iv\n#endif\n\tcmp $16, LEN\n\tjb .Lcbc_dec_just_ret\n\tmov 480(KEYP), KLEN\n\tadd $240, KEYP\n\tmovups (IVP), IV\n\tcmp $64, LEN\n\tjb .Lcbc_dec_loop1\n.align 4\n.Lcbc_dec_loop4:\n\tmovups (INP), IN1\n\tmovaps IN1, STATE1\n\tmovups 0x10(INP), IN2\n\tmovaps IN2, STATE2\n#ifdef __x86_64__\n\tmovups 0x20(INP), IN3\n\tmovaps IN3, STATE3\n\tmovups 0x30(INP), IN4\n\tmovaps IN4, STATE4\n#else\n\tmovups 0x20(INP), IN1\n\tmovaps IN1, STATE3\n\tmovups 0x30(INP), IN2\n\tmovaps IN2, STATE4\n#endif\n\tcall _aesni_dec4\n\tpxor IV, STATE1\n#ifdef __x86_64__\n\tpxor IN1, STATE2\n\tpxor IN2, STATE3\n\tpxor IN3, STATE4\n\tmovaps IN4, IV\n#else\n\tpxor IN1, STATE4\n\tmovaps IN2, IV\n\tmovups (INP), IN1\n\tpxor IN1, STATE2\n\tmovups 0x10(INP), IN2\n\tpxor IN2, STATE3\n#endif\n\tmovups STATE1, (OUTP)\n\tmovups STATE2, 0x10(OUTP)\n\tmovups STATE3, 0x20(OUTP)\n\tmovups STATE4, 0x30(OUTP)\n\tsub $64, LEN\n\tadd $64, INP\n\tadd $64, OUTP\n\tcmp $64, LEN\n\tjge .Lcbc_dec_loop4\n\tcmp $16, LEN\n\tjb .Lcbc_dec_ret\n.align 4\n.Lcbc_dec_loop1:\n\tmovups (INP), IN\n\tmovaps IN, STATE\n\tcall _aesni_dec1\n\tpxor IV, STATE\n\tmovups STATE, (OUTP)\n\tmovaps IN, IV\n\tsub $16, LEN\n\tadd $16, INP\n\tadd $16, OUTP\n\tcmp $16, LEN\n\tjge .Lcbc_dec_loop1\n.Lcbc_dec_ret:\n\tmovups IV, (IVP)\n.Lcbc_dec_just_ret:\n#ifndef __x86_64__\n\tpopl KLEN\n\tpopl KEYP\n\tpopl LEN\n\tpopl IVP\n#endif\n\tFRAME_END\n\tRET\nSYM_FUNC_END(aesni_cbc_dec)\n\n \nSYM_FUNC_START(aesni_cts_cbc_enc)\n\tFRAME_BEGIN\n#ifndef __x86_64__\n\tpushl IVP\n\tpushl LEN\n\tpushl KEYP\n\tpushl KLEN\n\tmovl (FRAME_OFFSET+20)(%esp), KEYP\t# ctx\n\tmovl (FRAME_OFFSET+24)(%esp), OUTP\t# dst\n\tmovl (FRAME_OFFSET+28)(%esp), INP\t# src\n\tmovl (FRAME_OFFSET+32)(%esp), LEN\t# len\n\tmovl (FRAME_OFFSET+36)(%esp), IVP\t# iv\n\tlea .Lcts_permute_table, T1\n#else\n\tlea .Lcts_permute_table(%rip), T1\n#endif\n\tmov 480(KEYP), KLEN\n\tmovups (IVP), STATE\n\tsub $16, LEN\n\tmov T1, IVP\n\tadd $32, IVP\n\tadd LEN, T1\n\tsub LEN, IVP\n\tmovups (T1), %xmm4\n\tmovups (IVP), %xmm5\n\n\tmovups (INP), IN1\n\tadd LEN, INP\n\tmovups (INP), IN2\n\n\tpxor IN1, STATE\n\tcall _aesni_enc1\n\n\tpshufb %xmm5, IN2\n\tpxor STATE, IN2\n\tpshufb %xmm4, STATE\n\tadd OUTP, LEN\n\tmovups STATE, (LEN)\n\n\tmovaps IN2, STATE\n\tcall _aesni_enc1\n\tmovups STATE, (OUTP)\n\n#ifndef __x86_64__\n\tpopl KLEN\n\tpopl KEYP\n\tpopl LEN\n\tpopl IVP\n#endif\n\tFRAME_END\n\tRET\nSYM_FUNC_END(aesni_cts_cbc_enc)\n\n \nSYM_FUNC_START(aesni_cts_cbc_dec)\n\tFRAME_BEGIN\n#ifndef __x86_64__\n\tpushl IVP\n\tpushl LEN\n\tpushl KEYP\n\tpushl KLEN\n\tmovl (FRAME_OFFSET+20)(%esp), KEYP\t# ctx\n\tmovl (FRAME_OFFSET+24)(%esp), OUTP\t# dst\n\tmovl (FRAME_OFFSET+28)(%esp), INP\t# src\n\tmovl (FRAME_OFFSET+32)(%esp), LEN\t# len\n\tmovl (FRAME_OFFSET+36)(%esp), IVP\t# iv\n\tlea .Lcts_permute_table, T1\n#else\n\tlea .Lcts_permute_table(%rip), T1\n#endif\n\tmov 480(KEYP), KLEN\n\tadd $240, KEYP\n\tmovups (IVP), IV\n\tsub $16, LEN\n\tmov T1, IVP\n\tadd $32, IVP\n\tadd LEN, T1\n\tsub LEN, IVP\n\tmovups (T1), %xmm4\n\n\tmovups (INP), STATE\n\tadd LEN, INP\n\tmovups (INP), IN1\n\n\tcall _aesni_dec1\n\tmovaps STATE, IN2\n\tpshufb %xmm4, STATE\n\tpxor IN1, STATE\n\n\tadd OUTP, LEN\n\tmovups STATE, (LEN)\n\n\tmovups (IVP), %xmm0\n\tpshufb %xmm0, IN1\n\tpblendvb IN2, IN1\n\tmovaps IN1, STATE\n\tcall _aesni_dec1\n\n\tpxor IV, STATE\n\tmovups STATE, (OUTP)\n\n#ifndef __x86_64__\n\tpopl KLEN\n\tpopl KEYP\n\tpopl LEN\n\tpopl IVP\n#endif\n\tFRAME_END\n\tRET\nSYM_FUNC_END(aesni_cts_cbc_dec)\n\n.pushsection .rodata\n.align 16\n.Lcts_permute_table:\n\t.byte\t\t0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80\n\t.byte\t\t0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80\n\t.byte\t\t0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07\n\t.byte\t\t0x08, 0x09, 0x0a, 0x0b, 0x0c, 0x0d, 0x0e, 0x0f\n\t.byte\t\t0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80\n\t.byte\t\t0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80, 0x80\n#ifdef __x86_64__\n.Lbswap_mask:\n\t.byte 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0\n#endif\n.popsection\n\n#ifdef __x86_64__\n \nSYM_FUNC_START_LOCAL(_aesni_inc_init)\n\tmovaps .Lbswap_mask(%rip), BSWAP_MASK\n\tmovaps IV, CTR\n\tpshufb BSWAP_MASK, CTR\n\tmov $1, TCTR_LOW\n\tmovq TCTR_LOW, INC\n\tmovq CTR, TCTR_LOW\n\tRET\nSYM_FUNC_END(_aesni_inc_init)\n\n \nSYM_FUNC_START_LOCAL(_aesni_inc)\n\tpaddq INC, CTR\n\tadd $1, TCTR_LOW\n\tjnc .Linc_low\n\tpslldq $8, INC\n\tpaddq INC, CTR\n\tpsrldq $8, INC\n.Linc_low:\n\tmovaps CTR, IV\n\tpshufb BSWAP_MASK, IV\n\tRET\nSYM_FUNC_END(_aesni_inc)\n\n \nSYM_FUNC_START(aesni_ctr_enc)\n\tFRAME_BEGIN\n\tcmp $16, LEN\n\tjb .Lctr_enc_just_ret\n\tmov 480(KEYP), KLEN\n\tmovups (IVP), IV\n\tcall _aesni_inc_init\n\tcmp $64, LEN\n\tjb .Lctr_enc_loop1\n.align 4\n.Lctr_enc_loop4:\n\tmovaps IV, STATE1\n\tcall _aesni_inc\n\tmovups (INP), IN1\n\tmovaps IV, STATE2\n\tcall _aesni_inc\n\tmovups 0x10(INP), IN2\n\tmovaps IV, STATE3\n\tcall _aesni_inc\n\tmovups 0x20(INP), IN3\n\tmovaps IV, STATE4\n\tcall _aesni_inc\n\tmovups 0x30(INP), IN4\n\tcall _aesni_enc4\n\tpxor IN1, STATE1\n\tmovups STATE1, (OUTP)\n\tpxor IN2, STATE2\n\tmovups STATE2, 0x10(OUTP)\n\tpxor IN3, STATE3\n\tmovups STATE3, 0x20(OUTP)\n\tpxor IN4, STATE4\n\tmovups STATE4, 0x30(OUTP)\n\tsub $64, LEN\n\tadd $64, INP\n\tadd $64, OUTP\n\tcmp $64, LEN\n\tjge .Lctr_enc_loop4\n\tcmp $16, LEN\n\tjb .Lctr_enc_ret\n.align 4\n.Lctr_enc_loop1:\n\tmovaps IV, STATE\n\tcall _aesni_inc\n\tmovups (INP), IN\n\tcall _aesni_enc1\n\tpxor IN, STATE\n\tmovups STATE, (OUTP)\n\tsub $16, LEN\n\tadd $16, INP\n\tadd $16, OUTP\n\tcmp $16, LEN\n\tjge .Lctr_enc_loop1\n.Lctr_enc_ret:\n\tmovups IV, (IVP)\n.Lctr_enc_just_ret:\n\tFRAME_END\n\tRET\nSYM_FUNC_END(aesni_ctr_enc)\n\n#endif\n\n.section\t.rodata.cst16.gf128mul_x_ble_mask, \"aM\", @progbits, 16\n.align 16\n.Lgf128mul_x_ble_mask:\n\t.octa 0x00000000000000010000000000000087\n.previous\n\n \n#define _aesni_gf128mul_x_ble() \\\n\tpshufd $0x13, IV, KEY; \\\n\tpaddq IV, IV; \\\n\tpsrad $31, KEY; \\\n\tpand GF128MUL_MASK, KEY; \\\n\tpxor KEY, IV;\n\n \nSYM_FUNC_START(aesni_xts_encrypt)\n\tFRAME_BEGIN\n#ifndef __x86_64__\n\tpushl IVP\n\tpushl LEN\n\tpushl KEYP\n\tpushl KLEN\n\tmovl (FRAME_OFFSET+20)(%esp), KEYP\t# ctx\n\tmovl (FRAME_OFFSET+24)(%esp), OUTP\t# dst\n\tmovl (FRAME_OFFSET+28)(%esp), INP\t# src\n\tmovl (FRAME_OFFSET+32)(%esp), LEN\t# len\n\tmovl (FRAME_OFFSET+36)(%esp), IVP\t# iv\n\tmovdqa .Lgf128mul_x_ble_mask, GF128MUL_MASK\n#else\n\tmovdqa .Lgf128mul_x_ble_mask(%rip), GF128MUL_MASK\n#endif\n\tmovups (IVP), IV\n\n\tmov 480(KEYP), KLEN\n\n.Lxts_enc_loop4:\n\tsub $64, LEN\n\tjl .Lxts_enc_1x\n\n\tmovdqa IV, STATE1\n\tmovdqu 0x00(INP), IN\n\tpxor IN, STATE1\n\tmovdqu IV, 0x00(OUTP)\n\n\t_aesni_gf128mul_x_ble()\n\tmovdqa IV, STATE2\n\tmovdqu 0x10(INP), IN\n\tpxor IN, STATE2\n\tmovdqu IV, 0x10(OUTP)\n\n\t_aesni_gf128mul_x_ble()\n\tmovdqa IV, STATE3\n\tmovdqu 0x20(INP), IN\n\tpxor IN, STATE3\n\tmovdqu IV, 0x20(OUTP)\n\n\t_aesni_gf128mul_x_ble()\n\tmovdqa IV, STATE4\n\tmovdqu 0x30(INP), IN\n\tpxor IN, STATE4\n\tmovdqu IV, 0x30(OUTP)\n\n\tcall _aesni_enc4\n\n\tmovdqu 0x00(OUTP), IN\n\tpxor IN, STATE1\n\tmovdqu STATE1, 0x00(OUTP)\n\n\tmovdqu 0x10(OUTP), IN\n\tpxor IN, STATE2\n\tmovdqu STATE2, 0x10(OUTP)\n\n\tmovdqu 0x20(OUTP), IN\n\tpxor IN, STATE3\n\tmovdqu STATE3, 0x20(OUTP)\n\n\tmovdqu 0x30(OUTP), IN\n\tpxor IN, STATE4\n\tmovdqu STATE4, 0x30(OUTP)\n\n\t_aesni_gf128mul_x_ble()\n\n\tadd $64, INP\n\tadd $64, OUTP\n\ttest LEN, LEN\n\tjnz .Lxts_enc_loop4\n\n.Lxts_enc_ret_iv:\n\tmovups IV, (IVP)\n\n.Lxts_enc_ret:\n#ifndef __x86_64__\n\tpopl KLEN\n\tpopl KEYP\n\tpopl LEN\n\tpopl IVP\n#endif\n\tFRAME_END\n\tRET\n\n.Lxts_enc_1x:\n\tadd $64, LEN\n\tjz .Lxts_enc_ret_iv\n\tsub $16, LEN\n\tjl .Lxts_enc_cts4\n\n.Lxts_enc_loop1:\n\tmovdqu (INP), STATE\n\tpxor IV, STATE\n\tcall _aesni_enc1\n\tpxor IV, STATE\n\t_aesni_gf128mul_x_ble()\n\n\ttest LEN, LEN\n\tjz .Lxts_enc_out\n\n\tadd $16, INP\n\tsub $16, LEN\n\tjl .Lxts_enc_cts1\n\n\tmovdqu STATE, (OUTP)\n\tadd $16, OUTP\n\tjmp .Lxts_enc_loop1\n\n.Lxts_enc_out:\n\tmovdqu STATE, (OUTP)\n\tjmp .Lxts_enc_ret_iv\n\n.Lxts_enc_cts4:\n\tmovdqa STATE4, STATE\n\tsub $16, OUTP\n\n.Lxts_enc_cts1:\n#ifndef __x86_64__\n\tlea .Lcts_permute_table, T1\n#else\n\tlea .Lcts_permute_table(%rip), T1\n#endif\n\tadd LEN, INP\t\t \n\tadd $16, LEN\t\t \n\tmovups (INP), IN1\n\n\tmov T1, IVP\n\tadd $32, IVP\n\tadd LEN, T1\n\tsub LEN, IVP\n\tadd OUTP, LEN\n\n\tmovups (T1), %xmm4\n\tmovaps STATE, IN2\n\tpshufb %xmm4, STATE\n\tmovups STATE, (LEN)\n\n\tmovups (IVP), %xmm0\n\tpshufb %xmm0, IN1\n\tpblendvb IN2, IN1\n\tmovaps IN1, STATE\n\n\tpxor IV, STATE\n\tcall _aesni_enc1\n\tpxor IV, STATE\n\n\tmovups STATE, (OUTP)\n\tjmp .Lxts_enc_ret\nSYM_FUNC_END(aesni_xts_encrypt)\n\n \nSYM_FUNC_START(aesni_xts_decrypt)\n\tFRAME_BEGIN\n#ifndef __x86_64__\n\tpushl IVP\n\tpushl LEN\n\tpushl KEYP\n\tpushl KLEN\n\tmovl (FRAME_OFFSET+20)(%esp), KEYP\t# ctx\n\tmovl (FRAME_OFFSET+24)(%esp), OUTP\t# dst\n\tmovl (FRAME_OFFSET+28)(%esp), INP\t# src\n\tmovl (FRAME_OFFSET+32)(%esp), LEN\t# len\n\tmovl (FRAME_OFFSET+36)(%esp), IVP\t# iv\n\tmovdqa .Lgf128mul_x_ble_mask, GF128MUL_MASK\n#else\n\tmovdqa .Lgf128mul_x_ble_mask(%rip), GF128MUL_MASK\n#endif\n\tmovups (IVP), IV\n\n\tmov 480(KEYP), KLEN\n\tadd $240, KEYP\n\n\ttest $15, LEN\n\tjz .Lxts_dec_loop4\n\tsub $16, LEN\n\n.Lxts_dec_loop4:\n\tsub $64, LEN\n\tjl .Lxts_dec_1x\n\n\tmovdqa IV, STATE1\n\tmovdqu 0x00(INP), IN\n\tpxor IN, STATE1\n\tmovdqu IV, 0x00(OUTP)\n\n\t_aesni_gf128mul_x_ble()\n\tmovdqa IV, STATE2\n\tmovdqu 0x10(INP), IN\n\tpxor IN, STATE2\n\tmovdqu IV, 0x10(OUTP)\n\n\t_aesni_gf128mul_x_ble()\n\tmovdqa IV, STATE3\n\tmovdqu 0x20(INP), IN\n\tpxor IN, STATE3\n\tmovdqu IV, 0x20(OUTP)\n\n\t_aesni_gf128mul_x_ble()\n\tmovdqa IV, STATE4\n\tmovdqu 0x30(INP), IN\n\tpxor IN, STATE4\n\tmovdqu IV, 0x30(OUTP)\n\n\tcall _aesni_dec4\n\n\tmovdqu 0x00(OUTP), IN\n\tpxor IN, STATE1\n\tmovdqu STATE1, 0x00(OUTP)\n\n\tmovdqu 0x10(OUTP), IN\n\tpxor IN, STATE2\n\tmovdqu STATE2, 0x10(OUTP)\n\n\tmovdqu 0x20(OUTP), IN\n\tpxor IN, STATE3\n\tmovdqu STATE3, 0x20(OUTP)\n\n\tmovdqu 0x30(OUTP), IN\n\tpxor IN, STATE4\n\tmovdqu STATE4, 0x30(OUTP)\n\n\t_aesni_gf128mul_x_ble()\n\n\tadd $64, INP\n\tadd $64, OUTP\n\ttest LEN, LEN\n\tjnz .Lxts_dec_loop4\n\n.Lxts_dec_ret_iv:\n\tmovups IV, (IVP)\n\n.Lxts_dec_ret:\n#ifndef __x86_64__\n\tpopl KLEN\n\tpopl KEYP\n\tpopl LEN\n\tpopl IVP\n#endif\n\tFRAME_END\n\tRET\n\n.Lxts_dec_1x:\n\tadd $64, LEN\n\tjz .Lxts_dec_ret_iv\n\n.Lxts_dec_loop1:\n\tmovdqu (INP), STATE\n\n\tadd $16, INP\n\tsub $16, LEN\n\tjl .Lxts_dec_cts1\n\n\tpxor IV, STATE\n\tcall _aesni_dec1\n\tpxor IV, STATE\n\t_aesni_gf128mul_x_ble()\n\n\ttest LEN, LEN\n\tjz .Lxts_dec_out\n\n\tmovdqu STATE, (OUTP)\n\tadd $16, OUTP\n\tjmp .Lxts_dec_loop1\n\n.Lxts_dec_out:\n\tmovdqu STATE, (OUTP)\n\tjmp .Lxts_dec_ret_iv\n\n.Lxts_dec_cts1:\n\tmovdqa IV, STATE4\n\t_aesni_gf128mul_x_ble()\n\n\tpxor IV, STATE\n\tcall _aesni_dec1\n\tpxor IV, STATE\n\n#ifndef __x86_64__\n\tlea .Lcts_permute_table, T1\n#else\n\tlea .Lcts_permute_table(%rip), T1\n#endif\n\tadd LEN, INP\t\t \n\tadd $16, LEN\t\t \n\tmovups (INP), IN1\n\n\tmov T1, IVP\n\tadd $32, IVP\n\tadd LEN, T1\n\tsub LEN, IVP\n\tadd OUTP, LEN\n\n\tmovups (T1), %xmm4\n\tmovaps STATE, IN2\n\tpshufb %xmm4, STATE\n\tmovups STATE, (LEN)\n\n\tmovups (IVP), %xmm0\n\tpshufb %xmm0, IN1\n\tpblendvb IN2, IN1\n\tmovaps IN1, STATE\n\n\tpxor STATE4, STATE\n\tcall _aesni_dec1\n\tpxor STATE4, STATE\n\n\tmovups STATE, (OUTP)\n\tjmp .Lxts_dec_ret\nSYM_FUNC_END(aesni_xts_decrypt)\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}