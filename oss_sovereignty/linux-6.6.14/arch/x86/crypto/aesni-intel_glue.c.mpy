{
  "module_name": "aesni-intel_glue.c",
  "hash_id": "0bbc77b8d833027ad61c06405dbcae366f6e12d7c65d6ffeb826a0cbe495c38c",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/crypto/aesni-intel_glue.c",
  "human_readable_source": "\n \n\n#include <linux/hardirq.h>\n#include <linux/types.h>\n#include <linux/module.h>\n#include <linux/err.h>\n#include <crypto/algapi.h>\n#include <crypto/aes.h>\n#include <crypto/ctr.h>\n#include <crypto/b128ops.h>\n#include <crypto/gcm.h>\n#include <crypto/xts.h>\n#include <asm/cpu_device_id.h>\n#include <asm/simd.h>\n#include <crypto/scatterwalk.h>\n#include <crypto/internal/aead.h>\n#include <crypto/internal/simd.h>\n#include <crypto/internal/skcipher.h>\n#include <linux/jump_label.h>\n#include <linux/workqueue.h>\n#include <linux/spinlock.h>\n#include <linux/static_call.h>\n\n\n#define AESNI_ALIGN\t16\n#define AESNI_ALIGN_ATTR __attribute__ ((__aligned__(AESNI_ALIGN)))\n#define AES_BLOCK_MASK\t(~(AES_BLOCK_SIZE - 1))\n#define RFC4106_HASH_SUBKEY_SIZE 16\n#define AESNI_ALIGN_EXTRA ((AESNI_ALIGN - 1) & ~(CRYPTO_MINALIGN - 1))\n#define CRYPTO_AES_CTX_SIZE (sizeof(struct crypto_aes_ctx) + AESNI_ALIGN_EXTRA)\n#define XTS_AES_CTX_SIZE (sizeof(struct aesni_xts_ctx) + AESNI_ALIGN_EXTRA)\n\n \nstruct aesni_rfc4106_gcm_ctx {\n\tu8 hash_subkey[16] AESNI_ALIGN_ATTR;\n\tstruct crypto_aes_ctx aes_key_expanded AESNI_ALIGN_ATTR;\n\tu8 nonce[4];\n};\n\nstruct generic_gcmaes_ctx {\n\tu8 hash_subkey[16] AESNI_ALIGN_ATTR;\n\tstruct crypto_aes_ctx aes_key_expanded AESNI_ALIGN_ATTR;\n};\n\nstruct aesni_xts_ctx {\n\tu8 raw_tweak_ctx[sizeof(struct crypto_aes_ctx)] AESNI_ALIGN_ATTR;\n\tu8 raw_crypt_ctx[sizeof(struct crypto_aes_ctx)] AESNI_ALIGN_ATTR;\n};\n\n#define GCM_BLOCK_LEN 16\n\nstruct gcm_context_data {\n\t \n\tu8 aad_hash[GCM_BLOCK_LEN];\n\tu64 aad_length;\n\tu64 in_length;\n\tu8 partial_block_enc_key[GCM_BLOCK_LEN];\n\tu8 orig_IV[GCM_BLOCK_LEN];\n\tu8 current_counter[GCM_BLOCK_LEN];\n\tu64 partial_block_len;\n\tu64 unused;\n\tu8 hash_keys[GCM_BLOCK_LEN * 16];\n};\n\nasmlinkage int aesni_set_key(struct crypto_aes_ctx *ctx, const u8 *in_key,\n\t\t\t     unsigned int key_len);\nasmlinkage void aesni_enc(const void *ctx, u8 *out, const u8 *in);\nasmlinkage void aesni_dec(const void *ctx, u8 *out, const u8 *in);\nasmlinkage void aesni_ecb_enc(struct crypto_aes_ctx *ctx, u8 *out,\n\t\t\t      const u8 *in, unsigned int len);\nasmlinkage void aesni_ecb_dec(struct crypto_aes_ctx *ctx, u8 *out,\n\t\t\t      const u8 *in, unsigned int len);\nasmlinkage void aesni_cbc_enc(struct crypto_aes_ctx *ctx, u8 *out,\n\t\t\t      const u8 *in, unsigned int len, u8 *iv);\nasmlinkage void aesni_cbc_dec(struct crypto_aes_ctx *ctx, u8 *out,\n\t\t\t      const u8 *in, unsigned int len, u8 *iv);\nasmlinkage void aesni_cts_cbc_enc(struct crypto_aes_ctx *ctx, u8 *out,\n\t\t\t\t  const u8 *in, unsigned int len, u8 *iv);\nasmlinkage void aesni_cts_cbc_dec(struct crypto_aes_ctx *ctx, u8 *out,\n\t\t\t\t  const u8 *in, unsigned int len, u8 *iv);\n\n#define AVX_GEN2_OPTSIZE 640\n#define AVX_GEN4_OPTSIZE 4096\n\nasmlinkage void aesni_xts_encrypt(const struct crypto_aes_ctx *ctx, u8 *out,\n\t\t\t\t  const u8 *in, unsigned int len, u8 *iv);\n\nasmlinkage void aesni_xts_decrypt(const struct crypto_aes_ctx *ctx, u8 *out,\n\t\t\t\t  const u8 *in, unsigned int len, u8 *iv);\n\n#ifdef CONFIG_X86_64\n\nasmlinkage void aesni_ctr_enc(struct crypto_aes_ctx *ctx, u8 *out,\n\t\t\t      const u8 *in, unsigned int len, u8 *iv);\nDEFINE_STATIC_CALL(aesni_ctr_enc_tfm, aesni_ctr_enc);\n\n \nasmlinkage void aesni_gcm_init(void *ctx,\n\t\t\t       struct gcm_context_data *gdata,\n\t\t\t       u8 *iv,\n\t\t\t       u8 *hash_subkey, const u8 *aad,\n\t\t\t       unsigned long aad_len);\nasmlinkage void aesni_gcm_enc_update(void *ctx,\n\t\t\t\t     struct gcm_context_data *gdata, u8 *out,\n\t\t\t\t     const u8 *in, unsigned long plaintext_len);\nasmlinkage void aesni_gcm_dec_update(void *ctx,\n\t\t\t\t     struct gcm_context_data *gdata, u8 *out,\n\t\t\t\t     const u8 *in,\n\t\t\t\t     unsigned long ciphertext_len);\nasmlinkage void aesni_gcm_finalize(void *ctx,\n\t\t\t\t   struct gcm_context_data *gdata,\n\t\t\t\t   u8 *auth_tag, unsigned long auth_tag_len);\n\nasmlinkage void aes_ctr_enc_128_avx_by8(const u8 *in, u8 *iv,\n\t\tvoid *keys, u8 *out, unsigned int num_bytes);\nasmlinkage void aes_ctr_enc_192_avx_by8(const u8 *in, u8 *iv,\n\t\tvoid *keys, u8 *out, unsigned int num_bytes);\nasmlinkage void aes_ctr_enc_256_avx_by8(const u8 *in, u8 *iv,\n\t\tvoid *keys, u8 *out, unsigned int num_bytes);\n\n\nasmlinkage void aes_xctr_enc_128_avx_by8(const u8 *in, const u8 *iv,\n\tconst void *keys, u8 *out, unsigned int num_bytes,\n\tunsigned int byte_ctr);\n\nasmlinkage void aes_xctr_enc_192_avx_by8(const u8 *in, const u8 *iv,\n\tconst void *keys, u8 *out, unsigned int num_bytes,\n\tunsigned int byte_ctr);\n\nasmlinkage void aes_xctr_enc_256_avx_by8(const u8 *in, const u8 *iv,\n\tconst void *keys, u8 *out, unsigned int num_bytes,\n\tunsigned int byte_ctr);\n\n \nasmlinkage void aesni_gcm_init_avx_gen2(void *my_ctx_data,\n\t\t\t\t\tstruct gcm_context_data *gdata,\n\t\t\t\t\tu8 *iv,\n\t\t\t\t\tu8 *hash_subkey,\n\t\t\t\t\tconst u8 *aad,\n\t\t\t\t\tunsigned long aad_len);\n\nasmlinkage void aesni_gcm_enc_update_avx_gen2(void *ctx,\n\t\t\t\t     struct gcm_context_data *gdata, u8 *out,\n\t\t\t\t     const u8 *in, unsigned long plaintext_len);\nasmlinkage void aesni_gcm_dec_update_avx_gen2(void *ctx,\n\t\t\t\t     struct gcm_context_data *gdata, u8 *out,\n\t\t\t\t     const u8 *in,\n\t\t\t\t     unsigned long ciphertext_len);\nasmlinkage void aesni_gcm_finalize_avx_gen2(void *ctx,\n\t\t\t\t   struct gcm_context_data *gdata,\n\t\t\t\t   u8 *auth_tag, unsigned long auth_tag_len);\n\n \nasmlinkage void aesni_gcm_init_avx_gen4(void *my_ctx_data,\n\t\t\t\t\tstruct gcm_context_data *gdata,\n\t\t\t\t\tu8 *iv,\n\t\t\t\t\tu8 *hash_subkey,\n\t\t\t\t\tconst u8 *aad,\n\t\t\t\t\tunsigned long aad_len);\n\nasmlinkage void aesni_gcm_enc_update_avx_gen4(void *ctx,\n\t\t\t\t     struct gcm_context_data *gdata, u8 *out,\n\t\t\t\t     const u8 *in, unsigned long plaintext_len);\nasmlinkage void aesni_gcm_dec_update_avx_gen4(void *ctx,\n\t\t\t\t     struct gcm_context_data *gdata, u8 *out,\n\t\t\t\t     const u8 *in,\n\t\t\t\t     unsigned long ciphertext_len);\nasmlinkage void aesni_gcm_finalize_avx_gen4(void *ctx,\n\t\t\t\t   struct gcm_context_data *gdata,\n\t\t\t\t   u8 *auth_tag, unsigned long auth_tag_len);\n\nstatic __ro_after_init DEFINE_STATIC_KEY_FALSE(gcm_use_avx);\nstatic __ro_after_init DEFINE_STATIC_KEY_FALSE(gcm_use_avx2);\n\nstatic inline struct\naesni_rfc4106_gcm_ctx *aesni_rfc4106_gcm_ctx_get(struct crypto_aead *tfm)\n{\n\tunsigned long align = AESNI_ALIGN;\n\n\tif (align <= crypto_tfm_ctx_alignment())\n\t\talign = 1;\n\treturn PTR_ALIGN(crypto_aead_ctx(tfm), align);\n}\n\nstatic inline struct\ngeneric_gcmaes_ctx *generic_gcmaes_ctx_get(struct crypto_aead *tfm)\n{\n\tunsigned long align = AESNI_ALIGN;\n\n\tif (align <= crypto_tfm_ctx_alignment())\n\t\talign = 1;\n\treturn PTR_ALIGN(crypto_aead_ctx(tfm), align);\n}\n#endif\n\nstatic inline struct crypto_aes_ctx *aes_ctx(void *raw_ctx)\n{\n\tunsigned long addr = (unsigned long)raw_ctx;\n\tunsigned long align = AESNI_ALIGN;\n\n\tif (align <= crypto_tfm_ctx_alignment())\n\t\talign = 1;\n\treturn (struct crypto_aes_ctx *)ALIGN(addr, align);\n}\n\nstatic int aes_set_key_common(struct crypto_aes_ctx *ctx,\n\t\t\t      const u8 *in_key, unsigned int key_len)\n{\n\tint err;\n\n\tif (key_len != AES_KEYSIZE_128 && key_len != AES_KEYSIZE_192 &&\n\t    key_len != AES_KEYSIZE_256)\n\t\treturn -EINVAL;\n\n\tif (!crypto_simd_usable())\n\t\terr = aes_expandkey(ctx, in_key, key_len);\n\telse {\n\t\tkernel_fpu_begin();\n\t\terr = aesni_set_key(ctx, in_key, key_len);\n\t\tkernel_fpu_end();\n\t}\n\n\treturn err;\n}\n\nstatic int aes_set_key(struct crypto_tfm *tfm, const u8 *in_key,\n\t\t       unsigned int key_len)\n{\n\treturn aes_set_key_common(aes_ctx(crypto_tfm_ctx(tfm)), in_key,\n\t\t\t\t  key_len);\n}\n\nstatic void aesni_encrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src)\n{\n\tstruct crypto_aes_ctx *ctx = aes_ctx(crypto_tfm_ctx(tfm));\n\n\tif (!crypto_simd_usable()) {\n\t\taes_encrypt(ctx, dst, src);\n\t} else {\n\t\tkernel_fpu_begin();\n\t\taesni_enc(ctx, dst, src);\n\t\tkernel_fpu_end();\n\t}\n}\n\nstatic void aesni_decrypt(struct crypto_tfm *tfm, u8 *dst, const u8 *src)\n{\n\tstruct crypto_aes_ctx *ctx = aes_ctx(crypto_tfm_ctx(tfm));\n\n\tif (!crypto_simd_usable()) {\n\t\taes_decrypt(ctx, dst, src);\n\t} else {\n\t\tkernel_fpu_begin();\n\t\taesni_dec(ctx, dst, src);\n\t\tkernel_fpu_end();\n\t}\n}\n\nstatic int aesni_skcipher_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t         unsigned int len)\n{\n\treturn aes_set_key_common(aes_ctx(crypto_skcipher_ctx(tfm)), key, len);\n}\n\nstatic int ecb_encrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct crypto_aes_ctx *ctx = aes_ctx(crypto_skcipher_ctx(tfm));\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes)) {\n\t\tkernel_fpu_begin();\n\t\taesni_ecb_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,\n\t\t\t      nbytes & AES_BLOCK_MASK);\n\t\tkernel_fpu_end();\n\t\tnbytes &= AES_BLOCK_SIZE - 1;\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\n\treturn err;\n}\n\nstatic int ecb_decrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct crypto_aes_ctx *ctx = aes_ctx(crypto_skcipher_ctx(tfm));\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes)) {\n\t\tkernel_fpu_begin();\n\t\taesni_ecb_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,\n\t\t\t      nbytes & AES_BLOCK_MASK);\n\t\tkernel_fpu_end();\n\t\tnbytes &= AES_BLOCK_SIZE - 1;\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\n\treturn err;\n}\n\nstatic int cbc_encrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct crypto_aes_ctx *ctx = aes_ctx(crypto_skcipher_ctx(tfm));\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes)) {\n\t\tkernel_fpu_begin();\n\t\taesni_cbc_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,\n\t\t\t      nbytes & AES_BLOCK_MASK, walk.iv);\n\t\tkernel_fpu_end();\n\t\tnbytes &= AES_BLOCK_SIZE - 1;\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\n\treturn err;\n}\n\nstatic int cbc_decrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct crypto_aes_ctx *ctx = aes_ctx(crypto_skcipher_ctx(tfm));\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes)) {\n\t\tkernel_fpu_begin();\n\t\taesni_cbc_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,\n\t\t\t      nbytes & AES_BLOCK_MASK, walk.iv);\n\t\tkernel_fpu_end();\n\t\tnbytes &= AES_BLOCK_SIZE - 1;\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\n\treturn err;\n}\n\nstatic int cts_cbc_encrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct crypto_aes_ctx *ctx = aes_ctx(crypto_skcipher_ctx(tfm));\n\tint cbc_blocks = DIV_ROUND_UP(req->cryptlen, AES_BLOCK_SIZE) - 2;\n\tstruct scatterlist *src = req->src, *dst = req->dst;\n\tstruct scatterlist sg_src[2], sg_dst[2];\n\tstruct skcipher_request subreq;\n\tstruct skcipher_walk walk;\n\tint err;\n\n\tskcipher_request_set_tfm(&subreq, tfm);\n\tskcipher_request_set_callback(&subreq, skcipher_request_flags(req),\n\t\t\t\t      NULL, NULL);\n\n\tif (req->cryptlen <= AES_BLOCK_SIZE) {\n\t\tif (req->cryptlen < AES_BLOCK_SIZE)\n\t\t\treturn -EINVAL;\n\t\tcbc_blocks = 1;\n\t}\n\n\tif (cbc_blocks > 0) {\n\t\tskcipher_request_set_crypt(&subreq, req->src, req->dst,\n\t\t\t\t\t   cbc_blocks * AES_BLOCK_SIZE,\n\t\t\t\t\t   req->iv);\n\n\t\terr = cbc_encrypt(&subreq);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (req->cryptlen == AES_BLOCK_SIZE)\n\t\t\treturn 0;\n\n\t\tdst = src = scatterwalk_ffwd(sg_src, req->src, subreq.cryptlen);\n\t\tif (req->dst != req->src)\n\t\t\tdst = scatterwalk_ffwd(sg_dst, req->dst,\n\t\t\t\t\t       subreq.cryptlen);\n\t}\n\n\t \n\tskcipher_request_set_crypt(&subreq, src, dst,\n\t\t\t\t   req->cryptlen - cbc_blocks * AES_BLOCK_SIZE,\n\t\t\t\t   req->iv);\n\n\terr = skcipher_walk_virt(&walk, &subreq, false);\n\tif (err)\n\t\treturn err;\n\n\tkernel_fpu_begin();\n\taesni_cts_cbc_enc(ctx, walk.dst.virt.addr, walk.src.virt.addr,\n\t\t\t  walk.nbytes, walk.iv);\n\tkernel_fpu_end();\n\n\treturn skcipher_walk_done(&walk, 0);\n}\n\nstatic int cts_cbc_decrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct crypto_aes_ctx *ctx = aes_ctx(crypto_skcipher_ctx(tfm));\n\tint cbc_blocks = DIV_ROUND_UP(req->cryptlen, AES_BLOCK_SIZE) - 2;\n\tstruct scatterlist *src = req->src, *dst = req->dst;\n\tstruct scatterlist sg_src[2], sg_dst[2];\n\tstruct skcipher_request subreq;\n\tstruct skcipher_walk walk;\n\tint err;\n\n\tskcipher_request_set_tfm(&subreq, tfm);\n\tskcipher_request_set_callback(&subreq, skcipher_request_flags(req),\n\t\t\t\t      NULL, NULL);\n\n\tif (req->cryptlen <= AES_BLOCK_SIZE) {\n\t\tif (req->cryptlen < AES_BLOCK_SIZE)\n\t\t\treturn -EINVAL;\n\t\tcbc_blocks = 1;\n\t}\n\n\tif (cbc_blocks > 0) {\n\t\tskcipher_request_set_crypt(&subreq, req->src, req->dst,\n\t\t\t\t\t   cbc_blocks * AES_BLOCK_SIZE,\n\t\t\t\t\t   req->iv);\n\n\t\terr = cbc_decrypt(&subreq);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (req->cryptlen == AES_BLOCK_SIZE)\n\t\t\treturn 0;\n\n\t\tdst = src = scatterwalk_ffwd(sg_src, req->src, subreq.cryptlen);\n\t\tif (req->dst != req->src)\n\t\t\tdst = scatterwalk_ffwd(sg_dst, req->dst,\n\t\t\t\t\t       subreq.cryptlen);\n\t}\n\n\t \n\tskcipher_request_set_crypt(&subreq, src, dst,\n\t\t\t\t   req->cryptlen - cbc_blocks * AES_BLOCK_SIZE,\n\t\t\t\t   req->iv);\n\n\terr = skcipher_walk_virt(&walk, &subreq, false);\n\tif (err)\n\t\treturn err;\n\n\tkernel_fpu_begin();\n\taesni_cts_cbc_dec(ctx, walk.dst.virt.addr, walk.src.virt.addr,\n\t\t\t  walk.nbytes, walk.iv);\n\tkernel_fpu_end();\n\n\treturn skcipher_walk_done(&walk, 0);\n}\n\n#ifdef CONFIG_X86_64\nstatic void aesni_ctr_enc_avx_tfm(struct crypto_aes_ctx *ctx, u8 *out,\n\t\t\t      const u8 *in, unsigned int len, u8 *iv)\n{\n\t \n\tif (ctx->key_length == AES_KEYSIZE_128)\n\t\taes_ctr_enc_128_avx_by8(in, iv, (void *)ctx, out, len);\n\telse if (ctx->key_length == AES_KEYSIZE_192)\n\t\taes_ctr_enc_192_avx_by8(in, iv, (void *)ctx, out, len);\n\telse\n\t\taes_ctr_enc_256_avx_by8(in, iv, (void *)ctx, out, len);\n}\n\nstatic int ctr_crypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct crypto_aes_ctx *ctx = aes_ctx(crypto_skcipher_ctx(tfm));\n\tu8 keystream[AES_BLOCK_SIZE];\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes) > 0) {\n\t\tkernel_fpu_begin();\n\t\tif (nbytes & AES_BLOCK_MASK)\n\t\t\tstatic_call(aesni_ctr_enc_tfm)(ctx, walk.dst.virt.addr,\n\t\t\t\t\t\t       walk.src.virt.addr,\n\t\t\t\t\t\t       nbytes & AES_BLOCK_MASK,\n\t\t\t\t\t\t       walk.iv);\n\t\tnbytes &= ~AES_BLOCK_MASK;\n\n\t\tif (walk.nbytes == walk.total && nbytes > 0) {\n\t\t\taesni_enc(ctx, keystream, walk.iv);\n\t\t\tcrypto_xor_cpy(walk.dst.virt.addr + walk.nbytes - nbytes,\n\t\t\t\t       walk.src.virt.addr + walk.nbytes - nbytes,\n\t\t\t\t       keystream, nbytes);\n\t\t\tcrypto_inc(walk.iv, AES_BLOCK_SIZE);\n\t\t\tnbytes = 0;\n\t\t}\n\t\tkernel_fpu_end();\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\treturn err;\n}\n\nstatic void aesni_xctr_enc_avx_tfm(struct crypto_aes_ctx *ctx, u8 *out,\n\t\t\t\t   const u8 *in, unsigned int len, u8 *iv,\n\t\t\t\t   unsigned int byte_ctr)\n{\n\tif (ctx->key_length == AES_KEYSIZE_128)\n\t\taes_xctr_enc_128_avx_by8(in, iv, (void *)ctx, out, len,\n\t\t\t\t\t byte_ctr);\n\telse if (ctx->key_length == AES_KEYSIZE_192)\n\t\taes_xctr_enc_192_avx_by8(in, iv, (void *)ctx, out, len,\n\t\t\t\t\t byte_ctr);\n\telse\n\t\taes_xctr_enc_256_avx_by8(in, iv, (void *)ctx, out, len,\n\t\t\t\t\t byte_ctr);\n}\n\nstatic int xctr_crypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct crypto_aes_ctx *ctx = aes_ctx(crypto_skcipher_ctx(tfm));\n\tu8 keystream[AES_BLOCK_SIZE];\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tunsigned int byte_ctr = 0;\n\tint err;\n\t__le32 block[AES_BLOCK_SIZE / sizeof(__le32)];\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes) > 0) {\n\t\tkernel_fpu_begin();\n\t\tif (nbytes & AES_BLOCK_MASK)\n\t\t\taesni_xctr_enc_avx_tfm(ctx, walk.dst.virt.addr,\n\t\t\t\twalk.src.virt.addr, nbytes & AES_BLOCK_MASK,\n\t\t\t\twalk.iv, byte_ctr);\n\t\tnbytes &= ~AES_BLOCK_MASK;\n\t\tbyte_ctr += walk.nbytes - nbytes;\n\n\t\tif (walk.nbytes == walk.total && nbytes > 0) {\n\t\t\tmemcpy(block, walk.iv, AES_BLOCK_SIZE);\n\t\t\tblock[0] ^= cpu_to_le32(1 + byte_ctr / AES_BLOCK_SIZE);\n\t\t\taesni_enc(ctx, keystream, (u8 *)block);\n\t\t\tcrypto_xor_cpy(walk.dst.virt.addr + walk.nbytes -\n\t\t\t\t       nbytes, walk.src.virt.addr + walk.nbytes\n\t\t\t\t       - nbytes, keystream, nbytes);\n\t\t\tbyte_ctr += nbytes;\n\t\t\tnbytes = 0;\n\t\t}\n\t\tkernel_fpu_end();\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\treturn err;\n}\n\nstatic int\nrfc4106_set_hash_subkey(u8 *hash_subkey, const u8 *key, unsigned int key_len)\n{\n\tstruct crypto_aes_ctx ctx;\n\tint ret;\n\n\tret = aes_expandkey(&ctx, key, key_len);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\t \n\tmemset(hash_subkey, 0, RFC4106_HASH_SUBKEY_SIZE);\n\n\taes_encrypt(&ctx, hash_subkey, hash_subkey);\n\n\tmemzero_explicit(&ctx, sizeof(ctx));\n\treturn 0;\n}\n\nstatic int common_rfc4106_set_key(struct crypto_aead *aead, const u8 *key,\n\t\t\t\t  unsigned int key_len)\n{\n\tstruct aesni_rfc4106_gcm_ctx *ctx = aesni_rfc4106_gcm_ctx_get(aead);\n\n\tif (key_len < 4)\n\t\treturn -EINVAL;\n\n\t \n\tkey_len -= 4;\n\n\tmemcpy(ctx->nonce, key + key_len, sizeof(ctx->nonce));\n\n\treturn aes_set_key_common(&ctx->aes_key_expanded, key, key_len) ?:\n\t       rfc4106_set_hash_subkey(ctx->hash_subkey, key, key_len);\n}\n\n \nstatic int common_rfc4106_set_authsize(struct crypto_aead *aead,\n\t\t\t\t       unsigned int authsize)\n{\n\tswitch (authsize) {\n\tcase 8:\n\tcase 12:\n\tcase 16:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int generic_gcmaes_set_authsize(struct crypto_aead *tfm,\n\t\t\t\t       unsigned int authsize)\n{\n\tswitch (authsize) {\n\tcase 4:\n\tcase 8:\n\tcase 12:\n\tcase 13:\n\tcase 14:\n\tcase 15:\n\tcase 16:\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int gcmaes_crypt_by_sg(bool enc, struct aead_request *req,\n\t\t\t      unsigned int assoclen, u8 *hash_subkey,\n\t\t\t      u8 *iv, void *aes_ctx, u8 *auth_tag,\n\t\t\t      unsigned long auth_tag_len)\n{\n\tu8 databuf[sizeof(struct gcm_context_data) + (AESNI_ALIGN - 8)] __aligned(8);\n\tstruct gcm_context_data *data = PTR_ALIGN((void *)databuf, AESNI_ALIGN);\n\tunsigned long left = req->cryptlen;\n\tstruct scatter_walk assoc_sg_walk;\n\tstruct skcipher_walk walk;\n\tbool do_avx, do_avx2;\n\tu8 *assocmem = NULL;\n\tu8 *assoc;\n\tint err;\n\n\tif (!enc)\n\t\tleft -= auth_tag_len;\n\n\tdo_avx = (left >= AVX_GEN2_OPTSIZE);\n\tdo_avx2 = (left >= AVX_GEN4_OPTSIZE);\n\n\t \n\tif (req->src->length >= assoclen && req->src->length) {\n\t\tscatterwalk_start(&assoc_sg_walk, req->src);\n\t\tassoc = scatterwalk_map(&assoc_sg_walk);\n\t} else {\n\t\tgfp_t flags = (req->base.flags & CRYPTO_TFM_REQ_MAY_SLEEP) ?\n\t\t\t      GFP_KERNEL : GFP_ATOMIC;\n\n\t\t \n\t\tassocmem = kmalloc(assoclen, flags);\n\t\tif (unlikely(!assocmem))\n\t\t\treturn -ENOMEM;\n\t\tassoc = assocmem;\n\n\t\tscatterwalk_map_and_copy(assoc, req->src, 0, assoclen, 0);\n\t}\n\n\tkernel_fpu_begin();\n\tif (static_branch_likely(&gcm_use_avx2) && do_avx2)\n\t\taesni_gcm_init_avx_gen4(aes_ctx, data, iv, hash_subkey, assoc,\n\t\t\t\t\tassoclen);\n\telse if (static_branch_likely(&gcm_use_avx) && do_avx)\n\t\taesni_gcm_init_avx_gen2(aes_ctx, data, iv, hash_subkey, assoc,\n\t\t\t\t\tassoclen);\n\telse\n\t\taesni_gcm_init(aes_ctx, data, iv, hash_subkey, assoc, assoclen);\n\tkernel_fpu_end();\n\n\tif (!assocmem)\n\t\tscatterwalk_unmap(assoc);\n\telse\n\t\tkfree(assocmem);\n\n\terr = enc ? skcipher_walk_aead_encrypt(&walk, req, false)\n\t\t  : skcipher_walk_aead_decrypt(&walk, req, false);\n\n\twhile (walk.nbytes > 0) {\n\t\tkernel_fpu_begin();\n\t\tif (static_branch_likely(&gcm_use_avx2) && do_avx2) {\n\t\t\tif (enc)\n\t\t\t\taesni_gcm_enc_update_avx_gen4(aes_ctx, data,\n\t\t\t\t\t\t\t      walk.dst.virt.addr,\n\t\t\t\t\t\t\t      walk.src.virt.addr,\n\t\t\t\t\t\t\t      walk.nbytes);\n\t\t\telse\n\t\t\t\taesni_gcm_dec_update_avx_gen4(aes_ctx, data,\n\t\t\t\t\t\t\t      walk.dst.virt.addr,\n\t\t\t\t\t\t\t      walk.src.virt.addr,\n\t\t\t\t\t\t\t      walk.nbytes);\n\t\t} else if (static_branch_likely(&gcm_use_avx) && do_avx) {\n\t\t\tif (enc)\n\t\t\t\taesni_gcm_enc_update_avx_gen2(aes_ctx, data,\n\t\t\t\t\t\t\t      walk.dst.virt.addr,\n\t\t\t\t\t\t\t      walk.src.virt.addr,\n\t\t\t\t\t\t\t      walk.nbytes);\n\t\t\telse\n\t\t\t\taesni_gcm_dec_update_avx_gen2(aes_ctx, data,\n\t\t\t\t\t\t\t      walk.dst.virt.addr,\n\t\t\t\t\t\t\t      walk.src.virt.addr,\n\t\t\t\t\t\t\t      walk.nbytes);\n\t\t} else if (enc) {\n\t\t\taesni_gcm_enc_update(aes_ctx, data, walk.dst.virt.addr,\n\t\t\t\t\t     walk.src.virt.addr, walk.nbytes);\n\t\t} else {\n\t\t\taesni_gcm_dec_update(aes_ctx, data, walk.dst.virt.addr,\n\t\t\t\t\t     walk.src.virt.addr, walk.nbytes);\n\t\t}\n\t\tkernel_fpu_end();\n\n\t\terr = skcipher_walk_done(&walk, 0);\n\t}\n\n\tif (err)\n\t\treturn err;\n\n\tkernel_fpu_begin();\n\tif (static_branch_likely(&gcm_use_avx2) && do_avx2)\n\t\taesni_gcm_finalize_avx_gen4(aes_ctx, data, auth_tag,\n\t\t\t\t\t    auth_tag_len);\n\telse if (static_branch_likely(&gcm_use_avx) && do_avx)\n\t\taesni_gcm_finalize_avx_gen2(aes_ctx, data, auth_tag,\n\t\t\t\t\t    auth_tag_len);\n\telse\n\t\taesni_gcm_finalize(aes_ctx, data, auth_tag, auth_tag_len);\n\tkernel_fpu_end();\n\n\treturn 0;\n}\n\nstatic int gcmaes_encrypt(struct aead_request *req, unsigned int assoclen,\n\t\t\t  u8 *hash_subkey, u8 *iv, void *aes_ctx)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tunsigned long auth_tag_len = crypto_aead_authsize(tfm);\n\tu8 auth_tag[16];\n\tint err;\n\n\terr = gcmaes_crypt_by_sg(true, req, assoclen, hash_subkey, iv, aes_ctx,\n\t\t\t\t auth_tag, auth_tag_len);\n\tif (err)\n\t\treturn err;\n\n\tscatterwalk_map_and_copy(auth_tag, req->dst,\n\t\t\t\t req->assoclen + req->cryptlen,\n\t\t\t\t auth_tag_len, 1);\n\treturn 0;\n}\n\nstatic int gcmaes_decrypt(struct aead_request *req, unsigned int assoclen,\n\t\t\t  u8 *hash_subkey, u8 *iv, void *aes_ctx)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tunsigned long auth_tag_len = crypto_aead_authsize(tfm);\n\tu8 auth_tag_msg[16];\n\tu8 auth_tag[16];\n\tint err;\n\n\terr = gcmaes_crypt_by_sg(false, req, assoclen, hash_subkey, iv, aes_ctx,\n\t\t\t\t auth_tag, auth_tag_len);\n\tif (err)\n\t\treturn err;\n\n\t \n\tscatterwalk_map_and_copy(auth_tag_msg, req->src,\n\t\t\t\t req->assoclen + req->cryptlen - auth_tag_len,\n\t\t\t\t auth_tag_len, 0);\n\n\t \n\tif (crypto_memneq(auth_tag_msg, auth_tag, auth_tag_len)) {\n\t\tmemzero_explicit(auth_tag, sizeof(auth_tag));\n\t\treturn -EBADMSG;\n\t}\n\treturn 0;\n}\n\nstatic int helper_rfc4106_encrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct aesni_rfc4106_gcm_ctx *ctx = aesni_rfc4106_gcm_ctx_get(tfm);\n\tvoid *aes_ctx = &(ctx->aes_key_expanded);\n\tu8 ivbuf[16 + (AESNI_ALIGN - 8)] __aligned(8);\n\tu8 *iv = PTR_ALIGN(&ivbuf[0], AESNI_ALIGN);\n\tunsigned int i;\n\t__be32 counter = cpu_to_be32(1);\n\n\t \n\t \n\t \n\tif (unlikely(req->assoclen != 16 && req->assoclen != 20))\n\t\treturn -EINVAL;\n\n\t \n\tfor (i = 0; i < 4; i++)\n\t\t*(iv+i) = ctx->nonce[i];\n\tfor (i = 0; i < 8; i++)\n\t\t*(iv+4+i) = req->iv[i];\n\t*((__be32 *)(iv+12)) = counter;\n\n\treturn gcmaes_encrypt(req, req->assoclen - 8, ctx->hash_subkey, iv,\n\t\t\t      aes_ctx);\n}\n\nstatic int helper_rfc4106_decrypt(struct aead_request *req)\n{\n\t__be32 counter = cpu_to_be32(1);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct aesni_rfc4106_gcm_ctx *ctx = aesni_rfc4106_gcm_ctx_get(tfm);\n\tvoid *aes_ctx = &(ctx->aes_key_expanded);\n\tu8 ivbuf[16 + (AESNI_ALIGN - 8)] __aligned(8);\n\tu8 *iv = PTR_ALIGN(&ivbuf[0], AESNI_ALIGN);\n\tunsigned int i;\n\n\tif (unlikely(req->assoclen != 16 && req->assoclen != 20))\n\t\treturn -EINVAL;\n\n\t \n\t \n\t \n\n\t \n\tfor (i = 0; i < 4; i++)\n\t\t*(iv+i) = ctx->nonce[i];\n\tfor (i = 0; i < 8; i++)\n\t\t*(iv+4+i) = req->iv[i];\n\t*((__be32 *)(iv+12)) = counter;\n\n\treturn gcmaes_decrypt(req, req->assoclen - 8, ctx->hash_subkey, iv,\n\t\t\t      aes_ctx);\n}\n#endif\n\nstatic int xts_aesni_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\t    unsigned int keylen)\n{\n\tstruct aesni_xts_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tint err;\n\n\terr = xts_verify_key(tfm, key, keylen);\n\tif (err)\n\t\treturn err;\n\n\tkeylen /= 2;\n\n\t \n\terr = aes_set_key_common(aes_ctx(ctx->raw_crypt_ctx), key, keylen);\n\tif (err)\n\t\treturn err;\n\n\t \n\treturn aes_set_key_common(aes_ctx(ctx->raw_tweak_ctx), key + keylen,\n\t\t\t\t  keylen);\n}\n\nstatic int xts_crypt(struct skcipher_request *req, bool encrypt)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct aesni_xts_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tint tail = req->cryptlen % AES_BLOCK_SIZE;\n\tstruct skcipher_request subreq;\n\tstruct skcipher_walk walk;\n\tint err;\n\n\tif (req->cryptlen < AES_BLOCK_SIZE)\n\t\treturn -EINVAL;\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\tif (!walk.nbytes)\n\t\treturn err;\n\n\tif (unlikely(tail > 0 && walk.nbytes < walk.total)) {\n\t\tint blocks = DIV_ROUND_UP(req->cryptlen, AES_BLOCK_SIZE) - 2;\n\n\t\tskcipher_walk_abort(&walk);\n\n\t\tskcipher_request_set_tfm(&subreq, tfm);\n\t\tskcipher_request_set_callback(&subreq,\n\t\t\t\t\t      skcipher_request_flags(req),\n\t\t\t\t\t      NULL, NULL);\n\t\tskcipher_request_set_crypt(&subreq, req->src, req->dst,\n\t\t\t\t\t   blocks * AES_BLOCK_SIZE, req->iv);\n\t\treq = &subreq;\n\n\t\terr = skcipher_walk_virt(&walk, req, false);\n\t\tif (!walk.nbytes)\n\t\t\treturn err;\n\t} else {\n\t\ttail = 0;\n\t}\n\n\tkernel_fpu_begin();\n\n\t \n\taesni_enc(aes_ctx(ctx->raw_tweak_ctx), walk.iv, walk.iv);\n\n\twhile (walk.nbytes > 0) {\n\t\tint nbytes = walk.nbytes;\n\n\t\tif (nbytes < walk.total)\n\t\t\tnbytes &= ~(AES_BLOCK_SIZE - 1);\n\n\t\tif (encrypt)\n\t\t\taesni_xts_encrypt(aes_ctx(ctx->raw_crypt_ctx),\n\t\t\t\t\t  walk.dst.virt.addr, walk.src.virt.addr,\n\t\t\t\t\t  nbytes, walk.iv);\n\t\telse\n\t\t\taesni_xts_decrypt(aes_ctx(ctx->raw_crypt_ctx),\n\t\t\t\t\t  walk.dst.virt.addr, walk.src.virt.addr,\n\t\t\t\t\t  nbytes, walk.iv);\n\t\tkernel_fpu_end();\n\n\t\terr = skcipher_walk_done(&walk, walk.nbytes - nbytes);\n\n\t\tif (walk.nbytes > 0)\n\t\t\tkernel_fpu_begin();\n\t}\n\n\tif (unlikely(tail > 0 && !err)) {\n\t\tstruct scatterlist sg_src[2], sg_dst[2];\n\t\tstruct scatterlist *src, *dst;\n\n\t\tdst = src = scatterwalk_ffwd(sg_src, req->src, req->cryptlen);\n\t\tif (req->dst != req->src)\n\t\t\tdst = scatterwalk_ffwd(sg_dst, req->dst, req->cryptlen);\n\n\t\tskcipher_request_set_crypt(req, src, dst, AES_BLOCK_SIZE + tail,\n\t\t\t\t\t   req->iv);\n\n\t\terr = skcipher_walk_virt(&walk, &subreq, false);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tkernel_fpu_begin();\n\t\tif (encrypt)\n\t\t\taesni_xts_encrypt(aes_ctx(ctx->raw_crypt_ctx),\n\t\t\t\t\t  walk.dst.virt.addr, walk.src.virt.addr,\n\t\t\t\t\t  walk.nbytes, walk.iv);\n\t\telse\n\t\t\taesni_xts_decrypt(aes_ctx(ctx->raw_crypt_ctx),\n\t\t\t\t\t  walk.dst.virt.addr, walk.src.virt.addr,\n\t\t\t\t\t  walk.nbytes, walk.iv);\n\t\tkernel_fpu_end();\n\n\t\terr = skcipher_walk_done(&walk, 0);\n\t}\n\treturn err;\n}\n\nstatic int xts_encrypt(struct skcipher_request *req)\n{\n\treturn xts_crypt(req, true);\n}\n\nstatic int xts_decrypt(struct skcipher_request *req)\n{\n\treturn xts_crypt(req, false);\n}\n\nstatic struct crypto_alg aesni_cipher_alg = {\n\t.cra_name\t\t= \"aes\",\n\t.cra_driver_name\t= \"aes-aesni\",\n\t.cra_priority\t\t= 300,\n\t.cra_flags\t\t= CRYPTO_ALG_TYPE_CIPHER,\n\t.cra_blocksize\t\t= AES_BLOCK_SIZE,\n\t.cra_ctxsize\t\t= CRYPTO_AES_CTX_SIZE,\n\t.cra_module\t\t= THIS_MODULE,\n\t.cra_u\t= {\n\t\t.cipher\t= {\n\t\t\t.cia_min_keysize\t= AES_MIN_KEY_SIZE,\n\t\t\t.cia_max_keysize\t= AES_MAX_KEY_SIZE,\n\t\t\t.cia_setkey\t\t= aes_set_key,\n\t\t\t.cia_encrypt\t\t= aesni_encrypt,\n\t\t\t.cia_decrypt\t\t= aesni_decrypt\n\t\t}\n\t}\n};\n\nstatic struct skcipher_alg aesni_skciphers[] = {\n\t{\n\t\t.base = {\n\t\t\t.cra_name\t\t= \"__ecb(aes)\",\n\t\t\t.cra_driver_name\t= \"__ecb-aes-aesni\",\n\t\t\t.cra_priority\t\t= 400,\n\t\t\t.cra_flags\t\t= CRYPTO_ALG_INTERNAL,\n\t\t\t.cra_blocksize\t\t= AES_BLOCK_SIZE,\n\t\t\t.cra_ctxsize\t\t= CRYPTO_AES_CTX_SIZE,\n\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t},\n\t\t.min_keysize\t= AES_MIN_KEY_SIZE,\n\t\t.max_keysize\t= AES_MAX_KEY_SIZE,\n\t\t.setkey\t\t= aesni_skcipher_setkey,\n\t\t.encrypt\t= ecb_encrypt,\n\t\t.decrypt\t= ecb_decrypt,\n\t}, {\n\t\t.base = {\n\t\t\t.cra_name\t\t= \"__cbc(aes)\",\n\t\t\t.cra_driver_name\t= \"__cbc-aes-aesni\",\n\t\t\t.cra_priority\t\t= 400,\n\t\t\t.cra_flags\t\t= CRYPTO_ALG_INTERNAL,\n\t\t\t.cra_blocksize\t\t= AES_BLOCK_SIZE,\n\t\t\t.cra_ctxsize\t\t= CRYPTO_AES_CTX_SIZE,\n\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t},\n\t\t.min_keysize\t= AES_MIN_KEY_SIZE,\n\t\t.max_keysize\t= AES_MAX_KEY_SIZE,\n\t\t.ivsize\t\t= AES_BLOCK_SIZE,\n\t\t.setkey\t\t= aesni_skcipher_setkey,\n\t\t.encrypt\t= cbc_encrypt,\n\t\t.decrypt\t= cbc_decrypt,\n\t}, {\n\t\t.base = {\n\t\t\t.cra_name\t\t= \"__cts(cbc(aes))\",\n\t\t\t.cra_driver_name\t= \"__cts-cbc-aes-aesni\",\n\t\t\t.cra_priority\t\t= 400,\n\t\t\t.cra_flags\t\t= CRYPTO_ALG_INTERNAL,\n\t\t\t.cra_blocksize\t\t= AES_BLOCK_SIZE,\n\t\t\t.cra_ctxsize\t\t= CRYPTO_AES_CTX_SIZE,\n\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t},\n\t\t.min_keysize\t= AES_MIN_KEY_SIZE,\n\t\t.max_keysize\t= AES_MAX_KEY_SIZE,\n\t\t.ivsize\t\t= AES_BLOCK_SIZE,\n\t\t.walksize\t= 2 * AES_BLOCK_SIZE,\n\t\t.setkey\t\t= aesni_skcipher_setkey,\n\t\t.encrypt\t= cts_cbc_encrypt,\n\t\t.decrypt\t= cts_cbc_decrypt,\n#ifdef CONFIG_X86_64\n\t}, {\n\t\t.base = {\n\t\t\t.cra_name\t\t= \"__ctr(aes)\",\n\t\t\t.cra_driver_name\t= \"__ctr-aes-aesni\",\n\t\t\t.cra_priority\t\t= 400,\n\t\t\t.cra_flags\t\t= CRYPTO_ALG_INTERNAL,\n\t\t\t.cra_blocksize\t\t= 1,\n\t\t\t.cra_ctxsize\t\t= CRYPTO_AES_CTX_SIZE,\n\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t},\n\t\t.min_keysize\t= AES_MIN_KEY_SIZE,\n\t\t.max_keysize\t= AES_MAX_KEY_SIZE,\n\t\t.ivsize\t\t= AES_BLOCK_SIZE,\n\t\t.chunksize\t= AES_BLOCK_SIZE,\n\t\t.setkey\t\t= aesni_skcipher_setkey,\n\t\t.encrypt\t= ctr_crypt,\n\t\t.decrypt\t= ctr_crypt,\n#endif\n\t}, {\n\t\t.base = {\n\t\t\t.cra_name\t\t= \"__xts(aes)\",\n\t\t\t.cra_driver_name\t= \"__xts-aes-aesni\",\n\t\t\t.cra_priority\t\t= 401,\n\t\t\t.cra_flags\t\t= CRYPTO_ALG_INTERNAL,\n\t\t\t.cra_blocksize\t\t= AES_BLOCK_SIZE,\n\t\t\t.cra_ctxsize\t\t= XTS_AES_CTX_SIZE,\n\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t},\n\t\t.min_keysize\t= 2 * AES_MIN_KEY_SIZE,\n\t\t.max_keysize\t= 2 * AES_MAX_KEY_SIZE,\n\t\t.ivsize\t\t= AES_BLOCK_SIZE,\n\t\t.walksize\t= 2 * AES_BLOCK_SIZE,\n\t\t.setkey\t\t= xts_aesni_setkey,\n\t\t.encrypt\t= xts_encrypt,\n\t\t.decrypt\t= xts_decrypt,\n\t}\n};\n\nstatic\nstruct simd_skcipher_alg *aesni_simd_skciphers[ARRAY_SIZE(aesni_skciphers)];\n\n#ifdef CONFIG_X86_64\n \nstatic struct skcipher_alg aesni_xctr = {\n\t.base = {\n\t\t.cra_name\t\t= \"__xctr(aes)\",\n\t\t.cra_driver_name\t= \"__xctr-aes-aesni\",\n\t\t.cra_priority\t\t= 400,\n\t\t.cra_flags\t\t= CRYPTO_ALG_INTERNAL,\n\t\t.cra_blocksize\t\t= 1,\n\t\t.cra_ctxsize\t\t= CRYPTO_AES_CTX_SIZE,\n\t\t.cra_module\t\t= THIS_MODULE,\n\t},\n\t.min_keysize\t= AES_MIN_KEY_SIZE,\n\t.max_keysize\t= AES_MAX_KEY_SIZE,\n\t.ivsize\t\t= AES_BLOCK_SIZE,\n\t.chunksize\t= AES_BLOCK_SIZE,\n\t.setkey\t\t= aesni_skcipher_setkey,\n\t.encrypt\t= xctr_crypt,\n\t.decrypt\t= xctr_crypt,\n};\n\nstatic struct simd_skcipher_alg *aesni_simd_xctr;\n#endif  \n\n#ifdef CONFIG_X86_64\nstatic int generic_gcmaes_set_key(struct crypto_aead *aead, const u8 *key,\n\t\t\t\t  unsigned int key_len)\n{\n\tstruct generic_gcmaes_ctx *ctx = generic_gcmaes_ctx_get(aead);\n\n\treturn aes_set_key_common(&ctx->aes_key_expanded, key, key_len) ?:\n\t       rfc4106_set_hash_subkey(ctx->hash_subkey, key, key_len);\n}\n\nstatic int generic_gcmaes_encrypt(struct aead_request *req)\n{\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct generic_gcmaes_ctx *ctx = generic_gcmaes_ctx_get(tfm);\n\tvoid *aes_ctx = &(ctx->aes_key_expanded);\n\tu8 ivbuf[16 + (AESNI_ALIGN - 8)] __aligned(8);\n\tu8 *iv = PTR_ALIGN(&ivbuf[0], AESNI_ALIGN);\n\t__be32 counter = cpu_to_be32(1);\n\n\tmemcpy(iv, req->iv, 12);\n\t*((__be32 *)(iv+12)) = counter;\n\n\treturn gcmaes_encrypt(req, req->assoclen, ctx->hash_subkey, iv,\n\t\t\t      aes_ctx);\n}\n\nstatic int generic_gcmaes_decrypt(struct aead_request *req)\n{\n\t__be32 counter = cpu_to_be32(1);\n\tstruct crypto_aead *tfm = crypto_aead_reqtfm(req);\n\tstruct generic_gcmaes_ctx *ctx = generic_gcmaes_ctx_get(tfm);\n\tvoid *aes_ctx = &(ctx->aes_key_expanded);\n\tu8 ivbuf[16 + (AESNI_ALIGN - 8)] __aligned(8);\n\tu8 *iv = PTR_ALIGN(&ivbuf[0], AESNI_ALIGN);\n\n\tmemcpy(iv, req->iv, 12);\n\t*((__be32 *)(iv+12)) = counter;\n\n\treturn gcmaes_decrypt(req, req->assoclen, ctx->hash_subkey, iv,\n\t\t\t      aes_ctx);\n}\n\nstatic struct aead_alg aesni_aeads[] = { {\n\t.setkey\t\t\t= common_rfc4106_set_key,\n\t.setauthsize\t\t= common_rfc4106_set_authsize,\n\t.encrypt\t\t= helper_rfc4106_encrypt,\n\t.decrypt\t\t= helper_rfc4106_decrypt,\n\t.ivsize\t\t\t= GCM_RFC4106_IV_SIZE,\n\t.maxauthsize\t\t= 16,\n\t.base = {\n\t\t.cra_name\t\t= \"__rfc4106(gcm(aes))\",\n\t\t.cra_driver_name\t= \"__rfc4106-gcm-aesni\",\n\t\t.cra_priority\t\t= 400,\n\t\t.cra_flags\t\t= CRYPTO_ALG_INTERNAL,\n\t\t.cra_blocksize\t\t= 1,\n\t\t.cra_ctxsize\t\t= sizeof(struct aesni_rfc4106_gcm_ctx),\n\t\t.cra_alignmask\t\t= 0,\n\t\t.cra_module\t\t= THIS_MODULE,\n\t},\n}, {\n\t.setkey\t\t\t= generic_gcmaes_set_key,\n\t.setauthsize\t\t= generic_gcmaes_set_authsize,\n\t.encrypt\t\t= generic_gcmaes_encrypt,\n\t.decrypt\t\t= generic_gcmaes_decrypt,\n\t.ivsize\t\t\t= GCM_AES_IV_SIZE,\n\t.maxauthsize\t\t= 16,\n\t.base = {\n\t\t.cra_name\t\t= \"__gcm(aes)\",\n\t\t.cra_driver_name\t= \"__generic-gcm-aesni\",\n\t\t.cra_priority\t\t= 400,\n\t\t.cra_flags\t\t= CRYPTO_ALG_INTERNAL,\n\t\t.cra_blocksize\t\t= 1,\n\t\t.cra_ctxsize\t\t= sizeof(struct generic_gcmaes_ctx),\n\t\t.cra_alignmask\t\t= 0,\n\t\t.cra_module\t\t= THIS_MODULE,\n\t},\n} };\n#else\nstatic struct aead_alg aesni_aeads[0];\n#endif\n\nstatic struct simd_aead_alg *aesni_simd_aeads[ARRAY_SIZE(aesni_aeads)];\n\nstatic const struct x86_cpu_id aesni_cpu_id[] = {\n\tX86_MATCH_FEATURE(X86_FEATURE_AES, NULL),\n\t{}\n};\nMODULE_DEVICE_TABLE(x86cpu, aesni_cpu_id);\n\nstatic int __init aesni_init(void)\n{\n\tint err;\n\n\tif (!x86_match_cpu(aesni_cpu_id))\n\t\treturn -ENODEV;\n#ifdef CONFIG_X86_64\n\tif (boot_cpu_has(X86_FEATURE_AVX2)) {\n\t\tpr_info(\"AVX2 version of gcm_enc/dec engaged.\\n\");\n\t\tstatic_branch_enable(&gcm_use_avx);\n\t\tstatic_branch_enable(&gcm_use_avx2);\n\t} else\n\tif (boot_cpu_has(X86_FEATURE_AVX)) {\n\t\tpr_info(\"AVX version of gcm_enc/dec engaged.\\n\");\n\t\tstatic_branch_enable(&gcm_use_avx);\n\t} else {\n\t\tpr_info(\"SSE version of gcm_enc/dec engaged.\\n\");\n\t}\n\tif (boot_cpu_has(X86_FEATURE_AVX)) {\n\t\t \n\t\tstatic_call_update(aesni_ctr_enc_tfm, aesni_ctr_enc_avx_tfm);\n\t\tpr_info(\"AES CTR mode by8 optimization enabled\\n\");\n\t}\n#endif  \n\n\terr = crypto_register_alg(&aesni_cipher_alg);\n\tif (err)\n\t\treturn err;\n\n\terr = simd_register_skciphers_compat(aesni_skciphers,\n\t\t\t\t\t     ARRAY_SIZE(aesni_skciphers),\n\t\t\t\t\t     aesni_simd_skciphers);\n\tif (err)\n\t\tgoto unregister_cipher;\n\n\terr = simd_register_aeads_compat(aesni_aeads, ARRAY_SIZE(aesni_aeads),\n\t\t\t\t\t aesni_simd_aeads);\n\tif (err)\n\t\tgoto unregister_skciphers;\n\n#ifdef CONFIG_X86_64\n\tif (boot_cpu_has(X86_FEATURE_AVX))\n\t\terr = simd_register_skciphers_compat(&aesni_xctr, 1,\n\t\t\t\t\t\t     &aesni_simd_xctr);\n\tif (err)\n\t\tgoto unregister_aeads;\n#endif  \n\n\treturn 0;\n\n#ifdef CONFIG_X86_64\nunregister_aeads:\n\tsimd_unregister_aeads(aesni_aeads, ARRAY_SIZE(aesni_aeads),\n\t\t\t\taesni_simd_aeads);\n#endif  \n\nunregister_skciphers:\n\tsimd_unregister_skciphers(aesni_skciphers, ARRAY_SIZE(aesni_skciphers),\n\t\t\t\t  aesni_simd_skciphers);\nunregister_cipher:\n\tcrypto_unregister_alg(&aesni_cipher_alg);\n\treturn err;\n}\n\nstatic void __exit aesni_exit(void)\n{\n\tsimd_unregister_aeads(aesni_aeads, ARRAY_SIZE(aesni_aeads),\n\t\t\t      aesni_simd_aeads);\n\tsimd_unregister_skciphers(aesni_skciphers, ARRAY_SIZE(aesni_skciphers),\n\t\t\t\t  aesni_simd_skciphers);\n\tcrypto_unregister_alg(&aesni_cipher_alg);\n#ifdef CONFIG_X86_64\n\tif (boot_cpu_has(X86_FEATURE_AVX))\n\t\tsimd_unregister_skciphers(&aesni_xctr, 1, &aesni_simd_xctr);\n#endif  \n}\n\nlate_initcall(aesni_init);\nmodule_exit(aesni_exit);\n\nMODULE_DESCRIPTION(\"Rijndael (AES) Cipher Algorithm, Intel AES-NI instructions optimized\");\nMODULE_LICENSE(\"GPL\");\nMODULE_ALIAS_CRYPTO(\"aes\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}