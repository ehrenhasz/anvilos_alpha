{
  "module_name": "chacha-ssse3-x86_64.S",
  "hash_id": "b4cb5d2eecd8b67ae33cf6f32099aa4833938ddf4e2234b5556fc5d191930a54",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/crypto/chacha-ssse3-x86_64.S",
  "human_readable_source": " \n \n\n#include <linux/linkage.h>\n#include <asm/frame.h>\n\n.section\t.rodata.cst16.ROT8, \"aM\", @progbits, 16\n.align 16\nROT8:\t.octa 0x0e0d0c0f0a09080b0605040702010003\n.section\t.rodata.cst16.ROT16, \"aM\", @progbits, 16\n.align 16\nROT16:\t.octa 0x0d0c0f0e09080b0a0504070601000302\n.section\t.rodata.cst16.CTRINC, \"aM\", @progbits, 16\n.align 16\nCTRINC:\t.octa 0x00000003000000020000000100000000\n\n.text\n\n \nSYM_FUNC_START_LOCAL(chacha_permute)\n\n\tmovdqa\t\tROT8(%rip),%xmm4\n\tmovdqa\t\tROT16(%rip),%xmm5\n\n.Ldoubleround:\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 16)\n\tpaddd\t\t%xmm1,%xmm0\n\tpxor\t\t%xmm0,%xmm3\n\tpshufb\t\t%xmm5,%xmm3\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 12)\n\tpaddd\t\t%xmm3,%xmm2\n\tpxor\t\t%xmm2,%xmm1\n\tmovdqa\t\t%xmm1,%xmm6\n\tpslld\t\t$12,%xmm6\n\tpsrld\t\t$20,%xmm1\n\tpor\t\t%xmm6,%xmm1\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 8)\n\tpaddd\t\t%xmm1,%xmm0\n\tpxor\t\t%xmm0,%xmm3\n\tpshufb\t\t%xmm4,%xmm3\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 7)\n\tpaddd\t\t%xmm3,%xmm2\n\tpxor\t\t%xmm2,%xmm1\n\tmovdqa\t\t%xmm1,%xmm7\n\tpslld\t\t$7,%xmm7\n\tpsrld\t\t$25,%xmm1\n\tpor\t\t%xmm7,%xmm1\n\n\t# x1 = shuffle32(x1, MASK(0, 3, 2, 1))\n\tpshufd\t\t$0x39,%xmm1,%xmm1\n\t# x2 = shuffle32(x2, MASK(1, 0, 3, 2))\n\tpshufd\t\t$0x4e,%xmm2,%xmm2\n\t# x3 = shuffle32(x3, MASK(2, 1, 0, 3))\n\tpshufd\t\t$0x93,%xmm3,%xmm3\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 16)\n\tpaddd\t\t%xmm1,%xmm0\n\tpxor\t\t%xmm0,%xmm3\n\tpshufb\t\t%xmm5,%xmm3\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 12)\n\tpaddd\t\t%xmm3,%xmm2\n\tpxor\t\t%xmm2,%xmm1\n\tmovdqa\t\t%xmm1,%xmm6\n\tpslld\t\t$12,%xmm6\n\tpsrld\t\t$20,%xmm1\n\tpor\t\t%xmm6,%xmm1\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 8)\n\tpaddd\t\t%xmm1,%xmm0\n\tpxor\t\t%xmm0,%xmm3\n\tpshufb\t\t%xmm4,%xmm3\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 7)\n\tpaddd\t\t%xmm3,%xmm2\n\tpxor\t\t%xmm2,%xmm1\n\tmovdqa\t\t%xmm1,%xmm7\n\tpslld\t\t$7,%xmm7\n\tpsrld\t\t$25,%xmm1\n\tpor\t\t%xmm7,%xmm1\n\n\t# x1 = shuffle32(x1, MASK(2, 1, 0, 3))\n\tpshufd\t\t$0x93,%xmm1,%xmm1\n\t# x2 = shuffle32(x2, MASK(1, 0, 3, 2))\n\tpshufd\t\t$0x4e,%xmm2,%xmm2\n\t# x3 = shuffle32(x3, MASK(0, 3, 2, 1))\n\tpshufd\t\t$0x39,%xmm3,%xmm3\n\n\tsub\t\t$2,%r8d\n\tjnz\t\t.Ldoubleround\n\n\tRET\nSYM_FUNC_END(chacha_permute)\n\nSYM_FUNC_START(chacha_block_xor_ssse3)\n\t# %rdi: Input state matrix, s\n\t# %rsi: up to 1 data block output, o\n\t# %rdx: up to 1 data block input, i\n\t# %rcx: input/output length in bytes\n\t# %r8d: nrounds\n\tFRAME_BEGIN\n\n\t# x0..3 = s0..3\n\tmovdqu\t\t0x00(%rdi),%xmm0\n\tmovdqu\t\t0x10(%rdi),%xmm1\n\tmovdqu\t\t0x20(%rdi),%xmm2\n\tmovdqu\t\t0x30(%rdi),%xmm3\n\tmovdqa\t\t%xmm0,%xmm8\n\tmovdqa\t\t%xmm1,%xmm9\n\tmovdqa\t\t%xmm2,%xmm10\n\tmovdqa\t\t%xmm3,%xmm11\n\n\tmov\t\t%rcx,%rax\n\tcall\t\tchacha_permute\n\n\t# o0 = i0 ^ (x0 + s0)\n\tpaddd\t\t%xmm8,%xmm0\n\tcmp\t\t$0x10,%rax\n\tjl\t\t.Lxorpart\n\tmovdqu\t\t0x00(%rdx),%xmm4\n\tpxor\t\t%xmm4,%xmm0\n\tmovdqu\t\t%xmm0,0x00(%rsi)\n\t# o1 = i1 ^ (x1 + s1)\n\tpaddd\t\t%xmm9,%xmm1\n\tmovdqa\t\t%xmm1,%xmm0\n\tcmp\t\t$0x20,%rax\n\tjl\t\t.Lxorpart\n\tmovdqu\t\t0x10(%rdx),%xmm0\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0x10(%rsi)\n\t# o2 = i2 ^ (x2 + s2)\n\tpaddd\t\t%xmm10,%xmm2\n\tmovdqa\t\t%xmm2,%xmm0\n\tcmp\t\t$0x30,%rax\n\tjl\t\t.Lxorpart\n\tmovdqu\t\t0x20(%rdx),%xmm0\n\tpxor\t\t%xmm2,%xmm0\n\tmovdqu\t\t%xmm0,0x20(%rsi)\n\t# o3 = i3 ^ (x3 + s3)\n\tpaddd\t\t%xmm11,%xmm3\n\tmovdqa\t\t%xmm3,%xmm0\n\tcmp\t\t$0x40,%rax\n\tjl\t\t.Lxorpart\n\tmovdqu\t\t0x30(%rdx),%xmm0\n\tpxor\t\t%xmm3,%xmm0\n\tmovdqu\t\t%xmm0,0x30(%rsi)\n\n.Ldone:\n\tFRAME_END\n\tRET\n\n.Lxorpart:\n\t# xor remaining bytes from partial register into output\n\tmov\t\t%rax,%r9\n\tand\t\t$0x0f,%r9\n\tjz\t\t.Ldone\n\tand\t\t$~0x0f,%rax\n\n\tmov\t\t%rsi,%r11\n\n\tlea\t\t8(%rsp),%r10\n\tsub\t\t$0x10,%rsp\n\tand\t\t$~31,%rsp\n\n\tlea\t\t(%rdx,%rax),%rsi\n\tmov\t\t%rsp,%rdi\n\tmov\t\t%r9,%rcx\n\trep movsb\n\n\tpxor\t\t0x00(%rsp),%xmm0\n\tmovdqa\t\t%xmm0,0x00(%rsp)\n\n\tmov\t\t%rsp,%rsi\n\tlea\t\t(%r11,%rax),%rdi\n\tmov\t\t%r9,%rcx\n\trep movsb\n\n\tlea\t\t-8(%r10),%rsp\n\tjmp\t\t.Ldone\n\nSYM_FUNC_END(chacha_block_xor_ssse3)\n\nSYM_FUNC_START(hchacha_block_ssse3)\n\t# %rdi: Input state matrix, s\n\t# %rsi: output (8 32-bit words)\n\t# %edx: nrounds\n\tFRAME_BEGIN\n\n\tmovdqu\t\t0x00(%rdi),%xmm0\n\tmovdqu\t\t0x10(%rdi),%xmm1\n\tmovdqu\t\t0x20(%rdi),%xmm2\n\tmovdqu\t\t0x30(%rdi),%xmm3\n\n\tmov\t\t%edx,%r8d\n\tcall\t\tchacha_permute\n\n\tmovdqu\t\t%xmm0,0x00(%rsi)\n\tmovdqu\t\t%xmm3,0x10(%rsi)\n\n\tFRAME_END\n\tRET\nSYM_FUNC_END(hchacha_block_ssse3)\n\nSYM_FUNC_START(chacha_4block_xor_ssse3)\n\t# %rdi: Input state matrix, s\n\t# %rsi: up to 4 data blocks output, o\n\t# %rdx: up to 4 data blocks input, i\n\t# %rcx: input/output length in bytes\n\t# %r8d: nrounds\n\n\t# This function encrypts four consecutive ChaCha blocks by loading the\n\t# the state matrix in SSE registers four times. As we need some scratch\n\t# registers, we save the first four registers on the stack. The\n\t# algorithm performs each operation on the corresponding word of each\n\t# state matrix, hence requires no word shuffling. For final XORing step\n\t# we transpose the matrix by interleaving 32- and then 64-bit words,\n\t# which allows us to do XOR in SSE registers. 8/16-bit word rotation is\n\t# done with the slightly better performing SSSE3 byte shuffling,\n\t# 7/12-bit word rotation uses traditional shift+OR.\n\n\tlea\t\t8(%rsp),%r10\n\tsub\t\t$0x80,%rsp\n\tand\t\t$~63,%rsp\n\tmov\t\t%rcx,%rax\n\n\t# x0..15[0-3] = s0..3[0..3]\n\tmovq\t\t0x00(%rdi),%xmm1\n\tpshufd\t\t$0x00,%xmm1,%xmm0\n\tpshufd\t\t$0x55,%xmm1,%xmm1\n\tmovq\t\t0x08(%rdi),%xmm3\n\tpshufd\t\t$0x00,%xmm3,%xmm2\n\tpshufd\t\t$0x55,%xmm3,%xmm3\n\tmovq\t\t0x10(%rdi),%xmm5\n\tpshufd\t\t$0x00,%xmm5,%xmm4\n\tpshufd\t\t$0x55,%xmm5,%xmm5\n\tmovq\t\t0x18(%rdi),%xmm7\n\tpshufd\t\t$0x00,%xmm7,%xmm6\n\tpshufd\t\t$0x55,%xmm7,%xmm7\n\tmovq\t\t0x20(%rdi),%xmm9\n\tpshufd\t\t$0x00,%xmm9,%xmm8\n\tpshufd\t\t$0x55,%xmm9,%xmm9\n\tmovq\t\t0x28(%rdi),%xmm11\n\tpshufd\t\t$0x00,%xmm11,%xmm10\n\tpshufd\t\t$0x55,%xmm11,%xmm11\n\tmovq\t\t0x30(%rdi),%xmm13\n\tpshufd\t\t$0x00,%xmm13,%xmm12\n\tpshufd\t\t$0x55,%xmm13,%xmm13\n\tmovq\t\t0x38(%rdi),%xmm15\n\tpshufd\t\t$0x00,%xmm15,%xmm14\n\tpshufd\t\t$0x55,%xmm15,%xmm15\n\t# x0..3 on stack\n\tmovdqa\t\t%xmm0,0x00(%rsp)\n\tmovdqa\t\t%xmm1,0x10(%rsp)\n\tmovdqa\t\t%xmm2,0x20(%rsp)\n\tmovdqa\t\t%xmm3,0x30(%rsp)\n\n\tmovdqa\t\tCTRINC(%rip),%xmm1\n\tmovdqa\t\tROT8(%rip),%xmm2\n\tmovdqa\t\tROT16(%rip),%xmm3\n\n\t# x12 += counter values 0-3\n\tpaddd\t\t%xmm1,%xmm12\n\n.Ldoubleround4:\n\t# x0 += x4, x12 = rotl32(x12 ^ x0, 16)\n\tmovdqa\t\t0x00(%rsp),%xmm0\n\tpaddd\t\t%xmm4,%xmm0\n\tmovdqa\t\t%xmm0,0x00(%rsp)\n\tpxor\t\t%xmm0,%xmm12\n\tpshufb\t\t%xmm3,%xmm12\n\t# x1 += x5, x13 = rotl32(x13 ^ x1, 16)\n\tmovdqa\t\t0x10(%rsp),%xmm0\n\tpaddd\t\t%xmm5,%xmm0\n\tmovdqa\t\t%xmm0,0x10(%rsp)\n\tpxor\t\t%xmm0,%xmm13\n\tpshufb\t\t%xmm3,%xmm13\n\t# x2 += x6, x14 = rotl32(x14 ^ x2, 16)\n\tmovdqa\t\t0x20(%rsp),%xmm0\n\tpaddd\t\t%xmm6,%xmm0\n\tmovdqa\t\t%xmm0,0x20(%rsp)\n\tpxor\t\t%xmm0,%xmm14\n\tpshufb\t\t%xmm3,%xmm14\n\t# x3 += x7, x15 = rotl32(x15 ^ x3, 16)\n\tmovdqa\t\t0x30(%rsp),%xmm0\n\tpaddd\t\t%xmm7,%xmm0\n\tmovdqa\t\t%xmm0,0x30(%rsp)\n\tpxor\t\t%xmm0,%xmm15\n\tpshufb\t\t%xmm3,%xmm15\n\n\t# x8 += x12, x4 = rotl32(x4 ^ x8, 12)\n\tpaddd\t\t%xmm12,%xmm8\n\tpxor\t\t%xmm8,%xmm4\n\tmovdqa\t\t%xmm4,%xmm0\n\tpslld\t\t$12,%xmm0\n\tpsrld\t\t$20,%xmm4\n\tpor\t\t%xmm0,%xmm4\n\t# x9 += x13, x5 = rotl32(x5 ^ x9, 12)\n\tpaddd\t\t%xmm13,%xmm9\n\tpxor\t\t%xmm9,%xmm5\n\tmovdqa\t\t%xmm5,%xmm0\n\tpslld\t\t$12,%xmm0\n\tpsrld\t\t$20,%xmm5\n\tpor\t\t%xmm0,%xmm5\n\t# x10 += x14, x6 = rotl32(x6 ^ x10, 12)\n\tpaddd\t\t%xmm14,%xmm10\n\tpxor\t\t%xmm10,%xmm6\n\tmovdqa\t\t%xmm6,%xmm0\n\tpslld\t\t$12,%xmm0\n\tpsrld\t\t$20,%xmm6\n\tpor\t\t%xmm0,%xmm6\n\t# x11 += x15, x7 = rotl32(x7 ^ x11, 12)\n\tpaddd\t\t%xmm15,%xmm11\n\tpxor\t\t%xmm11,%xmm7\n\tmovdqa\t\t%xmm7,%xmm0\n\tpslld\t\t$12,%xmm0\n\tpsrld\t\t$20,%xmm7\n\tpor\t\t%xmm0,%xmm7\n\n\t# x0 += x4, x12 = rotl32(x12 ^ x0, 8)\n\tmovdqa\t\t0x00(%rsp),%xmm0\n\tpaddd\t\t%xmm4,%xmm0\n\tmovdqa\t\t%xmm0,0x00(%rsp)\n\tpxor\t\t%xmm0,%xmm12\n\tpshufb\t\t%xmm2,%xmm12\n\t# x1 += x5, x13 = rotl32(x13 ^ x1, 8)\n\tmovdqa\t\t0x10(%rsp),%xmm0\n\tpaddd\t\t%xmm5,%xmm0\n\tmovdqa\t\t%xmm0,0x10(%rsp)\n\tpxor\t\t%xmm0,%xmm13\n\tpshufb\t\t%xmm2,%xmm13\n\t# x2 += x6, x14 = rotl32(x14 ^ x2, 8)\n\tmovdqa\t\t0x20(%rsp),%xmm0\n\tpaddd\t\t%xmm6,%xmm0\n\tmovdqa\t\t%xmm0,0x20(%rsp)\n\tpxor\t\t%xmm0,%xmm14\n\tpshufb\t\t%xmm2,%xmm14\n\t# x3 += x7, x15 = rotl32(x15 ^ x3, 8)\n\tmovdqa\t\t0x30(%rsp),%xmm0\n\tpaddd\t\t%xmm7,%xmm0\n\tmovdqa\t\t%xmm0,0x30(%rsp)\n\tpxor\t\t%xmm0,%xmm15\n\tpshufb\t\t%xmm2,%xmm15\n\n\t# x8 += x12, x4 = rotl32(x4 ^ x8, 7)\n\tpaddd\t\t%xmm12,%xmm8\n\tpxor\t\t%xmm8,%xmm4\n\tmovdqa\t\t%xmm4,%xmm0\n\tpslld\t\t$7,%xmm0\n\tpsrld\t\t$25,%xmm4\n\tpor\t\t%xmm0,%xmm4\n\t# x9 += x13, x5 = rotl32(x5 ^ x9, 7)\n\tpaddd\t\t%xmm13,%xmm9\n\tpxor\t\t%xmm9,%xmm5\n\tmovdqa\t\t%xmm5,%xmm0\n\tpslld\t\t$7,%xmm0\n\tpsrld\t\t$25,%xmm5\n\tpor\t\t%xmm0,%xmm5\n\t# x10 += x14, x6 = rotl32(x6 ^ x10, 7)\n\tpaddd\t\t%xmm14,%xmm10\n\tpxor\t\t%xmm10,%xmm6\n\tmovdqa\t\t%xmm6,%xmm0\n\tpslld\t\t$7,%xmm0\n\tpsrld\t\t$25,%xmm6\n\tpor\t\t%xmm0,%xmm6\n\t# x11 += x15, x7 = rotl32(x7 ^ x11, 7)\n\tpaddd\t\t%xmm15,%xmm11\n\tpxor\t\t%xmm11,%xmm7\n\tmovdqa\t\t%xmm7,%xmm0\n\tpslld\t\t$7,%xmm0\n\tpsrld\t\t$25,%xmm7\n\tpor\t\t%xmm0,%xmm7\n\n\t# x0 += x5, x15 = rotl32(x15 ^ x0, 16)\n\tmovdqa\t\t0x00(%rsp),%xmm0\n\tpaddd\t\t%xmm5,%xmm0\n\tmovdqa\t\t%xmm0,0x00(%rsp)\n\tpxor\t\t%xmm0,%xmm15\n\tpshufb\t\t%xmm3,%xmm15\n\t# x1 += x6, x12 = rotl32(x12 ^ x1, 16)\n\tmovdqa\t\t0x10(%rsp),%xmm0\n\tpaddd\t\t%xmm6,%xmm0\n\tmovdqa\t\t%xmm0,0x10(%rsp)\n\tpxor\t\t%xmm0,%xmm12\n\tpshufb\t\t%xmm3,%xmm12\n\t# x2 += x7, x13 = rotl32(x13 ^ x2, 16)\n\tmovdqa\t\t0x20(%rsp),%xmm0\n\tpaddd\t\t%xmm7,%xmm0\n\tmovdqa\t\t%xmm0,0x20(%rsp)\n\tpxor\t\t%xmm0,%xmm13\n\tpshufb\t\t%xmm3,%xmm13\n\t# x3 += x4, x14 = rotl32(x14 ^ x3, 16)\n\tmovdqa\t\t0x30(%rsp),%xmm0\n\tpaddd\t\t%xmm4,%xmm0\n\tmovdqa\t\t%xmm0,0x30(%rsp)\n\tpxor\t\t%xmm0,%xmm14\n\tpshufb\t\t%xmm3,%xmm14\n\n\t# x10 += x15, x5 = rotl32(x5 ^ x10, 12)\n\tpaddd\t\t%xmm15,%xmm10\n\tpxor\t\t%xmm10,%xmm5\n\tmovdqa\t\t%xmm5,%xmm0\n\tpslld\t\t$12,%xmm0\n\tpsrld\t\t$20,%xmm5\n\tpor\t\t%xmm0,%xmm5\n\t# x11 += x12, x6 = rotl32(x6 ^ x11, 12)\n\tpaddd\t\t%xmm12,%xmm11\n\tpxor\t\t%xmm11,%xmm6\n\tmovdqa\t\t%xmm6,%xmm0\n\tpslld\t\t$12,%xmm0\n\tpsrld\t\t$20,%xmm6\n\tpor\t\t%xmm0,%xmm6\n\t# x8 += x13, x7 = rotl32(x7 ^ x8, 12)\n\tpaddd\t\t%xmm13,%xmm8\n\tpxor\t\t%xmm8,%xmm7\n\tmovdqa\t\t%xmm7,%xmm0\n\tpslld\t\t$12,%xmm0\n\tpsrld\t\t$20,%xmm7\n\tpor\t\t%xmm0,%xmm7\n\t# x9 += x14, x4 = rotl32(x4 ^ x9, 12)\n\tpaddd\t\t%xmm14,%xmm9\n\tpxor\t\t%xmm9,%xmm4\n\tmovdqa\t\t%xmm4,%xmm0\n\tpslld\t\t$12,%xmm0\n\tpsrld\t\t$20,%xmm4\n\tpor\t\t%xmm0,%xmm4\n\n\t# x0 += x5, x15 = rotl32(x15 ^ x0, 8)\n\tmovdqa\t\t0x00(%rsp),%xmm0\n\tpaddd\t\t%xmm5,%xmm0\n\tmovdqa\t\t%xmm0,0x00(%rsp)\n\tpxor\t\t%xmm0,%xmm15\n\tpshufb\t\t%xmm2,%xmm15\n\t# x1 += x6, x12 = rotl32(x12 ^ x1, 8)\n\tmovdqa\t\t0x10(%rsp),%xmm0\n\tpaddd\t\t%xmm6,%xmm0\n\tmovdqa\t\t%xmm0,0x10(%rsp)\n\tpxor\t\t%xmm0,%xmm12\n\tpshufb\t\t%xmm2,%xmm12\n\t# x2 += x7, x13 = rotl32(x13 ^ x2, 8)\n\tmovdqa\t\t0x20(%rsp),%xmm0\n\tpaddd\t\t%xmm7,%xmm0\n\tmovdqa\t\t%xmm0,0x20(%rsp)\n\tpxor\t\t%xmm0,%xmm13\n\tpshufb\t\t%xmm2,%xmm13\n\t# x3 += x4, x14 = rotl32(x14 ^ x3, 8)\n\tmovdqa\t\t0x30(%rsp),%xmm0\n\tpaddd\t\t%xmm4,%xmm0\n\tmovdqa\t\t%xmm0,0x30(%rsp)\n\tpxor\t\t%xmm0,%xmm14\n\tpshufb\t\t%xmm2,%xmm14\n\n\t# x10 += x15, x5 = rotl32(x5 ^ x10, 7)\n\tpaddd\t\t%xmm15,%xmm10\n\tpxor\t\t%xmm10,%xmm5\n\tmovdqa\t\t%xmm5,%xmm0\n\tpslld\t\t$7,%xmm0\n\tpsrld\t\t$25,%xmm5\n\tpor\t\t%xmm0,%xmm5\n\t# x11 += x12, x6 = rotl32(x6 ^ x11, 7)\n\tpaddd\t\t%xmm12,%xmm11\n\tpxor\t\t%xmm11,%xmm6\n\tmovdqa\t\t%xmm6,%xmm0\n\tpslld\t\t$7,%xmm0\n\tpsrld\t\t$25,%xmm6\n\tpor\t\t%xmm0,%xmm6\n\t# x8 += x13, x7 = rotl32(x7 ^ x8, 7)\n\tpaddd\t\t%xmm13,%xmm8\n\tpxor\t\t%xmm8,%xmm7\n\tmovdqa\t\t%xmm7,%xmm0\n\tpslld\t\t$7,%xmm0\n\tpsrld\t\t$25,%xmm7\n\tpor\t\t%xmm0,%xmm7\n\t# x9 += x14, x4 = rotl32(x4 ^ x9, 7)\n\tpaddd\t\t%xmm14,%xmm9\n\tpxor\t\t%xmm9,%xmm4\n\tmovdqa\t\t%xmm4,%xmm0\n\tpslld\t\t$7,%xmm0\n\tpsrld\t\t$25,%xmm4\n\tpor\t\t%xmm0,%xmm4\n\n\tsub\t\t$2,%r8d\n\tjnz\t\t.Ldoubleround4\n\n\t# x0[0-3] += s0[0]\n\t# x1[0-3] += s0[1]\n\tmovq\t\t0x00(%rdi),%xmm3\n\tpshufd\t\t$0x00,%xmm3,%xmm2\n\tpshufd\t\t$0x55,%xmm3,%xmm3\n\tpaddd\t\t0x00(%rsp),%xmm2\n\tmovdqa\t\t%xmm2,0x00(%rsp)\n\tpaddd\t\t0x10(%rsp),%xmm3\n\tmovdqa\t\t%xmm3,0x10(%rsp)\n\t# x2[0-3] += s0[2]\n\t# x3[0-3] += s0[3]\n\tmovq\t\t0x08(%rdi),%xmm3\n\tpshufd\t\t$0x00,%xmm3,%xmm2\n\tpshufd\t\t$0x55,%xmm3,%xmm3\n\tpaddd\t\t0x20(%rsp),%xmm2\n\tmovdqa\t\t%xmm2,0x20(%rsp)\n\tpaddd\t\t0x30(%rsp),%xmm3\n\tmovdqa\t\t%xmm3,0x30(%rsp)\n\n\t# x4[0-3] += s1[0]\n\t# x5[0-3] += s1[1]\n\tmovq\t\t0x10(%rdi),%xmm3\n\tpshufd\t\t$0x00,%xmm3,%xmm2\n\tpshufd\t\t$0x55,%xmm3,%xmm3\n\tpaddd\t\t%xmm2,%xmm4\n\tpaddd\t\t%xmm3,%xmm5\n\t# x6[0-3] += s1[2]\n\t# x7[0-3] += s1[3]\n\tmovq\t\t0x18(%rdi),%xmm3\n\tpshufd\t\t$0x00,%xmm3,%xmm2\n\tpshufd\t\t$0x55,%xmm3,%xmm3\n\tpaddd\t\t%xmm2,%xmm6\n\tpaddd\t\t%xmm3,%xmm7\n\n\t# x8[0-3] += s2[0]\n\t# x9[0-3] += s2[1]\n\tmovq\t\t0x20(%rdi),%xmm3\n\tpshufd\t\t$0x00,%xmm3,%xmm2\n\tpshufd\t\t$0x55,%xmm3,%xmm3\n\tpaddd\t\t%xmm2,%xmm8\n\tpaddd\t\t%xmm3,%xmm9\n\t# x10[0-3] += s2[2]\n\t# x11[0-3] += s2[3]\n\tmovq\t\t0x28(%rdi),%xmm3\n\tpshufd\t\t$0x00,%xmm3,%xmm2\n\tpshufd\t\t$0x55,%xmm3,%xmm3\n\tpaddd\t\t%xmm2,%xmm10\n\tpaddd\t\t%xmm3,%xmm11\n\n\t# x12[0-3] += s3[0]\n\t# x13[0-3] += s3[1]\n\tmovq\t\t0x30(%rdi),%xmm3\n\tpshufd\t\t$0x00,%xmm3,%xmm2\n\tpshufd\t\t$0x55,%xmm3,%xmm3\n\tpaddd\t\t%xmm2,%xmm12\n\tpaddd\t\t%xmm3,%xmm13\n\t# x14[0-3] += s3[2]\n\t# x15[0-3] += s3[3]\n\tmovq\t\t0x38(%rdi),%xmm3\n\tpshufd\t\t$0x00,%xmm3,%xmm2\n\tpshufd\t\t$0x55,%xmm3,%xmm3\n\tpaddd\t\t%xmm2,%xmm14\n\tpaddd\t\t%xmm3,%xmm15\n\n\t# x12 += counter values 0-3\n\tpaddd\t\t%xmm1,%xmm12\n\n\t# interleave 32-bit words in state n, n+1\n\tmovdqa\t\t0x00(%rsp),%xmm0\n\tmovdqa\t\t0x10(%rsp),%xmm1\n\tmovdqa\t\t%xmm0,%xmm2\n\tpunpckldq\t%xmm1,%xmm2\n\tpunpckhdq\t%xmm1,%xmm0\n\tmovdqa\t\t%xmm2,0x00(%rsp)\n\tmovdqa\t\t%xmm0,0x10(%rsp)\n\tmovdqa\t\t0x20(%rsp),%xmm0\n\tmovdqa\t\t0x30(%rsp),%xmm1\n\tmovdqa\t\t%xmm0,%xmm2\n\tpunpckldq\t%xmm1,%xmm2\n\tpunpckhdq\t%xmm1,%xmm0\n\tmovdqa\t\t%xmm2,0x20(%rsp)\n\tmovdqa\t\t%xmm0,0x30(%rsp)\n\tmovdqa\t\t%xmm4,%xmm0\n\tpunpckldq\t%xmm5,%xmm4\n\tpunpckhdq\t%xmm5,%xmm0\n\tmovdqa\t\t%xmm0,%xmm5\n\tmovdqa\t\t%xmm6,%xmm0\n\tpunpckldq\t%xmm7,%xmm6\n\tpunpckhdq\t%xmm7,%xmm0\n\tmovdqa\t\t%xmm0,%xmm7\n\tmovdqa\t\t%xmm8,%xmm0\n\tpunpckldq\t%xmm9,%xmm8\n\tpunpckhdq\t%xmm9,%xmm0\n\tmovdqa\t\t%xmm0,%xmm9\n\tmovdqa\t\t%xmm10,%xmm0\n\tpunpckldq\t%xmm11,%xmm10\n\tpunpckhdq\t%xmm11,%xmm0\n\tmovdqa\t\t%xmm0,%xmm11\n\tmovdqa\t\t%xmm12,%xmm0\n\tpunpckldq\t%xmm13,%xmm12\n\tpunpckhdq\t%xmm13,%xmm0\n\tmovdqa\t\t%xmm0,%xmm13\n\tmovdqa\t\t%xmm14,%xmm0\n\tpunpckldq\t%xmm15,%xmm14\n\tpunpckhdq\t%xmm15,%xmm0\n\tmovdqa\t\t%xmm0,%xmm15\n\n\t# interleave 64-bit words in state n, n+2\n\tmovdqa\t\t0x00(%rsp),%xmm0\n\tmovdqa\t\t0x20(%rsp),%xmm1\n\tmovdqa\t\t%xmm0,%xmm2\n\tpunpcklqdq\t%xmm1,%xmm2\n\tpunpckhqdq\t%xmm1,%xmm0\n\tmovdqa\t\t%xmm2,0x00(%rsp)\n\tmovdqa\t\t%xmm0,0x20(%rsp)\n\tmovdqa\t\t0x10(%rsp),%xmm0\n\tmovdqa\t\t0x30(%rsp),%xmm1\n\tmovdqa\t\t%xmm0,%xmm2\n\tpunpcklqdq\t%xmm1,%xmm2\n\tpunpckhqdq\t%xmm1,%xmm0\n\tmovdqa\t\t%xmm2,0x10(%rsp)\n\tmovdqa\t\t%xmm0,0x30(%rsp)\n\tmovdqa\t\t%xmm4,%xmm0\n\tpunpcklqdq\t%xmm6,%xmm4\n\tpunpckhqdq\t%xmm6,%xmm0\n\tmovdqa\t\t%xmm0,%xmm6\n\tmovdqa\t\t%xmm5,%xmm0\n\tpunpcklqdq\t%xmm7,%xmm5\n\tpunpckhqdq\t%xmm7,%xmm0\n\tmovdqa\t\t%xmm0,%xmm7\n\tmovdqa\t\t%xmm8,%xmm0\n\tpunpcklqdq\t%xmm10,%xmm8\n\tpunpckhqdq\t%xmm10,%xmm0\n\tmovdqa\t\t%xmm0,%xmm10\n\tmovdqa\t\t%xmm9,%xmm0\n\tpunpcklqdq\t%xmm11,%xmm9\n\tpunpckhqdq\t%xmm11,%xmm0\n\tmovdqa\t\t%xmm0,%xmm11\n\tmovdqa\t\t%xmm12,%xmm0\n\tpunpcklqdq\t%xmm14,%xmm12\n\tpunpckhqdq\t%xmm14,%xmm0\n\tmovdqa\t\t%xmm0,%xmm14\n\tmovdqa\t\t%xmm13,%xmm0\n\tpunpcklqdq\t%xmm15,%xmm13\n\tpunpckhqdq\t%xmm15,%xmm0\n\tmovdqa\t\t%xmm0,%xmm15\n\n\t# xor with corresponding input, write to output\n\tmovdqa\t\t0x00(%rsp),%xmm0\n\tcmp\t\t$0x10,%rax\n\tjl\t\t.Lxorpart4\n\tmovdqu\t\t0x00(%rdx),%xmm1\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0x00(%rsi)\n\n\tmovdqu\t\t%xmm4,%xmm0\n\tcmp\t\t$0x20,%rax\n\tjl\t\t.Lxorpart4\n\tmovdqu\t\t0x10(%rdx),%xmm1\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0x10(%rsi)\n\n\tmovdqu\t\t%xmm8,%xmm0\n\tcmp\t\t$0x30,%rax\n\tjl\t\t.Lxorpart4\n\tmovdqu\t\t0x20(%rdx),%xmm1\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0x20(%rsi)\n\n\tmovdqu\t\t%xmm12,%xmm0\n\tcmp\t\t$0x40,%rax\n\tjl\t\t.Lxorpart4\n\tmovdqu\t\t0x30(%rdx),%xmm1\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0x30(%rsi)\n\n\tmovdqa\t\t0x20(%rsp),%xmm0\n\tcmp\t\t$0x50,%rax\n\tjl\t\t.Lxorpart4\n\tmovdqu\t\t0x40(%rdx),%xmm1\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0x40(%rsi)\n\n\tmovdqu\t\t%xmm6,%xmm0\n\tcmp\t\t$0x60,%rax\n\tjl\t\t.Lxorpart4\n\tmovdqu\t\t0x50(%rdx),%xmm1\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0x50(%rsi)\n\n\tmovdqu\t\t%xmm10,%xmm0\n\tcmp\t\t$0x70,%rax\n\tjl\t\t.Lxorpart4\n\tmovdqu\t\t0x60(%rdx),%xmm1\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0x60(%rsi)\n\n\tmovdqu\t\t%xmm14,%xmm0\n\tcmp\t\t$0x80,%rax\n\tjl\t\t.Lxorpart4\n\tmovdqu\t\t0x70(%rdx),%xmm1\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0x70(%rsi)\n\n\tmovdqa\t\t0x10(%rsp),%xmm0\n\tcmp\t\t$0x90,%rax\n\tjl\t\t.Lxorpart4\n\tmovdqu\t\t0x80(%rdx),%xmm1\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0x80(%rsi)\n\n\tmovdqu\t\t%xmm5,%xmm0\n\tcmp\t\t$0xa0,%rax\n\tjl\t\t.Lxorpart4\n\tmovdqu\t\t0x90(%rdx),%xmm1\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0x90(%rsi)\n\n\tmovdqu\t\t%xmm9,%xmm0\n\tcmp\t\t$0xb0,%rax\n\tjl\t\t.Lxorpart4\n\tmovdqu\t\t0xa0(%rdx),%xmm1\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0xa0(%rsi)\n\n\tmovdqu\t\t%xmm13,%xmm0\n\tcmp\t\t$0xc0,%rax\n\tjl\t\t.Lxorpart4\n\tmovdqu\t\t0xb0(%rdx),%xmm1\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0xb0(%rsi)\n\n\tmovdqa\t\t0x30(%rsp),%xmm0\n\tcmp\t\t$0xd0,%rax\n\tjl\t\t.Lxorpart4\n\tmovdqu\t\t0xc0(%rdx),%xmm1\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0xc0(%rsi)\n\n\tmovdqu\t\t%xmm7,%xmm0\n\tcmp\t\t$0xe0,%rax\n\tjl\t\t.Lxorpart4\n\tmovdqu\t\t0xd0(%rdx),%xmm1\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0xd0(%rsi)\n\n\tmovdqu\t\t%xmm11,%xmm0\n\tcmp\t\t$0xf0,%rax\n\tjl\t\t.Lxorpart4\n\tmovdqu\t\t0xe0(%rdx),%xmm1\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0xe0(%rsi)\n\n\tmovdqu\t\t%xmm15,%xmm0\n\tcmp\t\t$0x100,%rax\n\tjl\t\t.Lxorpart4\n\tmovdqu\t\t0xf0(%rdx),%xmm1\n\tpxor\t\t%xmm1,%xmm0\n\tmovdqu\t\t%xmm0,0xf0(%rsi)\n\n.Ldone4:\n\tlea\t\t-8(%r10),%rsp\n\tRET\n\n.Lxorpart4:\n\t# xor remaining bytes from partial register into output\n\tmov\t\t%rax,%r9\n\tand\t\t$0x0f,%r9\n\tjz\t\t.Ldone4\n\tand\t\t$~0x0f,%rax\n\n\tmov\t\t%rsi,%r11\n\n\tlea\t\t(%rdx,%rax),%rsi\n\tmov\t\t%rsp,%rdi\n\tmov\t\t%r9,%rcx\n\trep movsb\n\n\tpxor\t\t0x00(%rsp),%xmm0\n\tmovdqa\t\t%xmm0,0x00(%rsp)\n\n\tmov\t\t%rsp,%rsi\n\tlea\t\t(%r11,%rax),%rdi\n\tmov\t\t%r9,%rcx\n\trep movsb\n\n\tjmp\t\t.Ldone4\n\nSYM_FUNC_END(chacha_4block_xor_ssse3)\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}