{
  "module_name": "sm4_aesni_avx_glue.c",
  "hash_id": "e2f914d656929eb7cc318a21dfab33e0f79e2f43ef85e238293c762e42c1d512",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/crypto/sm4_aesni_avx_glue.c",
  "human_readable_source": " \n \n\n#include <linux/module.h>\n#include <linux/crypto.h>\n#include <linux/kernel.h>\n#include <asm/simd.h>\n#include <crypto/internal/simd.h>\n#include <crypto/internal/skcipher.h>\n#include <crypto/sm4.h>\n#include \"sm4-avx.h\"\n\n#define SM4_CRYPT8_BLOCK_SIZE\t(SM4_BLOCK_SIZE * 8)\n\nasmlinkage void sm4_aesni_avx_crypt4(const u32 *rk, u8 *dst,\n\t\t\t\tconst u8 *src, int nblocks);\nasmlinkage void sm4_aesni_avx_crypt8(const u32 *rk, u8 *dst,\n\t\t\t\tconst u8 *src, int nblocks);\nasmlinkage void sm4_aesni_avx_ctr_enc_blk8(const u32 *rk, u8 *dst,\n\t\t\t\tconst u8 *src, u8 *iv);\nasmlinkage void sm4_aesni_avx_cbc_dec_blk8(const u32 *rk, u8 *dst,\n\t\t\t\tconst u8 *src, u8 *iv);\nasmlinkage void sm4_aesni_avx_cfb_dec_blk8(const u32 *rk, u8 *dst,\n\t\t\t\tconst u8 *src, u8 *iv);\n\nstatic int sm4_skcipher_setkey(struct crypto_skcipher *tfm, const u8 *key,\n\t\t\tunsigned int key_len)\n{\n\tstruct sm4_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\treturn sm4_expandkey(ctx, key, key_len);\n}\n\nstatic int ecb_do_crypt(struct skcipher_request *req, const u32 *rkey)\n{\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes) > 0) {\n\t\tconst u8 *src = walk.src.virt.addr;\n\t\tu8 *dst = walk.dst.virt.addr;\n\n\t\tkernel_fpu_begin();\n\t\twhile (nbytes >= SM4_CRYPT8_BLOCK_SIZE) {\n\t\t\tsm4_aesni_avx_crypt8(rkey, dst, src, 8);\n\t\t\tdst += SM4_CRYPT8_BLOCK_SIZE;\n\t\t\tsrc += SM4_CRYPT8_BLOCK_SIZE;\n\t\t\tnbytes -= SM4_CRYPT8_BLOCK_SIZE;\n\t\t}\n\t\twhile (nbytes >= SM4_BLOCK_SIZE) {\n\t\t\tunsigned int nblocks = min(nbytes >> 4, 4u);\n\t\t\tsm4_aesni_avx_crypt4(rkey, dst, src, nblocks);\n\t\t\tdst += nblocks * SM4_BLOCK_SIZE;\n\t\t\tsrc += nblocks * SM4_BLOCK_SIZE;\n\t\t\tnbytes -= nblocks * SM4_BLOCK_SIZE;\n\t\t}\n\t\tkernel_fpu_end();\n\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\n\treturn err;\n}\n\nint sm4_avx_ecb_encrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct sm4_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\treturn ecb_do_crypt(req, ctx->rkey_enc);\n}\nEXPORT_SYMBOL_GPL(sm4_avx_ecb_encrypt);\n\nint sm4_avx_ecb_decrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct sm4_ctx *ctx = crypto_skcipher_ctx(tfm);\n\n\treturn ecb_do_crypt(req, ctx->rkey_dec);\n}\nEXPORT_SYMBOL_GPL(sm4_avx_ecb_decrypt);\n\nint sm4_cbc_encrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct sm4_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes) > 0) {\n\t\tconst u8 *iv = walk.iv;\n\t\tconst u8 *src = walk.src.virt.addr;\n\t\tu8 *dst = walk.dst.virt.addr;\n\n\t\twhile (nbytes >= SM4_BLOCK_SIZE) {\n\t\t\tcrypto_xor_cpy(dst, src, iv, SM4_BLOCK_SIZE);\n\t\t\tsm4_crypt_block(ctx->rkey_enc, dst, dst);\n\t\t\tiv = dst;\n\t\t\tsrc += SM4_BLOCK_SIZE;\n\t\t\tdst += SM4_BLOCK_SIZE;\n\t\t\tnbytes -= SM4_BLOCK_SIZE;\n\t\t}\n\t\tif (iv != walk.iv)\n\t\t\tmemcpy(walk.iv, iv, SM4_BLOCK_SIZE);\n\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(sm4_cbc_encrypt);\n\nint sm4_avx_cbc_decrypt(struct skcipher_request *req,\n\t\t\tunsigned int bsize, sm4_crypt_func func)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct sm4_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes) > 0) {\n\t\tconst u8 *src = walk.src.virt.addr;\n\t\tu8 *dst = walk.dst.virt.addr;\n\n\t\tkernel_fpu_begin();\n\n\t\twhile (nbytes >= bsize) {\n\t\t\tfunc(ctx->rkey_dec, dst, src, walk.iv);\n\t\t\tdst += bsize;\n\t\t\tsrc += bsize;\n\t\t\tnbytes -= bsize;\n\t\t}\n\n\t\twhile (nbytes >= SM4_BLOCK_SIZE) {\n\t\t\tu8 keystream[SM4_BLOCK_SIZE * 8];\n\t\t\tu8 iv[SM4_BLOCK_SIZE];\n\t\t\tunsigned int nblocks = min(nbytes >> 4, 8u);\n\t\t\tint i;\n\n\t\t\tsm4_aesni_avx_crypt8(ctx->rkey_dec, keystream,\n\t\t\t\t\t\tsrc, nblocks);\n\n\t\t\tsrc += ((int)nblocks - 2) * SM4_BLOCK_SIZE;\n\t\t\tdst += (nblocks - 1) * SM4_BLOCK_SIZE;\n\t\t\tmemcpy(iv, src + SM4_BLOCK_SIZE, SM4_BLOCK_SIZE);\n\n\t\t\tfor (i = nblocks - 1; i > 0; i--) {\n\t\t\t\tcrypto_xor_cpy(dst, src,\n\t\t\t\t\t&keystream[i * SM4_BLOCK_SIZE],\n\t\t\t\t\tSM4_BLOCK_SIZE);\n\t\t\t\tsrc -= SM4_BLOCK_SIZE;\n\t\t\t\tdst -= SM4_BLOCK_SIZE;\n\t\t\t}\n\t\t\tcrypto_xor_cpy(dst, walk.iv, keystream, SM4_BLOCK_SIZE);\n\t\t\tmemcpy(walk.iv, iv, SM4_BLOCK_SIZE);\n\t\t\tdst += nblocks * SM4_BLOCK_SIZE;\n\t\t\tsrc += (nblocks + 1) * SM4_BLOCK_SIZE;\n\t\t\tnbytes -= nblocks * SM4_BLOCK_SIZE;\n\t\t}\n\n\t\tkernel_fpu_end();\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(sm4_avx_cbc_decrypt);\n\nstatic int cbc_decrypt(struct skcipher_request *req)\n{\n\treturn sm4_avx_cbc_decrypt(req, SM4_CRYPT8_BLOCK_SIZE,\n\t\t\t\tsm4_aesni_avx_cbc_dec_blk8);\n}\n\nint sm4_cfb_encrypt(struct skcipher_request *req)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct sm4_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes) > 0) {\n\t\tu8 keystream[SM4_BLOCK_SIZE];\n\t\tconst u8 *iv = walk.iv;\n\t\tconst u8 *src = walk.src.virt.addr;\n\t\tu8 *dst = walk.dst.virt.addr;\n\n\t\twhile (nbytes >= SM4_BLOCK_SIZE) {\n\t\t\tsm4_crypt_block(ctx->rkey_enc, keystream, iv);\n\t\t\tcrypto_xor_cpy(dst, src, keystream, SM4_BLOCK_SIZE);\n\t\t\tiv = dst;\n\t\t\tsrc += SM4_BLOCK_SIZE;\n\t\t\tdst += SM4_BLOCK_SIZE;\n\t\t\tnbytes -= SM4_BLOCK_SIZE;\n\t\t}\n\t\tif (iv != walk.iv)\n\t\t\tmemcpy(walk.iv, iv, SM4_BLOCK_SIZE);\n\n\t\t \n\t\tif (walk.nbytes == walk.total && nbytes > 0) {\n\t\t\tsm4_crypt_block(ctx->rkey_enc, keystream, walk.iv);\n\t\t\tcrypto_xor_cpy(dst, src, keystream, nbytes);\n\t\t\tnbytes = 0;\n\t\t}\n\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(sm4_cfb_encrypt);\n\nint sm4_avx_cfb_decrypt(struct skcipher_request *req,\n\t\t\tunsigned int bsize, sm4_crypt_func func)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct sm4_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes) > 0) {\n\t\tconst u8 *src = walk.src.virt.addr;\n\t\tu8 *dst = walk.dst.virt.addr;\n\n\t\tkernel_fpu_begin();\n\n\t\twhile (nbytes >= bsize) {\n\t\t\tfunc(ctx->rkey_enc, dst, src, walk.iv);\n\t\t\tdst += bsize;\n\t\t\tsrc += bsize;\n\t\t\tnbytes -= bsize;\n\t\t}\n\n\t\twhile (nbytes >= SM4_BLOCK_SIZE) {\n\t\t\tu8 keystream[SM4_BLOCK_SIZE * 8];\n\t\t\tunsigned int nblocks = min(nbytes >> 4, 8u);\n\n\t\t\tmemcpy(keystream, walk.iv, SM4_BLOCK_SIZE);\n\t\t\tif (nblocks > 1)\n\t\t\t\tmemcpy(&keystream[SM4_BLOCK_SIZE], src,\n\t\t\t\t\t(nblocks - 1) * SM4_BLOCK_SIZE);\n\t\t\tmemcpy(walk.iv, src + (nblocks - 1) * SM4_BLOCK_SIZE,\n\t\t\t\tSM4_BLOCK_SIZE);\n\n\t\t\tsm4_aesni_avx_crypt8(ctx->rkey_enc, keystream,\n\t\t\t\t\t\tkeystream, nblocks);\n\n\t\t\tcrypto_xor_cpy(dst, src, keystream,\n\t\t\t\t\tnblocks * SM4_BLOCK_SIZE);\n\t\t\tdst += nblocks * SM4_BLOCK_SIZE;\n\t\t\tsrc += nblocks * SM4_BLOCK_SIZE;\n\t\t\tnbytes -= nblocks * SM4_BLOCK_SIZE;\n\t\t}\n\n\t\tkernel_fpu_end();\n\n\t\t \n\t\tif (walk.nbytes == walk.total && nbytes > 0) {\n\t\t\tu8 keystream[SM4_BLOCK_SIZE];\n\n\t\t\tsm4_crypt_block(ctx->rkey_enc, keystream, walk.iv);\n\t\t\tcrypto_xor_cpy(dst, src, keystream, nbytes);\n\t\t\tnbytes = 0;\n\t\t}\n\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(sm4_avx_cfb_decrypt);\n\nstatic int cfb_decrypt(struct skcipher_request *req)\n{\n\treturn sm4_avx_cfb_decrypt(req, SM4_CRYPT8_BLOCK_SIZE,\n\t\t\t\tsm4_aesni_avx_cfb_dec_blk8);\n}\n\nint sm4_avx_ctr_crypt(struct skcipher_request *req,\n\t\t\tunsigned int bsize, sm4_crypt_func func)\n{\n\tstruct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);\n\tstruct sm4_ctx *ctx = crypto_skcipher_ctx(tfm);\n\tstruct skcipher_walk walk;\n\tunsigned int nbytes;\n\tint err;\n\n\terr = skcipher_walk_virt(&walk, req, false);\n\n\twhile ((nbytes = walk.nbytes) > 0) {\n\t\tconst u8 *src = walk.src.virt.addr;\n\t\tu8 *dst = walk.dst.virt.addr;\n\n\t\tkernel_fpu_begin();\n\n\t\twhile (nbytes >= bsize) {\n\t\t\tfunc(ctx->rkey_enc, dst, src, walk.iv);\n\t\t\tdst += bsize;\n\t\t\tsrc += bsize;\n\t\t\tnbytes -= bsize;\n\t\t}\n\n\t\twhile (nbytes >= SM4_BLOCK_SIZE) {\n\t\t\tu8 keystream[SM4_BLOCK_SIZE * 8];\n\t\t\tunsigned int nblocks = min(nbytes >> 4, 8u);\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < nblocks; i++) {\n\t\t\t\tmemcpy(&keystream[i * SM4_BLOCK_SIZE],\n\t\t\t\t\twalk.iv, SM4_BLOCK_SIZE);\n\t\t\t\tcrypto_inc(walk.iv, SM4_BLOCK_SIZE);\n\t\t\t}\n\t\t\tsm4_aesni_avx_crypt8(ctx->rkey_enc, keystream,\n\t\t\t\t\tkeystream, nblocks);\n\n\t\t\tcrypto_xor_cpy(dst, src, keystream,\n\t\t\t\t\tnblocks * SM4_BLOCK_SIZE);\n\t\t\tdst += nblocks * SM4_BLOCK_SIZE;\n\t\t\tsrc += nblocks * SM4_BLOCK_SIZE;\n\t\t\tnbytes -= nblocks * SM4_BLOCK_SIZE;\n\t\t}\n\n\t\tkernel_fpu_end();\n\n\t\t \n\t\tif (walk.nbytes == walk.total && nbytes > 0) {\n\t\t\tu8 keystream[SM4_BLOCK_SIZE];\n\n\t\t\tmemcpy(keystream, walk.iv, SM4_BLOCK_SIZE);\n\t\t\tcrypto_inc(walk.iv, SM4_BLOCK_SIZE);\n\n\t\t\tsm4_crypt_block(ctx->rkey_enc, keystream, keystream);\n\n\t\t\tcrypto_xor_cpy(dst, src, keystream, nbytes);\n\t\t\tdst += nbytes;\n\t\t\tsrc += nbytes;\n\t\t\tnbytes = 0;\n\t\t}\n\n\t\terr = skcipher_walk_done(&walk, nbytes);\n\t}\n\n\treturn err;\n}\nEXPORT_SYMBOL_GPL(sm4_avx_ctr_crypt);\n\nstatic int ctr_crypt(struct skcipher_request *req)\n{\n\treturn sm4_avx_ctr_crypt(req, SM4_CRYPT8_BLOCK_SIZE,\n\t\t\t\tsm4_aesni_avx_ctr_enc_blk8);\n}\n\nstatic struct skcipher_alg sm4_aesni_avx_skciphers[] = {\n\t{\n\t\t.base = {\n\t\t\t.cra_name\t\t= \"__ecb(sm4)\",\n\t\t\t.cra_driver_name\t= \"__ecb-sm4-aesni-avx\",\n\t\t\t.cra_priority\t\t= 400,\n\t\t\t.cra_flags\t\t= CRYPTO_ALG_INTERNAL,\n\t\t\t.cra_blocksize\t\t= SM4_BLOCK_SIZE,\n\t\t\t.cra_ctxsize\t\t= sizeof(struct sm4_ctx),\n\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t},\n\t\t.min_keysize\t= SM4_KEY_SIZE,\n\t\t.max_keysize\t= SM4_KEY_SIZE,\n\t\t.walksize\t= 8 * SM4_BLOCK_SIZE,\n\t\t.setkey\t\t= sm4_skcipher_setkey,\n\t\t.encrypt\t= sm4_avx_ecb_encrypt,\n\t\t.decrypt\t= sm4_avx_ecb_decrypt,\n\t}, {\n\t\t.base = {\n\t\t\t.cra_name\t\t= \"__cbc(sm4)\",\n\t\t\t.cra_driver_name\t= \"__cbc-sm4-aesni-avx\",\n\t\t\t.cra_priority\t\t= 400,\n\t\t\t.cra_flags\t\t= CRYPTO_ALG_INTERNAL,\n\t\t\t.cra_blocksize\t\t= SM4_BLOCK_SIZE,\n\t\t\t.cra_ctxsize\t\t= sizeof(struct sm4_ctx),\n\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t},\n\t\t.min_keysize\t= SM4_KEY_SIZE,\n\t\t.max_keysize\t= SM4_KEY_SIZE,\n\t\t.ivsize\t\t= SM4_BLOCK_SIZE,\n\t\t.walksize\t= 8 * SM4_BLOCK_SIZE,\n\t\t.setkey\t\t= sm4_skcipher_setkey,\n\t\t.encrypt\t= sm4_cbc_encrypt,\n\t\t.decrypt\t= cbc_decrypt,\n\t}, {\n\t\t.base = {\n\t\t\t.cra_name\t\t= \"__cfb(sm4)\",\n\t\t\t.cra_driver_name\t= \"__cfb-sm4-aesni-avx\",\n\t\t\t.cra_priority\t\t= 400,\n\t\t\t.cra_flags\t\t= CRYPTO_ALG_INTERNAL,\n\t\t\t.cra_blocksize\t\t= 1,\n\t\t\t.cra_ctxsize\t\t= sizeof(struct sm4_ctx),\n\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t},\n\t\t.min_keysize\t= SM4_KEY_SIZE,\n\t\t.max_keysize\t= SM4_KEY_SIZE,\n\t\t.ivsize\t\t= SM4_BLOCK_SIZE,\n\t\t.chunksize\t= SM4_BLOCK_SIZE,\n\t\t.walksize\t= 8 * SM4_BLOCK_SIZE,\n\t\t.setkey\t\t= sm4_skcipher_setkey,\n\t\t.encrypt\t= sm4_cfb_encrypt,\n\t\t.decrypt\t= cfb_decrypt,\n\t}, {\n\t\t.base = {\n\t\t\t.cra_name\t\t= \"__ctr(sm4)\",\n\t\t\t.cra_driver_name\t= \"__ctr-sm4-aesni-avx\",\n\t\t\t.cra_priority\t\t= 400,\n\t\t\t.cra_flags\t\t= CRYPTO_ALG_INTERNAL,\n\t\t\t.cra_blocksize\t\t= 1,\n\t\t\t.cra_ctxsize\t\t= sizeof(struct sm4_ctx),\n\t\t\t.cra_module\t\t= THIS_MODULE,\n\t\t},\n\t\t.min_keysize\t= SM4_KEY_SIZE,\n\t\t.max_keysize\t= SM4_KEY_SIZE,\n\t\t.ivsize\t\t= SM4_BLOCK_SIZE,\n\t\t.chunksize\t= SM4_BLOCK_SIZE,\n\t\t.walksize\t= 8 * SM4_BLOCK_SIZE,\n\t\t.setkey\t\t= sm4_skcipher_setkey,\n\t\t.encrypt\t= ctr_crypt,\n\t\t.decrypt\t= ctr_crypt,\n\t}\n};\n\nstatic struct simd_skcipher_alg *\nsimd_sm4_aesni_avx_skciphers[ARRAY_SIZE(sm4_aesni_avx_skciphers)];\n\nstatic int __init sm4_init(void)\n{\n\tconst char *feature_name;\n\n\tif (!boot_cpu_has(X86_FEATURE_AVX) ||\n\t    !boot_cpu_has(X86_FEATURE_AES) ||\n\t    !boot_cpu_has(X86_FEATURE_OSXSAVE)) {\n\t\tpr_info(\"AVX or AES-NI instructions are not detected.\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (!cpu_has_xfeatures(XFEATURE_MASK_SSE | XFEATURE_MASK_YMM,\n\t\t\t\t&feature_name)) {\n\t\tpr_info(\"CPU feature '%s' is not supported.\\n\", feature_name);\n\t\treturn -ENODEV;\n\t}\n\n\treturn simd_register_skciphers_compat(sm4_aesni_avx_skciphers,\n\t\t\t\t\tARRAY_SIZE(sm4_aesni_avx_skciphers),\n\t\t\t\t\tsimd_sm4_aesni_avx_skciphers);\n}\n\nstatic void __exit sm4_exit(void)\n{\n\tsimd_unregister_skciphers(sm4_aesni_avx_skciphers,\n\t\t\t\t\tARRAY_SIZE(sm4_aesni_avx_skciphers),\n\t\t\t\t\tsimd_sm4_aesni_avx_skciphers);\n}\n\nmodule_init(sm4_init);\nmodule_exit(sm4_exit);\n\nMODULE_LICENSE(\"GPL v2\");\nMODULE_AUTHOR(\"Tianjia Zhang <tianjia.zhang@linux.alibaba.com>\");\nMODULE_DESCRIPTION(\"SM4 Cipher Algorithm, AES-NI/AVX optimized\");\nMODULE_ALIAS_CRYPTO(\"sm4\");\nMODULE_ALIAS_CRYPTO(\"sm4-aesni-avx\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}