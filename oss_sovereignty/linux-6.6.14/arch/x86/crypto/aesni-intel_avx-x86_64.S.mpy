{
  "module_name": "aesni-intel_avx-x86_64.S",
  "hash_id": "a9b7fbd8629b321a689d48ae6a0fae383b2d54e20b38a3eb442b751c270d00dc",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/crypto/aesni-intel_avx-x86_64.S",
  "human_readable_source": "########################################################################\n# Copyright (c) 2013, Intel Corporation\n#\n# This software is available to you under a choice of one of two\n# licenses.  You may choose to be licensed under the terms of the GNU\n# General Public License (GPL) Version 2, available from the file\n# COPYING in the main directory of this source tree, or the\n# OpenIB.org BSD license below:\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n# * Redistributions of source code must retain the above copyright\n#   notice, this list of conditions and the following disclaimer.\n#\n# * Redistributions in binary form must reproduce the above copyright\n#   notice, this list of conditions and the following disclaimer in the\n#   documentation and/or other materials provided with the\n#   distribution.\n#\n# * Neither the name of the Intel Corporation nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n#\n#\n# THIS SOFTWARE IS PROVIDED BY INTEL CORPORATION \"\"AS IS\"\" AND ANY\n# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL INTEL CORPORATION OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES# LOSS OF USE, DATA, OR\n# PROFITS# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n# LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n# NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n########################################################################\n##\n## Authors:\n##\tErdinc Ozturk <erdinc.ozturk@intel.com>\n##\tVinodh Gopal <vinodh.gopal@intel.com>\n##\tJames Guilford <james.guilford@intel.com>\n##\tTim Chen <tim.c.chen@linux.intel.com>\n##\n## References:\n##       This code was derived and highly optimized from the code described in paper:\n##               Vinodh Gopal et. al. Optimized Galois-Counter-Mode Implementation\n##\t\t\ton Intel Architecture Processors. August, 2010\n##       The details of the implementation is explained in:\n##               Erdinc Ozturk et. al. Enabling High-Performance Galois-Counter-Mode\n##\t\t\ton Intel Architecture Processors. October, 2012.\n##\n## Assumptions:\n##\n##\n##\n## iv:\n##       0                   1                   2                   3\n##       0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n##       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n##       |                             Salt  (From the SA)               |\n##       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n##       |                     Initialization Vector                     |\n##       |         (This is the sequence number from IPSec header)       |\n##       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n##       |                              0x1                              |\n##       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n##\n##\n##\n## AAD:\n##       AAD padded to 128 bits with 0\n##       for example, assume AAD is a u32 vector\n##\n##       if AAD is 8 bytes:\n##       AAD[3] = {A0, A1}#\n##       padded AAD in xmm register = {A1 A0 0 0}\n##\n##       0                   1                   2                   3\n##       0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n##       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n##       |                               SPI (A1)                        |\n##       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n##       |                     32-bit Sequence Number (A0)               |\n##       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n##       |                              0x0                              |\n##       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n##\n##                                       AAD Format with 32-bit Sequence Number\n##\n##       if AAD is 12 bytes:\n##       AAD[3] = {A0, A1, A2}#\n##       padded AAD in xmm register = {A2 A1 A0 0}\n##\n##       0                   1                   2                   3\n##       0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n##       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n##       |                               SPI (A2)                        |\n##       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n##       |                 64-bit Extended Sequence Number {A1,A0}       |\n##       |                                                               |\n##       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n##       |                              0x0                              |\n##       +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n##\n##        AAD Format with 64-bit Extended Sequence Number\n##\n##\n## aadLen:\n##       from the definition of the spec, aadLen can only be 8 or 12 bytes.\n##\t The code additionally supports aadLen of length 16 bytes.\n##\n## TLen:\n##       from the definition of the spec, TLen can only be 8, 12 or 16 bytes.\n##\n## poly = x^128 + x^127 + x^126 + x^121 + 1\n## throughout the code, one tab and two tab indentations are used. one tab is\n## for GHASH part, two tabs is for AES part.\n##\n\n#include <linux/linkage.h>\n\n# constants in mergeable sections, linker can reorder and merge\n.section\t.rodata.cst16.POLY, \"aM\", @progbits, 16\n.align 16\nPOLY:            .octa     0xC2000000000000000000000000000001\n\n.section\t.rodata.cst16.POLY2, \"aM\", @progbits, 16\n.align 16\nPOLY2:           .octa     0xC20000000000000000000001C2000000\n\n.section\t.rodata.cst16.TWOONE, \"aM\", @progbits, 16\n.align 16\nTWOONE:          .octa     0x00000001000000000000000000000001\n\n.section\t.rodata.cst16.SHUF_MASK, \"aM\", @progbits, 16\n.align 16\nSHUF_MASK:       .octa     0x000102030405060708090A0B0C0D0E0F\n\n.section\t.rodata.cst16.ONE, \"aM\", @progbits, 16\n.align 16\nONE:             .octa     0x00000000000000000000000000000001\n\n.section\t.rodata.cst16.ONEf, \"aM\", @progbits, 16\n.align 16\nONEf:            .octa     0x01000000000000000000000000000000\n\n# order of these constants should not change.\n# more specifically, ALL_F should follow SHIFT_MASK, and zero should follow ALL_F\n.section\t.rodata, \"a\", @progbits\n.align 16\nSHIFT_MASK:      .octa     0x0f0e0d0c0b0a09080706050403020100\nALL_F:           .octa     0xffffffffffffffffffffffffffffffff\n                 .octa     0x00000000000000000000000000000000\n\n.text\n\n\n#define AadHash 16*0\n#define AadLen 16*1\n#define InLen (16*1)+8\n#define PBlockEncKey 16*2\n#define OrigIV 16*3\n#define CurCount 16*4\n#define PBlockLen 16*5\n\nHashKey        = 16*6   # store HashKey <<1 mod poly here\nHashKey_2      = 16*7   # store HashKey^2 <<1 mod poly here\nHashKey_3      = 16*8   # store HashKey^3 <<1 mod poly here\nHashKey_4      = 16*9   # store HashKey^4 <<1 mod poly here\nHashKey_5      = 16*10   # store HashKey^5 <<1 mod poly here\nHashKey_6      = 16*11   # store HashKey^6 <<1 mod poly here\nHashKey_7      = 16*12   # store HashKey^7 <<1 mod poly here\nHashKey_8      = 16*13   # store HashKey^8 <<1 mod poly here\nHashKey_k      = 16*14   # store XOR of HashKey <<1 mod poly here (for Karatsuba purposes)\nHashKey_2_k    = 16*15   # store XOR of HashKey^2 <<1 mod poly here (for Karatsuba purposes)\nHashKey_3_k    = 16*16   # store XOR of HashKey^3 <<1 mod poly here (for Karatsuba purposes)\nHashKey_4_k    = 16*17   # store XOR of HashKey^4 <<1 mod poly here (for Karatsuba purposes)\nHashKey_5_k    = 16*18   # store XOR of HashKey^5 <<1 mod poly here (for Karatsuba purposes)\nHashKey_6_k    = 16*19   # store XOR of HashKey^6 <<1 mod poly here (for Karatsuba purposes)\nHashKey_7_k    = 16*20   # store XOR of HashKey^7 <<1 mod poly here (for Karatsuba purposes)\nHashKey_8_k    = 16*21   # store XOR of HashKey^8 <<1 mod poly here (for Karatsuba purposes)\n\n#define arg1 %rdi\n#define arg2 %rsi\n#define arg3 %rdx\n#define arg4 %rcx\n#define arg5 %r8\n#define arg6 %r9\n#define keysize 2*15*16(arg1)\n\ni = 0\nj = 0\n\nout_order = 0\nin_order = 1\nDEC = 0\nENC = 1\n\n.macro define_reg r n\nreg_\\r = %xmm\\n\n.endm\n\n.macro setreg\n.altmacro\ndefine_reg i %i\ndefine_reg j %j\n.noaltmacro\n.endm\n\nTMP1 =   16*0    # Temporary storage for AAD\nTMP2 =   16*1    # Temporary storage for AES State 2 (State 1 is stored in an XMM register)\nTMP3 =   16*2    # Temporary storage for AES State 3\nTMP4 =   16*3    # Temporary storage for AES State 4\nTMP5 =   16*4    # Temporary storage for AES State 5\nTMP6 =   16*5    # Temporary storage for AES State 6\nTMP7 =   16*6    # Temporary storage for AES State 7\nTMP8 =   16*7    # Temporary storage for AES State 8\n\nVARIABLE_OFFSET = 16*8\n\n################################\n# Utility Macros\n################################\n\n.macro FUNC_SAVE\n        push    %r12\n        push    %r13\n        push    %r15\n\n\tpush\t%rbp\n\tmov\t%rsp, %rbp\n\n        sub     $VARIABLE_OFFSET, %rsp\n        and     $~63, %rsp                    # align rsp to 64 bytes\n.endm\n\n.macro FUNC_RESTORE\n        mov     %rbp, %rsp\n\tpop\t%rbp\n\n        pop     %r15\n        pop     %r13\n        pop     %r12\n.endm\n\n# Encryption of a single block\n.macro ENCRYPT_SINGLE_BLOCK REP XMM0\n                vpxor    (arg1), \\XMM0, \\XMM0\n               i = 1\n               setreg\n.rep \\REP\n                vaesenc  16*i(arg1), \\XMM0, \\XMM0\n               i = (i+1)\n               setreg\n.endr\n                vaesenclast 16*i(arg1), \\XMM0, \\XMM0\n.endm\n\n# combined for GCM encrypt and decrypt functions\n# clobbering all xmm registers\n# clobbering r10, r11, r12, r13, r15, rax\n.macro  GCM_ENC_DEC INITIAL_BLOCKS GHASH_8_ENCRYPT_8_PARALLEL GHASH_LAST_8 GHASH_MUL ENC_DEC REP\n        vmovdqu AadHash(arg2), %xmm8\n        vmovdqu  HashKey(arg2), %xmm13      # xmm13 = HashKey\n        add arg5, InLen(arg2)\n\n        # initialize the data pointer offset as zero\n        xor     %r11d, %r11d\n\n        PARTIAL_BLOCK \\GHASH_MUL, arg3, arg4, arg5, %r11, %xmm8, \\ENC_DEC\n        sub %r11, arg5\n\n        mov     arg5, %r13                  # save the number of bytes of plaintext/ciphertext\n        and     $-16, %r13                  # r13 = r13 - (r13 mod 16)\n\n        mov     %r13, %r12\n        shr     $4, %r12\n        and     $7, %r12\n        jz      .L_initial_num_blocks_is_0\\@\n\n        cmp     $7, %r12\n        je      .L_initial_num_blocks_is_7\\@\n        cmp     $6, %r12\n        je      .L_initial_num_blocks_is_6\\@\n        cmp     $5, %r12\n        je      .L_initial_num_blocks_is_5\\@\n        cmp     $4, %r12\n        je      .L_initial_num_blocks_is_4\\@\n        cmp     $3, %r12\n        je      .L_initial_num_blocks_is_3\\@\n        cmp     $2, %r12\n        je      .L_initial_num_blocks_is_2\\@\n\n        jmp     .L_initial_num_blocks_is_1\\@\n\n.L_initial_num_blocks_is_7\\@:\n        \\INITIAL_BLOCKS  \\REP, 7, %xmm12, %xmm13, %xmm14, %xmm15, %xmm11, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm10, %xmm0, \\ENC_DEC\n        sub     $16*7, %r13\n        jmp     .L_initial_blocks_encrypted\\@\n\n.L_initial_num_blocks_is_6\\@:\n        \\INITIAL_BLOCKS  \\REP, 6, %xmm12, %xmm13, %xmm14, %xmm15, %xmm11, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm10, %xmm0, \\ENC_DEC\n        sub     $16*6, %r13\n        jmp     .L_initial_blocks_encrypted\\@\n\n.L_initial_num_blocks_is_5\\@:\n        \\INITIAL_BLOCKS  \\REP, 5, %xmm12, %xmm13, %xmm14, %xmm15, %xmm11, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm10, %xmm0, \\ENC_DEC\n        sub     $16*5, %r13\n        jmp     .L_initial_blocks_encrypted\\@\n\n.L_initial_num_blocks_is_4\\@:\n        \\INITIAL_BLOCKS  \\REP, 4, %xmm12, %xmm13, %xmm14, %xmm15, %xmm11, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm10, %xmm0, \\ENC_DEC\n        sub     $16*4, %r13\n        jmp     .L_initial_blocks_encrypted\\@\n\n.L_initial_num_blocks_is_3\\@:\n        \\INITIAL_BLOCKS  \\REP, 3, %xmm12, %xmm13, %xmm14, %xmm15, %xmm11, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm10, %xmm0, \\ENC_DEC\n        sub     $16*3, %r13\n        jmp     .L_initial_blocks_encrypted\\@\n\n.L_initial_num_blocks_is_2\\@:\n        \\INITIAL_BLOCKS  \\REP, 2, %xmm12, %xmm13, %xmm14, %xmm15, %xmm11, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm10, %xmm0, \\ENC_DEC\n        sub     $16*2, %r13\n        jmp     .L_initial_blocks_encrypted\\@\n\n.L_initial_num_blocks_is_1\\@:\n        \\INITIAL_BLOCKS  \\REP, 1, %xmm12, %xmm13, %xmm14, %xmm15, %xmm11, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm10, %xmm0, \\ENC_DEC\n        sub     $16*1, %r13\n        jmp     .L_initial_blocks_encrypted\\@\n\n.L_initial_num_blocks_is_0\\@:\n        \\INITIAL_BLOCKS  \\REP, 0, %xmm12, %xmm13, %xmm14, %xmm15, %xmm11, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm10, %xmm0, \\ENC_DEC\n\n\n.L_initial_blocks_encrypted\\@:\n        test    %r13, %r13\n        je      .L_zero_cipher_left\\@\n\n        sub     $128, %r13\n        je      .L_eight_cipher_left\\@\n\n\n\n\n        vmovd   %xmm9, %r15d\n        and     $255, %r15d\n        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9\n\n\n.L_encrypt_by_8_new\\@:\n        cmp     $(255-8), %r15d\n        jg      .L_encrypt_by_8\\@\n\n\n\n        add     $8, %r15b\n        \\GHASH_8_ENCRYPT_8_PARALLEL      \\REP, %xmm0, %xmm10, %xmm11, %xmm12, %xmm13, %xmm14, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm15, out_order, \\ENC_DEC\n        add     $128, %r11\n        sub     $128, %r13\n        jne     .L_encrypt_by_8_new\\@\n\n        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9\n        jmp     .L_eight_cipher_left\\@\n\n.L_encrypt_by_8\\@:\n        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9\n        add     $8, %r15b\n        \\GHASH_8_ENCRYPT_8_PARALLEL      \\REP, %xmm0, %xmm10, %xmm11, %xmm12, %xmm13, %xmm14, %xmm9, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8, %xmm15, in_order, \\ENC_DEC\n        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9\n        add     $128, %r11\n        sub     $128, %r13\n        jne     .L_encrypt_by_8_new\\@\n\n        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9\n\n\n\n\n.L_eight_cipher_left\\@:\n        \\GHASH_LAST_8    %xmm0, %xmm10, %xmm11, %xmm12, %xmm13, %xmm14, %xmm15, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5, %xmm6, %xmm7, %xmm8\n\n\n.L_zero_cipher_left\\@:\n        vmovdqu %xmm14, AadHash(arg2)\n        vmovdqu %xmm9, CurCount(arg2)\n\n        # check for 0 length\n        mov     arg5, %r13\n        and     $15, %r13                            # r13 = (arg5 mod 16)\n\n        je      .L_multiple_of_16_bytes\\@\n\n        # handle the last <16 Byte block separately\n\n        mov %r13, PBlockLen(arg2)\n\n        vpaddd  ONE(%rip), %xmm9, %xmm9              # INCR CNT to get Yn\n        vmovdqu %xmm9, CurCount(arg2)\n        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9\n\n        ENCRYPT_SINGLE_BLOCK    \\REP, %xmm9                # E(K, Yn)\n        vmovdqu %xmm9, PBlockEncKey(arg2)\n\n        cmp $16, arg5\n        jge .L_large_enough_update\\@\n\n        lea (arg4,%r11,1), %r10\n        mov %r13, %r12\n\n        READ_PARTIAL_BLOCK %r10 %r12 %xmm1\n\n        lea     SHIFT_MASK+16(%rip), %r12\n        sub     %r13, %r12                           # adjust the shuffle mask pointer to be\n\t\t\t\t\t\t     # able to shift 16-r13 bytes (r13 is the\n\t# number of bytes in plaintext mod 16)\n\n        jmp .L_final_ghash_mul\\@\n\n.L_large_enough_update\\@:\n        sub $16, %r11\n        add %r13, %r11\n\n        # receive the last <16 Byte block\n        vmovdqu\t(arg4, %r11, 1), %xmm1\n\n        sub\t%r13, %r11\n        add\t$16, %r11\n\n        lea\tSHIFT_MASK+16(%rip), %r12\n        # adjust the shuffle mask pointer to be able to shift 16-r13 bytes\n        # (r13 is the number of bytes in plaintext mod 16)\n        sub\t%r13, %r12\n        # get the appropriate shuffle mask\n        vmovdqu\t(%r12), %xmm2\n        # shift right 16-r13 bytes\n        vpshufb  %xmm2, %xmm1, %xmm1\n\n.L_final_ghash_mul\\@:\n        .if  \\ENC_DEC ==  DEC\n        vmovdqa %xmm1, %xmm2\n        vpxor   %xmm1, %xmm9, %xmm9                  # Plaintext XOR E(K, Yn)\n        vmovdqu ALL_F-SHIFT_MASK(%r12), %xmm1        # get the appropriate mask to\n\t\t\t\t\t\t     # mask out top 16-r13 bytes of xmm9\n        vpand   %xmm1, %xmm9, %xmm9                  # mask out top 16-r13 bytes of xmm9\n        vpand   %xmm1, %xmm2, %xmm2\n        vpshufb SHUF_MASK(%rip), %xmm2, %xmm2\n        vpxor   %xmm2, %xmm14, %xmm14\n\n        vmovdqu %xmm14, AadHash(arg2)\n        .else\n        vpxor   %xmm1, %xmm9, %xmm9                  # Plaintext XOR E(K, Yn)\n        vmovdqu ALL_F-SHIFT_MASK(%r12), %xmm1        # get the appropriate mask to\n\t\t\t\t\t\t     # mask out top 16-r13 bytes of xmm9\n        vpand   %xmm1, %xmm9, %xmm9                  # mask out top 16-r13 bytes of xmm9\n        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9\n        vpxor   %xmm9, %xmm14, %xmm14\n\n        vmovdqu %xmm14, AadHash(arg2)\n        vpshufb SHUF_MASK(%rip), %xmm9, %xmm9        # shuffle xmm9 back to output as ciphertext\n        .endif\n\n\n        #############################\n        # output r13 Bytes\n        vmovq   %xmm9, %rax\n        cmp     $8, %r13\n        jle     .L_less_than_8_bytes_left\\@\n\n        mov     %rax, (arg3 , %r11)\n        add     $8, %r11\n        vpsrldq $8, %xmm9, %xmm9\n        vmovq   %xmm9, %rax\n        sub     $8, %r13\n\n.L_less_than_8_bytes_left\\@:\n        movb    %al, (arg3 , %r11)\n        add     $1, %r11\n        shr     $8, %rax\n        sub     $1, %r13\n        jne     .L_less_than_8_bytes_left\\@\n        #############################\n\n.L_multiple_of_16_bytes\\@:\n.endm\n\n\n# GCM_COMPLETE Finishes update of tag of last partial block\n# Output: Authorization Tag (AUTH_TAG)\n# Clobbers rax, r10-r12, and xmm0, xmm1, xmm5-xmm15\n.macro GCM_COMPLETE GHASH_MUL REP AUTH_TAG AUTH_TAG_LEN\n        vmovdqu AadHash(arg2), %xmm14\n        vmovdqu HashKey(arg2), %xmm13\n\n        mov PBlockLen(arg2), %r12\n        test %r12, %r12\n        je .L_partial_done\\@\n\n\t#GHASH computation for the last <16 Byte block\n        \\GHASH_MUL       %xmm14, %xmm13, %xmm0, %xmm10, %xmm11, %xmm5, %xmm6\n\n.L_partial_done\\@:\n        mov AadLen(arg2), %r12                          # r12 = aadLen (number of bytes)\n        shl     $3, %r12                             # convert into number of bits\n        vmovd   %r12d, %xmm15                        # len(A) in xmm15\n\n        mov InLen(arg2), %r12\n        shl     $3, %r12                        # len(C) in bits  (*128)\n        vmovq   %r12, %xmm1\n        vpslldq $8, %xmm15, %xmm15                   # xmm15 = len(A)|| 0x0000000000000000\n        vpxor   %xmm1, %xmm15, %xmm15                # xmm15 = len(A)||len(C)\n\n        vpxor   %xmm15, %xmm14, %xmm14\n        \\GHASH_MUL       %xmm14, %xmm13, %xmm0, %xmm10, %xmm11, %xmm5, %xmm6    # final GHASH computation\n        vpshufb SHUF_MASK(%rip), %xmm14, %xmm14      # perform a 16Byte swap\n\n        vmovdqu OrigIV(arg2), %xmm9\n\n        ENCRYPT_SINGLE_BLOCK    \\REP, %xmm9                # E(K, Y0)\n\n        vpxor   %xmm14, %xmm9, %xmm9\n\n\n\n.L_return_T\\@:\n        mov     \\AUTH_TAG, %r10              # r10 = authTag\n        mov     \\AUTH_TAG_LEN, %r11              # r11 = auth_tag_len\n\n        cmp     $16, %r11\n        je      .L_T_16\\@\n\n        cmp     $8, %r11\n        jl      .L_T_4\\@\n\n.L_T_8\\@:\n        vmovq   %xmm9, %rax\n        mov     %rax, (%r10)\n        add     $8, %r10\n        sub     $8, %r11\n        vpsrldq $8, %xmm9, %xmm9\n        test    %r11, %r11\n        je     .L_return_T_done\\@\n.L_T_4\\@:\n        vmovd   %xmm9, %eax\n        mov     %eax, (%r10)\n        add     $4, %r10\n        sub     $4, %r11\n        vpsrldq     $4, %xmm9, %xmm9\n        test    %r11, %r11\n        je     .L_return_T_done\\@\n.L_T_123\\@:\n        vmovd     %xmm9, %eax\n        cmp     $2, %r11\n        jl     .L_T_1\\@\n        mov     %ax, (%r10)\n        cmp     $2, %r11\n        je     .L_return_T_done\\@\n        add     $2, %r10\n        sar     $16, %eax\n.L_T_1\\@:\n        mov     %al, (%r10)\n        jmp     .L_return_T_done\\@\n\n.L_T_16\\@:\n        vmovdqu %xmm9, (%r10)\n\n.L_return_T_done\\@:\n.endm\n\n.macro CALC_AAD_HASH GHASH_MUL AAD AADLEN T1 T2 T3 T4 T5 T6 T7 T8\n\n\tmov     \\AAD, %r10                      # r10 = AAD\n\tmov     \\AADLEN, %r12                      # r12 = aadLen\n\n\n\tmov     %r12, %r11\n\n\tvpxor   \\T8, \\T8, \\T8\n\tvpxor   \\T7, \\T7, \\T7\n\tcmp     $16, %r11\n\tjl      .L_get_AAD_rest8\\@\n.L_get_AAD_blocks\\@:\n\tvmovdqu (%r10), \\T7\n\tvpshufb SHUF_MASK(%rip), \\T7, \\T7\n\tvpxor   \\T7, \\T8, \\T8\n\t\\GHASH_MUL       \\T8, \\T2, \\T1, \\T3, \\T4, \\T5, \\T6\n\tadd     $16, %r10\n\tsub     $16, %r12\n\tsub     $16, %r11\n\tcmp     $16, %r11\n\tjge     .L_get_AAD_blocks\\@\n\tvmovdqu \\T8, \\T7\n\ttest    %r11, %r11\n\tje      .L_get_AAD_done\\@\n\n\tvpxor   \\T7, \\T7, \\T7\n\n\t \n.L_get_AAD_rest8\\@:\n\tcmp     $4, %r11\n\tjle     .L_get_AAD_rest4\\@\n\tmovq    (%r10), \\T1\n\tadd     $8, %r10\n\tsub     $8, %r11\n\tvpslldq $8, \\T1, \\T1\n\tvpsrldq $8, \\T7, \\T7\n\tvpxor   \\T1, \\T7, \\T7\n\tjmp     .L_get_AAD_rest8\\@\n.L_get_AAD_rest4\\@:\n\ttest    %r11, %r11\n\tjle     .L_get_AAD_rest0\\@\n\tmov     (%r10), %eax\n\tmovq    %rax, \\T1\n\tadd     $4, %r10\n\tsub     $4, %r11\n\tvpslldq $12, \\T1, \\T1\n\tvpsrldq $4, \\T7, \\T7\n\tvpxor   \\T1, \\T7, \\T7\n.L_get_AAD_rest0\\@:\n\t \n\tleaq\tALL_F(%rip), %r11\n\tsubq\t%r12, %r11\n\tvmovdqu\t16(%r11), \\T1\n\tandq\t$~3, %r11\n\tvpshufb (%r11), \\T7, \\T7\n\tvpand\t\\T1, \\T7, \\T7\n.L_get_AAD_rest_final\\@:\n\tvpshufb SHUF_MASK(%rip), \\T7, \\T7\n\tvpxor   \\T8, \\T7, \\T7\n\t\\GHASH_MUL       \\T7, \\T2, \\T1, \\T3, \\T4, \\T5, \\T6\n\n.L_get_AAD_done\\@:\n        vmovdqu \\T7, AadHash(arg2)\n.endm\n\n.macro INIT GHASH_MUL PRECOMPUTE\n        mov arg6, %r11\n        mov %r11, AadLen(arg2) # ctx_data.aad_length = aad_length\n        xor %r11d, %r11d\n        mov %r11, InLen(arg2) # ctx_data.in_length = 0\n\n        mov %r11, PBlockLen(arg2) # ctx_data.partial_block_length = 0\n        mov %r11, PBlockEncKey(arg2) # ctx_data.partial_block_enc_key = 0\n        mov arg3, %rax\n        movdqu (%rax), %xmm0\n        movdqu %xmm0, OrigIV(arg2) # ctx_data.orig_IV = iv\n\n        vpshufb SHUF_MASK(%rip), %xmm0, %xmm0\n        movdqu %xmm0, CurCount(arg2) # ctx_data.current_counter = iv\n\n        vmovdqu  (arg4), %xmm6              # xmm6 = HashKey\n\n        vpshufb  SHUF_MASK(%rip), %xmm6, %xmm6\n        ###############  PRECOMPUTATION of HashKey<<1 mod poly from the HashKey\n        vmovdqa  %xmm6, %xmm2\n        vpsllq   $1, %xmm6, %xmm6\n        vpsrlq   $63, %xmm2, %xmm2\n        vmovdqa  %xmm2, %xmm1\n        vpslldq  $8, %xmm2, %xmm2\n        vpsrldq  $8, %xmm1, %xmm1\n        vpor     %xmm2, %xmm6, %xmm6\n        #reduction\n        vpshufd  $0b00100100, %xmm1, %xmm2\n        vpcmpeqd TWOONE(%rip), %xmm2, %xmm2\n        vpand    POLY(%rip), %xmm2, %xmm2\n        vpxor    %xmm2, %xmm6, %xmm6        # xmm6 holds the HashKey<<1 mod poly\n        #######################################################################\n        vmovdqu  %xmm6, HashKey(arg2)       # store HashKey<<1 mod poly\n\n        CALC_AAD_HASH \\GHASH_MUL, arg5, arg6, %xmm2, %xmm6, %xmm3, %xmm4, %xmm5, %xmm7, %xmm1, %xmm0\n\n        \\PRECOMPUTE  %xmm6, %xmm0, %xmm1, %xmm2, %xmm3, %xmm4, %xmm5\n.endm\n\n\n# Reads DLEN bytes starting at DPTR and stores in XMMDst\n# where 0 < DLEN < 16\n# Clobbers %rax, DLEN\n.macro READ_PARTIAL_BLOCK DPTR DLEN XMMDst\n        vpxor \\XMMDst, \\XMMDst, \\XMMDst\n\n        cmp $8, \\DLEN\n        jl .L_read_lt8_\\@\n        mov (\\DPTR), %rax\n        vpinsrq $0, %rax, \\XMMDst, \\XMMDst\n        sub $8, \\DLEN\n        jz .L_done_read_partial_block_\\@\n        xor %eax, %eax\n.L_read_next_byte_\\@:\n        shl $8, %rax\n        mov 7(\\DPTR, \\DLEN, 1), %al\n        dec \\DLEN\n        jnz .L_read_next_byte_\\@\n        vpinsrq $1, %rax, \\XMMDst, \\XMMDst\n        jmp .L_done_read_partial_block_\\@\n.L_read_lt8_\\@:\n        xor %eax, %eax\n.L_read_next_byte_lt8_\\@:\n        shl $8, %rax\n        mov -1(\\DPTR, \\DLEN, 1), %al\n        dec \\DLEN\n        jnz .L_read_next_byte_lt8_\\@\n        vpinsrq $0, %rax, \\XMMDst, \\XMMDst\n.L_done_read_partial_block_\\@:\n.endm\n\n# PARTIAL_BLOCK: Handles encryption/decryption and the tag partial blocks\n# between update calls.\n# Requires the input data be at least 1 byte long due to READ_PARTIAL_BLOCK\n# Outputs encrypted bytes, and updates hash and partial info in gcm_data_context\n# Clobbers rax, r10, r12, r13, xmm0-6, xmm9-13\n.macro PARTIAL_BLOCK GHASH_MUL CYPH_PLAIN_OUT PLAIN_CYPH_IN PLAIN_CYPH_LEN DATA_OFFSET \\\n        AAD_HASH ENC_DEC\n        mov \tPBlockLen(arg2), %r13\n        test\t%r13, %r13\n        je\t.L_partial_block_done_\\@\t# Leave Macro if no partial blocks\n        # Read in input data without over reading\n        cmp\t$16, \\PLAIN_CYPH_LEN\n        jl\t.L_fewer_than_16_bytes_\\@\n        vmovdqu\t(\\PLAIN_CYPH_IN), %xmm1\t# If more than 16 bytes, just fill xmm\n        jmp\t.L_data_read_\\@\n\n.L_fewer_than_16_bytes_\\@:\n        lea\t(\\PLAIN_CYPH_IN, \\DATA_OFFSET, 1), %r10\n        mov\t\\PLAIN_CYPH_LEN, %r12\n        READ_PARTIAL_BLOCK %r10 %r12 %xmm1\n\n        mov PBlockLen(arg2), %r13\n\n.L_data_read_\\@:\t\t\t\t# Finished reading in data\n\n        vmovdqu\tPBlockEncKey(arg2), %xmm9\n        vmovdqu\tHashKey(arg2), %xmm13\n\n        lea\tSHIFT_MASK(%rip), %r12\n\n        # adjust the shuffle mask pointer to be able to shift r13 bytes\n        # r16-r13 is the number of bytes in plaintext mod 16)\n        add\t%r13, %r12\n        vmovdqu\t(%r12), %xmm2\t\t# get the appropriate shuffle mask\n        vpshufb %xmm2, %xmm9, %xmm9\t\t# shift right r13 bytes\n\n.if  \\ENC_DEC ==  DEC\n        vmovdqa\t%xmm1, %xmm3\n        pxor\t%xmm1, %xmm9\t\t# Cyphertext XOR E(K, Yn)\n\n        mov\t\\PLAIN_CYPH_LEN, %r10\n        add\t%r13, %r10\n        # Set r10 to be the amount of data left in CYPH_PLAIN_IN after filling\n        sub\t$16, %r10\n        # Determine if if partial block is not being filled and\n        # shift mask accordingly\n        jge\t.L_no_extra_mask_1_\\@\n        sub\t%r10, %r12\n.L_no_extra_mask_1_\\@:\n\n        vmovdqu\tALL_F-SHIFT_MASK(%r12), %xmm1\n        # get the appropriate mask to mask out bottom r13 bytes of xmm9\n        vpand\t%xmm1, %xmm9, %xmm9\t\t# mask out bottom r13 bytes of xmm9\n\n        vpand\t%xmm1, %xmm3, %xmm3\n        vmovdqa\tSHUF_MASK(%rip), %xmm10\n        vpshufb\t%xmm10, %xmm3, %xmm3\n        vpshufb\t%xmm2, %xmm3, %xmm3\n        vpxor\t%xmm3, \\AAD_HASH, \\AAD_HASH\n\n        test\t%r10, %r10\n        jl\t.L_partial_incomplete_1_\\@\n\n        # GHASH computation for the last <16 Byte block\n        \\GHASH_MUL \\AAD_HASH, %xmm13, %xmm0, %xmm10, %xmm11, %xmm5, %xmm6\n        xor\t%eax,%eax\n\n        mov\t%rax, PBlockLen(arg2)\n        jmp\t.L_dec_done_\\@\n.L_partial_incomplete_1_\\@:\n        add\t\\PLAIN_CYPH_LEN, PBlockLen(arg2)\n.L_dec_done_\\@:\n        vmovdqu\t\\AAD_HASH, AadHash(arg2)\n.else\n        vpxor\t%xmm1, %xmm9, %xmm9\t\t\t# Plaintext XOR E(K, Yn)\n\n        mov\t\\PLAIN_CYPH_LEN, %r10\n        add\t%r13, %r10\n        # Set r10 to be the amount of data left in CYPH_PLAIN_IN after filling\n        sub\t$16, %r10\n        # Determine if if partial block is not being filled and\n        # shift mask accordingly\n        jge\t.L_no_extra_mask_2_\\@\n        sub\t%r10, %r12\n.L_no_extra_mask_2_\\@:\n\n        vmovdqu\tALL_F-SHIFT_MASK(%r12), %xmm1\n        # get the appropriate mask to mask out bottom r13 bytes of xmm9\n        vpand\t%xmm1, %xmm9, %xmm9\n\n        vmovdqa\tSHUF_MASK(%rip), %xmm1\n        vpshufb %xmm1, %xmm9, %xmm9\n        vpshufb %xmm2, %xmm9, %xmm9\n        vpxor\t%xmm9, \\AAD_HASH, \\AAD_HASH\n\n        test\t%r10, %r10\n        jl\t.L_partial_incomplete_2_\\@\n\n        # GHASH computation for the last <16 Byte block\n        \\GHASH_MUL \\AAD_HASH, %xmm13, %xmm0, %xmm10, %xmm11, %xmm5, %xmm6\n        xor\t%eax,%eax\n\n        mov\t%rax, PBlockLen(arg2)\n        jmp\t.L_encode_done_\\@\n.L_partial_incomplete_2_\\@:\n        add\t\\PLAIN_CYPH_LEN, PBlockLen(arg2)\n.L_encode_done_\\@:\n        vmovdqu\t\\AAD_HASH, AadHash(arg2)\n\n        vmovdqa\tSHUF_MASK(%rip), %xmm10\n        # shuffle xmm9 back to output as ciphertext\n        vpshufb\t%xmm10, %xmm9, %xmm9\n        vpshufb\t%xmm2, %xmm9, %xmm9\n.endif\n        # output encrypted Bytes\n        test\t%r10, %r10\n        jl\t.L_partial_fill_\\@\n        mov\t%r13, %r12\n        mov\t$16, %r13\n        # Set r13 to be the number of bytes to write out\n        sub\t%r12, %r13\n        jmp\t.L_count_set_\\@\n.L_partial_fill_\\@:\n        mov\t\\PLAIN_CYPH_LEN, %r13\n.L_count_set_\\@:\n        vmovdqa\t%xmm9, %xmm0\n        vmovq\t%xmm0, %rax\n        cmp\t$8, %r13\n        jle\t.L_less_than_8_bytes_left_\\@\n\n        mov\t%rax, (\\CYPH_PLAIN_OUT, \\DATA_OFFSET, 1)\n        add\t$8, \\DATA_OFFSET\n        psrldq\t$8, %xmm0\n        vmovq\t%xmm0, %rax\n        sub\t$8, %r13\n.L_less_than_8_bytes_left_\\@:\n        movb\t%al, (\\CYPH_PLAIN_OUT, \\DATA_OFFSET, 1)\n        add\t$1, \\DATA_OFFSET\n        shr\t$8, %rax\n        sub\t$1, %r13\n        jne\t.L_less_than_8_bytes_left_\\@\n.L_partial_block_done_\\@:\n.endm # PARTIAL_BLOCK\n\n###############################################################################\n# GHASH_MUL MACRO to implement: Data*HashKey mod (128,127,126,121,0)\n# Input: A and B (128-bits each, bit-reflected)\n# Output: C = A*B*x mod poly, (i.e. >>1 )\n# To compute GH = GH*HashKey mod poly, give HK = HashKey<<1 mod poly as input\n# GH = GH * HK * x mod poly which is equivalent to GH*HashKey mod poly.\n###############################################################################\n.macro  GHASH_MUL_AVX GH HK T1 T2 T3 T4 T5\n\n        vpshufd         $0b01001110, \\GH, \\T2\n        vpshufd         $0b01001110, \\HK, \\T3\n        vpxor           \\GH     , \\T2, \\T2      # T2 = (a1+a0)\n        vpxor           \\HK     , \\T3, \\T3      # T3 = (b1+b0)\n\n        vpclmulqdq      $0x11, \\HK, \\GH, \\T1    # T1 = a1*b1\n        vpclmulqdq      $0x00, \\HK, \\GH, \\GH    # GH = a0*b0\n        vpclmulqdq      $0x00, \\T3, \\T2, \\T2    # T2 = (a1+a0)*(b1+b0)\n        vpxor           \\GH, \\T2,\\T2\n        vpxor           \\T1, \\T2,\\T2            # T2 = a0*b1+a1*b0\n\n        vpslldq         $8, \\T2,\\T3             # shift-L T3 2 DWs\n        vpsrldq         $8, \\T2,\\T2             # shift-R T2 2 DWs\n        vpxor           \\T3, \\GH, \\GH\n        vpxor           \\T2, \\T1, \\T1           # <T1:GH> = GH x HK\n\n        #first phase of the reduction\n        vpslld  $31, \\GH, \\T2                   # packed right shifting << 31\n        vpslld  $30, \\GH, \\T3                   # packed right shifting shift << 30\n        vpslld  $25, \\GH, \\T4                   # packed right shifting shift << 25\n\n        vpxor   \\T3, \\T2, \\T2                   # xor the shifted versions\n        vpxor   \\T4, \\T2, \\T2\n\n        vpsrldq $4, \\T2, \\T5                    # shift-R T5 1 DW\n\n        vpslldq $12, \\T2, \\T2                   # shift-L T2 3 DWs\n        vpxor   \\T2, \\GH, \\GH                   # first phase of the reduction complete\n\n        #second phase of the reduction\n\n        vpsrld  $1,\\GH, \\T2                     # packed left shifting >> 1\n        vpsrld  $2,\\GH, \\T3                     # packed left shifting >> 2\n        vpsrld  $7,\\GH, \\T4                     # packed left shifting >> 7\n        vpxor   \\T3, \\T2, \\T2                   # xor the shifted versions\n        vpxor   \\T4, \\T2, \\T2\n\n        vpxor   \\T5, \\T2, \\T2\n        vpxor   \\T2, \\GH, \\GH\n        vpxor   \\T1, \\GH, \\GH                   # the result is in GH\n\n\n.endm\n\n.macro PRECOMPUTE_AVX HK T1 T2 T3 T4 T5 T6\n\n        # Haskey_i_k holds XORed values of the low and high parts of the Haskey_i\n        vmovdqa  \\HK, \\T5\n\n        vpshufd  $0b01001110, \\T5, \\T1\n        vpxor    \\T5, \\T1, \\T1\n        vmovdqu  \\T1, HashKey_k(arg2)\n\n        GHASH_MUL_AVX \\T5, \\HK, \\T1, \\T3, \\T4, \\T6, \\T2  #  T5 = HashKey^2<<1 mod poly\n        vmovdqu  \\T5, HashKey_2(arg2)                    #  [HashKey_2] = HashKey^2<<1 mod poly\n        vpshufd  $0b01001110, \\T5, \\T1\n        vpxor    \\T5, \\T1, \\T1\n        vmovdqu  \\T1, HashKey_2_k(arg2)\n\n        GHASH_MUL_AVX \\T5, \\HK, \\T1, \\T3, \\T4, \\T6, \\T2  #  T5 = HashKey^3<<1 mod poly\n        vmovdqu  \\T5, HashKey_3(arg2)\n        vpshufd  $0b01001110, \\T5, \\T1\n        vpxor    \\T5, \\T1, \\T1\n        vmovdqu  \\T1, HashKey_3_k(arg2)\n\n        GHASH_MUL_AVX \\T5, \\HK, \\T1, \\T3, \\T4, \\T6, \\T2  #  T5 = HashKey^4<<1 mod poly\n        vmovdqu  \\T5, HashKey_4(arg2)\n        vpshufd  $0b01001110, \\T5, \\T1\n        vpxor    \\T5, \\T1, \\T1\n        vmovdqu  \\T1, HashKey_4_k(arg2)\n\n        GHASH_MUL_AVX \\T5, \\HK, \\T1, \\T3, \\T4, \\T6, \\T2  #  T5 = HashKey^5<<1 mod poly\n        vmovdqu  \\T5, HashKey_5(arg2)\n        vpshufd  $0b01001110, \\T5, \\T1\n        vpxor    \\T5, \\T1, \\T1\n        vmovdqu  \\T1, HashKey_5_k(arg2)\n\n        GHASH_MUL_AVX \\T5, \\HK, \\T1, \\T3, \\T4, \\T6, \\T2  #  T5 = HashKey^6<<1 mod poly\n        vmovdqu  \\T5, HashKey_6(arg2)\n        vpshufd  $0b01001110, \\T5, \\T1\n        vpxor    \\T5, \\T1, \\T1\n        vmovdqu  \\T1, HashKey_6_k(arg2)\n\n        GHASH_MUL_AVX \\T5, \\HK, \\T1, \\T3, \\T4, \\T6, \\T2  #  T5 = HashKey^7<<1 mod poly\n        vmovdqu  \\T5, HashKey_7(arg2)\n        vpshufd  $0b01001110, \\T5, \\T1\n        vpxor    \\T5, \\T1, \\T1\n        vmovdqu  \\T1, HashKey_7_k(arg2)\n\n        GHASH_MUL_AVX \\T5, \\HK, \\T1, \\T3, \\T4, \\T6, \\T2  #  T5 = HashKey^8<<1 mod poly\n        vmovdqu  \\T5, HashKey_8(arg2)\n        vpshufd  $0b01001110, \\T5, \\T1\n        vpxor    \\T5, \\T1, \\T1\n        vmovdqu  \\T1, HashKey_8_k(arg2)\n\n.endm\n\n## if a = number of total plaintext bytes\n## b = floor(a/16)\n## num_initial_blocks = b mod 4#\n## encrypt the initial num_initial_blocks blocks and apply ghash on the ciphertext\n## r10, r11, r12, rax are clobbered\n## arg1, arg2, arg3, arg4 are used as pointers only, not modified\n\n.macro INITIAL_BLOCKS_AVX REP num_initial_blocks T1 T2 T3 T4 T5 CTR XMM1 XMM2 XMM3 XMM4 XMM5 XMM6 XMM7 XMM8 T6 T_key ENC_DEC\n\ti = (8-\\num_initial_blocks)\n\tsetreg\n        vmovdqu AadHash(arg2), reg_i\n\n\t# start AES for num_initial_blocks blocks\n\tvmovdqu CurCount(arg2), \\CTR\n\n\ti = (9-\\num_initial_blocks)\n\tsetreg\n.rep \\num_initial_blocks\n                vpaddd  ONE(%rip), \\CTR, \\CTR\t\t# INCR Y0\n                vmovdqa \\CTR, reg_i\n                vpshufb SHUF_MASK(%rip), reg_i, reg_i   # perform a 16Byte swap\n\ti = (i+1)\n\tsetreg\n.endr\n\n\tvmovdqa  (arg1), \\T_key\n\ti = (9-\\num_initial_blocks)\n\tsetreg\n.rep \\num_initial_blocks\n                vpxor   \\T_key, reg_i, reg_i\n\ti = (i+1)\n\tsetreg\n.endr\n\n       j = 1\n       setreg\n.rep \\REP\n       vmovdqa  16*j(arg1), \\T_key\n\ti = (9-\\num_initial_blocks)\n\tsetreg\n.rep \\num_initial_blocks\n        vaesenc \\T_key, reg_i, reg_i\n\ti = (i+1)\n\tsetreg\n.endr\n\n       j = (j+1)\n       setreg\n.endr\n\n\tvmovdqa  16*j(arg1), \\T_key\n\ti = (9-\\num_initial_blocks)\n\tsetreg\n.rep \\num_initial_blocks\n        vaesenclast      \\T_key, reg_i, reg_i\n\ti = (i+1)\n\tsetreg\n.endr\n\n\ti = (9-\\num_initial_blocks)\n\tsetreg\n.rep \\num_initial_blocks\n                vmovdqu (arg4, %r11), \\T1\n                vpxor   \\T1, reg_i, reg_i\n                vmovdqu reg_i, (arg3 , %r11)           # write back ciphertext for num_initial_blocks blocks\n                add     $16, %r11\n.if  \\ENC_DEC == DEC\n                vmovdqa \\T1, reg_i\n.endif\n                vpshufb SHUF_MASK(%rip), reg_i, reg_i  # prepare ciphertext for GHASH computations\n\ti = (i+1)\n\tsetreg\n.endr\n\n\n\ti = (8-\\num_initial_blocks)\n\tj = (9-\\num_initial_blocks)\n\tsetreg\n\n.rep \\num_initial_blocks\n        vpxor    reg_i, reg_j, reg_j\n        GHASH_MUL_AVX       reg_j, \\T2, \\T1, \\T3, \\T4, \\T5, \\T6 # apply GHASH on num_initial_blocks blocks\n\ti = (i+1)\n\tj = (j+1)\n\tsetreg\n.endr\n        # XMM8 has the combined result here\n\n        vmovdqa  \\XMM8, TMP1(%rsp)\n        vmovdqa  \\XMM8, \\T3\n\n        cmp     $128, %r13\n        jl      .L_initial_blocks_done\\@                  # no need for precomputed constants\n\n###############################################################################\n# Haskey_i_k holds XORed values of the low and high parts of the Haskey_i\n                vpaddd   ONE(%rip), \\CTR, \\CTR          # INCR Y0\n                vmovdqa  \\CTR, \\XMM1\n                vpshufb  SHUF_MASK(%rip), \\XMM1, \\XMM1  # perform a 16Byte swap\n\n                vpaddd   ONE(%rip), \\CTR, \\CTR          # INCR Y0\n                vmovdqa  \\CTR, \\XMM2\n                vpshufb  SHUF_MASK(%rip), \\XMM2, \\XMM2  # perform a 16Byte swap\n\n                vpaddd   ONE(%rip), \\CTR, \\CTR          # INCR Y0\n                vmovdqa  \\CTR, \\XMM3\n                vpshufb  SHUF_MASK(%rip), \\XMM3, \\XMM3  # perform a 16Byte swap\n\n                vpaddd   ONE(%rip), \\CTR, \\CTR          # INCR Y0\n                vmovdqa  \\CTR, \\XMM4\n                vpshufb  SHUF_MASK(%rip), \\XMM4, \\XMM4  # perform a 16Byte swap\n\n                vpaddd   ONE(%rip), \\CTR, \\CTR          # INCR Y0\n                vmovdqa  \\CTR, \\XMM5\n                vpshufb  SHUF_MASK(%rip), \\XMM5, \\XMM5  # perform a 16Byte swap\n\n                vpaddd   ONE(%rip), \\CTR, \\CTR          # INCR Y0\n                vmovdqa  \\CTR, \\XMM6\n                vpshufb  SHUF_MASK(%rip), \\XMM6, \\XMM6  # perform a 16Byte swap\n\n                vpaddd   ONE(%rip), \\CTR, \\CTR          # INCR Y0\n                vmovdqa  \\CTR, \\XMM7\n                vpshufb  SHUF_MASK(%rip), \\XMM7, \\XMM7  # perform a 16Byte swap\n\n                vpaddd   ONE(%rip), \\CTR, \\CTR          # INCR Y0\n                vmovdqa  \\CTR, \\XMM8\n                vpshufb  SHUF_MASK(%rip), \\XMM8, \\XMM8  # perform a 16Byte swap\n\n                vmovdqa  (arg1), \\T_key\n                vpxor    \\T_key, \\XMM1, \\XMM1\n                vpxor    \\T_key, \\XMM2, \\XMM2\n                vpxor    \\T_key, \\XMM3, \\XMM3\n                vpxor    \\T_key, \\XMM4, \\XMM4\n                vpxor    \\T_key, \\XMM5, \\XMM5\n                vpxor    \\T_key, \\XMM6, \\XMM6\n                vpxor    \\T_key, \\XMM7, \\XMM7\n                vpxor    \\T_key, \\XMM8, \\XMM8\n\n               i = 1\n               setreg\n.rep    \\REP       # do REP rounds\n                vmovdqa  16*i(arg1), \\T_key\n                vaesenc  \\T_key, \\XMM1, \\XMM1\n                vaesenc  \\T_key, \\XMM2, \\XMM2\n                vaesenc  \\T_key, \\XMM3, \\XMM3\n                vaesenc  \\T_key, \\XMM4, \\XMM4\n                vaesenc  \\T_key, \\XMM5, \\XMM5\n                vaesenc  \\T_key, \\XMM6, \\XMM6\n                vaesenc  \\T_key, \\XMM7, \\XMM7\n                vaesenc  \\T_key, \\XMM8, \\XMM8\n               i = (i+1)\n               setreg\n.endr\n\n                vmovdqa  16*i(arg1), \\T_key\n                vaesenclast  \\T_key, \\XMM1, \\XMM1\n                vaesenclast  \\T_key, \\XMM2, \\XMM2\n                vaesenclast  \\T_key, \\XMM3, \\XMM3\n                vaesenclast  \\T_key, \\XMM4, \\XMM4\n                vaesenclast  \\T_key, \\XMM5, \\XMM5\n                vaesenclast  \\T_key, \\XMM6, \\XMM6\n                vaesenclast  \\T_key, \\XMM7, \\XMM7\n                vaesenclast  \\T_key, \\XMM8, \\XMM8\n\n                vmovdqu  (arg4, %r11), \\T1\n                vpxor    \\T1, \\XMM1, \\XMM1\n                vmovdqu  \\XMM1, (arg3 , %r11)\n                .if   \\ENC_DEC == DEC\n                vmovdqa  \\T1, \\XMM1\n                .endif\n\n                vmovdqu  16*1(arg4, %r11), \\T1\n                vpxor    \\T1, \\XMM2, \\XMM2\n                vmovdqu  \\XMM2, 16*1(arg3 , %r11)\n                .if   \\ENC_DEC == DEC\n                vmovdqa  \\T1, \\XMM2\n                .endif\n\n                vmovdqu  16*2(arg4, %r11), \\T1\n                vpxor    \\T1, \\XMM3, \\XMM3\n                vmovdqu  \\XMM3, 16*2(arg3 , %r11)\n                .if   \\ENC_DEC == DEC\n                vmovdqa  \\T1, \\XMM3\n                .endif\n\n                vmovdqu  16*3(arg4, %r11), \\T1\n                vpxor    \\T1, \\XMM4, \\XMM4\n                vmovdqu  \\XMM4, 16*3(arg3 , %r11)\n                .if   \\ENC_DEC == DEC\n                vmovdqa  \\T1, \\XMM4\n                .endif\n\n                vmovdqu  16*4(arg4, %r11), \\T1\n                vpxor    \\T1, \\XMM5, \\XMM5\n                vmovdqu  \\XMM5, 16*4(arg3 , %r11)\n                .if   \\ENC_DEC == DEC\n                vmovdqa  \\T1, \\XMM5\n                .endif\n\n                vmovdqu  16*5(arg4, %r11), \\T1\n                vpxor    \\T1, \\XMM6, \\XMM6\n                vmovdqu  \\XMM6, 16*5(arg3 , %r11)\n                .if   \\ENC_DEC == DEC\n                vmovdqa  \\T1, \\XMM6\n                .endif\n\n                vmovdqu  16*6(arg4, %r11), \\T1\n                vpxor    \\T1, \\XMM7, \\XMM7\n                vmovdqu  \\XMM7, 16*6(arg3 , %r11)\n                .if   \\ENC_DEC == DEC\n                vmovdqa  \\T1, \\XMM7\n                .endif\n\n                vmovdqu  16*7(arg4, %r11), \\T1\n                vpxor    \\T1, \\XMM8, \\XMM8\n                vmovdqu  \\XMM8, 16*7(arg3 , %r11)\n                .if   \\ENC_DEC == DEC\n                vmovdqa  \\T1, \\XMM8\n                .endif\n\n                add     $128, %r11\n\n                vpshufb  SHUF_MASK(%rip), \\XMM1, \\XMM1     # perform a 16Byte swap\n                vpxor    TMP1(%rsp), \\XMM1, \\XMM1          # combine GHASHed value with the corresponding ciphertext\n                vpshufb  SHUF_MASK(%rip), \\XMM2, \\XMM2     # perform a 16Byte swap\n                vpshufb  SHUF_MASK(%rip), \\XMM3, \\XMM3     # perform a 16Byte swap\n                vpshufb  SHUF_MASK(%rip), \\XMM4, \\XMM4     # perform a 16Byte swap\n                vpshufb  SHUF_MASK(%rip), \\XMM5, \\XMM5     # perform a 16Byte swap\n                vpshufb  SHUF_MASK(%rip), \\XMM6, \\XMM6     # perform a 16Byte swap\n                vpshufb  SHUF_MASK(%rip), \\XMM7, \\XMM7     # perform a 16Byte swap\n                vpshufb  SHUF_MASK(%rip), \\XMM8, \\XMM8     # perform a 16Byte swap\n\n###############################################################################\n\n.L_initial_blocks_done\\@:\n\n.endm\n\n# encrypt 8 blocks at a time\n# ghash the 8 previously encrypted ciphertext blocks\n# arg1, arg2, arg3, arg4 are used as pointers only, not modified\n# r11 is the data offset value\n.macro GHASH_8_ENCRYPT_8_PARALLEL_AVX REP T1 T2 T3 T4 T5 T6 CTR XMM1 XMM2 XMM3 XMM4 XMM5 XMM6 XMM7 XMM8 T7 loop_idx ENC_DEC\n\n        vmovdqa \\XMM1, \\T2\n        vmovdqa \\XMM2, TMP2(%rsp)\n        vmovdqa \\XMM3, TMP3(%rsp)\n        vmovdqa \\XMM4, TMP4(%rsp)\n        vmovdqa \\XMM5, TMP5(%rsp)\n        vmovdqa \\XMM6, TMP6(%rsp)\n        vmovdqa \\XMM7, TMP7(%rsp)\n        vmovdqa \\XMM8, TMP8(%rsp)\n\n.if \\loop_idx == in_order\n                vpaddd  ONE(%rip), \\CTR, \\XMM1           # INCR CNT\n                vpaddd  ONE(%rip), \\XMM1, \\XMM2\n                vpaddd  ONE(%rip), \\XMM2, \\XMM3\n                vpaddd  ONE(%rip), \\XMM3, \\XMM4\n                vpaddd  ONE(%rip), \\XMM4, \\XMM5\n                vpaddd  ONE(%rip), \\XMM5, \\XMM6\n                vpaddd  ONE(%rip), \\XMM6, \\XMM7\n                vpaddd  ONE(%rip), \\XMM7, \\XMM8\n                vmovdqa \\XMM8, \\CTR\n\n                vpshufb SHUF_MASK(%rip), \\XMM1, \\XMM1    # perform a 16Byte swap\n                vpshufb SHUF_MASK(%rip), \\XMM2, \\XMM2    # perform a 16Byte swap\n                vpshufb SHUF_MASK(%rip), \\XMM3, \\XMM3    # perform a 16Byte swap\n                vpshufb SHUF_MASK(%rip), \\XMM4, \\XMM4    # perform a 16Byte swap\n                vpshufb SHUF_MASK(%rip), \\XMM5, \\XMM5    # perform a 16Byte swap\n                vpshufb SHUF_MASK(%rip), \\XMM6, \\XMM6    # perform a 16Byte swap\n                vpshufb SHUF_MASK(%rip), \\XMM7, \\XMM7    # perform a 16Byte swap\n                vpshufb SHUF_MASK(%rip), \\XMM8, \\XMM8    # perform a 16Byte swap\n.else\n                vpaddd  ONEf(%rip), \\CTR, \\XMM1           # INCR CNT\n                vpaddd  ONEf(%rip), \\XMM1, \\XMM2\n                vpaddd  ONEf(%rip), \\XMM2, \\XMM3\n                vpaddd  ONEf(%rip), \\XMM3, \\XMM4\n                vpaddd  ONEf(%rip), \\XMM4, \\XMM5\n                vpaddd  ONEf(%rip), \\XMM5, \\XMM6\n                vpaddd  ONEf(%rip), \\XMM6, \\XMM7\n                vpaddd  ONEf(%rip), \\XMM7, \\XMM8\n                vmovdqa \\XMM8, \\CTR\n.endif\n\n\n        #######################################################################\n\n                vmovdqu (arg1), \\T1\n                vpxor   \\T1, \\XMM1, \\XMM1\n                vpxor   \\T1, \\XMM2, \\XMM2\n                vpxor   \\T1, \\XMM3, \\XMM3\n                vpxor   \\T1, \\XMM4, \\XMM4\n                vpxor   \\T1, \\XMM5, \\XMM5\n                vpxor   \\T1, \\XMM6, \\XMM6\n                vpxor   \\T1, \\XMM7, \\XMM7\n                vpxor   \\T1, \\XMM8, \\XMM8\n\n        #######################################################################\n\n\n\n\n\n                vmovdqu 16*1(arg1), \\T1\n                vaesenc \\T1, \\XMM1, \\XMM1\n                vaesenc \\T1, \\XMM2, \\XMM2\n                vaesenc \\T1, \\XMM3, \\XMM3\n                vaesenc \\T1, \\XMM4, \\XMM4\n                vaesenc \\T1, \\XMM5, \\XMM5\n                vaesenc \\T1, \\XMM6, \\XMM6\n                vaesenc \\T1, \\XMM7, \\XMM7\n                vaesenc \\T1, \\XMM8, \\XMM8\n\n                vmovdqu 16*2(arg1), \\T1\n                vaesenc \\T1, \\XMM1, \\XMM1\n                vaesenc \\T1, \\XMM2, \\XMM2\n                vaesenc \\T1, \\XMM3, \\XMM3\n                vaesenc \\T1, \\XMM4, \\XMM4\n                vaesenc \\T1, \\XMM5, \\XMM5\n                vaesenc \\T1, \\XMM6, \\XMM6\n                vaesenc \\T1, \\XMM7, \\XMM7\n                vaesenc \\T1, \\XMM8, \\XMM8\n\n\n        #######################################################################\n\n        vmovdqu         HashKey_8(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\T2, \\T4             # T4 = a1*b1\n        vpclmulqdq      $0x00, \\T5, \\T2, \\T7             # T7 = a0*b0\n\n        vpshufd         $0b01001110, \\T2, \\T6\n        vpxor           \\T2, \\T6, \\T6\n\n        vmovdqu         HashKey_8_k(arg2), \\T5\n        vpclmulqdq      $0x00, \\T5, \\T6, \\T6\n\n                vmovdqu 16*3(arg1), \\T1\n                vaesenc \\T1, \\XMM1, \\XMM1\n                vaesenc \\T1, \\XMM2, \\XMM2\n                vaesenc \\T1, \\XMM3, \\XMM3\n                vaesenc \\T1, \\XMM4, \\XMM4\n                vaesenc \\T1, \\XMM5, \\XMM5\n                vaesenc \\T1, \\XMM6, \\XMM6\n                vaesenc \\T1, \\XMM7, \\XMM7\n                vaesenc \\T1, \\XMM8, \\XMM8\n\n        vmovdqa         TMP2(%rsp), \\T1\n        vmovdqu         HashKey_7(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T4, \\T4\n        vpclmulqdq      $0x00, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T7, \\T7\n\n        vpshufd         $0b01001110, \\T1, \\T3\n        vpxor           \\T1, \\T3, \\T3\n        vmovdqu         HashKey_7_k(arg2), \\T5\n        vpclmulqdq      $0x10, \\T5, \\T3, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n                vmovdqu 16*4(arg1), \\T1\n                vaesenc \\T1, \\XMM1, \\XMM1\n                vaesenc \\T1, \\XMM2, \\XMM2\n                vaesenc \\T1, \\XMM3, \\XMM3\n                vaesenc \\T1, \\XMM4, \\XMM4\n                vaesenc \\T1, \\XMM5, \\XMM5\n                vaesenc \\T1, \\XMM6, \\XMM6\n                vaesenc \\T1, \\XMM7, \\XMM7\n                vaesenc \\T1, \\XMM8, \\XMM8\n\n        #######################################################################\n\n        vmovdqa         TMP3(%rsp), \\T1\n        vmovdqu         HashKey_6(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T4, \\T4\n        vpclmulqdq      $0x00, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T7, \\T7\n\n        vpshufd         $0b01001110, \\T1, \\T3\n        vpxor           \\T1, \\T3, \\T3\n        vmovdqu         HashKey_6_k(arg2), \\T5\n        vpclmulqdq      $0x10, \\T5, \\T3, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n                vmovdqu 16*5(arg1), \\T1\n                vaesenc \\T1, \\XMM1, \\XMM1\n                vaesenc \\T1, \\XMM2, \\XMM2\n                vaesenc \\T1, \\XMM3, \\XMM3\n                vaesenc \\T1, \\XMM4, \\XMM4\n                vaesenc \\T1, \\XMM5, \\XMM5\n                vaesenc \\T1, \\XMM6, \\XMM6\n                vaesenc \\T1, \\XMM7, \\XMM7\n                vaesenc \\T1, \\XMM8, \\XMM8\n\n        vmovdqa         TMP4(%rsp), \\T1\n        vmovdqu         HashKey_5(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T4, \\T4\n        vpclmulqdq      $0x00, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T7, \\T7\n\n        vpshufd         $0b01001110, \\T1, \\T3\n        vpxor           \\T1, \\T3, \\T3\n        vmovdqu         HashKey_5_k(arg2), \\T5\n        vpclmulqdq      $0x10, \\T5, \\T3, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n                vmovdqu 16*6(arg1), \\T1\n                vaesenc \\T1, \\XMM1, \\XMM1\n                vaesenc \\T1, \\XMM2, \\XMM2\n                vaesenc \\T1, \\XMM3, \\XMM3\n                vaesenc \\T1, \\XMM4, \\XMM4\n                vaesenc \\T1, \\XMM5, \\XMM5\n                vaesenc \\T1, \\XMM6, \\XMM6\n                vaesenc \\T1, \\XMM7, \\XMM7\n                vaesenc \\T1, \\XMM8, \\XMM8\n\n\n        vmovdqa         TMP5(%rsp), \\T1\n        vmovdqu         HashKey_4(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T4, \\T4\n        vpclmulqdq      $0x00, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T7, \\T7\n\n        vpshufd         $0b01001110, \\T1, \\T3\n        vpxor           \\T1, \\T3, \\T3\n        vmovdqu         HashKey_4_k(arg2), \\T5\n        vpclmulqdq      $0x10, \\T5, \\T3, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n                vmovdqu 16*7(arg1), \\T1\n                vaesenc \\T1, \\XMM1, \\XMM1\n                vaesenc \\T1, \\XMM2, \\XMM2\n                vaesenc \\T1, \\XMM3, \\XMM3\n                vaesenc \\T1, \\XMM4, \\XMM4\n                vaesenc \\T1, \\XMM5, \\XMM5\n                vaesenc \\T1, \\XMM6, \\XMM6\n                vaesenc \\T1, \\XMM7, \\XMM7\n                vaesenc \\T1, \\XMM8, \\XMM8\n\n        vmovdqa         TMP6(%rsp), \\T1\n        vmovdqu         HashKey_3(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T4, \\T4\n        vpclmulqdq      $0x00, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T7, \\T7\n\n        vpshufd         $0b01001110, \\T1, \\T3\n        vpxor           \\T1, \\T3, \\T3\n        vmovdqu         HashKey_3_k(arg2), \\T5\n        vpclmulqdq      $0x10, \\T5, \\T3, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n\n                vmovdqu 16*8(arg1), \\T1\n                vaesenc \\T1, \\XMM1, \\XMM1\n                vaesenc \\T1, \\XMM2, \\XMM2\n                vaesenc \\T1, \\XMM3, \\XMM3\n                vaesenc \\T1, \\XMM4, \\XMM4\n                vaesenc \\T1, \\XMM5, \\XMM5\n                vaesenc \\T1, \\XMM6, \\XMM6\n                vaesenc \\T1, \\XMM7, \\XMM7\n                vaesenc \\T1, \\XMM8, \\XMM8\n\n        vmovdqa         TMP7(%rsp), \\T1\n        vmovdqu         HashKey_2(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T4, \\T4\n        vpclmulqdq      $0x00, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T7, \\T7\n\n        vpshufd         $0b01001110, \\T1, \\T3\n        vpxor           \\T1, \\T3, \\T3\n        vmovdqu         HashKey_2_k(arg2), \\T5\n        vpclmulqdq      $0x10, \\T5, \\T3, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n        #######################################################################\n\n                vmovdqu 16*9(arg1), \\T5\n                vaesenc \\T5, \\XMM1, \\XMM1\n                vaesenc \\T5, \\XMM2, \\XMM2\n                vaesenc \\T5, \\XMM3, \\XMM3\n                vaesenc \\T5, \\XMM4, \\XMM4\n                vaesenc \\T5, \\XMM5, \\XMM5\n                vaesenc \\T5, \\XMM6, \\XMM6\n                vaesenc \\T5, \\XMM7, \\XMM7\n                vaesenc \\T5, \\XMM8, \\XMM8\n\n        vmovdqa         TMP8(%rsp), \\T1\n        vmovdqu         HashKey(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T4, \\T4\n        vpclmulqdq      $0x00, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T7, \\T7\n\n        vpshufd         $0b01001110, \\T1, \\T3\n        vpxor           \\T1, \\T3, \\T3\n        vmovdqu         HashKey_k(arg2), \\T5\n        vpclmulqdq      $0x10, \\T5, \\T3, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n        vpxor           \\T4, \\T6, \\T6\n        vpxor           \\T7, \\T6, \\T6\n\n                vmovdqu 16*10(arg1), \\T5\n\n        i = 11\n        setreg\n.rep (\\REP-9)\n\n        vaesenc \\T5, \\XMM1, \\XMM1\n        vaesenc \\T5, \\XMM2, \\XMM2\n        vaesenc \\T5, \\XMM3, \\XMM3\n        vaesenc \\T5, \\XMM4, \\XMM4\n        vaesenc \\T5, \\XMM5, \\XMM5\n        vaesenc \\T5, \\XMM6, \\XMM6\n        vaesenc \\T5, \\XMM7, \\XMM7\n        vaesenc \\T5, \\XMM8, \\XMM8\n\n        vmovdqu 16*i(arg1), \\T5\n        i = i + 1\n        setreg\n.endr\n\n\ti = 0\n\tj = 1\n\tsetreg\n.rep 8\n\t\tvpxor\t16*i(arg4, %r11), \\T5, \\T2\n                .if \\ENC_DEC == ENC\n                vaesenclast     \\T2, reg_j, reg_j\n                .else\n                vaesenclast     \\T2, reg_j, \\T3\n                vmovdqu 16*i(arg4, %r11), reg_j\n                vmovdqu \\T3, 16*i(arg3, %r11)\n                .endif\n\ti = (i+1)\n\tj = (j+1)\n\tsetreg\n.endr\n\t#######################################################################\n\n\n\tvpslldq\t$8, \\T6, \\T3\t\t\t\t# shift-L T3 2 DWs\n\tvpsrldq\t$8, \\T6, \\T6\t\t\t\t# shift-R T2 2 DWs\n\tvpxor\t\\T3, \\T7, \\T7\n\tvpxor\t\\T4, \\T6, \\T6\t\t\t\t# accumulate the results in T6:T7\n\n\n\n\t#######################################################################\n\t#first phase of the reduction\n\t#######################################################################\n        vpslld  $31, \\T7, \\T2                           # packed right shifting << 31\n        vpslld  $30, \\T7, \\T3                           # packed right shifting shift << 30\n        vpslld  $25, \\T7, \\T4                           # packed right shifting shift << 25\n\n        vpxor   \\T3, \\T2, \\T2                           # xor the shifted versions\n        vpxor   \\T4, \\T2, \\T2\n\n        vpsrldq $4, \\T2, \\T1                            # shift-R T1 1 DW\n\n        vpslldq $12, \\T2, \\T2                           # shift-L T2 3 DWs\n        vpxor   \\T2, \\T7, \\T7                           # first phase of the reduction complete\n\t#######################################################################\n                .if \\ENC_DEC == ENC\n\t\tvmovdqu\t \\XMM1,\t16*0(arg3,%r11)\t\t# Write to the Ciphertext buffer\n\t\tvmovdqu\t \\XMM2,\t16*1(arg3,%r11)\t\t# Write to the Ciphertext buffer\n\t\tvmovdqu\t \\XMM3,\t16*2(arg3,%r11)\t\t# Write to the Ciphertext buffer\n\t\tvmovdqu\t \\XMM4,\t16*3(arg3,%r11)\t\t# Write to the Ciphertext buffer\n\t\tvmovdqu\t \\XMM5,\t16*4(arg3,%r11)\t\t# Write to the Ciphertext buffer\n\t\tvmovdqu\t \\XMM6,\t16*5(arg3,%r11)\t\t# Write to the Ciphertext buffer\n\t\tvmovdqu\t \\XMM7,\t16*6(arg3,%r11)\t\t# Write to the Ciphertext buffer\n\t\tvmovdqu\t \\XMM8,\t16*7(arg3,%r11)\t\t# Write to the Ciphertext buffer\n                .endif\n\n\t#######################################################################\n\t#second phase of the reduction\n        vpsrld  $1, \\T7, \\T2                            # packed left shifting >> 1\n        vpsrld  $2, \\T7, \\T3                            # packed left shifting >> 2\n        vpsrld  $7, \\T7, \\T4                            # packed left shifting >> 7\n        vpxor   \\T3, \\T2, \\T2                           # xor the shifted versions\n        vpxor   \\T4, \\T2, \\T2\n\n        vpxor   \\T1, \\T2, \\T2\n        vpxor   \\T2, \\T7, \\T7\n        vpxor   \\T7, \\T6, \\T6                           # the result is in T6\n\t#######################################################################\n\n\t\tvpshufb\tSHUF_MASK(%rip), \\XMM1, \\XMM1\t# perform a 16Byte swap\n\t\tvpshufb\tSHUF_MASK(%rip), \\XMM2, \\XMM2\t# perform a 16Byte swap\n\t\tvpshufb\tSHUF_MASK(%rip), \\XMM3, \\XMM3\t# perform a 16Byte swap\n\t\tvpshufb\tSHUF_MASK(%rip), \\XMM4, \\XMM4\t# perform a 16Byte swap\n\t\tvpshufb\tSHUF_MASK(%rip), \\XMM5, \\XMM5\t# perform a 16Byte swap\n\t\tvpshufb\tSHUF_MASK(%rip), \\XMM6, \\XMM6\t# perform a 16Byte swap\n\t\tvpshufb\tSHUF_MASK(%rip), \\XMM7, \\XMM7\t# perform a 16Byte swap\n\t\tvpshufb\tSHUF_MASK(%rip), \\XMM8, \\XMM8\t# perform a 16Byte swap\n\n\n\tvpxor\t\\T6, \\XMM1, \\XMM1\n\n\n\n.endm\n\n\n# GHASH the last 4 ciphertext blocks.\n.macro  GHASH_LAST_8_AVX T1 T2 T3 T4 T5 T6 T7 XMM1 XMM2 XMM3 XMM4 XMM5 XMM6 XMM7 XMM8\n\n        ## Karatsuba Method\n\n\n        vpshufd         $0b01001110, \\XMM1, \\T2\n        vpxor           \\XMM1, \\T2, \\T2\n        vmovdqu         HashKey_8(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\XMM1, \\T6\n        vpclmulqdq      $0x00, \\T5, \\XMM1, \\T7\n\n        vmovdqu         HashKey_8_k(arg2), \\T3\n        vpclmulqdq      $0x00, \\T3, \\T2, \\XMM1\n\n        ######################\n\n        vpshufd         $0b01001110, \\XMM2, \\T2\n        vpxor           \\XMM2, \\T2, \\T2\n        vmovdqu         HashKey_7(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\XMM2, \\T4\n        vpxor           \\T4, \\T6, \\T6\n\n        vpclmulqdq      $0x00, \\T5, \\XMM2, \\T4\n        vpxor           \\T4, \\T7, \\T7\n\n        vmovdqu         HashKey_7_k(arg2), \\T3\n        vpclmulqdq      $0x00, \\T3, \\T2, \\T2\n        vpxor           \\T2, \\XMM1, \\XMM1\n\n        ######################\n\n        vpshufd         $0b01001110, \\XMM3, \\T2\n        vpxor           \\XMM3, \\T2, \\T2\n        vmovdqu         HashKey_6(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\XMM3, \\T4\n        vpxor           \\T4, \\T6, \\T6\n\n        vpclmulqdq      $0x00, \\T5, \\XMM3, \\T4\n        vpxor           \\T4, \\T7, \\T7\n\n        vmovdqu         HashKey_6_k(arg2), \\T3\n        vpclmulqdq      $0x00, \\T3, \\T2, \\T2\n        vpxor           \\T2, \\XMM1, \\XMM1\n\n        ######################\n\n        vpshufd         $0b01001110, \\XMM4, \\T2\n        vpxor           \\XMM4, \\T2, \\T2\n        vmovdqu         HashKey_5(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\XMM4, \\T4\n        vpxor           \\T4, \\T6, \\T6\n\n        vpclmulqdq      $0x00, \\T5, \\XMM4, \\T4\n        vpxor           \\T4, \\T7, \\T7\n\n        vmovdqu         HashKey_5_k(arg2), \\T3\n        vpclmulqdq      $0x00, \\T3, \\T2, \\T2\n        vpxor           \\T2, \\XMM1, \\XMM1\n\n        ######################\n\n        vpshufd         $0b01001110, \\XMM5, \\T2\n        vpxor           \\XMM5, \\T2, \\T2\n        vmovdqu         HashKey_4(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\XMM5, \\T4\n        vpxor           \\T4, \\T6, \\T6\n\n        vpclmulqdq      $0x00, \\T5, \\XMM5, \\T4\n        vpxor           \\T4, \\T7, \\T7\n\n        vmovdqu         HashKey_4_k(arg2), \\T3\n        vpclmulqdq      $0x00, \\T3, \\T2, \\T2\n        vpxor           \\T2, \\XMM1, \\XMM1\n\n        ######################\n\n        vpshufd         $0b01001110, \\XMM6, \\T2\n        vpxor           \\XMM6, \\T2, \\T2\n        vmovdqu         HashKey_3(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\XMM6, \\T4\n        vpxor           \\T4, \\T6, \\T6\n\n        vpclmulqdq      $0x00, \\T5, \\XMM6, \\T4\n        vpxor           \\T4, \\T7, \\T7\n\n        vmovdqu         HashKey_3_k(arg2), \\T3\n        vpclmulqdq      $0x00, \\T3, \\T2, \\T2\n        vpxor           \\T2, \\XMM1, \\XMM1\n\n        ######################\n\n        vpshufd         $0b01001110, \\XMM7, \\T2\n        vpxor           \\XMM7, \\T2, \\T2\n        vmovdqu         HashKey_2(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\XMM7, \\T4\n        vpxor           \\T4, \\T6, \\T6\n\n        vpclmulqdq      $0x00, \\T5, \\XMM7, \\T4\n        vpxor           \\T4, \\T7, \\T7\n\n        vmovdqu         HashKey_2_k(arg2), \\T3\n        vpclmulqdq      $0x00, \\T3, \\T2, \\T2\n        vpxor           \\T2, \\XMM1, \\XMM1\n\n        ######################\n\n        vpshufd         $0b01001110, \\XMM8, \\T2\n        vpxor           \\XMM8, \\T2, \\T2\n        vmovdqu         HashKey(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\XMM8, \\T4\n        vpxor           \\T4, \\T6, \\T6\n\n        vpclmulqdq      $0x00, \\T5, \\XMM8, \\T4\n        vpxor           \\T4, \\T7, \\T7\n\n        vmovdqu         HashKey_k(arg2), \\T3\n        vpclmulqdq      $0x00, \\T3, \\T2, \\T2\n\n        vpxor           \\T2, \\XMM1, \\XMM1\n        vpxor           \\T6, \\XMM1, \\XMM1\n        vpxor           \\T7, \\XMM1, \\T2\n\n\n\n\n        vpslldq $8, \\T2, \\T4\n        vpsrldq $8, \\T2, \\T2\n\n        vpxor   \\T4, \\T7, \\T7\n        vpxor   \\T2, \\T6, \\T6   # <T6:T7> holds the result of\n\t\t\t\t# the accumulated carry-less multiplications\n\n        #######################################################################\n        #first phase of the reduction\n        vpslld  $31, \\T7, \\T2   # packed right shifting << 31\n        vpslld  $30, \\T7, \\T3   # packed right shifting shift << 30\n        vpslld  $25, \\T7, \\T4   # packed right shifting shift << 25\n\n        vpxor   \\T3, \\T2, \\T2   # xor the shifted versions\n        vpxor   \\T4, \\T2, \\T2\n\n        vpsrldq $4, \\T2, \\T1    # shift-R T1 1 DW\n\n        vpslldq $12, \\T2, \\T2   # shift-L T2 3 DWs\n        vpxor   \\T2, \\T7, \\T7   # first phase of the reduction complete\n        #######################################################################\n\n\n        #second phase of the reduction\n        vpsrld  $1, \\T7, \\T2    # packed left shifting >> 1\n        vpsrld  $2, \\T7, \\T3    # packed left shifting >> 2\n        vpsrld  $7, \\T7, \\T4    # packed left shifting >> 7\n        vpxor   \\T3, \\T2, \\T2   # xor the shifted versions\n        vpxor   \\T4, \\T2, \\T2\n\n        vpxor   \\T1, \\T2, \\T2\n        vpxor   \\T2, \\T7, \\T7\n        vpxor   \\T7, \\T6, \\T6   # the result is in T6\n\n.endm\n\n#############################################################\n#void   aesni_gcm_precomp_avx_gen2\n#        (gcm_data     *my_ctx_data,\n#         gcm_context_data *data,\n#        u8     *hash_subkey#  \n#        u8      *iv,  \n#        const   u8 *aad,  \n#        u64     aad_len)  \n#############################################################\nSYM_FUNC_START(aesni_gcm_init_avx_gen2)\n        FUNC_SAVE\n        INIT GHASH_MUL_AVX, PRECOMPUTE_AVX\n        FUNC_RESTORE\n        RET\nSYM_FUNC_END(aesni_gcm_init_avx_gen2)\n\n###############################################################################\n#void   aesni_gcm_enc_update_avx_gen2(\n#        gcm_data        *my_ctx_data,      \n#        gcm_context_data *data,\n#        u8      *out,  \n#        const   u8 *in,  \n#        u64     plaintext_len)  \n###############################################################################\nSYM_FUNC_START(aesni_gcm_enc_update_avx_gen2)\n        FUNC_SAVE\n        mov     keysize, %eax\n        cmp     $32, %eax\n        je      key_256_enc_update\n        cmp     $16, %eax\n        je      key_128_enc_update\n        # must be 192\n        GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, ENC, 11\n        FUNC_RESTORE\n        RET\nkey_128_enc_update:\n        GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, ENC, 9\n        FUNC_RESTORE\n        RET\nkey_256_enc_update:\n        GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, ENC, 13\n        FUNC_RESTORE\n        RET\nSYM_FUNC_END(aesni_gcm_enc_update_avx_gen2)\n\n###############################################################################\n#void   aesni_gcm_dec_update_avx_gen2(\n#        gcm_data        *my_ctx_data,      \n#        gcm_context_data *data,\n#        u8      *out,  \n#        const   u8 *in,  \n#        u64     plaintext_len)  \n###############################################################################\nSYM_FUNC_START(aesni_gcm_dec_update_avx_gen2)\n        FUNC_SAVE\n        mov     keysize,%eax\n        cmp     $32, %eax\n        je      key_256_dec_update\n        cmp     $16, %eax\n        je      key_128_dec_update\n        # must be 192\n        GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, DEC, 11\n        FUNC_RESTORE\n        RET\nkey_128_dec_update:\n        GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, DEC, 9\n        FUNC_RESTORE\n        RET\nkey_256_dec_update:\n        GCM_ENC_DEC INITIAL_BLOCKS_AVX, GHASH_8_ENCRYPT_8_PARALLEL_AVX, GHASH_LAST_8_AVX, GHASH_MUL_AVX, DEC, 13\n        FUNC_RESTORE\n        RET\nSYM_FUNC_END(aesni_gcm_dec_update_avx_gen2)\n\n###############################################################################\n#void   aesni_gcm_finalize_avx_gen2(\n#        gcm_data        *my_ctx_data,      \n#        gcm_context_data *data,\n#        u8      *auth_tag,  \n#        u64     auth_tag_len)#  \n###############################################################################\nSYM_FUNC_START(aesni_gcm_finalize_avx_gen2)\n        FUNC_SAVE\n        mov\tkeysize,%eax\n        cmp     $32, %eax\n        je      key_256_finalize\n        cmp     $16, %eax\n        je      key_128_finalize\n        # must be 192\n        GCM_COMPLETE GHASH_MUL_AVX, 11, arg3, arg4\n        FUNC_RESTORE\n        RET\nkey_128_finalize:\n        GCM_COMPLETE GHASH_MUL_AVX, 9, arg3, arg4\n        FUNC_RESTORE\n        RET\nkey_256_finalize:\n        GCM_COMPLETE GHASH_MUL_AVX, 13, arg3, arg4\n        FUNC_RESTORE\n        RET\nSYM_FUNC_END(aesni_gcm_finalize_avx_gen2)\n\n###############################################################################\n# GHASH_MUL MACRO to implement: Data*HashKey mod (128,127,126,121,0)\n# Input: A and B (128-bits each, bit-reflected)\n# Output: C = A*B*x mod poly, (i.e. >>1 )\n# To compute GH = GH*HashKey mod poly, give HK = HashKey<<1 mod poly as input\n# GH = GH * HK * x mod poly which is equivalent to GH*HashKey mod poly.\n###############################################################################\n.macro  GHASH_MUL_AVX2 GH HK T1 T2 T3 T4 T5\n\n        vpclmulqdq      $0x11,\\HK,\\GH,\\T1      # T1 = a1*b1\n        vpclmulqdq      $0x00,\\HK,\\GH,\\T2      # T2 = a0*b0\n        vpclmulqdq      $0x01,\\HK,\\GH,\\T3      # T3 = a1*b0\n        vpclmulqdq      $0x10,\\HK,\\GH,\\GH      # GH = a0*b1\n        vpxor           \\T3, \\GH, \\GH\n\n\n        vpsrldq         $8 , \\GH, \\T3          # shift-R GH 2 DWs\n        vpslldq         $8 , \\GH, \\GH          # shift-L GH 2 DWs\n\n        vpxor           \\T3, \\T1, \\T1\n        vpxor           \\T2, \\GH, \\GH\n\n        #######################################################################\n        #first phase of the reduction\n        vmovdqa         POLY2(%rip), \\T3\n\n        vpclmulqdq      $0x01, \\GH, \\T3, \\T2\n        vpslldq         $8, \\T2, \\T2           # shift-L T2 2 DWs\n\n        vpxor           \\T2, \\GH, \\GH          # first phase of the reduction complete\n        #######################################################################\n        #second phase of the reduction\n        vpclmulqdq      $0x00, \\GH, \\T3, \\T2\n        vpsrldq         $4, \\T2, \\T2           # shift-R T2 1 DW (Shift-R only 1-DW to obtain 2-DWs shift-R)\n\n        vpclmulqdq      $0x10, \\GH, \\T3, \\GH\n        vpslldq         $4, \\GH, \\GH           # shift-L GH 1 DW (Shift-L 1-DW to obtain result with no shifts)\n\n        vpxor           \\T2, \\GH, \\GH          # second phase of the reduction complete\n        #######################################################################\n        vpxor           \\T1, \\GH, \\GH          # the result is in GH\n\n\n.endm\n\n.macro PRECOMPUTE_AVX2 HK T1 T2 T3 T4 T5 T6\n\n        # Haskey_i_k holds XORed values of the low and high parts of the Haskey_i\n        vmovdqa  \\HK, \\T5\n        GHASH_MUL_AVX2 \\T5, \\HK, \\T1, \\T3, \\T4, \\T6, \\T2    #  T5 = HashKey^2<<1 mod poly\n        vmovdqu  \\T5, HashKey_2(arg2)                       #  [HashKey_2] = HashKey^2<<1 mod poly\n\n        GHASH_MUL_AVX2 \\T5, \\HK, \\T1, \\T3, \\T4, \\T6, \\T2    #  T5 = HashKey^3<<1 mod poly\n        vmovdqu  \\T5, HashKey_3(arg2)\n\n        GHASH_MUL_AVX2 \\T5, \\HK, \\T1, \\T3, \\T4, \\T6, \\T2    #  T5 = HashKey^4<<1 mod poly\n        vmovdqu  \\T5, HashKey_4(arg2)\n\n        GHASH_MUL_AVX2 \\T5, \\HK, \\T1, \\T3, \\T4, \\T6, \\T2    #  T5 = HashKey^5<<1 mod poly\n        vmovdqu  \\T5, HashKey_5(arg2)\n\n        GHASH_MUL_AVX2 \\T5, \\HK, \\T1, \\T3, \\T4, \\T6, \\T2    #  T5 = HashKey^6<<1 mod poly\n        vmovdqu  \\T5, HashKey_6(arg2)\n\n        GHASH_MUL_AVX2 \\T5, \\HK, \\T1, \\T3, \\T4, \\T6, \\T2    #  T5 = HashKey^7<<1 mod poly\n        vmovdqu  \\T5, HashKey_7(arg2)\n\n        GHASH_MUL_AVX2 \\T5, \\HK, \\T1, \\T3, \\T4, \\T6, \\T2    #  T5 = HashKey^8<<1 mod poly\n        vmovdqu  \\T5, HashKey_8(arg2)\n\n.endm\n\n## if a = number of total plaintext bytes\n## b = floor(a/16)\n## num_initial_blocks = b mod 4#\n## encrypt the initial num_initial_blocks blocks and apply ghash on the ciphertext\n## r10, r11, r12, rax are clobbered\n## arg1, arg2, arg3, arg4 are used as pointers only, not modified\n\n.macro INITIAL_BLOCKS_AVX2 REP num_initial_blocks T1 T2 T3 T4 T5 CTR XMM1 XMM2 XMM3 XMM4 XMM5 XMM6 XMM7 XMM8 T6 T_key ENC_DEC VER\n\ti = (8-\\num_initial_blocks)\n\tsetreg\n\tvmovdqu AadHash(arg2), reg_i\n\n\t# start AES for num_initial_blocks blocks\n\tvmovdqu CurCount(arg2), \\CTR\n\n\ti = (9-\\num_initial_blocks)\n\tsetreg\n.rep \\num_initial_blocks\n                vpaddd  ONE(%rip), \\CTR, \\CTR   # INCR Y0\n                vmovdqa \\CTR, reg_i\n                vpshufb SHUF_MASK(%rip), reg_i, reg_i     # perform a 16Byte swap\n\ti = (i+1)\n\tsetreg\n.endr\n\n\tvmovdqa  (arg1), \\T_key\n\ti = (9-\\num_initial_blocks)\n\tsetreg\n.rep \\num_initial_blocks\n                vpxor   \\T_key, reg_i, reg_i\n\ti = (i+1)\n\tsetreg\n.endr\n\n\tj = 1\n\tsetreg\n.rep \\REP\n\tvmovdqa  16*j(arg1), \\T_key\n\ti = (9-\\num_initial_blocks)\n\tsetreg\n.rep \\num_initial_blocks\n        vaesenc \\T_key, reg_i, reg_i\n\ti = (i+1)\n\tsetreg\n.endr\n\n\tj = (j+1)\n\tsetreg\n.endr\n\n\n\tvmovdqa  16*j(arg1), \\T_key\n\ti = (9-\\num_initial_blocks)\n\tsetreg\n.rep \\num_initial_blocks\n        vaesenclast      \\T_key, reg_i, reg_i\n\ti = (i+1)\n\tsetreg\n.endr\n\n\ti = (9-\\num_initial_blocks)\n\tsetreg\n.rep \\num_initial_blocks\n                vmovdqu (arg4, %r11), \\T1\n                vpxor   \\T1, reg_i, reg_i\n                vmovdqu reg_i, (arg3 , %r11)           # write back ciphertext for\n\t\t\t\t\t\t       # num_initial_blocks blocks\n                add     $16, %r11\n.if  \\ENC_DEC == DEC\n                vmovdqa \\T1, reg_i\n.endif\n                vpshufb SHUF_MASK(%rip), reg_i, reg_i  # prepare ciphertext for GHASH computations\n\ti = (i+1)\n\tsetreg\n.endr\n\n\n\ti = (8-\\num_initial_blocks)\n\tj = (9-\\num_initial_blocks)\n\tsetreg\n\n.rep \\num_initial_blocks\n        vpxor    reg_i, reg_j, reg_j\n        GHASH_MUL_AVX2       reg_j, \\T2, \\T1, \\T3, \\T4, \\T5, \\T6  # apply GHASH on num_initial_blocks blocks\n\ti = (i+1)\n\tj = (j+1)\n\tsetreg\n.endr\n        # XMM8 has the combined result here\n\n        vmovdqa  \\XMM8, TMP1(%rsp)\n        vmovdqa  \\XMM8, \\T3\n\n        cmp     $128, %r13\n        jl      .L_initial_blocks_done\\@                  # no need for precomputed constants\n\n###############################################################################\n# Haskey_i_k holds XORed values of the low and high parts of the Haskey_i\n                vpaddd   ONE(%rip), \\CTR, \\CTR          # INCR Y0\n                vmovdqa  \\CTR, \\XMM1\n                vpshufb  SHUF_MASK(%rip), \\XMM1, \\XMM1  # perform a 16Byte swap\n\n                vpaddd   ONE(%rip), \\CTR, \\CTR          # INCR Y0\n                vmovdqa  \\CTR, \\XMM2\n                vpshufb  SHUF_MASK(%rip), \\XMM2, \\XMM2  # perform a 16Byte swap\n\n                vpaddd   ONE(%rip), \\CTR, \\CTR          # INCR Y0\n                vmovdqa  \\CTR, \\XMM3\n                vpshufb  SHUF_MASK(%rip), \\XMM3, \\XMM3  # perform a 16Byte swap\n\n                vpaddd   ONE(%rip), \\CTR, \\CTR          # INCR Y0\n                vmovdqa  \\CTR, \\XMM4\n                vpshufb  SHUF_MASK(%rip), \\XMM4, \\XMM4  # perform a 16Byte swap\n\n                vpaddd   ONE(%rip), \\CTR, \\CTR          # INCR Y0\n                vmovdqa  \\CTR, \\XMM5\n                vpshufb  SHUF_MASK(%rip), \\XMM5, \\XMM5  # perform a 16Byte swap\n\n                vpaddd   ONE(%rip), \\CTR, \\CTR          # INCR Y0\n                vmovdqa  \\CTR, \\XMM6\n                vpshufb  SHUF_MASK(%rip), \\XMM6, \\XMM6  # perform a 16Byte swap\n\n                vpaddd   ONE(%rip), \\CTR, \\CTR          # INCR Y0\n                vmovdqa  \\CTR, \\XMM7\n                vpshufb  SHUF_MASK(%rip), \\XMM7, \\XMM7  # perform a 16Byte swap\n\n                vpaddd   ONE(%rip), \\CTR, \\CTR          # INCR Y0\n                vmovdqa  \\CTR, \\XMM8\n                vpshufb  SHUF_MASK(%rip), \\XMM8, \\XMM8  # perform a 16Byte swap\n\n                vmovdqa  (arg1), \\T_key\n                vpxor    \\T_key, \\XMM1, \\XMM1\n                vpxor    \\T_key, \\XMM2, \\XMM2\n                vpxor    \\T_key, \\XMM3, \\XMM3\n                vpxor    \\T_key, \\XMM4, \\XMM4\n                vpxor    \\T_key, \\XMM5, \\XMM5\n                vpxor    \\T_key, \\XMM6, \\XMM6\n                vpxor    \\T_key, \\XMM7, \\XMM7\n                vpxor    \\T_key, \\XMM8, \\XMM8\n\n\t\ti = 1\n\t\tsetreg\n.rep    \\REP       # do REP rounds\n                vmovdqa  16*i(arg1), \\T_key\n                vaesenc  \\T_key, \\XMM1, \\XMM1\n                vaesenc  \\T_key, \\XMM2, \\XMM2\n                vaesenc  \\T_key, \\XMM3, \\XMM3\n                vaesenc  \\T_key, \\XMM4, \\XMM4\n                vaesenc  \\T_key, \\XMM5, \\XMM5\n                vaesenc  \\T_key, \\XMM6, \\XMM6\n                vaesenc  \\T_key, \\XMM7, \\XMM7\n                vaesenc  \\T_key, \\XMM8, \\XMM8\n\t\ti = (i+1)\n\t\tsetreg\n.endr\n\n\n                vmovdqa  16*i(arg1), \\T_key\n                vaesenclast  \\T_key, \\XMM1, \\XMM1\n                vaesenclast  \\T_key, \\XMM2, \\XMM2\n                vaesenclast  \\T_key, \\XMM3, \\XMM3\n                vaesenclast  \\T_key, \\XMM4, \\XMM4\n                vaesenclast  \\T_key, \\XMM5, \\XMM5\n                vaesenclast  \\T_key, \\XMM6, \\XMM6\n                vaesenclast  \\T_key, \\XMM7, \\XMM7\n                vaesenclast  \\T_key, \\XMM8, \\XMM8\n\n                vmovdqu  (arg4, %r11), \\T1\n                vpxor    \\T1, \\XMM1, \\XMM1\n                vmovdqu  \\XMM1, (arg3 , %r11)\n                .if   \\ENC_DEC == DEC\n                vmovdqa  \\T1, \\XMM1\n                .endif\n\n                vmovdqu  16*1(arg4, %r11), \\T1\n                vpxor    \\T1, \\XMM2, \\XMM2\n                vmovdqu  \\XMM2, 16*1(arg3 , %r11)\n                .if   \\ENC_DEC == DEC\n                vmovdqa  \\T1, \\XMM2\n                .endif\n\n                vmovdqu  16*2(arg4, %r11), \\T1\n                vpxor    \\T1, \\XMM3, \\XMM3\n                vmovdqu  \\XMM3, 16*2(arg3 , %r11)\n                .if   \\ENC_DEC == DEC\n                vmovdqa  \\T1, \\XMM3\n                .endif\n\n                vmovdqu  16*3(arg4, %r11), \\T1\n                vpxor    \\T1, \\XMM4, \\XMM4\n                vmovdqu  \\XMM4, 16*3(arg3 , %r11)\n                .if   \\ENC_DEC == DEC\n                vmovdqa  \\T1, \\XMM4\n                .endif\n\n                vmovdqu  16*4(arg4, %r11), \\T1\n                vpxor    \\T1, \\XMM5, \\XMM5\n                vmovdqu  \\XMM5, 16*4(arg3 , %r11)\n                .if   \\ENC_DEC == DEC\n                vmovdqa  \\T1, \\XMM5\n                .endif\n\n                vmovdqu  16*5(arg4, %r11), \\T1\n                vpxor    \\T1, \\XMM6, \\XMM6\n                vmovdqu  \\XMM6, 16*5(arg3 , %r11)\n                .if   \\ENC_DEC == DEC\n                vmovdqa  \\T1, \\XMM6\n                .endif\n\n                vmovdqu  16*6(arg4, %r11), \\T1\n                vpxor    \\T1, \\XMM7, \\XMM7\n                vmovdqu  \\XMM7, 16*6(arg3 , %r11)\n                .if   \\ENC_DEC == DEC\n                vmovdqa  \\T1, \\XMM7\n                .endif\n\n                vmovdqu  16*7(arg4, %r11), \\T1\n                vpxor    \\T1, \\XMM8, \\XMM8\n                vmovdqu  \\XMM8, 16*7(arg3 , %r11)\n                .if   \\ENC_DEC == DEC\n                vmovdqa  \\T1, \\XMM8\n                .endif\n\n                add     $128, %r11\n\n                vpshufb  SHUF_MASK(%rip), \\XMM1, \\XMM1     # perform a 16Byte swap\n                vpxor    TMP1(%rsp), \\XMM1, \\XMM1          # combine GHASHed value with\n\t\t\t\t\t\t\t   # the corresponding ciphertext\n                vpshufb  SHUF_MASK(%rip), \\XMM2, \\XMM2     # perform a 16Byte swap\n                vpshufb  SHUF_MASK(%rip), \\XMM3, \\XMM3     # perform a 16Byte swap\n                vpshufb  SHUF_MASK(%rip), \\XMM4, \\XMM4     # perform a 16Byte swap\n                vpshufb  SHUF_MASK(%rip), \\XMM5, \\XMM5     # perform a 16Byte swap\n                vpshufb  SHUF_MASK(%rip), \\XMM6, \\XMM6     # perform a 16Byte swap\n                vpshufb  SHUF_MASK(%rip), \\XMM7, \\XMM7     # perform a 16Byte swap\n                vpshufb  SHUF_MASK(%rip), \\XMM8, \\XMM8     # perform a 16Byte swap\n\n###############################################################################\n\n.L_initial_blocks_done\\@:\n\n\n.endm\n\n\n\n# encrypt 8 blocks at a time\n# ghash the 8 previously encrypted ciphertext blocks\n# arg1, arg2, arg3, arg4 are used as pointers only, not modified\n# r11 is the data offset value\n.macro GHASH_8_ENCRYPT_8_PARALLEL_AVX2 REP T1 T2 T3 T4 T5 T6 CTR XMM1 XMM2 XMM3 XMM4 XMM5 XMM6 XMM7 XMM8 T7 loop_idx ENC_DEC\n\n        vmovdqa \\XMM1, \\T2\n        vmovdqa \\XMM2, TMP2(%rsp)\n        vmovdqa \\XMM3, TMP3(%rsp)\n        vmovdqa \\XMM4, TMP4(%rsp)\n        vmovdqa \\XMM5, TMP5(%rsp)\n        vmovdqa \\XMM6, TMP6(%rsp)\n        vmovdqa \\XMM7, TMP7(%rsp)\n        vmovdqa \\XMM8, TMP8(%rsp)\n\n.if \\loop_idx == in_order\n                vpaddd  ONE(%rip), \\CTR, \\XMM1            # INCR CNT\n                vpaddd  ONE(%rip), \\XMM1, \\XMM2\n                vpaddd  ONE(%rip), \\XMM2, \\XMM3\n                vpaddd  ONE(%rip), \\XMM3, \\XMM4\n                vpaddd  ONE(%rip), \\XMM4, \\XMM5\n                vpaddd  ONE(%rip), \\XMM5, \\XMM6\n                vpaddd  ONE(%rip), \\XMM6, \\XMM7\n                vpaddd  ONE(%rip), \\XMM7, \\XMM8\n                vmovdqa \\XMM8, \\CTR\n\n                vpshufb SHUF_MASK(%rip), \\XMM1, \\XMM1     # perform a 16Byte swap\n                vpshufb SHUF_MASK(%rip), \\XMM2, \\XMM2     # perform a 16Byte swap\n                vpshufb SHUF_MASK(%rip), \\XMM3, \\XMM3     # perform a 16Byte swap\n                vpshufb SHUF_MASK(%rip), \\XMM4, \\XMM4     # perform a 16Byte swap\n                vpshufb SHUF_MASK(%rip), \\XMM5, \\XMM5     # perform a 16Byte swap\n                vpshufb SHUF_MASK(%rip), \\XMM6, \\XMM6     # perform a 16Byte swap\n                vpshufb SHUF_MASK(%rip), \\XMM7, \\XMM7     # perform a 16Byte swap\n                vpshufb SHUF_MASK(%rip), \\XMM8, \\XMM8     # perform a 16Byte swap\n.else\n                vpaddd  ONEf(%rip), \\CTR, \\XMM1            # INCR CNT\n                vpaddd  ONEf(%rip), \\XMM1, \\XMM2\n                vpaddd  ONEf(%rip), \\XMM2, \\XMM3\n                vpaddd  ONEf(%rip), \\XMM3, \\XMM4\n                vpaddd  ONEf(%rip), \\XMM4, \\XMM5\n                vpaddd  ONEf(%rip), \\XMM5, \\XMM6\n                vpaddd  ONEf(%rip), \\XMM6, \\XMM7\n                vpaddd  ONEf(%rip), \\XMM7, \\XMM8\n                vmovdqa \\XMM8, \\CTR\n.endif\n\n\n        #######################################################################\n\n                vmovdqu (arg1), \\T1\n                vpxor   \\T1, \\XMM1, \\XMM1\n                vpxor   \\T1, \\XMM2, \\XMM2\n                vpxor   \\T1, \\XMM3, \\XMM3\n                vpxor   \\T1, \\XMM4, \\XMM4\n                vpxor   \\T1, \\XMM5, \\XMM5\n                vpxor   \\T1, \\XMM6, \\XMM6\n                vpxor   \\T1, \\XMM7, \\XMM7\n                vpxor   \\T1, \\XMM8, \\XMM8\n\n        #######################################################################\n\n\n\n\n\n                vmovdqu 16*1(arg1), \\T1\n                vaesenc \\T1, \\XMM1, \\XMM1\n                vaesenc \\T1, \\XMM2, \\XMM2\n                vaesenc \\T1, \\XMM3, \\XMM3\n                vaesenc \\T1, \\XMM4, \\XMM4\n                vaesenc \\T1, \\XMM5, \\XMM5\n                vaesenc \\T1, \\XMM6, \\XMM6\n                vaesenc \\T1, \\XMM7, \\XMM7\n                vaesenc \\T1, \\XMM8, \\XMM8\n\n                vmovdqu 16*2(arg1), \\T1\n                vaesenc \\T1, \\XMM1, \\XMM1\n                vaesenc \\T1, \\XMM2, \\XMM2\n                vaesenc \\T1, \\XMM3, \\XMM3\n                vaesenc \\T1, \\XMM4, \\XMM4\n                vaesenc \\T1, \\XMM5, \\XMM5\n                vaesenc \\T1, \\XMM6, \\XMM6\n                vaesenc \\T1, \\XMM7, \\XMM7\n                vaesenc \\T1, \\XMM8, \\XMM8\n\n\n        #######################################################################\n\n        vmovdqu         HashKey_8(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\T2, \\T4              # T4 = a1*b1\n        vpclmulqdq      $0x00, \\T5, \\T2, \\T7              # T7 = a0*b0\n        vpclmulqdq      $0x01, \\T5, \\T2, \\T6              # T6 = a1*b0\n        vpclmulqdq      $0x10, \\T5, \\T2, \\T5              # T5 = a0*b1\n        vpxor           \\T5, \\T6, \\T6\n\n                vmovdqu 16*3(arg1), \\T1\n                vaesenc \\T1, \\XMM1, \\XMM1\n                vaesenc \\T1, \\XMM2, \\XMM2\n                vaesenc \\T1, \\XMM3, \\XMM3\n                vaesenc \\T1, \\XMM4, \\XMM4\n                vaesenc \\T1, \\XMM5, \\XMM5\n                vaesenc \\T1, \\XMM6, \\XMM6\n                vaesenc \\T1, \\XMM7, \\XMM7\n                vaesenc \\T1, \\XMM8, \\XMM8\n\n        vmovdqa         TMP2(%rsp), \\T1\n        vmovdqu         HashKey_7(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T4, \\T4\n\n        vpclmulqdq      $0x00, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T7, \\T7\n\n        vpclmulqdq      $0x01, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n        vpclmulqdq      $0x10, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n                vmovdqu 16*4(arg1), \\T1\n                vaesenc \\T1, \\XMM1, \\XMM1\n                vaesenc \\T1, \\XMM2, \\XMM2\n                vaesenc \\T1, \\XMM3, \\XMM3\n                vaesenc \\T1, \\XMM4, \\XMM4\n                vaesenc \\T1, \\XMM5, \\XMM5\n                vaesenc \\T1, \\XMM6, \\XMM6\n                vaesenc \\T1, \\XMM7, \\XMM7\n                vaesenc \\T1, \\XMM8, \\XMM8\n\n        #######################################################################\n\n        vmovdqa         TMP3(%rsp), \\T1\n        vmovdqu         HashKey_6(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T4, \\T4\n\n        vpclmulqdq      $0x00, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T7, \\T7\n\n        vpclmulqdq      $0x01, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n        vpclmulqdq      $0x10, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n                vmovdqu 16*5(arg1), \\T1\n                vaesenc \\T1, \\XMM1, \\XMM1\n                vaesenc \\T1, \\XMM2, \\XMM2\n                vaesenc \\T1, \\XMM3, \\XMM3\n                vaesenc \\T1, \\XMM4, \\XMM4\n                vaesenc \\T1, \\XMM5, \\XMM5\n                vaesenc \\T1, \\XMM6, \\XMM6\n                vaesenc \\T1, \\XMM7, \\XMM7\n                vaesenc \\T1, \\XMM8, \\XMM8\n\n        vmovdqa         TMP4(%rsp), \\T1\n        vmovdqu         HashKey_5(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T4, \\T4\n\n        vpclmulqdq      $0x00, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T7, \\T7\n\n        vpclmulqdq      $0x01, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n        vpclmulqdq      $0x10, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n                vmovdqu 16*6(arg1), \\T1\n                vaesenc \\T1, \\XMM1, \\XMM1\n                vaesenc \\T1, \\XMM2, \\XMM2\n                vaesenc \\T1, \\XMM3, \\XMM3\n                vaesenc \\T1, \\XMM4, \\XMM4\n                vaesenc \\T1, \\XMM5, \\XMM5\n                vaesenc \\T1, \\XMM6, \\XMM6\n                vaesenc \\T1, \\XMM7, \\XMM7\n                vaesenc \\T1, \\XMM8, \\XMM8\n\n\n        vmovdqa         TMP5(%rsp), \\T1\n        vmovdqu         HashKey_4(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T4, \\T4\n\n        vpclmulqdq      $0x00, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T7, \\T7\n\n        vpclmulqdq      $0x01, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n        vpclmulqdq      $0x10, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n                vmovdqu 16*7(arg1), \\T1\n                vaesenc \\T1, \\XMM1, \\XMM1\n                vaesenc \\T1, \\XMM2, \\XMM2\n                vaesenc \\T1, \\XMM3, \\XMM3\n                vaesenc \\T1, \\XMM4, \\XMM4\n                vaesenc \\T1, \\XMM5, \\XMM5\n                vaesenc \\T1, \\XMM6, \\XMM6\n                vaesenc \\T1, \\XMM7, \\XMM7\n                vaesenc \\T1, \\XMM8, \\XMM8\n\n        vmovdqa         TMP6(%rsp), \\T1\n        vmovdqu         HashKey_3(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T4, \\T4\n\n        vpclmulqdq      $0x00, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T7, \\T7\n\n        vpclmulqdq      $0x01, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n        vpclmulqdq      $0x10, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n                vmovdqu 16*8(arg1), \\T1\n                vaesenc \\T1, \\XMM1, \\XMM1\n                vaesenc \\T1, \\XMM2, \\XMM2\n                vaesenc \\T1, \\XMM3, \\XMM3\n                vaesenc \\T1, \\XMM4, \\XMM4\n                vaesenc \\T1, \\XMM5, \\XMM5\n                vaesenc \\T1, \\XMM6, \\XMM6\n                vaesenc \\T1, \\XMM7, \\XMM7\n                vaesenc \\T1, \\XMM8, \\XMM8\n\n        vmovdqa         TMP7(%rsp), \\T1\n        vmovdqu         HashKey_2(arg2), \\T5\n        vpclmulqdq      $0x11, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T4, \\T4\n\n        vpclmulqdq      $0x00, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T7, \\T7\n\n        vpclmulqdq      $0x01, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n        vpclmulqdq      $0x10, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n\n        #######################################################################\n\n                vmovdqu 16*9(arg1), \\T5\n                vaesenc \\T5, \\XMM1, \\XMM1\n                vaesenc \\T5, \\XMM2, \\XMM2\n                vaesenc \\T5, \\XMM3, \\XMM3\n                vaesenc \\T5, \\XMM4, \\XMM4\n                vaesenc \\T5, \\XMM5, \\XMM5\n                vaesenc \\T5, \\XMM6, \\XMM6\n                vaesenc \\T5, \\XMM7, \\XMM7\n                vaesenc \\T5, \\XMM8, \\XMM8\n\n        vmovdqa         TMP8(%rsp), \\T1\n        vmovdqu         HashKey(arg2), \\T5\n\n        vpclmulqdq      $0x00, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T7, \\T7\n\n        vpclmulqdq      $0x01, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n        vpclmulqdq      $0x10, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T6, \\T6\n\n        vpclmulqdq      $0x11, \\T5, \\T1, \\T3\n        vpxor           \\T3, \\T4, \\T1\n\n\n                vmovdqu 16*10(arg1), \\T5\n\n        i = 11\n        setreg\n.rep (\\REP-9)\n        vaesenc \\T5, \\XMM1, \\XMM1\n        vaesenc \\T5, \\XMM2, \\XMM2\n        vaesenc \\T5, \\XMM3, \\XMM3\n        vaesenc \\T5, \\XMM4, \\XMM4\n        vaesenc \\T5, \\XMM5, \\XMM5\n        vaesenc \\T5, \\XMM6, \\XMM6\n        vaesenc \\T5, \\XMM7, \\XMM7\n        vaesenc \\T5, \\XMM8, \\XMM8\n\n        vmovdqu 16*i(arg1), \\T5\n        i = i + 1\n        setreg\n.endr\n\n\ti = 0\n\tj = 1\n\tsetreg\n.rep 8\n\t\tvpxor\t16*i(arg4, %r11), \\T5, \\T2\n                .if \\ENC_DEC == ENC\n                vaesenclast     \\T2, reg_j, reg_j\n                .else\n                vaesenclast     \\T2, reg_j, \\T3\n                vmovdqu 16*i(arg4, %r11), reg_j\n                vmovdqu \\T3, 16*i(arg3, %r11)\n                .endif\n\ti = (i+1)\n\tj = (j+1)\n\tsetreg\n.endr\n\t#######################################################################\n\n\n\tvpslldq\t$8, \\T6, \\T3\t\t\t\t# shift-L T3 2 DWs\n\tvpsrldq\t$8, \\T6, \\T6\t\t\t\t# shift-R T2 2 DWs\n\tvpxor\t\\T3, \\T7, \\T7\n\tvpxor\t\\T6, \\T1, \\T1\t\t\t\t# accumulate the results in T1:T7\n\n\n\n\t#######################################################################\n\t#first phase of the reduction\n\tvmovdqa         POLY2(%rip), \\T3\n\n\tvpclmulqdq\t$0x01, \\T7, \\T3, \\T2\n\tvpslldq\t\t$8, \\T2, \\T2\t\t\t# shift-L xmm2 2 DWs\n\n\tvpxor\t\t\\T2, \\T7, \\T7\t\t\t# first phase of the reduction complete\n\t#######################################################################\n                .if \\ENC_DEC == ENC\n\t\tvmovdqu\t \\XMM1,\t16*0(arg3,%r11)\t\t# Write to the Ciphertext buffer\n\t\tvmovdqu\t \\XMM2,\t16*1(arg3,%r11)\t\t# Write to the Ciphertext buffer\n\t\tvmovdqu\t \\XMM3,\t16*2(arg3,%r11)\t\t# Write to the Ciphertext buffer\n\t\tvmovdqu\t \\XMM4,\t16*3(arg3,%r11)\t\t# Write to the Ciphertext buffer\n\t\tvmovdqu\t \\XMM5,\t16*4(arg3,%r11)\t\t# Write to the Ciphertext buffer\n\t\tvmovdqu\t \\XMM6,\t16*5(arg3,%r11)\t\t# Write to the Ciphertext buffer\n\t\tvmovdqu\t \\XMM7,\t16*6(arg3,%r11)\t\t# Write to the Ciphertext buffer\n\t\tvmovdqu\t \\XMM8,\t16*7(arg3,%r11)\t\t# Write to the Ciphertext buffer\n                .endif\n\n\t#######################################################################\n\t#second phase of the reduction\n\tvpclmulqdq\t$0x00, \\T7, \\T3, \\T2\n\tvpsrldq\t\t$4, \\T2, \\T2\t\t\t# shift-R xmm2 1 DW (Shift-R only 1-DW to obtain 2-DWs shift-R)\n\n\tvpclmulqdq\t$0x10, \\T7, \\T3, \\T4\n\tvpslldq\t\t$4, \\T4, \\T4\t\t\t# shift-L xmm0 1 DW (Shift-L 1-DW to obtain result with no shifts)\n\n\tvpxor\t\t\\T2, \\T4, \\T4\t\t\t# second phase of the reduction complete\n\t#######################################################################\n\tvpxor\t\t\\T4, \\T1, \\T1\t\t\t# the result is in T1\n\n\t\tvpshufb\tSHUF_MASK(%rip), \\XMM1, \\XMM1\t# perform a 16Byte swap\n\t\tvpshufb\tSHUF_MASK(%rip), \\XMM2, \\XMM2\t# perform a 16Byte swap\n\t\tvpshufb\tSHUF_MASK(%rip), \\XMM3, \\XMM3\t# perform a 16Byte swap\n\t\tvpshufb\tSHUF_MASK(%rip), \\XMM4, \\XMM4\t# perform a 16Byte swap\n\t\tvpshufb\tSHUF_MASK(%rip), \\XMM5, \\XMM5\t# perform a 16Byte swap\n\t\tvpshufb\tSHUF_MASK(%rip), \\XMM6, \\XMM6\t# perform a 16Byte swap\n\t\tvpshufb\tSHUF_MASK(%rip), \\XMM7, \\XMM7\t# perform a 16Byte swap\n\t\tvpshufb\tSHUF_MASK(%rip), \\XMM8, \\XMM8\t# perform a 16Byte swap\n\n\n\tvpxor\t\\T1, \\XMM1, \\XMM1\n\n\n\n.endm\n\n\n# GHASH the last 4 ciphertext blocks.\n.macro  GHASH_LAST_8_AVX2 T1 T2 T3 T4 T5 T6 T7 XMM1 XMM2 XMM3 XMM4 XMM5 XMM6 XMM7 XMM8\n\n        ## Karatsuba Method\n\n        vmovdqu         HashKey_8(arg2), \\T5\n\n        vpshufd         $0b01001110, \\XMM1, \\T2\n        vpshufd         $0b01001110, \\T5, \\T3\n        vpxor           \\XMM1, \\T2, \\T2\n        vpxor           \\T5, \\T3, \\T3\n\n        vpclmulqdq      $0x11, \\T5, \\XMM1, \\T6\n        vpclmulqdq      $0x00, \\T5, \\XMM1, \\T7\n\n        vpclmulqdq      $0x00, \\T3, \\T2, \\XMM1\n\n        ######################\n\n        vmovdqu         HashKey_7(arg2), \\T5\n        vpshufd         $0b01001110, \\XMM2, \\T2\n        vpshufd         $0b01001110, \\T5, \\T3\n        vpxor           \\XMM2, \\T2, \\T2\n        vpxor           \\T5, \\T3, \\T3\n\n        vpclmulqdq      $0x11, \\T5, \\XMM2, \\T4\n        vpxor           \\T4, \\T6, \\T6\n\n        vpclmulqdq      $0x00, \\T5, \\XMM2, \\T4\n        vpxor           \\T4, \\T7, \\T7\n\n        vpclmulqdq      $0x00, \\T3, \\T2, \\T2\n\n        vpxor           \\T2, \\XMM1, \\XMM1\n\n        ######################\n\n        vmovdqu         HashKey_6(arg2), \\T5\n        vpshufd         $0b01001110, \\XMM3, \\T2\n        vpshufd         $0b01001110, \\T5, \\T3\n        vpxor           \\XMM3, \\T2, \\T2\n        vpxor           \\T5, \\T3, \\T3\n\n        vpclmulqdq      $0x11, \\T5, \\XMM3, \\T4\n        vpxor           \\T4, \\T6, \\T6\n\n        vpclmulqdq      $0x00, \\T5, \\XMM3, \\T4\n        vpxor           \\T4, \\T7, \\T7\n\n        vpclmulqdq      $0x00, \\T3, \\T2, \\T2\n\n        vpxor           \\T2, \\XMM1, \\XMM1\n\n        ######################\n\n        vmovdqu         HashKey_5(arg2), \\T5\n        vpshufd         $0b01001110, \\XMM4, \\T2\n        vpshufd         $0b01001110, \\T5, \\T3\n        vpxor           \\XMM4, \\T2, \\T2\n        vpxor           \\T5, \\T3, \\T3\n\n        vpclmulqdq      $0x11, \\T5, \\XMM4, \\T4\n        vpxor           \\T4, \\T6, \\T6\n\n        vpclmulqdq      $0x00, \\T5, \\XMM4, \\T4\n        vpxor           \\T4, \\T7, \\T7\n\n        vpclmulqdq      $0x00, \\T3, \\T2, \\T2\n\n        vpxor           \\T2, \\XMM1, \\XMM1\n\n        ######################\n\n        vmovdqu         HashKey_4(arg2), \\T5\n        vpshufd         $0b01001110, \\XMM5, \\T2\n        vpshufd         $0b01001110, \\T5, \\T3\n        vpxor           \\XMM5, \\T2, \\T2\n        vpxor           \\T5, \\T3, \\T3\n\n        vpclmulqdq      $0x11, \\T5, \\XMM5, \\T4\n        vpxor           \\T4, \\T6, \\T6\n\n        vpclmulqdq      $0x00, \\T5, \\XMM5, \\T4\n        vpxor           \\T4, \\T7, \\T7\n\n        vpclmulqdq      $0x00, \\T3, \\T2, \\T2\n\n        vpxor           \\T2, \\XMM1, \\XMM1\n\n        ######################\n\n        vmovdqu         HashKey_3(arg2), \\T5\n        vpshufd         $0b01001110, \\XMM6, \\T2\n        vpshufd         $0b01001110, \\T5, \\T3\n        vpxor           \\XMM6, \\T2, \\T2\n        vpxor           \\T5, \\T3, \\T3\n\n        vpclmulqdq      $0x11, \\T5, \\XMM6, \\T4\n        vpxor           \\T4, \\T6, \\T6\n\n        vpclmulqdq      $0x00, \\T5, \\XMM6, \\T4\n        vpxor           \\T4, \\T7, \\T7\n\n        vpclmulqdq      $0x00, \\T3, \\T2, \\T2\n\n        vpxor           \\T2, \\XMM1, \\XMM1\n\n        ######################\n\n        vmovdqu         HashKey_2(arg2), \\T5\n        vpshufd         $0b01001110, \\XMM7, \\T2\n        vpshufd         $0b01001110, \\T5, \\T3\n        vpxor           \\XMM7, \\T2, \\T2\n        vpxor           \\T5, \\T3, \\T3\n\n        vpclmulqdq      $0x11, \\T5, \\XMM7, \\T4\n        vpxor           \\T4, \\T6, \\T6\n\n        vpclmulqdq      $0x00, \\T5, \\XMM7, \\T4\n        vpxor           \\T4, \\T7, \\T7\n\n        vpclmulqdq      $0x00, \\T3, \\T2, \\T2\n\n        vpxor           \\T2, \\XMM1, \\XMM1\n\n        ######################\n\n        vmovdqu         HashKey(arg2), \\T5\n        vpshufd         $0b01001110, \\XMM8, \\T2\n        vpshufd         $0b01001110, \\T5, \\T3\n        vpxor           \\XMM8, \\T2, \\T2\n        vpxor           \\T5, \\T3, \\T3\n\n        vpclmulqdq      $0x11, \\T5, \\XMM8, \\T4\n        vpxor           \\T4, \\T6, \\T6\n\n        vpclmulqdq      $0x00, \\T5, \\XMM8, \\T4\n        vpxor           \\T4, \\T7, \\T7\n\n        vpclmulqdq      $0x00, \\T3, \\T2, \\T2\n\n        vpxor           \\T2, \\XMM1, \\XMM1\n        vpxor           \\T6, \\XMM1, \\XMM1\n        vpxor           \\T7, \\XMM1, \\T2\n\n\n\n\n        vpslldq $8, \\T2, \\T4\n        vpsrldq $8, \\T2, \\T2\n\n        vpxor   \\T4, \\T7, \\T7\n        vpxor   \\T2, \\T6, \\T6                      # <T6:T7> holds the result of the\n\t\t\t\t\t\t   # accumulated carry-less multiplications\n\n        #######################################################################\n        #first phase of the reduction\n        vmovdqa         POLY2(%rip), \\T3\n\n        vpclmulqdq      $0x01, \\T7, \\T3, \\T2\n        vpslldq         $8, \\T2, \\T2               # shift-L xmm2 2 DWs\n\n        vpxor           \\T2, \\T7, \\T7              # first phase of the reduction complete\n        #######################################################################\n\n\n        #second phase of the reduction\n        vpclmulqdq      $0x00, \\T7, \\T3, \\T2\n        vpsrldq         $4, \\T2, \\T2               # shift-R T2 1 DW (Shift-R only 1-DW to obtain 2-DWs shift-R)\n\n        vpclmulqdq      $0x10, \\T7, \\T3, \\T4\n        vpslldq         $4, \\T4, \\T4               # shift-L T4 1 DW (Shift-L 1-DW to obtain result with no shifts)\n\n        vpxor           \\T2, \\T4, \\T4              # second phase of the reduction complete\n        #######################################################################\n        vpxor           \\T4, \\T6, \\T6              # the result is in T6\n.endm\n\n\n\n#############################################################\n#void   aesni_gcm_init_avx_gen4\n#        (gcm_data     *my_ctx_data,\n#         gcm_context_data *data,\n#        u8      *iv,  \n#        u8     *hash_subkey#  \n#        const   u8 *aad,  \n#        u64     aad_len)  \n#############################################################\nSYM_FUNC_START(aesni_gcm_init_avx_gen4)\n        FUNC_SAVE\n        INIT GHASH_MUL_AVX2, PRECOMPUTE_AVX2\n        FUNC_RESTORE\n        RET\nSYM_FUNC_END(aesni_gcm_init_avx_gen4)\n\n###############################################################################\n#void   aesni_gcm_enc_avx_gen4(\n#        gcm_data        *my_ctx_data,      \n#        gcm_context_data *data,\n#        u8      *out,  \n#        const   u8 *in,  \n#        u64     plaintext_len)  \n###############################################################################\nSYM_FUNC_START(aesni_gcm_enc_update_avx_gen4)\n        FUNC_SAVE\n        mov     keysize,%eax\n        cmp     $32, %eax\n        je      key_256_enc_update4\n        cmp     $16, %eax\n        je      key_128_enc_update4\n        # must be 192\n        GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, ENC, 11\n        FUNC_RESTORE\n\tRET\nkey_128_enc_update4:\n        GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, ENC, 9\n        FUNC_RESTORE\n\tRET\nkey_256_enc_update4:\n        GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, ENC, 13\n        FUNC_RESTORE\n\tRET\nSYM_FUNC_END(aesni_gcm_enc_update_avx_gen4)\n\n###############################################################################\n#void   aesni_gcm_dec_update_avx_gen4(\n#        gcm_data        *my_ctx_data,      \n#        gcm_context_data *data,\n#        u8      *out,  \n#        const   u8 *in,  \n#        u64     plaintext_len)  \n###############################################################################\nSYM_FUNC_START(aesni_gcm_dec_update_avx_gen4)\n        FUNC_SAVE\n        mov     keysize,%eax\n        cmp     $32, %eax\n        je      key_256_dec_update4\n        cmp     $16, %eax\n        je      key_128_dec_update4\n        # must be 192\n        GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, DEC, 11\n        FUNC_RESTORE\n        RET\nkey_128_dec_update4:\n        GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, DEC, 9\n        FUNC_RESTORE\n        RET\nkey_256_dec_update4:\n        GCM_ENC_DEC INITIAL_BLOCKS_AVX2, GHASH_8_ENCRYPT_8_PARALLEL_AVX2, GHASH_LAST_8_AVX2, GHASH_MUL_AVX2, DEC, 13\n        FUNC_RESTORE\n        RET\nSYM_FUNC_END(aesni_gcm_dec_update_avx_gen4)\n\n###############################################################################\n#void   aesni_gcm_finalize_avx_gen4(\n#        gcm_data        *my_ctx_data,      \n#        gcm_context_data *data,\n#        u8      *auth_tag,  \n#        u64     auth_tag_len)#  \n###############################################################################\nSYM_FUNC_START(aesni_gcm_finalize_avx_gen4)\n        FUNC_SAVE\n        mov\tkeysize,%eax\n        cmp     $32, %eax\n        je      key_256_finalize4\n        cmp     $16, %eax\n        je      key_128_finalize4\n        # must be 192\n        GCM_COMPLETE GHASH_MUL_AVX2, 11, arg3, arg4\n        FUNC_RESTORE\n        RET\nkey_128_finalize4:\n        GCM_COMPLETE GHASH_MUL_AVX2, 9, arg3, arg4\n        FUNC_RESTORE\n        RET\nkey_256_finalize4:\n        GCM_COMPLETE GHASH_MUL_AVX2, 13, arg3, arg4\n        FUNC_RESTORE\n        RET\nSYM_FUNC_END(aesni_gcm_finalize_avx_gen4)\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}