{
  "module_name": "chacha-avx512vl-x86_64.S",
  "hash_id": "057ed597d121b8d2caabd2908d80112b61add0255bb186ec06cac7f5a80757aa",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/crypto/chacha-avx512vl-x86_64.S",
  "human_readable_source": " \n \n\n#include <linux/linkage.h>\n\n.section\t.rodata.cst32.CTR2BL, \"aM\", @progbits, 32\n.align 32\nCTR2BL:\t.octa 0x00000000000000000000000000000000\n\t.octa 0x00000000000000000000000000000001\n\n.section\t.rodata.cst32.CTR4BL, \"aM\", @progbits, 32\n.align 32\nCTR4BL:\t.octa 0x00000000000000000000000000000002\n\t.octa 0x00000000000000000000000000000003\n\n.section\t.rodata.cst32.CTR8BL, \"aM\", @progbits, 32\n.align 32\nCTR8BL:\t.octa 0x00000003000000020000000100000000\n\t.octa 0x00000007000000060000000500000004\n\n.text\n\nSYM_FUNC_START(chacha_2block_xor_avx512vl)\n\t# %rdi: Input state matrix, s\n\t# %rsi: up to 2 data blocks output, o\n\t# %rdx: up to 2 data blocks input, i\n\t# %rcx: input/output length in bytes\n\t# %r8d: nrounds\n\n\t# This function encrypts two ChaCha blocks by loading the state\n\t# matrix twice across four AVX registers. It performs matrix operations\n\t# on four words in each matrix in parallel, but requires shuffling to\n\t# rearrange the words after each round.\n\n\tvzeroupper\n\n\t# x0..3[0-2] = s0..3\n\tvbroadcasti128\t0x00(%rdi),%ymm0\n\tvbroadcasti128\t0x10(%rdi),%ymm1\n\tvbroadcasti128\t0x20(%rdi),%ymm2\n\tvbroadcasti128\t0x30(%rdi),%ymm3\n\n\tvpaddd\t\tCTR2BL(%rip),%ymm3,%ymm3\n\n\tvmovdqa\t\t%ymm0,%ymm8\n\tvmovdqa\t\t%ymm1,%ymm9\n\tvmovdqa\t\t%ymm2,%ymm10\n\tvmovdqa\t\t%ymm3,%ymm11\n\n.Ldoubleround:\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 16)\n\tvpaddd\t\t%ymm1,%ymm0,%ymm0\n\tvpxord\t\t%ymm0,%ymm3,%ymm3\n\tvprold\t\t$16,%ymm3,%ymm3\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 12)\n\tvpaddd\t\t%ymm3,%ymm2,%ymm2\n\tvpxord\t\t%ymm2,%ymm1,%ymm1\n\tvprold\t\t$12,%ymm1,%ymm1\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 8)\n\tvpaddd\t\t%ymm1,%ymm0,%ymm0\n\tvpxord\t\t%ymm0,%ymm3,%ymm3\n\tvprold\t\t$8,%ymm3,%ymm3\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 7)\n\tvpaddd\t\t%ymm3,%ymm2,%ymm2\n\tvpxord\t\t%ymm2,%ymm1,%ymm1\n\tvprold\t\t$7,%ymm1,%ymm1\n\n\t# x1 = shuffle32(x1, MASK(0, 3, 2, 1))\n\tvpshufd\t\t$0x39,%ymm1,%ymm1\n\t# x2 = shuffle32(x2, MASK(1, 0, 3, 2))\n\tvpshufd\t\t$0x4e,%ymm2,%ymm2\n\t# x3 = shuffle32(x3, MASK(2, 1, 0, 3))\n\tvpshufd\t\t$0x93,%ymm3,%ymm3\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 16)\n\tvpaddd\t\t%ymm1,%ymm0,%ymm0\n\tvpxord\t\t%ymm0,%ymm3,%ymm3\n\tvprold\t\t$16,%ymm3,%ymm3\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 12)\n\tvpaddd\t\t%ymm3,%ymm2,%ymm2\n\tvpxord\t\t%ymm2,%ymm1,%ymm1\n\tvprold\t\t$12,%ymm1,%ymm1\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 8)\n\tvpaddd\t\t%ymm1,%ymm0,%ymm0\n\tvpxord\t\t%ymm0,%ymm3,%ymm3\n\tvprold\t\t$8,%ymm3,%ymm3\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 7)\n\tvpaddd\t\t%ymm3,%ymm2,%ymm2\n\tvpxord\t\t%ymm2,%ymm1,%ymm1\n\tvprold\t\t$7,%ymm1,%ymm1\n\n\t# x1 = shuffle32(x1, MASK(2, 1, 0, 3))\n\tvpshufd\t\t$0x93,%ymm1,%ymm1\n\t# x2 = shuffle32(x2, MASK(1, 0, 3, 2))\n\tvpshufd\t\t$0x4e,%ymm2,%ymm2\n\t# x3 = shuffle32(x3, MASK(0, 3, 2, 1))\n\tvpshufd\t\t$0x39,%ymm3,%ymm3\n\n\tsub\t\t$2,%r8d\n\tjnz\t\t.Ldoubleround\n\n\t# o0 = i0 ^ (x0 + s0)\n\tvpaddd\t\t%ymm8,%ymm0,%ymm7\n\tcmp\t\t$0x10,%rcx\n\tjl\t\t.Lxorpart2\n\tvpxord\t\t0x00(%rdx),%xmm7,%xmm6\n\tvmovdqu\t\t%xmm6,0x00(%rsi)\n\tvextracti128\t$1,%ymm7,%xmm0\n\t# o1 = i1 ^ (x1 + s1)\n\tvpaddd\t\t%ymm9,%ymm1,%ymm7\n\tcmp\t\t$0x20,%rcx\n\tjl\t\t.Lxorpart2\n\tvpxord\t\t0x10(%rdx),%xmm7,%xmm6\n\tvmovdqu\t\t%xmm6,0x10(%rsi)\n\tvextracti128\t$1,%ymm7,%xmm1\n\t# o2 = i2 ^ (x2 + s2)\n\tvpaddd\t\t%ymm10,%ymm2,%ymm7\n\tcmp\t\t$0x30,%rcx\n\tjl\t\t.Lxorpart2\n\tvpxord\t\t0x20(%rdx),%xmm7,%xmm6\n\tvmovdqu\t\t%xmm6,0x20(%rsi)\n\tvextracti128\t$1,%ymm7,%xmm2\n\t# o3 = i3 ^ (x3 + s3)\n\tvpaddd\t\t%ymm11,%ymm3,%ymm7\n\tcmp\t\t$0x40,%rcx\n\tjl\t\t.Lxorpart2\n\tvpxord\t\t0x30(%rdx),%xmm7,%xmm6\n\tvmovdqu\t\t%xmm6,0x30(%rsi)\n\tvextracti128\t$1,%ymm7,%xmm3\n\n\t# xor and write second block\n\tvmovdqa\t\t%xmm0,%xmm7\n\tcmp\t\t$0x50,%rcx\n\tjl\t\t.Lxorpart2\n\tvpxord\t\t0x40(%rdx),%xmm7,%xmm6\n\tvmovdqu\t\t%xmm6,0x40(%rsi)\n\n\tvmovdqa\t\t%xmm1,%xmm7\n\tcmp\t\t$0x60,%rcx\n\tjl\t\t.Lxorpart2\n\tvpxord\t\t0x50(%rdx),%xmm7,%xmm6\n\tvmovdqu\t\t%xmm6,0x50(%rsi)\n\n\tvmovdqa\t\t%xmm2,%xmm7\n\tcmp\t\t$0x70,%rcx\n\tjl\t\t.Lxorpart2\n\tvpxord\t\t0x60(%rdx),%xmm7,%xmm6\n\tvmovdqu\t\t%xmm6,0x60(%rsi)\n\n\tvmovdqa\t\t%xmm3,%xmm7\n\tcmp\t\t$0x80,%rcx\n\tjl\t\t.Lxorpart2\n\tvpxord\t\t0x70(%rdx),%xmm7,%xmm6\n\tvmovdqu\t\t%xmm6,0x70(%rsi)\n\n.Ldone2:\n\tvzeroupper\n\tRET\n\n.Lxorpart2:\n\t# xor remaining bytes from partial register into output\n\tmov\t\t%rcx,%rax\n\tand\t\t$0xf,%rcx\n\tjz\t\t.Ldone2\n\tmov\t\t%rax,%r9\n\tand\t\t$~0xf,%r9\n\n\tmov\t\t$1,%rax\n\tshld\t\t%cl,%rax,%rax\n\tsub\t\t$1,%rax\n\tkmovq\t\t%rax,%k1\n\n\tvmovdqu8\t(%rdx,%r9),%xmm1{%k1}{z}\n\tvpxord\t\t%xmm7,%xmm1,%xmm1\n\tvmovdqu8\t%xmm1,(%rsi,%r9){%k1}\n\n\tjmp\t\t.Ldone2\n\nSYM_FUNC_END(chacha_2block_xor_avx512vl)\n\nSYM_FUNC_START(chacha_4block_xor_avx512vl)\n\t# %rdi: Input state matrix, s\n\t# %rsi: up to 4 data blocks output, o\n\t# %rdx: up to 4 data blocks input, i\n\t# %rcx: input/output length in bytes\n\t# %r8d: nrounds\n\n\t# This function encrypts four ChaCha blocks by loading the state\n\t# matrix four times across eight AVX registers. It performs matrix\n\t# operations on four words in two matrices in parallel, sequentially\n\t# to the operations on the four words of the other two matrices. The\n\t# required word shuffling has a rather high latency, we can do the\n\t# arithmetic on two matrix-pairs without much slowdown.\n\n\tvzeroupper\n\n\t# x0..3[0-4] = s0..3\n\tvbroadcasti128\t0x00(%rdi),%ymm0\n\tvbroadcasti128\t0x10(%rdi),%ymm1\n\tvbroadcasti128\t0x20(%rdi),%ymm2\n\tvbroadcasti128\t0x30(%rdi),%ymm3\n\n\tvmovdqa\t\t%ymm0,%ymm4\n\tvmovdqa\t\t%ymm1,%ymm5\n\tvmovdqa\t\t%ymm2,%ymm6\n\tvmovdqa\t\t%ymm3,%ymm7\n\n\tvpaddd\t\tCTR2BL(%rip),%ymm3,%ymm3\n\tvpaddd\t\tCTR4BL(%rip),%ymm7,%ymm7\n\n\tvmovdqa\t\t%ymm0,%ymm11\n\tvmovdqa\t\t%ymm1,%ymm12\n\tvmovdqa\t\t%ymm2,%ymm13\n\tvmovdqa\t\t%ymm3,%ymm14\n\tvmovdqa\t\t%ymm7,%ymm15\n\n.Ldoubleround4:\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 16)\n\tvpaddd\t\t%ymm1,%ymm0,%ymm0\n\tvpxord\t\t%ymm0,%ymm3,%ymm3\n\tvprold\t\t$16,%ymm3,%ymm3\n\n\tvpaddd\t\t%ymm5,%ymm4,%ymm4\n\tvpxord\t\t%ymm4,%ymm7,%ymm7\n\tvprold\t\t$16,%ymm7,%ymm7\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 12)\n\tvpaddd\t\t%ymm3,%ymm2,%ymm2\n\tvpxord\t\t%ymm2,%ymm1,%ymm1\n\tvprold\t\t$12,%ymm1,%ymm1\n\n\tvpaddd\t\t%ymm7,%ymm6,%ymm6\n\tvpxord\t\t%ymm6,%ymm5,%ymm5\n\tvprold\t\t$12,%ymm5,%ymm5\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 8)\n\tvpaddd\t\t%ymm1,%ymm0,%ymm0\n\tvpxord\t\t%ymm0,%ymm3,%ymm3\n\tvprold\t\t$8,%ymm3,%ymm3\n\n\tvpaddd\t\t%ymm5,%ymm4,%ymm4\n\tvpxord\t\t%ymm4,%ymm7,%ymm7\n\tvprold\t\t$8,%ymm7,%ymm7\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 7)\n\tvpaddd\t\t%ymm3,%ymm2,%ymm2\n\tvpxord\t\t%ymm2,%ymm1,%ymm1\n\tvprold\t\t$7,%ymm1,%ymm1\n\n\tvpaddd\t\t%ymm7,%ymm6,%ymm6\n\tvpxord\t\t%ymm6,%ymm5,%ymm5\n\tvprold\t\t$7,%ymm5,%ymm5\n\n\t# x1 = shuffle32(x1, MASK(0, 3, 2, 1))\n\tvpshufd\t\t$0x39,%ymm1,%ymm1\n\tvpshufd\t\t$0x39,%ymm5,%ymm5\n\t# x2 = shuffle32(x2, MASK(1, 0, 3, 2))\n\tvpshufd\t\t$0x4e,%ymm2,%ymm2\n\tvpshufd\t\t$0x4e,%ymm6,%ymm6\n\t# x3 = shuffle32(x3, MASK(2, 1, 0, 3))\n\tvpshufd\t\t$0x93,%ymm3,%ymm3\n\tvpshufd\t\t$0x93,%ymm7,%ymm7\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 16)\n\tvpaddd\t\t%ymm1,%ymm0,%ymm0\n\tvpxord\t\t%ymm0,%ymm3,%ymm3\n\tvprold\t\t$16,%ymm3,%ymm3\n\n\tvpaddd\t\t%ymm5,%ymm4,%ymm4\n\tvpxord\t\t%ymm4,%ymm7,%ymm7\n\tvprold\t\t$16,%ymm7,%ymm7\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 12)\n\tvpaddd\t\t%ymm3,%ymm2,%ymm2\n\tvpxord\t\t%ymm2,%ymm1,%ymm1\n\tvprold\t\t$12,%ymm1,%ymm1\n\n\tvpaddd\t\t%ymm7,%ymm6,%ymm6\n\tvpxord\t\t%ymm6,%ymm5,%ymm5\n\tvprold\t\t$12,%ymm5,%ymm5\n\n\t# x0 += x1, x3 = rotl32(x3 ^ x0, 8)\n\tvpaddd\t\t%ymm1,%ymm0,%ymm0\n\tvpxord\t\t%ymm0,%ymm3,%ymm3\n\tvprold\t\t$8,%ymm3,%ymm3\n\n\tvpaddd\t\t%ymm5,%ymm4,%ymm4\n\tvpxord\t\t%ymm4,%ymm7,%ymm7\n\tvprold\t\t$8,%ymm7,%ymm7\n\n\t# x2 += x3, x1 = rotl32(x1 ^ x2, 7)\n\tvpaddd\t\t%ymm3,%ymm2,%ymm2\n\tvpxord\t\t%ymm2,%ymm1,%ymm1\n\tvprold\t\t$7,%ymm1,%ymm1\n\n\tvpaddd\t\t%ymm7,%ymm6,%ymm6\n\tvpxord\t\t%ymm6,%ymm5,%ymm5\n\tvprold\t\t$7,%ymm5,%ymm5\n\n\t# x1 = shuffle32(x1, MASK(2, 1, 0, 3))\n\tvpshufd\t\t$0x93,%ymm1,%ymm1\n\tvpshufd\t\t$0x93,%ymm5,%ymm5\n\t# x2 = shuffle32(x2, MASK(1, 0, 3, 2))\n\tvpshufd\t\t$0x4e,%ymm2,%ymm2\n\tvpshufd\t\t$0x4e,%ymm6,%ymm6\n\t# x3 = shuffle32(x3, MASK(0, 3, 2, 1))\n\tvpshufd\t\t$0x39,%ymm3,%ymm3\n\tvpshufd\t\t$0x39,%ymm7,%ymm7\n\n\tsub\t\t$2,%r8d\n\tjnz\t\t.Ldoubleround4\n\n\t# o0 = i0 ^ (x0 + s0), first block\n\tvpaddd\t\t%ymm11,%ymm0,%ymm10\n\tcmp\t\t$0x10,%rcx\n\tjl\t\t.Lxorpart4\n\tvpxord\t\t0x00(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x00(%rsi)\n\tvextracti128\t$1,%ymm10,%xmm0\n\t# o1 = i1 ^ (x1 + s1), first block\n\tvpaddd\t\t%ymm12,%ymm1,%ymm10\n\tcmp\t\t$0x20,%rcx\n\tjl\t\t.Lxorpart4\n\tvpxord\t\t0x10(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x10(%rsi)\n\tvextracti128\t$1,%ymm10,%xmm1\n\t# o2 = i2 ^ (x2 + s2), first block\n\tvpaddd\t\t%ymm13,%ymm2,%ymm10\n\tcmp\t\t$0x30,%rcx\n\tjl\t\t.Lxorpart4\n\tvpxord\t\t0x20(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x20(%rsi)\n\tvextracti128\t$1,%ymm10,%xmm2\n\t# o3 = i3 ^ (x3 + s3), first block\n\tvpaddd\t\t%ymm14,%ymm3,%ymm10\n\tcmp\t\t$0x40,%rcx\n\tjl\t\t.Lxorpart4\n\tvpxord\t\t0x30(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x30(%rsi)\n\tvextracti128\t$1,%ymm10,%xmm3\n\n\t# xor and write second block\n\tvmovdqa\t\t%xmm0,%xmm10\n\tcmp\t\t$0x50,%rcx\n\tjl\t\t.Lxorpart4\n\tvpxord\t\t0x40(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x40(%rsi)\n\n\tvmovdqa\t\t%xmm1,%xmm10\n\tcmp\t\t$0x60,%rcx\n\tjl\t\t.Lxorpart4\n\tvpxord\t\t0x50(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x50(%rsi)\n\n\tvmovdqa\t\t%xmm2,%xmm10\n\tcmp\t\t$0x70,%rcx\n\tjl\t\t.Lxorpart4\n\tvpxord\t\t0x60(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x60(%rsi)\n\n\tvmovdqa\t\t%xmm3,%xmm10\n\tcmp\t\t$0x80,%rcx\n\tjl\t\t.Lxorpart4\n\tvpxord\t\t0x70(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x70(%rsi)\n\n\t# o0 = i0 ^ (x0 + s0), third block\n\tvpaddd\t\t%ymm11,%ymm4,%ymm10\n\tcmp\t\t$0x90,%rcx\n\tjl\t\t.Lxorpart4\n\tvpxord\t\t0x80(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x80(%rsi)\n\tvextracti128\t$1,%ymm10,%xmm4\n\t# o1 = i1 ^ (x1 + s1), third block\n\tvpaddd\t\t%ymm12,%ymm5,%ymm10\n\tcmp\t\t$0xa0,%rcx\n\tjl\t\t.Lxorpart4\n\tvpxord\t\t0x90(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0x90(%rsi)\n\tvextracti128\t$1,%ymm10,%xmm5\n\t# o2 = i2 ^ (x2 + s2), third block\n\tvpaddd\t\t%ymm13,%ymm6,%ymm10\n\tcmp\t\t$0xb0,%rcx\n\tjl\t\t.Lxorpart4\n\tvpxord\t\t0xa0(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0xa0(%rsi)\n\tvextracti128\t$1,%ymm10,%xmm6\n\t# o3 = i3 ^ (x3 + s3), third block\n\tvpaddd\t\t%ymm15,%ymm7,%ymm10\n\tcmp\t\t$0xc0,%rcx\n\tjl\t\t.Lxorpart4\n\tvpxord\t\t0xb0(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0xb0(%rsi)\n\tvextracti128\t$1,%ymm10,%xmm7\n\n\t# xor and write fourth block\n\tvmovdqa\t\t%xmm4,%xmm10\n\tcmp\t\t$0xd0,%rcx\n\tjl\t\t.Lxorpart4\n\tvpxord\t\t0xc0(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0xc0(%rsi)\n\n\tvmovdqa\t\t%xmm5,%xmm10\n\tcmp\t\t$0xe0,%rcx\n\tjl\t\t.Lxorpart4\n\tvpxord\t\t0xd0(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0xd0(%rsi)\n\n\tvmovdqa\t\t%xmm6,%xmm10\n\tcmp\t\t$0xf0,%rcx\n\tjl\t\t.Lxorpart4\n\tvpxord\t\t0xe0(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0xe0(%rsi)\n\n\tvmovdqa\t\t%xmm7,%xmm10\n\tcmp\t\t$0x100,%rcx\n\tjl\t\t.Lxorpart4\n\tvpxord\t\t0xf0(%rdx),%xmm10,%xmm9\n\tvmovdqu\t\t%xmm9,0xf0(%rsi)\n\n.Ldone4:\n\tvzeroupper\n\tRET\n\n.Lxorpart4:\n\t# xor remaining bytes from partial register into output\n\tmov\t\t%rcx,%rax\n\tand\t\t$0xf,%rcx\n\tjz\t\t.Ldone4\n\tmov\t\t%rax,%r9\n\tand\t\t$~0xf,%r9\n\n\tmov\t\t$1,%rax\n\tshld\t\t%cl,%rax,%rax\n\tsub\t\t$1,%rax\n\tkmovq\t\t%rax,%k1\n\n\tvmovdqu8\t(%rdx,%r9),%xmm1{%k1}{z}\n\tvpxord\t\t%xmm10,%xmm1,%xmm1\n\tvmovdqu8\t%xmm1,(%rsi,%r9){%k1}\n\n\tjmp\t\t.Ldone4\n\nSYM_FUNC_END(chacha_4block_xor_avx512vl)\n\nSYM_FUNC_START(chacha_8block_xor_avx512vl)\n\t# %rdi: Input state matrix, s\n\t# %rsi: up to 8 data blocks output, o\n\t# %rdx: up to 8 data blocks input, i\n\t# %rcx: input/output length in bytes\n\t# %r8d: nrounds\n\n\t# This function encrypts eight consecutive ChaCha blocks by loading\n\t# the state matrix in AVX registers eight times. Compared to AVX2, this\n\t# mostly benefits from the new rotate instructions in VL and the\n\t# additional registers.\n\n\tvzeroupper\n\n\t# x0..15[0-7] = s[0..15]\n\tvpbroadcastd\t0x00(%rdi),%ymm0\n\tvpbroadcastd\t0x04(%rdi),%ymm1\n\tvpbroadcastd\t0x08(%rdi),%ymm2\n\tvpbroadcastd\t0x0c(%rdi),%ymm3\n\tvpbroadcastd\t0x10(%rdi),%ymm4\n\tvpbroadcastd\t0x14(%rdi),%ymm5\n\tvpbroadcastd\t0x18(%rdi),%ymm6\n\tvpbroadcastd\t0x1c(%rdi),%ymm7\n\tvpbroadcastd\t0x20(%rdi),%ymm8\n\tvpbroadcastd\t0x24(%rdi),%ymm9\n\tvpbroadcastd\t0x28(%rdi),%ymm10\n\tvpbroadcastd\t0x2c(%rdi),%ymm11\n\tvpbroadcastd\t0x30(%rdi),%ymm12\n\tvpbroadcastd\t0x34(%rdi),%ymm13\n\tvpbroadcastd\t0x38(%rdi),%ymm14\n\tvpbroadcastd\t0x3c(%rdi),%ymm15\n\n\t# x12 += counter values 0-3\n\tvpaddd\t\tCTR8BL(%rip),%ymm12,%ymm12\n\n\tvmovdqa64\t%ymm0,%ymm16\n\tvmovdqa64\t%ymm1,%ymm17\n\tvmovdqa64\t%ymm2,%ymm18\n\tvmovdqa64\t%ymm3,%ymm19\n\tvmovdqa64\t%ymm4,%ymm20\n\tvmovdqa64\t%ymm5,%ymm21\n\tvmovdqa64\t%ymm6,%ymm22\n\tvmovdqa64\t%ymm7,%ymm23\n\tvmovdqa64\t%ymm8,%ymm24\n\tvmovdqa64\t%ymm9,%ymm25\n\tvmovdqa64\t%ymm10,%ymm26\n\tvmovdqa64\t%ymm11,%ymm27\n\tvmovdqa64\t%ymm12,%ymm28\n\tvmovdqa64\t%ymm13,%ymm29\n\tvmovdqa64\t%ymm14,%ymm30\n\tvmovdqa64\t%ymm15,%ymm31\n\n.Ldoubleround8:\n\t# x0 += x4, x12 = rotl32(x12 ^ x0, 16)\n\tvpaddd\t\t%ymm0,%ymm4,%ymm0\n\tvpxord\t\t%ymm0,%ymm12,%ymm12\n\tvprold\t\t$16,%ymm12,%ymm12\n\t# x1 += x5, x13 = rotl32(x13 ^ x1, 16)\n\tvpaddd\t\t%ymm1,%ymm5,%ymm1\n\tvpxord\t\t%ymm1,%ymm13,%ymm13\n\tvprold\t\t$16,%ymm13,%ymm13\n\t# x2 += x6, x14 = rotl32(x14 ^ x2, 16)\n\tvpaddd\t\t%ymm2,%ymm6,%ymm2\n\tvpxord\t\t%ymm2,%ymm14,%ymm14\n\tvprold\t\t$16,%ymm14,%ymm14\n\t# x3 += x7, x15 = rotl32(x15 ^ x3, 16)\n\tvpaddd\t\t%ymm3,%ymm7,%ymm3\n\tvpxord\t\t%ymm3,%ymm15,%ymm15\n\tvprold\t\t$16,%ymm15,%ymm15\n\n\t# x8 += x12, x4 = rotl32(x4 ^ x8, 12)\n\tvpaddd\t\t%ymm12,%ymm8,%ymm8\n\tvpxord\t\t%ymm8,%ymm4,%ymm4\n\tvprold\t\t$12,%ymm4,%ymm4\n\t# x9 += x13, x5 = rotl32(x5 ^ x9, 12)\n\tvpaddd\t\t%ymm13,%ymm9,%ymm9\n\tvpxord\t\t%ymm9,%ymm5,%ymm5\n\tvprold\t\t$12,%ymm5,%ymm5\n\t# x10 += x14, x6 = rotl32(x6 ^ x10, 12)\n\tvpaddd\t\t%ymm14,%ymm10,%ymm10\n\tvpxord\t\t%ymm10,%ymm6,%ymm6\n\tvprold\t\t$12,%ymm6,%ymm6\n\t# x11 += x15, x7 = rotl32(x7 ^ x11, 12)\n\tvpaddd\t\t%ymm15,%ymm11,%ymm11\n\tvpxord\t\t%ymm11,%ymm7,%ymm7\n\tvprold\t\t$12,%ymm7,%ymm7\n\n\t# x0 += x4, x12 = rotl32(x12 ^ x0, 8)\n\tvpaddd\t\t%ymm0,%ymm4,%ymm0\n\tvpxord\t\t%ymm0,%ymm12,%ymm12\n\tvprold\t\t$8,%ymm12,%ymm12\n\t# x1 += x5, x13 = rotl32(x13 ^ x1, 8)\n\tvpaddd\t\t%ymm1,%ymm5,%ymm1\n\tvpxord\t\t%ymm1,%ymm13,%ymm13\n\tvprold\t\t$8,%ymm13,%ymm13\n\t# x2 += x6, x14 = rotl32(x14 ^ x2, 8)\n\tvpaddd\t\t%ymm2,%ymm6,%ymm2\n\tvpxord\t\t%ymm2,%ymm14,%ymm14\n\tvprold\t\t$8,%ymm14,%ymm14\n\t# x3 += x7, x15 = rotl32(x15 ^ x3, 8)\n\tvpaddd\t\t%ymm3,%ymm7,%ymm3\n\tvpxord\t\t%ymm3,%ymm15,%ymm15\n\tvprold\t\t$8,%ymm15,%ymm15\n\n\t# x8 += x12, x4 = rotl32(x4 ^ x8, 7)\n\tvpaddd\t\t%ymm12,%ymm8,%ymm8\n\tvpxord\t\t%ymm8,%ymm4,%ymm4\n\tvprold\t\t$7,%ymm4,%ymm4\n\t# x9 += x13, x5 = rotl32(x5 ^ x9, 7)\n\tvpaddd\t\t%ymm13,%ymm9,%ymm9\n\tvpxord\t\t%ymm9,%ymm5,%ymm5\n\tvprold\t\t$7,%ymm5,%ymm5\n\t# x10 += x14, x6 = rotl32(x6 ^ x10, 7)\n\tvpaddd\t\t%ymm14,%ymm10,%ymm10\n\tvpxord\t\t%ymm10,%ymm6,%ymm6\n\tvprold\t\t$7,%ymm6,%ymm6\n\t# x11 += x15, x7 = rotl32(x7 ^ x11, 7)\n\tvpaddd\t\t%ymm15,%ymm11,%ymm11\n\tvpxord\t\t%ymm11,%ymm7,%ymm7\n\tvprold\t\t$7,%ymm7,%ymm7\n\n\t# x0 += x5, x15 = rotl32(x15 ^ x0, 16)\n\tvpaddd\t\t%ymm0,%ymm5,%ymm0\n\tvpxord\t\t%ymm0,%ymm15,%ymm15\n\tvprold\t\t$16,%ymm15,%ymm15\n\t# x1 += x6, x12 = rotl32(x12 ^ x1, 16)\n\tvpaddd\t\t%ymm1,%ymm6,%ymm1\n\tvpxord\t\t%ymm1,%ymm12,%ymm12\n\tvprold\t\t$16,%ymm12,%ymm12\n\t# x2 += x7, x13 = rotl32(x13 ^ x2, 16)\n\tvpaddd\t\t%ymm2,%ymm7,%ymm2\n\tvpxord\t\t%ymm2,%ymm13,%ymm13\n\tvprold\t\t$16,%ymm13,%ymm13\n\t# x3 += x4, x14 = rotl32(x14 ^ x3, 16)\n\tvpaddd\t\t%ymm3,%ymm4,%ymm3\n\tvpxord\t\t%ymm3,%ymm14,%ymm14\n\tvprold\t\t$16,%ymm14,%ymm14\n\n\t# x10 += x15, x5 = rotl32(x5 ^ x10, 12)\n\tvpaddd\t\t%ymm15,%ymm10,%ymm10\n\tvpxord\t\t%ymm10,%ymm5,%ymm5\n\tvprold\t\t$12,%ymm5,%ymm5\n\t# x11 += x12, x6 = rotl32(x6 ^ x11, 12)\n\tvpaddd\t\t%ymm12,%ymm11,%ymm11\n\tvpxord\t\t%ymm11,%ymm6,%ymm6\n\tvprold\t\t$12,%ymm6,%ymm6\n\t# x8 += x13, x7 = rotl32(x7 ^ x8, 12)\n\tvpaddd\t\t%ymm13,%ymm8,%ymm8\n\tvpxord\t\t%ymm8,%ymm7,%ymm7\n\tvprold\t\t$12,%ymm7,%ymm7\n\t# x9 += x14, x4 = rotl32(x4 ^ x9, 12)\n\tvpaddd\t\t%ymm14,%ymm9,%ymm9\n\tvpxord\t\t%ymm9,%ymm4,%ymm4\n\tvprold\t\t$12,%ymm4,%ymm4\n\n\t# x0 += x5, x15 = rotl32(x15 ^ x0, 8)\n\tvpaddd\t\t%ymm0,%ymm5,%ymm0\n\tvpxord\t\t%ymm0,%ymm15,%ymm15\n\tvprold\t\t$8,%ymm15,%ymm15\n\t# x1 += x6, x12 = rotl32(x12 ^ x1, 8)\n\tvpaddd\t\t%ymm1,%ymm6,%ymm1\n\tvpxord\t\t%ymm1,%ymm12,%ymm12\n\tvprold\t\t$8,%ymm12,%ymm12\n\t# x2 += x7, x13 = rotl32(x13 ^ x2, 8)\n\tvpaddd\t\t%ymm2,%ymm7,%ymm2\n\tvpxord\t\t%ymm2,%ymm13,%ymm13\n\tvprold\t\t$8,%ymm13,%ymm13\n\t# x3 += x4, x14 = rotl32(x14 ^ x3, 8)\n\tvpaddd\t\t%ymm3,%ymm4,%ymm3\n\tvpxord\t\t%ymm3,%ymm14,%ymm14\n\tvprold\t\t$8,%ymm14,%ymm14\n\n\t# x10 += x15, x5 = rotl32(x5 ^ x10, 7)\n\tvpaddd\t\t%ymm15,%ymm10,%ymm10\n\tvpxord\t\t%ymm10,%ymm5,%ymm5\n\tvprold\t\t$7,%ymm5,%ymm5\n\t# x11 += x12, x6 = rotl32(x6 ^ x11, 7)\n\tvpaddd\t\t%ymm12,%ymm11,%ymm11\n\tvpxord\t\t%ymm11,%ymm6,%ymm6\n\tvprold\t\t$7,%ymm6,%ymm6\n\t# x8 += x13, x7 = rotl32(x7 ^ x8, 7)\n\tvpaddd\t\t%ymm13,%ymm8,%ymm8\n\tvpxord\t\t%ymm8,%ymm7,%ymm7\n\tvprold\t\t$7,%ymm7,%ymm7\n\t# x9 += x14, x4 = rotl32(x4 ^ x9, 7)\n\tvpaddd\t\t%ymm14,%ymm9,%ymm9\n\tvpxord\t\t%ymm9,%ymm4,%ymm4\n\tvprold\t\t$7,%ymm4,%ymm4\n\n\tsub\t\t$2,%r8d\n\tjnz\t\t.Ldoubleround8\n\n\t# x0..15[0-3] += s[0..15]\n\tvpaddd\t\t%ymm16,%ymm0,%ymm0\n\tvpaddd\t\t%ymm17,%ymm1,%ymm1\n\tvpaddd\t\t%ymm18,%ymm2,%ymm2\n\tvpaddd\t\t%ymm19,%ymm3,%ymm3\n\tvpaddd\t\t%ymm20,%ymm4,%ymm4\n\tvpaddd\t\t%ymm21,%ymm5,%ymm5\n\tvpaddd\t\t%ymm22,%ymm6,%ymm6\n\tvpaddd\t\t%ymm23,%ymm7,%ymm7\n\tvpaddd\t\t%ymm24,%ymm8,%ymm8\n\tvpaddd\t\t%ymm25,%ymm9,%ymm9\n\tvpaddd\t\t%ymm26,%ymm10,%ymm10\n\tvpaddd\t\t%ymm27,%ymm11,%ymm11\n\tvpaddd\t\t%ymm28,%ymm12,%ymm12\n\tvpaddd\t\t%ymm29,%ymm13,%ymm13\n\tvpaddd\t\t%ymm30,%ymm14,%ymm14\n\tvpaddd\t\t%ymm31,%ymm15,%ymm15\n\n\t# interleave 32-bit words in state n, n+1\n\tvpunpckldq\t%ymm1,%ymm0,%ymm16\n\tvpunpckhdq\t%ymm1,%ymm0,%ymm17\n\tvpunpckldq\t%ymm3,%ymm2,%ymm18\n\tvpunpckhdq\t%ymm3,%ymm2,%ymm19\n\tvpunpckldq\t%ymm5,%ymm4,%ymm20\n\tvpunpckhdq\t%ymm5,%ymm4,%ymm21\n\tvpunpckldq\t%ymm7,%ymm6,%ymm22\n\tvpunpckhdq\t%ymm7,%ymm6,%ymm23\n\tvpunpckldq\t%ymm9,%ymm8,%ymm24\n\tvpunpckhdq\t%ymm9,%ymm8,%ymm25\n\tvpunpckldq\t%ymm11,%ymm10,%ymm26\n\tvpunpckhdq\t%ymm11,%ymm10,%ymm27\n\tvpunpckldq\t%ymm13,%ymm12,%ymm28\n\tvpunpckhdq\t%ymm13,%ymm12,%ymm29\n\tvpunpckldq\t%ymm15,%ymm14,%ymm30\n\tvpunpckhdq\t%ymm15,%ymm14,%ymm31\n\n\t# interleave 64-bit words in state n, n+2\n\tvpunpcklqdq\t%ymm18,%ymm16,%ymm0\n\tvpunpcklqdq\t%ymm19,%ymm17,%ymm1\n\tvpunpckhqdq\t%ymm18,%ymm16,%ymm2\n\tvpunpckhqdq\t%ymm19,%ymm17,%ymm3\n\tvpunpcklqdq\t%ymm22,%ymm20,%ymm4\n\tvpunpcklqdq\t%ymm23,%ymm21,%ymm5\n\tvpunpckhqdq\t%ymm22,%ymm20,%ymm6\n\tvpunpckhqdq\t%ymm23,%ymm21,%ymm7\n\tvpunpcklqdq\t%ymm26,%ymm24,%ymm8\n\tvpunpcklqdq\t%ymm27,%ymm25,%ymm9\n\tvpunpckhqdq\t%ymm26,%ymm24,%ymm10\n\tvpunpckhqdq\t%ymm27,%ymm25,%ymm11\n\tvpunpcklqdq\t%ymm30,%ymm28,%ymm12\n\tvpunpcklqdq\t%ymm31,%ymm29,%ymm13\n\tvpunpckhqdq\t%ymm30,%ymm28,%ymm14\n\tvpunpckhqdq\t%ymm31,%ymm29,%ymm15\n\n\t# interleave 128-bit words in state n, n+4\n\t# xor/write first four blocks\n\tvmovdqa64\t%ymm0,%ymm16\n\tvperm2i128\t$0x20,%ymm4,%ymm0,%ymm0\n\tcmp\t\t$0x0020,%rcx\n\tjl\t\t.Lxorpart8\n\tvpxord\t\t0x0000(%rdx),%ymm0,%ymm0\n\tvmovdqu64\t%ymm0,0x0000(%rsi)\n\tvmovdqa64\t%ymm16,%ymm0\n\tvperm2i128\t$0x31,%ymm4,%ymm0,%ymm4\n\n\tvperm2i128\t$0x20,%ymm12,%ymm8,%ymm0\n\tcmp\t\t$0x0040,%rcx\n\tjl\t\t.Lxorpart8\n\tvpxord\t\t0x0020(%rdx),%ymm0,%ymm0\n\tvmovdqu64\t%ymm0,0x0020(%rsi)\n\tvperm2i128\t$0x31,%ymm12,%ymm8,%ymm12\n\n\tvperm2i128\t$0x20,%ymm6,%ymm2,%ymm0\n\tcmp\t\t$0x0060,%rcx\n\tjl\t\t.Lxorpart8\n\tvpxord\t\t0x0040(%rdx),%ymm0,%ymm0\n\tvmovdqu64\t%ymm0,0x0040(%rsi)\n\tvperm2i128\t$0x31,%ymm6,%ymm2,%ymm6\n\n\tvperm2i128\t$0x20,%ymm14,%ymm10,%ymm0\n\tcmp\t\t$0x0080,%rcx\n\tjl\t\t.Lxorpart8\n\tvpxord\t\t0x0060(%rdx),%ymm0,%ymm0\n\tvmovdqu64\t%ymm0,0x0060(%rsi)\n\tvperm2i128\t$0x31,%ymm14,%ymm10,%ymm14\n\n\tvperm2i128\t$0x20,%ymm5,%ymm1,%ymm0\n\tcmp\t\t$0x00a0,%rcx\n\tjl\t\t.Lxorpart8\n\tvpxord\t\t0x0080(%rdx),%ymm0,%ymm0\n\tvmovdqu64\t%ymm0,0x0080(%rsi)\n\tvperm2i128\t$0x31,%ymm5,%ymm1,%ymm5\n\n\tvperm2i128\t$0x20,%ymm13,%ymm9,%ymm0\n\tcmp\t\t$0x00c0,%rcx\n\tjl\t\t.Lxorpart8\n\tvpxord\t\t0x00a0(%rdx),%ymm0,%ymm0\n\tvmovdqu64\t%ymm0,0x00a0(%rsi)\n\tvperm2i128\t$0x31,%ymm13,%ymm9,%ymm13\n\n\tvperm2i128\t$0x20,%ymm7,%ymm3,%ymm0\n\tcmp\t\t$0x00e0,%rcx\n\tjl\t\t.Lxorpart8\n\tvpxord\t\t0x00c0(%rdx),%ymm0,%ymm0\n\tvmovdqu64\t%ymm0,0x00c0(%rsi)\n\tvperm2i128\t$0x31,%ymm7,%ymm3,%ymm7\n\n\tvperm2i128\t$0x20,%ymm15,%ymm11,%ymm0\n\tcmp\t\t$0x0100,%rcx\n\tjl\t\t.Lxorpart8\n\tvpxord\t\t0x00e0(%rdx),%ymm0,%ymm0\n\tvmovdqu64\t%ymm0,0x00e0(%rsi)\n\tvperm2i128\t$0x31,%ymm15,%ymm11,%ymm15\n\n\t# xor remaining blocks, write to output\n\tvmovdqa64\t%ymm4,%ymm0\n\tcmp\t\t$0x0120,%rcx\n\tjl\t\t.Lxorpart8\n\tvpxord\t\t0x0100(%rdx),%ymm0,%ymm0\n\tvmovdqu64\t%ymm0,0x0100(%rsi)\n\n\tvmovdqa64\t%ymm12,%ymm0\n\tcmp\t\t$0x0140,%rcx\n\tjl\t\t.Lxorpart8\n\tvpxord\t\t0x0120(%rdx),%ymm0,%ymm0\n\tvmovdqu64\t%ymm0,0x0120(%rsi)\n\n\tvmovdqa64\t%ymm6,%ymm0\n\tcmp\t\t$0x0160,%rcx\n\tjl\t\t.Lxorpart8\n\tvpxord\t\t0x0140(%rdx),%ymm0,%ymm0\n\tvmovdqu64\t%ymm0,0x0140(%rsi)\n\n\tvmovdqa64\t%ymm14,%ymm0\n\tcmp\t\t$0x0180,%rcx\n\tjl\t\t.Lxorpart8\n\tvpxord\t\t0x0160(%rdx),%ymm0,%ymm0\n\tvmovdqu64\t%ymm0,0x0160(%rsi)\n\n\tvmovdqa64\t%ymm5,%ymm0\n\tcmp\t\t$0x01a0,%rcx\n\tjl\t\t.Lxorpart8\n\tvpxord\t\t0x0180(%rdx),%ymm0,%ymm0\n\tvmovdqu64\t%ymm0,0x0180(%rsi)\n\n\tvmovdqa64\t%ymm13,%ymm0\n\tcmp\t\t$0x01c0,%rcx\n\tjl\t\t.Lxorpart8\n\tvpxord\t\t0x01a0(%rdx),%ymm0,%ymm0\n\tvmovdqu64\t%ymm0,0x01a0(%rsi)\n\n\tvmovdqa64\t%ymm7,%ymm0\n\tcmp\t\t$0x01e0,%rcx\n\tjl\t\t.Lxorpart8\n\tvpxord\t\t0x01c0(%rdx),%ymm0,%ymm0\n\tvmovdqu64\t%ymm0,0x01c0(%rsi)\n\n\tvmovdqa64\t%ymm15,%ymm0\n\tcmp\t\t$0x0200,%rcx\n\tjl\t\t.Lxorpart8\n\tvpxord\t\t0x01e0(%rdx),%ymm0,%ymm0\n\tvmovdqu64\t%ymm0,0x01e0(%rsi)\n\n.Ldone8:\n\tvzeroupper\n\tRET\n\n.Lxorpart8:\n\t# xor remaining bytes from partial register into output\n\tmov\t\t%rcx,%rax\n\tand\t\t$0x1f,%rcx\n\tjz\t\t.Ldone8\n\tmov\t\t%rax,%r9\n\tand\t\t$~0x1f,%r9\n\n\tmov\t\t$1,%rax\n\tshld\t\t%cl,%rax,%rax\n\tsub\t\t$1,%rax\n\tkmovq\t\t%rax,%k1\n\n\tvmovdqu8\t(%rdx,%r9),%ymm1{%k1}{z}\n\tvpxord\t\t%ymm0,%ymm1,%ymm1\n\tvmovdqu8\t%ymm1,(%rsi,%r9){%k1}\n\n\tjmp\t\t.Ldone8\n\nSYM_FUNC_END(chacha_8block_xor_avx512vl)\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}