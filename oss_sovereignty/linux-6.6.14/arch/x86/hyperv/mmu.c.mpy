{
  "module_name": "mmu.c",
  "hash_id": "256425cc1b6495a8c438f3e06e44b44ed80ae491566c31972ca1460280a1f906",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/hyperv/mmu.c",
  "human_readable_source": "#define pr_fmt(fmt)  \"Hyper-V: \" fmt\n\n#include <linux/hyperv.h>\n#include <linux/log2.h>\n#include <linux/slab.h>\n#include <linux/types.h>\n\n#include <asm/fpu/api.h>\n#include <asm/mshyperv.h>\n#include <asm/msr.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n\n#define CREATE_TRACE_POINTS\n#include <asm/trace/hyperv.h>\n\n \n#define HV_TLB_FLUSH_UNIT (4096 * PAGE_SIZE)\n\nstatic u64 hyperv_flush_tlb_others_ex(const struct cpumask *cpus,\n\t\t\t\t      const struct flush_tlb_info *info);\n\n \nstatic inline int fill_gva_list(u64 gva_list[], int offset,\n\t\t\t\tunsigned long start, unsigned long end)\n{\n\tint gva_n = offset;\n\tunsigned long cur = start, diff;\n\n\tdo {\n\t\tdiff = end > cur ? end - cur : 0;\n\n\t\tgva_list[gva_n] = cur & PAGE_MASK;\n\t\t \n\t\tif (diff >= HV_TLB_FLUSH_UNIT) {\n\t\t\tgva_list[gva_n] |= ~PAGE_MASK;\n\t\t\tcur += HV_TLB_FLUSH_UNIT;\n\t\t}  else if (diff) {\n\t\t\tgva_list[gva_n] |= (diff - 1) >> PAGE_SHIFT;\n\t\t\tcur = end;\n\t\t}\n\n\t\tgva_n++;\n\n\t} while (cur < end);\n\n\treturn gva_n - offset;\n}\n\nstatic bool cpu_is_lazy(int cpu)\n{\n\treturn per_cpu(cpu_tlbstate_shared.is_lazy, cpu);\n}\n\nstatic void hyperv_flush_tlb_multi(const struct cpumask *cpus,\n\t\t\t\t   const struct flush_tlb_info *info)\n{\n\tint cpu, vcpu, gva_n, max_gvas;\n\tstruct hv_tlb_flush *flush;\n\tu64 status;\n\tunsigned long flags;\n\tbool do_lazy = !info->freed_tables;\n\n\ttrace_hyperv_mmu_flush_tlb_multi(cpus, info);\n\n\tif (!hv_hypercall_pg)\n\t\tgoto do_native;\n\n\tlocal_irq_save(flags);\n\n\tflush = *this_cpu_ptr(hyperv_pcpu_input_arg);\n\n\tif (unlikely(!flush)) {\n\t\tlocal_irq_restore(flags);\n\t\tgoto do_native;\n\t}\n\n\tif (info->mm) {\n\t\t \n\t\tflush->address_space = virt_to_phys(info->mm->pgd);\n\t\tflush->address_space &= CR3_ADDR_MASK;\n\t\tflush->flags = 0;\n\t} else {\n\t\tflush->address_space = 0;\n\t\tflush->flags = HV_FLUSH_ALL_VIRTUAL_ADDRESS_SPACES;\n\t}\n\n\tflush->processor_mask = 0;\n\tif (cpumask_equal(cpus, cpu_present_mask)) {\n\t\tflush->flags |= HV_FLUSH_ALL_PROCESSORS;\n\t} else {\n\t\t \n\t\tcpu = cpumask_last(cpus);\n\n\t\tif (cpu < nr_cpumask_bits && hv_cpu_number_to_vp_number(cpu) >= 64)\n\t\t\tgoto do_ex_hypercall;\n\n\t\tfor_each_cpu(cpu, cpus) {\n\t\t\tif (do_lazy && cpu_is_lazy(cpu))\n\t\t\t\tcontinue;\n\t\t\tvcpu = hv_cpu_number_to_vp_number(cpu);\n\t\t\tif (vcpu == VP_INVAL) {\n\t\t\t\tlocal_irq_restore(flags);\n\t\t\t\tgoto do_native;\n\t\t\t}\n\n\t\t\tif (vcpu >= 64)\n\t\t\t\tgoto do_ex_hypercall;\n\n\t\t\t__set_bit(vcpu, (unsigned long *)\n\t\t\t\t  &flush->processor_mask);\n\t\t}\n\n\t\t \n\t\tif (!flush->processor_mask) {\n\t\t\tlocal_irq_restore(flags);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t \n\tmax_gvas = (PAGE_SIZE - sizeof(*flush)) / sizeof(flush->gva_list[0]);\n\n\tif (info->end == TLB_FLUSH_ALL) {\n\t\tflush->flags |= HV_FLUSH_NON_GLOBAL_MAPPINGS_ONLY;\n\t\tstatus = hv_do_hypercall(HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE,\n\t\t\t\t\t flush, NULL);\n\t} else if (info->end &&\n\t\t   ((info->end - info->start)/HV_TLB_FLUSH_UNIT) > max_gvas) {\n\t\tstatus = hv_do_hypercall(HVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE,\n\t\t\t\t\t flush, NULL);\n\t} else {\n\t\tgva_n = fill_gva_list(flush->gva_list, 0,\n\t\t\t\t      info->start, info->end);\n\t\tstatus = hv_do_rep_hypercall(HVCALL_FLUSH_VIRTUAL_ADDRESS_LIST,\n\t\t\t\t\t     gva_n, 0, flush, NULL);\n\t}\n\tgoto check_status;\n\ndo_ex_hypercall:\n\tstatus = hyperv_flush_tlb_others_ex(cpus, info);\n\ncheck_status:\n\tlocal_irq_restore(flags);\n\n\tif (hv_result_success(status))\n\t\treturn;\ndo_native:\n\tnative_flush_tlb_multi(cpus, info);\n}\n\nstatic u64 hyperv_flush_tlb_others_ex(const struct cpumask *cpus,\n\t\t\t\t      const struct flush_tlb_info *info)\n{\n\tint nr_bank = 0, max_gvas, gva_n;\n\tstruct hv_tlb_flush_ex *flush;\n\tu64 status;\n\n\tif (!(ms_hyperv.hints & HV_X64_EX_PROCESSOR_MASKS_RECOMMENDED))\n\t\treturn HV_STATUS_INVALID_PARAMETER;\n\n\tflush = *this_cpu_ptr(hyperv_pcpu_input_arg);\n\n\tif (info->mm) {\n\t\t \n\t\tflush->address_space = virt_to_phys(info->mm->pgd);\n\t\tflush->address_space &= CR3_ADDR_MASK;\n\t\tflush->flags = 0;\n\t} else {\n\t\tflush->address_space = 0;\n\t\tflush->flags = HV_FLUSH_ALL_VIRTUAL_ADDRESS_SPACES;\n\t}\n\n\tflush->hv_vp_set.valid_bank_mask = 0;\n\n\tflush->hv_vp_set.format = HV_GENERIC_SET_SPARSE_4K;\n\tnr_bank = cpumask_to_vpset_skip(&flush->hv_vp_set, cpus,\n\t\t\tinfo->freed_tables ? NULL : cpu_is_lazy);\n\tif (nr_bank < 0)\n\t\treturn HV_STATUS_INVALID_PARAMETER;\n\n\t \n\tmax_gvas =\n\t\t(PAGE_SIZE - sizeof(*flush) - nr_bank *\n\t\t sizeof(flush->hv_vp_set.bank_contents[0])) /\n\t\tsizeof(flush->gva_list[0]);\n\n\tif (info->end == TLB_FLUSH_ALL) {\n\t\tflush->flags |= HV_FLUSH_NON_GLOBAL_MAPPINGS_ONLY;\n\t\tstatus = hv_do_rep_hypercall(\n\t\t\tHVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE_EX,\n\t\t\t0, nr_bank, flush, NULL);\n\t} else if (info->end &&\n\t\t   ((info->end - info->start)/HV_TLB_FLUSH_UNIT) > max_gvas) {\n\t\tstatus = hv_do_rep_hypercall(\n\t\t\tHVCALL_FLUSH_VIRTUAL_ADDRESS_SPACE_EX,\n\t\t\t0, nr_bank, flush, NULL);\n\t} else {\n\t\tgva_n = fill_gva_list(flush->gva_list, nr_bank,\n\t\t\t\t      info->start, info->end);\n\t\tstatus = hv_do_rep_hypercall(\n\t\t\tHVCALL_FLUSH_VIRTUAL_ADDRESS_LIST_EX,\n\t\t\tgva_n, nr_bank, flush, NULL);\n\t}\n\n\treturn status;\n}\n\nvoid hyperv_setup_mmu_ops(void)\n{\n\tif (!(ms_hyperv.hints & HV_X64_REMOTE_TLB_FLUSH_RECOMMENDED))\n\t\treturn;\n\n\tpr_info(\"Using hypercall for remote TLB flush\\n\");\n\tpv_ops.mmu.flush_tlb_multi = hyperv_flush_tlb_multi;\n\tpv_ops.mmu.tlb_remove_table = tlb_remove_table;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}