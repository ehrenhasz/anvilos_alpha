{
  "module_name": "setup.c",
  "hash_id": "69b2448426323f226c63533e78852810bfd02512164e21662eb86d71ac9166db",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/xen/setup.c",
  "human_readable_source": "\n \n\n#include <linux/init.h>\n#include <linux/iscsi_ibft.h>\n#include <linux/sched.h>\n#include <linux/kstrtox.h>\n#include <linux/mm.h>\n#include <linux/pm.h>\n#include <linux/memblock.h>\n#include <linux/cpuidle.h>\n#include <linux/cpufreq.h>\n#include <linux/memory_hotplug.h>\n\n#include <asm/elf.h>\n#include <asm/vdso.h>\n#include <asm/e820/api.h>\n#include <asm/setup.h>\n#include <asm/acpi.h>\n#include <asm/numa.h>\n#include <asm/idtentry.h>\n#include <asm/xen/hypervisor.h>\n#include <asm/xen/hypercall.h>\n\n#include <xen/xen.h>\n#include <xen/page.h>\n#include <xen/interface/callback.h>\n#include <xen/interface/memory.h>\n#include <xen/interface/physdev.h>\n#include <xen/features.h>\n#include <xen/hvc-console.h>\n#include \"xen-ops.h\"\n#include \"mmu.h\"\n\n#define GB(x) ((uint64_t)(x) * 1024 * 1024 * 1024)\n\n \nstruct xen_memory_region xen_extra_mem[XEN_EXTRA_MEM_MAX_REGIONS] __initdata;\n\n \nunsigned long xen_released_pages;\n\n \nbool xen_pv_pci_possible;\n\n \nstatic struct e820_table xen_e820_table __initdata;\n\n \n#define REMAP_SIZE\t(P2M_PER_PAGE - 3)\nstatic struct {\n\tunsigned long\tnext_area_mfn;\n\tunsigned long\ttarget_pfn;\n\tunsigned long\tsize;\n\tunsigned long\tmfns[REMAP_SIZE];\n} xen_remap_buf __initdata __aligned(PAGE_SIZE);\nstatic unsigned long xen_remap_mfn __initdata = INVALID_P2M_ENTRY;\n\n \n#define EXTRA_MEM_RATIO\t\t(10)\n\nstatic bool xen_512gb_limit __initdata = IS_ENABLED(CONFIG_XEN_512GB);\n\nstatic void __init xen_parse_512gb(void)\n{\n\tbool val = false;\n\tchar *arg;\n\n\targ = strstr(xen_start_info->cmd_line, \"xen_512gb_limit\");\n\tif (!arg)\n\t\treturn;\n\n\targ = strstr(xen_start_info->cmd_line, \"xen_512gb_limit=\");\n\tif (!arg)\n\t\tval = true;\n\telse if (kstrtobool(arg + strlen(\"xen_512gb_limit=\"), &val))\n\t\treturn;\n\n\txen_512gb_limit = val;\n}\n\nstatic void __init xen_add_extra_mem(unsigned long start_pfn,\n\t\t\t\t     unsigned long n_pfns)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++) {\n\t\t \n\t\tif (xen_extra_mem[i].n_pfns == 0) {\n\t\t\txen_extra_mem[i].start_pfn = start_pfn;\n\t\t\txen_extra_mem[i].n_pfns = n_pfns;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (xen_extra_mem[i].start_pfn + xen_extra_mem[i].n_pfns ==\n\t\t    start_pfn) {\n\t\t\txen_extra_mem[i].n_pfns += n_pfns;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (i == XEN_EXTRA_MEM_MAX_REGIONS)\n\t\tprintk(KERN_WARNING \"Warning: not enough extra memory regions\\n\");\n\n\tmemblock_reserve(PFN_PHYS(start_pfn), PFN_PHYS(n_pfns));\n}\n\nstatic void __init xen_del_extra_mem(unsigned long start_pfn,\n\t\t\t\t     unsigned long n_pfns)\n{\n\tint i;\n\tunsigned long start_r, size_r;\n\n\tfor (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++) {\n\t\tstart_r = xen_extra_mem[i].start_pfn;\n\t\tsize_r = xen_extra_mem[i].n_pfns;\n\n\t\t \n\t\tif (start_r == start_pfn) {\n\t\t\tBUG_ON(n_pfns > size_r);\n\t\t\txen_extra_mem[i].start_pfn += n_pfns;\n\t\t\txen_extra_mem[i].n_pfns -= n_pfns;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (start_r + size_r == start_pfn + n_pfns) {\n\t\t\tBUG_ON(n_pfns > size_r);\n\t\t\txen_extra_mem[i].n_pfns -= n_pfns;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (start_pfn > start_r && start_pfn < start_r + size_r) {\n\t\t\tBUG_ON(start_pfn + n_pfns > start_r + size_r);\n\t\t\txen_extra_mem[i].n_pfns = start_pfn - start_r;\n\t\t\t \n\t\t\txen_add_extra_mem(start_pfn + n_pfns, start_r + size_r -\n\t\t\t\t\t  (start_pfn + n_pfns));\n\t\t\tbreak;\n\t\t}\n\t}\n\tmemblock_phys_free(PFN_PHYS(start_pfn), PFN_PHYS(n_pfns));\n}\n\n \nunsigned long __ref xen_chk_extra_mem(unsigned long pfn)\n{\n\tint i;\n\n\tfor (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++) {\n\t\tif (pfn >= xen_extra_mem[i].start_pfn &&\n\t\t    pfn < xen_extra_mem[i].start_pfn + xen_extra_mem[i].n_pfns)\n\t\t\treturn INVALID_P2M_ENTRY;\n\t}\n\n\treturn IDENTITY_FRAME(pfn);\n}\n\n \nvoid __init xen_inv_extra_mem(void)\n{\n\tunsigned long pfn, pfn_s, pfn_e;\n\tint i;\n\n\tfor (i = 0; i < XEN_EXTRA_MEM_MAX_REGIONS; i++) {\n\t\tif (!xen_extra_mem[i].n_pfns)\n\t\t\tcontinue;\n\t\tpfn_s = xen_extra_mem[i].start_pfn;\n\t\tpfn_e = pfn_s + xen_extra_mem[i].n_pfns;\n\t\tfor (pfn = pfn_s; pfn < pfn_e; pfn++)\n\t\t\tset_phys_to_machine(pfn, INVALID_P2M_ENTRY);\n\t}\n}\n\n \nstatic unsigned long __init xen_find_pfn_range(unsigned long *min_pfn)\n{\n\tconst struct e820_entry *entry = xen_e820_table.entries;\n\tunsigned int i;\n\tunsigned long done = 0;\n\n\tfor (i = 0; i < xen_e820_table.nr_entries; i++, entry++) {\n\t\tunsigned long s_pfn;\n\t\tunsigned long e_pfn;\n\n\t\tif (entry->type != E820_TYPE_RAM)\n\t\t\tcontinue;\n\n\t\te_pfn = PFN_DOWN(entry->addr + entry->size);\n\n\t\t \n\t\tif (e_pfn <= *min_pfn)\n\t\t\tcontinue;\n\n\t\ts_pfn = PFN_UP(entry->addr);\n\n\t\t \n\t\tif (s_pfn <= *min_pfn) {\n\t\t\tdone = e_pfn - *min_pfn;\n\t\t} else {\n\t\t\tdone = e_pfn - s_pfn;\n\t\t\t*min_pfn = s_pfn;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn done;\n}\n\nstatic int __init xen_free_mfn(unsigned long mfn)\n{\n\tstruct xen_memory_reservation reservation = {\n\t\t.address_bits = 0,\n\t\t.extent_order = 0,\n\t\t.domid        = DOMID_SELF\n\t};\n\n\tset_xen_guest_handle(reservation.extent_start, &mfn);\n\treservation.nr_extents = 1;\n\n\treturn HYPERVISOR_memory_op(XENMEM_decrease_reservation, &reservation);\n}\n\n \nstatic void __init xen_set_identity_and_release_chunk(unsigned long start_pfn,\n\t\t\tunsigned long end_pfn, unsigned long nr_pages)\n{\n\tunsigned long pfn, end;\n\tint ret;\n\n\tWARN_ON(start_pfn > end_pfn);\n\n\t \n\tend = min(end_pfn, nr_pages);\n\tfor (pfn = start_pfn; pfn < end; pfn++) {\n\t\tunsigned long mfn = pfn_to_mfn(pfn);\n\n\t\t \n\t\tif (mfn == INVALID_P2M_ENTRY || mfn_to_pfn(mfn) != pfn)\n\t\t\tcontinue;\n\n\t\tret = xen_free_mfn(mfn);\n\t\tWARN(ret != 1, \"Failed to release pfn %lx err=%d\\n\", pfn, ret);\n\n\t\tif (ret == 1) {\n\t\t\txen_released_pages++;\n\t\t\tif (!__set_phys_to_machine(pfn, INVALID_P2M_ENTRY))\n\t\t\t\tbreak;\n\t\t} else\n\t\t\tbreak;\n\t}\n\n\tset_phys_range_identity(start_pfn, end_pfn);\n}\n\n \nstatic void __init xen_update_mem_tables(unsigned long pfn, unsigned long mfn)\n{\n\tstruct mmu_update update = {\n\t\t.ptr = ((uint64_t)mfn << PAGE_SHIFT) | MMU_MACHPHYS_UPDATE,\n\t\t.val = pfn\n\t};\n\n\t \n\tif (!set_phys_to_machine(pfn, mfn)) {\n\t\tWARN(1, \"Failed to set p2m mapping for pfn=%ld mfn=%ld\\n\",\n\t\t     pfn, mfn);\n\t\tBUG();\n\t}\n\n\t \n\tif (HYPERVISOR_mmu_update(&update, 1, NULL, DOMID_SELF) < 0) {\n\t\tWARN(1, \"Failed to set m2p mapping for mfn=%ld pfn=%ld\\n\",\n\t\t     mfn, pfn);\n\t\tBUG();\n\t}\n\n\tif (HYPERVISOR_update_va_mapping((unsigned long)__va(pfn << PAGE_SHIFT),\n\t\t\t\t\t mfn_pte(mfn, PAGE_KERNEL), 0)) {\n\t\tWARN(1, \"Failed to update kernel mapping for mfn=%ld pfn=%ld\\n\",\n\t\t      mfn, pfn);\n\t\tBUG();\n\t}\n}\n\n \nstatic void __init xen_do_set_identity_and_remap_chunk(\n        unsigned long start_pfn, unsigned long size, unsigned long remap_pfn)\n{\n\tunsigned long buf = (unsigned long)&xen_remap_buf;\n\tunsigned long mfn_save, mfn;\n\tunsigned long ident_pfn_iter, remap_pfn_iter;\n\tunsigned long ident_end_pfn = start_pfn + size;\n\tunsigned long left = size;\n\tunsigned int i, chunk;\n\n\tWARN_ON(size == 0);\n\n\tmfn_save = virt_to_mfn((void *)buf);\n\n\tfor (ident_pfn_iter = start_pfn, remap_pfn_iter = remap_pfn;\n\t     ident_pfn_iter < ident_end_pfn;\n\t     ident_pfn_iter += REMAP_SIZE, remap_pfn_iter += REMAP_SIZE) {\n\t\tchunk = (left < REMAP_SIZE) ? left : REMAP_SIZE;\n\n\t\t \n\t\tmfn = pfn_to_mfn(ident_pfn_iter);\n\t\tset_pte_mfn(buf, mfn, PAGE_KERNEL);\n\n\t\t \n\t\txen_remap_buf.next_area_mfn = xen_remap_mfn;\n\t\txen_remap_buf.target_pfn = remap_pfn_iter;\n\t\txen_remap_buf.size = chunk;\n\t\tfor (i = 0; i < chunk; i++)\n\t\t\txen_remap_buf.mfns[i] = pfn_to_mfn(ident_pfn_iter + i);\n\n\t\t \n\t\txen_remap_mfn = mfn;\n\n\t\t \n\t\tset_phys_range_identity(ident_pfn_iter, ident_pfn_iter + chunk);\n\n\t\tleft -= chunk;\n\t}\n\n\t \n\tset_pte_mfn(buf, mfn_save, PAGE_KERNEL);\n}\n\n \nstatic unsigned long __init xen_set_identity_and_remap_chunk(\n\tunsigned long start_pfn, unsigned long end_pfn, unsigned long nr_pages,\n\tunsigned long remap_pfn)\n{\n\tunsigned long pfn;\n\tunsigned long i = 0;\n\tunsigned long n = end_pfn - start_pfn;\n\n\tif (remap_pfn == 0)\n\t\tremap_pfn = nr_pages;\n\n\twhile (i < n) {\n\t\tunsigned long cur_pfn = start_pfn + i;\n\t\tunsigned long left = n - i;\n\t\tunsigned long size = left;\n\t\tunsigned long remap_range_size;\n\n\t\t \n\t\tif (cur_pfn >= nr_pages) {\n\t\t\t \n\t\t\tset_phys_range_identity(cur_pfn, cur_pfn + size);\n\t\t\tbreak;\n\t\t}\n\t\tif (cur_pfn + size > nr_pages)\n\t\t\tsize = nr_pages - cur_pfn;\n\n\t\tremap_range_size = xen_find_pfn_range(&remap_pfn);\n\t\tif (!remap_range_size) {\n\t\t\tpr_warn(\"Unable to find available pfn range, not remapping identity pages\\n\");\n\t\t\txen_set_identity_and_release_chunk(cur_pfn,\n\t\t\t\t\t\tcur_pfn + left, nr_pages);\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (size > remap_range_size)\n\t\t\tsize = remap_range_size;\n\n\t\txen_do_set_identity_and_remap_chunk(cur_pfn, size, remap_pfn);\n\n\t\t \n\t\ti += size;\n\t\tremap_pfn += size;\n\t}\n\n\t \n\tfor (pfn = start_pfn; pfn <= max_pfn_mapped && pfn < end_pfn; pfn++)\n\t\t(void)HYPERVISOR_update_va_mapping(\n\t\t\t(unsigned long)__va(pfn << PAGE_SHIFT),\n\t\t\tnative_make_pte(0), 0);\n\n\treturn remap_pfn;\n}\n\nstatic unsigned long __init xen_count_remap_pages(\n\tunsigned long start_pfn, unsigned long end_pfn, unsigned long nr_pages,\n\tunsigned long remap_pages)\n{\n\tif (start_pfn >= nr_pages)\n\t\treturn remap_pages;\n\n\treturn remap_pages + min(end_pfn, nr_pages) - start_pfn;\n}\n\nstatic unsigned long __init xen_foreach_remap_area(unsigned long nr_pages,\n\tunsigned long (*func)(unsigned long start_pfn, unsigned long end_pfn,\n\t\t\t      unsigned long nr_pages, unsigned long last_val))\n{\n\tphys_addr_t start = 0;\n\tunsigned long ret_val = 0;\n\tconst struct e820_entry *entry = xen_e820_table.entries;\n\tint i;\n\n\t \n\tfor (i = 0; i < xen_e820_table.nr_entries; i++, entry++) {\n\t\tphys_addr_t end = entry->addr + entry->size;\n\t\tif (entry->type == E820_TYPE_RAM || i == xen_e820_table.nr_entries - 1) {\n\t\t\tunsigned long start_pfn = PFN_DOWN(start);\n\t\t\tunsigned long end_pfn = PFN_UP(end);\n\n\t\t\tif (entry->type == E820_TYPE_RAM)\n\t\t\t\tend_pfn = PFN_UP(entry->addr);\n\n\t\t\tif (start_pfn < end_pfn)\n\t\t\t\tret_val = func(start_pfn, end_pfn, nr_pages,\n\t\t\t\t\t       ret_val);\n\t\t\tstart = end;\n\t\t}\n\t}\n\n\treturn ret_val;\n}\n\n \nvoid __init xen_remap_memory(void)\n{\n\tunsigned long buf = (unsigned long)&xen_remap_buf;\n\tunsigned long mfn_save, pfn;\n\tunsigned long remapped = 0;\n\tunsigned int i;\n\tunsigned long pfn_s = ~0UL;\n\tunsigned long len = 0;\n\n\tmfn_save = virt_to_mfn((void *)buf);\n\n\twhile (xen_remap_mfn != INVALID_P2M_ENTRY) {\n\t\t \n\t\tset_pte_mfn(buf, xen_remap_mfn, PAGE_KERNEL);\n\n\t\tBUG_ON(xen_remap_mfn != xen_remap_buf.mfns[0]);\n\n\t\tpfn = xen_remap_buf.target_pfn;\n\t\tfor (i = 0; i < xen_remap_buf.size; i++) {\n\t\t\txen_update_mem_tables(pfn, xen_remap_buf.mfns[i]);\n\t\t\tremapped++;\n\t\t\tpfn++;\n\t\t}\n\t\tif (pfn_s == ~0UL || pfn == pfn_s) {\n\t\t\tpfn_s = xen_remap_buf.target_pfn;\n\t\t\tlen += xen_remap_buf.size;\n\t\t} else if (pfn_s + len == xen_remap_buf.target_pfn) {\n\t\t\tlen += xen_remap_buf.size;\n\t\t} else {\n\t\t\txen_del_extra_mem(pfn_s, len);\n\t\t\tpfn_s = xen_remap_buf.target_pfn;\n\t\t\tlen = xen_remap_buf.size;\n\t\t}\n\t\txen_remap_mfn = xen_remap_buf.next_area_mfn;\n\t}\n\n\tif (pfn_s != ~0UL && len)\n\t\txen_del_extra_mem(pfn_s, len);\n\n\tset_pte_mfn(buf, mfn_save, PAGE_KERNEL);\n\n\tpr_info(\"Remapped %ld page(s)\\n\", remapped);\n}\n\nstatic unsigned long __init xen_get_pages_limit(void)\n{\n\tunsigned long limit;\n\n\tlimit = MAXMEM / PAGE_SIZE;\n\tif (!xen_initial_domain() && xen_512gb_limit)\n\t\tlimit = GB(512) / PAGE_SIZE;\n\n\treturn limit;\n}\n\nstatic unsigned long __init xen_get_max_pages(void)\n{\n\tunsigned long max_pages, limit;\n\tdomid_t domid = DOMID_SELF;\n\tlong ret;\n\n\tlimit = xen_get_pages_limit();\n\tmax_pages = limit;\n\n\t \n\tif (xen_initial_domain()) {\n\t\tret = HYPERVISOR_memory_op(XENMEM_maximum_reservation, &domid);\n\t\tif (ret > 0)\n\t\t\tmax_pages = ret;\n\t}\n\n\treturn min(max_pages, limit);\n}\n\nstatic void __init xen_align_and_add_e820_region(phys_addr_t start,\n\t\t\t\t\t\t phys_addr_t size, int type)\n{\n\tphys_addr_t end = start + size;\n\n\t \n\tif (type == E820_TYPE_RAM) {\n\t\tstart = PAGE_ALIGN(start);\n\t\tend &= ~((phys_addr_t)PAGE_SIZE - 1);\n#ifdef CONFIG_MEMORY_HOTPLUG\n\t\t \n\t\tmax_mem_size = end;\n#endif\n\t}\n\n\te820__range_add(start, end - start, type);\n}\n\nstatic void __init xen_ignore_unusable(void)\n{\n\tstruct e820_entry *entry = xen_e820_table.entries;\n\tunsigned int i;\n\n\tfor (i = 0; i < xen_e820_table.nr_entries; i++, entry++) {\n\t\tif (entry->type == E820_TYPE_UNUSABLE)\n\t\t\tentry->type = E820_TYPE_RAM;\n\t}\n}\n\nbool __init xen_is_e820_reserved(phys_addr_t start, phys_addr_t size)\n{\n\tstruct e820_entry *entry;\n\tunsigned mapcnt;\n\tphys_addr_t end;\n\n\tif (!size)\n\t\treturn false;\n\n\tend = start + size;\n\tentry = xen_e820_table.entries;\n\n\tfor (mapcnt = 0; mapcnt < xen_e820_table.nr_entries; mapcnt++) {\n\t\tif (entry->type == E820_TYPE_RAM && entry->addr <= start &&\n\t\t    (entry->addr + entry->size) >= end)\n\t\t\treturn false;\n\n\t\tentry++;\n\t}\n\n\treturn true;\n}\n\n \nphys_addr_t __init xen_find_free_area(phys_addr_t size)\n{\n\tunsigned mapcnt;\n\tphys_addr_t addr, start;\n\tstruct e820_entry *entry = xen_e820_table.entries;\n\n\tfor (mapcnt = 0; mapcnt < xen_e820_table.nr_entries; mapcnt++, entry++) {\n\t\tif (entry->type != E820_TYPE_RAM || entry->size < size)\n\t\t\tcontinue;\n\t\tstart = entry->addr;\n\t\tfor (addr = start; addr < start + size; addr += PAGE_SIZE) {\n\t\t\tif (!memblock_is_reserved(addr))\n\t\t\t\tcontinue;\n\t\t\tstart = addr + PAGE_SIZE;\n\t\t\tif (start + size > entry->addr + entry->size)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (addr >= start + size) {\n\t\t\tmemblock_reserve(start, size);\n\t\t\treturn start;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic void __init xen_phys_memcpy(phys_addr_t dest, phys_addr_t src,\n\t\t\t\t   phys_addr_t n)\n{\n\tphys_addr_t dest_off, src_off, dest_len, src_len, len;\n\tvoid *from, *to;\n\n\twhile (n) {\n\t\tdest_off = dest & ~PAGE_MASK;\n\t\tsrc_off = src & ~PAGE_MASK;\n\t\tdest_len = n;\n\t\tif (dest_len > (NR_FIX_BTMAPS << PAGE_SHIFT) - dest_off)\n\t\t\tdest_len = (NR_FIX_BTMAPS << PAGE_SHIFT) - dest_off;\n\t\tsrc_len = n;\n\t\tif (src_len > (NR_FIX_BTMAPS << PAGE_SHIFT) - src_off)\n\t\t\tsrc_len = (NR_FIX_BTMAPS << PAGE_SHIFT) - src_off;\n\t\tlen = min(dest_len, src_len);\n\t\tto = early_memremap(dest - dest_off, dest_len + dest_off);\n\t\tfrom = early_memremap(src - src_off, src_len + src_off);\n\t\tmemcpy(to, from, len);\n\t\tearly_memunmap(to, dest_len + dest_off);\n\t\tearly_memunmap(from, src_len + src_off);\n\t\tn -= len;\n\t\tdest += len;\n\t\tsrc += len;\n\t}\n}\n\n \nstatic void __init xen_reserve_xen_mfnlist(void)\n{\n\tphys_addr_t start, size;\n\n\tif (xen_start_info->mfn_list >= __START_KERNEL_map) {\n\t\tstart = __pa(xen_start_info->mfn_list);\n\t\tsize = PFN_ALIGN(xen_start_info->nr_pages *\n\t\t\t\t sizeof(unsigned long));\n\t} else {\n\t\tstart = PFN_PHYS(xen_start_info->first_p2m_pfn);\n\t\tsize = PFN_PHYS(xen_start_info->nr_p2m_frames);\n\t}\n\n\tmemblock_reserve(start, size);\n\tif (!xen_is_e820_reserved(start, size))\n\t\treturn;\n\n\txen_relocate_p2m();\n\tmemblock_phys_free(start, size);\n}\n\n \nchar * __init xen_memory_setup(void)\n{\n\tunsigned long max_pfn, pfn_s, n_pfns;\n\tphys_addr_t mem_end, addr, size, chunk_size;\n\tu32 type;\n\tint rc;\n\tstruct xen_memory_map memmap;\n\tunsigned long max_pages;\n\tunsigned long extra_pages = 0;\n\tint i;\n\tint op;\n\n\txen_parse_512gb();\n\tmax_pfn = xen_get_pages_limit();\n\tmax_pfn = min(max_pfn, xen_start_info->nr_pages);\n\tmem_end = PFN_PHYS(max_pfn);\n\n\tmemmap.nr_entries = ARRAY_SIZE(xen_e820_table.entries);\n\tset_xen_guest_handle(memmap.buffer, xen_e820_table.entries);\n\n#if defined(CONFIG_MEMORY_HOTPLUG) && defined(CONFIG_XEN_BALLOON)\n\txen_saved_max_mem_size = max_mem_size;\n#endif\n\n\top = xen_initial_domain() ?\n\t\tXENMEM_machine_memory_map :\n\t\tXENMEM_memory_map;\n\trc = HYPERVISOR_memory_op(op, &memmap);\n\tif (rc == -ENOSYS) {\n\t\tBUG_ON(xen_initial_domain());\n\t\tmemmap.nr_entries = 1;\n\t\txen_e820_table.entries[0].addr = 0ULL;\n\t\txen_e820_table.entries[0].size = mem_end;\n\t\t \n\t\txen_e820_table.entries[0].size += 8ULL << 20;\n\t\txen_e820_table.entries[0].type = E820_TYPE_RAM;\n\t\trc = 0;\n\t}\n\tBUG_ON(rc);\n\tBUG_ON(memmap.nr_entries == 0);\n\txen_e820_table.nr_entries = memmap.nr_entries;\n\n\tif (xen_initial_domain()) {\n\t\t \n\t\txen_ignore_unusable();\n\n#ifdef CONFIG_ISCSI_IBFT_FIND\n\t\t \n\t\txen_e820_table.entries[xen_e820_table.nr_entries].addr = IBFT_START;\n\t\txen_e820_table.entries[xen_e820_table.nr_entries].size = IBFT_END - IBFT_START;\n\t\txen_e820_table.entries[xen_e820_table.nr_entries].type = E820_TYPE_RESERVED;\n\t\txen_e820_table.nr_entries++;\n#endif\n\t}\n\n\t \n\te820__update_table(&xen_e820_table);\n\n\tmax_pages = xen_get_max_pages();\n\n\t \n\tmax_pages += xen_foreach_remap_area(max_pfn, xen_count_remap_pages);\n\n\tif (max_pages > max_pfn)\n\t\textra_pages += max_pages - max_pfn;\n\n\t \n\textra_pages = min3(EXTRA_MEM_RATIO * min(max_pfn, PFN_DOWN(MAXMEM)),\n\t\t\t   extra_pages, max_pages - max_pfn);\n\ti = 0;\n\taddr = xen_e820_table.entries[0].addr;\n\tsize = xen_e820_table.entries[0].size;\n\twhile (i < xen_e820_table.nr_entries) {\n\t\tbool discard = false;\n\n\t\tchunk_size = size;\n\t\ttype = xen_e820_table.entries[i].type;\n\n\t\tif (type == E820_TYPE_RESERVED)\n\t\t\txen_pv_pci_possible = true;\n\n\t\tif (type == E820_TYPE_RAM) {\n\t\t\tif (addr < mem_end) {\n\t\t\t\tchunk_size = min(size, mem_end - addr);\n\t\t\t} else if (extra_pages) {\n\t\t\t\tchunk_size = min(size, PFN_PHYS(extra_pages));\n\t\t\t\tpfn_s = PFN_UP(addr);\n\t\t\t\tn_pfns = PFN_DOWN(addr + chunk_size) - pfn_s;\n\t\t\t\textra_pages -= n_pfns;\n\t\t\t\txen_add_extra_mem(pfn_s, n_pfns);\n\t\t\t\txen_max_p2m_pfn = pfn_s + n_pfns;\n\t\t\t} else\n\t\t\t\tdiscard = true;\n\t\t}\n\n\t\tif (!discard)\n\t\t\txen_align_and_add_e820_region(addr, chunk_size, type);\n\n\t\taddr += chunk_size;\n\t\tsize -= chunk_size;\n\t\tif (size == 0) {\n\t\t\ti++;\n\t\t\tif (i < xen_e820_table.nr_entries) {\n\t\t\t\taddr = xen_e820_table.entries[i].addr;\n\t\t\t\tsize = xen_e820_table.entries[i].size;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tset_phys_range_identity(addr / PAGE_SIZE, ~0ul);\n\n\t \n\te820__range_add(ISA_START_ADDRESS, ISA_END_ADDRESS - ISA_START_ADDRESS, E820_TYPE_RESERVED);\n\n\te820__update_table(e820_table);\n\n\t \n\tif (xen_is_e820_reserved(__pa_symbol(_text),\n\t\t\t__pa_symbol(__bss_stop) - __pa_symbol(_text))) {\n\t\txen_raw_console_write(\"Xen hypervisor allocated kernel memory conflicts with E820 map\\n\");\n\t\tBUG();\n\t}\n\n\t \n\txen_pt_check_e820();\n\n\txen_reserve_xen_mfnlist();\n\n\t \n\tif (xen_is_e820_reserved(boot_params.hdr.ramdisk_image,\n\t\t\t\t boot_params.hdr.ramdisk_size)) {\n\t\tphys_addr_t new_area, start, size;\n\n\t\tnew_area = xen_find_free_area(boot_params.hdr.ramdisk_size);\n\t\tif (!new_area) {\n\t\t\txen_raw_console_write(\"Can't find new memory area for initrd needed due to E820 map conflict\\n\");\n\t\t\tBUG();\n\t\t}\n\n\t\tstart = boot_params.hdr.ramdisk_image;\n\t\tsize = boot_params.hdr.ramdisk_size;\n\t\txen_phys_memcpy(new_area, start, size);\n\t\tpr_info(\"initrd moved from [mem %#010llx-%#010llx] to [mem %#010llx-%#010llx]\\n\",\n\t\t\tstart, start + size, new_area, new_area + size);\n\t\tmemblock_phys_free(start, size);\n\t\tboot_params.hdr.ramdisk_image = new_area;\n\t\tboot_params.ext_ramdisk_image = new_area >> 32;\n\t}\n\n\t \n\txen_foreach_remap_area(max_pfn, xen_set_identity_and_remap_chunk);\n\n\tpr_info(\"Released %ld page(s)\\n\", xen_released_pages);\n\n\treturn \"Xen\";\n}\n\nstatic int register_callback(unsigned type, const void *func)\n{\n\tstruct callback_register callback = {\n\t\t.type = type,\n\t\t.address = XEN_CALLBACK(__KERNEL_CS, func),\n\t\t.flags = CALLBACKF_mask_events,\n\t};\n\n\treturn HYPERVISOR_callback_op(CALLBACKOP_register, &callback);\n}\n\nvoid xen_enable_sysenter(void)\n{\n\tif (cpu_feature_enabled(X86_FEATURE_SYSENTER32) &&\n\t    register_callback(CALLBACKTYPE_sysenter, xen_entry_SYSENTER_compat))\n\t\tsetup_clear_cpu_cap(X86_FEATURE_SYSENTER32);\n}\n\nvoid xen_enable_syscall(void)\n{\n\tint ret;\n\n\tret = register_callback(CALLBACKTYPE_syscall, xen_entry_SYSCALL_64);\n\tif (ret != 0) {\n\t\tprintk(KERN_ERR \"Failed to set syscall callback: %d\\n\", ret);\n\t\t \n\t}\n\n\tif (cpu_feature_enabled(X86_FEATURE_SYSCALL32) &&\n\t    register_callback(CALLBACKTYPE_syscall32, xen_entry_SYSCALL_compat))\n\t\tsetup_clear_cpu_cap(X86_FEATURE_SYSCALL32);\n}\n\nstatic void __init xen_pvmmu_arch_setup(void)\n{\n\tHYPERVISOR_vm_assist(VMASST_CMD_enable, VMASST_TYPE_writable_pagetables);\n\n\tif (register_callback(CALLBACKTYPE_event,\n\t\t\t      xen_asm_exc_xen_hypervisor_callback) ||\n\t    register_callback(CALLBACKTYPE_failsafe, xen_failsafe_callback))\n\t\tBUG();\n\n\txen_enable_sysenter();\n\txen_enable_syscall();\n}\n\n \nvoid __init xen_arch_setup(void)\n{\n\txen_panic_handler_init();\n\txen_pvmmu_arch_setup();\n\n#ifdef CONFIG_ACPI\n\tif (!(xen_start_info->flags & SIF_INITDOMAIN)) {\n\t\tprintk(KERN_INFO \"ACPI in unprivileged domain disabled\\n\");\n\t\tdisable_acpi();\n\t}\n#endif\n\n\tmemcpy(boot_command_line, xen_start_info->cmd_line,\n\t       MAX_GUEST_CMDLINE > COMMAND_LINE_SIZE ?\n\t       COMMAND_LINE_SIZE : MAX_GUEST_CMDLINE);\n\n\t \n\tdisable_cpuidle();\n\tdisable_cpufreq();\n\tWARN_ON(xen_set_default_idle());\n#ifdef CONFIG_NUMA\n\tnuma_off = 1;\n#endif\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}