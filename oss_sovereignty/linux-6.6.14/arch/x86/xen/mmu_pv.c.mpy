{
  "module_name": "mmu_pv.c",
  "hash_id": "e215f49cc0ccec5828d481aa88ddef7ec02294bb6a7650528ba61d6b351816f5",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/xen/mmu_pv.c",
  "human_readable_source": "\n\n \n#include <linux/sched/mm.h>\n#include <linux/debugfs.h>\n#include <linux/bug.h>\n#include <linux/vmalloc.h>\n#include <linux/export.h>\n#include <linux/init.h>\n#include <linux/gfp.h>\n#include <linux/memblock.h>\n#include <linux/seq_file.h>\n#include <linux/crash_dump.h>\n#include <linux/pgtable.h>\n#ifdef CONFIG_KEXEC_CORE\n#include <linux/kexec.h>\n#endif\n\n#include <trace/events/xen.h>\n\n#include <asm/tlbflush.h>\n#include <asm/fixmap.h>\n#include <asm/mmu_context.h>\n#include <asm/setup.h>\n#include <asm/paravirt.h>\n#include <asm/e820/api.h>\n#include <asm/linkage.h>\n#include <asm/page.h>\n#include <asm/init.h>\n#include <asm/memtype.h>\n#include <asm/smp.h>\n#include <asm/tlb.h>\n\n#include <asm/xen/hypercall.h>\n#include <asm/xen/hypervisor.h>\n\n#include <xen/xen.h>\n#include <xen/page.h>\n#include <xen/interface/xen.h>\n#include <xen/interface/hvm/hvm_op.h>\n#include <xen/interface/version.h>\n#include <xen/interface/memory.h>\n#include <xen/hvc-console.h>\n#include <xen/swiotlb-xen.h>\n\n#include \"multicalls.h\"\n#include \"mmu.h\"\n#include \"debugfs.h\"\n\n \npteval_t xen_pte_val(pte_t pte);\npgdval_t xen_pgd_val(pgd_t pgd);\npmdval_t xen_pmd_val(pmd_t pmd);\npudval_t xen_pud_val(pud_t pud);\np4dval_t xen_p4d_val(p4d_t p4d);\npte_t xen_make_pte(pteval_t pte);\npgd_t xen_make_pgd(pgdval_t pgd);\npmd_t xen_make_pmd(pmdval_t pmd);\npud_t xen_make_pud(pudval_t pud);\np4d_t xen_make_p4d(p4dval_t p4d);\npte_t xen_make_pte_init(pteval_t pte);\n\n#ifdef CONFIG_X86_VSYSCALL_EMULATION\n \nstatic pud_t level3_user_vsyscall[PTRS_PER_PUD] __page_aligned_bss;\n#endif\n\n \nstatic DEFINE_SPINLOCK(xen_reservation_lock);\n\n \nDEFINE_PER_CPU(unsigned long, xen_cr3);\t  \nDEFINE_PER_CPU(unsigned long, xen_current_cr3);\t  \n\nstatic phys_addr_t xen_pt_base, xen_pt_size __initdata;\n\nstatic DEFINE_STATIC_KEY_FALSE(xen_struct_pages_ready);\n\n \n#define USER_LIMIT\t((STACK_TOP_MAX + PGDIR_SIZE - 1) & PGDIR_MASK)\n\nvoid make_lowmem_page_readonly(void *vaddr)\n{\n\tpte_t *pte, ptev;\n\tunsigned long address = (unsigned long)vaddr;\n\tunsigned int level;\n\n\tpte = lookup_address(address, &level);\n\tif (pte == NULL)\n\t\treturn;\t\t \n\n\tptev = pte_wrprotect(*pte);\n\n\tif (HYPERVISOR_update_va_mapping(address, ptev, 0))\n\t\tBUG();\n}\n\nvoid make_lowmem_page_readwrite(void *vaddr)\n{\n\tpte_t *pte, ptev;\n\tunsigned long address = (unsigned long)vaddr;\n\tunsigned int level;\n\n\tpte = lookup_address(address, &level);\n\tif (pte == NULL)\n\t\treturn;\t\t \n\n\tptev = pte_mkwrite_novma(*pte);\n\n\tif (HYPERVISOR_update_va_mapping(address, ptev, 0))\n\t\tBUG();\n}\n\n\n \nstatic bool xen_page_pinned(void *ptr)\n{\n\tif (static_branch_likely(&xen_struct_pages_ready)) {\n\t\tstruct page *page = virt_to_page(ptr);\n\n\t\treturn PagePinned(page);\n\t}\n\treturn true;\n}\n\nstatic void xen_extend_mmu_update(const struct mmu_update *update)\n{\n\tstruct multicall_space mcs;\n\tstruct mmu_update *u;\n\n\tmcs = xen_mc_extend_args(__HYPERVISOR_mmu_update, sizeof(*u));\n\n\tif (mcs.mc != NULL) {\n\t\tmcs.mc->args[1]++;\n\t} else {\n\t\tmcs = __xen_mc_entry(sizeof(*u));\n\t\tMULTI_mmu_update(mcs.mc, mcs.args, 1, NULL, DOMID_SELF);\n\t}\n\n\tu = mcs.args;\n\t*u = *update;\n}\n\nstatic void xen_extend_mmuext_op(const struct mmuext_op *op)\n{\n\tstruct multicall_space mcs;\n\tstruct mmuext_op *u;\n\n\tmcs = xen_mc_extend_args(__HYPERVISOR_mmuext_op, sizeof(*u));\n\n\tif (mcs.mc != NULL) {\n\t\tmcs.mc->args[1]++;\n\t} else {\n\t\tmcs = __xen_mc_entry(sizeof(*u));\n\t\tMULTI_mmuext_op(mcs.mc, mcs.args, 1, NULL, DOMID_SELF);\n\t}\n\n\tu = mcs.args;\n\t*u = *op;\n}\n\nstatic void xen_set_pmd_hyper(pmd_t *ptr, pmd_t val)\n{\n\tstruct mmu_update u;\n\n\tpreempt_disable();\n\n\txen_mc_batch();\n\n\t \n\tu.ptr = arbitrary_virt_to_machine(ptr).maddr;\n\tu.val = pmd_val_ma(val);\n\txen_extend_mmu_update(&u);\n\n\txen_mc_issue(XEN_LAZY_MMU);\n\n\tpreempt_enable();\n}\n\nstatic void xen_set_pmd(pmd_t *ptr, pmd_t val)\n{\n\ttrace_xen_mmu_set_pmd(ptr, val);\n\n\t \n\tif (!xen_page_pinned(ptr)) {\n\t\t*ptr = val;\n\t\treturn;\n\t}\n\n\txen_set_pmd_hyper(ptr, val);\n}\n\n \nvoid __init set_pte_mfn(unsigned long vaddr, unsigned long mfn, pgprot_t flags)\n{\n\tif (HYPERVISOR_update_va_mapping(vaddr, mfn_pte(mfn, flags),\n\t\t\t\t\t UVMF_INVLPG))\n\t\tBUG();\n}\n\nstatic bool xen_batched_set_pte(pte_t *ptep, pte_t pteval)\n{\n\tstruct mmu_update u;\n\n\tif (xen_get_lazy_mode() != XEN_LAZY_MMU)\n\t\treturn false;\n\n\txen_mc_batch();\n\n\tu.ptr = virt_to_machine(ptep).maddr | MMU_NORMAL_PT_UPDATE;\n\tu.val = pte_val_ma(pteval);\n\txen_extend_mmu_update(&u);\n\n\txen_mc_issue(XEN_LAZY_MMU);\n\n\treturn true;\n}\n\nstatic inline void __xen_set_pte(pte_t *ptep, pte_t pteval)\n{\n\tif (!xen_batched_set_pte(ptep, pteval)) {\n\t\t \n\t\tstruct mmu_update u;\n\n\t\tu.ptr = virt_to_machine(ptep).maddr | MMU_NORMAL_PT_UPDATE;\n\t\tu.val = pte_val_ma(pteval);\n\t\tHYPERVISOR_mmu_update(&u, 1, NULL, DOMID_SELF);\n\t}\n}\n\nstatic void xen_set_pte(pte_t *ptep, pte_t pteval)\n{\n\ttrace_xen_mmu_set_pte(ptep, pteval);\n\t__xen_set_pte(ptep, pteval);\n}\n\npte_t xen_ptep_modify_prot_start(struct vm_area_struct *vma,\n\t\t\t\t unsigned long addr, pte_t *ptep)\n{\n\t \n\ttrace_xen_mmu_ptep_modify_prot_start(vma->vm_mm, addr, ptep, *ptep);\n\treturn *ptep;\n}\n\nvoid xen_ptep_modify_prot_commit(struct vm_area_struct *vma, unsigned long addr,\n\t\t\t\t pte_t *ptep, pte_t pte)\n{\n\tstruct mmu_update u;\n\n\ttrace_xen_mmu_ptep_modify_prot_commit(vma->vm_mm, addr, ptep, pte);\n\txen_mc_batch();\n\n\tu.ptr = virt_to_machine(ptep).maddr | MMU_PT_UPDATE_PRESERVE_AD;\n\tu.val = pte_val_ma(pte);\n\txen_extend_mmu_update(&u);\n\n\txen_mc_issue(XEN_LAZY_MMU);\n}\n\n \nstatic pteval_t pte_mfn_to_pfn(pteval_t val)\n{\n\tif (val & _PAGE_PRESENT) {\n\t\tunsigned long mfn = (val & XEN_PTE_MFN_MASK) >> PAGE_SHIFT;\n\t\tunsigned long pfn = mfn_to_pfn(mfn);\n\n\t\tpteval_t flags = val & PTE_FLAGS_MASK;\n\t\tif (unlikely(pfn == ~0))\n\t\t\tval = flags & ~_PAGE_PRESENT;\n\t\telse\n\t\t\tval = ((pteval_t)pfn << PAGE_SHIFT) | flags;\n\t}\n\n\treturn val;\n}\n\nstatic pteval_t pte_pfn_to_mfn(pteval_t val)\n{\n\tif (val & _PAGE_PRESENT) {\n\t\tunsigned long pfn = (val & PTE_PFN_MASK) >> PAGE_SHIFT;\n\t\tpteval_t flags = val & PTE_FLAGS_MASK;\n\t\tunsigned long mfn;\n\n\t\tmfn = __pfn_to_mfn(pfn);\n\n\t\t \n\t\tif (unlikely(mfn == INVALID_P2M_ENTRY)) {\n\t\t\tmfn = 0;\n\t\t\tflags = 0;\n\t\t} else\n\t\t\tmfn &= ~(FOREIGN_FRAME_BIT | IDENTITY_FRAME_BIT);\n\t\tval = ((pteval_t)mfn << PAGE_SHIFT) | flags;\n\t}\n\n\treturn val;\n}\n\n__visible pteval_t xen_pte_val(pte_t pte)\n{\n\tpteval_t pteval = pte.pte;\n\n\treturn pte_mfn_to_pfn(pteval);\n}\nPV_CALLEE_SAVE_REGS_THUNK(xen_pte_val);\n\n__visible pgdval_t xen_pgd_val(pgd_t pgd)\n{\n\treturn pte_mfn_to_pfn(pgd.pgd);\n}\nPV_CALLEE_SAVE_REGS_THUNK(xen_pgd_val);\n\n__visible pte_t xen_make_pte(pteval_t pte)\n{\n\tpte = pte_pfn_to_mfn(pte);\n\n\treturn native_make_pte(pte);\n}\nPV_CALLEE_SAVE_REGS_THUNK(xen_make_pte);\n\n__visible pgd_t xen_make_pgd(pgdval_t pgd)\n{\n\tpgd = pte_pfn_to_mfn(pgd);\n\treturn native_make_pgd(pgd);\n}\nPV_CALLEE_SAVE_REGS_THUNK(xen_make_pgd);\n\n__visible pmdval_t xen_pmd_val(pmd_t pmd)\n{\n\treturn pte_mfn_to_pfn(pmd.pmd);\n}\nPV_CALLEE_SAVE_REGS_THUNK(xen_pmd_val);\n\nstatic void xen_set_pud_hyper(pud_t *ptr, pud_t val)\n{\n\tstruct mmu_update u;\n\n\tpreempt_disable();\n\n\txen_mc_batch();\n\n\t \n\tu.ptr = arbitrary_virt_to_machine(ptr).maddr;\n\tu.val = pud_val_ma(val);\n\txen_extend_mmu_update(&u);\n\n\txen_mc_issue(XEN_LAZY_MMU);\n\n\tpreempt_enable();\n}\n\nstatic void xen_set_pud(pud_t *ptr, pud_t val)\n{\n\ttrace_xen_mmu_set_pud(ptr, val);\n\n\t \n\tif (!xen_page_pinned(ptr)) {\n\t\t*ptr = val;\n\t\treturn;\n\t}\n\n\txen_set_pud_hyper(ptr, val);\n}\n\n__visible pmd_t xen_make_pmd(pmdval_t pmd)\n{\n\tpmd = pte_pfn_to_mfn(pmd);\n\treturn native_make_pmd(pmd);\n}\nPV_CALLEE_SAVE_REGS_THUNK(xen_make_pmd);\n\n__visible pudval_t xen_pud_val(pud_t pud)\n{\n\treturn pte_mfn_to_pfn(pud.pud);\n}\nPV_CALLEE_SAVE_REGS_THUNK(xen_pud_val);\n\n__visible pud_t xen_make_pud(pudval_t pud)\n{\n\tpud = pte_pfn_to_mfn(pud);\n\n\treturn native_make_pud(pud);\n}\nPV_CALLEE_SAVE_REGS_THUNK(xen_make_pud);\n\nstatic pgd_t *xen_get_user_pgd(pgd_t *pgd)\n{\n\tpgd_t *pgd_page = (pgd_t *)(((unsigned long)pgd) & PAGE_MASK);\n\tunsigned offset = pgd - pgd_page;\n\tpgd_t *user_ptr = NULL;\n\n\tif (offset < pgd_index(USER_LIMIT)) {\n\t\tstruct page *page = virt_to_page(pgd_page);\n\t\tuser_ptr = (pgd_t *)page->private;\n\t\tif (user_ptr)\n\t\t\tuser_ptr += offset;\n\t}\n\n\treturn user_ptr;\n}\n\nstatic void __xen_set_p4d_hyper(p4d_t *ptr, p4d_t val)\n{\n\tstruct mmu_update u;\n\n\tu.ptr = virt_to_machine(ptr).maddr;\n\tu.val = p4d_val_ma(val);\n\txen_extend_mmu_update(&u);\n}\n\n \nstatic void __init xen_set_p4d_hyper(p4d_t *ptr, p4d_t val)\n{\n\tpreempt_disable();\n\n\txen_mc_batch();\n\n\t__xen_set_p4d_hyper(ptr, val);\n\n\txen_mc_issue(XEN_LAZY_MMU);\n\n\tpreempt_enable();\n}\n\nstatic void xen_set_p4d(p4d_t *ptr, p4d_t val)\n{\n\tpgd_t *user_ptr = xen_get_user_pgd((pgd_t *)ptr);\n\tpgd_t pgd_val;\n\n\ttrace_xen_mmu_set_p4d(ptr, (p4d_t *)user_ptr, val);\n\n\t \n\tif (!xen_page_pinned(ptr)) {\n\t\t*ptr = val;\n\t\tif (user_ptr) {\n\t\t\tWARN_ON(xen_page_pinned(user_ptr));\n\t\t\tpgd_val.pgd = p4d_val_ma(val);\n\t\t\t*user_ptr = pgd_val;\n\t\t}\n\t\treturn;\n\t}\n\n\t \n\txen_mc_batch();\n\n\t__xen_set_p4d_hyper(ptr, val);\n\tif (user_ptr)\n\t\t__xen_set_p4d_hyper((p4d_t *)user_ptr, val);\n\n\txen_mc_issue(XEN_LAZY_MMU);\n}\n\n#if CONFIG_PGTABLE_LEVELS >= 5\n__visible p4dval_t xen_p4d_val(p4d_t p4d)\n{\n\treturn pte_mfn_to_pfn(p4d.p4d);\n}\nPV_CALLEE_SAVE_REGS_THUNK(xen_p4d_val);\n\n__visible p4d_t xen_make_p4d(p4dval_t p4d)\n{\n\tp4d = pte_pfn_to_mfn(p4d);\n\n\treturn native_make_p4d(p4d);\n}\nPV_CALLEE_SAVE_REGS_THUNK(xen_make_p4d);\n#endif   \n\nstatic void xen_pmd_walk(struct mm_struct *mm, pmd_t *pmd,\n\t\t\t void (*func)(struct mm_struct *mm, struct page *,\n\t\t\t\t      enum pt_level),\n\t\t\t bool last, unsigned long limit)\n{\n\tint i, nr;\n\n\tnr = last ? pmd_index(limit) + 1 : PTRS_PER_PMD;\n\tfor (i = 0; i < nr; i++) {\n\t\tif (!pmd_none(pmd[i]))\n\t\t\t(*func)(mm, pmd_page(pmd[i]), PT_PTE);\n\t}\n}\n\nstatic void xen_pud_walk(struct mm_struct *mm, pud_t *pud,\n\t\t\t void (*func)(struct mm_struct *mm, struct page *,\n\t\t\t\t      enum pt_level),\n\t\t\t bool last, unsigned long limit)\n{\n\tint i, nr;\n\n\tnr = last ? pud_index(limit) + 1 : PTRS_PER_PUD;\n\tfor (i = 0; i < nr; i++) {\n\t\tpmd_t *pmd;\n\n\t\tif (pud_none(pud[i]))\n\t\t\tcontinue;\n\n\t\tpmd = pmd_offset(&pud[i], 0);\n\t\tif (PTRS_PER_PMD > 1)\n\t\t\t(*func)(mm, virt_to_page(pmd), PT_PMD);\n\t\txen_pmd_walk(mm, pmd, func, last && i == nr - 1, limit);\n\t}\n}\n\nstatic void xen_p4d_walk(struct mm_struct *mm, p4d_t *p4d,\n\t\t\t void (*func)(struct mm_struct *mm, struct page *,\n\t\t\t\t      enum pt_level),\n\t\t\t bool last, unsigned long limit)\n{\n\tpud_t *pud;\n\n\n\tif (p4d_none(*p4d))\n\t\treturn;\n\n\tpud = pud_offset(p4d, 0);\n\tif (PTRS_PER_PUD > 1)\n\t\t(*func)(mm, virt_to_page(pud), PT_PUD);\n\txen_pud_walk(mm, pud, func, last, limit);\n}\n\n \nstatic void __xen_pgd_walk(struct mm_struct *mm, pgd_t *pgd,\n\t\t\t   void (*func)(struct mm_struct *mm, struct page *,\n\t\t\t\t\tenum pt_level),\n\t\t\t   unsigned long limit)\n{\n\tint i, nr;\n\tunsigned hole_low = 0, hole_high = 0;\n\n\t \n\tlimit--;\n\tBUG_ON(limit >= FIXADDR_TOP);\n\n\t \n\thole_low = pgd_index(GUARD_HOLE_BASE_ADDR);\n\thole_high = pgd_index(GUARD_HOLE_END_ADDR);\n\n\tnr = pgd_index(limit) + 1;\n\tfor (i = 0; i < nr; i++) {\n\t\tp4d_t *p4d;\n\n\t\tif (i >= hole_low && i < hole_high)\n\t\t\tcontinue;\n\n\t\tif (pgd_none(pgd[i]))\n\t\t\tcontinue;\n\n\t\tp4d = p4d_offset(&pgd[i], 0);\n\t\txen_p4d_walk(mm, p4d, func, i == nr - 1, limit);\n\t}\n\n\t \n\t(*func)(mm, virt_to_page(pgd), PT_PGD);\n}\n\nstatic void xen_pgd_walk(struct mm_struct *mm,\n\t\t\t void (*func)(struct mm_struct *mm, struct page *,\n\t\t\t\t      enum pt_level),\n\t\t\t unsigned long limit)\n{\n\t__xen_pgd_walk(mm, mm->pgd, func, limit);\n}\n\n \nstatic spinlock_t *xen_pte_lock(struct page *page, struct mm_struct *mm)\n{\n\tspinlock_t *ptl = NULL;\n\n#if USE_SPLIT_PTE_PTLOCKS\n\tptl = ptlock_ptr(page_ptdesc(page));\n\tspin_lock_nest_lock(ptl, &mm->page_table_lock);\n#endif\n\n\treturn ptl;\n}\n\nstatic void xen_pte_unlock(void *v)\n{\n\tspinlock_t *ptl = v;\n\tspin_unlock(ptl);\n}\n\nstatic void xen_do_pin(unsigned level, unsigned long pfn)\n{\n\tstruct mmuext_op op;\n\n\top.cmd = level;\n\top.arg1.mfn = pfn_to_mfn(pfn);\n\n\txen_extend_mmuext_op(&op);\n}\n\nstatic void xen_pin_page(struct mm_struct *mm, struct page *page,\n\t\t\t enum pt_level level)\n{\n\tunsigned pgfl = TestSetPagePinned(page);\n\n\tif (!pgfl) {\n\t\tvoid *pt = lowmem_page_address(page);\n\t\tunsigned long pfn = page_to_pfn(page);\n\t\tstruct multicall_space mcs = __xen_mc_entry(0);\n\t\tspinlock_t *ptl;\n\n\t\t \n\t\tptl = NULL;\n\t\tif (level == PT_PTE)\n\t\t\tptl = xen_pte_lock(page, mm);\n\n\t\tMULTI_update_va_mapping(mcs.mc, (unsigned long)pt,\n\t\t\t\t\tpfn_pte(pfn, PAGE_KERNEL_RO),\n\t\t\t\t\tlevel == PT_PGD ? UVMF_TLB_FLUSH : 0);\n\n\t\tif (ptl) {\n\t\t\txen_do_pin(MMUEXT_PIN_L1_TABLE, pfn);\n\n\t\t\t \n\t\t\txen_mc_callback(xen_pte_unlock, ptl);\n\t\t}\n\t}\n}\n\n \nstatic void __xen_pgd_pin(struct mm_struct *mm, pgd_t *pgd)\n{\n\tpgd_t *user_pgd = xen_get_user_pgd(pgd);\n\n\ttrace_xen_mmu_pgd_pin(mm, pgd);\n\n\txen_mc_batch();\n\n\t__xen_pgd_walk(mm, pgd, xen_pin_page, USER_LIMIT);\n\n\txen_do_pin(MMUEXT_PIN_L4_TABLE, PFN_DOWN(__pa(pgd)));\n\n\tif (user_pgd) {\n\t\txen_pin_page(mm, virt_to_page(user_pgd), PT_PGD);\n\t\txen_do_pin(MMUEXT_PIN_L4_TABLE,\n\t\t\t   PFN_DOWN(__pa(user_pgd)));\n\t}\n\n\txen_mc_issue(0);\n}\n\nstatic void xen_pgd_pin(struct mm_struct *mm)\n{\n\t__xen_pgd_pin(mm, mm->pgd);\n}\n\n \nvoid xen_mm_pin_all(void)\n{\n\tstruct page *page;\n\n\tspin_lock(&pgd_lock);\n\n\tlist_for_each_entry(page, &pgd_list, lru) {\n\t\tif (!PagePinned(page)) {\n\t\t\t__xen_pgd_pin(&init_mm, (pgd_t *)page_address(page));\n\t\t\tSetPageSavePinned(page);\n\t\t}\n\t}\n\n\tspin_unlock(&pgd_lock);\n}\n\nstatic void __init xen_mark_pinned(struct mm_struct *mm, struct page *page,\n\t\t\t\t   enum pt_level level)\n{\n\tSetPagePinned(page);\n}\n\n \nstatic void __init xen_after_bootmem(void)\n{\n\tstatic_branch_enable(&xen_struct_pages_ready);\n#ifdef CONFIG_X86_VSYSCALL_EMULATION\n\tSetPagePinned(virt_to_page(level3_user_vsyscall));\n#endif\n\txen_pgd_walk(&init_mm, xen_mark_pinned, FIXADDR_TOP);\n}\n\nstatic void xen_unpin_page(struct mm_struct *mm, struct page *page,\n\t\t\t   enum pt_level level)\n{\n\tunsigned pgfl = TestClearPagePinned(page);\n\n\tif (pgfl) {\n\t\tvoid *pt = lowmem_page_address(page);\n\t\tunsigned long pfn = page_to_pfn(page);\n\t\tspinlock_t *ptl = NULL;\n\t\tstruct multicall_space mcs;\n\n\t\t \n\t\tif (level == PT_PTE) {\n\t\t\tptl = xen_pte_lock(page, mm);\n\n\t\t\tif (ptl)\n\t\t\t\txen_do_pin(MMUEXT_UNPIN_TABLE, pfn);\n\t\t}\n\n\t\tmcs = __xen_mc_entry(0);\n\n\t\tMULTI_update_va_mapping(mcs.mc, (unsigned long)pt,\n\t\t\t\t\tpfn_pte(pfn, PAGE_KERNEL),\n\t\t\t\t\tlevel == PT_PGD ? UVMF_TLB_FLUSH : 0);\n\n\t\tif (ptl) {\n\t\t\t \n\t\t\txen_mc_callback(xen_pte_unlock, ptl);\n\t\t}\n\t}\n}\n\n \nstatic void __xen_pgd_unpin(struct mm_struct *mm, pgd_t *pgd)\n{\n\tpgd_t *user_pgd = xen_get_user_pgd(pgd);\n\n\ttrace_xen_mmu_pgd_unpin(mm, pgd);\n\n\txen_mc_batch();\n\n\txen_do_pin(MMUEXT_UNPIN_TABLE, PFN_DOWN(__pa(pgd)));\n\n\tif (user_pgd) {\n\t\txen_do_pin(MMUEXT_UNPIN_TABLE,\n\t\t\t   PFN_DOWN(__pa(user_pgd)));\n\t\txen_unpin_page(mm, virt_to_page(user_pgd), PT_PGD);\n\t}\n\n\t__xen_pgd_walk(mm, pgd, xen_unpin_page, USER_LIMIT);\n\n\txen_mc_issue(0);\n}\n\nstatic void xen_pgd_unpin(struct mm_struct *mm)\n{\n\t__xen_pgd_unpin(mm, mm->pgd);\n}\n\n \nvoid xen_mm_unpin_all(void)\n{\n\tstruct page *page;\n\n\tspin_lock(&pgd_lock);\n\n\tlist_for_each_entry(page, &pgd_list, lru) {\n\t\tif (PageSavePinned(page)) {\n\t\t\tBUG_ON(!PagePinned(page));\n\t\t\t__xen_pgd_unpin(&init_mm, (pgd_t *)page_address(page));\n\t\t\tClearPageSavePinned(page);\n\t\t}\n\t}\n\n\tspin_unlock(&pgd_lock);\n}\n\nstatic void xen_enter_mmap(struct mm_struct *mm)\n{\n\tspin_lock(&mm->page_table_lock);\n\txen_pgd_pin(mm);\n\tspin_unlock(&mm->page_table_lock);\n}\n\nstatic void drop_mm_ref_this_cpu(void *info)\n{\n\tstruct mm_struct *mm = info;\n\n\tif (this_cpu_read(cpu_tlbstate.loaded_mm) == mm)\n\t\tleave_mm(smp_processor_id());\n\n\t \n\tif (this_cpu_read(xen_current_cr3) == __pa(mm->pgd))\n\t\txen_mc_flush();\n}\n\n#ifdef CONFIG_SMP\n \nstatic void xen_drop_mm_ref(struct mm_struct *mm)\n{\n\tcpumask_var_t mask;\n\tunsigned cpu;\n\n\tdrop_mm_ref_this_cpu(mm);\n\n\t \n\tif (!alloc_cpumask_var(&mask, GFP_ATOMIC)) {\n\t\tfor_each_online_cpu(cpu) {\n\t\t\tif (per_cpu(xen_current_cr3, cpu) != __pa(mm->pgd))\n\t\t\t\tcontinue;\n\t\t\tsmp_call_function_single(cpu, drop_mm_ref_this_cpu, mm, 1);\n\t\t}\n\t\treturn;\n\t}\n\n\t \n\tcpumask_clear(mask);\n\tfor_each_online_cpu(cpu) {\n\t\tif (per_cpu(xen_current_cr3, cpu) == __pa(mm->pgd))\n\t\t\tcpumask_set_cpu(cpu, mask);\n\t}\n\n\tsmp_call_function_many(mask, drop_mm_ref_this_cpu, mm, 1);\n\tfree_cpumask_var(mask);\n}\n#else\nstatic void xen_drop_mm_ref(struct mm_struct *mm)\n{\n\tdrop_mm_ref_this_cpu(mm);\n}\n#endif\n\n \nstatic void xen_exit_mmap(struct mm_struct *mm)\n{\n\tget_cpu();\t\t \n\txen_drop_mm_ref(mm);\n\tput_cpu();\n\n\tspin_lock(&mm->page_table_lock);\n\n\t \n\tif (xen_page_pinned(mm->pgd))\n\t\txen_pgd_unpin(mm);\n\n\tspin_unlock(&mm->page_table_lock);\n}\n\nstatic void xen_post_allocator_init(void);\n\nstatic void __init pin_pagetable_pfn(unsigned cmd, unsigned long pfn)\n{\n\tstruct mmuext_op op;\n\n\top.cmd = cmd;\n\top.arg1.mfn = pfn_to_mfn(pfn);\n\tif (HYPERVISOR_mmuext_op(&op, 1, NULL, DOMID_SELF))\n\t\tBUG();\n}\n\nstatic void __init xen_cleanhighmap(unsigned long vaddr,\n\t\t\t\t    unsigned long vaddr_end)\n{\n\tunsigned long kernel_end = roundup((unsigned long)_brk_end, PMD_SIZE) - 1;\n\tpmd_t *pmd = level2_kernel_pgt + pmd_index(vaddr);\n\n\t \n\tfor (; vaddr <= vaddr_end && (pmd < (level2_kernel_pgt + PTRS_PER_PMD));\n\t\t\tpmd++, vaddr += PMD_SIZE) {\n\t\tif (pmd_none(*pmd))\n\t\t\tcontinue;\n\t\tif (vaddr < (unsigned long) _text || vaddr > kernel_end)\n\t\t\tset_pmd(pmd, __pmd(0));\n\t}\n\t \n\txen_mc_flush();\n}\n\n \nstatic void __init xen_free_ro_pages(unsigned long paddr, unsigned long size)\n{\n\tvoid *vaddr = __va(paddr);\n\tvoid *vaddr_end = vaddr + size;\n\n\tfor (; vaddr < vaddr_end; vaddr += PAGE_SIZE)\n\t\tmake_lowmem_page_readwrite(vaddr);\n\n\tmemblock_phys_free(paddr, size);\n}\n\nstatic void __init xen_cleanmfnmap_free_pgtbl(void *pgtbl, bool unpin)\n{\n\tunsigned long pa = __pa(pgtbl) & PHYSICAL_PAGE_MASK;\n\n\tif (unpin)\n\t\tpin_pagetable_pfn(MMUEXT_UNPIN_TABLE, PFN_DOWN(pa));\n\tClearPagePinned(virt_to_page(__va(pa)));\n\txen_free_ro_pages(pa, PAGE_SIZE);\n}\n\nstatic void __init xen_cleanmfnmap_pmd(pmd_t *pmd, bool unpin)\n{\n\tunsigned long pa;\n\tpte_t *pte_tbl;\n\tint i;\n\n\tif (pmd_large(*pmd)) {\n\t\tpa = pmd_val(*pmd) & PHYSICAL_PAGE_MASK;\n\t\txen_free_ro_pages(pa, PMD_SIZE);\n\t\treturn;\n\t}\n\n\tpte_tbl = pte_offset_kernel(pmd, 0);\n\tfor (i = 0; i < PTRS_PER_PTE; i++) {\n\t\tif (pte_none(pte_tbl[i]))\n\t\t\tcontinue;\n\t\tpa = pte_pfn(pte_tbl[i]) << PAGE_SHIFT;\n\t\txen_free_ro_pages(pa, PAGE_SIZE);\n\t}\n\tset_pmd(pmd, __pmd(0));\n\txen_cleanmfnmap_free_pgtbl(pte_tbl, unpin);\n}\n\nstatic void __init xen_cleanmfnmap_pud(pud_t *pud, bool unpin)\n{\n\tunsigned long pa;\n\tpmd_t *pmd_tbl;\n\tint i;\n\n\tif (pud_large(*pud)) {\n\t\tpa = pud_val(*pud) & PHYSICAL_PAGE_MASK;\n\t\txen_free_ro_pages(pa, PUD_SIZE);\n\t\treturn;\n\t}\n\n\tpmd_tbl = pmd_offset(pud, 0);\n\tfor (i = 0; i < PTRS_PER_PMD; i++) {\n\t\tif (pmd_none(pmd_tbl[i]))\n\t\t\tcontinue;\n\t\txen_cleanmfnmap_pmd(pmd_tbl + i, unpin);\n\t}\n\tset_pud(pud, __pud(0));\n\txen_cleanmfnmap_free_pgtbl(pmd_tbl, unpin);\n}\n\nstatic void __init xen_cleanmfnmap_p4d(p4d_t *p4d, bool unpin)\n{\n\tunsigned long pa;\n\tpud_t *pud_tbl;\n\tint i;\n\n\tif (p4d_large(*p4d)) {\n\t\tpa = p4d_val(*p4d) & PHYSICAL_PAGE_MASK;\n\t\txen_free_ro_pages(pa, P4D_SIZE);\n\t\treturn;\n\t}\n\n\tpud_tbl = pud_offset(p4d, 0);\n\tfor (i = 0; i < PTRS_PER_PUD; i++) {\n\t\tif (pud_none(pud_tbl[i]))\n\t\t\tcontinue;\n\t\txen_cleanmfnmap_pud(pud_tbl + i, unpin);\n\t}\n\tset_p4d(p4d, __p4d(0));\n\txen_cleanmfnmap_free_pgtbl(pud_tbl, unpin);\n}\n\n \nstatic void __init xen_cleanmfnmap(unsigned long vaddr)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tbool unpin;\n\n\tunpin = (vaddr == 2 * PGDIR_SIZE);\n\tvaddr &= PMD_MASK;\n\tpgd = pgd_offset_k(vaddr);\n\tp4d = p4d_offset(pgd, 0);\n\tif (!p4d_none(*p4d))\n\t\txen_cleanmfnmap_p4d(p4d, unpin);\n}\n\nstatic void __init xen_pagetable_p2m_free(void)\n{\n\tunsigned long size;\n\tunsigned long addr;\n\n\tsize = PAGE_ALIGN(xen_start_info->nr_pages * sizeof(unsigned long));\n\n\t \n\tif ((unsigned long)xen_p2m_addr == xen_start_info->mfn_list)\n\t\treturn;\n\n\t \n\tmemset((void *)xen_start_info->mfn_list, 0xff, size);\n\n\taddr = xen_start_info->mfn_list;\n\t \n\tsize = roundup(size, PMD_SIZE);\n\n\tif (addr >= __START_KERNEL_map) {\n\t\txen_cleanhighmap(addr, addr + size);\n\t\tsize = PAGE_ALIGN(xen_start_info->nr_pages *\n\t\t\t\t  sizeof(unsigned long));\n\t\tmemblock_free((void *)addr, size);\n\t} else {\n\t\txen_cleanmfnmap(addr);\n\t}\n}\n\nstatic void __init xen_pagetable_cleanhighmap(void)\n{\n\tunsigned long size;\n\tunsigned long addr;\n\n\t \n\taddr = xen_start_info->pt_base;\n\tsize = xen_start_info->nr_pt_frames * PAGE_SIZE;\n\n\txen_cleanhighmap(addr, roundup(addr + size, PMD_SIZE * 2));\n\txen_start_info->pt_base = (unsigned long)__va(__pa(xen_start_info->pt_base));\n}\n\nstatic void __init xen_pagetable_p2m_setup(void)\n{\n\txen_vmalloc_p2m_tree();\n\n\txen_pagetable_p2m_free();\n\n\txen_pagetable_cleanhighmap();\n\n\t \n\txen_start_info->mfn_list = (unsigned long)xen_p2m_addr;\n}\n\nstatic void __init xen_pagetable_init(void)\n{\n\t \n\tpv_ops.mmu.set_pte = __xen_set_pte;\n\n\tpaging_init();\n\txen_post_allocator_init();\n\n\txen_pagetable_p2m_setup();\n\n\t \n\txen_build_mfn_list_list();\n\n\t \n\txen_remap_memory();\n\txen_setup_mfn_list_list();\n}\n\nstatic noinstr void xen_write_cr2(unsigned long cr2)\n{\n\tthis_cpu_read(xen_vcpu)->arch.cr2 = cr2;\n}\n\nstatic noinline void xen_flush_tlb(void)\n{\n\tstruct mmuext_op *op;\n\tstruct multicall_space mcs;\n\n\tpreempt_disable();\n\n\tmcs = xen_mc_entry(sizeof(*op));\n\n\top = mcs.args;\n\top->cmd = MMUEXT_TLB_FLUSH_LOCAL;\n\tMULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);\n\n\txen_mc_issue(XEN_LAZY_MMU);\n\n\tpreempt_enable();\n}\n\nstatic void xen_flush_tlb_one_user(unsigned long addr)\n{\n\tstruct mmuext_op *op;\n\tstruct multicall_space mcs;\n\n\ttrace_xen_mmu_flush_tlb_one_user(addr);\n\n\tpreempt_disable();\n\n\tmcs = xen_mc_entry(sizeof(*op));\n\top = mcs.args;\n\top->cmd = MMUEXT_INVLPG_LOCAL;\n\top->arg1.linear_addr = addr & PAGE_MASK;\n\tMULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);\n\n\txen_mc_issue(XEN_LAZY_MMU);\n\n\tpreempt_enable();\n}\n\nstatic void xen_flush_tlb_multi(const struct cpumask *cpus,\n\t\t\t\tconst struct flush_tlb_info *info)\n{\n\tstruct {\n\t\tstruct mmuext_op op;\n\t\tDECLARE_BITMAP(mask, NR_CPUS);\n\t} *args;\n\tstruct multicall_space mcs;\n\tconst size_t mc_entry_size = sizeof(args->op) +\n\t\tsizeof(args->mask[0]) * BITS_TO_LONGS(num_possible_cpus());\n\n\ttrace_xen_mmu_flush_tlb_multi(cpus, info->mm, info->start, info->end);\n\n\tif (cpumask_empty(cpus))\n\t\treturn;\t\t \n\n\tmcs = xen_mc_entry(mc_entry_size);\n\targs = mcs.args;\n\targs->op.arg2.vcpumask = to_cpumask(args->mask);\n\n\t \n\tcpumask_and(to_cpumask(args->mask), cpus, cpu_online_mask);\n\n\targs->op.cmd = MMUEXT_TLB_FLUSH_MULTI;\n\tif (info->end != TLB_FLUSH_ALL &&\n\t    (info->end - info->start) <= PAGE_SIZE) {\n\t\targs->op.cmd = MMUEXT_INVLPG_MULTI;\n\t\targs->op.arg1.linear_addr = info->start;\n\t}\n\n\tMULTI_mmuext_op(mcs.mc, &args->op, 1, NULL, DOMID_SELF);\n\n\txen_mc_issue(XEN_LAZY_MMU);\n}\n\nstatic unsigned long xen_read_cr3(void)\n{\n\treturn this_cpu_read(xen_cr3);\n}\n\nstatic void set_current_cr3(void *v)\n{\n\tthis_cpu_write(xen_current_cr3, (unsigned long)v);\n}\n\nstatic void __xen_write_cr3(bool kernel, unsigned long cr3)\n{\n\tstruct mmuext_op op;\n\tunsigned long mfn;\n\n\ttrace_xen_mmu_write_cr3(kernel, cr3);\n\n\tif (cr3)\n\t\tmfn = pfn_to_mfn(PFN_DOWN(cr3));\n\telse\n\t\tmfn = 0;\n\n\tWARN_ON(mfn == 0 && kernel);\n\n\top.cmd = kernel ? MMUEXT_NEW_BASEPTR : MMUEXT_NEW_USER_BASEPTR;\n\top.arg1.mfn = mfn;\n\n\txen_extend_mmuext_op(&op);\n\n\tif (kernel) {\n\t\tthis_cpu_write(xen_cr3, cr3);\n\n\t\t \n\t\txen_mc_callback(set_current_cr3, (void *)cr3);\n\t}\n}\nstatic void xen_write_cr3(unsigned long cr3)\n{\n\tpgd_t *user_pgd = xen_get_user_pgd(__va(cr3));\n\n\tBUG_ON(preemptible());\n\n\txen_mc_batch();   \n\n\t \n\tthis_cpu_write(xen_cr3, cr3);\n\n\t__xen_write_cr3(true, cr3);\n\n\tif (user_pgd)\n\t\t__xen_write_cr3(false, __pa(user_pgd));\n\telse\n\t\t__xen_write_cr3(false, 0);\n\n\txen_mc_issue(XEN_LAZY_CPU);   \n}\n\n \nstatic void __init xen_write_cr3_init(unsigned long cr3)\n{\n\tBUG_ON(preemptible());\n\n\txen_mc_batch();   \n\n\t \n\tthis_cpu_write(xen_cr3, cr3);\n\n\t__xen_write_cr3(true, cr3);\n\n\txen_mc_issue(XEN_LAZY_CPU);   \n}\n\nstatic int xen_pgd_alloc(struct mm_struct *mm)\n{\n\tpgd_t *pgd = mm->pgd;\n\tstruct page *page = virt_to_page(pgd);\n\tpgd_t *user_pgd;\n\tint ret = -ENOMEM;\n\n\tBUG_ON(PagePinned(virt_to_page(pgd)));\n\tBUG_ON(page->private != 0);\n\n\tuser_pgd = (pgd_t *)__get_free_page(GFP_KERNEL | __GFP_ZERO);\n\tpage->private = (unsigned long)user_pgd;\n\n\tif (user_pgd != NULL) {\n#ifdef CONFIG_X86_VSYSCALL_EMULATION\n\t\tuser_pgd[pgd_index(VSYSCALL_ADDR)] =\n\t\t\t__pgd(__pa(level3_user_vsyscall) | _PAGE_TABLE);\n#endif\n\t\tret = 0;\n\t}\n\n\tBUG_ON(PagePinned(virt_to_page(xen_get_user_pgd(pgd))));\n\n\treturn ret;\n}\n\nstatic void xen_pgd_free(struct mm_struct *mm, pgd_t *pgd)\n{\n\tpgd_t *user_pgd = xen_get_user_pgd(pgd);\n\n\tif (user_pgd)\n\t\tfree_page((unsigned long)user_pgd);\n}\n\n \nstatic void __init xen_set_pte_init(pte_t *ptep, pte_t pte)\n{\n\tif (unlikely(is_early_ioremap_ptep(ptep)))\n\t\t__xen_set_pte(ptep, pte);\n\telse\n\t\tnative_set_pte(ptep, pte);\n}\n\n__visible pte_t xen_make_pte_init(pteval_t pte)\n{\n\tunsigned long pfn;\n\n\t \n\tpfn = (pte & PTE_PFN_MASK) >> PAGE_SHIFT;\n\tif (xen_start_info->mfn_list < __START_KERNEL_map &&\n\t    pfn >= xen_start_info->first_p2m_pfn &&\n\t    pfn < xen_start_info->first_p2m_pfn + xen_start_info->nr_p2m_frames)\n\t\tpte &= ~_PAGE_RW;\n\n\tpte = pte_pfn_to_mfn(pte);\n\treturn native_make_pte(pte);\n}\nPV_CALLEE_SAVE_REGS_THUNK(xen_make_pte_init);\n\n \nstatic void __init xen_alloc_pte_init(struct mm_struct *mm, unsigned long pfn)\n{\n#ifdef CONFIG_FLATMEM\n\tBUG_ON(mem_map);\t \n#endif\n\tmake_lowmem_page_readonly(__va(PFN_PHYS(pfn)));\n\tpin_pagetable_pfn(MMUEXT_PIN_L1_TABLE, pfn);\n}\n\n \nstatic void __init xen_alloc_pmd_init(struct mm_struct *mm, unsigned long pfn)\n{\n#ifdef CONFIG_FLATMEM\n\tBUG_ON(mem_map);\t \n#endif\n\tmake_lowmem_page_readonly(__va(PFN_PHYS(pfn)));\n}\n\n \nstatic void __init xen_release_pte_init(unsigned long pfn)\n{\n\tpin_pagetable_pfn(MMUEXT_UNPIN_TABLE, pfn);\n\tmake_lowmem_page_readwrite(__va(PFN_PHYS(pfn)));\n}\n\nstatic void __init xen_release_pmd_init(unsigned long pfn)\n{\n\tmake_lowmem_page_readwrite(__va(PFN_PHYS(pfn)));\n}\n\nstatic inline void __pin_pagetable_pfn(unsigned cmd, unsigned long pfn)\n{\n\tstruct multicall_space mcs;\n\tstruct mmuext_op *op;\n\n\tmcs = __xen_mc_entry(sizeof(*op));\n\top = mcs.args;\n\top->cmd = cmd;\n\top->arg1.mfn = pfn_to_mfn(pfn);\n\n\tMULTI_mmuext_op(mcs.mc, mcs.args, 1, NULL, DOMID_SELF);\n}\n\nstatic inline void __set_pfn_prot(unsigned long pfn, pgprot_t prot)\n{\n\tstruct multicall_space mcs;\n\tunsigned long addr = (unsigned long)__va(pfn << PAGE_SHIFT);\n\n\tmcs = __xen_mc_entry(0);\n\tMULTI_update_va_mapping(mcs.mc, (unsigned long)addr,\n\t\t\t\tpfn_pte(pfn, prot), 0);\n}\n\n \nstatic inline void xen_alloc_ptpage(struct mm_struct *mm, unsigned long pfn,\n\t\t\t\t    unsigned level)\n{\n\tbool pinned = xen_page_pinned(mm->pgd);\n\n\ttrace_xen_mmu_alloc_ptpage(mm, pfn, level, pinned);\n\n\tif (pinned) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\tpinned = false;\n\t\tif (static_branch_likely(&xen_struct_pages_ready)) {\n\t\t\tpinned = PagePinned(page);\n\t\t\tSetPagePinned(page);\n\t\t}\n\n\t\txen_mc_batch();\n\n\t\t__set_pfn_prot(pfn, PAGE_KERNEL_RO);\n\n\t\tif (level == PT_PTE && USE_SPLIT_PTE_PTLOCKS && !pinned)\n\t\t\t__pin_pagetable_pfn(MMUEXT_PIN_L1_TABLE, pfn);\n\n\t\txen_mc_issue(XEN_LAZY_MMU);\n\t}\n}\n\nstatic void xen_alloc_pte(struct mm_struct *mm, unsigned long pfn)\n{\n\txen_alloc_ptpage(mm, pfn, PT_PTE);\n}\n\nstatic void xen_alloc_pmd(struct mm_struct *mm, unsigned long pfn)\n{\n\txen_alloc_ptpage(mm, pfn, PT_PMD);\n}\n\n \nstatic inline void xen_release_ptpage(unsigned long pfn, unsigned level)\n{\n\tstruct page *page = pfn_to_page(pfn);\n\tbool pinned = PagePinned(page);\n\n\ttrace_xen_mmu_release_ptpage(pfn, level, pinned);\n\n\tif (pinned) {\n\t\txen_mc_batch();\n\n\t\tif (level == PT_PTE && USE_SPLIT_PTE_PTLOCKS)\n\t\t\t__pin_pagetable_pfn(MMUEXT_UNPIN_TABLE, pfn);\n\n\t\t__set_pfn_prot(pfn, PAGE_KERNEL);\n\n\t\txen_mc_issue(XEN_LAZY_MMU);\n\n\t\tClearPagePinned(page);\n\t}\n}\n\nstatic void xen_release_pte(unsigned long pfn)\n{\n\txen_release_ptpage(pfn, PT_PTE);\n}\n\nstatic void xen_release_pmd(unsigned long pfn)\n{\n\txen_release_ptpage(pfn, PT_PMD);\n}\n\nstatic void xen_alloc_pud(struct mm_struct *mm, unsigned long pfn)\n{\n\txen_alloc_ptpage(mm, pfn, PT_PUD);\n}\n\nstatic void xen_release_pud(unsigned long pfn)\n{\n\txen_release_ptpage(pfn, PT_PUD);\n}\n\n \nstatic void * __init __ka(phys_addr_t paddr)\n{\n\treturn (void *)(paddr + __START_KERNEL_map);\n}\n\n \nstatic unsigned long __init m2p(phys_addr_t maddr)\n{\n\tphys_addr_t paddr;\n\n\tmaddr &= XEN_PTE_MFN_MASK;\n\tpaddr = mfn_to_pfn(maddr >> PAGE_SHIFT) << PAGE_SHIFT;\n\n\treturn paddr;\n}\n\n \nstatic void * __init m2v(phys_addr_t maddr)\n{\n\treturn __ka(m2p(maddr));\n}\n\n \nstatic void __init set_page_prot_flags(void *addr, pgprot_t prot,\n\t\t\t\t       unsigned long flags)\n{\n\tunsigned long pfn = __pa(addr) >> PAGE_SHIFT;\n\tpte_t pte = pfn_pte(pfn, prot);\n\n\tif (HYPERVISOR_update_va_mapping((unsigned long)addr, pte, flags))\n\t\tBUG();\n}\nstatic void __init set_page_prot(void *addr, pgprot_t prot)\n{\n\treturn set_page_prot_flags(addr, prot, UVMF_NONE);\n}\n\nvoid __init xen_setup_machphys_mapping(void)\n{\n\tstruct xen_machphys_mapping mapping;\n\n\tif (HYPERVISOR_memory_op(XENMEM_machphys_mapping, &mapping) == 0) {\n\t\tmachine_to_phys_mapping = (unsigned long *)mapping.v_start;\n\t\tmachine_to_phys_nr = mapping.max_mfn + 1;\n\t} else {\n\t\tmachine_to_phys_nr = MACH2PHYS_NR_ENTRIES;\n\t}\n}\n\nstatic void __init convert_pfn_mfn(void *v)\n{\n\tpte_t *pte = v;\n\tint i;\n\n\t \n\tfor (i = 0; i < PTRS_PER_PTE; i++)\n\t\tpte[i] = xen_make_pte(pte[i].pte);\n}\nstatic void __init check_pt_base(unsigned long *pt_base, unsigned long *pt_end,\n\t\t\t\t unsigned long addr)\n{\n\tif (*pt_base == PFN_DOWN(__pa(addr))) {\n\t\tset_page_prot_flags((void *)addr, PAGE_KERNEL, UVMF_INVLPG);\n\t\tclear_page((void *)addr);\n\t\t(*pt_base)++;\n\t}\n\tif (*pt_end == PFN_DOWN(__pa(addr))) {\n\t\tset_page_prot_flags((void *)addr, PAGE_KERNEL, UVMF_INVLPG);\n\t\tclear_page((void *)addr);\n\t\t(*pt_end)--;\n\t}\n}\n \nvoid __init xen_setup_kernel_pagetable(pgd_t *pgd, unsigned long max_pfn)\n{\n\tpud_t *l3;\n\tpmd_t *l2;\n\tunsigned long addr[3];\n\tunsigned long pt_base, pt_end;\n\tunsigned i;\n\n\t \n\tif (xen_start_info->mfn_list < __START_KERNEL_map)\n\t\tmax_pfn_mapped = xen_start_info->first_p2m_pfn;\n\telse\n\t\tmax_pfn_mapped = PFN_DOWN(__pa(xen_start_info->mfn_list));\n\n\tpt_base = PFN_DOWN(__pa(xen_start_info->pt_base));\n\tpt_end = pt_base + xen_start_info->nr_pt_frames;\n\n\t \n\tinit_top_pgt[0] = __pgd(0);\n\n\t \n\t \n\t \n\tconvert_pfn_mfn(init_top_pgt);\n\n\t \n\tconvert_pfn_mfn(level3_ident_pgt);\n\t \n\t \n\tconvert_pfn_mfn(level3_kernel_pgt);\n\n\t \n\tconvert_pfn_mfn(level2_fixmap_pgt);\n\n\t \n\tl3 = m2v(pgd[pgd_index(__START_KERNEL_map)].pgd);\n\tl2 = m2v(l3[pud_index(__START_KERNEL_map)].pud);\n\n\taddr[0] = (unsigned long)pgd;\n\taddr[1] = (unsigned long)l3;\n\taddr[2] = (unsigned long)l2;\n\t \n\tcopy_page(level2_ident_pgt, l2);\n\t \n\tcopy_page(level2_kernel_pgt, l2);\n\n\t \n\tif (__supported_pte_mask & _PAGE_NX) {\n\t\tfor (i = 0; i < PTRS_PER_PMD; ++i) {\n\t\t\tif (pmd_none(level2_ident_pgt[i]))\n\t\t\t\tcontinue;\n\t\t\tlevel2_ident_pgt[i] = pmd_set_flags(level2_ident_pgt[i], _PAGE_NX);\n\t\t}\n\t}\n\n\t \n\ti = pgd_index(xen_start_info->mfn_list);\n\tif (i && i < pgd_index(__START_KERNEL_map))\n\t\tinit_top_pgt[i] = ((pgd_t *)xen_start_info->pt_base)[i];\n\n\t \n\tset_page_prot(init_top_pgt, PAGE_KERNEL_RO);\n\tset_page_prot(level3_ident_pgt, PAGE_KERNEL_RO);\n\tset_page_prot(level3_kernel_pgt, PAGE_KERNEL_RO);\n\tset_page_prot(level2_ident_pgt, PAGE_KERNEL_RO);\n\tset_page_prot(level2_kernel_pgt, PAGE_KERNEL_RO);\n\tset_page_prot(level2_fixmap_pgt, PAGE_KERNEL_RO);\n\n\tfor (i = 0; i < FIXMAP_PMD_NUM; i++) {\n\t\tset_page_prot(level1_fixmap_pgt + i * PTRS_PER_PTE,\n\t\t\t      PAGE_KERNEL_RO);\n\t}\n\n\t \n\tpin_pagetable_pfn(MMUEXT_PIN_L4_TABLE,\n\t\t\t  PFN_DOWN(__pa_symbol(init_top_pgt)));\n\n\t \n\tpin_pagetable_pfn(MMUEXT_UNPIN_TABLE, PFN_DOWN(__pa(pgd)));\n\n#ifdef CONFIG_X86_VSYSCALL_EMULATION\n\t \n\tset_page_prot(level3_user_vsyscall, PAGE_KERNEL_RO);\n\tpin_pagetable_pfn(MMUEXT_PIN_L3_TABLE,\n\t\t\t  PFN_DOWN(__pa_symbol(level3_user_vsyscall)));\n#endif\n\n\t \n\txen_mc_batch();\n\t__xen_write_cr3(true, __pa(init_top_pgt));\n\txen_mc_issue(XEN_LAZY_CPU);\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(addr); i++)\n\t\tcheck_pt_base(&pt_base, &pt_end, addr[i]);\n\n\t \n\txen_pt_base = PFN_PHYS(pt_base);\n\txen_pt_size = (pt_end - pt_base) * PAGE_SIZE;\n\tmemblock_reserve(xen_pt_base, xen_pt_size);\n\n\t \n\txen_start_info = (struct start_info *)__va(__pa(xen_start_info));\n}\n\n \nstatic unsigned long __init xen_read_phys_ulong(phys_addr_t addr)\n{\n\tunsigned long *vaddr;\n\tunsigned long val;\n\n\tvaddr = early_memremap_ro(addr, sizeof(val));\n\tval = *vaddr;\n\tearly_memunmap(vaddr, sizeof(val));\n\treturn val;\n}\n\n \nstatic phys_addr_t __init xen_early_virt_to_phys(unsigned long vaddr)\n{\n\tphys_addr_t pa;\n\tpgd_t pgd;\n\tpud_t pud;\n\tpmd_t pmd;\n\tpte_t pte;\n\n\tpa = read_cr3_pa();\n\tpgd = native_make_pgd(xen_read_phys_ulong(pa + pgd_index(vaddr) *\n\t\t\t\t\t\t       sizeof(pgd)));\n\tif (!pgd_present(pgd))\n\t\treturn 0;\n\n\tpa = pgd_val(pgd) & PTE_PFN_MASK;\n\tpud = native_make_pud(xen_read_phys_ulong(pa + pud_index(vaddr) *\n\t\t\t\t\t\t       sizeof(pud)));\n\tif (!pud_present(pud))\n\t\treturn 0;\n\tpa = pud_val(pud) & PTE_PFN_MASK;\n\tif (pud_large(pud))\n\t\treturn pa + (vaddr & ~PUD_MASK);\n\n\tpmd = native_make_pmd(xen_read_phys_ulong(pa + pmd_index(vaddr) *\n\t\t\t\t\t\t       sizeof(pmd)));\n\tif (!pmd_present(pmd))\n\t\treturn 0;\n\tpa = pmd_val(pmd) & PTE_PFN_MASK;\n\tif (pmd_large(pmd))\n\t\treturn pa + (vaddr & ~PMD_MASK);\n\n\tpte = native_make_pte(xen_read_phys_ulong(pa + pte_index(vaddr) *\n\t\t\t\t\t\t       sizeof(pte)));\n\tif (!pte_present(pte))\n\t\treturn 0;\n\tpa = pte_pfn(pte) << PAGE_SHIFT;\n\n\treturn pa | (vaddr & ~PAGE_MASK);\n}\n\n \nvoid __init xen_relocate_p2m(void)\n{\n\tphys_addr_t size, new_area, pt_phys, pmd_phys, pud_phys;\n\tunsigned long p2m_pfn, p2m_pfn_end, n_frames, pfn, pfn_end;\n\tint n_pte, n_pt, n_pmd, n_pud, idx_pte, idx_pt, idx_pmd, idx_pud;\n\tpte_t *pt;\n\tpmd_t *pmd;\n\tpud_t *pud;\n\tpgd_t *pgd;\n\tunsigned long *new_p2m;\n\n\tsize = PAGE_ALIGN(xen_start_info->nr_pages * sizeof(unsigned long));\n\tn_pte = roundup(size, PAGE_SIZE) >> PAGE_SHIFT;\n\tn_pt = roundup(size, PMD_SIZE) >> PMD_SHIFT;\n\tn_pmd = roundup(size, PUD_SIZE) >> PUD_SHIFT;\n\tn_pud = roundup(size, P4D_SIZE) >> P4D_SHIFT;\n\tn_frames = n_pte + n_pt + n_pmd + n_pud;\n\n\tnew_area = xen_find_free_area(PFN_PHYS(n_frames));\n\tif (!new_area) {\n\t\txen_raw_console_write(\"Can't find new memory area for p2m needed due to E820 map conflict\\n\");\n\t\tBUG();\n\t}\n\n\t \n\tpud_phys = new_area;\n\tpmd_phys = pud_phys + PFN_PHYS(n_pud);\n\tpt_phys = pmd_phys + PFN_PHYS(n_pmd);\n\tp2m_pfn = PFN_DOWN(pt_phys) + n_pt;\n\n\tpgd = __va(read_cr3_pa());\n\tnew_p2m = (unsigned long *)(2 * PGDIR_SIZE);\n\tfor (idx_pud = 0; idx_pud < n_pud; idx_pud++) {\n\t\tpud = early_memremap(pud_phys, PAGE_SIZE);\n\t\tclear_page(pud);\n\t\tfor (idx_pmd = 0; idx_pmd < min(n_pmd, PTRS_PER_PUD);\n\t\t\t\tidx_pmd++) {\n\t\t\tpmd = early_memremap(pmd_phys, PAGE_SIZE);\n\t\t\tclear_page(pmd);\n\t\t\tfor (idx_pt = 0; idx_pt < min(n_pt, PTRS_PER_PMD);\n\t\t\t\t\tidx_pt++) {\n\t\t\t\tpt = early_memremap(pt_phys, PAGE_SIZE);\n\t\t\t\tclear_page(pt);\n\t\t\t\tfor (idx_pte = 0;\n\t\t\t\t     idx_pte < min(n_pte, PTRS_PER_PTE);\n\t\t\t\t     idx_pte++) {\n\t\t\t\t\tpt[idx_pte] = pfn_pte(p2m_pfn,\n\t\t\t\t\t\t\t      PAGE_KERNEL);\n\t\t\t\t\tp2m_pfn++;\n\t\t\t\t}\n\t\t\t\tn_pte -= PTRS_PER_PTE;\n\t\t\t\tearly_memunmap(pt, PAGE_SIZE);\n\t\t\t\tmake_lowmem_page_readonly(__va(pt_phys));\n\t\t\t\tpin_pagetable_pfn(MMUEXT_PIN_L1_TABLE,\n\t\t\t\t\t\tPFN_DOWN(pt_phys));\n\t\t\t\tpmd[idx_pt] = __pmd(_PAGE_TABLE | pt_phys);\n\t\t\t\tpt_phys += PAGE_SIZE;\n\t\t\t}\n\t\t\tn_pt -= PTRS_PER_PMD;\n\t\t\tearly_memunmap(pmd, PAGE_SIZE);\n\t\t\tmake_lowmem_page_readonly(__va(pmd_phys));\n\t\t\tpin_pagetable_pfn(MMUEXT_PIN_L2_TABLE,\n\t\t\t\t\tPFN_DOWN(pmd_phys));\n\t\t\tpud[idx_pmd] = __pud(_PAGE_TABLE | pmd_phys);\n\t\t\tpmd_phys += PAGE_SIZE;\n\t\t}\n\t\tn_pmd -= PTRS_PER_PUD;\n\t\tearly_memunmap(pud, PAGE_SIZE);\n\t\tmake_lowmem_page_readonly(__va(pud_phys));\n\t\tpin_pagetable_pfn(MMUEXT_PIN_L3_TABLE, PFN_DOWN(pud_phys));\n\t\tset_pgd(pgd + 2 + idx_pud, __pgd(_PAGE_TABLE | pud_phys));\n\t\tpud_phys += PAGE_SIZE;\n\t}\n\n\t \n\tmemcpy(new_p2m, xen_p2m_addr, size);\n\txen_p2m_addr = new_p2m;\n\n\t \n\tp2m_pfn = PFN_DOWN(xen_early_virt_to_phys(xen_start_info->mfn_list));\n\tBUG_ON(!p2m_pfn);\n\tp2m_pfn_end = p2m_pfn + PFN_DOWN(size);\n\n\tif (xen_start_info->mfn_list < __START_KERNEL_map) {\n\t\tpfn = xen_start_info->first_p2m_pfn;\n\t\tpfn_end = xen_start_info->first_p2m_pfn +\n\t\t\t  xen_start_info->nr_p2m_frames;\n\t\tset_pgd(pgd + 1, __pgd(0));\n\t} else {\n\t\tpfn = p2m_pfn;\n\t\tpfn_end = p2m_pfn_end;\n\t}\n\n\tmemblock_phys_free(PFN_PHYS(pfn), PAGE_SIZE * (pfn_end - pfn));\n\twhile (pfn < pfn_end) {\n\t\tif (pfn == p2m_pfn) {\n\t\t\tpfn = p2m_pfn_end;\n\t\t\tcontinue;\n\t\t}\n\t\tmake_lowmem_page_readwrite(__va(PFN_PHYS(pfn)));\n\t\tpfn++;\n\t}\n\n\txen_start_info->mfn_list = (unsigned long)xen_p2m_addr;\n\txen_start_info->first_p2m_pfn =  PFN_DOWN(new_area);\n\txen_start_info->nr_p2m_frames = n_frames;\n}\n\nvoid __init xen_reserve_special_pages(void)\n{\n\tphys_addr_t paddr;\n\n\tmemblock_reserve(__pa(xen_start_info), PAGE_SIZE);\n\tif (xen_start_info->store_mfn) {\n\t\tpaddr = PFN_PHYS(mfn_to_pfn(xen_start_info->store_mfn));\n\t\tmemblock_reserve(paddr, PAGE_SIZE);\n\t}\n\tif (!xen_initial_domain()) {\n\t\tpaddr = PFN_PHYS(mfn_to_pfn(xen_start_info->console.domU.mfn));\n\t\tmemblock_reserve(paddr, PAGE_SIZE);\n\t}\n}\n\nvoid __init xen_pt_check_e820(void)\n{\n\tif (xen_is_e820_reserved(xen_pt_base, xen_pt_size)) {\n\t\txen_raw_console_write(\"Xen hypervisor allocated page table memory conflicts with E820 map\\n\");\n\t\tBUG();\n\t}\n}\n\nstatic unsigned char dummy_mapping[PAGE_SIZE] __page_aligned_bss;\n\nstatic void xen_set_fixmap(unsigned idx, phys_addr_t phys, pgprot_t prot)\n{\n\tpte_t pte;\n\tunsigned long vaddr;\n\n\tphys >>= PAGE_SHIFT;\n\n\tswitch (idx) {\n\tcase FIX_BTMAP_END ... FIX_BTMAP_BEGIN:\n#ifdef CONFIG_X86_VSYSCALL_EMULATION\n\tcase VSYSCALL_PAGE:\n#endif\n\t\t \n\t\tpte = pfn_pte(phys, prot);\n\t\tbreak;\n\n#ifdef CONFIG_X86_LOCAL_APIC\n\tcase FIX_APIC_BASE:\t \n\t\tpte = pfn_pte(PFN_DOWN(__pa(dummy_mapping)), PAGE_KERNEL);\n\t\tbreak;\n#endif\n\n#ifdef CONFIG_X86_IO_APIC\n\tcase FIX_IO_APIC_BASE_0 ... FIX_IO_APIC_BASE_END:\n\t\t \n\t\tpte = pfn_pte(PFN_DOWN(__pa(dummy_mapping)), PAGE_KERNEL);\n\t\tbreak;\n#endif\n\n\tcase FIX_PARAVIRT_BOOTMAP:\n\t\t \n\t\tpte = mfn_pte(phys, prot);\n\t\tbreak;\n\n\tdefault:\n\t\t \n\t\tpte = mfn_pte(phys, prot);\n\t\tbreak;\n\t}\n\n\tvaddr = __fix_to_virt(idx);\n\tif (HYPERVISOR_update_va_mapping(vaddr, pte, UVMF_INVLPG))\n\t\tBUG();\n\n#ifdef CONFIG_X86_VSYSCALL_EMULATION\n\t \n\tif (idx == VSYSCALL_PAGE)\n\t\tset_pte_vaddr_pud(level3_user_vsyscall, vaddr, pte);\n#endif\n}\n\nstatic void xen_enter_lazy_mmu(void)\n{\n\tenter_lazy(XEN_LAZY_MMU);\n}\n\nstatic void xen_flush_lazy_mmu(void)\n{\n\tpreempt_disable();\n\n\tif (xen_get_lazy_mode() == XEN_LAZY_MMU) {\n\t\tarch_leave_lazy_mmu_mode();\n\t\tarch_enter_lazy_mmu_mode();\n\t}\n\n\tpreempt_enable();\n}\n\nstatic void __init xen_post_allocator_init(void)\n{\n\tpv_ops.mmu.set_pte = xen_set_pte;\n\tpv_ops.mmu.set_pmd = xen_set_pmd;\n\tpv_ops.mmu.set_pud = xen_set_pud;\n\tpv_ops.mmu.set_p4d = xen_set_p4d;\n\n\t \n\tpv_ops.mmu.alloc_pte = xen_alloc_pte;\n\tpv_ops.mmu.alloc_pmd = xen_alloc_pmd;\n\tpv_ops.mmu.release_pte = xen_release_pte;\n\tpv_ops.mmu.release_pmd = xen_release_pmd;\n\tpv_ops.mmu.alloc_pud = xen_alloc_pud;\n\tpv_ops.mmu.release_pud = xen_release_pud;\n\tpv_ops.mmu.make_pte = PV_CALLEE_SAVE(xen_make_pte);\n\n\tpv_ops.mmu.write_cr3 = &xen_write_cr3;\n}\n\nstatic void xen_leave_lazy_mmu(void)\n{\n\tpreempt_disable();\n\txen_mc_flush();\n\tleave_lazy(XEN_LAZY_MMU);\n\tpreempt_enable();\n}\n\nstatic const typeof(pv_ops) xen_mmu_ops __initconst = {\n\t.mmu = {\n\t\t.read_cr2 = __PV_IS_CALLEE_SAVE(xen_read_cr2),\n\t\t.write_cr2 = xen_write_cr2,\n\n\t\t.read_cr3 = xen_read_cr3,\n\t\t.write_cr3 = xen_write_cr3_init,\n\n\t\t.flush_tlb_user = xen_flush_tlb,\n\t\t.flush_tlb_kernel = xen_flush_tlb,\n\t\t.flush_tlb_one_user = xen_flush_tlb_one_user,\n\t\t.flush_tlb_multi = xen_flush_tlb_multi,\n\t\t.tlb_remove_table = tlb_remove_table,\n\n\t\t.pgd_alloc = xen_pgd_alloc,\n\t\t.pgd_free = xen_pgd_free,\n\n\t\t.alloc_pte = xen_alloc_pte_init,\n\t\t.release_pte = xen_release_pte_init,\n\t\t.alloc_pmd = xen_alloc_pmd_init,\n\t\t.release_pmd = xen_release_pmd_init,\n\n\t\t.set_pte = xen_set_pte_init,\n\t\t.set_pmd = xen_set_pmd_hyper,\n\n\t\t.ptep_modify_prot_start = xen_ptep_modify_prot_start,\n\t\t.ptep_modify_prot_commit = xen_ptep_modify_prot_commit,\n\n\t\t.pte_val = PV_CALLEE_SAVE(xen_pte_val),\n\t\t.pgd_val = PV_CALLEE_SAVE(xen_pgd_val),\n\n\t\t.make_pte = PV_CALLEE_SAVE(xen_make_pte_init),\n\t\t.make_pgd = PV_CALLEE_SAVE(xen_make_pgd),\n\n\t\t.set_pud = xen_set_pud_hyper,\n\n\t\t.make_pmd = PV_CALLEE_SAVE(xen_make_pmd),\n\t\t.pmd_val = PV_CALLEE_SAVE(xen_pmd_val),\n\n\t\t.pud_val = PV_CALLEE_SAVE(xen_pud_val),\n\t\t.make_pud = PV_CALLEE_SAVE(xen_make_pud),\n\t\t.set_p4d = xen_set_p4d_hyper,\n\n\t\t.alloc_pud = xen_alloc_pmd_init,\n\t\t.release_pud = xen_release_pmd_init,\n\n#if CONFIG_PGTABLE_LEVELS >= 5\n\t\t.p4d_val = PV_CALLEE_SAVE(xen_p4d_val),\n\t\t.make_p4d = PV_CALLEE_SAVE(xen_make_p4d),\n#endif\n\n\t\t.enter_mmap = xen_enter_mmap,\n\t\t.exit_mmap = xen_exit_mmap,\n\n\t\t.lazy_mode = {\n\t\t\t.enter = xen_enter_lazy_mmu,\n\t\t\t.leave = xen_leave_lazy_mmu,\n\t\t\t.flush = xen_flush_lazy_mmu,\n\t\t},\n\n\t\t.set_fixmap = xen_set_fixmap,\n\t},\n};\n\nvoid __init xen_init_mmu_ops(void)\n{\n\tx86_init.paging.pagetable_init = xen_pagetable_init;\n\tx86_init.hyper.init_after_bootmem = xen_after_bootmem;\n\n\tpv_ops.mmu = xen_mmu_ops.mmu;\n\n\tmemset(dummy_mapping, 0xff, PAGE_SIZE);\n}\n\n \n#define MAX_CONTIG_ORDER 9  \nstatic unsigned long discontig_frames[1<<MAX_CONTIG_ORDER];\n\n#define VOID_PTE (mfn_pte(0, __pgprot(0)))\nstatic void xen_zap_pfn_range(unsigned long vaddr, unsigned int order,\n\t\t\t\tunsigned long *in_frames,\n\t\t\t\tunsigned long *out_frames)\n{\n\tint i;\n\tstruct multicall_space mcs;\n\n\txen_mc_batch();\n\tfor (i = 0; i < (1UL<<order); i++, vaddr += PAGE_SIZE) {\n\t\tmcs = __xen_mc_entry(0);\n\n\t\tif (in_frames)\n\t\t\tin_frames[i] = virt_to_mfn((void *)vaddr);\n\n\t\tMULTI_update_va_mapping(mcs.mc, vaddr, VOID_PTE, 0);\n\t\t__set_phys_to_machine(virt_to_pfn((void *)vaddr), INVALID_P2M_ENTRY);\n\n\t\tif (out_frames)\n\t\t\tout_frames[i] = virt_to_pfn((void *)vaddr);\n\t}\n\txen_mc_issue(0);\n}\n\n \nstatic void xen_remap_exchanged_ptes(unsigned long vaddr, int order,\n\t\t\t\t     unsigned long *mfns,\n\t\t\t\t     unsigned long first_mfn)\n{\n\tunsigned i, limit;\n\tunsigned long mfn;\n\n\txen_mc_batch();\n\n\tlimit = 1u << order;\n\tfor (i = 0; i < limit; i++, vaddr += PAGE_SIZE) {\n\t\tstruct multicall_space mcs;\n\t\tunsigned flags;\n\n\t\tmcs = __xen_mc_entry(0);\n\t\tif (mfns)\n\t\t\tmfn = mfns[i];\n\t\telse\n\t\t\tmfn = first_mfn + i;\n\n\t\tif (i < (limit - 1))\n\t\t\tflags = 0;\n\t\telse {\n\t\t\tif (order == 0)\n\t\t\t\tflags = UVMF_INVLPG | UVMF_ALL;\n\t\t\telse\n\t\t\t\tflags = UVMF_TLB_FLUSH | UVMF_ALL;\n\t\t}\n\n\t\tMULTI_update_va_mapping(mcs.mc, vaddr,\n\t\t\t\tmfn_pte(mfn, PAGE_KERNEL), flags);\n\n\t\tset_phys_to_machine(virt_to_pfn((void *)vaddr), mfn);\n\t}\n\n\txen_mc_issue(0);\n}\n\n \nstatic int xen_exchange_memory(unsigned long extents_in, unsigned int order_in,\n\t\t\t       unsigned long *pfns_in,\n\t\t\t       unsigned long extents_out,\n\t\t\t       unsigned int order_out,\n\t\t\t       unsigned long *mfns_out,\n\t\t\t       unsigned int address_bits)\n{\n\tlong rc;\n\tint success;\n\n\tstruct xen_memory_exchange exchange = {\n\t\t.in = {\n\t\t\t.nr_extents   = extents_in,\n\t\t\t.extent_order = order_in,\n\t\t\t.extent_start = pfns_in,\n\t\t\t.domid        = DOMID_SELF\n\t\t},\n\t\t.out = {\n\t\t\t.nr_extents   = extents_out,\n\t\t\t.extent_order = order_out,\n\t\t\t.extent_start = mfns_out,\n\t\t\t.address_bits = address_bits,\n\t\t\t.domid        = DOMID_SELF\n\t\t}\n\t};\n\n\tBUG_ON(extents_in << order_in != extents_out << order_out);\n\n\trc = HYPERVISOR_memory_op(XENMEM_exchange, &exchange);\n\tsuccess = (exchange.nr_exchanged == extents_in);\n\n\tBUG_ON(!success && ((exchange.nr_exchanged != 0) || (rc == 0)));\n\tBUG_ON(success && (rc != 0));\n\n\treturn success;\n}\n\nint xen_create_contiguous_region(phys_addr_t pstart, unsigned int order,\n\t\t\t\t unsigned int address_bits,\n\t\t\t\t dma_addr_t *dma_handle)\n{\n\tunsigned long *in_frames = discontig_frames, out_frame;\n\tunsigned long  flags;\n\tint            success;\n\tunsigned long vstart = (unsigned long)phys_to_virt(pstart);\n\n\tif (unlikely(order > MAX_CONTIG_ORDER))\n\t\treturn -ENOMEM;\n\n\tmemset((void *) vstart, 0, PAGE_SIZE << order);\n\n\tspin_lock_irqsave(&xen_reservation_lock, flags);\n\n\t \n\txen_zap_pfn_range(vstart, order, in_frames, NULL);\n\n\t \n\tout_frame = virt_to_pfn((void *)vstart);\n\tsuccess = xen_exchange_memory(1UL << order, 0, in_frames,\n\t\t\t\t      1, order, &out_frame,\n\t\t\t\t      address_bits);\n\n\t \n\tif (success)\n\t\txen_remap_exchanged_ptes(vstart, order, NULL, out_frame);\n\telse\n\t\txen_remap_exchanged_ptes(vstart, order, in_frames, 0);\n\n\tspin_unlock_irqrestore(&xen_reservation_lock, flags);\n\n\t*dma_handle = virt_to_machine(vstart).maddr;\n\treturn success ? 0 : -ENOMEM;\n}\n\nvoid xen_destroy_contiguous_region(phys_addr_t pstart, unsigned int order)\n{\n\tunsigned long *out_frames = discontig_frames, in_frame;\n\tunsigned long  flags;\n\tint success;\n\tunsigned long vstart;\n\n\tif (unlikely(order > MAX_CONTIG_ORDER))\n\t\treturn;\n\n\tvstart = (unsigned long)phys_to_virt(pstart);\n\tmemset((void *) vstart, 0, PAGE_SIZE << order);\n\n\tspin_lock_irqsave(&xen_reservation_lock, flags);\n\n\t \n\tin_frame = virt_to_mfn((void *)vstart);\n\n\t \n\txen_zap_pfn_range(vstart, order, NULL, out_frames);\n\n\t \n\tsuccess = xen_exchange_memory(1, order, &in_frame, 1UL << order,\n\t\t\t\t\t0, out_frames, 0);\n\n\t \n\tif (success)\n\t\txen_remap_exchanged_ptes(vstart, order, out_frames, 0);\n\telse\n\t\txen_remap_exchanged_ptes(vstart, order, NULL, in_frame);\n\n\tspin_unlock_irqrestore(&xen_reservation_lock, flags);\n}\n\nstatic noinline void xen_flush_tlb_all(void)\n{\n\tstruct mmuext_op *op;\n\tstruct multicall_space mcs;\n\n\tpreempt_disable();\n\n\tmcs = xen_mc_entry(sizeof(*op));\n\n\top = mcs.args;\n\top->cmd = MMUEXT_TLB_FLUSH_ALL;\n\tMULTI_mmuext_op(mcs.mc, op, 1, NULL, DOMID_SELF);\n\n\txen_mc_issue(XEN_LAZY_MMU);\n\n\tpreempt_enable();\n}\n\n#define REMAP_BATCH_SIZE 16\n\nstruct remap_data {\n\txen_pfn_t *pfn;\n\tbool contiguous;\n\tbool no_translate;\n\tpgprot_t prot;\n\tstruct mmu_update *mmu_update;\n};\n\nstatic int remap_area_pfn_pte_fn(pte_t *ptep, unsigned long addr, void *data)\n{\n\tstruct remap_data *rmd = data;\n\tpte_t pte = pte_mkspecial(mfn_pte(*rmd->pfn, rmd->prot));\n\n\t \n\tif (rmd->contiguous)\n\t\t(*rmd->pfn)++;\n\telse\n\t\trmd->pfn++;\n\n\trmd->mmu_update->ptr = virt_to_machine(ptep).maddr;\n\trmd->mmu_update->ptr |= rmd->no_translate ?\n\t\tMMU_PT_UPDATE_NO_TRANSLATE :\n\t\tMMU_NORMAL_PT_UPDATE;\n\trmd->mmu_update->val = pte_val_ma(pte);\n\trmd->mmu_update++;\n\n\treturn 0;\n}\n\nint xen_remap_pfn(struct vm_area_struct *vma, unsigned long addr,\n\t\t  xen_pfn_t *pfn, int nr, int *err_ptr, pgprot_t prot,\n\t\t  unsigned int domid, bool no_translate)\n{\n\tint err = 0;\n\tstruct remap_data rmd;\n\tstruct mmu_update mmu_update[REMAP_BATCH_SIZE];\n\tunsigned long range;\n\tint mapped = 0;\n\n\tBUG_ON(!((vma->vm_flags & (VM_PFNMAP | VM_IO)) == (VM_PFNMAP | VM_IO)));\n\n\trmd.pfn = pfn;\n\trmd.prot = prot;\n\t \n\trmd.contiguous = !err_ptr;\n\trmd.no_translate = no_translate;\n\n\twhile (nr) {\n\t\tint index = 0;\n\t\tint done = 0;\n\t\tint batch = min(REMAP_BATCH_SIZE, nr);\n\t\tint batch_left = batch;\n\n\t\trange = (unsigned long)batch << PAGE_SHIFT;\n\n\t\trmd.mmu_update = mmu_update;\n\t\terr = apply_to_page_range(vma->vm_mm, addr, range,\n\t\t\t\t\t  remap_area_pfn_pte_fn, &rmd);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\t \n\t\tdo {\n\t\t\tint i;\n\n\t\t\terr = HYPERVISOR_mmu_update(&mmu_update[index],\n\t\t\t\t\t\t    batch_left, &done, domid);\n\n\t\t\t \n\t\t\tif (err_ptr) {\n\t\t\t\tfor (i = index; i < index + done; i++)\n\t\t\t\t\terr_ptr[i] = 0;\n\t\t\t}\n\t\t\tif (err < 0) {\n\t\t\t\tif (!err_ptr)\n\t\t\t\t\tgoto out;\n\t\t\t\terr_ptr[i] = err;\n\t\t\t\tdone++;  \n\t\t\t} else\n\t\t\t\tmapped += done;\n\t\t\tbatch_left -= done;\n\t\t\tindex += done;\n\t\t} while (batch_left);\n\n\t\tnr -= batch;\n\t\taddr += range;\n\t\tif (err_ptr)\n\t\t\terr_ptr += batch;\n\t\tcond_resched();\n\t}\nout:\n\n\txen_flush_tlb_all();\n\n\treturn err < 0 ? err : mapped;\n}\nEXPORT_SYMBOL_GPL(xen_remap_pfn);\n\n#ifdef CONFIG_KEXEC_CORE\nphys_addr_t paddr_vmcoreinfo_note(void)\n{\n\tif (xen_pv_domain())\n\t\treturn virt_to_machine(vmcoreinfo_note).maddr;\n\telse\n\t\treturn __pa(vmcoreinfo_note);\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}