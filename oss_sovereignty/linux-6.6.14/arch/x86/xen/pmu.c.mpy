{
  "module_name": "pmu.c",
  "hash_id": "acf367daaaab2a91b8b899deb11b803d26a87a538010f199a92bbb010b6c0f6e",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/xen/pmu.c",
  "human_readable_source": "\n#include <linux/types.h>\n#include <linux/interrupt.h>\n\n#include <asm/xen/hypercall.h>\n#include <xen/xen.h>\n#include <xen/page.h>\n#include <xen/interface/xen.h>\n#include <xen/interface/vcpu.h>\n#include <xen/interface/xenpmu.h>\n\n#include \"xen-ops.h\"\n#include \"pmu.h\"\n\n \n#include \"../events/perf_event.h\"\n\n#define XENPMU_IRQ_PROCESSING    1\nstruct xenpmu {\n\t \n\tstruct xen_pmu_data *xenpmu_data;\n\n\tuint8_t flags;\n};\nstatic DEFINE_PER_CPU(struct xenpmu, xenpmu_shared);\n#define get_xenpmu_data()    (this_cpu_ptr(&xenpmu_shared)->xenpmu_data)\n#define get_xenpmu_flags()   (this_cpu_ptr(&xenpmu_shared)->flags)\n\n \n#define field_offset(ctxt, field) ((void *)((uintptr_t)ctxt + \\\n\t\t\t\t\t    (uintptr_t)ctxt->field))\n\n \n#define F15H_NUM_COUNTERS   6\n#define F10H_NUM_COUNTERS   4\n\nstatic __read_mostly uint32_t amd_counters_base;\nstatic __read_mostly uint32_t amd_ctrls_base;\nstatic __read_mostly int amd_msr_step;\nstatic __read_mostly int k7_counters_mirrored;\nstatic __read_mostly int amd_num_counters;\n\n \n#define MSR_TYPE_COUNTER            0\n#define MSR_TYPE_CTRL               1\n#define MSR_TYPE_GLOBAL             2\n#define MSR_TYPE_ARCH_COUNTER       3\n#define MSR_TYPE_ARCH_CTRL          4\n\n \n#define PMU_GENERAL_NR_SHIFT        8\n#define PMU_GENERAL_NR_BITS         8\n#define PMU_GENERAL_NR_MASK         (((1 << PMU_GENERAL_NR_BITS) - 1) \\\n\t\t\t\t     << PMU_GENERAL_NR_SHIFT)\n\n \n#define PMU_FIXED_NR_SHIFT          0\n#define PMU_FIXED_NR_BITS           5\n#define PMU_FIXED_NR_MASK           (((1 << PMU_FIXED_NR_BITS) - 1) \\\n\t\t\t\t     << PMU_FIXED_NR_SHIFT)\n\n \n#define MSR_PMC_ALIAS_MASK          (~(MSR_IA32_PERFCTR0 ^ MSR_IA32_PMC0))\n\n#define INTEL_PMC_TYPE_SHIFT        30\n\nstatic __read_mostly int intel_num_arch_counters, intel_num_fixed_counters;\n\n\nstatic void xen_pmu_arch_init(void)\n{\n\tif (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) {\n\n\t\tswitch (boot_cpu_data.x86) {\n\t\tcase 0x15:\n\t\t\tamd_num_counters = F15H_NUM_COUNTERS;\n\t\t\tamd_counters_base = MSR_F15H_PERF_CTR;\n\t\t\tamd_ctrls_base = MSR_F15H_PERF_CTL;\n\t\t\tamd_msr_step = 2;\n\t\t\tk7_counters_mirrored = 1;\n\t\t\tbreak;\n\t\tcase 0x10:\n\t\tcase 0x12:\n\t\tcase 0x14:\n\t\tcase 0x16:\n\t\tdefault:\n\t\t\tamd_num_counters = F10H_NUM_COUNTERS;\n\t\t\tamd_counters_base = MSR_K7_PERFCTR0;\n\t\t\tamd_ctrls_base = MSR_K7_EVNTSEL0;\n\t\t\tamd_msr_step = 1;\n\t\t\tk7_counters_mirrored = 0;\n\t\t\tbreak;\n\t\t}\n\t} else if (boot_cpu_data.x86_vendor == X86_VENDOR_HYGON) {\n\t\tamd_num_counters = F10H_NUM_COUNTERS;\n\t\tamd_counters_base = MSR_K7_PERFCTR0;\n\t\tamd_ctrls_base = MSR_K7_EVNTSEL0;\n\t\tamd_msr_step = 1;\n\t\tk7_counters_mirrored = 0;\n\t} else {\n\t\tuint32_t eax, ebx, ecx, edx;\n\n\t\tcpuid(0xa, &eax, &ebx, &ecx, &edx);\n\n\t\tintel_num_arch_counters = (eax & PMU_GENERAL_NR_MASK) >>\n\t\t\tPMU_GENERAL_NR_SHIFT;\n\t\tintel_num_fixed_counters = (edx & PMU_FIXED_NR_MASK) >>\n\t\t\tPMU_FIXED_NR_SHIFT;\n\t}\n}\n\nstatic inline uint32_t get_fam15h_addr(u32 addr)\n{\n\tswitch (addr) {\n\tcase MSR_K7_PERFCTR0:\n\tcase MSR_K7_PERFCTR1:\n\tcase MSR_K7_PERFCTR2:\n\tcase MSR_K7_PERFCTR3:\n\t\treturn MSR_F15H_PERF_CTR + (addr - MSR_K7_PERFCTR0);\n\tcase MSR_K7_EVNTSEL0:\n\tcase MSR_K7_EVNTSEL1:\n\tcase MSR_K7_EVNTSEL2:\n\tcase MSR_K7_EVNTSEL3:\n\t\treturn MSR_F15H_PERF_CTL + (addr - MSR_K7_EVNTSEL0);\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn addr;\n}\n\nstatic inline bool is_amd_pmu_msr(unsigned int msr)\n{\n\tif (boot_cpu_data.x86_vendor != X86_VENDOR_AMD &&\n\t    boot_cpu_data.x86_vendor != X86_VENDOR_HYGON)\n\t\treturn false;\n\n\tif ((msr >= MSR_F15H_PERF_CTL &&\n\t     msr < MSR_F15H_PERF_CTR + (amd_num_counters * 2)) ||\n\t    (msr >= MSR_K7_EVNTSEL0 &&\n\t     msr < MSR_K7_PERFCTR0 + amd_num_counters))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool is_intel_pmu_msr(u32 msr_index, int *type, int *index)\n{\n\tu32 msr_index_pmc;\n\n\tif (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL &&\n\t    boot_cpu_data.x86_vendor != X86_VENDOR_CENTAUR &&\n\t    boot_cpu_data.x86_vendor != X86_VENDOR_ZHAOXIN)\n\t\treturn false;\n\n\tswitch (msr_index) {\n\tcase MSR_CORE_PERF_FIXED_CTR_CTRL:\n\tcase MSR_IA32_DS_AREA:\n\tcase MSR_IA32_PEBS_ENABLE:\n\t\t*type = MSR_TYPE_CTRL;\n\t\treturn true;\n\n\tcase MSR_CORE_PERF_GLOBAL_CTRL:\n\tcase MSR_CORE_PERF_GLOBAL_STATUS:\n\tcase MSR_CORE_PERF_GLOBAL_OVF_CTRL:\n\t\t*type = MSR_TYPE_GLOBAL;\n\t\treturn true;\n\n\tdefault:\n\n\t\tif ((msr_index >= MSR_CORE_PERF_FIXED_CTR0) &&\n\t\t    (msr_index < MSR_CORE_PERF_FIXED_CTR0 +\n\t\t\t\t intel_num_fixed_counters)) {\n\t\t\t*index = msr_index - MSR_CORE_PERF_FIXED_CTR0;\n\t\t\t*type = MSR_TYPE_COUNTER;\n\t\t\treturn true;\n\t\t}\n\n\t\tif ((msr_index >= MSR_P6_EVNTSEL0) &&\n\t\t    (msr_index < MSR_P6_EVNTSEL0 +  intel_num_arch_counters)) {\n\t\t\t*index = msr_index - MSR_P6_EVNTSEL0;\n\t\t\t*type = MSR_TYPE_ARCH_CTRL;\n\t\t\treturn true;\n\t\t}\n\n\t\tmsr_index_pmc = msr_index & MSR_PMC_ALIAS_MASK;\n\t\tif ((msr_index_pmc >= MSR_IA32_PERFCTR0) &&\n\t\t    (msr_index_pmc < MSR_IA32_PERFCTR0 +\n\t\t\t\t     intel_num_arch_counters)) {\n\t\t\t*type = MSR_TYPE_ARCH_COUNTER;\n\t\t\t*index = msr_index_pmc - MSR_IA32_PERFCTR0;\n\t\t\treturn true;\n\t\t}\n\t\treturn false;\n\t}\n}\n\nstatic bool xen_intel_pmu_emulate(unsigned int msr, u64 *val, int type,\n\t\t\t\t  int index, bool is_read)\n{\n\tuint64_t *reg = NULL;\n\tstruct xen_pmu_intel_ctxt *ctxt;\n\tuint64_t *fix_counters;\n\tstruct xen_pmu_cntr_pair *arch_cntr_pair;\n\tstruct xen_pmu_data *xenpmu_data = get_xenpmu_data();\n\tuint8_t xenpmu_flags = get_xenpmu_flags();\n\n\n\tif (!xenpmu_data || !(xenpmu_flags & XENPMU_IRQ_PROCESSING))\n\t\treturn false;\n\n\tctxt = &xenpmu_data->pmu.c.intel;\n\n\tswitch (msr) {\n\tcase MSR_CORE_PERF_GLOBAL_OVF_CTRL:\n\t\treg = &ctxt->global_ovf_ctrl;\n\t\tbreak;\n\tcase MSR_CORE_PERF_GLOBAL_STATUS:\n\t\treg = &ctxt->global_status;\n\t\tbreak;\n\tcase MSR_CORE_PERF_GLOBAL_CTRL:\n\t\treg = &ctxt->global_ctrl;\n\t\tbreak;\n\tcase MSR_CORE_PERF_FIXED_CTR_CTRL:\n\t\treg = &ctxt->fixed_ctrl;\n\t\tbreak;\n\tdefault:\n\t\tswitch (type) {\n\t\tcase MSR_TYPE_COUNTER:\n\t\t\tfix_counters = field_offset(ctxt, fixed_counters);\n\t\t\treg = &fix_counters[index];\n\t\t\tbreak;\n\t\tcase MSR_TYPE_ARCH_COUNTER:\n\t\t\tarch_cntr_pair = field_offset(ctxt, arch_counters);\n\t\t\treg = &arch_cntr_pair[index].counter;\n\t\t\tbreak;\n\t\tcase MSR_TYPE_ARCH_CTRL:\n\t\t\tarch_cntr_pair = field_offset(ctxt, arch_counters);\n\t\t\treg = &arch_cntr_pair[index].control;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tif (reg) {\n\t\tif (is_read)\n\t\t\t*val = *reg;\n\t\telse {\n\t\t\t*reg = *val;\n\n\t\t\tif (msr == MSR_CORE_PERF_GLOBAL_OVF_CTRL)\n\t\t\t\tctxt->global_status &= (~(*val));\n\t\t}\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool xen_amd_pmu_emulate(unsigned int msr, u64 *val, bool is_read)\n{\n\tuint64_t *reg = NULL;\n\tint i, off = 0;\n\tstruct xen_pmu_amd_ctxt *ctxt;\n\tuint64_t *counter_regs, *ctrl_regs;\n\tstruct xen_pmu_data *xenpmu_data = get_xenpmu_data();\n\tuint8_t xenpmu_flags = get_xenpmu_flags();\n\n\tif (!xenpmu_data || !(xenpmu_flags & XENPMU_IRQ_PROCESSING))\n\t\treturn false;\n\n\tif (k7_counters_mirrored &&\n\t    ((msr >= MSR_K7_EVNTSEL0) && (msr <= MSR_K7_PERFCTR3)))\n\t\tmsr = get_fam15h_addr(msr);\n\n\tctxt = &xenpmu_data->pmu.c.amd;\n\tfor (i = 0; i < amd_num_counters; i++) {\n\t\tif (msr == amd_ctrls_base + off) {\n\t\t\tctrl_regs = field_offset(ctxt, ctrls);\n\t\t\treg = &ctrl_regs[i];\n\t\t\tbreak;\n\t\t} else if (msr == amd_counters_base + off) {\n\t\t\tcounter_regs = field_offset(ctxt, counters);\n\t\t\treg = &counter_regs[i];\n\t\t\tbreak;\n\t\t}\n\t\toff += amd_msr_step;\n\t}\n\n\tif (reg) {\n\t\tif (is_read)\n\t\t\t*val = *reg;\n\t\telse\n\t\t\t*reg = *val;\n\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic bool pmu_msr_chk_emulated(unsigned int msr, uint64_t *val, bool is_read,\n\t\t\t\t bool *emul)\n{\n\tint type, index = 0;\n\n\tif (is_amd_pmu_msr(msr))\n\t\t*emul = xen_amd_pmu_emulate(msr, val, is_read);\n\telse if (is_intel_pmu_msr(msr, &type, &index))\n\t\t*emul = xen_intel_pmu_emulate(msr, val, type, index, is_read);\n\telse\n\t\treturn false;\n\n\treturn true;\n}\n\nbool pmu_msr_read(unsigned int msr, uint64_t *val, int *err)\n{\n\tbool emulated;\n\n\tif (!pmu_msr_chk_emulated(msr, val, true, &emulated))\n\t\treturn false;\n\n\tif (!emulated) {\n\t\t*val = err ? native_read_msr_safe(msr, err)\n\t\t\t   : native_read_msr(msr);\n\t}\n\n\treturn true;\n}\n\nbool pmu_msr_write(unsigned int msr, uint32_t low, uint32_t high, int *err)\n{\n\tuint64_t val = ((uint64_t)high << 32) | low;\n\tbool emulated;\n\n\tif (!pmu_msr_chk_emulated(msr, &val, false, &emulated))\n\t\treturn false;\n\n\tif (!emulated) {\n\t\tif (err)\n\t\t\t*err = native_write_msr_safe(msr, low, high);\n\t\telse\n\t\t\tnative_write_msr(msr, low, high);\n\t}\n\n\treturn true;\n}\n\nstatic unsigned long long xen_amd_read_pmc(int counter)\n{\n\tstruct xen_pmu_amd_ctxt *ctxt;\n\tuint64_t *counter_regs;\n\tstruct xen_pmu_data *xenpmu_data = get_xenpmu_data();\n\tuint8_t xenpmu_flags = get_xenpmu_flags();\n\n\tif (!xenpmu_data || !(xenpmu_flags & XENPMU_IRQ_PROCESSING)) {\n\t\tuint32_t msr;\n\t\tint err;\n\n\t\tmsr = amd_counters_base + (counter * amd_msr_step);\n\t\treturn native_read_msr_safe(msr, &err);\n\t}\n\n\tctxt = &xenpmu_data->pmu.c.amd;\n\tcounter_regs = field_offset(ctxt, counters);\n\treturn counter_regs[counter];\n}\n\nstatic unsigned long long xen_intel_read_pmc(int counter)\n{\n\tstruct xen_pmu_intel_ctxt *ctxt;\n\tuint64_t *fixed_counters;\n\tstruct xen_pmu_cntr_pair *arch_cntr_pair;\n\tstruct xen_pmu_data *xenpmu_data = get_xenpmu_data();\n\tuint8_t xenpmu_flags = get_xenpmu_flags();\n\n\tif (!xenpmu_data || !(xenpmu_flags & XENPMU_IRQ_PROCESSING)) {\n\t\tuint32_t msr;\n\t\tint err;\n\n\t\tif (counter & (1 << INTEL_PMC_TYPE_SHIFT))\n\t\t\tmsr = MSR_CORE_PERF_FIXED_CTR0 + (counter & 0xffff);\n\t\telse\n\t\t\tmsr = MSR_IA32_PERFCTR0 + counter;\n\n\t\treturn native_read_msr_safe(msr, &err);\n\t}\n\n\tctxt = &xenpmu_data->pmu.c.intel;\n\tif (counter & (1 << INTEL_PMC_TYPE_SHIFT)) {\n\t\tfixed_counters = field_offset(ctxt, fixed_counters);\n\t\treturn fixed_counters[counter & 0xffff];\n\t}\n\n\tarch_cntr_pair = field_offset(ctxt, arch_counters);\n\treturn arch_cntr_pair[counter].counter;\n}\n\nunsigned long long xen_read_pmc(int counter)\n{\n\tif (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL)\n\t\treturn xen_amd_read_pmc(counter);\n\telse\n\t\treturn xen_intel_read_pmc(counter);\n}\n\nint pmu_apic_update(uint32_t val)\n{\n\tint ret;\n\tstruct xen_pmu_data *xenpmu_data = get_xenpmu_data();\n\n\tif (!xenpmu_data) {\n\t\tpr_warn_once(\"%s: pmudata not initialized\\n\", __func__);\n\t\treturn -EINVAL;\n\t}\n\n\txenpmu_data->pmu.l.lapic_lvtpc = val;\n\n\tif (get_xenpmu_flags() & XENPMU_IRQ_PROCESSING)\n\t\treturn 0;\n\n\tret = HYPERVISOR_xenpmu_op(XENPMU_lvtpc_set, NULL);\n\n\treturn ret;\n}\n\n \nstatic unsigned int xen_guest_state(void)\n{\n\tconst struct xen_pmu_data *xenpmu_data = get_xenpmu_data();\n\tunsigned int state = 0;\n\n\tif (!xenpmu_data) {\n\t\tpr_warn_once(\"%s: pmudata not initialized\\n\", __func__);\n\t\treturn state;\n\t}\n\n\tif (!xen_initial_domain() || (xenpmu_data->domain_id >= DOMID_SELF))\n\t\treturn state;\n\n\tstate |= PERF_GUEST_ACTIVE;\n\n\tif (xenpmu_data->pmu.pmu_flags & PMU_SAMPLE_PV) {\n\t\tif (xenpmu_data->pmu.pmu_flags & PMU_SAMPLE_USER)\n\t\t\tstate |= PERF_GUEST_USER;\n\t} else if (xenpmu_data->pmu.r.regs.cpl & 3) {\n\t\tstate |= PERF_GUEST_USER;\n\t}\n\n\treturn state;\n}\n\nstatic unsigned long xen_get_guest_ip(void)\n{\n\tconst struct xen_pmu_data *xenpmu_data = get_xenpmu_data();\n\n\tif (!xenpmu_data) {\n\t\tpr_warn_once(\"%s: pmudata not initialized\\n\", __func__);\n\t\treturn 0;\n\t}\n\n\treturn xenpmu_data->pmu.r.regs.ip;\n}\n\nstatic struct perf_guest_info_callbacks xen_guest_cbs = {\n\t.state                  = xen_guest_state,\n\t.get_ip\t\t\t= xen_get_guest_ip,\n};\n\n \nstatic void xen_convert_regs(const struct xen_pmu_regs *xen_regs,\n\t\t\t     struct pt_regs *regs, uint64_t pmu_flags)\n{\n\tregs->ip = xen_regs->ip;\n\tregs->cs = xen_regs->cs;\n\tregs->sp = xen_regs->sp;\n\n\tif (pmu_flags & PMU_SAMPLE_PV) {\n\t\tif (pmu_flags & PMU_SAMPLE_USER)\n\t\t\tregs->cs |= 3;\n\t\telse\n\t\t\tregs->cs &= ~3;\n\t} else {\n\t\tif (xen_regs->cpl)\n\t\t\tregs->cs |= 3;\n\t\telse\n\t\t\tregs->cs &= ~3;\n\t}\n}\n\nirqreturn_t xen_pmu_irq_handler(int irq, void *dev_id)\n{\n\tint err, ret = IRQ_NONE;\n\tstruct pt_regs regs = {0};\n\tconst struct xen_pmu_data *xenpmu_data = get_xenpmu_data();\n\tuint8_t xenpmu_flags = get_xenpmu_flags();\n\n\tif (!xenpmu_data) {\n\t\tpr_warn_once(\"%s: pmudata not initialized\\n\", __func__);\n\t\treturn ret;\n\t}\n\n\tthis_cpu_ptr(&xenpmu_shared)->flags =\n\t\txenpmu_flags | XENPMU_IRQ_PROCESSING;\n\txen_convert_regs(&xenpmu_data->pmu.r.regs, &regs,\n\t\t\t xenpmu_data->pmu.pmu_flags);\n\tif (x86_pmu.handle_irq(&regs))\n\t\tret = IRQ_HANDLED;\n\n\t \n\terr = HYPERVISOR_xenpmu_op(XENPMU_flush, NULL);\n\tthis_cpu_ptr(&xenpmu_shared)->flags = xenpmu_flags;\n\tif (err) {\n\t\tpr_warn_once(\"%s: failed hypercall, err: %d\\n\", __func__, err);\n\t\treturn IRQ_NONE;\n\t}\n\n\treturn ret;\n}\n\nbool is_xen_pmu;\n\nvoid xen_pmu_init(int cpu)\n{\n\tint err;\n\tstruct xen_pmu_params xp;\n\tunsigned long pfn;\n\tstruct xen_pmu_data *xenpmu_data;\n\n\tBUILD_BUG_ON(sizeof(struct xen_pmu_data) > PAGE_SIZE);\n\n\tif (xen_hvm_domain() || (cpu != 0 && !is_xen_pmu))\n\t\treturn;\n\n\txenpmu_data = (struct xen_pmu_data *)get_zeroed_page(GFP_KERNEL);\n\tif (!xenpmu_data) {\n\t\tpr_err(\"VPMU init: No memory\\n\");\n\t\treturn;\n\t}\n\tpfn = virt_to_pfn(xenpmu_data);\n\n\txp.val = pfn_to_mfn(pfn);\n\txp.vcpu = cpu;\n\txp.version.maj = XENPMU_VER_MAJ;\n\txp.version.min = XENPMU_VER_MIN;\n\terr = HYPERVISOR_xenpmu_op(XENPMU_init, &xp);\n\tif (err)\n\t\tgoto fail;\n\n\tper_cpu(xenpmu_shared, cpu).xenpmu_data = xenpmu_data;\n\tper_cpu(xenpmu_shared, cpu).flags = 0;\n\n\tif (!is_xen_pmu) {\n\t\tis_xen_pmu = true;\n\t\tperf_register_guest_info_callbacks(&xen_guest_cbs);\n\t\txen_pmu_arch_init();\n\t}\n\n\treturn;\n\nfail:\n\tif (err == -EOPNOTSUPP || err == -ENOSYS)\n\t\tpr_info_once(\"VPMU disabled by hypervisor.\\n\");\n\telse\n\t\tpr_info_once(\"Could not initialize VPMU for cpu %d, error %d\\n\",\n\t\t\tcpu, err);\n\tfree_pages((unsigned long)xenpmu_data, 0);\n}\n\nvoid xen_pmu_finish(int cpu)\n{\n\tstruct xen_pmu_params xp;\n\n\tif (xen_hvm_domain())\n\t\treturn;\n\n\txp.vcpu = cpu;\n\txp.version.maj = XENPMU_VER_MAJ;\n\txp.version.min = XENPMU_VER_MIN;\n\n\t(void)HYPERVISOR_xenpmu_op(XENPMU_finish, &xp);\n\n\tfree_pages((unsigned long)per_cpu(xenpmu_shared, cpu).xenpmu_data, 0);\n\tper_cpu(xenpmu_shared, cpu).xenpmu_data = NULL;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}