{
  "module_name": "mem_encrypt_amd.c",
  "hash_id": "1baa1e547bb74ef73fc4894ce4a67637e13baa80d0aef17edde6ed25db9f8c68",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/mm/mem_encrypt_amd.c",
  "human_readable_source": "\n \n\n#define DISABLE_BRANCH_PROFILING\n\n#include <linux/linkage.h>\n#include <linux/init.h>\n#include <linux/mm.h>\n#include <linux/dma-direct.h>\n#include <linux/swiotlb.h>\n#include <linux/mem_encrypt.h>\n#include <linux/device.h>\n#include <linux/kernel.h>\n#include <linux/bitops.h>\n#include <linux/dma-mapping.h>\n#include <linux/virtio_config.h>\n#include <linux/virtio_anchor.h>\n#include <linux/cc_platform.h>\n\n#include <asm/tlbflush.h>\n#include <asm/fixmap.h>\n#include <asm/setup.h>\n#include <asm/mem_encrypt.h>\n#include <asm/bootparam.h>\n#include <asm/set_memory.h>\n#include <asm/cacheflush.h>\n#include <asm/processor-flags.h>\n#include <asm/msr.h>\n#include <asm/cmdline.h>\n#include <asm/sev.h>\n#include <asm/ia32.h>\n\n#include \"mm_internal.h\"\n\n \nu64 sme_me_mask __section(\".data\") = 0;\nu64 sev_status __section(\".data\") = 0;\nu64 sev_check_data __section(\".data\") = 0;\nEXPORT_SYMBOL(sme_me_mask);\n\n \nstatic char sme_early_buffer[PAGE_SIZE] __initdata __aligned(PAGE_SIZE);\n\n \nstatic inline void __init snp_memcpy(void *dst, void *src, size_t sz,\n\t\t\t\t     unsigned long paddr, bool decrypt)\n{\n\tunsigned long npages = PAGE_ALIGN(sz) >> PAGE_SHIFT;\n\n\tif (decrypt) {\n\t\t \n\t\tearly_snp_set_memory_shared((unsigned long)__va(paddr), paddr, npages);\n\n\t\tmemcpy(dst, src, sz);\n\n\t\t \n\t\tearly_snp_set_memory_private((unsigned long)__va(paddr), paddr, npages);\n\t} else {\n\t\t \n\t\tmemcpy(dst, src, sz);\n\t}\n}\n\n \nstatic void __init __sme_early_enc_dec(resource_size_t paddr,\n\t\t\t\t       unsigned long size, bool enc)\n{\n\tvoid *src, *dst;\n\tsize_t len;\n\n\tif (!sme_me_mask)\n\t\treturn;\n\n\twbinvd();\n\n\t \n\twhile (size) {\n\t\tlen = min_t(size_t, sizeof(sme_early_buffer), size);\n\n\t\t \n\t\tsrc = enc ? early_memremap_decrypted_wp(paddr, len) :\n\t\t\t    early_memremap_encrypted_wp(paddr, len);\n\n\t\tdst = enc ? early_memremap_encrypted(paddr, len) :\n\t\t\t    early_memremap_decrypted(paddr, len);\n\n\t\t \n\t\tBUG_ON(!src || !dst);\n\n\t\t \n\t\tif (cc_platform_has(CC_ATTR_GUEST_SEV_SNP)) {\n\t\t\tsnp_memcpy(sme_early_buffer, src, len, paddr, enc);\n\t\t\tsnp_memcpy(dst, sme_early_buffer, len, paddr, !enc);\n\t\t} else {\n\t\t\tmemcpy(sme_early_buffer, src, len);\n\t\t\tmemcpy(dst, sme_early_buffer, len);\n\t\t}\n\n\t\tearly_memunmap(dst, len);\n\t\tearly_memunmap(src, len);\n\n\t\tpaddr += len;\n\t\tsize -= len;\n\t}\n}\n\nvoid __init sme_early_encrypt(resource_size_t paddr, unsigned long size)\n{\n\t__sme_early_enc_dec(paddr, size, true);\n}\n\nvoid __init sme_early_decrypt(resource_size_t paddr, unsigned long size)\n{\n\t__sme_early_enc_dec(paddr, size, false);\n}\n\nstatic void __init __sme_early_map_unmap_mem(void *vaddr, unsigned long size,\n\t\t\t\t\t     bool map)\n{\n\tunsigned long paddr = (unsigned long)vaddr - __PAGE_OFFSET;\n\tpmdval_t pmd_flags, pmd;\n\n\t \n\tpmd_flags = __sme_clr(early_pmd_flags);\n\n\tdo {\n\t\tpmd = map ? (paddr & PMD_MASK) + pmd_flags : 0;\n\t\t__early_make_pgtable((unsigned long)vaddr, pmd);\n\n\t\tvaddr += PMD_SIZE;\n\t\tpaddr += PMD_SIZE;\n\t\tsize = (size <= PMD_SIZE) ? 0 : size - PMD_SIZE;\n\t} while (size);\n\n\tflush_tlb_local();\n}\n\nvoid __init sme_unmap_bootdata(char *real_mode_data)\n{\n\tstruct boot_params *boot_data;\n\tunsigned long cmdline_paddr;\n\n\tif (!cc_platform_has(CC_ATTR_HOST_MEM_ENCRYPT))\n\t\treturn;\n\n\t \n\tboot_data = (struct boot_params *)real_mode_data;\n\tcmdline_paddr = boot_data->hdr.cmd_line_ptr | ((u64)boot_data->ext_cmd_line_ptr << 32);\n\n\t__sme_early_map_unmap_mem(real_mode_data, sizeof(boot_params), false);\n\n\tif (!cmdline_paddr)\n\t\treturn;\n\n\t__sme_early_map_unmap_mem(__va(cmdline_paddr), COMMAND_LINE_SIZE, false);\n}\n\nvoid __init sme_map_bootdata(char *real_mode_data)\n{\n\tstruct boot_params *boot_data;\n\tunsigned long cmdline_paddr;\n\n\tif (!cc_platform_has(CC_ATTR_HOST_MEM_ENCRYPT))\n\t\treturn;\n\n\t__sme_early_map_unmap_mem(real_mode_data, sizeof(boot_params), true);\n\n\t \n\tboot_data = (struct boot_params *)real_mode_data;\n\tcmdline_paddr = boot_data->hdr.cmd_line_ptr | ((u64)boot_data->ext_cmd_line_ptr << 32);\n\n\tif (!cmdline_paddr)\n\t\treturn;\n\n\t__sme_early_map_unmap_mem(__va(cmdline_paddr), COMMAND_LINE_SIZE, true);\n}\n\nvoid __init sev_setup_arch(void)\n{\n\tphys_addr_t total_mem = memblock_phys_mem_size();\n\tunsigned long size;\n\n\tif (!cc_platform_has(CC_ATTR_GUEST_MEM_ENCRYPT))\n\t\treturn;\n\n\t \n\tsize = total_mem * 6 / 100;\n\tsize = clamp_val(size, IO_TLB_DEFAULT_SIZE, SZ_1G);\n\tswiotlb_adjust_size(size);\n\n\t \n\tvirtio_set_mem_acc_cb(virtio_require_restricted_mem_acc);\n}\n\nstatic unsigned long pg_level_to_pfn(int level, pte_t *kpte, pgprot_t *ret_prot)\n{\n\tunsigned long pfn = 0;\n\tpgprot_t prot;\n\n\tswitch (level) {\n\tcase PG_LEVEL_4K:\n\t\tpfn = pte_pfn(*kpte);\n\t\tprot = pte_pgprot(*kpte);\n\t\tbreak;\n\tcase PG_LEVEL_2M:\n\t\tpfn = pmd_pfn(*(pmd_t *)kpte);\n\t\tprot = pmd_pgprot(*(pmd_t *)kpte);\n\t\tbreak;\n\tcase PG_LEVEL_1G:\n\t\tpfn = pud_pfn(*(pud_t *)kpte);\n\t\tprot = pud_pgprot(*(pud_t *)kpte);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"Invalid level for kpte\\n\");\n\t\treturn 0;\n\t}\n\n\tif (ret_prot)\n\t\t*ret_prot = prot;\n\n\treturn pfn;\n}\n\nstatic bool amd_enc_tlb_flush_required(bool enc)\n{\n\treturn true;\n}\n\nstatic bool amd_enc_cache_flush_required(void)\n{\n\treturn !cpu_feature_enabled(X86_FEATURE_SME_COHERENT);\n}\n\nstatic void enc_dec_hypercall(unsigned long vaddr, unsigned long size, bool enc)\n{\n#ifdef CONFIG_PARAVIRT\n\tunsigned long vaddr_end = vaddr + size;\n\n\twhile (vaddr < vaddr_end) {\n\t\tint psize, pmask, level;\n\t\tunsigned long pfn;\n\t\tpte_t *kpte;\n\n\t\tkpte = lookup_address(vaddr, &level);\n\t\tif (!kpte || pte_none(*kpte)) {\n\t\t\tWARN_ONCE(1, \"kpte lookup for vaddr\\n\");\n\t\t\treturn;\n\t\t}\n\n\t\tpfn = pg_level_to_pfn(level, kpte, NULL);\n\t\tif (!pfn)\n\t\t\tcontinue;\n\n\t\tpsize = page_level_size(level);\n\t\tpmask = page_level_mask(level);\n\n\t\tnotify_page_enc_status_changed(pfn, psize >> PAGE_SHIFT, enc);\n\n\t\tvaddr = (vaddr & pmask) + psize;\n\t}\n#endif\n}\n\nstatic bool amd_enc_status_change_prepare(unsigned long vaddr, int npages, bool enc)\n{\n\t \n\tif (cc_platform_has(CC_ATTR_GUEST_SEV_SNP) && !enc)\n\t\tsnp_set_memory_shared(vaddr, npages);\n\n\treturn true;\n}\n\n \nstatic bool amd_enc_status_change_finish(unsigned long vaddr, int npages, bool enc)\n{\n\t \n\tif (cc_platform_has(CC_ATTR_GUEST_SEV_SNP) && enc)\n\t\tsnp_set_memory_private(vaddr, npages);\n\n\tif (!cc_platform_has(CC_ATTR_HOST_MEM_ENCRYPT))\n\t\tenc_dec_hypercall(vaddr, npages << PAGE_SHIFT, enc);\n\n\treturn true;\n}\n\nstatic void __init __set_clr_pte_enc(pte_t *kpte, int level, bool enc)\n{\n\tpgprot_t old_prot, new_prot;\n\tunsigned long pfn, pa, size;\n\tpte_t new_pte;\n\n\tpfn = pg_level_to_pfn(level, kpte, &old_prot);\n\tif (!pfn)\n\t\treturn;\n\n\tnew_prot = old_prot;\n\tif (enc)\n\t\tpgprot_val(new_prot) |= _PAGE_ENC;\n\telse\n\t\tpgprot_val(new_prot) &= ~_PAGE_ENC;\n\n\t \n\tif (pgprot_val(old_prot) == pgprot_val(new_prot))\n\t\treturn;\n\n\tpa = pfn << PAGE_SHIFT;\n\tsize = page_level_size(level);\n\n\t \n\tclflush_cache_range(__va(pa), size);\n\n\t \n\tif (enc) {\n\t\tsme_early_encrypt(pa, size);\n\t} else {\n\t\tsme_early_decrypt(pa, size);\n\n\t\t \n\t\tearly_snp_set_memory_shared((unsigned long)__va(pa), pa, 1);\n\t}\n\n\t \n\tnew_pte = pfn_pte(pfn, new_prot);\n\tset_pte_atomic(kpte, new_pte);\n\n\t \n\tif (enc)\n\t\tearly_snp_set_memory_private((unsigned long)__va(pa), pa, 1);\n}\n\nstatic int __init early_set_memory_enc_dec(unsigned long vaddr,\n\t\t\t\t\t   unsigned long size, bool enc)\n{\n\tunsigned long vaddr_end, vaddr_next, start;\n\tunsigned long psize, pmask;\n\tint split_page_size_mask;\n\tint level, ret;\n\tpte_t *kpte;\n\n\tstart = vaddr;\n\tvaddr_next = vaddr;\n\tvaddr_end = vaddr + size;\n\n\tfor (; vaddr < vaddr_end; vaddr = vaddr_next) {\n\t\tkpte = lookup_address(vaddr, &level);\n\t\tif (!kpte || pte_none(*kpte)) {\n\t\t\tret = 1;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (level == PG_LEVEL_4K) {\n\t\t\t__set_clr_pte_enc(kpte, level, enc);\n\t\t\tvaddr_next = (vaddr & PAGE_MASK) + PAGE_SIZE;\n\t\t\tcontinue;\n\t\t}\n\n\t\tpsize = page_level_size(level);\n\t\tpmask = page_level_mask(level);\n\n\t\t \n\t\tif (vaddr == (vaddr & pmask) &&\n\t\t    ((vaddr_end - vaddr) >= psize)) {\n\t\t\t__set_clr_pte_enc(kpte, level, enc);\n\t\t\tvaddr_next = (vaddr & pmask) + psize;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (level == PG_LEVEL_2M)\n\t\t\tsplit_page_size_mask = 0;\n\t\telse\n\t\t\tsplit_page_size_mask = 1 << PG_LEVEL_2M;\n\n\t\t \n\t\tkernel_physical_mapping_change(__pa(vaddr & pmask),\n\t\t\t\t\t       __pa((vaddr_end & pmask) + psize),\n\t\t\t\t\t       split_page_size_mask);\n\t}\n\n\tret = 0;\n\n\tearly_set_mem_enc_dec_hypercall(start, size, enc);\nout:\n\t__flush_tlb_all();\n\treturn ret;\n}\n\nint __init early_set_memory_decrypted(unsigned long vaddr, unsigned long size)\n{\n\treturn early_set_memory_enc_dec(vaddr, size, false);\n}\n\nint __init early_set_memory_encrypted(unsigned long vaddr, unsigned long size)\n{\n\treturn early_set_memory_enc_dec(vaddr, size, true);\n}\n\nvoid __init early_set_mem_enc_dec_hypercall(unsigned long vaddr, unsigned long size, bool enc)\n{\n\tenc_dec_hypercall(vaddr, size, enc);\n}\n\nvoid __init sme_early_init(void)\n{\n\tif (!sme_me_mask)\n\t\treturn;\n\n\tearly_pmd_flags = __sme_set(early_pmd_flags);\n\n\t__supported_pte_mask = __sme_set(__supported_pte_mask);\n\n\t \n\tadd_encrypt_protection_map();\n\n\tx86_platform.guest.enc_status_change_prepare = amd_enc_status_change_prepare;\n\tx86_platform.guest.enc_status_change_finish  = amd_enc_status_change_finish;\n\tx86_platform.guest.enc_tlb_flush_required    = amd_enc_tlb_flush_required;\n\tx86_platform.guest.enc_cache_flush_required  = amd_enc_cache_flush_required;\n\n\t \n\tif (sev_status & MSR_AMD64_SEV_ES_ENABLED)\n\t\tx86_cpuinit.parallel_bringup = false;\n\n\t \n\tif (sev_status & MSR_AMD64_SEV_ENABLED)\n\t\tia32_disable();\n}\n\nvoid __init mem_encrypt_free_decrypted_mem(void)\n{\n\tunsigned long vaddr, vaddr_end, npages;\n\tint r;\n\n\tvaddr = (unsigned long)__start_bss_decrypted_unused;\n\tvaddr_end = (unsigned long)__end_bss_decrypted;\n\tnpages = (vaddr_end - vaddr) >> PAGE_SHIFT;\n\n\t \n\tif (sme_me_mask) {\n\t\tr = set_memory_encrypted(vaddr, npages);\n\t\tif (r) {\n\t\t\tpr_warn(\"failed to free unused decrypted pages\\n\");\n\t\t\treturn;\n\t\t}\n\t}\n\n\tfree_init_pages(\"unused decrypted\", vaddr, vaddr_end);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}