{
  "module_name": "init.c",
  "hash_id": "0cd6d7f6fa252d88ba18402d483fe84e4af452d6e92c6469ac488f017ca25177",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/mm/init.c",
  "human_readable_source": "#include <linux/gfp.h>\n#include <linux/initrd.h>\n#include <linux/ioport.h>\n#include <linux/swap.h>\n#include <linux/memblock.h>\n#include <linux/swapfile.h>\n#include <linux/swapops.h>\n#include <linux/kmemleak.h>\n#include <linux/sched/task.h>\n\n#include <asm/set_memory.h>\n#include <asm/cpu_device_id.h>\n#include <asm/e820/api.h>\n#include <asm/init.h>\n#include <asm/page.h>\n#include <asm/page_types.h>\n#include <asm/sections.h>\n#include <asm/setup.h>\n#include <asm/tlbflush.h>\n#include <asm/tlb.h>\n#include <asm/proto.h>\n#include <asm/dma.h>\t\t \n#include <asm/kaslr.h>\n#include <asm/hypervisor.h>\n#include <asm/cpufeature.h>\n#include <asm/pti.h>\n#include <asm/text-patching.h>\n#include <asm/memtype.h>\n#include <asm/paravirt.h>\n\n \n#include <trace/events/tlb.h>\n\n#include \"mm_internal.h\"\n\n \nstatic uint16_t __cachemode2pte_tbl[_PAGE_CACHE_MODE_NUM] = {\n\t[_PAGE_CACHE_MODE_WB      ]\t= 0         | 0        ,\n\t[_PAGE_CACHE_MODE_WC      ]\t= 0         | _PAGE_PCD,\n\t[_PAGE_CACHE_MODE_UC_MINUS]\t= 0         | _PAGE_PCD,\n\t[_PAGE_CACHE_MODE_UC      ]\t= _PAGE_PWT | _PAGE_PCD,\n\t[_PAGE_CACHE_MODE_WT      ]\t= 0         | _PAGE_PCD,\n\t[_PAGE_CACHE_MODE_WP      ]\t= 0         | _PAGE_PCD,\n};\n\nunsigned long cachemode2protval(enum page_cache_mode pcm)\n{\n\tif (likely(pcm == 0))\n\t\treturn 0;\n\treturn __cachemode2pte_tbl[pcm];\n}\nEXPORT_SYMBOL(cachemode2protval);\n\nstatic uint8_t __pte2cachemode_tbl[8] = {\n\t[__pte2cm_idx( 0        | 0         | 0        )] = _PAGE_CACHE_MODE_WB,\n\t[__pte2cm_idx(_PAGE_PWT | 0         | 0        )] = _PAGE_CACHE_MODE_UC_MINUS,\n\t[__pte2cm_idx( 0        | _PAGE_PCD | 0        )] = _PAGE_CACHE_MODE_UC_MINUS,\n\t[__pte2cm_idx(_PAGE_PWT | _PAGE_PCD | 0        )] = _PAGE_CACHE_MODE_UC,\n\t[__pte2cm_idx( 0        | 0         | _PAGE_PAT)] = _PAGE_CACHE_MODE_WB,\n\t[__pte2cm_idx(_PAGE_PWT | 0         | _PAGE_PAT)] = _PAGE_CACHE_MODE_UC_MINUS,\n\t[__pte2cm_idx(0         | _PAGE_PCD | _PAGE_PAT)] = _PAGE_CACHE_MODE_UC_MINUS,\n\t[__pte2cm_idx(_PAGE_PWT | _PAGE_PCD | _PAGE_PAT)] = _PAGE_CACHE_MODE_UC,\n};\n\n \nbool x86_has_pat_wp(void)\n{\n\tuint16_t prot = __cachemode2pte_tbl[_PAGE_CACHE_MODE_WP];\n\n\treturn __pte2cachemode_tbl[__pte2cm_idx(prot)] == _PAGE_CACHE_MODE_WP;\n}\n\nenum page_cache_mode pgprot2cachemode(pgprot_t pgprot)\n{\n\tunsigned long masked;\n\n\tmasked = pgprot_val(pgprot) & _PAGE_CACHE_MASK;\n\tif (likely(masked == 0))\n\t\treturn 0;\n\treturn __pte2cachemode_tbl[__pte2cm_idx(masked)];\n}\n\nstatic unsigned long __initdata pgt_buf_start;\nstatic unsigned long __initdata pgt_buf_end;\nstatic unsigned long __initdata pgt_buf_top;\n\nstatic unsigned long min_pfn_mapped;\n\nstatic bool __initdata can_use_brk_pgt = true;\n\n \n__ref void *alloc_low_pages(unsigned int num)\n{\n\tunsigned long pfn;\n\tint i;\n\n\tif (after_bootmem) {\n\t\tunsigned int order;\n\n\t\torder = get_order((unsigned long)num << PAGE_SHIFT);\n\t\treturn (void *)__get_free_pages(GFP_ATOMIC | __GFP_ZERO, order);\n\t}\n\n\tif ((pgt_buf_end + num) > pgt_buf_top || !can_use_brk_pgt) {\n\t\tunsigned long ret = 0;\n\n\t\tif (min_pfn_mapped < max_pfn_mapped) {\n\t\t\tret = memblock_phys_alloc_range(\n\t\t\t\t\tPAGE_SIZE * num, PAGE_SIZE,\n\t\t\t\t\tmin_pfn_mapped << PAGE_SHIFT,\n\t\t\t\t\tmax_pfn_mapped << PAGE_SHIFT);\n\t\t}\n\t\tif (!ret && can_use_brk_pgt)\n\t\t\tret = __pa(extend_brk(PAGE_SIZE * num, PAGE_SIZE));\n\n\t\tif (!ret)\n\t\t\tpanic(\"alloc_low_pages: can not alloc memory\");\n\n\t\tpfn = ret >> PAGE_SHIFT;\n\t} else {\n\t\tpfn = pgt_buf_end;\n\t\tpgt_buf_end += num;\n\t}\n\n\tfor (i = 0; i < num; i++) {\n\t\tvoid *adr;\n\n\t\tadr = __va((pfn + i) << PAGE_SHIFT);\n\t\tclear_page(adr);\n\t}\n\n\treturn __va(pfn << PAGE_SHIFT);\n}\n\n \n\n#ifndef CONFIG_X86_5LEVEL\n#define INIT_PGD_PAGE_TABLES    3\n#else\n#define INIT_PGD_PAGE_TABLES    4\n#endif\n\n#ifndef CONFIG_RANDOMIZE_MEMORY\n#define INIT_PGD_PAGE_COUNT      (2 * INIT_PGD_PAGE_TABLES)\n#else\n#define INIT_PGD_PAGE_COUNT      (4 * INIT_PGD_PAGE_TABLES)\n#endif\n\n#define INIT_PGT_BUF_SIZE\t(INIT_PGD_PAGE_COUNT * PAGE_SIZE)\nRESERVE_BRK(early_pgt_alloc, INIT_PGT_BUF_SIZE);\nvoid  __init early_alloc_pgt_buf(void)\n{\n\tunsigned long tables = INIT_PGT_BUF_SIZE;\n\tphys_addr_t base;\n\n\tbase = __pa(extend_brk(tables, PAGE_SIZE));\n\n\tpgt_buf_start = base >> PAGE_SHIFT;\n\tpgt_buf_end = pgt_buf_start;\n\tpgt_buf_top = pgt_buf_start + (tables >> PAGE_SHIFT);\n}\n\nint after_bootmem;\n\nearly_param_on_off(\"gbpages\", \"nogbpages\", direct_gbpages, CONFIG_X86_DIRECT_GBPAGES);\n\nstruct map_range {\n\tunsigned long start;\n\tunsigned long end;\n\tunsigned page_size_mask;\n};\n\nstatic int page_size_mask;\n\n \nstatic inline void cr4_set_bits_and_update_boot(unsigned long mask)\n{\n\tmmu_cr4_features |= mask;\n\tif (trampoline_cr4_features)\n\t\t*trampoline_cr4_features = mmu_cr4_features;\n\tcr4_set_bits(mask);\n}\n\nstatic void __init probe_page_size_mask(void)\n{\n\t \n\tif (boot_cpu_has(X86_FEATURE_PSE) && !debug_pagealloc_enabled())\n\t\tpage_size_mask |= 1 << PG_LEVEL_2M;\n\telse\n\t\tdirect_gbpages = 0;\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_PSE))\n\t\tcr4_set_bits_and_update_boot(X86_CR4_PSE);\n\n\t \n\t__supported_pte_mask &= ~_PAGE_GLOBAL;\n\tif (boot_cpu_has(X86_FEATURE_PGE)) {\n\t\tcr4_set_bits_and_update_boot(X86_CR4_PGE);\n\t\t__supported_pte_mask |= _PAGE_GLOBAL;\n\t}\n\n\t \n\t__default_kernel_pte_mask = __supported_pte_mask;\n\t \n\tif (cpu_feature_enabled(X86_FEATURE_PTI))\n\t\t__default_kernel_pte_mask &= ~_PAGE_GLOBAL;\n\n\t \n\tif (direct_gbpages && boot_cpu_has(X86_FEATURE_GBPAGES)) {\n\t\tprintk(KERN_INFO \"Using GB pages for direct mapping\\n\");\n\t\tpage_size_mask |= 1 << PG_LEVEL_1G;\n\t} else {\n\t\tdirect_gbpages = 0;\n\t}\n}\n\n#define INTEL_MATCH(_model) { .vendor  = X86_VENDOR_INTEL,\t\\\n\t\t\t      .family  = 6,\t\t\t\\\n\t\t\t      .model = _model,\t\t\t\\\n\t\t\t    }\n \nstatic const struct x86_cpu_id invlpg_miss_ids[] = {\n\tINTEL_MATCH(INTEL_FAM6_ALDERLAKE   ),\n\tINTEL_MATCH(INTEL_FAM6_ALDERLAKE_L ),\n\tINTEL_MATCH(INTEL_FAM6_ATOM_GRACEMONT ),\n\tINTEL_MATCH(INTEL_FAM6_RAPTORLAKE  ),\n\tINTEL_MATCH(INTEL_FAM6_RAPTORLAKE_P),\n\tINTEL_MATCH(INTEL_FAM6_RAPTORLAKE_S),\n\t{}\n};\n\nstatic void setup_pcid(void)\n{\n\tif (!IS_ENABLED(CONFIG_X86_64))\n\t\treturn;\n\n\tif (!boot_cpu_has(X86_FEATURE_PCID))\n\t\treturn;\n\n\tif (x86_match_cpu(invlpg_miss_ids)) {\n\t\tpr_info(\"Incomplete global flushes, disabling PCID\");\n\t\tsetup_clear_cpu_cap(X86_FEATURE_PCID);\n\t\treturn;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_PGE)) {\n\t\t \n\t\tcr4_set_bits(X86_CR4_PCIDE);\n\t} else {\n\t\t \n\t\tsetup_clear_cpu_cap(X86_FEATURE_PCID);\n\t}\n}\n\n#ifdef CONFIG_X86_32\n#define NR_RANGE_MR 3\n#else  \n#define NR_RANGE_MR 5\n#endif\n\nstatic int __meminit save_mr(struct map_range *mr, int nr_range,\n\t\t\t     unsigned long start_pfn, unsigned long end_pfn,\n\t\t\t     unsigned long page_size_mask)\n{\n\tif (start_pfn < end_pfn) {\n\t\tif (nr_range >= NR_RANGE_MR)\n\t\t\tpanic(\"run out of range for init_memory_mapping\\n\");\n\t\tmr[nr_range].start = start_pfn<<PAGE_SHIFT;\n\t\tmr[nr_range].end   = end_pfn<<PAGE_SHIFT;\n\t\tmr[nr_range].page_size_mask = page_size_mask;\n\t\tnr_range++;\n\t}\n\n\treturn nr_range;\n}\n\n \nstatic void __ref adjust_range_page_size_mask(struct map_range *mr,\n\t\t\t\t\t\t\t int nr_range)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_range; i++) {\n\t\tif ((page_size_mask & (1<<PG_LEVEL_2M)) &&\n\t\t    !(mr[i].page_size_mask & (1<<PG_LEVEL_2M))) {\n\t\t\tunsigned long start = round_down(mr[i].start, PMD_SIZE);\n\t\t\tunsigned long end = round_up(mr[i].end, PMD_SIZE);\n\n#ifdef CONFIG_X86_32\n\t\t\tif ((end >> PAGE_SHIFT) > max_low_pfn)\n\t\t\t\tcontinue;\n#endif\n\n\t\t\tif (memblock_is_region_memory(start, end - start))\n\t\t\t\tmr[i].page_size_mask |= 1<<PG_LEVEL_2M;\n\t\t}\n\t\tif ((page_size_mask & (1<<PG_LEVEL_1G)) &&\n\t\t    !(mr[i].page_size_mask & (1<<PG_LEVEL_1G))) {\n\t\t\tunsigned long start = round_down(mr[i].start, PUD_SIZE);\n\t\t\tunsigned long end = round_up(mr[i].end, PUD_SIZE);\n\n\t\t\tif (memblock_is_region_memory(start, end - start))\n\t\t\t\tmr[i].page_size_mask |= 1<<PG_LEVEL_1G;\n\t\t}\n\t}\n}\n\nstatic const char *page_size_string(struct map_range *mr)\n{\n\tstatic const char str_1g[] = \"1G\";\n\tstatic const char str_2m[] = \"2M\";\n\tstatic const char str_4m[] = \"4M\";\n\tstatic const char str_4k[] = \"4k\";\n\n\tif (mr->page_size_mask & (1<<PG_LEVEL_1G))\n\t\treturn str_1g;\n\t \n\tif (IS_ENABLED(CONFIG_X86_32) &&\n\t    !IS_ENABLED(CONFIG_X86_PAE) &&\n\t    mr->page_size_mask & (1<<PG_LEVEL_2M))\n\t\treturn str_4m;\n\n\tif (mr->page_size_mask & (1<<PG_LEVEL_2M))\n\t\treturn str_2m;\n\n\treturn str_4k;\n}\n\nstatic int __meminit split_mem_range(struct map_range *mr, int nr_range,\n\t\t\t\t     unsigned long start,\n\t\t\t\t     unsigned long end)\n{\n\tunsigned long start_pfn, end_pfn, limit_pfn;\n\tunsigned long pfn;\n\tint i;\n\n\tlimit_pfn = PFN_DOWN(end);\n\n\t \n\tpfn = start_pfn = PFN_DOWN(start);\n#ifdef CONFIG_X86_32\n\t \n\tif (pfn == 0)\n\t\tend_pfn = PFN_DOWN(PMD_SIZE);\n\telse\n\t\tend_pfn = round_up(pfn, PFN_DOWN(PMD_SIZE));\n#else  \n\tend_pfn = round_up(pfn, PFN_DOWN(PMD_SIZE));\n#endif\n\tif (end_pfn > limit_pfn)\n\t\tend_pfn = limit_pfn;\n\tif (start_pfn < end_pfn) {\n\t\tnr_range = save_mr(mr, nr_range, start_pfn, end_pfn, 0);\n\t\tpfn = end_pfn;\n\t}\n\n\t \n\tstart_pfn = round_up(pfn, PFN_DOWN(PMD_SIZE));\n#ifdef CONFIG_X86_32\n\tend_pfn = round_down(limit_pfn, PFN_DOWN(PMD_SIZE));\n#else  \n\tend_pfn = round_up(pfn, PFN_DOWN(PUD_SIZE));\n\tif (end_pfn > round_down(limit_pfn, PFN_DOWN(PMD_SIZE)))\n\t\tend_pfn = round_down(limit_pfn, PFN_DOWN(PMD_SIZE));\n#endif\n\n\tif (start_pfn < end_pfn) {\n\t\tnr_range = save_mr(mr, nr_range, start_pfn, end_pfn,\n\t\t\t\tpage_size_mask & (1<<PG_LEVEL_2M));\n\t\tpfn = end_pfn;\n\t}\n\n#ifdef CONFIG_X86_64\n\t \n\tstart_pfn = round_up(pfn, PFN_DOWN(PUD_SIZE));\n\tend_pfn = round_down(limit_pfn, PFN_DOWN(PUD_SIZE));\n\tif (start_pfn < end_pfn) {\n\t\tnr_range = save_mr(mr, nr_range, start_pfn, end_pfn,\n\t\t\t\tpage_size_mask &\n\t\t\t\t ((1<<PG_LEVEL_2M)|(1<<PG_LEVEL_1G)));\n\t\tpfn = end_pfn;\n\t}\n\n\t \n\tstart_pfn = round_up(pfn, PFN_DOWN(PMD_SIZE));\n\tend_pfn = round_down(limit_pfn, PFN_DOWN(PMD_SIZE));\n\tif (start_pfn < end_pfn) {\n\t\tnr_range = save_mr(mr, nr_range, start_pfn, end_pfn,\n\t\t\t\tpage_size_mask & (1<<PG_LEVEL_2M));\n\t\tpfn = end_pfn;\n\t}\n#endif\n\n\t \n\tstart_pfn = pfn;\n\tend_pfn = limit_pfn;\n\tnr_range = save_mr(mr, nr_range, start_pfn, end_pfn, 0);\n\n\tif (!after_bootmem)\n\t\tadjust_range_page_size_mask(mr, nr_range);\n\n\t \n\tfor (i = 0; nr_range > 1 && i < nr_range - 1; i++) {\n\t\tunsigned long old_start;\n\t\tif (mr[i].end != mr[i+1].start ||\n\t\t    mr[i].page_size_mask != mr[i+1].page_size_mask)\n\t\t\tcontinue;\n\t\t \n\t\told_start = mr[i].start;\n\t\tmemmove(&mr[i], &mr[i+1],\n\t\t\t(nr_range - 1 - i) * sizeof(struct map_range));\n\t\tmr[i--].start = old_start;\n\t\tnr_range--;\n\t}\n\n\tfor (i = 0; i < nr_range; i++)\n\t\tpr_debug(\" [mem %#010lx-%#010lx] page %s\\n\",\n\t\t\t\tmr[i].start, mr[i].end - 1,\n\t\t\t\tpage_size_string(&mr[i]));\n\n\treturn nr_range;\n}\n\nstruct range pfn_mapped[E820_MAX_ENTRIES];\nint nr_pfn_mapped;\n\nstatic void add_pfn_range_mapped(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tnr_pfn_mapped = add_range_with_merge(pfn_mapped, E820_MAX_ENTRIES,\n\t\t\t\t\t     nr_pfn_mapped, start_pfn, end_pfn);\n\tnr_pfn_mapped = clean_sort_range(pfn_mapped, E820_MAX_ENTRIES);\n\n\tmax_pfn_mapped = max(max_pfn_mapped, end_pfn);\n\n\tif (start_pfn < (1UL<<(32-PAGE_SHIFT)))\n\t\tmax_low_pfn_mapped = max(max_low_pfn_mapped,\n\t\t\t\t\t min(end_pfn, 1UL<<(32-PAGE_SHIFT)));\n}\n\nbool pfn_range_is_mapped(unsigned long start_pfn, unsigned long end_pfn)\n{\n\tint i;\n\n\tfor (i = 0; i < nr_pfn_mapped; i++)\n\t\tif ((start_pfn >= pfn_mapped[i].start) &&\n\t\t    (end_pfn <= pfn_mapped[i].end))\n\t\t\treturn true;\n\n\treturn false;\n}\n\n \nunsigned long __ref init_memory_mapping(unsigned long start,\n\t\t\t\t\tunsigned long end, pgprot_t prot)\n{\n\tstruct map_range mr[NR_RANGE_MR];\n\tunsigned long ret = 0;\n\tint nr_range, i;\n\n\tpr_debug(\"init_memory_mapping: [mem %#010lx-%#010lx]\\n\",\n\t       start, end - 1);\n\n\tmemset(mr, 0, sizeof(mr));\n\tnr_range = split_mem_range(mr, 0, start, end);\n\n\tfor (i = 0; i < nr_range; i++)\n\t\tret = kernel_physical_mapping_init(mr[i].start, mr[i].end,\n\t\t\t\t\t\t   mr[i].page_size_mask,\n\t\t\t\t\t\t   prot);\n\n\tadd_pfn_range_mapped(start >> PAGE_SHIFT, ret >> PAGE_SHIFT);\n\n\treturn ret >> PAGE_SHIFT;\n}\n\n \nstatic unsigned long __init init_range_memory_mapping(\n\t\t\t\t\t   unsigned long r_start,\n\t\t\t\t\t   unsigned long r_end)\n{\n\tunsigned long start_pfn, end_pfn;\n\tunsigned long mapped_ram_size = 0;\n\tint i;\n\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, NULL) {\n\t\tu64 start = clamp_val(PFN_PHYS(start_pfn), r_start, r_end);\n\t\tu64 end = clamp_val(PFN_PHYS(end_pfn), r_start, r_end);\n\t\tif (start >= end)\n\t\t\tcontinue;\n\n\t\t \n\t\tcan_use_brk_pgt = max(start, (u64)pgt_buf_end<<PAGE_SHIFT) >=\n\t\t\t\t    min(end, (u64)pgt_buf_top<<PAGE_SHIFT);\n\t\tinit_memory_mapping(start, end, PAGE_KERNEL);\n\t\tmapped_ram_size += end - start;\n\t\tcan_use_brk_pgt = true;\n\t}\n\n\treturn mapped_ram_size;\n}\n\nstatic unsigned long __init get_new_step_size(unsigned long step_size)\n{\n\t \n\treturn step_size << (PMD_SHIFT - PAGE_SHIFT - 1);\n}\n\n \nstatic void __init memory_map_top_down(unsigned long map_start,\n\t\t\t\t       unsigned long map_end)\n{\n\tunsigned long real_end, last_start;\n\tunsigned long step_size;\n\tunsigned long addr;\n\tunsigned long mapped_ram_size = 0;\n\n\t \n\taddr = memblock_phys_alloc_range(PMD_SIZE, PMD_SIZE, map_start,\n\t\t\t\t\t map_end);\n\tmemblock_phys_free(addr, PMD_SIZE);\n\treal_end = addr + PMD_SIZE;\n\n\t \n\tstep_size = PMD_SIZE;\n\tmax_pfn_mapped = 0;  \n\tmin_pfn_mapped = real_end >> PAGE_SHIFT;\n\tlast_start = real_end;\n\n\t \n\twhile (last_start > map_start) {\n\t\tunsigned long start;\n\n\t\tif (last_start > step_size) {\n\t\t\tstart = round_down(last_start - 1, step_size);\n\t\t\tif (start < map_start)\n\t\t\t\tstart = map_start;\n\t\t} else\n\t\t\tstart = map_start;\n\t\tmapped_ram_size += init_range_memory_mapping(start,\n\t\t\t\t\t\t\tlast_start);\n\t\tlast_start = start;\n\t\tmin_pfn_mapped = last_start >> PAGE_SHIFT;\n\t\tif (mapped_ram_size >= step_size)\n\t\t\tstep_size = get_new_step_size(step_size);\n\t}\n\n\tif (real_end < map_end)\n\t\tinit_range_memory_mapping(real_end, map_end);\n}\n\n \nstatic void __init memory_map_bottom_up(unsigned long map_start,\n\t\t\t\t\tunsigned long map_end)\n{\n\tunsigned long next, start;\n\tunsigned long mapped_ram_size = 0;\n\t \n\tunsigned long step_size = PMD_SIZE;\n\n\tstart = map_start;\n\tmin_pfn_mapped = start >> PAGE_SHIFT;\n\n\t \n\twhile (start < map_end) {\n\t\tif (step_size && map_end - start > step_size) {\n\t\t\tnext = round_up(start + 1, step_size);\n\t\t\tif (next > map_end)\n\t\t\t\tnext = map_end;\n\t\t} else {\n\t\t\tnext = map_end;\n\t\t}\n\n\t\tmapped_ram_size += init_range_memory_mapping(start, next);\n\t\tstart = next;\n\n\t\tif (mapped_ram_size >= step_size)\n\t\t\tstep_size = get_new_step_size(step_size);\n\t}\n}\n\n \nstatic void __init init_trampoline(void)\n{\n#ifdef CONFIG_X86_64\n\t \n\tif (!kaslr_memory_enabled())\n\t\ttrampoline_pgd_entry = init_top_pgt[pgd_index(__PAGE_OFFSET)];\n\telse\n\t\tinit_trampoline_kaslr();\n#endif\n}\n\nvoid __init init_mem_mapping(void)\n{\n\tunsigned long end;\n\n\tpti_check_boottime_disable();\n\tprobe_page_size_mask();\n\tsetup_pcid();\n\n#ifdef CONFIG_X86_64\n\tend = max_pfn << PAGE_SHIFT;\n#else\n\tend = max_low_pfn << PAGE_SHIFT;\n#endif\n\n\t \n\tinit_memory_mapping(0, ISA_END_ADDRESS, PAGE_KERNEL);\n\n\t \n\tinit_trampoline();\n\n\t \n\tif (memblock_bottom_up()) {\n\t\tunsigned long kernel_end = __pa_symbol(_end);\n\n\t\t \n\t\tmemory_map_bottom_up(kernel_end, end);\n\t\tmemory_map_bottom_up(ISA_END_ADDRESS, kernel_end);\n\t} else {\n\t\tmemory_map_top_down(ISA_END_ADDRESS, end);\n\t}\n\n#ifdef CONFIG_X86_64\n\tif (max_pfn > max_low_pfn) {\n\t\t \n\t\tmax_low_pfn = max_pfn;\n\t}\n#else\n\tearly_ioremap_page_table_range_init();\n#endif\n\n\tload_cr3(swapper_pg_dir);\n\t__flush_tlb_all();\n\n\tx86_init.hyper.init_mem_mapping();\n\n\tearly_memtest(0, max_pfn_mapped << PAGE_SHIFT);\n}\n\n \nvoid __init poking_init(void)\n{\n\tspinlock_t *ptl;\n\tpte_t *ptep;\n\n\tpoking_mm = mm_alloc();\n\tBUG_ON(!poking_mm);\n\n\t \n\tparavirt_enter_mmap(poking_mm);\n\n\t \n\tpoking_addr = TASK_UNMAPPED_BASE;\n\tif (IS_ENABLED(CONFIG_RANDOMIZE_BASE))\n\t\tpoking_addr += (kaslr_get_random_long(\"Poking\") & PAGE_MASK) %\n\t\t\t(TASK_SIZE - TASK_UNMAPPED_BASE - 3 * PAGE_SIZE);\n\n\tif (((poking_addr + PAGE_SIZE) & ~PMD_MASK) == 0)\n\t\tpoking_addr += PAGE_SIZE;\n\n\t \n\tptep = get_locked_pte(poking_mm, poking_addr, &ptl);\n\tBUG_ON(!ptep);\n\tpte_unmap_unlock(ptep, ptl);\n}\n\n \nint devmem_is_allowed(unsigned long pagenr)\n{\n\tif (region_intersects(PFN_PHYS(pagenr), PAGE_SIZE,\n\t\t\t\tIORESOURCE_SYSTEM_RAM, IORES_DESC_NONE)\n\t\t\t!= REGION_DISJOINT) {\n\t\t \n\t\tif (pagenr < 256)\n\t\t\treturn 2;\n\n\t\treturn 0;\n\t}\n\n\t \n\tif (iomem_is_exclusive(pagenr << PAGE_SHIFT)) {\n\t\t \n\t\tif (pagenr < 256)\n\t\t\treturn 1;\n\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nvoid free_init_pages(const char *what, unsigned long begin, unsigned long end)\n{\n\tunsigned long begin_aligned, end_aligned;\n\n\t \n\tbegin_aligned = PAGE_ALIGN(begin);\n\tend_aligned   = end & PAGE_MASK;\n\n\tif (WARN_ON(begin_aligned != begin || end_aligned != end)) {\n\t\tbegin = begin_aligned;\n\t\tend   = end_aligned;\n\t}\n\n\tif (begin >= end)\n\t\treturn;\n\n\t \n\tif (debug_pagealloc_enabled()) {\n\t\tpr_info(\"debug: unmapping init [mem %#010lx-%#010lx]\\n\",\n\t\t\tbegin, end - 1);\n\t\t \n\t\tkmemleak_free_part((void *)begin, end - begin);\n\t\tset_memory_np(begin, (end - begin) >> PAGE_SHIFT);\n\t} else {\n\t\t \n\t\tset_memory_nx(begin, (end - begin) >> PAGE_SHIFT);\n\t\tset_memory_rw(begin, (end - begin) >> PAGE_SHIFT);\n\n\t\tfree_reserved_area((void *)begin, (void *)end,\n\t\t\t\t   POISON_FREE_INITMEM, what);\n\t}\n}\n\n \nvoid free_kernel_image_pages(const char *what, void *begin, void *end)\n{\n\tunsigned long begin_ul = (unsigned long)begin;\n\tunsigned long end_ul = (unsigned long)end;\n\tunsigned long len_pages = (end_ul - begin_ul) >> PAGE_SHIFT;\n\n\tfree_init_pages(what, begin_ul, end_ul);\n\n\t \n\tif (IS_ENABLED(CONFIG_X86_64) && cpu_feature_enabled(X86_FEATURE_PTI))\n\t\tset_memory_np_noalias(begin_ul, len_pages);\n}\n\nvoid __ref free_initmem(void)\n{\n\te820__reallocate_tables();\n\n\tmem_encrypt_free_decrypted_mem();\n\n\tfree_kernel_image_pages(\"unused kernel image (initmem)\",\n\t\t\t\t&__init_begin, &__init_end);\n}\n\n#ifdef CONFIG_BLK_DEV_INITRD\nvoid __init free_initrd_mem(unsigned long start, unsigned long end)\n{\n\t \n\tfree_init_pages(\"initrd\", start, PAGE_ALIGN(end));\n}\n#endif\n\n \nvoid __init memblock_find_dma_reserve(void)\n{\n#ifdef CONFIG_X86_64\n\tu64 nr_pages = 0, nr_free_pages = 0;\n\tunsigned long start_pfn, end_pfn;\n\tphys_addr_t start_addr, end_addr;\n\tint i;\n\tu64 u;\n\n\t \n\tnr_pages = 0;\n\tfor_each_mem_pfn_range(i, MAX_NUMNODES, &start_pfn, &end_pfn, NULL) {\n\t\tstart_pfn = min(start_pfn, MAX_DMA_PFN);\n\t\tend_pfn   = min(end_pfn,   MAX_DMA_PFN);\n\n\t\tnr_pages += end_pfn - start_pfn;\n\t}\n\n\t \n\tnr_free_pages = 0;\n\tfor_each_free_mem_range(u, NUMA_NO_NODE, MEMBLOCK_NONE, &start_addr, &end_addr, NULL) {\n\t\tstart_pfn = min_t(unsigned long, PFN_UP(start_addr), MAX_DMA_PFN);\n\t\tend_pfn   = min_t(unsigned long, PFN_DOWN(end_addr), MAX_DMA_PFN);\n\n\t\tif (start_pfn < end_pfn)\n\t\t\tnr_free_pages += end_pfn - start_pfn;\n\t}\n\n\tset_dma_reserve(nr_pages - nr_free_pages);\n#endif\n}\n\nvoid __init zone_sizes_init(void)\n{\n\tunsigned long max_zone_pfns[MAX_NR_ZONES];\n\n\tmemset(max_zone_pfns, 0, sizeof(max_zone_pfns));\n\n#ifdef CONFIG_ZONE_DMA\n\tmax_zone_pfns[ZONE_DMA]\t\t= min(MAX_DMA_PFN, max_low_pfn);\n#endif\n#ifdef CONFIG_ZONE_DMA32\n\tmax_zone_pfns[ZONE_DMA32]\t= min(MAX_DMA32_PFN, max_low_pfn);\n#endif\n\tmax_zone_pfns[ZONE_NORMAL]\t= max_low_pfn;\n#ifdef CONFIG_HIGHMEM\n\tmax_zone_pfns[ZONE_HIGHMEM]\t= max_pfn;\n#endif\n\n\tfree_area_init(max_zone_pfns);\n}\n\n__visible DEFINE_PER_CPU_ALIGNED(struct tlb_state, cpu_tlbstate) = {\n\t.loaded_mm = &init_mm,\n\t.next_asid = 1,\n\t.cr4 = ~0UL,\t \n};\n\n#ifdef CONFIG_ADDRESS_MASKING\nDEFINE_PER_CPU(u64, tlbstate_untag_mask);\nEXPORT_PER_CPU_SYMBOL(tlbstate_untag_mask);\n#endif\n\nvoid update_cache_mode_entry(unsigned entry, enum page_cache_mode cache)\n{\n\t \n\tBUG_ON(!entry && cache != _PAGE_CACHE_MODE_WB);\n\n\t__cachemode2pte_tbl[cache] = __cm_idx2pte(entry);\n\t__pte2cachemode_tbl[entry] = cache;\n}\n\n#ifdef CONFIG_SWAP\nunsigned long arch_max_swapfile_size(void)\n{\n\tunsigned long pages;\n\n\tpages = generic_max_swapfile_size();\n\n\tif (boot_cpu_has_bug(X86_BUG_L1TF) && l1tf_mitigation != L1TF_MITIGATION_OFF) {\n\t\t \n\t\tunsigned long long l1tf_limit = l1tf_pfn_limit();\n\t\t \n#if CONFIG_PGTABLE_LEVELS > 2\n\t\tl1tf_limit <<= PAGE_SHIFT - SWP_OFFSET_FIRST_BIT;\n#endif\n\t\tpages = min_t(unsigned long long, l1tf_limit, pages);\n\t}\n\treturn pages;\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}