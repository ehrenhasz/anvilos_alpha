{
  "module_name": "cpu_entry_area.c",
  "hash_id": "c9048c7861c6042cb6e2026762b7938669b660c0f3385fe09d218f7c0075c484",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/mm/cpu_entry_area.c",
  "human_readable_source": "\n\n#include <linux/spinlock.h>\n#include <linux/percpu.h>\n#include <linux/kallsyms.h>\n#include <linux/kcore.h>\n#include <linux/pgtable.h>\n\n#include <asm/cpu_entry_area.h>\n#include <asm/fixmap.h>\n#include <asm/desc.h>\n#include <asm/kasan.h>\n#include <asm/setup.h>\n\nstatic DEFINE_PER_CPU_PAGE_ALIGNED(struct entry_stack_page, entry_stack_storage);\n\n#ifdef CONFIG_X86_64\nstatic DEFINE_PER_CPU_PAGE_ALIGNED(struct exception_stacks, exception_stacks);\nDEFINE_PER_CPU(struct cea_exception_stacks*, cea_exception_stacks);\n\nstatic DEFINE_PER_CPU_READ_MOSTLY(unsigned long, _cea_offset);\n\nstatic __always_inline unsigned int cea_offset(unsigned int cpu)\n{\n\treturn per_cpu(_cea_offset, cpu);\n}\n\nstatic __init void init_cea_offsets(void)\n{\n\tunsigned int max_cea;\n\tunsigned int i, j;\n\n\tif (!kaslr_enabled()) {\n\t\tfor_each_possible_cpu(i)\n\t\t\tper_cpu(_cea_offset, i) = i;\n\t\treturn;\n\t}\n\n\tmax_cea = (CPU_ENTRY_AREA_MAP_SIZE - PAGE_SIZE) / CPU_ENTRY_AREA_SIZE;\n\n\t \n\tfor_each_possible_cpu(i) {\n\t\tunsigned int cea;\n\nagain:\n\t\tcea = get_random_u32_below(max_cea);\n\n\t\tfor_each_possible_cpu(j) {\n\t\t\tif (cea_offset(j) == cea)\n\t\t\t\tgoto again;\n\n\t\t\tif (i == j)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tper_cpu(_cea_offset, i) = cea;\n\t}\n}\n#else  \nDECLARE_PER_CPU_PAGE_ALIGNED(struct doublefault_stack, doublefault_stack);\n\nstatic __always_inline unsigned int cea_offset(unsigned int cpu)\n{\n\treturn cpu;\n}\nstatic inline void init_cea_offsets(void) { }\n#endif\n\n \nnoinstr struct cpu_entry_area *get_cpu_entry_area(int cpu)\n{\n\tunsigned long va = CPU_ENTRY_AREA_PER_CPU + cea_offset(cpu) * CPU_ENTRY_AREA_SIZE;\n\tBUILD_BUG_ON(sizeof(struct cpu_entry_area) % PAGE_SIZE != 0);\n\n\treturn (struct cpu_entry_area *) va;\n}\nEXPORT_SYMBOL(get_cpu_entry_area);\n\nvoid cea_set_pte(void *cea_vaddr, phys_addr_t pa, pgprot_t flags)\n{\n\tunsigned long va = (unsigned long) cea_vaddr;\n\tpte_t pte = pfn_pte(pa >> PAGE_SHIFT, flags);\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_PGE) &&\n\t    (pgprot_val(flags) & _PAGE_PRESENT))\n\t\tpte = pte_set_flags(pte, _PAGE_GLOBAL);\n\n\tset_pte_vaddr(va, pte);\n}\n\nstatic void __init\ncea_map_percpu_pages(void *cea_vaddr, void *ptr, int pages, pgprot_t prot)\n{\n\tfor ( ; pages; pages--, cea_vaddr+= PAGE_SIZE, ptr += PAGE_SIZE)\n\t\tcea_set_pte(cea_vaddr, per_cpu_ptr_to_phys(ptr), prot);\n}\n\nstatic void __init percpu_setup_debug_store(unsigned int cpu)\n{\n#ifdef CONFIG_CPU_SUP_INTEL\n\tunsigned int npages;\n\tvoid *cea;\n\n\tif (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL)\n\t\treturn;\n\n\tcea = &get_cpu_entry_area(cpu)->cpu_debug_store;\n\tnpages = sizeof(struct debug_store) / PAGE_SIZE;\n\tBUILD_BUG_ON(sizeof(struct debug_store) % PAGE_SIZE != 0);\n\tcea_map_percpu_pages(cea, &per_cpu(cpu_debug_store, cpu), npages,\n\t\t\t     PAGE_KERNEL);\n\n\tcea = &get_cpu_entry_area(cpu)->cpu_debug_buffers;\n\t \n\tnpages = sizeof(struct debug_store_buffers) / PAGE_SIZE;\n\tfor (; npages; npages--, cea += PAGE_SIZE)\n\t\tcea_set_pte(cea, 0, PAGE_NONE);\n#endif\n}\n\n#ifdef CONFIG_X86_64\n\n#define cea_map_stack(name) do {\t\t\t\t\t\\\n\tnpages = sizeof(estacks->name## _stack) / PAGE_SIZE;\t\t\\\n\tcea_map_percpu_pages(cea->estacks.name## _stack,\t\t\\\n\t\t\testacks->name## _stack, npages, PAGE_KERNEL);\t\\\n\t} while (0)\n\nstatic void __init percpu_setup_exception_stacks(unsigned int cpu)\n{\n\tstruct exception_stacks *estacks = per_cpu_ptr(&exception_stacks, cpu);\n\tstruct cpu_entry_area *cea = get_cpu_entry_area(cpu);\n\tunsigned int npages;\n\n\tBUILD_BUG_ON(sizeof(exception_stacks) % PAGE_SIZE != 0);\n\n\tper_cpu(cea_exception_stacks, cpu) = &cea->estacks;\n\n\t \n\tcea_map_stack(DF);\n\tcea_map_stack(NMI);\n\tcea_map_stack(DB);\n\tcea_map_stack(MCE);\n\n\tif (IS_ENABLED(CONFIG_AMD_MEM_ENCRYPT)) {\n\t\tif (cc_platform_has(CC_ATTR_GUEST_STATE_ENCRYPT)) {\n\t\t\tcea_map_stack(VC);\n\t\t\tcea_map_stack(VC2);\n\t\t}\n\t}\n}\n#else\nstatic inline void percpu_setup_exception_stacks(unsigned int cpu)\n{\n\tstruct cpu_entry_area *cea = get_cpu_entry_area(cpu);\n\n\tcea_map_percpu_pages(&cea->doublefault_stack,\n\t\t\t     &per_cpu(doublefault_stack, cpu), 1, PAGE_KERNEL);\n}\n#endif\n\n \nstatic void __init setup_cpu_entry_area(unsigned int cpu)\n{\n\tstruct cpu_entry_area *cea = get_cpu_entry_area(cpu);\n#ifdef CONFIG_X86_64\n\t \n\tpgprot_t gdt_prot = PAGE_KERNEL_RO;\n\tpgprot_t tss_prot = PAGE_KERNEL_RO;\n#else\n\t \n\tpgprot_t gdt_prot = PAGE_KERNEL;\n\tpgprot_t tss_prot = PAGE_KERNEL;\n#endif\n\n\tkasan_populate_shadow_for_vaddr(cea, CPU_ENTRY_AREA_SIZE,\n\t\t\t\t\tearly_cpu_to_node(cpu));\n\n\tcea_set_pte(&cea->gdt, get_cpu_gdt_paddr(cpu), gdt_prot);\n\n\tcea_map_percpu_pages(&cea->entry_stack_page,\n\t\t\t     per_cpu_ptr(&entry_stack_storage, cpu), 1,\n\t\t\t     PAGE_KERNEL);\n\n\t \n\tBUILD_BUG_ON((offsetof(struct tss_struct, x86_tss) ^\n\t\t      offsetofend(struct tss_struct, x86_tss)) & PAGE_MASK);\n\tBUILD_BUG_ON(sizeof(struct tss_struct) % PAGE_SIZE != 0);\n\t \n\tBUILD_BUG_ON(offsetof(struct tss_struct, x86_tss) != 0);\n\tBUILD_BUG_ON(sizeof(struct x86_hw_tss) != 0x68);\n\n\tcea_map_percpu_pages(&cea->tss, &per_cpu(cpu_tss_rw, cpu),\n\t\t\t     sizeof(struct tss_struct) / PAGE_SIZE, tss_prot);\n\n#ifdef CONFIG_X86_32\n\tper_cpu(cpu_entry_area, cpu) = cea;\n#endif\n\n\tpercpu_setup_exception_stacks(cpu);\n\n\tpercpu_setup_debug_store(cpu);\n}\n\nstatic __init void setup_cpu_entry_area_ptes(void)\n{\n#ifdef CONFIG_X86_32\n\tunsigned long start, end;\n\n\t \n\tBUILD_BUG_ON((CPU_ENTRY_AREA_PAGES+1)*PAGE_SIZE != CPU_ENTRY_AREA_MAP_SIZE);\n\tBUG_ON(CPU_ENTRY_AREA_BASE & ~PMD_MASK);\n\n\tstart = CPU_ENTRY_AREA_BASE;\n\tend = start + CPU_ENTRY_AREA_MAP_SIZE;\n\n\t \n\tfor (; start < end && start >= CPU_ENTRY_AREA_BASE; start += PMD_SIZE)\n\t\tpopulate_extra_pte(start);\n#endif\n}\n\nvoid __init setup_cpu_entry_areas(void)\n{\n\tunsigned int cpu;\n\n\tinit_cea_offsets();\n\n\tsetup_cpu_entry_area_ptes();\n\n\tfor_each_possible_cpu(cpu)\n\t\tsetup_cpu_entry_area(cpu);\n\n\t \n\tsync_initial_page_table();\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}