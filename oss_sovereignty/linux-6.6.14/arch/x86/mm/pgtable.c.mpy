{
  "module_name": "pgtable.c",
  "hash_id": "e3cffcc67996cfb1228891a4ecc423539a78acf590bde93e278d05949f2452b3",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/mm/pgtable.c",
  "human_readable_source": "\n#include <linux/mm.h>\n#include <linux/gfp.h>\n#include <linux/hugetlb.h>\n#include <asm/pgalloc.h>\n#include <asm/tlb.h>\n#include <asm/fixmap.h>\n#include <asm/mtrr.h>\n\n#ifdef CONFIG_DYNAMIC_PHYSICAL_MASK\nphys_addr_t physical_mask __ro_after_init = (1ULL << __PHYSICAL_MASK_SHIFT) - 1;\nEXPORT_SYMBOL(physical_mask);\n#endif\n\n#ifdef CONFIG_HIGHPTE\n#define PGTABLE_HIGHMEM __GFP_HIGHMEM\n#else\n#define PGTABLE_HIGHMEM 0\n#endif\n\n#ifndef CONFIG_PARAVIRT\nstatic inline\nvoid paravirt_tlb_remove_table(struct mmu_gather *tlb, void *table)\n{\n\ttlb_remove_page(tlb, table);\n}\n#endif\n\ngfp_t __userpte_alloc_gfp = GFP_PGTABLE_USER | PGTABLE_HIGHMEM;\n\npgtable_t pte_alloc_one(struct mm_struct *mm)\n{\n\treturn __pte_alloc_one(mm, __userpte_alloc_gfp);\n}\n\nstatic int __init setup_userpte(char *arg)\n{\n\tif (!arg)\n\t\treturn -EINVAL;\n\n\t \n\tif (strcmp(arg, \"nohigh\") == 0)\n\t\t__userpte_alloc_gfp &= ~__GFP_HIGHMEM;\n\telse\n\t\treturn -EINVAL;\n\treturn 0;\n}\nearly_param(\"userpte\", setup_userpte);\n\nvoid ___pte_free_tlb(struct mmu_gather *tlb, struct page *pte)\n{\n\tpagetable_pte_dtor(page_ptdesc(pte));\n\tparavirt_release_pte(page_to_pfn(pte));\n\tparavirt_tlb_remove_table(tlb, pte);\n}\n\n#if CONFIG_PGTABLE_LEVELS > 2\nvoid ___pmd_free_tlb(struct mmu_gather *tlb, pmd_t *pmd)\n{\n\tstruct ptdesc *ptdesc = virt_to_ptdesc(pmd);\n\tparavirt_release_pmd(__pa(pmd) >> PAGE_SHIFT);\n\t \n#ifdef CONFIG_X86_PAE\n\ttlb->need_flush_all = 1;\n#endif\n\tpagetable_pmd_dtor(ptdesc);\n\tparavirt_tlb_remove_table(tlb, ptdesc_page(ptdesc));\n}\n\n#if CONFIG_PGTABLE_LEVELS > 3\nvoid ___pud_free_tlb(struct mmu_gather *tlb, pud_t *pud)\n{\n\tparavirt_release_pud(__pa(pud) >> PAGE_SHIFT);\n\tparavirt_tlb_remove_table(tlb, virt_to_page(pud));\n}\n\n#if CONFIG_PGTABLE_LEVELS > 4\nvoid ___p4d_free_tlb(struct mmu_gather *tlb, p4d_t *p4d)\n{\n\tparavirt_release_p4d(__pa(p4d) >> PAGE_SHIFT);\n\tparavirt_tlb_remove_table(tlb, virt_to_page(p4d));\n}\n#endif\t \n#endif\t \n#endif\t \n\nstatic inline void pgd_list_add(pgd_t *pgd)\n{\n\tstruct ptdesc *ptdesc = virt_to_ptdesc(pgd);\n\n\tlist_add(&ptdesc->pt_list, &pgd_list);\n}\n\nstatic inline void pgd_list_del(pgd_t *pgd)\n{\n\tstruct ptdesc *ptdesc = virt_to_ptdesc(pgd);\n\n\tlist_del(&ptdesc->pt_list);\n}\n\n#define UNSHARED_PTRS_PER_PGD\t\t\t\t\\\n\t(SHARED_KERNEL_PMD ? KERNEL_PGD_BOUNDARY : PTRS_PER_PGD)\n#define MAX_UNSHARED_PTRS_PER_PGD\t\t\t\\\n\tmax_t(size_t, KERNEL_PGD_BOUNDARY, PTRS_PER_PGD)\n\n\nstatic void pgd_set_mm(pgd_t *pgd, struct mm_struct *mm)\n{\n\tvirt_to_ptdesc(pgd)->pt_mm = mm;\n}\n\nstruct mm_struct *pgd_page_get_mm(struct page *page)\n{\n\treturn page_ptdesc(page)->pt_mm;\n}\n\nstatic void pgd_ctor(struct mm_struct *mm, pgd_t *pgd)\n{\n\t \n\tif (CONFIG_PGTABLE_LEVELS == 2 ||\n\t    (CONFIG_PGTABLE_LEVELS == 3 && SHARED_KERNEL_PMD) ||\n\t    CONFIG_PGTABLE_LEVELS >= 4) {\n\t\tclone_pgd_range(pgd + KERNEL_PGD_BOUNDARY,\n\t\t\t\tswapper_pg_dir + KERNEL_PGD_BOUNDARY,\n\t\t\t\tKERNEL_PGD_PTRS);\n\t}\n\n\t \n\tif (!SHARED_KERNEL_PMD) {\n\t\tpgd_set_mm(pgd, mm);\n\t\tpgd_list_add(pgd);\n\t}\n}\n\nstatic void pgd_dtor(pgd_t *pgd)\n{\n\tif (SHARED_KERNEL_PMD)\n\t\treturn;\n\n\tspin_lock(&pgd_lock);\n\tpgd_list_del(pgd);\n\tspin_unlock(&pgd_lock);\n}\n\n \n\n#ifdef CONFIG_X86_PAE\n \n#define PREALLOCATED_PMDS\tUNSHARED_PTRS_PER_PGD\n#define MAX_PREALLOCATED_PMDS\tMAX_UNSHARED_PTRS_PER_PGD\n\n \n#define PREALLOCATED_USER_PMDS\t (boot_cpu_has(X86_FEATURE_PTI) ? \\\n\t\t\t\t\tKERNEL_PGD_PTRS : 0)\n#define MAX_PREALLOCATED_USER_PMDS KERNEL_PGD_PTRS\n\nvoid pud_populate(struct mm_struct *mm, pud_t *pudp, pmd_t *pmd)\n{\n\tparavirt_alloc_pmd(mm, __pa(pmd) >> PAGE_SHIFT);\n\n\t \n\tset_pud(pudp, __pud(__pa(pmd) | _PAGE_PRESENT));\n\n\t \n\tflush_tlb_mm(mm);\n}\n#else   \n\n \n#define PREALLOCATED_PMDS\t0\n#define MAX_PREALLOCATED_PMDS\t0\n#define PREALLOCATED_USER_PMDS\t 0\n#define MAX_PREALLOCATED_USER_PMDS 0\n#endif\t \n\nstatic void free_pmds(struct mm_struct *mm, pmd_t *pmds[], int count)\n{\n\tint i;\n\tstruct ptdesc *ptdesc;\n\n\tfor (i = 0; i < count; i++)\n\t\tif (pmds[i]) {\n\t\t\tptdesc = virt_to_ptdesc(pmds[i]);\n\n\t\t\tpagetable_pmd_dtor(ptdesc);\n\t\t\tpagetable_free(ptdesc);\n\t\t\tmm_dec_nr_pmds(mm);\n\t\t}\n}\n\nstatic int preallocate_pmds(struct mm_struct *mm, pmd_t *pmds[], int count)\n{\n\tint i;\n\tbool failed = false;\n\tgfp_t gfp = GFP_PGTABLE_USER;\n\n\tif (mm == &init_mm)\n\t\tgfp &= ~__GFP_ACCOUNT;\n\tgfp &= ~__GFP_HIGHMEM;\n\n\tfor (i = 0; i < count; i++) {\n\t\tpmd_t *pmd = NULL;\n\t\tstruct ptdesc *ptdesc = pagetable_alloc(gfp, 0);\n\n\t\tif (!ptdesc)\n\t\t\tfailed = true;\n\t\tif (ptdesc && !pagetable_pmd_ctor(ptdesc)) {\n\t\t\tpagetable_free(ptdesc);\n\t\t\tptdesc = NULL;\n\t\t\tfailed = true;\n\t\t}\n\t\tif (ptdesc) {\n\t\t\tmm_inc_nr_pmds(mm);\n\t\t\tpmd = ptdesc_address(ptdesc);\n\t\t}\n\n\t\tpmds[i] = pmd;\n\t}\n\n\tif (failed) {\n\t\tfree_pmds(mm, pmds, count);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void mop_up_one_pmd(struct mm_struct *mm, pgd_t *pgdp)\n{\n\tpgd_t pgd = *pgdp;\n\n\tif (pgd_val(pgd) != 0) {\n\t\tpmd_t *pmd = (pmd_t *)pgd_page_vaddr(pgd);\n\n\t\tpgd_clear(pgdp);\n\n\t\tparavirt_release_pmd(pgd_val(pgd) >> PAGE_SHIFT);\n\t\tpmd_free(mm, pmd);\n\t\tmm_dec_nr_pmds(mm);\n\t}\n}\n\nstatic void pgd_mop_up_pmds(struct mm_struct *mm, pgd_t *pgdp)\n{\n\tint i;\n\n\tfor (i = 0; i < PREALLOCATED_PMDS; i++)\n\t\tmop_up_one_pmd(mm, &pgdp[i]);\n\n#ifdef CONFIG_PAGE_TABLE_ISOLATION\n\n\tif (!boot_cpu_has(X86_FEATURE_PTI))\n\t\treturn;\n\n\tpgdp = kernel_to_user_pgdp(pgdp);\n\n\tfor (i = 0; i < PREALLOCATED_USER_PMDS; i++)\n\t\tmop_up_one_pmd(mm, &pgdp[i + KERNEL_PGD_BOUNDARY]);\n#endif\n}\n\nstatic void pgd_prepopulate_pmd(struct mm_struct *mm, pgd_t *pgd, pmd_t *pmds[])\n{\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tint i;\n\n\tp4d = p4d_offset(pgd, 0);\n\tpud = pud_offset(p4d, 0);\n\n\tfor (i = 0; i < PREALLOCATED_PMDS; i++, pud++) {\n\t\tpmd_t *pmd = pmds[i];\n\n\t\tif (i >= KERNEL_PGD_BOUNDARY)\n\t\t\tmemcpy(pmd, (pmd_t *)pgd_page_vaddr(swapper_pg_dir[i]),\n\t\t\t       sizeof(pmd_t) * PTRS_PER_PMD);\n\n\t\tpud_populate(mm, pud, pmd);\n\t}\n}\n\n#ifdef CONFIG_PAGE_TABLE_ISOLATION\nstatic void pgd_prepopulate_user_pmd(struct mm_struct *mm,\n\t\t\t\t     pgd_t *k_pgd, pmd_t *pmds[])\n{\n\tpgd_t *s_pgd = kernel_to_user_pgdp(swapper_pg_dir);\n\tpgd_t *u_pgd = kernel_to_user_pgdp(k_pgd);\n\tp4d_t *u_p4d;\n\tpud_t *u_pud;\n\tint i;\n\n\tu_p4d = p4d_offset(u_pgd, 0);\n\tu_pud = pud_offset(u_p4d, 0);\n\n\ts_pgd += KERNEL_PGD_BOUNDARY;\n\tu_pud += KERNEL_PGD_BOUNDARY;\n\n\tfor (i = 0; i < PREALLOCATED_USER_PMDS; i++, u_pud++, s_pgd++) {\n\t\tpmd_t *pmd = pmds[i];\n\n\t\tmemcpy(pmd, (pmd_t *)pgd_page_vaddr(*s_pgd),\n\t\t       sizeof(pmd_t) * PTRS_PER_PMD);\n\n\t\tpud_populate(mm, u_pud, pmd);\n\t}\n\n}\n#else\nstatic void pgd_prepopulate_user_pmd(struct mm_struct *mm,\n\t\t\t\t     pgd_t *k_pgd, pmd_t *pmds[])\n{\n}\n#endif\n \n#ifdef CONFIG_X86_PAE\n\n#include <linux/slab.h>\n\n#define PGD_SIZE\t(PTRS_PER_PGD * sizeof(pgd_t))\n#define PGD_ALIGN\t32\n\nstatic struct kmem_cache *pgd_cache;\n\nvoid __init pgtable_cache_init(void)\n{\n\t \n\tif (!SHARED_KERNEL_PMD)\n\t\treturn;\n\n\t \n\tpgd_cache = kmem_cache_create(\"pgd_cache\", PGD_SIZE, PGD_ALIGN,\n\t\t\t\t      SLAB_PANIC, NULL);\n}\n\nstatic inline pgd_t *_pgd_alloc(void)\n{\n\t \n\tif (!SHARED_KERNEL_PMD)\n\t\treturn (pgd_t *)__get_free_pages(GFP_PGTABLE_USER,\n\t\t\t\t\t\t PGD_ALLOCATION_ORDER);\n\n\t \n\treturn kmem_cache_alloc(pgd_cache, GFP_PGTABLE_USER);\n}\n\nstatic inline void _pgd_free(pgd_t *pgd)\n{\n\tif (!SHARED_KERNEL_PMD)\n\t\tfree_pages((unsigned long)pgd, PGD_ALLOCATION_ORDER);\n\telse\n\t\tkmem_cache_free(pgd_cache, pgd);\n}\n#else\n\nstatic inline pgd_t *_pgd_alloc(void)\n{\n\treturn (pgd_t *)__get_free_pages(GFP_PGTABLE_USER,\n\t\t\t\t\t PGD_ALLOCATION_ORDER);\n}\n\nstatic inline void _pgd_free(pgd_t *pgd)\n{\n\tfree_pages((unsigned long)pgd, PGD_ALLOCATION_ORDER);\n}\n#endif  \n\npgd_t *pgd_alloc(struct mm_struct *mm)\n{\n\tpgd_t *pgd;\n\tpmd_t *u_pmds[MAX_PREALLOCATED_USER_PMDS];\n\tpmd_t *pmds[MAX_PREALLOCATED_PMDS];\n\n\tpgd = _pgd_alloc();\n\n\tif (pgd == NULL)\n\t\tgoto out;\n\n\tmm->pgd = pgd;\n\n\tif (sizeof(pmds) != 0 &&\n\t\t\tpreallocate_pmds(mm, pmds, PREALLOCATED_PMDS) != 0)\n\t\tgoto out_free_pgd;\n\n\tif (sizeof(u_pmds) != 0 &&\n\t\t\tpreallocate_pmds(mm, u_pmds, PREALLOCATED_USER_PMDS) != 0)\n\t\tgoto out_free_pmds;\n\n\tif (paravirt_pgd_alloc(mm) != 0)\n\t\tgoto out_free_user_pmds;\n\n\t \n\tspin_lock(&pgd_lock);\n\n\tpgd_ctor(mm, pgd);\n\tif (sizeof(pmds) != 0)\n\t\tpgd_prepopulate_pmd(mm, pgd, pmds);\n\n\tif (sizeof(u_pmds) != 0)\n\t\tpgd_prepopulate_user_pmd(mm, pgd, u_pmds);\n\n\tspin_unlock(&pgd_lock);\n\n\treturn pgd;\n\nout_free_user_pmds:\n\tif (sizeof(u_pmds) != 0)\n\t\tfree_pmds(mm, u_pmds, PREALLOCATED_USER_PMDS);\nout_free_pmds:\n\tif (sizeof(pmds) != 0)\n\t\tfree_pmds(mm, pmds, PREALLOCATED_PMDS);\nout_free_pgd:\n\t_pgd_free(pgd);\nout:\n\treturn NULL;\n}\n\nvoid pgd_free(struct mm_struct *mm, pgd_t *pgd)\n{\n\tpgd_mop_up_pmds(mm, pgd);\n\tpgd_dtor(pgd);\n\tparavirt_pgd_free(mm, pgd);\n\t_pgd_free(pgd);\n}\n\n \nint ptep_set_access_flags(struct vm_area_struct *vma,\n\t\t\t  unsigned long address, pte_t *ptep,\n\t\t\t  pte_t entry, int dirty)\n{\n\tint changed = !pte_same(*ptep, entry);\n\n\tif (changed && dirty)\n\t\tset_pte(ptep, entry);\n\n\treturn changed;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nint pmdp_set_access_flags(struct vm_area_struct *vma,\n\t\t\t  unsigned long address, pmd_t *pmdp,\n\t\t\t  pmd_t entry, int dirty)\n{\n\tint changed = !pmd_same(*pmdp, entry);\n\n\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);\n\n\tif (changed && dirty) {\n\t\tset_pmd(pmdp, entry);\n\t\t \n\t}\n\n\treturn changed;\n}\n\nint pudp_set_access_flags(struct vm_area_struct *vma, unsigned long address,\n\t\t\t  pud_t *pudp, pud_t entry, int dirty)\n{\n\tint changed = !pud_same(*pudp, entry);\n\n\tVM_BUG_ON(address & ~HPAGE_PUD_MASK);\n\n\tif (changed && dirty) {\n\t\tset_pud(pudp, entry);\n\t\t \n\t}\n\n\treturn changed;\n}\n#endif\n\nint ptep_test_and_clear_young(struct vm_area_struct *vma,\n\t\t\t      unsigned long addr, pte_t *ptep)\n{\n\tint ret = 0;\n\n\tif (pte_young(*ptep))\n\t\tret = test_and_clear_bit(_PAGE_BIT_ACCESSED,\n\t\t\t\t\t (unsigned long *) &ptep->pte);\n\n\treturn ret;\n}\n\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) || defined(CONFIG_ARCH_HAS_NONLEAF_PMD_YOUNG)\nint pmdp_test_and_clear_young(struct vm_area_struct *vma,\n\t\t\t      unsigned long addr, pmd_t *pmdp)\n{\n\tint ret = 0;\n\n\tif (pmd_young(*pmdp))\n\t\tret = test_and_clear_bit(_PAGE_BIT_ACCESSED,\n\t\t\t\t\t (unsigned long *)pmdp);\n\n\treturn ret;\n}\n#endif\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nint pudp_test_and_clear_young(struct vm_area_struct *vma,\n\t\t\t      unsigned long addr, pud_t *pudp)\n{\n\tint ret = 0;\n\n\tif (pud_young(*pudp))\n\t\tret = test_and_clear_bit(_PAGE_BIT_ACCESSED,\n\t\t\t\t\t (unsigned long *)pudp);\n\n\treturn ret;\n}\n#endif\n\nint ptep_clear_flush_young(struct vm_area_struct *vma,\n\t\t\t   unsigned long address, pte_t *ptep)\n{\n\t \n\treturn ptep_test_and_clear_young(vma, address, ptep);\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nint pmdp_clear_flush_young(struct vm_area_struct *vma,\n\t\t\t   unsigned long address, pmd_t *pmdp)\n{\n\tint young;\n\n\tVM_BUG_ON(address & ~HPAGE_PMD_MASK);\n\n\tyoung = pmdp_test_and_clear_young(vma, address, pmdp);\n\tif (young)\n\t\tflush_tlb_range(vma, address, address + HPAGE_PMD_SIZE);\n\n\treturn young;\n}\n\npmd_t pmdp_invalidate_ad(struct vm_area_struct *vma, unsigned long address,\n\t\t\t pmd_t *pmdp)\n{\n\t \n\treturn pmdp_establish(vma, address, pmdp, pmd_mkinvalid(*pmdp));\n}\n#endif\n\n \nvoid __init reserve_top_address(unsigned long reserve)\n{\n#ifdef CONFIG_X86_32\n\tBUG_ON(fixmaps_set > 0);\n\t__FIXADDR_TOP = round_down(-reserve, 1 << PMD_SHIFT) - PAGE_SIZE;\n\tprintk(KERN_INFO \"Reserving virtual address space above 0x%08lx (rounded to 0x%08lx)\\n\",\n\t       -reserve, __FIXADDR_TOP + PAGE_SIZE);\n#endif\n}\n\nint fixmaps_set;\n\nvoid __native_set_fixmap(enum fixed_addresses idx, pte_t pte)\n{\n\tunsigned long address = __fix_to_virt(idx);\n\n#ifdef CONFIG_X86_64\n        \n\tBUILD_BUG_ON(__end_of_permanent_fixed_addresses >\n\t\t     (FIXMAP_PMD_NUM * PTRS_PER_PTE));\n#endif\n\n\tif (idx >= __end_of_fixed_addresses) {\n\t\tBUG();\n\t\treturn;\n\t}\n\tset_pte_vaddr(address, pte);\n\tfixmaps_set++;\n}\n\nvoid native_set_fixmap(unsigned   idx,\n\t\t       phys_addr_t phys, pgprot_t flags)\n{\n\t \n\tpgprot_val(flags) &= __default_kernel_pte_mask;\n\n\t__native_set_fixmap(idx, pfn_pte(phys >> PAGE_SHIFT, flags));\n}\n\n#ifdef CONFIG_HAVE_ARCH_HUGE_VMAP\n#ifdef CONFIG_X86_5LEVEL\n \nint p4d_set_huge(p4d_t *p4d, phys_addr_t addr, pgprot_t prot)\n{\n\treturn 0;\n}\n\n \nvoid p4d_clear_huge(p4d_t *p4d)\n{\n}\n#endif\n\n \nint pud_set_huge(pud_t *pud, phys_addr_t addr, pgprot_t prot)\n{\n\tu8 uniform;\n\n\tmtrr_type_lookup(addr, addr + PUD_SIZE, &uniform);\n\tif (!uniform)\n\t\treturn 0;\n\n\t \n\tif (pud_present(*pud) && !pud_huge(*pud))\n\t\treturn 0;\n\n\tset_pte((pte_t *)pud, pfn_pte(\n\t\t(u64)addr >> PAGE_SHIFT,\n\t\t__pgprot(protval_4k_2_large(pgprot_val(prot)) | _PAGE_PSE)));\n\n\treturn 1;\n}\n\n \nint pmd_set_huge(pmd_t *pmd, phys_addr_t addr, pgprot_t prot)\n{\n\tu8 uniform;\n\n\tmtrr_type_lookup(addr, addr + PMD_SIZE, &uniform);\n\tif (!uniform) {\n\t\tpr_warn_once(\"%s: Cannot satisfy [mem %#010llx-%#010llx] with a huge-page mapping due to MTRR override.\\n\",\n\t\t\t     __func__, addr, addr + PMD_SIZE);\n\t\treturn 0;\n\t}\n\n\t \n\tif (pmd_present(*pmd) && !pmd_huge(*pmd))\n\t\treturn 0;\n\n\tset_pte((pte_t *)pmd, pfn_pte(\n\t\t(u64)addr >> PAGE_SHIFT,\n\t\t__pgprot(protval_4k_2_large(pgprot_val(prot)) | _PAGE_PSE)));\n\n\treturn 1;\n}\n\n \nint pud_clear_huge(pud_t *pud)\n{\n\tif (pud_large(*pud)) {\n\t\tpud_clear(pud);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n \nint pmd_clear_huge(pmd_t *pmd)\n{\n\tif (pmd_large(*pmd)) {\n\t\tpmd_clear(pmd);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_X86_64\n \nint pud_free_pmd_page(pud_t *pud, unsigned long addr)\n{\n\tpmd_t *pmd, *pmd_sv;\n\tpte_t *pte;\n\tint i;\n\n\tpmd = pud_pgtable(*pud);\n\tpmd_sv = (pmd_t *)__get_free_page(GFP_KERNEL);\n\tif (!pmd_sv)\n\t\treturn 0;\n\n\tfor (i = 0; i < PTRS_PER_PMD; i++) {\n\t\tpmd_sv[i] = pmd[i];\n\t\tif (!pmd_none(pmd[i]))\n\t\t\tpmd_clear(&pmd[i]);\n\t}\n\n\tpud_clear(pud);\n\n\t \n\tflush_tlb_kernel_range(addr, addr + PAGE_SIZE-1);\n\n\tfor (i = 0; i < PTRS_PER_PMD; i++) {\n\t\tif (!pmd_none(pmd_sv[i])) {\n\t\t\tpte = (pte_t *)pmd_page_vaddr(pmd_sv[i]);\n\t\t\tfree_page((unsigned long)pte);\n\t\t}\n\t}\n\n\tfree_page((unsigned long)pmd_sv);\n\n\tpagetable_pmd_dtor(virt_to_ptdesc(pmd));\n\tfree_page((unsigned long)pmd);\n\n\treturn 1;\n}\n\n \nint pmd_free_pte_page(pmd_t *pmd, unsigned long addr)\n{\n\tpte_t *pte;\n\n\tpte = (pte_t *)pmd_page_vaddr(*pmd);\n\tpmd_clear(pmd);\n\n\t \n\tflush_tlb_kernel_range(addr, addr + PAGE_SIZE-1);\n\n\tfree_page((unsigned long)pte);\n\n\treturn 1;\n}\n\n#else  \n\n \nint pmd_free_pte_page(pmd_t *pmd, unsigned long addr)\n{\n\treturn pmd_none(*pmd);\n}\n\n#endif  \n#endif\t \n\npte_t pte_mkwrite(pte_t pte, struct vm_area_struct *vma)\n{\n\tif (vma->vm_flags & VM_SHADOW_STACK)\n\t\treturn pte_mkwrite_shstk(pte);\n\n\tpte = pte_mkwrite_novma(pte);\n\n\treturn pte_clear_saveddirty(pte);\n}\n\npmd_t pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma)\n{\n\tif (vma->vm_flags & VM_SHADOW_STACK)\n\t\treturn pmd_mkwrite_shstk(pmd);\n\n\tpmd = pmd_mkwrite_novma(pmd);\n\n\treturn pmd_clear_saveddirty(pmd);\n}\n\nvoid arch_check_zapped_pte(struct vm_area_struct *vma, pte_t pte)\n{\n\t \n\tVM_WARN_ON_ONCE(!(vma->vm_flags & VM_SHADOW_STACK) &&\n\t\t\tpte_shstk(pte));\n}\n\nvoid arch_check_zapped_pmd(struct vm_area_struct *vma, pmd_t pmd)\n{\n\t \n\tVM_WARN_ON_ONCE(!(vma->vm_flags & VM_SHADOW_STACK) &&\n\t\t\tpmd_shstk(pmd));\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}