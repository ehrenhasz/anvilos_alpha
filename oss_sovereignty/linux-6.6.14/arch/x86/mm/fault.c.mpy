{
  "module_name": "fault.c",
  "hash_id": "7776cdb511e840998cdd86477cad3243872222c4c8ae2866e132946687e5361a",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/mm/fault.c",
  "human_readable_source": "\n \n#include <linux/sched.h>\t\t \n#include <linux/sched/task_stack.h>\t \n#include <linux/kdebug.h>\t\t \n#include <linux/extable.h>\t\t \n#include <linux/memblock.h>\t\t \n#include <linux/kfence.h>\t\t \n#include <linux/kprobes.h>\t\t \n#include <linux/mmiotrace.h>\t\t \n#include <linux/perf_event.h>\t\t \n#include <linux/hugetlb.h>\t\t \n#include <linux/prefetch.h>\t\t \n#include <linux/context_tracking.h>\t \n#include <linux/uaccess.h>\t\t \n#include <linux/efi.h>\t\t\t \n#include <linux/mm_types.h>\n#include <linux/mm.h>\t\t\t \n\n#include <asm/cpufeature.h>\t\t \n#include <asm/traps.h>\t\t\t \n#include <asm/fixmap.h>\t\t\t \n#include <asm/vsyscall.h>\t\t \n#include <asm/vm86.h>\t\t\t \n#include <asm/mmu_context.h>\t\t \n#include <asm/efi.h>\t\t\t \n#include <asm/desc.h>\t\t\t \n#include <asm/cpu_entry_area.h>\t\t \n#include <asm/pgtable_areas.h>\t\t \n#include <asm/kvm_para.h>\t\t \n#include <asm/vdso.h>\t\t\t \n#include <asm/irq_stack.h>\n\n#define CREATE_TRACE_POINTS\n#include <asm/trace/exceptions.h>\n\n \nstatic nokprobe_inline int\nkmmio_fault(struct pt_regs *regs, unsigned long addr)\n{\n\tif (unlikely(is_kmmio_active()))\n\t\tif (kmmio_handler(regs, addr) == 1)\n\t\t\treturn -1;\n\treturn 0;\n}\n\n \nstatic inline int\ncheck_prefetch_opcode(struct pt_regs *regs, unsigned char *instr,\n\t\t      unsigned char opcode, int *prefetch)\n{\n\tunsigned char instr_hi = opcode & 0xf0;\n\tunsigned char instr_lo = opcode & 0x0f;\n\n\tswitch (instr_hi) {\n\tcase 0x20:\n\tcase 0x30:\n\t\t \n\t\treturn ((instr_lo & 7) == 0x6);\n#ifdef CONFIG_X86_64\n\tcase 0x40:\n\t\t \n\t\treturn (!user_mode(regs) || user_64bit_mode(regs));\n#endif\n\tcase 0x60:\n\t\t \n\t\treturn (instr_lo & 0xC) == 0x4;\n\tcase 0xF0:\n\t\t \n\t\treturn !instr_lo || (instr_lo>>1) == 1;\n\tcase 0x00:\n\t\t \n\t\tif (get_kernel_nofault(opcode, instr))\n\t\t\treturn 0;\n\n\t\t*prefetch = (instr_lo == 0xF) &&\n\t\t\t(opcode == 0x0D || opcode == 0x18);\n\t\treturn 0;\n\tdefault:\n\t\treturn 0;\n\t}\n}\n\nstatic bool is_amd_k8_pre_npt(void)\n{\n\tstruct cpuinfo_x86 *c = &boot_cpu_data;\n\n\treturn unlikely(IS_ENABLED(CONFIG_CPU_SUP_AMD) &&\n\t\t\tc->x86_vendor == X86_VENDOR_AMD &&\n\t\t\tc->x86 == 0xf && c->x86_model < 0x40);\n}\n\nstatic int\nis_prefetch(struct pt_regs *regs, unsigned long error_code, unsigned long addr)\n{\n\tunsigned char *max_instr;\n\tunsigned char *instr;\n\tint prefetch = 0;\n\n\t \n\tif (!is_amd_k8_pre_npt())\n\t\treturn 0;\n\n\t \n\tif (error_code & X86_PF_INSTR)\n\t\treturn 0;\n\n\tinstr = (void *)convert_ip_to_linear(current, regs);\n\tmax_instr = instr + 15;\n\n\t \n\tpagefault_disable();\n\n\twhile (instr < max_instr) {\n\t\tunsigned char opcode;\n\n\t\tif (user_mode(regs)) {\n\t\t\tif (get_user(opcode, (unsigned char __user *) instr))\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (get_kernel_nofault(opcode, instr))\n\t\t\t\tbreak;\n\t\t}\n\n\t\tinstr++;\n\n\t\tif (!check_prefetch_opcode(regs, instr, opcode, &prefetch))\n\t\t\tbreak;\n\t}\n\n\tpagefault_enable();\n\treturn prefetch;\n}\n\nDEFINE_SPINLOCK(pgd_lock);\nLIST_HEAD(pgd_list);\n\n#ifdef CONFIG_X86_32\nstatic inline pmd_t *vmalloc_sync_one(pgd_t *pgd, unsigned long address)\n{\n\tunsigned index = pgd_index(address);\n\tpgd_t *pgd_k;\n\tp4d_t *p4d, *p4d_k;\n\tpud_t *pud, *pud_k;\n\tpmd_t *pmd, *pmd_k;\n\n\tpgd += index;\n\tpgd_k = init_mm.pgd + index;\n\n\tif (!pgd_present(*pgd_k))\n\t\treturn NULL;\n\n\t \n\tp4d = p4d_offset(pgd, address);\n\tp4d_k = p4d_offset(pgd_k, address);\n\tif (!p4d_present(*p4d_k))\n\t\treturn NULL;\n\n\tpud = pud_offset(p4d, address);\n\tpud_k = pud_offset(p4d_k, address);\n\tif (!pud_present(*pud_k))\n\t\treturn NULL;\n\n\tpmd = pmd_offset(pud, address);\n\tpmd_k = pmd_offset(pud_k, address);\n\n\tif (pmd_present(*pmd) != pmd_present(*pmd_k))\n\t\tset_pmd(pmd, *pmd_k);\n\n\tif (!pmd_present(*pmd_k))\n\t\treturn NULL;\n\telse\n\t\tBUG_ON(pmd_pfn(*pmd) != pmd_pfn(*pmd_k));\n\n\treturn pmd_k;\n}\n\n \nstatic noinline int vmalloc_fault(unsigned long address)\n{\n\tunsigned long pgd_paddr;\n\tpmd_t *pmd_k;\n\tpte_t *pte_k;\n\n\t \n\tif (!(address >= VMALLOC_START && address < VMALLOC_END))\n\t\treturn -1;\n\n\t \n\tpgd_paddr = read_cr3_pa();\n\tpmd_k = vmalloc_sync_one(__va(pgd_paddr), address);\n\tif (!pmd_k)\n\t\treturn -1;\n\n\tif (pmd_large(*pmd_k))\n\t\treturn 0;\n\n\tpte_k = pte_offset_kernel(pmd_k, address);\n\tif (!pte_present(*pte_k))\n\t\treturn -1;\n\n\treturn 0;\n}\nNOKPROBE_SYMBOL(vmalloc_fault);\n\nvoid arch_sync_kernel_mappings(unsigned long start, unsigned long end)\n{\n\tunsigned long addr;\n\n\tfor (addr = start & PMD_MASK;\n\t     addr >= TASK_SIZE_MAX && addr < VMALLOC_END;\n\t     addr += PMD_SIZE) {\n\t\tstruct page *page;\n\n\t\tspin_lock(&pgd_lock);\n\t\tlist_for_each_entry(page, &pgd_list, lru) {\n\t\t\tspinlock_t *pgt_lock;\n\n\t\t\t \n\t\t\tpgt_lock = &pgd_page_get_mm(page)->page_table_lock;\n\n\t\t\tspin_lock(pgt_lock);\n\t\t\tvmalloc_sync_one(page_address(page), addr);\n\t\t\tspin_unlock(pgt_lock);\n\t\t}\n\t\tspin_unlock(&pgd_lock);\n\t}\n}\n\nstatic bool low_pfn(unsigned long pfn)\n{\n\treturn pfn < max_low_pfn;\n}\n\nstatic void dump_pagetable(unsigned long address)\n{\n\tpgd_t *base = __va(read_cr3_pa());\n\tpgd_t *pgd = &base[pgd_index(address)];\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\n#ifdef CONFIG_X86_PAE\n\tpr_info(\"*pdpt = %016Lx \", pgd_val(*pgd));\n\tif (!low_pfn(pgd_val(*pgd) >> PAGE_SHIFT) || !pgd_present(*pgd))\n\t\tgoto out;\n#define pr_pde pr_cont\n#else\n#define pr_pde pr_info\n#endif\n\tp4d = p4d_offset(pgd, address);\n\tpud = pud_offset(p4d, address);\n\tpmd = pmd_offset(pud, address);\n\tpr_pde(\"*pde = %0*Lx \", sizeof(*pmd) * 2, (u64)pmd_val(*pmd));\n#undef pr_pde\n\n\t \n\tif (!low_pfn(pmd_pfn(*pmd)) || !pmd_present(*pmd) || pmd_large(*pmd))\n\t\tgoto out;\n\n\tpte = pte_offset_kernel(pmd, address);\n\tpr_cont(\"*pte = %0*Lx \", sizeof(*pte) * 2, (u64)pte_val(*pte));\nout:\n\tpr_cont(\"\\n\");\n}\n\n#else  \n\n#ifdef CONFIG_CPU_SUP_AMD\nstatic const char errata93_warning[] =\nKERN_ERR \n\"******* Your BIOS seems to not contain a fix for K8 errata #93\\n\"\n\"******* Working around it, but it may cause SEGVs or burn power.\\n\"\n\"******* Please consider a BIOS update.\\n\"\n\"******* Disabling USB legacy in the BIOS may also help.\\n\";\n#endif\n\nstatic int bad_address(void *p)\n{\n\tunsigned long dummy;\n\n\treturn get_kernel_nofault(dummy, (unsigned long *)p);\n}\n\nstatic void dump_pagetable(unsigned long address)\n{\n\tpgd_t *base = __va(read_cr3_pa());\n\tpgd_t *pgd = base + pgd_index(address);\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\n\tif (bad_address(pgd))\n\t\tgoto bad;\n\n\tpr_info(\"PGD %lx \", pgd_val(*pgd));\n\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\n\tp4d = p4d_offset(pgd, address);\n\tif (bad_address(p4d))\n\t\tgoto bad;\n\n\tpr_cont(\"P4D %lx \", p4d_val(*p4d));\n\tif (!p4d_present(*p4d) || p4d_large(*p4d))\n\t\tgoto out;\n\n\tpud = pud_offset(p4d, address);\n\tif (bad_address(pud))\n\t\tgoto bad;\n\n\tpr_cont(\"PUD %lx \", pud_val(*pud));\n\tif (!pud_present(*pud) || pud_large(*pud))\n\t\tgoto out;\n\n\tpmd = pmd_offset(pud, address);\n\tif (bad_address(pmd))\n\t\tgoto bad;\n\n\tpr_cont(\"PMD %lx \", pmd_val(*pmd));\n\tif (!pmd_present(*pmd) || pmd_large(*pmd))\n\t\tgoto out;\n\n\tpte = pte_offset_kernel(pmd, address);\n\tif (bad_address(pte))\n\t\tgoto bad;\n\n\tpr_cont(\"PTE %lx\", pte_val(*pte));\nout:\n\tpr_cont(\"\\n\");\n\treturn;\nbad:\n\tpr_info(\"BAD\\n\");\n}\n\n#endif  \n\n \nstatic int is_errata93(struct pt_regs *regs, unsigned long address)\n{\n#if defined(CONFIG_X86_64) && defined(CONFIG_CPU_SUP_AMD)\n\tif (boot_cpu_data.x86_vendor != X86_VENDOR_AMD\n\t    || boot_cpu_data.x86 != 0xf)\n\t\treturn 0;\n\n\tif (user_mode(regs))\n\t\treturn 0;\n\n\tif (address != regs->ip)\n\t\treturn 0;\n\n\tif ((address >> 32) != 0)\n\t\treturn 0;\n\n\taddress |= 0xffffffffUL << 32;\n\tif ((address >= (u64)_stext && address <= (u64)_etext) ||\n\t    (address >= MODULES_VADDR && address <= MODULES_END)) {\n\t\tprintk_once(errata93_warning);\n\t\tregs->ip = address;\n\t\treturn 1;\n\t}\n#endif\n\treturn 0;\n}\n\n \nstatic int is_errata100(struct pt_regs *regs, unsigned long address)\n{\n#ifdef CONFIG_X86_64\n\tif ((regs->cs == __USER32_CS || (regs->cs & (1<<2))) && (address >> 32))\n\t\treturn 1;\n#endif\n\treturn 0;\n}\n\n \nstatic int is_f00f_bug(struct pt_regs *regs, unsigned long error_code,\n\t\t       unsigned long address)\n{\n#ifdef CONFIG_X86_F00F_BUG\n\tif (boot_cpu_has_bug(X86_BUG_F00F) && !(error_code & X86_PF_USER) &&\n\t    idt_is_f00f_address(address)) {\n\t\thandle_invalid_op(regs);\n\t\treturn 1;\n\t}\n#endif\n\treturn 0;\n}\n\nstatic void show_ldttss(const struct desc_ptr *gdt, const char *name, u16 index)\n{\n\tu32 offset = (index >> 3) * sizeof(struct desc_struct);\n\tunsigned long addr;\n\tstruct ldttss_desc desc;\n\n\tif (index == 0) {\n\t\tpr_alert(\"%s: NULL\\n\", name);\n\t\treturn;\n\t}\n\n\tif (offset + sizeof(struct ldttss_desc) >= gdt->size) {\n\t\tpr_alert(\"%s: 0x%hx -- out of bounds\\n\", name, index);\n\t\treturn;\n\t}\n\n\tif (copy_from_kernel_nofault(&desc, (void *)(gdt->address + offset),\n\t\t\t      sizeof(struct ldttss_desc))) {\n\t\tpr_alert(\"%s: 0x%hx -- GDT entry is not readable\\n\",\n\t\t\t name, index);\n\t\treturn;\n\t}\n\n\taddr = desc.base0 | (desc.base1 << 16) | ((unsigned long)desc.base2 << 24);\n#ifdef CONFIG_X86_64\n\taddr |= ((u64)desc.base3 << 32);\n#endif\n\tpr_alert(\"%s: 0x%hx -- base=0x%lx limit=0x%x\\n\",\n\t\t name, index, addr, (desc.limit0 | (desc.limit1 << 16)));\n}\n\nstatic void\nshow_fault_oops(struct pt_regs *regs, unsigned long error_code, unsigned long address)\n{\n\tif (!oops_may_print())\n\t\treturn;\n\n\tif (error_code & X86_PF_INSTR) {\n\t\tunsigned int level;\n\t\tpgd_t *pgd;\n\t\tpte_t *pte;\n\n\t\tpgd = __va(read_cr3_pa());\n\t\tpgd += pgd_index(address);\n\n\t\tpte = lookup_address_in_pgd(pgd, address, &level);\n\n\t\tif (pte && pte_present(*pte) && !pte_exec(*pte))\n\t\t\tpr_crit(\"kernel tried to execute NX-protected page - exploit attempt? (uid: %d)\\n\",\n\t\t\t\tfrom_kuid(&init_user_ns, current_uid()));\n\t\tif (pte && pte_present(*pte) && pte_exec(*pte) &&\n\t\t\t\t(pgd_flags(*pgd) & _PAGE_USER) &&\n\t\t\t\t(__read_cr4() & X86_CR4_SMEP))\n\t\t\tpr_crit(\"unable to execute userspace code (SMEP?) (uid: %d)\\n\",\n\t\t\t\tfrom_kuid(&init_user_ns, current_uid()));\n\t}\n\n\tif (address < PAGE_SIZE && !user_mode(regs))\n\t\tpr_alert(\"BUG: kernel NULL pointer dereference, address: %px\\n\",\n\t\t\t(void *)address);\n\telse\n\t\tpr_alert(\"BUG: unable to handle page fault for address: %px\\n\",\n\t\t\t(void *)address);\n\n\tpr_alert(\"#PF: %s %s in %s mode\\n\",\n\t\t (error_code & X86_PF_USER)  ? \"user\" : \"supervisor\",\n\t\t (error_code & X86_PF_INSTR) ? \"instruction fetch\" :\n\t\t (error_code & X86_PF_WRITE) ? \"write access\" :\n\t\t\t\t\t       \"read access\",\n\t\t\t     user_mode(regs) ? \"user\" : \"kernel\");\n\tpr_alert(\"#PF: error_code(0x%04lx) - %s\\n\", error_code,\n\t\t !(error_code & X86_PF_PROT) ? \"not-present page\" :\n\t\t (error_code & X86_PF_RSVD)  ? \"reserved bit violation\" :\n\t\t (error_code & X86_PF_PK)    ? \"protection keys violation\" :\n\t\t\t\t\t       \"permissions violation\");\n\n\tif (!(error_code & X86_PF_USER) && user_mode(regs)) {\n\t\tstruct desc_ptr idt, gdt;\n\t\tu16 ldtr, tr;\n\n\t\t \n\t\tstore_idt(&idt);\n\n\t\t \n\t\tnative_store_gdt(&gdt);\n\n\t\tpr_alert(\"IDT: 0x%lx (limit=0x%hx) GDT: 0x%lx (limit=0x%hx)\\n\",\n\t\t\t idt.address, idt.size, gdt.address, gdt.size);\n\n\t\tstore_ldt(ldtr);\n\t\tshow_ldttss(&gdt, \"LDTR\", ldtr);\n\n\t\tstore_tr(tr);\n\t\tshow_ldttss(&gdt, \"TR\", tr);\n\t}\n\n\tdump_pagetable(address);\n}\n\nstatic noinline void\npgtable_bad(struct pt_regs *regs, unsigned long error_code,\n\t    unsigned long address)\n{\n\tstruct task_struct *tsk;\n\tunsigned long flags;\n\tint sig;\n\n\tflags = oops_begin();\n\ttsk = current;\n\tsig = SIGKILL;\n\n\tprintk(KERN_ALERT \"%s: Corrupted page table at address %lx\\n\",\n\t       tsk->comm, address);\n\tdump_pagetable(address);\n\n\tif (__die(\"Bad pagetable\", regs, error_code))\n\t\tsig = 0;\n\n\toops_end(flags, regs, sig);\n}\n\nstatic void sanitize_error_code(unsigned long address,\n\t\t\t\tunsigned long *error_code)\n{\n\t \n\tif (address >= TASK_SIZE_MAX)\n\t\t*error_code |= X86_PF_PROT;\n}\n\nstatic void set_signal_archinfo(unsigned long address,\n\t\t\t\tunsigned long error_code)\n{\n\tstruct task_struct *tsk = current;\n\n\ttsk->thread.trap_nr = X86_TRAP_PF;\n\ttsk->thread.error_code = error_code | X86_PF_USER;\n\ttsk->thread.cr2 = address;\n}\n\nstatic noinline void\npage_fault_oops(struct pt_regs *regs, unsigned long error_code,\n\t\tunsigned long address)\n{\n#ifdef CONFIG_VMAP_STACK\n\tstruct stack_info info;\n#endif\n\tunsigned long flags;\n\tint sig;\n\n\tif (user_mode(regs)) {\n\t\t \n\t\tgoto oops;\n\t}\n\n#ifdef CONFIG_VMAP_STACK\n\t \n\tif (is_vmalloc_addr((void *)address) &&\n\t    get_stack_guard_info((void *)address, &info)) {\n\t\t \n\t\tcall_on_stack(__this_cpu_ist_top_va(DF) - sizeof(void*),\n\t\t\t      handle_stack_overflow,\n\t\t\t      ASM_CALL_ARG3,\n\t\t\t      , [arg1] \"r\" (regs), [arg2] \"r\" (address), [arg3] \"r\" (&info));\n\n\t\tunreachable();\n\t}\n#endif\n\n\t \n\tif (IS_ENABLED(CONFIG_EFI))\n\t\tefi_crash_gracefully_on_page_fault(address);\n\n\t \n\tif (!(error_code & X86_PF_PROT) &&\n\t    kfence_handle_page_fault(address, error_code & X86_PF_WRITE, regs))\n\t\treturn;\n\noops:\n\t \n\tflags = oops_begin();\n\n\tshow_fault_oops(regs, error_code, address);\n\n\tif (task_stack_end_corrupted(current))\n\t\tprintk(KERN_EMERG \"Thread overran stack, or stack corrupted\\n\");\n\n\tsig = SIGKILL;\n\tif (__die(\"Oops\", regs, error_code))\n\t\tsig = 0;\n\n\t \n\tprintk(KERN_DEFAULT \"CR2: %016lx\\n\", address);\n\n\toops_end(flags, regs, sig);\n}\n\nstatic noinline void\nkernelmode_fixup_or_oops(struct pt_regs *regs, unsigned long error_code,\n\t\t\t unsigned long address, int signal, int si_code,\n\t\t\t u32 pkey)\n{\n\tWARN_ON_ONCE(user_mode(regs));\n\n\t \n\tif (fixup_exception(regs, X86_TRAP_PF, error_code, address)) {\n\t\t \n\t\tif (in_interrupt())\n\t\t\treturn;\n\n\t\t \n\t\tif (current->thread.sig_on_uaccess_err && signal) {\n\t\t\tsanitize_error_code(address, &error_code);\n\n\t\t\tset_signal_archinfo(address, error_code);\n\n\t\t\tif (si_code == SEGV_PKUERR) {\n\t\t\t\tforce_sig_pkuerr((void __user *)address, pkey);\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tforce_sig_fault(signal, si_code, (void __user *)address);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\treturn;\n\t}\n\n\t \n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tpage_fault_oops(regs, error_code, address);\n}\n\n \nstatic inline void\nshow_signal_msg(struct pt_regs *regs, unsigned long error_code,\n\t\tunsigned long address, struct task_struct *tsk)\n{\n\tconst char *loglvl = task_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG;\n\t \n\tint cpu = raw_smp_processor_id();\n\n\tif (!unhandled_signal(tsk, SIGSEGV))\n\t\treturn;\n\n\tif (!printk_ratelimit())\n\t\treturn;\n\n\tprintk(\"%s%s[%d]: segfault at %lx ip %px sp %px error %lx\",\n\t\tloglvl, tsk->comm, task_pid_nr(tsk), address,\n\t\t(void *)regs->ip, (void *)regs->sp, error_code);\n\n\tprint_vma_addr(KERN_CONT \" in \", regs->ip);\n\n\t \n\tprintk(KERN_CONT \" likely on CPU %d (core %d, socket %d)\", cpu,\n\t       topology_core_id(cpu), topology_physical_package_id(cpu));\n\n\n\tprintk(KERN_CONT \"\\n\");\n\n\tshow_opcodes(regs, loglvl);\n}\n\n \nstatic bool is_vsyscall_vaddr(unsigned long vaddr)\n{\n\treturn unlikely((vaddr & PAGE_MASK) == VSYSCALL_ADDR);\n}\n\nstatic void\n__bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,\n\t\t       unsigned long address, u32 pkey, int si_code)\n{\n\tstruct task_struct *tsk = current;\n\n\tif (!user_mode(regs)) {\n\t\tkernelmode_fixup_or_oops(regs, error_code, address,\n\t\t\t\t\t SIGSEGV, si_code, pkey);\n\t\treturn;\n\t}\n\n\tif (!(error_code & X86_PF_USER)) {\n\t\t \n\t\tpage_fault_oops(regs, error_code, address);\n\t\treturn;\n\t}\n\n\t \n\tlocal_irq_enable();\n\n\t \n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tif (is_errata100(regs, address))\n\t\treturn;\n\n\tsanitize_error_code(address, &error_code);\n\n\tif (fixup_vdso_exception(regs, X86_TRAP_PF, error_code, address))\n\t\treturn;\n\n\tif (likely(show_unhandled_signals))\n\t\tshow_signal_msg(regs, error_code, address, tsk);\n\n\tset_signal_archinfo(address, error_code);\n\n\tif (si_code == SEGV_PKUERR)\n\t\tforce_sig_pkuerr((void __user *)address, pkey);\n\telse\n\t\tforce_sig_fault(SIGSEGV, si_code, (void __user *)address);\n\n\tlocal_irq_disable();\n}\n\nstatic noinline void\nbad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,\n\t\t     unsigned long address)\n{\n\t__bad_area_nosemaphore(regs, error_code, address, 0, SEGV_MAPERR);\n}\n\nstatic void\n__bad_area(struct pt_regs *regs, unsigned long error_code,\n\t   unsigned long address, u32 pkey, int si_code)\n{\n\tstruct mm_struct *mm = current->mm;\n\t \n\tmmap_read_unlock(mm);\n\n\t__bad_area_nosemaphore(regs, error_code, address, pkey, si_code);\n}\n\nstatic inline bool bad_area_access_from_pkeys(unsigned long error_code,\n\t\tstruct vm_area_struct *vma)\n{\n\t \n\tbool foreign = false;\n\n\tif (!cpu_feature_enabled(X86_FEATURE_OSPKE))\n\t\treturn false;\n\tif (error_code & X86_PF_PK)\n\t\treturn true;\n\t \n\tif (!arch_vma_access_permitted(vma, (error_code & X86_PF_WRITE),\n\t\t\t\t       (error_code & X86_PF_INSTR), foreign))\n\t\treturn true;\n\treturn false;\n}\n\nstatic noinline void\nbad_area_access_error(struct pt_regs *regs, unsigned long error_code,\n\t\t      unsigned long address, struct vm_area_struct *vma)\n{\n\t \n\tif (bad_area_access_from_pkeys(error_code, vma)) {\n\t\t \n\t\tu32 pkey = vma_pkey(vma);\n\n\t\t__bad_area(regs, error_code, address, pkey, SEGV_PKUERR);\n\t} else {\n\t\t__bad_area(regs, error_code, address, 0, SEGV_ACCERR);\n\t}\n}\n\nstatic void\ndo_sigbus(struct pt_regs *regs, unsigned long error_code, unsigned long address,\n\t  vm_fault_t fault)\n{\n\t \n\tif (!user_mode(regs)) {\n\t\tkernelmode_fixup_or_oops(regs, error_code, address,\n\t\t\t\t\t SIGBUS, BUS_ADRERR, ARCH_DEFAULT_PKEY);\n\t\treturn;\n\t}\n\n\t \n\tif (is_prefetch(regs, error_code, address))\n\t\treturn;\n\n\tsanitize_error_code(address, &error_code);\n\n\tif (fixup_vdso_exception(regs, X86_TRAP_PF, error_code, address))\n\t\treturn;\n\n\tset_signal_archinfo(address, error_code);\n\n#ifdef CONFIG_MEMORY_FAILURE\n\tif (fault & (VM_FAULT_HWPOISON|VM_FAULT_HWPOISON_LARGE)) {\n\t\tstruct task_struct *tsk = current;\n\t\tunsigned lsb = 0;\n\n\t\tpr_err(\n\t\"MCE: Killing %s:%d due to hardware memory corruption fault at %lx\\n\",\n\t\t\ttsk->comm, tsk->pid, address);\n\t\tif (fault & VM_FAULT_HWPOISON_LARGE)\n\t\t\tlsb = hstate_index_to_shift(VM_FAULT_GET_HINDEX(fault));\n\t\tif (fault & VM_FAULT_HWPOISON)\n\t\t\tlsb = PAGE_SHIFT;\n\t\tforce_sig_mceerr(BUS_MCEERR_AR, (void __user *)address, lsb);\n\t\treturn;\n\t}\n#endif\n\tforce_sig_fault(SIGBUS, BUS_ADRERR, (void __user *)address);\n}\n\nstatic int spurious_kernel_fault_check(unsigned long error_code, pte_t *pte)\n{\n\tif ((error_code & X86_PF_WRITE) && !pte_write(*pte))\n\t\treturn 0;\n\n\tif ((error_code & X86_PF_INSTR) && !pte_exec(*pte))\n\t\treturn 0;\n\n\treturn 1;\n}\n\n \nstatic noinline int\nspurious_kernel_fault(unsigned long error_code, unsigned long address)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpte_t *pte;\n\tint ret;\n\n\t \n\tif (error_code != (X86_PF_WRITE | X86_PF_PROT) &&\n\t    error_code != (X86_PF_INSTR | X86_PF_PROT))\n\t\treturn 0;\n\n\tpgd = init_mm.pgd + pgd_index(address);\n\tif (!pgd_present(*pgd))\n\t\treturn 0;\n\n\tp4d = p4d_offset(pgd, address);\n\tif (!p4d_present(*p4d))\n\t\treturn 0;\n\n\tif (p4d_large(*p4d))\n\t\treturn spurious_kernel_fault_check(error_code, (pte_t *) p4d);\n\n\tpud = pud_offset(p4d, address);\n\tif (!pud_present(*pud))\n\t\treturn 0;\n\n\tif (pud_large(*pud))\n\t\treturn spurious_kernel_fault_check(error_code, (pte_t *) pud);\n\n\tpmd = pmd_offset(pud, address);\n\tif (!pmd_present(*pmd))\n\t\treturn 0;\n\n\tif (pmd_large(*pmd))\n\t\treturn spurious_kernel_fault_check(error_code, (pte_t *) pmd);\n\n\tpte = pte_offset_kernel(pmd, address);\n\tif (!pte_present(*pte))\n\t\treturn 0;\n\n\tret = spurious_kernel_fault_check(error_code, pte);\n\tif (!ret)\n\t\treturn 0;\n\n\t \n\tret = spurious_kernel_fault_check(error_code, (pte_t *) pmd);\n\tWARN_ONCE(!ret, \"PMD has incorrect permission bits\\n\");\n\n\treturn ret;\n}\nNOKPROBE_SYMBOL(spurious_kernel_fault);\n\nint show_unhandled_signals = 1;\n\nstatic inline int\naccess_error(unsigned long error_code, struct vm_area_struct *vma)\n{\n\t \n\tbool foreign = false;\n\n\t \n\tif (error_code & X86_PF_PK)\n\t\treturn 1;\n\n\t \n\tif (unlikely(error_code & X86_PF_SGX))\n\t\treturn 1;\n\n\t \n\tif (!arch_vma_access_permitted(vma, (error_code & X86_PF_WRITE),\n\t\t\t\t       (error_code & X86_PF_INSTR), foreign))\n\t\treturn 1;\n\n\t \n\tif (error_code & X86_PF_SHSTK) {\n\t\tif (unlikely(!(vma->vm_flags & VM_SHADOW_STACK)))\n\t\t\treturn 1;\n\t\tif (unlikely(!(vma->vm_flags & VM_WRITE)))\n\t\t\treturn 1;\n\t\treturn 0;\n\t}\n\n\tif (error_code & X86_PF_WRITE) {\n\t\t \n\t\tif (unlikely(vma->vm_flags & VM_SHADOW_STACK))\n\t\t\treturn 1;\n\t\tif (unlikely(!(vma->vm_flags & VM_WRITE)))\n\t\t\treturn 1;\n\t\treturn 0;\n\t}\n\n\t \n\tif (unlikely(error_code & X86_PF_PROT))\n\t\treturn 1;\n\n\t \n\tif (unlikely(!vma_is_accessible(vma)))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nbool fault_in_kernel_space(unsigned long address)\n{\n\t \n\tif (IS_ENABLED(CONFIG_X86_64) && is_vsyscall_vaddr(address))\n\t\treturn false;\n\n\treturn address >= TASK_SIZE_MAX;\n}\n\n \nstatic void\ndo_kern_addr_fault(struct pt_regs *regs, unsigned long hw_error_code,\n\t\t   unsigned long address)\n{\n\t \n\tWARN_ON_ONCE(hw_error_code & X86_PF_PK);\n\n#ifdef CONFIG_X86_32\n\t \n\tif (!(hw_error_code & (X86_PF_RSVD | X86_PF_USER | X86_PF_PROT))) {\n\t\tif (vmalloc_fault(address) >= 0)\n\t\t\treturn;\n\t}\n#endif\n\n\tif (is_f00f_bug(regs, hw_error_code, address))\n\t\treturn;\n\n\t \n\tif (spurious_kernel_fault(hw_error_code, address))\n\t\treturn;\n\n\t \n\tif (WARN_ON_ONCE(kprobe_page_fault(regs, X86_TRAP_PF)))\n\t\treturn;\n\n\t \n\tbad_area_nosemaphore(regs, hw_error_code, address);\n}\nNOKPROBE_SYMBOL(do_kern_addr_fault);\n\n \nstatic inline\nvoid do_user_addr_fault(struct pt_regs *regs,\n\t\t\tunsigned long error_code,\n\t\t\tunsigned long address)\n{\n\tstruct vm_area_struct *vma;\n\tstruct task_struct *tsk;\n\tstruct mm_struct *mm;\n\tvm_fault_t fault;\n\tunsigned int flags = FAULT_FLAG_DEFAULT;\n\n\ttsk = current;\n\tmm = tsk->mm;\n\n\tif (unlikely((error_code & (X86_PF_USER | X86_PF_INSTR)) == X86_PF_INSTR)) {\n\t\t \n\t\tif (is_errata93(regs, address))\n\t\t\treturn;\n\n\t\tpage_fault_oops(regs, error_code, address);\n\t\treturn;\n\t}\n\n\t \n\tif (WARN_ON_ONCE(kprobe_page_fault(regs, X86_TRAP_PF)))\n\t\treturn;\n\n\t \n\tif (unlikely(error_code & X86_PF_RSVD))\n\t\tpgtable_bad(regs, error_code, address);\n\n\t \n\tif (unlikely(cpu_feature_enabled(X86_FEATURE_SMAP) &&\n\t\t     !(error_code & X86_PF_USER) &&\n\t\t     !(regs->flags & X86_EFLAGS_AC))) {\n\t\t \n\t\tpage_fault_oops(regs, error_code, address);\n\t\treturn;\n\t}\n\n\t \n\tif (unlikely(faulthandler_disabled() || !mm)) {\n\t\tbad_area_nosemaphore(regs, error_code, address);\n\t\treturn;\n\t}\n\n\t \n\tif (user_mode(regs)) {\n\t\tlocal_irq_enable();\n\t\tflags |= FAULT_FLAG_USER;\n\t} else {\n\t\tif (regs->flags & X86_EFLAGS_IF)\n\t\t\tlocal_irq_enable();\n\t}\n\n\tperf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);\n\n\t \n\tif (error_code & X86_PF_SHSTK)\n\t\tflags |= FAULT_FLAG_WRITE;\n\tif (error_code & X86_PF_WRITE)\n\t\tflags |= FAULT_FLAG_WRITE;\n\tif (error_code & X86_PF_INSTR)\n\t\tflags |= FAULT_FLAG_INSTRUCTION;\n\n#ifdef CONFIG_X86_64\n\t \n\tif (is_vsyscall_vaddr(address)) {\n\t\tif (emulate_vsyscall(error_code, regs, address))\n\t\t\treturn;\n\t}\n#endif\n\n\tif (!(flags & FAULT_FLAG_USER))\n\t\tgoto lock_mmap;\n\n\tvma = lock_vma_under_rcu(mm, address);\n\tif (!vma)\n\t\tgoto lock_mmap;\n\n\tif (unlikely(access_error(error_code, vma))) {\n\t\tvma_end_read(vma);\n\t\tgoto lock_mmap;\n\t}\n\tfault = handle_mm_fault(vma, address, flags | FAULT_FLAG_VMA_LOCK, regs);\n\tif (!(fault & (VM_FAULT_RETRY | VM_FAULT_COMPLETED)))\n\t\tvma_end_read(vma);\n\n\tif (!(fault & VM_FAULT_RETRY)) {\n\t\tcount_vm_vma_lock_event(VMA_LOCK_SUCCESS);\n\t\tgoto done;\n\t}\n\tcount_vm_vma_lock_event(VMA_LOCK_RETRY);\n\n\t \n\tif (fault_signal_pending(fault, regs)) {\n\t\tif (!user_mode(regs))\n\t\t\tkernelmode_fixup_or_oops(regs, error_code, address,\n\t\t\t\t\t\t SIGBUS, BUS_ADRERR,\n\t\t\t\t\t\t ARCH_DEFAULT_PKEY);\n\t\treturn;\n\t}\nlock_mmap:\n\nretry:\n\tvma = lock_mm_and_find_vma(mm, address, regs);\n\tif (unlikely(!vma)) {\n\t\tbad_area_nosemaphore(regs, error_code, address);\n\t\treturn;\n\t}\n\n\t \n\tif (unlikely(access_error(error_code, vma))) {\n\t\tbad_area_access_error(regs, error_code, address, vma);\n\t\treturn;\n\t}\n\n\t \n\tfault = handle_mm_fault(vma, address, flags, regs);\n\n\tif (fault_signal_pending(fault, regs)) {\n\t\t \n\t\tif (!user_mode(regs))\n\t\t\tkernelmode_fixup_or_oops(regs, error_code, address,\n\t\t\t\t\t\t SIGBUS, BUS_ADRERR,\n\t\t\t\t\t\t ARCH_DEFAULT_PKEY);\n\t\treturn;\n\t}\n\n\t \n\tif (fault & VM_FAULT_COMPLETED)\n\t\treturn;\n\n\t \n\tif (unlikely(fault & VM_FAULT_RETRY)) {\n\t\tflags |= FAULT_FLAG_TRIED;\n\t\tgoto retry;\n\t}\n\n\tmmap_read_unlock(mm);\ndone:\n\tif (likely(!(fault & VM_FAULT_ERROR)))\n\t\treturn;\n\n\tif (fatal_signal_pending(current) && !user_mode(regs)) {\n\t\tkernelmode_fixup_or_oops(regs, error_code, address,\n\t\t\t\t\t 0, 0, ARCH_DEFAULT_PKEY);\n\t\treturn;\n\t}\n\n\tif (fault & VM_FAULT_OOM) {\n\t\t \n\t\tif (!user_mode(regs)) {\n\t\t\tkernelmode_fixup_or_oops(regs, error_code, address,\n\t\t\t\t\t\t SIGSEGV, SEGV_MAPERR,\n\t\t\t\t\t\t ARCH_DEFAULT_PKEY);\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tpagefault_out_of_memory();\n\t} else {\n\t\tif (fault & (VM_FAULT_SIGBUS|VM_FAULT_HWPOISON|\n\t\t\t     VM_FAULT_HWPOISON_LARGE))\n\t\t\tdo_sigbus(regs, error_code, address, fault);\n\t\telse if (fault & VM_FAULT_SIGSEGV)\n\t\t\tbad_area_nosemaphore(regs, error_code, address);\n\t\telse\n\t\t\tBUG();\n\t}\n}\nNOKPROBE_SYMBOL(do_user_addr_fault);\n\nstatic __always_inline void\ntrace_page_fault_entries(struct pt_regs *regs, unsigned long error_code,\n\t\t\t unsigned long address)\n{\n\tif (!trace_pagefault_enabled())\n\t\treturn;\n\n\tif (user_mode(regs))\n\t\ttrace_page_fault_user(address, regs, error_code);\n\telse\n\t\ttrace_page_fault_kernel(address, regs, error_code);\n}\n\nstatic __always_inline void\nhandle_page_fault(struct pt_regs *regs, unsigned long error_code,\n\t\t\t      unsigned long address)\n{\n\ttrace_page_fault_entries(regs, error_code, address);\n\n\tif (unlikely(kmmio_fault(regs, address)))\n\t\treturn;\n\n\t \n\tif (unlikely(fault_in_kernel_space(address))) {\n\t\tdo_kern_addr_fault(regs, error_code, address);\n\t} else {\n\t\tdo_user_addr_fault(regs, error_code, address);\n\t\t \n\t\tlocal_irq_disable();\n\t}\n}\n\nDEFINE_IDTENTRY_RAW_ERRORCODE(exc_page_fault)\n{\n\tunsigned long address = read_cr2();\n\tirqentry_state_t state;\n\n\tprefetchw(&current->mm->mmap_lock);\n\n\t \n\tif (kvm_handle_async_pf(regs, (u32)address))\n\t\treturn;\n\n\t \n\tstate = irqentry_enter(regs);\n\n\tinstrumentation_begin();\n\thandle_page_fault(regs, error_code, address);\n\tinstrumentation_end();\n\n\tirqentry_exit(regs, state);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}