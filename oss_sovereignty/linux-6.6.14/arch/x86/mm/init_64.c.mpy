{
  "module_name": "init_64.c",
  "hash_id": "f3eb19b07b6d85804848d9c9368ac59f51a02b5668b18e8ee1ef71f6dda76b90",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/mm/init_64.c",
  "human_readable_source": "\n \n\n#include <linux/signal.h>\n#include <linux/sched.h>\n#include <linux/kernel.h>\n#include <linux/errno.h>\n#include <linux/string.h>\n#include <linux/types.h>\n#include <linux/ptrace.h>\n#include <linux/mman.h>\n#include <linux/mm.h>\n#include <linux/swap.h>\n#include <linux/smp.h>\n#include <linux/init.h>\n#include <linux/initrd.h>\n#include <linux/pagemap.h>\n#include <linux/memblock.h>\n#include <linux/proc_fs.h>\n#include <linux/pci.h>\n#include <linux/pfn.h>\n#include <linux/poison.h>\n#include <linux/dma-mapping.h>\n#include <linux/memory.h>\n#include <linux/memory_hotplug.h>\n#include <linux/memremap.h>\n#include <linux/nmi.h>\n#include <linux/gfp.h>\n#include <linux/kcore.h>\n#include <linux/bootmem_info.h>\n\n#include <asm/processor.h>\n#include <asm/bios_ebda.h>\n#include <linux/uaccess.h>\n#include <asm/pgalloc.h>\n#include <asm/dma.h>\n#include <asm/fixmap.h>\n#include <asm/e820/api.h>\n#include <asm/apic.h>\n#include <asm/tlb.h>\n#include <asm/mmu_context.h>\n#include <asm/proto.h>\n#include <asm/smp.h>\n#include <asm/sections.h>\n#include <asm/kdebug.h>\n#include <asm/numa.h>\n#include <asm/set_memory.h>\n#include <asm/init.h>\n#include <asm/uv/uv.h>\n#include <asm/setup.h>\n#include <asm/ftrace.h>\n\n#include \"mm_internal.h\"\n\n#include \"ident_map.c\"\n\n#define DEFINE_POPULATE(fname, type1, type2, init)\t\t\\\nstatic inline void fname##_init(struct mm_struct *mm,\t\t\\\n\t\ttype1##_t *arg1, type2##_t *arg2, bool init)\t\\\n{\t\t\t\t\t\t\t\t\\\n\tif (init)\t\t\t\t\t\t\\\n\t\tfname##_safe(mm, arg1, arg2);\t\t\t\\\n\telse\t\t\t\t\t\t\t\\\n\t\tfname(mm, arg1, arg2);\t\t\t\t\\\n}\n\nDEFINE_POPULATE(p4d_populate, p4d, pud, init)\nDEFINE_POPULATE(pgd_populate, pgd, p4d, init)\nDEFINE_POPULATE(pud_populate, pud, pmd, init)\nDEFINE_POPULATE(pmd_populate_kernel, pmd, pte, init)\n\n#define DEFINE_ENTRY(type1, type2, init)\t\t\t\\\nstatic inline void set_##type1##_init(type1##_t *arg1,\t\t\\\n\t\t\ttype2##_t arg2, bool init)\t\t\\\n{\t\t\t\t\t\t\t\t\\\n\tif (init)\t\t\t\t\t\t\\\n\t\tset_##type1##_safe(arg1, arg2);\t\t\t\\\n\telse\t\t\t\t\t\t\t\\\n\t\tset_##type1(arg1, arg2);\t\t\t\\\n}\n\nDEFINE_ENTRY(p4d, p4d, init)\nDEFINE_ENTRY(pud, pud, init)\nDEFINE_ENTRY(pmd, pmd, init)\nDEFINE_ENTRY(pte, pte, init)\n\nstatic inline pgprot_t prot_sethuge(pgprot_t prot)\n{\n\tWARN_ON_ONCE(pgprot_val(prot) & _PAGE_PAT);\n\n\treturn __pgprot(pgprot_val(prot) | _PAGE_PSE);\n}\n\n \n\n \npteval_t __supported_pte_mask __read_mostly = ~0;\n \npteval_t __default_kernel_pte_mask __read_mostly = ~0;\nEXPORT_SYMBOL_GPL(__supported_pte_mask);\n \nEXPORT_SYMBOL(__default_kernel_pte_mask);\n\nint force_personality32;\n\n \nstatic int __init nonx32_setup(char *str)\n{\n\tif (!strcmp(str, \"on\"))\n\t\tforce_personality32 &= ~READ_IMPLIES_EXEC;\n\telse if (!strcmp(str, \"off\"))\n\t\tforce_personality32 |= READ_IMPLIES_EXEC;\n\treturn 1;\n}\n__setup(\"noexec32=\", nonx32_setup);\n\nstatic void sync_global_pgds_l5(unsigned long start, unsigned long end)\n{\n\tunsigned long addr;\n\n\tfor (addr = start; addr <= end; addr = ALIGN(addr + 1, PGDIR_SIZE)) {\n\t\tconst pgd_t *pgd_ref = pgd_offset_k(addr);\n\t\tstruct page *page;\n\n\t\t \n\t\tif (addr < start)\n\t\t\tbreak;\n\n\t\tif (pgd_none(*pgd_ref))\n\t\t\tcontinue;\n\n\t\tspin_lock(&pgd_lock);\n\t\tlist_for_each_entry(page, &pgd_list, lru) {\n\t\t\tpgd_t *pgd;\n\t\t\tspinlock_t *pgt_lock;\n\n\t\t\tpgd = (pgd_t *)page_address(page) + pgd_index(addr);\n\t\t\t \n\t\t\tpgt_lock = &pgd_page_get_mm(page)->page_table_lock;\n\t\t\tspin_lock(pgt_lock);\n\n\t\t\tif (!pgd_none(*pgd_ref) && !pgd_none(*pgd))\n\t\t\t\tBUG_ON(pgd_page_vaddr(*pgd) != pgd_page_vaddr(*pgd_ref));\n\n\t\t\tif (pgd_none(*pgd))\n\t\t\t\tset_pgd(pgd, *pgd_ref);\n\n\t\t\tspin_unlock(pgt_lock);\n\t\t}\n\t\tspin_unlock(&pgd_lock);\n\t}\n}\n\nstatic void sync_global_pgds_l4(unsigned long start, unsigned long end)\n{\n\tunsigned long addr;\n\n\tfor (addr = start; addr <= end; addr = ALIGN(addr + 1, PGDIR_SIZE)) {\n\t\tpgd_t *pgd_ref = pgd_offset_k(addr);\n\t\tconst p4d_t *p4d_ref;\n\t\tstruct page *page;\n\n\t\t \n\t\tMAYBE_BUILD_BUG_ON(pgd_none(*pgd_ref));\n\t\tp4d_ref = p4d_offset(pgd_ref, addr);\n\n\t\tif (p4d_none(*p4d_ref))\n\t\t\tcontinue;\n\n\t\tspin_lock(&pgd_lock);\n\t\tlist_for_each_entry(page, &pgd_list, lru) {\n\t\t\tpgd_t *pgd;\n\t\t\tp4d_t *p4d;\n\t\t\tspinlock_t *pgt_lock;\n\n\t\t\tpgd = (pgd_t *)page_address(page) + pgd_index(addr);\n\t\t\tp4d = p4d_offset(pgd, addr);\n\t\t\t \n\t\t\tpgt_lock = &pgd_page_get_mm(page)->page_table_lock;\n\t\t\tspin_lock(pgt_lock);\n\n\t\t\tif (!p4d_none(*p4d_ref) && !p4d_none(*p4d))\n\t\t\t\tBUG_ON(p4d_pgtable(*p4d)\n\t\t\t\t       != p4d_pgtable(*p4d_ref));\n\n\t\t\tif (p4d_none(*p4d))\n\t\t\t\tset_p4d(p4d, *p4d_ref);\n\n\t\t\tspin_unlock(pgt_lock);\n\t\t}\n\t\tspin_unlock(&pgd_lock);\n\t}\n}\n\n \nstatic void sync_global_pgds(unsigned long start, unsigned long end)\n{\n\tif (pgtable_l5_enabled())\n\t\tsync_global_pgds_l5(start, end);\n\telse\n\t\tsync_global_pgds_l4(start, end);\n}\n\n \nstatic __ref void *spp_getpage(void)\n{\n\tvoid *ptr;\n\n\tif (after_bootmem)\n\t\tptr = (void *) get_zeroed_page(GFP_ATOMIC);\n\telse\n\t\tptr = memblock_alloc(PAGE_SIZE, PAGE_SIZE);\n\n\tif (!ptr || ((unsigned long)ptr & ~PAGE_MASK)) {\n\t\tpanic(\"set_pte_phys: cannot allocate page data %s\\n\",\n\t\t\tafter_bootmem ? \"after bootmem\" : \"\");\n\t}\n\n\tpr_debug(\"spp_getpage %p\\n\", ptr);\n\n\treturn ptr;\n}\n\nstatic p4d_t *fill_p4d(pgd_t *pgd, unsigned long vaddr)\n{\n\tif (pgd_none(*pgd)) {\n\t\tp4d_t *p4d = (p4d_t *)spp_getpage();\n\t\tpgd_populate(&init_mm, pgd, p4d);\n\t\tif (p4d != p4d_offset(pgd, 0))\n\t\t\tprintk(KERN_ERR \"PAGETABLE BUG #00! %p <-> %p\\n\",\n\t\t\t       p4d, p4d_offset(pgd, 0));\n\t}\n\treturn p4d_offset(pgd, vaddr);\n}\n\nstatic pud_t *fill_pud(p4d_t *p4d, unsigned long vaddr)\n{\n\tif (p4d_none(*p4d)) {\n\t\tpud_t *pud = (pud_t *)spp_getpage();\n\t\tp4d_populate(&init_mm, p4d, pud);\n\t\tif (pud != pud_offset(p4d, 0))\n\t\t\tprintk(KERN_ERR \"PAGETABLE BUG #01! %p <-> %p\\n\",\n\t\t\t       pud, pud_offset(p4d, 0));\n\t}\n\treturn pud_offset(p4d, vaddr);\n}\n\nstatic pmd_t *fill_pmd(pud_t *pud, unsigned long vaddr)\n{\n\tif (pud_none(*pud)) {\n\t\tpmd_t *pmd = (pmd_t *) spp_getpage();\n\t\tpud_populate(&init_mm, pud, pmd);\n\t\tif (pmd != pmd_offset(pud, 0))\n\t\t\tprintk(KERN_ERR \"PAGETABLE BUG #02! %p <-> %p\\n\",\n\t\t\t       pmd, pmd_offset(pud, 0));\n\t}\n\treturn pmd_offset(pud, vaddr);\n}\n\nstatic pte_t *fill_pte(pmd_t *pmd, unsigned long vaddr)\n{\n\tif (pmd_none(*pmd)) {\n\t\tpte_t *pte = (pte_t *) spp_getpage();\n\t\tpmd_populate_kernel(&init_mm, pmd, pte);\n\t\tif (pte != pte_offset_kernel(pmd, 0))\n\t\t\tprintk(KERN_ERR \"PAGETABLE BUG #03!\\n\");\n\t}\n\treturn pte_offset_kernel(pmd, vaddr);\n}\n\nstatic void __set_pte_vaddr(pud_t *pud, unsigned long vaddr, pte_t new_pte)\n{\n\tpmd_t *pmd = fill_pmd(pud, vaddr);\n\tpte_t *pte = fill_pte(pmd, vaddr);\n\n\tset_pte(pte, new_pte);\n\n\t \n\tflush_tlb_one_kernel(vaddr);\n}\n\nvoid set_pte_vaddr_p4d(p4d_t *p4d_page, unsigned long vaddr, pte_t new_pte)\n{\n\tp4d_t *p4d = p4d_page + p4d_index(vaddr);\n\tpud_t *pud = fill_pud(p4d, vaddr);\n\n\t__set_pte_vaddr(pud, vaddr, new_pte);\n}\n\nvoid set_pte_vaddr_pud(pud_t *pud_page, unsigned long vaddr, pte_t new_pte)\n{\n\tpud_t *pud = pud_page + pud_index(vaddr);\n\n\t__set_pte_vaddr(pud, vaddr, new_pte);\n}\n\nvoid set_pte_vaddr(unsigned long vaddr, pte_t pteval)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d_page;\n\n\tpr_debug(\"set_pte_vaddr %lx to %lx\\n\", vaddr, native_pte_val(pteval));\n\n\tpgd = pgd_offset_k(vaddr);\n\tif (pgd_none(*pgd)) {\n\t\tprintk(KERN_ERR\n\t\t\t\"PGD FIXMAP MISSING, it should be setup in head.S!\\n\");\n\t\treturn;\n\t}\n\n\tp4d_page = p4d_offset(pgd, 0);\n\tset_pte_vaddr_p4d(p4d_page, vaddr, pteval);\n}\n\npmd_t * __init populate_extra_pmd(unsigned long vaddr)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\n\tpgd = pgd_offset_k(vaddr);\n\tp4d = fill_p4d(pgd, vaddr);\n\tpud = fill_pud(p4d, vaddr);\n\treturn fill_pmd(pud, vaddr);\n}\n\npte_t * __init populate_extra_pte(unsigned long vaddr)\n{\n\tpmd_t *pmd;\n\n\tpmd = populate_extra_pmd(vaddr);\n\treturn fill_pte(pmd, vaddr);\n}\n\n \nstatic void __init __init_extra_mapping(unsigned long phys, unsigned long size,\n\t\t\t\t\tenum page_cache_mode cache)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tpgprot_t prot;\n\n\tpgprot_val(prot) = pgprot_val(PAGE_KERNEL_LARGE) |\n\t\tprotval_4k_2_large(cachemode2protval(cache));\n\tBUG_ON((phys & ~PMD_MASK) || (size & ~PMD_MASK));\n\tfor (; size; phys += PMD_SIZE, size -= PMD_SIZE) {\n\t\tpgd = pgd_offset_k((unsigned long)__va(phys));\n\t\tif (pgd_none(*pgd)) {\n\t\t\tp4d = (p4d_t *) spp_getpage();\n\t\t\tset_pgd(pgd, __pgd(__pa(p4d) | _KERNPG_TABLE |\n\t\t\t\t\t\t_PAGE_USER));\n\t\t}\n\t\tp4d = p4d_offset(pgd, (unsigned long)__va(phys));\n\t\tif (p4d_none(*p4d)) {\n\t\t\tpud = (pud_t *) spp_getpage();\n\t\t\tset_p4d(p4d, __p4d(__pa(pud) | _KERNPG_TABLE |\n\t\t\t\t\t\t_PAGE_USER));\n\t\t}\n\t\tpud = pud_offset(p4d, (unsigned long)__va(phys));\n\t\tif (pud_none(*pud)) {\n\t\t\tpmd = (pmd_t *) spp_getpage();\n\t\t\tset_pud(pud, __pud(__pa(pmd) | _KERNPG_TABLE |\n\t\t\t\t\t\t_PAGE_USER));\n\t\t}\n\t\tpmd = pmd_offset(pud, phys);\n\t\tBUG_ON(!pmd_none(*pmd));\n\t\tset_pmd(pmd, __pmd(phys | pgprot_val(prot)));\n\t}\n}\n\nvoid __init init_extra_mapping_wb(unsigned long phys, unsigned long size)\n{\n\t__init_extra_mapping(phys, size, _PAGE_CACHE_MODE_WB);\n}\n\nvoid __init init_extra_mapping_uc(unsigned long phys, unsigned long size)\n{\n\t__init_extra_mapping(phys, size, _PAGE_CACHE_MODE_UC);\n}\n\n \nvoid __init cleanup_highmap(void)\n{\n\tunsigned long vaddr = __START_KERNEL_map;\n\tunsigned long vaddr_end = __START_KERNEL_map + KERNEL_IMAGE_SIZE;\n\tunsigned long end = roundup((unsigned long)_brk_end, PMD_SIZE) - 1;\n\tpmd_t *pmd = level2_kernel_pgt;\n\n\t \n\tif (max_pfn_mapped)\n\t\tvaddr_end = __START_KERNEL_map + (max_pfn_mapped << PAGE_SHIFT);\n\n\tfor (; vaddr + PMD_SIZE - 1 < vaddr_end; pmd++, vaddr += PMD_SIZE) {\n\t\tif (pmd_none(*pmd))\n\t\t\tcontinue;\n\t\tif (vaddr < (unsigned long) _text || vaddr > end)\n\t\t\tset_pmd(pmd, __pmd(0));\n\t}\n}\n\n \nstatic unsigned long __meminit\nphys_pte_init(pte_t *pte_page, unsigned long paddr, unsigned long paddr_end,\n\t      pgprot_t prot, bool init)\n{\n\tunsigned long pages = 0, paddr_next;\n\tunsigned long paddr_last = paddr_end;\n\tpte_t *pte;\n\tint i;\n\n\tpte = pte_page + pte_index(paddr);\n\ti = pte_index(paddr);\n\n\tfor (; i < PTRS_PER_PTE; i++, paddr = paddr_next, pte++) {\n\t\tpaddr_next = (paddr & PAGE_MASK) + PAGE_SIZE;\n\t\tif (paddr >= paddr_end) {\n\t\t\tif (!after_bootmem &&\n\t\t\t    !e820__mapped_any(paddr & PAGE_MASK, paddr_next,\n\t\t\t\t\t     E820_TYPE_RAM) &&\n\t\t\t    !e820__mapped_any(paddr & PAGE_MASK, paddr_next,\n\t\t\t\t\t     E820_TYPE_RESERVED_KERN))\n\t\t\t\tset_pte_init(pte, __pte(0), init);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!pte_none(*pte)) {\n\t\t\tif (!after_bootmem)\n\t\t\t\tpages++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (0)\n\t\t\tpr_info(\"   pte=%p addr=%lx pte=%016lx\\n\", pte, paddr,\n\t\t\t\tpfn_pte(paddr >> PAGE_SHIFT, PAGE_KERNEL).pte);\n\t\tpages++;\n\t\tset_pte_init(pte, pfn_pte(paddr >> PAGE_SHIFT, prot), init);\n\t\tpaddr_last = (paddr & PAGE_MASK) + PAGE_SIZE;\n\t}\n\n\tupdate_page_count(PG_LEVEL_4K, pages);\n\n\treturn paddr_last;\n}\n\n \nstatic unsigned long __meminit\nphys_pmd_init(pmd_t *pmd_page, unsigned long paddr, unsigned long paddr_end,\n\t      unsigned long page_size_mask, pgprot_t prot, bool init)\n{\n\tunsigned long pages = 0, paddr_next;\n\tunsigned long paddr_last = paddr_end;\n\n\tint i = pmd_index(paddr);\n\n\tfor (; i < PTRS_PER_PMD; i++, paddr = paddr_next) {\n\t\tpmd_t *pmd = pmd_page + pmd_index(paddr);\n\t\tpte_t *pte;\n\t\tpgprot_t new_prot = prot;\n\n\t\tpaddr_next = (paddr & PMD_MASK) + PMD_SIZE;\n\t\tif (paddr >= paddr_end) {\n\t\t\tif (!after_bootmem &&\n\t\t\t    !e820__mapped_any(paddr & PMD_MASK, paddr_next,\n\t\t\t\t\t     E820_TYPE_RAM) &&\n\t\t\t    !e820__mapped_any(paddr & PMD_MASK, paddr_next,\n\t\t\t\t\t     E820_TYPE_RESERVED_KERN))\n\t\t\t\tset_pmd_init(pmd, __pmd(0), init);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!pmd_none(*pmd)) {\n\t\t\tif (!pmd_large(*pmd)) {\n\t\t\t\tspin_lock(&init_mm.page_table_lock);\n\t\t\t\tpte = (pte_t *)pmd_page_vaddr(*pmd);\n\t\t\t\tpaddr_last = phys_pte_init(pte, paddr,\n\t\t\t\t\t\t\t   paddr_end, prot,\n\t\t\t\t\t\t\t   init);\n\t\t\t\tspin_unlock(&init_mm.page_table_lock);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t \n\t\t\tif (page_size_mask & (1 << PG_LEVEL_2M)) {\n\t\t\t\tif (!after_bootmem)\n\t\t\t\t\tpages++;\n\t\t\t\tpaddr_last = paddr_next;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tnew_prot = pte_pgprot(pte_clrhuge(*(pte_t *)pmd));\n\t\t}\n\n\t\tif (page_size_mask & (1<<PG_LEVEL_2M)) {\n\t\t\tpages++;\n\t\t\tspin_lock(&init_mm.page_table_lock);\n\t\t\tset_pmd_init(pmd,\n\t\t\t\t     pfn_pmd(paddr >> PAGE_SHIFT, prot_sethuge(prot)),\n\t\t\t\t     init);\n\t\t\tspin_unlock(&init_mm.page_table_lock);\n\t\t\tpaddr_last = paddr_next;\n\t\t\tcontinue;\n\t\t}\n\n\t\tpte = alloc_low_page();\n\t\tpaddr_last = phys_pte_init(pte, paddr, paddr_end, new_prot, init);\n\n\t\tspin_lock(&init_mm.page_table_lock);\n\t\tpmd_populate_kernel_init(&init_mm, pmd, pte, init);\n\t\tspin_unlock(&init_mm.page_table_lock);\n\t}\n\tupdate_page_count(PG_LEVEL_2M, pages);\n\treturn paddr_last;\n}\n\n \nstatic unsigned long __meminit\nphys_pud_init(pud_t *pud_page, unsigned long paddr, unsigned long paddr_end,\n\t      unsigned long page_size_mask, pgprot_t _prot, bool init)\n{\n\tunsigned long pages = 0, paddr_next;\n\tunsigned long paddr_last = paddr_end;\n\tunsigned long vaddr = (unsigned long)__va(paddr);\n\tint i = pud_index(vaddr);\n\n\tfor (; i < PTRS_PER_PUD; i++, paddr = paddr_next) {\n\t\tpud_t *pud;\n\t\tpmd_t *pmd;\n\t\tpgprot_t prot = _prot;\n\n\t\tvaddr = (unsigned long)__va(paddr);\n\t\tpud = pud_page + pud_index(vaddr);\n\t\tpaddr_next = (paddr & PUD_MASK) + PUD_SIZE;\n\n\t\tif (paddr >= paddr_end) {\n\t\t\tif (!after_bootmem &&\n\t\t\t    !e820__mapped_any(paddr & PUD_MASK, paddr_next,\n\t\t\t\t\t     E820_TYPE_RAM) &&\n\t\t\t    !e820__mapped_any(paddr & PUD_MASK, paddr_next,\n\t\t\t\t\t     E820_TYPE_RESERVED_KERN))\n\t\t\t\tset_pud_init(pud, __pud(0), init);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!pud_none(*pud)) {\n\t\t\tif (!pud_large(*pud)) {\n\t\t\t\tpmd = pmd_offset(pud, 0);\n\t\t\t\tpaddr_last = phys_pmd_init(pmd, paddr,\n\t\t\t\t\t\t\t   paddr_end,\n\t\t\t\t\t\t\t   page_size_mask,\n\t\t\t\t\t\t\t   prot, init);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t \n\t\t\tif (page_size_mask & (1 << PG_LEVEL_1G)) {\n\t\t\t\tif (!after_bootmem)\n\t\t\t\t\tpages++;\n\t\t\t\tpaddr_last = paddr_next;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tprot = pte_pgprot(pte_clrhuge(*(pte_t *)pud));\n\t\t}\n\n\t\tif (page_size_mask & (1<<PG_LEVEL_1G)) {\n\t\t\tpages++;\n\t\t\tspin_lock(&init_mm.page_table_lock);\n\t\t\tset_pud_init(pud,\n\t\t\t\t     pfn_pud(paddr >> PAGE_SHIFT, prot_sethuge(prot)),\n\t\t\t\t     init);\n\t\t\tspin_unlock(&init_mm.page_table_lock);\n\t\t\tpaddr_last = paddr_next;\n\t\t\tcontinue;\n\t\t}\n\n\t\tpmd = alloc_low_page();\n\t\tpaddr_last = phys_pmd_init(pmd, paddr, paddr_end,\n\t\t\t\t\t   page_size_mask, prot, init);\n\n\t\tspin_lock(&init_mm.page_table_lock);\n\t\tpud_populate_init(&init_mm, pud, pmd, init);\n\t\tspin_unlock(&init_mm.page_table_lock);\n\t}\n\n\tupdate_page_count(PG_LEVEL_1G, pages);\n\n\treturn paddr_last;\n}\n\nstatic unsigned long __meminit\nphys_p4d_init(p4d_t *p4d_page, unsigned long paddr, unsigned long paddr_end,\n\t      unsigned long page_size_mask, pgprot_t prot, bool init)\n{\n\tunsigned long vaddr, vaddr_end, vaddr_next, paddr_next, paddr_last;\n\n\tpaddr_last = paddr_end;\n\tvaddr = (unsigned long)__va(paddr);\n\tvaddr_end = (unsigned long)__va(paddr_end);\n\n\tif (!pgtable_l5_enabled())\n\t\treturn phys_pud_init((pud_t *) p4d_page, paddr, paddr_end,\n\t\t\t\t     page_size_mask, prot, init);\n\n\tfor (; vaddr < vaddr_end; vaddr = vaddr_next) {\n\t\tp4d_t *p4d = p4d_page + p4d_index(vaddr);\n\t\tpud_t *pud;\n\n\t\tvaddr_next = (vaddr & P4D_MASK) + P4D_SIZE;\n\t\tpaddr = __pa(vaddr);\n\n\t\tif (paddr >= paddr_end) {\n\t\t\tpaddr_next = __pa(vaddr_next);\n\t\t\tif (!after_bootmem &&\n\t\t\t    !e820__mapped_any(paddr & P4D_MASK, paddr_next,\n\t\t\t\t\t     E820_TYPE_RAM) &&\n\t\t\t    !e820__mapped_any(paddr & P4D_MASK, paddr_next,\n\t\t\t\t\t     E820_TYPE_RESERVED_KERN))\n\t\t\t\tset_p4d_init(p4d, __p4d(0), init);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!p4d_none(*p4d)) {\n\t\t\tpud = pud_offset(p4d, 0);\n\t\t\tpaddr_last = phys_pud_init(pud, paddr, __pa(vaddr_end),\n\t\t\t\t\tpage_size_mask, prot, init);\n\t\t\tcontinue;\n\t\t}\n\n\t\tpud = alloc_low_page();\n\t\tpaddr_last = phys_pud_init(pud, paddr, __pa(vaddr_end),\n\t\t\t\t\t   page_size_mask, prot, init);\n\n\t\tspin_lock(&init_mm.page_table_lock);\n\t\tp4d_populate_init(&init_mm, p4d, pud, init);\n\t\tspin_unlock(&init_mm.page_table_lock);\n\t}\n\n\treturn paddr_last;\n}\n\nstatic unsigned long __meminit\n__kernel_physical_mapping_init(unsigned long paddr_start,\n\t\t\t       unsigned long paddr_end,\n\t\t\t       unsigned long page_size_mask,\n\t\t\t       pgprot_t prot, bool init)\n{\n\tbool pgd_changed = false;\n\tunsigned long vaddr, vaddr_start, vaddr_end, vaddr_next, paddr_last;\n\n\tpaddr_last = paddr_end;\n\tvaddr = (unsigned long)__va(paddr_start);\n\tvaddr_end = (unsigned long)__va(paddr_end);\n\tvaddr_start = vaddr;\n\n\tfor (; vaddr < vaddr_end; vaddr = vaddr_next) {\n\t\tpgd_t *pgd = pgd_offset_k(vaddr);\n\t\tp4d_t *p4d;\n\n\t\tvaddr_next = (vaddr & PGDIR_MASK) + PGDIR_SIZE;\n\n\t\tif (pgd_val(*pgd)) {\n\t\t\tp4d = (p4d_t *)pgd_page_vaddr(*pgd);\n\t\t\tpaddr_last = phys_p4d_init(p4d, __pa(vaddr),\n\t\t\t\t\t\t   __pa(vaddr_end),\n\t\t\t\t\t\t   page_size_mask,\n\t\t\t\t\t\t   prot, init);\n\t\t\tcontinue;\n\t\t}\n\n\t\tp4d = alloc_low_page();\n\t\tpaddr_last = phys_p4d_init(p4d, __pa(vaddr), __pa(vaddr_end),\n\t\t\t\t\t   page_size_mask, prot, init);\n\n\t\tspin_lock(&init_mm.page_table_lock);\n\t\tif (pgtable_l5_enabled())\n\t\t\tpgd_populate_init(&init_mm, pgd, p4d, init);\n\t\telse\n\t\t\tp4d_populate_init(&init_mm, p4d_offset(pgd, vaddr),\n\t\t\t\t\t  (pud_t *) p4d, init);\n\n\t\tspin_unlock(&init_mm.page_table_lock);\n\t\tpgd_changed = true;\n\t}\n\n\tif (pgd_changed)\n\t\tsync_global_pgds(vaddr_start, vaddr_end - 1);\n\n\treturn paddr_last;\n}\n\n\n \nunsigned long __meminit\nkernel_physical_mapping_init(unsigned long paddr_start,\n\t\t\t     unsigned long paddr_end,\n\t\t\t     unsigned long page_size_mask, pgprot_t prot)\n{\n\treturn __kernel_physical_mapping_init(paddr_start, paddr_end,\n\t\t\t\t\t      page_size_mask, prot, true);\n}\n\n \nunsigned long __meminit\nkernel_physical_mapping_change(unsigned long paddr_start,\n\t\t\t       unsigned long paddr_end,\n\t\t\t       unsigned long page_size_mask)\n{\n\treturn __kernel_physical_mapping_init(paddr_start, paddr_end,\n\t\t\t\t\t      page_size_mask, PAGE_KERNEL,\n\t\t\t\t\t      false);\n}\n\n#ifndef CONFIG_NUMA\nvoid __init initmem_init(void)\n{\n\tmemblock_set_node(0, PHYS_ADDR_MAX, &memblock.memory, 0);\n}\n#endif\n\nvoid __init paging_init(void)\n{\n\tsparse_init();\n\n\t \n\tnode_clear_state(0, N_MEMORY);\n\tnode_clear_state(0, N_NORMAL_MEMORY);\n\n\tzone_sizes_init();\n}\n\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\n#define PAGE_UNUSED 0xFD\n\n \nstatic unsigned long unused_pmd_start __meminitdata;\n\nstatic void __meminit vmemmap_flush_unused_pmd(void)\n{\n\tif (!unused_pmd_start)\n\t\treturn;\n\t \n\tmemset((void *)unused_pmd_start, PAGE_UNUSED,\n\t       ALIGN(unused_pmd_start, PMD_SIZE) - unused_pmd_start);\n\tunused_pmd_start = 0;\n}\n\n#ifdef CONFIG_MEMORY_HOTPLUG\n \nstatic bool __meminit vmemmap_pmd_is_unused(unsigned long addr, unsigned long end)\n{\n\tunsigned long start = ALIGN_DOWN(addr, PMD_SIZE);\n\n\t \n\tvmemmap_flush_unused_pmd();\n\tmemset((void *)addr, PAGE_UNUSED, end - addr);\n\n\treturn !memchr_inv((void *)start, PAGE_UNUSED, PMD_SIZE);\n}\n#endif\n\nstatic void __meminit __vmemmap_use_sub_pmd(unsigned long start)\n{\n\t \n\tmemset((void *)start, 0, sizeof(struct page));\n}\n\nstatic void __meminit vmemmap_use_sub_pmd(unsigned long start, unsigned long end)\n{\n\t \n\tif (unused_pmd_start == start) {\n\t\tif (likely(IS_ALIGNED(end, PMD_SIZE)))\n\t\t\tunused_pmd_start = 0;\n\t\telse\n\t\t\tunused_pmd_start = end;\n\t\treturn;\n\t}\n\n\t \n\tvmemmap_flush_unused_pmd();\n\t__vmemmap_use_sub_pmd(start);\n}\n\n\nstatic void __meminit vmemmap_use_new_sub_pmd(unsigned long start, unsigned long end)\n{\n\tconst unsigned long page = ALIGN_DOWN(start, PMD_SIZE);\n\n\tvmemmap_flush_unused_pmd();\n\n\t \n\t__vmemmap_use_sub_pmd(start);\n\n\t \n\tif (!IS_ALIGNED(start, PMD_SIZE))\n\t\tmemset((void *)page, PAGE_UNUSED, start - page);\n\n\t \n\tif (!IS_ALIGNED(end, PMD_SIZE))\n\t\tunused_pmd_start = end;\n}\n#endif\n\n \n#ifdef CONFIG_MEMORY_HOTPLUG\n \nstatic void update_end_of_memory_vars(u64 start, u64 size)\n{\n\tunsigned long end_pfn = PFN_UP(start + size);\n\n\tif (end_pfn > max_pfn) {\n\t\tmax_pfn = end_pfn;\n\t\tmax_low_pfn = end_pfn;\n\t\thigh_memory = (void *)__va(max_pfn * PAGE_SIZE - 1) + 1;\n\t}\n}\n\nint add_pages(int nid, unsigned long start_pfn, unsigned long nr_pages,\n\t      struct mhp_params *params)\n{\n\tint ret;\n\n\tret = __add_pages(nid, start_pfn, nr_pages, params);\n\tWARN_ON_ONCE(ret);\n\n\t \n\tupdate_end_of_memory_vars(start_pfn << PAGE_SHIFT,\n\t\t\t\t  nr_pages << PAGE_SHIFT);\n\n\treturn ret;\n}\n\nint arch_add_memory(int nid, u64 start, u64 size,\n\t\t    struct mhp_params *params)\n{\n\tunsigned long start_pfn = start >> PAGE_SHIFT;\n\tunsigned long nr_pages = size >> PAGE_SHIFT;\n\n\tinit_memory_mapping(start, start + size, params->pgprot);\n\n\treturn add_pages(nid, start_pfn, nr_pages, params);\n}\n\nstatic void __meminit free_pagetable(struct page *page, int order)\n{\n\tunsigned long magic;\n\tunsigned int nr_pages = 1 << order;\n\n\t \n\tif (PageReserved(page)) {\n\t\t__ClearPageReserved(page);\n\n\t\tmagic = page->index;\n\t\tif (magic == SECTION_INFO || magic == MIX_SECTION_INFO) {\n\t\t\twhile (nr_pages--)\n\t\t\t\tput_page_bootmem(page++);\n\t\t} else\n\t\t\twhile (nr_pages--)\n\t\t\t\tfree_reserved_page(page++);\n\t} else\n\t\tfree_pages((unsigned long)page_address(page), order);\n}\n\nstatic void __meminit free_hugepage_table(struct page *page,\n\t\tstruct vmem_altmap *altmap)\n{\n\tif (altmap)\n\t\tvmem_altmap_free(altmap, PMD_SIZE / PAGE_SIZE);\n\telse\n\t\tfree_pagetable(page, get_order(PMD_SIZE));\n}\n\nstatic void __meminit free_pte_table(pte_t *pte_start, pmd_t *pmd)\n{\n\tpte_t *pte;\n\tint i;\n\n\tfor (i = 0; i < PTRS_PER_PTE; i++) {\n\t\tpte = pte_start + i;\n\t\tif (!pte_none(*pte))\n\t\t\treturn;\n\t}\n\n\t \n\tfree_pagetable(pmd_page(*pmd), 0);\n\tspin_lock(&init_mm.page_table_lock);\n\tpmd_clear(pmd);\n\tspin_unlock(&init_mm.page_table_lock);\n}\n\nstatic void __meminit free_pmd_table(pmd_t *pmd_start, pud_t *pud)\n{\n\tpmd_t *pmd;\n\tint i;\n\n\tfor (i = 0; i < PTRS_PER_PMD; i++) {\n\t\tpmd = pmd_start + i;\n\t\tif (!pmd_none(*pmd))\n\t\t\treturn;\n\t}\n\n\t \n\tfree_pagetable(pud_page(*pud), 0);\n\tspin_lock(&init_mm.page_table_lock);\n\tpud_clear(pud);\n\tspin_unlock(&init_mm.page_table_lock);\n}\n\nstatic void __meminit free_pud_table(pud_t *pud_start, p4d_t *p4d)\n{\n\tpud_t *pud;\n\tint i;\n\n\tfor (i = 0; i < PTRS_PER_PUD; i++) {\n\t\tpud = pud_start + i;\n\t\tif (!pud_none(*pud))\n\t\t\treturn;\n\t}\n\n\t \n\tfree_pagetable(p4d_page(*p4d), 0);\n\tspin_lock(&init_mm.page_table_lock);\n\tp4d_clear(p4d);\n\tspin_unlock(&init_mm.page_table_lock);\n}\n\nstatic void __meminit\nremove_pte_table(pte_t *pte_start, unsigned long addr, unsigned long end,\n\t\t bool direct)\n{\n\tunsigned long next, pages = 0;\n\tpte_t *pte;\n\tphys_addr_t phys_addr;\n\n\tpte = pte_start + pte_index(addr);\n\tfor (; addr < end; addr = next, pte++) {\n\t\tnext = (addr + PAGE_SIZE) & PAGE_MASK;\n\t\tif (next > end)\n\t\t\tnext = end;\n\n\t\tif (!pte_present(*pte))\n\t\t\tcontinue;\n\n\t\t \n\t\tphys_addr = pte_val(*pte) + (addr & PAGE_MASK);\n\t\tif (phys_addr < (phys_addr_t)0x40000000)\n\t\t\treturn;\n\n\t\tif (!direct)\n\t\t\tfree_pagetable(pte_page(*pte), 0);\n\n\t\tspin_lock(&init_mm.page_table_lock);\n\t\tpte_clear(&init_mm, addr, pte);\n\t\tspin_unlock(&init_mm.page_table_lock);\n\n\t\t \n\t\tpages++;\n\t}\n\n\t \n\tflush_tlb_all();\n\tif (direct)\n\t\tupdate_page_count(PG_LEVEL_4K, -pages);\n}\n\nstatic void __meminit\nremove_pmd_table(pmd_t *pmd_start, unsigned long addr, unsigned long end,\n\t\t bool direct, struct vmem_altmap *altmap)\n{\n\tunsigned long next, pages = 0;\n\tpte_t *pte_base;\n\tpmd_t *pmd;\n\n\tpmd = pmd_start + pmd_index(addr);\n\tfor (; addr < end; addr = next, pmd++) {\n\t\tnext = pmd_addr_end(addr, end);\n\n\t\tif (!pmd_present(*pmd))\n\t\t\tcontinue;\n\n\t\tif (pmd_large(*pmd)) {\n\t\t\tif (IS_ALIGNED(addr, PMD_SIZE) &&\n\t\t\t    IS_ALIGNED(next, PMD_SIZE)) {\n\t\t\t\tif (!direct)\n\t\t\t\t\tfree_hugepage_table(pmd_page(*pmd),\n\t\t\t\t\t\t\t    altmap);\n\n\t\t\t\tspin_lock(&init_mm.page_table_lock);\n\t\t\t\tpmd_clear(pmd);\n\t\t\t\tspin_unlock(&init_mm.page_table_lock);\n\t\t\t\tpages++;\n\t\t\t}\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\n\t\t\telse if (vmemmap_pmd_is_unused(addr, next)) {\n\t\t\t\t\tfree_hugepage_table(pmd_page(*pmd),\n\t\t\t\t\t\t\t    altmap);\n\t\t\t\t\tspin_lock(&init_mm.page_table_lock);\n\t\t\t\t\tpmd_clear(pmd);\n\t\t\t\t\tspin_unlock(&init_mm.page_table_lock);\n\t\t\t}\n#endif\n\t\t\tcontinue;\n\t\t}\n\n\t\tpte_base = (pte_t *)pmd_page_vaddr(*pmd);\n\t\tremove_pte_table(pte_base, addr, next, direct);\n\t\tfree_pte_table(pte_base, pmd);\n\t}\n\n\t \n\tif (direct)\n\t\tupdate_page_count(PG_LEVEL_2M, -pages);\n}\n\nstatic void __meminit\nremove_pud_table(pud_t *pud_start, unsigned long addr, unsigned long end,\n\t\t struct vmem_altmap *altmap, bool direct)\n{\n\tunsigned long next, pages = 0;\n\tpmd_t *pmd_base;\n\tpud_t *pud;\n\n\tpud = pud_start + pud_index(addr);\n\tfor (; addr < end; addr = next, pud++) {\n\t\tnext = pud_addr_end(addr, end);\n\n\t\tif (!pud_present(*pud))\n\t\t\tcontinue;\n\n\t\tif (pud_large(*pud) &&\n\t\t    IS_ALIGNED(addr, PUD_SIZE) &&\n\t\t    IS_ALIGNED(next, PUD_SIZE)) {\n\t\t\tspin_lock(&init_mm.page_table_lock);\n\t\t\tpud_clear(pud);\n\t\t\tspin_unlock(&init_mm.page_table_lock);\n\t\t\tpages++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tpmd_base = pmd_offset(pud, 0);\n\t\tremove_pmd_table(pmd_base, addr, next, direct, altmap);\n\t\tfree_pmd_table(pmd_base, pud);\n\t}\n\n\tif (direct)\n\t\tupdate_page_count(PG_LEVEL_1G, -pages);\n}\n\nstatic void __meminit\nremove_p4d_table(p4d_t *p4d_start, unsigned long addr, unsigned long end,\n\t\t struct vmem_altmap *altmap, bool direct)\n{\n\tunsigned long next, pages = 0;\n\tpud_t *pud_base;\n\tp4d_t *p4d;\n\n\tp4d = p4d_start + p4d_index(addr);\n\tfor (; addr < end; addr = next, p4d++) {\n\t\tnext = p4d_addr_end(addr, end);\n\n\t\tif (!p4d_present(*p4d))\n\t\t\tcontinue;\n\n\t\tBUILD_BUG_ON(p4d_large(*p4d));\n\n\t\tpud_base = pud_offset(p4d, 0);\n\t\tremove_pud_table(pud_base, addr, next, altmap, direct);\n\t\t \n\t\tif (pgtable_l5_enabled())\n\t\t\tfree_pud_table(pud_base, p4d);\n\t}\n\n\tif (direct)\n\t\tupdate_page_count(PG_LEVEL_512G, -pages);\n}\n\n \nstatic void __meminit\nremove_pagetable(unsigned long start, unsigned long end, bool direct,\n\t\tstruct vmem_altmap *altmap)\n{\n\tunsigned long next;\n\tunsigned long addr;\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\n\tfor (addr = start; addr < end; addr = next) {\n\t\tnext = pgd_addr_end(addr, end);\n\n\t\tpgd = pgd_offset_k(addr);\n\t\tif (!pgd_present(*pgd))\n\t\t\tcontinue;\n\n\t\tp4d = p4d_offset(pgd, 0);\n\t\tremove_p4d_table(p4d, addr, next, altmap, direct);\n\t}\n\n\tflush_tlb_all();\n}\n\nvoid __ref vmemmap_free(unsigned long start, unsigned long end,\n\t\tstruct vmem_altmap *altmap)\n{\n\tVM_BUG_ON(!PAGE_ALIGNED(start));\n\tVM_BUG_ON(!PAGE_ALIGNED(end));\n\n\tremove_pagetable(start, end, false, altmap);\n}\n\nstatic void __meminit\nkernel_physical_mapping_remove(unsigned long start, unsigned long end)\n{\n\tstart = (unsigned long)__va(start);\n\tend = (unsigned long)__va(end);\n\n\tremove_pagetable(start, end, true, NULL);\n}\n\nvoid __ref arch_remove_memory(u64 start, u64 size, struct vmem_altmap *altmap)\n{\n\tunsigned long start_pfn = start >> PAGE_SHIFT;\n\tunsigned long nr_pages = size >> PAGE_SHIFT;\n\n\t__remove_pages(start_pfn, nr_pages, altmap);\n\tkernel_physical_mapping_remove(start, start + size);\n}\n#endif  \n\nstatic struct kcore_list kcore_vsyscall;\n\nstatic void __init register_page_bootmem_info(void)\n{\n#if defined(CONFIG_NUMA) || defined(CONFIG_HUGETLB_PAGE_OPTIMIZE_VMEMMAP)\n\tint i;\n\n\tfor_each_online_node(i)\n\t\tregister_page_bootmem_info_node(NODE_DATA(i));\n#endif\n}\n\n \nstatic void __init preallocate_vmalloc_pages(void)\n{\n\tunsigned long addr;\n\tconst char *lvl;\n\n\tfor (addr = VMALLOC_START; addr <= VMEMORY_END; addr = ALIGN(addr + 1, PGDIR_SIZE)) {\n\t\tpgd_t *pgd = pgd_offset_k(addr);\n\t\tp4d_t *p4d;\n\t\tpud_t *pud;\n\n\t\tlvl = \"p4d\";\n\t\tp4d = p4d_alloc(&init_mm, pgd, addr);\n\t\tif (!p4d)\n\t\t\tgoto failed;\n\n\t\tif (pgtable_l5_enabled())\n\t\t\tcontinue;\n\n\t\t \n\t\tlvl = \"pud\";\n\t\tpud = pud_alloc(&init_mm, p4d, addr);\n\t\tif (!pud)\n\t\t\tgoto failed;\n\t}\n\n\treturn;\n\nfailed:\n\n\t \n\tpanic(\"Failed to pre-allocate %s pages for vmalloc area\\n\", lvl);\n}\n\nvoid __init mem_init(void)\n{\n\tpci_iommu_alloc();\n\n\t \n\n\t \n\tmemblock_free_all();\n\tafter_bootmem = 1;\n\tx86_init.hyper.init_after_bootmem();\n\n\t \n\tregister_page_bootmem_info();\n\n\t \n\tif (get_gate_vma(&init_mm))\n\t\tkclist_add(&kcore_vsyscall, (void *)VSYSCALL_ADDR, PAGE_SIZE, KCORE_USER);\n\n\tpreallocate_vmalloc_pages();\n}\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\nint __init deferred_page_init_max_threads(const struct cpumask *node_cpumask)\n{\n\t \n\treturn max_t(int, cpumask_weight(node_cpumask), 1);\n}\n#endif\n\nint kernel_set_to_readonly;\n\nvoid mark_rodata_ro(void)\n{\n\tunsigned long start = PFN_ALIGN(_text);\n\tunsigned long rodata_start = PFN_ALIGN(__start_rodata);\n\tunsigned long end = (unsigned long)__end_rodata_hpage_align;\n\tunsigned long text_end = PFN_ALIGN(_etext);\n\tunsigned long rodata_end = PFN_ALIGN(__end_rodata);\n\tunsigned long all_end;\n\n\tprintk(KERN_INFO \"Write protecting the kernel read-only data: %luk\\n\",\n\t       (end - start) >> 10);\n\tset_memory_ro(start, (end - start) >> PAGE_SHIFT);\n\n\tkernel_set_to_readonly = 1;\n\n\t \n\tall_end = roundup((unsigned long)_brk_end, PMD_SIZE);\n\tset_memory_nx(text_end, (all_end - text_end) >> PAGE_SHIFT);\n\n\tset_ftrace_ops_ro();\n\n#ifdef CONFIG_CPA_DEBUG\n\tprintk(KERN_INFO \"Testing CPA: undo %lx-%lx\\n\", start, end);\n\tset_memory_rw(start, (end-start) >> PAGE_SHIFT);\n\n\tprintk(KERN_INFO \"Testing CPA: again\\n\");\n\tset_memory_ro(start, (end-start) >> PAGE_SHIFT);\n#endif\n\n\tfree_kernel_image_pages(\"unused kernel image (text/rodata gap)\",\n\t\t\t\t(void *)text_end, (void *)rodata_start);\n\tfree_kernel_image_pages(\"unused kernel image (rodata/data gap)\",\n\t\t\t\t(void *)rodata_end, (void *)_sdata);\n\n\tdebug_checkwx();\n}\n\n \n#define MAX_BLOCK_SIZE (2UL << 30)\n\n \n#define MEM_SIZE_FOR_LARGE_BLOCK (64UL << 30)\n\n \nstatic unsigned long set_memory_block_size;\nint __init set_memory_block_size_order(unsigned int order)\n{\n\tunsigned long size = 1UL << order;\n\n\tif (size > MEM_SIZE_FOR_LARGE_BLOCK || size < MIN_MEMORY_BLOCK_SIZE)\n\t\treturn -EINVAL;\n\n\tset_memory_block_size = size;\n\treturn 0;\n}\n\nstatic unsigned long probe_memory_block_size(void)\n{\n\tunsigned long boot_mem_end = max_pfn << PAGE_SHIFT;\n\tunsigned long bz;\n\n\t \n\tbz = set_memory_block_size;\n\tif (bz)\n\t\tgoto done;\n\n\t \n\tif (boot_mem_end < MEM_SIZE_FOR_LARGE_BLOCK) {\n\t\tbz = MIN_MEMORY_BLOCK_SIZE;\n\t\tgoto done;\n\t}\n\n\t \n\tif (!boot_cpu_has(X86_FEATURE_HYPERVISOR)) {\n\t\tbz = MAX_BLOCK_SIZE;\n\t\tgoto done;\n\t}\n\n\t \n\tfor (bz = MAX_BLOCK_SIZE; bz > MIN_MEMORY_BLOCK_SIZE; bz >>= 1) {\n\t\tif (IS_ALIGNED(boot_mem_end, bz))\n\t\t\tbreak;\n\t}\ndone:\n\tpr_info(\"x86/mm: Memory block size: %ldMB\\n\", bz >> 20);\n\n\treturn bz;\n}\n\nstatic unsigned long memory_block_size_probed;\nunsigned long memory_block_size_bytes(void)\n{\n\tif (!memory_block_size_probed)\n\t\tmemory_block_size_probed = probe_memory_block_size();\n\n\treturn memory_block_size_probed;\n}\n\n#ifdef CONFIG_SPARSEMEM_VMEMMAP\n \nstatic long __meminitdata addr_start, addr_end;\nstatic void __meminitdata *p_start, *p_end;\nstatic int __meminitdata node_start;\n\nvoid __meminit vmemmap_set_pmd(pmd_t *pmd, void *p, int node,\n\t\t\t       unsigned long addr, unsigned long next)\n{\n\tpte_t entry;\n\n\tentry = pfn_pte(__pa(p) >> PAGE_SHIFT,\n\t\t\tPAGE_KERNEL_LARGE);\n\tset_pmd(pmd, __pmd(pte_val(entry)));\n\n\t \n\tif (p_end != p || node_start != node) {\n\t\tif (p_start)\n\t\t\tpr_debug(\" [%lx-%lx] PMD -> [%p-%p] on node %d\\n\",\n\t\t\t\taddr_start, addr_end-1, p_start, p_end-1, node_start);\n\t\taddr_start = addr;\n\t\tnode_start = node;\n\t\tp_start = p;\n\t}\n\n\taddr_end = addr + PMD_SIZE;\n\tp_end = p + PMD_SIZE;\n\n\tif (!IS_ALIGNED(addr, PMD_SIZE) ||\n\t\t!IS_ALIGNED(next, PMD_SIZE))\n\t\tvmemmap_use_new_sub_pmd(addr, next);\n}\n\nint __meminit vmemmap_check_pmd(pmd_t *pmd, int node,\n\t\t\t\tunsigned long addr, unsigned long next)\n{\n\tint large = pmd_large(*pmd);\n\n\tif (pmd_large(*pmd)) {\n\t\tvmemmap_verify((pte_t *)pmd, node, addr, next);\n\t\tvmemmap_use_sub_pmd(addr, next);\n\t}\n\n\treturn large;\n}\n\nint __meminit vmemmap_populate(unsigned long start, unsigned long end, int node,\n\t\tstruct vmem_altmap *altmap)\n{\n\tint err;\n\n\tVM_BUG_ON(!PAGE_ALIGNED(start));\n\tVM_BUG_ON(!PAGE_ALIGNED(end));\n\n\tif (end - start < PAGES_PER_SECTION * sizeof(struct page))\n\t\terr = vmemmap_populate_basepages(start, end, node, NULL);\n\telse if (boot_cpu_has(X86_FEATURE_PSE))\n\t\terr = vmemmap_populate_hugepages(start, end, node, altmap);\n\telse if (altmap) {\n\t\tpr_err_once(\"%s: no cpu support for altmap allocations\\n\",\n\t\t\t\t__func__);\n\t\terr = -ENOMEM;\n\t} else\n\t\terr = vmemmap_populate_basepages(start, end, node, NULL);\n\tif (!err)\n\t\tsync_global_pgds(start, end - 1);\n\treturn err;\n}\n\n#ifdef CONFIG_HAVE_BOOTMEM_INFO_NODE\nvoid register_page_bootmem_memmap(unsigned long section_nr,\n\t\t\t\t  struct page *start_page, unsigned long nr_pages)\n{\n\tunsigned long addr = (unsigned long)start_page;\n\tunsigned long end = (unsigned long)(start_page + nr_pages);\n\tunsigned long next;\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\tunsigned int nr_pmd_pages;\n\tstruct page *page;\n\n\tfor (; addr < end; addr = next) {\n\t\tpte_t *pte = NULL;\n\n\t\tpgd = pgd_offset_k(addr);\n\t\tif (pgd_none(*pgd)) {\n\t\t\tnext = (addr + PAGE_SIZE) & PAGE_MASK;\n\t\t\tcontinue;\n\t\t}\n\t\tget_page_bootmem(section_nr, pgd_page(*pgd), MIX_SECTION_INFO);\n\n\t\tp4d = p4d_offset(pgd, addr);\n\t\tif (p4d_none(*p4d)) {\n\t\t\tnext = (addr + PAGE_SIZE) & PAGE_MASK;\n\t\t\tcontinue;\n\t\t}\n\t\tget_page_bootmem(section_nr, p4d_page(*p4d), MIX_SECTION_INFO);\n\n\t\tpud = pud_offset(p4d, addr);\n\t\tif (pud_none(*pud)) {\n\t\t\tnext = (addr + PAGE_SIZE) & PAGE_MASK;\n\t\t\tcontinue;\n\t\t}\n\t\tget_page_bootmem(section_nr, pud_page(*pud), MIX_SECTION_INFO);\n\n\t\tif (!boot_cpu_has(X86_FEATURE_PSE)) {\n\t\t\tnext = (addr + PAGE_SIZE) & PAGE_MASK;\n\t\t\tpmd = pmd_offset(pud, addr);\n\t\t\tif (pmd_none(*pmd))\n\t\t\t\tcontinue;\n\t\t\tget_page_bootmem(section_nr, pmd_page(*pmd),\n\t\t\t\t\t MIX_SECTION_INFO);\n\n\t\t\tpte = pte_offset_kernel(pmd, addr);\n\t\t\tif (pte_none(*pte))\n\t\t\t\tcontinue;\n\t\t\tget_page_bootmem(section_nr, pte_page(*pte),\n\t\t\t\t\t SECTION_INFO);\n\t\t} else {\n\t\t\tnext = pmd_addr_end(addr, end);\n\n\t\t\tpmd = pmd_offset(pud, addr);\n\t\t\tif (pmd_none(*pmd))\n\t\t\t\tcontinue;\n\n\t\t\tnr_pmd_pages = 1 << get_order(PMD_SIZE);\n\t\t\tpage = pmd_page(*pmd);\n\t\t\twhile (nr_pmd_pages--)\n\t\t\t\tget_page_bootmem(section_nr, page++,\n\t\t\t\t\t\t SECTION_INFO);\n\t\t}\n\t}\n}\n#endif\n\nvoid __meminit vmemmap_populate_print_last(void)\n{\n\tif (p_start) {\n\t\tpr_debug(\" [%lx-%lx] PMD -> [%p-%p] on node %d\\n\",\n\t\t\taddr_start, addr_end-1, p_start, p_end-1, node_start);\n\t\tp_start = NULL;\n\t\tp_end = NULL;\n\t\tnode_start = 0;\n\t}\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}