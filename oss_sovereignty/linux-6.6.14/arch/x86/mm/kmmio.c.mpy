{
  "module_name": "kmmio.c",
  "hash_id": "fb7c602c8545422d4c0c184f0526cab9271bce8892273862251d27e48503471d",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/mm/kmmio.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/list.h>\n#include <linux/rculist.h>\n#include <linux/spinlock.h>\n#include <linux/hash.h>\n#include <linux/export.h>\n#include <linux/kernel.h>\n#include <linux/uaccess.h>\n#include <linux/ptrace.h>\n#include <linux/preempt.h>\n#include <linux/percpu.h>\n#include <linux/kdebug.h>\n#include <linux/mutex.h>\n#include <linux/io.h>\n#include <linux/slab.h>\n#include <asm/cacheflush.h>\n#include <asm/tlbflush.h>\n#include <linux/errno.h>\n#include <asm/debugreg.h>\n#include <linux/mmiotrace.h>\n\n#define KMMIO_PAGE_HASH_BITS 4\n#define KMMIO_PAGE_TABLE_SIZE (1 << KMMIO_PAGE_HASH_BITS)\n\nstruct kmmio_fault_page {\n\tstruct list_head list;\n\tstruct kmmio_fault_page *release_next;\n\tunsigned long addr;  \n\tpteval_t old_presence;  \n\tbool armed;\n\n\t \n\tint count;\n\n\tbool scheduled_for_release;\n};\n\nstruct kmmio_delayed_release {\n\tstruct rcu_head rcu;\n\tstruct kmmio_fault_page *release_list;\n};\n\nstruct kmmio_context {\n\tstruct kmmio_fault_page *fpage;\n\tstruct kmmio_probe *probe;\n\tunsigned long saved_flags;\n\tunsigned long addr;\n\tint active;\n};\n\n \nstatic arch_spinlock_t kmmio_lock = __ARCH_SPIN_LOCK_UNLOCKED;\n\n \nunsigned int kmmio_count;\n\n \nstatic struct list_head kmmio_page_table[KMMIO_PAGE_TABLE_SIZE];\nstatic LIST_HEAD(kmmio_probes);\n\nstatic struct list_head *kmmio_page_list(unsigned long addr)\n{\n\tunsigned int l;\n\tpte_t *pte = lookup_address(addr, &l);\n\n\tif (!pte)\n\t\treturn NULL;\n\taddr &= page_level_mask(l);\n\n\treturn &kmmio_page_table[hash_long(addr, KMMIO_PAGE_HASH_BITS)];\n}\n\n \nstatic DEFINE_PER_CPU(struct kmmio_context, kmmio_ctx);\n\n \n \nstatic struct kmmio_probe *get_kmmio_probe(unsigned long addr)\n{\n\tstruct kmmio_probe *p;\n\tlist_for_each_entry_rcu(p, &kmmio_probes, list) {\n\t\tif (addr >= p->addr && addr < (p->addr + p->len))\n\t\t\treturn p;\n\t}\n\treturn NULL;\n}\n\n \nstatic struct kmmio_fault_page *get_kmmio_fault_page(unsigned long addr)\n{\n\tstruct list_head *head;\n\tstruct kmmio_fault_page *f;\n\tunsigned int l;\n\tpte_t *pte = lookup_address(addr, &l);\n\n\tif (!pte)\n\t\treturn NULL;\n\taddr &= page_level_mask(l);\n\thead = kmmio_page_list(addr);\n\tlist_for_each_entry_rcu(f, head, list) {\n\t\tif (f->addr == addr)\n\t\t\treturn f;\n\t}\n\treturn NULL;\n}\n\nstatic void clear_pmd_presence(pmd_t *pmd, bool clear, pmdval_t *old)\n{\n\tpmd_t new_pmd;\n\tpmdval_t v = pmd_val(*pmd);\n\tif (clear) {\n\t\t*old = v;\n\t\tnew_pmd = pmd_mkinvalid(*pmd);\n\t} else {\n\t\t \n\t\tnew_pmd = __pmd(*old);\n\t}\n\tset_pmd(pmd, new_pmd);\n}\n\nstatic void clear_pte_presence(pte_t *pte, bool clear, pteval_t *old)\n{\n\tpteval_t v = pte_val(*pte);\n\tif (clear) {\n\t\t*old = v;\n\t\t \n\t\tpte_clear(&init_mm, 0, pte);\n\t} else {\n\t\t \n\t\tset_pte_atomic(pte, __pte(*old));\n\t}\n}\n\nstatic int clear_page_presence(struct kmmio_fault_page *f, bool clear)\n{\n\tunsigned int level;\n\tpte_t *pte = lookup_address(f->addr, &level);\n\n\tif (!pte) {\n\t\tpr_err(\"no pte for addr 0x%08lx\\n\", f->addr);\n\t\treturn -1;\n\t}\n\n\tswitch (level) {\n\tcase PG_LEVEL_2M:\n\t\tclear_pmd_presence((pmd_t *)pte, clear, &f->old_presence);\n\t\tbreak;\n\tcase PG_LEVEL_4K:\n\t\tclear_pte_presence(pte, clear, &f->old_presence);\n\t\tbreak;\n\tdefault:\n\t\tpr_err(\"unexpected page level 0x%x.\\n\", level);\n\t\treturn -1;\n\t}\n\n\tflush_tlb_one_kernel(f->addr);\n\treturn 0;\n}\n\n \nstatic int arm_kmmio_fault_page(struct kmmio_fault_page *f)\n{\n\tint ret;\n\tWARN_ONCE(f->armed, KERN_ERR pr_fmt(\"kmmio page already armed.\\n\"));\n\tif (f->armed) {\n\t\tpr_warn(\"double-arm: addr 0x%08lx, ref %d, old %d\\n\",\n\t\t\tf->addr, f->count, !!f->old_presence);\n\t}\n\tret = clear_page_presence(f, true);\n\tWARN_ONCE(ret < 0, KERN_ERR pr_fmt(\"arming at 0x%08lx failed.\\n\"),\n\t\t  f->addr);\n\tf->armed = true;\n\treturn ret;\n}\n\n \nstatic void disarm_kmmio_fault_page(struct kmmio_fault_page *f)\n{\n\tint ret = clear_page_presence(f, false);\n\tWARN_ONCE(ret < 0,\n\t\t\tKERN_ERR \"kmmio disarming at 0x%08lx failed.\\n\", f->addr);\n\tf->armed = false;\n}\n\n \n \nint kmmio_handler(struct pt_regs *regs, unsigned long addr)\n{\n\tstruct kmmio_context *ctx;\n\tstruct kmmio_fault_page *faultpage;\n\tint ret = 0;  \n\tunsigned long page_base = addr;\n\tunsigned int l;\n\tpte_t *pte = lookup_address(addr, &l);\n\tif (!pte)\n\t\treturn -EINVAL;\n\tpage_base &= page_level_mask(l);\n\n\t \n\trcu_read_lock_sched_notrace();\n\n\tfaultpage = get_kmmio_fault_page(page_base);\n\tif (!faultpage) {\n\t\t \n\t\tgoto no_kmmio;\n\t}\n\n\tctx = this_cpu_ptr(&kmmio_ctx);\n\tif (ctx->active) {\n\t\tif (page_base == ctx->addr) {\n\t\t\t \n\t\t\tpr_debug(\"secondary hit for 0x%08lx CPU %d.\\n\",\n\t\t\t\t addr, smp_processor_id());\n\n\t\t\tif (!faultpage->old_presence)\n\t\t\t\tpr_info(\"unexpected secondary hit for address 0x%08lx on CPU %d.\\n\",\n\t\t\t\t\taddr, smp_processor_id());\n\t\t} else {\n\t\t\t \n\t\t\tpr_emerg(\"recursive probe hit on CPU %d, for address 0x%08lx. Ignoring.\\n\",\n\t\t\t\t smp_processor_id(), addr);\n\t\t\tpr_emerg(\"previous hit was at 0x%08lx.\\n\", ctx->addr);\n\t\t\tdisarm_kmmio_fault_page(faultpage);\n\t\t}\n\t\tgoto no_kmmio;\n\t}\n\tctx->active++;\n\n\tctx->fpage = faultpage;\n\tctx->probe = get_kmmio_probe(page_base);\n\tctx->saved_flags = (regs->flags & (X86_EFLAGS_TF | X86_EFLAGS_IF));\n\tctx->addr = page_base;\n\n\tif (ctx->probe && ctx->probe->pre_handler)\n\t\tctx->probe->pre_handler(ctx->probe, regs, addr);\n\n\t \n\tregs->flags |= X86_EFLAGS_TF;\n\tregs->flags &= ~X86_EFLAGS_IF;\n\n\t \n\tdisarm_kmmio_fault_page(ctx->fpage);\n\n\t \n\n\treturn 1;  \n\nno_kmmio:\n\trcu_read_unlock_sched_notrace();\n\treturn ret;\n}\n\n \nstatic int post_kmmio_handler(unsigned long condition, struct pt_regs *regs)\n{\n\tint ret = 0;\n\tstruct kmmio_context *ctx = this_cpu_ptr(&kmmio_ctx);\n\n\tif (!ctx->active) {\n\t\t \n\t\tpr_warn(\"unexpected debug trap on CPU %d.\\n\", smp_processor_id());\n\t\tgoto out;\n\t}\n\n\tif (ctx->probe && ctx->probe->post_handler)\n\t\tctx->probe->post_handler(ctx->probe, condition, regs);\n\n\t \n\tarch_spin_lock(&kmmio_lock);\n\tif (ctx->fpage->count)\n\t\tarm_kmmio_fault_page(ctx->fpage);\n\tarch_spin_unlock(&kmmio_lock);\n\n\tregs->flags &= ~X86_EFLAGS_TF;\n\tregs->flags |= ctx->saved_flags;\n\n\t \n\tctx->active--;\n\tBUG_ON(ctx->active);\n\trcu_read_unlock_sched_notrace();\n\n\t \n\tif (!(regs->flags & X86_EFLAGS_TF))\n\t\tret = 1;\nout:\n\treturn ret;\n}\n\n \nstatic int add_kmmio_fault_page(unsigned long addr)\n{\n\tstruct kmmio_fault_page *f;\n\n\tf = get_kmmio_fault_page(addr);\n\tif (f) {\n\t\tif (!f->count)\n\t\t\tarm_kmmio_fault_page(f);\n\t\tf->count++;\n\t\treturn 0;\n\t}\n\n\tf = kzalloc(sizeof(*f), GFP_ATOMIC);\n\tif (!f)\n\t\treturn -1;\n\n\tf->count = 1;\n\tf->addr = addr;\n\n\tif (arm_kmmio_fault_page(f)) {\n\t\tkfree(f);\n\t\treturn -1;\n\t}\n\n\tlist_add_rcu(&f->list, kmmio_page_list(f->addr));\n\n\treturn 0;\n}\n\n \nstatic void release_kmmio_fault_page(unsigned long addr,\n\t\t\t\tstruct kmmio_fault_page **release_list)\n{\n\tstruct kmmio_fault_page *f;\n\n\tf = get_kmmio_fault_page(addr);\n\tif (!f)\n\t\treturn;\n\n\tf->count--;\n\tBUG_ON(f->count < 0);\n\tif (!f->count) {\n\t\tdisarm_kmmio_fault_page(f);\n\t\tif (!f->scheduled_for_release) {\n\t\t\tf->release_next = *release_list;\n\t\t\t*release_list = f;\n\t\t\tf->scheduled_for_release = true;\n\t\t}\n\t}\n}\n\n \nint register_kmmio_probe(struct kmmio_probe *p)\n{\n\tunsigned long flags;\n\tint ret = 0;\n\tunsigned long size = 0;\n\tunsigned long addr = p->addr & PAGE_MASK;\n\tconst unsigned long size_lim = p->len + (p->addr & ~PAGE_MASK);\n\tunsigned int l;\n\tpte_t *pte;\n\n\tlocal_irq_save(flags);\n\tarch_spin_lock(&kmmio_lock);\n\tif (get_kmmio_probe(addr)) {\n\t\tret = -EEXIST;\n\t\tgoto out;\n\t}\n\n\tpte = lookup_address(addr, &l);\n\tif (!pte) {\n\t\tret = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tkmmio_count++;\n\tlist_add_rcu(&p->list, &kmmio_probes);\n\twhile (size < size_lim) {\n\t\tif (add_kmmio_fault_page(addr + size))\n\t\t\tpr_err(\"Unable to set page fault.\\n\");\n\t\tsize += page_level_size(l);\n\t}\nout:\n\tarch_spin_unlock(&kmmio_lock);\n\tlocal_irq_restore(flags);\n\n\t \n\treturn ret;\n}\nEXPORT_SYMBOL(register_kmmio_probe);\n\nstatic void rcu_free_kmmio_fault_pages(struct rcu_head *head)\n{\n\tstruct kmmio_delayed_release *dr = container_of(\n\t\t\t\t\t\thead,\n\t\t\t\t\t\tstruct kmmio_delayed_release,\n\t\t\t\t\t\trcu);\n\tstruct kmmio_fault_page *f = dr->release_list;\n\twhile (f) {\n\t\tstruct kmmio_fault_page *next = f->release_next;\n\t\tBUG_ON(f->count);\n\t\tkfree(f);\n\t\tf = next;\n\t}\n\tkfree(dr);\n}\n\nstatic void remove_kmmio_fault_pages(struct rcu_head *head)\n{\n\tstruct kmmio_delayed_release *dr =\n\t\tcontainer_of(head, struct kmmio_delayed_release, rcu);\n\tstruct kmmio_fault_page *f = dr->release_list;\n\tstruct kmmio_fault_page **prevp = &dr->release_list;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tarch_spin_lock(&kmmio_lock);\n\twhile (f) {\n\t\tif (!f->count) {\n\t\t\tlist_del_rcu(&f->list);\n\t\t\tprevp = &f->release_next;\n\t\t} else {\n\t\t\t*prevp = f->release_next;\n\t\t\tf->release_next = NULL;\n\t\t\tf->scheduled_for_release = false;\n\t\t}\n\t\tf = *prevp;\n\t}\n\tarch_spin_unlock(&kmmio_lock);\n\tlocal_irq_restore(flags);\n\n\t \n\tcall_rcu(&dr->rcu, rcu_free_kmmio_fault_pages);\n}\n\n \nvoid unregister_kmmio_probe(struct kmmio_probe *p)\n{\n\tunsigned long flags;\n\tunsigned long size = 0;\n\tunsigned long addr = p->addr & PAGE_MASK;\n\tconst unsigned long size_lim = p->len + (p->addr & ~PAGE_MASK);\n\tstruct kmmio_fault_page *release_list = NULL;\n\tstruct kmmio_delayed_release *drelease;\n\tunsigned int l;\n\tpte_t *pte;\n\n\tpte = lookup_address(addr, &l);\n\tif (!pte)\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\tarch_spin_lock(&kmmio_lock);\n\twhile (size < size_lim) {\n\t\trelease_kmmio_fault_page(addr + size, &release_list);\n\t\tsize += page_level_size(l);\n\t}\n\tlist_del_rcu(&p->list);\n\tkmmio_count--;\n\tarch_spin_unlock(&kmmio_lock);\n\tlocal_irq_restore(flags);\n\n\tif (!release_list)\n\t\treturn;\n\n\tdrelease = kmalloc(sizeof(*drelease), GFP_ATOMIC);\n\tif (!drelease) {\n\t\tpr_crit(\"leaking kmmio_fault_page objects.\\n\");\n\t\treturn;\n\t}\n\tdrelease->release_list = release_list;\n\n\t \n\tcall_rcu(&drelease->rcu, remove_kmmio_fault_pages);\n}\nEXPORT_SYMBOL(unregister_kmmio_probe);\n\nstatic int\nkmmio_die_notifier(struct notifier_block *nb, unsigned long val, void *args)\n{\n\tstruct die_args *arg = args;\n\tunsigned long* dr6_p = (unsigned long *)ERR_PTR(arg->err);\n\n\tif (val == DIE_DEBUG && (*dr6_p & DR_STEP))\n\t\tif (post_kmmio_handler(*dr6_p, arg->regs) == 1) {\n\t\t\t \n\t\t\t*dr6_p &= ~DR_STEP;\n\t\t\treturn NOTIFY_STOP;\n\t\t}\n\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block nb_die = {\n\t.notifier_call = kmmio_die_notifier\n};\n\nint kmmio_init(void)\n{\n\tint i;\n\n\tfor (i = 0; i < KMMIO_PAGE_TABLE_SIZE; i++)\n\t\tINIT_LIST_HEAD(&kmmio_page_table[i]);\n\n\treturn register_die_notifier(&nb_die);\n}\n\nvoid kmmio_cleanup(void)\n{\n\tint i;\n\n\tunregister_die_notifier(&nb_die);\n\tfor (i = 0; i < KMMIO_PAGE_TABLE_SIZE; i++) {\n\t\tWARN_ONCE(!list_empty(&kmmio_page_table[i]),\n\t\t\tKERN_ERR \"kmmio_page_table not empty at cleanup, any further tracing will leak memory.\\n\");\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}