{
  "module_name": "tlb.c",
  "hash_id": "72c2b70a5635c032b8e1d3e2603fd9b4a22a93bcf8705619747fb16a1f9b7196",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/mm/tlb.c",
  "human_readable_source": "\n#include <linux/init.h>\n\n#include <linux/mm.h>\n#include <linux/spinlock.h>\n#include <linux/smp.h>\n#include <linux/interrupt.h>\n#include <linux/export.h>\n#include <linux/cpu.h>\n#include <linux/debugfs.h>\n#include <linux/sched/smt.h>\n#include <linux/task_work.h>\n#include <linux/mmu_notifier.h>\n\n#include <asm/tlbflush.h>\n#include <asm/mmu_context.h>\n#include <asm/nospec-branch.h>\n#include <asm/cache.h>\n#include <asm/cacheflush.h>\n#include <asm/apic.h>\n#include <asm/perf_event.h>\n\n#include \"mm_internal.h\"\n\n#ifdef CONFIG_PARAVIRT\n# define STATIC_NOPV\n#else\n# define STATIC_NOPV\t\t\tstatic\n# define __flush_tlb_local\t\tnative_flush_tlb_local\n# define __flush_tlb_global\t\tnative_flush_tlb_global\n# define __flush_tlb_one_user(addr)\tnative_flush_tlb_one_user(addr)\n# define __flush_tlb_multi(msk, info)\tnative_flush_tlb_multi(msk, info)\n#endif\n\n \n\n \n#define LAST_USER_MM_IBPB\t0x1UL\n#define LAST_USER_MM_L1D_FLUSH\t0x2UL\n#define LAST_USER_MM_SPEC_MASK\t(LAST_USER_MM_IBPB | LAST_USER_MM_L1D_FLUSH)\n\n \n#define LAST_USER_MM_INIT\tLAST_USER_MM_IBPB\n\n \n\n \n#define CR3_HW_ASID_BITS\t\t12\n\n \n#ifdef CONFIG_PAGE_TABLE_ISOLATION\n# define PTI_CONSUMED_PCID_BITS\t1\n#else\n# define PTI_CONSUMED_PCID_BITS\t0\n#endif\n\n#define CR3_AVAIL_PCID_BITS (X86_CR3_PCID_BITS - PTI_CONSUMED_PCID_BITS)\n\n \n#define MAX_ASID_AVAILABLE ((1 << CR3_AVAIL_PCID_BITS) - 2)\n\n \nstatic inline u16 kern_pcid(u16 asid)\n{\n\tVM_WARN_ON_ONCE(asid > MAX_ASID_AVAILABLE);\n\n#ifdef CONFIG_PAGE_TABLE_ISOLATION\n\t \n\tBUILD_BUG_ON(TLB_NR_DYN_ASIDS >= (1 << X86_CR3_PTI_PCID_USER_BIT));\n\n\t \n\tVM_WARN_ON_ONCE(asid & (1 << X86_CR3_PTI_PCID_USER_BIT));\n#endif\n\t \n\treturn asid + 1;\n}\n\n \nstatic inline u16 user_pcid(u16 asid)\n{\n\tu16 ret = kern_pcid(asid);\n#ifdef CONFIG_PAGE_TABLE_ISOLATION\n\tret |= 1 << X86_CR3_PTI_PCID_USER_BIT;\n#endif\n\treturn ret;\n}\n\nstatic inline unsigned long build_cr3(pgd_t *pgd, u16 asid, unsigned long lam)\n{\n\tunsigned long cr3 = __sme_pa(pgd) | lam;\n\n\tif (static_cpu_has(X86_FEATURE_PCID)) {\n\t\tVM_WARN_ON_ONCE(asid > MAX_ASID_AVAILABLE);\n\t\tcr3 |= kern_pcid(asid);\n\t} else {\n\t\tVM_WARN_ON_ONCE(asid != 0);\n\t}\n\n\treturn cr3;\n}\n\nstatic inline unsigned long build_cr3_noflush(pgd_t *pgd, u16 asid,\n\t\t\t\t\t      unsigned long lam)\n{\n\t \n\tVM_WARN_ON_ONCE(!boot_cpu_has(X86_FEATURE_PCID));\n\treturn build_cr3(pgd, asid, lam) | CR3_NOFLUSH;\n}\n\n \nstatic void clear_asid_other(void)\n{\n\tu16 asid;\n\n\t \n\tif (!static_cpu_has(X86_FEATURE_PTI)) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn;\n\t}\n\n\tfor (asid = 0; asid < TLB_NR_DYN_ASIDS; asid++) {\n\t\t \n\t\tif (asid == this_cpu_read(cpu_tlbstate.loaded_mm_asid))\n\t\t\tcontinue;\n\t\t \n\t\tthis_cpu_write(cpu_tlbstate.ctxs[asid].ctx_id, 0);\n\t}\n\tthis_cpu_write(cpu_tlbstate.invalidate_other, false);\n}\n\natomic64_t last_mm_ctx_id = ATOMIC64_INIT(1);\n\n\nstatic void choose_new_asid(struct mm_struct *next, u64 next_tlb_gen,\n\t\t\t    u16 *new_asid, bool *need_flush)\n{\n\tu16 asid;\n\n\tif (!static_cpu_has(X86_FEATURE_PCID)) {\n\t\t*new_asid = 0;\n\t\t*need_flush = true;\n\t\treturn;\n\t}\n\n\tif (this_cpu_read(cpu_tlbstate.invalidate_other))\n\t\tclear_asid_other();\n\n\tfor (asid = 0; asid < TLB_NR_DYN_ASIDS; asid++) {\n\t\tif (this_cpu_read(cpu_tlbstate.ctxs[asid].ctx_id) !=\n\t\t    next->context.ctx_id)\n\t\t\tcontinue;\n\n\t\t*new_asid = asid;\n\t\t*need_flush = (this_cpu_read(cpu_tlbstate.ctxs[asid].tlb_gen) <\n\t\t\t       next_tlb_gen);\n\t\treturn;\n\t}\n\n\t \n\t*new_asid = this_cpu_add_return(cpu_tlbstate.next_asid, 1) - 1;\n\tif (*new_asid >= TLB_NR_DYN_ASIDS) {\n\t\t*new_asid = 0;\n\t\tthis_cpu_write(cpu_tlbstate.next_asid, 1);\n\t}\n\t*need_flush = true;\n}\n\n \nstatic inline void invalidate_user_asid(u16 asid)\n{\n\t \n\tif (!IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION))\n\t\treturn;\n\n\t \n\tif (!cpu_feature_enabled(X86_FEATURE_PCID))\n\t\treturn;\n\n\tif (!static_cpu_has(X86_FEATURE_PTI))\n\t\treturn;\n\n\t__set_bit(kern_pcid(asid),\n\t\t  (unsigned long *)this_cpu_ptr(&cpu_tlbstate.user_pcid_flush_mask));\n}\n\nstatic void load_new_mm_cr3(pgd_t *pgdir, u16 new_asid, unsigned long lam,\n\t\t\t    bool need_flush)\n{\n\tunsigned long new_mm_cr3;\n\n\tif (need_flush) {\n\t\tinvalidate_user_asid(new_asid);\n\t\tnew_mm_cr3 = build_cr3(pgdir, new_asid, lam);\n\t} else {\n\t\tnew_mm_cr3 = build_cr3_noflush(pgdir, new_asid, lam);\n\t}\n\n\t \n\twrite_cr3(new_mm_cr3);\n}\n\nvoid leave_mm(int cpu)\n{\n\tstruct mm_struct *loaded_mm = this_cpu_read(cpu_tlbstate.loaded_mm);\n\n\t \n\tif (loaded_mm == &init_mm)\n\t\treturn;\n\n\t \n\tWARN_ON(!this_cpu_read(cpu_tlbstate_shared.is_lazy));\n\n\tswitch_mm(NULL, &init_mm, NULL);\n}\nEXPORT_SYMBOL_GPL(leave_mm);\n\nvoid switch_mm(struct mm_struct *prev, struct mm_struct *next,\n\t       struct task_struct *tsk)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tswitch_mm_irqs_off(prev, next, tsk);\n\tlocal_irq_restore(flags);\n}\n\n \nstatic void l1d_flush_force_sigbus(struct callback_head *ch)\n{\n\tforce_sig(SIGBUS);\n}\n\nstatic void l1d_flush_evaluate(unsigned long prev_mm, unsigned long next_mm,\n\t\t\t\tstruct task_struct *next)\n{\n\t \n\tif (prev_mm & LAST_USER_MM_L1D_FLUSH)\n\t\twrmsrl(MSR_IA32_FLUSH_CMD, L1D_FLUSH);\n\n\t \n\tif (likely(!(next_mm & LAST_USER_MM_L1D_FLUSH)))\n\t\treturn;\n\n\t \n\tif (this_cpu_read(cpu_info.smt_active)) {\n\t\tclear_ti_thread_flag(&next->thread_info, TIF_SPEC_L1D_FLUSH);\n\t\tnext->l1d_flush_kill.func = l1d_flush_force_sigbus;\n\t\ttask_work_add(next, &next->l1d_flush_kill, TWA_RESUME);\n\t}\n}\n\nstatic unsigned long mm_mangle_tif_spec_bits(struct task_struct *next)\n{\n\tunsigned long next_tif = read_task_thread_flags(next);\n\tunsigned long spec_bits = (next_tif >> TIF_SPEC_IB) & LAST_USER_MM_SPEC_MASK;\n\n\t \n\tBUILD_BUG_ON(TIF_SPEC_L1D_FLUSH != TIF_SPEC_IB + 1);\n\n\treturn (unsigned long)next->mm | spec_bits;\n}\n\nstatic void cond_mitigation(struct task_struct *next)\n{\n\tunsigned long prev_mm, next_mm;\n\n\tif (!next || !next->mm)\n\t\treturn;\n\n\tnext_mm = mm_mangle_tif_spec_bits(next);\n\tprev_mm = this_cpu_read(cpu_tlbstate.last_user_mm_spec);\n\n\t \n\tif (static_branch_likely(&switch_mm_cond_ibpb)) {\n\t\t \n\t\tif (next_mm != prev_mm &&\n\t\t    (next_mm | prev_mm) & LAST_USER_MM_IBPB)\n\t\t\tindirect_branch_prediction_barrier();\n\t}\n\n\tif (static_branch_unlikely(&switch_mm_always_ibpb)) {\n\t\t \n\t\tif ((prev_mm & ~LAST_USER_MM_SPEC_MASK) !=\n\t\t\t\t\t(unsigned long)next->mm)\n\t\t\tindirect_branch_prediction_barrier();\n\t}\n\n\tif (static_branch_unlikely(&switch_mm_cond_l1d_flush)) {\n\t\t \n\t\tif (unlikely((prev_mm | next_mm) & LAST_USER_MM_L1D_FLUSH))\n\t\t\tl1d_flush_evaluate(prev_mm, next_mm, next);\n\t}\n\n\tthis_cpu_write(cpu_tlbstate.last_user_mm_spec, next_mm);\n}\n\n#ifdef CONFIG_PERF_EVENTS\nstatic inline void cr4_update_pce_mm(struct mm_struct *mm)\n{\n\tif (static_branch_unlikely(&rdpmc_always_available_key) ||\n\t    (!static_branch_unlikely(&rdpmc_never_available_key) &&\n\t     atomic_read(&mm->context.perf_rdpmc_allowed))) {\n\t\t \n\t\tperf_clear_dirty_counters();\n\t\tcr4_set_bits_irqsoff(X86_CR4_PCE);\n\t} else\n\t\tcr4_clear_bits_irqsoff(X86_CR4_PCE);\n}\n\nvoid cr4_update_pce(void *ignored)\n{\n\tcr4_update_pce_mm(this_cpu_read(cpu_tlbstate.loaded_mm));\n}\n\n#else\nstatic inline void cr4_update_pce_mm(struct mm_struct *mm) { }\n#endif\n\nvoid switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,\n\t\t\tstruct task_struct *tsk)\n{\n\tstruct mm_struct *real_prev = this_cpu_read(cpu_tlbstate.loaded_mm);\n\tu16 prev_asid = this_cpu_read(cpu_tlbstate.loaded_mm_asid);\n\tunsigned long new_lam = mm_lam_cr3_mask(next);\n\tbool was_lazy = this_cpu_read(cpu_tlbstate_shared.is_lazy);\n\tunsigned cpu = smp_processor_id();\n\tu64 next_tlb_gen;\n\tbool need_flush;\n\tu16 new_asid;\n\n\t \n\n\t \n\tif (IS_ENABLED(CONFIG_PROVE_LOCKING))\n\t\tWARN_ON_ONCE(!irqs_disabled());\n\n\t \n#ifdef CONFIG_DEBUG_VM\n\tif (WARN_ON_ONCE(__read_cr3() != build_cr3(real_prev->pgd, prev_asid,\n\t\t\t\t\t\t   tlbstate_lam_cr3_mask()))) {\n\t\t \n\t\t__flush_tlb_all();\n\t}\n#endif\n\tif (was_lazy)\n\t\tthis_cpu_write(cpu_tlbstate_shared.is_lazy, false);\n\n\t \n\tif (real_prev == next) {\n\t\t \n\t\tVM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[prev_asid].ctx_id) !=\n\t\t\t   next->context.ctx_id);\n\n\t\t \n\n\t\t \n\t\tif (WARN_ON_ONCE(real_prev != &init_mm &&\n\t\t\t\t !cpumask_test_cpu(cpu, mm_cpumask(next))))\n\t\t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n\n\t\t \n\t\tif (!was_lazy)\n\t\t\treturn;\n\n\t\t \n\t\tsmp_mb();\n\t\tnext_tlb_gen = atomic64_read(&next->context.tlb_gen);\n\t\tif (this_cpu_read(cpu_tlbstate.ctxs[prev_asid].tlb_gen) ==\n\t\t\t\tnext_tlb_gen)\n\t\t\treturn;\n\n\t\t \n\t\tnew_asid = prev_asid;\n\t\tneed_flush = true;\n\t} else {\n\t\t \n\t\tcond_mitigation(tsk);\n\n\t\t \n\t\tif (real_prev != &init_mm) {\n\t\t\tVM_WARN_ON_ONCE(!cpumask_test_cpu(cpu,\n\t\t\t\t\t\tmm_cpumask(real_prev)));\n\t\t\tcpumask_clear_cpu(cpu, mm_cpumask(real_prev));\n\t\t}\n\n\t\t \n\t\tif (next != &init_mm)\n\t\t\tcpumask_set_cpu(cpu, mm_cpumask(next));\n\t\tnext_tlb_gen = atomic64_read(&next->context.tlb_gen);\n\n\t\tchoose_new_asid(next, next_tlb_gen, &new_asid, &need_flush);\n\n\t\t \n\t\tthis_cpu_write(cpu_tlbstate.loaded_mm, LOADED_MM_SWITCHING);\n\t\tbarrier();\n\t}\n\n\tset_tlbstate_lam_mode(next);\n\tif (need_flush) {\n\t\tthis_cpu_write(cpu_tlbstate.ctxs[new_asid].ctx_id, next->context.ctx_id);\n\t\tthis_cpu_write(cpu_tlbstate.ctxs[new_asid].tlb_gen, next_tlb_gen);\n\t\tload_new_mm_cr3(next->pgd, new_asid, new_lam, true);\n\n\t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);\n\t} else {\n\t\t \n\t\tload_new_mm_cr3(next->pgd, new_asid, new_lam, false);\n\n\t\ttrace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, 0);\n\t}\n\n\t \n\tbarrier();\n\n\tthis_cpu_write(cpu_tlbstate.loaded_mm, next);\n\tthis_cpu_write(cpu_tlbstate.loaded_mm_asid, new_asid);\n\n\tif (next != real_prev) {\n\t\tcr4_update_pce_mm(next);\n\t\tswitch_ldt(real_prev, next);\n\t}\n}\n\n \nvoid enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)\n{\n\tif (this_cpu_read(cpu_tlbstate.loaded_mm) == &init_mm)\n\t\treturn;\n\n\tthis_cpu_write(cpu_tlbstate_shared.is_lazy, true);\n}\n\n \nvoid initialize_tlbstate_and_flush(void)\n{\n\tint i;\n\tstruct mm_struct *mm = this_cpu_read(cpu_tlbstate.loaded_mm);\n\tu64 tlb_gen = atomic64_read(&init_mm.context.tlb_gen);\n\tunsigned long cr3 = __read_cr3();\n\n\t \n\tWARN_ON((cr3 & CR3_ADDR_MASK) != __pa(mm->pgd));\n\n\t \n\tWARN_ON(cr3 & (X86_CR3_LAM_U48 | X86_CR3_LAM_U57));\n\tWARN_ON(mm_lam_cr3_mask(mm));\n\n\t \n\tWARN_ON(boot_cpu_has(X86_FEATURE_PCID) &&\n\t\t!(cr4_read_shadow() & X86_CR4_PCIDE));\n\n\t \n\twrite_cr3(build_cr3(mm->pgd, 0, 0));\n\n\t \n\tthis_cpu_write(cpu_tlbstate.last_user_mm_spec, LAST_USER_MM_INIT);\n\tthis_cpu_write(cpu_tlbstate.loaded_mm_asid, 0);\n\tthis_cpu_write(cpu_tlbstate.next_asid, 1);\n\tthis_cpu_write(cpu_tlbstate.ctxs[0].ctx_id, mm->context.ctx_id);\n\tthis_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen, tlb_gen);\n\tset_tlbstate_lam_mode(mm);\n\n\tfor (i = 1; i < TLB_NR_DYN_ASIDS; i++)\n\t\tthis_cpu_write(cpu_tlbstate.ctxs[i].ctx_id, 0);\n}\n\n \nstatic void flush_tlb_func(void *info)\n{\n\t \n\tconst struct flush_tlb_info *f = info;\n\tstruct mm_struct *loaded_mm = this_cpu_read(cpu_tlbstate.loaded_mm);\n\tu32 loaded_mm_asid = this_cpu_read(cpu_tlbstate.loaded_mm_asid);\n\tu64 local_tlb_gen = this_cpu_read(cpu_tlbstate.ctxs[loaded_mm_asid].tlb_gen);\n\tbool local = smp_processor_id() == f->initiating_cpu;\n\tunsigned long nr_invalidate = 0;\n\tu64 mm_tlb_gen;\n\n\t \n\tVM_WARN_ON(!irqs_disabled());\n\n\tif (!local) {\n\t\tinc_irq_stat(irq_tlb_count);\n\t\tcount_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);\n\n\t\t \n\t\tif (f->mm && f->mm != loaded_mm)\n\t\t\treturn;\n\t}\n\n\tif (unlikely(loaded_mm == &init_mm))\n\t\treturn;\n\n\tVM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[loaded_mm_asid].ctx_id) !=\n\t\t   loaded_mm->context.ctx_id);\n\n\tif (this_cpu_read(cpu_tlbstate_shared.is_lazy)) {\n\t\t \n\t\tswitch_mm_irqs_off(NULL, &init_mm, NULL);\n\t\treturn;\n\t}\n\n\tif (unlikely(f->new_tlb_gen != TLB_GENERATION_INVALID &&\n\t\t     f->new_tlb_gen <= local_tlb_gen)) {\n\t\t \n\t\treturn;\n\t}\n\n\t \n\tmm_tlb_gen = atomic64_read(&loaded_mm->context.tlb_gen);\n\n\tif (unlikely(local_tlb_gen == mm_tlb_gen)) {\n\t\t \n\t\tgoto done;\n\t}\n\n\tWARN_ON_ONCE(local_tlb_gen > mm_tlb_gen);\n\tWARN_ON_ONCE(f->new_tlb_gen > mm_tlb_gen);\n\n\t \n\tif (f->end != TLB_FLUSH_ALL &&\n\t    f->new_tlb_gen == local_tlb_gen + 1 &&\n\t    f->new_tlb_gen == mm_tlb_gen) {\n\t\t \n\t\tunsigned long addr = f->start;\n\n\t\t \n\t\tVM_WARN_ON(f->new_tlb_gen == TLB_GENERATION_INVALID);\n\n\t\t \n\t\tVM_WARN_ON(f->mm == NULL);\n\n\t\tnr_invalidate = (f->end - f->start) >> f->stride_shift;\n\n\t\twhile (addr < f->end) {\n\t\t\tflush_tlb_one_user(addr);\n\t\t\taddr += 1UL << f->stride_shift;\n\t\t}\n\t\tif (local)\n\t\t\tcount_vm_tlb_events(NR_TLB_LOCAL_FLUSH_ONE, nr_invalidate);\n\t} else {\n\t\t \n\t\tnr_invalidate = TLB_FLUSH_ALL;\n\n\t\tflush_tlb_local();\n\t\tif (local)\n\t\t\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);\n\t}\n\n\t \n\tthis_cpu_write(cpu_tlbstate.ctxs[loaded_mm_asid].tlb_gen, mm_tlb_gen);\n\n\t \ndone:\n\ttrace_tlb_flush(!local ? TLB_REMOTE_SHOOTDOWN :\n\t\t\t\t(f->mm == NULL) ? TLB_LOCAL_SHOOTDOWN :\n\t\t\t\t\t\t  TLB_LOCAL_MM_SHOOTDOWN,\n\t\t\tnr_invalidate);\n}\n\nstatic bool tlb_is_not_lazy(int cpu, void *data)\n{\n\treturn !per_cpu(cpu_tlbstate_shared.is_lazy, cpu);\n}\n\nDEFINE_PER_CPU_SHARED_ALIGNED(struct tlb_state_shared, cpu_tlbstate_shared);\nEXPORT_PER_CPU_SYMBOL(cpu_tlbstate_shared);\n\nSTATIC_NOPV void native_flush_tlb_multi(const struct cpumask *cpumask,\n\t\t\t\t\t const struct flush_tlb_info *info)\n{\n\t \n\tcount_vm_tlb_event(NR_TLB_REMOTE_FLUSH);\n\tif (info->end == TLB_FLUSH_ALL)\n\t\ttrace_tlb_flush(TLB_REMOTE_SEND_IPI, TLB_FLUSH_ALL);\n\telse\n\t\ttrace_tlb_flush(TLB_REMOTE_SEND_IPI,\n\t\t\t\t(info->end - info->start) >> PAGE_SHIFT);\n\n\t \n\tif (info->freed_tables)\n\t\ton_each_cpu_mask(cpumask, flush_tlb_func, (void *)info, true);\n\telse\n\t\ton_each_cpu_cond_mask(tlb_is_not_lazy, flush_tlb_func,\n\t\t\t\t(void *)info, 1, cpumask);\n}\n\nvoid flush_tlb_multi(const struct cpumask *cpumask,\n\t\t      const struct flush_tlb_info *info)\n{\n\t__flush_tlb_multi(cpumask, info);\n}\n\n \nunsigned long tlb_single_page_flush_ceiling __read_mostly = 33;\n\nstatic DEFINE_PER_CPU_SHARED_ALIGNED(struct flush_tlb_info, flush_tlb_info);\n\n#ifdef CONFIG_DEBUG_VM\nstatic DEFINE_PER_CPU(unsigned int, flush_tlb_info_idx);\n#endif\n\nstatic struct flush_tlb_info *get_flush_tlb_info(struct mm_struct *mm,\n\t\t\tunsigned long start, unsigned long end,\n\t\t\tunsigned int stride_shift, bool freed_tables,\n\t\t\tu64 new_tlb_gen)\n{\n\tstruct flush_tlb_info *info = this_cpu_ptr(&flush_tlb_info);\n\n#ifdef CONFIG_DEBUG_VM\n\t \n\tBUG_ON(this_cpu_inc_return(flush_tlb_info_idx) != 1);\n#endif\n\n\tinfo->start\t\t= start;\n\tinfo->end\t\t= end;\n\tinfo->mm\t\t= mm;\n\tinfo->stride_shift\t= stride_shift;\n\tinfo->freed_tables\t= freed_tables;\n\tinfo->new_tlb_gen\t= new_tlb_gen;\n\tinfo->initiating_cpu\t= smp_processor_id();\n\n\treturn info;\n}\n\nstatic void put_flush_tlb_info(void)\n{\n#ifdef CONFIG_DEBUG_VM\n\t \n\tbarrier();\n\tthis_cpu_dec(flush_tlb_info_idx);\n#endif\n}\n\nvoid flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,\n\t\t\t\tunsigned long end, unsigned int stride_shift,\n\t\t\t\tbool freed_tables)\n{\n\tstruct flush_tlb_info *info;\n\tu64 new_tlb_gen;\n\tint cpu;\n\n\tcpu = get_cpu();\n\n\t \n\tif ((end == TLB_FLUSH_ALL) ||\n\t    ((end - start) >> stride_shift) > tlb_single_page_flush_ceiling) {\n\t\tstart = 0;\n\t\tend = TLB_FLUSH_ALL;\n\t}\n\n\t \n\tnew_tlb_gen = inc_mm_tlb_gen(mm);\n\n\tinfo = get_flush_tlb_info(mm, start, end, stride_shift, freed_tables,\n\t\t\t\t  new_tlb_gen);\n\n\t \n\tif (cpumask_any_but(mm_cpumask(mm), cpu) < nr_cpu_ids) {\n\t\tflush_tlb_multi(mm_cpumask(mm), info);\n\t} else if (mm == this_cpu_read(cpu_tlbstate.loaded_mm)) {\n\t\tlockdep_assert_irqs_enabled();\n\t\tlocal_irq_disable();\n\t\tflush_tlb_func(info);\n\t\tlocal_irq_enable();\n\t}\n\n\tput_flush_tlb_info();\n\tput_cpu();\n\tmmu_notifier_arch_invalidate_secondary_tlbs(mm, start, end);\n}\n\n\nstatic void do_flush_tlb_all(void *info)\n{\n\tcount_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);\n\t__flush_tlb_all();\n}\n\nvoid flush_tlb_all(void)\n{\n\tcount_vm_tlb_event(NR_TLB_REMOTE_FLUSH);\n\ton_each_cpu(do_flush_tlb_all, NULL, 1);\n}\n\nstatic void do_kernel_range_flush(void *info)\n{\n\tstruct flush_tlb_info *f = info;\n\tunsigned long addr;\n\n\t \n\tfor (addr = f->start; addr < f->end; addr += PAGE_SIZE)\n\t\tflush_tlb_one_kernel(addr);\n}\n\nvoid flush_tlb_kernel_range(unsigned long start, unsigned long end)\n{\n\t \n\tif (end == TLB_FLUSH_ALL ||\n\t    (end - start) > tlb_single_page_flush_ceiling << PAGE_SHIFT) {\n\t\ton_each_cpu(do_flush_tlb_all, NULL, 1);\n\t} else {\n\t\tstruct flush_tlb_info *info;\n\n\t\tpreempt_disable();\n\t\tinfo = get_flush_tlb_info(NULL, start, end, 0, false,\n\t\t\t\t\t  TLB_GENERATION_INVALID);\n\n\t\ton_each_cpu(do_kernel_range_flush, info, 1);\n\n\t\tput_flush_tlb_info();\n\t\tpreempt_enable();\n\t}\n}\n\n \nunsigned long __get_current_cr3_fast(void)\n{\n\tunsigned long cr3 =\n\t\tbuild_cr3(this_cpu_read(cpu_tlbstate.loaded_mm)->pgd,\n\t\t\t  this_cpu_read(cpu_tlbstate.loaded_mm_asid),\n\t\t\t  tlbstate_lam_cr3_mask());\n\n\t \n\tVM_WARN_ON(in_nmi() || preemptible());\n\n\tVM_BUG_ON(cr3 != __read_cr3());\n\treturn cr3;\n}\nEXPORT_SYMBOL_GPL(__get_current_cr3_fast);\n\n \nvoid flush_tlb_one_kernel(unsigned long addr)\n{\n\tcount_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);\n\n\t \n\tflush_tlb_one_user(addr);\n\n\tif (!static_cpu_has(X86_FEATURE_PTI))\n\t\treturn;\n\n\t \n\tthis_cpu_write(cpu_tlbstate.invalidate_other, true);\n}\n\n \nSTATIC_NOPV void native_flush_tlb_one_user(unsigned long addr)\n{\n\tu32 loaded_mm_asid;\n\tbool cpu_pcide;\n\n\t \n\tasm volatile(\"invlpg (%0)\" ::\"r\" (addr) : \"memory\");\n\n\t \n\tif (!static_cpu_has(X86_FEATURE_PTI))\n\t\treturn;\n\n\tloaded_mm_asid = this_cpu_read(cpu_tlbstate.loaded_mm_asid);\n\tcpu_pcide      = this_cpu_read(cpu_tlbstate.cr4) & X86_CR4_PCIDE;\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_INVPCID) && cpu_pcide)\n\t\tinvpcid_flush_one(user_pcid(loaded_mm_asid), addr);\n\telse\n\t\tinvalidate_user_asid(loaded_mm_asid);\n}\n\nvoid flush_tlb_one_user(unsigned long addr)\n{\n\t__flush_tlb_one_user(addr);\n}\n\n \nSTATIC_NOPV void native_flush_tlb_global(void)\n{\n\tunsigned long flags;\n\n\tif (static_cpu_has(X86_FEATURE_INVPCID)) {\n\t\t \n\t\tinvpcid_flush_all();\n\t\treturn;\n\t}\n\n\t \n\traw_local_irq_save(flags);\n\n\t__native_tlb_flush_global(this_cpu_read(cpu_tlbstate.cr4));\n\n\traw_local_irq_restore(flags);\n}\n\n \nSTATIC_NOPV void native_flush_tlb_local(void)\n{\n\t \n\tWARN_ON_ONCE(preemptible());\n\n\tinvalidate_user_asid(this_cpu_read(cpu_tlbstate.loaded_mm_asid));\n\n\t \n\tnative_write_cr3(__native_read_cr3());\n}\n\nvoid flush_tlb_local(void)\n{\n\t__flush_tlb_local();\n}\n\n \nvoid __flush_tlb_all(void)\n{\n\t \n\tVM_WARN_ON_ONCE(preemptible());\n\n\tif (cpu_feature_enabled(X86_FEATURE_PGE)) {\n\t\t__flush_tlb_global();\n\t} else {\n\t\t \n\t\tflush_tlb_local();\n\t}\n}\nEXPORT_SYMBOL_GPL(__flush_tlb_all);\n\nvoid arch_tlbbatch_flush(struct arch_tlbflush_unmap_batch *batch)\n{\n\tstruct flush_tlb_info *info;\n\n\tint cpu = get_cpu();\n\n\tinfo = get_flush_tlb_info(NULL, 0, TLB_FLUSH_ALL, 0, false,\n\t\t\t\t  TLB_GENERATION_INVALID);\n\t \n\tif (cpumask_any_but(&batch->cpumask, cpu) < nr_cpu_ids) {\n\t\tflush_tlb_multi(&batch->cpumask, info);\n\t} else if (cpumask_test_cpu(cpu, &batch->cpumask)) {\n\t\tlockdep_assert_irqs_enabled();\n\t\tlocal_irq_disable();\n\t\tflush_tlb_func(info);\n\t\tlocal_irq_enable();\n\t}\n\n\tcpumask_clear(&batch->cpumask);\n\n\tput_flush_tlb_info();\n\tput_cpu();\n}\n\n \nbool nmi_uaccess_okay(void)\n{\n\tstruct mm_struct *loaded_mm = this_cpu_read(cpu_tlbstate.loaded_mm);\n\tstruct mm_struct *current_mm = current->mm;\n\n\tVM_WARN_ON_ONCE(!loaded_mm);\n\n\t \n\tif (loaded_mm != current_mm)\n\t\treturn false;\n\n\tVM_WARN_ON_ONCE(current_mm->pgd != __va(read_cr3_pa()));\n\n\treturn true;\n}\n\nstatic ssize_t tlbflush_read_file(struct file *file, char __user *user_buf,\n\t\t\t     size_t count, loff_t *ppos)\n{\n\tchar buf[32];\n\tunsigned int len;\n\n\tlen = sprintf(buf, \"%ld\\n\", tlb_single_page_flush_ceiling);\n\treturn simple_read_from_buffer(user_buf, count, ppos, buf, len);\n}\n\nstatic ssize_t tlbflush_write_file(struct file *file,\n\t\t const char __user *user_buf, size_t count, loff_t *ppos)\n{\n\tchar buf[32];\n\tssize_t len;\n\tint ceiling;\n\n\tlen = min(count, sizeof(buf) - 1);\n\tif (copy_from_user(buf, user_buf, len))\n\t\treturn -EFAULT;\n\n\tbuf[len] = '\\0';\n\tif (kstrtoint(buf, 0, &ceiling))\n\t\treturn -EINVAL;\n\n\tif (ceiling < 0)\n\t\treturn -EINVAL;\n\n\ttlb_single_page_flush_ceiling = ceiling;\n\treturn count;\n}\n\nstatic const struct file_operations fops_tlbflush = {\n\t.read = tlbflush_read_file,\n\t.write = tlbflush_write_file,\n\t.llseek = default_llseek,\n};\n\nstatic int __init create_tlb_single_page_flush_ceiling(void)\n{\n\tdebugfs_create_file(\"tlb_single_page_flush_ceiling\", S_IRUSR | S_IWUSR,\n\t\t\t    arch_debugfs_dir, NULL, &fops_tlbflush);\n\treturn 0;\n}\nlate_initcall(create_tlb_single_page_flush_ceiling);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}