{
  "module_name": "ioremap.c",
  "hash_id": "87cb7f87452cd0439f94f80f1c9118a871951b54b383750819ef828994c22f3b",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/mm/ioremap.c",
  "human_readable_source": "\n \n\n#include <linux/memblock.h>\n#include <linux/init.h>\n#include <linux/io.h>\n#include <linux/ioport.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/mmiotrace.h>\n#include <linux/cc_platform.h>\n#include <linux/efi.h>\n#include <linux/pgtable.h>\n#include <linux/kmsan.h>\n\n#include <asm/set_memory.h>\n#include <asm/e820/api.h>\n#include <asm/efi.h>\n#include <asm/fixmap.h>\n#include <asm/tlbflush.h>\n#include <asm/pgalloc.h>\n#include <asm/memtype.h>\n#include <asm/setup.h>\n\n#include \"physaddr.h\"\n\n \nstruct ioremap_desc {\n\tunsigned int flags;\n};\n\n \nint ioremap_change_attr(unsigned long vaddr, unsigned long size,\n\t\t\tenum page_cache_mode pcm)\n{\n\tunsigned long nrpages = size >> PAGE_SHIFT;\n\tint err;\n\n\tswitch (pcm) {\n\tcase _PAGE_CACHE_MODE_UC:\n\tdefault:\n\t\terr = _set_memory_uc(vaddr, nrpages);\n\t\tbreak;\n\tcase _PAGE_CACHE_MODE_WC:\n\t\terr = _set_memory_wc(vaddr, nrpages);\n\t\tbreak;\n\tcase _PAGE_CACHE_MODE_WT:\n\t\terr = _set_memory_wt(vaddr, nrpages);\n\t\tbreak;\n\tcase _PAGE_CACHE_MODE_WB:\n\t\terr = _set_memory_wb(vaddr, nrpages);\n\t\tbreak;\n\t}\n\n\treturn err;\n}\n\n \nstatic unsigned int __ioremap_check_ram(struct resource *res)\n{\n\tunsigned long start_pfn, stop_pfn;\n\tunsigned long i;\n\n\tif ((res->flags & IORESOURCE_SYSTEM_RAM) != IORESOURCE_SYSTEM_RAM)\n\t\treturn 0;\n\n\tstart_pfn = (res->start + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tstop_pfn = (res->end + 1) >> PAGE_SHIFT;\n\tif (stop_pfn > start_pfn) {\n\t\tfor (i = 0; i < (stop_pfn - start_pfn); ++i)\n\t\t\tif (pfn_valid(start_pfn + i) &&\n\t\t\t    !PageReserved(pfn_to_page(start_pfn + i)))\n\t\t\t\treturn IORES_MAP_SYSTEM_RAM;\n\t}\n\n\treturn 0;\n}\n\n \nstatic unsigned int __ioremap_check_encrypted(struct resource *res)\n{\n\tif (!cc_platform_has(CC_ATTR_GUEST_MEM_ENCRYPT))\n\t\treturn 0;\n\n\tswitch (res->desc) {\n\tcase IORES_DESC_NONE:\n\tcase IORES_DESC_RESERVED:\n\t\tbreak;\n\tdefault:\n\t\treturn IORES_MAP_ENCRYPTED;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void __ioremap_check_other(resource_size_t addr, struct ioremap_desc *desc)\n{\n\tif (!cc_platform_has(CC_ATTR_GUEST_MEM_ENCRYPT))\n\t\treturn;\n\n\tif (x86_platform.hyper.is_private_mmio(addr)) {\n\t\tdesc->flags |= IORES_MAP_ENCRYPTED;\n\t\treturn;\n\t}\n\n\tif (!IS_ENABLED(CONFIG_EFI))\n\t\treturn;\n\n\tif (efi_mem_type(addr) == EFI_RUNTIME_SERVICES_DATA ||\n\t    (efi_mem_type(addr) == EFI_BOOT_SERVICES_DATA &&\n\t     efi_mem_attributes(addr) & EFI_MEMORY_RUNTIME))\n\t\tdesc->flags |= IORES_MAP_ENCRYPTED;\n}\n\nstatic int __ioremap_collect_map_flags(struct resource *res, void *arg)\n{\n\tstruct ioremap_desc *desc = arg;\n\n\tif (!(desc->flags & IORES_MAP_SYSTEM_RAM))\n\t\tdesc->flags |= __ioremap_check_ram(res);\n\n\tif (!(desc->flags & IORES_MAP_ENCRYPTED))\n\t\tdesc->flags |= __ioremap_check_encrypted(res);\n\n\treturn ((desc->flags & (IORES_MAP_SYSTEM_RAM | IORES_MAP_ENCRYPTED)) ==\n\t\t\t       (IORES_MAP_SYSTEM_RAM | IORES_MAP_ENCRYPTED));\n}\n\n \nstatic void __ioremap_check_mem(resource_size_t addr, unsigned long size,\n\t\t\t\tstruct ioremap_desc *desc)\n{\n\tu64 start, end;\n\n\tstart = (u64)addr;\n\tend = start + size - 1;\n\tmemset(desc, 0, sizeof(struct ioremap_desc));\n\n\twalk_mem_res(start, end, desc, __ioremap_collect_map_flags);\n\n\t__ioremap_check_other(addr, desc);\n}\n\n \nstatic void __iomem *\n__ioremap_caller(resource_size_t phys_addr, unsigned long size,\n\t\t enum page_cache_mode pcm, void *caller, bool encrypted)\n{\n\tunsigned long offset, vaddr;\n\tresource_size_t last_addr;\n\tconst resource_size_t unaligned_phys_addr = phys_addr;\n\tconst unsigned long unaligned_size = size;\n\tstruct ioremap_desc io_desc;\n\tstruct vm_struct *area;\n\tenum page_cache_mode new_pcm;\n\tpgprot_t prot;\n\tint retval;\n\tvoid __iomem *ret_addr;\n\n\t \n\tlast_addr = phys_addr + size - 1;\n\tif (!size || last_addr < phys_addr)\n\t\treturn NULL;\n\n\tif (!phys_addr_valid(phys_addr)) {\n\t\tprintk(KERN_WARNING \"ioremap: invalid physical address %llx\\n\",\n\t\t       (unsigned long long)phys_addr);\n\t\tWARN_ON_ONCE(1);\n\t\treturn NULL;\n\t}\n\n\t__ioremap_check_mem(phys_addr, size, &io_desc);\n\n\t \n\tif (io_desc.flags & IORES_MAP_SYSTEM_RAM) {\n\t\tWARN_ONCE(1, \"ioremap on RAM at %pa - %pa\\n\",\n\t\t\t  &phys_addr, &last_addr);\n\t\treturn NULL;\n\t}\n\n\t \n\toffset = phys_addr & ~PAGE_MASK;\n\tphys_addr &= PAGE_MASK;\n\tsize = PAGE_ALIGN(last_addr+1) - phys_addr;\n\n\t \n\tphys_addr &= PHYSICAL_PAGE_MASK;\n\n\tretval = memtype_reserve(phys_addr, (u64)phys_addr + size,\n\t\t\t\t\t\tpcm, &new_pcm);\n\tif (retval) {\n\t\tprintk(KERN_ERR \"ioremap memtype_reserve failed %d\\n\", retval);\n\t\treturn NULL;\n\t}\n\n\tif (pcm != new_pcm) {\n\t\tif (!is_new_memtype_allowed(phys_addr, size, pcm, new_pcm)) {\n\t\t\tprintk(KERN_ERR\n\t\t\"ioremap error for 0x%llx-0x%llx, requested 0x%x, got 0x%x\\n\",\n\t\t\t\t(unsigned long long)phys_addr,\n\t\t\t\t(unsigned long long)(phys_addr + size),\n\t\t\t\tpcm, new_pcm);\n\t\t\tgoto err_free_memtype;\n\t\t}\n\t\tpcm = new_pcm;\n\t}\n\n\t \n\tprot = PAGE_KERNEL_IO;\n\tif ((io_desc.flags & IORES_MAP_ENCRYPTED) || encrypted)\n\t\tprot = pgprot_encrypted(prot);\n\telse\n\t\tprot = pgprot_decrypted(prot);\n\n\tswitch (pcm) {\n\tcase _PAGE_CACHE_MODE_UC:\n\tdefault:\n\t\tprot = __pgprot(pgprot_val(prot) |\n\t\t\t\tcachemode2protval(_PAGE_CACHE_MODE_UC));\n\t\tbreak;\n\tcase _PAGE_CACHE_MODE_UC_MINUS:\n\t\tprot = __pgprot(pgprot_val(prot) |\n\t\t\t\tcachemode2protval(_PAGE_CACHE_MODE_UC_MINUS));\n\t\tbreak;\n\tcase _PAGE_CACHE_MODE_WC:\n\t\tprot = __pgprot(pgprot_val(prot) |\n\t\t\t\tcachemode2protval(_PAGE_CACHE_MODE_WC));\n\t\tbreak;\n\tcase _PAGE_CACHE_MODE_WT:\n\t\tprot = __pgprot(pgprot_val(prot) |\n\t\t\t\tcachemode2protval(_PAGE_CACHE_MODE_WT));\n\t\tbreak;\n\tcase _PAGE_CACHE_MODE_WB:\n\t\tbreak;\n\t}\n\n\t \n\tarea = get_vm_area_caller(size, VM_IOREMAP, caller);\n\tif (!area)\n\t\tgoto err_free_memtype;\n\tarea->phys_addr = phys_addr;\n\tvaddr = (unsigned long) area->addr;\n\n\tif (memtype_kernel_map_sync(phys_addr, size, pcm))\n\t\tgoto err_free_area;\n\n\tif (ioremap_page_range(vaddr, vaddr + size, phys_addr, prot))\n\t\tgoto err_free_area;\n\n\tret_addr = (void __iomem *) (vaddr + offset);\n\tmmiotrace_ioremap(unaligned_phys_addr, unaligned_size, ret_addr);\n\n\t \n\tif (iomem_map_sanity_check(unaligned_phys_addr, unaligned_size))\n\t\tpr_warn(\"caller %pS mapping multiple BARs\\n\", caller);\n\n\treturn ret_addr;\nerr_free_area:\n\tfree_vm_area(area);\nerr_free_memtype:\n\tmemtype_free(phys_addr, phys_addr + size);\n\treturn NULL;\n}\n\n \nvoid __iomem *ioremap(resource_size_t phys_addr, unsigned long size)\n{\n\t \n\tenum page_cache_mode pcm = _PAGE_CACHE_MODE_UC_MINUS;\n\n\treturn __ioremap_caller(phys_addr, size, pcm,\n\t\t\t\t__builtin_return_address(0), false);\n}\nEXPORT_SYMBOL(ioremap);\n\n \nvoid __iomem *ioremap_uc(resource_size_t phys_addr, unsigned long size)\n{\n\tenum page_cache_mode pcm = _PAGE_CACHE_MODE_UC;\n\n\treturn __ioremap_caller(phys_addr, size, pcm,\n\t\t\t\t__builtin_return_address(0), false);\n}\nEXPORT_SYMBOL_GPL(ioremap_uc);\n\n \nvoid __iomem *ioremap_wc(resource_size_t phys_addr, unsigned long size)\n{\n\treturn __ioremap_caller(phys_addr, size, _PAGE_CACHE_MODE_WC,\n\t\t\t\t\t__builtin_return_address(0), false);\n}\nEXPORT_SYMBOL(ioremap_wc);\n\n \nvoid __iomem *ioremap_wt(resource_size_t phys_addr, unsigned long size)\n{\n\treturn __ioremap_caller(phys_addr, size, _PAGE_CACHE_MODE_WT,\n\t\t\t\t\t__builtin_return_address(0), false);\n}\nEXPORT_SYMBOL(ioremap_wt);\n\nvoid __iomem *ioremap_encrypted(resource_size_t phys_addr, unsigned long size)\n{\n\treturn __ioremap_caller(phys_addr, size, _PAGE_CACHE_MODE_WB,\n\t\t\t\t__builtin_return_address(0), true);\n}\nEXPORT_SYMBOL(ioremap_encrypted);\n\nvoid __iomem *ioremap_cache(resource_size_t phys_addr, unsigned long size)\n{\n\treturn __ioremap_caller(phys_addr, size, _PAGE_CACHE_MODE_WB,\n\t\t\t\t__builtin_return_address(0), false);\n}\nEXPORT_SYMBOL(ioremap_cache);\n\nvoid __iomem *ioremap_prot(resource_size_t phys_addr, unsigned long size,\n\t\t\t\tunsigned long prot_val)\n{\n\treturn __ioremap_caller(phys_addr, size,\n\t\t\t\tpgprot2cachemode(__pgprot(prot_val)),\n\t\t\t\t__builtin_return_address(0), false);\n}\nEXPORT_SYMBOL(ioremap_prot);\n\n \nvoid iounmap(volatile void __iomem *addr)\n{\n\tstruct vm_struct *p, *o;\n\n\tif ((void __force *)addr <= high_memory)\n\t\treturn;\n\n\t \n\tif ((void __force *)addr >= phys_to_virt(ISA_START_ADDRESS) &&\n\t    (void __force *)addr < phys_to_virt(ISA_END_ADDRESS)) {\n\t\tWARN(1, \"iounmap() called for ISA range not obtained using ioremap()\\n\");\n\t\treturn;\n\t}\n\n\tmmiotrace_iounmap(addr);\n\n\taddr = (volatile void __iomem *)\n\t\t(PAGE_MASK & (unsigned long __force)addr);\n\n\t \n\tp = find_vm_area((void __force *)addr);\n\n\tif (!p) {\n\t\tprintk(KERN_ERR \"iounmap: bad address %p\\n\", addr);\n\t\tdump_stack();\n\t\treturn;\n\t}\n\n\tkmsan_iounmap_page_range((unsigned long)addr,\n\t\t(unsigned long)addr + get_vm_area_size(p));\n\tmemtype_free(p->phys_addr, p->phys_addr + get_vm_area_size(p));\n\n\t \n\to = remove_vm_area((void __force *)addr);\n\tBUG_ON(p != o || o == NULL);\n\tkfree(p);\n}\nEXPORT_SYMBOL(iounmap);\n\n \nvoid *xlate_dev_mem_ptr(phys_addr_t phys)\n{\n\tunsigned long start  = phys &  PAGE_MASK;\n\tunsigned long offset = phys & ~PAGE_MASK;\n\tvoid *vaddr;\n\n\t \n\tvaddr = memremap(start, PAGE_SIZE, MEMREMAP_WB);\n\n\t \n\tif (vaddr)\n\t\tvaddr += offset;\n\n\treturn vaddr;\n}\n\nvoid unxlate_dev_mem_ptr(phys_addr_t phys, void *addr)\n{\n\tmemunmap((void *)((unsigned long)addr & PAGE_MASK));\n}\n\n#ifdef CONFIG_AMD_MEM_ENCRYPT\n \nstatic bool memremap_should_map_decrypted(resource_size_t phys_addr,\n\t\t\t\t\t  unsigned long size)\n{\n\tint is_pmem;\n\n\t \n\tis_pmem = region_intersects(phys_addr, size, IORESOURCE_MEM,\n\t\t\t\t    IORES_DESC_PERSISTENT_MEMORY);\n\tif (is_pmem != REGION_DISJOINT)\n\t\treturn true;\n\n\t \n\tif (efi_enabled(EFI_BOOT)) {\n\t\tswitch (efi_mem_type(phys_addr)) {\n\t\tcase EFI_RESERVED_TYPE:\n\t\t\tif (efi_mem_attributes(phys_addr) & EFI_MEMORY_NV)\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\t \n\tswitch (e820__get_entry_type(phys_addr, phys_addr + size - 1)) {\n\tcase E820_TYPE_RESERVED:\n\tcase E820_TYPE_ACPI:\n\tcase E820_TYPE_NVS:\n\tcase E820_TYPE_UNUSABLE:\n\t\t \n\t\tif (cc_platform_has(CC_ATTR_GUEST_MEM_ENCRYPT))\n\t\t\tbreak;\n\t\tfallthrough;\n\n\tcase E820_TYPE_PRAM:\n\t\treturn true;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn false;\n}\n\n \nstatic bool memremap_is_efi_data(resource_size_t phys_addr,\n\t\t\t\t unsigned long size)\n{\n\tu64 paddr;\n\n\t \n\tif (!efi_enabled(EFI_BOOT))\n\t\treturn false;\n\n\tpaddr = boot_params.efi_info.efi_memmap_hi;\n\tpaddr <<= 32;\n\tpaddr |= boot_params.efi_info.efi_memmap;\n\tif (phys_addr == paddr)\n\t\treturn true;\n\n\tpaddr = boot_params.efi_info.efi_systab_hi;\n\tpaddr <<= 32;\n\tpaddr |= boot_params.efi_info.efi_systab;\n\tif (phys_addr == paddr)\n\t\treturn true;\n\n\tif (efi_is_table_address(phys_addr))\n\t\treturn true;\n\n\tswitch (efi_mem_type(phys_addr)) {\n\tcase EFI_BOOT_SERVICES_DATA:\n\tcase EFI_RUNTIME_SERVICES_DATA:\n\t\treturn true;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn false;\n}\n\n \nstatic bool memremap_is_setup_data(resource_size_t phys_addr,\n\t\t\t\t   unsigned long size)\n{\n\tstruct setup_indirect *indirect;\n\tstruct setup_data *data;\n\tu64 paddr, paddr_next;\n\n\tpaddr = boot_params.hdr.setup_data;\n\twhile (paddr) {\n\t\tunsigned int len;\n\n\t\tif (phys_addr == paddr)\n\t\t\treturn true;\n\n\t\tdata = memremap(paddr, sizeof(*data),\n\t\t\t\tMEMREMAP_WB | MEMREMAP_DEC);\n\t\tif (!data) {\n\t\t\tpr_warn(\"failed to memremap setup_data entry\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t\tpaddr_next = data->next;\n\t\tlen = data->len;\n\n\t\tif ((phys_addr > paddr) && (phys_addr < (paddr + len))) {\n\t\t\tmemunmap(data);\n\t\t\treturn true;\n\t\t}\n\n\t\tif (data->type == SETUP_INDIRECT) {\n\t\t\tmemunmap(data);\n\t\t\tdata = memremap(paddr, sizeof(*data) + len,\n\t\t\t\t\tMEMREMAP_WB | MEMREMAP_DEC);\n\t\t\tif (!data) {\n\t\t\t\tpr_warn(\"failed to memremap indirect setup_data\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tindirect = (struct setup_indirect *)data->data;\n\n\t\t\tif (indirect->type != SETUP_INDIRECT) {\n\t\t\t\tpaddr = indirect->addr;\n\t\t\t\tlen = indirect->len;\n\t\t\t}\n\t\t}\n\n\t\tmemunmap(data);\n\n\t\tif ((phys_addr > paddr) && (phys_addr < (paddr + len)))\n\t\t\treturn true;\n\n\t\tpaddr = paddr_next;\n\t}\n\n\treturn false;\n}\n\n \nstatic bool __init early_memremap_is_setup_data(resource_size_t phys_addr,\n\t\t\t\t\t\tunsigned long size)\n{\n\tstruct setup_indirect *indirect;\n\tstruct setup_data *data;\n\tu64 paddr, paddr_next;\n\n\tpaddr = boot_params.hdr.setup_data;\n\twhile (paddr) {\n\t\tunsigned int len, size;\n\n\t\tif (phys_addr == paddr)\n\t\t\treturn true;\n\n\t\tdata = early_memremap_decrypted(paddr, sizeof(*data));\n\t\tif (!data) {\n\t\t\tpr_warn(\"failed to early memremap setup_data entry\\n\");\n\t\t\treturn false;\n\t\t}\n\n\t\tsize = sizeof(*data);\n\n\t\tpaddr_next = data->next;\n\t\tlen = data->len;\n\n\t\tif ((phys_addr > paddr) && (phys_addr < (paddr + len))) {\n\t\t\tearly_memunmap(data, sizeof(*data));\n\t\t\treturn true;\n\t\t}\n\n\t\tif (data->type == SETUP_INDIRECT) {\n\t\t\tsize += len;\n\t\t\tearly_memunmap(data, sizeof(*data));\n\t\t\tdata = early_memremap_decrypted(paddr, size);\n\t\t\tif (!data) {\n\t\t\t\tpr_warn(\"failed to early memremap indirect setup_data\\n\");\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tindirect = (struct setup_indirect *)data->data;\n\n\t\t\tif (indirect->type != SETUP_INDIRECT) {\n\t\t\t\tpaddr = indirect->addr;\n\t\t\t\tlen = indirect->len;\n\t\t\t}\n\t\t}\n\n\t\tearly_memunmap(data, size);\n\n\t\tif ((phys_addr > paddr) && (phys_addr < (paddr + len)))\n\t\t\treturn true;\n\n\t\tpaddr = paddr_next;\n\t}\n\n\treturn false;\n}\n\n \nbool arch_memremap_can_ram_remap(resource_size_t phys_addr, unsigned long size,\n\t\t\t\t unsigned long flags)\n{\n\tif (!cc_platform_has(CC_ATTR_MEM_ENCRYPT))\n\t\treturn true;\n\n\tif (flags & MEMREMAP_ENC)\n\t\treturn true;\n\n\tif (flags & MEMREMAP_DEC)\n\t\treturn false;\n\n\tif (cc_platform_has(CC_ATTR_HOST_MEM_ENCRYPT)) {\n\t\tif (memremap_is_setup_data(phys_addr, size) ||\n\t\t    memremap_is_efi_data(phys_addr, size))\n\t\t\treturn false;\n\t}\n\n\treturn !memremap_should_map_decrypted(phys_addr, size);\n}\n\n \npgprot_t __init early_memremap_pgprot_adjust(resource_size_t phys_addr,\n\t\t\t\t\t     unsigned long size,\n\t\t\t\t\t     pgprot_t prot)\n{\n\tbool encrypted_prot;\n\n\tif (!cc_platform_has(CC_ATTR_MEM_ENCRYPT))\n\t\treturn prot;\n\n\tencrypted_prot = true;\n\n\tif (cc_platform_has(CC_ATTR_HOST_MEM_ENCRYPT)) {\n\t\tif (early_memremap_is_setup_data(phys_addr, size) ||\n\t\t    memremap_is_efi_data(phys_addr, size))\n\t\t\tencrypted_prot = false;\n\t}\n\n\tif (encrypted_prot && memremap_should_map_decrypted(phys_addr, size))\n\t\tencrypted_prot = false;\n\n\treturn encrypted_prot ? pgprot_encrypted(prot)\n\t\t\t      : pgprot_decrypted(prot);\n}\n\nbool phys_mem_access_encrypted(unsigned long phys_addr, unsigned long size)\n{\n\treturn arch_memremap_can_ram_remap(phys_addr, size, 0);\n}\n\n \nvoid __init *early_memremap_encrypted(resource_size_t phys_addr,\n\t\t\t\t      unsigned long size)\n{\n\treturn early_memremap_prot(phys_addr, size, __PAGE_KERNEL_ENC);\n}\n\n \nvoid __init *early_memremap_encrypted_wp(resource_size_t phys_addr,\n\t\t\t\t\t unsigned long size)\n{\n\tif (!x86_has_pat_wp())\n\t\treturn NULL;\n\treturn early_memremap_prot(phys_addr, size, __PAGE_KERNEL_ENC_WP);\n}\n\n \nvoid __init *early_memremap_decrypted(resource_size_t phys_addr,\n\t\t\t\t      unsigned long size)\n{\n\treturn early_memremap_prot(phys_addr, size, __PAGE_KERNEL_NOENC);\n}\n\n \nvoid __init *early_memremap_decrypted_wp(resource_size_t phys_addr,\n\t\t\t\t\t unsigned long size)\n{\n\tif (!x86_has_pat_wp())\n\t\treturn NULL;\n\treturn early_memremap_prot(phys_addr, size, __PAGE_KERNEL_NOENC_WP);\n}\n#endif\t \n\nstatic pte_t bm_pte[PAGE_SIZE/sizeof(pte_t)] __page_aligned_bss;\n\nstatic inline pmd_t * __init early_ioremap_pmd(unsigned long addr)\n{\n\t \n\tpgd_t *base = __va(read_cr3_pa());\n\tpgd_t *pgd = &base[pgd_index(addr)];\n\tp4d_t *p4d = p4d_offset(pgd, addr);\n\tpud_t *pud = pud_offset(p4d, addr);\n\tpmd_t *pmd = pmd_offset(pud, addr);\n\n\treturn pmd;\n}\n\nstatic inline pte_t * __init early_ioremap_pte(unsigned long addr)\n{\n\treturn &bm_pte[pte_index(addr)];\n}\n\nbool __init is_early_ioremap_ptep(pte_t *ptep)\n{\n\treturn ptep >= &bm_pte[0] && ptep < &bm_pte[PAGE_SIZE/sizeof(pte_t)];\n}\n\nvoid __init early_ioremap_init(void)\n{\n\tpmd_t *pmd;\n\n#ifdef CONFIG_X86_64\n\tBUILD_BUG_ON((fix_to_virt(0) + PAGE_SIZE) & ((1 << PMD_SHIFT) - 1));\n#else\n\tWARN_ON((fix_to_virt(0) + PAGE_SIZE) & ((1 << PMD_SHIFT) - 1));\n#endif\n\n\tearly_ioremap_setup();\n\n\tpmd = early_ioremap_pmd(fix_to_virt(FIX_BTMAP_BEGIN));\n\tmemset(bm_pte, 0, sizeof(bm_pte));\n\tpmd_populate_kernel(&init_mm, pmd, bm_pte);\n\n\t \n#define __FIXADDR_TOP (-PAGE_SIZE)\n\tBUILD_BUG_ON((__fix_to_virt(FIX_BTMAP_BEGIN) >> PMD_SHIFT)\n\t\t     != (__fix_to_virt(FIX_BTMAP_END) >> PMD_SHIFT));\n#undef __FIXADDR_TOP\n\tif (pmd != early_ioremap_pmd(fix_to_virt(FIX_BTMAP_END))) {\n\t\tWARN_ON(1);\n\t\tprintk(KERN_WARNING \"pmd %p != %p\\n\",\n\t\t       pmd, early_ioremap_pmd(fix_to_virt(FIX_BTMAP_END)));\n\t\tprintk(KERN_WARNING \"fix_to_virt(FIX_BTMAP_BEGIN): %08lx\\n\",\n\t\t\tfix_to_virt(FIX_BTMAP_BEGIN));\n\t\tprintk(KERN_WARNING \"fix_to_virt(FIX_BTMAP_END):   %08lx\\n\",\n\t\t\tfix_to_virt(FIX_BTMAP_END));\n\n\t\tprintk(KERN_WARNING \"FIX_BTMAP_END:       %d\\n\", FIX_BTMAP_END);\n\t\tprintk(KERN_WARNING \"FIX_BTMAP_BEGIN:     %d\\n\",\n\t\t       FIX_BTMAP_BEGIN);\n\t}\n}\n\nvoid __init __early_set_fixmap(enum fixed_addresses idx,\n\t\t\t       phys_addr_t phys, pgprot_t flags)\n{\n\tunsigned long addr = __fix_to_virt(idx);\n\tpte_t *pte;\n\n\tif (idx >= __end_of_fixed_addresses) {\n\t\tBUG();\n\t\treturn;\n\t}\n\tpte = early_ioremap_pte(addr);\n\n\t \n\tpgprot_val(flags) &= __supported_pte_mask;\n\n\tif (pgprot_val(flags))\n\t\tset_pte(pte, pfn_pte(phys >> PAGE_SHIFT, flags));\n\telse\n\t\tpte_clear(&init_mm, addr, pte);\n\tflush_tlb_one_kernel(addr);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}