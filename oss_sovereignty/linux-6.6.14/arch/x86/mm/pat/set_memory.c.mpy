{
  "module_name": "set_memory.c",
  "hash_id": "dab9bc6776fd8e19c812aded393fee097031dbc983eaa8c3ce4370e95d4dc685",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/mm/pat/set_memory.c",
  "human_readable_source": "\n \n#include <linux/highmem.h>\n#include <linux/memblock.h>\n#include <linux/sched.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/seq_file.h>\n#include <linux/proc_fs.h>\n#include <linux/debugfs.h>\n#include <linux/pfn.h>\n#include <linux/percpu.h>\n#include <linux/gfp.h>\n#include <linux/pci.h>\n#include <linux/vmalloc.h>\n#include <linux/libnvdimm.h>\n#include <linux/vmstat.h>\n#include <linux/kernel.h>\n#include <linux/cc_platform.h>\n#include <linux/set_memory.h>\n#include <linux/memregion.h>\n\n#include <asm/e820/api.h>\n#include <asm/processor.h>\n#include <asm/tlbflush.h>\n#include <asm/sections.h>\n#include <asm/setup.h>\n#include <linux/uaccess.h>\n#include <asm/pgalloc.h>\n#include <asm/proto.h>\n#include <asm/memtype.h>\n#include <asm/hyperv-tlfs.h>\n#include <asm/mshyperv.h>\n\n#include \"../mm_internal.h\"\n\n \nstruct cpa_data {\n\tunsigned long\t*vaddr;\n\tpgd_t\t\t*pgd;\n\tpgprot_t\tmask_set;\n\tpgprot_t\tmask_clr;\n\tunsigned long\tnumpages;\n\tunsigned long\tcurpage;\n\tunsigned long\tpfn;\n\tunsigned int\tflags;\n\tunsigned int\tforce_split\t\t: 1,\n\t\t\tforce_static_prot\t: 1,\n\t\t\tforce_flush_all\t\t: 1;\n\tstruct page\t**pages;\n};\n\nenum cpa_warn {\n\tCPA_CONFLICT,\n\tCPA_PROTECT,\n\tCPA_DETECT,\n};\n\nstatic const int cpa_warn_level = CPA_PROTECT;\n\n \nstatic DEFINE_SPINLOCK(cpa_lock);\n\n#define CPA_FLUSHTLB 1\n#define CPA_ARRAY 2\n#define CPA_PAGES_ARRAY 4\n#define CPA_NO_CHECK_ALIAS 8  \n\nstatic inline pgprot_t cachemode2pgprot(enum page_cache_mode pcm)\n{\n\treturn __pgprot(cachemode2protval(pcm));\n}\n\n#ifdef CONFIG_PROC_FS\nstatic unsigned long direct_pages_count[PG_LEVEL_NUM];\n\nvoid update_page_count(int level, unsigned long pages)\n{\n\t \n\tspin_lock(&pgd_lock);\n\tdirect_pages_count[level] += pages;\n\tspin_unlock(&pgd_lock);\n}\n\nstatic void split_page_count(int level)\n{\n\tif (direct_pages_count[level] == 0)\n\t\treturn;\n\n\tdirect_pages_count[level]--;\n\tif (system_state == SYSTEM_RUNNING) {\n\t\tif (level == PG_LEVEL_2M)\n\t\t\tcount_vm_event(DIRECT_MAP_LEVEL2_SPLIT);\n\t\telse if (level == PG_LEVEL_1G)\n\t\t\tcount_vm_event(DIRECT_MAP_LEVEL3_SPLIT);\n\t}\n\tdirect_pages_count[level - 1] += PTRS_PER_PTE;\n}\n\nvoid arch_report_meminfo(struct seq_file *m)\n{\n\tseq_printf(m, \"DirectMap4k:    %8lu kB\\n\",\n\t\t\tdirect_pages_count[PG_LEVEL_4K] << 2);\n#if defined(CONFIG_X86_64) || defined(CONFIG_X86_PAE)\n\tseq_printf(m, \"DirectMap2M:    %8lu kB\\n\",\n\t\t\tdirect_pages_count[PG_LEVEL_2M] << 11);\n#else\n\tseq_printf(m, \"DirectMap4M:    %8lu kB\\n\",\n\t\t\tdirect_pages_count[PG_LEVEL_2M] << 12);\n#endif\n\tif (direct_gbpages)\n\t\tseq_printf(m, \"DirectMap1G:    %8lu kB\\n\",\n\t\t\tdirect_pages_count[PG_LEVEL_1G] << 20);\n}\n#else\nstatic inline void split_page_count(int level) { }\n#endif\n\n#ifdef CONFIG_X86_CPA_STATISTICS\n\nstatic unsigned long cpa_1g_checked;\nstatic unsigned long cpa_1g_sameprot;\nstatic unsigned long cpa_1g_preserved;\nstatic unsigned long cpa_2m_checked;\nstatic unsigned long cpa_2m_sameprot;\nstatic unsigned long cpa_2m_preserved;\nstatic unsigned long cpa_4k_install;\n\nstatic inline void cpa_inc_1g_checked(void)\n{\n\tcpa_1g_checked++;\n}\n\nstatic inline void cpa_inc_2m_checked(void)\n{\n\tcpa_2m_checked++;\n}\n\nstatic inline void cpa_inc_4k_install(void)\n{\n\tdata_race(cpa_4k_install++);\n}\n\nstatic inline void cpa_inc_lp_sameprot(int level)\n{\n\tif (level == PG_LEVEL_1G)\n\t\tcpa_1g_sameprot++;\n\telse\n\t\tcpa_2m_sameprot++;\n}\n\nstatic inline void cpa_inc_lp_preserved(int level)\n{\n\tif (level == PG_LEVEL_1G)\n\t\tcpa_1g_preserved++;\n\telse\n\t\tcpa_2m_preserved++;\n}\n\nstatic int cpastats_show(struct seq_file *m, void *p)\n{\n\tseq_printf(m, \"1G pages checked:     %16lu\\n\", cpa_1g_checked);\n\tseq_printf(m, \"1G pages sameprot:    %16lu\\n\", cpa_1g_sameprot);\n\tseq_printf(m, \"1G pages preserved:   %16lu\\n\", cpa_1g_preserved);\n\tseq_printf(m, \"2M pages checked:     %16lu\\n\", cpa_2m_checked);\n\tseq_printf(m, \"2M pages sameprot:    %16lu\\n\", cpa_2m_sameprot);\n\tseq_printf(m, \"2M pages preserved:   %16lu\\n\", cpa_2m_preserved);\n\tseq_printf(m, \"4K pages set-checked: %16lu\\n\", cpa_4k_install);\n\treturn 0;\n}\n\nstatic int cpastats_open(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, cpastats_show, NULL);\n}\n\nstatic const struct file_operations cpastats_fops = {\n\t.open\t\t= cpastats_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= single_release,\n};\n\nstatic int __init cpa_stats_init(void)\n{\n\tdebugfs_create_file(\"cpa_stats\", S_IRUSR, arch_debugfs_dir, NULL,\n\t\t\t    &cpastats_fops);\n\treturn 0;\n}\nlate_initcall(cpa_stats_init);\n#else\nstatic inline void cpa_inc_1g_checked(void) { }\nstatic inline void cpa_inc_2m_checked(void) { }\nstatic inline void cpa_inc_4k_install(void) { }\nstatic inline void cpa_inc_lp_sameprot(int level) { }\nstatic inline void cpa_inc_lp_preserved(int level) { }\n#endif\n\n\nstatic inline int\nwithin(unsigned long addr, unsigned long start, unsigned long end)\n{\n\treturn addr >= start && addr < end;\n}\n\nstatic inline int\nwithin_inclusive(unsigned long addr, unsigned long start, unsigned long end)\n{\n\treturn addr >= start && addr <= end;\n}\n\n#ifdef CONFIG_X86_64\n\n \n\nstatic inline unsigned long highmap_start_pfn(void)\n{\n\treturn __pa_symbol(_text) >> PAGE_SHIFT;\n}\n\nstatic inline unsigned long highmap_end_pfn(void)\n{\n\t \n\treturn __pa_symbol(roundup(_brk_end, PMD_SIZE) - 1) >> PAGE_SHIFT;\n}\n\nstatic bool __cpa_pfn_in_highmap(unsigned long pfn)\n{\n\t \n\treturn within_inclusive(pfn, highmap_start_pfn(), highmap_end_pfn());\n}\n\n#else\n\nstatic bool __cpa_pfn_in_highmap(unsigned long pfn)\n{\n\t \n\treturn false;\n}\n\n#endif\n\n \nstatic inline unsigned long fix_addr(unsigned long addr)\n{\n#ifdef CONFIG_X86_64\n\treturn (long)(addr << 1) >> 1;\n#else\n\treturn addr;\n#endif\n}\n\nstatic unsigned long __cpa_addr(struct cpa_data *cpa, unsigned long idx)\n{\n\tif (cpa->flags & CPA_PAGES_ARRAY) {\n\t\tstruct page *page = cpa->pages[idx];\n\n\t\tif (unlikely(PageHighMem(page)))\n\t\t\treturn 0;\n\n\t\treturn (unsigned long)page_address(page);\n\t}\n\n\tif (cpa->flags & CPA_ARRAY)\n\t\treturn cpa->vaddr[idx];\n\n\treturn *cpa->vaddr + idx * PAGE_SIZE;\n}\n\n \n\nstatic void clflush_cache_range_opt(void *vaddr, unsigned int size)\n{\n\tconst unsigned long clflush_size = boot_cpu_data.x86_clflush_size;\n\tvoid *p = (void *)((unsigned long)vaddr & ~(clflush_size - 1));\n\tvoid *vend = vaddr + size;\n\n\tif (p >= vend)\n\t\treturn;\n\n\tfor (; p < vend; p += clflush_size)\n\t\tclflushopt(p);\n}\n\n \nvoid clflush_cache_range(void *vaddr, unsigned int size)\n{\n\tmb();\n\tclflush_cache_range_opt(vaddr, size);\n\tmb();\n}\nEXPORT_SYMBOL_GPL(clflush_cache_range);\n\n#ifdef CONFIG_ARCH_HAS_PMEM_API\nvoid arch_invalidate_pmem(void *addr, size_t size)\n{\n\tclflush_cache_range(addr, size);\n}\nEXPORT_SYMBOL_GPL(arch_invalidate_pmem);\n#endif\n\n#ifdef CONFIG_ARCH_HAS_CPU_CACHE_INVALIDATE_MEMREGION\nbool cpu_cache_has_invalidate_memregion(void)\n{\n\treturn !cpu_feature_enabled(X86_FEATURE_HYPERVISOR);\n}\nEXPORT_SYMBOL_NS_GPL(cpu_cache_has_invalidate_memregion, DEVMEM);\n\nint cpu_cache_invalidate_memregion(int res_desc)\n{\n\tif (WARN_ON_ONCE(!cpu_cache_has_invalidate_memregion()))\n\t\treturn -ENXIO;\n\twbinvd_on_all_cpus();\n\treturn 0;\n}\nEXPORT_SYMBOL_NS_GPL(cpu_cache_invalidate_memregion, DEVMEM);\n#endif\n\nstatic void __cpa_flush_all(void *arg)\n{\n\tunsigned long cache = (unsigned long)arg;\n\n\t \n\t__flush_tlb_all();\n\n\tif (cache && boot_cpu_data.x86 >= 4)\n\t\twbinvd();\n}\n\nstatic void cpa_flush_all(unsigned long cache)\n{\n\tBUG_ON(irqs_disabled() && !early_boot_irqs_disabled);\n\n\ton_each_cpu(__cpa_flush_all, (void *) cache, 1);\n}\n\nstatic void __cpa_flush_tlb(void *data)\n{\n\tstruct cpa_data *cpa = data;\n\tunsigned int i;\n\n\tfor (i = 0; i < cpa->numpages; i++)\n\t\tflush_tlb_one_kernel(fix_addr(__cpa_addr(cpa, i)));\n}\n\nstatic void cpa_flush(struct cpa_data *data, int cache)\n{\n\tstruct cpa_data *cpa = data;\n\tunsigned int i;\n\n\tBUG_ON(irqs_disabled() && !early_boot_irqs_disabled);\n\n\tif (cache && !static_cpu_has(X86_FEATURE_CLFLUSH)) {\n\t\tcpa_flush_all(cache);\n\t\treturn;\n\t}\n\n\tif (cpa->force_flush_all || cpa->numpages > tlb_single_page_flush_ceiling)\n\t\tflush_tlb_all();\n\telse\n\t\ton_each_cpu(__cpa_flush_tlb, cpa, 1);\n\n\tif (!cache)\n\t\treturn;\n\n\tmb();\n\tfor (i = 0; i < cpa->numpages; i++) {\n\t\tunsigned long addr = __cpa_addr(cpa, i);\n\t\tunsigned int level;\n\n\t\tpte_t *pte = lookup_address(addr, &level);\n\n\t\t \n\t\tif (pte && (pte_val(*pte) & _PAGE_PRESENT))\n\t\t\tclflush_cache_range_opt((void *)fix_addr(addr), PAGE_SIZE);\n\t}\n\tmb();\n}\n\nstatic bool overlaps(unsigned long r1_start, unsigned long r1_end,\n\t\t     unsigned long r2_start, unsigned long r2_end)\n{\n\treturn (r1_start <= r2_end && r1_end >= r2_start) ||\n\t\t(r2_start <= r1_end && r2_end >= r1_start);\n}\n\n#ifdef CONFIG_PCI_BIOS\n \n#define BIOS_PFN\tPFN_DOWN(BIOS_BEGIN)\n#define BIOS_PFN_END\tPFN_DOWN(BIOS_END - 1)\n\nstatic pgprotval_t protect_pci_bios(unsigned long spfn, unsigned long epfn)\n{\n\tif (pcibios_enabled && overlaps(spfn, epfn, BIOS_PFN, BIOS_PFN_END))\n\t\treturn _PAGE_NX;\n\treturn 0;\n}\n#else\nstatic pgprotval_t protect_pci_bios(unsigned long spfn, unsigned long epfn)\n{\n\treturn 0;\n}\n#endif\n\n \nstatic pgprotval_t protect_rodata(unsigned long spfn, unsigned long epfn)\n{\n\tunsigned long epfn_ro, spfn_ro = PFN_DOWN(__pa_symbol(__start_rodata));\n\n\t \n\tepfn_ro = PFN_DOWN(__pa_symbol(__end_rodata)) - 1;\n\n\tif (kernel_set_to_readonly && overlaps(spfn, epfn, spfn_ro, epfn_ro))\n\t\treturn _PAGE_RW;\n\treturn 0;\n}\n\n \nstatic pgprotval_t protect_kernel_text(unsigned long start, unsigned long end)\n{\n\tunsigned long t_end = (unsigned long)_etext - 1;\n\tunsigned long t_start = (unsigned long)_text;\n\n\tif (overlaps(start, end, t_start, t_end))\n\t\treturn _PAGE_NX;\n\treturn 0;\n}\n\n#if defined(CONFIG_X86_64)\n \nstatic pgprotval_t protect_kernel_text_ro(unsigned long start,\n\t\t\t\t\t  unsigned long end)\n{\n\tunsigned long t_end = (unsigned long)__end_rodata_hpage_align - 1;\n\tunsigned long t_start = (unsigned long)_text;\n\tunsigned int level;\n\n\tif (!kernel_set_to_readonly || !overlaps(start, end, t_start, t_end))\n\t\treturn 0;\n\t \n\tif (lookup_address(start, &level) && (level != PG_LEVEL_4K))\n\t\treturn _PAGE_RW;\n\treturn 0;\n}\n#else\nstatic pgprotval_t protect_kernel_text_ro(unsigned long start,\n\t\t\t\t\t  unsigned long end)\n{\n\treturn 0;\n}\n#endif\n\nstatic inline bool conflicts(pgprot_t prot, pgprotval_t val)\n{\n\treturn (pgprot_val(prot) & ~val) != pgprot_val(prot);\n}\n\nstatic inline void check_conflict(int warnlvl, pgprot_t prot, pgprotval_t val,\n\t\t\t\t  unsigned long start, unsigned long end,\n\t\t\t\t  unsigned long pfn, const char *txt)\n{\n\tstatic const char *lvltxt[] = {\n\t\t[CPA_CONFLICT]\t= \"conflict\",\n\t\t[CPA_PROTECT]\t= \"protect\",\n\t\t[CPA_DETECT]\t= \"detect\",\n\t};\n\n\tif (warnlvl > cpa_warn_level || !conflicts(prot, val))\n\t\treturn;\n\n\tpr_warn(\"CPA %8s %10s: 0x%016lx - 0x%016lx PFN %lx req %016llx prevent %016llx\\n\",\n\t\tlvltxt[warnlvl], txt, start, end, pfn, (unsigned long long)pgprot_val(prot),\n\t\t(unsigned long long)val);\n}\n\n \nstatic inline pgprot_t static_protections(pgprot_t prot, unsigned long start,\n\t\t\t\t\t  unsigned long pfn, unsigned long npg,\n\t\t\t\t\t  unsigned long lpsize, int warnlvl)\n{\n\tpgprotval_t forbidden, res;\n\tunsigned long end;\n\n\t \n\tif (!(pgprot_val(prot) & _PAGE_PRESENT))\n\t\treturn prot;\n\n\t \n\tend = start + npg * PAGE_SIZE - 1;\n\n\tres = protect_kernel_text(start, end);\n\tcheck_conflict(warnlvl, prot, res, start, end, pfn, \"Text NX\");\n\tforbidden = res;\n\n\t \n\tif (lpsize != (npg * PAGE_SIZE) || (start & (lpsize - 1))) {\n\t\tres = protect_kernel_text_ro(start, end);\n\t\tcheck_conflict(warnlvl, prot, res, start, end, pfn, \"Text RO\");\n\t\tforbidden |= res;\n\t}\n\n\t \n\tres = protect_pci_bios(pfn, pfn + npg - 1);\n\tcheck_conflict(warnlvl, prot, res, start, end, pfn, \"PCIBIOS NX\");\n\tforbidden |= res;\n\n\tres = protect_rodata(pfn, pfn + npg - 1);\n\tcheck_conflict(warnlvl, prot, res, start, end, pfn, \"Rodata RO\");\n\tforbidden |= res;\n\n\treturn __pgprot(pgprot_val(prot) & ~forbidden);\n}\n\n \nstatic inline pgprot_t verify_rwx(pgprot_t old, pgprot_t new, unsigned long start,\n\t\t\t\t  unsigned long pfn, unsigned long npg)\n{\n\tunsigned long end;\n\n\t \n\tif (IS_ENABLED(CONFIG_X86_32))\n\t\treturn new;\n\n\t \n\tif (!(__supported_pte_mask & _PAGE_NX))\n\t\treturn new;\n\n\tif (!((pgprot_val(old) ^ pgprot_val(new)) & (_PAGE_RW | _PAGE_NX)))\n\t\treturn new;\n\n\tif ((pgprot_val(new) & (_PAGE_RW | _PAGE_NX)) != _PAGE_RW)\n\t\treturn new;\n\n\tend = start + npg * PAGE_SIZE - 1;\n\tWARN_ONCE(1, \"CPA detected W^X violation: %016llx -> %016llx range: 0x%016lx - 0x%016lx PFN %lx\\n\",\n\t\t  (unsigned long long)pgprot_val(old),\n\t\t  (unsigned long long)pgprot_val(new),\n\t\t  start, end, pfn);\n\n\t \n\treturn new;\n}\n\n \npte_t *lookup_address_in_pgd(pgd_t *pgd, unsigned long address,\n\t\t\t     unsigned int *level)\n{\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd;\n\n\t*level = PG_LEVEL_NONE;\n\n\tif (pgd_none(*pgd))\n\t\treturn NULL;\n\n\tp4d = p4d_offset(pgd, address);\n\tif (p4d_none(*p4d))\n\t\treturn NULL;\n\n\t*level = PG_LEVEL_512G;\n\tif (p4d_large(*p4d) || !p4d_present(*p4d))\n\t\treturn (pte_t *)p4d;\n\n\tpud = pud_offset(p4d, address);\n\tif (pud_none(*pud))\n\t\treturn NULL;\n\n\t*level = PG_LEVEL_1G;\n\tif (pud_large(*pud) || !pud_present(*pud))\n\t\treturn (pte_t *)pud;\n\n\tpmd = pmd_offset(pud, address);\n\tif (pmd_none(*pmd))\n\t\treturn NULL;\n\n\t*level = PG_LEVEL_2M;\n\tif (pmd_large(*pmd) || !pmd_present(*pmd))\n\t\treturn (pte_t *)pmd;\n\n\t*level = PG_LEVEL_4K;\n\n\treturn pte_offset_kernel(pmd, address);\n}\n\n \npte_t *lookup_address(unsigned long address, unsigned int *level)\n{\n\treturn lookup_address_in_pgd(pgd_offset_k(address), address, level);\n}\nEXPORT_SYMBOL_GPL(lookup_address);\n\nstatic pte_t *_lookup_address_cpa(struct cpa_data *cpa, unsigned long address,\n\t\t\t\t  unsigned int *level)\n{\n\tif (cpa->pgd)\n\t\treturn lookup_address_in_pgd(cpa->pgd + pgd_index(address),\n\t\t\t\t\t       address, level);\n\n\treturn lookup_address(address, level);\n}\n\n \npmd_t *lookup_pmd_address(unsigned long address)\n{\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\n\tpgd = pgd_offset_k(address);\n\tif (pgd_none(*pgd))\n\t\treturn NULL;\n\n\tp4d = p4d_offset(pgd, address);\n\tif (p4d_none(*p4d) || p4d_large(*p4d) || !p4d_present(*p4d))\n\t\treturn NULL;\n\n\tpud = pud_offset(p4d, address);\n\tif (pud_none(*pud) || pud_large(*pud) || !pud_present(*pud))\n\t\treturn NULL;\n\n\treturn pmd_offset(pud, address);\n}\n\n \nphys_addr_t slow_virt_to_phys(void *__virt_addr)\n{\n\tunsigned long virt_addr = (unsigned long)__virt_addr;\n\tphys_addr_t phys_addr;\n\tunsigned long offset;\n\tenum pg_level level;\n\tpte_t *pte;\n\n\tpte = lookup_address(virt_addr, &level);\n\tBUG_ON(!pte);\n\n\t \n\tswitch (level) {\n\tcase PG_LEVEL_1G:\n\t\tphys_addr = (phys_addr_t)pud_pfn(*(pud_t *)pte) << PAGE_SHIFT;\n\t\toffset = virt_addr & ~PUD_MASK;\n\t\tbreak;\n\tcase PG_LEVEL_2M:\n\t\tphys_addr = (phys_addr_t)pmd_pfn(*(pmd_t *)pte) << PAGE_SHIFT;\n\t\toffset = virt_addr & ~PMD_MASK;\n\t\tbreak;\n\tdefault:\n\t\tphys_addr = (phys_addr_t)pte_pfn(*pte) << PAGE_SHIFT;\n\t\toffset = virt_addr & ~PAGE_MASK;\n\t}\n\n\treturn (phys_addr_t)(phys_addr | offset);\n}\nEXPORT_SYMBOL_GPL(slow_virt_to_phys);\n\n \nstatic void __set_pmd_pte(pte_t *kpte, unsigned long address, pte_t pte)\n{\n\t \n\tset_pte_atomic(kpte, pte);\n#ifdef CONFIG_X86_32\n\tif (!SHARED_KERNEL_PMD) {\n\t\tstruct page *page;\n\n\t\tlist_for_each_entry(page, &pgd_list, lru) {\n\t\t\tpgd_t *pgd;\n\t\t\tp4d_t *p4d;\n\t\t\tpud_t *pud;\n\t\t\tpmd_t *pmd;\n\n\t\t\tpgd = (pgd_t *)page_address(page) + pgd_index(address);\n\t\t\tp4d = p4d_offset(pgd, address);\n\t\t\tpud = pud_offset(p4d, address);\n\t\t\tpmd = pmd_offset(pud, address);\n\t\t\tset_pte_atomic((pte_t *)pmd, pte);\n\t\t}\n\t}\n#endif\n}\n\nstatic pgprot_t pgprot_clear_protnone_bits(pgprot_t prot)\n{\n\t \n\tif (!(pgprot_val(prot) & _PAGE_PRESENT))\n\t\tpgprot_val(prot) &= ~_PAGE_GLOBAL;\n\n\treturn prot;\n}\n\nstatic int __should_split_large_page(pte_t *kpte, unsigned long address,\n\t\t\t\t     struct cpa_data *cpa)\n{\n\tunsigned long numpages, pmask, psize, lpaddr, pfn, old_pfn;\n\tpgprot_t old_prot, new_prot, req_prot, chk_prot;\n\tpte_t new_pte, *tmp;\n\tenum pg_level level;\n\n\t \n\ttmp = _lookup_address_cpa(cpa, address, &level);\n\tif (tmp != kpte)\n\t\treturn 1;\n\n\tswitch (level) {\n\tcase PG_LEVEL_2M:\n\t\told_prot = pmd_pgprot(*(pmd_t *)kpte);\n\t\told_pfn = pmd_pfn(*(pmd_t *)kpte);\n\t\tcpa_inc_2m_checked();\n\t\tbreak;\n\tcase PG_LEVEL_1G:\n\t\told_prot = pud_pgprot(*(pud_t *)kpte);\n\t\told_pfn = pud_pfn(*(pud_t *)kpte);\n\t\tcpa_inc_1g_checked();\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tpsize = page_level_size(level);\n\tpmask = page_level_mask(level);\n\n\t \n\tlpaddr = (address + psize) & pmask;\n\tnumpages = (lpaddr - address) >> PAGE_SHIFT;\n\tif (numpages < cpa->numpages)\n\t\tcpa->numpages = numpages;\n\n\t \n\n\t \n\treq_prot = pgprot_large_2_4k(old_prot);\n\n\tpgprot_val(req_prot) &= ~pgprot_val(cpa->mask_clr);\n\tpgprot_val(req_prot) |= pgprot_val(cpa->mask_set);\n\n\t \n\treq_prot = pgprot_4k_2_large(req_prot);\n\treq_prot = pgprot_clear_protnone_bits(req_prot);\n\tif (pgprot_val(req_prot) & _PAGE_PRESENT)\n\t\tpgprot_val(req_prot) |= _PAGE_PSE;\n\n\t \n\tpfn = old_pfn + ((address & (psize - 1)) >> PAGE_SHIFT);\n\tcpa->pfn = pfn;\n\n\t \n\tlpaddr = address & pmask;\n\tnumpages = psize >> PAGE_SHIFT;\n\n\t \n\tchk_prot = static_protections(old_prot, lpaddr, old_pfn, numpages,\n\t\t\t\t      psize, CPA_CONFLICT);\n\n\tif (WARN_ON_ONCE(pgprot_val(chk_prot) != pgprot_val(old_prot))) {\n\t\t \n\t\tcpa->force_static_prot = 1;\n\t\treturn 1;\n\t}\n\n\t \n\tif (pgprot_val(req_prot) == pgprot_val(old_prot)) {\n\t\tcpa_inc_lp_sameprot(level);\n\t\treturn 0;\n\t}\n\n\t \n\tif (address != lpaddr || cpa->numpages != numpages)\n\t\treturn 1;\n\n\t \n\tnew_prot = static_protections(req_prot, lpaddr, old_pfn, numpages,\n\t\t\t\t      psize, CPA_DETECT);\n\n\tnew_prot = verify_rwx(old_prot, new_prot, lpaddr, old_pfn, numpages);\n\n\t \n\tif (pgprot_val(req_prot) != pgprot_val(new_prot))\n\t\treturn 1;\n\n\t \n\tnew_pte = pfn_pte(old_pfn, new_prot);\n\t__set_pmd_pte(kpte, address, new_pte);\n\tcpa->flags |= CPA_FLUSHTLB;\n\tcpa_inc_lp_preserved(level);\n\treturn 0;\n}\n\nstatic int should_split_large_page(pte_t *kpte, unsigned long address,\n\t\t\t\t   struct cpa_data *cpa)\n{\n\tint do_split;\n\n\tif (cpa->force_split)\n\t\treturn 1;\n\n\tspin_lock(&pgd_lock);\n\tdo_split = __should_split_large_page(kpte, address, cpa);\n\tspin_unlock(&pgd_lock);\n\n\treturn do_split;\n}\n\nstatic void split_set_pte(struct cpa_data *cpa, pte_t *pte, unsigned long pfn,\n\t\t\t  pgprot_t ref_prot, unsigned long address,\n\t\t\t  unsigned long size)\n{\n\tunsigned int npg = PFN_DOWN(size);\n\tpgprot_t prot;\n\n\t \n\tif (!cpa->force_static_prot)\n\t\tgoto set;\n\n\t \n\tprot = static_protections(ref_prot, address, pfn, npg, 0, CPA_PROTECT);\n\n\tif (pgprot_val(prot) == pgprot_val(ref_prot))\n\t\tgoto set;\n\n\t \n\tif (size == PAGE_SIZE)\n\t\tref_prot = prot;\n\telse\n\t\tpr_warn_once(\"CPA: Cannot fixup static protections for PUD split\\n\");\nset:\n\tset_pte(pte, pfn_pte(pfn, ref_prot));\n}\n\nstatic int\n__split_large_page(struct cpa_data *cpa, pte_t *kpte, unsigned long address,\n\t\t   struct page *base)\n{\n\tunsigned long lpaddr, lpinc, ref_pfn, pfn, pfninc = 1;\n\tpte_t *pbase = (pte_t *)page_address(base);\n\tunsigned int i, level;\n\tpgprot_t ref_prot;\n\tpte_t *tmp;\n\n\tspin_lock(&pgd_lock);\n\t \n\ttmp = _lookup_address_cpa(cpa, address, &level);\n\tif (tmp != kpte) {\n\t\tspin_unlock(&pgd_lock);\n\t\treturn 1;\n\t}\n\n\tparavirt_alloc_pte(&init_mm, page_to_pfn(base));\n\n\tswitch (level) {\n\tcase PG_LEVEL_2M:\n\t\tref_prot = pmd_pgprot(*(pmd_t *)kpte);\n\t\t \n\t\tref_prot = pgprot_large_2_4k(ref_prot);\n\t\tref_pfn = pmd_pfn(*(pmd_t *)kpte);\n\t\tlpaddr = address & PMD_MASK;\n\t\tlpinc = PAGE_SIZE;\n\t\tbreak;\n\n\tcase PG_LEVEL_1G:\n\t\tref_prot = pud_pgprot(*(pud_t *)kpte);\n\t\tref_pfn = pud_pfn(*(pud_t *)kpte);\n\t\tpfninc = PMD_SIZE >> PAGE_SHIFT;\n\t\tlpaddr = address & PUD_MASK;\n\t\tlpinc = PMD_SIZE;\n\t\t \n\t\tif (!(pgprot_val(ref_prot) & _PAGE_PRESENT))\n\t\t\tpgprot_val(ref_prot) &= ~_PAGE_PSE;\n\t\tbreak;\n\n\tdefault:\n\t\tspin_unlock(&pgd_lock);\n\t\treturn 1;\n\t}\n\n\tref_prot = pgprot_clear_protnone_bits(ref_prot);\n\n\t \n\tpfn = ref_pfn;\n\tfor (i = 0; i < PTRS_PER_PTE; i++, pfn += pfninc, lpaddr += lpinc)\n\t\tsplit_set_pte(cpa, pbase + i, pfn, ref_prot, lpaddr, lpinc);\n\n\tif (virt_addr_valid(address)) {\n\t\tunsigned long pfn = PFN_DOWN(__pa(address));\n\n\t\tif (pfn_range_is_mapped(pfn, pfn + 1))\n\t\t\tsplit_page_count(level);\n\t}\n\n\t \n\t__set_pmd_pte(kpte, address, mk_pte(base, __pgprot(_KERNPG_TABLE)));\n\n\t \n\tflush_tlb_all();\n\tspin_unlock(&pgd_lock);\n\n\treturn 0;\n}\n\nstatic int split_large_page(struct cpa_data *cpa, pte_t *kpte,\n\t\t\t    unsigned long address)\n{\n\tstruct page *base;\n\n\tif (!debug_pagealloc_enabled())\n\t\tspin_unlock(&cpa_lock);\n\tbase = alloc_pages(GFP_KERNEL, 0);\n\tif (!debug_pagealloc_enabled())\n\t\tspin_lock(&cpa_lock);\n\tif (!base)\n\t\treturn -ENOMEM;\n\n\tif (__split_large_page(cpa, kpte, address, base))\n\t\t__free_page(base);\n\n\treturn 0;\n}\n\nstatic bool try_to_free_pte_page(pte_t *pte)\n{\n\tint i;\n\n\tfor (i = 0; i < PTRS_PER_PTE; i++)\n\t\tif (!pte_none(pte[i]))\n\t\t\treturn false;\n\n\tfree_page((unsigned long)pte);\n\treturn true;\n}\n\nstatic bool try_to_free_pmd_page(pmd_t *pmd)\n{\n\tint i;\n\n\tfor (i = 0; i < PTRS_PER_PMD; i++)\n\t\tif (!pmd_none(pmd[i]))\n\t\t\treturn false;\n\n\tfree_page((unsigned long)pmd);\n\treturn true;\n}\n\nstatic bool unmap_pte_range(pmd_t *pmd, unsigned long start, unsigned long end)\n{\n\tpte_t *pte = pte_offset_kernel(pmd, start);\n\n\twhile (start < end) {\n\t\tset_pte(pte, __pte(0));\n\n\t\tstart += PAGE_SIZE;\n\t\tpte++;\n\t}\n\n\tif (try_to_free_pte_page((pte_t *)pmd_page_vaddr(*pmd))) {\n\t\tpmd_clear(pmd);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void __unmap_pmd_range(pud_t *pud, pmd_t *pmd,\n\t\t\t      unsigned long start, unsigned long end)\n{\n\tif (unmap_pte_range(pmd, start, end))\n\t\tif (try_to_free_pmd_page(pud_pgtable(*pud)))\n\t\t\tpud_clear(pud);\n}\n\nstatic void unmap_pmd_range(pud_t *pud, unsigned long start, unsigned long end)\n{\n\tpmd_t *pmd = pmd_offset(pud, start);\n\n\t \n\tif (start & (PMD_SIZE - 1)) {\n\t\tunsigned long next_page = (start + PMD_SIZE) & PMD_MASK;\n\t\tunsigned long pre_end = min_t(unsigned long, end, next_page);\n\n\t\t__unmap_pmd_range(pud, pmd, start, pre_end);\n\n\t\tstart = pre_end;\n\t\tpmd++;\n\t}\n\n\t \n\twhile (end - start >= PMD_SIZE) {\n\t\tif (pmd_large(*pmd))\n\t\t\tpmd_clear(pmd);\n\t\telse\n\t\t\t__unmap_pmd_range(pud, pmd, start, start + PMD_SIZE);\n\n\t\tstart += PMD_SIZE;\n\t\tpmd++;\n\t}\n\n\t \n\tif (start < end)\n\t\treturn __unmap_pmd_range(pud, pmd, start, end);\n\n\t \n\tif (!pud_none(*pud))\n\t\tif (try_to_free_pmd_page(pud_pgtable(*pud)))\n\t\t\tpud_clear(pud);\n}\n\nstatic void unmap_pud_range(p4d_t *p4d, unsigned long start, unsigned long end)\n{\n\tpud_t *pud = pud_offset(p4d, start);\n\n\t \n\tif (start & (PUD_SIZE - 1)) {\n\t\tunsigned long next_page = (start + PUD_SIZE) & PUD_MASK;\n\t\tunsigned long pre_end\t= min_t(unsigned long, end, next_page);\n\n\t\tunmap_pmd_range(pud, start, pre_end);\n\n\t\tstart = pre_end;\n\t\tpud++;\n\t}\n\n\t \n\twhile (end - start >= PUD_SIZE) {\n\n\t\tif (pud_large(*pud))\n\t\t\tpud_clear(pud);\n\t\telse\n\t\t\tunmap_pmd_range(pud, start, start + PUD_SIZE);\n\n\t\tstart += PUD_SIZE;\n\t\tpud++;\n\t}\n\n\t \n\tif (start < end)\n\t\tunmap_pmd_range(pud, start, end);\n\n\t \n}\n\nstatic int alloc_pte_page(pmd_t *pmd)\n{\n\tpte_t *pte = (pte_t *)get_zeroed_page(GFP_KERNEL);\n\tif (!pte)\n\t\treturn -1;\n\n\tset_pmd(pmd, __pmd(__pa(pte) | _KERNPG_TABLE));\n\treturn 0;\n}\n\nstatic int alloc_pmd_page(pud_t *pud)\n{\n\tpmd_t *pmd = (pmd_t *)get_zeroed_page(GFP_KERNEL);\n\tif (!pmd)\n\t\treturn -1;\n\n\tset_pud(pud, __pud(__pa(pmd) | _KERNPG_TABLE));\n\treturn 0;\n}\n\nstatic void populate_pte(struct cpa_data *cpa,\n\t\t\t unsigned long start, unsigned long end,\n\t\t\t unsigned num_pages, pmd_t *pmd, pgprot_t pgprot)\n{\n\tpte_t *pte;\n\n\tpte = pte_offset_kernel(pmd, start);\n\n\tpgprot = pgprot_clear_protnone_bits(pgprot);\n\n\twhile (num_pages-- && start < end) {\n\t\tset_pte(pte, pfn_pte(cpa->pfn, pgprot));\n\n\t\tstart\t += PAGE_SIZE;\n\t\tcpa->pfn++;\n\t\tpte++;\n\t}\n}\n\nstatic long populate_pmd(struct cpa_data *cpa,\n\t\t\t unsigned long start, unsigned long end,\n\t\t\t unsigned num_pages, pud_t *pud, pgprot_t pgprot)\n{\n\tlong cur_pages = 0;\n\tpmd_t *pmd;\n\tpgprot_t pmd_pgprot;\n\n\t \n\tif (start & (PMD_SIZE - 1)) {\n\t\tunsigned long pre_end = start + (num_pages << PAGE_SHIFT);\n\t\tunsigned long next_page = (start + PMD_SIZE) & PMD_MASK;\n\n\t\tpre_end   = min_t(unsigned long, pre_end, next_page);\n\t\tcur_pages = (pre_end - start) >> PAGE_SHIFT;\n\t\tcur_pages = min_t(unsigned int, num_pages, cur_pages);\n\n\t\t \n\t\tpmd = pmd_offset(pud, start);\n\t\tif (pmd_none(*pmd))\n\t\t\tif (alloc_pte_page(pmd))\n\t\t\t\treturn -1;\n\n\t\tpopulate_pte(cpa, start, pre_end, cur_pages, pmd, pgprot);\n\n\t\tstart = pre_end;\n\t}\n\n\t \n\tif (num_pages == cur_pages)\n\t\treturn cur_pages;\n\n\tpmd_pgprot = pgprot_4k_2_large(pgprot);\n\n\twhile (end - start >= PMD_SIZE) {\n\n\t\t \n\t\tif (pud_none(*pud))\n\t\t\tif (alloc_pmd_page(pud))\n\t\t\t\treturn -1;\n\n\t\tpmd = pmd_offset(pud, start);\n\n\t\tset_pmd(pmd, pmd_mkhuge(pfn_pmd(cpa->pfn,\n\t\t\t\t\tcanon_pgprot(pmd_pgprot))));\n\n\t\tstart\t  += PMD_SIZE;\n\t\tcpa->pfn  += PMD_SIZE >> PAGE_SHIFT;\n\t\tcur_pages += PMD_SIZE >> PAGE_SHIFT;\n\t}\n\n\t \n\tif (start < end) {\n\t\tpmd = pmd_offset(pud, start);\n\t\tif (pmd_none(*pmd))\n\t\t\tif (alloc_pte_page(pmd))\n\t\t\t\treturn -1;\n\n\t\tpopulate_pte(cpa, start, end, num_pages - cur_pages,\n\t\t\t     pmd, pgprot);\n\t}\n\treturn num_pages;\n}\n\nstatic int populate_pud(struct cpa_data *cpa, unsigned long start, p4d_t *p4d,\n\t\t\tpgprot_t pgprot)\n{\n\tpud_t *pud;\n\tunsigned long end;\n\tlong cur_pages = 0;\n\tpgprot_t pud_pgprot;\n\n\tend = start + (cpa->numpages << PAGE_SHIFT);\n\n\t \n\tif (start & (PUD_SIZE - 1)) {\n\t\tunsigned long pre_end;\n\t\tunsigned long next_page = (start + PUD_SIZE) & PUD_MASK;\n\n\t\tpre_end   = min_t(unsigned long, end, next_page);\n\t\tcur_pages = (pre_end - start) >> PAGE_SHIFT;\n\t\tcur_pages = min_t(int, (int)cpa->numpages, cur_pages);\n\n\t\tpud = pud_offset(p4d, start);\n\n\t\t \n\t\tif (pud_none(*pud))\n\t\t\tif (alloc_pmd_page(pud))\n\t\t\t\treturn -1;\n\n\t\tcur_pages = populate_pmd(cpa, start, pre_end, cur_pages,\n\t\t\t\t\t pud, pgprot);\n\t\tif (cur_pages < 0)\n\t\t\treturn cur_pages;\n\n\t\tstart = pre_end;\n\t}\n\n\t \n\tif (cpa->numpages == cur_pages)\n\t\treturn cur_pages;\n\n\tpud = pud_offset(p4d, start);\n\tpud_pgprot = pgprot_4k_2_large(pgprot);\n\n\t \n\twhile (boot_cpu_has(X86_FEATURE_GBPAGES) && end - start >= PUD_SIZE) {\n\t\tset_pud(pud, pud_mkhuge(pfn_pud(cpa->pfn,\n\t\t\t\t   canon_pgprot(pud_pgprot))));\n\n\t\tstart\t  += PUD_SIZE;\n\t\tcpa->pfn  += PUD_SIZE >> PAGE_SHIFT;\n\t\tcur_pages += PUD_SIZE >> PAGE_SHIFT;\n\t\tpud++;\n\t}\n\n\t \n\tif (start < end) {\n\t\tlong tmp;\n\n\t\tpud = pud_offset(p4d, start);\n\t\tif (pud_none(*pud))\n\t\t\tif (alloc_pmd_page(pud))\n\t\t\t\treturn -1;\n\n\t\ttmp = populate_pmd(cpa, start, end, cpa->numpages - cur_pages,\n\t\t\t\t   pud, pgprot);\n\t\tif (tmp < 0)\n\t\t\treturn cur_pages;\n\n\t\tcur_pages += tmp;\n\t}\n\treturn cur_pages;\n}\n\n \nstatic int populate_pgd(struct cpa_data *cpa, unsigned long addr)\n{\n\tpgprot_t pgprot = __pgprot(_KERNPG_TABLE);\n\tpud_t *pud = NULL;\t \n\tp4d_t *p4d;\n\tpgd_t *pgd_entry;\n\tlong ret;\n\n\tpgd_entry = cpa->pgd + pgd_index(addr);\n\n\tif (pgd_none(*pgd_entry)) {\n\t\tp4d = (p4d_t *)get_zeroed_page(GFP_KERNEL);\n\t\tif (!p4d)\n\t\t\treturn -1;\n\n\t\tset_pgd(pgd_entry, __pgd(__pa(p4d) | _KERNPG_TABLE));\n\t}\n\n\t \n\tp4d = p4d_offset(pgd_entry, addr);\n\tif (p4d_none(*p4d)) {\n\t\tpud = (pud_t *)get_zeroed_page(GFP_KERNEL);\n\t\tif (!pud)\n\t\t\treturn -1;\n\n\t\tset_p4d(p4d, __p4d(__pa(pud) | _KERNPG_TABLE));\n\t}\n\n\tpgprot_val(pgprot) &= ~pgprot_val(cpa->mask_clr);\n\tpgprot_val(pgprot) |=  pgprot_val(cpa->mask_set);\n\n\tret = populate_pud(cpa, addr, p4d, pgprot);\n\tif (ret < 0) {\n\t\t \n\t\tunmap_pud_range(p4d, addr,\n\t\t\t\taddr + (cpa->numpages << PAGE_SHIFT));\n\t\treturn ret;\n\t}\n\n\tcpa->numpages = ret;\n\treturn 0;\n}\n\nstatic int __cpa_process_fault(struct cpa_data *cpa, unsigned long vaddr,\n\t\t\t       int primary)\n{\n\tif (cpa->pgd) {\n\t\t \n\t\treturn populate_pgd(cpa, vaddr);\n\t}\n\n\t \n\tif (!primary) {\n\t\tcpa->numpages = 1;\n\t\treturn 0;\n\t}\n\n\t \n\tif (within(vaddr, PAGE_OFFSET,\n\t\t   PAGE_OFFSET + (max_pfn_mapped << PAGE_SHIFT))) {\n\t\tcpa->numpages = 1;\n\t\tcpa->pfn = __pa(vaddr) >> PAGE_SHIFT;\n\t\treturn 0;\n\n\t} else if (__cpa_pfn_in_highmap(cpa->pfn)) {\n\t\t \n\t\treturn -EFAULT;\n\t} else {\n\t\tWARN(1, KERN_WARNING \"CPA: called for zero pte. \"\n\t\t\t\"vaddr = %lx cpa->vaddr = %lx\\n\", vaddr,\n\t\t\t*cpa->vaddr);\n\n\t\treturn -EFAULT;\n\t}\n}\n\nstatic int __change_page_attr(struct cpa_data *cpa, int primary)\n{\n\tunsigned long address;\n\tint do_split, err;\n\tunsigned int level;\n\tpte_t *kpte, old_pte;\n\n\taddress = __cpa_addr(cpa, cpa->curpage);\nrepeat:\n\tkpte = _lookup_address_cpa(cpa, address, &level);\n\tif (!kpte)\n\t\treturn __cpa_process_fault(cpa, address, primary);\n\n\told_pte = *kpte;\n\tif (pte_none(old_pte))\n\t\treturn __cpa_process_fault(cpa, address, primary);\n\n\tif (level == PG_LEVEL_4K) {\n\t\tpte_t new_pte;\n\t\tpgprot_t old_prot = pte_pgprot(old_pte);\n\t\tpgprot_t new_prot = pte_pgprot(old_pte);\n\t\tunsigned long pfn = pte_pfn(old_pte);\n\n\t\tpgprot_val(new_prot) &= ~pgprot_val(cpa->mask_clr);\n\t\tpgprot_val(new_prot) |= pgprot_val(cpa->mask_set);\n\n\t\tcpa_inc_4k_install();\n\t\t \n\t\tnew_prot = static_protections(new_prot, address, pfn, 1, 0,\n\t\t\t\t\t      CPA_PROTECT);\n\n\t\tnew_prot = verify_rwx(old_prot, new_prot, address, pfn, 1);\n\n\t\tnew_prot = pgprot_clear_protnone_bits(new_prot);\n\n\t\t \n\t\tnew_pte = pfn_pte(pfn, new_prot);\n\t\tcpa->pfn = pfn;\n\t\t \n\t\tif (pte_val(old_pte) != pte_val(new_pte)) {\n\t\t\tset_pte_atomic(kpte, new_pte);\n\t\t\tcpa->flags |= CPA_FLUSHTLB;\n\t\t}\n\t\tcpa->numpages = 1;\n\t\treturn 0;\n\t}\n\n\t \n\tdo_split = should_split_large_page(kpte, address, cpa);\n\t \n\tif (do_split <= 0)\n\t\treturn do_split;\n\n\t \n\terr = split_large_page(cpa, kpte, address);\n\tif (!err)\n\t\tgoto repeat;\n\n\treturn err;\n}\n\nstatic int __change_page_attr_set_clr(struct cpa_data *cpa, int primary);\n\n \nstatic int cpa_process_alias(struct cpa_data *cpa)\n{\n\tstruct cpa_data alias_cpa;\n\tunsigned long laddr = (unsigned long)__va(cpa->pfn << PAGE_SHIFT);\n\tunsigned long vaddr;\n\tint ret;\n\n\tif (!pfn_range_is_mapped(cpa->pfn, cpa->pfn + 1))\n\t\treturn 0;\n\n\t \n\tvaddr = __cpa_addr(cpa, cpa->curpage);\n\tif (!(within(vaddr, PAGE_OFFSET,\n\t\t    PAGE_OFFSET + (max_pfn_mapped << PAGE_SHIFT)))) {\n\n\t\talias_cpa = *cpa;\n\t\talias_cpa.vaddr = &laddr;\n\t\talias_cpa.flags &= ~(CPA_PAGES_ARRAY | CPA_ARRAY);\n\t\talias_cpa.curpage = 0;\n\n\t\t \n\t\tif (__supported_pte_mask & _PAGE_NX) {\n\t\t\talias_cpa.mask_clr.pgprot &= ~_PAGE_NX;\n\t\t\talias_cpa.mask_set.pgprot &= ~_PAGE_NX;\n\t\t}\n\n\t\tcpa->force_flush_all = 1;\n\n\t\tret = __change_page_attr_set_clr(&alias_cpa, 0);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n#ifdef CONFIG_X86_64\n\t \n\tif (!within(vaddr, (unsigned long)_text, _brk_end) &&\n\t    __cpa_pfn_in_highmap(cpa->pfn)) {\n\t\tunsigned long temp_cpa_vaddr = (cpa->pfn << PAGE_SHIFT) +\n\t\t\t\t\t       __START_KERNEL_map - phys_base;\n\t\talias_cpa = *cpa;\n\t\talias_cpa.vaddr = &temp_cpa_vaddr;\n\t\talias_cpa.flags &= ~(CPA_PAGES_ARRAY | CPA_ARRAY);\n\t\talias_cpa.curpage = 0;\n\n\t\t \n\t\tif (__supported_pte_mask & _PAGE_NX) {\n\t\t\talias_cpa.mask_clr.pgprot &= ~_PAGE_NX;\n\t\t\talias_cpa.mask_set.pgprot &= ~_PAGE_NX;\n\t\t}\n\n\t\tcpa->force_flush_all = 1;\n\t\t \n\t\t__change_page_attr_set_clr(&alias_cpa, 0);\n\t}\n#endif\n\n\treturn 0;\n}\n\nstatic int __change_page_attr_set_clr(struct cpa_data *cpa, int primary)\n{\n\tunsigned long numpages = cpa->numpages;\n\tunsigned long rempages = numpages;\n\tint ret = 0;\n\n\t \n\tif (!(pgprot_val(cpa->mask_set) | pgprot_val(cpa->mask_clr)) &&\n\t    !cpa->force_split)\n\t\treturn ret;\n\n\twhile (rempages) {\n\t\t \n\t\tcpa->numpages = rempages;\n\t\t \n\t\tif (cpa->flags & (CPA_ARRAY | CPA_PAGES_ARRAY))\n\t\t\tcpa->numpages = 1;\n\n\t\tif (!debug_pagealloc_enabled())\n\t\t\tspin_lock(&cpa_lock);\n\t\tret = __change_page_attr(cpa, primary);\n\t\tif (!debug_pagealloc_enabled())\n\t\t\tspin_unlock(&cpa_lock);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\tif (primary && !(cpa->flags & CPA_NO_CHECK_ALIAS)) {\n\t\t\tret = cpa_process_alias(cpa);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tBUG_ON(cpa->numpages > rempages || !cpa->numpages);\n\t\trempages -= cpa->numpages;\n\t\tcpa->curpage += cpa->numpages;\n\t}\n\nout:\n\t \n\tcpa->numpages = numpages;\n\treturn ret;\n}\n\nstatic int change_page_attr_set_clr(unsigned long *addr, int numpages,\n\t\t\t\t    pgprot_t mask_set, pgprot_t mask_clr,\n\t\t\t\t    int force_split, int in_flag,\n\t\t\t\t    struct page **pages)\n{\n\tstruct cpa_data cpa;\n\tint ret, cache;\n\n\tmemset(&cpa, 0, sizeof(cpa));\n\n\t \n\tmask_set = canon_pgprot(mask_set);\n\n\tif (!pgprot_val(mask_set) && !pgprot_val(mask_clr) && !force_split)\n\t\treturn 0;\n\n\t \n\tif (in_flag & CPA_ARRAY) {\n\t\tint i;\n\t\tfor (i = 0; i < numpages; i++) {\n\t\t\tif (addr[i] & ~PAGE_MASK) {\n\t\t\t\taddr[i] &= PAGE_MASK;\n\t\t\t\tWARN_ON_ONCE(1);\n\t\t\t}\n\t\t}\n\t} else if (!(in_flag & CPA_PAGES_ARRAY)) {\n\t\t \n\t\tif (*addr & ~PAGE_MASK) {\n\t\t\t*addr &= PAGE_MASK;\n\t\t\t \n\t\t\tWARN_ON_ONCE(1);\n\t\t}\n\t}\n\n\t \n\tkmap_flush_unused();\n\n\tvm_unmap_aliases();\n\n\tcpa.vaddr = addr;\n\tcpa.pages = pages;\n\tcpa.numpages = numpages;\n\tcpa.mask_set = mask_set;\n\tcpa.mask_clr = mask_clr;\n\tcpa.flags = in_flag;\n\tcpa.curpage = 0;\n\tcpa.force_split = force_split;\n\n\tret = __change_page_attr_set_clr(&cpa, 1);\n\n\t \n\tif (!(cpa.flags & CPA_FLUSHTLB))\n\t\tgoto out;\n\n\t \n\tcache = !!pgprot2cachemode(mask_set);\n\n\t \n\tif (ret) {\n\t\tcpa_flush_all(cache);\n\t\tgoto out;\n\t}\n\n\tcpa_flush(&cpa, cache);\nout:\n\treturn ret;\n}\n\nstatic inline int change_page_attr_set(unsigned long *addr, int numpages,\n\t\t\t\t       pgprot_t mask, int array)\n{\n\treturn change_page_attr_set_clr(addr, numpages, mask, __pgprot(0), 0,\n\t\t(array ? CPA_ARRAY : 0), NULL);\n}\n\nstatic inline int change_page_attr_clear(unsigned long *addr, int numpages,\n\t\t\t\t\t pgprot_t mask, int array)\n{\n\treturn change_page_attr_set_clr(addr, numpages, __pgprot(0), mask, 0,\n\t\t(array ? CPA_ARRAY : 0), NULL);\n}\n\nstatic inline int cpa_set_pages_array(struct page **pages, int numpages,\n\t\t\t\t       pgprot_t mask)\n{\n\treturn change_page_attr_set_clr(NULL, numpages, mask, __pgprot(0), 0,\n\t\tCPA_PAGES_ARRAY, pages);\n}\n\nstatic inline int cpa_clear_pages_array(struct page **pages, int numpages,\n\t\t\t\t\t pgprot_t mask)\n{\n\treturn change_page_attr_set_clr(NULL, numpages, __pgprot(0), mask, 0,\n\t\tCPA_PAGES_ARRAY, pages);\n}\n\n \nint __set_memory_prot(unsigned long addr, int numpages, pgprot_t prot)\n{\n\treturn change_page_attr_set_clr(&addr, numpages, prot,\n\t\t\t\t\t__pgprot(~pgprot_val(prot)), 0, 0,\n\t\t\t\t\tNULL);\n}\n\nint _set_memory_uc(unsigned long addr, int numpages)\n{\n\t \n\treturn change_page_attr_set(&addr, numpages,\n\t\t\t\t    cachemode2pgprot(_PAGE_CACHE_MODE_UC_MINUS),\n\t\t\t\t    0);\n}\n\nint set_memory_uc(unsigned long addr, int numpages)\n{\n\tint ret;\n\n\t \n\tret = memtype_reserve(__pa(addr), __pa(addr) + numpages * PAGE_SIZE,\n\t\t\t      _PAGE_CACHE_MODE_UC_MINUS, NULL);\n\tif (ret)\n\t\tgoto out_err;\n\n\tret = _set_memory_uc(addr, numpages);\n\tif (ret)\n\t\tgoto out_free;\n\n\treturn 0;\n\nout_free:\n\tmemtype_free(__pa(addr), __pa(addr) + numpages * PAGE_SIZE);\nout_err:\n\treturn ret;\n}\nEXPORT_SYMBOL(set_memory_uc);\n\nint _set_memory_wc(unsigned long addr, int numpages)\n{\n\tint ret;\n\n\tret = change_page_attr_set(&addr, numpages,\n\t\t\t\t   cachemode2pgprot(_PAGE_CACHE_MODE_UC_MINUS),\n\t\t\t\t   0);\n\tif (!ret) {\n\t\tret = change_page_attr_set_clr(&addr, numpages,\n\t\t\t\t\t       cachemode2pgprot(_PAGE_CACHE_MODE_WC),\n\t\t\t\t\t       __pgprot(_PAGE_CACHE_MASK),\n\t\t\t\t\t       0, 0, NULL);\n\t}\n\treturn ret;\n}\n\nint set_memory_wc(unsigned long addr, int numpages)\n{\n\tint ret;\n\n\tret = memtype_reserve(__pa(addr), __pa(addr) + numpages * PAGE_SIZE,\n\t\t_PAGE_CACHE_MODE_WC, NULL);\n\tif (ret)\n\t\treturn ret;\n\n\tret = _set_memory_wc(addr, numpages);\n\tif (ret)\n\t\tmemtype_free(__pa(addr), __pa(addr) + numpages * PAGE_SIZE);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(set_memory_wc);\n\nint _set_memory_wt(unsigned long addr, int numpages)\n{\n\treturn change_page_attr_set(&addr, numpages,\n\t\t\t\t    cachemode2pgprot(_PAGE_CACHE_MODE_WT), 0);\n}\n\nint _set_memory_wb(unsigned long addr, int numpages)\n{\n\t \n\treturn change_page_attr_clear(&addr, numpages,\n\t\t\t\t      __pgprot(_PAGE_CACHE_MASK), 0);\n}\n\nint set_memory_wb(unsigned long addr, int numpages)\n{\n\tint ret;\n\n\tret = _set_memory_wb(addr, numpages);\n\tif (ret)\n\t\treturn ret;\n\n\tmemtype_free(__pa(addr), __pa(addr) + numpages * PAGE_SIZE);\n\treturn 0;\n}\nEXPORT_SYMBOL(set_memory_wb);\n\n \n#ifdef CONFIG_X86_64\nint set_mce_nospec(unsigned long pfn)\n{\n\tunsigned long decoy_addr;\n\tint rc;\n\n\t \n\tif (arch_is_platform_page(pfn << PAGE_SHIFT))\n\t\treturn 0;\n\t \n\tdecoy_addr = (pfn << PAGE_SHIFT) + (PAGE_OFFSET ^ BIT(63));\n\n\trc = set_memory_np(decoy_addr, 1);\n\tif (rc)\n\t\tpr_warn(\"Could not invalidate pfn=0x%lx from 1:1 map\\n\", pfn);\n\treturn rc;\n}\n\nstatic int set_memory_p(unsigned long *addr, int numpages)\n{\n\treturn change_page_attr_set(addr, numpages, __pgprot(_PAGE_PRESENT), 0);\n}\n\n \nint clear_mce_nospec(unsigned long pfn)\n{\n\tunsigned long addr = (unsigned long) pfn_to_kaddr(pfn);\n\n\treturn set_memory_p(&addr, 1);\n}\nEXPORT_SYMBOL_GPL(clear_mce_nospec);\n#endif  \n\nint set_memory_x(unsigned long addr, int numpages)\n{\n\tif (!(__supported_pte_mask & _PAGE_NX))\n\t\treturn 0;\n\n\treturn change_page_attr_clear(&addr, numpages, __pgprot(_PAGE_NX), 0);\n}\n\nint set_memory_nx(unsigned long addr, int numpages)\n{\n\tif (!(__supported_pte_mask & _PAGE_NX))\n\t\treturn 0;\n\n\treturn change_page_attr_set(&addr, numpages, __pgprot(_PAGE_NX), 0);\n}\n\nint set_memory_ro(unsigned long addr, int numpages)\n{\n\treturn change_page_attr_clear(&addr, numpages, __pgprot(_PAGE_RW | _PAGE_DIRTY), 0);\n}\n\nint set_memory_rox(unsigned long addr, int numpages)\n{\n\tpgprot_t clr = __pgprot(_PAGE_RW | _PAGE_DIRTY);\n\n\tif (__supported_pte_mask & _PAGE_NX)\n\t\tclr.pgprot |= _PAGE_NX;\n\n\treturn change_page_attr_clear(&addr, numpages, clr, 0);\n}\n\nint set_memory_rw(unsigned long addr, int numpages)\n{\n\treturn change_page_attr_set(&addr, numpages, __pgprot(_PAGE_RW), 0);\n}\n\nint set_memory_np(unsigned long addr, int numpages)\n{\n\treturn change_page_attr_clear(&addr, numpages, __pgprot(_PAGE_PRESENT), 0);\n}\n\nint set_memory_np_noalias(unsigned long addr, int numpages)\n{\n\treturn change_page_attr_set_clr(&addr, numpages, __pgprot(0),\n\t\t\t\t\t__pgprot(_PAGE_PRESENT), 0,\n\t\t\t\t\tCPA_NO_CHECK_ALIAS, NULL);\n}\n\nint set_memory_4k(unsigned long addr, int numpages)\n{\n\treturn change_page_attr_set_clr(&addr, numpages, __pgprot(0),\n\t\t\t\t\t__pgprot(0), 1, 0, NULL);\n}\n\nint set_memory_nonglobal(unsigned long addr, int numpages)\n{\n\treturn change_page_attr_clear(&addr, numpages,\n\t\t\t\t      __pgprot(_PAGE_GLOBAL), 0);\n}\n\nint set_memory_global(unsigned long addr, int numpages)\n{\n\treturn change_page_attr_set(&addr, numpages,\n\t\t\t\t    __pgprot(_PAGE_GLOBAL), 0);\n}\n\n \nstatic int __set_memory_enc_pgtable(unsigned long addr, int numpages, bool enc)\n{\n\tpgprot_t empty = __pgprot(0);\n\tstruct cpa_data cpa;\n\tint ret;\n\n\t \n\tif (WARN_ONCE(addr & ~PAGE_MASK, \"misaligned address: %#lx\\n\", addr))\n\t\taddr &= PAGE_MASK;\n\n\tmemset(&cpa, 0, sizeof(cpa));\n\tcpa.vaddr = &addr;\n\tcpa.numpages = numpages;\n\tcpa.mask_set = enc ? pgprot_encrypted(empty) : pgprot_decrypted(empty);\n\tcpa.mask_clr = enc ? pgprot_decrypted(empty) : pgprot_encrypted(empty);\n\tcpa.pgd = init_mm.pgd;\n\n\t \n\tkmap_flush_unused();\n\tvm_unmap_aliases();\n\n\t \n\tif (x86_platform.guest.enc_tlb_flush_required(enc))\n\t\tcpa_flush(&cpa, x86_platform.guest.enc_cache_flush_required());\n\n\t \n\tif (!x86_platform.guest.enc_status_change_prepare(addr, numpages, enc))\n\t\treturn -EIO;\n\n\tret = __change_page_attr_set_clr(&cpa, 1);\n\n\t \n\tcpa_flush(&cpa, 0);\n\n\t \n\tif (!ret) {\n\t\tif (!x86_platform.guest.enc_status_change_finish(addr, numpages, enc))\n\t\t\tret = -EIO;\n\t}\n\n\treturn ret;\n}\n\nstatic int __set_memory_enc_dec(unsigned long addr, int numpages, bool enc)\n{\n\tif (cc_platform_has(CC_ATTR_MEM_ENCRYPT))\n\t\treturn __set_memory_enc_pgtable(addr, numpages, enc);\n\n\treturn 0;\n}\n\nint set_memory_encrypted(unsigned long addr, int numpages)\n{\n\treturn __set_memory_enc_dec(addr, numpages, true);\n}\nEXPORT_SYMBOL_GPL(set_memory_encrypted);\n\nint set_memory_decrypted(unsigned long addr, int numpages)\n{\n\treturn __set_memory_enc_dec(addr, numpages, false);\n}\nEXPORT_SYMBOL_GPL(set_memory_decrypted);\n\nint set_pages_uc(struct page *page, int numpages)\n{\n\tunsigned long addr = (unsigned long)page_address(page);\n\n\treturn set_memory_uc(addr, numpages);\n}\nEXPORT_SYMBOL(set_pages_uc);\n\nstatic int _set_pages_array(struct page **pages, int numpages,\n\t\tenum page_cache_mode new_type)\n{\n\tunsigned long start;\n\tunsigned long end;\n\tenum page_cache_mode set_type;\n\tint i;\n\tint free_idx;\n\tint ret;\n\n\tfor (i = 0; i < numpages; i++) {\n\t\tif (PageHighMem(pages[i]))\n\t\t\tcontinue;\n\t\tstart = page_to_pfn(pages[i]) << PAGE_SHIFT;\n\t\tend = start + PAGE_SIZE;\n\t\tif (memtype_reserve(start, end, new_type, NULL))\n\t\t\tgoto err_out;\n\t}\n\n\t \n\tset_type = (new_type == _PAGE_CACHE_MODE_WC) ?\n\t\t\t\t_PAGE_CACHE_MODE_UC_MINUS : new_type;\n\n\tret = cpa_set_pages_array(pages, numpages,\n\t\t\t\t  cachemode2pgprot(set_type));\n\tif (!ret && new_type == _PAGE_CACHE_MODE_WC)\n\t\tret = change_page_attr_set_clr(NULL, numpages,\n\t\t\t\t\t       cachemode2pgprot(\n\t\t\t\t\t\t_PAGE_CACHE_MODE_WC),\n\t\t\t\t\t       __pgprot(_PAGE_CACHE_MASK),\n\t\t\t\t\t       0, CPA_PAGES_ARRAY, pages);\n\tif (ret)\n\t\tgoto err_out;\n\treturn 0;  \nerr_out:\n\tfree_idx = i;\n\tfor (i = 0; i < free_idx; i++) {\n\t\tif (PageHighMem(pages[i]))\n\t\t\tcontinue;\n\t\tstart = page_to_pfn(pages[i]) << PAGE_SHIFT;\n\t\tend = start + PAGE_SIZE;\n\t\tmemtype_free(start, end);\n\t}\n\treturn -EINVAL;\n}\n\nint set_pages_array_uc(struct page **pages, int numpages)\n{\n\treturn _set_pages_array(pages, numpages, _PAGE_CACHE_MODE_UC_MINUS);\n}\nEXPORT_SYMBOL(set_pages_array_uc);\n\nint set_pages_array_wc(struct page **pages, int numpages)\n{\n\treturn _set_pages_array(pages, numpages, _PAGE_CACHE_MODE_WC);\n}\nEXPORT_SYMBOL(set_pages_array_wc);\n\nint set_pages_wb(struct page *page, int numpages)\n{\n\tunsigned long addr = (unsigned long)page_address(page);\n\n\treturn set_memory_wb(addr, numpages);\n}\nEXPORT_SYMBOL(set_pages_wb);\n\nint set_pages_array_wb(struct page **pages, int numpages)\n{\n\tint retval;\n\tunsigned long start;\n\tunsigned long end;\n\tint i;\n\n\t \n\tretval = cpa_clear_pages_array(pages, numpages,\n\t\t\t__pgprot(_PAGE_CACHE_MASK));\n\tif (retval)\n\t\treturn retval;\n\n\tfor (i = 0; i < numpages; i++) {\n\t\tif (PageHighMem(pages[i]))\n\t\t\tcontinue;\n\t\tstart = page_to_pfn(pages[i]) << PAGE_SHIFT;\n\t\tend = start + PAGE_SIZE;\n\t\tmemtype_free(start, end);\n\t}\n\n\treturn 0;\n}\nEXPORT_SYMBOL(set_pages_array_wb);\n\nint set_pages_ro(struct page *page, int numpages)\n{\n\tunsigned long addr = (unsigned long)page_address(page);\n\n\treturn set_memory_ro(addr, numpages);\n}\n\nint set_pages_rw(struct page *page, int numpages)\n{\n\tunsigned long addr = (unsigned long)page_address(page);\n\n\treturn set_memory_rw(addr, numpages);\n}\n\nstatic int __set_pages_p(struct page *page, int numpages)\n{\n\tunsigned long tempaddr = (unsigned long) page_address(page);\n\tstruct cpa_data cpa = { .vaddr = &tempaddr,\n\t\t\t\t.pgd = NULL,\n\t\t\t\t.numpages = numpages,\n\t\t\t\t.mask_set = __pgprot(_PAGE_PRESENT | _PAGE_RW),\n\t\t\t\t.mask_clr = __pgprot(0),\n\t\t\t\t.flags = CPA_NO_CHECK_ALIAS };\n\n\t \n\treturn __change_page_attr_set_clr(&cpa, 1);\n}\n\nstatic int __set_pages_np(struct page *page, int numpages)\n{\n\tunsigned long tempaddr = (unsigned long) page_address(page);\n\tstruct cpa_data cpa = { .vaddr = &tempaddr,\n\t\t\t\t.pgd = NULL,\n\t\t\t\t.numpages = numpages,\n\t\t\t\t.mask_set = __pgprot(0),\n\t\t\t\t.mask_clr = __pgprot(_PAGE_PRESENT | _PAGE_RW),\n\t\t\t\t.flags = CPA_NO_CHECK_ALIAS };\n\n\t \n\treturn __change_page_attr_set_clr(&cpa, 1);\n}\n\nint set_direct_map_invalid_noflush(struct page *page)\n{\n\treturn __set_pages_np(page, 1);\n}\n\nint set_direct_map_default_noflush(struct page *page)\n{\n\treturn __set_pages_p(page, 1);\n}\n\n#ifdef CONFIG_DEBUG_PAGEALLOC\nvoid __kernel_map_pages(struct page *page, int numpages, int enable)\n{\n\tif (PageHighMem(page))\n\t\treturn;\n\tif (!enable) {\n\t\tdebug_check_no_locks_freed(page_address(page),\n\t\t\t\t\t   numpages * PAGE_SIZE);\n\t}\n\n\t \n\tif (enable)\n\t\t__set_pages_p(page, numpages);\n\telse\n\t\t__set_pages_np(page, numpages);\n\n\t \n\tpreempt_disable();\n\t__flush_tlb_all();\n\tpreempt_enable();\n\n\tarch_flush_lazy_mmu_mode();\n}\n#endif  \n\nbool kernel_page_present(struct page *page)\n{\n\tunsigned int level;\n\tpte_t *pte;\n\n\tif (PageHighMem(page))\n\t\treturn false;\n\n\tpte = lookup_address((unsigned long)page_address(page), &level);\n\treturn (pte_val(*pte) & _PAGE_PRESENT);\n}\n\nint __init kernel_map_pages_in_pgd(pgd_t *pgd, u64 pfn, unsigned long address,\n\t\t\t\t   unsigned numpages, unsigned long page_flags)\n{\n\tint retval = -EINVAL;\n\n\tstruct cpa_data cpa = {\n\t\t.vaddr = &address,\n\t\t.pfn = pfn,\n\t\t.pgd = pgd,\n\t\t.numpages = numpages,\n\t\t.mask_set = __pgprot(0),\n\t\t.mask_clr = __pgprot(~page_flags & (_PAGE_NX|_PAGE_RW)),\n\t\t.flags = CPA_NO_CHECK_ALIAS,\n\t};\n\n\tWARN_ONCE(num_online_cpus() > 1, \"Don't call after initializing SMP\");\n\n\tif (!(__supported_pte_mask & _PAGE_NX))\n\t\tgoto out;\n\n\tif (!(page_flags & _PAGE_ENC))\n\t\tcpa.mask_clr = pgprot_encrypted(cpa.mask_clr);\n\n\tcpa.mask_set = __pgprot(_PAGE_PRESENT | page_flags);\n\n\tretval = __change_page_attr_set_clr(&cpa, 1);\n\t__flush_tlb_all();\n\nout:\n\treturn retval;\n}\n\n \nint __init kernel_unmap_pages_in_pgd(pgd_t *pgd, unsigned long address,\n\t\t\t\t     unsigned long numpages)\n{\n\tint retval;\n\n\t \n\tstruct cpa_data cpa = {\n\t\t.vaddr\t\t= &address,\n\t\t.pfn\t\t= 0,\n\t\t.pgd\t\t= pgd,\n\t\t.numpages\t= numpages,\n\t\t.mask_set\t= __pgprot(0),\n\t\t.mask_clr\t= __pgprot(_PAGE_PRESENT | _PAGE_RW),\n\t\t.flags\t\t= CPA_NO_CHECK_ALIAS,\n\t};\n\n\tWARN_ONCE(num_online_cpus() > 1, \"Don't call after initializing SMP\");\n\n\tretval = __change_page_attr_set_clr(&cpa, 1);\n\t__flush_tlb_all();\n\n\treturn retval;\n}\n\n \n#ifdef CONFIG_CPA_DEBUG\n#include \"cpa-test.c\"\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}