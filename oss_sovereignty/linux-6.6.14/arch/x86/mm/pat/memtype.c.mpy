{
  "module_name": "memtype.c",
  "hash_id": "24973f2c32830d67f071a4de0c4c1481ecadd2535219a9f61e528457ba44d1f6",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/mm/pat/memtype.c",
  "human_readable_source": "\n \n\n#include <linux/seq_file.h>\n#include <linux/memblock.h>\n#include <linux/debugfs.h>\n#include <linux/ioport.h>\n#include <linux/kernel.h>\n#include <linux/pfn_t.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n#include <linux/fs.h>\n#include <linux/rbtree.h>\n\n#include <asm/cacheflush.h>\n#include <asm/cacheinfo.h>\n#include <asm/processor.h>\n#include <asm/tlbflush.h>\n#include <asm/x86_init.h>\n#include <asm/fcntl.h>\n#include <asm/e820/api.h>\n#include <asm/mtrr.h>\n#include <asm/page.h>\n#include <asm/msr.h>\n#include <asm/memtype.h>\n#include <asm/io.h>\n\n#include \"memtype.h\"\n#include \"../mm_internal.h\"\n\n#undef pr_fmt\n#define pr_fmt(fmt) \"\" fmt\n\nstatic bool __read_mostly pat_disabled = !IS_ENABLED(CONFIG_X86_PAT);\nstatic u64 __ro_after_init pat_msr_val;\n\n \nstatic void __init pat_disable(const char *msg_reason)\n{\n\tif (pat_disabled)\n\t\treturn;\n\n\tpat_disabled = true;\n\tpr_info(\"x86/PAT: %s\\n\", msg_reason);\n\n\tmemory_caching_control &= ~CACHE_PAT;\n}\n\nstatic int __init nopat(char *str)\n{\n\tpat_disable(\"PAT support disabled via boot option.\");\n\treturn 0;\n}\nearly_param(\"nopat\", nopat);\n\nbool pat_enabled(void)\n{\n\treturn !pat_disabled;\n}\nEXPORT_SYMBOL_GPL(pat_enabled);\n\nint pat_debug_enable;\n\nstatic int __init pat_debug_setup(char *str)\n{\n\tpat_debug_enable = 1;\n\treturn 1;\n}\n__setup(\"debugpat\", pat_debug_setup);\n\n#ifdef CONFIG_X86_PAT\n \n\n#define _PGMT_WB\t\t0\n#define _PGMT_WC\t\t(1UL << PG_arch_1)\n#define _PGMT_UC_MINUS\t\t(1UL << PG_uncached)\n#define _PGMT_WT\t\t(1UL << PG_uncached | 1UL << PG_arch_1)\n#define _PGMT_MASK\t\t(1UL << PG_uncached | 1UL << PG_arch_1)\n#define _PGMT_CLEAR_MASK\t(~_PGMT_MASK)\n\nstatic inline enum page_cache_mode get_page_memtype(struct page *pg)\n{\n\tunsigned long pg_flags = pg->flags & _PGMT_MASK;\n\n\tif (pg_flags == _PGMT_WB)\n\t\treturn _PAGE_CACHE_MODE_WB;\n\telse if (pg_flags == _PGMT_WC)\n\t\treturn _PAGE_CACHE_MODE_WC;\n\telse if (pg_flags == _PGMT_UC_MINUS)\n\t\treturn _PAGE_CACHE_MODE_UC_MINUS;\n\telse\n\t\treturn _PAGE_CACHE_MODE_WT;\n}\n\nstatic inline void set_page_memtype(struct page *pg,\n\t\t\t\t    enum page_cache_mode memtype)\n{\n\tunsigned long memtype_flags;\n\tunsigned long old_flags;\n\tunsigned long new_flags;\n\n\tswitch (memtype) {\n\tcase _PAGE_CACHE_MODE_WC:\n\t\tmemtype_flags = _PGMT_WC;\n\t\tbreak;\n\tcase _PAGE_CACHE_MODE_UC_MINUS:\n\t\tmemtype_flags = _PGMT_UC_MINUS;\n\t\tbreak;\n\tcase _PAGE_CACHE_MODE_WT:\n\t\tmemtype_flags = _PGMT_WT;\n\t\tbreak;\n\tcase _PAGE_CACHE_MODE_WB:\n\tdefault:\n\t\tmemtype_flags = _PGMT_WB;\n\t\tbreak;\n\t}\n\n\told_flags = READ_ONCE(pg->flags);\n\tdo {\n\t\tnew_flags = (old_flags & _PGMT_CLEAR_MASK) | memtype_flags;\n\t} while (!try_cmpxchg(&pg->flags, &old_flags, new_flags));\n}\n#else\nstatic inline enum page_cache_mode get_page_memtype(struct page *pg)\n{\n\treturn -1;\n}\nstatic inline void set_page_memtype(struct page *pg,\n\t\t\t\t    enum page_cache_mode memtype)\n{\n}\n#endif\n\nenum {\n\tPAT_UC = 0,\t\t \n\tPAT_WC = 1,\t\t \n\tPAT_WT = 4,\t\t \n\tPAT_WP = 5,\t\t \n\tPAT_WB = 6,\t\t \n\tPAT_UC_MINUS = 7,\t \n};\n\n#define CM(c) (_PAGE_CACHE_MODE_ ## c)\n\nstatic enum page_cache_mode __init pat_get_cache_mode(unsigned int pat_val,\n\t\t\t\t\t\t      char *msg)\n{\n\tenum page_cache_mode cache;\n\tchar *cache_mode;\n\n\tswitch (pat_val) {\n\tcase PAT_UC:       cache = CM(UC);       cache_mode = \"UC  \"; break;\n\tcase PAT_WC:       cache = CM(WC);       cache_mode = \"WC  \"; break;\n\tcase PAT_WT:       cache = CM(WT);       cache_mode = \"WT  \"; break;\n\tcase PAT_WP:       cache = CM(WP);       cache_mode = \"WP  \"; break;\n\tcase PAT_WB:       cache = CM(WB);       cache_mode = \"WB  \"; break;\n\tcase PAT_UC_MINUS: cache = CM(UC_MINUS); cache_mode = \"UC- \"; break;\n\tdefault:           cache = CM(WB);       cache_mode = \"WB  \"; break;\n\t}\n\n\tmemcpy(msg, cache_mode, 4);\n\n\treturn cache;\n}\n\n#undef CM\n\n \nstatic void __init init_cache_modes(u64 pat)\n{\n\tenum page_cache_mode cache;\n\tchar pat_msg[33];\n\tint i;\n\n\tpat_msg[32] = 0;\n\tfor (i = 7; i >= 0; i--) {\n\t\tcache = pat_get_cache_mode((pat >> (i * 8)) & 7,\n\t\t\t\t\t   pat_msg + 4 * i);\n\t\tupdate_cache_mode_entry(i, cache);\n\t}\n\tpr_info(\"x86/PAT: Configuration [0-7]: %s\\n\", pat_msg);\n}\n\nvoid pat_cpu_init(void)\n{\n\tif (!boot_cpu_has(X86_FEATURE_PAT)) {\n\t\t \n\t\tpanic(\"x86/PAT: PAT enabled, but not supported by secondary CPU\\n\");\n\t}\n\n\twrmsrl(MSR_IA32_CR_PAT, pat_msr_val);\n}\n\n \nvoid __init pat_bp_init(void)\n{\n\tstruct cpuinfo_x86 *c = &boot_cpu_data;\n#define PAT(p0, p1, p2, p3, p4, p5, p6, p7)\t\t\t\\\n\t(((u64)PAT_ ## p0) | ((u64)PAT_ ## p1 << 8) |\t\t\\\n\t((u64)PAT_ ## p2 << 16) | ((u64)PAT_ ## p3 << 24) |\t\\\n\t((u64)PAT_ ## p4 << 32) | ((u64)PAT_ ## p5 << 40) |\t\\\n\t((u64)PAT_ ## p6 << 48) | ((u64)PAT_ ## p7 << 56))\n\n\n\tif (!IS_ENABLED(CONFIG_X86_PAT))\n\t\tpr_info_once(\"x86/PAT: PAT support disabled because CONFIG_X86_PAT is disabled in the kernel.\\n\");\n\n\tif (!cpu_feature_enabled(X86_FEATURE_PAT))\n\t\tpat_disable(\"PAT not supported by the CPU.\");\n\telse\n\t\trdmsrl(MSR_IA32_CR_PAT, pat_msr_val);\n\n\tif (!pat_msr_val) {\n\t\tpat_disable(\"PAT support disabled by the firmware.\");\n\n\t\t \n\t\tpat_msr_val = PAT(WB, WT, UC_MINUS, UC, WB, WT, UC_MINUS, UC);\n\t}\n\n\t \n\tif (pat_disabled ||\n\t    cpu_feature_enabled(X86_FEATURE_XENPV) ||\n\t    cpu_feature_enabled(X86_FEATURE_TDX_GUEST)) {\n\t\tinit_cache_modes(pat_msr_val);\n\t\treturn;\n\t}\n\n\tif ((c->x86_vendor == X86_VENDOR_INTEL) &&\n\t    (((c->x86 == 0x6) && (c->x86_model <= 0xd)) ||\n\t     ((c->x86 == 0xf) && (c->x86_model <= 0x6)))) {\n\t\t \n\t\tpat_msr_val = PAT(WB, WC, UC_MINUS, UC, WB, WC, UC_MINUS, UC);\n\t} else {\n\t\t \n\t\tpat_msr_val = PAT(WB, WC, UC_MINUS, UC, WB, WP, UC_MINUS, WT);\n\t}\n\n\tmemory_caching_control |= CACHE_PAT;\n\n\tinit_cache_modes(pat_msr_val);\n#undef PAT\n}\n\nstatic DEFINE_SPINLOCK(memtype_lock);\t \n\n \nstatic unsigned long pat_x_mtrr_type(u64 start, u64 end,\n\t\t\t\t     enum page_cache_mode req_type)\n{\n\t \n\tif (req_type == _PAGE_CACHE_MODE_WB) {\n\t\tu8 mtrr_type, uniform;\n\n\t\tmtrr_type = mtrr_type_lookup(start, end, &uniform);\n\t\tif (mtrr_type != MTRR_TYPE_WRBACK)\n\t\t\treturn _PAGE_CACHE_MODE_UC_MINUS;\n\n\t\treturn _PAGE_CACHE_MODE_WB;\n\t}\n\n\treturn req_type;\n}\n\nstruct pagerange_state {\n\tunsigned long\t\tcur_pfn;\n\tint\t\t\tram;\n\tint\t\t\tnot_ram;\n};\n\nstatic int\npagerange_is_ram_callback(unsigned long initial_pfn, unsigned long total_nr_pages, void *arg)\n{\n\tstruct pagerange_state *state = arg;\n\n\tstate->not_ram\t|= initial_pfn > state->cur_pfn;\n\tstate->ram\t|= total_nr_pages > 0;\n\tstate->cur_pfn\t = initial_pfn + total_nr_pages;\n\n\treturn state->ram && state->not_ram;\n}\n\nstatic int pat_pagerange_is_ram(resource_size_t start, resource_size_t end)\n{\n\tint ret = 0;\n\tunsigned long start_pfn = start >> PAGE_SHIFT;\n\tunsigned long end_pfn = (end + PAGE_SIZE - 1) >> PAGE_SHIFT;\n\tstruct pagerange_state state = {start_pfn, 0, 0};\n\n\t \n\tif (start_pfn < ISA_END_ADDRESS >> PAGE_SHIFT)\n\t\tstart_pfn = ISA_END_ADDRESS >> PAGE_SHIFT;\n\n\tif (start_pfn < end_pfn) {\n\t\tret = walk_system_ram_range(start_pfn, end_pfn - start_pfn,\n\t\t\t\t&state, pagerange_is_ram_callback);\n\t}\n\n\treturn (ret > 0) ? -1 : (state.ram ? 1 : 0);\n}\n\n \nstatic int reserve_ram_pages_type(u64 start, u64 end,\n\t\t\t\t  enum page_cache_mode req_type,\n\t\t\t\t  enum page_cache_mode *new_type)\n{\n\tstruct page *page;\n\tu64 pfn;\n\n\tif (req_type == _PAGE_CACHE_MODE_WP) {\n\t\tif (new_type)\n\t\t\t*new_type = _PAGE_CACHE_MODE_UC_MINUS;\n\t\treturn -EINVAL;\n\t}\n\n\tif (req_type == _PAGE_CACHE_MODE_UC) {\n\t\t \n\t\tWARN_ON_ONCE(1);\n\t\treq_type = _PAGE_CACHE_MODE_UC_MINUS;\n\t}\n\n\tfor (pfn = (start >> PAGE_SHIFT); pfn < (end >> PAGE_SHIFT); ++pfn) {\n\t\tenum page_cache_mode type;\n\n\t\tpage = pfn_to_page(pfn);\n\t\ttype = get_page_memtype(page);\n\t\tif (type != _PAGE_CACHE_MODE_WB) {\n\t\t\tpr_info(\"x86/PAT: reserve_ram_pages_type failed [mem %#010Lx-%#010Lx], track 0x%x, req 0x%x\\n\",\n\t\t\t\tstart, end - 1, type, req_type);\n\t\t\tif (new_type)\n\t\t\t\t*new_type = type;\n\n\t\t\treturn -EBUSY;\n\t\t}\n\t}\n\n\tif (new_type)\n\t\t*new_type = req_type;\n\n\tfor (pfn = (start >> PAGE_SHIFT); pfn < (end >> PAGE_SHIFT); ++pfn) {\n\t\tpage = pfn_to_page(pfn);\n\t\tset_page_memtype(page, req_type);\n\t}\n\treturn 0;\n}\n\nstatic int free_ram_pages_type(u64 start, u64 end)\n{\n\tstruct page *page;\n\tu64 pfn;\n\n\tfor (pfn = (start >> PAGE_SHIFT); pfn < (end >> PAGE_SHIFT); ++pfn) {\n\t\tpage = pfn_to_page(pfn);\n\t\tset_page_memtype(page, _PAGE_CACHE_MODE_WB);\n\t}\n\treturn 0;\n}\n\nstatic u64 sanitize_phys(u64 address)\n{\n\t \n\tif (IS_ENABLED(CONFIG_X86_64))\n\t\treturn address & __PHYSICAL_MASK;\n\treturn address;\n}\n\n \nint memtype_reserve(u64 start, u64 end, enum page_cache_mode req_type,\n\t\t    enum page_cache_mode *new_type)\n{\n\tstruct memtype *entry_new;\n\tenum page_cache_mode actual_type;\n\tint is_range_ram;\n\tint err = 0;\n\n\tstart = sanitize_phys(start);\n\n\t \n\tend = sanitize_phys(end - 1) + 1;\n\tif (start >= end) {\n\t\tWARN(1, \"%s failed: [mem %#010Lx-%#010Lx], req %s\\n\", __func__,\n\t\t\t\tstart, end - 1, cattr_name(req_type));\n\t\treturn -EINVAL;\n\t}\n\n\tif (!pat_enabled()) {\n\t\t \n\t\tif (new_type)\n\t\t\t*new_type = req_type;\n\t\treturn 0;\n\t}\n\n\t \n\tif (x86_platform.is_untracked_pat_range(start, end)) {\n\t\tif (new_type)\n\t\t\t*new_type = _PAGE_CACHE_MODE_WB;\n\t\treturn 0;\n\t}\n\n\t \n\tactual_type = pat_x_mtrr_type(start, end, req_type);\n\n\tif (new_type)\n\t\t*new_type = actual_type;\n\n\tis_range_ram = pat_pagerange_is_ram(start, end);\n\tif (is_range_ram == 1) {\n\n\t\terr = reserve_ram_pages_type(start, end, req_type, new_type);\n\n\t\treturn err;\n\t} else if (is_range_ram < 0) {\n\t\treturn -EINVAL;\n\t}\n\n\tentry_new = kzalloc(sizeof(struct memtype), GFP_KERNEL);\n\tif (!entry_new)\n\t\treturn -ENOMEM;\n\n\tentry_new->start = start;\n\tentry_new->end\t = end;\n\tentry_new->type\t = actual_type;\n\n\tspin_lock(&memtype_lock);\n\n\terr = memtype_check_insert(entry_new, new_type);\n\tif (err) {\n\t\tpr_info(\"x86/PAT: memtype_reserve failed [mem %#010Lx-%#010Lx], track %s, req %s\\n\",\n\t\t\tstart, end - 1,\n\t\t\tcattr_name(entry_new->type), cattr_name(req_type));\n\t\tkfree(entry_new);\n\t\tspin_unlock(&memtype_lock);\n\n\t\treturn err;\n\t}\n\n\tspin_unlock(&memtype_lock);\n\n\tdprintk(\"memtype_reserve added [mem %#010Lx-%#010Lx], track %s, req %s, ret %s\\n\",\n\t\tstart, end - 1, cattr_name(entry_new->type), cattr_name(req_type),\n\t\tnew_type ? cattr_name(*new_type) : \"-\");\n\n\treturn err;\n}\n\nint memtype_free(u64 start, u64 end)\n{\n\tint is_range_ram;\n\tstruct memtype *entry_old;\n\n\tif (!pat_enabled())\n\t\treturn 0;\n\n\tstart = sanitize_phys(start);\n\tend = sanitize_phys(end);\n\n\t \n\tif (x86_platform.is_untracked_pat_range(start, end))\n\t\treturn 0;\n\n\tis_range_ram = pat_pagerange_is_ram(start, end);\n\tif (is_range_ram == 1)\n\t\treturn free_ram_pages_type(start, end);\n\tif (is_range_ram < 0)\n\t\treturn -EINVAL;\n\n\tspin_lock(&memtype_lock);\n\tentry_old = memtype_erase(start, end);\n\tspin_unlock(&memtype_lock);\n\n\tif (IS_ERR(entry_old)) {\n\t\tpr_info(\"x86/PAT: %s:%d freeing invalid memtype [mem %#010Lx-%#010Lx]\\n\",\n\t\t\tcurrent->comm, current->pid, start, end - 1);\n\t\treturn -EINVAL;\n\t}\n\n\tkfree(entry_old);\n\n\tdprintk(\"memtype_free request [mem %#010Lx-%#010Lx]\\n\", start, end - 1);\n\n\treturn 0;\n}\n\n\n \nstatic enum page_cache_mode lookup_memtype(u64 paddr)\n{\n\tenum page_cache_mode rettype = _PAGE_CACHE_MODE_WB;\n\tstruct memtype *entry;\n\n\tif (x86_platform.is_untracked_pat_range(paddr, paddr + PAGE_SIZE))\n\t\treturn rettype;\n\n\tif (pat_pagerange_is_ram(paddr, paddr + PAGE_SIZE)) {\n\t\tstruct page *page;\n\n\t\tpage = pfn_to_page(paddr >> PAGE_SHIFT);\n\t\treturn get_page_memtype(page);\n\t}\n\n\tspin_lock(&memtype_lock);\n\n\tentry = memtype_lookup(paddr);\n\tif (entry != NULL)\n\t\trettype = entry->type;\n\telse\n\t\trettype = _PAGE_CACHE_MODE_UC_MINUS;\n\n\tspin_unlock(&memtype_lock);\n\n\treturn rettype;\n}\n\n \nbool pat_pfn_immune_to_uc_mtrr(unsigned long pfn)\n{\n\tenum page_cache_mode cm = lookup_memtype(PFN_PHYS(pfn));\n\n\treturn cm == _PAGE_CACHE_MODE_UC ||\n\t       cm == _PAGE_CACHE_MODE_UC_MINUS ||\n\t       cm == _PAGE_CACHE_MODE_WC;\n}\nEXPORT_SYMBOL_GPL(pat_pfn_immune_to_uc_mtrr);\n\n \nint memtype_reserve_io(resource_size_t start, resource_size_t end,\n\t\t\tenum page_cache_mode *type)\n{\n\tresource_size_t size = end - start;\n\tenum page_cache_mode req_type = *type;\n\tenum page_cache_mode new_type;\n\tint ret;\n\n\tWARN_ON_ONCE(iomem_map_sanity_check(start, size));\n\n\tret = memtype_reserve(start, end, req_type, &new_type);\n\tif (ret)\n\t\tgoto out_err;\n\n\tif (!is_new_memtype_allowed(start, size, req_type, new_type))\n\t\tgoto out_free;\n\n\tif (memtype_kernel_map_sync(start, size, new_type) < 0)\n\t\tgoto out_free;\n\n\t*type = new_type;\n\treturn 0;\n\nout_free:\n\tmemtype_free(start, end);\n\tret = -EBUSY;\nout_err:\n\treturn ret;\n}\n\n \nvoid memtype_free_io(resource_size_t start, resource_size_t end)\n{\n\tmemtype_free(start, end);\n}\n\n#ifdef CONFIG_X86_PAT\nint arch_io_reserve_memtype_wc(resource_size_t start, resource_size_t size)\n{\n\tenum page_cache_mode type = _PAGE_CACHE_MODE_WC;\n\n\treturn memtype_reserve_io(start, start + size, &type);\n}\nEXPORT_SYMBOL(arch_io_reserve_memtype_wc);\n\nvoid arch_io_free_memtype_wc(resource_size_t start, resource_size_t size)\n{\n\tmemtype_free_io(start, start + size);\n}\nEXPORT_SYMBOL(arch_io_free_memtype_wc);\n#endif\n\npgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,\n\t\t\t\tunsigned long size, pgprot_t vma_prot)\n{\n\tif (!phys_mem_access_encrypted(pfn << PAGE_SHIFT, size))\n\t\tvma_prot = pgprot_decrypted(vma_prot);\n\n\treturn vma_prot;\n}\n\n#ifdef CONFIG_STRICT_DEVMEM\n \nstatic inline int range_is_allowed(unsigned long pfn, unsigned long size)\n{\n\treturn 1;\n}\n#else\n \nstatic inline int range_is_allowed(unsigned long pfn, unsigned long size)\n{\n\tu64 from = ((u64)pfn) << PAGE_SHIFT;\n\tu64 to = from + size;\n\tu64 cursor = from;\n\n\tif (!pat_enabled())\n\t\treturn 1;\n\n\twhile (cursor < to) {\n\t\tif (!devmem_is_allowed(pfn))\n\t\t\treturn 0;\n\t\tcursor += PAGE_SIZE;\n\t\tpfn++;\n\t}\n\treturn 1;\n}\n#endif  \n\nint phys_mem_access_prot_allowed(struct file *file, unsigned long pfn,\n\t\t\t\tunsigned long size, pgprot_t *vma_prot)\n{\n\tenum page_cache_mode pcm = _PAGE_CACHE_MODE_WB;\n\n\tif (!range_is_allowed(pfn, size))\n\t\treturn 0;\n\n\tif (file->f_flags & O_DSYNC)\n\t\tpcm = _PAGE_CACHE_MODE_UC_MINUS;\n\n\t*vma_prot = __pgprot((pgprot_val(*vma_prot) & ~_PAGE_CACHE_MASK) |\n\t\t\t     cachemode2protval(pcm));\n\treturn 1;\n}\n\n \nint memtype_kernel_map_sync(u64 base, unsigned long size,\n\t\t\t    enum page_cache_mode pcm)\n{\n\tunsigned long id_sz;\n\n\tif (base > __pa(high_memory-1))\n\t\treturn 0;\n\n\t \n\tif (!page_is_ram(base >> PAGE_SHIFT))\n\t\treturn 0;\n\n\tid_sz = (__pa(high_memory-1) <= base + size) ?\n\t\t\t\t__pa(high_memory) - base : size;\n\n\tif (ioremap_change_attr((unsigned long)__va(base), id_sz, pcm) < 0) {\n\t\tpr_info(\"x86/PAT: %s:%d ioremap_change_attr failed %s for [mem %#010Lx-%#010Lx]\\n\",\n\t\t\tcurrent->comm, current->pid,\n\t\t\tcattr_name(pcm),\n\t\t\tbase, (unsigned long long)(base + size-1));\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n \nstatic int reserve_pfn_range(u64 paddr, unsigned long size, pgprot_t *vma_prot,\n\t\t\t\tint strict_prot)\n{\n\tint is_ram = 0;\n\tint ret;\n\tenum page_cache_mode want_pcm = pgprot2cachemode(*vma_prot);\n\tenum page_cache_mode pcm = want_pcm;\n\n\tis_ram = pat_pagerange_is_ram(paddr, paddr + size);\n\n\t \n\tif (is_ram) {\n\t\tif (!pat_enabled())\n\t\t\treturn 0;\n\n\t\tpcm = lookup_memtype(paddr);\n\t\tif (want_pcm != pcm) {\n\t\t\tpr_warn(\"x86/PAT: %s:%d map pfn RAM range req %s for [mem %#010Lx-%#010Lx], got %s\\n\",\n\t\t\t\tcurrent->comm, current->pid,\n\t\t\t\tcattr_name(want_pcm),\n\t\t\t\t(unsigned long long)paddr,\n\t\t\t\t(unsigned long long)(paddr + size - 1),\n\t\t\t\tcattr_name(pcm));\n\t\t\t*vma_prot = __pgprot((pgprot_val(*vma_prot) &\n\t\t\t\t\t     (~_PAGE_CACHE_MASK)) |\n\t\t\t\t\t     cachemode2protval(pcm));\n\t\t}\n\t\treturn 0;\n\t}\n\n\tret = memtype_reserve(paddr, paddr + size, want_pcm, &pcm);\n\tif (ret)\n\t\treturn ret;\n\n\tif (pcm != want_pcm) {\n\t\tif (strict_prot ||\n\t\t    !is_new_memtype_allowed(paddr, size, want_pcm, pcm)) {\n\t\t\tmemtype_free(paddr, paddr + size);\n\t\t\tpr_err(\"x86/PAT: %s:%d map pfn expected mapping type %s for [mem %#010Lx-%#010Lx], got %s\\n\",\n\t\t\t       current->comm, current->pid,\n\t\t\t       cattr_name(want_pcm),\n\t\t\t       (unsigned long long)paddr,\n\t\t\t       (unsigned long long)(paddr + size - 1),\n\t\t\t       cattr_name(pcm));\n\t\t\treturn -EINVAL;\n\t\t}\n\t\t \n\t\t*vma_prot = __pgprot((pgprot_val(*vma_prot) &\n\t\t\t\t      (~_PAGE_CACHE_MASK)) |\n\t\t\t\t     cachemode2protval(pcm));\n\t}\n\n\tif (memtype_kernel_map_sync(paddr, size, pcm) < 0) {\n\t\tmemtype_free(paddr, paddr + size);\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\n \nstatic void free_pfn_range(u64 paddr, unsigned long size)\n{\n\tint is_ram;\n\n\tis_ram = pat_pagerange_is_ram(paddr, paddr + size);\n\tif (is_ram == 0)\n\t\tmemtype_free(paddr, paddr + size);\n}\n\n \nint track_pfn_copy(struct vm_area_struct *vma)\n{\n\tresource_size_t paddr;\n\tunsigned long prot;\n\tunsigned long vma_size = vma->vm_end - vma->vm_start;\n\tpgprot_t pgprot;\n\n\tif (vma->vm_flags & VM_PAT) {\n\t\t \n\t\tif (follow_phys(vma, vma->vm_start, 0, &prot, &paddr)) {\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tpgprot = __pgprot(prot);\n\t\treturn reserve_pfn_range(paddr, vma_size, &pgprot, 1);\n\t}\n\n\treturn 0;\n}\n\n \nint track_pfn_remap(struct vm_area_struct *vma, pgprot_t *prot,\n\t\t    unsigned long pfn, unsigned long addr, unsigned long size)\n{\n\tresource_size_t paddr = (resource_size_t)pfn << PAGE_SHIFT;\n\tenum page_cache_mode pcm;\n\n\t \n\tif (!vma || (addr == vma->vm_start\n\t\t\t\t&& size == (vma->vm_end - vma->vm_start))) {\n\t\tint ret;\n\n\t\tret = reserve_pfn_range(paddr, size, prot, 0);\n\t\tif (ret == 0 && vma)\n\t\t\tvm_flags_set(vma, VM_PAT);\n\t\treturn ret;\n\t}\n\n\tif (!pat_enabled())\n\t\treturn 0;\n\n\t \n\tpcm = lookup_memtype(paddr);\n\n\t \n\twhile (size > PAGE_SIZE) {\n\t\tsize -= PAGE_SIZE;\n\t\tpaddr += PAGE_SIZE;\n\t\tif (pcm != lookup_memtype(paddr))\n\t\t\treturn -EINVAL;\n\t}\n\n\t*prot = __pgprot((pgprot_val(*prot) & (~_PAGE_CACHE_MASK)) |\n\t\t\t cachemode2protval(pcm));\n\n\treturn 0;\n}\n\nvoid track_pfn_insert(struct vm_area_struct *vma, pgprot_t *prot, pfn_t pfn)\n{\n\tenum page_cache_mode pcm;\n\n\tif (!pat_enabled())\n\t\treturn;\n\n\t \n\tpcm = lookup_memtype(pfn_t_to_phys(pfn));\n\t*prot = __pgprot((pgprot_val(*prot) & (~_PAGE_CACHE_MASK)) |\n\t\t\t cachemode2protval(pcm));\n}\n\n \nvoid untrack_pfn(struct vm_area_struct *vma, unsigned long pfn,\n\t\t unsigned long size, bool mm_wr_locked)\n{\n\tresource_size_t paddr;\n\tunsigned long prot;\n\n\tif (vma && !(vma->vm_flags & VM_PAT))\n\t\treturn;\n\n\t \n\tpaddr = (resource_size_t)pfn << PAGE_SHIFT;\n\tif (!paddr && !size) {\n\t\tif (follow_phys(vma, vma->vm_start, 0, &prot, &paddr)) {\n\t\t\tWARN_ON_ONCE(1);\n\t\t\treturn;\n\t\t}\n\n\t\tsize = vma->vm_end - vma->vm_start;\n\t}\n\tfree_pfn_range(paddr, size);\n\tif (vma) {\n\t\tif (mm_wr_locked)\n\t\t\tvm_flags_clear(vma, VM_PAT);\n\t\telse\n\t\t\t__vm_flags_mod(vma, 0, VM_PAT);\n\t}\n}\n\n \nvoid untrack_pfn_clear(struct vm_area_struct *vma)\n{\n\tvm_flags_clear(vma, VM_PAT);\n}\n\npgprot_t pgprot_writecombine(pgprot_t prot)\n{\n\treturn __pgprot(pgprot_val(prot) |\n\t\t\t\tcachemode2protval(_PAGE_CACHE_MODE_WC));\n}\nEXPORT_SYMBOL_GPL(pgprot_writecombine);\n\npgprot_t pgprot_writethrough(pgprot_t prot)\n{\n\treturn __pgprot(pgprot_val(prot) |\n\t\t\t\tcachemode2protval(_PAGE_CACHE_MODE_WT));\n}\nEXPORT_SYMBOL_GPL(pgprot_writethrough);\n\n#if defined(CONFIG_DEBUG_FS) && defined(CONFIG_X86_PAT)\n\n \nstatic struct memtype *memtype_get_idx(loff_t pos)\n{\n\tstruct memtype *entry_print;\n\tint ret;\n\n\tentry_print  = kzalloc(sizeof(struct memtype), GFP_KERNEL);\n\tif (!entry_print)\n\t\treturn NULL;\n\n\tspin_lock(&memtype_lock);\n\tret = memtype_copy_nth_element(entry_print, pos);\n\tspin_unlock(&memtype_lock);\n\n\t \n\tif (ret) {\n\t\tkfree(entry_print);\n\t\treturn NULL;\n\t}\n\n\treturn entry_print;\n}\n\nstatic void *memtype_seq_start(struct seq_file *seq, loff_t *pos)\n{\n\tif (*pos == 0) {\n\t\t++*pos;\n\t\tseq_puts(seq, \"PAT memtype list:\\n\");\n\t}\n\n\treturn memtype_get_idx(*pos);\n}\n\nstatic void *memtype_seq_next(struct seq_file *seq, void *v, loff_t *pos)\n{\n\tkfree(v);\n\t++*pos;\n\treturn memtype_get_idx(*pos);\n}\n\nstatic void memtype_seq_stop(struct seq_file *seq, void *v)\n{\n\tkfree(v);\n}\n\nstatic int memtype_seq_show(struct seq_file *seq, void *v)\n{\n\tstruct memtype *entry_print = (struct memtype *)v;\n\n\tseq_printf(seq, \"PAT: [mem 0x%016Lx-0x%016Lx] %s\\n\",\n\t\t\tentry_print->start,\n\t\t\tentry_print->end,\n\t\t\tcattr_name(entry_print->type));\n\n\treturn 0;\n}\n\nstatic const struct seq_operations memtype_seq_ops = {\n\t.start = memtype_seq_start,\n\t.next  = memtype_seq_next,\n\t.stop  = memtype_seq_stop,\n\t.show  = memtype_seq_show,\n};\n\nstatic int memtype_seq_open(struct inode *inode, struct file *file)\n{\n\treturn seq_open(file, &memtype_seq_ops);\n}\n\nstatic const struct file_operations memtype_fops = {\n\t.open    = memtype_seq_open,\n\t.read    = seq_read,\n\t.llseek  = seq_lseek,\n\t.release = seq_release,\n};\n\nstatic int __init pat_memtype_list_init(void)\n{\n\tif (pat_enabled()) {\n\t\tdebugfs_create_file(\"pat_memtype_list\", S_IRUSR,\n\t\t\t\t    arch_debugfs_dir, NULL, &memtype_fops);\n\t}\n\treturn 0;\n}\nlate_initcall(pat_memtype_list_init);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}