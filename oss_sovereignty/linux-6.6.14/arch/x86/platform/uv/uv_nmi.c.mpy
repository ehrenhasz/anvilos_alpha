{
  "module_name": "uv_nmi.c",
  "hash_id": "c11a918959b089e532e3ac1df012ab505bd1ef6c3e113adac6d4680380870571",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/platform/uv/uv_nmi.c",
  "human_readable_source": "\n \n\n#include <linux/cpu.h>\n#include <linux/delay.h>\n#include <linux/kdb.h>\n#include <linux/kexec.h>\n#include <linux/kgdb.h>\n#include <linux/moduleparam.h>\n#include <linux/nmi.h>\n#include <linux/sched.h>\n#include <linux/sched/debug.h>\n#include <linux/slab.h>\n#include <linux/clocksource.h>\n\n#include <asm/apic.h>\n#include <asm/current.h>\n#include <asm/kdebug.h>\n#include <asm/local64.h>\n#include <asm/nmi.h>\n#include <asm/reboot.h>\n#include <asm/traps.h>\n#include <asm/uv/uv.h>\n#include <asm/uv/uv_hub.h>\n#include <asm/uv/uv_mmrs.h>\n\n \n\nstatic struct uv_hub_nmi_s **uv_hub_nmi_list;\n\nDEFINE_PER_CPU(struct uv_cpu_nmi_s, uv_cpu_nmi);\n\n \nstatic unsigned long uvh_nmi_mmrx;\t\t \nstatic unsigned long uvh_nmi_mmrx_clear;\t \nstatic int uvh_nmi_mmrx_shift;\t\t\t \nstatic char *uvh_nmi_mmrx_type;\t\t\t \n\n \nstatic unsigned long uvh_nmi_mmrx_supported;\t \n\n \nstatic unsigned long uvh_nmi_mmrx_req;\t\t \nstatic int uvh_nmi_mmrx_req_shift;\t\t \n\n \n#define NMI_CONTROL_PORT\t0x70\n#define NMI_DUMMY_PORT\t\t0x71\n#define PAD_OWN_GPP_D_0\t\t0x2c\n#define GPI_NMI_STS_GPP_D_0\t0x164\n#define GPI_NMI_ENA_GPP_D_0\t0x174\n#define STS_GPP_D_0_MASK\t0x1\n#define PAD_CFG_DW0_GPP_D_0\t0x4c0\n#define GPIROUTNMI\t\t(1ul << 17)\n#define PCH_PCR_GPIO_1_BASE\t0xfdae0000ul\n#define PCH_PCR_GPIO_ADDRESS(offset) (int *)((u64)(pch_base) | (u64)(offset))\n\nstatic u64 *pch_base;\nstatic unsigned long nmi_mmr;\nstatic unsigned long nmi_mmr_clear;\nstatic unsigned long nmi_mmr_pending;\n\nstatic atomic_t\tuv_in_nmi;\nstatic atomic_t uv_nmi_cpu = ATOMIC_INIT(-1);\nstatic atomic_t uv_nmi_cpus_in_nmi = ATOMIC_INIT(-1);\nstatic atomic_t uv_nmi_slave_continue;\nstatic cpumask_var_t uv_nmi_cpu_mask;\n\nstatic atomic_t uv_nmi_kexec_failed;\n\n \n#define SLAVE_CLEAR\t0\n#define SLAVE_CONTINUE\t1\n#define SLAVE_EXIT\t2\n\n \nstatic int uv_nmi_loglevel = CONSOLE_LOGLEVEL_DEFAULT;\nmodule_param_named(dump_loglevel, uv_nmi_loglevel, int, 0644);\n\n \nstatic int param_get_local64(char *buffer, const struct kernel_param *kp)\n{\n\treturn sprintf(buffer, \"%lu\\n\", local64_read((local64_t *)kp->arg));\n}\n\nstatic int param_set_local64(const char *val, const struct kernel_param *kp)\n{\n\t \n\tlocal64_set((local64_t *)kp->arg, 0);\n\treturn 0;\n}\n\nstatic const struct kernel_param_ops param_ops_local64 = {\n\t.get = param_get_local64,\n\t.set = param_set_local64,\n};\n#define param_check_local64(name, p) __param_check(name, p, local64_t)\n\nstatic local64_t uv_nmi_count;\nmodule_param_named(nmi_count, uv_nmi_count, local64, 0644);\n\nstatic local64_t uv_nmi_misses;\nmodule_param_named(nmi_misses, uv_nmi_misses, local64, 0644);\n\nstatic local64_t uv_nmi_ping_count;\nmodule_param_named(ping_count, uv_nmi_ping_count, local64, 0644);\n\nstatic local64_t uv_nmi_ping_misses;\nmodule_param_named(ping_misses, uv_nmi_ping_misses, local64, 0644);\n\n \nstatic int uv_nmi_initial_delay = 100;\nmodule_param_named(initial_delay, uv_nmi_initial_delay, int, 0644);\n\nstatic int uv_nmi_slave_delay = 100;\nmodule_param_named(slave_delay, uv_nmi_slave_delay, int, 0644);\n\nstatic int uv_nmi_loop_delay = 100;\nmodule_param_named(loop_delay, uv_nmi_loop_delay, int, 0644);\n\nstatic int uv_nmi_trigger_delay = 10000;\nmodule_param_named(trigger_delay, uv_nmi_trigger_delay, int, 0644);\n\nstatic int uv_nmi_wait_count = 100;\nmodule_param_named(wait_count, uv_nmi_wait_count, int, 0644);\n\nstatic int uv_nmi_retry_count = 500;\nmodule_param_named(retry_count, uv_nmi_retry_count, int, 0644);\n\nstatic bool uv_pch_intr_enable = true;\nstatic bool uv_pch_intr_now_enabled;\nmodule_param_named(pch_intr_enable, uv_pch_intr_enable, bool, 0644);\n\nstatic bool uv_pch_init_enable = true;\nmodule_param_named(pch_init_enable, uv_pch_init_enable, bool, 0644);\n\nstatic int uv_nmi_debug;\nmodule_param_named(debug, uv_nmi_debug, int, 0644);\n\n#define nmi_debug(fmt, ...)\t\t\t\t\\\n\tdo {\t\t\t\t\t\t\\\n\t\tif (uv_nmi_debug)\t\t\t\\\n\t\t\tpr_info(fmt, ##__VA_ARGS__);\t\\\n\t} while (0)\n\n \n#define\tACTION_LEN\t16\nstatic struct nmi_action {\n\tchar\t*action;\n\tchar\t*desc;\n} valid_acts[] = {\n\t{\t\"kdump\",\t\"do kernel crash dump\"\t\t\t},\n\t{\t\"dump\",\t\t\"dump process stack for each cpu\"\t},\n\t{\t\"ips\",\t\t\"dump Inst Ptr info for each cpu\"\t},\n\t{\t\"kdb\",\t\t\"enter KDB (needs kgdboc= assignment)\"\t},\n\t{\t\"kgdb\",\t\t\"enter KGDB (needs gdb target remote)\"\t},\n\t{\t\"health\",\t\"check if CPUs respond to NMI\"\t\t},\n};\ntypedef char action_t[ACTION_LEN];\nstatic action_t uv_nmi_action = { \"dump\" };\n\nstatic int param_get_action(char *buffer, const struct kernel_param *kp)\n{\n\treturn sprintf(buffer, \"%s\\n\", uv_nmi_action);\n}\n\nstatic int param_set_action(const char *val, const struct kernel_param *kp)\n{\n\tint i;\n\tint n = ARRAY_SIZE(valid_acts);\n\tchar arg[ACTION_LEN];\n\n\t \n\tstrscpy(arg, val, strnchrnul(val, sizeof(arg)-1, '\\n') - val + 1);\n\n\tfor (i = 0; i < n; i++)\n\t\tif (!strcmp(arg, valid_acts[i].action))\n\t\t\tbreak;\n\n\tif (i < n) {\n\t\tstrscpy(uv_nmi_action, arg, sizeof(uv_nmi_action));\n\t\tpr_info(\"UV: New NMI action:%s\\n\", uv_nmi_action);\n\t\treturn 0;\n\t}\n\n\tpr_err(\"UV: Invalid NMI action:%s, valid actions are:\\n\", arg);\n\tfor (i = 0; i < n; i++)\n\t\tpr_err(\"UV: %-8s - %s\\n\",\n\t\t\tvalid_acts[i].action, valid_acts[i].desc);\n\treturn -EINVAL;\n}\n\nstatic const struct kernel_param_ops param_ops_action = {\n\t.get = param_get_action,\n\t.set = param_set_action,\n};\n#define param_check_action(name, p) __param_check(name, p, action_t)\n\nmodule_param_named(action, uv_nmi_action, action, 0644);\n\nstatic inline bool uv_nmi_action_is(const char *action)\n{\n\treturn (strncmp(uv_nmi_action, action, strlen(action)) == 0);\n}\n\n \nstatic void uv_nmi_setup_mmrs(void)\n{\n\tbool new_nmi_method_only = false;\n\n\t \n\tif (UVH_EVENT_OCCURRED0_EXTIO_INT0_MASK) {\t \n\t\tuvh_nmi_mmrx = UVH_EVENT_OCCURRED0;\n\t\tuvh_nmi_mmrx_clear = UVH_EVENT_OCCURRED0_ALIAS;\n\t\tuvh_nmi_mmrx_shift = UVH_EVENT_OCCURRED0_EXTIO_INT0_SHFT;\n\t\tuvh_nmi_mmrx_type = \"OCRD0-EXTIO_INT0\";\n\n\t\tuvh_nmi_mmrx_supported = UVH_EXTIO_INT0_BROADCAST;\n\t\tuvh_nmi_mmrx_req = UVH_BIOS_KERNEL_MMR_ALIAS_2;\n\t\tuvh_nmi_mmrx_req_shift = 62;\n\n\t} else if (UVH_EVENT_OCCURRED1_EXTIO_INT0_MASK) {  \n\t\tuvh_nmi_mmrx = UVH_EVENT_OCCURRED1;\n\t\tuvh_nmi_mmrx_clear = UVH_EVENT_OCCURRED1_ALIAS;\n\t\tuvh_nmi_mmrx_shift = UVH_EVENT_OCCURRED1_EXTIO_INT0_SHFT;\n\t\tuvh_nmi_mmrx_type = \"OCRD1-EXTIO_INT0\";\n\n\t\tnew_nmi_method_only = true;\t\t \n\t\tuvh_nmi_mmrx_req = 0;\t\t\t \n\n\t} else {\n\t\tpr_err(\"UV:%s:NMI support not available on this system\\n\", __func__);\n\t\treturn;\n\t}\n\n\t \n\tif (new_nmi_method_only || uv_read_local_mmr(uvh_nmi_mmrx_supported)) {\n\t\tif (uvh_nmi_mmrx_req)\n\t\t\tuv_write_local_mmr(uvh_nmi_mmrx_req,\n\t\t\t\t\t\t1UL << uvh_nmi_mmrx_req_shift);\n\t\tnmi_mmr = uvh_nmi_mmrx;\n\t\tnmi_mmr_clear = uvh_nmi_mmrx_clear;\n\t\tnmi_mmr_pending = 1UL << uvh_nmi_mmrx_shift;\n\t\tpr_info(\"UV: SMI NMI support: %s\\n\", uvh_nmi_mmrx_type);\n\t} else {\n\t\tnmi_mmr = UVH_NMI_MMR;\n\t\tnmi_mmr_clear = UVH_NMI_MMR_CLEAR;\n\t\tnmi_mmr_pending = 1UL << UVH_NMI_MMR_SHIFT;\n\t\tpr_info(\"UV: SMI NMI support: %s\\n\", UVH_NMI_MMR_TYPE);\n\t}\n}\n\n \nstatic inline int uv_nmi_test_mmr(struct uv_hub_nmi_s *hub_nmi)\n{\n\thub_nmi->nmi_value = uv_read_local_mmr(nmi_mmr);\n\tatomic_inc(&hub_nmi->read_mmr_count);\n\treturn !!(hub_nmi->nmi_value & nmi_mmr_pending);\n}\n\nstatic inline void uv_local_mmr_clear_nmi(void)\n{\n\tuv_write_local_mmr(nmi_mmr_clear, nmi_mmr_pending);\n}\n\n \nstatic inline void uv_reassert_nmi(void)\n{\n\t \n\toutb(0x8f, NMI_CONTROL_PORT);\n\tinb(NMI_DUMMY_PORT);\t\t \n\toutb(0x0f, NMI_CONTROL_PORT);\n\tinb(NMI_DUMMY_PORT);\t\t \n}\n\nstatic void uv_init_hubless_pch_io(int offset, int mask, int data)\n{\n\tint *addr = PCH_PCR_GPIO_ADDRESS(offset);\n\tint readd = readl(addr);\n\n\tif (mask) {\t\t\t \n\t\tint writed = (readd & ~mask) | data;\n\n\t\tnmi_debug(\"UV:PCH: %p = %x & %x | %x (%x)\\n\",\n\t\t\taddr, readd, ~mask, data, writed);\n\t\twritel(writed, addr);\n\t} else if (readd & data) {\t \n\t\tnmi_debug(\"UV:PCH: %p = %x\\n\", addr, data);\n\t\twritel(data, addr);\n\t}\n\n\t(void)readl(addr);\t\t \n}\n\nstatic void uv_nmi_setup_hubless_intr(void)\n{\n\tuv_pch_intr_now_enabled = uv_pch_intr_enable;\n\n\tuv_init_hubless_pch_io(\n\t\tPAD_CFG_DW0_GPP_D_0, GPIROUTNMI,\n\t\tuv_pch_intr_now_enabled ? GPIROUTNMI : 0);\n\n\tnmi_debug(\"UV:NMI: GPP_D_0 interrupt %s\\n\",\n\t\tuv_pch_intr_now_enabled ? \"enabled\" : \"disabled\");\n}\n\nstatic struct init_nmi {\n\tunsigned int\toffset;\n\tunsigned int\tmask;\n\tunsigned int\tdata;\n} init_nmi[] = {\n\t{\t \n\t.offset = 0x84,\n\t.mask = 0x1,\n\t.data = 0x0,\t \n\t},\n\n \n\t{\t \n\t.offset = 0x104,\n\t.mask = 0x0,\n\t.data = 0x1,\t \n\t},\n\t{\t \n\t.offset = 0x124,\n\t.mask = 0x0,\n\t.data = 0x1,\t \n\t},\n\t{\t \n\t.offset = 0x144,\n\t.mask = 0x0,\n\t.data = 0x1,\t \n\t},\n\t{\t \n\t.offset = 0x164,\n\t.mask = 0x0,\n\t.data = 0x1,\t \n\t},\n\n \n\t{\t \n\t.offset = 0x114,\n\t.mask = 0x1,\n\t.data = 0x0,\t \n\t},\n\t{\t \n\t.offset = 0x134,\n\t.mask = 0x1,\n\t.data = 0x0,\t \n\t},\n\t{\t \n\t.offset = 0x154,\n\t.mask = 0x1,\n\t.data = 0x0,\t \n\t},\n\t{\t \n\t.offset = 0x174,\n\t.mask = 0x1,\n\t.data = 0x0,\t \n\t},\n\n \n\t{\t \n\t.offset = 0x4c0,\n\t.mask = 0xffffffff,\n\t.data = 0x82020100,\n \n\t},\n\n \n\t{\t \n\t.offset = 0x4c4,\n\t.mask = 0x3c00,\n\t.data = 0,\t \n\t},\n};\n\nstatic void uv_init_hubless_pch_d0(void)\n{\n\tint i, read;\n\n\tread = *PCH_PCR_GPIO_ADDRESS(PAD_OWN_GPP_D_0);\n\tif (read != 0) {\n\t\tpr_info(\"UV: Hubless NMI already configured\\n\");\n\t\treturn;\n\t}\n\n\tnmi_debug(\"UV: Initializing UV Hubless NMI on PCH\\n\");\n\tfor (i = 0; i < ARRAY_SIZE(init_nmi); i++) {\n\t\tuv_init_hubless_pch_io(init_nmi[i].offset,\n\t\t\t\t\tinit_nmi[i].mask,\n\t\t\t\t\tinit_nmi[i].data);\n\t}\n}\n\nstatic int uv_nmi_test_hubless(struct uv_hub_nmi_s *hub_nmi)\n{\n\tint *pstat = PCH_PCR_GPIO_ADDRESS(GPI_NMI_STS_GPP_D_0);\n\tint status = *pstat;\n\n\thub_nmi->nmi_value = status;\n\tatomic_inc(&hub_nmi->read_mmr_count);\n\n\tif (!(status & STS_GPP_D_0_MASK))\t \n\t\treturn 0;\n\n\t*pstat = STS_GPP_D_0_MASK;\t \n\t(void)*pstat;\t\t\t \n\n\treturn 1;\n}\n\nstatic int uv_test_nmi(struct uv_hub_nmi_s *hub_nmi)\n{\n\tif (hub_nmi->hub_present)\n\t\treturn uv_nmi_test_mmr(hub_nmi);\n\n\tif (hub_nmi->pch_owner)\t\t \n\t\treturn uv_nmi_test_hubless(hub_nmi);\n\n\treturn -1;\n}\n\n \nstatic int uv_set_in_nmi(int cpu, struct uv_hub_nmi_s *hub_nmi)\n{\n\tint first = atomic_add_unless(&hub_nmi->in_nmi, 1, 1);\n\n\tif (first) {\n\t\tatomic_set(&hub_nmi->cpu_owner, cpu);\n\t\tif (atomic_add_unless(&uv_in_nmi, 1, 1))\n\t\t\tatomic_set(&uv_nmi_cpu, cpu);\n\n\t\tatomic_inc(&hub_nmi->nmi_count);\n\t}\n\treturn first;\n}\n\n \nstatic int uv_check_nmi(struct uv_hub_nmi_s *hub_nmi)\n{\n\tint cpu = smp_processor_id();\n\tint nmi = 0;\n\tint nmi_detected = 0;\n\n\tlocal64_inc(&uv_nmi_count);\n\tthis_cpu_inc(uv_cpu_nmi.queries);\n\n\tdo {\n\t\tnmi = atomic_read(&hub_nmi->in_nmi);\n\t\tif (nmi)\n\t\t\tbreak;\n\n\t\tif (raw_spin_trylock(&hub_nmi->nmi_lock)) {\n\t\t\tnmi_detected = uv_test_nmi(hub_nmi);\n\n\t\t\t \n\t\t\tif (nmi_detected > 0) {\n\t\t\t\tuv_set_in_nmi(cpu, hub_nmi);\n\t\t\t\tnmi = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\telse if (nmi_detected < 0)\n\t\t\t\tgoto slave_wait;\n\n\t\t\t \n\t\t\traw_spin_unlock(&hub_nmi->nmi_lock);\n\n\t\t} else {\n\n\t\t\t \nslave_wait:\t\tcpu_relax();\n\t\t\tudelay(uv_nmi_slave_delay);\n\n\t\t\t \n\t\t\tnmi = atomic_read(&hub_nmi->in_nmi);\n\t\t\tif (nmi)\n\t\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (!nmi) {\n\t\t\tnmi = atomic_read(&uv_in_nmi);\n\t\t\tif (nmi)\n\t\t\t\tuv_set_in_nmi(cpu, hub_nmi);\n\t\t}\n\n\t\t \n\t\tif (nmi_detected < 0)\n\t\t\traw_spin_unlock(&hub_nmi->nmi_lock);\n\n\t} while (0);\n\n\tif (!nmi)\n\t\tlocal64_inc(&uv_nmi_misses);\n\n\treturn nmi;\n}\n\n \nstatic inline void uv_clear_nmi(int cpu)\n{\n\tstruct uv_hub_nmi_s *hub_nmi = uv_hub_nmi;\n\n\tif (cpu == atomic_read(&hub_nmi->cpu_owner)) {\n\t\tatomic_set(&hub_nmi->cpu_owner, -1);\n\t\tatomic_set(&hub_nmi->in_nmi, 0);\n\t\tif (hub_nmi->hub_present)\n\t\t\tuv_local_mmr_clear_nmi();\n\t\telse\n\t\t\tuv_reassert_nmi();\n\t\traw_spin_unlock(&hub_nmi->nmi_lock);\n\t}\n}\n\n \nstatic void uv_nmi_nr_cpus_ping(void)\n{\n\tint cpu;\n\n\tfor_each_cpu(cpu, uv_nmi_cpu_mask)\n\t\tuv_cpu_nmi_per(cpu).pinging = 1;\n\n\t__apic_send_IPI_mask(uv_nmi_cpu_mask, APIC_DM_NMI);\n}\n\n \nstatic void uv_nmi_cleanup_mask(void)\n{\n\tint cpu;\n\n\tfor_each_cpu(cpu, uv_nmi_cpu_mask) {\n\t\tuv_cpu_nmi_per(cpu).pinging =  0;\n\t\tuv_cpu_nmi_per(cpu).state = UV_NMI_STATE_OUT;\n\t\tcpumask_clear_cpu(cpu, uv_nmi_cpu_mask);\n\t}\n}\n\n \nstatic int uv_nmi_wait_cpus(int first)\n{\n\tint i, j, k, n = num_online_cpus();\n\tint last_k = 0, waiting = 0;\n\tint cpu = smp_processor_id();\n\n\tif (first) {\n\t\tcpumask_copy(uv_nmi_cpu_mask, cpu_online_mask);\n\t\tk = 0;\n\t} else {\n\t\tk = n - cpumask_weight(uv_nmi_cpu_mask);\n\t}\n\n\t \n\tif (first && uv_pch_intr_now_enabled) {\n\t\tcpumask_clear_cpu(cpu, uv_nmi_cpu_mask);\n\t\treturn n - k - 1;\n\t}\n\n\tudelay(uv_nmi_initial_delay);\n\tfor (i = 0; i < uv_nmi_retry_count; i++) {\n\t\tint loop_delay = uv_nmi_loop_delay;\n\n\t\tfor_each_cpu(j, uv_nmi_cpu_mask) {\n\t\t\tif (uv_cpu_nmi_per(j).state) {\n\t\t\t\tcpumask_clear_cpu(j, uv_nmi_cpu_mask);\n\t\t\t\tif (++k >= n)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (k >= n) {\t\t \n\t\t\tk = n;\n\t\t\tbreak;\n\t\t}\n\t\tif (last_k != k) {\t \n\t\t\tlast_k = k;\n\t\t\twaiting = 0;\n\t\t} else if (++waiting > uv_nmi_wait_count)\n\t\t\tbreak;\n\n\t\t \n\t\tif (waiting && (n - k) == 1 &&\n\t\t    cpumask_test_cpu(0, uv_nmi_cpu_mask))\n\t\t\tloop_delay *= 100;\n\n\t\tudelay(loop_delay);\n\t}\n\tatomic_set(&uv_nmi_cpus_in_nmi, k);\n\treturn n - k;\n}\n\n \nstatic void uv_nmi_wait(int master)\n{\n\t \n\tthis_cpu_write(uv_cpu_nmi.state, UV_NMI_STATE_IN);\n\n\t \n\tif (!master)\n\t\treturn;\n\n\tdo {\n\t\t \n\t\tif (!uv_nmi_wait_cpus(1))\n\t\t\tbreak;\n\n\t\t \n\t\tpr_alert(\"UV: Sending NMI IPI to %d CPUs: %*pbl\\n\",\n\t\t\t cpumask_weight(uv_nmi_cpu_mask),\n\t\t\t cpumask_pr_args(uv_nmi_cpu_mask));\n\n\t\tuv_nmi_nr_cpus_ping();\n\n\t\t \n\t\tif (!uv_nmi_wait_cpus(0))\n\t\t\tbreak;\n\n\t\tpr_alert(\"UV: %d CPUs not in NMI loop: %*pbl\\n\",\n\t\t\t cpumask_weight(uv_nmi_cpu_mask),\n\t\t\t cpumask_pr_args(uv_nmi_cpu_mask));\n\t} while (0);\n\n\tpr_alert(\"UV: %d of %d CPUs in NMI\\n\",\n\t\tatomic_read(&uv_nmi_cpus_in_nmi), num_online_cpus());\n}\n\n \nstatic void uv_nmi_dump_cpu_ip_hdr(void)\n{\n\tpr_info(\"\\nUV: %4s %6s %-32s %s   (Note: PID 0 not listed)\\n\",\n\t\t\"CPU\", \"PID\", \"COMMAND\", \"IP\");\n}\n\n \nstatic void uv_nmi_dump_cpu_ip(int cpu, struct pt_regs *regs)\n{\n\tpr_info(\"UV: %4d %6d %-32.32s %pS\",\n\t\tcpu, current->pid, current->comm, (void *)regs->ip);\n}\n\n \nstatic void uv_nmi_dump_state_cpu(int cpu, struct pt_regs *regs)\n{\n\tconst char *dots = \" ................................. \";\n\n\tif (cpu == 0)\n\t\tuv_nmi_dump_cpu_ip_hdr();\n\n\tif (current->pid != 0 || !uv_nmi_action_is(\"ips\"))\n\t\tuv_nmi_dump_cpu_ip(cpu, regs);\n\n\tif (uv_nmi_action_is(\"dump\")) {\n\t\tpr_info(\"UV:%sNMI process trace for CPU %d\\n\", dots, cpu);\n\t\tshow_regs(regs);\n\t}\n\n\tthis_cpu_write(uv_cpu_nmi.state, UV_NMI_STATE_DUMP_DONE);\n}\n\n \nstatic void uv_nmi_trigger_dump(int cpu)\n{\n\tint retry = uv_nmi_trigger_delay;\n\n\tif (uv_cpu_nmi_per(cpu).state != UV_NMI_STATE_IN)\n\t\treturn;\n\n\tuv_cpu_nmi_per(cpu).state = UV_NMI_STATE_DUMP;\n\tdo {\n\t\tcpu_relax();\n\t\tudelay(10);\n\t\tif (uv_cpu_nmi_per(cpu).state\n\t\t\t\t!= UV_NMI_STATE_DUMP)\n\t\t\treturn;\n\t} while (--retry > 0);\n\n\tpr_crit(\"UV: CPU %d stuck in process dump function\\n\", cpu);\n\tuv_cpu_nmi_per(cpu).state = UV_NMI_STATE_DUMP_DONE;\n}\n\n \nstatic void uv_nmi_sync_exit(int master)\n{\n\tatomic_dec(&uv_nmi_cpus_in_nmi);\n\tif (master) {\n\t\twhile (atomic_read(&uv_nmi_cpus_in_nmi) > 0)\n\t\t\tcpu_relax();\n\t\tatomic_set(&uv_nmi_slave_continue, SLAVE_CLEAR);\n\t} else {\n\t\twhile (atomic_read(&uv_nmi_slave_continue))\n\t\t\tcpu_relax();\n\t}\n}\n\n \nstatic void uv_nmi_action_health(int cpu, struct pt_regs *regs, int master)\n{\n\tif (master) {\n\t\tint in = atomic_read(&uv_nmi_cpus_in_nmi);\n\t\tint out = num_online_cpus() - in;\n\n\t\tpr_alert(\"UV: NMI CPU health check (non-responding:%d)\\n\", out);\n\t\tatomic_set(&uv_nmi_slave_continue, SLAVE_EXIT);\n\t} else {\n\t\twhile (!atomic_read(&uv_nmi_slave_continue))\n\t\t\tcpu_relax();\n\t}\n\tuv_nmi_sync_exit(master);\n}\n\n \nstatic void uv_nmi_dump_state(int cpu, struct pt_regs *regs, int master)\n{\n\tif (master) {\n\t\tint tcpu;\n\t\tint ignored = 0;\n\t\tint saved_console_loglevel = console_loglevel;\n\n\t\tpr_alert(\"UV: tracing %s for %d CPUs from CPU %d\\n\",\n\t\t\tuv_nmi_action_is(\"ips\") ? \"IPs\" : \"processes\",\n\t\t\tatomic_read(&uv_nmi_cpus_in_nmi), cpu);\n\n\t\tconsole_loglevel = uv_nmi_loglevel;\n\t\tatomic_set(&uv_nmi_slave_continue, SLAVE_EXIT);\n\t\tfor_each_online_cpu(tcpu) {\n\t\t\tif (cpumask_test_cpu(tcpu, uv_nmi_cpu_mask))\n\t\t\t\tignored++;\n\t\t\telse if (tcpu == cpu)\n\t\t\t\tuv_nmi_dump_state_cpu(tcpu, regs);\n\t\t\telse\n\t\t\t\tuv_nmi_trigger_dump(tcpu);\n\t\t}\n\t\tif (ignored)\n\t\t\tpr_alert(\"UV: %d CPUs ignored NMI\\n\", ignored);\n\n\t\tconsole_loglevel = saved_console_loglevel;\n\t\tpr_alert(\"UV: process trace complete\\n\");\n\t} else {\n\t\twhile (!atomic_read(&uv_nmi_slave_continue))\n\t\t\tcpu_relax();\n\t\twhile (this_cpu_read(uv_cpu_nmi.state) != UV_NMI_STATE_DUMP)\n\t\t\tcpu_relax();\n\t\tuv_nmi_dump_state_cpu(cpu, regs);\n\t}\n\tuv_nmi_sync_exit(master);\n}\n\nstatic void uv_nmi_touch_watchdogs(void)\n{\n\ttouch_softlockup_watchdog_sync();\n\tclocksource_touch_watchdog();\n\trcu_cpu_stall_reset();\n\ttouch_nmi_watchdog();\n}\n\nstatic void uv_nmi_kdump(int cpu, int main, struct pt_regs *regs)\n{\n\t \n\tif (!kexec_crash_image) {\n\t\tif (main)\n\t\t\tpr_err(\"UV: NMI error: kdump kernel not loaded\\n\");\n\t\treturn;\n\t}\n\n\t \n\tif (main) {\n\t\tpr_emerg(\"UV: NMI executing crash_kexec on CPU%d\\n\", cpu);\n\t\tcrash_kexec(regs);\n\n\t\tpr_emerg(\"UV: crash_kexec unexpectedly returned\\n\");\n\t\tatomic_set(&uv_nmi_kexec_failed, 1);\n\n\t} else {  \n\n\t\t \n\t\twhile (atomic_read(&uv_nmi_kexec_failed) == 0) {\n\n\t\t\t \n\t\t\trun_crash_ipi_callback(regs);\n\n\t\t\tmdelay(10);\n\t\t}\n\t}\n}\n\n#ifdef CONFIG_KGDB\n#ifdef CONFIG_KGDB_KDB\nstatic inline int uv_nmi_kdb_reason(void)\n{\n\treturn KDB_REASON_SYSTEM_NMI;\n}\n#else  \nstatic inline int uv_nmi_kdb_reason(void)\n{\n\t \n\tif (uv_nmi_action_is(\"kgdb\"))\n\t\treturn 0;\n\n\tpr_err(\"UV: NMI error: KDB is not enabled in this kernel\\n\");\n\treturn -1;\n}\n#endif  \n\n \nstatic void uv_call_kgdb_kdb(int cpu, struct pt_regs *regs, int master)\n{\n\tif (master) {\n\t\tint reason = uv_nmi_kdb_reason();\n\t\tint ret;\n\n\t\tif (reason < 0)\n\t\t\treturn;\n\n\t\t \n\t\tret = kgdb_nmicallin(cpu, X86_TRAP_NMI, regs, reason,\n\t\t\t\t&uv_nmi_slave_continue);\n\t\tif (ret) {\n\t\t\tpr_alert(\"KGDB returned error, is kgdboc set?\\n\");\n\t\t\tatomic_set(&uv_nmi_slave_continue, SLAVE_EXIT);\n\t\t}\n\t} else {\n\t\t \n\t\tint sig;\n\n\t\tdo {\n\t\t\tcpu_relax();\n\t\t\tsig = atomic_read(&uv_nmi_slave_continue);\n\t\t} while (!sig);\n\n\t\t \n\t\tif (sig == SLAVE_CONTINUE)\n\t\t\tkgdb_nmicallback(cpu, regs);\n\t}\n\tuv_nmi_sync_exit(master);\n}\n\n#else  \nstatic inline void uv_call_kgdb_kdb(int cpu, struct pt_regs *regs, int master)\n{\n\tpr_err(\"UV: NMI error: KGDB is not enabled in this kernel\\n\");\n}\n#endif  \n\n \nstatic int uv_handle_nmi(unsigned int reason, struct pt_regs *regs)\n{\n\tstruct uv_hub_nmi_s *hub_nmi = uv_hub_nmi;\n\tint cpu = smp_processor_id();\n\tint master = 0;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\n\t \n\tif (!this_cpu_read(uv_cpu_nmi.pinging) && !uv_check_nmi(hub_nmi)) {\n\t\tlocal_irq_restore(flags);\n\t\treturn NMI_DONE;\n\t}\n\n\t \n\tmaster = (atomic_read(&uv_nmi_cpu) == cpu);\n\n\t \n\tif (uv_nmi_action_is(\"kdump\")) {\n\t\tuv_nmi_kdump(cpu, master, regs);\n\n\t\t \n\t\tif (master)\n\t\t\tstrscpy(uv_nmi_action, \"dump\", sizeof(uv_nmi_action));\n\t}\n\n\t \n\tuv_nmi_wait(master);\n\n\t \n\tif (uv_nmi_action_is(\"health\")) {\n\t\tuv_nmi_action_health(cpu, regs, master);\n\t} else if (uv_nmi_action_is(\"ips\") || uv_nmi_action_is(\"dump\")) {\n\t\tuv_nmi_dump_state(cpu, regs, master);\n\t} else if (uv_nmi_action_is(\"kdb\") || uv_nmi_action_is(\"kgdb\")) {\n\t\tuv_call_kgdb_kdb(cpu, regs, master);\n\t} else {\n\t\tif (master)\n\t\t\tpr_alert(\"UV: unknown NMI action: %s\\n\", uv_nmi_action);\n\t\tuv_nmi_sync_exit(master);\n\t}\n\n\t \n\tthis_cpu_write(uv_cpu_nmi.state, UV_NMI_STATE_OUT);\n\n\t \n\tuv_clear_nmi(cpu);\n\n\t \n\tif (master) {\n\t\tif (!cpumask_empty(uv_nmi_cpu_mask))\n\t\t\tuv_nmi_cleanup_mask();\n\t\tatomic_set(&uv_nmi_cpus_in_nmi, -1);\n\t\tatomic_set(&uv_nmi_cpu, -1);\n\t\tatomic_set(&uv_in_nmi, 0);\n\t\tatomic_set(&uv_nmi_kexec_failed, 0);\n\t\tatomic_set(&uv_nmi_slave_continue, SLAVE_CLEAR);\n\t}\n\n\tuv_nmi_touch_watchdogs();\n\tlocal_irq_restore(flags);\n\n\treturn NMI_HANDLED;\n}\n\n \nstatic int uv_handle_nmi_ping(unsigned int reason, struct pt_regs *regs)\n{\n\tint ret;\n\n\tthis_cpu_inc(uv_cpu_nmi.queries);\n\tif (!this_cpu_read(uv_cpu_nmi.pinging)) {\n\t\tlocal64_inc(&uv_nmi_ping_misses);\n\t\treturn NMI_DONE;\n\t}\n\n\tthis_cpu_inc(uv_cpu_nmi.pings);\n\tlocal64_inc(&uv_nmi_ping_count);\n\tret = uv_handle_nmi(reason, regs);\n\tthis_cpu_write(uv_cpu_nmi.pinging, 0);\n\treturn ret;\n}\n\nstatic void uv_register_nmi_notifier(void)\n{\n\tif (register_nmi_handler(NMI_UNKNOWN, uv_handle_nmi, 0, \"uv\"))\n\t\tpr_warn(\"UV: NMI handler failed to register\\n\");\n\n\tif (register_nmi_handler(NMI_LOCAL, uv_handle_nmi_ping, 0, \"uvping\"))\n\t\tpr_warn(\"UV: PING NMI handler failed to register\\n\");\n}\n\nvoid uv_nmi_init(void)\n{\n\tunsigned int value;\n\n\t \n\tvalue = apic_read(APIC_LVT1) | APIC_DM_NMI;\n\tvalue &= ~APIC_LVT_MASKED;\n\tapic_write(APIC_LVT1, value);\n}\n\n \nstatic void __init uv_nmi_setup_common(bool hubbed)\n{\n\tint size = sizeof(void *) * (1 << NODES_SHIFT);\n\tint cpu;\n\n\tuv_hub_nmi_list = kzalloc(size, GFP_KERNEL);\n\tnmi_debug(\"UV: NMI hub list @ 0x%p (%d)\\n\", uv_hub_nmi_list, size);\n\tBUG_ON(!uv_hub_nmi_list);\n\tsize = sizeof(struct uv_hub_nmi_s);\n\tfor_each_present_cpu(cpu) {\n\t\tint nid = cpu_to_node(cpu);\n\t\tif (uv_hub_nmi_list[nid] == NULL) {\n\t\t\tuv_hub_nmi_list[nid] = kzalloc_node(size,\n\t\t\t\t\t\t\t    GFP_KERNEL, nid);\n\t\t\tBUG_ON(!uv_hub_nmi_list[nid]);\n\t\t\traw_spin_lock_init(&(uv_hub_nmi_list[nid]->nmi_lock));\n\t\t\tatomic_set(&uv_hub_nmi_list[nid]->cpu_owner, -1);\n\t\t\tuv_hub_nmi_list[nid]->hub_present = hubbed;\n\t\t\tuv_hub_nmi_list[nid]->pch_owner = (nid == 0);\n\t\t}\n\t\tuv_hub_nmi_per(cpu) = uv_hub_nmi_list[nid];\n\t}\n\tBUG_ON(!alloc_cpumask_var(&uv_nmi_cpu_mask, GFP_KERNEL));\n}\n\n \nvoid __init uv_nmi_setup(void)\n{\n\tuv_nmi_setup_mmrs();\n\tuv_nmi_setup_common(true);\n\tuv_register_nmi_notifier();\n\tpr_info(\"UV: Hub NMI enabled\\n\");\n}\n\n \nvoid __init uv_nmi_setup_hubless(void)\n{\n\tuv_nmi_setup_common(false);\n\tpch_base = xlate_dev_mem_ptr(PCH_PCR_GPIO_1_BASE);\n\tnmi_debug(\"UV: PCH base:%p from 0x%lx, GPP_D_0\\n\",\n\t\tpch_base, PCH_PCR_GPIO_1_BASE);\n\tif (uv_pch_init_enable)\n\t\tuv_init_hubless_pch_d0();\n\tuv_init_hubless_pch_io(GPI_NMI_ENA_GPP_D_0,\n\t\t\t\tSTS_GPP_D_0_MASK, STS_GPP_D_0_MASK);\n\tuv_nmi_setup_hubless_intr();\n\t \n\tuv_reassert_nmi();\n\tuv_register_nmi_notifier();\n\tpr_info(\"UV: PCH NMI enabled\\n\");\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}