{
  "module_name": "tlbflush.h",
  "hash_id": "0cc5400f5e93b6fa8f1f7fc499013e3b7baf0654ddca6eb3d98f305d9c83c464",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/include/asm/tlbflush.h",
  "human_readable_source": " \n#ifndef _ASM_X86_TLBFLUSH_H\n#define _ASM_X86_TLBFLUSH_H\n\n#include <linux/mm_types.h>\n#include <linux/mmu_notifier.h>\n#include <linux/sched.h>\n\n#include <asm/processor.h>\n#include <asm/cpufeature.h>\n#include <asm/special_insns.h>\n#include <asm/smp.h>\n#include <asm/invpcid.h>\n#include <asm/pti.h>\n#include <asm/processor-flags.h>\n#include <asm/pgtable.h>\n\nDECLARE_PER_CPU(u64, tlbstate_untag_mask);\n\nvoid __flush_tlb_all(void);\n\n#define TLB_FLUSH_ALL\t-1UL\n#define TLB_GENERATION_INVALID\t0\n\nvoid cr4_update_irqsoff(unsigned long set, unsigned long clear);\nunsigned long cr4_read_shadow(void);\n\n \nstatic inline void cr4_set_bits_irqsoff(unsigned long mask)\n{\n\tcr4_update_irqsoff(mask, 0);\n}\n\n \nstatic inline void cr4_clear_bits_irqsoff(unsigned long mask)\n{\n\tcr4_update_irqsoff(0, mask);\n}\n\n \nstatic inline void cr4_set_bits(unsigned long mask)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tcr4_set_bits_irqsoff(mask);\n\tlocal_irq_restore(flags);\n}\n\n \nstatic inline void cr4_clear_bits(unsigned long mask)\n{\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tcr4_clear_bits_irqsoff(mask);\n\tlocal_irq_restore(flags);\n}\n\n#ifndef MODULE\n \n#define TLB_NR_DYN_ASIDS\t6\n\nstruct tlb_context {\n\tu64 ctx_id;\n\tu64 tlb_gen;\n};\n\nstruct tlb_state {\n\t \n\tstruct mm_struct *loaded_mm;\n\n#define LOADED_MM_SWITCHING ((struct mm_struct *)1UL)\n\n\t \n\tunion {\n\t\tstruct mm_struct\t*last_user_mm;\n\t\tunsigned long\t\tlast_user_mm_spec;\n\t};\n\n\tu16 loaded_mm_asid;\n\tu16 next_asid;\n\n\t \n\tbool invalidate_other;\n\n#ifdef CONFIG_ADDRESS_MASKING\n\t \n\tu8 lam;\n#endif\n\n\t \n\tunsigned short user_pcid_flush_mask;\n\n\t \n\tunsigned long cr4;\n\n\t \n\tstruct tlb_context ctxs[TLB_NR_DYN_ASIDS];\n};\nDECLARE_PER_CPU_ALIGNED(struct tlb_state, cpu_tlbstate);\n\nstruct tlb_state_shared {\n\t \n\tbool is_lazy;\n};\nDECLARE_PER_CPU_SHARED_ALIGNED(struct tlb_state_shared, cpu_tlbstate_shared);\n\nbool nmi_uaccess_okay(void);\n#define nmi_uaccess_okay nmi_uaccess_okay\n\n \nstatic inline void cr4_init_shadow(void)\n{\n\tthis_cpu_write(cpu_tlbstate.cr4, __read_cr4());\n}\n\nextern unsigned long mmu_cr4_features;\nextern u32 *trampoline_cr4_features;\n\nextern void initialize_tlbstate_and_flush(void);\n\n \nstruct flush_tlb_info {\n\t \n\tstruct mm_struct\t*mm;\n\tunsigned long\t\tstart;\n\tunsigned long\t\tend;\n\tu64\t\t\tnew_tlb_gen;\n\tunsigned int\t\tinitiating_cpu;\n\tu8\t\t\tstride_shift;\n\tu8\t\t\tfreed_tables;\n};\n\nvoid flush_tlb_local(void);\nvoid flush_tlb_one_user(unsigned long addr);\nvoid flush_tlb_one_kernel(unsigned long addr);\nvoid flush_tlb_multi(const struct cpumask *cpumask,\n\t\t      const struct flush_tlb_info *info);\n\n#ifdef CONFIG_PARAVIRT\n#include <asm/paravirt.h>\n#endif\n\n#define flush_tlb_mm(mm)\t\t\t\t\t\t\\\n\t\tflush_tlb_mm_range(mm, 0UL, TLB_FLUSH_ALL, 0UL, true)\n\n#define flush_tlb_range(vma, start, end)\t\t\t\t\\\n\tflush_tlb_mm_range((vma)->vm_mm, start, end,\t\t\t\\\n\t\t\t   ((vma)->vm_flags & VM_HUGETLB)\t\t\\\n\t\t\t\t? huge_page_shift(hstate_vma(vma))\t\\\n\t\t\t\t: PAGE_SHIFT, false)\n\nextern void flush_tlb_all(void);\nextern void flush_tlb_mm_range(struct mm_struct *mm, unsigned long start,\n\t\t\t\tunsigned long end, unsigned int stride_shift,\n\t\t\t\tbool freed_tables);\nextern void flush_tlb_kernel_range(unsigned long start, unsigned long end);\n\nstatic inline void flush_tlb_page(struct vm_area_struct *vma, unsigned long a)\n{\n\tflush_tlb_mm_range(vma->vm_mm, a, a + PAGE_SIZE, PAGE_SHIFT, false);\n}\n\nstatic inline bool arch_tlbbatch_should_defer(struct mm_struct *mm)\n{\n\tbool should_defer = false;\n\n\t \n\tif (cpumask_any_but(mm_cpumask(mm), get_cpu()) < nr_cpu_ids)\n\t\tshould_defer = true;\n\tput_cpu();\n\n\treturn should_defer;\n}\n\nstatic inline u64 inc_mm_tlb_gen(struct mm_struct *mm)\n{\n\t \n\treturn atomic64_inc_return(&mm->context.tlb_gen);\n}\n\nstatic inline void arch_tlbbatch_add_pending(struct arch_tlbflush_unmap_batch *batch,\n\t\t\t\t\t     struct mm_struct *mm,\n\t\t\t\t\t     unsigned long uaddr)\n{\n\tinc_mm_tlb_gen(mm);\n\tcpumask_or(&batch->cpumask, &batch->cpumask, mm_cpumask(mm));\n\tmmu_notifier_arch_invalidate_secondary_tlbs(mm, 0, -1UL);\n}\n\nstatic inline void arch_flush_tlb_batched_pending(struct mm_struct *mm)\n{\n\tflush_tlb_mm(mm);\n}\n\nextern void arch_tlbbatch_flush(struct arch_tlbflush_unmap_batch *batch);\n\nstatic inline bool pte_flags_need_flush(unsigned long oldflags,\n\t\t\t\t\tunsigned long newflags,\n\t\t\t\t\tbool ignore_access)\n{\n\t \n\tconst pteval_t flush_on_clear = _PAGE_DIRTY | _PAGE_PRESENT |\n\t\t\t\t\t_PAGE_ACCESSED;\n\tconst pteval_t software_flags = _PAGE_SOFTW1 | _PAGE_SOFTW2 |\n\t\t\t\t\t_PAGE_SOFTW3 | _PAGE_SOFTW4 |\n\t\t\t\t\t_PAGE_SAVED_DIRTY;\n\tconst pteval_t flush_on_change = _PAGE_RW | _PAGE_USER | _PAGE_PWT |\n\t\t\t  _PAGE_PCD | _PAGE_PSE | _PAGE_GLOBAL | _PAGE_PAT |\n\t\t\t  _PAGE_PAT_LARGE | _PAGE_PKEY_BIT0 | _PAGE_PKEY_BIT1 |\n\t\t\t  _PAGE_PKEY_BIT2 | _PAGE_PKEY_BIT3 | _PAGE_NX;\n\tunsigned long diff = oldflags ^ newflags;\n\n\tBUILD_BUG_ON(flush_on_clear & software_flags);\n\tBUILD_BUG_ON(flush_on_clear & flush_on_change);\n\tBUILD_BUG_ON(flush_on_change & software_flags);\n\n\t \n\tdiff &= ~software_flags;\n\n\tif (ignore_access)\n\t\tdiff &= ~_PAGE_ACCESSED;\n\n\t \n\tif (diff & oldflags & flush_on_clear)\n\t\treturn true;\n\n\t \n\tif (diff & flush_on_change)\n\t\treturn true;\n\n\t \n\tif (IS_ENABLED(CONFIG_DEBUG_VM) &&\n\t    (diff & ~(flush_on_clear | software_flags | flush_on_change))) {\n\t\tVM_WARN_ON_ONCE(1);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic inline bool pte_needs_flush(pte_t oldpte, pte_t newpte)\n{\n\t \n\tif (!(pte_flags(oldpte) & _PAGE_PRESENT))\n\t\treturn false;\n\n\t \n\tif (pte_pfn(oldpte) != pte_pfn(newpte))\n\t\treturn true;\n\n\t \n\treturn pte_flags_need_flush(pte_flags(oldpte), pte_flags(newpte),\n\t\t\t\t    true);\n}\n#define pte_needs_flush pte_needs_flush\n\n \nstatic inline bool huge_pmd_needs_flush(pmd_t oldpmd, pmd_t newpmd)\n{\n\t \n\tif (!(pmd_flags(oldpmd) & _PAGE_PRESENT))\n\t\treturn false;\n\n\t \n\tif (pmd_pfn(oldpmd) != pmd_pfn(newpmd))\n\t\treturn true;\n\n\t \n\treturn pte_flags_need_flush(pmd_flags(oldpmd), pmd_flags(newpmd),\n\t\t\t\t    false);\n}\n#define huge_pmd_needs_flush huge_pmd_needs_flush\n\n#ifdef CONFIG_ADDRESS_MASKING\nstatic inline  u64 tlbstate_lam_cr3_mask(void)\n{\n\tu64 lam = this_cpu_read(cpu_tlbstate.lam);\n\n\treturn lam << X86_CR3_LAM_U57_BIT;\n}\n\nstatic inline void set_tlbstate_lam_mode(struct mm_struct *mm)\n{\n\tthis_cpu_write(cpu_tlbstate.lam,\n\t\t       mm->context.lam_cr3_mask >> X86_CR3_LAM_U57_BIT);\n\tthis_cpu_write(tlbstate_untag_mask, mm->context.untag_mask);\n}\n\n#else\n\nstatic inline u64 tlbstate_lam_cr3_mask(void)\n{\n\treturn 0;\n}\n\nstatic inline void set_tlbstate_lam_mode(struct mm_struct *mm)\n{\n}\n#endif\n#endif  \n\nstatic inline void __native_tlb_flush_global(unsigned long cr4)\n{\n\tnative_write_cr4(cr4 ^ X86_CR4_PGE);\n\tnative_write_cr4(cr4);\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}