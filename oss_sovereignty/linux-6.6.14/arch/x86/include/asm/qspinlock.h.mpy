{
  "module_name": "qspinlock.h",
  "hash_id": "d6243b4cc4be21e908c985fe584a783db4520a77ee203d0342571366712530cf",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/include/asm/qspinlock.h",
  "human_readable_source": " \n#ifndef _ASM_X86_QSPINLOCK_H\n#define _ASM_X86_QSPINLOCK_H\n\n#include <linux/jump_label.h>\n#include <asm/cpufeature.h>\n#include <asm-generic/qspinlock_types.h>\n#include <asm/paravirt.h>\n#include <asm/rmwcc.h>\n\n#define _Q_PENDING_LOOPS\t(1 << 9)\n\n#define queued_fetch_set_pending_acquire queued_fetch_set_pending_acquire\nstatic __always_inline u32 queued_fetch_set_pending_acquire(struct qspinlock *lock)\n{\n\tu32 val;\n\n\t \n\tval = GEN_BINARY_RMWcc(LOCK_PREFIX \"btsl\", lock->val.counter, c,\n\t\t\t       \"I\", _Q_PENDING_OFFSET) * _Q_PENDING_VAL;\n\tval |= atomic_read(&lock->val) & ~_Q_PENDING_MASK;\n\n\treturn val;\n}\n\n#ifdef CONFIG_PARAVIRT_SPINLOCKS\nextern void native_queued_spin_lock_slowpath(struct qspinlock *lock, u32 val);\nextern void __pv_init_lock_hash(void);\nextern void __pv_queued_spin_lock_slowpath(struct qspinlock *lock, u32 val);\nextern void __raw_callee_save___pv_queued_spin_unlock(struct qspinlock *lock);\nextern bool nopvspin;\n\n#define\tqueued_spin_unlock queued_spin_unlock\n \nstatic inline void native_queued_spin_unlock(struct qspinlock *lock)\n{\n\tsmp_store_release(&lock->locked, 0);\n}\n\nstatic inline void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)\n{\n\tpv_queued_spin_lock_slowpath(lock, val);\n}\n\nstatic inline void queued_spin_unlock(struct qspinlock *lock)\n{\n\tkcsan_release();\n\tpv_queued_spin_unlock(lock);\n}\n\n#define vcpu_is_preempted vcpu_is_preempted\nstatic inline bool vcpu_is_preempted(long cpu)\n{\n\treturn pv_vcpu_is_preempted(cpu);\n}\n#endif\n\n#ifdef CONFIG_PARAVIRT\n \nDECLARE_STATIC_KEY_TRUE(virt_spin_lock_key);\n\n \n#define virt_spin_lock virt_spin_lock\nstatic inline bool virt_spin_lock(struct qspinlock *lock)\n{\n\tif (!static_branch_likely(&virt_spin_lock_key))\n\t\treturn false;\n\n\t \n\n\tdo {\n\t\twhile (atomic_read(&lock->val) != 0)\n\t\t\tcpu_relax();\n\t} while (atomic_cmpxchg(&lock->val, 0, _Q_LOCKED_VAL) != 0);\n\n\treturn true;\n}\n\n#endif  \n\n#include <asm-generic/qspinlock.h>\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}