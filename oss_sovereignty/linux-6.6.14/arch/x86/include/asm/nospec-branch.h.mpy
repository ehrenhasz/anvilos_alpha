{
  "module_name": "nospec-branch.h",
  "hash_id": "ad725918810158ccadc241a0fdf00beec99b5033fc46eccbfb1c8e407f9eccbe",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/include/asm/nospec-branch.h",
  "human_readable_source": " \n\n#ifndef _ASM_X86_NOSPEC_BRANCH_H_\n#define _ASM_X86_NOSPEC_BRANCH_H_\n\n#include <linux/static_key.h>\n#include <linux/objtool.h>\n#include <linux/linkage.h>\n\n#include <asm/alternative.h>\n#include <asm/cpufeatures.h>\n#include <asm/msr-index.h>\n#include <asm/unwind_hints.h>\n#include <asm/percpu.h>\n#include <asm/current.h>\n\n \n#define RET_DEPTH_SHIFT\t\t\t5\n#define RSB_RET_STUFF_LOOPS\t\t16\n#define RET_DEPTH_INIT\t\t\t0x8000000000000000ULL\n#define RET_DEPTH_INIT_FROM_CALL\t0xfc00000000000000ULL\n#define RET_DEPTH_CREDIT\t\t0xffffffffffffffffULL\n\n#ifdef CONFIG_CALL_THUNKS_DEBUG\n# define CALL_THUNKS_DEBUG_INC_CALLS\t\t\t\t\\\n\tincq\t%gs:__x86_call_count;\n# define CALL_THUNKS_DEBUG_INC_RETS\t\t\t\t\\\n\tincq\t%gs:__x86_ret_count;\n# define CALL_THUNKS_DEBUG_INC_STUFFS\t\t\t\t\\\n\tincq\t%gs:__x86_stuffs_count;\n# define CALL_THUNKS_DEBUG_INC_CTXSW\t\t\t\t\\\n\tincq\t%gs:__x86_ctxsw_count;\n#else\n# define CALL_THUNKS_DEBUG_INC_CALLS\n# define CALL_THUNKS_DEBUG_INC_RETS\n# define CALL_THUNKS_DEBUG_INC_STUFFS\n# define CALL_THUNKS_DEBUG_INC_CTXSW\n#endif\n\n#if defined(CONFIG_CALL_DEPTH_TRACKING) && !defined(COMPILE_OFFSETS)\n\n#include <asm/asm-offsets.h>\n\n#define CREDIT_CALL_DEPTH\t\t\t\t\t\\\n\tmovq\t$-1, PER_CPU_VAR(pcpu_hot + X86_call_depth);\n\n#define ASM_CREDIT_CALL_DEPTH\t\t\t\t\t\\\n\tmovq\t$-1, PER_CPU_VAR(pcpu_hot + X86_call_depth);\n\n#define RESET_CALL_DEPTH\t\t\t\t\t\\\n\txor\t%eax, %eax;\t\t\t\t\t\\\n\tbts\t$63, %rax;\t\t\t\t\t\\\n\tmovq\t%rax, PER_CPU_VAR(pcpu_hot + X86_call_depth);\n\n#define RESET_CALL_DEPTH_FROM_CALL\t\t\t\t\\\n\tmovb\t$0xfc, %al;\t\t\t\t\t\\\n\tshl\t$56, %rax;\t\t\t\t\t\\\n\tmovq\t%rax, PER_CPU_VAR(pcpu_hot + X86_call_depth);\t\\\n\tCALL_THUNKS_DEBUG_INC_CALLS\n\n#define INCREMENT_CALL_DEPTH\t\t\t\t\t\\\n\tsarq\t$5, %gs:pcpu_hot + X86_call_depth;\t\t\\\n\tCALL_THUNKS_DEBUG_INC_CALLS\n\n#define ASM_INCREMENT_CALL_DEPTH\t\t\t\t\\\n\tsarq\t$5, PER_CPU_VAR(pcpu_hot + X86_call_depth);\t\\\n\tCALL_THUNKS_DEBUG_INC_CALLS\n\n#else\n#define CREDIT_CALL_DEPTH\n#define ASM_CREDIT_CALL_DEPTH\n#define RESET_CALL_DEPTH\n#define INCREMENT_CALL_DEPTH\n#define ASM_INCREMENT_CALL_DEPTH\n#define RESET_CALL_DEPTH_FROM_CALL\n#endif\n\n \n\n#define RETPOLINE_THUNK_SIZE\t32\n#define RSB_CLEAR_LOOPS\t\t32\t \n\n \n#define __FILL_RETURN_SLOT\t\t\t\\\n\tANNOTATE_INTRA_FUNCTION_CALL;\t\t\\\n\tcall\t772f;\t\t\t\t\\\n\tint3;\t\t\t\t\t\\\n772:\n\n \n#ifdef CONFIG_X86_64\n#define __FILL_RETURN_BUFFER(reg, nr)\t\t\t\\\n\tmov\t$(nr/2), reg;\t\t\t\t\\\n771:\t\t\t\t\t\t\t\\\n\t__FILL_RETURN_SLOT\t\t\t\t\\\n\t__FILL_RETURN_SLOT\t\t\t\t\\\n\tadd\t$(BITS_PER_LONG/8) * 2, %_ASM_SP;\t\\\n\tdec\treg;\t\t\t\t\t\\\n\tjnz\t771b;\t\t\t\t\t\\\n\t \t\t\\\n\tlfence;\t\t\t\t\t\t\\\n\tASM_CREDIT_CALL_DEPTH\t\t\t\t\\\n\tCALL_THUNKS_DEBUG_INC_CTXSW\n#else\n \n#define __FILL_RETURN_BUFFER(reg, nr)\t\t\t\\\n\t.rept nr;\t\t\t\t\t\\\n\t__FILL_RETURN_SLOT;\t\t\t\t\\\n\t.endr;\t\t\t\t\t\t\\\n\tadd\t$(BITS_PER_LONG/8) * nr, %_ASM_SP;\n#endif\n\n \n#define __FILL_ONE_RETURN\t\t\t\t\\\n\t__FILL_RETURN_SLOT\t\t\t\t\\\n\tadd\t$(BITS_PER_LONG/8), %_ASM_SP;\t\t\\\n\tlfence;\n\n#ifdef __ASSEMBLY__\n\n \n.macro ANNOTATE_RETPOLINE_SAFE\n.Lhere_\\@:\n\t.pushsection .discard.retpoline_safe\n\t.long .Lhere_\\@\n\t.popsection\n.endm\n\n \n#define ANNOTATE_UNRET_SAFE ANNOTATE_RETPOLINE_SAFE\n\n \n.macro VALIDATE_UNRET_END\n#if defined(CONFIG_NOINSTR_VALIDATION) && \\\n\t(defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_SRSO))\n\tANNOTATE_RETPOLINE_SAFE\n\tnop\n#endif\n.endm\n\n \n.macro __CS_PREFIX reg:req\n\t.irp rs,r8,r9,r10,r11,r12,r13,r14,r15\n\t.ifc \\reg,\\rs\n\t.byte 0x2e\n\t.endif\n\t.endr\n.endm\n\n \n.macro JMP_NOSPEC reg:req\n#ifdef CONFIG_RETPOLINE\n\t__CS_PREFIX \\reg\n\tjmp\t__x86_indirect_thunk_\\reg\n#else\n\tjmp\t*%\\reg\n\tint3\n#endif\n.endm\n\n.macro CALL_NOSPEC reg:req\n#ifdef CONFIG_RETPOLINE\n\t__CS_PREFIX \\reg\n\tcall\t__x86_indirect_thunk_\\reg\n#else\n\tcall\t*%\\reg\n#endif\n.endm\n\n  \n.macro FILL_RETURN_BUFFER reg:req nr:req ftr:req ftr2=ALT_NOT(X86_FEATURE_ALWAYS)\n\tALTERNATIVE_2 \"jmp .Lskip_rsb_\\@\", \\\n\t\t__stringify(__FILL_RETURN_BUFFER(\\reg,\\nr)), \\ftr, \\\n\t\t__stringify(nop;nop;__FILL_ONE_RETURN), \\ftr2\n\n.Lskip_rsb_\\@:\n.endm\n\n#if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_SRSO)\n#define CALL_UNTRAIN_RET\t\"call entry_untrain_ret\"\n#else\n#define CALL_UNTRAIN_RET\t\"\"\n#endif\n\n \n.macro UNTRAIN_RET\n#if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_IBPB_ENTRY) || \\\n\tdefined(CONFIG_CALL_DEPTH_TRACKING) || defined(CONFIG_CPU_SRSO)\n\tVALIDATE_UNRET_END\n\tALTERNATIVE_3 \"\",\t\t\t\t\t\t\\\n\t\t      CALL_UNTRAIN_RET, X86_FEATURE_UNRET,\t\t\\\n\t\t      \"call entry_ibpb\", X86_FEATURE_ENTRY_IBPB,\t\\\n\t\t      __stringify(RESET_CALL_DEPTH), X86_FEATURE_CALL_DEPTH\n#endif\n.endm\n\n.macro UNTRAIN_RET_VM\n#if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_IBPB_ENTRY) || \\\n\tdefined(CONFIG_CALL_DEPTH_TRACKING) || defined(CONFIG_CPU_SRSO)\n\tVALIDATE_UNRET_END\n\tALTERNATIVE_3 \"\",\t\t\t\t\t\t\\\n\t\t      CALL_UNTRAIN_RET, X86_FEATURE_UNRET,\t\t\\\n\t\t      \"call entry_ibpb\", X86_FEATURE_IBPB_ON_VMEXIT,\t\\\n\t\t      __stringify(RESET_CALL_DEPTH), X86_FEATURE_CALL_DEPTH\n#endif\n.endm\n\n.macro UNTRAIN_RET_FROM_CALL\n#if defined(CONFIG_CPU_UNRET_ENTRY) || defined(CONFIG_CPU_IBPB_ENTRY) || \\\n\tdefined(CONFIG_CALL_DEPTH_TRACKING) || defined(CONFIG_CPU_SRSO)\n\tVALIDATE_UNRET_END\n\tALTERNATIVE_3 \"\",\t\t\t\t\t\t\\\n\t\t      CALL_UNTRAIN_RET, X86_FEATURE_UNRET,\t\t\\\n\t\t      \"call entry_ibpb\", X86_FEATURE_ENTRY_IBPB,\t\\\n\t\t      __stringify(RESET_CALL_DEPTH_FROM_CALL), X86_FEATURE_CALL_DEPTH\n#endif\n.endm\n\n\n.macro CALL_DEPTH_ACCOUNT\n#ifdef CONFIG_CALL_DEPTH_TRACKING\n\tALTERNATIVE \"\",\t\t\t\t\t\t\t\\\n\t\t    __stringify(ASM_INCREMENT_CALL_DEPTH), X86_FEATURE_CALL_DEPTH\n#endif\n.endm\n\n#else  \n\n#define ANNOTATE_RETPOLINE_SAFE\t\t\t\t\t\\\n\t\"999:\\n\\t\"\t\t\t\t\t\t\\\n\t\".pushsection .discard.retpoline_safe\\n\\t\"\t\t\\\n\t\".long 999b\\n\\t\"\t\t\t\t\t\\\n\t\".popsection\\n\\t\"\n\ntypedef u8 retpoline_thunk_t[RETPOLINE_THUNK_SIZE];\nextern retpoline_thunk_t __x86_indirect_thunk_array[];\nextern retpoline_thunk_t __x86_indirect_call_thunk_array[];\nextern retpoline_thunk_t __x86_indirect_jump_thunk_array[];\n\n#ifdef CONFIG_RETHUNK\nextern void __x86_return_thunk(void);\n#else\nstatic inline void __x86_return_thunk(void) {}\n#endif\n\nextern void retbleed_return_thunk(void);\nextern void srso_return_thunk(void);\nextern void srso_alias_return_thunk(void);\n\nextern void retbleed_untrain_ret(void);\nextern void srso_untrain_ret(void);\nextern void srso_alias_untrain_ret(void);\n\nextern void entry_untrain_ret(void);\nextern void entry_ibpb(void);\n\nextern void (*x86_return_thunk)(void);\n\n#ifdef CONFIG_CALL_DEPTH_TRACKING\nextern void __x86_return_skl(void);\n\nstatic inline void x86_set_skl_return_thunk(void)\n{\n\tx86_return_thunk = &__x86_return_skl;\n}\n\n#define CALL_DEPTH_ACCOUNT\t\t\t\t\t\\\n\tALTERNATIVE(\"\",\t\t\t\t\t\t\\\n\t\t    __stringify(INCREMENT_CALL_DEPTH),\t\t\\\n\t\t    X86_FEATURE_CALL_DEPTH)\n\n#ifdef CONFIG_CALL_THUNKS_DEBUG\nDECLARE_PER_CPU(u64, __x86_call_count);\nDECLARE_PER_CPU(u64, __x86_ret_count);\nDECLARE_PER_CPU(u64, __x86_stuffs_count);\nDECLARE_PER_CPU(u64, __x86_ctxsw_count);\n#endif\n#else\nstatic inline void x86_set_skl_return_thunk(void) {}\n\n#define CALL_DEPTH_ACCOUNT \"\"\n\n#endif\n\n#ifdef CONFIG_RETPOLINE\n\n#define GEN(reg) \\\n\textern retpoline_thunk_t __x86_indirect_thunk_ ## reg;\n#include <asm/GEN-for-each-reg.h>\n#undef GEN\n\n#define GEN(reg)\t\t\t\t\t\t\\\n\textern retpoline_thunk_t __x86_indirect_call_thunk_ ## reg;\n#include <asm/GEN-for-each-reg.h>\n#undef GEN\n\n#define GEN(reg)\t\t\t\t\t\t\\\n\textern retpoline_thunk_t __x86_indirect_jump_thunk_ ## reg;\n#include <asm/GEN-for-each-reg.h>\n#undef GEN\n\n#ifdef CONFIG_X86_64\n\n \n# define CALL_NOSPEC\t\t\t\t\t\t\\\n\tALTERNATIVE_2(\t\t\t\t\t\t\\\n\tANNOTATE_RETPOLINE_SAFE\t\t\t\t\t\\\n\t\"call *%[thunk_target]\\n\",\t\t\t\t\\\n\t\"call __x86_indirect_thunk_%V[thunk_target]\\n\",\t\t\\\n\tX86_FEATURE_RETPOLINE,\t\t\t\t\t\\\n\t\"lfence;\\n\"\t\t\t\t\t\t\\\n\tANNOTATE_RETPOLINE_SAFE\t\t\t\t\t\\\n\t\"call *%[thunk_target]\\n\",\t\t\t\t\\\n\tX86_FEATURE_RETPOLINE_LFENCE)\n\n# define THUNK_TARGET(addr) [thunk_target] \"r\" (addr)\n\n#else  \n \n# define CALL_NOSPEC\t\t\t\t\t\t\\\n\tALTERNATIVE_2(\t\t\t\t\t\t\\\n\tANNOTATE_RETPOLINE_SAFE\t\t\t\t\t\\\n\t\"call *%[thunk_target]\\n\",\t\t\t\t\\\n\t\"       jmp    904f;\\n\"\t\t\t\t\t\\\n\t\"       .align 16\\n\"\t\t\t\t\t\\\n\t\"901:\tcall   903f;\\n\"\t\t\t\t\t\\\n\t\"902:\tpause;\\n\"\t\t\t\t\t\\\n\t\"    \tlfence;\\n\"\t\t\t\t\t\\\n\t\"       jmp    902b;\\n\"\t\t\t\t\t\\\n\t\"       .align 16\\n\"\t\t\t\t\t\\\n\t\"903:\tlea    4(%%esp), %%esp;\\n\"\t\t\t\\\n\t\"       pushl  %[thunk_target];\\n\"\t\t\t\\\n\t\"       ret;\\n\"\t\t\t\t\t\t\\\n\t\"       .align 16\\n\"\t\t\t\t\t\\\n\t\"904:\tcall   901b;\\n\",\t\t\t\t\\\n\tX86_FEATURE_RETPOLINE,\t\t\t\t\t\\\n\t\"lfence;\\n\"\t\t\t\t\t\t\\\n\tANNOTATE_RETPOLINE_SAFE\t\t\t\t\t\\\n\t\"call *%[thunk_target]\\n\",\t\t\t\t\\\n\tX86_FEATURE_RETPOLINE_LFENCE)\n\n# define THUNK_TARGET(addr) [thunk_target] \"rm\" (addr)\n#endif\n#else  \n# define CALL_NOSPEC \"call *%[thunk_target]\\n\"\n# define THUNK_TARGET(addr) [thunk_target] \"rm\" (addr)\n#endif\n\n \nenum spectre_v2_mitigation {\n\tSPECTRE_V2_NONE,\n\tSPECTRE_V2_RETPOLINE,\n\tSPECTRE_V2_LFENCE,\n\tSPECTRE_V2_EIBRS,\n\tSPECTRE_V2_EIBRS_RETPOLINE,\n\tSPECTRE_V2_EIBRS_LFENCE,\n\tSPECTRE_V2_IBRS,\n};\n\n \nenum spectre_v2_user_mitigation {\n\tSPECTRE_V2_USER_NONE,\n\tSPECTRE_V2_USER_STRICT,\n\tSPECTRE_V2_USER_STRICT_PREFERRED,\n\tSPECTRE_V2_USER_PRCTL,\n\tSPECTRE_V2_USER_SECCOMP,\n};\n\n \nenum ssb_mitigation {\n\tSPEC_STORE_BYPASS_NONE,\n\tSPEC_STORE_BYPASS_DISABLE,\n\tSPEC_STORE_BYPASS_PRCTL,\n\tSPEC_STORE_BYPASS_SECCOMP,\n};\n\nstatic __always_inline\nvoid alternative_msr_write(unsigned int msr, u64 val, unsigned int feature)\n{\n\tasm volatile(ALTERNATIVE(\"\", \"wrmsr\", %c[feature])\n\t\t: : \"c\" (msr),\n\t\t    \"a\" ((u32)val),\n\t\t    \"d\" ((u32)(val >> 32)),\n\t\t    [feature] \"i\" (feature)\n\t\t: \"memory\");\n}\n\nextern u64 x86_pred_cmd;\n\nstatic inline void indirect_branch_prediction_barrier(void)\n{\n\talternative_msr_write(MSR_IA32_PRED_CMD, x86_pred_cmd, X86_FEATURE_USE_IBPB);\n}\n\n \nextern u64 x86_spec_ctrl_base;\nDECLARE_PER_CPU(u64, x86_spec_ctrl_current);\nextern void update_spec_ctrl_cond(u64 val);\nextern u64 spec_ctrl_current(void);\n\n \n#define firmware_restrict_branch_speculation_start()\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tpreempt_disable();\t\t\t\t\t\t\\\n\talternative_msr_write(MSR_IA32_SPEC_CTRL,\t\t\t\\\n\t\t\t      spec_ctrl_current() | SPEC_CTRL_IBRS,\t\\\n\t\t\t      X86_FEATURE_USE_IBRS_FW);\t\t\t\\\n\talternative_msr_write(MSR_IA32_PRED_CMD, PRED_CMD_IBPB,\t\t\\\n\t\t\t      X86_FEATURE_USE_IBPB_FW);\t\t\t\\\n} while (0)\n\n#define firmware_restrict_branch_speculation_end()\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\talternative_msr_write(MSR_IA32_SPEC_CTRL,\t\t\t\\\n\t\t\t      spec_ctrl_current(),\t\t\t\\\n\t\t\t      X86_FEATURE_USE_IBRS_FW);\t\t\t\\\n\tpreempt_enable();\t\t\t\t\t\t\\\n} while (0)\n\nDECLARE_STATIC_KEY_FALSE(switch_to_cond_stibp);\nDECLARE_STATIC_KEY_FALSE(switch_mm_cond_ibpb);\nDECLARE_STATIC_KEY_FALSE(switch_mm_always_ibpb);\n\nDECLARE_STATIC_KEY_FALSE(mds_user_clear);\nDECLARE_STATIC_KEY_FALSE(mds_idle_clear);\n\nDECLARE_STATIC_KEY_FALSE(switch_mm_cond_l1d_flush);\n\nDECLARE_STATIC_KEY_FALSE(mmio_stale_data_clear);\n\n#include <asm/segment.h>\n\n \nstatic __always_inline void mds_clear_cpu_buffers(void)\n{\n\tstatic const u16 ds = __KERNEL_DS;\n\n\t \n\tasm volatile(\"verw %[ds]\" : : [ds] \"m\" (ds) : \"cc\");\n}\n\n \nstatic __always_inline void mds_user_clear_cpu_buffers(void)\n{\n\tif (static_branch_likely(&mds_user_clear))\n\t\tmds_clear_cpu_buffers();\n}\n\n \nstatic __always_inline void mds_idle_clear_cpu_buffers(void)\n{\n\tif (static_branch_likely(&mds_idle_clear))\n\t\tmds_clear_cpu_buffers();\n}\n\n#endif  \n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}