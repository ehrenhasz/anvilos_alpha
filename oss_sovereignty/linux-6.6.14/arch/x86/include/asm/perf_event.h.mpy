{
  "module_name": "perf_event.h",
  "hash_id": "854e62f6bcf00d5a2b735b0ca8ba4b82aaf9ae7af8416be1463dafcc4bd838ae",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/include/asm/perf_event.h",
  "human_readable_source": " \n#ifndef _ASM_X86_PERF_EVENT_H\n#define _ASM_X86_PERF_EVENT_H\n\n#include <linux/static_call.h>\n\n \n\n#define INTEL_PMC_MAX_GENERIC\t\t\t\t       32\n#define INTEL_PMC_MAX_FIXED\t\t\t\t       16\n#define INTEL_PMC_IDX_FIXED\t\t\t\t       32\n\n#define X86_PMC_IDX_MAX\t\t\t\t\t       64\n\n#define MSR_ARCH_PERFMON_PERFCTR0\t\t\t      0xc1\n#define MSR_ARCH_PERFMON_PERFCTR1\t\t\t      0xc2\n\n#define MSR_ARCH_PERFMON_EVENTSEL0\t\t\t     0x186\n#define MSR_ARCH_PERFMON_EVENTSEL1\t\t\t     0x187\n\n#define ARCH_PERFMON_EVENTSEL_EVENT\t\t\t0x000000FFULL\n#define ARCH_PERFMON_EVENTSEL_UMASK\t\t\t0x0000FF00ULL\n#define ARCH_PERFMON_EVENTSEL_USR\t\t\t(1ULL << 16)\n#define ARCH_PERFMON_EVENTSEL_OS\t\t\t(1ULL << 17)\n#define ARCH_PERFMON_EVENTSEL_EDGE\t\t\t(1ULL << 18)\n#define ARCH_PERFMON_EVENTSEL_PIN_CONTROL\t\t(1ULL << 19)\n#define ARCH_PERFMON_EVENTSEL_INT\t\t\t(1ULL << 20)\n#define ARCH_PERFMON_EVENTSEL_ANY\t\t\t(1ULL << 21)\n#define ARCH_PERFMON_EVENTSEL_ENABLE\t\t\t(1ULL << 22)\n#define ARCH_PERFMON_EVENTSEL_INV\t\t\t(1ULL << 23)\n#define ARCH_PERFMON_EVENTSEL_CMASK\t\t\t0xFF000000ULL\n\n#define INTEL_FIXED_BITS_MASK\t\t\t\t0xFULL\n#define INTEL_FIXED_BITS_STRIDE\t\t\t4\n#define INTEL_FIXED_0_KERNEL\t\t\t\t(1ULL << 0)\n#define INTEL_FIXED_0_USER\t\t\t\t(1ULL << 1)\n#define INTEL_FIXED_0_ANYTHREAD\t\t\t(1ULL << 2)\n#define INTEL_FIXED_0_ENABLE_PMI\t\t\t(1ULL << 3)\n\n#define HSW_IN_TX\t\t\t\t\t(1ULL << 32)\n#define HSW_IN_TX_CHECKPOINTED\t\t\t\t(1ULL << 33)\n#define ICL_EVENTSEL_ADAPTIVE\t\t\t\t(1ULL << 34)\n#define ICL_FIXED_0_ADAPTIVE\t\t\t\t(1ULL << 32)\n\n#define intel_fixed_bits_by_idx(_idx, _bits)\t\t\t\\\n\t((_bits) << ((_idx) * INTEL_FIXED_BITS_STRIDE))\n\n#define AMD64_EVENTSEL_INT_CORE_ENABLE\t\t\t(1ULL << 36)\n#define AMD64_EVENTSEL_GUESTONLY\t\t\t(1ULL << 40)\n#define AMD64_EVENTSEL_HOSTONLY\t\t\t\t(1ULL << 41)\n\n#define AMD64_EVENTSEL_INT_CORE_SEL_SHIFT\t\t37\n#define AMD64_EVENTSEL_INT_CORE_SEL_MASK\t\t\\\n\t(0xFULL << AMD64_EVENTSEL_INT_CORE_SEL_SHIFT)\n\n#define AMD64_EVENTSEL_EVENT\t\\\n\t(ARCH_PERFMON_EVENTSEL_EVENT | (0x0FULL << 32))\n#define INTEL_ARCH_EVENT_MASK\t\\\n\t(ARCH_PERFMON_EVENTSEL_UMASK | ARCH_PERFMON_EVENTSEL_EVENT)\n\n#define AMD64_L3_SLICE_SHIFT\t\t\t\t48\n#define AMD64_L3_SLICE_MASK\t\t\t\t\\\n\t(0xFULL << AMD64_L3_SLICE_SHIFT)\n#define AMD64_L3_SLICEID_MASK\t\t\t\t\\\n\t(0x7ULL << AMD64_L3_SLICE_SHIFT)\n\n#define AMD64_L3_THREAD_SHIFT\t\t\t\t56\n#define AMD64_L3_THREAD_MASK\t\t\t\t\\\n\t(0xFFULL << AMD64_L3_THREAD_SHIFT)\n#define AMD64_L3_F19H_THREAD_MASK\t\t\t\\\n\t(0x3ULL << AMD64_L3_THREAD_SHIFT)\n\n#define AMD64_L3_EN_ALL_CORES\t\t\t\tBIT_ULL(47)\n#define AMD64_L3_EN_ALL_SLICES\t\t\t\tBIT_ULL(46)\n\n#define AMD64_L3_COREID_SHIFT\t\t\t\t42\n#define AMD64_L3_COREID_MASK\t\t\t\t\\\n\t(0x7ULL << AMD64_L3_COREID_SHIFT)\n\n#define X86_RAW_EVENT_MASK\t\t\\\n\t(ARCH_PERFMON_EVENTSEL_EVENT |\t\\\n\t ARCH_PERFMON_EVENTSEL_UMASK |\t\\\n\t ARCH_PERFMON_EVENTSEL_EDGE  |\t\\\n\t ARCH_PERFMON_EVENTSEL_INV   |\t\\\n\t ARCH_PERFMON_EVENTSEL_CMASK)\n#define X86_ALL_EVENT_FLAGS  \t\t\t\\\n\t(ARCH_PERFMON_EVENTSEL_EDGE |  \t\t\\\n\t ARCH_PERFMON_EVENTSEL_INV | \t\t\\\n\t ARCH_PERFMON_EVENTSEL_CMASK | \t\t\\\n\t ARCH_PERFMON_EVENTSEL_ANY | \t\t\\\n\t ARCH_PERFMON_EVENTSEL_PIN_CONTROL | \t\\\n\t HSW_IN_TX | \t\t\t\t\\\n\t HSW_IN_TX_CHECKPOINTED)\n#define AMD64_RAW_EVENT_MASK\t\t\\\n\t(X86_RAW_EVENT_MASK          |  \\\n\t AMD64_EVENTSEL_EVENT)\n#define AMD64_RAW_EVENT_MASK_NB\t\t\\\n\t(AMD64_EVENTSEL_EVENT        |  \\\n\t ARCH_PERFMON_EVENTSEL_UMASK)\n\n#define AMD64_PERFMON_V2_EVENTSEL_EVENT_NB\t\\\n\t(AMD64_EVENTSEL_EVENT\t|\t\t\\\n\t GENMASK_ULL(37, 36))\n\n#define AMD64_PERFMON_V2_EVENTSEL_UMASK_NB\t\\\n\t(ARCH_PERFMON_EVENTSEL_UMASK\t|\t\\\n\t GENMASK_ULL(27, 24))\n\n#define AMD64_PERFMON_V2_RAW_EVENT_MASK_NB\t\t\\\n\t(AMD64_PERFMON_V2_EVENTSEL_EVENT_NB\t|\t\\\n\t AMD64_PERFMON_V2_EVENTSEL_UMASK_NB)\n\n#define AMD64_NUM_COUNTERS\t\t\t\t4\n#define AMD64_NUM_COUNTERS_CORE\t\t\t\t6\n#define AMD64_NUM_COUNTERS_NB\t\t\t\t4\n\n#define ARCH_PERFMON_UNHALTED_CORE_CYCLES_SEL\t\t0x3c\n#define ARCH_PERFMON_UNHALTED_CORE_CYCLES_UMASK\t\t(0x00 << 8)\n#define ARCH_PERFMON_UNHALTED_CORE_CYCLES_INDEX\t\t0\n#define ARCH_PERFMON_UNHALTED_CORE_CYCLES_PRESENT \\\n\t\t(1 << (ARCH_PERFMON_UNHALTED_CORE_CYCLES_INDEX))\n\n#define ARCH_PERFMON_BRANCH_MISSES_RETIRED\t\t6\n#define ARCH_PERFMON_EVENTS_COUNT\t\t\t7\n\n#define PEBS_DATACFG_MEMINFO\tBIT_ULL(0)\n#define PEBS_DATACFG_GP\tBIT_ULL(1)\n#define PEBS_DATACFG_XMMS\tBIT_ULL(2)\n#define PEBS_DATACFG_LBRS\tBIT_ULL(3)\n#define PEBS_DATACFG_LBR_SHIFT\t24\n\n \n#define PEBS_UPDATE_DS_SW\tBIT_ULL(63)\n\n \nunion cpuid10_eax {\n\tstruct {\n\t\tunsigned int version_id:8;\n\t\tunsigned int num_counters:8;\n\t\tunsigned int bit_width:8;\n\t\tunsigned int mask_length:8;\n\t} split;\n\tunsigned int full;\n};\n\nunion cpuid10_ebx {\n\tstruct {\n\t\tunsigned int no_unhalted_core_cycles:1;\n\t\tunsigned int no_instructions_retired:1;\n\t\tunsigned int no_unhalted_reference_cycles:1;\n\t\tunsigned int no_llc_reference:1;\n\t\tunsigned int no_llc_misses:1;\n\t\tunsigned int no_branch_instruction_retired:1;\n\t\tunsigned int no_branch_misses_retired:1;\n\t} split;\n\tunsigned int full;\n};\n\nunion cpuid10_edx {\n\tstruct {\n\t\tunsigned int num_counters_fixed:5;\n\t\tunsigned int bit_width_fixed:8;\n\t\tunsigned int reserved1:2;\n\t\tunsigned int anythread_deprecated:1;\n\t\tunsigned int reserved2:16;\n\t} split;\n\tunsigned int full;\n};\n\n \n#define ARCH_PERFMON_EXT_LEAF\t\t\t0x00000023\n#define ARCH_PERFMON_NUM_COUNTER_LEAF_BIT\t0x1\n#define ARCH_PERFMON_NUM_COUNTER_LEAF\t\t0x1\n\n \nunion cpuid28_eax {\n\tstruct {\n\t\t \n\t\tunsigned int\tlbr_depth_mask:8;\n\t\tunsigned int\treserved:22;\n\t\t \n\t\tunsigned int\tlbr_deep_c_reset:1;\n\t\t \n\t\tunsigned int\tlbr_lip:1;\n\t} split;\n\tunsigned int\t\tfull;\n};\n\nunion cpuid28_ebx {\n\tstruct {\n\t\t \n\t\tunsigned int    lbr_cpl:1;\n\t\t \n\t\tunsigned int    lbr_filter:1;\n\t\t \n\t\tunsigned int    lbr_call_stack:1;\n\t} split;\n\tunsigned int            full;\n};\n\nunion cpuid28_ecx {\n\tstruct {\n\t\t \n\t\tunsigned int    lbr_mispred:1;\n\t\t \n\t\tunsigned int    lbr_timed_lbr:1;\n\t\t \n\t\tunsigned int    lbr_br_type:1;\n\t} split;\n\tunsigned int            full;\n};\n\n \nunion cpuid_0x80000022_ebx {\n\tstruct {\n\t\t \n\t\tunsigned int\tnum_core_pmc:4;\n\t\t \n\t\tunsigned int\tlbr_v2_stack_sz:6;\n\t\t \n\t\tunsigned int\tnum_df_pmc:6;\n\t} split;\n\tunsigned int\t\tfull;\n};\n\nstruct x86_pmu_capability {\n\tint\t\tversion;\n\tint\t\tnum_counters_gp;\n\tint\t\tnum_counters_fixed;\n\tint\t\tbit_width_gp;\n\tint\t\tbit_width_fixed;\n\tunsigned int\tevents_mask;\n\tint\t\tevents_mask_len;\n\tunsigned int\tpebs_ept\t:1;\n};\n\n \n\n \n#define INTEL_PMC_FIXED_RDPMC_BASE\t\t(1 << 30)\n#define INTEL_PMC_FIXED_RDPMC_METRICS\t\t(1 << 29)\n\n \n#define MSR_ARCH_PERFMON_FIXED_CTR_CTRL\t0x38d\n\n \n\n \n#define MSR_ARCH_PERFMON_FIXED_CTR0\t0x309\n#define INTEL_PMC_IDX_FIXED_INSTRUCTIONS\t(INTEL_PMC_IDX_FIXED + 0)\n\n \n#define MSR_ARCH_PERFMON_FIXED_CTR1\t0x30a\n#define INTEL_PMC_IDX_FIXED_CPU_CYCLES\t(INTEL_PMC_IDX_FIXED + 1)\n\n \n#define MSR_ARCH_PERFMON_FIXED_CTR2\t0x30b\n#define INTEL_PMC_IDX_FIXED_REF_CYCLES\t(INTEL_PMC_IDX_FIXED + 2)\n#define INTEL_PMC_MSK_FIXED_REF_CYCLES\t(1ULL << INTEL_PMC_IDX_FIXED_REF_CYCLES)\n\n \n#define MSR_ARCH_PERFMON_FIXED_CTR3\t0x30c\n#define INTEL_PMC_IDX_FIXED_SLOTS\t(INTEL_PMC_IDX_FIXED + 3)\n#define INTEL_PMC_MSK_FIXED_SLOTS\t(1ULL << INTEL_PMC_IDX_FIXED_SLOTS)\n\nstatic inline bool use_fixed_pseudo_encoding(u64 code)\n{\n\treturn !(code & 0xff);\n}\n\n \n#define INTEL_PMC_IDX_FIXED_BTS\t\t\t(INTEL_PMC_IDX_FIXED + 15)\n\n \n#define INTEL_PMC_IDX_METRIC_BASE\t\t(INTEL_PMC_IDX_FIXED + 16)\n#define INTEL_PMC_IDX_TD_RETIRING\t\t(INTEL_PMC_IDX_METRIC_BASE + 0)\n#define INTEL_PMC_IDX_TD_BAD_SPEC\t\t(INTEL_PMC_IDX_METRIC_BASE + 1)\n#define INTEL_PMC_IDX_TD_FE_BOUND\t\t(INTEL_PMC_IDX_METRIC_BASE + 2)\n#define INTEL_PMC_IDX_TD_BE_BOUND\t\t(INTEL_PMC_IDX_METRIC_BASE + 3)\n#define INTEL_PMC_IDX_TD_HEAVY_OPS\t\t(INTEL_PMC_IDX_METRIC_BASE + 4)\n#define INTEL_PMC_IDX_TD_BR_MISPREDICT\t\t(INTEL_PMC_IDX_METRIC_BASE + 5)\n#define INTEL_PMC_IDX_TD_FETCH_LAT\t\t(INTEL_PMC_IDX_METRIC_BASE + 6)\n#define INTEL_PMC_IDX_TD_MEM_BOUND\t\t(INTEL_PMC_IDX_METRIC_BASE + 7)\n#define INTEL_PMC_IDX_METRIC_END\t\tINTEL_PMC_IDX_TD_MEM_BOUND\n#define INTEL_PMC_MSK_TOPDOWN\t\t\t((0xffull << INTEL_PMC_IDX_METRIC_BASE) | \\\n\t\t\t\t\t\tINTEL_PMC_MSK_FIXED_SLOTS)\n\n \n#define INTEL_TD_SLOTS\t\t\t\t0x0400\t \n \n#define INTEL_TD_METRIC_RETIRING\t\t0x8000\t \n#define INTEL_TD_METRIC_BAD_SPEC\t\t0x8100\t \n#define INTEL_TD_METRIC_FE_BOUND\t\t0x8200\t \n#define INTEL_TD_METRIC_BE_BOUND\t\t0x8300\t \n \n#define INTEL_TD_METRIC_HEAVY_OPS\t\t0x8400   \n#define INTEL_TD_METRIC_BR_MISPREDICT\t\t0x8500   \n#define INTEL_TD_METRIC_FETCH_LAT\t\t0x8600   \n#define INTEL_TD_METRIC_MEM_BOUND\t\t0x8700   \n\n#define INTEL_TD_METRIC_MAX\t\t\tINTEL_TD_METRIC_MEM_BOUND\n#define INTEL_TD_METRIC_NUM\t\t\t8\n\nstatic inline bool is_metric_idx(int idx)\n{\n\treturn (unsigned)(idx - INTEL_PMC_IDX_METRIC_BASE) < INTEL_TD_METRIC_NUM;\n}\n\nstatic inline bool is_topdown_idx(int idx)\n{\n\treturn is_metric_idx(idx) || idx == INTEL_PMC_IDX_FIXED_SLOTS;\n}\n\n#define INTEL_PMC_OTHER_TOPDOWN_BITS(bit)\t\\\n\t\t\t(~(0x1ull << bit) & INTEL_PMC_MSK_TOPDOWN)\n\n#define GLOBAL_STATUS_COND_CHG\t\t\tBIT_ULL(63)\n#define GLOBAL_STATUS_BUFFER_OVF_BIT\t\t62\n#define GLOBAL_STATUS_BUFFER_OVF\t\tBIT_ULL(GLOBAL_STATUS_BUFFER_OVF_BIT)\n#define GLOBAL_STATUS_UNC_OVF\t\t\tBIT_ULL(61)\n#define GLOBAL_STATUS_ASIF\t\t\tBIT_ULL(60)\n#define GLOBAL_STATUS_COUNTERS_FROZEN\t\tBIT_ULL(59)\n#define GLOBAL_STATUS_LBRS_FROZEN_BIT\t\t58\n#define GLOBAL_STATUS_LBRS_FROZEN\t\tBIT_ULL(GLOBAL_STATUS_LBRS_FROZEN_BIT)\n#define GLOBAL_STATUS_TRACE_TOPAPMI_BIT\t\t55\n#define GLOBAL_STATUS_TRACE_TOPAPMI\t\tBIT_ULL(GLOBAL_STATUS_TRACE_TOPAPMI_BIT)\n#define GLOBAL_STATUS_PERF_METRICS_OVF_BIT\t48\n\n#define GLOBAL_CTRL_EN_PERF_METRICS\t\t48\n \n#define INTEL_PMC_IDX_FIXED_VLBR\t(GLOBAL_STATUS_LBRS_FROZEN_BIT)\n\n \n#define INTEL_FIXED_VLBR_EVENT\t0x1b00\n\n \n\nstruct pebs_basic {\n\tu64 format_size;\n\tu64 ip;\n\tu64 applicable_counters;\n\tu64 tsc;\n};\n\nstruct pebs_meminfo {\n\tu64 address;\n\tu64 aux;\n\tu64 latency;\n\tu64 tsx_tuning;\n};\n\nstruct pebs_gprs {\n\tu64 flags, ip, ax, cx, dx, bx, sp, bp, si, di;\n\tu64 r8, r9, r10, r11, r12, r13, r14, r15;\n};\n\nstruct pebs_xmm {\n\tu64 xmm[16*2];\t \n};\n\n \n#define EXT_PERFMON_DEBUG_FEATURES\t\t0x80000022\n\n \n\n#define IBS_CPUID_FEATURES\t\t0x8000001b\n\n \n#define IBS_CAPS_AVAIL\t\t\t(1U<<0)\n#define IBS_CAPS_FETCHSAM\t\t(1U<<1)\n#define IBS_CAPS_OPSAM\t\t\t(1U<<2)\n#define IBS_CAPS_RDWROPCNT\t\t(1U<<3)\n#define IBS_CAPS_OPCNT\t\t\t(1U<<4)\n#define IBS_CAPS_BRNTRGT\t\t(1U<<5)\n#define IBS_CAPS_OPCNTEXT\t\t(1U<<6)\n#define IBS_CAPS_RIPINVALIDCHK\t\t(1U<<7)\n#define IBS_CAPS_OPBRNFUSE\t\t(1U<<8)\n#define IBS_CAPS_FETCHCTLEXTD\t\t(1U<<9)\n#define IBS_CAPS_OPDATA4\t\t(1U<<10)\n#define IBS_CAPS_ZEN4\t\t\t(1U<<11)\n\n#define IBS_CAPS_DEFAULT\t\t(IBS_CAPS_AVAIL\t\t\\\n\t\t\t\t\t | IBS_CAPS_FETCHSAM\t\\\n\t\t\t\t\t | IBS_CAPS_OPSAM)\n\n \n#define IBSCTL\t\t\t\t0x1cc\n#define IBSCTL_LVT_OFFSET_VALID\t\t(1ULL<<8)\n#define IBSCTL_LVT_OFFSET_MASK\t\t0x0F\n\n \n#define IBS_FETCH_L3MISSONLY\t(1ULL<<59)\n#define IBS_FETCH_RAND_EN\t(1ULL<<57)\n#define IBS_FETCH_VAL\t\t(1ULL<<49)\n#define IBS_FETCH_ENABLE\t(1ULL<<48)\n#define IBS_FETCH_CNT\t\t0xFFFF0000ULL\n#define IBS_FETCH_MAX_CNT\t0x0000FFFFULL\n\n \n#define IBS_OP_CUR_CNT\t\t(0xFFF80ULL<<32)\n#define IBS_OP_CUR_CNT_RAND\t(0x0007FULL<<32)\n#define IBS_OP_CNT_CTL\t\t(1ULL<<19)\n#define IBS_OP_VAL\t\t(1ULL<<18)\n#define IBS_OP_ENABLE\t\t(1ULL<<17)\n#define IBS_OP_L3MISSONLY\t(1ULL<<16)\n#define IBS_OP_MAX_CNT\t\t0x0000FFFFULL\n#define IBS_OP_MAX_CNT_EXT\t0x007FFFFFULL\t \n#define IBS_OP_MAX_CNT_EXT_MASK\t(0x7FULL<<20)\t \n#define IBS_RIP_INVALID\t\t(1ULL<<38)\n\n#ifdef CONFIG_X86_LOCAL_APIC\nextern u32 get_ibs_caps(void);\nextern int forward_event_to_ibs(struct perf_event *event);\n#else\nstatic inline u32 get_ibs_caps(void) { return 0; }\nstatic inline int forward_event_to_ibs(struct perf_event *event) { return -ENOENT; }\n#endif\n\n#ifdef CONFIG_PERF_EVENTS\nextern void perf_events_lapic_init(void);\n\n \n#define PERF_EFLAGS_EXACT\t(1UL << 3)\n#define PERF_EFLAGS_VM\t\t(1UL << 5)\n\nstruct pt_regs;\nstruct x86_perf_regs {\n\tstruct pt_regs\tregs;\n\tu64\t\t*xmm_regs;\n};\n\nextern unsigned long perf_instruction_pointer(struct pt_regs *regs);\nextern unsigned long perf_misc_flags(struct pt_regs *regs);\n#define perf_misc_flags(regs)\tperf_misc_flags(regs)\n\n#include <asm/stacktrace.h>\n\n \n#define perf_arch_fetch_caller_regs(regs, __ip)\t\t{\t\\\n\t(regs)->ip = (__ip);\t\t\t\t\t\\\n\t(regs)->sp = (unsigned long)__builtin_frame_address(0);\t\\\n\t(regs)->cs = __KERNEL_CS;\t\t\t\t\\\n\tregs->flags = 0;\t\t\t\t\t\\\n}\n\nstruct perf_guest_switch_msr {\n\tunsigned msr;\n\tu64 host, guest;\n};\n\nstruct x86_pmu_lbr {\n\tunsigned int\tnr;\n\tunsigned int\tfrom;\n\tunsigned int\tto;\n\tunsigned int\tinfo;\n};\n\nextern void perf_get_x86_pmu_capability(struct x86_pmu_capability *cap);\nextern u64 perf_get_hw_event_config(int hw_event);\nextern void perf_check_microcode(void);\nextern void perf_clear_dirty_counters(void);\nextern int x86_perf_rdpmc_index(struct perf_event *event);\n#else\nstatic inline void perf_get_x86_pmu_capability(struct x86_pmu_capability *cap)\n{\n\tmemset(cap, 0, sizeof(*cap));\n}\n\nstatic inline u64 perf_get_hw_event_config(int hw_event)\n{\n\treturn 0;\n}\n\nstatic inline void perf_events_lapic_init(void)\t{ }\nstatic inline void perf_check_microcode(void) { }\n#endif\n\n#if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_CPU_SUP_INTEL)\nextern struct perf_guest_switch_msr *perf_guest_get_msrs(int *nr, void *data);\nextern void x86_perf_get_lbr(struct x86_pmu_lbr *lbr);\n#else\nstruct perf_guest_switch_msr *perf_guest_get_msrs(int *nr, void *data);\nstatic inline void x86_perf_get_lbr(struct x86_pmu_lbr *lbr)\n{\n\tmemset(lbr, 0, sizeof(*lbr));\n}\n#endif\n\n#ifdef CONFIG_CPU_SUP_INTEL\n extern void intel_pt_handle_vmx(int on);\n#else\nstatic inline void intel_pt_handle_vmx(int on)\n{\n\n}\n#endif\n\n#if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_CPU_SUP_AMD)\n extern void amd_pmu_enable_virt(void);\n extern void amd_pmu_disable_virt(void);\n\n#if defined(CONFIG_PERF_EVENTS_AMD_BRS)\n\n#define PERF_NEEDS_LOPWR_CB 1\n\n \nextern void perf_amd_brs_lopwr_cb(bool lopwr_in);\n\nDECLARE_STATIC_CALL(perf_lopwr_cb, perf_amd_brs_lopwr_cb);\n\nstatic __always_inline void perf_lopwr_cb(bool lopwr_in)\n{\n\tstatic_call_mod(perf_lopwr_cb)(lopwr_in);\n}\n\n#endif  \n\n#else\n static inline void amd_pmu_enable_virt(void) { }\n static inline void amd_pmu_disable_virt(void) { }\n#endif\n\n#define arch_perf_out_copy_user copy_from_user_nmi\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}