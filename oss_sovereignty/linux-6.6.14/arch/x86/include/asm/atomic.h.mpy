{
  "module_name": "atomic.h",
  "hash_id": "258d1cfacc1a1453464341ef64d5a6f22da5fb1a38910f84da6aa5b9062ec4a7",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/include/asm/atomic.h",
  "human_readable_source": " \n#ifndef _ASM_X86_ATOMIC_H\n#define _ASM_X86_ATOMIC_H\n\n#include <linux/compiler.h>\n#include <linux/types.h>\n#include <asm/alternative.h>\n#include <asm/cmpxchg.h>\n#include <asm/rmwcc.h>\n#include <asm/barrier.h>\n\n \n\nstatic __always_inline int arch_atomic_read(const atomic_t *v)\n{\n\t \n\treturn __READ_ONCE((v)->counter);\n}\n\nstatic __always_inline void arch_atomic_set(atomic_t *v, int i)\n{\n\t__WRITE_ONCE(v->counter, i);\n}\n\nstatic __always_inline void arch_atomic_add(int i, atomic_t *v)\n{\n\tasm volatile(LOCK_PREFIX \"addl %1,%0\"\n\t\t     : \"+m\" (v->counter)\n\t\t     : \"ir\" (i) : \"memory\");\n}\n\nstatic __always_inline void arch_atomic_sub(int i, atomic_t *v)\n{\n\tasm volatile(LOCK_PREFIX \"subl %1,%0\"\n\t\t     : \"+m\" (v->counter)\n\t\t     : \"ir\" (i) : \"memory\");\n}\n\nstatic __always_inline bool arch_atomic_sub_and_test(int i, atomic_t *v)\n{\n\treturn GEN_BINARY_RMWcc(LOCK_PREFIX \"subl\", v->counter, e, \"er\", i);\n}\n#define arch_atomic_sub_and_test arch_atomic_sub_and_test\n\nstatic __always_inline void arch_atomic_inc(atomic_t *v)\n{\n\tasm volatile(LOCK_PREFIX \"incl %0\"\n\t\t     : \"+m\" (v->counter) :: \"memory\");\n}\n#define arch_atomic_inc arch_atomic_inc\n\nstatic __always_inline void arch_atomic_dec(atomic_t *v)\n{\n\tasm volatile(LOCK_PREFIX \"decl %0\"\n\t\t     : \"+m\" (v->counter) :: \"memory\");\n}\n#define arch_atomic_dec arch_atomic_dec\n\nstatic __always_inline bool arch_atomic_dec_and_test(atomic_t *v)\n{\n\treturn GEN_UNARY_RMWcc(LOCK_PREFIX \"decl\", v->counter, e);\n}\n#define arch_atomic_dec_and_test arch_atomic_dec_and_test\n\nstatic __always_inline bool arch_atomic_inc_and_test(atomic_t *v)\n{\n\treturn GEN_UNARY_RMWcc(LOCK_PREFIX \"incl\", v->counter, e);\n}\n#define arch_atomic_inc_and_test arch_atomic_inc_and_test\n\nstatic __always_inline bool arch_atomic_add_negative(int i, atomic_t *v)\n{\n\treturn GEN_BINARY_RMWcc(LOCK_PREFIX \"addl\", v->counter, s, \"er\", i);\n}\n#define arch_atomic_add_negative arch_atomic_add_negative\n\nstatic __always_inline int arch_atomic_add_return(int i, atomic_t *v)\n{\n\treturn i + xadd(&v->counter, i);\n}\n#define arch_atomic_add_return arch_atomic_add_return\n\nstatic __always_inline int arch_atomic_sub_return(int i, atomic_t *v)\n{\n\treturn arch_atomic_add_return(-i, v);\n}\n#define arch_atomic_sub_return arch_atomic_sub_return\n\nstatic __always_inline int arch_atomic_fetch_add(int i, atomic_t *v)\n{\n\treturn xadd(&v->counter, i);\n}\n#define arch_atomic_fetch_add arch_atomic_fetch_add\n\nstatic __always_inline int arch_atomic_fetch_sub(int i, atomic_t *v)\n{\n\treturn xadd(&v->counter, -i);\n}\n#define arch_atomic_fetch_sub arch_atomic_fetch_sub\n\nstatic __always_inline int arch_atomic_cmpxchg(atomic_t *v, int old, int new)\n{\n\treturn arch_cmpxchg(&v->counter, old, new);\n}\n#define arch_atomic_cmpxchg arch_atomic_cmpxchg\n\nstatic __always_inline bool arch_atomic_try_cmpxchg(atomic_t *v, int *old, int new)\n{\n\treturn arch_try_cmpxchg(&v->counter, old, new);\n}\n#define arch_atomic_try_cmpxchg arch_atomic_try_cmpxchg\n\nstatic __always_inline int arch_atomic_xchg(atomic_t *v, int new)\n{\n\treturn arch_xchg(&v->counter, new);\n}\n#define arch_atomic_xchg arch_atomic_xchg\n\nstatic __always_inline void arch_atomic_and(int i, atomic_t *v)\n{\n\tasm volatile(LOCK_PREFIX \"andl %1,%0\"\n\t\t\t: \"+m\" (v->counter)\n\t\t\t: \"ir\" (i)\n\t\t\t: \"memory\");\n}\n\nstatic __always_inline int arch_atomic_fetch_and(int i, atomic_t *v)\n{\n\tint val = arch_atomic_read(v);\n\n\tdo { } while (!arch_atomic_try_cmpxchg(v, &val, val & i));\n\n\treturn val;\n}\n#define arch_atomic_fetch_and arch_atomic_fetch_and\n\nstatic __always_inline void arch_atomic_or(int i, atomic_t *v)\n{\n\tasm volatile(LOCK_PREFIX \"orl %1,%0\"\n\t\t\t: \"+m\" (v->counter)\n\t\t\t: \"ir\" (i)\n\t\t\t: \"memory\");\n}\n\nstatic __always_inline int arch_atomic_fetch_or(int i, atomic_t *v)\n{\n\tint val = arch_atomic_read(v);\n\n\tdo { } while (!arch_atomic_try_cmpxchg(v, &val, val | i));\n\n\treturn val;\n}\n#define arch_atomic_fetch_or arch_atomic_fetch_or\n\nstatic __always_inline void arch_atomic_xor(int i, atomic_t *v)\n{\n\tasm volatile(LOCK_PREFIX \"xorl %1,%0\"\n\t\t\t: \"+m\" (v->counter)\n\t\t\t: \"ir\" (i)\n\t\t\t: \"memory\");\n}\n\nstatic __always_inline int arch_atomic_fetch_xor(int i, atomic_t *v)\n{\n\tint val = arch_atomic_read(v);\n\n\tdo { } while (!arch_atomic_try_cmpxchg(v, &val, val ^ i));\n\n\treturn val;\n}\n#define arch_atomic_fetch_xor arch_atomic_fetch_xor\n\n#ifdef CONFIG_X86_32\n# include <asm/atomic64_32.h>\n#else\n# include <asm/atomic64_64.h>\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}