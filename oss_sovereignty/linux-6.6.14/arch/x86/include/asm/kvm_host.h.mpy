{
  "module_name": "kvm_host.h",
  "hash_id": "d8ba232c9d459fff54023c01b1b549aa639eee4f3537cbcd2514f9a02cec8236",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/include/asm/kvm_host.h",
  "human_readable_source": " \n \n\n#ifndef _ASM_X86_KVM_HOST_H\n#define _ASM_X86_KVM_HOST_H\n\n#include <linux/types.h>\n#include <linux/mm.h>\n#include <linux/mmu_notifier.h>\n#include <linux/tracepoint.h>\n#include <linux/cpumask.h>\n#include <linux/irq_work.h>\n#include <linux/irq.h>\n#include <linux/workqueue.h>\n\n#include <linux/kvm.h>\n#include <linux/kvm_para.h>\n#include <linux/kvm_types.h>\n#include <linux/perf_event.h>\n#include <linux/pvclock_gtod.h>\n#include <linux/clocksource.h>\n#include <linux/irqbypass.h>\n#include <linux/hyperv.h>\n#include <linux/kfifo.h>\n\n#include <asm/apic.h>\n#include <asm/pvclock-abi.h>\n#include <asm/desc.h>\n#include <asm/mtrr.h>\n#include <asm/msr-index.h>\n#include <asm/asm.h>\n#include <asm/kvm_page_track.h>\n#include <asm/kvm_vcpu_regs.h>\n#include <asm/hyperv-tlfs.h>\n\n#define __KVM_HAVE_ARCH_VCPU_DEBUGFS\n\n#define KVM_MAX_VCPUS 1024\n\n \n#define KVM_VCPU_ID_RATIO 4\n#define KVM_MAX_VCPU_IDS (KVM_MAX_VCPUS * KVM_VCPU_ID_RATIO)\n\n \n#define KVM_INTERNAL_MEM_SLOTS 3\n\n#define KVM_HALT_POLL_NS_DEFAULT 200000\n\n#define KVM_IRQCHIP_NUM_PINS  KVM_IOAPIC_NUM_PINS\n\n#define KVM_DIRTY_LOG_MANUAL_CAPS   (KVM_DIRTY_LOG_MANUAL_PROTECT_ENABLE | \\\n\t\t\t\t\tKVM_DIRTY_LOG_INITIALLY_SET)\n\n#define KVM_BUS_LOCK_DETECTION_VALID_MODE\t(KVM_BUS_LOCK_DETECTION_OFF | \\\n\t\t\t\t\t\t KVM_BUS_LOCK_DETECTION_EXIT)\n\n#define KVM_X86_NOTIFY_VMEXIT_VALID_BITS\t(KVM_X86_NOTIFY_VMEXIT_ENABLED | \\\n\t\t\t\t\t\t KVM_X86_NOTIFY_VMEXIT_USER)\n\n \n#define KVM_REQ_MIGRATE_TIMER\t\tKVM_ARCH_REQ(0)\n#define KVM_REQ_REPORT_TPR_ACCESS\tKVM_ARCH_REQ(1)\n#define KVM_REQ_TRIPLE_FAULT\t\tKVM_ARCH_REQ(2)\n#define KVM_REQ_MMU_SYNC\t\tKVM_ARCH_REQ(3)\n#define KVM_REQ_CLOCK_UPDATE\t\tKVM_ARCH_REQ(4)\n#define KVM_REQ_LOAD_MMU_PGD\t\tKVM_ARCH_REQ(5)\n#define KVM_REQ_EVENT\t\t\tKVM_ARCH_REQ(6)\n#define KVM_REQ_APF_HALT\t\tKVM_ARCH_REQ(7)\n#define KVM_REQ_STEAL_UPDATE\t\tKVM_ARCH_REQ(8)\n#define KVM_REQ_NMI\t\t\tKVM_ARCH_REQ(9)\n#define KVM_REQ_PMU\t\t\tKVM_ARCH_REQ(10)\n#define KVM_REQ_PMI\t\t\tKVM_ARCH_REQ(11)\n#ifdef CONFIG_KVM_SMM\n#define KVM_REQ_SMI\t\t\tKVM_ARCH_REQ(12)\n#endif\n#define KVM_REQ_MASTERCLOCK_UPDATE\tKVM_ARCH_REQ(13)\n#define KVM_REQ_MCLOCK_INPROGRESS \\\n\tKVM_ARCH_REQ_FLAGS(14, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)\n#define KVM_REQ_SCAN_IOAPIC \\\n\tKVM_ARCH_REQ_FLAGS(15, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)\n#define KVM_REQ_GLOBAL_CLOCK_UPDATE\tKVM_ARCH_REQ(16)\n#define KVM_REQ_APIC_PAGE_RELOAD \\\n\tKVM_ARCH_REQ_FLAGS(17, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)\n#define KVM_REQ_HV_CRASH\t\tKVM_ARCH_REQ(18)\n#define KVM_REQ_IOAPIC_EOI_EXIT\t\tKVM_ARCH_REQ(19)\n#define KVM_REQ_HV_RESET\t\tKVM_ARCH_REQ(20)\n#define KVM_REQ_HV_EXIT\t\t\tKVM_ARCH_REQ(21)\n#define KVM_REQ_HV_STIMER\t\tKVM_ARCH_REQ(22)\n#define KVM_REQ_LOAD_EOI_EXITMAP\tKVM_ARCH_REQ(23)\n#define KVM_REQ_GET_NESTED_STATE_PAGES\tKVM_ARCH_REQ(24)\n#define KVM_REQ_APICV_UPDATE \\\n\tKVM_ARCH_REQ_FLAGS(25, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)\n#define KVM_REQ_TLB_FLUSH_CURRENT\tKVM_ARCH_REQ(26)\n#define KVM_REQ_TLB_FLUSH_GUEST \\\n\tKVM_ARCH_REQ_FLAGS(27, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)\n#define KVM_REQ_APF_READY\t\tKVM_ARCH_REQ(28)\n#define KVM_REQ_MSR_FILTER_CHANGED\tKVM_ARCH_REQ(29)\n#define KVM_REQ_UPDATE_CPU_DIRTY_LOGGING \\\n\tKVM_ARCH_REQ_FLAGS(30, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)\n#define KVM_REQ_MMU_FREE_OBSOLETE_ROOTS \\\n\tKVM_ARCH_REQ_FLAGS(31, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)\n#define KVM_REQ_HV_TLB_FLUSH \\\n\tKVM_ARCH_REQ_FLAGS(32, KVM_REQUEST_WAIT | KVM_REQUEST_NO_WAKEUP)\n\n#define CR0_RESERVED_BITS                                               \\\n\t(~(unsigned long)(X86_CR0_PE | X86_CR0_MP | X86_CR0_EM | X86_CR0_TS \\\n\t\t\t  | X86_CR0_ET | X86_CR0_NE | X86_CR0_WP | X86_CR0_AM \\\n\t\t\t  | X86_CR0_NW | X86_CR0_CD | X86_CR0_PG))\n\n#define CR4_RESERVED_BITS                                               \\\n\t(~(unsigned long)(X86_CR4_VME | X86_CR4_PVI | X86_CR4_TSD | X86_CR4_DE\\\n\t\t\t  | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_MCE     \\\n\t\t\t  | X86_CR4_PGE | X86_CR4_PCE | X86_CR4_OSFXSR | X86_CR4_PCIDE \\\n\t\t\t  | X86_CR4_OSXSAVE | X86_CR4_SMEP | X86_CR4_FSGSBASE \\\n\t\t\t  | X86_CR4_OSXMMEXCPT | X86_CR4_LA57 | X86_CR4_VMXE \\\n\t\t\t  | X86_CR4_SMAP | X86_CR4_PKE | X86_CR4_UMIP))\n\n#define CR8_RESERVED_BITS (~(unsigned long)X86_CR8_TPR)\n\n\n\n#define INVALID_PAGE (~(hpa_t)0)\n#define VALID_PAGE(x) ((x) != INVALID_PAGE)\n\n \n#define KVM_MAX_HUGEPAGE_LEVEL\tPG_LEVEL_1G\n#define KVM_NR_PAGE_SIZES\t(KVM_MAX_HUGEPAGE_LEVEL - PG_LEVEL_4K + 1)\n#define KVM_HPAGE_GFN_SHIFT(x)\t(((x) - 1) * 9)\n#define KVM_HPAGE_SHIFT(x)\t(PAGE_SHIFT + KVM_HPAGE_GFN_SHIFT(x))\n#define KVM_HPAGE_SIZE(x)\t(1UL << KVM_HPAGE_SHIFT(x))\n#define KVM_HPAGE_MASK(x)\t(~(KVM_HPAGE_SIZE(x) - 1))\n#define KVM_PAGES_PER_HPAGE(x)\t(KVM_HPAGE_SIZE(x) / PAGE_SIZE)\n\n#define KVM_MEMSLOT_PAGES_TO_MMU_PAGES_RATIO 50\n#define KVM_MIN_ALLOC_MMU_PAGES 64UL\n#define KVM_MMU_HASH_SHIFT 12\n#define KVM_NUM_MMU_PAGES (1 << KVM_MMU_HASH_SHIFT)\n#define KVM_MIN_FREE_MMU_PAGES 5\n#define KVM_REFILL_PAGES 25\n#define KVM_MAX_CPUID_ENTRIES 256\n#define KVM_NR_FIXED_MTRR_REGION 88\n#define KVM_NR_VAR_MTRR 8\n\n#define ASYNC_PF_PER_VCPU 64\n\nenum kvm_reg {\n\tVCPU_REGS_RAX = __VCPU_REGS_RAX,\n\tVCPU_REGS_RCX = __VCPU_REGS_RCX,\n\tVCPU_REGS_RDX = __VCPU_REGS_RDX,\n\tVCPU_REGS_RBX = __VCPU_REGS_RBX,\n\tVCPU_REGS_RSP = __VCPU_REGS_RSP,\n\tVCPU_REGS_RBP = __VCPU_REGS_RBP,\n\tVCPU_REGS_RSI = __VCPU_REGS_RSI,\n\tVCPU_REGS_RDI = __VCPU_REGS_RDI,\n#ifdef CONFIG_X86_64\n\tVCPU_REGS_R8  = __VCPU_REGS_R8,\n\tVCPU_REGS_R9  = __VCPU_REGS_R9,\n\tVCPU_REGS_R10 = __VCPU_REGS_R10,\n\tVCPU_REGS_R11 = __VCPU_REGS_R11,\n\tVCPU_REGS_R12 = __VCPU_REGS_R12,\n\tVCPU_REGS_R13 = __VCPU_REGS_R13,\n\tVCPU_REGS_R14 = __VCPU_REGS_R14,\n\tVCPU_REGS_R15 = __VCPU_REGS_R15,\n#endif\n\tVCPU_REGS_RIP,\n\tNR_VCPU_REGS,\n\n\tVCPU_EXREG_PDPTR = NR_VCPU_REGS,\n\tVCPU_EXREG_CR0,\n\tVCPU_EXREG_CR3,\n\tVCPU_EXREG_CR4,\n\tVCPU_EXREG_RFLAGS,\n\tVCPU_EXREG_SEGMENTS,\n\tVCPU_EXREG_EXIT_INFO_1,\n\tVCPU_EXREG_EXIT_INFO_2,\n};\n\nenum {\n\tVCPU_SREG_ES,\n\tVCPU_SREG_CS,\n\tVCPU_SREG_SS,\n\tVCPU_SREG_DS,\n\tVCPU_SREG_FS,\n\tVCPU_SREG_GS,\n\tVCPU_SREG_TR,\n\tVCPU_SREG_LDTR,\n};\n\nenum exit_fastpath_completion {\n\tEXIT_FASTPATH_NONE,\n\tEXIT_FASTPATH_REENTER_GUEST,\n\tEXIT_FASTPATH_EXIT_HANDLED,\n};\ntypedef enum exit_fastpath_completion fastpath_t;\n\nstruct x86_emulate_ctxt;\nstruct x86_exception;\nunion kvm_smram;\nenum x86_intercept;\nenum x86_intercept_stage;\n\n#define KVM_NR_DB_REGS\t4\n\n#define DR6_BUS_LOCK   (1 << 11)\n#define DR6_BD\t\t(1 << 13)\n#define DR6_BS\t\t(1 << 14)\n#define DR6_BT\t\t(1 << 15)\n#define DR6_RTM\t\t(1 << 16)\n \n#define DR6_ACTIVE_LOW\t0xffff0ff0\n#define DR6_VOLATILE\t0x0001e80f\n#define DR6_FIXED_1\t(DR6_ACTIVE_LOW & ~DR6_VOLATILE)\n\n#define DR7_BP_EN_MASK\t0x000000ff\n#define DR7_GE\t\t(1 << 9)\n#define DR7_GD\t\t(1 << 13)\n#define DR7_FIXED_1\t0x00000400\n#define DR7_VOLATILE\t0xffff2bff\n\n#define KVM_GUESTDBG_VALID_MASK \\\n\t(KVM_GUESTDBG_ENABLE | \\\n\tKVM_GUESTDBG_SINGLESTEP | \\\n\tKVM_GUESTDBG_USE_HW_BP | \\\n\tKVM_GUESTDBG_USE_SW_BP | \\\n\tKVM_GUESTDBG_INJECT_BP | \\\n\tKVM_GUESTDBG_INJECT_DB | \\\n\tKVM_GUESTDBG_BLOCKIRQ)\n\n\n#define PFERR_PRESENT_BIT 0\n#define PFERR_WRITE_BIT 1\n#define PFERR_USER_BIT 2\n#define PFERR_RSVD_BIT 3\n#define PFERR_FETCH_BIT 4\n#define PFERR_PK_BIT 5\n#define PFERR_SGX_BIT 15\n#define PFERR_GUEST_FINAL_BIT 32\n#define PFERR_GUEST_PAGE_BIT 33\n#define PFERR_IMPLICIT_ACCESS_BIT 48\n\n#define PFERR_PRESENT_MASK\tBIT(PFERR_PRESENT_BIT)\n#define PFERR_WRITE_MASK\tBIT(PFERR_WRITE_BIT)\n#define PFERR_USER_MASK\t\tBIT(PFERR_USER_BIT)\n#define PFERR_RSVD_MASK\t\tBIT(PFERR_RSVD_BIT)\n#define PFERR_FETCH_MASK\tBIT(PFERR_FETCH_BIT)\n#define PFERR_PK_MASK\t\tBIT(PFERR_PK_BIT)\n#define PFERR_SGX_MASK\t\tBIT(PFERR_SGX_BIT)\n#define PFERR_GUEST_FINAL_MASK\tBIT_ULL(PFERR_GUEST_FINAL_BIT)\n#define PFERR_GUEST_PAGE_MASK\tBIT_ULL(PFERR_GUEST_PAGE_BIT)\n#define PFERR_IMPLICIT_ACCESS\tBIT_ULL(PFERR_IMPLICIT_ACCESS_BIT)\n\n#define PFERR_NESTED_GUEST_PAGE (PFERR_GUEST_PAGE_MASK |\t\\\n\t\t\t\t PFERR_WRITE_MASK |\t\t\\\n\t\t\t\t PFERR_PRESENT_MASK)\n\n \n#define KVM_APIC_CHECK_VAPIC\t0\n \n#define KVM_APIC_PV_EOI_PENDING\t1\n\nstruct kvm_kernel_irq_routing_entry;\n\n \nunion kvm_mmu_page_role {\n\tu32 word;\n\tstruct {\n\t\tunsigned level:4;\n\t\tunsigned has_4_byte_gpte:1;\n\t\tunsigned quadrant:2;\n\t\tunsigned direct:1;\n\t\tunsigned access:3;\n\t\tunsigned invalid:1;\n\t\tunsigned efer_nx:1;\n\t\tunsigned cr0_wp:1;\n\t\tunsigned smep_andnot_wp:1;\n\t\tunsigned smap_andnot_wp:1;\n\t\tunsigned ad_disabled:1;\n\t\tunsigned guest_mode:1;\n\t\tunsigned passthrough:1;\n\t\tunsigned :5;\n\n\t\t \n\t\tunsigned smm:8;\n\t};\n};\n\n \nunion kvm_mmu_extended_role {\n\tu32 word;\n\tstruct {\n\t\tunsigned int valid:1;\n\t\tunsigned int execonly:1;\n\t\tunsigned int cr4_pse:1;\n\t\tunsigned int cr4_pke:1;\n\t\tunsigned int cr4_smap:1;\n\t\tunsigned int cr4_smep:1;\n\t\tunsigned int cr4_la57:1;\n\t\tunsigned int efer_lma:1;\n\t};\n};\n\nunion kvm_cpu_role {\n\tu64 as_u64;\n\tstruct {\n\t\tunion kvm_mmu_page_role base;\n\t\tunion kvm_mmu_extended_role ext;\n\t};\n};\n\nstruct kvm_rmap_head {\n\tunsigned long val;\n};\n\nstruct kvm_pio_request {\n\tunsigned long linear_rip;\n\tunsigned long count;\n\tint in;\n\tint port;\n\tint size;\n};\n\n#define PT64_ROOT_MAX_LEVEL 5\n\nstruct rsvd_bits_validate {\n\tu64 rsvd_bits_mask[2][PT64_ROOT_MAX_LEVEL];\n\tu64 bad_mt_xwr;\n};\n\nstruct kvm_mmu_root_info {\n\tgpa_t pgd;\n\thpa_t hpa;\n};\n\n#define KVM_MMU_ROOT_INFO_INVALID \\\n\t((struct kvm_mmu_root_info) { .pgd = INVALID_PAGE, .hpa = INVALID_PAGE })\n\n#define KVM_MMU_NUM_PREV_ROOTS 3\n\n#define KVM_MMU_ROOT_CURRENT\t\tBIT(0)\n#define KVM_MMU_ROOT_PREVIOUS(i)\tBIT(1+i)\n#define KVM_MMU_ROOTS_ALL\t\t(BIT(1 + KVM_MMU_NUM_PREV_ROOTS) - 1)\n\n#define KVM_HAVE_MMU_RWLOCK\n\nstruct kvm_mmu_page;\nstruct kvm_page_fault;\n\n \nstruct kvm_mmu {\n\tunsigned long (*get_guest_pgd)(struct kvm_vcpu *vcpu);\n\tu64 (*get_pdptr)(struct kvm_vcpu *vcpu, int index);\n\tint (*page_fault)(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault);\n\tvoid (*inject_page_fault)(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct x86_exception *fault);\n\tgpa_t (*gva_to_gpa)(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,\n\t\t\t    gpa_t gva_or_gpa, u64 access,\n\t\t\t    struct x86_exception *exception);\n\tint (*sync_spte)(struct kvm_vcpu *vcpu,\n\t\t\t struct kvm_mmu_page *sp, int i);\n\tstruct kvm_mmu_root_info root;\n\tunion kvm_cpu_role cpu_role;\n\tunion kvm_mmu_page_role root_role;\n\n\t \n\tu32 pkru_mask;\n\n\tstruct kvm_mmu_root_info prev_roots[KVM_MMU_NUM_PREV_ROOTS];\n\n\t \n\tu8 permissions[16];\n\n\tu64 *pae_root;\n\tu64 *pml4_root;\n\tu64 *pml5_root;\n\n\t \n\tstruct rsvd_bits_validate shadow_zero_check;\n\n\tstruct rsvd_bits_validate guest_rsvd_check;\n\n\tu64 pdptrs[4];  \n};\n\nenum pmc_type {\n\tKVM_PMC_GP = 0,\n\tKVM_PMC_FIXED,\n};\n\nstruct kvm_pmc {\n\tenum pmc_type type;\n\tu8 idx;\n\tbool is_paused;\n\tbool intr;\n\tu64 counter;\n\tu64 prev_counter;\n\tu64 eventsel;\n\tstruct perf_event *perf_event;\n\tstruct kvm_vcpu *vcpu;\n\t \n\tu64 current_config;\n};\n\n \n#define KVM_INTEL_PMC_MAX_GENERIC\t8\n#define MSR_ARCH_PERFMON_PERFCTR_MAX\t(MSR_ARCH_PERFMON_PERFCTR0 + KVM_INTEL_PMC_MAX_GENERIC - 1)\n#define MSR_ARCH_PERFMON_EVENTSEL_MAX\t(MSR_ARCH_PERFMON_EVENTSEL0 + KVM_INTEL_PMC_MAX_GENERIC - 1)\n#define KVM_PMC_MAX_FIXED\t3\n#define MSR_ARCH_PERFMON_FIXED_CTR_MAX\t(MSR_ARCH_PERFMON_FIXED_CTR0 + KVM_PMC_MAX_FIXED - 1)\n#define KVM_AMD_PMC_MAX_GENERIC\t6\nstruct kvm_pmu {\n\tu8 version;\n\tunsigned nr_arch_gp_counters;\n\tunsigned nr_arch_fixed_counters;\n\tunsigned available_event_types;\n\tu64 fixed_ctr_ctrl;\n\tu64 fixed_ctr_ctrl_mask;\n\tu64 global_ctrl;\n\tu64 global_status;\n\tu64 counter_bitmask[2];\n\tu64 global_ctrl_mask;\n\tu64 global_status_mask;\n\tu64 reserved_bits;\n\tu64 raw_event_mask;\n\tstruct kvm_pmc gp_counters[KVM_INTEL_PMC_MAX_GENERIC];\n\tstruct kvm_pmc fixed_counters[KVM_PMC_MAX_FIXED];\n\n\t \n\tunion {\n\t\tDECLARE_BITMAP(reprogram_pmi, X86_PMC_IDX_MAX);\n\t\tatomic64_t __reprogram_pmi;\n\t};\n\tDECLARE_BITMAP(all_valid_pmc_idx, X86_PMC_IDX_MAX);\n\tDECLARE_BITMAP(pmc_in_use, X86_PMC_IDX_MAX);\n\n\tu64 ds_area;\n\tu64 pebs_enable;\n\tu64 pebs_enable_mask;\n\tu64 pebs_data_cfg;\n\tu64 pebs_data_cfg_mask;\n\n\t \n\tu64 host_cross_mapped_mask;\n\n\t \n\tbool need_cleanup;\n\n\t \n\tu8 event_count;\n};\n\nstruct kvm_pmu_ops;\n\nenum {\n\tKVM_DEBUGREG_BP_ENABLED = 1,\n\tKVM_DEBUGREG_WONT_EXIT = 2,\n};\n\nstruct kvm_mtrr_range {\n\tu64 base;\n\tu64 mask;\n\tstruct list_head node;\n};\n\nstruct kvm_mtrr {\n\tstruct kvm_mtrr_range var_ranges[KVM_NR_VAR_MTRR];\n\tmtrr_type fixed_ranges[KVM_NR_FIXED_MTRR_REGION];\n\tu64 deftype;\n\n\tstruct list_head head;\n};\n\n \nstruct kvm_vcpu_hv_stimer {\n\tstruct hrtimer timer;\n\tint index;\n\tunion hv_stimer_config config;\n\tu64 count;\n\tu64 exp_time;\n\tstruct hv_message msg;\n\tbool msg_pending;\n};\n\n \nstruct kvm_vcpu_hv_synic {\n\tu64 version;\n\tu64 control;\n\tu64 msg_page;\n\tu64 evt_page;\n\tatomic64_t sint[HV_SYNIC_SINT_COUNT];\n\tatomic_t sint_to_gsi[HV_SYNIC_SINT_COUNT];\n\tDECLARE_BITMAP(auto_eoi_bitmap, 256);\n\tDECLARE_BITMAP(vec_bitmap, 256);\n\tbool active;\n\tbool dont_zero_synic_pages;\n};\n\n \n#define KVM_HV_TLB_FLUSH_FIFO_SIZE (16)\n \n#define KVM_HV_TLB_FLUSHALL_ENTRY  ((u64)-1)\n\nenum hv_tlb_flush_fifos {\n\tHV_L1_TLB_FLUSH_FIFO,\n\tHV_L2_TLB_FLUSH_FIFO,\n\tHV_NR_TLB_FLUSH_FIFOS,\n};\n\nstruct kvm_vcpu_hv_tlb_flush_fifo {\n\tspinlock_t write_lock;\n\tDECLARE_KFIFO(entries, u64, KVM_HV_TLB_FLUSH_FIFO_SIZE);\n};\n\n \nstruct kvm_vcpu_hv {\n\tstruct kvm_vcpu *vcpu;\n\tu32 vp_index;\n\tu64 hv_vapic;\n\ts64 runtime_offset;\n\tstruct kvm_vcpu_hv_synic synic;\n\tstruct kvm_hyperv_exit exit;\n\tstruct kvm_vcpu_hv_stimer stimer[HV_SYNIC_STIMER_COUNT];\n\tDECLARE_BITMAP(stimer_pending_bitmap, HV_SYNIC_STIMER_COUNT);\n\tbool enforce_cpuid;\n\tstruct {\n\t\tu32 features_eax;  \n\t\tu32 features_ebx;  \n\t\tu32 features_edx;  \n\t\tu32 enlightenments_eax;  \n\t\tu32 enlightenments_ebx;  \n\t\tu32 syndbg_cap_eax;  \n\t\tu32 nested_eax;  \n\t\tu32 nested_ebx;  \n\t} cpuid_cache;\n\n\tstruct kvm_vcpu_hv_tlb_flush_fifo tlb_flush_fifo[HV_NR_TLB_FLUSH_FIFOS];\n\n\t \n\tu64 sparse_banks[HV_MAX_SPARSE_VCPU_BANKS];\n\n\tstruct hv_vp_assist_page vp_assist_page;\n\n\tstruct {\n\t\tu64 pa_page_gpa;\n\t\tu64 vm_id;\n\t\tu32 vp_id;\n\t} nested;\n};\n\nstruct kvm_hypervisor_cpuid {\n\tu32 base;\n\tu32 limit;\n};\n\n \nstruct kvm_vcpu_xen {\n\tu64 hypercall_rip;\n\tu32 current_runstate;\n\tu8 upcall_vector;\n\tstruct gfn_to_pfn_cache vcpu_info_cache;\n\tstruct gfn_to_pfn_cache vcpu_time_info_cache;\n\tstruct gfn_to_pfn_cache runstate_cache;\n\tstruct gfn_to_pfn_cache runstate2_cache;\n\tu64 last_steal;\n\tu64 runstate_entry_time;\n\tu64 runstate_times[4];\n\tunsigned long evtchn_pending_sel;\n\tu32 vcpu_id;  \n\tu32 timer_virq;\n\tu64 timer_expires;  \n\tatomic_t timer_pending;\n\tstruct hrtimer timer;\n\tint poll_evtchn;\n\tstruct timer_list poll_timer;\n\tstruct kvm_hypervisor_cpuid cpuid;\n};\n\nstruct kvm_queued_exception {\n\tbool pending;\n\tbool injected;\n\tbool has_error_code;\n\tu8 vector;\n\tu32 error_code;\n\tunsigned long payload;\n\tbool has_payload;\n};\n\nstruct kvm_vcpu_arch {\n\t \n\tunsigned long regs[NR_VCPU_REGS];\n\tu32 regs_avail;\n\tu32 regs_dirty;\n\n\tunsigned long cr0;\n\tunsigned long cr0_guest_owned_bits;\n\tunsigned long cr2;\n\tunsigned long cr3;\n\tunsigned long cr4;\n\tunsigned long cr4_guest_owned_bits;\n\tunsigned long cr4_guest_rsvd_bits;\n\tunsigned long cr8;\n\tu32 host_pkru;\n\tu32 pkru;\n\tu32 hflags;\n\tu64 efer;\n\tu64 apic_base;\n\tstruct kvm_lapic *apic;     \n\tbool load_eoi_exitmap_pending;\n\tDECLARE_BITMAP(ioapic_handled_vectors, 256);\n\tunsigned long apic_attention;\n\tint32_t apic_arb_prio;\n\tint mp_state;\n\tu64 ia32_misc_enable_msr;\n\tu64 smbase;\n\tu64 smi_count;\n\tbool at_instruction_boundary;\n\tbool tpr_access_reporting;\n\tbool xfd_no_write_intercept;\n\tu64 ia32_xss;\n\tu64 microcode_version;\n\tu64 arch_capabilities;\n\tu64 perf_capabilities;\n\n\t \n\tstruct kvm_mmu *mmu;\n\n\t \n\tstruct kvm_mmu root_mmu;\n\n\t \n\tstruct kvm_mmu guest_mmu;\n\n\t \n\tstruct kvm_mmu nested_mmu;\n\n\t \n\tstruct kvm_mmu *walk_mmu;\n\n\tstruct kvm_mmu_memory_cache mmu_pte_list_desc_cache;\n\tstruct kvm_mmu_memory_cache mmu_shadow_page_cache;\n\tstruct kvm_mmu_memory_cache mmu_shadowed_info_cache;\n\tstruct kvm_mmu_memory_cache mmu_page_header_cache;\n\n\t \n\tstruct fpu_guest guest_fpu;\n\n\tu64 xcr0;\n\tu64 guest_supported_xcr0;\n\n\tstruct kvm_pio_request pio;\n\tvoid *pio_data;\n\tvoid *sev_pio_data;\n\tunsigned sev_pio_count;\n\n\tu8 event_exit_inst_len;\n\n\tbool exception_from_userspace;\n\n\t \n\tstruct kvm_queued_exception exception;\n\t \n\tstruct kvm_queued_exception exception_vmexit;\n\n\tstruct kvm_queued_interrupt {\n\t\tbool injected;\n\t\tbool soft;\n\t\tu8 nr;\n\t} interrupt;\n\n\tint halt_request;  \n\n\tint cpuid_nent;\n\tstruct kvm_cpuid_entry2 *cpuid_entries;\n\tstruct kvm_hypervisor_cpuid kvm_cpuid;\n\n\t \n#define KVM_MAX_NR_GOVERNED_FEATURES BITS_PER_LONG\n\n\t \n\tstruct {\n\t\tDECLARE_BITMAP(enabled, KVM_MAX_NR_GOVERNED_FEATURES);\n\t} governed_features;\n\n\tu64 reserved_gpa_bits;\n\tint maxphyaddr;\n\n\t \n\n\tstruct x86_emulate_ctxt *emulate_ctxt;\n\tbool emulate_regs_need_sync_to_vcpu;\n\tbool emulate_regs_need_sync_from_vcpu;\n\tint (*complete_userspace_io)(struct kvm_vcpu *vcpu);\n\n\tgpa_t time;\n\tstruct pvclock_vcpu_time_info hv_clock;\n\tunsigned int hw_tsc_khz;\n\tstruct gfn_to_pfn_cache pv_time;\n\t \n\tbool pvclock_set_guest_stopped_request;\n\n\tstruct {\n\t\tu8 preempted;\n\t\tu64 msr_val;\n\t\tu64 last_steal;\n\t\tstruct gfn_to_hva_cache cache;\n\t} st;\n\n\tu64 l1_tsc_offset;\n\tu64 tsc_offset;  \n\tu64 last_guest_tsc;\n\tu64 last_host_tsc;\n\tu64 tsc_offset_adjustment;\n\tu64 this_tsc_nsec;\n\tu64 this_tsc_write;\n\tu64 this_tsc_generation;\n\tbool tsc_catchup;\n\tbool tsc_always_catchup;\n\ts8 virtual_tsc_shift;\n\tu32 virtual_tsc_mult;\n\tu32 virtual_tsc_khz;\n\ts64 ia32_tsc_adjust_msr;\n\tu64 msr_ia32_power_ctl;\n\tu64 l1_tsc_scaling_ratio;\n\tu64 tsc_scaling_ratio;  \n\n\tatomic_t nmi_queued;   \n\t \n\tunsigned int nmi_pending;\n\tbool nmi_injected;     \n\tbool smi_pending;     \n\tu8 handling_intr_from_guest;\n\n\tstruct kvm_mtrr mtrr_state;\n\tu64 pat;\n\n\tunsigned switch_db_regs;\n\tunsigned long db[KVM_NR_DB_REGS];\n\tunsigned long dr6;\n\tunsigned long dr7;\n\tunsigned long eff_db[KVM_NR_DB_REGS];\n\tunsigned long guest_debug_dr7;\n\tu64 msr_platform_info;\n\tu64 msr_misc_features_enables;\n\n\tu64 mcg_cap;\n\tu64 mcg_status;\n\tu64 mcg_ctl;\n\tu64 mcg_ext_ctl;\n\tu64 *mce_banks;\n\tu64 *mci_ctl2_banks;\n\n\t \n\tu64 mmio_gva;\n\tunsigned mmio_access;\n\tgfn_t mmio_gfn;\n\tu64 mmio_gen;\n\n\tstruct kvm_pmu pmu;\n\n\t \n\tunsigned long singlestep_rip;\n\n\tbool hyperv_enabled;\n\tstruct kvm_vcpu_hv *hyperv;\n\tstruct kvm_vcpu_xen xen;\n\n\tcpumask_var_t wbinvd_dirty_mask;\n\n\tunsigned long last_retry_eip;\n\tunsigned long last_retry_addr;\n\n\tstruct {\n\t\tbool halted;\n\t\tgfn_t gfns[ASYNC_PF_PER_VCPU];\n\t\tstruct gfn_to_hva_cache data;\n\t\tu64 msr_en_val;  \n\t\tu64 msr_int_val;  \n\t\tu16 vec;\n\t\tu32 id;\n\t\tbool send_user_only;\n\t\tu32 host_apf_flags;\n\t\tbool delivery_as_pf_vmexit;\n\t\tbool pageready_pending;\n\t} apf;\n\n\t \n\tstruct {\n\t\tu64 length;\n\t\tu64 status;\n\t} osvw;\n\n\tstruct {\n\t\tu64 msr_val;\n\t\tstruct gfn_to_hva_cache data;\n\t} pv_eoi;\n\n\tu64 msr_kvm_poll_control;\n\n\t \n\tunsigned long exit_qualification;\n\n\t \n\tstruct {\n\t\tbool pv_unhalted;\n\t} pv;\n\n\tint pending_ioapic_eoi;\n\tint pending_external_vector;\n\n\t \n\tbool preempted_in_kernel;\n\n\t \n\tbool l1tf_flush_l1d;\n\n\t \n\tint last_vmentry_cpu;\n\n\t \n\tu64 msr_hwcr;\n\n\t \n\tstruct {\n\t\t \n\t\tu32 features;\n\n\t\t \n\t\tbool enforce;\n\t} pv_cpuid;\n\n\t \n\tbool guest_state_protected;\n\n\t \n\tbool pdptrs_from_userspace;\n\n#if IS_ENABLED(CONFIG_HYPERV)\n\thpa_t hv_root_tdp;\n#endif\n};\n\nstruct kvm_lpage_info {\n\tint disallow_lpage;\n};\n\nstruct kvm_arch_memory_slot {\n\tstruct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];\n\tstruct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];\n\tunsigned short *gfn_write_track;\n};\n\n \nenum kvm_apic_logical_mode {\n\t \n\tKVM_APIC_MODE_SW_DISABLED,\n\t \n\tKVM_APIC_MODE_XAPIC_CLUSTER,\n\t \n\tKVM_APIC_MODE_XAPIC_FLAT,\n\t \n\tKVM_APIC_MODE_X2APIC,\n\t \n\tKVM_APIC_MODE_MAP_DISABLED,\n};\n\nstruct kvm_apic_map {\n\tstruct rcu_head rcu;\n\tenum kvm_apic_logical_mode logical_mode;\n\tu32 max_apic_id;\n\tunion {\n\t\tstruct kvm_lapic *xapic_flat_map[8];\n\t\tstruct kvm_lapic *xapic_cluster_map[16][4];\n\t};\n\tstruct kvm_lapic *phys_map[];\n};\n\n \nstruct kvm_hv_syndbg {\n\tstruct {\n\t\tu64 control;\n\t\tu64 status;\n\t\tu64 send_page;\n\t\tu64 recv_page;\n\t\tu64 pending_page;\n\t} control;\n\tu64 options;\n};\n\n \nenum hv_tsc_page_status {\n\t \n\tHV_TSC_PAGE_UNSET = 0,\n\t \n\tHV_TSC_PAGE_GUEST_CHANGED,\n\t \n\tHV_TSC_PAGE_HOST_CHANGED,\n\t \n\tHV_TSC_PAGE_SET,\n\t \n\tHV_TSC_PAGE_BROKEN,\n};\n\n \nstruct kvm_hv {\n\tstruct mutex hv_lock;\n\tu64 hv_guest_os_id;\n\tu64 hv_hypercall;\n\tu64 hv_tsc_page;\n\tenum hv_tsc_page_status hv_tsc_page_status;\n\n\t \n\tu64 hv_crash_param[HV_X64_MSR_CRASH_PARAMS];\n\tu64 hv_crash_ctl;\n\n\tstruct ms_hyperv_tsc_page tsc_ref;\n\n\tstruct idr conn_to_evt;\n\n\tu64 hv_reenlightenment_control;\n\tu64 hv_tsc_emulation_control;\n\tu64 hv_tsc_emulation_status;\n\tu64 hv_invtsc_control;\n\n\t \n\tatomic_t num_mismatched_vp_indexes;\n\n\t \n\tunsigned int synic_auto_eoi_used;\n\n\tstruct hv_partition_assist_pg *hv_pa_pg;\n\tstruct kvm_hv_syndbg hv_syndbg;\n};\n\nstruct msr_bitmap_range {\n\tu32 flags;\n\tu32 nmsrs;\n\tu32 base;\n\tunsigned long *bitmap;\n};\n\n \nstruct kvm_xen {\n\tstruct mutex xen_lock;\n\tu32 xen_version;\n\tbool long_mode;\n\tbool runstate_update_flag;\n\tu8 upcall_vector;\n\tstruct gfn_to_pfn_cache shinfo_cache;\n\tstruct idr evtchn_ports;\n\tunsigned long poll_mask[BITS_TO_LONGS(KVM_MAX_VCPUS)];\n};\n\nenum kvm_irqchip_mode {\n\tKVM_IRQCHIP_NONE,\n\tKVM_IRQCHIP_KERNEL,        \n\tKVM_IRQCHIP_SPLIT,         \n};\n\nstruct kvm_x86_msr_filter {\n\tu8 count;\n\tbool default_allow:1;\n\tstruct msr_bitmap_range ranges[16];\n};\n\nstruct kvm_x86_pmu_event_filter {\n\t__u32 action;\n\t__u32 nevents;\n\t__u32 fixed_counter_bitmap;\n\t__u32 flags;\n\t__u32 nr_includes;\n\t__u32 nr_excludes;\n\t__u64 *includes;\n\t__u64 *excludes;\n\t__u64 events[];\n};\n\nenum kvm_apicv_inhibit {\n\n\t \n\t \n\t \n\n\t \n\tAPICV_INHIBIT_REASON_DISABLE,\n\n\t \n\tAPICV_INHIBIT_REASON_HYPERV,\n\n\t \n\tAPICV_INHIBIT_REASON_ABSENT,\n\n\t \n\tAPICV_INHIBIT_REASON_BLOCKIRQ,\n\n\t \n\tAPICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED,\n\n\t \n\tAPICV_INHIBIT_REASON_APIC_ID_MODIFIED,\n\tAPICV_INHIBIT_REASON_APIC_BASE_MODIFIED,\n\n\t \n\t \n\t \n\n\t \n\tAPICV_INHIBIT_REASON_NESTED,\n\n\t \n\tAPICV_INHIBIT_REASON_IRQWIN,\n\n\t \n\tAPICV_INHIBIT_REASON_PIT_REINJ,\n\n\t \n\tAPICV_INHIBIT_REASON_SEV,\n\n\t \n\tAPICV_INHIBIT_REASON_LOGICAL_ID_ALIASED,\n};\n\nstruct kvm_arch {\n\tunsigned long n_used_mmu_pages;\n\tunsigned long n_requested_mmu_pages;\n\tunsigned long n_max_mmu_pages;\n\tunsigned int indirect_shadow_pages;\n\tu8 mmu_valid_gen;\n\tstruct hlist_head mmu_page_hash[KVM_NUM_MMU_PAGES];\n\tstruct list_head active_mmu_pages;\n\tstruct list_head zapped_obsolete_pages;\n\t \n\tstruct list_head possible_nx_huge_pages;\n#ifdef CONFIG_KVM_EXTERNAL_WRITE_TRACKING\n\tstruct kvm_page_track_notifier_head track_notifier_head;\n#endif\n\t \n\tspinlock_t mmu_unsync_pages_lock;\n\n\tstruct list_head assigned_dev_head;\n\tstruct iommu_domain *iommu_domain;\n\tbool iommu_noncoherent;\n#define __KVM_HAVE_ARCH_NONCOHERENT_DMA\n\tatomic_t noncoherent_dma_count;\n#define __KVM_HAVE_ARCH_ASSIGNED_DEVICE\n\tatomic_t assigned_device_count;\n\tstruct kvm_pic *vpic;\n\tstruct kvm_ioapic *vioapic;\n\tstruct kvm_pit *vpit;\n\tatomic_t vapics_in_nmi_mode;\n\tstruct mutex apic_map_lock;\n\tstruct kvm_apic_map __rcu *apic_map;\n\tatomic_t apic_map_dirty;\n\n\tbool apic_access_memslot_enabled;\n\tbool apic_access_memslot_inhibited;\n\n\t \n\tstruct rw_semaphore apicv_update_lock;\n\tunsigned long apicv_inhibit_reasons;\n\n\tgpa_t wall_clock;\n\n\tbool mwait_in_guest;\n\tbool hlt_in_guest;\n\tbool pause_in_guest;\n\tbool cstate_in_guest;\n\n\tunsigned long irq_sources_bitmap;\n\ts64 kvmclock_offset;\n\n\t \n\traw_spinlock_t tsc_write_lock;\n\tu64 last_tsc_nsec;\n\tu64 last_tsc_write;\n\tu32 last_tsc_khz;\n\tu64 last_tsc_offset;\n\tu64 cur_tsc_nsec;\n\tu64 cur_tsc_write;\n\tu64 cur_tsc_offset;\n\tu64 cur_tsc_generation;\n\tint nr_vcpus_matched_tsc;\n\n\tu32 default_tsc_khz;\n\n\tseqcount_raw_spinlock_t pvclock_sc;\n\tbool use_master_clock;\n\tu64 master_kernel_ns;\n\tu64 master_cycle_now;\n\tstruct delayed_work kvmclock_update_work;\n\tstruct delayed_work kvmclock_sync_work;\n\n\tstruct kvm_xen_hvm_config xen_hvm_config;\n\n\t \n\tstruct hlist_head mask_notifier_list;\n\n\tstruct kvm_hv hyperv;\n\tstruct kvm_xen xen;\n\n\tbool backwards_tsc_observed;\n\tbool boot_vcpu_runs_old_kvmclock;\n\tu32 bsp_vcpu_id;\n\n\tu64 disabled_quirks;\n\n\tenum kvm_irqchip_mode irqchip_mode;\n\tu8 nr_reserved_ioapic_pins;\n\n\tbool disabled_lapic_found;\n\n\tbool x2apic_format;\n\tbool x2apic_broadcast_quirk_disabled;\n\n\tbool guest_can_read_msr_platform_info;\n\tbool exception_payload_enabled;\n\n\tbool triple_fault_event;\n\n\tbool bus_lock_detection_enabled;\n\tbool enable_pmu;\n\n\tu32 notify_window;\n\tu32 notify_vmexit_flags;\n\t \n\tbool exit_on_emulation_error;\n\n\t \n\tu32 user_space_msr_mask;\n\tstruct kvm_x86_msr_filter __rcu *msr_filter;\n\n\tu32 hypercall_exit_enabled;\n\n\t \n\tbool sgx_provisioning_allowed;\n\n\tstruct kvm_x86_pmu_event_filter __rcu *pmu_event_filter;\n\tstruct task_struct *nx_huge_page_recovery_thread;\n\n#ifdef CONFIG_X86_64\n\t \n\tatomic64_t tdp_mmu_pages;\n\n\t \n\tstruct list_head tdp_mmu_roots;\n\n\t \n\tspinlock_t tdp_mmu_pages_lock;\n#endif  \n\n\t \n\tbool shadow_root_allocated;\n\n#if IS_ENABLED(CONFIG_HYPERV)\n\thpa_t\thv_root_tdp;\n\tspinlock_t hv_root_tdp_lock;\n#endif\n\t \n\tu32 max_vcpu_ids;\n\n\tbool disable_nx_huge_pages;\n\n\t \n\tstruct kvm_mmu_memory_cache split_shadow_page_cache;\n\tstruct kvm_mmu_memory_cache split_page_header_cache;\n\n\t \n#define SPLIT_DESC_CACHE_MIN_NR_OBJECTS (SPTE_ENT_PER_PAGE + 1)\n\tstruct kvm_mmu_memory_cache split_desc_cache;\n};\n\nstruct kvm_vm_stat {\n\tstruct kvm_vm_stat_generic generic;\n\tu64 mmu_shadow_zapped;\n\tu64 mmu_pte_write;\n\tu64 mmu_pde_zapped;\n\tu64 mmu_flooded;\n\tu64 mmu_recycled;\n\tu64 mmu_cache_miss;\n\tu64 mmu_unsync;\n\tunion {\n\t\tstruct {\n\t\t\tatomic64_t pages_4k;\n\t\t\tatomic64_t pages_2m;\n\t\t\tatomic64_t pages_1g;\n\t\t};\n\t\tatomic64_t pages[KVM_NR_PAGE_SIZES];\n\t};\n\tu64 nx_lpage_splits;\n\tu64 max_mmu_page_hash_collisions;\n\tu64 max_mmu_rmap_size;\n};\n\nstruct kvm_vcpu_stat {\n\tstruct kvm_vcpu_stat_generic generic;\n\tu64 pf_taken;\n\tu64 pf_fixed;\n\tu64 pf_emulate;\n\tu64 pf_spurious;\n\tu64 pf_fast;\n\tu64 pf_mmio_spte_created;\n\tu64 pf_guest;\n\tu64 tlb_flush;\n\tu64 invlpg;\n\n\tu64 exits;\n\tu64 io_exits;\n\tu64 mmio_exits;\n\tu64 signal_exits;\n\tu64 irq_window_exits;\n\tu64 nmi_window_exits;\n\tu64 l1d_flush;\n\tu64 halt_exits;\n\tu64 request_irq_exits;\n\tu64 irq_exits;\n\tu64 host_state_reload;\n\tu64 fpu_reload;\n\tu64 insn_emulation;\n\tu64 insn_emulation_fail;\n\tu64 hypercalls;\n\tu64 irq_injections;\n\tu64 nmi_injections;\n\tu64 req_event;\n\tu64 nested_run;\n\tu64 directed_yield_attempted;\n\tu64 directed_yield_successful;\n\tu64 preemption_reported;\n\tu64 preemption_other;\n\tu64 guest_mode;\n\tu64 notify_window_exits;\n};\n\nstruct x86_instruction_info;\n\nstruct msr_data {\n\tbool host_initiated;\n\tu32 index;\n\tu64 data;\n};\n\nstruct kvm_lapic_irq {\n\tu32 vector;\n\tu16 delivery_mode;\n\tu16 dest_mode;\n\tbool level;\n\tu16 trig_mode;\n\tu32 shorthand;\n\tu32 dest_id;\n\tbool msi_redir_hint;\n};\n\nstatic inline u16 kvm_lapic_irq_dest_mode(bool dest_mode_logical)\n{\n\treturn dest_mode_logical ? APIC_DEST_LOGICAL : APIC_DEST_PHYSICAL;\n}\n\nstruct kvm_x86_ops {\n\tconst char *name;\n\n\tint (*check_processor_compatibility)(void);\n\n\tint (*hardware_enable)(void);\n\tvoid (*hardware_disable)(void);\n\tvoid (*hardware_unsetup)(void);\n\tbool (*has_emulated_msr)(struct kvm *kvm, u32 index);\n\tvoid (*vcpu_after_set_cpuid)(struct kvm_vcpu *vcpu);\n\n\tunsigned int vm_size;\n\tint (*vm_init)(struct kvm *kvm);\n\tvoid (*vm_destroy)(struct kvm *kvm);\n\n\t \n\tint (*vcpu_precreate)(struct kvm *kvm);\n\tint (*vcpu_create)(struct kvm_vcpu *vcpu);\n\tvoid (*vcpu_free)(struct kvm_vcpu *vcpu);\n\tvoid (*vcpu_reset)(struct kvm_vcpu *vcpu, bool init_event);\n\n\tvoid (*prepare_switch_to_guest)(struct kvm_vcpu *vcpu);\n\tvoid (*vcpu_load)(struct kvm_vcpu *vcpu, int cpu);\n\tvoid (*vcpu_put)(struct kvm_vcpu *vcpu);\n\n\tvoid (*update_exception_bitmap)(struct kvm_vcpu *vcpu);\n\tint (*get_msr)(struct kvm_vcpu *vcpu, struct msr_data *msr);\n\tint (*set_msr)(struct kvm_vcpu *vcpu, struct msr_data *msr);\n\tu64 (*get_segment_base)(struct kvm_vcpu *vcpu, int seg);\n\tvoid (*get_segment)(struct kvm_vcpu *vcpu,\n\t\t\t    struct kvm_segment *var, int seg);\n\tint (*get_cpl)(struct kvm_vcpu *vcpu);\n\tvoid (*set_segment)(struct kvm_vcpu *vcpu,\n\t\t\t    struct kvm_segment *var, int seg);\n\tvoid (*get_cs_db_l_bits)(struct kvm_vcpu *vcpu, int *db, int *l);\n\tbool (*is_valid_cr0)(struct kvm_vcpu *vcpu, unsigned long cr0);\n\tvoid (*set_cr0)(struct kvm_vcpu *vcpu, unsigned long cr0);\n\tvoid (*post_set_cr3)(struct kvm_vcpu *vcpu, unsigned long cr3);\n\tbool (*is_valid_cr4)(struct kvm_vcpu *vcpu, unsigned long cr4);\n\tvoid (*set_cr4)(struct kvm_vcpu *vcpu, unsigned long cr4);\n\tint (*set_efer)(struct kvm_vcpu *vcpu, u64 efer);\n\tvoid (*get_idt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);\n\tvoid (*set_idt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);\n\tvoid (*get_gdt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);\n\tvoid (*set_gdt)(struct kvm_vcpu *vcpu, struct desc_ptr *dt);\n\tvoid (*sync_dirty_debug_regs)(struct kvm_vcpu *vcpu);\n\tvoid (*set_dr7)(struct kvm_vcpu *vcpu, unsigned long value);\n\tvoid (*cache_reg)(struct kvm_vcpu *vcpu, enum kvm_reg reg);\n\tunsigned long (*get_rflags)(struct kvm_vcpu *vcpu);\n\tvoid (*set_rflags)(struct kvm_vcpu *vcpu, unsigned long rflags);\n\tbool (*get_if_flag)(struct kvm_vcpu *vcpu);\n\n\tvoid (*flush_tlb_all)(struct kvm_vcpu *vcpu);\n\tvoid (*flush_tlb_current)(struct kvm_vcpu *vcpu);\n\tint  (*flush_remote_tlbs)(struct kvm *kvm);\n\tint  (*flush_remote_tlbs_range)(struct kvm *kvm, gfn_t gfn,\n\t\t\t\t\tgfn_t nr_pages);\n\n\t \n\tvoid (*flush_tlb_gva)(struct kvm_vcpu *vcpu, gva_t addr);\n\n\t \n\tvoid (*flush_tlb_guest)(struct kvm_vcpu *vcpu);\n\n\tint (*vcpu_pre_run)(struct kvm_vcpu *vcpu);\n\tenum exit_fastpath_completion (*vcpu_run)(struct kvm_vcpu *vcpu);\n\tint (*handle_exit)(struct kvm_vcpu *vcpu,\n\t\tenum exit_fastpath_completion exit_fastpath);\n\tint (*skip_emulated_instruction)(struct kvm_vcpu *vcpu);\n\tvoid (*update_emulated_instruction)(struct kvm_vcpu *vcpu);\n\tvoid (*set_interrupt_shadow)(struct kvm_vcpu *vcpu, int mask);\n\tu32 (*get_interrupt_shadow)(struct kvm_vcpu *vcpu);\n\tvoid (*patch_hypercall)(struct kvm_vcpu *vcpu,\n\t\t\t\tunsigned char *hypercall_addr);\n\tvoid (*inject_irq)(struct kvm_vcpu *vcpu, bool reinjected);\n\tvoid (*inject_nmi)(struct kvm_vcpu *vcpu);\n\tvoid (*inject_exception)(struct kvm_vcpu *vcpu);\n\tvoid (*cancel_injection)(struct kvm_vcpu *vcpu);\n\tint (*interrupt_allowed)(struct kvm_vcpu *vcpu, bool for_injection);\n\tint (*nmi_allowed)(struct kvm_vcpu *vcpu, bool for_injection);\n\tbool (*get_nmi_mask)(struct kvm_vcpu *vcpu);\n\tvoid (*set_nmi_mask)(struct kvm_vcpu *vcpu, bool masked);\n\t \n\tbool (*is_vnmi_pending)(struct kvm_vcpu *vcpu);\n\t \n\tbool (*set_vnmi_pending)(struct kvm_vcpu *vcpu);\n\tvoid (*enable_nmi_window)(struct kvm_vcpu *vcpu);\n\tvoid (*enable_irq_window)(struct kvm_vcpu *vcpu);\n\tvoid (*update_cr8_intercept)(struct kvm_vcpu *vcpu, int tpr, int irr);\n\tbool (*check_apicv_inhibit_reasons)(enum kvm_apicv_inhibit reason);\n\tconst unsigned long required_apicv_inhibits;\n\tbool allow_apicv_in_x2apic_without_x2apic_virtualization;\n\tvoid (*refresh_apicv_exec_ctrl)(struct kvm_vcpu *vcpu);\n\tvoid (*hwapic_irr_update)(struct kvm_vcpu *vcpu, int max_irr);\n\tvoid (*hwapic_isr_update)(int isr);\n\tbool (*guest_apic_has_interrupt)(struct kvm_vcpu *vcpu);\n\tvoid (*load_eoi_exitmap)(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap);\n\tvoid (*set_virtual_apic_mode)(struct kvm_vcpu *vcpu);\n\tvoid (*set_apic_access_page_addr)(struct kvm_vcpu *vcpu);\n\tvoid (*deliver_interrupt)(struct kvm_lapic *apic, int delivery_mode,\n\t\t\t\t  int trig_mode, int vector);\n\tint (*sync_pir_to_irr)(struct kvm_vcpu *vcpu);\n\tint (*set_tss_addr)(struct kvm *kvm, unsigned int addr);\n\tint (*set_identity_map_addr)(struct kvm *kvm, u64 ident_addr);\n\tu8 (*get_mt_mask)(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio);\n\n\tvoid (*load_mmu_pgd)(struct kvm_vcpu *vcpu, hpa_t root_hpa,\n\t\t\t     int root_level);\n\n\tbool (*has_wbinvd_exit)(void);\n\n\tu64 (*get_l2_tsc_offset)(struct kvm_vcpu *vcpu);\n\tu64 (*get_l2_tsc_multiplier)(struct kvm_vcpu *vcpu);\n\tvoid (*write_tsc_offset)(struct kvm_vcpu *vcpu);\n\tvoid (*write_tsc_multiplier)(struct kvm_vcpu *vcpu);\n\n\t \n\tvoid (*get_exit_info)(struct kvm_vcpu *vcpu, u32 *reason,\n\t\t\t      u64 *info1, u64 *info2,\n\t\t\t      u32 *exit_int_info, u32 *exit_int_info_err_code);\n\n\tint (*check_intercept)(struct kvm_vcpu *vcpu,\n\t\t\t       struct x86_instruction_info *info,\n\t\t\t       enum x86_intercept_stage stage,\n\t\t\t       struct x86_exception *exception);\n\tvoid (*handle_exit_irqoff)(struct kvm_vcpu *vcpu);\n\n\tvoid (*request_immediate_exit)(struct kvm_vcpu *vcpu);\n\n\tvoid (*sched_in)(struct kvm_vcpu *kvm, int cpu);\n\n\t \n\tint cpu_dirty_log_size;\n\tvoid (*update_cpu_dirty_logging)(struct kvm_vcpu *vcpu);\n\n\tconst struct kvm_x86_nested_ops *nested_ops;\n\n\tvoid (*vcpu_blocking)(struct kvm_vcpu *vcpu);\n\tvoid (*vcpu_unblocking)(struct kvm_vcpu *vcpu);\n\n\tint (*pi_update_irte)(struct kvm *kvm, unsigned int host_irq,\n\t\t\t      uint32_t guest_irq, bool set);\n\tvoid (*pi_start_assignment)(struct kvm *kvm);\n\tvoid (*apicv_pre_state_restore)(struct kvm_vcpu *vcpu);\n\tvoid (*apicv_post_state_restore)(struct kvm_vcpu *vcpu);\n\tbool (*dy_apicv_has_pending_interrupt)(struct kvm_vcpu *vcpu);\n\n\tint (*set_hv_timer)(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc,\n\t\t\t    bool *expired);\n\tvoid (*cancel_hv_timer)(struct kvm_vcpu *vcpu);\n\n\tvoid (*setup_mce)(struct kvm_vcpu *vcpu);\n\n#ifdef CONFIG_KVM_SMM\n\tint (*smi_allowed)(struct kvm_vcpu *vcpu, bool for_injection);\n\tint (*enter_smm)(struct kvm_vcpu *vcpu, union kvm_smram *smram);\n\tint (*leave_smm)(struct kvm_vcpu *vcpu, const union kvm_smram *smram);\n\tvoid (*enable_smi_window)(struct kvm_vcpu *vcpu);\n#endif\n\n\tint (*mem_enc_ioctl)(struct kvm *kvm, void __user *argp);\n\tint (*mem_enc_register_region)(struct kvm *kvm, struct kvm_enc_region *argp);\n\tint (*mem_enc_unregister_region)(struct kvm *kvm, struct kvm_enc_region *argp);\n\tint (*vm_copy_enc_context_from)(struct kvm *kvm, unsigned int source_fd);\n\tint (*vm_move_enc_context_from)(struct kvm *kvm, unsigned int source_fd);\n\tvoid (*guest_memory_reclaimed)(struct kvm *kvm);\n\n\tint (*get_msr_feature)(struct kvm_msr_entry *entry);\n\n\tbool (*can_emulate_instruction)(struct kvm_vcpu *vcpu, int emul_type,\n\t\t\t\t\tvoid *insn, int insn_len);\n\n\tbool (*apic_init_signal_blocked)(struct kvm_vcpu *vcpu);\n\tint (*enable_l2_tlb_flush)(struct kvm_vcpu *vcpu);\n\n\tvoid (*migrate_timers)(struct kvm_vcpu *vcpu);\n\tvoid (*msr_filter_changed)(struct kvm_vcpu *vcpu);\n\tint (*complete_emulated_msr)(struct kvm_vcpu *vcpu, int err);\n\n\tvoid (*vcpu_deliver_sipi_vector)(struct kvm_vcpu *vcpu, u8 vector);\n\n\t \n\tunsigned long (*vcpu_get_apicv_inhibit_reasons)(struct kvm_vcpu *vcpu);\n};\n\nstruct kvm_x86_nested_ops {\n\tvoid (*leave_nested)(struct kvm_vcpu *vcpu);\n\tbool (*is_exception_vmexit)(struct kvm_vcpu *vcpu, u8 vector,\n\t\t\t\t    u32 error_code);\n\tint (*check_events)(struct kvm_vcpu *vcpu);\n\tbool (*has_events)(struct kvm_vcpu *vcpu);\n\tvoid (*triple_fault)(struct kvm_vcpu *vcpu);\n\tint (*get_state)(struct kvm_vcpu *vcpu,\n\t\t\t struct kvm_nested_state __user *user_kvm_nested_state,\n\t\t\t unsigned user_data_size);\n\tint (*set_state)(struct kvm_vcpu *vcpu,\n\t\t\t struct kvm_nested_state __user *user_kvm_nested_state,\n\t\t\t struct kvm_nested_state *kvm_state);\n\tbool (*get_nested_state_pages)(struct kvm_vcpu *vcpu);\n\tint (*write_log_dirty)(struct kvm_vcpu *vcpu, gpa_t l2_gpa);\n\n\tint (*enable_evmcs)(struct kvm_vcpu *vcpu,\n\t\t\t    uint16_t *vmcs_version);\n\tuint16_t (*get_evmcs_version)(struct kvm_vcpu *vcpu);\n\tvoid (*hv_inject_synthetic_vmexit_post_tlb_flush)(struct kvm_vcpu *vcpu);\n};\n\nstruct kvm_x86_init_ops {\n\tint (*hardware_setup)(void);\n\tunsigned int (*handle_intel_pt_intr)(void);\n\n\tstruct kvm_x86_ops *runtime_ops;\n\tstruct kvm_pmu_ops *pmu_ops;\n};\n\nstruct kvm_arch_async_pf {\n\tu32 token;\n\tgfn_t gfn;\n\tunsigned long cr3;\n\tbool direct_map;\n};\n\nextern u32 __read_mostly kvm_nr_uret_msrs;\nextern u64 __read_mostly host_efer;\nextern bool __read_mostly allow_smaller_maxphyaddr;\nextern bool __read_mostly enable_apicv;\nextern struct kvm_x86_ops kvm_x86_ops;\n\n#define KVM_X86_OP(func) \\\n\tDECLARE_STATIC_CALL(kvm_x86_##func, *(((struct kvm_x86_ops *)0)->func));\n#define KVM_X86_OP_OPTIONAL KVM_X86_OP\n#define KVM_X86_OP_OPTIONAL_RET0 KVM_X86_OP\n#include <asm/kvm-x86-ops.h>\n\nint kvm_x86_vendor_init(struct kvm_x86_init_ops *ops);\nvoid kvm_x86_vendor_exit(void);\n\n#define __KVM_HAVE_ARCH_VM_ALLOC\nstatic inline struct kvm *kvm_arch_alloc_vm(void)\n{\n\treturn __vmalloc(kvm_x86_ops.vm_size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);\n}\n\n#define __KVM_HAVE_ARCH_VM_FREE\nvoid kvm_arch_free_vm(struct kvm *kvm);\n\n#define __KVM_HAVE_ARCH_FLUSH_REMOTE_TLBS\nstatic inline int kvm_arch_flush_remote_tlbs(struct kvm *kvm)\n{\n\tif (kvm_x86_ops.flush_remote_tlbs &&\n\t    !static_call(kvm_x86_flush_remote_tlbs)(kvm))\n\t\treturn 0;\n\telse\n\t\treturn -ENOTSUPP;\n}\n\n#define __KVM_HAVE_ARCH_FLUSH_REMOTE_TLBS_RANGE\n\n#define kvm_arch_pmi_in_guest(vcpu) \\\n\t((vcpu) && (vcpu)->arch.handling_intr_from_guest)\n\nvoid __init kvm_mmu_x86_module_init(void);\nint kvm_mmu_vendor_module_init(void);\nvoid kvm_mmu_vendor_module_exit(void);\n\nvoid kvm_mmu_destroy(struct kvm_vcpu *vcpu);\nint kvm_mmu_create(struct kvm_vcpu *vcpu);\nvoid kvm_mmu_init_vm(struct kvm *kvm);\nvoid kvm_mmu_uninit_vm(struct kvm *kvm);\n\nvoid kvm_mmu_after_set_cpuid(struct kvm_vcpu *vcpu);\nvoid kvm_mmu_reset_context(struct kvm_vcpu *vcpu);\nvoid kvm_mmu_slot_remove_write_access(struct kvm *kvm,\n\t\t\t\t      const struct kvm_memory_slot *memslot,\n\t\t\t\t      int start_level);\nvoid kvm_mmu_slot_try_split_huge_pages(struct kvm *kvm,\n\t\t\t\t       const struct kvm_memory_slot *memslot,\n\t\t\t\t       int target_level);\nvoid kvm_mmu_try_split_huge_pages(struct kvm *kvm,\n\t\t\t\t  const struct kvm_memory_slot *memslot,\n\t\t\t\t  u64 start, u64 end,\n\t\t\t\t  int target_level);\nvoid kvm_mmu_zap_collapsible_sptes(struct kvm *kvm,\n\t\t\t\t   const struct kvm_memory_slot *memslot);\nvoid kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,\n\t\t\t\t   const struct kvm_memory_slot *memslot);\nvoid kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm, u64 gen);\nvoid kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned long kvm_nr_mmu_pages);\n\nint load_pdptrs(struct kvm_vcpu *vcpu, unsigned long cr3);\n\nint emulator_write_phys(struct kvm_vcpu *vcpu, gpa_t gpa,\n\t\t\t  const void *val, int bytes);\n\nstruct kvm_irq_mask_notifier {\n\tvoid (*func)(struct kvm_irq_mask_notifier *kimn, bool masked);\n\tint irq;\n\tstruct hlist_node link;\n};\n\nvoid kvm_register_irq_mask_notifier(struct kvm *kvm, int irq,\n\t\t\t\t    struct kvm_irq_mask_notifier *kimn);\nvoid kvm_unregister_irq_mask_notifier(struct kvm *kvm, int irq,\n\t\t\t\t      struct kvm_irq_mask_notifier *kimn);\nvoid kvm_fire_mask_notifiers(struct kvm *kvm, unsigned irqchip, unsigned pin,\n\t\t\t     bool mask);\n\nextern bool tdp_enabled;\n\nu64 vcpu_tsc_khz(struct kvm_vcpu *vcpu);\n\n \n#define EMULTYPE_NO_DECODE\t    (1 << 0)\n#define EMULTYPE_TRAP_UD\t    (1 << 1)\n#define EMULTYPE_SKIP\t\t    (1 << 2)\n#define EMULTYPE_ALLOW_RETRY_PF\t    (1 << 3)\n#define EMULTYPE_TRAP_UD_FORCED\t    (1 << 4)\n#define EMULTYPE_VMWARE_GP\t    (1 << 5)\n#define EMULTYPE_PF\t\t    (1 << 6)\n#define EMULTYPE_COMPLETE_USER_EXIT (1 << 7)\n#define EMULTYPE_WRITE_PF_TO_SP\t    (1 << 8)\n\nint kvm_emulate_instruction(struct kvm_vcpu *vcpu, int emulation_type);\nint kvm_emulate_instruction_from_buffer(struct kvm_vcpu *vcpu,\n\t\t\t\t\tvoid *insn, int insn_len);\nvoid __kvm_prepare_emulation_failure_exit(struct kvm_vcpu *vcpu,\n\t\t\t\t\t  u64 *data, u8 ndata);\nvoid kvm_prepare_emulation_failure_exit(struct kvm_vcpu *vcpu);\n\nvoid kvm_enable_efer_bits(u64);\nbool kvm_valid_efer(struct kvm_vcpu *vcpu, u64 efer);\nint __kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data, bool host_initiated);\nint kvm_get_msr(struct kvm_vcpu *vcpu, u32 index, u64 *data);\nint kvm_set_msr(struct kvm_vcpu *vcpu, u32 index, u64 data);\nint kvm_emulate_rdmsr(struct kvm_vcpu *vcpu);\nint kvm_emulate_wrmsr(struct kvm_vcpu *vcpu);\nint kvm_emulate_as_nop(struct kvm_vcpu *vcpu);\nint kvm_emulate_invd(struct kvm_vcpu *vcpu);\nint kvm_emulate_mwait(struct kvm_vcpu *vcpu);\nint kvm_handle_invalid_op(struct kvm_vcpu *vcpu);\nint kvm_emulate_monitor(struct kvm_vcpu *vcpu);\n\nint kvm_fast_pio(struct kvm_vcpu *vcpu, int size, unsigned short port, int in);\nint kvm_emulate_cpuid(struct kvm_vcpu *vcpu);\nint kvm_emulate_halt(struct kvm_vcpu *vcpu);\nint kvm_emulate_halt_noskip(struct kvm_vcpu *vcpu);\nint kvm_emulate_ap_reset_hold(struct kvm_vcpu *vcpu);\nint kvm_emulate_wbinvd(struct kvm_vcpu *vcpu);\n\nvoid kvm_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);\nvoid kvm_set_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg);\nint kvm_load_segment_descriptor(struct kvm_vcpu *vcpu, u16 selector, int seg);\nvoid kvm_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, u8 vector);\n\nint kvm_task_switch(struct kvm_vcpu *vcpu, u16 tss_selector, int idt_index,\n\t\t    int reason, bool has_error_code, u32 error_code);\n\nvoid kvm_post_set_cr0(struct kvm_vcpu *vcpu, unsigned long old_cr0, unsigned long cr0);\nvoid kvm_post_set_cr4(struct kvm_vcpu *vcpu, unsigned long old_cr4, unsigned long cr4);\nint kvm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);\nint kvm_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3);\nint kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);\nint kvm_set_cr8(struct kvm_vcpu *vcpu, unsigned long cr8);\nint kvm_set_dr(struct kvm_vcpu *vcpu, int dr, unsigned long val);\nvoid kvm_get_dr(struct kvm_vcpu *vcpu, int dr, unsigned long *val);\nunsigned long kvm_get_cr8(struct kvm_vcpu *vcpu);\nvoid kvm_lmsw(struct kvm_vcpu *vcpu, unsigned long msw);\nint kvm_emulate_xsetbv(struct kvm_vcpu *vcpu);\n\nint kvm_get_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr);\nint kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr);\n\nunsigned long kvm_get_rflags(struct kvm_vcpu *vcpu);\nvoid kvm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags);\nint kvm_emulate_rdpmc(struct kvm_vcpu *vcpu);\n\nvoid kvm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr);\nvoid kvm_queue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);\nvoid kvm_queue_exception_p(struct kvm_vcpu *vcpu, unsigned nr, unsigned long payload);\nvoid kvm_requeue_exception(struct kvm_vcpu *vcpu, unsigned nr);\nvoid kvm_requeue_exception_e(struct kvm_vcpu *vcpu, unsigned nr, u32 error_code);\nvoid kvm_inject_page_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault);\nvoid kvm_inject_emulated_page_fault(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct x86_exception *fault);\nbool kvm_require_cpl(struct kvm_vcpu *vcpu, int required_cpl);\nbool kvm_require_dr(struct kvm_vcpu *vcpu, int dr);\n\nstatic inline int __kvm_irq_line_state(unsigned long *irq_state,\n\t\t\t\t       int irq_source_id, int level)\n{\n\t \n\tif (level)\n\t\t__set_bit(irq_source_id, irq_state);\n\telse\n\t\t__clear_bit(irq_source_id, irq_state);\n\n\treturn !!(*irq_state);\n}\n\nint kvm_pic_set_irq(struct kvm_pic *pic, int irq, int irq_source_id, int level);\nvoid kvm_pic_clear_all(struct kvm_pic *pic, int irq_source_id);\n\nvoid kvm_inject_nmi(struct kvm_vcpu *vcpu);\nint kvm_get_nr_pending_nmis(struct kvm_vcpu *vcpu);\n\nvoid kvm_update_dr7(struct kvm_vcpu *vcpu);\n\nint kvm_mmu_unprotect_page(struct kvm *kvm, gfn_t gfn);\nvoid kvm_mmu_free_roots(struct kvm *kvm, struct kvm_mmu *mmu,\n\t\t\tulong roots_to_free);\nvoid kvm_mmu_free_guest_mode_roots(struct kvm *kvm, struct kvm_mmu *mmu);\ngpa_t kvm_mmu_gva_to_gpa_read(struct kvm_vcpu *vcpu, gva_t gva,\n\t\t\t      struct x86_exception *exception);\ngpa_t kvm_mmu_gva_to_gpa_write(struct kvm_vcpu *vcpu, gva_t gva,\n\t\t\t       struct x86_exception *exception);\ngpa_t kvm_mmu_gva_to_gpa_system(struct kvm_vcpu *vcpu, gva_t gva,\n\t\t\t\tstruct x86_exception *exception);\n\nbool kvm_apicv_activated(struct kvm *kvm);\nbool kvm_vcpu_apicv_activated(struct kvm_vcpu *vcpu);\nvoid __kvm_vcpu_update_apicv(struct kvm_vcpu *vcpu);\nvoid __kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,\n\t\t\t\t      enum kvm_apicv_inhibit reason, bool set);\nvoid kvm_set_or_clear_apicv_inhibit(struct kvm *kvm,\n\t\t\t\t    enum kvm_apicv_inhibit reason, bool set);\n\nstatic inline void kvm_set_apicv_inhibit(struct kvm *kvm,\n\t\t\t\t\t enum kvm_apicv_inhibit reason)\n{\n\tkvm_set_or_clear_apicv_inhibit(kvm, reason, true);\n}\n\nstatic inline void kvm_clear_apicv_inhibit(struct kvm *kvm,\n\t\t\t\t\t   enum kvm_apicv_inhibit reason)\n{\n\tkvm_set_or_clear_apicv_inhibit(kvm, reason, false);\n}\n\nint kvm_emulate_hypercall(struct kvm_vcpu *vcpu);\n\nint kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,\n\t\t       void *insn, int insn_len);\nvoid kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva);\nvoid kvm_mmu_invalidate_addr(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,\n\t\t\t     u64 addr, unsigned long roots);\nvoid kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid);\nvoid kvm_mmu_new_pgd(struct kvm_vcpu *vcpu, gpa_t new_pgd);\n\nvoid kvm_configure_mmu(bool enable_tdp, int tdp_forced_root_level,\n\t\t       int tdp_max_root_level, int tdp_huge_page_level);\n\nstatic inline u16 kvm_read_ldt(void)\n{\n\tu16 ldt;\n\tasm(\"sldt %0\" : \"=g\"(ldt));\n\treturn ldt;\n}\n\nstatic inline void kvm_load_ldt(u16 sel)\n{\n\tasm(\"lldt %0\" : : \"rm\"(sel));\n}\n\n#ifdef CONFIG_X86_64\nstatic inline unsigned long read_msr(unsigned long msr)\n{\n\tu64 value;\n\n\trdmsrl(msr, value);\n\treturn value;\n}\n#endif\n\nstatic inline void kvm_inject_gp(struct kvm_vcpu *vcpu, u32 error_code)\n{\n\tkvm_queue_exception_e(vcpu, GP_VECTOR, error_code);\n}\n\n#define TSS_IOPB_BASE_OFFSET 0x66\n#define TSS_BASE_SIZE 0x68\n#define TSS_IOPB_SIZE (65536 / 8)\n#define TSS_REDIRECTION_SIZE (256 / 8)\n#define RMODE_TSS_SIZE\t\t\t\t\t\t\t\\\n\t(TSS_BASE_SIZE + TSS_REDIRECTION_SIZE + TSS_IOPB_SIZE + 1)\n\nenum {\n\tTASK_SWITCH_CALL = 0,\n\tTASK_SWITCH_IRET = 1,\n\tTASK_SWITCH_JMP = 2,\n\tTASK_SWITCH_GATE = 3,\n};\n\n#define HF_GUEST_MASK\t\t(1 << 0)  \n\n#ifdef CONFIG_KVM_SMM\n#define HF_SMM_MASK\t\t(1 << 1)\n#define HF_SMM_INSIDE_NMI_MASK\t(1 << 2)\n\n# define __KVM_VCPU_MULTIPLE_ADDRESS_SPACE\n# define KVM_ADDRESS_SPACE_NUM 2\n# define kvm_arch_vcpu_memslots_id(vcpu) ((vcpu)->arch.hflags & HF_SMM_MASK ? 1 : 0)\n# define kvm_memslots_for_spte_role(kvm, role) __kvm_memslots(kvm, (role).smm)\n#else\n# define kvm_memslots_for_spte_role(kvm, role) __kvm_memslots(kvm, 0)\n#endif\n\n#define KVM_ARCH_WANT_MMU_NOTIFIER\n\nint kvm_cpu_has_injectable_intr(struct kvm_vcpu *v);\nint kvm_cpu_has_interrupt(struct kvm_vcpu *vcpu);\nint kvm_cpu_has_extint(struct kvm_vcpu *v);\nint kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu);\nint kvm_cpu_get_interrupt(struct kvm_vcpu *v);\nvoid kvm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event);\n\nint kvm_pv_send_ipi(struct kvm *kvm, unsigned long ipi_bitmap_low,\n\t\t    unsigned long ipi_bitmap_high, u32 min,\n\t\t    unsigned long icr, int op_64_bit);\n\nint kvm_add_user_return_msr(u32 msr);\nint kvm_find_user_return_msr(u32 msr);\nint kvm_set_user_return_msr(unsigned index, u64 val, u64 mask);\n\nstatic inline bool kvm_is_supported_user_return_msr(u32 msr)\n{\n\treturn kvm_find_user_return_msr(msr) >= 0;\n}\n\nu64 kvm_scale_tsc(u64 tsc, u64 ratio);\nu64 kvm_read_l1_tsc(struct kvm_vcpu *vcpu, u64 host_tsc);\nu64 kvm_calc_nested_tsc_offset(u64 l1_offset, u64 l2_offset, u64 l2_multiplier);\nu64 kvm_calc_nested_tsc_multiplier(u64 l1_multiplier, u64 l2_multiplier);\n\nunsigned long kvm_get_linear_rip(struct kvm_vcpu *vcpu);\nbool kvm_is_linear_rip(struct kvm_vcpu *vcpu, unsigned long linear_rip);\n\nvoid kvm_make_scan_ioapic_request(struct kvm *kvm);\nvoid kvm_make_scan_ioapic_request_mask(struct kvm *kvm,\n\t\t\t\t       unsigned long *vcpu_bitmap);\n\nbool kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct kvm_async_pf *work);\nvoid kvm_arch_async_page_present(struct kvm_vcpu *vcpu,\n\t\t\t\t struct kvm_async_pf *work);\nvoid kvm_arch_async_page_ready(struct kvm_vcpu *vcpu,\n\t\t\t       struct kvm_async_pf *work);\nvoid kvm_arch_async_page_present_queued(struct kvm_vcpu *vcpu);\nbool kvm_arch_can_dequeue_async_page_present(struct kvm_vcpu *vcpu);\nextern bool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn);\n\nint kvm_skip_emulated_instruction(struct kvm_vcpu *vcpu);\nint kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err);\nvoid __kvm_request_immediate_exit(struct kvm_vcpu *vcpu);\n\nvoid __user *__x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa,\n\t\t\t\t     u32 size);\nbool kvm_vcpu_is_reset_bsp(struct kvm_vcpu *vcpu);\nbool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu);\n\nbool kvm_intr_is_single_vcpu(struct kvm *kvm, struct kvm_lapic_irq *irq,\n\t\t\t     struct kvm_vcpu **dest_vcpu);\n\nvoid kvm_set_msi_irq(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,\n\t\t     struct kvm_lapic_irq *irq);\n\nstatic inline bool kvm_irq_is_postable(struct kvm_lapic_irq *irq)\n{\n\t \n\treturn (irq->delivery_mode == APIC_DM_FIXED ||\n\t\tirq->delivery_mode == APIC_DM_LOWEST);\n}\n\nstatic inline void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu)\n{\n\tstatic_call_cond(kvm_x86_vcpu_blocking)(vcpu);\n}\n\nstatic inline void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu)\n{\n\tstatic_call_cond(kvm_x86_vcpu_unblocking)(vcpu);\n}\n\nstatic inline int kvm_cpu_get_apicid(int mps_cpu)\n{\n#ifdef CONFIG_X86_LOCAL_APIC\n\treturn default_cpu_present_to_apicid(mps_cpu);\n#else\n\tWARN_ON_ONCE(1);\n\treturn BAD_APICID;\n#endif\n}\n\nint memslot_rmap_alloc(struct kvm_memory_slot *slot, unsigned long npages);\n\n#define KVM_CLOCK_VALID_FLAGS\t\t\t\t\t\t\\\n\t(KVM_CLOCK_TSC_STABLE | KVM_CLOCK_REALTIME | KVM_CLOCK_HOST_TSC)\n\n#define KVM_X86_VALID_QUIRKS\t\t\t\\\n\t(KVM_X86_QUIRK_LINT0_REENABLED |\t\\\n\t KVM_X86_QUIRK_CD_NW_CLEARED |\t\t\\\n\t KVM_X86_QUIRK_LAPIC_MMIO_HOLE |\t\\\n\t KVM_X86_QUIRK_OUT_7E_INC_RIP |\t\t\\\n\t KVM_X86_QUIRK_MISC_ENABLE_NO_MWAIT |\t\\\n\t KVM_X86_QUIRK_FIX_HYPERCALL_INSN |\t\\\n\t KVM_X86_QUIRK_MWAIT_NEVER_UD_FAULTS)\n\n \n#define KVM_EXIT_HYPERCALL_MBZ\t\tGENMASK_ULL(31, 1)\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}