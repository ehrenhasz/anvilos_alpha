{
  "module_name": "atomic64_64.h",
  "hash_id": "2107e61321f640a35384abbdd4d1485d9d2d1505ec50ef87376523bf5b2c6ac3",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/include/asm/atomic64_64.h",
  "human_readable_source": " \n#ifndef _ASM_X86_ATOMIC64_64_H\n#define _ASM_X86_ATOMIC64_64_H\n\n#include <linux/types.h>\n#include <asm/alternative.h>\n#include <asm/cmpxchg.h>\n\n \n\n#define ATOMIC64_INIT(i)\t{ (i) }\n\nstatic __always_inline s64 arch_atomic64_read(const atomic64_t *v)\n{\n\treturn __READ_ONCE((v)->counter);\n}\n\nstatic __always_inline void arch_atomic64_set(atomic64_t *v, s64 i)\n{\n\t__WRITE_ONCE(v->counter, i);\n}\n\nstatic __always_inline void arch_atomic64_add(s64 i, atomic64_t *v)\n{\n\tasm volatile(LOCK_PREFIX \"addq %1,%0\"\n\t\t     : \"=m\" (v->counter)\n\t\t     : \"er\" (i), \"m\" (v->counter) : \"memory\");\n}\n\nstatic __always_inline void arch_atomic64_sub(s64 i, atomic64_t *v)\n{\n\tasm volatile(LOCK_PREFIX \"subq %1,%0\"\n\t\t     : \"=m\" (v->counter)\n\t\t     : \"er\" (i), \"m\" (v->counter) : \"memory\");\n}\n\nstatic __always_inline bool arch_atomic64_sub_and_test(s64 i, atomic64_t *v)\n{\n\treturn GEN_BINARY_RMWcc(LOCK_PREFIX \"subq\", v->counter, e, \"er\", i);\n}\n#define arch_atomic64_sub_and_test arch_atomic64_sub_and_test\n\nstatic __always_inline void arch_atomic64_inc(atomic64_t *v)\n{\n\tasm volatile(LOCK_PREFIX \"incq %0\"\n\t\t     : \"=m\" (v->counter)\n\t\t     : \"m\" (v->counter) : \"memory\");\n}\n#define arch_atomic64_inc arch_atomic64_inc\n\nstatic __always_inline void arch_atomic64_dec(atomic64_t *v)\n{\n\tasm volatile(LOCK_PREFIX \"decq %0\"\n\t\t     : \"=m\" (v->counter)\n\t\t     : \"m\" (v->counter) : \"memory\");\n}\n#define arch_atomic64_dec arch_atomic64_dec\n\nstatic __always_inline bool arch_atomic64_dec_and_test(atomic64_t *v)\n{\n\treturn GEN_UNARY_RMWcc(LOCK_PREFIX \"decq\", v->counter, e);\n}\n#define arch_atomic64_dec_and_test arch_atomic64_dec_and_test\n\nstatic __always_inline bool arch_atomic64_inc_and_test(atomic64_t *v)\n{\n\treturn GEN_UNARY_RMWcc(LOCK_PREFIX \"incq\", v->counter, e);\n}\n#define arch_atomic64_inc_and_test arch_atomic64_inc_and_test\n\nstatic __always_inline bool arch_atomic64_add_negative(s64 i, atomic64_t *v)\n{\n\treturn GEN_BINARY_RMWcc(LOCK_PREFIX \"addq\", v->counter, s, \"er\", i);\n}\n#define arch_atomic64_add_negative arch_atomic64_add_negative\n\nstatic __always_inline s64 arch_atomic64_add_return(s64 i, atomic64_t *v)\n{\n\treturn i + xadd(&v->counter, i);\n}\n#define arch_atomic64_add_return arch_atomic64_add_return\n\nstatic __always_inline s64 arch_atomic64_sub_return(s64 i, atomic64_t *v)\n{\n\treturn arch_atomic64_add_return(-i, v);\n}\n#define arch_atomic64_sub_return arch_atomic64_sub_return\n\nstatic __always_inline s64 arch_atomic64_fetch_add(s64 i, atomic64_t *v)\n{\n\treturn xadd(&v->counter, i);\n}\n#define arch_atomic64_fetch_add arch_atomic64_fetch_add\n\nstatic __always_inline s64 arch_atomic64_fetch_sub(s64 i, atomic64_t *v)\n{\n\treturn xadd(&v->counter, -i);\n}\n#define arch_atomic64_fetch_sub arch_atomic64_fetch_sub\n\nstatic __always_inline s64 arch_atomic64_cmpxchg(atomic64_t *v, s64 old, s64 new)\n{\n\treturn arch_cmpxchg(&v->counter, old, new);\n}\n#define arch_atomic64_cmpxchg arch_atomic64_cmpxchg\n\nstatic __always_inline bool arch_atomic64_try_cmpxchg(atomic64_t *v, s64 *old, s64 new)\n{\n\treturn arch_try_cmpxchg(&v->counter, old, new);\n}\n#define arch_atomic64_try_cmpxchg arch_atomic64_try_cmpxchg\n\nstatic __always_inline s64 arch_atomic64_xchg(atomic64_t *v, s64 new)\n{\n\treturn arch_xchg(&v->counter, new);\n}\n#define arch_atomic64_xchg arch_atomic64_xchg\n\nstatic __always_inline void arch_atomic64_and(s64 i, atomic64_t *v)\n{\n\tasm volatile(LOCK_PREFIX \"andq %1,%0\"\n\t\t\t: \"+m\" (v->counter)\n\t\t\t: \"er\" (i)\n\t\t\t: \"memory\");\n}\n\nstatic __always_inline s64 arch_atomic64_fetch_and(s64 i, atomic64_t *v)\n{\n\ts64 val = arch_atomic64_read(v);\n\n\tdo {\n\t} while (!arch_atomic64_try_cmpxchg(v, &val, val & i));\n\treturn val;\n}\n#define arch_atomic64_fetch_and arch_atomic64_fetch_and\n\nstatic __always_inline void arch_atomic64_or(s64 i, atomic64_t *v)\n{\n\tasm volatile(LOCK_PREFIX \"orq %1,%0\"\n\t\t\t: \"+m\" (v->counter)\n\t\t\t: \"er\" (i)\n\t\t\t: \"memory\");\n}\n\nstatic __always_inline s64 arch_atomic64_fetch_or(s64 i, atomic64_t *v)\n{\n\ts64 val = arch_atomic64_read(v);\n\n\tdo {\n\t} while (!arch_atomic64_try_cmpxchg(v, &val, val | i));\n\treturn val;\n}\n#define arch_atomic64_fetch_or arch_atomic64_fetch_or\n\nstatic __always_inline void arch_atomic64_xor(s64 i, atomic64_t *v)\n{\n\tasm volatile(LOCK_PREFIX \"xorq %1,%0\"\n\t\t\t: \"+m\" (v->counter)\n\t\t\t: \"er\" (i)\n\t\t\t: \"memory\");\n}\n\nstatic __always_inline s64 arch_atomic64_fetch_xor(s64 i, atomic64_t *v)\n{\n\ts64 val = arch_atomic64_read(v);\n\n\tdo {\n\t} while (!arch_atomic64_try_cmpxchg(v, &val, val ^ i));\n\treturn val;\n}\n#define arch_atomic64_fetch_xor arch_atomic64_fetch_xor\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}