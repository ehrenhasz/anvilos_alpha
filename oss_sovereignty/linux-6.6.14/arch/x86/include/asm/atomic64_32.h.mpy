{
  "module_name": "atomic64_32.h",
  "hash_id": "334c1aab6f84a5ba974b69a3ca87ebff5d3ffff6fef063ffd51cafbd52c6fc1e",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/include/asm/atomic64_32.h",
  "human_readable_source": " \n#ifndef _ASM_X86_ATOMIC64_32_H\n#define _ASM_X86_ATOMIC64_32_H\n\n#include <linux/compiler.h>\n#include <linux/types.h>\n\n\n \n\ntypedef struct {\n\ts64 __aligned(8) counter;\n} atomic64_t;\n\n#define ATOMIC64_INIT(val)\t{ (val) }\n\n#define __ATOMIC64_DECL(sym) void atomic64_##sym(atomic64_t *, ...)\n#ifndef ATOMIC64_EXPORT\n#define ATOMIC64_DECL_ONE __ATOMIC64_DECL\n#else\n#define ATOMIC64_DECL_ONE(sym) __ATOMIC64_DECL(sym); \\\n\tATOMIC64_EXPORT(atomic64_##sym)\n#endif\n\n#ifdef CONFIG_X86_CMPXCHG64\n#define __alternative_atomic64(f, g, out, in...) \\\n\tasm volatile(\"call %P[func]\" \\\n\t\t     : out : [func] \"i\" (atomic64_##g##_cx8), ## in)\n\n#define ATOMIC64_DECL(sym) ATOMIC64_DECL_ONE(sym##_cx8)\n#else\n#define __alternative_atomic64(f, g, out, in...) \\\n\talternative_call(atomic64_##f##_386, atomic64_##g##_cx8, \\\n\t\t\t X86_FEATURE_CX8, ASM_OUTPUT2(out), ## in)\n\n#define ATOMIC64_DECL(sym) ATOMIC64_DECL_ONE(sym##_cx8); \\\n\tATOMIC64_DECL_ONE(sym##_386)\n\nATOMIC64_DECL_ONE(add_386);\nATOMIC64_DECL_ONE(sub_386);\nATOMIC64_DECL_ONE(inc_386);\nATOMIC64_DECL_ONE(dec_386);\n#endif\n\n#define alternative_atomic64(f, out, in...) \\\n\t__alternative_atomic64(f, f, ASM_OUTPUT2(out), ## in)\n\nATOMIC64_DECL(read);\nATOMIC64_DECL(set);\nATOMIC64_DECL(xchg);\nATOMIC64_DECL(add_return);\nATOMIC64_DECL(sub_return);\nATOMIC64_DECL(inc_return);\nATOMIC64_DECL(dec_return);\nATOMIC64_DECL(dec_if_positive);\nATOMIC64_DECL(inc_not_zero);\nATOMIC64_DECL(add_unless);\n\n#undef ATOMIC64_DECL\n#undef ATOMIC64_DECL_ONE\n#undef __ATOMIC64_DECL\n#undef ATOMIC64_EXPORT\n\nstatic __always_inline s64 arch_atomic64_cmpxchg(atomic64_t *v, s64 o, s64 n)\n{\n\treturn arch_cmpxchg64(&v->counter, o, n);\n}\n#define arch_atomic64_cmpxchg arch_atomic64_cmpxchg\n\nstatic __always_inline s64 arch_atomic64_xchg(atomic64_t *v, s64 n)\n{\n\ts64 o;\n\tunsigned high = (unsigned)(n >> 32);\n\tunsigned low = (unsigned)n;\n\talternative_atomic64(xchg, \"=&A\" (o),\n\t\t\t     \"S\" (v), \"b\" (low), \"c\" (high)\n\t\t\t     : \"memory\");\n\treturn o;\n}\n#define arch_atomic64_xchg arch_atomic64_xchg\n\nstatic __always_inline void arch_atomic64_set(atomic64_t *v, s64 i)\n{\n\tunsigned high = (unsigned)(i >> 32);\n\tunsigned low = (unsigned)i;\n\talternative_atomic64(set,  ,\n\t\t\t     \"S\" (v), \"b\" (low), \"c\" (high)\n\t\t\t     : \"eax\", \"edx\", \"memory\");\n}\n\nstatic __always_inline s64 arch_atomic64_read(const atomic64_t *v)\n{\n\ts64 r;\n\talternative_atomic64(read, \"=&A\" (r), \"c\" (v) : \"memory\");\n\treturn r;\n}\n\nstatic __always_inline s64 arch_atomic64_add_return(s64 i, atomic64_t *v)\n{\n\talternative_atomic64(add_return,\n\t\t\t     ASM_OUTPUT2(\"+A\" (i), \"+c\" (v)),\n\t\t\t     ASM_NO_INPUT_CLOBBER(\"memory\"));\n\treturn i;\n}\n#define arch_atomic64_add_return arch_atomic64_add_return\n\nstatic __always_inline s64 arch_atomic64_sub_return(s64 i, atomic64_t *v)\n{\n\talternative_atomic64(sub_return,\n\t\t\t     ASM_OUTPUT2(\"+A\" (i), \"+c\" (v)),\n\t\t\t     ASM_NO_INPUT_CLOBBER(\"memory\"));\n\treturn i;\n}\n#define arch_atomic64_sub_return arch_atomic64_sub_return\n\nstatic __always_inline s64 arch_atomic64_inc_return(atomic64_t *v)\n{\n\ts64 a;\n\talternative_atomic64(inc_return, \"=&A\" (a),\n\t\t\t     \"S\" (v) : \"memory\", \"ecx\");\n\treturn a;\n}\n#define arch_atomic64_inc_return arch_atomic64_inc_return\n\nstatic __always_inline s64 arch_atomic64_dec_return(atomic64_t *v)\n{\n\ts64 a;\n\talternative_atomic64(dec_return, \"=&A\" (a),\n\t\t\t     \"S\" (v) : \"memory\", \"ecx\");\n\treturn a;\n}\n#define arch_atomic64_dec_return arch_atomic64_dec_return\n\nstatic __always_inline s64 arch_atomic64_add(s64 i, atomic64_t *v)\n{\n\t__alternative_atomic64(add, add_return,\n\t\t\t       ASM_OUTPUT2(\"+A\" (i), \"+c\" (v)),\n\t\t\t       ASM_NO_INPUT_CLOBBER(\"memory\"));\n\treturn i;\n}\n\nstatic __always_inline s64 arch_atomic64_sub(s64 i, atomic64_t *v)\n{\n\t__alternative_atomic64(sub, sub_return,\n\t\t\t       ASM_OUTPUT2(\"+A\" (i), \"+c\" (v)),\n\t\t\t       ASM_NO_INPUT_CLOBBER(\"memory\"));\n\treturn i;\n}\n\nstatic __always_inline void arch_atomic64_inc(atomic64_t *v)\n{\n\t__alternative_atomic64(inc, inc_return,  ,\n\t\t\t       \"S\" (v) : \"memory\", \"eax\", \"ecx\", \"edx\");\n}\n#define arch_atomic64_inc arch_atomic64_inc\n\nstatic __always_inline void arch_atomic64_dec(atomic64_t *v)\n{\n\t__alternative_atomic64(dec, dec_return,  ,\n\t\t\t       \"S\" (v) : \"memory\", \"eax\", \"ecx\", \"edx\");\n}\n#define arch_atomic64_dec arch_atomic64_dec\n\nstatic __always_inline int arch_atomic64_add_unless(atomic64_t *v, s64 a, s64 u)\n{\n\tunsigned low = (unsigned)u;\n\tunsigned high = (unsigned)(u >> 32);\n\talternative_atomic64(add_unless,\n\t\t\t     ASM_OUTPUT2(\"+A\" (a), \"+c\" (low), \"+D\" (high)),\n\t\t\t     \"S\" (v) : \"memory\");\n\treturn (int)a;\n}\n#define arch_atomic64_add_unless arch_atomic64_add_unless\n\nstatic __always_inline int arch_atomic64_inc_not_zero(atomic64_t *v)\n{\n\tint r;\n\talternative_atomic64(inc_not_zero, \"=&a\" (r),\n\t\t\t     \"S\" (v) : \"ecx\", \"edx\", \"memory\");\n\treturn r;\n}\n#define arch_atomic64_inc_not_zero arch_atomic64_inc_not_zero\n\nstatic __always_inline s64 arch_atomic64_dec_if_positive(atomic64_t *v)\n{\n\ts64 r;\n\talternative_atomic64(dec_if_positive, \"=&A\" (r),\n\t\t\t     \"S\" (v) : \"ecx\", \"memory\");\n\treturn r;\n}\n#define arch_atomic64_dec_if_positive arch_atomic64_dec_if_positive\n\n#undef alternative_atomic64\n#undef __alternative_atomic64\n\nstatic __always_inline void arch_atomic64_and(s64 i, atomic64_t *v)\n{\n\ts64 old, c = 0;\n\n\twhile ((old = arch_atomic64_cmpxchg(v, c, c & i)) != c)\n\t\tc = old;\n}\n\nstatic __always_inline s64 arch_atomic64_fetch_and(s64 i, atomic64_t *v)\n{\n\ts64 old, c = 0;\n\n\twhile ((old = arch_atomic64_cmpxchg(v, c, c & i)) != c)\n\t\tc = old;\n\n\treturn old;\n}\n#define arch_atomic64_fetch_and arch_atomic64_fetch_and\n\nstatic __always_inline void arch_atomic64_or(s64 i, atomic64_t *v)\n{\n\ts64 old, c = 0;\n\n\twhile ((old = arch_atomic64_cmpxchg(v, c, c | i)) != c)\n\t\tc = old;\n}\n\nstatic __always_inline s64 arch_atomic64_fetch_or(s64 i, atomic64_t *v)\n{\n\ts64 old, c = 0;\n\n\twhile ((old = arch_atomic64_cmpxchg(v, c, c | i)) != c)\n\t\tc = old;\n\n\treturn old;\n}\n#define arch_atomic64_fetch_or arch_atomic64_fetch_or\n\nstatic __always_inline void arch_atomic64_xor(s64 i, atomic64_t *v)\n{\n\ts64 old, c = 0;\n\n\twhile ((old = arch_atomic64_cmpxchg(v, c, c ^ i)) != c)\n\t\tc = old;\n}\n\nstatic __always_inline s64 arch_atomic64_fetch_xor(s64 i, atomic64_t *v)\n{\n\ts64 old, c = 0;\n\n\twhile ((old = arch_atomic64_cmpxchg(v, c, c ^ i)) != c)\n\t\tc = old;\n\n\treturn old;\n}\n#define arch_atomic64_fetch_xor arch_atomic64_fetch_xor\n\nstatic __always_inline s64 arch_atomic64_fetch_add(s64 i, atomic64_t *v)\n{\n\ts64 old, c = 0;\n\n\twhile ((old = arch_atomic64_cmpxchg(v, c, c + i)) != c)\n\t\tc = old;\n\n\treturn old;\n}\n#define arch_atomic64_fetch_add arch_atomic64_fetch_add\n\n#define arch_atomic64_fetch_sub(i, v)\tarch_atomic64_fetch_add(-(i), (v))\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}