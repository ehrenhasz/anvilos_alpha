{
  "module_name": "pgtable.h",
  "hash_id": "f575fdd2ebd269366b4f76f64ec0dcb785ab040b0fb316f4c808fc9fe24ce5b5",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/include/asm/pgtable.h",
  "human_readable_source": " \n#ifndef _ASM_X86_PGTABLE_H\n#define _ASM_X86_PGTABLE_H\n\n#include <linux/mem_encrypt.h>\n#include <asm/page.h>\n#include <asm/pgtable_types.h>\n\n \n#define pgprot_noncached(prot)\t\t\t\t\t\t\\\n\t((boot_cpu_data.x86 > 3)\t\t\t\t\t\\\n\t ? (__pgprot(pgprot_val(prot) |\t\t\t\t\t\\\n\t\t     cachemode2protval(_PAGE_CACHE_MODE_UC_MINUS)))\t\\\n\t : (prot))\n\n#ifndef __ASSEMBLY__\n#include <linux/spinlock.h>\n#include <asm/x86_init.h>\n#include <asm/pkru.h>\n#include <asm/fpu/api.h>\n#include <asm/coco.h>\n#include <asm-generic/pgtable_uffd.h>\n#include <linux/page_table_check.h>\n\nextern pgd_t early_top_pgt[PTRS_PER_PGD];\nbool __init __early_make_pgtable(unsigned long address, pmdval_t pmd);\n\nstruct seq_file;\nvoid ptdump_walk_pgd_level(struct seq_file *m, struct mm_struct *mm);\nvoid ptdump_walk_pgd_level_debugfs(struct seq_file *m, struct mm_struct *mm,\n\t\t\t\t   bool user);\nvoid ptdump_walk_pgd_level_checkwx(void);\nvoid ptdump_walk_user_pgd_level_checkwx(void);\n\n \n#define pgprot_encrypted(prot)\t__pgprot(cc_mkenc(pgprot_val(prot)))\n#define pgprot_decrypted(prot)\t__pgprot(cc_mkdec(pgprot_val(prot)))\n\n#ifdef CONFIG_DEBUG_WX\n#define debug_checkwx()\t\tptdump_walk_pgd_level_checkwx()\n#define debug_checkwx_user()\tptdump_walk_user_pgd_level_checkwx()\n#else\n#define debug_checkwx()\t\tdo { } while (0)\n#define debug_checkwx_user()\tdo { } while (0)\n#endif\n\n \nextern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)]\n\t__visible;\n#define ZERO_PAGE(vaddr) ((void)(vaddr),virt_to_page(empty_zero_page))\n\nextern spinlock_t pgd_lock;\nextern struct list_head pgd_list;\n\nextern struct mm_struct *pgd_page_get_mm(struct page *page);\n\nextern pmdval_t early_pmd_flags;\n\n#ifdef CONFIG_PARAVIRT_XXL\n#include <asm/paravirt.h>\n#else   \n#define set_pte(ptep, pte)\t\tnative_set_pte(ptep, pte)\n\n#define set_pte_atomic(ptep, pte)\t\t\t\t\t\\\n\tnative_set_pte_atomic(ptep, pte)\n\n#define set_pmd(pmdp, pmd)\t\tnative_set_pmd(pmdp, pmd)\n\n#ifndef __PAGETABLE_P4D_FOLDED\n#define set_pgd(pgdp, pgd)\t\tnative_set_pgd(pgdp, pgd)\n#define pgd_clear(pgd)\t\t\t(pgtable_l5_enabled() ? native_pgd_clear(pgd) : 0)\n#endif\n\n#ifndef set_p4d\n# define set_p4d(p4dp, p4d)\t\tnative_set_p4d(p4dp, p4d)\n#endif\n\n#ifndef __PAGETABLE_PUD_FOLDED\n#define p4d_clear(p4d)\t\t\tnative_p4d_clear(p4d)\n#endif\n\n#ifndef set_pud\n# define set_pud(pudp, pud)\t\tnative_set_pud(pudp, pud)\n#endif\n\n#ifndef __PAGETABLE_PUD_FOLDED\n#define pud_clear(pud)\t\t\tnative_pud_clear(pud)\n#endif\n\n#define pte_clear(mm, addr, ptep)\tnative_pte_clear(mm, addr, ptep)\n#define pmd_clear(pmd)\t\t\tnative_pmd_clear(pmd)\n\n#define pgd_val(x)\tnative_pgd_val(x)\n#define __pgd(x)\tnative_make_pgd(x)\n\n#ifndef __PAGETABLE_P4D_FOLDED\n#define p4d_val(x)\tnative_p4d_val(x)\n#define __p4d(x)\tnative_make_p4d(x)\n#endif\n\n#ifndef __PAGETABLE_PUD_FOLDED\n#define pud_val(x)\tnative_pud_val(x)\n#define __pud(x)\tnative_make_pud(x)\n#endif\n\n#ifndef __PAGETABLE_PMD_FOLDED\n#define pmd_val(x)\tnative_pmd_val(x)\n#define __pmd(x)\tnative_make_pmd(x)\n#endif\n\n#define pte_val(x)\tnative_pte_val(x)\n#define __pte(x)\tnative_make_pte(x)\n\n#define arch_end_context_switch(prev)\tdo {} while(0)\n#endif\t \n\n \nstatic inline bool pte_dirty(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_DIRTY_BITS;\n}\n\nstatic inline bool pte_shstk(pte_t pte)\n{\n\treturn cpu_feature_enabled(X86_FEATURE_SHSTK) &&\n\t       (pte_flags(pte) & (_PAGE_RW | _PAGE_DIRTY)) == _PAGE_DIRTY;\n}\n\nstatic inline int pte_young(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_ACCESSED;\n}\n\nstatic inline bool pmd_dirty(pmd_t pmd)\n{\n\treturn pmd_flags(pmd) & _PAGE_DIRTY_BITS;\n}\n\nstatic inline bool pmd_shstk(pmd_t pmd)\n{\n\treturn cpu_feature_enabled(X86_FEATURE_SHSTK) &&\n\t       (pmd_flags(pmd) & (_PAGE_RW | _PAGE_DIRTY | _PAGE_PSE)) ==\n\t       (_PAGE_DIRTY | _PAGE_PSE);\n}\n\n#define pmd_young pmd_young\nstatic inline int pmd_young(pmd_t pmd)\n{\n\treturn pmd_flags(pmd) & _PAGE_ACCESSED;\n}\n\nstatic inline bool pud_dirty(pud_t pud)\n{\n\treturn pud_flags(pud) & _PAGE_DIRTY_BITS;\n}\n\nstatic inline int pud_young(pud_t pud)\n{\n\treturn pud_flags(pud) & _PAGE_ACCESSED;\n}\n\nstatic inline int pte_write(pte_t pte)\n{\n\t \n\treturn (pte_flags(pte) & _PAGE_RW) || pte_shstk(pte);\n}\n\n#define pmd_write pmd_write\nstatic inline int pmd_write(pmd_t pmd)\n{\n\t \n\treturn (pmd_flags(pmd) & _PAGE_RW) || pmd_shstk(pmd);\n}\n\n#define pud_write pud_write\nstatic inline int pud_write(pud_t pud)\n{\n\treturn pud_flags(pud) & _PAGE_RW;\n}\n\nstatic inline int pte_huge(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_PSE;\n}\n\nstatic inline int pte_global(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_GLOBAL;\n}\n\nstatic inline int pte_exec(pte_t pte)\n{\n\treturn !(pte_flags(pte) & _PAGE_NX);\n}\n\nstatic inline int pte_special(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_SPECIAL;\n}\n\n \n\nstatic inline u64 protnone_mask(u64 val);\n\n#define PFN_PTE_SHIFT\tPAGE_SHIFT\n\nstatic inline unsigned long pte_pfn(pte_t pte)\n{\n\tphys_addr_t pfn = pte_val(pte);\n\tpfn ^= protnone_mask(pfn);\n\treturn (pfn & PTE_PFN_MASK) >> PAGE_SHIFT;\n}\n\nstatic inline unsigned long pmd_pfn(pmd_t pmd)\n{\n\tphys_addr_t pfn = pmd_val(pmd);\n\tpfn ^= protnone_mask(pfn);\n\treturn (pfn & pmd_pfn_mask(pmd)) >> PAGE_SHIFT;\n}\n\nstatic inline unsigned long pud_pfn(pud_t pud)\n{\n\tphys_addr_t pfn = pud_val(pud);\n\tpfn ^= protnone_mask(pfn);\n\treturn (pfn & pud_pfn_mask(pud)) >> PAGE_SHIFT;\n}\n\nstatic inline unsigned long p4d_pfn(p4d_t p4d)\n{\n\treturn (p4d_val(p4d) & p4d_pfn_mask(p4d)) >> PAGE_SHIFT;\n}\n\nstatic inline unsigned long pgd_pfn(pgd_t pgd)\n{\n\treturn (pgd_val(pgd) & PTE_PFN_MASK) >> PAGE_SHIFT;\n}\n\n#define p4d_leaf\tp4d_large\nstatic inline int p4d_large(p4d_t p4d)\n{\n\t \n\treturn 0;\n}\n\n#define pte_page(pte)\tpfn_to_page(pte_pfn(pte))\n\n#define pmd_leaf\tpmd_large\nstatic inline int pmd_large(pmd_t pte)\n{\n\treturn pmd_flags(pte) & _PAGE_PSE;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n \nstatic inline int pmd_trans_huge(pmd_t pmd)\n{\n\treturn (pmd_val(pmd) & (_PAGE_PSE|_PAGE_DEVMAP)) == _PAGE_PSE;\n}\n\n#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD\nstatic inline int pud_trans_huge(pud_t pud)\n{\n\treturn (pud_val(pud) & (_PAGE_PSE|_PAGE_DEVMAP)) == _PAGE_PSE;\n}\n#endif\n\n#define has_transparent_hugepage has_transparent_hugepage\nstatic inline int has_transparent_hugepage(void)\n{\n\treturn boot_cpu_has(X86_FEATURE_PSE);\n}\n\n#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP\nstatic inline int pmd_devmap(pmd_t pmd)\n{\n\treturn !!(pmd_val(pmd) & _PAGE_DEVMAP);\n}\n\n#ifdef CONFIG_HAVE_ARCH_TRANSPARENT_HUGEPAGE_PUD\nstatic inline int pud_devmap(pud_t pud)\n{\n\treturn !!(pud_val(pud) & _PAGE_DEVMAP);\n}\n#else\nstatic inline int pud_devmap(pud_t pud)\n{\n\treturn 0;\n}\n#endif\n\nstatic inline int pgd_devmap(pgd_t pgd)\n{\n\treturn 0;\n}\n#endif\n#endif  \n\nstatic inline pte_t pte_set_flags(pte_t pte, pteval_t set)\n{\n\tpteval_t v = native_pte_val(pte);\n\n\treturn native_make_pte(v | set);\n}\n\nstatic inline pte_t pte_clear_flags(pte_t pte, pteval_t clear)\n{\n\tpteval_t v = native_pte_val(pte);\n\n\treturn native_make_pte(v & ~clear);\n}\n\n \nstatic inline pgprotval_t mksaveddirty_shift(pgprotval_t v)\n{\n\tpgprotval_t cond = (~v >> _PAGE_BIT_RW) & 1;\n\n\tv |= ((v >> _PAGE_BIT_DIRTY) & cond) << _PAGE_BIT_SAVED_DIRTY;\n\tv &= ~(cond << _PAGE_BIT_DIRTY);\n\n\treturn v;\n}\n\nstatic inline pgprotval_t clear_saveddirty_shift(pgprotval_t v)\n{\n\tpgprotval_t cond = (v >> _PAGE_BIT_RW) & 1;\n\n\tv |= ((v >> _PAGE_BIT_SAVED_DIRTY) & cond) << _PAGE_BIT_DIRTY;\n\tv &= ~(cond << _PAGE_BIT_SAVED_DIRTY);\n\n\treturn v;\n}\n\nstatic inline pte_t pte_mksaveddirty(pte_t pte)\n{\n\tpteval_t v = native_pte_val(pte);\n\n\tv = mksaveddirty_shift(v);\n\treturn native_make_pte(v);\n}\n\nstatic inline pte_t pte_clear_saveddirty(pte_t pte)\n{\n\tpteval_t v = native_pte_val(pte);\n\n\tv = clear_saveddirty_shift(v);\n\treturn native_make_pte(v);\n}\n\nstatic inline pte_t pte_wrprotect(pte_t pte)\n{\n\tpte = pte_clear_flags(pte, _PAGE_RW);\n\n\t \n\treturn pte_mksaveddirty(pte);\n}\n\n#ifdef CONFIG_HAVE_ARCH_USERFAULTFD_WP\nstatic inline int pte_uffd_wp(pte_t pte)\n{\n\tbool wp = pte_flags(pte) & _PAGE_UFFD_WP;\n\n#ifdef CONFIG_DEBUG_VM\n\t \n\tWARN_ON_ONCE(wp && pte_write(pte));\n#endif\n\n\treturn wp;\n}\n\nstatic inline pte_t pte_mkuffd_wp(pte_t pte)\n{\n\treturn pte_wrprotect(pte_set_flags(pte, _PAGE_UFFD_WP));\n}\n\nstatic inline pte_t pte_clear_uffd_wp(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_UFFD_WP);\n}\n#endif  \n\nstatic inline pte_t pte_mkclean(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_DIRTY_BITS);\n}\n\nstatic inline pte_t pte_mkold(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_ACCESSED);\n}\n\nstatic inline pte_t pte_mkexec(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_NX);\n}\n\nstatic inline pte_t pte_mkdirty(pte_t pte)\n{\n\tpte = pte_set_flags(pte, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);\n\n\treturn pte_mksaveddirty(pte);\n}\n\nstatic inline pte_t pte_mkwrite_shstk(pte_t pte)\n{\n\tpte = pte_clear_flags(pte, _PAGE_RW);\n\n\treturn pte_set_flags(pte, _PAGE_DIRTY);\n}\n\nstatic inline pte_t pte_mkyoung(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_ACCESSED);\n}\n\nstatic inline pte_t pte_mkwrite_novma(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_RW);\n}\n\nstruct vm_area_struct;\npte_t pte_mkwrite(pte_t pte, struct vm_area_struct *vma);\n#define pte_mkwrite pte_mkwrite\n\nstatic inline pte_t pte_mkhuge(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_PSE);\n}\n\nstatic inline pte_t pte_clrhuge(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_PSE);\n}\n\nstatic inline pte_t pte_mkglobal(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_GLOBAL);\n}\n\nstatic inline pte_t pte_clrglobal(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_GLOBAL);\n}\n\nstatic inline pte_t pte_mkspecial(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_SPECIAL);\n}\n\nstatic inline pte_t pte_mkdevmap(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_SPECIAL|_PAGE_DEVMAP);\n}\n\nstatic inline pmd_t pmd_set_flags(pmd_t pmd, pmdval_t set)\n{\n\tpmdval_t v = native_pmd_val(pmd);\n\n\treturn native_make_pmd(v | set);\n}\n\nstatic inline pmd_t pmd_clear_flags(pmd_t pmd, pmdval_t clear)\n{\n\tpmdval_t v = native_pmd_val(pmd);\n\n\treturn native_make_pmd(v & ~clear);\n}\n\n \nstatic inline pmd_t pmd_mksaveddirty(pmd_t pmd)\n{\n\tpmdval_t v = native_pmd_val(pmd);\n\n\tv = mksaveddirty_shift(v);\n\treturn native_make_pmd(v);\n}\n\n \nstatic inline pmd_t pmd_clear_saveddirty(pmd_t pmd)\n{\n\tpmdval_t v = native_pmd_val(pmd);\n\n\tv = clear_saveddirty_shift(v);\n\treturn native_make_pmd(v);\n}\n\nstatic inline pmd_t pmd_wrprotect(pmd_t pmd)\n{\n\tpmd = pmd_clear_flags(pmd, _PAGE_RW);\n\n\t \n\treturn pmd_mksaveddirty(pmd);\n}\n\n#ifdef CONFIG_HAVE_ARCH_USERFAULTFD_WP\nstatic inline int pmd_uffd_wp(pmd_t pmd)\n{\n\treturn pmd_flags(pmd) & _PAGE_UFFD_WP;\n}\n\nstatic inline pmd_t pmd_mkuffd_wp(pmd_t pmd)\n{\n\treturn pmd_wrprotect(pmd_set_flags(pmd, _PAGE_UFFD_WP));\n}\n\nstatic inline pmd_t pmd_clear_uffd_wp(pmd_t pmd)\n{\n\treturn pmd_clear_flags(pmd, _PAGE_UFFD_WP);\n}\n#endif  \n\nstatic inline pmd_t pmd_mkold(pmd_t pmd)\n{\n\treturn pmd_clear_flags(pmd, _PAGE_ACCESSED);\n}\n\nstatic inline pmd_t pmd_mkclean(pmd_t pmd)\n{\n\treturn pmd_clear_flags(pmd, _PAGE_DIRTY_BITS);\n}\n\nstatic inline pmd_t pmd_mkdirty(pmd_t pmd)\n{\n\tpmd = pmd_set_flags(pmd, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);\n\n\treturn pmd_mksaveddirty(pmd);\n}\n\nstatic inline pmd_t pmd_mkwrite_shstk(pmd_t pmd)\n{\n\tpmd = pmd_clear_flags(pmd, _PAGE_RW);\n\n\treturn pmd_set_flags(pmd, _PAGE_DIRTY);\n}\n\nstatic inline pmd_t pmd_mkdevmap(pmd_t pmd)\n{\n\treturn pmd_set_flags(pmd, _PAGE_DEVMAP);\n}\n\nstatic inline pmd_t pmd_mkhuge(pmd_t pmd)\n{\n\treturn pmd_set_flags(pmd, _PAGE_PSE);\n}\n\nstatic inline pmd_t pmd_mkyoung(pmd_t pmd)\n{\n\treturn pmd_set_flags(pmd, _PAGE_ACCESSED);\n}\n\nstatic inline pmd_t pmd_mkwrite_novma(pmd_t pmd)\n{\n\treturn pmd_set_flags(pmd, _PAGE_RW);\n}\n\npmd_t pmd_mkwrite(pmd_t pmd, struct vm_area_struct *vma);\n#define pmd_mkwrite pmd_mkwrite\n\nstatic inline pud_t pud_set_flags(pud_t pud, pudval_t set)\n{\n\tpudval_t v = native_pud_val(pud);\n\n\treturn native_make_pud(v | set);\n}\n\nstatic inline pud_t pud_clear_flags(pud_t pud, pudval_t clear)\n{\n\tpudval_t v = native_pud_val(pud);\n\n\treturn native_make_pud(v & ~clear);\n}\n\n \nstatic inline pud_t pud_mksaveddirty(pud_t pud)\n{\n\tpudval_t v = native_pud_val(pud);\n\n\tv = mksaveddirty_shift(v);\n\treturn native_make_pud(v);\n}\n\n \nstatic inline pud_t pud_clear_saveddirty(pud_t pud)\n{\n\tpudval_t v = native_pud_val(pud);\n\n\tv = clear_saveddirty_shift(v);\n\treturn native_make_pud(v);\n}\n\nstatic inline pud_t pud_mkold(pud_t pud)\n{\n\treturn pud_clear_flags(pud, _PAGE_ACCESSED);\n}\n\nstatic inline pud_t pud_mkclean(pud_t pud)\n{\n\treturn pud_clear_flags(pud, _PAGE_DIRTY_BITS);\n}\n\nstatic inline pud_t pud_wrprotect(pud_t pud)\n{\n\tpud = pud_clear_flags(pud, _PAGE_RW);\n\n\t \n\treturn pud_mksaveddirty(pud);\n}\n\nstatic inline pud_t pud_mkdirty(pud_t pud)\n{\n\tpud = pud_set_flags(pud, _PAGE_DIRTY | _PAGE_SOFT_DIRTY);\n\n\treturn pud_mksaveddirty(pud);\n}\n\nstatic inline pud_t pud_mkdevmap(pud_t pud)\n{\n\treturn pud_set_flags(pud, _PAGE_DEVMAP);\n}\n\nstatic inline pud_t pud_mkhuge(pud_t pud)\n{\n\treturn pud_set_flags(pud, _PAGE_PSE);\n}\n\nstatic inline pud_t pud_mkyoung(pud_t pud)\n{\n\treturn pud_set_flags(pud, _PAGE_ACCESSED);\n}\n\nstatic inline pud_t pud_mkwrite(pud_t pud)\n{\n\tpud = pud_set_flags(pud, _PAGE_RW);\n\n\treturn pud_clear_saveddirty(pud);\n}\n\n#ifdef CONFIG_HAVE_ARCH_SOFT_DIRTY\nstatic inline int pte_soft_dirty(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_SOFT_DIRTY;\n}\n\nstatic inline int pmd_soft_dirty(pmd_t pmd)\n{\n\treturn pmd_flags(pmd) & _PAGE_SOFT_DIRTY;\n}\n\nstatic inline int pud_soft_dirty(pud_t pud)\n{\n\treturn pud_flags(pud) & _PAGE_SOFT_DIRTY;\n}\n\nstatic inline pte_t pte_mksoft_dirty(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_SOFT_DIRTY);\n}\n\nstatic inline pmd_t pmd_mksoft_dirty(pmd_t pmd)\n{\n\treturn pmd_set_flags(pmd, _PAGE_SOFT_DIRTY);\n}\n\nstatic inline pud_t pud_mksoft_dirty(pud_t pud)\n{\n\treturn pud_set_flags(pud, _PAGE_SOFT_DIRTY);\n}\n\nstatic inline pte_t pte_clear_soft_dirty(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_SOFT_DIRTY);\n}\n\nstatic inline pmd_t pmd_clear_soft_dirty(pmd_t pmd)\n{\n\treturn pmd_clear_flags(pmd, _PAGE_SOFT_DIRTY);\n}\n\nstatic inline pud_t pud_clear_soft_dirty(pud_t pud)\n{\n\treturn pud_clear_flags(pud, _PAGE_SOFT_DIRTY);\n}\n\n#endif  \n\n \nstatic inline pgprotval_t massage_pgprot(pgprot_t pgprot)\n{\n\tpgprotval_t protval = pgprot_val(pgprot);\n\n\tif (protval & _PAGE_PRESENT)\n\t\tprotval &= __supported_pte_mask;\n\n\treturn protval;\n}\n\nstatic inline pgprotval_t check_pgprot(pgprot_t pgprot)\n{\n\tpgprotval_t massaged_val = massage_pgprot(pgprot);\n\n\t \n#ifdef CONFIG_DEBUG_VM\n\tWARN_ONCE(pgprot_val(pgprot) != massaged_val,\n\t\t  \"attempted to set unsupported pgprot: %016llx \"\n\t\t  \"bits: %016llx supported: %016llx\\n\",\n\t\t  (u64)pgprot_val(pgprot),\n\t\t  (u64)pgprot_val(pgprot) ^ massaged_val,\n\t\t  (u64)__supported_pte_mask);\n#endif\n\n\treturn massaged_val;\n}\n\nstatic inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)\n{\n\tphys_addr_t pfn = (phys_addr_t)page_nr << PAGE_SHIFT;\n\tpfn ^= protnone_mask(pgprot_val(pgprot));\n\tpfn &= PTE_PFN_MASK;\n\treturn __pte(pfn | check_pgprot(pgprot));\n}\n\nstatic inline pmd_t pfn_pmd(unsigned long page_nr, pgprot_t pgprot)\n{\n\tphys_addr_t pfn = (phys_addr_t)page_nr << PAGE_SHIFT;\n\tpfn ^= protnone_mask(pgprot_val(pgprot));\n\tpfn &= PHYSICAL_PMD_PAGE_MASK;\n\treturn __pmd(pfn | check_pgprot(pgprot));\n}\n\nstatic inline pud_t pfn_pud(unsigned long page_nr, pgprot_t pgprot)\n{\n\tphys_addr_t pfn = (phys_addr_t)page_nr << PAGE_SHIFT;\n\tpfn ^= protnone_mask(pgprot_val(pgprot));\n\tpfn &= PHYSICAL_PUD_PAGE_MASK;\n\treturn __pud(pfn | check_pgprot(pgprot));\n}\n\nstatic inline pmd_t pmd_mkinvalid(pmd_t pmd)\n{\n\treturn pfn_pmd(pmd_pfn(pmd),\n\t\t      __pgprot(pmd_flags(pmd) & ~(_PAGE_PRESENT|_PAGE_PROTNONE)));\n}\n\nstatic inline u64 flip_protnone_guard(u64 oldval, u64 val, u64 mask);\n\nstatic inline pte_t pte_modify(pte_t pte, pgprot_t newprot)\n{\n\tpteval_t val = pte_val(pte), oldval = val;\n\tpte_t pte_result;\n\n\t \n\tval &= _PAGE_CHG_MASK;\n\tval |= check_pgprot(newprot) & ~_PAGE_CHG_MASK;\n\tval = flip_protnone_guard(oldval, val, PTE_PFN_MASK);\n\n\tpte_result = __pte(val);\n\n\t \n\tif (oldval & _PAGE_RW)\n\t\tpte_result = pte_mksaveddirty(pte_result);\n\telse\n\t\tpte_result = pte_clear_saveddirty(pte_result);\n\n\treturn pte_result;\n}\n\nstatic inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)\n{\n\tpmdval_t val = pmd_val(pmd), oldval = val;\n\tpmd_t pmd_result;\n\n\tval &= (_HPAGE_CHG_MASK & ~_PAGE_DIRTY);\n\tval |= check_pgprot(newprot) & ~_HPAGE_CHG_MASK;\n\tval = flip_protnone_guard(oldval, val, PHYSICAL_PMD_PAGE_MASK);\n\n\tpmd_result = __pmd(val);\n\n\t \n\tif (oldval & _PAGE_RW)\n\t\tpmd_result = pmd_mksaveddirty(pmd_result);\n\telse\n\t\tpmd_result = pmd_clear_saveddirty(pmd_result);\n\n\treturn pmd_result;\n}\n\n \n#define pgprot_modify pgprot_modify\nstatic inline pgprot_t pgprot_modify(pgprot_t oldprot, pgprot_t newprot)\n{\n\tpgprotval_t preservebits = pgprot_val(oldprot) & _PAGE_CHG_MASK;\n\tpgprotval_t addbits = pgprot_val(newprot) & ~_PAGE_CHG_MASK;\n\treturn __pgprot(preservebits | addbits);\n}\n\n#define pte_pgprot(x) __pgprot(pte_flags(x))\n#define pmd_pgprot(x) __pgprot(pmd_flags(x))\n#define pud_pgprot(x) __pgprot(pud_flags(x))\n#define p4d_pgprot(x) __pgprot(p4d_flags(x))\n\n#define canon_pgprot(p) __pgprot(massage_pgprot(p))\n\nstatic inline int is_new_memtype_allowed(u64 paddr, unsigned long size,\n\t\t\t\t\t enum page_cache_mode pcm,\n\t\t\t\t\t enum page_cache_mode new_pcm)\n{\n\t \n\tif (x86_platform.is_untracked_pat_range(paddr, paddr + size))\n\t\treturn 1;\n\n\t \n\tif ((pcm == _PAGE_CACHE_MODE_UC_MINUS &&\n\t     new_pcm == _PAGE_CACHE_MODE_WB) ||\n\t    (pcm == _PAGE_CACHE_MODE_WC &&\n\t     new_pcm == _PAGE_CACHE_MODE_WB) ||\n\t    (pcm == _PAGE_CACHE_MODE_WT &&\n\t     new_pcm == _PAGE_CACHE_MODE_WB) ||\n\t    (pcm == _PAGE_CACHE_MODE_WT &&\n\t     new_pcm == _PAGE_CACHE_MODE_WC)) {\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\npmd_t *populate_extra_pmd(unsigned long vaddr);\npte_t *populate_extra_pte(unsigned long vaddr);\n\n#ifdef CONFIG_PAGE_TABLE_ISOLATION\npgd_t __pti_set_user_pgtbl(pgd_t *pgdp, pgd_t pgd);\n\n \nstatic inline pgd_t pti_set_user_pgtbl(pgd_t *pgdp, pgd_t pgd)\n{\n\tif (!static_cpu_has(X86_FEATURE_PTI))\n\t\treturn pgd;\n\treturn __pti_set_user_pgtbl(pgdp, pgd);\n}\n#else    \nstatic inline pgd_t pti_set_user_pgtbl(pgd_t *pgdp, pgd_t pgd)\n{\n\treturn pgd;\n}\n#endif   \n\n#endif\t \n\n\n#ifdef CONFIG_X86_32\n# include <asm/pgtable_32.h>\n#else\n# include <asm/pgtable_64.h>\n#endif\n\n#ifndef __ASSEMBLY__\n#include <linux/mm_types.h>\n#include <linux/mmdebug.h>\n#include <linux/log2.h>\n#include <asm/fixmap.h>\n\nstatic inline int pte_none(pte_t pte)\n{\n\treturn !(pte.pte & ~(_PAGE_KNL_ERRATUM_MASK));\n}\n\n#define __HAVE_ARCH_PTE_SAME\nstatic inline int pte_same(pte_t a, pte_t b)\n{\n\treturn a.pte == b.pte;\n}\n\nstatic inline pte_t pte_next_pfn(pte_t pte)\n{\n\tif (__pte_needs_invert(pte_val(pte)))\n\t\treturn __pte(pte_val(pte) - (1UL << PFN_PTE_SHIFT));\n\treturn __pte(pte_val(pte) + (1UL << PFN_PTE_SHIFT));\n}\n#define pte_next_pfn\tpte_next_pfn\n\nstatic inline int pte_present(pte_t a)\n{\n\treturn pte_flags(a) & (_PAGE_PRESENT | _PAGE_PROTNONE);\n}\n\n#ifdef CONFIG_ARCH_HAS_PTE_DEVMAP\nstatic inline int pte_devmap(pte_t a)\n{\n\treturn (pte_flags(a) & _PAGE_DEVMAP) == _PAGE_DEVMAP;\n}\n#endif\n\n#define pte_accessible pte_accessible\nstatic inline bool pte_accessible(struct mm_struct *mm, pte_t a)\n{\n\tif (pte_flags(a) & _PAGE_PRESENT)\n\t\treturn true;\n\n\tif ((pte_flags(a) & _PAGE_PROTNONE) &&\n\t\t\tatomic_read(&mm->tlb_flush_pending))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline int pmd_present(pmd_t pmd)\n{\n\t \n\treturn pmd_flags(pmd) & (_PAGE_PRESENT | _PAGE_PROTNONE | _PAGE_PSE);\n}\n\n#ifdef CONFIG_NUMA_BALANCING\n \nstatic inline int pte_protnone(pte_t pte)\n{\n\treturn (pte_flags(pte) & (_PAGE_PROTNONE | _PAGE_PRESENT))\n\t\t== _PAGE_PROTNONE;\n}\n\nstatic inline int pmd_protnone(pmd_t pmd)\n{\n\treturn (pmd_flags(pmd) & (_PAGE_PROTNONE | _PAGE_PRESENT))\n\t\t== _PAGE_PROTNONE;\n}\n#endif  \n\nstatic inline int pmd_none(pmd_t pmd)\n{\n\t \n\tunsigned long val = native_pmd_val(pmd);\n\treturn (val & ~_PAGE_KNL_ERRATUM_MASK) == 0;\n}\n\nstatic inline unsigned long pmd_page_vaddr(pmd_t pmd)\n{\n\treturn (unsigned long)__va(pmd_val(pmd) & pmd_pfn_mask(pmd));\n}\n\n \n#define pmd_page(pmd)\tpfn_to_page(pmd_pfn(pmd))\n\n \n#define mk_pte(page, pgprot)\t\t\t\t\t\t  \\\n({\t\t\t\t\t\t\t\t\t  \\\n\tpgprot_t __pgprot = pgprot;\t\t\t\t\t  \\\n\t\t\t\t\t\t\t\t\t  \\\n\tWARN_ON_ONCE((pgprot_val(__pgprot) & (_PAGE_DIRTY | _PAGE_RW)) == \\\n\t\t    _PAGE_DIRTY);\t\t\t\t\t  \\\n\tpfn_pte(page_to_pfn(page), __pgprot);\t\t\t\t  \\\n})\n\nstatic inline int pmd_bad(pmd_t pmd)\n{\n\treturn (pmd_flags(pmd) & ~(_PAGE_USER | _PAGE_ACCESSED)) !=\n\t       (_KERNPG_TABLE & ~_PAGE_ACCESSED);\n}\n\nstatic inline unsigned long pages_to_mb(unsigned long npg)\n{\n\treturn npg >> (20 - PAGE_SHIFT);\n}\n\n#if CONFIG_PGTABLE_LEVELS > 2\nstatic inline int pud_none(pud_t pud)\n{\n\treturn (native_pud_val(pud) & ~(_PAGE_KNL_ERRATUM_MASK)) == 0;\n}\n\nstatic inline int pud_present(pud_t pud)\n{\n\treturn pud_flags(pud) & _PAGE_PRESENT;\n}\n\nstatic inline pmd_t *pud_pgtable(pud_t pud)\n{\n\treturn (pmd_t *)__va(pud_val(pud) & pud_pfn_mask(pud));\n}\n\n \n#define pud_page(pud)\tpfn_to_page(pud_pfn(pud))\n\n#define pud_leaf\tpud_large\nstatic inline int pud_large(pud_t pud)\n{\n\treturn (pud_val(pud) & (_PAGE_PSE | _PAGE_PRESENT)) ==\n\t\t(_PAGE_PSE | _PAGE_PRESENT);\n}\n\nstatic inline int pud_bad(pud_t pud)\n{\n\treturn (pud_flags(pud) & ~(_KERNPG_TABLE | _PAGE_USER)) != 0;\n}\n#else\n#define pud_leaf\tpud_large\nstatic inline int pud_large(pud_t pud)\n{\n\treturn 0;\n}\n#endif\t \n\n#if CONFIG_PGTABLE_LEVELS > 3\nstatic inline int p4d_none(p4d_t p4d)\n{\n\treturn (native_p4d_val(p4d) & ~(_PAGE_KNL_ERRATUM_MASK)) == 0;\n}\n\nstatic inline int p4d_present(p4d_t p4d)\n{\n\treturn p4d_flags(p4d) & _PAGE_PRESENT;\n}\n\nstatic inline pud_t *p4d_pgtable(p4d_t p4d)\n{\n\treturn (pud_t *)__va(p4d_val(p4d) & p4d_pfn_mask(p4d));\n}\n\n \n#define p4d_page(p4d)\tpfn_to_page(p4d_pfn(p4d))\n\nstatic inline int p4d_bad(p4d_t p4d)\n{\n\tunsigned long ignore_flags = _KERNPG_TABLE | _PAGE_USER;\n\n\tif (IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION))\n\t\tignore_flags |= _PAGE_NX;\n\n\treturn (p4d_flags(p4d) & ~ignore_flags) != 0;\n}\n#endif   \n\nstatic inline unsigned long p4d_index(unsigned long address)\n{\n\treturn (address >> P4D_SHIFT) & (PTRS_PER_P4D - 1);\n}\n\n#if CONFIG_PGTABLE_LEVELS > 4\nstatic inline int pgd_present(pgd_t pgd)\n{\n\tif (!pgtable_l5_enabled())\n\t\treturn 1;\n\treturn pgd_flags(pgd) & _PAGE_PRESENT;\n}\n\nstatic inline unsigned long pgd_page_vaddr(pgd_t pgd)\n{\n\treturn (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);\n}\n\n \n#define pgd_page(pgd)\tpfn_to_page(pgd_pfn(pgd))\n\n \nstatic inline p4d_t *p4d_offset(pgd_t *pgd, unsigned long address)\n{\n\tif (!pgtable_l5_enabled())\n\t\treturn (p4d_t *)pgd;\n\treturn (p4d_t *)pgd_page_vaddr(*pgd) + p4d_index(address);\n}\n\nstatic inline int pgd_bad(pgd_t pgd)\n{\n\tunsigned long ignore_flags = _PAGE_USER;\n\n\tif (!pgtable_l5_enabled())\n\t\treturn 0;\n\n\tif (IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION))\n\t\tignore_flags |= _PAGE_NX;\n\n\treturn (pgd_flags(pgd) & ~ignore_flags) != _KERNPG_TABLE;\n}\n\nstatic inline int pgd_none(pgd_t pgd)\n{\n\tif (!pgtable_l5_enabled())\n\t\treturn 0;\n\t \n\treturn !native_pgd_val(pgd);\n}\n#endif\t \n\n#endif\t \n\n#define KERNEL_PGD_BOUNDARY\tpgd_index(PAGE_OFFSET)\n#define KERNEL_PGD_PTRS\t\t(PTRS_PER_PGD - KERNEL_PGD_BOUNDARY)\n\n#ifndef __ASSEMBLY__\n\nextern int direct_gbpages;\nvoid init_mem_mapping(void);\nvoid early_alloc_pgt_buf(void);\nextern void memblock_find_dma_reserve(void);\nvoid __init poking_init(void);\nunsigned long init_memory_mapping(unsigned long start,\n\t\t\t\t  unsigned long end, pgprot_t prot);\n\n#ifdef CONFIG_X86_64\nextern pgd_t trampoline_pgd_entry;\n#endif\n\n \nstatic inline pte_t native_local_ptep_get_and_clear(pte_t *ptep)\n{\n\tpte_t res = *ptep;\n\n\t \n\tnative_pte_clear(NULL, 0, ptep);\n\treturn res;\n}\n\nstatic inline pmd_t native_local_pmdp_get_and_clear(pmd_t *pmdp)\n{\n\tpmd_t res = *pmdp;\n\n\tnative_pmd_clear(pmdp);\n\treturn res;\n}\n\nstatic inline pud_t native_local_pudp_get_and_clear(pud_t *pudp)\n{\n\tpud_t res = *pudp;\n\n\tnative_pud_clear(pudp);\n\treturn res;\n}\n\nstatic inline void set_pmd_at(struct mm_struct *mm, unsigned long addr,\n\t\t\t      pmd_t *pmdp, pmd_t pmd)\n{\n\tpage_table_check_pmd_set(mm, pmdp, pmd);\n\tset_pmd(pmdp, pmd);\n}\n\nstatic inline void set_pud_at(struct mm_struct *mm, unsigned long addr,\n\t\t\t      pud_t *pudp, pud_t pud)\n{\n\tpage_table_check_pud_set(mm, pudp, pud);\n\tnative_set_pud(pudp, pud);\n}\n\n \nstruct vm_area_struct;\n\n#define  __HAVE_ARCH_PTEP_SET_ACCESS_FLAGS\nextern int ptep_set_access_flags(struct vm_area_struct *vma,\n\t\t\t\t unsigned long address, pte_t *ptep,\n\t\t\t\t pte_t entry, int dirty);\n\n#define __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG\nextern int ptep_test_and_clear_young(struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr, pte_t *ptep);\n\n#define __HAVE_ARCH_PTEP_CLEAR_YOUNG_FLUSH\nextern int ptep_clear_flush_young(struct vm_area_struct *vma,\n\t\t\t\t  unsigned long address, pte_t *ptep);\n\n#define __HAVE_ARCH_PTEP_GET_AND_CLEAR\nstatic inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t       pte_t *ptep)\n{\n\tpte_t pte = native_ptep_get_and_clear(ptep);\n\tpage_table_check_pte_clear(mm, pte);\n\treturn pte;\n}\n\n#define __HAVE_ARCH_PTEP_GET_AND_CLEAR_FULL\nstatic inline pte_t ptep_get_and_clear_full(struct mm_struct *mm,\n\t\t\t\t\t    unsigned long addr, pte_t *ptep,\n\t\t\t\t\t    int full)\n{\n\tpte_t pte;\n\tif (full) {\n\t\t \n\t\tpte = native_local_ptep_get_and_clear(ptep);\n\t\tpage_table_check_pte_clear(mm, pte);\n\t} else {\n\t\tpte = ptep_get_and_clear(mm, addr, ptep);\n\t}\n\treturn pte;\n}\n\n#define __HAVE_ARCH_PTEP_SET_WRPROTECT\nstatic inline void ptep_set_wrprotect(struct mm_struct *mm,\n\t\t\t\t      unsigned long addr, pte_t *ptep)\n{\n\t \n\tpte_t old_pte, new_pte;\n\n\told_pte = READ_ONCE(*ptep);\n\tdo {\n\t\tnew_pte = pte_wrprotect(old_pte);\n\t} while (!try_cmpxchg((long *)&ptep->pte, (long *)&old_pte, *(long *)&new_pte));\n}\n\n#define flush_tlb_fix_spurious_fault(vma, address, ptep) do { } while (0)\n\n#define mk_pmd(page, pgprot)   pfn_pmd(page_to_pfn(page), (pgprot))\n\n#define  __HAVE_ARCH_PMDP_SET_ACCESS_FLAGS\nextern int pmdp_set_access_flags(struct vm_area_struct *vma,\n\t\t\t\t unsigned long address, pmd_t *pmdp,\n\t\t\t\t pmd_t entry, int dirty);\nextern int pudp_set_access_flags(struct vm_area_struct *vma,\n\t\t\t\t unsigned long address, pud_t *pudp,\n\t\t\t\t pud_t entry, int dirty);\n\n#define __HAVE_ARCH_PMDP_TEST_AND_CLEAR_YOUNG\nextern int pmdp_test_and_clear_young(struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr, pmd_t *pmdp);\nextern int pudp_test_and_clear_young(struct vm_area_struct *vma,\n\t\t\t\t     unsigned long addr, pud_t *pudp);\n\n#define __HAVE_ARCH_PMDP_CLEAR_YOUNG_FLUSH\nextern int pmdp_clear_flush_young(struct vm_area_struct *vma,\n\t\t\t\t  unsigned long address, pmd_t *pmdp);\n\n\n#define __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR\nstatic inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm, unsigned long addr,\n\t\t\t\t       pmd_t *pmdp)\n{\n\tpmd_t pmd = native_pmdp_get_and_clear(pmdp);\n\n\tpage_table_check_pmd_clear(mm, pmd);\n\n\treturn pmd;\n}\n\n#define __HAVE_ARCH_PUDP_HUGE_GET_AND_CLEAR\nstatic inline pud_t pudp_huge_get_and_clear(struct mm_struct *mm,\n\t\t\t\t\tunsigned long addr, pud_t *pudp)\n{\n\tpud_t pud = native_pudp_get_and_clear(pudp);\n\n\tpage_table_check_pud_clear(mm, pud);\n\n\treturn pud;\n}\n\n#define __HAVE_ARCH_PMDP_SET_WRPROTECT\nstatic inline void pmdp_set_wrprotect(struct mm_struct *mm,\n\t\t\t\t      unsigned long addr, pmd_t *pmdp)\n{\n\t \n\tpmd_t old_pmd, new_pmd;\n\n\told_pmd = READ_ONCE(*pmdp);\n\tdo {\n\t\tnew_pmd = pmd_wrprotect(old_pmd);\n\t} while (!try_cmpxchg((long *)pmdp, (long *)&old_pmd, *(long *)&new_pmd));\n}\n\n#ifndef pmdp_establish\n#define pmdp_establish pmdp_establish\nstatic inline pmd_t pmdp_establish(struct vm_area_struct *vma,\n\t\tunsigned long address, pmd_t *pmdp, pmd_t pmd)\n{\n\tpage_table_check_pmd_set(vma->vm_mm, pmdp, pmd);\n\tif (IS_ENABLED(CONFIG_SMP)) {\n\t\treturn xchg(pmdp, pmd);\n\t} else {\n\t\tpmd_t old = *pmdp;\n\t\tWRITE_ONCE(*pmdp, pmd);\n\t\treturn old;\n\t}\n}\n#endif\n\n#define __HAVE_ARCH_PMDP_INVALIDATE_AD\nextern pmd_t pmdp_invalidate_ad(struct vm_area_struct *vma,\n\t\t\t\tunsigned long address, pmd_t *pmdp);\n\n \nstatic inline bool pgdp_maps_userspace(void *__ptr)\n{\n\tunsigned long ptr = (unsigned long)__ptr;\n\n\treturn (((ptr & ~PAGE_MASK) / sizeof(pgd_t)) < PGD_KERNEL_START);\n}\n\n#define pgd_leaf\tpgd_large\nstatic inline int pgd_large(pgd_t pgd) { return 0; }\n\n#ifdef CONFIG_PAGE_TABLE_ISOLATION\n \n#define PTI_PGTABLE_SWITCH_BIT\tPAGE_SHIFT\n\n \nstatic inline void *ptr_set_bit(void *ptr, int bit)\n{\n\tunsigned long __ptr = (unsigned long)ptr;\n\n\t__ptr |= BIT(bit);\n\treturn (void *)__ptr;\n}\nstatic inline void *ptr_clear_bit(void *ptr, int bit)\n{\n\tunsigned long __ptr = (unsigned long)ptr;\n\n\t__ptr &= ~BIT(bit);\n\treturn (void *)__ptr;\n}\n\nstatic inline pgd_t *kernel_to_user_pgdp(pgd_t *pgdp)\n{\n\treturn ptr_set_bit(pgdp, PTI_PGTABLE_SWITCH_BIT);\n}\n\nstatic inline pgd_t *user_to_kernel_pgdp(pgd_t *pgdp)\n{\n\treturn ptr_clear_bit(pgdp, PTI_PGTABLE_SWITCH_BIT);\n}\n\nstatic inline p4d_t *kernel_to_user_p4dp(p4d_t *p4dp)\n{\n\treturn ptr_set_bit(p4dp, PTI_PGTABLE_SWITCH_BIT);\n}\n\nstatic inline p4d_t *user_to_kernel_p4dp(p4d_t *p4dp)\n{\n\treturn ptr_clear_bit(p4dp, PTI_PGTABLE_SWITCH_BIT);\n}\n#endif  \n\n \nstatic inline void clone_pgd_range(pgd_t *dst, pgd_t *src, int count)\n{\n\tmemcpy(dst, src, count * sizeof(pgd_t));\n#ifdef CONFIG_PAGE_TABLE_ISOLATION\n\tif (!static_cpu_has(X86_FEATURE_PTI))\n\t\treturn;\n\t \n\tmemcpy(kernel_to_user_pgdp(dst), kernel_to_user_pgdp(src),\n\t       count * sizeof(pgd_t));\n#endif\n}\n\n#define PTE_SHIFT ilog2(PTRS_PER_PTE)\nstatic inline int page_level_shift(enum pg_level level)\n{\n\treturn (PAGE_SHIFT - PTE_SHIFT) + level * PTE_SHIFT;\n}\nstatic inline unsigned long page_level_size(enum pg_level level)\n{\n\treturn 1UL << page_level_shift(level);\n}\nstatic inline unsigned long page_level_mask(enum pg_level level)\n{\n\treturn ~(page_level_size(level) - 1);\n}\n\n \nstatic inline void update_mmu_cache(struct vm_area_struct *vma,\n\t\tunsigned long addr, pte_t *ptep)\n{\n}\nstatic inline void update_mmu_cache_range(struct vm_fault *vmf,\n\t\tstruct vm_area_struct *vma, unsigned long addr,\n\t\tpte_t *ptep, unsigned int nr)\n{\n}\nstatic inline void update_mmu_cache_pmd(struct vm_area_struct *vma,\n\t\tunsigned long addr, pmd_t *pmd)\n{\n}\nstatic inline void update_mmu_cache_pud(struct vm_area_struct *vma,\n\t\tunsigned long addr, pud_t *pud)\n{\n}\nstatic inline pte_t pte_swp_mkexclusive(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_SWP_EXCLUSIVE);\n}\n\nstatic inline int pte_swp_exclusive(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_SWP_EXCLUSIVE;\n}\n\nstatic inline pte_t pte_swp_clear_exclusive(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_SWP_EXCLUSIVE);\n}\n\n#ifdef CONFIG_HAVE_ARCH_SOFT_DIRTY\nstatic inline pte_t pte_swp_mksoft_dirty(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_SWP_SOFT_DIRTY);\n}\n\nstatic inline int pte_swp_soft_dirty(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_SWP_SOFT_DIRTY;\n}\n\nstatic inline pte_t pte_swp_clear_soft_dirty(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_SWP_SOFT_DIRTY);\n}\n\n#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION\nstatic inline pmd_t pmd_swp_mksoft_dirty(pmd_t pmd)\n{\n\treturn pmd_set_flags(pmd, _PAGE_SWP_SOFT_DIRTY);\n}\n\nstatic inline int pmd_swp_soft_dirty(pmd_t pmd)\n{\n\treturn pmd_flags(pmd) & _PAGE_SWP_SOFT_DIRTY;\n}\n\nstatic inline pmd_t pmd_swp_clear_soft_dirty(pmd_t pmd)\n{\n\treturn pmd_clear_flags(pmd, _PAGE_SWP_SOFT_DIRTY);\n}\n#endif\n#endif\n\n#ifdef CONFIG_HAVE_ARCH_USERFAULTFD_WP\nstatic inline pte_t pte_swp_mkuffd_wp(pte_t pte)\n{\n\treturn pte_set_flags(pte, _PAGE_SWP_UFFD_WP);\n}\n\nstatic inline int pte_swp_uffd_wp(pte_t pte)\n{\n\treturn pte_flags(pte) & _PAGE_SWP_UFFD_WP;\n}\n\nstatic inline pte_t pte_swp_clear_uffd_wp(pte_t pte)\n{\n\treturn pte_clear_flags(pte, _PAGE_SWP_UFFD_WP);\n}\n\nstatic inline pmd_t pmd_swp_mkuffd_wp(pmd_t pmd)\n{\n\treturn pmd_set_flags(pmd, _PAGE_SWP_UFFD_WP);\n}\n\nstatic inline int pmd_swp_uffd_wp(pmd_t pmd)\n{\n\treturn pmd_flags(pmd) & _PAGE_SWP_UFFD_WP;\n}\n\nstatic inline pmd_t pmd_swp_clear_uffd_wp(pmd_t pmd)\n{\n\treturn pmd_clear_flags(pmd, _PAGE_SWP_UFFD_WP);\n}\n#endif  \n\nstatic inline u16 pte_flags_pkey(unsigned long pte_flags)\n{\n#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS\n\t \n\treturn (pte_flags & _PAGE_PKEY_MASK) >> _PAGE_BIT_PKEY_BIT0;\n#else\n\treturn 0;\n#endif\n}\n\nstatic inline bool __pkru_allows_pkey(u16 pkey, bool write)\n{\n\tu32 pkru = read_pkru();\n\n\tif (!__pkru_allows_read(pkru, pkey))\n\t\treturn false;\n\tif (write && !__pkru_allows_write(pkru, pkey))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nstatic inline bool __pte_access_permitted(unsigned long pteval, bool write)\n{\n\tunsigned long need_pte_bits = _PAGE_PRESENT|_PAGE_USER;\n\n\t \n\tif (write)\n\t\tneed_pte_bits |= _PAGE_RW;\n\n\tif ((pteval & need_pte_bits) != need_pte_bits)\n\t\treturn 0;\n\n\treturn __pkru_allows_pkey(pte_flags_pkey(pteval), write);\n}\n\n#define pte_access_permitted pte_access_permitted\nstatic inline bool pte_access_permitted(pte_t pte, bool write)\n{\n\treturn __pte_access_permitted(pte_val(pte), write);\n}\n\n#define pmd_access_permitted pmd_access_permitted\nstatic inline bool pmd_access_permitted(pmd_t pmd, bool write)\n{\n\treturn __pte_access_permitted(pmd_val(pmd), write);\n}\n\n#define pud_access_permitted pud_access_permitted\nstatic inline bool pud_access_permitted(pud_t pud, bool write)\n{\n\treturn __pte_access_permitted(pud_val(pud), write);\n}\n\n#define __HAVE_ARCH_PFN_MODIFY_ALLOWED 1\nextern bool pfn_modify_allowed(unsigned long pfn, pgprot_t prot);\n\nstatic inline bool arch_has_pfn_modify_check(void)\n{\n\treturn boot_cpu_has_bug(X86_BUG_L1TF);\n}\n\n#define arch_has_hw_pte_young arch_has_hw_pte_young\nstatic inline bool arch_has_hw_pte_young(void)\n{\n\treturn true;\n}\n\n#define arch_check_zapped_pte arch_check_zapped_pte\nvoid arch_check_zapped_pte(struct vm_area_struct *vma, pte_t pte);\n\n#define arch_check_zapped_pmd arch_check_zapped_pmd\nvoid arch_check_zapped_pmd(struct vm_area_struct *vma, pmd_t pmd);\n\n#ifdef CONFIG_XEN_PV\n#define arch_has_hw_nonleaf_pmd_young arch_has_hw_nonleaf_pmd_young\nstatic inline bool arch_has_hw_nonleaf_pmd_young(void)\n{\n\treturn !cpu_feature_enabled(X86_FEATURE_XENPV);\n}\n#endif\n\n#ifdef CONFIG_PAGE_TABLE_CHECK\nstatic inline bool pte_user_accessible_page(pte_t pte)\n{\n\treturn (pte_val(pte) & _PAGE_PRESENT) && (pte_val(pte) & _PAGE_USER);\n}\n\nstatic inline bool pmd_user_accessible_page(pmd_t pmd)\n{\n\treturn pmd_leaf(pmd) && (pmd_val(pmd) & _PAGE_PRESENT) && (pmd_val(pmd) & _PAGE_USER);\n}\n\nstatic inline bool pud_user_accessible_page(pud_t pud)\n{\n\treturn pud_leaf(pud) && (pud_val(pud) & _PAGE_PRESENT) && (pud_val(pud) & _PAGE_USER);\n}\n#endif\n\n#endif\t \n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}