{
  "module_name": "uaccess_64.h",
  "hash_id": "58f8d20ef0e1a878e9bde6e2930732fb593b13794e6e7361b0c6647d91bdbb2d",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/include/asm/uaccess_64.h",
  "human_readable_source": " \n#ifndef _ASM_X86_UACCESS_64_H\n#define _ASM_X86_UACCESS_64_H\n\n \n#include <linux/compiler.h>\n#include <linux/lockdep.h>\n#include <linux/kasan-checks.h>\n#include <asm/alternative.h>\n#include <asm/cpufeatures.h>\n#include <asm/page.h>\n\n#ifdef CONFIG_ADDRESS_MASKING\n \nstatic inline unsigned long __untagged_addr(unsigned long addr)\n{\n\t \n\tasm (ALTERNATIVE(\"\",\n\t\t\t \"and %%gs:tlbstate_untag_mask, %[addr]\\n\\t\", X86_FEATURE_LAM)\n\t     : [addr] \"+r\" (addr) : \"m\" (tlbstate_untag_mask));\n\n\treturn addr;\n}\n\n#define untagged_addr(addr)\t({\t\t\t\t\t\\\n\tunsigned long __addr = (__force unsigned long)(addr);\t\t\\\n\t(__force __typeof__(addr))__untagged_addr(__addr);\t\t\\\n})\n\nstatic inline unsigned long __untagged_addr_remote(struct mm_struct *mm,\n\t\t\t\t\t\t   unsigned long addr)\n{\n\tmmap_assert_locked(mm);\n\treturn addr & (mm)->context.untag_mask;\n}\n\n#define untagged_addr_remote(mm, addr)\t({\t\t\t\t\\\n\tunsigned long __addr = (__force unsigned long)(addr);\t\t\\\n\t(__force __typeof__(addr))__untagged_addr_remote(mm, __addr);\t\\\n})\n\n#endif\n\n \n#define valid_user_address(x) ((long)(x) >= 0)\n\n \nstatic inline bool __access_ok(const void __user *ptr, unsigned long size)\n{\n\tif (__builtin_constant_p(size <= PAGE_SIZE) && size <= PAGE_SIZE) {\n\t\treturn valid_user_address(ptr);\n\t} else {\n\t\tunsigned long sum = size + (unsigned long)ptr;\n\t\treturn valid_user_address(sum) && sum >= (unsigned long)ptr;\n\t}\n}\n#define __access_ok __access_ok\n\n \n\n \n__must_check unsigned long\nrep_movs_alternative(void *to, const void *from, unsigned len);\n\nstatic __always_inline __must_check unsigned long\ncopy_user_generic(void *to, const void *from, unsigned long len)\n{\n\tstac();\n\t \n\tasm volatile(\n\t\t\"1:\\n\\t\"\n\t\tALTERNATIVE(\"rep movsb\",\n\t\t\t    \"call rep_movs_alternative\", ALT_NOT(X86_FEATURE_FSRM))\n\t\t\"2:\\n\"\n\t\t_ASM_EXTABLE_UA(1b, 2b)\n\t\t:\"+c\" (len), \"+D\" (to), \"+S\" (from), ASM_CALL_CONSTRAINT\n\t\t: : \"memory\", \"rax\");\n\tclac();\n\treturn len;\n}\n\nstatic __always_inline __must_check unsigned long\nraw_copy_from_user(void *dst, const void __user *src, unsigned long size)\n{\n\treturn copy_user_generic(dst, (__force void *)src, size);\n}\n\nstatic __always_inline __must_check unsigned long\nraw_copy_to_user(void __user *dst, const void *src, unsigned long size)\n{\n\treturn copy_user_generic((__force void *)dst, src, size);\n}\n\nextern long __copy_user_nocache(void *dst, const void __user *src, unsigned size);\nextern long __copy_user_flushcache(void *dst, const void __user *src, unsigned size);\n\nstatic inline int\n__copy_from_user_inatomic_nocache(void *dst, const void __user *src,\n\t\t\t\t  unsigned size)\n{\n\tlong ret;\n\tkasan_check_write(dst, size);\n\tstac();\n\tret = __copy_user_nocache(dst, src, size);\n\tclac();\n\treturn ret;\n}\n\nstatic inline int\n__copy_from_user_flushcache(void *dst, const void __user *src, unsigned size)\n{\n\tkasan_check_write(dst, size);\n\treturn __copy_user_flushcache(dst, src, size);\n}\n\n \n\n__must_check unsigned long\nrep_stos_alternative(void __user *addr, unsigned long len);\n\nstatic __always_inline __must_check unsigned long __clear_user(void __user *addr, unsigned long size)\n{\n\tmight_fault();\n\tstac();\n\n\t \n\tasm volatile(\n\t\t\"1:\\n\\t\"\n\t\tALTERNATIVE(\"rep stosb\",\n\t\t\t    \"call rep_stos_alternative\", ALT_NOT(X86_FEATURE_FSRS))\n\t\t\"2:\\n\"\n\t       _ASM_EXTABLE_UA(1b, 2b)\n\t       : \"+c\" (size), \"+D\" (addr), ASM_CALL_CONSTRAINT\n\t       : \"a\" (0));\n\n\tclac();\n\n\treturn size;\n}\n\nstatic __always_inline unsigned long clear_user(void __user *to, unsigned long n)\n{\n\tif (__access_ok(to, n))\n\t\treturn __clear_user(to, n);\n\treturn n;\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}