{
  "module_name": "processor.h",
  "hash_id": "0b499c8a21ca627683fba03acfaf6e95904178bbb8ec1fc3a751675c5f2f2ee5",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/include/asm/processor.h",
  "human_readable_source": " \n#ifndef _ASM_X86_PROCESSOR_H\n#define _ASM_X86_PROCESSOR_H\n\n#include <asm/processor-flags.h>\n\n \nstruct task_struct;\nstruct mm_struct;\nstruct io_bitmap;\nstruct vm86;\n\n#include <asm/math_emu.h>\n#include <asm/segment.h>\n#include <asm/types.h>\n#include <uapi/asm/sigcontext.h>\n#include <asm/current.h>\n#include <asm/cpufeatures.h>\n#include <asm/cpuid.h>\n#include <asm/page.h>\n#include <asm/pgtable_types.h>\n#include <asm/percpu.h>\n#include <asm/msr.h>\n#include <asm/desc_defs.h>\n#include <asm/nops.h>\n#include <asm/special_insns.h>\n#include <asm/fpu/types.h>\n#include <asm/unwind_hints.h>\n#include <asm/vmxfeatures.h>\n#include <asm/vdso/processor.h>\n#include <asm/shstk.h>\n\n#include <linux/personality.h>\n#include <linux/cache.h>\n#include <linux/threads.h>\n#include <linux/math64.h>\n#include <linux/err.h>\n#include <linux/irqflags.h>\n#include <linux/mem_encrypt.h>\n\n \n#define NET_IP_ALIGN\t0\n\n#define HBP_NUM 4\n\n \n#ifdef CONFIG_X86_VSMP\n# define ARCH_MIN_TASKALIGN\t\t(1 << INTERNODE_CACHE_SHIFT)\n# define ARCH_MIN_MMSTRUCT_ALIGN\t(1 << INTERNODE_CACHE_SHIFT)\n#else\n# define ARCH_MIN_TASKALIGN\t\t__alignof__(union fpregs_state)\n# define ARCH_MIN_MMSTRUCT_ALIGN\t0\n#endif\n\nenum tlb_infos {\n\tENTRIES,\n\tNR_INFO\n};\n\nextern u16 __read_mostly tlb_lli_4k[NR_INFO];\nextern u16 __read_mostly tlb_lli_2m[NR_INFO];\nextern u16 __read_mostly tlb_lli_4m[NR_INFO];\nextern u16 __read_mostly tlb_lld_4k[NR_INFO];\nextern u16 __read_mostly tlb_lld_2m[NR_INFO];\nextern u16 __read_mostly tlb_lld_4m[NR_INFO];\nextern u16 __read_mostly tlb_lld_1g[NR_INFO];\n\n \n\nstruct cpuinfo_x86 {\n\t__u8\t\t\tx86;\t\t \n\t__u8\t\t\tx86_vendor;\t \n\t__u8\t\t\tx86_model;\n\t__u8\t\t\tx86_stepping;\n#ifdef CONFIG_X86_64\n\t \n\tint\t\t\tx86_tlbsize;\n#endif\n#ifdef CONFIG_X86_VMX_FEATURE_NAMES\n\t__u32\t\t\tvmx_capability[NVMXINTS];\n#endif\n\t__u8\t\t\tx86_virt_bits;\n\t__u8\t\t\tx86_phys_bits;\n\t \n\t__u8\t\t\tx86_coreid_bits;\n\t__u8\t\t\tcu_id;\n\t \n\t__u32\t\t\textended_cpuid_level;\n\t \n\tint\t\t\tcpuid_level;\n\t \n\tunion {\n\t\t__u32\t\tx86_capability[NCAPINTS + NBUGINTS];\n\t\tunsigned long\tx86_capability_alignment;\n\t};\n\tchar\t\t\tx86_vendor_id[16];\n\tchar\t\t\tx86_model_id[64];\n\t \n\tunsigned int\t\tx86_cache_size;\n\tint\t\t\tx86_cache_alignment;\t \n\t \n\tint\t\t\tx86_cache_max_rmid;\t \n\tint\t\t\tx86_cache_occ_scale;\t \n\tint\t\t\tx86_cache_mbm_width_offset;\n\tint\t\t\tx86_power;\n\tunsigned long\t\tloops_per_jiffy;\n\t \n\tu64\t\t\tppin;\n\t \n\tu16\t\t\tx86_max_cores;\n\tu16\t\t\tapicid;\n\tu16\t\t\tinitial_apicid;\n\tu16\t\t\tx86_clflush_size;\n\t \n\tu16\t\t\tbooted_cores;\n\t \n\tu16\t\t\tphys_proc_id;\n\t \n\tu16\t\t\tlogical_proc_id;\n\t \n\tu16\t\t\tcpu_core_id;\n\tu16\t\t\tcpu_die_id;\n\tu16\t\t\tlogical_die_id;\n\t \n\tu16\t\t\tcpu_index;\n\t \n\tbool\t\t\tsmt_active;\n\tu32\t\t\tmicrocode;\n\t \n\tu8\t\t\tx86_cache_bits;\n\tunsigned\t\tinitialized : 1;\n} __randomize_layout;\n\n#define X86_VENDOR_INTEL\t0\n#define X86_VENDOR_CYRIX\t1\n#define X86_VENDOR_AMD\t\t2\n#define X86_VENDOR_UMC\t\t3\n#define X86_VENDOR_CENTAUR\t5\n#define X86_VENDOR_TRANSMETA\t7\n#define X86_VENDOR_NSC\t\t8\n#define X86_VENDOR_HYGON\t9\n#define X86_VENDOR_ZHAOXIN\t10\n#define X86_VENDOR_VORTEX\t11\n#define X86_VENDOR_NUM\t\t12\n\n#define X86_VENDOR_UNKNOWN\t0xff\n\n \nextern struct cpuinfo_x86\tboot_cpu_data;\nextern struct cpuinfo_x86\tnew_cpu_data;\n\nextern __u32\t\t\tcpu_caps_cleared[NCAPINTS + NBUGINTS];\nextern __u32\t\t\tcpu_caps_set[NCAPINTS + NBUGINTS];\n\n#ifdef CONFIG_SMP\nDECLARE_PER_CPU_READ_MOSTLY(struct cpuinfo_x86, cpu_info);\n#define cpu_data(cpu)\t\tper_cpu(cpu_info, cpu)\n#else\n#define cpu_info\t\tboot_cpu_data\n#define cpu_data(cpu)\t\tboot_cpu_data\n#endif\n\nextern const struct seq_operations cpuinfo_op;\n\n#define cache_line_size()\t(boot_cpu_data.x86_cache_alignment)\n\nextern void cpu_detect(struct cpuinfo_x86 *c);\n\nstatic inline unsigned long long l1tf_pfn_limit(void)\n{\n\treturn BIT_ULL(boot_cpu_data.x86_cache_bits - 1 - PAGE_SHIFT);\n}\n\nextern void early_cpu_init(void);\nextern void identify_secondary_cpu(struct cpuinfo_x86 *);\nextern void print_cpu_info(struct cpuinfo_x86 *);\nvoid print_cpu_msr(struct cpuinfo_x86 *);\n\n \nstatic inline unsigned long read_cr3_pa(void)\n{\n\treturn __read_cr3() & CR3_ADDR_MASK;\n}\n\nstatic inline unsigned long native_read_cr3_pa(void)\n{\n\treturn __native_read_cr3() & CR3_ADDR_MASK;\n}\n\nstatic inline void load_cr3(pgd_t *pgdir)\n{\n\twrite_cr3(__sme_pa(pgdir));\n}\n\n \n#ifdef CONFIG_X86_32\n \nstruct x86_hw_tss {\n\tunsigned short\t\tback_link, __blh;\n\tunsigned long\t\tsp0;\n\tunsigned short\t\tss0, __ss0h;\n\tunsigned long\t\tsp1;\n\n\t \n\tunsigned short\t\tss1;\t \n\n\tunsigned short\t\t__ss1h;\n\tunsigned long\t\tsp2;\n\tunsigned short\t\tss2, __ss2h;\n\tunsigned long\t\t__cr3;\n\tunsigned long\t\tip;\n\tunsigned long\t\tflags;\n\tunsigned long\t\tax;\n\tunsigned long\t\tcx;\n\tunsigned long\t\tdx;\n\tunsigned long\t\tbx;\n\tunsigned long\t\tsp;\n\tunsigned long\t\tbp;\n\tunsigned long\t\tsi;\n\tunsigned long\t\tdi;\n\tunsigned short\t\tes, __esh;\n\tunsigned short\t\tcs, __csh;\n\tunsigned short\t\tss, __ssh;\n\tunsigned short\t\tds, __dsh;\n\tunsigned short\t\tfs, __fsh;\n\tunsigned short\t\tgs, __gsh;\n\tunsigned short\t\tldt, __ldth;\n\tunsigned short\t\ttrace;\n\tunsigned short\t\tio_bitmap_base;\n\n} __attribute__((packed));\n#else\nstruct x86_hw_tss {\n\tu32\t\t\treserved1;\n\tu64\t\t\tsp0;\n\tu64\t\t\tsp1;\n\n\t \n\tu64\t\t\tsp2;\n\n\tu64\t\t\treserved2;\n\tu64\t\t\tist[7];\n\tu32\t\t\treserved3;\n\tu32\t\t\treserved4;\n\tu16\t\t\treserved5;\n\tu16\t\t\tio_bitmap_base;\n\n} __attribute__((packed));\n#endif\n\n \n#define IO_BITMAP_BITS\t\t\t65536\n#define IO_BITMAP_BYTES\t\t\t(IO_BITMAP_BITS / BITS_PER_BYTE)\n#define IO_BITMAP_LONGS\t\t\t(IO_BITMAP_BYTES / sizeof(long))\n\n#define IO_BITMAP_OFFSET_VALID_MAP\t\t\t\t\\\n\t(offsetof(struct tss_struct, io_bitmap.bitmap) -\t\\\n\t offsetof(struct tss_struct, x86_tss))\n\n#define IO_BITMAP_OFFSET_VALID_ALL\t\t\t\t\\\n\t(offsetof(struct tss_struct, io_bitmap.mapall) -\t\\\n\t offsetof(struct tss_struct, x86_tss))\n\n#ifdef CONFIG_X86_IOPL_IOPERM\n \n# define __KERNEL_TSS_LIMIT\t\\\n\t(IO_BITMAP_OFFSET_VALID_ALL + IO_BITMAP_BYTES + \\\n\t sizeof(unsigned long) - 1)\n#else\n# define __KERNEL_TSS_LIMIT\t\\\n\t(offsetof(struct tss_struct, x86_tss) + sizeof(struct x86_hw_tss) - 1)\n#endif\n\n \n#define IO_BITMAP_OFFSET_INVALID\t(__KERNEL_TSS_LIMIT + 1)\n\nstruct entry_stack {\n\tchar\tstack[PAGE_SIZE];\n};\n\nstruct entry_stack_page {\n\tstruct entry_stack stack;\n} __aligned(PAGE_SIZE);\n\n \nstruct x86_io_bitmap {\n\t \n\tu64\t\t\tprev_sequence;\n\n\t \n\tunsigned int\t\tprev_max;\n\n\t \n\tunsigned long\t\tbitmap[IO_BITMAP_LONGS + 1];\n\n\t \n\tunsigned long\t\tmapall[IO_BITMAP_LONGS + 1];\n};\n\nstruct tss_struct {\n\t \n\tstruct x86_hw_tss\tx86_tss;\n\n\tstruct x86_io_bitmap\tio_bitmap;\n} __aligned(PAGE_SIZE);\n\nDECLARE_PER_CPU_PAGE_ALIGNED(struct tss_struct, cpu_tss_rw);\n\n \nstruct irq_stack {\n\tchar\t\tstack[IRQ_STACK_SIZE];\n} __aligned(IRQ_STACK_SIZE);\n\n#ifdef CONFIG_X86_64\nstruct fixed_percpu_data {\n\t \n\tchar\t\tgs_base[40];\n\tunsigned long\tstack_canary;\n};\n\nDECLARE_PER_CPU_FIRST(struct fixed_percpu_data, fixed_percpu_data) __visible;\nDECLARE_INIT_PER_CPU(fixed_percpu_data);\n\nstatic inline unsigned long cpu_kernelmode_gs_base(int cpu)\n{\n\treturn (unsigned long)per_cpu(fixed_percpu_data.gs_base, cpu);\n}\n\nextern asmlinkage void ignore_sysret(void);\n\n \nvoid current_save_fsgs(void);\n#else\t \n#ifdef CONFIG_STACKPROTECTOR\nDECLARE_PER_CPU(unsigned long, __stack_chk_guard);\n#endif\n#endif\t \n\nstruct perf_event;\n\nstruct thread_struct {\n\t \n\tstruct desc_struct\ttls_array[GDT_ENTRY_TLS_ENTRIES];\n#ifdef CONFIG_X86_32\n\tunsigned long\t\tsp0;\n#endif\n\tunsigned long\t\tsp;\n#ifdef CONFIG_X86_32\n\tunsigned long\t\tsysenter_cs;\n#else\n\tunsigned short\t\tes;\n\tunsigned short\t\tds;\n\tunsigned short\t\tfsindex;\n\tunsigned short\t\tgsindex;\n#endif\n\n#ifdef CONFIG_X86_64\n\tunsigned long\t\tfsbase;\n\tunsigned long\t\tgsbase;\n#else\n\t \n\tunsigned long fs;\n\tunsigned long gs;\n#endif\n\n\t \n\tstruct perf_event\t*ptrace_bps[HBP_NUM];\n\t \n\tunsigned long           virtual_dr6;\n\t \n\tunsigned long           ptrace_dr7;\n\t \n\tunsigned long\t\tcr2;\n\tunsigned long\t\ttrap_nr;\n\tunsigned long\t\terror_code;\n#ifdef CONFIG_VM86\n\t \n\tstruct vm86\t\t*vm86;\n#endif\n\t \n\tstruct io_bitmap\t*io_bitmap;\n\n\t \n\tunsigned long\t\tiopl_emul;\n\n\tunsigned int\t\tiopl_warn:1;\n\tunsigned int\t\tsig_on_uaccess_err:1;\n\n\t \n\tu32\t\t\tpkru;\n\n#ifdef CONFIG_X86_USER_SHADOW_STACK\n\tunsigned long\t\tfeatures;\n\tunsigned long\t\tfeatures_locked;\n\n\tstruct thread_shstk\tshstk;\n#endif\n\n\t \n\tstruct fpu\t\tfpu;\n\t \n};\n\nextern void fpu_thread_struct_whitelist(unsigned long *offset, unsigned long *size);\n\nstatic inline void arch_thread_struct_whitelist(unsigned long *offset,\n\t\t\t\t\t\tunsigned long *size)\n{\n\tfpu_thread_struct_whitelist(offset, size);\n}\n\nstatic inline void\nnative_load_sp0(unsigned long sp0)\n{\n\tthis_cpu_write(cpu_tss_rw.x86_tss.sp0, sp0);\n}\n\nstatic __always_inline void native_swapgs(void)\n{\n#ifdef CONFIG_X86_64\n\tasm volatile(\"swapgs\" ::: \"memory\");\n#endif\n}\n\nstatic __always_inline unsigned long current_top_of_stack(void)\n{\n\t \n\treturn this_cpu_read_stable(pcpu_hot.top_of_stack);\n}\n\nstatic __always_inline bool on_thread_stack(void)\n{\n\treturn (unsigned long)(current_top_of_stack() -\n\t\t\t       current_stack_pointer) < THREAD_SIZE;\n}\n\n#ifdef CONFIG_PARAVIRT_XXL\n#include <asm/paravirt.h>\n#else\n\nstatic inline void load_sp0(unsigned long sp0)\n{\n\tnative_load_sp0(sp0);\n}\n\n#endif  \n\nunsigned long __get_wchan(struct task_struct *p);\n\nextern void select_idle_routine(const struct cpuinfo_x86 *c);\nextern void amd_e400_c1e_apic_setup(void);\n\nextern unsigned long\t\tboot_option_idle_override;\n\nenum idle_boot_override {IDLE_NO_OVERRIDE=0, IDLE_HALT, IDLE_NOMWAIT,\n\t\t\t IDLE_POLL};\n\nextern void enable_sep_cpu(void);\n\n\n \nextern struct desc_ptr\t\tearly_gdt_descr;\n\nextern void switch_gdt_and_percpu_base(int);\nextern void load_direct_gdt(int);\nextern void load_fixmap_gdt(int);\nextern void cpu_init(void);\nextern void cpu_init_exception_handling(void);\nextern void cr4_init(void);\n\nstatic inline unsigned long get_debugctlmsr(void)\n{\n\tunsigned long debugctlmsr = 0;\n\n#ifndef CONFIG_X86_DEBUGCTLMSR\n\tif (boot_cpu_data.x86 < 6)\n\t\treturn 0;\n#endif\n\trdmsrl(MSR_IA32_DEBUGCTLMSR, debugctlmsr);\n\n\treturn debugctlmsr;\n}\n\nstatic inline void update_debugctlmsr(unsigned long debugctlmsr)\n{\n#ifndef CONFIG_X86_DEBUGCTLMSR\n\tif (boot_cpu_data.x86 < 6)\n\t\treturn;\n#endif\n\twrmsrl(MSR_IA32_DEBUGCTLMSR, debugctlmsr);\n}\n\nextern void set_task_blockstep(struct task_struct *task, bool on);\n\n \nextern int\t\t\tbootloader_type;\nextern int\t\t\tbootloader_version;\n\nextern char\t\t\tignore_fpu_irq;\n\n#define HAVE_ARCH_PICK_MMAP_LAYOUT 1\n#define ARCH_HAS_PREFETCHW\n\n#ifdef CONFIG_X86_32\n# define BASE_PREFETCH\t\t\"\"\n# define ARCH_HAS_PREFETCH\n#else\n# define BASE_PREFETCH\t\t\"prefetcht0 %P1\"\n#endif\n\n \nstatic inline void prefetch(const void *x)\n{\n\talternative_input(BASE_PREFETCH, \"prefetchnta %P1\",\n\t\t\t  X86_FEATURE_XMM,\n\t\t\t  \"m\" (*(const char *)x));\n}\n\n \nstatic __always_inline void prefetchw(const void *x)\n{\n\talternative_input(BASE_PREFETCH, \"prefetchw %P1\",\n\t\t\t  X86_FEATURE_3DNOWPREFETCH,\n\t\t\t  \"m\" (*(const char *)x));\n}\n\n#define TOP_OF_INIT_STACK ((unsigned long)&init_stack + sizeof(init_stack) - \\\n\t\t\t   TOP_OF_KERNEL_STACK_PADDING)\n\n#define task_top_of_stack(task) ((unsigned long)(task_pt_regs(task) + 1))\n\n#define task_pt_regs(task) \\\n({\t\t\t\t\t\t\t\t\t\\\n\tunsigned long __ptr = (unsigned long)task_stack_page(task);\t\\\n\t__ptr += THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING;\t\t\\\n\t((struct pt_regs *)__ptr) - 1;\t\t\t\t\t\\\n})\n\n#ifdef CONFIG_X86_32\n#define INIT_THREAD  {\t\t\t\t\t\t\t  \\\n\t.sp0\t\t\t= TOP_OF_INIT_STACK,\t\t\t  \\\n\t.sysenter_cs\t\t= __KERNEL_CS,\t\t\t\t  \\\n}\n\n#define KSTK_ESP(task)\t\t(task_pt_regs(task)->sp)\n\n#else\nextern unsigned long __end_init_task[];\n\n#define INIT_THREAD {\t\t\t\t\t\t\t    \\\n\t.sp\t= (unsigned long)&__end_init_task - sizeof(struct pt_regs), \\\n}\n\nextern unsigned long KSTK_ESP(struct task_struct *task);\n\n#endif  \n\nextern void start_thread(struct pt_regs *regs, unsigned long new_ip,\n\t\t\t\t\t       unsigned long new_sp);\n\n \n#define __TASK_UNMAPPED_BASE(task_size)\t(PAGE_ALIGN(task_size / 3))\n#define TASK_UNMAPPED_BASE\t\t__TASK_UNMAPPED_BASE(TASK_SIZE_LOW)\n\n#define KSTK_EIP(task)\t\t(task_pt_regs(task)->ip)\n\n \n#define GET_TSC_CTL(adr)\tget_tsc_mode((adr))\n#define SET_TSC_CTL(val)\tset_tsc_mode((val))\n\nextern int get_tsc_mode(unsigned long adr);\nextern int set_tsc_mode(unsigned int val);\n\nDECLARE_PER_CPU(u64, msr_misc_features_shadow);\n\nextern u16 get_llc_id(unsigned int cpu);\n\n#ifdef CONFIG_CPU_SUP_AMD\nextern u32 amd_get_nodes_per_socket(void);\nextern u32 amd_get_highest_perf(void);\nextern void amd_clear_divider(void);\nextern void amd_check_microcode(void);\n#else\nstatic inline u32 amd_get_nodes_per_socket(void)\t{ return 0; }\nstatic inline u32 amd_get_highest_perf(void)\t\t{ return 0; }\nstatic inline void amd_clear_divider(void)\t\t{ }\nstatic inline void amd_check_microcode(void)\t\t{ }\n#endif\n\nextern unsigned long arch_align_stack(unsigned long sp);\nvoid free_init_pages(const char *what, unsigned long begin, unsigned long end);\nextern void free_kernel_image_pages(const char *what, void *begin, void *end);\n\nvoid default_idle(void);\n#ifdef\tCONFIG_XEN\nbool xen_set_default_idle(void);\n#else\n#define xen_set_default_idle 0\n#endif\n\nvoid __noreturn stop_this_cpu(void *dummy);\nvoid microcode_check(struct cpuinfo_x86 *prev_info);\nvoid store_cpu_caps(struct cpuinfo_x86 *info);\n\nenum l1tf_mitigations {\n\tL1TF_MITIGATION_OFF,\n\tL1TF_MITIGATION_FLUSH_NOWARN,\n\tL1TF_MITIGATION_FLUSH,\n\tL1TF_MITIGATION_FLUSH_NOSMT,\n\tL1TF_MITIGATION_FULL,\n\tL1TF_MITIGATION_FULL_FORCE\n};\n\nextern enum l1tf_mitigations l1tf_mitigation;\n\nenum mds_mitigations {\n\tMDS_MITIGATION_OFF,\n\tMDS_MITIGATION_FULL,\n\tMDS_MITIGATION_VMWERV,\n};\n\n#ifdef CONFIG_X86_SGX\nint arch_memory_failure(unsigned long pfn, int flags);\n#define arch_memory_failure arch_memory_failure\n\nbool arch_is_platform_page(u64 paddr);\n#define arch_is_platform_page arch_is_platform_page\n#endif\n\nextern bool gds_ucode_mitigated(void);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}