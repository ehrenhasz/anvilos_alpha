{
  "module_name": "xor_avx.h",
  "hash_id": "dc9f9bc62872c54c763cb9c440507f37ce29973225e6cc369ed8960b25edc3eb",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/include/asm/xor_avx.h",
  "human_readable_source": " \n#ifndef _ASM_X86_XOR_AVX_H\n#define _ASM_X86_XOR_AVX_H\n\n \n\n#include <linux/compiler.h>\n#include <asm/fpu/api.h>\n\n#define BLOCK4(i) \\\n\t\tBLOCK(32 * i, 0) \\\n\t\tBLOCK(32 * (i + 1), 1) \\\n\t\tBLOCK(32 * (i + 2), 2) \\\n\t\tBLOCK(32 * (i + 3), 3)\n\n#define BLOCK16() \\\n\t\tBLOCK4(0) \\\n\t\tBLOCK4(4) \\\n\t\tBLOCK4(8) \\\n\t\tBLOCK4(12)\n\nstatic void xor_avx_2(unsigned long bytes, unsigned long * __restrict p0,\n\t\t      const unsigned long * __restrict p1)\n{\n\tunsigned long lines = bytes >> 9;\n\n\tkernel_fpu_begin();\n\n\twhile (lines--) {\n#undef BLOCK\n#define BLOCK(i, reg) \\\ndo { \\\n\tasm volatile(\"vmovdqa %0, %%ymm\" #reg : : \"m\" (p1[i / sizeof(*p1)])); \\\n\tasm volatile(\"vxorps %0, %%ymm\" #reg \", %%ymm\"  #reg : : \\\n\t\t\"m\" (p0[i / sizeof(*p0)])); \\\n\tasm volatile(\"vmovdqa %%ymm\" #reg \", %0\" : \\\n\t\t\"=m\" (p0[i / sizeof(*p0)])); \\\n} while (0);\n\n\t\tBLOCK16()\n\n\t\tp0 = (unsigned long *)((uintptr_t)p0 + 512);\n\t\tp1 = (unsigned long *)((uintptr_t)p1 + 512);\n\t}\n\n\tkernel_fpu_end();\n}\n\nstatic void xor_avx_3(unsigned long bytes, unsigned long * __restrict p0,\n\t\t      const unsigned long * __restrict p1,\n\t\t      const unsigned long * __restrict p2)\n{\n\tunsigned long lines = bytes >> 9;\n\n\tkernel_fpu_begin();\n\n\twhile (lines--) {\n#undef BLOCK\n#define BLOCK(i, reg) \\\ndo { \\\n\tasm volatile(\"vmovdqa %0, %%ymm\" #reg : : \"m\" (p2[i / sizeof(*p2)])); \\\n\tasm volatile(\"vxorps %0, %%ymm\" #reg \", %%ymm\" #reg : : \\\n\t\t\"m\" (p1[i / sizeof(*p1)])); \\\n\tasm volatile(\"vxorps %0, %%ymm\" #reg \", %%ymm\" #reg : : \\\n\t\t\"m\" (p0[i / sizeof(*p0)])); \\\n\tasm volatile(\"vmovdqa %%ymm\" #reg \", %0\" : \\\n\t\t\"=m\" (p0[i / sizeof(*p0)])); \\\n} while (0);\n\n\t\tBLOCK16()\n\n\t\tp0 = (unsigned long *)((uintptr_t)p0 + 512);\n\t\tp1 = (unsigned long *)((uintptr_t)p1 + 512);\n\t\tp2 = (unsigned long *)((uintptr_t)p2 + 512);\n\t}\n\n\tkernel_fpu_end();\n}\n\nstatic void xor_avx_4(unsigned long bytes, unsigned long * __restrict p0,\n\t\t      const unsigned long * __restrict p1,\n\t\t      const unsigned long * __restrict p2,\n\t\t      const unsigned long * __restrict p3)\n{\n\tunsigned long lines = bytes >> 9;\n\n\tkernel_fpu_begin();\n\n\twhile (lines--) {\n#undef BLOCK\n#define BLOCK(i, reg) \\\ndo { \\\n\tasm volatile(\"vmovdqa %0, %%ymm\" #reg : : \"m\" (p3[i / sizeof(*p3)])); \\\n\tasm volatile(\"vxorps %0, %%ymm\" #reg \", %%ymm\" #reg : : \\\n\t\t\"m\" (p2[i / sizeof(*p2)])); \\\n\tasm volatile(\"vxorps %0, %%ymm\" #reg \", %%ymm\" #reg : : \\\n\t\t\"m\" (p1[i / sizeof(*p1)])); \\\n\tasm volatile(\"vxorps %0, %%ymm\" #reg \", %%ymm\" #reg : : \\\n\t\t\"m\" (p0[i / sizeof(*p0)])); \\\n\tasm volatile(\"vmovdqa %%ymm\" #reg \", %0\" : \\\n\t\t\"=m\" (p0[i / sizeof(*p0)])); \\\n} while (0);\n\n\t\tBLOCK16();\n\n\t\tp0 = (unsigned long *)((uintptr_t)p0 + 512);\n\t\tp1 = (unsigned long *)((uintptr_t)p1 + 512);\n\t\tp2 = (unsigned long *)((uintptr_t)p2 + 512);\n\t\tp3 = (unsigned long *)((uintptr_t)p3 + 512);\n\t}\n\n\tkernel_fpu_end();\n}\n\nstatic void xor_avx_5(unsigned long bytes, unsigned long * __restrict p0,\n\t     const unsigned long * __restrict p1,\n\t     const unsigned long * __restrict p2,\n\t     const unsigned long * __restrict p3,\n\t     const unsigned long * __restrict p4)\n{\n\tunsigned long lines = bytes >> 9;\n\n\tkernel_fpu_begin();\n\n\twhile (lines--) {\n#undef BLOCK\n#define BLOCK(i, reg) \\\ndo { \\\n\tasm volatile(\"vmovdqa %0, %%ymm\" #reg : : \"m\" (p4[i / sizeof(*p4)])); \\\n\tasm volatile(\"vxorps %0, %%ymm\" #reg \", %%ymm\" #reg : : \\\n\t\t\"m\" (p3[i / sizeof(*p3)])); \\\n\tasm volatile(\"vxorps %0, %%ymm\" #reg \", %%ymm\" #reg : : \\\n\t\t\"m\" (p2[i / sizeof(*p2)])); \\\n\tasm volatile(\"vxorps %0, %%ymm\" #reg \", %%ymm\" #reg : : \\\n\t\t\"m\" (p1[i / sizeof(*p1)])); \\\n\tasm volatile(\"vxorps %0, %%ymm\" #reg \", %%ymm\" #reg : : \\\n\t\t\"m\" (p0[i / sizeof(*p0)])); \\\n\tasm volatile(\"vmovdqa %%ymm\" #reg \", %0\" : \\\n\t\t\"=m\" (p0[i / sizeof(*p0)])); \\\n} while (0);\n\n\t\tBLOCK16()\n\n\t\tp0 = (unsigned long *)((uintptr_t)p0 + 512);\n\t\tp1 = (unsigned long *)((uintptr_t)p1 + 512);\n\t\tp2 = (unsigned long *)((uintptr_t)p2 + 512);\n\t\tp3 = (unsigned long *)((uintptr_t)p3 + 512);\n\t\tp4 = (unsigned long *)((uintptr_t)p4 + 512);\n\t}\n\n\tkernel_fpu_end();\n}\n\nstatic struct xor_block_template xor_block_avx = {\n\t.name = \"avx\",\n\t.do_2 = xor_avx_2,\n\t.do_3 = xor_avx_3,\n\t.do_4 = xor_avx_4,\n\t.do_5 = xor_avx_5,\n};\n\n#define AVX_XOR_SPEED \\\ndo { \\\n\tif (boot_cpu_has(X86_FEATURE_AVX) && boot_cpu_has(X86_FEATURE_OSXSAVE)) \\\n\t\txor_speed(&xor_block_avx); \\\n} while (0)\n\n#define AVX_SELECT(FASTEST) \\\n\t(boot_cpu_has(X86_FEATURE_AVX) && boot_cpu_has(X86_FEATURE_OSXSAVE) ? &xor_block_avx : FASTEST)\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}