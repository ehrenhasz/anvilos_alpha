{
  "module_name": "mmu_context.h",
  "hash_id": "482ccda88783122f00a5c17e20703b21e7595bda0dc440705653fca06faef823",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/include/asm/mmu_context.h",
  "human_readable_source": " \n#ifndef _ASM_X86_MMU_CONTEXT_H\n#define _ASM_X86_MMU_CONTEXT_H\n\n#include <asm/desc.h>\n#include <linux/atomic.h>\n#include <linux/mm_types.h>\n#include <linux/pkeys.h>\n\n#include <trace/events/tlb.h>\n\n#include <asm/tlbflush.h>\n#include <asm/paravirt.h>\n#include <asm/debugreg.h>\n#include <asm/gsseg.h>\n\nextern atomic64_t last_mm_ctx_id;\n\n#ifdef CONFIG_PERF_EVENTS\nDECLARE_STATIC_KEY_FALSE(rdpmc_never_available_key);\nDECLARE_STATIC_KEY_FALSE(rdpmc_always_available_key);\nvoid cr4_update_pce(void *ignored);\n#endif\n\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\n \nstruct ldt_struct {\n\t \n\tstruct desc_struct\t*entries;\n\tunsigned int\t\tnr_entries;\n\n\t \n\tint\t\t\tslot;\n};\n\n \nstatic inline void init_new_context_ldt(struct mm_struct *mm)\n{\n\tmm->context.ldt = NULL;\n\tinit_rwsem(&mm->context.ldt_usr_sem);\n}\nint ldt_dup_context(struct mm_struct *oldmm, struct mm_struct *mm);\nvoid destroy_context_ldt(struct mm_struct *mm);\nvoid ldt_arch_exit_mmap(struct mm_struct *mm);\n#else\t \nstatic inline void init_new_context_ldt(struct mm_struct *mm) { }\nstatic inline int ldt_dup_context(struct mm_struct *oldmm,\n\t\t\t\t  struct mm_struct *mm)\n{\n\treturn 0;\n}\nstatic inline void destroy_context_ldt(struct mm_struct *mm) { }\nstatic inline void ldt_arch_exit_mmap(struct mm_struct *mm) { }\n#endif\n\n#ifdef CONFIG_MODIFY_LDT_SYSCALL\nextern void load_mm_ldt(struct mm_struct *mm);\nextern void switch_ldt(struct mm_struct *prev, struct mm_struct *next);\n#else\nstatic inline void load_mm_ldt(struct mm_struct *mm)\n{\n\tclear_LDT();\n}\nstatic inline void switch_ldt(struct mm_struct *prev, struct mm_struct *next)\n{\n\tDEBUG_LOCKS_WARN_ON(preemptible());\n}\n#endif\n\n#ifdef CONFIG_ADDRESS_MASKING\nstatic inline unsigned long mm_lam_cr3_mask(struct mm_struct *mm)\n{\n\treturn mm->context.lam_cr3_mask;\n}\n\nstatic inline void dup_lam(struct mm_struct *oldmm, struct mm_struct *mm)\n{\n\tmm->context.lam_cr3_mask = oldmm->context.lam_cr3_mask;\n\tmm->context.untag_mask = oldmm->context.untag_mask;\n}\n\n#define mm_untag_mask mm_untag_mask\nstatic inline unsigned long mm_untag_mask(struct mm_struct *mm)\n{\n\treturn mm->context.untag_mask;\n}\n\nstatic inline void mm_reset_untag_mask(struct mm_struct *mm)\n{\n\tmm->context.untag_mask = -1UL;\n}\n\n#define arch_pgtable_dma_compat arch_pgtable_dma_compat\nstatic inline bool arch_pgtable_dma_compat(struct mm_struct *mm)\n{\n\treturn !mm_lam_cr3_mask(mm) ||\n\t\ttest_bit(MM_CONTEXT_FORCE_TAGGED_SVA, &mm->context.flags);\n}\n#else\n\nstatic inline unsigned long mm_lam_cr3_mask(struct mm_struct *mm)\n{\n\treturn 0;\n}\n\nstatic inline void dup_lam(struct mm_struct *oldmm, struct mm_struct *mm)\n{\n}\n\nstatic inline void mm_reset_untag_mask(struct mm_struct *mm)\n{\n}\n#endif\n\n#define enter_lazy_tlb enter_lazy_tlb\nextern void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk);\n\n \n#define init_new_context init_new_context\nstatic inline int init_new_context(struct task_struct *tsk,\n\t\t\t\t   struct mm_struct *mm)\n{\n\tmutex_init(&mm->context.lock);\n\n\tmm->context.ctx_id = atomic64_inc_return(&last_mm_ctx_id);\n\tatomic64_set(&mm->context.tlb_gen, 0);\n\n#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS\n\tif (cpu_feature_enabled(X86_FEATURE_OSPKE)) {\n\t\t \n\t\tmm->context.pkey_allocation_map = 0x1;\n\t\t \n\t\tmm->context.execute_only_pkey = -1;\n\t}\n#endif\n\tmm_reset_untag_mask(mm);\n\tinit_new_context_ldt(mm);\n\treturn 0;\n}\n\n#define destroy_context destroy_context\nstatic inline void destroy_context(struct mm_struct *mm)\n{\n\tdestroy_context_ldt(mm);\n}\n\nextern void switch_mm(struct mm_struct *prev, struct mm_struct *next,\n\t\t      struct task_struct *tsk);\n\nextern void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,\n\t\t\t       struct task_struct *tsk);\n#define switch_mm_irqs_off switch_mm_irqs_off\n\n#define activate_mm(prev, next)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tparavirt_enter_mmap(next);\t\t\\\n\tswitch_mm((prev), (next), NULL);\t\\\n} while (0);\n\n#ifdef CONFIG_X86_32\n#define deactivate_mm(tsk, mm)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tloadsegment(gs, 0);\t\t\t\\\n} while (0)\n#else\n#define deactivate_mm(tsk, mm)\t\t\t\\\ndo {\t\t\t\t\t\t\\\n\tshstk_free(tsk);\t\t\t\\\n\tload_gs_index(0);\t\t\t\\\n\tloadsegment(fs, 0);\t\t\t\\\n} while (0)\n#endif\n\nstatic inline void arch_dup_pkeys(struct mm_struct *oldmm,\n\t\t\t\t  struct mm_struct *mm)\n{\n#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS\n\tif (!cpu_feature_enabled(X86_FEATURE_OSPKE))\n\t\treturn;\n\n\t \n\tmm->context.pkey_allocation_map = oldmm->context.pkey_allocation_map;\n\tmm->context.execute_only_pkey   = oldmm->context.execute_only_pkey;\n#endif\n}\n\nstatic inline int arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)\n{\n\tarch_dup_pkeys(oldmm, mm);\n\tparavirt_enter_mmap(mm);\n\tdup_lam(oldmm, mm);\n\treturn ldt_dup_context(oldmm, mm);\n}\n\nstatic inline void arch_exit_mmap(struct mm_struct *mm)\n{\n\tparavirt_arch_exit_mmap(mm);\n\tldt_arch_exit_mmap(mm);\n}\n\n#ifdef CONFIG_X86_64\nstatic inline bool is_64bit_mm(struct mm_struct *mm)\n{\n\treturn\t!IS_ENABLED(CONFIG_IA32_EMULATION) ||\n\t\t!test_bit(MM_CONTEXT_UPROBE_IA32, &mm->context.flags);\n}\n#else\nstatic inline bool is_64bit_mm(struct mm_struct *mm)\n{\n\treturn false;\n}\n#endif\n\nstatic inline void arch_unmap(struct mm_struct *mm, unsigned long start,\n\t\t\t      unsigned long end)\n{\n}\n\n \nstatic inline bool arch_vma_access_permitted(struct vm_area_struct *vma,\n\t\tbool write, bool execute, bool foreign)\n{\n\t \n\tif (execute)\n\t\treturn true;\n\t \n\tif (foreign || vma_is_foreign(vma))\n\t\treturn true;\n\treturn __pkru_allows_pkey(vma_pkey(vma), write);\n}\n\nunsigned long __get_current_cr3_fast(void);\n\n#include <asm-generic/mmu_context.h>\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}