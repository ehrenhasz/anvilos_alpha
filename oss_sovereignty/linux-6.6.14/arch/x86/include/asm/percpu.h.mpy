{
  "module_name": "percpu.h",
  "hash_id": "f79dea52b64e0f8efffc6091b149986ee077f4869b99b7b546d53e4e77f9012d",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/include/asm/percpu.h",
  "human_readable_source": " \n#ifndef _ASM_X86_PERCPU_H\n#define _ASM_X86_PERCPU_H\n\n#ifdef CONFIG_X86_64\n#define __percpu_seg\t\tgs\n#else\n#define __percpu_seg\t\tfs\n#endif\n\n#ifdef __ASSEMBLY__\n\n#ifdef CONFIG_SMP\n#define PER_CPU_VAR(var)\t%__percpu_seg:var\n#else  \n#define PER_CPU_VAR(var)\tvar\n#endif\t \n\n#ifdef CONFIG_X86_64_SMP\n#define INIT_PER_CPU_VAR(var)  init_per_cpu__##var\n#else\n#define INIT_PER_CPU_VAR(var)  var\n#endif\n\n#else  \n\n#include <linux/kernel.h>\n#include <linux/stringify.h>\n\n#ifdef CONFIG_SMP\n#define __percpu_prefix\t\t\"%%\"__stringify(__percpu_seg)\":\"\n#define __my_cpu_offset\t\tthis_cpu_read(this_cpu_off)\n\n \n#define arch_raw_cpu_ptr(ptr)\t\t\t\t\\\n({\t\t\t\t\t\t\t\\\n\tunsigned long tcp_ptr__;\t\t\t\\\n\tasm (\"add \" __percpu_arg(1) \", %0\"\t\t\\\n\t     : \"=r\" (tcp_ptr__)\t\t\t\t\\\n\t     : \"m\" (this_cpu_off), \"0\" (ptr));\t\t\\\n\t(typeof(*(ptr)) __kernel __force *)tcp_ptr__;\t\\\n})\n#else\n#define __percpu_prefix\t\t\"\"\n#endif\n\n#define __percpu_arg(x)\t\t__percpu_prefix \"%\" #x\n\n \n#define DECLARE_INIT_PER_CPU(var) \\\n       extern typeof(var) init_per_cpu_var(var)\n\n#ifdef CONFIG_X86_64_SMP\n#define init_per_cpu_var(var)  init_per_cpu__##var\n#else\n#define init_per_cpu_var(var)  var\n#endif\n\n \n\n#define __pcpu_type_1 u8\n#define __pcpu_type_2 u16\n#define __pcpu_type_4 u32\n#define __pcpu_type_8 u64\n\n#define __pcpu_cast_1(val) ((u8)(((unsigned long) val) & 0xff))\n#define __pcpu_cast_2(val) ((u16)(((unsigned long) val) & 0xffff))\n#define __pcpu_cast_4(val) ((u32)(((unsigned long) val) & 0xffffffff))\n#define __pcpu_cast_8(val) ((u64)(val))\n\n#define __pcpu_op1_1(op, dst) op \"b \" dst\n#define __pcpu_op1_2(op, dst) op \"w \" dst\n#define __pcpu_op1_4(op, dst) op \"l \" dst\n#define __pcpu_op1_8(op, dst) op \"q \" dst\n\n#define __pcpu_op2_1(op, src, dst) op \"b \" src \", \" dst\n#define __pcpu_op2_2(op, src, dst) op \"w \" src \", \" dst\n#define __pcpu_op2_4(op, src, dst) op \"l \" src \", \" dst\n#define __pcpu_op2_8(op, src, dst) op \"q \" src \", \" dst\n\n#define __pcpu_reg_1(mod, x) mod \"q\" (x)\n#define __pcpu_reg_2(mod, x) mod \"r\" (x)\n#define __pcpu_reg_4(mod, x) mod \"r\" (x)\n#define __pcpu_reg_8(mod, x) mod \"r\" (x)\n\n#define __pcpu_reg_imm_1(x) \"qi\" (x)\n#define __pcpu_reg_imm_2(x) \"ri\" (x)\n#define __pcpu_reg_imm_4(x) \"ri\" (x)\n#define __pcpu_reg_imm_8(x) \"re\" (x)\n\n#define percpu_to_op(size, qual, op, _var, _val)\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\t__pcpu_type_##size pto_val__ = __pcpu_cast_##size(_val);\t\\\n\tif (0) {\t\t                                        \\\n\t\ttypeof(_var) pto_tmp__;\t\t\t\t\t\\\n\t\tpto_tmp__ = (_val);\t\t\t\t\t\\\n\t\t(void)pto_tmp__;\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tasm qual(__pcpu_op2_##size(op, \"%[val]\", __percpu_arg([var]))\t\\\n\t    : [var] \"+m\" (_var)\t\t\t\t\t\t\\\n\t    : [val] __pcpu_reg_imm_##size(pto_val__));\t\t\t\\\n} while (0)\n\n#define percpu_unary_op(size, qual, op, _var)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tasm qual (__pcpu_op1_##size(op, __percpu_arg([var]))\t\t\\\n\t    : [var] \"+m\" (_var));\t\t\t\t\t\\\n})\n\n \n#define percpu_add_op(size, qual, var, val)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tconst int pao_ID__ = (__builtin_constant_p(val) &&\t\t\\\n\t\t\t      ((val) == 1 || (val) == -1)) ?\t\t\\\n\t\t\t\t(int)(val) : 0;\t\t\t\t\\\n\tif (0) {\t\t\t\t\t\t\t\\\n\t\ttypeof(var) pao_tmp__;\t\t\t\t\t\\\n\t\tpao_tmp__ = (val);\t\t\t\t\t\\\n\t\t(void)pao_tmp__;\t\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\\\n\tif (pao_ID__ == 1)\t\t\t\t\t\t\\\n\t\tpercpu_unary_op(size, qual, \"inc\", var);\t\t\\\n\telse if (pao_ID__ == -1)\t\t\t\t\t\\\n\t\tpercpu_unary_op(size, qual, \"dec\", var);\t\t\\\n\telse\t\t\t\t\t\t\t\t\\\n\t\tpercpu_to_op(size, qual, \"add\", var, val);\t\t\\\n} while (0)\n\n#define percpu_from_op(size, qual, op, _var)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__pcpu_type_##size pfo_val__;\t\t\t\t\t\\\n\tasm qual (__pcpu_op2_##size(op, __percpu_arg([var]), \"%[val]\")\t\\\n\t    : [val] __pcpu_reg_##size(\"=\", pfo_val__)\t\t\t\\\n\t    : [var] \"m\" (_var));\t\t\t\t\t\\\n\t(typeof(_var))(unsigned long) pfo_val__;\t\t\t\\\n})\n\n#define percpu_stable_op(size, op, _var)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__pcpu_type_##size pfo_val__;\t\t\t\t\t\\\n\tasm(__pcpu_op2_##size(op, __percpu_arg(P[var]), \"%[val]\")\t\\\n\t    : [val] __pcpu_reg_##size(\"=\", pfo_val__)\t\t\t\\\n\t    : [var] \"p\" (&(_var)));\t\t\t\t\t\\\n\t(typeof(_var))(unsigned long) pfo_val__;\t\t\t\\\n})\n\n \n#define percpu_add_return_op(size, qual, _var, _val)\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__pcpu_type_##size paro_tmp__ = __pcpu_cast_##size(_val);\t\\\n\tasm qual (__pcpu_op2_##size(\"xadd\", \"%[tmp]\",\t\t\t\\\n\t\t\t\t     __percpu_arg([var]))\t\t\\\n\t\t  : [tmp] __pcpu_reg_##size(\"+\", paro_tmp__),\t\t\\\n\t\t    [var] \"+m\" (_var)\t\t\t\t\t\\\n\t\t  : : \"memory\");\t\t\t\t\t\\\n\t(typeof(_var))(unsigned long) (paro_tmp__ + _val);\t\t\\\n})\n\n \n#define percpu_xchg_op(size, qual, _var, _nval)\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__pcpu_type_##size pxo_old__;\t\t\t\t\t\\\n\t__pcpu_type_##size pxo_new__ = __pcpu_cast_##size(_nval);\t\\\n\tasm qual (__pcpu_op2_##size(\"mov\", __percpu_arg([var]),\t\t\\\n\t\t\t\t    \"%[oval]\")\t\t\t\t\\\n\t\t  \"\\n1:\\t\"\t\t\t\t\t\t\\\n\t\t  __pcpu_op2_##size(\"cmpxchg\", \"%[nval]\",\t\t\\\n\t\t\t\t    __percpu_arg([var]))\t\t\\\n\t\t  \"\\n\\tjnz 1b\"\t\t\t\t\t\t\\\n\t\t  : [oval] \"=&a\" (pxo_old__),\t\t\t\t\\\n\t\t    [var] \"+m\" (_var)\t\t\t\t\t\\\n\t\t  : [nval] __pcpu_reg_##size(, pxo_new__)\t\t\\\n\t\t  : \"memory\");\t\t\t\t\t\t\\\n\t(typeof(_var))(unsigned long) pxo_old__;\t\t\t\\\n})\n\n \n#define percpu_cmpxchg_op(size, qual, _var, _oval, _nval)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\t__pcpu_type_##size pco_old__ = __pcpu_cast_##size(_oval);\t\\\n\t__pcpu_type_##size pco_new__ = __pcpu_cast_##size(_nval);\t\\\n\tasm qual (__pcpu_op2_##size(\"cmpxchg\", \"%[nval]\",\t\t\\\n\t\t\t\t    __percpu_arg([var]))\t\t\\\n\t\t  : [oval] \"+a\" (pco_old__),\t\t\t\t\\\n\t\t    [var] \"+m\" (_var)\t\t\t\t\t\\\n\t\t  : [nval] __pcpu_reg_##size(, pco_new__)\t\t\\\n\t\t  : \"memory\");\t\t\t\t\t\t\\\n\t(typeof(_var))(unsigned long) pco_old__;\t\t\t\\\n})\n\n#if defined(CONFIG_X86_32) && !defined(CONFIG_UML)\n#define percpu_cmpxchg64_op(size, qual, _var, _oval, _nval)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tunion {\t\t\t\t\t\t\t\t\\\n\t\tu64 var;\t\t\t\t\t\t\\\n\t\tstruct {\t\t\t\t\t\t\\\n\t\t\tu32 low, high;\t\t\t\t\t\\\n\t\t};\t\t\t\t\t\t\t\\\n\t} old__, new__;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\told__.var = _oval;\t\t\t\t\t\t\\\n\tnew__.var = _nval;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tasm qual (ALTERNATIVE(\"leal %P[var], %%esi; call this_cpu_cmpxchg8b_emu\", \\\n\t\t\t      \"cmpxchg8b \" __percpu_arg([var]), X86_FEATURE_CX8) \\\n\t\t  : [var] \"+m\" (_var),\t\t\t\t\t\\\n\t\t    \"+a\" (old__.low),\t\t\t\t\t\\\n\t\t    \"+d\" (old__.high)\t\t\t\t\t\\\n\t\t  : \"b\" (new__.low),\t\t\t\t\t\\\n\t\t    \"c\" (new__.high)\t\t\t\t\t\\\n\t\t  : \"memory\", \"esi\");\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\told__.var;\t\t\t\t\t\t\t\\\n})\n\n#define raw_cpu_cmpxchg64(pcp, oval, nval)\tpercpu_cmpxchg64_op(8,         , pcp, oval, nval)\n#define this_cpu_cmpxchg64(pcp, oval, nval)\tpercpu_cmpxchg64_op(8, volatile, pcp, oval, nval)\n#endif\n\n#ifdef CONFIG_X86_64\n#define raw_cpu_cmpxchg64(pcp, oval, nval)\tpercpu_cmpxchg_op(8,         , pcp, oval, nval);\n#define this_cpu_cmpxchg64(pcp, oval, nval)\tpercpu_cmpxchg_op(8, volatile, pcp, oval, nval);\n\n#define percpu_cmpxchg128_op(size, qual, _var, _oval, _nval)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tunion {\t\t\t\t\t\t\t\t\\\n\t\tu128 var;\t\t\t\t\t\t\\\n\t\tstruct {\t\t\t\t\t\t\\\n\t\t\tu64 low, high;\t\t\t\t\t\\\n\t\t};\t\t\t\t\t\t\t\\\n\t} old__, new__;\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\told__.var = _oval;\t\t\t\t\t\t\\\n\tnew__.var = _nval;\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tasm qual (ALTERNATIVE(\"leaq %P[var], %%rsi; call this_cpu_cmpxchg16b_emu\", \\\n\t\t\t      \"cmpxchg16b \" __percpu_arg([var]), X86_FEATURE_CX16) \\\n\t\t  : [var] \"+m\" (_var),\t\t\t\t\t\\\n\t\t    \"+a\" (old__.low),\t\t\t\t\t\\\n\t\t    \"+d\" (old__.high)\t\t\t\t\t\\\n\t\t  : \"b\" (new__.low),\t\t\t\t\t\\\n\t\t    \"c\" (new__.high)\t\t\t\t\t\\\n\t\t  : \"memory\", \"rsi\");\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\told__.var;\t\t\t\t\t\t\t\\\n})\n\n#define raw_cpu_cmpxchg128(pcp, oval, nval)\tpercpu_cmpxchg128_op(16,         , pcp, oval, nval)\n#define this_cpu_cmpxchg128(pcp, oval, nval)\tpercpu_cmpxchg128_op(16, volatile, pcp, oval, nval)\n#endif\n\n \n#define this_cpu_read_stable_1(pcp)\tpercpu_stable_op(1, \"mov\", pcp)\n#define this_cpu_read_stable_2(pcp)\tpercpu_stable_op(2, \"mov\", pcp)\n#define this_cpu_read_stable_4(pcp)\tpercpu_stable_op(4, \"mov\", pcp)\n#define this_cpu_read_stable_8(pcp)\tpercpu_stable_op(8, \"mov\", pcp)\n#define this_cpu_read_stable(pcp)\t__pcpu_size_call_return(this_cpu_read_stable_, pcp)\n\n#define raw_cpu_read_1(pcp)\t\tpercpu_from_op(1, , \"mov\", pcp)\n#define raw_cpu_read_2(pcp)\t\tpercpu_from_op(2, , \"mov\", pcp)\n#define raw_cpu_read_4(pcp)\t\tpercpu_from_op(4, , \"mov\", pcp)\n\n#define raw_cpu_write_1(pcp, val)\tpercpu_to_op(1, , \"mov\", (pcp), val)\n#define raw_cpu_write_2(pcp, val)\tpercpu_to_op(2, , \"mov\", (pcp), val)\n#define raw_cpu_write_4(pcp, val)\tpercpu_to_op(4, , \"mov\", (pcp), val)\n#define raw_cpu_add_1(pcp, val)\t\tpercpu_add_op(1, , (pcp), val)\n#define raw_cpu_add_2(pcp, val)\t\tpercpu_add_op(2, , (pcp), val)\n#define raw_cpu_add_4(pcp, val)\t\tpercpu_add_op(4, , (pcp), val)\n#define raw_cpu_and_1(pcp, val)\t\tpercpu_to_op(1, , \"and\", (pcp), val)\n#define raw_cpu_and_2(pcp, val)\t\tpercpu_to_op(2, , \"and\", (pcp), val)\n#define raw_cpu_and_4(pcp, val)\t\tpercpu_to_op(4, , \"and\", (pcp), val)\n#define raw_cpu_or_1(pcp, val)\t\tpercpu_to_op(1, , \"or\", (pcp), val)\n#define raw_cpu_or_2(pcp, val)\t\tpercpu_to_op(2, , \"or\", (pcp), val)\n#define raw_cpu_or_4(pcp, val)\t\tpercpu_to_op(4, , \"or\", (pcp), val)\n\n \n#define raw_percpu_xchg_op(var, nval)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\ttypeof(var) pxo_ret__ = raw_cpu_read(var);\t\t\t\\\n\traw_cpu_write(var, (nval));\t\t\t\t\t\\\n\tpxo_ret__;\t\t\t\t\t\t\t\\\n})\n\n#define raw_cpu_xchg_1(pcp, val)\traw_percpu_xchg_op(pcp, val)\n#define raw_cpu_xchg_2(pcp, val)\traw_percpu_xchg_op(pcp, val)\n#define raw_cpu_xchg_4(pcp, val)\traw_percpu_xchg_op(pcp, val)\n\n#define this_cpu_read_1(pcp)\t\tpercpu_from_op(1, volatile, \"mov\", pcp)\n#define this_cpu_read_2(pcp)\t\tpercpu_from_op(2, volatile, \"mov\", pcp)\n#define this_cpu_read_4(pcp)\t\tpercpu_from_op(4, volatile, \"mov\", pcp)\n#define this_cpu_write_1(pcp, val)\tpercpu_to_op(1, volatile, \"mov\", (pcp), val)\n#define this_cpu_write_2(pcp, val)\tpercpu_to_op(2, volatile, \"mov\", (pcp), val)\n#define this_cpu_write_4(pcp, val)\tpercpu_to_op(4, volatile, \"mov\", (pcp), val)\n#define this_cpu_add_1(pcp, val)\tpercpu_add_op(1, volatile, (pcp), val)\n#define this_cpu_add_2(pcp, val)\tpercpu_add_op(2, volatile, (pcp), val)\n#define this_cpu_add_4(pcp, val)\tpercpu_add_op(4, volatile, (pcp), val)\n#define this_cpu_and_1(pcp, val)\tpercpu_to_op(1, volatile, \"and\", (pcp), val)\n#define this_cpu_and_2(pcp, val)\tpercpu_to_op(2, volatile, \"and\", (pcp), val)\n#define this_cpu_and_4(pcp, val)\tpercpu_to_op(4, volatile, \"and\", (pcp), val)\n#define this_cpu_or_1(pcp, val)\t\tpercpu_to_op(1, volatile, \"or\", (pcp), val)\n#define this_cpu_or_2(pcp, val)\t\tpercpu_to_op(2, volatile, \"or\", (pcp), val)\n#define this_cpu_or_4(pcp, val)\t\tpercpu_to_op(4, volatile, \"or\", (pcp), val)\n#define this_cpu_xchg_1(pcp, nval)\tpercpu_xchg_op(1, volatile, pcp, nval)\n#define this_cpu_xchg_2(pcp, nval)\tpercpu_xchg_op(2, volatile, pcp, nval)\n#define this_cpu_xchg_4(pcp, nval)\tpercpu_xchg_op(4, volatile, pcp, nval)\n\n#define raw_cpu_add_return_1(pcp, val)\t\tpercpu_add_return_op(1, , pcp, val)\n#define raw_cpu_add_return_2(pcp, val)\t\tpercpu_add_return_op(2, , pcp, val)\n#define raw_cpu_add_return_4(pcp, val)\t\tpercpu_add_return_op(4, , pcp, val)\n#define raw_cpu_cmpxchg_1(pcp, oval, nval)\tpercpu_cmpxchg_op(1, , pcp, oval, nval)\n#define raw_cpu_cmpxchg_2(pcp, oval, nval)\tpercpu_cmpxchg_op(2, , pcp, oval, nval)\n#define raw_cpu_cmpxchg_4(pcp, oval, nval)\tpercpu_cmpxchg_op(4, , pcp, oval, nval)\n\n#define this_cpu_add_return_1(pcp, val)\t\tpercpu_add_return_op(1, volatile, pcp, val)\n#define this_cpu_add_return_2(pcp, val)\t\tpercpu_add_return_op(2, volatile, pcp, val)\n#define this_cpu_add_return_4(pcp, val)\t\tpercpu_add_return_op(4, volatile, pcp, val)\n#define this_cpu_cmpxchg_1(pcp, oval, nval)\tpercpu_cmpxchg_op(1, volatile, pcp, oval, nval)\n#define this_cpu_cmpxchg_2(pcp, oval, nval)\tpercpu_cmpxchg_op(2, volatile, pcp, oval, nval)\n#define this_cpu_cmpxchg_4(pcp, oval, nval)\tpercpu_cmpxchg_op(4, volatile, pcp, oval, nval)\n\n \n#ifdef CONFIG_X86_64\n#define raw_cpu_read_8(pcp)\t\t\tpercpu_from_op(8, , \"mov\", pcp)\n#define raw_cpu_write_8(pcp, val)\t\tpercpu_to_op(8, , \"mov\", (pcp), val)\n#define raw_cpu_add_8(pcp, val)\t\t\tpercpu_add_op(8, , (pcp), val)\n#define raw_cpu_and_8(pcp, val)\t\t\tpercpu_to_op(8, , \"and\", (pcp), val)\n#define raw_cpu_or_8(pcp, val)\t\t\tpercpu_to_op(8, , \"or\", (pcp), val)\n#define raw_cpu_add_return_8(pcp, val)\t\tpercpu_add_return_op(8, , pcp, val)\n#define raw_cpu_xchg_8(pcp, nval)\t\traw_percpu_xchg_op(pcp, nval)\n#define raw_cpu_cmpxchg_8(pcp, oval, nval)\tpercpu_cmpxchg_op(8, , pcp, oval, nval)\n\n#define this_cpu_read_8(pcp)\t\t\tpercpu_from_op(8, volatile, \"mov\", pcp)\n#define this_cpu_write_8(pcp, val)\t\tpercpu_to_op(8, volatile, \"mov\", (pcp), val)\n#define this_cpu_add_8(pcp, val)\t\tpercpu_add_op(8, volatile, (pcp), val)\n#define this_cpu_and_8(pcp, val)\t\tpercpu_to_op(8, volatile, \"and\", (pcp), val)\n#define this_cpu_or_8(pcp, val)\t\t\tpercpu_to_op(8, volatile, \"or\", (pcp), val)\n#define this_cpu_add_return_8(pcp, val)\t\tpercpu_add_return_op(8, volatile, pcp, val)\n#define this_cpu_xchg_8(pcp, nval)\t\tpercpu_xchg_op(8, volatile, pcp, nval)\n#define this_cpu_cmpxchg_8(pcp, oval, nval)\tpercpu_cmpxchg_op(8, volatile, pcp, oval, nval)\n#endif\n\nstatic __always_inline bool x86_this_cpu_constant_test_bit(unsigned int nr,\n                        const unsigned long __percpu *addr)\n{\n\tunsigned long __percpu *a =\n\t\t(unsigned long __percpu *)addr + nr / BITS_PER_LONG;\n\n#ifdef CONFIG_X86_64\n\treturn ((1UL << (nr % BITS_PER_LONG)) & raw_cpu_read_8(*a)) != 0;\n#else\n\treturn ((1UL << (nr % BITS_PER_LONG)) & raw_cpu_read_4(*a)) != 0;\n#endif\n}\n\nstatic inline bool x86_this_cpu_variable_test_bit(int nr,\n                        const unsigned long __percpu *addr)\n{\n\tbool oldbit;\n\n\tasm volatile(\"btl \"__percpu_arg(2)\",%1\"\n\t\t\tCC_SET(c)\n\t\t\t: CC_OUT(c) (oldbit)\n\t\t\t: \"m\" (*(unsigned long __percpu *)addr), \"Ir\" (nr));\n\n\treturn oldbit;\n}\n\n#define x86_this_cpu_test_bit(nr, addr)\t\t\t\\\n\t(__builtin_constant_p((nr))\t\t\t\\\n\t ? x86_this_cpu_constant_test_bit((nr), (addr))\t\\\n\t : x86_this_cpu_variable_test_bit((nr), (addr)))\n\n\n#include <asm-generic/percpu.h>\n\n \nDECLARE_PER_CPU_READ_MOSTLY(unsigned long, this_cpu_off);\n\n#endif  \n\n#ifdef CONFIG_SMP\n\n \n\n#define\tDEFINE_EARLY_PER_CPU(_type, _name, _initvalue)\t\t\t\\\n\tDEFINE_PER_CPU(_type, _name) = _initvalue;\t\t\t\\\n\t__typeof__(_type) _name##_early_map[NR_CPUS] __initdata =\t\\\n\t\t\t\t{ [0 ... NR_CPUS-1] = _initvalue };\t\\\n\t__typeof__(_type) *_name##_early_ptr __refdata = _name##_early_map\n\n#define DEFINE_EARLY_PER_CPU_READ_MOSTLY(_type, _name, _initvalue)\t\\\n\tDEFINE_PER_CPU_READ_MOSTLY(_type, _name) = _initvalue;\t\t\\\n\t__typeof__(_type) _name##_early_map[NR_CPUS] __initdata =\t\\\n\t\t\t\t{ [0 ... NR_CPUS-1] = _initvalue };\t\\\n\t__typeof__(_type) *_name##_early_ptr __refdata = _name##_early_map\n\n#define EXPORT_EARLY_PER_CPU_SYMBOL(_name)\t\t\t\\\n\tEXPORT_PER_CPU_SYMBOL(_name)\n\n#define DECLARE_EARLY_PER_CPU(_type, _name)\t\t\t\\\n\tDECLARE_PER_CPU(_type, _name);\t\t\t\t\\\n\textern __typeof__(_type) *_name##_early_ptr;\t\t\\\n\textern __typeof__(_type)  _name##_early_map[]\n\n#define DECLARE_EARLY_PER_CPU_READ_MOSTLY(_type, _name)\t\t\\\n\tDECLARE_PER_CPU_READ_MOSTLY(_type, _name);\t\t\\\n\textern __typeof__(_type) *_name##_early_ptr;\t\t\\\n\textern __typeof__(_type)  _name##_early_map[]\n\n#define\tearly_per_cpu_ptr(_name) (_name##_early_ptr)\n#define\tearly_per_cpu_map(_name, _idx) (_name##_early_map[_idx])\n#define\tearly_per_cpu(_name, _cpu) \t\t\t\t\\\n\t*(early_per_cpu_ptr(_name) ?\t\t\t\t\\\n\t\t&early_per_cpu_ptr(_name)[_cpu] :\t\t\\\n\t\t&per_cpu(_name, _cpu))\n\n#else\t \n#define\tDEFINE_EARLY_PER_CPU(_type, _name, _initvalue)\t\t\\\n\tDEFINE_PER_CPU(_type, _name) = _initvalue\n\n#define DEFINE_EARLY_PER_CPU_READ_MOSTLY(_type, _name, _initvalue)\t\\\n\tDEFINE_PER_CPU_READ_MOSTLY(_type, _name) = _initvalue\n\n#define EXPORT_EARLY_PER_CPU_SYMBOL(_name)\t\t\t\\\n\tEXPORT_PER_CPU_SYMBOL(_name)\n\n#define DECLARE_EARLY_PER_CPU(_type, _name)\t\t\t\\\n\tDECLARE_PER_CPU(_type, _name)\n\n#define DECLARE_EARLY_PER_CPU_READ_MOSTLY(_type, _name)\t\t\\\n\tDECLARE_PER_CPU_READ_MOSTLY(_type, _name)\n\n#define\tearly_per_cpu(_name, _cpu) per_cpu(_name, _cpu)\n#define\tearly_per_cpu_ptr(_name) NULL\n \n\n#endif\t \n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}