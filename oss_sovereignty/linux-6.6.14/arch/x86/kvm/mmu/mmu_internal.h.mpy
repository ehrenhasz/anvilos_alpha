{
  "module_name": "mmu_internal.h",
  "hash_id": "981fb3ad0a44669015d6fb7305cbcd52d04b43161331cd7f7a0f771bda4c4795",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/mmu/mmu_internal.h",
  "human_readable_source": " \n#ifndef __KVM_X86_MMU_INTERNAL_H\n#define __KVM_X86_MMU_INTERNAL_H\n\n#include <linux/types.h>\n#include <linux/kvm_host.h>\n#include <asm/kvm_host.h>\n\n#ifdef CONFIG_KVM_PROVE_MMU\n#define KVM_MMU_WARN_ON(x) WARN_ON_ONCE(x)\n#else\n#define KVM_MMU_WARN_ON(x) BUILD_BUG_ON_INVALID(x)\n#endif\n\n \n#define __PT_LEVEL_SHIFT(level, bits_per_level)\t\\\n\t(PAGE_SHIFT + ((level) - 1) * (bits_per_level))\n#define __PT_INDEX(address, level, bits_per_level) \\\n\t(((address) >> __PT_LEVEL_SHIFT(level, bits_per_level)) & ((1 << (bits_per_level)) - 1))\n\n#define __PT_LVL_ADDR_MASK(base_addr_mask, level, bits_per_level) \\\n\t((base_addr_mask) & ~((1ULL << (PAGE_SHIFT + (((level) - 1) * (bits_per_level)))) - 1))\n\n#define __PT_LVL_OFFSET_MASK(base_addr_mask, level, bits_per_level) \\\n\t((base_addr_mask) & ((1ULL << (PAGE_SHIFT + (((level) - 1) * (bits_per_level)))) - 1))\n\n#define __PT_ENT_PER_PAGE(bits_per_level)  (1 << (bits_per_level))\n\n \n#define INVALID_PAE_ROOT\t0\n#define IS_VALID_PAE_ROOT(x)\t(!!(x))\n\nstatic inline hpa_t kvm_mmu_get_dummy_root(void)\n{\n\treturn my_zero_pfn(0) << PAGE_SHIFT;\n}\n\nstatic inline bool kvm_mmu_is_dummy_root(hpa_t shadow_page)\n{\n\treturn is_zero_pfn(shadow_page >> PAGE_SHIFT);\n}\n\ntypedef u64 __rcu *tdp_ptep_t;\n\nstruct kvm_mmu_page {\n\t \n\tstruct list_head link;\n\tstruct hlist_node hash_link;\n\n\tbool tdp_mmu_page;\n\tbool unsync;\n\tunion {\n\t\tu8 mmu_valid_gen;\n\n\t\t \n\t\tbool tdp_mmu_scheduled_root_to_zap;\n\t};\n\n\t  \n\tbool nx_huge_page_disallowed;\n\n\t \n\tunion kvm_mmu_page_role role;\n\tgfn_t gfn;\n\n\tu64 *spt;\n\n\t \n\tu64 *shadowed_translation;\n\n\t \n\tunion {\n\t\tint root_count;\n\t\trefcount_t tdp_mmu_root_count;\n\t};\n\tunsigned int unsync_children;\n\tunion {\n\t\tstruct kvm_rmap_head parent_ptes;  \n\t\ttdp_ptep_t ptep;\n\t};\n\tDECLARE_BITMAP(unsync_child_bitmap, 512);\n\n\t \n\tstruct list_head possible_nx_huge_page_link;\n#ifdef CONFIG_X86_32\n\t \n\tint clear_spte_count;\n#endif\n\n\t \n\tatomic_t write_flooding_count;\n\n#ifdef CONFIG_X86_64\n\t \n\tstruct rcu_head rcu_head;\n#endif\n};\n\nextern struct kmem_cache *mmu_page_header_cache;\n\nstatic inline int kvm_mmu_role_as_id(union kvm_mmu_page_role role)\n{\n\treturn role.smm ? 1 : 0;\n}\n\nstatic inline int kvm_mmu_page_as_id(struct kvm_mmu_page *sp)\n{\n\treturn kvm_mmu_role_as_id(sp->role);\n}\n\nstatic inline bool kvm_mmu_page_ad_need_write_protect(struct kvm_mmu_page *sp)\n{\n\t \n\treturn kvm_x86_ops.cpu_dirty_log_size && sp->role.guest_mode;\n}\n\nstatic inline gfn_t gfn_round_for_level(gfn_t gfn, int level)\n{\n\treturn gfn & -KVM_PAGES_PER_HPAGE(level);\n}\n\nint mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,\n\t\t\t    gfn_t gfn, bool can_unsync, bool prefetch);\n\nvoid kvm_mmu_gfn_disallow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn);\nvoid kvm_mmu_gfn_allow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn);\nbool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,\n\t\t\t\t    struct kvm_memory_slot *slot, u64 gfn,\n\t\t\t\t    int min_level);\n\n \nstatic inline void kvm_flush_remote_tlbs_gfn(struct kvm *kvm, gfn_t gfn, int level)\n{\n\tkvm_flush_remote_tlbs_range(kvm, gfn_round_for_level(gfn, level),\n\t\t\t\t    KVM_PAGES_PER_HPAGE(level));\n}\n\nunsigned int pte_list_count(struct kvm_rmap_head *rmap_head);\n\nextern int nx_huge_pages;\nstatic inline bool is_nx_huge_page_enabled(struct kvm *kvm)\n{\n\treturn READ_ONCE(nx_huge_pages) && !kvm->arch.disable_nx_huge_pages;\n}\n\nstruct kvm_page_fault {\n\t \n\tconst gpa_t addr;\n\tconst u32 error_code;\n\tconst bool prefetch;\n\n\t \n\tconst bool exec;\n\tconst bool write;\n\tconst bool present;\n\tconst bool rsvd;\n\tconst bool user;\n\n\t \n\tconst bool is_tdp;\n\tconst bool nx_huge_page_workaround_enabled;\n\n\t \n\tbool huge_page_disallowed;\n\n\t \n\tu8 max_level;\n\n\t \n\tu8 req_level;\n\n\t \n\tu8 goal_level;\n\n\t \n\tgfn_t gfn;\n\n\t \n\tstruct kvm_memory_slot *slot;\n\n\t \n\tunsigned long mmu_seq;\n\tkvm_pfn_t pfn;\n\thva_t hva;\n\tbool map_writable;\n\n\t \n\tbool write_fault_to_shadow_pgtable;\n};\n\nint kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault);\n\n \nenum {\n\tRET_PF_CONTINUE = 0,\n\tRET_PF_RETRY,\n\tRET_PF_EMULATE,\n\tRET_PF_INVALID,\n\tRET_PF_FIXED,\n\tRET_PF_SPURIOUS,\n};\n\nstatic inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t\t\tu32 err, bool prefetch, int *emulation_type)\n{\n\tstruct kvm_page_fault fault = {\n\t\t.addr = cr2_or_gpa,\n\t\t.error_code = err,\n\t\t.exec = err & PFERR_FETCH_MASK,\n\t\t.write = err & PFERR_WRITE_MASK,\n\t\t.present = err & PFERR_PRESENT_MASK,\n\t\t.rsvd = err & PFERR_RSVD_MASK,\n\t\t.user = err & PFERR_USER_MASK,\n\t\t.prefetch = prefetch,\n\t\t.is_tdp = likely(vcpu->arch.mmu->page_fault == kvm_tdp_page_fault),\n\t\t.nx_huge_page_workaround_enabled =\n\t\t\tis_nx_huge_page_enabled(vcpu->kvm),\n\n\t\t.max_level = KVM_MAX_HUGEPAGE_LEVEL,\n\t\t.req_level = PG_LEVEL_4K,\n\t\t.goal_level = PG_LEVEL_4K,\n\t};\n\tint r;\n\n\tif (vcpu->arch.mmu->root_role.direct) {\n\t\tfault.gfn = fault.addr >> PAGE_SHIFT;\n\t\tfault.slot = kvm_vcpu_gfn_to_memslot(vcpu, fault.gfn);\n\t}\n\n\t \n\tif (!prefetch)\n\t\tvcpu->stat.pf_taken++;\n\n\tif (IS_ENABLED(CONFIG_RETPOLINE) && fault.is_tdp)\n\t\tr = kvm_tdp_page_fault(vcpu, &fault);\n\telse\n\t\tr = vcpu->arch.mmu->page_fault(vcpu, &fault);\n\n\tif (fault.write_fault_to_shadow_pgtable && emulation_type)\n\t\t*emulation_type |= EMULTYPE_WRITE_PF_TO_SP;\n\n\t \n\tif (r == RET_PF_FIXED)\n\t\tvcpu->stat.pf_fixed++;\n\telse if (prefetch)\n\t\t;\n\telse if (r == RET_PF_EMULATE)\n\t\tvcpu->stat.pf_emulate++;\n\telse if (r == RET_PF_SPURIOUS)\n\t\tvcpu->stat.pf_spurious++;\n\treturn r;\n}\n\nint kvm_mmu_max_mapping_level(struct kvm *kvm,\n\t\t\t      const struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t      int max_level);\nvoid kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault);\nvoid disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_level);\n\nvoid *mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc);\n\nvoid track_possible_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp);\nvoid untrack_possible_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp);\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}