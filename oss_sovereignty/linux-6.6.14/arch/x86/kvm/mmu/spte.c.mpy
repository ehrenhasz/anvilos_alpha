{
  "module_name": "spte.c",
  "hash_id": "355bf07b212cd6b1da9e58dc4593456c2af86e79d53d46dc333616d87282b8e9",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/mmu/spte.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/kvm_host.h>\n#include \"mmu.h\"\n#include \"mmu_internal.h\"\n#include \"x86.h\"\n#include \"spte.h\"\n\n#include <asm/e820/api.h>\n#include <asm/memtype.h>\n#include <asm/vmx.h>\n\nbool __read_mostly enable_mmio_caching = true;\nstatic bool __ro_after_init allow_mmio_caching;\nmodule_param_named(mmio_caching, enable_mmio_caching, bool, 0444);\nEXPORT_SYMBOL_GPL(enable_mmio_caching);\n\nu64 __read_mostly shadow_host_writable_mask;\nu64 __read_mostly shadow_mmu_writable_mask;\nu64 __read_mostly shadow_nx_mask;\nu64 __read_mostly shadow_x_mask;  \nu64 __read_mostly shadow_user_mask;\nu64 __read_mostly shadow_accessed_mask;\nu64 __read_mostly shadow_dirty_mask;\nu64 __read_mostly shadow_mmio_value;\nu64 __read_mostly shadow_mmio_mask;\nu64 __read_mostly shadow_mmio_access_mask;\nu64 __read_mostly shadow_present_mask;\nu64 __read_mostly shadow_memtype_mask;\nu64 __read_mostly shadow_me_value;\nu64 __read_mostly shadow_me_mask;\nu64 __read_mostly shadow_acc_track_mask;\n\nu64 __read_mostly shadow_nonpresent_or_rsvd_mask;\nu64 __read_mostly shadow_nonpresent_or_rsvd_lower_gfn_mask;\n\nu8 __read_mostly shadow_phys_bits;\n\nvoid __init kvm_mmu_spte_module_init(void)\n{\n\t \n\tallow_mmio_caching = enable_mmio_caching;\n}\n\nstatic u64 generation_mmio_spte_mask(u64 gen)\n{\n\tu64 mask;\n\n\tWARN_ON_ONCE(gen & ~MMIO_SPTE_GEN_MASK);\n\n\tmask = (gen << MMIO_SPTE_GEN_LOW_SHIFT) & MMIO_SPTE_GEN_LOW_MASK;\n\tmask |= (gen << MMIO_SPTE_GEN_HIGH_SHIFT) & MMIO_SPTE_GEN_HIGH_MASK;\n\treturn mask;\n}\n\nu64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access)\n{\n\tu64 gen = kvm_vcpu_memslots(vcpu)->generation & MMIO_SPTE_GEN_MASK;\n\tu64 spte = generation_mmio_spte_mask(gen);\n\tu64 gpa = gfn << PAGE_SHIFT;\n\n\tWARN_ON_ONCE(!shadow_mmio_value);\n\n\taccess &= shadow_mmio_access_mask;\n\tspte |= shadow_mmio_value | access;\n\tspte |= gpa | shadow_nonpresent_or_rsvd_mask;\n\tspte |= (gpa & shadow_nonpresent_or_rsvd_mask)\n\t\t<< SHADOW_NONPRESENT_OR_RSVD_MASK_LEN;\n\n\treturn spte;\n}\n\nstatic bool kvm_is_mmio_pfn(kvm_pfn_t pfn)\n{\n\tif (pfn_valid(pfn))\n\t\treturn !is_zero_pfn(pfn) && PageReserved(pfn_to_page(pfn)) &&\n\t\t\t \n\t\t\t(!pat_enabled() || pat_pfn_immune_to_uc_mtrr(pfn));\n\n\treturn !e820__mapped_raw_any(pfn_to_hpa(pfn),\n\t\t\t\t     pfn_to_hpa(pfn + 1) - 1,\n\t\t\t\t     E820_TYPE_RAM);\n}\n\n \nbool spte_has_volatile_bits(u64 spte)\n{\n\t \n\tif (!is_writable_pte(spte) && is_mmu_writable_spte(spte))\n\t\treturn true;\n\n\tif (is_access_track_spte(spte))\n\t\treturn true;\n\n\tif (spte_ad_enabled(spte)) {\n\t\tif (!(spte & shadow_accessed_mask) ||\n\t\t    (is_writable_pte(spte) && !(spte & shadow_dirty_mask)))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nbool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,\n\t       const struct kvm_memory_slot *slot,\n\t       unsigned int pte_access, gfn_t gfn, kvm_pfn_t pfn,\n\t       u64 old_spte, bool prefetch, bool can_unsync,\n\t       bool host_writable, u64 *new_spte)\n{\n\tint level = sp->role.level;\n\tu64 spte = SPTE_MMU_PRESENT_MASK;\n\tbool wrprot = false;\n\n\tWARN_ON_ONCE(!pte_access && !shadow_present_mask);\n\n\tif (sp->role.ad_disabled)\n\t\tspte |= SPTE_TDP_AD_DISABLED;\n\telse if (kvm_mmu_page_ad_need_write_protect(sp))\n\t\tspte |= SPTE_TDP_AD_WRPROT_ONLY;\n\n\t \n\tspte |= shadow_present_mask;\n\tif (!prefetch)\n\t\tspte |= spte_shadow_accessed_mask(spte);\n\n\t \n\tif (level > PG_LEVEL_4K && (pte_access & ACC_EXEC_MASK) &&\n\t    is_nx_huge_page_enabled(vcpu->kvm)) {\n\t\tpte_access &= ~ACC_EXEC_MASK;\n\t}\n\n\tif (pte_access & ACC_EXEC_MASK)\n\t\tspte |= shadow_x_mask;\n\telse\n\t\tspte |= shadow_nx_mask;\n\n\tif (pte_access & ACC_USER_MASK)\n\t\tspte |= shadow_user_mask;\n\n\tif (level > PG_LEVEL_4K)\n\t\tspte |= PT_PAGE_SIZE_MASK;\n\n\tif (shadow_memtype_mask)\n\t\tspte |= static_call(kvm_x86_get_mt_mask)(vcpu, gfn,\n\t\t\t\t\t\t\t kvm_is_mmio_pfn(pfn));\n\tif (host_writable)\n\t\tspte |= shadow_host_writable_mask;\n\telse\n\t\tpte_access &= ~ACC_WRITE_MASK;\n\n\tif (shadow_me_value && !kvm_is_mmio_pfn(pfn))\n\t\tspte |= shadow_me_value;\n\n\tspte |= (u64)pfn << PAGE_SHIFT;\n\n\tif (pte_access & ACC_WRITE_MASK) {\n\t\tspte |= PT_WRITABLE_MASK | shadow_mmu_writable_mask;\n\n\t\t \n\t\tif (is_writable_pte(old_spte))\n\t\t\tgoto out;\n\n\t\t \n\t\tif (mmu_try_to_unsync_pages(vcpu->kvm, slot, gfn, can_unsync, prefetch)) {\n\t\t\twrprot = true;\n\t\t\tpte_access &= ~ACC_WRITE_MASK;\n\t\t\tspte &= ~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);\n\t\t}\n\t}\n\n\tif (pte_access & ACC_WRITE_MASK)\n\t\tspte |= spte_shadow_dirty_mask(spte);\n\nout:\n\tif (prefetch)\n\t\tspte = mark_spte_for_access_track(spte);\n\n\tWARN_ONCE(is_rsvd_spte(&vcpu->arch.mmu->shadow_zero_check, spte, level),\n\t\t  \"spte = 0x%llx, level = %d, rsvd bits = 0x%llx\", spte, level,\n\t\t  get_rsvd_bits(&vcpu->arch.mmu->shadow_zero_check, spte, level));\n\n\tif ((spte & PT_WRITABLE_MASK) && kvm_slot_dirty_track_enabled(slot)) {\n\t\t \n\t\tWARN_ON_ONCE(level > PG_LEVEL_4K);\n\t\tmark_page_dirty_in_slot(vcpu->kvm, slot, gfn);\n\t}\n\n\t*new_spte = spte;\n\treturn wrprot;\n}\n\nstatic u64 make_spte_executable(u64 spte)\n{\n\tbool is_access_track = is_access_track_spte(spte);\n\n\tif (is_access_track)\n\t\tspte = restore_acc_track_spte(spte);\n\n\tspte &= ~shadow_nx_mask;\n\tspte |= shadow_x_mask;\n\n\tif (is_access_track)\n\t\tspte = mark_spte_for_access_track(spte);\n\n\treturn spte;\n}\n\n \nu64 make_huge_page_split_spte(struct kvm *kvm, u64 huge_spte, union kvm_mmu_page_role role,\n\t\t\t      int index)\n{\n\tu64 child_spte;\n\n\tif (WARN_ON_ONCE(!is_shadow_present_pte(huge_spte)))\n\t\treturn 0;\n\n\tif (WARN_ON_ONCE(!is_large_pte(huge_spte)))\n\t\treturn 0;\n\n\tchild_spte = huge_spte;\n\n\t \n\tchild_spte |= (index * KVM_PAGES_PER_HPAGE(role.level)) << PAGE_SHIFT;\n\n\tif (role.level == PG_LEVEL_4K) {\n\t\tchild_spte &= ~PT_PAGE_SIZE_MASK;\n\n\t\t \n\t\tif ((role.access & ACC_EXEC_MASK) && is_nx_huge_page_enabled(kvm))\n\t\t\tchild_spte = make_spte_executable(child_spte);\n\t}\n\n\treturn child_spte;\n}\n\n\nu64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled)\n{\n\tu64 spte = SPTE_MMU_PRESENT_MASK;\n\n\tspte |= __pa(child_pt) | shadow_present_mask | PT_WRITABLE_MASK |\n\t\tshadow_user_mask | shadow_x_mask | shadow_me_value;\n\n\tif (ad_disabled)\n\t\tspte |= SPTE_TDP_AD_DISABLED;\n\telse\n\t\tspte |= shadow_accessed_mask;\n\n\treturn spte;\n}\n\nu64 kvm_mmu_changed_pte_notifier_make_spte(u64 old_spte, kvm_pfn_t new_pfn)\n{\n\tu64 new_spte;\n\n\tnew_spte = old_spte & ~SPTE_BASE_ADDR_MASK;\n\tnew_spte |= (u64)new_pfn << PAGE_SHIFT;\n\n\tnew_spte &= ~PT_WRITABLE_MASK;\n\tnew_spte &= ~shadow_host_writable_mask;\n\tnew_spte &= ~shadow_mmu_writable_mask;\n\n\tnew_spte = mark_spte_for_access_track(new_spte);\n\n\treturn new_spte;\n}\n\nu64 mark_spte_for_access_track(u64 spte)\n{\n\tif (spte_ad_enabled(spte))\n\t\treturn spte & ~shadow_accessed_mask;\n\n\tif (is_access_track_spte(spte))\n\t\treturn spte;\n\n\tcheck_spte_writable_invariants(spte);\n\n\tWARN_ONCE(spte & (SHADOW_ACC_TRACK_SAVED_BITS_MASK <<\n\t\t\t  SHADOW_ACC_TRACK_SAVED_BITS_SHIFT),\n\t\t  \"Access Tracking saved bit locations are not zero\\n\");\n\n\tspte |= (spte & SHADOW_ACC_TRACK_SAVED_BITS_MASK) <<\n\t\tSHADOW_ACC_TRACK_SAVED_BITS_SHIFT;\n\tspte &= ~shadow_acc_track_mask;\n\n\treturn spte;\n}\n\nvoid kvm_mmu_set_mmio_spte_mask(u64 mmio_value, u64 mmio_mask, u64 access_mask)\n{\n\tBUG_ON((u64)(unsigned)access_mask != access_mask);\n\tWARN_ON(mmio_value & shadow_nonpresent_or_rsvd_lower_gfn_mask);\n\n\t \n\tenable_mmio_caching = allow_mmio_caching;\n\tif (!enable_mmio_caching)\n\t\tmmio_value = 0;\n\n\t \n\tif (WARN_ON(mmio_mask & ~SPTE_MMIO_ALLOWED_MASK))\n\t\tmmio_value = 0;\n\n\t \n\tif (WARN_ON(mmio_value & (shadow_nonpresent_or_rsvd_mask <<\n\t\t\t\t  SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)))\n\t\tmmio_value = 0;\n\n\t \n\tif (WARN_ON((mmio_value & mmio_mask) != mmio_value) ||\n\t    WARN_ON(mmio_value && (REMOVED_SPTE & mmio_mask) == mmio_value))\n\t\tmmio_value = 0;\n\n\tif (!mmio_value)\n\t\tenable_mmio_caching = false;\n\n\tshadow_mmio_value = mmio_value;\n\tshadow_mmio_mask  = mmio_mask;\n\tshadow_mmio_access_mask = access_mask;\n}\nEXPORT_SYMBOL_GPL(kvm_mmu_set_mmio_spte_mask);\n\nvoid kvm_mmu_set_me_spte_mask(u64 me_value, u64 me_mask)\n{\n\t \n\tif (WARN_ON(me_value & ~me_mask))\n\t\tme_value = me_mask = 0;\n\n\tshadow_me_value = me_value;\n\tshadow_me_mask = me_mask;\n}\nEXPORT_SYMBOL_GPL(kvm_mmu_set_me_spte_mask);\n\nvoid kvm_mmu_set_ept_masks(bool has_ad_bits, bool has_exec_only)\n{\n\tshadow_user_mask\t= VMX_EPT_READABLE_MASK;\n\tshadow_accessed_mask\t= has_ad_bits ? VMX_EPT_ACCESS_BIT : 0ull;\n\tshadow_dirty_mask\t= has_ad_bits ? VMX_EPT_DIRTY_BIT : 0ull;\n\tshadow_nx_mask\t\t= 0ull;\n\tshadow_x_mask\t\t= VMX_EPT_EXECUTABLE_MASK;\n\tshadow_present_mask\t= has_exec_only ? 0ull : VMX_EPT_READABLE_MASK;\n\t \n\tshadow_memtype_mask\t= VMX_EPT_MT_MASK | VMX_EPT_IPAT_BIT;\n\tshadow_acc_track_mask\t= VMX_EPT_RWX_MASK;\n\tshadow_host_writable_mask = EPT_SPTE_HOST_WRITABLE;\n\tshadow_mmu_writable_mask  = EPT_SPTE_MMU_WRITABLE;\n\n\t \n\tkvm_mmu_set_mmio_spte_mask(VMX_EPT_MISCONFIG_WX_VALUE,\n\t\t\t\t   VMX_EPT_RWX_MASK, 0);\n}\nEXPORT_SYMBOL_GPL(kvm_mmu_set_ept_masks);\n\nvoid kvm_mmu_reset_all_pte_masks(void)\n{\n\tu8 low_phys_bits;\n\tu64 mask;\n\n\tshadow_phys_bits = kvm_get_shadow_phys_bits();\n\n\t \n\tshadow_nonpresent_or_rsvd_mask = 0;\n\tlow_phys_bits = boot_cpu_data.x86_phys_bits;\n\tif (boot_cpu_has_bug(X86_BUG_L1TF) &&\n\t    !WARN_ON_ONCE(boot_cpu_data.x86_cache_bits >=\n\t\t\t  52 - SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)) {\n\t\tlow_phys_bits = boot_cpu_data.x86_cache_bits\n\t\t\t- SHADOW_NONPRESENT_OR_RSVD_MASK_LEN;\n\t\tshadow_nonpresent_or_rsvd_mask =\n\t\t\trsvd_bits(low_phys_bits, boot_cpu_data.x86_cache_bits - 1);\n\t}\n\n\tshadow_nonpresent_or_rsvd_lower_gfn_mask =\n\t\tGENMASK_ULL(low_phys_bits - 1, PAGE_SHIFT);\n\n\tshadow_user_mask\t= PT_USER_MASK;\n\tshadow_accessed_mask\t= PT_ACCESSED_MASK;\n\tshadow_dirty_mask\t= PT_DIRTY_MASK;\n\tshadow_nx_mask\t\t= PT64_NX_MASK;\n\tshadow_x_mask\t\t= 0;\n\tshadow_present_mask\t= PT_PRESENT_MASK;\n\n\t \n\tshadow_memtype_mask\t= 0;\n\tshadow_acc_track_mask\t= 0;\n\tshadow_me_mask\t\t= 0;\n\tshadow_me_value\t\t= 0;\n\n\tshadow_host_writable_mask = DEFAULT_SPTE_HOST_WRITABLE;\n\tshadow_mmu_writable_mask  = DEFAULT_SPTE_MMU_WRITABLE;\n\n\t \n\tif (shadow_phys_bits < 52)\n\t\tmask = BIT_ULL(51) | PT_PRESENT_MASK;\n\telse\n\t\tmask = 0;\n\n\tkvm_mmu_set_mmio_spte_mask(mask, mask, ACC_WRITE_MASK | ACC_USER_MASK);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}