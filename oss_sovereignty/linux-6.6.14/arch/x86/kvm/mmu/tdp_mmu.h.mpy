{
  "module_name": "tdp_mmu.h",
  "hash_id": "96d9a28bcffbd8ee9f4ff44608e907420eae2b3dd4550d5164e4b22a1e6a168b",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/mmu/tdp_mmu.h",
  "human_readable_source": "\n\n#ifndef __KVM_X86_MMU_TDP_MMU_H\n#define __KVM_X86_MMU_TDP_MMU_H\n\n#include <linux/kvm_host.h>\n\n#include \"spte.h\"\n\nvoid kvm_mmu_init_tdp_mmu(struct kvm *kvm);\nvoid kvm_mmu_uninit_tdp_mmu(struct kvm *kvm);\n\nhpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu);\n\n__must_check static inline bool kvm_tdp_mmu_get_root(struct kvm_mmu_page *root)\n{\n\treturn refcount_inc_not_zero(&root->tdp_mmu_root_count);\n}\n\nvoid kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,\n\t\t\t  bool shared);\n\nbool kvm_tdp_mmu_zap_leafs(struct kvm *kvm, gfn_t start, gfn_t end, bool flush);\nbool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp);\nvoid kvm_tdp_mmu_zap_all(struct kvm *kvm);\nvoid kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm);\nvoid kvm_tdp_mmu_zap_invalidated_roots(struct kvm *kvm);\n\nint kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault);\n\nbool kvm_tdp_mmu_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range,\n\t\t\t\t bool flush);\nbool kvm_tdp_mmu_age_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range);\nbool kvm_tdp_mmu_test_age_gfn(struct kvm *kvm, struct kvm_gfn_range *range);\nbool kvm_tdp_mmu_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range);\n\nbool kvm_tdp_mmu_wrprot_slot(struct kvm *kvm,\n\t\t\t     const struct kvm_memory_slot *slot, int min_level);\nbool kvm_tdp_mmu_clear_dirty_slot(struct kvm *kvm,\n\t\t\t\t  const struct kvm_memory_slot *slot);\nvoid kvm_tdp_mmu_clear_dirty_pt_masked(struct kvm *kvm,\n\t\t\t\t       struct kvm_memory_slot *slot,\n\t\t\t\t       gfn_t gfn, unsigned long mask,\n\t\t\t\t       bool wrprot);\nvoid kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,\n\t\t\t\t       const struct kvm_memory_slot *slot);\n\nbool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t\t   int min_level);\n\nvoid kvm_tdp_mmu_try_split_huge_pages(struct kvm *kvm,\n\t\t\t\t      const struct kvm_memory_slot *slot,\n\t\t\t\t      gfn_t start, gfn_t end,\n\t\t\t\t      int target_level, bool shared);\n\nstatic inline void kvm_tdp_mmu_walk_lockless_begin(void)\n{\n\trcu_read_lock();\n}\n\nstatic inline void kvm_tdp_mmu_walk_lockless_end(void)\n{\n\trcu_read_unlock();\n}\n\nint kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,\n\t\t\t int *root_level);\nu64 *kvm_tdp_mmu_fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, u64 addr,\n\t\t\t\t\tu64 *spte);\n\n#ifdef CONFIG_X86_64\nstatic inline bool is_tdp_mmu_page(struct kvm_mmu_page *sp) { return sp->tdp_mmu_page; }\n#else\nstatic inline bool is_tdp_mmu_page(struct kvm_mmu_page *sp) { return false; }\n#endif\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}