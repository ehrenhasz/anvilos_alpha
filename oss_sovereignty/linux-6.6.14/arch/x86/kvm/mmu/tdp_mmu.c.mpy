{
  "module_name": "tdp_mmu.c",
  "hash_id": "15a0f406f22ca5e452624a6c33af1bc3d9daa371ea681f596f5f79f6c0203112",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/mmu/tdp_mmu.c",
  "human_readable_source": "\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include \"mmu.h\"\n#include \"mmu_internal.h\"\n#include \"mmutrace.h\"\n#include \"tdp_iter.h\"\n#include \"tdp_mmu.h\"\n#include \"spte.h\"\n\n#include <asm/cmpxchg.h>\n#include <trace/events/kvm.h>\n\n \nvoid kvm_mmu_init_tdp_mmu(struct kvm *kvm)\n{\n\tINIT_LIST_HEAD(&kvm->arch.tdp_mmu_roots);\n\tspin_lock_init(&kvm->arch.tdp_mmu_pages_lock);\n}\n\n \nstatic __always_inline bool kvm_lockdep_assert_mmu_lock_held(struct kvm *kvm,\n\t\t\t\t\t\t\t     bool shared)\n{\n\tif (shared)\n\t\tlockdep_assert_held_read(&kvm->mmu_lock);\n\telse\n\t\tlockdep_assert_held_write(&kvm->mmu_lock);\n\n\treturn true;\n}\n\nvoid kvm_mmu_uninit_tdp_mmu(struct kvm *kvm)\n{\n\t \n\tkvm_tdp_mmu_invalidate_all_roots(kvm);\n\tkvm_tdp_mmu_zap_invalidated_roots(kvm);\n\n\tWARN_ON(atomic64_read(&kvm->arch.tdp_mmu_pages));\n\tWARN_ON(!list_empty(&kvm->arch.tdp_mmu_roots));\n\n\t \n\trcu_barrier();\n}\n\nstatic void tdp_mmu_free_sp(struct kvm_mmu_page *sp)\n{\n\tfree_page((unsigned long)sp->spt);\n\tkmem_cache_free(mmu_page_header_cache, sp);\n}\n\n \nstatic void tdp_mmu_free_sp_rcu_callback(struct rcu_head *head)\n{\n\tstruct kvm_mmu_page *sp = container_of(head, struct kvm_mmu_page,\n\t\t\t\t\t       rcu_head);\n\n\ttdp_mmu_free_sp(sp);\n}\n\nvoid kvm_tdp_mmu_put_root(struct kvm *kvm, struct kvm_mmu_page *root,\n\t\t\t  bool shared)\n{\n\tkvm_lockdep_assert_mmu_lock_held(kvm, shared);\n\n\tif (!refcount_dec_and_test(&root->tdp_mmu_root_count))\n\t\treturn;\n\n\t \n\tKVM_BUG_ON(!is_tdp_mmu_page(root) || !root->role.invalid, kvm);\n\n\tspin_lock(&kvm->arch.tdp_mmu_pages_lock);\n\tlist_del_rcu(&root->link);\n\tspin_unlock(&kvm->arch.tdp_mmu_pages_lock);\n\tcall_rcu(&root->rcu_head, tdp_mmu_free_sp_rcu_callback);\n}\n\n \nstatic struct kvm_mmu_page *tdp_mmu_next_root(struct kvm *kvm,\n\t\t\t\t\t      struct kvm_mmu_page *prev_root,\n\t\t\t\t\t      bool shared, bool only_valid)\n{\n\tstruct kvm_mmu_page *next_root;\n\n\trcu_read_lock();\n\n\tif (prev_root)\n\t\tnext_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,\n\t\t\t\t\t\t  &prev_root->link,\n\t\t\t\t\t\t  typeof(*prev_root), link);\n\telse\n\t\tnext_root = list_first_or_null_rcu(&kvm->arch.tdp_mmu_roots,\n\t\t\t\t\t\t   typeof(*next_root), link);\n\n\twhile (next_root) {\n\t\tif ((!only_valid || !next_root->role.invalid) &&\n\t\t    kvm_tdp_mmu_get_root(next_root))\n\t\t\tbreak;\n\n\t\tnext_root = list_next_or_null_rcu(&kvm->arch.tdp_mmu_roots,\n\t\t\t\t&next_root->link, typeof(*next_root), link);\n\t}\n\n\trcu_read_unlock();\n\n\tif (prev_root)\n\t\tkvm_tdp_mmu_put_root(kvm, prev_root, shared);\n\n\treturn next_root;\n}\n\n \n#define __for_each_tdp_mmu_root_yield_safe(_kvm, _root, _as_id, _shared, _only_valid)\\\n\tfor (_root = tdp_mmu_next_root(_kvm, NULL, _shared, _only_valid);\t\\\n\t     _root;\t\t\t\t\t\t\t\t\\\n\t     _root = tdp_mmu_next_root(_kvm, _root, _shared, _only_valid))\t\\\n\t\tif (kvm_lockdep_assert_mmu_lock_held(_kvm, _shared) &&\t\t\\\n\t\t    kvm_mmu_page_as_id(_root) != _as_id) {\t\t\t\\\n\t\t} else\n\n#define for_each_valid_tdp_mmu_root_yield_safe(_kvm, _root, _as_id, _shared)\t\\\n\t__for_each_tdp_mmu_root_yield_safe(_kvm, _root, _as_id, _shared, true)\n\n#define for_each_tdp_mmu_root_yield_safe(_kvm, _root, _shared)\t\t\t\\\n\tfor (_root = tdp_mmu_next_root(_kvm, NULL, _shared, false);\t\t\\\n\t     _root;\t\t\t\t\t\t\t\t\\\n\t     _root = tdp_mmu_next_root(_kvm, _root, _shared, false))\t\t\\\n\t\tif (!kvm_lockdep_assert_mmu_lock_held(_kvm, _shared)) {\t\t\\\n\t\t} else\n\n \n#define for_each_tdp_mmu_root(_kvm, _root, _as_id)\t\t\t\\\n\tlist_for_each_entry(_root, &_kvm->arch.tdp_mmu_roots, link)\t\\\n\t\tif (kvm_lockdep_assert_mmu_lock_held(_kvm, false) &&\t\\\n\t\t    kvm_mmu_page_as_id(_root) != _as_id) {\t\t\\\n\t\t} else\n\nstatic struct kvm_mmu_page *tdp_mmu_alloc_sp(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_mmu_page *sp;\n\n\tsp = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_page_header_cache);\n\tsp->spt = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_shadow_page_cache);\n\n\treturn sp;\n}\n\nstatic void tdp_mmu_init_sp(struct kvm_mmu_page *sp, tdp_ptep_t sptep,\n\t\t\t    gfn_t gfn, union kvm_mmu_page_role role)\n{\n\tINIT_LIST_HEAD(&sp->possible_nx_huge_page_link);\n\n\tset_page_private(virt_to_page(sp->spt), (unsigned long)sp);\n\n\tsp->role = role;\n\tsp->gfn = gfn;\n\tsp->ptep = sptep;\n\tsp->tdp_mmu_page = true;\n\n\ttrace_kvm_mmu_get_page(sp, true);\n}\n\nstatic void tdp_mmu_init_child_sp(struct kvm_mmu_page *child_sp,\n\t\t\t\t  struct tdp_iter *iter)\n{\n\tstruct kvm_mmu_page *parent_sp;\n\tunion kvm_mmu_page_role role;\n\n\tparent_sp = sptep_to_sp(rcu_dereference(iter->sptep));\n\n\trole = parent_sp->role;\n\trole.level--;\n\n\ttdp_mmu_init_sp(child_sp, iter->sptep, iter->gfn, role);\n}\n\nhpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu)\n{\n\tunion kvm_mmu_page_role role = vcpu->arch.mmu->root_role;\n\tstruct kvm *kvm = vcpu->kvm;\n\tstruct kvm_mmu_page *root;\n\n\tlockdep_assert_held_write(&kvm->mmu_lock);\n\n\t \n\tfor_each_tdp_mmu_root(kvm, root, kvm_mmu_role_as_id(role)) {\n\t\tif (root->role.word == role.word &&\n\t\t    kvm_tdp_mmu_get_root(root))\n\t\t\tgoto out;\n\t}\n\n\troot = tdp_mmu_alloc_sp(vcpu);\n\ttdp_mmu_init_sp(root, NULL, 0, role);\n\n\t \n\trefcount_set(&root->tdp_mmu_root_count, 2);\n\n\tspin_lock(&kvm->arch.tdp_mmu_pages_lock);\n\tlist_add_rcu(&root->link, &kvm->arch.tdp_mmu_roots);\n\tspin_unlock(&kvm->arch.tdp_mmu_pages_lock);\n\nout:\n\treturn __pa(root->spt);\n}\n\nstatic void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,\n\t\t\t\tu64 old_spte, u64 new_spte, int level,\n\t\t\t\tbool shared);\n\nstatic void tdp_account_mmu_page(struct kvm *kvm, struct kvm_mmu_page *sp)\n{\n\tkvm_account_pgtable_pages((void *)sp->spt, +1);\n\tatomic64_inc(&kvm->arch.tdp_mmu_pages);\n}\n\nstatic void tdp_unaccount_mmu_page(struct kvm *kvm, struct kvm_mmu_page *sp)\n{\n\tkvm_account_pgtable_pages((void *)sp->spt, -1);\n\tatomic64_dec(&kvm->arch.tdp_mmu_pages);\n}\n\n \nstatic void tdp_mmu_unlink_sp(struct kvm *kvm, struct kvm_mmu_page *sp,\n\t\t\t      bool shared)\n{\n\ttdp_unaccount_mmu_page(kvm, sp);\n\n\tif (!sp->nx_huge_page_disallowed)\n\t\treturn;\n\n\tif (shared)\n\t\tspin_lock(&kvm->arch.tdp_mmu_pages_lock);\n\telse\n\t\tlockdep_assert_held_write(&kvm->mmu_lock);\n\n\tsp->nx_huge_page_disallowed = false;\n\tuntrack_possible_nx_huge_page(kvm, sp);\n\n\tif (shared)\n\t\tspin_unlock(&kvm->arch.tdp_mmu_pages_lock);\n}\n\n \nstatic void handle_removed_pt(struct kvm *kvm, tdp_ptep_t pt, bool shared)\n{\n\tstruct kvm_mmu_page *sp = sptep_to_sp(rcu_dereference(pt));\n\tint level = sp->role.level;\n\tgfn_t base_gfn = sp->gfn;\n\tint i;\n\n\ttrace_kvm_mmu_prepare_zap_page(sp);\n\n\ttdp_mmu_unlink_sp(kvm, sp, shared);\n\n\tfor (i = 0; i < SPTE_ENT_PER_PAGE; i++) {\n\t\ttdp_ptep_t sptep = pt + i;\n\t\tgfn_t gfn = base_gfn + i * KVM_PAGES_PER_HPAGE(level);\n\t\tu64 old_spte;\n\n\t\tif (shared) {\n\t\t\t \n\t\t\tfor (;;) {\n\t\t\t\told_spte = kvm_tdp_mmu_write_spte_atomic(sptep, REMOVED_SPTE);\n\t\t\t\tif (!is_removed_spte(old_spte))\n\t\t\t\t\tbreak;\n\t\t\t\tcpu_relax();\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\told_spte = kvm_tdp_mmu_read_spte(sptep);\n\t\t\tif (!is_shadow_present_pte(old_spte))\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\told_spte = kvm_tdp_mmu_write_spte(sptep, old_spte,\n\t\t\t\t\t\t\t  REMOVED_SPTE, level);\n\t\t}\n\t\thandle_changed_spte(kvm, kvm_mmu_page_as_id(sp), gfn,\n\t\t\t\t    old_spte, REMOVED_SPTE, level, shared);\n\t}\n\n\tcall_rcu(&sp->rcu_head, tdp_mmu_free_sp_rcu_callback);\n}\n\n \nstatic void handle_changed_spte(struct kvm *kvm, int as_id, gfn_t gfn,\n\t\t\t\tu64 old_spte, u64 new_spte, int level,\n\t\t\t\tbool shared)\n{\n\tbool was_present = is_shadow_present_pte(old_spte);\n\tbool is_present = is_shadow_present_pte(new_spte);\n\tbool was_leaf = was_present && is_last_spte(old_spte, level);\n\tbool is_leaf = is_present && is_last_spte(new_spte, level);\n\tbool pfn_changed = spte_to_pfn(old_spte) != spte_to_pfn(new_spte);\n\n\tWARN_ON_ONCE(level > PT64_ROOT_MAX_LEVEL);\n\tWARN_ON_ONCE(level < PG_LEVEL_4K);\n\tWARN_ON_ONCE(gfn & (KVM_PAGES_PER_HPAGE(level) - 1));\n\n\t \n\tif (was_leaf && is_leaf && pfn_changed) {\n\t\tpr_err(\"Invalid SPTE change: cannot replace a present leaf\\n\"\n\t\t       \"SPTE with another present leaf SPTE mapping a\\n\"\n\t\t       \"different PFN!\\n\"\n\t\t       \"as_id: %d gfn: %llx old_spte: %llx new_spte: %llx level: %d\",\n\t\t       as_id, gfn, old_spte, new_spte, level);\n\n\t\t \n\t\tBUG();\n\t}\n\n\tif (old_spte == new_spte)\n\t\treturn;\n\n\ttrace_kvm_tdp_mmu_spte_changed(as_id, gfn, level, old_spte, new_spte);\n\n\tif (is_leaf)\n\t\tcheck_spte_writable_invariants(new_spte);\n\n\t \n\tif (!was_present && !is_present) {\n\t\t \n\t\tif (WARN_ON_ONCE(!is_mmio_spte(old_spte) &&\n\t\t\t\t !is_mmio_spte(new_spte) &&\n\t\t\t\t !is_removed_spte(new_spte)))\n\t\t\tpr_err(\"Unexpected SPTE change! Nonpresent SPTEs\\n\"\n\t\t\t       \"should not be replaced with another,\\n\"\n\t\t\t       \"different nonpresent SPTE, unless one or both\\n\"\n\t\t\t       \"are MMIO SPTEs, or the new SPTE is\\n\"\n\t\t\t       \"a temporary removed SPTE.\\n\"\n\t\t\t       \"as_id: %d gfn: %llx old_spte: %llx new_spte: %llx level: %d\",\n\t\t\t       as_id, gfn, old_spte, new_spte, level);\n\t\treturn;\n\t}\n\n\tif (is_leaf != was_leaf)\n\t\tkvm_update_page_stats(kvm, level, is_leaf ? 1 : -1);\n\n\tif (was_leaf && is_dirty_spte(old_spte) &&\n\t    (!is_present || !is_dirty_spte(new_spte) || pfn_changed))\n\t\tkvm_set_pfn_dirty(spte_to_pfn(old_spte));\n\n\t \n\tif (was_present && !was_leaf &&\n\t    (is_leaf || !is_present || WARN_ON_ONCE(pfn_changed)))\n\t\thandle_removed_pt(kvm, spte_to_child_pt(old_spte, level), shared);\n\n\tif (was_leaf && is_accessed_spte(old_spte) &&\n\t    (!is_present || !is_accessed_spte(new_spte) || pfn_changed))\n\t\tkvm_set_pfn_accessed(spte_to_pfn(old_spte));\n}\n\n \nstatic inline int tdp_mmu_set_spte_atomic(struct kvm *kvm,\n\t\t\t\t\t  struct tdp_iter *iter,\n\t\t\t\t\t  u64 new_spte)\n{\n\tu64 *sptep = rcu_dereference(iter->sptep);\n\n\t \n\tWARN_ON_ONCE(iter->yielded || is_removed_spte(iter->old_spte));\n\n\tlockdep_assert_held_read(&kvm->mmu_lock);\n\n\t \n\tif (!try_cmpxchg64(sptep, &iter->old_spte, new_spte))\n\t\treturn -EBUSY;\n\n\thandle_changed_spte(kvm, iter->as_id, iter->gfn, iter->old_spte,\n\t\t\t    new_spte, iter->level, true);\n\n\treturn 0;\n}\n\nstatic inline int tdp_mmu_zap_spte_atomic(struct kvm *kvm,\n\t\t\t\t\t  struct tdp_iter *iter)\n{\n\tint ret;\n\n\t \n\tret = tdp_mmu_set_spte_atomic(kvm, iter, REMOVED_SPTE);\n\tif (ret)\n\t\treturn ret;\n\n\tkvm_flush_remote_tlbs_gfn(kvm, iter->gfn, iter->level);\n\n\t \n\t__kvm_tdp_mmu_write_spte(iter->sptep, 0);\n\n\treturn 0;\n}\n\n\n \nstatic u64 tdp_mmu_set_spte(struct kvm *kvm, int as_id, tdp_ptep_t sptep,\n\t\t\t    u64 old_spte, u64 new_spte, gfn_t gfn, int level)\n{\n\tlockdep_assert_held_write(&kvm->mmu_lock);\n\n\t \n\tWARN_ON_ONCE(is_removed_spte(old_spte) || is_removed_spte(new_spte));\n\n\told_spte = kvm_tdp_mmu_write_spte(sptep, old_spte, new_spte, level);\n\n\thandle_changed_spte(kvm, as_id, gfn, old_spte, new_spte, level, false);\n\treturn old_spte;\n}\n\nstatic inline void tdp_mmu_iter_set_spte(struct kvm *kvm, struct tdp_iter *iter,\n\t\t\t\t\t u64 new_spte)\n{\n\tWARN_ON_ONCE(iter->yielded);\n\titer->old_spte = tdp_mmu_set_spte(kvm, iter->as_id, iter->sptep,\n\t\t\t\t\t  iter->old_spte, new_spte,\n\t\t\t\t\t  iter->gfn, iter->level);\n}\n\n#define tdp_root_for_each_pte(_iter, _root, _start, _end) \\\n\tfor_each_tdp_pte(_iter, _root, _start, _end)\n\n#define tdp_root_for_each_leaf_pte(_iter, _root, _start, _end)\t\\\n\ttdp_root_for_each_pte(_iter, _root, _start, _end)\t\t\\\n\t\tif (!is_shadow_present_pte(_iter.old_spte) ||\t\t\\\n\t\t    !is_last_spte(_iter.old_spte, _iter.level))\t\t\\\n\t\t\tcontinue;\t\t\t\t\t\\\n\t\telse\n\n#define tdp_mmu_for_each_pte(_iter, _mmu, _start, _end)\t\t\\\n\tfor_each_tdp_pte(_iter, root_to_sp(_mmu->root.hpa), _start, _end)\n\n \nstatic inline bool __must_check tdp_mmu_iter_cond_resched(struct kvm *kvm,\n\t\t\t\t\t\t\t  struct tdp_iter *iter,\n\t\t\t\t\t\t\t  bool flush, bool shared)\n{\n\tWARN_ON_ONCE(iter->yielded);\n\n\t \n\tif (iter->next_last_level_gfn == iter->yielded_gfn)\n\t\treturn false;\n\n\tif (need_resched() || rwlock_needbreak(&kvm->mmu_lock)) {\n\t\tif (flush)\n\t\t\tkvm_flush_remote_tlbs(kvm);\n\n\t\trcu_read_unlock();\n\n\t\tif (shared)\n\t\t\tcond_resched_rwlock_read(&kvm->mmu_lock);\n\t\telse\n\t\t\tcond_resched_rwlock_write(&kvm->mmu_lock);\n\n\t\trcu_read_lock();\n\n\t\tWARN_ON_ONCE(iter->gfn > iter->next_last_level_gfn);\n\n\t\titer->yielded = true;\n\t}\n\n\treturn iter->yielded;\n}\n\nstatic inline gfn_t tdp_mmu_max_gfn_exclusive(void)\n{\n\t \n\treturn kvm_mmu_max_gfn() + 1;\n}\n\nstatic void __tdp_mmu_zap_root(struct kvm *kvm, struct kvm_mmu_page *root,\n\t\t\t       bool shared, int zap_level)\n{\n\tstruct tdp_iter iter;\n\n\tgfn_t end = tdp_mmu_max_gfn_exclusive();\n\tgfn_t start = 0;\n\n\tfor_each_tdp_pte_min_level(iter, root, zap_level, start, end) {\nretry:\n\t\tif (tdp_mmu_iter_cond_resched(kvm, &iter, false, shared))\n\t\t\tcontinue;\n\n\t\tif (!is_shadow_present_pte(iter.old_spte))\n\t\t\tcontinue;\n\n\t\tif (iter.level > zap_level)\n\t\t\tcontinue;\n\n\t\tif (!shared)\n\t\t\ttdp_mmu_iter_set_spte(kvm, &iter, 0);\n\t\telse if (tdp_mmu_set_spte_atomic(kvm, &iter, 0))\n\t\t\tgoto retry;\n\t}\n}\n\nstatic void tdp_mmu_zap_root(struct kvm *kvm, struct kvm_mmu_page *root,\n\t\t\t     bool shared)\n{\n\n\t \n\tWARN_ON_ONCE(!refcount_read(&root->tdp_mmu_root_count));\n\n\tkvm_lockdep_assert_mmu_lock_held(kvm, shared);\n\n\trcu_read_lock();\n\n\t \n\t__tdp_mmu_zap_root(kvm, root, shared, PG_LEVEL_1G);\n\t__tdp_mmu_zap_root(kvm, root, shared, root->role.level);\n\n\trcu_read_unlock();\n}\n\nbool kvm_tdp_mmu_zap_sp(struct kvm *kvm, struct kvm_mmu_page *sp)\n{\n\tu64 old_spte;\n\n\t \n\tif (WARN_ON_ONCE(!sp->ptep))\n\t\treturn false;\n\n\told_spte = kvm_tdp_mmu_read_spte(sp->ptep);\n\tif (WARN_ON_ONCE(!is_shadow_present_pte(old_spte)))\n\t\treturn false;\n\n\ttdp_mmu_set_spte(kvm, kvm_mmu_page_as_id(sp), sp->ptep, old_spte, 0,\n\t\t\t sp->gfn, sp->role.level + 1);\n\n\treturn true;\n}\n\n \nstatic bool tdp_mmu_zap_leafs(struct kvm *kvm, struct kvm_mmu_page *root,\n\t\t\t      gfn_t start, gfn_t end, bool can_yield, bool flush)\n{\n\tstruct tdp_iter iter;\n\n\tend = min(end, tdp_mmu_max_gfn_exclusive());\n\n\tlockdep_assert_held_write(&kvm->mmu_lock);\n\n\trcu_read_lock();\n\n\tfor_each_tdp_pte_min_level(iter, root, PG_LEVEL_4K, start, end) {\n\t\tif (can_yield &&\n\t\t    tdp_mmu_iter_cond_resched(kvm, &iter, flush, false)) {\n\t\t\tflush = false;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!is_shadow_present_pte(iter.old_spte) ||\n\t\t    !is_last_spte(iter.old_spte, iter.level))\n\t\t\tcontinue;\n\n\t\ttdp_mmu_iter_set_spte(kvm, &iter, 0);\n\t\tflush = true;\n\t}\n\n\trcu_read_unlock();\n\n\t \n\treturn flush;\n}\n\n \nbool kvm_tdp_mmu_zap_leafs(struct kvm *kvm, gfn_t start, gfn_t end, bool flush)\n{\n\tstruct kvm_mmu_page *root;\n\n\tfor_each_tdp_mmu_root_yield_safe(kvm, root, false)\n\t\tflush = tdp_mmu_zap_leafs(kvm, root, start, end, true, flush);\n\n\treturn flush;\n}\n\nvoid kvm_tdp_mmu_zap_all(struct kvm *kvm)\n{\n\tstruct kvm_mmu_page *root;\n\n\t \n\tfor_each_tdp_mmu_root_yield_safe(kvm, root, false)\n\t\ttdp_mmu_zap_root(kvm, root, false);\n}\n\n \nvoid kvm_tdp_mmu_zap_invalidated_roots(struct kvm *kvm)\n{\n\tstruct kvm_mmu_page *root;\n\n\tread_lock(&kvm->mmu_lock);\n\n\tfor_each_tdp_mmu_root_yield_safe(kvm, root, true) {\n\t\tif (!root->tdp_mmu_scheduled_root_to_zap)\n\t\t\tcontinue;\n\n\t\troot->tdp_mmu_scheduled_root_to_zap = false;\n\t\tKVM_BUG_ON(!root->role.invalid, kvm);\n\n\t\t \n\t\ttdp_mmu_zap_root(kvm, root, true);\n\n\t\t \n\t\tkvm_tdp_mmu_put_root(kvm, root, true);\n\t}\n\n\tread_unlock(&kvm->mmu_lock);\n}\n\n \nvoid kvm_tdp_mmu_invalidate_all_roots(struct kvm *kvm)\n{\n\tstruct kvm_mmu_page *root;\n\n\t \n\tif (IS_ENABLED(CONFIG_PROVE_LOCKING) &&\n\t    refcount_read(&kvm->users_count) && kvm->created_vcpus)\n\t\tlockdep_assert_held_write(&kvm->mmu_lock);\n\n\t \n\tlist_for_each_entry(root, &kvm->arch.tdp_mmu_roots, link) {\n\t\t \n\t\tif (!root->role.invalid) {\n\t\t\troot->tdp_mmu_scheduled_root_to_zap = true;\n\t\t\troot->role.invalid = true;\n\t\t}\n\t}\n}\n\n \nstatic int tdp_mmu_map_handle_target_level(struct kvm_vcpu *vcpu,\n\t\t\t\t\t  struct kvm_page_fault *fault,\n\t\t\t\t\t  struct tdp_iter *iter)\n{\n\tstruct kvm_mmu_page *sp = sptep_to_sp(rcu_dereference(iter->sptep));\n\tu64 new_spte;\n\tint ret = RET_PF_FIXED;\n\tbool wrprot = false;\n\n\tif (WARN_ON_ONCE(sp->role.level != fault->goal_level))\n\t\treturn RET_PF_RETRY;\n\n\tif (unlikely(!fault->slot))\n\t\tnew_spte = make_mmio_spte(vcpu, iter->gfn, ACC_ALL);\n\telse\n\t\twrprot = make_spte(vcpu, sp, fault->slot, ACC_ALL, iter->gfn,\n\t\t\t\t\t fault->pfn, iter->old_spte, fault->prefetch, true,\n\t\t\t\t\t fault->map_writable, &new_spte);\n\n\tif (new_spte == iter->old_spte)\n\t\tret = RET_PF_SPURIOUS;\n\telse if (tdp_mmu_set_spte_atomic(vcpu->kvm, iter, new_spte))\n\t\treturn RET_PF_RETRY;\n\telse if (is_shadow_present_pte(iter->old_spte) &&\n\t\t !is_last_spte(iter->old_spte, iter->level))\n\t\tkvm_flush_remote_tlbs_gfn(vcpu->kvm, iter->gfn, iter->level);\n\n\t \n\tif (wrprot) {\n\t\tif (fault->write)\n\t\t\tret = RET_PF_EMULATE;\n\t}\n\n\t \n\tif (unlikely(is_mmio_spte(new_spte))) {\n\t\tvcpu->stat.pf_mmio_spte_created++;\n\t\ttrace_mark_mmio_spte(rcu_dereference(iter->sptep), iter->gfn,\n\t\t\t\t     new_spte);\n\t\tret = RET_PF_EMULATE;\n\t} else {\n\t\ttrace_kvm_mmu_set_spte(iter->level, iter->gfn,\n\t\t\t\t       rcu_dereference(iter->sptep));\n\t}\n\n\treturn ret;\n}\n\n \nstatic int tdp_mmu_link_sp(struct kvm *kvm, struct tdp_iter *iter,\n\t\t\t   struct kvm_mmu_page *sp, bool shared)\n{\n\tu64 spte = make_nonleaf_spte(sp->spt, !kvm_ad_enabled());\n\tint ret = 0;\n\n\tif (shared) {\n\t\tret = tdp_mmu_set_spte_atomic(kvm, iter, spte);\n\t\tif (ret)\n\t\t\treturn ret;\n\t} else {\n\t\ttdp_mmu_iter_set_spte(kvm, iter, spte);\n\t}\n\n\ttdp_account_mmu_page(kvm, sp);\n\n\treturn 0;\n}\n\nstatic int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,\n\t\t\t\t   struct kvm_mmu_page *sp, bool shared);\n\n \nint kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tstruct kvm *kvm = vcpu->kvm;\n\tstruct tdp_iter iter;\n\tstruct kvm_mmu_page *sp;\n\tint ret = RET_PF_RETRY;\n\n\tkvm_mmu_hugepage_adjust(vcpu, fault);\n\n\ttrace_kvm_mmu_spte_requested(fault);\n\n\trcu_read_lock();\n\n\ttdp_mmu_for_each_pte(iter, mmu, fault->gfn, fault->gfn + 1) {\n\t\tint r;\n\n\t\tif (fault->nx_huge_page_workaround_enabled)\n\t\t\tdisallowed_hugepage_adjust(fault, iter.old_spte, iter.level);\n\n\t\t \n\t\tif (is_removed_spte(iter.old_spte))\n\t\t\tgoto retry;\n\n\t\tif (iter.level == fault->goal_level)\n\t\t\tgoto map_target_level;\n\n\t\t \n\t\tif (is_shadow_present_pte(iter.old_spte) &&\n\t\t    !is_large_pte(iter.old_spte))\n\t\t\tcontinue;\n\n\t\t \n\t\tsp = tdp_mmu_alloc_sp(vcpu);\n\t\ttdp_mmu_init_child_sp(sp, &iter);\n\n\t\tsp->nx_huge_page_disallowed = fault->huge_page_disallowed;\n\n\t\tif (is_shadow_present_pte(iter.old_spte))\n\t\t\tr = tdp_mmu_split_huge_page(kvm, &iter, sp, true);\n\t\telse\n\t\t\tr = tdp_mmu_link_sp(kvm, &iter, sp, true);\n\n\t\t \n\t\tif (r) {\n\t\t\ttdp_mmu_free_sp(sp);\n\t\t\tgoto retry;\n\t\t}\n\n\t\tif (fault->huge_page_disallowed &&\n\t\t    fault->req_level >= iter.level) {\n\t\t\tspin_lock(&kvm->arch.tdp_mmu_pages_lock);\n\t\t\tif (sp->nx_huge_page_disallowed)\n\t\t\t\ttrack_possible_nx_huge_page(kvm, sp);\n\t\t\tspin_unlock(&kvm->arch.tdp_mmu_pages_lock);\n\t\t}\n\t}\n\n\t \n\tWARN_ON_ONCE(iter.level == fault->goal_level);\n\tgoto retry;\n\nmap_target_level:\n\tret = tdp_mmu_map_handle_target_level(vcpu, fault, &iter);\n\nretry:\n\trcu_read_unlock();\n\treturn ret;\n}\n\nbool kvm_tdp_mmu_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range,\n\t\t\t\t bool flush)\n{\n\tstruct kvm_mmu_page *root;\n\n\t__for_each_tdp_mmu_root_yield_safe(kvm, root, range->slot->as_id, false, false)\n\t\tflush = tdp_mmu_zap_leafs(kvm, root, range->start, range->end,\n\t\t\t\t\t  range->may_block, flush);\n\n\treturn flush;\n}\n\ntypedef bool (*tdp_handler_t)(struct kvm *kvm, struct tdp_iter *iter,\n\t\t\t      struct kvm_gfn_range *range);\n\nstatic __always_inline bool kvm_tdp_mmu_handle_gfn(struct kvm *kvm,\n\t\t\t\t\t\t   struct kvm_gfn_range *range,\n\t\t\t\t\t\t   tdp_handler_t handler)\n{\n\tstruct kvm_mmu_page *root;\n\tstruct tdp_iter iter;\n\tbool ret = false;\n\n\t \n\tfor_each_tdp_mmu_root(kvm, root, range->slot->as_id) {\n\t\trcu_read_lock();\n\n\t\ttdp_root_for_each_leaf_pte(iter, root, range->start, range->end)\n\t\t\tret |= handler(kvm, &iter, range);\n\n\t\trcu_read_unlock();\n\t}\n\n\treturn ret;\n}\n\n \nstatic bool age_gfn_range(struct kvm *kvm, struct tdp_iter *iter,\n\t\t\t  struct kvm_gfn_range *range)\n{\n\tu64 new_spte;\n\n\t \n\tif (!is_accessed_spte(iter->old_spte))\n\t\treturn false;\n\n\tif (spte_ad_enabled(iter->old_spte)) {\n\t\titer->old_spte = tdp_mmu_clear_spte_bits(iter->sptep,\n\t\t\t\t\t\t\t iter->old_spte,\n\t\t\t\t\t\t\t shadow_accessed_mask,\n\t\t\t\t\t\t\t iter->level);\n\t\tnew_spte = iter->old_spte & ~shadow_accessed_mask;\n\t} else {\n\t\t \n\t\tif (is_writable_pte(iter->old_spte))\n\t\t\tkvm_set_pfn_dirty(spte_to_pfn(iter->old_spte));\n\n\t\tnew_spte = mark_spte_for_access_track(iter->old_spte);\n\t\titer->old_spte = kvm_tdp_mmu_write_spte(iter->sptep,\n\t\t\t\t\t\t\titer->old_spte, new_spte,\n\t\t\t\t\t\t\titer->level);\n\t}\n\n\ttrace_kvm_tdp_mmu_spte_changed(iter->as_id, iter->gfn, iter->level,\n\t\t\t\t       iter->old_spte, new_spte);\n\treturn true;\n}\n\nbool kvm_tdp_mmu_age_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)\n{\n\treturn kvm_tdp_mmu_handle_gfn(kvm, range, age_gfn_range);\n}\n\nstatic bool test_age_gfn(struct kvm *kvm, struct tdp_iter *iter,\n\t\t\t struct kvm_gfn_range *range)\n{\n\treturn is_accessed_spte(iter->old_spte);\n}\n\nbool kvm_tdp_mmu_test_age_gfn(struct kvm *kvm, struct kvm_gfn_range *range)\n{\n\treturn kvm_tdp_mmu_handle_gfn(kvm, range, test_age_gfn);\n}\n\nstatic bool set_spte_gfn(struct kvm *kvm, struct tdp_iter *iter,\n\t\t\t struct kvm_gfn_range *range)\n{\n\tu64 new_spte;\n\n\t \n\tWARN_ON_ONCE(pte_huge(range->arg.pte) || range->start + 1 != range->end);\n\n\tif (iter->level != PG_LEVEL_4K ||\n\t    !is_shadow_present_pte(iter->old_spte))\n\t\treturn false;\n\n\t \n\ttdp_mmu_iter_set_spte(kvm, iter, 0);\n\n\tif (!pte_write(range->arg.pte)) {\n\t\tnew_spte = kvm_mmu_changed_pte_notifier_make_spte(iter->old_spte,\n\t\t\t\t\t\t\t\t  pte_pfn(range->arg.pte));\n\n\t\ttdp_mmu_iter_set_spte(kvm, iter, new_spte);\n\t}\n\n\treturn true;\n}\n\n \nbool kvm_tdp_mmu_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)\n{\n\t \n\treturn kvm_tdp_mmu_handle_gfn(kvm, range, set_spte_gfn);\n}\n\n \nstatic bool wrprot_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,\n\t\t\t     gfn_t start, gfn_t end, int min_level)\n{\n\tstruct tdp_iter iter;\n\tu64 new_spte;\n\tbool spte_set = false;\n\n\trcu_read_lock();\n\n\tBUG_ON(min_level > KVM_MAX_HUGEPAGE_LEVEL);\n\n\tfor_each_tdp_pte_min_level(iter, root, min_level, start, end) {\nretry:\n\t\tif (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))\n\t\t\tcontinue;\n\n\t\tif (!is_shadow_present_pte(iter.old_spte) ||\n\t\t    !is_last_spte(iter.old_spte, iter.level) ||\n\t\t    !(iter.old_spte & PT_WRITABLE_MASK))\n\t\t\tcontinue;\n\n\t\tnew_spte = iter.old_spte & ~PT_WRITABLE_MASK;\n\n\t\tif (tdp_mmu_set_spte_atomic(kvm, &iter, new_spte))\n\t\t\tgoto retry;\n\n\t\tspte_set = true;\n\t}\n\n\trcu_read_unlock();\n\treturn spte_set;\n}\n\n \nbool kvm_tdp_mmu_wrprot_slot(struct kvm *kvm,\n\t\t\t     const struct kvm_memory_slot *slot, int min_level)\n{\n\tstruct kvm_mmu_page *root;\n\tbool spte_set = false;\n\n\tlockdep_assert_held_read(&kvm->mmu_lock);\n\n\tfor_each_valid_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)\n\t\tspte_set |= wrprot_gfn_range(kvm, root, slot->base_gfn,\n\t\t\t     slot->base_gfn + slot->npages, min_level);\n\n\treturn spte_set;\n}\n\nstatic struct kvm_mmu_page *__tdp_mmu_alloc_sp_for_split(gfp_t gfp)\n{\n\tstruct kvm_mmu_page *sp;\n\n\tgfp |= __GFP_ZERO;\n\n\tsp = kmem_cache_alloc(mmu_page_header_cache, gfp);\n\tif (!sp)\n\t\treturn NULL;\n\n\tsp->spt = (void *)__get_free_page(gfp);\n\tif (!sp->spt) {\n\t\tkmem_cache_free(mmu_page_header_cache, sp);\n\t\treturn NULL;\n\t}\n\n\treturn sp;\n}\n\nstatic struct kvm_mmu_page *tdp_mmu_alloc_sp_for_split(struct kvm *kvm,\n\t\t\t\t\t\t       struct tdp_iter *iter,\n\t\t\t\t\t\t       bool shared)\n{\n\tstruct kvm_mmu_page *sp;\n\n\t \n\tsp = __tdp_mmu_alloc_sp_for_split(GFP_NOWAIT | __GFP_ACCOUNT);\n\tif (sp)\n\t\treturn sp;\n\n\trcu_read_unlock();\n\n\tif (shared)\n\t\tread_unlock(&kvm->mmu_lock);\n\telse\n\t\twrite_unlock(&kvm->mmu_lock);\n\n\titer->yielded = true;\n\tsp = __tdp_mmu_alloc_sp_for_split(GFP_KERNEL_ACCOUNT);\n\n\tif (shared)\n\t\tread_lock(&kvm->mmu_lock);\n\telse\n\t\twrite_lock(&kvm->mmu_lock);\n\n\trcu_read_lock();\n\n\treturn sp;\n}\n\n \nstatic int tdp_mmu_split_huge_page(struct kvm *kvm, struct tdp_iter *iter,\n\t\t\t\t   struct kvm_mmu_page *sp, bool shared)\n{\n\tconst u64 huge_spte = iter->old_spte;\n\tconst int level = iter->level;\n\tint ret, i;\n\n\t \n\tfor (i = 0; i < SPTE_ENT_PER_PAGE; i++)\n\t\tsp->spt[i] = make_huge_page_split_spte(kvm, huge_spte, sp->role, i);\n\n\t \n\tret = tdp_mmu_link_sp(kvm, iter, sp, shared);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tkvm_update_page_stats(kvm, level - 1, SPTE_ENT_PER_PAGE);\n\nout:\n\ttrace_kvm_mmu_split_huge_page(iter->gfn, huge_spte, level, ret);\n\treturn ret;\n}\n\nstatic int tdp_mmu_split_huge_pages_root(struct kvm *kvm,\n\t\t\t\t\t struct kvm_mmu_page *root,\n\t\t\t\t\t gfn_t start, gfn_t end,\n\t\t\t\t\t int target_level, bool shared)\n{\n\tstruct kvm_mmu_page *sp = NULL;\n\tstruct tdp_iter iter;\n\tint ret = 0;\n\n\trcu_read_lock();\n\n\t \n\tfor_each_tdp_pte_min_level(iter, root, target_level + 1, start, end) {\nretry:\n\t\tif (tdp_mmu_iter_cond_resched(kvm, &iter, false, shared))\n\t\t\tcontinue;\n\n\t\tif (!is_shadow_present_pte(iter.old_spte) || !is_large_pte(iter.old_spte))\n\t\t\tcontinue;\n\n\t\tif (!sp) {\n\t\t\tsp = tdp_mmu_alloc_sp_for_split(kvm, &iter, shared);\n\t\t\tif (!sp) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\ttrace_kvm_mmu_split_huge_page(iter.gfn,\n\t\t\t\t\t\t\t      iter.old_spte,\n\t\t\t\t\t\t\t      iter.level, ret);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (iter.yielded)\n\t\t\t\tcontinue;\n\t\t}\n\n\t\ttdp_mmu_init_child_sp(sp, &iter);\n\n\t\tif (tdp_mmu_split_huge_page(kvm, &iter, sp, shared))\n\t\t\tgoto retry;\n\n\t\tsp = NULL;\n\t}\n\n\trcu_read_unlock();\n\n\t \n\tif (sp)\n\t\ttdp_mmu_free_sp(sp);\n\n\treturn ret;\n}\n\n\n \nvoid kvm_tdp_mmu_try_split_huge_pages(struct kvm *kvm,\n\t\t\t\t      const struct kvm_memory_slot *slot,\n\t\t\t\t      gfn_t start, gfn_t end,\n\t\t\t\t      int target_level, bool shared)\n{\n\tstruct kvm_mmu_page *root;\n\tint r = 0;\n\n\tkvm_lockdep_assert_mmu_lock_held(kvm, shared);\n\n\tfor_each_valid_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, shared) {\n\t\tr = tdp_mmu_split_huge_pages_root(kvm, root, start, end, target_level, shared);\n\t\tif (r) {\n\t\t\tkvm_tdp_mmu_put_root(kvm, root, shared);\n\t\t\tbreak;\n\t\t}\n\t}\n}\n\n \nstatic bool clear_dirty_gfn_range(struct kvm *kvm, struct kvm_mmu_page *root,\n\t\t\t   gfn_t start, gfn_t end)\n{\n\tu64 dbit = kvm_ad_enabled() ? shadow_dirty_mask : PT_WRITABLE_MASK;\n\tstruct tdp_iter iter;\n\tbool spte_set = false;\n\n\trcu_read_lock();\n\n\ttdp_root_for_each_leaf_pte(iter, root, start, end) {\nretry:\n\t\tif (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))\n\t\t\tcontinue;\n\n\t\tif (!is_shadow_present_pte(iter.old_spte))\n\t\t\tcontinue;\n\n\t\tKVM_MMU_WARN_ON(kvm_ad_enabled() &&\n\t\t\t\tspte_ad_need_write_protect(iter.old_spte));\n\n\t\tif (!(iter.old_spte & dbit))\n\t\t\tcontinue;\n\n\t\tif (tdp_mmu_set_spte_atomic(kvm, &iter, iter.old_spte & ~dbit))\n\t\t\tgoto retry;\n\n\t\tspte_set = true;\n\t}\n\n\trcu_read_unlock();\n\treturn spte_set;\n}\n\n \nbool kvm_tdp_mmu_clear_dirty_slot(struct kvm *kvm,\n\t\t\t\t  const struct kvm_memory_slot *slot)\n{\n\tstruct kvm_mmu_page *root;\n\tbool spte_set = false;\n\n\tlockdep_assert_held_read(&kvm->mmu_lock);\n\n\tfor_each_valid_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)\n\t\tspte_set |= clear_dirty_gfn_range(kvm, root, slot->base_gfn,\n\t\t\t\tslot->base_gfn + slot->npages);\n\n\treturn spte_set;\n}\n\n \nstatic void clear_dirty_pt_masked(struct kvm *kvm, struct kvm_mmu_page *root,\n\t\t\t\t  gfn_t gfn, unsigned long mask, bool wrprot)\n{\n\tu64 dbit = (wrprot || !kvm_ad_enabled()) ? PT_WRITABLE_MASK :\n\t\t\t\t\t\t   shadow_dirty_mask;\n\tstruct tdp_iter iter;\n\n\tlockdep_assert_held_write(&kvm->mmu_lock);\n\n\trcu_read_lock();\n\n\ttdp_root_for_each_leaf_pte(iter, root, gfn + __ffs(mask),\n\t\t\t\t    gfn + BITS_PER_LONG) {\n\t\tif (!mask)\n\t\t\tbreak;\n\n\t\tKVM_MMU_WARN_ON(kvm_ad_enabled() &&\n\t\t\t\tspte_ad_need_write_protect(iter.old_spte));\n\n\t\tif (iter.level > PG_LEVEL_4K ||\n\t\t    !(mask & (1UL << (iter.gfn - gfn))))\n\t\t\tcontinue;\n\n\t\tmask &= ~(1UL << (iter.gfn - gfn));\n\n\t\tif (!(iter.old_spte & dbit))\n\t\t\tcontinue;\n\n\t\titer.old_spte = tdp_mmu_clear_spte_bits(iter.sptep,\n\t\t\t\t\t\t\titer.old_spte, dbit,\n\t\t\t\t\t\t\titer.level);\n\n\t\ttrace_kvm_tdp_mmu_spte_changed(iter.as_id, iter.gfn, iter.level,\n\t\t\t\t\t       iter.old_spte,\n\t\t\t\t\t       iter.old_spte & ~dbit);\n\t\tkvm_set_pfn_dirty(spte_to_pfn(iter.old_spte));\n\t}\n\n\trcu_read_unlock();\n}\n\n \nvoid kvm_tdp_mmu_clear_dirty_pt_masked(struct kvm *kvm,\n\t\t\t\t       struct kvm_memory_slot *slot,\n\t\t\t\t       gfn_t gfn, unsigned long mask,\n\t\t\t\t       bool wrprot)\n{\n\tstruct kvm_mmu_page *root;\n\n\tfor_each_tdp_mmu_root(kvm, root, slot->as_id)\n\t\tclear_dirty_pt_masked(kvm, root, gfn, mask, wrprot);\n}\n\nstatic void zap_collapsible_spte_range(struct kvm *kvm,\n\t\t\t\t       struct kvm_mmu_page *root,\n\t\t\t\t       const struct kvm_memory_slot *slot)\n{\n\tgfn_t start = slot->base_gfn;\n\tgfn_t end = start + slot->npages;\n\tstruct tdp_iter iter;\n\tint max_mapping_level;\n\n\trcu_read_lock();\n\n\tfor_each_tdp_pte_min_level(iter, root, PG_LEVEL_2M, start, end) {\nretry:\n\t\tif (tdp_mmu_iter_cond_resched(kvm, &iter, false, true))\n\t\t\tcontinue;\n\n\t\tif (iter.level > KVM_MAX_HUGEPAGE_LEVEL ||\n\t\t    !is_shadow_present_pte(iter.old_spte))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (is_last_spte(iter.old_spte, iter.level))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (iter.gfn < start || iter.gfn >= end)\n\t\t\tcontinue;\n\n\t\tmax_mapping_level = kvm_mmu_max_mapping_level(kvm, slot,\n\t\t\t\t\t\t\t      iter.gfn, PG_LEVEL_NUM);\n\t\tif (max_mapping_level < iter.level)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (tdp_mmu_zap_spte_atomic(kvm, &iter))\n\t\t\tgoto retry;\n\t}\n\n\trcu_read_unlock();\n}\n\n \nvoid kvm_tdp_mmu_zap_collapsible_sptes(struct kvm *kvm,\n\t\t\t\t       const struct kvm_memory_slot *slot)\n{\n\tstruct kvm_mmu_page *root;\n\n\tlockdep_assert_held_read(&kvm->mmu_lock);\n\n\tfor_each_valid_tdp_mmu_root_yield_safe(kvm, root, slot->as_id, true)\n\t\tzap_collapsible_spte_range(kvm, root, slot);\n}\n\n \nstatic bool write_protect_gfn(struct kvm *kvm, struct kvm_mmu_page *root,\n\t\t\t      gfn_t gfn, int min_level)\n{\n\tstruct tdp_iter iter;\n\tu64 new_spte;\n\tbool spte_set = false;\n\n\tBUG_ON(min_level > KVM_MAX_HUGEPAGE_LEVEL);\n\n\trcu_read_lock();\n\n\tfor_each_tdp_pte_min_level(iter, root, min_level, gfn, gfn + 1) {\n\t\tif (!is_shadow_present_pte(iter.old_spte) ||\n\t\t    !is_last_spte(iter.old_spte, iter.level))\n\t\t\tcontinue;\n\n\t\tnew_spte = iter.old_spte &\n\t\t\t~(PT_WRITABLE_MASK | shadow_mmu_writable_mask);\n\n\t\tif (new_spte == iter.old_spte)\n\t\t\tbreak;\n\n\t\ttdp_mmu_iter_set_spte(kvm, &iter, new_spte);\n\t\tspte_set = true;\n\t}\n\n\trcu_read_unlock();\n\n\treturn spte_set;\n}\n\n \nbool kvm_tdp_mmu_write_protect_gfn(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t\t   int min_level)\n{\n\tstruct kvm_mmu_page *root;\n\tbool spte_set = false;\n\n\tlockdep_assert_held_write(&kvm->mmu_lock);\n\tfor_each_tdp_mmu_root(kvm, root, slot->as_id)\n\t\tspte_set |= write_protect_gfn(kvm, root, gfn, min_level);\n\n\treturn spte_set;\n}\n\n \nint kvm_tdp_mmu_get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes,\n\t\t\t int *root_level)\n{\n\tstruct tdp_iter iter;\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tgfn_t gfn = addr >> PAGE_SHIFT;\n\tint leaf = -1;\n\n\t*root_level = vcpu->arch.mmu->root_role.level;\n\n\ttdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {\n\t\tleaf = iter.level;\n\t\tsptes[leaf] = iter.old_spte;\n\t}\n\n\treturn leaf;\n}\n\n \nu64 *kvm_tdp_mmu_fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, u64 addr,\n\t\t\t\t\tu64 *spte)\n{\n\tstruct tdp_iter iter;\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tgfn_t gfn = addr >> PAGE_SHIFT;\n\ttdp_ptep_t sptep = NULL;\n\n\ttdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {\n\t\t*spte = iter.old_spte;\n\t\tsptep = iter.sptep;\n\t}\n\n\t \n\treturn rcu_dereference(sptep);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}