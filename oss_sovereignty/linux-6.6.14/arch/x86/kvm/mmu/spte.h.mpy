{
  "module_name": "spte.h",
  "hash_id": "0a322bab9b62ef76c38d881059849f26f6a87b11304120c5b0551fc9022a34ae",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/mmu/spte.h",
  "human_readable_source": "\n\n#ifndef KVM_X86_MMU_SPTE_H\n#define KVM_X86_MMU_SPTE_H\n\n#include \"mmu.h\"\n#include \"mmu_internal.h\"\n\n \n#define SPTE_MMU_PRESENT_MASK\t\tBIT_ULL(11)\n\n \n#define SPTE_TDP_AD_SHIFT\t\t52\n#define SPTE_TDP_AD_MASK\t\t(3ULL << SPTE_TDP_AD_SHIFT)\n#define SPTE_TDP_AD_ENABLED\t\t(0ULL << SPTE_TDP_AD_SHIFT)\n#define SPTE_TDP_AD_DISABLED\t\t(1ULL << SPTE_TDP_AD_SHIFT)\n#define SPTE_TDP_AD_WRPROT_ONLY\t\t(2ULL << SPTE_TDP_AD_SHIFT)\nstatic_assert(SPTE_TDP_AD_ENABLED == 0);\n\n#ifdef CONFIG_DYNAMIC_PHYSICAL_MASK\n#define SPTE_BASE_ADDR_MASK (physical_mask & ~(u64)(PAGE_SIZE-1))\n#else\n#define SPTE_BASE_ADDR_MASK (((1ULL << 52) - 1) & ~(u64)(PAGE_SIZE-1))\n#endif\n\n#define SPTE_PERM_MASK (PT_PRESENT_MASK | PT_WRITABLE_MASK | shadow_user_mask \\\n\t\t\t| shadow_x_mask | shadow_nx_mask | shadow_me_mask)\n\n#define ACC_EXEC_MASK    1\n#define ACC_WRITE_MASK   PT_WRITABLE_MASK\n#define ACC_USER_MASK    PT_USER_MASK\n#define ACC_ALL          (ACC_EXEC_MASK | ACC_WRITE_MASK | ACC_USER_MASK)\n\n \n#define SPTE_EPT_READABLE_MASK\t\t\t0x1ull\n#define SPTE_EPT_EXECUTABLE_MASK\t\t0x4ull\n\n#define SPTE_LEVEL_BITS\t\t\t9\n#define SPTE_LEVEL_SHIFT(level)\t\t__PT_LEVEL_SHIFT(level, SPTE_LEVEL_BITS)\n#define SPTE_INDEX(address, level)\t__PT_INDEX(address, level, SPTE_LEVEL_BITS)\n#define SPTE_ENT_PER_PAGE\t\t__PT_ENT_PER_PAGE(SPTE_LEVEL_BITS)\n\n \n#define SHADOW_ACC_TRACK_SAVED_BITS_MASK (SPTE_EPT_READABLE_MASK | \\\n\t\t\t\t\t  SPTE_EPT_EXECUTABLE_MASK)\n#define SHADOW_ACC_TRACK_SAVED_BITS_SHIFT 54\n#define SHADOW_ACC_TRACK_SAVED_MASK\t(SHADOW_ACC_TRACK_SAVED_BITS_MASK << \\\n\t\t\t\t\t SHADOW_ACC_TRACK_SAVED_BITS_SHIFT)\nstatic_assert(!(SPTE_TDP_AD_MASK & SHADOW_ACC_TRACK_SAVED_MASK));\n\n \n\n \n#define DEFAULT_SPTE_HOST_WRITABLE\tBIT_ULL(9)\n#define DEFAULT_SPTE_MMU_WRITABLE\tBIT_ULL(10)\n\n \n#define EPT_SPTE_HOST_WRITABLE\t\tBIT_ULL(57)\n#define EPT_SPTE_MMU_WRITABLE\t\tBIT_ULL(58)\n\nstatic_assert(!(EPT_SPTE_HOST_WRITABLE & SPTE_TDP_AD_MASK));\nstatic_assert(!(EPT_SPTE_MMU_WRITABLE & SPTE_TDP_AD_MASK));\nstatic_assert(!(EPT_SPTE_HOST_WRITABLE & SHADOW_ACC_TRACK_SAVED_MASK));\nstatic_assert(!(EPT_SPTE_MMU_WRITABLE & SHADOW_ACC_TRACK_SAVED_MASK));\n\n \n#undef SHADOW_ACC_TRACK_SAVED_MASK\n\n \n\n#define MMIO_SPTE_GEN_LOW_START\t\t3\n#define MMIO_SPTE_GEN_LOW_END\t\t10\n\n#define MMIO_SPTE_GEN_HIGH_START\t52\n#define MMIO_SPTE_GEN_HIGH_END\t\t62\n\n#define MMIO_SPTE_GEN_LOW_MASK\t\tGENMASK_ULL(MMIO_SPTE_GEN_LOW_END, \\\n\t\t\t\t\t\t    MMIO_SPTE_GEN_LOW_START)\n#define MMIO_SPTE_GEN_HIGH_MASK\t\tGENMASK_ULL(MMIO_SPTE_GEN_HIGH_END, \\\n\t\t\t\t\t\t    MMIO_SPTE_GEN_HIGH_START)\nstatic_assert(!(SPTE_MMU_PRESENT_MASK &\n\t\t(MMIO_SPTE_GEN_LOW_MASK | MMIO_SPTE_GEN_HIGH_MASK)));\n\n \n#define SPTE_MMIO_ALLOWED_MASK (BIT_ULL(63) | GENMASK_ULL(51, 12) | GENMASK_ULL(2, 0))\nstatic_assert(!(SPTE_MMIO_ALLOWED_MASK &\n\t\t(SPTE_MMU_PRESENT_MASK | MMIO_SPTE_GEN_LOW_MASK | MMIO_SPTE_GEN_HIGH_MASK)));\n\n#define MMIO_SPTE_GEN_LOW_BITS\t\t(MMIO_SPTE_GEN_LOW_END - MMIO_SPTE_GEN_LOW_START + 1)\n#define MMIO_SPTE_GEN_HIGH_BITS\t\t(MMIO_SPTE_GEN_HIGH_END - MMIO_SPTE_GEN_HIGH_START + 1)\n\n \nstatic_assert(MMIO_SPTE_GEN_LOW_BITS == 8 && MMIO_SPTE_GEN_HIGH_BITS == 11);\n\n#define MMIO_SPTE_GEN_LOW_SHIFT\t\t(MMIO_SPTE_GEN_LOW_START - 0)\n#define MMIO_SPTE_GEN_HIGH_SHIFT\t(MMIO_SPTE_GEN_HIGH_START - MMIO_SPTE_GEN_LOW_BITS)\n\n#define MMIO_SPTE_GEN_MASK\t\tGENMASK_ULL(MMIO_SPTE_GEN_LOW_BITS + MMIO_SPTE_GEN_HIGH_BITS - 1, 0)\n\nextern u64 __read_mostly shadow_host_writable_mask;\nextern u64 __read_mostly shadow_mmu_writable_mask;\nextern u64 __read_mostly shadow_nx_mask;\nextern u64 __read_mostly shadow_x_mask;  \nextern u64 __read_mostly shadow_user_mask;\nextern u64 __read_mostly shadow_accessed_mask;\nextern u64 __read_mostly shadow_dirty_mask;\nextern u64 __read_mostly shadow_mmio_value;\nextern u64 __read_mostly shadow_mmio_mask;\nextern u64 __read_mostly shadow_mmio_access_mask;\nextern u64 __read_mostly shadow_present_mask;\nextern u64 __read_mostly shadow_memtype_mask;\nextern u64 __read_mostly shadow_me_value;\nextern u64 __read_mostly shadow_me_mask;\n\n \nextern u64 __read_mostly shadow_acc_track_mask;\n\n \nextern u64 __read_mostly shadow_nonpresent_or_rsvd_mask;\n\n \n#define SHADOW_NONPRESENT_OR_RSVD_MASK_LEN 5\n\n \n#define REMOVED_SPTE\t0x5a0ULL\n\n \nstatic_assert(!(REMOVED_SPTE & SPTE_MMU_PRESENT_MASK));\n\nstatic inline bool is_removed_spte(u64 spte)\n{\n\treturn spte == REMOVED_SPTE;\n}\n\n \nstatic inline int spte_index(u64 *sptep)\n{\n\treturn ((unsigned long)sptep / sizeof(*sptep)) & (SPTE_ENT_PER_PAGE - 1);\n}\n\n \nextern u64 __read_mostly shadow_nonpresent_or_rsvd_lower_gfn_mask;\n\nstatic inline struct kvm_mmu_page *to_shadow_page(hpa_t shadow_page)\n{\n\tstruct page *page = pfn_to_page((shadow_page) >> PAGE_SHIFT);\n\n\treturn (struct kvm_mmu_page *)page_private(page);\n}\n\nstatic inline struct kvm_mmu_page *spte_to_child_sp(u64 spte)\n{\n\treturn to_shadow_page(spte & SPTE_BASE_ADDR_MASK);\n}\n\nstatic inline struct kvm_mmu_page *sptep_to_sp(u64 *sptep)\n{\n\treturn to_shadow_page(__pa(sptep));\n}\n\nstatic inline struct kvm_mmu_page *root_to_sp(hpa_t root)\n{\n\tif (kvm_mmu_is_dummy_root(root))\n\t\treturn NULL;\n\n\t \n\treturn spte_to_child_sp(root);\n}\n\nstatic inline bool is_mmio_spte(u64 spte)\n{\n\treturn (spte & shadow_mmio_mask) == shadow_mmio_value &&\n\t       likely(enable_mmio_caching);\n}\n\nstatic inline bool is_shadow_present_pte(u64 pte)\n{\n\treturn !!(pte & SPTE_MMU_PRESENT_MASK);\n}\n\n \nstatic inline bool kvm_ad_enabled(void)\n{\n\treturn !!shadow_accessed_mask;\n}\n\nstatic inline bool sp_ad_disabled(struct kvm_mmu_page *sp)\n{\n\treturn sp->role.ad_disabled;\n}\n\nstatic inline bool spte_ad_enabled(u64 spte)\n{\n\tKVM_MMU_WARN_ON(!is_shadow_present_pte(spte));\n\treturn (spte & SPTE_TDP_AD_MASK) != SPTE_TDP_AD_DISABLED;\n}\n\nstatic inline bool spte_ad_need_write_protect(u64 spte)\n{\n\tKVM_MMU_WARN_ON(!is_shadow_present_pte(spte));\n\t \n\treturn (spte & SPTE_TDP_AD_MASK) != SPTE_TDP_AD_ENABLED;\n}\n\nstatic inline u64 spte_shadow_accessed_mask(u64 spte)\n{\n\tKVM_MMU_WARN_ON(!is_shadow_present_pte(spte));\n\treturn spte_ad_enabled(spte) ? shadow_accessed_mask : 0;\n}\n\nstatic inline u64 spte_shadow_dirty_mask(u64 spte)\n{\n\tKVM_MMU_WARN_ON(!is_shadow_present_pte(spte));\n\treturn spte_ad_enabled(spte) ? shadow_dirty_mask : 0;\n}\n\nstatic inline bool is_access_track_spte(u64 spte)\n{\n\treturn !spte_ad_enabled(spte) && (spte & shadow_acc_track_mask) == 0;\n}\n\nstatic inline bool is_large_pte(u64 pte)\n{\n\treturn pte & PT_PAGE_SIZE_MASK;\n}\n\nstatic inline bool is_last_spte(u64 pte, int level)\n{\n\treturn (level == PG_LEVEL_4K) || is_large_pte(pte);\n}\n\nstatic inline bool is_executable_pte(u64 spte)\n{\n\treturn (spte & (shadow_x_mask | shadow_nx_mask)) == shadow_x_mask;\n}\n\nstatic inline kvm_pfn_t spte_to_pfn(u64 pte)\n{\n\treturn (pte & SPTE_BASE_ADDR_MASK) >> PAGE_SHIFT;\n}\n\nstatic inline bool is_accessed_spte(u64 spte)\n{\n\tu64 accessed_mask = spte_shadow_accessed_mask(spte);\n\n\treturn accessed_mask ? spte & accessed_mask\n\t\t\t     : !is_access_track_spte(spte);\n}\n\nstatic inline bool is_dirty_spte(u64 spte)\n{\n\tu64 dirty_mask = spte_shadow_dirty_mask(spte);\n\n\treturn dirty_mask ? spte & dirty_mask : spte & PT_WRITABLE_MASK;\n}\n\nstatic inline u64 get_rsvd_bits(struct rsvd_bits_validate *rsvd_check, u64 pte,\n\t\t\t\tint level)\n{\n\tint bit7 = (pte >> 7) & 1;\n\n\treturn rsvd_check->rsvd_bits_mask[bit7][level-1];\n}\n\nstatic inline bool __is_rsvd_bits_set(struct rsvd_bits_validate *rsvd_check,\n\t\t\t\t      u64 pte, int level)\n{\n\treturn pte & get_rsvd_bits(rsvd_check, pte, level);\n}\n\nstatic inline bool __is_bad_mt_xwr(struct rsvd_bits_validate *rsvd_check,\n\t\t\t\t   u64 pte)\n{\n\treturn rsvd_check->bad_mt_xwr & BIT_ULL(pte & 0x3f);\n}\n\nstatic __always_inline bool is_rsvd_spte(struct rsvd_bits_validate *rsvd_check,\n\t\t\t\t\t u64 spte, int level)\n{\n\treturn __is_bad_mt_xwr(rsvd_check, spte) ||\n\t       __is_rsvd_bits_set(rsvd_check, spte, level);\n}\n\n \nstatic inline bool is_writable_pte(unsigned long pte)\n{\n\treturn pte & PT_WRITABLE_MASK;\n}\n\n \nstatic inline void check_spte_writable_invariants(u64 spte)\n{\n\tif (spte & shadow_mmu_writable_mask)\n\t\tWARN_ONCE(!(spte & shadow_host_writable_mask),\n\t\t\t  KBUILD_MODNAME \": MMU-writable SPTE is not Host-writable: %llx\",\n\t\t\t  spte);\n\telse\n\t\tWARN_ONCE(is_writable_pte(spte),\n\t\t\t  KBUILD_MODNAME \": Writable SPTE is not MMU-writable: %llx\", spte);\n}\n\nstatic inline bool is_mmu_writable_spte(u64 spte)\n{\n\treturn spte & shadow_mmu_writable_mask;\n}\n\nstatic inline u64 get_mmio_spte_generation(u64 spte)\n{\n\tu64 gen;\n\n\tgen = (spte & MMIO_SPTE_GEN_LOW_MASK) >> MMIO_SPTE_GEN_LOW_SHIFT;\n\tgen |= (spte & MMIO_SPTE_GEN_HIGH_MASK) >> MMIO_SPTE_GEN_HIGH_SHIFT;\n\treturn gen;\n}\n\nbool spte_has_volatile_bits(u64 spte);\n\nbool make_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,\n\t       const struct kvm_memory_slot *slot,\n\t       unsigned int pte_access, gfn_t gfn, kvm_pfn_t pfn,\n\t       u64 old_spte, bool prefetch, bool can_unsync,\n\t       bool host_writable, u64 *new_spte);\nu64 make_huge_page_split_spte(struct kvm *kvm, u64 huge_spte,\n\t\t      \t      union kvm_mmu_page_role role, int index);\nu64 make_nonleaf_spte(u64 *child_pt, bool ad_disabled);\nu64 make_mmio_spte(struct kvm_vcpu *vcpu, u64 gfn, unsigned int access);\nu64 mark_spte_for_access_track(u64 spte);\n\n \nstatic inline u64 restore_acc_track_spte(u64 spte)\n{\n\tu64 saved_bits = (spte >> SHADOW_ACC_TRACK_SAVED_BITS_SHIFT)\n\t\t\t & SHADOW_ACC_TRACK_SAVED_BITS_MASK;\n\n\tspte &= ~shadow_acc_track_mask;\n\tspte &= ~(SHADOW_ACC_TRACK_SAVED_BITS_MASK <<\n\t\t  SHADOW_ACC_TRACK_SAVED_BITS_SHIFT);\n\tspte |= saved_bits;\n\n\treturn spte;\n}\n\nu64 kvm_mmu_changed_pte_notifier_make_spte(u64 old_spte, kvm_pfn_t new_pfn);\n\nvoid __init kvm_mmu_spte_module_init(void);\nvoid kvm_mmu_reset_all_pte_masks(void);\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}