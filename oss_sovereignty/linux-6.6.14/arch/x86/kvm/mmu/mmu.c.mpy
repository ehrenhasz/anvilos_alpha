{
  "module_name": "mmu.c",
  "hash_id": "5109ad8bc62b70aef9bba1c2516428ed2aa0d89e447e08fa12273bb5a2dd1e0c",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/mmu/mmu.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include \"irq.h\"\n#include \"ioapic.h\"\n#include \"mmu.h\"\n#include \"mmu_internal.h\"\n#include \"tdp_mmu.h\"\n#include \"x86.h\"\n#include \"kvm_cache_regs.h\"\n#include \"smm.h\"\n#include \"kvm_emulate.h\"\n#include \"page_track.h\"\n#include \"cpuid.h\"\n#include \"spte.h\"\n\n#include <linux/kvm_host.h>\n#include <linux/types.h>\n#include <linux/string.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/moduleparam.h>\n#include <linux/export.h>\n#include <linux/swap.h>\n#include <linux/hugetlb.h>\n#include <linux/compiler.h>\n#include <linux/srcu.h>\n#include <linux/slab.h>\n#include <linux/sched/signal.h>\n#include <linux/uaccess.h>\n#include <linux/hash.h>\n#include <linux/kern_levels.h>\n#include <linux/kstrtox.h>\n#include <linux/kthread.h>\n\n#include <asm/page.h>\n#include <asm/memtype.h>\n#include <asm/cmpxchg.h>\n#include <asm/io.h>\n#include <asm/set_memory.h>\n#include <asm/vmx.h>\n\n#include \"trace.h\"\n\nextern bool itlb_multihit_kvm_mitigation;\n\nstatic bool nx_hugepage_mitigation_hard_disabled;\n\nint __read_mostly nx_huge_pages = -1;\nstatic uint __read_mostly nx_huge_pages_recovery_period_ms;\n#ifdef CONFIG_PREEMPT_RT\n \nstatic uint __read_mostly nx_huge_pages_recovery_ratio = 0;\n#else\nstatic uint __read_mostly nx_huge_pages_recovery_ratio = 60;\n#endif\n\nstatic int get_nx_huge_pages(char *buffer, const struct kernel_param *kp);\nstatic int set_nx_huge_pages(const char *val, const struct kernel_param *kp);\nstatic int set_nx_huge_pages_recovery_param(const char *val, const struct kernel_param *kp);\n\nstatic const struct kernel_param_ops nx_huge_pages_ops = {\n\t.set = set_nx_huge_pages,\n\t.get = get_nx_huge_pages,\n};\n\nstatic const struct kernel_param_ops nx_huge_pages_recovery_param_ops = {\n\t.set = set_nx_huge_pages_recovery_param,\n\t.get = param_get_uint,\n};\n\nmodule_param_cb(nx_huge_pages, &nx_huge_pages_ops, &nx_huge_pages, 0644);\n__MODULE_PARM_TYPE(nx_huge_pages, \"bool\");\nmodule_param_cb(nx_huge_pages_recovery_ratio, &nx_huge_pages_recovery_param_ops,\n\t\t&nx_huge_pages_recovery_ratio, 0644);\n__MODULE_PARM_TYPE(nx_huge_pages_recovery_ratio, \"uint\");\nmodule_param_cb(nx_huge_pages_recovery_period_ms, &nx_huge_pages_recovery_param_ops,\n\t\t&nx_huge_pages_recovery_period_ms, 0644);\n__MODULE_PARM_TYPE(nx_huge_pages_recovery_period_ms, \"uint\");\n\nstatic bool __read_mostly force_flush_and_sync_on_reuse;\nmodule_param_named(flush_on_reuse, force_flush_and_sync_on_reuse, bool, 0644);\n\n \nbool tdp_enabled = false;\n\nstatic bool __ro_after_init tdp_mmu_allowed;\n\n#ifdef CONFIG_X86_64\nbool __read_mostly tdp_mmu_enabled = true;\nmodule_param_named(tdp_mmu, tdp_mmu_enabled, bool, 0444);\n#endif\n\nstatic int max_huge_page_level __read_mostly;\nstatic int tdp_root_level __read_mostly;\nstatic int max_tdp_level __read_mostly;\n\n#define PTE_PREFETCH_NUM\t\t8\n\n#include <trace/events/kvm.h>\n\n \n#define PTE_LIST_EXT 14\n\n \nstruct pte_list_desc {\n\tstruct pte_list_desc *more;\n\t \n\tu32 spte_count;\n\t \n\tu32 tail_count;\n\tu64 *sptes[PTE_LIST_EXT];\n};\n\nstruct kvm_shadow_walk_iterator {\n\tu64 addr;\n\thpa_t shadow_addr;\n\tu64 *sptep;\n\tint level;\n\tunsigned index;\n};\n\n#define for_each_shadow_entry_using_root(_vcpu, _root, _addr, _walker)     \\\n\tfor (shadow_walk_init_using_root(&(_walker), (_vcpu),              \\\n\t\t\t\t\t (_root), (_addr));                \\\n\t     shadow_walk_okay(&(_walker));\t\t\t           \\\n\t     shadow_walk_next(&(_walker)))\n\n#define for_each_shadow_entry(_vcpu, _addr, _walker)            \\\n\tfor (shadow_walk_init(&(_walker), _vcpu, _addr);\t\\\n\t     shadow_walk_okay(&(_walker));\t\t\t\\\n\t     shadow_walk_next(&(_walker)))\n\n#define for_each_shadow_entry_lockless(_vcpu, _addr, _walker, spte)\t\\\n\tfor (shadow_walk_init(&(_walker), _vcpu, _addr);\t\t\\\n\t     shadow_walk_okay(&(_walker)) &&\t\t\t\t\\\n\t\t({ spte = mmu_spte_get_lockless(_walker.sptep); 1; });\t\\\n\t     __shadow_walk_next(&(_walker), spte))\n\nstatic struct kmem_cache *pte_list_desc_cache;\nstruct kmem_cache *mmu_page_header_cache;\nstatic struct percpu_counter kvm_total_used_mmu_pages;\n\nstatic void mmu_spte_set(u64 *sptep, u64 spte);\n\nstruct kvm_mmu_role_regs {\n\tconst unsigned long cr0;\n\tconst unsigned long cr4;\n\tconst u64 efer;\n};\n\n#define CREATE_TRACE_POINTS\n#include \"mmutrace.h\"\n\n \n#define BUILD_MMU_ROLE_REGS_ACCESSOR(reg, name, flag)\t\t\t\\\nstatic inline bool __maybe_unused\t\t\t\t\t\\\n____is_##reg##_##name(const struct kvm_mmu_role_regs *regs)\t\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\treturn !!(regs->reg & flag);\t\t\t\t\t\\\n}\nBUILD_MMU_ROLE_REGS_ACCESSOR(cr0, pg, X86_CR0_PG);\nBUILD_MMU_ROLE_REGS_ACCESSOR(cr0, wp, X86_CR0_WP);\nBUILD_MMU_ROLE_REGS_ACCESSOR(cr4, pse, X86_CR4_PSE);\nBUILD_MMU_ROLE_REGS_ACCESSOR(cr4, pae, X86_CR4_PAE);\nBUILD_MMU_ROLE_REGS_ACCESSOR(cr4, smep, X86_CR4_SMEP);\nBUILD_MMU_ROLE_REGS_ACCESSOR(cr4, smap, X86_CR4_SMAP);\nBUILD_MMU_ROLE_REGS_ACCESSOR(cr4, pke, X86_CR4_PKE);\nBUILD_MMU_ROLE_REGS_ACCESSOR(cr4, la57, X86_CR4_LA57);\nBUILD_MMU_ROLE_REGS_ACCESSOR(efer, nx, EFER_NX);\nBUILD_MMU_ROLE_REGS_ACCESSOR(efer, lma, EFER_LMA);\n\n \n#define BUILD_MMU_ROLE_ACCESSOR(base_or_ext, reg, name)\t\t\\\nstatic inline bool __maybe_unused is_##reg##_##name(struct kvm_mmu *mmu)\t\\\n{\t\t\t\t\t\t\t\t\\\n\treturn !!(mmu->cpu_role. base_or_ext . reg##_##name);\t\\\n}\nBUILD_MMU_ROLE_ACCESSOR(base, cr0, wp);\nBUILD_MMU_ROLE_ACCESSOR(ext,  cr4, pse);\nBUILD_MMU_ROLE_ACCESSOR(ext,  cr4, smep);\nBUILD_MMU_ROLE_ACCESSOR(ext,  cr4, smap);\nBUILD_MMU_ROLE_ACCESSOR(ext,  cr4, pke);\nBUILD_MMU_ROLE_ACCESSOR(ext,  cr4, la57);\nBUILD_MMU_ROLE_ACCESSOR(base, efer, nx);\nBUILD_MMU_ROLE_ACCESSOR(ext,  efer, lma);\n\nstatic inline bool is_cr0_pg(struct kvm_mmu *mmu)\n{\n        return mmu->cpu_role.base.level > 0;\n}\n\nstatic inline bool is_cr4_pae(struct kvm_mmu *mmu)\n{\n        return !mmu->cpu_role.base.has_4_byte_gpte;\n}\n\nstatic struct kvm_mmu_role_regs vcpu_to_role_regs(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_mmu_role_regs regs = {\n\t\t.cr0 = kvm_read_cr0_bits(vcpu, KVM_MMU_CR0_ROLE_BITS),\n\t\t.cr4 = kvm_read_cr4_bits(vcpu, KVM_MMU_CR4_ROLE_BITS),\n\t\t.efer = vcpu->arch.efer,\n\t};\n\n\treturn regs;\n}\n\nstatic unsigned long get_guest_cr3(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_read_cr3(vcpu);\n}\n\nstatic inline unsigned long kvm_mmu_get_guest_pgd(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t  struct kvm_mmu *mmu)\n{\n\tif (IS_ENABLED(CONFIG_RETPOLINE) && mmu->get_guest_pgd == get_guest_cr3)\n\t\treturn kvm_read_cr3(vcpu);\n\n\treturn mmu->get_guest_pgd(vcpu);\n}\n\nstatic inline bool kvm_available_flush_remote_tlbs_range(void)\n{\n\treturn kvm_x86_ops.flush_remote_tlbs_range;\n}\n\nint kvm_arch_flush_remote_tlbs_range(struct kvm *kvm, gfn_t gfn, u64 nr_pages)\n{\n\tif (!kvm_x86_ops.flush_remote_tlbs_range)\n\t\treturn -EOPNOTSUPP;\n\n\treturn static_call(kvm_x86_flush_remote_tlbs_range)(kvm, gfn, nr_pages);\n}\n\nstatic gfn_t kvm_mmu_page_get_gfn(struct kvm_mmu_page *sp, int index);\n\n \nstatic void kvm_flush_remote_tlbs_sptep(struct kvm *kvm, u64 *sptep)\n{\n\tstruct kvm_mmu_page *sp = sptep_to_sp(sptep);\n\tgfn_t gfn = kvm_mmu_page_get_gfn(sp, spte_index(sptep));\n\n\tkvm_flush_remote_tlbs_gfn(kvm, gfn, sp->role.level);\n}\n\nstatic void mark_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, u64 gfn,\n\t\t\t   unsigned int access)\n{\n\tu64 spte = make_mmio_spte(vcpu, gfn, access);\n\n\ttrace_mark_mmio_spte(sptep, gfn, spte);\n\tmmu_spte_set(sptep, spte);\n}\n\nstatic gfn_t get_mmio_spte_gfn(u64 spte)\n{\n\tu64 gpa = spte & shadow_nonpresent_or_rsvd_lower_gfn_mask;\n\n\tgpa |= (spte >> SHADOW_NONPRESENT_OR_RSVD_MASK_LEN)\n\t       & shadow_nonpresent_or_rsvd_mask;\n\n\treturn gpa >> PAGE_SHIFT;\n}\n\nstatic unsigned get_mmio_spte_access(u64 spte)\n{\n\treturn spte & shadow_mmio_access_mask;\n}\n\nstatic bool check_mmio_spte(struct kvm_vcpu *vcpu, u64 spte)\n{\n\tu64 kvm_gen, spte_gen, gen;\n\n\tgen = kvm_vcpu_memslots(vcpu)->generation;\n\tif (unlikely(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS))\n\t\treturn false;\n\n\tkvm_gen = gen & MMIO_SPTE_GEN_MASK;\n\tspte_gen = get_mmio_spte_generation(spte);\n\n\ttrace_check_mmio_spte(spte, kvm_gen, spte_gen);\n\treturn likely(kvm_gen == spte_gen);\n}\n\nstatic int is_cpuid_PSE36(void)\n{\n\treturn 1;\n}\n\n#ifdef CONFIG_X86_64\nstatic void __set_spte(u64 *sptep, u64 spte)\n{\n\tWRITE_ONCE(*sptep, spte);\n}\n\nstatic void __update_clear_spte_fast(u64 *sptep, u64 spte)\n{\n\tWRITE_ONCE(*sptep, spte);\n}\n\nstatic u64 __update_clear_spte_slow(u64 *sptep, u64 spte)\n{\n\treturn xchg(sptep, spte);\n}\n\nstatic u64 __get_spte_lockless(u64 *sptep)\n{\n\treturn READ_ONCE(*sptep);\n}\n#else\nunion split_spte {\n\tstruct {\n\t\tu32 spte_low;\n\t\tu32 spte_high;\n\t};\n\tu64 spte;\n};\n\nstatic void count_spte_clear(u64 *sptep, u64 spte)\n{\n\tstruct kvm_mmu_page *sp =  sptep_to_sp(sptep);\n\n\tif (is_shadow_present_pte(spte))\n\t\treturn;\n\n\t \n\tsmp_wmb();\n\tsp->clear_spte_count++;\n}\n\nstatic void __set_spte(u64 *sptep, u64 spte)\n{\n\tunion split_spte *ssptep, sspte;\n\n\tssptep = (union split_spte *)sptep;\n\tsspte = (union split_spte)spte;\n\n\tssptep->spte_high = sspte.spte_high;\n\n\t \n\tsmp_wmb();\n\n\tWRITE_ONCE(ssptep->spte_low, sspte.spte_low);\n}\n\nstatic void __update_clear_spte_fast(u64 *sptep, u64 spte)\n{\n\tunion split_spte *ssptep, sspte;\n\n\tssptep = (union split_spte *)sptep;\n\tsspte = (union split_spte)spte;\n\n\tWRITE_ONCE(ssptep->spte_low, sspte.spte_low);\n\n\t \n\tsmp_wmb();\n\n\tssptep->spte_high = sspte.spte_high;\n\tcount_spte_clear(sptep, spte);\n}\n\nstatic u64 __update_clear_spte_slow(u64 *sptep, u64 spte)\n{\n\tunion split_spte *ssptep, sspte, orig;\n\n\tssptep = (union split_spte *)sptep;\n\tsspte = (union split_spte)spte;\n\n\t \n\torig.spte_low = xchg(&ssptep->spte_low, sspte.spte_low);\n\torig.spte_high = ssptep->spte_high;\n\tssptep->spte_high = sspte.spte_high;\n\tcount_spte_clear(sptep, spte);\n\n\treturn orig.spte;\n}\n\n \nstatic u64 __get_spte_lockless(u64 *sptep)\n{\n\tstruct kvm_mmu_page *sp =  sptep_to_sp(sptep);\n\tunion split_spte spte, *orig = (union split_spte *)sptep;\n\tint count;\n\nretry:\n\tcount = sp->clear_spte_count;\n\tsmp_rmb();\n\n\tspte.spte_low = orig->spte_low;\n\tsmp_rmb();\n\n\tspte.spte_high = orig->spte_high;\n\tsmp_rmb();\n\n\tif (unlikely(spte.spte_low != orig->spte_low ||\n\t      count != sp->clear_spte_count))\n\t\tgoto retry;\n\n\treturn spte.spte;\n}\n#endif\n\n \nstatic void mmu_spte_set(u64 *sptep, u64 new_spte)\n{\n\tWARN_ON_ONCE(is_shadow_present_pte(*sptep));\n\t__set_spte(sptep, new_spte);\n}\n\n \nstatic u64 mmu_spte_update_no_track(u64 *sptep, u64 new_spte)\n{\n\tu64 old_spte = *sptep;\n\n\tWARN_ON_ONCE(!is_shadow_present_pte(new_spte));\n\tcheck_spte_writable_invariants(new_spte);\n\n\tif (!is_shadow_present_pte(old_spte)) {\n\t\tmmu_spte_set(sptep, new_spte);\n\t\treturn old_spte;\n\t}\n\n\tif (!spte_has_volatile_bits(old_spte))\n\t\t__update_clear_spte_fast(sptep, new_spte);\n\telse\n\t\told_spte = __update_clear_spte_slow(sptep, new_spte);\n\n\tWARN_ON_ONCE(spte_to_pfn(old_spte) != spte_to_pfn(new_spte));\n\n\treturn old_spte;\n}\n\n \nstatic bool mmu_spte_update(u64 *sptep, u64 new_spte)\n{\n\tbool flush = false;\n\tu64 old_spte = mmu_spte_update_no_track(sptep, new_spte);\n\n\tif (!is_shadow_present_pte(old_spte))\n\t\treturn false;\n\n\t \n\tif (is_mmu_writable_spte(old_spte) &&\n\t      !is_writable_pte(new_spte))\n\t\tflush = true;\n\n\t \n\n\tif (is_accessed_spte(old_spte) && !is_accessed_spte(new_spte)) {\n\t\tflush = true;\n\t\tkvm_set_pfn_accessed(spte_to_pfn(old_spte));\n\t}\n\n\tif (is_dirty_spte(old_spte) && !is_dirty_spte(new_spte)) {\n\t\tflush = true;\n\t\tkvm_set_pfn_dirty(spte_to_pfn(old_spte));\n\t}\n\n\treturn flush;\n}\n\n \nstatic u64 mmu_spte_clear_track_bits(struct kvm *kvm, u64 *sptep)\n{\n\tkvm_pfn_t pfn;\n\tu64 old_spte = *sptep;\n\tint level = sptep_to_sp(sptep)->role.level;\n\tstruct page *page;\n\n\tif (!is_shadow_present_pte(old_spte) ||\n\t    !spte_has_volatile_bits(old_spte))\n\t\t__update_clear_spte_fast(sptep, 0ull);\n\telse\n\t\told_spte = __update_clear_spte_slow(sptep, 0ull);\n\n\tif (!is_shadow_present_pte(old_spte))\n\t\treturn old_spte;\n\n\tkvm_update_page_stats(kvm, level, -1);\n\n\tpfn = spte_to_pfn(old_spte);\n\n\t \n\tpage = kvm_pfn_to_refcounted_page(pfn);\n\tWARN_ON_ONCE(page && !page_count(page));\n\n\tif (is_accessed_spte(old_spte))\n\t\tkvm_set_pfn_accessed(pfn);\n\n\tif (is_dirty_spte(old_spte))\n\t\tkvm_set_pfn_dirty(pfn);\n\n\treturn old_spte;\n}\n\n \nstatic void mmu_spte_clear_no_track(u64 *sptep)\n{\n\t__update_clear_spte_fast(sptep, 0ull);\n}\n\nstatic u64 mmu_spte_get_lockless(u64 *sptep)\n{\n\treturn __get_spte_lockless(sptep);\n}\n\n \nstatic bool mmu_spte_age(u64 *sptep)\n{\n\tu64 spte = mmu_spte_get_lockless(sptep);\n\n\tif (!is_accessed_spte(spte))\n\t\treturn false;\n\n\tif (spte_ad_enabled(spte)) {\n\t\tclear_bit((ffs(shadow_accessed_mask) - 1),\n\t\t\t  (unsigned long *)sptep);\n\t} else {\n\t\t \n\t\tif (is_writable_pte(spte))\n\t\t\tkvm_set_pfn_dirty(spte_to_pfn(spte));\n\n\t\tspte = mark_spte_for_access_track(spte);\n\t\tmmu_spte_update_no_track(sptep, spte);\n\t}\n\n\treturn true;\n}\n\nstatic inline bool is_tdp_mmu_active(struct kvm_vcpu *vcpu)\n{\n\treturn tdp_mmu_enabled && vcpu->arch.mmu->root_role.direct;\n}\n\nstatic void walk_shadow_page_lockless_begin(struct kvm_vcpu *vcpu)\n{\n\tif (is_tdp_mmu_active(vcpu)) {\n\t\tkvm_tdp_mmu_walk_lockless_begin();\n\t} else {\n\t\t \n\t\tlocal_irq_disable();\n\n\t\t \n\t\tsmp_store_mb(vcpu->mode, READING_SHADOW_PAGE_TABLES);\n\t}\n}\n\nstatic void walk_shadow_page_lockless_end(struct kvm_vcpu *vcpu)\n{\n\tif (is_tdp_mmu_active(vcpu)) {\n\t\tkvm_tdp_mmu_walk_lockless_end();\n\t} else {\n\t\t \n\t\tsmp_store_release(&vcpu->mode, OUTSIDE_GUEST_MODE);\n\t\tlocal_irq_enable();\n\t}\n}\n\nstatic int mmu_topup_memory_caches(struct kvm_vcpu *vcpu, bool maybe_indirect)\n{\n\tint r;\n\n\t \n\tr = kvm_mmu_topup_memory_cache(&vcpu->arch.mmu_pte_list_desc_cache,\n\t\t\t\t       1 + PT64_ROOT_MAX_LEVEL + PTE_PREFETCH_NUM);\n\tif (r)\n\t\treturn r;\n\tr = kvm_mmu_topup_memory_cache(&vcpu->arch.mmu_shadow_page_cache,\n\t\t\t\t       PT64_ROOT_MAX_LEVEL);\n\tif (r)\n\t\treturn r;\n\tif (maybe_indirect) {\n\t\tr = kvm_mmu_topup_memory_cache(&vcpu->arch.mmu_shadowed_info_cache,\n\t\t\t\t\t       PT64_ROOT_MAX_LEVEL);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\treturn kvm_mmu_topup_memory_cache(&vcpu->arch.mmu_page_header_cache,\n\t\t\t\t\t  PT64_ROOT_MAX_LEVEL);\n}\n\nstatic void mmu_free_memory_caches(struct kvm_vcpu *vcpu)\n{\n\tkvm_mmu_free_memory_cache(&vcpu->arch.mmu_pte_list_desc_cache);\n\tkvm_mmu_free_memory_cache(&vcpu->arch.mmu_shadow_page_cache);\n\tkvm_mmu_free_memory_cache(&vcpu->arch.mmu_shadowed_info_cache);\n\tkvm_mmu_free_memory_cache(&vcpu->arch.mmu_page_header_cache);\n}\n\nstatic void mmu_free_pte_list_desc(struct pte_list_desc *pte_list_desc)\n{\n\tkmem_cache_free(pte_list_desc_cache, pte_list_desc);\n}\n\nstatic bool sp_has_gptes(struct kvm_mmu_page *sp);\n\nstatic gfn_t kvm_mmu_page_get_gfn(struct kvm_mmu_page *sp, int index)\n{\n\tif (sp->role.passthrough)\n\t\treturn sp->gfn;\n\n\tif (!sp->role.direct)\n\t\treturn sp->shadowed_translation[index] >> PAGE_SHIFT;\n\n\treturn sp->gfn + (index << ((sp->role.level - 1) * SPTE_LEVEL_BITS));\n}\n\n \nstatic u32 kvm_mmu_page_get_access(struct kvm_mmu_page *sp, int index)\n{\n\tif (sp_has_gptes(sp))\n\t\treturn sp->shadowed_translation[index] & ACC_ALL;\n\n\t \n\treturn sp->role.access;\n}\n\nstatic void kvm_mmu_page_set_translation(struct kvm_mmu_page *sp, int index,\n\t\t\t\t\t gfn_t gfn, unsigned int access)\n{\n\tif (sp_has_gptes(sp)) {\n\t\tsp->shadowed_translation[index] = (gfn << PAGE_SHIFT) | access;\n\t\treturn;\n\t}\n\n\tWARN_ONCE(access != kvm_mmu_page_get_access(sp, index),\n\t          \"access mismatch under %s page %llx (expected %u, got %u)\\n\",\n\t          sp->role.passthrough ? \"passthrough\" : \"direct\",\n\t          sp->gfn, kvm_mmu_page_get_access(sp, index), access);\n\n\tWARN_ONCE(gfn != kvm_mmu_page_get_gfn(sp, index),\n\t          \"gfn mismatch under %s page %llx (expected %llx, got %llx)\\n\",\n\t          sp->role.passthrough ? \"passthrough\" : \"direct\",\n\t          sp->gfn, kvm_mmu_page_get_gfn(sp, index), gfn);\n}\n\nstatic void kvm_mmu_page_set_access(struct kvm_mmu_page *sp, int index,\n\t\t\t\t    unsigned int access)\n{\n\tgfn_t gfn = kvm_mmu_page_get_gfn(sp, index);\n\n\tkvm_mmu_page_set_translation(sp, index, gfn, access);\n}\n\n \nstatic struct kvm_lpage_info *lpage_info_slot(gfn_t gfn,\n\t\tconst struct kvm_memory_slot *slot, int level)\n{\n\tunsigned long idx;\n\n\tidx = gfn_to_index(gfn, slot->base_gfn, level);\n\treturn &slot->arch.lpage_info[level - 2][idx];\n}\n\nstatic void update_gfn_disallow_lpage_count(const struct kvm_memory_slot *slot,\n\t\t\t\t\t    gfn_t gfn, int count)\n{\n\tstruct kvm_lpage_info *linfo;\n\tint i;\n\n\tfor (i = PG_LEVEL_2M; i <= KVM_MAX_HUGEPAGE_LEVEL; ++i) {\n\t\tlinfo = lpage_info_slot(gfn, slot, i);\n\t\tlinfo->disallow_lpage += count;\n\t\tWARN_ON_ONCE(linfo->disallow_lpage < 0);\n\t}\n}\n\nvoid kvm_mmu_gfn_disallow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn)\n{\n\tupdate_gfn_disallow_lpage_count(slot, gfn, 1);\n}\n\nvoid kvm_mmu_gfn_allow_lpage(const struct kvm_memory_slot *slot, gfn_t gfn)\n{\n\tupdate_gfn_disallow_lpage_count(slot, gfn, -1);\n}\n\nstatic void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)\n{\n\tstruct kvm_memslots *slots;\n\tstruct kvm_memory_slot *slot;\n\tgfn_t gfn;\n\n\tkvm->arch.indirect_shadow_pages++;\n\tgfn = sp->gfn;\n\tslots = kvm_memslots_for_spte_role(kvm, sp->role);\n\tslot = __gfn_to_memslot(slots, gfn);\n\n\t \n\tif (sp->role.level > PG_LEVEL_4K)\n\t\treturn __kvm_write_track_add_gfn(kvm, slot, gfn);\n\n\tkvm_mmu_gfn_disallow_lpage(slot, gfn);\n\n\tif (kvm_mmu_slot_gfn_write_protect(kvm, slot, gfn, PG_LEVEL_4K))\n\t\tkvm_flush_remote_tlbs_gfn(kvm, gfn, PG_LEVEL_4K);\n}\n\nvoid track_possible_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)\n{\n\t \n\tif (!list_empty(&sp->possible_nx_huge_page_link))\n\t\treturn;\n\n\t++kvm->stat.nx_lpage_splits;\n\tlist_add_tail(&sp->possible_nx_huge_page_link,\n\t\t      &kvm->arch.possible_nx_huge_pages);\n}\n\nstatic void account_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp,\n\t\t\t\t bool nx_huge_page_possible)\n{\n\tsp->nx_huge_page_disallowed = true;\n\n\tif (nx_huge_page_possible)\n\t\ttrack_possible_nx_huge_page(kvm, sp);\n}\n\nstatic void unaccount_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)\n{\n\tstruct kvm_memslots *slots;\n\tstruct kvm_memory_slot *slot;\n\tgfn_t gfn;\n\n\tkvm->arch.indirect_shadow_pages--;\n\tgfn = sp->gfn;\n\tslots = kvm_memslots_for_spte_role(kvm, sp->role);\n\tslot = __gfn_to_memslot(slots, gfn);\n\tif (sp->role.level > PG_LEVEL_4K)\n\t\treturn __kvm_write_track_remove_gfn(kvm, slot, gfn);\n\n\tkvm_mmu_gfn_allow_lpage(slot, gfn);\n}\n\nvoid untrack_possible_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)\n{\n\tif (list_empty(&sp->possible_nx_huge_page_link))\n\t\treturn;\n\n\t--kvm->stat.nx_lpage_splits;\n\tlist_del_init(&sp->possible_nx_huge_page_link);\n}\n\nstatic void unaccount_nx_huge_page(struct kvm *kvm, struct kvm_mmu_page *sp)\n{\n\tsp->nx_huge_page_disallowed = false;\n\n\tuntrack_possible_nx_huge_page(kvm, sp);\n}\n\nstatic struct kvm_memory_slot *gfn_to_memslot_dirty_bitmap(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t\t   gfn_t gfn,\n\t\t\t\t\t\t\t   bool no_dirty_log)\n{\n\tstruct kvm_memory_slot *slot;\n\n\tslot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);\n\tif (!slot || slot->flags & KVM_MEMSLOT_INVALID)\n\t\treturn NULL;\n\tif (no_dirty_log && kvm_slot_dirty_track_enabled(slot))\n\t\treturn NULL;\n\n\treturn slot;\n}\n\n \n\n \nstatic int pte_list_add(struct kvm_mmu_memory_cache *cache, u64 *spte,\n\t\t\tstruct kvm_rmap_head *rmap_head)\n{\n\tstruct pte_list_desc *desc;\n\tint count = 0;\n\n\tif (!rmap_head->val) {\n\t\trmap_head->val = (unsigned long)spte;\n\t} else if (!(rmap_head->val & 1)) {\n\t\tdesc = kvm_mmu_memory_cache_alloc(cache);\n\t\tdesc->sptes[0] = (u64 *)rmap_head->val;\n\t\tdesc->sptes[1] = spte;\n\t\tdesc->spte_count = 2;\n\t\tdesc->tail_count = 0;\n\t\trmap_head->val = (unsigned long)desc | 1;\n\t\t++count;\n\t} else {\n\t\tdesc = (struct pte_list_desc *)(rmap_head->val & ~1ul);\n\t\tcount = desc->tail_count + desc->spte_count;\n\n\t\t \n\t\tif (desc->spte_count == PTE_LIST_EXT) {\n\t\t\tdesc = kvm_mmu_memory_cache_alloc(cache);\n\t\t\tdesc->more = (struct pte_list_desc *)(rmap_head->val & ~1ul);\n\t\t\tdesc->spte_count = 0;\n\t\t\tdesc->tail_count = count;\n\t\t\trmap_head->val = (unsigned long)desc | 1;\n\t\t}\n\t\tdesc->sptes[desc->spte_count++] = spte;\n\t}\n\treturn count;\n}\n\nstatic void pte_list_desc_remove_entry(struct kvm *kvm,\n\t\t\t\t       struct kvm_rmap_head *rmap_head,\n\t\t\t\t       struct pte_list_desc *desc, int i)\n{\n\tstruct pte_list_desc *head_desc = (struct pte_list_desc *)(rmap_head->val & ~1ul);\n\tint j = head_desc->spte_count - 1;\n\n\t \n\tKVM_BUG_ON_DATA_CORRUPTION(j < 0, kvm);\n\n\t \n\tdesc->sptes[i] = head_desc->sptes[j];\n\thead_desc->sptes[j] = NULL;\n\thead_desc->spte_count--;\n\tif (head_desc->spte_count)\n\t\treturn;\n\n\t \n\tif (!head_desc->more)\n\t\trmap_head->val = 0;\n\telse\n\t\trmap_head->val = (unsigned long)head_desc->more | 1;\n\tmmu_free_pte_list_desc(head_desc);\n}\n\nstatic void pte_list_remove(struct kvm *kvm, u64 *spte,\n\t\t\t    struct kvm_rmap_head *rmap_head)\n{\n\tstruct pte_list_desc *desc;\n\tint i;\n\n\tif (KVM_BUG_ON_DATA_CORRUPTION(!rmap_head->val, kvm))\n\t\treturn;\n\n\tif (!(rmap_head->val & 1)) {\n\t\tif (KVM_BUG_ON_DATA_CORRUPTION((u64 *)rmap_head->val != spte, kvm))\n\t\t\treturn;\n\n\t\trmap_head->val = 0;\n\t} else {\n\t\tdesc = (struct pte_list_desc *)(rmap_head->val & ~1ul);\n\t\twhile (desc) {\n\t\t\tfor (i = 0; i < desc->spte_count; ++i) {\n\t\t\t\tif (desc->sptes[i] == spte) {\n\t\t\t\t\tpte_list_desc_remove_entry(kvm, rmap_head,\n\t\t\t\t\t\t\t\t   desc, i);\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t}\n\t\t\tdesc = desc->more;\n\t\t}\n\n\t\tKVM_BUG_ON_DATA_CORRUPTION(true, kvm);\n\t}\n}\n\nstatic void kvm_zap_one_rmap_spte(struct kvm *kvm,\n\t\t\t\t  struct kvm_rmap_head *rmap_head, u64 *sptep)\n{\n\tmmu_spte_clear_track_bits(kvm, sptep);\n\tpte_list_remove(kvm, sptep, rmap_head);\n}\n\n \nstatic bool kvm_zap_all_rmap_sptes(struct kvm *kvm,\n\t\t\t\t   struct kvm_rmap_head *rmap_head)\n{\n\tstruct pte_list_desc *desc, *next;\n\tint i;\n\n\tif (!rmap_head->val)\n\t\treturn false;\n\n\tif (!(rmap_head->val & 1)) {\n\t\tmmu_spte_clear_track_bits(kvm, (u64 *)rmap_head->val);\n\t\tgoto out;\n\t}\n\n\tdesc = (struct pte_list_desc *)(rmap_head->val & ~1ul);\n\n\tfor (; desc; desc = next) {\n\t\tfor (i = 0; i < desc->spte_count; i++)\n\t\t\tmmu_spte_clear_track_bits(kvm, desc->sptes[i]);\n\t\tnext = desc->more;\n\t\tmmu_free_pte_list_desc(desc);\n\t}\nout:\n\t \n\trmap_head->val = 0;\n\treturn true;\n}\n\nunsigned int pte_list_count(struct kvm_rmap_head *rmap_head)\n{\n\tstruct pte_list_desc *desc;\n\n\tif (!rmap_head->val)\n\t\treturn 0;\n\telse if (!(rmap_head->val & 1))\n\t\treturn 1;\n\n\tdesc = (struct pte_list_desc *)(rmap_head->val & ~1ul);\n\treturn desc->tail_count + desc->spte_count;\n}\n\nstatic struct kvm_rmap_head *gfn_to_rmap(gfn_t gfn, int level,\n\t\t\t\t\t const struct kvm_memory_slot *slot)\n{\n\tunsigned long idx;\n\n\tidx = gfn_to_index(gfn, slot->base_gfn, level);\n\treturn &slot->arch.rmap[level - PG_LEVEL_4K][idx];\n}\n\nstatic void rmap_remove(struct kvm *kvm, u64 *spte)\n{\n\tstruct kvm_memslots *slots;\n\tstruct kvm_memory_slot *slot;\n\tstruct kvm_mmu_page *sp;\n\tgfn_t gfn;\n\tstruct kvm_rmap_head *rmap_head;\n\n\tsp = sptep_to_sp(spte);\n\tgfn = kvm_mmu_page_get_gfn(sp, spte_index(spte));\n\n\t \n\tslots = kvm_memslots_for_spte_role(kvm, sp->role);\n\n\tslot = __gfn_to_memslot(slots, gfn);\n\trmap_head = gfn_to_rmap(gfn, sp->role.level, slot);\n\n\tpte_list_remove(kvm, spte, rmap_head);\n}\n\n \nstruct rmap_iterator {\n\t \n\tstruct pte_list_desc *desc;\t \n\tint pos;\t\t\t \n};\n\n \nstatic u64 *rmap_get_first(struct kvm_rmap_head *rmap_head,\n\t\t\t   struct rmap_iterator *iter)\n{\n\tu64 *sptep;\n\n\tif (!rmap_head->val)\n\t\treturn NULL;\n\n\tif (!(rmap_head->val & 1)) {\n\t\titer->desc = NULL;\n\t\tsptep = (u64 *)rmap_head->val;\n\t\tgoto out;\n\t}\n\n\titer->desc = (struct pte_list_desc *)(rmap_head->val & ~1ul);\n\titer->pos = 0;\n\tsptep = iter->desc->sptes[iter->pos];\nout:\n\tBUG_ON(!is_shadow_present_pte(*sptep));\n\treturn sptep;\n}\n\n \nstatic u64 *rmap_get_next(struct rmap_iterator *iter)\n{\n\tu64 *sptep;\n\n\tif (iter->desc) {\n\t\tif (iter->pos < PTE_LIST_EXT - 1) {\n\t\t\t++iter->pos;\n\t\t\tsptep = iter->desc->sptes[iter->pos];\n\t\t\tif (sptep)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\titer->desc = iter->desc->more;\n\n\t\tif (iter->desc) {\n\t\t\titer->pos = 0;\n\t\t\t \n\t\t\tsptep = iter->desc->sptes[iter->pos];\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\treturn NULL;\nout:\n\tBUG_ON(!is_shadow_present_pte(*sptep));\n\treturn sptep;\n}\n\n#define for_each_rmap_spte(_rmap_head_, _iter_, _spte_)\t\t\t\\\n\tfor (_spte_ = rmap_get_first(_rmap_head_, _iter_);\t\t\\\n\t     _spte_; _spte_ = rmap_get_next(_iter_))\n\nstatic void drop_spte(struct kvm *kvm, u64 *sptep)\n{\n\tu64 old_spte = mmu_spte_clear_track_bits(kvm, sptep);\n\n\tif (is_shadow_present_pte(old_spte))\n\t\trmap_remove(kvm, sptep);\n}\n\nstatic void drop_large_spte(struct kvm *kvm, u64 *sptep, bool flush)\n{\n\tstruct kvm_mmu_page *sp;\n\n\tsp = sptep_to_sp(sptep);\n\tWARN_ON_ONCE(sp->role.level == PG_LEVEL_4K);\n\n\tdrop_spte(kvm, sptep);\n\n\tif (flush)\n\t\tkvm_flush_remote_tlbs_sptep(kvm, sptep);\n}\n\n \nstatic bool spte_write_protect(u64 *sptep, bool pt_protect)\n{\n\tu64 spte = *sptep;\n\n\tif (!is_writable_pte(spte) &&\n\t    !(pt_protect && is_mmu_writable_spte(spte)))\n\t\treturn false;\n\n\tif (pt_protect)\n\t\tspte &= ~shadow_mmu_writable_mask;\n\tspte = spte & ~PT_WRITABLE_MASK;\n\n\treturn mmu_spte_update(sptep, spte);\n}\n\nstatic bool rmap_write_protect(struct kvm_rmap_head *rmap_head,\n\t\t\t       bool pt_protect)\n{\n\tu64 *sptep;\n\tstruct rmap_iterator iter;\n\tbool flush = false;\n\n\tfor_each_rmap_spte(rmap_head, &iter, sptep)\n\t\tflush |= spte_write_protect(sptep, pt_protect);\n\n\treturn flush;\n}\n\nstatic bool spte_clear_dirty(u64 *sptep)\n{\n\tu64 spte = *sptep;\n\n\tKVM_MMU_WARN_ON(!spte_ad_enabled(spte));\n\tspte &= ~shadow_dirty_mask;\n\treturn mmu_spte_update(sptep, spte);\n}\n\nstatic bool spte_wrprot_for_clear_dirty(u64 *sptep)\n{\n\tbool was_writable = test_and_clear_bit(PT_WRITABLE_SHIFT,\n\t\t\t\t\t       (unsigned long *)sptep);\n\tif (was_writable && !spte_ad_enabled(*sptep))\n\t\tkvm_set_pfn_dirty(spte_to_pfn(*sptep));\n\n\treturn was_writable;\n}\n\n \nstatic bool __rmap_clear_dirty(struct kvm *kvm, struct kvm_rmap_head *rmap_head,\n\t\t\t       const struct kvm_memory_slot *slot)\n{\n\tu64 *sptep;\n\tstruct rmap_iterator iter;\n\tbool flush = false;\n\n\tfor_each_rmap_spte(rmap_head, &iter, sptep)\n\t\tif (spte_ad_need_write_protect(*sptep))\n\t\t\tflush |= spte_wrprot_for_clear_dirty(sptep);\n\t\telse\n\t\t\tflush |= spte_clear_dirty(sptep);\n\n\treturn flush;\n}\n\n \nstatic void kvm_mmu_write_protect_pt_masked(struct kvm *kvm,\n\t\t\t\t     struct kvm_memory_slot *slot,\n\t\t\t\t     gfn_t gfn_offset, unsigned long mask)\n{\n\tstruct kvm_rmap_head *rmap_head;\n\n\tif (tdp_mmu_enabled)\n\t\tkvm_tdp_mmu_clear_dirty_pt_masked(kvm, slot,\n\t\t\t\tslot->base_gfn + gfn_offset, mask, true);\n\n\tif (!kvm_memslots_have_rmaps(kvm))\n\t\treturn;\n\n\twhile (mask) {\n\t\trmap_head = gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),\n\t\t\t\t\tPG_LEVEL_4K, slot);\n\t\trmap_write_protect(rmap_head, false);\n\n\t\t \n\t\tmask &= mask - 1;\n\t}\n}\n\n \nstatic void kvm_mmu_clear_dirty_pt_masked(struct kvm *kvm,\n\t\t\t\t\t struct kvm_memory_slot *slot,\n\t\t\t\t\t gfn_t gfn_offset, unsigned long mask)\n{\n\tstruct kvm_rmap_head *rmap_head;\n\n\tif (tdp_mmu_enabled)\n\t\tkvm_tdp_mmu_clear_dirty_pt_masked(kvm, slot,\n\t\t\t\tslot->base_gfn + gfn_offset, mask, false);\n\n\tif (!kvm_memslots_have_rmaps(kvm))\n\t\treturn;\n\n\twhile (mask) {\n\t\trmap_head = gfn_to_rmap(slot->base_gfn + gfn_offset + __ffs(mask),\n\t\t\t\t\tPG_LEVEL_4K, slot);\n\t\t__rmap_clear_dirty(kvm, rmap_head, slot);\n\n\t\t \n\t\tmask &= mask - 1;\n\t}\n}\n\n \nvoid kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,\n\t\t\t\tstruct kvm_memory_slot *slot,\n\t\t\t\tgfn_t gfn_offset, unsigned long mask)\n{\n\t \n\tif (kvm_dirty_log_manual_protect_and_init_set(kvm)) {\n\t\tgfn_t start = slot->base_gfn + gfn_offset + __ffs(mask);\n\t\tgfn_t end = slot->base_gfn + gfn_offset + __fls(mask);\n\n\t\tif (READ_ONCE(eager_page_split))\n\t\t\tkvm_mmu_try_split_huge_pages(kvm, slot, start, end, PG_LEVEL_4K);\n\n\t\tkvm_mmu_slot_gfn_write_protect(kvm, slot, start, PG_LEVEL_2M);\n\n\t\t \n\t\tif (ALIGN(start << PAGE_SHIFT, PMD_SIZE) !=\n\t\t    ALIGN(end << PAGE_SHIFT, PMD_SIZE))\n\t\t\tkvm_mmu_slot_gfn_write_protect(kvm, slot, end,\n\t\t\t\t\t\t       PG_LEVEL_2M);\n\t}\n\n\t \n\tif (kvm_x86_ops.cpu_dirty_log_size)\n\t\tkvm_mmu_clear_dirty_pt_masked(kvm, slot, gfn_offset, mask);\n\telse\n\t\tkvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, mask);\n}\n\nint kvm_cpu_dirty_log_size(void)\n{\n\treturn kvm_x86_ops.cpu_dirty_log_size;\n}\n\nbool kvm_mmu_slot_gfn_write_protect(struct kvm *kvm,\n\t\t\t\t    struct kvm_memory_slot *slot, u64 gfn,\n\t\t\t\t    int min_level)\n{\n\tstruct kvm_rmap_head *rmap_head;\n\tint i;\n\tbool write_protected = false;\n\n\tif (kvm_memslots_have_rmaps(kvm)) {\n\t\tfor (i = min_level; i <= KVM_MAX_HUGEPAGE_LEVEL; ++i) {\n\t\t\trmap_head = gfn_to_rmap(gfn, i, slot);\n\t\t\twrite_protected |= rmap_write_protect(rmap_head, true);\n\t\t}\n\t}\n\n\tif (tdp_mmu_enabled)\n\t\twrite_protected |=\n\t\t\tkvm_tdp_mmu_write_protect_gfn(kvm, slot, gfn, min_level);\n\n\treturn write_protected;\n}\n\nstatic bool kvm_vcpu_write_protect_gfn(struct kvm_vcpu *vcpu, u64 gfn)\n{\n\tstruct kvm_memory_slot *slot;\n\n\tslot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);\n\treturn kvm_mmu_slot_gfn_write_protect(vcpu->kvm, slot, gfn, PG_LEVEL_4K);\n}\n\nstatic bool __kvm_zap_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,\n\t\t\t   const struct kvm_memory_slot *slot)\n{\n\treturn kvm_zap_all_rmap_sptes(kvm, rmap_head);\n}\n\nstatic bool kvm_zap_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,\n\t\t\t struct kvm_memory_slot *slot, gfn_t gfn, int level,\n\t\t\t pte_t unused)\n{\n\treturn __kvm_zap_rmap(kvm, rmap_head, slot);\n}\n\nstatic bool kvm_set_pte_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,\n\t\t\t     struct kvm_memory_slot *slot, gfn_t gfn, int level,\n\t\t\t     pte_t pte)\n{\n\tu64 *sptep;\n\tstruct rmap_iterator iter;\n\tbool need_flush = false;\n\tu64 new_spte;\n\tkvm_pfn_t new_pfn;\n\n\tWARN_ON_ONCE(pte_huge(pte));\n\tnew_pfn = pte_pfn(pte);\n\nrestart:\n\tfor_each_rmap_spte(rmap_head, &iter, sptep) {\n\t\tneed_flush = true;\n\n\t\tif (pte_write(pte)) {\n\t\t\tkvm_zap_one_rmap_spte(kvm, rmap_head, sptep);\n\t\t\tgoto restart;\n\t\t} else {\n\t\t\tnew_spte = kvm_mmu_changed_pte_notifier_make_spte(\n\t\t\t\t\t*sptep, new_pfn);\n\n\t\t\tmmu_spte_clear_track_bits(kvm, sptep);\n\t\t\tmmu_spte_set(sptep, new_spte);\n\t\t}\n\t}\n\n\tif (need_flush && kvm_available_flush_remote_tlbs_range()) {\n\t\tkvm_flush_remote_tlbs_gfn(kvm, gfn, level);\n\t\treturn false;\n\t}\n\n\treturn need_flush;\n}\n\nstruct slot_rmap_walk_iterator {\n\t \n\tconst struct kvm_memory_slot *slot;\n\tgfn_t start_gfn;\n\tgfn_t end_gfn;\n\tint start_level;\n\tint end_level;\n\n\t \n\tgfn_t gfn;\n\tstruct kvm_rmap_head *rmap;\n\tint level;\n\n\t \n\tstruct kvm_rmap_head *end_rmap;\n};\n\nstatic void rmap_walk_init_level(struct slot_rmap_walk_iterator *iterator,\n\t\t\t\t int level)\n{\n\titerator->level = level;\n\titerator->gfn = iterator->start_gfn;\n\titerator->rmap = gfn_to_rmap(iterator->gfn, level, iterator->slot);\n\titerator->end_rmap = gfn_to_rmap(iterator->end_gfn, level, iterator->slot);\n}\n\nstatic void slot_rmap_walk_init(struct slot_rmap_walk_iterator *iterator,\n\t\t\t\tconst struct kvm_memory_slot *slot,\n\t\t\t\tint start_level, int end_level,\n\t\t\t\tgfn_t start_gfn, gfn_t end_gfn)\n{\n\titerator->slot = slot;\n\titerator->start_level = start_level;\n\titerator->end_level = end_level;\n\titerator->start_gfn = start_gfn;\n\titerator->end_gfn = end_gfn;\n\n\trmap_walk_init_level(iterator, iterator->start_level);\n}\n\nstatic bool slot_rmap_walk_okay(struct slot_rmap_walk_iterator *iterator)\n{\n\treturn !!iterator->rmap;\n}\n\nstatic void slot_rmap_walk_next(struct slot_rmap_walk_iterator *iterator)\n{\n\twhile (++iterator->rmap <= iterator->end_rmap) {\n\t\titerator->gfn += (1UL << KVM_HPAGE_GFN_SHIFT(iterator->level));\n\n\t\tif (iterator->rmap->val)\n\t\t\treturn;\n\t}\n\n\tif (++iterator->level > iterator->end_level) {\n\t\titerator->rmap = NULL;\n\t\treturn;\n\t}\n\n\trmap_walk_init_level(iterator, iterator->level);\n}\n\n#define for_each_slot_rmap_range(_slot_, _start_level_, _end_level_,\t\\\n\t   _start_gfn, _end_gfn, _iter_)\t\t\t\t\\\n\tfor (slot_rmap_walk_init(_iter_, _slot_, _start_level_,\t\t\\\n\t\t\t\t _end_level_, _start_gfn, _end_gfn);\t\\\n\t     slot_rmap_walk_okay(_iter_);\t\t\t\t\\\n\t     slot_rmap_walk_next(_iter_))\n\ntypedef bool (*rmap_handler_t)(struct kvm *kvm, struct kvm_rmap_head *rmap_head,\n\t\t\t       struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t       int level, pte_t pte);\n\nstatic __always_inline bool kvm_handle_gfn_range(struct kvm *kvm,\n\t\t\t\t\t\t struct kvm_gfn_range *range,\n\t\t\t\t\t\t rmap_handler_t handler)\n{\n\tstruct slot_rmap_walk_iterator iterator;\n\tbool ret = false;\n\n\tfor_each_slot_rmap_range(range->slot, PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL,\n\t\t\t\t range->start, range->end - 1, &iterator)\n\t\tret |= handler(kvm, iterator.rmap, range->slot, iterator.gfn,\n\t\t\t       iterator.level, range->arg.pte);\n\n\treturn ret;\n}\n\nbool kvm_unmap_gfn_range(struct kvm *kvm, struct kvm_gfn_range *range)\n{\n\tbool flush = false;\n\n\tif (kvm_memslots_have_rmaps(kvm))\n\t\tflush = kvm_handle_gfn_range(kvm, range, kvm_zap_rmap);\n\n\tif (tdp_mmu_enabled)\n\t\tflush = kvm_tdp_mmu_unmap_gfn_range(kvm, range, flush);\n\n\tif (kvm_x86_ops.set_apic_access_page_addr &&\n\t    range->slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT)\n\t\tkvm_make_all_cpus_request(kvm, KVM_REQ_APIC_PAGE_RELOAD);\n\n\treturn flush;\n}\n\nbool kvm_set_spte_gfn(struct kvm *kvm, struct kvm_gfn_range *range)\n{\n\tbool flush = false;\n\n\tif (kvm_memslots_have_rmaps(kvm))\n\t\tflush = kvm_handle_gfn_range(kvm, range, kvm_set_pte_rmap);\n\n\tif (tdp_mmu_enabled)\n\t\tflush |= kvm_tdp_mmu_set_spte_gfn(kvm, range);\n\n\treturn flush;\n}\n\nstatic bool kvm_age_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,\n\t\t\t struct kvm_memory_slot *slot, gfn_t gfn, int level,\n\t\t\t pte_t unused)\n{\n\tu64 *sptep;\n\tstruct rmap_iterator iter;\n\tint young = 0;\n\n\tfor_each_rmap_spte(rmap_head, &iter, sptep)\n\t\tyoung |= mmu_spte_age(sptep);\n\n\treturn young;\n}\n\nstatic bool kvm_test_age_rmap(struct kvm *kvm, struct kvm_rmap_head *rmap_head,\n\t\t\t      struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t      int level, pte_t unused)\n{\n\tu64 *sptep;\n\tstruct rmap_iterator iter;\n\n\tfor_each_rmap_spte(rmap_head, &iter, sptep)\n\t\tif (is_accessed_spte(*sptep))\n\t\t\treturn true;\n\treturn false;\n}\n\n#define RMAP_RECYCLE_THRESHOLD 1000\n\nstatic void __rmap_add(struct kvm *kvm,\n\t\t       struct kvm_mmu_memory_cache *cache,\n\t\t       const struct kvm_memory_slot *slot,\n\t\t       u64 *spte, gfn_t gfn, unsigned int access)\n{\n\tstruct kvm_mmu_page *sp;\n\tstruct kvm_rmap_head *rmap_head;\n\tint rmap_count;\n\n\tsp = sptep_to_sp(spte);\n\tkvm_mmu_page_set_translation(sp, spte_index(spte), gfn, access);\n\tkvm_update_page_stats(kvm, sp->role.level, 1);\n\n\trmap_head = gfn_to_rmap(gfn, sp->role.level, slot);\n\trmap_count = pte_list_add(cache, spte, rmap_head);\n\n\tif (rmap_count > kvm->stat.max_mmu_rmap_size)\n\t\tkvm->stat.max_mmu_rmap_size = rmap_count;\n\tif (rmap_count > RMAP_RECYCLE_THRESHOLD) {\n\t\tkvm_zap_all_rmap_sptes(kvm, rmap_head);\n\t\tkvm_flush_remote_tlbs_gfn(kvm, gfn, sp->role.level);\n\t}\n}\n\nstatic void rmap_add(struct kvm_vcpu *vcpu, const struct kvm_memory_slot *slot,\n\t\t     u64 *spte, gfn_t gfn, unsigned int access)\n{\n\tstruct kvm_mmu_memory_cache *cache = &vcpu->arch.mmu_pte_list_desc_cache;\n\n\t__rmap_add(vcpu->kvm, cache, slot, spte, gfn, access);\n}\n\nbool kvm_age_gfn(struct kvm *kvm, struct kvm_gfn_range *range)\n{\n\tbool young = false;\n\n\tif (kvm_memslots_have_rmaps(kvm))\n\t\tyoung = kvm_handle_gfn_range(kvm, range, kvm_age_rmap);\n\n\tif (tdp_mmu_enabled)\n\t\tyoung |= kvm_tdp_mmu_age_gfn_range(kvm, range);\n\n\treturn young;\n}\n\nbool kvm_test_age_gfn(struct kvm *kvm, struct kvm_gfn_range *range)\n{\n\tbool young = false;\n\n\tif (kvm_memslots_have_rmaps(kvm))\n\t\tyoung = kvm_handle_gfn_range(kvm, range, kvm_test_age_rmap);\n\n\tif (tdp_mmu_enabled)\n\t\tyoung |= kvm_tdp_mmu_test_age_gfn(kvm, range);\n\n\treturn young;\n}\n\nstatic void kvm_mmu_check_sptes_at_free(struct kvm_mmu_page *sp)\n{\n#ifdef CONFIG_KVM_PROVE_MMU\n\tint i;\n\n\tfor (i = 0; i < SPTE_ENT_PER_PAGE; i++) {\n\t\tif (KVM_MMU_WARN_ON(is_shadow_present_pte(sp->spt[i])))\n\t\t\tpr_err_ratelimited(\"SPTE %llx (@ %p) for gfn %llx shadow-present at free\",\n\t\t\t\t\t   sp->spt[i], &sp->spt[i],\n\t\t\t\t\t   kvm_mmu_page_get_gfn(sp, i));\n\t}\n#endif\n}\n\n \nstatic inline void kvm_mod_used_mmu_pages(struct kvm *kvm, long nr)\n{\n\tkvm->arch.n_used_mmu_pages += nr;\n\tpercpu_counter_add(&kvm_total_used_mmu_pages, nr);\n}\n\nstatic void kvm_account_mmu_page(struct kvm *kvm, struct kvm_mmu_page *sp)\n{\n\tkvm_mod_used_mmu_pages(kvm, +1);\n\tkvm_account_pgtable_pages((void *)sp->spt, +1);\n}\n\nstatic void kvm_unaccount_mmu_page(struct kvm *kvm, struct kvm_mmu_page *sp)\n{\n\tkvm_mod_used_mmu_pages(kvm, -1);\n\tkvm_account_pgtable_pages((void *)sp->spt, -1);\n}\n\nstatic void kvm_mmu_free_shadow_page(struct kvm_mmu_page *sp)\n{\n\tkvm_mmu_check_sptes_at_free(sp);\n\n\thlist_del(&sp->hash_link);\n\tlist_del(&sp->link);\n\tfree_page((unsigned long)sp->spt);\n\tif (!sp->role.direct)\n\t\tfree_page((unsigned long)sp->shadowed_translation);\n\tkmem_cache_free(mmu_page_header_cache, sp);\n}\n\nstatic unsigned kvm_page_table_hashfn(gfn_t gfn)\n{\n\treturn hash_64(gfn, KVM_MMU_HASH_SHIFT);\n}\n\nstatic void mmu_page_add_parent_pte(struct kvm_mmu_memory_cache *cache,\n\t\t\t\t    struct kvm_mmu_page *sp, u64 *parent_pte)\n{\n\tif (!parent_pte)\n\t\treturn;\n\n\tpte_list_add(cache, parent_pte, &sp->parent_ptes);\n}\n\nstatic void mmu_page_remove_parent_pte(struct kvm *kvm, struct kvm_mmu_page *sp,\n\t\t\t\t       u64 *parent_pte)\n{\n\tpte_list_remove(kvm, parent_pte, &sp->parent_ptes);\n}\n\nstatic void drop_parent_pte(struct kvm *kvm, struct kvm_mmu_page *sp,\n\t\t\t    u64 *parent_pte)\n{\n\tmmu_page_remove_parent_pte(kvm, sp, parent_pte);\n\tmmu_spte_clear_no_track(parent_pte);\n}\n\nstatic void mark_unsync(u64 *spte);\nstatic void kvm_mmu_mark_parents_unsync(struct kvm_mmu_page *sp)\n{\n\tu64 *sptep;\n\tstruct rmap_iterator iter;\n\n\tfor_each_rmap_spte(&sp->parent_ptes, &iter, sptep) {\n\t\tmark_unsync(sptep);\n\t}\n}\n\nstatic void mark_unsync(u64 *spte)\n{\n\tstruct kvm_mmu_page *sp;\n\n\tsp = sptep_to_sp(spte);\n\tif (__test_and_set_bit(spte_index(spte), sp->unsync_child_bitmap))\n\t\treturn;\n\tif (sp->unsync_children++)\n\t\treturn;\n\tkvm_mmu_mark_parents_unsync(sp);\n}\n\n#define KVM_PAGE_ARRAY_NR 16\n\nstruct kvm_mmu_pages {\n\tstruct mmu_page_and_offset {\n\t\tstruct kvm_mmu_page *sp;\n\t\tunsigned int idx;\n\t} page[KVM_PAGE_ARRAY_NR];\n\tunsigned int nr;\n};\n\nstatic int mmu_pages_add(struct kvm_mmu_pages *pvec, struct kvm_mmu_page *sp,\n\t\t\t int idx)\n{\n\tint i;\n\n\tif (sp->unsync)\n\t\tfor (i=0; i < pvec->nr; i++)\n\t\t\tif (pvec->page[i].sp == sp)\n\t\t\t\treturn 0;\n\n\tpvec->page[pvec->nr].sp = sp;\n\tpvec->page[pvec->nr].idx = idx;\n\tpvec->nr++;\n\treturn (pvec->nr == KVM_PAGE_ARRAY_NR);\n}\n\nstatic inline void clear_unsync_child_bit(struct kvm_mmu_page *sp, int idx)\n{\n\t--sp->unsync_children;\n\tWARN_ON_ONCE((int)sp->unsync_children < 0);\n\t__clear_bit(idx, sp->unsync_child_bitmap);\n}\n\nstatic int __mmu_unsync_walk(struct kvm_mmu_page *sp,\n\t\t\t   struct kvm_mmu_pages *pvec)\n{\n\tint i, ret, nr_unsync_leaf = 0;\n\n\tfor_each_set_bit(i, sp->unsync_child_bitmap, 512) {\n\t\tstruct kvm_mmu_page *child;\n\t\tu64 ent = sp->spt[i];\n\n\t\tif (!is_shadow_present_pte(ent) || is_large_pte(ent)) {\n\t\t\tclear_unsync_child_bit(sp, i);\n\t\t\tcontinue;\n\t\t}\n\n\t\tchild = spte_to_child_sp(ent);\n\n\t\tif (child->unsync_children) {\n\t\t\tif (mmu_pages_add(pvec, child, i))\n\t\t\t\treturn -ENOSPC;\n\n\t\t\tret = __mmu_unsync_walk(child, pvec);\n\t\t\tif (!ret) {\n\t\t\t\tclear_unsync_child_bit(sp, i);\n\t\t\t\tcontinue;\n\t\t\t} else if (ret > 0) {\n\t\t\t\tnr_unsync_leaf += ret;\n\t\t\t} else\n\t\t\t\treturn ret;\n\t\t} else if (child->unsync) {\n\t\t\tnr_unsync_leaf++;\n\t\t\tif (mmu_pages_add(pvec, child, i))\n\t\t\t\treturn -ENOSPC;\n\t\t} else\n\t\t\tclear_unsync_child_bit(sp, i);\n\t}\n\n\treturn nr_unsync_leaf;\n}\n\n#define INVALID_INDEX (-1)\n\nstatic int mmu_unsync_walk(struct kvm_mmu_page *sp,\n\t\t\t   struct kvm_mmu_pages *pvec)\n{\n\tpvec->nr = 0;\n\tif (!sp->unsync_children)\n\t\treturn 0;\n\n\tmmu_pages_add(pvec, sp, INVALID_INDEX);\n\treturn __mmu_unsync_walk(sp, pvec);\n}\n\nstatic void kvm_unlink_unsync_page(struct kvm *kvm, struct kvm_mmu_page *sp)\n{\n\tWARN_ON_ONCE(!sp->unsync);\n\ttrace_kvm_mmu_sync_page(sp);\n\tsp->unsync = 0;\n\t--kvm->stat.mmu_unsync;\n}\n\nstatic bool kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,\n\t\t\t\t     struct list_head *invalid_list);\nstatic void kvm_mmu_commit_zap_page(struct kvm *kvm,\n\t\t\t\t    struct list_head *invalid_list);\n\nstatic bool sp_has_gptes(struct kvm_mmu_page *sp)\n{\n\tif (sp->role.direct)\n\t\treturn false;\n\n\tif (sp->role.passthrough)\n\t\treturn false;\n\n\treturn true;\n}\n\n#define for_each_valid_sp(_kvm, _sp, _list)\t\t\t\t\\\n\thlist_for_each_entry(_sp, _list, hash_link)\t\t\t\\\n\t\tif (is_obsolete_sp((_kvm), (_sp))) {\t\t\t\\\n\t\t} else\n\n#define for_each_gfn_valid_sp_with_gptes(_kvm, _sp, _gfn)\t\t\\\n\tfor_each_valid_sp(_kvm, _sp,\t\t\t\t\t\\\n\t  &(_kvm)->arch.mmu_page_hash[kvm_page_table_hashfn(_gfn)])\t\\\n\t\tif ((_sp)->gfn != (_gfn) || !sp_has_gptes(_sp)) {} else\n\nstatic bool kvm_sync_page_check(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)\n{\n\tunion kvm_mmu_page_role root_role = vcpu->arch.mmu->root_role;\n\n\t \n\tconst union kvm_mmu_page_role sync_role_ign = {\n\t\t.level = 0xf,\n\t\t.access = 0x7,\n\t\t.quadrant = 0x3,\n\t\t.passthrough = 0x1,\n\t};\n\n\t \n\tif (WARN_ON_ONCE(sp->role.direct || !vcpu->arch.mmu->sync_spte ||\n\t\t\t (sp->role.word ^ root_role.word) & ~sync_role_ign.word))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int kvm_sync_spte(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp, int i)\n{\n\tif (!sp->spt[i])\n\t\treturn 0;\n\n\treturn vcpu->arch.mmu->sync_spte(vcpu, sp, i);\n}\n\nstatic int __kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)\n{\n\tint flush = 0;\n\tint i;\n\n\tif (!kvm_sync_page_check(vcpu, sp))\n\t\treturn -1;\n\n\tfor (i = 0; i < SPTE_ENT_PER_PAGE; i++) {\n\t\tint ret = kvm_sync_spte(vcpu, sp, i);\n\n\t\tif (ret < -1)\n\t\t\treturn -1;\n\t\tflush |= ret;\n\t}\n\n\t \n\treturn flush;\n}\n\nstatic int kvm_sync_page(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp,\n\t\t\t struct list_head *invalid_list)\n{\n\tint ret = __kvm_sync_page(vcpu, sp);\n\n\tif (ret < 0)\n\t\tkvm_mmu_prepare_zap_page(vcpu->kvm, sp, invalid_list);\n\treturn ret;\n}\n\nstatic bool kvm_mmu_remote_flush_or_zap(struct kvm *kvm,\n\t\t\t\t\tstruct list_head *invalid_list,\n\t\t\t\t\tbool remote_flush)\n{\n\tif (!remote_flush && list_empty(invalid_list))\n\t\treturn false;\n\n\tif (!list_empty(invalid_list))\n\t\tkvm_mmu_commit_zap_page(kvm, invalid_list);\n\telse\n\t\tkvm_flush_remote_tlbs(kvm);\n\treturn true;\n}\n\nstatic bool is_obsolete_sp(struct kvm *kvm, struct kvm_mmu_page *sp)\n{\n\tif (sp->role.invalid)\n\t\treturn true;\n\n\t \n\treturn !is_tdp_mmu_page(sp) &&\n\t       unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);\n}\n\nstruct mmu_page_path {\n\tstruct kvm_mmu_page *parent[PT64_ROOT_MAX_LEVEL];\n\tunsigned int idx[PT64_ROOT_MAX_LEVEL];\n};\n\n#define for_each_sp(pvec, sp, parents, i)\t\t\t\\\n\t\tfor (i = mmu_pages_first(&pvec, &parents);\t\\\n\t\t\ti < pvec.nr && ({ sp = pvec.page[i].sp; 1;});\t\\\n\t\t\ti = mmu_pages_next(&pvec, &parents, i))\n\nstatic int mmu_pages_next(struct kvm_mmu_pages *pvec,\n\t\t\t  struct mmu_page_path *parents,\n\t\t\t  int i)\n{\n\tint n;\n\n\tfor (n = i+1; n < pvec->nr; n++) {\n\t\tstruct kvm_mmu_page *sp = pvec->page[n].sp;\n\t\tunsigned idx = pvec->page[n].idx;\n\t\tint level = sp->role.level;\n\n\t\tparents->idx[level-1] = idx;\n\t\tif (level == PG_LEVEL_4K)\n\t\t\tbreak;\n\n\t\tparents->parent[level-2] = sp;\n\t}\n\n\treturn n;\n}\n\nstatic int mmu_pages_first(struct kvm_mmu_pages *pvec,\n\t\t\t   struct mmu_page_path *parents)\n{\n\tstruct kvm_mmu_page *sp;\n\tint level;\n\n\tif (pvec->nr == 0)\n\t\treturn 0;\n\n\tWARN_ON_ONCE(pvec->page[0].idx != INVALID_INDEX);\n\n\tsp = pvec->page[0].sp;\n\tlevel = sp->role.level;\n\tWARN_ON_ONCE(level == PG_LEVEL_4K);\n\n\tparents->parent[level-2] = sp;\n\n\t \n\tparents->parent[level-1] = NULL;\n\treturn mmu_pages_next(pvec, parents, 0);\n}\n\nstatic void mmu_pages_clear_parents(struct mmu_page_path *parents)\n{\n\tstruct kvm_mmu_page *sp;\n\tunsigned int level = 0;\n\n\tdo {\n\t\tunsigned int idx = parents->idx[level];\n\t\tsp = parents->parent[level];\n\t\tif (!sp)\n\t\t\treturn;\n\n\t\tWARN_ON_ONCE(idx == INVALID_INDEX);\n\t\tclear_unsync_child_bit(sp, idx);\n\t\tlevel++;\n\t} while (!sp->unsync_children);\n}\n\nstatic int mmu_sync_children(struct kvm_vcpu *vcpu,\n\t\t\t     struct kvm_mmu_page *parent, bool can_yield)\n{\n\tint i;\n\tstruct kvm_mmu_page *sp;\n\tstruct mmu_page_path parents;\n\tstruct kvm_mmu_pages pages;\n\tLIST_HEAD(invalid_list);\n\tbool flush = false;\n\n\twhile (mmu_unsync_walk(parent, &pages)) {\n\t\tbool protected = false;\n\n\t\tfor_each_sp(pages, sp, parents, i)\n\t\t\tprotected |= kvm_vcpu_write_protect_gfn(vcpu, sp->gfn);\n\n\t\tif (protected) {\n\t\t\tkvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, true);\n\t\t\tflush = false;\n\t\t}\n\n\t\tfor_each_sp(pages, sp, parents, i) {\n\t\t\tkvm_unlink_unsync_page(vcpu->kvm, sp);\n\t\t\tflush |= kvm_sync_page(vcpu, sp, &invalid_list) > 0;\n\t\t\tmmu_pages_clear_parents(&parents);\n\t\t}\n\t\tif (need_resched() || rwlock_needbreak(&vcpu->kvm->mmu_lock)) {\n\t\t\tkvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, flush);\n\t\t\tif (!can_yield) {\n\t\t\t\tkvm_make_request(KVM_REQ_MMU_SYNC, vcpu);\n\t\t\t\treturn -EINTR;\n\t\t\t}\n\n\t\t\tcond_resched_rwlock_write(&vcpu->kvm->mmu_lock);\n\t\t\tflush = false;\n\t\t}\n\t}\n\n\tkvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, flush);\n\treturn 0;\n}\n\nstatic void __clear_sp_write_flooding_count(struct kvm_mmu_page *sp)\n{\n\tatomic_set(&sp->write_flooding_count,  0);\n}\n\nstatic void clear_sp_write_flooding_count(u64 *spte)\n{\n\t__clear_sp_write_flooding_count(sptep_to_sp(spte));\n}\n\n \nstatic struct kvm_mmu_page *kvm_mmu_find_shadow_page(struct kvm *kvm,\n\t\t\t\t\t\t     struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t     gfn_t gfn,\n\t\t\t\t\t\t     struct hlist_head *sp_list,\n\t\t\t\t\t\t     union kvm_mmu_page_role role)\n{\n\tstruct kvm_mmu_page *sp;\n\tint ret;\n\tint collisions = 0;\n\tLIST_HEAD(invalid_list);\n\n\tfor_each_valid_sp(kvm, sp, sp_list) {\n\t\tif (sp->gfn != gfn) {\n\t\t\tcollisions++;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (sp->role.word != role.word) {\n\t\t\t \n\t\t\tif (role.level > PG_LEVEL_4K && sp->unsync)\n\t\t\t\tkvm_mmu_prepare_zap_page(kvm, sp,\n\t\t\t\t\t\t\t &invalid_list);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (sp->role.direct)\n\t\t\tgoto out;\n\n\t\tif (sp->unsync) {\n\t\t\tif (KVM_BUG_ON(!vcpu, kvm))\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tret = kvm_sync_page(vcpu, sp, &invalid_list);\n\t\t\tif (ret < 0)\n\t\t\t\tbreak;\n\n\t\t\tWARN_ON_ONCE(!list_empty(&invalid_list));\n\t\t\tif (ret > 0)\n\t\t\t\tkvm_flush_remote_tlbs(kvm);\n\t\t}\n\n\t\t__clear_sp_write_flooding_count(sp);\n\n\t\tgoto out;\n\t}\n\n\tsp = NULL;\n\t++kvm->stat.mmu_cache_miss;\n\nout:\n\tkvm_mmu_commit_zap_page(kvm, &invalid_list);\n\n\tif (collisions > kvm->stat.max_mmu_page_hash_collisions)\n\t\tkvm->stat.max_mmu_page_hash_collisions = collisions;\n\treturn sp;\n}\n\n \nstruct shadow_page_caches {\n\tstruct kvm_mmu_memory_cache *page_header_cache;\n\tstruct kvm_mmu_memory_cache *shadow_page_cache;\n\tstruct kvm_mmu_memory_cache *shadowed_info_cache;\n};\n\nstatic struct kvm_mmu_page *kvm_mmu_alloc_shadow_page(struct kvm *kvm,\n\t\t\t\t\t\t      struct shadow_page_caches *caches,\n\t\t\t\t\t\t      gfn_t gfn,\n\t\t\t\t\t\t      struct hlist_head *sp_list,\n\t\t\t\t\t\t      union kvm_mmu_page_role role)\n{\n\tstruct kvm_mmu_page *sp;\n\n\tsp = kvm_mmu_memory_cache_alloc(caches->page_header_cache);\n\tsp->spt = kvm_mmu_memory_cache_alloc(caches->shadow_page_cache);\n\tif (!role.direct)\n\t\tsp->shadowed_translation = kvm_mmu_memory_cache_alloc(caches->shadowed_info_cache);\n\n\tset_page_private(virt_to_page(sp->spt), (unsigned long)sp);\n\n\tINIT_LIST_HEAD(&sp->possible_nx_huge_page_link);\n\n\t \n\tsp->mmu_valid_gen = kvm->arch.mmu_valid_gen;\n\tlist_add(&sp->link, &kvm->arch.active_mmu_pages);\n\tkvm_account_mmu_page(kvm, sp);\n\n\tsp->gfn = gfn;\n\tsp->role = role;\n\thlist_add_head(&sp->hash_link, sp_list);\n\tif (sp_has_gptes(sp))\n\t\taccount_shadowed(kvm, sp);\n\n\treturn sp;\n}\n\n \nstatic struct kvm_mmu_page *__kvm_mmu_get_shadow_page(struct kvm *kvm,\n\t\t\t\t\t\t      struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t      struct shadow_page_caches *caches,\n\t\t\t\t\t\t      gfn_t gfn,\n\t\t\t\t\t\t      union kvm_mmu_page_role role)\n{\n\tstruct hlist_head *sp_list;\n\tstruct kvm_mmu_page *sp;\n\tbool created = false;\n\n\tsp_list = &kvm->arch.mmu_page_hash[kvm_page_table_hashfn(gfn)];\n\n\tsp = kvm_mmu_find_shadow_page(kvm, vcpu, gfn, sp_list, role);\n\tif (!sp) {\n\t\tcreated = true;\n\t\tsp = kvm_mmu_alloc_shadow_page(kvm, caches, gfn, sp_list, role);\n\t}\n\n\ttrace_kvm_mmu_get_page(sp, created);\n\treturn sp;\n}\n\nstatic struct kvm_mmu_page *kvm_mmu_get_shadow_page(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t    gfn_t gfn,\n\t\t\t\t\t\t    union kvm_mmu_page_role role)\n{\n\tstruct shadow_page_caches caches = {\n\t\t.page_header_cache = &vcpu->arch.mmu_page_header_cache,\n\t\t.shadow_page_cache = &vcpu->arch.mmu_shadow_page_cache,\n\t\t.shadowed_info_cache = &vcpu->arch.mmu_shadowed_info_cache,\n\t};\n\n\treturn __kvm_mmu_get_shadow_page(vcpu->kvm, vcpu, &caches, gfn, role);\n}\n\nstatic union kvm_mmu_page_role kvm_mmu_child_role(u64 *sptep, bool direct,\n\t\t\t\t\t\t  unsigned int access)\n{\n\tstruct kvm_mmu_page *parent_sp = sptep_to_sp(sptep);\n\tunion kvm_mmu_page_role role;\n\n\trole = parent_sp->role;\n\trole.level--;\n\trole.access = access;\n\trole.direct = direct;\n\trole.passthrough = 0;\n\n\t \n\tif (role.has_4_byte_gpte) {\n\t\tWARN_ON_ONCE(role.level != PG_LEVEL_4K);\n\t\trole.quadrant = spte_index(sptep) & 1;\n\t}\n\n\treturn role;\n}\n\nstatic struct kvm_mmu_page *kvm_mmu_get_child_sp(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t u64 *sptep, gfn_t gfn,\n\t\t\t\t\t\t bool direct, unsigned int access)\n{\n\tunion kvm_mmu_page_role role;\n\n\tif (is_shadow_present_pte(*sptep) && !is_large_pte(*sptep))\n\t\treturn ERR_PTR(-EEXIST);\n\n\trole = kvm_mmu_child_role(sptep, direct, access);\n\treturn kvm_mmu_get_shadow_page(vcpu, gfn, role);\n}\n\nstatic void shadow_walk_init_using_root(struct kvm_shadow_walk_iterator *iterator,\n\t\t\t\t\tstruct kvm_vcpu *vcpu, hpa_t root,\n\t\t\t\t\tu64 addr)\n{\n\titerator->addr = addr;\n\titerator->shadow_addr = root;\n\titerator->level = vcpu->arch.mmu->root_role.level;\n\n\tif (iterator->level >= PT64_ROOT_4LEVEL &&\n\t    vcpu->arch.mmu->cpu_role.base.level < PT64_ROOT_4LEVEL &&\n\t    !vcpu->arch.mmu->root_role.direct)\n\t\titerator->level = PT32E_ROOT_LEVEL;\n\n\tif (iterator->level == PT32E_ROOT_LEVEL) {\n\t\t \n\t\tBUG_ON(root != vcpu->arch.mmu->root.hpa);\n\n\t\titerator->shadow_addr\n\t\t\t= vcpu->arch.mmu->pae_root[(addr >> 30) & 3];\n\t\titerator->shadow_addr &= SPTE_BASE_ADDR_MASK;\n\t\t--iterator->level;\n\t\tif (!iterator->shadow_addr)\n\t\t\titerator->level = 0;\n\t}\n}\n\nstatic void shadow_walk_init(struct kvm_shadow_walk_iterator *iterator,\n\t\t\t     struct kvm_vcpu *vcpu, u64 addr)\n{\n\tshadow_walk_init_using_root(iterator, vcpu, vcpu->arch.mmu->root.hpa,\n\t\t\t\t    addr);\n}\n\nstatic bool shadow_walk_okay(struct kvm_shadow_walk_iterator *iterator)\n{\n\tif (iterator->level < PG_LEVEL_4K)\n\t\treturn false;\n\n\titerator->index = SPTE_INDEX(iterator->addr, iterator->level);\n\titerator->sptep\t= ((u64 *)__va(iterator->shadow_addr)) + iterator->index;\n\treturn true;\n}\n\nstatic void __shadow_walk_next(struct kvm_shadow_walk_iterator *iterator,\n\t\t\t       u64 spte)\n{\n\tif (!is_shadow_present_pte(spte) || is_last_spte(spte, iterator->level)) {\n\t\titerator->level = 0;\n\t\treturn;\n\t}\n\n\titerator->shadow_addr = spte & SPTE_BASE_ADDR_MASK;\n\t--iterator->level;\n}\n\nstatic void shadow_walk_next(struct kvm_shadow_walk_iterator *iterator)\n{\n\t__shadow_walk_next(iterator, *iterator->sptep);\n}\n\nstatic void __link_shadow_page(struct kvm *kvm,\n\t\t\t       struct kvm_mmu_memory_cache *cache, u64 *sptep,\n\t\t\t       struct kvm_mmu_page *sp, bool flush)\n{\n\tu64 spte;\n\n\tBUILD_BUG_ON(VMX_EPT_WRITABLE_MASK != PT_WRITABLE_MASK);\n\n\t \n\tif (is_shadow_present_pte(*sptep))\n\t\tdrop_large_spte(kvm, sptep, flush);\n\n\tspte = make_nonleaf_spte(sp->spt, sp_ad_disabled(sp));\n\n\tmmu_spte_set(sptep, spte);\n\n\tmmu_page_add_parent_pte(cache, sp, sptep);\n\n\t \n\tif (WARN_ON_ONCE(sp->unsync_children) || sp->unsync)\n\t\tmark_unsync(sptep);\n}\n\nstatic void link_shadow_page(struct kvm_vcpu *vcpu, u64 *sptep,\n\t\t\t     struct kvm_mmu_page *sp)\n{\n\t__link_shadow_page(vcpu->kvm, &vcpu->arch.mmu_pte_list_desc_cache, sptep, sp, true);\n}\n\nstatic void validate_direct_spte(struct kvm_vcpu *vcpu, u64 *sptep,\n\t\t\t\t   unsigned direct_access)\n{\n\tif (is_shadow_present_pte(*sptep) && !is_large_pte(*sptep)) {\n\t\tstruct kvm_mmu_page *child;\n\n\t\t \n\t\tchild = spte_to_child_sp(*sptep);\n\t\tif (child->role.access == direct_access)\n\t\t\treturn;\n\n\t\tdrop_parent_pte(vcpu->kvm, child, sptep);\n\t\tkvm_flush_remote_tlbs_sptep(vcpu->kvm, sptep);\n\t}\n}\n\n \nstatic int mmu_page_zap_pte(struct kvm *kvm, struct kvm_mmu_page *sp,\n\t\t\t    u64 *spte, struct list_head *invalid_list)\n{\n\tu64 pte;\n\tstruct kvm_mmu_page *child;\n\n\tpte = *spte;\n\tif (is_shadow_present_pte(pte)) {\n\t\tif (is_last_spte(pte, sp->role.level)) {\n\t\t\tdrop_spte(kvm, spte);\n\t\t} else {\n\t\t\tchild = spte_to_child_sp(pte);\n\t\t\tdrop_parent_pte(kvm, child, spte);\n\n\t\t\t \n\t\t\tif (tdp_enabled && invalid_list &&\n\t\t\t    child->role.guest_mode && !child->parent_ptes.val)\n\t\t\t\treturn kvm_mmu_prepare_zap_page(kvm, child,\n\t\t\t\t\t\t\t\tinvalid_list);\n\t\t}\n\t} else if (is_mmio_spte(pte)) {\n\t\tmmu_spte_clear_no_track(spte);\n\t}\n\treturn 0;\n}\n\nstatic int kvm_mmu_page_unlink_children(struct kvm *kvm,\n\t\t\t\t\tstruct kvm_mmu_page *sp,\n\t\t\t\t\tstruct list_head *invalid_list)\n{\n\tint zapped = 0;\n\tunsigned i;\n\n\tfor (i = 0; i < SPTE_ENT_PER_PAGE; ++i)\n\t\tzapped += mmu_page_zap_pte(kvm, sp, sp->spt + i, invalid_list);\n\n\treturn zapped;\n}\n\nstatic void kvm_mmu_unlink_parents(struct kvm *kvm, struct kvm_mmu_page *sp)\n{\n\tu64 *sptep;\n\tstruct rmap_iterator iter;\n\n\twhile ((sptep = rmap_get_first(&sp->parent_ptes, &iter)))\n\t\tdrop_parent_pte(kvm, sp, sptep);\n}\n\nstatic int mmu_zap_unsync_children(struct kvm *kvm,\n\t\t\t\t   struct kvm_mmu_page *parent,\n\t\t\t\t   struct list_head *invalid_list)\n{\n\tint i, zapped = 0;\n\tstruct mmu_page_path parents;\n\tstruct kvm_mmu_pages pages;\n\n\tif (parent->role.level == PG_LEVEL_4K)\n\t\treturn 0;\n\n\twhile (mmu_unsync_walk(parent, &pages)) {\n\t\tstruct kvm_mmu_page *sp;\n\n\t\tfor_each_sp(pages, sp, parents, i) {\n\t\t\tkvm_mmu_prepare_zap_page(kvm, sp, invalid_list);\n\t\t\tmmu_pages_clear_parents(&parents);\n\t\t\tzapped++;\n\t\t}\n\t}\n\n\treturn zapped;\n}\n\nstatic bool __kvm_mmu_prepare_zap_page(struct kvm *kvm,\n\t\t\t\t       struct kvm_mmu_page *sp,\n\t\t\t\t       struct list_head *invalid_list,\n\t\t\t\t       int *nr_zapped)\n{\n\tbool list_unstable, zapped_root = false;\n\n\tlockdep_assert_held_write(&kvm->mmu_lock);\n\ttrace_kvm_mmu_prepare_zap_page(sp);\n\t++kvm->stat.mmu_shadow_zapped;\n\t*nr_zapped = mmu_zap_unsync_children(kvm, sp, invalid_list);\n\t*nr_zapped += kvm_mmu_page_unlink_children(kvm, sp, invalid_list);\n\tkvm_mmu_unlink_parents(kvm, sp);\n\n\t \n\tlist_unstable = *nr_zapped;\n\n\tif (!sp->role.invalid && sp_has_gptes(sp))\n\t\tunaccount_shadowed(kvm, sp);\n\n\tif (sp->unsync)\n\t\tkvm_unlink_unsync_page(kvm, sp);\n\tif (!sp->root_count) {\n\t\t \n\t\t(*nr_zapped)++;\n\n\t\t \n\t\tif (sp->role.invalid)\n\t\t\tlist_add(&sp->link, invalid_list);\n\t\telse\n\t\t\tlist_move(&sp->link, invalid_list);\n\t\tkvm_unaccount_mmu_page(kvm, sp);\n\t} else {\n\t\t \n\t\tlist_del(&sp->link);\n\n\t\t \n\t\tzapped_root = !is_obsolete_sp(kvm, sp);\n\t}\n\n\tif (sp->nx_huge_page_disallowed)\n\t\tunaccount_nx_huge_page(kvm, sp);\n\n\tsp->role.invalid = 1;\n\n\t \n\tif (zapped_root)\n\t\tkvm_make_all_cpus_request(kvm, KVM_REQ_MMU_FREE_OBSOLETE_ROOTS);\n\treturn list_unstable;\n}\n\nstatic bool kvm_mmu_prepare_zap_page(struct kvm *kvm, struct kvm_mmu_page *sp,\n\t\t\t\t     struct list_head *invalid_list)\n{\n\tint nr_zapped;\n\n\t__kvm_mmu_prepare_zap_page(kvm, sp, invalid_list, &nr_zapped);\n\treturn nr_zapped;\n}\n\nstatic void kvm_mmu_commit_zap_page(struct kvm *kvm,\n\t\t\t\t    struct list_head *invalid_list)\n{\n\tstruct kvm_mmu_page *sp, *nsp;\n\n\tif (list_empty(invalid_list))\n\t\treturn;\n\n\t \n\tkvm_flush_remote_tlbs(kvm);\n\n\tlist_for_each_entry_safe(sp, nsp, invalid_list, link) {\n\t\tWARN_ON_ONCE(!sp->role.invalid || sp->root_count);\n\t\tkvm_mmu_free_shadow_page(sp);\n\t}\n}\n\nstatic unsigned long kvm_mmu_zap_oldest_mmu_pages(struct kvm *kvm,\n\t\t\t\t\t\t  unsigned long nr_to_zap)\n{\n\tunsigned long total_zapped = 0;\n\tstruct kvm_mmu_page *sp, *tmp;\n\tLIST_HEAD(invalid_list);\n\tbool unstable;\n\tint nr_zapped;\n\n\tif (list_empty(&kvm->arch.active_mmu_pages))\n\t\treturn 0;\n\nrestart:\n\tlist_for_each_entry_safe_reverse(sp, tmp, &kvm->arch.active_mmu_pages, link) {\n\t\t \n\t\tif (sp->root_count)\n\t\t\tcontinue;\n\n\t\tunstable = __kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list,\n\t\t\t\t\t\t      &nr_zapped);\n\t\ttotal_zapped += nr_zapped;\n\t\tif (total_zapped >= nr_to_zap)\n\t\t\tbreak;\n\n\t\tif (unstable)\n\t\t\tgoto restart;\n\t}\n\n\tkvm_mmu_commit_zap_page(kvm, &invalid_list);\n\n\tkvm->stat.mmu_recycled += total_zapped;\n\treturn total_zapped;\n}\n\nstatic inline unsigned long kvm_mmu_available_pages(struct kvm *kvm)\n{\n\tif (kvm->arch.n_max_mmu_pages > kvm->arch.n_used_mmu_pages)\n\t\treturn kvm->arch.n_max_mmu_pages -\n\t\t\tkvm->arch.n_used_mmu_pages;\n\n\treturn 0;\n}\n\nstatic int make_mmu_pages_available(struct kvm_vcpu *vcpu)\n{\n\tunsigned long avail = kvm_mmu_available_pages(vcpu->kvm);\n\n\tif (likely(avail >= KVM_MIN_FREE_MMU_PAGES))\n\t\treturn 0;\n\n\tkvm_mmu_zap_oldest_mmu_pages(vcpu->kvm, KVM_REFILL_PAGES - avail);\n\n\t \n\tif (!kvm_mmu_available_pages(vcpu->kvm))\n\t\treturn -ENOSPC;\n\treturn 0;\n}\n\n \nvoid kvm_mmu_change_mmu_pages(struct kvm *kvm, unsigned long goal_nr_mmu_pages)\n{\n\twrite_lock(&kvm->mmu_lock);\n\n\tif (kvm->arch.n_used_mmu_pages > goal_nr_mmu_pages) {\n\t\tkvm_mmu_zap_oldest_mmu_pages(kvm, kvm->arch.n_used_mmu_pages -\n\t\t\t\t\t\t  goal_nr_mmu_pages);\n\n\t\tgoal_nr_mmu_pages = kvm->arch.n_used_mmu_pages;\n\t}\n\n\tkvm->arch.n_max_mmu_pages = goal_nr_mmu_pages;\n\n\twrite_unlock(&kvm->mmu_lock);\n}\n\nint kvm_mmu_unprotect_page(struct kvm *kvm, gfn_t gfn)\n{\n\tstruct kvm_mmu_page *sp;\n\tLIST_HEAD(invalid_list);\n\tint r;\n\n\tr = 0;\n\twrite_lock(&kvm->mmu_lock);\n\tfor_each_gfn_valid_sp_with_gptes(kvm, sp, gfn) {\n\t\tr = 1;\n\t\tkvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);\n\t}\n\tkvm_mmu_commit_zap_page(kvm, &invalid_list);\n\twrite_unlock(&kvm->mmu_lock);\n\n\treturn r;\n}\n\nstatic int kvm_mmu_unprotect_page_virt(struct kvm_vcpu *vcpu, gva_t gva)\n{\n\tgpa_t gpa;\n\tint r;\n\n\tif (vcpu->arch.mmu->root_role.direct)\n\t\treturn 0;\n\n\tgpa = kvm_mmu_gva_to_gpa_read(vcpu, gva, NULL);\n\n\tr = kvm_mmu_unprotect_page(vcpu->kvm, gpa >> PAGE_SHIFT);\n\n\treturn r;\n}\n\nstatic void kvm_unsync_page(struct kvm *kvm, struct kvm_mmu_page *sp)\n{\n\ttrace_kvm_mmu_unsync_page(sp);\n\t++kvm->stat.mmu_unsync;\n\tsp->unsync = 1;\n\n\tkvm_mmu_mark_parents_unsync(sp);\n}\n\n \nint mmu_try_to_unsync_pages(struct kvm *kvm, const struct kvm_memory_slot *slot,\n\t\t\t    gfn_t gfn, bool can_unsync, bool prefetch)\n{\n\tstruct kvm_mmu_page *sp;\n\tbool locked = false;\n\n\t \n\tif (kvm_gfn_is_write_tracked(kvm, slot, gfn))\n\t\treturn -EPERM;\n\n\t \n\tfor_each_gfn_valid_sp_with_gptes(kvm, sp, gfn) {\n\t\tif (!can_unsync)\n\t\t\treturn -EPERM;\n\n\t\tif (sp->unsync)\n\t\t\tcontinue;\n\n\t\tif (prefetch)\n\t\t\treturn -EEXIST;\n\n\t\t \n\t\tif (!locked) {\n\t\t\tlocked = true;\n\t\t\tspin_lock(&kvm->arch.mmu_unsync_pages_lock);\n\n\t\t\t \n\t\t\tif (READ_ONCE(sp->unsync))\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tWARN_ON_ONCE(sp->role.level != PG_LEVEL_4K);\n\t\tkvm_unsync_page(kvm, sp);\n\t}\n\tif (locked)\n\t\tspin_unlock(&kvm->arch.mmu_unsync_pages_lock);\n\n\t \n\tsmp_wmb();\n\n\treturn 0;\n}\n\nstatic int mmu_set_spte(struct kvm_vcpu *vcpu, struct kvm_memory_slot *slot,\n\t\t\tu64 *sptep, unsigned int pte_access, gfn_t gfn,\n\t\t\tkvm_pfn_t pfn, struct kvm_page_fault *fault)\n{\n\tstruct kvm_mmu_page *sp = sptep_to_sp(sptep);\n\tint level = sp->role.level;\n\tint was_rmapped = 0;\n\tint ret = RET_PF_FIXED;\n\tbool flush = false;\n\tbool wrprot;\n\tu64 spte;\n\n\t \n\tbool host_writable = !fault || fault->map_writable;\n\tbool prefetch = !fault || fault->prefetch;\n\tbool write_fault = fault && fault->write;\n\n\tif (unlikely(is_noslot_pfn(pfn))) {\n\t\tvcpu->stat.pf_mmio_spte_created++;\n\t\tmark_mmio_spte(vcpu, sptep, gfn, pte_access);\n\t\treturn RET_PF_EMULATE;\n\t}\n\n\tif (is_shadow_present_pte(*sptep)) {\n\t\t \n\t\tif (level > PG_LEVEL_4K && !is_large_pte(*sptep)) {\n\t\t\tstruct kvm_mmu_page *child;\n\t\t\tu64 pte = *sptep;\n\n\t\t\tchild = spte_to_child_sp(pte);\n\t\t\tdrop_parent_pte(vcpu->kvm, child, sptep);\n\t\t\tflush = true;\n\t\t} else if (pfn != spte_to_pfn(*sptep)) {\n\t\t\tdrop_spte(vcpu->kvm, sptep);\n\t\t\tflush = true;\n\t\t} else\n\t\t\twas_rmapped = 1;\n\t}\n\n\twrprot = make_spte(vcpu, sp, slot, pte_access, gfn, pfn, *sptep, prefetch,\n\t\t\t   true, host_writable, &spte);\n\n\tif (*sptep == spte) {\n\t\tret = RET_PF_SPURIOUS;\n\t} else {\n\t\tflush |= mmu_spte_update(sptep, spte);\n\t\ttrace_kvm_mmu_set_spte(level, gfn, sptep);\n\t}\n\n\tif (wrprot) {\n\t\tif (write_fault)\n\t\t\tret = RET_PF_EMULATE;\n\t}\n\n\tif (flush)\n\t\tkvm_flush_remote_tlbs_gfn(vcpu->kvm, gfn, level);\n\n\tif (!was_rmapped) {\n\t\tWARN_ON_ONCE(ret == RET_PF_SPURIOUS);\n\t\trmap_add(vcpu, slot, sptep, gfn, pte_access);\n\t} else {\n\t\t \n\t\tkvm_mmu_page_set_access(sp, spte_index(sptep), pte_access);\n\t}\n\n\treturn ret;\n}\n\nstatic int direct_pte_prefetch_many(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_mmu_page *sp,\n\t\t\t\t    u64 *start, u64 *end)\n{\n\tstruct page *pages[PTE_PREFETCH_NUM];\n\tstruct kvm_memory_slot *slot;\n\tunsigned int access = sp->role.access;\n\tint i, ret;\n\tgfn_t gfn;\n\n\tgfn = kvm_mmu_page_get_gfn(sp, spte_index(start));\n\tslot = gfn_to_memslot_dirty_bitmap(vcpu, gfn, access & ACC_WRITE_MASK);\n\tif (!slot)\n\t\treturn -1;\n\n\tret = gfn_to_page_many_atomic(slot, gfn, pages, end - start);\n\tif (ret <= 0)\n\t\treturn -1;\n\n\tfor (i = 0; i < ret; i++, gfn++, start++) {\n\t\tmmu_set_spte(vcpu, slot, start, access, gfn,\n\t\t\t     page_to_pfn(pages[i]), NULL);\n\t\tput_page(pages[i]);\n\t}\n\n\treturn 0;\n}\n\nstatic void __direct_pte_prefetch(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_mmu_page *sp, u64 *sptep)\n{\n\tu64 *spte, *start = NULL;\n\tint i;\n\n\tWARN_ON_ONCE(!sp->role.direct);\n\n\ti = spte_index(sptep) & ~(PTE_PREFETCH_NUM - 1);\n\tspte = sp->spt + i;\n\n\tfor (i = 0; i < PTE_PREFETCH_NUM; i++, spte++) {\n\t\tif (is_shadow_present_pte(*spte) || spte == sptep) {\n\t\t\tif (!start)\n\t\t\t\tcontinue;\n\t\t\tif (direct_pte_prefetch_many(vcpu, sp, start, spte) < 0)\n\t\t\t\treturn;\n\t\t\tstart = NULL;\n\t\t} else if (!start)\n\t\t\tstart = spte;\n\t}\n\tif (start)\n\t\tdirect_pte_prefetch_many(vcpu, sp, start, spte);\n}\n\nstatic void direct_pte_prefetch(struct kvm_vcpu *vcpu, u64 *sptep)\n{\n\tstruct kvm_mmu_page *sp;\n\n\tsp = sptep_to_sp(sptep);\n\n\t \n\tif (sp_ad_disabled(sp))\n\t\treturn;\n\n\tif (sp->role.level > PG_LEVEL_4K)\n\t\treturn;\n\n\t \n\tif (unlikely(vcpu->kvm->mmu_invalidate_in_progress))\n\t\treturn;\n\n\t__direct_pte_prefetch(vcpu, sp, sptep);\n}\n\n \nstatic int host_pfn_mapping_level(struct kvm *kvm, gfn_t gfn,\n\t\t\t\t  const struct kvm_memory_slot *slot)\n{\n\tint level = PG_LEVEL_4K;\n\tunsigned long hva;\n\tunsigned long flags;\n\tpgd_t pgd;\n\tp4d_t p4d;\n\tpud_t pud;\n\tpmd_t pmd;\n\n\t \n\thva = __gfn_to_hva_memslot(slot, gfn);\n\n\t \n\tlocal_irq_save(flags);\n\n\t \n\tpgd = READ_ONCE(*pgd_offset(kvm->mm, hva));\n\tif (pgd_none(pgd))\n\t\tgoto out;\n\n\tp4d = READ_ONCE(*p4d_offset(&pgd, hva));\n\tif (p4d_none(p4d) || !p4d_present(p4d))\n\t\tgoto out;\n\n\tpud = READ_ONCE(*pud_offset(&p4d, hva));\n\tif (pud_none(pud) || !pud_present(pud))\n\t\tgoto out;\n\n\tif (pud_large(pud)) {\n\t\tlevel = PG_LEVEL_1G;\n\t\tgoto out;\n\t}\n\n\tpmd = READ_ONCE(*pmd_offset(&pud, hva));\n\tif (pmd_none(pmd) || !pmd_present(pmd))\n\t\tgoto out;\n\n\tif (pmd_large(pmd))\n\t\tlevel = PG_LEVEL_2M;\n\nout:\n\tlocal_irq_restore(flags);\n\treturn level;\n}\n\nint kvm_mmu_max_mapping_level(struct kvm *kvm,\n\t\t\t      const struct kvm_memory_slot *slot, gfn_t gfn,\n\t\t\t      int max_level)\n{\n\tstruct kvm_lpage_info *linfo;\n\tint host_level;\n\n\tmax_level = min(max_level, max_huge_page_level);\n\tfor ( ; max_level > PG_LEVEL_4K; max_level--) {\n\t\tlinfo = lpage_info_slot(gfn, slot, max_level);\n\t\tif (!linfo->disallow_lpage)\n\t\t\tbreak;\n\t}\n\n\tif (max_level == PG_LEVEL_4K)\n\t\treturn PG_LEVEL_4K;\n\n\thost_level = host_pfn_mapping_level(kvm, gfn, slot);\n\treturn min(host_level, max_level);\n}\n\nvoid kvm_mmu_hugepage_adjust(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)\n{\n\tstruct kvm_memory_slot *slot = fault->slot;\n\tkvm_pfn_t mask;\n\n\tfault->huge_page_disallowed = fault->exec && fault->nx_huge_page_workaround_enabled;\n\n\tif (unlikely(fault->max_level == PG_LEVEL_4K))\n\t\treturn;\n\n\tif (is_error_noslot_pfn(fault->pfn))\n\t\treturn;\n\n\tif (kvm_slot_dirty_track_enabled(slot))\n\t\treturn;\n\n\t \n\tfault->req_level = kvm_mmu_max_mapping_level(vcpu->kvm, slot,\n\t\t\t\t\t\t     fault->gfn, fault->max_level);\n\tif (fault->req_level == PG_LEVEL_4K || fault->huge_page_disallowed)\n\t\treturn;\n\n\t \n\tfault->goal_level = fault->req_level;\n\tmask = KVM_PAGES_PER_HPAGE(fault->goal_level) - 1;\n\tVM_BUG_ON((fault->gfn & mask) != (fault->pfn & mask));\n\tfault->pfn &= ~mask;\n}\n\nvoid disallowed_hugepage_adjust(struct kvm_page_fault *fault, u64 spte, int cur_level)\n{\n\tif (cur_level > PG_LEVEL_4K &&\n\t    cur_level == fault->goal_level &&\n\t    is_shadow_present_pte(spte) &&\n\t    !is_large_pte(spte) &&\n\t    spte_to_child_sp(spte)->nx_huge_page_disallowed) {\n\t\t \n\t\tu64 page_mask = KVM_PAGES_PER_HPAGE(cur_level) -\n\t\t\t\tKVM_PAGES_PER_HPAGE(cur_level - 1);\n\t\tfault->pfn |= fault->gfn & page_mask;\n\t\tfault->goal_level--;\n\t}\n}\n\nstatic int direct_map(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)\n{\n\tstruct kvm_shadow_walk_iterator it;\n\tstruct kvm_mmu_page *sp;\n\tint ret;\n\tgfn_t base_gfn = fault->gfn;\n\n\tkvm_mmu_hugepage_adjust(vcpu, fault);\n\n\ttrace_kvm_mmu_spte_requested(fault);\n\tfor_each_shadow_entry(vcpu, fault->addr, it) {\n\t\t \n\t\tif (fault->nx_huge_page_workaround_enabled)\n\t\t\tdisallowed_hugepage_adjust(fault, *it.sptep, it.level);\n\n\t\tbase_gfn = gfn_round_for_level(fault->gfn, it.level);\n\t\tif (it.level == fault->goal_level)\n\t\t\tbreak;\n\n\t\tsp = kvm_mmu_get_child_sp(vcpu, it.sptep, base_gfn, true, ACC_ALL);\n\t\tif (sp == ERR_PTR(-EEXIST))\n\t\t\tcontinue;\n\n\t\tlink_shadow_page(vcpu, it.sptep, sp);\n\t\tif (fault->huge_page_disallowed)\n\t\t\taccount_nx_huge_page(vcpu->kvm, sp,\n\t\t\t\t\t     fault->req_level >= it.level);\n\t}\n\n\tif (WARN_ON_ONCE(it.level != fault->goal_level))\n\t\treturn -EFAULT;\n\n\tret = mmu_set_spte(vcpu, fault->slot, it.sptep, ACC_ALL,\n\t\t\t   base_gfn, fault->pfn, fault);\n\tif (ret == RET_PF_SPURIOUS)\n\t\treturn ret;\n\n\tdirect_pte_prefetch(vcpu, it.sptep);\n\treturn ret;\n}\n\nstatic void kvm_send_hwpoison_signal(struct kvm_memory_slot *slot, gfn_t gfn)\n{\n\tunsigned long hva = gfn_to_hva_memslot(slot, gfn);\n\n\tsend_sig_mceerr(BUS_MCEERR_AR, (void __user *)hva, PAGE_SHIFT, current);\n}\n\nstatic int kvm_handle_error_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)\n{\n\tif (is_sigpending_pfn(fault->pfn)) {\n\t\tkvm_handle_signal_exit(vcpu);\n\t\treturn -EINTR;\n\t}\n\n\t \n\tif (fault->pfn == KVM_PFN_ERR_RO_FAULT)\n\t\treturn RET_PF_EMULATE;\n\n\tif (fault->pfn == KVM_PFN_ERR_HWPOISON) {\n\t\tkvm_send_hwpoison_signal(fault->slot, fault->gfn);\n\t\treturn RET_PF_RETRY;\n\t}\n\n\treturn -EFAULT;\n}\n\nstatic int kvm_handle_noslot_fault(struct kvm_vcpu *vcpu,\n\t\t\t\t   struct kvm_page_fault *fault,\n\t\t\t\t   unsigned int access)\n{\n\tgva_t gva = fault->is_tdp ? 0 : fault->addr;\n\n\tvcpu_cache_mmio_info(vcpu, gva, fault->gfn,\n\t\t\t     access & shadow_mmio_access_mask);\n\n\t \n\tif (unlikely(!enable_mmio_caching))\n\t\treturn RET_PF_EMULATE;\n\n\t \n\tif (unlikely(fault->gfn > kvm_mmu_max_gfn()))\n\t\treturn RET_PF_EMULATE;\n\n\treturn RET_PF_CONTINUE;\n}\n\nstatic bool page_fault_can_be_fast(struct kvm_page_fault *fault)\n{\n\t \n\tif (fault->rsvd)\n\t\treturn false;\n\n\t \n\tif (!fault->present)\n\t\treturn !kvm_ad_enabled();\n\n\t \n\treturn fault->write;\n}\n\n \nstatic bool fast_pf_fix_direct_spte(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_page_fault *fault,\n\t\t\t\t    u64 *sptep, u64 old_spte, u64 new_spte)\n{\n\t \n\tif (!try_cmpxchg64(sptep, &old_spte, new_spte))\n\t\treturn false;\n\n\tif (is_writable_pte(new_spte) && !is_writable_pte(old_spte))\n\t\tmark_page_dirty_in_slot(vcpu->kvm, fault->slot, fault->gfn);\n\n\treturn true;\n}\n\nstatic bool is_access_allowed(struct kvm_page_fault *fault, u64 spte)\n{\n\tif (fault->exec)\n\t\treturn is_executable_pte(spte);\n\n\tif (fault->write)\n\t\treturn is_writable_pte(spte);\n\n\t \n\treturn spte & PT_PRESENT_MASK;\n}\n\n \nstatic u64 *fast_pf_get_last_sptep(struct kvm_vcpu *vcpu, gpa_t gpa, u64 *spte)\n{\n\tstruct kvm_shadow_walk_iterator iterator;\n\tu64 old_spte;\n\tu64 *sptep = NULL;\n\n\tfor_each_shadow_entry_lockless(vcpu, gpa, iterator, old_spte) {\n\t\tsptep = iterator.sptep;\n\t\t*spte = old_spte;\n\t}\n\n\treturn sptep;\n}\n\n \nstatic int fast_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)\n{\n\tstruct kvm_mmu_page *sp;\n\tint ret = RET_PF_INVALID;\n\tu64 spte = 0ull;\n\tu64 *sptep = NULL;\n\tuint retry_count = 0;\n\n\tif (!page_fault_can_be_fast(fault))\n\t\treturn ret;\n\n\twalk_shadow_page_lockless_begin(vcpu);\n\n\tdo {\n\t\tu64 new_spte;\n\n\t\tif (tdp_mmu_enabled)\n\t\t\tsptep = kvm_tdp_mmu_fast_pf_get_last_sptep(vcpu, fault->addr, &spte);\n\t\telse\n\t\t\tsptep = fast_pf_get_last_sptep(vcpu, fault->addr, &spte);\n\n\t\tif (!is_shadow_present_pte(spte))\n\t\t\tbreak;\n\n\t\tsp = sptep_to_sp(sptep);\n\t\tif (!is_last_spte(spte, sp->role.level))\n\t\t\tbreak;\n\n\t\t \n\t\tif (is_access_allowed(fault, spte)) {\n\t\t\tret = RET_PF_SPURIOUS;\n\t\t\tbreak;\n\t\t}\n\n\t\tnew_spte = spte;\n\n\t\t \n\t\tif (unlikely(!kvm_ad_enabled()) && is_access_track_spte(spte))\n\t\t\tnew_spte = restore_acc_track_spte(new_spte);\n\n\t\t \n\t\tif (fault->write && is_mmu_writable_spte(spte)) {\n\t\t\tnew_spte |= PT_WRITABLE_MASK;\n\n\t\t\t \n\t\t\tif (sp->role.level > PG_LEVEL_4K &&\n\t\t\t    kvm_slot_dirty_track_enabled(fault->slot))\n\t\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (new_spte == spte ||\n\t\t    !is_access_allowed(fault, new_spte))\n\t\t\tbreak;\n\n\t\t \n\t\tif (fast_pf_fix_direct_spte(vcpu, fault, sptep, spte, new_spte)) {\n\t\t\tret = RET_PF_FIXED;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (++retry_count > 4) {\n\t\t\tpr_warn_once(\"Fast #PF retrying more than 4 times.\\n\");\n\t\t\tbreak;\n\t\t}\n\n\t} while (true);\n\n\ttrace_fast_page_fault(vcpu, fault, sptep, spte, ret);\n\twalk_shadow_page_lockless_end(vcpu);\n\n\tif (ret != RET_PF_INVALID)\n\t\tvcpu->stat.pf_fast++;\n\n\treturn ret;\n}\n\nstatic void mmu_free_root_page(struct kvm *kvm, hpa_t *root_hpa,\n\t\t\t       struct list_head *invalid_list)\n{\n\tstruct kvm_mmu_page *sp;\n\n\tif (!VALID_PAGE(*root_hpa))\n\t\treturn;\n\n\tsp = root_to_sp(*root_hpa);\n\tif (WARN_ON_ONCE(!sp))\n\t\treturn;\n\n\tif (is_tdp_mmu_page(sp))\n\t\tkvm_tdp_mmu_put_root(kvm, sp, false);\n\telse if (!--sp->root_count && sp->role.invalid)\n\t\tkvm_mmu_prepare_zap_page(kvm, sp, invalid_list);\n\n\t*root_hpa = INVALID_PAGE;\n}\n\n \nvoid kvm_mmu_free_roots(struct kvm *kvm, struct kvm_mmu *mmu,\n\t\t\tulong roots_to_free)\n{\n\tint i;\n\tLIST_HEAD(invalid_list);\n\tbool free_active_root;\n\n\tWARN_ON_ONCE(roots_to_free & ~KVM_MMU_ROOTS_ALL);\n\n\tBUILD_BUG_ON(KVM_MMU_NUM_PREV_ROOTS >= BITS_PER_LONG);\n\n\t \n\tfree_active_root = (roots_to_free & KVM_MMU_ROOT_CURRENT)\n\t\t&& VALID_PAGE(mmu->root.hpa);\n\n\tif (!free_active_root) {\n\t\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)\n\t\t\tif ((roots_to_free & KVM_MMU_ROOT_PREVIOUS(i)) &&\n\t\t\t    VALID_PAGE(mmu->prev_roots[i].hpa))\n\t\t\t\tbreak;\n\n\t\tif (i == KVM_MMU_NUM_PREV_ROOTS)\n\t\t\treturn;\n\t}\n\n\twrite_lock(&kvm->mmu_lock);\n\n\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)\n\t\tif (roots_to_free & KVM_MMU_ROOT_PREVIOUS(i))\n\t\t\tmmu_free_root_page(kvm, &mmu->prev_roots[i].hpa,\n\t\t\t\t\t   &invalid_list);\n\n\tif (free_active_root) {\n\t\tif (kvm_mmu_is_dummy_root(mmu->root.hpa)) {\n\t\t\t \n\t\t} else if (root_to_sp(mmu->root.hpa)) {\n\t\t\tmmu_free_root_page(kvm, &mmu->root.hpa, &invalid_list);\n\t\t} else if (mmu->pae_root) {\n\t\t\tfor (i = 0; i < 4; ++i) {\n\t\t\t\tif (!IS_VALID_PAE_ROOT(mmu->pae_root[i]))\n\t\t\t\t\tcontinue;\n\n\t\t\t\tmmu_free_root_page(kvm, &mmu->pae_root[i],\n\t\t\t\t\t\t   &invalid_list);\n\t\t\t\tmmu->pae_root[i] = INVALID_PAE_ROOT;\n\t\t\t}\n\t\t}\n\t\tmmu->root.hpa = INVALID_PAGE;\n\t\tmmu->root.pgd = 0;\n\t}\n\n\tkvm_mmu_commit_zap_page(kvm, &invalid_list);\n\twrite_unlock(&kvm->mmu_lock);\n}\nEXPORT_SYMBOL_GPL(kvm_mmu_free_roots);\n\nvoid kvm_mmu_free_guest_mode_roots(struct kvm *kvm, struct kvm_mmu *mmu)\n{\n\tunsigned long roots_to_free = 0;\n\tstruct kvm_mmu_page *sp;\n\thpa_t root_hpa;\n\tint i;\n\n\t \n\tWARN_ON_ONCE(mmu->root_role.guest_mode);\n\n\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {\n\t\troot_hpa = mmu->prev_roots[i].hpa;\n\t\tif (!VALID_PAGE(root_hpa))\n\t\t\tcontinue;\n\n\t\tsp = root_to_sp(root_hpa);\n\t\tif (!sp || sp->role.guest_mode)\n\t\t\troots_to_free |= KVM_MMU_ROOT_PREVIOUS(i);\n\t}\n\n\tkvm_mmu_free_roots(kvm, mmu, roots_to_free);\n}\nEXPORT_SYMBOL_GPL(kvm_mmu_free_guest_mode_roots);\n\nstatic hpa_t mmu_alloc_root(struct kvm_vcpu *vcpu, gfn_t gfn, int quadrant,\n\t\t\t    u8 level)\n{\n\tunion kvm_mmu_page_role role = vcpu->arch.mmu->root_role;\n\tstruct kvm_mmu_page *sp;\n\n\trole.level = level;\n\trole.quadrant = quadrant;\n\n\tWARN_ON_ONCE(quadrant && !role.has_4_byte_gpte);\n\tWARN_ON_ONCE(role.direct && role.has_4_byte_gpte);\n\n\tsp = kvm_mmu_get_shadow_page(vcpu, gfn, role);\n\t++sp->root_count;\n\n\treturn __pa(sp->spt);\n}\n\nstatic int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tu8 shadow_root_level = mmu->root_role.level;\n\thpa_t root;\n\tunsigned i;\n\tint r;\n\n\twrite_lock(&vcpu->kvm->mmu_lock);\n\tr = make_mmu_pages_available(vcpu);\n\tif (r < 0)\n\t\tgoto out_unlock;\n\n\tif (tdp_mmu_enabled) {\n\t\troot = kvm_tdp_mmu_get_vcpu_root_hpa(vcpu);\n\t\tmmu->root.hpa = root;\n\t} else if (shadow_root_level >= PT64_ROOT_4LEVEL) {\n\t\troot = mmu_alloc_root(vcpu, 0, 0, shadow_root_level);\n\t\tmmu->root.hpa = root;\n\t} else if (shadow_root_level == PT32E_ROOT_LEVEL) {\n\t\tif (WARN_ON_ONCE(!mmu->pae_root)) {\n\t\t\tr = -EIO;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tfor (i = 0; i < 4; ++i) {\n\t\t\tWARN_ON_ONCE(IS_VALID_PAE_ROOT(mmu->pae_root[i]));\n\n\t\t\troot = mmu_alloc_root(vcpu, i << (30 - PAGE_SHIFT), 0,\n\t\t\t\t\t      PT32_ROOT_LEVEL);\n\t\t\tmmu->pae_root[i] = root | PT_PRESENT_MASK |\n\t\t\t\t\t   shadow_me_value;\n\t\t}\n\t\tmmu->root.hpa = __pa(mmu->pae_root);\n\t} else {\n\t\tWARN_ONCE(1, \"Bad TDP root level = %d\\n\", shadow_root_level);\n\t\tr = -EIO;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tmmu->root.pgd = 0;\nout_unlock:\n\twrite_unlock(&vcpu->kvm->mmu_lock);\n\treturn r;\n}\n\nstatic int mmu_first_shadow_root_alloc(struct kvm *kvm)\n{\n\tstruct kvm_memslots *slots;\n\tstruct kvm_memory_slot *slot;\n\tint r = 0, i, bkt;\n\n\t \n\tif (kvm_shadow_root_allocated(kvm))\n\t\treturn 0;\n\n\tmutex_lock(&kvm->slots_arch_lock);\n\n\t \n\tif (kvm_shadow_root_allocated(kvm))\n\t\tgoto out_unlock;\n\n\t \n\tif (kvm_memslots_have_rmaps(kvm) &&\n\t    kvm_page_track_write_tracking_enabled(kvm))\n\t\tgoto out_success;\n\n\tfor (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {\n\t\tslots = __kvm_memslots(kvm, i);\n\t\tkvm_for_each_memslot(slot, bkt, slots) {\n\t\t\t \n\t\t\tr = memslot_rmap_alloc(slot, slot->npages);\n\t\t\tif (r)\n\t\t\t\tgoto out_unlock;\n\t\t\tr = kvm_page_track_write_tracking_alloc(slot);\n\t\t\tif (r)\n\t\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\t \nout_success:\n\tsmp_store_release(&kvm->arch.shadow_root_allocated, true);\n\nout_unlock:\n\tmutex_unlock(&kvm->slots_arch_lock);\n\treturn r;\n}\n\nstatic int mmu_alloc_shadow_roots(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tu64 pdptrs[4], pm_mask;\n\tgfn_t root_gfn, root_pgd;\n\tint quadrant, i, r;\n\thpa_t root;\n\n\troot_pgd = kvm_mmu_get_guest_pgd(vcpu, mmu);\n\troot_gfn = root_pgd >> PAGE_SHIFT;\n\n\tif (!kvm_vcpu_is_visible_gfn(vcpu, root_gfn)) {\n\t\tmmu->root.hpa = kvm_mmu_get_dummy_root();\n\t\treturn 0;\n\t}\n\n\t \n\tif (mmu->cpu_role.base.level == PT32E_ROOT_LEVEL) {\n\t\tfor (i = 0; i < 4; ++i) {\n\t\t\tpdptrs[i] = mmu->get_pdptr(vcpu, i);\n\t\t\tif (!(pdptrs[i] & PT_PRESENT_MASK))\n\t\t\t\tcontinue;\n\n\t\t\tif (!kvm_vcpu_is_visible_gfn(vcpu, pdptrs[i] >> PAGE_SHIFT))\n\t\t\t\tpdptrs[i] = 0;\n\t\t}\n\t}\n\n\tr = mmu_first_shadow_root_alloc(vcpu->kvm);\n\tif (r)\n\t\treturn r;\n\n\twrite_lock(&vcpu->kvm->mmu_lock);\n\tr = make_mmu_pages_available(vcpu);\n\tif (r < 0)\n\t\tgoto out_unlock;\n\n\t \n\tif (mmu->cpu_role.base.level >= PT64_ROOT_4LEVEL) {\n\t\troot = mmu_alloc_root(vcpu, root_gfn, 0,\n\t\t\t\t      mmu->root_role.level);\n\t\tmmu->root.hpa = root;\n\t\tgoto set_root_pgd;\n\t}\n\n\tif (WARN_ON_ONCE(!mmu->pae_root)) {\n\t\tr = -EIO;\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tpm_mask = PT_PRESENT_MASK | shadow_me_value;\n\tif (mmu->root_role.level >= PT64_ROOT_4LEVEL) {\n\t\tpm_mask |= PT_ACCESSED_MASK | PT_WRITABLE_MASK | PT_USER_MASK;\n\n\t\tif (WARN_ON_ONCE(!mmu->pml4_root)) {\n\t\t\tr = -EIO;\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tmmu->pml4_root[0] = __pa(mmu->pae_root) | pm_mask;\n\n\t\tif (mmu->root_role.level == PT64_ROOT_5LEVEL) {\n\t\t\tif (WARN_ON_ONCE(!mmu->pml5_root)) {\n\t\t\t\tr = -EIO;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\t\t\tmmu->pml5_root[0] = __pa(mmu->pml4_root) | pm_mask;\n\t\t}\n\t}\n\n\tfor (i = 0; i < 4; ++i) {\n\t\tWARN_ON_ONCE(IS_VALID_PAE_ROOT(mmu->pae_root[i]));\n\n\t\tif (mmu->cpu_role.base.level == PT32E_ROOT_LEVEL) {\n\t\t\tif (!(pdptrs[i] & PT_PRESENT_MASK)) {\n\t\t\t\tmmu->pae_root[i] = INVALID_PAE_ROOT;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\troot_gfn = pdptrs[i] >> PAGE_SHIFT;\n\t\t}\n\n\t\t \n\t\tquadrant = (mmu->cpu_role.base.level == PT32_ROOT_LEVEL) ? i : 0;\n\n\t\troot = mmu_alloc_root(vcpu, root_gfn, quadrant, PT32_ROOT_LEVEL);\n\t\tmmu->pae_root[i] = root | pm_mask;\n\t}\n\n\tif (mmu->root_role.level == PT64_ROOT_5LEVEL)\n\t\tmmu->root.hpa = __pa(mmu->pml5_root);\n\telse if (mmu->root_role.level == PT64_ROOT_4LEVEL)\n\t\tmmu->root.hpa = __pa(mmu->pml4_root);\n\telse\n\t\tmmu->root.hpa = __pa(mmu->pae_root);\n\nset_root_pgd:\n\tmmu->root.pgd = root_pgd;\nout_unlock:\n\twrite_unlock(&vcpu->kvm->mmu_lock);\n\n\treturn r;\n}\n\nstatic int mmu_alloc_special_roots(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tbool need_pml5 = mmu->root_role.level > PT64_ROOT_4LEVEL;\n\tu64 *pml5_root = NULL;\n\tu64 *pml4_root = NULL;\n\tu64 *pae_root;\n\n\t \n\tif (mmu->root_role.direct ||\n\t    mmu->cpu_role.base.level >= PT64_ROOT_4LEVEL ||\n\t    mmu->root_role.level < PT64_ROOT_4LEVEL)\n\t\treturn 0;\n\n\t \n\tif (mmu->pae_root && mmu->pml4_root && (!need_pml5 || mmu->pml5_root))\n\t\treturn 0;\n\n\t \n\tif (WARN_ON_ONCE(!tdp_enabled || mmu->pae_root || mmu->pml4_root ||\n\t\t\t (need_pml5 && mmu->pml5_root)))\n\t\treturn -EIO;\n\n\t \n\tpae_root = (void *)get_zeroed_page(GFP_KERNEL_ACCOUNT);\n\tif (!pae_root)\n\t\treturn -ENOMEM;\n\n#ifdef CONFIG_X86_64\n\tpml4_root = (void *)get_zeroed_page(GFP_KERNEL_ACCOUNT);\n\tif (!pml4_root)\n\t\tgoto err_pml4;\n\n\tif (need_pml5) {\n\t\tpml5_root = (void *)get_zeroed_page(GFP_KERNEL_ACCOUNT);\n\t\tif (!pml5_root)\n\t\t\tgoto err_pml5;\n\t}\n#endif\n\n\tmmu->pae_root = pae_root;\n\tmmu->pml4_root = pml4_root;\n\tmmu->pml5_root = pml5_root;\n\n\treturn 0;\n\n#ifdef CONFIG_X86_64\nerr_pml5:\n\tfree_page((unsigned long)pml4_root);\nerr_pml4:\n\tfree_page((unsigned long)pae_root);\n\treturn -ENOMEM;\n#endif\n}\n\nstatic bool is_unsync_root(hpa_t root)\n{\n\tstruct kvm_mmu_page *sp;\n\n\tif (!VALID_PAGE(root) || kvm_mmu_is_dummy_root(root))\n\t\treturn false;\n\n\t \n\tsmp_rmb();\n\tsp = root_to_sp(root);\n\n\t \n\tif (WARN_ON_ONCE(!sp))\n\t\treturn false;\n\n\tif (sp->unsync || sp->unsync_children)\n\t\treturn true;\n\n\treturn false;\n}\n\nvoid kvm_mmu_sync_roots(struct kvm_vcpu *vcpu)\n{\n\tint i;\n\tstruct kvm_mmu_page *sp;\n\n\tif (vcpu->arch.mmu->root_role.direct)\n\t\treturn;\n\n\tif (!VALID_PAGE(vcpu->arch.mmu->root.hpa))\n\t\treturn;\n\n\tvcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);\n\n\tif (vcpu->arch.mmu->cpu_role.base.level >= PT64_ROOT_4LEVEL) {\n\t\thpa_t root = vcpu->arch.mmu->root.hpa;\n\n\t\tif (!is_unsync_root(root))\n\t\t\treturn;\n\n\t\tsp = root_to_sp(root);\n\n\t\twrite_lock(&vcpu->kvm->mmu_lock);\n\t\tmmu_sync_children(vcpu, sp, true);\n\t\twrite_unlock(&vcpu->kvm->mmu_lock);\n\t\treturn;\n\t}\n\n\twrite_lock(&vcpu->kvm->mmu_lock);\n\n\tfor (i = 0; i < 4; ++i) {\n\t\thpa_t root = vcpu->arch.mmu->pae_root[i];\n\n\t\tif (IS_VALID_PAE_ROOT(root)) {\n\t\t\tsp = spte_to_child_sp(root);\n\t\t\tmmu_sync_children(vcpu, sp, true);\n\t\t}\n\t}\n\n\twrite_unlock(&vcpu->kvm->mmu_lock);\n}\n\nvoid kvm_mmu_sync_prev_roots(struct kvm_vcpu *vcpu)\n{\n\tunsigned long roots_to_free = 0;\n\tint i;\n\n\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)\n\t\tif (is_unsync_root(vcpu->arch.mmu->prev_roots[i].hpa))\n\t\t\troots_to_free |= KVM_MMU_ROOT_PREVIOUS(i);\n\n\t \n\tkvm_mmu_free_roots(vcpu->kvm, vcpu->arch.mmu, roots_to_free);\n}\n\nstatic gpa_t nonpaging_gva_to_gpa(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,\n\t\t\t\t  gpa_t vaddr, u64 access,\n\t\t\t\t  struct x86_exception *exception)\n{\n\tif (exception)\n\t\texception->error_code = 0;\n\treturn kvm_translate_gpa(vcpu, mmu, vaddr, access, exception);\n}\n\nstatic bool mmio_info_in_cache(struct kvm_vcpu *vcpu, u64 addr, bool direct)\n{\n\t \n\tif (mmu_is_nested(vcpu))\n\t\treturn false;\n\n\tif (direct)\n\t\treturn vcpu_match_mmio_gpa(vcpu, addr);\n\n\treturn vcpu_match_mmio_gva(vcpu, addr);\n}\n\n \nstatic int get_walk(struct kvm_vcpu *vcpu, u64 addr, u64 *sptes, int *root_level)\n{\n\tstruct kvm_shadow_walk_iterator iterator;\n\tint leaf = -1;\n\tu64 spte;\n\n\tfor (shadow_walk_init(&iterator, vcpu, addr),\n\t     *root_level = iterator.level;\n\t     shadow_walk_okay(&iterator);\n\t     __shadow_walk_next(&iterator, spte)) {\n\t\tleaf = iterator.level;\n\t\tspte = mmu_spte_get_lockless(iterator.sptep);\n\n\t\tsptes[leaf] = spte;\n\t}\n\n\treturn leaf;\n}\n\n \nstatic bool get_mmio_spte(struct kvm_vcpu *vcpu, u64 addr, u64 *sptep)\n{\n\tu64 sptes[PT64_ROOT_MAX_LEVEL + 1];\n\tstruct rsvd_bits_validate *rsvd_check;\n\tint root, leaf, level;\n\tbool reserved = false;\n\n\twalk_shadow_page_lockless_begin(vcpu);\n\n\tif (is_tdp_mmu_active(vcpu))\n\t\tleaf = kvm_tdp_mmu_get_walk(vcpu, addr, sptes, &root);\n\telse\n\t\tleaf = get_walk(vcpu, addr, sptes, &root);\n\n\twalk_shadow_page_lockless_end(vcpu);\n\n\tif (unlikely(leaf < 0)) {\n\t\t*sptep = 0ull;\n\t\treturn reserved;\n\t}\n\n\t*sptep = sptes[leaf];\n\n\t \n\tif (!is_shadow_present_pte(sptes[leaf]))\n\t\tleaf++;\n\n\trsvd_check = &vcpu->arch.mmu->shadow_zero_check;\n\n\tfor (level = root; level >= leaf; level--)\n\t\treserved |= is_rsvd_spte(rsvd_check, sptes[level], level);\n\n\tif (reserved) {\n\t\tpr_err(\"%s: reserved bits set on MMU-present spte, addr 0x%llx, hierarchy:\\n\",\n\t\t       __func__, addr);\n\t\tfor (level = root; level >= leaf; level--)\n\t\t\tpr_err(\"------ spte = 0x%llx level = %d, rsvd bits = 0x%llx\",\n\t\t\t       sptes[level], level,\n\t\t\t       get_rsvd_bits(rsvd_check, sptes[level], level));\n\t}\n\n\treturn reserved;\n}\n\nstatic int handle_mmio_page_fault(struct kvm_vcpu *vcpu, u64 addr, bool direct)\n{\n\tu64 spte;\n\tbool reserved;\n\n\tif (mmio_info_in_cache(vcpu, addr, direct))\n\t\treturn RET_PF_EMULATE;\n\n\treserved = get_mmio_spte(vcpu, addr, &spte);\n\tif (WARN_ON_ONCE(reserved))\n\t\treturn -EINVAL;\n\n\tif (is_mmio_spte(spte)) {\n\t\tgfn_t gfn = get_mmio_spte_gfn(spte);\n\t\tunsigned int access = get_mmio_spte_access(spte);\n\n\t\tif (!check_mmio_spte(vcpu, spte))\n\t\t\treturn RET_PF_INVALID;\n\n\t\tif (direct)\n\t\t\taddr = 0;\n\n\t\ttrace_handle_mmio_page_fault(addr, gfn, access);\n\t\tvcpu_cache_mmio_info(vcpu, addr, gfn, access);\n\t\treturn RET_PF_EMULATE;\n\t}\n\n\t \n\treturn RET_PF_RETRY;\n}\n\nstatic bool page_fault_handle_page_track(struct kvm_vcpu *vcpu,\n\t\t\t\t\t struct kvm_page_fault *fault)\n{\n\tif (unlikely(fault->rsvd))\n\t\treturn false;\n\n\tif (!fault->present || !fault->write)\n\t\treturn false;\n\n\t \n\tif (kvm_gfn_is_write_tracked(vcpu->kvm, fault->slot, fault->gfn))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic void shadow_page_table_clear_flood(struct kvm_vcpu *vcpu, gva_t addr)\n{\n\tstruct kvm_shadow_walk_iterator iterator;\n\tu64 spte;\n\n\twalk_shadow_page_lockless_begin(vcpu);\n\tfor_each_shadow_entry_lockless(vcpu, addr, iterator, spte)\n\t\tclear_sp_write_flooding_count(iterator.sptep);\n\twalk_shadow_page_lockless_end(vcpu);\n}\n\nstatic u32 alloc_apf_token(struct kvm_vcpu *vcpu)\n{\n\t \n\tu32 id = vcpu->arch.apf.id;\n\n\tif (id << 12 == 0)\n\t\tvcpu->arch.apf.id = 1;\n\n\treturn (vcpu->arch.apf.id++ << 12) | vcpu->vcpu_id;\n}\n\nstatic bool kvm_arch_setup_async_pf(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t\t    gfn_t gfn)\n{\n\tstruct kvm_arch_async_pf arch;\n\n\tarch.token = alloc_apf_token(vcpu);\n\tarch.gfn = gfn;\n\tarch.direct_map = vcpu->arch.mmu->root_role.direct;\n\tarch.cr3 = kvm_mmu_get_guest_pgd(vcpu, vcpu->arch.mmu);\n\n\treturn kvm_setup_async_pf(vcpu, cr2_or_gpa,\n\t\t\t\t  kvm_vcpu_gfn_to_hva(vcpu, gfn), &arch);\n}\n\nvoid kvm_arch_async_page_ready(struct kvm_vcpu *vcpu, struct kvm_async_pf *work)\n{\n\tint r;\n\n\tif ((vcpu->arch.mmu->root_role.direct != work->arch.direct_map) ||\n\t      work->wakeup_all)\n\t\treturn;\n\n\tr = kvm_mmu_reload(vcpu);\n\tif (unlikely(r))\n\t\treturn;\n\n\tif (!vcpu->arch.mmu->root_role.direct &&\n\t      work->arch.cr3 != kvm_mmu_get_guest_pgd(vcpu, vcpu->arch.mmu))\n\t\treturn;\n\n\tkvm_mmu_do_page_fault(vcpu, work->cr2_or_gpa, 0, true, NULL);\n}\n\nstatic int __kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)\n{\n\tstruct kvm_memory_slot *slot = fault->slot;\n\tbool async;\n\n\t \n\tif (slot && (slot->flags & KVM_MEMSLOT_INVALID))\n\t\treturn RET_PF_RETRY;\n\n\tif (!kvm_is_visible_memslot(slot)) {\n\t\t \n\t\tif (is_guest_mode(vcpu)) {\n\t\t\tfault->slot = NULL;\n\t\t\tfault->pfn = KVM_PFN_NOSLOT;\n\t\t\tfault->map_writable = false;\n\t\t\treturn RET_PF_CONTINUE;\n\t\t}\n\t\t \n\t\tif (slot && slot->id == APIC_ACCESS_PAGE_PRIVATE_MEMSLOT &&\n\t\t    !kvm_apicv_activated(vcpu->kvm))\n\t\t\treturn RET_PF_EMULATE;\n\t}\n\n\tasync = false;\n\tfault->pfn = __gfn_to_pfn_memslot(slot, fault->gfn, false, false, &async,\n\t\t\t\t\t  fault->write, &fault->map_writable,\n\t\t\t\t\t  &fault->hva);\n\tif (!async)\n\t\treturn RET_PF_CONTINUE;  \n\n\tif (!fault->prefetch && kvm_can_do_async_pf(vcpu)) {\n\t\ttrace_kvm_try_async_get_page(fault->addr, fault->gfn);\n\t\tif (kvm_find_async_pf_gfn(vcpu, fault->gfn)) {\n\t\t\ttrace_kvm_async_pf_repeated_fault(fault->addr, fault->gfn);\n\t\t\tkvm_make_request(KVM_REQ_APF_HALT, vcpu);\n\t\t\treturn RET_PF_RETRY;\n\t\t} else if (kvm_arch_setup_async_pf(vcpu, fault->addr, fault->gfn)) {\n\t\t\treturn RET_PF_RETRY;\n\t\t}\n\t}\n\n\t \n\tfault->pfn = __gfn_to_pfn_memslot(slot, fault->gfn, false, true, NULL,\n\t\t\t\t\t  fault->write, &fault->map_writable,\n\t\t\t\t\t  &fault->hva);\n\treturn RET_PF_CONTINUE;\n}\n\nstatic int kvm_faultin_pfn(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault,\n\t\t\t   unsigned int access)\n{\n\tint ret;\n\n\tfault->mmu_seq = vcpu->kvm->mmu_invalidate_seq;\n\tsmp_rmb();\n\n\tret = __kvm_faultin_pfn(vcpu, fault);\n\tif (ret != RET_PF_CONTINUE)\n\t\treturn ret;\n\n\tif (unlikely(is_error_pfn(fault->pfn)))\n\t\treturn kvm_handle_error_pfn(vcpu, fault);\n\n\tif (unlikely(!fault->slot))\n\t\treturn kvm_handle_noslot_fault(vcpu, fault, access);\n\n\treturn RET_PF_CONTINUE;\n}\n\n \nstatic bool is_page_fault_stale(struct kvm_vcpu *vcpu,\n\t\t\t\tstruct kvm_page_fault *fault)\n{\n\tstruct kvm_mmu_page *sp = root_to_sp(vcpu->arch.mmu->root.hpa);\n\n\t \n\tif (sp && is_obsolete_sp(vcpu->kvm, sp))\n\t\treturn true;\n\n\t \n\tif (!sp && kvm_test_request(KVM_REQ_MMU_FREE_OBSOLETE_ROOTS, vcpu))\n\t\treturn true;\n\n\treturn fault->slot &&\n\t       mmu_invalidate_retry_hva(vcpu->kvm, fault->mmu_seq, fault->hva);\n}\n\nstatic int direct_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)\n{\n\tint r;\n\n\t \n\tif (WARN_ON_ONCE(kvm_mmu_is_dummy_root(vcpu->arch.mmu->root.hpa)))\n\t\treturn RET_PF_RETRY;\n\n\tif (page_fault_handle_page_track(vcpu, fault))\n\t\treturn RET_PF_EMULATE;\n\n\tr = fast_page_fault(vcpu, fault);\n\tif (r != RET_PF_INVALID)\n\t\treturn r;\n\n\tr = mmu_topup_memory_caches(vcpu, false);\n\tif (r)\n\t\treturn r;\n\n\tr = kvm_faultin_pfn(vcpu, fault, ACC_ALL);\n\tif (r != RET_PF_CONTINUE)\n\t\treturn r;\n\n\tr = RET_PF_RETRY;\n\twrite_lock(&vcpu->kvm->mmu_lock);\n\n\tif (is_page_fault_stale(vcpu, fault))\n\t\tgoto out_unlock;\n\n\tr = make_mmu_pages_available(vcpu);\n\tif (r)\n\t\tgoto out_unlock;\n\n\tr = direct_map(vcpu, fault);\n\nout_unlock:\n\twrite_unlock(&vcpu->kvm->mmu_lock);\n\tkvm_release_pfn_clean(fault->pfn);\n\treturn r;\n}\n\nstatic int nonpaging_page_fault(struct kvm_vcpu *vcpu,\n\t\t\t\tstruct kvm_page_fault *fault)\n{\n\t \n\tfault->max_level = PG_LEVEL_2M;\n\treturn direct_page_fault(vcpu, fault);\n}\n\nint kvm_handle_page_fault(struct kvm_vcpu *vcpu, u64 error_code,\n\t\t\t\tu64 fault_address, char *insn, int insn_len)\n{\n\tint r = 1;\n\tu32 flags = vcpu->arch.apf.host_apf_flags;\n\n#ifndef CONFIG_X86_64\n\t \n\tif (WARN_ON_ONCE(fault_address >> 32))\n\t\treturn -EFAULT;\n#endif\n\n\tvcpu->arch.l1tf_flush_l1d = true;\n\tif (!flags) {\n\t\ttrace_kvm_page_fault(vcpu, fault_address, error_code);\n\n\t\tif (kvm_event_needs_reinjection(vcpu))\n\t\t\tkvm_mmu_unprotect_page_virt(vcpu, fault_address);\n\t\tr = kvm_mmu_page_fault(vcpu, fault_address, error_code, insn,\n\t\t\t\tinsn_len);\n\t} else if (flags & KVM_PV_REASON_PAGE_NOT_PRESENT) {\n\t\tvcpu->arch.apf.host_apf_flags = 0;\n\t\tlocal_irq_disable();\n\t\tkvm_async_pf_task_wait_schedule(fault_address);\n\t\tlocal_irq_enable();\n\t} else {\n\t\tWARN_ONCE(1, \"Unexpected host async PF flags: %x\\n\", flags);\n\t}\n\n\treturn r;\n}\nEXPORT_SYMBOL_GPL(kvm_handle_page_fault);\n\n#ifdef CONFIG_X86_64\nstatic int kvm_tdp_mmu_page_fault(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_page_fault *fault)\n{\n\tint r;\n\n\tif (page_fault_handle_page_track(vcpu, fault))\n\t\treturn RET_PF_EMULATE;\n\n\tr = fast_page_fault(vcpu, fault);\n\tif (r != RET_PF_INVALID)\n\t\treturn r;\n\n\tr = mmu_topup_memory_caches(vcpu, false);\n\tif (r)\n\t\treturn r;\n\n\tr = kvm_faultin_pfn(vcpu, fault, ACC_ALL);\n\tif (r != RET_PF_CONTINUE)\n\t\treturn r;\n\n\tr = RET_PF_RETRY;\n\tread_lock(&vcpu->kvm->mmu_lock);\n\n\tif (is_page_fault_stale(vcpu, fault))\n\t\tgoto out_unlock;\n\n\tr = kvm_tdp_mmu_map(vcpu, fault);\n\nout_unlock:\n\tread_unlock(&vcpu->kvm->mmu_lock);\n\tkvm_release_pfn_clean(fault->pfn);\n\treturn r;\n}\n#endif\n\nint kvm_tdp_page_fault(struct kvm_vcpu *vcpu, struct kvm_page_fault *fault)\n{\n\t \n\tif (shadow_memtype_mask && kvm_arch_has_noncoherent_dma(vcpu->kvm)) {\n\t\tfor ( ; fault->max_level > PG_LEVEL_4K; --fault->max_level) {\n\t\t\tint page_num = KVM_PAGES_PER_HPAGE(fault->max_level);\n\t\t\tgfn_t base = gfn_round_for_level(fault->gfn,\n\t\t\t\t\t\t\t fault->max_level);\n\n\t\t\tif (kvm_mtrr_check_gfn_range_consistency(vcpu, base, page_num))\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n#ifdef CONFIG_X86_64\n\tif (tdp_mmu_enabled)\n\t\treturn kvm_tdp_mmu_page_fault(vcpu, fault);\n#endif\n\n\treturn direct_page_fault(vcpu, fault);\n}\n\nstatic void nonpaging_init_context(struct kvm_mmu *context)\n{\n\tcontext->page_fault = nonpaging_page_fault;\n\tcontext->gva_to_gpa = nonpaging_gva_to_gpa;\n\tcontext->sync_spte = NULL;\n}\n\nstatic inline bool is_root_usable(struct kvm_mmu_root_info *root, gpa_t pgd,\n\t\t\t\t  union kvm_mmu_page_role role)\n{\n\tstruct kvm_mmu_page *sp;\n\n\tif (!VALID_PAGE(root->hpa))\n\t\treturn false;\n\n\tif (!role.direct && pgd != root->pgd)\n\t\treturn false;\n\n\tsp = root_to_sp(root->hpa);\n\tif (WARN_ON_ONCE(!sp))\n\t\treturn false;\n\n\treturn role.word == sp->role.word;\n}\n\n \nstatic bool cached_root_find_and_keep_current(struct kvm *kvm, struct kvm_mmu *mmu,\n\t\t\t\t\t      gpa_t new_pgd,\n\t\t\t\t\t      union kvm_mmu_page_role new_role)\n{\n\tuint i;\n\n\tif (is_root_usable(&mmu->root, new_pgd, new_role))\n\t\treturn true;\n\n\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {\n\t\t \n\t\tswap(mmu->root, mmu->prev_roots[i]);\n\t\tif (is_root_usable(&mmu->root, new_pgd, new_role))\n\t\t\treturn true;\n\t}\n\n\tkvm_mmu_free_roots(kvm, mmu, KVM_MMU_ROOT_CURRENT);\n\treturn false;\n}\n\n \nstatic bool cached_root_find_without_current(struct kvm *kvm, struct kvm_mmu *mmu,\n\t\t\t\t\t     gpa_t new_pgd,\n\t\t\t\t\t     union kvm_mmu_page_role new_role)\n{\n\tuint i;\n\n\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)\n\t\tif (is_root_usable(&mmu->prev_roots[i], new_pgd, new_role))\n\t\t\tgoto hit;\n\n\treturn false;\n\nhit:\n\tswap(mmu->root, mmu->prev_roots[i]);\n\t \n\tfor (; i < KVM_MMU_NUM_PREV_ROOTS - 1; i++)\n\t\tmmu->prev_roots[i] = mmu->prev_roots[i + 1];\n\tmmu->prev_roots[i].hpa = INVALID_PAGE;\n\treturn true;\n}\n\nstatic bool fast_pgd_switch(struct kvm *kvm, struct kvm_mmu *mmu,\n\t\t\t    gpa_t new_pgd, union kvm_mmu_page_role new_role)\n{\n\t \n\tif (VALID_PAGE(mmu->root.hpa) && !root_to_sp(mmu->root.hpa))\n\t\tkvm_mmu_free_roots(kvm, mmu, KVM_MMU_ROOT_CURRENT);\n\n\tif (VALID_PAGE(mmu->root.hpa))\n\t\treturn cached_root_find_and_keep_current(kvm, mmu, new_pgd, new_role);\n\telse\n\t\treturn cached_root_find_without_current(kvm, mmu, new_pgd, new_role);\n}\n\nvoid kvm_mmu_new_pgd(struct kvm_vcpu *vcpu, gpa_t new_pgd)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tunion kvm_mmu_page_role new_role = mmu->root_role;\n\n\t \n\tif (!fast_pgd_switch(vcpu->kvm, mmu, new_pgd, new_role))\n\t\treturn;\n\n\t \n\tkvm_make_request(KVM_REQ_LOAD_MMU_PGD, vcpu);\n\n\tif (force_flush_and_sync_on_reuse) {\n\t\tkvm_make_request(KVM_REQ_MMU_SYNC, vcpu);\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);\n\t}\n\n\t \n\tvcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);\n\n\t \n\tif (!new_role.direct) {\n\t\tstruct kvm_mmu_page *sp = root_to_sp(vcpu->arch.mmu->root.hpa);\n\n\t\tif (!WARN_ON_ONCE(!sp))\n\t\t\t__clear_sp_write_flooding_count(sp);\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_mmu_new_pgd);\n\nstatic bool sync_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, gfn_t gfn,\n\t\t\t   unsigned int access)\n{\n\tif (unlikely(is_mmio_spte(*sptep))) {\n\t\tif (gfn != get_mmio_spte_gfn(*sptep)) {\n\t\t\tmmu_spte_clear_no_track(sptep);\n\t\t\treturn true;\n\t\t}\n\n\t\tmark_mmio_spte(vcpu, sptep, gfn, access);\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n#define PTTYPE_EPT 18  \n#define PTTYPE PTTYPE_EPT\n#include \"paging_tmpl.h\"\n#undef PTTYPE\n\n#define PTTYPE 64\n#include \"paging_tmpl.h\"\n#undef PTTYPE\n\n#define PTTYPE 32\n#include \"paging_tmpl.h\"\n#undef PTTYPE\n\nstatic void __reset_rsvds_bits_mask(struct rsvd_bits_validate *rsvd_check,\n\t\t\t\t    u64 pa_bits_rsvd, int level, bool nx,\n\t\t\t\t    bool gbpages, bool pse, bool amd)\n{\n\tu64 gbpages_bit_rsvd = 0;\n\tu64 nonleaf_bit8_rsvd = 0;\n\tu64 high_bits_rsvd;\n\n\trsvd_check->bad_mt_xwr = 0;\n\n\tif (!gbpages)\n\t\tgbpages_bit_rsvd = rsvd_bits(7, 7);\n\n\tif (level == PT32E_ROOT_LEVEL)\n\t\thigh_bits_rsvd = pa_bits_rsvd & rsvd_bits(0, 62);\n\telse\n\t\thigh_bits_rsvd = pa_bits_rsvd & rsvd_bits(0, 51);\n\n\t \n\tif (!nx)\n\t\thigh_bits_rsvd |= rsvd_bits(63, 63);\n\n\t \n\tif (amd)\n\t\tnonleaf_bit8_rsvd = rsvd_bits(8, 8);\n\n\tswitch (level) {\n\tcase PT32_ROOT_LEVEL:\n\t\t \n\t\trsvd_check->rsvd_bits_mask[0][1] = 0;\n\t\trsvd_check->rsvd_bits_mask[0][0] = 0;\n\t\trsvd_check->rsvd_bits_mask[1][0] =\n\t\t\trsvd_check->rsvd_bits_mask[0][0];\n\n\t\tif (!pse) {\n\t\t\trsvd_check->rsvd_bits_mask[1][1] = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (is_cpuid_PSE36())\n\t\t\t \n\t\t\trsvd_check->rsvd_bits_mask[1][1] = rsvd_bits(17, 21);\n\t\telse\n\t\t\t \n\t\t\trsvd_check->rsvd_bits_mask[1][1] = rsvd_bits(13, 21);\n\t\tbreak;\n\tcase PT32E_ROOT_LEVEL:\n\t\trsvd_check->rsvd_bits_mask[0][2] = rsvd_bits(63, 63) |\n\t\t\t\t\t\t   high_bits_rsvd |\n\t\t\t\t\t\t   rsvd_bits(5, 8) |\n\t\t\t\t\t\t   rsvd_bits(1, 2);\t \n\t\trsvd_check->rsvd_bits_mask[0][1] = high_bits_rsvd;\t \n\t\trsvd_check->rsvd_bits_mask[0][0] = high_bits_rsvd;\t \n\t\trsvd_check->rsvd_bits_mask[1][1] = high_bits_rsvd |\n\t\t\t\t\t\t   rsvd_bits(13, 20);\t \n\t\trsvd_check->rsvd_bits_mask[1][0] =\n\t\t\trsvd_check->rsvd_bits_mask[0][0];\n\t\tbreak;\n\tcase PT64_ROOT_5LEVEL:\n\t\trsvd_check->rsvd_bits_mask[0][4] = high_bits_rsvd |\n\t\t\t\t\t\t   nonleaf_bit8_rsvd |\n\t\t\t\t\t\t   rsvd_bits(7, 7);\n\t\trsvd_check->rsvd_bits_mask[1][4] =\n\t\t\trsvd_check->rsvd_bits_mask[0][4];\n\t\tfallthrough;\n\tcase PT64_ROOT_4LEVEL:\n\t\trsvd_check->rsvd_bits_mask[0][3] = high_bits_rsvd |\n\t\t\t\t\t\t   nonleaf_bit8_rsvd |\n\t\t\t\t\t\t   rsvd_bits(7, 7);\n\t\trsvd_check->rsvd_bits_mask[0][2] = high_bits_rsvd |\n\t\t\t\t\t\t   gbpages_bit_rsvd;\n\t\trsvd_check->rsvd_bits_mask[0][1] = high_bits_rsvd;\n\t\trsvd_check->rsvd_bits_mask[0][0] = high_bits_rsvd;\n\t\trsvd_check->rsvd_bits_mask[1][3] =\n\t\t\trsvd_check->rsvd_bits_mask[0][3];\n\t\trsvd_check->rsvd_bits_mask[1][2] = high_bits_rsvd |\n\t\t\t\t\t\t   gbpages_bit_rsvd |\n\t\t\t\t\t\t   rsvd_bits(13, 29);\n\t\trsvd_check->rsvd_bits_mask[1][1] = high_bits_rsvd |\n\t\t\t\t\t\t   rsvd_bits(13, 20);  \n\t\trsvd_check->rsvd_bits_mask[1][0] =\n\t\t\trsvd_check->rsvd_bits_mask[0][0];\n\t\tbreak;\n\t}\n}\n\nstatic void reset_guest_rsvds_bits_mask(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_mmu *context)\n{\n\t__reset_rsvds_bits_mask(&context->guest_rsvd_check,\n\t\t\t\tvcpu->arch.reserved_gpa_bits,\n\t\t\t\tcontext->cpu_role.base.level, is_efer_nx(context),\n\t\t\t\tguest_can_use(vcpu, X86_FEATURE_GBPAGES),\n\t\t\t\tis_cr4_pse(context),\n\t\t\t\tguest_cpuid_is_amd_or_hygon(vcpu));\n}\n\nstatic void __reset_rsvds_bits_mask_ept(struct rsvd_bits_validate *rsvd_check,\n\t\t\t\t\tu64 pa_bits_rsvd, bool execonly,\n\t\t\t\t\tint huge_page_level)\n{\n\tu64 high_bits_rsvd = pa_bits_rsvd & rsvd_bits(0, 51);\n\tu64 large_1g_rsvd = 0, large_2m_rsvd = 0;\n\tu64 bad_mt_xwr;\n\n\tif (huge_page_level < PG_LEVEL_1G)\n\t\tlarge_1g_rsvd = rsvd_bits(7, 7);\n\tif (huge_page_level < PG_LEVEL_2M)\n\t\tlarge_2m_rsvd = rsvd_bits(7, 7);\n\n\trsvd_check->rsvd_bits_mask[0][4] = high_bits_rsvd | rsvd_bits(3, 7);\n\trsvd_check->rsvd_bits_mask[0][3] = high_bits_rsvd | rsvd_bits(3, 7);\n\trsvd_check->rsvd_bits_mask[0][2] = high_bits_rsvd | rsvd_bits(3, 6) | large_1g_rsvd;\n\trsvd_check->rsvd_bits_mask[0][1] = high_bits_rsvd | rsvd_bits(3, 6) | large_2m_rsvd;\n\trsvd_check->rsvd_bits_mask[0][0] = high_bits_rsvd;\n\n\t \n\trsvd_check->rsvd_bits_mask[1][4] = rsvd_check->rsvd_bits_mask[0][4];\n\trsvd_check->rsvd_bits_mask[1][3] = rsvd_check->rsvd_bits_mask[0][3];\n\trsvd_check->rsvd_bits_mask[1][2] = high_bits_rsvd | rsvd_bits(12, 29) | large_1g_rsvd;\n\trsvd_check->rsvd_bits_mask[1][1] = high_bits_rsvd | rsvd_bits(12, 20) | large_2m_rsvd;\n\trsvd_check->rsvd_bits_mask[1][0] = rsvd_check->rsvd_bits_mask[0][0];\n\n\tbad_mt_xwr = 0xFFull << (2 * 8);\t \n\tbad_mt_xwr |= 0xFFull << (3 * 8);\t \n\tbad_mt_xwr |= 0xFFull << (7 * 8);\t \n\tbad_mt_xwr |= REPEAT_BYTE(1ull << 2);\t \n\tbad_mt_xwr |= REPEAT_BYTE(1ull << 6);\t \n\tif (!execonly) {\n\t\t \n\t\tbad_mt_xwr |= REPEAT_BYTE(1ull << 4);\n\t}\n\trsvd_check->bad_mt_xwr = bad_mt_xwr;\n}\n\nstatic void reset_rsvds_bits_mask_ept(struct kvm_vcpu *vcpu,\n\t\tstruct kvm_mmu *context, bool execonly, int huge_page_level)\n{\n\t__reset_rsvds_bits_mask_ept(&context->guest_rsvd_check,\n\t\t\t\t    vcpu->arch.reserved_gpa_bits, execonly,\n\t\t\t\t    huge_page_level);\n}\n\nstatic inline u64 reserved_hpa_bits(void)\n{\n\treturn rsvd_bits(shadow_phys_bits, 63);\n}\n\n \nstatic void reset_shadow_zero_bits_mask(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_mmu *context)\n{\n\t \n\tbool is_amd = true;\n\t \n\tbool is_pse = false;\n\tstruct rsvd_bits_validate *shadow_zero_check;\n\tint i;\n\n\tWARN_ON_ONCE(context->root_role.level < PT32E_ROOT_LEVEL);\n\n\tshadow_zero_check = &context->shadow_zero_check;\n\t__reset_rsvds_bits_mask(shadow_zero_check, reserved_hpa_bits(),\n\t\t\t\tcontext->root_role.level,\n\t\t\t\tcontext->root_role.efer_nx,\n\t\t\t\tguest_can_use(vcpu, X86_FEATURE_GBPAGES),\n\t\t\t\tis_pse, is_amd);\n\n\tif (!shadow_me_mask)\n\t\treturn;\n\n\tfor (i = context->root_role.level; --i >= 0;) {\n\t\t \n\t\tshadow_zero_check->rsvd_bits_mask[0][i] |= shadow_me_mask;\n\t\tshadow_zero_check->rsvd_bits_mask[1][i] |= shadow_me_mask;\n\t\tshadow_zero_check->rsvd_bits_mask[0][i] &= ~shadow_me_value;\n\t\tshadow_zero_check->rsvd_bits_mask[1][i] &= ~shadow_me_value;\n\t}\n\n}\n\nstatic inline bool boot_cpu_is_amd(void)\n{\n\tWARN_ON_ONCE(!tdp_enabled);\n\treturn shadow_x_mask == 0;\n}\n\n \nstatic void reset_tdp_shadow_zero_bits_mask(struct kvm_mmu *context)\n{\n\tstruct rsvd_bits_validate *shadow_zero_check;\n\tint i;\n\n\tshadow_zero_check = &context->shadow_zero_check;\n\n\tif (boot_cpu_is_amd())\n\t\t__reset_rsvds_bits_mask(shadow_zero_check, reserved_hpa_bits(),\n\t\t\t\t\tcontext->root_role.level, true,\n\t\t\t\t\tboot_cpu_has(X86_FEATURE_GBPAGES),\n\t\t\t\t\tfalse, true);\n\telse\n\t\t__reset_rsvds_bits_mask_ept(shadow_zero_check,\n\t\t\t\t\t    reserved_hpa_bits(), false,\n\t\t\t\t\t    max_huge_page_level);\n\n\tif (!shadow_me_mask)\n\t\treturn;\n\n\tfor (i = context->root_role.level; --i >= 0;) {\n\t\tshadow_zero_check->rsvd_bits_mask[0][i] &= ~shadow_me_mask;\n\t\tshadow_zero_check->rsvd_bits_mask[1][i] &= ~shadow_me_mask;\n\t}\n}\n\n \nstatic void\nreset_ept_shadow_zero_bits_mask(struct kvm_mmu *context, bool execonly)\n{\n\t__reset_rsvds_bits_mask_ept(&context->shadow_zero_check,\n\t\t\t\t    reserved_hpa_bits(), execonly,\n\t\t\t\t    max_huge_page_level);\n}\n\n#define BYTE_MASK(access) \\\n\t((1 & (access) ? 2 : 0) | \\\n\t (2 & (access) ? 4 : 0) | \\\n\t (3 & (access) ? 8 : 0) | \\\n\t (4 & (access) ? 16 : 0) | \\\n\t (5 & (access) ? 32 : 0) | \\\n\t (6 & (access) ? 64 : 0) | \\\n\t (7 & (access) ? 128 : 0))\n\n\nstatic void update_permission_bitmask(struct kvm_mmu *mmu, bool ept)\n{\n\tunsigned byte;\n\n\tconst u8 x = BYTE_MASK(ACC_EXEC_MASK);\n\tconst u8 w = BYTE_MASK(ACC_WRITE_MASK);\n\tconst u8 u = BYTE_MASK(ACC_USER_MASK);\n\n\tbool cr4_smep = is_cr4_smep(mmu);\n\tbool cr4_smap = is_cr4_smap(mmu);\n\tbool cr0_wp = is_cr0_wp(mmu);\n\tbool efer_nx = is_efer_nx(mmu);\n\n\tfor (byte = 0; byte < ARRAY_SIZE(mmu->permissions); ++byte) {\n\t\tunsigned pfec = byte << 1;\n\n\t\t \n\n\t\t \n\t\tu8 wf = (pfec & PFERR_WRITE_MASK) ? (u8)~w : 0;\n\t\t \n\t\tu8 uf = (pfec & PFERR_USER_MASK) ? (u8)~u : 0;\n\t\t \n\t\tu8 ff = (pfec & PFERR_FETCH_MASK) ? (u8)~x : 0;\n\t\t \n\t\tu8 smepf = 0;\n\t\t \n\t\tu8 smapf = 0;\n\n\t\tif (!ept) {\n\t\t\t \n\t\t\tu8 kf = (pfec & PFERR_USER_MASK) ? 0 : u;\n\n\t\t\t \n\t\t\tif (!efer_nx)\n\t\t\t\tff = 0;\n\n\t\t\t \n\t\t\tif (!cr0_wp)\n\t\t\t\twf = (pfec & PFERR_USER_MASK) ? wf : 0;\n\n\t\t\t \n\t\t\tif (cr4_smep)\n\t\t\t\tsmepf = (pfec & PFERR_FETCH_MASK) ? kf : 0;\n\n\t\t\t \n\t\t\tif (cr4_smap)\n\t\t\t\tsmapf = (pfec & (PFERR_RSVD_MASK|PFERR_FETCH_MASK)) ? 0 : kf;\n\t\t}\n\n\t\tmmu->permissions[byte] = ff | uf | wf | smepf | smapf;\n\t}\n}\n\n \nstatic void update_pkru_bitmask(struct kvm_mmu *mmu)\n{\n\tunsigned bit;\n\tbool wp;\n\n\tmmu->pkru_mask = 0;\n\n\tif (!is_cr4_pke(mmu))\n\t\treturn;\n\n\twp = is_cr0_wp(mmu);\n\n\tfor (bit = 0; bit < ARRAY_SIZE(mmu->permissions); ++bit) {\n\t\tunsigned pfec, pkey_bits;\n\t\tbool check_pkey, check_write, ff, uf, wf, pte_user;\n\n\t\tpfec = bit << 1;\n\t\tff = pfec & PFERR_FETCH_MASK;\n\t\tuf = pfec & PFERR_USER_MASK;\n\t\twf = pfec & PFERR_WRITE_MASK;\n\n\t\t \n\t\tpte_user = pfec & PFERR_RSVD_MASK;\n\n\t\t \n\t\tcheck_pkey = (!ff && pte_user);\n\t\t \n\t\tcheck_write = check_pkey && wf && (uf || wp);\n\n\t\t \n\t\tpkey_bits = !!check_pkey;\n\t\t \n\t\tpkey_bits |= (!!check_write) << 1;\n\n\t\tmmu->pkru_mask |= (pkey_bits & 3) << pfec;\n\t}\n}\n\nstatic void reset_guest_paging_metadata(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_mmu *mmu)\n{\n\tif (!is_cr0_pg(mmu))\n\t\treturn;\n\n\treset_guest_rsvds_bits_mask(vcpu, mmu);\n\tupdate_permission_bitmask(mmu, false);\n\tupdate_pkru_bitmask(mmu);\n}\n\nstatic void paging64_init_context(struct kvm_mmu *context)\n{\n\tcontext->page_fault = paging64_page_fault;\n\tcontext->gva_to_gpa = paging64_gva_to_gpa;\n\tcontext->sync_spte = paging64_sync_spte;\n}\n\nstatic void paging32_init_context(struct kvm_mmu *context)\n{\n\tcontext->page_fault = paging32_page_fault;\n\tcontext->gva_to_gpa = paging32_gva_to_gpa;\n\tcontext->sync_spte = paging32_sync_spte;\n}\n\nstatic union kvm_cpu_role kvm_calc_cpu_role(struct kvm_vcpu *vcpu,\n\t\t\t\t\t    const struct kvm_mmu_role_regs *regs)\n{\n\tunion kvm_cpu_role role = {0};\n\n\trole.base.access = ACC_ALL;\n\trole.base.smm = is_smm(vcpu);\n\trole.base.guest_mode = is_guest_mode(vcpu);\n\trole.ext.valid = 1;\n\n\tif (!____is_cr0_pg(regs)) {\n\t\trole.base.direct = 1;\n\t\treturn role;\n\t}\n\n\trole.base.efer_nx = ____is_efer_nx(regs);\n\trole.base.cr0_wp = ____is_cr0_wp(regs);\n\trole.base.smep_andnot_wp = ____is_cr4_smep(regs) && !____is_cr0_wp(regs);\n\trole.base.smap_andnot_wp = ____is_cr4_smap(regs) && !____is_cr0_wp(regs);\n\trole.base.has_4_byte_gpte = !____is_cr4_pae(regs);\n\n\tif (____is_efer_lma(regs))\n\t\trole.base.level = ____is_cr4_la57(regs) ? PT64_ROOT_5LEVEL\n\t\t\t\t\t\t\t: PT64_ROOT_4LEVEL;\n\telse if (____is_cr4_pae(regs))\n\t\trole.base.level = PT32E_ROOT_LEVEL;\n\telse\n\t\trole.base.level = PT32_ROOT_LEVEL;\n\n\trole.ext.cr4_smep = ____is_cr4_smep(regs);\n\trole.ext.cr4_smap = ____is_cr4_smap(regs);\n\trole.ext.cr4_pse = ____is_cr4_pse(regs);\n\n\t \n\trole.ext.cr4_pke = ____is_efer_lma(regs) && ____is_cr4_pke(regs);\n\trole.ext.cr4_la57 = ____is_efer_lma(regs) && ____is_cr4_la57(regs);\n\trole.ext.efer_lma = ____is_efer_lma(regs);\n\treturn role;\n}\n\nvoid __kvm_mmu_refresh_passthrough_bits(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_mmu *mmu)\n{\n\tconst bool cr0_wp = kvm_is_cr0_bit_set(vcpu, X86_CR0_WP);\n\n\tBUILD_BUG_ON((KVM_MMU_CR0_ROLE_BITS & KVM_POSSIBLE_CR0_GUEST_BITS) != X86_CR0_WP);\n\tBUILD_BUG_ON((KVM_MMU_CR4_ROLE_BITS & KVM_POSSIBLE_CR4_GUEST_BITS));\n\n\tif (is_cr0_wp(mmu) == cr0_wp)\n\t\treturn;\n\n\tmmu->cpu_role.base.cr0_wp = cr0_wp;\n\treset_guest_paging_metadata(vcpu, mmu);\n}\n\nstatic inline int kvm_mmu_get_tdp_level(struct kvm_vcpu *vcpu)\n{\n\t \n\tif (tdp_root_level)\n\t\treturn tdp_root_level;\n\n\t \n\tif (max_tdp_level == 5 && cpuid_maxphyaddr(vcpu) <= 48)\n\t\treturn 4;\n\n\treturn max_tdp_level;\n}\n\nstatic union kvm_mmu_page_role\nkvm_calc_tdp_mmu_root_page_role(struct kvm_vcpu *vcpu,\n\t\t\t\tunion kvm_cpu_role cpu_role)\n{\n\tunion kvm_mmu_page_role role = {0};\n\n\trole.access = ACC_ALL;\n\trole.cr0_wp = true;\n\trole.efer_nx = true;\n\trole.smm = cpu_role.base.smm;\n\trole.guest_mode = cpu_role.base.guest_mode;\n\trole.ad_disabled = !kvm_ad_enabled();\n\trole.level = kvm_mmu_get_tdp_level(vcpu);\n\trole.direct = true;\n\trole.has_4_byte_gpte = false;\n\n\treturn role;\n}\n\nstatic void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu,\n\t\t\t     union kvm_cpu_role cpu_role)\n{\n\tstruct kvm_mmu *context = &vcpu->arch.root_mmu;\n\tunion kvm_mmu_page_role root_role = kvm_calc_tdp_mmu_root_page_role(vcpu, cpu_role);\n\n\tif (cpu_role.as_u64 == context->cpu_role.as_u64 &&\n\t    root_role.word == context->root_role.word)\n\t\treturn;\n\n\tcontext->cpu_role.as_u64 = cpu_role.as_u64;\n\tcontext->root_role.word = root_role.word;\n\tcontext->page_fault = kvm_tdp_page_fault;\n\tcontext->sync_spte = NULL;\n\tcontext->get_guest_pgd = get_guest_cr3;\n\tcontext->get_pdptr = kvm_pdptr_read;\n\tcontext->inject_page_fault = kvm_inject_page_fault;\n\n\tif (!is_cr0_pg(context))\n\t\tcontext->gva_to_gpa = nonpaging_gva_to_gpa;\n\telse if (is_cr4_pae(context))\n\t\tcontext->gva_to_gpa = paging64_gva_to_gpa;\n\telse\n\t\tcontext->gva_to_gpa = paging32_gva_to_gpa;\n\n\treset_guest_paging_metadata(vcpu, context);\n\treset_tdp_shadow_zero_bits_mask(context);\n}\n\nstatic void shadow_mmu_init_context(struct kvm_vcpu *vcpu, struct kvm_mmu *context,\n\t\t\t\t    union kvm_cpu_role cpu_role,\n\t\t\t\t    union kvm_mmu_page_role root_role)\n{\n\tif (cpu_role.as_u64 == context->cpu_role.as_u64 &&\n\t    root_role.word == context->root_role.word)\n\t\treturn;\n\n\tcontext->cpu_role.as_u64 = cpu_role.as_u64;\n\tcontext->root_role.word = root_role.word;\n\n\tif (!is_cr0_pg(context))\n\t\tnonpaging_init_context(context);\n\telse if (is_cr4_pae(context))\n\t\tpaging64_init_context(context);\n\telse\n\t\tpaging32_init_context(context);\n\n\treset_guest_paging_metadata(vcpu, context);\n\treset_shadow_zero_bits_mask(vcpu, context);\n}\n\nstatic void kvm_init_shadow_mmu(struct kvm_vcpu *vcpu,\n\t\t\t\tunion kvm_cpu_role cpu_role)\n{\n\tstruct kvm_mmu *context = &vcpu->arch.root_mmu;\n\tunion kvm_mmu_page_role root_role;\n\n\troot_role = cpu_role.base;\n\n\t \n\troot_role.level = max_t(u32, root_role.level, PT32E_ROOT_LEVEL);\n\n\t \n\troot_role.efer_nx = true;\n\n\tshadow_mmu_init_context(vcpu, context, cpu_role, root_role);\n}\n\nvoid kvm_init_shadow_npt_mmu(struct kvm_vcpu *vcpu, unsigned long cr0,\n\t\t\t     unsigned long cr4, u64 efer, gpa_t nested_cr3)\n{\n\tstruct kvm_mmu *context = &vcpu->arch.guest_mmu;\n\tstruct kvm_mmu_role_regs regs = {\n\t\t.cr0 = cr0,\n\t\t.cr4 = cr4 & ~X86_CR4_PKE,\n\t\t.efer = efer,\n\t};\n\tunion kvm_cpu_role cpu_role = kvm_calc_cpu_role(vcpu, &regs);\n\tunion kvm_mmu_page_role root_role;\n\n\t \n\tWARN_ON_ONCE(cpu_role.base.direct);\n\n\troot_role = cpu_role.base;\n\troot_role.level = kvm_mmu_get_tdp_level(vcpu);\n\tif (root_role.level == PT64_ROOT_5LEVEL &&\n\t    cpu_role.base.level == PT64_ROOT_4LEVEL)\n\t\troot_role.passthrough = 1;\n\n\tshadow_mmu_init_context(vcpu, context, cpu_role, root_role);\n\tkvm_mmu_new_pgd(vcpu, nested_cr3);\n}\nEXPORT_SYMBOL_GPL(kvm_init_shadow_npt_mmu);\n\nstatic union kvm_cpu_role\nkvm_calc_shadow_ept_root_page_role(struct kvm_vcpu *vcpu, bool accessed_dirty,\n\t\t\t\t   bool execonly, u8 level)\n{\n\tunion kvm_cpu_role role = {0};\n\n\t \n\tWARN_ON_ONCE(is_smm(vcpu));\n\trole.base.level = level;\n\trole.base.has_4_byte_gpte = false;\n\trole.base.direct = false;\n\trole.base.ad_disabled = !accessed_dirty;\n\trole.base.guest_mode = true;\n\trole.base.access = ACC_ALL;\n\n\trole.ext.word = 0;\n\trole.ext.execonly = execonly;\n\trole.ext.valid = 1;\n\n\treturn role;\n}\n\nvoid kvm_init_shadow_ept_mmu(struct kvm_vcpu *vcpu, bool execonly,\n\t\t\t     int huge_page_level, bool accessed_dirty,\n\t\t\t     gpa_t new_eptp)\n{\n\tstruct kvm_mmu *context = &vcpu->arch.guest_mmu;\n\tu8 level = vmx_eptp_page_walk_level(new_eptp);\n\tunion kvm_cpu_role new_mode =\n\t\tkvm_calc_shadow_ept_root_page_role(vcpu, accessed_dirty,\n\t\t\t\t\t\t   execonly, level);\n\n\tif (new_mode.as_u64 != context->cpu_role.as_u64) {\n\t\t \n\t\tcontext->cpu_role.as_u64 = new_mode.as_u64;\n\t\tcontext->root_role.word = new_mode.base.word;\n\n\t\tcontext->page_fault = ept_page_fault;\n\t\tcontext->gva_to_gpa = ept_gva_to_gpa;\n\t\tcontext->sync_spte = ept_sync_spte;\n\n\t\tupdate_permission_bitmask(context, true);\n\t\tcontext->pkru_mask = 0;\n\t\treset_rsvds_bits_mask_ept(vcpu, context, execonly, huge_page_level);\n\t\treset_ept_shadow_zero_bits_mask(context, execonly);\n\t}\n\n\tkvm_mmu_new_pgd(vcpu, new_eptp);\n}\nEXPORT_SYMBOL_GPL(kvm_init_shadow_ept_mmu);\n\nstatic void init_kvm_softmmu(struct kvm_vcpu *vcpu,\n\t\t\t     union kvm_cpu_role cpu_role)\n{\n\tstruct kvm_mmu *context = &vcpu->arch.root_mmu;\n\n\tkvm_init_shadow_mmu(vcpu, cpu_role);\n\n\tcontext->get_guest_pgd     = get_guest_cr3;\n\tcontext->get_pdptr         = kvm_pdptr_read;\n\tcontext->inject_page_fault = kvm_inject_page_fault;\n}\n\nstatic void init_kvm_nested_mmu(struct kvm_vcpu *vcpu,\n\t\t\t\tunion kvm_cpu_role new_mode)\n{\n\tstruct kvm_mmu *g_context = &vcpu->arch.nested_mmu;\n\n\tif (new_mode.as_u64 == g_context->cpu_role.as_u64)\n\t\treturn;\n\n\tg_context->cpu_role.as_u64   = new_mode.as_u64;\n\tg_context->get_guest_pgd     = get_guest_cr3;\n\tg_context->get_pdptr         = kvm_pdptr_read;\n\tg_context->inject_page_fault = kvm_inject_page_fault;\n\n\t \n\tg_context->sync_spte         = NULL;\n\n\t \n\tif (!is_paging(vcpu))\n\t\tg_context->gva_to_gpa = nonpaging_gva_to_gpa;\n\telse if (is_long_mode(vcpu))\n\t\tg_context->gva_to_gpa = paging64_gva_to_gpa;\n\telse if (is_pae(vcpu))\n\t\tg_context->gva_to_gpa = paging64_gva_to_gpa;\n\telse\n\t\tg_context->gva_to_gpa = paging32_gva_to_gpa;\n\n\treset_guest_paging_metadata(vcpu, g_context);\n}\n\nvoid kvm_init_mmu(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_mmu_role_regs regs = vcpu_to_role_regs(vcpu);\n\tunion kvm_cpu_role cpu_role = kvm_calc_cpu_role(vcpu, &regs);\n\n\tif (mmu_is_nested(vcpu))\n\t\tinit_kvm_nested_mmu(vcpu, cpu_role);\n\telse if (tdp_enabled)\n\t\tinit_kvm_tdp_mmu(vcpu, cpu_role);\n\telse\n\t\tinit_kvm_softmmu(vcpu, cpu_role);\n}\nEXPORT_SYMBOL_GPL(kvm_init_mmu);\n\nvoid kvm_mmu_after_set_cpuid(struct kvm_vcpu *vcpu)\n{\n\t \n\tvcpu->arch.root_mmu.root_role.word = 0;\n\tvcpu->arch.guest_mmu.root_role.word = 0;\n\tvcpu->arch.nested_mmu.root_role.word = 0;\n\tvcpu->arch.root_mmu.cpu_role.ext.valid = 0;\n\tvcpu->arch.guest_mmu.cpu_role.ext.valid = 0;\n\tvcpu->arch.nested_mmu.cpu_role.ext.valid = 0;\n\tkvm_mmu_reset_context(vcpu);\n\n\t \n\tKVM_BUG_ON(kvm_vcpu_has_run(vcpu), vcpu->kvm);\n}\n\nvoid kvm_mmu_reset_context(struct kvm_vcpu *vcpu)\n{\n\tkvm_mmu_unload(vcpu);\n\tkvm_init_mmu(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_mmu_reset_context);\n\nint kvm_mmu_load(struct kvm_vcpu *vcpu)\n{\n\tint r;\n\n\tr = mmu_topup_memory_caches(vcpu, !vcpu->arch.mmu->root_role.direct);\n\tif (r)\n\t\tgoto out;\n\tr = mmu_alloc_special_roots(vcpu);\n\tif (r)\n\t\tgoto out;\n\tif (vcpu->arch.mmu->root_role.direct)\n\t\tr = mmu_alloc_direct_roots(vcpu);\n\telse\n\t\tr = mmu_alloc_shadow_roots(vcpu);\n\tif (r)\n\t\tgoto out;\n\n\tkvm_mmu_sync_roots(vcpu);\n\n\tkvm_mmu_load_pgd(vcpu);\n\n\t \n\tstatic_call(kvm_x86_flush_tlb_current)(vcpu);\nout:\n\treturn r;\n}\n\nvoid kvm_mmu_unload(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\n\tkvm_mmu_free_roots(kvm, &vcpu->arch.root_mmu, KVM_MMU_ROOTS_ALL);\n\tWARN_ON_ONCE(VALID_PAGE(vcpu->arch.root_mmu.root.hpa));\n\tkvm_mmu_free_roots(kvm, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);\n\tWARN_ON_ONCE(VALID_PAGE(vcpu->arch.guest_mmu.root.hpa));\n\tvcpu_clear_mmio_info(vcpu, MMIO_GVA_ANY);\n}\n\nstatic bool is_obsolete_root(struct kvm *kvm, hpa_t root_hpa)\n{\n\tstruct kvm_mmu_page *sp;\n\n\tif (!VALID_PAGE(root_hpa))\n\t\treturn false;\n\n\t \n\tsp = root_to_sp(root_hpa);\n\treturn !sp || is_obsolete_sp(kvm, sp);\n}\n\nstatic void __kvm_mmu_free_obsolete_roots(struct kvm *kvm, struct kvm_mmu *mmu)\n{\n\tunsigned long roots_to_free = 0;\n\tint i;\n\n\tif (is_obsolete_root(kvm, mmu->root.hpa))\n\t\troots_to_free |= KVM_MMU_ROOT_CURRENT;\n\n\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {\n\t\tif (is_obsolete_root(kvm, mmu->prev_roots[i].hpa))\n\t\t\troots_to_free |= KVM_MMU_ROOT_PREVIOUS(i);\n\t}\n\n\tif (roots_to_free)\n\t\tkvm_mmu_free_roots(kvm, mmu, roots_to_free);\n}\n\nvoid kvm_mmu_free_obsolete_roots(struct kvm_vcpu *vcpu)\n{\n\t__kvm_mmu_free_obsolete_roots(vcpu->kvm, &vcpu->arch.root_mmu);\n\t__kvm_mmu_free_obsolete_roots(vcpu->kvm, &vcpu->arch.guest_mmu);\n}\n\nstatic u64 mmu_pte_write_fetch_gpte(struct kvm_vcpu *vcpu, gpa_t *gpa,\n\t\t\t\t    int *bytes)\n{\n\tu64 gentry = 0;\n\tint r;\n\n\t \n\tif (is_pae(vcpu) && *bytes == 4) {\n\t\t \n\t\t*gpa &= ~(gpa_t)7;\n\t\t*bytes = 8;\n\t}\n\n\tif (*bytes == 4 || *bytes == 8) {\n\t\tr = kvm_vcpu_read_guest_atomic(vcpu, *gpa, &gentry, *bytes);\n\t\tif (r)\n\t\t\tgentry = 0;\n\t}\n\n\treturn gentry;\n}\n\n \nstatic bool detect_write_flooding(struct kvm_mmu_page *sp)\n{\n\t \n\tif (sp->role.level == PG_LEVEL_4K)\n\t\treturn false;\n\n\tatomic_inc(&sp->write_flooding_count);\n\treturn atomic_read(&sp->write_flooding_count) >= 3;\n}\n\n \nstatic bool detect_write_misaligned(struct kvm_mmu_page *sp, gpa_t gpa,\n\t\t\t\t    int bytes)\n{\n\tunsigned offset, pte_size, misaligned;\n\n\toffset = offset_in_page(gpa);\n\tpte_size = sp->role.has_4_byte_gpte ? 4 : 8;\n\n\t \n\tif (!(offset & (pte_size - 1)) && bytes == 1)\n\t\treturn false;\n\n\tmisaligned = (offset ^ (offset + bytes - 1)) & ~(pte_size - 1);\n\tmisaligned |= bytes < 4;\n\n\treturn misaligned;\n}\n\nstatic u64 *get_written_sptes(struct kvm_mmu_page *sp, gpa_t gpa, int *nspte)\n{\n\tunsigned page_offset, quadrant;\n\tu64 *spte;\n\tint level;\n\n\tpage_offset = offset_in_page(gpa);\n\tlevel = sp->role.level;\n\t*nspte = 1;\n\tif (sp->role.has_4_byte_gpte) {\n\t\tpage_offset <<= 1;\t \n\t\t \n\t\tif (level == PT32_ROOT_LEVEL) {\n\t\t\tpage_offset &= ~7;  \n\t\t\tpage_offset <<= 1;\n\t\t\t*nspte = 2;\n\t\t}\n\t\tquadrant = page_offset >> PAGE_SHIFT;\n\t\tpage_offset &= ~PAGE_MASK;\n\t\tif (quadrant != sp->role.quadrant)\n\t\t\treturn NULL;\n\t}\n\n\tspte = &sp->spt[page_offset / sizeof(*spte)];\n\treturn spte;\n}\n\nvoid kvm_mmu_track_write(struct kvm_vcpu *vcpu, gpa_t gpa, const u8 *new,\n\t\t\t int bytes)\n{\n\tgfn_t gfn = gpa >> PAGE_SHIFT;\n\tstruct kvm_mmu_page *sp;\n\tLIST_HEAD(invalid_list);\n\tu64 entry, gentry, *spte;\n\tint npte;\n\tbool flush = false;\n\n\t \n\tif (!READ_ONCE(vcpu->kvm->arch.indirect_shadow_pages))\n\t\treturn;\n\n\twrite_lock(&vcpu->kvm->mmu_lock);\n\n\tgentry = mmu_pte_write_fetch_gpte(vcpu, &gpa, &bytes);\n\n\t++vcpu->kvm->stat.mmu_pte_write;\n\n\tfor_each_gfn_valid_sp_with_gptes(vcpu->kvm, sp, gfn) {\n\t\tif (detect_write_misaligned(sp, gpa, bytes) ||\n\t\t      detect_write_flooding(sp)) {\n\t\t\tkvm_mmu_prepare_zap_page(vcpu->kvm, sp, &invalid_list);\n\t\t\t++vcpu->kvm->stat.mmu_flooded;\n\t\t\tcontinue;\n\t\t}\n\n\t\tspte = get_written_sptes(sp, gpa, &npte);\n\t\tif (!spte)\n\t\t\tcontinue;\n\n\t\twhile (npte--) {\n\t\t\tentry = *spte;\n\t\t\tmmu_page_zap_pte(vcpu->kvm, sp, spte, NULL);\n\t\t\tif (gentry && sp->role.level != PG_LEVEL_4K)\n\t\t\t\t++vcpu->kvm->stat.mmu_pde_zapped;\n\t\t\tif (is_shadow_present_pte(entry))\n\t\t\t\tflush = true;\n\t\t\t++spte;\n\t\t}\n\t}\n\tkvm_mmu_remote_flush_or_zap(vcpu->kvm, &invalid_list, flush);\n\twrite_unlock(&vcpu->kvm->mmu_lock);\n}\n\nint noinline kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,\n\t\t       void *insn, int insn_len)\n{\n\tint r, emulation_type = EMULTYPE_PF;\n\tbool direct = vcpu->arch.mmu->root_role.direct;\n\n\t \n\tif (WARN_ON_ONCE(error_code & PFERR_IMPLICIT_ACCESS))\n\t\terror_code &= ~PFERR_IMPLICIT_ACCESS;\n\n\tif (WARN_ON_ONCE(!VALID_PAGE(vcpu->arch.mmu->root.hpa)))\n\t\treturn RET_PF_RETRY;\n\n\tr = RET_PF_INVALID;\n\tif (unlikely(error_code & PFERR_RSVD_MASK)) {\n\t\tr = handle_mmio_page_fault(vcpu, cr2_or_gpa, direct);\n\t\tif (r == RET_PF_EMULATE)\n\t\t\tgoto emulate;\n\t}\n\n\tif (r == RET_PF_INVALID) {\n\t\tr = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa,\n\t\t\t\t\t  lower_32_bits(error_code), false,\n\t\t\t\t\t  &emulation_type);\n\t\tif (KVM_BUG_ON(r == RET_PF_INVALID, vcpu->kvm))\n\t\t\treturn -EIO;\n\t}\n\n\tif (r < 0)\n\t\treturn r;\n\tif (r != RET_PF_EMULATE)\n\t\treturn 1;\n\n\t \n\tif (vcpu->arch.mmu->root_role.direct &&\n\t    (error_code & PFERR_NESTED_GUEST_PAGE) == PFERR_NESTED_GUEST_PAGE) {\n\t\tkvm_mmu_unprotect_page(vcpu->kvm, gpa_to_gfn(cr2_or_gpa));\n\t\treturn 1;\n\t}\n\n\t \n\tif (!mmio_info_in_cache(vcpu, cr2_or_gpa, direct) && !is_guest_mode(vcpu))\n\t\temulation_type |= EMULTYPE_ALLOW_RETRY_PF;\nemulate:\n\treturn x86_emulate_instruction(vcpu, cr2_or_gpa, emulation_type, insn,\n\t\t\t\t       insn_len);\n}\nEXPORT_SYMBOL_GPL(kvm_mmu_page_fault);\n\nstatic void __kvm_mmu_invalidate_addr(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,\n\t\t\t\t      u64 addr, hpa_t root_hpa)\n{\n\tstruct kvm_shadow_walk_iterator iterator;\n\n\tvcpu_clear_mmio_info(vcpu, addr);\n\n\t \n\tif (WARN_ON_ONCE(mmu != vcpu->arch.mmu))\n\t\treturn;\n\n\tif (!VALID_PAGE(root_hpa))\n\t\treturn;\n\n\twrite_lock(&vcpu->kvm->mmu_lock);\n\tfor_each_shadow_entry_using_root(vcpu, root_hpa, addr, iterator) {\n\t\tstruct kvm_mmu_page *sp = sptep_to_sp(iterator.sptep);\n\n\t\tif (sp->unsync) {\n\t\t\tint ret = kvm_sync_spte(vcpu, sp, iterator.index);\n\n\t\t\tif (ret < 0)\n\t\t\t\tmmu_page_zap_pte(vcpu->kvm, sp, iterator.sptep, NULL);\n\t\t\tif (ret)\n\t\t\t\tkvm_flush_remote_tlbs_sptep(vcpu->kvm, iterator.sptep);\n\t\t}\n\n\t\tif (!sp->unsync_children)\n\t\t\tbreak;\n\t}\n\twrite_unlock(&vcpu->kvm->mmu_lock);\n}\n\nvoid kvm_mmu_invalidate_addr(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu,\n\t\t\t     u64 addr, unsigned long roots)\n{\n\tint i;\n\n\tWARN_ON_ONCE(roots & ~KVM_MMU_ROOTS_ALL);\n\n\t \n\tif (mmu != &vcpu->arch.guest_mmu) {\n\t\t \n\t\tif (is_noncanonical_address(addr, vcpu))\n\t\t\treturn;\n\n\t\tstatic_call(kvm_x86_flush_tlb_gva)(vcpu, addr);\n\t}\n\n\tif (!mmu->sync_spte)\n\t\treturn;\n\n\tif (roots & KVM_MMU_ROOT_CURRENT)\n\t\t__kvm_mmu_invalidate_addr(vcpu, mmu, addr, mmu->root.hpa);\n\n\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {\n\t\tif (roots & KVM_MMU_ROOT_PREVIOUS(i))\n\t\t\t__kvm_mmu_invalidate_addr(vcpu, mmu, addr, mmu->prev_roots[i].hpa);\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_mmu_invalidate_addr);\n\nvoid kvm_mmu_invlpg(struct kvm_vcpu *vcpu, gva_t gva)\n{\n\t \n\tkvm_mmu_invalidate_addr(vcpu, vcpu->arch.walk_mmu, gva, KVM_MMU_ROOTS_ALL);\n\t++vcpu->stat.invlpg;\n}\nEXPORT_SYMBOL_GPL(kvm_mmu_invlpg);\n\n\nvoid kvm_mmu_invpcid_gva(struct kvm_vcpu *vcpu, gva_t gva, unsigned long pcid)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tunsigned long roots = 0;\n\tuint i;\n\n\tif (pcid == kvm_get_active_pcid(vcpu))\n\t\troots |= KVM_MMU_ROOT_CURRENT;\n\n\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {\n\t\tif (VALID_PAGE(mmu->prev_roots[i].hpa) &&\n\t\t    pcid == kvm_get_pcid(vcpu, mmu->prev_roots[i].pgd))\n\t\t\troots |= KVM_MMU_ROOT_PREVIOUS(i);\n\t}\n\n\tif (roots)\n\t\tkvm_mmu_invalidate_addr(vcpu, mmu, gva, roots);\n\t++vcpu->stat.invlpg;\n\n\t \n}\n\nvoid kvm_configure_mmu(bool enable_tdp, int tdp_forced_root_level,\n\t\t       int tdp_max_root_level, int tdp_huge_page_level)\n{\n\ttdp_enabled = enable_tdp;\n\ttdp_root_level = tdp_forced_root_level;\n\tmax_tdp_level = tdp_max_root_level;\n\n#ifdef CONFIG_X86_64\n\ttdp_mmu_enabled = tdp_mmu_allowed && tdp_enabled;\n#endif\n\t \n\tif (tdp_enabled)\n\t\tmax_huge_page_level = tdp_huge_page_level;\n\telse if (boot_cpu_has(X86_FEATURE_GBPAGES))\n\t\tmax_huge_page_level = PG_LEVEL_1G;\n\telse\n\t\tmax_huge_page_level = PG_LEVEL_2M;\n}\nEXPORT_SYMBOL_GPL(kvm_configure_mmu);\n\n \ntypedef bool (*slot_rmaps_handler) (struct kvm *kvm,\n\t\t\t\t    struct kvm_rmap_head *rmap_head,\n\t\t\t\t    const struct kvm_memory_slot *slot);\n\nstatic __always_inline bool __walk_slot_rmaps(struct kvm *kvm,\n\t\t\t\t\t      const struct kvm_memory_slot *slot,\n\t\t\t\t\t      slot_rmaps_handler fn,\n\t\t\t\t\t      int start_level, int end_level,\n\t\t\t\t\t      gfn_t start_gfn, gfn_t end_gfn,\n\t\t\t\t\t      bool flush_on_yield, bool flush)\n{\n\tstruct slot_rmap_walk_iterator iterator;\n\n\tlockdep_assert_held_write(&kvm->mmu_lock);\n\n\tfor_each_slot_rmap_range(slot, start_level, end_level, start_gfn,\n\t\t\tend_gfn, &iterator) {\n\t\tif (iterator.rmap)\n\t\t\tflush |= fn(kvm, iterator.rmap, slot);\n\n\t\tif (need_resched() || rwlock_needbreak(&kvm->mmu_lock)) {\n\t\t\tif (flush && flush_on_yield) {\n\t\t\t\tkvm_flush_remote_tlbs_range(kvm, start_gfn,\n\t\t\t\t\t\t\t    iterator.gfn - start_gfn + 1);\n\t\t\t\tflush = false;\n\t\t\t}\n\t\t\tcond_resched_rwlock_write(&kvm->mmu_lock);\n\t\t}\n\t}\n\n\treturn flush;\n}\n\nstatic __always_inline bool walk_slot_rmaps(struct kvm *kvm,\n\t\t\t\t\t    const struct kvm_memory_slot *slot,\n\t\t\t\t\t    slot_rmaps_handler fn,\n\t\t\t\t\t    int start_level, int end_level,\n\t\t\t\t\t    bool flush_on_yield)\n{\n\treturn __walk_slot_rmaps(kvm, slot, fn, start_level, end_level,\n\t\t\t\t slot->base_gfn, slot->base_gfn + slot->npages - 1,\n\t\t\t\t flush_on_yield, false);\n}\n\nstatic __always_inline bool walk_slot_rmaps_4k(struct kvm *kvm,\n\t\t\t\t\t       const struct kvm_memory_slot *slot,\n\t\t\t\t\t       slot_rmaps_handler fn,\n\t\t\t\t\t       bool flush_on_yield)\n{\n\treturn walk_slot_rmaps(kvm, slot, fn, PG_LEVEL_4K, PG_LEVEL_4K, flush_on_yield);\n}\n\nstatic void free_mmu_pages(struct kvm_mmu *mmu)\n{\n\tif (!tdp_enabled && mmu->pae_root)\n\t\tset_memory_encrypted((unsigned long)mmu->pae_root, 1);\n\tfree_page((unsigned long)mmu->pae_root);\n\tfree_page((unsigned long)mmu->pml4_root);\n\tfree_page((unsigned long)mmu->pml5_root);\n}\n\nstatic int __kvm_mmu_create(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu)\n{\n\tstruct page *page;\n\tint i;\n\n\tmmu->root.hpa = INVALID_PAGE;\n\tmmu->root.pgd = 0;\n\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++)\n\t\tmmu->prev_roots[i] = KVM_MMU_ROOT_INFO_INVALID;\n\n\t \n\tif (!tdp_enabled && mmu == &vcpu->arch.guest_mmu)\n\t\treturn 0;\n\n\t \n\tif (tdp_enabled && kvm_mmu_get_tdp_level(vcpu) > PT32E_ROOT_LEVEL)\n\t\treturn 0;\n\n\tpage = alloc_page(GFP_KERNEL_ACCOUNT | __GFP_DMA32);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tmmu->pae_root = page_address(page);\n\n\t \n\tif (!tdp_enabled)\n\t\tset_memory_decrypted((unsigned long)mmu->pae_root, 1);\n\telse\n\t\tWARN_ON_ONCE(shadow_me_value);\n\n\tfor (i = 0; i < 4; ++i)\n\t\tmmu->pae_root[i] = INVALID_PAE_ROOT;\n\n\treturn 0;\n}\n\nint kvm_mmu_create(struct kvm_vcpu *vcpu)\n{\n\tint ret;\n\n\tvcpu->arch.mmu_pte_list_desc_cache.kmem_cache = pte_list_desc_cache;\n\tvcpu->arch.mmu_pte_list_desc_cache.gfp_zero = __GFP_ZERO;\n\n\tvcpu->arch.mmu_page_header_cache.kmem_cache = mmu_page_header_cache;\n\tvcpu->arch.mmu_page_header_cache.gfp_zero = __GFP_ZERO;\n\n\tvcpu->arch.mmu_shadow_page_cache.gfp_zero = __GFP_ZERO;\n\n\tvcpu->arch.mmu = &vcpu->arch.root_mmu;\n\tvcpu->arch.walk_mmu = &vcpu->arch.root_mmu;\n\n\tret = __kvm_mmu_create(vcpu, &vcpu->arch.guest_mmu);\n\tif (ret)\n\t\treturn ret;\n\n\tret = __kvm_mmu_create(vcpu, &vcpu->arch.root_mmu);\n\tif (ret)\n\t\tgoto fail_allocate_root;\n\n\treturn ret;\n fail_allocate_root:\n\tfree_mmu_pages(&vcpu->arch.guest_mmu);\n\treturn ret;\n}\n\n#define BATCH_ZAP_PAGES\t10\nstatic void kvm_zap_obsolete_pages(struct kvm *kvm)\n{\n\tstruct kvm_mmu_page *sp, *node;\n\tint nr_zapped, batch = 0;\n\tbool unstable;\n\nrestart:\n\tlist_for_each_entry_safe_reverse(sp, node,\n\t      &kvm->arch.active_mmu_pages, link) {\n\t\t \n\t\tif (!is_obsolete_sp(kvm, sp))\n\t\t\tbreak;\n\n\t\t \n\t\tif (WARN_ON_ONCE(sp->role.invalid))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (batch >= BATCH_ZAP_PAGES &&\n\t\t    cond_resched_rwlock_write(&kvm->mmu_lock)) {\n\t\t\tbatch = 0;\n\t\t\tgoto restart;\n\t\t}\n\n\t\tunstable = __kvm_mmu_prepare_zap_page(kvm, sp,\n\t\t\t\t&kvm->arch.zapped_obsolete_pages, &nr_zapped);\n\t\tbatch += nr_zapped;\n\n\t\tif (unstable)\n\t\t\tgoto restart;\n\t}\n\n\t \n\tkvm_mmu_commit_zap_page(kvm, &kvm->arch.zapped_obsolete_pages);\n}\n\n \nstatic void kvm_mmu_zap_all_fast(struct kvm *kvm)\n{\n\tlockdep_assert_held(&kvm->slots_lock);\n\n\twrite_lock(&kvm->mmu_lock);\n\ttrace_kvm_mmu_zap_all_fast(kvm);\n\n\t \n\tkvm->arch.mmu_valid_gen = kvm->arch.mmu_valid_gen ? 0 : 1;\n\n\t \n\tif (tdp_mmu_enabled)\n\t\tkvm_tdp_mmu_invalidate_all_roots(kvm);\n\n\t \n\tkvm_make_all_cpus_request(kvm, KVM_REQ_MMU_FREE_OBSOLETE_ROOTS);\n\n\tkvm_zap_obsolete_pages(kvm);\n\n\twrite_unlock(&kvm->mmu_lock);\n\n\t \n\tif (tdp_mmu_enabled)\n\t\tkvm_tdp_mmu_zap_invalidated_roots(kvm);\n}\n\nstatic bool kvm_has_zapped_obsolete_pages(struct kvm *kvm)\n{\n\treturn unlikely(!list_empty_careful(&kvm->arch.zapped_obsolete_pages));\n}\n\nvoid kvm_mmu_init_vm(struct kvm *kvm)\n{\n\tINIT_LIST_HEAD(&kvm->arch.active_mmu_pages);\n\tINIT_LIST_HEAD(&kvm->arch.zapped_obsolete_pages);\n\tINIT_LIST_HEAD(&kvm->arch.possible_nx_huge_pages);\n\tspin_lock_init(&kvm->arch.mmu_unsync_pages_lock);\n\n\tif (tdp_mmu_enabled)\n\t\tkvm_mmu_init_tdp_mmu(kvm);\n\n\tkvm->arch.split_page_header_cache.kmem_cache = mmu_page_header_cache;\n\tkvm->arch.split_page_header_cache.gfp_zero = __GFP_ZERO;\n\n\tkvm->arch.split_shadow_page_cache.gfp_zero = __GFP_ZERO;\n\n\tkvm->arch.split_desc_cache.kmem_cache = pte_list_desc_cache;\n\tkvm->arch.split_desc_cache.gfp_zero = __GFP_ZERO;\n}\n\nstatic void mmu_free_vm_memory_caches(struct kvm *kvm)\n{\n\tkvm_mmu_free_memory_cache(&kvm->arch.split_desc_cache);\n\tkvm_mmu_free_memory_cache(&kvm->arch.split_page_header_cache);\n\tkvm_mmu_free_memory_cache(&kvm->arch.split_shadow_page_cache);\n}\n\nvoid kvm_mmu_uninit_vm(struct kvm *kvm)\n{\n\tif (tdp_mmu_enabled)\n\t\tkvm_mmu_uninit_tdp_mmu(kvm);\n\n\tmmu_free_vm_memory_caches(kvm);\n}\n\nstatic bool kvm_rmap_zap_gfn_range(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end)\n{\n\tconst struct kvm_memory_slot *memslot;\n\tstruct kvm_memslots *slots;\n\tstruct kvm_memslot_iter iter;\n\tbool flush = false;\n\tgfn_t start, end;\n\tint i;\n\n\tif (!kvm_memslots_have_rmaps(kvm))\n\t\treturn flush;\n\n\tfor (i = 0; i < KVM_ADDRESS_SPACE_NUM; i++) {\n\t\tslots = __kvm_memslots(kvm, i);\n\n\t\tkvm_for_each_memslot_in_gfn_range(&iter, slots, gfn_start, gfn_end) {\n\t\t\tmemslot = iter.slot;\n\t\t\tstart = max(gfn_start, memslot->base_gfn);\n\t\t\tend = min(gfn_end, memslot->base_gfn + memslot->npages);\n\t\t\tif (WARN_ON_ONCE(start >= end))\n\t\t\t\tcontinue;\n\n\t\t\tflush = __walk_slot_rmaps(kvm, memslot, __kvm_zap_rmap,\n\t\t\t\t\t\t  PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL,\n\t\t\t\t\t\t  start, end - 1, true, flush);\n\t\t}\n\t}\n\n\treturn flush;\n}\n\n \nvoid kvm_zap_gfn_range(struct kvm *kvm, gfn_t gfn_start, gfn_t gfn_end)\n{\n\tbool flush;\n\n\tif (WARN_ON_ONCE(gfn_end <= gfn_start))\n\t\treturn;\n\n\twrite_lock(&kvm->mmu_lock);\n\n\tkvm_mmu_invalidate_begin(kvm, 0, -1ul);\n\n\tflush = kvm_rmap_zap_gfn_range(kvm, gfn_start, gfn_end);\n\n\tif (tdp_mmu_enabled)\n\t\tflush = kvm_tdp_mmu_zap_leafs(kvm, gfn_start, gfn_end, flush);\n\n\tif (flush)\n\t\tkvm_flush_remote_tlbs_range(kvm, gfn_start, gfn_end - gfn_start);\n\n\tkvm_mmu_invalidate_end(kvm, 0, -1ul);\n\n\twrite_unlock(&kvm->mmu_lock);\n}\n\nstatic bool slot_rmap_write_protect(struct kvm *kvm,\n\t\t\t\t    struct kvm_rmap_head *rmap_head,\n\t\t\t\t    const struct kvm_memory_slot *slot)\n{\n\treturn rmap_write_protect(rmap_head, false);\n}\n\nvoid kvm_mmu_slot_remove_write_access(struct kvm *kvm,\n\t\t\t\t      const struct kvm_memory_slot *memslot,\n\t\t\t\t      int start_level)\n{\n\tif (kvm_memslots_have_rmaps(kvm)) {\n\t\twrite_lock(&kvm->mmu_lock);\n\t\twalk_slot_rmaps(kvm, memslot, slot_rmap_write_protect,\n\t\t\t\tstart_level, KVM_MAX_HUGEPAGE_LEVEL, false);\n\t\twrite_unlock(&kvm->mmu_lock);\n\t}\n\n\tif (tdp_mmu_enabled) {\n\t\tread_lock(&kvm->mmu_lock);\n\t\tkvm_tdp_mmu_wrprot_slot(kvm, memslot, start_level);\n\t\tread_unlock(&kvm->mmu_lock);\n\t}\n}\n\nstatic inline bool need_topup(struct kvm_mmu_memory_cache *cache, int min)\n{\n\treturn kvm_mmu_memory_cache_nr_free_objects(cache) < min;\n}\n\nstatic bool need_topup_split_caches_or_resched(struct kvm *kvm)\n{\n\tif (need_resched() || rwlock_needbreak(&kvm->mmu_lock))\n\t\treturn true;\n\n\t \n\treturn need_topup(&kvm->arch.split_desc_cache, SPLIT_DESC_CACHE_MIN_NR_OBJECTS) ||\n\t       need_topup(&kvm->arch.split_page_header_cache, 1) ||\n\t       need_topup(&kvm->arch.split_shadow_page_cache, 1);\n}\n\nstatic int topup_split_caches(struct kvm *kvm)\n{\n\t \n\tconst int capacity = SPLIT_DESC_CACHE_MIN_NR_OBJECTS +\n\t\t\t     KVM_ARCH_NR_OBJS_PER_MEMORY_CACHE;\n\tint r;\n\n\tlockdep_assert_held(&kvm->slots_lock);\n\n\tr = __kvm_mmu_topup_memory_cache(&kvm->arch.split_desc_cache, capacity,\n\t\t\t\t\t SPLIT_DESC_CACHE_MIN_NR_OBJECTS);\n\tif (r)\n\t\treturn r;\n\n\tr = kvm_mmu_topup_memory_cache(&kvm->arch.split_page_header_cache, 1);\n\tif (r)\n\t\treturn r;\n\n\treturn kvm_mmu_topup_memory_cache(&kvm->arch.split_shadow_page_cache, 1);\n}\n\nstatic struct kvm_mmu_page *shadow_mmu_get_sp_for_split(struct kvm *kvm, u64 *huge_sptep)\n{\n\tstruct kvm_mmu_page *huge_sp = sptep_to_sp(huge_sptep);\n\tstruct shadow_page_caches caches = {};\n\tunion kvm_mmu_page_role role;\n\tunsigned int access;\n\tgfn_t gfn;\n\n\tgfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));\n\taccess = kvm_mmu_page_get_access(huge_sp, spte_index(huge_sptep));\n\n\t \n\trole = kvm_mmu_child_role(huge_sptep,  true, access);\n\n\t \n\tcaches.page_header_cache = &kvm->arch.split_page_header_cache;\n\tcaches.shadow_page_cache = &kvm->arch.split_shadow_page_cache;\n\n\t \n\treturn __kvm_mmu_get_shadow_page(kvm, NULL, &caches, gfn, role);\n}\n\nstatic void shadow_mmu_split_huge_page(struct kvm *kvm,\n\t\t\t\t       const struct kvm_memory_slot *slot,\n\t\t\t\t       u64 *huge_sptep)\n\n{\n\tstruct kvm_mmu_memory_cache *cache = &kvm->arch.split_desc_cache;\n\tu64 huge_spte = READ_ONCE(*huge_sptep);\n\tstruct kvm_mmu_page *sp;\n\tbool flush = false;\n\tu64 *sptep, spte;\n\tgfn_t gfn;\n\tint index;\n\n\tsp = shadow_mmu_get_sp_for_split(kvm, huge_sptep);\n\n\tfor (index = 0; index < SPTE_ENT_PER_PAGE; index++) {\n\t\tsptep = &sp->spt[index];\n\t\tgfn = kvm_mmu_page_get_gfn(sp, index);\n\n\t\t \n\t\tif (is_shadow_present_pte(*sptep)) {\n\t\t\tflush |= !is_last_spte(*sptep, sp->role.level);\n\t\t\tcontinue;\n\t\t}\n\n\t\tspte = make_huge_page_split_spte(kvm, huge_spte, sp->role, index);\n\t\tmmu_spte_set(sptep, spte);\n\t\t__rmap_add(kvm, cache, slot, sptep, gfn, sp->role.access);\n\t}\n\n\t__link_shadow_page(kvm, cache, huge_sptep, sp, flush);\n}\n\nstatic int shadow_mmu_try_split_huge_page(struct kvm *kvm,\n\t\t\t\t\t  const struct kvm_memory_slot *slot,\n\t\t\t\t\t  u64 *huge_sptep)\n{\n\tstruct kvm_mmu_page *huge_sp = sptep_to_sp(huge_sptep);\n\tint level, r = 0;\n\tgfn_t gfn;\n\tu64 spte;\n\n\t \n\tgfn = kvm_mmu_page_get_gfn(huge_sp, spte_index(huge_sptep));\n\tlevel = huge_sp->role.level;\n\tspte = *huge_sptep;\n\n\tif (kvm_mmu_available_pages(kvm) <= KVM_MIN_FREE_MMU_PAGES) {\n\t\tr = -ENOSPC;\n\t\tgoto out;\n\t}\n\n\tif (need_topup_split_caches_or_resched(kvm)) {\n\t\twrite_unlock(&kvm->mmu_lock);\n\t\tcond_resched();\n\t\t \n\t\tr = topup_split_caches(kvm) ?: -EAGAIN;\n\t\twrite_lock(&kvm->mmu_lock);\n\t\tgoto out;\n\t}\n\n\tshadow_mmu_split_huge_page(kvm, slot, huge_sptep);\n\nout:\n\ttrace_kvm_mmu_split_huge_page(gfn, spte, level, r);\n\treturn r;\n}\n\nstatic bool shadow_mmu_try_split_huge_pages(struct kvm *kvm,\n\t\t\t\t\t    struct kvm_rmap_head *rmap_head,\n\t\t\t\t\t    const struct kvm_memory_slot *slot)\n{\n\tstruct rmap_iterator iter;\n\tstruct kvm_mmu_page *sp;\n\tu64 *huge_sptep;\n\tint r;\n\nrestart:\n\tfor_each_rmap_spte(rmap_head, &iter, huge_sptep) {\n\t\tsp = sptep_to_sp(huge_sptep);\n\n\t\t \n\t\tif (WARN_ON_ONCE(!sp->role.guest_mode))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (WARN_ON_ONCE(!is_large_pte(*huge_sptep)))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (WARN_ON_ONCE(sp->unsync))\n\t\t\tcontinue;\n\n\t\t \n\t\tif (sp->role.invalid)\n\t\t\tcontinue;\n\n\t\tr = shadow_mmu_try_split_huge_page(kvm, slot, huge_sptep);\n\n\t\t \n\t\tif (!r || r == -EAGAIN)\n\t\t\tgoto restart;\n\n\t\t \n\t\tbreak;\n\t}\n\n\treturn false;\n}\n\nstatic void kvm_shadow_mmu_try_split_huge_pages(struct kvm *kvm,\n\t\t\t\t\t\tconst struct kvm_memory_slot *slot,\n\t\t\t\t\t\tgfn_t start, gfn_t end,\n\t\t\t\t\t\tint target_level)\n{\n\tint level;\n\n\t \n\tfor (level = KVM_MAX_HUGEPAGE_LEVEL; level > target_level; level--)\n\t\t__walk_slot_rmaps(kvm, slot, shadow_mmu_try_split_huge_pages,\n\t\t\t\t  level, level, start, end - 1, true, false);\n}\n\n \nvoid kvm_mmu_try_split_huge_pages(struct kvm *kvm,\n\t\t\t\t   const struct kvm_memory_slot *memslot,\n\t\t\t\t   u64 start, u64 end,\n\t\t\t\t   int target_level)\n{\n\tif (!tdp_mmu_enabled)\n\t\treturn;\n\n\tif (kvm_memslots_have_rmaps(kvm))\n\t\tkvm_shadow_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level);\n\n\tkvm_tdp_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level, false);\n\n\t \n}\n\nvoid kvm_mmu_slot_try_split_huge_pages(struct kvm *kvm,\n\t\t\t\t\tconst struct kvm_memory_slot *memslot,\n\t\t\t\t\tint target_level)\n{\n\tu64 start = memslot->base_gfn;\n\tu64 end = start + memslot->npages;\n\n\tif (!tdp_mmu_enabled)\n\t\treturn;\n\n\tif (kvm_memslots_have_rmaps(kvm)) {\n\t\twrite_lock(&kvm->mmu_lock);\n\t\tkvm_shadow_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level);\n\t\twrite_unlock(&kvm->mmu_lock);\n\t}\n\n\tread_lock(&kvm->mmu_lock);\n\tkvm_tdp_mmu_try_split_huge_pages(kvm, memslot, start, end, target_level, true);\n\tread_unlock(&kvm->mmu_lock);\n\n\t \n}\n\nstatic bool kvm_mmu_zap_collapsible_spte(struct kvm *kvm,\n\t\t\t\t\t struct kvm_rmap_head *rmap_head,\n\t\t\t\t\t const struct kvm_memory_slot *slot)\n{\n\tu64 *sptep;\n\tstruct rmap_iterator iter;\n\tint need_tlb_flush = 0;\n\tstruct kvm_mmu_page *sp;\n\nrestart:\n\tfor_each_rmap_spte(rmap_head, &iter, sptep) {\n\t\tsp = sptep_to_sp(sptep);\n\n\t\t \n\t\tif (sp->role.direct &&\n\t\t    sp->role.level < kvm_mmu_max_mapping_level(kvm, slot, sp->gfn,\n\t\t\t\t\t\t\t       PG_LEVEL_NUM)) {\n\t\t\tkvm_zap_one_rmap_spte(kvm, rmap_head, sptep);\n\n\t\t\tif (kvm_available_flush_remote_tlbs_range())\n\t\t\t\tkvm_flush_remote_tlbs_sptep(kvm, sptep);\n\t\t\telse\n\t\t\t\tneed_tlb_flush = 1;\n\n\t\t\tgoto restart;\n\t\t}\n\t}\n\n\treturn need_tlb_flush;\n}\n\nstatic void kvm_rmap_zap_collapsible_sptes(struct kvm *kvm,\n\t\t\t\t\t   const struct kvm_memory_slot *slot)\n{\n\t \n\tif (walk_slot_rmaps(kvm, slot, kvm_mmu_zap_collapsible_spte,\n\t\t\t    PG_LEVEL_4K, KVM_MAX_HUGEPAGE_LEVEL - 1, true))\n\t\tkvm_flush_remote_tlbs_memslot(kvm, slot);\n}\n\nvoid kvm_mmu_zap_collapsible_sptes(struct kvm *kvm,\n\t\t\t\t   const struct kvm_memory_slot *slot)\n{\n\tif (kvm_memslots_have_rmaps(kvm)) {\n\t\twrite_lock(&kvm->mmu_lock);\n\t\tkvm_rmap_zap_collapsible_sptes(kvm, slot);\n\t\twrite_unlock(&kvm->mmu_lock);\n\t}\n\n\tif (tdp_mmu_enabled) {\n\t\tread_lock(&kvm->mmu_lock);\n\t\tkvm_tdp_mmu_zap_collapsible_sptes(kvm, slot);\n\t\tread_unlock(&kvm->mmu_lock);\n\t}\n}\n\nvoid kvm_mmu_slot_leaf_clear_dirty(struct kvm *kvm,\n\t\t\t\t   const struct kvm_memory_slot *memslot)\n{\n\tif (kvm_memslots_have_rmaps(kvm)) {\n\t\twrite_lock(&kvm->mmu_lock);\n\t\t \n\t\twalk_slot_rmaps_4k(kvm, memslot, __rmap_clear_dirty, false);\n\t\twrite_unlock(&kvm->mmu_lock);\n\t}\n\n\tif (tdp_mmu_enabled) {\n\t\tread_lock(&kvm->mmu_lock);\n\t\tkvm_tdp_mmu_clear_dirty_slot(kvm, memslot);\n\t\tread_unlock(&kvm->mmu_lock);\n\t}\n\n\t \n}\n\nstatic void kvm_mmu_zap_all(struct kvm *kvm)\n{\n\tstruct kvm_mmu_page *sp, *node;\n\tLIST_HEAD(invalid_list);\n\tint ign;\n\n\twrite_lock(&kvm->mmu_lock);\nrestart:\n\tlist_for_each_entry_safe(sp, node, &kvm->arch.active_mmu_pages, link) {\n\t\tif (WARN_ON_ONCE(sp->role.invalid))\n\t\t\tcontinue;\n\t\tif (__kvm_mmu_prepare_zap_page(kvm, sp, &invalid_list, &ign))\n\t\t\tgoto restart;\n\t\tif (cond_resched_rwlock_write(&kvm->mmu_lock))\n\t\t\tgoto restart;\n\t}\n\n\tkvm_mmu_commit_zap_page(kvm, &invalid_list);\n\n\tif (tdp_mmu_enabled)\n\t\tkvm_tdp_mmu_zap_all(kvm);\n\n\twrite_unlock(&kvm->mmu_lock);\n}\n\nvoid kvm_arch_flush_shadow_all(struct kvm *kvm)\n{\n\tkvm_mmu_zap_all(kvm);\n}\n\nvoid kvm_arch_flush_shadow_memslot(struct kvm *kvm,\n\t\t\t\t   struct kvm_memory_slot *slot)\n{\n\tkvm_mmu_zap_all_fast(kvm);\n}\n\nvoid kvm_mmu_invalidate_mmio_sptes(struct kvm *kvm, u64 gen)\n{\n\tWARN_ON_ONCE(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS);\n\n\tgen &= MMIO_SPTE_GEN_MASK;\n\n\t \n\tgen &= ~((u64)KVM_ADDRESS_SPACE_NUM - 1);\n\n\t \n\tif (unlikely(gen == 0)) {\n\t\tkvm_debug_ratelimited(\"zapping shadow pages for mmio generation wraparound\\n\");\n\t\tkvm_mmu_zap_all_fast(kvm);\n\t}\n}\n\nstatic unsigned long mmu_shrink_scan(struct shrinker *shrink,\n\t\t\t\t     struct shrink_control *sc)\n{\n\tstruct kvm *kvm;\n\tint nr_to_scan = sc->nr_to_scan;\n\tunsigned long freed = 0;\n\n\tmutex_lock(&kvm_lock);\n\n\tlist_for_each_entry(kvm, &vm_list, vm_list) {\n\t\tint idx;\n\t\tLIST_HEAD(invalid_list);\n\n\t\t \n\t\tif (!nr_to_scan--)\n\t\t\tbreak;\n\t\t \n\t\tif (!kvm->arch.n_used_mmu_pages &&\n\t\t    !kvm_has_zapped_obsolete_pages(kvm))\n\t\t\tcontinue;\n\n\t\tidx = srcu_read_lock(&kvm->srcu);\n\t\twrite_lock(&kvm->mmu_lock);\n\n\t\tif (kvm_has_zapped_obsolete_pages(kvm)) {\n\t\t\tkvm_mmu_commit_zap_page(kvm,\n\t\t\t      &kvm->arch.zapped_obsolete_pages);\n\t\t\tgoto unlock;\n\t\t}\n\n\t\tfreed = kvm_mmu_zap_oldest_mmu_pages(kvm, sc->nr_to_scan);\n\nunlock:\n\t\twrite_unlock(&kvm->mmu_lock);\n\t\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\t\t \n\t\tlist_move_tail(&kvm->vm_list, &vm_list);\n\t\tbreak;\n\t}\n\n\tmutex_unlock(&kvm_lock);\n\treturn freed;\n}\n\nstatic unsigned long mmu_shrink_count(struct shrinker *shrink,\n\t\t\t\t      struct shrink_control *sc)\n{\n\treturn percpu_counter_read_positive(&kvm_total_used_mmu_pages);\n}\n\nstatic struct shrinker mmu_shrinker = {\n\t.count_objects = mmu_shrink_count,\n\t.scan_objects = mmu_shrink_scan,\n\t.seeks = DEFAULT_SEEKS * 10,\n};\n\nstatic void mmu_destroy_caches(void)\n{\n\tkmem_cache_destroy(pte_list_desc_cache);\n\tkmem_cache_destroy(mmu_page_header_cache);\n}\n\nstatic int get_nx_huge_pages(char *buffer, const struct kernel_param *kp)\n{\n\tif (nx_hugepage_mitigation_hard_disabled)\n\t\treturn sysfs_emit(buffer, \"never\\n\");\n\n\treturn param_get_bool(buffer, kp);\n}\n\nstatic bool get_nx_auto_mode(void)\n{\n\t \n\treturn boot_cpu_has_bug(X86_BUG_ITLB_MULTIHIT) && !cpu_mitigations_off();\n}\n\nstatic void __set_nx_huge_pages(bool val)\n{\n\tnx_huge_pages = itlb_multihit_kvm_mitigation = val;\n}\n\nstatic int set_nx_huge_pages(const char *val, const struct kernel_param *kp)\n{\n\tbool old_val = nx_huge_pages;\n\tbool new_val;\n\n\tif (nx_hugepage_mitigation_hard_disabled)\n\t\treturn -EPERM;\n\n\t \n\tif (sysfs_streq(val, \"off\")) {\n\t\tnew_val = 0;\n\t} else if (sysfs_streq(val, \"force\")) {\n\t\tnew_val = 1;\n\t} else if (sysfs_streq(val, \"auto\")) {\n\t\tnew_val = get_nx_auto_mode();\n\t} else if (sysfs_streq(val, \"never\")) {\n\t\tnew_val = 0;\n\n\t\tmutex_lock(&kvm_lock);\n\t\tif (!list_empty(&vm_list)) {\n\t\t\tmutex_unlock(&kvm_lock);\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tnx_hugepage_mitigation_hard_disabled = true;\n\t\tmutex_unlock(&kvm_lock);\n\t} else if (kstrtobool(val, &new_val) < 0) {\n\t\treturn -EINVAL;\n\t}\n\n\t__set_nx_huge_pages(new_val);\n\n\tif (new_val != old_val) {\n\t\tstruct kvm *kvm;\n\n\t\tmutex_lock(&kvm_lock);\n\n\t\tlist_for_each_entry(kvm, &vm_list, vm_list) {\n\t\t\tmutex_lock(&kvm->slots_lock);\n\t\t\tkvm_mmu_zap_all_fast(kvm);\n\t\t\tmutex_unlock(&kvm->slots_lock);\n\n\t\t\twake_up_process(kvm->arch.nx_huge_page_recovery_thread);\n\t\t}\n\t\tmutex_unlock(&kvm_lock);\n\t}\n\n\treturn 0;\n}\n\n \nvoid __init kvm_mmu_x86_module_init(void)\n{\n\tif (nx_huge_pages == -1)\n\t\t__set_nx_huge_pages(get_nx_auto_mode());\n\n\t \n\ttdp_mmu_allowed = tdp_mmu_enabled;\n\n\tkvm_mmu_spte_module_init();\n}\n\n \nint kvm_mmu_vendor_module_init(void)\n{\n\tint ret = -ENOMEM;\n\n\t \n\tBUILD_BUG_ON(sizeof(union kvm_mmu_page_role) != sizeof(u32));\n\tBUILD_BUG_ON(sizeof(union kvm_mmu_extended_role) != sizeof(u32));\n\tBUILD_BUG_ON(sizeof(union kvm_cpu_role) != sizeof(u64));\n\n\tkvm_mmu_reset_all_pte_masks();\n\n\tpte_list_desc_cache = kmem_cache_create(\"pte_list_desc\",\n\t\t\t\t\t    sizeof(struct pte_list_desc),\n\t\t\t\t\t    0, SLAB_ACCOUNT, NULL);\n\tif (!pte_list_desc_cache)\n\t\tgoto out;\n\n\tmmu_page_header_cache = kmem_cache_create(\"kvm_mmu_page_header\",\n\t\t\t\t\t\t  sizeof(struct kvm_mmu_page),\n\t\t\t\t\t\t  0, SLAB_ACCOUNT, NULL);\n\tif (!mmu_page_header_cache)\n\t\tgoto out;\n\n\tif (percpu_counter_init(&kvm_total_used_mmu_pages, 0, GFP_KERNEL))\n\t\tgoto out;\n\n\tret = register_shrinker(&mmu_shrinker, \"x86-mmu\");\n\tif (ret)\n\t\tgoto out_shrinker;\n\n\treturn 0;\n\nout_shrinker:\n\tpercpu_counter_destroy(&kvm_total_used_mmu_pages);\nout:\n\tmmu_destroy_caches();\n\treturn ret;\n}\n\nvoid kvm_mmu_destroy(struct kvm_vcpu *vcpu)\n{\n\tkvm_mmu_unload(vcpu);\n\tfree_mmu_pages(&vcpu->arch.root_mmu);\n\tfree_mmu_pages(&vcpu->arch.guest_mmu);\n\tmmu_free_memory_caches(vcpu);\n}\n\nvoid kvm_mmu_vendor_module_exit(void)\n{\n\tmmu_destroy_caches();\n\tpercpu_counter_destroy(&kvm_total_used_mmu_pages);\n\tunregister_shrinker(&mmu_shrinker);\n}\n\n \nstatic bool calc_nx_huge_pages_recovery_period(uint *period)\n{\n\t \n\tbool enabled = READ_ONCE(nx_huge_pages);\n\tuint ratio = READ_ONCE(nx_huge_pages_recovery_ratio);\n\n\tif (!enabled || !ratio)\n\t\treturn false;\n\n\t*period = READ_ONCE(nx_huge_pages_recovery_period_ms);\n\tif (!*period) {\n\t\t \n\t\tratio = min(ratio, 3600u);\n\t\t*period = 60 * 60 * 1000 / ratio;\n\t}\n\treturn true;\n}\n\nstatic int set_nx_huge_pages_recovery_param(const char *val, const struct kernel_param *kp)\n{\n\tbool was_recovery_enabled, is_recovery_enabled;\n\tuint old_period, new_period;\n\tint err;\n\n\tif (nx_hugepage_mitigation_hard_disabled)\n\t\treturn -EPERM;\n\n\twas_recovery_enabled = calc_nx_huge_pages_recovery_period(&old_period);\n\n\terr = param_set_uint(val, kp);\n\tif (err)\n\t\treturn err;\n\n\tis_recovery_enabled = calc_nx_huge_pages_recovery_period(&new_period);\n\n\tif (is_recovery_enabled &&\n\t    (!was_recovery_enabled || old_period > new_period)) {\n\t\tstruct kvm *kvm;\n\n\t\tmutex_lock(&kvm_lock);\n\n\t\tlist_for_each_entry(kvm, &vm_list, vm_list)\n\t\t\twake_up_process(kvm->arch.nx_huge_page_recovery_thread);\n\n\t\tmutex_unlock(&kvm_lock);\n\t}\n\n\treturn err;\n}\n\nstatic void kvm_recover_nx_huge_pages(struct kvm *kvm)\n{\n\tunsigned long nx_lpage_splits = kvm->stat.nx_lpage_splits;\n\tstruct kvm_memory_slot *slot;\n\tint rcu_idx;\n\tstruct kvm_mmu_page *sp;\n\tunsigned int ratio;\n\tLIST_HEAD(invalid_list);\n\tbool flush = false;\n\tulong to_zap;\n\n\trcu_idx = srcu_read_lock(&kvm->srcu);\n\twrite_lock(&kvm->mmu_lock);\n\n\t \n\trcu_read_lock();\n\n\tratio = READ_ONCE(nx_huge_pages_recovery_ratio);\n\tto_zap = ratio ? DIV_ROUND_UP(nx_lpage_splits, ratio) : 0;\n\tfor ( ; to_zap; --to_zap) {\n\t\tif (list_empty(&kvm->arch.possible_nx_huge_pages))\n\t\t\tbreak;\n\n\t\t \n\t\tsp = list_first_entry(&kvm->arch.possible_nx_huge_pages,\n\t\t\t\t      struct kvm_mmu_page,\n\t\t\t\t      possible_nx_huge_page_link);\n\t\tWARN_ON_ONCE(!sp->nx_huge_page_disallowed);\n\t\tWARN_ON_ONCE(!sp->role.direct);\n\n\t\t \n\t\tslot = NULL;\n\t\tif (atomic_read(&kvm->nr_memslots_dirty_logging)) {\n\t\t\tstruct kvm_memslots *slots;\n\n\t\t\tslots = kvm_memslots_for_spte_role(kvm, sp->role);\n\t\t\tslot = __gfn_to_memslot(slots, sp->gfn);\n\t\t\tWARN_ON_ONCE(!slot);\n\t\t}\n\n\t\tif (slot && kvm_slot_dirty_track_enabled(slot))\n\t\t\tunaccount_nx_huge_page(kvm, sp);\n\t\telse if (is_tdp_mmu_page(sp))\n\t\t\tflush |= kvm_tdp_mmu_zap_sp(kvm, sp);\n\t\telse\n\t\t\tkvm_mmu_prepare_zap_page(kvm, sp, &invalid_list);\n\t\tWARN_ON_ONCE(sp->nx_huge_page_disallowed);\n\n\t\tif (need_resched() || rwlock_needbreak(&kvm->mmu_lock)) {\n\t\t\tkvm_mmu_remote_flush_or_zap(kvm, &invalid_list, flush);\n\t\t\trcu_read_unlock();\n\n\t\t\tcond_resched_rwlock_write(&kvm->mmu_lock);\n\t\t\tflush = false;\n\n\t\t\trcu_read_lock();\n\t\t}\n\t}\n\tkvm_mmu_remote_flush_or_zap(kvm, &invalid_list, flush);\n\n\trcu_read_unlock();\n\n\twrite_unlock(&kvm->mmu_lock);\n\tsrcu_read_unlock(&kvm->srcu, rcu_idx);\n}\n\nstatic long get_nx_huge_page_recovery_timeout(u64 start_time)\n{\n\tbool enabled;\n\tuint period;\n\n\tenabled = calc_nx_huge_pages_recovery_period(&period);\n\n\treturn enabled ? start_time + msecs_to_jiffies(period) - get_jiffies_64()\n\t\t       : MAX_SCHEDULE_TIMEOUT;\n}\n\nstatic int kvm_nx_huge_page_recovery_worker(struct kvm *kvm, uintptr_t data)\n{\n\tu64 start_time;\n\tlong remaining_time;\n\n\twhile (true) {\n\t\tstart_time = get_jiffies_64();\n\t\tremaining_time = get_nx_huge_page_recovery_timeout(start_time);\n\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\twhile (!kthread_should_stop() && remaining_time > 0) {\n\t\t\tschedule_timeout(remaining_time);\n\t\t\tremaining_time = get_nx_huge_page_recovery_timeout(start_time);\n\t\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\t}\n\n\t\tset_current_state(TASK_RUNNING);\n\n\t\tif (kthread_should_stop())\n\t\t\treturn 0;\n\n\t\tkvm_recover_nx_huge_pages(kvm);\n\t}\n}\n\nint kvm_mmu_post_init_vm(struct kvm *kvm)\n{\n\tint err;\n\n\tif (nx_hugepage_mitigation_hard_disabled)\n\t\treturn 0;\n\n\terr = kvm_vm_create_worker_thread(kvm, kvm_nx_huge_page_recovery_worker, 0,\n\t\t\t\t\t  \"kvm-nx-lpage-recovery\",\n\t\t\t\t\t  &kvm->arch.nx_huge_page_recovery_thread);\n\tif (!err)\n\t\tkthread_unpark(kvm->arch.nx_huge_page_recovery_thread);\n\n\treturn err;\n}\n\nvoid kvm_mmu_pre_destroy_vm(struct kvm *kvm)\n{\n\tif (kvm->arch.nx_huge_page_recovery_thread)\n\t\tkthread_stop(kvm->arch.nx_huge_page_recovery_thread);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}