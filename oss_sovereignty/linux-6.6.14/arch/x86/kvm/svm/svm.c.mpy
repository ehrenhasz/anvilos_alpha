{
  "module_name": "svm.c",
  "hash_id": "a4275e1301be7622b692cc66a09f1573d0baef95103d94e08f3f0898f2562e83",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/svm/svm.c",
  "human_readable_source": "#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/kvm_host.h>\n\n#include \"irq.h\"\n#include \"mmu.h\"\n#include \"kvm_cache_regs.h\"\n#include \"x86.h\"\n#include \"smm.h\"\n#include \"cpuid.h\"\n#include \"pmu.h\"\n\n#include <linux/module.h>\n#include <linux/mod_devicetable.h>\n#include <linux/kernel.h>\n#include <linux/vmalloc.h>\n#include <linux/highmem.h>\n#include <linux/amd-iommu.h>\n#include <linux/sched.h>\n#include <linux/trace_events.h>\n#include <linux/slab.h>\n#include <linux/hashtable.h>\n#include <linux/objtool.h>\n#include <linux/psp-sev.h>\n#include <linux/file.h>\n#include <linux/pagemap.h>\n#include <linux/swap.h>\n#include <linux/rwsem.h>\n#include <linux/cc_platform.h>\n#include <linux/smp.h>\n\n#include <asm/apic.h>\n#include <asm/perf_event.h>\n#include <asm/tlbflush.h>\n#include <asm/desc.h>\n#include <asm/debugreg.h>\n#include <asm/kvm_para.h>\n#include <asm/irq_remapping.h>\n#include <asm/spec-ctrl.h>\n#include <asm/cpu_device_id.h>\n#include <asm/traps.h>\n#include <asm/reboot.h>\n#include <asm/fpu/api.h>\n\n#include <trace/events/ipi.h>\n\n#include \"trace.h\"\n\n#include \"svm.h\"\n#include \"svm_ops.h\"\n\n#include \"kvm_onhyperv.h\"\n#include \"svm_onhyperv.h\"\n\nMODULE_AUTHOR(\"Qumranet\");\nMODULE_LICENSE(\"GPL\");\n\n#ifdef MODULE\nstatic const struct x86_cpu_id svm_cpu_id[] = {\n\tX86_MATCH_FEATURE(X86_FEATURE_SVM, NULL),\n\t{}\n};\nMODULE_DEVICE_TABLE(x86cpu, svm_cpu_id);\n#endif\n\n#define SEG_TYPE_LDT 2\n#define SEG_TYPE_BUSY_TSS16 3\n\nstatic bool erratum_383_found __read_mostly;\n\nu32 msrpm_offsets[MSRPM_OFFSETS] __read_mostly;\n\n \nstatic uint64_t osvw_len = 4, osvw_status;\n\nstatic DEFINE_PER_CPU(u64, current_tsc_ratio);\n\n#define X2APIC_MSR(x)\t(APIC_BASE_MSR + (x >> 4))\n\nstatic const struct svm_direct_access_msrs {\n\tu32 index;    \n\tbool always;  \n} direct_access_msrs[MAX_DIRECT_ACCESS_MSRS] = {\n\t{ .index = MSR_STAR,\t\t\t\t.always = true  },\n\t{ .index = MSR_IA32_SYSENTER_CS,\t\t.always = true  },\n\t{ .index = MSR_IA32_SYSENTER_EIP,\t\t.always = false },\n\t{ .index = MSR_IA32_SYSENTER_ESP,\t\t.always = false },\n#ifdef CONFIG_X86_64\n\t{ .index = MSR_GS_BASE,\t\t\t\t.always = true  },\n\t{ .index = MSR_FS_BASE,\t\t\t\t.always = true  },\n\t{ .index = MSR_KERNEL_GS_BASE,\t\t\t.always = true  },\n\t{ .index = MSR_LSTAR,\t\t\t\t.always = true  },\n\t{ .index = MSR_CSTAR,\t\t\t\t.always = true  },\n\t{ .index = MSR_SYSCALL_MASK,\t\t\t.always = true  },\n#endif\n\t{ .index = MSR_IA32_SPEC_CTRL,\t\t\t.always = false },\n\t{ .index = MSR_IA32_PRED_CMD,\t\t\t.always = false },\n\t{ .index = MSR_IA32_FLUSH_CMD,\t\t\t.always = false },\n\t{ .index = MSR_IA32_LASTBRANCHFROMIP,\t\t.always = false },\n\t{ .index = MSR_IA32_LASTBRANCHTOIP,\t\t.always = false },\n\t{ .index = MSR_IA32_LASTINTFROMIP,\t\t.always = false },\n\t{ .index = MSR_IA32_LASTINTTOIP,\t\t.always = false },\n\t{ .index = MSR_EFER,\t\t\t\t.always = false },\n\t{ .index = MSR_IA32_CR_PAT,\t\t\t.always = false },\n\t{ .index = MSR_AMD64_SEV_ES_GHCB,\t\t.always = true  },\n\t{ .index = MSR_TSC_AUX,\t\t\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_ID),\t\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_LVR),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_TASKPRI),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_ARBPRI),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_PROCPRI),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_EOI),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_RRR),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_LDR),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_DFR),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_SPIV),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_ISR),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_TMR),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_IRR),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_ESR),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_ICR),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_ICR2),\t\t.always = false },\n\n\t \n\t{ .index = X2APIC_MSR(APIC_LVTTHMR),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_LVTPC),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_LVT0),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_LVT1),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_LVTERR),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_TMICT),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_TMCCT),\t\t.always = false },\n\t{ .index = X2APIC_MSR(APIC_TDCR),\t\t.always = false },\n\t{ .index = MSR_INVALID,\t\t\t\t.always = false },\n};\n\n \n\nstatic unsigned short pause_filter_thresh = KVM_DEFAULT_PLE_GAP;\nmodule_param(pause_filter_thresh, ushort, 0444);\n\nstatic unsigned short pause_filter_count = KVM_SVM_DEFAULT_PLE_WINDOW;\nmodule_param(pause_filter_count, ushort, 0444);\n\n \nstatic unsigned short pause_filter_count_grow = KVM_DEFAULT_PLE_WINDOW_GROW;\nmodule_param(pause_filter_count_grow, ushort, 0444);\n\n \nstatic unsigned short pause_filter_count_shrink = KVM_DEFAULT_PLE_WINDOW_SHRINK;\nmodule_param(pause_filter_count_shrink, ushort, 0444);\n\n \nstatic unsigned short pause_filter_count_max = KVM_SVM_DEFAULT_PLE_WINDOW_MAX;\nmodule_param(pause_filter_count_max, ushort, 0444);\n\n \nbool npt_enabled = true;\nmodule_param_named(npt, npt_enabled, bool, 0444);\n\n \nstatic int nested = true;\nmodule_param(nested, int, S_IRUGO);\n\n \nint nrips = true;\nmodule_param(nrips, int, 0444);\n\n \nstatic int vls = true;\nmodule_param(vls, int, 0444);\n\n \nint vgif = true;\nmodule_param(vgif, int, 0444);\n\n \nstatic int lbrv = true;\nmodule_param(lbrv, int, 0444);\n\nstatic int tsc_scaling = true;\nmodule_param(tsc_scaling, int, 0444);\n\n \nstatic bool avic;\nmodule_param(avic, bool, 0444);\n\nbool __read_mostly dump_invalid_vmcb;\nmodule_param(dump_invalid_vmcb, bool, 0644);\n\n\nbool intercept_smi = true;\nmodule_param(intercept_smi, bool, 0444);\n\nbool vnmi = true;\nmodule_param(vnmi, bool, 0444);\n\nstatic bool svm_gp_erratum_intercept = true;\n\nstatic u8 rsm_ins_bytes[] = \"\\x0f\\xaa\";\n\nstatic unsigned long iopm_base;\n\nDEFINE_PER_CPU(struct svm_cpu_data, svm_data);\n\n \nstatic int tsc_aux_uret_slot __read_mostly = -1;\n\nstatic const u32 msrpm_ranges[] = {0, 0xc0000000, 0xc0010000};\n\n#define NUM_MSR_MAPS ARRAY_SIZE(msrpm_ranges)\n#define MSRS_RANGE_SIZE 2048\n#define MSRS_IN_RANGE (MSRS_RANGE_SIZE * 8 / 2)\n\nu32 svm_msrpm_offset(u32 msr)\n{\n\tu32 offset;\n\tint i;\n\n\tfor (i = 0; i < NUM_MSR_MAPS; i++) {\n\t\tif (msr < msrpm_ranges[i] ||\n\t\t    msr >= msrpm_ranges[i] + MSRS_IN_RANGE)\n\t\t\tcontinue;\n\n\t\toffset  = (msr - msrpm_ranges[i]) / 4;  \n\t\toffset += (i * MSRS_RANGE_SIZE);        \n\n\t\t \n\t\treturn offset / 4;\n\t}\n\n\t \n\treturn MSR_INVALID;\n}\n\nstatic void svm_flush_tlb_current(struct kvm_vcpu *vcpu);\n\nstatic int get_npt_level(void)\n{\n#ifdef CONFIG_X86_64\n\treturn pgtable_l5_enabled() ? PT64_ROOT_5LEVEL : PT64_ROOT_4LEVEL;\n#else\n\treturn PT32E_ROOT_LEVEL;\n#endif\n}\n\nint svm_set_efer(struct kvm_vcpu *vcpu, u64 efer)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu64 old_efer = vcpu->arch.efer;\n\tvcpu->arch.efer = efer;\n\n\tif (!npt_enabled) {\n\t\t \n\t\tefer |= EFER_NX;\n\n\t\tif (!(efer & EFER_LMA))\n\t\t\tefer &= ~EFER_LME;\n\t}\n\n\tif ((old_efer & EFER_SVME) != (efer & EFER_SVME)) {\n\t\tif (!(efer & EFER_SVME)) {\n\t\t\tsvm_leave_nested(vcpu);\n\t\t\tsvm_set_gif(svm, true);\n\t\t\t \n\t\t\tif (!enable_vmware_backdoor)\n\t\t\t\tclr_exception_intercept(svm, GP_VECTOR);\n\n\t\t\t \n\t\t\tif (!is_smm(vcpu))\n\t\t\t\tsvm_free_nested(svm);\n\n\t\t} else {\n\t\t\tint ret = svm_allocate_nested(svm);\n\n\t\t\tif (ret) {\n\t\t\t\tvcpu->arch.efer = old_efer;\n\t\t\t\treturn ret;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (svm_gp_erratum_intercept && !sev_guest(vcpu->kvm))\n\t\t\t\tset_exception_intercept(svm, GP_VECTOR);\n\t\t}\n\t}\n\n\tsvm->vmcb->save.efer = efer | EFER_SVME;\n\tvmcb_mark_dirty(svm->vmcb, VMCB_CR);\n\treturn 0;\n}\n\nstatic u32 svm_get_interrupt_shadow(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu32 ret = 0;\n\n\tif (svm->vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK)\n\t\tret = KVM_X86_SHADOW_INT_STI | KVM_X86_SHADOW_INT_MOV_SS;\n\treturn ret;\n}\n\nstatic void svm_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (mask == 0)\n\t\tsvm->vmcb->control.int_state &= ~SVM_INTERRUPT_SHADOW_MASK;\n\telse\n\t\tsvm->vmcb->control.int_state |= SVM_INTERRUPT_SHADOW_MASK;\n\n}\nstatic bool svm_can_emulate_instruction(struct kvm_vcpu *vcpu, int emul_type,\n\t\t\t\t\tvoid *insn, int insn_len);\n\nstatic int __svm_skip_emulated_instruction(struct kvm_vcpu *vcpu,\n\t\t\t\t\t   bool commit_side_effects)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tunsigned long old_rflags;\n\n\t \n\tif (sev_es_guest(vcpu->kvm))\n\t\tgoto done;\n\n\tif (nrips && svm->vmcb->control.next_rip != 0) {\n\t\tWARN_ON_ONCE(!static_cpu_has(X86_FEATURE_NRIPS));\n\t\tsvm->next_rip = svm->vmcb->control.next_rip;\n\t}\n\n\tif (!svm->next_rip) {\n\t\t \n\t\tif (!svm_can_emulate_instruction(vcpu, EMULTYPE_SKIP, NULL, 0))\n\t\t\treturn 0;\n\n\t\tif (unlikely(!commit_side_effects))\n\t\t\told_rflags = svm->vmcb->save.rflags;\n\n\t\tif (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))\n\t\t\treturn 0;\n\n\t\tif (unlikely(!commit_side_effects))\n\t\t\tsvm->vmcb->save.rflags = old_rflags;\n\t} else {\n\t\tkvm_rip_write(vcpu, svm->next_rip);\n\t}\n\ndone:\n\tif (likely(commit_side_effects))\n\t\tsvm_set_interrupt_shadow(vcpu, 0);\n\n\treturn 1;\n}\n\nstatic int svm_skip_emulated_instruction(struct kvm_vcpu *vcpu)\n{\n\treturn __svm_skip_emulated_instruction(vcpu, true);\n}\n\nstatic int svm_update_soft_interrupt_rip(struct kvm_vcpu *vcpu)\n{\n\tunsigned long rip, old_rip = kvm_rip_read(vcpu);\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\t \n\tif (!__svm_skip_emulated_instruction(vcpu, !nrips))\n\t\treturn -EIO;\n\n\trip = kvm_rip_read(vcpu);\n\n\t \n\tsvm->soft_int_injected = true;\n\tsvm->soft_int_csbase = svm->vmcb->save.cs.base;\n\tsvm->soft_int_old_rip = old_rip;\n\tsvm->soft_int_next_rip = rip;\n\n\tif (nrips)\n\t\tkvm_rip_write(vcpu, old_rip);\n\n\tif (static_cpu_has(X86_FEATURE_NRIPS))\n\t\tsvm->vmcb->control.next_rip = rip;\n\n\treturn 0;\n}\n\nstatic void svm_inject_exception(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_queued_exception *ex = &vcpu->arch.exception;\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tkvm_deliver_exception_payload(vcpu, ex);\n\n\tif (kvm_exception_is_soft(ex->vector) &&\n\t    svm_update_soft_interrupt_rip(vcpu))\n\t\treturn;\n\n\tsvm->vmcb->control.event_inj = ex->vector\n\t\t| SVM_EVTINJ_VALID\n\t\t| (ex->has_error_code ? SVM_EVTINJ_VALID_ERR : 0)\n\t\t| SVM_EVTINJ_TYPE_EXEPT;\n\tsvm->vmcb->control.event_inj_err = ex->error_code;\n}\n\nstatic void svm_init_erratum_383(void)\n{\n\tu32 low, high;\n\tint err;\n\tu64 val;\n\n\tif (!static_cpu_has_bug(X86_BUG_AMD_TLB_MMATCH))\n\t\treturn;\n\n\t \n\tval = native_read_msr_safe(MSR_AMD64_DC_CFG, &err);\n\tif (err)\n\t\treturn;\n\n\tval |= (1ULL << 47);\n\n\tlow  = lower_32_bits(val);\n\thigh = upper_32_bits(val);\n\n\tnative_write_msr_safe(MSR_AMD64_DC_CFG, low, high);\n\n\terratum_383_found = true;\n}\n\nstatic void svm_init_osvw(struct kvm_vcpu *vcpu)\n{\n\t \n\tvcpu->arch.osvw.length = (osvw_len >= 3) ? (osvw_len) : 3;\n\tvcpu->arch.osvw.status = osvw_status & ~(6ULL);\n\n\t \n\tif (osvw_len == 0 && boot_cpu_data.x86 == 0x10)\n\t\tvcpu->arch.osvw.status |= 1;\n}\n\nstatic bool __kvm_is_svm_supported(void)\n{\n\tint cpu = smp_processor_id();\n\tstruct cpuinfo_x86 *c = &cpu_data(cpu);\n\n\tu64 vm_cr;\n\n\tif (c->x86_vendor != X86_VENDOR_AMD &&\n\t    c->x86_vendor != X86_VENDOR_HYGON) {\n\t\tpr_err(\"CPU %d isn't AMD or Hygon\\n\", cpu);\n\t\treturn false;\n\t}\n\n\tif (!cpu_has(c, X86_FEATURE_SVM)) {\n\t\tpr_err(\"SVM not supported by CPU %d\\n\", cpu);\n\t\treturn false;\n\t}\n\n\tif (cc_platform_has(CC_ATTR_GUEST_MEM_ENCRYPT)) {\n\t\tpr_info(\"KVM is unsupported when running as an SEV guest\\n\");\n\t\treturn false;\n\t}\n\n\trdmsrl(MSR_VM_CR, vm_cr);\n\tif (vm_cr & (1 << SVM_VM_CR_SVM_DISABLE)) {\n\t\tpr_err(\"SVM disabled (by BIOS) in MSR_VM_CR on CPU %d\\n\", cpu);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool kvm_is_svm_supported(void)\n{\n\tbool supported;\n\n\tmigrate_disable();\n\tsupported = __kvm_is_svm_supported();\n\tmigrate_enable();\n\n\treturn supported;\n}\n\nstatic int svm_check_processor_compat(void)\n{\n\tif (!__kvm_is_svm_supported())\n\t\treturn -EIO;\n\n\treturn 0;\n}\n\nstatic void __svm_write_tsc_multiplier(u64 multiplier)\n{\n\tif (multiplier == __this_cpu_read(current_tsc_ratio))\n\t\treturn;\n\n\twrmsrl(MSR_AMD64_TSC_RATIO, multiplier);\n\t__this_cpu_write(current_tsc_ratio, multiplier);\n}\n\nstatic inline void kvm_cpu_svm_disable(void)\n{\n\tuint64_t efer;\n\n\twrmsrl(MSR_VM_HSAVE_PA, 0);\n\trdmsrl(MSR_EFER, efer);\n\tif (efer & EFER_SVME) {\n\t\t \n\t\tstgi();\n\t\twrmsrl(MSR_EFER, efer & ~EFER_SVME);\n\t}\n}\n\nstatic void svm_emergency_disable(void)\n{\n\tkvm_rebooting = true;\n\n\tkvm_cpu_svm_disable();\n}\n\nstatic void svm_hardware_disable(void)\n{\n\t \n\tif (tsc_scaling)\n\t\t__svm_write_tsc_multiplier(SVM_TSC_RATIO_DEFAULT);\n\n\tkvm_cpu_svm_disable();\n\n\tamd_pmu_disable_virt();\n}\n\nstatic int svm_hardware_enable(void)\n{\n\n\tstruct svm_cpu_data *sd;\n\tuint64_t efer;\n\tint me = raw_smp_processor_id();\n\n\trdmsrl(MSR_EFER, efer);\n\tif (efer & EFER_SVME)\n\t\treturn -EBUSY;\n\n\tsd = per_cpu_ptr(&svm_data, me);\n\tsd->asid_generation = 1;\n\tsd->max_asid = cpuid_ebx(SVM_CPUID_FUNC) - 1;\n\tsd->next_asid = sd->max_asid + 1;\n\tsd->min_asid = max_sev_asid + 1;\n\n\twrmsrl(MSR_EFER, efer | EFER_SVME);\n\n\twrmsrl(MSR_VM_HSAVE_PA, sd->save_area_pa);\n\n\tif (static_cpu_has(X86_FEATURE_TSCRATEMSR)) {\n\t\t \n\t\t__svm_write_tsc_multiplier(SVM_TSC_RATIO_DEFAULT);\n\t}\n\n\n\t \n\tif (cpu_has(&boot_cpu_data, X86_FEATURE_OSVW)) {\n\t\tuint64_t len, status = 0;\n\t\tint err;\n\n\t\tlen = native_read_msr_safe(MSR_AMD64_OSVW_ID_LENGTH, &err);\n\t\tif (!err)\n\t\t\tstatus = native_read_msr_safe(MSR_AMD64_OSVW_STATUS,\n\t\t\t\t\t\t      &err);\n\n\t\tif (err)\n\t\t\tosvw_status = osvw_len = 0;\n\t\telse {\n\t\t\tif (len < osvw_len)\n\t\t\t\tosvw_len = len;\n\t\t\tosvw_status |= status;\n\t\t\tosvw_status &= (1ULL << osvw_len) - 1;\n\t\t}\n\t} else\n\t\tosvw_status = osvw_len = 0;\n\n\tsvm_init_erratum_383();\n\n\tamd_pmu_enable_virt();\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_V_TSC_AUX)) {\n\t\tstruct sev_es_save_area *hostsa;\n\t\tu32 __maybe_unused msr_hi;\n\n\t\thostsa = (struct sev_es_save_area *)(page_address(sd->save_area) + 0x400);\n\n\t\trdmsr(MSR_TSC_AUX, hostsa->tsc_aux, msr_hi);\n\t}\n\n\treturn 0;\n}\n\nstatic void svm_cpu_uninit(int cpu)\n{\n\tstruct svm_cpu_data *sd = per_cpu_ptr(&svm_data, cpu);\n\n\tif (!sd->save_area)\n\t\treturn;\n\n\tkfree(sd->sev_vmcbs);\n\t__free_page(sd->save_area);\n\tsd->save_area_pa = 0;\n\tsd->save_area = NULL;\n}\n\nstatic int svm_cpu_init(int cpu)\n{\n\tstruct svm_cpu_data *sd = per_cpu_ptr(&svm_data, cpu);\n\tint ret = -ENOMEM;\n\n\tmemset(sd, 0, sizeof(struct svm_cpu_data));\n\tsd->save_area = alloc_page(GFP_KERNEL | __GFP_ZERO);\n\tif (!sd->save_area)\n\t\treturn ret;\n\n\tret = sev_cpu_init(sd);\n\tif (ret)\n\t\tgoto free_save_area;\n\n\tsd->save_area_pa = __sme_page_pa(sd->save_area);\n\treturn 0;\n\nfree_save_area:\n\t__free_page(sd->save_area);\n\tsd->save_area = NULL;\n\treturn ret;\n\n}\n\nstatic void set_dr_intercepts(struct vcpu_svm *svm)\n{\n\tstruct vmcb *vmcb = svm->vmcb01.ptr;\n\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR0_READ);\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR1_READ);\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR2_READ);\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR3_READ);\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR4_READ);\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR5_READ);\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR6_READ);\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR0_WRITE);\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR1_WRITE);\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR2_WRITE);\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR3_WRITE);\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR4_WRITE);\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR5_WRITE);\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR6_WRITE);\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR7_READ);\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR7_WRITE);\n\n\trecalc_intercepts(svm);\n}\n\nstatic void clr_dr_intercepts(struct vcpu_svm *svm)\n{\n\tstruct vmcb *vmcb = svm->vmcb01.ptr;\n\n\tvmcb->control.intercepts[INTERCEPT_DR] = 0;\n\n\trecalc_intercepts(svm);\n}\n\nstatic int direct_access_msr_slot(u32 msr)\n{\n\tu32 i;\n\n\tfor (i = 0; direct_access_msrs[i].index != MSR_INVALID; i++)\n\t\tif (direct_access_msrs[i].index == msr)\n\t\t\treturn i;\n\n\treturn -ENOENT;\n}\n\nstatic void set_shadow_msr_intercept(struct kvm_vcpu *vcpu, u32 msr, int read,\n\t\t\t\t     int write)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tint slot = direct_access_msr_slot(msr);\n\n\tif (slot == -ENOENT)\n\t\treturn;\n\n\t \n\tif (read)\n\t\tset_bit(slot, svm->shadow_msr_intercept.read);\n\telse\n\t\tclear_bit(slot, svm->shadow_msr_intercept.read);\n\n\tif (write)\n\t\tset_bit(slot, svm->shadow_msr_intercept.write);\n\telse\n\t\tclear_bit(slot, svm->shadow_msr_intercept.write);\n}\n\nstatic bool valid_msr_intercept(u32 index)\n{\n\treturn direct_access_msr_slot(index) != -ENOENT;\n}\n\nstatic bool msr_write_intercepted(struct kvm_vcpu *vcpu, u32 msr)\n{\n\tu8 bit_write;\n\tunsigned long tmp;\n\tu32 offset;\n\tu32 *msrpm;\n\n\t \n\tmsrpm = is_guest_mode(vcpu) ? to_svm(vcpu)->nested.msrpm:\n\t\t\t\t      to_svm(vcpu)->msrpm;\n\n\toffset    = svm_msrpm_offset(msr);\n\tbit_write = 2 * (msr & 0x0f) + 1;\n\ttmp       = msrpm[offset];\n\n\tBUG_ON(offset == MSR_INVALID);\n\n\treturn test_bit(bit_write, &tmp);\n}\n\nstatic void set_msr_interception_bitmap(struct kvm_vcpu *vcpu, u32 *msrpm,\n\t\t\t\t\tu32 msr, int read, int write)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu8 bit_read, bit_write;\n\tunsigned long tmp;\n\tu32 offset;\n\n\t \n\tWARN_ON(!valid_msr_intercept(msr));\n\n\t \n\tif (read && !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_READ))\n\t\tread = 0;\n\n\tif (write && !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_WRITE))\n\t\twrite = 0;\n\n\toffset    = svm_msrpm_offset(msr);\n\tbit_read  = 2 * (msr & 0x0f);\n\tbit_write = 2 * (msr & 0x0f) + 1;\n\ttmp       = msrpm[offset];\n\n\tBUG_ON(offset == MSR_INVALID);\n\n\tread  ? clear_bit(bit_read,  &tmp) : set_bit(bit_read,  &tmp);\n\twrite ? clear_bit(bit_write, &tmp) : set_bit(bit_write, &tmp);\n\n\tmsrpm[offset] = tmp;\n\n\tsvm_hv_vmcb_dirty_nested_enlightenments(vcpu);\n\tsvm->nested.force_msr_bitmap_recalc = true;\n}\n\nvoid set_msr_interception(struct kvm_vcpu *vcpu, u32 *msrpm, u32 msr,\n\t\t\t  int read, int write)\n{\n\tset_shadow_msr_intercept(vcpu, msr, read, write);\n\tset_msr_interception_bitmap(vcpu, msrpm, msr, read, write);\n}\n\nu32 *svm_vcpu_alloc_msrpm(void)\n{\n\tunsigned int order = get_order(MSRPM_SIZE);\n\tstruct page *pages = alloc_pages(GFP_KERNEL_ACCOUNT, order);\n\tu32 *msrpm;\n\n\tif (!pages)\n\t\treturn NULL;\n\n\tmsrpm = page_address(pages);\n\tmemset(msrpm, 0xff, PAGE_SIZE * (1 << order));\n\n\treturn msrpm;\n}\n\nvoid svm_vcpu_init_msrpm(struct kvm_vcpu *vcpu, u32 *msrpm)\n{\n\tint i;\n\n\tfor (i = 0; direct_access_msrs[i].index != MSR_INVALID; i++) {\n\t\tif (!direct_access_msrs[i].always)\n\t\t\tcontinue;\n\t\tset_msr_interception(vcpu, msrpm, direct_access_msrs[i].index, 1, 1);\n\t}\n}\n\nvoid svm_set_x2apic_msr_interception(struct vcpu_svm *svm, bool intercept)\n{\n\tint i;\n\n\tif (intercept == svm->x2avic_msrs_intercepted)\n\t\treturn;\n\n\tif (!x2avic_enabled)\n\t\treturn;\n\n\tfor (i = 0; i < MAX_DIRECT_ACCESS_MSRS; i++) {\n\t\tint index = direct_access_msrs[i].index;\n\n\t\tif ((index < APIC_BASE_MSR) ||\n\t\t    (index > APIC_BASE_MSR + 0xff))\n\t\t\tcontinue;\n\t\tset_msr_interception(&svm->vcpu, svm->msrpm, index,\n\t\t\t\t     !intercept, !intercept);\n\t}\n\n\tsvm->x2avic_msrs_intercepted = intercept;\n}\n\nvoid svm_vcpu_free_msrpm(u32 *msrpm)\n{\n\t__free_pages(virt_to_page(msrpm), get_order(MSRPM_SIZE));\n}\n\nstatic void svm_msr_filter_changed(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu32 i;\n\n\t \n\tfor (i = 0; direct_access_msrs[i].index != MSR_INVALID; i++) {\n\t\tu32 msr = direct_access_msrs[i].index;\n\t\tu32 read = test_bit(i, svm->shadow_msr_intercept.read);\n\t\tu32 write = test_bit(i, svm->shadow_msr_intercept.write);\n\n\t\tset_msr_interception_bitmap(vcpu, svm->msrpm, msr, read, write);\n\t}\n}\n\nstatic void add_msr_offset(u32 offset)\n{\n\tint i;\n\n\tfor (i = 0; i < MSRPM_OFFSETS; ++i) {\n\n\t\t \n\t\tif (msrpm_offsets[i] == offset)\n\t\t\treturn;\n\n\t\t \n\t\tif (msrpm_offsets[i] != MSR_INVALID)\n\t\t\tcontinue;\n\n\t\t \n\t\tmsrpm_offsets[i] = offset;\n\n\t\treturn;\n\t}\n\n\t \n\tBUG();\n}\n\nstatic void init_msrpm_offsets(void)\n{\n\tint i;\n\n\tmemset(msrpm_offsets, 0xff, sizeof(msrpm_offsets));\n\n\tfor (i = 0; direct_access_msrs[i].index != MSR_INVALID; i++) {\n\t\tu32 offset;\n\n\t\toffset = svm_msrpm_offset(direct_access_msrs[i].index);\n\t\tBUG_ON(offset == MSR_INVALID);\n\n\t\tadd_msr_offset(offset);\n\t}\n}\n\nvoid svm_copy_lbrs(struct vmcb *to_vmcb, struct vmcb *from_vmcb)\n{\n\tto_vmcb->save.dbgctl\t\t= from_vmcb->save.dbgctl;\n\tto_vmcb->save.br_from\t\t= from_vmcb->save.br_from;\n\tto_vmcb->save.br_to\t\t= from_vmcb->save.br_to;\n\tto_vmcb->save.last_excp_from\t= from_vmcb->save.last_excp_from;\n\tto_vmcb->save.last_excp_to\t= from_vmcb->save.last_excp_to;\n\n\tvmcb_mark_dirty(to_vmcb, VMCB_LBR);\n}\n\nstatic void svm_enable_lbrv(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tsvm->vmcb->control.virt_ext |= LBR_CTL_ENABLE_MASK;\n\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTBRANCHFROMIP, 1, 1);\n\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTBRANCHTOIP, 1, 1);\n\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTINTFROMIP, 1, 1);\n\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTINTTOIP, 1, 1);\n\n\t \n\tif (is_guest_mode(vcpu))\n\t\tsvm_copy_lbrs(svm->vmcb, svm->vmcb01.ptr);\n}\n\nstatic void svm_disable_lbrv(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tsvm->vmcb->control.virt_ext &= ~LBR_CTL_ENABLE_MASK;\n\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTBRANCHFROMIP, 0, 0);\n\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTBRANCHTOIP, 0, 0);\n\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTINTFROMIP, 0, 0);\n\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTINTTOIP, 0, 0);\n\n\t \n\tif (is_guest_mode(vcpu))\n\t\tsvm_copy_lbrs(svm->vmcb01.ptr, svm->vmcb);\n}\n\nstatic struct vmcb *svm_get_lbr_vmcb(struct vcpu_svm *svm)\n{\n\t \n\treturn svm->vmcb->control.virt_ext & LBR_CTL_ENABLE_MASK ? svm->vmcb :\n\t\t\t\t\t\t\t\t   svm->vmcb01.ptr;\n}\n\nvoid svm_update_lbrv(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tbool current_enable_lbrv = svm->vmcb->control.virt_ext & LBR_CTL_ENABLE_MASK;\n\tbool enable_lbrv = (svm_get_lbr_vmcb(svm)->save.dbgctl & DEBUGCTLMSR_LBR) ||\n\t\t\t    (is_guest_mode(vcpu) && guest_can_use(vcpu, X86_FEATURE_LBRV) &&\n\t\t\t    (svm->nested.ctl.virt_ext & LBR_CTL_ENABLE_MASK));\n\n\tif (enable_lbrv == current_enable_lbrv)\n\t\treturn;\n\n\tif (enable_lbrv)\n\t\tsvm_enable_lbrv(vcpu);\n\telse\n\t\tsvm_disable_lbrv(vcpu);\n}\n\nvoid disable_nmi_singlestep(struct vcpu_svm *svm)\n{\n\tsvm->nmi_singlestep = false;\n\n\tif (!(svm->vcpu.guest_debug & KVM_GUESTDBG_SINGLESTEP)) {\n\t\t \n\t\tif (!(svm->nmi_singlestep_guest_rflags & X86_EFLAGS_TF))\n\t\t\tsvm->vmcb->save.rflags &= ~X86_EFLAGS_TF;\n\t\tif (!(svm->nmi_singlestep_guest_rflags & X86_EFLAGS_RF))\n\t\t\tsvm->vmcb->save.rflags &= ~X86_EFLAGS_RF;\n\t}\n}\n\nstatic void grow_ple_window(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct vmcb_control_area *control = &svm->vmcb->control;\n\tint old = control->pause_filter_count;\n\n\tif (kvm_pause_in_guest(vcpu->kvm))\n\t\treturn;\n\n\tcontrol->pause_filter_count = __grow_ple_window(old,\n\t\t\t\t\t\t\tpause_filter_count,\n\t\t\t\t\t\t\tpause_filter_count_grow,\n\t\t\t\t\t\t\tpause_filter_count_max);\n\n\tif (control->pause_filter_count != old) {\n\t\tvmcb_mark_dirty(svm->vmcb, VMCB_INTERCEPTS);\n\t\ttrace_kvm_ple_window_update(vcpu->vcpu_id,\n\t\t\t\t\t    control->pause_filter_count, old);\n\t}\n}\n\nstatic void shrink_ple_window(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct vmcb_control_area *control = &svm->vmcb->control;\n\tint old = control->pause_filter_count;\n\n\tif (kvm_pause_in_guest(vcpu->kvm))\n\t\treturn;\n\n\tcontrol->pause_filter_count =\n\t\t\t\t__shrink_ple_window(old,\n\t\t\t\t\t\t    pause_filter_count,\n\t\t\t\t\t\t    pause_filter_count_shrink,\n\t\t\t\t\t\t    pause_filter_count);\n\tif (control->pause_filter_count != old) {\n\t\tvmcb_mark_dirty(svm->vmcb, VMCB_INTERCEPTS);\n\t\ttrace_kvm_ple_window_update(vcpu->vcpu_id,\n\t\t\t\t\t    control->pause_filter_count, old);\n\t}\n}\n\nstatic void svm_hardware_unsetup(void)\n{\n\tint cpu;\n\n\tsev_hardware_unsetup();\n\n\tfor_each_possible_cpu(cpu)\n\t\tsvm_cpu_uninit(cpu);\n\n\t__free_pages(pfn_to_page(iopm_base >> PAGE_SHIFT),\n\tget_order(IOPM_SIZE));\n\tiopm_base = 0;\n}\n\nstatic void init_seg(struct vmcb_seg *seg)\n{\n\tseg->selector = 0;\n\tseg->attrib = SVM_SELECTOR_P_MASK | SVM_SELECTOR_S_MASK |\n\t\t      SVM_SELECTOR_WRITE_MASK;  \n\tseg->limit = 0xffff;\n\tseg->base = 0;\n}\n\nstatic void init_sys_seg(struct vmcb_seg *seg, uint32_t type)\n{\n\tseg->selector = 0;\n\tseg->attrib = SVM_SELECTOR_P_MASK | type;\n\tseg->limit = 0xffff;\n\tseg->base = 0;\n}\n\nstatic u64 svm_get_l2_tsc_offset(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\treturn svm->nested.ctl.tsc_offset;\n}\n\nstatic u64 svm_get_l2_tsc_multiplier(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\treturn svm->tsc_ratio_msr;\n}\n\nstatic void svm_write_tsc_offset(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tsvm->vmcb01.ptr->control.tsc_offset = vcpu->arch.l1_tsc_offset;\n\tsvm->vmcb->control.tsc_offset = vcpu->arch.tsc_offset;\n\tvmcb_mark_dirty(svm->vmcb, VMCB_INTERCEPTS);\n}\n\nvoid svm_write_tsc_multiplier(struct kvm_vcpu *vcpu)\n{\n\tpreempt_disable();\n\tif (to_svm(vcpu)->guest_state_loaded)\n\t\t__svm_write_tsc_multiplier(vcpu->arch.tsc_scaling_ratio);\n\tpreempt_enable();\n}\n\n \nstatic void svm_recalc_instruction_intercepts(struct kvm_vcpu *vcpu,\n\t\t\t\t\t      struct vcpu_svm *svm)\n{\n\t \n\tif (kvm_cpu_cap_has(X86_FEATURE_INVPCID)) {\n\t\tif (!npt_enabled ||\n\t\t    !guest_cpuid_has(&svm->vcpu, X86_FEATURE_INVPCID))\n\t\t\tsvm_set_intercept(svm, INTERCEPT_INVPCID);\n\t\telse\n\t\t\tsvm_clr_intercept(svm, INTERCEPT_INVPCID);\n\t}\n\n\tif (kvm_cpu_cap_has(X86_FEATURE_RDTSCP)) {\n\t\tif (guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP))\n\t\t\tsvm_clr_intercept(svm, INTERCEPT_RDTSCP);\n\t\telse\n\t\t\tsvm_set_intercept(svm, INTERCEPT_RDTSCP);\n\t}\n}\n\nstatic inline void init_vmcb_after_set_cpuid(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (guest_cpuid_is_intel(vcpu)) {\n\t\t \n\t\tsvm_set_intercept(svm, INTERCEPT_VMLOAD);\n\t\tsvm_set_intercept(svm, INTERCEPT_VMSAVE);\n\t\tsvm->vmcb->control.virt_ext &= ~VIRTUAL_VMLOAD_VMSAVE_ENABLE_MASK;\n\n\t\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_SYSENTER_EIP, 0, 0);\n\t\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_SYSENTER_ESP, 0, 0);\n\t} else {\n\t\t \n\t\tif (vls) {\n\t\t\tsvm_clr_intercept(svm, INTERCEPT_VMLOAD);\n\t\t\tsvm_clr_intercept(svm, INTERCEPT_VMSAVE);\n\t\t\tsvm->vmcb->control.virt_ext |= VIRTUAL_VMLOAD_VMSAVE_ENABLE_MASK;\n\t\t}\n\t\t \n\t\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_SYSENTER_EIP, 1, 1);\n\t\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_SYSENTER_ESP, 1, 1);\n\t}\n}\n\nstatic void init_vmcb(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct vmcb *vmcb = svm->vmcb01.ptr;\n\tstruct vmcb_control_area *control = &vmcb->control;\n\tstruct vmcb_save_area *save = &vmcb->save;\n\n\tsvm_set_intercept(svm, INTERCEPT_CR0_READ);\n\tsvm_set_intercept(svm, INTERCEPT_CR3_READ);\n\tsvm_set_intercept(svm, INTERCEPT_CR4_READ);\n\tsvm_set_intercept(svm, INTERCEPT_CR0_WRITE);\n\tsvm_set_intercept(svm, INTERCEPT_CR3_WRITE);\n\tsvm_set_intercept(svm, INTERCEPT_CR4_WRITE);\n\tif (!kvm_vcpu_apicv_active(vcpu))\n\t\tsvm_set_intercept(svm, INTERCEPT_CR8_WRITE);\n\n\tset_dr_intercepts(svm);\n\n\tset_exception_intercept(svm, PF_VECTOR);\n\tset_exception_intercept(svm, UD_VECTOR);\n\tset_exception_intercept(svm, MC_VECTOR);\n\tset_exception_intercept(svm, AC_VECTOR);\n\tset_exception_intercept(svm, DB_VECTOR);\n\t \n\tif (enable_vmware_backdoor)\n\t\tset_exception_intercept(svm, GP_VECTOR);\n\n\tsvm_set_intercept(svm, INTERCEPT_INTR);\n\tsvm_set_intercept(svm, INTERCEPT_NMI);\n\n\tif (intercept_smi)\n\t\tsvm_set_intercept(svm, INTERCEPT_SMI);\n\n\tsvm_set_intercept(svm, INTERCEPT_SELECTIVE_CR0);\n\tsvm_set_intercept(svm, INTERCEPT_RDPMC);\n\tsvm_set_intercept(svm, INTERCEPT_CPUID);\n\tsvm_set_intercept(svm, INTERCEPT_INVD);\n\tsvm_set_intercept(svm, INTERCEPT_INVLPG);\n\tsvm_set_intercept(svm, INTERCEPT_INVLPGA);\n\tsvm_set_intercept(svm, INTERCEPT_IOIO_PROT);\n\tsvm_set_intercept(svm, INTERCEPT_MSR_PROT);\n\tsvm_set_intercept(svm, INTERCEPT_TASK_SWITCH);\n\tsvm_set_intercept(svm, INTERCEPT_SHUTDOWN);\n\tsvm_set_intercept(svm, INTERCEPT_VMRUN);\n\tsvm_set_intercept(svm, INTERCEPT_VMMCALL);\n\tsvm_set_intercept(svm, INTERCEPT_VMLOAD);\n\tsvm_set_intercept(svm, INTERCEPT_VMSAVE);\n\tsvm_set_intercept(svm, INTERCEPT_STGI);\n\tsvm_set_intercept(svm, INTERCEPT_CLGI);\n\tsvm_set_intercept(svm, INTERCEPT_SKINIT);\n\tsvm_set_intercept(svm, INTERCEPT_WBINVD);\n\tsvm_set_intercept(svm, INTERCEPT_XSETBV);\n\tsvm_set_intercept(svm, INTERCEPT_RDPRU);\n\tsvm_set_intercept(svm, INTERCEPT_RSM);\n\n\tif (!kvm_mwait_in_guest(vcpu->kvm)) {\n\t\tsvm_set_intercept(svm, INTERCEPT_MONITOR);\n\t\tsvm_set_intercept(svm, INTERCEPT_MWAIT);\n\t}\n\n\tif (!kvm_hlt_in_guest(vcpu->kvm))\n\t\tsvm_set_intercept(svm, INTERCEPT_HLT);\n\n\tcontrol->iopm_base_pa = __sme_set(iopm_base);\n\tcontrol->msrpm_base_pa = __sme_set(__pa(svm->msrpm));\n\tcontrol->int_ctl = V_INTR_MASKING_MASK;\n\n\tinit_seg(&save->es);\n\tinit_seg(&save->ss);\n\tinit_seg(&save->ds);\n\tinit_seg(&save->fs);\n\tinit_seg(&save->gs);\n\n\tsave->cs.selector = 0xf000;\n\tsave->cs.base = 0xffff0000;\n\t \n\tsave->cs.attrib = SVM_SELECTOR_READ_MASK | SVM_SELECTOR_P_MASK |\n\t\tSVM_SELECTOR_S_MASK | SVM_SELECTOR_CODE_MASK;\n\tsave->cs.limit = 0xffff;\n\n\tsave->gdtr.base = 0;\n\tsave->gdtr.limit = 0xffff;\n\tsave->idtr.base = 0;\n\tsave->idtr.limit = 0xffff;\n\n\tinit_sys_seg(&save->ldtr, SEG_TYPE_LDT);\n\tinit_sys_seg(&save->tr, SEG_TYPE_BUSY_TSS16);\n\n\tif (npt_enabled) {\n\t\t \n\t\tcontrol->nested_ctl |= SVM_NESTED_CTL_NP_ENABLE;\n\t\tsvm_clr_intercept(svm, INTERCEPT_INVLPG);\n\t\tclr_exception_intercept(svm, PF_VECTOR);\n\t\tsvm_clr_intercept(svm, INTERCEPT_CR3_READ);\n\t\tsvm_clr_intercept(svm, INTERCEPT_CR3_WRITE);\n\t\tsave->g_pat = vcpu->arch.pat;\n\t\tsave->cr3 = 0;\n\t}\n\tsvm->current_vmcb->asid_generation = 0;\n\tsvm->asid = 0;\n\n\tsvm->nested.vmcb12_gpa = INVALID_GPA;\n\tsvm->nested.last_vmcb12_gpa = INVALID_GPA;\n\n\tif (!kvm_pause_in_guest(vcpu->kvm)) {\n\t\tcontrol->pause_filter_count = pause_filter_count;\n\t\tif (pause_filter_thresh)\n\t\t\tcontrol->pause_filter_thresh = pause_filter_thresh;\n\t\tsvm_set_intercept(svm, INTERCEPT_PAUSE);\n\t} else {\n\t\tsvm_clr_intercept(svm, INTERCEPT_PAUSE);\n\t}\n\n\tsvm_recalc_instruction_intercepts(vcpu, svm);\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_V_SPEC_CTRL))\n\t\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);\n\n\tif (kvm_vcpu_apicv_active(vcpu))\n\t\tavic_init_vmcb(svm, vmcb);\n\n\tif (vnmi)\n\t\tsvm->vmcb->control.int_ctl |= V_NMI_ENABLE_MASK;\n\n\tif (vgif) {\n\t\tsvm_clr_intercept(svm, INTERCEPT_STGI);\n\t\tsvm_clr_intercept(svm, INTERCEPT_CLGI);\n\t\tsvm->vmcb->control.int_ctl |= V_GIF_ENABLE_MASK;\n\t}\n\n\tif (sev_guest(vcpu->kvm))\n\t\tsev_init_vmcb(svm);\n\n\tsvm_hv_init_vmcb(vmcb);\n\tinit_vmcb_after_set_cpuid(vcpu);\n\n\tvmcb_mark_all_dirty(vmcb);\n\n\tenable_gif(svm);\n}\n\nstatic void __svm_vcpu_reset(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tsvm_vcpu_init_msrpm(vcpu, svm->msrpm);\n\n\tsvm_init_osvw(vcpu);\n\tvcpu->arch.microcode_version = 0x01000065;\n\tsvm->tsc_ratio_msr = kvm_caps.default_tsc_scaling_ratio;\n\n\tsvm->nmi_masked = false;\n\tsvm->awaiting_iret_completion = false;\n\n\tif (sev_es_guest(vcpu->kvm))\n\t\tsev_es_vcpu_reset(svm);\n}\n\nstatic void svm_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tsvm->spec_ctrl = 0;\n\tsvm->virt_spec_ctrl = 0;\n\n\tinit_vmcb(vcpu);\n\n\tif (!init_event)\n\t\t__svm_vcpu_reset(vcpu);\n}\n\nvoid svm_switch_vmcb(struct vcpu_svm *svm, struct kvm_vmcb_info *target_vmcb)\n{\n\tsvm->current_vmcb = target_vmcb;\n\tsvm->vmcb = target_vmcb->ptr;\n}\n\nstatic int svm_vcpu_create(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm;\n\tstruct page *vmcb01_page;\n\tstruct page *vmsa_page = NULL;\n\tint err;\n\n\tBUILD_BUG_ON(offsetof(struct vcpu_svm, vcpu) != 0);\n\tsvm = to_svm(vcpu);\n\n\terr = -ENOMEM;\n\tvmcb01_page = alloc_page(GFP_KERNEL_ACCOUNT | __GFP_ZERO);\n\tif (!vmcb01_page)\n\t\tgoto out;\n\n\tif (sev_es_guest(vcpu->kvm)) {\n\t\t \n\t\tvmsa_page = alloc_page(GFP_KERNEL_ACCOUNT | __GFP_ZERO);\n\t\tif (!vmsa_page)\n\t\t\tgoto error_free_vmcb_page;\n\n\t\t \n\t\tfpstate_set_confidential(&vcpu->arch.guest_fpu);\n\t}\n\n\terr = avic_init_vcpu(svm);\n\tif (err)\n\t\tgoto error_free_vmsa_page;\n\n\tsvm->msrpm = svm_vcpu_alloc_msrpm();\n\tif (!svm->msrpm) {\n\t\terr = -ENOMEM;\n\t\tgoto error_free_vmsa_page;\n\t}\n\n\tsvm->x2avic_msrs_intercepted = true;\n\n\tsvm->vmcb01.ptr = page_address(vmcb01_page);\n\tsvm->vmcb01.pa = __sme_set(page_to_pfn(vmcb01_page) << PAGE_SHIFT);\n\tsvm_switch_vmcb(svm, &svm->vmcb01);\n\n\tif (vmsa_page)\n\t\tsvm->sev_es.vmsa = page_address(vmsa_page);\n\n\tsvm->guest_state_loaded = false;\n\n\treturn 0;\n\nerror_free_vmsa_page:\n\tif (vmsa_page)\n\t\t__free_page(vmsa_page);\nerror_free_vmcb_page:\n\t__free_page(vmcb01_page);\nout:\n\treturn err;\n}\n\nstatic void svm_clear_current_vmcb(struct vmcb *vmcb)\n{\n\tint i;\n\n\tfor_each_online_cpu(i)\n\t\tcmpxchg(per_cpu_ptr(&svm_data.current_vmcb, i), vmcb, NULL);\n}\n\nstatic void svm_vcpu_free(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\t \n\tsvm_clear_current_vmcb(svm->vmcb);\n\n\tsvm_leave_nested(vcpu);\n\tsvm_free_nested(svm);\n\n\tsev_free_vcpu(vcpu);\n\n\t__free_page(pfn_to_page(__sme_clr(svm->vmcb01.pa) >> PAGE_SHIFT));\n\t__free_pages(virt_to_page(svm->msrpm), get_order(MSRPM_SIZE));\n}\n\nstatic void svm_prepare_switch_to_guest(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct svm_cpu_data *sd = per_cpu_ptr(&svm_data, vcpu->cpu);\n\n\tif (sev_es_guest(vcpu->kvm))\n\t\tsev_es_unmap_ghcb(svm);\n\n\tif (svm->guest_state_loaded)\n\t\treturn;\n\n\t \n\tvmsave(sd->save_area_pa);\n\tif (sev_es_guest(vcpu->kvm)) {\n\t\tstruct sev_es_save_area *hostsa;\n\t\thostsa = (struct sev_es_save_area *)(page_address(sd->save_area) + 0x400);\n\n\t\tsev_es_prepare_switch_to_guest(hostsa);\n\t}\n\n\tif (tsc_scaling)\n\t\t__svm_write_tsc_multiplier(vcpu->arch.tsc_scaling_ratio);\n\n\t \n\tif (likely(tsc_aux_uret_slot >= 0) &&\n\t    (!boot_cpu_has(X86_FEATURE_V_TSC_AUX) || !sev_es_guest(vcpu->kvm)))\n\t\tkvm_set_user_return_msr(tsc_aux_uret_slot, svm->tsc_aux, -1ull);\n\n\tsvm->guest_state_loaded = true;\n}\n\nstatic void svm_prepare_host_switch(struct kvm_vcpu *vcpu)\n{\n\tto_svm(vcpu)->guest_state_loaded = false;\n}\n\nstatic void svm_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct svm_cpu_data *sd = per_cpu_ptr(&svm_data, cpu);\n\n\tif (sd->current_vmcb != svm->vmcb) {\n\t\tsd->current_vmcb = svm->vmcb;\n\n\t\tif (!cpu_feature_enabled(X86_FEATURE_IBPB_ON_VMEXIT))\n\t\t\tindirect_branch_prediction_barrier();\n\t}\n\tif (kvm_vcpu_apicv_active(vcpu))\n\t\tavic_vcpu_load(vcpu, cpu);\n}\n\nstatic void svm_vcpu_put(struct kvm_vcpu *vcpu)\n{\n\tif (kvm_vcpu_apicv_active(vcpu))\n\t\tavic_vcpu_put(vcpu);\n\n\tsvm_prepare_host_switch(vcpu);\n\n\t++vcpu->stat.host_state_reload;\n}\n\nstatic unsigned long svm_get_rflags(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tunsigned long rflags = svm->vmcb->save.rflags;\n\n\tif (svm->nmi_singlestep) {\n\t\t \n\t\tif (!(svm->nmi_singlestep_guest_rflags & X86_EFLAGS_TF))\n\t\t\trflags &= ~X86_EFLAGS_TF;\n\t\tif (!(svm->nmi_singlestep_guest_rflags & X86_EFLAGS_RF))\n\t\t\trflags &= ~X86_EFLAGS_RF;\n\t}\n\treturn rflags;\n}\n\nstatic void svm_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)\n{\n\tif (to_svm(vcpu)->nmi_singlestep)\n\t\trflags |= (X86_EFLAGS_TF | X86_EFLAGS_RF);\n\n        \n\tto_svm(vcpu)->vmcb->save.rflags = rflags;\n}\n\nstatic bool svm_get_if_flag(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcb *vmcb = to_svm(vcpu)->vmcb;\n\n\treturn sev_es_guest(vcpu->kvm)\n\t\t? vmcb->control.int_state & SVM_GUEST_INTERRUPT_MASK\n\t\t: kvm_get_rflags(vcpu) & X86_EFLAGS_IF;\n}\n\nstatic void svm_cache_reg(struct kvm_vcpu *vcpu, enum kvm_reg reg)\n{\n\tkvm_register_mark_available(vcpu, reg);\n\n\tswitch (reg) {\n\tcase VCPU_EXREG_PDPTR:\n\t\t \n\t\tif (npt_enabled)\n\t\t\tload_pdptrs(vcpu, kvm_read_cr3(vcpu));\n\t\tbreak;\n\tdefault:\n\t\tKVM_BUG_ON(1, vcpu->kvm);\n\t}\n}\n\nstatic void svm_set_vintr(struct vcpu_svm *svm)\n{\n\tstruct vmcb_control_area *control;\n\n\t \n\tWARN_ON(kvm_vcpu_apicv_activated(&svm->vcpu));\n\n\tsvm_set_intercept(svm, INTERCEPT_VINTR);\n\n\t \n\tif (!svm_is_intercept(svm, INTERCEPT_VINTR))\n\t\treturn;\n\n\t \n\tcontrol = &svm->vmcb->control;\n\tcontrol->int_vector = 0x0;\n\tcontrol->int_ctl &= ~V_INTR_PRIO_MASK;\n\tcontrol->int_ctl |= V_IRQ_MASK |\n\t\t((  0xf) << V_INTR_PRIO_SHIFT);\n\tvmcb_mark_dirty(svm->vmcb, VMCB_INTR);\n}\n\nstatic void svm_clear_vintr(struct vcpu_svm *svm)\n{\n\tsvm_clr_intercept(svm, INTERCEPT_VINTR);\n\n\t \n\tsvm->vmcb->control.int_ctl &= ~V_IRQ_INJECTION_BITS_MASK;\n\tif (is_guest_mode(&svm->vcpu)) {\n\t\tsvm->vmcb01.ptr->control.int_ctl &= ~V_IRQ_INJECTION_BITS_MASK;\n\n\t\tWARN_ON((svm->vmcb->control.int_ctl & V_TPR_MASK) !=\n\t\t\t(svm->nested.ctl.int_ctl & V_TPR_MASK));\n\n\t\tsvm->vmcb->control.int_ctl |= svm->nested.ctl.int_ctl &\n\t\t\tV_IRQ_INJECTION_BITS_MASK;\n\n\t\tsvm->vmcb->control.int_vector = svm->nested.ctl.int_vector;\n\t}\n\n\tvmcb_mark_dirty(svm->vmcb, VMCB_INTR);\n}\n\nstatic struct vmcb_seg *svm_seg(struct kvm_vcpu *vcpu, int seg)\n{\n\tstruct vmcb_save_area *save = &to_svm(vcpu)->vmcb->save;\n\tstruct vmcb_save_area *save01 = &to_svm(vcpu)->vmcb01.ptr->save;\n\n\tswitch (seg) {\n\tcase VCPU_SREG_CS: return &save->cs;\n\tcase VCPU_SREG_DS: return &save->ds;\n\tcase VCPU_SREG_ES: return &save->es;\n\tcase VCPU_SREG_FS: return &save01->fs;\n\tcase VCPU_SREG_GS: return &save01->gs;\n\tcase VCPU_SREG_SS: return &save->ss;\n\tcase VCPU_SREG_TR: return &save01->tr;\n\tcase VCPU_SREG_LDTR: return &save01->ldtr;\n\t}\n\tBUG();\n\treturn NULL;\n}\n\nstatic u64 svm_get_segment_base(struct kvm_vcpu *vcpu, int seg)\n{\n\tstruct vmcb_seg *s = svm_seg(vcpu, seg);\n\n\treturn s->base;\n}\n\nstatic void svm_get_segment(struct kvm_vcpu *vcpu,\n\t\t\t    struct kvm_segment *var, int seg)\n{\n\tstruct vmcb_seg *s = svm_seg(vcpu, seg);\n\n\tvar->base = s->base;\n\tvar->limit = s->limit;\n\tvar->selector = s->selector;\n\tvar->type = s->attrib & SVM_SELECTOR_TYPE_MASK;\n\tvar->s = (s->attrib >> SVM_SELECTOR_S_SHIFT) & 1;\n\tvar->dpl = (s->attrib >> SVM_SELECTOR_DPL_SHIFT) & 3;\n\tvar->present = (s->attrib >> SVM_SELECTOR_P_SHIFT) & 1;\n\tvar->avl = (s->attrib >> SVM_SELECTOR_AVL_SHIFT) & 1;\n\tvar->l = (s->attrib >> SVM_SELECTOR_L_SHIFT) & 1;\n\tvar->db = (s->attrib >> SVM_SELECTOR_DB_SHIFT) & 1;\n\n\t \n\tvar->g = s->limit > 0xfffff;\n\n\t \n\tvar->unusable = !var->present;\n\n\tswitch (seg) {\n\tcase VCPU_SREG_TR:\n\t\t \n\t\tvar->type |= 0x2;\n\t\tbreak;\n\tcase VCPU_SREG_DS:\n\tcase VCPU_SREG_ES:\n\tcase VCPU_SREG_FS:\n\tcase VCPU_SREG_GS:\n\t\t \n\t\tif (!var->unusable)\n\t\t\tvar->type |= 0x1;\n\t\tbreak;\n\tcase VCPU_SREG_SS:\n\t\t \n\t\tif (var->unusable)\n\t\t\tvar->db = 0;\n\t\t \n\t\tvar->dpl = to_svm(vcpu)->vmcb->save.cpl;\n\t\tbreak;\n\t}\n}\n\nstatic int svm_get_cpl(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcb_save_area *save = &to_svm(vcpu)->vmcb->save;\n\n\treturn save->cpl;\n}\n\nstatic void svm_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l)\n{\n\tstruct kvm_segment cs;\n\n\tsvm_get_segment(vcpu, &cs, VCPU_SREG_CS);\n\t*db = cs.db;\n\t*l = cs.l;\n}\n\nstatic void svm_get_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tdt->size = svm->vmcb->save.idtr.limit;\n\tdt->address = svm->vmcb->save.idtr.base;\n}\n\nstatic void svm_set_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tsvm->vmcb->save.idtr.limit = dt->size;\n\tsvm->vmcb->save.idtr.base = dt->address ;\n\tvmcb_mark_dirty(svm->vmcb, VMCB_DT);\n}\n\nstatic void svm_get_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tdt->size = svm->vmcb->save.gdtr.limit;\n\tdt->address = svm->vmcb->save.gdtr.base;\n}\n\nstatic void svm_set_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tsvm->vmcb->save.gdtr.limit = dt->size;\n\tsvm->vmcb->save.gdtr.base = dt->address ;\n\tvmcb_mark_dirty(svm->vmcb, VMCB_DT);\n}\n\nstatic void sev_post_set_cr3(struct kvm_vcpu *vcpu, unsigned long cr3)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\t \n\tif (sev_es_guest(vcpu->kvm)) {\n\t\tsvm->vmcb->save.cr3 = cr3;\n\t\tvmcb_mark_dirty(svm->vmcb, VMCB_CR);\n\t}\n}\n\nstatic bool svm_is_valid_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)\n{\n\treturn true;\n}\n\nvoid svm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu64 hcr0 = cr0;\n\tbool old_paging = is_paging(vcpu);\n\n#ifdef CONFIG_X86_64\n\tif (vcpu->arch.efer & EFER_LME) {\n\t\tif (!is_paging(vcpu) && (cr0 & X86_CR0_PG)) {\n\t\t\tvcpu->arch.efer |= EFER_LMA;\n\t\t\tif (!vcpu->arch.guest_state_protected)\n\t\t\t\tsvm->vmcb->save.efer |= EFER_LMA | EFER_LME;\n\t\t}\n\n\t\tif (is_paging(vcpu) && !(cr0 & X86_CR0_PG)) {\n\t\t\tvcpu->arch.efer &= ~EFER_LMA;\n\t\t\tif (!vcpu->arch.guest_state_protected)\n\t\t\t\tsvm->vmcb->save.efer &= ~(EFER_LMA | EFER_LME);\n\t\t}\n\t}\n#endif\n\tvcpu->arch.cr0 = cr0;\n\n\tif (!npt_enabled) {\n\t\thcr0 |= X86_CR0_PG | X86_CR0_WP;\n\t\tif (old_paging != is_paging(vcpu))\n\t\t\tsvm_set_cr4(vcpu, kvm_read_cr4(vcpu));\n\t}\n\n\t \n\tif (kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_CD_NW_CLEARED))\n\t\thcr0 &= ~(X86_CR0_CD | X86_CR0_NW);\n\n\tsvm->vmcb->save.cr0 = hcr0;\n\tvmcb_mark_dirty(svm->vmcb, VMCB_CR);\n\n\t \n\tif (sev_es_guest(vcpu->kvm))\n\t\treturn;\n\n\tif (hcr0 == cr0) {\n\t\t \n\t\tsvm_clr_intercept(svm, INTERCEPT_CR0_READ);\n\t\tsvm_clr_intercept(svm, INTERCEPT_CR0_WRITE);\n\t} else {\n\t\tsvm_set_intercept(svm, INTERCEPT_CR0_READ);\n\t\tsvm_set_intercept(svm, INTERCEPT_CR0_WRITE);\n\t}\n}\n\nstatic bool svm_is_valid_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)\n{\n\treturn true;\n}\n\nvoid svm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)\n{\n\tunsigned long host_cr4_mce = cr4_read_shadow() & X86_CR4_MCE;\n\tunsigned long old_cr4 = vcpu->arch.cr4;\n\n\tif (npt_enabled && ((old_cr4 ^ cr4) & X86_CR4_PGE))\n\t\tsvm_flush_tlb_current(vcpu);\n\n\tvcpu->arch.cr4 = cr4;\n\tif (!npt_enabled) {\n\t\tcr4 |= X86_CR4_PAE;\n\n\t\tif (!is_paging(vcpu))\n\t\t\tcr4 &= ~(X86_CR4_SMEP | X86_CR4_SMAP | X86_CR4_PKE);\n\t}\n\tcr4 |= host_cr4_mce;\n\tto_svm(vcpu)->vmcb->save.cr4 = cr4;\n\tvmcb_mark_dirty(to_svm(vcpu)->vmcb, VMCB_CR);\n\n\tif ((cr4 ^ old_cr4) & (X86_CR4_OSXSAVE | X86_CR4_PKE))\n\t\tkvm_update_cpuid_runtime(vcpu);\n}\n\nstatic void svm_set_segment(struct kvm_vcpu *vcpu,\n\t\t\t    struct kvm_segment *var, int seg)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct vmcb_seg *s = svm_seg(vcpu, seg);\n\n\ts->base = var->base;\n\ts->limit = var->limit;\n\ts->selector = var->selector;\n\ts->attrib = (var->type & SVM_SELECTOR_TYPE_MASK);\n\ts->attrib |= (var->s & 1) << SVM_SELECTOR_S_SHIFT;\n\ts->attrib |= (var->dpl & 3) << SVM_SELECTOR_DPL_SHIFT;\n\ts->attrib |= ((var->present & 1) && !var->unusable) << SVM_SELECTOR_P_SHIFT;\n\ts->attrib |= (var->avl & 1) << SVM_SELECTOR_AVL_SHIFT;\n\ts->attrib |= (var->l & 1) << SVM_SELECTOR_L_SHIFT;\n\ts->attrib |= (var->db & 1) << SVM_SELECTOR_DB_SHIFT;\n\ts->attrib |= (var->g & 1) << SVM_SELECTOR_G_SHIFT;\n\n\t \n\tif (seg == VCPU_SREG_SS)\n\t\t \n\t\tsvm->vmcb->save.cpl = (var->dpl & 3);\n\n\tvmcb_mark_dirty(svm->vmcb, VMCB_SEG);\n}\n\nstatic void svm_update_exception_bitmap(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tclr_exception_intercept(svm, BP_VECTOR);\n\n\tif (vcpu->guest_debug & KVM_GUESTDBG_ENABLE) {\n\t\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)\n\t\t\tset_exception_intercept(svm, BP_VECTOR);\n\t}\n}\n\nstatic void new_asid(struct vcpu_svm *svm, struct svm_cpu_data *sd)\n{\n\tif (sd->next_asid > sd->max_asid) {\n\t\t++sd->asid_generation;\n\t\tsd->next_asid = sd->min_asid;\n\t\tsvm->vmcb->control.tlb_ctl = TLB_CONTROL_FLUSH_ALL_ASID;\n\t\tvmcb_mark_dirty(svm->vmcb, VMCB_ASID);\n\t}\n\n\tsvm->current_vmcb->asid_generation = sd->asid_generation;\n\tsvm->asid = sd->next_asid++;\n}\n\nstatic void svm_set_dr6(struct vcpu_svm *svm, unsigned long value)\n{\n\tstruct vmcb *vmcb = svm->vmcb;\n\n\tif (svm->vcpu.arch.guest_state_protected)\n\t\treturn;\n\n\tif (unlikely(value != vmcb->save.dr6)) {\n\t\tvmcb->save.dr6 = value;\n\t\tvmcb_mark_dirty(vmcb, VMCB_DR);\n\t}\n}\n\nstatic void svm_sync_dirty_debug_regs(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (WARN_ON_ONCE(sev_es_guest(vcpu->kvm)))\n\t\treturn;\n\n\tget_debugreg(vcpu->arch.db[0], 0);\n\tget_debugreg(vcpu->arch.db[1], 1);\n\tget_debugreg(vcpu->arch.db[2], 2);\n\tget_debugreg(vcpu->arch.db[3], 3);\n\t \n\tvcpu->arch.dr6 = svm->vmcb->save.dr6;\n\tvcpu->arch.dr7 = svm->vmcb->save.dr7;\n\tvcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;\n\tset_dr_intercepts(svm);\n}\n\nstatic void svm_set_dr7(struct kvm_vcpu *vcpu, unsigned long value)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (vcpu->arch.guest_state_protected)\n\t\treturn;\n\n\tsvm->vmcb->save.dr7 = value;\n\tvmcb_mark_dirty(svm->vmcb, VMCB_DR);\n}\n\nstatic int pf_interception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tu64 fault_address = svm->vmcb->control.exit_info_2;\n\tu64 error_code = svm->vmcb->control.exit_info_1;\n\n\treturn kvm_handle_page_fault(vcpu, error_code, fault_address,\n\t\t\tstatic_cpu_has(X86_FEATURE_DECODEASSISTS) ?\n\t\t\tsvm->vmcb->control.insn_bytes : NULL,\n\t\t\tsvm->vmcb->control.insn_len);\n}\n\nstatic int npf_interception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tu64 fault_address = svm->vmcb->control.exit_info_2;\n\tu64 error_code = svm->vmcb->control.exit_info_1;\n\n\ttrace_kvm_page_fault(vcpu, fault_address, error_code);\n\treturn kvm_mmu_page_fault(vcpu, fault_address, error_code,\n\t\t\tstatic_cpu_has(X86_FEATURE_DECODEASSISTS) ?\n\t\t\tsvm->vmcb->control.insn_bytes : NULL,\n\t\t\tsvm->vmcb->control.insn_len);\n}\n\nstatic int db_interception(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_run *kvm_run = vcpu->run;\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (!(vcpu->guest_debug &\n\t      (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP)) &&\n\t\t!svm->nmi_singlestep) {\n\t\tu32 payload = svm->vmcb->save.dr6 ^ DR6_ACTIVE_LOW;\n\t\tkvm_queue_exception_p(vcpu, DB_VECTOR, payload);\n\t\treturn 1;\n\t}\n\n\tif (svm->nmi_singlestep) {\n\t\tdisable_nmi_singlestep(svm);\n\t\t \n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t}\n\n\tif (vcpu->guest_debug &\n\t    (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP)) {\n\t\tkvm_run->exit_reason = KVM_EXIT_DEBUG;\n\t\tkvm_run->debug.arch.dr6 = svm->vmcb->save.dr6;\n\t\tkvm_run->debug.arch.dr7 = svm->vmcb->save.dr7;\n\t\tkvm_run->debug.arch.pc =\n\t\t\tsvm->vmcb->save.cs.base + svm->vmcb->save.rip;\n\t\tkvm_run->debug.arch.exception = DB_VECTOR;\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic int bp_interception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct kvm_run *kvm_run = vcpu->run;\n\n\tkvm_run->exit_reason = KVM_EXIT_DEBUG;\n\tkvm_run->debug.arch.pc = svm->vmcb->save.cs.base + svm->vmcb->save.rip;\n\tkvm_run->debug.arch.exception = BP_VECTOR;\n\treturn 0;\n}\n\nstatic int ud_interception(struct kvm_vcpu *vcpu)\n{\n\treturn handle_ud(vcpu);\n}\n\nstatic int ac_interception(struct kvm_vcpu *vcpu)\n{\n\tkvm_queue_exception_e(vcpu, AC_VECTOR, 0);\n\treturn 1;\n}\n\nstatic bool is_erratum_383(void)\n{\n\tint err, i;\n\tu64 value;\n\n\tif (!erratum_383_found)\n\t\treturn false;\n\n\tvalue = native_read_msr_safe(MSR_IA32_MC0_STATUS, &err);\n\tif (err)\n\t\treturn false;\n\n\t \n\tvalue &= ~(1ULL << 62);\n\n\tif (value != 0xb600000000010015ULL)\n\t\treturn false;\n\n\t \n\tfor (i = 0; i < 6; ++i)\n\t\tnative_write_msr_safe(MSR_IA32_MCx_STATUS(i), 0, 0);\n\n\tvalue = native_read_msr_safe(MSR_IA32_MCG_STATUS, &err);\n\tif (!err) {\n\t\tu32 low, high;\n\n\t\tvalue &= ~(1ULL << 2);\n\t\tlow    = lower_32_bits(value);\n\t\thigh   = upper_32_bits(value);\n\n\t\tnative_write_msr_safe(MSR_IA32_MCG_STATUS, low, high);\n\t}\n\n\t \n\t__flush_tlb_all();\n\n\treturn true;\n}\n\nstatic void svm_handle_mce(struct kvm_vcpu *vcpu)\n{\n\tif (is_erratum_383()) {\n\t\t \n\t\tpr_err(\"Guest triggered AMD Erratum 383\\n\");\n\n\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\n\t\treturn;\n\t}\n\n\t \n\tkvm_machine_check();\n}\n\nstatic int mc_interception(struct kvm_vcpu *vcpu)\n{\n\treturn 1;\n}\n\nstatic int shutdown_interception(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_run *kvm_run = vcpu->run;\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\t \n\tif (sev_es_guest(vcpu->kvm))\n\t\treturn -EINVAL;\n\n\t \n\tclear_page(svm->vmcb);\n\tkvm_vcpu_reset(vcpu, true);\n\n\tkvm_run->exit_reason = KVM_EXIT_SHUTDOWN;\n\treturn 0;\n}\n\nstatic int io_interception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu32 io_info = svm->vmcb->control.exit_info_1;  \n\tint size, in, string;\n\tunsigned port;\n\n\t++vcpu->stat.io_exits;\n\tstring = (io_info & SVM_IOIO_STR_MASK) != 0;\n\tin = (io_info & SVM_IOIO_TYPE_MASK) != 0;\n\tport = io_info >> 16;\n\tsize = (io_info & SVM_IOIO_SIZE_MASK) >> SVM_IOIO_SIZE_SHIFT;\n\n\tif (string) {\n\t\tif (sev_es_guest(vcpu->kvm))\n\t\t\treturn sev_es_string_io(svm, size, port, in);\n\t\telse\n\t\t\treturn kvm_emulate_instruction(vcpu, 0);\n\t}\n\n\tsvm->next_rip = svm->vmcb->control.exit_info_2;\n\n\treturn kvm_fast_pio(vcpu, size, port, in);\n}\n\nstatic int nmi_interception(struct kvm_vcpu *vcpu)\n{\n\treturn 1;\n}\n\nstatic int smi_interception(struct kvm_vcpu *vcpu)\n{\n\treturn 1;\n}\n\nstatic int intr_interception(struct kvm_vcpu *vcpu)\n{\n\t++vcpu->stat.irq_exits;\n\treturn 1;\n}\n\nstatic int vmload_vmsave_interception(struct kvm_vcpu *vcpu, bool vmload)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct vmcb *vmcb12;\n\tstruct kvm_host_map map;\n\tint ret;\n\n\tif (nested_svm_check_permissions(vcpu))\n\t\treturn 1;\n\n\tret = kvm_vcpu_map(vcpu, gpa_to_gfn(svm->vmcb->save.rax), &map);\n\tif (ret) {\n\t\tif (ret == -EINVAL)\n\t\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\tvmcb12 = map.hva;\n\n\tret = kvm_skip_emulated_instruction(vcpu);\n\n\tif (vmload) {\n\t\tsvm_copy_vmloadsave_state(svm->vmcb, vmcb12);\n\t\tsvm->sysenter_eip_hi = 0;\n\t\tsvm->sysenter_esp_hi = 0;\n\t} else {\n\t\tsvm_copy_vmloadsave_state(vmcb12, svm->vmcb);\n\t}\n\n\tkvm_vcpu_unmap(vcpu, &map, true);\n\n\treturn ret;\n}\n\nstatic int vmload_interception(struct kvm_vcpu *vcpu)\n{\n\treturn vmload_vmsave_interception(vcpu, true);\n}\n\nstatic int vmsave_interception(struct kvm_vcpu *vcpu)\n{\n\treturn vmload_vmsave_interception(vcpu, false);\n}\n\nstatic int vmrun_interception(struct kvm_vcpu *vcpu)\n{\n\tif (nested_svm_check_permissions(vcpu))\n\t\treturn 1;\n\n\treturn nested_svm_vmrun(vcpu);\n}\n\nenum {\n\tNONE_SVM_INSTR,\n\tSVM_INSTR_VMRUN,\n\tSVM_INSTR_VMLOAD,\n\tSVM_INSTR_VMSAVE,\n};\n\n \nstatic int svm_instr_opcode(struct kvm_vcpu *vcpu)\n{\n\tstruct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;\n\n\tif (ctxt->b != 0x1 || ctxt->opcode_len != 2)\n\t\treturn NONE_SVM_INSTR;\n\n\tswitch (ctxt->modrm) {\n\tcase 0xd8:  \n\t\treturn SVM_INSTR_VMRUN;\n\tcase 0xda:  \n\t\treturn SVM_INSTR_VMLOAD;\n\tcase 0xdb:  \n\t\treturn SVM_INSTR_VMSAVE;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn NONE_SVM_INSTR;\n}\n\nstatic int emulate_svm_instr(struct kvm_vcpu *vcpu, int opcode)\n{\n\tconst int guest_mode_exit_codes[] = {\n\t\t[SVM_INSTR_VMRUN] = SVM_EXIT_VMRUN,\n\t\t[SVM_INSTR_VMLOAD] = SVM_EXIT_VMLOAD,\n\t\t[SVM_INSTR_VMSAVE] = SVM_EXIT_VMSAVE,\n\t};\n\tint (*const svm_instr_handlers[])(struct kvm_vcpu *vcpu) = {\n\t\t[SVM_INSTR_VMRUN] = vmrun_interception,\n\t\t[SVM_INSTR_VMLOAD] = vmload_interception,\n\t\t[SVM_INSTR_VMSAVE] = vmsave_interception,\n\t};\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tint ret;\n\n\tif (is_guest_mode(vcpu)) {\n\t\t \n\t\tret = nested_svm_simple_vmexit(svm, guest_mode_exit_codes[opcode]);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\treturn 1;\n\t}\n\treturn svm_instr_handlers[opcode](vcpu);\n}\n\n \nstatic int gp_interception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu32 error_code = svm->vmcb->control.exit_info_1;\n\tint opcode;\n\n\t \n\tif (error_code)\n\t\tgoto reinject;\n\n\t \n\tif (x86_decode_emulated_instruction(vcpu, 0, NULL, 0) != EMULATION_OK)\n\t\tgoto reinject;\n\n\topcode = svm_instr_opcode(vcpu);\n\n\tif (opcode == NONE_SVM_INSTR) {\n\t\tif (!enable_vmware_backdoor)\n\t\t\tgoto reinject;\n\n\t\t \n\t\tif (!is_guest_mode(vcpu))\n\t\t\treturn kvm_emulate_instruction(vcpu,\n\t\t\t\tEMULTYPE_VMWARE_GP | EMULTYPE_NO_DECODE);\n\t} else {\n\t\t \n\t\tif (svm->vmcb->save.rax & ~PAGE_MASK)\n\t\t\tgoto reinject;\n\n\t\treturn emulate_svm_instr(vcpu, opcode);\n\t}\n\nreinject:\n\tkvm_queue_exception_e(vcpu, GP_VECTOR, error_code);\n\treturn 1;\n}\n\nvoid svm_set_gif(struct vcpu_svm *svm, bool value)\n{\n\tif (value) {\n\t\t \n\t\tif (vgif)\n\t\t\tsvm_clr_intercept(svm, INTERCEPT_STGI);\n\t\tif (svm_is_intercept(svm, INTERCEPT_VINTR))\n\t\t\tsvm_clear_vintr(svm);\n\n\t\tenable_gif(svm);\n\t\tif (svm->vcpu.arch.smi_pending ||\n\t\t    svm->vcpu.arch.nmi_pending ||\n\t\t    kvm_cpu_has_injectable_intr(&svm->vcpu) ||\n\t\t    kvm_apic_has_pending_init_or_sipi(&svm->vcpu))\n\t\t\tkvm_make_request(KVM_REQ_EVENT, &svm->vcpu);\n\t} else {\n\t\tdisable_gif(svm);\n\n\t\t \n\t\tif (!vgif)\n\t\t\tsvm_clear_vintr(svm);\n\t}\n}\n\nstatic int stgi_interception(struct kvm_vcpu *vcpu)\n{\n\tint ret;\n\n\tif (nested_svm_check_permissions(vcpu))\n\t\treturn 1;\n\n\tret = kvm_skip_emulated_instruction(vcpu);\n\tsvm_set_gif(to_svm(vcpu), true);\n\treturn ret;\n}\n\nstatic int clgi_interception(struct kvm_vcpu *vcpu)\n{\n\tint ret;\n\n\tif (nested_svm_check_permissions(vcpu))\n\t\treturn 1;\n\n\tret = kvm_skip_emulated_instruction(vcpu);\n\tsvm_set_gif(to_svm(vcpu), false);\n\treturn ret;\n}\n\nstatic int invlpga_interception(struct kvm_vcpu *vcpu)\n{\n\tgva_t gva = kvm_rax_read(vcpu);\n\tu32 asid = kvm_rcx_read(vcpu);\n\n\t \n\tif (!is_long_mode(vcpu))\n\t\tgva = (u32)gva;\n\n\ttrace_kvm_invlpga(to_svm(vcpu)->vmcb->save.rip, asid, gva);\n\n\t \n\tkvm_mmu_invlpg(vcpu, gva);\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int skinit_interception(struct kvm_vcpu *vcpu)\n{\n\ttrace_kvm_skinit(to_svm(vcpu)->vmcb->save.rip, kvm_rax_read(vcpu));\n\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\treturn 1;\n}\n\nstatic int task_switch_interception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu16 tss_selector;\n\tint reason;\n\tint int_type = svm->vmcb->control.exit_int_info &\n\t\tSVM_EXITINTINFO_TYPE_MASK;\n\tint int_vec = svm->vmcb->control.exit_int_info & SVM_EVTINJ_VEC_MASK;\n\tuint32_t type =\n\t\tsvm->vmcb->control.exit_int_info & SVM_EXITINTINFO_TYPE_MASK;\n\tuint32_t idt_v =\n\t\tsvm->vmcb->control.exit_int_info & SVM_EXITINTINFO_VALID;\n\tbool has_error_code = false;\n\tu32 error_code = 0;\n\n\ttss_selector = (u16)svm->vmcb->control.exit_info_1;\n\n\tif (svm->vmcb->control.exit_info_2 &\n\t    (1ULL << SVM_EXITINFOSHIFT_TS_REASON_IRET))\n\t\treason = TASK_SWITCH_IRET;\n\telse if (svm->vmcb->control.exit_info_2 &\n\t\t (1ULL << SVM_EXITINFOSHIFT_TS_REASON_JMP))\n\t\treason = TASK_SWITCH_JMP;\n\telse if (idt_v)\n\t\treason = TASK_SWITCH_GATE;\n\telse\n\t\treason = TASK_SWITCH_CALL;\n\n\tif (reason == TASK_SWITCH_GATE) {\n\t\tswitch (type) {\n\t\tcase SVM_EXITINTINFO_TYPE_NMI:\n\t\t\tvcpu->arch.nmi_injected = false;\n\t\t\tbreak;\n\t\tcase SVM_EXITINTINFO_TYPE_EXEPT:\n\t\t\tif (svm->vmcb->control.exit_info_2 &\n\t\t\t    (1ULL << SVM_EXITINFOSHIFT_TS_HAS_ERROR_CODE)) {\n\t\t\t\thas_error_code = true;\n\t\t\t\terror_code =\n\t\t\t\t\t(u32)svm->vmcb->control.exit_info_2;\n\t\t\t}\n\t\t\tkvm_clear_exception_queue(vcpu);\n\t\t\tbreak;\n\t\tcase SVM_EXITINTINFO_TYPE_INTR:\n\t\tcase SVM_EXITINTINFO_TYPE_SOFT:\n\t\t\tkvm_clear_interrupt_queue(vcpu);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (reason != TASK_SWITCH_GATE ||\n\t    int_type == SVM_EXITINTINFO_TYPE_SOFT ||\n\t    (int_type == SVM_EXITINTINFO_TYPE_EXEPT &&\n\t     (int_vec == OF_VECTOR || int_vec == BP_VECTOR))) {\n\t\tif (!svm_skip_emulated_instruction(vcpu))\n\t\t\treturn 0;\n\t}\n\n\tif (int_type != SVM_EXITINTINFO_TYPE_SOFT)\n\t\tint_vec = -1;\n\n\treturn kvm_task_switch(vcpu, tss_selector, int_vec, reason,\n\t\t\t       has_error_code, error_code);\n}\n\nstatic void svm_clr_iret_intercept(struct vcpu_svm *svm)\n{\n\tif (!sev_es_guest(svm->vcpu.kvm))\n\t\tsvm_clr_intercept(svm, INTERCEPT_IRET);\n}\n\nstatic void svm_set_iret_intercept(struct vcpu_svm *svm)\n{\n\tif (!sev_es_guest(svm->vcpu.kvm))\n\t\tsvm_set_intercept(svm, INTERCEPT_IRET);\n}\n\nstatic int iret_interception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tWARN_ON_ONCE(sev_es_guest(vcpu->kvm));\n\n\t++vcpu->stat.nmi_window_exits;\n\tsvm->awaiting_iret_completion = true;\n\n\tsvm_clr_iret_intercept(svm);\n\tsvm->nmi_iret_rip = kvm_rip_read(vcpu);\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\treturn 1;\n}\n\nstatic int invlpg_interception(struct kvm_vcpu *vcpu)\n{\n\tif (!static_cpu_has(X86_FEATURE_DECODEASSISTS))\n\t\treturn kvm_emulate_instruction(vcpu, 0);\n\n\tkvm_mmu_invlpg(vcpu, to_svm(vcpu)->vmcb->control.exit_info_1);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int emulate_on_interception(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_emulate_instruction(vcpu, 0);\n}\n\nstatic int rsm_interception(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_emulate_instruction_from_buffer(vcpu, rsm_ins_bytes, 2);\n}\n\nstatic bool check_selective_cr0_intercepted(struct kvm_vcpu *vcpu,\n\t\t\t\t\t    unsigned long val)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tunsigned long cr0 = vcpu->arch.cr0;\n\tbool ret = false;\n\n\tif (!is_guest_mode(vcpu) ||\n\t    (!(vmcb12_is_intercept(&svm->nested.ctl, INTERCEPT_SELECTIVE_CR0))))\n\t\treturn false;\n\n\tcr0 &= ~SVM_CR0_SELECTIVE_MASK;\n\tval &= ~SVM_CR0_SELECTIVE_MASK;\n\n\tif (cr0 ^ val) {\n\t\tsvm->vmcb->control.exit_code = SVM_EXIT_CR0_SEL_WRITE;\n\t\tret = (nested_svm_exit_handled(svm) == NESTED_EXIT_DONE);\n\t}\n\n\treturn ret;\n}\n\n#define CR_VALID (1ULL << 63)\n\nstatic int cr_interception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tint reg, cr;\n\tunsigned long val;\n\tint err;\n\n\tif (!static_cpu_has(X86_FEATURE_DECODEASSISTS))\n\t\treturn emulate_on_interception(vcpu);\n\n\tif (unlikely((svm->vmcb->control.exit_info_1 & CR_VALID) == 0))\n\t\treturn emulate_on_interception(vcpu);\n\n\treg = svm->vmcb->control.exit_info_1 & SVM_EXITINFO_REG_MASK;\n\tif (svm->vmcb->control.exit_code == SVM_EXIT_CR0_SEL_WRITE)\n\t\tcr = SVM_EXIT_WRITE_CR0 - SVM_EXIT_READ_CR0;\n\telse\n\t\tcr = svm->vmcb->control.exit_code - SVM_EXIT_READ_CR0;\n\n\terr = 0;\n\tif (cr >= 16) {  \n\t\tcr -= 16;\n\t\tval = kvm_register_read(vcpu, reg);\n\t\ttrace_kvm_cr_write(cr, val);\n\t\tswitch (cr) {\n\t\tcase 0:\n\t\t\tif (!check_selective_cr0_intercepted(vcpu, val))\n\t\t\t\terr = kvm_set_cr0(vcpu, val);\n\t\t\telse\n\t\t\t\treturn 1;\n\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\terr = kvm_set_cr3(vcpu, val);\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\terr = kvm_set_cr4(vcpu, val);\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\terr = kvm_set_cr8(vcpu, val);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN(1, \"unhandled write to CR%d\", cr);\n\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\treturn 1;\n\t\t}\n\t} else {  \n\t\tswitch (cr) {\n\t\tcase 0:\n\t\t\tval = kvm_read_cr0(vcpu);\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tval = vcpu->arch.cr2;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tval = kvm_read_cr3(vcpu);\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\tval = kvm_read_cr4(vcpu);\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tval = kvm_get_cr8(vcpu);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN(1, \"unhandled read from CR%d\", cr);\n\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\t\treturn 1;\n\t\t}\n\t\tkvm_register_write(vcpu, reg, val);\n\t\ttrace_kvm_cr_read(cr, val);\n\t}\n\treturn kvm_complete_insn_gp(vcpu, err);\n}\n\nstatic int cr_trap(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tunsigned long old_value, new_value;\n\tunsigned int cr;\n\tint ret = 0;\n\n\tnew_value = (unsigned long)svm->vmcb->control.exit_info_1;\n\n\tcr = svm->vmcb->control.exit_code - SVM_EXIT_CR0_WRITE_TRAP;\n\tswitch (cr) {\n\tcase 0:\n\t\told_value = kvm_read_cr0(vcpu);\n\t\tsvm_set_cr0(vcpu, new_value);\n\n\t\tkvm_post_set_cr0(vcpu, old_value, new_value);\n\t\tbreak;\n\tcase 4:\n\t\told_value = kvm_read_cr4(vcpu);\n\t\tsvm_set_cr4(vcpu, new_value);\n\n\t\tkvm_post_set_cr4(vcpu, old_value, new_value);\n\t\tbreak;\n\tcase 8:\n\t\tret = kvm_set_cr8(vcpu, new_value);\n\t\tbreak;\n\tdefault:\n\t\tWARN(1, \"unhandled CR%d write trap\", cr);\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\treturn kvm_complete_insn_gp(vcpu, ret);\n}\n\nstatic int dr_interception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tint reg, dr;\n\tunsigned long val;\n\tint err = 0;\n\n\t \n\tif (sev_es_guest(vcpu->kvm))\n\t\treturn 1;\n\n\tif (vcpu->guest_debug == 0) {\n\t\t \n\t\tclr_dr_intercepts(svm);\n\t\tvcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;\n\t\treturn 1;\n\t}\n\n\tif (!boot_cpu_has(X86_FEATURE_DECODEASSISTS))\n\t\treturn emulate_on_interception(vcpu);\n\n\treg = svm->vmcb->control.exit_info_1 & SVM_EXITINFO_REG_MASK;\n\tdr = svm->vmcb->control.exit_code - SVM_EXIT_READ_DR0;\n\tif (dr >= 16) {  \n\t\tdr -= 16;\n\t\tval = kvm_register_read(vcpu, reg);\n\t\terr = kvm_set_dr(vcpu, dr, val);\n\t} else {\n\t\tkvm_get_dr(vcpu, dr, &val);\n\t\tkvm_register_write(vcpu, reg, val);\n\t}\n\n\treturn kvm_complete_insn_gp(vcpu, err);\n}\n\nstatic int cr8_write_interception(struct kvm_vcpu *vcpu)\n{\n\tint r;\n\n\tu8 cr8_prev = kvm_get_cr8(vcpu);\n\t \n\tr = cr_interception(vcpu);\n\tif (lapic_in_kernel(vcpu))\n\t\treturn r;\n\tif (cr8_prev <= kvm_get_cr8(vcpu))\n\t\treturn r;\n\tvcpu->run->exit_reason = KVM_EXIT_SET_TPR;\n\treturn 0;\n}\n\nstatic int efer_trap(struct kvm_vcpu *vcpu)\n{\n\tstruct msr_data msr_info;\n\tint ret;\n\n\t \n\tmsr_info.host_initiated = false;\n\tmsr_info.index = MSR_EFER;\n\tmsr_info.data = to_svm(vcpu)->vmcb->control.exit_info_1 & ~EFER_SVME;\n\tret = kvm_set_msr_common(vcpu, &msr_info);\n\n\treturn kvm_complete_insn_gp(vcpu, ret);\n}\n\nstatic int svm_get_msr_feature(struct kvm_msr_entry *msr)\n{\n\tmsr->data = 0;\n\n\tswitch (msr->index) {\n\tcase MSR_AMD64_DE_CFG:\n\t\tif (cpu_feature_enabled(X86_FEATURE_LFENCE_RDTSC))\n\t\t\tmsr->data |= MSR_AMD64_DE_CFG_LFENCE_SERIALIZE;\n\t\tbreak;\n\tdefault:\n\t\treturn KVM_MSR_RET_INVALID;\n\t}\n\n\treturn 0;\n}\n\nstatic int svm_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tswitch (msr_info->index) {\n\tcase MSR_AMD64_TSC_RATIO:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_can_use(vcpu, X86_FEATURE_TSCRATEMSR))\n\t\t\treturn 1;\n\t\tmsr_info->data = svm->tsc_ratio_msr;\n\t\tbreak;\n\tcase MSR_STAR:\n\t\tmsr_info->data = svm->vmcb01.ptr->save.star;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase MSR_LSTAR:\n\t\tmsr_info->data = svm->vmcb01.ptr->save.lstar;\n\t\tbreak;\n\tcase MSR_CSTAR:\n\t\tmsr_info->data = svm->vmcb01.ptr->save.cstar;\n\t\tbreak;\n\tcase MSR_KERNEL_GS_BASE:\n\t\tmsr_info->data = svm->vmcb01.ptr->save.kernel_gs_base;\n\t\tbreak;\n\tcase MSR_SYSCALL_MASK:\n\t\tmsr_info->data = svm->vmcb01.ptr->save.sfmask;\n\t\tbreak;\n#endif\n\tcase MSR_IA32_SYSENTER_CS:\n\t\tmsr_info->data = svm->vmcb01.ptr->save.sysenter_cs;\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_EIP:\n\t\tmsr_info->data = (u32)svm->vmcb01.ptr->save.sysenter_eip;\n\t\tif (guest_cpuid_is_intel(vcpu))\n\t\t\tmsr_info->data |= (u64)svm->sysenter_eip_hi << 32;\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_ESP:\n\t\tmsr_info->data = svm->vmcb01.ptr->save.sysenter_esp;\n\t\tif (guest_cpuid_is_intel(vcpu))\n\t\t\tmsr_info->data |= (u64)svm->sysenter_esp_hi << 32;\n\t\tbreak;\n\tcase MSR_TSC_AUX:\n\t\tmsr_info->data = svm->tsc_aux;\n\t\tbreak;\n\tcase MSR_IA32_DEBUGCTLMSR:\n\t\tmsr_info->data = svm_get_lbr_vmcb(svm)->save.dbgctl;\n\t\tbreak;\n\tcase MSR_IA32_LASTBRANCHFROMIP:\n\t\tmsr_info->data = svm_get_lbr_vmcb(svm)->save.br_from;\n\t\tbreak;\n\tcase MSR_IA32_LASTBRANCHTOIP:\n\t\tmsr_info->data = svm_get_lbr_vmcb(svm)->save.br_to;\n\t\tbreak;\n\tcase MSR_IA32_LASTINTFROMIP:\n\t\tmsr_info->data = svm_get_lbr_vmcb(svm)->save.last_excp_from;\n\t\tbreak;\n\tcase MSR_IA32_LASTINTTOIP:\n\t\tmsr_info->data = svm_get_lbr_vmcb(svm)->save.last_excp_to;\n\t\tbreak;\n\tcase MSR_VM_HSAVE_PA:\n\t\tmsr_info->data = svm->nested.hsave_msr;\n\t\tbreak;\n\tcase MSR_VM_CR:\n\t\tmsr_info->data = svm->nested.vm_cr_msr;\n\t\tbreak;\n\tcase MSR_IA32_SPEC_CTRL:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_has_spec_ctrl_msr(vcpu))\n\t\t\treturn 1;\n\n\t\tif (boot_cpu_has(X86_FEATURE_V_SPEC_CTRL))\n\t\t\tmsr_info->data = svm->vmcb->save.spec_ctrl;\n\t\telse\n\t\t\tmsr_info->data = svm->spec_ctrl;\n\t\tbreak;\n\tcase MSR_AMD64_VIRT_SPEC_CTRL:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_VIRT_SSBD))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = svm->virt_spec_ctrl;\n\t\tbreak;\n\tcase MSR_F15H_IC_CFG: {\n\n\t\tint family, model;\n\n\t\tfamily = guest_cpuid_family(vcpu);\n\t\tmodel  = guest_cpuid_model(vcpu);\n\n\t\tif (family < 0 || model < 0)\n\t\t\treturn kvm_get_msr_common(vcpu, msr_info);\n\n\t\tmsr_info->data = 0;\n\n\t\tif (family == 0x15 &&\n\t\t    (model >= 0x2 && model < 0x20))\n\t\t\tmsr_info->data = 0x1E;\n\t\t}\n\t\tbreak;\n\tcase MSR_AMD64_DE_CFG:\n\t\tmsr_info->data = svm->msr_decfg;\n\t\tbreak;\n\tdefault:\n\t\treturn kvm_get_msr_common(vcpu, msr_info);\n\t}\n\treturn 0;\n}\n\nstatic int svm_complete_emulated_msr(struct kvm_vcpu *vcpu, int err)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tif (!err || !sev_es_guest(vcpu->kvm) || WARN_ON_ONCE(!svm->sev_es.ghcb))\n\t\treturn kvm_complete_insn_gp(vcpu, err);\n\n\tghcb_set_sw_exit_info_1(svm->sev_es.ghcb, 1);\n\tghcb_set_sw_exit_info_2(svm->sev_es.ghcb,\n\t\t\t\tX86_TRAP_GP |\n\t\t\t\tSVM_EVTINJ_TYPE_EXEPT |\n\t\t\t\tSVM_EVTINJ_VALID);\n\treturn 1;\n}\n\nstatic int svm_set_vm_cr(struct kvm_vcpu *vcpu, u64 data)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tint svm_dis, chg_mask;\n\n\tif (data & ~SVM_VM_CR_VALID_MASK)\n\t\treturn 1;\n\n\tchg_mask = SVM_VM_CR_VALID_MASK;\n\n\tif (svm->nested.vm_cr_msr & SVM_VM_CR_SVM_DIS_MASK)\n\t\tchg_mask &= ~(SVM_VM_CR_SVM_LOCK_MASK | SVM_VM_CR_SVM_DIS_MASK);\n\n\tsvm->nested.vm_cr_msr &= ~chg_mask;\n\tsvm->nested.vm_cr_msr |= (data & chg_mask);\n\n\tsvm_dis = svm->nested.vm_cr_msr & SVM_VM_CR_SVM_DIS_MASK;\n\n\t \n\tif (svm_dis && (vcpu->arch.efer & EFER_SVME))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic int svm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tint ret = 0;\n\n\tu32 ecx = msr->index;\n\tu64 data = msr->data;\n\tswitch (ecx) {\n\tcase MSR_AMD64_TSC_RATIO:\n\n\t\tif (!guest_can_use(vcpu, X86_FEATURE_TSCRATEMSR)) {\n\n\t\t\tif (!msr->host_initiated)\n\t\t\t\treturn 1;\n\t\t\t \n\t\t\tif (data != 0 && data != svm->tsc_ratio_msr)\n\t\t\t\treturn 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (data & SVM_TSC_RATIO_RSVD)\n\t\t\treturn 1;\n\n\t\tsvm->tsc_ratio_msr = data;\n\n\t\tif (guest_can_use(vcpu, X86_FEATURE_TSCRATEMSR) &&\n\t\t    is_guest_mode(vcpu))\n\t\t\tnested_svm_update_tsc_ratio_msr(vcpu);\n\n\t\tbreak;\n\tcase MSR_IA32_CR_PAT:\n\t\tret = kvm_set_msr_common(vcpu, msr);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tsvm->vmcb01.ptr->save.g_pat = data;\n\t\tif (is_guest_mode(vcpu))\n\t\t\tnested_vmcb02_compute_g_pat(svm);\n\t\tvmcb_mark_dirty(svm->vmcb, VMCB_NPT);\n\t\tbreak;\n\tcase MSR_IA32_SPEC_CTRL:\n\t\tif (!msr->host_initiated &&\n\t\t    !guest_has_spec_ctrl_msr(vcpu))\n\t\t\treturn 1;\n\n\t\tif (kvm_spec_ctrl_test_value(data))\n\t\t\treturn 1;\n\n\t\tif (boot_cpu_has(X86_FEATURE_V_SPEC_CTRL))\n\t\t\tsvm->vmcb->save.spec_ctrl = data;\n\t\telse\n\t\t\tsvm->spec_ctrl = data;\n\t\tif (!data)\n\t\t\tbreak;\n\n\t\t \n\t\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_SPEC_CTRL, 1, 1);\n\t\tbreak;\n\tcase MSR_AMD64_VIRT_SPEC_CTRL:\n\t\tif (!msr->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_VIRT_SSBD))\n\t\t\treturn 1;\n\n\t\tif (data & ~SPEC_CTRL_SSBD)\n\t\t\treturn 1;\n\n\t\tsvm->virt_spec_ctrl = data;\n\t\tbreak;\n\tcase MSR_STAR:\n\t\tsvm->vmcb01.ptr->save.star = data;\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase MSR_LSTAR:\n\t\tsvm->vmcb01.ptr->save.lstar = data;\n\t\tbreak;\n\tcase MSR_CSTAR:\n\t\tsvm->vmcb01.ptr->save.cstar = data;\n\t\tbreak;\n\tcase MSR_KERNEL_GS_BASE:\n\t\tsvm->vmcb01.ptr->save.kernel_gs_base = data;\n\t\tbreak;\n\tcase MSR_SYSCALL_MASK:\n\t\tsvm->vmcb01.ptr->save.sfmask = data;\n\t\tbreak;\n#endif\n\tcase MSR_IA32_SYSENTER_CS:\n\t\tsvm->vmcb01.ptr->save.sysenter_cs = data;\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_EIP:\n\t\tsvm->vmcb01.ptr->save.sysenter_eip = (u32)data;\n\t\t \n\t\tsvm->sysenter_eip_hi = guest_cpuid_is_intel(vcpu) ? (data >> 32) : 0;\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_ESP:\n\t\tsvm->vmcb01.ptr->save.sysenter_esp = (u32)data;\n\t\tsvm->sysenter_esp_hi = guest_cpuid_is_intel(vcpu) ? (data >> 32) : 0;\n\t\tbreak;\n\tcase MSR_TSC_AUX:\n\t\t \n\t\tif (boot_cpu_has(X86_FEATURE_V_TSC_AUX) && sev_es_guest(vcpu->kvm))\n\t\t\tbreak;\n\n\t\t \n\t\tpreempt_disable();\n\t\tret = kvm_set_user_return_msr(tsc_aux_uret_slot, data, -1ull);\n\t\tpreempt_enable();\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tsvm->tsc_aux = data;\n\t\tbreak;\n\tcase MSR_IA32_DEBUGCTLMSR:\n\t\tif (!lbrv) {\n\t\t\tkvm_pr_unimpl_wrmsr(vcpu, ecx, data);\n\t\t\tbreak;\n\t\t}\n\t\tif (data & DEBUGCTL_RESERVED_BITS)\n\t\t\treturn 1;\n\n\t\tsvm_get_lbr_vmcb(svm)->save.dbgctl = data;\n\t\tsvm_update_lbrv(vcpu);\n\t\tbreak;\n\tcase MSR_VM_HSAVE_PA:\n\t\t \n\t\tif (!msr->host_initiated && !page_address_valid(vcpu, data))\n\t\t\treturn 1;\n\n\t\tsvm->nested.hsave_msr = data & PAGE_MASK;\n\t\tbreak;\n\tcase MSR_VM_CR:\n\t\treturn svm_set_vm_cr(vcpu, data);\n\tcase MSR_VM_IGNNE:\n\t\tkvm_pr_unimpl_wrmsr(vcpu, ecx, data);\n\t\tbreak;\n\tcase MSR_AMD64_DE_CFG: {\n\t\tstruct kvm_msr_entry msr_entry;\n\n\t\tmsr_entry.index = msr->index;\n\t\tif (svm_get_msr_feature(&msr_entry))\n\t\t\treturn 1;\n\n\t\t \n\t\tif (data & ~msr_entry.data)\n\t\t\treturn 1;\n\n\t\t \n\t\tif (!msr->host_initiated && (data ^ msr_entry.data))\n\t\t\treturn 1;\n\n\t\tsvm->msr_decfg = data;\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn kvm_set_msr_common(vcpu, msr);\n\t}\n\treturn ret;\n}\n\nstatic int msr_interception(struct kvm_vcpu *vcpu)\n{\n\tif (to_svm(vcpu)->vmcb->control.exit_info_1)\n\t\treturn kvm_emulate_wrmsr(vcpu);\n\telse\n\t\treturn kvm_emulate_rdmsr(vcpu);\n}\n\nstatic int interrupt_window_interception(struct kvm_vcpu *vcpu)\n{\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\tsvm_clear_vintr(to_svm(vcpu));\n\n\t \n\tkvm_clear_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);\n\n\t++vcpu->stat.irq_window_exits;\n\treturn 1;\n}\n\nstatic int pause_interception(struct kvm_vcpu *vcpu)\n{\n\tbool in_kernel;\n\t \n\tin_kernel = !sev_es_guest(vcpu->kvm) && svm_get_cpl(vcpu) == 0;\n\n\tgrow_ple_window(vcpu);\n\n\tkvm_vcpu_on_spin(vcpu, in_kernel);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int invpcid_interception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tunsigned long type;\n\tgva_t gva;\n\n\tif (!guest_cpuid_has(vcpu, X86_FEATURE_INVPCID)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\t \n\ttype = svm->vmcb->control.exit_info_2;\n\tgva = svm->vmcb->control.exit_info_1;\n\n\treturn kvm_handle_invpcid(vcpu, type, gva);\n}\n\nstatic int (*const svm_exit_handlers[])(struct kvm_vcpu *vcpu) = {\n\t[SVM_EXIT_READ_CR0]\t\t\t= cr_interception,\n\t[SVM_EXIT_READ_CR3]\t\t\t= cr_interception,\n\t[SVM_EXIT_READ_CR4]\t\t\t= cr_interception,\n\t[SVM_EXIT_READ_CR8]\t\t\t= cr_interception,\n\t[SVM_EXIT_CR0_SEL_WRITE]\t\t= cr_interception,\n\t[SVM_EXIT_WRITE_CR0]\t\t\t= cr_interception,\n\t[SVM_EXIT_WRITE_CR3]\t\t\t= cr_interception,\n\t[SVM_EXIT_WRITE_CR4]\t\t\t= cr_interception,\n\t[SVM_EXIT_WRITE_CR8]\t\t\t= cr8_write_interception,\n\t[SVM_EXIT_READ_DR0]\t\t\t= dr_interception,\n\t[SVM_EXIT_READ_DR1]\t\t\t= dr_interception,\n\t[SVM_EXIT_READ_DR2]\t\t\t= dr_interception,\n\t[SVM_EXIT_READ_DR3]\t\t\t= dr_interception,\n\t[SVM_EXIT_READ_DR4]\t\t\t= dr_interception,\n\t[SVM_EXIT_READ_DR5]\t\t\t= dr_interception,\n\t[SVM_EXIT_READ_DR6]\t\t\t= dr_interception,\n\t[SVM_EXIT_READ_DR7]\t\t\t= dr_interception,\n\t[SVM_EXIT_WRITE_DR0]\t\t\t= dr_interception,\n\t[SVM_EXIT_WRITE_DR1]\t\t\t= dr_interception,\n\t[SVM_EXIT_WRITE_DR2]\t\t\t= dr_interception,\n\t[SVM_EXIT_WRITE_DR3]\t\t\t= dr_interception,\n\t[SVM_EXIT_WRITE_DR4]\t\t\t= dr_interception,\n\t[SVM_EXIT_WRITE_DR5]\t\t\t= dr_interception,\n\t[SVM_EXIT_WRITE_DR6]\t\t\t= dr_interception,\n\t[SVM_EXIT_WRITE_DR7]\t\t\t= dr_interception,\n\t[SVM_EXIT_EXCP_BASE + DB_VECTOR]\t= db_interception,\n\t[SVM_EXIT_EXCP_BASE + BP_VECTOR]\t= bp_interception,\n\t[SVM_EXIT_EXCP_BASE + UD_VECTOR]\t= ud_interception,\n\t[SVM_EXIT_EXCP_BASE + PF_VECTOR]\t= pf_interception,\n\t[SVM_EXIT_EXCP_BASE + MC_VECTOR]\t= mc_interception,\n\t[SVM_EXIT_EXCP_BASE + AC_VECTOR]\t= ac_interception,\n\t[SVM_EXIT_EXCP_BASE + GP_VECTOR]\t= gp_interception,\n\t[SVM_EXIT_INTR]\t\t\t\t= intr_interception,\n\t[SVM_EXIT_NMI]\t\t\t\t= nmi_interception,\n\t[SVM_EXIT_SMI]\t\t\t\t= smi_interception,\n\t[SVM_EXIT_VINTR]\t\t\t= interrupt_window_interception,\n\t[SVM_EXIT_RDPMC]\t\t\t= kvm_emulate_rdpmc,\n\t[SVM_EXIT_CPUID]\t\t\t= kvm_emulate_cpuid,\n\t[SVM_EXIT_IRET]                         = iret_interception,\n\t[SVM_EXIT_INVD]                         = kvm_emulate_invd,\n\t[SVM_EXIT_PAUSE]\t\t\t= pause_interception,\n\t[SVM_EXIT_HLT]\t\t\t\t= kvm_emulate_halt,\n\t[SVM_EXIT_INVLPG]\t\t\t= invlpg_interception,\n\t[SVM_EXIT_INVLPGA]\t\t\t= invlpga_interception,\n\t[SVM_EXIT_IOIO]\t\t\t\t= io_interception,\n\t[SVM_EXIT_MSR]\t\t\t\t= msr_interception,\n\t[SVM_EXIT_TASK_SWITCH]\t\t\t= task_switch_interception,\n\t[SVM_EXIT_SHUTDOWN]\t\t\t= shutdown_interception,\n\t[SVM_EXIT_VMRUN]\t\t\t= vmrun_interception,\n\t[SVM_EXIT_VMMCALL]\t\t\t= kvm_emulate_hypercall,\n\t[SVM_EXIT_VMLOAD]\t\t\t= vmload_interception,\n\t[SVM_EXIT_VMSAVE]\t\t\t= vmsave_interception,\n\t[SVM_EXIT_STGI]\t\t\t\t= stgi_interception,\n\t[SVM_EXIT_CLGI]\t\t\t\t= clgi_interception,\n\t[SVM_EXIT_SKINIT]\t\t\t= skinit_interception,\n\t[SVM_EXIT_RDTSCP]\t\t\t= kvm_handle_invalid_op,\n\t[SVM_EXIT_WBINVD]                       = kvm_emulate_wbinvd,\n\t[SVM_EXIT_MONITOR]\t\t\t= kvm_emulate_monitor,\n\t[SVM_EXIT_MWAIT]\t\t\t= kvm_emulate_mwait,\n\t[SVM_EXIT_XSETBV]\t\t\t= kvm_emulate_xsetbv,\n\t[SVM_EXIT_RDPRU]\t\t\t= kvm_handle_invalid_op,\n\t[SVM_EXIT_EFER_WRITE_TRAP]\t\t= efer_trap,\n\t[SVM_EXIT_CR0_WRITE_TRAP]\t\t= cr_trap,\n\t[SVM_EXIT_CR4_WRITE_TRAP]\t\t= cr_trap,\n\t[SVM_EXIT_CR8_WRITE_TRAP]\t\t= cr_trap,\n\t[SVM_EXIT_INVPCID]                      = invpcid_interception,\n\t[SVM_EXIT_NPF]\t\t\t\t= npf_interception,\n\t[SVM_EXIT_RSM]                          = rsm_interception,\n\t[SVM_EXIT_AVIC_INCOMPLETE_IPI]\t\t= avic_incomplete_ipi_interception,\n\t[SVM_EXIT_AVIC_UNACCELERATED_ACCESS]\t= avic_unaccelerated_access_interception,\n\t[SVM_EXIT_VMGEXIT]\t\t\t= sev_handle_vmgexit,\n};\n\nstatic void dump_vmcb(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct vmcb_control_area *control = &svm->vmcb->control;\n\tstruct vmcb_save_area *save = &svm->vmcb->save;\n\tstruct vmcb_save_area *save01 = &svm->vmcb01.ptr->save;\n\n\tif (!dump_invalid_vmcb) {\n\t\tpr_warn_ratelimited(\"set kvm_amd.dump_invalid_vmcb=1 to dump internal KVM state.\\n\");\n\t\treturn;\n\t}\n\n\tpr_err(\"VMCB %p, last attempted VMRUN on CPU %d\\n\",\n\t       svm->current_vmcb->ptr, vcpu->arch.last_vmentry_cpu);\n\tpr_err(\"VMCB Control Area:\\n\");\n\tpr_err(\"%-20s%04x\\n\", \"cr_read:\", control->intercepts[INTERCEPT_CR] & 0xffff);\n\tpr_err(\"%-20s%04x\\n\", \"cr_write:\", control->intercepts[INTERCEPT_CR] >> 16);\n\tpr_err(\"%-20s%04x\\n\", \"dr_read:\", control->intercepts[INTERCEPT_DR] & 0xffff);\n\tpr_err(\"%-20s%04x\\n\", \"dr_write:\", control->intercepts[INTERCEPT_DR] >> 16);\n\tpr_err(\"%-20s%08x\\n\", \"exceptions:\", control->intercepts[INTERCEPT_EXCEPTION]);\n\tpr_err(\"%-20s%08x %08x\\n\", \"intercepts:\",\n              control->intercepts[INTERCEPT_WORD3],\n\t       control->intercepts[INTERCEPT_WORD4]);\n\tpr_err(\"%-20s%d\\n\", \"pause filter count:\", control->pause_filter_count);\n\tpr_err(\"%-20s%d\\n\", \"pause filter threshold:\",\n\t       control->pause_filter_thresh);\n\tpr_err(\"%-20s%016llx\\n\", \"iopm_base_pa:\", control->iopm_base_pa);\n\tpr_err(\"%-20s%016llx\\n\", \"msrpm_base_pa:\", control->msrpm_base_pa);\n\tpr_err(\"%-20s%016llx\\n\", \"tsc_offset:\", control->tsc_offset);\n\tpr_err(\"%-20s%d\\n\", \"asid:\", control->asid);\n\tpr_err(\"%-20s%d\\n\", \"tlb_ctl:\", control->tlb_ctl);\n\tpr_err(\"%-20s%08x\\n\", \"int_ctl:\", control->int_ctl);\n\tpr_err(\"%-20s%08x\\n\", \"int_vector:\", control->int_vector);\n\tpr_err(\"%-20s%08x\\n\", \"int_state:\", control->int_state);\n\tpr_err(\"%-20s%08x\\n\", \"exit_code:\", control->exit_code);\n\tpr_err(\"%-20s%016llx\\n\", \"exit_info1:\", control->exit_info_1);\n\tpr_err(\"%-20s%016llx\\n\", \"exit_info2:\", control->exit_info_2);\n\tpr_err(\"%-20s%08x\\n\", \"exit_int_info:\", control->exit_int_info);\n\tpr_err(\"%-20s%08x\\n\", \"exit_int_info_err:\", control->exit_int_info_err);\n\tpr_err(\"%-20s%lld\\n\", \"nested_ctl:\", control->nested_ctl);\n\tpr_err(\"%-20s%016llx\\n\", \"nested_cr3:\", control->nested_cr3);\n\tpr_err(\"%-20s%016llx\\n\", \"avic_vapic_bar:\", control->avic_vapic_bar);\n\tpr_err(\"%-20s%016llx\\n\", \"ghcb:\", control->ghcb_gpa);\n\tpr_err(\"%-20s%08x\\n\", \"event_inj:\", control->event_inj);\n\tpr_err(\"%-20s%08x\\n\", \"event_inj_err:\", control->event_inj_err);\n\tpr_err(\"%-20s%lld\\n\", \"virt_ext:\", control->virt_ext);\n\tpr_err(\"%-20s%016llx\\n\", \"next_rip:\", control->next_rip);\n\tpr_err(\"%-20s%016llx\\n\", \"avic_backing_page:\", control->avic_backing_page);\n\tpr_err(\"%-20s%016llx\\n\", \"avic_logical_id:\", control->avic_logical_id);\n\tpr_err(\"%-20s%016llx\\n\", \"avic_physical_id:\", control->avic_physical_id);\n\tpr_err(\"%-20s%016llx\\n\", \"vmsa_pa:\", control->vmsa_pa);\n\tpr_err(\"VMCB State Save Area:\\n\");\n\tpr_err(\"%-5s s: %04x a: %04x l: %08x b: %016llx\\n\",\n\t       \"es:\",\n\t       save->es.selector, save->es.attrib,\n\t       save->es.limit, save->es.base);\n\tpr_err(\"%-5s s: %04x a: %04x l: %08x b: %016llx\\n\",\n\t       \"cs:\",\n\t       save->cs.selector, save->cs.attrib,\n\t       save->cs.limit, save->cs.base);\n\tpr_err(\"%-5s s: %04x a: %04x l: %08x b: %016llx\\n\",\n\t       \"ss:\",\n\t       save->ss.selector, save->ss.attrib,\n\t       save->ss.limit, save->ss.base);\n\tpr_err(\"%-5s s: %04x a: %04x l: %08x b: %016llx\\n\",\n\t       \"ds:\",\n\t       save->ds.selector, save->ds.attrib,\n\t       save->ds.limit, save->ds.base);\n\tpr_err(\"%-5s s: %04x a: %04x l: %08x b: %016llx\\n\",\n\t       \"fs:\",\n\t       save01->fs.selector, save01->fs.attrib,\n\t       save01->fs.limit, save01->fs.base);\n\tpr_err(\"%-5s s: %04x a: %04x l: %08x b: %016llx\\n\",\n\t       \"gs:\",\n\t       save01->gs.selector, save01->gs.attrib,\n\t       save01->gs.limit, save01->gs.base);\n\tpr_err(\"%-5s s: %04x a: %04x l: %08x b: %016llx\\n\",\n\t       \"gdtr:\",\n\t       save->gdtr.selector, save->gdtr.attrib,\n\t       save->gdtr.limit, save->gdtr.base);\n\tpr_err(\"%-5s s: %04x a: %04x l: %08x b: %016llx\\n\",\n\t       \"ldtr:\",\n\t       save01->ldtr.selector, save01->ldtr.attrib,\n\t       save01->ldtr.limit, save01->ldtr.base);\n\tpr_err(\"%-5s s: %04x a: %04x l: %08x b: %016llx\\n\",\n\t       \"idtr:\",\n\t       save->idtr.selector, save->idtr.attrib,\n\t       save->idtr.limit, save->idtr.base);\n\tpr_err(\"%-5s s: %04x a: %04x l: %08x b: %016llx\\n\",\n\t       \"tr:\",\n\t       save01->tr.selector, save01->tr.attrib,\n\t       save01->tr.limit, save01->tr.base);\n\tpr_err(\"vmpl: %d   cpl:  %d               efer:          %016llx\\n\",\n\t       save->vmpl, save->cpl, save->efer);\n\tpr_err(\"%-15s %016llx %-13s %016llx\\n\",\n\t       \"cr0:\", save->cr0, \"cr2:\", save->cr2);\n\tpr_err(\"%-15s %016llx %-13s %016llx\\n\",\n\t       \"cr3:\", save->cr3, \"cr4:\", save->cr4);\n\tpr_err(\"%-15s %016llx %-13s %016llx\\n\",\n\t       \"dr6:\", save->dr6, \"dr7:\", save->dr7);\n\tpr_err(\"%-15s %016llx %-13s %016llx\\n\",\n\t       \"rip:\", save->rip, \"rflags:\", save->rflags);\n\tpr_err(\"%-15s %016llx %-13s %016llx\\n\",\n\t       \"rsp:\", save->rsp, \"rax:\", save->rax);\n\tpr_err(\"%-15s %016llx %-13s %016llx\\n\",\n\t       \"star:\", save01->star, \"lstar:\", save01->lstar);\n\tpr_err(\"%-15s %016llx %-13s %016llx\\n\",\n\t       \"cstar:\", save01->cstar, \"sfmask:\", save01->sfmask);\n\tpr_err(\"%-15s %016llx %-13s %016llx\\n\",\n\t       \"kernel_gs_base:\", save01->kernel_gs_base,\n\t       \"sysenter_cs:\", save01->sysenter_cs);\n\tpr_err(\"%-15s %016llx %-13s %016llx\\n\",\n\t       \"sysenter_esp:\", save01->sysenter_esp,\n\t       \"sysenter_eip:\", save01->sysenter_eip);\n\tpr_err(\"%-15s %016llx %-13s %016llx\\n\",\n\t       \"gpat:\", save->g_pat, \"dbgctl:\", save->dbgctl);\n\tpr_err(\"%-15s %016llx %-13s %016llx\\n\",\n\t       \"br_from:\", save->br_from, \"br_to:\", save->br_to);\n\tpr_err(\"%-15s %016llx %-13s %016llx\\n\",\n\t       \"excp_from:\", save->last_excp_from,\n\t       \"excp_to:\", save->last_excp_to);\n}\n\nstatic bool svm_check_exit_valid(u64 exit_code)\n{\n\treturn (exit_code < ARRAY_SIZE(svm_exit_handlers) &&\n\t\tsvm_exit_handlers[exit_code]);\n}\n\nstatic int svm_handle_invalid_exit(struct kvm_vcpu *vcpu, u64 exit_code)\n{\n\tvcpu_unimpl(vcpu, \"svm: unexpected exit reason 0x%llx\\n\", exit_code);\n\tdump_vmcb(vcpu);\n\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;\n\tvcpu->run->internal.ndata = 2;\n\tvcpu->run->internal.data[0] = exit_code;\n\tvcpu->run->internal.data[1] = vcpu->arch.last_vmentry_cpu;\n\treturn 0;\n}\n\nint svm_invoke_exit_handler(struct kvm_vcpu *vcpu, u64 exit_code)\n{\n\tif (!svm_check_exit_valid(exit_code))\n\t\treturn svm_handle_invalid_exit(vcpu, exit_code);\n\n#ifdef CONFIG_RETPOLINE\n\tif (exit_code == SVM_EXIT_MSR)\n\t\treturn msr_interception(vcpu);\n\telse if (exit_code == SVM_EXIT_VINTR)\n\t\treturn interrupt_window_interception(vcpu);\n\telse if (exit_code == SVM_EXIT_INTR)\n\t\treturn intr_interception(vcpu);\n\telse if (exit_code == SVM_EXIT_HLT)\n\t\treturn kvm_emulate_halt(vcpu);\n\telse if (exit_code == SVM_EXIT_NPF)\n\t\treturn npf_interception(vcpu);\n#endif\n\treturn svm_exit_handlers[exit_code](vcpu);\n}\n\nstatic void svm_get_exit_info(struct kvm_vcpu *vcpu, u32 *reason,\n\t\t\t      u64 *info1, u64 *info2,\n\t\t\t      u32 *intr_info, u32 *error_code)\n{\n\tstruct vmcb_control_area *control = &to_svm(vcpu)->vmcb->control;\n\n\t*reason = control->exit_code;\n\t*info1 = control->exit_info_1;\n\t*info2 = control->exit_info_2;\n\t*intr_info = control->exit_int_info;\n\tif ((*intr_info & SVM_EXITINTINFO_VALID) &&\n\t    (*intr_info & SVM_EXITINTINFO_VALID_ERR))\n\t\t*error_code = control->exit_int_info_err;\n\telse\n\t\t*error_code = 0;\n}\n\nstatic int svm_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct kvm_run *kvm_run = vcpu->run;\n\tu32 exit_code = svm->vmcb->control.exit_code;\n\n\t \n\tif (!sev_es_guest(vcpu->kvm)) {\n\t\tif (!svm_is_intercept(svm, INTERCEPT_CR0_WRITE))\n\t\t\tvcpu->arch.cr0 = svm->vmcb->save.cr0;\n\t\tif (npt_enabled)\n\t\t\tvcpu->arch.cr3 = svm->vmcb->save.cr3;\n\t}\n\n\tif (is_guest_mode(vcpu)) {\n\t\tint vmexit;\n\n\t\ttrace_kvm_nested_vmexit(vcpu, KVM_ISA_SVM);\n\n\t\tvmexit = nested_svm_exit_special(svm);\n\n\t\tif (vmexit == NESTED_EXIT_CONTINUE)\n\t\t\tvmexit = nested_svm_exit_handled(svm);\n\n\t\tif (vmexit == NESTED_EXIT_DONE)\n\t\t\treturn 1;\n\t}\n\n\tif (svm->vmcb->control.exit_code == SVM_EXIT_ERR) {\n\t\tkvm_run->exit_reason = KVM_EXIT_FAIL_ENTRY;\n\t\tkvm_run->fail_entry.hardware_entry_failure_reason\n\t\t\t= svm->vmcb->control.exit_code;\n\t\tkvm_run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;\n\t\tdump_vmcb(vcpu);\n\t\treturn 0;\n\t}\n\n\tif (exit_fastpath != EXIT_FASTPATH_NONE)\n\t\treturn 1;\n\n\treturn svm_invoke_exit_handler(vcpu, exit_code);\n}\n\nstatic void pre_svm_run(struct kvm_vcpu *vcpu)\n{\n\tstruct svm_cpu_data *sd = per_cpu_ptr(&svm_data, vcpu->cpu);\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\t \n\tif (unlikely(svm->current_vmcb->cpu != vcpu->cpu)) {\n\t\tsvm->current_vmcb->asid_generation = 0;\n\t\tvmcb_mark_all_dirty(svm->vmcb);\n\t\tsvm->current_vmcb->cpu = vcpu->cpu;\n        }\n\n\tif (sev_guest(vcpu->kvm))\n\t\treturn pre_sev_run(svm, vcpu->cpu);\n\n\t \n\tif (svm->current_vmcb->asid_generation != sd->asid_generation)\n\t\tnew_asid(svm, sd);\n}\n\nstatic void svm_inject_nmi(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tsvm->vmcb->control.event_inj = SVM_EVTINJ_VALID | SVM_EVTINJ_TYPE_NMI;\n\n\tif (svm->nmi_l1_to_l2)\n\t\treturn;\n\n\tsvm->nmi_masked = true;\n\tsvm_set_iret_intercept(svm);\n\t++vcpu->stat.nmi_injections;\n}\n\nstatic bool svm_is_vnmi_pending(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (!is_vnmi_enabled(svm))\n\t\treturn false;\n\n\treturn !!(svm->vmcb->control.int_ctl & V_NMI_PENDING_MASK);\n}\n\nstatic bool svm_set_vnmi_pending(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (!is_vnmi_enabled(svm))\n\t\treturn false;\n\n\tif (svm->vmcb->control.int_ctl & V_NMI_PENDING_MASK)\n\t\treturn false;\n\n\tsvm->vmcb->control.int_ctl |= V_NMI_PENDING_MASK;\n\tvmcb_mark_dirty(svm->vmcb, VMCB_INTR);\n\n\t \n\t++vcpu->stat.nmi_injections;\n\n\treturn true;\n}\n\nstatic void svm_inject_irq(struct kvm_vcpu *vcpu, bool reinjected)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu32 type;\n\n\tif (vcpu->arch.interrupt.soft) {\n\t\tif (svm_update_soft_interrupt_rip(vcpu))\n\t\t\treturn;\n\n\t\ttype = SVM_EVTINJ_TYPE_SOFT;\n\t} else {\n\t\ttype = SVM_EVTINJ_TYPE_INTR;\n\t}\n\n\ttrace_kvm_inj_virq(vcpu->arch.interrupt.nr,\n\t\t\t   vcpu->arch.interrupt.soft, reinjected);\n\t++vcpu->stat.irq_injections;\n\n\tsvm->vmcb->control.event_inj = vcpu->arch.interrupt.nr |\n\t\t\t\t       SVM_EVTINJ_VALID | type;\n}\n\nvoid svm_complete_interrupt_delivery(struct kvm_vcpu *vcpu, int delivery_mode,\n\t\t\t\t     int trig_mode, int vector)\n{\n\t \n\tbool in_guest_mode = (smp_load_acquire(&vcpu->mode) == IN_GUEST_MODE);\n\n\t \n\tif (!READ_ONCE(vcpu->arch.apic->apicv_active)) {\n\t\t \n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t\treturn;\n\t}\n\n\ttrace_kvm_apicv_accept_irq(vcpu->vcpu_id, delivery_mode, trig_mode, vector);\n\tif (in_guest_mode) {\n\t\t \n\t\tavic_ring_doorbell(vcpu);\n\t} else {\n\t\t \n\t\tkvm_vcpu_wake_up(vcpu);\n\t}\n}\n\nstatic void svm_deliver_interrupt(struct kvm_lapic *apic,  int delivery_mode,\n\t\t\t\t  int trig_mode, int vector)\n{\n\tkvm_lapic_set_irr(vector, apic);\n\n\t \n\tsmp_mb__after_atomic();\n\tsvm_complete_interrupt_delivery(apic->vcpu, delivery_mode, trig_mode, vector);\n}\n\nstatic void svm_update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\t \n\tif (sev_es_guest(vcpu->kvm))\n\t\treturn;\n\n\tif (nested_svm_virtualize_tpr(vcpu))\n\t\treturn;\n\n\tsvm_clr_intercept(svm, INTERCEPT_CR8_WRITE);\n\n\tif (irr == -1)\n\t\treturn;\n\n\tif (tpr >= irr)\n\t\tsvm_set_intercept(svm, INTERCEPT_CR8_WRITE);\n}\n\nstatic bool svm_get_nmi_mask(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (is_vnmi_enabled(svm))\n\t\treturn svm->vmcb->control.int_ctl & V_NMI_BLOCKING_MASK;\n\telse\n\t\treturn svm->nmi_masked;\n}\n\nstatic void svm_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (is_vnmi_enabled(svm)) {\n\t\tif (masked)\n\t\t\tsvm->vmcb->control.int_ctl |= V_NMI_BLOCKING_MASK;\n\t\telse\n\t\t\tsvm->vmcb->control.int_ctl &= ~V_NMI_BLOCKING_MASK;\n\n\t} else {\n\t\tsvm->nmi_masked = masked;\n\t\tif (masked)\n\t\t\tsvm_set_iret_intercept(svm);\n\t\telse\n\t\t\tsvm_clr_iret_intercept(svm);\n\t}\n}\n\nbool svm_nmi_blocked(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct vmcb *vmcb = svm->vmcb;\n\n\tif (!gif_set(svm))\n\t\treturn true;\n\n\tif (is_guest_mode(vcpu) && nested_exit_on_nmi(svm))\n\t\treturn false;\n\n\tif (svm_get_nmi_mask(vcpu))\n\t\treturn true;\n\n\treturn vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK;\n}\n\nstatic int svm_nmi_allowed(struct kvm_vcpu *vcpu, bool for_injection)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tif (svm->nested.nested_run_pending)\n\t\treturn -EBUSY;\n\n\tif (svm_nmi_blocked(vcpu))\n\t\treturn 0;\n\n\t \n\tif (for_injection && is_guest_mode(vcpu) && nested_exit_on_nmi(svm))\n\t\treturn -EBUSY;\n\treturn 1;\n}\n\nbool svm_interrupt_blocked(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct vmcb *vmcb = svm->vmcb;\n\n\tif (!gif_set(svm))\n\t\treturn true;\n\n\tif (is_guest_mode(vcpu)) {\n\t\t \n\t\tif ((svm->nested.ctl.int_ctl & V_INTR_MASKING_MASK)\n\t\t    ? !(svm->vmcb01.ptr->save.rflags & X86_EFLAGS_IF)\n\t\t    : !(kvm_get_rflags(vcpu) & X86_EFLAGS_IF))\n\t\t\treturn true;\n\n\t\t \n\t\tif (nested_exit_on_intr(svm))\n\t\t\treturn false;\n\t} else {\n\t\tif (!svm_get_if_flag(vcpu))\n\t\t\treturn true;\n\t}\n\n\treturn (vmcb->control.int_state & SVM_INTERRUPT_SHADOW_MASK);\n}\n\nstatic int svm_interrupt_allowed(struct kvm_vcpu *vcpu, bool for_injection)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (svm->nested.nested_run_pending)\n\t\treturn -EBUSY;\n\n\tif (svm_interrupt_blocked(vcpu))\n\t\treturn 0;\n\n\t \n\tif (for_injection && is_guest_mode(vcpu) && nested_exit_on_intr(svm))\n\t\treturn -EBUSY;\n\n\treturn 1;\n}\n\nstatic void svm_enable_irq_window(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\t \n\tif (vgif || gif_set(svm)) {\n\t\t \n\t\tif (!is_guest_mode(vcpu))\n\t\t\tkvm_set_apicv_inhibit(vcpu->kvm, APICV_INHIBIT_REASON_IRQWIN);\n\n\t\tsvm_set_vintr(svm);\n\t}\n}\n\nstatic void svm_enable_nmi_window(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\t \n\tWARN_ON_ONCE(is_vnmi_enabled(svm));\n\n\tif (svm_get_nmi_mask(vcpu) && !svm->awaiting_iret_completion)\n\t\treturn;  \n\n\t \n\tif (sev_es_guest(vcpu->kvm))\n\t\treturn;\n\n\tif (!gif_set(svm)) {\n\t\tif (vgif)\n\t\t\tsvm_set_intercept(svm, INTERCEPT_STGI);\n\t\treturn;  \n\t}\n\n\t \n\tsvm->nmi_singlestep_guest_rflags = svm_get_rflags(vcpu);\n\tsvm->nmi_singlestep = true;\n\tsvm->vmcb->save.rflags |= (X86_EFLAGS_TF | X86_EFLAGS_RF);\n}\n\nstatic void svm_flush_tlb_asid(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\t \n\tkvm_hv_vcpu_purge_flush_tlb(vcpu);\n\n\t \n\tif (static_cpu_has(X86_FEATURE_FLUSHBYASID))\n\t\tsvm->vmcb->control.tlb_ctl = TLB_CONTROL_FLUSH_ASID;\n\telse\n\t\tsvm->current_vmcb->asid_generation--;\n}\n\nstatic void svm_flush_tlb_current(struct kvm_vcpu *vcpu)\n{\n\thpa_t root_tdp = vcpu->arch.mmu->root.hpa;\n\n\t \n\tif (svm_hv_is_enlightened_tlb_enabled(vcpu) && VALID_PAGE(root_tdp))\n\t\thyperv_flush_guest_mapping(root_tdp);\n\n\tsvm_flush_tlb_asid(vcpu);\n}\n\nstatic void svm_flush_tlb_all(struct kvm_vcpu *vcpu)\n{\n\t \n\tif (WARN_ON_ONCE(svm_hv_is_enlightened_tlb_enabled(vcpu)))\n\t\thv_flush_remote_tlbs(vcpu->kvm);\n\n\tsvm_flush_tlb_asid(vcpu);\n}\n\nstatic void svm_flush_tlb_gva(struct kvm_vcpu *vcpu, gva_t gva)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tinvlpga(gva, svm->vmcb->control.asid);\n}\n\nstatic inline void sync_cr8_to_lapic(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (nested_svm_virtualize_tpr(vcpu))\n\t\treturn;\n\n\tif (!svm_is_intercept(svm, INTERCEPT_CR8_WRITE)) {\n\t\tint cr8 = svm->vmcb->control.int_ctl & V_TPR_MASK;\n\t\tkvm_set_cr8(vcpu, cr8);\n\t}\n}\n\nstatic inline void sync_lapic_to_cr8(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu64 cr8;\n\n\tif (nested_svm_virtualize_tpr(vcpu) ||\n\t    kvm_vcpu_apicv_active(vcpu))\n\t\treturn;\n\n\tcr8 = kvm_get_cr8(vcpu);\n\tsvm->vmcb->control.int_ctl &= ~V_TPR_MASK;\n\tsvm->vmcb->control.int_ctl |= cr8 & V_TPR_MASK;\n}\n\nstatic void svm_complete_soft_interrupt(struct kvm_vcpu *vcpu, u8 vector,\n\t\t\t\t\tint type)\n{\n\tbool is_exception = (type == SVM_EXITINTINFO_TYPE_EXEPT);\n\tbool is_soft = (type == SVM_EXITINTINFO_TYPE_SOFT);\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\t \n\tif (nrips && (is_soft || (is_exception && kvm_exception_is_soft(vector))) &&\n\t    kvm_is_linear_rip(vcpu, svm->soft_int_old_rip + svm->soft_int_csbase))\n\t\tsvm->vmcb->control.next_rip = svm->soft_int_next_rip;\n\t \n\telse if (!nrips && (is_soft || is_exception) &&\n\t\t kvm_is_linear_rip(vcpu, svm->soft_int_next_rip + svm->soft_int_csbase))\n\t\tkvm_rip_write(vcpu, svm->soft_int_old_rip);\n}\n\nstatic void svm_complete_interrupts(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu8 vector;\n\tint type;\n\tu32 exitintinfo = svm->vmcb->control.exit_int_info;\n\tbool nmi_l1_to_l2 = svm->nmi_l1_to_l2;\n\tbool soft_int_injected = svm->soft_int_injected;\n\n\tsvm->nmi_l1_to_l2 = false;\n\tsvm->soft_int_injected = false;\n\n\t \n\tif (svm->awaiting_iret_completion &&\n\t    kvm_rip_read(vcpu) != svm->nmi_iret_rip) {\n\t\tsvm->awaiting_iret_completion = false;\n\t\tsvm->nmi_masked = false;\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t}\n\n\tvcpu->arch.nmi_injected = false;\n\tkvm_clear_exception_queue(vcpu);\n\tkvm_clear_interrupt_queue(vcpu);\n\n\tif (!(exitintinfo & SVM_EXITINTINFO_VALID))\n\t\treturn;\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\tvector = exitintinfo & SVM_EXITINTINFO_VEC_MASK;\n\ttype = exitintinfo & SVM_EXITINTINFO_TYPE_MASK;\n\n\tif (soft_int_injected)\n\t\tsvm_complete_soft_interrupt(vcpu, vector, type);\n\n\tswitch (type) {\n\tcase SVM_EXITINTINFO_TYPE_NMI:\n\t\tvcpu->arch.nmi_injected = true;\n\t\tsvm->nmi_l1_to_l2 = nmi_l1_to_l2;\n\t\tbreak;\n\tcase SVM_EXITINTINFO_TYPE_EXEPT:\n\t\t \n\t\tif (vector == X86_TRAP_VC)\n\t\t\tbreak;\n\n\t\tif (exitintinfo & SVM_EXITINTINFO_VALID_ERR) {\n\t\t\tu32 err = svm->vmcb->control.exit_int_info_err;\n\t\t\tkvm_requeue_exception_e(vcpu, vector, err);\n\n\t\t} else\n\t\t\tkvm_requeue_exception(vcpu, vector);\n\t\tbreak;\n\tcase SVM_EXITINTINFO_TYPE_INTR:\n\t\tkvm_queue_interrupt(vcpu, vector, false);\n\t\tbreak;\n\tcase SVM_EXITINTINFO_TYPE_SOFT:\n\t\tkvm_queue_interrupt(vcpu, vector, true);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n}\n\nstatic void svm_cancel_injection(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct vmcb_control_area *control = &svm->vmcb->control;\n\n\tcontrol->exit_int_info = control->event_inj;\n\tcontrol->exit_int_info_err = control->event_inj_err;\n\tcontrol->event_inj = 0;\n\tsvm_complete_interrupts(vcpu);\n}\n\nstatic int svm_vcpu_pre_run(struct kvm_vcpu *vcpu)\n{\n\treturn 1;\n}\n\nstatic fastpath_t svm_exit_handlers_fastpath(struct kvm_vcpu *vcpu)\n{\n\tif (to_svm(vcpu)->vmcb->control.exit_code == SVM_EXIT_MSR &&\n\t    to_svm(vcpu)->vmcb->control.exit_info_1)\n\t\treturn handle_fastpath_set_msr_irqoff(vcpu);\n\n\treturn EXIT_FASTPATH_NONE;\n}\n\nstatic noinstr void svm_vcpu_enter_exit(struct kvm_vcpu *vcpu, bool spec_ctrl_intercepted)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tguest_state_enter_irqoff();\n\n\tamd_clear_divider();\n\n\tif (sev_es_guest(vcpu->kvm))\n\t\t__svm_sev_es_vcpu_run(svm, spec_ctrl_intercepted);\n\telse\n\t\t__svm_vcpu_run(svm, spec_ctrl_intercepted);\n\n\tguest_state_exit_irqoff();\n}\n\nstatic __no_kcsan fastpath_t svm_vcpu_run(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tbool spec_ctrl_intercepted = msr_write_intercepted(vcpu, MSR_IA32_SPEC_CTRL);\n\n\ttrace_kvm_entry(vcpu);\n\n\tsvm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];\n\tsvm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];\n\tsvm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];\n\n\t \n\tif (svm->nmi_singlestep && svm->vmcb->control.event_inj) {\n\t\t \n\t\tdisable_nmi_singlestep(svm);\n\t\tsmp_send_reschedule(vcpu->cpu);\n\t}\n\n\tpre_svm_run(vcpu);\n\n\tsync_lapic_to_cr8(vcpu);\n\n\tif (unlikely(svm->asid != svm->vmcb->control.asid)) {\n\t\tsvm->vmcb->control.asid = svm->asid;\n\t\tvmcb_mark_dirty(svm->vmcb, VMCB_ASID);\n\t}\n\tsvm->vmcb->save.cr2 = vcpu->arch.cr2;\n\n\tsvm_hv_update_vp_id(svm->vmcb, vcpu);\n\n\t \n\tif (unlikely(vcpu->arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT))\n\t\tsvm_set_dr6(svm, vcpu->arch.dr6);\n\telse\n\t\tsvm_set_dr6(svm, DR6_ACTIVE_LOW);\n\n\tclgi();\n\tkvm_load_guest_xsave_state(vcpu);\n\n\tkvm_wait_lapic_expire(vcpu);\n\n\t \n\tif (!static_cpu_has(X86_FEATURE_V_SPEC_CTRL))\n\t\tx86_spec_ctrl_set_guest(svm->virt_spec_ctrl);\n\n\tsvm_vcpu_enter_exit(vcpu, spec_ctrl_intercepted);\n\n\tif (!static_cpu_has(X86_FEATURE_V_SPEC_CTRL))\n\t\tx86_spec_ctrl_restore_host(svm->virt_spec_ctrl);\n\n\tif (!sev_es_guest(vcpu->kvm)) {\n\t\tvcpu->arch.cr2 = svm->vmcb->save.cr2;\n\t\tvcpu->arch.regs[VCPU_REGS_RAX] = svm->vmcb->save.rax;\n\t\tvcpu->arch.regs[VCPU_REGS_RSP] = svm->vmcb->save.rsp;\n\t\tvcpu->arch.regs[VCPU_REGS_RIP] = svm->vmcb->save.rip;\n\t}\n\tvcpu->arch.regs_dirty = 0;\n\n\tif (unlikely(svm->vmcb->control.exit_code == SVM_EXIT_NMI))\n\t\tkvm_before_interrupt(vcpu, KVM_HANDLING_NMI);\n\n\tkvm_load_host_xsave_state(vcpu);\n\tstgi();\n\n\t \n\n\tif (unlikely(svm->vmcb->control.exit_code == SVM_EXIT_NMI))\n\t\tkvm_after_interrupt(vcpu);\n\n\tsync_cr8_to_lapic(vcpu);\n\n\tsvm->next_rip = 0;\n\tif (is_guest_mode(vcpu)) {\n\t\tnested_sync_control_from_vmcb02(svm);\n\n\t\t \n\t\tif (svm->nested.nested_run_pending &&\n\t\t    svm->vmcb->control.exit_code != SVM_EXIT_ERR)\n                        ++vcpu->stat.nested_run;\n\n\t\tsvm->nested.nested_run_pending = 0;\n\t}\n\n\tsvm->vmcb->control.tlb_ctl = TLB_CONTROL_DO_NOTHING;\n\tvmcb_mark_all_clean(svm->vmcb);\n\n\t \n\tif (svm->vmcb->control.exit_code == SVM_EXIT_EXCP_BASE + PF_VECTOR)\n\t\tvcpu->arch.apf.host_apf_flags =\n\t\t\tkvm_read_and_reset_apf_flags();\n\n\tvcpu->arch.regs_avail &= ~SVM_REGS_LAZY_LOAD_SET;\n\n\t \n\tif (unlikely(svm->vmcb->control.exit_code ==\n\t\t     SVM_EXIT_EXCP_BASE + MC_VECTOR))\n\t\tsvm_handle_mce(vcpu);\n\n\ttrace_kvm_exit(vcpu, KVM_ISA_SVM);\n\n\tsvm_complete_interrupts(vcpu);\n\n\tif (is_guest_mode(vcpu))\n\t\treturn EXIT_FASTPATH_NONE;\n\n\treturn svm_exit_handlers_fastpath(vcpu);\n}\n\nstatic void svm_load_mmu_pgd(struct kvm_vcpu *vcpu, hpa_t root_hpa,\n\t\t\t     int root_level)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tunsigned long cr3;\n\n\tif (npt_enabled) {\n\t\tsvm->vmcb->control.nested_cr3 = __sme_set(root_hpa);\n\t\tvmcb_mark_dirty(svm->vmcb, VMCB_NPT);\n\n\t\thv_track_root_tdp(vcpu, root_hpa);\n\n\t\tcr3 = vcpu->arch.cr3;\n\t} else if (root_level >= PT64_ROOT_4LEVEL) {\n\t\tcr3 = __sme_set(root_hpa) | kvm_get_active_pcid(vcpu);\n\t} else {\n\t\t \n\t\tWARN_ON_ONCE(kvm_get_active_pcid(vcpu));\n\t\tcr3 = root_hpa;\n\t}\n\n\tsvm->vmcb->save.cr3 = cr3;\n\tvmcb_mark_dirty(svm->vmcb, VMCB_CR);\n}\n\nstatic void\nsvm_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)\n{\n\t \n\thypercall[0] = 0x0f;\n\thypercall[1] = 0x01;\n\thypercall[2] = 0xd9;\n}\n\n \nstatic bool svm_has_emulated_msr(struct kvm *kvm, u32 index)\n{\n\tswitch (index) {\n\tcase MSR_IA32_MCG_EXT_CTL:\n\tcase KVM_FIRST_EMULATED_VMX_MSR ... KVM_LAST_EMULATED_VMX_MSR:\n\t\treturn false;\n\tcase MSR_IA32_SMBASE:\n\t\tif (!IS_ENABLED(CONFIG_KVM_SMM))\n\t\t\treturn false;\n\t\t \n\t\tif (kvm && sev_es_guest(kvm))\n\t\t\treturn false;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn true;\n}\n\nstatic void svm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_XSAVE) &&\n\t    boot_cpu_has(X86_FEATURE_XSAVES) &&\n\t    guest_cpuid_has(vcpu, X86_FEATURE_XSAVE))\n\t\tkvm_governed_feature_set(vcpu, X86_FEATURE_XSAVES);\n\n\tkvm_governed_feature_check_and_set(vcpu, X86_FEATURE_NRIPS);\n\tkvm_governed_feature_check_and_set(vcpu, X86_FEATURE_TSCRATEMSR);\n\tkvm_governed_feature_check_and_set(vcpu, X86_FEATURE_LBRV);\n\n\t \n\tif (!guest_cpuid_is_intel(vcpu))\n\t\tkvm_governed_feature_check_and_set(vcpu, X86_FEATURE_V_VMSAVE_VMLOAD);\n\n\tkvm_governed_feature_check_and_set(vcpu, X86_FEATURE_PAUSEFILTER);\n\tkvm_governed_feature_check_and_set(vcpu, X86_FEATURE_PFTHRESHOLD);\n\tkvm_governed_feature_check_and_set(vcpu, X86_FEATURE_VGIF);\n\tkvm_governed_feature_check_and_set(vcpu, X86_FEATURE_VNMI);\n\n\tsvm_recalc_instruction_intercepts(vcpu, svm);\n\n\tif (boot_cpu_has(X86_FEATURE_IBPB))\n\t\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_PRED_CMD, 0,\n\t\t\t\t     !!guest_has_pred_cmd_msr(vcpu));\n\n\tif (boot_cpu_has(X86_FEATURE_FLUSH_L1D))\n\t\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_FLUSH_CMD, 0,\n\t\t\t\t     !!guest_cpuid_has(vcpu, X86_FEATURE_FLUSH_L1D));\n\n\tif (sev_guest(vcpu->kvm))\n\t\tsev_vcpu_after_set_cpuid(svm);\n\n\tinit_vmcb_after_set_cpuid(vcpu);\n}\n\nstatic bool svm_has_wbinvd_exit(void)\n{\n\treturn true;\n}\n\n#define PRE_EX(exit)  { .exit_code = (exit), \\\n\t\t\t.stage = X86_ICPT_PRE_EXCEPT, }\n#define POST_EX(exit) { .exit_code = (exit), \\\n\t\t\t.stage = X86_ICPT_POST_EXCEPT, }\n#define POST_MEM(exit) { .exit_code = (exit), \\\n\t\t\t.stage = X86_ICPT_POST_MEMACCESS, }\n\nstatic const struct __x86_intercept {\n\tu32 exit_code;\n\tenum x86_intercept_stage stage;\n} x86_intercept_map[] = {\n\t[x86_intercept_cr_read]\t\t= POST_EX(SVM_EXIT_READ_CR0),\n\t[x86_intercept_cr_write]\t= POST_EX(SVM_EXIT_WRITE_CR0),\n\t[x86_intercept_clts]\t\t= POST_EX(SVM_EXIT_WRITE_CR0),\n\t[x86_intercept_lmsw]\t\t= POST_EX(SVM_EXIT_WRITE_CR0),\n\t[x86_intercept_smsw]\t\t= POST_EX(SVM_EXIT_READ_CR0),\n\t[x86_intercept_dr_read]\t\t= POST_EX(SVM_EXIT_READ_DR0),\n\t[x86_intercept_dr_write]\t= POST_EX(SVM_EXIT_WRITE_DR0),\n\t[x86_intercept_sldt]\t\t= POST_EX(SVM_EXIT_LDTR_READ),\n\t[x86_intercept_str]\t\t= POST_EX(SVM_EXIT_TR_READ),\n\t[x86_intercept_lldt]\t\t= POST_EX(SVM_EXIT_LDTR_WRITE),\n\t[x86_intercept_ltr]\t\t= POST_EX(SVM_EXIT_TR_WRITE),\n\t[x86_intercept_sgdt]\t\t= POST_EX(SVM_EXIT_GDTR_READ),\n\t[x86_intercept_sidt]\t\t= POST_EX(SVM_EXIT_IDTR_READ),\n\t[x86_intercept_lgdt]\t\t= POST_EX(SVM_EXIT_GDTR_WRITE),\n\t[x86_intercept_lidt]\t\t= POST_EX(SVM_EXIT_IDTR_WRITE),\n\t[x86_intercept_vmrun]\t\t= POST_EX(SVM_EXIT_VMRUN),\n\t[x86_intercept_vmmcall]\t\t= POST_EX(SVM_EXIT_VMMCALL),\n\t[x86_intercept_vmload]\t\t= POST_EX(SVM_EXIT_VMLOAD),\n\t[x86_intercept_vmsave]\t\t= POST_EX(SVM_EXIT_VMSAVE),\n\t[x86_intercept_stgi]\t\t= POST_EX(SVM_EXIT_STGI),\n\t[x86_intercept_clgi]\t\t= POST_EX(SVM_EXIT_CLGI),\n\t[x86_intercept_skinit]\t\t= POST_EX(SVM_EXIT_SKINIT),\n\t[x86_intercept_invlpga]\t\t= POST_EX(SVM_EXIT_INVLPGA),\n\t[x86_intercept_rdtscp]\t\t= POST_EX(SVM_EXIT_RDTSCP),\n\t[x86_intercept_monitor]\t\t= POST_MEM(SVM_EXIT_MONITOR),\n\t[x86_intercept_mwait]\t\t= POST_EX(SVM_EXIT_MWAIT),\n\t[x86_intercept_invlpg]\t\t= POST_EX(SVM_EXIT_INVLPG),\n\t[x86_intercept_invd]\t\t= POST_EX(SVM_EXIT_INVD),\n\t[x86_intercept_wbinvd]\t\t= POST_EX(SVM_EXIT_WBINVD),\n\t[x86_intercept_wrmsr]\t\t= POST_EX(SVM_EXIT_MSR),\n\t[x86_intercept_rdtsc]\t\t= POST_EX(SVM_EXIT_RDTSC),\n\t[x86_intercept_rdmsr]\t\t= POST_EX(SVM_EXIT_MSR),\n\t[x86_intercept_rdpmc]\t\t= POST_EX(SVM_EXIT_RDPMC),\n\t[x86_intercept_cpuid]\t\t= PRE_EX(SVM_EXIT_CPUID),\n\t[x86_intercept_rsm]\t\t= PRE_EX(SVM_EXIT_RSM),\n\t[x86_intercept_pause]\t\t= PRE_EX(SVM_EXIT_PAUSE),\n\t[x86_intercept_pushf]\t\t= PRE_EX(SVM_EXIT_PUSHF),\n\t[x86_intercept_popf]\t\t= PRE_EX(SVM_EXIT_POPF),\n\t[x86_intercept_intn]\t\t= PRE_EX(SVM_EXIT_SWINT),\n\t[x86_intercept_iret]\t\t= PRE_EX(SVM_EXIT_IRET),\n\t[x86_intercept_icebp]\t\t= PRE_EX(SVM_EXIT_ICEBP),\n\t[x86_intercept_hlt]\t\t= POST_EX(SVM_EXIT_HLT),\n\t[x86_intercept_in]\t\t= POST_EX(SVM_EXIT_IOIO),\n\t[x86_intercept_ins]\t\t= POST_EX(SVM_EXIT_IOIO),\n\t[x86_intercept_out]\t\t= POST_EX(SVM_EXIT_IOIO),\n\t[x86_intercept_outs]\t\t= POST_EX(SVM_EXIT_IOIO),\n\t[x86_intercept_xsetbv]\t\t= PRE_EX(SVM_EXIT_XSETBV),\n};\n\n#undef PRE_EX\n#undef POST_EX\n#undef POST_MEM\n\nstatic int svm_check_intercept(struct kvm_vcpu *vcpu,\n\t\t\t       struct x86_instruction_info *info,\n\t\t\t       enum x86_intercept_stage stage,\n\t\t\t       struct x86_exception *exception)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tint vmexit, ret = X86EMUL_CONTINUE;\n\tstruct __x86_intercept icpt_info;\n\tstruct vmcb *vmcb = svm->vmcb;\n\n\tif (info->intercept >= ARRAY_SIZE(x86_intercept_map))\n\t\tgoto out;\n\n\ticpt_info = x86_intercept_map[info->intercept];\n\n\tif (stage != icpt_info.stage)\n\t\tgoto out;\n\n\tswitch (icpt_info.exit_code) {\n\tcase SVM_EXIT_READ_CR0:\n\t\tif (info->intercept == x86_intercept_cr_read)\n\t\t\ticpt_info.exit_code += info->modrm_reg;\n\t\tbreak;\n\tcase SVM_EXIT_WRITE_CR0: {\n\t\tunsigned long cr0, val;\n\n\t\tif (info->intercept == x86_intercept_cr_write)\n\t\t\ticpt_info.exit_code += info->modrm_reg;\n\n\t\tif (icpt_info.exit_code != SVM_EXIT_WRITE_CR0 ||\n\t\t    info->intercept == x86_intercept_clts)\n\t\t\tbreak;\n\n\t\tif (!(vmcb12_is_intercept(&svm->nested.ctl,\n\t\t\t\t\tINTERCEPT_SELECTIVE_CR0)))\n\t\t\tbreak;\n\n\t\tcr0 = vcpu->arch.cr0 & ~SVM_CR0_SELECTIVE_MASK;\n\t\tval = info->src_val  & ~SVM_CR0_SELECTIVE_MASK;\n\n\t\tif (info->intercept == x86_intercept_lmsw) {\n\t\t\tcr0 &= 0xfUL;\n\t\t\tval &= 0xfUL;\n\t\t\t \n\t\t\tif (cr0 & X86_CR0_PE)\n\t\t\t\tval |= X86_CR0_PE;\n\t\t}\n\n\t\tif (cr0 ^ val)\n\t\t\ticpt_info.exit_code = SVM_EXIT_CR0_SEL_WRITE;\n\n\t\tbreak;\n\t}\n\tcase SVM_EXIT_READ_DR0:\n\tcase SVM_EXIT_WRITE_DR0:\n\t\ticpt_info.exit_code += info->modrm_reg;\n\t\tbreak;\n\tcase SVM_EXIT_MSR:\n\t\tif (info->intercept == x86_intercept_wrmsr)\n\t\t\tvmcb->control.exit_info_1 = 1;\n\t\telse\n\t\t\tvmcb->control.exit_info_1 = 0;\n\t\tbreak;\n\tcase SVM_EXIT_PAUSE:\n\t\t \n\t\tif (info->rep_prefix != REPE_PREFIX)\n\t\t\tgoto out;\n\t\tbreak;\n\tcase SVM_EXIT_IOIO: {\n\t\tu64 exit_info;\n\t\tu32 bytes;\n\n\t\tif (info->intercept == x86_intercept_in ||\n\t\t    info->intercept == x86_intercept_ins) {\n\t\t\texit_info = ((info->src_val & 0xffff) << 16) |\n\t\t\t\tSVM_IOIO_TYPE_MASK;\n\t\t\tbytes = info->dst_bytes;\n\t\t} else {\n\t\t\texit_info = (info->dst_val & 0xffff) << 16;\n\t\t\tbytes = info->src_bytes;\n\t\t}\n\n\t\tif (info->intercept == x86_intercept_outs ||\n\t\t    info->intercept == x86_intercept_ins)\n\t\t\texit_info |= SVM_IOIO_STR_MASK;\n\n\t\tif (info->rep_prefix)\n\t\t\texit_info |= SVM_IOIO_REP_MASK;\n\n\t\tbytes = min(bytes, 4u);\n\n\t\texit_info |= bytes << SVM_IOIO_SIZE_SHIFT;\n\n\t\texit_info |= (u32)info->ad_bytes << (SVM_IOIO_ASIZE_SHIFT - 1);\n\n\t\tvmcb->control.exit_info_1 = exit_info;\n\t\tvmcb->control.exit_info_2 = info->next_rip;\n\n\t\tbreak;\n\t}\n\tdefault:\n\t\tbreak;\n\t}\n\n\t \n\tif (static_cpu_has(X86_FEATURE_NRIPS))\n\t\tvmcb->control.next_rip  = info->next_rip;\n\tvmcb->control.exit_code = icpt_info.exit_code;\n\tvmexit = nested_svm_exit_handled(svm);\n\n\tret = (vmexit == NESTED_EXIT_DONE) ? X86EMUL_INTERCEPTED\n\t\t\t\t\t   : X86EMUL_CONTINUE;\n\nout:\n\treturn ret;\n}\n\nstatic void svm_handle_exit_irqoff(struct kvm_vcpu *vcpu)\n{\n\tif (to_svm(vcpu)->vmcb->control.exit_code == SVM_EXIT_INTR)\n\t\tvcpu->arch.at_instruction_boundary = true;\n}\n\nstatic void svm_sched_in(struct kvm_vcpu *vcpu, int cpu)\n{\n\tif (!kvm_pause_in_guest(vcpu->kvm))\n\t\tshrink_ple_window(vcpu);\n}\n\nstatic void svm_setup_mce(struct kvm_vcpu *vcpu)\n{\n\t \n\tvcpu->arch.mcg_cap &= 0x1ff;\n}\n\n#ifdef CONFIG_KVM_SMM\nbool svm_smi_blocked(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\t \n\tif (!gif_set(svm))\n\t\treturn true;\n\n\treturn is_smm(vcpu);\n}\n\nstatic int svm_smi_allowed(struct kvm_vcpu *vcpu, bool for_injection)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tif (svm->nested.nested_run_pending)\n\t\treturn -EBUSY;\n\n\tif (svm_smi_blocked(vcpu))\n\t\treturn 0;\n\n\t \n\tif (for_injection && is_guest_mode(vcpu) && nested_exit_on_smi(svm))\n\t\treturn -EBUSY;\n\n\treturn 1;\n}\n\nstatic int svm_enter_smm(struct kvm_vcpu *vcpu, union kvm_smram *smram)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct kvm_host_map map_save;\n\tint ret;\n\n\tif (!is_guest_mode(vcpu))\n\t\treturn 0;\n\n\t \n\n\tif (!guest_cpuid_has(vcpu, X86_FEATURE_LM))\n\t\treturn 1;\n\n\tsmram->smram64.svm_guest_flag = 1;\n\tsmram->smram64.svm_guest_vmcb_gpa = svm->nested.vmcb12_gpa;\n\n\tsvm->vmcb->save.rax = vcpu->arch.regs[VCPU_REGS_RAX];\n\tsvm->vmcb->save.rsp = vcpu->arch.regs[VCPU_REGS_RSP];\n\tsvm->vmcb->save.rip = vcpu->arch.regs[VCPU_REGS_RIP];\n\n\tret = nested_svm_simple_vmexit(svm, SVM_EXIT_SW);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (kvm_vcpu_map(vcpu, gpa_to_gfn(svm->nested.hsave_msr), &map_save))\n\t\treturn 1;\n\n\tBUILD_BUG_ON(offsetof(struct vmcb, save) != 0x400);\n\n\tsvm_copy_vmrun_state(map_save.hva + 0x400,\n\t\t\t     &svm->vmcb01.ptr->save);\n\n\tkvm_vcpu_unmap(vcpu, &map_save, true);\n\treturn 0;\n}\n\nstatic int svm_leave_smm(struct kvm_vcpu *vcpu, const union kvm_smram *smram)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct kvm_host_map map, map_save;\n\tstruct vmcb *vmcb12;\n\tint ret;\n\n\tconst struct kvm_smram_state_64 *smram64 = &smram->smram64;\n\n\tif (!guest_cpuid_has(vcpu, X86_FEATURE_LM))\n\t\treturn 0;\n\n\t \n\tif (!smram64->svm_guest_flag)\n\t\treturn 0;\n\n\tif (!guest_cpuid_has(vcpu, X86_FEATURE_SVM))\n\t\treturn 1;\n\n\tif (!(smram64->efer & EFER_SVME))\n\t\treturn 1;\n\n\tif (kvm_vcpu_map(vcpu, gpa_to_gfn(smram64->svm_guest_vmcb_gpa), &map))\n\t\treturn 1;\n\n\tret = 1;\n\tif (kvm_vcpu_map(vcpu, gpa_to_gfn(svm->nested.hsave_msr), &map_save))\n\t\tgoto unmap_map;\n\n\tif (svm_allocate_nested(svm))\n\t\tgoto unmap_save;\n\n\t \n\n\tsvm_copy_vmrun_state(&svm->vmcb01.ptr->save, map_save.hva + 0x400);\n\n\t \n\n\tvmcb_mark_all_dirty(svm->vmcb01.ptr);\n\n\tvmcb12 = map.hva;\n\tnested_copy_vmcb_control_to_cache(svm, &vmcb12->control);\n\tnested_copy_vmcb_save_to_cache(svm, &vmcb12->save);\n\tret = enter_svm_guest_mode(vcpu, smram64->svm_guest_vmcb_gpa, vmcb12, false);\n\n\tif (ret)\n\t\tgoto unmap_save;\n\n\tsvm->nested.nested_run_pending = 1;\n\nunmap_save:\n\tkvm_vcpu_unmap(vcpu, &map_save, true);\nunmap_map:\n\tkvm_vcpu_unmap(vcpu, &map, true);\n\treturn ret;\n}\n\nstatic void svm_enable_smi_window(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (!gif_set(svm)) {\n\t\tif (vgif)\n\t\t\tsvm_set_intercept(svm, INTERCEPT_STGI);\n\t\t \n\t} else {\n\t\t \n\t}\n}\n#endif\n\nstatic bool svm_can_emulate_instruction(struct kvm_vcpu *vcpu, int emul_type,\n\t\t\t\t\tvoid *insn, int insn_len)\n{\n\tbool smep, smap, is_user;\n\tu64 error_code;\n\n\t \n\tif (!sev_guest(vcpu->kvm))\n\t\treturn true;\n\n\t \n\tWARN_ON_ONCE(emul_type & (EMULTYPE_TRAP_UD |\n\t\t\t\t  EMULTYPE_TRAP_UD_FORCED |\n\t\t\t\t  EMULTYPE_VMWARE_GP));\n\n\t \n\tif (sev_es_guest(vcpu->kvm))\n\t\treturn false;\n\n\t \n\tif (emul_type & EMULTYPE_NO_DECODE)\n\t\treturn true;\n\n\t \n\tif (unlikely(!insn)) {\n\t\tif (!(emul_type & EMULTYPE_SKIP))\n\t\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn false;\n\t}\n\n\t \n\tif (likely(insn_len))\n\t\treturn true;\n\n\t \n\terror_code = to_svm(vcpu)->vmcb->control.exit_info_1;\n\tif (error_code & (PFERR_GUEST_PAGE_MASK | PFERR_FETCH_MASK))\n\t\tgoto resume_guest;\n\n\tsmep = kvm_is_cr4_bit_set(vcpu, X86_CR4_SMEP);\n\tsmap = kvm_is_cr4_bit_set(vcpu, X86_CR4_SMAP);\n\tis_user = svm_get_cpl(vcpu) == 3;\n\tif (smap && (!smep || is_user)) {\n\t\tpr_err_ratelimited(\"SEV Guest triggered AMD Erratum 1096\\n\");\n\n\t\t \n\t\tif (is_user)\n\t\t\tkvm_inject_gp(vcpu, 0);\n\t\telse\n\t\t\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\t}\n\nresume_guest:\n\t \n\treturn false;\n}\n\nstatic bool svm_apic_init_signal_blocked(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\treturn !gif_set(svm);\n}\n\nstatic void svm_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, u8 vector)\n{\n\tif (!sev_es_guest(vcpu->kvm))\n\t\treturn kvm_vcpu_deliver_sipi_vector(vcpu, vector);\n\n\tsev_vcpu_deliver_sipi_vector(vcpu, vector);\n}\n\nstatic void svm_vm_destroy(struct kvm *kvm)\n{\n\tavic_vm_destroy(kvm);\n\tsev_vm_destroy(kvm);\n}\n\nstatic int svm_vm_init(struct kvm *kvm)\n{\n\tif (!pause_filter_count || !pause_filter_thresh)\n\t\tkvm->arch.pause_in_guest = true;\n\n\tif (enable_apicv) {\n\t\tint ret = avic_vm_init(kvm);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic struct kvm_x86_ops svm_x86_ops __initdata = {\n\t.name = KBUILD_MODNAME,\n\n\t.check_processor_compatibility = svm_check_processor_compat,\n\n\t.hardware_unsetup = svm_hardware_unsetup,\n\t.hardware_enable = svm_hardware_enable,\n\t.hardware_disable = svm_hardware_disable,\n\t.has_emulated_msr = svm_has_emulated_msr,\n\n\t.vcpu_create = svm_vcpu_create,\n\t.vcpu_free = svm_vcpu_free,\n\t.vcpu_reset = svm_vcpu_reset,\n\n\t.vm_size = sizeof(struct kvm_svm),\n\t.vm_init = svm_vm_init,\n\t.vm_destroy = svm_vm_destroy,\n\n\t.prepare_switch_to_guest = svm_prepare_switch_to_guest,\n\t.vcpu_load = svm_vcpu_load,\n\t.vcpu_put = svm_vcpu_put,\n\t.vcpu_blocking = avic_vcpu_blocking,\n\t.vcpu_unblocking = avic_vcpu_unblocking,\n\n\t.update_exception_bitmap = svm_update_exception_bitmap,\n\t.get_msr_feature = svm_get_msr_feature,\n\t.get_msr = svm_get_msr,\n\t.set_msr = svm_set_msr,\n\t.get_segment_base = svm_get_segment_base,\n\t.get_segment = svm_get_segment,\n\t.set_segment = svm_set_segment,\n\t.get_cpl = svm_get_cpl,\n\t.get_cs_db_l_bits = svm_get_cs_db_l_bits,\n\t.is_valid_cr0 = svm_is_valid_cr0,\n\t.set_cr0 = svm_set_cr0,\n\t.post_set_cr3 = sev_post_set_cr3,\n\t.is_valid_cr4 = svm_is_valid_cr4,\n\t.set_cr4 = svm_set_cr4,\n\t.set_efer = svm_set_efer,\n\t.get_idt = svm_get_idt,\n\t.set_idt = svm_set_idt,\n\t.get_gdt = svm_get_gdt,\n\t.set_gdt = svm_set_gdt,\n\t.set_dr7 = svm_set_dr7,\n\t.sync_dirty_debug_regs = svm_sync_dirty_debug_regs,\n\t.cache_reg = svm_cache_reg,\n\t.get_rflags = svm_get_rflags,\n\t.set_rflags = svm_set_rflags,\n\t.get_if_flag = svm_get_if_flag,\n\n\t.flush_tlb_all = svm_flush_tlb_all,\n\t.flush_tlb_current = svm_flush_tlb_current,\n\t.flush_tlb_gva = svm_flush_tlb_gva,\n\t.flush_tlb_guest = svm_flush_tlb_asid,\n\n\t.vcpu_pre_run = svm_vcpu_pre_run,\n\t.vcpu_run = svm_vcpu_run,\n\t.handle_exit = svm_handle_exit,\n\t.skip_emulated_instruction = svm_skip_emulated_instruction,\n\t.update_emulated_instruction = NULL,\n\t.set_interrupt_shadow = svm_set_interrupt_shadow,\n\t.get_interrupt_shadow = svm_get_interrupt_shadow,\n\t.patch_hypercall = svm_patch_hypercall,\n\t.inject_irq = svm_inject_irq,\n\t.inject_nmi = svm_inject_nmi,\n\t.is_vnmi_pending = svm_is_vnmi_pending,\n\t.set_vnmi_pending = svm_set_vnmi_pending,\n\t.inject_exception = svm_inject_exception,\n\t.cancel_injection = svm_cancel_injection,\n\t.interrupt_allowed = svm_interrupt_allowed,\n\t.nmi_allowed = svm_nmi_allowed,\n\t.get_nmi_mask = svm_get_nmi_mask,\n\t.set_nmi_mask = svm_set_nmi_mask,\n\t.enable_nmi_window = svm_enable_nmi_window,\n\t.enable_irq_window = svm_enable_irq_window,\n\t.update_cr8_intercept = svm_update_cr8_intercept,\n\t.set_virtual_apic_mode = avic_refresh_virtual_apic_mode,\n\t.refresh_apicv_exec_ctrl = avic_refresh_apicv_exec_ctrl,\n\t.apicv_post_state_restore = avic_apicv_post_state_restore,\n\t.required_apicv_inhibits = AVIC_REQUIRED_APICV_INHIBITS,\n\n\t.get_exit_info = svm_get_exit_info,\n\n\t.vcpu_after_set_cpuid = svm_vcpu_after_set_cpuid,\n\n\t.has_wbinvd_exit = svm_has_wbinvd_exit,\n\n\t.get_l2_tsc_offset = svm_get_l2_tsc_offset,\n\t.get_l2_tsc_multiplier = svm_get_l2_tsc_multiplier,\n\t.write_tsc_offset = svm_write_tsc_offset,\n\t.write_tsc_multiplier = svm_write_tsc_multiplier,\n\n\t.load_mmu_pgd = svm_load_mmu_pgd,\n\n\t.check_intercept = svm_check_intercept,\n\t.handle_exit_irqoff = svm_handle_exit_irqoff,\n\n\t.request_immediate_exit = __kvm_request_immediate_exit,\n\n\t.sched_in = svm_sched_in,\n\n\t.nested_ops = &svm_nested_ops,\n\n\t.deliver_interrupt = svm_deliver_interrupt,\n\t.pi_update_irte = avic_pi_update_irte,\n\t.setup_mce = svm_setup_mce,\n\n#ifdef CONFIG_KVM_SMM\n\t.smi_allowed = svm_smi_allowed,\n\t.enter_smm = svm_enter_smm,\n\t.leave_smm = svm_leave_smm,\n\t.enable_smi_window = svm_enable_smi_window,\n#endif\n\n\t.mem_enc_ioctl = sev_mem_enc_ioctl,\n\t.mem_enc_register_region = sev_mem_enc_register_region,\n\t.mem_enc_unregister_region = sev_mem_enc_unregister_region,\n\t.guest_memory_reclaimed = sev_guest_memory_reclaimed,\n\n\t.vm_copy_enc_context_from = sev_vm_copy_enc_context_from,\n\t.vm_move_enc_context_from = sev_vm_move_enc_context_from,\n\n\t.can_emulate_instruction = svm_can_emulate_instruction,\n\n\t.apic_init_signal_blocked = svm_apic_init_signal_blocked,\n\n\t.msr_filter_changed = svm_msr_filter_changed,\n\t.complete_emulated_msr = svm_complete_emulated_msr,\n\n\t.vcpu_deliver_sipi_vector = svm_vcpu_deliver_sipi_vector,\n\t.vcpu_get_apicv_inhibit_reasons = avic_vcpu_get_apicv_inhibit_reasons,\n};\n\n \nstatic __init void svm_adjust_mmio_mask(void)\n{\n\tunsigned int enc_bit, mask_bit;\n\tu64 msr, mask;\n\n\t \n\tif (cpuid_eax(0x80000000) < 0x8000001f)\n\t\treturn;\n\n\t \n\trdmsrl(MSR_AMD64_SYSCFG, msr);\n\tif (!(msr & MSR_AMD64_SYSCFG_MEM_ENCRYPT))\n\t\treturn;\n\n\tenc_bit = cpuid_ebx(0x8000001f) & 0x3f;\n\tmask_bit = boot_cpu_data.x86_phys_bits;\n\n\t \n\tif (enc_bit == mask_bit)\n\t\tmask_bit++;\n\n\t \n\tmask = (mask_bit < 52) ? rsvd_bits(mask_bit, 51) | PT_PRESENT_MASK : 0;\n\n\tkvm_mmu_set_mmio_spte_mask(mask, mask, PT_WRITABLE_MASK | PT_USER_MASK);\n}\n\nstatic __init void svm_set_cpu_caps(void)\n{\n\tkvm_set_cpu_caps();\n\n\tkvm_caps.supported_perf_cap = 0;\n\tkvm_caps.supported_xss = 0;\n\n\t \n\tif (nested) {\n\t\tkvm_cpu_cap_set(X86_FEATURE_SVM);\n\t\tkvm_cpu_cap_set(X86_FEATURE_VMCBCLEAN);\n\n\t\tif (nrips)\n\t\t\tkvm_cpu_cap_set(X86_FEATURE_NRIPS);\n\n\t\tif (npt_enabled)\n\t\t\tkvm_cpu_cap_set(X86_FEATURE_NPT);\n\n\t\tif (tsc_scaling)\n\t\t\tkvm_cpu_cap_set(X86_FEATURE_TSCRATEMSR);\n\n\t\tif (vls)\n\t\t\tkvm_cpu_cap_set(X86_FEATURE_V_VMSAVE_VMLOAD);\n\t\tif (lbrv)\n\t\t\tkvm_cpu_cap_set(X86_FEATURE_LBRV);\n\n\t\tif (boot_cpu_has(X86_FEATURE_PAUSEFILTER))\n\t\t\tkvm_cpu_cap_set(X86_FEATURE_PAUSEFILTER);\n\n\t\tif (boot_cpu_has(X86_FEATURE_PFTHRESHOLD))\n\t\t\tkvm_cpu_cap_set(X86_FEATURE_PFTHRESHOLD);\n\n\t\tif (vgif)\n\t\t\tkvm_cpu_cap_set(X86_FEATURE_VGIF);\n\n\t\tif (vnmi)\n\t\t\tkvm_cpu_cap_set(X86_FEATURE_VNMI);\n\n\t\t \n\t\tkvm_cpu_cap_set(X86_FEATURE_SVME_ADDR_CHK);\n\t}\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD) ||\n\t    boot_cpu_has(X86_FEATURE_AMD_SSBD))\n\t\tkvm_cpu_cap_set(X86_FEATURE_VIRT_SSBD);\n\n\tif (enable_pmu) {\n\t\t \n\t\tif (kvm_pmu_cap.num_counters_gp < AMD64_NUM_COUNTERS_CORE)\n\t\t\tkvm_pmu_cap.num_counters_gp = min(AMD64_NUM_COUNTERS,\n\t\t\t\t\t\t\t  kvm_pmu_cap.num_counters_gp);\n\t\telse\n\t\t\tkvm_cpu_cap_check_and_set(X86_FEATURE_PERFCTR_CORE);\n\n\t\tif (kvm_pmu_cap.version != 2 ||\n\t\t    !kvm_cpu_cap_has(X86_FEATURE_PERFCTR_CORE))\n\t\t\tkvm_cpu_cap_clear(X86_FEATURE_PERFMON_V2);\n\t}\n\n\t \n\tsev_set_cpu_caps();\n}\n\nstatic __init int svm_hardware_setup(void)\n{\n\tint cpu;\n\tstruct page *iopm_pages;\n\tvoid *iopm_va;\n\tint r;\n\tunsigned int order = get_order(IOPM_SIZE);\n\n\t \n\tif (!boot_cpu_has(X86_FEATURE_NX)) {\n\t\tpr_err_ratelimited(\"NX (Execute Disable) not supported\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\tkvm_enable_efer_bits(EFER_NX);\n\n\tiopm_pages = alloc_pages(GFP_KERNEL, order);\n\n\tif (!iopm_pages)\n\t\treturn -ENOMEM;\n\n\tiopm_va = page_address(iopm_pages);\n\tmemset(iopm_va, 0xff, PAGE_SIZE * (1 << order));\n\tiopm_base = page_to_pfn(iopm_pages) << PAGE_SHIFT;\n\n\tinit_msrpm_offsets();\n\n\tkvm_caps.supported_xcr0 &= ~(XFEATURE_MASK_BNDREGS |\n\t\t\t\t     XFEATURE_MASK_BNDCSR);\n\n\tif (boot_cpu_has(X86_FEATURE_FXSR_OPT))\n\t\tkvm_enable_efer_bits(EFER_FFXSR);\n\n\tif (tsc_scaling) {\n\t\tif (!boot_cpu_has(X86_FEATURE_TSCRATEMSR)) {\n\t\t\ttsc_scaling = false;\n\t\t} else {\n\t\t\tpr_info(\"TSC scaling supported\\n\");\n\t\t\tkvm_caps.has_tsc_control = true;\n\t\t}\n\t}\n\tkvm_caps.max_tsc_scaling_ratio = SVM_TSC_RATIO_MAX;\n\tkvm_caps.tsc_scaling_ratio_frac_bits = 32;\n\n\ttsc_aux_uret_slot = kvm_add_user_return_msr(MSR_TSC_AUX);\n\n\tif (boot_cpu_has(X86_FEATURE_AUTOIBRS))\n\t\tkvm_enable_efer_bits(EFER_AUTOIBRS);\n\n\t \n\tif (!boot_cpu_has(X86_FEATURE_PAUSEFILTER)) {\n\t\tpause_filter_count = 0;\n\t\tpause_filter_thresh = 0;\n\t} else if (!boot_cpu_has(X86_FEATURE_PFTHRESHOLD)) {\n\t\tpause_filter_thresh = 0;\n\t}\n\n\tif (nested) {\n\t\tpr_info(\"Nested Virtualization enabled\\n\");\n\t\tkvm_enable_efer_bits(EFER_SVME | EFER_LMSLE);\n\t}\n\n\t \n\tif (!IS_ENABLED(CONFIG_X86_64) && !IS_ENABLED(CONFIG_X86_PAE))\n\t\tnpt_enabled = false;\n\n\tif (!boot_cpu_has(X86_FEATURE_NPT))\n\t\tnpt_enabled = false;\n\n\t \n\tkvm_configure_mmu(npt_enabled, get_npt_level(),\n\t\t\t  get_npt_level(), PG_LEVEL_1G);\n\tpr_info(\"Nested Paging %sabled\\n\", npt_enabled ? \"en\" : \"dis\");\n\n\t \n\tkvm_mmu_set_me_spte_mask(sme_me_mask, sme_me_mask);\n\n\tsvm_adjust_mmio_mask();\n\n\tnrips = nrips && boot_cpu_has(X86_FEATURE_NRIPS);\n\n\t \n\tsev_hardware_setup();\n\n\tsvm_hv_hardware_setup();\n\n\tfor_each_possible_cpu(cpu) {\n\t\tr = svm_cpu_init(cpu);\n\t\tif (r)\n\t\t\tgoto err;\n\t}\n\n\tenable_apicv = avic = avic && avic_hardware_setup();\n\n\tif (!enable_apicv) {\n\t\tsvm_x86_ops.vcpu_blocking = NULL;\n\t\tsvm_x86_ops.vcpu_unblocking = NULL;\n\t\tsvm_x86_ops.vcpu_get_apicv_inhibit_reasons = NULL;\n\t} else if (!x2avic_enabled) {\n\t\tsvm_x86_ops.allow_apicv_in_x2apic_without_x2apic_virtualization = true;\n\t}\n\n\tif (vls) {\n\t\tif (!npt_enabled ||\n\t\t    !boot_cpu_has(X86_FEATURE_V_VMSAVE_VMLOAD) ||\n\t\t    !IS_ENABLED(CONFIG_X86_64)) {\n\t\t\tvls = false;\n\t\t} else {\n\t\t\tpr_info(\"Virtual VMLOAD VMSAVE supported\\n\");\n\t\t}\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_SVME_ADDR_CHK))\n\t\tsvm_gp_erratum_intercept = false;\n\n\tif (vgif) {\n\t\tif (!boot_cpu_has(X86_FEATURE_VGIF))\n\t\t\tvgif = false;\n\t\telse\n\t\t\tpr_info(\"Virtual GIF supported\\n\");\n\t}\n\n\tvnmi = vgif && vnmi && boot_cpu_has(X86_FEATURE_VNMI);\n\tif (vnmi)\n\t\tpr_info(\"Virtual NMI enabled\\n\");\n\n\tif (!vnmi) {\n\t\tsvm_x86_ops.is_vnmi_pending = NULL;\n\t\tsvm_x86_ops.set_vnmi_pending = NULL;\n\t}\n\n\n\tif (lbrv) {\n\t\tif (!boot_cpu_has(X86_FEATURE_LBRV))\n\t\t\tlbrv = false;\n\t\telse\n\t\t\tpr_info(\"LBR virtualization supported\\n\");\n\t}\n\n\tif (!enable_pmu)\n\t\tpr_info(\"PMU virtualization is disabled\\n\");\n\n\tsvm_set_cpu_caps();\n\n\t \n\tallow_smaller_maxphyaddr = !npt_enabled;\n\n\treturn 0;\n\nerr:\n\tsvm_hardware_unsetup();\n\treturn r;\n}\n\n\nstatic struct kvm_x86_init_ops svm_init_ops __initdata = {\n\t.hardware_setup = svm_hardware_setup,\n\n\t.runtime_ops = &svm_x86_ops,\n\t.pmu_ops = &amd_pmu_ops,\n};\n\nstatic void __svm_exit(void)\n{\n\tkvm_x86_vendor_exit();\n\n\tcpu_emergency_unregister_virt_callback(svm_emergency_disable);\n}\n\nstatic int __init svm_init(void)\n{\n\tint r;\n\n\t__unused_size_checks();\n\n\tif (!kvm_is_svm_supported())\n\t\treturn -EOPNOTSUPP;\n\n\tr = kvm_x86_vendor_init(&svm_init_ops);\n\tif (r)\n\t\treturn r;\n\n\tcpu_emergency_register_virt_callback(svm_emergency_disable);\n\n\t \n\tr = kvm_init(sizeof(struct vcpu_svm), __alignof__(struct vcpu_svm),\n\t\t     THIS_MODULE);\n\tif (r)\n\t\tgoto err_kvm_init;\n\n\treturn 0;\n\nerr_kvm_init:\n\t__svm_exit();\n\treturn r;\n}\n\nstatic void __exit svm_exit(void)\n{\n\tkvm_exit();\n\t__svm_exit();\n}\n\nmodule_init(svm_init)\nmodule_exit(svm_exit)\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}