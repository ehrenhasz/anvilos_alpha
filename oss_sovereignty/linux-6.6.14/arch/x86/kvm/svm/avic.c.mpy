{
  "module_name": "avic.c",
  "hash_id": "5107411f06de5fe81f3a52cd4b57e7c1dfe39dd38c28881d845c4602038bdd8c",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/svm/avic.c",
  "human_readable_source": "\n \n\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/kvm_types.h>\n#include <linux/hashtable.h>\n#include <linux/amd-iommu.h>\n#include <linux/kvm_host.h>\n\n#include <asm/irq_remapping.h>\n\n#include \"trace.h\"\n#include \"lapic.h\"\n#include \"x86.h\"\n#include \"irq.h\"\n#include \"svm.h\"\n\n \n#define AVIC_VCPU_ID_MASK\t\tAVIC_PHYSICAL_MAX_INDEX_MASK\n\n#define AVIC_VM_ID_SHIFT\t\tHWEIGHT32(AVIC_PHYSICAL_MAX_INDEX_MASK)\n#define AVIC_VM_ID_MASK\t\t\t(GENMASK(31, AVIC_VM_ID_SHIFT) >> AVIC_VM_ID_SHIFT)\n\n#define AVIC_GATAG_TO_VMID(x)\t\t((x >> AVIC_VM_ID_SHIFT) & AVIC_VM_ID_MASK)\n#define AVIC_GATAG_TO_VCPUID(x)\t\t(x & AVIC_VCPU_ID_MASK)\n\n#define __AVIC_GATAG(vm_id, vcpu_id)\t((((vm_id) & AVIC_VM_ID_MASK) << AVIC_VM_ID_SHIFT) | \\\n\t\t\t\t\t ((vcpu_id) & AVIC_VCPU_ID_MASK))\n#define AVIC_GATAG(vm_id, vcpu_id)\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tu32 ga_tag = __AVIC_GATAG(vm_id, vcpu_id);\t\t\t\\\n\t\t\t\t\t\t\t\t\t\\\n\tWARN_ON_ONCE(AVIC_GATAG_TO_VCPUID(ga_tag) != (vcpu_id));\t\\\n\tWARN_ON_ONCE(AVIC_GATAG_TO_VMID(ga_tag) != (vm_id));\t\t\\\n\tga_tag;\t\t\t\t\t\t\t\t\\\n})\n\nstatic_assert(__AVIC_GATAG(AVIC_VM_ID_MASK, AVIC_VCPU_ID_MASK) == -1u);\n\nstatic bool force_avic;\nmodule_param_unsafe(force_avic, bool, 0444);\n\n \n#define SVM_VM_DATA_HASH_BITS\t8\nstatic DEFINE_HASHTABLE(svm_vm_data_hash, SVM_VM_DATA_HASH_BITS);\nstatic u32 next_vm_id = 0;\nstatic bool next_vm_id_wrapped = 0;\nstatic DEFINE_SPINLOCK(svm_vm_data_hash_lock);\nbool x2avic_enabled;\n\n \nstruct amd_svm_iommu_ir {\n\tstruct list_head node;\t \n\tvoid *data;\t\t \n};\n\nstatic void avic_activate_vmcb(struct vcpu_svm *svm)\n{\n\tstruct vmcb *vmcb = svm->vmcb01.ptr;\n\n\tvmcb->control.int_ctl &= ~(AVIC_ENABLE_MASK | X2APIC_MODE_MASK);\n\tvmcb->control.avic_physical_id &= ~AVIC_PHYSICAL_MAX_INDEX_MASK;\n\n\tvmcb->control.int_ctl |= AVIC_ENABLE_MASK;\n\n\t \n\tif (x2avic_enabled && apic_x2apic_mode(svm->vcpu.arch.apic)) {\n\t\tvmcb->control.int_ctl |= X2APIC_MODE_MASK;\n\t\tvmcb->control.avic_physical_id |= X2AVIC_MAX_PHYSICAL_ID;\n\t\t \n\t\tsvm_set_x2apic_msr_interception(svm, false);\n\t} else {\n\t\t \n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_CURRENT, &svm->vcpu);\n\n\t\t \n\t\tvmcb->control.avic_physical_id |= AVIC_MAX_PHYSICAL_ID;\n\t\t \n\t\tsvm_set_x2apic_msr_interception(svm, true);\n\t}\n}\n\nstatic void avic_deactivate_vmcb(struct vcpu_svm *svm)\n{\n\tstruct vmcb *vmcb = svm->vmcb01.ptr;\n\n\tvmcb->control.int_ctl &= ~(AVIC_ENABLE_MASK | X2APIC_MODE_MASK);\n\tvmcb->control.avic_physical_id &= ~AVIC_PHYSICAL_MAX_INDEX_MASK;\n\n\t \n\tif (is_guest_mode(&svm->vcpu) &&\n\t    vmcb12_is_intercept(&svm->nested.ctl, INTERCEPT_MSR_PROT))\n\t\treturn;\n\n\t \n\tsvm_set_x2apic_msr_interception(svm, true);\n}\n\n \nint avic_ga_log_notifier(u32 ga_tag)\n{\n\tunsigned long flags;\n\tstruct kvm_svm *kvm_svm;\n\tstruct kvm_vcpu *vcpu = NULL;\n\tu32 vm_id = AVIC_GATAG_TO_VMID(ga_tag);\n\tu32 vcpu_id = AVIC_GATAG_TO_VCPUID(ga_tag);\n\n\tpr_debug(\"SVM: %s: vm_id=%#x, vcpu_id=%#x\\n\", __func__, vm_id, vcpu_id);\n\ttrace_kvm_avic_ga_log(vm_id, vcpu_id);\n\n\tspin_lock_irqsave(&svm_vm_data_hash_lock, flags);\n\thash_for_each_possible(svm_vm_data_hash, kvm_svm, hnode, vm_id) {\n\t\tif (kvm_svm->avic_vm_id != vm_id)\n\t\t\tcontinue;\n\t\tvcpu = kvm_get_vcpu_by_id(&kvm_svm->kvm, vcpu_id);\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&svm_vm_data_hash_lock, flags);\n\n\t \n\tif (vcpu)\n\t\tkvm_vcpu_wake_up(vcpu);\n\n\treturn 0;\n}\n\nvoid avic_vm_destroy(struct kvm *kvm)\n{\n\tunsigned long flags;\n\tstruct kvm_svm *kvm_svm = to_kvm_svm(kvm);\n\n\tif (!enable_apicv)\n\t\treturn;\n\n\tif (kvm_svm->avic_logical_id_table_page)\n\t\t__free_page(kvm_svm->avic_logical_id_table_page);\n\tif (kvm_svm->avic_physical_id_table_page)\n\t\t__free_page(kvm_svm->avic_physical_id_table_page);\n\n\tspin_lock_irqsave(&svm_vm_data_hash_lock, flags);\n\thash_del(&kvm_svm->hnode);\n\tspin_unlock_irqrestore(&svm_vm_data_hash_lock, flags);\n}\n\nint avic_vm_init(struct kvm *kvm)\n{\n\tunsigned long flags;\n\tint err = -ENOMEM;\n\tstruct kvm_svm *kvm_svm = to_kvm_svm(kvm);\n\tstruct kvm_svm *k2;\n\tstruct page *p_page;\n\tstruct page *l_page;\n\tu32 vm_id;\n\n\tif (!enable_apicv)\n\t\treturn 0;\n\n\t \n\tp_page = alloc_page(GFP_KERNEL_ACCOUNT | __GFP_ZERO);\n\tif (!p_page)\n\t\tgoto free_avic;\n\n\tkvm_svm->avic_physical_id_table_page = p_page;\n\n\t \n\tl_page = alloc_page(GFP_KERNEL_ACCOUNT | __GFP_ZERO);\n\tif (!l_page)\n\t\tgoto free_avic;\n\n\tkvm_svm->avic_logical_id_table_page = l_page;\n\n\tspin_lock_irqsave(&svm_vm_data_hash_lock, flags);\n again:\n\tvm_id = next_vm_id = (next_vm_id + 1) & AVIC_VM_ID_MASK;\n\tif (vm_id == 0) {  \n\t\tnext_vm_id_wrapped = 1;\n\t\tgoto again;\n\t}\n\t \n\tif (next_vm_id_wrapped) {\n\t\thash_for_each_possible(svm_vm_data_hash, k2, hnode, vm_id) {\n\t\t\tif (k2->avic_vm_id == vm_id)\n\t\t\t\tgoto again;\n\t\t}\n\t}\n\tkvm_svm->avic_vm_id = vm_id;\n\thash_add(svm_vm_data_hash, &kvm_svm->hnode, kvm_svm->avic_vm_id);\n\tspin_unlock_irqrestore(&svm_vm_data_hash_lock, flags);\n\n\treturn 0;\n\nfree_avic:\n\tavic_vm_destroy(kvm);\n\treturn err;\n}\n\nvoid avic_init_vmcb(struct vcpu_svm *svm, struct vmcb *vmcb)\n{\n\tstruct kvm_svm *kvm_svm = to_kvm_svm(svm->vcpu.kvm);\n\tphys_addr_t bpa = __sme_set(page_to_phys(svm->avic_backing_page));\n\tphys_addr_t lpa = __sme_set(page_to_phys(kvm_svm->avic_logical_id_table_page));\n\tphys_addr_t ppa = __sme_set(page_to_phys(kvm_svm->avic_physical_id_table_page));\n\n\tvmcb->control.avic_backing_page = bpa & AVIC_HPA_MASK;\n\tvmcb->control.avic_logical_id = lpa & AVIC_HPA_MASK;\n\tvmcb->control.avic_physical_id = ppa & AVIC_HPA_MASK;\n\tvmcb->control.avic_vapic_bar = APIC_DEFAULT_PHYS_BASE & VMCB_AVIC_APIC_BAR_MASK;\n\n\tif (kvm_apicv_activated(svm->vcpu.kvm))\n\t\tavic_activate_vmcb(svm);\n\telse\n\t\tavic_deactivate_vmcb(svm);\n}\n\nstatic u64 *avic_get_physical_id_entry(struct kvm_vcpu *vcpu,\n\t\t\t\t       unsigned int index)\n{\n\tu64 *avic_physical_id_table;\n\tstruct kvm_svm *kvm_svm = to_kvm_svm(vcpu->kvm);\n\n\tif ((!x2avic_enabled && index > AVIC_MAX_PHYSICAL_ID) ||\n\t    (index > X2AVIC_MAX_PHYSICAL_ID))\n\t\treturn NULL;\n\n\tavic_physical_id_table = page_address(kvm_svm->avic_physical_id_table_page);\n\n\treturn &avic_physical_id_table[index];\n}\n\nstatic int avic_init_backing_page(struct kvm_vcpu *vcpu)\n{\n\tu64 *entry, new_entry;\n\tint id = vcpu->vcpu_id;\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif ((!x2avic_enabled && id > AVIC_MAX_PHYSICAL_ID) ||\n\t    (id > X2AVIC_MAX_PHYSICAL_ID))\n\t\treturn -EINVAL;\n\n\tif (!vcpu->arch.apic->regs)\n\t\treturn -EINVAL;\n\n\tif (kvm_apicv_activated(vcpu->kvm)) {\n\t\tint ret;\n\n\t\t \n\t\tret = kvm_alloc_apic_access_page(vcpu->kvm);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tsvm->avic_backing_page = virt_to_page(vcpu->arch.apic->regs);\n\n\t \n\tentry = avic_get_physical_id_entry(vcpu, id);\n\tif (!entry)\n\t\treturn -EINVAL;\n\n\tnew_entry = __sme_set((page_to_phys(svm->avic_backing_page) &\n\t\t\t      AVIC_PHYSICAL_ID_ENTRY_BACKING_PAGE_MASK) |\n\t\t\t      AVIC_PHYSICAL_ID_ENTRY_VALID_MASK);\n\tWRITE_ONCE(*entry, new_entry);\n\n\tsvm->avic_physical_id_cache = entry;\n\n\treturn 0;\n}\n\nvoid avic_ring_doorbell(struct kvm_vcpu *vcpu)\n{\n\t \n\tint cpu = READ_ONCE(vcpu->cpu);\n\n\tif (cpu != get_cpu()) {\n\t\twrmsrl(MSR_AMD64_SVM_AVIC_DOORBELL, kvm_cpu_get_apicid(cpu));\n\t\ttrace_kvm_avic_doorbell(vcpu->vcpu_id, kvm_cpu_get_apicid(cpu));\n\t}\n\tput_cpu();\n}\n\n\nstatic void avic_kick_vcpu(struct kvm_vcpu *vcpu, u32 icrl)\n{\n\tvcpu->arch.apic->irr_pending = true;\n\tsvm_complete_interrupt_delivery(vcpu,\n\t\t\t\t\ticrl & APIC_MODE_MASK,\n\t\t\t\t\ticrl & APIC_INT_LEVELTRIG,\n\t\t\t\t\ticrl & APIC_VECTOR_MASK);\n}\n\nstatic void avic_kick_vcpu_by_physical_id(struct kvm *kvm, u32 physical_id,\n\t\t\t\t\t  u32 icrl)\n{\n\t \n\tstruct kvm_vcpu *target_vcpu = kvm_get_vcpu_by_id(kvm, physical_id);\n\n\t \n\tif (unlikely(!target_vcpu))\n\t\treturn;\n\n\tavic_kick_vcpu(target_vcpu, icrl);\n}\n\nstatic void avic_kick_vcpu_by_logical_id(struct kvm *kvm, u32 *avic_logical_id_table,\n\t\t\t\t\t u32 logid_index, u32 icrl)\n{\n\tu32 physical_id;\n\n\tif (avic_logical_id_table) {\n\t\tu32 logid_entry = avic_logical_id_table[logid_index];\n\n\t\t \n\t\tif (unlikely(!(logid_entry & AVIC_LOGICAL_ID_ENTRY_VALID_MASK)))\n\t\t\treturn;\n\n\t\tphysical_id = logid_entry &\n\t\t\t      AVIC_LOGICAL_ID_ENTRY_GUEST_PHYSICAL_ID_MASK;\n\t} else {\n\t\t \n\t\tphysical_id = logid_index;\n\t}\n\n\tavic_kick_vcpu_by_physical_id(kvm, physical_id, icrl);\n}\n\n \nstatic int avic_kick_target_vcpus_fast(struct kvm *kvm, struct kvm_lapic *source,\n\t\t\t\t       u32 icrl, u32 icrh, u32 index)\n{\n\tint dest_mode = icrl & APIC_DEST_MASK;\n\tint shorthand = icrl & APIC_SHORT_MASK;\n\tstruct kvm_svm *kvm_svm = to_kvm_svm(kvm);\n\tu32 dest;\n\n\tif (shorthand != APIC_DEST_NOSHORT)\n\t\treturn -EINVAL;\n\n\tif (apic_x2apic_mode(source))\n\t\tdest = icrh;\n\telse\n\t\tdest = GET_XAPIC_DEST_FIELD(icrh);\n\n\tif (dest_mode == APIC_DEST_PHYSICAL) {\n\t\t \n\t\tif (apic_x2apic_mode(source) && dest == X2APIC_BROADCAST)\n\t\t\treturn -EINVAL;\n\t\tif (!apic_x2apic_mode(source) && dest == APIC_BROADCAST)\n\t\t\treturn -EINVAL;\n\n\t\tif (WARN_ON_ONCE(dest != index))\n\t\t\treturn -EINVAL;\n\n\t\tavic_kick_vcpu_by_physical_id(kvm, dest, icrl);\n\t} else {\n\t\tu32 *avic_logical_id_table;\n\t\tunsigned long bitmap, i;\n\t\tu32 cluster;\n\n\t\tif (apic_x2apic_mode(source)) {\n\t\t\t \n\t\t\tbitmap = dest & 0xFFFF;\n\t\t\tcluster = (dest >> 16) << 4;\n\t\t} else if (kvm_lapic_get_reg(source, APIC_DFR) == APIC_DFR_FLAT) {\n\t\t\t \n\t\t\tbitmap = dest;\n\t\t\tcluster = 0;\n\t\t} else {\n\t\t\t \n\t\t\tbitmap = dest & 0xF;\n\t\t\tcluster = (dest >> 4) << 2;\n\t\t}\n\n\t\t \n\t\tif (unlikely(!bitmap))\n\t\t\treturn 0;\n\n\t\tif (apic_x2apic_mode(source))\n\t\t\tavic_logical_id_table = NULL;\n\t\telse\n\t\t\tavic_logical_id_table = page_address(kvm_svm->avic_logical_id_table_page);\n\n\t\t \n\t\tfor_each_set_bit(i, &bitmap, 16)\n\t\t\tavic_kick_vcpu_by_logical_id(kvm, avic_logical_id_table,\n\t\t\t\t\t\t     cluster + i, icrl);\n\t}\n\n\treturn 0;\n}\n\nstatic void avic_kick_target_vcpus(struct kvm *kvm, struct kvm_lapic *source,\n\t\t\t\t   u32 icrl, u32 icrh, u32 index)\n{\n\tu32 dest = apic_x2apic_mode(source) ? icrh : GET_XAPIC_DEST_FIELD(icrh);\n\tunsigned long i;\n\tstruct kvm_vcpu *vcpu;\n\n\tif (!avic_kick_target_vcpus_fast(kvm, source, icrl, icrh, index))\n\t\treturn;\n\n\ttrace_kvm_avic_kick_vcpu_slowpath(icrh, icrl, index);\n\n\t \n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tif (kvm_apic_match_dest(vcpu, source, icrl & APIC_SHORT_MASK,\n\t\t\t\t\tdest, icrl & APIC_DEST_MASK))\n\t\t\tavic_kick_vcpu(vcpu, icrl);\n\t}\n}\n\nint avic_incomplete_ipi_interception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu32 icrh = svm->vmcb->control.exit_info_1 >> 32;\n\tu32 icrl = svm->vmcb->control.exit_info_1;\n\tu32 id = svm->vmcb->control.exit_info_2 >> 32;\n\tu32 index = svm->vmcb->control.exit_info_2 & 0x1FF;\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\ttrace_kvm_avic_incomplete_ipi(vcpu->vcpu_id, icrh, icrl, id, index);\n\n\tswitch (id) {\n\tcase AVIC_IPI_FAILURE_INVALID_TARGET:\n\tcase AVIC_IPI_FAILURE_INVALID_INT_TYPE:\n\t\t \n\t\tif (icrl & APIC_ICR_BUSY)\n\t\t\tkvm_apic_write_nodecode(vcpu, APIC_ICR);\n\t\telse\n\t\t\tkvm_apic_send_ipi(apic, icrl, icrh);\n\t\tbreak;\n\tcase AVIC_IPI_FAILURE_TARGET_NOT_RUNNING:\n\t\t \n\t\tavic_kick_target_vcpus(vcpu->kvm, apic, icrl, icrh, index);\n\t\tbreak;\n\tcase AVIC_IPI_FAILURE_INVALID_BACKING_PAGE:\n\t\tWARN_ONCE(1, \"Invalid backing page\\n\");\n\t\tbreak;\n\tcase AVIC_IPI_FAILURE_INVALID_IPI_VECTOR:\n\t\t \n\t\tbreak;\n\tdefault:\n\t\tvcpu_unimpl(vcpu, \"Unknown avic incomplete IPI interception\\n\");\n\t}\n\n\treturn 1;\n}\n\nunsigned long avic_vcpu_get_apicv_inhibit_reasons(struct kvm_vcpu *vcpu)\n{\n\tif (is_guest_mode(vcpu))\n\t\treturn APICV_INHIBIT_REASON_NESTED;\n\treturn 0;\n}\n\nstatic u32 *avic_get_logical_id_entry(struct kvm_vcpu *vcpu, u32 ldr, bool flat)\n{\n\tstruct kvm_svm *kvm_svm = to_kvm_svm(vcpu->kvm);\n\tu32 *logical_apic_id_table;\n\tu32 cluster, index;\n\n\tldr = GET_APIC_LOGICAL_ID(ldr);\n\n\tif (flat) {\n\t\tcluster = 0;\n\t} else {\n\t\tcluster = (ldr >> 4);\n\t\tif (cluster >= 0xf)\n\t\t\treturn NULL;\n\t\tldr &= 0xf;\n\t}\n\tif (!ldr || !is_power_of_2(ldr))\n\t\treturn NULL;\n\n\tindex = __ffs(ldr);\n\tif (WARN_ON_ONCE(index > 7))\n\t\treturn NULL;\n\tindex += (cluster << 2);\n\n\tlogical_apic_id_table = (u32 *) page_address(kvm_svm->avic_logical_id_table_page);\n\n\treturn &logical_apic_id_table[index];\n}\n\nstatic void avic_ldr_write(struct kvm_vcpu *vcpu, u8 g_physical_id, u32 ldr)\n{\n\tbool flat;\n\tu32 *entry, new_entry;\n\n\tflat = kvm_lapic_get_reg(vcpu->arch.apic, APIC_DFR) == APIC_DFR_FLAT;\n\tentry = avic_get_logical_id_entry(vcpu, ldr, flat);\n\tif (!entry)\n\t\treturn;\n\n\tnew_entry = READ_ONCE(*entry);\n\tnew_entry &= ~AVIC_LOGICAL_ID_ENTRY_GUEST_PHYSICAL_ID_MASK;\n\tnew_entry |= (g_physical_id & AVIC_LOGICAL_ID_ENTRY_GUEST_PHYSICAL_ID_MASK);\n\tnew_entry |= AVIC_LOGICAL_ID_ENTRY_VALID_MASK;\n\tWRITE_ONCE(*entry, new_entry);\n}\n\nstatic void avic_invalidate_logical_id_entry(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tbool flat = svm->dfr_reg == APIC_DFR_FLAT;\n\tu32 *entry;\n\n\t \n\tif (apic_x2apic_mode(vcpu->arch.apic))\n\t\treturn;\n\n\tentry = avic_get_logical_id_entry(vcpu, svm->ldr_reg, flat);\n\tif (entry)\n\t\tclear_bit(AVIC_LOGICAL_ID_ENTRY_VALID_BIT, (unsigned long *)entry);\n}\n\nstatic void avic_handle_ldr_update(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu32 ldr = kvm_lapic_get_reg(vcpu->arch.apic, APIC_LDR);\n\tu32 id = kvm_xapic_id(vcpu->arch.apic);\n\n\t \n\tif (apic_x2apic_mode(vcpu->arch.apic))\n\t\treturn;\n\n\tif (ldr == svm->ldr_reg)\n\t\treturn;\n\n\tavic_invalidate_logical_id_entry(vcpu);\n\n\tsvm->ldr_reg = ldr;\n\tavic_ldr_write(vcpu, id, ldr);\n}\n\nstatic void avic_handle_dfr_update(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tu32 dfr = kvm_lapic_get_reg(vcpu->arch.apic, APIC_DFR);\n\n\tif (svm->dfr_reg == dfr)\n\t\treturn;\n\n\tavic_invalidate_logical_id_entry(vcpu);\n\tsvm->dfr_reg = dfr;\n}\n\nstatic int avic_unaccel_trap_write(struct kvm_vcpu *vcpu)\n{\n\tu32 offset = to_svm(vcpu)->vmcb->control.exit_info_1 &\n\t\t\t\tAVIC_UNACCEL_ACCESS_OFFSET_MASK;\n\n\tswitch (offset) {\n\tcase APIC_LDR:\n\t\tavic_handle_ldr_update(vcpu);\n\t\tbreak;\n\tcase APIC_DFR:\n\t\tavic_handle_dfr_update(vcpu);\n\t\tbreak;\n\tcase APIC_RRR:\n\t\t \n\t\treturn 1;\n\tdefault:\n\t\tbreak;\n\t}\n\n\tkvm_apic_write_nodecode(vcpu, offset);\n\treturn 1;\n}\n\nstatic bool is_avic_unaccelerated_access_trap(u32 offset)\n{\n\tbool ret = false;\n\n\tswitch (offset) {\n\tcase APIC_ID:\n\tcase APIC_EOI:\n\tcase APIC_RRR:\n\tcase APIC_LDR:\n\tcase APIC_DFR:\n\tcase APIC_SPIV:\n\tcase APIC_ESR:\n\tcase APIC_ICR:\n\tcase APIC_LVTT:\n\tcase APIC_LVTTHMR:\n\tcase APIC_LVTPC:\n\tcase APIC_LVT0:\n\tcase APIC_LVT1:\n\tcase APIC_LVTERR:\n\tcase APIC_TMICT:\n\tcase APIC_TDCR:\n\t\tret = true;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\nint avic_unaccelerated_access_interception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tint ret = 0;\n\tu32 offset = svm->vmcb->control.exit_info_1 &\n\t\t     AVIC_UNACCEL_ACCESS_OFFSET_MASK;\n\tu32 vector = svm->vmcb->control.exit_info_2 &\n\t\t     AVIC_UNACCEL_ACCESS_VECTOR_MASK;\n\tbool write = (svm->vmcb->control.exit_info_1 >> 32) &\n\t\t     AVIC_UNACCEL_ACCESS_WRITE_MASK;\n\tbool trap = is_avic_unaccelerated_access_trap(offset);\n\n\ttrace_kvm_avic_unaccelerated_access(vcpu->vcpu_id, offset,\n\t\t\t\t\t    trap, write, vector);\n\tif (trap) {\n\t\t \n\t\tWARN_ONCE(!write, \"svm: Handling trap read.\\n\");\n\t\tret = avic_unaccel_trap_write(vcpu);\n\t} else {\n\t\t \n\t\tret = kvm_emulate_instruction(vcpu, 0);\n\t}\n\n\treturn ret;\n}\n\nint avic_init_vcpu(struct vcpu_svm *svm)\n{\n\tint ret;\n\tstruct kvm_vcpu *vcpu = &svm->vcpu;\n\n\tif (!enable_apicv || !irqchip_in_kernel(vcpu->kvm))\n\t\treturn 0;\n\n\tret = avic_init_backing_page(vcpu);\n\tif (ret)\n\t\treturn ret;\n\n\tINIT_LIST_HEAD(&svm->ir_list);\n\tspin_lock_init(&svm->ir_list_lock);\n\tsvm->dfr_reg = APIC_DFR_FLAT;\n\n\treturn ret;\n}\n\nvoid avic_apicv_post_state_restore(struct kvm_vcpu *vcpu)\n{\n\tavic_handle_dfr_update(vcpu);\n\tavic_handle_ldr_update(vcpu);\n}\n\nstatic int avic_set_pi_irte_mode(struct kvm_vcpu *vcpu, bool activate)\n{\n\tint ret = 0;\n\tunsigned long flags;\n\tstruct amd_svm_iommu_ir *ir;\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tif (!kvm_arch_has_assigned_device(vcpu->kvm))\n\t\treturn 0;\n\n\t \n\tspin_lock_irqsave(&svm->ir_list_lock, flags);\n\n\tif (list_empty(&svm->ir_list))\n\t\tgoto out;\n\n\tlist_for_each_entry(ir, &svm->ir_list, node) {\n\t\tif (activate)\n\t\t\tret = amd_iommu_activate_guest_mode(ir->data);\n\t\telse\n\t\t\tret = amd_iommu_deactivate_guest_mode(ir->data);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\nout:\n\tspin_unlock_irqrestore(&svm->ir_list_lock, flags);\n\treturn ret;\n}\n\nstatic void svm_ir_list_del(struct vcpu_svm *svm, struct amd_iommu_pi_data *pi)\n{\n\tunsigned long flags;\n\tstruct amd_svm_iommu_ir *cur;\n\n\tspin_lock_irqsave(&svm->ir_list_lock, flags);\n\tlist_for_each_entry(cur, &svm->ir_list, node) {\n\t\tif (cur->data != pi->ir_data)\n\t\t\tcontinue;\n\t\tlist_del(&cur->node);\n\t\tkfree(cur);\n\t\tbreak;\n\t}\n\tspin_unlock_irqrestore(&svm->ir_list_lock, flags);\n}\n\nstatic int svm_ir_list_add(struct vcpu_svm *svm, struct amd_iommu_pi_data *pi)\n{\n\tint ret = 0;\n\tunsigned long flags;\n\tstruct amd_svm_iommu_ir *ir;\n\tu64 entry;\n\n\t \n\tif (pi->ir_data && (pi->prev_ga_tag != 0)) {\n\t\tstruct kvm *kvm = svm->vcpu.kvm;\n\t\tu32 vcpu_id = AVIC_GATAG_TO_VCPUID(pi->prev_ga_tag);\n\t\tstruct kvm_vcpu *prev_vcpu = kvm_get_vcpu_by_id(kvm, vcpu_id);\n\t\tstruct vcpu_svm *prev_svm;\n\n\t\tif (!prev_vcpu) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tprev_svm = to_svm(prev_vcpu);\n\t\tsvm_ir_list_del(prev_svm, pi);\n\t}\n\n\t \n\tir = kzalloc(sizeof(struct amd_svm_iommu_ir), GFP_KERNEL_ACCOUNT);\n\tif (!ir) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\tir->data = pi->ir_data;\n\n\tspin_lock_irqsave(&svm->ir_list_lock, flags);\n\n\t \n\tentry = READ_ONCE(*(svm->avic_physical_id_cache));\n\tif (entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK)\n\t\tamd_iommu_update_ga(entry & AVIC_PHYSICAL_ID_ENTRY_HOST_PHYSICAL_ID_MASK,\n\t\t\t\t    true, pi->ir_data);\n\n\tlist_add(&ir->node, &svm->ir_list);\n\tspin_unlock_irqrestore(&svm->ir_list_lock, flags);\nout:\n\treturn ret;\n}\n\n \nstatic int\nget_pi_vcpu_info(struct kvm *kvm, struct kvm_kernel_irq_routing_entry *e,\n\t\t struct vcpu_data *vcpu_info, struct vcpu_svm **svm)\n{\n\tstruct kvm_lapic_irq irq;\n\tstruct kvm_vcpu *vcpu = NULL;\n\n\tkvm_set_msi_irq(kvm, e, &irq);\n\n\tif (!kvm_intr_is_single_vcpu(kvm, &irq, &vcpu) ||\n\t    !kvm_irq_is_postable(&irq)) {\n\t\tpr_debug(\"SVM: %s: use legacy intr remap mode for irq %u\\n\",\n\t\t\t __func__, irq.vector);\n\t\treturn -1;\n\t}\n\n\tpr_debug(\"SVM: %s: use GA mode for irq %u\\n\", __func__,\n\t\t irq.vector);\n\t*svm = to_svm(vcpu);\n\tvcpu_info->pi_desc_addr = __sme_set(page_to_phys((*svm)->avic_backing_page));\n\tvcpu_info->vector = irq.vector;\n\n\treturn 0;\n}\n\n \nint avic_pi_update_irte(struct kvm *kvm, unsigned int host_irq,\n\t\t\tuint32_t guest_irq, bool set)\n{\n\tstruct kvm_kernel_irq_routing_entry *e;\n\tstruct kvm_irq_routing_table *irq_rt;\n\tint idx, ret = 0;\n\n\tif (!kvm_arch_has_assigned_device(kvm) ||\n\t    !irq_remapping_cap(IRQ_POSTING_CAP))\n\t\treturn 0;\n\n\tpr_debug(\"SVM: %s: host_irq=%#x, guest_irq=%#x, set=%#x\\n\",\n\t\t __func__, host_irq, guest_irq, set);\n\n\tidx = srcu_read_lock(&kvm->irq_srcu);\n\tirq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);\n\n\tif (guest_irq >= irq_rt->nr_rt_entries ||\n\t\thlist_empty(&irq_rt->map[guest_irq])) {\n\t\tpr_warn_once(\"no route for guest_irq %u/%u (broken user space?)\\n\",\n\t\t\t     guest_irq, irq_rt->nr_rt_entries);\n\t\tgoto out;\n\t}\n\n\thlist_for_each_entry(e, &irq_rt->map[guest_irq], link) {\n\t\tstruct vcpu_data vcpu_info;\n\t\tstruct vcpu_svm *svm = NULL;\n\n\t\tif (e->type != KVM_IRQ_ROUTING_MSI)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!get_pi_vcpu_info(kvm, e, &vcpu_info, &svm) && set &&\n\t\t    kvm_vcpu_apicv_active(&svm->vcpu)) {\n\t\t\tstruct amd_iommu_pi_data pi;\n\n\t\t\t \n\t\t\tpi.base = __sme_set(page_to_phys(svm->avic_backing_page) &\n\t\t\t\t\t    AVIC_HPA_MASK);\n\t\t\tpi.ga_tag = AVIC_GATAG(to_kvm_svm(kvm)->avic_vm_id,\n\t\t\t\t\t\t     svm->vcpu.vcpu_id);\n\t\t\tpi.is_guest_mode = true;\n\t\t\tpi.vcpu_data = &vcpu_info;\n\t\t\tret = irq_set_vcpu_affinity(host_irq, &pi);\n\n\t\t\t \n\t\t\tif (!ret && pi.is_guest_mode)\n\t\t\t\tsvm_ir_list_add(svm, &pi);\n\t\t} else {\n\t\t\t \n\t\t\tstruct amd_iommu_pi_data pi;\n\n\t\t\t \n\t\t\tpi.prev_ga_tag = 0;\n\t\t\tpi.is_guest_mode = false;\n\t\t\tret = irq_set_vcpu_affinity(host_irq, &pi);\n\n\t\t\t \n\t\t\tif (!ret && pi.prev_ga_tag) {\n\t\t\t\tint id = AVIC_GATAG_TO_VCPUID(pi.prev_ga_tag);\n\t\t\t\tstruct kvm_vcpu *vcpu;\n\n\t\t\t\tvcpu = kvm_get_vcpu_by_id(kvm, id);\n\t\t\t\tif (vcpu)\n\t\t\t\t\tsvm_ir_list_del(to_svm(vcpu), &pi);\n\t\t\t}\n\t\t}\n\n\t\tif (!ret && svm) {\n\t\t\ttrace_kvm_pi_irte_update(host_irq, svm->vcpu.vcpu_id,\n\t\t\t\t\t\t e->gsi, vcpu_info.vector,\n\t\t\t\t\t\t vcpu_info.pi_desc_addr, set);\n\t\t}\n\n\t\tif (ret < 0) {\n\t\t\tpr_err(\"%s: failed to update PI IRTE\\n\", __func__);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = 0;\nout:\n\tsrcu_read_unlock(&kvm->irq_srcu, idx);\n\treturn ret;\n}\n\nstatic inline int\navic_update_iommu_vcpu_affinity(struct kvm_vcpu *vcpu, int cpu, bool r)\n{\n\tint ret = 0;\n\tstruct amd_svm_iommu_ir *ir;\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\tlockdep_assert_held(&svm->ir_list_lock);\n\n\tif (!kvm_arch_has_assigned_device(vcpu->kvm))\n\t\treturn 0;\n\n\t \n\tif (list_empty(&svm->ir_list))\n\t\treturn 0;\n\n\tlist_for_each_entry(ir, &svm->ir_list, node) {\n\t\tret = amd_iommu_update_ga(cpu, r, ir->data);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nvoid avic_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\n{\n\tu64 entry;\n\tint h_physical_id = kvm_cpu_get_apicid(cpu);\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tunsigned long flags;\n\n\tlockdep_assert_preemption_disabled();\n\n\tif (WARN_ON(h_physical_id & ~AVIC_PHYSICAL_ID_ENTRY_HOST_PHYSICAL_ID_MASK))\n\t\treturn;\n\n\t \n\tif (kvm_vcpu_is_blocking(vcpu))\n\t\treturn;\n\n\t \n\tspin_lock_irqsave(&svm->ir_list_lock, flags);\n\n\tentry = READ_ONCE(*(svm->avic_physical_id_cache));\n\tWARN_ON_ONCE(entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK);\n\n\tentry &= ~AVIC_PHYSICAL_ID_ENTRY_HOST_PHYSICAL_ID_MASK;\n\tentry |= (h_physical_id & AVIC_PHYSICAL_ID_ENTRY_HOST_PHYSICAL_ID_MASK);\n\tentry |= AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;\n\n\tWRITE_ONCE(*(svm->avic_physical_id_cache), entry);\n\tavic_update_iommu_vcpu_affinity(vcpu, h_physical_id, true);\n\n\tspin_unlock_irqrestore(&svm->ir_list_lock, flags);\n}\n\nvoid avic_vcpu_put(struct kvm_vcpu *vcpu)\n{\n\tu64 entry;\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tunsigned long flags;\n\n\tlockdep_assert_preemption_disabled();\n\n\t \n\tentry = READ_ONCE(*(svm->avic_physical_id_cache));\n\n\t \n\tif (!(entry & AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK))\n\t\treturn;\n\n\t \n\tspin_lock_irqsave(&svm->ir_list_lock, flags);\n\n\tavic_update_iommu_vcpu_affinity(vcpu, -1, 0);\n\n\tentry &= ~AVIC_PHYSICAL_ID_ENTRY_IS_RUNNING_MASK;\n\tWRITE_ONCE(*(svm->avic_physical_id_cache), entry);\n\n\tspin_unlock_irqrestore(&svm->ir_list_lock, flags);\n\n}\n\nvoid avic_refresh_virtual_apic_mode(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct vmcb *vmcb = svm->vmcb01.ptr;\n\n\tif (!lapic_in_kernel(vcpu) || !enable_apicv)\n\t\treturn;\n\n\tif (kvm_vcpu_apicv_active(vcpu)) {\n\t\t \n\t\tavic_apicv_post_state_restore(vcpu);\n\t\tavic_activate_vmcb(svm);\n\t} else {\n\t\tavic_deactivate_vmcb(svm);\n\t}\n\tvmcb_mark_dirty(vmcb, VMCB_AVIC);\n}\n\nvoid avic_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)\n{\n\tbool activated = kvm_vcpu_apicv_active(vcpu);\n\n\tif (!enable_apicv)\n\t\treturn;\n\n\tavic_refresh_virtual_apic_mode(vcpu);\n\n\tif (activated)\n\t\tavic_vcpu_load(vcpu, vcpu->cpu);\n\telse\n\t\tavic_vcpu_put(vcpu);\n\n\tavic_set_pi_irte_mode(vcpu, activated);\n}\n\nvoid avic_vcpu_blocking(struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_vcpu_apicv_active(vcpu))\n\t\treturn;\n\n        \n\tavic_vcpu_put(vcpu);\n}\n\nvoid avic_vcpu_unblocking(struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_vcpu_apicv_active(vcpu))\n\t\treturn;\n\n\tavic_vcpu_load(vcpu, vcpu->cpu);\n}\n\n \nbool avic_hardware_setup(void)\n{\n\tif (!npt_enabled)\n\t\treturn false;\n\n\t \n\tif (!boot_cpu_has(X86_FEATURE_AVIC) && !force_avic) {\n\t\tif (boot_cpu_has(X86_FEATURE_X2AVIC)) {\n\t\t\tpr_warn(FW_BUG \"Cannot support x2AVIC due to AVIC is disabled\");\n\t\t\tpr_warn(FW_BUG \"Try enable AVIC using force_avic option\");\n\t\t}\n\t\treturn false;\n\t}\n\n\tif (boot_cpu_has(X86_FEATURE_AVIC)) {\n\t\tpr_info(\"AVIC enabled\\n\");\n\t} else if (force_avic) {\n\t\t \n\t\tpr_warn(\"AVIC is not supported in CPUID but force enabled\");\n\t\tpr_warn(\"Your system might crash and burn\");\n\t}\n\n\t \n\tx2avic_enabled = boot_cpu_has(X86_FEATURE_X2AVIC);\n\tif (x2avic_enabled)\n\t\tpr_info(\"x2AVIC enabled\\n\");\n\n\tamd_iommu_register_ga_log_notifier(&avic_ga_log_notifier);\n\n\treturn true;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}