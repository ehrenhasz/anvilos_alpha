{
  "module_name": "svm.h",
  "hash_id": "6803c36a4390afcacf48a1ecd946dc5503117bae77f1bb892bb55d629736c2f6",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/svm/svm.h",
  "human_readable_source": "\n \n\n#ifndef __SVM_SVM_H\n#define __SVM_SVM_H\n\n#include <linux/kvm_types.h>\n#include <linux/kvm_host.h>\n#include <linux/bits.h>\n\n#include <asm/svm.h>\n#include <asm/sev-common.h>\n\n#include \"cpuid.h\"\n#include \"kvm_cache_regs.h\"\n\n#define __sme_page_pa(x) __sme_set(page_to_pfn(x) << PAGE_SHIFT)\n\n#define\tIOPM_SIZE PAGE_SIZE * 3\n#define\tMSRPM_SIZE PAGE_SIZE * 2\n\n#define MAX_DIRECT_ACCESS_MSRS\t46\n#define MSRPM_OFFSETS\t32\nextern u32 msrpm_offsets[MSRPM_OFFSETS] __read_mostly;\nextern bool npt_enabled;\nextern int nrips;\nextern int vgif;\nextern bool intercept_smi;\nextern bool x2avic_enabled;\nextern bool vnmi;\n\n \nenum {\n\tVMCB_INTERCEPTS,  \n\tVMCB_PERM_MAP,    \n\tVMCB_ASID,\t  \n\tVMCB_INTR,\t  \n\tVMCB_NPT,         \n\tVMCB_CR,\t  \n\tVMCB_DR,          \n\tVMCB_DT,          \n\tVMCB_SEG,         \n\tVMCB_CR2,         \n\tVMCB_LBR,         \n\tVMCB_AVIC,        \n\tVMCB_SW = 31,     \n};\n\n#define VMCB_ALL_CLEAN_MASK (\t\t\t\t\t\\\n\t(1U << VMCB_INTERCEPTS) | (1U << VMCB_PERM_MAP) |\t\\\n\t(1U << VMCB_ASID) | (1U << VMCB_INTR) |\t\t\t\\\n\t(1U << VMCB_NPT) | (1U << VMCB_CR) | (1U << VMCB_DR) |\t\\\n\t(1U << VMCB_DT) | (1U << VMCB_SEG) | (1U << VMCB_CR2) |\t\\\n\t(1U << VMCB_LBR) | (1U << VMCB_AVIC) |\t\t\t\\\n\t(1U << VMCB_SW))\n\n \n#define VMCB_ALWAYS_DIRTY_MASK\t((1U << VMCB_INTR) | (1U << VMCB_CR2))\n\nstruct kvm_sev_info {\n\tbool active;\t\t \n\tbool es_active;\t\t \n\tunsigned int asid;\t \n\tunsigned int handle;\t \n\tint fd;\t\t\t \n\tunsigned long pages_locked;  \n\tstruct list_head regions_list;   \n\tu64 ap_jump_table;\t \n\tstruct kvm *enc_context_owner;  \n\tstruct list_head mirror_vms;  \n\tstruct list_head mirror_entry;  \n\tstruct misc_cg *misc_cg;  \n\tatomic_t migration_in_progress;\n};\n\nstruct kvm_svm {\n\tstruct kvm kvm;\n\n\t \n\tu32 avic_vm_id;\n\tstruct page *avic_logical_id_table_page;\n\tstruct page *avic_physical_id_table_page;\n\tstruct hlist_node hnode;\n\n\tstruct kvm_sev_info sev_info;\n};\n\nstruct kvm_vcpu;\n\nstruct kvm_vmcb_info {\n\tstruct vmcb *ptr;\n\tunsigned long pa;\n\tint cpu;\n\tuint64_t asid_generation;\n};\n\nstruct vmcb_save_area_cached {\n\tu64 efer;\n\tu64 cr4;\n\tu64 cr3;\n\tu64 cr0;\n\tu64 dr7;\n\tu64 dr6;\n};\n\nstruct vmcb_ctrl_area_cached {\n\tu32 intercepts[MAX_INTERCEPT];\n\tu16 pause_filter_thresh;\n\tu16 pause_filter_count;\n\tu64 iopm_base_pa;\n\tu64 msrpm_base_pa;\n\tu64 tsc_offset;\n\tu32 asid;\n\tu8 tlb_ctl;\n\tu32 int_ctl;\n\tu32 int_vector;\n\tu32 int_state;\n\tu32 exit_code;\n\tu32 exit_code_hi;\n\tu64 exit_info_1;\n\tu64 exit_info_2;\n\tu32 exit_int_info;\n\tu32 exit_int_info_err;\n\tu64 nested_ctl;\n\tu32 event_inj;\n\tu32 event_inj_err;\n\tu64 next_rip;\n\tu64 nested_cr3;\n\tu64 virt_ext;\n\tu32 clean;\n\tunion {\n\t\tstruct hv_vmcb_enlightenments hv_enlightenments;\n\t\tu8 reserved_sw[32];\n\t};\n};\n\nstruct svm_nested_state {\n\tstruct kvm_vmcb_info vmcb02;\n\tu64 hsave_msr;\n\tu64 vm_cr_msr;\n\tu64 vmcb12_gpa;\n\tu64 last_vmcb12_gpa;\n\n\t \n\tu32 *msrpm;\n\n\t \n\tbool nested_run_pending;\n\n\t \n\tstruct vmcb_ctrl_area_cached ctl;\n\n\t \n\tstruct vmcb_save_area_cached save;\n\n\tbool initialized;\n\n\t \n\tbool force_msr_bitmap_recalc;\n};\n\nstruct vcpu_sev_es_state {\n\t \n\tstruct sev_es_save_area *vmsa;\n\tstruct ghcb *ghcb;\n\tu8 valid_bitmap[16];\n\tstruct kvm_host_map ghcb_map;\n\tbool received_first_sipi;\n\n\t \n\tu64 sw_scratch;\n\tvoid *ghcb_sa;\n\tu32 ghcb_sa_len;\n\tbool ghcb_sa_sync;\n\tbool ghcb_sa_free;\n};\n\nstruct vcpu_svm {\n\tstruct kvm_vcpu vcpu;\n\t \n\tstruct vmcb *vmcb;\n\tstruct kvm_vmcb_info vmcb01;\n\tstruct kvm_vmcb_info *current_vmcb;\n\tu32 asid;\n\tu32 sysenter_esp_hi;\n\tu32 sysenter_eip_hi;\n\tuint64_t tsc_aux;\n\n\tu64 msr_decfg;\n\n\tu64 next_rip;\n\n\tu64 spec_ctrl;\n\n\tu64 tsc_ratio_msr;\n\t \n\tu64 virt_spec_ctrl;\n\n\tu32 *msrpm;\n\n\tulong nmi_iret_rip;\n\n\tstruct svm_nested_state nested;\n\n\t \n\tbool nmi_masked;\n\n\t \n\tbool awaiting_iret_completion;\n\n\t \n\tbool nmi_singlestep;\n\tu64 nmi_singlestep_guest_rflags;\n\n\tbool nmi_l1_to_l2;\n\n\tunsigned long soft_int_csbase;\n\tunsigned long soft_int_old_rip;\n\tunsigned long soft_int_next_rip;\n\tbool soft_int_injected;\n\n\tu32 ldr_reg;\n\tu32 dfr_reg;\n\tstruct page *avic_backing_page;\n\tu64 *avic_physical_id_cache;\n\n\t \n\tstruct list_head ir_list;\n\tspinlock_t ir_list_lock;\n\n\t \n\tstruct {\n\t\tDECLARE_BITMAP(read, MAX_DIRECT_ACCESS_MSRS);\n\t\tDECLARE_BITMAP(write, MAX_DIRECT_ACCESS_MSRS);\n\t} shadow_msr_intercept;\n\n\tstruct vcpu_sev_es_state sev_es;\n\n\tbool guest_state_loaded;\n\n\tbool x2avic_msrs_intercepted;\n\n\t \n\tbool guest_gif;\n};\n\nstruct svm_cpu_data {\n\tu64 asid_generation;\n\tu32 max_asid;\n\tu32 next_asid;\n\tu32 min_asid;\n\n\tstruct page *save_area;\n\tunsigned long save_area_pa;\n\n\tstruct vmcb *current_vmcb;\n\n\t \n\tstruct vmcb **sev_vmcbs;\n};\n\nDECLARE_PER_CPU(struct svm_cpu_data, svm_data);\n\nvoid recalc_intercepts(struct vcpu_svm *svm);\n\nstatic __always_inline struct kvm_svm *to_kvm_svm(struct kvm *kvm)\n{\n\treturn container_of(kvm, struct kvm_svm, kvm);\n}\n\nstatic __always_inline bool sev_guest(struct kvm *kvm)\n{\n#ifdef CONFIG_KVM_AMD_SEV\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\n\treturn sev->active;\n#else\n\treturn false;\n#endif\n}\n\nstatic __always_inline bool sev_es_guest(struct kvm *kvm)\n{\n#ifdef CONFIG_KVM_AMD_SEV\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\n\treturn sev->es_active && !WARN_ON_ONCE(!sev->active);\n#else\n\treturn false;\n#endif\n}\n\nstatic inline void vmcb_mark_all_dirty(struct vmcb *vmcb)\n{\n\tvmcb->control.clean = 0;\n}\n\nstatic inline void vmcb_mark_all_clean(struct vmcb *vmcb)\n{\n\tvmcb->control.clean = VMCB_ALL_CLEAN_MASK\n\t\t\t       & ~VMCB_ALWAYS_DIRTY_MASK;\n}\n\nstatic inline void vmcb_mark_dirty(struct vmcb *vmcb, int bit)\n{\n\tvmcb->control.clean &= ~(1 << bit);\n}\n\nstatic inline bool vmcb_is_dirty(struct vmcb *vmcb, int bit)\n{\n        return !test_bit(bit, (unsigned long *)&vmcb->control.clean);\n}\n\nstatic __always_inline struct vcpu_svm *to_svm(struct kvm_vcpu *vcpu)\n{\n\treturn container_of(vcpu, struct vcpu_svm, vcpu);\n}\n\n \n#define SVM_REGS_LAZY_LOAD_SET\t(1 << VCPU_EXREG_PDPTR)\n\nstatic inline void vmcb_set_intercept(struct vmcb_control_area *control, u32 bit)\n{\n\tWARN_ON_ONCE(bit >= 32 * MAX_INTERCEPT);\n\t__set_bit(bit, (unsigned long *)&control->intercepts);\n}\n\nstatic inline void vmcb_clr_intercept(struct vmcb_control_area *control, u32 bit)\n{\n\tWARN_ON_ONCE(bit >= 32 * MAX_INTERCEPT);\n\t__clear_bit(bit, (unsigned long *)&control->intercepts);\n}\n\nstatic inline bool vmcb_is_intercept(struct vmcb_control_area *control, u32 bit)\n{\n\tWARN_ON_ONCE(bit >= 32 * MAX_INTERCEPT);\n\treturn test_bit(bit, (unsigned long *)&control->intercepts);\n}\n\nstatic inline bool vmcb12_is_intercept(struct vmcb_ctrl_area_cached *control, u32 bit)\n{\n\tWARN_ON_ONCE(bit >= 32 * MAX_INTERCEPT);\n\treturn test_bit(bit, (unsigned long *)&control->intercepts);\n}\n\nstatic inline void set_exception_intercept(struct vcpu_svm *svm, u32 bit)\n{\n\tstruct vmcb *vmcb = svm->vmcb01.ptr;\n\n\tWARN_ON_ONCE(bit >= 32);\n\tvmcb_set_intercept(&vmcb->control, INTERCEPT_EXCEPTION_OFFSET + bit);\n\n\trecalc_intercepts(svm);\n}\n\nstatic inline void clr_exception_intercept(struct vcpu_svm *svm, u32 bit)\n{\n\tstruct vmcb *vmcb = svm->vmcb01.ptr;\n\n\tWARN_ON_ONCE(bit >= 32);\n\tvmcb_clr_intercept(&vmcb->control, INTERCEPT_EXCEPTION_OFFSET + bit);\n\n\trecalc_intercepts(svm);\n}\n\nstatic inline void svm_set_intercept(struct vcpu_svm *svm, int bit)\n{\n\tstruct vmcb *vmcb = svm->vmcb01.ptr;\n\n\tvmcb_set_intercept(&vmcb->control, bit);\n\n\trecalc_intercepts(svm);\n}\n\nstatic inline void svm_clr_intercept(struct vcpu_svm *svm, int bit)\n{\n\tstruct vmcb *vmcb = svm->vmcb01.ptr;\n\n\tvmcb_clr_intercept(&vmcb->control, bit);\n\n\trecalc_intercepts(svm);\n}\n\nstatic inline bool svm_is_intercept(struct vcpu_svm *svm, int bit)\n{\n\treturn vmcb_is_intercept(&svm->vmcb->control, bit);\n}\n\nstatic inline bool nested_vgif_enabled(struct vcpu_svm *svm)\n{\n\treturn guest_can_use(&svm->vcpu, X86_FEATURE_VGIF) &&\n\t       (svm->nested.ctl.int_ctl & V_GIF_ENABLE_MASK);\n}\n\nstatic inline struct vmcb *get_vgif_vmcb(struct vcpu_svm *svm)\n{\n\tif (!vgif)\n\t\treturn NULL;\n\n\tif (is_guest_mode(&svm->vcpu) && !nested_vgif_enabled(svm))\n\t\treturn svm->nested.vmcb02.ptr;\n\telse\n\t\treturn svm->vmcb01.ptr;\n}\n\nstatic inline void enable_gif(struct vcpu_svm *svm)\n{\n\tstruct vmcb *vmcb = get_vgif_vmcb(svm);\n\n\tif (vmcb)\n\t\tvmcb->control.int_ctl |= V_GIF_MASK;\n\telse\n\t\tsvm->guest_gif = true;\n}\n\nstatic inline void disable_gif(struct vcpu_svm *svm)\n{\n\tstruct vmcb *vmcb = get_vgif_vmcb(svm);\n\n\tif (vmcb)\n\t\tvmcb->control.int_ctl &= ~V_GIF_MASK;\n\telse\n\t\tsvm->guest_gif = false;\n}\n\nstatic inline bool gif_set(struct vcpu_svm *svm)\n{\n\tstruct vmcb *vmcb = get_vgif_vmcb(svm);\n\n\tif (vmcb)\n\t\treturn !!(vmcb->control.int_ctl & V_GIF_MASK);\n\telse\n\t\treturn svm->guest_gif;\n}\n\nstatic inline bool nested_npt_enabled(struct vcpu_svm *svm)\n{\n\treturn svm->nested.ctl.nested_ctl & SVM_NESTED_CTL_NP_ENABLE;\n}\n\nstatic inline bool nested_vnmi_enabled(struct vcpu_svm *svm)\n{\n\treturn guest_can_use(&svm->vcpu, X86_FEATURE_VNMI) &&\n\t       (svm->nested.ctl.int_ctl & V_NMI_ENABLE_MASK);\n}\n\nstatic inline bool is_x2apic_msrpm_offset(u32 offset)\n{\n\t \n\tu32 msr = offset * 16;\n\n\treturn (msr >= APIC_BASE_MSR) &&\n\t       (msr < (APIC_BASE_MSR + 0x100));\n}\n\nstatic inline struct vmcb *get_vnmi_vmcb_l1(struct vcpu_svm *svm)\n{\n\tif (!vnmi)\n\t\treturn NULL;\n\n\tif (is_guest_mode(&svm->vcpu))\n\t\treturn NULL;\n\telse\n\t\treturn svm->vmcb01.ptr;\n}\n\nstatic inline bool is_vnmi_enabled(struct vcpu_svm *svm)\n{\n\tstruct vmcb *vmcb = get_vnmi_vmcb_l1(svm);\n\n\tif (vmcb)\n\t\treturn !!(vmcb->control.int_ctl & V_NMI_ENABLE_MASK);\n\telse\n\t\treturn false;\n}\n\n \n#define MSR_INVALID\t\t\t\t0xffffffffU\n\n#define DEBUGCTL_RESERVED_BITS (~(0x3fULL))\n\nextern bool dump_invalid_vmcb;\n\nu32 svm_msrpm_offset(u32 msr);\nu32 *svm_vcpu_alloc_msrpm(void);\nvoid svm_vcpu_init_msrpm(struct kvm_vcpu *vcpu, u32 *msrpm);\nvoid svm_vcpu_free_msrpm(u32 *msrpm);\nvoid svm_copy_lbrs(struct vmcb *to_vmcb, struct vmcb *from_vmcb);\nvoid svm_update_lbrv(struct kvm_vcpu *vcpu);\n\nint svm_set_efer(struct kvm_vcpu *vcpu, u64 efer);\nvoid svm_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0);\nvoid svm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);\nvoid disable_nmi_singlestep(struct vcpu_svm *svm);\nbool svm_smi_blocked(struct kvm_vcpu *vcpu);\nbool svm_nmi_blocked(struct kvm_vcpu *vcpu);\nbool svm_interrupt_blocked(struct kvm_vcpu *vcpu);\nvoid svm_set_gif(struct vcpu_svm *svm, bool value);\nint svm_invoke_exit_handler(struct kvm_vcpu *vcpu, u64 exit_code);\nvoid set_msr_interception(struct kvm_vcpu *vcpu, u32 *msrpm, u32 msr,\n\t\t\t  int read, int write);\nvoid svm_set_x2apic_msr_interception(struct vcpu_svm *svm, bool disable);\nvoid svm_complete_interrupt_delivery(struct kvm_vcpu *vcpu, int delivery_mode,\n\t\t\t\t     int trig_mode, int vec);\n\n \n\n#define NESTED_EXIT_HOST\t0\t \n#define NESTED_EXIT_DONE\t1\t \n#define NESTED_EXIT_CONTINUE\t2\t \n\nstatic inline bool nested_svm_virtualize_tpr(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\treturn is_guest_mode(vcpu) && (svm->nested.ctl.int_ctl & V_INTR_MASKING_MASK);\n}\n\nstatic inline bool nested_exit_on_smi(struct vcpu_svm *svm)\n{\n\treturn vmcb12_is_intercept(&svm->nested.ctl, INTERCEPT_SMI);\n}\n\nstatic inline bool nested_exit_on_intr(struct vcpu_svm *svm)\n{\n\treturn vmcb12_is_intercept(&svm->nested.ctl, INTERCEPT_INTR);\n}\n\nstatic inline bool nested_exit_on_nmi(struct vcpu_svm *svm)\n{\n\treturn vmcb12_is_intercept(&svm->nested.ctl, INTERCEPT_NMI);\n}\n\nint enter_svm_guest_mode(struct kvm_vcpu *vcpu,\n\t\t\t u64 vmcb_gpa, struct vmcb *vmcb12, bool from_vmrun);\nvoid svm_leave_nested(struct kvm_vcpu *vcpu);\nvoid svm_free_nested(struct vcpu_svm *svm);\nint svm_allocate_nested(struct vcpu_svm *svm);\nint nested_svm_vmrun(struct kvm_vcpu *vcpu);\nvoid svm_copy_vmrun_state(struct vmcb_save_area *to_save,\n\t\t\t  struct vmcb_save_area *from_save);\nvoid svm_copy_vmloadsave_state(struct vmcb *to_vmcb, struct vmcb *from_vmcb);\nint nested_svm_vmexit(struct vcpu_svm *svm);\n\nstatic inline int nested_svm_simple_vmexit(struct vcpu_svm *svm, u32 exit_code)\n{\n\tsvm->vmcb->control.exit_code   = exit_code;\n\tsvm->vmcb->control.exit_info_1 = 0;\n\tsvm->vmcb->control.exit_info_2 = 0;\n\treturn nested_svm_vmexit(svm);\n}\n\nint nested_svm_exit_handled(struct vcpu_svm *svm);\nint nested_svm_check_permissions(struct kvm_vcpu *vcpu);\nint nested_svm_check_exception(struct vcpu_svm *svm, unsigned nr,\n\t\t\t       bool has_error_code, u32 error_code);\nint nested_svm_exit_special(struct vcpu_svm *svm);\nvoid nested_svm_update_tsc_ratio_msr(struct kvm_vcpu *vcpu);\nvoid svm_write_tsc_multiplier(struct kvm_vcpu *vcpu);\nvoid nested_copy_vmcb_control_to_cache(struct vcpu_svm *svm,\n\t\t\t\t       struct vmcb_control_area *control);\nvoid nested_copy_vmcb_save_to_cache(struct vcpu_svm *svm,\n\t\t\t\t    struct vmcb_save_area *save);\nvoid nested_sync_control_from_vmcb02(struct vcpu_svm *svm);\nvoid nested_vmcb02_compute_g_pat(struct vcpu_svm *svm);\nvoid svm_switch_vmcb(struct vcpu_svm *svm, struct kvm_vmcb_info *target_vmcb);\n\nextern struct kvm_x86_nested_ops svm_nested_ops;\n\n \n#define AVIC_REQUIRED_APICV_INHIBITS\t\t\t\\\n(\t\t\t\t\t\t\t\\\n\tBIT(APICV_INHIBIT_REASON_DISABLE) |\t\t\\\n\tBIT(APICV_INHIBIT_REASON_ABSENT) |\t\t\\\n\tBIT(APICV_INHIBIT_REASON_HYPERV) |\t\t\\\n\tBIT(APICV_INHIBIT_REASON_NESTED) |\t\t\\\n\tBIT(APICV_INHIBIT_REASON_IRQWIN) |\t\t\\\n\tBIT(APICV_INHIBIT_REASON_PIT_REINJ) |\t\t\\\n\tBIT(APICV_INHIBIT_REASON_BLOCKIRQ) |\t\t\\\n\tBIT(APICV_INHIBIT_REASON_SEV)      |\t\t\\\n\tBIT(APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED) |\t\\\n\tBIT(APICV_INHIBIT_REASON_APIC_ID_MODIFIED) |\t\\\n\tBIT(APICV_INHIBIT_REASON_APIC_BASE_MODIFIED) |\t\\\n\tBIT(APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED)\t\\\n)\n\nbool avic_hardware_setup(void);\nint avic_ga_log_notifier(u32 ga_tag);\nvoid avic_vm_destroy(struct kvm *kvm);\nint avic_vm_init(struct kvm *kvm);\nvoid avic_init_vmcb(struct vcpu_svm *svm, struct vmcb *vmcb);\nint avic_incomplete_ipi_interception(struct kvm_vcpu *vcpu);\nint avic_unaccelerated_access_interception(struct kvm_vcpu *vcpu);\nint avic_init_vcpu(struct vcpu_svm *svm);\nvoid avic_vcpu_load(struct kvm_vcpu *vcpu, int cpu);\nvoid avic_vcpu_put(struct kvm_vcpu *vcpu);\nvoid avic_apicv_post_state_restore(struct kvm_vcpu *vcpu);\nvoid avic_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu);\nint avic_pi_update_irte(struct kvm *kvm, unsigned int host_irq,\n\t\t\tuint32_t guest_irq, bool set);\nvoid avic_vcpu_blocking(struct kvm_vcpu *vcpu);\nvoid avic_vcpu_unblocking(struct kvm_vcpu *vcpu);\nvoid avic_ring_doorbell(struct kvm_vcpu *vcpu);\nunsigned long avic_vcpu_get_apicv_inhibit_reasons(struct kvm_vcpu *vcpu);\nvoid avic_refresh_virtual_apic_mode(struct kvm_vcpu *vcpu);\n\n\n \n\n#define GHCB_VERSION_MAX\t1ULL\n#define GHCB_VERSION_MIN\t1ULL\n\n\nextern unsigned int max_sev_asid;\n\nvoid sev_vm_destroy(struct kvm *kvm);\nint sev_mem_enc_ioctl(struct kvm *kvm, void __user *argp);\nint sev_mem_enc_register_region(struct kvm *kvm,\n\t\t\t\tstruct kvm_enc_region *range);\nint sev_mem_enc_unregister_region(struct kvm *kvm,\n\t\t\t\t  struct kvm_enc_region *range);\nint sev_vm_copy_enc_context_from(struct kvm *kvm, unsigned int source_fd);\nint sev_vm_move_enc_context_from(struct kvm *kvm, unsigned int source_fd);\nvoid sev_guest_memory_reclaimed(struct kvm *kvm);\n\nvoid pre_sev_run(struct vcpu_svm *svm, int cpu);\nvoid __init sev_set_cpu_caps(void);\nvoid __init sev_hardware_setup(void);\nvoid sev_hardware_unsetup(void);\nint sev_cpu_init(struct svm_cpu_data *sd);\nvoid sev_init_vmcb(struct vcpu_svm *svm);\nvoid sev_vcpu_after_set_cpuid(struct vcpu_svm *svm);\nvoid sev_free_vcpu(struct kvm_vcpu *vcpu);\nint sev_handle_vmgexit(struct kvm_vcpu *vcpu);\nint sev_es_string_io(struct vcpu_svm *svm, int size, unsigned int port, int in);\nvoid sev_es_vcpu_reset(struct vcpu_svm *svm);\nvoid sev_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, u8 vector);\nvoid sev_es_prepare_switch_to_guest(struct sev_es_save_area *hostsa);\nvoid sev_es_unmap_ghcb(struct vcpu_svm *svm);\n\n \n\nvoid __svm_sev_es_vcpu_run(struct vcpu_svm *svm, bool spec_ctrl_intercepted);\nvoid __svm_vcpu_run(struct vcpu_svm *svm, bool spec_ctrl_intercepted);\n\n#define DEFINE_KVM_GHCB_ACCESSORS(field)\t\t\t\t\t\t\\\n\tstatic __always_inline bool kvm_ghcb_##field##_is_valid(const struct vcpu_svm *svm) \\\n\t{\t\t\t\t\t\t\t\t\t\\\n\t\treturn test_bit(GHCB_BITMAP_IDX(field),\t\t\t\t\\\n\t\t\t\t(unsigned long *)&svm->sev_es.valid_bitmap);\t\\\n\t}\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\\\n\tstatic __always_inline u64 kvm_ghcb_get_##field##_if_valid(struct vcpu_svm *svm, struct ghcb *ghcb) \\\n\t{\t\t\t\t\t\t\t\t\t\\\n\t\treturn kvm_ghcb_##field##_is_valid(svm) ? ghcb->save.field : 0;\t\\\n\t}\t\t\t\t\t\t\t\t\t\\\n\nDEFINE_KVM_GHCB_ACCESSORS(cpl)\nDEFINE_KVM_GHCB_ACCESSORS(rax)\nDEFINE_KVM_GHCB_ACCESSORS(rcx)\nDEFINE_KVM_GHCB_ACCESSORS(rdx)\nDEFINE_KVM_GHCB_ACCESSORS(rbx)\nDEFINE_KVM_GHCB_ACCESSORS(rsi)\nDEFINE_KVM_GHCB_ACCESSORS(sw_exit_code)\nDEFINE_KVM_GHCB_ACCESSORS(sw_exit_info_1)\nDEFINE_KVM_GHCB_ACCESSORS(sw_exit_info_2)\nDEFINE_KVM_GHCB_ACCESSORS(sw_scratch)\nDEFINE_KVM_GHCB_ACCESSORS(xcr0)\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}