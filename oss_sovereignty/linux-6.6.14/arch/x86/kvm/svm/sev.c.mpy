{
  "module_name": "sev.c",
  "hash_id": "0ff5b8aa19dad02cf65e12ff538668e6cb8e1bd027193a26c3f859f286ded514",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/svm/sev.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/kvm_types.h>\n#include <linux/kvm_host.h>\n#include <linux/kernel.h>\n#include <linux/highmem.h>\n#include <linux/psp.h>\n#include <linux/psp-sev.h>\n#include <linux/pagemap.h>\n#include <linux/swap.h>\n#include <linux/misc_cgroup.h>\n#include <linux/processor.h>\n#include <linux/trace_events.h>\n\n#include <asm/pkru.h>\n#include <asm/trapnr.h>\n#include <asm/fpu/xcr.h>\n#include <asm/debugreg.h>\n\n#include \"mmu.h\"\n#include \"x86.h\"\n#include \"svm.h\"\n#include \"svm_ops.h\"\n#include \"cpuid.h\"\n#include \"trace.h\"\n\n#ifndef CONFIG_KVM_AMD_SEV\n \n#define MISC_CG_RES_SEV MISC_CG_RES_TYPES\n#define MISC_CG_RES_SEV_ES MISC_CG_RES_TYPES\n#endif\n\n#ifdef CONFIG_KVM_AMD_SEV\n \nstatic bool sev_enabled = true;\nmodule_param_named(sev, sev_enabled, bool, 0444);\n\n \nstatic bool sev_es_enabled = true;\nmodule_param_named(sev_es, sev_es_enabled, bool, 0444);\n\n \nstatic bool sev_es_debug_swap_enabled = true;\nmodule_param_named(debug_swap, sev_es_debug_swap_enabled, bool, 0444);\n#else\n#define sev_enabled false\n#define sev_es_enabled false\n#define sev_es_debug_swap_enabled false\n#endif  \n\nstatic u8 sev_enc_bit;\nstatic DECLARE_RWSEM(sev_deactivate_lock);\nstatic DEFINE_MUTEX(sev_bitmap_lock);\nunsigned int max_sev_asid;\nstatic unsigned int min_sev_asid;\nstatic unsigned long sev_me_mask;\nstatic unsigned int nr_asids;\nstatic unsigned long *sev_asid_bitmap;\nstatic unsigned long *sev_reclaim_asid_bitmap;\n\nstruct enc_region {\n\tstruct list_head list;\n\tunsigned long npages;\n\tstruct page **pages;\n\tunsigned long uaddr;\n\tunsigned long size;\n};\n\n \nstatic int sev_flush_asids(int min_asid, int max_asid)\n{\n\tint ret, asid, error = 0;\n\n\t \n\tasid = find_next_bit(sev_reclaim_asid_bitmap, nr_asids, min_asid);\n\tif (asid > max_asid)\n\t\treturn -EBUSY;\n\n\t \n\tdown_write(&sev_deactivate_lock);\n\n\twbinvd_on_all_cpus();\n\tret = sev_guest_df_flush(&error);\n\n\tup_write(&sev_deactivate_lock);\n\n\tif (ret)\n\t\tpr_err(\"SEV: DF_FLUSH failed, ret=%d, error=%#x\\n\", ret, error);\n\n\treturn ret;\n}\n\nstatic inline bool is_mirroring_enc_context(struct kvm *kvm)\n{\n\treturn !!to_kvm_svm(kvm)->sev_info.enc_context_owner;\n}\n\n \nstatic bool __sev_recycle_asids(int min_asid, int max_asid)\n{\n\tif (sev_flush_asids(min_asid, max_asid))\n\t\treturn false;\n\n\t \n\tbitmap_xor(sev_asid_bitmap, sev_asid_bitmap, sev_reclaim_asid_bitmap,\n\t\t   nr_asids);\n\tbitmap_zero(sev_reclaim_asid_bitmap, nr_asids);\n\n\treturn true;\n}\n\nstatic int sev_misc_cg_try_charge(struct kvm_sev_info *sev)\n{\n\tenum misc_res_type type = sev->es_active ? MISC_CG_RES_SEV_ES : MISC_CG_RES_SEV;\n\treturn misc_cg_try_charge(type, sev->misc_cg, 1);\n}\n\nstatic void sev_misc_cg_uncharge(struct kvm_sev_info *sev)\n{\n\tenum misc_res_type type = sev->es_active ? MISC_CG_RES_SEV_ES : MISC_CG_RES_SEV;\n\tmisc_cg_uncharge(type, sev->misc_cg, 1);\n}\n\nstatic int sev_asid_new(struct kvm_sev_info *sev)\n{\n\tint asid, min_asid, max_asid, ret;\n\tbool retry = true;\n\n\tWARN_ON(sev->misc_cg);\n\tsev->misc_cg = get_current_misc_cg();\n\tret = sev_misc_cg_try_charge(sev);\n\tif (ret) {\n\t\tput_misc_cg(sev->misc_cg);\n\t\tsev->misc_cg = NULL;\n\t\treturn ret;\n\t}\n\n\tmutex_lock(&sev_bitmap_lock);\n\n\t \n\tmin_asid = sev->es_active ? 1 : min_sev_asid;\n\tmax_asid = sev->es_active ? min_sev_asid - 1 : max_sev_asid;\nagain:\n\tasid = find_next_zero_bit(sev_asid_bitmap, max_asid + 1, min_asid);\n\tif (asid > max_asid) {\n\t\tif (retry && __sev_recycle_asids(min_asid, max_asid)) {\n\t\t\tretry = false;\n\t\t\tgoto again;\n\t\t}\n\t\tmutex_unlock(&sev_bitmap_lock);\n\t\tret = -EBUSY;\n\t\tgoto e_uncharge;\n\t}\n\n\t__set_bit(asid, sev_asid_bitmap);\n\n\tmutex_unlock(&sev_bitmap_lock);\n\n\treturn asid;\ne_uncharge:\n\tsev_misc_cg_uncharge(sev);\n\tput_misc_cg(sev->misc_cg);\n\tsev->misc_cg = NULL;\n\treturn ret;\n}\n\nstatic int sev_get_asid(struct kvm *kvm)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\n\treturn sev->asid;\n}\n\nstatic void sev_asid_free(struct kvm_sev_info *sev)\n{\n\tstruct svm_cpu_data *sd;\n\tint cpu;\n\n\tmutex_lock(&sev_bitmap_lock);\n\n\t__set_bit(sev->asid, sev_reclaim_asid_bitmap);\n\n\tfor_each_possible_cpu(cpu) {\n\t\tsd = per_cpu_ptr(&svm_data, cpu);\n\t\tsd->sev_vmcbs[sev->asid] = NULL;\n\t}\n\n\tmutex_unlock(&sev_bitmap_lock);\n\n\tsev_misc_cg_uncharge(sev);\n\tput_misc_cg(sev->misc_cg);\n\tsev->misc_cg = NULL;\n}\n\nstatic void sev_decommission(unsigned int handle)\n{\n\tstruct sev_data_decommission decommission;\n\n\tif (!handle)\n\t\treturn;\n\n\tdecommission.handle = handle;\n\tsev_guest_decommission(&decommission, NULL);\n}\n\nstatic void sev_unbind_asid(struct kvm *kvm, unsigned int handle)\n{\n\tstruct sev_data_deactivate deactivate;\n\n\tif (!handle)\n\t\treturn;\n\n\tdeactivate.handle = handle;\n\n\t \n\tdown_read(&sev_deactivate_lock);\n\tsev_guest_deactivate(&deactivate, NULL);\n\tup_read(&sev_deactivate_lock);\n\n\tsev_decommission(handle);\n}\n\nstatic int sev_guest_init(struct kvm *kvm, struct kvm_sev_cmd *argp)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tint asid, ret;\n\n\tif (kvm->created_vcpus)\n\t\treturn -EINVAL;\n\n\tret = -EBUSY;\n\tif (unlikely(sev->active))\n\t\treturn ret;\n\n\tsev->active = true;\n\tsev->es_active = argp->id == KVM_SEV_ES_INIT;\n\tasid = sev_asid_new(sev);\n\tif (asid < 0)\n\t\tgoto e_no_asid;\n\tsev->asid = asid;\n\n\tret = sev_platform_init(&argp->error);\n\tif (ret)\n\t\tgoto e_free;\n\n\tINIT_LIST_HEAD(&sev->regions_list);\n\tINIT_LIST_HEAD(&sev->mirror_vms);\n\n\tkvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_SEV);\n\n\treturn 0;\n\ne_free:\n\tsev_asid_free(sev);\n\tsev->asid = 0;\ne_no_asid:\n\tsev->es_active = false;\n\tsev->active = false;\n\treturn ret;\n}\n\nstatic int sev_bind_asid(struct kvm *kvm, unsigned int handle, int *error)\n{\n\tstruct sev_data_activate activate;\n\tint asid = sev_get_asid(kvm);\n\tint ret;\n\n\t \n\tactivate.handle = handle;\n\tactivate.asid   = asid;\n\tret = sev_guest_activate(&activate, error);\n\n\treturn ret;\n}\n\nstatic int __sev_issue_cmd(int fd, int id, void *data, int *error)\n{\n\tstruct fd f;\n\tint ret;\n\n\tf = fdget(fd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tret = sev_issue_cmd_external_user(f.file, id, data, error);\n\n\tfdput(f);\n\treturn ret;\n}\n\nstatic int sev_issue_cmd(struct kvm *kvm, int id, void *data, int *error)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\n\treturn __sev_issue_cmd(sev->fd, id, data, error);\n}\n\nstatic int sev_launch_start(struct kvm *kvm, struct kvm_sev_cmd *argp)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct sev_data_launch_start start;\n\tstruct kvm_sev_launch_start params;\n\tvoid *dh_blob, *session_blob;\n\tint *error = &argp->error;\n\tint ret;\n\n\tif (!sev_guest(kvm))\n\t\treturn -ENOTTY;\n\n\tif (copy_from_user(&params, (void __user *)(uintptr_t)argp->data, sizeof(params)))\n\t\treturn -EFAULT;\n\n\tmemset(&start, 0, sizeof(start));\n\n\tdh_blob = NULL;\n\tif (params.dh_uaddr) {\n\t\tdh_blob = psp_copy_user_blob(params.dh_uaddr, params.dh_len);\n\t\tif (IS_ERR(dh_blob))\n\t\t\treturn PTR_ERR(dh_blob);\n\n\t\tstart.dh_cert_address = __sme_set(__pa(dh_blob));\n\t\tstart.dh_cert_len = params.dh_len;\n\t}\n\n\tsession_blob = NULL;\n\tif (params.session_uaddr) {\n\t\tsession_blob = psp_copy_user_blob(params.session_uaddr, params.session_len);\n\t\tif (IS_ERR(session_blob)) {\n\t\t\tret = PTR_ERR(session_blob);\n\t\t\tgoto e_free_dh;\n\t\t}\n\n\t\tstart.session_address = __sme_set(__pa(session_blob));\n\t\tstart.session_len = params.session_len;\n\t}\n\n\tstart.handle = params.handle;\n\tstart.policy = params.policy;\n\n\t \n\tret = __sev_issue_cmd(argp->sev_fd, SEV_CMD_LAUNCH_START, &start, error);\n\tif (ret)\n\t\tgoto e_free_session;\n\n\t \n\tret = sev_bind_asid(kvm, start.handle, error);\n\tif (ret) {\n\t\tsev_decommission(start.handle);\n\t\tgoto e_free_session;\n\t}\n\n\t \n\tparams.handle = start.handle;\n\tif (copy_to_user((void __user *)(uintptr_t)argp->data, &params, sizeof(params))) {\n\t\tsev_unbind_asid(kvm, start.handle);\n\t\tret = -EFAULT;\n\t\tgoto e_free_session;\n\t}\n\n\tsev->handle = start.handle;\n\tsev->fd = argp->sev_fd;\n\ne_free_session:\n\tkfree(session_blob);\ne_free_dh:\n\tkfree(dh_blob);\n\treturn ret;\n}\n\nstatic struct page **sev_pin_memory(struct kvm *kvm, unsigned long uaddr,\n\t\t\t\t    unsigned long ulen, unsigned long *n,\n\t\t\t\t    int write)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tunsigned long npages, size;\n\tint npinned;\n\tunsigned long locked, lock_limit;\n\tstruct page **pages;\n\tunsigned long first, last;\n\tint ret;\n\n\tlockdep_assert_held(&kvm->lock);\n\n\tif (ulen == 0 || uaddr + ulen < uaddr)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tfirst = (uaddr & PAGE_MASK) >> PAGE_SHIFT;\n\tlast = ((uaddr + ulen - 1) & PAGE_MASK) >> PAGE_SHIFT;\n\tnpages = (last - first + 1);\n\n\tlocked = sev->pages_locked + npages;\n\tlock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;\n\tif (locked > lock_limit && !capable(CAP_IPC_LOCK)) {\n\t\tpr_err(\"SEV: %lu locked pages exceed the lock limit of %lu.\\n\", locked, lock_limit);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tif (WARN_ON_ONCE(npages > INT_MAX))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t \n\tsize = npages * sizeof(struct page *);\n\tif (size > PAGE_SIZE)\n\t\tpages = __vmalloc(size, GFP_KERNEL_ACCOUNT | __GFP_ZERO);\n\telse\n\t\tpages = kmalloc(size, GFP_KERNEL_ACCOUNT);\n\n\tif (!pages)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\t \n\tnpinned = pin_user_pages_fast(uaddr, npages, write ? FOLL_WRITE : 0, pages);\n\tif (npinned != npages) {\n\t\tpr_err(\"SEV: Failure locking %lu pages.\\n\", npages);\n\t\tret = -ENOMEM;\n\t\tgoto err;\n\t}\n\n\t*n = npages;\n\tsev->pages_locked = locked;\n\n\treturn pages;\n\nerr:\n\tif (npinned > 0)\n\t\tunpin_user_pages(pages, npinned);\n\n\tkvfree(pages);\n\treturn ERR_PTR(ret);\n}\n\nstatic void sev_unpin_memory(struct kvm *kvm, struct page **pages,\n\t\t\t     unsigned long npages)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\n\tunpin_user_pages(pages, npages);\n\tkvfree(pages);\n\tsev->pages_locked -= npages;\n}\n\nstatic void sev_clflush_pages(struct page *pages[], unsigned long npages)\n{\n\tuint8_t *page_virtual;\n\tunsigned long i;\n\n\tif (this_cpu_has(X86_FEATURE_SME_COHERENT) || npages == 0 ||\n\t    pages == NULL)\n\t\treturn;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tpage_virtual = kmap_local_page(pages[i]);\n\t\tclflush_cache_range(page_virtual, PAGE_SIZE);\n\t\tkunmap_local(page_virtual);\n\t\tcond_resched();\n\t}\n}\n\nstatic unsigned long get_num_contig_pages(unsigned long idx,\n\t\t\t\tstruct page **inpages, unsigned long npages)\n{\n\tunsigned long paddr, next_paddr;\n\tunsigned long i = idx + 1, pages = 1;\n\n\t \n\tpaddr = __sme_page_pa(inpages[idx]);\n\twhile (i < npages) {\n\t\tnext_paddr = __sme_page_pa(inpages[i++]);\n\t\tif ((paddr + PAGE_SIZE) == next_paddr) {\n\t\t\tpages++;\n\t\t\tpaddr = next_paddr;\n\t\t\tcontinue;\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn pages;\n}\n\nstatic int sev_launch_update_data(struct kvm *kvm, struct kvm_sev_cmd *argp)\n{\n\tunsigned long vaddr, vaddr_end, next_vaddr, npages, pages, size, i;\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct kvm_sev_launch_update_data params;\n\tstruct sev_data_launch_update_data data;\n\tstruct page **inpages;\n\tint ret;\n\n\tif (!sev_guest(kvm))\n\t\treturn -ENOTTY;\n\n\tif (copy_from_user(&params, (void __user *)(uintptr_t)argp->data, sizeof(params)))\n\t\treturn -EFAULT;\n\n\tvaddr = params.uaddr;\n\tsize = params.len;\n\tvaddr_end = vaddr + size;\n\n\t \n\tinpages = sev_pin_memory(kvm, vaddr, size, &npages, 1);\n\tif (IS_ERR(inpages))\n\t\treturn PTR_ERR(inpages);\n\n\t \n\tsev_clflush_pages(inpages, npages);\n\n\tdata.reserved = 0;\n\tdata.handle = sev->handle;\n\n\tfor (i = 0; vaddr < vaddr_end; vaddr = next_vaddr, i += pages) {\n\t\tint offset, len;\n\n\t\t \n\t\toffset = vaddr & (PAGE_SIZE - 1);\n\n\t\t \n\t\tpages = get_num_contig_pages(i, inpages, npages);\n\n\t\tlen = min_t(size_t, ((pages * PAGE_SIZE) - offset), size);\n\n\t\tdata.len = len;\n\t\tdata.address = __sme_page_pa(inpages[i]) + offset;\n\t\tret = sev_issue_cmd(kvm, SEV_CMD_LAUNCH_UPDATE_DATA, &data, &argp->error);\n\t\tif (ret)\n\t\t\tgoto e_unpin;\n\n\t\tsize -= len;\n\t\tnext_vaddr = vaddr + len;\n\t}\n\ne_unpin:\n\t \n\tfor (i = 0; i < npages; i++) {\n\t\tset_page_dirty_lock(inpages[i]);\n\t\tmark_page_accessed(inpages[i]);\n\t}\n\t \n\tsev_unpin_memory(kvm, inpages, npages);\n\treturn ret;\n}\n\nstatic int sev_es_sync_vmsa(struct vcpu_svm *svm)\n{\n\tstruct sev_es_save_area *save = svm->sev_es.vmsa;\n\n\t \n\tif (svm->vcpu.guest_debug || (svm->vmcb->save.dr7 & ~DR7_FIXED_1))\n\t\treturn -EINVAL;\n\n\t \n\tmemcpy(save, &svm->vmcb->save, sizeof(svm->vmcb->save));\n\n\t \n\tsave->rax = svm->vcpu.arch.regs[VCPU_REGS_RAX];\n\tsave->rbx = svm->vcpu.arch.regs[VCPU_REGS_RBX];\n\tsave->rcx = svm->vcpu.arch.regs[VCPU_REGS_RCX];\n\tsave->rdx = svm->vcpu.arch.regs[VCPU_REGS_RDX];\n\tsave->rsp = svm->vcpu.arch.regs[VCPU_REGS_RSP];\n\tsave->rbp = svm->vcpu.arch.regs[VCPU_REGS_RBP];\n\tsave->rsi = svm->vcpu.arch.regs[VCPU_REGS_RSI];\n\tsave->rdi = svm->vcpu.arch.regs[VCPU_REGS_RDI];\n#ifdef CONFIG_X86_64\n\tsave->r8  = svm->vcpu.arch.regs[VCPU_REGS_R8];\n\tsave->r9  = svm->vcpu.arch.regs[VCPU_REGS_R9];\n\tsave->r10 = svm->vcpu.arch.regs[VCPU_REGS_R10];\n\tsave->r11 = svm->vcpu.arch.regs[VCPU_REGS_R11];\n\tsave->r12 = svm->vcpu.arch.regs[VCPU_REGS_R12];\n\tsave->r13 = svm->vcpu.arch.regs[VCPU_REGS_R13];\n\tsave->r14 = svm->vcpu.arch.regs[VCPU_REGS_R14];\n\tsave->r15 = svm->vcpu.arch.regs[VCPU_REGS_R15];\n#endif\n\tsave->rip = svm->vcpu.arch.regs[VCPU_REGS_RIP];\n\n\t \n\tsave->xcr0 = svm->vcpu.arch.xcr0;\n\tsave->pkru = svm->vcpu.arch.pkru;\n\tsave->xss  = svm->vcpu.arch.ia32_xss;\n\tsave->dr6  = svm->vcpu.arch.dr6;\n\n\tif (sev_es_debug_swap_enabled)\n\t\tsave->sev_features |= SVM_SEV_FEAT_DEBUG_SWAP;\n\n\tpr_debug(\"Virtual Machine Save Area (VMSA):\\n\");\n\tprint_hex_dump_debug(\"\", DUMP_PREFIX_NONE, 16, 1, save, sizeof(*save), false);\n\n\treturn 0;\n}\n\nstatic int __sev_launch_update_vmsa(struct kvm *kvm, struct kvm_vcpu *vcpu,\n\t\t\t\t    int *error)\n{\n\tstruct sev_data_launch_update_vmsa vmsa;\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tint ret;\n\n\tif (vcpu->guest_debug) {\n\t\tpr_warn_once(\"KVM_SET_GUEST_DEBUG for SEV-ES guest is not supported\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tret = sev_es_sync_vmsa(svm);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tclflush_cache_range(svm->sev_es.vmsa, PAGE_SIZE);\n\n\tvmsa.reserved = 0;\n\tvmsa.handle = to_kvm_svm(kvm)->sev_info.handle;\n\tvmsa.address = __sme_pa(svm->sev_es.vmsa);\n\tvmsa.len = PAGE_SIZE;\n\tret = sev_issue_cmd(kvm, SEV_CMD_LAUNCH_UPDATE_VMSA, &vmsa, error);\n\tif (ret)\n\t  return ret;\n\n\tvcpu->arch.guest_state_protected = true;\n\treturn 0;\n}\n\nstatic int sev_launch_update_vmsa(struct kvm *kvm, struct kvm_sev_cmd *argp)\n{\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\tint ret;\n\n\tif (!sev_es_guest(kvm))\n\t\treturn -ENOTTY;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tret = mutex_lock_killable(&vcpu->mutex);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tret = __sev_launch_update_vmsa(kvm, vcpu, &argp->error);\n\n\t\tmutex_unlock(&vcpu->mutex);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstatic int sev_launch_measure(struct kvm *kvm, struct kvm_sev_cmd *argp)\n{\n\tvoid __user *measure = (void __user *)(uintptr_t)argp->data;\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct sev_data_launch_measure data;\n\tstruct kvm_sev_launch_measure params;\n\tvoid __user *p = NULL;\n\tvoid *blob = NULL;\n\tint ret;\n\n\tif (!sev_guest(kvm))\n\t\treturn -ENOTTY;\n\n\tif (copy_from_user(&params, measure, sizeof(params)))\n\t\treturn -EFAULT;\n\n\tmemset(&data, 0, sizeof(data));\n\n\t \n\tif (!params.len)\n\t\tgoto cmd;\n\n\tp = (void __user *)(uintptr_t)params.uaddr;\n\tif (p) {\n\t\tif (params.len > SEV_FW_BLOB_MAX_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\tblob = kzalloc(params.len, GFP_KERNEL_ACCOUNT);\n\t\tif (!blob)\n\t\t\treturn -ENOMEM;\n\n\t\tdata.address = __psp_pa(blob);\n\t\tdata.len = params.len;\n\t}\n\ncmd:\n\tdata.handle = sev->handle;\n\tret = sev_issue_cmd(kvm, SEV_CMD_LAUNCH_MEASURE, &data, &argp->error);\n\n\t \n\tif (!params.len)\n\t\tgoto done;\n\n\tif (ret)\n\t\tgoto e_free_blob;\n\n\tif (blob) {\n\t\tif (copy_to_user(p, blob, params.len))\n\t\t\tret = -EFAULT;\n\t}\n\ndone:\n\tparams.len = data.len;\n\tif (copy_to_user(measure, &params, sizeof(params)))\n\t\tret = -EFAULT;\ne_free_blob:\n\tkfree(blob);\n\treturn ret;\n}\n\nstatic int sev_launch_finish(struct kvm *kvm, struct kvm_sev_cmd *argp)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct sev_data_launch_finish data;\n\n\tif (!sev_guest(kvm))\n\t\treturn -ENOTTY;\n\n\tdata.handle = sev->handle;\n\treturn sev_issue_cmd(kvm, SEV_CMD_LAUNCH_FINISH, &data, &argp->error);\n}\n\nstatic int sev_guest_status(struct kvm *kvm, struct kvm_sev_cmd *argp)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct kvm_sev_guest_status params;\n\tstruct sev_data_guest_status data;\n\tint ret;\n\n\tif (!sev_guest(kvm))\n\t\treturn -ENOTTY;\n\n\tmemset(&data, 0, sizeof(data));\n\n\tdata.handle = sev->handle;\n\tret = sev_issue_cmd(kvm, SEV_CMD_GUEST_STATUS, &data, &argp->error);\n\tif (ret)\n\t\treturn ret;\n\n\tparams.policy = data.policy;\n\tparams.state = data.state;\n\tparams.handle = data.handle;\n\n\tif (copy_to_user((void __user *)(uintptr_t)argp->data, &params, sizeof(params)))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}\n\nstatic int __sev_issue_dbg_cmd(struct kvm *kvm, unsigned long src,\n\t\t\t       unsigned long dst, int size,\n\t\t\t       int *error, bool enc)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct sev_data_dbg data;\n\n\tdata.reserved = 0;\n\tdata.handle = sev->handle;\n\tdata.dst_addr = dst;\n\tdata.src_addr = src;\n\tdata.len = size;\n\n\treturn sev_issue_cmd(kvm,\n\t\t\t     enc ? SEV_CMD_DBG_ENCRYPT : SEV_CMD_DBG_DECRYPT,\n\t\t\t     &data, error);\n}\n\nstatic int __sev_dbg_decrypt(struct kvm *kvm, unsigned long src_paddr,\n\t\t\t     unsigned long dst_paddr, int sz, int *err)\n{\n\tint offset;\n\n\t \n\toffset = src_paddr & 15;\n\tsrc_paddr = round_down(src_paddr, 16);\n\tsz = round_up(sz + offset, 16);\n\n\treturn __sev_issue_dbg_cmd(kvm, src_paddr, dst_paddr, sz, err, false);\n}\n\nstatic int __sev_dbg_decrypt_user(struct kvm *kvm, unsigned long paddr,\n\t\t\t\t  void __user *dst_uaddr,\n\t\t\t\t  unsigned long dst_paddr,\n\t\t\t\t  int size, int *err)\n{\n\tstruct page *tpage = NULL;\n\tint ret, offset;\n\n\t \n\tif (!IS_ALIGNED(dst_paddr, 16) ||\n\t    !IS_ALIGNED(paddr,     16) ||\n\t    !IS_ALIGNED(size,      16)) {\n\t\ttpage = (void *)alloc_page(GFP_KERNEL_ACCOUNT | __GFP_ZERO);\n\t\tif (!tpage)\n\t\t\treturn -ENOMEM;\n\n\t\tdst_paddr = __sme_page_pa(tpage);\n\t}\n\n\tret = __sev_dbg_decrypt(kvm, paddr, dst_paddr, size, err);\n\tif (ret)\n\t\tgoto e_free;\n\n\tif (tpage) {\n\t\toffset = paddr & 15;\n\t\tif (copy_to_user(dst_uaddr, page_address(tpage) + offset, size))\n\t\t\tret = -EFAULT;\n\t}\n\ne_free:\n\tif (tpage)\n\t\t__free_page(tpage);\n\n\treturn ret;\n}\n\nstatic int __sev_dbg_encrypt_user(struct kvm *kvm, unsigned long paddr,\n\t\t\t\t  void __user *vaddr,\n\t\t\t\t  unsigned long dst_paddr,\n\t\t\t\t  void __user *dst_vaddr,\n\t\t\t\t  int size, int *error)\n{\n\tstruct page *src_tpage = NULL;\n\tstruct page *dst_tpage = NULL;\n\tint ret, len = size;\n\n\t \n\tif (!IS_ALIGNED((unsigned long)vaddr, 16)) {\n\t\tsrc_tpage = alloc_page(GFP_KERNEL_ACCOUNT);\n\t\tif (!src_tpage)\n\t\t\treturn -ENOMEM;\n\n\t\tif (copy_from_user(page_address(src_tpage), vaddr, size)) {\n\t\t\t__free_page(src_tpage);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\tpaddr = __sme_page_pa(src_tpage);\n\t}\n\n\t \n\tif (!IS_ALIGNED((unsigned long)dst_vaddr, 16) || !IS_ALIGNED(size, 16)) {\n\t\tint dst_offset;\n\n\t\tdst_tpage = alloc_page(GFP_KERNEL_ACCOUNT);\n\t\tif (!dst_tpage) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto e_free;\n\t\t}\n\n\t\tret = __sev_dbg_decrypt(kvm, dst_paddr,\n\t\t\t\t\t__sme_page_pa(dst_tpage), size, error);\n\t\tif (ret)\n\t\t\tgoto e_free;\n\n\t\t \n\t\tdst_offset = dst_paddr & 15;\n\n\t\tif (src_tpage)\n\t\t\tmemcpy(page_address(dst_tpage) + dst_offset,\n\t\t\t       page_address(src_tpage), size);\n\t\telse {\n\t\t\tif (copy_from_user(page_address(dst_tpage) + dst_offset,\n\t\t\t\t\t   vaddr, size)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto e_free;\n\t\t\t}\n\t\t}\n\n\t\tpaddr = __sme_page_pa(dst_tpage);\n\t\tdst_paddr = round_down(dst_paddr, 16);\n\t\tlen = round_up(size, 16);\n\t}\n\n\tret = __sev_issue_dbg_cmd(kvm, paddr, dst_paddr, len, error, true);\n\ne_free:\n\tif (src_tpage)\n\t\t__free_page(src_tpage);\n\tif (dst_tpage)\n\t\t__free_page(dst_tpage);\n\treturn ret;\n}\n\nstatic int sev_dbg_crypt(struct kvm *kvm, struct kvm_sev_cmd *argp, bool dec)\n{\n\tunsigned long vaddr, vaddr_end, next_vaddr;\n\tunsigned long dst_vaddr;\n\tstruct page **src_p, **dst_p;\n\tstruct kvm_sev_dbg debug;\n\tunsigned long n;\n\tunsigned int size;\n\tint ret;\n\n\tif (!sev_guest(kvm))\n\t\treturn -ENOTTY;\n\n\tif (copy_from_user(&debug, (void __user *)(uintptr_t)argp->data, sizeof(debug)))\n\t\treturn -EFAULT;\n\n\tif (!debug.len || debug.src_uaddr + debug.len < debug.src_uaddr)\n\t\treturn -EINVAL;\n\tif (!debug.dst_uaddr)\n\t\treturn -EINVAL;\n\n\tvaddr = debug.src_uaddr;\n\tsize = debug.len;\n\tvaddr_end = vaddr + size;\n\tdst_vaddr = debug.dst_uaddr;\n\n\tfor (; vaddr < vaddr_end; vaddr = next_vaddr) {\n\t\tint len, s_off, d_off;\n\n\t\t \n\t\tsrc_p = sev_pin_memory(kvm, vaddr & PAGE_MASK, PAGE_SIZE, &n, 0);\n\t\tif (IS_ERR(src_p))\n\t\t\treturn PTR_ERR(src_p);\n\n\t\tdst_p = sev_pin_memory(kvm, dst_vaddr & PAGE_MASK, PAGE_SIZE, &n, 1);\n\t\tif (IS_ERR(dst_p)) {\n\t\t\tsev_unpin_memory(kvm, src_p, n);\n\t\t\treturn PTR_ERR(dst_p);\n\t\t}\n\n\t\t \n\t\tsev_clflush_pages(src_p, 1);\n\t\tsev_clflush_pages(dst_p, 1);\n\n\t\t \n\t\ts_off = vaddr & ~PAGE_MASK;\n\t\td_off = dst_vaddr & ~PAGE_MASK;\n\t\tlen = min_t(size_t, (PAGE_SIZE - s_off), size);\n\n\t\tif (dec)\n\t\t\tret = __sev_dbg_decrypt_user(kvm,\n\t\t\t\t\t\t     __sme_page_pa(src_p[0]) + s_off,\n\t\t\t\t\t\t     (void __user *)dst_vaddr,\n\t\t\t\t\t\t     __sme_page_pa(dst_p[0]) + d_off,\n\t\t\t\t\t\t     len, &argp->error);\n\t\telse\n\t\t\tret = __sev_dbg_encrypt_user(kvm,\n\t\t\t\t\t\t     __sme_page_pa(src_p[0]) + s_off,\n\t\t\t\t\t\t     (void __user *)vaddr,\n\t\t\t\t\t\t     __sme_page_pa(dst_p[0]) + d_off,\n\t\t\t\t\t\t     (void __user *)dst_vaddr,\n\t\t\t\t\t\t     len, &argp->error);\n\n\t\tsev_unpin_memory(kvm, src_p, n);\n\t\tsev_unpin_memory(kvm, dst_p, n);\n\n\t\tif (ret)\n\t\t\tgoto err;\n\n\t\tnext_vaddr = vaddr + len;\n\t\tdst_vaddr = dst_vaddr + len;\n\t\tsize -= len;\n\t}\nerr:\n\treturn ret;\n}\n\nstatic int sev_launch_secret(struct kvm *kvm, struct kvm_sev_cmd *argp)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct sev_data_launch_secret data;\n\tstruct kvm_sev_launch_secret params;\n\tstruct page **pages;\n\tvoid *blob, *hdr;\n\tunsigned long n, i;\n\tint ret, offset;\n\n\tif (!sev_guest(kvm))\n\t\treturn -ENOTTY;\n\n\tif (copy_from_user(&params, (void __user *)(uintptr_t)argp->data, sizeof(params)))\n\t\treturn -EFAULT;\n\n\tpages = sev_pin_memory(kvm, params.guest_uaddr, params.guest_len, &n, 1);\n\tif (IS_ERR(pages))\n\t\treturn PTR_ERR(pages);\n\n\t \n\tsev_clflush_pages(pages, n);\n\n\t \n\tif (get_num_contig_pages(0, pages, n) != n) {\n\t\tret = -EINVAL;\n\t\tgoto e_unpin_memory;\n\t}\n\n\tmemset(&data, 0, sizeof(data));\n\n\toffset = params.guest_uaddr & (PAGE_SIZE - 1);\n\tdata.guest_address = __sme_page_pa(pages[0]) + offset;\n\tdata.guest_len = params.guest_len;\n\n\tblob = psp_copy_user_blob(params.trans_uaddr, params.trans_len);\n\tif (IS_ERR(blob)) {\n\t\tret = PTR_ERR(blob);\n\t\tgoto e_unpin_memory;\n\t}\n\n\tdata.trans_address = __psp_pa(blob);\n\tdata.trans_len = params.trans_len;\n\n\thdr = psp_copy_user_blob(params.hdr_uaddr, params.hdr_len);\n\tif (IS_ERR(hdr)) {\n\t\tret = PTR_ERR(hdr);\n\t\tgoto e_free_blob;\n\t}\n\tdata.hdr_address = __psp_pa(hdr);\n\tdata.hdr_len = params.hdr_len;\n\n\tdata.handle = sev->handle;\n\tret = sev_issue_cmd(kvm, SEV_CMD_LAUNCH_UPDATE_SECRET, &data, &argp->error);\n\n\tkfree(hdr);\n\ne_free_blob:\n\tkfree(blob);\ne_unpin_memory:\n\t \n\tfor (i = 0; i < n; i++) {\n\t\tset_page_dirty_lock(pages[i]);\n\t\tmark_page_accessed(pages[i]);\n\t}\n\tsev_unpin_memory(kvm, pages, n);\n\treturn ret;\n}\n\nstatic int sev_get_attestation_report(struct kvm *kvm, struct kvm_sev_cmd *argp)\n{\n\tvoid __user *report = (void __user *)(uintptr_t)argp->data;\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct sev_data_attestation_report data;\n\tstruct kvm_sev_attestation_report params;\n\tvoid __user *p;\n\tvoid *blob = NULL;\n\tint ret;\n\n\tif (!sev_guest(kvm))\n\t\treturn -ENOTTY;\n\n\tif (copy_from_user(&params, (void __user *)(uintptr_t)argp->data, sizeof(params)))\n\t\treturn -EFAULT;\n\n\tmemset(&data, 0, sizeof(data));\n\n\t \n\tif (!params.len)\n\t\tgoto cmd;\n\n\tp = (void __user *)(uintptr_t)params.uaddr;\n\tif (p) {\n\t\tif (params.len > SEV_FW_BLOB_MAX_SIZE)\n\t\t\treturn -EINVAL;\n\n\t\tblob = kzalloc(params.len, GFP_KERNEL_ACCOUNT);\n\t\tif (!blob)\n\t\t\treturn -ENOMEM;\n\n\t\tdata.address = __psp_pa(blob);\n\t\tdata.len = params.len;\n\t\tmemcpy(data.mnonce, params.mnonce, sizeof(params.mnonce));\n\t}\ncmd:\n\tdata.handle = sev->handle;\n\tret = sev_issue_cmd(kvm, SEV_CMD_ATTESTATION_REPORT, &data, &argp->error);\n\t \n\tif (!params.len)\n\t\tgoto done;\n\n\tif (ret)\n\t\tgoto e_free_blob;\n\n\tif (blob) {\n\t\tif (copy_to_user(p, blob, params.len))\n\t\t\tret = -EFAULT;\n\t}\n\ndone:\n\tparams.len = data.len;\n\tif (copy_to_user(report, &params, sizeof(params)))\n\t\tret = -EFAULT;\ne_free_blob:\n\tkfree(blob);\n\treturn ret;\n}\n\n \nstatic int\n__sev_send_start_query_session_length(struct kvm *kvm, struct kvm_sev_cmd *argp,\n\t\t\t\t      struct kvm_sev_send_start *params)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct sev_data_send_start data;\n\tint ret;\n\n\tmemset(&data, 0, sizeof(data));\n\tdata.handle = sev->handle;\n\tret = sev_issue_cmd(kvm, SEV_CMD_SEND_START, &data, &argp->error);\n\n\tparams->session_len = data.session_len;\n\tif (copy_to_user((void __user *)(uintptr_t)argp->data, params,\n\t\t\t\tsizeof(struct kvm_sev_send_start)))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}\n\nstatic int sev_send_start(struct kvm *kvm, struct kvm_sev_cmd *argp)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct sev_data_send_start data;\n\tstruct kvm_sev_send_start params;\n\tvoid *amd_certs, *session_data;\n\tvoid *pdh_cert, *plat_certs;\n\tint ret;\n\n\tif (!sev_guest(kvm))\n\t\treturn -ENOTTY;\n\n\tif (copy_from_user(&params, (void __user *)(uintptr_t)argp->data,\n\t\t\t\tsizeof(struct kvm_sev_send_start)))\n\t\treturn -EFAULT;\n\n\t \n\tif (!params.session_len)\n\t\treturn __sev_send_start_query_session_length(kvm, argp,\n\t\t\t\t&params);\n\n\t \n\tif (!params.pdh_cert_uaddr || !params.pdh_cert_len ||\n\t    !params.session_uaddr || params.session_len > SEV_FW_BLOB_MAX_SIZE)\n\t\treturn -EINVAL;\n\n\t \n\tsession_data = kzalloc(params.session_len, GFP_KERNEL_ACCOUNT);\n\tif (!session_data)\n\t\treturn -ENOMEM;\n\n\t \n\tpdh_cert = psp_copy_user_blob(params.pdh_cert_uaddr,\n\t\t\t\tparams.pdh_cert_len);\n\tif (IS_ERR(pdh_cert)) {\n\t\tret = PTR_ERR(pdh_cert);\n\t\tgoto e_free_session;\n\t}\n\n\tplat_certs = psp_copy_user_blob(params.plat_certs_uaddr,\n\t\t\t\tparams.plat_certs_len);\n\tif (IS_ERR(plat_certs)) {\n\t\tret = PTR_ERR(plat_certs);\n\t\tgoto e_free_pdh;\n\t}\n\n\tamd_certs = psp_copy_user_blob(params.amd_certs_uaddr,\n\t\t\t\tparams.amd_certs_len);\n\tif (IS_ERR(amd_certs)) {\n\t\tret = PTR_ERR(amd_certs);\n\t\tgoto e_free_plat_cert;\n\t}\n\n\t \n\tmemset(&data, 0, sizeof(data));\n\tdata.pdh_cert_address = __psp_pa(pdh_cert);\n\tdata.pdh_cert_len = params.pdh_cert_len;\n\tdata.plat_certs_address = __psp_pa(plat_certs);\n\tdata.plat_certs_len = params.plat_certs_len;\n\tdata.amd_certs_address = __psp_pa(amd_certs);\n\tdata.amd_certs_len = params.amd_certs_len;\n\tdata.session_address = __psp_pa(session_data);\n\tdata.session_len = params.session_len;\n\tdata.handle = sev->handle;\n\n\tret = sev_issue_cmd(kvm, SEV_CMD_SEND_START, &data, &argp->error);\n\n\tif (!ret && copy_to_user((void __user *)(uintptr_t)params.session_uaddr,\n\t\t\tsession_data, params.session_len)) {\n\t\tret = -EFAULT;\n\t\tgoto e_free_amd_cert;\n\t}\n\n\tparams.policy = data.policy;\n\tparams.session_len = data.session_len;\n\tif (copy_to_user((void __user *)(uintptr_t)argp->data, &params,\n\t\t\t\tsizeof(struct kvm_sev_send_start)))\n\t\tret = -EFAULT;\n\ne_free_amd_cert:\n\tkfree(amd_certs);\ne_free_plat_cert:\n\tkfree(plat_certs);\ne_free_pdh:\n\tkfree(pdh_cert);\ne_free_session:\n\tkfree(session_data);\n\treturn ret;\n}\n\n \nstatic int\n__sev_send_update_data_query_lengths(struct kvm *kvm, struct kvm_sev_cmd *argp,\n\t\t\t\t     struct kvm_sev_send_update_data *params)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct sev_data_send_update_data data;\n\tint ret;\n\n\tmemset(&data, 0, sizeof(data));\n\tdata.handle = sev->handle;\n\tret = sev_issue_cmd(kvm, SEV_CMD_SEND_UPDATE_DATA, &data, &argp->error);\n\n\tparams->hdr_len = data.hdr_len;\n\tparams->trans_len = data.trans_len;\n\n\tif (copy_to_user((void __user *)(uintptr_t)argp->data, params,\n\t\t\t sizeof(struct kvm_sev_send_update_data)))\n\t\tret = -EFAULT;\n\n\treturn ret;\n}\n\nstatic int sev_send_update_data(struct kvm *kvm, struct kvm_sev_cmd *argp)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct sev_data_send_update_data data;\n\tstruct kvm_sev_send_update_data params;\n\tvoid *hdr, *trans_data;\n\tstruct page **guest_page;\n\tunsigned long n;\n\tint ret, offset;\n\n\tif (!sev_guest(kvm))\n\t\treturn -ENOTTY;\n\n\tif (copy_from_user(&params, (void __user *)(uintptr_t)argp->data,\n\t\t\tsizeof(struct kvm_sev_send_update_data)))\n\t\treturn -EFAULT;\n\n\t \n\tif (!params.trans_len || !params.hdr_len)\n\t\treturn __sev_send_update_data_query_lengths(kvm, argp, &params);\n\n\tif (!params.trans_uaddr || !params.guest_uaddr ||\n\t    !params.guest_len || !params.hdr_uaddr)\n\t\treturn -EINVAL;\n\n\t \n\toffset = params.guest_uaddr & (PAGE_SIZE - 1);\n\tif (params.guest_len > PAGE_SIZE || (params.guest_len + offset) > PAGE_SIZE)\n\t\treturn -EINVAL;\n\n\t \n\tguest_page = sev_pin_memory(kvm, params.guest_uaddr & PAGE_MASK,\n\t\t\t\t    PAGE_SIZE, &n, 0);\n\tif (IS_ERR(guest_page))\n\t\treturn PTR_ERR(guest_page);\n\n\t \n\tret = -ENOMEM;\n\thdr = kzalloc(params.hdr_len, GFP_KERNEL_ACCOUNT);\n\tif (!hdr)\n\t\tgoto e_unpin;\n\n\ttrans_data = kzalloc(params.trans_len, GFP_KERNEL_ACCOUNT);\n\tif (!trans_data)\n\t\tgoto e_free_hdr;\n\n\tmemset(&data, 0, sizeof(data));\n\tdata.hdr_address = __psp_pa(hdr);\n\tdata.hdr_len = params.hdr_len;\n\tdata.trans_address = __psp_pa(trans_data);\n\tdata.trans_len = params.trans_len;\n\n\t \n\tdata.guest_address = (page_to_pfn(guest_page[0]) << PAGE_SHIFT) + offset;\n\tdata.guest_address |= sev_me_mask;\n\tdata.guest_len = params.guest_len;\n\tdata.handle = sev->handle;\n\n\tret = sev_issue_cmd(kvm, SEV_CMD_SEND_UPDATE_DATA, &data, &argp->error);\n\n\tif (ret)\n\t\tgoto e_free_trans_data;\n\n\t \n\tif (copy_to_user((void __user *)(uintptr_t)params.trans_uaddr,\n\t\t\t trans_data, params.trans_len)) {\n\t\tret = -EFAULT;\n\t\tgoto e_free_trans_data;\n\t}\n\n\t \n\tif (copy_to_user((void __user *)(uintptr_t)params.hdr_uaddr, hdr,\n\t\t\t params.hdr_len))\n\t\tret = -EFAULT;\n\ne_free_trans_data:\n\tkfree(trans_data);\ne_free_hdr:\n\tkfree(hdr);\ne_unpin:\n\tsev_unpin_memory(kvm, guest_page, n);\n\n\treturn ret;\n}\n\nstatic int sev_send_finish(struct kvm *kvm, struct kvm_sev_cmd *argp)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct sev_data_send_finish data;\n\n\tif (!sev_guest(kvm))\n\t\treturn -ENOTTY;\n\n\tdata.handle = sev->handle;\n\treturn sev_issue_cmd(kvm, SEV_CMD_SEND_FINISH, &data, &argp->error);\n}\n\nstatic int sev_send_cancel(struct kvm *kvm, struct kvm_sev_cmd *argp)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct sev_data_send_cancel data;\n\n\tif (!sev_guest(kvm))\n\t\treturn -ENOTTY;\n\n\tdata.handle = sev->handle;\n\treturn sev_issue_cmd(kvm, SEV_CMD_SEND_CANCEL, &data, &argp->error);\n}\n\nstatic int sev_receive_start(struct kvm *kvm, struct kvm_sev_cmd *argp)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct sev_data_receive_start start;\n\tstruct kvm_sev_receive_start params;\n\tint *error = &argp->error;\n\tvoid *session_data;\n\tvoid *pdh_data;\n\tint ret;\n\n\tif (!sev_guest(kvm))\n\t\treturn -ENOTTY;\n\n\t \n\tif (copy_from_user(&params, (void __user *)(uintptr_t)argp->data,\n\t\t\tsizeof(struct kvm_sev_receive_start)))\n\t\treturn -EFAULT;\n\n\t \n\tif (!params.pdh_uaddr || !params.pdh_len ||\n\t    !params.session_uaddr || !params.session_len)\n\t\treturn -EINVAL;\n\n\tpdh_data = psp_copy_user_blob(params.pdh_uaddr, params.pdh_len);\n\tif (IS_ERR(pdh_data))\n\t\treturn PTR_ERR(pdh_data);\n\n\tsession_data = psp_copy_user_blob(params.session_uaddr,\n\t\t\tparams.session_len);\n\tif (IS_ERR(session_data)) {\n\t\tret = PTR_ERR(session_data);\n\t\tgoto e_free_pdh;\n\t}\n\n\tmemset(&start, 0, sizeof(start));\n\tstart.handle = params.handle;\n\tstart.policy = params.policy;\n\tstart.pdh_cert_address = __psp_pa(pdh_data);\n\tstart.pdh_cert_len = params.pdh_len;\n\tstart.session_address = __psp_pa(session_data);\n\tstart.session_len = params.session_len;\n\n\t \n\tret = __sev_issue_cmd(argp->sev_fd, SEV_CMD_RECEIVE_START, &start,\n\t\t\t\terror);\n\tif (ret)\n\t\tgoto e_free_session;\n\n\t \n\tret = sev_bind_asid(kvm, start.handle, error);\n\tif (ret) {\n\t\tsev_decommission(start.handle);\n\t\tgoto e_free_session;\n\t}\n\n\tparams.handle = start.handle;\n\tif (copy_to_user((void __user *)(uintptr_t)argp->data,\n\t\t\t &params, sizeof(struct kvm_sev_receive_start))) {\n\t\tret = -EFAULT;\n\t\tsev_unbind_asid(kvm, start.handle);\n\t\tgoto e_free_session;\n\t}\n\n    \tsev->handle = start.handle;\n\tsev->fd = argp->sev_fd;\n\ne_free_session:\n\tkfree(session_data);\ne_free_pdh:\n\tkfree(pdh_data);\n\n\treturn ret;\n}\n\nstatic int sev_receive_update_data(struct kvm *kvm, struct kvm_sev_cmd *argp)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct kvm_sev_receive_update_data params;\n\tstruct sev_data_receive_update_data data;\n\tvoid *hdr = NULL, *trans = NULL;\n\tstruct page **guest_page;\n\tunsigned long n;\n\tint ret, offset;\n\n\tif (!sev_guest(kvm))\n\t\treturn -EINVAL;\n\n\tif (copy_from_user(&params, (void __user *)(uintptr_t)argp->data,\n\t\t\tsizeof(struct kvm_sev_receive_update_data)))\n\t\treturn -EFAULT;\n\n\tif (!params.hdr_uaddr || !params.hdr_len ||\n\t    !params.guest_uaddr || !params.guest_len ||\n\t    !params.trans_uaddr || !params.trans_len)\n\t\treturn -EINVAL;\n\n\t \n\toffset = params.guest_uaddr & (PAGE_SIZE - 1);\n\tif (params.guest_len > PAGE_SIZE || (params.guest_len + offset) > PAGE_SIZE)\n\t\treturn -EINVAL;\n\n\thdr = psp_copy_user_blob(params.hdr_uaddr, params.hdr_len);\n\tif (IS_ERR(hdr))\n\t\treturn PTR_ERR(hdr);\n\n\ttrans = psp_copy_user_blob(params.trans_uaddr, params.trans_len);\n\tif (IS_ERR(trans)) {\n\t\tret = PTR_ERR(trans);\n\t\tgoto e_free_hdr;\n\t}\n\n\tmemset(&data, 0, sizeof(data));\n\tdata.hdr_address = __psp_pa(hdr);\n\tdata.hdr_len = params.hdr_len;\n\tdata.trans_address = __psp_pa(trans);\n\tdata.trans_len = params.trans_len;\n\n\t \n\tguest_page = sev_pin_memory(kvm, params.guest_uaddr & PAGE_MASK,\n\t\t\t\t    PAGE_SIZE, &n, 1);\n\tif (IS_ERR(guest_page)) {\n\t\tret = PTR_ERR(guest_page);\n\t\tgoto e_free_trans;\n\t}\n\n\t \n\tsev_clflush_pages(guest_page, n);\n\n\t \n\tdata.guest_address = (page_to_pfn(guest_page[0]) << PAGE_SHIFT) + offset;\n\tdata.guest_address |= sev_me_mask;\n\tdata.guest_len = params.guest_len;\n\tdata.handle = sev->handle;\n\n\tret = sev_issue_cmd(kvm, SEV_CMD_RECEIVE_UPDATE_DATA, &data,\n\t\t\t\t&argp->error);\n\n\tsev_unpin_memory(kvm, guest_page, n);\n\ne_free_trans:\n\tkfree(trans);\ne_free_hdr:\n\tkfree(hdr);\n\n\treturn ret;\n}\n\nstatic int sev_receive_finish(struct kvm *kvm, struct kvm_sev_cmd *argp)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct sev_data_receive_finish data;\n\n\tif (!sev_guest(kvm))\n\t\treturn -ENOTTY;\n\n\tdata.handle = sev->handle;\n\treturn sev_issue_cmd(kvm, SEV_CMD_RECEIVE_FINISH, &data, &argp->error);\n}\n\nstatic bool is_cmd_allowed_from_mirror(u32 cmd_id)\n{\n\t \n\tif (cmd_id == KVM_SEV_LAUNCH_UPDATE_VMSA ||\n\t    cmd_id == KVM_SEV_GUEST_STATUS || cmd_id == KVM_SEV_DBG_DECRYPT ||\n\t    cmd_id == KVM_SEV_DBG_ENCRYPT)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic int sev_lock_two_vms(struct kvm *dst_kvm, struct kvm *src_kvm)\n{\n\tstruct kvm_sev_info *dst_sev = &to_kvm_svm(dst_kvm)->sev_info;\n\tstruct kvm_sev_info *src_sev = &to_kvm_svm(src_kvm)->sev_info;\n\tint r = -EBUSY;\n\n\tif (dst_kvm == src_kvm)\n\t\treturn -EINVAL;\n\n\t \n\tif (atomic_cmpxchg_acquire(&dst_sev->migration_in_progress, 0, 1))\n\t\treturn -EBUSY;\n\n\tif (atomic_cmpxchg_acquire(&src_sev->migration_in_progress, 0, 1))\n\t\tgoto release_dst;\n\n\tr = -EINTR;\n\tif (mutex_lock_killable(&dst_kvm->lock))\n\t\tgoto release_src;\n\tif (mutex_lock_killable_nested(&src_kvm->lock, SINGLE_DEPTH_NESTING))\n\t\tgoto unlock_dst;\n\treturn 0;\n\nunlock_dst:\n\tmutex_unlock(&dst_kvm->lock);\nrelease_src:\n\tatomic_set_release(&src_sev->migration_in_progress, 0);\nrelease_dst:\n\tatomic_set_release(&dst_sev->migration_in_progress, 0);\n\treturn r;\n}\n\nstatic void sev_unlock_two_vms(struct kvm *dst_kvm, struct kvm *src_kvm)\n{\n\tstruct kvm_sev_info *dst_sev = &to_kvm_svm(dst_kvm)->sev_info;\n\tstruct kvm_sev_info *src_sev = &to_kvm_svm(src_kvm)->sev_info;\n\n\tmutex_unlock(&dst_kvm->lock);\n\tmutex_unlock(&src_kvm->lock);\n\tatomic_set_release(&dst_sev->migration_in_progress, 0);\n\tatomic_set_release(&src_sev->migration_in_progress, 0);\n}\n\n \nenum sev_migration_role {\n\tSEV_MIGRATION_SOURCE = 0,\n\tSEV_MIGRATION_TARGET,\n\tSEV_NR_MIGRATION_ROLES,\n};\n\nstatic int sev_lock_vcpus_for_migration(struct kvm *kvm,\n\t\t\t\t\tenum sev_migration_role role)\n{\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i, j;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tif (mutex_lock_killable_nested(&vcpu->mutex, role))\n\t\t\tgoto out_unlock;\n\n#ifdef CONFIG_PROVE_LOCKING\n\t\tif (!i)\n\t\t\t \n\t\t\trole = SEV_NR_MIGRATION_ROLES;\n\t\telse\n\t\t\tmutex_release(&vcpu->mutex.dep_map, _THIS_IP_);\n#endif\n\t}\n\n\treturn 0;\n\nout_unlock:\n\n\tkvm_for_each_vcpu(j, vcpu, kvm) {\n\t\tif (i == j)\n\t\t\tbreak;\n\n#ifdef CONFIG_PROVE_LOCKING\n\t\tif (j)\n\t\t\tmutex_acquire(&vcpu->mutex.dep_map, role, 0, _THIS_IP_);\n#endif\n\n\t\tmutex_unlock(&vcpu->mutex);\n\t}\n\treturn -EINTR;\n}\n\nstatic void sev_unlock_vcpus_for_migration(struct kvm *kvm)\n{\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\tbool first = true;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tif (first)\n\t\t\tfirst = false;\n\t\telse\n\t\t\tmutex_acquire(&vcpu->mutex.dep_map,\n\t\t\t\t      SEV_NR_MIGRATION_ROLES, 0, _THIS_IP_);\n\n\t\tmutex_unlock(&vcpu->mutex);\n\t}\n}\n\nstatic void sev_migrate_from(struct kvm *dst_kvm, struct kvm *src_kvm)\n{\n\tstruct kvm_sev_info *dst = &to_kvm_svm(dst_kvm)->sev_info;\n\tstruct kvm_sev_info *src = &to_kvm_svm(src_kvm)->sev_info;\n\tstruct kvm_vcpu *dst_vcpu, *src_vcpu;\n\tstruct vcpu_svm *dst_svm, *src_svm;\n\tstruct kvm_sev_info *mirror;\n\tunsigned long i;\n\n\tdst->active = true;\n\tdst->asid = src->asid;\n\tdst->handle = src->handle;\n\tdst->pages_locked = src->pages_locked;\n\tdst->enc_context_owner = src->enc_context_owner;\n\tdst->es_active = src->es_active;\n\n\tsrc->asid = 0;\n\tsrc->active = false;\n\tsrc->handle = 0;\n\tsrc->pages_locked = 0;\n\tsrc->enc_context_owner = NULL;\n\tsrc->es_active = false;\n\n\tlist_cut_before(&dst->regions_list, &src->regions_list, &src->regions_list);\n\n\t \n\tlist_cut_before(&dst->mirror_vms, &src->mirror_vms, &src->mirror_vms);\n\tlist_for_each_entry(mirror, &dst->mirror_vms, mirror_entry) {\n\t\tkvm_get_kvm(dst_kvm);\n\t\tkvm_put_kvm(src_kvm);\n\t\tmirror->enc_context_owner = dst_kvm;\n\t}\n\n\t \n\tif (is_mirroring_enc_context(dst_kvm)) {\n\t\tstruct kvm_sev_info *owner_sev_info =\n\t\t\t&to_kvm_svm(dst->enc_context_owner)->sev_info;\n\n\t\tlist_del(&src->mirror_entry);\n\t\tlist_add_tail(&dst->mirror_entry, &owner_sev_info->mirror_vms);\n\t}\n\n\tkvm_for_each_vcpu(i, dst_vcpu, dst_kvm) {\n\t\tdst_svm = to_svm(dst_vcpu);\n\n\t\tsev_init_vmcb(dst_svm);\n\n\t\tif (!dst->es_active)\n\t\t\tcontinue;\n\n\t\t \n\t\tsrc_vcpu = kvm_get_vcpu(src_kvm, i);\n\t\tsrc_svm = to_svm(src_vcpu);\n\n\t\t \n\t\tmemcpy(&dst_svm->sev_es, &src_svm->sev_es, sizeof(src_svm->sev_es));\n\t\tdst_svm->vmcb->control.ghcb_gpa = src_svm->vmcb->control.ghcb_gpa;\n\t\tdst_svm->vmcb->control.vmsa_pa = src_svm->vmcb->control.vmsa_pa;\n\t\tdst_vcpu->arch.guest_state_protected = true;\n\n\t\tmemset(&src_svm->sev_es, 0, sizeof(src_svm->sev_es));\n\t\tsrc_svm->vmcb->control.ghcb_gpa = INVALID_PAGE;\n\t\tsrc_svm->vmcb->control.vmsa_pa = INVALID_PAGE;\n\t\tsrc_vcpu->arch.guest_state_protected = false;\n\t}\n}\n\nstatic int sev_check_source_vcpus(struct kvm *dst, struct kvm *src)\n{\n\tstruct kvm_vcpu *src_vcpu;\n\tunsigned long i;\n\n\tif (!sev_es_guest(src))\n\t\treturn 0;\n\n\tif (atomic_read(&src->online_vcpus) != atomic_read(&dst->online_vcpus))\n\t\treturn -EINVAL;\n\n\tkvm_for_each_vcpu(i, src_vcpu, src) {\n\t\tif (!src_vcpu->arch.guest_state_protected)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nint sev_vm_move_enc_context_from(struct kvm *kvm, unsigned int source_fd)\n{\n\tstruct kvm_sev_info *dst_sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct kvm_sev_info *src_sev, *cg_cleanup_sev;\n\tstruct fd f = fdget(source_fd);\n\tstruct kvm *source_kvm;\n\tbool charged = false;\n\tint ret;\n\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tif (!file_is_kvm(f.file)) {\n\t\tret = -EBADF;\n\t\tgoto out_fput;\n\t}\n\n\tsource_kvm = f.file->private_data;\n\tret = sev_lock_two_vms(kvm, source_kvm);\n\tif (ret)\n\t\tgoto out_fput;\n\n\tif (sev_guest(kvm) || !sev_guest(source_kvm)) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\tsrc_sev = &to_kvm_svm(source_kvm)->sev_info;\n\n\tdst_sev->misc_cg = get_current_misc_cg();\n\tcg_cleanup_sev = dst_sev;\n\tif (dst_sev->misc_cg != src_sev->misc_cg) {\n\t\tret = sev_misc_cg_try_charge(dst_sev);\n\t\tif (ret)\n\t\t\tgoto out_dst_cgroup;\n\t\tcharged = true;\n\t}\n\n\tret = sev_lock_vcpus_for_migration(kvm, SEV_MIGRATION_SOURCE);\n\tif (ret)\n\t\tgoto out_dst_cgroup;\n\tret = sev_lock_vcpus_for_migration(source_kvm, SEV_MIGRATION_TARGET);\n\tif (ret)\n\t\tgoto out_dst_vcpu;\n\n\tret = sev_check_source_vcpus(kvm, source_kvm);\n\tif (ret)\n\t\tgoto out_source_vcpu;\n\n\tsev_migrate_from(kvm, source_kvm);\n\tkvm_vm_dead(source_kvm);\n\tcg_cleanup_sev = src_sev;\n\tret = 0;\n\nout_source_vcpu:\n\tsev_unlock_vcpus_for_migration(source_kvm);\nout_dst_vcpu:\n\tsev_unlock_vcpus_for_migration(kvm);\nout_dst_cgroup:\n\t \n\tif (charged)\n\t\tsev_misc_cg_uncharge(cg_cleanup_sev);\n\tput_misc_cg(cg_cleanup_sev->misc_cg);\n\tcg_cleanup_sev->misc_cg = NULL;\nout_unlock:\n\tsev_unlock_two_vms(kvm, source_kvm);\nout_fput:\n\tfdput(f);\n\treturn ret;\n}\n\nint sev_mem_enc_ioctl(struct kvm *kvm, void __user *argp)\n{\n\tstruct kvm_sev_cmd sev_cmd;\n\tint r;\n\n\tif (!sev_enabled)\n\t\treturn -ENOTTY;\n\n\tif (!argp)\n\t\treturn 0;\n\n\tif (copy_from_user(&sev_cmd, argp, sizeof(struct kvm_sev_cmd)))\n\t\treturn -EFAULT;\n\n\tmutex_lock(&kvm->lock);\n\n\t \n\tif (is_mirroring_enc_context(kvm) &&\n\t    !is_cmd_allowed_from_mirror(sev_cmd.id)) {\n\t\tr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tswitch (sev_cmd.id) {\n\tcase KVM_SEV_ES_INIT:\n\t\tif (!sev_es_enabled) {\n\t\t\tr = -ENOTTY;\n\t\t\tgoto out;\n\t\t}\n\t\tfallthrough;\n\tcase KVM_SEV_INIT:\n\t\tr = sev_guest_init(kvm, &sev_cmd);\n\t\tbreak;\n\tcase KVM_SEV_LAUNCH_START:\n\t\tr = sev_launch_start(kvm, &sev_cmd);\n\t\tbreak;\n\tcase KVM_SEV_LAUNCH_UPDATE_DATA:\n\t\tr = sev_launch_update_data(kvm, &sev_cmd);\n\t\tbreak;\n\tcase KVM_SEV_LAUNCH_UPDATE_VMSA:\n\t\tr = sev_launch_update_vmsa(kvm, &sev_cmd);\n\t\tbreak;\n\tcase KVM_SEV_LAUNCH_MEASURE:\n\t\tr = sev_launch_measure(kvm, &sev_cmd);\n\t\tbreak;\n\tcase KVM_SEV_LAUNCH_FINISH:\n\t\tr = sev_launch_finish(kvm, &sev_cmd);\n\t\tbreak;\n\tcase KVM_SEV_GUEST_STATUS:\n\t\tr = sev_guest_status(kvm, &sev_cmd);\n\t\tbreak;\n\tcase KVM_SEV_DBG_DECRYPT:\n\t\tr = sev_dbg_crypt(kvm, &sev_cmd, true);\n\t\tbreak;\n\tcase KVM_SEV_DBG_ENCRYPT:\n\t\tr = sev_dbg_crypt(kvm, &sev_cmd, false);\n\t\tbreak;\n\tcase KVM_SEV_LAUNCH_SECRET:\n\t\tr = sev_launch_secret(kvm, &sev_cmd);\n\t\tbreak;\n\tcase KVM_SEV_GET_ATTESTATION_REPORT:\n\t\tr = sev_get_attestation_report(kvm, &sev_cmd);\n\t\tbreak;\n\tcase KVM_SEV_SEND_START:\n\t\tr = sev_send_start(kvm, &sev_cmd);\n\t\tbreak;\n\tcase KVM_SEV_SEND_UPDATE_DATA:\n\t\tr = sev_send_update_data(kvm, &sev_cmd);\n\t\tbreak;\n\tcase KVM_SEV_SEND_FINISH:\n\t\tr = sev_send_finish(kvm, &sev_cmd);\n\t\tbreak;\n\tcase KVM_SEV_SEND_CANCEL:\n\t\tr = sev_send_cancel(kvm, &sev_cmd);\n\t\tbreak;\n\tcase KVM_SEV_RECEIVE_START:\n\t\tr = sev_receive_start(kvm, &sev_cmd);\n\t\tbreak;\n\tcase KVM_SEV_RECEIVE_UPDATE_DATA:\n\t\tr = sev_receive_update_data(kvm, &sev_cmd);\n\t\tbreak;\n\tcase KVM_SEV_RECEIVE_FINISH:\n\t\tr = sev_receive_finish(kvm, &sev_cmd);\n\t\tbreak;\n\tdefault:\n\t\tr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tif (copy_to_user(argp, &sev_cmd, sizeof(struct kvm_sev_cmd)))\n\t\tr = -EFAULT;\n\nout:\n\tmutex_unlock(&kvm->lock);\n\treturn r;\n}\n\nint sev_mem_enc_register_region(struct kvm *kvm,\n\t\t\t\tstruct kvm_enc_region *range)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct enc_region *region;\n\tint ret = 0;\n\n\tif (!sev_guest(kvm))\n\t\treturn -ENOTTY;\n\n\t \n\tif (is_mirroring_enc_context(kvm))\n\t\treturn -EINVAL;\n\n\tif (range->addr > ULONG_MAX || range->size > ULONG_MAX)\n\t\treturn -EINVAL;\n\n\tregion = kzalloc(sizeof(*region), GFP_KERNEL_ACCOUNT);\n\tif (!region)\n\t\treturn -ENOMEM;\n\n\tmutex_lock(&kvm->lock);\n\tregion->pages = sev_pin_memory(kvm, range->addr, range->size, &region->npages, 1);\n\tif (IS_ERR(region->pages)) {\n\t\tret = PTR_ERR(region->pages);\n\t\tmutex_unlock(&kvm->lock);\n\t\tgoto e_free;\n\t}\n\n\tregion->uaddr = range->addr;\n\tregion->size = range->size;\n\n\tlist_add_tail(&region->list, &sev->regions_list);\n\tmutex_unlock(&kvm->lock);\n\n\t \n\tsev_clflush_pages(region->pages, region->npages);\n\n\treturn ret;\n\ne_free:\n\tkfree(region);\n\treturn ret;\n}\n\nstatic struct enc_region *\nfind_enc_region(struct kvm *kvm, struct kvm_enc_region *range)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct list_head *head = &sev->regions_list;\n\tstruct enc_region *i;\n\n\tlist_for_each_entry(i, head, list) {\n\t\tif (i->uaddr == range->addr &&\n\t\t    i->size == range->size)\n\t\t\treturn i;\n\t}\n\n\treturn NULL;\n}\n\nstatic void __unregister_enc_region_locked(struct kvm *kvm,\n\t\t\t\t\t   struct enc_region *region)\n{\n\tsev_unpin_memory(kvm, region->pages, region->npages);\n\tlist_del(&region->list);\n\tkfree(region);\n}\n\nint sev_mem_enc_unregister_region(struct kvm *kvm,\n\t\t\t\t  struct kvm_enc_region *range)\n{\n\tstruct enc_region *region;\n\tint ret;\n\n\t \n\tif (is_mirroring_enc_context(kvm))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&kvm->lock);\n\n\tif (!sev_guest(kvm)) {\n\t\tret = -ENOTTY;\n\t\tgoto failed;\n\t}\n\n\tregion = find_enc_region(kvm, range);\n\tif (!region) {\n\t\tret = -EINVAL;\n\t\tgoto failed;\n\t}\n\n\t \n\twbinvd_on_all_cpus();\n\n\t__unregister_enc_region_locked(kvm, region);\n\n\tmutex_unlock(&kvm->lock);\n\treturn 0;\n\nfailed:\n\tmutex_unlock(&kvm->lock);\n\treturn ret;\n}\n\nint sev_vm_copy_enc_context_from(struct kvm *kvm, unsigned int source_fd)\n{\n\tstruct fd f = fdget(source_fd);\n\tstruct kvm *source_kvm;\n\tstruct kvm_sev_info *source_sev, *mirror_sev;\n\tint ret;\n\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\tif (!file_is_kvm(f.file)) {\n\t\tret = -EBADF;\n\t\tgoto e_source_fput;\n\t}\n\n\tsource_kvm = f.file->private_data;\n\tret = sev_lock_two_vms(kvm, source_kvm);\n\tif (ret)\n\t\tgoto e_source_fput;\n\n\t \n\tif (sev_guest(kvm) || !sev_guest(source_kvm) ||\n\t    is_mirroring_enc_context(source_kvm) || kvm->created_vcpus) {\n\t\tret = -EINVAL;\n\t\tgoto e_unlock;\n\t}\n\n\t \n\tsource_sev = &to_kvm_svm(source_kvm)->sev_info;\n\tkvm_get_kvm(source_kvm);\n\tmirror_sev = &to_kvm_svm(kvm)->sev_info;\n\tlist_add_tail(&mirror_sev->mirror_entry, &source_sev->mirror_vms);\n\n\t \n\tmirror_sev->enc_context_owner = source_kvm;\n\tmirror_sev->active = true;\n\tmirror_sev->asid = source_sev->asid;\n\tmirror_sev->fd = source_sev->fd;\n\tmirror_sev->es_active = source_sev->es_active;\n\tmirror_sev->handle = source_sev->handle;\n\tINIT_LIST_HEAD(&mirror_sev->regions_list);\n\tINIT_LIST_HEAD(&mirror_sev->mirror_vms);\n\tret = 0;\n\n\t \n\ne_unlock:\n\tsev_unlock_two_vms(kvm, source_kvm);\ne_source_fput:\n\tfdput(f);\n\treturn ret;\n}\n\nvoid sev_vm_destroy(struct kvm *kvm)\n{\n\tstruct kvm_sev_info *sev = &to_kvm_svm(kvm)->sev_info;\n\tstruct list_head *head = &sev->regions_list;\n\tstruct list_head *pos, *q;\n\n\tif (!sev_guest(kvm))\n\t\treturn;\n\n\tWARN_ON(!list_empty(&sev->mirror_vms));\n\n\t \n\tif (is_mirroring_enc_context(kvm)) {\n\t\tstruct kvm *owner_kvm = sev->enc_context_owner;\n\n\t\tmutex_lock(&owner_kvm->lock);\n\t\tlist_del(&sev->mirror_entry);\n\t\tmutex_unlock(&owner_kvm->lock);\n\t\tkvm_put_kvm(owner_kvm);\n\t\treturn;\n\t}\n\n\t \n\twbinvd_on_all_cpus();\n\n\t \n\tif (!list_empty(head)) {\n\t\tlist_for_each_safe(pos, q, head) {\n\t\t\t__unregister_enc_region_locked(kvm,\n\t\t\t\tlist_entry(pos, struct enc_region, list));\n\t\t\tcond_resched();\n\t\t}\n\t}\n\n\tsev_unbind_asid(kvm, sev->handle);\n\tsev_asid_free(sev);\n}\n\nvoid __init sev_set_cpu_caps(void)\n{\n\tif (!sev_enabled)\n\t\tkvm_cpu_cap_clear(X86_FEATURE_SEV);\n\tif (!sev_es_enabled)\n\t\tkvm_cpu_cap_clear(X86_FEATURE_SEV_ES);\n}\n\nvoid __init sev_hardware_setup(void)\n{\n#ifdef CONFIG_KVM_AMD_SEV\n\tunsigned int eax, ebx, ecx, edx, sev_asid_count, sev_es_asid_count;\n\tbool sev_es_supported = false;\n\tbool sev_supported = false;\n\n\tif (!sev_enabled || !npt_enabled || !nrips)\n\t\tgoto out;\n\n\t \n\tif (!boot_cpu_has(X86_FEATURE_SEV) ||\n\t    WARN_ON_ONCE(!boot_cpu_has(X86_FEATURE_DECODEASSISTS)))\n\t\tgoto out;\n\n\t \n\tcpuid(0x8000001f, &eax, &ebx, &ecx, &edx);\n\n\t \n\tsev_enc_bit = ebx & 0x3f;\n\n\t \n\tmax_sev_asid = ecx;\n\tif (!max_sev_asid)\n\t\tgoto out;\n\n\t \n\tmin_sev_asid = edx;\n\tsev_me_mask = 1UL << (ebx & 0x3f);\n\n\t \n\tnr_asids = max_sev_asid + 1;\n\tsev_asid_bitmap = bitmap_zalloc(nr_asids, GFP_KERNEL);\n\tif (!sev_asid_bitmap)\n\t\tgoto out;\n\n\tsev_reclaim_asid_bitmap = bitmap_zalloc(nr_asids, GFP_KERNEL);\n\tif (!sev_reclaim_asid_bitmap) {\n\t\tbitmap_free(sev_asid_bitmap);\n\t\tsev_asid_bitmap = NULL;\n\t\tgoto out;\n\t}\n\n\tsev_asid_count = max_sev_asid - min_sev_asid + 1;\n\tWARN_ON_ONCE(misc_cg_set_capacity(MISC_CG_RES_SEV, sev_asid_count));\n\tsev_supported = true;\n\n\t \n\tif (!sev_es_enabled)\n\t\tgoto out;\n\n\t \n\tif (!enable_mmio_caching)\n\t\tgoto out;\n\n\t \n\tif (!boot_cpu_has(X86_FEATURE_SEV_ES))\n\t\tgoto out;\n\n\t \n\tif (min_sev_asid == 1)\n\t\tgoto out;\n\n\tsev_es_asid_count = min_sev_asid - 1;\n\tWARN_ON_ONCE(misc_cg_set_capacity(MISC_CG_RES_SEV_ES, sev_es_asid_count));\n\tsev_es_supported = true;\n\nout:\n\tif (boot_cpu_has(X86_FEATURE_SEV))\n\t\tpr_info(\"SEV %s (ASIDs %u - %u)\\n\",\n\t\t\tsev_supported ? \"enabled\" : \"disabled\",\n\t\t\tmin_sev_asid, max_sev_asid);\n\tif (boot_cpu_has(X86_FEATURE_SEV_ES))\n\t\tpr_info(\"SEV-ES %s (ASIDs %u - %u)\\n\",\n\t\t\tsev_es_supported ? \"enabled\" : \"disabled\",\n\t\t\tmin_sev_asid > 1 ? 1 : 0, min_sev_asid - 1);\n\n\tsev_enabled = sev_supported;\n\tsev_es_enabled = sev_es_supported;\n\tif (!sev_es_enabled || !cpu_feature_enabled(X86_FEATURE_DEBUG_SWAP) ||\n\t    !cpu_feature_enabled(X86_FEATURE_NO_NESTED_DATA_BP))\n\t\tsev_es_debug_swap_enabled = false;\n#endif\n}\n\nvoid sev_hardware_unsetup(void)\n{\n\tif (!sev_enabled)\n\t\treturn;\n\n\t \n\tsev_flush_asids(1, max_sev_asid);\n\n\tbitmap_free(sev_asid_bitmap);\n\tbitmap_free(sev_reclaim_asid_bitmap);\n\n\tmisc_cg_set_capacity(MISC_CG_RES_SEV, 0);\n\tmisc_cg_set_capacity(MISC_CG_RES_SEV_ES, 0);\n}\n\nint sev_cpu_init(struct svm_cpu_data *sd)\n{\n\tif (!sev_enabled)\n\t\treturn 0;\n\n\tsd->sev_vmcbs = kcalloc(nr_asids, sizeof(void *), GFP_KERNEL);\n\tif (!sd->sev_vmcbs)\n\t\treturn -ENOMEM;\n\n\treturn 0;\n}\n\n \nstatic void sev_flush_encrypted_page(struct kvm_vcpu *vcpu, void *va)\n{\n\tint asid = to_kvm_svm(vcpu->kvm)->sev_info.asid;\n\n\t \n\tunsigned long addr = (unsigned long)va;\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_SME_COHERENT)) {\n\t\tclflush_cache_range(va, PAGE_SIZE);\n\t\treturn;\n\t}\n\n\t \n\tif (WARN_ON_ONCE(wrmsrl_safe(MSR_AMD64_VM_PAGE_FLUSH, addr | asid)))\n\t\tgoto do_wbinvd;\n\n\treturn;\n\ndo_wbinvd:\n\twbinvd_on_all_cpus();\n}\n\nvoid sev_guest_memory_reclaimed(struct kvm *kvm)\n{\n\tif (!sev_guest(kvm))\n\t\treturn;\n\n\twbinvd_on_all_cpus();\n}\n\nvoid sev_free_vcpu(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm;\n\n\tif (!sev_es_guest(vcpu->kvm))\n\t\treturn;\n\n\tsvm = to_svm(vcpu);\n\n\tif (vcpu->arch.guest_state_protected)\n\t\tsev_flush_encrypted_page(vcpu, svm->sev_es.vmsa);\n\n\t__free_page(virt_to_page(svm->sev_es.vmsa));\n\n\tif (svm->sev_es.ghcb_sa_free)\n\t\tkvfree(svm->sev_es.ghcb_sa);\n}\n\nstatic void dump_ghcb(struct vcpu_svm *svm)\n{\n\tstruct ghcb *ghcb = svm->sev_es.ghcb;\n\tunsigned int nbits;\n\n\t \n\tif (!dump_invalid_vmcb) {\n\t\tpr_warn_ratelimited(\"set kvm_amd.dump_invalid_vmcb=1 to dump internal KVM state.\\n\");\n\t\treturn;\n\t}\n\n\tnbits = sizeof(ghcb->save.valid_bitmap) * 8;\n\n\tpr_err(\"GHCB (GPA=%016llx):\\n\", svm->vmcb->control.ghcb_gpa);\n\tpr_err(\"%-20s%016llx is_valid: %u\\n\", \"sw_exit_code\",\n\t       ghcb->save.sw_exit_code, ghcb_sw_exit_code_is_valid(ghcb));\n\tpr_err(\"%-20s%016llx is_valid: %u\\n\", \"sw_exit_info_1\",\n\t       ghcb->save.sw_exit_info_1, ghcb_sw_exit_info_1_is_valid(ghcb));\n\tpr_err(\"%-20s%016llx is_valid: %u\\n\", \"sw_exit_info_2\",\n\t       ghcb->save.sw_exit_info_2, ghcb_sw_exit_info_2_is_valid(ghcb));\n\tpr_err(\"%-20s%016llx is_valid: %u\\n\", \"sw_scratch\",\n\t       ghcb->save.sw_scratch, ghcb_sw_scratch_is_valid(ghcb));\n\tpr_err(\"%-20s%*pb\\n\", \"valid_bitmap\", nbits, ghcb->save.valid_bitmap);\n}\n\nstatic void sev_es_sync_to_ghcb(struct vcpu_svm *svm)\n{\n\tstruct kvm_vcpu *vcpu = &svm->vcpu;\n\tstruct ghcb *ghcb = svm->sev_es.ghcb;\n\n\t \n\tghcb_set_rax(ghcb, vcpu->arch.regs[VCPU_REGS_RAX]);\n\tghcb_set_rbx(ghcb, vcpu->arch.regs[VCPU_REGS_RBX]);\n\tghcb_set_rcx(ghcb, vcpu->arch.regs[VCPU_REGS_RCX]);\n\tghcb_set_rdx(ghcb, vcpu->arch.regs[VCPU_REGS_RDX]);\n}\n\nstatic void sev_es_sync_from_ghcb(struct vcpu_svm *svm)\n{\n\tstruct vmcb_control_area *control = &svm->vmcb->control;\n\tstruct kvm_vcpu *vcpu = &svm->vcpu;\n\tstruct ghcb *ghcb = svm->sev_es.ghcb;\n\tu64 exit_code;\n\n\t \n\tmemset(vcpu->arch.regs, 0, sizeof(vcpu->arch.regs));\n\n\tBUILD_BUG_ON(sizeof(svm->sev_es.valid_bitmap) != sizeof(ghcb->save.valid_bitmap));\n\tmemcpy(&svm->sev_es.valid_bitmap, &ghcb->save.valid_bitmap, sizeof(ghcb->save.valid_bitmap));\n\n\tvcpu->arch.regs[VCPU_REGS_RAX] = kvm_ghcb_get_rax_if_valid(svm, ghcb);\n\tvcpu->arch.regs[VCPU_REGS_RBX] = kvm_ghcb_get_rbx_if_valid(svm, ghcb);\n\tvcpu->arch.regs[VCPU_REGS_RCX] = kvm_ghcb_get_rcx_if_valid(svm, ghcb);\n\tvcpu->arch.regs[VCPU_REGS_RDX] = kvm_ghcb_get_rdx_if_valid(svm, ghcb);\n\tvcpu->arch.regs[VCPU_REGS_RSI] = kvm_ghcb_get_rsi_if_valid(svm, ghcb);\n\n\tsvm->vmcb->save.cpl = kvm_ghcb_get_cpl_if_valid(svm, ghcb);\n\n\tif (kvm_ghcb_xcr0_is_valid(svm)) {\n\t\tvcpu->arch.xcr0 = ghcb_get_xcr0(ghcb);\n\t\tkvm_update_cpuid_runtime(vcpu);\n\t}\n\n\t \n\texit_code = ghcb_get_sw_exit_code(ghcb);\n\tcontrol->exit_code = lower_32_bits(exit_code);\n\tcontrol->exit_code_hi = upper_32_bits(exit_code);\n\tcontrol->exit_info_1 = ghcb_get_sw_exit_info_1(ghcb);\n\tcontrol->exit_info_2 = ghcb_get_sw_exit_info_2(ghcb);\n\tsvm->sev_es.sw_scratch = kvm_ghcb_get_sw_scratch_if_valid(svm, ghcb);\n\n\t \n\tmemset(ghcb->save.valid_bitmap, 0, sizeof(ghcb->save.valid_bitmap));\n}\n\nstatic u64 kvm_ghcb_get_sw_exit_code(struct vmcb_control_area *control)\n{\n\treturn (((u64)control->exit_code_hi) << 32) | control->exit_code;\n}\n\nstatic int sev_es_validate_vmgexit(struct vcpu_svm *svm)\n{\n\tstruct vmcb_control_area *control = &svm->vmcb->control;\n\tstruct kvm_vcpu *vcpu = &svm->vcpu;\n\tu64 exit_code;\n\tu64 reason;\n\n\t \n\texit_code = kvm_ghcb_get_sw_exit_code(control);\n\n\t \n\tif (svm->sev_es.ghcb->ghcb_usage) {\n\t\treason = GHCB_ERR_INVALID_USAGE;\n\t\tgoto vmgexit_err;\n\t}\n\n\treason = GHCB_ERR_MISSING_INPUT;\n\n\tif (!kvm_ghcb_sw_exit_code_is_valid(svm) ||\n\t    !kvm_ghcb_sw_exit_info_1_is_valid(svm) ||\n\t    !kvm_ghcb_sw_exit_info_2_is_valid(svm))\n\t\tgoto vmgexit_err;\n\n\tswitch (exit_code) {\n\tcase SVM_EXIT_READ_DR7:\n\t\tbreak;\n\tcase SVM_EXIT_WRITE_DR7:\n\t\tif (!kvm_ghcb_rax_is_valid(svm))\n\t\t\tgoto vmgexit_err;\n\t\tbreak;\n\tcase SVM_EXIT_RDTSC:\n\t\tbreak;\n\tcase SVM_EXIT_RDPMC:\n\t\tif (!kvm_ghcb_rcx_is_valid(svm))\n\t\t\tgoto vmgexit_err;\n\t\tbreak;\n\tcase SVM_EXIT_CPUID:\n\t\tif (!kvm_ghcb_rax_is_valid(svm) ||\n\t\t    !kvm_ghcb_rcx_is_valid(svm))\n\t\t\tgoto vmgexit_err;\n\t\tif (vcpu->arch.regs[VCPU_REGS_RAX] == 0xd)\n\t\t\tif (!kvm_ghcb_xcr0_is_valid(svm))\n\t\t\t\tgoto vmgexit_err;\n\t\tbreak;\n\tcase SVM_EXIT_INVD:\n\t\tbreak;\n\tcase SVM_EXIT_IOIO:\n\t\tif (control->exit_info_1 & SVM_IOIO_STR_MASK) {\n\t\t\tif (!kvm_ghcb_sw_scratch_is_valid(svm))\n\t\t\t\tgoto vmgexit_err;\n\t\t} else {\n\t\t\tif (!(control->exit_info_1 & SVM_IOIO_TYPE_MASK))\n\t\t\t\tif (!kvm_ghcb_rax_is_valid(svm))\n\t\t\t\t\tgoto vmgexit_err;\n\t\t}\n\t\tbreak;\n\tcase SVM_EXIT_MSR:\n\t\tif (!kvm_ghcb_rcx_is_valid(svm))\n\t\t\tgoto vmgexit_err;\n\t\tif (control->exit_info_1) {\n\t\t\tif (!kvm_ghcb_rax_is_valid(svm) ||\n\t\t\t    !kvm_ghcb_rdx_is_valid(svm))\n\t\t\t\tgoto vmgexit_err;\n\t\t}\n\t\tbreak;\n\tcase SVM_EXIT_VMMCALL:\n\t\tif (!kvm_ghcb_rax_is_valid(svm) ||\n\t\t    !kvm_ghcb_cpl_is_valid(svm))\n\t\t\tgoto vmgexit_err;\n\t\tbreak;\n\tcase SVM_EXIT_RDTSCP:\n\t\tbreak;\n\tcase SVM_EXIT_WBINVD:\n\t\tbreak;\n\tcase SVM_EXIT_MONITOR:\n\t\tif (!kvm_ghcb_rax_is_valid(svm) ||\n\t\t    !kvm_ghcb_rcx_is_valid(svm) ||\n\t\t    !kvm_ghcb_rdx_is_valid(svm))\n\t\t\tgoto vmgexit_err;\n\t\tbreak;\n\tcase SVM_EXIT_MWAIT:\n\t\tif (!kvm_ghcb_rax_is_valid(svm) ||\n\t\t    !kvm_ghcb_rcx_is_valid(svm))\n\t\t\tgoto vmgexit_err;\n\t\tbreak;\n\tcase SVM_VMGEXIT_MMIO_READ:\n\tcase SVM_VMGEXIT_MMIO_WRITE:\n\t\tif (!kvm_ghcb_sw_scratch_is_valid(svm))\n\t\t\tgoto vmgexit_err;\n\t\tbreak;\n\tcase SVM_VMGEXIT_NMI_COMPLETE:\n\tcase SVM_VMGEXIT_AP_HLT_LOOP:\n\tcase SVM_VMGEXIT_AP_JUMP_TABLE:\n\tcase SVM_VMGEXIT_UNSUPPORTED_EVENT:\n\t\tbreak;\n\tdefault:\n\t\treason = GHCB_ERR_INVALID_EVENT;\n\t\tgoto vmgexit_err;\n\t}\n\n\treturn 0;\n\nvmgexit_err:\n\tif (reason == GHCB_ERR_INVALID_USAGE) {\n\t\tvcpu_unimpl(vcpu, \"vmgexit: ghcb usage %#x is not valid\\n\",\n\t\t\t    svm->sev_es.ghcb->ghcb_usage);\n\t} else if (reason == GHCB_ERR_INVALID_EVENT) {\n\t\tvcpu_unimpl(vcpu, \"vmgexit: exit code %#llx is not valid\\n\",\n\t\t\t    exit_code);\n\t} else {\n\t\tvcpu_unimpl(vcpu, \"vmgexit: exit code %#llx input is not valid\\n\",\n\t\t\t    exit_code);\n\t\tdump_ghcb(svm);\n\t}\n\n\tghcb_set_sw_exit_info_1(svm->sev_es.ghcb, 2);\n\tghcb_set_sw_exit_info_2(svm->sev_es.ghcb, reason);\n\n\t \n\treturn 1;\n}\n\nvoid sev_es_unmap_ghcb(struct vcpu_svm *svm)\n{\n\tif (!svm->sev_es.ghcb)\n\t\treturn;\n\n\tif (svm->sev_es.ghcb_sa_free) {\n\t\t \n\t\tif (svm->sev_es.ghcb_sa_sync) {\n\t\t\tkvm_write_guest(svm->vcpu.kvm,\n\t\t\t\t\tsvm->sev_es.sw_scratch,\n\t\t\t\t\tsvm->sev_es.ghcb_sa,\n\t\t\t\t\tsvm->sev_es.ghcb_sa_len);\n\t\t\tsvm->sev_es.ghcb_sa_sync = false;\n\t\t}\n\n\t\tkvfree(svm->sev_es.ghcb_sa);\n\t\tsvm->sev_es.ghcb_sa = NULL;\n\t\tsvm->sev_es.ghcb_sa_free = false;\n\t}\n\n\ttrace_kvm_vmgexit_exit(svm->vcpu.vcpu_id, svm->sev_es.ghcb);\n\n\tsev_es_sync_to_ghcb(svm);\n\n\tkvm_vcpu_unmap(&svm->vcpu, &svm->sev_es.ghcb_map, true);\n\tsvm->sev_es.ghcb = NULL;\n}\n\nvoid pre_sev_run(struct vcpu_svm *svm, int cpu)\n{\n\tstruct svm_cpu_data *sd = per_cpu_ptr(&svm_data, cpu);\n\tint asid = sev_get_asid(svm->vcpu.kvm);\n\n\t \n\tsvm->asid = asid;\n\n\t \n\tif (sd->sev_vmcbs[asid] == svm->vmcb &&\n\t    svm->vcpu.arch.last_vmentry_cpu == cpu)\n\t\treturn;\n\n\tsd->sev_vmcbs[asid] = svm->vmcb;\n\tsvm->vmcb->control.tlb_ctl = TLB_CONTROL_FLUSH_ASID;\n\tvmcb_mark_dirty(svm->vmcb, VMCB_ASID);\n}\n\n#define GHCB_SCRATCH_AREA_LIMIT\t\t(16ULL * PAGE_SIZE)\nstatic int setup_vmgexit_scratch(struct vcpu_svm *svm, bool sync, u64 len)\n{\n\tstruct vmcb_control_area *control = &svm->vmcb->control;\n\tu64 ghcb_scratch_beg, ghcb_scratch_end;\n\tu64 scratch_gpa_beg, scratch_gpa_end;\n\tvoid *scratch_va;\n\n\tscratch_gpa_beg = svm->sev_es.sw_scratch;\n\tif (!scratch_gpa_beg) {\n\t\tpr_err(\"vmgexit: scratch gpa not provided\\n\");\n\t\tgoto e_scratch;\n\t}\n\n\tscratch_gpa_end = scratch_gpa_beg + len;\n\tif (scratch_gpa_end < scratch_gpa_beg) {\n\t\tpr_err(\"vmgexit: scratch length (%#llx) not valid for scratch address (%#llx)\\n\",\n\t\t       len, scratch_gpa_beg);\n\t\tgoto e_scratch;\n\t}\n\n\tif ((scratch_gpa_beg & PAGE_MASK) == control->ghcb_gpa) {\n\t\t \n\t\tghcb_scratch_beg = control->ghcb_gpa +\n\t\t\t\t   offsetof(struct ghcb, shared_buffer);\n\t\tghcb_scratch_end = control->ghcb_gpa +\n\t\t\t\t   offsetof(struct ghcb, reserved_0xff0);\n\n\t\t \n\t\tif (scratch_gpa_beg < ghcb_scratch_beg ||\n\t\t    scratch_gpa_end > ghcb_scratch_end) {\n\t\t\tpr_err(\"vmgexit: scratch area is outside of GHCB shared buffer area (%#llx - %#llx)\\n\",\n\t\t\t       scratch_gpa_beg, scratch_gpa_end);\n\t\t\tgoto e_scratch;\n\t\t}\n\n\t\tscratch_va = (void *)svm->sev_es.ghcb;\n\t\tscratch_va += (scratch_gpa_beg - control->ghcb_gpa);\n\t} else {\n\t\t \n\t\tif (len > GHCB_SCRATCH_AREA_LIMIT) {\n\t\t\tpr_err(\"vmgexit: scratch area exceeds KVM limits (%#llx requested, %#llx limit)\\n\",\n\t\t\t       len, GHCB_SCRATCH_AREA_LIMIT);\n\t\t\tgoto e_scratch;\n\t\t}\n\t\tscratch_va = kvzalloc(len, GFP_KERNEL_ACCOUNT);\n\t\tif (!scratch_va)\n\t\t\treturn -ENOMEM;\n\n\t\tif (kvm_read_guest(svm->vcpu.kvm, scratch_gpa_beg, scratch_va, len)) {\n\t\t\t \n\t\t\tpr_err(\"vmgexit: kvm_read_guest for scratch area failed\\n\");\n\n\t\t\tkvfree(scratch_va);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\t \n\t\tsvm->sev_es.ghcb_sa_sync = sync;\n\t\tsvm->sev_es.ghcb_sa_free = true;\n\t}\n\n\tsvm->sev_es.ghcb_sa = scratch_va;\n\tsvm->sev_es.ghcb_sa_len = len;\n\n\treturn 0;\n\ne_scratch:\n\tghcb_set_sw_exit_info_1(svm->sev_es.ghcb, 2);\n\tghcb_set_sw_exit_info_2(svm->sev_es.ghcb, GHCB_ERR_INVALID_SCRATCH_AREA);\n\n\treturn 1;\n}\n\nstatic void set_ghcb_msr_bits(struct vcpu_svm *svm, u64 value, u64 mask,\n\t\t\t      unsigned int pos)\n{\n\tsvm->vmcb->control.ghcb_gpa &= ~(mask << pos);\n\tsvm->vmcb->control.ghcb_gpa |= (value & mask) << pos;\n}\n\nstatic u64 get_ghcb_msr_bits(struct vcpu_svm *svm, u64 mask, unsigned int pos)\n{\n\treturn (svm->vmcb->control.ghcb_gpa >> pos) & mask;\n}\n\nstatic void set_ghcb_msr(struct vcpu_svm *svm, u64 value)\n{\n\tsvm->vmcb->control.ghcb_gpa = value;\n}\n\nstatic int sev_handle_vmgexit_msr_protocol(struct vcpu_svm *svm)\n{\n\tstruct vmcb_control_area *control = &svm->vmcb->control;\n\tstruct kvm_vcpu *vcpu = &svm->vcpu;\n\tu64 ghcb_info;\n\tint ret = 1;\n\n\tghcb_info = control->ghcb_gpa & GHCB_MSR_INFO_MASK;\n\n\ttrace_kvm_vmgexit_msr_protocol_enter(svm->vcpu.vcpu_id,\n\t\t\t\t\t     control->ghcb_gpa);\n\n\tswitch (ghcb_info) {\n\tcase GHCB_MSR_SEV_INFO_REQ:\n\t\tset_ghcb_msr(svm, GHCB_MSR_SEV_INFO(GHCB_VERSION_MAX,\n\t\t\t\t\t\t    GHCB_VERSION_MIN,\n\t\t\t\t\t\t    sev_enc_bit));\n\t\tbreak;\n\tcase GHCB_MSR_CPUID_REQ: {\n\t\tu64 cpuid_fn, cpuid_reg, cpuid_value;\n\n\t\tcpuid_fn = get_ghcb_msr_bits(svm,\n\t\t\t\t\t     GHCB_MSR_CPUID_FUNC_MASK,\n\t\t\t\t\t     GHCB_MSR_CPUID_FUNC_POS);\n\n\t\t \n\t\tvcpu->arch.regs[VCPU_REGS_RAX] = cpuid_fn;\n\t\tvcpu->arch.regs[VCPU_REGS_RCX] = 0;\n\n\t\tret = svm_invoke_exit_handler(vcpu, SVM_EXIT_CPUID);\n\t\tif (!ret) {\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\n\t\tcpuid_reg = get_ghcb_msr_bits(svm,\n\t\t\t\t\t      GHCB_MSR_CPUID_REG_MASK,\n\t\t\t\t\t      GHCB_MSR_CPUID_REG_POS);\n\t\tif (cpuid_reg == 0)\n\t\t\tcpuid_value = vcpu->arch.regs[VCPU_REGS_RAX];\n\t\telse if (cpuid_reg == 1)\n\t\t\tcpuid_value = vcpu->arch.regs[VCPU_REGS_RBX];\n\t\telse if (cpuid_reg == 2)\n\t\t\tcpuid_value = vcpu->arch.regs[VCPU_REGS_RCX];\n\t\telse\n\t\t\tcpuid_value = vcpu->arch.regs[VCPU_REGS_RDX];\n\n\t\tset_ghcb_msr_bits(svm, cpuid_value,\n\t\t\t\t  GHCB_MSR_CPUID_VALUE_MASK,\n\t\t\t\t  GHCB_MSR_CPUID_VALUE_POS);\n\n\t\tset_ghcb_msr_bits(svm, GHCB_MSR_CPUID_RESP,\n\t\t\t\t  GHCB_MSR_INFO_MASK,\n\t\t\t\t  GHCB_MSR_INFO_POS);\n\t\tbreak;\n\t}\n\tcase GHCB_MSR_TERM_REQ: {\n\t\tu64 reason_set, reason_code;\n\n\t\treason_set = get_ghcb_msr_bits(svm,\n\t\t\t\t\t       GHCB_MSR_TERM_REASON_SET_MASK,\n\t\t\t\t\t       GHCB_MSR_TERM_REASON_SET_POS);\n\t\treason_code = get_ghcb_msr_bits(svm,\n\t\t\t\t\t\tGHCB_MSR_TERM_REASON_MASK,\n\t\t\t\t\t\tGHCB_MSR_TERM_REASON_POS);\n\t\tpr_info(\"SEV-ES guest requested termination: %#llx:%#llx\\n\",\n\t\t\treason_set, reason_code);\n\n\t\tvcpu->run->exit_reason = KVM_EXIT_SYSTEM_EVENT;\n\t\tvcpu->run->system_event.type = KVM_SYSTEM_EVENT_SEV_TERM;\n\t\tvcpu->run->system_event.ndata = 1;\n\t\tvcpu->run->system_event.data[0] = control->ghcb_gpa;\n\n\t\treturn 0;\n\t}\n\tdefault:\n\t\t \n\t\tbreak;\n\t}\n\n\ttrace_kvm_vmgexit_msr_protocol_exit(svm->vcpu.vcpu_id,\n\t\t\t\t\t    control->ghcb_gpa, ret);\n\n\treturn ret;\n}\n\nint sev_handle_vmgexit(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\tstruct vmcb_control_area *control = &svm->vmcb->control;\n\tu64 ghcb_gpa, exit_code;\n\tint ret;\n\n\t \n\tghcb_gpa = control->ghcb_gpa;\n\tif (ghcb_gpa & GHCB_MSR_INFO_MASK)\n\t\treturn sev_handle_vmgexit_msr_protocol(svm);\n\n\tif (!ghcb_gpa) {\n\t\tvcpu_unimpl(vcpu, \"vmgexit: GHCB gpa is not set\\n\");\n\n\t\t \n\t\treturn 1;\n\t}\n\n\tif (kvm_vcpu_map(vcpu, ghcb_gpa >> PAGE_SHIFT, &svm->sev_es.ghcb_map)) {\n\t\t \n\t\tvcpu_unimpl(vcpu, \"vmgexit: error mapping GHCB [%#llx] from guest\\n\",\n\t\t\t    ghcb_gpa);\n\n\t\t \n\t\treturn 1;\n\t}\n\n\tsvm->sev_es.ghcb = svm->sev_es.ghcb_map.hva;\n\n\ttrace_kvm_vmgexit_enter(vcpu->vcpu_id, svm->sev_es.ghcb);\n\n\tsev_es_sync_from_ghcb(svm);\n\tret = sev_es_validate_vmgexit(svm);\n\tif (ret)\n\t\treturn ret;\n\n\tghcb_set_sw_exit_info_1(svm->sev_es.ghcb, 0);\n\tghcb_set_sw_exit_info_2(svm->sev_es.ghcb, 0);\n\n\texit_code = kvm_ghcb_get_sw_exit_code(control);\n\tswitch (exit_code) {\n\tcase SVM_VMGEXIT_MMIO_READ:\n\t\tret = setup_vmgexit_scratch(svm, true, control->exit_info_2);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tret = kvm_sev_es_mmio_read(vcpu,\n\t\t\t\t\t   control->exit_info_1,\n\t\t\t\t\t   control->exit_info_2,\n\t\t\t\t\t   svm->sev_es.ghcb_sa);\n\t\tbreak;\n\tcase SVM_VMGEXIT_MMIO_WRITE:\n\t\tret = setup_vmgexit_scratch(svm, false, control->exit_info_2);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tret = kvm_sev_es_mmio_write(vcpu,\n\t\t\t\t\t    control->exit_info_1,\n\t\t\t\t\t    control->exit_info_2,\n\t\t\t\t\t    svm->sev_es.ghcb_sa);\n\t\tbreak;\n\tcase SVM_VMGEXIT_NMI_COMPLETE:\n\t\t++vcpu->stat.nmi_window_exits;\n\t\tsvm->nmi_masked = false;\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\tret = 1;\n\t\tbreak;\n\tcase SVM_VMGEXIT_AP_HLT_LOOP:\n\t\tret = kvm_emulate_ap_reset_hold(vcpu);\n\t\tbreak;\n\tcase SVM_VMGEXIT_AP_JUMP_TABLE: {\n\t\tstruct kvm_sev_info *sev = &to_kvm_svm(vcpu->kvm)->sev_info;\n\n\t\tswitch (control->exit_info_1) {\n\t\tcase 0:\n\t\t\t \n\t\t\tsev->ap_jump_table = control->exit_info_2;\n\t\t\tbreak;\n\t\tcase 1:\n\t\t\t \n\t\t\tghcb_set_sw_exit_info_2(svm->sev_es.ghcb, sev->ap_jump_table);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tpr_err(\"svm: vmgexit: unsupported AP jump table request - exit_info_1=%#llx\\n\",\n\t\t\t       control->exit_info_1);\n\t\t\tghcb_set_sw_exit_info_1(svm->sev_es.ghcb, 2);\n\t\t\tghcb_set_sw_exit_info_2(svm->sev_es.ghcb, GHCB_ERR_INVALID_INPUT);\n\t\t}\n\n\t\tret = 1;\n\t\tbreak;\n\t}\n\tcase SVM_VMGEXIT_UNSUPPORTED_EVENT:\n\t\tvcpu_unimpl(vcpu,\n\t\t\t    \"vmgexit: unsupported event - exit_info_1=%#llx, exit_info_2=%#llx\\n\",\n\t\t\t    control->exit_info_1, control->exit_info_2);\n\t\tret = -EINVAL;\n\t\tbreak;\n\tdefault:\n\t\tret = svm_invoke_exit_handler(vcpu, exit_code);\n\t}\n\n\treturn ret;\n}\n\nint sev_es_string_io(struct vcpu_svm *svm, int size, unsigned int port, int in)\n{\n\tint count;\n\tint bytes;\n\tint r;\n\n\tif (svm->vmcb->control.exit_info_2 > INT_MAX)\n\t\treturn -EINVAL;\n\n\tcount = svm->vmcb->control.exit_info_2;\n\tif (unlikely(check_mul_overflow(count, size, &bytes)))\n\t\treturn -EINVAL;\n\n\tr = setup_vmgexit_scratch(svm, in, bytes);\n\tif (r)\n\t\treturn r;\n\n\treturn kvm_sev_es_string_io(&svm->vcpu, size, port, svm->sev_es.ghcb_sa,\n\t\t\t\t    count, in);\n}\n\nstatic void sev_es_vcpu_after_set_cpuid(struct vcpu_svm *svm)\n{\n\tstruct kvm_vcpu *vcpu = &svm->vcpu;\n\n\tif (boot_cpu_has(X86_FEATURE_V_TSC_AUX)) {\n\t\tbool v_tsc_aux = guest_cpuid_has(vcpu, X86_FEATURE_RDTSCP) ||\n\t\t\t\t guest_cpuid_has(vcpu, X86_FEATURE_RDPID);\n\n\t\tset_msr_interception(vcpu, svm->msrpm, MSR_TSC_AUX, v_tsc_aux, v_tsc_aux);\n\t}\n}\n\nvoid sev_vcpu_after_set_cpuid(struct vcpu_svm *svm)\n{\n\tstruct kvm_vcpu *vcpu = &svm->vcpu;\n\tstruct kvm_cpuid_entry2 *best;\n\n\t \n\tbest = kvm_find_cpuid_entry(vcpu, 0x8000001F);\n\tif (best)\n\t\tvcpu->arch.reserved_gpa_bits &= ~(1UL << (best->ebx & 0x3f));\n\n\tif (sev_es_guest(svm->vcpu.kvm))\n\t\tsev_es_vcpu_after_set_cpuid(svm);\n}\n\nstatic void sev_es_init_vmcb(struct vcpu_svm *svm)\n{\n\tstruct vmcb *vmcb = svm->vmcb01.ptr;\n\tstruct kvm_vcpu *vcpu = &svm->vcpu;\n\n\tsvm->vmcb->control.nested_ctl |= SVM_NESTED_CTL_SEV_ES_ENABLE;\n\tsvm->vmcb->control.virt_ext |= LBR_CTL_ENABLE_MASK;\n\n\t \n\tif (svm->sev_es.vmsa)\n\t\tsvm->vmcb->control.vmsa_pa = __pa(svm->sev_es.vmsa);\n\n\t \n\tsvm_clr_intercept(svm, INTERCEPT_CR0_READ);\n\tsvm_clr_intercept(svm, INTERCEPT_CR4_READ);\n\tsvm_clr_intercept(svm, INTERCEPT_CR8_READ);\n\tsvm_clr_intercept(svm, INTERCEPT_CR0_WRITE);\n\tsvm_clr_intercept(svm, INTERCEPT_CR4_WRITE);\n\tsvm_clr_intercept(svm, INTERCEPT_CR8_WRITE);\n\n\tsvm_clr_intercept(svm, INTERCEPT_SELECTIVE_CR0);\n\n\t \n\tsvm_set_intercept(svm, TRAP_EFER_WRITE);\n\tsvm_set_intercept(svm, TRAP_CR0_WRITE);\n\tsvm_set_intercept(svm, TRAP_CR4_WRITE);\n\tsvm_set_intercept(svm, TRAP_CR8_WRITE);\n\n\tvmcb->control.intercepts[INTERCEPT_DR] = 0;\n\tif (!sev_es_debug_swap_enabled) {\n\t\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR7_READ);\n\t\tvmcb_set_intercept(&vmcb->control, INTERCEPT_DR7_WRITE);\n\t\trecalc_intercepts(svm);\n\t} else {\n\t\t \n\t\tclr_exception_intercept(svm, DB_VECTOR);\n\t}\n\n\t \n\tsvm_clr_intercept(svm, INTERCEPT_XSETBV);\n\n\t \n\tset_msr_interception(vcpu, svm->msrpm, MSR_EFER, 1, 1);\n\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_CR_PAT, 1, 1);\n\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTBRANCHFROMIP, 1, 1);\n\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTBRANCHTOIP, 1, 1);\n\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTINTFROMIP, 1, 1);\n\tset_msr_interception(vcpu, svm->msrpm, MSR_IA32_LASTINTTOIP, 1, 1);\n}\n\nvoid sev_init_vmcb(struct vcpu_svm *svm)\n{\n\tsvm->vmcb->control.nested_ctl |= SVM_NESTED_CTL_SEV_ENABLE;\n\tclr_exception_intercept(svm, UD_VECTOR);\n\n\t \n\tclr_exception_intercept(svm, GP_VECTOR);\n\n\tif (sev_es_guest(svm->vcpu.kvm))\n\t\tsev_es_init_vmcb(svm);\n}\n\nvoid sev_es_vcpu_reset(struct vcpu_svm *svm)\n{\n\t \n\tset_ghcb_msr(svm, GHCB_MSR_SEV_INFO(GHCB_VERSION_MAX,\n\t\t\t\t\t    GHCB_VERSION_MIN,\n\t\t\t\t\t    sev_enc_bit));\n}\n\nvoid sev_es_prepare_switch_to_guest(struct sev_es_save_area *hostsa)\n{\n\t \n\thostsa->xcr0 = xgetbv(XCR_XFEATURE_ENABLED_MASK);\n\thostsa->pkru = read_pkru();\n\thostsa->xss = host_xss;\n\n\t \n\tif (sev_es_debug_swap_enabled) {\n\t\thostsa->dr0 = native_get_debugreg(0);\n\t\thostsa->dr1 = native_get_debugreg(1);\n\t\thostsa->dr2 = native_get_debugreg(2);\n\t\thostsa->dr3 = native_get_debugreg(3);\n\t\thostsa->dr0_addr_mask = amd_get_dr_addr_mask(0);\n\t\thostsa->dr1_addr_mask = amd_get_dr_addr_mask(1);\n\t\thostsa->dr2_addr_mask = amd_get_dr_addr_mask(2);\n\t\thostsa->dr3_addr_mask = amd_get_dr_addr_mask(3);\n\t}\n}\n\nvoid sev_vcpu_deliver_sipi_vector(struct kvm_vcpu *vcpu, u8 vector)\n{\n\tstruct vcpu_svm *svm = to_svm(vcpu);\n\n\t \n\tif (!svm->sev_es.received_first_sipi) {\n\t\tsvm->sev_es.received_first_sipi = true;\n\t\treturn;\n\t}\n\n\t \n\tif (!svm->sev_es.ghcb)\n\t\treturn;\n\n\tghcb_set_sw_exit_info_2(svm->sev_es.ghcb, 1);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}