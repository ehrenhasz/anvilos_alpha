{
  "module_name": "x86.h",
  "hash_id": "da95c8dbdb08f71234d1bef93b2ec63e67e450481b85a201fc9b98986265a0e0",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/x86.h",
  "human_readable_source": " \n#ifndef ARCH_X86_KVM_X86_H\n#define ARCH_X86_KVM_X86_H\n\n#include <linux/kvm_host.h>\n#include <asm/fpu/xstate.h>\n#include <asm/mce.h>\n#include <asm/pvclock.h>\n#include \"kvm_cache_regs.h\"\n#include \"kvm_emulate.h\"\n\nstruct kvm_caps {\n\t \n\tbool has_tsc_control;\n\t \n\tu32  max_guest_tsc_khz;\n\t \n\tu8   tsc_scaling_ratio_frac_bits;\n\t \n\tu64  max_tsc_scaling_ratio;\n\t \n\tu64  default_tsc_scaling_ratio;\n\t \n\tbool has_bus_lock_exit;\n\t \n\tbool has_notify_vmexit;\n\n\tu64 supported_mce_cap;\n\tu64 supported_xcr0;\n\tu64 supported_xss;\n\tu64 supported_perf_cap;\n};\n\nvoid kvm_spurious_fault(void);\n\n#define KVM_NESTED_VMENTER_CONSISTENCY_CHECK(consistency_check)\t\t\\\n({\t\t\t\t\t\t\t\t\t\\\n\tbool failed = (consistency_check);\t\t\t\t\\\n\tif (failed)\t\t\t\t\t\t\t\\\n\t\ttrace_kvm_nested_vmenter_failed(#consistency_check, 0);\t\\\n\tfailed;\t\t\t\t\t\t\t\t\\\n})\n\n \n#define KVM_FIRST_EMULATED_VMX_MSR\tMSR_IA32_VMX_BASIC\n#define KVM_LAST_EMULATED_VMX_MSR\tMSR_IA32_VMX_VMFUNC\n\n#define KVM_DEFAULT_PLE_GAP\t\t128\n#define KVM_VMX_DEFAULT_PLE_WINDOW\t4096\n#define KVM_DEFAULT_PLE_WINDOW_GROW\t2\n#define KVM_DEFAULT_PLE_WINDOW_SHRINK\t0\n#define KVM_VMX_DEFAULT_PLE_WINDOW_MAX\tUINT_MAX\n#define KVM_SVM_DEFAULT_PLE_WINDOW_MAX\tUSHRT_MAX\n#define KVM_SVM_DEFAULT_PLE_WINDOW\t3000\n\nstatic inline unsigned int __grow_ple_window(unsigned int val,\n\t\tunsigned int base, unsigned int modifier, unsigned int max)\n{\n\tu64 ret = val;\n\n\tif (modifier < 1)\n\t\treturn base;\n\n\tif (modifier < base)\n\t\tret *= modifier;\n\telse\n\t\tret += modifier;\n\n\treturn min(ret, (u64)max);\n}\n\nstatic inline unsigned int __shrink_ple_window(unsigned int val,\n\t\tunsigned int base, unsigned int modifier, unsigned int min)\n{\n\tif (modifier < 1)\n\t\treturn base;\n\n\tif (modifier < base)\n\t\tval /= modifier;\n\telse\n\t\tval -= modifier;\n\n\treturn max(val, min);\n}\n\n#define MSR_IA32_CR_PAT_DEFAULT  0x0007040600070406ULL\n\nvoid kvm_service_local_tlb_flush_requests(struct kvm_vcpu *vcpu);\nint kvm_check_nested_events(struct kvm_vcpu *vcpu);\n\nstatic inline bool kvm_vcpu_has_run(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.last_vmentry_cpu != -1;\n}\n\nstatic inline bool kvm_is_exception_pending(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.exception.pending ||\n\t       vcpu->arch.exception_vmexit.pending ||\n\t       kvm_test_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n}\n\nstatic inline void kvm_clear_exception_queue(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.exception.pending = false;\n\tvcpu->arch.exception.injected = false;\n\tvcpu->arch.exception_vmexit.pending = false;\n}\n\nstatic inline void kvm_queue_interrupt(struct kvm_vcpu *vcpu, u8 vector,\n\tbool soft)\n{\n\tvcpu->arch.interrupt.injected = true;\n\tvcpu->arch.interrupt.soft = soft;\n\tvcpu->arch.interrupt.nr = vector;\n}\n\nstatic inline void kvm_clear_interrupt_queue(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.interrupt.injected = false;\n}\n\nstatic inline bool kvm_event_needs_reinjection(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.exception.injected || vcpu->arch.interrupt.injected ||\n\t\tvcpu->arch.nmi_injected;\n}\n\nstatic inline bool kvm_exception_is_soft(unsigned int nr)\n{\n\treturn (nr == BP_VECTOR) || (nr == OF_VECTOR);\n}\n\nstatic inline bool is_protmode(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_is_cr0_bit_set(vcpu, X86_CR0_PE);\n}\n\nstatic inline bool is_long_mode(struct kvm_vcpu *vcpu)\n{\n#ifdef CONFIG_X86_64\n\treturn !!(vcpu->arch.efer & EFER_LMA);\n#else\n\treturn false;\n#endif\n}\n\nstatic inline bool is_64_bit_mode(struct kvm_vcpu *vcpu)\n{\n\tint cs_db, cs_l;\n\n\tWARN_ON_ONCE(vcpu->arch.guest_state_protected);\n\n\tif (!is_long_mode(vcpu))\n\t\treturn false;\n\tstatic_call(kvm_x86_get_cs_db_l_bits)(vcpu, &cs_db, &cs_l);\n\treturn cs_l;\n}\n\nstatic inline bool is_64_bit_hypercall(struct kvm_vcpu *vcpu)\n{\n\t \n\treturn vcpu->arch.guest_state_protected || is_64_bit_mode(vcpu);\n}\n\nstatic inline bool x86_exception_has_error_code(unsigned int vector)\n{\n\tstatic u32 exception_has_error_code = BIT(DF_VECTOR) | BIT(TS_VECTOR) |\n\t\t\tBIT(NP_VECTOR) | BIT(SS_VECTOR) | BIT(GP_VECTOR) |\n\t\t\tBIT(PF_VECTOR) | BIT(AC_VECTOR);\n\n\treturn (1U << vector) & exception_has_error_code;\n}\n\nstatic inline bool mmu_is_nested(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.walk_mmu == &vcpu->arch.nested_mmu;\n}\n\nstatic inline bool is_pae(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_is_cr4_bit_set(vcpu, X86_CR4_PAE);\n}\n\nstatic inline bool is_pse(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_is_cr4_bit_set(vcpu, X86_CR4_PSE);\n}\n\nstatic inline bool is_paging(struct kvm_vcpu *vcpu)\n{\n\treturn likely(kvm_is_cr0_bit_set(vcpu, X86_CR0_PG));\n}\n\nstatic inline bool is_pae_paging(struct kvm_vcpu *vcpu)\n{\n\treturn !is_long_mode(vcpu) && is_pae(vcpu) && is_paging(vcpu);\n}\n\nstatic inline u8 vcpu_virt_addr_bits(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_is_cr4_bit_set(vcpu, X86_CR4_LA57) ? 57 : 48;\n}\n\nstatic inline bool is_noncanonical_address(u64 la, struct kvm_vcpu *vcpu)\n{\n\treturn !__is_canonical_address(la, vcpu_virt_addr_bits(vcpu));\n}\n\nstatic inline void vcpu_cache_mmio_info(struct kvm_vcpu *vcpu,\n\t\t\t\t\tgva_t gva, gfn_t gfn, unsigned access)\n{\n\tu64 gen = kvm_memslots(vcpu->kvm)->generation;\n\n\tif (unlikely(gen & KVM_MEMSLOT_GEN_UPDATE_IN_PROGRESS))\n\t\treturn;\n\n\t \n\tvcpu->arch.mmio_gva = mmu_is_nested(vcpu) ? 0 : gva & PAGE_MASK;\n\tvcpu->arch.mmio_access = access;\n\tvcpu->arch.mmio_gfn = gfn;\n\tvcpu->arch.mmio_gen = gen;\n}\n\nstatic inline bool vcpu_match_mmio_gen(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.mmio_gen == kvm_memslots(vcpu->kvm)->generation;\n}\n\n \n#define MMIO_GVA_ANY (~(gva_t)0)\n\nstatic inline void vcpu_clear_mmio_info(struct kvm_vcpu *vcpu, gva_t gva)\n{\n\tif (gva != MMIO_GVA_ANY && vcpu->arch.mmio_gva != (gva & PAGE_MASK))\n\t\treturn;\n\n\tvcpu->arch.mmio_gva = 0;\n}\n\nstatic inline bool vcpu_match_mmio_gva(struct kvm_vcpu *vcpu, unsigned long gva)\n{\n\tif (vcpu_match_mmio_gen(vcpu) && vcpu->arch.mmio_gva &&\n\t      vcpu->arch.mmio_gva == (gva & PAGE_MASK))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool vcpu_match_mmio_gpa(struct kvm_vcpu *vcpu, gpa_t gpa)\n{\n\tif (vcpu_match_mmio_gen(vcpu) && vcpu->arch.mmio_gfn &&\n\t      vcpu->arch.mmio_gfn == gpa >> PAGE_SHIFT)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline unsigned long kvm_register_read(struct kvm_vcpu *vcpu, int reg)\n{\n\tunsigned long val = kvm_register_read_raw(vcpu, reg);\n\n\treturn is_64_bit_mode(vcpu) ? val : (u32)val;\n}\n\nstatic inline void kvm_register_write(struct kvm_vcpu *vcpu,\n\t\t\t\t       int reg, unsigned long val)\n{\n\tif (!is_64_bit_mode(vcpu))\n\t\tval = (u32)val;\n\treturn kvm_register_write_raw(vcpu, reg, val);\n}\n\nstatic inline bool kvm_check_has_quirk(struct kvm *kvm, u64 quirk)\n{\n\treturn !(kvm->arch.disabled_quirks & quirk);\n}\n\nvoid kvm_inject_realmode_interrupt(struct kvm_vcpu *vcpu, int irq, int inc_eip);\n\nu64 get_kvmclock_ns(struct kvm *kvm);\n\nint kvm_read_guest_virt(struct kvm_vcpu *vcpu,\n\tgva_t addr, void *val, unsigned int bytes,\n\tstruct x86_exception *exception);\n\nint kvm_write_guest_virt_system(struct kvm_vcpu *vcpu,\n\tgva_t addr, void *val, unsigned int bytes,\n\tstruct x86_exception *exception);\n\nint handle_ud(struct kvm_vcpu *vcpu);\n\nvoid kvm_deliver_exception_payload(struct kvm_vcpu *vcpu,\n\t\t\t\t   struct kvm_queued_exception *ex);\n\nvoid kvm_vcpu_mtrr_init(struct kvm_vcpu *vcpu);\nu8 kvm_mtrr_get_guest_memory_type(struct kvm_vcpu *vcpu, gfn_t gfn);\nint kvm_mtrr_set_msr(struct kvm_vcpu *vcpu, u32 msr, u64 data);\nint kvm_mtrr_get_msr(struct kvm_vcpu *vcpu, u32 msr, u64 *pdata);\nbool kvm_mtrr_check_gfn_range_consistency(struct kvm_vcpu *vcpu, gfn_t gfn,\n\t\t\t\t\t  int page_num);\nbool kvm_vector_hashing_enabled(void);\nvoid kvm_fixup_and_inject_pf_error(struct kvm_vcpu *vcpu, gva_t gva, u16 error_code);\nint x86_decode_emulated_instruction(struct kvm_vcpu *vcpu, int emulation_type,\n\t\t\t\t    void *insn, int insn_len);\nint x86_emulate_instruction(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,\n\t\t\t    int emulation_type, void *insn, int insn_len);\nfastpath_t handle_fastpath_set_msr_irqoff(struct kvm_vcpu *vcpu);\n\nextern u64 host_xcr0;\nextern u64 host_xss;\nextern u64 host_arch_capabilities;\n\nextern struct kvm_caps kvm_caps;\n\nextern bool enable_pmu;\n\n \nstatic inline u64 kvm_get_filtered_xcr0(void)\n{\n\tu64 permitted_xcr0 = kvm_caps.supported_xcr0;\n\n\tBUILD_BUG_ON(XFEATURE_MASK_USER_DYNAMIC != XFEATURE_MASK_XTILE_DATA);\n\n\tif (permitted_xcr0 & XFEATURE_MASK_USER_DYNAMIC) {\n\t\tpermitted_xcr0 &= xstate_get_guest_group_perm();\n\n\t\t \n\t\tif (!(permitted_xcr0 & XFEATURE_MASK_XTILE_DATA))\n\t\t\tpermitted_xcr0 &= ~XFEATURE_MASK_XTILE_CFG;\n\t}\n\treturn permitted_xcr0;\n}\n\nstatic inline bool kvm_mpx_supported(void)\n{\n\treturn (kvm_caps.supported_xcr0 & (XFEATURE_MASK_BNDREGS | XFEATURE_MASK_BNDCSR))\n\t\t== (XFEATURE_MASK_BNDREGS | XFEATURE_MASK_BNDCSR);\n}\n\nextern unsigned int min_timer_period_us;\n\nextern bool enable_vmware_backdoor;\n\nextern int pi_inject_timer;\n\nextern bool report_ignored_msrs;\n\nextern bool eager_page_split;\n\nstatic inline void kvm_pr_unimpl_wrmsr(struct kvm_vcpu *vcpu, u32 msr, u64 data)\n{\n\tif (report_ignored_msrs)\n\t\tvcpu_unimpl(vcpu, \"Unhandled WRMSR(0x%x) = 0x%llx\\n\", msr, data);\n}\n\nstatic inline void kvm_pr_unimpl_rdmsr(struct kvm_vcpu *vcpu, u32 msr)\n{\n\tif (report_ignored_msrs)\n\t\tvcpu_unimpl(vcpu, \"Unhandled RDMSR(0x%x)\\n\", msr);\n}\n\nstatic inline u64 nsec_to_cycles(struct kvm_vcpu *vcpu, u64 nsec)\n{\n\treturn pvclock_scale_delta(nsec, vcpu->arch.virtual_tsc_mult,\n\t\t\t\t   vcpu->arch.virtual_tsc_shift);\n}\n\n \n#define do_shl32_div32(n, base)\t\t\t\t\t\\\n\t({\t\t\t\t\t\t\t\\\n\t    u32 __quot, __rem;\t\t\t\t\t\\\n\t    asm(\"divl %2\" : \"=a\" (__quot), \"=d\" (__rem)\t\t\\\n\t\t\t: \"rm\" (base), \"0\" (0), \"1\" ((u32) n));\t\\\n\t    n = __quot;\t\t\t\t\t\t\\\n\t    __rem;\t\t\t\t\t\t\\\n\t })\n\nstatic inline bool kvm_mwait_in_guest(struct kvm *kvm)\n{\n\treturn kvm->arch.mwait_in_guest;\n}\n\nstatic inline bool kvm_hlt_in_guest(struct kvm *kvm)\n{\n\treturn kvm->arch.hlt_in_guest;\n}\n\nstatic inline bool kvm_pause_in_guest(struct kvm *kvm)\n{\n\treturn kvm->arch.pause_in_guest;\n}\n\nstatic inline bool kvm_cstate_in_guest(struct kvm *kvm)\n{\n\treturn kvm->arch.cstate_in_guest;\n}\n\nstatic inline bool kvm_notify_vmexit_enabled(struct kvm *kvm)\n{\n\treturn kvm->arch.notify_vmexit_flags & KVM_X86_NOTIFY_VMEXIT_ENABLED;\n}\n\nenum kvm_intr_type {\n\t \n\tKVM_HANDLING_IRQ = 1,\n\tKVM_HANDLING_NMI,\n};\n\nstatic __always_inline void kvm_before_interrupt(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t enum kvm_intr_type intr)\n{\n\tWRITE_ONCE(vcpu->arch.handling_intr_from_guest, (u8)intr);\n}\n\nstatic __always_inline void kvm_after_interrupt(struct kvm_vcpu *vcpu)\n{\n\tWRITE_ONCE(vcpu->arch.handling_intr_from_guest, 0);\n}\n\nstatic inline bool kvm_handling_nmi_from_guest(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.handling_intr_from_guest == KVM_HANDLING_NMI;\n}\n\nstatic inline bool kvm_pat_valid(u64 data)\n{\n\tif (data & 0xF8F8F8F8F8F8F8F8ull)\n\t\treturn false;\n\t \n\treturn (data | ((data & 0x0202020202020202ull) << 1)) == data;\n}\n\nstatic inline bool kvm_dr7_valid(u64 data)\n{\n\t \n\treturn !(data >> 32);\n}\nstatic inline bool kvm_dr6_valid(u64 data)\n{\n\t \n\treturn !(data >> 32);\n}\n\n \nstatic inline void kvm_machine_check(void)\n{\n#if defined(CONFIG_X86_MCE)\n\tstruct pt_regs regs = {\n\t\t.cs = 3,  \n\t\t.flags = X86_EFLAGS_IF,\n\t};\n\n\tdo_machine_check(&regs);\n#endif\n}\n\nvoid kvm_load_guest_xsave_state(struct kvm_vcpu *vcpu);\nvoid kvm_load_host_xsave_state(struct kvm_vcpu *vcpu);\nint kvm_spec_ctrl_test_value(u64 value);\nbool __kvm_is_valid_cr4(struct kvm_vcpu *vcpu, unsigned long cr4);\nint kvm_handle_memory_failure(struct kvm_vcpu *vcpu, int r,\n\t\t\t      struct x86_exception *e);\nint kvm_handle_invpcid(struct kvm_vcpu *vcpu, unsigned long type, gva_t gva);\nbool kvm_msr_allowed(struct kvm_vcpu *vcpu, u32 index, u32 type);\n\n \n#define  KVM_MSR_RET_INVALID\t2\t \n#define  KVM_MSR_RET_FILTERED\t3\t \n\n#define __cr4_reserved_bits(__cpu_has, __c)             \\\n({                                                      \\\n\tu64 __reserved_bits = CR4_RESERVED_BITS;        \\\n                                                        \\\n\tif (!__cpu_has(__c, X86_FEATURE_XSAVE))         \\\n\t\t__reserved_bits |= X86_CR4_OSXSAVE;     \\\n\tif (!__cpu_has(__c, X86_FEATURE_SMEP))          \\\n\t\t__reserved_bits |= X86_CR4_SMEP;        \\\n\tif (!__cpu_has(__c, X86_FEATURE_SMAP))          \\\n\t\t__reserved_bits |= X86_CR4_SMAP;        \\\n\tif (!__cpu_has(__c, X86_FEATURE_FSGSBASE))      \\\n\t\t__reserved_bits |= X86_CR4_FSGSBASE;    \\\n\tif (!__cpu_has(__c, X86_FEATURE_PKU))           \\\n\t\t__reserved_bits |= X86_CR4_PKE;         \\\n\tif (!__cpu_has(__c, X86_FEATURE_LA57))          \\\n\t\t__reserved_bits |= X86_CR4_LA57;        \\\n\tif (!__cpu_has(__c, X86_FEATURE_UMIP))          \\\n\t\t__reserved_bits |= X86_CR4_UMIP;        \\\n\tif (!__cpu_has(__c, X86_FEATURE_VMX))           \\\n\t\t__reserved_bits |= X86_CR4_VMXE;        \\\n\tif (!__cpu_has(__c, X86_FEATURE_PCID))          \\\n\t\t__reserved_bits |= X86_CR4_PCIDE;       \\\n\t__reserved_bits;                                \\\n})\n\nint kvm_sev_es_mmio_write(struct kvm_vcpu *vcpu, gpa_t src, unsigned int bytes,\n\t\t\t  void *dst);\nint kvm_sev_es_mmio_read(struct kvm_vcpu *vcpu, gpa_t src, unsigned int bytes,\n\t\t\t void *dst);\nint kvm_sev_es_string_io(struct kvm_vcpu *vcpu, unsigned int size,\n\t\t\t unsigned int port, void *data,  unsigned int count,\n\t\t\t int in);\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}