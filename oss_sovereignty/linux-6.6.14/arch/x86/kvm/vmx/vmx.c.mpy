{
  "module_name": "vmx.c",
  "hash_id": "89bb4319ed3cce52292a3c1073b1a307240add37ef6a191516f412fc7159b578",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/vmx/vmx.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/highmem.h>\n#include <linux/hrtimer.h>\n#include <linux/kernel.h>\n#include <linux/kvm_host.h>\n#include <linux/module.h>\n#include <linux/moduleparam.h>\n#include <linux/mod_devicetable.h>\n#include <linux/mm.h>\n#include <linux/objtool.h>\n#include <linux/sched.h>\n#include <linux/sched/smt.h>\n#include <linux/slab.h>\n#include <linux/tboot.h>\n#include <linux/trace_events.h>\n#include <linux/entry-kvm.h>\n\n#include <asm/apic.h>\n#include <asm/asm.h>\n#include <asm/cpu.h>\n#include <asm/cpu_device_id.h>\n#include <asm/debugreg.h>\n#include <asm/desc.h>\n#include <asm/fpu/api.h>\n#include <asm/fpu/xstate.h>\n#include <asm/idtentry.h>\n#include <asm/io.h>\n#include <asm/irq_remapping.h>\n#include <asm/reboot.h>\n#include <asm/perf_event.h>\n#include <asm/mmu_context.h>\n#include <asm/mshyperv.h>\n#include <asm/mwait.h>\n#include <asm/spec-ctrl.h>\n#include <asm/vmx.h>\n\n#include \"capabilities.h\"\n#include \"cpuid.h\"\n#include \"hyperv.h\"\n#include \"kvm_onhyperv.h\"\n#include \"irq.h\"\n#include \"kvm_cache_regs.h\"\n#include \"lapic.h\"\n#include \"mmu.h\"\n#include \"nested.h\"\n#include \"pmu.h\"\n#include \"sgx.h\"\n#include \"trace.h\"\n#include \"vmcs.h\"\n#include \"vmcs12.h\"\n#include \"vmx.h\"\n#include \"x86.h\"\n#include \"smm.h\"\n\nMODULE_AUTHOR(\"Qumranet\");\nMODULE_LICENSE(\"GPL\");\n\n#ifdef MODULE\nstatic const struct x86_cpu_id vmx_cpu_id[] = {\n\tX86_MATCH_FEATURE(X86_FEATURE_VMX, NULL),\n\t{}\n};\nMODULE_DEVICE_TABLE(x86cpu, vmx_cpu_id);\n#endif\n\nbool __read_mostly enable_vpid = 1;\nmodule_param_named(vpid, enable_vpid, bool, 0444);\n\nstatic bool __read_mostly enable_vnmi = 1;\nmodule_param_named(vnmi, enable_vnmi, bool, S_IRUGO);\n\nbool __read_mostly flexpriority_enabled = 1;\nmodule_param_named(flexpriority, flexpriority_enabled, bool, S_IRUGO);\n\nbool __read_mostly enable_ept = 1;\nmodule_param_named(ept, enable_ept, bool, S_IRUGO);\n\nbool __read_mostly enable_unrestricted_guest = 1;\nmodule_param_named(unrestricted_guest,\n\t\t\tenable_unrestricted_guest, bool, S_IRUGO);\n\nbool __read_mostly enable_ept_ad_bits = 1;\nmodule_param_named(eptad, enable_ept_ad_bits, bool, S_IRUGO);\n\nstatic bool __read_mostly emulate_invalid_guest_state = true;\nmodule_param(emulate_invalid_guest_state, bool, S_IRUGO);\n\nstatic bool __read_mostly fasteoi = 1;\nmodule_param(fasteoi, bool, S_IRUGO);\n\nmodule_param(enable_apicv, bool, S_IRUGO);\n\nbool __read_mostly enable_ipiv = true;\nmodule_param(enable_ipiv, bool, 0444);\n\n \nstatic bool __read_mostly nested = 1;\nmodule_param(nested, bool, S_IRUGO);\n\nbool __read_mostly enable_pml = 1;\nmodule_param_named(pml, enable_pml, bool, S_IRUGO);\n\nstatic bool __read_mostly error_on_inconsistent_vmcs_config = true;\nmodule_param(error_on_inconsistent_vmcs_config, bool, 0444);\n\nstatic bool __read_mostly dump_invalid_vmcs = 0;\nmodule_param(dump_invalid_vmcs, bool, 0644);\n\n#define MSR_BITMAP_MODE_X2APIC\t\t1\n#define MSR_BITMAP_MODE_X2APIC_APICV\t2\n\n#define KVM_VMX_TSC_MULTIPLIER_MAX     0xffffffffffffffffULL\n\n \nstatic int __read_mostly cpu_preemption_timer_multi;\nstatic bool __read_mostly enable_preemption_timer = 1;\n#ifdef CONFIG_X86_64\nmodule_param_named(preemption_timer, enable_preemption_timer, bool, S_IRUGO);\n#endif\n\nextern bool __read_mostly allow_smaller_maxphyaddr;\nmodule_param(allow_smaller_maxphyaddr, bool, S_IRUGO);\n\n#define KVM_VM_CR0_ALWAYS_OFF (X86_CR0_NW | X86_CR0_CD)\n#define KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST X86_CR0_NE\n#define KVM_VM_CR0_ALWAYS_ON\t\t\t\t\\\n\t(KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST | X86_CR0_PG | X86_CR0_PE)\n\n#define KVM_VM_CR4_ALWAYS_ON_UNRESTRICTED_GUEST X86_CR4_VMXE\n#define KVM_PMODE_VM_CR4_ALWAYS_ON (X86_CR4_PAE | X86_CR4_VMXE)\n#define KVM_RMODE_VM_CR4_ALWAYS_ON (X86_CR4_VME | X86_CR4_PAE | X86_CR4_VMXE)\n\n#define RMODE_GUEST_OWNED_EFLAGS_BITS (~(X86_EFLAGS_IOPL | X86_EFLAGS_VM))\n\n#define MSR_IA32_RTIT_STATUS_MASK (~(RTIT_STATUS_FILTEREN | \\\n\tRTIT_STATUS_CONTEXTEN | RTIT_STATUS_TRIGGEREN | \\\n\tRTIT_STATUS_ERROR | RTIT_STATUS_STOPPED | \\\n\tRTIT_STATUS_BYTECNT))\n\n \nstatic u32 vmx_possible_passthrough_msrs[MAX_POSSIBLE_PASSTHROUGH_MSRS] = {\n\tMSR_IA32_SPEC_CTRL,\n\tMSR_IA32_PRED_CMD,\n\tMSR_IA32_FLUSH_CMD,\n\tMSR_IA32_TSC,\n#ifdef CONFIG_X86_64\n\tMSR_FS_BASE,\n\tMSR_GS_BASE,\n\tMSR_KERNEL_GS_BASE,\n\tMSR_IA32_XFD,\n\tMSR_IA32_XFD_ERR,\n#endif\n\tMSR_IA32_SYSENTER_CS,\n\tMSR_IA32_SYSENTER_ESP,\n\tMSR_IA32_SYSENTER_EIP,\n\tMSR_CORE_C1_RES,\n\tMSR_CORE_C3_RESIDENCY,\n\tMSR_CORE_C6_RESIDENCY,\n\tMSR_CORE_C7_RESIDENCY,\n};\n\n \nstatic unsigned int ple_gap = KVM_DEFAULT_PLE_GAP;\nmodule_param(ple_gap, uint, 0444);\n\nstatic unsigned int ple_window = KVM_VMX_DEFAULT_PLE_WINDOW;\nmodule_param(ple_window, uint, 0444);\n\n \nstatic unsigned int ple_window_grow = KVM_DEFAULT_PLE_WINDOW_GROW;\nmodule_param(ple_window_grow, uint, 0444);\n\n \nstatic unsigned int ple_window_shrink = KVM_DEFAULT_PLE_WINDOW_SHRINK;\nmodule_param(ple_window_shrink, uint, 0444);\n\n \nstatic unsigned int ple_window_max        = KVM_VMX_DEFAULT_PLE_WINDOW_MAX;\nmodule_param(ple_window_max, uint, 0444);\n\n \nint __read_mostly pt_mode = PT_MODE_SYSTEM;\nmodule_param(pt_mode, int, S_IRUGO);\n\nstatic DEFINE_STATIC_KEY_FALSE(vmx_l1d_should_flush);\nstatic DEFINE_STATIC_KEY_FALSE(vmx_l1d_flush_cond);\nstatic DEFINE_MUTEX(vmx_l1d_flush_mutex);\n\n \nstatic enum vmx_l1d_flush_state __read_mostly vmentry_l1d_flush_param = VMENTER_L1D_FLUSH_AUTO;\n\nstatic const struct {\n\tconst char *option;\n\tbool for_parse;\n} vmentry_l1d_param[] = {\n\t[VMENTER_L1D_FLUSH_AUTO]\t = {\"auto\", true},\n\t[VMENTER_L1D_FLUSH_NEVER]\t = {\"never\", true},\n\t[VMENTER_L1D_FLUSH_COND]\t = {\"cond\", true},\n\t[VMENTER_L1D_FLUSH_ALWAYS]\t = {\"always\", true},\n\t[VMENTER_L1D_FLUSH_EPT_DISABLED] = {\"EPT disabled\", false},\n\t[VMENTER_L1D_FLUSH_NOT_REQUIRED] = {\"not required\", false},\n};\n\n#define L1D_CACHE_ORDER 4\nstatic void *vmx_l1d_flush_pages;\n\nstatic int vmx_setup_l1d_flush(enum vmx_l1d_flush_state l1tf)\n{\n\tstruct page *page;\n\tunsigned int i;\n\n\tif (!boot_cpu_has_bug(X86_BUG_L1TF)) {\n\t\tl1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;\n\t\treturn 0;\n\t}\n\n\tif (!enable_ept) {\n\t\tl1tf_vmx_mitigation = VMENTER_L1D_FLUSH_EPT_DISABLED;\n\t\treturn 0;\n\t}\n\n\tif (host_arch_capabilities & ARCH_CAP_SKIP_VMENTRY_L1DFLUSH) {\n\t\tl1tf_vmx_mitigation = VMENTER_L1D_FLUSH_NOT_REQUIRED;\n\t\treturn 0;\n\t}\n\n\t \n\tif (l1tf == VMENTER_L1D_FLUSH_AUTO) {\n\t\tswitch (l1tf_mitigation) {\n\t\tcase L1TF_MITIGATION_OFF:\n\t\t\tl1tf = VMENTER_L1D_FLUSH_NEVER;\n\t\t\tbreak;\n\t\tcase L1TF_MITIGATION_FLUSH_NOWARN:\n\t\tcase L1TF_MITIGATION_FLUSH:\n\t\tcase L1TF_MITIGATION_FLUSH_NOSMT:\n\t\t\tl1tf = VMENTER_L1D_FLUSH_COND;\n\t\t\tbreak;\n\t\tcase L1TF_MITIGATION_FULL:\n\t\tcase L1TF_MITIGATION_FULL_FORCE:\n\t\t\tl1tf = VMENTER_L1D_FLUSH_ALWAYS;\n\t\t\tbreak;\n\t\t}\n\t} else if (l1tf_mitigation == L1TF_MITIGATION_FULL_FORCE) {\n\t\tl1tf = VMENTER_L1D_FLUSH_ALWAYS;\n\t}\n\n\tif (l1tf != VMENTER_L1D_FLUSH_NEVER && !vmx_l1d_flush_pages &&\n\t    !boot_cpu_has(X86_FEATURE_FLUSH_L1D)) {\n\t\t \n\t\tpage = alloc_pages(GFP_KERNEL, L1D_CACHE_ORDER);\n\t\tif (!page)\n\t\t\treturn -ENOMEM;\n\t\tvmx_l1d_flush_pages = page_address(page);\n\n\t\t \n\t\tfor (i = 0; i < 1u << L1D_CACHE_ORDER; ++i) {\n\t\t\tmemset(vmx_l1d_flush_pages + i * PAGE_SIZE, i + 1,\n\t\t\t       PAGE_SIZE);\n\t\t}\n\t}\n\n\tl1tf_vmx_mitigation = l1tf;\n\n\tif (l1tf != VMENTER_L1D_FLUSH_NEVER)\n\t\tstatic_branch_enable(&vmx_l1d_should_flush);\n\telse\n\t\tstatic_branch_disable(&vmx_l1d_should_flush);\n\n\tif (l1tf == VMENTER_L1D_FLUSH_COND)\n\t\tstatic_branch_enable(&vmx_l1d_flush_cond);\n\telse\n\t\tstatic_branch_disable(&vmx_l1d_flush_cond);\n\treturn 0;\n}\n\nstatic int vmentry_l1d_flush_parse(const char *s)\n{\n\tunsigned int i;\n\n\tif (s) {\n\t\tfor (i = 0; i < ARRAY_SIZE(vmentry_l1d_param); i++) {\n\t\t\tif (vmentry_l1d_param[i].for_parse &&\n\t\t\t    sysfs_streq(s, vmentry_l1d_param[i].option))\n\t\t\t\treturn i;\n\t\t}\n\t}\n\treturn -EINVAL;\n}\n\nstatic int vmentry_l1d_flush_set(const char *s, const struct kernel_param *kp)\n{\n\tint l1tf, ret;\n\n\tl1tf = vmentry_l1d_flush_parse(s);\n\tif (l1tf < 0)\n\t\treturn l1tf;\n\n\tif (!boot_cpu_has(X86_BUG_L1TF))\n\t\treturn 0;\n\n\t \n\tif (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_AUTO) {\n\t\tvmentry_l1d_flush_param = l1tf;\n\t\treturn 0;\n\t}\n\n\tmutex_lock(&vmx_l1d_flush_mutex);\n\tret = vmx_setup_l1d_flush(l1tf);\n\tmutex_unlock(&vmx_l1d_flush_mutex);\n\treturn ret;\n}\n\nstatic int vmentry_l1d_flush_get(char *s, const struct kernel_param *kp)\n{\n\tif (WARN_ON_ONCE(l1tf_vmx_mitigation >= ARRAY_SIZE(vmentry_l1d_param)))\n\t\treturn sysfs_emit(s, \"???\\n\");\n\n\treturn sysfs_emit(s, \"%s\\n\", vmentry_l1d_param[l1tf_vmx_mitigation].option);\n}\n\nstatic __always_inline void vmx_disable_fb_clear(struct vcpu_vmx *vmx)\n{\n\tu64 msr;\n\n\tif (!vmx->disable_fb_clear)\n\t\treturn;\n\n\tmsr = __rdmsr(MSR_IA32_MCU_OPT_CTRL);\n\tmsr |= FB_CLEAR_DIS;\n\tnative_wrmsrl(MSR_IA32_MCU_OPT_CTRL, msr);\n\t \n\tvmx->msr_ia32_mcu_opt_ctrl = msr;\n}\n\nstatic __always_inline void vmx_enable_fb_clear(struct vcpu_vmx *vmx)\n{\n\tif (!vmx->disable_fb_clear)\n\t\treturn;\n\n\tvmx->msr_ia32_mcu_opt_ctrl &= ~FB_CLEAR_DIS;\n\tnative_wrmsrl(MSR_IA32_MCU_OPT_CTRL, vmx->msr_ia32_mcu_opt_ctrl);\n}\n\nstatic void vmx_update_fb_clear_dis(struct kvm_vcpu *vcpu, struct vcpu_vmx *vmx)\n{\n\tvmx->disable_fb_clear = (host_arch_capabilities & ARCH_CAP_FB_CLEAR_CTRL) &&\n\t\t\t\t!boot_cpu_has_bug(X86_BUG_MDS) &&\n\t\t\t\t!boot_cpu_has_bug(X86_BUG_TAA);\n\n\t \n\tif ((vcpu->arch.arch_capabilities & ARCH_CAP_FB_CLEAR) ||\n\t   ((vcpu->arch.arch_capabilities & ARCH_CAP_MDS_NO) &&\n\t    (vcpu->arch.arch_capabilities & ARCH_CAP_TAA_NO) &&\n\t    (vcpu->arch.arch_capabilities & ARCH_CAP_PSDP_NO) &&\n\t    (vcpu->arch.arch_capabilities & ARCH_CAP_FBSDP_NO) &&\n\t    (vcpu->arch.arch_capabilities & ARCH_CAP_SBDR_SSDP_NO)))\n\t\tvmx->disable_fb_clear = false;\n}\n\nstatic const struct kernel_param_ops vmentry_l1d_flush_ops = {\n\t.set = vmentry_l1d_flush_set,\n\t.get = vmentry_l1d_flush_get,\n};\nmodule_param_cb(vmentry_l1d_flush, &vmentry_l1d_flush_ops, NULL, 0644);\n\nstatic u32 vmx_segment_access_rights(struct kvm_segment *var);\n\nvoid vmx_vmexit(void);\n\n#define vmx_insn_failed(fmt...)\t\t\\\ndo {\t\t\t\t\t\\\n\tWARN_ONCE(1, fmt);\t\t\\\n\tpr_warn_ratelimited(fmt);\t\\\n} while (0)\n\nnoinline void vmread_error(unsigned long field)\n{\n\tvmx_insn_failed(\"vmread failed: field=%lx\\n\", field);\n}\n\n#ifndef CONFIG_CC_HAS_ASM_GOTO_OUTPUT\nnoinstr void vmread_error_trampoline2(unsigned long field, bool fault)\n{\n\tif (fault) {\n\t\tkvm_spurious_fault();\n\t} else {\n\t\tinstrumentation_begin();\n\t\tvmread_error(field);\n\t\tinstrumentation_end();\n\t}\n}\n#endif\n\nnoinline void vmwrite_error(unsigned long field, unsigned long value)\n{\n\tvmx_insn_failed(\"vmwrite failed: field=%lx val=%lx err=%u\\n\",\n\t\t\tfield, value, vmcs_read32(VM_INSTRUCTION_ERROR));\n}\n\nnoinline void vmclear_error(struct vmcs *vmcs, u64 phys_addr)\n{\n\tvmx_insn_failed(\"vmclear failed: %p/%llx err=%u\\n\",\n\t\t\tvmcs, phys_addr, vmcs_read32(VM_INSTRUCTION_ERROR));\n}\n\nnoinline void vmptrld_error(struct vmcs *vmcs, u64 phys_addr)\n{\n\tvmx_insn_failed(\"vmptrld failed: %p/%llx err=%u\\n\",\n\t\t\tvmcs, phys_addr, vmcs_read32(VM_INSTRUCTION_ERROR));\n}\n\nnoinline void invvpid_error(unsigned long ext, u16 vpid, gva_t gva)\n{\n\tvmx_insn_failed(\"invvpid failed: ext=0x%lx vpid=%u gva=0x%lx\\n\",\n\t\t\text, vpid, gva);\n}\n\nnoinline void invept_error(unsigned long ext, u64 eptp, gpa_t gpa)\n{\n\tvmx_insn_failed(\"invept failed: ext=0x%lx eptp=%llx gpa=0x%llx\\n\",\n\t\t\text, eptp, gpa);\n}\n\nstatic DEFINE_PER_CPU(struct vmcs *, vmxarea);\nDEFINE_PER_CPU(struct vmcs *, current_vmcs);\n \nstatic DEFINE_PER_CPU(struct list_head, loaded_vmcss_on_cpu);\n\nstatic DECLARE_BITMAP(vmx_vpid_bitmap, VMX_NR_VPIDS);\nstatic DEFINE_SPINLOCK(vmx_vpid_lock);\n\nstruct vmcs_config vmcs_config __ro_after_init;\nstruct vmx_capability vmx_capability __ro_after_init;\n\n#define VMX_SEGMENT_FIELD(seg)\t\t\t\t\t\\\n\t[VCPU_SREG_##seg] = {                                   \\\n\t\t.selector = GUEST_##seg##_SELECTOR,\t\t\\\n\t\t.base = GUEST_##seg##_BASE,\t\t   \t\\\n\t\t.limit = GUEST_##seg##_LIMIT,\t\t   \t\\\n\t\t.ar_bytes = GUEST_##seg##_AR_BYTES,\t   \t\\\n\t}\n\nstatic const struct kvm_vmx_segment_field {\n\tunsigned selector;\n\tunsigned base;\n\tunsigned limit;\n\tunsigned ar_bytes;\n} kvm_vmx_segment_fields[] = {\n\tVMX_SEGMENT_FIELD(CS),\n\tVMX_SEGMENT_FIELD(DS),\n\tVMX_SEGMENT_FIELD(ES),\n\tVMX_SEGMENT_FIELD(FS),\n\tVMX_SEGMENT_FIELD(GS),\n\tVMX_SEGMENT_FIELD(SS),\n\tVMX_SEGMENT_FIELD(TR),\n\tVMX_SEGMENT_FIELD(LDTR),\n};\n\nstatic inline void vmx_segment_cache_clear(struct vcpu_vmx *vmx)\n{\n\tvmx->segment_cache.bitmask = 0;\n}\n\nstatic unsigned long host_idt_base;\n\n#if IS_ENABLED(CONFIG_HYPERV)\nstatic struct kvm_x86_ops vmx_x86_ops __initdata;\n\nstatic bool __read_mostly enlightened_vmcs = true;\nmodule_param(enlightened_vmcs, bool, 0444);\n\nstatic int hv_enable_l2_tlb_flush(struct kvm_vcpu *vcpu)\n{\n\tstruct hv_enlightened_vmcs *evmcs;\n\tstruct hv_partition_assist_pg **p_hv_pa_pg =\n\t\t\t&to_kvm_hv(vcpu->kvm)->hv_pa_pg;\n\t \n\tif (!*p_hv_pa_pg)\n\t\t*p_hv_pa_pg = kzalloc(PAGE_SIZE, GFP_KERNEL_ACCOUNT);\n\n\tif (!*p_hv_pa_pg)\n\t\treturn -ENOMEM;\n\n\tevmcs = (struct hv_enlightened_vmcs *)to_vmx(vcpu)->loaded_vmcs->vmcs;\n\n\tevmcs->partition_assist_page =\n\t\t__pa(*p_hv_pa_pg);\n\tevmcs->hv_vm_id = (unsigned long)vcpu->kvm;\n\tevmcs->hv_enlightenments_control.nested_flush_hypercall = 1;\n\n\treturn 0;\n}\n\nstatic __init void hv_init_evmcs(void)\n{\n\tint cpu;\n\n\tif (!enlightened_vmcs)\n\t\treturn;\n\n\t \n\tif (ms_hyperv.hints & HV_X64_ENLIGHTENED_VMCS_RECOMMENDED &&\n\t    (ms_hyperv.nested_features & HV_X64_ENLIGHTENED_VMCS_VERSION) >=\n\t     KVM_EVMCS_VERSION) {\n\n\t\t \n\t\tfor_each_online_cpu(cpu) {\n\t\t\tif (!hv_get_vp_assist_page(cpu)) {\n\t\t\t\tenlightened_vmcs = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (enlightened_vmcs) {\n\t\t\tpr_info(\"Using Hyper-V Enlightened VMCS\\n\");\n\t\t\tstatic_branch_enable(&__kvm_is_using_evmcs);\n\t\t}\n\n\t\tif (ms_hyperv.nested_features & HV_X64_NESTED_DIRECT_FLUSH)\n\t\t\tvmx_x86_ops.enable_l2_tlb_flush\n\t\t\t\t= hv_enable_l2_tlb_flush;\n\n\t} else {\n\t\tenlightened_vmcs = false;\n\t}\n}\n\nstatic void hv_reset_evmcs(void)\n{\n\tstruct hv_vp_assist_page *vp_ap;\n\n\tif (!kvm_is_using_evmcs())\n\t\treturn;\n\n\t \n\tvp_ap = hv_get_vp_assist_page(smp_processor_id());\n\tif (WARN_ON_ONCE(!vp_ap))\n\t\treturn;\n\n\t \n\tvp_ap->nested_control.features.directhypercall = 0;\n\tvp_ap->current_nested_vmcs = 0;\n\tvp_ap->enlighten_vmentry = 0;\n}\n\n#else  \nstatic void hv_init_evmcs(void) {}\nstatic void hv_reset_evmcs(void) {}\n#endif  \n\n \nstatic u32 vmx_preemption_cpu_tfms[] = {\n \n0x000206E6,\n \n \n \n0x00020652,\n \n0x00020655,\n \n \n \n0x000106E5,\n \n0x000106A0,\n \n0x000106A1,\n \n0x000106A4,\n  \n  \n  \n0x000106A5,\n  \n0x000306A8,\n};\n\nstatic inline bool cpu_has_broken_vmx_preemption_timer(void)\n{\n\tu32 eax = cpuid_eax(0x00000001), i;\n\n\t \n\teax &= ~(0x3U << 14 | 0xfU << 28);\n\tfor (i = 0; i < ARRAY_SIZE(vmx_preemption_cpu_tfms); i++)\n\t\tif (eax == vmx_preemption_cpu_tfms[i])\n\t\t\treturn true;\n\n\treturn false;\n}\n\nstatic inline bool cpu_need_virtualize_apic_accesses(struct kvm_vcpu *vcpu)\n{\n\treturn flexpriority_enabled && lapic_in_kernel(vcpu);\n}\n\nstatic int possible_passthrough_msr_slot(u32 msr)\n{\n\tu32 i;\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_possible_passthrough_msrs); i++)\n\t\tif (vmx_possible_passthrough_msrs[i] == msr)\n\t\t\treturn i;\n\n\treturn -ENOENT;\n}\n\nstatic bool is_valid_passthrough_msr(u32 msr)\n{\n\tbool r;\n\n\tswitch (msr) {\n\tcase 0x800 ... 0x8ff:\n\t\t \n\t\treturn true;\n\tcase MSR_IA32_RTIT_STATUS:\n\tcase MSR_IA32_RTIT_OUTPUT_BASE:\n\tcase MSR_IA32_RTIT_OUTPUT_MASK:\n\tcase MSR_IA32_RTIT_CR3_MATCH:\n\tcase MSR_IA32_RTIT_ADDR0_A ... MSR_IA32_RTIT_ADDR3_B:\n\t\t \n\tcase MSR_LBR_SELECT:\n\tcase MSR_LBR_TOS:\n\tcase MSR_LBR_INFO_0 ... MSR_LBR_INFO_0 + 31:\n\tcase MSR_LBR_NHM_FROM ... MSR_LBR_NHM_FROM + 31:\n\tcase MSR_LBR_NHM_TO ... MSR_LBR_NHM_TO + 31:\n\tcase MSR_LBR_CORE_FROM ... MSR_LBR_CORE_FROM + 8:\n\tcase MSR_LBR_CORE_TO ... MSR_LBR_CORE_TO + 8:\n\t\t \n\t\treturn true;\n\t}\n\n\tr = possible_passthrough_msr_slot(msr) != -ENOENT;\n\n\tWARN(!r, \"Invalid MSR %x, please adapt vmx_possible_passthrough_msrs[]\", msr);\n\n\treturn r;\n}\n\nstruct vmx_uret_msr *vmx_find_uret_msr(struct vcpu_vmx *vmx, u32 msr)\n{\n\tint i;\n\n\ti = kvm_find_user_return_msr(msr);\n\tif (i >= 0)\n\t\treturn &vmx->guest_uret_msrs[i];\n\treturn NULL;\n}\n\nstatic int vmx_set_guest_uret_msr(struct vcpu_vmx *vmx,\n\t\t\t\t  struct vmx_uret_msr *msr, u64 data)\n{\n\tunsigned int slot = msr - vmx->guest_uret_msrs;\n\tint ret = 0;\n\n\tif (msr->load_into_hardware) {\n\t\tpreempt_disable();\n\t\tret = kvm_set_user_return_msr(slot, data, msr->mask);\n\t\tpreempt_enable();\n\t}\n\tif (!ret)\n\t\tmsr->data = data;\n\treturn ret;\n}\n\n \nstatic int kvm_cpu_vmxoff(void)\n{\n\tasm_volatile_goto(\"1: vmxoff\\n\\t\"\n\t\t\t  _ASM_EXTABLE(1b, %l[fault])\n\t\t\t  ::: \"cc\", \"memory\" : fault);\n\n\tcr4_clear_bits(X86_CR4_VMXE);\n\treturn 0;\n\nfault:\n\tcr4_clear_bits(X86_CR4_VMXE);\n\treturn -EIO;\n}\n\nstatic void vmx_emergency_disable(void)\n{\n\tint cpu = raw_smp_processor_id();\n\tstruct loaded_vmcs *v;\n\n\tkvm_rebooting = true;\n\n\t \n\tif (!(__read_cr4() & X86_CR4_VMXE))\n\t\treturn;\n\n\tlist_for_each_entry(v, &per_cpu(loaded_vmcss_on_cpu, cpu),\n\t\t\t    loaded_vmcss_on_cpu_link)\n\t\tvmcs_clear(v->vmcs);\n\n\tkvm_cpu_vmxoff();\n}\n\nstatic void __loaded_vmcs_clear(void *arg)\n{\n\tstruct loaded_vmcs *loaded_vmcs = arg;\n\tint cpu = raw_smp_processor_id();\n\n\tif (loaded_vmcs->cpu != cpu)\n\t\treturn;  \n\tif (per_cpu(current_vmcs, cpu) == loaded_vmcs->vmcs)\n\t\tper_cpu(current_vmcs, cpu) = NULL;\n\n\tvmcs_clear(loaded_vmcs->vmcs);\n\tif (loaded_vmcs->shadow_vmcs && loaded_vmcs->launched)\n\t\tvmcs_clear(loaded_vmcs->shadow_vmcs);\n\n\tlist_del(&loaded_vmcs->loaded_vmcss_on_cpu_link);\n\n\t \n\tsmp_wmb();\n\n\tloaded_vmcs->cpu = -1;\n\tloaded_vmcs->launched = 0;\n}\n\nvoid loaded_vmcs_clear(struct loaded_vmcs *loaded_vmcs)\n{\n\tint cpu = loaded_vmcs->cpu;\n\n\tif (cpu != -1)\n\t\tsmp_call_function_single(cpu,\n\t\t\t __loaded_vmcs_clear, loaded_vmcs, 1);\n}\n\nstatic bool vmx_segment_cache_test_set(struct vcpu_vmx *vmx, unsigned seg,\n\t\t\t\t       unsigned field)\n{\n\tbool ret;\n\tu32 mask = 1 << (seg * SEG_FIELD_NR + field);\n\n\tif (!kvm_register_is_available(&vmx->vcpu, VCPU_EXREG_SEGMENTS)) {\n\t\tkvm_register_mark_available(&vmx->vcpu, VCPU_EXREG_SEGMENTS);\n\t\tvmx->segment_cache.bitmask = 0;\n\t}\n\tret = vmx->segment_cache.bitmask & mask;\n\tvmx->segment_cache.bitmask |= mask;\n\treturn ret;\n}\n\nstatic u16 vmx_read_guest_seg_selector(struct vcpu_vmx *vmx, unsigned seg)\n{\n\tu16 *p = &vmx->segment_cache.seg[seg].selector;\n\n\tif (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_SEL))\n\t\t*p = vmcs_read16(kvm_vmx_segment_fields[seg].selector);\n\treturn *p;\n}\n\nstatic ulong vmx_read_guest_seg_base(struct vcpu_vmx *vmx, unsigned seg)\n{\n\tulong *p = &vmx->segment_cache.seg[seg].base;\n\n\tif (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_BASE))\n\t\t*p = vmcs_readl(kvm_vmx_segment_fields[seg].base);\n\treturn *p;\n}\n\nstatic u32 vmx_read_guest_seg_limit(struct vcpu_vmx *vmx, unsigned seg)\n{\n\tu32 *p = &vmx->segment_cache.seg[seg].limit;\n\n\tif (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_LIMIT))\n\t\t*p = vmcs_read32(kvm_vmx_segment_fields[seg].limit);\n\treturn *p;\n}\n\nstatic u32 vmx_read_guest_seg_ar(struct vcpu_vmx *vmx, unsigned seg)\n{\n\tu32 *p = &vmx->segment_cache.seg[seg].ar;\n\n\tif (!vmx_segment_cache_test_set(vmx, seg, SEG_FIELD_AR))\n\t\t*p = vmcs_read32(kvm_vmx_segment_fields[seg].ar_bytes);\n\treturn *p;\n}\n\nvoid vmx_update_exception_bitmap(struct kvm_vcpu *vcpu)\n{\n\tu32 eb;\n\n\teb = (1u << PF_VECTOR) | (1u << UD_VECTOR) | (1u << MC_VECTOR) |\n\t     (1u << DB_VECTOR) | (1u << AC_VECTOR);\n\t \n\tif (enable_vmware_backdoor)\n\t\teb |= (1u << GP_VECTOR);\n\tif ((vcpu->guest_debug &\n\t     (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP)) ==\n\t    (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP))\n\t\teb |= 1u << BP_VECTOR;\n\tif (to_vmx(vcpu)->rmode.vm86_active)\n\t\teb = ~0;\n\tif (!vmx_need_pf_intercept(vcpu))\n\t\teb &= ~(1u << PF_VECTOR);\n\n\t \n\tif (is_guest_mode(vcpu))\n\t\teb |= get_vmcs12(vcpu)->exception_bitmap;\n\telse {\n\t\tint mask = 0, match = 0;\n\n\t\tif (enable_ept && (eb & (1u << PF_VECTOR))) {\n\t\t\t \n\t\t\tmask = PFERR_PRESENT_MASK | PFERR_RSVD_MASK;\n\t\t\tmatch = PFERR_PRESENT_MASK;\n\t\t}\n\t\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, mask);\n\t\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, match);\n\t}\n\n\t \n\tif (vcpu->arch.xfd_no_write_intercept)\n\t\teb |= (1u << NM_VECTOR);\n\n\tvmcs_write32(EXCEPTION_BITMAP, eb);\n}\n\n \nstatic bool msr_write_intercepted(struct vcpu_vmx *vmx, u32 msr)\n{\n\tif (!(exec_controls_get(vmx) & CPU_BASED_USE_MSR_BITMAPS))\n\t\treturn true;\n\n\treturn vmx_test_msr_bitmap_write(vmx->loaded_vmcs->msr_bitmap, msr);\n}\n\nunsigned int __vmx_vcpu_run_flags(struct vcpu_vmx *vmx)\n{\n\tunsigned int flags = 0;\n\n\tif (vmx->loaded_vmcs->launched)\n\t\tflags |= VMX_RUN_VMRESUME;\n\n\t \n\tif (!msr_write_intercepted(vmx, MSR_IA32_SPEC_CTRL))\n\t\tflags |= VMX_RUN_SAVE_SPEC_CTRL;\n\n\treturn flags;\n}\n\nstatic __always_inline void clear_atomic_switch_msr_special(struct vcpu_vmx *vmx,\n\t\tunsigned long entry, unsigned long exit)\n{\n\tvm_entry_controls_clearbit(vmx, entry);\n\tvm_exit_controls_clearbit(vmx, exit);\n}\n\nint vmx_find_loadstore_msr_slot(struct vmx_msrs *m, u32 msr)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < m->nr; ++i) {\n\t\tif (m->val[i].index == msr)\n\t\t\treturn i;\n\t}\n\treturn -ENOENT;\n}\n\nstatic void clear_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr)\n{\n\tint i;\n\tstruct msr_autoload *m = &vmx->msr_autoload;\n\n\tswitch (msr) {\n\tcase MSR_EFER:\n\t\tif (cpu_has_load_ia32_efer()) {\n\t\t\tclear_atomic_switch_msr_special(vmx,\n\t\t\t\t\tVM_ENTRY_LOAD_IA32_EFER,\n\t\t\t\t\tVM_EXIT_LOAD_IA32_EFER);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\tcase MSR_CORE_PERF_GLOBAL_CTRL:\n\t\tif (cpu_has_load_perf_global_ctrl()) {\n\t\t\tclear_atomic_switch_msr_special(vmx,\n\t\t\t\t\tVM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL,\n\t\t\t\t\tVM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\t}\n\ti = vmx_find_loadstore_msr_slot(&m->guest, msr);\n\tif (i < 0)\n\t\tgoto skip_guest;\n\t--m->guest.nr;\n\tm->guest.val[i] = m->guest.val[m->guest.nr];\n\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->guest.nr);\n\nskip_guest:\n\ti = vmx_find_loadstore_msr_slot(&m->host, msr);\n\tif (i < 0)\n\t\treturn;\n\n\t--m->host.nr;\n\tm->host.val[i] = m->host.val[m->host.nr];\n\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->host.nr);\n}\n\nstatic __always_inline void add_atomic_switch_msr_special(struct vcpu_vmx *vmx,\n\t\tunsigned long entry, unsigned long exit,\n\t\tunsigned long guest_val_vmcs, unsigned long host_val_vmcs,\n\t\tu64 guest_val, u64 host_val)\n{\n\tvmcs_write64(guest_val_vmcs, guest_val);\n\tif (host_val_vmcs != HOST_IA32_EFER)\n\t\tvmcs_write64(host_val_vmcs, host_val);\n\tvm_entry_controls_setbit(vmx, entry);\n\tvm_exit_controls_setbit(vmx, exit);\n}\n\nstatic void add_atomic_switch_msr(struct vcpu_vmx *vmx, unsigned msr,\n\t\t\t\t  u64 guest_val, u64 host_val, bool entry_only)\n{\n\tint i, j = 0;\n\tstruct msr_autoload *m = &vmx->msr_autoload;\n\n\tswitch (msr) {\n\tcase MSR_EFER:\n\t\tif (cpu_has_load_ia32_efer()) {\n\t\t\tadd_atomic_switch_msr_special(vmx,\n\t\t\t\t\tVM_ENTRY_LOAD_IA32_EFER,\n\t\t\t\t\tVM_EXIT_LOAD_IA32_EFER,\n\t\t\t\t\tGUEST_IA32_EFER,\n\t\t\t\t\tHOST_IA32_EFER,\n\t\t\t\t\tguest_val, host_val);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\tcase MSR_CORE_PERF_GLOBAL_CTRL:\n\t\tif (cpu_has_load_perf_global_ctrl()) {\n\t\t\tadd_atomic_switch_msr_special(vmx,\n\t\t\t\t\tVM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL,\n\t\t\t\t\tVM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL,\n\t\t\t\t\tGUEST_IA32_PERF_GLOBAL_CTRL,\n\t\t\t\t\tHOST_IA32_PERF_GLOBAL_CTRL,\n\t\t\t\t\tguest_val, host_val);\n\t\t\treturn;\n\t\t}\n\t\tbreak;\n\tcase MSR_IA32_PEBS_ENABLE:\n\t\t \n\t\twrmsrl(MSR_IA32_PEBS_ENABLE, 0);\n\t}\n\n\ti = vmx_find_loadstore_msr_slot(&m->guest, msr);\n\tif (!entry_only)\n\t\tj = vmx_find_loadstore_msr_slot(&m->host, msr);\n\n\tif ((i < 0 && m->guest.nr == MAX_NR_LOADSTORE_MSRS) ||\n\t    (j < 0 &&  m->host.nr == MAX_NR_LOADSTORE_MSRS)) {\n\t\tprintk_once(KERN_WARNING \"Not enough msr switch entries. \"\n\t\t\t\t\"Can't add msr %x\\n\", msr);\n\t\treturn;\n\t}\n\tif (i < 0) {\n\t\ti = m->guest.nr++;\n\t\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, m->guest.nr);\n\t}\n\tm->guest.val[i].index = msr;\n\tm->guest.val[i].value = guest_val;\n\n\tif (entry_only)\n\t\treturn;\n\n\tif (j < 0) {\n\t\tj = m->host.nr++;\n\t\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, m->host.nr);\n\t}\n\tm->host.val[j].index = msr;\n\tm->host.val[j].value = host_val;\n}\n\nstatic bool update_transition_efer(struct vcpu_vmx *vmx)\n{\n\tu64 guest_efer = vmx->vcpu.arch.efer;\n\tu64 ignore_bits = 0;\n\tint i;\n\n\t \n\tif (!enable_ept)\n\t\tguest_efer |= EFER_NX;\n\n\t \n\tignore_bits |= EFER_SCE;\n#ifdef CONFIG_X86_64\n\tignore_bits |= EFER_LMA | EFER_LME;\n\t \n\tif (guest_efer & EFER_LMA)\n\t\tignore_bits &= ~(u64)EFER_SCE;\n#endif\n\n\t \n\tif (cpu_has_load_ia32_efer() ||\n\t    (enable_ept && ((vmx->vcpu.arch.efer ^ host_efer) & EFER_NX))) {\n\t\tif (!(guest_efer & EFER_LMA))\n\t\t\tguest_efer &= ~EFER_LME;\n\t\tif (guest_efer != host_efer)\n\t\t\tadd_atomic_switch_msr(vmx, MSR_EFER,\n\t\t\t\t\t      guest_efer, host_efer, false);\n\t\telse\n\t\t\tclear_atomic_switch_msr(vmx, MSR_EFER);\n\t\treturn false;\n\t}\n\n\ti = kvm_find_user_return_msr(MSR_EFER);\n\tif (i < 0)\n\t\treturn false;\n\n\tclear_atomic_switch_msr(vmx, MSR_EFER);\n\n\tguest_efer &= ~ignore_bits;\n\tguest_efer |= host_efer & ignore_bits;\n\n\tvmx->guest_uret_msrs[i].data = guest_efer;\n\tvmx->guest_uret_msrs[i].mask = ~ignore_bits;\n\n\treturn true;\n}\n\n#ifdef CONFIG_X86_32\n \nstatic unsigned long segment_base(u16 selector)\n{\n\tstruct desc_struct *table;\n\tunsigned long v;\n\n\tif (!(selector & ~SEGMENT_RPL_MASK))\n\t\treturn 0;\n\n\ttable = get_current_gdt_ro();\n\n\tif ((selector & SEGMENT_TI_MASK) == SEGMENT_LDT) {\n\t\tu16 ldt_selector = kvm_read_ldt();\n\n\t\tif (!(ldt_selector & ~SEGMENT_RPL_MASK))\n\t\t\treturn 0;\n\n\t\ttable = (struct desc_struct *)segment_base(ldt_selector);\n\t}\n\tv = get_desc_base(&table[selector >> 3]);\n\treturn v;\n}\n#endif\n\nstatic inline bool pt_can_write_msr(struct vcpu_vmx *vmx)\n{\n\treturn vmx_pt_mode_is_host_guest() &&\n\t       !(vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN);\n}\n\nstatic inline bool pt_output_base_valid(struct kvm_vcpu *vcpu, u64 base)\n{\n\t \n\treturn kvm_vcpu_is_legal_aligned_gpa(vcpu, base, 128);\n}\n\nstatic inline void pt_load_msr(struct pt_ctx *ctx, u32 addr_range)\n{\n\tu32 i;\n\n\twrmsrl(MSR_IA32_RTIT_STATUS, ctx->status);\n\twrmsrl(MSR_IA32_RTIT_OUTPUT_BASE, ctx->output_base);\n\twrmsrl(MSR_IA32_RTIT_OUTPUT_MASK, ctx->output_mask);\n\twrmsrl(MSR_IA32_RTIT_CR3_MATCH, ctx->cr3_match);\n\tfor (i = 0; i < addr_range; i++) {\n\t\twrmsrl(MSR_IA32_RTIT_ADDR0_A + i * 2, ctx->addr_a[i]);\n\t\twrmsrl(MSR_IA32_RTIT_ADDR0_B + i * 2, ctx->addr_b[i]);\n\t}\n}\n\nstatic inline void pt_save_msr(struct pt_ctx *ctx, u32 addr_range)\n{\n\tu32 i;\n\n\trdmsrl(MSR_IA32_RTIT_STATUS, ctx->status);\n\trdmsrl(MSR_IA32_RTIT_OUTPUT_BASE, ctx->output_base);\n\trdmsrl(MSR_IA32_RTIT_OUTPUT_MASK, ctx->output_mask);\n\trdmsrl(MSR_IA32_RTIT_CR3_MATCH, ctx->cr3_match);\n\tfor (i = 0; i < addr_range; i++) {\n\t\trdmsrl(MSR_IA32_RTIT_ADDR0_A + i * 2, ctx->addr_a[i]);\n\t\trdmsrl(MSR_IA32_RTIT_ADDR0_B + i * 2, ctx->addr_b[i]);\n\t}\n}\n\nstatic void pt_guest_enter(struct vcpu_vmx *vmx)\n{\n\tif (vmx_pt_mode_is_system())\n\t\treturn;\n\n\t \n\trdmsrl(MSR_IA32_RTIT_CTL, vmx->pt_desc.host.ctl);\n\tif (vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN) {\n\t\twrmsrl(MSR_IA32_RTIT_CTL, 0);\n\t\tpt_save_msr(&vmx->pt_desc.host, vmx->pt_desc.num_address_ranges);\n\t\tpt_load_msr(&vmx->pt_desc.guest, vmx->pt_desc.num_address_ranges);\n\t}\n}\n\nstatic void pt_guest_exit(struct vcpu_vmx *vmx)\n{\n\tif (vmx_pt_mode_is_system())\n\t\treturn;\n\n\tif (vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN) {\n\t\tpt_save_msr(&vmx->pt_desc.guest, vmx->pt_desc.num_address_ranges);\n\t\tpt_load_msr(&vmx->pt_desc.host, vmx->pt_desc.num_address_ranges);\n\t}\n\n\t \n\tif (vmx->pt_desc.host.ctl)\n\t\twrmsrl(MSR_IA32_RTIT_CTL, vmx->pt_desc.host.ctl);\n}\n\nvoid vmx_set_host_fs_gs(struct vmcs_host_state *host, u16 fs_sel, u16 gs_sel,\n\t\t\tunsigned long fs_base, unsigned long gs_base)\n{\n\tif (unlikely(fs_sel != host->fs_sel)) {\n\t\tif (!(fs_sel & 7))\n\t\t\tvmcs_write16(HOST_FS_SELECTOR, fs_sel);\n\t\telse\n\t\t\tvmcs_write16(HOST_FS_SELECTOR, 0);\n\t\thost->fs_sel = fs_sel;\n\t}\n\tif (unlikely(gs_sel != host->gs_sel)) {\n\t\tif (!(gs_sel & 7))\n\t\t\tvmcs_write16(HOST_GS_SELECTOR, gs_sel);\n\t\telse\n\t\t\tvmcs_write16(HOST_GS_SELECTOR, 0);\n\t\thost->gs_sel = gs_sel;\n\t}\n\tif (unlikely(fs_base != host->fs_base)) {\n\t\tvmcs_writel(HOST_FS_BASE, fs_base);\n\t\thost->fs_base = fs_base;\n\t}\n\tif (unlikely(gs_base != host->gs_base)) {\n\t\tvmcs_writel(HOST_GS_BASE, gs_base);\n\t\thost->gs_base = gs_base;\n\t}\n}\n\nvoid vmx_prepare_switch_to_guest(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs_host_state *host_state;\n#ifdef CONFIG_X86_64\n\tint cpu = raw_smp_processor_id();\n#endif\n\tunsigned long fs_base, gs_base;\n\tu16 fs_sel, gs_sel;\n\tint i;\n\n\tvmx->req_immediate_exit = false;\n\n\t \n\tif (!vmx->guest_uret_msrs_loaded) {\n\t\tvmx->guest_uret_msrs_loaded = true;\n\t\tfor (i = 0; i < kvm_nr_uret_msrs; ++i) {\n\t\t\tif (!vmx->guest_uret_msrs[i].load_into_hardware)\n\t\t\t\tcontinue;\n\n\t\t\tkvm_set_user_return_msr(i,\n\t\t\t\t\t\tvmx->guest_uret_msrs[i].data,\n\t\t\t\t\t\tvmx->guest_uret_msrs[i].mask);\n\t\t}\n\t}\n\n\tif (vmx->nested.need_vmcs12_to_shadow_sync)\n\t\tnested_sync_vmcs12_to_shadow(vcpu);\n\n\tif (vmx->guest_state_loaded)\n\t\treturn;\n\n\thost_state = &vmx->loaded_vmcs->host_state;\n\n\t \n\thost_state->ldt_sel = kvm_read_ldt();\n\n#ifdef CONFIG_X86_64\n\tsavesegment(ds, host_state->ds_sel);\n\tsavesegment(es, host_state->es_sel);\n\n\tgs_base = cpu_kernelmode_gs_base(cpu);\n\tif (likely(is_64bit_mm(current->mm))) {\n\t\tcurrent_save_fsgs();\n\t\tfs_sel = current->thread.fsindex;\n\t\tgs_sel = current->thread.gsindex;\n\t\tfs_base = current->thread.fsbase;\n\t\tvmx->msr_host_kernel_gs_base = current->thread.gsbase;\n\t} else {\n\t\tsavesegment(fs, fs_sel);\n\t\tsavesegment(gs, gs_sel);\n\t\tfs_base = read_msr(MSR_FS_BASE);\n\t\tvmx->msr_host_kernel_gs_base = read_msr(MSR_KERNEL_GS_BASE);\n\t}\n\n\twrmsrl(MSR_KERNEL_GS_BASE, vmx->msr_guest_kernel_gs_base);\n#else\n\tsavesegment(fs, fs_sel);\n\tsavesegment(gs, gs_sel);\n\tfs_base = segment_base(fs_sel);\n\tgs_base = segment_base(gs_sel);\n#endif\n\n\tvmx_set_host_fs_gs(host_state, fs_sel, gs_sel, fs_base, gs_base);\n\tvmx->guest_state_loaded = true;\n}\n\nstatic void vmx_prepare_switch_to_host(struct vcpu_vmx *vmx)\n{\n\tstruct vmcs_host_state *host_state;\n\n\tif (!vmx->guest_state_loaded)\n\t\treturn;\n\n\thost_state = &vmx->loaded_vmcs->host_state;\n\n\t++vmx->vcpu.stat.host_state_reload;\n\n#ifdef CONFIG_X86_64\n\trdmsrl(MSR_KERNEL_GS_BASE, vmx->msr_guest_kernel_gs_base);\n#endif\n\tif (host_state->ldt_sel || (host_state->gs_sel & 7)) {\n\t\tkvm_load_ldt(host_state->ldt_sel);\n#ifdef CONFIG_X86_64\n\t\tload_gs_index(host_state->gs_sel);\n#else\n\t\tloadsegment(gs, host_state->gs_sel);\n#endif\n\t}\n\tif (host_state->fs_sel & 7)\n\t\tloadsegment(fs, host_state->fs_sel);\n#ifdef CONFIG_X86_64\n\tif (unlikely(host_state->ds_sel | host_state->es_sel)) {\n\t\tloadsegment(ds, host_state->ds_sel);\n\t\tloadsegment(es, host_state->es_sel);\n\t}\n#endif\n\tinvalidate_tss_limit();\n#ifdef CONFIG_X86_64\n\twrmsrl(MSR_KERNEL_GS_BASE, vmx->msr_host_kernel_gs_base);\n#endif\n\tload_fixmap_gdt(raw_smp_processor_id());\n\tvmx->guest_state_loaded = false;\n\tvmx->guest_uret_msrs_loaded = false;\n}\n\n#ifdef CONFIG_X86_64\nstatic u64 vmx_read_guest_kernel_gs_base(struct vcpu_vmx *vmx)\n{\n\tpreempt_disable();\n\tif (vmx->guest_state_loaded)\n\t\trdmsrl(MSR_KERNEL_GS_BASE, vmx->msr_guest_kernel_gs_base);\n\tpreempt_enable();\n\treturn vmx->msr_guest_kernel_gs_base;\n}\n\nstatic void vmx_write_guest_kernel_gs_base(struct vcpu_vmx *vmx, u64 data)\n{\n\tpreempt_disable();\n\tif (vmx->guest_state_loaded)\n\t\twrmsrl(MSR_KERNEL_GS_BASE, data);\n\tpreempt_enable();\n\tvmx->msr_guest_kernel_gs_base = data;\n}\n#endif\n\nvoid vmx_vcpu_load_vmcs(struct kvm_vcpu *vcpu, int cpu,\n\t\t\tstruct loaded_vmcs *buddy)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tbool already_loaded = vmx->loaded_vmcs->cpu == cpu;\n\tstruct vmcs *prev;\n\n\tif (!already_loaded) {\n\t\tloaded_vmcs_clear(vmx->loaded_vmcs);\n\t\tlocal_irq_disable();\n\n\t\t \n\t\tsmp_rmb();\n\n\t\tlist_add(&vmx->loaded_vmcs->loaded_vmcss_on_cpu_link,\n\t\t\t &per_cpu(loaded_vmcss_on_cpu, cpu));\n\t\tlocal_irq_enable();\n\t}\n\n\tprev = per_cpu(current_vmcs, cpu);\n\tif (prev != vmx->loaded_vmcs->vmcs) {\n\t\tper_cpu(current_vmcs, cpu) = vmx->loaded_vmcs->vmcs;\n\t\tvmcs_load(vmx->loaded_vmcs->vmcs);\n\n\t\t \n\t\tif (!buddy || WARN_ON_ONCE(buddy->vmcs != prev))\n\t\t\tindirect_branch_prediction_barrier();\n\t}\n\n\tif (!already_loaded) {\n\t\tvoid *gdt = get_current_gdt_ro();\n\n\t\t \n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);\n\n\t\t \n\t\tvmcs_writel(HOST_TR_BASE,\n\t\t\t    (unsigned long)&get_cpu_entry_area(cpu)->tss.x86_tss);\n\t\tvmcs_writel(HOST_GDTR_BASE, (unsigned long)gdt);    \n\n\t\tif (IS_ENABLED(CONFIG_IA32_EMULATION) || IS_ENABLED(CONFIG_X86_32)) {\n\t\t\t \n\t\t\tvmcs_writel(HOST_IA32_SYSENTER_ESP,\n\t\t\t\t    (unsigned long)(cpu_entry_stack(cpu) + 1));\n\t\t}\n\n\t\tvmx->loaded_vmcs->cpu = cpu;\n\t}\n}\n\n \nstatic void vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tvmx_vcpu_load_vmcs(vcpu, cpu, NULL);\n\n\tvmx_vcpu_pi_load(vcpu, cpu);\n\n\tvmx->host_debugctlmsr = get_debugctlmsr();\n}\n\nstatic void vmx_vcpu_put(struct kvm_vcpu *vcpu)\n{\n\tvmx_vcpu_pi_put(vcpu);\n\n\tvmx_prepare_switch_to_host(to_vmx(vcpu));\n}\n\nbool vmx_emulation_required(struct kvm_vcpu *vcpu)\n{\n\treturn emulate_invalid_guest_state && !vmx_guest_state_valid(vcpu);\n}\n\nunsigned long vmx_get_rflags(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long rflags, save_rflags;\n\n\tif (!kvm_register_is_available(vcpu, VCPU_EXREG_RFLAGS)) {\n\t\tkvm_register_mark_available(vcpu, VCPU_EXREG_RFLAGS);\n\t\trflags = vmcs_readl(GUEST_RFLAGS);\n\t\tif (vmx->rmode.vm86_active) {\n\t\t\trflags &= RMODE_GUEST_OWNED_EFLAGS_BITS;\n\t\t\tsave_rflags = vmx->rmode.save_rflags;\n\t\t\trflags |= save_rflags & ~RMODE_GUEST_OWNED_EFLAGS_BITS;\n\t\t}\n\t\tvmx->rflags = rflags;\n\t}\n\treturn vmx->rflags;\n}\n\nvoid vmx_set_rflags(struct kvm_vcpu *vcpu, unsigned long rflags)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long old_rflags;\n\n\t \n\tif (is_unrestricted_guest(vcpu)) {\n\t\tkvm_register_mark_available(vcpu, VCPU_EXREG_RFLAGS);\n\t\tvmx->rflags = rflags;\n\t\tvmcs_writel(GUEST_RFLAGS, rflags);\n\t\treturn;\n\t}\n\n\told_rflags = vmx_get_rflags(vcpu);\n\tvmx->rflags = rflags;\n\tif (vmx->rmode.vm86_active) {\n\t\tvmx->rmode.save_rflags = rflags;\n\t\trflags |= X86_EFLAGS_IOPL | X86_EFLAGS_VM;\n\t}\n\tvmcs_writel(GUEST_RFLAGS, rflags);\n\n\tif ((old_rflags ^ vmx->rflags) & X86_EFLAGS_VM)\n\t\tvmx->emulation_required = vmx_emulation_required(vcpu);\n}\n\nstatic bool vmx_get_if_flag(struct kvm_vcpu *vcpu)\n{\n\treturn vmx_get_rflags(vcpu) & X86_EFLAGS_IF;\n}\n\nu32 vmx_get_interrupt_shadow(struct kvm_vcpu *vcpu)\n{\n\tu32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);\n\tint ret = 0;\n\n\tif (interruptibility & GUEST_INTR_STATE_STI)\n\t\tret |= KVM_X86_SHADOW_INT_STI;\n\tif (interruptibility & GUEST_INTR_STATE_MOV_SS)\n\t\tret |= KVM_X86_SHADOW_INT_MOV_SS;\n\n\treturn ret;\n}\n\nvoid vmx_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask)\n{\n\tu32 interruptibility_old = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);\n\tu32 interruptibility = interruptibility_old;\n\n\tinterruptibility &= ~(GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS);\n\n\tif (mask & KVM_X86_SHADOW_INT_MOV_SS)\n\t\tinterruptibility |= GUEST_INTR_STATE_MOV_SS;\n\telse if (mask & KVM_X86_SHADOW_INT_STI)\n\t\tinterruptibility |= GUEST_INTR_STATE_STI;\n\n\tif ((interruptibility != interruptibility_old))\n\t\tvmcs_write32(GUEST_INTERRUPTIBILITY_INFO, interruptibility);\n}\n\nstatic int vmx_rtit_ctl_check(struct kvm_vcpu *vcpu, u64 data)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long value;\n\n\t \n\tif (data & vmx->pt_desc.ctl_bitmask)\n\t\treturn 1;\n\n\t \n\tif ((vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN) &&\n\t\t((vmx->pt_desc.guest.ctl ^ data) & ~RTIT_CTL_TRACEEN))\n\t\treturn 1;\n\n\t \n\tif ((data & RTIT_CTL_TRACEEN) && !(data & RTIT_CTL_TOPA) &&\n\t\t!(data & RTIT_CTL_FABRIC_EN) &&\n\t\t!intel_pt_validate_cap(vmx->pt_desc.caps,\n\t\t\t\t\tPT_CAP_single_range_output))\n\t\treturn 1;\n\n\t \n\tvalue = intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_mtc_periods);\n\tif (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_mtc) &&\n\t\t\t!test_bit((data & RTIT_CTL_MTC_RANGE) >>\n\t\t\tRTIT_CTL_MTC_RANGE_OFFSET, &value))\n\t\treturn 1;\n\tvalue = intel_pt_validate_cap(vmx->pt_desc.caps,\n\t\t\t\t\t\tPT_CAP_cycle_thresholds);\n\tif (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_psb_cyc) &&\n\t\t\t!test_bit((data & RTIT_CTL_CYC_THRESH) >>\n\t\t\tRTIT_CTL_CYC_THRESH_OFFSET, &value))\n\t\treturn 1;\n\tvalue = intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_psb_periods);\n\tif (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_psb_cyc) &&\n\t\t\t!test_bit((data & RTIT_CTL_PSB_FREQ) >>\n\t\t\tRTIT_CTL_PSB_FREQ_OFFSET, &value))\n\t\treturn 1;\n\n\t \n\tvalue = (data & RTIT_CTL_ADDR0) >> RTIT_CTL_ADDR0_OFFSET;\n\tif ((value && (vmx->pt_desc.num_address_ranges < 1)) || (value > 2))\n\t\treturn 1;\n\tvalue = (data & RTIT_CTL_ADDR1) >> RTIT_CTL_ADDR1_OFFSET;\n\tif ((value && (vmx->pt_desc.num_address_ranges < 2)) || (value > 2))\n\t\treturn 1;\n\tvalue = (data & RTIT_CTL_ADDR2) >> RTIT_CTL_ADDR2_OFFSET;\n\tif ((value && (vmx->pt_desc.num_address_ranges < 3)) || (value > 2))\n\t\treturn 1;\n\tvalue = (data & RTIT_CTL_ADDR3) >> RTIT_CTL_ADDR3_OFFSET;\n\tif ((value && (vmx->pt_desc.num_address_ranges < 4)) || (value > 2))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic bool vmx_can_emulate_instruction(struct kvm_vcpu *vcpu, int emul_type,\n\t\t\t\t\tvoid *insn, int insn_len)\n{\n\t \n\tif (to_vmx(vcpu)->exit_reason.enclave_mode) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic int skip_emulated_instruction(struct kvm_vcpu *vcpu)\n{\n\tunion vmx_exit_reason exit_reason = to_vmx(vcpu)->exit_reason;\n\tunsigned long rip, orig_rip;\n\tu32 instr_len;\n\n\t \n\tif (!static_cpu_has(X86_FEATURE_HYPERVISOR) ||\n\t    exit_reason.basic != EXIT_REASON_EPT_MISCONFIG) {\n\t\tinstr_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\n\t\t \n\t\tif (!instr_len)\n\t\t\tgoto rip_updated;\n\n\t\tWARN_ONCE(exit_reason.enclave_mode,\n\t\t\t  \"skipping instruction after SGX enclave VM-Exit\");\n\n\t\torig_rip = kvm_rip_read(vcpu);\n\t\trip = orig_rip + instr_len;\n#ifdef CONFIG_X86_64\n\t\t \n\t\tif (unlikely(((rip ^ orig_rip) >> 31) == 3) && !is_64_bit_mode(vcpu))\n\t\t\trip = (u32)rip;\n#endif\n\t\tkvm_rip_write(vcpu, rip);\n\t} else {\n\t\tif (!kvm_emulate_instruction(vcpu, EMULTYPE_SKIP))\n\t\t\treturn 0;\n\t}\n\nrip_updated:\n\t \n\tvmx_set_interrupt_shadow(vcpu, 0);\n\n\treturn 1;\n}\n\n \nstatic void vmx_update_emulated_instruction(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (!is_guest_mode(vcpu))\n\t\treturn;\n\n\t \n\tif (nested_cpu_has_mtf(vmcs12) &&\n\t    (!vcpu->arch.exception.pending ||\n\t     vcpu->arch.exception.vector == DB_VECTOR) &&\n\t    (!vcpu->arch.exception_vmexit.pending ||\n\t     vcpu->arch.exception_vmexit.vector == DB_VECTOR)) {\n\t\tvmx->nested.mtf_pending = true;\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t} else {\n\t\tvmx->nested.mtf_pending = false;\n\t}\n}\n\nstatic int vmx_skip_emulated_instruction(struct kvm_vcpu *vcpu)\n{\n\tvmx_update_emulated_instruction(vcpu);\n\treturn skip_emulated_instruction(vcpu);\n}\n\nstatic void vmx_clear_hlt(struct kvm_vcpu *vcpu)\n{\n\t \n\tif (kvm_hlt_in_guest(vcpu->kvm) &&\n\t\t\tvmcs_read32(GUEST_ACTIVITY_STATE) == GUEST_ACTIVITY_HLT)\n\t\tvmcs_write32(GUEST_ACTIVITY_STATE, GUEST_ACTIVITY_ACTIVE);\n}\n\nstatic void vmx_inject_exception(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_queued_exception *ex = &vcpu->arch.exception;\n\tu32 intr_info = ex->vector | INTR_INFO_VALID_MASK;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tkvm_deliver_exception_payload(vcpu, ex);\n\n\tif (ex->has_error_code) {\n\t\t \n\t\tvmcs_write32(VM_ENTRY_EXCEPTION_ERROR_CODE, (u16)ex->error_code);\n\t\tintr_info |= INTR_INFO_DELIVER_CODE_MASK;\n\t}\n\n\tif (vmx->rmode.vm86_active) {\n\t\tint inc_eip = 0;\n\t\tif (kvm_exception_is_soft(ex->vector))\n\t\t\tinc_eip = vcpu->arch.event_exit_inst_len;\n\t\tkvm_inject_realmode_interrupt(vcpu, ex->vector, inc_eip);\n\t\treturn;\n\t}\n\n\tWARN_ON_ONCE(vmx->emulation_required);\n\n\tif (kvm_exception_is_soft(ex->vector)) {\n\t\tvmcs_write32(VM_ENTRY_INSTRUCTION_LEN,\n\t\t\t     vmx->vcpu.arch.event_exit_inst_len);\n\t\tintr_info |= INTR_TYPE_SOFT_EXCEPTION;\n\t} else\n\t\tintr_info |= INTR_TYPE_HARD_EXCEPTION;\n\n\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr_info);\n\n\tvmx_clear_hlt(vcpu);\n}\n\nstatic void vmx_setup_uret_msr(struct vcpu_vmx *vmx, unsigned int msr,\n\t\t\t       bool load_into_hardware)\n{\n\tstruct vmx_uret_msr *uret_msr;\n\n\turet_msr = vmx_find_uret_msr(vmx, msr);\n\tif (!uret_msr)\n\t\treturn;\n\n\turet_msr->load_into_hardware = load_into_hardware;\n}\n\n \nstatic void vmx_setup_uret_msrs(struct vcpu_vmx *vmx)\n{\n#ifdef CONFIG_X86_64\n\tbool load_syscall_msrs;\n\n\t \n\tload_syscall_msrs = is_long_mode(&vmx->vcpu) &&\n\t\t\t    (vmx->vcpu.arch.efer & EFER_SCE);\n\n\tvmx_setup_uret_msr(vmx, MSR_STAR, load_syscall_msrs);\n\tvmx_setup_uret_msr(vmx, MSR_LSTAR, load_syscall_msrs);\n\tvmx_setup_uret_msr(vmx, MSR_SYSCALL_MASK, load_syscall_msrs);\n#endif\n\tvmx_setup_uret_msr(vmx, MSR_EFER, update_transition_efer(vmx));\n\n\tvmx_setup_uret_msr(vmx, MSR_TSC_AUX,\n\t\t\t   guest_cpuid_has(&vmx->vcpu, X86_FEATURE_RDTSCP) ||\n\t\t\t   guest_cpuid_has(&vmx->vcpu, X86_FEATURE_RDPID));\n\n\t \n\tvmx_setup_uret_msr(vmx, MSR_IA32_TSX_CTRL, boot_cpu_has(X86_FEATURE_RTM));\n\n\t \n\tvmx->guest_uret_msrs_loaded = false;\n}\n\nu64 vmx_get_l2_tsc_offset(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\tif (nested_cpu_has(vmcs12, CPU_BASED_USE_TSC_OFFSETTING))\n\t\treturn vmcs12->tsc_offset;\n\n\treturn 0;\n}\n\nu64 vmx_get_l2_tsc_multiplier(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\tif (nested_cpu_has(vmcs12, CPU_BASED_USE_TSC_OFFSETTING) &&\n\t    nested_cpu_has2(vmcs12, SECONDARY_EXEC_TSC_SCALING))\n\t\treturn vmcs12->tsc_multiplier;\n\n\treturn kvm_caps.default_tsc_scaling_ratio;\n}\n\nstatic void vmx_write_tsc_offset(struct kvm_vcpu *vcpu)\n{\n\tvmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);\n}\n\nstatic void vmx_write_tsc_multiplier(struct kvm_vcpu *vcpu)\n{\n\tvmcs_write64(TSC_MULTIPLIER, vcpu->arch.tsc_scaling_ratio);\n}\n\n \n#define KVM_SUPPORTED_FEATURE_CONTROL  (FEAT_CTL_LOCKED\t\t\t | \\\n\t\t\t\t\tFEAT_CTL_VMX_ENABLED_INSIDE_SMX\t | \\\n\t\t\t\t\tFEAT_CTL_VMX_ENABLED_OUTSIDE_SMX | \\\n\t\t\t\t\tFEAT_CTL_SGX_LC_ENABLED\t\t | \\\n\t\t\t\t\tFEAT_CTL_SGX_ENABLED\t\t | \\\n\t\t\t\t\tFEAT_CTL_LMCE_ENABLED)\n\nstatic inline bool is_vmx_feature_control_msr_valid(struct vcpu_vmx *vmx,\n\t\t\t\t\t\t    struct msr_data *msr)\n{\n\tuint64_t valid_bits;\n\n\t \n\tWARN_ON_ONCE(vmx->msr_ia32_feature_control_valid_bits &\n\t\t     ~KVM_SUPPORTED_FEATURE_CONTROL);\n\n\tif (!msr->host_initiated &&\n\t    (vmx->msr_ia32_feature_control & FEAT_CTL_LOCKED))\n\t\treturn false;\n\n\tif (msr->host_initiated)\n\t\tvalid_bits = KVM_SUPPORTED_FEATURE_CONTROL;\n\telse\n\t\tvalid_bits = vmx->msr_ia32_feature_control_valid_bits;\n\n\treturn !(msr->data & ~valid_bits);\n}\n\nstatic int vmx_get_msr_feature(struct kvm_msr_entry *msr)\n{\n\tswitch (msr->index) {\n\tcase KVM_FIRST_EMULATED_VMX_MSR ... KVM_LAST_EMULATED_VMX_MSR:\n\t\tif (!nested)\n\t\t\treturn 1;\n\t\treturn vmx_get_vmx_msr(&vmcs_config.nested, msr->index, &msr->data);\n\tdefault:\n\t\treturn KVM_MSR_RET_INVALID;\n\t}\n}\n\n \nstatic int vmx_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmx_uret_msr *msr;\n\tu32 index;\n\n\tswitch (msr_info->index) {\n#ifdef CONFIG_X86_64\n\tcase MSR_FS_BASE:\n\t\tmsr_info->data = vmcs_readl(GUEST_FS_BASE);\n\t\tbreak;\n\tcase MSR_GS_BASE:\n\t\tmsr_info->data = vmcs_readl(GUEST_GS_BASE);\n\t\tbreak;\n\tcase MSR_KERNEL_GS_BASE:\n\t\tmsr_info->data = vmx_read_guest_kernel_gs_base(vmx);\n\t\tbreak;\n#endif\n\tcase MSR_EFER:\n\t\treturn kvm_get_msr_common(vcpu, msr_info);\n\tcase MSR_IA32_TSX_CTRL:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !(vcpu->arch.arch_capabilities & ARCH_CAP_TSX_CTRL_MSR))\n\t\t\treturn 1;\n\t\tgoto find_uret_msr;\n\tcase MSR_IA32_UMWAIT_CONTROL:\n\t\tif (!msr_info->host_initiated && !vmx_has_waitpkg(vmx))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = vmx->msr_ia32_umwait_control;\n\t\tbreak;\n\tcase MSR_IA32_SPEC_CTRL:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_has_spec_ctrl_msr(vcpu))\n\t\t\treturn 1;\n\n\t\tmsr_info->data = to_vmx(vcpu)->spec_ctrl;\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_CS:\n\t\tmsr_info->data = vmcs_read32(GUEST_SYSENTER_CS);\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_EIP:\n\t\tmsr_info->data = vmcs_readl(GUEST_SYSENTER_EIP);\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_ESP:\n\t\tmsr_info->data = vmcs_readl(GUEST_SYSENTER_ESP);\n\t\tbreak;\n\tcase MSR_IA32_BNDCFGS:\n\t\tif (!kvm_mpx_supported() ||\n\t\t    (!msr_info->host_initiated &&\n\t\t     !guest_cpuid_has(vcpu, X86_FEATURE_MPX)))\n\t\t\treturn 1;\n\t\tmsr_info->data = vmcs_read64(GUEST_BNDCFGS);\n\t\tbreak;\n\tcase MSR_IA32_MCG_EXT_CTL:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !(vmx->msr_ia32_feature_control &\n\t\t      FEAT_CTL_LMCE_ENABLED))\n\t\t\treturn 1;\n\t\tmsr_info->data = vcpu->arch.mcg_ext_ctl;\n\t\tbreak;\n\tcase MSR_IA32_FEAT_CTL:\n\t\tmsr_info->data = vmx->msr_ia32_feature_control;\n\t\tbreak;\n\tcase MSR_IA32_SGXLEPUBKEYHASH0 ... MSR_IA32_SGXLEPUBKEYHASH3:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_cpuid_has(vcpu, X86_FEATURE_SGX_LC))\n\t\t\treturn 1;\n\t\tmsr_info->data = to_vmx(vcpu)->msr_ia32_sgxlepubkeyhash\n\t\t\t[msr_info->index - MSR_IA32_SGXLEPUBKEYHASH0];\n\t\tbreak;\n\tcase KVM_FIRST_EMULATED_VMX_MSR ... KVM_LAST_EMULATED_VMX_MSR:\n\t\tif (!guest_can_use(vcpu, X86_FEATURE_VMX))\n\t\t\treturn 1;\n\t\tif (vmx_get_vmx_msr(&vmx->nested.msrs, msr_info->index,\n\t\t\t\t    &msr_info->data))\n\t\t\treturn 1;\n\t\t \n\t\tif (!msr_info->host_initiated && guest_cpuid_has_evmcs(vcpu))\n\t\t\tnested_evmcs_filter_control_msr(vcpu, msr_info->index,\n\t\t\t\t\t\t\t&msr_info->data);\n\t\tbreak;\n\tcase MSR_IA32_RTIT_CTL:\n\t\tif (!vmx_pt_mode_is_host_guest())\n\t\t\treturn 1;\n\t\tmsr_info->data = vmx->pt_desc.guest.ctl;\n\t\tbreak;\n\tcase MSR_IA32_RTIT_STATUS:\n\t\tif (!vmx_pt_mode_is_host_guest())\n\t\t\treturn 1;\n\t\tmsr_info->data = vmx->pt_desc.guest.status;\n\t\tbreak;\n\tcase MSR_IA32_RTIT_CR3_MATCH:\n\t\tif (!vmx_pt_mode_is_host_guest() ||\n\t\t\t!intel_pt_validate_cap(vmx->pt_desc.caps,\n\t\t\t\t\t\tPT_CAP_cr3_filtering))\n\t\t\treturn 1;\n\t\tmsr_info->data = vmx->pt_desc.guest.cr3_match;\n\t\tbreak;\n\tcase MSR_IA32_RTIT_OUTPUT_BASE:\n\t\tif (!vmx_pt_mode_is_host_guest() ||\n\t\t\t(!intel_pt_validate_cap(vmx->pt_desc.caps,\n\t\t\t\t\tPT_CAP_topa_output) &&\n\t\t\t !intel_pt_validate_cap(vmx->pt_desc.caps,\n\t\t\t\t\tPT_CAP_single_range_output)))\n\t\t\treturn 1;\n\t\tmsr_info->data = vmx->pt_desc.guest.output_base;\n\t\tbreak;\n\tcase MSR_IA32_RTIT_OUTPUT_MASK:\n\t\tif (!vmx_pt_mode_is_host_guest() ||\n\t\t\t(!intel_pt_validate_cap(vmx->pt_desc.caps,\n\t\t\t\t\tPT_CAP_topa_output) &&\n\t\t\t !intel_pt_validate_cap(vmx->pt_desc.caps,\n\t\t\t\t\tPT_CAP_single_range_output)))\n\t\t\treturn 1;\n\t\tmsr_info->data = vmx->pt_desc.guest.output_mask;\n\t\tbreak;\n\tcase MSR_IA32_RTIT_ADDR0_A ... MSR_IA32_RTIT_ADDR3_B:\n\t\tindex = msr_info->index - MSR_IA32_RTIT_ADDR0_A;\n\t\tif (!vmx_pt_mode_is_host_guest() ||\n\t\t    (index >= 2 * vmx->pt_desc.num_address_ranges))\n\t\t\treturn 1;\n\t\tif (index % 2)\n\t\t\tmsr_info->data = vmx->pt_desc.guest.addr_b[index / 2];\n\t\telse\n\t\t\tmsr_info->data = vmx->pt_desc.guest.addr_a[index / 2];\n\t\tbreak;\n\tcase MSR_IA32_DEBUGCTLMSR:\n\t\tmsr_info->data = vmcs_read64(GUEST_IA32_DEBUGCTL);\n\t\tbreak;\n\tdefault:\n\tfind_uret_msr:\n\t\tmsr = vmx_find_uret_msr(vmx, msr_info->index);\n\t\tif (msr) {\n\t\t\tmsr_info->data = msr->data;\n\t\t\tbreak;\n\t\t}\n\t\treturn kvm_get_msr_common(vcpu, msr_info);\n\t}\n\n\treturn 0;\n}\n\nstatic u64 nested_vmx_truncate_sysenter_addr(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t    u64 data)\n{\n#ifdef CONFIG_X86_64\n\tif (!guest_cpuid_has(vcpu, X86_FEATURE_LM))\n\t\treturn (u32)data;\n#endif\n\treturn (unsigned long)data;\n}\n\nstatic u64 vmx_get_supported_debugctl(struct kvm_vcpu *vcpu, bool host_initiated)\n{\n\tu64 debugctl = 0;\n\n\tif (boot_cpu_has(X86_FEATURE_BUS_LOCK_DETECT) &&\n\t    (host_initiated || guest_cpuid_has(vcpu, X86_FEATURE_BUS_LOCK_DETECT)))\n\t\tdebugctl |= DEBUGCTLMSR_BUS_LOCK_DETECT;\n\n\tif ((kvm_caps.supported_perf_cap & PMU_CAP_LBR_FMT) &&\n\t    (host_initiated || intel_pmu_lbr_is_enabled(vcpu)))\n\t\tdebugctl |= DEBUGCTLMSR_LBR | DEBUGCTLMSR_FREEZE_LBRS_ON_PMI;\n\n\treturn debugctl;\n}\n\n \nstatic int vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmx_uret_msr *msr;\n\tint ret = 0;\n\tu32 msr_index = msr_info->index;\n\tu64 data = msr_info->data;\n\tu32 index;\n\n\tswitch (msr_index) {\n\tcase MSR_EFER:\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t\tbreak;\n#ifdef CONFIG_X86_64\n\tcase MSR_FS_BASE:\n\t\tvmx_segment_cache_clear(vmx);\n\t\tvmcs_writel(GUEST_FS_BASE, data);\n\t\tbreak;\n\tcase MSR_GS_BASE:\n\t\tvmx_segment_cache_clear(vmx);\n\t\tvmcs_writel(GUEST_GS_BASE, data);\n\t\tbreak;\n\tcase MSR_KERNEL_GS_BASE:\n\t\tvmx_write_guest_kernel_gs_base(vmx, data);\n\t\tbreak;\n\tcase MSR_IA32_XFD:\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t\t \n\t\tif (!ret && data) {\n\t\t\tvmx_disable_intercept_for_msr(vcpu, MSR_IA32_XFD,\n\t\t\t\t\t\t      MSR_TYPE_RW);\n\t\t\tvcpu->arch.xfd_no_write_intercept = true;\n\t\t\tvmx_update_exception_bitmap(vcpu);\n\t\t}\n\t\tbreak;\n#endif\n\tcase MSR_IA32_SYSENTER_CS:\n\t\tif (is_guest_mode(vcpu))\n\t\t\tget_vmcs12(vcpu)->guest_sysenter_cs = data;\n\t\tvmcs_write32(GUEST_SYSENTER_CS, data);\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_EIP:\n\t\tif (is_guest_mode(vcpu)) {\n\t\t\tdata = nested_vmx_truncate_sysenter_addr(vcpu, data);\n\t\t\tget_vmcs12(vcpu)->guest_sysenter_eip = data;\n\t\t}\n\t\tvmcs_writel(GUEST_SYSENTER_EIP, data);\n\t\tbreak;\n\tcase MSR_IA32_SYSENTER_ESP:\n\t\tif (is_guest_mode(vcpu)) {\n\t\t\tdata = nested_vmx_truncate_sysenter_addr(vcpu, data);\n\t\t\tget_vmcs12(vcpu)->guest_sysenter_esp = data;\n\t\t}\n\t\tvmcs_writel(GUEST_SYSENTER_ESP, data);\n\t\tbreak;\n\tcase MSR_IA32_DEBUGCTLMSR: {\n\t\tu64 invalid;\n\n\t\tinvalid = data & ~vmx_get_supported_debugctl(vcpu, msr_info->host_initiated);\n\t\tif (invalid & (DEBUGCTLMSR_BTF|DEBUGCTLMSR_LBR)) {\n\t\t\tkvm_pr_unimpl_wrmsr(vcpu, msr_index, data);\n\t\t\tdata &= ~(DEBUGCTLMSR_BTF|DEBUGCTLMSR_LBR);\n\t\t\tinvalid &= ~(DEBUGCTLMSR_BTF|DEBUGCTLMSR_LBR);\n\t\t}\n\n\t\tif (invalid)\n\t\t\treturn 1;\n\n\t\tif (is_guest_mode(vcpu) && get_vmcs12(vcpu)->vm_exit_controls &\n\t\t\t\t\t\tVM_EXIT_SAVE_DEBUG_CONTROLS)\n\t\t\tget_vmcs12(vcpu)->guest_ia32_debugctl = data;\n\n\t\tvmcs_write64(GUEST_IA32_DEBUGCTL, data);\n\t\tif (intel_pmu_lbr_is_enabled(vcpu) && !to_vmx(vcpu)->lbr_desc.event &&\n\t\t    (data & DEBUGCTLMSR_LBR))\n\t\t\tintel_pmu_create_guest_lbr_event(vcpu);\n\t\treturn 0;\n\t}\n\tcase MSR_IA32_BNDCFGS:\n\t\tif (!kvm_mpx_supported() ||\n\t\t    (!msr_info->host_initiated &&\n\t\t     !guest_cpuid_has(vcpu, X86_FEATURE_MPX)))\n\t\t\treturn 1;\n\t\tif (is_noncanonical_address(data & PAGE_MASK, vcpu) ||\n\t\t    (data & MSR_IA32_BNDCFGS_RSVD))\n\t\t\treturn 1;\n\n\t\tif (is_guest_mode(vcpu) &&\n\t\t    ((vmx->nested.msrs.entry_ctls_high & VM_ENTRY_LOAD_BNDCFGS) ||\n\t\t     (vmx->nested.msrs.exit_ctls_high & VM_EXIT_CLEAR_BNDCFGS)))\n\t\t\tget_vmcs12(vcpu)->guest_bndcfgs = data;\n\n\t\tvmcs_write64(GUEST_BNDCFGS, data);\n\t\tbreak;\n\tcase MSR_IA32_UMWAIT_CONTROL:\n\t\tif (!msr_info->host_initiated && !vmx_has_waitpkg(vmx))\n\t\t\treturn 1;\n\n\t\t \n\t\tif (data & (BIT_ULL(1) | GENMASK_ULL(63, 32)))\n\t\t\treturn 1;\n\n\t\tvmx->msr_ia32_umwait_control = data;\n\t\tbreak;\n\tcase MSR_IA32_SPEC_CTRL:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !guest_has_spec_ctrl_msr(vcpu))\n\t\t\treturn 1;\n\n\t\tif (kvm_spec_ctrl_test_value(data))\n\t\t\treturn 1;\n\n\t\tvmx->spec_ctrl = data;\n\t\tif (!data)\n\t\t\tbreak;\n\n\t\t \n\t\tvmx_disable_intercept_for_msr(vcpu,\n\t\t\t\t\t      MSR_IA32_SPEC_CTRL,\n\t\t\t\t\t      MSR_TYPE_RW);\n\t\tbreak;\n\tcase MSR_IA32_TSX_CTRL:\n\t\tif (!msr_info->host_initiated &&\n\t\t    !(vcpu->arch.arch_capabilities & ARCH_CAP_TSX_CTRL_MSR))\n\t\t\treturn 1;\n\t\tif (data & ~(TSX_CTRL_RTM_DISABLE | TSX_CTRL_CPUID_CLEAR))\n\t\t\treturn 1;\n\t\tgoto find_uret_msr;\n\tcase MSR_IA32_CR_PAT:\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t\tif (ret)\n\t\t\tbreak;\n\n\t\tif (is_guest_mode(vcpu) &&\n\t\t    get_vmcs12(vcpu)->vm_exit_controls & VM_EXIT_SAVE_IA32_PAT)\n\t\t\tget_vmcs12(vcpu)->guest_ia32_pat = data;\n\n\t\tif (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT)\n\t\t\tvmcs_write64(GUEST_IA32_PAT, data);\n\t\tbreak;\n\tcase MSR_IA32_MCG_EXT_CTL:\n\t\tif ((!msr_info->host_initiated &&\n\t\t     !(to_vmx(vcpu)->msr_ia32_feature_control &\n\t\t       FEAT_CTL_LMCE_ENABLED)) ||\n\t\t    (data & ~MCG_EXT_CTL_LMCE_EN))\n\t\t\treturn 1;\n\t\tvcpu->arch.mcg_ext_ctl = data;\n\t\tbreak;\n\tcase MSR_IA32_FEAT_CTL:\n\t\tif (!is_vmx_feature_control_msr_valid(vmx, msr_info))\n\t\t\treturn 1;\n\n\t\tvmx->msr_ia32_feature_control = data;\n\t\tif (msr_info->host_initiated && data == 0)\n\t\t\tvmx_leave_nested(vcpu);\n\n\t\t \n\t\tvmx_write_encls_bitmap(vcpu, NULL);\n\t\tbreak;\n\tcase MSR_IA32_SGXLEPUBKEYHASH0 ... MSR_IA32_SGXLEPUBKEYHASH3:\n\t\t \n\t\tif (!msr_info->host_initiated &&\n\t\t    (!guest_cpuid_has(vcpu, X86_FEATURE_SGX_LC) ||\n\t\t    ((vmx->msr_ia32_feature_control & FEAT_CTL_LOCKED) &&\n\t\t    !(vmx->msr_ia32_feature_control & FEAT_CTL_SGX_LC_ENABLED))))\n\t\t\treturn 1;\n\t\tvmx->msr_ia32_sgxlepubkeyhash\n\t\t\t[msr_index - MSR_IA32_SGXLEPUBKEYHASH0] = data;\n\t\tbreak;\n\tcase KVM_FIRST_EMULATED_VMX_MSR ... KVM_LAST_EMULATED_VMX_MSR:\n\t\tif (!msr_info->host_initiated)\n\t\t\treturn 1;  \n\t\tif (!guest_can_use(vcpu, X86_FEATURE_VMX))\n\t\t\treturn 1;\n\t\treturn vmx_set_vmx_msr(vcpu, msr_index, data);\n\tcase MSR_IA32_RTIT_CTL:\n\t\tif (!vmx_pt_mode_is_host_guest() ||\n\t\t\tvmx_rtit_ctl_check(vcpu, data) ||\n\t\t\tvmx->nested.vmxon)\n\t\t\treturn 1;\n\t\tvmcs_write64(GUEST_IA32_RTIT_CTL, data);\n\t\tvmx->pt_desc.guest.ctl = data;\n\t\tpt_update_intercept_for_msr(vcpu);\n\t\tbreak;\n\tcase MSR_IA32_RTIT_STATUS:\n\t\tif (!pt_can_write_msr(vmx))\n\t\t\treturn 1;\n\t\tif (data & MSR_IA32_RTIT_STATUS_MASK)\n\t\t\treturn 1;\n\t\tvmx->pt_desc.guest.status = data;\n\t\tbreak;\n\tcase MSR_IA32_RTIT_CR3_MATCH:\n\t\tif (!pt_can_write_msr(vmx))\n\t\t\treturn 1;\n\t\tif (!intel_pt_validate_cap(vmx->pt_desc.caps,\n\t\t\t\t\t   PT_CAP_cr3_filtering))\n\t\t\treturn 1;\n\t\tvmx->pt_desc.guest.cr3_match = data;\n\t\tbreak;\n\tcase MSR_IA32_RTIT_OUTPUT_BASE:\n\t\tif (!pt_can_write_msr(vmx))\n\t\t\treturn 1;\n\t\tif (!intel_pt_validate_cap(vmx->pt_desc.caps,\n\t\t\t\t\t   PT_CAP_topa_output) &&\n\t\t    !intel_pt_validate_cap(vmx->pt_desc.caps,\n\t\t\t\t\t   PT_CAP_single_range_output))\n\t\t\treturn 1;\n\t\tif (!pt_output_base_valid(vcpu, data))\n\t\t\treturn 1;\n\t\tvmx->pt_desc.guest.output_base = data;\n\t\tbreak;\n\tcase MSR_IA32_RTIT_OUTPUT_MASK:\n\t\tif (!pt_can_write_msr(vmx))\n\t\t\treturn 1;\n\t\tif (!intel_pt_validate_cap(vmx->pt_desc.caps,\n\t\t\t\t\t   PT_CAP_topa_output) &&\n\t\t    !intel_pt_validate_cap(vmx->pt_desc.caps,\n\t\t\t\t\t   PT_CAP_single_range_output))\n\t\t\treturn 1;\n\t\tvmx->pt_desc.guest.output_mask = data;\n\t\tbreak;\n\tcase MSR_IA32_RTIT_ADDR0_A ... MSR_IA32_RTIT_ADDR3_B:\n\t\tif (!pt_can_write_msr(vmx))\n\t\t\treturn 1;\n\t\tindex = msr_info->index - MSR_IA32_RTIT_ADDR0_A;\n\t\tif (index >= 2 * vmx->pt_desc.num_address_ranges)\n\t\t\treturn 1;\n\t\tif (is_noncanonical_address(data, vcpu))\n\t\t\treturn 1;\n\t\tif (index % 2)\n\t\t\tvmx->pt_desc.guest.addr_b[index / 2] = data;\n\t\telse\n\t\t\tvmx->pt_desc.guest.addr_a[index / 2] = data;\n\t\tbreak;\n\tcase MSR_IA32_PERF_CAPABILITIES:\n\t\tif (data && !vcpu_to_pmu(vcpu)->version)\n\t\t\treturn 1;\n\t\tif (data & PMU_CAP_LBR_FMT) {\n\t\t\tif ((data & PMU_CAP_LBR_FMT) !=\n\t\t\t    (kvm_caps.supported_perf_cap & PMU_CAP_LBR_FMT))\n\t\t\t\treturn 1;\n\t\t\tif (!cpuid_model_is_consistent(vcpu))\n\t\t\t\treturn 1;\n\t\t}\n\t\tif (data & PERF_CAP_PEBS_FORMAT) {\n\t\t\tif ((data & PERF_CAP_PEBS_MASK) !=\n\t\t\t    (kvm_caps.supported_perf_cap & PERF_CAP_PEBS_MASK))\n\t\t\t\treturn 1;\n\t\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_DS))\n\t\t\t\treturn 1;\n\t\t\tif (!guest_cpuid_has(vcpu, X86_FEATURE_DTES64))\n\t\t\t\treturn 1;\n\t\t\tif (!cpuid_model_is_consistent(vcpu))\n\t\t\t\treturn 1;\n\t\t}\n\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t\tbreak;\n\n\tdefault:\n\tfind_uret_msr:\n\t\tmsr = vmx_find_uret_msr(vmx, msr_index);\n\t\tif (msr)\n\t\t\tret = vmx_set_guest_uret_msr(vmx, msr, data);\n\t\telse\n\t\t\tret = kvm_set_msr_common(vcpu, msr_info);\n\t}\n\n\t \n\tif (msr_index == MSR_IA32_ARCH_CAPABILITIES)\n\t\tvmx_update_fb_clear_dis(vcpu, vmx);\n\n\treturn ret;\n}\n\nstatic void vmx_cache_reg(struct kvm_vcpu *vcpu, enum kvm_reg reg)\n{\n\tunsigned long guest_owned_bits;\n\n\tkvm_register_mark_available(vcpu, reg);\n\n\tswitch (reg) {\n\tcase VCPU_REGS_RSP:\n\t\tvcpu->arch.regs[VCPU_REGS_RSP] = vmcs_readl(GUEST_RSP);\n\t\tbreak;\n\tcase VCPU_REGS_RIP:\n\t\tvcpu->arch.regs[VCPU_REGS_RIP] = vmcs_readl(GUEST_RIP);\n\t\tbreak;\n\tcase VCPU_EXREG_PDPTR:\n\t\tif (enable_ept)\n\t\t\tept_save_pdptrs(vcpu);\n\t\tbreak;\n\tcase VCPU_EXREG_CR0:\n\t\tguest_owned_bits = vcpu->arch.cr0_guest_owned_bits;\n\n\t\tvcpu->arch.cr0 &= ~guest_owned_bits;\n\t\tvcpu->arch.cr0 |= vmcs_readl(GUEST_CR0) & guest_owned_bits;\n\t\tbreak;\n\tcase VCPU_EXREG_CR3:\n\t\t \n\t\tif (!(exec_controls_get(to_vmx(vcpu)) & CPU_BASED_CR3_LOAD_EXITING))\n\t\t\tvcpu->arch.cr3 = vmcs_readl(GUEST_CR3);\n\t\tbreak;\n\tcase VCPU_EXREG_CR4:\n\t\tguest_owned_bits = vcpu->arch.cr4_guest_owned_bits;\n\n\t\tvcpu->arch.cr4 &= ~guest_owned_bits;\n\t\tvcpu->arch.cr4 |= vmcs_readl(GUEST_CR4) & guest_owned_bits;\n\t\tbreak;\n\tdefault:\n\t\tKVM_BUG_ON(1, vcpu->kvm);\n\t\tbreak;\n\t}\n}\n\n \nstatic bool cpu_has_sgx(void)\n{\n\treturn cpuid_eax(0) >= 0x12 && (cpuid_eax(0x12) & BIT(0));\n}\n\n \nstatic bool cpu_has_perf_global_ctrl_bug(void)\n{\n\tif (boot_cpu_data.x86 == 0x6) {\n\t\tswitch (boot_cpu_data.x86_model) {\n\t\tcase INTEL_FAM6_NEHALEM_EP:\t \n\t\tcase INTEL_FAM6_NEHALEM:\t \n\t\tcase INTEL_FAM6_WESTMERE:\t \n\t\tcase INTEL_FAM6_WESTMERE_EP:\t \n\t\tcase INTEL_FAM6_NEHALEM_EX:\t \n\t\t\treturn true;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn false;\n}\n\nstatic int adjust_vmx_controls(u32 ctl_min, u32 ctl_opt, u32 msr, u32 *result)\n{\n\tu32 vmx_msr_low, vmx_msr_high;\n\tu32 ctl = ctl_min | ctl_opt;\n\n\trdmsr(msr, vmx_msr_low, vmx_msr_high);\n\n\tctl &= vmx_msr_high;  \n\tctl |= vmx_msr_low;   \n\n\t \n\tif (ctl_min & ~ctl)\n\t\treturn -EIO;\n\n\t*result = ctl;\n\treturn 0;\n}\n\nstatic u64 adjust_vmx_controls64(u64 ctl_opt, u32 msr)\n{\n\tu64 allowed;\n\n\trdmsrl(msr, allowed);\n\n\treturn  ctl_opt & allowed;\n}\n\nstatic int setup_vmcs_config(struct vmcs_config *vmcs_conf,\n\t\t\t     struct vmx_capability *vmx_cap)\n{\n\tu32 vmx_msr_low, vmx_msr_high;\n\tu32 _pin_based_exec_control = 0;\n\tu32 _cpu_based_exec_control = 0;\n\tu32 _cpu_based_2nd_exec_control = 0;\n\tu64 _cpu_based_3rd_exec_control = 0;\n\tu32 _vmexit_control = 0;\n\tu32 _vmentry_control = 0;\n\tu64 misc_msr;\n\tint i;\n\n\t \n\tstruct {\n\t\tu32 entry_control;\n\t\tu32 exit_control;\n\t} const vmcs_entry_exit_pairs[] = {\n\t\t{ VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL,\tVM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL },\n\t\t{ VM_ENTRY_LOAD_IA32_PAT,\t\tVM_EXIT_LOAD_IA32_PAT },\n\t\t{ VM_ENTRY_LOAD_IA32_EFER,\t\tVM_EXIT_LOAD_IA32_EFER },\n\t\t{ VM_ENTRY_LOAD_BNDCFGS,\t\tVM_EXIT_CLEAR_BNDCFGS },\n\t\t{ VM_ENTRY_LOAD_IA32_RTIT_CTL,\t\tVM_EXIT_CLEAR_IA32_RTIT_CTL },\n\t};\n\n\tmemset(vmcs_conf, 0, sizeof(*vmcs_conf));\n\n\tif (adjust_vmx_controls(KVM_REQUIRED_VMX_CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t\tKVM_OPTIONAL_VMX_CPU_BASED_VM_EXEC_CONTROL,\n\t\t\t\tMSR_IA32_VMX_PROCBASED_CTLS,\n\t\t\t\t&_cpu_based_exec_control))\n\t\treturn -EIO;\n\tif (_cpu_based_exec_control & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) {\n\t\tif (adjust_vmx_controls(KVM_REQUIRED_VMX_SECONDARY_VM_EXEC_CONTROL,\n\t\t\t\t\tKVM_OPTIONAL_VMX_SECONDARY_VM_EXEC_CONTROL,\n\t\t\t\t\tMSR_IA32_VMX_PROCBASED_CTLS2,\n\t\t\t\t\t&_cpu_based_2nd_exec_control))\n\t\t\treturn -EIO;\n\t}\n#ifndef CONFIG_X86_64\n\tif (!(_cpu_based_2nd_exec_control &\n\t\t\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES))\n\t\t_cpu_based_exec_control &= ~CPU_BASED_TPR_SHADOW;\n#endif\n\n\tif (!(_cpu_based_exec_control & CPU_BASED_TPR_SHADOW))\n\t\t_cpu_based_2nd_exec_control &= ~(\n\t\t\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\tSECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |\n\t\t\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n\n\trdmsr_safe(MSR_IA32_VMX_EPT_VPID_CAP,\n\t\t&vmx_cap->ept, &vmx_cap->vpid);\n\n\tif (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_ENABLE_EPT) &&\n\t    vmx_cap->ept) {\n\t\tpr_warn_once(\"EPT CAP should not exist if not support \"\n\t\t\t\t\"1-setting enable EPT VM-execution control\\n\");\n\n\t\tif (error_on_inconsistent_vmcs_config)\n\t\t\treturn -EIO;\n\n\t\tvmx_cap->ept = 0;\n\t}\n\tif (!(_cpu_based_2nd_exec_control & SECONDARY_EXEC_ENABLE_VPID) &&\n\t    vmx_cap->vpid) {\n\t\tpr_warn_once(\"VPID CAP should not exist if not support \"\n\t\t\t\t\"1-setting enable VPID VM-execution control\\n\");\n\n\t\tif (error_on_inconsistent_vmcs_config)\n\t\t\treturn -EIO;\n\n\t\tvmx_cap->vpid = 0;\n\t}\n\n\tif (!cpu_has_sgx())\n\t\t_cpu_based_2nd_exec_control &= ~SECONDARY_EXEC_ENCLS_EXITING;\n\n\tif (_cpu_based_exec_control & CPU_BASED_ACTIVATE_TERTIARY_CONTROLS)\n\t\t_cpu_based_3rd_exec_control =\n\t\t\tadjust_vmx_controls64(KVM_OPTIONAL_VMX_TERTIARY_VM_EXEC_CONTROL,\n\t\t\t\t\t      MSR_IA32_VMX_PROCBASED_CTLS3);\n\n\tif (adjust_vmx_controls(KVM_REQUIRED_VMX_VM_EXIT_CONTROLS,\n\t\t\t\tKVM_OPTIONAL_VMX_VM_EXIT_CONTROLS,\n\t\t\t\tMSR_IA32_VMX_EXIT_CTLS,\n\t\t\t\t&_vmexit_control))\n\t\treturn -EIO;\n\n\tif (adjust_vmx_controls(KVM_REQUIRED_VMX_PIN_BASED_VM_EXEC_CONTROL,\n\t\t\t\tKVM_OPTIONAL_VMX_PIN_BASED_VM_EXEC_CONTROL,\n\t\t\t\tMSR_IA32_VMX_PINBASED_CTLS,\n\t\t\t\t&_pin_based_exec_control))\n\t\treturn -EIO;\n\n\tif (cpu_has_broken_vmx_preemption_timer())\n\t\t_pin_based_exec_control &= ~PIN_BASED_VMX_PREEMPTION_TIMER;\n\tif (!(_cpu_based_2nd_exec_control &\n\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))\n\t\t_pin_based_exec_control &= ~PIN_BASED_POSTED_INTR;\n\n\tif (adjust_vmx_controls(KVM_REQUIRED_VMX_VM_ENTRY_CONTROLS,\n\t\t\t\tKVM_OPTIONAL_VMX_VM_ENTRY_CONTROLS,\n\t\t\t\tMSR_IA32_VMX_ENTRY_CTLS,\n\t\t\t\t&_vmentry_control))\n\t\treturn -EIO;\n\n\tfor (i = 0; i < ARRAY_SIZE(vmcs_entry_exit_pairs); i++) {\n\t\tu32 n_ctrl = vmcs_entry_exit_pairs[i].entry_control;\n\t\tu32 x_ctrl = vmcs_entry_exit_pairs[i].exit_control;\n\n\t\tif (!(_vmentry_control & n_ctrl) == !(_vmexit_control & x_ctrl))\n\t\t\tcontinue;\n\n\t\tpr_warn_once(\"Inconsistent VM-Entry/VM-Exit pair, entry = %x, exit = %x\\n\",\n\t\t\t     _vmentry_control & n_ctrl, _vmexit_control & x_ctrl);\n\n\t\tif (error_on_inconsistent_vmcs_config)\n\t\t\treturn -EIO;\n\n\t\t_vmentry_control &= ~n_ctrl;\n\t\t_vmexit_control &= ~x_ctrl;\n\t}\n\n\trdmsr(MSR_IA32_VMX_BASIC, vmx_msr_low, vmx_msr_high);\n\n\t \n\tif ((vmx_msr_high & 0x1fff) > PAGE_SIZE)\n\t\treturn -EIO;\n\n#ifdef CONFIG_X86_64\n\t \n\tif (vmx_msr_high & (1u<<16))\n\t\treturn -EIO;\n#endif\n\n\t \n\tif (((vmx_msr_high >> 18) & 15) != 6)\n\t\treturn -EIO;\n\n\trdmsrl(MSR_IA32_VMX_MISC, misc_msr);\n\n\tvmcs_conf->size = vmx_msr_high & 0x1fff;\n\tvmcs_conf->basic_cap = vmx_msr_high & ~0x1fff;\n\n\tvmcs_conf->revision_id = vmx_msr_low;\n\n\tvmcs_conf->pin_based_exec_ctrl = _pin_based_exec_control;\n\tvmcs_conf->cpu_based_exec_ctrl = _cpu_based_exec_control;\n\tvmcs_conf->cpu_based_2nd_exec_ctrl = _cpu_based_2nd_exec_control;\n\tvmcs_conf->cpu_based_3rd_exec_ctrl = _cpu_based_3rd_exec_control;\n\tvmcs_conf->vmexit_ctrl         = _vmexit_control;\n\tvmcs_conf->vmentry_ctrl        = _vmentry_control;\n\tvmcs_conf->misc\t= misc_msr;\n\n#if IS_ENABLED(CONFIG_HYPERV)\n\tif (enlightened_vmcs)\n\t\tevmcs_sanitize_exec_ctrls(vmcs_conf);\n#endif\n\n\treturn 0;\n}\n\nstatic bool __kvm_is_vmx_supported(void)\n{\n\tint cpu = smp_processor_id();\n\n\tif (!(cpuid_ecx(1) & feature_bit(VMX))) {\n\t\tpr_err(\"VMX not supported by CPU %d\\n\", cpu);\n\t\treturn false;\n\t}\n\n\tif (!this_cpu_has(X86_FEATURE_MSR_IA32_FEAT_CTL) ||\n\t    !this_cpu_has(X86_FEATURE_VMX)) {\n\t\tpr_err(\"VMX not enabled (by BIOS) in MSR_IA32_FEAT_CTL on CPU %d\\n\", cpu);\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool kvm_is_vmx_supported(void)\n{\n\tbool supported;\n\n\tmigrate_disable();\n\tsupported = __kvm_is_vmx_supported();\n\tmigrate_enable();\n\n\treturn supported;\n}\n\nstatic int vmx_check_processor_compat(void)\n{\n\tint cpu = raw_smp_processor_id();\n\tstruct vmcs_config vmcs_conf;\n\tstruct vmx_capability vmx_cap;\n\n\tif (!__kvm_is_vmx_supported())\n\t\treturn -EIO;\n\n\tif (setup_vmcs_config(&vmcs_conf, &vmx_cap) < 0) {\n\t\tpr_err(\"Failed to setup VMCS config on CPU %d\\n\", cpu);\n\t\treturn -EIO;\n\t}\n\tif (nested)\n\t\tnested_vmx_setup_ctls_msrs(&vmcs_conf, vmx_cap.ept);\n\tif (memcmp(&vmcs_config, &vmcs_conf, sizeof(struct vmcs_config))) {\n\t\tpr_err(\"Inconsistent VMCS config on CPU %d\\n\", cpu);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}\n\nstatic int kvm_cpu_vmxon(u64 vmxon_pointer)\n{\n\tu64 msr;\n\n\tcr4_set_bits(X86_CR4_VMXE);\n\n\tasm_volatile_goto(\"1: vmxon %[vmxon_pointer]\\n\\t\"\n\t\t\t  _ASM_EXTABLE(1b, %l[fault])\n\t\t\t  : : [vmxon_pointer] \"m\"(vmxon_pointer)\n\t\t\t  : : fault);\n\treturn 0;\n\nfault:\n\tWARN_ONCE(1, \"VMXON faulted, MSR_IA32_FEAT_CTL (0x3a) = 0x%llx\\n\",\n\t\t  rdmsrl_safe(MSR_IA32_FEAT_CTL, &msr) ? 0xdeadbeef : msr);\n\tcr4_clear_bits(X86_CR4_VMXE);\n\n\treturn -EFAULT;\n}\n\nstatic int vmx_hardware_enable(void)\n{\n\tint cpu = raw_smp_processor_id();\n\tu64 phys_addr = __pa(per_cpu(vmxarea, cpu));\n\tint r;\n\n\tif (cr4_read_shadow() & X86_CR4_VMXE)\n\t\treturn -EBUSY;\n\n\t \n\tif (kvm_is_using_evmcs() && !hv_get_vp_assist_page(cpu))\n\t\treturn -EFAULT;\n\n\tintel_pt_handle_vmx(1);\n\n\tr = kvm_cpu_vmxon(phys_addr);\n\tif (r) {\n\t\tintel_pt_handle_vmx(0);\n\t\treturn r;\n\t}\n\n\tif (enable_ept)\n\t\tept_sync_global();\n\n\treturn 0;\n}\n\nstatic void vmclear_local_loaded_vmcss(void)\n{\n\tint cpu = raw_smp_processor_id();\n\tstruct loaded_vmcs *v, *n;\n\n\tlist_for_each_entry_safe(v, n, &per_cpu(loaded_vmcss_on_cpu, cpu),\n\t\t\t\t loaded_vmcss_on_cpu_link)\n\t\t__loaded_vmcs_clear(v);\n}\n\nstatic void vmx_hardware_disable(void)\n{\n\tvmclear_local_loaded_vmcss();\n\n\tif (kvm_cpu_vmxoff())\n\t\tkvm_spurious_fault();\n\n\thv_reset_evmcs();\n\n\tintel_pt_handle_vmx(0);\n}\n\nstruct vmcs *alloc_vmcs_cpu(bool shadow, int cpu, gfp_t flags)\n{\n\tint node = cpu_to_node(cpu);\n\tstruct page *pages;\n\tstruct vmcs *vmcs;\n\n\tpages = __alloc_pages_node(node, flags, 0);\n\tif (!pages)\n\t\treturn NULL;\n\tvmcs = page_address(pages);\n\tmemset(vmcs, 0, vmcs_config.size);\n\n\t \n\tif (kvm_is_using_evmcs())\n\t\tvmcs->hdr.revision_id = KVM_EVMCS_VERSION;\n\telse\n\t\tvmcs->hdr.revision_id = vmcs_config.revision_id;\n\n\tif (shadow)\n\t\tvmcs->hdr.shadow_vmcs = 1;\n\treturn vmcs;\n}\n\nvoid free_vmcs(struct vmcs *vmcs)\n{\n\tfree_page((unsigned long)vmcs);\n}\n\n \nvoid free_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)\n{\n\tif (!loaded_vmcs->vmcs)\n\t\treturn;\n\tloaded_vmcs_clear(loaded_vmcs);\n\tfree_vmcs(loaded_vmcs->vmcs);\n\tloaded_vmcs->vmcs = NULL;\n\tif (loaded_vmcs->msr_bitmap)\n\t\tfree_page((unsigned long)loaded_vmcs->msr_bitmap);\n\tWARN_ON(loaded_vmcs->shadow_vmcs != NULL);\n}\n\nint alloc_loaded_vmcs(struct loaded_vmcs *loaded_vmcs)\n{\n\tloaded_vmcs->vmcs = alloc_vmcs(false);\n\tif (!loaded_vmcs->vmcs)\n\t\treturn -ENOMEM;\n\n\tvmcs_clear(loaded_vmcs->vmcs);\n\n\tloaded_vmcs->shadow_vmcs = NULL;\n\tloaded_vmcs->hv_timer_soft_disabled = false;\n\tloaded_vmcs->cpu = -1;\n\tloaded_vmcs->launched = 0;\n\n\tif (cpu_has_vmx_msr_bitmap()) {\n\t\tloaded_vmcs->msr_bitmap = (unsigned long *)\n\t\t\t\t__get_free_page(GFP_KERNEL_ACCOUNT);\n\t\tif (!loaded_vmcs->msr_bitmap)\n\t\t\tgoto out_vmcs;\n\t\tmemset(loaded_vmcs->msr_bitmap, 0xff, PAGE_SIZE);\n\t}\n\n\tmemset(&loaded_vmcs->host_state, 0, sizeof(struct vmcs_host_state));\n\tmemset(&loaded_vmcs->controls_shadow, 0,\n\t\tsizeof(struct vmcs_controls_shadow));\n\n\treturn 0;\n\nout_vmcs:\n\tfree_loaded_vmcs(loaded_vmcs);\n\treturn -ENOMEM;\n}\n\nstatic void free_kvm_area(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tfree_vmcs(per_cpu(vmxarea, cpu));\n\t\tper_cpu(vmxarea, cpu) = NULL;\n\t}\n}\n\nstatic __init int alloc_kvm_area(void)\n{\n\tint cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tstruct vmcs *vmcs;\n\n\t\tvmcs = alloc_vmcs_cpu(false, cpu, GFP_KERNEL);\n\t\tif (!vmcs) {\n\t\t\tfree_kvm_area();\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\t \n\t\tif (kvm_is_using_evmcs())\n\t\t\tvmcs->hdr.revision_id = vmcs_config.revision_id;\n\n\t\tper_cpu(vmxarea, cpu) = vmcs;\n\t}\n\treturn 0;\n}\n\nstatic void fix_pmode_seg(struct kvm_vcpu *vcpu, int seg,\n\t\tstruct kvm_segment *save)\n{\n\tif (!emulate_invalid_guest_state) {\n\t\t \n\t\tif (seg == VCPU_SREG_CS || seg == VCPU_SREG_SS)\n\t\t\tsave->selector &= ~SEGMENT_RPL_MASK;\n\t\tsave->dpl = save->selector & SEGMENT_RPL_MASK;\n\t\tsave->s = 1;\n\t}\n\t__vmx_set_segment(vcpu, save, seg);\n}\n\nstatic void enter_pmode(struct kvm_vcpu *vcpu)\n{\n\tunsigned long flags;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t \n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_ES], VCPU_SREG_ES);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_DS], VCPU_SREG_DS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_FS], VCPU_SREG_FS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_GS], VCPU_SREG_GS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_SS], VCPU_SREG_SS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_CS], VCPU_SREG_CS);\n\n\tvmx->rmode.vm86_active = 0;\n\n\t__vmx_set_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_TR], VCPU_SREG_TR);\n\n\tflags = vmcs_readl(GUEST_RFLAGS);\n\tflags &= RMODE_GUEST_OWNED_EFLAGS_BITS;\n\tflags |= vmx->rmode.save_rflags & ~RMODE_GUEST_OWNED_EFLAGS_BITS;\n\tvmcs_writel(GUEST_RFLAGS, flags);\n\n\tvmcs_writel(GUEST_CR4, (vmcs_readl(GUEST_CR4) & ~X86_CR4_VME) |\n\t\t\t(vmcs_readl(CR4_READ_SHADOW) & X86_CR4_VME));\n\n\tvmx_update_exception_bitmap(vcpu);\n\n\tfix_pmode_seg(vcpu, VCPU_SREG_CS, &vmx->rmode.segs[VCPU_SREG_CS]);\n\tfix_pmode_seg(vcpu, VCPU_SREG_SS, &vmx->rmode.segs[VCPU_SREG_SS]);\n\tfix_pmode_seg(vcpu, VCPU_SREG_ES, &vmx->rmode.segs[VCPU_SREG_ES]);\n\tfix_pmode_seg(vcpu, VCPU_SREG_DS, &vmx->rmode.segs[VCPU_SREG_DS]);\n\tfix_pmode_seg(vcpu, VCPU_SREG_FS, &vmx->rmode.segs[VCPU_SREG_FS]);\n\tfix_pmode_seg(vcpu, VCPU_SREG_GS, &vmx->rmode.segs[VCPU_SREG_GS]);\n}\n\nstatic void fix_rmode_seg(int seg, struct kvm_segment *save)\n{\n\tconst struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];\n\tstruct kvm_segment var = *save;\n\n\tvar.dpl = 0x3;\n\tif (seg == VCPU_SREG_CS)\n\t\tvar.type = 0x3;\n\n\tif (!emulate_invalid_guest_state) {\n\t\tvar.selector = var.base >> 4;\n\t\tvar.base = var.base & 0xffff0;\n\t\tvar.limit = 0xffff;\n\t\tvar.g = 0;\n\t\tvar.db = 0;\n\t\tvar.present = 1;\n\t\tvar.s = 1;\n\t\tvar.l = 0;\n\t\tvar.unusable = 0;\n\t\tvar.type = 0x3;\n\t\tvar.avl = 0;\n\t\tif (save->base & 0xf)\n\t\t\tpr_warn_once(\"segment base is not paragraph aligned \"\n\t\t\t\t     \"when entering protected mode (seg=%d)\", seg);\n\t}\n\n\tvmcs_write16(sf->selector, var.selector);\n\tvmcs_writel(sf->base, var.base);\n\tvmcs_write32(sf->limit, var.limit);\n\tvmcs_write32(sf->ar_bytes, vmx_segment_access_rights(&var));\n}\n\nstatic void enter_rmode(struct kvm_vcpu *vcpu)\n{\n\tunsigned long flags;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct kvm_vmx *kvm_vmx = to_kvm_vmx(vcpu->kvm);\n\n\t \n\tWARN_ON_ONCE(is_guest_mode(vcpu));\n\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_TR], VCPU_SREG_TR);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_ES], VCPU_SREG_ES);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_DS], VCPU_SREG_DS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_FS], VCPU_SREG_FS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_GS], VCPU_SREG_GS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_SS], VCPU_SREG_SS);\n\tvmx_get_segment(vcpu, &vmx->rmode.segs[VCPU_SREG_CS], VCPU_SREG_CS);\n\n\tvmx->rmode.vm86_active = 1;\n\n\tvmx_segment_cache_clear(vmx);\n\n\tvmcs_writel(GUEST_TR_BASE, kvm_vmx->tss_addr);\n\tvmcs_write32(GUEST_TR_LIMIT, RMODE_TSS_SIZE - 1);\n\tvmcs_write32(GUEST_TR_AR_BYTES, 0x008b);\n\n\tflags = vmcs_readl(GUEST_RFLAGS);\n\tvmx->rmode.save_rflags = flags;\n\n\tflags |= X86_EFLAGS_IOPL | X86_EFLAGS_VM;\n\n\tvmcs_writel(GUEST_RFLAGS, flags);\n\tvmcs_writel(GUEST_CR4, vmcs_readl(GUEST_CR4) | X86_CR4_VME);\n\tvmx_update_exception_bitmap(vcpu);\n\n\tfix_rmode_seg(VCPU_SREG_SS, &vmx->rmode.segs[VCPU_SREG_SS]);\n\tfix_rmode_seg(VCPU_SREG_CS, &vmx->rmode.segs[VCPU_SREG_CS]);\n\tfix_rmode_seg(VCPU_SREG_ES, &vmx->rmode.segs[VCPU_SREG_ES]);\n\tfix_rmode_seg(VCPU_SREG_DS, &vmx->rmode.segs[VCPU_SREG_DS]);\n\tfix_rmode_seg(VCPU_SREG_GS, &vmx->rmode.segs[VCPU_SREG_GS]);\n\tfix_rmode_seg(VCPU_SREG_FS, &vmx->rmode.segs[VCPU_SREG_FS]);\n}\n\nint vmx_set_efer(struct kvm_vcpu *vcpu, u64 efer)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t \n\tif (!vmx_find_uret_msr(vmx, MSR_EFER))\n\t\treturn 0;\n\n\tvcpu->arch.efer = efer;\n#ifdef CONFIG_X86_64\n\tif (efer & EFER_LMA)\n\t\tvm_entry_controls_setbit(vmx, VM_ENTRY_IA32E_MODE);\n\telse\n\t\tvm_entry_controls_clearbit(vmx, VM_ENTRY_IA32E_MODE);\n#else\n\tif (KVM_BUG_ON(efer & EFER_LMA, vcpu->kvm))\n\t\treturn 1;\n#endif\n\n\tvmx_setup_uret_msrs(vmx);\n\treturn 0;\n}\n\n#ifdef CONFIG_X86_64\n\nstatic void enter_lmode(struct kvm_vcpu *vcpu)\n{\n\tu32 guest_tr_ar;\n\n\tvmx_segment_cache_clear(to_vmx(vcpu));\n\n\tguest_tr_ar = vmcs_read32(GUEST_TR_AR_BYTES);\n\tif ((guest_tr_ar & VMX_AR_TYPE_MASK) != VMX_AR_TYPE_BUSY_64_TSS) {\n\t\tpr_debug_ratelimited(\"%s: tss fixup for long mode. \\n\",\n\t\t\t\t     __func__);\n\t\tvmcs_write32(GUEST_TR_AR_BYTES,\n\t\t\t     (guest_tr_ar & ~VMX_AR_TYPE_MASK)\n\t\t\t     | VMX_AR_TYPE_BUSY_64_TSS);\n\t}\n\tvmx_set_efer(vcpu, vcpu->arch.efer | EFER_LMA);\n}\n\nstatic void exit_lmode(struct kvm_vcpu *vcpu)\n{\n\tvmx_set_efer(vcpu, vcpu->arch.efer & ~EFER_LMA);\n}\n\n#endif\n\nstatic void vmx_flush_tlb_all(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t \n\tif (enable_ept) {\n\t\tept_sync_global();\n\t} else if (enable_vpid) {\n\t\tif (cpu_has_vmx_invvpid_global()) {\n\t\t\tvpid_sync_vcpu_global();\n\t\t} else {\n\t\t\tvpid_sync_vcpu_single(vmx->vpid);\n\t\t\tvpid_sync_vcpu_single(vmx->nested.vpid02);\n\t\t}\n\t}\n}\n\nstatic inline int vmx_get_current_vpid(struct kvm_vcpu *vcpu)\n{\n\tif (is_guest_mode(vcpu))\n\t\treturn nested_get_vpid02(vcpu);\n\treturn to_vmx(vcpu)->vpid;\n}\n\nstatic void vmx_flush_tlb_current(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.mmu;\n\tu64 root_hpa = mmu->root.hpa;\n\n\t \n\tif (!VALID_PAGE(root_hpa))\n\t\treturn;\n\n\tif (enable_ept)\n\t\tept_sync_context(construct_eptp(vcpu, root_hpa,\n\t\t\t\t\t\tmmu->root_role.level));\n\telse\n\t\tvpid_sync_context(vmx_get_current_vpid(vcpu));\n}\n\nstatic void vmx_flush_tlb_gva(struct kvm_vcpu *vcpu, gva_t addr)\n{\n\t \n\tvpid_sync_vcpu_addr(vmx_get_current_vpid(vcpu), addr);\n}\n\nstatic void vmx_flush_tlb_guest(struct kvm_vcpu *vcpu)\n{\n\t \n\tvpid_sync_context(vmx_get_current_vpid(vcpu));\n}\n\nvoid vmx_ept_load_pdptrs(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\n\tif (!kvm_register_is_dirty(vcpu, VCPU_EXREG_PDPTR))\n\t\treturn;\n\n\tif (is_pae_paging(vcpu)) {\n\t\tvmcs_write64(GUEST_PDPTR0, mmu->pdptrs[0]);\n\t\tvmcs_write64(GUEST_PDPTR1, mmu->pdptrs[1]);\n\t\tvmcs_write64(GUEST_PDPTR2, mmu->pdptrs[2]);\n\t\tvmcs_write64(GUEST_PDPTR3, mmu->pdptrs[3]);\n\t}\n}\n\nvoid ept_save_pdptrs(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_mmu *mmu = vcpu->arch.walk_mmu;\n\n\tif (WARN_ON_ONCE(!is_pae_paging(vcpu)))\n\t\treturn;\n\n\tmmu->pdptrs[0] = vmcs_read64(GUEST_PDPTR0);\n\tmmu->pdptrs[1] = vmcs_read64(GUEST_PDPTR1);\n\tmmu->pdptrs[2] = vmcs_read64(GUEST_PDPTR2);\n\tmmu->pdptrs[3] = vmcs_read64(GUEST_PDPTR3);\n\n\tkvm_register_mark_available(vcpu, VCPU_EXREG_PDPTR);\n}\n\n#define CR3_EXITING_BITS (CPU_BASED_CR3_LOAD_EXITING | \\\n\t\t\t  CPU_BASED_CR3_STORE_EXITING)\n\nstatic bool vmx_is_valid_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)\n{\n\tif (is_guest_mode(vcpu))\n\t\treturn nested_guest_cr0_valid(vcpu, cr0);\n\n\tif (to_vmx(vcpu)->nested.vmxon)\n\t\treturn nested_host_cr0_valid(vcpu, cr0);\n\n\treturn true;\n}\n\nvoid vmx_set_cr0(struct kvm_vcpu *vcpu, unsigned long cr0)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long hw_cr0, old_cr0_pg;\n\tu32 tmp;\n\n\told_cr0_pg = kvm_read_cr0_bits(vcpu, X86_CR0_PG);\n\n\thw_cr0 = (cr0 & ~KVM_VM_CR0_ALWAYS_OFF);\n\tif (enable_unrestricted_guest)\n\t\thw_cr0 |= KVM_VM_CR0_ALWAYS_ON_UNRESTRICTED_GUEST;\n\telse {\n\t\thw_cr0 |= KVM_VM_CR0_ALWAYS_ON;\n\t\tif (!enable_ept)\n\t\t\thw_cr0 |= X86_CR0_WP;\n\n\t\tif (vmx->rmode.vm86_active && (cr0 & X86_CR0_PE))\n\t\t\tenter_pmode(vcpu);\n\n\t\tif (!vmx->rmode.vm86_active && !(cr0 & X86_CR0_PE))\n\t\t\tenter_rmode(vcpu);\n\t}\n\n\tvmcs_writel(CR0_READ_SHADOW, cr0);\n\tvmcs_writel(GUEST_CR0, hw_cr0);\n\tvcpu->arch.cr0 = cr0;\n\tkvm_register_mark_available(vcpu, VCPU_EXREG_CR0);\n\n#ifdef CONFIG_X86_64\n\tif (vcpu->arch.efer & EFER_LME) {\n\t\tif (!old_cr0_pg && (cr0 & X86_CR0_PG))\n\t\t\tenter_lmode(vcpu);\n\t\telse if (old_cr0_pg && !(cr0 & X86_CR0_PG))\n\t\t\texit_lmode(vcpu);\n\t}\n#endif\n\n\tif (enable_ept && !enable_unrestricted_guest) {\n\t\t \n\t\tif (!kvm_register_is_available(vcpu, VCPU_EXREG_CR3))\n\t\t\tvmx_cache_reg(vcpu, VCPU_EXREG_CR3);\n\n\t\t \n\t\tif (!(cr0 & X86_CR0_PG)) {\n\t\t\texec_controls_setbit(vmx, CR3_EXITING_BITS);\n\t\t} else if (!is_guest_mode(vcpu)) {\n\t\t\texec_controls_clearbit(vmx, CR3_EXITING_BITS);\n\t\t} else {\n\t\t\ttmp = exec_controls_get(vmx);\n\t\t\ttmp &= ~CR3_EXITING_BITS;\n\t\t\ttmp |= get_vmcs12(vcpu)->cpu_based_vm_exec_control & CR3_EXITING_BITS;\n\t\t\texec_controls_set(vmx, tmp);\n\t\t}\n\n\t\t \n\t\tif ((old_cr0_pg ^ cr0) & X86_CR0_PG)\n\t\t\tvmx_set_cr4(vcpu, kvm_read_cr4(vcpu));\n\n\t\t \n\t\tif (!(old_cr0_pg & X86_CR0_PG) && (cr0 & X86_CR0_PG))\n\t\t\tkvm_register_mark_dirty(vcpu, VCPU_EXREG_CR3);\n\t}\n\n\t \n\tvmx->emulation_required = vmx_emulation_required(vcpu);\n}\n\nstatic int vmx_get_max_ept_level(void)\n{\n\tif (cpu_has_vmx_ept_5levels())\n\t\treturn 5;\n\treturn 4;\n}\n\nu64 construct_eptp(struct kvm_vcpu *vcpu, hpa_t root_hpa, int root_level)\n{\n\tu64 eptp = VMX_EPTP_MT_WB;\n\n\teptp |= (root_level == 5) ? VMX_EPTP_PWL_5 : VMX_EPTP_PWL_4;\n\n\tif (enable_ept_ad_bits &&\n\t    (!is_guest_mode(vcpu) || nested_ept_ad_enabled(vcpu)))\n\t\teptp |= VMX_EPTP_AD_ENABLE_BIT;\n\teptp |= root_hpa;\n\n\treturn eptp;\n}\n\nstatic void vmx_load_mmu_pgd(struct kvm_vcpu *vcpu, hpa_t root_hpa,\n\t\t\t     int root_level)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\tbool update_guest_cr3 = true;\n\tunsigned long guest_cr3;\n\tu64 eptp;\n\n\tif (enable_ept) {\n\t\teptp = construct_eptp(vcpu, root_hpa, root_level);\n\t\tvmcs_write64(EPT_POINTER, eptp);\n\n\t\thv_track_root_tdp(vcpu, root_hpa);\n\n\t\tif (!enable_unrestricted_guest && !is_paging(vcpu))\n\t\t\tguest_cr3 = to_kvm_vmx(kvm)->ept_identity_map_addr;\n\t\telse if (kvm_register_is_dirty(vcpu, VCPU_EXREG_CR3))\n\t\t\tguest_cr3 = vcpu->arch.cr3;\n\t\telse  \n\t\t\tupdate_guest_cr3 = false;\n\t\tvmx_ept_load_pdptrs(vcpu);\n\t} else {\n\t\tguest_cr3 = root_hpa | kvm_get_active_pcid(vcpu);\n\t}\n\n\tif (update_guest_cr3)\n\t\tvmcs_writel(GUEST_CR3, guest_cr3);\n}\n\n\nstatic bool vmx_is_valid_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)\n{\n\t \n\tif ((cr4 & X86_CR4_VMXE) && is_smm(vcpu))\n\t\treturn false;\n\n\tif (to_vmx(vcpu)->nested.vmxon && !nested_cr4_valid(vcpu, cr4))\n\t\treturn false;\n\n\treturn true;\n}\n\nvoid vmx_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)\n{\n\tunsigned long old_cr4 = kvm_read_cr4(vcpu);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long hw_cr4;\n\n\t \n\thw_cr4 = (cr4_read_shadow() & X86_CR4_MCE) | (cr4 & ~X86_CR4_MCE);\n\tif (enable_unrestricted_guest)\n\t\thw_cr4 |= KVM_VM_CR4_ALWAYS_ON_UNRESTRICTED_GUEST;\n\telse if (vmx->rmode.vm86_active)\n\t\thw_cr4 |= KVM_RMODE_VM_CR4_ALWAYS_ON;\n\telse\n\t\thw_cr4 |= KVM_PMODE_VM_CR4_ALWAYS_ON;\n\n\tif (vmx_umip_emulated()) {\n\t\tif (cr4 & X86_CR4_UMIP) {\n\t\t\tsecondary_exec_controls_setbit(vmx, SECONDARY_EXEC_DESC);\n\t\t\thw_cr4 &= ~X86_CR4_UMIP;\n\t\t} else if (!is_guest_mode(vcpu) ||\n\t\t\t!nested_cpu_has2(get_vmcs12(vcpu), SECONDARY_EXEC_DESC)) {\n\t\t\tsecondary_exec_controls_clearbit(vmx, SECONDARY_EXEC_DESC);\n\t\t}\n\t}\n\n\tvcpu->arch.cr4 = cr4;\n\tkvm_register_mark_available(vcpu, VCPU_EXREG_CR4);\n\n\tif (!enable_unrestricted_guest) {\n\t\tif (enable_ept) {\n\t\t\tif (!is_paging(vcpu)) {\n\t\t\t\thw_cr4 &= ~X86_CR4_PAE;\n\t\t\t\thw_cr4 |= X86_CR4_PSE;\n\t\t\t} else if (!(cr4 & X86_CR4_PAE)) {\n\t\t\t\thw_cr4 &= ~X86_CR4_PAE;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (!is_paging(vcpu))\n\t\t\thw_cr4 &= ~(X86_CR4_SMEP | X86_CR4_SMAP | X86_CR4_PKE);\n\t}\n\n\tvmcs_writel(CR4_READ_SHADOW, cr4);\n\tvmcs_writel(GUEST_CR4, hw_cr4);\n\n\tif ((cr4 ^ old_cr4) & (X86_CR4_OSXSAVE | X86_CR4_PKE))\n\t\tkvm_update_cpuid_runtime(vcpu);\n}\n\nvoid vmx_get_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 ar;\n\n\tif (vmx->rmode.vm86_active && seg != VCPU_SREG_LDTR) {\n\t\t*var = vmx->rmode.segs[seg];\n\t\tif (seg == VCPU_SREG_TR\n\t\t    || var->selector == vmx_read_guest_seg_selector(vmx, seg))\n\t\t\treturn;\n\t\tvar->base = vmx_read_guest_seg_base(vmx, seg);\n\t\tvar->selector = vmx_read_guest_seg_selector(vmx, seg);\n\t\treturn;\n\t}\n\tvar->base = vmx_read_guest_seg_base(vmx, seg);\n\tvar->limit = vmx_read_guest_seg_limit(vmx, seg);\n\tvar->selector = vmx_read_guest_seg_selector(vmx, seg);\n\tar = vmx_read_guest_seg_ar(vmx, seg);\n\tvar->unusable = (ar >> 16) & 1;\n\tvar->type = ar & 15;\n\tvar->s = (ar >> 4) & 1;\n\tvar->dpl = (ar >> 5) & 3;\n\t \n\tvar->present = !var->unusable;\n\tvar->avl = (ar >> 12) & 1;\n\tvar->l = (ar >> 13) & 1;\n\tvar->db = (ar >> 14) & 1;\n\tvar->g = (ar >> 15) & 1;\n}\n\nstatic u64 vmx_get_segment_base(struct kvm_vcpu *vcpu, int seg)\n{\n\tstruct kvm_segment s;\n\n\tif (to_vmx(vcpu)->rmode.vm86_active) {\n\t\tvmx_get_segment(vcpu, &s, seg);\n\t\treturn s.base;\n\t}\n\treturn vmx_read_guest_seg_base(to_vmx(vcpu), seg);\n}\n\nint vmx_get_cpl(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (unlikely(vmx->rmode.vm86_active))\n\t\treturn 0;\n\telse {\n\t\tint ar = vmx_read_guest_seg_ar(vmx, VCPU_SREG_SS);\n\t\treturn VMX_AR_DPL(ar);\n\t}\n}\n\nstatic u32 vmx_segment_access_rights(struct kvm_segment *var)\n{\n\tu32 ar;\n\n\tar = var->type & 15;\n\tar |= (var->s & 1) << 4;\n\tar |= (var->dpl & 3) << 5;\n\tar |= (var->present & 1) << 7;\n\tar |= (var->avl & 1) << 12;\n\tar |= (var->l & 1) << 13;\n\tar |= (var->db & 1) << 14;\n\tar |= (var->g & 1) << 15;\n\tar |= (var->unusable || !var->present) << 16;\n\n\treturn ar;\n}\n\nvoid __vmx_set_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tconst struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];\n\n\tvmx_segment_cache_clear(vmx);\n\n\tif (vmx->rmode.vm86_active && seg != VCPU_SREG_LDTR) {\n\t\tvmx->rmode.segs[seg] = *var;\n\t\tif (seg == VCPU_SREG_TR)\n\t\t\tvmcs_write16(sf->selector, var->selector);\n\t\telse if (var->s)\n\t\t\tfix_rmode_seg(seg, &vmx->rmode.segs[seg]);\n\t\treturn;\n\t}\n\n\tvmcs_writel(sf->base, var->base);\n\tvmcs_write32(sf->limit, var->limit);\n\tvmcs_write16(sf->selector, var->selector);\n\n\t \n\tif (is_unrestricted_guest(vcpu) && (seg != VCPU_SREG_LDTR))\n\t\tvar->type |= 0x1;  \n\n\tvmcs_write32(sf->ar_bytes, vmx_segment_access_rights(var));\n}\n\nstatic void vmx_set_segment(struct kvm_vcpu *vcpu, struct kvm_segment *var, int seg)\n{\n\t__vmx_set_segment(vcpu, var, seg);\n\n\tto_vmx(vcpu)->emulation_required = vmx_emulation_required(vcpu);\n}\n\nstatic void vmx_get_cs_db_l_bits(struct kvm_vcpu *vcpu, int *db, int *l)\n{\n\tu32 ar = vmx_read_guest_seg_ar(to_vmx(vcpu), VCPU_SREG_CS);\n\n\t*db = (ar >> 14) & 1;\n\t*l = (ar >> 13) & 1;\n}\n\nstatic void vmx_get_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\n{\n\tdt->size = vmcs_read32(GUEST_IDTR_LIMIT);\n\tdt->address = vmcs_readl(GUEST_IDTR_BASE);\n}\n\nstatic void vmx_set_idt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\n{\n\tvmcs_write32(GUEST_IDTR_LIMIT, dt->size);\n\tvmcs_writel(GUEST_IDTR_BASE, dt->address);\n}\n\nstatic void vmx_get_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\n{\n\tdt->size = vmcs_read32(GUEST_GDTR_LIMIT);\n\tdt->address = vmcs_readl(GUEST_GDTR_BASE);\n}\n\nstatic void vmx_set_gdt(struct kvm_vcpu *vcpu, struct desc_ptr *dt)\n{\n\tvmcs_write32(GUEST_GDTR_LIMIT, dt->size);\n\tvmcs_writel(GUEST_GDTR_BASE, dt->address);\n}\n\nstatic bool rmode_segment_valid(struct kvm_vcpu *vcpu, int seg)\n{\n\tstruct kvm_segment var;\n\tu32 ar;\n\n\tvmx_get_segment(vcpu, &var, seg);\n\tvar.dpl = 0x3;\n\tif (seg == VCPU_SREG_CS)\n\t\tvar.type = 0x3;\n\tar = vmx_segment_access_rights(&var);\n\n\tif (var.base != (var.selector << 4))\n\t\treturn false;\n\tif (var.limit != 0xffff)\n\t\treturn false;\n\tif (ar != 0xf3)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool code_segment_valid(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment cs;\n\tunsigned int cs_rpl;\n\n\tvmx_get_segment(vcpu, &cs, VCPU_SREG_CS);\n\tcs_rpl = cs.selector & SEGMENT_RPL_MASK;\n\n\tif (cs.unusable)\n\t\treturn false;\n\tif (~cs.type & (VMX_AR_TYPE_CODE_MASK|VMX_AR_TYPE_ACCESSES_MASK))\n\t\treturn false;\n\tif (!cs.s)\n\t\treturn false;\n\tif (cs.type & VMX_AR_TYPE_WRITEABLE_MASK) {\n\t\tif (cs.dpl > cs_rpl)\n\t\t\treturn false;\n\t} else {\n\t\tif (cs.dpl != cs_rpl)\n\t\t\treturn false;\n\t}\n\tif (!cs.present)\n\t\treturn false;\n\n\t \n\treturn true;\n}\n\nstatic bool stack_segment_valid(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment ss;\n\tunsigned int ss_rpl;\n\n\tvmx_get_segment(vcpu, &ss, VCPU_SREG_SS);\n\tss_rpl = ss.selector & SEGMENT_RPL_MASK;\n\n\tif (ss.unusable)\n\t\treturn true;\n\tif (ss.type != 3 && ss.type != 7)\n\t\treturn false;\n\tif (!ss.s)\n\t\treturn false;\n\tif (ss.dpl != ss_rpl)  \n\t\treturn false;\n\tif (!ss.present)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool data_segment_valid(struct kvm_vcpu *vcpu, int seg)\n{\n\tstruct kvm_segment var;\n\tunsigned int rpl;\n\n\tvmx_get_segment(vcpu, &var, seg);\n\trpl = var.selector & SEGMENT_RPL_MASK;\n\n\tif (var.unusable)\n\t\treturn true;\n\tif (!var.s)\n\t\treturn false;\n\tif (!var.present)\n\t\treturn false;\n\tif (~var.type & (VMX_AR_TYPE_CODE_MASK|VMX_AR_TYPE_WRITEABLE_MASK)) {\n\t\tif (var.dpl < rpl)  \n\t\t\treturn false;\n\t}\n\n\t \n\treturn true;\n}\n\nstatic bool tr_valid(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment tr;\n\n\tvmx_get_segment(vcpu, &tr, VCPU_SREG_TR);\n\n\tif (tr.unusable)\n\t\treturn false;\n\tif (tr.selector & SEGMENT_TI_MASK)\t \n\t\treturn false;\n\tif (tr.type != 3 && tr.type != 11)  \n\t\treturn false;\n\tif (!tr.present)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool ldtr_valid(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment ldtr;\n\n\tvmx_get_segment(vcpu, &ldtr, VCPU_SREG_LDTR);\n\n\tif (ldtr.unusable)\n\t\treturn true;\n\tif (ldtr.selector & SEGMENT_TI_MASK)\t \n\t\treturn false;\n\tif (ldtr.type != 2)\n\t\treturn false;\n\tif (!ldtr.present)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic bool cs_ss_rpl_check(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment cs, ss;\n\n\tvmx_get_segment(vcpu, &cs, VCPU_SREG_CS);\n\tvmx_get_segment(vcpu, &ss, VCPU_SREG_SS);\n\n\treturn ((cs.selector & SEGMENT_RPL_MASK) ==\n\t\t (ss.selector & SEGMENT_RPL_MASK));\n}\n\n \nbool __vmx_guest_state_valid(struct kvm_vcpu *vcpu)\n{\n\t \n\tif (!is_protmode(vcpu) || (vmx_get_rflags(vcpu) & X86_EFLAGS_VM)) {\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_CS))\n\t\t\treturn false;\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_SS))\n\t\t\treturn false;\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_DS))\n\t\t\treturn false;\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_ES))\n\t\t\treturn false;\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_FS))\n\t\t\treturn false;\n\t\tif (!rmode_segment_valid(vcpu, VCPU_SREG_GS))\n\t\t\treturn false;\n\t} else {\n\t \n\t\tif (!cs_ss_rpl_check(vcpu))\n\t\t\treturn false;\n\t\tif (!code_segment_valid(vcpu))\n\t\t\treturn false;\n\t\tif (!stack_segment_valid(vcpu))\n\t\t\treturn false;\n\t\tif (!data_segment_valid(vcpu, VCPU_SREG_DS))\n\t\t\treturn false;\n\t\tif (!data_segment_valid(vcpu, VCPU_SREG_ES))\n\t\t\treturn false;\n\t\tif (!data_segment_valid(vcpu, VCPU_SREG_FS))\n\t\t\treturn false;\n\t\tif (!data_segment_valid(vcpu, VCPU_SREG_GS))\n\t\t\treturn false;\n\t\tif (!tr_valid(vcpu))\n\t\t\treturn false;\n\t\tif (!ldtr_valid(vcpu))\n\t\t\treturn false;\n\t}\n\t \n\n\treturn true;\n}\n\nstatic int init_rmode_tss(struct kvm *kvm, void __user *ua)\n{\n\tconst void *zero_page = (const void *) __va(page_to_phys(ZERO_PAGE(0)));\n\tu16 data;\n\tint i;\n\n\tfor (i = 0; i < 3; i++) {\n\t\tif (__copy_to_user(ua + PAGE_SIZE * i, zero_page, PAGE_SIZE))\n\t\t\treturn -EFAULT;\n\t}\n\n\tdata = TSS_BASE_SIZE + TSS_REDIRECTION_SIZE;\n\tif (__copy_to_user(ua + TSS_IOPB_BASE_OFFSET, &data, sizeof(u16)))\n\t\treturn -EFAULT;\n\n\tdata = ~0;\n\tif (__copy_to_user(ua + RMODE_TSS_SIZE - 1, &data, sizeof(u8)))\n\t\treturn -EFAULT;\n\n\treturn 0;\n}\n\nstatic int init_rmode_identity_map(struct kvm *kvm)\n{\n\tstruct kvm_vmx *kvm_vmx = to_kvm_vmx(kvm);\n\tint i, r = 0;\n\tvoid __user *uaddr;\n\tu32 tmp;\n\n\t \n\tmutex_lock(&kvm->slots_lock);\n\n\tif (likely(kvm_vmx->ept_identity_pagetable_done))\n\t\tgoto out;\n\n\tif (!kvm_vmx->ept_identity_map_addr)\n\t\tkvm_vmx->ept_identity_map_addr = VMX_EPT_IDENTITY_PAGETABLE_ADDR;\n\n\tuaddr = __x86_set_memory_region(kvm,\n\t\t\t\t\tIDENTITY_PAGETABLE_PRIVATE_MEMSLOT,\n\t\t\t\t\tkvm_vmx->ept_identity_map_addr,\n\t\t\t\t\tPAGE_SIZE);\n\tif (IS_ERR(uaddr)) {\n\t\tr = PTR_ERR(uaddr);\n\t\tgoto out;\n\t}\n\n\t \n\tfor (i = 0; i < (PAGE_SIZE / sizeof(tmp)); i++) {\n\t\ttmp = (i << 22) + (_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |\n\t\t\t_PAGE_ACCESSED | _PAGE_DIRTY | _PAGE_PSE);\n\t\tif (__copy_to_user(uaddr + i * sizeof(tmp), &tmp, sizeof(tmp))) {\n\t\t\tr = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\t}\n\tkvm_vmx->ept_identity_pagetable_done = true;\n\nout:\n\tmutex_unlock(&kvm->slots_lock);\n\treturn r;\n}\n\nstatic void seg_setup(int seg)\n{\n\tconst struct kvm_vmx_segment_field *sf = &kvm_vmx_segment_fields[seg];\n\tunsigned int ar;\n\n\tvmcs_write16(sf->selector, 0);\n\tvmcs_writel(sf->base, 0);\n\tvmcs_write32(sf->limit, 0xffff);\n\tar = 0x93;\n\tif (seg == VCPU_SREG_CS)\n\t\tar |= 0x08;  \n\n\tvmcs_write32(sf->ar_bytes, ar);\n}\n\nint allocate_vpid(void)\n{\n\tint vpid;\n\n\tif (!enable_vpid)\n\t\treturn 0;\n\tspin_lock(&vmx_vpid_lock);\n\tvpid = find_first_zero_bit(vmx_vpid_bitmap, VMX_NR_VPIDS);\n\tif (vpid < VMX_NR_VPIDS)\n\t\t__set_bit(vpid, vmx_vpid_bitmap);\n\telse\n\t\tvpid = 0;\n\tspin_unlock(&vmx_vpid_lock);\n\treturn vpid;\n}\n\nvoid free_vpid(int vpid)\n{\n\tif (!enable_vpid || vpid == 0)\n\t\treturn;\n\tspin_lock(&vmx_vpid_lock);\n\t__clear_bit(vpid, vmx_vpid_bitmap);\n\tspin_unlock(&vmx_vpid_lock);\n}\n\nstatic void vmx_msr_bitmap_l01_changed(struct vcpu_vmx *vmx)\n{\n\t \n\tif (kvm_is_using_evmcs()) {\n\t\tstruct hv_enlightened_vmcs *evmcs = (void *)vmx->vmcs01.vmcs;\n\n\t\tif (evmcs->hv_enlightenments_control.msr_bitmap)\n\t\t\tevmcs->hv_clean_fields &=\n\t\t\t\t~HV_VMX_ENLIGHTENED_CLEAN_FIELD_MSR_BITMAP;\n\t}\n\n\tvmx->nested.force_msr_bitmap_recalc = true;\n}\n\nvoid vmx_disable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long *msr_bitmap = vmx->vmcs01.msr_bitmap;\n\n\tif (!cpu_has_vmx_msr_bitmap())\n\t\treturn;\n\n\tvmx_msr_bitmap_l01_changed(vmx);\n\n\t \n\tif (is_valid_passthrough_msr(msr)) {\n\t\tint idx = possible_passthrough_msr_slot(msr);\n\n\t\tif (idx != -ENOENT) {\n\t\t\tif (type & MSR_TYPE_R)\n\t\t\t\tclear_bit(idx, vmx->shadow_msr_intercept.read);\n\t\t\tif (type & MSR_TYPE_W)\n\t\t\t\tclear_bit(idx, vmx->shadow_msr_intercept.write);\n\t\t}\n\t}\n\n\tif ((type & MSR_TYPE_R) &&\n\t    !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_READ)) {\n\t\tvmx_set_msr_bitmap_read(msr_bitmap, msr);\n\t\ttype &= ~MSR_TYPE_R;\n\t}\n\n\tif ((type & MSR_TYPE_W) &&\n\t    !kvm_msr_allowed(vcpu, msr, KVM_MSR_FILTER_WRITE)) {\n\t\tvmx_set_msr_bitmap_write(msr_bitmap, msr);\n\t\ttype &= ~MSR_TYPE_W;\n\t}\n\n\tif (type & MSR_TYPE_R)\n\t\tvmx_clear_msr_bitmap_read(msr_bitmap, msr);\n\n\tif (type & MSR_TYPE_W)\n\t\tvmx_clear_msr_bitmap_write(msr_bitmap, msr);\n}\n\nvoid vmx_enable_intercept_for_msr(struct kvm_vcpu *vcpu, u32 msr, int type)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long *msr_bitmap = vmx->vmcs01.msr_bitmap;\n\n\tif (!cpu_has_vmx_msr_bitmap())\n\t\treturn;\n\n\tvmx_msr_bitmap_l01_changed(vmx);\n\n\t \n\tif (is_valid_passthrough_msr(msr)) {\n\t\tint idx = possible_passthrough_msr_slot(msr);\n\n\t\tif (idx != -ENOENT) {\n\t\t\tif (type & MSR_TYPE_R)\n\t\t\t\tset_bit(idx, vmx->shadow_msr_intercept.read);\n\t\t\tif (type & MSR_TYPE_W)\n\t\t\t\tset_bit(idx, vmx->shadow_msr_intercept.write);\n\t\t}\n\t}\n\n\tif (type & MSR_TYPE_R)\n\t\tvmx_set_msr_bitmap_read(msr_bitmap, msr);\n\n\tif (type & MSR_TYPE_W)\n\t\tvmx_set_msr_bitmap_write(msr_bitmap, msr);\n}\n\nstatic void vmx_update_msr_bitmap_x2apic(struct kvm_vcpu *vcpu)\n{\n\t \n\tconst int read_idx = APIC_BASE_MSR / BITS_PER_LONG_LONG;\n\tconst int write_idx = read_idx + (0x800 / sizeof(u64));\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu64 *msr_bitmap = (u64 *)vmx->vmcs01.msr_bitmap;\n\tu8 mode;\n\n\tif (!cpu_has_vmx_msr_bitmap() || WARN_ON_ONCE(!lapic_in_kernel(vcpu)))\n\t\treturn;\n\n\tif (cpu_has_secondary_exec_ctrls() &&\n\t    (secondary_exec_controls_get(vmx) &\n\t     SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE)) {\n\t\tmode = MSR_BITMAP_MODE_X2APIC;\n\t\tif (enable_apicv && kvm_vcpu_apicv_active(vcpu))\n\t\t\tmode |= MSR_BITMAP_MODE_X2APIC_APICV;\n\t} else {\n\t\tmode = 0;\n\t}\n\n\tif (mode == vmx->x2apic_msr_bitmap_mode)\n\t\treturn;\n\n\tvmx->x2apic_msr_bitmap_mode = mode;\n\n\t \n\tif (mode & MSR_BITMAP_MODE_X2APIC_APICV)\n\t\tmsr_bitmap[read_idx] = ~kvm_lapic_readable_reg_mask(vcpu->arch.apic);\n\telse\n\t\tmsr_bitmap[read_idx] = ~0ull;\n\tmsr_bitmap[write_idx] = ~0ull;\n\n\t \n\tvmx_set_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TASKPRI), MSR_TYPE_RW,\n\t\t\t\t  !(mode & MSR_BITMAP_MODE_X2APIC));\n\n\tif (mode & MSR_BITMAP_MODE_X2APIC_APICV) {\n\t\tvmx_enable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_TMCCT), MSR_TYPE_RW);\n\t\tvmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_EOI), MSR_TYPE_W);\n\t\tvmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_SELF_IPI), MSR_TYPE_W);\n\t\tif (enable_ipiv)\n\t\t\tvmx_disable_intercept_for_msr(vcpu, X2APIC_MSR(APIC_ICR), MSR_TYPE_RW);\n\t}\n}\n\nvoid pt_update_intercept_for_msr(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tbool flag = !(vmx->pt_desc.guest.ctl & RTIT_CTL_TRACEEN);\n\tu32 i;\n\n\tvmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_STATUS, MSR_TYPE_RW, flag);\n\tvmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_BASE, MSR_TYPE_RW, flag);\n\tvmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_OUTPUT_MASK, MSR_TYPE_RW, flag);\n\tvmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_CR3_MATCH, MSR_TYPE_RW, flag);\n\tfor (i = 0; i < vmx->pt_desc.num_address_ranges; i++) {\n\t\tvmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_A + i * 2, MSR_TYPE_RW, flag);\n\t\tvmx_set_intercept_for_msr(vcpu, MSR_IA32_RTIT_ADDR0_B + i * 2, MSR_TYPE_RW, flag);\n\t}\n}\n\nstatic bool vmx_guest_apic_has_interrupt(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tvoid *vapic_page;\n\tu32 vppr;\n\tint rvi;\n\n\tif (WARN_ON_ONCE(!is_guest_mode(vcpu)) ||\n\t\t!nested_cpu_has_vid(get_vmcs12(vcpu)) ||\n\t\tWARN_ON_ONCE(!vmx->nested.virtual_apic_map.gfn))\n\t\treturn false;\n\n\trvi = vmx_get_rvi();\n\n\tvapic_page = vmx->nested.virtual_apic_map.hva;\n\tvppr = *((u32 *)(vapic_page + APIC_PROCPRI));\n\n\treturn ((rvi & 0xf0) > (vppr & 0xf0));\n}\n\nstatic void vmx_msr_filter_changed(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 i;\n\n\t \n\tfor (i = 0; i < ARRAY_SIZE(vmx_possible_passthrough_msrs); i++) {\n\t\tu32 msr = vmx_possible_passthrough_msrs[i];\n\n\t\tif (!test_bit(i, vmx->shadow_msr_intercept.read))\n\t\t\tvmx_disable_intercept_for_msr(vcpu, msr, MSR_TYPE_R);\n\n\t\tif (!test_bit(i, vmx->shadow_msr_intercept.write))\n\t\t\tvmx_disable_intercept_for_msr(vcpu, msr, MSR_TYPE_W);\n\t}\n\n\t \n\tif (vmx_pt_mode_is_host_guest())\n\t\tpt_update_intercept_for_msr(vcpu);\n}\n\nstatic inline void kvm_vcpu_trigger_posted_interrupt(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t     int pi_vec)\n{\n#ifdef CONFIG_SMP\n\tif (vcpu->mode == IN_GUEST_MODE) {\n\t\t \n\n\t\tif (vcpu != kvm_get_running_vcpu())\n\t\t\t__apic_send_IPI_mask(get_cpu_mask(vcpu->cpu), pi_vec);\n\t\treturn;\n\t}\n#endif\n\t \n\tkvm_vcpu_wake_up(vcpu);\n}\n\nstatic int vmx_deliver_nested_posted_interrupt(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\tint vector)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (is_guest_mode(vcpu) &&\n\t    vector == vmx->nested.posted_intr_nv) {\n\t\t \n\t\tvmx->nested.pi_pending = true;\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\t\t \n\t\tsmp_mb__after_atomic();\n\n\t\t \n\t\tkvm_vcpu_trigger_posted_interrupt(vcpu, POSTED_INTR_NESTED_VECTOR);\n\t\treturn 0;\n\t}\n\treturn -1;\n}\n \nstatic int vmx_deliver_posted_interrupt(struct kvm_vcpu *vcpu, int vector)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint r;\n\n\tr = vmx_deliver_nested_posted_interrupt(vcpu, vector);\n\tif (!r)\n\t\treturn 0;\n\n\t \n\tif (!vcpu->arch.apic->apicv_active)\n\t\treturn -1;\n\n\tif (pi_test_and_set_pir(vector, &vmx->pi_desc))\n\t\treturn 0;\n\n\t \n\tif (pi_test_and_set_on(&vmx->pi_desc))\n\t\treturn 0;\n\n\t \n\tkvm_vcpu_trigger_posted_interrupt(vcpu, POSTED_INTR_VECTOR);\n\treturn 0;\n}\n\nstatic void vmx_deliver_interrupt(struct kvm_lapic *apic, int delivery_mode,\n\t\t\t\t  int trig_mode, int vector)\n{\n\tstruct kvm_vcpu *vcpu = apic->vcpu;\n\n\tif (vmx_deliver_posted_interrupt(vcpu, vector)) {\n\t\tkvm_lapic_set_irr(vector, apic);\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t} else {\n\t\ttrace_kvm_apicv_accept_irq(vcpu->vcpu_id, delivery_mode,\n\t\t\t\t\t   trig_mode, vector);\n\t}\n}\n\n \nvoid vmx_set_constant_host_state(struct vcpu_vmx *vmx)\n{\n\tu32 low32, high32;\n\tunsigned long tmpl;\n\tunsigned long cr0, cr3, cr4;\n\n\tcr0 = read_cr0();\n\tWARN_ON(cr0 & X86_CR0_TS);\n\tvmcs_writel(HOST_CR0, cr0);   \n\n\t \n\tcr3 = __read_cr3();\n\tvmcs_writel(HOST_CR3, cr3);\t\t \n\tvmx->loaded_vmcs->host_state.cr3 = cr3;\n\n\t \n\tcr4 = cr4_read_shadow();\n\tvmcs_writel(HOST_CR4, cr4);\t\t\t \n\tvmx->loaded_vmcs->host_state.cr4 = cr4;\n\n\tvmcs_write16(HOST_CS_SELECTOR, __KERNEL_CS);   \n#ifdef CONFIG_X86_64\n\t \n\tvmcs_write16(HOST_DS_SELECTOR, 0);\n\tvmcs_write16(HOST_ES_SELECTOR, 0);\n#else\n\tvmcs_write16(HOST_DS_SELECTOR, __KERNEL_DS);   \n\tvmcs_write16(HOST_ES_SELECTOR, __KERNEL_DS);   \n#endif\n\tvmcs_write16(HOST_SS_SELECTOR, __KERNEL_DS);   \n\tvmcs_write16(HOST_TR_SELECTOR, GDT_ENTRY_TSS*8);   \n\n\tvmcs_writel(HOST_IDTR_BASE, host_idt_base);    \n\n\tvmcs_writel(HOST_RIP, (unsigned long)vmx_vmexit);  \n\n\trdmsr(MSR_IA32_SYSENTER_CS, low32, high32);\n\tvmcs_write32(HOST_IA32_SYSENTER_CS, low32);\n\n\t \n\tif (!IS_ENABLED(CONFIG_IA32_EMULATION) && !IS_ENABLED(CONFIG_X86_32))\n\t\tvmcs_writel(HOST_IA32_SYSENTER_ESP, 0);\n\n\trdmsrl(MSR_IA32_SYSENTER_EIP, tmpl);\n\tvmcs_writel(HOST_IA32_SYSENTER_EIP, tmpl);    \n\n\tif (vmcs_config.vmexit_ctrl & VM_EXIT_LOAD_IA32_PAT) {\n\t\trdmsr(MSR_IA32_CR_PAT, low32, high32);\n\t\tvmcs_write64(HOST_IA32_PAT, low32 | ((u64) high32 << 32));\n\t}\n\n\tif (cpu_has_load_ia32_efer())\n\t\tvmcs_write64(HOST_IA32_EFER, host_efer);\n}\n\nvoid set_cr4_guest_host_mask(struct vcpu_vmx *vmx)\n{\n\tstruct kvm_vcpu *vcpu = &vmx->vcpu;\n\n\tvcpu->arch.cr4_guest_owned_bits = KVM_POSSIBLE_CR4_GUEST_BITS &\n\t\t\t\t\t  ~vcpu->arch.cr4_guest_rsvd_bits;\n\tif (!enable_ept) {\n\t\tvcpu->arch.cr4_guest_owned_bits &= ~X86_CR4_TLBFLUSH_BITS;\n\t\tvcpu->arch.cr4_guest_owned_bits &= ~X86_CR4_PDPTR_BITS;\n\t}\n\tif (is_guest_mode(&vmx->vcpu))\n\t\tvcpu->arch.cr4_guest_owned_bits &=\n\t\t\t~get_vmcs12(vcpu)->cr4_guest_host_mask;\n\tvmcs_writel(CR4_GUEST_HOST_MASK, ~vcpu->arch.cr4_guest_owned_bits);\n}\n\nstatic u32 vmx_pin_based_exec_ctrl(struct vcpu_vmx *vmx)\n{\n\tu32 pin_based_exec_ctrl = vmcs_config.pin_based_exec_ctrl;\n\n\tif (!kvm_vcpu_apicv_active(&vmx->vcpu))\n\t\tpin_based_exec_ctrl &= ~PIN_BASED_POSTED_INTR;\n\n\tif (!enable_vnmi)\n\t\tpin_based_exec_ctrl &= ~PIN_BASED_VIRTUAL_NMIS;\n\n\tif (!enable_preemption_timer)\n\t\tpin_based_exec_ctrl &= ~PIN_BASED_VMX_PREEMPTION_TIMER;\n\n\treturn pin_based_exec_ctrl;\n}\n\nstatic u32 vmx_vmentry_ctrl(void)\n{\n\tu32 vmentry_ctrl = vmcs_config.vmentry_ctrl;\n\n\tif (vmx_pt_mode_is_system())\n\t\tvmentry_ctrl &= ~(VM_ENTRY_PT_CONCEAL_PIP |\n\t\t\t\t  VM_ENTRY_LOAD_IA32_RTIT_CTL);\n\t \n\tvmentry_ctrl &= ~(VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL |\n\t\t\t  VM_ENTRY_LOAD_IA32_EFER |\n\t\t\t  VM_ENTRY_IA32E_MODE);\n\n\tif (cpu_has_perf_global_ctrl_bug())\n\t\tvmentry_ctrl &= ~VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL;\n\n\treturn vmentry_ctrl;\n}\n\nstatic u32 vmx_vmexit_ctrl(void)\n{\n\tu32 vmexit_ctrl = vmcs_config.vmexit_ctrl;\n\n\t \n\tvmexit_ctrl &= ~(VM_EXIT_SAVE_IA32_PAT | VM_EXIT_SAVE_IA32_EFER |\n\t\t\t VM_EXIT_SAVE_VMX_PREEMPTION_TIMER);\n\n\tif (vmx_pt_mode_is_system())\n\t\tvmexit_ctrl &= ~(VM_EXIT_PT_CONCEAL_PIP |\n\t\t\t\t VM_EXIT_CLEAR_IA32_RTIT_CTL);\n\n\tif (cpu_has_perf_global_ctrl_bug())\n\t\tvmexit_ctrl &= ~VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL;\n\n\t \n\treturn vmexit_ctrl &\n\t\t~(VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL | VM_EXIT_LOAD_IA32_EFER);\n}\n\nstatic void vmx_refresh_apicv_exec_ctrl(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (is_guest_mode(vcpu)) {\n\t\tvmx->nested.update_vmcs01_apicv_status = true;\n\t\treturn;\n\t}\n\n\tpin_controls_set(vmx, vmx_pin_based_exec_ctrl(vmx));\n\n\tif (kvm_vcpu_apicv_active(vcpu)) {\n\t\tsecondary_exec_controls_setbit(vmx,\n\t\t\t\t\t       SECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\t\t       SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n\t\tif (enable_ipiv)\n\t\t\ttertiary_exec_controls_setbit(vmx, TERTIARY_EXEC_IPI_VIRT);\n\t} else {\n\t\tsecondary_exec_controls_clearbit(vmx,\n\t\t\t\t\t\t SECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\t\t\t SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n\t\tif (enable_ipiv)\n\t\t\ttertiary_exec_controls_clearbit(vmx, TERTIARY_EXEC_IPI_VIRT);\n\t}\n\n\tvmx_update_msr_bitmap_x2apic(vcpu);\n}\n\nstatic u32 vmx_exec_control(struct vcpu_vmx *vmx)\n{\n\tu32 exec_control = vmcs_config.cpu_based_exec_ctrl;\n\n\t \n\texec_control &= ~(CPU_BASED_RDTSC_EXITING |\n\t\t\t  CPU_BASED_USE_IO_BITMAPS |\n\t\t\t  CPU_BASED_MONITOR_TRAP_FLAG |\n\t\t\t  CPU_BASED_PAUSE_EXITING);\n\n\t \n\texec_control &= ~(CPU_BASED_INTR_WINDOW_EXITING |\n\t\t\t  CPU_BASED_NMI_WINDOW_EXITING);\n\n\tif (vmx->vcpu.arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT)\n\t\texec_control &= ~CPU_BASED_MOV_DR_EXITING;\n\n\tif (!cpu_need_tpr_shadow(&vmx->vcpu))\n\t\texec_control &= ~CPU_BASED_TPR_SHADOW;\n\n#ifdef CONFIG_X86_64\n\tif (exec_control & CPU_BASED_TPR_SHADOW)\n\t\texec_control &= ~(CPU_BASED_CR8_LOAD_EXITING |\n\t\t\t\t  CPU_BASED_CR8_STORE_EXITING);\n\telse\n\t\texec_control |= CPU_BASED_CR8_STORE_EXITING |\n\t\t\t\tCPU_BASED_CR8_LOAD_EXITING;\n#endif\n\t \n\tif (enable_ept)\n\t\texec_control &= ~(CPU_BASED_CR3_LOAD_EXITING |\n\t\t\t\t  CPU_BASED_CR3_STORE_EXITING |\n\t\t\t\t  CPU_BASED_INVLPG_EXITING);\n\tif (kvm_mwait_in_guest(vmx->vcpu.kvm))\n\t\texec_control &= ~(CPU_BASED_MWAIT_EXITING |\n\t\t\t\tCPU_BASED_MONITOR_EXITING);\n\tif (kvm_hlt_in_guest(vmx->vcpu.kvm))\n\t\texec_control &= ~CPU_BASED_HLT_EXITING;\n\treturn exec_control;\n}\n\nstatic u64 vmx_tertiary_exec_control(struct vcpu_vmx *vmx)\n{\n\tu64 exec_control = vmcs_config.cpu_based_3rd_exec_ctrl;\n\n\t \n\tif (!enable_ipiv || !kvm_vcpu_apicv_active(&vmx->vcpu))\n\t\texec_control &= ~TERTIARY_EXEC_IPI_VIRT;\n\n\treturn exec_control;\n}\n\n \nstatic inline void\nvmx_adjust_secondary_exec_control(struct vcpu_vmx *vmx, u32 *exec_control,\n\t\t\t\t  u32 control, bool enabled, bool exiting)\n{\n\t \n\tif (enabled == exiting)\n\t\t*exec_control &= ~control;\n\n\t \n\tif (nested) {\n\t\t \n\t\tif (WARN_ON_ONCE(!(vmcs_config.nested.secondary_ctls_high & control)))\n\t\t\tenabled = false;\n\n\t\tif (enabled)\n\t\t\tvmx->nested.msrs.secondary_ctls_high |= control;\n\t\telse\n\t\t\tvmx->nested.msrs.secondary_ctls_high &= ~control;\n\t}\n}\n\n \n#define vmx_adjust_sec_exec_control(vmx, exec_control, name, feat_name, ctrl_name, exiting)\t\\\n({\t\t\t\t\t\t\t\t\t\t\t\t\\\n\tstruct kvm_vcpu *__vcpu = &(vmx)->vcpu;\t\t\t\t\t\t\t\\\n\tbool __enabled;\t\t\t\t\t\t\t\t\t\t\\\n\t\t\t\t\t\t\t\t\t\t\t\t\\\n\tif (cpu_has_vmx_##name()) {\t\t\t\t\t\t\t\t\\\n\t\tif (kvm_is_governed_feature(X86_FEATURE_##feat_name))\t\t\t\t\\\n\t\t\t__enabled = guest_can_use(__vcpu, X86_FEATURE_##feat_name);\t\t\\\n\t\telse\t\t\t\t\t\t\t\t\t\t\\\n\t\t\t__enabled = guest_cpuid_has(__vcpu, X86_FEATURE_##feat_name);\t\t\\\n\t\tvmx_adjust_secondary_exec_control(vmx, exec_control, SECONDARY_EXEC_##ctrl_name,\\\n\t\t\t\t\t\t  __enabled, exiting);\t\t\t\t\\\n\t}\t\t\t\t\t\t\t\t\t\t\t\\\n})\n\n \n#define vmx_adjust_sec_exec_feature(vmx, exec_control, lname, uname) \\\n\tvmx_adjust_sec_exec_control(vmx, exec_control, lname, uname, ENABLE_##uname, false)\n\n#define vmx_adjust_sec_exec_exiting(vmx, exec_control, lname, uname) \\\n\tvmx_adjust_sec_exec_control(vmx, exec_control, lname, uname, uname##_EXITING, true)\n\nstatic u32 vmx_secondary_exec_control(struct vcpu_vmx *vmx)\n{\n\tstruct kvm_vcpu *vcpu = &vmx->vcpu;\n\n\tu32 exec_control = vmcs_config.cpu_based_2nd_exec_ctrl;\n\n\tif (vmx_pt_mode_is_system())\n\t\texec_control &= ~(SECONDARY_EXEC_PT_USE_GPA | SECONDARY_EXEC_PT_CONCEAL_VMX);\n\tif (!cpu_need_virtualize_apic_accesses(vcpu))\n\t\texec_control &= ~SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;\n\tif (vmx->vpid == 0)\n\t\texec_control &= ~SECONDARY_EXEC_ENABLE_VPID;\n\tif (!enable_ept) {\n\t\texec_control &= ~SECONDARY_EXEC_ENABLE_EPT;\n\t\tenable_unrestricted_guest = 0;\n\t}\n\tif (!enable_unrestricted_guest)\n\t\texec_control &= ~SECONDARY_EXEC_UNRESTRICTED_GUEST;\n\tif (kvm_pause_in_guest(vmx->vcpu.kvm))\n\t\texec_control &= ~SECONDARY_EXEC_PAUSE_LOOP_EXITING;\n\tif (!kvm_vcpu_apicv_active(vcpu))\n\t\texec_control &= ~(SECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\t  SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);\n\texec_control &= ~SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;\n\n\t \n\texec_control &= ~SECONDARY_EXEC_ENABLE_VMFUNC;\n\n\t \n\texec_control &= ~SECONDARY_EXEC_DESC;\n\n\t \n\texec_control &= ~SECONDARY_EXEC_SHADOW_VMCS;\n\n\t \n\tif (!enable_pml || !atomic_read(&vcpu->kvm->nr_memslots_dirty_logging))\n\t\texec_control &= ~SECONDARY_EXEC_ENABLE_PML;\n\n\tvmx_adjust_sec_exec_feature(vmx, &exec_control, xsaves, XSAVES);\n\n\t \n\tif (cpu_has_vmx_rdtscp()) {\n\t\tbool rdpid_or_rdtscp_enabled =\n\t\t\tguest_cpuid_has(vcpu, X86_FEATURE_RDTSCP) ||\n\t\t\tguest_cpuid_has(vcpu, X86_FEATURE_RDPID);\n\n\t\tvmx_adjust_secondary_exec_control(vmx, &exec_control,\n\t\t\t\t\t\t  SECONDARY_EXEC_ENABLE_RDTSCP,\n\t\t\t\t\t\t  rdpid_or_rdtscp_enabled, false);\n\t}\n\n\tvmx_adjust_sec_exec_feature(vmx, &exec_control, invpcid, INVPCID);\n\n\tvmx_adjust_sec_exec_exiting(vmx, &exec_control, rdrand, RDRAND);\n\tvmx_adjust_sec_exec_exiting(vmx, &exec_control, rdseed, RDSEED);\n\n\tvmx_adjust_sec_exec_control(vmx, &exec_control, waitpkg, WAITPKG,\n\t\t\t\t    ENABLE_USR_WAIT_PAUSE, false);\n\n\tif (!vcpu->kvm->arch.bus_lock_detection_enabled)\n\t\texec_control &= ~SECONDARY_EXEC_BUS_LOCK_DETECTION;\n\n\tif (!kvm_notify_vmexit_enabled(vcpu->kvm))\n\t\texec_control &= ~SECONDARY_EXEC_NOTIFY_VM_EXITING;\n\n\treturn exec_control;\n}\n\nstatic inline int vmx_get_pid_table_order(struct kvm *kvm)\n{\n\treturn get_order(kvm->arch.max_vcpu_ids * sizeof(*to_kvm_vmx(kvm)->pid_table));\n}\n\nstatic int vmx_alloc_ipiv_pid_table(struct kvm *kvm)\n{\n\tstruct page *pages;\n\tstruct kvm_vmx *kvm_vmx = to_kvm_vmx(kvm);\n\n\tif (!irqchip_in_kernel(kvm) || !enable_ipiv)\n\t\treturn 0;\n\n\tif (kvm_vmx->pid_table)\n\t\treturn 0;\n\n\tpages = alloc_pages(GFP_KERNEL_ACCOUNT | __GFP_ZERO,\n\t\t\t    vmx_get_pid_table_order(kvm));\n\tif (!pages)\n\t\treturn -ENOMEM;\n\n\tkvm_vmx->pid_table = (void *)page_address(pages);\n\treturn 0;\n}\n\nstatic int vmx_vcpu_precreate(struct kvm *kvm)\n{\n\treturn vmx_alloc_ipiv_pid_table(kvm);\n}\n\n#define VMX_XSS_EXIT_BITMAP 0\n\nstatic void init_vmcs(struct vcpu_vmx *vmx)\n{\n\tstruct kvm *kvm = vmx->vcpu.kvm;\n\tstruct kvm_vmx *kvm_vmx = to_kvm_vmx(kvm);\n\n\tif (nested)\n\t\tnested_vmx_set_vmcs_shadowing_bitmap();\n\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmcs_write64(MSR_BITMAP, __pa(vmx->vmcs01.msr_bitmap));\n\n\tvmcs_write64(VMCS_LINK_POINTER, INVALID_GPA);  \n\n\t \n\tpin_controls_set(vmx, vmx_pin_based_exec_ctrl(vmx));\n\n\texec_controls_set(vmx, vmx_exec_control(vmx));\n\n\tif (cpu_has_secondary_exec_ctrls())\n\t\tsecondary_exec_controls_set(vmx, vmx_secondary_exec_control(vmx));\n\n\tif (cpu_has_tertiary_exec_ctrls())\n\t\ttertiary_exec_controls_set(vmx, vmx_tertiary_exec_control(vmx));\n\n\tif (enable_apicv && lapic_in_kernel(&vmx->vcpu)) {\n\t\tvmcs_write64(EOI_EXIT_BITMAP0, 0);\n\t\tvmcs_write64(EOI_EXIT_BITMAP1, 0);\n\t\tvmcs_write64(EOI_EXIT_BITMAP2, 0);\n\t\tvmcs_write64(EOI_EXIT_BITMAP3, 0);\n\n\t\tvmcs_write16(GUEST_INTR_STATUS, 0);\n\n\t\tvmcs_write16(POSTED_INTR_NV, POSTED_INTR_VECTOR);\n\t\tvmcs_write64(POSTED_INTR_DESC_ADDR, __pa((&vmx->pi_desc)));\n\t}\n\n\tif (vmx_can_use_ipiv(&vmx->vcpu)) {\n\t\tvmcs_write64(PID_POINTER_TABLE, __pa(kvm_vmx->pid_table));\n\t\tvmcs_write16(LAST_PID_POINTER_INDEX, kvm->arch.max_vcpu_ids - 1);\n\t}\n\n\tif (!kvm_pause_in_guest(kvm)) {\n\t\tvmcs_write32(PLE_GAP, ple_gap);\n\t\tvmx->ple_window = ple_window;\n\t\tvmx->ple_window_dirty = true;\n\t}\n\n\tif (kvm_notify_vmexit_enabled(kvm))\n\t\tvmcs_write32(NOTIFY_WINDOW, kvm->arch.notify_window);\n\n\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, 0);\n\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, 0);\n\tvmcs_write32(CR3_TARGET_COUNT, 0);            \n\n\tvmcs_write16(HOST_FS_SELECTOR, 0);             \n\tvmcs_write16(HOST_GS_SELECTOR, 0);             \n\tvmx_set_constant_host_state(vmx);\n\tvmcs_writel(HOST_FS_BASE, 0);  \n\tvmcs_writel(HOST_GS_BASE, 0);  \n\n\tif (cpu_has_vmx_vmfunc())\n\t\tvmcs_write64(VM_FUNCTION_CONTROL, 0);\n\n\tvmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);\n\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, 0);\n\tvmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host.val));\n\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, 0);\n\tvmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest.val));\n\n\tif (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT)\n\t\tvmcs_write64(GUEST_IA32_PAT, vmx->vcpu.arch.pat);\n\n\tvm_exit_controls_set(vmx, vmx_vmexit_ctrl());\n\n\t \n\tvm_entry_controls_set(vmx, vmx_vmentry_ctrl());\n\n\tvmx->vcpu.arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();\n\tvmcs_writel(CR0_GUEST_HOST_MASK, ~vmx->vcpu.arch.cr0_guest_owned_bits);\n\n\tset_cr4_guest_host_mask(vmx);\n\n\tif (vmx->vpid != 0)\n\t\tvmcs_write16(VIRTUAL_PROCESSOR_ID, vmx->vpid);\n\n\tif (cpu_has_vmx_xsaves())\n\t\tvmcs_write64(XSS_EXIT_BITMAP, VMX_XSS_EXIT_BITMAP);\n\n\tif (enable_pml) {\n\t\tvmcs_write64(PML_ADDRESS, page_to_phys(vmx->pml_pg));\n\t\tvmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);\n\t}\n\n\tvmx_write_encls_bitmap(&vmx->vcpu, NULL);\n\n\tif (vmx_pt_mode_is_host_guest()) {\n\t\tmemset(&vmx->pt_desc, 0, sizeof(vmx->pt_desc));\n\t\t \n\t\tvmx->pt_desc.guest.output_mask = 0x7F;\n\t\tvmcs_write64(GUEST_IA32_RTIT_CTL, 0);\n\t}\n\n\tvmcs_write32(GUEST_SYSENTER_CS, 0);\n\tvmcs_writel(GUEST_SYSENTER_ESP, 0);\n\tvmcs_writel(GUEST_SYSENTER_EIP, 0);\n\tvmcs_write64(GUEST_IA32_DEBUGCTL, 0);\n\n\tif (cpu_has_vmx_tpr_shadow()) {\n\t\tvmcs_write64(VIRTUAL_APIC_PAGE_ADDR, 0);\n\t\tif (cpu_need_tpr_shadow(&vmx->vcpu))\n\t\t\tvmcs_write64(VIRTUAL_APIC_PAGE_ADDR,\n\t\t\t\t     __pa(vmx->vcpu.arch.apic->regs));\n\t\tvmcs_write32(TPR_THRESHOLD, 0);\n\t}\n\n\tvmx_setup_uret_msrs(vmx);\n}\n\nstatic void __vmx_vcpu_reset(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tinit_vmcs(vmx);\n\n\tif (nested)\n\t\tmemcpy(&vmx->nested.msrs, &vmcs_config.nested, sizeof(vmx->nested.msrs));\n\n\tvcpu_setup_sgx_lepubkeyhash(vcpu);\n\n\tvmx->nested.posted_intr_nv = -1;\n\tvmx->nested.vmxon_ptr = INVALID_GPA;\n\tvmx->nested.current_vmptr = INVALID_GPA;\n\tvmx->nested.hv_evmcs_vmptr = EVMPTR_INVALID;\n\n\tvcpu->arch.microcode_version = 0x100000000ULL;\n\tvmx->msr_ia32_feature_control_valid_bits = FEAT_CTL_LOCKED;\n\n\t \n\tvmx->pi_desc.nv = POSTED_INTR_VECTOR;\n\tvmx->pi_desc.sn = 1;\n}\n\nstatic void vmx_vcpu_reset(struct kvm_vcpu *vcpu, bool init_event)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (!init_event)\n\t\t__vmx_vcpu_reset(vcpu);\n\n\tvmx->rmode.vm86_active = 0;\n\tvmx->spec_ctrl = 0;\n\n\tvmx->msr_ia32_umwait_control = 0;\n\n\tvmx->hv_deadline_tsc = -1;\n\tkvm_set_cr8(vcpu, 0);\n\n\tvmx_segment_cache_clear(vmx);\n\tkvm_register_mark_available(vcpu, VCPU_EXREG_SEGMENTS);\n\n\tseg_setup(VCPU_SREG_CS);\n\tvmcs_write16(GUEST_CS_SELECTOR, 0xf000);\n\tvmcs_writel(GUEST_CS_BASE, 0xffff0000ul);\n\n\tseg_setup(VCPU_SREG_DS);\n\tseg_setup(VCPU_SREG_ES);\n\tseg_setup(VCPU_SREG_FS);\n\tseg_setup(VCPU_SREG_GS);\n\tseg_setup(VCPU_SREG_SS);\n\n\tvmcs_write16(GUEST_TR_SELECTOR, 0);\n\tvmcs_writel(GUEST_TR_BASE, 0);\n\tvmcs_write32(GUEST_TR_LIMIT, 0xffff);\n\tvmcs_write32(GUEST_TR_AR_BYTES, 0x008b);\n\n\tvmcs_write16(GUEST_LDTR_SELECTOR, 0);\n\tvmcs_writel(GUEST_LDTR_BASE, 0);\n\tvmcs_write32(GUEST_LDTR_LIMIT, 0xffff);\n\tvmcs_write32(GUEST_LDTR_AR_BYTES, 0x00082);\n\n\tvmcs_writel(GUEST_GDTR_BASE, 0);\n\tvmcs_write32(GUEST_GDTR_LIMIT, 0xffff);\n\n\tvmcs_writel(GUEST_IDTR_BASE, 0);\n\tvmcs_write32(GUEST_IDTR_LIMIT, 0xffff);\n\n\tvmcs_write32(GUEST_ACTIVITY_STATE, GUEST_ACTIVITY_ACTIVE);\n\tvmcs_write32(GUEST_INTERRUPTIBILITY_INFO, 0);\n\tvmcs_writel(GUEST_PENDING_DBG_EXCEPTIONS, 0);\n\tif (kvm_mpx_supported())\n\t\tvmcs_write64(GUEST_BNDCFGS, 0);\n\n\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);   \n\n\tkvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);\n\n\tvpid_sync_context(vmx->vpid);\n\n\tvmx_update_fb_clear_dis(vcpu, vmx);\n}\n\nstatic void vmx_enable_irq_window(struct kvm_vcpu *vcpu)\n{\n\texec_controls_setbit(to_vmx(vcpu), CPU_BASED_INTR_WINDOW_EXITING);\n}\n\nstatic void vmx_enable_nmi_window(struct kvm_vcpu *vcpu)\n{\n\tif (!enable_vnmi ||\n\t    vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_STI) {\n\t\tvmx_enable_irq_window(vcpu);\n\t\treturn;\n\t}\n\n\texec_controls_setbit(to_vmx(vcpu), CPU_BASED_NMI_WINDOW_EXITING);\n}\n\nstatic void vmx_inject_irq(struct kvm_vcpu *vcpu, bool reinjected)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tuint32_t intr;\n\tint irq = vcpu->arch.interrupt.nr;\n\n\ttrace_kvm_inj_virq(irq, vcpu->arch.interrupt.soft, reinjected);\n\n\t++vcpu->stat.irq_injections;\n\tif (vmx->rmode.vm86_active) {\n\t\tint inc_eip = 0;\n\t\tif (vcpu->arch.interrupt.soft)\n\t\t\tinc_eip = vcpu->arch.event_exit_inst_len;\n\t\tkvm_inject_realmode_interrupt(vcpu, irq, inc_eip);\n\t\treturn;\n\t}\n\tintr = irq | INTR_INFO_VALID_MASK;\n\tif (vcpu->arch.interrupt.soft) {\n\t\tintr |= INTR_TYPE_SOFT_INTR;\n\t\tvmcs_write32(VM_ENTRY_INSTRUCTION_LEN,\n\t\t\t     vmx->vcpu.arch.event_exit_inst_len);\n\t} else\n\t\tintr |= INTR_TYPE_EXT_INTR;\n\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr);\n\n\tvmx_clear_hlt(vcpu);\n}\n\nstatic void vmx_inject_nmi(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (!enable_vnmi) {\n\t\t \n\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 1;\n\t\tvmx->loaded_vmcs->vnmi_blocked_time = 0;\n\t}\n\n\t++vcpu->stat.nmi_injections;\n\tvmx->loaded_vmcs->nmi_known_unmasked = false;\n\n\tif (vmx->rmode.vm86_active) {\n\t\tkvm_inject_realmode_interrupt(vcpu, NMI_VECTOR, 0);\n\t\treturn;\n\t}\n\n\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD,\n\t\t\tINTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK | NMI_VECTOR);\n\n\tvmx_clear_hlt(vcpu);\n}\n\nbool vmx_get_nmi_mask(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tbool masked;\n\n\tif (!enable_vnmi)\n\t\treturn vmx->loaded_vmcs->soft_vnmi_blocked;\n\tif (vmx->loaded_vmcs->nmi_known_unmasked)\n\t\treturn false;\n\tmasked = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) & GUEST_INTR_STATE_NMI;\n\tvmx->loaded_vmcs->nmi_known_unmasked = !masked;\n\treturn masked;\n}\n\nvoid vmx_set_nmi_mask(struct kvm_vcpu *vcpu, bool masked)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (!enable_vnmi) {\n\t\tif (vmx->loaded_vmcs->soft_vnmi_blocked != masked) {\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = masked;\n\t\t\tvmx->loaded_vmcs->vnmi_blocked_time = 0;\n\t\t}\n\t} else {\n\t\tvmx->loaded_vmcs->nmi_known_unmasked = !masked;\n\t\tif (masked)\n\t\t\tvmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,\n\t\t\t\t      GUEST_INTR_STATE_NMI);\n\t\telse\n\t\t\tvmcs_clear_bits(GUEST_INTERRUPTIBILITY_INFO,\n\t\t\t\t\tGUEST_INTR_STATE_NMI);\n\t}\n}\n\nbool vmx_nmi_blocked(struct kvm_vcpu *vcpu)\n{\n\tif (is_guest_mode(vcpu) && nested_exit_on_nmi(vcpu))\n\t\treturn false;\n\n\tif (!enable_vnmi && to_vmx(vcpu)->loaded_vmcs->soft_vnmi_blocked)\n\t\treturn true;\n\n\treturn (vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &\n\t\t(GUEST_INTR_STATE_MOV_SS | GUEST_INTR_STATE_STI |\n\t\t GUEST_INTR_STATE_NMI));\n}\n\nstatic int vmx_nmi_allowed(struct kvm_vcpu *vcpu, bool for_injection)\n{\n\tif (to_vmx(vcpu)->nested.nested_run_pending)\n\t\treturn -EBUSY;\n\n\t \n\tif (for_injection && is_guest_mode(vcpu) && nested_exit_on_nmi(vcpu))\n\t\treturn -EBUSY;\n\n\treturn !vmx_nmi_blocked(vcpu);\n}\n\nbool vmx_interrupt_blocked(struct kvm_vcpu *vcpu)\n{\n\tif (is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))\n\t\treturn false;\n\n\treturn !(vmx_get_rflags(vcpu) & X86_EFLAGS_IF) ||\n\t       (vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &\n\t\t(GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));\n}\n\nstatic int vmx_interrupt_allowed(struct kvm_vcpu *vcpu, bool for_injection)\n{\n\tif (to_vmx(vcpu)->nested.nested_run_pending)\n\t\treturn -EBUSY;\n\n\t \n\tif (for_injection && is_guest_mode(vcpu) && nested_exit_on_intr(vcpu))\n\t\treturn -EBUSY;\n\n\treturn !vmx_interrupt_blocked(vcpu);\n}\n\nstatic int vmx_set_tss_addr(struct kvm *kvm, unsigned int addr)\n{\n\tvoid __user *ret;\n\n\tif (enable_unrestricted_guest)\n\t\treturn 0;\n\n\tmutex_lock(&kvm->slots_lock);\n\tret = __x86_set_memory_region(kvm, TSS_PRIVATE_MEMSLOT, addr,\n\t\t\t\t      PAGE_SIZE * 3);\n\tmutex_unlock(&kvm->slots_lock);\n\n\tif (IS_ERR(ret))\n\t\treturn PTR_ERR(ret);\n\n\tto_kvm_vmx(kvm)->tss_addr = addr;\n\n\treturn init_rmode_tss(kvm, ret);\n}\n\nstatic int vmx_set_identity_map_addr(struct kvm *kvm, u64 ident_addr)\n{\n\tto_kvm_vmx(kvm)->ept_identity_map_addr = ident_addr;\n\treturn 0;\n}\n\nstatic bool rmode_exception(struct kvm_vcpu *vcpu, int vec)\n{\n\tswitch (vec) {\n\tcase BP_VECTOR:\n\t\t \n\t\tto_vmx(vcpu)->vcpu.arch.event_exit_inst_len =\n\t\t\tvmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\t\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)\n\t\t\treturn false;\n\t\tfallthrough;\n\tcase DB_VECTOR:\n\t\treturn !(vcpu->guest_debug &\n\t\t\t(KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP));\n\tcase DE_VECTOR:\n\tcase OF_VECTOR:\n\tcase BR_VECTOR:\n\tcase UD_VECTOR:\n\tcase DF_VECTOR:\n\tcase SS_VECTOR:\n\tcase GP_VECTOR:\n\tcase MF_VECTOR:\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic int handle_rmode_exception(struct kvm_vcpu *vcpu,\n\t\t\t\t  int vec, u32 err_code)\n{\n\t \n\tif (((vec == GP_VECTOR) || (vec == SS_VECTOR)) && err_code == 0) {\n\t\tif (kvm_emulate_instruction(vcpu, 0)) {\n\t\t\tif (vcpu->arch.halt_request) {\n\t\t\t\tvcpu->arch.halt_request = 0;\n\t\t\t\treturn kvm_emulate_halt_noskip(vcpu);\n\t\t\t}\n\t\t\treturn 1;\n\t\t}\n\t\treturn 0;\n\t}\n\n\t \n\tkvm_queue_exception(vcpu, vec);\n\treturn 1;\n}\n\nstatic int handle_machine_check(struct kvm_vcpu *vcpu)\n{\n\t \n\treturn 1;\n}\n\n \nbool vmx_guest_inject_ac(struct kvm_vcpu *vcpu)\n{\n\tif (!boot_cpu_has(X86_FEATURE_SPLIT_LOCK_DETECT))\n\t\treturn true;\n\n\treturn vmx_get_cpl(vcpu) == 3 && kvm_is_cr0_bit_set(vcpu, X86_CR0_AM) &&\n\t       (kvm_get_rflags(vcpu) & X86_EFLAGS_AC);\n}\n\nstatic int handle_exception_nmi(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct kvm_run *kvm_run = vcpu->run;\n\tu32 intr_info, ex_no, error_code;\n\tunsigned long cr2, dr6;\n\tu32 vect_info;\n\n\tvect_info = vmx->idt_vectoring_info;\n\tintr_info = vmx_get_intr_info(vcpu);\n\n\t \n\tif (is_machine_check(intr_info) || is_nmi(intr_info))\n\t\treturn 1;\n\n\t \n\tif (is_nm_fault(intr_info)) {\n\t\tkvm_queue_exception(vcpu, NM_VECTOR);\n\t\treturn 1;\n\t}\n\n\tif (is_invalid_opcode(intr_info))\n\t\treturn handle_ud(vcpu);\n\n\terror_code = 0;\n\tif (intr_info & INTR_INFO_DELIVER_CODE_MASK)\n\t\terror_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);\n\n\tif (!vmx->rmode.vm86_active && is_gp_fault(intr_info)) {\n\t\tWARN_ON_ONCE(!enable_vmware_backdoor);\n\n\t\t \n\t\tif (error_code) {\n\t\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, error_code);\n\t\t\treturn 1;\n\t\t}\n\t\treturn kvm_emulate_instruction(vcpu, EMULTYPE_VMWARE_GP);\n\t}\n\n\t \n\tif ((vect_info & VECTORING_INFO_VALID_MASK) &&\n\t    !(is_page_fault(intr_info) && !(error_code & PFERR_RSVD_MASK))) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_SIMUL_EX;\n\t\tvcpu->run->internal.ndata = 4;\n\t\tvcpu->run->internal.data[0] = vect_info;\n\t\tvcpu->run->internal.data[1] = intr_info;\n\t\tvcpu->run->internal.data[2] = error_code;\n\t\tvcpu->run->internal.data[3] = vcpu->arch.last_vmentry_cpu;\n\t\treturn 0;\n\t}\n\n\tif (is_page_fault(intr_info)) {\n\t\tcr2 = vmx_get_exit_qual(vcpu);\n\t\tif (enable_ept && !vcpu->arch.apf.host_apf_flags) {\n\t\t\t \n\t\t\tWARN_ON_ONCE(!allow_smaller_maxphyaddr);\n\t\t\tkvm_fixup_and_inject_pf_error(vcpu, cr2, error_code);\n\t\t\treturn 1;\n\t\t} else\n\t\t\treturn kvm_handle_page_fault(vcpu, error_code, cr2, NULL, 0);\n\t}\n\n\tex_no = intr_info & INTR_INFO_VECTOR_MASK;\n\n\tif (vmx->rmode.vm86_active && rmode_exception(vcpu, ex_no))\n\t\treturn handle_rmode_exception(vcpu, ex_no, error_code);\n\n\tswitch (ex_no) {\n\tcase DB_VECTOR:\n\t\tdr6 = vmx_get_exit_qual(vcpu);\n\t\tif (!(vcpu->guest_debug &\n\t\t      (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))) {\n\t\t\t \n\t\t\tif (is_icebp(intr_info))\n\t\t\t\tWARN_ON(!skip_emulated_instruction(vcpu));\n\t\t\telse if ((vmx_get_rflags(vcpu) & X86_EFLAGS_TF) &&\n\t\t\t\t (vmcs_read32(GUEST_INTERRUPTIBILITY_INFO) &\n\t\t\t\t  (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)))\n\t\t\t\tvmcs_writel(GUEST_PENDING_DBG_EXCEPTIONS,\n\t\t\t\t\t    vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS) | DR6_BS);\n\n\t\t\tkvm_queue_exception_p(vcpu, DB_VECTOR, dr6);\n\t\t\treturn 1;\n\t\t}\n\t\tkvm_run->debug.arch.dr6 = dr6 | DR6_ACTIVE_LOW;\n\t\tkvm_run->debug.arch.dr7 = vmcs_readl(GUEST_DR7);\n\t\tfallthrough;\n\tcase BP_VECTOR:\n\t\t \n\t\tvmx->vcpu.arch.event_exit_inst_len =\n\t\t\tvmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\t\tkvm_run->exit_reason = KVM_EXIT_DEBUG;\n\t\tkvm_run->debug.arch.pc = kvm_get_linear_rip(vcpu);\n\t\tkvm_run->debug.arch.exception = ex_no;\n\t\tbreak;\n\tcase AC_VECTOR:\n\t\tif (vmx_guest_inject_ac(vcpu)) {\n\t\t\tkvm_queue_exception_e(vcpu, AC_VECTOR, error_code);\n\t\t\treturn 1;\n\t\t}\n\n\t\t \n\t\tif (handle_guest_split_lock(kvm_rip_read(vcpu)))\n\t\t\treturn 1;\n\t\tfallthrough;\n\tdefault:\n\t\tkvm_run->exit_reason = KVM_EXIT_EXCEPTION;\n\t\tkvm_run->ex.exception = ex_no;\n\t\tkvm_run->ex.error_code = error_code;\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic __always_inline int handle_external_interrupt(struct kvm_vcpu *vcpu)\n{\n\t++vcpu->stat.irq_exits;\n\treturn 1;\n}\n\nstatic int handle_triple_fault(struct kvm_vcpu *vcpu)\n{\n\tvcpu->run->exit_reason = KVM_EXIT_SHUTDOWN;\n\tvcpu->mmio_needed = 0;\n\treturn 0;\n}\n\nstatic int handle_io(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification;\n\tint size, in, string;\n\tunsigned port;\n\n\texit_qualification = vmx_get_exit_qual(vcpu);\n\tstring = (exit_qualification & 16) != 0;\n\n\t++vcpu->stat.io_exits;\n\n\tif (string)\n\t\treturn kvm_emulate_instruction(vcpu, 0);\n\n\tport = exit_qualification >> 16;\n\tsize = (exit_qualification & 7) + 1;\n\tin = (exit_qualification & 8) != 0;\n\n\treturn kvm_fast_pio(vcpu, size, port, in);\n}\n\nstatic void\nvmx_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)\n{\n\t \n\thypercall[0] = 0x0f;\n\thypercall[1] = 0x01;\n\thypercall[2] = 0xc1;\n}\n\n \nstatic int handle_set_cr0(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tif (is_guest_mode(vcpu)) {\n\t\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\t\tunsigned long orig_val = val;\n\n\t\t \n\t\tval = (val & ~vmcs12->cr0_guest_host_mask) |\n\t\t\t(vmcs12->guest_cr0 & vmcs12->cr0_guest_host_mask);\n\n\t\tif (kvm_set_cr0(vcpu, val))\n\t\t\treturn 1;\n\t\tvmcs_writel(CR0_READ_SHADOW, orig_val);\n\t\treturn 0;\n\t} else {\n\t\treturn kvm_set_cr0(vcpu, val);\n\t}\n}\n\nstatic int handle_set_cr4(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tif (is_guest_mode(vcpu)) {\n\t\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\t\tunsigned long orig_val = val;\n\n\t\t \n\t\tval = (val & ~vmcs12->cr4_guest_host_mask) |\n\t\t\t(vmcs12->guest_cr4 & vmcs12->cr4_guest_host_mask);\n\t\tif (kvm_set_cr4(vcpu, val))\n\t\t\treturn 1;\n\t\tvmcs_writel(CR4_READ_SHADOW, orig_val);\n\t\treturn 0;\n\t} else\n\t\treturn kvm_set_cr4(vcpu, val);\n}\n\nstatic int handle_desc(struct kvm_vcpu *vcpu)\n{\n\t \n\tBUILD_BUG_ON(KVM_POSSIBLE_CR4_GUEST_BITS & X86_CR4_UMIP);\n\n\tWARN_ON_ONCE(!kvm_is_cr4_bit_set(vcpu, X86_CR4_UMIP));\n\treturn kvm_emulate_instruction(vcpu, 0);\n}\n\nstatic int handle_cr(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification, val;\n\tint cr;\n\tint reg;\n\tint err;\n\tint ret;\n\n\texit_qualification = vmx_get_exit_qual(vcpu);\n\tcr = exit_qualification & 15;\n\treg = (exit_qualification >> 8) & 15;\n\tswitch ((exit_qualification >> 4) & 3) {\n\tcase 0:  \n\t\tval = kvm_register_read(vcpu, reg);\n\t\ttrace_kvm_cr_write(cr, val);\n\t\tswitch (cr) {\n\t\tcase 0:\n\t\t\terr = handle_set_cr0(vcpu, val);\n\t\t\treturn kvm_complete_insn_gp(vcpu, err);\n\t\tcase 3:\n\t\t\tWARN_ON_ONCE(enable_unrestricted_guest);\n\n\t\t\terr = kvm_set_cr3(vcpu, val);\n\t\t\treturn kvm_complete_insn_gp(vcpu, err);\n\t\tcase 4:\n\t\t\terr = handle_set_cr4(vcpu, val);\n\t\t\treturn kvm_complete_insn_gp(vcpu, err);\n\t\tcase 8: {\n\t\t\t\tu8 cr8_prev = kvm_get_cr8(vcpu);\n\t\t\t\tu8 cr8 = (u8)val;\n\t\t\t\terr = kvm_set_cr8(vcpu, cr8);\n\t\t\t\tret = kvm_complete_insn_gp(vcpu, err);\n\t\t\t\tif (lapic_in_kernel(vcpu))\n\t\t\t\t\treturn ret;\n\t\t\t\tif (cr8_prev <= cr8)\n\t\t\t\t\treturn ret;\n\t\t\t\t \n\t\t\t\tvcpu->run->exit_reason = KVM_EXIT_SET_TPR;\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\tcase 2:  \n\t\tKVM_BUG(1, vcpu->kvm, \"Guest always owns CR0.TS\");\n\t\treturn -EIO;\n\tcase 1:  \n\t\tswitch (cr) {\n\t\tcase 3:\n\t\t\tWARN_ON_ONCE(enable_unrestricted_guest);\n\n\t\t\tval = kvm_read_cr3(vcpu);\n\t\t\tkvm_register_write(vcpu, reg, val);\n\t\t\ttrace_kvm_cr_read(cr, val);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\tcase 8:\n\t\t\tval = kvm_get_cr8(vcpu);\n\t\t\tkvm_register_write(vcpu, reg, val);\n\t\t\ttrace_kvm_cr_read(cr, val);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\t}\n\t\tbreak;\n\tcase 3:  \n\t\tval = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;\n\t\ttrace_kvm_cr_write(0, (kvm_read_cr0_bits(vcpu, ~0xful) | val));\n\t\tkvm_lmsw(vcpu, val);\n\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\tdefault:\n\t\tbreak;\n\t}\n\tvcpu->run->exit_reason = 0;\n\tvcpu_unimpl(vcpu, \"unhandled control register: op %d cr %d\\n\",\n\t       (int)(exit_qualification >> 4) & 3, cr);\n\treturn 0;\n}\n\nstatic int handle_dr(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification;\n\tint dr, dr7, reg;\n\tint err = 1;\n\n\texit_qualification = vmx_get_exit_qual(vcpu);\n\tdr = exit_qualification & DEBUG_REG_ACCESS_NUM;\n\n\t \n\tif (!kvm_require_dr(vcpu, dr))\n\t\treturn 1;\n\n\tif (vmx_get_cpl(vcpu) > 0)\n\t\tgoto out;\n\n\tdr7 = vmcs_readl(GUEST_DR7);\n\tif (dr7 & DR7_GD) {\n\t\t \n\t\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP) {\n\t\t\tvcpu->run->debug.arch.dr6 = DR6_BD | DR6_ACTIVE_LOW;\n\t\t\tvcpu->run->debug.arch.dr7 = dr7;\n\t\t\tvcpu->run->debug.arch.pc = kvm_get_linear_rip(vcpu);\n\t\t\tvcpu->run->debug.arch.exception = DB_VECTOR;\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_DEBUG;\n\t\t\treturn 0;\n\t\t} else {\n\t\t\tkvm_queue_exception_p(vcpu, DB_VECTOR, DR6_BD);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\tif (vcpu->guest_debug == 0) {\n\t\texec_controls_clearbit(to_vmx(vcpu), CPU_BASED_MOV_DR_EXITING);\n\n\t\t \n\t\tvcpu->arch.switch_db_regs |= KVM_DEBUGREG_WONT_EXIT;\n\t\treturn 1;\n\t}\n\n\treg = DEBUG_REG_ACCESS_REG(exit_qualification);\n\tif (exit_qualification & TYPE_MOV_FROM_DR) {\n\t\tunsigned long val;\n\n\t\tkvm_get_dr(vcpu, dr, &val);\n\t\tkvm_register_write(vcpu, reg, val);\n\t\terr = 0;\n\t} else {\n\t\terr = kvm_set_dr(vcpu, dr, kvm_register_read(vcpu, reg));\n\t}\n\nout:\n\treturn kvm_complete_insn_gp(vcpu, err);\n}\n\nstatic void vmx_sync_dirty_debug_regs(struct kvm_vcpu *vcpu)\n{\n\tget_debugreg(vcpu->arch.db[0], 0);\n\tget_debugreg(vcpu->arch.db[1], 1);\n\tget_debugreg(vcpu->arch.db[2], 2);\n\tget_debugreg(vcpu->arch.db[3], 3);\n\tget_debugreg(vcpu->arch.dr6, 6);\n\tvcpu->arch.dr7 = vmcs_readl(GUEST_DR7);\n\n\tvcpu->arch.switch_db_regs &= ~KVM_DEBUGREG_WONT_EXIT;\n\texec_controls_setbit(to_vmx(vcpu), CPU_BASED_MOV_DR_EXITING);\n\n\t \n\tset_debugreg(DR6_RESERVED, 6);\n}\n\nstatic void vmx_set_dr7(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tvmcs_writel(GUEST_DR7, val);\n}\n\nstatic int handle_tpr_below_threshold(struct kvm_vcpu *vcpu)\n{\n\tkvm_apic_update_ppr(vcpu);\n\treturn 1;\n}\n\nstatic int handle_interrupt_window(struct kvm_vcpu *vcpu)\n{\n\texec_controls_clearbit(to_vmx(vcpu), CPU_BASED_INTR_WINDOW_EXITING);\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\t++vcpu->stat.irq_window_exits;\n\treturn 1;\n}\n\nstatic int handle_invlpg(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification = vmx_get_exit_qual(vcpu);\n\n\tkvm_mmu_invlpg(vcpu, exit_qualification);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int handle_apic_access(struct kvm_vcpu *vcpu)\n{\n\tif (likely(fasteoi)) {\n\t\tunsigned long exit_qualification = vmx_get_exit_qual(vcpu);\n\t\tint access_type, offset;\n\n\t\taccess_type = exit_qualification & APIC_ACCESS_TYPE;\n\t\toffset = exit_qualification & APIC_ACCESS_OFFSET;\n\t\t \n\t\tif ((access_type == TYPE_LINEAR_APIC_INST_WRITE) &&\n\t\t    (offset == APIC_EOI)) {\n\t\t\tkvm_lapic_set_eoi(vcpu);\n\t\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t\t}\n\t}\n\treturn kvm_emulate_instruction(vcpu, 0);\n}\n\nstatic int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification = vmx_get_exit_qual(vcpu);\n\tint vector = exit_qualification & 0xff;\n\n\t \n\tkvm_apic_set_eoi_accelerated(vcpu, vector);\n\treturn 1;\n}\n\nstatic int handle_apic_write(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification = vmx_get_exit_qual(vcpu);\n\n\t \n\tu32 offset = exit_qualification & 0xff0;\n\n\tkvm_apic_write_nodecode(vcpu, offset);\n\treturn 1;\n}\n\nstatic int handle_task_switch(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long exit_qualification;\n\tbool has_error_code = false;\n\tu32 error_code = 0;\n\tu16 tss_selector;\n\tint reason, type, idt_v, idt_index;\n\n\tidt_v = (vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK);\n\tidt_index = (vmx->idt_vectoring_info & VECTORING_INFO_VECTOR_MASK);\n\ttype = (vmx->idt_vectoring_info & VECTORING_INFO_TYPE_MASK);\n\n\texit_qualification = vmx_get_exit_qual(vcpu);\n\n\treason = (u32)exit_qualification >> 30;\n\tif (reason == TASK_SWITCH_GATE && idt_v) {\n\t\tswitch (type) {\n\t\tcase INTR_TYPE_NMI_INTR:\n\t\t\tvcpu->arch.nmi_injected = false;\n\t\t\tvmx_set_nmi_mask(vcpu, true);\n\t\t\tbreak;\n\t\tcase INTR_TYPE_EXT_INTR:\n\t\tcase INTR_TYPE_SOFT_INTR:\n\t\t\tkvm_clear_interrupt_queue(vcpu);\n\t\t\tbreak;\n\t\tcase INTR_TYPE_HARD_EXCEPTION:\n\t\t\tif (vmx->idt_vectoring_info &\n\t\t\t    VECTORING_INFO_DELIVER_CODE_MASK) {\n\t\t\t\thas_error_code = true;\n\t\t\t\terror_code =\n\t\t\t\t\tvmcs_read32(IDT_VECTORING_ERROR_CODE);\n\t\t\t}\n\t\t\tfallthrough;\n\t\tcase INTR_TYPE_SOFT_EXCEPTION:\n\t\t\tkvm_clear_exception_queue(vcpu);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\ttss_selector = exit_qualification;\n\n\tif (!idt_v || (type != INTR_TYPE_HARD_EXCEPTION &&\n\t\t       type != INTR_TYPE_EXT_INTR &&\n\t\t       type != INTR_TYPE_NMI_INTR))\n\t\tWARN_ON(!skip_emulated_instruction(vcpu));\n\n\t \n\treturn kvm_task_switch(vcpu, tss_selector,\n\t\t\t       type == INTR_TYPE_SOFT_INTR ? idt_index : -1,\n\t\t\t       reason, has_error_code, error_code);\n}\n\nstatic int handle_ept_violation(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification;\n\tgpa_t gpa;\n\tu64 error_code;\n\n\texit_qualification = vmx_get_exit_qual(vcpu);\n\n\t \n\tif (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&\n\t\t\tenable_vnmi &&\n\t\t\t(exit_qualification & INTR_INFO_UNBLOCK_NMI))\n\t\tvmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO, GUEST_INTR_STATE_NMI);\n\n\tgpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);\n\ttrace_kvm_page_fault(vcpu, gpa, exit_qualification);\n\n\t \n\terror_code = (exit_qualification & EPT_VIOLATION_ACC_READ)\n\t\t     ? PFERR_USER_MASK : 0;\n\t \n\terror_code |= (exit_qualification & EPT_VIOLATION_ACC_WRITE)\n\t\t      ? PFERR_WRITE_MASK : 0;\n\t \n\terror_code |= (exit_qualification & EPT_VIOLATION_ACC_INSTR)\n\t\t      ? PFERR_FETCH_MASK : 0;\n\t \n\terror_code |= (exit_qualification & EPT_VIOLATION_RWX_MASK)\n\t\t      ? PFERR_PRESENT_MASK : 0;\n\n\terror_code |= (exit_qualification & EPT_VIOLATION_GVA_TRANSLATED) != 0 ?\n\t       PFERR_GUEST_FINAL_MASK : PFERR_GUEST_PAGE_MASK;\n\n\tvcpu->arch.exit_qualification = exit_qualification;\n\n\t \n\tif (unlikely(allow_smaller_maxphyaddr && kvm_vcpu_is_illegal_gpa(vcpu, gpa)))\n\t\treturn kvm_emulate_instruction(vcpu, 0);\n\n\treturn kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);\n}\n\nstatic int handle_ept_misconfig(struct kvm_vcpu *vcpu)\n{\n\tgpa_t gpa;\n\n\tif (!vmx_can_emulate_instruction(vcpu, EMULTYPE_PF, NULL, 0))\n\t\treturn 1;\n\n\t \n\tgpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);\n\tif (!is_guest_mode(vcpu) &&\n\t    !kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {\n\t\ttrace_kvm_fast_mmio(gpa);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\treturn kvm_mmu_page_fault(vcpu, gpa, PFERR_RSVD_MASK, NULL, 0);\n}\n\nstatic int handle_nmi_window(struct kvm_vcpu *vcpu)\n{\n\tif (KVM_BUG_ON(!enable_vnmi, vcpu->kvm))\n\t\treturn -EIO;\n\n\texec_controls_clearbit(to_vmx(vcpu), CPU_BASED_NMI_WINDOW_EXITING);\n\t++vcpu->stat.nmi_window_exits;\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\treturn 1;\n}\n\nstatic bool vmx_emulation_required_with_pending_exception(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\treturn vmx->emulation_required && !vmx->rmode.vm86_active &&\n\t       (kvm_is_exception_pending(vcpu) || vcpu->arch.exception.injected);\n}\n\nstatic int handle_invalid_guest_state(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tbool intr_window_requested;\n\tunsigned count = 130;\n\n\tintr_window_requested = exec_controls_get(vmx) &\n\t\t\t\tCPU_BASED_INTR_WINDOW_EXITING;\n\n\twhile (vmx->emulation_required && count-- != 0) {\n\t\tif (intr_window_requested && !vmx_interrupt_blocked(vcpu))\n\t\t\treturn handle_interrupt_window(&vmx->vcpu);\n\n\t\tif (kvm_test_request(KVM_REQ_EVENT, vcpu))\n\t\t\treturn 1;\n\n\t\tif (!kvm_emulate_instruction(vcpu, 0))\n\t\t\treturn 0;\n\n\t\tif (vmx_emulation_required_with_pending_exception(vcpu)) {\n\t\t\tkvm_prepare_emulation_failure_exit(vcpu);\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (vcpu->arch.halt_request) {\n\t\t\tvcpu->arch.halt_request = 0;\n\t\t\treturn kvm_emulate_halt_noskip(vcpu);\n\t\t}\n\n\t\t \n\t\tif (__xfer_to_guest_mode_work_pending())\n\t\t\treturn 1;\n\t}\n\n\treturn 1;\n}\n\nstatic int vmx_vcpu_pre_run(struct kvm_vcpu *vcpu)\n{\n\tif (vmx_emulation_required_with_pending_exception(vcpu)) {\n\t\tkvm_prepare_emulation_failure_exit(vcpu);\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic void grow_ple_window(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned int old = vmx->ple_window;\n\n\tvmx->ple_window = __grow_ple_window(old, ple_window,\n\t\t\t\t\t    ple_window_grow,\n\t\t\t\t\t    ple_window_max);\n\n\tif (vmx->ple_window != old) {\n\t\tvmx->ple_window_dirty = true;\n\t\ttrace_kvm_ple_window_update(vcpu->vcpu_id,\n\t\t\t\t\t    vmx->ple_window, old);\n\t}\n}\n\nstatic void shrink_ple_window(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned int old = vmx->ple_window;\n\n\tvmx->ple_window = __shrink_ple_window(old, ple_window,\n\t\t\t\t\t      ple_window_shrink,\n\t\t\t\t\t      ple_window);\n\n\tif (vmx->ple_window != old) {\n\t\tvmx->ple_window_dirty = true;\n\t\ttrace_kvm_ple_window_update(vcpu->vcpu_id,\n\t\t\t\t\t    vmx->ple_window, old);\n\t}\n}\n\n \nstatic int handle_pause(struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_pause_in_guest(vcpu->kvm))\n\t\tgrow_ple_window(vcpu);\n\n\t \n\tkvm_vcpu_on_spin(vcpu, true);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int handle_monitor_trap(struct kvm_vcpu *vcpu)\n{\n\treturn 1;\n}\n\nstatic int handle_invpcid(struct kvm_vcpu *vcpu)\n{\n\tu32 vmx_instruction_info;\n\tunsigned long type;\n\tgva_t gva;\n\tstruct {\n\t\tu64 pcid;\n\t\tu64 gla;\n\t} operand;\n\tint gpr_index;\n\n\tif (!guest_cpuid_has(vcpu, X86_FEATURE_INVPCID)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tvmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\tgpr_index = vmx_get_instr_info_reg2(vmx_instruction_info);\n\ttype = kvm_register_read(vcpu, gpr_index);\n\n\t \n\tif (get_vmx_mem_address(vcpu, vmx_get_exit_qual(vcpu),\n\t\t\t\tvmx_instruction_info, false,\n\t\t\t\tsizeof(operand), &gva))\n\t\treturn 1;\n\n\treturn kvm_handle_invpcid(vcpu, type, gva);\n}\n\nstatic int handle_pml_full(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qualification;\n\n\ttrace_kvm_pml_full(vcpu->vcpu_id);\n\n\texit_qualification = vmx_get_exit_qual(vcpu);\n\n\t \n\tif (!(to_vmx(vcpu)->idt_vectoring_info & VECTORING_INFO_VALID_MASK) &&\n\t\t\tenable_vnmi &&\n\t\t\t(exit_qualification & INTR_INFO_UNBLOCK_NMI))\n\t\tvmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,\n\t\t\t\tGUEST_INTR_STATE_NMI);\n\n\t \n\treturn 1;\n}\n\nstatic fastpath_t handle_fastpath_preemption_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (!vmx->req_immediate_exit &&\n\t    !unlikely(vmx->loaded_vmcs->hv_timer_soft_disabled)) {\n\t\tkvm_lapic_expired_hv_timer(vcpu);\n\t\treturn EXIT_FASTPATH_REENTER_GUEST;\n\t}\n\n\treturn EXIT_FASTPATH_NONE;\n}\n\nstatic int handle_preemption_timer(struct kvm_vcpu *vcpu)\n{\n\thandle_fastpath_preemption_timer(vcpu);\n\treturn 1;\n}\n\n \nstatic int handle_vmx_instruction(struct kvm_vcpu *vcpu)\n{\n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\treturn 1;\n}\n\n#ifndef CONFIG_X86_SGX_KVM\nstatic int handle_encls(struct kvm_vcpu *vcpu)\n{\n\t \n\tkvm_queue_exception(vcpu, UD_VECTOR);\n\treturn 1;\n}\n#endif  \n\nstatic int handle_bus_lock_vmexit(struct kvm_vcpu *vcpu)\n{\n\t \n\tto_vmx(vcpu)->exit_reason.bus_lock_detected = true;\n\treturn 1;\n}\n\nstatic int handle_notify(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qual = vmx_get_exit_qual(vcpu);\n\tbool context_invalid = exit_qual & NOTIFY_VM_CONTEXT_INVALID;\n\n\t++vcpu->stat.notify_window_exits;\n\n\t \n\tif (enable_vnmi && (exit_qual & INTR_INFO_UNBLOCK_NMI))\n\t\tvmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,\n\t\t\t      GUEST_INTR_STATE_NMI);\n\n\tif (vcpu->kvm->arch.notify_vmexit_flags & KVM_X86_NOTIFY_VMEXIT_USER ||\n\t    context_invalid) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_NOTIFY;\n\t\tvcpu->run->notify.flags = context_invalid ?\n\t\t\t\t\t  KVM_NOTIFY_CONTEXT_INVALID : 0;\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\n \nstatic int (*kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {\n\t[EXIT_REASON_EXCEPTION_NMI]           = handle_exception_nmi,\n\t[EXIT_REASON_EXTERNAL_INTERRUPT]      = handle_external_interrupt,\n\t[EXIT_REASON_TRIPLE_FAULT]            = handle_triple_fault,\n\t[EXIT_REASON_NMI_WINDOW]\t      = handle_nmi_window,\n\t[EXIT_REASON_IO_INSTRUCTION]          = handle_io,\n\t[EXIT_REASON_CR_ACCESS]               = handle_cr,\n\t[EXIT_REASON_DR_ACCESS]               = handle_dr,\n\t[EXIT_REASON_CPUID]                   = kvm_emulate_cpuid,\n\t[EXIT_REASON_MSR_READ]                = kvm_emulate_rdmsr,\n\t[EXIT_REASON_MSR_WRITE]               = kvm_emulate_wrmsr,\n\t[EXIT_REASON_INTERRUPT_WINDOW]        = handle_interrupt_window,\n\t[EXIT_REASON_HLT]                     = kvm_emulate_halt,\n\t[EXIT_REASON_INVD]\t\t      = kvm_emulate_invd,\n\t[EXIT_REASON_INVLPG]\t\t      = handle_invlpg,\n\t[EXIT_REASON_RDPMC]                   = kvm_emulate_rdpmc,\n\t[EXIT_REASON_VMCALL]                  = kvm_emulate_hypercall,\n\t[EXIT_REASON_VMCLEAR]\t\t      = handle_vmx_instruction,\n\t[EXIT_REASON_VMLAUNCH]\t\t      = handle_vmx_instruction,\n\t[EXIT_REASON_VMPTRLD]\t\t      = handle_vmx_instruction,\n\t[EXIT_REASON_VMPTRST]\t\t      = handle_vmx_instruction,\n\t[EXIT_REASON_VMREAD]\t\t      = handle_vmx_instruction,\n\t[EXIT_REASON_VMRESUME]\t\t      = handle_vmx_instruction,\n\t[EXIT_REASON_VMWRITE]\t\t      = handle_vmx_instruction,\n\t[EXIT_REASON_VMOFF]\t\t      = handle_vmx_instruction,\n\t[EXIT_REASON_VMON]\t\t      = handle_vmx_instruction,\n\t[EXIT_REASON_TPR_BELOW_THRESHOLD]     = handle_tpr_below_threshold,\n\t[EXIT_REASON_APIC_ACCESS]             = handle_apic_access,\n\t[EXIT_REASON_APIC_WRITE]              = handle_apic_write,\n\t[EXIT_REASON_EOI_INDUCED]             = handle_apic_eoi_induced,\n\t[EXIT_REASON_WBINVD]                  = kvm_emulate_wbinvd,\n\t[EXIT_REASON_XSETBV]                  = kvm_emulate_xsetbv,\n\t[EXIT_REASON_TASK_SWITCH]             = handle_task_switch,\n\t[EXIT_REASON_MCE_DURING_VMENTRY]      = handle_machine_check,\n\t[EXIT_REASON_GDTR_IDTR]\t\t      = handle_desc,\n\t[EXIT_REASON_LDTR_TR]\t\t      = handle_desc,\n\t[EXIT_REASON_EPT_VIOLATION]\t      = handle_ept_violation,\n\t[EXIT_REASON_EPT_MISCONFIG]           = handle_ept_misconfig,\n\t[EXIT_REASON_PAUSE_INSTRUCTION]       = handle_pause,\n\t[EXIT_REASON_MWAIT_INSTRUCTION]\t      = kvm_emulate_mwait,\n\t[EXIT_REASON_MONITOR_TRAP_FLAG]       = handle_monitor_trap,\n\t[EXIT_REASON_MONITOR_INSTRUCTION]     = kvm_emulate_monitor,\n\t[EXIT_REASON_INVEPT]                  = handle_vmx_instruction,\n\t[EXIT_REASON_INVVPID]                 = handle_vmx_instruction,\n\t[EXIT_REASON_RDRAND]                  = kvm_handle_invalid_op,\n\t[EXIT_REASON_RDSEED]                  = kvm_handle_invalid_op,\n\t[EXIT_REASON_PML_FULL]\t\t      = handle_pml_full,\n\t[EXIT_REASON_INVPCID]                 = handle_invpcid,\n\t[EXIT_REASON_VMFUNC]\t\t      = handle_vmx_instruction,\n\t[EXIT_REASON_PREEMPTION_TIMER]\t      = handle_preemption_timer,\n\t[EXIT_REASON_ENCLS]\t\t      = handle_encls,\n\t[EXIT_REASON_BUS_LOCK]                = handle_bus_lock_vmexit,\n\t[EXIT_REASON_NOTIFY]\t\t      = handle_notify,\n};\n\nstatic const int kvm_vmx_max_exit_handlers =\n\tARRAY_SIZE(kvm_vmx_exit_handlers);\n\nstatic void vmx_get_exit_info(struct kvm_vcpu *vcpu, u32 *reason,\n\t\t\t      u64 *info1, u64 *info2,\n\t\t\t      u32 *intr_info, u32 *error_code)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t*reason = vmx->exit_reason.full;\n\t*info1 = vmx_get_exit_qual(vcpu);\n\tif (!(vmx->exit_reason.failed_vmentry)) {\n\t\t*info2 = vmx->idt_vectoring_info;\n\t\t*intr_info = vmx_get_intr_info(vcpu);\n\t\tif (is_exception_with_error_code(*intr_info))\n\t\t\t*error_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);\n\t\telse\n\t\t\t*error_code = 0;\n\t} else {\n\t\t*info2 = 0;\n\t\t*intr_info = 0;\n\t\t*error_code = 0;\n\t}\n}\n\nstatic void vmx_destroy_pml_buffer(struct vcpu_vmx *vmx)\n{\n\tif (vmx->pml_pg) {\n\t\t__free_page(vmx->pml_pg);\n\t\tvmx->pml_pg = NULL;\n\t}\n}\n\nstatic void vmx_flush_pml_buffer(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu64 *pml_buf;\n\tu16 pml_idx;\n\n\tpml_idx = vmcs_read16(GUEST_PML_INDEX);\n\n\t \n\tif (pml_idx == (PML_ENTITY_NUM - 1))\n\t\treturn;\n\n\t \n\tif (pml_idx >= PML_ENTITY_NUM)\n\t\tpml_idx = 0;\n\telse\n\t\tpml_idx++;\n\n\tpml_buf = page_address(vmx->pml_pg);\n\tfor (; pml_idx < PML_ENTITY_NUM; pml_idx++) {\n\t\tu64 gpa;\n\n\t\tgpa = pml_buf[pml_idx];\n\t\tWARN_ON(gpa & (PAGE_SIZE - 1));\n\t\tkvm_vcpu_mark_page_dirty(vcpu, gpa >> PAGE_SHIFT);\n\t}\n\n\t \n\tvmcs_write16(GUEST_PML_INDEX, PML_ENTITY_NUM - 1);\n}\n\nstatic void vmx_dump_sel(char *name, uint32_t sel)\n{\n\tpr_err(\"%s sel=0x%04x, attr=0x%05x, limit=0x%08x, base=0x%016lx\\n\",\n\t       name, vmcs_read16(sel),\n\t       vmcs_read32(sel + GUEST_ES_AR_BYTES - GUEST_ES_SELECTOR),\n\t       vmcs_read32(sel + GUEST_ES_LIMIT - GUEST_ES_SELECTOR),\n\t       vmcs_readl(sel + GUEST_ES_BASE - GUEST_ES_SELECTOR));\n}\n\nstatic void vmx_dump_dtsel(char *name, uint32_t limit)\n{\n\tpr_err(\"%s                           limit=0x%08x, base=0x%016lx\\n\",\n\t       name, vmcs_read32(limit),\n\t       vmcs_readl(limit + GUEST_GDTR_BASE - GUEST_GDTR_LIMIT));\n}\n\nstatic void vmx_dump_msrs(char *name, struct vmx_msrs *m)\n{\n\tunsigned int i;\n\tstruct vmx_msr_entry *e;\n\n\tpr_err(\"MSR %s:\\n\", name);\n\tfor (i = 0, e = m->val; i < m->nr; ++i, ++e)\n\t\tpr_err(\"  %2d: msr=0x%08x value=0x%016llx\\n\", i, e->index, e->value);\n}\n\nvoid dump_vmcs(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 vmentry_ctl, vmexit_ctl;\n\tu32 cpu_based_exec_ctrl, pin_based_exec_ctrl, secondary_exec_control;\n\tu64 tertiary_exec_control;\n\tunsigned long cr4;\n\tint efer_slot;\n\n\tif (!dump_invalid_vmcs) {\n\t\tpr_warn_ratelimited(\"set kvm_intel.dump_invalid_vmcs=1 to dump internal KVM state.\\n\");\n\t\treturn;\n\t}\n\n\tvmentry_ctl = vmcs_read32(VM_ENTRY_CONTROLS);\n\tvmexit_ctl = vmcs_read32(VM_EXIT_CONTROLS);\n\tcpu_based_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);\n\tpin_based_exec_ctrl = vmcs_read32(PIN_BASED_VM_EXEC_CONTROL);\n\tcr4 = vmcs_readl(GUEST_CR4);\n\n\tif (cpu_has_secondary_exec_ctrls())\n\t\tsecondary_exec_control = vmcs_read32(SECONDARY_VM_EXEC_CONTROL);\n\telse\n\t\tsecondary_exec_control = 0;\n\n\tif (cpu_has_tertiary_exec_ctrls())\n\t\ttertiary_exec_control = vmcs_read64(TERTIARY_VM_EXEC_CONTROL);\n\telse\n\t\ttertiary_exec_control = 0;\n\n\tpr_err(\"VMCS %p, last attempted VM-entry on CPU %d\\n\",\n\t       vmx->loaded_vmcs->vmcs, vcpu->arch.last_vmentry_cpu);\n\tpr_err(\"*** Guest State ***\\n\");\n\tpr_err(\"CR0: actual=0x%016lx, shadow=0x%016lx, gh_mask=%016lx\\n\",\n\t       vmcs_readl(GUEST_CR0), vmcs_readl(CR0_READ_SHADOW),\n\t       vmcs_readl(CR0_GUEST_HOST_MASK));\n\tpr_err(\"CR4: actual=0x%016lx, shadow=0x%016lx, gh_mask=%016lx\\n\",\n\t       cr4, vmcs_readl(CR4_READ_SHADOW), vmcs_readl(CR4_GUEST_HOST_MASK));\n\tpr_err(\"CR3 = 0x%016lx\\n\", vmcs_readl(GUEST_CR3));\n\tif (cpu_has_vmx_ept()) {\n\t\tpr_err(\"PDPTR0 = 0x%016llx  PDPTR1 = 0x%016llx\\n\",\n\t\t       vmcs_read64(GUEST_PDPTR0), vmcs_read64(GUEST_PDPTR1));\n\t\tpr_err(\"PDPTR2 = 0x%016llx  PDPTR3 = 0x%016llx\\n\",\n\t\t       vmcs_read64(GUEST_PDPTR2), vmcs_read64(GUEST_PDPTR3));\n\t}\n\tpr_err(\"RSP = 0x%016lx  RIP = 0x%016lx\\n\",\n\t       vmcs_readl(GUEST_RSP), vmcs_readl(GUEST_RIP));\n\tpr_err(\"RFLAGS=0x%08lx         DR7 = 0x%016lx\\n\",\n\t       vmcs_readl(GUEST_RFLAGS), vmcs_readl(GUEST_DR7));\n\tpr_err(\"Sysenter RSP=%016lx CS:RIP=%04x:%016lx\\n\",\n\t       vmcs_readl(GUEST_SYSENTER_ESP),\n\t       vmcs_read32(GUEST_SYSENTER_CS), vmcs_readl(GUEST_SYSENTER_EIP));\n\tvmx_dump_sel(\"CS:  \", GUEST_CS_SELECTOR);\n\tvmx_dump_sel(\"DS:  \", GUEST_DS_SELECTOR);\n\tvmx_dump_sel(\"SS:  \", GUEST_SS_SELECTOR);\n\tvmx_dump_sel(\"ES:  \", GUEST_ES_SELECTOR);\n\tvmx_dump_sel(\"FS:  \", GUEST_FS_SELECTOR);\n\tvmx_dump_sel(\"GS:  \", GUEST_GS_SELECTOR);\n\tvmx_dump_dtsel(\"GDTR:\", GUEST_GDTR_LIMIT);\n\tvmx_dump_sel(\"LDTR:\", GUEST_LDTR_SELECTOR);\n\tvmx_dump_dtsel(\"IDTR:\", GUEST_IDTR_LIMIT);\n\tvmx_dump_sel(\"TR:  \", GUEST_TR_SELECTOR);\n\tefer_slot = vmx_find_loadstore_msr_slot(&vmx->msr_autoload.guest, MSR_EFER);\n\tif (vmentry_ctl & VM_ENTRY_LOAD_IA32_EFER)\n\t\tpr_err(\"EFER= 0x%016llx\\n\", vmcs_read64(GUEST_IA32_EFER));\n\telse if (efer_slot >= 0)\n\t\tpr_err(\"EFER= 0x%016llx (autoload)\\n\",\n\t\t       vmx->msr_autoload.guest.val[efer_slot].value);\n\telse if (vmentry_ctl & VM_ENTRY_IA32E_MODE)\n\t\tpr_err(\"EFER= 0x%016llx (effective)\\n\",\n\t\t       vcpu->arch.efer | (EFER_LMA | EFER_LME));\n\telse\n\t\tpr_err(\"EFER= 0x%016llx (effective)\\n\",\n\t\t       vcpu->arch.efer & ~(EFER_LMA | EFER_LME));\n\tif (vmentry_ctl & VM_ENTRY_LOAD_IA32_PAT)\n\t\tpr_err(\"PAT = 0x%016llx\\n\", vmcs_read64(GUEST_IA32_PAT));\n\tpr_err(\"DebugCtl = 0x%016llx  DebugExceptions = 0x%016lx\\n\",\n\t       vmcs_read64(GUEST_IA32_DEBUGCTL),\n\t       vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS));\n\tif (cpu_has_load_perf_global_ctrl() &&\n\t    vmentry_ctl & VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL)\n\t\tpr_err(\"PerfGlobCtl = 0x%016llx\\n\",\n\t\t       vmcs_read64(GUEST_IA32_PERF_GLOBAL_CTRL));\n\tif (vmentry_ctl & VM_ENTRY_LOAD_BNDCFGS)\n\t\tpr_err(\"BndCfgS = 0x%016llx\\n\", vmcs_read64(GUEST_BNDCFGS));\n\tpr_err(\"Interruptibility = %08x  ActivityState = %08x\\n\",\n\t       vmcs_read32(GUEST_INTERRUPTIBILITY_INFO),\n\t       vmcs_read32(GUEST_ACTIVITY_STATE));\n\tif (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)\n\t\tpr_err(\"InterruptStatus = %04x\\n\",\n\t\t       vmcs_read16(GUEST_INTR_STATUS));\n\tif (vmcs_read32(VM_ENTRY_MSR_LOAD_COUNT) > 0)\n\t\tvmx_dump_msrs(\"guest autoload\", &vmx->msr_autoload.guest);\n\tif (vmcs_read32(VM_EXIT_MSR_STORE_COUNT) > 0)\n\t\tvmx_dump_msrs(\"guest autostore\", &vmx->msr_autostore.guest);\n\n\tpr_err(\"*** Host State ***\\n\");\n\tpr_err(\"RIP = 0x%016lx  RSP = 0x%016lx\\n\",\n\t       vmcs_readl(HOST_RIP), vmcs_readl(HOST_RSP));\n\tpr_err(\"CS=%04x SS=%04x DS=%04x ES=%04x FS=%04x GS=%04x TR=%04x\\n\",\n\t       vmcs_read16(HOST_CS_SELECTOR), vmcs_read16(HOST_SS_SELECTOR),\n\t       vmcs_read16(HOST_DS_SELECTOR), vmcs_read16(HOST_ES_SELECTOR),\n\t       vmcs_read16(HOST_FS_SELECTOR), vmcs_read16(HOST_GS_SELECTOR),\n\t       vmcs_read16(HOST_TR_SELECTOR));\n\tpr_err(\"FSBase=%016lx GSBase=%016lx TRBase=%016lx\\n\",\n\t       vmcs_readl(HOST_FS_BASE), vmcs_readl(HOST_GS_BASE),\n\t       vmcs_readl(HOST_TR_BASE));\n\tpr_err(\"GDTBase=%016lx IDTBase=%016lx\\n\",\n\t       vmcs_readl(HOST_GDTR_BASE), vmcs_readl(HOST_IDTR_BASE));\n\tpr_err(\"CR0=%016lx CR3=%016lx CR4=%016lx\\n\",\n\t       vmcs_readl(HOST_CR0), vmcs_readl(HOST_CR3),\n\t       vmcs_readl(HOST_CR4));\n\tpr_err(\"Sysenter RSP=%016lx CS:RIP=%04x:%016lx\\n\",\n\t       vmcs_readl(HOST_IA32_SYSENTER_ESP),\n\t       vmcs_read32(HOST_IA32_SYSENTER_CS),\n\t       vmcs_readl(HOST_IA32_SYSENTER_EIP));\n\tif (vmexit_ctl & VM_EXIT_LOAD_IA32_EFER)\n\t\tpr_err(\"EFER= 0x%016llx\\n\", vmcs_read64(HOST_IA32_EFER));\n\tif (vmexit_ctl & VM_EXIT_LOAD_IA32_PAT)\n\t\tpr_err(\"PAT = 0x%016llx\\n\", vmcs_read64(HOST_IA32_PAT));\n\tif (cpu_has_load_perf_global_ctrl() &&\n\t    vmexit_ctl & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL)\n\t\tpr_err(\"PerfGlobCtl = 0x%016llx\\n\",\n\t\t       vmcs_read64(HOST_IA32_PERF_GLOBAL_CTRL));\n\tif (vmcs_read32(VM_EXIT_MSR_LOAD_COUNT) > 0)\n\t\tvmx_dump_msrs(\"host autoload\", &vmx->msr_autoload.host);\n\n\tpr_err(\"*** Control State ***\\n\");\n\tpr_err(\"CPUBased=0x%08x SecondaryExec=0x%08x TertiaryExec=0x%016llx\\n\",\n\t       cpu_based_exec_ctrl, secondary_exec_control, tertiary_exec_control);\n\tpr_err(\"PinBased=0x%08x EntryControls=%08x ExitControls=%08x\\n\",\n\t       pin_based_exec_ctrl, vmentry_ctl, vmexit_ctl);\n\tpr_err(\"ExceptionBitmap=%08x PFECmask=%08x PFECmatch=%08x\\n\",\n\t       vmcs_read32(EXCEPTION_BITMAP),\n\t       vmcs_read32(PAGE_FAULT_ERROR_CODE_MASK),\n\t       vmcs_read32(PAGE_FAULT_ERROR_CODE_MATCH));\n\tpr_err(\"VMEntry: intr_info=%08x errcode=%08x ilen=%08x\\n\",\n\t       vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),\n\t       vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE),\n\t       vmcs_read32(VM_ENTRY_INSTRUCTION_LEN));\n\tpr_err(\"VMExit: intr_info=%08x errcode=%08x ilen=%08x\\n\",\n\t       vmcs_read32(VM_EXIT_INTR_INFO),\n\t       vmcs_read32(VM_EXIT_INTR_ERROR_CODE),\n\t       vmcs_read32(VM_EXIT_INSTRUCTION_LEN));\n\tpr_err(\"        reason=%08x qualification=%016lx\\n\",\n\t       vmcs_read32(VM_EXIT_REASON), vmcs_readl(EXIT_QUALIFICATION));\n\tpr_err(\"IDTVectoring: info=%08x errcode=%08x\\n\",\n\t       vmcs_read32(IDT_VECTORING_INFO_FIELD),\n\t       vmcs_read32(IDT_VECTORING_ERROR_CODE));\n\tpr_err(\"TSC Offset = 0x%016llx\\n\", vmcs_read64(TSC_OFFSET));\n\tif (secondary_exec_control & SECONDARY_EXEC_TSC_SCALING)\n\t\tpr_err(\"TSC Multiplier = 0x%016llx\\n\",\n\t\t       vmcs_read64(TSC_MULTIPLIER));\n\tif (cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW) {\n\t\tif (secondary_exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY) {\n\t\t\tu16 status = vmcs_read16(GUEST_INTR_STATUS);\n\t\t\tpr_err(\"SVI|RVI = %02x|%02x \", status >> 8, status & 0xff);\n\t\t}\n\t\tpr_cont(\"TPR Threshold = 0x%02x\\n\", vmcs_read32(TPR_THRESHOLD));\n\t\tif (secondary_exec_control & SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)\n\t\t\tpr_err(\"APIC-access addr = 0x%016llx \", vmcs_read64(APIC_ACCESS_ADDR));\n\t\tpr_cont(\"virt-APIC addr = 0x%016llx\\n\", vmcs_read64(VIRTUAL_APIC_PAGE_ADDR));\n\t}\n\tif (pin_based_exec_ctrl & PIN_BASED_POSTED_INTR)\n\t\tpr_err(\"PostedIntrVec = 0x%02x\\n\", vmcs_read16(POSTED_INTR_NV));\n\tif ((secondary_exec_control & SECONDARY_EXEC_ENABLE_EPT))\n\t\tpr_err(\"EPT pointer = 0x%016llx\\n\", vmcs_read64(EPT_POINTER));\n\tif (secondary_exec_control & SECONDARY_EXEC_PAUSE_LOOP_EXITING)\n\t\tpr_err(\"PLE Gap=%08x Window=%08x\\n\",\n\t\t       vmcs_read32(PLE_GAP), vmcs_read32(PLE_WINDOW));\n\tif (secondary_exec_control & SECONDARY_EXEC_ENABLE_VPID)\n\t\tpr_err(\"Virtual processor ID = 0x%04x\\n\",\n\t\t       vmcs_read16(VIRTUAL_PROCESSOR_ID));\n}\n\n \nstatic int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunion vmx_exit_reason exit_reason = vmx->exit_reason;\n\tu32 vectoring_info = vmx->idt_vectoring_info;\n\tu16 exit_handler_index;\n\n\t \n\tif (enable_pml && !is_guest_mode(vcpu))\n\t\tvmx_flush_pml_buffer(vcpu);\n\n\t \n\tif (KVM_BUG_ON(vmx->nested.nested_run_pending, vcpu->kvm))\n\t\treturn -EIO;\n\n\tif (is_guest_mode(vcpu)) {\n\t\t \n\t\tif (exit_reason.basic == EXIT_REASON_PML_FULL)\n\t\t\tgoto unexpected_vmexit;\n\n\t\t \n\t\tnested_mark_vmcs12_pages_dirty(vcpu);\n\n\t\t \n\t\tif (vmx->emulation_required) {\n\t\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_TRIPLE_FAULT, 0, 0);\n\t\t\treturn 1;\n\t\t}\n\n\t\tif (nested_vmx_reflect_vmexit(vcpu))\n\t\t\treturn 1;\n\t}\n\n\t \n\tif (vmx->emulation_required)\n\t\treturn handle_invalid_guest_state(vcpu);\n\n\tif (exit_reason.failed_vmentry) {\n\t\tdump_vmcs(vcpu);\n\t\tvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\n\t\tvcpu->run->fail_entry.hardware_entry_failure_reason\n\t\t\t= exit_reason.full;\n\t\tvcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;\n\t\treturn 0;\n\t}\n\n\tif (unlikely(vmx->fail)) {\n\t\tdump_vmcs(vcpu);\n\t\tvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\n\t\tvcpu->run->fail_entry.hardware_entry_failure_reason\n\t\t\t= vmcs_read32(VM_INSTRUCTION_ERROR);\n\t\tvcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;\n\t\treturn 0;\n\t}\n\n\t \n\tif ((vectoring_info & VECTORING_INFO_VALID_MASK) &&\n\t    (exit_reason.basic != EXIT_REASON_EXCEPTION_NMI &&\n\t     exit_reason.basic != EXIT_REASON_EPT_VIOLATION &&\n\t     exit_reason.basic != EXIT_REASON_PML_FULL &&\n\t     exit_reason.basic != EXIT_REASON_APIC_ACCESS &&\n\t     exit_reason.basic != EXIT_REASON_TASK_SWITCH &&\n\t     exit_reason.basic != EXIT_REASON_NOTIFY)) {\n\t\tint ndata = 3;\n\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_DELIVERY_EV;\n\t\tvcpu->run->internal.data[0] = vectoring_info;\n\t\tvcpu->run->internal.data[1] = exit_reason.full;\n\t\tvcpu->run->internal.data[2] = vcpu->arch.exit_qualification;\n\t\tif (exit_reason.basic == EXIT_REASON_EPT_MISCONFIG) {\n\t\t\tvcpu->run->internal.data[ndata++] =\n\t\t\t\tvmcs_read64(GUEST_PHYSICAL_ADDRESS);\n\t\t}\n\t\tvcpu->run->internal.data[ndata++] = vcpu->arch.last_vmentry_cpu;\n\t\tvcpu->run->internal.ndata = ndata;\n\t\treturn 0;\n\t}\n\n\tif (unlikely(!enable_vnmi &&\n\t\t     vmx->loaded_vmcs->soft_vnmi_blocked)) {\n\t\tif (!vmx_interrupt_blocked(vcpu)) {\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n\t\t} else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&\n\t\t\t   vcpu->arch.nmi_pending) {\n\t\t\t \n\t\t\tprintk(KERN_WARNING \"%s: Breaking out of NMI-blocked \"\n\t\t\t       \"state on VCPU %d after 1 s timeout\\n\",\n\t\t\t       __func__, vcpu->vcpu_id);\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n\t\t}\n\t}\n\n\tif (exit_fastpath != EXIT_FASTPATH_NONE)\n\t\treturn 1;\n\n\tif (exit_reason.basic >= kvm_vmx_max_exit_handlers)\n\t\tgoto unexpected_vmexit;\n#ifdef CONFIG_RETPOLINE\n\tif (exit_reason.basic == EXIT_REASON_MSR_WRITE)\n\t\treturn kvm_emulate_wrmsr(vcpu);\n\telse if (exit_reason.basic == EXIT_REASON_PREEMPTION_TIMER)\n\t\treturn handle_preemption_timer(vcpu);\n\telse if (exit_reason.basic == EXIT_REASON_INTERRUPT_WINDOW)\n\t\treturn handle_interrupt_window(vcpu);\n\telse if (exit_reason.basic == EXIT_REASON_EXTERNAL_INTERRUPT)\n\t\treturn handle_external_interrupt(vcpu);\n\telse if (exit_reason.basic == EXIT_REASON_HLT)\n\t\treturn kvm_emulate_halt(vcpu);\n\telse if (exit_reason.basic == EXIT_REASON_EPT_MISCONFIG)\n\t\treturn handle_ept_misconfig(vcpu);\n#endif\n\n\texit_handler_index = array_index_nospec((u16)exit_reason.basic,\n\t\t\t\t\t\tkvm_vmx_max_exit_handlers);\n\tif (!kvm_vmx_exit_handlers[exit_handler_index])\n\t\tgoto unexpected_vmexit;\n\n\treturn kvm_vmx_exit_handlers[exit_handler_index](vcpu);\n\nunexpected_vmexit:\n\tvcpu_unimpl(vcpu, \"vmx: unexpected exit reason 0x%x\\n\",\n\t\t    exit_reason.full);\n\tdump_vmcs(vcpu);\n\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\tvcpu->run->internal.suberror =\n\t\t\tKVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;\n\tvcpu->run->internal.ndata = 2;\n\tvcpu->run->internal.data[0] = exit_reason.full;\n\tvcpu->run->internal.data[1] = vcpu->arch.last_vmentry_cpu;\n\treturn 0;\n}\n\nstatic int vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)\n{\n\tint ret = __vmx_handle_exit(vcpu, exit_fastpath);\n\n\t \n\tif (to_vmx(vcpu)->exit_reason.bus_lock_detected) {\n\t\tif (ret > 0)\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_X86_BUS_LOCK;\n\n\t\tvcpu->run->flags |= KVM_RUN_X86_BUS_LOCK;\n\t\treturn 0;\n\t}\n\treturn ret;\n}\n\n \nstatic noinstr void vmx_l1d_flush(struct kvm_vcpu *vcpu)\n{\n\tint size = PAGE_SIZE << L1D_CACHE_ORDER;\n\n\t \n\tif (static_branch_likely(&vmx_l1d_flush_cond)) {\n\t\tbool flush_l1d;\n\n\t\t \n\t\tflush_l1d = vcpu->arch.l1tf_flush_l1d;\n\t\tvcpu->arch.l1tf_flush_l1d = false;\n\n\t\t \n\t\tflush_l1d |= kvm_get_cpu_l1tf_flush_l1d();\n\t\tkvm_clear_cpu_l1tf_flush_l1d();\n\n\t\tif (!flush_l1d)\n\t\t\treturn;\n\t}\n\n\tvcpu->stat.l1d_flush++;\n\n\tif (static_cpu_has(X86_FEATURE_FLUSH_L1D)) {\n\t\tnative_wrmsrl(MSR_IA32_FLUSH_CMD, L1D_FLUSH);\n\t\treturn;\n\t}\n\n\tasm volatile(\n\t\t \n\t\t\"xorl\t%%eax, %%eax\\n\"\n\t\t\".Lpopulate_tlb:\\n\\t\"\n\t\t\"movzbl\t(%[flush_pages], %%\" _ASM_AX \"), %%ecx\\n\\t\"\n\t\t\"addl\t$4096, %%eax\\n\\t\"\n\t\t\"cmpl\t%%eax, %[size]\\n\\t\"\n\t\t\"jne\t.Lpopulate_tlb\\n\\t\"\n\t\t\"xorl\t%%eax, %%eax\\n\\t\"\n\t\t\"cpuid\\n\\t\"\n\t\t \n\t\t\"xorl\t%%eax, %%eax\\n\"\n\t\t\".Lfill_cache:\\n\"\n\t\t\"movzbl\t(%[flush_pages], %%\" _ASM_AX \"), %%ecx\\n\\t\"\n\t\t\"addl\t$64, %%eax\\n\\t\"\n\t\t\"cmpl\t%%eax, %[size]\\n\\t\"\n\t\t\"jne\t.Lfill_cache\\n\\t\"\n\t\t\"lfence\\n\"\n\t\t:: [flush_pages] \"r\" (vmx_l1d_flush_pages),\n\t\t    [size] \"r\" (size)\n\t\t: \"eax\", \"ebx\", \"ecx\", \"edx\");\n}\n\nstatic void vmx_update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tint tpr_threshold;\n\n\tif (is_guest_mode(vcpu) &&\n\t\tnested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))\n\t\treturn;\n\n\ttpr_threshold = (irr == -1 || tpr < irr) ? 0 : irr;\n\tif (is_guest_mode(vcpu))\n\t\tto_vmx(vcpu)->nested.l1_tpr_threshold = tpr_threshold;\n\telse\n\t\tvmcs_write32(TPR_THRESHOLD, tpr_threshold);\n}\n\nvoid vmx_set_virtual_apic_mode(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 sec_exec_control;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn;\n\n\tif (!flexpriority_enabled &&\n\t    !cpu_has_vmx_virtualize_x2apic_mode())\n\t\treturn;\n\n\t \n\tif (is_guest_mode(vcpu)) {\n\t\tvmx->nested.change_vmcs01_virtual_apic_mode = true;\n\t\treturn;\n\t}\n\n\tsec_exec_control = secondary_exec_controls_get(vmx);\n\tsec_exec_control &= ~(SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |\n\t\t\t      SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE);\n\n\tswitch (kvm_get_apic_mode(vcpu)) {\n\tcase LAPIC_MODE_INVALID:\n\t\tWARN_ONCE(true, \"Invalid local APIC state\");\n\t\tbreak;\n\tcase LAPIC_MODE_DISABLED:\n\t\tbreak;\n\tcase LAPIC_MODE_XAPIC:\n\t\tif (flexpriority_enabled) {\n\t\t\tsec_exec_control |=\n\t\t\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;\n\t\t\tkvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);\n\n\t\t\t \n\t\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);\n\t\t}\n\t\tbreak;\n\tcase LAPIC_MODE_X2APIC:\n\t\tif (cpu_has_vmx_virtualize_x2apic_mode())\n\t\t\tsec_exec_control |=\n\t\t\t\tSECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE;\n\t\tbreak;\n\t}\n\tsecondary_exec_controls_set(vmx, sec_exec_control);\n\n\tvmx_update_msr_bitmap_x2apic(vcpu);\n}\n\nstatic void vmx_set_apic_access_page_addr(struct kvm_vcpu *vcpu)\n{\n\tconst gfn_t gfn = APIC_DEFAULT_PHYS_BASE >> PAGE_SHIFT;\n\tstruct kvm *kvm = vcpu->kvm;\n\tstruct kvm_memslots *slots = kvm_memslots(kvm);\n\tstruct kvm_memory_slot *slot;\n\tunsigned long mmu_seq;\n\tkvm_pfn_t pfn;\n\n\t \n\tif (is_guest_mode(vcpu)) {\n\t\tto_vmx(vcpu)->nested.reload_vmcs01_apic_access_page = true;\n\t\treturn;\n\t}\n\n\tif (!(secondary_exec_controls_get(to_vmx(vcpu)) &\n\t    SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES))\n\t\treturn;\n\n\t \n\tslot = id_to_memslot(slots, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT);\n\tif (!slot || slot->flags & KVM_MEMSLOT_INVALID)\n\t\treturn;\n\n\t \n\tmmu_seq = kvm->mmu_invalidate_seq;\n\tsmp_rmb();\n\n\t \n\tpfn = gfn_to_pfn_memslot(slot, gfn);\n\tif (is_error_noslot_pfn(pfn))\n\t\treturn;\n\n\tread_lock(&vcpu->kvm->mmu_lock);\n\tif (mmu_invalidate_retry_hva(kvm, mmu_seq,\n\t\t\t\t     gfn_to_hva_memslot(slot, gfn))) {\n\t\tkvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);\n\t\tread_unlock(&vcpu->kvm->mmu_lock);\n\t\tgoto out;\n\t}\n\n\tvmcs_write64(APIC_ACCESS_ADDR, pfn_to_hpa(pfn));\n\tread_unlock(&vcpu->kvm->mmu_lock);\n\n\t \nout:\n\t \n\tkvm_release_pfn_clean(pfn);\n}\n\nstatic void vmx_hwapic_isr_update(int max_isr)\n{\n\tu16 status;\n\tu8 old;\n\n\tif (max_isr == -1)\n\t\tmax_isr = 0;\n\n\tstatus = vmcs_read16(GUEST_INTR_STATUS);\n\told = status >> 8;\n\tif (max_isr != old) {\n\t\tstatus &= 0xff;\n\t\tstatus |= max_isr << 8;\n\t\tvmcs_write16(GUEST_INTR_STATUS, status);\n\t}\n}\n\nstatic void vmx_set_rvi(int vector)\n{\n\tu16 status;\n\tu8 old;\n\n\tif (vector == -1)\n\t\tvector = 0;\n\n\tstatus = vmcs_read16(GUEST_INTR_STATUS);\n\told = (u8)status & 0xff;\n\tif ((u8)vector != old) {\n\t\tstatus &= ~0xff;\n\t\tstatus |= (u8)vector;\n\t\tvmcs_write16(GUEST_INTR_STATUS, status);\n\t}\n}\n\nstatic void vmx_hwapic_irr_update(struct kvm_vcpu *vcpu, int max_irr)\n{\n\t \n\tif (!is_guest_mode(vcpu))\n\t\tvmx_set_rvi(max_irr);\n}\n\nstatic int vmx_sync_pir_to_irr(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint max_irr;\n\tbool got_posted_interrupt;\n\n\tif (KVM_BUG_ON(!enable_apicv, vcpu->kvm))\n\t\treturn -EIO;\n\n\tif (pi_test_on(&vmx->pi_desc)) {\n\t\tpi_clear_on(&vmx->pi_desc);\n\t\t \n\t\tsmp_mb__after_atomic();\n\t\tgot_posted_interrupt =\n\t\t\tkvm_apic_update_irr(vcpu, vmx->pi_desc.pir, &max_irr);\n\t} else {\n\t\tmax_irr = kvm_lapic_find_highest_irr(vcpu);\n\t\tgot_posted_interrupt = false;\n\t}\n\n\t \n\tif (!is_guest_mode(vcpu) && kvm_vcpu_apicv_active(vcpu))\n\t\tvmx_set_rvi(max_irr);\n\telse if (got_posted_interrupt)\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\treturn max_irr;\n}\n\nstatic void vmx_load_eoi_exitmap(struct kvm_vcpu *vcpu, u64 *eoi_exit_bitmap)\n{\n\tif (!kvm_vcpu_apicv_active(vcpu))\n\t\treturn;\n\n\tvmcs_write64(EOI_EXIT_BITMAP0, eoi_exit_bitmap[0]);\n\tvmcs_write64(EOI_EXIT_BITMAP1, eoi_exit_bitmap[1]);\n\tvmcs_write64(EOI_EXIT_BITMAP2, eoi_exit_bitmap[2]);\n\tvmcs_write64(EOI_EXIT_BITMAP3, eoi_exit_bitmap[3]);\n}\n\nstatic void vmx_apicv_pre_state_restore(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tpi_clear_on(&vmx->pi_desc);\n\tmemset(vmx->pi_desc.pir, 0, sizeof(vmx->pi_desc.pir));\n}\n\nvoid vmx_do_interrupt_irqoff(unsigned long entry);\nvoid vmx_do_nmi_irqoff(void);\n\nstatic void handle_nm_fault_irqoff(struct kvm_vcpu *vcpu)\n{\n\t \n\tif (vcpu->arch.guest_fpu.fpstate->xfd)\n\t\trdmsrl(MSR_IA32_XFD_ERR, vcpu->arch.guest_fpu.xfd_err);\n}\n\nstatic void handle_exception_irqoff(struct vcpu_vmx *vmx)\n{\n\tu32 intr_info = vmx_get_intr_info(&vmx->vcpu);\n\n\t \n\tif (is_page_fault(intr_info))\n\t\tvmx->vcpu.arch.apf.host_apf_flags = kvm_read_and_reset_apf_flags();\n\t \n\telse if (is_nm_fault(intr_info))\n\t\thandle_nm_fault_irqoff(&vmx->vcpu);\n\t \n\telse if (is_machine_check(intr_info))\n\t\tkvm_machine_check();\n}\n\nstatic void handle_external_interrupt_irqoff(struct kvm_vcpu *vcpu)\n{\n\tu32 intr_info = vmx_get_intr_info(vcpu);\n\tunsigned int vector = intr_info & INTR_INFO_VECTOR_MASK;\n\tgate_desc *desc = (gate_desc *)host_idt_base + vector;\n\n\tif (KVM_BUG(!is_external_intr(intr_info), vcpu->kvm,\n\t    \"unexpected VM-Exit interrupt info: 0x%x\", intr_info))\n\t\treturn;\n\n\tkvm_before_interrupt(vcpu, KVM_HANDLING_IRQ);\n\tvmx_do_interrupt_irqoff(gate_offset(desc));\n\tkvm_after_interrupt(vcpu);\n\n\tvcpu->arch.at_instruction_boundary = true;\n}\n\nstatic void vmx_handle_exit_irqoff(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (vmx->emulation_required)\n\t\treturn;\n\n\tif (vmx->exit_reason.basic == EXIT_REASON_EXTERNAL_INTERRUPT)\n\t\thandle_external_interrupt_irqoff(vcpu);\n\telse if (vmx->exit_reason.basic == EXIT_REASON_EXCEPTION_NMI)\n\t\thandle_exception_irqoff(vmx);\n}\n\n \nstatic bool vmx_has_emulated_msr(struct kvm *kvm, u32 index)\n{\n\tswitch (index) {\n\tcase MSR_IA32_SMBASE:\n\t\tif (!IS_ENABLED(CONFIG_KVM_SMM))\n\t\t\treturn false;\n\t\t \n\t\treturn enable_unrestricted_guest || emulate_invalid_guest_state;\n\tcase KVM_FIRST_EMULATED_VMX_MSR ... KVM_LAST_EMULATED_VMX_MSR:\n\t\treturn nested;\n\tcase MSR_AMD64_VIRT_SPEC_CTRL:\n\tcase MSR_AMD64_TSC_RATIO:\n\t\t \n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\nstatic void vmx_recover_nmi_blocking(struct vcpu_vmx *vmx)\n{\n\tu32 exit_intr_info;\n\tbool unblock_nmi;\n\tu8 vector;\n\tbool idtv_info_valid;\n\n\tidtv_info_valid = vmx->idt_vectoring_info & VECTORING_INFO_VALID_MASK;\n\n\tif (enable_vnmi) {\n\t\tif (vmx->loaded_vmcs->nmi_known_unmasked)\n\t\t\treturn;\n\n\t\texit_intr_info = vmx_get_intr_info(&vmx->vcpu);\n\t\tunblock_nmi = (exit_intr_info & INTR_INFO_UNBLOCK_NMI) != 0;\n\t\tvector = exit_intr_info & INTR_INFO_VECTOR_MASK;\n\t\t \n\t\tif ((exit_intr_info & INTR_INFO_VALID_MASK) && unblock_nmi &&\n\t\t    vector != DF_VECTOR && !idtv_info_valid)\n\t\t\tvmcs_set_bits(GUEST_INTERRUPTIBILITY_INFO,\n\t\t\t\t      GUEST_INTR_STATE_NMI);\n\t\telse\n\t\t\tvmx->loaded_vmcs->nmi_known_unmasked =\n\t\t\t\t!(vmcs_read32(GUEST_INTERRUPTIBILITY_INFO)\n\t\t\t\t  & GUEST_INTR_STATE_NMI);\n\t} else if (unlikely(vmx->loaded_vmcs->soft_vnmi_blocked))\n\t\tvmx->loaded_vmcs->vnmi_blocked_time +=\n\t\t\tktime_to_ns(ktime_sub(ktime_get(),\n\t\t\t\t\t      vmx->loaded_vmcs->entry_time));\n}\n\nstatic void __vmx_complete_interrupts(struct kvm_vcpu *vcpu,\n\t\t\t\t      u32 idt_vectoring_info,\n\t\t\t\t      int instr_len_field,\n\t\t\t\t      int error_code_field)\n{\n\tu8 vector;\n\tint type;\n\tbool idtv_info_valid;\n\n\tidtv_info_valid = idt_vectoring_info & VECTORING_INFO_VALID_MASK;\n\n\tvcpu->arch.nmi_injected = false;\n\tkvm_clear_exception_queue(vcpu);\n\tkvm_clear_interrupt_queue(vcpu);\n\n\tif (!idtv_info_valid)\n\t\treturn;\n\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\tvector = idt_vectoring_info & VECTORING_INFO_VECTOR_MASK;\n\ttype = idt_vectoring_info & VECTORING_INFO_TYPE_MASK;\n\n\tswitch (type) {\n\tcase INTR_TYPE_NMI_INTR:\n\t\tvcpu->arch.nmi_injected = true;\n\t\t \n\t\tvmx_set_nmi_mask(vcpu, false);\n\t\tbreak;\n\tcase INTR_TYPE_SOFT_EXCEPTION:\n\t\tvcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);\n\t\tfallthrough;\n\tcase INTR_TYPE_HARD_EXCEPTION:\n\t\tif (idt_vectoring_info & VECTORING_INFO_DELIVER_CODE_MASK) {\n\t\t\tu32 err = vmcs_read32(error_code_field);\n\t\t\tkvm_requeue_exception_e(vcpu, vector, err);\n\t\t} else\n\t\t\tkvm_requeue_exception(vcpu, vector);\n\t\tbreak;\n\tcase INTR_TYPE_SOFT_INTR:\n\t\tvcpu->arch.event_exit_inst_len = vmcs_read32(instr_len_field);\n\t\tfallthrough;\n\tcase INTR_TYPE_EXT_INTR:\n\t\tkvm_queue_interrupt(vcpu, vector, type == INTR_TYPE_SOFT_INTR);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic void vmx_complete_interrupts(struct vcpu_vmx *vmx)\n{\n\t__vmx_complete_interrupts(&vmx->vcpu, vmx->idt_vectoring_info,\n\t\t\t\t  VM_EXIT_INSTRUCTION_LEN,\n\t\t\t\t  IDT_VECTORING_ERROR_CODE);\n}\n\nstatic void vmx_cancel_injection(struct kvm_vcpu *vcpu)\n{\n\t__vmx_complete_interrupts(vcpu,\n\t\t\t\t  vmcs_read32(VM_ENTRY_INTR_INFO_FIELD),\n\t\t\t\t  VM_ENTRY_INSTRUCTION_LEN,\n\t\t\t\t  VM_ENTRY_EXCEPTION_ERROR_CODE);\n\n\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);\n}\n\nstatic void atomic_switch_perf_msrs(struct vcpu_vmx *vmx)\n{\n\tint i, nr_msrs;\n\tstruct perf_guest_switch_msr *msrs;\n\tstruct kvm_pmu *pmu = vcpu_to_pmu(&vmx->vcpu);\n\n\tpmu->host_cross_mapped_mask = 0;\n\tif (pmu->pebs_enable & pmu->global_ctrl)\n\t\tintel_pmu_cross_mapped_check(pmu);\n\n\t \n\tmsrs = perf_guest_get_msrs(&nr_msrs, (void *)pmu);\n\tif (!msrs)\n\t\treturn;\n\n\tfor (i = 0; i < nr_msrs; i++)\n\t\tif (msrs[i].host == msrs[i].guest)\n\t\t\tclear_atomic_switch_msr(vmx, msrs[i].msr);\n\t\telse\n\t\t\tadd_atomic_switch_msr(vmx, msrs[i].msr, msrs[i].guest,\n\t\t\t\t\tmsrs[i].host, false);\n}\n\nstatic void vmx_update_hv_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu64 tscl;\n\tu32 delta_tsc;\n\n\tif (vmx->req_immediate_exit) {\n\t\tvmcs_write32(VMX_PREEMPTION_TIMER_VALUE, 0);\n\t\tvmx->loaded_vmcs->hv_timer_soft_disabled = false;\n\t} else if (vmx->hv_deadline_tsc != -1) {\n\t\ttscl = rdtsc();\n\t\tif (vmx->hv_deadline_tsc > tscl)\n\t\t\t \n\t\t\tdelta_tsc = (u32)((vmx->hv_deadline_tsc - tscl) >>\n\t\t\t\tcpu_preemption_timer_multi);\n\t\telse\n\t\t\tdelta_tsc = 0;\n\n\t\tvmcs_write32(VMX_PREEMPTION_TIMER_VALUE, delta_tsc);\n\t\tvmx->loaded_vmcs->hv_timer_soft_disabled = false;\n\t} else if (!vmx->loaded_vmcs->hv_timer_soft_disabled) {\n\t\tvmcs_write32(VMX_PREEMPTION_TIMER_VALUE, -1);\n\t\tvmx->loaded_vmcs->hv_timer_soft_disabled = true;\n\t}\n}\n\nvoid noinstr vmx_update_host_rsp(struct vcpu_vmx *vmx, unsigned long host_rsp)\n{\n\tif (unlikely(host_rsp != vmx->loaded_vmcs->host_state.rsp)) {\n\t\tvmx->loaded_vmcs->host_state.rsp = host_rsp;\n\t\tvmcs_writel(HOST_RSP, host_rsp);\n\t}\n}\n\nvoid noinstr vmx_spec_ctrl_restore_host(struct vcpu_vmx *vmx,\n\t\t\t\t\tunsigned int flags)\n{\n\tu64 hostval = this_cpu_read(x86_spec_ctrl_current);\n\n\tif (!cpu_feature_enabled(X86_FEATURE_MSR_SPEC_CTRL))\n\t\treturn;\n\n\tif (flags & VMX_RUN_SAVE_SPEC_CTRL)\n\t\tvmx->spec_ctrl = __rdmsr(MSR_IA32_SPEC_CTRL);\n\n\t \n\tif (cpu_feature_enabled(X86_FEATURE_KERNEL_IBRS) ||\n\t    vmx->spec_ctrl != hostval)\n\t\tnative_wrmsrl(MSR_IA32_SPEC_CTRL, hostval);\n\n\tbarrier_nospec();\n}\n\nstatic fastpath_t vmx_exit_handlers_fastpath(struct kvm_vcpu *vcpu)\n{\n\tswitch (to_vmx(vcpu)->exit_reason.basic) {\n\tcase EXIT_REASON_MSR_WRITE:\n\t\treturn handle_fastpath_set_msr_irqoff(vcpu);\n\tcase EXIT_REASON_PREEMPTION_TIMER:\n\t\treturn handle_fastpath_preemption_timer(vcpu);\n\tdefault:\n\t\treturn EXIT_FASTPATH_NONE;\n\t}\n}\n\nstatic noinstr void vmx_vcpu_enter_exit(struct kvm_vcpu *vcpu,\n\t\t\t\t\tunsigned int flags)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tguest_state_enter_irqoff();\n\n\t \n\tif (static_branch_unlikely(&vmx_l1d_should_flush))\n\t\tvmx_l1d_flush(vcpu);\n\telse if (static_branch_unlikely(&mds_user_clear))\n\t\tmds_clear_cpu_buffers();\n\telse if (static_branch_unlikely(&mmio_stale_data_clear) &&\n\t\t kvm_arch_has_assigned_device(vcpu->kvm))\n\t\tmds_clear_cpu_buffers();\n\n\tvmx_disable_fb_clear(vmx);\n\n\tif (vcpu->arch.cr2 != native_read_cr2())\n\t\tnative_write_cr2(vcpu->arch.cr2);\n\n\tvmx->fail = __vmx_vcpu_run(vmx, (unsigned long *)&vcpu->arch.regs,\n\t\t\t\t   flags);\n\n\tvcpu->arch.cr2 = native_read_cr2();\n\tvcpu->arch.regs_avail &= ~VMX_REGS_LAZY_LOAD_SET;\n\n\tvmx->idt_vectoring_info = 0;\n\n\tvmx_enable_fb_clear(vmx);\n\n\tif (unlikely(vmx->fail)) {\n\t\tvmx->exit_reason.full = 0xdead;\n\t\tgoto out;\n\t}\n\n\tvmx->exit_reason.full = vmcs_read32(VM_EXIT_REASON);\n\tif (likely(!vmx->exit_reason.failed_vmentry))\n\t\tvmx->idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);\n\n\tif ((u16)vmx->exit_reason.basic == EXIT_REASON_EXCEPTION_NMI &&\n\t    is_nmi(vmx_get_intr_info(vcpu))) {\n\t\tkvm_before_interrupt(vcpu, KVM_HANDLING_NMI);\n\t\tvmx_do_nmi_irqoff();\n\t\tkvm_after_interrupt(vcpu);\n\t}\n\nout:\n\tguest_state_exit_irqoff();\n}\n\nstatic fastpath_t vmx_vcpu_run(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long cr3, cr4;\n\n\t \n\tif (unlikely(!enable_vnmi &&\n\t\t     vmx->loaded_vmcs->soft_vnmi_blocked))\n\t\tvmx->loaded_vmcs->entry_time = ktime_get();\n\n\t \n\tif (unlikely(vmx->emulation_required)) {\n\t\tvmx->fail = 0;\n\n\t\tvmx->exit_reason.full = EXIT_REASON_INVALID_STATE;\n\t\tvmx->exit_reason.failed_vmentry = 1;\n\t\tkvm_register_mark_available(vcpu, VCPU_EXREG_EXIT_INFO_1);\n\t\tvmx->exit_qualification = ENTRY_FAIL_DEFAULT;\n\t\tkvm_register_mark_available(vcpu, VCPU_EXREG_EXIT_INFO_2);\n\t\tvmx->exit_intr_info = 0;\n\t\treturn EXIT_FASTPATH_NONE;\n\t}\n\n\ttrace_kvm_entry(vcpu);\n\n\tif (vmx->ple_window_dirty) {\n\t\tvmx->ple_window_dirty = false;\n\t\tvmcs_write32(PLE_WINDOW, vmx->ple_window);\n\t}\n\n\t \n\tWARN_ON_ONCE(vmx->nested.need_vmcs12_to_shadow_sync);\n\n\tif (kvm_register_is_dirty(vcpu, VCPU_REGS_RSP))\n\t\tvmcs_writel(GUEST_RSP, vcpu->arch.regs[VCPU_REGS_RSP]);\n\tif (kvm_register_is_dirty(vcpu, VCPU_REGS_RIP))\n\t\tvmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]);\n\tvcpu->arch.regs_dirty = 0;\n\n\t \n\tcr3 = __get_current_cr3_fast();\n\tif (unlikely(cr3 != vmx->loaded_vmcs->host_state.cr3)) {\n\t\tvmcs_writel(HOST_CR3, cr3);\n\t\tvmx->loaded_vmcs->host_state.cr3 = cr3;\n\t}\n\n\tcr4 = cr4_read_shadow();\n\tif (unlikely(cr4 != vmx->loaded_vmcs->host_state.cr4)) {\n\t\tvmcs_writel(HOST_CR4, cr4);\n\t\tvmx->loaded_vmcs->host_state.cr4 = cr4;\n\t}\n\n\t \n\tif (unlikely(vcpu->arch.switch_db_regs & KVM_DEBUGREG_WONT_EXIT))\n\t\tset_debugreg(vcpu->arch.dr6, 6);\n\n\t \n\tif (vcpu->guest_debug & KVM_GUESTDBG_SINGLESTEP)\n\t\tvmx_set_interrupt_shadow(vcpu, 0);\n\n\tkvm_load_guest_xsave_state(vcpu);\n\n\tpt_guest_enter(vmx);\n\n\tatomic_switch_perf_msrs(vmx);\n\tif (intel_pmu_lbr_is_enabled(vcpu))\n\t\tvmx_passthrough_lbr_msrs(vcpu);\n\n\tif (enable_preemption_timer)\n\t\tvmx_update_hv_timer(vcpu);\n\n\tkvm_wait_lapic_expire(vcpu);\n\n\t \n\tvmx_vcpu_enter_exit(vcpu, __vmx_vcpu_run_flags(vmx));\n\n\t \n\tif (kvm_is_using_evmcs()) {\n\t\tcurrent_evmcs->hv_clean_fields |=\n\t\t\tHV_VMX_ENLIGHTENED_CLEAN_FIELD_ALL;\n\n\t\tcurrent_evmcs->hv_vp_id = kvm_hv_get_vpindex(vcpu);\n\t}\n\n\t \n\tif (vmx->host_debugctlmsr)\n\t\tupdate_debugctlmsr(vmx->host_debugctlmsr);\n\n#ifndef CONFIG_X86_64\n\t \n\tloadsegment(ds, __USER_DS);\n\tloadsegment(es, __USER_DS);\n#endif\n\n\tpt_guest_exit(vmx);\n\n\tkvm_load_host_xsave_state(vcpu);\n\n\tif (is_guest_mode(vcpu)) {\n\t\t \n\t\tif (vmx->nested.nested_run_pending &&\n\t\t    !vmx->exit_reason.failed_vmentry)\n\t\t\t++vcpu->stat.nested_run;\n\n\t\tvmx->nested.nested_run_pending = 0;\n\t}\n\n\tif (unlikely(vmx->fail))\n\t\treturn EXIT_FASTPATH_NONE;\n\n\tif (unlikely((u16)vmx->exit_reason.basic == EXIT_REASON_MCE_DURING_VMENTRY))\n\t\tkvm_machine_check();\n\n\ttrace_kvm_exit(vcpu, KVM_ISA_VMX);\n\n\tif (unlikely(vmx->exit_reason.failed_vmentry))\n\t\treturn EXIT_FASTPATH_NONE;\n\n\tvmx->loaded_vmcs->launched = 1;\n\n\tvmx_recover_nmi_blocking(vmx);\n\tvmx_complete_interrupts(vmx);\n\n\tif (is_guest_mode(vcpu))\n\t\treturn EXIT_FASTPATH_NONE;\n\n\treturn vmx_exit_handlers_fastpath(vcpu);\n}\n\nstatic void vmx_vcpu_free(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (enable_pml)\n\t\tvmx_destroy_pml_buffer(vmx);\n\tfree_vpid(vmx->vpid);\n\tnested_vmx_free_vcpu(vcpu);\n\tfree_loaded_vmcs(vmx->loaded_vmcs);\n}\n\nstatic int vmx_vcpu_create(struct kvm_vcpu *vcpu)\n{\n\tstruct vmx_uret_msr *tsx_ctrl;\n\tstruct vcpu_vmx *vmx;\n\tint i, err;\n\n\tBUILD_BUG_ON(offsetof(struct vcpu_vmx, vcpu) != 0);\n\tvmx = to_vmx(vcpu);\n\n\tINIT_LIST_HEAD(&vmx->pi_wakeup_list);\n\n\terr = -ENOMEM;\n\n\tvmx->vpid = allocate_vpid();\n\n\t \n\tif (enable_pml) {\n\t\tvmx->pml_pg = alloc_page(GFP_KERNEL_ACCOUNT | __GFP_ZERO);\n\t\tif (!vmx->pml_pg)\n\t\t\tgoto free_vpid;\n\t}\n\n\tfor (i = 0; i < kvm_nr_uret_msrs; ++i)\n\t\tvmx->guest_uret_msrs[i].mask = -1ull;\n\tif (boot_cpu_has(X86_FEATURE_RTM)) {\n\t\t \n\t\ttsx_ctrl = vmx_find_uret_msr(vmx, MSR_IA32_TSX_CTRL);\n\t\tif (tsx_ctrl)\n\t\t\ttsx_ctrl->mask = ~(u64)TSX_CTRL_CPUID_CLEAR;\n\t}\n\n\terr = alloc_loaded_vmcs(&vmx->vmcs01);\n\tif (err < 0)\n\t\tgoto free_pml;\n\n\t \n\tif (kvm_is_using_evmcs() &&\n\t    (ms_hyperv.nested_features & HV_X64_NESTED_MSR_BITMAP)) {\n\t\tstruct hv_enlightened_vmcs *evmcs = (void *)vmx->vmcs01.vmcs;\n\n\t\tevmcs->hv_enlightenments_control.msr_bitmap = 1;\n\t}\n\n\t \n\tbitmap_fill(vmx->shadow_msr_intercept.read, MAX_POSSIBLE_PASSTHROUGH_MSRS);\n\tbitmap_fill(vmx->shadow_msr_intercept.write, MAX_POSSIBLE_PASSTHROUGH_MSRS);\n\n\tvmx_disable_intercept_for_msr(vcpu, MSR_IA32_TSC, MSR_TYPE_R);\n#ifdef CONFIG_X86_64\n\tvmx_disable_intercept_for_msr(vcpu, MSR_FS_BASE, MSR_TYPE_RW);\n\tvmx_disable_intercept_for_msr(vcpu, MSR_GS_BASE, MSR_TYPE_RW);\n\tvmx_disable_intercept_for_msr(vcpu, MSR_KERNEL_GS_BASE, MSR_TYPE_RW);\n#endif\n\tvmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_CS, MSR_TYPE_RW);\n\tvmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_ESP, MSR_TYPE_RW);\n\tvmx_disable_intercept_for_msr(vcpu, MSR_IA32_SYSENTER_EIP, MSR_TYPE_RW);\n\tif (kvm_cstate_in_guest(vcpu->kvm)) {\n\t\tvmx_disable_intercept_for_msr(vcpu, MSR_CORE_C1_RES, MSR_TYPE_R);\n\t\tvmx_disable_intercept_for_msr(vcpu, MSR_CORE_C3_RESIDENCY, MSR_TYPE_R);\n\t\tvmx_disable_intercept_for_msr(vcpu, MSR_CORE_C6_RESIDENCY, MSR_TYPE_R);\n\t\tvmx_disable_intercept_for_msr(vcpu, MSR_CORE_C7_RESIDENCY, MSR_TYPE_R);\n\t}\n\n\tvmx->loaded_vmcs = &vmx->vmcs01;\n\n\tif (cpu_need_virtualize_apic_accesses(vcpu)) {\n\t\terr = kvm_alloc_apic_access_page(vcpu->kvm);\n\t\tif (err)\n\t\t\tgoto free_vmcs;\n\t}\n\n\tif (enable_ept && !enable_unrestricted_guest) {\n\t\terr = init_rmode_identity_map(vcpu->kvm);\n\t\tif (err)\n\t\t\tgoto free_vmcs;\n\t}\n\n\tif (vmx_can_use_ipiv(vcpu))\n\t\tWRITE_ONCE(to_kvm_vmx(vcpu->kvm)->pid_table[vcpu->vcpu_id],\n\t\t\t   __pa(&vmx->pi_desc) | PID_TABLE_ENTRY_VALID);\n\n\treturn 0;\n\nfree_vmcs:\n\tfree_loaded_vmcs(vmx->loaded_vmcs);\nfree_pml:\n\tvmx_destroy_pml_buffer(vmx);\nfree_vpid:\n\tfree_vpid(vmx->vpid);\n\treturn err;\n}\n\n#define L1TF_MSG_SMT \"L1TF CPU bug present and SMT on, data leak possible. See CVE-2018-3646 and https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/l1tf.html for details.\\n\"\n#define L1TF_MSG_L1D \"L1TF CPU bug present and virtualization mitigation disabled, data leak possible. See CVE-2018-3646 and https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/l1tf.html for details.\\n\"\n\nstatic int vmx_vm_init(struct kvm *kvm)\n{\n\tif (!ple_gap)\n\t\tkvm->arch.pause_in_guest = true;\n\n\tif (boot_cpu_has(X86_BUG_L1TF) && enable_ept) {\n\t\tswitch (l1tf_mitigation) {\n\t\tcase L1TF_MITIGATION_OFF:\n\t\tcase L1TF_MITIGATION_FLUSH_NOWARN:\n\t\t\t \n\t\t\tbreak;\n\t\tcase L1TF_MITIGATION_FLUSH:\n\t\tcase L1TF_MITIGATION_FLUSH_NOSMT:\n\t\tcase L1TF_MITIGATION_FULL:\n\t\t\t \n\t\t\tif (sched_smt_active())\n\t\t\t\tpr_warn_once(L1TF_MSG_SMT);\n\t\t\tif (l1tf_vmx_mitigation == VMENTER_L1D_FLUSH_NEVER)\n\t\t\t\tpr_warn_once(L1TF_MSG_L1D);\n\t\t\tbreak;\n\t\tcase L1TF_MITIGATION_FULL_FORCE:\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic u8 vmx_get_mt_mask(struct kvm_vcpu *vcpu, gfn_t gfn, bool is_mmio)\n{\n\tu8 cache;\n\n\t \n\n\tif (is_mmio)\n\t\treturn MTRR_TYPE_UNCACHABLE << VMX_EPT_MT_EPTE_SHIFT;\n\n\tif (!kvm_arch_has_noncoherent_dma(vcpu->kvm))\n\t\treturn (MTRR_TYPE_WRBACK << VMX_EPT_MT_EPTE_SHIFT) | VMX_EPT_IPAT_BIT;\n\n\tif (kvm_read_cr0_bits(vcpu, X86_CR0_CD)) {\n\t\tif (kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_CD_NW_CLEARED))\n\t\t\tcache = MTRR_TYPE_WRBACK;\n\t\telse\n\t\t\tcache = MTRR_TYPE_UNCACHABLE;\n\n\t\treturn (cache << VMX_EPT_MT_EPTE_SHIFT) | VMX_EPT_IPAT_BIT;\n\t}\n\n\treturn kvm_mtrr_get_guest_memory_type(vcpu, gfn) << VMX_EPT_MT_EPTE_SHIFT;\n}\n\nstatic void vmcs_set_secondary_exec_control(struct vcpu_vmx *vmx, u32 new_ctl)\n{\n\t \n\tu32 mask =\n\t\tSECONDARY_EXEC_SHADOW_VMCS |\n\t\tSECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |\n\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |\n\t\tSECONDARY_EXEC_DESC;\n\n\tu32 cur_ctl = secondary_exec_controls_get(vmx);\n\n\tsecondary_exec_controls_set(vmx, (new_ctl & ~mask) | (cur_ctl & mask));\n}\n\n \nstatic void nested_vmx_cr_fixed1_bits_update(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct kvm_cpuid_entry2 *entry;\n\n\tvmx->nested.msrs.cr0_fixed1 = 0xffffffff;\n\tvmx->nested.msrs.cr4_fixed1 = X86_CR4_PCE;\n\n#define cr4_fixed1_update(_cr4_mask, _reg, _cpuid_mask) do {\t\t\\\n\tif (entry && (entry->_reg & (_cpuid_mask)))\t\t\t\\\n\t\tvmx->nested.msrs.cr4_fixed1 |= (_cr4_mask);\t\\\n} while (0)\n\n\tentry = kvm_find_cpuid_entry(vcpu, 0x1);\n\tcr4_fixed1_update(X86_CR4_VME,        edx, feature_bit(VME));\n\tcr4_fixed1_update(X86_CR4_PVI,        edx, feature_bit(VME));\n\tcr4_fixed1_update(X86_CR4_TSD,        edx, feature_bit(TSC));\n\tcr4_fixed1_update(X86_CR4_DE,         edx, feature_bit(DE));\n\tcr4_fixed1_update(X86_CR4_PSE,        edx, feature_bit(PSE));\n\tcr4_fixed1_update(X86_CR4_PAE,        edx, feature_bit(PAE));\n\tcr4_fixed1_update(X86_CR4_MCE,        edx, feature_bit(MCE));\n\tcr4_fixed1_update(X86_CR4_PGE,        edx, feature_bit(PGE));\n\tcr4_fixed1_update(X86_CR4_OSFXSR,     edx, feature_bit(FXSR));\n\tcr4_fixed1_update(X86_CR4_OSXMMEXCPT, edx, feature_bit(XMM));\n\tcr4_fixed1_update(X86_CR4_VMXE,       ecx, feature_bit(VMX));\n\tcr4_fixed1_update(X86_CR4_SMXE,       ecx, feature_bit(SMX));\n\tcr4_fixed1_update(X86_CR4_PCIDE,      ecx, feature_bit(PCID));\n\tcr4_fixed1_update(X86_CR4_OSXSAVE,    ecx, feature_bit(XSAVE));\n\n\tentry = kvm_find_cpuid_entry_index(vcpu, 0x7, 0);\n\tcr4_fixed1_update(X86_CR4_FSGSBASE,   ebx, feature_bit(FSGSBASE));\n\tcr4_fixed1_update(X86_CR4_SMEP,       ebx, feature_bit(SMEP));\n\tcr4_fixed1_update(X86_CR4_SMAP,       ebx, feature_bit(SMAP));\n\tcr4_fixed1_update(X86_CR4_PKE,        ecx, feature_bit(PKU));\n\tcr4_fixed1_update(X86_CR4_UMIP,       ecx, feature_bit(UMIP));\n\tcr4_fixed1_update(X86_CR4_LA57,       ecx, feature_bit(LA57));\n\n#undef cr4_fixed1_update\n}\n\nstatic void update_intel_pt_cfg(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct kvm_cpuid_entry2 *best = NULL;\n\tint i;\n\n\tfor (i = 0; i < PT_CPUID_LEAVES; i++) {\n\t\tbest = kvm_find_cpuid_entry_index(vcpu, 0x14, i);\n\t\tif (!best)\n\t\t\treturn;\n\t\tvmx->pt_desc.caps[CPUID_EAX + i*PT_CPUID_REGS_NUM] = best->eax;\n\t\tvmx->pt_desc.caps[CPUID_EBX + i*PT_CPUID_REGS_NUM] = best->ebx;\n\t\tvmx->pt_desc.caps[CPUID_ECX + i*PT_CPUID_REGS_NUM] = best->ecx;\n\t\tvmx->pt_desc.caps[CPUID_EDX + i*PT_CPUID_REGS_NUM] = best->edx;\n\t}\n\n\t \n\tvmx->pt_desc.num_address_ranges = intel_pt_validate_cap(vmx->pt_desc.caps,\n\t\t\t\t\t\tPT_CAP_num_address_ranges);\n\n\t \n\tvmx->pt_desc.ctl_bitmask = ~(RTIT_CTL_TRACEEN | RTIT_CTL_OS |\n\t\t\tRTIT_CTL_USR | RTIT_CTL_TSC_EN | RTIT_CTL_DISRETC |\n\t\t\tRTIT_CTL_BRANCH_EN);\n\n\t \n\tif (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_cr3_filtering))\n\t\tvmx->pt_desc.ctl_bitmask &= ~RTIT_CTL_CR3EN;\n\n\t \n\tif (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_psb_cyc))\n\t\tvmx->pt_desc.ctl_bitmask &= ~(RTIT_CTL_CYCLEACC |\n\t\t\t\tRTIT_CTL_CYC_THRESH | RTIT_CTL_PSB_FREQ);\n\n\t \n\tif (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_mtc))\n\t\tvmx->pt_desc.ctl_bitmask &= ~(RTIT_CTL_MTC_EN |\n\t\t\t\t\t      RTIT_CTL_MTC_RANGE);\n\n\t \n\tif (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_ptwrite))\n\t\tvmx->pt_desc.ctl_bitmask &= ~(RTIT_CTL_FUP_ON_PTW |\n\t\t\t\t\t\t\tRTIT_CTL_PTW_EN);\n\n\t \n\tif (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_power_event_trace))\n\t\tvmx->pt_desc.ctl_bitmask &= ~RTIT_CTL_PWR_EVT_EN;\n\n\t \n\tif (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_topa_output))\n\t\tvmx->pt_desc.ctl_bitmask &= ~RTIT_CTL_TOPA;\n\n\t \n\tif (intel_pt_validate_cap(vmx->pt_desc.caps, PT_CAP_output_subsys))\n\t\tvmx->pt_desc.ctl_bitmask &= ~RTIT_CTL_FABRIC_EN;\n\n\t \n\tfor (i = 0; i < vmx->pt_desc.num_address_ranges; i++)\n\t\tvmx->pt_desc.ctl_bitmask &= ~(0xfULL << (32 + i * 4));\n}\n\nstatic void vmx_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_XSAVE) &&\n\t    guest_cpuid_has(vcpu, X86_FEATURE_XSAVE))\n\t\tkvm_governed_feature_check_and_set(vcpu, X86_FEATURE_XSAVES);\n\n\tkvm_governed_feature_check_and_set(vcpu, X86_FEATURE_VMX);\n\n\tvmx_setup_uret_msrs(vmx);\n\n\tif (cpu_has_secondary_exec_ctrls())\n\t\tvmcs_set_secondary_exec_control(vmx,\n\t\t\t\t\t\tvmx_secondary_exec_control(vmx));\n\n\tif (guest_can_use(vcpu, X86_FEATURE_VMX))\n\t\tvmx->msr_ia32_feature_control_valid_bits |=\n\t\t\tFEAT_CTL_VMX_ENABLED_INSIDE_SMX |\n\t\t\tFEAT_CTL_VMX_ENABLED_OUTSIDE_SMX;\n\telse\n\t\tvmx->msr_ia32_feature_control_valid_bits &=\n\t\t\t~(FEAT_CTL_VMX_ENABLED_INSIDE_SMX |\n\t\t\t  FEAT_CTL_VMX_ENABLED_OUTSIDE_SMX);\n\n\tif (guest_can_use(vcpu, X86_FEATURE_VMX))\n\t\tnested_vmx_cr_fixed1_bits_update(vcpu);\n\n\tif (boot_cpu_has(X86_FEATURE_INTEL_PT) &&\n\t\t\tguest_cpuid_has(vcpu, X86_FEATURE_INTEL_PT))\n\t\tupdate_intel_pt_cfg(vcpu);\n\n\tif (boot_cpu_has(X86_FEATURE_RTM)) {\n\t\tstruct vmx_uret_msr *msr;\n\t\tmsr = vmx_find_uret_msr(vmx, MSR_IA32_TSX_CTRL);\n\t\tif (msr) {\n\t\t\tbool enabled = guest_cpuid_has(vcpu, X86_FEATURE_RTM);\n\t\t\tvmx_set_guest_uret_msr(vmx, msr, enabled ? 0 : TSX_CTRL_RTM_DISABLE);\n\t\t}\n\t}\n\n\tif (kvm_cpu_cap_has(X86_FEATURE_XFD))\n\t\tvmx_set_intercept_for_msr(vcpu, MSR_IA32_XFD_ERR, MSR_TYPE_R,\n\t\t\t\t\t  !guest_cpuid_has(vcpu, X86_FEATURE_XFD));\n\n\tif (boot_cpu_has(X86_FEATURE_IBPB))\n\t\tvmx_set_intercept_for_msr(vcpu, MSR_IA32_PRED_CMD, MSR_TYPE_W,\n\t\t\t\t\t  !guest_has_pred_cmd_msr(vcpu));\n\n\tif (boot_cpu_has(X86_FEATURE_FLUSH_L1D))\n\t\tvmx_set_intercept_for_msr(vcpu, MSR_IA32_FLUSH_CMD, MSR_TYPE_W,\n\t\t\t\t\t  !guest_cpuid_has(vcpu, X86_FEATURE_FLUSH_L1D));\n\n\tset_cr4_guest_host_mask(vmx);\n\n\tvmx_write_encls_bitmap(vcpu, NULL);\n\tif (guest_cpuid_has(vcpu, X86_FEATURE_SGX))\n\t\tvmx->msr_ia32_feature_control_valid_bits |= FEAT_CTL_SGX_ENABLED;\n\telse\n\t\tvmx->msr_ia32_feature_control_valid_bits &= ~FEAT_CTL_SGX_ENABLED;\n\n\tif (guest_cpuid_has(vcpu, X86_FEATURE_SGX_LC))\n\t\tvmx->msr_ia32_feature_control_valid_bits |=\n\t\t\tFEAT_CTL_SGX_LC_ENABLED;\n\telse\n\t\tvmx->msr_ia32_feature_control_valid_bits &=\n\t\t\t~FEAT_CTL_SGX_LC_ENABLED;\n\n\t \n\tvmx_update_exception_bitmap(vcpu);\n}\n\nstatic u64 vmx_get_perf_capabilities(void)\n{\n\tu64 perf_cap = PMU_CAP_FW_WRITES;\n\tstruct x86_pmu_lbr lbr;\n\tu64 host_perf_cap = 0;\n\n\tif (!enable_pmu)\n\t\treturn 0;\n\n\tif (boot_cpu_has(X86_FEATURE_PDCM))\n\t\trdmsrl(MSR_IA32_PERF_CAPABILITIES, host_perf_cap);\n\n\tif (!cpu_feature_enabled(X86_FEATURE_ARCH_LBR)) {\n\t\tx86_perf_get_lbr(&lbr);\n\t\tif (lbr.nr)\n\t\t\tperf_cap |= host_perf_cap & PMU_CAP_LBR_FMT;\n\t}\n\n\tif (vmx_pebs_supported()) {\n\t\tperf_cap |= host_perf_cap & PERF_CAP_PEBS_MASK;\n\t\tif ((perf_cap & PERF_CAP_PEBS_FORMAT) < 4)\n\t\t\tperf_cap &= ~PERF_CAP_PEBS_BASELINE;\n\t}\n\n\treturn perf_cap;\n}\n\nstatic __init void vmx_set_cpu_caps(void)\n{\n\tkvm_set_cpu_caps();\n\n\t \n\tif (nested)\n\t\tkvm_cpu_cap_set(X86_FEATURE_VMX);\n\n\t \n\tif (kvm_mpx_supported())\n\t\tkvm_cpu_cap_check_and_set(X86_FEATURE_MPX);\n\tif (!cpu_has_vmx_invpcid())\n\t\tkvm_cpu_cap_clear(X86_FEATURE_INVPCID);\n\tif (vmx_pt_mode_is_host_guest())\n\t\tkvm_cpu_cap_check_and_set(X86_FEATURE_INTEL_PT);\n\tif (vmx_pebs_supported()) {\n\t\tkvm_cpu_cap_check_and_set(X86_FEATURE_DS);\n\t\tkvm_cpu_cap_check_and_set(X86_FEATURE_DTES64);\n\t}\n\n\tif (!enable_pmu)\n\t\tkvm_cpu_cap_clear(X86_FEATURE_PDCM);\n\tkvm_caps.supported_perf_cap = vmx_get_perf_capabilities();\n\n\tif (!enable_sgx) {\n\t\tkvm_cpu_cap_clear(X86_FEATURE_SGX);\n\t\tkvm_cpu_cap_clear(X86_FEATURE_SGX_LC);\n\t\tkvm_cpu_cap_clear(X86_FEATURE_SGX1);\n\t\tkvm_cpu_cap_clear(X86_FEATURE_SGX2);\n\t}\n\n\tif (vmx_umip_emulated())\n\t\tkvm_cpu_cap_set(X86_FEATURE_UMIP);\n\n\t \n\tkvm_caps.supported_xss = 0;\n\tif (!cpu_has_vmx_xsaves())\n\t\tkvm_cpu_cap_clear(X86_FEATURE_XSAVES);\n\n\t \n\tif (!cpu_has_vmx_rdtscp()) {\n\t\tkvm_cpu_cap_clear(X86_FEATURE_RDTSCP);\n\t\tkvm_cpu_cap_clear(X86_FEATURE_RDPID);\n\t}\n\n\tif (cpu_has_vmx_waitpkg())\n\t\tkvm_cpu_cap_check_and_set(X86_FEATURE_WAITPKG);\n}\n\nstatic void vmx_request_immediate_exit(struct kvm_vcpu *vcpu)\n{\n\tto_vmx(vcpu)->req_immediate_exit = true;\n}\n\nstatic int vmx_check_intercept_io(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct x86_instruction_info *info)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tunsigned short port;\n\tbool intercept;\n\tint size;\n\n\tif (info->intercept == x86_intercept_in ||\n\t    info->intercept == x86_intercept_ins) {\n\t\tport = info->src_val;\n\t\tsize = info->dst_bytes;\n\t} else {\n\t\tport = info->dst_val;\n\t\tsize = info->src_bytes;\n\t}\n\n\t \n\tif (!nested_cpu_has(vmcs12, CPU_BASED_USE_IO_BITMAPS))\n\t\tintercept = nested_cpu_has(vmcs12,\n\t\t\t\t\t   CPU_BASED_UNCOND_IO_EXITING);\n\telse\n\t\tintercept = nested_vmx_check_io_bitmaps(vcpu, port, size);\n\n\t \n\treturn intercept ? X86EMUL_UNHANDLEABLE : X86EMUL_CONTINUE;\n}\n\nstatic int vmx_check_intercept(struct kvm_vcpu *vcpu,\n\t\t\t       struct x86_instruction_info *info,\n\t\t\t       enum x86_intercept_stage stage,\n\t\t\t       struct x86_exception *exception)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\tswitch (info->intercept) {\n\t \n\tcase x86_intercept_rdpid:\n\t\tif (!nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_RDTSCP)) {\n\t\t\texception->vector = UD_VECTOR;\n\t\t\texception->error_code_valid = false;\n\t\t\treturn X86EMUL_PROPAGATE_FAULT;\n\t\t}\n\t\tbreak;\n\n\tcase x86_intercept_in:\n\tcase x86_intercept_ins:\n\tcase x86_intercept_out:\n\tcase x86_intercept_outs:\n\t\treturn vmx_check_intercept_io(vcpu, info);\n\n\tcase x86_intercept_lgdt:\n\tcase x86_intercept_lidt:\n\tcase x86_intercept_lldt:\n\tcase x86_intercept_ltr:\n\tcase x86_intercept_sgdt:\n\tcase x86_intercept_sidt:\n\tcase x86_intercept_sldt:\n\tcase x86_intercept_str:\n\t\tif (!nested_cpu_has2(vmcs12, SECONDARY_EXEC_DESC))\n\t\t\treturn X86EMUL_CONTINUE;\n\n\t\t \n\t\tbreak;\n\n\tcase x86_intercept_pause:\n\t\t \n\t\tif ((info->rep_prefix != REPE_PREFIX) ||\n\t\t    !nested_cpu_has2(vmcs12, CPU_BASED_PAUSE_EXITING))\n\t\t\treturn X86EMUL_CONTINUE;\n\n\t\tbreak;\n\n\t \n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn X86EMUL_UNHANDLEABLE;\n}\n\n#ifdef CONFIG_X86_64\n \nstatic inline int u64_shl_div_u64(u64 a, unsigned int shift,\n\t\t\t\t  u64 divisor, u64 *result)\n{\n\tu64 low = a << shift, high = a >> (64 - shift);\n\n\t \n\tif (high >= divisor)\n\t\treturn 1;\n\n\t \n\tasm(\"divq %2\\n\\t\" : \"=a\" (low), \"=d\" (high) :\n\t    \"rm\" (divisor), \"0\" (low), \"1\" (high));\n\t*result = low;\n\n\treturn 0;\n}\n\nstatic int vmx_set_hv_timer(struct kvm_vcpu *vcpu, u64 guest_deadline_tsc,\n\t\t\t    bool *expired)\n{\n\tstruct vcpu_vmx *vmx;\n\tu64 tscl, guest_tscl, delta_tsc, lapic_timer_advance_cycles;\n\tstruct kvm_timer *ktimer = &vcpu->arch.apic->lapic_timer;\n\n\tvmx = to_vmx(vcpu);\n\ttscl = rdtsc();\n\tguest_tscl = kvm_read_l1_tsc(vcpu, tscl);\n\tdelta_tsc = max(guest_deadline_tsc, guest_tscl) - guest_tscl;\n\tlapic_timer_advance_cycles = nsec_to_cycles(vcpu,\n\t\t\t\t\t\t    ktimer->timer_advance_ns);\n\n\tif (delta_tsc > lapic_timer_advance_cycles)\n\t\tdelta_tsc -= lapic_timer_advance_cycles;\n\telse\n\t\tdelta_tsc = 0;\n\n\t \n\tif (vcpu->arch.l1_tsc_scaling_ratio != kvm_caps.default_tsc_scaling_ratio &&\n\t    delta_tsc && u64_shl_div_u64(delta_tsc,\n\t\t\t\tkvm_caps.tsc_scaling_ratio_frac_bits,\n\t\t\t\tvcpu->arch.l1_tsc_scaling_ratio, &delta_tsc))\n\t\treturn -ERANGE;\n\n\t \n\tif (delta_tsc >> (cpu_preemption_timer_multi + 32))\n\t\treturn -ERANGE;\n\n\tvmx->hv_deadline_tsc = tscl + delta_tsc;\n\t*expired = !delta_tsc;\n\treturn 0;\n}\n\nstatic void vmx_cancel_hv_timer(struct kvm_vcpu *vcpu)\n{\n\tto_vmx(vcpu)->hv_deadline_tsc = -1;\n}\n#endif\n\nstatic void vmx_sched_in(struct kvm_vcpu *vcpu, int cpu)\n{\n\tif (!kvm_pause_in_guest(vcpu->kvm))\n\t\tshrink_ple_window(vcpu);\n}\n\nvoid vmx_update_cpu_dirty_logging(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (WARN_ON_ONCE(!enable_pml))\n\t\treturn;\n\n\tif (is_guest_mode(vcpu)) {\n\t\tvmx->nested.update_vmcs01_cpu_dirty_logging = true;\n\t\treturn;\n\t}\n\n\t \n\tif (atomic_read(&vcpu->kvm->nr_memslots_dirty_logging))\n\t\tsecondary_exec_controls_setbit(vmx, SECONDARY_EXEC_ENABLE_PML);\n\telse\n\t\tsecondary_exec_controls_clearbit(vmx, SECONDARY_EXEC_ENABLE_PML);\n}\n\nstatic void vmx_setup_mce(struct kvm_vcpu *vcpu)\n{\n\tif (vcpu->arch.mcg_cap & MCG_LMCE_P)\n\t\tto_vmx(vcpu)->msr_ia32_feature_control_valid_bits |=\n\t\t\tFEAT_CTL_LMCE_ENABLED;\n\telse\n\t\tto_vmx(vcpu)->msr_ia32_feature_control_valid_bits &=\n\t\t\t~FEAT_CTL_LMCE_ENABLED;\n}\n\n#ifdef CONFIG_KVM_SMM\nstatic int vmx_smi_allowed(struct kvm_vcpu *vcpu, bool for_injection)\n{\n\t \n\tif (to_vmx(vcpu)->nested.nested_run_pending)\n\t\treturn -EBUSY;\n\treturn !is_smm(vcpu);\n}\n\nstatic int vmx_enter_smm(struct kvm_vcpu *vcpu, union kvm_smram *smram)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t \n\tvmx->nested.smm.guest_mode = is_guest_mode(vcpu);\n\tif (vmx->nested.smm.guest_mode)\n\t\tnested_vmx_vmexit(vcpu, -1, 0, 0);\n\n\tvmx->nested.smm.vmxon = vmx->nested.vmxon;\n\tvmx->nested.vmxon = false;\n\tvmx_clear_hlt(vcpu);\n\treturn 0;\n}\n\nstatic int vmx_leave_smm(struct kvm_vcpu *vcpu, const union kvm_smram *smram)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint ret;\n\n\tif (vmx->nested.smm.vmxon) {\n\t\tvmx->nested.vmxon = true;\n\t\tvmx->nested.smm.vmxon = false;\n\t}\n\n\tif (vmx->nested.smm.guest_mode) {\n\t\tret = nested_vmx_enter_non_root_mode(vcpu, false);\n\t\tif (ret)\n\t\t\treturn ret;\n\n\t\tvmx->nested.nested_run_pending = 1;\n\t\tvmx->nested.smm.guest_mode = false;\n\t}\n\treturn 0;\n}\n\nstatic void vmx_enable_smi_window(struct kvm_vcpu *vcpu)\n{\n\t \n}\n#endif\n\nstatic bool vmx_apic_init_signal_blocked(struct kvm_vcpu *vcpu)\n{\n\treturn to_vmx(vcpu)->nested.vmxon && !is_guest_mode(vcpu);\n}\n\nstatic void vmx_migrate_timers(struct kvm_vcpu *vcpu)\n{\n\tif (is_guest_mode(vcpu)) {\n\t\tstruct hrtimer *timer = &to_vmx(vcpu)->nested.preemption_timer;\n\n\t\tif (hrtimer_try_to_cancel(timer) == 1)\n\t\t\thrtimer_start_expires(timer, HRTIMER_MODE_ABS_PINNED);\n\t}\n}\n\nstatic void vmx_hardware_unsetup(void)\n{\n\tkvm_set_posted_intr_wakeup_handler(NULL);\n\n\tif (nested)\n\t\tnested_vmx_hardware_unsetup();\n\n\tfree_kvm_area();\n}\n\n#define VMX_REQUIRED_APICV_INHIBITS\t\t\t\\\n(\t\t\t\t\t\t\t\\\n\tBIT(APICV_INHIBIT_REASON_DISABLE)|\t\t\\\n\tBIT(APICV_INHIBIT_REASON_ABSENT) |\t\t\\\n\tBIT(APICV_INHIBIT_REASON_HYPERV) |\t\t\\\n\tBIT(APICV_INHIBIT_REASON_BLOCKIRQ) |\t\t\\\n\tBIT(APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED) |\t\\\n\tBIT(APICV_INHIBIT_REASON_APIC_ID_MODIFIED) |\t\\\n\tBIT(APICV_INHIBIT_REASON_APIC_BASE_MODIFIED)\t\\\n)\n\nstatic void vmx_vm_destroy(struct kvm *kvm)\n{\n\tstruct kvm_vmx *kvm_vmx = to_kvm_vmx(kvm);\n\n\tfree_pages((unsigned long)kvm_vmx->pid_table, vmx_get_pid_table_order(kvm));\n}\n\nstatic struct kvm_x86_ops vmx_x86_ops __initdata = {\n\t.name = KBUILD_MODNAME,\n\n\t.check_processor_compatibility = vmx_check_processor_compat,\n\n\t.hardware_unsetup = vmx_hardware_unsetup,\n\n\t.hardware_enable = vmx_hardware_enable,\n\t.hardware_disable = vmx_hardware_disable,\n\t.has_emulated_msr = vmx_has_emulated_msr,\n\n\t.vm_size = sizeof(struct kvm_vmx),\n\t.vm_init = vmx_vm_init,\n\t.vm_destroy = vmx_vm_destroy,\n\n\t.vcpu_precreate = vmx_vcpu_precreate,\n\t.vcpu_create = vmx_vcpu_create,\n\t.vcpu_free = vmx_vcpu_free,\n\t.vcpu_reset = vmx_vcpu_reset,\n\n\t.prepare_switch_to_guest = vmx_prepare_switch_to_guest,\n\t.vcpu_load = vmx_vcpu_load,\n\t.vcpu_put = vmx_vcpu_put,\n\n\t.update_exception_bitmap = vmx_update_exception_bitmap,\n\t.get_msr_feature = vmx_get_msr_feature,\n\t.get_msr = vmx_get_msr,\n\t.set_msr = vmx_set_msr,\n\t.get_segment_base = vmx_get_segment_base,\n\t.get_segment = vmx_get_segment,\n\t.set_segment = vmx_set_segment,\n\t.get_cpl = vmx_get_cpl,\n\t.get_cs_db_l_bits = vmx_get_cs_db_l_bits,\n\t.is_valid_cr0 = vmx_is_valid_cr0,\n\t.set_cr0 = vmx_set_cr0,\n\t.is_valid_cr4 = vmx_is_valid_cr4,\n\t.set_cr4 = vmx_set_cr4,\n\t.set_efer = vmx_set_efer,\n\t.get_idt = vmx_get_idt,\n\t.set_idt = vmx_set_idt,\n\t.get_gdt = vmx_get_gdt,\n\t.set_gdt = vmx_set_gdt,\n\t.set_dr7 = vmx_set_dr7,\n\t.sync_dirty_debug_regs = vmx_sync_dirty_debug_regs,\n\t.cache_reg = vmx_cache_reg,\n\t.get_rflags = vmx_get_rflags,\n\t.set_rflags = vmx_set_rflags,\n\t.get_if_flag = vmx_get_if_flag,\n\n\t.flush_tlb_all = vmx_flush_tlb_all,\n\t.flush_tlb_current = vmx_flush_tlb_current,\n\t.flush_tlb_gva = vmx_flush_tlb_gva,\n\t.flush_tlb_guest = vmx_flush_tlb_guest,\n\n\t.vcpu_pre_run = vmx_vcpu_pre_run,\n\t.vcpu_run = vmx_vcpu_run,\n\t.handle_exit = vmx_handle_exit,\n\t.skip_emulated_instruction = vmx_skip_emulated_instruction,\n\t.update_emulated_instruction = vmx_update_emulated_instruction,\n\t.set_interrupt_shadow = vmx_set_interrupt_shadow,\n\t.get_interrupt_shadow = vmx_get_interrupt_shadow,\n\t.patch_hypercall = vmx_patch_hypercall,\n\t.inject_irq = vmx_inject_irq,\n\t.inject_nmi = vmx_inject_nmi,\n\t.inject_exception = vmx_inject_exception,\n\t.cancel_injection = vmx_cancel_injection,\n\t.interrupt_allowed = vmx_interrupt_allowed,\n\t.nmi_allowed = vmx_nmi_allowed,\n\t.get_nmi_mask = vmx_get_nmi_mask,\n\t.set_nmi_mask = vmx_set_nmi_mask,\n\t.enable_nmi_window = vmx_enable_nmi_window,\n\t.enable_irq_window = vmx_enable_irq_window,\n\t.update_cr8_intercept = vmx_update_cr8_intercept,\n\t.set_virtual_apic_mode = vmx_set_virtual_apic_mode,\n\t.set_apic_access_page_addr = vmx_set_apic_access_page_addr,\n\t.refresh_apicv_exec_ctrl = vmx_refresh_apicv_exec_ctrl,\n\t.load_eoi_exitmap = vmx_load_eoi_exitmap,\n\t.apicv_pre_state_restore = vmx_apicv_pre_state_restore,\n\t.required_apicv_inhibits = VMX_REQUIRED_APICV_INHIBITS,\n\t.hwapic_irr_update = vmx_hwapic_irr_update,\n\t.hwapic_isr_update = vmx_hwapic_isr_update,\n\t.guest_apic_has_interrupt = vmx_guest_apic_has_interrupt,\n\t.sync_pir_to_irr = vmx_sync_pir_to_irr,\n\t.deliver_interrupt = vmx_deliver_interrupt,\n\t.dy_apicv_has_pending_interrupt = pi_has_pending_interrupt,\n\n\t.set_tss_addr = vmx_set_tss_addr,\n\t.set_identity_map_addr = vmx_set_identity_map_addr,\n\t.get_mt_mask = vmx_get_mt_mask,\n\n\t.get_exit_info = vmx_get_exit_info,\n\n\t.vcpu_after_set_cpuid = vmx_vcpu_after_set_cpuid,\n\n\t.has_wbinvd_exit = cpu_has_vmx_wbinvd_exit,\n\n\t.get_l2_tsc_offset = vmx_get_l2_tsc_offset,\n\t.get_l2_tsc_multiplier = vmx_get_l2_tsc_multiplier,\n\t.write_tsc_offset = vmx_write_tsc_offset,\n\t.write_tsc_multiplier = vmx_write_tsc_multiplier,\n\n\t.load_mmu_pgd = vmx_load_mmu_pgd,\n\n\t.check_intercept = vmx_check_intercept,\n\t.handle_exit_irqoff = vmx_handle_exit_irqoff,\n\n\t.request_immediate_exit = vmx_request_immediate_exit,\n\n\t.sched_in = vmx_sched_in,\n\n\t.cpu_dirty_log_size = PML_ENTITY_NUM,\n\t.update_cpu_dirty_logging = vmx_update_cpu_dirty_logging,\n\n\t.nested_ops = &vmx_nested_ops,\n\n\t.pi_update_irte = vmx_pi_update_irte,\n\t.pi_start_assignment = vmx_pi_start_assignment,\n\n#ifdef CONFIG_X86_64\n\t.set_hv_timer = vmx_set_hv_timer,\n\t.cancel_hv_timer = vmx_cancel_hv_timer,\n#endif\n\n\t.setup_mce = vmx_setup_mce,\n\n#ifdef CONFIG_KVM_SMM\n\t.smi_allowed = vmx_smi_allowed,\n\t.enter_smm = vmx_enter_smm,\n\t.leave_smm = vmx_leave_smm,\n\t.enable_smi_window = vmx_enable_smi_window,\n#endif\n\n\t.can_emulate_instruction = vmx_can_emulate_instruction,\n\t.apic_init_signal_blocked = vmx_apic_init_signal_blocked,\n\t.migrate_timers = vmx_migrate_timers,\n\n\t.msr_filter_changed = vmx_msr_filter_changed,\n\t.complete_emulated_msr = kvm_complete_insn_gp,\n\n\t.vcpu_deliver_sipi_vector = kvm_vcpu_deliver_sipi_vector,\n};\n\nstatic unsigned int vmx_handle_intel_pt_intr(void)\n{\n\tstruct kvm_vcpu *vcpu = kvm_get_running_vcpu();\n\n\t \n\tif (!vcpu || !kvm_handling_nmi_from_guest(vcpu))\n\t\treturn 0;\n\n\tkvm_make_request(KVM_REQ_PMI, vcpu);\n\t__set_bit(MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI_BIT,\n\t\t  (unsigned long *)&vcpu->arch.pmu.global_status);\n\treturn 1;\n}\n\nstatic __init void vmx_setup_user_return_msrs(void)\n{\n\n\t \n\tconst u32 vmx_uret_msrs_list[] = {\n\t#ifdef CONFIG_X86_64\n\t\tMSR_SYSCALL_MASK, MSR_LSTAR, MSR_CSTAR,\n\t#endif\n\t\tMSR_EFER, MSR_TSC_AUX, MSR_STAR,\n\t\tMSR_IA32_TSX_CTRL,\n\t};\n\tint i;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(vmx_uret_msrs_list) != MAX_NR_USER_RETURN_MSRS);\n\n\tfor (i = 0; i < ARRAY_SIZE(vmx_uret_msrs_list); ++i)\n\t\tkvm_add_user_return_msr(vmx_uret_msrs_list[i]);\n}\n\nstatic void __init vmx_setup_me_spte_mask(void)\n{\n\tu64 me_mask = 0;\n\n\t \n\tif (boot_cpu_data.x86_phys_bits != kvm_get_shadow_phys_bits())\n\t\tme_mask = rsvd_bits(boot_cpu_data.x86_phys_bits,\n\t\t\tkvm_get_shadow_phys_bits() - 1);\n\t \n\tkvm_mmu_set_me_spte_mask(0, me_mask);\n}\n\nstatic struct kvm_x86_init_ops vmx_init_ops __initdata;\n\nstatic __init int hardware_setup(void)\n{\n\tunsigned long host_bndcfgs;\n\tstruct desc_ptr dt;\n\tint r;\n\n\tstore_idt(&dt);\n\thost_idt_base = dt.address;\n\n\tvmx_setup_user_return_msrs();\n\n\tif (setup_vmcs_config(&vmcs_config, &vmx_capability) < 0)\n\t\treturn -EIO;\n\n\tif (cpu_has_perf_global_ctrl_bug())\n\t\tpr_warn_once(\"VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL \"\n\t\t\t     \"does not work properly. Using workaround\\n\");\n\n\tif (boot_cpu_has(X86_FEATURE_NX))\n\t\tkvm_enable_efer_bits(EFER_NX);\n\n\tif (boot_cpu_has(X86_FEATURE_MPX)) {\n\t\trdmsrl(MSR_IA32_BNDCFGS, host_bndcfgs);\n\t\tWARN_ONCE(host_bndcfgs, \"BNDCFGS in host will be lost\");\n\t}\n\n\tif (!cpu_has_vmx_mpx())\n\t\tkvm_caps.supported_xcr0 &= ~(XFEATURE_MASK_BNDREGS |\n\t\t\t\t\t     XFEATURE_MASK_BNDCSR);\n\n\tif (!cpu_has_vmx_vpid() || !cpu_has_vmx_invvpid() ||\n\t    !(cpu_has_vmx_invvpid_single() || cpu_has_vmx_invvpid_global()))\n\t\tenable_vpid = 0;\n\n\tif (!cpu_has_vmx_ept() ||\n\t    !cpu_has_vmx_ept_4levels() ||\n\t    !cpu_has_vmx_ept_mt_wb() ||\n\t    !cpu_has_vmx_invept_global())\n\t\tenable_ept = 0;\n\n\t \n\tif (!enable_ept && !boot_cpu_has(X86_FEATURE_NX)) {\n\t\tpr_err_ratelimited(\"NX (Execute Disable) not supported\\n\");\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!cpu_has_vmx_ept_ad_bits() || !enable_ept)\n\t\tenable_ept_ad_bits = 0;\n\n\tif (!cpu_has_vmx_unrestricted_guest() || !enable_ept)\n\t\tenable_unrestricted_guest = 0;\n\n\tif (!cpu_has_vmx_flexpriority())\n\t\tflexpriority_enabled = 0;\n\n\tif (!cpu_has_virtual_nmis())\n\t\tenable_vnmi = 0;\n\n#ifdef CONFIG_X86_SGX_KVM\n\tif (!cpu_has_vmx_encls_vmexit())\n\t\tenable_sgx = false;\n#endif\n\n\t \n\tif (!flexpriority_enabled)\n\t\tvmx_x86_ops.set_apic_access_page_addr = NULL;\n\n\tif (!cpu_has_vmx_tpr_shadow())\n\t\tvmx_x86_ops.update_cr8_intercept = NULL;\n\n#if IS_ENABLED(CONFIG_HYPERV)\n\tif (ms_hyperv.nested_features & HV_X64_NESTED_GUEST_MAPPING_FLUSH\n\t    && enable_ept) {\n\t\tvmx_x86_ops.flush_remote_tlbs = hv_flush_remote_tlbs;\n\t\tvmx_x86_ops.flush_remote_tlbs_range = hv_flush_remote_tlbs_range;\n\t}\n#endif\n\n\tif (!cpu_has_vmx_ple()) {\n\t\tple_gap = 0;\n\t\tple_window = 0;\n\t\tple_window_grow = 0;\n\t\tple_window_max = 0;\n\t\tple_window_shrink = 0;\n\t}\n\n\tif (!cpu_has_vmx_apicv())\n\t\tenable_apicv = 0;\n\tif (!enable_apicv)\n\t\tvmx_x86_ops.sync_pir_to_irr = NULL;\n\n\tif (!enable_apicv || !cpu_has_vmx_ipiv())\n\t\tenable_ipiv = false;\n\n\tif (cpu_has_vmx_tsc_scaling())\n\t\tkvm_caps.has_tsc_control = true;\n\n\tkvm_caps.max_tsc_scaling_ratio = KVM_VMX_TSC_MULTIPLIER_MAX;\n\tkvm_caps.tsc_scaling_ratio_frac_bits = 48;\n\tkvm_caps.has_bus_lock_exit = cpu_has_vmx_bus_lock_detection();\n\tkvm_caps.has_notify_vmexit = cpu_has_notify_vmexit();\n\n\tset_bit(0, vmx_vpid_bitmap);  \n\n\tif (enable_ept)\n\t\tkvm_mmu_set_ept_masks(enable_ept_ad_bits,\n\t\t\t\t      cpu_has_vmx_ept_execute_only());\n\n\t \n\tvmx_setup_me_spte_mask();\n\n\tkvm_configure_mmu(enable_ept, 0, vmx_get_max_ept_level(),\n\t\t\t  ept_caps_to_lpage_level(vmx_capability.ept));\n\n\t \n\tif (!enable_ept || !enable_ept_ad_bits || !cpu_has_vmx_pml())\n\t\tenable_pml = 0;\n\n\tif (!enable_pml)\n\t\tvmx_x86_ops.cpu_dirty_log_size = 0;\n\n\tif (!cpu_has_vmx_preemption_timer())\n\t\tenable_preemption_timer = false;\n\n\tif (enable_preemption_timer) {\n\t\tu64 use_timer_freq = 5000ULL * 1000 * 1000;\n\n\t\tcpu_preemption_timer_multi =\n\t\t\tvmcs_config.misc & VMX_MISC_PREEMPTION_TIMER_RATE_MASK;\n\n\t\tif (tsc_khz)\n\t\t\tuse_timer_freq = (u64)tsc_khz * 1000;\n\t\tuse_timer_freq >>= cpu_preemption_timer_multi;\n\n\t\t \n\t\tif (use_timer_freq > 0xffffffffu / 10)\n\t\t\tenable_preemption_timer = false;\n\t}\n\n\tif (!enable_preemption_timer) {\n\t\tvmx_x86_ops.set_hv_timer = NULL;\n\t\tvmx_x86_ops.cancel_hv_timer = NULL;\n\t\tvmx_x86_ops.request_immediate_exit = __kvm_request_immediate_exit;\n\t}\n\n\tkvm_caps.supported_mce_cap |= MCG_LMCE_P;\n\tkvm_caps.supported_mce_cap |= MCG_CMCI_P;\n\n\tif (pt_mode != PT_MODE_SYSTEM && pt_mode != PT_MODE_HOST_GUEST)\n\t\treturn -EINVAL;\n\tif (!enable_ept || !enable_pmu || !cpu_has_vmx_intel_pt())\n\t\tpt_mode = PT_MODE_SYSTEM;\n\tif (pt_mode == PT_MODE_HOST_GUEST)\n\t\tvmx_init_ops.handle_intel_pt_intr = vmx_handle_intel_pt_intr;\n\telse\n\t\tvmx_init_ops.handle_intel_pt_intr = NULL;\n\n\tsetup_default_sgx_lepubkeyhash();\n\n\tif (nested) {\n\t\tnested_vmx_setup_ctls_msrs(&vmcs_config, vmx_capability.ept);\n\n\t\tr = nested_vmx_hardware_setup(kvm_vmx_exit_handlers);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tvmx_set_cpu_caps();\n\n\tr = alloc_kvm_area();\n\tif (r && nested)\n\t\tnested_vmx_hardware_unsetup();\n\n\tkvm_set_posted_intr_wakeup_handler(pi_wakeup_handler);\n\n\treturn r;\n}\n\nstatic struct kvm_x86_init_ops vmx_init_ops __initdata = {\n\t.hardware_setup = hardware_setup,\n\t.handle_intel_pt_intr = NULL,\n\n\t.runtime_ops = &vmx_x86_ops,\n\t.pmu_ops = &intel_pmu_ops,\n};\n\nstatic void vmx_cleanup_l1d_flush(void)\n{\n\tif (vmx_l1d_flush_pages) {\n\t\tfree_pages((unsigned long)vmx_l1d_flush_pages, L1D_CACHE_ORDER);\n\t\tvmx_l1d_flush_pages = NULL;\n\t}\n\t \n\tl1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;\n}\n\nstatic void __vmx_exit(void)\n{\n\tallow_smaller_maxphyaddr = false;\n\n\tcpu_emergency_unregister_virt_callback(vmx_emergency_disable);\n\n\tvmx_cleanup_l1d_flush();\n}\n\nstatic void vmx_exit(void)\n{\n\tkvm_exit();\n\tkvm_x86_vendor_exit();\n\n\t__vmx_exit();\n}\nmodule_exit(vmx_exit);\n\nstatic int __init vmx_init(void)\n{\n\tint r, cpu;\n\n\tif (!kvm_is_vmx_supported())\n\t\treturn -EOPNOTSUPP;\n\n\t \n\thv_init_evmcs();\n\n\tr = kvm_x86_vendor_init(&vmx_init_ops);\n\tif (r)\n\t\treturn r;\n\n\t \n\tr = vmx_setup_l1d_flush(vmentry_l1d_flush_param);\n\tif (r)\n\t\tgoto err_l1d_flush;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tINIT_LIST_HEAD(&per_cpu(loaded_vmcss_on_cpu, cpu));\n\n\t\tpi_init_cpu(cpu);\n\t}\n\n\tcpu_emergency_register_virt_callback(vmx_emergency_disable);\n\n\tvmx_check_vmcs12_offsets();\n\n\t \n\tif (!enable_ept)\n\t\tallow_smaller_maxphyaddr = true;\n\n\t \n\tr = kvm_init(sizeof(struct vcpu_vmx), __alignof__(struct vcpu_vmx),\n\t\t     THIS_MODULE);\n\tif (r)\n\t\tgoto err_kvm_init;\n\n\treturn 0;\n\nerr_kvm_init:\n\t__vmx_exit();\nerr_l1d_flush:\n\tkvm_x86_vendor_exit();\n\treturn r;\n}\nmodule_init(vmx_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}