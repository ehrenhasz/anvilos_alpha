{
  "module_name": "pmu_intel.c",
  "hash_id": "15fce2b305157f63112e218b182be1e99486b6a74be463cf90425825b3be0b28",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/vmx/pmu_intel.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/types.h>\n#include <linux/kvm_host.h>\n#include <linux/perf_event.h>\n#include <asm/perf_event.h>\n#include \"x86.h\"\n#include \"cpuid.h\"\n#include \"lapic.h\"\n#include \"nested.h\"\n#include \"pmu.h\"\n\n#define MSR_PMC_FULL_WIDTH_BIT      (MSR_IA32_PMC0 - MSR_IA32_PERFCTR0)\n\nenum intel_pmu_architectural_events {\n\t \n\tINTEL_ARCH_CPU_CYCLES,\n\tINTEL_ARCH_INSTRUCTIONS_RETIRED,\n\tINTEL_ARCH_REFERENCE_CYCLES,\n\tINTEL_ARCH_LLC_REFERENCES,\n\tINTEL_ARCH_LLC_MISSES,\n\tINTEL_ARCH_BRANCHES_RETIRED,\n\tINTEL_ARCH_BRANCHES_MISPREDICTED,\n\n\tNR_REAL_INTEL_ARCH_EVENTS,\n\n\t \n\tPSEUDO_ARCH_REFERENCE_CYCLES = NR_REAL_INTEL_ARCH_EVENTS,\n\tNR_INTEL_ARCH_EVENTS,\n};\n\nstatic struct {\n\tu8 eventsel;\n\tu8 unit_mask;\n} const intel_arch_events[] = {\n\t[INTEL_ARCH_CPU_CYCLES]\t\t\t= { 0x3c, 0x00 },\n\t[INTEL_ARCH_INSTRUCTIONS_RETIRED]\t= { 0xc0, 0x00 },\n\t[INTEL_ARCH_REFERENCE_CYCLES]\t\t= { 0x3c, 0x01 },\n\t[INTEL_ARCH_LLC_REFERENCES]\t\t= { 0x2e, 0x4f },\n\t[INTEL_ARCH_LLC_MISSES]\t\t\t= { 0x2e, 0x41 },\n\t[INTEL_ARCH_BRANCHES_RETIRED]\t\t= { 0xc4, 0x00 },\n\t[INTEL_ARCH_BRANCHES_MISPREDICTED]\t= { 0xc5, 0x00 },\n\t[PSEUDO_ARCH_REFERENCE_CYCLES]\t\t= { 0x00, 0x03 },\n};\n\n \nstatic int fixed_pmc_events[] = {\n\t[0] = INTEL_ARCH_INSTRUCTIONS_RETIRED,\n\t[1] = INTEL_ARCH_CPU_CYCLES,\n\t[2] = PSEUDO_ARCH_REFERENCE_CYCLES,\n};\n\nstatic void reprogram_fixed_counters(struct kvm_pmu *pmu, u64 data)\n{\n\tstruct kvm_pmc *pmc;\n\tu8 old_fixed_ctr_ctrl = pmu->fixed_ctr_ctrl;\n\tint i;\n\n\tpmu->fixed_ctr_ctrl = data;\n\tfor (i = 0; i < pmu->nr_arch_fixed_counters; i++) {\n\t\tu8 new_ctrl = fixed_ctrl_field(data, i);\n\t\tu8 old_ctrl = fixed_ctrl_field(old_fixed_ctr_ctrl, i);\n\n\t\tif (old_ctrl == new_ctrl)\n\t\t\tcontinue;\n\n\t\tpmc = get_fixed_pmc(pmu, MSR_CORE_PERF_FIXED_CTR0 + i);\n\n\t\t__set_bit(INTEL_PMC_IDX_FIXED + i, pmu->pmc_in_use);\n\t\tkvm_pmu_request_counter_reprogram(pmc);\n\t}\n}\n\nstatic struct kvm_pmc *intel_pmc_idx_to_pmc(struct kvm_pmu *pmu, int pmc_idx)\n{\n\tif (pmc_idx < INTEL_PMC_IDX_FIXED) {\n\t\treturn get_gp_pmc(pmu, MSR_P6_EVNTSEL0 + pmc_idx,\n\t\t\t\t  MSR_P6_EVNTSEL0);\n\t} else {\n\t\tu32 idx = pmc_idx - INTEL_PMC_IDX_FIXED;\n\n\t\treturn get_fixed_pmc(pmu, idx + MSR_CORE_PERF_FIXED_CTR0);\n\t}\n}\n\nstatic bool intel_hw_event_available(struct kvm_pmc *pmc)\n{\n\tstruct kvm_pmu *pmu = pmc_to_pmu(pmc);\n\tu8 event_select = pmc->eventsel & ARCH_PERFMON_EVENTSEL_EVENT;\n\tu8 unit_mask = (pmc->eventsel & ARCH_PERFMON_EVENTSEL_UMASK) >> 8;\n\tint i;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(intel_arch_events) != NR_INTEL_ARCH_EVENTS);\n\n\t \n\tfor (i = 0; i < NR_REAL_INTEL_ARCH_EVENTS; i++) {\n\t\tif (intel_arch_events[i].eventsel != event_select ||\n\t\t    intel_arch_events[i].unit_mask != unit_mask)\n\t\t\tcontinue;\n\n\t\treturn pmu->available_event_types & BIT(i);\n\t}\n\n\treturn true;\n}\n\nstatic bool intel_is_valid_rdpmc_ecx(struct kvm_vcpu *vcpu, unsigned int idx)\n{\n\tstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\n\tbool fixed = idx & (1u << 30);\n\n\tidx &= ~(3u << 30);\n\n\treturn fixed ? idx < pmu->nr_arch_fixed_counters\n\t\t     : idx < pmu->nr_arch_gp_counters;\n}\n\nstatic struct kvm_pmc *intel_rdpmc_ecx_to_pmc(struct kvm_vcpu *vcpu,\n\t\t\t\t\t    unsigned int idx, u64 *mask)\n{\n\tstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\n\tbool fixed = idx & (1u << 30);\n\tstruct kvm_pmc *counters;\n\tunsigned int num_counters;\n\n\tidx &= ~(3u << 30);\n\tif (fixed) {\n\t\tcounters = pmu->fixed_counters;\n\t\tnum_counters = pmu->nr_arch_fixed_counters;\n\t} else {\n\t\tcounters = pmu->gp_counters;\n\t\tnum_counters = pmu->nr_arch_gp_counters;\n\t}\n\tif (idx >= num_counters)\n\t\treturn NULL;\n\t*mask &= pmu->counter_bitmask[fixed ? KVM_PMC_FIXED : KVM_PMC_GP];\n\treturn &counters[array_index_nospec(idx, num_counters)];\n}\n\nstatic inline u64 vcpu_get_perf_capabilities(struct kvm_vcpu *vcpu)\n{\n\tif (!guest_cpuid_has(vcpu, X86_FEATURE_PDCM))\n\t\treturn 0;\n\n\treturn vcpu->arch.perf_capabilities;\n}\n\nstatic inline bool fw_writes_is_enabled(struct kvm_vcpu *vcpu)\n{\n\treturn (vcpu_get_perf_capabilities(vcpu) & PMU_CAP_FW_WRITES) != 0;\n}\n\nstatic inline struct kvm_pmc *get_fw_gp_pmc(struct kvm_pmu *pmu, u32 msr)\n{\n\tif (!fw_writes_is_enabled(pmu_to_vcpu(pmu)))\n\t\treturn NULL;\n\n\treturn get_gp_pmc(pmu, msr, MSR_IA32_PMC0);\n}\n\nstatic bool intel_pmu_is_valid_lbr_msr(struct kvm_vcpu *vcpu, u32 index)\n{\n\tstruct x86_pmu_lbr *records = vcpu_to_lbr_records(vcpu);\n\tbool ret = false;\n\n\tif (!intel_pmu_lbr_is_enabled(vcpu))\n\t\treturn ret;\n\n\tret = (index == MSR_LBR_SELECT) || (index == MSR_LBR_TOS) ||\n\t\t(index >= records->from && index < records->from + records->nr) ||\n\t\t(index >= records->to && index < records->to + records->nr);\n\n\tif (!ret && records->info)\n\t\tret = (index >= records->info && index < records->info + records->nr);\n\n\treturn ret;\n}\n\nstatic bool intel_is_valid_msr(struct kvm_vcpu *vcpu, u32 msr)\n{\n\tstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\n\tu64 perf_capabilities;\n\tint ret;\n\n\tswitch (msr) {\n\tcase MSR_CORE_PERF_FIXED_CTR_CTRL:\n\t\treturn kvm_pmu_has_perf_global_ctrl(pmu);\n\tcase MSR_IA32_PEBS_ENABLE:\n\t\tret = vcpu_get_perf_capabilities(vcpu) & PERF_CAP_PEBS_FORMAT;\n\t\tbreak;\n\tcase MSR_IA32_DS_AREA:\n\t\tret = guest_cpuid_has(vcpu, X86_FEATURE_DS);\n\t\tbreak;\n\tcase MSR_PEBS_DATA_CFG:\n\t\tperf_capabilities = vcpu_get_perf_capabilities(vcpu);\n\t\tret = (perf_capabilities & PERF_CAP_PEBS_BASELINE) &&\n\t\t\t((perf_capabilities & PERF_CAP_PEBS_FORMAT) > 3);\n\t\tbreak;\n\tdefault:\n\t\tret = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0) ||\n\t\t\tget_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0) ||\n\t\t\tget_fixed_pmc(pmu, msr) || get_fw_gp_pmc(pmu, msr) ||\n\t\t\tintel_pmu_is_valid_lbr_msr(vcpu, msr);\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic struct kvm_pmc *intel_msr_idx_to_pmc(struct kvm_vcpu *vcpu, u32 msr)\n{\n\tstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\n\tstruct kvm_pmc *pmc;\n\n\tpmc = get_fixed_pmc(pmu, msr);\n\tpmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0);\n\tpmc = pmc ? pmc : get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0);\n\n\treturn pmc;\n}\n\nstatic inline void intel_pmu_release_guest_lbr_event(struct kvm_vcpu *vcpu)\n{\n\tstruct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);\n\n\tif (lbr_desc->event) {\n\t\tperf_event_release_kernel(lbr_desc->event);\n\t\tlbr_desc->event = NULL;\n\t\tvcpu_to_pmu(vcpu)->event_count--;\n\t}\n}\n\nint intel_pmu_create_guest_lbr_event(struct kvm_vcpu *vcpu)\n{\n\tstruct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);\n\tstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\n\tstruct perf_event *event;\n\n\t \n\tstruct perf_event_attr attr = {\n\t\t.type = PERF_TYPE_RAW,\n\t\t.size = sizeof(attr),\n\t\t.config = INTEL_FIXED_VLBR_EVENT,\n\t\t.sample_type = PERF_SAMPLE_BRANCH_STACK,\n\t\t.pinned = true,\n\t\t.exclude_host = true,\n\t\t.branch_sample_type = PERF_SAMPLE_BRANCH_CALL_STACK |\n\t\t\t\t\tPERF_SAMPLE_BRANCH_USER,\n\t};\n\n\tif (unlikely(lbr_desc->event)) {\n\t\t__set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);\n\t\treturn 0;\n\t}\n\n\tevent = perf_event_create_kernel_counter(&attr, -1,\n\t\t\t\t\t\tcurrent, NULL, NULL);\n\tif (IS_ERR(event)) {\n\t\tpr_debug_ratelimited(\"%s: failed %ld\\n\",\n\t\t\t\t\t__func__, PTR_ERR(event));\n\t\treturn PTR_ERR(event);\n\t}\n\tlbr_desc->event = event;\n\tpmu->event_count++;\n\t__set_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);\n\treturn 0;\n}\n\n \nstatic bool intel_pmu_handle_lbr_msrs_access(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct msr_data *msr_info, bool read)\n{\n\tstruct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);\n\tu32 index = msr_info->index;\n\n\tif (!intel_pmu_is_valid_lbr_msr(vcpu, index))\n\t\treturn false;\n\n\tif (!lbr_desc->event && intel_pmu_create_guest_lbr_event(vcpu) < 0)\n\t\tgoto dummy;\n\n\t \n\tlocal_irq_disable();\n\tif (lbr_desc->event->state == PERF_EVENT_STATE_ACTIVE) {\n\t\tif (read)\n\t\t\trdmsrl(index, msr_info->data);\n\t\telse\n\t\t\twrmsrl(index, msr_info->data);\n\t\t__set_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);\n\t\tlocal_irq_enable();\n\t\treturn true;\n\t}\n\tclear_bit(INTEL_PMC_IDX_FIXED_VLBR, vcpu_to_pmu(vcpu)->pmc_in_use);\n\tlocal_irq_enable();\n\ndummy:\n\tif (read)\n\t\tmsr_info->data = 0;\n\treturn true;\n}\n\nstatic int intel_pmu_get_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\n\tstruct kvm_pmc *pmc;\n\tu32 msr = msr_info->index;\n\n\tswitch (msr) {\n\tcase MSR_CORE_PERF_FIXED_CTR_CTRL:\n\t\tmsr_info->data = pmu->fixed_ctr_ctrl;\n\t\tbreak;\n\tcase MSR_IA32_PEBS_ENABLE:\n\t\tmsr_info->data = pmu->pebs_enable;\n\t\tbreak;\n\tcase MSR_IA32_DS_AREA:\n\t\tmsr_info->data = pmu->ds_area;\n\t\tbreak;\n\tcase MSR_PEBS_DATA_CFG:\n\t\tmsr_info->data = pmu->pebs_data_cfg;\n\t\tbreak;\n\tdefault:\n\t\tif ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||\n\t\t    (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {\n\t\t\tu64 val = pmc_read_counter(pmc);\n\t\t\tmsr_info->data =\n\t\t\t\tval & pmu->counter_bitmask[KVM_PMC_GP];\n\t\t\tbreak;\n\t\t} else if ((pmc = get_fixed_pmc(pmu, msr))) {\n\t\t\tu64 val = pmc_read_counter(pmc);\n\t\t\tmsr_info->data =\n\t\t\t\tval & pmu->counter_bitmask[KVM_PMC_FIXED];\n\t\t\tbreak;\n\t\t} else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {\n\t\t\tmsr_info->data = pmc->eventsel;\n\t\t\tbreak;\n\t\t} else if (intel_pmu_handle_lbr_msrs_access(vcpu, msr_info, true)) {\n\t\t\tbreak;\n\t\t}\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int intel_pmu_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)\n{\n\tstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\n\tstruct kvm_pmc *pmc;\n\tu32 msr = msr_info->index;\n\tu64 data = msr_info->data;\n\tu64 reserved_bits, diff;\n\n\tswitch (msr) {\n\tcase MSR_CORE_PERF_FIXED_CTR_CTRL:\n\t\tif (data & pmu->fixed_ctr_ctrl_mask)\n\t\t\treturn 1;\n\n\t\tif (pmu->fixed_ctr_ctrl != data)\n\t\t\treprogram_fixed_counters(pmu, data);\n\t\tbreak;\n\tcase MSR_IA32_PEBS_ENABLE:\n\t\tif (data & pmu->pebs_enable_mask)\n\t\t\treturn 1;\n\n\t\tif (pmu->pebs_enable != data) {\n\t\t\tdiff = pmu->pebs_enable ^ data;\n\t\t\tpmu->pebs_enable = data;\n\t\t\treprogram_counters(pmu, diff);\n\t\t}\n\t\tbreak;\n\tcase MSR_IA32_DS_AREA:\n\t\tif (is_noncanonical_address(data, vcpu))\n\t\t\treturn 1;\n\n\t\tpmu->ds_area = data;\n\t\tbreak;\n\tcase MSR_PEBS_DATA_CFG:\n\t\tif (data & pmu->pebs_data_cfg_mask)\n\t\t\treturn 1;\n\n\t\tpmu->pebs_data_cfg = data;\n\t\tbreak;\n\tdefault:\n\t\tif ((pmc = get_gp_pmc(pmu, msr, MSR_IA32_PERFCTR0)) ||\n\t\t    (pmc = get_gp_pmc(pmu, msr, MSR_IA32_PMC0))) {\n\t\t\tif ((msr & MSR_PMC_FULL_WIDTH_BIT) &&\n\t\t\t    (data & ~pmu->counter_bitmask[KVM_PMC_GP]))\n\t\t\t\treturn 1;\n\n\t\t\tif (!msr_info->host_initiated &&\n\t\t\t    !(msr & MSR_PMC_FULL_WIDTH_BIT))\n\t\t\t\tdata = (s64)(s32)data;\n\t\t\tpmc_write_counter(pmc, data);\n\t\t\tpmc_update_sample_period(pmc);\n\t\t\tbreak;\n\t\t} else if ((pmc = get_fixed_pmc(pmu, msr))) {\n\t\t\tpmc_write_counter(pmc, data);\n\t\t\tpmc_update_sample_period(pmc);\n\t\t\tbreak;\n\t\t} else if ((pmc = get_gp_pmc(pmu, msr, MSR_P6_EVNTSEL0))) {\n\t\t\treserved_bits = pmu->reserved_bits;\n\t\t\tif ((pmc->idx == 2) &&\n\t\t\t    (pmu->raw_event_mask & HSW_IN_TX_CHECKPOINTED))\n\t\t\t\treserved_bits ^= HSW_IN_TX_CHECKPOINTED;\n\t\t\tif (data & reserved_bits)\n\t\t\t\treturn 1;\n\n\t\t\tif (data != pmc->eventsel) {\n\t\t\t\tpmc->eventsel = data;\n\t\t\t\tkvm_pmu_request_counter_reprogram(pmc);\n\t\t\t}\n\t\t\tbreak;\n\t\t} else if (intel_pmu_handle_lbr_msrs_access(vcpu, msr_info, false)) {\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic void setup_fixed_pmc_eventsel(struct kvm_pmu *pmu)\n{\n\tint i;\n\n\tBUILD_BUG_ON(ARRAY_SIZE(fixed_pmc_events) != KVM_PMC_MAX_FIXED);\n\n\tfor (i = 0; i < pmu->nr_arch_fixed_counters; i++) {\n\t\tint index = array_index_nospec(i, KVM_PMC_MAX_FIXED);\n\t\tstruct kvm_pmc *pmc = &pmu->fixed_counters[index];\n\t\tu32 event = fixed_pmc_events[index];\n\n\t\tpmc->eventsel = (intel_arch_events[event].unit_mask << 8) |\n\t\t\t\t intel_arch_events[event].eventsel;\n\t}\n}\n\nstatic void intel_pmu_refresh(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\n\tstruct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);\n\tstruct kvm_cpuid_entry2 *entry;\n\tunion cpuid10_eax eax;\n\tunion cpuid10_edx edx;\n\tu64 perf_capabilities;\n\tu64 counter_mask;\n\tint i;\n\n\tpmu->nr_arch_gp_counters = 0;\n\tpmu->nr_arch_fixed_counters = 0;\n\tpmu->counter_bitmask[KVM_PMC_GP] = 0;\n\tpmu->counter_bitmask[KVM_PMC_FIXED] = 0;\n\tpmu->version = 0;\n\tpmu->reserved_bits = 0xffffffff00200000ull;\n\tpmu->raw_event_mask = X86_RAW_EVENT_MASK;\n\tpmu->global_ctrl_mask = ~0ull;\n\tpmu->global_status_mask = ~0ull;\n\tpmu->fixed_ctr_ctrl_mask = ~0ull;\n\tpmu->pebs_enable_mask = ~0ull;\n\tpmu->pebs_data_cfg_mask = ~0ull;\n\n\tmemset(&lbr_desc->records, 0, sizeof(lbr_desc->records));\n\n\t \n\tif (KVM_BUG_ON(lbr_desc->msr_passthrough, vcpu->kvm))\n\t\treturn;\n\n\tentry = kvm_find_cpuid_entry(vcpu, 0xa);\n\tif (!entry || !vcpu->kvm->arch.enable_pmu)\n\t\treturn;\n\teax.full = entry->eax;\n\tedx.full = entry->edx;\n\n\tpmu->version = eax.split.version_id;\n\tif (!pmu->version)\n\t\treturn;\n\n\tpmu->nr_arch_gp_counters = min_t(int, eax.split.num_counters,\n\t\t\t\t\t kvm_pmu_cap.num_counters_gp);\n\teax.split.bit_width = min_t(int, eax.split.bit_width,\n\t\t\t\t    kvm_pmu_cap.bit_width_gp);\n\tpmu->counter_bitmask[KVM_PMC_GP] = ((u64)1 << eax.split.bit_width) - 1;\n\teax.split.mask_length = min_t(int, eax.split.mask_length,\n\t\t\t\t      kvm_pmu_cap.events_mask_len);\n\tpmu->available_event_types = ~entry->ebx &\n\t\t\t\t\t((1ull << eax.split.mask_length) - 1);\n\n\tif (pmu->version == 1) {\n\t\tpmu->nr_arch_fixed_counters = 0;\n\t} else {\n\t\tpmu->nr_arch_fixed_counters = min_t(int, edx.split.num_counters_fixed,\n\t\t\t\t\t\t    kvm_pmu_cap.num_counters_fixed);\n\t\tedx.split.bit_width_fixed = min_t(int, edx.split.bit_width_fixed,\n\t\t\t\t\t\t  kvm_pmu_cap.bit_width_fixed);\n\t\tpmu->counter_bitmask[KVM_PMC_FIXED] =\n\t\t\t((u64)1 << edx.split.bit_width_fixed) - 1;\n\t\tsetup_fixed_pmc_eventsel(pmu);\n\t}\n\n\tfor (i = 0; i < pmu->nr_arch_fixed_counters; i++)\n\t\tpmu->fixed_ctr_ctrl_mask &= ~(0xbull << (i * 4));\n\tcounter_mask = ~(((1ull << pmu->nr_arch_gp_counters) - 1) |\n\t\t(((1ull << pmu->nr_arch_fixed_counters) - 1) << INTEL_PMC_IDX_FIXED));\n\tpmu->global_ctrl_mask = counter_mask;\n\n\t \n\tpmu->global_status_mask = pmu->global_ctrl_mask\n\t\t\t& ~(MSR_CORE_PERF_GLOBAL_OVF_CTRL_OVF_BUF |\n\t\t\t    MSR_CORE_PERF_GLOBAL_OVF_CTRL_COND_CHGD);\n\tif (vmx_pt_mode_is_host_guest())\n\t\tpmu->global_status_mask &=\n\t\t\t\t~MSR_CORE_PERF_GLOBAL_OVF_CTRL_TRACE_TOPA_PMI;\n\n\tentry = kvm_find_cpuid_entry_index(vcpu, 7, 0);\n\tif (entry &&\n\t    (boot_cpu_has(X86_FEATURE_HLE) || boot_cpu_has(X86_FEATURE_RTM)) &&\n\t    (entry->ebx & (X86_FEATURE_HLE|X86_FEATURE_RTM))) {\n\t\tpmu->reserved_bits ^= HSW_IN_TX;\n\t\tpmu->raw_event_mask |= (HSW_IN_TX|HSW_IN_TX_CHECKPOINTED);\n\t}\n\n\tbitmap_set(pmu->all_valid_pmc_idx,\n\t\t0, pmu->nr_arch_gp_counters);\n\tbitmap_set(pmu->all_valid_pmc_idx,\n\t\tINTEL_PMC_MAX_GENERIC, pmu->nr_arch_fixed_counters);\n\n\tperf_capabilities = vcpu_get_perf_capabilities(vcpu);\n\tif (cpuid_model_is_consistent(vcpu) &&\n\t    (perf_capabilities & PMU_CAP_LBR_FMT))\n\t\tx86_perf_get_lbr(&lbr_desc->records);\n\telse\n\t\tlbr_desc->records.nr = 0;\n\n\tif (lbr_desc->records.nr)\n\t\tbitmap_set(pmu->all_valid_pmc_idx, INTEL_PMC_IDX_FIXED_VLBR, 1);\n\n\tif (perf_capabilities & PERF_CAP_PEBS_FORMAT) {\n\t\tif (perf_capabilities & PERF_CAP_PEBS_BASELINE) {\n\t\t\tpmu->pebs_enable_mask = counter_mask;\n\t\t\tpmu->reserved_bits &= ~ICL_EVENTSEL_ADAPTIVE;\n\t\t\tfor (i = 0; i < pmu->nr_arch_fixed_counters; i++) {\n\t\t\t\tpmu->fixed_ctr_ctrl_mask &=\n\t\t\t\t\t~(1ULL << (INTEL_PMC_IDX_FIXED + i * 4));\n\t\t\t}\n\t\t\tpmu->pebs_data_cfg_mask = ~0xff00000full;\n\t\t} else {\n\t\t\tpmu->pebs_enable_mask =\n\t\t\t\t~((1ull << pmu->nr_arch_gp_counters) - 1);\n\t\t}\n\t}\n}\n\nstatic void intel_pmu_init(struct kvm_vcpu *vcpu)\n{\n\tint i;\n\tstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\n\tstruct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);\n\n\tfor (i = 0; i < KVM_INTEL_PMC_MAX_GENERIC; i++) {\n\t\tpmu->gp_counters[i].type = KVM_PMC_GP;\n\t\tpmu->gp_counters[i].vcpu = vcpu;\n\t\tpmu->gp_counters[i].idx = i;\n\t\tpmu->gp_counters[i].current_config = 0;\n\t}\n\n\tfor (i = 0; i < KVM_PMC_MAX_FIXED; i++) {\n\t\tpmu->fixed_counters[i].type = KVM_PMC_FIXED;\n\t\tpmu->fixed_counters[i].vcpu = vcpu;\n\t\tpmu->fixed_counters[i].idx = i + INTEL_PMC_IDX_FIXED;\n\t\tpmu->fixed_counters[i].current_config = 0;\n\t}\n\n\tlbr_desc->records.nr = 0;\n\tlbr_desc->event = NULL;\n\tlbr_desc->msr_passthrough = false;\n}\n\nstatic void intel_pmu_reset(struct kvm_vcpu *vcpu)\n{\n\tintel_pmu_release_guest_lbr_event(vcpu);\n}\n\n \nstatic void intel_pmu_legacy_freezing_lbrs_on_pmi(struct kvm_vcpu *vcpu)\n{\n\tu64 data = vmcs_read64(GUEST_IA32_DEBUGCTL);\n\n\tif (data & DEBUGCTLMSR_FREEZE_LBRS_ON_PMI) {\n\t\tdata &= ~DEBUGCTLMSR_LBR;\n\t\tvmcs_write64(GUEST_IA32_DEBUGCTL, data);\n\t}\n}\n\nstatic void intel_pmu_deliver_pmi(struct kvm_vcpu *vcpu)\n{\n\tu8 version = vcpu_to_pmu(vcpu)->version;\n\n\tif (!intel_pmu_lbr_is_enabled(vcpu))\n\t\treturn;\n\n\tif (version > 1 && version < 4)\n\t\tintel_pmu_legacy_freezing_lbrs_on_pmi(vcpu);\n}\n\nstatic void vmx_update_intercept_for_lbr_msrs(struct kvm_vcpu *vcpu, bool set)\n{\n\tstruct x86_pmu_lbr *lbr = vcpu_to_lbr_records(vcpu);\n\tint i;\n\n\tfor (i = 0; i < lbr->nr; i++) {\n\t\tvmx_set_intercept_for_msr(vcpu, lbr->from + i, MSR_TYPE_RW, set);\n\t\tvmx_set_intercept_for_msr(vcpu, lbr->to + i, MSR_TYPE_RW, set);\n\t\tif (lbr->info)\n\t\t\tvmx_set_intercept_for_msr(vcpu, lbr->info + i, MSR_TYPE_RW, set);\n\t}\n\n\tvmx_set_intercept_for_msr(vcpu, MSR_LBR_SELECT, MSR_TYPE_RW, set);\n\tvmx_set_intercept_for_msr(vcpu, MSR_LBR_TOS, MSR_TYPE_RW, set);\n}\n\nstatic inline void vmx_disable_lbr_msrs_passthrough(struct kvm_vcpu *vcpu)\n{\n\tstruct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);\n\n\tif (!lbr_desc->msr_passthrough)\n\t\treturn;\n\n\tvmx_update_intercept_for_lbr_msrs(vcpu, true);\n\tlbr_desc->msr_passthrough = false;\n}\n\nstatic inline void vmx_enable_lbr_msrs_passthrough(struct kvm_vcpu *vcpu)\n{\n\tstruct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);\n\n\tif (lbr_desc->msr_passthrough)\n\t\treturn;\n\n\tvmx_update_intercept_for_lbr_msrs(vcpu, false);\n\tlbr_desc->msr_passthrough = true;\n}\n\n \nvoid vmx_passthrough_lbr_msrs(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_pmu *pmu = vcpu_to_pmu(vcpu);\n\tstruct lbr_desc *lbr_desc = vcpu_to_lbr_desc(vcpu);\n\n\tif (!lbr_desc->event) {\n\t\tvmx_disable_lbr_msrs_passthrough(vcpu);\n\t\tif (vmcs_read64(GUEST_IA32_DEBUGCTL) & DEBUGCTLMSR_LBR)\n\t\t\tgoto warn;\n\t\tif (test_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use))\n\t\t\tgoto warn;\n\t\treturn;\n\t}\n\n\tif (lbr_desc->event->state < PERF_EVENT_STATE_ACTIVE) {\n\t\tvmx_disable_lbr_msrs_passthrough(vcpu);\n\t\t__clear_bit(INTEL_PMC_IDX_FIXED_VLBR, pmu->pmc_in_use);\n\t\tgoto warn;\n\t} else\n\t\tvmx_enable_lbr_msrs_passthrough(vcpu);\n\n\treturn;\n\nwarn:\n\tpr_warn_ratelimited(\"vcpu-%d: fail to passthrough LBR.\\n\", vcpu->vcpu_id);\n}\n\nstatic void intel_pmu_cleanup(struct kvm_vcpu *vcpu)\n{\n\tif (!(vmcs_read64(GUEST_IA32_DEBUGCTL) & DEBUGCTLMSR_LBR))\n\t\tintel_pmu_release_guest_lbr_event(vcpu);\n}\n\nvoid intel_pmu_cross_mapped_check(struct kvm_pmu *pmu)\n{\n\tstruct kvm_pmc *pmc = NULL;\n\tint bit, hw_idx;\n\n\tfor_each_set_bit(bit, (unsigned long *)&pmu->global_ctrl,\n\t\t\t X86_PMC_IDX_MAX) {\n\t\tpmc = intel_pmc_idx_to_pmc(pmu, bit);\n\n\t\tif (!pmc || !pmc_speculative_in_use(pmc) ||\n\t\t    !pmc_is_globally_enabled(pmc) || !pmc->perf_event)\n\t\t\tcontinue;\n\n\t\t \n\t\thw_idx = pmc->perf_event->hw.idx;\n\t\tif (hw_idx != pmc->idx && hw_idx > -1)\n\t\t\tpmu->host_cross_mapped_mask |= BIT_ULL(hw_idx);\n\t}\n}\n\nstruct kvm_pmu_ops intel_pmu_ops __initdata = {\n\t.hw_event_available = intel_hw_event_available,\n\t.pmc_idx_to_pmc = intel_pmc_idx_to_pmc,\n\t.rdpmc_ecx_to_pmc = intel_rdpmc_ecx_to_pmc,\n\t.msr_idx_to_pmc = intel_msr_idx_to_pmc,\n\t.is_valid_rdpmc_ecx = intel_is_valid_rdpmc_ecx,\n\t.is_valid_msr = intel_is_valid_msr,\n\t.get_msr = intel_pmu_get_msr,\n\t.set_msr = intel_pmu_set_msr,\n\t.refresh = intel_pmu_refresh,\n\t.init = intel_pmu_init,\n\t.reset = intel_pmu_reset,\n\t.deliver_pmi = intel_pmu_deliver_pmi,\n\t.cleanup = intel_pmu_cleanup,\n\t.EVENTSEL_EVENT = ARCH_PERFMON_EVENTSEL_EVENT,\n\t.MAX_NR_GP_COUNTERS = KVM_INTEL_PMC_MAX_GENERIC,\n\t.MIN_NR_GP_COUNTERS = 1,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}