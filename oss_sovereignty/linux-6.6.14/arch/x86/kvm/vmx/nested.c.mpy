{
  "module_name": "nested.c",
  "hash_id": "b8208990dbc5838ae5dfaec2f14b6c5041aab115039e967f03c0827abb9f4527",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/vmx/nested.c",
  "human_readable_source": "\n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/objtool.h>\n#include <linux/percpu.h>\n\n#include <asm/debugreg.h>\n#include <asm/mmu_context.h>\n\n#include \"cpuid.h\"\n#include \"hyperv.h\"\n#include \"mmu.h\"\n#include \"nested.h\"\n#include \"pmu.h\"\n#include \"sgx.h\"\n#include \"trace.h\"\n#include \"vmx.h\"\n#include \"x86.h\"\n#include \"smm.h\"\n\nstatic bool __read_mostly enable_shadow_vmcs = 1;\nmodule_param_named(enable_shadow_vmcs, enable_shadow_vmcs, bool, S_IRUGO);\n\nstatic bool __read_mostly nested_early_check = 0;\nmodule_param(nested_early_check, bool, S_IRUGO);\n\n#define CC KVM_NESTED_VMENTER_CONSISTENCY_CHECK\n\n \n#define VMX_VPID_EXTENT_SUPPORTED_MASK\t\t\\\n\t(VMX_VPID_EXTENT_INDIVIDUAL_ADDR_BIT |\t\\\n\tVMX_VPID_EXTENT_SINGLE_CONTEXT_BIT |\t\\\n\tVMX_VPID_EXTENT_GLOBAL_CONTEXT_BIT |\t\\\n\tVMX_VPID_EXTENT_SINGLE_NON_GLOBAL_BIT)\n\n#define VMX_MISC_EMULATED_PREEMPTION_TIMER_RATE 5\n\nenum {\n\tVMX_VMREAD_BITMAP,\n\tVMX_VMWRITE_BITMAP,\n\tVMX_BITMAP_NR\n};\nstatic unsigned long *vmx_bitmap[VMX_BITMAP_NR];\n\n#define vmx_vmread_bitmap                    (vmx_bitmap[VMX_VMREAD_BITMAP])\n#define vmx_vmwrite_bitmap                   (vmx_bitmap[VMX_VMWRITE_BITMAP])\n\nstruct shadow_vmcs_field {\n\tu16\tencoding;\n\tu16\toffset;\n};\nstatic struct shadow_vmcs_field shadow_read_only_fields[] = {\n#define SHADOW_FIELD_RO(x, y) { x, offsetof(struct vmcs12, y) },\n#include \"vmcs_shadow_fields.h\"\n};\nstatic int max_shadow_read_only_fields =\n\tARRAY_SIZE(shadow_read_only_fields);\n\nstatic struct shadow_vmcs_field shadow_read_write_fields[] = {\n#define SHADOW_FIELD_RW(x, y) { x, offsetof(struct vmcs12, y) },\n#include \"vmcs_shadow_fields.h\"\n};\nstatic int max_shadow_read_write_fields =\n\tARRAY_SIZE(shadow_read_write_fields);\n\nstatic void init_vmcs_shadow_fields(void)\n{\n\tint i, j;\n\n\tmemset(vmx_vmread_bitmap, 0xff, PAGE_SIZE);\n\tmemset(vmx_vmwrite_bitmap, 0xff, PAGE_SIZE);\n\n\tfor (i = j = 0; i < max_shadow_read_only_fields; i++) {\n\t\tstruct shadow_vmcs_field entry = shadow_read_only_fields[i];\n\t\tu16 field = entry.encoding;\n\n\t\tif (vmcs_field_width(field) == VMCS_FIELD_WIDTH_U64 &&\n\t\t    (i + 1 == max_shadow_read_only_fields ||\n\t\t     shadow_read_only_fields[i + 1].encoding != field + 1))\n\t\t\tpr_err(\"Missing field from shadow_read_only_field %x\\n\",\n\t\t\t       field + 1);\n\n\t\tclear_bit(field, vmx_vmread_bitmap);\n\t\tif (field & 1)\n#ifdef CONFIG_X86_64\n\t\t\tcontinue;\n#else\n\t\t\tentry.offset += sizeof(u32);\n#endif\n\t\tshadow_read_only_fields[j++] = entry;\n\t}\n\tmax_shadow_read_only_fields = j;\n\n\tfor (i = j = 0; i < max_shadow_read_write_fields; i++) {\n\t\tstruct shadow_vmcs_field entry = shadow_read_write_fields[i];\n\t\tu16 field = entry.encoding;\n\n\t\tif (vmcs_field_width(field) == VMCS_FIELD_WIDTH_U64 &&\n\t\t    (i + 1 == max_shadow_read_write_fields ||\n\t\t     shadow_read_write_fields[i + 1].encoding != field + 1))\n\t\t\tpr_err(\"Missing field from shadow_read_write_field %x\\n\",\n\t\t\t       field + 1);\n\n\t\tWARN_ONCE(field >= GUEST_ES_AR_BYTES &&\n\t\t\t  field <= GUEST_TR_AR_BYTES,\n\t\t\t  \"Update vmcs12_write_any() to drop reserved bits from AR_BYTES\");\n\n\t\t \n\t\tswitch (field) {\n\t\tcase GUEST_PML_INDEX:\n\t\t\tif (!cpu_has_vmx_pml())\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase VMX_PREEMPTION_TIMER_VALUE:\n\t\t\tif (!cpu_has_vmx_preemption_timer())\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tcase GUEST_INTR_STATUS:\n\t\t\tif (!cpu_has_vmx_apicv())\n\t\t\t\tcontinue;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\n\t\tclear_bit(field, vmx_vmwrite_bitmap);\n\t\tclear_bit(field, vmx_vmread_bitmap);\n\t\tif (field & 1)\n#ifdef CONFIG_X86_64\n\t\t\tcontinue;\n#else\n\t\t\tentry.offset += sizeof(u32);\n#endif\n\t\tshadow_read_write_fields[j++] = entry;\n\t}\n\tmax_shadow_read_write_fields = j;\n}\n\n \nstatic int nested_vmx_succeed(struct kvm_vcpu *vcpu)\n{\n\tvmx_set_rflags(vcpu, vmx_get_rflags(vcpu)\n\t\t\t& ~(X86_EFLAGS_CF | X86_EFLAGS_PF | X86_EFLAGS_AF |\n\t\t\t    X86_EFLAGS_ZF | X86_EFLAGS_SF | X86_EFLAGS_OF));\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int nested_vmx_failInvalid(struct kvm_vcpu *vcpu)\n{\n\tvmx_set_rflags(vcpu, (vmx_get_rflags(vcpu)\n\t\t\t& ~(X86_EFLAGS_PF | X86_EFLAGS_AF | X86_EFLAGS_ZF |\n\t\t\t    X86_EFLAGS_SF | X86_EFLAGS_OF))\n\t\t\t| X86_EFLAGS_CF);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int nested_vmx_failValid(struct kvm_vcpu *vcpu,\n\t\t\t\tu32 vm_instruction_error)\n{\n\tvmx_set_rflags(vcpu, (vmx_get_rflags(vcpu)\n\t\t\t& ~(X86_EFLAGS_CF | X86_EFLAGS_PF | X86_EFLAGS_AF |\n\t\t\t    X86_EFLAGS_SF | X86_EFLAGS_OF))\n\t\t\t| X86_EFLAGS_ZF);\n\tget_vmcs12(vcpu)->vm_instruction_error = vm_instruction_error;\n\t \n\tif (to_vmx(vcpu)->nested.hv_evmcs_vmptr != EVMPTR_INVALID)\n\t\tto_vmx(vcpu)->nested.need_vmcs12_to_shadow_sync = true;\n\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int nested_vmx_fail(struct kvm_vcpu *vcpu, u32 vm_instruction_error)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t \n\tif (vmx->nested.current_vmptr == INVALID_GPA &&\n\t    !evmptr_is_valid(vmx->nested.hv_evmcs_vmptr))\n\t\treturn nested_vmx_failInvalid(vcpu);\n\n\treturn nested_vmx_failValid(vcpu, vm_instruction_error);\n}\n\nstatic void nested_vmx_abort(struct kvm_vcpu *vcpu, u32 indicator)\n{\n\t \n\tkvm_make_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\tpr_debug_ratelimited(\"nested vmx abort, indicator %d\\n\", indicator);\n}\n\nstatic inline bool vmx_control_verify(u32 control, u32 low, u32 high)\n{\n\treturn fixed_bits_valid(control, low, high);\n}\n\nstatic inline u64 vmx_control_msr(u32 low, u32 high)\n{\n\treturn low | ((u64)high << 32);\n}\n\nstatic void vmx_disable_shadow_vmcs(struct vcpu_vmx *vmx)\n{\n\tsecondary_exec_controls_clearbit(vmx, SECONDARY_EXEC_SHADOW_VMCS);\n\tvmcs_write64(VMCS_LINK_POINTER, INVALID_GPA);\n\tvmx->nested.need_vmcs12_to_shadow_sync = false;\n}\n\nstatic inline void nested_release_evmcs(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_vcpu_hv *hv_vcpu = to_hv_vcpu(vcpu);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (evmptr_is_valid(vmx->nested.hv_evmcs_vmptr)) {\n\t\tkvm_vcpu_unmap(vcpu, &vmx->nested.hv_evmcs_map, true);\n\t\tvmx->nested.hv_evmcs = NULL;\n\t}\n\n\tvmx->nested.hv_evmcs_vmptr = EVMPTR_INVALID;\n\n\tif (hv_vcpu) {\n\t\thv_vcpu->nested.pa_page_gpa = INVALID_GPA;\n\t\thv_vcpu->nested.vm_id = 0;\n\t\thv_vcpu->nested.vp_id = 0;\n\t}\n}\n\nstatic void vmx_sync_vmcs_host_state(struct vcpu_vmx *vmx,\n\t\t\t\t     struct loaded_vmcs *prev)\n{\n\tstruct vmcs_host_state *dest, *src;\n\n\tif (unlikely(!vmx->guest_state_loaded))\n\t\treturn;\n\n\tsrc = &prev->host_state;\n\tdest = &vmx->loaded_vmcs->host_state;\n\n\tvmx_set_host_fs_gs(dest, src->fs_sel, src->gs_sel, src->fs_base, src->gs_base);\n\tdest->ldt_sel = src->ldt_sel;\n#ifdef CONFIG_X86_64\n\tdest->ds_sel = src->ds_sel;\n\tdest->es_sel = src->es_sel;\n#endif\n}\n\nstatic void vmx_switch_vmcs(struct kvm_vcpu *vcpu, struct loaded_vmcs *vmcs)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct loaded_vmcs *prev;\n\tint cpu;\n\n\tif (WARN_ON_ONCE(vmx->loaded_vmcs == vmcs))\n\t\treturn;\n\n\tcpu = get_cpu();\n\tprev = vmx->loaded_vmcs;\n\tvmx->loaded_vmcs = vmcs;\n\tvmx_vcpu_load_vmcs(vcpu, cpu, prev);\n\tvmx_sync_vmcs_host_state(vmx, prev);\n\tput_cpu();\n\n\tvcpu->arch.regs_avail = ~VMX_REGS_LAZY_LOAD_SET;\n\n\t \n\tvcpu->arch.regs_dirty = 0;\n}\n\n \nstatic void free_nested(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (WARN_ON_ONCE(vmx->loaded_vmcs != &vmx->vmcs01))\n\t\tvmx_switch_vmcs(vcpu, &vmx->vmcs01);\n\n\tif (!vmx->nested.vmxon && !vmx->nested.smm.vmxon)\n\t\treturn;\n\n\tkvm_clear_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu);\n\n\tvmx->nested.vmxon = false;\n\tvmx->nested.smm.vmxon = false;\n\tvmx->nested.vmxon_ptr = INVALID_GPA;\n\tfree_vpid(vmx->nested.vpid02);\n\tvmx->nested.posted_intr_nv = -1;\n\tvmx->nested.current_vmptr = INVALID_GPA;\n\tif (enable_shadow_vmcs) {\n\t\tvmx_disable_shadow_vmcs(vmx);\n\t\tvmcs_clear(vmx->vmcs01.shadow_vmcs);\n\t\tfree_vmcs(vmx->vmcs01.shadow_vmcs);\n\t\tvmx->vmcs01.shadow_vmcs = NULL;\n\t}\n\tkfree(vmx->nested.cached_vmcs12);\n\tvmx->nested.cached_vmcs12 = NULL;\n\tkfree(vmx->nested.cached_shadow_vmcs12);\n\tvmx->nested.cached_shadow_vmcs12 = NULL;\n\t \n\tkvm_vcpu_unmap(vcpu, &vmx->nested.apic_access_page_map, false);\n\tkvm_vcpu_unmap(vcpu, &vmx->nested.virtual_apic_map, true);\n\tkvm_vcpu_unmap(vcpu, &vmx->nested.pi_desc_map, true);\n\tvmx->nested.pi_desc = NULL;\n\n\tkvm_mmu_free_roots(vcpu->kvm, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);\n\n\tnested_release_evmcs(vcpu);\n\n\tfree_loaded_vmcs(&vmx->nested.vmcs02);\n}\n\n \nvoid nested_vmx_free_vcpu(struct kvm_vcpu *vcpu)\n{\n\tvcpu_load(vcpu);\n\tvmx_leave_nested(vcpu);\n\tvcpu_put(vcpu);\n}\n\n#define EPTP_PA_MASK   GENMASK_ULL(51, 12)\n\nstatic bool nested_ept_root_matches(hpa_t root_hpa, u64 root_eptp, u64 eptp)\n{\n\treturn VALID_PAGE(root_hpa) &&\n\t       ((root_eptp & EPTP_PA_MASK) == (eptp & EPTP_PA_MASK));\n}\n\nstatic void nested_ept_invalidate_addr(struct kvm_vcpu *vcpu, gpa_t eptp,\n\t\t\t\t       gpa_t addr)\n{\n\tunsigned long roots = 0;\n\tuint i;\n\tstruct kvm_mmu_root_info *cached_root;\n\n\tWARN_ON_ONCE(!mmu_is_nested(vcpu));\n\n\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {\n\t\tcached_root = &vcpu->arch.mmu->prev_roots[i];\n\n\t\tif (nested_ept_root_matches(cached_root->hpa, cached_root->pgd,\n\t\t\t\t\t    eptp))\n\t\t\troots |= KVM_MMU_ROOT_PREVIOUS(i);\n\t}\n\tif (roots)\n\t\tkvm_mmu_invalidate_addr(vcpu, vcpu->arch.mmu, addr, roots);\n}\n\nstatic void nested_ept_inject_page_fault(struct kvm_vcpu *vcpu,\n\t\tstruct x86_exception *fault)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 vm_exit_reason;\n\tunsigned long exit_qualification = vcpu->arch.exit_qualification;\n\n\tif (vmx->nested.pml_full) {\n\t\tvm_exit_reason = EXIT_REASON_PML_FULL;\n\t\tvmx->nested.pml_full = false;\n\t\texit_qualification &= INTR_INFO_UNBLOCK_NMI;\n\t} else {\n\t\tif (fault->error_code & PFERR_RSVD_MASK)\n\t\t\tvm_exit_reason = EXIT_REASON_EPT_MISCONFIG;\n\t\telse\n\t\t\tvm_exit_reason = EXIT_REASON_EPT_VIOLATION;\n\n\t\t \n\t\tnested_ept_invalidate_addr(vcpu, vmcs12->ept_pointer,\n\t\t\t\t\t   fault->address);\n\t}\n\n\tnested_vmx_vmexit(vcpu, vm_exit_reason, 0, exit_qualification);\n\tvmcs12->guest_physical_address = fault->address;\n}\n\nstatic void nested_ept_new_eptp(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tbool execonly = vmx->nested.msrs.ept_caps & VMX_EPT_EXECUTE_ONLY_BIT;\n\tint ept_lpage_level = ept_caps_to_lpage_level(vmx->nested.msrs.ept_caps);\n\n\tkvm_init_shadow_ept_mmu(vcpu, execonly, ept_lpage_level,\n\t\t\t\tnested_ept_ad_enabled(vcpu),\n\t\t\t\tnested_ept_get_eptp(vcpu));\n}\n\nstatic void nested_ept_init_mmu_context(struct kvm_vcpu *vcpu)\n{\n\tWARN_ON(mmu_is_nested(vcpu));\n\n\tvcpu->arch.mmu = &vcpu->arch.guest_mmu;\n\tnested_ept_new_eptp(vcpu);\n\tvcpu->arch.mmu->get_guest_pgd     = nested_ept_get_eptp;\n\tvcpu->arch.mmu->inject_page_fault = nested_ept_inject_page_fault;\n\tvcpu->arch.mmu->get_pdptr         = kvm_pdptr_read;\n\n\tvcpu->arch.walk_mmu              = &vcpu->arch.nested_mmu;\n}\n\nstatic void nested_ept_uninit_mmu_context(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.mmu = &vcpu->arch.root_mmu;\n\tvcpu->arch.walk_mmu = &vcpu->arch.root_mmu;\n}\n\nstatic bool nested_vmx_is_page_fault_vmexit(struct vmcs12 *vmcs12,\n\t\t\t\t\t    u16 error_code)\n{\n\tbool inequality, bit;\n\n\tbit = (vmcs12->exception_bitmap & (1u << PF_VECTOR)) != 0;\n\tinequality =\n\t\t(error_code & vmcs12->page_fault_error_code_mask) !=\n\t\t vmcs12->page_fault_error_code_match;\n\treturn inequality ^ bit;\n}\n\nstatic bool nested_vmx_is_exception_vmexit(struct kvm_vcpu *vcpu, u8 vector,\n\t\t\t\t\t   u32 error_code)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\t \n\tif (vector == PF_VECTOR)\n\t\treturn nested_vmx_is_page_fault_vmexit(vmcs12, (u16)error_code);\n\n\treturn (vmcs12->exception_bitmap & (1u << vector));\n}\n\nstatic int nested_vmx_check_io_bitmap_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t       struct vmcs12 *vmcs12)\n{\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_USE_IO_BITMAPS))\n\t\treturn 0;\n\n\tif (CC(!page_address_valid(vcpu, vmcs12->io_bitmap_a)) ||\n\t    CC(!page_address_valid(vcpu, vmcs12->io_bitmap_b)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_msr_bitmap_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\tstruct vmcs12 *vmcs12)\n{\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_USE_MSR_BITMAPS))\n\t\treturn 0;\n\n\tif (CC(!page_address_valid(vcpu, vmcs12->msr_bitmap)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_tpr_shadow_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\tstruct vmcs12 *vmcs12)\n{\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW))\n\t\treturn 0;\n\n\tif (CC(!page_address_valid(vcpu, vmcs12->virtual_apic_page_addr)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\n \nstatic void nested_vmx_disable_intercept_for_x2apic_msr(unsigned long *msr_bitmap_l1,\n\t\t\t\t\t\t\tunsigned long *msr_bitmap_l0,\n\t\t\t\t\t\t\tu32 msr, int type)\n{\n\tif (type & MSR_TYPE_R && !vmx_test_msr_bitmap_read(msr_bitmap_l1, msr))\n\t\tvmx_clear_msr_bitmap_read(msr_bitmap_l0, msr);\n\n\tif (type & MSR_TYPE_W && !vmx_test_msr_bitmap_write(msr_bitmap_l1, msr))\n\t\tvmx_clear_msr_bitmap_write(msr_bitmap_l0, msr);\n}\n\nstatic inline void enable_x2apic_msr_intercepts(unsigned long *msr_bitmap)\n{\n\tint msr;\n\n\tfor (msr = 0x800; msr <= 0x8ff; msr += BITS_PER_LONG) {\n\t\tunsigned word = msr / BITS_PER_LONG;\n\n\t\tmsr_bitmap[word] = ~0;\n\t\tmsr_bitmap[word + (0x800 / sizeof(long))] = ~0;\n\t}\n}\n\n#define BUILD_NVMX_MSR_INTERCEPT_HELPER(rw)\t\t\t\t\t\\\nstatic inline\t\t\t\t\t\t\t\t\t\\\nvoid nested_vmx_set_msr_##rw##_intercept(struct vcpu_vmx *vmx,\t\t\t\\\n\t\t\t\t\t unsigned long *msr_bitmap_l1,\t\t\\\n\t\t\t\t\t unsigned long *msr_bitmap_l0, u32 msr)\t\\\n{\t\t\t\t\t\t\t\t\t\t\\\n\tif (vmx_test_msr_bitmap_##rw(vmx->vmcs01.msr_bitmap, msr) ||\t\t\\\n\t    vmx_test_msr_bitmap_##rw(msr_bitmap_l1, msr))\t\t\t\\\n\t\tvmx_set_msr_bitmap_##rw(msr_bitmap_l0, msr);\t\t\t\\\n\telse\t\t\t\t\t\t\t\t\t\\\n\t\tvmx_clear_msr_bitmap_##rw(msr_bitmap_l0, msr);\t\t\t\\\n}\nBUILD_NVMX_MSR_INTERCEPT_HELPER(read)\nBUILD_NVMX_MSR_INTERCEPT_HELPER(write)\n\nstatic inline void nested_vmx_set_intercept_for_msr(struct vcpu_vmx *vmx,\n\t\t\t\t\t\t    unsigned long *msr_bitmap_l1,\n\t\t\t\t\t\t    unsigned long *msr_bitmap_l0,\n\t\t\t\t\t\t    u32 msr, int types)\n{\n\tif (types & MSR_TYPE_R)\n\t\tnested_vmx_set_msr_read_intercept(vmx, msr_bitmap_l1,\n\t\t\t\t\t\t  msr_bitmap_l0, msr);\n\tif (types & MSR_TYPE_W)\n\t\tnested_vmx_set_msr_write_intercept(vmx, msr_bitmap_l1,\n\t\t\t\t\t\t   msr_bitmap_l0, msr);\n}\n\n \nstatic inline bool nested_vmx_prepare_msr_bitmap(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t struct vmcs12 *vmcs12)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint msr;\n\tunsigned long *msr_bitmap_l1;\n\tunsigned long *msr_bitmap_l0 = vmx->nested.vmcs02.msr_bitmap;\n\tstruct hv_enlightened_vmcs *evmcs = vmx->nested.hv_evmcs;\n\tstruct kvm_host_map *map = &vmx->nested.msr_bitmap_map;\n\n\t \n\tif (!cpu_has_vmx_msr_bitmap() ||\n\t    !nested_cpu_has(vmcs12, CPU_BASED_USE_MSR_BITMAPS))\n\t\treturn false;\n\n\t \n\tif (!vmx->nested.force_msr_bitmap_recalc && evmcs &&\n\t    evmcs->hv_enlightenments_control.msr_bitmap &&\n\t    evmcs->hv_clean_fields & HV_VMX_ENLIGHTENED_CLEAN_FIELD_MSR_BITMAP)\n\t\treturn true;\n\n\tif (kvm_vcpu_map(vcpu, gpa_to_gfn(vmcs12->msr_bitmap), map))\n\t\treturn false;\n\n\tmsr_bitmap_l1 = (unsigned long *)map->hva;\n\n\t \n\tenable_x2apic_msr_intercepts(msr_bitmap_l0);\n\n\tif (nested_cpu_has_virt_x2apic_mode(vmcs12)) {\n\t\tif (nested_cpu_has_apic_reg_virt(vmcs12)) {\n\t\t\t \n\t\t\tfor (msr = 0x800; msr <= 0x8ff; msr += BITS_PER_LONG) {\n\t\t\t\tunsigned word = msr / BITS_PER_LONG;\n\n\t\t\t\tmsr_bitmap_l0[word] = msr_bitmap_l1[word];\n\t\t\t}\n\t\t}\n\n\t\tnested_vmx_disable_intercept_for_x2apic_msr(\n\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\tX2APIC_MSR(APIC_TASKPRI),\n\t\t\tMSR_TYPE_R | MSR_TYPE_W);\n\n\t\tif (nested_cpu_has_vid(vmcs12)) {\n\t\t\tnested_vmx_disable_intercept_for_x2apic_msr(\n\t\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\tX2APIC_MSR(APIC_EOI),\n\t\t\t\tMSR_TYPE_W);\n\t\t\tnested_vmx_disable_intercept_for_x2apic_msr(\n\t\t\t\tmsr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\tX2APIC_MSR(APIC_SELF_IPI),\n\t\t\t\tMSR_TYPE_W);\n\t\t}\n\t}\n\n\t \n#ifdef CONFIG_X86_64\n\tnested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\t\t MSR_FS_BASE, MSR_TYPE_RW);\n\n\tnested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\t\t MSR_GS_BASE, MSR_TYPE_RW);\n\n\tnested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\t\t MSR_KERNEL_GS_BASE, MSR_TYPE_RW);\n#endif\n\tnested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\t\t MSR_IA32_SPEC_CTRL, MSR_TYPE_RW);\n\n\tnested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\t\t MSR_IA32_PRED_CMD, MSR_TYPE_W);\n\n\tnested_vmx_set_intercept_for_msr(vmx, msr_bitmap_l1, msr_bitmap_l0,\n\t\t\t\t\t MSR_IA32_FLUSH_CMD, MSR_TYPE_W);\n\n\tkvm_vcpu_unmap(vcpu, &vmx->nested.msr_bitmap_map, false);\n\n\tvmx->nested.force_msr_bitmap_recalc = false;\n\n\treturn true;\n}\n\nstatic void nested_cache_shadow_vmcs12(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmcs12 *vmcs12)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct gfn_to_hva_cache *ghc = &vmx->nested.shadow_vmcs12_cache;\n\n\tif (!nested_cpu_has_shadow_vmcs(vmcs12) ||\n\t    vmcs12->vmcs_link_pointer == INVALID_GPA)\n\t\treturn;\n\n\tif (ghc->gpa != vmcs12->vmcs_link_pointer &&\n\t    kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc,\n\t\t\t\t      vmcs12->vmcs_link_pointer, VMCS12_SIZE))\n\t\treturn;\n\n\tkvm_read_guest_cached(vmx->vcpu.kvm, ghc, get_shadow_vmcs12(vcpu),\n\t\t\t      VMCS12_SIZE);\n}\n\nstatic void nested_flush_cached_shadow_vmcs12(struct kvm_vcpu *vcpu,\n\t\t\t\t\t      struct vmcs12 *vmcs12)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct gfn_to_hva_cache *ghc = &vmx->nested.shadow_vmcs12_cache;\n\n\tif (!nested_cpu_has_shadow_vmcs(vmcs12) ||\n\t    vmcs12->vmcs_link_pointer == INVALID_GPA)\n\t\treturn;\n\n\tif (ghc->gpa != vmcs12->vmcs_link_pointer &&\n\t    kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc,\n\t\t\t\t      vmcs12->vmcs_link_pointer, VMCS12_SIZE))\n\t\treturn;\n\n\tkvm_write_guest_cached(vmx->vcpu.kvm, ghc, get_shadow_vmcs12(vcpu),\n\t\t\t       VMCS12_SIZE);\n}\n\n \nstatic bool nested_exit_intr_ack_set(struct kvm_vcpu *vcpu)\n{\n\treturn get_vmcs12(vcpu)->vm_exit_controls &\n\t\tVM_EXIT_ACK_INTR_ON_EXIT;\n}\n\nstatic int nested_vmx_check_apic_access_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t  struct vmcs12 *vmcs12)\n{\n\tif (nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES) &&\n\t    CC(!page_address_valid(vcpu, vmcs12->apic_access_addr)))\n\t\treturn -EINVAL;\n\telse\n\t\treturn 0;\n}\n\nstatic int nested_vmx_check_apicv_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t   struct vmcs12 *vmcs12)\n{\n\tif (!nested_cpu_has_virt_x2apic_mode(vmcs12) &&\n\t    !nested_cpu_has_apic_reg_virt(vmcs12) &&\n\t    !nested_cpu_has_vid(vmcs12) &&\n\t    !nested_cpu_has_posted_intr(vmcs12))\n\t\treturn 0;\n\n\t \n\tif (CC(nested_cpu_has_virt_x2apic_mode(vmcs12) &&\n\t       nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)))\n\t\treturn -EINVAL;\n\n\t \n\tif (CC(nested_cpu_has_vid(vmcs12) && !nested_exit_on_intr(vcpu)))\n\t\treturn -EINVAL;\n\n\t \n\tif (nested_cpu_has_posted_intr(vmcs12) &&\n\t   (CC(!nested_cpu_has_vid(vmcs12)) ||\n\t    CC(!nested_exit_intr_ack_set(vcpu)) ||\n\t    CC((vmcs12->posted_intr_nv & 0xff00)) ||\n\t    CC(!kvm_vcpu_is_legal_aligned_gpa(vcpu, vmcs12->posted_intr_desc_addr, 64))))\n\t\treturn -EINVAL;\n\n\t \n\tif (CC(!nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_msr_switch(struct kvm_vcpu *vcpu,\n\t\t\t\t       u32 count, u64 addr)\n{\n\tif (count == 0)\n\t\treturn 0;\n\n\tif (!kvm_vcpu_is_legal_aligned_gpa(vcpu, addr, 16) ||\n\t    !kvm_vcpu_is_legal_gpa(vcpu, (addr + count * sizeof(struct vmx_msr_entry) - 1)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_exit_msr_switch_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t     struct vmcs12 *vmcs12)\n{\n\tif (CC(nested_vmx_check_msr_switch(vcpu,\n\t\t\t\t\t   vmcs12->vm_exit_msr_load_count,\n\t\t\t\t\t   vmcs12->vm_exit_msr_load_addr)) ||\n\t    CC(nested_vmx_check_msr_switch(vcpu,\n\t\t\t\t\t   vmcs12->vm_exit_msr_store_count,\n\t\t\t\t\t   vmcs12->vm_exit_msr_store_addr)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_entry_msr_switch_controls(struct kvm_vcpu *vcpu,\n                                                      struct vmcs12 *vmcs12)\n{\n\tif (CC(nested_vmx_check_msr_switch(vcpu,\n\t\t\t\t\t   vmcs12->vm_entry_msr_load_count,\n\t\t\t\t\t   vmcs12->vm_entry_msr_load_addr)))\n                return -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_pml_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t struct vmcs12 *vmcs12)\n{\n\tif (!nested_cpu_has_pml(vmcs12))\n\t\treturn 0;\n\n\tif (CC(!nested_cpu_has_ept(vmcs12)) ||\n\t    CC(!page_address_valid(vcpu, vmcs12->pml_address)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_unrestricted_guest_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t\tstruct vmcs12 *vmcs12)\n{\n\tif (CC(nested_cpu_has2(vmcs12, SECONDARY_EXEC_UNRESTRICTED_GUEST) &&\n\t       !nested_cpu_has_ept(vmcs12)))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic int nested_vmx_check_mode_based_ept_exec_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t\t struct vmcs12 *vmcs12)\n{\n\tif (CC(nested_cpu_has2(vmcs12, SECONDARY_EXEC_MODE_BASED_EPT_EXEC) &&\n\t       !nested_cpu_has_ept(vmcs12)))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic int nested_vmx_check_shadow_vmcs_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t struct vmcs12 *vmcs12)\n{\n\tif (!nested_cpu_has_shadow_vmcs(vmcs12))\n\t\treturn 0;\n\n\tif (CC(!page_address_valid(vcpu, vmcs12->vmread_bitmap)) ||\n\t    CC(!page_address_valid(vcpu, vmcs12->vmwrite_bitmap)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_msr_check_common(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmx_msr_entry *e)\n{\n\t \n\tif (CC(vcpu->arch.apic_base & X2APIC_ENABLE && e->index >> 8 == 0x8))\n\t\treturn -EINVAL;\n\tif (CC(e->index == MSR_IA32_UCODE_WRITE) ||  \n\t    CC(e->index == MSR_IA32_UCODE_REV))\n\t\treturn -EINVAL;\n\tif (CC(e->reserved != 0))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic int nested_vmx_load_msr_check(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct vmx_msr_entry *e)\n{\n\tif (CC(e->index == MSR_FS_BASE) ||\n\t    CC(e->index == MSR_GS_BASE) ||\n\t    CC(e->index == MSR_IA32_SMM_MONITOR_CTL) ||  \n\t    nested_vmx_msr_check_common(vcpu, e))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic int nested_vmx_store_msr_check(struct kvm_vcpu *vcpu,\n\t\t\t\t      struct vmx_msr_entry *e)\n{\n\tif (CC(e->index == MSR_IA32_SMBASE) ||  \n\t    nested_vmx_msr_check_common(vcpu, e))\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic u32 nested_vmx_max_atomic_switch_msrs(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu64 vmx_misc = vmx_control_msr(vmx->nested.msrs.misc_low,\n\t\t\t\t       vmx->nested.msrs.misc_high);\n\n\treturn (vmx_misc_max_msr(vmx_misc) + 1) * VMX_MISC_MSR_LIST_MULTIPLIER;\n}\n\n \nstatic u32 nested_vmx_load_msr(struct kvm_vcpu *vcpu, u64 gpa, u32 count)\n{\n\tu32 i;\n\tstruct vmx_msr_entry e;\n\tu32 max_msr_list_size = nested_vmx_max_atomic_switch_msrs(vcpu);\n\n\tfor (i = 0; i < count; i++) {\n\t\tif (unlikely(i >= max_msr_list_size))\n\t\t\tgoto fail;\n\n\t\tif (kvm_vcpu_read_guest(vcpu, gpa + i * sizeof(e),\n\t\t\t\t\t&e, sizeof(e))) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s cannot read MSR entry (%u, 0x%08llx)\\n\",\n\t\t\t\t__func__, i, gpa + i * sizeof(e));\n\t\t\tgoto fail;\n\t\t}\n\t\tif (nested_vmx_load_msr_check(vcpu, &e)) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s check failed (%u, 0x%x, 0x%x)\\n\",\n\t\t\t\t__func__, i, e.index, e.reserved);\n\t\t\tgoto fail;\n\t\t}\n\t\tif (kvm_set_msr(vcpu, e.index, e.value)) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s cannot write MSR (%u, 0x%x, 0x%llx)\\n\",\n\t\t\t\t__func__, i, e.index, e.value);\n\t\t\tgoto fail;\n\t\t}\n\t}\n\treturn 0;\nfail:\n\t \n\treturn i + 1;\n}\n\nstatic bool nested_vmx_get_vmexit_msr_value(struct kvm_vcpu *vcpu,\n\t\t\t\t\t    u32 msr_index,\n\t\t\t\t\t    u64 *data)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t \n\tif (msr_index == MSR_IA32_TSC) {\n\t\tint i = vmx_find_loadstore_msr_slot(&vmx->msr_autostore.guest,\n\t\t\t\t\t\t    MSR_IA32_TSC);\n\n\t\tif (i >= 0) {\n\t\t\tu64 val = vmx->msr_autostore.guest.val[i].value;\n\n\t\t\t*data = kvm_read_l1_tsc(vcpu, val);\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (kvm_get_msr(vcpu, msr_index, data)) {\n\t\tpr_debug_ratelimited(\"%s cannot read MSR (0x%x)\\n\", __func__,\n\t\t\tmsr_index);\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic bool read_and_check_msr_entry(struct kvm_vcpu *vcpu, u64 gpa, int i,\n\t\t\t\t     struct vmx_msr_entry *e)\n{\n\tif (kvm_vcpu_read_guest(vcpu,\n\t\t\t\tgpa + i * sizeof(*e),\n\t\t\t\te, 2 * sizeof(u32))) {\n\t\tpr_debug_ratelimited(\n\t\t\t\"%s cannot read MSR entry (%u, 0x%08llx)\\n\",\n\t\t\t__func__, i, gpa + i * sizeof(*e));\n\t\treturn false;\n\t}\n\tif (nested_vmx_store_msr_check(vcpu, e)) {\n\t\tpr_debug_ratelimited(\n\t\t\t\"%s check failed (%u, 0x%x, 0x%x)\\n\",\n\t\t\t__func__, i, e->index, e->reserved);\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic int nested_vmx_store_msr(struct kvm_vcpu *vcpu, u64 gpa, u32 count)\n{\n\tu64 data;\n\tu32 i;\n\tstruct vmx_msr_entry e;\n\tu32 max_msr_list_size = nested_vmx_max_atomic_switch_msrs(vcpu);\n\n\tfor (i = 0; i < count; i++) {\n\t\tif (unlikely(i >= max_msr_list_size))\n\t\t\treturn -EINVAL;\n\n\t\tif (!read_and_check_msr_entry(vcpu, gpa, i, &e))\n\t\t\treturn -EINVAL;\n\n\t\tif (!nested_vmx_get_vmexit_msr_value(vcpu, e.index, &data))\n\t\t\treturn -EINVAL;\n\n\t\tif (kvm_vcpu_write_guest(vcpu,\n\t\t\t\t\t gpa + i * sizeof(e) +\n\t\t\t\t\t     offsetof(struct vmx_msr_entry, value),\n\t\t\t\t\t &data, sizeof(data))) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s cannot write MSR (%u, 0x%x, 0x%llx)\\n\",\n\t\t\t\t__func__, i, e.index, data);\n\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic bool nested_msr_store_list_has_msr(struct kvm_vcpu *vcpu, u32 msr_index)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tu32 count = vmcs12->vm_exit_msr_store_count;\n\tu64 gpa = vmcs12->vm_exit_msr_store_addr;\n\tstruct vmx_msr_entry e;\n\tu32 i;\n\n\tfor (i = 0; i < count; i++) {\n\t\tif (!read_and_check_msr_entry(vcpu, gpa, i, &e))\n\t\t\treturn false;\n\n\t\tif (e.index == msr_index)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void prepare_vmx_msr_autostore_list(struct kvm_vcpu *vcpu,\n\t\t\t\t\t   u32 msr_index)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmx_msrs *autostore = &vmx->msr_autostore.guest;\n\tbool in_vmcs12_store_list;\n\tint msr_autostore_slot;\n\tbool in_autostore_list;\n\tint last;\n\n\tmsr_autostore_slot = vmx_find_loadstore_msr_slot(autostore, msr_index);\n\tin_autostore_list = msr_autostore_slot >= 0;\n\tin_vmcs12_store_list = nested_msr_store_list_has_msr(vcpu, msr_index);\n\n\tif (in_vmcs12_store_list && !in_autostore_list) {\n\t\tif (autostore->nr == MAX_NR_LOADSTORE_MSRS) {\n\t\t\t \n\t\t\tpr_warn_ratelimited(\n\t\t\t\t\"Not enough msr entries in msr_autostore.  Can't add msr %x\\n\",\n\t\t\t\tmsr_index);\n\t\t\treturn;\n\t\t}\n\t\tlast = autostore->nr++;\n\t\tautostore->val[last].index = msr_index;\n\t} else if (!in_vmcs12_store_list && in_autostore_list) {\n\t\tlast = --autostore->nr;\n\t\tautostore->val[msr_autostore_slot] = autostore->val[last];\n\t}\n}\n\n \nstatic int nested_vmx_load_cr3(struct kvm_vcpu *vcpu, unsigned long cr3,\n\t\t\t       bool nested_ept, bool reload_pdptrs,\n\t\t\t       enum vm_entry_failure_code *entry_failure_code)\n{\n\tif (CC(kvm_vcpu_is_illegal_gpa(vcpu, cr3))) {\n\t\t*entry_failure_code = ENTRY_FAIL_DEFAULT;\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (reload_pdptrs && !nested_ept && is_pae_paging(vcpu) &&\n\t    CC(!load_pdptrs(vcpu, cr3))) {\n\t\t*entry_failure_code = ENTRY_FAIL_PDPTE;\n\t\treturn -EINVAL;\n\t}\n\n\tvcpu->arch.cr3 = cr3;\n\tkvm_register_mark_dirty(vcpu, VCPU_EXREG_CR3);\n\n\t \n\tkvm_init_mmu(vcpu);\n\n\tif (!nested_ept)\n\t\tkvm_mmu_new_pgd(vcpu, cr3);\n\n\treturn 0;\n}\n\n \nstatic bool nested_has_guest_tlb_tag(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\treturn enable_ept ||\n\t       (nested_cpu_has_vpid(vmcs12) && to_vmx(vcpu)->nested.vpid02);\n}\n\nstatic void nested_vmx_transition_tlb_flush(struct kvm_vcpu *vcpu,\n\t\t\t\t\t    struct vmcs12 *vmcs12,\n\t\t\t\t\t    bool is_vmenter)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t \n\tif (to_hv_vcpu(vcpu) && enable_ept)\n\t\tkvm_make_request(KVM_REQ_HV_TLB_FLUSH, vcpu);\n\n\t \n\tif (!nested_cpu_has_vpid(vmcs12)) {\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu);\n\t\treturn;\n\t}\n\n\t \n\tWARN_ON(!enable_vpid);\n\n\t \n\tif (is_vmenter && vmcs12->virtual_processor_id != vmx->nested.last_vpid) {\n\t\tvmx->nested.last_vpid = vmcs12->virtual_processor_id;\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu);\n\t\treturn;\n\t}\n\n\t \n\tif (!nested_has_guest_tlb_tag(vcpu))\n\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_CURRENT, vcpu);\n}\n\nstatic bool is_bitwise_subset(u64 superset, u64 subset, u64 mask)\n{\n\tsuperset &= mask;\n\tsubset &= mask;\n\n\treturn (superset | subset) == superset;\n}\n\nstatic int vmx_restore_vmx_basic(struct vcpu_vmx *vmx, u64 data)\n{\n\tconst u64 feature_and_reserved =\n\t\t \n\t\tBIT_ULL(49) | BIT_ULL(54) | BIT_ULL(55) |\n\t\t \n\t\tBIT_ULL(31) | GENMASK_ULL(47, 45) | GENMASK_ULL(63, 56);\n\tu64 vmx_basic = vmcs_config.nested.basic;\n\n\tif (!is_bitwise_subset(vmx_basic, data, feature_and_reserved))\n\t\treturn -EINVAL;\n\n\t \n\tif (data & BIT_ULL(48))\n\t\treturn -EINVAL;\n\n\tif (vmx_basic_vmcs_revision_id(vmx_basic) !=\n\t    vmx_basic_vmcs_revision_id(data))\n\t\treturn -EINVAL;\n\n\tif (vmx_basic_vmcs_size(vmx_basic) > vmx_basic_vmcs_size(data))\n\t\treturn -EINVAL;\n\n\tvmx->nested.msrs.basic = data;\n\treturn 0;\n}\n\nstatic void vmx_get_control_msr(struct nested_vmx_msrs *msrs, u32 msr_index,\n\t\t\t\tu32 **low, u32 **high)\n{\n\tswitch (msr_index) {\n\tcase MSR_IA32_VMX_TRUE_PINBASED_CTLS:\n\t\t*low = &msrs->pinbased_ctls_low;\n\t\t*high = &msrs->pinbased_ctls_high;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_PROCBASED_CTLS:\n\t\t*low = &msrs->procbased_ctls_low;\n\t\t*high = &msrs->procbased_ctls_high;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_EXIT_CTLS:\n\t\t*low = &msrs->exit_ctls_low;\n\t\t*high = &msrs->exit_ctls_high;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_ENTRY_CTLS:\n\t\t*low = &msrs->entry_ctls_low;\n\t\t*high = &msrs->entry_ctls_high;\n\t\tbreak;\n\tcase MSR_IA32_VMX_PROCBASED_CTLS2:\n\t\t*low = &msrs->secondary_ctls_low;\n\t\t*high = &msrs->secondary_ctls_high;\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic int\nvmx_restore_control_msr(struct vcpu_vmx *vmx, u32 msr_index, u64 data)\n{\n\tu32 *lowp, *highp;\n\tu64 supported;\n\n\tvmx_get_control_msr(&vmcs_config.nested, msr_index, &lowp, &highp);\n\n\tsupported = vmx_control_msr(*lowp, *highp);\n\n\t \n\tif (!is_bitwise_subset(data, supported, GENMASK_ULL(31, 0)))\n\t\treturn -EINVAL;\n\n\t \n\tif (!is_bitwise_subset(supported, data, GENMASK_ULL(63, 32)))\n\t\treturn -EINVAL;\n\n\tvmx_get_control_msr(&vmx->nested.msrs, msr_index, &lowp, &highp);\n\t*lowp = data;\n\t*highp = data >> 32;\n\treturn 0;\n}\n\nstatic int vmx_restore_vmx_misc(struct vcpu_vmx *vmx, u64 data)\n{\n\tconst u64 feature_and_reserved_bits =\n\t\t \n\t\tBIT_ULL(5) | GENMASK_ULL(8, 6) | BIT_ULL(14) | BIT_ULL(15) |\n\t\tBIT_ULL(28) | BIT_ULL(29) | BIT_ULL(30) |\n\t\t \n\t\tGENMASK_ULL(13, 9) | BIT_ULL(31);\n\tu64 vmx_misc = vmx_control_msr(vmcs_config.nested.misc_low,\n\t\t\t\t       vmcs_config.nested.misc_high);\n\n\tif (!is_bitwise_subset(vmx_misc, data, feature_and_reserved_bits))\n\t\treturn -EINVAL;\n\n\tif ((vmx->nested.msrs.pinbased_ctls_high &\n\t     PIN_BASED_VMX_PREEMPTION_TIMER) &&\n\t    vmx_misc_preemption_timer_rate(data) !=\n\t    vmx_misc_preemption_timer_rate(vmx_misc))\n\t\treturn -EINVAL;\n\n\tif (vmx_misc_cr3_count(data) > vmx_misc_cr3_count(vmx_misc))\n\t\treturn -EINVAL;\n\n\tif (vmx_misc_max_msr(data) > vmx_misc_max_msr(vmx_misc))\n\t\treturn -EINVAL;\n\n\tif (vmx_misc_mseg_revid(data) != vmx_misc_mseg_revid(vmx_misc))\n\t\treturn -EINVAL;\n\n\tvmx->nested.msrs.misc_low = data;\n\tvmx->nested.msrs.misc_high = data >> 32;\n\n\treturn 0;\n}\n\nstatic int vmx_restore_vmx_ept_vpid_cap(struct vcpu_vmx *vmx, u64 data)\n{\n\tu64 vmx_ept_vpid_cap = vmx_control_msr(vmcs_config.nested.ept_caps,\n\t\t\t\t\t       vmcs_config.nested.vpid_caps);\n\n\t \n\tif (!is_bitwise_subset(vmx_ept_vpid_cap, data, -1ULL))\n\t\treturn -EINVAL;\n\n\tvmx->nested.msrs.ept_caps = data;\n\tvmx->nested.msrs.vpid_caps = data >> 32;\n\treturn 0;\n}\n\nstatic u64 *vmx_get_fixed0_msr(struct nested_vmx_msrs *msrs, u32 msr_index)\n{\n\tswitch (msr_index) {\n\tcase MSR_IA32_VMX_CR0_FIXED0:\n\t\treturn &msrs->cr0_fixed0;\n\tcase MSR_IA32_VMX_CR4_FIXED0:\n\t\treturn &msrs->cr4_fixed0;\n\tdefault:\n\t\tBUG();\n\t}\n}\n\nstatic int vmx_restore_fixed0_msr(struct vcpu_vmx *vmx, u32 msr_index, u64 data)\n{\n\tconst u64 *msr = vmx_get_fixed0_msr(&vmcs_config.nested, msr_index);\n\n\t \n\tif (!is_bitwise_subset(data, *msr, -1ULL))\n\t\treturn -EINVAL;\n\n\t*vmx_get_fixed0_msr(&vmx->nested.msrs, msr_index) = data;\n\treturn 0;\n}\n\n \nint vmx_set_vmx_msr(struct kvm_vcpu *vcpu, u32 msr_index, u64 data)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t \n\tif (vmx->nested.vmxon)\n\t\treturn -EBUSY;\n\n\tswitch (msr_index) {\n\tcase MSR_IA32_VMX_BASIC:\n\t\treturn vmx_restore_vmx_basic(vmx, data);\n\tcase MSR_IA32_VMX_PINBASED_CTLS:\n\tcase MSR_IA32_VMX_PROCBASED_CTLS:\n\tcase MSR_IA32_VMX_EXIT_CTLS:\n\tcase MSR_IA32_VMX_ENTRY_CTLS:\n\t\t \n\t\treturn -EINVAL;\n\tcase MSR_IA32_VMX_TRUE_PINBASED_CTLS:\n\tcase MSR_IA32_VMX_TRUE_PROCBASED_CTLS:\n\tcase MSR_IA32_VMX_TRUE_EXIT_CTLS:\n\tcase MSR_IA32_VMX_TRUE_ENTRY_CTLS:\n\tcase MSR_IA32_VMX_PROCBASED_CTLS2:\n\t\treturn vmx_restore_control_msr(vmx, msr_index, data);\n\tcase MSR_IA32_VMX_MISC:\n\t\treturn vmx_restore_vmx_misc(vmx, data);\n\tcase MSR_IA32_VMX_CR0_FIXED0:\n\tcase MSR_IA32_VMX_CR4_FIXED0:\n\t\treturn vmx_restore_fixed0_msr(vmx, msr_index, data);\n\tcase MSR_IA32_VMX_CR0_FIXED1:\n\tcase MSR_IA32_VMX_CR4_FIXED1:\n\t\t \n\t\treturn -EINVAL;\n\tcase MSR_IA32_VMX_EPT_VPID_CAP:\n\t\treturn vmx_restore_vmx_ept_vpid_cap(vmx, data);\n\tcase MSR_IA32_VMX_VMCS_ENUM:\n\t\tvmx->nested.msrs.vmcs_enum = data;\n\t\treturn 0;\n\tcase MSR_IA32_VMX_VMFUNC:\n\t\tif (data & ~vmcs_config.nested.vmfunc_controls)\n\t\t\treturn -EINVAL;\n\t\tvmx->nested.msrs.vmfunc_controls = data;\n\t\treturn 0;\n\tdefault:\n\t\t \n\t\treturn -EINVAL;\n\t}\n}\n\n \nint vmx_get_vmx_msr(struct nested_vmx_msrs *msrs, u32 msr_index, u64 *pdata)\n{\n\tswitch (msr_index) {\n\tcase MSR_IA32_VMX_BASIC:\n\t\t*pdata = msrs->basic;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_PINBASED_CTLS:\n\tcase MSR_IA32_VMX_PINBASED_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->pinbased_ctls_low,\n\t\t\tmsrs->pinbased_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_PINBASED_CTLS)\n\t\t\t*pdata |= PIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_PROCBASED_CTLS:\n\tcase MSR_IA32_VMX_PROCBASED_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->procbased_ctls_low,\n\t\t\tmsrs->procbased_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_PROCBASED_CTLS)\n\t\t\t*pdata |= CPU_BASED_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_EXIT_CTLS:\n\tcase MSR_IA32_VMX_EXIT_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->exit_ctls_low,\n\t\t\tmsrs->exit_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_EXIT_CTLS)\n\t\t\t*pdata |= VM_EXIT_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_TRUE_ENTRY_CTLS:\n\tcase MSR_IA32_VMX_ENTRY_CTLS:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->entry_ctls_low,\n\t\t\tmsrs->entry_ctls_high);\n\t\tif (msr_index == MSR_IA32_VMX_ENTRY_CTLS)\n\t\t\t*pdata |= VM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR;\n\t\tbreak;\n\tcase MSR_IA32_VMX_MISC:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->misc_low,\n\t\t\tmsrs->misc_high);\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR0_FIXED0:\n\t\t*pdata = msrs->cr0_fixed0;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR0_FIXED1:\n\t\t*pdata = msrs->cr0_fixed1;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR4_FIXED0:\n\t\t*pdata = msrs->cr4_fixed0;\n\t\tbreak;\n\tcase MSR_IA32_VMX_CR4_FIXED1:\n\t\t*pdata = msrs->cr4_fixed1;\n\t\tbreak;\n\tcase MSR_IA32_VMX_VMCS_ENUM:\n\t\t*pdata = msrs->vmcs_enum;\n\t\tbreak;\n\tcase MSR_IA32_VMX_PROCBASED_CTLS2:\n\t\t*pdata = vmx_control_msr(\n\t\t\tmsrs->secondary_ctls_low,\n\t\t\tmsrs->secondary_ctls_high);\n\t\tbreak;\n\tcase MSR_IA32_VMX_EPT_VPID_CAP:\n\t\t*pdata = msrs->ept_caps |\n\t\t\t((u64)msrs->vpid_caps << 32);\n\t\tbreak;\n\tcase MSR_IA32_VMX_VMFUNC:\n\t\t*pdata = msrs->vmfunc_controls;\n\t\tbreak;\n\tdefault:\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void copy_shadow_to_vmcs12(struct vcpu_vmx *vmx)\n{\n\tstruct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;\n\tstruct vmcs12 *vmcs12 = get_vmcs12(&vmx->vcpu);\n\tstruct shadow_vmcs_field field;\n\tunsigned long val;\n\tint i;\n\n\tif (WARN_ON(!shadow_vmcs))\n\t\treturn;\n\n\tpreempt_disable();\n\n\tvmcs_load(shadow_vmcs);\n\n\tfor (i = 0; i < max_shadow_read_write_fields; i++) {\n\t\tfield = shadow_read_write_fields[i];\n\t\tval = __vmcs_readl(field.encoding);\n\t\tvmcs12_write_any(vmcs12, field.encoding, field.offset, val);\n\t}\n\n\tvmcs_clear(shadow_vmcs);\n\tvmcs_load(vmx->loaded_vmcs->vmcs);\n\n\tpreempt_enable();\n}\n\nstatic void copy_vmcs12_to_shadow(struct vcpu_vmx *vmx)\n{\n\tconst struct shadow_vmcs_field *fields[] = {\n\t\tshadow_read_write_fields,\n\t\tshadow_read_only_fields\n\t};\n\tconst int max_fields[] = {\n\t\tmax_shadow_read_write_fields,\n\t\tmax_shadow_read_only_fields\n\t};\n\tstruct vmcs *shadow_vmcs = vmx->vmcs01.shadow_vmcs;\n\tstruct vmcs12 *vmcs12 = get_vmcs12(&vmx->vcpu);\n\tstruct shadow_vmcs_field field;\n\tunsigned long val;\n\tint i, q;\n\n\tif (WARN_ON(!shadow_vmcs))\n\t\treturn;\n\n\tvmcs_load(shadow_vmcs);\n\n\tfor (q = 0; q < ARRAY_SIZE(fields); q++) {\n\t\tfor (i = 0; i < max_fields[q]; i++) {\n\t\t\tfield = fields[q][i];\n\t\t\tval = vmcs12_read_any(vmcs12, field.encoding,\n\t\t\t\t\t      field.offset);\n\t\t\t__vmcs_writel(field.encoding, val);\n\t\t}\n\t}\n\n\tvmcs_clear(shadow_vmcs);\n\tvmcs_load(vmx->loaded_vmcs->vmcs);\n}\n\nstatic void copy_enlightened_to_vmcs12(struct vcpu_vmx *vmx, u32 hv_clean_fields)\n{\n\tstruct vmcs12 *vmcs12 = vmx->nested.cached_vmcs12;\n\tstruct hv_enlightened_vmcs *evmcs = vmx->nested.hv_evmcs;\n\tstruct kvm_vcpu_hv *hv_vcpu = to_hv_vcpu(&vmx->vcpu);\n\n\t \n\tvmcs12->tpr_threshold = evmcs->tpr_threshold;\n\tvmcs12->guest_rip = evmcs->guest_rip;\n\n\tif (unlikely(!(hv_clean_fields &\n\t\t       HV_VMX_ENLIGHTENED_CLEAN_FIELD_ENLIGHTENMENTSCONTROL))) {\n\t\thv_vcpu->nested.pa_page_gpa = evmcs->partition_assist_page;\n\t\thv_vcpu->nested.vm_id = evmcs->hv_vm_id;\n\t\thv_vcpu->nested.vp_id = evmcs->hv_vp_id;\n\t}\n\n\tif (unlikely(!(hv_clean_fields &\n\t\t       HV_VMX_ENLIGHTENED_CLEAN_FIELD_GUEST_BASIC))) {\n\t\tvmcs12->guest_rsp = evmcs->guest_rsp;\n\t\tvmcs12->guest_rflags = evmcs->guest_rflags;\n\t\tvmcs12->guest_interruptibility_info =\n\t\t\tevmcs->guest_interruptibility_info;\n\t\t \n\t}\n\n\tif (unlikely(!(hv_clean_fields &\n\t\t       HV_VMX_ENLIGHTENED_CLEAN_FIELD_CONTROL_PROC))) {\n\t\tvmcs12->cpu_based_vm_exec_control =\n\t\t\tevmcs->cpu_based_vm_exec_control;\n\t}\n\n\tif (unlikely(!(hv_clean_fields &\n\t\t       HV_VMX_ENLIGHTENED_CLEAN_FIELD_CONTROL_EXCPN))) {\n\t\tvmcs12->exception_bitmap = evmcs->exception_bitmap;\n\t}\n\n\tif (unlikely(!(hv_clean_fields &\n\t\t       HV_VMX_ENLIGHTENED_CLEAN_FIELD_CONTROL_ENTRY))) {\n\t\tvmcs12->vm_entry_controls = evmcs->vm_entry_controls;\n\t}\n\n\tif (unlikely(!(hv_clean_fields &\n\t\t       HV_VMX_ENLIGHTENED_CLEAN_FIELD_CONTROL_EVENT))) {\n\t\tvmcs12->vm_entry_intr_info_field =\n\t\t\tevmcs->vm_entry_intr_info_field;\n\t\tvmcs12->vm_entry_exception_error_code =\n\t\t\tevmcs->vm_entry_exception_error_code;\n\t\tvmcs12->vm_entry_instruction_len =\n\t\t\tevmcs->vm_entry_instruction_len;\n\t}\n\n\tif (unlikely(!(hv_clean_fields &\n\t\t       HV_VMX_ENLIGHTENED_CLEAN_FIELD_HOST_GRP1))) {\n\t\tvmcs12->host_ia32_pat = evmcs->host_ia32_pat;\n\t\tvmcs12->host_ia32_efer = evmcs->host_ia32_efer;\n\t\tvmcs12->host_cr0 = evmcs->host_cr0;\n\t\tvmcs12->host_cr3 = evmcs->host_cr3;\n\t\tvmcs12->host_cr4 = evmcs->host_cr4;\n\t\tvmcs12->host_ia32_sysenter_esp = evmcs->host_ia32_sysenter_esp;\n\t\tvmcs12->host_ia32_sysenter_eip = evmcs->host_ia32_sysenter_eip;\n\t\tvmcs12->host_rip = evmcs->host_rip;\n\t\tvmcs12->host_ia32_sysenter_cs = evmcs->host_ia32_sysenter_cs;\n\t\tvmcs12->host_es_selector = evmcs->host_es_selector;\n\t\tvmcs12->host_cs_selector = evmcs->host_cs_selector;\n\t\tvmcs12->host_ss_selector = evmcs->host_ss_selector;\n\t\tvmcs12->host_ds_selector = evmcs->host_ds_selector;\n\t\tvmcs12->host_fs_selector = evmcs->host_fs_selector;\n\t\tvmcs12->host_gs_selector = evmcs->host_gs_selector;\n\t\tvmcs12->host_tr_selector = evmcs->host_tr_selector;\n\t\tvmcs12->host_ia32_perf_global_ctrl = evmcs->host_ia32_perf_global_ctrl;\n\t\t \n\t}\n\n\tif (unlikely(!(hv_clean_fields &\n\t\t       HV_VMX_ENLIGHTENED_CLEAN_FIELD_CONTROL_GRP1))) {\n\t\tvmcs12->pin_based_vm_exec_control =\n\t\t\tevmcs->pin_based_vm_exec_control;\n\t\tvmcs12->vm_exit_controls = evmcs->vm_exit_controls;\n\t\tvmcs12->secondary_vm_exec_control =\n\t\t\tevmcs->secondary_vm_exec_control;\n\t}\n\n\tif (unlikely(!(hv_clean_fields &\n\t\t       HV_VMX_ENLIGHTENED_CLEAN_FIELD_IO_BITMAP))) {\n\t\tvmcs12->io_bitmap_a = evmcs->io_bitmap_a;\n\t\tvmcs12->io_bitmap_b = evmcs->io_bitmap_b;\n\t}\n\n\tif (unlikely(!(hv_clean_fields &\n\t\t       HV_VMX_ENLIGHTENED_CLEAN_FIELD_MSR_BITMAP))) {\n\t\tvmcs12->msr_bitmap = evmcs->msr_bitmap;\n\t}\n\n\tif (unlikely(!(hv_clean_fields &\n\t\t       HV_VMX_ENLIGHTENED_CLEAN_FIELD_GUEST_GRP2))) {\n\t\tvmcs12->guest_es_base = evmcs->guest_es_base;\n\t\tvmcs12->guest_cs_base = evmcs->guest_cs_base;\n\t\tvmcs12->guest_ss_base = evmcs->guest_ss_base;\n\t\tvmcs12->guest_ds_base = evmcs->guest_ds_base;\n\t\tvmcs12->guest_fs_base = evmcs->guest_fs_base;\n\t\tvmcs12->guest_gs_base = evmcs->guest_gs_base;\n\t\tvmcs12->guest_ldtr_base = evmcs->guest_ldtr_base;\n\t\tvmcs12->guest_tr_base = evmcs->guest_tr_base;\n\t\tvmcs12->guest_gdtr_base = evmcs->guest_gdtr_base;\n\t\tvmcs12->guest_idtr_base = evmcs->guest_idtr_base;\n\t\tvmcs12->guest_es_limit = evmcs->guest_es_limit;\n\t\tvmcs12->guest_cs_limit = evmcs->guest_cs_limit;\n\t\tvmcs12->guest_ss_limit = evmcs->guest_ss_limit;\n\t\tvmcs12->guest_ds_limit = evmcs->guest_ds_limit;\n\t\tvmcs12->guest_fs_limit = evmcs->guest_fs_limit;\n\t\tvmcs12->guest_gs_limit = evmcs->guest_gs_limit;\n\t\tvmcs12->guest_ldtr_limit = evmcs->guest_ldtr_limit;\n\t\tvmcs12->guest_tr_limit = evmcs->guest_tr_limit;\n\t\tvmcs12->guest_gdtr_limit = evmcs->guest_gdtr_limit;\n\t\tvmcs12->guest_idtr_limit = evmcs->guest_idtr_limit;\n\t\tvmcs12->guest_es_ar_bytes = evmcs->guest_es_ar_bytes;\n\t\tvmcs12->guest_cs_ar_bytes = evmcs->guest_cs_ar_bytes;\n\t\tvmcs12->guest_ss_ar_bytes = evmcs->guest_ss_ar_bytes;\n\t\tvmcs12->guest_ds_ar_bytes = evmcs->guest_ds_ar_bytes;\n\t\tvmcs12->guest_fs_ar_bytes = evmcs->guest_fs_ar_bytes;\n\t\tvmcs12->guest_gs_ar_bytes = evmcs->guest_gs_ar_bytes;\n\t\tvmcs12->guest_ldtr_ar_bytes = evmcs->guest_ldtr_ar_bytes;\n\t\tvmcs12->guest_tr_ar_bytes = evmcs->guest_tr_ar_bytes;\n\t\tvmcs12->guest_es_selector = evmcs->guest_es_selector;\n\t\tvmcs12->guest_cs_selector = evmcs->guest_cs_selector;\n\t\tvmcs12->guest_ss_selector = evmcs->guest_ss_selector;\n\t\tvmcs12->guest_ds_selector = evmcs->guest_ds_selector;\n\t\tvmcs12->guest_fs_selector = evmcs->guest_fs_selector;\n\t\tvmcs12->guest_gs_selector = evmcs->guest_gs_selector;\n\t\tvmcs12->guest_ldtr_selector = evmcs->guest_ldtr_selector;\n\t\tvmcs12->guest_tr_selector = evmcs->guest_tr_selector;\n\t}\n\n\tif (unlikely(!(hv_clean_fields &\n\t\t       HV_VMX_ENLIGHTENED_CLEAN_FIELD_CONTROL_GRP2))) {\n\t\tvmcs12->tsc_offset = evmcs->tsc_offset;\n\t\tvmcs12->virtual_apic_page_addr = evmcs->virtual_apic_page_addr;\n\t\tvmcs12->xss_exit_bitmap = evmcs->xss_exit_bitmap;\n\t\tvmcs12->encls_exiting_bitmap = evmcs->encls_exiting_bitmap;\n\t\tvmcs12->tsc_multiplier = evmcs->tsc_multiplier;\n\t}\n\n\tif (unlikely(!(hv_clean_fields &\n\t\t       HV_VMX_ENLIGHTENED_CLEAN_FIELD_CRDR))) {\n\t\tvmcs12->cr0_guest_host_mask = evmcs->cr0_guest_host_mask;\n\t\tvmcs12->cr4_guest_host_mask = evmcs->cr4_guest_host_mask;\n\t\tvmcs12->cr0_read_shadow = evmcs->cr0_read_shadow;\n\t\tvmcs12->cr4_read_shadow = evmcs->cr4_read_shadow;\n\t\tvmcs12->guest_cr0 = evmcs->guest_cr0;\n\t\tvmcs12->guest_cr3 = evmcs->guest_cr3;\n\t\tvmcs12->guest_cr4 = evmcs->guest_cr4;\n\t\tvmcs12->guest_dr7 = evmcs->guest_dr7;\n\t}\n\n\tif (unlikely(!(hv_clean_fields &\n\t\t       HV_VMX_ENLIGHTENED_CLEAN_FIELD_HOST_POINTER))) {\n\t\tvmcs12->host_fs_base = evmcs->host_fs_base;\n\t\tvmcs12->host_gs_base = evmcs->host_gs_base;\n\t\tvmcs12->host_tr_base = evmcs->host_tr_base;\n\t\tvmcs12->host_gdtr_base = evmcs->host_gdtr_base;\n\t\tvmcs12->host_idtr_base = evmcs->host_idtr_base;\n\t\tvmcs12->host_rsp = evmcs->host_rsp;\n\t}\n\n\tif (unlikely(!(hv_clean_fields &\n\t\t       HV_VMX_ENLIGHTENED_CLEAN_FIELD_CONTROL_XLAT))) {\n\t\tvmcs12->ept_pointer = evmcs->ept_pointer;\n\t\tvmcs12->virtual_processor_id = evmcs->virtual_processor_id;\n\t}\n\n\tif (unlikely(!(hv_clean_fields &\n\t\t       HV_VMX_ENLIGHTENED_CLEAN_FIELD_GUEST_GRP1))) {\n\t\tvmcs12->vmcs_link_pointer = evmcs->vmcs_link_pointer;\n\t\tvmcs12->guest_ia32_debugctl = evmcs->guest_ia32_debugctl;\n\t\tvmcs12->guest_ia32_pat = evmcs->guest_ia32_pat;\n\t\tvmcs12->guest_ia32_efer = evmcs->guest_ia32_efer;\n\t\tvmcs12->guest_pdptr0 = evmcs->guest_pdptr0;\n\t\tvmcs12->guest_pdptr1 = evmcs->guest_pdptr1;\n\t\tvmcs12->guest_pdptr2 = evmcs->guest_pdptr2;\n\t\tvmcs12->guest_pdptr3 = evmcs->guest_pdptr3;\n\t\tvmcs12->guest_pending_dbg_exceptions =\n\t\t\tevmcs->guest_pending_dbg_exceptions;\n\t\tvmcs12->guest_sysenter_esp = evmcs->guest_sysenter_esp;\n\t\tvmcs12->guest_sysenter_eip = evmcs->guest_sysenter_eip;\n\t\tvmcs12->guest_bndcfgs = evmcs->guest_bndcfgs;\n\t\tvmcs12->guest_activity_state = evmcs->guest_activity_state;\n\t\tvmcs12->guest_sysenter_cs = evmcs->guest_sysenter_cs;\n\t\tvmcs12->guest_ia32_perf_global_ctrl = evmcs->guest_ia32_perf_global_ctrl;\n\t\t \n\t}\n\n\t \n\n\t \n\n\treturn;\n}\n\nstatic void copy_vmcs12_to_enlightened(struct vcpu_vmx *vmx)\n{\n\tstruct vmcs12 *vmcs12 = vmx->nested.cached_vmcs12;\n\tstruct hv_enlightened_vmcs *evmcs = vmx->nested.hv_evmcs;\n\n\t \n\n\tevmcs->guest_es_selector = vmcs12->guest_es_selector;\n\tevmcs->guest_cs_selector = vmcs12->guest_cs_selector;\n\tevmcs->guest_ss_selector = vmcs12->guest_ss_selector;\n\tevmcs->guest_ds_selector = vmcs12->guest_ds_selector;\n\tevmcs->guest_fs_selector = vmcs12->guest_fs_selector;\n\tevmcs->guest_gs_selector = vmcs12->guest_gs_selector;\n\tevmcs->guest_ldtr_selector = vmcs12->guest_ldtr_selector;\n\tevmcs->guest_tr_selector = vmcs12->guest_tr_selector;\n\n\tevmcs->guest_es_limit = vmcs12->guest_es_limit;\n\tevmcs->guest_cs_limit = vmcs12->guest_cs_limit;\n\tevmcs->guest_ss_limit = vmcs12->guest_ss_limit;\n\tevmcs->guest_ds_limit = vmcs12->guest_ds_limit;\n\tevmcs->guest_fs_limit = vmcs12->guest_fs_limit;\n\tevmcs->guest_gs_limit = vmcs12->guest_gs_limit;\n\tevmcs->guest_ldtr_limit = vmcs12->guest_ldtr_limit;\n\tevmcs->guest_tr_limit = vmcs12->guest_tr_limit;\n\tevmcs->guest_gdtr_limit = vmcs12->guest_gdtr_limit;\n\tevmcs->guest_idtr_limit = vmcs12->guest_idtr_limit;\n\n\tevmcs->guest_es_ar_bytes = vmcs12->guest_es_ar_bytes;\n\tevmcs->guest_cs_ar_bytes = vmcs12->guest_cs_ar_bytes;\n\tevmcs->guest_ss_ar_bytes = vmcs12->guest_ss_ar_bytes;\n\tevmcs->guest_ds_ar_bytes = vmcs12->guest_ds_ar_bytes;\n\tevmcs->guest_fs_ar_bytes = vmcs12->guest_fs_ar_bytes;\n\tevmcs->guest_gs_ar_bytes = vmcs12->guest_gs_ar_bytes;\n\tevmcs->guest_ldtr_ar_bytes = vmcs12->guest_ldtr_ar_bytes;\n\tevmcs->guest_tr_ar_bytes = vmcs12->guest_tr_ar_bytes;\n\n\tevmcs->guest_es_base = vmcs12->guest_es_base;\n\tevmcs->guest_cs_base = vmcs12->guest_cs_base;\n\tevmcs->guest_ss_base = vmcs12->guest_ss_base;\n\tevmcs->guest_ds_base = vmcs12->guest_ds_base;\n\tevmcs->guest_fs_base = vmcs12->guest_fs_base;\n\tevmcs->guest_gs_base = vmcs12->guest_gs_base;\n\tevmcs->guest_ldtr_base = vmcs12->guest_ldtr_base;\n\tevmcs->guest_tr_base = vmcs12->guest_tr_base;\n\tevmcs->guest_gdtr_base = vmcs12->guest_gdtr_base;\n\tevmcs->guest_idtr_base = vmcs12->guest_idtr_base;\n\n\tevmcs->guest_ia32_pat = vmcs12->guest_ia32_pat;\n\tevmcs->guest_ia32_efer = vmcs12->guest_ia32_efer;\n\n\tevmcs->guest_pdptr0 = vmcs12->guest_pdptr0;\n\tevmcs->guest_pdptr1 = vmcs12->guest_pdptr1;\n\tevmcs->guest_pdptr2 = vmcs12->guest_pdptr2;\n\tevmcs->guest_pdptr3 = vmcs12->guest_pdptr3;\n\n\tevmcs->guest_pending_dbg_exceptions =\n\t\tvmcs12->guest_pending_dbg_exceptions;\n\tevmcs->guest_sysenter_esp = vmcs12->guest_sysenter_esp;\n\tevmcs->guest_sysenter_eip = vmcs12->guest_sysenter_eip;\n\n\tevmcs->guest_activity_state = vmcs12->guest_activity_state;\n\tevmcs->guest_sysenter_cs = vmcs12->guest_sysenter_cs;\n\n\tevmcs->guest_cr0 = vmcs12->guest_cr0;\n\tevmcs->guest_cr3 = vmcs12->guest_cr3;\n\tevmcs->guest_cr4 = vmcs12->guest_cr4;\n\tevmcs->guest_dr7 = vmcs12->guest_dr7;\n\n\tevmcs->guest_physical_address = vmcs12->guest_physical_address;\n\n\tevmcs->vm_instruction_error = vmcs12->vm_instruction_error;\n\tevmcs->vm_exit_reason = vmcs12->vm_exit_reason;\n\tevmcs->vm_exit_intr_info = vmcs12->vm_exit_intr_info;\n\tevmcs->vm_exit_intr_error_code = vmcs12->vm_exit_intr_error_code;\n\tevmcs->idt_vectoring_info_field = vmcs12->idt_vectoring_info_field;\n\tevmcs->idt_vectoring_error_code = vmcs12->idt_vectoring_error_code;\n\tevmcs->vm_exit_instruction_len = vmcs12->vm_exit_instruction_len;\n\tevmcs->vmx_instruction_info = vmcs12->vmx_instruction_info;\n\n\tevmcs->exit_qualification = vmcs12->exit_qualification;\n\n\tevmcs->guest_linear_address = vmcs12->guest_linear_address;\n\tevmcs->guest_rsp = vmcs12->guest_rsp;\n\tevmcs->guest_rflags = vmcs12->guest_rflags;\n\n\tevmcs->guest_interruptibility_info =\n\t\tvmcs12->guest_interruptibility_info;\n\tevmcs->cpu_based_vm_exec_control = vmcs12->cpu_based_vm_exec_control;\n\tevmcs->vm_entry_controls = vmcs12->vm_entry_controls;\n\tevmcs->vm_entry_intr_info_field = vmcs12->vm_entry_intr_info_field;\n\tevmcs->vm_entry_exception_error_code =\n\t\tvmcs12->vm_entry_exception_error_code;\n\tevmcs->vm_entry_instruction_len = vmcs12->vm_entry_instruction_len;\n\n\tevmcs->guest_rip = vmcs12->guest_rip;\n\n\tevmcs->guest_bndcfgs = vmcs12->guest_bndcfgs;\n\n\treturn;\n}\n\n \nstatic enum nested_evmptrld_status nested_vmx_handle_enlightened_vmptrld(\n\tstruct kvm_vcpu *vcpu, bool from_launch)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tbool evmcs_gpa_changed = false;\n\tu64 evmcs_gpa;\n\n\tif (likely(!guest_cpuid_has_evmcs(vcpu)))\n\t\treturn EVMPTRLD_DISABLED;\n\n\tevmcs_gpa = nested_get_evmptr(vcpu);\n\tif (!evmptr_is_valid(evmcs_gpa)) {\n\t\tnested_release_evmcs(vcpu);\n\t\treturn EVMPTRLD_DISABLED;\n\t}\n\n\tif (unlikely(evmcs_gpa != vmx->nested.hv_evmcs_vmptr)) {\n\t\tvmx->nested.current_vmptr = INVALID_GPA;\n\n\t\tnested_release_evmcs(vcpu);\n\n\t\tif (kvm_vcpu_map(vcpu, gpa_to_gfn(evmcs_gpa),\n\t\t\t\t &vmx->nested.hv_evmcs_map))\n\t\t\treturn EVMPTRLD_ERROR;\n\n\t\tvmx->nested.hv_evmcs = vmx->nested.hv_evmcs_map.hva;\n\n\t\t \n\t\tif ((vmx->nested.hv_evmcs->revision_id != KVM_EVMCS_VERSION) &&\n\t\t    (vmx->nested.hv_evmcs->revision_id != VMCS12_REVISION)) {\n\t\t\tnested_release_evmcs(vcpu);\n\t\t\treturn EVMPTRLD_VMFAIL;\n\t\t}\n\n\t\tvmx->nested.hv_evmcs_vmptr = evmcs_gpa;\n\n\t\tevmcs_gpa_changed = true;\n\t\t \n\t\tif (from_launch) {\n\t\t\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\t\t\tmemset(vmcs12, 0, sizeof(*vmcs12));\n\t\t\tvmcs12->hdr.revision_id = VMCS12_REVISION;\n\t\t}\n\n\t}\n\n\t \n\tif (from_launch || evmcs_gpa_changed) {\n\t\tvmx->nested.hv_evmcs->hv_clean_fields &=\n\t\t\t~HV_VMX_ENLIGHTENED_CLEAN_FIELD_ALL;\n\n\t\tvmx->nested.force_msr_bitmap_recalc = true;\n\t}\n\n\treturn EVMPTRLD_SUCCEEDED;\n}\n\nvoid nested_sync_vmcs12_to_shadow(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (evmptr_is_valid(vmx->nested.hv_evmcs_vmptr))\n\t\tcopy_vmcs12_to_enlightened(vmx);\n\telse\n\t\tcopy_vmcs12_to_shadow(vmx);\n\n\tvmx->nested.need_vmcs12_to_shadow_sync = false;\n}\n\nstatic enum hrtimer_restart vmx_preemption_timer_fn(struct hrtimer *timer)\n{\n\tstruct vcpu_vmx *vmx =\n\t\tcontainer_of(timer, struct vcpu_vmx, nested.preemption_timer);\n\n\tvmx->nested.preemption_timer_expired = true;\n\tkvm_make_request(KVM_REQ_EVENT, &vmx->vcpu);\n\tkvm_vcpu_kick(&vmx->vcpu);\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic u64 vmx_calc_preemption_timer_value(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\tu64 l1_scaled_tsc = kvm_read_l1_tsc(vcpu, rdtsc()) >>\n\t\t\t    VMX_MISC_EMULATED_PREEMPTION_TIMER_RATE;\n\n\tif (!vmx->nested.has_preemption_timer_deadline) {\n\t\tvmx->nested.preemption_timer_deadline =\n\t\t\tvmcs12->vmx_preemption_timer_value + l1_scaled_tsc;\n\t\tvmx->nested.has_preemption_timer_deadline = true;\n\t}\n\treturn vmx->nested.preemption_timer_deadline - l1_scaled_tsc;\n}\n\nstatic void vmx_start_preemption_timer(struct kvm_vcpu *vcpu,\n\t\t\t\t\tu64 preemption_timeout)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t \n\tif (preemption_timeout == 0) {\n\t\tvmx_preemption_timer_fn(&vmx->nested.preemption_timer);\n\t\treturn;\n\t}\n\n\tif (vcpu->arch.virtual_tsc_khz == 0)\n\t\treturn;\n\n\tpreemption_timeout <<= VMX_MISC_EMULATED_PREEMPTION_TIMER_RATE;\n\tpreemption_timeout *= 1000000;\n\tdo_div(preemption_timeout, vcpu->arch.virtual_tsc_khz);\n\thrtimer_start(&vmx->nested.preemption_timer,\n\t\t      ktime_add_ns(ktime_get(), preemption_timeout),\n\t\t      HRTIMER_MODE_ABS_PINNED);\n}\n\nstatic u64 nested_vmx_calc_efer(struct vcpu_vmx *vmx, struct vmcs12 *vmcs12)\n{\n\tif (vmx->nested.nested_run_pending &&\n\t    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_IA32_EFER))\n\t\treturn vmcs12->guest_ia32_efer;\n\telse if (vmcs12->vm_entry_controls & VM_ENTRY_IA32E_MODE)\n\t\treturn vmx->vcpu.arch.efer | (EFER_LMA | EFER_LME);\n\telse\n\t\treturn vmx->vcpu.arch.efer & ~(EFER_LMA | EFER_LME);\n}\n\nstatic void prepare_vmcs02_constant_state(struct vcpu_vmx *vmx)\n{\n\tstruct kvm *kvm = vmx->vcpu.kvm;\n\n\t \n\tif (vmx->nested.vmcs02_initialized)\n\t\treturn;\n\tvmx->nested.vmcs02_initialized = true;\n\n\t \n\tif (enable_ept && nested_early_check)\n\t\tvmcs_write64(EPT_POINTER,\n\t\t\t     construct_eptp(&vmx->vcpu, 0, PT64_ROOT_4LEVEL));\n\n\t \n\tif (cpu_has_vmx_vmfunc())\n\t\tvmcs_write64(VM_FUNCTION_CONTROL, 0);\n\n\tif (cpu_has_vmx_posted_intr())\n\t\tvmcs_write16(POSTED_INTR_NV, POSTED_INTR_NESTED_VECTOR);\n\n\tif (cpu_has_vmx_msr_bitmap())\n\t\tvmcs_write64(MSR_BITMAP, __pa(vmx->nested.vmcs02.msr_bitmap));\n\n\t \n\tif (enable_pml) {\n\t\tvmcs_write64(PML_ADDRESS, 0);\n\t\tvmcs_write16(GUEST_PML_INDEX, -1);\n\t}\n\n\tif (cpu_has_vmx_encls_vmexit())\n\t\tvmcs_write64(ENCLS_EXITING_BITMAP, INVALID_GPA);\n\n\tif (kvm_notify_vmexit_enabled(kvm))\n\t\tvmcs_write32(NOTIFY_WINDOW, kvm->arch.notify_window);\n\n\t \n\tvmcs_write64(VM_EXIT_MSR_STORE_ADDR, __pa(vmx->msr_autostore.guest.val));\n\tvmcs_write64(VM_EXIT_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.host.val));\n\tvmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, __pa(vmx->msr_autoload.guest.val));\n\n\tvmx_set_constant_host_state(vmx);\n}\n\nstatic void prepare_vmcs02_early_rare(struct vcpu_vmx *vmx,\n\t\t\t\t      struct vmcs12 *vmcs12)\n{\n\tprepare_vmcs02_constant_state(vmx);\n\n\tvmcs_write64(VMCS_LINK_POINTER, INVALID_GPA);\n\n\tif (enable_vpid) {\n\t\tif (nested_cpu_has_vpid(vmcs12) && vmx->nested.vpid02)\n\t\t\tvmcs_write16(VIRTUAL_PROCESSOR_ID, vmx->nested.vpid02);\n\t\telse\n\t\t\tvmcs_write16(VIRTUAL_PROCESSOR_ID, vmx->vpid);\n\t}\n}\n\nstatic void prepare_vmcs02_early(struct vcpu_vmx *vmx, struct loaded_vmcs *vmcs01,\n\t\t\t\t struct vmcs12 *vmcs12)\n{\n\tu32 exec_control;\n\tu64 guest_efer = nested_vmx_calc_efer(vmx, vmcs12);\n\n\tif (vmx->nested.dirty_vmcs12 || evmptr_is_valid(vmx->nested.hv_evmcs_vmptr))\n\t\tprepare_vmcs02_early_rare(vmx, vmcs12);\n\n\t \n\texec_control = __pin_controls_get(vmcs01);\n\texec_control |= (vmcs12->pin_based_vm_exec_control &\n\t\t\t ~PIN_BASED_VMX_PREEMPTION_TIMER);\n\n\t \n\tvmx->nested.pi_pending = false;\n\tif (nested_cpu_has_posted_intr(vmcs12))\n\t\tvmx->nested.posted_intr_nv = vmcs12->posted_intr_nv;\n\telse\n\t\texec_control &= ~PIN_BASED_POSTED_INTR;\n\tpin_controls_set(vmx, exec_control);\n\n\t \n\texec_control = __exec_controls_get(vmcs01);  \n\texec_control &= ~CPU_BASED_INTR_WINDOW_EXITING;\n\texec_control &= ~CPU_BASED_NMI_WINDOW_EXITING;\n\texec_control &= ~CPU_BASED_TPR_SHADOW;\n\texec_control |= vmcs12->cpu_based_vm_exec_control;\n\n\tvmx->nested.l1_tpr_threshold = -1;\n\tif (exec_control & CPU_BASED_TPR_SHADOW)\n\t\tvmcs_write32(TPR_THRESHOLD, vmcs12->tpr_threshold);\n#ifdef CONFIG_X86_64\n\telse\n\t\texec_control |= CPU_BASED_CR8_LOAD_EXITING |\n\t\t\t\tCPU_BASED_CR8_STORE_EXITING;\n#endif\n\n\t \n\texec_control |= CPU_BASED_UNCOND_IO_EXITING;\n\texec_control &= ~CPU_BASED_USE_IO_BITMAPS;\n\n\t \n\texec_control &= ~CPU_BASED_USE_MSR_BITMAPS;\n\texec_control |= exec_controls_get(vmx) & CPU_BASED_USE_MSR_BITMAPS;\n\n\texec_controls_set(vmx, exec_control);\n\n\t \n\tif (cpu_has_secondary_exec_ctrls()) {\n\t\texec_control = __secondary_exec_controls_get(vmcs01);\n\n\t\t \n\t\texec_control &= ~(SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES |\n\t\t\t\t  SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |\n\t\t\t\t  SECONDARY_EXEC_ENABLE_INVPCID |\n\t\t\t\t  SECONDARY_EXEC_ENABLE_RDTSCP |\n\t\t\t\t  SECONDARY_EXEC_ENABLE_XSAVES |\n\t\t\t\t  SECONDARY_EXEC_ENABLE_USR_WAIT_PAUSE |\n\t\t\t\t  SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |\n\t\t\t\t  SECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\t\t\t  SECONDARY_EXEC_ENABLE_VMFUNC |\n\t\t\t\t  SECONDARY_EXEC_DESC);\n\n\t\tif (nested_cpu_has(vmcs12,\n\t\t\t\t   CPU_BASED_ACTIVATE_SECONDARY_CONTROLS))\n\t\t\texec_control |= vmcs12->secondary_vm_exec_control;\n\n\t\t \n\t\texec_control &= ~SECONDARY_EXEC_ENABLE_PML;\n\n\t\t \n\t\texec_control &= ~SECONDARY_EXEC_SHADOW_VMCS;\n\n\t\t \n\t\tif (vmx_umip_emulated() && (vmcs12->guest_cr4 & X86_CR4_UMIP))\n\t\t\texec_control |= SECONDARY_EXEC_DESC;\n\n\t\tif (exec_control & SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY)\n\t\t\tvmcs_write16(GUEST_INTR_STATUS,\n\t\t\t\tvmcs12->guest_intr_status);\n\n\t\tif (!nested_cpu_has2(vmcs12, SECONDARY_EXEC_UNRESTRICTED_GUEST))\n\t\t    exec_control &= ~SECONDARY_EXEC_UNRESTRICTED_GUEST;\n\n\t\tif (exec_control & SECONDARY_EXEC_ENCLS_EXITING)\n\t\t\tvmx_write_encls_bitmap(&vmx->vcpu, vmcs12);\n\n\t\tsecondary_exec_controls_set(vmx, exec_control);\n\t}\n\n\t \n\texec_control = __vm_entry_controls_get(vmcs01);\n\texec_control |= (vmcs12->vm_entry_controls &\n\t\t\t ~VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL);\n\texec_control &= ~(VM_ENTRY_IA32E_MODE | VM_ENTRY_LOAD_IA32_EFER);\n\tif (cpu_has_load_ia32_efer()) {\n\t\tif (guest_efer & EFER_LMA)\n\t\t\texec_control |= VM_ENTRY_IA32E_MODE;\n\t\tif (guest_efer != host_efer)\n\t\t\texec_control |= VM_ENTRY_LOAD_IA32_EFER;\n\t}\n\tvm_entry_controls_set(vmx, exec_control);\n\n\t \n\texec_control = __vm_exit_controls_get(vmcs01);\n\tif (cpu_has_load_ia32_efer() && guest_efer != host_efer)\n\t\texec_control |= VM_EXIT_LOAD_IA32_EFER;\n\telse\n\t\texec_control &= ~VM_EXIT_LOAD_IA32_EFER;\n\tvm_exit_controls_set(vmx, exec_control);\n\n\t \n\tif (vmx->nested.nested_run_pending) {\n\t\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD,\n\t\t\t     vmcs12->vm_entry_intr_info_field);\n\t\tvmcs_write32(VM_ENTRY_EXCEPTION_ERROR_CODE,\n\t\t\t     vmcs12->vm_entry_exception_error_code);\n\t\tvmcs_write32(VM_ENTRY_INSTRUCTION_LEN,\n\t\t\t     vmcs12->vm_entry_instruction_len);\n\t\tvmcs_write32(GUEST_INTERRUPTIBILITY_INFO,\n\t\t\t     vmcs12->guest_interruptibility_info);\n\t\tvmx->loaded_vmcs->nmi_known_unmasked =\n\t\t\t!(vmcs12->guest_interruptibility_info & GUEST_INTR_STATE_NMI);\n\t} else {\n\t\tvmcs_write32(VM_ENTRY_INTR_INFO_FIELD, 0);\n\t}\n}\n\nstatic void prepare_vmcs02_rare(struct vcpu_vmx *vmx, struct vmcs12 *vmcs12)\n{\n\tstruct hv_enlightened_vmcs *hv_evmcs = vmx->nested.hv_evmcs;\n\n\tif (!hv_evmcs || !(hv_evmcs->hv_clean_fields &\n\t\t\t   HV_VMX_ENLIGHTENED_CLEAN_FIELD_GUEST_GRP2)) {\n\t\tvmcs_write16(GUEST_ES_SELECTOR, vmcs12->guest_es_selector);\n\t\tvmcs_write16(GUEST_CS_SELECTOR, vmcs12->guest_cs_selector);\n\t\tvmcs_write16(GUEST_SS_SELECTOR, vmcs12->guest_ss_selector);\n\t\tvmcs_write16(GUEST_DS_SELECTOR, vmcs12->guest_ds_selector);\n\t\tvmcs_write16(GUEST_FS_SELECTOR, vmcs12->guest_fs_selector);\n\t\tvmcs_write16(GUEST_GS_SELECTOR, vmcs12->guest_gs_selector);\n\t\tvmcs_write16(GUEST_LDTR_SELECTOR, vmcs12->guest_ldtr_selector);\n\t\tvmcs_write16(GUEST_TR_SELECTOR, vmcs12->guest_tr_selector);\n\t\tvmcs_write32(GUEST_ES_LIMIT, vmcs12->guest_es_limit);\n\t\tvmcs_write32(GUEST_CS_LIMIT, vmcs12->guest_cs_limit);\n\t\tvmcs_write32(GUEST_SS_LIMIT, vmcs12->guest_ss_limit);\n\t\tvmcs_write32(GUEST_DS_LIMIT, vmcs12->guest_ds_limit);\n\t\tvmcs_write32(GUEST_FS_LIMIT, vmcs12->guest_fs_limit);\n\t\tvmcs_write32(GUEST_GS_LIMIT, vmcs12->guest_gs_limit);\n\t\tvmcs_write32(GUEST_LDTR_LIMIT, vmcs12->guest_ldtr_limit);\n\t\tvmcs_write32(GUEST_TR_LIMIT, vmcs12->guest_tr_limit);\n\t\tvmcs_write32(GUEST_GDTR_LIMIT, vmcs12->guest_gdtr_limit);\n\t\tvmcs_write32(GUEST_IDTR_LIMIT, vmcs12->guest_idtr_limit);\n\t\tvmcs_write32(GUEST_CS_AR_BYTES, vmcs12->guest_cs_ar_bytes);\n\t\tvmcs_write32(GUEST_SS_AR_BYTES, vmcs12->guest_ss_ar_bytes);\n\t\tvmcs_write32(GUEST_ES_AR_BYTES, vmcs12->guest_es_ar_bytes);\n\t\tvmcs_write32(GUEST_DS_AR_BYTES, vmcs12->guest_ds_ar_bytes);\n\t\tvmcs_write32(GUEST_FS_AR_BYTES, vmcs12->guest_fs_ar_bytes);\n\t\tvmcs_write32(GUEST_GS_AR_BYTES, vmcs12->guest_gs_ar_bytes);\n\t\tvmcs_write32(GUEST_LDTR_AR_BYTES, vmcs12->guest_ldtr_ar_bytes);\n\t\tvmcs_write32(GUEST_TR_AR_BYTES, vmcs12->guest_tr_ar_bytes);\n\t\tvmcs_writel(GUEST_ES_BASE, vmcs12->guest_es_base);\n\t\tvmcs_writel(GUEST_CS_BASE, vmcs12->guest_cs_base);\n\t\tvmcs_writel(GUEST_SS_BASE, vmcs12->guest_ss_base);\n\t\tvmcs_writel(GUEST_DS_BASE, vmcs12->guest_ds_base);\n\t\tvmcs_writel(GUEST_FS_BASE, vmcs12->guest_fs_base);\n\t\tvmcs_writel(GUEST_GS_BASE, vmcs12->guest_gs_base);\n\t\tvmcs_writel(GUEST_LDTR_BASE, vmcs12->guest_ldtr_base);\n\t\tvmcs_writel(GUEST_TR_BASE, vmcs12->guest_tr_base);\n\t\tvmcs_writel(GUEST_GDTR_BASE, vmcs12->guest_gdtr_base);\n\t\tvmcs_writel(GUEST_IDTR_BASE, vmcs12->guest_idtr_base);\n\n\t\tvmx->segment_cache.bitmask = 0;\n\t}\n\n\tif (!hv_evmcs || !(hv_evmcs->hv_clean_fields &\n\t\t\t   HV_VMX_ENLIGHTENED_CLEAN_FIELD_GUEST_GRP1)) {\n\t\tvmcs_write32(GUEST_SYSENTER_CS, vmcs12->guest_sysenter_cs);\n\t\tvmcs_writel(GUEST_PENDING_DBG_EXCEPTIONS,\n\t\t\t    vmcs12->guest_pending_dbg_exceptions);\n\t\tvmcs_writel(GUEST_SYSENTER_ESP, vmcs12->guest_sysenter_esp);\n\t\tvmcs_writel(GUEST_SYSENTER_EIP, vmcs12->guest_sysenter_eip);\n\n\t\t \n\t\tif (enable_ept) {\n\t\t\tvmcs_write64(GUEST_PDPTR0, vmcs12->guest_pdptr0);\n\t\t\tvmcs_write64(GUEST_PDPTR1, vmcs12->guest_pdptr1);\n\t\t\tvmcs_write64(GUEST_PDPTR2, vmcs12->guest_pdptr2);\n\t\t\tvmcs_write64(GUEST_PDPTR3, vmcs12->guest_pdptr3);\n\t\t}\n\n\t\tif (kvm_mpx_supported() && vmx->nested.nested_run_pending &&\n\t\t    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_BNDCFGS))\n\t\t\tvmcs_write64(GUEST_BNDCFGS, vmcs12->guest_bndcfgs);\n\t}\n\n\tif (nested_cpu_has_xsaves(vmcs12))\n\t\tvmcs_write64(XSS_EXIT_BITMAP, vmcs12->xss_exit_bitmap);\n\n\t \n\tif (vmx_need_pf_intercept(&vmx->vcpu)) {\n\t\t \n\t\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, 0);\n\t\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, 0);\n\t} else {\n\t\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, vmcs12->page_fault_error_code_mask);\n\t\tvmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, vmcs12->page_fault_error_code_match);\n\t}\n\n\tif (cpu_has_vmx_apicv()) {\n\t\tvmcs_write64(EOI_EXIT_BITMAP0, vmcs12->eoi_exit_bitmap0);\n\t\tvmcs_write64(EOI_EXIT_BITMAP1, vmcs12->eoi_exit_bitmap1);\n\t\tvmcs_write64(EOI_EXIT_BITMAP2, vmcs12->eoi_exit_bitmap2);\n\t\tvmcs_write64(EOI_EXIT_BITMAP3, vmcs12->eoi_exit_bitmap3);\n\t}\n\n\t \n\tprepare_vmx_msr_autostore_list(&vmx->vcpu, MSR_IA32_TSC);\n\n\tvmcs_write32(VM_EXIT_MSR_STORE_COUNT, vmx->msr_autostore.guest.nr);\n\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx->msr_autoload.host.nr);\n\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx->msr_autoload.guest.nr);\n\n\tset_cr4_guest_host_mask(vmx);\n}\n\n \nstatic int prepare_vmcs02(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,\n\t\t\t  bool from_vmentry,\n\t\t\t  enum vm_entry_failure_code *entry_failure_code)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tbool load_guest_pdptrs_vmcs12 = false;\n\n\tif (vmx->nested.dirty_vmcs12 || evmptr_is_valid(vmx->nested.hv_evmcs_vmptr)) {\n\t\tprepare_vmcs02_rare(vmx, vmcs12);\n\t\tvmx->nested.dirty_vmcs12 = false;\n\n\t\tload_guest_pdptrs_vmcs12 = !evmptr_is_valid(vmx->nested.hv_evmcs_vmptr) ||\n\t\t\t!(vmx->nested.hv_evmcs->hv_clean_fields &\n\t\t\t  HV_VMX_ENLIGHTENED_CLEAN_FIELD_GUEST_GRP1);\n\t}\n\n\tif (vmx->nested.nested_run_pending &&\n\t    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_DEBUG_CONTROLS)) {\n\t\tkvm_set_dr(vcpu, 7, vmcs12->guest_dr7);\n\t\tvmcs_write64(GUEST_IA32_DEBUGCTL, vmcs12->guest_ia32_debugctl);\n\t} else {\n\t\tkvm_set_dr(vcpu, 7, vcpu->arch.dr7);\n\t\tvmcs_write64(GUEST_IA32_DEBUGCTL, vmx->nested.pre_vmenter_debugctl);\n\t}\n\tif (kvm_mpx_supported() && (!vmx->nested.nested_run_pending ||\n\t    !(vmcs12->vm_entry_controls & VM_ENTRY_LOAD_BNDCFGS)))\n\t\tvmcs_write64(GUEST_BNDCFGS, vmx->nested.pre_vmenter_bndcfgs);\n\tvmx_set_rflags(vcpu, vmcs12->guest_rflags);\n\n\t \n\tvmx_update_exception_bitmap(vcpu);\n\tvcpu->arch.cr0_guest_owned_bits &= ~vmcs12->cr0_guest_host_mask;\n\tvmcs_writel(CR0_GUEST_HOST_MASK, ~vcpu->arch.cr0_guest_owned_bits);\n\n\tif (vmx->nested.nested_run_pending &&\n\t    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_IA32_PAT)) {\n\t\tvmcs_write64(GUEST_IA32_PAT, vmcs12->guest_ia32_pat);\n\t\tvcpu->arch.pat = vmcs12->guest_ia32_pat;\n\t} else if (vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PAT) {\n\t\tvmcs_write64(GUEST_IA32_PAT, vmx->vcpu.arch.pat);\n\t}\n\n\tvcpu->arch.tsc_offset = kvm_calc_nested_tsc_offset(\n\t\t\tvcpu->arch.l1_tsc_offset,\n\t\t\tvmx_get_l2_tsc_offset(vcpu),\n\t\t\tvmx_get_l2_tsc_multiplier(vcpu));\n\n\tvcpu->arch.tsc_scaling_ratio = kvm_calc_nested_tsc_multiplier(\n\t\t\tvcpu->arch.l1_tsc_scaling_ratio,\n\t\t\tvmx_get_l2_tsc_multiplier(vcpu));\n\n\tvmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);\n\tif (kvm_caps.has_tsc_control)\n\t\tvmcs_write64(TSC_MULTIPLIER, vcpu->arch.tsc_scaling_ratio);\n\n\tnested_vmx_transition_tlb_flush(vcpu, vmcs12, true);\n\n\tif (nested_cpu_has_ept(vmcs12))\n\t\tnested_ept_init_mmu_context(vcpu);\n\n\t \n\tvmx_set_cr0(vcpu, vmcs12->guest_cr0);\n\tvmcs_writel(CR0_READ_SHADOW, nested_read_cr0(vmcs12));\n\n\tvmx_set_cr4(vcpu, vmcs12->guest_cr4);\n\tvmcs_writel(CR4_READ_SHADOW, nested_read_cr4(vmcs12));\n\n\tvcpu->arch.efer = nested_vmx_calc_efer(vmx, vmcs12);\n\t \n\tvmx_set_efer(vcpu, vcpu->arch.efer);\n\n\t \n\tif (CC(from_vmentry && !vmx_guest_state_valid(vcpu))) {\n\t\t*entry_failure_code = ENTRY_FAIL_DEFAULT;\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (nested_vmx_load_cr3(vcpu, vmcs12->guest_cr3, nested_cpu_has_ept(vmcs12),\n\t\t\t\tfrom_vmentry, entry_failure_code))\n\t\treturn -EINVAL;\n\n\t \n\tif (enable_ept)\n\t\tvmcs_writel(GUEST_CR3, vmcs12->guest_cr3);\n\n\t \n\tif (load_guest_pdptrs_vmcs12 && nested_cpu_has_ept(vmcs12) &&\n\t    is_pae_paging(vcpu)) {\n\t\tvmcs_write64(GUEST_PDPTR0, vmcs12->guest_pdptr0);\n\t\tvmcs_write64(GUEST_PDPTR1, vmcs12->guest_pdptr1);\n\t\tvmcs_write64(GUEST_PDPTR2, vmcs12->guest_pdptr2);\n\t\tvmcs_write64(GUEST_PDPTR3, vmcs12->guest_pdptr3);\n\t}\n\n\tif ((vmcs12->vm_entry_controls & VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL) &&\n\t    kvm_pmu_has_perf_global_ctrl(vcpu_to_pmu(vcpu)) &&\n\t    WARN_ON_ONCE(kvm_set_msr(vcpu, MSR_CORE_PERF_GLOBAL_CTRL,\n\t\t\t\t     vmcs12->guest_ia32_perf_global_ctrl))) {\n\t\t*entry_failure_code = ENTRY_FAIL_DEFAULT;\n\t\treturn -EINVAL;\n\t}\n\n\tkvm_rsp_write(vcpu, vmcs12->guest_rsp);\n\tkvm_rip_write(vcpu, vmcs12->guest_rip);\n\n\t \n\tif (evmptr_is_valid(vmx->nested.hv_evmcs_vmptr))\n\t\tvmx->nested.hv_evmcs->hv_clean_fields |=\n\t\t\tHV_VMX_ENLIGHTENED_CLEAN_FIELD_ALL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_nmi_controls(struct vmcs12 *vmcs12)\n{\n\tif (CC(!nested_cpu_has_nmi_exiting(vmcs12) &&\n\t       nested_cpu_has_virtual_nmis(vmcs12)))\n\t\treturn -EINVAL;\n\n\tif (CC(!nested_cpu_has_virtual_nmis(vmcs12) &&\n\t       nested_cpu_has(vmcs12, CPU_BASED_NMI_WINDOW_EXITING)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic bool nested_vmx_check_eptp(struct kvm_vcpu *vcpu, u64 new_eptp)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t \n\tswitch (new_eptp & VMX_EPTP_MT_MASK) {\n\tcase VMX_EPTP_MT_UC:\n\t\tif (CC(!(vmx->nested.msrs.ept_caps & VMX_EPTP_UC_BIT)))\n\t\t\treturn false;\n\t\tbreak;\n\tcase VMX_EPTP_MT_WB:\n\t\tif (CC(!(vmx->nested.msrs.ept_caps & VMX_EPTP_WB_BIT)))\n\t\t\treturn false;\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\t \n\tswitch (new_eptp & VMX_EPTP_PWL_MASK) {\n\tcase VMX_EPTP_PWL_5:\n\t\tif (CC(!(vmx->nested.msrs.ept_caps & VMX_EPT_PAGE_WALK_5_BIT)))\n\t\t\treturn false;\n\t\tbreak;\n\tcase VMX_EPTP_PWL_4:\n\t\tif (CC(!(vmx->nested.msrs.ept_caps & VMX_EPT_PAGE_WALK_4_BIT)))\n\t\t\treturn false;\n\t\tbreak;\n\tdefault:\n\t\treturn false;\n\t}\n\n\t \n\tif (CC(kvm_vcpu_is_illegal_gpa(vcpu, new_eptp) || ((new_eptp >> 7) & 0x1f)))\n\t\treturn false;\n\n\t \n\tif (new_eptp & VMX_EPTP_AD_ENABLE_BIT) {\n\t\tif (CC(!(vmx->nested.msrs.ept_caps & VMX_EPT_AD_BIT)))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nstatic int nested_check_vm_execution_controls(struct kvm_vcpu *vcpu,\n                                              struct vmcs12 *vmcs12)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (CC(!vmx_control_verify(vmcs12->pin_based_vm_exec_control,\n\t\t\t\t   vmx->nested.msrs.pinbased_ctls_low,\n\t\t\t\t   vmx->nested.msrs.pinbased_ctls_high)) ||\n\t    CC(!vmx_control_verify(vmcs12->cpu_based_vm_exec_control,\n\t\t\t\t   vmx->nested.msrs.procbased_ctls_low,\n\t\t\t\t   vmx->nested.msrs.procbased_ctls_high)))\n\t\treturn -EINVAL;\n\n\tif (nested_cpu_has(vmcs12, CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) &&\n\t    CC(!vmx_control_verify(vmcs12->secondary_vm_exec_control,\n\t\t\t\t   vmx->nested.msrs.secondary_ctls_low,\n\t\t\t\t   vmx->nested.msrs.secondary_ctls_high)))\n\t\treturn -EINVAL;\n\n\tif (CC(vmcs12->cr3_target_count > nested_cpu_vmx_misc_cr3_count(vcpu)) ||\n\t    nested_vmx_check_io_bitmap_controls(vcpu, vmcs12) ||\n\t    nested_vmx_check_msr_bitmap_controls(vcpu, vmcs12) ||\n\t    nested_vmx_check_tpr_shadow_controls(vcpu, vmcs12) ||\n\t    nested_vmx_check_apic_access_controls(vcpu, vmcs12) ||\n\t    nested_vmx_check_apicv_controls(vcpu, vmcs12) ||\n\t    nested_vmx_check_nmi_controls(vmcs12) ||\n\t    nested_vmx_check_pml_controls(vcpu, vmcs12) ||\n\t    nested_vmx_check_unrestricted_guest_controls(vcpu, vmcs12) ||\n\t    nested_vmx_check_mode_based_ept_exec_controls(vcpu, vmcs12) ||\n\t    nested_vmx_check_shadow_vmcs_controls(vcpu, vmcs12) ||\n\t    CC(nested_cpu_has_vpid(vmcs12) && !vmcs12->virtual_processor_id))\n\t\treturn -EINVAL;\n\n\tif (!nested_cpu_has_preemption_timer(vmcs12) &&\n\t    nested_cpu_has_save_preemption_timer(vmcs12))\n\t\treturn -EINVAL;\n\n\tif (nested_cpu_has_ept(vmcs12) &&\n\t    CC(!nested_vmx_check_eptp(vcpu, vmcs12->ept_pointer)))\n\t\treturn -EINVAL;\n\n\tif (nested_cpu_has_vmfunc(vmcs12)) {\n\t\tif (CC(vmcs12->vm_function_control &\n\t\t       ~vmx->nested.msrs.vmfunc_controls))\n\t\t\treturn -EINVAL;\n\n\t\tif (nested_cpu_has_eptp_switching(vmcs12)) {\n\t\t\tif (CC(!nested_cpu_has_ept(vmcs12)) ||\n\t\t\t    CC(!page_address_valid(vcpu, vmcs12->eptp_list_address)))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int nested_check_vm_exit_controls(struct kvm_vcpu *vcpu,\n                                         struct vmcs12 *vmcs12)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (CC(!vmx_control_verify(vmcs12->vm_exit_controls,\n\t\t\t\t    vmx->nested.msrs.exit_ctls_low,\n\t\t\t\t    vmx->nested.msrs.exit_ctls_high)) ||\n\t    CC(nested_vmx_check_exit_msr_switch_controls(vcpu, vmcs12)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\n \nstatic int nested_check_vm_entry_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t  struct vmcs12 *vmcs12)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (CC(!vmx_control_verify(vmcs12->vm_entry_controls,\n\t\t\t\t    vmx->nested.msrs.entry_ctls_low,\n\t\t\t\t    vmx->nested.msrs.entry_ctls_high)))\n\t\treturn -EINVAL;\n\n\t \n\tif (vmcs12->vm_entry_intr_info_field & INTR_INFO_VALID_MASK) {\n\t\tu32 intr_info = vmcs12->vm_entry_intr_info_field;\n\t\tu8 vector = intr_info & INTR_INFO_VECTOR_MASK;\n\t\tu32 intr_type = intr_info & INTR_INFO_INTR_TYPE_MASK;\n\t\tbool has_error_code = intr_info & INTR_INFO_DELIVER_CODE_MASK;\n\t\tbool should_have_error_code;\n\t\tbool urg = nested_cpu_has2(vmcs12,\n\t\t\t\t\t   SECONDARY_EXEC_UNRESTRICTED_GUEST);\n\t\tbool prot_mode = !urg || vmcs12->guest_cr0 & X86_CR0_PE;\n\n\t\t \n\t\tif (CC(intr_type == INTR_TYPE_RESERVED) ||\n\t\t    CC(intr_type == INTR_TYPE_OTHER_EVENT &&\n\t\t       !nested_cpu_supports_monitor_trap_flag(vcpu)))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (CC(intr_type == INTR_TYPE_NMI_INTR && vector != NMI_VECTOR) ||\n\t\t    CC(intr_type == INTR_TYPE_HARD_EXCEPTION && vector > 31) ||\n\t\t    CC(intr_type == INTR_TYPE_OTHER_EVENT && vector != 0))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tshould_have_error_code =\n\t\t\tintr_type == INTR_TYPE_HARD_EXCEPTION && prot_mode &&\n\t\t\tx86_exception_has_error_code(vector);\n\t\tif (CC(has_error_code != should_have_error_code))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (CC(has_error_code &&\n\t\t       vmcs12->vm_entry_exception_error_code & GENMASK(31, 16)))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (CC(intr_info & INTR_INFO_RESVD_BITS_MASK))\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tswitch (intr_type) {\n\t\tcase INTR_TYPE_SOFT_EXCEPTION:\n\t\tcase INTR_TYPE_SOFT_INTR:\n\t\tcase INTR_TYPE_PRIV_SW_EXCEPTION:\n\t\t\tif (CC(vmcs12->vm_entry_instruction_len > 15) ||\n\t\t\t    CC(vmcs12->vm_entry_instruction_len == 0 &&\n\t\t\t    CC(!nested_cpu_has_zero_length_injection(vcpu))))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\t}\n\n\tif (nested_vmx_check_entry_msr_switch_controls(vcpu, vmcs12))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_controls(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct vmcs12 *vmcs12)\n{\n\tif (nested_check_vm_execution_controls(vcpu, vmcs12) ||\n\t    nested_check_vm_exit_controls(vcpu, vmcs12) ||\n\t    nested_check_vm_entry_controls(vcpu, vmcs12))\n\t\treturn -EINVAL;\n\n\tif (guest_cpuid_has_evmcs(vcpu))\n\t\treturn nested_evmcs_check_controls(vmcs12);\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_address_space_size(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmcs12 *vmcs12)\n{\n#ifdef CONFIG_X86_64\n\tif (CC(!!(vmcs12->vm_exit_controls & VM_EXIT_HOST_ADDR_SPACE_SIZE) !=\n\t\t!!(vcpu->arch.efer & EFER_LMA)))\n\t\treturn -EINVAL;\n#endif\n\treturn 0;\n}\n\nstatic int nested_vmx_check_host_state(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmcs12 *vmcs12)\n{\n\tbool ia32e = !!(vmcs12->vm_exit_controls & VM_EXIT_HOST_ADDR_SPACE_SIZE);\n\n\tif (CC(!nested_host_cr0_valid(vcpu, vmcs12->host_cr0)) ||\n\t    CC(!nested_host_cr4_valid(vcpu, vmcs12->host_cr4)) ||\n\t    CC(kvm_vcpu_is_illegal_gpa(vcpu, vmcs12->host_cr3)))\n\t\treturn -EINVAL;\n\n\tif (CC(is_noncanonical_address(vmcs12->host_ia32_sysenter_esp, vcpu)) ||\n\t    CC(is_noncanonical_address(vmcs12->host_ia32_sysenter_eip, vcpu)))\n\t\treturn -EINVAL;\n\n\tif ((vmcs12->vm_exit_controls & VM_EXIT_LOAD_IA32_PAT) &&\n\t    CC(!kvm_pat_valid(vmcs12->host_ia32_pat)))\n\t\treturn -EINVAL;\n\n\tif ((vmcs12->vm_exit_controls & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL) &&\n\t    CC(!kvm_valid_perf_global_ctrl(vcpu_to_pmu(vcpu),\n\t\t\t\t\t   vmcs12->host_ia32_perf_global_ctrl)))\n\t\treturn -EINVAL;\n\n\tif (ia32e) {\n\t\tif (CC(!(vmcs12->host_cr4 & X86_CR4_PAE)))\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (CC(vmcs12->vm_entry_controls & VM_ENTRY_IA32E_MODE) ||\n\t\t    CC(vmcs12->host_cr4 & X86_CR4_PCIDE) ||\n\t\t    CC((vmcs12->host_rip) >> 32))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (CC(vmcs12->host_cs_selector & (SEGMENT_RPL_MASK | SEGMENT_TI_MASK)) ||\n\t    CC(vmcs12->host_ss_selector & (SEGMENT_RPL_MASK | SEGMENT_TI_MASK)) ||\n\t    CC(vmcs12->host_ds_selector & (SEGMENT_RPL_MASK | SEGMENT_TI_MASK)) ||\n\t    CC(vmcs12->host_es_selector & (SEGMENT_RPL_MASK | SEGMENT_TI_MASK)) ||\n\t    CC(vmcs12->host_fs_selector & (SEGMENT_RPL_MASK | SEGMENT_TI_MASK)) ||\n\t    CC(vmcs12->host_gs_selector & (SEGMENT_RPL_MASK | SEGMENT_TI_MASK)) ||\n\t    CC(vmcs12->host_tr_selector & (SEGMENT_RPL_MASK | SEGMENT_TI_MASK)) ||\n\t    CC(vmcs12->host_cs_selector == 0) ||\n\t    CC(vmcs12->host_tr_selector == 0) ||\n\t    CC(vmcs12->host_ss_selector == 0 && !ia32e))\n\t\treturn -EINVAL;\n\n\tif (CC(is_noncanonical_address(vmcs12->host_fs_base, vcpu)) ||\n\t    CC(is_noncanonical_address(vmcs12->host_gs_base, vcpu)) ||\n\t    CC(is_noncanonical_address(vmcs12->host_gdtr_base, vcpu)) ||\n\t    CC(is_noncanonical_address(vmcs12->host_idtr_base, vcpu)) ||\n\t    CC(is_noncanonical_address(vmcs12->host_tr_base, vcpu)) ||\n\t    CC(is_noncanonical_address(vmcs12->host_rip, vcpu)))\n\t\treturn -EINVAL;\n\n\t \n\tif (vmcs12->vm_exit_controls & VM_EXIT_LOAD_IA32_EFER) {\n\t\tif (CC(!kvm_valid_efer(vcpu, vmcs12->host_ia32_efer)) ||\n\t\t    CC(ia32e != !!(vmcs12->host_ia32_efer & EFER_LMA)) ||\n\t\t    CC(ia32e != !!(vmcs12->host_ia32_efer & EFER_LME)))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_vmcs_link_ptr(struct kvm_vcpu *vcpu,\n\t\t\t\t\t  struct vmcs12 *vmcs12)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct gfn_to_hva_cache *ghc = &vmx->nested.shadow_vmcs12_cache;\n\tstruct vmcs_hdr hdr;\n\n\tif (vmcs12->vmcs_link_pointer == INVALID_GPA)\n\t\treturn 0;\n\n\tif (CC(!page_address_valid(vcpu, vmcs12->vmcs_link_pointer)))\n\t\treturn -EINVAL;\n\n\tif (ghc->gpa != vmcs12->vmcs_link_pointer &&\n\t    CC(kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc,\n\t\t\t\t\t vmcs12->vmcs_link_pointer, VMCS12_SIZE)))\n                return -EINVAL;\n\n\tif (CC(kvm_read_guest_offset_cached(vcpu->kvm, ghc, &hdr,\n\t\t\t\t\t    offsetof(struct vmcs12, hdr),\n\t\t\t\t\t    sizeof(hdr))))\n\t\treturn -EINVAL;\n\n\tif (CC(hdr.revision_id != VMCS12_REVISION) ||\n\t    CC(hdr.shadow_vmcs != nested_cpu_has_shadow_vmcs(vmcs12)))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\n \nstatic int nested_check_guest_non_reg_state(struct vmcs12 *vmcs12)\n{\n\tif (CC(vmcs12->guest_activity_state != GUEST_ACTIVITY_ACTIVE &&\n\t       vmcs12->guest_activity_state != GUEST_ACTIVITY_HLT &&\n\t       vmcs12->guest_activity_state != GUEST_ACTIVITY_WAIT_SIPI))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_guest_state(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct vmcs12 *vmcs12,\n\t\t\t\t\tenum vm_entry_failure_code *entry_failure_code)\n{\n\tbool ia32e = !!(vmcs12->vm_entry_controls & VM_ENTRY_IA32E_MODE);\n\n\t*entry_failure_code = ENTRY_FAIL_DEFAULT;\n\n\tif (CC(!nested_guest_cr0_valid(vcpu, vmcs12->guest_cr0)) ||\n\t    CC(!nested_guest_cr4_valid(vcpu, vmcs12->guest_cr4)))\n\t\treturn -EINVAL;\n\n\tif ((vmcs12->vm_entry_controls & VM_ENTRY_LOAD_DEBUG_CONTROLS) &&\n\t    CC(!kvm_dr7_valid(vmcs12->guest_dr7)))\n\t\treturn -EINVAL;\n\n\tif ((vmcs12->vm_entry_controls & VM_ENTRY_LOAD_IA32_PAT) &&\n\t    CC(!kvm_pat_valid(vmcs12->guest_ia32_pat)))\n\t\treturn -EINVAL;\n\n\tif (nested_vmx_check_vmcs_link_ptr(vcpu, vmcs12)) {\n\t\t*entry_failure_code = ENTRY_FAIL_VMCS_LINK_PTR;\n\t\treturn -EINVAL;\n\t}\n\n\tif ((vmcs12->vm_entry_controls & VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL) &&\n\t    CC(!kvm_valid_perf_global_ctrl(vcpu_to_pmu(vcpu),\n\t\t\t\t\t   vmcs12->guest_ia32_perf_global_ctrl)))\n\t\treturn -EINVAL;\n\n\tif (CC((vmcs12->guest_cr0 & (X86_CR0_PG | X86_CR0_PE)) == X86_CR0_PG))\n\t\treturn -EINVAL;\n\n\tif (CC(ia32e && !(vmcs12->guest_cr4 & X86_CR4_PAE)) ||\n\t    CC(ia32e && !(vmcs12->guest_cr0 & X86_CR0_PG)))\n\t\treturn -EINVAL;\n\n\t \n\tif (to_vmx(vcpu)->nested.nested_run_pending &&\n\t    (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_IA32_EFER)) {\n\t\tif (CC(!kvm_valid_efer(vcpu, vmcs12->guest_ia32_efer)) ||\n\t\t    CC(ia32e != !!(vmcs12->guest_ia32_efer & EFER_LMA)) ||\n\t\t    CC(((vmcs12->guest_cr0 & X86_CR0_PG) &&\n\t\t     ia32e != !!(vmcs12->guest_ia32_efer & EFER_LME))))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif ((vmcs12->vm_entry_controls & VM_ENTRY_LOAD_BNDCFGS) &&\n\t    (CC(is_noncanonical_address(vmcs12->guest_bndcfgs & PAGE_MASK, vcpu)) ||\n\t     CC((vmcs12->guest_bndcfgs & MSR_IA32_BNDCFGS_RSVD))))\n\t\treturn -EINVAL;\n\n\tif (nested_check_guest_non_reg_state(vmcs12))\n\t\treturn -EINVAL;\n\n\treturn 0;\n}\n\nstatic int nested_vmx_check_vmentry_hw(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunsigned long cr3, cr4;\n\tbool vm_fail;\n\n\tif (!nested_early_check)\n\t\treturn 0;\n\n\tif (vmx->msr_autoload.host.nr)\n\t\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, 0);\n\tif (vmx->msr_autoload.guest.nr)\n\t\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, 0);\n\n\tpreempt_disable();\n\n\tvmx_prepare_switch_to_guest(vcpu);\n\n\t \n\tvmcs_writel(GUEST_RFLAGS, 0);\n\n\tcr3 = __get_current_cr3_fast();\n\tif (unlikely(cr3 != vmx->loaded_vmcs->host_state.cr3)) {\n\t\tvmcs_writel(HOST_CR3, cr3);\n\t\tvmx->loaded_vmcs->host_state.cr3 = cr3;\n\t}\n\n\tcr4 = cr4_read_shadow();\n\tif (unlikely(cr4 != vmx->loaded_vmcs->host_state.cr4)) {\n\t\tvmcs_writel(HOST_CR4, cr4);\n\t\tvmx->loaded_vmcs->host_state.cr4 = cr4;\n\t}\n\n\tvm_fail = __vmx_vcpu_run(vmx, (unsigned long *)&vcpu->arch.regs,\n\t\t\t\t __vmx_vcpu_run_flags(vmx));\n\n\tif (vmx->msr_autoload.host.nr)\n\t\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx->msr_autoload.host.nr);\n\tif (vmx->msr_autoload.guest.nr)\n\t\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx->msr_autoload.guest.nr);\n\n\tif (vm_fail) {\n\t\tu32 error = vmcs_read32(VM_INSTRUCTION_ERROR);\n\n\t\tpreempt_enable();\n\n\t\ttrace_kvm_nested_vmenter_failed(\n\t\t\t\"early hardware check VM-instruction error: \", error);\n\t\tWARN_ON_ONCE(error != VMXERR_ENTRY_INVALID_CONTROL_FIELD);\n\t\treturn 1;\n\t}\n\n\t \n\tif (hw_breakpoint_active())\n\t\tset_debugreg(__this_cpu_read(cpu_dr7), 7);\n\tlocal_irq_enable();\n\tpreempt_enable();\n\n\t \n\tWARN_ON(!(vmcs_read32(VM_EXIT_REASON) &\n\t\tVMX_EXIT_REASONS_FAILED_VMENTRY));\n\n\treturn 0;\n}\n\nstatic bool nested_get_evmcs_page(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\t \n\tif (guest_cpuid_has_evmcs(vcpu) &&\n\t    vmx->nested.hv_evmcs_vmptr == EVMPTR_MAP_PENDING) {\n\t\tenum nested_evmptrld_status evmptrld_status =\n\t\t\tnested_vmx_handle_enlightened_vmptrld(vcpu, false);\n\n\t\tif (evmptrld_status == EVMPTRLD_VMFAIL ||\n\t\t    evmptrld_status == EVMPTRLD_ERROR)\n\t\t\treturn false;\n\n\t\t \n\t\tvmx->nested.need_vmcs12_to_shadow_sync = true;\n\t}\n\n\treturn true;\n}\n\nstatic bool nested_get_vmcs12_pages(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct kvm_host_map *map;\n\n\tif (!vcpu->arch.pdptrs_from_userspace &&\n\t    !nested_cpu_has_ept(vmcs12) && is_pae_paging(vcpu)) {\n\t\t \n\t\tif (CC(!load_pdptrs(vcpu, vcpu->arch.cr3)))\n\t\t\treturn false;\n\t}\n\n\n\tif (nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)) {\n\t\tmap = &vmx->nested.apic_access_page_map;\n\n\t\tif (!kvm_vcpu_map(vcpu, gpa_to_gfn(vmcs12->apic_access_addr), map)) {\n\t\t\tvmcs_write64(APIC_ACCESS_ADDR, pfn_to_hpa(map->pfn));\n\t\t} else {\n\t\t\tpr_debug_ratelimited(\"%s: no backing for APIC-access address in vmcs12\\n\",\n\t\t\t\t\t     __func__);\n\t\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\t\tvcpu->run->internal.suberror =\n\t\t\t\tKVM_INTERNAL_ERROR_EMULATION;\n\t\t\tvcpu->run->internal.ndata = 0;\n\t\t\treturn false;\n\t\t}\n\t}\n\n\tif (nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW)) {\n\t\tmap = &vmx->nested.virtual_apic_map;\n\n\t\tif (!kvm_vcpu_map(vcpu, gpa_to_gfn(vmcs12->virtual_apic_page_addr), map)) {\n\t\t\tvmcs_write64(VIRTUAL_APIC_PAGE_ADDR, pfn_to_hpa(map->pfn));\n\t\t} else if (nested_cpu_has(vmcs12, CPU_BASED_CR8_LOAD_EXITING) &&\n\t\t           nested_cpu_has(vmcs12, CPU_BASED_CR8_STORE_EXITING) &&\n\t\t\t   !nested_cpu_has2(vmcs12, SECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES)) {\n\t\t\t \n\t\t\texec_controls_clearbit(vmx, CPU_BASED_TPR_SHADOW);\n\t\t} else {\n\t\t\t \n\t\t\tvmcs_write64(VIRTUAL_APIC_PAGE_ADDR, INVALID_GPA);\n\t\t}\n\t}\n\n\tif (nested_cpu_has_posted_intr(vmcs12)) {\n\t\tmap = &vmx->nested.pi_desc_map;\n\n\t\tif (!kvm_vcpu_map(vcpu, gpa_to_gfn(vmcs12->posted_intr_desc_addr), map)) {\n\t\t\tvmx->nested.pi_desc =\n\t\t\t\t(struct pi_desc *)(((void *)map->hva) +\n\t\t\t\toffset_in_page(vmcs12->posted_intr_desc_addr));\n\t\t\tvmcs_write64(POSTED_INTR_DESC_ADDR,\n\t\t\t\t     pfn_to_hpa(map->pfn) + offset_in_page(vmcs12->posted_intr_desc_addr));\n\t\t} else {\n\t\t\t \n\t\t\tvmx->nested.pi_desc = NULL;\n\t\t\tpin_controls_clearbit(vmx, PIN_BASED_POSTED_INTR);\n\t\t}\n\t}\n\tif (nested_vmx_prepare_msr_bitmap(vcpu, vmcs12))\n\t\texec_controls_setbit(vmx, CPU_BASED_USE_MSR_BITMAPS);\n\telse\n\t\texec_controls_clearbit(vmx, CPU_BASED_USE_MSR_BITMAPS);\n\n\treturn true;\n}\n\nstatic bool vmx_get_nested_state_pages(struct kvm_vcpu *vcpu)\n{\n\t \n\tif (!nested_get_evmcs_page(vcpu)) {\n\t\tpr_debug_ratelimited(\"%s: enlightened vmptrld failed\\n\",\n\t\t\t\t     __func__);\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror =\n\t\t\tKVM_INTERNAL_ERROR_EMULATION;\n\t\tvcpu->run->internal.ndata = 0;\n\n\t\treturn false;\n\t}\n\n\tif (is_guest_mode(vcpu) && !nested_get_vmcs12_pages(vcpu))\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic int nested_vmx_write_pml_buffer(struct kvm_vcpu *vcpu, gpa_t gpa)\n{\n\tstruct vmcs12 *vmcs12;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tgpa_t dst;\n\n\tif (WARN_ON_ONCE(!is_guest_mode(vcpu)))\n\t\treturn 0;\n\n\tif (WARN_ON_ONCE(vmx->nested.pml_full))\n\t\treturn 1;\n\n\t \n\tvmcs12 = get_vmcs12(vcpu);\n\tif (!nested_cpu_has_pml(vmcs12))\n\t\treturn 0;\n\n\tif (vmcs12->guest_pml_index >= PML_ENTITY_NUM) {\n\t\tvmx->nested.pml_full = true;\n\t\treturn 1;\n\t}\n\n\tgpa &= ~0xFFFull;\n\tdst = vmcs12->pml_address + sizeof(u64) * vmcs12->guest_pml_index;\n\n\tif (kvm_write_guest_page(vcpu->kvm, gpa_to_gfn(dst), &gpa,\n\t\t\t\t offset_in_page(dst), sizeof(gpa)))\n\t\treturn 0;\n\n\tvmcs12->guest_pml_index--;\n\n\treturn 0;\n}\n\n \nstatic int nested_vmx_check_permission(struct kvm_vcpu *vcpu)\n{\n\tif (!to_vmx(vcpu)->nested.vmxon) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 0;\n\t}\n\n\tif (vmx_get_cpl(vcpu)) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 0;\n\t}\n\n\treturn 1;\n}\n\nstatic u8 vmx_has_apicv_interrupt(struct kvm_vcpu *vcpu)\n{\n\tu8 rvi = vmx_get_rvi();\n\tu8 vppr = kvm_lapic_get_reg(vcpu->arch.apic, APIC_PROCPRI);\n\n\treturn ((rvi & 0xf0) > (vppr & 0xf0));\n}\n\nstatic void load_vmcs12_host_state(struct kvm_vcpu *vcpu,\n\t\t\t\t   struct vmcs12 *vmcs12);\n\n \nenum nvmx_vmentry_status nested_vmx_enter_non_root_mode(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t\tbool from_vmentry)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tenum vm_entry_failure_code entry_failure_code;\n\tbool evaluate_pending_interrupts;\n\tunion vmx_exit_reason exit_reason = {\n\t\t.basic = EXIT_REASON_INVALID_STATE,\n\t\t.failed_vmentry = 1,\n\t};\n\tu32 failed_index;\n\n\ttrace_kvm_nested_vmenter(kvm_rip_read(vcpu),\n\t\t\t\t vmx->nested.current_vmptr,\n\t\t\t\t vmcs12->guest_rip,\n\t\t\t\t vmcs12->guest_intr_status,\n\t\t\t\t vmcs12->vm_entry_intr_info_field,\n\t\t\t\t vmcs12->secondary_vm_exec_control & SECONDARY_EXEC_ENABLE_EPT,\n\t\t\t\t vmcs12->ept_pointer,\n\t\t\t\t vmcs12->guest_cr3,\n\t\t\t\t KVM_ISA_VMX);\n\n\tkvm_service_local_tlb_flush_requests(vcpu);\n\n\tevaluate_pending_interrupts = exec_controls_get(vmx) &\n\t\t(CPU_BASED_INTR_WINDOW_EXITING | CPU_BASED_NMI_WINDOW_EXITING);\n\tif (likely(!evaluate_pending_interrupts) && kvm_vcpu_apicv_active(vcpu))\n\t\tevaluate_pending_interrupts |= vmx_has_apicv_interrupt(vcpu);\n\tif (!evaluate_pending_interrupts)\n\t\tevaluate_pending_interrupts |= kvm_apic_has_pending_init_or_sipi(vcpu);\n\n\tif (!vmx->nested.nested_run_pending ||\n\t    !(vmcs12->vm_entry_controls & VM_ENTRY_LOAD_DEBUG_CONTROLS))\n\t\tvmx->nested.pre_vmenter_debugctl = vmcs_read64(GUEST_IA32_DEBUGCTL);\n\tif (kvm_mpx_supported() &&\n\t    (!vmx->nested.nested_run_pending ||\n\t     !(vmcs12->vm_entry_controls & VM_ENTRY_LOAD_BNDCFGS)))\n\t\tvmx->nested.pre_vmenter_bndcfgs = vmcs_read64(GUEST_BNDCFGS);\n\n\t \n\tif (!enable_ept && !nested_early_check)\n\t\tvmcs_writel(GUEST_CR3, vcpu->arch.cr3);\n\n\tvmx_switch_vmcs(vcpu, &vmx->nested.vmcs02);\n\n\tprepare_vmcs02_early(vmx, &vmx->vmcs01, vmcs12);\n\n\tif (from_vmentry) {\n\t\tif (unlikely(!nested_get_vmcs12_pages(vcpu))) {\n\t\t\tvmx_switch_vmcs(vcpu, &vmx->vmcs01);\n\t\t\treturn NVMX_VMENTRY_KVM_INTERNAL_ERROR;\n\t\t}\n\n\t\tif (nested_vmx_check_vmentry_hw(vcpu)) {\n\t\t\tvmx_switch_vmcs(vcpu, &vmx->vmcs01);\n\t\t\treturn NVMX_VMENTRY_VMFAIL;\n\t\t}\n\n\t\tif (nested_vmx_check_guest_state(vcpu, vmcs12,\n\t\t\t\t\t\t &entry_failure_code)) {\n\t\t\texit_reason.basic = EXIT_REASON_INVALID_STATE;\n\t\t\tvmcs12->exit_qualification = entry_failure_code;\n\t\t\tgoto vmentry_fail_vmexit;\n\t\t}\n\t}\n\n\tenter_guest_mode(vcpu);\n\n\tif (prepare_vmcs02(vcpu, vmcs12, from_vmentry, &entry_failure_code)) {\n\t\texit_reason.basic = EXIT_REASON_INVALID_STATE;\n\t\tvmcs12->exit_qualification = entry_failure_code;\n\t\tgoto vmentry_fail_vmexit_guest_mode;\n\t}\n\n\tif (from_vmentry) {\n\t\tfailed_index = nested_vmx_load_msr(vcpu,\n\t\t\t\t\t\t   vmcs12->vm_entry_msr_load_addr,\n\t\t\t\t\t\t   vmcs12->vm_entry_msr_load_count);\n\t\tif (failed_index) {\n\t\t\texit_reason.basic = EXIT_REASON_MSR_LOAD_FAIL;\n\t\t\tvmcs12->exit_qualification = failed_index;\n\t\t\tgoto vmentry_fail_vmexit_guest_mode;\n\t\t}\n\t} else {\n\t\t \n\t\tkvm_make_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu);\n\t}\n\n\t \n\tif (unlikely(evaluate_pending_interrupts))\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\t \n\tvmx->nested.preemption_timer_expired = false;\n\tif (nested_cpu_has_preemption_timer(vmcs12)) {\n\t\tu64 timer_value = vmx_calc_preemption_timer_value(vcpu);\n\t\tvmx_start_preemption_timer(vcpu, timer_value);\n\t}\n\n\t \n\treturn NVMX_VMENTRY_SUCCESS;\n\n\t \nvmentry_fail_vmexit_guest_mode:\n\tif (vmcs12->cpu_based_vm_exec_control & CPU_BASED_USE_TSC_OFFSETTING)\n\t\tvcpu->arch.tsc_offset -= vmcs12->tsc_offset;\n\tleave_guest_mode(vcpu);\n\nvmentry_fail_vmexit:\n\tvmx_switch_vmcs(vcpu, &vmx->vmcs01);\n\n\tif (!from_vmentry)\n\t\treturn NVMX_VMENTRY_VMEXIT;\n\n\tload_vmcs12_host_state(vcpu, vmcs12);\n\tvmcs12->vm_exit_reason = exit_reason.full;\n\tif (enable_shadow_vmcs || evmptr_is_valid(vmx->nested.hv_evmcs_vmptr))\n\t\tvmx->nested.need_vmcs12_to_shadow_sync = true;\n\treturn NVMX_VMENTRY_VMEXIT;\n}\n\n \nstatic int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)\n{\n\tstruct vmcs12 *vmcs12;\n\tenum nvmx_vmentry_status status;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 interrupt_shadow = vmx_get_interrupt_shadow(vcpu);\n\tenum nested_evmptrld_status evmptrld_status;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tevmptrld_status = nested_vmx_handle_enlightened_vmptrld(vcpu, launch);\n\tif (evmptrld_status == EVMPTRLD_ERROR) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tkvm_pmu_trigger_event(vcpu, PERF_COUNT_HW_BRANCH_INSTRUCTIONS);\n\n\tif (CC(evmptrld_status == EVMPTRLD_VMFAIL))\n\t\treturn nested_vmx_failInvalid(vcpu);\n\n\tif (CC(!evmptr_is_valid(vmx->nested.hv_evmcs_vmptr) &&\n\t       vmx->nested.current_vmptr == INVALID_GPA))\n\t\treturn nested_vmx_failInvalid(vcpu);\n\n\tvmcs12 = get_vmcs12(vcpu);\n\n\t \n\tif (CC(vmcs12->hdr.shadow_vmcs))\n\t\treturn nested_vmx_failInvalid(vcpu);\n\n\tif (evmptr_is_valid(vmx->nested.hv_evmcs_vmptr)) {\n\t\tcopy_enlightened_to_vmcs12(vmx, vmx->nested.hv_evmcs->hv_clean_fields);\n\t\t \n\t\tvmcs12->launch_state = !launch;\n\t} else if (enable_shadow_vmcs) {\n\t\tcopy_shadow_to_vmcs12(vmx);\n\t}\n\n\t \n\tif (CC(interrupt_shadow & KVM_X86_SHADOW_INT_MOV_SS))\n\t\treturn nested_vmx_fail(vcpu, VMXERR_ENTRY_EVENTS_BLOCKED_BY_MOV_SS);\n\n\tif (CC(vmcs12->launch_state == launch))\n\t\treturn nested_vmx_fail(vcpu,\n\t\t\tlaunch ? VMXERR_VMLAUNCH_NONCLEAR_VMCS\n\t\t\t       : VMXERR_VMRESUME_NONLAUNCHED_VMCS);\n\n\tif (nested_vmx_check_controls(vcpu, vmcs12))\n\t\treturn nested_vmx_fail(vcpu, VMXERR_ENTRY_INVALID_CONTROL_FIELD);\n\n\tif (nested_vmx_check_address_space_size(vcpu, vmcs12))\n\t\treturn nested_vmx_fail(vcpu, VMXERR_ENTRY_INVALID_HOST_STATE_FIELD);\n\n\tif (nested_vmx_check_host_state(vcpu, vmcs12))\n\t\treturn nested_vmx_fail(vcpu, VMXERR_ENTRY_INVALID_HOST_STATE_FIELD);\n\n\t \n\tvmx->nested.nested_run_pending = 1;\n\tvmx->nested.has_preemption_timer_deadline = false;\n\tstatus = nested_vmx_enter_non_root_mode(vcpu, true);\n\tif (unlikely(status != NVMX_VMENTRY_SUCCESS))\n\t\tgoto vmentry_failed;\n\n\t \n\tif (nested_cpu_has_posted_intr(vmcs12) &&\n\t    kvm_apic_has_interrupt(vcpu) == vmx->nested.posted_intr_nv) {\n\t\tvmx->nested.pi_pending = true;\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\tkvm_apic_clear_irr(vcpu, vmx->nested.posted_intr_nv);\n\t}\n\n\t \n\tvmx->vcpu.arch.l1tf_flush_l1d = true;\n\n\t \n\tnested_cache_shadow_vmcs12(vcpu, vmcs12);\n\n\tswitch (vmcs12->guest_activity_state) {\n\tcase GUEST_ACTIVITY_HLT:\n\t\t \n\t\tif (!(vmcs12->vm_entry_intr_info_field & INTR_INFO_VALID_MASK) &&\n\t\t    !nested_cpu_has(vmcs12, CPU_BASED_NMI_WINDOW_EXITING) &&\n\t\t    !(nested_cpu_has(vmcs12, CPU_BASED_INTR_WINDOW_EXITING) &&\n\t\t      (vmcs12->guest_rflags & X86_EFLAGS_IF))) {\n\t\t\tvmx->nested.nested_run_pending = 0;\n\t\t\treturn kvm_emulate_halt_noskip(vcpu);\n\t\t}\n\t\tbreak;\n\tcase GUEST_ACTIVITY_WAIT_SIPI:\n\t\tvmx->nested.nested_run_pending = 0;\n\t\tvcpu->arch.mp_state = KVM_MP_STATE_INIT_RECEIVED;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 1;\n\nvmentry_failed:\n\tvmx->nested.nested_run_pending = 0;\n\tif (status == NVMX_VMENTRY_KVM_INTERNAL_ERROR)\n\t\treturn 0;\n\tif (status == NVMX_VMENTRY_VMEXIT)\n\t\treturn 1;\n\tWARN_ON_ONCE(status != NVMX_VMENTRY_VMFAIL);\n\treturn nested_vmx_fail(vcpu, VMXERR_ENTRY_INVALID_CONTROL_FIELD);\n}\n\n \nstatic inline unsigned long\nvmcs12_guest_cr0(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)\n{\n\treturn\n\t \t(vmcs_readl(GUEST_CR0) & vcpu->arch.cr0_guest_owned_bits) |\n\t \t(vmcs12->guest_cr0 & vmcs12->cr0_guest_host_mask) |\n\t \t(vmcs_readl(CR0_READ_SHADOW) & ~(vmcs12->cr0_guest_host_mask |\n\t\t\tvcpu->arch.cr0_guest_owned_bits));\n}\n\nstatic inline unsigned long\nvmcs12_guest_cr4(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)\n{\n\treturn\n\t \t(vmcs_readl(GUEST_CR4) & vcpu->arch.cr4_guest_owned_bits) |\n\t \t(vmcs12->guest_cr4 & vmcs12->cr4_guest_host_mask) |\n\t \t(vmcs_readl(CR4_READ_SHADOW) & ~(vmcs12->cr4_guest_host_mask |\n\t\t\tvcpu->arch.cr4_guest_owned_bits));\n}\n\nstatic void vmcs12_save_pending_event(struct kvm_vcpu *vcpu,\n\t\t\t\t      struct vmcs12 *vmcs12,\n\t\t\t\t      u32 vm_exit_reason, u32 exit_intr_info)\n{\n\tu32 idt_vectoring;\n\tunsigned int nr;\n\n\t \n\tif ((u16)vm_exit_reason == EXIT_REASON_TRIPLE_FAULT ||\n\t    ((u16)vm_exit_reason == EXIT_REASON_EXCEPTION_NMI &&\n\t     is_double_fault(exit_intr_info))) {\n\t\tvmcs12->idt_vectoring_info_field = 0;\n\t} else if (vcpu->arch.exception.injected) {\n\t\tnr = vcpu->arch.exception.vector;\n\t\tidt_vectoring = nr | VECTORING_INFO_VALID_MASK;\n\n\t\tif (kvm_exception_is_soft(nr)) {\n\t\t\tvmcs12->vm_exit_instruction_len =\n\t\t\t\tvcpu->arch.event_exit_inst_len;\n\t\t\tidt_vectoring |= INTR_TYPE_SOFT_EXCEPTION;\n\t\t} else\n\t\t\tidt_vectoring |= INTR_TYPE_HARD_EXCEPTION;\n\n\t\tif (vcpu->arch.exception.has_error_code) {\n\t\t\tidt_vectoring |= VECTORING_INFO_DELIVER_CODE_MASK;\n\t\t\tvmcs12->idt_vectoring_error_code =\n\t\t\t\tvcpu->arch.exception.error_code;\n\t\t}\n\n\t\tvmcs12->idt_vectoring_info_field = idt_vectoring;\n\t} else if (vcpu->arch.nmi_injected) {\n\t\tvmcs12->idt_vectoring_info_field =\n\t\t\tINTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK | NMI_VECTOR;\n\t} else if (vcpu->arch.interrupt.injected) {\n\t\tnr = vcpu->arch.interrupt.nr;\n\t\tidt_vectoring = nr | VECTORING_INFO_VALID_MASK;\n\n\t\tif (vcpu->arch.interrupt.soft) {\n\t\t\tidt_vectoring |= INTR_TYPE_SOFT_INTR;\n\t\t\tvmcs12->vm_entry_instruction_len =\n\t\t\t\tvcpu->arch.event_exit_inst_len;\n\t\t} else\n\t\t\tidt_vectoring |= INTR_TYPE_EXT_INTR;\n\n\t\tvmcs12->idt_vectoring_info_field = idt_vectoring;\n\t} else {\n\t\tvmcs12->idt_vectoring_info_field = 0;\n\t}\n}\n\n\nvoid nested_mark_vmcs12_pages_dirty(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tgfn_t gfn;\n\n\t \n\n\tif (nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW)) {\n\t\tgfn = vmcs12->virtual_apic_page_addr >> PAGE_SHIFT;\n\t\tkvm_vcpu_mark_page_dirty(vcpu, gfn);\n\t}\n\n\tif (nested_cpu_has_posted_intr(vmcs12)) {\n\t\tgfn = vmcs12->posted_intr_desc_addr >> PAGE_SHIFT;\n\t\tkvm_vcpu_mark_page_dirty(vcpu, gfn);\n\t}\n}\n\nstatic int vmx_complete_nested_posted_interrupt(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint max_irr;\n\tvoid *vapic_page;\n\tu16 status;\n\n\tif (!vmx->nested.pi_pending)\n\t\treturn 0;\n\n\tif (!vmx->nested.pi_desc)\n\t\tgoto mmio_needed;\n\n\tvmx->nested.pi_pending = false;\n\n\tif (!pi_test_and_clear_on(vmx->nested.pi_desc))\n\t\treturn 0;\n\n\tmax_irr = find_last_bit((unsigned long *)vmx->nested.pi_desc->pir, 256);\n\tif (max_irr != 256) {\n\t\tvapic_page = vmx->nested.virtual_apic_map.hva;\n\t\tif (!vapic_page)\n\t\t\tgoto mmio_needed;\n\n\t\t__kvm_apic_update_irr(vmx->nested.pi_desc->pir,\n\t\t\tvapic_page, &max_irr);\n\t\tstatus = vmcs_read16(GUEST_INTR_STATUS);\n\t\tif ((u8)max_irr > ((u8)status & 0xff)) {\n\t\t\tstatus &= ~0xff;\n\t\t\tstatus |= (u8)max_irr;\n\t\t\tvmcs_write16(GUEST_INTR_STATUS, status);\n\t\t}\n\t}\n\n\tnested_mark_vmcs12_pages_dirty(vcpu);\n\treturn 0;\n\nmmio_needed:\n\tkvm_handle_memory_failure(vcpu, X86EMUL_IO_NEEDED, NULL);\n\treturn -ENXIO;\n}\n\nstatic void nested_vmx_inject_exception_vmexit(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_queued_exception *ex = &vcpu->arch.exception_vmexit;\n\tu32 intr_info = ex->vector | INTR_INFO_VALID_MASK;\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tunsigned long exit_qual;\n\n\tif (ex->has_payload) {\n\t\texit_qual = ex->payload;\n\t} else if (ex->vector == PF_VECTOR) {\n\t\texit_qual = vcpu->arch.cr2;\n\t} else if (ex->vector == DB_VECTOR) {\n\t\texit_qual = vcpu->arch.dr6;\n\t\texit_qual &= ~DR6_BT;\n\t\texit_qual ^= DR6_ACTIVE_LOW;\n\t} else {\n\t\texit_qual = 0;\n\t}\n\n\t \n\tif (ex->has_error_code && is_protmode(vcpu)) {\n\t\t \n\t\tvmcs12->vm_exit_intr_error_code = (u16)ex->error_code;\n\t\tintr_info |= INTR_INFO_DELIVER_CODE_MASK;\n\t}\n\n\tif (kvm_exception_is_soft(ex->vector))\n\t\tintr_info |= INTR_TYPE_SOFT_EXCEPTION;\n\telse\n\t\tintr_info |= INTR_TYPE_HARD_EXCEPTION;\n\n\tif (!(vmcs12->idt_vectoring_info_field & VECTORING_INFO_VALID_MASK) &&\n\t    vmx_get_nmi_mask(vcpu))\n\t\tintr_info |= INTR_INFO_UNBLOCK_NMI;\n\n\tnested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI, intr_info, exit_qual);\n}\n\n \nstatic unsigned long vmx_get_pending_dbg_trap(struct kvm_queued_exception *ex)\n{\n\tif (!ex->pending || ex->vector != DB_VECTOR)\n\t\treturn 0;\n\n\t \n\treturn ex->payload & ~DR6_BD;\n}\n\n \nstatic bool vmx_is_low_priority_db_trap(struct kvm_queued_exception *ex)\n{\n\treturn vmx_get_pending_dbg_trap(ex) & ~DR6_BT;\n}\n\n \nstatic void nested_vmx_update_pending_dbg(struct kvm_vcpu *vcpu)\n{\n\tunsigned long pending_dbg;\n\n\tpending_dbg = vmx_get_pending_dbg_trap(&vcpu->arch.exception);\n\tif (pending_dbg)\n\t\tvmcs_writel(GUEST_PENDING_DBG_EXCEPTIONS, pending_dbg);\n}\n\nstatic bool nested_vmx_preemption_timer_pending(struct kvm_vcpu *vcpu)\n{\n\treturn nested_cpu_has_preemption_timer(get_vmcs12(vcpu)) &&\n\t       to_vmx(vcpu)->nested.preemption_timer_expired;\n}\n\nstatic bool vmx_has_nested_events(struct kvm_vcpu *vcpu)\n{\n\treturn nested_vmx_preemption_timer_pending(vcpu) ||\n\t       to_vmx(vcpu)->nested.mtf_pending;\n}\n\n \nstatic int vmx_check_nested_events(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\t \n\tbool block_nested_exceptions = vmx->nested.nested_run_pending;\n\t \n\tbool block_nested_events = block_nested_exceptions ||\n\t\t\t\t   kvm_event_needs_reinjection(vcpu);\n\n\tif (lapic_in_kernel(vcpu) &&\n\t\ttest_bit(KVM_APIC_INIT, &apic->pending_events)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_update_pending_dbg(vcpu);\n\t\tclear_bit(KVM_APIC_INIT, &apic->pending_events);\n\t\tif (vcpu->arch.mp_state != KVM_MP_STATE_INIT_RECEIVED)\n\t\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_INIT_SIGNAL, 0, 0);\n\n\t\t \n\t\tvmx->nested.mtf_pending = false;\n\t\treturn 0;\n\t}\n\n\tif (lapic_in_kernel(vcpu) &&\n\t    test_bit(KVM_APIC_SIPI, &apic->pending_events)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\n\t\tclear_bit(KVM_APIC_SIPI, &apic->pending_events);\n\t\tif (vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED) {\n\t\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_SIPI_SIGNAL, 0,\n\t\t\t\t\t\tapic->sipi_vector & 0xFFUL);\n\t\t\treturn 0;\n\t\t}\n\t\t \n\t}\n\n\t \n\tif (vcpu->arch.exception_vmexit.pending &&\n\t    !vmx_is_low_priority_db_trap(&vcpu->arch.exception_vmexit)) {\n\t\tif (block_nested_exceptions)\n\t\t\treturn -EBUSY;\n\n\t\tnested_vmx_inject_exception_vmexit(vcpu);\n\t\treturn 0;\n\t}\n\n\tif (vcpu->arch.exception.pending &&\n\t    !vmx_is_low_priority_db_trap(&vcpu->arch.exception)) {\n\t\tif (block_nested_exceptions)\n\t\t\treturn -EBUSY;\n\t\tgoto no_vmexit;\n\t}\n\n\tif (vmx->nested.mtf_pending) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_update_pending_dbg(vcpu);\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_MONITOR_TRAP_FLAG, 0, 0);\n\t\treturn 0;\n\t}\n\n\tif (vcpu->arch.exception_vmexit.pending) {\n\t\tif (block_nested_exceptions)\n\t\t\treturn -EBUSY;\n\n\t\tnested_vmx_inject_exception_vmexit(vcpu);\n\t\treturn 0;\n\t}\n\n\tif (vcpu->arch.exception.pending) {\n\t\tif (block_nested_exceptions)\n\t\t\treturn -EBUSY;\n\t\tgoto no_vmexit;\n\t}\n\n\tif (nested_vmx_preemption_timer_pending(vcpu)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_PREEMPTION_TIMER, 0, 0);\n\t\treturn 0;\n\t}\n\n\tif (vcpu->arch.smi_pending && !is_smm(vcpu)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tgoto no_vmexit;\n\t}\n\n\tif (vcpu->arch.nmi_pending && !vmx_nmi_blocked(vcpu)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tif (!nested_exit_on_nmi(vcpu))\n\t\t\tgoto no_vmexit;\n\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_EXCEPTION_NMI,\n\t\t\t\t  NMI_VECTOR | INTR_TYPE_NMI_INTR |\n\t\t\t\t  INTR_INFO_VALID_MASK, 0);\n\t\t \n\t\tvcpu->arch.nmi_pending = 0;\n\t\tvmx_set_nmi_mask(vcpu, true);\n\t\treturn 0;\n\t}\n\n\tif (kvm_cpu_has_interrupt(vcpu) && !vmx_interrupt_blocked(vcpu)) {\n\t\tif (block_nested_events)\n\t\t\treturn -EBUSY;\n\t\tif (!nested_exit_on_intr(vcpu))\n\t\t\tgoto no_vmexit;\n\t\tnested_vmx_vmexit(vcpu, EXIT_REASON_EXTERNAL_INTERRUPT, 0, 0);\n\t\treturn 0;\n\t}\n\nno_vmexit:\n\treturn vmx_complete_nested_posted_interrupt(vcpu);\n}\n\nstatic u32 vmx_get_preemption_timer_value(struct kvm_vcpu *vcpu)\n{\n\tktime_t remaining =\n\t\thrtimer_get_remaining(&to_vmx(vcpu)->nested.preemption_timer);\n\tu64 value;\n\n\tif (ktime_to_ns(remaining) <= 0)\n\t\treturn 0;\n\n\tvalue = ktime_to_ns(remaining) * vcpu->arch.virtual_tsc_khz;\n\tdo_div(value, 1000000);\n\treturn value >> VMX_MISC_EMULATED_PREEMPTION_TIMER_RATE;\n}\n\nstatic bool is_vmcs12_ext_field(unsigned long field)\n{\n\tswitch (field) {\n\tcase GUEST_ES_SELECTOR:\n\tcase GUEST_CS_SELECTOR:\n\tcase GUEST_SS_SELECTOR:\n\tcase GUEST_DS_SELECTOR:\n\tcase GUEST_FS_SELECTOR:\n\tcase GUEST_GS_SELECTOR:\n\tcase GUEST_LDTR_SELECTOR:\n\tcase GUEST_TR_SELECTOR:\n\tcase GUEST_ES_LIMIT:\n\tcase GUEST_CS_LIMIT:\n\tcase GUEST_SS_LIMIT:\n\tcase GUEST_DS_LIMIT:\n\tcase GUEST_FS_LIMIT:\n\tcase GUEST_GS_LIMIT:\n\tcase GUEST_LDTR_LIMIT:\n\tcase GUEST_TR_LIMIT:\n\tcase GUEST_GDTR_LIMIT:\n\tcase GUEST_IDTR_LIMIT:\n\tcase GUEST_ES_AR_BYTES:\n\tcase GUEST_DS_AR_BYTES:\n\tcase GUEST_FS_AR_BYTES:\n\tcase GUEST_GS_AR_BYTES:\n\tcase GUEST_LDTR_AR_BYTES:\n\tcase GUEST_TR_AR_BYTES:\n\tcase GUEST_ES_BASE:\n\tcase GUEST_CS_BASE:\n\tcase GUEST_SS_BASE:\n\tcase GUEST_DS_BASE:\n\tcase GUEST_FS_BASE:\n\tcase GUEST_GS_BASE:\n\tcase GUEST_LDTR_BASE:\n\tcase GUEST_TR_BASE:\n\tcase GUEST_GDTR_BASE:\n\tcase GUEST_IDTR_BASE:\n\tcase GUEST_PENDING_DBG_EXCEPTIONS:\n\tcase GUEST_BNDCFGS:\n\t\treturn true;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn false;\n}\n\nstatic void sync_vmcs02_to_vmcs12_rare(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmcs12 *vmcs12)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tvmcs12->guest_es_selector = vmcs_read16(GUEST_ES_SELECTOR);\n\tvmcs12->guest_cs_selector = vmcs_read16(GUEST_CS_SELECTOR);\n\tvmcs12->guest_ss_selector = vmcs_read16(GUEST_SS_SELECTOR);\n\tvmcs12->guest_ds_selector = vmcs_read16(GUEST_DS_SELECTOR);\n\tvmcs12->guest_fs_selector = vmcs_read16(GUEST_FS_SELECTOR);\n\tvmcs12->guest_gs_selector = vmcs_read16(GUEST_GS_SELECTOR);\n\tvmcs12->guest_ldtr_selector = vmcs_read16(GUEST_LDTR_SELECTOR);\n\tvmcs12->guest_tr_selector = vmcs_read16(GUEST_TR_SELECTOR);\n\tvmcs12->guest_es_limit = vmcs_read32(GUEST_ES_LIMIT);\n\tvmcs12->guest_cs_limit = vmcs_read32(GUEST_CS_LIMIT);\n\tvmcs12->guest_ss_limit = vmcs_read32(GUEST_SS_LIMIT);\n\tvmcs12->guest_ds_limit = vmcs_read32(GUEST_DS_LIMIT);\n\tvmcs12->guest_fs_limit = vmcs_read32(GUEST_FS_LIMIT);\n\tvmcs12->guest_gs_limit = vmcs_read32(GUEST_GS_LIMIT);\n\tvmcs12->guest_ldtr_limit = vmcs_read32(GUEST_LDTR_LIMIT);\n\tvmcs12->guest_tr_limit = vmcs_read32(GUEST_TR_LIMIT);\n\tvmcs12->guest_gdtr_limit = vmcs_read32(GUEST_GDTR_LIMIT);\n\tvmcs12->guest_idtr_limit = vmcs_read32(GUEST_IDTR_LIMIT);\n\tvmcs12->guest_es_ar_bytes = vmcs_read32(GUEST_ES_AR_BYTES);\n\tvmcs12->guest_ds_ar_bytes = vmcs_read32(GUEST_DS_AR_BYTES);\n\tvmcs12->guest_fs_ar_bytes = vmcs_read32(GUEST_FS_AR_BYTES);\n\tvmcs12->guest_gs_ar_bytes = vmcs_read32(GUEST_GS_AR_BYTES);\n\tvmcs12->guest_ldtr_ar_bytes = vmcs_read32(GUEST_LDTR_AR_BYTES);\n\tvmcs12->guest_tr_ar_bytes = vmcs_read32(GUEST_TR_AR_BYTES);\n\tvmcs12->guest_es_base = vmcs_readl(GUEST_ES_BASE);\n\tvmcs12->guest_cs_base = vmcs_readl(GUEST_CS_BASE);\n\tvmcs12->guest_ss_base = vmcs_readl(GUEST_SS_BASE);\n\tvmcs12->guest_ds_base = vmcs_readl(GUEST_DS_BASE);\n\tvmcs12->guest_fs_base = vmcs_readl(GUEST_FS_BASE);\n\tvmcs12->guest_gs_base = vmcs_readl(GUEST_GS_BASE);\n\tvmcs12->guest_ldtr_base = vmcs_readl(GUEST_LDTR_BASE);\n\tvmcs12->guest_tr_base = vmcs_readl(GUEST_TR_BASE);\n\tvmcs12->guest_gdtr_base = vmcs_readl(GUEST_GDTR_BASE);\n\tvmcs12->guest_idtr_base = vmcs_readl(GUEST_IDTR_BASE);\n\tvmcs12->guest_pending_dbg_exceptions =\n\t\tvmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS);\n\n\tvmx->nested.need_sync_vmcs02_to_vmcs12_rare = false;\n}\n\nstatic void copy_vmcs02_to_vmcs12_rare(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmcs12 *vmcs12)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint cpu;\n\n\tif (!vmx->nested.need_sync_vmcs02_to_vmcs12_rare)\n\t\treturn;\n\n\n\tWARN_ON_ONCE(vmx->loaded_vmcs != &vmx->vmcs01);\n\n\tcpu = get_cpu();\n\tvmx->loaded_vmcs = &vmx->nested.vmcs02;\n\tvmx_vcpu_load_vmcs(vcpu, cpu, &vmx->vmcs01);\n\n\tsync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);\n\n\tvmx->loaded_vmcs = &vmx->vmcs01;\n\tvmx_vcpu_load_vmcs(vcpu, cpu, &vmx->nested.vmcs02);\n\tput_cpu();\n}\n\n \nstatic void sync_vmcs02_to_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (evmptr_is_valid(vmx->nested.hv_evmcs_vmptr))\n\t\tsync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);\n\n\tvmx->nested.need_sync_vmcs02_to_vmcs12_rare =\n\t\t!evmptr_is_valid(vmx->nested.hv_evmcs_vmptr);\n\n\tvmcs12->guest_cr0 = vmcs12_guest_cr0(vcpu, vmcs12);\n\tvmcs12->guest_cr4 = vmcs12_guest_cr4(vcpu, vmcs12);\n\n\tvmcs12->guest_rsp = kvm_rsp_read(vcpu);\n\tvmcs12->guest_rip = kvm_rip_read(vcpu);\n\tvmcs12->guest_rflags = vmcs_readl(GUEST_RFLAGS);\n\n\tvmcs12->guest_cs_ar_bytes = vmcs_read32(GUEST_CS_AR_BYTES);\n\tvmcs12->guest_ss_ar_bytes = vmcs_read32(GUEST_SS_AR_BYTES);\n\n\tvmcs12->guest_interruptibility_info =\n\t\tvmcs_read32(GUEST_INTERRUPTIBILITY_INFO);\n\n\tif (vcpu->arch.mp_state == KVM_MP_STATE_HALTED)\n\t\tvmcs12->guest_activity_state = GUEST_ACTIVITY_HLT;\n\telse if (vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED)\n\t\tvmcs12->guest_activity_state = GUEST_ACTIVITY_WAIT_SIPI;\n\telse\n\t\tvmcs12->guest_activity_state = GUEST_ACTIVITY_ACTIVE;\n\n\tif (nested_cpu_has_preemption_timer(vmcs12) &&\n\t    vmcs12->vm_exit_controls & VM_EXIT_SAVE_VMX_PREEMPTION_TIMER &&\n\t    !vmx->nested.nested_run_pending)\n\t\tvmcs12->vmx_preemption_timer_value =\n\t\t\tvmx_get_preemption_timer_value(vcpu);\n\n\t \n\tif (enable_ept) {\n\t\tvmcs12->guest_cr3 = vmcs_readl(GUEST_CR3);\n\t\tif (nested_cpu_has_ept(vmcs12) && is_pae_paging(vcpu)) {\n\t\t\tvmcs12->guest_pdptr0 = vmcs_read64(GUEST_PDPTR0);\n\t\t\tvmcs12->guest_pdptr1 = vmcs_read64(GUEST_PDPTR1);\n\t\t\tvmcs12->guest_pdptr2 = vmcs_read64(GUEST_PDPTR2);\n\t\t\tvmcs12->guest_pdptr3 = vmcs_read64(GUEST_PDPTR3);\n\t\t}\n\t}\n\n\tvmcs12->guest_linear_address = vmcs_readl(GUEST_LINEAR_ADDRESS);\n\n\tif (nested_cpu_has_vid(vmcs12))\n\t\tvmcs12->guest_intr_status = vmcs_read16(GUEST_INTR_STATUS);\n\n\tvmcs12->vm_entry_controls =\n\t\t(vmcs12->vm_entry_controls & ~VM_ENTRY_IA32E_MODE) |\n\t\t(vm_entry_controls_get(to_vmx(vcpu)) & VM_ENTRY_IA32E_MODE);\n\n\tif (vmcs12->vm_exit_controls & VM_EXIT_SAVE_DEBUG_CONTROLS)\n\t\tkvm_get_dr(vcpu, 7, (unsigned long *)&vmcs12->guest_dr7);\n\n\tif (vmcs12->vm_exit_controls & VM_EXIT_SAVE_IA32_EFER)\n\t\tvmcs12->guest_ia32_efer = vcpu->arch.efer;\n}\n\n \nstatic void prepare_vmcs12(struct kvm_vcpu *vcpu, struct vmcs12 *vmcs12,\n\t\t\t   u32 vm_exit_reason, u32 exit_intr_info,\n\t\t\t   unsigned long exit_qualification)\n{\n\t \n\tvmcs12->vm_exit_reason = vm_exit_reason;\n\tif (to_vmx(vcpu)->exit_reason.enclave_mode)\n\t\tvmcs12->vm_exit_reason |= VMX_EXIT_REASONS_SGX_ENCLAVE_MODE;\n\tvmcs12->exit_qualification = exit_qualification;\n\n\t \n\tif (!(vmcs12->vm_exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY)) {\n\t\tvmcs12->launch_state = 1;\n\n\t\t \n\t\tvmcs12->vm_entry_intr_info_field &= ~INTR_INFO_VALID_MASK;\n\n\t\t \n\t\tvmcs12_save_pending_event(vcpu, vmcs12,\n\t\t\t\t\t  vm_exit_reason, exit_intr_info);\n\n\t\tvmcs12->vm_exit_intr_info = exit_intr_info;\n\t\tvmcs12->vm_exit_instruction_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);\n\t\tvmcs12->vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\n\t\t \n\t\tif (nested_vmx_store_msr(vcpu,\n\t\t\t\t\t vmcs12->vm_exit_msr_store_addr,\n\t\t\t\t\t vmcs12->vm_exit_msr_store_count))\n\t\t\tnested_vmx_abort(vcpu,\n\t\t\t\t\t VMX_ABORT_SAVE_GUEST_MSR_FAIL);\n\t}\n}\n\n \nstatic void load_vmcs12_host_state(struct kvm_vcpu *vcpu,\n\t\t\t\t   struct vmcs12 *vmcs12)\n{\n\tenum vm_entry_failure_code ignored;\n\tstruct kvm_segment seg;\n\n\tif (vmcs12->vm_exit_controls & VM_EXIT_LOAD_IA32_EFER)\n\t\tvcpu->arch.efer = vmcs12->host_ia32_efer;\n\telse if (vmcs12->vm_exit_controls & VM_EXIT_HOST_ADDR_SPACE_SIZE)\n\t\tvcpu->arch.efer |= (EFER_LMA | EFER_LME);\n\telse\n\t\tvcpu->arch.efer &= ~(EFER_LMA | EFER_LME);\n\tvmx_set_efer(vcpu, vcpu->arch.efer);\n\n\tkvm_rsp_write(vcpu, vmcs12->host_rsp);\n\tkvm_rip_write(vcpu, vmcs12->host_rip);\n\tvmx_set_rflags(vcpu, X86_EFLAGS_FIXED);\n\tvmx_set_interrupt_shadow(vcpu, 0);\n\n\t \n\tvcpu->arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();\n\tvmx_set_cr0(vcpu, vmcs12->host_cr0);\n\n\t \n\tvcpu->arch.cr4_guest_owned_bits = ~vmcs_readl(CR4_GUEST_HOST_MASK);\n\tvmx_set_cr4(vcpu, vmcs12->host_cr4);\n\n\tnested_ept_uninit_mmu_context(vcpu);\n\n\t \n\tif (nested_vmx_load_cr3(vcpu, vmcs12->host_cr3, false, true, &ignored))\n\t\tnested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_PDPTE_FAIL);\n\n\tnested_vmx_transition_tlb_flush(vcpu, vmcs12, false);\n\n\tvmcs_write32(GUEST_SYSENTER_CS, vmcs12->host_ia32_sysenter_cs);\n\tvmcs_writel(GUEST_SYSENTER_ESP, vmcs12->host_ia32_sysenter_esp);\n\tvmcs_writel(GUEST_SYSENTER_EIP, vmcs12->host_ia32_sysenter_eip);\n\tvmcs_writel(GUEST_IDTR_BASE, vmcs12->host_idtr_base);\n\tvmcs_writel(GUEST_GDTR_BASE, vmcs12->host_gdtr_base);\n\tvmcs_write32(GUEST_IDTR_LIMIT, 0xFFFF);\n\tvmcs_write32(GUEST_GDTR_LIMIT, 0xFFFF);\n\n\t \n\tif (vmcs12->vm_exit_controls & VM_EXIT_CLEAR_BNDCFGS)\n\t\tvmcs_write64(GUEST_BNDCFGS, 0);\n\n\tif (vmcs12->vm_exit_controls & VM_EXIT_LOAD_IA32_PAT) {\n\t\tvmcs_write64(GUEST_IA32_PAT, vmcs12->host_ia32_pat);\n\t\tvcpu->arch.pat = vmcs12->host_ia32_pat;\n\t}\n\tif ((vmcs12->vm_exit_controls & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL) &&\n\t    kvm_pmu_has_perf_global_ctrl(vcpu_to_pmu(vcpu)))\n\t\tWARN_ON_ONCE(kvm_set_msr(vcpu, MSR_CORE_PERF_GLOBAL_CTRL,\n\t\t\t\t\t vmcs12->host_ia32_perf_global_ctrl));\n\n\t \n\tseg = (struct kvm_segment) {\n\t\t.base = 0,\n\t\t.limit = 0xFFFFFFFF,\n\t\t.selector = vmcs12->host_cs_selector,\n\t\t.type = 11,\n\t\t.present = 1,\n\t\t.s = 1,\n\t\t.g = 1\n\t};\n\tif (vmcs12->vm_exit_controls & VM_EXIT_HOST_ADDR_SPACE_SIZE)\n\t\tseg.l = 1;\n\telse\n\t\tseg.db = 1;\n\t__vmx_set_segment(vcpu, &seg, VCPU_SREG_CS);\n\tseg = (struct kvm_segment) {\n\t\t.base = 0,\n\t\t.limit = 0xFFFFFFFF,\n\t\t.type = 3,\n\t\t.present = 1,\n\t\t.s = 1,\n\t\t.db = 1,\n\t\t.g = 1\n\t};\n\tseg.selector = vmcs12->host_ds_selector;\n\t__vmx_set_segment(vcpu, &seg, VCPU_SREG_DS);\n\tseg.selector = vmcs12->host_es_selector;\n\t__vmx_set_segment(vcpu, &seg, VCPU_SREG_ES);\n\tseg.selector = vmcs12->host_ss_selector;\n\t__vmx_set_segment(vcpu, &seg, VCPU_SREG_SS);\n\tseg.selector = vmcs12->host_fs_selector;\n\tseg.base = vmcs12->host_fs_base;\n\t__vmx_set_segment(vcpu, &seg, VCPU_SREG_FS);\n\tseg.selector = vmcs12->host_gs_selector;\n\tseg.base = vmcs12->host_gs_base;\n\t__vmx_set_segment(vcpu, &seg, VCPU_SREG_GS);\n\tseg = (struct kvm_segment) {\n\t\t.base = vmcs12->host_tr_base,\n\t\t.limit = 0x67,\n\t\t.selector = vmcs12->host_tr_selector,\n\t\t.type = 11,\n\t\t.present = 1\n\t};\n\t__vmx_set_segment(vcpu, &seg, VCPU_SREG_TR);\n\n\tmemset(&seg, 0, sizeof(seg));\n\tseg.unusable = 1;\n\t__vmx_set_segment(vcpu, &seg, VCPU_SREG_LDTR);\n\n\tkvm_set_dr(vcpu, 7, 0x400);\n\tvmcs_write64(GUEST_IA32_DEBUGCTL, 0);\n\n\tif (nested_vmx_load_msr(vcpu, vmcs12->vm_exit_msr_load_addr,\n\t\t\t\tvmcs12->vm_exit_msr_load_count))\n\t\tnested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);\n\n\tto_vmx(vcpu)->emulation_required = vmx_emulation_required(vcpu);\n}\n\nstatic inline u64 nested_vmx_get_vmcs01_guest_efer(struct vcpu_vmx *vmx)\n{\n\tstruct vmx_uret_msr *efer_msr;\n\tunsigned int i;\n\n\tif (vm_entry_controls_get(vmx) & VM_ENTRY_LOAD_IA32_EFER)\n\t\treturn vmcs_read64(GUEST_IA32_EFER);\n\n\tif (cpu_has_load_ia32_efer())\n\t\treturn host_efer;\n\n\tfor (i = 0; i < vmx->msr_autoload.guest.nr; ++i) {\n\t\tif (vmx->msr_autoload.guest.val[i].index == MSR_EFER)\n\t\t\treturn vmx->msr_autoload.guest.val[i].value;\n\t}\n\n\tefer_msr = vmx_find_uret_msr(vmx, MSR_EFER);\n\tif (efer_msr)\n\t\treturn efer_msr->data;\n\n\treturn host_efer;\n}\n\nstatic void nested_vmx_restore_host_state(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmx_msr_entry g, h;\n\tgpa_t gpa;\n\tu32 i, j;\n\n\tvcpu->arch.pat = vmcs_read64(GUEST_IA32_PAT);\n\n\tif (vmcs12->vm_entry_controls & VM_ENTRY_LOAD_DEBUG_CONTROLS) {\n\t\t \n\t\tif (vcpu->guest_debug & KVM_GUESTDBG_USE_HW_BP)\n\t\t\tkvm_set_dr(vcpu, 7, DR7_FIXED_1);\n\t\telse\n\t\t\tWARN_ON(kvm_set_dr(vcpu, 7, vmcs_readl(GUEST_DR7)));\n\t}\n\n\t \n\tvmx_set_efer(vcpu, nested_vmx_get_vmcs01_guest_efer(vmx));\n\n\tvcpu->arch.cr0_guest_owned_bits = vmx_l1_guest_owned_cr0_bits();\n\tvmx_set_cr0(vcpu, vmcs_readl(CR0_READ_SHADOW));\n\n\tvcpu->arch.cr4_guest_owned_bits = ~vmcs_readl(CR4_GUEST_HOST_MASK);\n\tvmx_set_cr4(vcpu, vmcs_readl(CR4_READ_SHADOW));\n\n\tnested_ept_uninit_mmu_context(vcpu);\n\tvcpu->arch.cr3 = vmcs_readl(GUEST_CR3);\n\tkvm_register_mark_available(vcpu, VCPU_EXREG_CR3);\n\n\t \n\tif (enable_ept && is_pae_paging(vcpu))\n\t\tept_save_pdptrs(vcpu);\n\n\tkvm_mmu_reset_context(vcpu);\n\n\t \n\tfor (i = 0; i < vmcs12->vm_entry_msr_load_count; i++) {\n\t\tgpa = vmcs12->vm_entry_msr_load_addr + (i * sizeof(g));\n\t\tif (kvm_vcpu_read_guest(vcpu, gpa, &g, sizeof(g))) {\n\t\t\tpr_debug_ratelimited(\n\t\t\t\t\"%s read MSR index failed (%u, 0x%08llx)\\n\",\n\t\t\t\t__func__, i, gpa);\n\t\t\tgoto vmabort;\n\t\t}\n\n\t\tfor (j = 0; j < vmcs12->vm_exit_msr_load_count; j++) {\n\t\t\tgpa = vmcs12->vm_exit_msr_load_addr + (j * sizeof(h));\n\t\t\tif (kvm_vcpu_read_guest(vcpu, gpa, &h, sizeof(h))) {\n\t\t\t\tpr_debug_ratelimited(\n\t\t\t\t\t\"%s read MSR failed (%u, 0x%08llx)\\n\",\n\t\t\t\t\t__func__, j, gpa);\n\t\t\t\tgoto vmabort;\n\t\t\t}\n\t\t\tif (h.index != g.index)\n\t\t\t\tcontinue;\n\t\t\tif (h.value == g.value)\n\t\t\t\tbreak;\n\n\t\t\tif (nested_vmx_load_msr_check(vcpu, &h)) {\n\t\t\t\tpr_debug_ratelimited(\n\t\t\t\t\t\"%s check failed (%u, 0x%x, 0x%x)\\n\",\n\t\t\t\t\t__func__, j, h.index, h.reserved);\n\t\t\t\tgoto vmabort;\n\t\t\t}\n\n\t\t\tif (kvm_set_msr(vcpu, h.index, h.value)) {\n\t\t\t\tpr_debug_ratelimited(\n\t\t\t\t\t\"%s WRMSR failed (%u, 0x%x, 0x%llx)\\n\",\n\t\t\t\t\t__func__, j, h.index, h.value);\n\t\t\t\tgoto vmabort;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn;\n\nvmabort:\n\tnested_vmx_abort(vcpu, VMX_ABORT_LOAD_HOST_MSR_FAIL);\n}\n\n \nvoid nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 vm_exit_reason,\n\t\t       u32 exit_intr_info, unsigned long exit_qualification)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\t \n\tvmx->nested.mtf_pending = false;\n\n\t \n\tWARN_ON_ONCE(vmx->nested.nested_run_pending);\n\n\tif (kvm_check_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu)) {\n\t\t \n\t\t(void)nested_get_evmcs_page(vcpu);\n\t}\n\n\t \n\tkvm_service_local_tlb_flush_requests(vcpu);\n\n\t \n\tif (enable_ept && is_pae_paging(vcpu))\n\t\tvmx_ept_load_pdptrs(vcpu);\n\n\tleave_guest_mode(vcpu);\n\n\tif (nested_cpu_has_preemption_timer(vmcs12))\n\t\thrtimer_cancel(&to_vmx(vcpu)->nested.preemption_timer);\n\n\tif (nested_cpu_has(vmcs12, CPU_BASED_USE_TSC_OFFSETTING)) {\n\t\tvcpu->arch.tsc_offset = vcpu->arch.l1_tsc_offset;\n\t\tif (nested_cpu_has2(vmcs12, SECONDARY_EXEC_TSC_SCALING))\n\t\t\tvcpu->arch.tsc_scaling_ratio = vcpu->arch.l1_tsc_scaling_ratio;\n\t}\n\n\tif (likely(!vmx->fail)) {\n\t\tsync_vmcs02_to_vmcs12(vcpu, vmcs12);\n\n\t\tif (vm_exit_reason != -1)\n\t\t\tprepare_vmcs12(vcpu, vmcs12, vm_exit_reason,\n\t\t\t\t       exit_intr_info, exit_qualification);\n\n\t\t \n\t\tnested_flush_cached_shadow_vmcs12(vcpu, vmcs12);\n\t} else {\n\t\t \n\t\tWARN_ON_ONCE(vmcs_read32(VM_INSTRUCTION_ERROR) !=\n\t\t\t     VMXERR_ENTRY_INVALID_CONTROL_FIELD);\n\t\tWARN_ON_ONCE(nested_early_check);\n\t}\n\n\t \n\tvcpu->arch.nmi_injected = false;\n\tkvm_clear_exception_queue(vcpu);\n\tkvm_clear_interrupt_queue(vcpu);\n\n\tvmx_switch_vmcs(vcpu, &vmx->vmcs01);\n\n\t \n\tif (guest_cpuid_has(vcpu, X86_FEATURE_SPEC_CTRL))\n\t\tindirect_branch_prediction_barrier();\n\n\t \n\tvmcs_write32(VM_EXIT_MSR_LOAD_COUNT, vmx->msr_autoload.host.nr);\n\tvmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, vmx->msr_autoload.guest.nr);\n\tvmcs_write64(TSC_OFFSET, vcpu->arch.tsc_offset);\n\tif (kvm_caps.has_tsc_control)\n\t\tvmcs_write64(TSC_MULTIPLIER, vcpu->arch.tsc_scaling_ratio);\n\n\tif (vmx->nested.l1_tpr_threshold != -1)\n\t\tvmcs_write32(TPR_THRESHOLD, vmx->nested.l1_tpr_threshold);\n\n\tif (vmx->nested.change_vmcs01_virtual_apic_mode) {\n\t\tvmx->nested.change_vmcs01_virtual_apic_mode = false;\n\t\tvmx_set_virtual_apic_mode(vcpu);\n\t}\n\n\tif (vmx->nested.update_vmcs01_cpu_dirty_logging) {\n\t\tvmx->nested.update_vmcs01_cpu_dirty_logging = false;\n\t\tvmx_update_cpu_dirty_logging(vcpu);\n\t}\n\n\t \n\tkvm_vcpu_unmap(vcpu, &vmx->nested.apic_access_page_map, false);\n\tkvm_vcpu_unmap(vcpu, &vmx->nested.virtual_apic_map, true);\n\tkvm_vcpu_unmap(vcpu, &vmx->nested.pi_desc_map, true);\n\tvmx->nested.pi_desc = NULL;\n\n\tif (vmx->nested.reload_vmcs01_apic_access_page) {\n\t\tvmx->nested.reload_vmcs01_apic_access_page = false;\n\t\tkvm_make_request(KVM_REQ_APIC_PAGE_RELOAD, vcpu);\n\t}\n\n\tif (vmx->nested.update_vmcs01_apicv_status) {\n\t\tvmx->nested.update_vmcs01_apicv_status = false;\n\t\tkvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);\n\t}\n\n\tif ((vm_exit_reason != -1) &&\n\t    (enable_shadow_vmcs || evmptr_is_valid(vmx->nested.hv_evmcs_vmptr)))\n\t\tvmx->nested.need_vmcs12_to_shadow_sync = true;\n\n\t \n\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\n\tif (likely(!vmx->fail)) {\n\t\tif ((u16)vm_exit_reason == EXIT_REASON_EXTERNAL_INTERRUPT &&\n\t\t    nested_exit_intr_ack_set(vcpu)) {\n\t\t\tint irq = kvm_cpu_get_interrupt(vcpu);\n\t\t\tWARN_ON(irq < 0);\n\t\t\tvmcs12->vm_exit_intr_info = irq |\n\t\t\t\tINTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR;\n\t\t}\n\n\t\tif (vm_exit_reason != -1)\n\t\t\ttrace_kvm_nested_vmexit_inject(vmcs12->vm_exit_reason,\n\t\t\t\t\t\t       vmcs12->exit_qualification,\n\t\t\t\t\t\t       vmcs12->idt_vectoring_info_field,\n\t\t\t\t\t\t       vmcs12->vm_exit_intr_info,\n\t\t\t\t\t\t       vmcs12->vm_exit_intr_error_code,\n\t\t\t\t\t\t       KVM_ISA_VMX);\n\n\t\tload_vmcs12_host_state(vcpu, vmcs12);\n\n\t\treturn;\n\t}\n\n\t \n\t(void)nested_vmx_fail(vcpu, VMXERR_ENTRY_INVALID_CONTROL_FIELD);\n\n\t \n\tnested_vmx_restore_host_state(vcpu);\n\n\tvmx->fail = 0;\n}\n\nstatic void nested_vmx_triple_fault(struct kvm_vcpu *vcpu)\n{\n\tkvm_clear_request(KVM_REQ_TRIPLE_FAULT, vcpu);\n\tnested_vmx_vmexit(vcpu, EXIT_REASON_TRIPLE_FAULT, 0, 0);\n}\n\n \nint get_vmx_mem_address(struct kvm_vcpu *vcpu, unsigned long exit_qualification,\n\t\t\tu32 vmx_instruction_info, bool wr, int len, gva_t *ret)\n{\n\tgva_t off;\n\tbool exn;\n\tstruct kvm_segment s;\n\n\t \n\tint  scaling = vmx_instruction_info & 3;\n\tint  addr_size = (vmx_instruction_info >> 7) & 7;\n\tbool is_reg = vmx_instruction_info & (1u << 10);\n\tint  seg_reg = (vmx_instruction_info >> 15) & 7;\n\tint  index_reg = (vmx_instruction_info >> 18) & 0xf;\n\tbool index_is_valid = !(vmx_instruction_info & (1u << 22));\n\tint  base_reg       = (vmx_instruction_info >> 23) & 0xf;\n\tbool base_is_valid  = !(vmx_instruction_info & (1u << 27));\n\n\tif (is_reg) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\t \n\t \n\toff = exit_qualification;  \n\tif (addr_size == 1)\n\t\toff = (gva_t)sign_extend64(off, 31);\n\telse if (addr_size == 0)\n\t\toff = (gva_t)sign_extend64(off, 15);\n\tif (base_is_valid)\n\t\toff += kvm_register_read(vcpu, base_reg);\n\tif (index_is_valid)\n\t\toff += kvm_register_read(vcpu, index_reg) << scaling;\n\tvmx_get_segment(vcpu, &s, seg_reg);\n\n\t \n\tif (addr_size == 1)  \n\t\toff &= 0xffffffff;\n\telse if (addr_size == 0)  \n\t\toff &= 0xffff;\n\n\t \n\texn = false;\n\tif (is_long_mode(vcpu)) {\n\t\t \n\t\tif (seg_reg == VCPU_SREG_FS || seg_reg == VCPU_SREG_GS)\n\t\t\t*ret = s.base + off;\n\t\telse\n\t\t\t*ret = off;\n\n\t\t \n\t\texn = is_noncanonical_address(*ret, vcpu);\n\t} else {\n\t\t \n\t\t*ret = (s.base + off) & 0xffffffff;\n\n\t\t \n\t\tif (wr)\n\t\t\t \n\t\t\texn = ((s.type & 0xa) == 0 || (s.type & 8));\n\t\telse\n\t\t\t \n\t\t\texn = ((s.type & 0xa) == 8);\n\t\tif (exn) {\n\t\t\tkvm_queue_exception_e(vcpu, GP_VECTOR, 0);\n\t\t\treturn 1;\n\t\t}\n\t\t \n\t\texn = (s.unusable != 0);\n\n\t\t \n\t\tif (!(s.base == 0 && s.limit == 0xffffffff &&\n\t\t     ((s.type & 8) || !(s.type & 4))))\n\t\t\texn = exn || ((u64)off + len - 1 > s.limit);\n\t}\n\tif (exn) {\n\t\tkvm_queue_exception_e(vcpu,\n\t\t\t\t      seg_reg == VCPU_SREG_SS ?\n\t\t\t\t\t\tSS_VECTOR : GP_VECTOR,\n\t\t\t\t      0);\n\t\treturn 1;\n\t}\n\n\treturn 0;\n}\n\nstatic int nested_vmx_get_vmptr(struct kvm_vcpu *vcpu, gpa_t *vmpointer,\n\t\t\t\tint *ret)\n{\n\tgva_t gva;\n\tstruct x86_exception e;\n\tint r;\n\n\tif (get_vmx_mem_address(vcpu, vmx_get_exit_qual(vcpu),\n\t\t\t\tvmcs_read32(VMX_INSTRUCTION_INFO), false,\n\t\t\t\tsizeof(*vmpointer), &gva)) {\n\t\t*ret = 1;\n\t\treturn -EINVAL;\n\t}\n\n\tr = kvm_read_guest_virt(vcpu, gva, vmpointer, sizeof(*vmpointer), &e);\n\tif (r != X86EMUL_CONTINUE) {\n\t\t*ret = kvm_handle_memory_failure(vcpu, r, &e);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\n \nstatic struct vmcs *alloc_shadow_vmcs(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct loaded_vmcs *loaded_vmcs = vmx->loaded_vmcs;\n\n\t \n\tif (WARN_ON(loaded_vmcs != &vmx->vmcs01 || loaded_vmcs->shadow_vmcs))\n\t\treturn loaded_vmcs->shadow_vmcs;\n\n\tloaded_vmcs->shadow_vmcs = alloc_vmcs(true);\n\tif (loaded_vmcs->shadow_vmcs)\n\t\tvmcs_clear(loaded_vmcs->shadow_vmcs);\n\n\treturn loaded_vmcs->shadow_vmcs;\n}\n\nstatic int enter_vmx_operation(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tint r;\n\n\tr = alloc_loaded_vmcs(&vmx->nested.vmcs02);\n\tif (r < 0)\n\t\tgoto out_vmcs02;\n\n\tvmx->nested.cached_vmcs12 = kzalloc(VMCS12_SIZE, GFP_KERNEL_ACCOUNT);\n\tif (!vmx->nested.cached_vmcs12)\n\t\tgoto out_cached_vmcs12;\n\n\tvmx->nested.shadow_vmcs12_cache.gpa = INVALID_GPA;\n\tvmx->nested.cached_shadow_vmcs12 = kzalloc(VMCS12_SIZE, GFP_KERNEL_ACCOUNT);\n\tif (!vmx->nested.cached_shadow_vmcs12)\n\t\tgoto out_cached_shadow_vmcs12;\n\n\tif (enable_shadow_vmcs && !alloc_shadow_vmcs(vcpu))\n\t\tgoto out_shadow_vmcs;\n\n\thrtimer_init(&vmx->nested.preemption_timer, CLOCK_MONOTONIC,\n\t\t     HRTIMER_MODE_ABS_PINNED);\n\tvmx->nested.preemption_timer.function = vmx_preemption_timer_fn;\n\n\tvmx->nested.vpid02 = allocate_vpid();\n\n\tvmx->nested.vmcs02_initialized = false;\n\tvmx->nested.vmxon = true;\n\n\tif (vmx_pt_mode_is_host_guest()) {\n\t\tvmx->pt_desc.guest.ctl = 0;\n\t\tpt_update_intercept_for_msr(vcpu);\n\t}\n\n\treturn 0;\n\nout_shadow_vmcs:\n\tkfree(vmx->nested.cached_shadow_vmcs12);\n\nout_cached_shadow_vmcs12:\n\tkfree(vmx->nested.cached_vmcs12);\n\nout_cached_vmcs12:\n\tfree_loaded_vmcs(&vmx->nested.vmcs02);\n\nout_vmcs02:\n\treturn -ENOMEM;\n}\n\n \nstatic int handle_vmxon(struct kvm_vcpu *vcpu)\n{\n\tint ret;\n\tgpa_t vmptr;\n\tuint32_t revision;\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tconst u64 VMXON_NEEDED_FEATURES = FEAT_CTL_LOCKED\n\t\t| FEAT_CTL_VMX_ENABLED_OUTSIDE_SMX;\n\n\t \n\tif (!kvm_is_cr4_bit_set(vcpu, X86_CR4_VMXE)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\t \n\tif (vmx_get_cpl(vcpu)) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\tif (vmx->nested.vmxon)\n\t\treturn nested_vmx_fail(vcpu, VMXERR_VMXON_IN_VMX_ROOT_OPERATION);\n\n\t \n\tif (!nested_host_cr0_valid(vcpu, kvm_read_cr0(vcpu)) ||\n\t    !nested_host_cr4_valid(vcpu, kvm_read_cr4(vcpu))) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\tif ((vmx->msr_ia32_feature_control & VMXON_NEEDED_FEATURES)\n\t\t\t!= VMXON_NEEDED_FEATURES) {\n\t\tkvm_inject_gp(vcpu, 0);\n\t\treturn 1;\n\t}\n\n\tif (nested_vmx_get_vmptr(vcpu, &vmptr, &ret))\n\t\treturn ret;\n\n\t \n\tif (!page_address_valid(vcpu, vmptr))\n\t\treturn nested_vmx_failInvalid(vcpu);\n\n\tif (kvm_read_guest(vcpu->kvm, vmptr, &revision, sizeof(revision)) ||\n\t    revision != VMCS12_REVISION)\n\t\treturn nested_vmx_failInvalid(vcpu);\n\n\tvmx->nested.vmxon_ptr = vmptr;\n\tret = enter_vmx_operation(vcpu);\n\tif (ret)\n\t\treturn ret;\n\n\treturn nested_vmx_succeed(vcpu);\n}\n\nstatic inline void nested_release_vmcs12(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\n\tif (vmx->nested.current_vmptr == INVALID_GPA)\n\t\treturn;\n\n\tcopy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));\n\n\tif (enable_shadow_vmcs) {\n\t\t \n\t\tcopy_shadow_to_vmcs12(vmx);\n\t\tvmx_disable_shadow_vmcs(vmx);\n\t}\n\tvmx->nested.posted_intr_nv = -1;\n\n\t \n\tkvm_vcpu_write_guest_page(vcpu,\n\t\t\t\t  vmx->nested.current_vmptr >> PAGE_SHIFT,\n\t\t\t\t  vmx->nested.cached_vmcs12, 0, VMCS12_SIZE);\n\n\tkvm_mmu_free_roots(vcpu->kvm, &vcpu->arch.guest_mmu, KVM_MMU_ROOTS_ALL);\n\n\tvmx->nested.current_vmptr = INVALID_GPA;\n}\n\n \nstatic int handle_vmxoff(struct kvm_vcpu *vcpu)\n{\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tfree_nested(vcpu);\n\n\tif (kvm_apic_has_pending_init_or_sipi(vcpu))\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\treturn nested_vmx_succeed(vcpu);\n}\n\n \nstatic int handle_vmclear(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 zero = 0;\n\tgpa_t vmptr;\n\tint r;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (nested_vmx_get_vmptr(vcpu, &vmptr, &r))\n\t\treturn r;\n\n\tif (!page_address_valid(vcpu, vmptr))\n\t\treturn nested_vmx_fail(vcpu, VMXERR_VMCLEAR_INVALID_ADDRESS);\n\n\tif (vmptr == vmx->nested.vmxon_ptr)\n\t\treturn nested_vmx_fail(vcpu, VMXERR_VMCLEAR_VMXON_POINTER);\n\n\t \n\tif (likely(!guest_cpuid_has_evmcs(vcpu) ||\n\t\t   !evmptr_is_valid(nested_get_evmptr(vcpu)))) {\n\t\tif (vmptr == vmx->nested.current_vmptr)\n\t\t\tnested_release_vmcs12(vcpu);\n\n\t\t \n\t\t(void)kvm_vcpu_write_guest(vcpu,\n\t\t\t\t\t   vmptr + offsetof(struct vmcs12,\n\t\t\t\t\t\t\t    launch_state),\n\t\t\t\t\t   &zero, sizeof(zero));\n\t} else if (vmx->nested.hv_evmcs && vmptr == vmx->nested.hv_evmcs_vmptr) {\n\t\tnested_release_evmcs(vcpu);\n\t}\n\n\treturn nested_vmx_succeed(vcpu);\n}\n\n \nstatic int handle_vmlaunch(struct kvm_vcpu *vcpu)\n{\n\treturn nested_vmx_run(vcpu, true);\n}\n\n \nstatic int handle_vmresume(struct kvm_vcpu *vcpu)\n{\n\n\treturn nested_vmx_run(vcpu, false);\n}\n\nstatic int handle_vmread(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12 = is_guest_mode(vcpu) ? get_shadow_vmcs12(vcpu)\n\t\t\t\t\t\t    : get_vmcs12(vcpu);\n\tunsigned long exit_qualification = vmx_get_exit_qual(vcpu);\n\tu32 instr_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct x86_exception e;\n\tunsigned long field;\n\tu64 value;\n\tgva_t gva = 0;\n\tshort offset;\n\tint len, r;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\t \n\tfield = kvm_register_read(vcpu, (((instr_info) >> 28) & 0xf));\n\n\tif (!evmptr_is_valid(vmx->nested.hv_evmcs_vmptr)) {\n\t\t \n\t\tif (vmx->nested.current_vmptr == INVALID_GPA ||\n\t\t    (is_guest_mode(vcpu) &&\n\t\t     get_vmcs12(vcpu)->vmcs_link_pointer == INVALID_GPA))\n\t\t\treturn nested_vmx_failInvalid(vcpu);\n\n\t\toffset = get_vmcs12_field_offset(field);\n\t\tif (offset < 0)\n\t\t\treturn nested_vmx_fail(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);\n\n\t\tif (!is_guest_mode(vcpu) && is_vmcs12_ext_field(field))\n\t\t\tcopy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);\n\n\t\t \n\t\tvalue = vmcs12_read_any(vmcs12, field, offset);\n\t} else {\n\t\t \n\t\tif (WARN_ON_ONCE(is_guest_mode(vcpu)))\n\t\t\treturn nested_vmx_failInvalid(vcpu);\n\n\t\toffset = evmcs_field_offset(field, NULL);\n\t\tif (offset < 0)\n\t\t\treturn nested_vmx_fail(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);\n\n\t\t \n\t\tvalue = evmcs_read_any(vmx->nested.hv_evmcs, field, offset);\n\t}\n\n\t \n\tif (instr_info & BIT(10)) {\n\t\tkvm_register_write(vcpu, (((instr_info) >> 3) & 0xf), value);\n\t} else {\n\t\tlen = is_64_bit_mode(vcpu) ? 8 : 4;\n\t\tif (get_vmx_mem_address(vcpu, exit_qualification,\n\t\t\t\t\tinstr_info, true, len, &gva))\n\t\t\treturn 1;\n\t\t \n\t\tr = kvm_write_guest_virt_system(vcpu, gva, &value, len, &e);\n\t\tif (r != X86EMUL_CONTINUE)\n\t\t\treturn kvm_handle_memory_failure(vcpu, r, &e);\n\t}\n\n\treturn nested_vmx_succeed(vcpu);\n}\n\nstatic bool is_shadow_field_rw(unsigned long field)\n{\n\tswitch (field) {\n#define SHADOW_FIELD_RW(x, y) case x:\n#include \"vmcs_shadow_fields.h\"\n\t\treturn true;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn false;\n}\n\nstatic bool is_shadow_field_ro(unsigned long field)\n{\n\tswitch (field) {\n#define SHADOW_FIELD_RO(x, y) case x:\n#include \"vmcs_shadow_fields.h\"\n\t\treturn true;\n\tdefault:\n\t\tbreak;\n\t}\n\treturn false;\n}\n\nstatic int handle_vmwrite(struct kvm_vcpu *vcpu)\n{\n\tstruct vmcs12 *vmcs12 = is_guest_mode(vcpu) ? get_shadow_vmcs12(vcpu)\n\t\t\t\t\t\t    : get_vmcs12(vcpu);\n\tunsigned long exit_qualification = vmx_get_exit_qual(vcpu);\n\tu32 instr_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct x86_exception e;\n\tunsigned long field;\n\tshort offset;\n\tgva_t gva;\n\tint len, r;\n\n\t \n\tu64 value = 0;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\t \n\tif (vmx->nested.current_vmptr == INVALID_GPA ||\n\t    (is_guest_mode(vcpu) &&\n\t     get_vmcs12(vcpu)->vmcs_link_pointer == INVALID_GPA))\n\t\treturn nested_vmx_failInvalid(vcpu);\n\n\tif (instr_info & BIT(10))\n\t\tvalue = kvm_register_read(vcpu, (((instr_info) >> 3) & 0xf));\n\telse {\n\t\tlen = is_64_bit_mode(vcpu) ? 8 : 4;\n\t\tif (get_vmx_mem_address(vcpu, exit_qualification,\n\t\t\t\t\tinstr_info, false, len, &gva))\n\t\t\treturn 1;\n\t\tr = kvm_read_guest_virt(vcpu, gva, &value, len, &e);\n\t\tif (r != X86EMUL_CONTINUE)\n\t\t\treturn kvm_handle_memory_failure(vcpu, r, &e);\n\t}\n\n\tfield = kvm_register_read(vcpu, (((instr_info) >> 28) & 0xf));\n\n\toffset = get_vmcs12_field_offset(field);\n\tif (offset < 0)\n\t\treturn nested_vmx_fail(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);\n\n\t \n\tif (vmcs_field_readonly(field) &&\n\t    !nested_cpu_has_vmwrite_any_field(vcpu))\n\t\treturn nested_vmx_fail(vcpu, VMXERR_VMWRITE_READ_ONLY_VMCS_COMPONENT);\n\n\t \n\tif (!is_guest_mode(vcpu) && !is_shadow_field_rw(field))\n\t\tcopy_vmcs02_to_vmcs12_rare(vcpu, vmcs12);\n\n\t \n\tif (field >= GUEST_ES_AR_BYTES && field <= GUEST_TR_AR_BYTES)\n\t\tvalue &= 0x1f0ff;\n\n\tvmcs12_write_any(vmcs12, field, offset, value);\n\n\t \n\tif (!is_guest_mode(vcpu) && !is_shadow_field_rw(field)) {\n\t\t \n\t\tif (enable_shadow_vmcs && is_shadow_field_ro(field)) {\n\t\t\tpreempt_disable();\n\t\t\tvmcs_load(vmx->vmcs01.shadow_vmcs);\n\n\t\t\t__vmcs_writel(field, value);\n\n\t\t\tvmcs_clear(vmx->vmcs01.shadow_vmcs);\n\t\t\tvmcs_load(vmx->loaded_vmcs->vmcs);\n\t\t\tpreempt_enable();\n\t\t}\n\t\tvmx->nested.dirty_vmcs12 = true;\n\t}\n\n\treturn nested_vmx_succeed(vcpu);\n}\n\nstatic void set_current_vmptr(struct vcpu_vmx *vmx, gpa_t vmptr)\n{\n\tvmx->nested.current_vmptr = vmptr;\n\tif (enable_shadow_vmcs) {\n\t\tsecondary_exec_controls_setbit(vmx, SECONDARY_EXEC_SHADOW_VMCS);\n\t\tvmcs_write64(VMCS_LINK_POINTER,\n\t\t\t     __pa(vmx->vmcs01.shadow_vmcs));\n\t\tvmx->nested.need_vmcs12_to_shadow_sync = true;\n\t}\n\tvmx->nested.dirty_vmcs12 = true;\n\tvmx->nested.force_msr_bitmap_recalc = true;\n}\n\n \nstatic int handle_vmptrld(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tgpa_t vmptr;\n\tint r;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (nested_vmx_get_vmptr(vcpu, &vmptr, &r))\n\t\treturn r;\n\n\tif (!page_address_valid(vcpu, vmptr))\n\t\treturn nested_vmx_fail(vcpu, VMXERR_VMPTRLD_INVALID_ADDRESS);\n\n\tif (vmptr == vmx->nested.vmxon_ptr)\n\t\treturn nested_vmx_fail(vcpu, VMXERR_VMPTRLD_VMXON_POINTER);\n\n\t \n\tif (evmptr_is_valid(vmx->nested.hv_evmcs_vmptr))\n\t\treturn 1;\n\n\tif (vmx->nested.current_vmptr != vmptr) {\n\t\tstruct gfn_to_hva_cache *ghc = &vmx->nested.vmcs12_cache;\n\t\tstruct vmcs_hdr hdr;\n\n\t\tif (kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, vmptr, VMCS12_SIZE)) {\n\t\t\t \n\t\t\treturn nested_vmx_fail(vcpu,\n\t\t\t\tVMXERR_VMPTRLD_INCORRECT_VMCS_REVISION_ID);\n\t\t}\n\n\t\tif (kvm_read_guest_offset_cached(vcpu->kvm, ghc, &hdr,\n\t\t\t\t\t\t offsetof(struct vmcs12, hdr),\n\t\t\t\t\t\t sizeof(hdr))) {\n\t\t\treturn nested_vmx_fail(vcpu,\n\t\t\t\tVMXERR_VMPTRLD_INCORRECT_VMCS_REVISION_ID);\n\t\t}\n\n\t\tif (hdr.revision_id != VMCS12_REVISION ||\n\t\t    (hdr.shadow_vmcs &&\n\t\t     !nested_cpu_has_vmx_shadow_vmcs(vcpu))) {\n\t\t\treturn nested_vmx_fail(vcpu,\n\t\t\t\tVMXERR_VMPTRLD_INCORRECT_VMCS_REVISION_ID);\n\t\t}\n\n\t\tnested_release_vmcs12(vcpu);\n\n\t\t \n\t\tif (kvm_read_guest_cached(vcpu->kvm, ghc, vmx->nested.cached_vmcs12,\n\t\t\t\t\t  VMCS12_SIZE)) {\n\t\t\treturn nested_vmx_fail(vcpu,\n\t\t\t\tVMXERR_VMPTRLD_INCORRECT_VMCS_REVISION_ID);\n\t\t}\n\n\t\tset_current_vmptr(vmx, vmptr);\n\t}\n\n\treturn nested_vmx_succeed(vcpu);\n}\n\n \nstatic int handle_vmptrst(struct kvm_vcpu *vcpu)\n{\n\tunsigned long exit_qual = vmx_get_exit_qual(vcpu);\n\tu32 instr_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\tgpa_t current_vmptr = to_vmx(vcpu)->nested.current_vmptr;\n\tstruct x86_exception e;\n\tgva_t gva;\n\tint r;\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tif (unlikely(evmptr_is_valid(to_vmx(vcpu)->nested.hv_evmcs_vmptr)))\n\t\treturn 1;\n\n\tif (get_vmx_mem_address(vcpu, exit_qual, instr_info,\n\t\t\t\ttrue, sizeof(gpa_t), &gva))\n\t\treturn 1;\n\t \n\tr = kvm_write_guest_virt_system(vcpu, gva, (void *)&current_vmptr,\n\t\t\t\t\tsizeof(gpa_t), &e);\n\tif (r != X86EMUL_CONTINUE)\n\t\treturn kvm_handle_memory_failure(vcpu, r, &e);\n\n\treturn nested_vmx_succeed(vcpu);\n}\n\n \nstatic int handle_invept(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 vmx_instruction_info, types;\n\tunsigned long type, roots_to_free;\n\tstruct kvm_mmu *mmu;\n\tgva_t gva;\n\tstruct x86_exception e;\n\tstruct {\n\t\tu64 eptp, gpa;\n\t} operand;\n\tint i, r, gpr_index;\n\n\tif (!(vmx->nested.msrs.secondary_ctls_high &\n\t      SECONDARY_EXEC_ENABLE_EPT) ||\n\t    !(vmx->nested.msrs.ept_caps & VMX_EPT_INVEPT_BIT)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tvmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\tgpr_index = vmx_get_instr_info_reg2(vmx_instruction_info);\n\ttype = kvm_register_read(vcpu, gpr_index);\n\n\ttypes = (vmx->nested.msrs.ept_caps >> VMX_EPT_EXTENT_SHIFT) & 6;\n\n\tif (type >= 32 || !(types & (1 << type)))\n\t\treturn nested_vmx_fail(vcpu, VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\n\t \n\tif (get_vmx_mem_address(vcpu, vmx_get_exit_qual(vcpu),\n\t\t\tvmx_instruction_info, false, sizeof(operand), &gva))\n\t\treturn 1;\n\tr = kvm_read_guest_virt(vcpu, gva, &operand, sizeof(operand), &e);\n\tif (r != X86EMUL_CONTINUE)\n\t\treturn kvm_handle_memory_failure(vcpu, r, &e);\n\n\t \n\tmmu = &vcpu->arch.guest_mmu;\n\n\tswitch (type) {\n\tcase VMX_EPT_EXTENT_CONTEXT:\n\t\tif (!nested_vmx_check_eptp(vcpu, operand.eptp))\n\t\t\treturn nested_vmx_fail(vcpu,\n\t\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\n\t\troots_to_free = 0;\n\t\tif (nested_ept_root_matches(mmu->root.hpa, mmu->root.pgd,\n\t\t\t\t\t    operand.eptp))\n\t\t\troots_to_free |= KVM_MMU_ROOT_CURRENT;\n\n\t\tfor (i = 0; i < KVM_MMU_NUM_PREV_ROOTS; i++) {\n\t\t\tif (nested_ept_root_matches(mmu->prev_roots[i].hpa,\n\t\t\t\t\t\t    mmu->prev_roots[i].pgd,\n\t\t\t\t\t\t    operand.eptp))\n\t\t\t\troots_to_free |= KVM_MMU_ROOT_PREVIOUS(i);\n\t\t}\n\t\tbreak;\n\tcase VMX_EPT_EXTENT_GLOBAL:\n\t\troots_to_free = KVM_MMU_ROOTS_ALL;\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t\tbreak;\n\t}\n\n\tif (roots_to_free)\n\t\tkvm_mmu_free_roots(vcpu->kvm, mmu, roots_to_free);\n\n\treturn nested_vmx_succeed(vcpu);\n}\n\nstatic int handle_invvpid(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tu32 vmx_instruction_info;\n\tunsigned long type, types;\n\tgva_t gva;\n\tstruct x86_exception e;\n\tstruct {\n\t\tu64 vpid;\n\t\tu64 gla;\n\t} operand;\n\tu16 vpid02;\n\tint r, gpr_index;\n\n\tif (!(vmx->nested.msrs.secondary_ctls_high &\n\t      SECONDARY_EXEC_ENABLE_VPID) ||\n\t\t\t!(vmx->nested.msrs.vpid_caps & VMX_VPID_INVVPID_BIT)) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tif (!nested_vmx_check_permission(vcpu))\n\t\treturn 1;\n\n\tvmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\tgpr_index = vmx_get_instr_info_reg2(vmx_instruction_info);\n\ttype = kvm_register_read(vcpu, gpr_index);\n\n\ttypes = (vmx->nested.msrs.vpid_caps &\n\t\t\tVMX_VPID_EXTENT_SUPPORTED_MASK) >> 8;\n\n\tif (type >= 32 || !(types & (1 << type)))\n\t\treturn nested_vmx_fail(vcpu,\n\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\n\t \n\tif (get_vmx_mem_address(vcpu, vmx_get_exit_qual(vcpu),\n\t\t\tvmx_instruction_info, false, sizeof(operand), &gva))\n\t\treturn 1;\n\tr = kvm_read_guest_virt(vcpu, gva, &operand, sizeof(operand), &e);\n\tif (r != X86EMUL_CONTINUE)\n\t\treturn kvm_handle_memory_failure(vcpu, r, &e);\n\n\tif (operand.vpid >> 16)\n\t\treturn nested_vmx_fail(vcpu,\n\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\n\tvpid02 = nested_get_vpid02(vcpu);\n\tswitch (type) {\n\tcase VMX_VPID_EXTENT_INDIVIDUAL_ADDR:\n\t\tif (!operand.vpid ||\n\t\t    is_noncanonical_address(operand.gla, vcpu))\n\t\t\treturn nested_vmx_fail(vcpu,\n\t\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\t\tvpid_sync_vcpu_addr(vpid02, operand.gla);\n\t\tbreak;\n\tcase VMX_VPID_EXTENT_SINGLE_CONTEXT:\n\tcase VMX_VPID_EXTENT_SINGLE_NON_GLOBAL:\n\t\tif (!operand.vpid)\n\t\t\treturn nested_vmx_fail(vcpu,\n\t\t\t\tVMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);\n\t\tvpid_sync_context(vpid02);\n\t\tbreak;\n\tcase VMX_VPID_EXTENT_ALL_CONTEXT:\n\t\tvpid_sync_context(vpid02);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn kvm_skip_emulated_instruction(vcpu);\n\t}\n\n\t \n\tif (!enable_ept)\n\t\tkvm_mmu_free_guest_mode_roots(vcpu->kvm, &vcpu->arch.root_mmu);\n\n\treturn nested_vmx_succeed(vcpu);\n}\n\nstatic int nested_vmx_eptp_switching(struct kvm_vcpu *vcpu,\n\t\t\t\t     struct vmcs12 *vmcs12)\n{\n\tu32 index = kvm_rcx_read(vcpu);\n\tu64 new_eptp;\n\n\tif (WARN_ON_ONCE(!nested_cpu_has_ept(vmcs12)))\n\t\treturn 1;\n\tif (index >= VMFUNC_EPTP_ENTRIES)\n\t\treturn 1;\n\n\tif (kvm_vcpu_read_guest_page(vcpu, vmcs12->eptp_list_address >> PAGE_SHIFT,\n\t\t\t\t     &new_eptp, index * 8, 8))\n\t\treturn 1;\n\n\t \n\tif (vmcs12->ept_pointer != new_eptp) {\n\t\tif (!nested_vmx_check_eptp(vcpu, new_eptp))\n\t\t\treturn 1;\n\n\t\tvmcs12->ept_pointer = new_eptp;\n\t\tnested_ept_new_eptp(vcpu);\n\n\t\tif (!nested_cpu_has_vpid(vmcs12))\n\t\t\tkvm_make_request(KVM_REQ_TLB_FLUSH_GUEST, vcpu);\n\t}\n\n\treturn 0;\n}\n\nstatic int handle_vmfunc(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12;\n\tu32 function = kvm_rax_read(vcpu);\n\n\t \n\tif (WARN_ON_ONCE(!is_guest_mode(vcpu))) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tvmcs12 = get_vmcs12(vcpu);\n\n\t \n\tif (WARN_ON_ONCE((function > 63) || !nested_cpu_has_vmfunc(vmcs12))) {\n\t\tkvm_queue_exception(vcpu, UD_VECTOR);\n\t\treturn 1;\n\t}\n\n\tif (!(vmcs12->vm_function_control & BIT_ULL(function)))\n\t\tgoto fail;\n\n\tswitch (function) {\n\tcase 0:\n\t\tif (nested_vmx_eptp_switching(vcpu, vmcs12))\n\t\t\tgoto fail;\n\t\tbreak;\n\tdefault:\n\t\tgoto fail;\n\t}\n\treturn kvm_skip_emulated_instruction(vcpu);\n\nfail:\n\t \n\tnested_vmx_vmexit(vcpu, vmx->exit_reason.full,\n\t\t\t  vmx_get_intr_info(vcpu),\n\t\t\t  vmx_get_exit_qual(vcpu));\n\treturn 1;\n}\n\n \nbool nested_vmx_check_io_bitmaps(struct kvm_vcpu *vcpu, unsigned int port,\n\t\t\t\t int size)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tgpa_t bitmap, last_bitmap;\n\tu8 b;\n\n\tlast_bitmap = INVALID_GPA;\n\tb = -1;\n\n\twhile (size > 0) {\n\t\tif (port < 0x8000)\n\t\t\tbitmap = vmcs12->io_bitmap_a;\n\t\telse if (port < 0x10000)\n\t\t\tbitmap = vmcs12->io_bitmap_b;\n\t\telse\n\t\t\treturn true;\n\t\tbitmap += (port & 0x7fff) / 8;\n\n\t\tif (last_bitmap != bitmap)\n\t\t\tif (kvm_vcpu_read_guest(vcpu, bitmap, &b, 1))\n\t\t\t\treturn true;\n\t\tif (b & (1 << (port & 7)))\n\t\t\treturn true;\n\n\t\tport++;\n\t\tsize--;\n\t\tlast_bitmap = bitmap;\n\t}\n\n\treturn false;\n}\n\nstatic bool nested_vmx_exit_handled_io(struct kvm_vcpu *vcpu,\n\t\t\t\t       struct vmcs12 *vmcs12)\n{\n\tunsigned long exit_qualification;\n\tunsigned short port;\n\tint size;\n\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_USE_IO_BITMAPS))\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_UNCOND_IO_EXITING);\n\n\texit_qualification = vmx_get_exit_qual(vcpu);\n\n\tport = exit_qualification >> 16;\n\tsize = (exit_qualification & 7) + 1;\n\n\treturn nested_vmx_check_io_bitmaps(vcpu, port, size);\n}\n\n \nstatic bool nested_vmx_exit_handled_msr(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct vmcs12 *vmcs12,\n\t\t\t\t\tunion vmx_exit_reason exit_reason)\n{\n\tu32 msr_index = kvm_rcx_read(vcpu);\n\tgpa_t bitmap;\n\n\tif (!nested_cpu_has(vmcs12, CPU_BASED_USE_MSR_BITMAPS))\n\t\treturn true;\n\n\t \n\tbitmap = vmcs12->msr_bitmap;\n\tif (exit_reason.basic == EXIT_REASON_MSR_WRITE)\n\t\tbitmap += 2048;\n\tif (msr_index >= 0xc0000000) {\n\t\tmsr_index -= 0xc0000000;\n\t\tbitmap += 1024;\n\t}\n\n\t \n\tif (msr_index < 1024*8) {\n\t\tunsigned char b;\n\t\tif (kvm_vcpu_read_guest(vcpu, bitmap + msr_index/8, &b, 1))\n\t\t\treturn true;\n\t\treturn 1 & (b >> (msr_index & 7));\n\t} else\n\t\treturn true;  \n}\n\n \nstatic bool nested_vmx_exit_handled_cr(struct kvm_vcpu *vcpu,\n\tstruct vmcs12 *vmcs12)\n{\n\tunsigned long exit_qualification = vmx_get_exit_qual(vcpu);\n\tint cr = exit_qualification & 15;\n\tint reg;\n\tunsigned long val;\n\n\tswitch ((exit_qualification >> 4) & 3) {\n\tcase 0:  \n\t\treg = (exit_qualification >> 8) & 15;\n\t\tval = kvm_register_read(vcpu, reg);\n\t\tswitch (cr) {\n\t\tcase 0:\n\t\t\tif (vmcs12->cr0_guest_host_mask &\n\t\t\t    (val ^ vmcs12->cr0_read_shadow))\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\tcase 3:\n\t\t\tif (nested_cpu_has(vmcs12, CPU_BASED_CR3_LOAD_EXITING))\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\tif (vmcs12->cr4_guest_host_mask &\n\t\t\t    (vmcs12->cr4_read_shadow ^ val))\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tif (nested_cpu_has(vmcs12, CPU_BASED_CR8_LOAD_EXITING))\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase 2:  \n\t\tif ((vmcs12->cr0_guest_host_mask & X86_CR0_TS) &&\n\t\t    (vmcs12->cr0_read_shadow & X86_CR0_TS))\n\t\t\treturn true;\n\t\tbreak;\n\tcase 1:  \n\t\tswitch (cr) {\n\t\tcase 3:\n\t\t\tif (vmcs12->cpu_based_vm_exec_control &\n\t\t\t    CPU_BASED_CR3_STORE_EXITING)\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\tcase 8:\n\t\t\tif (vmcs12->cpu_based_vm_exec_control &\n\t\t\t    CPU_BASED_CR8_STORE_EXITING)\n\t\t\t\treturn true;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase 3:  \n\t\t \n\t\tval = (exit_qualification >> LMSW_SOURCE_DATA_SHIFT) & 0x0f;\n\t\tif (vmcs12->cr0_guest_host_mask & 0xe &\n\t\t    (val ^ vmcs12->cr0_read_shadow))\n\t\t\treturn true;\n\t\tif ((vmcs12->cr0_guest_host_mask & 0x1) &&\n\t\t    !(vmcs12->cr0_read_shadow & 0x1) &&\n\t\t    (val & 0x1))\n\t\t\treturn true;\n\t\tbreak;\n\t}\n\treturn false;\n}\n\nstatic bool nested_vmx_exit_handled_encls(struct kvm_vcpu *vcpu,\n\t\t\t\t\t  struct vmcs12 *vmcs12)\n{\n\tu32 encls_leaf;\n\n\tif (!guest_cpuid_has(vcpu, X86_FEATURE_SGX) ||\n\t    !nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENCLS_EXITING))\n\t\treturn false;\n\n\tencls_leaf = kvm_rax_read(vcpu);\n\tif (encls_leaf > 62)\n\t\tencls_leaf = 63;\n\treturn vmcs12->encls_exiting_bitmap & BIT_ULL(encls_leaf);\n}\n\nstatic bool nested_vmx_exit_handled_vmcs_access(struct kvm_vcpu *vcpu,\n\tstruct vmcs12 *vmcs12, gpa_t bitmap)\n{\n\tu32 vmx_instruction_info;\n\tunsigned long field;\n\tu8 b;\n\n\tif (!nested_cpu_has_shadow_vmcs(vmcs12))\n\t\treturn true;\n\n\t \n\tvmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);\n\tfield = kvm_register_read(vcpu, (((vmx_instruction_info) >> 28) & 0xf));\n\n\t \n\tif (field >> 15)\n\t\treturn true;\n\n\tif (kvm_vcpu_read_guest(vcpu, bitmap + field/8, &b, 1))\n\t\treturn true;\n\n\treturn 1 & (b >> (field & 7));\n}\n\nstatic bool nested_vmx_exit_handled_mtf(struct vmcs12 *vmcs12)\n{\n\tu32 entry_intr_info = vmcs12->vm_entry_intr_info_field;\n\n\tif (nested_cpu_has_mtf(vmcs12))\n\t\treturn true;\n\n\t \n\treturn entry_intr_info == (INTR_INFO_VALID_MASK\n\t\t\t\t   | INTR_TYPE_OTHER_EVENT);\n}\n\n \nstatic bool nested_vmx_l0_wants_exit(struct kvm_vcpu *vcpu,\n\t\t\t\t     union vmx_exit_reason exit_reason)\n{\n\tu32 intr_info;\n\n\tswitch ((u16)exit_reason.basic) {\n\tcase EXIT_REASON_EXCEPTION_NMI:\n\t\tintr_info = vmx_get_intr_info(vcpu);\n\t\tif (is_nmi(intr_info))\n\t\t\treturn true;\n\t\telse if (is_page_fault(intr_info))\n\t\t\treturn vcpu->arch.apf.host_apf_flags ||\n\t\t\t       vmx_need_pf_intercept(vcpu);\n\t\telse if (is_debug(intr_info) &&\n\t\t\t vcpu->guest_debug &\n\t\t\t (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))\n\t\t\treturn true;\n\t\telse if (is_breakpoint(intr_info) &&\n\t\t\t vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)\n\t\t\treturn true;\n\t\telse if (is_alignment_check(intr_info) &&\n\t\t\t !vmx_guest_inject_ac(vcpu))\n\t\t\treturn true;\n\t\treturn false;\n\tcase EXIT_REASON_EXTERNAL_INTERRUPT:\n\t\treturn true;\n\tcase EXIT_REASON_MCE_DURING_VMENTRY:\n\t\treturn true;\n\tcase EXIT_REASON_EPT_VIOLATION:\n\t\t \n\t\treturn true;\n\tcase EXIT_REASON_EPT_MISCONFIG:\n\t\t \n\t\treturn true;\n\tcase EXIT_REASON_PREEMPTION_TIMER:\n\t\treturn true;\n\tcase EXIT_REASON_PML_FULL:\n\t\t \n\t\treturn true;\n\tcase EXIT_REASON_VMFUNC:\n\t\t \n\t\treturn true;\n\tcase EXIT_REASON_BUS_LOCK:\n\t\t \n\t\treturn true;\n\tcase EXIT_REASON_VMCALL:\n\t\t \n\t\treturn guest_hv_cpuid_has_l2_tlb_flush(vcpu) &&\n\t\t\tnested_evmcs_l2_tlb_flush_enabled(vcpu) &&\n\t\t\tkvm_hv_is_tlb_flush_hcall(vcpu);\n\tdefault:\n\t\tbreak;\n\t}\n\treturn false;\n}\n\n \nstatic bool nested_vmx_l1_wants_exit(struct kvm_vcpu *vcpu,\n\t\t\t\t     union vmx_exit_reason exit_reason)\n{\n\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\tu32 intr_info;\n\n\tswitch ((u16)exit_reason.basic) {\n\tcase EXIT_REASON_EXCEPTION_NMI:\n\t\tintr_info = vmx_get_intr_info(vcpu);\n\t\tif (is_nmi(intr_info))\n\t\t\treturn true;\n\t\telse if (is_page_fault(intr_info))\n\t\t\treturn true;\n\t\treturn vmcs12->exception_bitmap &\n\t\t\t\t(1u << (intr_info & INTR_INFO_VECTOR_MASK));\n\tcase EXIT_REASON_EXTERNAL_INTERRUPT:\n\t\treturn nested_exit_on_intr(vcpu);\n\tcase EXIT_REASON_TRIPLE_FAULT:\n\t\treturn true;\n\tcase EXIT_REASON_INTERRUPT_WINDOW:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_INTR_WINDOW_EXITING);\n\tcase EXIT_REASON_NMI_WINDOW:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_NMI_WINDOW_EXITING);\n\tcase EXIT_REASON_TASK_SWITCH:\n\t\treturn true;\n\tcase EXIT_REASON_CPUID:\n\t\treturn true;\n\tcase EXIT_REASON_HLT:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_HLT_EXITING);\n\tcase EXIT_REASON_INVD:\n\t\treturn true;\n\tcase EXIT_REASON_INVLPG:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_INVLPG_EXITING);\n\tcase EXIT_REASON_RDPMC:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_RDPMC_EXITING);\n\tcase EXIT_REASON_RDRAND:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_RDRAND_EXITING);\n\tcase EXIT_REASON_RDSEED:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_RDSEED_EXITING);\n\tcase EXIT_REASON_RDTSC: case EXIT_REASON_RDTSCP:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_RDTSC_EXITING);\n\tcase EXIT_REASON_VMREAD:\n\t\treturn nested_vmx_exit_handled_vmcs_access(vcpu, vmcs12,\n\t\t\tvmcs12->vmread_bitmap);\n\tcase EXIT_REASON_VMWRITE:\n\t\treturn nested_vmx_exit_handled_vmcs_access(vcpu, vmcs12,\n\t\t\tvmcs12->vmwrite_bitmap);\n\tcase EXIT_REASON_VMCALL: case EXIT_REASON_VMCLEAR:\n\tcase EXIT_REASON_VMLAUNCH: case EXIT_REASON_VMPTRLD:\n\tcase EXIT_REASON_VMPTRST: case EXIT_REASON_VMRESUME:\n\tcase EXIT_REASON_VMOFF: case EXIT_REASON_VMON:\n\tcase EXIT_REASON_INVEPT: case EXIT_REASON_INVVPID:\n\t\t \n\t\treturn true;\n\tcase EXIT_REASON_CR_ACCESS:\n\t\treturn nested_vmx_exit_handled_cr(vcpu, vmcs12);\n\tcase EXIT_REASON_DR_ACCESS:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MOV_DR_EXITING);\n\tcase EXIT_REASON_IO_INSTRUCTION:\n\t\treturn nested_vmx_exit_handled_io(vcpu, vmcs12);\n\tcase EXIT_REASON_GDTR_IDTR: case EXIT_REASON_LDTR_TR:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_DESC);\n\tcase EXIT_REASON_MSR_READ:\n\tcase EXIT_REASON_MSR_WRITE:\n\t\treturn nested_vmx_exit_handled_msr(vcpu, vmcs12, exit_reason);\n\tcase EXIT_REASON_INVALID_STATE:\n\t\treturn true;\n\tcase EXIT_REASON_MWAIT_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MWAIT_EXITING);\n\tcase EXIT_REASON_MONITOR_TRAP_FLAG:\n\t\treturn nested_vmx_exit_handled_mtf(vmcs12);\n\tcase EXIT_REASON_MONITOR_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_MONITOR_EXITING);\n\tcase EXIT_REASON_PAUSE_INSTRUCTION:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_PAUSE_EXITING) ||\n\t\t\tnested_cpu_has2(vmcs12,\n\t\t\t\tSECONDARY_EXEC_PAUSE_LOOP_EXITING);\n\tcase EXIT_REASON_MCE_DURING_VMENTRY:\n\t\treturn true;\n\tcase EXIT_REASON_TPR_BELOW_THRESHOLD:\n\t\treturn nested_cpu_has(vmcs12, CPU_BASED_TPR_SHADOW);\n\tcase EXIT_REASON_APIC_ACCESS:\n\tcase EXIT_REASON_APIC_WRITE:\n\tcase EXIT_REASON_EOI_INDUCED:\n\t\t \n\t\treturn true;\n\tcase EXIT_REASON_INVPCID:\n\t\treturn\n\t\t\tnested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_INVPCID) &&\n\t\t\tnested_cpu_has(vmcs12, CPU_BASED_INVLPG_EXITING);\n\tcase EXIT_REASON_WBINVD:\n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_WBINVD_EXITING);\n\tcase EXIT_REASON_XSETBV:\n\t\treturn true;\n\tcase EXIT_REASON_XSAVES: case EXIT_REASON_XRSTORS:\n\t\t \n\t\treturn nested_cpu_has2(vmcs12, SECONDARY_EXEC_ENABLE_XSAVES);\n\tcase EXIT_REASON_UMWAIT:\n\tcase EXIT_REASON_TPAUSE:\n\t\treturn nested_cpu_has2(vmcs12,\n\t\t\tSECONDARY_EXEC_ENABLE_USR_WAIT_PAUSE);\n\tcase EXIT_REASON_ENCLS:\n\t\treturn nested_vmx_exit_handled_encls(vcpu, vmcs12);\n\tcase EXIT_REASON_NOTIFY:\n\t\t \n\t\treturn false;\n\tdefault:\n\t\treturn true;\n\t}\n}\n\n \nbool nested_vmx_reflect_vmexit(struct kvm_vcpu *vcpu)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunion vmx_exit_reason exit_reason = vmx->exit_reason;\n\tunsigned long exit_qual;\n\tu32 exit_intr_info;\n\n\tWARN_ON_ONCE(vmx->nested.nested_run_pending);\n\n\t \n\tif (unlikely(vmx->fail)) {\n\t\ttrace_kvm_nested_vmenter_failed(\n\t\t\t\"hardware VM-instruction error: \",\n\t\t\tvmcs_read32(VM_INSTRUCTION_ERROR));\n\t\texit_intr_info = 0;\n\t\texit_qual = 0;\n\t\tgoto reflect_vmexit;\n\t}\n\n\ttrace_kvm_nested_vmexit(vcpu, KVM_ISA_VMX);\n\n\t \n\tif (nested_vmx_l0_wants_exit(vcpu, exit_reason))\n\t\treturn false;\n\n\t \n\tif (!nested_vmx_l1_wants_exit(vcpu, exit_reason))\n\t\treturn false;\n\n\t \n\texit_intr_info = vmx_get_intr_info(vcpu);\n\tif (is_exception_with_error_code(exit_intr_info)) {\n\t\tstruct vmcs12 *vmcs12 = get_vmcs12(vcpu);\n\n\t\tvmcs12->vm_exit_intr_error_code =\n\t\t\tvmcs_read32(VM_EXIT_INTR_ERROR_CODE);\n\t}\n\texit_qual = vmx_get_exit_qual(vcpu);\n\nreflect_vmexit:\n\tnested_vmx_vmexit(vcpu, exit_reason.full, exit_intr_info, exit_qual);\n\treturn true;\n}\n\nstatic int vmx_get_nested_state(struct kvm_vcpu *vcpu,\n\t\t\t\tstruct kvm_nested_state __user *user_kvm_nested_state,\n\t\t\t\tu32 user_data_size)\n{\n\tstruct vcpu_vmx *vmx;\n\tstruct vmcs12 *vmcs12;\n\tstruct kvm_nested_state kvm_state = {\n\t\t.flags = 0,\n\t\t.format = KVM_STATE_NESTED_FORMAT_VMX,\n\t\t.size = sizeof(kvm_state),\n\t\t.hdr.vmx.flags = 0,\n\t\t.hdr.vmx.vmxon_pa = INVALID_GPA,\n\t\t.hdr.vmx.vmcs12_pa = INVALID_GPA,\n\t\t.hdr.vmx.preemption_timer_deadline = 0,\n\t};\n\tstruct kvm_vmx_nested_state_data __user *user_vmx_nested_state =\n\t\t&user_kvm_nested_state->data.vmx[0];\n\n\tif (!vcpu)\n\t\treturn kvm_state.size + sizeof(*user_vmx_nested_state);\n\n\tvmx = to_vmx(vcpu);\n\tvmcs12 = get_vmcs12(vcpu);\n\n\tif (guest_can_use(vcpu, X86_FEATURE_VMX) &&\n\t    (vmx->nested.vmxon || vmx->nested.smm.vmxon)) {\n\t\tkvm_state.hdr.vmx.vmxon_pa = vmx->nested.vmxon_ptr;\n\t\tkvm_state.hdr.vmx.vmcs12_pa = vmx->nested.current_vmptr;\n\n\t\tif (vmx_has_valid_vmcs12(vcpu)) {\n\t\t\tkvm_state.size += sizeof(user_vmx_nested_state->vmcs12);\n\n\t\t\t \n\t\t\tif (vmx->nested.hv_evmcs_vmptr != EVMPTR_INVALID)\n\t\t\t\tkvm_state.flags |= KVM_STATE_NESTED_EVMCS;\n\n\t\t\tif (is_guest_mode(vcpu) &&\n\t\t\t    nested_cpu_has_shadow_vmcs(vmcs12) &&\n\t\t\t    vmcs12->vmcs_link_pointer != INVALID_GPA)\n\t\t\t\tkvm_state.size += sizeof(user_vmx_nested_state->shadow_vmcs12);\n\t\t}\n\n\t\tif (vmx->nested.smm.vmxon)\n\t\t\tkvm_state.hdr.vmx.smm.flags |= KVM_STATE_NESTED_SMM_VMXON;\n\n\t\tif (vmx->nested.smm.guest_mode)\n\t\t\tkvm_state.hdr.vmx.smm.flags |= KVM_STATE_NESTED_SMM_GUEST_MODE;\n\n\t\tif (is_guest_mode(vcpu)) {\n\t\t\tkvm_state.flags |= KVM_STATE_NESTED_GUEST_MODE;\n\n\t\t\tif (vmx->nested.nested_run_pending)\n\t\t\t\tkvm_state.flags |= KVM_STATE_NESTED_RUN_PENDING;\n\n\t\t\tif (vmx->nested.mtf_pending)\n\t\t\t\tkvm_state.flags |= KVM_STATE_NESTED_MTF_PENDING;\n\n\t\t\tif (nested_cpu_has_preemption_timer(vmcs12) &&\n\t\t\t    vmx->nested.has_preemption_timer_deadline) {\n\t\t\t\tkvm_state.hdr.vmx.flags |=\n\t\t\t\t\tKVM_STATE_VMX_PREEMPTION_TIMER_DEADLINE;\n\t\t\t\tkvm_state.hdr.vmx.preemption_timer_deadline =\n\t\t\t\t\tvmx->nested.preemption_timer_deadline;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (user_data_size < kvm_state.size)\n\t\tgoto out;\n\n\tif (copy_to_user(user_kvm_nested_state, &kvm_state, sizeof(kvm_state)))\n\t\treturn -EFAULT;\n\n\tif (!vmx_has_valid_vmcs12(vcpu))\n\t\tgoto out;\n\n\t \n\tif (is_guest_mode(vcpu)) {\n\t\tsync_vmcs02_to_vmcs12(vcpu, vmcs12);\n\t\tsync_vmcs02_to_vmcs12_rare(vcpu, vmcs12);\n\t} else  {\n\t\tcopy_vmcs02_to_vmcs12_rare(vcpu, get_vmcs12(vcpu));\n\t\tif (!vmx->nested.need_vmcs12_to_shadow_sync) {\n\t\t\tif (evmptr_is_valid(vmx->nested.hv_evmcs_vmptr))\n\t\t\t\t \n\t\t\t\tcopy_enlightened_to_vmcs12(vmx, 0);\n\t\t\telse if (enable_shadow_vmcs)\n\t\t\t\tcopy_shadow_to_vmcs12(vmx);\n\t\t}\n\t}\n\n\tBUILD_BUG_ON(sizeof(user_vmx_nested_state->vmcs12) < VMCS12_SIZE);\n\tBUILD_BUG_ON(sizeof(user_vmx_nested_state->shadow_vmcs12) < VMCS12_SIZE);\n\n\t \n\tif (copy_to_user(user_vmx_nested_state->vmcs12, vmcs12, VMCS12_SIZE))\n\t\treturn -EFAULT;\n\n\tif (nested_cpu_has_shadow_vmcs(vmcs12) &&\n\t    vmcs12->vmcs_link_pointer != INVALID_GPA) {\n\t\tif (copy_to_user(user_vmx_nested_state->shadow_vmcs12,\n\t\t\t\t get_shadow_vmcs12(vcpu), VMCS12_SIZE))\n\t\t\treturn -EFAULT;\n\t}\nout:\n\treturn kvm_state.size;\n}\n\nvoid vmx_leave_nested(struct kvm_vcpu *vcpu)\n{\n\tif (is_guest_mode(vcpu)) {\n\t\tto_vmx(vcpu)->nested.nested_run_pending = 0;\n\t\tnested_vmx_vmexit(vcpu, -1, 0, 0);\n\t}\n\tfree_nested(vcpu);\n}\n\nstatic int vmx_set_nested_state(struct kvm_vcpu *vcpu,\n\t\t\t\tstruct kvm_nested_state __user *user_kvm_nested_state,\n\t\t\t\tstruct kvm_nested_state *kvm_state)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tstruct vmcs12 *vmcs12;\n\tenum vm_entry_failure_code ignored;\n\tstruct kvm_vmx_nested_state_data __user *user_vmx_nested_state =\n\t\t&user_kvm_nested_state->data.vmx[0];\n\tint ret;\n\n\tif (kvm_state->format != KVM_STATE_NESTED_FORMAT_VMX)\n\t\treturn -EINVAL;\n\n\tif (kvm_state->hdr.vmx.vmxon_pa == INVALID_GPA) {\n\t\tif (kvm_state->hdr.vmx.smm.flags)\n\t\t\treturn -EINVAL;\n\n\t\tif (kvm_state->hdr.vmx.vmcs12_pa != INVALID_GPA)\n\t\t\treturn -EINVAL;\n\n\t\t \n\t\tif (kvm_state->flags & ~KVM_STATE_NESTED_EVMCS)\n\t\t\treturn -EINVAL;\n\t} else {\n\t\tif (!guest_can_use(vcpu, X86_FEATURE_VMX))\n\t\t\treturn -EINVAL;\n\n\t\tif (!page_address_valid(vcpu, kvm_state->hdr.vmx.vmxon_pa))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif ((kvm_state->hdr.vmx.smm.flags & KVM_STATE_NESTED_SMM_GUEST_MODE) &&\n\t    (kvm_state->flags & KVM_STATE_NESTED_GUEST_MODE))\n\t\treturn -EINVAL;\n\n\tif (kvm_state->hdr.vmx.smm.flags &\n\t    ~(KVM_STATE_NESTED_SMM_GUEST_MODE | KVM_STATE_NESTED_SMM_VMXON))\n\t\treturn -EINVAL;\n\n\tif (kvm_state->hdr.vmx.flags & ~KVM_STATE_VMX_PREEMPTION_TIMER_DEADLINE)\n\t\treturn -EINVAL;\n\n\t \n\tif (is_smm(vcpu) ?\n\t\t(kvm_state->flags &\n\t\t (KVM_STATE_NESTED_GUEST_MODE | KVM_STATE_NESTED_RUN_PENDING))\n\t\t: kvm_state->hdr.vmx.smm.flags)\n\t\treturn -EINVAL;\n\n\tif ((kvm_state->hdr.vmx.smm.flags & KVM_STATE_NESTED_SMM_GUEST_MODE) &&\n\t    !(kvm_state->hdr.vmx.smm.flags & KVM_STATE_NESTED_SMM_VMXON))\n\t\treturn -EINVAL;\n\n\tif ((kvm_state->flags & KVM_STATE_NESTED_EVMCS) &&\n\t    (!guest_can_use(vcpu, X86_FEATURE_VMX) ||\n\t     !vmx->nested.enlightened_vmcs_enabled))\n\t\t\treturn -EINVAL;\n\n\tvmx_leave_nested(vcpu);\n\n\tif (kvm_state->hdr.vmx.vmxon_pa == INVALID_GPA)\n\t\treturn 0;\n\n\tvmx->nested.vmxon_ptr = kvm_state->hdr.vmx.vmxon_pa;\n\tret = enter_vmx_operation(vcpu);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tif (kvm_state->size < sizeof(*kvm_state) + sizeof(*vmcs12)) {\n\t\t \n\t\tif ((kvm_state->flags & KVM_STATE_NESTED_GUEST_MODE) ||\n\t\t    (kvm_state->flags & KVM_STATE_NESTED_EVMCS) ||\n\t\t    (kvm_state->hdr.vmx.vmcs12_pa != INVALID_GPA))\n\t\t\treturn -EINVAL;\n\t\telse\n\t\t\treturn 0;\n\t}\n\n\tif (kvm_state->hdr.vmx.vmcs12_pa != INVALID_GPA) {\n\t\tif (kvm_state->hdr.vmx.vmcs12_pa == kvm_state->hdr.vmx.vmxon_pa ||\n\t\t    !page_address_valid(vcpu, kvm_state->hdr.vmx.vmcs12_pa))\n\t\t\treturn -EINVAL;\n\n\t\tset_current_vmptr(vmx, kvm_state->hdr.vmx.vmcs12_pa);\n\t} else if (kvm_state->flags & KVM_STATE_NESTED_EVMCS) {\n\t\t \n\t\tvmx->nested.hv_evmcs_vmptr = EVMPTR_MAP_PENDING;\n\t\tkvm_make_request(KVM_REQ_GET_NESTED_STATE_PAGES, vcpu);\n\t} else {\n\t\treturn -EINVAL;\n\t}\n\n\tif (kvm_state->hdr.vmx.smm.flags & KVM_STATE_NESTED_SMM_VMXON) {\n\t\tvmx->nested.smm.vmxon = true;\n\t\tvmx->nested.vmxon = false;\n\n\t\tif (kvm_state->hdr.vmx.smm.flags & KVM_STATE_NESTED_SMM_GUEST_MODE)\n\t\t\tvmx->nested.smm.guest_mode = true;\n\t}\n\n\tvmcs12 = get_vmcs12(vcpu);\n\tif (copy_from_user(vmcs12, user_vmx_nested_state->vmcs12, sizeof(*vmcs12)))\n\t\treturn -EFAULT;\n\n\tif (vmcs12->hdr.revision_id != VMCS12_REVISION)\n\t\treturn -EINVAL;\n\n\tif (!(kvm_state->flags & KVM_STATE_NESTED_GUEST_MODE))\n\t\treturn 0;\n\n\tvmx->nested.nested_run_pending =\n\t\t!!(kvm_state->flags & KVM_STATE_NESTED_RUN_PENDING);\n\n\tvmx->nested.mtf_pending =\n\t\t!!(kvm_state->flags & KVM_STATE_NESTED_MTF_PENDING);\n\n\tret = -EINVAL;\n\tif (nested_cpu_has_shadow_vmcs(vmcs12) &&\n\t    vmcs12->vmcs_link_pointer != INVALID_GPA) {\n\t\tstruct vmcs12 *shadow_vmcs12 = get_shadow_vmcs12(vcpu);\n\n\t\tif (kvm_state->size <\n\t\t    sizeof(*kvm_state) +\n\t\t    sizeof(user_vmx_nested_state->vmcs12) + sizeof(*shadow_vmcs12))\n\t\t\tgoto error_guest_mode;\n\n\t\tif (copy_from_user(shadow_vmcs12,\n\t\t\t\t   user_vmx_nested_state->shadow_vmcs12,\n\t\t\t\t   sizeof(*shadow_vmcs12))) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto error_guest_mode;\n\t\t}\n\n\t\tif (shadow_vmcs12->hdr.revision_id != VMCS12_REVISION ||\n\t\t    !shadow_vmcs12->hdr.shadow_vmcs)\n\t\t\tgoto error_guest_mode;\n\t}\n\n\tvmx->nested.has_preemption_timer_deadline = false;\n\tif (kvm_state->hdr.vmx.flags & KVM_STATE_VMX_PREEMPTION_TIMER_DEADLINE) {\n\t\tvmx->nested.has_preemption_timer_deadline = true;\n\t\tvmx->nested.preemption_timer_deadline =\n\t\t\tkvm_state->hdr.vmx.preemption_timer_deadline;\n\t}\n\n\tif (nested_vmx_check_controls(vcpu, vmcs12) ||\n\t    nested_vmx_check_host_state(vcpu, vmcs12) ||\n\t    nested_vmx_check_guest_state(vcpu, vmcs12, &ignored))\n\t\tgoto error_guest_mode;\n\n\tvmx->nested.dirty_vmcs12 = true;\n\tvmx->nested.force_msr_bitmap_recalc = true;\n\tret = nested_vmx_enter_non_root_mode(vcpu, false);\n\tif (ret)\n\t\tgoto error_guest_mode;\n\n\tif (vmx->nested.mtf_pending)\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\treturn 0;\n\nerror_guest_mode:\n\tvmx->nested.nested_run_pending = 0;\n\treturn ret;\n}\n\nvoid nested_vmx_set_vmcs_shadowing_bitmap(void)\n{\n\tif (enable_shadow_vmcs) {\n\t\tvmcs_write64(VMREAD_BITMAP, __pa(vmx_vmread_bitmap));\n\t\tvmcs_write64(VMWRITE_BITMAP, __pa(vmx_vmwrite_bitmap));\n\t}\n}\n\n \n#define VMCS12_IDX_TO_ENC(idx) ((u16)(((u16)(idx) >> 6) | ((u16)(idx) << 10)))\n\nstatic u64 nested_vmx_calc_vmcs_enum_msr(void)\n{\n\t \n\tunsigned int max_idx, idx;\n\tint i;\n\n\t \n\tmax_idx = 0;\n\tfor (i = 0; i < nr_vmcs12_fields; i++) {\n\t\t \n\t\tif (!vmcs12_field_offsets[i])\n\t\t\tcontinue;\n\n\t\tidx = vmcs_field_index(VMCS12_IDX_TO_ENC(i));\n\t\tif (idx > max_idx)\n\t\t\tmax_idx = idx;\n\t}\n\n\treturn (u64)max_idx << VMCS_FIELD_INDEX_SHIFT;\n}\n\nstatic void nested_vmx_setup_pinbased_ctls(struct vmcs_config *vmcs_conf,\n\t\t\t\t\t   struct nested_vmx_msrs *msrs)\n{\n\tmsrs->pinbased_ctls_low =\n\t\tPIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR;\n\n\tmsrs->pinbased_ctls_high = vmcs_conf->pin_based_exec_ctrl;\n\tmsrs->pinbased_ctls_high &=\n\t\tPIN_BASED_EXT_INTR_MASK |\n\t\tPIN_BASED_NMI_EXITING |\n\t\tPIN_BASED_VIRTUAL_NMIS |\n\t\t(enable_apicv ? PIN_BASED_POSTED_INTR : 0);\n\tmsrs->pinbased_ctls_high |=\n\t\tPIN_BASED_ALWAYSON_WITHOUT_TRUE_MSR |\n\t\tPIN_BASED_VMX_PREEMPTION_TIMER;\n}\n\nstatic void nested_vmx_setup_exit_ctls(struct vmcs_config *vmcs_conf,\n\t\t\t\t       struct nested_vmx_msrs *msrs)\n{\n\tmsrs->exit_ctls_low =\n\t\tVM_EXIT_ALWAYSON_WITHOUT_TRUE_MSR;\n\n\tmsrs->exit_ctls_high = vmcs_conf->vmexit_ctrl;\n\tmsrs->exit_ctls_high &=\n#ifdef CONFIG_X86_64\n\t\tVM_EXIT_HOST_ADDR_SPACE_SIZE |\n#endif\n\t\tVM_EXIT_LOAD_IA32_PAT | VM_EXIT_SAVE_IA32_PAT |\n\t\tVM_EXIT_CLEAR_BNDCFGS;\n\tmsrs->exit_ctls_high |=\n\t\tVM_EXIT_ALWAYSON_WITHOUT_TRUE_MSR |\n\t\tVM_EXIT_LOAD_IA32_EFER | VM_EXIT_SAVE_IA32_EFER |\n\t\tVM_EXIT_SAVE_VMX_PREEMPTION_TIMER | VM_EXIT_ACK_INTR_ON_EXIT |\n\t\tVM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL;\n\n\t \n\tmsrs->exit_ctls_low &= ~VM_EXIT_SAVE_DEBUG_CONTROLS;\n}\n\nstatic void nested_vmx_setup_entry_ctls(struct vmcs_config *vmcs_conf,\n\t\t\t\t\tstruct nested_vmx_msrs *msrs)\n{\n\tmsrs->entry_ctls_low =\n\t\tVM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR;\n\n\tmsrs->entry_ctls_high = vmcs_conf->vmentry_ctrl;\n\tmsrs->entry_ctls_high &=\n#ifdef CONFIG_X86_64\n\t\tVM_ENTRY_IA32E_MODE |\n#endif\n\t\tVM_ENTRY_LOAD_IA32_PAT | VM_ENTRY_LOAD_BNDCFGS;\n\tmsrs->entry_ctls_high |=\n\t\t(VM_ENTRY_ALWAYSON_WITHOUT_TRUE_MSR | VM_ENTRY_LOAD_IA32_EFER |\n\t\t VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL);\n\n\t \n\tmsrs->entry_ctls_low &= ~VM_ENTRY_LOAD_DEBUG_CONTROLS;\n}\n\nstatic void nested_vmx_setup_cpubased_ctls(struct vmcs_config *vmcs_conf,\n\t\t\t\t\t   struct nested_vmx_msrs *msrs)\n{\n\tmsrs->procbased_ctls_low =\n\t\tCPU_BASED_ALWAYSON_WITHOUT_TRUE_MSR;\n\n\tmsrs->procbased_ctls_high = vmcs_conf->cpu_based_exec_ctrl;\n\tmsrs->procbased_ctls_high &=\n\t\tCPU_BASED_INTR_WINDOW_EXITING |\n\t\tCPU_BASED_NMI_WINDOW_EXITING | CPU_BASED_USE_TSC_OFFSETTING |\n\t\tCPU_BASED_HLT_EXITING | CPU_BASED_INVLPG_EXITING |\n\t\tCPU_BASED_MWAIT_EXITING | CPU_BASED_CR3_LOAD_EXITING |\n\t\tCPU_BASED_CR3_STORE_EXITING |\n#ifdef CONFIG_X86_64\n\t\tCPU_BASED_CR8_LOAD_EXITING | CPU_BASED_CR8_STORE_EXITING |\n#endif\n\t\tCPU_BASED_MOV_DR_EXITING | CPU_BASED_UNCOND_IO_EXITING |\n\t\tCPU_BASED_USE_IO_BITMAPS | CPU_BASED_MONITOR_TRAP_FLAG |\n\t\tCPU_BASED_MONITOR_EXITING | CPU_BASED_RDPMC_EXITING |\n\t\tCPU_BASED_RDTSC_EXITING | CPU_BASED_PAUSE_EXITING |\n\t\tCPU_BASED_TPR_SHADOW | CPU_BASED_ACTIVATE_SECONDARY_CONTROLS;\n\t \n\tmsrs->procbased_ctls_high |=\n\t\tCPU_BASED_ALWAYSON_WITHOUT_TRUE_MSR |\n\t\tCPU_BASED_USE_MSR_BITMAPS;\n\n\t \n\tmsrs->procbased_ctls_low &=\n\t\t~(CPU_BASED_CR3_LOAD_EXITING | CPU_BASED_CR3_STORE_EXITING);\n}\n\nstatic void nested_vmx_setup_secondary_ctls(u32 ept_caps,\n\t\t\t\t\t    struct vmcs_config *vmcs_conf,\n\t\t\t\t\t    struct nested_vmx_msrs *msrs)\n{\n\tmsrs->secondary_ctls_low = 0;\n\n\tmsrs->secondary_ctls_high = vmcs_conf->cpu_based_2nd_exec_ctrl;\n\tmsrs->secondary_ctls_high &=\n\t\tSECONDARY_EXEC_DESC |\n\t\tSECONDARY_EXEC_ENABLE_RDTSCP |\n\t\tSECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE |\n\t\tSECONDARY_EXEC_WBINVD_EXITING |\n\t\tSECONDARY_EXEC_APIC_REGISTER_VIRT |\n\t\tSECONDARY_EXEC_VIRTUAL_INTR_DELIVERY |\n\t\tSECONDARY_EXEC_RDRAND_EXITING |\n\t\tSECONDARY_EXEC_ENABLE_INVPCID |\n\t\tSECONDARY_EXEC_ENABLE_VMFUNC |\n\t\tSECONDARY_EXEC_RDSEED_EXITING |\n\t\tSECONDARY_EXEC_ENABLE_XSAVES |\n\t\tSECONDARY_EXEC_TSC_SCALING |\n\t\tSECONDARY_EXEC_ENABLE_USR_WAIT_PAUSE;\n\n\t \n\tmsrs->secondary_ctls_high |=\n\t\tSECONDARY_EXEC_SHADOW_VMCS;\n\n\tif (enable_ept) {\n\t\t \n\t\tmsrs->secondary_ctls_high |=\n\t\t\tSECONDARY_EXEC_ENABLE_EPT;\n\t\tmsrs->ept_caps =\n\t\t\tVMX_EPT_PAGE_WALK_4_BIT |\n\t\t\tVMX_EPT_PAGE_WALK_5_BIT |\n\t\t\tVMX_EPTP_WB_BIT |\n\t\t\tVMX_EPT_INVEPT_BIT |\n\t\t\tVMX_EPT_EXECUTE_ONLY_BIT;\n\n\t\tmsrs->ept_caps &= ept_caps;\n\t\tmsrs->ept_caps |= VMX_EPT_EXTENT_GLOBAL_BIT |\n\t\t\tVMX_EPT_EXTENT_CONTEXT_BIT | VMX_EPT_2MB_PAGE_BIT |\n\t\t\tVMX_EPT_1GB_PAGE_BIT;\n\t\tif (enable_ept_ad_bits) {\n\t\t\tmsrs->secondary_ctls_high |=\n\t\t\t\tSECONDARY_EXEC_ENABLE_PML;\n\t\t\tmsrs->ept_caps |= VMX_EPT_AD_BIT;\n\t\t}\n\n\t\t \n\t\tif (cpu_has_vmx_vmfunc())\n\t\t\tmsrs->vmfunc_controls = VMX_VMFUNC_EPTP_SWITCHING;\n\t}\n\n\t \n\tif (enable_vpid) {\n\t\tmsrs->secondary_ctls_high |=\n\t\t\tSECONDARY_EXEC_ENABLE_VPID;\n\t\tmsrs->vpid_caps = VMX_VPID_INVVPID_BIT |\n\t\t\tVMX_VPID_EXTENT_SUPPORTED_MASK;\n\t}\n\n\tif (enable_unrestricted_guest)\n\t\tmsrs->secondary_ctls_high |=\n\t\t\tSECONDARY_EXEC_UNRESTRICTED_GUEST;\n\n\tif (flexpriority_enabled)\n\t\tmsrs->secondary_ctls_high |=\n\t\t\tSECONDARY_EXEC_VIRTUALIZE_APIC_ACCESSES;\n\n\tif (enable_sgx)\n\t\tmsrs->secondary_ctls_high |= SECONDARY_EXEC_ENCLS_EXITING;\n}\n\nstatic void nested_vmx_setup_misc_data(struct vmcs_config *vmcs_conf,\n\t\t\t\t       struct nested_vmx_msrs *msrs)\n{\n\tmsrs->misc_low = (u32)vmcs_conf->misc & VMX_MISC_SAVE_EFER_LMA;\n\tmsrs->misc_low |=\n\t\tMSR_IA32_VMX_MISC_VMWRITE_SHADOW_RO_FIELDS |\n\t\tVMX_MISC_EMULATED_PREEMPTION_TIMER_RATE |\n\t\tVMX_MISC_ACTIVITY_HLT |\n\t\tVMX_MISC_ACTIVITY_WAIT_SIPI;\n\tmsrs->misc_high = 0;\n}\n\nstatic void nested_vmx_setup_basic(struct nested_vmx_msrs *msrs)\n{\n\t \n\tmsrs->basic =\n\t\tVMCS12_REVISION |\n\t\tVMX_BASIC_TRUE_CTLS |\n\t\t((u64)VMCS12_SIZE << VMX_BASIC_VMCS_SIZE_SHIFT) |\n\t\t(VMX_BASIC_MEM_TYPE_WB << VMX_BASIC_MEM_TYPE_SHIFT);\n\n\tif (cpu_has_vmx_basic_inout())\n\t\tmsrs->basic |= VMX_BASIC_INOUT;\n}\n\nstatic void nested_vmx_setup_cr_fixed(struct nested_vmx_msrs *msrs)\n{\n\t \n#define VMXON_CR0_ALWAYSON     (X86_CR0_PE | X86_CR0_PG | X86_CR0_NE)\n#define VMXON_CR4_ALWAYSON     X86_CR4_VMXE\n\tmsrs->cr0_fixed0 = VMXON_CR0_ALWAYSON;\n\tmsrs->cr4_fixed0 = VMXON_CR4_ALWAYSON;\n\n\t \n\trdmsrl(MSR_IA32_VMX_CR0_FIXED1, msrs->cr0_fixed1);\n\trdmsrl(MSR_IA32_VMX_CR4_FIXED1, msrs->cr4_fixed1);\n\n\tif (vmx_umip_emulated())\n\t\tmsrs->cr4_fixed1 |= X86_CR4_UMIP;\n}\n\n \nvoid nested_vmx_setup_ctls_msrs(struct vmcs_config *vmcs_conf, u32 ept_caps)\n{\n\tstruct nested_vmx_msrs *msrs = &vmcs_conf->nested;\n\n\t \n\tnested_vmx_setup_pinbased_ctls(vmcs_conf, msrs);\n\n\tnested_vmx_setup_exit_ctls(vmcs_conf, msrs);\n\n\tnested_vmx_setup_entry_ctls(vmcs_conf, msrs);\n\n\tnested_vmx_setup_cpubased_ctls(vmcs_conf, msrs);\n\n\tnested_vmx_setup_secondary_ctls(ept_caps, vmcs_conf, msrs);\n\n\tnested_vmx_setup_misc_data(vmcs_conf, msrs);\n\n\tnested_vmx_setup_basic(msrs);\n\n\tnested_vmx_setup_cr_fixed(msrs);\n\n\tmsrs->vmcs_enum = nested_vmx_calc_vmcs_enum_msr();\n}\n\nvoid nested_vmx_hardware_unsetup(void)\n{\n\tint i;\n\n\tif (enable_shadow_vmcs) {\n\t\tfor (i = 0; i < VMX_BITMAP_NR; i++)\n\t\t\tfree_page((unsigned long)vmx_bitmap[i]);\n\t}\n}\n\n__init int nested_vmx_hardware_setup(int (*exit_handlers[])(struct kvm_vcpu *))\n{\n\tint i;\n\n\tif (!cpu_has_vmx_shadow_vmcs())\n\t\tenable_shadow_vmcs = 0;\n\tif (enable_shadow_vmcs) {\n\t\tfor (i = 0; i < VMX_BITMAP_NR; i++) {\n\t\t\t \n\t\t\tvmx_bitmap[i] = (unsigned long *)\n\t\t\t\t__get_free_page(GFP_KERNEL);\n\t\t\tif (!vmx_bitmap[i]) {\n\t\t\t\tnested_vmx_hardware_unsetup();\n\t\t\t\treturn -ENOMEM;\n\t\t\t}\n\t\t}\n\n\t\tinit_vmcs_shadow_fields();\n\t}\n\n\texit_handlers[EXIT_REASON_VMCLEAR]\t= handle_vmclear;\n\texit_handlers[EXIT_REASON_VMLAUNCH]\t= handle_vmlaunch;\n\texit_handlers[EXIT_REASON_VMPTRLD]\t= handle_vmptrld;\n\texit_handlers[EXIT_REASON_VMPTRST]\t= handle_vmptrst;\n\texit_handlers[EXIT_REASON_VMREAD]\t= handle_vmread;\n\texit_handlers[EXIT_REASON_VMRESUME]\t= handle_vmresume;\n\texit_handlers[EXIT_REASON_VMWRITE]\t= handle_vmwrite;\n\texit_handlers[EXIT_REASON_VMOFF]\t= handle_vmxoff;\n\texit_handlers[EXIT_REASON_VMON]\t\t= handle_vmxon;\n\texit_handlers[EXIT_REASON_INVEPT]\t= handle_invept;\n\texit_handlers[EXIT_REASON_INVVPID]\t= handle_invvpid;\n\texit_handlers[EXIT_REASON_VMFUNC]\t= handle_vmfunc;\n\n\treturn 0;\n}\n\nstruct kvm_x86_nested_ops vmx_nested_ops = {\n\t.leave_nested = vmx_leave_nested,\n\t.is_exception_vmexit = nested_vmx_is_exception_vmexit,\n\t.check_events = vmx_check_nested_events,\n\t.has_events = vmx_has_nested_events,\n\t.triple_fault = nested_vmx_triple_fault,\n\t.get_state = vmx_get_nested_state,\n\t.set_state = vmx_set_nested_state,\n\t.get_nested_state_pages = vmx_get_nested_state_pages,\n\t.write_log_dirty = nested_vmx_write_pml_buffer,\n\t.enable_evmcs = nested_enable_evmcs,\n\t.get_evmcs_version = nested_get_evmcs_version,\n\t.hv_inject_synthetic_vmexit_post_tlb_flush = vmx_hv_inject_synthetic_vmexit_post_tlb_flush,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}