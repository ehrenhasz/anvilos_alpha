{
  "module_name": "xen.c",
  "hash_id": "49abec0706ce60b50a76a15e5d2e78b705939ee35ea74da4c8947b9abd16e31a",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/xen.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include \"x86.h\"\n#include \"xen.h\"\n#include \"hyperv.h\"\n#include \"lapic.h\"\n\n#include <linux/eventfd.h>\n#include <linux/kvm_host.h>\n#include <linux/sched/stat.h>\n\n#include <trace/events/kvm.h>\n#include <xen/interface/xen.h>\n#include <xen/interface/vcpu.h>\n#include <xen/interface/version.h>\n#include <xen/interface/event_channel.h>\n#include <xen/interface/sched.h>\n\n#include <asm/xen/cpuid.h>\n\n#include \"cpuid.h\"\n#include \"trace.h\"\n\nstatic int kvm_xen_set_evtchn(struct kvm_xen_evtchn *xe, struct kvm *kvm);\nstatic int kvm_xen_setattr_evtchn(struct kvm *kvm, struct kvm_xen_hvm_attr *data);\nstatic bool kvm_xen_hcall_evtchn_send(struct kvm_vcpu *vcpu, u64 param, u64 *r);\n\nDEFINE_STATIC_KEY_DEFERRED_FALSE(kvm_xen_enabled, HZ);\n\nstatic int kvm_xen_shared_info_init(struct kvm *kvm, gfn_t gfn)\n{\n\tstruct gfn_to_pfn_cache *gpc = &kvm->arch.xen.shinfo_cache;\n\tstruct pvclock_wall_clock *wc;\n\tgpa_t gpa = gfn_to_gpa(gfn);\n\tu32 *wc_sec_hi;\n\tu32 wc_version;\n\tu64 wall_nsec;\n\tint ret = 0;\n\tint idx = srcu_read_lock(&kvm->srcu);\n\n\tif (gfn == KVM_XEN_INVALID_GFN) {\n\t\tkvm_gpc_deactivate(gpc);\n\t\tgoto out;\n\t}\n\n\tdo {\n\t\tret = kvm_gpc_activate(gpc, gpa, PAGE_SIZE);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\t \n\t\twall_nsec = ktime_get_real_ns() - get_kvmclock_ns(kvm);\n\n\t\t \n\t\tread_lock_irq(&gpc->lock);\n\n\t\tif (gpc->valid)\n\t\t\tbreak;\n\n\t\tread_unlock_irq(&gpc->lock);\n\t} while (1);\n\n\t \n\tBUILD_BUG_ON(offsetof(struct compat_shared_info, wc) != 0x900);\n\tBUILD_BUG_ON(offsetof(struct compat_shared_info, arch.wc_sec_hi) != 0x924);\n\tBUILD_BUG_ON(offsetof(struct pvclock_vcpu_time_info, version) != 0);\n\n#ifdef CONFIG_X86_64\n\t \n\tBUILD_BUG_ON(offsetof(struct shared_info, wc) != 0xc00);\n\tBUILD_BUG_ON(offsetof(struct shared_info, wc_sec_hi) != 0xc0c);\n\n\tif (IS_ENABLED(CONFIG_64BIT) && kvm->arch.xen.long_mode) {\n\t\tstruct shared_info *shinfo = gpc->khva;\n\n\t\twc_sec_hi = &shinfo->wc_sec_hi;\n\t\twc = &shinfo->wc;\n\t} else\n#endif\n\t{\n\t\tstruct compat_shared_info *shinfo = gpc->khva;\n\n\t\twc_sec_hi = &shinfo->arch.wc_sec_hi;\n\t\twc = &shinfo->wc;\n\t}\n\n\t \n\twc_version = wc->version = (wc->version + 1) | 1;\n\tsmp_wmb();\n\n\twc->nsec = do_div(wall_nsec,  1000000000);\n\twc->sec = (u32)wall_nsec;\n\t*wc_sec_hi = wall_nsec >> 32;\n\tsmp_wmb();\n\n\twc->version = wc_version + 1;\n\tread_unlock_irq(&gpc->lock);\n\n\tkvm_make_all_cpus_request(kvm, KVM_REQ_MASTERCLOCK_UPDATE);\n\nout:\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\treturn ret;\n}\n\nvoid kvm_xen_inject_timer_irqs(struct kvm_vcpu *vcpu)\n{\n\tif (atomic_read(&vcpu->arch.xen.timer_pending) > 0) {\n\t\tstruct kvm_xen_evtchn e;\n\n\t\te.vcpu_id = vcpu->vcpu_id;\n\t\te.vcpu_idx = vcpu->vcpu_idx;\n\t\te.port = vcpu->arch.xen.timer_virq;\n\t\te.priority = KVM_IRQ_ROUTING_XEN_EVTCHN_PRIO_2LEVEL;\n\n\t\tkvm_xen_set_evtchn(&e, vcpu->kvm);\n\n\t\tvcpu->arch.xen.timer_expires = 0;\n\t\tatomic_set(&vcpu->arch.xen.timer_pending, 0);\n\t}\n}\n\nstatic enum hrtimer_restart xen_timer_callback(struct hrtimer *timer)\n{\n\tstruct kvm_vcpu *vcpu = container_of(timer, struct kvm_vcpu,\n\t\t\t\t\t     arch.xen.timer);\n\tif (atomic_read(&vcpu->arch.xen.timer_pending))\n\t\treturn HRTIMER_NORESTART;\n\n\tatomic_inc(&vcpu->arch.xen.timer_pending);\n\tkvm_make_request(KVM_REQ_UNBLOCK, vcpu);\n\tkvm_vcpu_kick(vcpu);\n\n\treturn HRTIMER_NORESTART;\n}\n\nstatic void kvm_xen_start_timer(struct kvm_vcpu *vcpu, u64 guest_abs, s64 delta_ns)\n{\n\tatomic_set(&vcpu->arch.xen.timer_pending, 0);\n\tvcpu->arch.xen.timer_expires = guest_abs;\n\n\tif (delta_ns <= 0) {\n\t\txen_timer_callback(&vcpu->arch.xen.timer);\n\t} else {\n\t\tktime_t ktime_now = ktime_get();\n\t\thrtimer_start(&vcpu->arch.xen.timer,\n\t\t\t      ktime_add_ns(ktime_now, delta_ns),\n\t\t\t      HRTIMER_MODE_ABS_HARD);\n\t}\n}\n\nstatic void kvm_xen_stop_timer(struct kvm_vcpu *vcpu)\n{\n\thrtimer_cancel(&vcpu->arch.xen.timer);\n\tvcpu->arch.xen.timer_expires = 0;\n\tatomic_set(&vcpu->arch.xen.timer_pending, 0);\n}\n\nstatic void kvm_xen_init_timer(struct kvm_vcpu *vcpu)\n{\n\thrtimer_init(&vcpu->arch.xen.timer, CLOCK_MONOTONIC,\n\t\t     HRTIMER_MODE_ABS_HARD);\n\tvcpu->arch.xen.timer.function = xen_timer_callback;\n}\n\nstatic void kvm_xen_update_runstate_guest(struct kvm_vcpu *v, bool atomic)\n{\n\tstruct kvm_vcpu_xen *vx = &v->arch.xen;\n\tstruct gfn_to_pfn_cache *gpc1 = &vx->runstate_cache;\n\tstruct gfn_to_pfn_cache *gpc2 = &vx->runstate2_cache;\n\tsize_t user_len, user_len1, user_len2;\n\tstruct vcpu_runstate_info rs;\n\tunsigned long flags;\n\tsize_t times_ofs;\n\tuint8_t *update_bit = NULL;\n\tuint64_t entry_time;\n\tuint64_t *rs_times;\n\tint *rs_state;\n\n\t \n\tBUILD_BUG_ON(offsetof(struct vcpu_runstate_info, state) != 0);\n\tBUILD_BUG_ON(offsetof(struct compat_vcpu_runstate_info, state) != 0);\n\tBUILD_BUG_ON(sizeof(struct compat_vcpu_runstate_info) != 0x2c);\n#ifdef CONFIG_X86_64\n\t \n\tBUILD_BUG_ON(offsetof(struct vcpu_runstate_info, state_entry_time) !=\n\t\t     offsetof(struct compat_vcpu_runstate_info, state_entry_time) + 4);\n\tBUILD_BUG_ON(offsetof(struct vcpu_runstate_info, time) !=\n\t\t     offsetof(struct compat_vcpu_runstate_info, time) + 4);\n\tBUILD_BUG_ON(sizeof(struct vcpu_runstate_info) != 0x2c + 4);\n#endif\n\t \n\tBUILD_BUG_ON(offsetof(struct vcpu_runstate_info, state) !=\n\t\t     offsetof(struct compat_vcpu_runstate_info, state));\n\tBUILD_BUG_ON(sizeof_field(struct vcpu_runstate_info, state) !=\n\t\t     sizeof(vx->current_runstate));\n\tBUILD_BUG_ON(sizeof_field(struct compat_vcpu_runstate_info, state) !=\n\t\t     sizeof(vx->current_runstate));\n\n\t \n\tBUILD_BUG_ON(sizeof_field(struct vcpu_runstate_info, state_entry_time) !=\n\t\t     sizeof(uint64_t));\n\tBUILD_BUG_ON(sizeof_field(struct compat_vcpu_runstate_info, state_entry_time) !=\n\t\t     sizeof(uint64_t));\n\tBUILD_BUG_ON((XEN_RUNSTATE_UPDATE >> 56) != 0x80);\n\n\t \n\tBUILD_BUG_ON(offsetof(struct vcpu_runstate_info, state_entry_time) !=\n\t\t     offsetof(struct vcpu_runstate_info, time) - sizeof(uint64_t));\n\tBUILD_BUG_ON(offsetof(struct compat_vcpu_runstate_info, state_entry_time) !=\n\t\t     offsetof(struct compat_vcpu_runstate_info, time) - sizeof(uint64_t));\n\tBUILD_BUG_ON(sizeof_field(struct vcpu_runstate_info, time) !=\n\t\t     sizeof_field(struct compat_vcpu_runstate_info, time));\n\tBUILD_BUG_ON(sizeof_field(struct vcpu_runstate_info, time) !=\n\t\t     sizeof(vx->runstate_times));\n\n\tif (IS_ENABLED(CONFIG_64BIT) && v->kvm->arch.xen.long_mode) {\n\t\tuser_len = sizeof(struct vcpu_runstate_info);\n\t\ttimes_ofs = offsetof(struct vcpu_runstate_info,\n\t\t\t\t     state_entry_time);\n\t} else {\n\t\tuser_len = sizeof(struct compat_vcpu_runstate_info);\n\t\ttimes_ofs = offsetof(struct compat_vcpu_runstate_info,\n\t\t\t\t     state_entry_time);\n\t}\n\n\t \n\tif ((gpc1->gpa & ~PAGE_MASK) + user_len >= PAGE_SIZE) {\n\t\tuser_len1 = PAGE_SIZE - (gpc1->gpa & ~PAGE_MASK);\n\t\tuser_len2 = user_len - user_len1;\n\t} else {\n\t\tuser_len1 = user_len;\n\t\tuser_len2 = 0;\n\t}\n\tBUG_ON(user_len1 + user_len2 != user_len);\n\n retry:\n\t \n\tif (atomic) {\n\t\tlocal_irq_save(flags);\n\t\tif (!read_trylock(&gpc1->lock)) {\n\t\t\tlocal_irq_restore(flags);\n\t\t\treturn;\n\t\t}\n\t} else {\n\t\tread_lock_irqsave(&gpc1->lock, flags);\n\t}\n\twhile (!kvm_gpc_check(gpc1, user_len1)) {\n\t\tread_unlock_irqrestore(&gpc1->lock, flags);\n\n\t\t \n\t\tif (atomic)\n\t\t\treturn;\n\n\t\tif (kvm_gpc_refresh(gpc1, user_len1))\n\t\t\treturn;\n\n\t\tread_lock_irqsave(&gpc1->lock, flags);\n\t}\n\n\tif (likely(!user_len2)) {\n\t\t \n\t\trs_state = gpc1->khva;\n\t\trs_times = gpc1->khva + times_ofs;\n\t\tif (v->kvm->arch.xen.runstate_update_flag)\n\t\t\tupdate_bit = ((void *)(&rs_times[1])) - 1;\n\t} else {\n\t\t \n\t\tlock_set_subclass(&gpc1->lock.dep_map, 1, _THIS_IP_);\n\t\tif (atomic) {\n\t\t\tif (!read_trylock(&gpc2->lock)) {\n\t\t\t\tread_unlock_irqrestore(&gpc1->lock, flags);\n\t\t\t\treturn;\n\t\t\t}\n\t\t} else {\n\t\t\tread_lock(&gpc2->lock);\n\t\t}\n\n\t\tif (!kvm_gpc_check(gpc2, user_len2)) {\n\t\t\tread_unlock(&gpc2->lock);\n\t\t\tread_unlock_irqrestore(&gpc1->lock, flags);\n\n\t\t\t \n\t\t\tif (atomic)\n\t\t\t\treturn;\n\n\t\t\t \n\t\t\tif (kvm_gpc_activate(gpc2, gpc1->gpa + user_len1,\n\t\t\t\t\t     user_len2))\n\t\t\t\treturn;\n\n\t\t\t \n\t\t\tgoto retry;\n\t\t}\n\n\t\t \n\t\trs_times = &rs.state_entry_time;\n\n\t\t \n\t\trs_state = ((void *)rs_times) - times_ofs;\n\n\t\t \n\t\tif (v->kvm->arch.xen.runstate_update_flag) {\n\t\t\tif (user_len1 >= times_ofs + sizeof(uint64_t))\n\t\t\t\tupdate_bit = gpc1->khva + times_ofs +\n\t\t\t\t\tsizeof(uint64_t) - 1;\n\t\t\telse\n\t\t\t\tupdate_bit = gpc2->khva + times_ofs +\n\t\t\t\t\tsizeof(uint64_t) - 1 - user_len1;\n\t\t}\n\n#ifdef CONFIG_X86_64\n\t\t \n\t\tmemset(&rs, 0, offsetof(struct vcpu_runstate_info, state_entry_time));\n#endif\n\t}\n\n\t \n\tentry_time = vx->runstate_entry_time;\n\tif (update_bit) {\n\t\tentry_time |= XEN_RUNSTATE_UPDATE;\n\t\t*update_bit = (vx->runstate_entry_time | XEN_RUNSTATE_UPDATE) >> 56;\n\t\tsmp_wmb();\n\t}\n\n\t \n\t*rs_state = vx->current_runstate;\n\trs_times[0] = entry_time;\n\tmemcpy(rs_times + 1, vx->runstate_times, sizeof(vx->runstate_times));\n\n\t \n\tif (user_len2) {\n\t\tmemcpy(gpc1->khva, rs_state, user_len1);\n\t\tmemcpy(gpc2->khva, ((void *)rs_state) + user_len1, user_len2);\n\t}\n\tsmp_wmb();\n\n\t \n\tif (update_bit) {\n\t\tentry_time &= ~XEN_RUNSTATE_UPDATE;\n\t\t*update_bit = entry_time >> 56;\n\t\tsmp_wmb();\n\t}\n\n\tif (user_len2)\n\t\tread_unlock(&gpc2->lock);\n\n\tread_unlock_irqrestore(&gpc1->lock, flags);\n\n\tmark_page_dirty_in_slot(v->kvm, gpc1->memslot, gpc1->gpa >> PAGE_SHIFT);\n\tif (user_len2)\n\t\tmark_page_dirty_in_slot(v->kvm, gpc2->memslot, gpc2->gpa >> PAGE_SHIFT);\n}\n\nvoid kvm_xen_update_runstate(struct kvm_vcpu *v, int state)\n{\n\tstruct kvm_vcpu_xen *vx = &v->arch.xen;\n\tu64 now = get_kvmclock_ns(v->kvm);\n\tu64 delta_ns = now - vx->runstate_entry_time;\n\tu64 run_delay = current->sched_info.run_delay;\n\n\tif (unlikely(!vx->runstate_entry_time))\n\t\tvx->current_runstate = RUNSTATE_offline;\n\n\t \n\tif (vx->current_runstate == RUNSTATE_running) {\n\t\tu64 steal_ns = run_delay - vx->last_steal;\n\n\t\tdelta_ns -= steal_ns;\n\n\t\tvx->runstate_times[RUNSTATE_runnable] += steal_ns;\n\t}\n\tvx->last_steal = run_delay;\n\n\tvx->runstate_times[vx->current_runstate] += delta_ns;\n\tvx->current_runstate = state;\n\tvx->runstate_entry_time = now;\n\n\tif (vx->runstate_cache.active)\n\t\tkvm_xen_update_runstate_guest(v, state == RUNSTATE_runnable);\n}\n\nstatic void kvm_xen_inject_vcpu_vector(struct kvm_vcpu *v)\n{\n\tstruct kvm_lapic_irq irq = { };\n\tint r;\n\n\tirq.dest_id = v->vcpu_id;\n\tirq.vector = v->arch.xen.upcall_vector;\n\tirq.dest_mode = APIC_DEST_PHYSICAL;\n\tirq.shorthand = APIC_DEST_NOSHORT;\n\tirq.delivery_mode = APIC_DM_FIXED;\n\tirq.level = 1;\n\n\t \n\tWARN_ON_ONCE(!kvm_irq_delivery_to_apic_fast(v->kvm, NULL, &irq, &r, NULL));\n}\n\n \nvoid kvm_xen_inject_pending_events(struct kvm_vcpu *v)\n{\n\tunsigned long evtchn_pending_sel = READ_ONCE(v->arch.xen.evtchn_pending_sel);\n\tstruct gfn_to_pfn_cache *gpc = &v->arch.xen.vcpu_info_cache;\n\tunsigned long flags;\n\n\tif (!evtchn_pending_sel)\n\t\treturn;\n\n\t \n\tread_lock_irqsave(&gpc->lock, flags);\n\twhile (!kvm_gpc_check(gpc, sizeof(struct vcpu_info))) {\n\t\tread_unlock_irqrestore(&gpc->lock, flags);\n\n\t\tif (kvm_gpc_refresh(gpc, sizeof(struct vcpu_info)))\n\t\t\treturn;\n\n\t\tread_lock_irqsave(&gpc->lock, flags);\n\t}\n\n\t \n\tif (IS_ENABLED(CONFIG_64BIT) && v->kvm->arch.xen.long_mode) {\n\t\tstruct vcpu_info *vi = gpc->khva;\n\n\t\tasm volatile(LOCK_PREFIX \"orq %0, %1\\n\"\n\t\t\t     \"notq %0\\n\"\n\t\t\t     LOCK_PREFIX \"andq %0, %2\\n\"\n\t\t\t     : \"=r\" (evtchn_pending_sel),\n\t\t\t       \"+m\" (vi->evtchn_pending_sel),\n\t\t\t       \"+m\" (v->arch.xen.evtchn_pending_sel)\n\t\t\t     : \"0\" (evtchn_pending_sel));\n\t\tWRITE_ONCE(vi->evtchn_upcall_pending, 1);\n\t} else {\n\t\tu32 evtchn_pending_sel32 = evtchn_pending_sel;\n\t\tstruct compat_vcpu_info *vi = gpc->khva;\n\n\t\tasm volatile(LOCK_PREFIX \"orl %0, %1\\n\"\n\t\t\t     \"notl %0\\n\"\n\t\t\t     LOCK_PREFIX \"andl %0, %2\\n\"\n\t\t\t     : \"=r\" (evtchn_pending_sel32),\n\t\t\t       \"+m\" (vi->evtchn_pending_sel),\n\t\t\t       \"+m\" (v->arch.xen.evtchn_pending_sel)\n\t\t\t     : \"0\" (evtchn_pending_sel32));\n\t\tWRITE_ONCE(vi->evtchn_upcall_pending, 1);\n\t}\n\tread_unlock_irqrestore(&gpc->lock, flags);\n\n\t \n\tif (v->arch.xen.upcall_vector)\n\t\tkvm_xen_inject_vcpu_vector(v);\n\n\tmark_page_dirty_in_slot(v->kvm, gpc->memslot, gpc->gpa >> PAGE_SHIFT);\n}\n\nint __kvm_xen_has_interrupt(struct kvm_vcpu *v)\n{\n\tstruct gfn_to_pfn_cache *gpc = &v->arch.xen.vcpu_info_cache;\n\tunsigned long flags;\n\tu8 rc = 0;\n\n\t \n\n\t \n\tBUILD_BUG_ON(offsetof(struct vcpu_info, evtchn_upcall_pending) !=\n\t\t     offsetof(struct compat_vcpu_info, evtchn_upcall_pending));\n\tBUILD_BUG_ON(sizeof(rc) !=\n\t\t     sizeof_field(struct vcpu_info, evtchn_upcall_pending));\n\tBUILD_BUG_ON(sizeof(rc) !=\n\t\t     sizeof_field(struct compat_vcpu_info, evtchn_upcall_pending));\n\n\tread_lock_irqsave(&gpc->lock, flags);\n\twhile (!kvm_gpc_check(gpc, sizeof(struct vcpu_info))) {\n\t\tread_unlock_irqrestore(&gpc->lock, flags);\n\n\t\t \n\t\tif (in_atomic() || !task_is_running(current))\n\t\t\treturn 1;\n\n\t\tif (kvm_gpc_refresh(gpc, sizeof(struct vcpu_info))) {\n\t\t\t \n\t\t\treturn 0;\n\t\t}\n\t\tread_lock_irqsave(&gpc->lock, flags);\n\t}\n\n\trc = ((struct vcpu_info *)gpc->khva)->evtchn_upcall_pending;\n\tread_unlock_irqrestore(&gpc->lock, flags);\n\treturn rc;\n}\n\nint kvm_xen_hvm_set_attr(struct kvm *kvm, struct kvm_xen_hvm_attr *data)\n{\n\tint r = -ENOENT;\n\n\n\tswitch (data->type) {\n\tcase KVM_XEN_ATTR_TYPE_LONG_MODE:\n\t\tif (!IS_ENABLED(CONFIG_64BIT) && data->u.long_mode) {\n\t\t\tr = -EINVAL;\n\t\t} else {\n\t\t\tmutex_lock(&kvm->arch.xen.xen_lock);\n\t\t\tkvm->arch.xen.long_mode = !!data->u.long_mode;\n\t\t\tmutex_unlock(&kvm->arch.xen.xen_lock);\n\t\t\tr = 0;\n\t\t}\n\t\tbreak;\n\n\tcase KVM_XEN_ATTR_TYPE_SHARED_INFO:\n\t\tmutex_lock(&kvm->arch.xen.xen_lock);\n\t\tr = kvm_xen_shared_info_init(kvm, data->u.shared_info.gfn);\n\t\tmutex_unlock(&kvm->arch.xen.xen_lock);\n\t\tbreak;\n\n\tcase KVM_XEN_ATTR_TYPE_UPCALL_VECTOR:\n\t\tif (data->u.vector && data->u.vector < 0x10)\n\t\t\tr = -EINVAL;\n\t\telse {\n\t\t\tmutex_lock(&kvm->arch.xen.xen_lock);\n\t\t\tkvm->arch.xen.upcall_vector = data->u.vector;\n\t\t\tmutex_unlock(&kvm->arch.xen.xen_lock);\n\t\t\tr = 0;\n\t\t}\n\t\tbreak;\n\n\tcase KVM_XEN_ATTR_TYPE_EVTCHN:\n\t\tr = kvm_xen_setattr_evtchn(kvm, data);\n\t\tbreak;\n\n\tcase KVM_XEN_ATTR_TYPE_XEN_VERSION:\n\t\tmutex_lock(&kvm->arch.xen.xen_lock);\n\t\tkvm->arch.xen.xen_version = data->u.xen_version;\n\t\tmutex_unlock(&kvm->arch.xen.xen_lock);\n\t\tr = 0;\n\t\tbreak;\n\n\tcase KVM_XEN_ATTR_TYPE_RUNSTATE_UPDATE_FLAG:\n\t\tif (!sched_info_on()) {\n\t\t\tr = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tmutex_lock(&kvm->arch.xen.xen_lock);\n\t\tkvm->arch.xen.runstate_update_flag = !!data->u.runstate_update_flag;\n\t\tmutex_unlock(&kvm->arch.xen.xen_lock);\n\t\tr = 0;\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn r;\n}\n\nint kvm_xen_hvm_get_attr(struct kvm *kvm, struct kvm_xen_hvm_attr *data)\n{\n\tint r = -ENOENT;\n\n\tmutex_lock(&kvm->arch.xen.xen_lock);\n\n\tswitch (data->type) {\n\tcase KVM_XEN_ATTR_TYPE_LONG_MODE:\n\t\tdata->u.long_mode = kvm->arch.xen.long_mode;\n\t\tr = 0;\n\t\tbreak;\n\n\tcase KVM_XEN_ATTR_TYPE_SHARED_INFO:\n\t\tif (kvm->arch.xen.shinfo_cache.active)\n\t\t\tdata->u.shared_info.gfn = gpa_to_gfn(kvm->arch.xen.shinfo_cache.gpa);\n\t\telse\n\t\t\tdata->u.shared_info.gfn = KVM_XEN_INVALID_GFN;\n\t\tr = 0;\n\t\tbreak;\n\n\tcase KVM_XEN_ATTR_TYPE_UPCALL_VECTOR:\n\t\tdata->u.vector = kvm->arch.xen.upcall_vector;\n\t\tr = 0;\n\t\tbreak;\n\n\tcase KVM_XEN_ATTR_TYPE_XEN_VERSION:\n\t\tdata->u.xen_version = kvm->arch.xen.xen_version;\n\t\tr = 0;\n\t\tbreak;\n\n\tcase KVM_XEN_ATTR_TYPE_RUNSTATE_UPDATE_FLAG:\n\t\tif (!sched_info_on()) {\n\t\t\tr = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tdata->u.runstate_update_flag = kvm->arch.xen.runstate_update_flag;\n\t\tr = 0;\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tmutex_unlock(&kvm->arch.xen.xen_lock);\n\treturn r;\n}\n\nint kvm_xen_vcpu_set_attr(struct kvm_vcpu *vcpu, struct kvm_xen_vcpu_attr *data)\n{\n\tint idx, r = -ENOENT;\n\n\tmutex_lock(&vcpu->kvm->arch.xen.xen_lock);\n\tidx = srcu_read_lock(&vcpu->kvm->srcu);\n\n\tswitch (data->type) {\n\tcase KVM_XEN_VCPU_ATTR_TYPE_VCPU_INFO:\n\t\t \n\t\tBUILD_BUG_ON(sizeof(struct vcpu_info) !=\n\t\t\t     sizeof(struct compat_vcpu_info));\n\t\tBUILD_BUG_ON(offsetof(struct vcpu_info, time) !=\n\t\t\t     offsetof(struct compat_vcpu_info, time));\n\n\t\tif (data->u.gpa == KVM_XEN_INVALID_GPA) {\n\t\t\tkvm_gpc_deactivate(&vcpu->arch.xen.vcpu_info_cache);\n\t\t\tr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tr = kvm_gpc_activate(&vcpu->arch.xen.vcpu_info_cache,\n\t\t\t\t     data->u.gpa, sizeof(struct vcpu_info));\n\t\tif (!r)\n\t\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\n\t\tbreak;\n\n\tcase KVM_XEN_VCPU_ATTR_TYPE_VCPU_TIME_INFO:\n\t\tif (data->u.gpa == KVM_XEN_INVALID_GPA) {\n\t\t\tkvm_gpc_deactivate(&vcpu->arch.xen.vcpu_time_info_cache);\n\t\t\tr = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tr = kvm_gpc_activate(&vcpu->arch.xen.vcpu_time_info_cache,\n\t\t\t\t     data->u.gpa,\n\t\t\t\t     sizeof(struct pvclock_vcpu_time_info));\n\t\tif (!r)\n\t\t\tkvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);\n\t\tbreak;\n\n\tcase KVM_XEN_VCPU_ATTR_TYPE_RUNSTATE_ADDR: {\n\t\tsize_t sz, sz1, sz2;\n\n\t\tif (!sched_info_on()) {\n\t\t\tr = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tif (data->u.gpa == KVM_XEN_INVALID_GPA) {\n\t\t\tr = 0;\n\t\tdeactivate_out:\n\t\t\tkvm_gpc_deactivate(&vcpu->arch.xen.runstate_cache);\n\t\t\tkvm_gpc_deactivate(&vcpu->arch.xen.runstate2_cache);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tif (IS_ENABLED(CONFIG_64BIT) && vcpu->kvm->arch.xen.long_mode)\n\t\t\tsz = sizeof(struct vcpu_runstate_info);\n\t\telse\n\t\t\tsz = sizeof(struct compat_vcpu_runstate_info);\n\n\t\t \n\t\tsz1 = PAGE_SIZE - (data->u.gpa & ~PAGE_MASK);\n\t\tr = kvm_gpc_activate(&vcpu->arch.xen.runstate_cache,\n\t\t\t\t     data->u.gpa, sz1);\n\t\tif (r)\n\t\t\tgoto deactivate_out;\n\n\t\t \n\t\tif (sz1 >= sz) {\n\t\t\tkvm_gpc_deactivate(&vcpu->arch.xen.runstate2_cache);\n\t\t} else {\n\t\t\tsz2 = sz - sz1;\n\t\t\tBUG_ON((data->u.gpa + sz1) & ~PAGE_MASK);\n\t\t\tr = kvm_gpc_activate(&vcpu->arch.xen.runstate2_cache,\n\t\t\t\t\t     data->u.gpa + sz1, sz2);\n\t\t\tif (r)\n\t\t\t\tgoto deactivate_out;\n\t\t}\n\n\t\tkvm_xen_update_runstate_guest(vcpu, false);\n\t\tbreak;\n\t}\n\tcase KVM_XEN_VCPU_ATTR_TYPE_RUNSTATE_CURRENT:\n\t\tif (!sched_info_on()) {\n\t\t\tr = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tif (data->u.runstate.state > RUNSTATE_offline) {\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tkvm_xen_update_runstate(vcpu, data->u.runstate.state);\n\t\tr = 0;\n\t\tbreak;\n\n\tcase KVM_XEN_VCPU_ATTR_TYPE_RUNSTATE_DATA:\n\t\tif (!sched_info_on()) {\n\t\t\tr = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tif (data->u.runstate.state > RUNSTATE_offline) {\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (data->u.runstate.state_entry_time !=\n\t\t    (data->u.runstate.time_running +\n\t\t     data->u.runstate.time_runnable +\n\t\t     data->u.runstate.time_blocked +\n\t\t     data->u.runstate.time_offline)) {\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\tif (get_kvmclock_ns(vcpu->kvm) <\n\t\t    data->u.runstate.state_entry_time) {\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tvcpu->arch.xen.current_runstate = data->u.runstate.state;\n\t\tvcpu->arch.xen.runstate_entry_time =\n\t\t\tdata->u.runstate.state_entry_time;\n\t\tvcpu->arch.xen.runstate_times[RUNSTATE_running] =\n\t\t\tdata->u.runstate.time_running;\n\t\tvcpu->arch.xen.runstate_times[RUNSTATE_runnable] =\n\t\t\tdata->u.runstate.time_runnable;\n\t\tvcpu->arch.xen.runstate_times[RUNSTATE_blocked] =\n\t\t\tdata->u.runstate.time_blocked;\n\t\tvcpu->arch.xen.runstate_times[RUNSTATE_offline] =\n\t\t\tdata->u.runstate.time_offline;\n\t\tvcpu->arch.xen.last_steal = current->sched_info.run_delay;\n\t\tr = 0;\n\t\tbreak;\n\n\tcase KVM_XEN_VCPU_ATTR_TYPE_RUNSTATE_ADJUST:\n\t\tif (!sched_info_on()) {\n\t\t\tr = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tif (data->u.runstate.state > RUNSTATE_offline &&\n\t\t    data->u.runstate.state != (u64)-1) {\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\t\t \n\t\tif (data->u.runstate.state_entry_time !=\n\t\t    (data->u.runstate.time_running +\n\t\t     data->u.runstate.time_runnable +\n\t\t     data->u.runstate.time_blocked +\n\t\t     data->u.runstate.time_offline)) {\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (get_kvmclock_ns(vcpu->kvm) <\n\t\t    (vcpu->arch.xen.runstate_entry_time +\n\t\t     data->u.runstate.state_entry_time)) {\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tvcpu->arch.xen.runstate_entry_time +=\n\t\t\tdata->u.runstate.state_entry_time;\n\t\tvcpu->arch.xen.runstate_times[RUNSTATE_running] +=\n\t\t\tdata->u.runstate.time_running;\n\t\tvcpu->arch.xen.runstate_times[RUNSTATE_runnable] +=\n\t\t\tdata->u.runstate.time_runnable;\n\t\tvcpu->arch.xen.runstate_times[RUNSTATE_blocked] +=\n\t\t\tdata->u.runstate.time_blocked;\n\t\tvcpu->arch.xen.runstate_times[RUNSTATE_offline] +=\n\t\t\tdata->u.runstate.time_offline;\n\n\t\tif (data->u.runstate.state <= RUNSTATE_offline)\n\t\t\tkvm_xen_update_runstate(vcpu, data->u.runstate.state);\n\t\telse if (vcpu->arch.xen.runstate_cache.active)\n\t\t\tkvm_xen_update_runstate_guest(vcpu, false);\n\t\tr = 0;\n\t\tbreak;\n\n\tcase KVM_XEN_VCPU_ATTR_TYPE_VCPU_ID:\n\t\tif (data->u.vcpu_id >= KVM_MAX_VCPUS)\n\t\t\tr = -EINVAL;\n\t\telse {\n\t\t\tvcpu->arch.xen.vcpu_id = data->u.vcpu_id;\n\t\t\tr = 0;\n\t\t}\n\t\tbreak;\n\n\tcase KVM_XEN_VCPU_ATTR_TYPE_TIMER:\n\t\tif (data->u.timer.port &&\n\t\t    data->u.timer.priority != KVM_IRQ_ROUTING_XEN_EVTCHN_PRIO_2LEVEL) {\n\t\t\tr = -EINVAL;\n\t\t\tbreak;\n\t\t}\n\n\t\tif (!vcpu->arch.xen.timer.function)\n\t\t\tkvm_xen_init_timer(vcpu);\n\n\t\t \n\t\tkvm_xen_stop_timer(vcpu);\n\t\tvcpu->arch.xen.timer_virq = data->u.timer.port;\n\n\t\t \n\t\tif (data->u.timer.port && data->u.timer.expires_ns)\n\t\t\tkvm_xen_start_timer(vcpu, data->u.timer.expires_ns,\n\t\t\t\t\t    data->u.timer.expires_ns -\n\t\t\t\t\t    get_kvmclock_ns(vcpu->kvm));\n\n\t\tr = 0;\n\t\tbreak;\n\n\tcase KVM_XEN_VCPU_ATTR_TYPE_UPCALL_VECTOR:\n\t\tif (data->u.vector && data->u.vector < 0x10)\n\t\t\tr = -EINVAL;\n\t\telse {\n\t\t\tvcpu->arch.xen.upcall_vector = data->u.vector;\n\t\t\tr = 0;\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tsrcu_read_unlock(&vcpu->kvm->srcu, idx);\n\tmutex_unlock(&vcpu->kvm->arch.xen.xen_lock);\n\treturn r;\n}\n\nint kvm_xen_vcpu_get_attr(struct kvm_vcpu *vcpu, struct kvm_xen_vcpu_attr *data)\n{\n\tint r = -ENOENT;\n\n\tmutex_lock(&vcpu->kvm->arch.xen.xen_lock);\n\n\tswitch (data->type) {\n\tcase KVM_XEN_VCPU_ATTR_TYPE_VCPU_INFO:\n\t\tif (vcpu->arch.xen.vcpu_info_cache.active)\n\t\t\tdata->u.gpa = vcpu->arch.xen.vcpu_info_cache.gpa;\n\t\telse\n\t\t\tdata->u.gpa = KVM_XEN_INVALID_GPA;\n\t\tr = 0;\n\t\tbreak;\n\n\tcase KVM_XEN_VCPU_ATTR_TYPE_VCPU_TIME_INFO:\n\t\tif (vcpu->arch.xen.vcpu_time_info_cache.active)\n\t\t\tdata->u.gpa = vcpu->arch.xen.vcpu_time_info_cache.gpa;\n\t\telse\n\t\t\tdata->u.gpa = KVM_XEN_INVALID_GPA;\n\t\tr = 0;\n\t\tbreak;\n\n\tcase KVM_XEN_VCPU_ATTR_TYPE_RUNSTATE_ADDR:\n\t\tif (!sched_info_on()) {\n\t\t\tr = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tif (vcpu->arch.xen.runstate_cache.active) {\n\t\t\tdata->u.gpa = vcpu->arch.xen.runstate_cache.gpa;\n\t\t\tr = 0;\n\t\t}\n\t\tbreak;\n\n\tcase KVM_XEN_VCPU_ATTR_TYPE_RUNSTATE_CURRENT:\n\t\tif (!sched_info_on()) {\n\t\t\tr = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tdata->u.runstate.state = vcpu->arch.xen.current_runstate;\n\t\tr = 0;\n\t\tbreak;\n\n\tcase KVM_XEN_VCPU_ATTR_TYPE_RUNSTATE_DATA:\n\t\tif (!sched_info_on()) {\n\t\t\tr = -EOPNOTSUPP;\n\t\t\tbreak;\n\t\t}\n\t\tdata->u.runstate.state = vcpu->arch.xen.current_runstate;\n\t\tdata->u.runstate.state_entry_time =\n\t\t\tvcpu->arch.xen.runstate_entry_time;\n\t\tdata->u.runstate.time_running =\n\t\t\tvcpu->arch.xen.runstate_times[RUNSTATE_running];\n\t\tdata->u.runstate.time_runnable =\n\t\t\tvcpu->arch.xen.runstate_times[RUNSTATE_runnable];\n\t\tdata->u.runstate.time_blocked =\n\t\t\tvcpu->arch.xen.runstate_times[RUNSTATE_blocked];\n\t\tdata->u.runstate.time_offline =\n\t\t\tvcpu->arch.xen.runstate_times[RUNSTATE_offline];\n\t\tr = 0;\n\t\tbreak;\n\n\tcase KVM_XEN_VCPU_ATTR_TYPE_RUNSTATE_ADJUST:\n\t\tr = -EINVAL;\n\t\tbreak;\n\n\tcase KVM_XEN_VCPU_ATTR_TYPE_VCPU_ID:\n\t\tdata->u.vcpu_id = vcpu->arch.xen.vcpu_id;\n\t\tr = 0;\n\t\tbreak;\n\n\tcase KVM_XEN_VCPU_ATTR_TYPE_TIMER:\n\t\tdata->u.timer.port = vcpu->arch.xen.timer_virq;\n\t\tdata->u.timer.priority = KVM_IRQ_ROUTING_XEN_EVTCHN_PRIO_2LEVEL;\n\t\tdata->u.timer.expires_ns = vcpu->arch.xen.timer_expires;\n\t\tr = 0;\n\t\tbreak;\n\n\tcase KVM_XEN_VCPU_ATTR_TYPE_UPCALL_VECTOR:\n\t\tdata->u.vector = vcpu->arch.xen.upcall_vector;\n\t\tr = 0;\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tmutex_unlock(&vcpu->kvm->arch.xen.xen_lock);\n\treturn r;\n}\n\nint kvm_xen_write_hypercall_page(struct kvm_vcpu *vcpu, u64 data)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\tu32 page_num = data & ~PAGE_MASK;\n\tu64 page_addr = data & PAGE_MASK;\n\tbool lm = is_long_mode(vcpu);\n\n\t \n\tvcpu->kvm->arch.xen.long_mode = lm;\n\n\t \n\tif (kvm_xen_hypercall_enabled(kvm)) {\n\t\tu8 instructions[32];\n\t\tint i;\n\n\t\tif (page_num)\n\t\t\treturn 1;\n\n\t\t \n\t\tinstructions[0] = 0xb8;\n\n\t\t \n\t\tstatic_call(kvm_x86_patch_hypercall)(vcpu, instructions + 5);\n\n\t\t \n\t\tinstructions[8] = 0xc3;\n\n\t\t \n\t\tmemset(instructions + 9, 0xcc, sizeof(instructions) - 9);\n\n\t\tfor (i = 0; i < PAGE_SIZE / sizeof(instructions); i++) {\n\t\t\t*(u32 *)&instructions[1] = i;\n\t\t\tif (kvm_vcpu_write_guest(vcpu,\n\t\t\t\t\t\t page_addr + (i * sizeof(instructions)),\n\t\t\t\t\t\t instructions, sizeof(instructions)))\n\t\t\t\treturn 1;\n\t\t}\n\t} else {\n\t\t \n\t\thva_t blob_addr = lm ? kvm->arch.xen_hvm_config.blob_addr_64\n\t\t\t\t     : kvm->arch.xen_hvm_config.blob_addr_32;\n\t\tu8 blob_size = lm ? kvm->arch.xen_hvm_config.blob_size_64\n\t\t\t\t  : kvm->arch.xen_hvm_config.blob_size_32;\n\t\tu8 *page;\n\t\tint ret;\n\n\t\tif (page_num >= blob_size)\n\t\t\treturn 1;\n\n\t\tblob_addr += page_num * PAGE_SIZE;\n\n\t\tpage = memdup_user((u8 __user *)blob_addr, PAGE_SIZE);\n\t\tif (IS_ERR(page))\n\t\t\treturn PTR_ERR(page);\n\n\t\tret = kvm_vcpu_write_guest(vcpu, page_addr, page, PAGE_SIZE);\n\t\tkfree(page);\n\t\tif (ret)\n\t\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nint kvm_xen_hvm_config(struct kvm *kvm, struct kvm_xen_hvm_config *xhc)\n{\n\t \n\tu32 permitted_flags = KVM_XEN_HVM_CONFIG_INTERCEPT_HCALL |\n\t\tKVM_XEN_HVM_CONFIG_EVTCHN_SEND;\n\n\tif (xhc->flags & ~permitted_flags)\n\t\treturn -EINVAL;\n\n\t \n\tif ((xhc->flags & KVM_XEN_HVM_CONFIG_INTERCEPT_HCALL) &&\n\t    (xhc->blob_addr_32 || xhc->blob_addr_64 ||\n\t     xhc->blob_size_32 || xhc->blob_size_64))\n\t\treturn -EINVAL;\n\n\tmutex_lock(&kvm->arch.xen.xen_lock);\n\n\tif (xhc->msr && !kvm->arch.xen_hvm_config.msr)\n\t\tstatic_branch_inc(&kvm_xen_enabled.key);\n\telse if (!xhc->msr && kvm->arch.xen_hvm_config.msr)\n\t\tstatic_branch_slow_dec_deferred(&kvm_xen_enabled);\n\n\tmemcpy(&kvm->arch.xen_hvm_config, xhc, sizeof(*xhc));\n\n\tmutex_unlock(&kvm->arch.xen.xen_lock);\n\treturn 0;\n}\n\nstatic int kvm_xen_hypercall_set_result(struct kvm_vcpu *vcpu, u64 result)\n{\n\tkvm_rax_write(vcpu, result);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\n\nstatic int kvm_xen_hypercall_complete_userspace(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_run *run = vcpu->run;\n\n\tif (unlikely(!kvm_is_linear_rip(vcpu, vcpu->arch.xen.hypercall_rip)))\n\t\treturn 1;\n\n\treturn kvm_xen_hypercall_set_result(vcpu, run->xen.u.hcall.result);\n}\n\nstatic inline int max_evtchn_port(struct kvm *kvm)\n{\n\tif (IS_ENABLED(CONFIG_64BIT) && kvm->arch.xen.long_mode)\n\t\treturn EVTCHN_2L_NR_CHANNELS;\n\telse\n\t\treturn COMPAT_EVTCHN_2L_NR_CHANNELS;\n}\n\nstatic bool wait_pending_event(struct kvm_vcpu *vcpu, int nr_ports,\n\t\t\t       evtchn_port_t *ports)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\tstruct gfn_to_pfn_cache *gpc = &kvm->arch.xen.shinfo_cache;\n\tunsigned long *pending_bits;\n\tunsigned long flags;\n\tbool ret = true;\n\tint idx, i;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\tread_lock_irqsave(&gpc->lock, flags);\n\tif (!kvm_gpc_check(gpc, PAGE_SIZE))\n\t\tgoto out_rcu;\n\n\tret = false;\n\tif (IS_ENABLED(CONFIG_64BIT) && kvm->arch.xen.long_mode) {\n\t\tstruct shared_info *shinfo = gpc->khva;\n\t\tpending_bits = (unsigned long *)&shinfo->evtchn_pending;\n\t} else {\n\t\tstruct compat_shared_info *shinfo = gpc->khva;\n\t\tpending_bits = (unsigned long *)&shinfo->evtchn_pending;\n\t}\n\n\tfor (i = 0; i < nr_ports; i++) {\n\t\tif (test_bit(ports[i], pending_bits)) {\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\n out_rcu:\n\tread_unlock_irqrestore(&gpc->lock, flags);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\treturn ret;\n}\n\nstatic bool kvm_xen_schedop_poll(struct kvm_vcpu *vcpu, bool longmode,\n\t\t\t\t u64 param, u64 *r)\n{\n\tstruct sched_poll sched_poll;\n\tevtchn_port_t port, *ports;\n\tstruct x86_exception e;\n\tint i;\n\n\tif (!lapic_in_kernel(vcpu) ||\n\t    !(vcpu->kvm->arch.xen_hvm_config.flags & KVM_XEN_HVM_CONFIG_EVTCHN_SEND))\n\t\treturn false;\n\n\tif (IS_ENABLED(CONFIG_64BIT) && !longmode) {\n\t\tstruct compat_sched_poll sp32;\n\n\t\t \n\t\tBUILD_BUG_ON(sizeof(sp32) != 16);\n\n\t\tif (kvm_read_guest_virt(vcpu, param, &sp32, sizeof(sp32), &e)) {\n\t\t\t*r = -EFAULT;\n\t\t\treturn true;\n\t\t}\n\n\t\t \n\t\tsched_poll.ports = (void *)(unsigned long)(sp32.ports);\n\t\tsched_poll.nr_ports = sp32.nr_ports;\n\t\tsched_poll.timeout = sp32.timeout;\n\t} else {\n\t\tif (kvm_read_guest_virt(vcpu, param, &sched_poll,\n\t\t\t\t\tsizeof(sched_poll), &e)) {\n\t\t\t*r = -EFAULT;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\tif (unlikely(sched_poll.nr_ports > 1)) {\n\t\t \n\t\tif (sched_poll.nr_ports > 128) {\n\t\t\t*r = -EINVAL;\n\t\t\treturn true;\n\t\t}\n\n\t\tports = kmalloc_array(sched_poll.nr_ports,\n\t\t\t\t      sizeof(*ports), GFP_KERNEL);\n\t\tif (!ports) {\n\t\t\t*r = -ENOMEM;\n\t\t\treturn true;\n\t\t}\n\t} else\n\t\tports = &port;\n\n\tif (kvm_read_guest_virt(vcpu, (gva_t)sched_poll.ports, ports,\n\t\t\t\tsched_poll.nr_ports * sizeof(*ports), &e)) {\n\t\t*r = -EFAULT;\n\t\treturn true;\n\t}\n\n\tfor (i = 0; i < sched_poll.nr_ports; i++) {\n\t\tif (ports[i] >= max_evtchn_port(vcpu->kvm)) {\n\t\t\t*r = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (sched_poll.nr_ports == 1)\n\t\tvcpu->arch.xen.poll_evtchn = port;\n\telse\n\t\tvcpu->arch.xen.poll_evtchn = -1;\n\n\tset_bit(vcpu->vcpu_idx, vcpu->kvm->arch.xen.poll_mask);\n\n\tif (!wait_pending_event(vcpu, sched_poll.nr_ports, ports)) {\n\t\tvcpu->arch.mp_state = KVM_MP_STATE_HALTED;\n\n\t\tif (sched_poll.timeout)\n\t\t\tmod_timer(&vcpu->arch.xen.poll_timer,\n\t\t\t\t  jiffies + nsecs_to_jiffies(sched_poll.timeout));\n\n\t\tkvm_vcpu_halt(vcpu);\n\n\t\tif (sched_poll.timeout)\n\t\t\tdel_timer(&vcpu->arch.xen.poll_timer);\n\n\t\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\t}\n\n\tvcpu->arch.xen.poll_evtchn = 0;\n\t*r = 0;\nout:\n\t \n\tclear_bit(vcpu->vcpu_idx, vcpu->kvm->arch.xen.poll_mask);\n\n\tif (unlikely(sched_poll.nr_ports > 1))\n\t\tkfree(ports);\n\treturn true;\n}\n\nstatic void cancel_evtchn_poll(struct timer_list *t)\n{\n\tstruct kvm_vcpu *vcpu = from_timer(vcpu, t, arch.xen.poll_timer);\n\n\tkvm_make_request(KVM_REQ_UNBLOCK, vcpu);\n\tkvm_vcpu_kick(vcpu);\n}\n\nstatic bool kvm_xen_hcall_sched_op(struct kvm_vcpu *vcpu, bool longmode,\n\t\t\t\t   int cmd, u64 param, u64 *r)\n{\n\tswitch (cmd) {\n\tcase SCHEDOP_poll:\n\t\tif (kvm_xen_schedop_poll(vcpu, longmode, param, r))\n\t\t\treturn true;\n\t\tfallthrough;\n\tcase SCHEDOP_yield:\n\t\tkvm_vcpu_on_spin(vcpu, true);\n\t\t*r = 0;\n\t\treturn true;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn false;\n}\n\nstruct compat_vcpu_set_singleshot_timer {\n    uint64_t timeout_abs_ns;\n    uint32_t flags;\n} __attribute__((packed));\n\nstatic bool kvm_xen_hcall_vcpu_op(struct kvm_vcpu *vcpu, bool longmode, int cmd,\n\t\t\t\t  int vcpu_id, u64 param, u64 *r)\n{\n\tstruct vcpu_set_singleshot_timer oneshot;\n\tstruct x86_exception e;\n\ts64 delta;\n\n\tif (!kvm_xen_timer_enabled(vcpu))\n\t\treturn false;\n\n\tswitch (cmd) {\n\tcase VCPUOP_set_singleshot_timer:\n\t\tif (vcpu->arch.xen.vcpu_id != vcpu_id) {\n\t\t\t*r = -EINVAL;\n\t\t\treturn true;\n\t\t}\n\n\t\t \n\t\tBUILD_BUG_ON(offsetof(struct compat_vcpu_set_singleshot_timer, timeout_abs_ns) !=\n\t\t\t     offsetof(struct vcpu_set_singleshot_timer, timeout_abs_ns));\n\t\tBUILD_BUG_ON(sizeof_field(struct compat_vcpu_set_singleshot_timer, timeout_abs_ns) !=\n\t\t\t     sizeof_field(struct vcpu_set_singleshot_timer, timeout_abs_ns));\n\t\tBUILD_BUG_ON(offsetof(struct compat_vcpu_set_singleshot_timer, flags) !=\n\t\t\t     offsetof(struct vcpu_set_singleshot_timer, flags));\n\t\tBUILD_BUG_ON(sizeof_field(struct compat_vcpu_set_singleshot_timer, flags) !=\n\t\t\t     sizeof_field(struct vcpu_set_singleshot_timer, flags));\n\n\t\tif (kvm_read_guest_virt(vcpu, param, &oneshot, longmode ? sizeof(oneshot) :\n\t\t\t\t\tsizeof(struct compat_vcpu_set_singleshot_timer), &e)) {\n\t\t\t*r = -EFAULT;\n\t\t\treturn true;\n\t\t}\n\n\t\tdelta = oneshot.timeout_abs_ns - get_kvmclock_ns(vcpu->kvm);\n\t\tif ((oneshot.flags & VCPU_SSHOTTMR_future) && delta < 0) {\n\t\t\t*r = -ETIME;\n\t\t\treturn true;\n\t\t}\n\n\t\tkvm_xen_start_timer(vcpu, oneshot.timeout_abs_ns, delta);\n\t\t*r = 0;\n\t\treturn true;\n\n\tcase VCPUOP_stop_singleshot_timer:\n\t\tif (vcpu->arch.xen.vcpu_id != vcpu_id) {\n\t\t\t*r = -EINVAL;\n\t\t\treturn true;\n\t\t}\n\t\tkvm_xen_stop_timer(vcpu);\n\t\t*r = 0;\n\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nstatic bool kvm_xen_hcall_set_timer_op(struct kvm_vcpu *vcpu, uint64_t timeout,\n\t\t\t\t       u64 *r)\n{\n\tif (!kvm_xen_timer_enabled(vcpu))\n\t\treturn false;\n\n\tif (timeout) {\n\t\tuint64_t guest_now = get_kvmclock_ns(vcpu->kvm);\n\t\tint64_t delta = timeout - guest_now;\n\n\t\t \n\t\tif (unlikely((int64_t)timeout < 0 ||\n\t\t\t     (delta > 0 && (uint32_t) (delta >> 50) != 0))) {\n\t\t\tdelta = 100 * NSEC_PER_MSEC;\n\t\t\ttimeout = guest_now + delta;\n\t\t}\n\n\t\tkvm_xen_start_timer(vcpu, timeout, delta);\n\t} else {\n\t\tkvm_xen_stop_timer(vcpu);\n\t}\n\n\t*r = 0;\n\treturn true;\n}\n\nint kvm_xen_hypercall(struct kvm_vcpu *vcpu)\n{\n\tbool longmode;\n\tu64 input, params[6], r = -ENOSYS;\n\tbool handled = false;\n\tu8 cpl;\n\n\tinput = (u64)kvm_register_read(vcpu, VCPU_REGS_RAX);\n\n\t \n\tif ((input & 0x80000000) &&\n\t    kvm_hv_hypercall_enabled(vcpu))\n\t\treturn kvm_hv_hypercall(vcpu);\n\n\tlongmode = is_64_bit_hypercall(vcpu);\n\tif (!longmode) {\n\t\tparams[0] = (u32)kvm_rbx_read(vcpu);\n\t\tparams[1] = (u32)kvm_rcx_read(vcpu);\n\t\tparams[2] = (u32)kvm_rdx_read(vcpu);\n\t\tparams[3] = (u32)kvm_rsi_read(vcpu);\n\t\tparams[4] = (u32)kvm_rdi_read(vcpu);\n\t\tparams[5] = (u32)kvm_rbp_read(vcpu);\n\t}\n#ifdef CONFIG_X86_64\n\telse {\n\t\tparams[0] = (u64)kvm_rdi_read(vcpu);\n\t\tparams[1] = (u64)kvm_rsi_read(vcpu);\n\t\tparams[2] = (u64)kvm_rdx_read(vcpu);\n\t\tparams[3] = (u64)kvm_r10_read(vcpu);\n\t\tparams[4] = (u64)kvm_r8_read(vcpu);\n\t\tparams[5] = (u64)kvm_r9_read(vcpu);\n\t}\n#endif\n\tcpl = static_call(kvm_x86_get_cpl)(vcpu);\n\ttrace_kvm_xen_hypercall(cpl, input, params[0], params[1], params[2],\n\t\t\t\tparams[3], params[4], params[5]);\n\n\t \n\tif (unlikely(cpl > 0))\n\t\tgoto handle_in_userspace;\n\n\tswitch (input) {\n\tcase __HYPERVISOR_xen_version:\n\t\tif (params[0] == XENVER_version && vcpu->kvm->arch.xen.xen_version) {\n\t\t\tr = vcpu->kvm->arch.xen.xen_version;\n\t\t\thandled = true;\n\t\t}\n\t\tbreak;\n\tcase __HYPERVISOR_event_channel_op:\n\t\tif (params[0] == EVTCHNOP_send)\n\t\t\thandled = kvm_xen_hcall_evtchn_send(vcpu, params[1], &r);\n\t\tbreak;\n\tcase __HYPERVISOR_sched_op:\n\t\thandled = kvm_xen_hcall_sched_op(vcpu, longmode, params[0],\n\t\t\t\t\t\t params[1], &r);\n\t\tbreak;\n\tcase __HYPERVISOR_vcpu_op:\n\t\thandled = kvm_xen_hcall_vcpu_op(vcpu, longmode, params[0], params[1],\n\t\t\t\t\t\tparams[2], &r);\n\t\tbreak;\n\tcase __HYPERVISOR_set_timer_op: {\n\t\tu64 timeout = params[0];\n\t\t \n\t\tif (!longmode)\n\t\t\ttimeout |= params[1] << 32;\n\t\thandled = kvm_xen_hcall_set_timer_op(vcpu, timeout, &r);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tbreak;\n\t}\n\n\tif (handled)\n\t\treturn kvm_xen_hypercall_set_result(vcpu, r);\n\nhandle_in_userspace:\n\tvcpu->run->exit_reason = KVM_EXIT_XEN;\n\tvcpu->run->xen.type = KVM_EXIT_XEN_HCALL;\n\tvcpu->run->xen.u.hcall.longmode = longmode;\n\tvcpu->run->xen.u.hcall.cpl = cpl;\n\tvcpu->run->xen.u.hcall.input = input;\n\tvcpu->run->xen.u.hcall.params[0] = params[0];\n\tvcpu->run->xen.u.hcall.params[1] = params[1];\n\tvcpu->run->xen.u.hcall.params[2] = params[2];\n\tvcpu->run->xen.u.hcall.params[3] = params[3];\n\tvcpu->run->xen.u.hcall.params[4] = params[4];\n\tvcpu->run->xen.u.hcall.params[5] = params[5];\n\tvcpu->arch.xen.hypercall_rip = kvm_get_linear_rip(vcpu);\n\tvcpu->arch.complete_userspace_io =\n\t\tkvm_xen_hypercall_complete_userspace;\n\n\treturn 0;\n}\n\nstatic void kvm_xen_check_poller(struct kvm_vcpu *vcpu, int port)\n{\n\tint poll_evtchn = vcpu->arch.xen.poll_evtchn;\n\n\tif ((poll_evtchn == port || poll_evtchn == -1) &&\n\t    test_and_clear_bit(vcpu->vcpu_idx, vcpu->kvm->arch.xen.poll_mask)) {\n\t\tkvm_make_request(KVM_REQ_UNBLOCK, vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t}\n}\n\n \nint kvm_xen_set_evtchn_fast(struct kvm_xen_evtchn *xe, struct kvm *kvm)\n{\n\tstruct gfn_to_pfn_cache *gpc = &kvm->arch.xen.shinfo_cache;\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long *pending_bits, *mask_bits;\n\tunsigned long flags;\n\tint port_word_bit;\n\tbool kick_vcpu = false;\n\tint vcpu_idx, idx, rc;\n\n\tvcpu_idx = READ_ONCE(xe->vcpu_idx);\n\tif (vcpu_idx >= 0)\n\t\tvcpu = kvm_get_vcpu(kvm, vcpu_idx);\n\telse {\n\t\tvcpu = kvm_get_vcpu_by_id(kvm, xe->vcpu_id);\n\t\tif (!vcpu)\n\t\t\treturn -EINVAL;\n\t\tWRITE_ONCE(xe->vcpu_idx, vcpu->vcpu_idx);\n\t}\n\n\tif (!vcpu->arch.xen.vcpu_info_cache.active)\n\t\treturn -EINVAL;\n\n\tif (xe->port >= max_evtchn_port(kvm))\n\t\treturn -EINVAL;\n\n\trc = -EWOULDBLOCK;\n\n\tidx = srcu_read_lock(&kvm->srcu);\n\n\tread_lock_irqsave(&gpc->lock, flags);\n\tif (!kvm_gpc_check(gpc, PAGE_SIZE))\n\t\tgoto out_rcu;\n\n\tif (IS_ENABLED(CONFIG_64BIT) && kvm->arch.xen.long_mode) {\n\t\tstruct shared_info *shinfo = gpc->khva;\n\t\tpending_bits = (unsigned long *)&shinfo->evtchn_pending;\n\t\tmask_bits = (unsigned long *)&shinfo->evtchn_mask;\n\t\tport_word_bit = xe->port / 64;\n\t} else {\n\t\tstruct compat_shared_info *shinfo = gpc->khva;\n\t\tpending_bits = (unsigned long *)&shinfo->evtchn_pending;\n\t\tmask_bits = (unsigned long *)&shinfo->evtchn_mask;\n\t\tport_word_bit = xe->port / 32;\n\t}\n\n\t \n\tif (test_and_set_bit(xe->port, pending_bits)) {\n\t\trc = 0;  \n\t} else if (test_bit(xe->port, mask_bits)) {\n\t\trc = -ENOTCONN;  \n\t\tkvm_xen_check_poller(vcpu, xe->port);\n\t} else {\n\t\trc = 1;  \n\t\t \n\t\tread_unlock_irqrestore(&gpc->lock, flags);\n\t\tgpc = &vcpu->arch.xen.vcpu_info_cache;\n\n\t\tread_lock_irqsave(&gpc->lock, flags);\n\t\tif (!kvm_gpc_check(gpc, sizeof(struct vcpu_info))) {\n\t\t\t \n\t\t\tif (!test_and_set_bit(port_word_bit, &vcpu->arch.xen.evtchn_pending_sel))\n\t\t\t\tkick_vcpu = true;\n\t\t\tgoto out_rcu;\n\t\t}\n\n\t\tif (IS_ENABLED(CONFIG_64BIT) && kvm->arch.xen.long_mode) {\n\t\t\tstruct vcpu_info *vcpu_info = gpc->khva;\n\t\t\tif (!test_and_set_bit(port_word_bit, &vcpu_info->evtchn_pending_sel)) {\n\t\t\t\tWRITE_ONCE(vcpu_info->evtchn_upcall_pending, 1);\n\t\t\t\tkick_vcpu = true;\n\t\t\t}\n\t\t} else {\n\t\t\tstruct compat_vcpu_info *vcpu_info = gpc->khva;\n\t\t\tif (!test_and_set_bit(port_word_bit,\n\t\t\t\t\t      (unsigned long *)&vcpu_info->evtchn_pending_sel)) {\n\t\t\t\tWRITE_ONCE(vcpu_info->evtchn_upcall_pending, 1);\n\t\t\t\tkick_vcpu = true;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (kick_vcpu && vcpu->arch.xen.upcall_vector) {\n\t\t\tkvm_xen_inject_vcpu_vector(vcpu);\n\t\t\tkick_vcpu = false;\n\t\t}\n\t}\n\n out_rcu:\n\tread_unlock_irqrestore(&gpc->lock, flags);\n\tsrcu_read_unlock(&kvm->srcu, idx);\n\n\tif (kick_vcpu) {\n\t\tkvm_make_request(KVM_REQ_UNBLOCK, vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t}\n\n\treturn rc;\n}\n\nstatic int kvm_xen_set_evtchn(struct kvm_xen_evtchn *xe, struct kvm *kvm)\n{\n\tbool mm_borrowed = false;\n\tint rc;\n\n\trc = kvm_xen_set_evtchn_fast(xe, kvm);\n\tif (rc != -EWOULDBLOCK)\n\t\treturn rc;\n\n\tif (current->mm != kvm->mm) {\n\t\t \n\t\tif (WARN_ON_ONCE(current->mm))\n\t\t\treturn -EINVAL;\n\n\t\tkthread_use_mm(kvm->mm);\n\t\tmm_borrowed = true;\n\t}\n\n\tmutex_lock(&kvm->arch.xen.xen_lock);\n\n\t \n\tdo {\n\t\tstruct gfn_to_pfn_cache *gpc = &kvm->arch.xen.shinfo_cache;\n\t\tint idx;\n\n\t\trc = kvm_xen_set_evtchn_fast(xe, kvm);\n\t\tif (rc != -EWOULDBLOCK)\n\t\t\tbreak;\n\n\t\tidx = srcu_read_lock(&kvm->srcu);\n\t\trc = kvm_gpc_refresh(gpc, PAGE_SIZE);\n\t\tsrcu_read_unlock(&kvm->srcu, idx);\n\t} while(!rc);\n\n\tmutex_unlock(&kvm->arch.xen.xen_lock);\n\n\tif (mm_borrowed)\n\t\tkthread_unuse_mm(kvm->mm);\n\n\treturn rc;\n}\n\n \nstatic int evtchn_set_fn(struct kvm_kernel_irq_routing_entry *e, struct kvm *kvm,\n\t\t\t int irq_source_id, int level, bool line_status)\n{\n\tif (!level)\n\t\treturn -EINVAL;\n\n\treturn kvm_xen_set_evtchn(&e->xen_evtchn, kvm);\n}\n\n \nint kvm_xen_setup_evtchn(struct kvm *kvm,\n\t\t\t struct kvm_kernel_irq_routing_entry *e,\n\t\t\t const struct kvm_irq_routing_entry *ue)\n\n{\n\tstruct kvm_vcpu *vcpu;\n\n\tif (ue->u.xen_evtchn.port >= max_evtchn_port(kvm))\n\t\treturn -EINVAL;\n\n\t \n\tif (ue->u.xen_evtchn.priority != KVM_IRQ_ROUTING_XEN_EVTCHN_PRIO_2LEVEL)\n\t\treturn -EINVAL;\n\n\t \n\tvcpu = kvm_get_vcpu_by_id(kvm, ue->u.xen_evtchn.vcpu);\n\tif (vcpu)\n\t\te->xen_evtchn.vcpu_idx = vcpu->vcpu_idx;\n\telse\n\t\te->xen_evtchn.vcpu_idx = -1;\n\n\te->xen_evtchn.port = ue->u.xen_evtchn.port;\n\te->xen_evtchn.vcpu_id = ue->u.xen_evtchn.vcpu;\n\te->xen_evtchn.priority = ue->u.xen_evtchn.priority;\n\te->set = evtchn_set_fn;\n\n\treturn 0;\n}\n\n \nint kvm_xen_hvm_evtchn_send(struct kvm *kvm, struct kvm_irq_routing_xen_evtchn *uxe)\n{\n\tstruct kvm_xen_evtchn e;\n\tint ret;\n\n\tif (!uxe->port || uxe->port >= max_evtchn_port(kvm))\n\t\treturn -EINVAL;\n\n\t \n\tif (uxe->priority != KVM_IRQ_ROUTING_XEN_EVTCHN_PRIO_2LEVEL)\n\t\treturn -EINVAL;\n\n\te.port = uxe->port;\n\te.vcpu_id = uxe->vcpu;\n\te.vcpu_idx = -1;\n\te.priority = uxe->priority;\n\n\tret = kvm_xen_set_evtchn(&e, kvm);\n\n\t \n\tif (ret > 0 || ret == -ENOTCONN)\n\t\tret = 0;\n\n\treturn ret;\n}\n\n \nstruct evtchnfd {\n\tu32 send_port;\n\tu32 type;\n\tunion {\n\t\tstruct kvm_xen_evtchn port;\n\t\tstruct {\n\t\t\tu32 port;  \n\t\t\tstruct eventfd_ctx *ctx;\n\t\t} eventfd;\n\t} deliver;\n};\n\n \nstatic int kvm_xen_eventfd_update(struct kvm *kvm,\n\t\t\t\t  struct kvm_xen_hvm_attr *data)\n{\n\tu32 port = data->u.evtchn.send_port;\n\tstruct evtchnfd *evtchnfd;\n\tint ret;\n\n\t \n\tmutex_lock(&kvm->arch.xen.xen_lock);\n\tevtchnfd = idr_find(&kvm->arch.xen.evtchn_ports, port);\n\n\tret = -ENOENT;\n\tif (!evtchnfd)\n\t\tgoto out_unlock;\n\n\t \n\tret = -EINVAL;\n\tif (evtchnfd->type != data->u.evtchn.type)\n\t\tgoto out_unlock;\n\n\t \n\tif (!evtchnfd->deliver.port.port ||\n\t    evtchnfd->deliver.port.port != data->u.evtchn.deliver.port.port)\n\t\tgoto out_unlock;\n\n\t \n\tif (data->u.evtchn.deliver.port.priority != KVM_IRQ_ROUTING_XEN_EVTCHN_PRIO_2LEVEL)\n\t\tgoto out_unlock;\n\n\tevtchnfd->deliver.port.priority = data->u.evtchn.deliver.port.priority;\n\tif (evtchnfd->deliver.port.vcpu_id != data->u.evtchn.deliver.port.vcpu) {\n\t\tevtchnfd->deliver.port.vcpu_id = data->u.evtchn.deliver.port.vcpu;\n\t\tevtchnfd->deliver.port.vcpu_idx = -1;\n\t}\n\tret = 0;\nout_unlock:\n\tmutex_unlock(&kvm->arch.xen.xen_lock);\n\treturn ret;\n}\n\n \nstatic int kvm_xen_eventfd_assign(struct kvm *kvm,\n\t\t\t\t  struct kvm_xen_hvm_attr *data)\n{\n\tu32 port = data->u.evtchn.send_port;\n\tstruct eventfd_ctx *eventfd = NULL;\n\tstruct evtchnfd *evtchnfd;\n\tint ret = -EINVAL;\n\n\tevtchnfd = kzalloc(sizeof(struct evtchnfd), GFP_KERNEL);\n\tif (!evtchnfd)\n\t\treturn -ENOMEM;\n\n\tswitch(data->u.evtchn.type) {\n\tcase EVTCHNSTAT_ipi:\n\t\t \n\t\tif (data->u.evtchn.deliver.port.port != data->u.evtchn.send_port)\n\t\t\tgoto out_noeventfd;  \n\t\tbreak;\n\n\tcase EVTCHNSTAT_interdomain:\n\t\tif (data->u.evtchn.deliver.port.port) {\n\t\t\tif (data->u.evtchn.deliver.port.port >= max_evtchn_port(kvm))\n\t\t\t\tgoto out_noeventfd;  \n\t\t} else {\n\t\t\teventfd = eventfd_ctx_fdget(data->u.evtchn.deliver.eventfd.fd);\n\t\t\tif (IS_ERR(eventfd)) {\n\t\t\t\tret = PTR_ERR(eventfd);\n\t\t\t\tgoto out_noeventfd;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n\tcase EVTCHNSTAT_virq:\n\tcase EVTCHNSTAT_closed:\n\tcase EVTCHNSTAT_unbound:\n\tcase EVTCHNSTAT_pirq:\n\tdefault:  \n\t\tgoto out;  \n\t}\n\n\tevtchnfd->send_port = data->u.evtchn.send_port;\n\tevtchnfd->type = data->u.evtchn.type;\n\tif (eventfd) {\n\t\tevtchnfd->deliver.eventfd.ctx = eventfd;\n\t} else {\n\t\t \n\t\tif (data->u.evtchn.deliver.port.priority != KVM_IRQ_ROUTING_XEN_EVTCHN_PRIO_2LEVEL)\n\t\t\tgoto out;  \n\n\t\tevtchnfd->deliver.port.port = data->u.evtchn.deliver.port.port;\n\t\tevtchnfd->deliver.port.vcpu_id = data->u.evtchn.deliver.port.vcpu;\n\t\tevtchnfd->deliver.port.vcpu_idx = -1;\n\t\tevtchnfd->deliver.port.priority = data->u.evtchn.deliver.port.priority;\n\t}\n\n\tmutex_lock(&kvm->arch.xen.xen_lock);\n\tret = idr_alloc(&kvm->arch.xen.evtchn_ports, evtchnfd, port, port + 1,\n\t\t\tGFP_KERNEL);\n\tmutex_unlock(&kvm->arch.xen.xen_lock);\n\tif (ret >= 0)\n\t\treturn 0;\n\n\tif (ret == -ENOSPC)\n\t\tret = -EEXIST;\nout:\n\tif (eventfd)\n\t\teventfd_ctx_put(eventfd);\nout_noeventfd:\n\tkfree(evtchnfd);\n\treturn ret;\n}\n\nstatic int kvm_xen_eventfd_deassign(struct kvm *kvm, u32 port)\n{\n\tstruct evtchnfd *evtchnfd;\n\n\tmutex_lock(&kvm->arch.xen.xen_lock);\n\tevtchnfd = idr_remove(&kvm->arch.xen.evtchn_ports, port);\n\tmutex_unlock(&kvm->arch.xen.xen_lock);\n\n\tif (!evtchnfd)\n\t\treturn -ENOENT;\n\n\tsynchronize_srcu(&kvm->srcu);\n\tif (!evtchnfd->deliver.port.port)\n\t\teventfd_ctx_put(evtchnfd->deliver.eventfd.ctx);\n\tkfree(evtchnfd);\n\treturn 0;\n}\n\nstatic int kvm_xen_eventfd_reset(struct kvm *kvm)\n{\n\tstruct evtchnfd *evtchnfd, **all_evtchnfds;\n\tint i;\n\tint n = 0;\n\n\tmutex_lock(&kvm->arch.xen.xen_lock);\n\n\t \n\tidr_for_each_entry(&kvm->arch.xen.evtchn_ports, evtchnfd, i)\n\t\tn++;\n\n\tall_evtchnfds = kmalloc_array(n, sizeof(struct evtchnfd *), GFP_KERNEL);\n\tif (!all_evtchnfds) {\n\t\tmutex_unlock(&kvm->arch.xen.xen_lock);\n\t\treturn -ENOMEM;\n\t}\n\n\tn = 0;\n\tidr_for_each_entry(&kvm->arch.xen.evtchn_ports, evtchnfd, i) {\n\t\tall_evtchnfds[n++] = evtchnfd;\n\t\tidr_remove(&kvm->arch.xen.evtchn_ports, evtchnfd->send_port);\n\t}\n\tmutex_unlock(&kvm->arch.xen.xen_lock);\n\n\tsynchronize_srcu(&kvm->srcu);\n\n\twhile (n--) {\n\t\tevtchnfd = all_evtchnfds[n];\n\t\tif (!evtchnfd->deliver.port.port)\n\t\t\teventfd_ctx_put(evtchnfd->deliver.eventfd.ctx);\n\t\tkfree(evtchnfd);\n\t}\n\tkfree(all_evtchnfds);\n\n\treturn 0;\n}\n\nstatic int kvm_xen_setattr_evtchn(struct kvm *kvm, struct kvm_xen_hvm_attr *data)\n{\n\tu32 port = data->u.evtchn.send_port;\n\n\tif (data->u.evtchn.flags == KVM_XEN_EVTCHN_RESET)\n\t\treturn kvm_xen_eventfd_reset(kvm);\n\n\tif (!port || port >= max_evtchn_port(kvm))\n\t\treturn -EINVAL;\n\n\tif (data->u.evtchn.flags == KVM_XEN_EVTCHN_DEASSIGN)\n\t\treturn kvm_xen_eventfd_deassign(kvm, port);\n\tif (data->u.evtchn.flags == KVM_XEN_EVTCHN_UPDATE)\n\t\treturn kvm_xen_eventfd_update(kvm, data);\n\tif (data->u.evtchn.flags)\n\t\treturn -EINVAL;\n\n\treturn kvm_xen_eventfd_assign(kvm, data);\n}\n\nstatic bool kvm_xen_hcall_evtchn_send(struct kvm_vcpu *vcpu, u64 param, u64 *r)\n{\n\tstruct evtchnfd *evtchnfd;\n\tstruct evtchn_send send;\n\tstruct x86_exception e;\n\n\t \n\tBUILD_BUG_ON(sizeof(send) != 4);\n\tif (kvm_read_guest_virt(vcpu, param, &send, sizeof(send), &e)) {\n\t\t*r = -EFAULT;\n\t\treturn true;\n\t}\n\n\t \n\trcu_read_lock();\n\tevtchnfd = idr_find(&vcpu->kvm->arch.xen.evtchn_ports, send.port);\n\trcu_read_unlock();\n\tif (!evtchnfd)\n\t\treturn false;\n\n\tif (evtchnfd->deliver.port.port) {\n\t\tint ret = kvm_xen_set_evtchn(&evtchnfd->deliver.port, vcpu->kvm);\n\t\tif (ret < 0 && ret != -ENOTCONN)\n\t\t\treturn false;\n\t} else {\n\t\teventfd_signal(evtchnfd->deliver.eventfd.ctx, 1);\n\t}\n\n\t*r = 0;\n\treturn true;\n}\n\nvoid kvm_xen_init_vcpu(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.xen.vcpu_id = vcpu->vcpu_idx;\n\tvcpu->arch.xen.poll_evtchn = 0;\n\n\ttimer_setup(&vcpu->arch.xen.poll_timer, cancel_evtchn_poll, 0);\n\n\tkvm_gpc_init(&vcpu->arch.xen.runstate_cache, vcpu->kvm, NULL,\n\t\t     KVM_HOST_USES_PFN);\n\tkvm_gpc_init(&vcpu->arch.xen.runstate2_cache, vcpu->kvm, NULL,\n\t\t     KVM_HOST_USES_PFN);\n\tkvm_gpc_init(&vcpu->arch.xen.vcpu_info_cache, vcpu->kvm, NULL,\n\t\t     KVM_HOST_USES_PFN);\n\tkvm_gpc_init(&vcpu->arch.xen.vcpu_time_info_cache, vcpu->kvm, NULL,\n\t\t     KVM_HOST_USES_PFN);\n}\n\nvoid kvm_xen_destroy_vcpu(struct kvm_vcpu *vcpu)\n{\n\tif (kvm_xen_timer_enabled(vcpu))\n\t\tkvm_xen_stop_timer(vcpu);\n\n\tkvm_gpc_deactivate(&vcpu->arch.xen.runstate_cache);\n\tkvm_gpc_deactivate(&vcpu->arch.xen.runstate2_cache);\n\tkvm_gpc_deactivate(&vcpu->arch.xen.vcpu_info_cache);\n\tkvm_gpc_deactivate(&vcpu->arch.xen.vcpu_time_info_cache);\n\n\tdel_timer_sync(&vcpu->arch.xen.poll_timer);\n}\n\nvoid kvm_xen_update_tsc_info(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_cpuid_entry2 *entry;\n\tu32 function;\n\n\tif (!vcpu->arch.xen.cpuid.base)\n\t\treturn;\n\n\tfunction = vcpu->arch.xen.cpuid.base | XEN_CPUID_LEAF(3);\n\tif (function > vcpu->arch.xen.cpuid.limit)\n\t\treturn;\n\n\tentry = kvm_find_cpuid_entry_index(vcpu, function, 1);\n\tif (entry) {\n\t\tentry->ecx = vcpu->arch.hv_clock.tsc_to_system_mul;\n\t\tentry->edx = vcpu->arch.hv_clock.tsc_shift;\n\t}\n\n\tentry = kvm_find_cpuid_entry_index(vcpu, function, 2);\n\tif (entry)\n\t\tentry->eax = vcpu->arch.hw_tsc_khz;\n}\n\nvoid kvm_xen_init_vm(struct kvm *kvm)\n{\n\tmutex_init(&kvm->arch.xen.xen_lock);\n\tidr_init(&kvm->arch.xen.evtchn_ports);\n\tkvm_gpc_init(&kvm->arch.xen.shinfo_cache, kvm, NULL, KVM_HOST_USES_PFN);\n}\n\nvoid kvm_xen_destroy_vm(struct kvm *kvm)\n{\n\tstruct evtchnfd *evtchnfd;\n\tint i;\n\n\tkvm_gpc_deactivate(&kvm->arch.xen.shinfo_cache);\n\n\tidr_for_each_entry(&kvm->arch.xen.evtchn_ports, evtchnfd, i) {\n\t\tif (!evtchnfd->deliver.port.port)\n\t\t\teventfd_ctx_put(evtchnfd->deliver.eventfd.ctx);\n\t\tkfree(evtchnfd);\n\t}\n\tidr_destroy(&kvm->arch.xen.evtchn_ports);\n\n\tif (kvm->arch.xen_hvm_config.msr)\n\t\tstatic_branch_slow_dec_deferred(&kvm_xen_enabled);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}