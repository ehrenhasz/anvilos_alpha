{
  "module_name": "cpuid.c",
  "hash_id": "87cce168a824957a2a8c9a56831d7f85c1bb95cc3ca7f0a8353d20a4894a9044",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/cpuid.c",
  "human_readable_source": "\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/kvm_host.h>\n#include \"linux/lockdep.h\"\n#include <linux/export.h>\n#include <linux/vmalloc.h>\n#include <linux/uaccess.h>\n#include <linux/sched/stat.h>\n\n#include <asm/processor.h>\n#include <asm/user.h>\n#include <asm/fpu/xstate.h>\n#include <asm/sgx.h>\n#include <asm/cpuid.h>\n#include \"cpuid.h\"\n#include \"lapic.h\"\n#include \"mmu.h\"\n#include \"trace.h\"\n#include \"pmu.h\"\n#include \"xen.h\"\n\n \nu32 kvm_cpu_caps[NR_KVM_CPU_CAPS] __read_mostly;\nEXPORT_SYMBOL_GPL(kvm_cpu_caps);\n\nu32 xstate_required_size(u64 xstate_bv, bool compacted)\n{\n\tint feature_bit = 0;\n\tu32 ret = XSAVE_HDR_SIZE + XSAVE_HDR_OFFSET;\n\n\txstate_bv &= XFEATURE_MASK_EXTEND;\n\twhile (xstate_bv) {\n\t\tif (xstate_bv & 0x1) {\n\t\t        u32 eax, ebx, ecx, edx, offset;\n\t\t        cpuid_count(0xD, feature_bit, &eax, &ebx, &ecx, &edx);\n\t\t\t \n\t\t\tif (compacted)\n\t\t\t\toffset = (ecx & 0x2) ? ALIGN(ret, 64) : ret;\n\t\t\telse\n\t\t\t\toffset = ebx;\n\t\t\tret = max(ret, offset + eax);\n\t\t}\n\n\t\txstate_bv >>= 1;\n\t\tfeature_bit++;\n\t}\n\n\treturn ret;\n}\n\n#define F feature_bit\n\n \n#define SF(name)\t\t\t\t\t\t\\\n({\t\t\t\t\t\t\t\t\\\n\tBUILD_BUG_ON(X86_FEATURE_##name >= MAX_CPU_FEATURES);\t\\\n\t(boot_cpu_has(X86_FEATURE_##name) ? F(name) : 0);\t\\\n})\n\n \n#define KVM_CPUID_INDEX_NOT_SIGNIFICANT -1ull\n\nstatic inline struct kvm_cpuid_entry2 *cpuid_entry2_find(\n\tstruct kvm_cpuid_entry2 *entries, int nent, u32 function, u64 index)\n{\n\tstruct kvm_cpuid_entry2 *e;\n\tint i;\n\n\t \n\tlockdep_assert_irqs_enabled();\n\n\tfor (i = 0; i < nent; i++) {\n\t\te = &entries[i];\n\n\t\tif (e->function != function)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!(e->flags & KVM_CPUID_FLAG_SIGNIFCANT_INDEX) || e->index == index)\n\t\t\treturn e;\n\n\n\t\t \n\t\tif (index == KVM_CPUID_INDEX_NOT_SIGNIFICANT) {\n\t\t\t \n\t\t\tWARN_ON_ONCE(cpuid_function_is_indexed(function));\n\t\t\treturn e;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic int kvm_check_cpuid(struct kvm_vcpu *vcpu,\n\t\t\t   struct kvm_cpuid_entry2 *entries,\n\t\t\t   int nent)\n{\n\tstruct kvm_cpuid_entry2 *best;\n\tu64 xfeatures;\n\n\t \n\tbest = cpuid_entry2_find(entries, nent, 0x80000008,\n\t\t\t\t KVM_CPUID_INDEX_NOT_SIGNIFICANT);\n\tif (best) {\n\t\tint vaddr_bits = (best->eax & 0xff00) >> 8;\n\n\t\tif (vaddr_bits != 48 && vaddr_bits != 57 && vaddr_bits != 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\t \n\tbest = cpuid_entry2_find(entries, nent, 0xd, 0);\n\tif (!best)\n\t\treturn 0;\n\n\txfeatures = best->eax | ((u64)best->edx << 32);\n\txfeatures &= XFEATURE_MASK_USER_DYNAMIC;\n\tif (!xfeatures)\n\t\treturn 0;\n\n\treturn fpu_enable_guest_xfd_features(&vcpu->arch.guest_fpu, xfeatures);\n}\n\n \nstatic int kvm_cpuid_check_equal(struct kvm_vcpu *vcpu, struct kvm_cpuid_entry2 *e2,\n\t\t\t\t int nent)\n{\n\tstruct kvm_cpuid_entry2 *orig;\n\tint i;\n\n\tif (nent != vcpu->arch.cpuid_nent)\n\t\treturn -EINVAL;\n\n\tfor (i = 0; i < nent; i++) {\n\t\torig = &vcpu->arch.cpuid_entries[i];\n\t\tif (e2[i].function != orig->function ||\n\t\t    e2[i].index != orig->index ||\n\t\t    e2[i].flags != orig->flags ||\n\t\t    e2[i].eax != orig->eax || e2[i].ebx != orig->ebx ||\n\t\t    e2[i].ecx != orig->ecx || e2[i].edx != orig->edx)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic struct kvm_hypervisor_cpuid kvm_get_hypervisor_cpuid(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t\t    const char *sig)\n{\n\tstruct kvm_hypervisor_cpuid cpuid = {};\n\tstruct kvm_cpuid_entry2 *entry;\n\tu32 base;\n\n\tfor_each_possible_hypervisor_cpuid_base(base) {\n\t\tentry = kvm_find_cpuid_entry(vcpu, base);\n\n\t\tif (entry) {\n\t\t\tu32 signature[3];\n\n\t\t\tsignature[0] = entry->ebx;\n\t\t\tsignature[1] = entry->ecx;\n\t\t\tsignature[2] = entry->edx;\n\n\t\t\tif (!memcmp(signature, sig, sizeof(signature))) {\n\t\t\t\tcpuid.base = base;\n\t\t\t\tcpuid.limit = entry->eax;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn cpuid;\n}\n\nstatic struct kvm_cpuid_entry2 *__kvm_find_kvm_cpuid_features(struct kvm_vcpu *vcpu,\n\t\t\t\t\t      struct kvm_cpuid_entry2 *entries, int nent)\n{\n\tu32 base = vcpu->arch.kvm_cpuid.base;\n\n\tif (!base)\n\t\treturn NULL;\n\n\treturn cpuid_entry2_find(entries, nent, base | KVM_CPUID_FEATURES,\n\t\t\t\t KVM_CPUID_INDEX_NOT_SIGNIFICANT);\n}\n\nstatic struct kvm_cpuid_entry2 *kvm_find_kvm_cpuid_features(struct kvm_vcpu *vcpu)\n{\n\treturn __kvm_find_kvm_cpuid_features(vcpu, vcpu->arch.cpuid_entries,\n\t\t\t\t\t     vcpu->arch.cpuid_nent);\n}\n\nvoid kvm_update_pv_runtime(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_cpuid_entry2 *best = kvm_find_kvm_cpuid_features(vcpu);\n\n\t \n\tif (best)\n\t\tvcpu->arch.pv_cpuid.features = best->eax;\n}\n\n \nstatic u64 cpuid_get_supported_xcr0(struct kvm_cpuid_entry2 *entries, int nent)\n{\n\tstruct kvm_cpuid_entry2 *best;\n\n\tbest = cpuid_entry2_find(entries, nent, 0xd, 0);\n\tif (!best)\n\t\treturn 0;\n\n\treturn (best->eax | ((u64)best->edx << 32)) & kvm_caps.supported_xcr0;\n}\n\nstatic void __kvm_update_cpuid_runtime(struct kvm_vcpu *vcpu, struct kvm_cpuid_entry2 *entries,\n\t\t\t\t       int nent)\n{\n\tstruct kvm_cpuid_entry2 *best;\n\n\tbest = cpuid_entry2_find(entries, nent, 1, KVM_CPUID_INDEX_NOT_SIGNIFICANT);\n\tif (best) {\n\t\t \n\t\tif (boot_cpu_has(X86_FEATURE_XSAVE))\n\t\t\tcpuid_entry_change(best, X86_FEATURE_OSXSAVE,\n\t\t\t\t\t   kvm_is_cr4_bit_set(vcpu, X86_CR4_OSXSAVE));\n\n\t\tcpuid_entry_change(best, X86_FEATURE_APIC,\n\t\t\t   vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE);\n\t}\n\n\tbest = cpuid_entry2_find(entries, nent, 7, 0);\n\tif (best && boot_cpu_has(X86_FEATURE_PKU) && best->function == 0x7)\n\t\tcpuid_entry_change(best, X86_FEATURE_OSPKE,\n\t\t\t\t   kvm_is_cr4_bit_set(vcpu, X86_CR4_PKE));\n\n\tbest = cpuid_entry2_find(entries, nent, 0xD, 0);\n\tif (best)\n\t\tbest->ebx = xstate_required_size(vcpu->arch.xcr0, false);\n\n\tbest = cpuid_entry2_find(entries, nent, 0xD, 1);\n\tif (best && (cpuid_entry_has(best, X86_FEATURE_XSAVES) ||\n\t\t     cpuid_entry_has(best, X86_FEATURE_XSAVEC)))\n\t\tbest->ebx = xstate_required_size(vcpu->arch.xcr0, true);\n\n\tbest = __kvm_find_kvm_cpuid_features(vcpu, entries, nent);\n\tif (kvm_hlt_in_guest(vcpu->kvm) && best &&\n\t\t(best->eax & (1 << KVM_FEATURE_PV_UNHALT)))\n\t\tbest->eax &= ~(1 << KVM_FEATURE_PV_UNHALT);\n\n\tif (!kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_MISC_ENABLE_NO_MWAIT)) {\n\t\tbest = cpuid_entry2_find(entries, nent, 0x1, KVM_CPUID_INDEX_NOT_SIGNIFICANT);\n\t\tif (best)\n\t\t\tcpuid_entry_change(best, X86_FEATURE_MWAIT,\n\t\t\t\t\t   vcpu->arch.ia32_misc_enable_msr &\n\t\t\t\t\t   MSR_IA32_MISC_ENABLE_MWAIT);\n\t}\n}\n\nvoid kvm_update_cpuid_runtime(struct kvm_vcpu *vcpu)\n{\n\t__kvm_update_cpuid_runtime(vcpu, vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent);\n}\nEXPORT_SYMBOL_GPL(kvm_update_cpuid_runtime);\n\nstatic bool kvm_cpuid_has_hyperv(struct kvm_cpuid_entry2 *entries, int nent)\n{\n\tstruct kvm_cpuid_entry2 *entry;\n\n\tentry = cpuid_entry2_find(entries, nent, HYPERV_CPUID_INTERFACE,\n\t\t\t\t  KVM_CPUID_INDEX_NOT_SIGNIFICANT);\n\treturn entry && entry->eax == HYPERV_CPUID_SIGNATURE_EAX;\n}\n\nstatic void kvm_vcpu_after_set_cpuid(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tstruct kvm_cpuid_entry2 *best;\n\tbool allow_gbpages;\n\n\tBUILD_BUG_ON(KVM_NR_GOVERNED_FEATURES > KVM_MAX_NR_GOVERNED_FEATURES);\n\tbitmap_zero(vcpu->arch.governed_features.enabled,\n\t\t    KVM_MAX_NR_GOVERNED_FEATURES);\n\n\t \n\tallow_gbpages = tdp_enabled ? boot_cpu_has(X86_FEATURE_GBPAGES) :\n\t\t\t\t      guest_cpuid_has(vcpu, X86_FEATURE_GBPAGES);\n\tif (allow_gbpages)\n\t\tkvm_governed_feature_set(vcpu, X86_FEATURE_GBPAGES);\n\n\tbest = kvm_find_cpuid_entry(vcpu, 1);\n\tif (best && apic) {\n\t\tif (cpuid_entry_has(best, X86_FEATURE_TSC_DEADLINE_TIMER))\n\t\t\tapic->lapic_timer.timer_mode_mask = 3 << 17;\n\t\telse\n\t\t\tapic->lapic_timer.timer_mode_mask = 1 << 17;\n\n\t\tkvm_apic_set_version(vcpu);\n\t}\n\n\tvcpu->arch.guest_supported_xcr0 =\n\t\tcpuid_get_supported_xcr0(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent);\n\n\tkvm_update_pv_runtime(vcpu);\n\n\tvcpu->arch.maxphyaddr = cpuid_query_maxphyaddr(vcpu);\n\tvcpu->arch.reserved_gpa_bits = kvm_vcpu_reserved_gpa_bits_raw(vcpu);\n\n\tkvm_pmu_refresh(vcpu);\n\tvcpu->arch.cr4_guest_rsvd_bits =\n\t    __cr4_reserved_bits(guest_cpuid_has, vcpu);\n\n\tkvm_hv_set_cpuid(vcpu, kvm_cpuid_has_hyperv(vcpu->arch.cpuid_entries,\n\t\t\t\t\t\t    vcpu->arch.cpuid_nent));\n\n\t \n\tstatic_call(kvm_x86_vcpu_after_set_cpuid)(vcpu);\n\n\t \n\tkvm_mmu_after_set_cpuid(vcpu);\n}\n\nint cpuid_query_maxphyaddr(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_cpuid_entry2 *best;\n\n\tbest = kvm_find_cpuid_entry(vcpu, 0x80000000);\n\tif (!best || best->eax < 0x80000008)\n\t\tgoto not_found;\n\tbest = kvm_find_cpuid_entry(vcpu, 0x80000008);\n\tif (best)\n\t\treturn best->eax & 0xff;\nnot_found:\n\treturn 36;\n}\n\n \nu64 kvm_vcpu_reserved_gpa_bits_raw(struct kvm_vcpu *vcpu)\n{\n\treturn rsvd_bits(cpuid_maxphyaddr(vcpu), 63);\n}\n\nstatic int kvm_set_cpuid(struct kvm_vcpu *vcpu, struct kvm_cpuid_entry2 *e2,\n                        int nent)\n{\n\tint r;\n\n\t__kvm_update_cpuid_runtime(vcpu, e2, nent);\n\n\t \n\tif (kvm_vcpu_has_run(vcpu)) {\n\t\tr = kvm_cpuid_check_equal(vcpu, e2, nent);\n\t\tif (r)\n\t\t\treturn r;\n\n\t\tkvfree(e2);\n\t\treturn 0;\n\t}\n\n\tif (kvm_cpuid_has_hyperv(e2, nent)) {\n\t\tr = kvm_hv_vcpu_init(vcpu);\n\t\tif (r)\n\t\t\treturn r;\n\t}\n\n\tr = kvm_check_cpuid(vcpu, e2, nent);\n\tif (r)\n\t\treturn r;\n\n\tkvfree(vcpu->arch.cpuid_entries);\n\tvcpu->arch.cpuid_entries = e2;\n\tvcpu->arch.cpuid_nent = nent;\n\n\tvcpu->arch.kvm_cpuid = kvm_get_hypervisor_cpuid(vcpu, KVM_SIGNATURE);\n\tvcpu->arch.xen.cpuid = kvm_get_hypervisor_cpuid(vcpu, XEN_SIGNATURE);\n\tkvm_vcpu_after_set_cpuid(vcpu);\n\n\treturn 0;\n}\n\n \nint kvm_vcpu_ioctl_set_cpuid(struct kvm_vcpu *vcpu,\n\t\t\t     struct kvm_cpuid *cpuid,\n\t\t\t     struct kvm_cpuid_entry __user *entries)\n{\n\tint r, i;\n\tstruct kvm_cpuid_entry *e = NULL;\n\tstruct kvm_cpuid_entry2 *e2 = NULL;\n\n\tif (cpuid->nent > KVM_MAX_CPUID_ENTRIES)\n\t\treturn -E2BIG;\n\n\tif (cpuid->nent) {\n\t\te = vmemdup_user(entries, array_size(sizeof(*e), cpuid->nent));\n\t\tif (IS_ERR(e))\n\t\t\treturn PTR_ERR(e);\n\n\t\te2 = kvmalloc_array(cpuid->nent, sizeof(*e2), GFP_KERNEL_ACCOUNT);\n\t\tif (!e2) {\n\t\t\tr = -ENOMEM;\n\t\t\tgoto out_free_cpuid;\n\t\t}\n\t}\n\tfor (i = 0; i < cpuid->nent; i++) {\n\t\te2[i].function = e[i].function;\n\t\te2[i].eax = e[i].eax;\n\t\te2[i].ebx = e[i].ebx;\n\t\te2[i].ecx = e[i].ecx;\n\t\te2[i].edx = e[i].edx;\n\t\te2[i].index = 0;\n\t\te2[i].flags = 0;\n\t\te2[i].padding[0] = 0;\n\t\te2[i].padding[1] = 0;\n\t\te2[i].padding[2] = 0;\n\t}\n\n\tr = kvm_set_cpuid(vcpu, e2, cpuid->nent);\n\tif (r)\n\t\tkvfree(e2);\n\nout_free_cpuid:\n\tkvfree(e);\n\n\treturn r;\n}\n\nint kvm_vcpu_ioctl_set_cpuid2(struct kvm_vcpu *vcpu,\n\t\t\t      struct kvm_cpuid2 *cpuid,\n\t\t\t      struct kvm_cpuid_entry2 __user *entries)\n{\n\tstruct kvm_cpuid_entry2 *e2 = NULL;\n\tint r;\n\n\tif (cpuid->nent > KVM_MAX_CPUID_ENTRIES)\n\t\treturn -E2BIG;\n\n\tif (cpuid->nent) {\n\t\te2 = vmemdup_user(entries, array_size(sizeof(*e2), cpuid->nent));\n\t\tif (IS_ERR(e2))\n\t\t\treturn PTR_ERR(e2);\n\t}\n\n\tr = kvm_set_cpuid(vcpu, e2, cpuid->nent);\n\tif (r)\n\t\tkvfree(e2);\n\n\treturn r;\n}\n\nint kvm_vcpu_ioctl_get_cpuid2(struct kvm_vcpu *vcpu,\n\t\t\t      struct kvm_cpuid2 *cpuid,\n\t\t\t      struct kvm_cpuid_entry2 __user *entries)\n{\n\tif (cpuid->nent < vcpu->arch.cpuid_nent)\n\t\treturn -E2BIG;\n\n\tif (copy_to_user(entries, vcpu->arch.cpuid_entries,\n\t\t\t vcpu->arch.cpuid_nent * sizeof(struct kvm_cpuid_entry2)))\n\t\treturn -EFAULT;\n\n\tcpuid->nent = vcpu->arch.cpuid_nent;\n\treturn 0;\n}\n\n \nstatic __always_inline void __kvm_cpu_cap_mask(unsigned int leaf)\n{\n\tconst struct cpuid_reg cpuid = x86_feature_cpuid(leaf * 32);\n\tstruct kvm_cpuid_entry2 entry;\n\n\treverse_cpuid_check(leaf);\n\n\tcpuid_count(cpuid.function, cpuid.index,\n\t\t    &entry.eax, &entry.ebx, &entry.ecx, &entry.edx);\n\n\tkvm_cpu_caps[leaf] &= *__cpuid_entry_get_reg(&entry, cpuid.reg);\n}\n\nstatic __always_inline\nvoid kvm_cpu_cap_init_kvm_defined(enum kvm_only_cpuid_leafs leaf, u32 mask)\n{\n\t \n\tBUILD_BUG_ON(leaf < NCAPINTS);\n\n\tkvm_cpu_caps[leaf] = mask;\n\n\t__kvm_cpu_cap_mask(leaf);\n}\n\nstatic __always_inline void kvm_cpu_cap_mask(enum cpuid_leafs leaf, u32 mask)\n{\n\t \n\tBUILD_BUG_ON(leaf >= NCAPINTS);\n\n\tkvm_cpu_caps[leaf] &= mask;\n\n\t__kvm_cpu_cap_mask(leaf);\n}\n\nvoid kvm_set_cpu_caps(void)\n{\n#ifdef CONFIG_X86_64\n\tunsigned int f_gbpages = F(GBPAGES);\n\tunsigned int f_lm = F(LM);\n\tunsigned int f_xfd = F(XFD);\n#else\n\tunsigned int f_gbpages = 0;\n\tunsigned int f_lm = 0;\n\tunsigned int f_xfd = 0;\n#endif\n\tmemset(kvm_cpu_caps, 0, sizeof(kvm_cpu_caps));\n\n\tBUILD_BUG_ON(sizeof(kvm_cpu_caps) - (NKVMCAPINTS * sizeof(*kvm_cpu_caps)) >\n\t\t     sizeof(boot_cpu_data.x86_capability));\n\n\tmemcpy(&kvm_cpu_caps, &boot_cpu_data.x86_capability,\n\t       sizeof(kvm_cpu_caps) - (NKVMCAPINTS * sizeof(*kvm_cpu_caps)));\n\n\tkvm_cpu_cap_mask(CPUID_1_ECX,\n\t\t \n\t\tF(XMM3) | F(PCLMULQDQ) | 0   |\n\t\t0   |\n\t\t0   | F(SSSE3) | 0   | 0   |\n\t\tF(FMA) | F(CX16) | 0   | F(PDCM) |\n\t\tF(PCID) | 0   | F(XMM4_1) |\n\t\tF(XMM4_2) | F(X2APIC) | F(MOVBE) | F(POPCNT) |\n\t\t0   | F(AES) | F(XSAVE) | 0   | F(AVX) |\n\t\tF(F16C) | F(RDRAND)\n\t);\n\t \n\tkvm_cpu_cap_set(X86_FEATURE_X2APIC);\n\n\tkvm_cpu_cap_mask(CPUID_1_EDX,\n\t\tF(FPU) | F(VME) | F(DE) | F(PSE) |\n\t\tF(TSC) | F(MSR) | F(PAE) | F(MCE) |\n\t\tF(CX8) | F(APIC) | 0   | F(SEP) |\n\t\tF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\n\t\tF(PAT) | F(PSE36) | 0   | F(CLFLUSH) |\n\t\t0   | F(MMX) |\n\t\tF(FXSR) | F(XMM) | F(XMM2) | F(SELFSNOOP) |\n\t\t0  \n\t);\n\n\tkvm_cpu_cap_mask(CPUID_7_0_EBX,\n\t\tF(FSGSBASE) | F(SGX) | F(BMI1) | F(HLE) | F(AVX2) |\n\t\tF(FDP_EXCPTN_ONLY) | F(SMEP) | F(BMI2) | F(ERMS) | F(INVPCID) |\n\t\tF(RTM) | F(ZERO_FCS_FDS) | 0   | F(AVX512F) |\n\t\tF(AVX512DQ) | F(RDSEED) | F(ADX) | F(SMAP) | F(AVX512IFMA) |\n\t\tF(CLFLUSHOPT) | F(CLWB) | 0   | F(AVX512PF) |\n\t\tF(AVX512ER) | F(AVX512CD) | F(SHA_NI) | F(AVX512BW) |\n\t\tF(AVX512VL));\n\n\tkvm_cpu_cap_mask(CPUID_7_ECX,\n\t\tF(AVX512VBMI) | F(LA57) | F(PKU) | 0   | F(RDPID) |\n\t\tF(AVX512_VPOPCNTDQ) | F(UMIP) | F(AVX512_VBMI2) | F(GFNI) |\n\t\tF(VAES) | F(VPCLMULQDQ) | F(AVX512_VNNI) | F(AVX512_BITALG) |\n\t\tF(CLDEMOTE) | F(MOVDIRI) | F(MOVDIR64B) | 0   |\n\t\tF(SGX_LC) | F(BUS_LOCK_DETECT)\n\t);\n\t \n\tif (cpuid_ecx(7) & F(LA57))\n\t\tkvm_cpu_cap_set(X86_FEATURE_LA57);\n\n\t \n\tif (!tdp_enabled || !boot_cpu_has(X86_FEATURE_OSPKE))\n\t\tkvm_cpu_cap_clear(X86_FEATURE_PKU);\n\n\tkvm_cpu_cap_mask(CPUID_7_EDX,\n\t\tF(AVX512_4VNNIW) | F(AVX512_4FMAPS) | F(SPEC_CTRL) |\n\t\tF(SPEC_CTRL_SSBD) | F(ARCH_CAPABILITIES) | F(INTEL_STIBP) |\n\t\tF(MD_CLEAR) | F(AVX512_VP2INTERSECT) | F(FSRM) |\n\t\tF(SERIALIZE) | F(TSXLDTRK) | F(AVX512_FP16) |\n\t\tF(AMX_TILE) | F(AMX_INT8) | F(AMX_BF16) | F(FLUSH_L1D)\n\t);\n\n\t \n\tkvm_cpu_cap_set(X86_FEATURE_TSC_ADJUST);\n\tkvm_cpu_cap_set(X86_FEATURE_ARCH_CAPABILITIES);\n\n\tif (boot_cpu_has(X86_FEATURE_IBPB) && boot_cpu_has(X86_FEATURE_IBRS))\n\t\tkvm_cpu_cap_set(X86_FEATURE_SPEC_CTRL);\n\tif (boot_cpu_has(X86_FEATURE_STIBP))\n\t\tkvm_cpu_cap_set(X86_FEATURE_INTEL_STIBP);\n\tif (boot_cpu_has(X86_FEATURE_AMD_SSBD))\n\t\tkvm_cpu_cap_set(X86_FEATURE_SPEC_CTRL_SSBD);\n\n\tkvm_cpu_cap_mask(CPUID_7_1_EAX,\n\t\tF(AVX_VNNI) | F(AVX512_BF16) | F(CMPCCXADD) |\n\t\tF(FZRM) | F(FSRS) | F(FSRC) |\n\t\tF(AMX_FP16) | F(AVX_IFMA)\n\t);\n\n\tkvm_cpu_cap_init_kvm_defined(CPUID_7_1_EDX,\n\t\tF(AVX_VNNI_INT8) | F(AVX_NE_CONVERT) | F(PREFETCHITI) |\n\t\tF(AMX_COMPLEX)\n\t);\n\n\tkvm_cpu_cap_mask(CPUID_D_1_EAX,\n\t\tF(XSAVEOPT) | F(XSAVEC) | F(XGETBV1) | F(XSAVES) | f_xfd\n\t);\n\n\tkvm_cpu_cap_init_kvm_defined(CPUID_12_EAX,\n\t\tSF(SGX1) | SF(SGX2) | SF(SGX_EDECCSSA)\n\t);\n\n\tkvm_cpu_cap_mask(CPUID_8000_0001_ECX,\n\t\tF(LAHF_LM) | F(CMP_LEGACY) | 0   | 0   |\n\t\tF(CR8_LEGACY) | F(ABM) | F(SSE4A) | F(MISALIGNSSE) |\n\t\tF(3DNOWPREFETCH) | F(OSVW) | 0   | F(XOP) |\n\t\t0   | F(FMA4) | F(TBM) |\n\t\tF(TOPOEXT) | 0  \n\t);\n\n\tkvm_cpu_cap_mask(CPUID_8000_0001_EDX,\n\t\tF(FPU) | F(VME) | F(DE) | F(PSE) |\n\t\tF(TSC) | F(MSR) | F(PAE) | F(MCE) |\n\t\tF(CX8) | F(APIC) | 0   | F(SYSCALL) |\n\t\tF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\n\t\tF(PAT) | F(PSE36) | 0   |\n\t\tF(NX) | 0   | F(MMXEXT) | F(MMX) |\n\t\tF(FXSR) | F(FXSR_OPT) | f_gbpages | F(RDTSCP) |\n\t\t0   | f_lm | F(3DNOWEXT) | F(3DNOW)\n\t);\n\n\tif (!tdp_enabled && IS_ENABLED(CONFIG_X86_64))\n\t\tkvm_cpu_cap_set(X86_FEATURE_GBPAGES);\n\n\tkvm_cpu_cap_init_kvm_defined(CPUID_8000_0007_EDX,\n\t\tSF(CONSTANT_TSC)\n\t);\n\n\tkvm_cpu_cap_mask(CPUID_8000_0008_EBX,\n\t\tF(CLZERO) | F(XSAVEERPTR) |\n\t\tF(WBNOINVD) | F(AMD_IBPB) | F(AMD_IBRS) | F(AMD_SSBD) | F(VIRT_SSBD) |\n\t\tF(AMD_SSB_NO) | F(AMD_STIBP) | F(AMD_STIBP_ALWAYS_ON) |\n\t\tF(AMD_PSFD)\n\t);\n\n\t \n\tif (boot_cpu_has(X86_FEATURE_IBPB))\n\t\tkvm_cpu_cap_set(X86_FEATURE_AMD_IBPB);\n\tif (boot_cpu_has(X86_FEATURE_IBRS))\n\t\tkvm_cpu_cap_set(X86_FEATURE_AMD_IBRS);\n\tif (boot_cpu_has(X86_FEATURE_STIBP))\n\t\tkvm_cpu_cap_set(X86_FEATURE_AMD_STIBP);\n\tif (boot_cpu_has(X86_FEATURE_SPEC_CTRL_SSBD))\n\t\tkvm_cpu_cap_set(X86_FEATURE_AMD_SSBD);\n\tif (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))\n\t\tkvm_cpu_cap_set(X86_FEATURE_AMD_SSB_NO);\n\t \n\tif (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD) &&\n\t    !boot_cpu_has(X86_FEATURE_AMD_SSBD))\n\t\tkvm_cpu_cap_set(X86_FEATURE_VIRT_SSBD);\n\n\t \n\tkvm_cpu_cap_mask(CPUID_8000_000A_EDX, 0);\n\n\tkvm_cpu_cap_mask(CPUID_8000_001F_EAX,\n\t\t0   | F(SEV) | 0   | F(SEV_ES) |\n\t\tF(SME_COHERENT));\n\n\tkvm_cpu_cap_mask(CPUID_8000_0021_EAX,\n\t\tF(NO_NESTED_DATA_BP) | F(LFENCE_RDTSC) | 0   |\n\t\tF(NULL_SEL_CLR_BASE) | F(AUTOIBRS) | 0  \n\t);\n\n\tif (cpu_feature_enabled(X86_FEATURE_SRSO_NO))\n\t\tkvm_cpu_cap_set(X86_FEATURE_SRSO_NO);\n\n\tkvm_cpu_cap_init_kvm_defined(CPUID_8000_0022_EAX,\n\t\tF(PERFMON_V2)\n\t);\n\n\t \n\tif (cpu_feature_enabled(X86_FEATURE_LFENCE_RDTSC))\n\t\tkvm_cpu_cap_set(X86_FEATURE_LFENCE_RDTSC);\n\tif (!static_cpu_has_bug(X86_BUG_NULL_SEG))\n\t\tkvm_cpu_cap_set(X86_FEATURE_NULL_SEL_CLR_BASE);\n\tkvm_cpu_cap_set(X86_FEATURE_NO_SMM_CTL_MSR);\n\n\tkvm_cpu_cap_mask(CPUID_C000_0001_EDX,\n\t\tF(XSTORE) | F(XSTORE_EN) | F(XCRYPT) | F(XCRYPT_EN) |\n\t\tF(ACE2) | F(ACE2_EN) | F(PHE) | F(PHE_EN) |\n\t\tF(PMM) | F(PMM_EN)\n\t);\n\n\t \n\tif (WARN_ON((kvm_cpu_cap_has(X86_FEATURE_RDTSCP) ||\n\t\t     kvm_cpu_cap_has(X86_FEATURE_RDPID)) &&\n\t\t     !kvm_is_supported_user_return_msr(MSR_TSC_AUX))) {\n\t\tkvm_cpu_cap_clear(X86_FEATURE_RDTSCP);\n\t\tkvm_cpu_cap_clear(X86_FEATURE_RDPID);\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_set_cpu_caps);\n\nstruct kvm_cpuid_array {\n\tstruct kvm_cpuid_entry2 *entries;\n\tint maxnent;\n\tint nent;\n};\n\nstatic struct kvm_cpuid_entry2 *get_next_cpuid(struct kvm_cpuid_array *array)\n{\n\tif (array->nent >= array->maxnent)\n\t\treturn NULL;\n\n\treturn &array->entries[array->nent++];\n}\n\nstatic struct kvm_cpuid_entry2 *do_host_cpuid(struct kvm_cpuid_array *array,\n\t\t\t\t\t      u32 function, u32 index)\n{\n\tstruct kvm_cpuid_entry2 *entry = get_next_cpuid(array);\n\n\tif (!entry)\n\t\treturn NULL;\n\n\tmemset(entry, 0, sizeof(*entry));\n\tentry->function = function;\n\tentry->index = index;\n\tswitch (function & 0xC0000000) {\n\tcase 0x40000000:\n\t\t \n\t\treturn entry;\n\n\tcase 0x80000000:\n\t\t \n\t\t{\n\t\t\tstatic int max_cpuid_80000000;\n\t\t\tif (!READ_ONCE(max_cpuid_80000000))\n\t\t\t\tWRITE_ONCE(max_cpuid_80000000, cpuid_eax(0x80000000));\n\t\t\tif (function > READ_ONCE(max_cpuid_80000000))\n\t\t\t\treturn entry;\n\t\t}\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\tcpuid_count(entry->function, entry->index,\n\t\t    &entry->eax, &entry->ebx, &entry->ecx, &entry->edx);\n\n\tif (cpuid_function_is_indexed(function))\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\n\treturn entry;\n}\n\nstatic int __do_cpuid_func_emulated(struct kvm_cpuid_array *array, u32 func)\n{\n\tstruct kvm_cpuid_entry2 *entry;\n\n\tif (array->nent >= array->maxnent)\n\t\treturn -E2BIG;\n\n\tentry = &array->entries[array->nent];\n\tentry->function = func;\n\tentry->index = 0;\n\tentry->flags = 0;\n\n\tswitch (func) {\n\tcase 0:\n\t\tentry->eax = 7;\n\t\t++array->nent;\n\t\tbreak;\n\tcase 1:\n\t\tentry->ecx = F(MOVBE);\n\t\t++array->nent;\n\t\tbreak;\n\tcase 7:\n\t\tentry->flags |= KVM_CPUID_FLAG_SIGNIFCANT_INDEX;\n\t\tentry->eax = 0;\n\t\tif (kvm_cpu_cap_has(X86_FEATURE_RDTSCP))\n\t\t\tentry->ecx = F(RDPID);\n\t\t++array->nent;\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\nstatic inline int __do_cpuid_func(struct kvm_cpuid_array *array, u32 function)\n{\n\tstruct kvm_cpuid_entry2 *entry;\n\tint r, i, max_idx;\n\n\t \n\tget_cpu();\n\n\tr = -E2BIG;\n\n\tentry = do_host_cpuid(array, function, 0);\n\tif (!entry)\n\t\tgoto out;\n\n\tswitch (function) {\n\tcase 0:\n\t\t \n\t\tentry->eax = min(entry->eax, 0x1fU);\n\t\tbreak;\n\tcase 1:\n\t\tcpuid_entry_override(entry, CPUID_1_EDX);\n\t\tcpuid_entry_override(entry, CPUID_1_ECX);\n\t\tbreak;\n\tcase 2:\n\t\t \n\t\tWARN_ON_ONCE((entry->eax & 0xff) > 1);\n\t\tbreak;\n\t \n\tcase 4:\n\tcase 0x8000001d:\n\t\t \n\t\tfor (i = 1; entry->eax & 0x1f; ++i) {\n\t\t\tentry = do_host_cpuid(array, function, i);\n\t\t\tif (!entry)\n\t\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase 6:  \n\t\tentry->eax = 0x4;  \n\t\tentry->ebx = 0;\n\t\tentry->ecx = 0;\n\t\tentry->edx = 0;\n\t\tbreak;\n\t \n\tcase 7:\n\t\tentry->eax = min(entry->eax, 1u);\n\t\tcpuid_entry_override(entry, CPUID_7_0_EBX);\n\t\tcpuid_entry_override(entry, CPUID_7_ECX);\n\t\tcpuid_entry_override(entry, CPUID_7_EDX);\n\n\t\t \n\t\tif (entry->eax == 1) {\n\t\t\tentry = do_host_cpuid(array, function, 1);\n\t\t\tif (!entry)\n\t\t\t\tgoto out;\n\n\t\t\tcpuid_entry_override(entry, CPUID_7_1_EAX);\n\t\t\tcpuid_entry_override(entry, CPUID_7_1_EDX);\n\t\t\tentry->ebx = 0;\n\t\t\tentry->ecx = 0;\n\t\t}\n\t\tbreak;\n\tcase 0xa: {  \n\t\tunion cpuid10_eax eax;\n\t\tunion cpuid10_edx edx;\n\n\t\tif (!enable_pmu || !static_cpu_has(X86_FEATURE_ARCH_PERFMON)) {\n\t\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\teax.split.version_id = kvm_pmu_cap.version;\n\t\teax.split.num_counters = kvm_pmu_cap.num_counters_gp;\n\t\teax.split.bit_width = kvm_pmu_cap.bit_width_gp;\n\t\teax.split.mask_length = kvm_pmu_cap.events_mask_len;\n\t\tedx.split.num_counters_fixed = kvm_pmu_cap.num_counters_fixed;\n\t\tedx.split.bit_width_fixed = kvm_pmu_cap.bit_width_fixed;\n\n\t\tif (kvm_pmu_cap.version)\n\t\t\tedx.split.anythread_deprecated = 1;\n\t\tedx.split.reserved1 = 0;\n\t\tedx.split.reserved2 = 0;\n\n\t\tentry->eax = eax.full;\n\t\tentry->ebx = kvm_pmu_cap.events_mask;\n\t\tentry->ecx = 0;\n\t\tentry->edx = edx.full;\n\t\tbreak;\n\t}\n\tcase 0x1f:\n\tcase 0xb:\n\t\t \n\t\tentry->eax = entry->ebx = entry->ecx = 0;\n\t\tbreak;\n\tcase 0xd: {\n\t\tu64 permitted_xcr0 = kvm_get_filtered_xcr0();\n\t\tu64 permitted_xss = kvm_caps.supported_xss;\n\n\t\tentry->eax &= permitted_xcr0;\n\t\tentry->ebx = xstate_required_size(permitted_xcr0, false);\n\t\tentry->ecx = entry->ebx;\n\t\tentry->edx &= permitted_xcr0 >> 32;\n\t\tif (!permitted_xcr0)\n\t\t\tbreak;\n\n\t\tentry = do_host_cpuid(array, function, 1);\n\t\tif (!entry)\n\t\t\tgoto out;\n\n\t\tcpuid_entry_override(entry, CPUID_D_1_EAX);\n\t\tif (entry->eax & (F(XSAVES)|F(XSAVEC)))\n\t\t\tentry->ebx = xstate_required_size(permitted_xcr0 | permitted_xss,\n\t\t\t\t\t\t\t  true);\n\t\telse {\n\t\t\tWARN_ON_ONCE(permitted_xss != 0);\n\t\t\tentry->ebx = 0;\n\t\t}\n\t\tentry->ecx &= permitted_xss;\n\t\tentry->edx &= permitted_xss >> 32;\n\n\t\tfor (i = 2; i < 64; ++i) {\n\t\t\tbool s_state;\n\t\t\tif (permitted_xcr0 & BIT_ULL(i))\n\t\t\t\ts_state = false;\n\t\t\telse if (permitted_xss & BIT_ULL(i))\n\t\t\t\ts_state = true;\n\t\t\telse\n\t\t\t\tcontinue;\n\n\t\t\tentry = do_host_cpuid(array, function, i);\n\t\t\tif (!entry)\n\t\t\t\tgoto out;\n\n\t\t\t \n\t\t\tif (WARN_ON_ONCE(!entry->eax || (entry->ecx & 0x1) != s_state)) {\n\t\t\t\t--array->nent;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!kvm_cpu_cap_has(X86_FEATURE_XFD))\n\t\t\t\tentry->ecx &= ~BIT_ULL(2);\n\t\t\tentry->edx = 0;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 0x12:\n\t\t \n\t\tif (!kvm_cpu_cap_has(X86_FEATURE_SGX)) {\n\t\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tcpuid_entry_override(entry, CPUID_12_EAX);\n\t\tentry->ebx &= SGX_MISC_EXINFO;\n\n\t\tentry = do_host_cpuid(array, function, 1);\n\t\tif (!entry)\n\t\t\tgoto out;\n\n\t\t \n\t\tentry->eax &= SGX_ATTR_PRIV_MASK | SGX_ATTR_UNPRIV_MASK;\n\t\tentry->ebx &= 0;\n\t\tbreak;\n\t \n\tcase 0x14:\n\t\tif (!kvm_cpu_cap_has(X86_FEATURE_INTEL_PT)) {\n\t\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor (i = 1, max_idx = entry->eax; i <= max_idx; ++i) {\n\t\t\tif (!do_host_cpuid(array, function, i))\n\t\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\t \n\tcase 0x1d:\n\t\tif (!kvm_cpu_cap_has(X86_FEATURE_AMX_TILE)) {\n\t\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor (i = 1, max_idx = entry->eax; i <= max_idx; ++i) {\n\t\t\tif (!do_host_cpuid(array, function, i))\n\t\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase 0x1e:  \n\t\tif (!kvm_cpu_cap_has(X86_FEATURE_AMX_TILE)) {\n\t\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase KVM_CPUID_SIGNATURE: {\n\t\tconst u32 *sigptr = (const u32 *)KVM_SIGNATURE;\n\t\tentry->eax = KVM_CPUID_FEATURES;\n\t\tentry->ebx = sigptr[0];\n\t\tentry->ecx = sigptr[1];\n\t\tentry->edx = sigptr[2];\n\t\tbreak;\n\t}\n\tcase KVM_CPUID_FEATURES:\n\t\tentry->eax = (1 << KVM_FEATURE_CLOCKSOURCE) |\n\t\t\t     (1 << KVM_FEATURE_NOP_IO_DELAY) |\n\t\t\t     (1 << KVM_FEATURE_CLOCKSOURCE2) |\n\t\t\t     (1 << KVM_FEATURE_ASYNC_PF) |\n\t\t\t     (1 << KVM_FEATURE_PV_EOI) |\n\t\t\t     (1 << KVM_FEATURE_CLOCKSOURCE_STABLE_BIT) |\n\t\t\t     (1 << KVM_FEATURE_PV_UNHALT) |\n\t\t\t     (1 << KVM_FEATURE_PV_TLB_FLUSH) |\n\t\t\t     (1 << KVM_FEATURE_ASYNC_PF_VMEXIT) |\n\t\t\t     (1 << KVM_FEATURE_PV_SEND_IPI) |\n\t\t\t     (1 << KVM_FEATURE_POLL_CONTROL) |\n\t\t\t     (1 << KVM_FEATURE_PV_SCHED_YIELD) |\n\t\t\t     (1 << KVM_FEATURE_ASYNC_PF_INT);\n\n\t\tif (sched_info_on())\n\t\t\tentry->eax |= (1 << KVM_FEATURE_STEAL_TIME);\n\n\t\tentry->ebx = 0;\n\t\tentry->ecx = 0;\n\t\tentry->edx = 0;\n\t\tbreak;\n\tcase 0x80000000:\n\t\tentry->eax = min(entry->eax, 0x80000022);\n\t\t \n\t\tif (entry->eax >= 0x8000001d &&\n\t\t    (static_cpu_has(X86_FEATURE_LFENCE_RDTSC)\n\t\t     || !static_cpu_has_bug(X86_BUG_NULL_SEG)))\n\t\t\tentry->eax = max(entry->eax, 0x80000021);\n\t\tbreak;\n\tcase 0x80000001:\n\t\tentry->ebx &= ~GENMASK(27, 16);\n\t\tcpuid_entry_override(entry, CPUID_8000_0001_EDX);\n\t\tcpuid_entry_override(entry, CPUID_8000_0001_ECX);\n\t\tbreak;\n\tcase 0x80000005:\n\t\t \n\t\tbreak;\n\tcase 0x80000006:\n\t\t \n\t\tentry->edx &= ~GENMASK(17, 16);\n\t\tbreak;\n\tcase 0x80000007:  \n\t\tcpuid_entry_override(entry, CPUID_8000_0007_EDX);\n\n\t\t \n\t\tentry->edx &= boot_cpu_data.x86_power;\n\t\tentry->eax = entry->ebx = entry->ecx = 0;\n\t\tbreak;\n\tcase 0x80000008: {\n\t\tunsigned g_phys_as = (entry->eax >> 16) & 0xff;\n\t\tunsigned virt_as = max((entry->eax >> 8) & 0xff, 48U);\n\t\tunsigned phys_as = entry->eax & 0xff;\n\n\t\t \n\t\tif (!tdp_enabled)\n\t\t\tg_phys_as = boot_cpu_data.x86_phys_bits;\n\t\telse if (!g_phys_as)\n\t\t\tg_phys_as = phys_as;\n\n\t\tentry->eax = g_phys_as | (virt_as << 8);\n\t\tentry->ecx &= ~(GENMASK(31, 16) | GENMASK(11, 8));\n\t\tentry->edx = 0;\n\t\tcpuid_entry_override(entry, CPUID_8000_0008_EBX);\n\t\tbreak;\n\t}\n\tcase 0x8000000A:\n\t\tif (!kvm_cpu_cap_has(X86_FEATURE_SVM)) {\n\t\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\t\tbreak;\n\t\t}\n\t\tentry->eax = 1;  \n\t\tentry->ebx = 8;  \n\t\tentry->ecx = 0;  \n\t\tcpuid_entry_override(entry, CPUID_8000_000A_EDX);\n\t\tbreak;\n\tcase 0x80000019:\n\t\tentry->ecx = entry->edx = 0;\n\t\tbreak;\n\tcase 0x8000001a:\n\t\tentry->eax &= GENMASK(2, 0);\n\t\tentry->ebx = entry->ecx = entry->edx = 0;\n\t\tbreak;\n\tcase 0x8000001e:\n\t\t \n\t\tentry->eax = entry->ebx = entry->ecx = 0;\n\t\tentry->edx = 0;  \n\t\tbreak;\n\tcase 0x8000001F:\n\t\tif (!kvm_cpu_cap_has(X86_FEATURE_SEV)) {\n\t\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\t} else {\n\t\t\tcpuid_entry_override(entry, CPUID_8000_001F_EAX);\n\t\t\t \n\t\t\tentry->ebx &= ~GENMASK(31, 12);\n\t\t\t \n\t\t\tentry->ebx &= ~GENMASK(11, 6);\n\t\t}\n\t\tbreak;\n\tcase 0x80000020:\n\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\tbreak;\n\tcase 0x80000021:\n\t\tentry->ebx = entry->ecx = entry->edx = 0;\n\t\tcpuid_entry_override(entry, CPUID_8000_0021_EAX);\n\t\tbreak;\n\t \n\tcase 0x80000022: {\n\t\tunion cpuid_0x80000022_ebx ebx;\n\n\t\tentry->ecx = entry->edx = 0;\n\t\tif (!enable_pmu || !kvm_cpu_cap_has(X86_FEATURE_PERFMON_V2)) {\n\t\t\tentry->eax = entry->ebx;\n\t\t\tbreak;\n\t\t}\n\n\t\tcpuid_entry_override(entry, CPUID_8000_0022_EAX);\n\n\t\tif (kvm_cpu_cap_has(X86_FEATURE_PERFMON_V2))\n\t\t\tebx.split.num_core_pmc = kvm_pmu_cap.num_counters_gp;\n\t\telse if (kvm_cpu_cap_has(X86_FEATURE_PERFCTR_CORE))\n\t\t\tebx.split.num_core_pmc = AMD64_NUM_COUNTERS_CORE;\n\t\telse\n\t\t\tebx.split.num_core_pmc = AMD64_NUM_COUNTERS;\n\n\t\tentry->ebx = ebx.full;\n\t\tbreak;\n\t}\n\t \n\tcase 0xC0000000:\n\t\t \n\t\tentry->eax = min(entry->eax, 0xC0000004);\n\t\tbreak;\n\tcase 0xC0000001:\n\t\tcpuid_entry_override(entry, CPUID_C000_0001_EDX);\n\t\tbreak;\n\tcase 3:  \n\tcase 5:  \n\tcase 0xC0000002:\n\tcase 0xC0000003:\n\tcase 0xC0000004:\n\tdefault:\n\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\tbreak;\n\t}\n\n\tr = 0;\n\nout:\n\tput_cpu();\n\n\treturn r;\n}\n\nstatic int do_cpuid_func(struct kvm_cpuid_array *array, u32 func,\n\t\t\t unsigned int type)\n{\n\tif (type == KVM_GET_EMULATED_CPUID)\n\t\treturn __do_cpuid_func_emulated(array, func);\n\n\treturn __do_cpuid_func(array, func);\n}\n\n#define CENTAUR_CPUID_SIGNATURE 0xC0000000\n\nstatic int get_cpuid_func(struct kvm_cpuid_array *array, u32 func,\n\t\t\t  unsigned int type)\n{\n\tu32 limit;\n\tint r;\n\n\tif (func == CENTAUR_CPUID_SIGNATURE &&\n\t    boot_cpu_data.x86_vendor != X86_VENDOR_CENTAUR)\n\t\treturn 0;\n\n\tr = do_cpuid_func(array, func, type);\n\tif (r)\n\t\treturn r;\n\n\tlimit = array->entries[array->nent - 1].eax;\n\tfor (func = func + 1; func <= limit; ++func) {\n\t\tr = do_cpuid_func(array, func, type);\n\t\tif (r)\n\t\t\tbreak;\n\t}\n\n\treturn r;\n}\n\nstatic bool sanity_check_entries(struct kvm_cpuid_entry2 __user *entries,\n\t\t\t\t __u32 num_entries, unsigned int ioctl_type)\n{\n\tint i;\n\t__u32 pad[3];\n\n\tif (ioctl_type != KVM_GET_EMULATED_CPUID)\n\t\treturn false;\n\n\t \n\tfor (i = 0; i < num_entries; i++) {\n\t\tif (copy_from_user(pad, entries[i].padding, sizeof(pad)))\n\t\t\treturn true;\n\n\t\tif (pad[0] || pad[1] || pad[2])\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nint kvm_dev_ioctl_get_cpuid(struct kvm_cpuid2 *cpuid,\n\t\t\t    struct kvm_cpuid_entry2 __user *entries,\n\t\t\t    unsigned int type)\n{\n\tstatic const u32 funcs[] = {\n\t\t0, 0x80000000, CENTAUR_CPUID_SIGNATURE, KVM_CPUID_SIGNATURE,\n\t};\n\n\tstruct kvm_cpuid_array array = {\n\t\t.nent = 0,\n\t};\n\tint r, i;\n\n\tif (cpuid->nent < 1)\n\t\treturn -E2BIG;\n\tif (cpuid->nent > KVM_MAX_CPUID_ENTRIES)\n\t\tcpuid->nent = KVM_MAX_CPUID_ENTRIES;\n\n\tif (sanity_check_entries(entries, cpuid->nent, type))\n\t\treturn -EINVAL;\n\n\tarray.entries = kvcalloc(cpuid->nent, sizeof(struct kvm_cpuid_entry2), GFP_KERNEL);\n\tif (!array.entries)\n\t\treturn -ENOMEM;\n\n\tarray.maxnent = cpuid->nent;\n\n\tfor (i = 0; i < ARRAY_SIZE(funcs); i++) {\n\t\tr = get_cpuid_func(&array, funcs[i], type);\n\t\tif (r)\n\t\t\tgoto out_free;\n\t}\n\tcpuid->nent = array.nent;\n\n\tif (copy_to_user(entries, array.entries,\n\t\t\t array.nent * sizeof(struct kvm_cpuid_entry2)))\n\t\tr = -EFAULT;\n\nout_free:\n\tkvfree(array.entries);\n\treturn r;\n}\n\nstruct kvm_cpuid_entry2 *kvm_find_cpuid_entry_index(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t    u32 function, u32 index)\n{\n\treturn cpuid_entry2_find(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent,\n\t\t\t\t function, index);\n}\nEXPORT_SYMBOL_GPL(kvm_find_cpuid_entry_index);\n\nstruct kvm_cpuid_entry2 *kvm_find_cpuid_entry(struct kvm_vcpu *vcpu,\n\t\t\t\t\t      u32 function)\n{\n\treturn cpuid_entry2_find(vcpu->arch.cpuid_entries, vcpu->arch.cpuid_nent,\n\t\t\t\t function, KVM_CPUID_INDEX_NOT_SIGNIFICANT);\n}\nEXPORT_SYMBOL_GPL(kvm_find_cpuid_entry);\n\n \nstatic struct kvm_cpuid_entry2 *\nget_out_of_range_cpuid_entry(struct kvm_vcpu *vcpu, u32 *fn_ptr, u32 index)\n{\n\tstruct kvm_cpuid_entry2 *basic, *class;\n\tu32 function = *fn_ptr;\n\n\tbasic = kvm_find_cpuid_entry(vcpu, 0);\n\tif (!basic)\n\t\treturn NULL;\n\n\tif (is_guest_vendor_amd(basic->ebx, basic->ecx, basic->edx) ||\n\t    is_guest_vendor_hygon(basic->ebx, basic->ecx, basic->edx))\n\t\treturn NULL;\n\n\tif (function >= 0x40000000 && function <= 0x4fffffff)\n\t\tclass = kvm_find_cpuid_entry(vcpu, function & 0xffffff00);\n\telse if (function >= 0xc0000000)\n\t\tclass = kvm_find_cpuid_entry(vcpu, 0xc0000000);\n\telse\n\t\tclass = kvm_find_cpuid_entry(vcpu, function & 0x80000000);\n\n\tif (class && function <= class->eax)\n\t\treturn NULL;\n\n\t \n\t*fn_ptr = basic->eax;\n\n\t \n\treturn kvm_find_cpuid_entry_index(vcpu, basic->eax, index);\n}\n\nbool kvm_cpuid(struct kvm_vcpu *vcpu, u32 *eax, u32 *ebx,\n\t       u32 *ecx, u32 *edx, bool exact_only)\n{\n\tu32 orig_function = *eax, function = *eax, index = *ecx;\n\tstruct kvm_cpuid_entry2 *entry;\n\tbool exact, used_max_basic = false;\n\n\tentry = kvm_find_cpuid_entry_index(vcpu, function, index);\n\texact = !!entry;\n\n\tif (!entry && !exact_only) {\n\t\tentry = get_out_of_range_cpuid_entry(vcpu, &function, index);\n\t\tused_max_basic = !!entry;\n\t}\n\n\tif (entry) {\n\t\t*eax = entry->eax;\n\t\t*ebx = entry->ebx;\n\t\t*ecx = entry->ecx;\n\t\t*edx = entry->edx;\n\t\tif (function == 7 && index == 0) {\n\t\t\tu64 data;\n\t\t        if (!__kvm_get_msr(vcpu, MSR_IA32_TSX_CTRL, &data, true) &&\n\t\t\t    (data & TSX_CTRL_CPUID_CLEAR))\n\t\t\t\t*ebx &= ~(F(RTM) | F(HLE));\n\t\t} else if (function == 0x80000007) {\n\t\t\tif (kvm_hv_invtsc_suppressed(vcpu))\n\t\t\t\t*edx &= ~SF(CONSTANT_TSC);\n\t\t}\n\t} else {\n\t\t*eax = *ebx = *ecx = *edx = 0;\n\t\t \n\t\tif (function == 0xb || function == 0x1f) {\n\t\t\tentry = kvm_find_cpuid_entry_index(vcpu, function, 1);\n\t\t\tif (entry) {\n\t\t\t\t*ecx = index & 0xff;\n\t\t\t\t*edx = entry->edx;\n\t\t\t}\n\t\t}\n\t}\n\ttrace_kvm_cpuid(orig_function, index, *eax, *ebx, *ecx, *edx, exact,\n\t\t\tused_max_basic);\n\treturn exact;\n}\nEXPORT_SYMBOL_GPL(kvm_cpuid);\n\nint kvm_emulate_cpuid(struct kvm_vcpu *vcpu)\n{\n\tu32 eax, ebx, ecx, edx;\n\n\tif (cpuid_fault_enabled(vcpu) && !kvm_require_cpl(vcpu, 0))\n\t\treturn 1;\n\n\teax = kvm_rax_read(vcpu);\n\tecx = kvm_rcx_read(vcpu);\n\tkvm_cpuid(vcpu, &eax, &ebx, &ecx, &edx, false);\n\tkvm_rax_write(vcpu, eax);\n\tkvm_rbx_write(vcpu, ebx);\n\tkvm_rcx_write(vcpu, ecx);\n\tkvm_rdx_write(vcpu, edx);\n\treturn kvm_skip_emulated_instruction(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_emulate_cpuid);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}