{
  "module_name": "kvm_cache_regs.h",
  "hash_id": "955ec6ce2e6b47bbd65d2a10bac93d04a6b436cae40197503be401d0bbe9510c",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/kvm_cache_regs.h",
  "human_readable_source": " \n#ifndef ASM_KVM_CACHE_REGS_H\n#define ASM_KVM_CACHE_REGS_H\n\n#include <linux/kvm_host.h>\n\n#define KVM_POSSIBLE_CR0_GUEST_BITS\t(X86_CR0_TS | X86_CR0_WP)\n#define KVM_POSSIBLE_CR4_GUEST_BITS\t\t\t\t  \\\n\t(X86_CR4_PVI | X86_CR4_DE | X86_CR4_PCE | X86_CR4_OSFXSR  \\\n\t | X86_CR4_OSXMMEXCPT | X86_CR4_PGE | X86_CR4_TSD | X86_CR4_FSGSBASE)\n\n#define X86_CR0_PDPTR_BITS    (X86_CR0_CD | X86_CR0_NW | X86_CR0_PG)\n#define X86_CR4_TLBFLUSH_BITS (X86_CR4_PGE | X86_CR4_PCIDE | X86_CR4_PAE | X86_CR4_SMEP)\n#define X86_CR4_PDPTR_BITS    (X86_CR4_PGE | X86_CR4_PSE | X86_CR4_PAE | X86_CR4_SMEP)\n\nstatic_assert(!(KVM_POSSIBLE_CR0_GUEST_BITS & X86_CR0_PDPTR_BITS));\n\n#define BUILD_KVM_GPR_ACCESSORS(lname, uname)\t\t\t\t      \\\nstatic __always_inline unsigned long kvm_##lname##_read(struct kvm_vcpu *vcpu)\\\n{\t\t\t\t\t\t\t\t\t      \\\n\treturn vcpu->arch.regs[VCPU_REGS_##uname];\t\t\t      \\\n}\t\t\t\t\t\t\t\t\t      \\\nstatic __always_inline void kvm_##lname##_write(struct kvm_vcpu *vcpu,\t      \\\n\t\t\t\t\t\tunsigned long val)\t      \\\n{\t\t\t\t\t\t\t\t\t      \\\n\tvcpu->arch.regs[VCPU_REGS_##uname] = val;\t\t\t      \\\n}\nBUILD_KVM_GPR_ACCESSORS(rax, RAX)\nBUILD_KVM_GPR_ACCESSORS(rbx, RBX)\nBUILD_KVM_GPR_ACCESSORS(rcx, RCX)\nBUILD_KVM_GPR_ACCESSORS(rdx, RDX)\nBUILD_KVM_GPR_ACCESSORS(rbp, RBP)\nBUILD_KVM_GPR_ACCESSORS(rsi, RSI)\nBUILD_KVM_GPR_ACCESSORS(rdi, RDI)\n#ifdef CONFIG_X86_64\nBUILD_KVM_GPR_ACCESSORS(r8,  R8)\nBUILD_KVM_GPR_ACCESSORS(r9,  R9)\nBUILD_KVM_GPR_ACCESSORS(r10, R10)\nBUILD_KVM_GPR_ACCESSORS(r11, R11)\nBUILD_KVM_GPR_ACCESSORS(r12, R12)\nBUILD_KVM_GPR_ACCESSORS(r13, R13)\nBUILD_KVM_GPR_ACCESSORS(r14, R14)\nBUILD_KVM_GPR_ACCESSORS(r15, R15)\n#endif\n\n \nstatic inline bool kvm_register_is_available(struct kvm_vcpu *vcpu,\n\t\t\t\t\t     enum kvm_reg reg)\n{\n\treturn test_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);\n}\n\nstatic inline bool kvm_register_is_dirty(struct kvm_vcpu *vcpu,\n\t\t\t\t\t enum kvm_reg reg)\n{\n\treturn test_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);\n}\n\nstatic inline void kvm_register_mark_available(struct kvm_vcpu *vcpu,\n\t\t\t\t\t       enum kvm_reg reg)\n{\n\t__set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);\n}\n\nstatic inline void kvm_register_mark_dirty(struct kvm_vcpu *vcpu,\n\t\t\t\t\t   enum kvm_reg reg)\n{\n\t__set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);\n\t__set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);\n}\n\n \nstatic __always_inline bool kvm_register_test_and_mark_available(struct kvm_vcpu *vcpu,\n\t\t\t\t\t\t\t\t enum kvm_reg reg)\n{\n\treturn arch___test_and_set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);\n}\n\n \nstatic inline unsigned long kvm_register_read_raw(struct kvm_vcpu *vcpu, int reg)\n{\n\tif (WARN_ON_ONCE((unsigned int)reg >= NR_VCPU_REGS))\n\t\treturn 0;\n\n\tif (!kvm_register_is_available(vcpu, reg))\n\t\tstatic_call(kvm_x86_cache_reg)(vcpu, reg);\n\n\treturn vcpu->arch.regs[reg];\n}\n\nstatic inline void kvm_register_write_raw(struct kvm_vcpu *vcpu, int reg,\n\t\t\t\t\t  unsigned long val)\n{\n\tif (WARN_ON_ONCE((unsigned int)reg >= NR_VCPU_REGS))\n\t\treturn;\n\n\tvcpu->arch.regs[reg] = val;\n\tkvm_register_mark_dirty(vcpu, reg);\n}\n\nstatic inline unsigned long kvm_rip_read(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_register_read_raw(vcpu, VCPU_REGS_RIP);\n}\n\nstatic inline void kvm_rip_write(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tkvm_register_write_raw(vcpu, VCPU_REGS_RIP, val);\n}\n\nstatic inline unsigned long kvm_rsp_read(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_register_read_raw(vcpu, VCPU_REGS_RSP);\n}\n\nstatic inline void kvm_rsp_write(struct kvm_vcpu *vcpu, unsigned long val)\n{\n\tkvm_register_write_raw(vcpu, VCPU_REGS_RSP, val);\n}\n\nstatic inline u64 kvm_pdptr_read(struct kvm_vcpu *vcpu, int index)\n{\n\tmight_sleep();   \n\n\tif (!kvm_register_is_available(vcpu, VCPU_EXREG_PDPTR))\n\t\tstatic_call(kvm_x86_cache_reg)(vcpu, VCPU_EXREG_PDPTR);\n\n\treturn vcpu->arch.walk_mmu->pdptrs[index];\n}\n\nstatic inline void kvm_pdptr_write(struct kvm_vcpu *vcpu, int index, u64 value)\n{\n\tvcpu->arch.walk_mmu->pdptrs[index] = value;\n}\n\nstatic inline ulong kvm_read_cr0_bits(struct kvm_vcpu *vcpu, ulong mask)\n{\n\tulong tmask = mask & KVM_POSSIBLE_CR0_GUEST_BITS;\n\tif ((tmask & vcpu->arch.cr0_guest_owned_bits) &&\n\t    !kvm_register_is_available(vcpu, VCPU_EXREG_CR0))\n\t\tstatic_call(kvm_x86_cache_reg)(vcpu, VCPU_EXREG_CR0);\n\treturn vcpu->arch.cr0 & mask;\n}\n\nstatic __always_inline bool kvm_is_cr0_bit_set(struct kvm_vcpu *vcpu,\n\t\t\t\t\t       unsigned long cr0_bit)\n{\n\tBUILD_BUG_ON(!is_power_of_2(cr0_bit));\n\n\treturn !!kvm_read_cr0_bits(vcpu, cr0_bit);\n}\n\nstatic inline ulong kvm_read_cr0(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_read_cr0_bits(vcpu, ~0UL);\n}\n\nstatic inline ulong kvm_read_cr4_bits(struct kvm_vcpu *vcpu, ulong mask)\n{\n\tulong tmask = mask & KVM_POSSIBLE_CR4_GUEST_BITS;\n\tif ((tmask & vcpu->arch.cr4_guest_owned_bits) &&\n\t    !kvm_register_is_available(vcpu, VCPU_EXREG_CR4))\n\t\tstatic_call(kvm_x86_cache_reg)(vcpu, VCPU_EXREG_CR4);\n\treturn vcpu->arch.cr4 & mask;\n}\n\nstatic __always_inline bool kvm_is_cr4_bit_set(struct kvm_vcpu *vcpu,\n\t\t\t\t\t       unsigned long cr4_bit)\n{\n\tBUILD_BUG_ON(!is_power_of_2(cr4_bit));\n\n\treturn !!kvm_read_cr4_bits(vcpu, cr4_bit);\n}\n\nstatic inline ulong kvm_read_cr3(struct kvm_vcpu *vcpu)\n{\n\tif (!kvm_register_is_available(vcpu, VCPU_EXREG_CR3))\n\t\tstatic_call(kvm_x86_cache_reg)(vcpu, VCPU_EXREG_CR3);\n\treturn vcpu->arch.cr3;\n}\n\nstatic inline ulong kvm_read_cr4(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_read_cr4_bits(vcpu, ~0UL);\n}\n\nstatic inline u64 kvm_read_edx_eax(struct kvm_vcpu *vcpu)\n{\n\treturn (kvm_rax_read(vcpu) & -1u)\n\t\t| ((u64)(kvm_rdx_read(vcpu) & -1u) << 32);\n}\n\nstatic inline void enter_guest_mode(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.hflags |= HF_GUEST_MASK;\n\tvcpu->stat.guest_mode = 1;\n}\n\nstatic inline void leave_guest_mode(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.hflags &= ~HF_GUEST_MASK;\n\n\tif (vcpu->arch.load_eoi_exitmap_pending) {\n\t\tvcpu->arch.load_eoi_exitmap_pending = false;\n\t\tkvm_make_request(KVM_REQ_LOAD_EOI_EXITMAP, vcpu);\n\t}\n\n\tvcpu->stat.guest_mode = 0;\n}\n\nstatic inline bool is_guest_mode(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.hflags & HF_GUEST_MASK;\n}\n\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}