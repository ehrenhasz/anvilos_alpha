{
  "module_name": "smm.c",
  "hash_id": "bf2b1964cd8f851b6715f421bee8e75211efc3692a67f25362807813ce5ad771",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/smm.c",
  "human_readable_source": " \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/kvm_host.h>\n#include \"x86.h\"\n#include \"kvm_cache_regs.h\"\n#include \"kvm_emulate.h\"\n#include \"smm.h\"\n#include \"cpuid.h\"\n#include \"trace.h\"\n\n#define CHECK_SMRAM32_OFFSET(field, offset) \\\n\tASSERT_STRUCT_OFFSET(struct kvm_smram_state_32, field, offset - 0xFE00)\n\n#define CHECK_SMRAM64_OFFSET(field, offset) \\\n\tASSERT_STRUCT_OFFSET(struct kvm_smram_state_64, field, offset - 0xFE00)\n\nstatic void check_smram_offsets(void)\n{\n\t \n\tCHECK_SMRAM32_OFFSET(reserved1,\t\t\t0xFE00);\n\tCHECK_SMRAM32_OFFSET(smbase,\t\t\t0xFEF8);\n\tCHECK_SMRAM32_OFFSET(smm_revision,\t\t0xFEFC);\n\tCHECK_SMRAM32_OFFSET(io_inst_restart,\t\t0xFF00);\n\tCHECK_SMRAM32_OFFSET(auto_hlt_restart,\t\t0xFF02);\n\tCHECK_SMRAM32_OFFSET(io_restart_rdi,\t\t0xFF04);\n\tCHECK_SMRAM32_OFFSET(io_restart_rcx,\t\t0xFF08);\n\tCHECK_SMRAM32_OFFSET(io_restart_rsi,\t\t0xFF0C);\n\tCHECK_SMRAM32_OFFSET(io_restart_rip,\t\t0xFF10);\n\tCHECK_SMRAM32_OFFSET(cr4,\t\t\t0xFF14);\n\tCHECK_SMRAM32_OFFSET(reserved2,\t\t\t0xFF18);\n\tCHECK_SMRAM32_OFFSET(int_shadow,\t\t0xFF1A);\n\tCHECK_SMRAM32_OFFSET(reserved3,\t\t\t0xFF1B);\n\tCHECK_SMRAM32_OFFSET(ds,\t\t\t0xFF2C);\n\tCHECK_SMRAM32_OFFSET(fs,\t\t\t0xFF38);\n\tCHECK_SMRAM32_OFFSET(gs,\t\t\t0xFF44);\n\tCHECK_SMRAM32_OFFSET(idtr,\t\t\t0xFF50);\n\tCHECK_SMRAM32_OFFSET(tr,\t\t\t0xFF5C);\n\tCHECK_SMRAM32_OFFSET(gdtr,\t\t\t0xFF6C);\n\tCHECK_SMRAM32_OFFSET(ldtr,\t\t\t0xFF78);\n\tCHECK_SMRAM32_OFFSET(es,\t\t\t0xFF84);\n\tCHECK_SMRAM32_OFFSET(cs,\t\t\t0xFF90);\n\tCHECK_SMRAM32_OFFSET(ss,\t\t\t0xFF9C);\n\tCHECK_SMRAM32_OFFSET(es_sel,\t\t\t0xFFA8);\n\tCHECK_SMRAM32_OFFSET(cs_sel,\t\t\t0xFFAC);\n\tCHECK_SMRAM32_OFFSET(ss_sel,\t\t\t0xFFB0);\n\tCHECK_SMRAM32_OFFSET(ds_sel,\t\t\t0xFFB4);\n\tCHECK_SMRAM32_OFFSET(fs_sel,\t\t\t0xFFB8);\n\tCHECK_SMRAM32_OFFSET(gs_sel,\t\t\t0xFFBC);\n\tCHECK_SMRAM32_OFFSET(ldtr_sel,\t\t\t0xFFC0);\n\tCHECK_SMRAM32_OFFSET(tr_sel,\t\t\t0xFFC4);\n\tCHECK_SMRAM32_OFFSET(dr7,\t\t\t0xFFC8);\n\tCHECK_SMRAM32_OFFSET(dr6,\t\t\t0xFFCC);\n\tCHECK_SMRAM32_OFFSET(gprs,\t\t\t0xFFD0);\n\tCHECK_SMRAM32_OFFSET(eip,\t\t\t0xFFF0);\n\tCHECK_SMRAM32_OFFSET(eflags,\t\t\t0xFFF4);\n\tCHECK_SMRAM32_OFFSET(cr3,\t\t\t0xFFF8);\n\tCHECK_SMRAM32_OFFSET(cr0,\t\t\t0xFFFC);\n\n\t \n\tCHECK_SMRAM64_OFFSET(es,\t\t\t0xFE00);\n\tCHECK_SMRAM64_OFFSET(cs,\t\t\t0xFE10);\n\tCHECK_SMRAM64_OFFSET(ss,\t\t\t0xFE20);\n\tCHECK_SMRAM64_OFFSET(ds,\t\t\t0xFE30);\n\tCHECK_SMRAM64_OFFSET(fs,\t\t\t0xFE40);\n\tCHECK_SMRAM64_OFFSET(gs,\t\t\t0xFE50);\n\tCHECK_SMRAM64_OFFSET(gdtr,\t\t\t0xFE60);\n\tCHECK_SMRAM64_OFFSET(ldtr,\t\t\t0xFE70);\n\tCHECK_SMRAM64_OFFSET(idtr,\t\t\t0xFE80);\n\tCHECK_SMRAM64_OFFSET(tr,\t\t\t0xFE90);\n\tCHECK_SMRAM64_OFFSET(io_restart_rip,\t\t0xFEA0);\n\tCHECK_SMRAM64_OFFSET(io_restart_rcx,\t\t0xFEA8);\n\tCHECK_SMRAM64_OFFSET(io_restart_rsi,\t\t0xFEB0);\n\tCHECK_SMRAM64_OFFSET(io_restart_rdi,\t\t0xFEB8);\n\tCHECK_SMRAM64_OFFSET(io_restart_dword,\t\t0xFEC0);\n\tCHECK_SMRAM64_OFFSET(reserved1,\t\t\t0xFEC4);\n\tCHECK_SMRAM64_OFFSET(io_inst_restart,\t\t0xFEC8);\n\tCHECK_SMRAM64_OFFSET(auto_hlt_restart,\t\t0xFEC9);\n\tCHECK_SMRAM64_OFFSET(amd_nmi_mask,\t\t0xFECA);\n\tCHECK_SMRAM64_OFFSET(int_shadow,\t\t0xFECB);\n\tCHECK_SMRAM64_OFFSET(reserved2,\t\t\t0xFECC);\n\tCHECK_SMRAM64_OFFSET(efer,\t\t\t0xFED0);\n\tCHECK_SMRAM64_OFFSET(svm_guest_flag,\t\t0xFED8);\n\tCHECK_SMRAM64_OFFSET(svm_guest_vmcb_gpa,\t0xFEE0);\n\tCHECK_SMRAM64_OFFSET(svm_guest_virtual_int,\t0xFEE8);\n\tCHECK_SMRAM64_OFFSET(reserved3,\t\t\t0xFEF0);\n\tCHECK_SMRAM64_OFFSET(smm_revison,\t\t0xFEFC);\n\tCHECK_SMRAM64_OFFSET(smbase,\t\t\t0xFF00);\n\tCHECK_SMRAM64_OFFSET(reserved4,\t\t\t0xFF04);\n\tCHECK_SMRAM64_OFFSET(ssp,\t\t\t0xFF18);\n\tCHECK_SMRAM64_OFFSET(svm_guest_pat,\t\t0xFF20);\n\tCHECK_SMRAM64_OFFSET(svm_host_efer,\t\t0xFF28);\n\tCHECK_SMRAM64_OFFSET(svm_host_cr4,\t\t0xFF30);\n\tCHECK_SMRAM64_OFFSET(svm_host_cr3,\t\t0xFF38);\n\tCHECK_SMRAM64_OFFSET(svm_host_cr0,\t\t0xFF40);\n\tCHECK_SMRAM64_OFFSET(cr4,\t\t\t0xFF48);\n\tCHECK_SMRAM64_OFFSET(cr3,\t\t\t0xFF50);\n\tCHECK_SMRAM64_OFFSET(cr0,\t\t\t0xFF58);\n\tCHECK_SMRAM64_OFFSET(dr7,\t\t\t0xFF60);\n\tCHECK_SMRAM64_OFFSET(dr6,\t\t\t0xFF68);\n\tCHECK_SMRAM64_OFFSET(rflags,\t\t\t0xFF70);\n\tCHECK_SMRAM64_OFFSET(rip,\t\t\t0xFF78);\n\tCHECK_SMRAM64_OFFSET(gprs,\t\t\t0xFF80);\n\n\tBUILD_BUG_ON(sizeof(union kvm_smram) != 512);\n}\n\n#undef CHECK_SMRAM64_OFFSET\n#undef CHECK_SMRAM32_OFFSET\n\n\nvoid kvm_smm_changed(struct kvm_vcpu *vcpu, bool entering_smm)\n{\n\ttrace_kvm_smm_transition(vcpu->vcpu_id, vcpu->arch.smbase, entering_smm);\n\n\tif (entering_smm) {\n\t\tvcpu->arch.hflags |= HF_SMM_MASK;\n\t} else {\n\t\tvcpu->arch.hflags &= ~(HF_SMM_MASK | HF_SMM_INSIDE_NMI_MASK);\n\n\t\t \n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\n\t\t \n\t\tvcpu->arch.pdptrs_from_userspace = false;\n\t}\n\n\tkvm_mmu_reset_context(vcpu);\n}\n\nvoid process_smi(struct kvm_vcpu *vcpu)\n{\n\tvcpu->arch.smi_pending = true;\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n}\n\nstatic u32 enter_smm_get_segment_flags(struct kvm_segment *seg)\n{\n\tu32 flags = 0;\n\tflags |= seg->g       << 23;\n\tflags |= seg->db      << 22;\n\tflags |= seg->l       << 21;\n\tflags |= seg->avl     << 20;\n\tflags |= seg->present << 15;\n\tflags |= seg->dpl     << 13;\n\tflags |= seg->s       << 12;\n\tflags |= seg->type    << 8;\n\treturn flags;\n}\n\nstatic void enter_smm_save_seg_32(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_smm_seg_state_32 *state,\n\t\t\t\t  u32 *selector, int n)\n{\n\tstruct kvm_segment seg;\n\n\tkvm_get_segment(vcpu, &seg, n);\n\t*selector = seg.selector;\n\tstate->base = seg.base;\n\tstate->limit = seg.limit;\n\tstate->flags = enter_smm_get_segment_flags(&seg);\n}\n\n#ifdef CONFIG_X86_64\nstatic void enter_smm_save_seg_64(struct kvm_vcpu *vcpu,\n\t\t\t\t  struct kvm_smm_seg_state_64 *state,\n\t\t\t\t  int n)\n{\n\tstruct kvm_segment seg;\n\n\tkvm_get_segment(vcpu, &seg, n);\n\tstate->selector = seg.selector;\n\tstate->attributes = enter_smm_get_segment_flags(&seg) >> 8;\n\tstate->limit = seg.limit;\n\tstate->base = seg.base;\n}\n#endif\n\nstatic void enter_smm_save_state_32(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_smram_state_32 *smram)\n{\n\tstruct desc_ptr dt;\n\tunsigned long val;\n\tint i;\n\n\tsmram->cr0     = kvm_read_cr0(vcpu);\n\tsmram->cr3     = kvm_read_cr3(vcpu);\n\tsmram->eflags  = kvm_get_rflags(vcpu);\n\tsmram->eip     = kvm_rip_read(vcpu);\n\n\tfor (i = 0; i < 8; i++)\n\t\tsmram->gprs[i] = kvm_register_read_raw(vcpu, i);\n\n\tkvm_get_dr(vcpu, 6, &val);\n\tsmram->dr6     = (u32)val;\n\tkvm_get_dr(vcpu, 7, &val);\n\tsmram->dr7     = (u32)val;\n\n\tenter_smm_save_seg_32(vcpu, &smram->tr, &smram->tr_sel, VCPU_SREG_TR);\n\tenter_smm_save_seg_32(vcpu, &smram->ldtr, &smram->ldtr_sel, VCPU_SREG_LDTR);\n\n\tstatic_call(kvm_x86_get_gdt)(vcpu, &dt);\n\tsmram->gdtr.base = dt.address;\n\tsmram->gdtr.limit = dt.size;\n\n\tstatic_call(kvm_x86_get_idt)(vcpu, &dt);\n\tsmram->idtr.base = dt.address;\n\tsmram->idtr.limit = dt.size;\n\n\tenter_smm_save_seg_32(vcpu, &smram->es, &smram->es_sel, VCPU_SREG_ES);\n\tenter_smm_save_seg_32(vcpu, &smram->cs, &smram->cs_sel, VCPU_SREG_CS);\n\tenter_smm_save_seg_32(vcpu, &smram->ss, &smram->ss_sel, VCPU_SREG_SS);\n\n\tenter_smm_save_seg_32(vcpu, &smram->ds, &smram->ds_sel, VCPU_SREG_DS);\n\tenter_smm_save_seg_32(vcpu, &smram->fs, &smram->fs_sel, VCPU_SREG_FS);\n\tenter_smm_save_seg_32(vcpu, &smram->gs, &smram->gs_sel, VCPU_SREG_GS);\n\n\tsmram->cr4 = kvm_read_cr4(vcpu);\n\tsmram->smm_revision = 0x00020000;\n\tsmram->smbase = vcpu->arch.smbase;\n\n\tsmram->int_shadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);\n}\n\n#ifdef CONFIG_X86_64\nstatic void enter_smm_save_state_64(struct kvm_vcpu *vcpu,\n\t\t\t\t    struct kvm_smram_state_64 *smram)\n{\n\tstruct desc_ptr dt;\n\tunsigned long val;\n\tint i;\n\n\tfor (i = 0; i < 16; i++)\n\t\tsmram->gprs[15 - i] = kvm_register_read_raw(vcpu, i);\n\n\tsmram->rip    = kvm_rip_read(vcpu);\n\tsmram->rflags = kvm_get_rflags(vcpu);\n\n\n\tkvm_get_dr(vcpu, 6, &val);\n\tsmram->dr6 = val;\n\tkvm_get_dr(vcpu, 7, &val);\n\tsmram->dr7 = val;\n\n\tsmram->cr0 = kvm_read_cr0(vcpu);\n\tsmram->cr3 = kvm_read_cr3(vcpu);\n\tsmram->cr4 = kvm_read_cr4(vcpu);\n\n\tsmram->smbase = vcpu->arch.smbase;\n\tsmram->smm_revison = 0x00020064;\n\n\tsmram->efer = vcpu->arch.efer;\n\n\tenter_smm_save_seg_64(vcpu, &smram->tr, VCPU_SREG_TR);\n\n\tstatic_call(kvm_x86_get_idt)(vcpu, &dt);\n\tsmram->idtr.limit = dt.size;\n\tsmram->idtr.base = dt.address;\n\n\tenter_smm_save_seg_64(vcpu, &smram->ldtr, VCPU_SREG_LDTR);\n\n\tstatic_call(kvm_x86_get_gdt)(vcpu, &dt);\n\tsmram->gdtr.limit = dt.size;\n\tsmram->gdtr.base = dt.address;\n\n\tenter_smm_save_seg_64(vcpu, &smram->es, VCPU_SREG_ES);\n\tenter_smm_save_seg_64(vcpu, &smram->cs, VCPU_SREG_CS);\n\tenter_smm_save_seg_64(vcpu, &smram->ss, VCPU_SREG_SS);\n\tenter_smm_save_seg_64(vcpu, &smram->ds, VCPU_SREG_DS);\n\tenter_smm_save_seg_64(vcpu, &smram->fs, VCPU_SREG_FS);\n\tenter_smm_save_seg_64(vcpu, &smram->gs, VCPU_SREG_GS);\n\n\tsmram->int_shadow = static_call(kvm_x86_get_interrupt_shadow)(vcpu);\n}\n#endif\n\nvoid enter_smm(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_segment cs, ds;\n\tstruct desc_ptr dt;\n\tunsigned long cr0;\n\tunion kvm_smram smram;\n\n\tcheck_smram_offsets();\n\n\tmemset(smram.bytes, 0, sizeof(smram.bytes));\n\n#ifdef CONFIG_X86_64\n\tif (guest_cpuid_has(vcpu, X86_FEATURE_LM))\n\t\tenter_smm_save_state_64(vcpu, &smram.smram64);\n\telse\n#endif\n\t\tenter_smm_save_state_32(vcpu, &smram.smram32);\n\n\t \n\tif (static_call(kvm_x86_enter_smm)(vcpu, &smram))\n\t\tgoto error;\n\n\tkvm_smm_changed(vcpu, true);\n\n\tif (kvm_vcpu_write_guest(vcpu, vcpu->arch.smbase + 0xfe00, &smram, sizeof(smram)))\n\t\tgoto error;\n\n\tif (static_call(kvm_x86_get_nmi_mask)(vcpu))\n\t\tvcpu->arch.hflags |= HF_SMM_INSIDE_NMI_MASK;\n\telse\n\t\tstatic_call(kvm_x86_set_nmi_mask)(vcpu, true);\n\n\tkvm_set_rflags(vcpu, X86_EFLAGS_FIXED);\n\tkvm_rip_write(vcpu, 0x8000);\n\n\tstatic_call(kvm_x86_set_interrupt_shadow)(vcpu, 0);\n\n\tcr0 = vcpu->arch.cr0 & ~(X86_CR0_PE | X86_CR0_EM | X86_CR0_TS | X86_CR0_PG);\n\tstatic_call(kvm_x86_set_cr0)(vcpu, cr0);\n\tvcpu->arch.cr0 = cr0;\n\n\tstatic_call(kvm_x86_set_cr4)(vcpu, 0);\n\n\t \n\tdt.address = dt.size = 0;\n\tstatic_call(kvm_x86_set_idt)(vcpu, &dt);\n\n\tif (WARN_ON_ONCE(kvm_set_dr(vcpu, 7, DR7_FIXED_1)))\n\t\tgoto error;\n\n\tcs.selector = (vcpu->arch.smbase >> 4) & 0xffff;\n\tcs.base = vcpu->arch.smbase;\n\n\tds.selector = 0;\n\tds.base = 0;\n\n\tcs.limit    = ds.limit = 0xffffffff;\n\tcs.type     = ds.type = 0x3;\n\tcs.dpl      = ds.dpl = 0;\n\tcs.db       = ds.db = 0;\n\tcs.s        = ds.s = 1;\n\tcs.l        = ds.l = 0;\n\tcs.g        = ds.g = 1;\n\tcs.avl      = ds.avl = 0;\n\tcs.present  = ds.present = 1;\n\tcs.unusable = ds.unusable = 0;\n\tcs.padding  = ds.padding = 0;\n\n\tkvm_set_segment(vcpu, &cs, VCPU_SREG_CS);\n\tkvm_set_segment(vcpu, &ds, VCPU_SREG_DS);\n\tkvm_set_segment(vcpu, &ds, VCPU_SREG_ES);\n\tkvm_set_segment(vcpu, &ds, VCPU_SREG_FS);\n\tkvm_set_segment(vcpu, &ds, VCPU_SREG_GS);\n\tkvm_set_segment(vcpu, &ds, VCPU_SREG_SS);\n\n#ifdef CONFIG_X86_64\n\tif (guest_cpuid_has(vcpu, X86_FEATURE_LM))\n\t\tif (static_call(kvm_x86_set_efer)(vcpu, 0))\n\t\t\tgoto error;\n#endif\n\n\tkvm_update_cpuid_runtime(vcpu);\n\tkvm_mmu_reset_context(vcpu);\n\treturn;\nerror:\n\tkvm_vm_dead(vcpu->kvm);\n}\n\nstatic void rsm_set_desc_flags(struct kvm_segment *desc, u32 flags)\n{\n\tdesc->g    = (flags >> 23) & 1;\n\tdesc->db   = (flags >> 22) & 1;\n\tdesc->l    = (flags >> 21) & 1;\n\tdesc->avl  = (flags >> 20) & 1;\n\tdesc->present = (flags >> 15) & 1;\n\tdesc->dpl  = (flags >> 13) & 3;\n\tdesc->s    = (flags >> 12) & 1;\n\tdesc->type = (flags >>  8) & 15;\n\n\tdesc->unusable = !desc->present;\n\tdesc->padding = 0;\n}\n\nstatic int rsm_load_seg_32(struct kvm_vcpu *vcpu,\n\t\t\t   const struct kvm_smm_seg_state_32 *state,\n\t\t\t   u16 selector, int n)\n{\n\tstruct kvm_segment desc;\n\n\tdesc.selector =           selector;\n\tdesc.base =               state->base;\n\tdesc.limit =              state->limit;\n\trsm_set_desc_flags(&desc, state->flags);\n\tkvm_set_segment(vcpu, &desc, n);\n\treturn X86EMUL_CONTINUE;\n}\n\n#ifdef CONFIG_X86_64\n\nstatic int rsm_load_seg_64(struct kvm_vcpu *vcpu,\n\t\t\t   const struct kvm_smm_seg_state_64 *state,\n\t\t\t   int n)\n{\n\tstruct kvm_segment desc;\n\n\tdesc.selector =           state->selector;\n\trsm_set_desc_flags(&desc, state->attributes << 8);\n\tdesc.limit =              state->limit;\n\tdesc.base =               state->base;\n\tkvm_set_segment(vcpu, &desc, n);\n\treturn X86EMUL_CONTINUE;\n}\n#endif\n\nstatic int rsm_enter_protected_mode(struct kvm_vcpu *vcpu,\n\t\t\t\t    u64 cr0, u64 cr3, u64 cr4)\n{\n\tint bad;\n\tu64 pcid;\n\n\t \n\tpcid = 0;\n\tif (cr4 & X86_CR4_PCIDE) {\n\t\tpcid = cr3 & 0xfff;\n\t\tcr3 &= ~0xfff;\n\t}\n\n\tbad = kvm_set_cr3(vcpu, cr3);\n\tif (bad)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\t \n\tbad = kvm_set_cr4(vcpu, cr4 & ~X86_CR4_PCIDE);\n\tif (bad)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\tbad = kvm_set_cr0(vcpu, cr0);\n\tif (bad)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\tif (cr4 & X86_CR4_PCIDE) {\n\t\tbad = kvm_set_cr4(vcpu, cr4);\n\t\tif (bad)\n\t\t\treturn X86EMUL_UNHANDLEABLE;\n\t\tif (pcid) {\n\t\t\tbad = kvm_set_cr3(vcpu, cr3 | pcid);\n\t\t\tif (bad)\n\t\t\t\treturn X86EMUL_UNHANDLEABLE;\n\t\t}\n\n\t}\n\n\treturn X86EMUL_CONTINUE;\n}\n\nstatic int rsm_load_state_32(struct x86_emulate_ctxt *ctxt,\n\t\t\t     const struct kvm_smram_state_32 *smstate)\n{\n\tstruct kvm_vcpu *vcpu = ctxt->vcpu;\n\tstruct desc_ptr dt;\n\tint i, r;\n\n\tctxt->eflags =  smstate->eflags | X86_EFLAGS_FIXED;\n\tctxt->_eip =  smstate->eip;\n\n\tfor (i = 0; i < 8; i++)\n\t\t*reg_write(ctxt, i) = smstate->gprs[i];\n\n\tif (kvm_set_dr(vcpu, 6, smstate->dr6))\n\t\treturn X86EMUL_UNHANDLEABLE;\n\tif (kvm_set_dr(vcpu, 7, smstate->dr7))\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\trsm_load_seg_32(vcpu, &smstate->tr, smstate->tr_sel, VCPU_SREG_TR);\n\trsm_load_seg_32(vcpu, &smstate->ldtr, smstate->ldtr_sel, VCPU_SREG_LDTR);\n\n\tdt.address =               smstate->gdtr.base;\n\tdt.size =                  smstate->gdtr.limit;\n\tstatic_call(kvm_x86_set_gdt)(vcpu, &dt);\n\n\tdt.address =               smstate->idtr.base;\n\tdt.size =                  smstate->idtr.limit;\n\tstatic_call(kvm_x86_set_idt)(vcpu, &dt);\n\n\trsm_load_seg_32(vcpu, &smstate->es, smstate->es_sel, VCPU_SREG_ES);\n\trsm_load_seg_32(vcpu, &smstate->cs, smstate->cs_sel, VCPU_SREG_CS);\n\trsm_load_seg_32(vcpu, &smstate->ss, smstate->ss_sel, VCPU_SREG_SS);\n\n\trsm_load_seg_32(vcpu, &smstate->ds, smstate->ds_sel, VCPU_SREG_DS);\n\trsm_load_seg_32(vcpu, &smstate->fs, smstate->fs_sel, VCPU_SREG_FS);\n\trsm_load_seg_32(vcpu, &smstate->gs, smstate->gs_sel, VCPU_SREG_GS);\n\n\tvcpu->arch.smbase = smstate->smbase;\n\n\tr = rsm_enter_protected_mode(vcpu, smstate->cr0,\n\t\t\t\t\tsmstate->cr3, smstate->cr4);\n\n\tif (r != X86EMUL_CONTINUE)\n\t\treturn r;\n\n\tstatic_call(kvm_x86_set_interrupt_shadow)(vcpu, 0);\n\tctxt->interruptibility = (u8)smstate->int_shadow;\n\n\treturn r;\n}\n\n#ifdef CONFIG_X86_64\nstatic int rsm_load_state_64(struct x86_emulate_ctxt *ctxt,\n\t\t\t     const struct kvm_smram_state_64 *smstate)\n{\n\tstruct kvm_vcpu *vcpu = ctxt->vcpu;\n\tstruct desc_ptr dt;\n\tint i, r;\n\n\tfor (i = 0; i < 16; i++)\n\t\t*reg_write(ctxt, i) = smstate->gprs[15 - i];\n\n\tctxt->_eip   = smstate->rip;\n\tctxt->eflags = smstate->rflags | X86_EFLAGS_FIXED;\n\n\tif (kvm_set_dr(vcpu, 6, smstate->dr6))\n\t\treturn X86EMUL_UNHANDLEABLE;\n\tif (kvm_set_dr(vcpu, 7, smstate->dr7))\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\tvcpu->arch.smbase =         smstate->smbase;\n\n\tif (kvm_set_msr(vcpu, MSR_EFER, smstate->efer & ~EFER_LMA))\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\trsm_load_seg_64(vcpu, &smstate->tr, VCPU_SREG_TR);\n\n\tdt.size =                   smstate->idtr.limit;\n\tdt.address =                smstate->idtr.base;\n\tstatic_call(kvm_x86_set_idt)(vcpu, &dt);\n\n\trsm_load_seg_64(vcpu, &smstate->ldtr, VCPU_SREG_LDTR);\n\n\tdt.size =                   smstate->gdtr.limit;\n\tdt.address =                smstate->gdtr.base;\n\tstatic_call(kvm_x86_set_gdt)(vcpu, &dt);\n\n\tr = rsm_enter_protected_mode(vcpu, smstate->cr0, smstate->cr3, smstate->cr4);\n\tif (r != X86EMUL_CONTINUE)\n\t\treturn r;\n\n\trsm_load_seg_64(vcpu, &smstate->es, VCPU_SREG_ES);\n\trsm_load_seg_64(vcpu, &smstate->cs, VCPU_SREG_CS);\n\trsm_load_seg_64(vcpu, &smstate->ss, VCPU_SREG_SS);\n\trsm_load_seg_64(vcpu, &smstate->ds, VCPU_SREG_DS);\n\trsm_load_seg_64(vcpu, &smstate->fs, VCPU_SREG_FS);\n\trsm_load_seg_64(vcpu, &smstate->gs, VCPU_SREG_GS);\n\n\tstatic_call(kvm_x86_set_interrupt_shadow)(vcpu, 0);\n\tctxt->interruptibility = (u8)smstate->int_shadow;\n\n\treturn X86EMUL_CONTINUE;\n}\n#endif\n\nint emulator_leave_smm(struct x86_emulate_ctxt *ctxt)\n{\n\tstruct kvm_vcpu *vcpu = ctxt->vcpu;\n\tunsigned long cr0;\n\tunion kvm_smram smram;\n\tu64 smbase;\n\tint ret;\n\n\tsmbase = vcpu->arch.smbase;\n\n\tret = kvm_vcpu_read_guest(vcpu, smbase + 0xfe00, smram.bytes, sizeof(smram));\n\tif (ret < 0)\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n\tif ((vcpu->arch.hflags & HF_SMM_INSIDE_NMI_MASK) == 0)\n\t\tstatic_call(kvm_x86_set_nmi_mask)(vcpu, false);\n\n\tkvm_smm_changed(vcpu, false);\n\n\t \n#ifdef CONFIG_X86_64\n\tif (guest_cpuid_has(vcpu, X86_FEATURE_LM)) {\n\t\tstruct kvm_segment cs_desc;\n\t\tunsigned long cr4;\n\n\t\t \n\t\tcr4 = kvm_read_cr4(vcpu);\n\t\tif (cr4 & X86_CR4_PCIDE)\n\t\t\tkvm_set_cr4(vcpu, cr4 & ~X86_CR4_PCIDE);\n\n\t\t \n\t\tmemset(&cs_desc, 0, sizeof(cs_desc));\n\t\tcs_desc.type = 0xb;\n\t\tcs_desc.s = cs_desc.g = cs_desc.present = 1;\n\t\tkvm_set_segment(vcpu, &cs_desc, VCPU_SREG_CS);\n\t}\n#endif\n\n\t \n\tcr0 = kvm_read_cr0(vcpu);\n\tif (cr0 & X86_CR0_PE)\n\t\tkvm_set_cr0(vcpu, cr0 & ~(X86_CR0_PG | X86_CR0_PE));\n\n#ifdef CONFIG_X86_64\n\tif (guest_cpuid_has(vcpu, X86_FEATURE_LM)) {\n\t\tunsigned long cr4, efer;\n\n\t\t \n\t\tcr4 = kvm_read_cr4(vcpu);\n\t\tif (cr4 & X86_CR4_PAE)\n\t\t\tkvm_set_cr4(vcpu, cr4 & ~X86_CR4_PAE);\n\n\t\t \n\t\tefer = 0;\n\t\tkvm_set_msr(vcpu, MSR_EFER, efer);\n\t}\n#endif\n\n\t \n\tif (static_call(kvm_x86_leave_smm)(vcpu, &smram))\n\t\treturn X86EMUL_UNHANDLEABLE;\n\n#ifdef CONFIG_X86_64\n\tif (guest_cpuid_has(vcpu, X86_FEATURE_LM))\n\t\treturn rsm_load_state_64(ctxt, &smram.smram64);\n\telse\n#endif\n\t\treturn rsm_load_state_32(ctxt, &smram.smram32);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}