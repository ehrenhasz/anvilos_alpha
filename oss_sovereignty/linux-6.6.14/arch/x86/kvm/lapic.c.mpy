{
  "module_name": "lapic.c",
  "hash_id": "452b0cacd22ff90aee2d3ff8c8af5850accd999d679659ded4fb2b03bb767940",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/kvm/lapic.c",
  "human_readable_source": "\n\n \n#define pr_fmt(fmt) KBUILD_MODNAME \": \" fmt\n\n#include <linux/kvm_host.h>\n#include <linux/kvm.h>\n#include <linux/mm.h>\n#include <linux/highmem.h>\n#include <linux/smp.h>\n#include <linux/hrtimer.h>\n#include <linux/io.h>\n#include <linux/export.h>\n#include <linux/math64.h>\n#include <linux/slab.h>\n#include <asm/processor.h>\n#include <asm/mce.h>\n#include <asm/msr.h>\n#include <asm/page.h>\n#include <asm/current.h>\n#include <asm/apicdef.h>\n#include <asm/delay.h>\n#include <linux/atomic.h>\n#include <linux/jump_label.h>\n#include \"kvm_cache_regs.h\"\n#include \"irq.h\"\n#include \"ioapic.h\"\n#include \"trace.h\"\n#include \"x86.h\"\n#include \"cpuid.h\"\n#include \"hyperv.h\"\n#include \"smm.h\"\n\n#ifndef CONFIG_X86_64\n#define mod_64(x, y) ((x) - (y) * div64_u64(x, y))\n#else\n#define mod_64(x, y) ((x) % (y))\n#endif\n\n \n#define APIC_VERSION\t\t\t0x14UL\n#define LAPIC_MMIO_LENGTH\t\t(1 << 12)\n \n#define MAX_APIC_VECTOR\t\t\t256\n#define APIC_VECTORS_PER_REG\t\t32\n\nstatic bool lapic_timer_advance_dynamic __read_mostly;\n#define LAPIC_TIMER_ADVANCE_ADJUST_MIN\t100\t \n#define LAPIC_TIMER_ADVANCE_ADJUST_MAX\t10000\t \n#define LAPIC_TIMER_ADVANCE_NS_INIT\t1000\n#define LAPIC_TIMER_ADVANCE_NS_MAX     5000\n \n#define LAPIC_TIMER_ADVANCE_ADJUST_STEP 8\nstatic int kvm_lapic_msr_read(struct kvm_lapic *apic, u32 reg, u64 *data);\nstatic int kvm_lapic_msr_write(struct kvm_lapic *apic, u32 reg, u64 data);\n\nstatic inline void __kvm_lapic_set_reg(char *regs, int reg_off, u32 val)\n{\n\t*((u32 *) (regs + reg_off)) = val;\n}\n\nstatic inline void kvm_lapic_set_reg(struct kvm_lapic *apic, int reg_off, u32 val)\n{\n\t__kvm_lapic_set_reg(apic->regs, reg_off, val);\n}\n\nstatic __always_inline u64 __kvm_lapic_get_reg64(char *regs, int reg)\n{\n\tBUILD_BUG_ON(reg != APIC_ICR);\n\treturn *((u64 *) (regs + reg));\n}\n\nstatic __always_inline u64 kvm_lapic_get_reg64(struct kvm_lapic *apic, int reg)\n{\n\treturn __kvm_lapic_get_reg64(apic->regs, reg);\n}\n\nstatic __always_inline void __kvm_lapic_set_reg64(char *regs, int reg, u64 val)\n{\n\tBUILD_BUG_ON(reg != APIC_ICR);\n\t*((u64 *) (regs + reg)) = val;\n}\n\nstatic __always_inline void kvm_lapic_set_reg64(struct kvm_lapic *apic,\n\t\t\t\t\t\tint reg, u64 val)\n{\n\t__kvm_lapic_set_reg64(apic->regs, reg, val);\n}\n\nstatic inline int apic_test_vector(int vec, void *bitmap)\n{\n\treturn test_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));\n}\n\nbool kvm_apic_pending_eoi(struct kvm_vcpu *vcpu, int vector)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\treturn apic_test_vector(vector, apic->regs + APIC_ISR) ||\n\t\tapic_test_vector(vector, apic->regs + APIC_IRR);\n}\n\nstatic inline int __apic_test_and_set_vector(int vec, void *bitmap)\n{\n\treturn __test_and_set_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));\n}\n\nstatic inline int __apic_test_and_clear_vector(int vec, void *bitmap)\n{\n\treturn __test_and_clear_bit(VEC_POS(vec), (bitmap) + REG_POS(vec));\n}\n\n__read_mostly DEFINE_STATIC_KEY_DEFERRED_FALSE(apic_hw_disabled, HZ);\n__read_mostly DEFINE_STATIC_KEY_DEFERRED_FALSE(apic_sw_disabled, HZ);\n\nstatic inline int apic_enabled(struct kvm_lapic *apic)\n{\n\treturn kvm_apic_sw_enabled(apic) &&\tkvm_apic_hw_enabled(apic);\n}\n\n#define LVT_MASK\t\\\n\t(APIC_LVT_MASKED | APIC_SEND_PENDING | APIC_VECTOR_MASK)\n\n#define LINT_MASK\t\\\n\t(LVT_MASK | APIC_MODE_MASK | APIC_INPUT_POLARITY | \\\n\t APIC_LVT_REMOTE_IRR | APIC_LVT_LEVEL_TRIGGER)\n\nstatic inline u32 kvm_x2apic_id(struct kvm_lapic *apic)\n{\n\treturn apic->vcpu->vcpu_id;\n}\n\nstatic bool kvm_can_post_timer_interrupt(struct kvm_vcpu *vcpu)\n{\n\treturn pi_inject_timer && kvm_vcpu_apicv_active(vcpu) &&\n\t\t(kvm_mwait_in_guest(vcpu->kvm) || kvm_hlt_in_guest(vcpu->kvm));\n}\n\nbool kvm_can_use_hv_timer(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_x86_ops.set_hv_timer\n\t       && !(kvm_mwait_in_guest(vcpu->kvm) ||\n\t\t    kvm_can_post_timer_interrupt(vcpu));\n}\n\nstatic bool kvm_use_posted_timer_interrupt(struct kvm_vcpu *vcpu)\n{\n\treturn kvm_can_post_timer_interrupt(vcpu) && vcpu->mode == IN_GUEST_MODE;\n}\n\nstatic inline u32 kvm_apic_calc_x2apic_ldr(u32 id)\n{\n\treturn ((id >> 4) << 16) | (1 << (id & 0xf));\n}\n\nstatic inline bool kvm_apic_map_get_logical_dest(struct kvm_apic_map *map,\n\t\tu32 dest_id, struct kvm_lapic ***cluster, u16 *mask) {\n\tswitch (map->logical_mode) {\n\tcase KVM_APIC_MODE_SW_DISABLED:\n\t\t \n\t\t*cluster = map->xapic_flat_map;\n\t\t*mask = 0;\n\t\treturn true;\n\tcase KVM_APIC_MODE_X2APIC: {\n\t\tu32 offset = (dest_id >> 16) * 16;\n\t\tu32 max_apic_id = map->max_apic_id;\n\n\t\tif (offset <= max_apic_id) {\n\t\t\tu8 cluster_size = min(max_apic_id - offset + 1, 16U);\n\n\t\t\toffset = array_index_nospec(offset, map->max_apic_id + 1);\n\t\t\t*cluster = &map->phys_map[offset];\n\t\t\t*mask = dest_id & (0xffff >> (16 - cluster_size));\n\t\t} else {\n\t\t\t*mask = 0;\n\t\t}\n\n\t\treturn true;\n\t\t}\n\tcase KVM_APIC_MODE_XAPIC_FLAT:\n\t\t*cluster = map->xapic_flat_map;\n\t\t*mask = dest_id & 0xff;\n\t\treturn true;\n\tcase KVM_APIC_MODE_XAPIC_CLUSTER:\n\t\t*cluster = map->xapic_cluster_map[(dest_id >> 4) & 0xf];\n\t\t*mask = dest_id & 0xf;\n\t\treturn true;\n\tcase KVM_APIC_MODE_MAP_DISABLED:\n\t\treturn false;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn false;\n\t}\n}\n\nstatic void kvm_apic_map_free(struct rcu_head *rcu)\n{\n\tstruct kvm_apic_map *map = container_of(rcu, struct kvm_apic_map, rcu);\n\n\tkvfree(map);\n}\n\nstatic int kvm_recalculate_phys_map(struct kvm_apic_map *new,\n\t\t\t\t    struct kvm_vcpu *vcpu,\n\t\t\t\t    bool *xapic_id_mismatch)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 x2apic_id = kvm_x2apic_id(apic);\n\tu32 xapic_id = kvm_xapic_id(apic);\n\tu32 physical_id;\n\n\t \n\tif (WARN_ON_ONCE(xapic_id > new->max_apic_id))\n\t\treturn -EINVAL;\n\n\t \n\tif (x2apic_id > new->max_apic_id)\n\t\treturn -E2BIG;\n\n\t \n\tif (!apic_x2apic_mode(apic) && xapic_id != (u8)vcpu->vcpu_id)\n\t\t*xapic_id_mismatch = true;\n\n\t \n\tif (vcpu->kvm->arch.x2apic_format) {\n\t\t \n\t\tif (apic_x2apic_mode(apic) || x2apic_id > 0xff)\n\t\t\tnew->phys_map[x2apic_id] = apic;\n\n\t\tif (!apic_x2apic_mode(apic) && !new->phys_map[xapic_id])\n\t\t\tnew->phys_map[xapic_id] = apic;\n\t} else {\n\t\t \n\t\tif (apic_x2apic_mode(apic))\n\t\t\tphysical_id = x2apic_id;\n\t\telse\n\t\t\tphysical_id = xapic_id;\n\n\t\tif (new->phys_map[physical_id])\n\t\t\treturn -EINVAL;\n\n\t\tnew->phys_map[physical_id] = apic;\n\t}\n\n\treturn 0;\n}\n\nstatic void kvm_recalculate_logical_map(struct kvm_apic_map *new,\n\t\t\t\t\tstruct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tenum kvm_apic_logical_mode logical_mode;\n\tstruct kvm_lapic **cluster;\n\tu16 mask;\n\tu32 ldr;\n\n\tif (new->logical_mode == KVM_APIC_MODE_MAP_DISABLED)\n\t\treturn;\n\n\tif (!kvm_apic_sw_enabled(apic))\n\t\treturn;\n\n\tldr = kvm_lapic_get_reg(apic, APIC_LDR);\n\tif (!ldr)\n\t\treturn;\n\n\tif (apic_x2apic_mode(apic)) {\n\t\tlogical_mode = KVM_APIC_MODE_X2APIC;\n\t} else {\n\t\tldr = GET_APIC_LOGICAL_ID(ldr);\n\t\tif (kvm_lapic_get_reg(apic, APIC_DFR) == APIC_DFR_FLAT)\n\t\t\tlogical_mode = KVM_APIC_MODE_XAPIC_FLAT;\n\t\telse\n\t\t\tlogical_mode = KVM_APIC_MODE_XAPIC_CLUSTER;\n\t}\n\n\t \n\tif (new->logical_mode == KVM_APIC_MODE_SW_DISABLED) {\n\t\tnew->logical_mode = logical_mode;\n\t} else if (new->logical_mode != logical_mode) {\n\t\tnew->logical_mode = KVM_APIC_MODE_MAP_DISABLED;\n\t\treturn;\n\t}\n\n\t \n\tif (apic_x2apic_mode(apic)) {\n\t\tWARN_ON_ONCE(ldr != kvm_apic_calc_x2apic_ldr(kvm_x2apic_id(apic)));\n\t\treturn;\n\t}\n\n\tif (WARN_ON_ONCE(!kvm_apic_map_get_logical_dest(new, ldr,\n\t\t\t\t\t\t\t&cluster, &mask))) {\n\t\tnew->logical_mode = KVM_APIC_MODE_MAP_DISABLED;\n\t\treturn;\n\t}\n\n\tif (!mask)\n\t\treturn;\n\n\tldr = ffs(mask) - 1;\n\tif (!is_power_of_2(mask) || cluster[ldr])\n\t\tnew->logical_mode = KVM_APIC_MODE_MAP_DISABLED;\n\telse\n\t\tcluster[ldr] = apic;\n}\n\n \nenum {\n\tCLEAN,\n\tUPDATE_IN_PROGRESS,\n\tDIRTY\n};\n\nvoid kvm_recalculate_apic_map(struct kvm *kvm)\n{\n\tstruct kvm_apic_map *new, *old = NULL;\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long i;\n\tu32 max_id = 255;  \n\tbool xapic_id_mismatch;\n\tint r;\n\n\t \n\tif (atomic_read_acquire(&kvm->arch.apic_map_dirty) == CLEAN)\n\t\treturn;\n\n\tWARN_ONCE(!irqchip_in_kernel(kvm),\n\t\t  \"Dirty APIC map without an in-kernel local APIC\");\n\n\tmutex_lock(&kvm->arch.apic_map_lock);\n\nretry:\n\t \n\tif (atomic_cmpxchg_acquire(&kvm->arch.apic_map_dirty,\n\t\t\t\t   DIRTY, UPDATE_IN_PROGRESS) == CLEAN) {\n\t\t \n\t\tmutex_unlock(&kvm->arch.apic_map_lock);\n\t\treturn;\n\t}\n\n\t \n\txapic_id_mismatch = false;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm)\n\t\tif (kvm_apic_present(vcpu))\n\t\t\tmax_id = max(max_id, kvm_x2apic_id(vcpu->arch.apic));\n\n\tnew = kvzalloc(sizeof(struct kvm_apic_map) +\n\t                   sizeof(struct kvm_lapic *) * ((u64)max_id + 1),\n\t\t\t   GFP_KERNEL_ACCOUNT);\n\n\tif (!new)\n\t\tgoto out;\n\n\tnew->max_apic_id = max_id;\n\tnew->logical_mode = KVM_APIC_MODE_SW_DISABLED;\n\n\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\tif (!kvm_apic_present(vcpu))\n\t\t\tcontinue;\n\n\t\tr = kvm_recalculate_phys_map(new, vcpu, &xapic_id_mismatch);\n\t\tif (r) {\n\t\t\tkvfree(new);\n\t\t\tnew = NULL;\n\t\t\tif (r == -E2BIG) {\n\t\t\t\tcond_resched();\n\t\t\t\tgoto retry;\n\t\t\t}\n\n\t\t\tgoto out;\n\t\t}\n\n\t\tkvm_recalculate_logical_map(new, vcpu);\n\t}\nout:\n\t \n\tif (!new)\n\t\tkvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);\n\telse\n\t\tkvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_PHYSICAL_ID_ALIASED);\n\n\tif (!new || new->logical_mode == KVM_APIC_MODE_MAP_DISABLED)\n\t\tkvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);\n\telse\n\t\tkvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_LOGICAL_ID_ALIASED);\n\n\tif (xapic_id_mismatch)\n\t\tkvm_set_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);\n\telse\n\t\tkvm_clear_apicv_inhibit(kvm, APICV_INHIBIT_REASON_APIC_ID_MODIFIED);\n\n\told = rcu_dereference_protected(kvm->arch.apic_map,\n\t\t\tlockdep_is_held(&kvm->arch.apic_map_lock));\n\trcu_assign_pointer(kvm->arch.apic_map, new);\n\t \n\tatomic_cmpxchg_release(&kvm->arch.apic_map_dirty,\n\t\t\t       UPDATE_IN_PROGRESS, CLEAN);\n\tmutex_unlock(&kvm->arch.apic_map_lock);\n\n\tif (old)\n\t\tcall_rcu(&old->rcu, kvm_apic_map_free);\n\n\tkvm_make_scan_ioapic_request(kvm);\n}\n\nstatic inline void apic_set_spiv(struct kvm_lapic *apic, u32 val)\n{\n\tbool enabled = val & APIC_SPIV_APIC_ENABLED;\n\n\tkvm_lapic_set_reg(apic, APIC_SPIV, val);\n\n\tif (enabled != apic->sw_enabled) {\n\t\tapic->sw_enabled = enabled;\n\t\tif (enabled)\n\t\t\tstatic_branch_slow_dec_deferred(&apic_sw_disabled);\n\t\telse\n\t\t\tstatic_branch_inc(&apic_sw_disabled.key);\n\n\t\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n\t}\n\n\t \n\tif (enabled)\n\t\tkvm_make_request(KVM_REQ_APF_READY, apic->vcpu);\n}\n\nstatic inline void kvm_apic_set_xapic_id(struct kvm_lapic *apic, u8 id)\n{\n\tkvm_lapic_set_reg(apic, APIC_ID, id << 24);\n\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n}\n\nstatic inline void kvm_apic_set_ldr(struct kvm_lapic *apic, u32 id)\n{\n\tkvm_lapic_set_reg(apic, APIC_LDR, id);\n\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n}\n\nstatic inline void kvm_apic_set_dfr(struct kvm_lapic *apic, u32 val)\n{\n\tkvm_lapic_set_reg(apic, APIC_DFR, val);\n\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n}\n\nstatic inline void kvm_apic_set_x2apic_id(struct kvm_lapic *apic, u32 id)\n{\n\tu32 ldr = kvm_apic_calc_x2apic_ldr(id);\n\n\tWARN_ON_ONCE(id != apic->vcpu->vcpu_id);\n\n\tkvm_lapic_set_reg(apic, APIC_ID, id);\n\tkvm_lapic_set_reg(apic, APIC_LDR, ldr);\n\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n}\n\nstatic inline int apic_lvt_enabled(struct kvm_lapic *apic, int lvt_type)\n{\n\treturn !(kvm_lapic_get_reg(apic, lvt_type) & APIC_LVT_MASKED);\n}\n\nstatic inline int apic_lvtt_oneshot(struct kvm_lapic *apic)\n{\n\treturn apic->lapic_timer.timer_mode == APIC_LVT_TIMER_ONESHOT;\n}\n\nstatic inline int apic_lvtt_period(struct kvm_lapic *apic)\n{\n\treturn apic->lapic_timer.timer_mode == APIC_LVT_TIMER_PERIODIC;\n}\n\nstatic inline int apic_lvtt_tscdeadline(struct kvm_lapic *apic)\n{\n\treturn apic->lapic_timer.timer_mode == APIC_LVT_TIMER_TSCDEADLINE;\n}\n\nstatic inline int apic_lvt_nmi_mode(u32 lvt_val)\n{\n\treturn (lvt_val & (APIC_MODE_MASK | APIC_LVT_MASKED)) == APIC_DM_NMI;\n}\n\nstatic inline bool kvm_lapic_lvt_supported(struct kvm_lapic *apic, int lvt_index)\n{\n\treturn apic->nr_lvt_entries > lvt_index;\n}\n\nstatic inline int kvm_apic_calc_nr_lvt_entries(struct kvm_vcpu *vcpu)\n{\n\treturn KVM_APIC_MAX_NR_LVT_ENTRIES - !(vcpu->arch.mcg_cap & MCG_CMCI_P);\n}\n\nvoid kvm_apic_set_version(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 v = 0;\n\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn;\n\n\tv = APIC_VERSION | ((apic->nr_lvt_entries - 1) << 16);\n\n\t \n\tif (guest_cpuid_has(vcpu, X86_FEATURE_X2APIC) &&\n\t    !ioapic_in_kernel(vcpu->kvm))\n\t\tv |= APIC_LVR_DIRECTED_EOI;\n\tkvm_lapic_set_reg(apic, APIC_LVR, v);\n}\n\nvoid kvm_apic_after_set_mcg_cap(struct kvm_vcpu *vcpu)\n{\n\tint nr_lvt_entries = kvm_apic_calc_nr_lvt_entries(vcpu);\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tint i;\n\n\tif (!lapic_in_kernel(vcpu) || nr_lvt_entries == apic->nr_lvt_entries)\n\t\treturn;\n\n\t \n\tfor (i = apic->nr_lvt_entries; i < nr_lvt_entries; i++)\n\t\tkvm_lapic_set_reg(apic, APIC_LVTx(i), APIC_LVT_MASKED);\n\n\tapic->nr_lvt_entries = nr_lvt_entries;\n\n\t \n\tkvm_apic_set_version(vcpu);\n}\n\nstatic const unsigned int apic_lvt_mask[KVM_APIC_MAX_NR_LVT_ENTRIES] = {\n\t[LVT_TIMER] = LVT_MASK,       \n\t[LVT_THERMAL_MONITOR] = LVT_MASK | APIC_MODE_MASK,\n\t[LVT_PERFORMANCE_COUNTER] = LVT_MASK | APIC_MODE_MASK,\n\t[LVT_LINT0] = LINT_MASK,\n\t[LVT_LINT1] = LINT_MASK,\n\t[LVT_ERROR] = LVT_MASK,\n\t[LVT_CMCI] = LVT_MASK | APIC_MODE_MASK\n};\n\nstatic int find_highest_vector(void *bitmap)\n{\n\tint vec;\n\tu32 *reg;\n\n\tfor (vec = MAX_APIC_VECTOR - APIC_VECTORS_PER_REG;\n\t     vec >= 0; vec -= APIC_VECTORS_PER_REG) {\n\t\treg = bitmap + REG_POS(vec);\n\t\tif (*reg)\n\t\t\treturn __fls(*reg) + vec;\n\t}\n\n\treturn -1;\n}\n\nstatic u8 count_vectors(void *bitmap)\n{\n\tint vec;\n\tu32 *reg;\n\tu8 count = 0;\n\n\tfor (vec = 0; vec < MAX_APIC_VECTOR; vec += APIC_VECTORS_PER_REG) {\n\t\treg = bitmap + REG_POS(vec);\n\t\tcount += hweight32(*reg);\n\t}\n\n\treturn count;\n}\n\nbool __kvm_apic_update_irr(u32 *pir, void *regs, int *max_irr)\n{\n\tu32 i, vec;\n\tu32 pir_val, irr_val, prev_irr_val;\n\tint max_updated_irr;\n\n\tmax_updated_irr = -1;\n\t*max_irr = -1;\n\n\tfor (i = vec = 0; i <= 7; i++, vec += 32) {\n\t\tu32 *p_irr = (u32 *)(regs + APIC_IRR + i * 0x10);\n\n\t\tirr_val = *p_irr;\n\t\tpir_val = READ_ONCE(pir[i]);\n\n\t\tif (pir_val) {\n\t\t\tpir_val = xchg(&pir[i], 0);\n\n\t\t\tprev_irr_val = irr_val;\n\t\t\tdo {\n\t\t\t\tirr_val = prev_irr_val | pir_val;\n\t\t\t} while (prev_irr_val != irr_val &&\n\t\t\t\t !try_cmpxchg(p_irr, &prev_irr_val, irr_val));\n\n\t\t\tif (prev_irr_val != irr_val)\n\t\t\t\tmax_updated_irr = __fls(irr_val ^ prev_irr_val) + vec;\n\t\t}\n\t\tif (irr_val)\n\t\t\t*max_irr = __fls(irr_val) + vec;\n\t}\n\n\treturn ((max_updated_irr != -1) &&\n\t\t(max_updated_irr == *max_irr));\n}\nEXPORT_SYMBOL_GPL(__kvm_apic_update_irr);\n\nbool kvm_apic_update_irr(struct kvm_vcpu *vcpu, u32 *pir, int *max_irr)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tbool irr_updated = __kvm_apic_update_irr(pir, apic->regs, max_irr);\n\n\tif (unlikely(!apic->apicv_active && irr_updated))\n\t\tapic->irr_pending = true;\n\treturn irr_updated;\n}\nEXPORT_SYMBOL_GPL(kvm_apic_update_irr);\n\nstatic inline int apic_search_irr(struct kvm_lapic *apic)\n{\n\treturn find_highest_vector(apic->regs + APIC_IRR);\n}\n\nstatic inline int apic_find_highest_irr(struct kvm_lapic *apic)\n{\n\tint result;\n\n\t \n\tif (!apic->irr_pending)\n\t\treturn -1;\n\n\tresult = apic_search_irr(apic);\n\tASSERT(result == -1 || result >= 16);\n\n\treturn result;\n}\n\nstatic inline void apic_clear_irr(int vec, struct kvm_lapic *apic)\n{\n\tif (unlikely(apic->apicv_active)) {\n\t\t \n\t\tkvm_lapic_clear_vector(vec, apic->regs + APIC_IRR);\n\t\tstatic_call_cond(kvm_x86_hwapic_irr_update)(apic->vcpu,\n\t\t\t\t\t\t\t    apic_find_highest_irr(apic));\n\t} else {\n\t\tapic->irr_pending = false;\n\t\tkvm_lapic_clear_vector(vec, apic->regs + APIC_IRR);\n\t\tif (apic_search_irr(apic) != -1)\n\t\t\tapic->irr_pending = true;\n\t}\n}\n\nvoid kvm_apic_clear_irr(struct kvm_vcpu *vcpu, int vec)\n{\n\tapic_clear_irr(vec, vcpu->arch.apic);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_clear_irr);\n\nstatic inline void apic_set_isr(int vec, struct kvm_lapic *apic)\n{\n\tif (__apic_test_and_set_vector(vec, apic->regs + APIC_ISR))\n\t\treturn;\n\n\t \n\tif (unlikely(apic->apicv_active))\n\t\tstatic_call_cond(kvm_x86_hwapic_isr_update)(vec);\n\telse {\n\t\t++apic->isr_count;\n\t\tBUG_ON(apic->isr_count > MAX_APIC_VECTOR);\n\t\t \n\t\tapic->highest_isr_cache = vec;\n\t}\n}\n\nstatic inline int apic_find_highest_isr(struct kvm_lapic *apic)\n{\n\tint result;\n\n\t \n\tif (!apic->isr_count)\n\t\treturn -1;\n\tif (likely(apic->highest_isr_cache != -1))\n\t\treturn apic->highest_isr_cache;\n\n\tresult = find_highest_vector(apic->regs + APIC_ISR);\n\tASSERT(result == -1 || result >= 16);\n\n\treturn result;\n}\n\nstatic inline void apic_clear_isr(int vec, struct kvm_lapic *apic)\n{\n\tif (!__apic_test_and_clear_vector(vec, apic->regs + APIC_ISR))\n\t\treturn;\n\n\t \n\tif (unlikely(apic->apicv_active))\n\t\tstatic_call_cond(kvm_x86_hwapic_isr_update)(apic_find_highest_isr(apic));\n\telse {\n\t\t--apic->isr_count;\n\t\tBUG_ON(apic->isr_count < 0);\n\t\tapic->highest_isr_cache = -1;\n\t}\n}\n\nint kvm_lapic_find_highest_irr(struct kvm_vcpu *vcpu)\n{\n\t \n\treturn apic_find_highest_irr(vcpu->arch.apic);\n}\nEXPORT_SYMBOL_GPL(kvm_lapic_find_highest_irr);\n\nstatic int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,\n\t\t\t     int vector, int level, int trig_mode,\n\t\t\t     struct dest_map *dest_map);\n\nint kvm_apic_set_irq(struct kvm_vcpu *vcpu, struct kvm_lapic_irq *irq,\n\t\t     struct dest_map *dest_map)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\treturn __apic_accept_irq(apic, irq->delivery_mode, irq->vector,\n\t\t\tirq->level, irq->trig_mode, dest_map);\n}\n\nstatic int __pv_send_ipi(unsigned long *ipi_bitmap, struct kvm_apic_map *map,\n\t\t\t struct kvm_lapic_irq *irq, u32 min)\n{\n\tint i, count = 0;\n\tstruct kvm_vcpu *vcpu;\n\n\tif (min > map->max_apic_id)\n\t\treturn 0;\n\n\tfor_each_set_bit(i, ipi_bitmap,\n\t\tmin((u32)BITS_PER_LONG, (map->max_apic_id - min + 1))) {\n\t\tif (map->phys_map[min + i]) {\n\t\t\tvcpu = map->phys_map[min + i]->vcpu;\n\t\t\tcount += kvm_apic_set_irq(vcpu, irq, NULL);\n\t\t}\n\t}\n\n\treturn count;\n}\n\nint kvm_pv_send_ipi(struct kvm *kvm, unsigned long ipi_bitmap_low,\n\t\t    unsigned long ipi_bitmap_high, u32 min,\n\t\t    unsigned long icr, int op_64_bit)\n{\n\tstruct kvm_apic_map *map;\n\tstruct kvm_lapic_irq irq = {0};\n\tint cluster_size = op_64_bit ? 64 : 32;\n\tint count;\n\n\tif (icr & (APIC_DEST_MASK | APIC_SHORT_MASK))\n\t\treturn -KVM_EINVAL;\n\n\tirq.vector = icr & APIC_VECTOR_MASK;\n\tirq.delivery_mode = icr & APIC_MODE_MASK;\n\tirq.level = (icr & APIC_INT_ASSERT) != 0;\n\tirq.trig_mode = icr & APIC_INT_LEVELTRIG;\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tcount = -EOPNOTSUPP;\n\tif (likely(map)) {\n\t\tcount = __pv_send_ipi(&ipi_bitmap_low, map, &irq, min);\n\t\tmin += cluster_size;\n\t\tcount += __pv_send_ipi(&ipi_bitmap_high, map, &irq, min);\n\t}\n\n\trcu_read_unlock();\n\treturn count;\n}\n\nstatic int pv_eoi_put_user(struct kvm_vcpu *vcpu, u8 val)\n{\n\n\treturn kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.pv_eoi.data, &val,\n\t\t\t\t      sizeof(val));\n}\n\nstatic int pv_eoi_get_user(struct kvm_vcpu *vcpu, u8 *val)\n{\n\n\treturn kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.pv_eoi.data, val,\n\t\t\t\t      sizeof(*val));\n}\n\nstatic inline bool pv_eoi_enabled(struct kvm_vcpu *vcpu)\n{\n\treturn vcpu->arch.pv_eoi.msr_val & KVM_MSR_ENABLED;\n}\n\nstatic void pv_eoi_set_pending(struct kvm_vcpu *vcpu)\n{\n\tif (pv_eoi_put_user(vcpu, KVM_PV_EOI_ENABLED) < 0)\n\t\treturn;\n\n\t__set_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);\n}\n\nstatic bool pv_eoi_test_and_clr_pending(struct kvm_vcpu *vcpu)\n{\n\tu8 val;\n\n\tif (pv_eoi_get_user(vcpu, &val) < 0)\n\t\treturn false;\n\n\tval &= KVM_PV_EOI_ENABLED;\n\n\tif (val && pv_eoi_put_user(vcpu, KVM_PV_EOI_DISABLED) < 0)\n\t\treturn false;\n\n\t \n\t__clear_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention);\n\n\treturn val;\n}\n\nstatic int apic_has_interrupt_for_ppr(struct kvm_lapic *apic, u32 ppr)\n{\n\tint highest_irr;\n\tif (kvm_x86_ops.sync_pir_to_irr)\n\t\thighest_irr = static_call(kvm_x86_sync_pir_to_irr)(apic->vcpu);\n\telse\n\t\thighest_irr = apic_find_highest_irr(apic);\n\tif (highest_irr == -1 || (highest_irr & 0xF0) <= ppr)\n\t\treturn -1;\n\treturn highest_irr;\n}\n\nstatic bool __apic_update_ppr(struct kvm_lapic *apic, u32 *new_ppr)\n{\n\tu32 tpr, isrv, ppr, old_ppr;\n\tint isr;\n\n\told_ppr = kvm_lapic_get_reg(apic, APIC_PROCPRI);\n\ttpr = kvm_lapic_get_reg(apic, APIC_TASKPRI);\n\tisr = apic_find_highest_isr(apic);\n\tisrv = (isr != -1) ? isr : 0;\n\n\tif ((tpr & 0xf0) >= (isrv & 0xf0))\n\t\tppr = tpr & 0xff;\n\telse\n\t\tppr = isrv & 0xf0;\n\n\t*new_ppr = ppr;\n\tif (old_ppr != ppr)\n\t\tkvm_lapic_set_reg(apic, APIC_PROCPRI, ppr);\n\n\treturn ppr < old_ppr;\n}\n\nstatic void apic_update_ppr(struct kvm_lapic *apic)\n{\n\tu32 ppr;\n\n\tif (__apic_update_ppr(apic, &ppr) &&\n\t    apic_has_interrupt_for_ppr(apic, ppr) != -1)\n\t\tkvm_make_request(KVM_REQ_EVENT, apic->vcpu);\n}\n\nvoid kvm_apic_update_ppr(struct kvm_vcpu *vcpu)\n{\n\tapic_update_ppr(vcpu->arch.apic);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_update_ppr);\n\nstatic void apic_set_tpr(struct kvm_lapic *apic, u32 tpr)\n{\n\tkvm_lapic_set_reg(apic, APIC_TASKPRI, tpr);\n\tapic_update_ppr(apic);\n}\n\nstatic bool kvm_apic_broadcast(struct kvm_lapic *apic, u32 mda)\n{\n\treturn mda == (apic_x2apic_mode(apic) ?\n\t\t\tX2APIC_BROADCAST : APIC_BROADCAST);\n}\n\nstatic bool kvm_apic_match_physical_addr(struct kvm_lapic *apic, u32 mda)\n{\n\tif (kvm_apic_broadcast(apic, mda))\n\t\treturn true;\n\n\t \n\tif (apic_x2apic_mode(apic) || mda > 0xff)\n\t\treturn mda == kvm_x2apic_id(apic);\n\n\treturn mda == kvm_xapic_id(apic);\n}\n\nstatic bool kvm_apic_match_logical_addr(struct kvm_lapic *apic, u32 mda)\n{\n\tu32 logical_id;\n\n\tif (kvm_apic_broadcast(apic, mda))\n\t\treturn true;\n\n\tlogical_id = kvm_lapic_get_reg(apic, APIC_LDR);\n\n\tif (apic_x2apic_mode(apic))\n\t\treturn ((logical_id >> 16) == (mda >> 16))\n\t\t       && (logical_id & mda & 0xffff) != 0;\n\n\tlogical_id = GET_APIC_LOGICAL_ID(logical_id);\n\n\tswitch (kvm_lapic_get_reg(apic, APIC_DFR)) {\n\tcase APIC_DFR_FLAT:\n\t\treturn (logical_id & mda) != 0;\n\tcase APIC_DFR_CLUSTER:\n\t\treturn ((logical_id >> 4) == (mda >> 4))\n\t\t       && (logical_id & mda & 0xf) != 0;\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n \nstatic u32 kvm_apic_mda(struct kvm_vcpu *vcpu, unsigned int dest_id,\n\t\tstruct kvm_lapic *source, struct kvm_lapic *target)\n{\n\tbool ipi = source != NULL;\n\n\tif (!vcpu->kvm->arch.x2apic_broadcast_quirk_disabled &&\n\t    !ipi && dest_id == APIC_BROADCAST && apic_x2apic_mode(target))\n\t\treturn X2APIC_BROADCAST;\n\n\treturn dest_id;\n}\n\nbool kvm_apic_match_dest(struct kvm_vcpu *vcpu, struct kvm_lapic *source,\n\t\t\t   int shorthand, unsigned int dest, int dest_mode)\n{\n\tstruct kvm_lapic *target = vcpu->arch.apic;\n\tu32 mda = kvm_apic_mda(vcpu, dest, source, target);\n\n\tASSERT(target);\n\tswitch (shorthand) {\n\tcase APIC_DEST_NOSHORT:\n\t\tif (dest_mode == APIC_DEST_PHYSICAL)\n\t\t\treturn kvm_apic_match_physical_addr(target, mda);\n\t\telse\n\t\t\treturn kvm_apic_match_logical_addr(target, mda);\n\tcase APIC_DEST_SELF:\n\t\treturn target == source;\n\tcase APIC_DEST_ALLINC:\n\t\treturn true;\n\tcase APIC_DEST_ALLBUT:\n\t\treturn target != source;\n\tdefault:\n\t\treturn false;\n\t}\n}\nEXPORT_SYMBOL_GPL(kvm_apic_match_dest);\n\nint kvm_vector_to_index(u32 vector, u32 dest_vcpus,\n\t\t       const unsigned long *bitmap, u32 bitmap_size)\n{\n\tu32 mod;\n\tint i, idx = -1;\n\n\tmod = vector % dest_vcpus;\n\n\tfor (i = 0; i <= mod; i++) {\n\t\tidx = find_next_bit(bitmap, bitmap_size, idx + 1);\n\t\tBUG_ON(idx == bitmap_size);\n\t}\n\n\treturn idx;\n}\n\nstatic void kvm_apic_disabled_lapic_found(struct kvm *kvm)\n{\n\tif (!kvm->arch.disabled_lapic_found) {\n\t\tkvm->arch.disabled_lapic_found = true;\n\t\tpr_info(\"Disabled LAPIC found during irq injection\\n\");\n\t}\n}\n\nstatic bool kvm_apic_is_broadcast_dest(struct kvm *kvm, struct kvm_lapic **src,\n\t\tstruct kvm_lapic_irq *irq, struct kvm_apic_map *map)\n{\n\tif (kvm->arch.x2apic_broadcast_quirk_disabled) {\n\t\tif ((irq->dest_id == APIC_BROADCAST &&\n\t\t     map->logical_mode != KVM_APIC_MODE_X2APIC))\n\t\t\treturn true;\n\t\tif (irq->dest_id == X2APIC_BROADCAST)\n\t\t\treturn true;\n\t} else {\n\t\tbool x2apic_ipi = src && *src && apic_x2apic_mode(*src);\n\t\tif (irq->dest_id == (x2apic_ipi ?\n\t\t                     X2APIC_BROADCAST : APIC_BROADCAST))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\n \nstatic inline bool kvm_apic_map_get_dest_lapic(struct kvm *kvm,\n\t\tstruct kvm_lapic **src, struct kvm_lapic_irq *irq,\n\t\tstruct kvm_apic_map *map, struct kvm_lapic ***dst,\n\t\tunsigned long *bitmap)\n{\n\tint i, lowest;\n\n\tif (irq->shorthand == APIC_DEST_SELF && src) {\n\t\t*dst = src;\n\t\t*bitmap = 1;\n\t\treturn true;\n\t} else if (irq->shorthand)\n\t\treturn false;\n\n\tif (!map || kvm_apic_is_broadcast_dest(kvm, src, irq, map))\n\t\treturn false;\n\n\tif (irq->dest_mode == APIC_DEST_PHYSICAL) {\n\t\tif (irq->dest_id > map->max_apic_id) {\n\t\t\t*bitmap = 0;\n\t\t} else {\n\t\t\tu32 dest_id = array_index_nospec(irq->dest_id, map->max_apic_id + 1);\n\t\t\t*dst = &map->phys_map[dest_id];\n\t\t\t*bitmap = 1;\n\t\t}\n\t\treturn true;\n\t}\n\n\t*bitmap = 0;\n\tif (!kvm_apic_map_get_logical_dest(map, irq->dest_id, dst,\n\t\t\t\t(u16 *)bitmap))\n\t\treturn false;\n\n\tif (!kvm_lowest_prio_delivery(irq))\n\t\treturn true;\n\n\tif (!kvm_vector_hashing_enabled()) {\n\t\tlowest = -1;\n\t\tfor_each_set_bit(i, bitmap, 16) {\n\t\t\tif (!(*dst)[i])\n\t\t\t\tcontinue;\n\t\t\tif (lowest < 0)\n\t\t\t\tlowest = i;\n\t\t\telse if (kvm_apic_compare_prio((*dst)[i]->vcpu,\n\t\t\t\t\t\t(*dst)[lowest]->vcpu) < 0)\n\t\t\t\tlowest = i;\n\t\t}\n\t} else {\n\t\tif (!*bitmap)\n\t\t\treturn true;\n\n\t\tlowest = kvm_vector_to_index(irq->vector, hweight16(*bitmap),\n\t\t\t\tbitmap, 16);\n\n\t\tif (!(*dst)[lowest]) {\n\t\t\tkvm_apic_disabled_lapic_found(kvm);\n\t\t\t*bitmap = 0;\n\t\t\treturn true;\n\t\t}\n\t}\n\n\t*bitmap = (lowest >= 0) ? 1 << lowest : 0;\n\n\treturn true;\n}\n\nbool kvm_irq_delivery_to_apic_fast(struct kvm *kvm, struct kvm_lapic *src,\n\t\tstruct kvm_lapic_irq *irq, int *r, struct dest_map *dest_map)\n{\n\tstruct kvm_apic_map *map;\n\tunsigned long bitmap;\n\tstruct kvm_lapic **dst = NULL;\n\tint i;\n\tbool ret;\n\n\t*r = -1;\n\n\tif (irq->shorthand == APIC_DEST_SELF) {\n\t\tif (KVM_BUG_ON(!src, kvm)) {\n\t\t\t*r = 0;\n\t\t\treturn true;\n\t\t}\n\t\t*r = kvm_apic_set_irq(src->vcpu, irq, dest_map);\n\t\treturn true;\n\t}\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dst, &bitmap);\n\tif (ret) {\n\t\t*r = 0;\n\t\tfor_each_set_bit(i, &bitmap, 16) {\n\t\t\tif (!dst[i])\n\t\t\t\tcontinue;\n\t\t\t*r += kvm_apic_set_irq(dst[i]->vcpu, irq, dest_map);\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\treturn ret;\n}\n\n \nbool kvm_intr_is_single_vcpu_fast(struct kvm *kvm, struct kvm_lapic_irq *irq,\n\t\t\tstruct kvm_vcpu **dest_vcpu)\n{\n\tstruct kvm_apic_map *map;\n\tunsigned long bitmap;\n\tstruct kvm_lapic **dst = NULL;\n\tbool ret = false;\n\n\tif (irq->shorthand)\n\t\treturn false;\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tif (kvm_apic_map_get_dest_lapic(kvm, NULL, irq, map, &dst, &bitmap) &&\n\t\t\thweight16(bitmap) == 1) {\n\t\tunsigned long i = find_first_bit(&bitmap, 16);\n\n\t\tif (dst[i]) {\n\t\t\t*dest_vcpu = dst[i]->vcpu;\n\t\t\tret = true;\n\t\t}\n\t}\n\n\trcu_read_unlock();\n\treturn ret;\n}\n\n \nstatic int __apic_accept_irq(struct kvm_lapic *apic, int delivery_mode,\n\t\t\t     int vector, int level, int trig_mode,\n\t\t\t     struct dest_map *dest_map)\n{\n\tint result = 0;\n\tstruct kvm_vcpu *vcpu = apic->vcpu;\n\n\ttrace_kvm_apic_accept_irq(vcpu->vcpu_id, delivery_mode,\n\t\t\t\t  trig_mode, vector);\n\tswitch (delivery_mode) {\n\tcase APIC_DM_LOWEST:\n\t\tvcpu->arch.apic_arb_prio++;\n\t\tfallthrough;\n\tcase APIC_DM_FIXED:\n\t\tif (unlikely(trig_mode && !level))\n\t\t\tbreak;\n\n\t\t \n\t\tif (unlikely(!apic_enabled(apic)))\n\t\t\tbreak;\n\n\t\tresult = 1;\n\n\t\tif (dest_map) {\n\t\t\t__set_bit(vcpu->vcpu_id, dest_map->map);\n\t\t\tdest_map->vectors[vcpu->vcpu_id] = vector;\n\t\t}\n\n\t\tif (apic_test_vector(vector, apic->regs + APIC_TMR) != !!trig_mode) {\n\t\t\tif (trig_mode)\n\t\t\t\tkvm_lapic_set_vector(vector,\n\t\t\t\t\t\t     apic->regs + APIC_TMR);\n\t\t\telse\n\t\t\t\tkvm_lapic_clear_vector(vector,\n\t\t\t\t\t\t       apic->regs + APIC_TMR);\n\t\t}\n\n\t\tstatic_call(kvm_x86_deliver_interrupt)(apic, delivery_mode,\n\t\t\t\t\t\t       trig_mode, vector);\n\t\tbreak;\n\n\tcase APIC_DM_REMRD:\n\t\tresult = 1;\n\t\tvcpu->arch.pv.pv_unhalted = 1;\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t\tbreak;\n\n\tcase APIC_DM_SMI:\n\t\tif (!kvm_inject_smi(vcpu)) {\n\t\t\tkvm_vcpu_kick(vcpu);\n\t\t\tresult = 1;\n\t\t}\n\t\tbreak;\n\n\tcase APIC_DM_NMI:\n\t\tresult = 1;\n\t\tkvm_inject_nmi(vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t\tbreak;\n\n\tcase APIC_DM_INIT:\n\t\tif (!trig_mode || level) {\n\t\t\tresult = 1;\n\t\t\t \n\t\t\tapic->pending_events = (1UL << KVM_APIC_INIT);\n\t\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\t\tkvm_vcpu_kick(vcpu);\n\t\t}\n\t\tbreak;\n\n\tcase APIC_DM_STARTUP:\n\t\tresult = 1;\n\t\tapic->sipi_vector = vector;\n\t\t \n\t\tsmp_wmb();\n\t\tset_bit(KVM_APIC_SIPI, &apic->pending_events);\n\t\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\t\tkvm_vcpu_kick(vcpu);\n\t\tbreak;\n\n\tcase APIC_DM_EXTINT:\n\t\t \n\t\tbreak;\n\n\tdefault:\n\t\tprintk(KERN_ERR \"TODO: unsupported delivery mode %x\\n\",\n\t\t       delivery_mode);\n\t\tbreak;\n\t}\n\treturn result;\n}\n\n \nvoid kvm_bitmap_or_dest_vcpus(struct kvm *kvm, struct kvm_lapic_irq *irq,\n\t\t\t      unsigned long *vcpu_bitmap)\n{\n\tstruct kvm_lapic **dest_vcpu = NULL;\n\tstruct kvm_lapic *src = NULL;\n\tstruct kvm_apic_map *map;\n\tstruct kvm_vcpu *vcpu;\n\tunsigned long bitmap, i;\n\tint vcpu_idx;\n\tbool ret;\n\n\trcu_read_lock();\n\tmap = rcu_dereference(kvm->arch.apic_map);\n\n\tret = kvm_apic_map_get_dest_lapic(kvm, &src, irq, map, &dest_vcpu,\n\t\t\t\t\t  &bitmap);\n\tif (ret) {\n\t\tfor_each_set_bit(i, &bitmap, 16) {\n\t\t\tif (!dest_vcpu[i])\n\t\t\t\tcontinue;\n\t\t\tvcpu_idx = dest_vcpu[i]->vcpu->vcpu_idx;\n\t\t\t__set_bit(vcpu_idx, vcpu_bitmap);\n\t\t}\n\t} else {\n\t\tkvm_for_each_vcpu(i, vcpu, kvm) {\n\t\t\tif (!kvm_apic_present(vcpu))\n\t\t\t\tcontinue;\n\t\t\tif (!kvm_apic_match_dest(vcpu, NULL,\n\t\t\t\t\t\t irq->shorthand,\n\t\t\t\t\t\t irq->dest_id,\n\t\t\t\t\t\t irq->dest_mode))\n\t\t\t\tcontinue;\n\t\t\t__set_bit(i, vcpu_bitmap);\n\t\t}\n\t}\n\trcu_read_unlock();\n}\n\nint kvm_apic_compare_prio(struct kvm_vcpu *vcpu1, struct kvm_vcpu *vcpu2)\n{\n\treturn vcpu1->arch.apic_arb_prio - vcpu2->arch.apic_arb_prio;\n}\n\nstatic bool kvm_ioapic_handles_vector(struct kvm_lapic *apic, int vector)\n{\n\treturn test_bit(vector, apic->vcpu->arch.ioapic_handled_vectors);\n}\n\nstatic void kvm_ioapic_send_eoi(struct kvm_lapic *apic, int vector)\n{\n\tint trigger_mode;\n\n\t \n\tif (!kvm_ioapic_handles_vector(apic, vector))\n\t\treturn;\n\n\t \n\tif (irqchip_split(apic->vcpu->kvm)) {\n\t\tapic->vcpu->arch.pending_ioapic_eoi = vector;\n\t\tkvm_make_request(KVM_REQ_IOAPIC_EOI_EXIT, apic->vcpu);\n\t\treturn;\n\t}\n\n\tif (apic_test_vector(vector, apic->regs + APIC_TMR))\n\t\ttrigger_mode = IOAPIC_LEVEL_TRIG;\n\telse\n\t\ttrigger_mode = IOAPIC_EDGE_TRIG;\n\n\tkvm_ioapic_update_eoi(apic->vcpu, vector, trigger_mode);\n}\n\nstatic int apic_set_eoi(struct kvm_lapic *apic)\n{\n\tint vector = apic_find_highest_isr(apic);\n\n\ttrace_kvm_eoi(apic, vector);\n\n\t \n\tif (vector == -1)\n\t\treturn vector;\n\n\tapic_clear_isr(vector, apic);\n\tapic_update_ppr(apic);\n\n\tif (to_hv_vcpu(apic->vcpu) &&\n\t    test_bit(vector, to_hv_synic(apic->vcpu)->vec_bitmap))\n\t\tkvm_hv_synic_send_eoi(apic->vcpu, vector);\n\n\tkvm_ioapic_send_eoi(apic, vector);\n\tkvm_make_request(KVM_REQ_EVENT, apic->vcpu);\n\treturn vector;\n}\n\n \nvoid kvm_apic_set_eoi_accelerated(struct kvm_vcpu *vcpu, int vector)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\ttrace_kvm_eoi(apic, vector);\n\n\tkvm_ioapic_send_eoi(apic, vector);\n\tkvm_make_request(KVM_REQ_EVENT, apic->vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_set_eoi_accelerated);\n\nvoid kvm_apic_send_ipi(struct kvm_lapic *apic, u32 icr_low, u32 icr_high)\n{\n\tstruct kvm_lapic_irq irq;\n\n\t \n\tWARN_ON_ONCE(icr_low & APIC_ICR_BUSY);\n\n\tirq.vector = icr_low & APIC_VECTOR_MASK;\n\tirq.delivery_mode = icr_low & APIC_MODE_MASK;\n\tirq.dest_mode = icr_low & APIC_DEST_MASK;\n\tirq.level = (icr_low & APIC_INT_ASSERT) != 0;\n\tirq.trig_mode = icr_low & APIC_INT_LEVELTRIG;\n\tirq.shorthand = icr_low & APIC_SHORT_MASK;\n\tirq.msi_redir_hint = false;\n\tif (apic_x2apic_mode(apic))\n\t\tirq.dest_id = icr_high;\n\telse\n\t\tirq.dest_id = GET_XAPIC_DEST_FIELD(icr_high);\n\n\ttrace_kvm_apic_ipi(icr_low, irq.dest_id);\n\n\tkvm_irq_delivery_to_apic(apic->vcpu->kvm, apic, &irq, NULL);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_send_ipi);\n\nstatic u32 apic_get_tmcct(struct kvm_lapic *apic)\n{\n\tktime_t remaining, now;\n\ts64 ns;\n\n\tASSERT(apic != NULL);\n\n\t \n\tif (kvm_lapic_get_reg(apic, APIC_TMICT) == 0 ||\n\t\tapic->lapic_timer.period == 0)\n\t\treturn 0;\n\n\tnow = ktime_get();\n\tremaining = ktime_sub(apic->lapic_timer.target_expiration, now);\n\tif (ktime_to_ns(remaining) < 0)\n\t\tremaining = 0;\n\n\tns = mod_64(ktime_to_ns(remaining), apic->lapic_timer.period);\n\treturn div64_u64(ns, (APIC_BUS_CYCLE_NS * apic->divide_count));\n}\n\nstatic void __report_tpr_access(struct kvm_lapic *apic, bool write)\n{\n\tstruct kvm_vcpu *vcpu = apic->vcpu;\n\tstruct kvm_run *run = vcpu->run;\n\n\tkvm_make_request(KVM_REQ_REPORT_TPR_ACCESS, vcpu);\n\trun->tpr_access.rip = kvm_rip_read(vcpu);\n\trun->tpr_access.is_write = write;\n}\n\nstatic inline void report_tpr_access(struct kvm_lapic *apic, bool write)\n{\n\tif (apic->vcpu->arch.tpr_access_reporting)\n\t\t__report_tpr_access(apic, write);\n}\n\nstatic u32 __apic_read(struct kvm_lapic *apic, unsigned int offset)\n{\n\tu32 val = 0;\n\n\tif (offset >= LAPIC_MMIO_LENGTH)\n\t\treturn 0;\n\n\tswitch (offset) {\n\tcase APIC_ARBPRI:\n\t\tbreak;\n\n\tcase APIC_TMCCT:\t \n\t\tif (apic_lvtt_tscdeadline(apic))\n\t\t\treturn 0;\n\n\t\tval = apic_get_tmcct(apic);\n\t\tbreak;\n\tcase APIC_PROCPRI:\n\t\tapic_update_ppr(apic);\n\t\tval = kvm_lapic_get_reg(apic, offset);\n\t\tbreak;\n\tcase APIC_TASKPRI:\n\t\treport_tpr_access(apic, false);\n\t\tfallthrough;\n\tdefault:\n\t\tval = kvm_lapic_get_reg(apic, offset);\n\t\tbreak;\n\t}\n\n\treturn val;\n}\n\nstatic inline struct kvm_lapic *to_lapic(struct kvm_io_device *dev)\n{\n\treturn container_of(dev, struct kvm_lapic, dev);\n}\n\n#define APIC_REG_MASK(reg)\t(1ull << ((reg) >> 4))\n#define APIC_REGS_MASK(first, count) \\\n\t(APIC_REG_MASK(first) * ((1ull << (count)) - 1))\n\nu64 kvm_lapic_readable_reg_mask(struct kvm_lapic *apic)\n{\n\t \n\tu64 valid_reg_mask =\n\t\tAPIC_REG_MASK(APIC_ID) |\n\t\tAPIC_REG_MASK(APIC_LVR) |\n\t\tAPIC_REG_MASK(APIC_TASKPRI) |\n\t\tAPIC_REG_MASK(APIC_PROCPRI) |\n\t\tAPIC_REG_MASK(APIC_LDR) |\n\t\tAPIC_REG_MASK(APIC_SPIV) |\n\t\tAPIC_REGS_MASK(APIC_ISR, APIC_ISR_NR) |\n\t\tAPIC_REGS_MASK(APIC_TMR, APIC_ISR_NR) |\n\t\tAPIC_REGS_MASK(APIC_IRR, APIC_ISR_NR) |\n\t\tAPIC_REG_MASK(APIC_ESR) |\n\t\tAPIC_REG_MASK(APIC_ICR) |\n\t\tAPIC_REG_MASK(APIC_LVTT) |\n\t\tAPIC_REG_MASK(APIC_LVTTHMR) |\n\t\tAPIC_REG_MASK(APIC_LVTPC) |\n\t\tAPIC_REG_MASK(APIC_LVT0) |\n\t\tAPIC_REG_MASK(APIC_LVT1) |\n\t\tAPIC_REG_MASK(APIC_LVTERR) |\n\t\tAPIC_REG_MASK(APIC_TMICT) |\n\t\tAPIC_REG_MASK(APIC_TMCCT) |\n\t\tAPIC_REG_MASK(APIC_TDCR);\n\n\tif (kvm_lapic_lvt_supported(apic, LVT_CMCI))\n\t\tvalid_reg_mask |= APIC_REG_MASK(APIC_LVTCMCI);\n\n\t \n\tif (!apic_x2apic_mode(apic))\n\t\tvalid_reg_mask |= APIC_REG_MASK(APIC_ARBPRI) |\n\t\t\t\t  APIC_REG_MASK(APIC_DFR) |\n\t\t\t\t  APIC_REG_MASK(APIC_ICR2);\n\n\treturn valid_reg_mask;\n}\nEXPORT_SYMBOL_GPL(kvm_lapic_readable_reg_mask);\n\nstatic int kvm_lapic_reg_read(struct kvm_lapic *apic, u32 offset, int len,\n\t\t\t      void *data)\n{\n\tunsigned char alignment = offset & 0xf;\n\tu32 result;\n\n\t \n\tWARN_ON_ONCE(apic_x2apic_mode(apic) && offset == APIC_ICR);\n\n\tif (alignment + len > 4)\n\t\treturn 1;\n\n\tif (offset > 0x3f0 ||\n\t    !(kvm_lapic_readable_reg_mask(apic) & APIC_REG_MASK(offset)))\n\t\treturn 1;\n\n\tresult = __apic_read(apic, offset & ~0xf);\n\n\ttrace_kvm_apic_read(offset, result);\n\n\tswitch (len) {\n\tcase 1:\n\tcase 2:\n\tcase 4:\n\t\tmemcpy(data, (char *)&result + alignment, len);\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_ERR \"Local APIC read with len = %x, \"\n\t\t       \"should be 1,2, or 4 instead\\n\", len);\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int apic_mmio_in_range(struct kvm_lapic *apic, gpa_t addr)\n{\n\treturn addr >= apic->base_address &&\n\t\taddr < apic->base_address + LAPIC_MMIO_LENGTH;\n}\n\nstatic int apic_mmio_read(struct kvm_vcpu *vcpu, struct kvm_io_device *this,\n\t\t\t   gpa_t address, int len, void *data)\n{\n\tstruct kvm_lapic *apic = to_lapic(this);\n\tu32 offset = address - apic->base_address;\n\n\tif (!apic_mmio_in_range(apic, address))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {\n\t\tif (!kvm_check_has_quirk(vcpu->kvm,\n\t\t\t\t\t KVM_X86_QUIRK_LAPIC_MMIO_HOLE))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\tmemset(data, 0xff, len);\n\t\treturn 0;\n\t}\n\n\tkvm_lapic_reg_read(apic, offset, len, data);\n\n\treturn 0;\n}\n\nstatic void update_divide_count(struct kvm_lapic *apic)\n{\n\tu32 tmp1, tmp2, tdcr;\n\n\ttdcr = kvm_lapic_get_reg(apic, APIC_TDCR);\n\ttmp1 = tdcr & 0xf;\n\ttmp2 = ((tmp1 & 0x3) | ((tmp1 & 0x8) >> 1)) + 1;\n\tapic->divide_count = 0x1 << (tmp2 & 0x7);\n}\n\nstatic void limit_periodic_timer_frequency(struct kvm_lapic *apic)\n{\n\t \n\tif (apic_lvtt_period(apic) && apic->lapic_timer.period) {\n\t\ts64 min_period = min_timer_period_us * 1000LL;\n\n\t\tif (apic->lapic_timer.period < min_period) {\n\t\t\tpr_info_ratelimited(\n\t\t\t    \"vcpu %i: requested %lld ns \"\n\t\t\t    \"lapic timer period limited to %lld ns\\n\",\n\t\t\t    apic->vcpu->vcpu_id,\n\t\t\t    apic->lapic_timer.period, min_period);\n\t\t\tapic->lapic_timer.period = min_period;\n\t\t}\n\t}\n}\n\nstatic void cancel_hv_timer(struct kvm_lapic *apic);\n\nstatic void cancel_apic_timer(struct kvm_lapic *apic)\n{\n\thrtimer_cancel(&apic->lapic_timer.timer);\n\tpreempt_disable();\n\tif (apic->lapic_timer.hv_timer_in_use)\n\t\tcancel_hv_timer(apic);\n\tpreempt_enable();\n\tatomic_set(&apic->lapic_timer.pending, 0);\n}\n\nstatic void apic_update_lvtt(struct kvm_lapic *apic)\n{\n\tu32 timer_mode = kvm_lapic_get_reg(apic, APIC_LVTT) &\n\t\t\tapic->lapic_timer.timer_mode_mask;\n\n\tif (apic->lapic_timer.timer_mode != timer_mode) {\n\t\tif (apic_lvtt_tscdeadline(apic) != (timer_mode ==\n\t\t\t\tAPIC_LVT_TIMER_TSCDEADLINE)) {\n\t\t\tcancel_apic_timer(apic);\n\t\t\tkvm_lapic_set_reg(apic, APIC_TMICT, 0);\n\t\t\tapic->lapic_timer.period = 0;\n\t\t\tapic->lapic_timer.tscdeadline = 0;\n\t\t}\n\t\tapic->lapic_timer.timer_mode = timer_mode;\n\t\tlimit_periodic_timer_frequency(apic);\n\t}\n}\n\n \n\nstatic bool lapic_timer_int_injected(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 reg = kvm_lapic_get_reg(apic, APIC_LVTT);\n\n\tif (kvm_apic_hw_enabled(apic)) {\n\t\tint vec = reg & APIC_VECTOR_MASK;\n\t\tvoid *bitmap = apic->regs + APIC_ISR;\n\n\t\tif (apic->apicv_active)\n\t\t\tbitmap = apic->regs + APIC_IRR;\n\n\t\tif (apic_test_vector(vec, bitmap))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline void __wait_lapic_expire(struct kvm_vcpu *vcpu, u64 guest_cycles)\n{\n\tu64 timer_advance_ns = vcpu->arch.apic->lapic_timer.timer_advance_ns;\n\n\t \n\tif (vcpu->arch.tsc_scaling_ratio == kvm_caps.default_tsc_scaling_ratio) {\n\t\t__delay(min(guest_cycles,\n\t\t\tnsec_to_cycles(vcpu, timer_advance_ns)));\n\t} else {\n\t\tu64 delay_ns = guest_cycles * 1000000ULL;\n\t\tdo_div(delay_ns, vcpu->arch.virtual_tsc_khz);\n\t\tndelay(min_t(u32, delay_ns, timer_advance_ns));\n\t}\n}\n\nstatic inline void adjust_lapic_timer_advance(struct kvm_vcpu *vcpu,\n\t\t\t\t\t      s64 advance_expire_delta)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 timer_advance_ns = apic->lapic_timer.timer_advance_ns;\n\tu64 ns;\n\n\t \n\tif (abs(advance_expire_delta) > LAPIC_TIMER_ADVANCE_ADJUST_MAX ||\n\t    abs(advance_expire_delta) < LAPIC_TIMER_ADVANCE_ADJUST_MIN)\n\t\treturn;\n\n\t \n\tif (advance_expire_delta < 0) {\n\t\tns = -advance_expire_delta * 1000000ULL;\n\t\tdo_div(ns, vcpu->arch.virtual_tsc_khz);\n\t\ttimer_advance_ns -= ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;\n\t} else {\n\t \n\t\tns = advance_expire_delta * 1000000ULL;\n\t\tdo_div(ns, vcpu->arch.virtual_tsc_khz);\n\t\ttimer_advance_ns += ns/LAPIC_TIMER_ADVANCE_ADJUST_STEP;\n\t}\n\n\tif (unlikely(timer_advance_ns > LAPIC_TIMER_ADVANCE_NS_MAX))\n\t\ttimer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;\n\tapic->lapic_timer.timer_advance_ns = timer_advance_ns;\n}\n\nstatic void __kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu64 guest_tsc, tsc_deadline;\n\n\ttsc_deadline = apic->lapic_timer.expired_tscdeadline;\n\tapic->lapic_timer.expired_tscdeadline = 0;\n\tguest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());\n\ttrace_kvm_wait_lapic_expire(vcpu->vcpu_id, guest_tsc - tsc_deadline);\n\n\tif (lapic_timer_advance_dynamic) {\n\t\tadjust_lapic_timer_advance(vcpu, guest_tsc - tsc_deadline);\n\t\t \n\t\tif (guest_tsc < tsc_deadline)\n\t\t\tguest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());\n\t}\n\n\tif (guest_tsc < tsc_deadline)\n\t\t__wait_lapic_expire(vcpu, tsc_deadline - guest_tsc);\n}\n\nvoid kvm_wait_lapic_expire(struct kvm_vcpu *vcpu)\n{\n\tif (lapic_in_kernel(vcpu) &&\n\t    vcpu->arch.apic->lapic_timer.expired_tscdeadline &&\n\t    vcpu->arch.apic->lapic_timer.timer_advance_ns &&\n\t    lapic_timer_int_injected(vcpu))\n\t\t__kvm_wait_lapic_expire(vcpu);\n}\nEXPORT_SYMBOL_GPL(kvm_wait_lapic_expire);\n\nstatic void kvm_apic_inject_pending_timer_irqs(struct kvm_lapic *apic)\n{\n\tstruct kvm_timer *ktimer = &apic->lapic_timer;\n\n\tkvm_apic_local_deliver(apic, APIC_LVTT);\n\tif (apic_lvtt_tscdeadline(apic)) {\n\t\tktimer->tscdeadline = 0;\n\t} else if (apic_lvtt_oneshot(apic)) {\n\t\tktimer->tscdeadline = 0;\n\t\tktimer->target_expiration = 0;\n\t}\n}\n\nstatic void apic_timer_expired(struct kvm_lapic *apic, bool from_timer_fn)\n{\n\tstruct kvm_vcpu *vcpu = apic->vcpu;\n\tstruct kvm_timer *ktimer = &apic->lapic_timer;\n\n\tif (atomic_read(&apic->lapic_timer.pending))\n\t\treturn;\n\n\tif (apic_lvtt_tscdeadline(apic) || ktimer->hv_timer_in_use)\n\t\tktimer->expired_tscdeadline = ktimer->tscdeadline;\n\n\tif (!from_timer_fn && apic->apicv_active) {\n\t\tWARN_ON(kvm_get_running_vcpu() != vcpu);\n\t\tkvm_apic_inject_pending_timer_irqs(apic);\n\t\treturn;\n\t}\n\n\tif (kvm_use_posted_timer_interrupt(apic->vcpu)) {\n\t\t \n\t\tif (vcpu->arch.apic->lapic_timer.expired_tscdeadline &&\n\t\t    vcpu->arch.apic->lapic_timer.timer_advance_ns)\n\t\t\t__kvm_wait_lapic_expire(vcpu);\n\t\tkvm_apic_inject_pending_timer_irqs(apic);\n\t\treturn;\n\t}\n\n\tatomic_inc(&apic->lapic_timer.pending);\n\tkvm_make_request(KVM_REQ_UNBLOCK, vcpu);\n\tif (from_timer_fn)\n\t\tkvm_vcpu_kick(vcpu);\n}\n\nstatic void start_sw_tscdeadline(struct kvm_lapic *apic)\n{\n\tstruct kvm_timer *ktimer = &apic->lapic_timer;\n\tu64 guest_tsc, tscdeadline = ktimer->tscdeadline;\n\tu64 ns = 0;\n\tktime_t expire;\n\tstruct kvm_vcpu *vcpu = apic->vcpu;\n\tunsigned long this_tsc_khz = vcpu->arch.virtual_tsc_khz;\n\tunsigned long flags;\n\tktime_t now;\n\n\tif (unlikely(!tscdeadline || !this_tsc_khz))\n\t\treturn;\n\n\tlocal_irq_save(flags);\n\n\tnow = ktime_get();\n\tguest_tsc = kvm_read_l1_tsc(vcpu, rdtsc());\n\n\tns = (tscdeadline - guest_tsc) * 1000000ULL;\n\tdo_div(ns, this_tsc_khz);\n\n\tif (likely(tscdeadline > guest_tsc) &&\n\t    likely(ns > apic->lapic_timer.timer_advance_ns)) {\n\t\texpire = ktime_add_ns(now, ns);\n\t\texpire = ktime_sub_ns(expire, ktimer->timer_advance_ns);\n\t\thrtimer_start(&ktimer->timer, expire, HRTIMER_MODE_ABS_HARD);\n\t} else\n\t\tapic_timer_expired(apic, false);\n\n\tlocal_irq_restore(flags);\n}\n\nstatic inline u64 tmict_to_ns(struct kvm_lapic *apic, u32 tmict)\n{\n\treturn (u64)tmict * APIC_BUS_CYCLE_NS * (u64)apic->divide_count;\n}\n\nstatic void update_target_expiration(struct kvm_lapic *apic, uint32_t old_divisor)\n{\n\tktime_t now, remaining;\n\tu64 ns_remaining_old, ns_remaining_new;\n\n\tapic->lapic_timer.period =\n\t\t\ttmict_to_ns(apic, kvm_lapic_get_reg(apic, APIC_TMICT));\n\tlimit_periodic_timer_frequency(apic);\n\n\tnow = ktime_get();\n\tremaining = ktime_sub(apic->lapic_timer.target_expiration, now);\n\tif (ktime_to_ns(remaining) < 0)\n\t\tremaining = 0;\n\n\tns_remaining_old = ktime_to_ns(remaining);\n\tns_remaining_new = mul_u64_u32_div(ns_remaining_old,\n\t                                   apic->divide_count, old_divisor);\n\n\tapic->lapic_timer.tscdeadline +=\n\t\tnsec_to_cycles(apic->vcpu, ns_remaining_new) -\n\t\tnsec_to_cycles(apic->vcpu, ns_remaining_old);\n\tapic->lapic_timer.target_expiration = ktime_add_ns(now, ns_remaining_new);\n}\n\nstatic bool set_target_expiration(struct kvm_lapic *apic, u32 count_reg)\n{\n\tktime_t now;\n\tu64 tscl = rdtsc();\n\ts64 deadline;\n\n\tnow = ktime_get();\n\tapic->lapic_timer.period =\n\t\t\ttmict_to_ns(apic, kvm_lapic_get_reg(apic, APIC_TMICT));\n\n\tif (!apic->lapic_timer.period) {\n\t\tapic->lapic_timer.tscdeadline = 0;\n\t\treturn false;\n\t}\n\n\tlimit_periodic_timer_frequency(apic);\n\tdeadline = apic->lapic_timer.period;\n\n\tif (apic_lvtt_period(apic) || apic_lvtt_oneshot(apic)) {\n\t\tif (unlikely(count_reg != APIC_TMICT)) {\n\t\t\tdeadline = tmict_to_ns(apic,\n\t\t\t\t     kvm_lapic_get_reg(apic, count_reg));\n\t\t\tif (unlikely(deadline <= 0)) {\n\t\t\t\tif (apic_lvtt_period(apic))\n\t\t\t\t\tdeadline = apic->lapic_timer.period;\n\t\t\t\telse\n\t\t\t\t\tdeadline = 0;\n\t\t\t}\n\t\t\telse if (unlikely(deadline > apic->lapic_timer.period)) {\n\t\t\t\tpr_info_ratelimited(\n\t\t\t\t    \"vcpu %i: requested lapic timer restore with \"\n\t\t\t\t    \"starting count register %#x=%u (%lld ns) > initial count (%lld ns). \"\n\t\t\t\t    \"Using initial count to start timer.\\n\",\n\t\t\t\t    apic->vcpu->vcpu_id,\n\t\t\t\t    count_reg,\n\t\t\t\t    kvm_lapic_get_reg(apic, count_reg),\n\t\t\t\t    deadline, apic->lapic_timer.period);\n\t\t\t\tkvm_lapic_set_reg(apic, count_reg, 0);\n\t\t\t\tdeadline = apic->lapic_timer.period;\n\t\t\t}\n\t\t}\n\t}\n\n\tapic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +\n\t\tnsec_to_cycles(apic->vcpu, deadline);\n\tapic->lapic_timer.target_expiration = ktime_add_ns(now, deadline);\n\n\treturn true;\n}\n\nstatic void advance_periodic_target_expiration(struct kvm_lapic *apic)\n{\n\tktime_t now = ktime_get();\n\tu64 tscl = rdtsc();\n\tktime_t delta;\n\n\t \n\tapic->lapic_timer.target_expiration =\n\t\tktime_add_ns(apic->lapic_timer.target_expiration,\n\t\t\t\tapic->lapic_timer.period);\n\tdelta = ktime_sub(apic->lapic_timer.target_expiration, now);\n\tapic->lapic_timer.tscdeadline = kvm_read_l1_tsc(apic->vcpu, tscl) +\n\t\tnsec_to_cycles(apic->vcpu, delta);\n}\n\nstatic void start_sw_period(struct kvm_lapic *apic)\n{\n\tif (!apic->lapic_timer.period)\n\t\treturn;\n\n\tif (ktime_after(ktime_get(),\n\t\t\tapic->lapic_timer.target_expiration)) {\n\t\tapic_timer_expired(apic, false);\n\n\t\tif (apic_lvtt_oneshot(apic))\n\t\t\treturn;\n\n\t\tadvance_periodic_target_expiration(apic);\n\t}\n\n\thrtimer_start(&apic->lapic_timer.timer,\n\t\tapic->lapic_timer.target_expiration,\n\t\tHRTIMER_MODE_ABS_HARD);\n}\n\nbool kvm_lapic_hv_timer_in_use(struct kvm_vcpu *vcpu)\n{\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn false;\n\n\treturn vcpu->arch.apic->lapic_timer.hv_timer_in_use;\n}\n\nstatic void cancel_hv_timer(struct kvm_lapic *apic)\n{\n\tWARN_ON(preemptible());\n\tWARN_ON(!apic->lapic_timer.hv_timer_in_use);\n\tstatic_call(kvm_x86_cancel_hv_timer)(apic->vcpu);\n\tapic->lapic_timer.hv_timer_in_use = false;\n}\n\nstatic bool start_hv_timer(struct kvm_lapic *apic)\n{\n\tstruct kvm_timer *ktimer = &apic->lapic_timer;\n\tstruct kvm_vcpu *vcpu = apic->vcpu;\n\tbool expired;\n\n\tWARN_ON(preemptible());\n\tif (!kvm_can_use_hv_timer(vcpu))\n\t\treturn false;\n\n\tif (!ktimer->tscdeadline)\n\t\treturn false;\n\n\tif (static_call(kvm_x86_set_hv_timer)(vcpu, ktimer->tscdeadline, &expired))\n\t\treturn false;\n\n\tktimer->hv_timer_in_use = true;\n\thrtimer_cancel(&ktimer->timer);\n\n\t \n\tif (!apic_lvtt_period(apic)) {\n\t\t \n\t\tif (atomic_read(&ktimer->pending)) {\n\t\t\tcancel_hv_timer(apic);\n\t\t} else if (expired) {\n\t\t\tapic_timer_expired(apic, false);\n\t\t\tcancel_hv_timer(apic);\n\t\t}\n\t}\n\n\ttrace_kvm_hv_timer_state(vcpu->vcpu_id, ktimer->hv_timer_in_use);\n\n\treturn true;\n}\n\nstatic void start_sw_timer(struct kvm_lapic *apic)\n{\n\tstruct kvm_timer *ktimer = &apic->lapic_timer;\n\n\tWARN_ON(preemptible());\n\tif (apic->lapic_timer.hv_timer_in_use)\n\t\tcancel_hv_timer(apic);\n\tif (!apic_lvtt_period(apic) && atomic_read(&ktimer->pending))\n\t\treturn;\n\n\tif (apic_lvtt_period(apic) || apic_lvtt_oneshot(apic))\n\t\tstart_sw_period(apic);\n\telse if (apic_lvtt_tscdeadline(apic))\n\t\tstart_sw_tscdeadline(apic);\n\ttrace_kvm_hv_timer_state(apic->vcpu->vcpu_id, false);\n}\n\nstatic void restart_apic_timer(struct kvm_lapic *apic)\n{\n\tpreempt_disable();\n\n\tif (!apic_lvtt_period(apic) && atomic_read(&apic->lapic_timer.pending))\n\t\tgoto out;\n\n\tif (!start_hv_timer(apic))\n\t\tstart_sw_timer(apic);\nout:\n\tpreempt_enable();\n}\n\nvoid kvm_lapic_expired_hv_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tpreempt_disable();\n\t \n\tif (!apic->lapic_timer.hv_timer_in_use)\n\t\tgoto out;\n\tWARN_ON(kvm_vcpu_is_blocking(vcpu));\n\tapic_timer_expired(apic, false);\n\tcancel_hv_timer(apic);\n\n\tif (apic_lvtt_period(apic) && apic->lapic_timer.period) {\n\t\tadvance_periodic_target_expiration(apic);\n\t\trestart_apic_timer(apic);\n\t}\nout:\n\tpreempt_enable();\n}\nEXPORT_SYMBOL_GPL(kvm_lapic_expired_hv_timer);\n\nvoid kvm_lapic_switch_to_hv_timer(struct kvm_vcpu *vcpu)\n{\n\trestart_apic_timer(vcpu->arch.apic);\n}\n\nvoid kvm_lapic_switch_to_sw_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tpreempt_disable();\n\t \n\tif (apic->lapic_timer.hv_timer_in_use)\n\t\tstart_sw_timer(apic);\n\tpreempt_enable();\n}\n\nvoid kvm_lapic_restart_hv_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tWARN_ON(!apic->lapic_timer.hv_timer_in_use);\n\trestart_apic_timer(apic);\n}\n\nstatic void __start_apic_timer(struct kvm_lapic *apic, u32 count_reg)\n{\n\tatomic_set(&apic->lapic_timer.pending, 0);\n\n\tif ((apic_lvtt_period(apic) || apic_lvtt_oneshot(apic))\n\t    && !set_target_expiration(apic, count_reg))\n\t\treturn;\n\n\trestart_apic_timer(apic);\n}\n\nstatic void start_apic_timer(struct kvm_lapic *apic)\n{\n\t__start_apic_timer(apic, APIC_TMICT);\n}\n\nstatic void apic_manage_nmi_watchdog(struct kvm_lapic *apic, u32 lvt0_val)\n{\n\tbool lvt0_in_nmi_mode = apic_lvt_nmi_mode(lvt0_val);\n\n\tif (apic->lvt0_in_nmi_mode != lvt0_in_nmi_mode) {\n\t\tapic->lvt0_in_nmi_mode = lvt0_in_nmi_mode;\n\t\tif (lvt0_in_nmi_mode) {\n\t\t\tatomic_inc(&apic->vcpu->kvm->arch.vapics_in_nmi_mode);\n\t\t} else\n\t\t\tatomic_dec(&apic->vcpu->kvm->arch.vapics_in_nmi_mode);\n\t}\n}\n\nstatic int get_lvt_index(u32 reg)\n{\n\tif (reg == APIC_LVTCMCI)\n\t\treturn LVT_CMCI;\n\tif (reg < APIC_LVTT || reg > APIC_LVTERR)\n\t\treturn -1;\n\treturn array_index_nospec(\n\t\t\t(reg - APIC_LVTT) >> 4, KVM_APIC_MAX_NR_LVT_ENTRIES);\n}\n\nstatic int kvm_lapic_reg_write(struct kvm_lapic *apic, u32 reg, u32 val)\n{\n\tint ret = 0;\n\n\ttrace_kvm_apic_write(reg, val);\n\n\tswitch (reg) {\n\tcase APIC_ID:\t\t \n\t\tif (!apic_x2apic_mode(apic)) {\n\t\t\tkvm_apic_set_xapic_id(apic, val >> 24);\n\t\t} else {\n\t\t\tret = 1;\n\t\t}\n\t\tbreak;\n\n\tcase APIC_TASKPRI:\n\t\treport_tpr_access(apic, true);\n\t\tapic_set_tpr(apic, val & 0xff);\n\t\tbreak;\n\n\tcase APIC_EOI:\n\t\tapic_set_eoi(apic);\n\t\tbreak;\n\n\tcase APIC_LDR:\n\t\tif (!apic_x2apic_mode(apic))\n\t\t\tkvm_apic_set_ldr(apic, val & APIC_LDR_MASK);\n\t\telse\n\t\t\tret = 1;\n\t\tbreak;\n\n\tcase APIC_DFR:\n\t\tif (!apic_x2apic_mode(apic))\n\t\t\tkvm_apic_set_dfr(apic, val | 0x0FFFFFFF);\n\t\telse\n\t\t\tret = 1;\n\t\tbreak;\n\n\tcase APIC_SPIV: {\n\t\tu32 mask = 0x3ff;\n\t\tif (kvm_lapic_get_reg(apic, APIC_LVR) & APIC_LVR_DIRECTED_EOI)\n\t\t\tmask |= APIC_SPIV_DIRECTED_EOI;\n\t\tapic_set_spiv(apic, val & mask);\n\t\tif (!(val & APIC_SPIV_APIC_ENABLED)) {\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < apic->nr_lvt_entries; i++) {\n\t\t\t\tkvm_lapic_set_reg(apic, APIC_LVTx(i),\n\t\t\t\t\tkvm_lapic_get_reg(apic, APIC_LVTx(i)) | APIC_LVT_MASKED);\n\t\t\t}\n\t\t\tapic_update_lvtt(apic);\n\t\t\tatomic_set(&apic->lapic_timer.pending, 0);\n\n\t\t}\n\t\tbreak;\n\t}\n\tcase APIC_ICR:\n\t\tWARN_ON_ONCE(apic_x2apic_mode(apic));\n\n\t\t \n\t\tval &= ~APIC_ICR_BUSY;\n\t\tkvm_apic_send_ipi(apic, val, kvm_lapic_get_reg(apic, APIC_ICR2));\n\t\tkvm_lapic_set_reg(apic, APIC_ICR, val);\n\t\tbreak;\n\tcase APIC_ICR2:\n\t\tif (apic_x2apic_mode(apic))\n\t\t\tret = 1;\n\t\telse\n\t\t\tkvm_lapic_set_reg(apic, APIC_ICR2, val & 0xff000000);\n\t\tbreak;\n\n\tcase APIC_LVT0:\n\t\tapic_manage_nmi_watchdog(apic, val);\n\t\tfallthrough;\n\tcase APIC_LVTTHMR:\n\tcase APIC_LVTPC:\n\tcase APIC_LVT1:\n\tcase APIC_LVTERR:\n\tcase APIC_LVTCMCI: {\n\t\tu32 index = get_lvt_index(reg);\n\t\tif (!kvm_lapic_lvt_supported(apic, index)) {\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\t\tif (!kvm_apic_sw_enabled(apic))\n\t\t\tval |= APIC_LVT_MASKED;\n\t\tval &= apic_lvt_mask[index];\n\t\tkvm_lapic_set_reg(apic, reg, val);\n\t\tbreak;\n\t}\n\n\tcase APIC_LVTT:\n\t\tif (!kvm_apic_sw_enabled(apic))\n\t\t\tval |= APIC_LVT_MASKED;\n\t\tval &= (apic_lvt_mask[0] | apic->lapic_timer.timer_mode_mask);\n\t\tkvm_lapic_set_reg(apic, APIC_LVTT, val);\n\t\tapic_update_lvtt(apic);\n\t\tbreak;\n\n\tcase APIC_TMICT:\n\t\tif (apic_lvtt_tscdeadline(apic))\n\t\t\tbreak;\n\n\t\tcancel_apic_timer(apic);\n\t\tkvm_lapic_set_reg(apic, APIC_TMICT, val);\n\t\tstart_apic_timer(apic);\n\t\tbreak;\n\n\tcase APIC_TDCR: {\n\t\tuint32_t old_divisor = apic->divide_count;\n\n\t\tkvm_lapic_set_reg(apic, APIC_TDCR, val & 0xb);\n\t\tupdate_divide_count(apic);\n\t\tif (apic->divide_count != old_divisor &&\n\t\t\t\tapic->lapic_timer.period) {\n\t\t\thrtimer_cancel(&apic->lapic_timer.timer);\n\t\t\tupdate_target_expiration(apic, old_divisor);\n\t\t\trestart_apic_timer(apic);\n\t\t}\n\t\tbreak;\n\t}\n\tcase APIC_ESR:\n\t\tif (apic_x2apic_mode(apic) && val != 0)\n\t\t\tret = 1;\n\t\tbreak;\n\n\tcase APIC_SELF_IPI:\n\t\t \n\t\tif (!apic_x2apic_mode(apic) || (val & ~APIC_VECTOR_MASK))\n\t\t\tret = 1;\n\t\telse\n\t\t\tkvm_apic_send_ipi(apic, APIC_DEST_SELF | val, 0);\n\t\tbreak;\n\tdefault:\n\t\tret = 1;\n\t\tbreak;\n\t}\n\n\t \n\tkvm_recalculate_apic_map(apic->vcpu->kvm);\n\n\treturn ret;\n}\n\nstatic int apic_mmio_write(struct kvm_vcpu *vcpu, struct kvm_io_device *this,\n\t\t\t    gpa_t address, int len, const void *data)\n{\n\tstruct kvm_lapic *apic = to_lapic(this);\n\tunsigned int offset = address - apic->base_address;\n\tu32 val;\n\n\tif (!apic_mmio_in_range(apic, address))\n\t\treturn -EOPNOTSUPP;\n\n\tif (!kvm_apic_hw_enabled(apic) || apic_x2apic_mode(apic)) {\n\t\tif (!kvm_check_has_quirk(vcpu->kvm,\n\t\t\t\t\t KVM_X86_QUIRK_LAPIC_MMIO_HOLE))\n\t\t\treturn -EOPNOTSUPP;\n\n\t\treturn 0;\n\t}\n\n\t \n\tif (len != 4 || (offset & 0xf))\n\t\treturn 0;\n\n\tval = *(u32*)data;\n\n\tkvm_lapic_reg_write(apic, offset & 0xff0, val);\n\n\treturn 0;\n}\n\nvoid kvm_lapic_set_eoi(struct kvm_vcpu *vcpu)\n{\n\tkvm_lapic_reg_write(vcpu->arch.apic, APIC_EOI, 0);\n}\nEXPORT_SYMBOL_GPL(kvm_lapic_set_eoi);\n\n \nvoid kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\t \n\tif (apic_x2apic_mode(apic) && offset == APIC_ICR)\n\t\tkvm_x2apic_icr_write(apic, kvm_lapic_get_reg64(apic, APIC_ICR));\n\telse\n\t\tkvm_lapic_reg_write(apic, offset, kvm_lapic_get_reg(apic, offset));\n}\nEXPORT_SYMBOL_GPL(kvm_apic_write_nodecode);\n\nvoid kvm_free_lapic(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (!vcpu->arch.apic)\n\t\treturn;\n\n\thrtimer_cancel(&apic->lapic_timer.timer);\n\n\tif (!(vcpu->arch.apic_base & MSR_IA32_APICBASE_ENABLE))\n\t\tstatic_branch_slow_dec_deferred(&apic_hw_disabled);\n\n\tif (!apic->sw_enabled)\n\t\tstatic_branch_slow_dec_deferred(&apic_sw_disabled);\n\n\tif (apic->regs)\n\t\tfree_page((unsigned long)apic->regs);\n\n\tkfree(apic);\n}\n\n \nu64 kvm_get_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (!kvm_apic_present(vcpu) || !apic_lvtt_tscdeadline(apic))\n\t\treturn 0;\n\n\treturn apic->lapic_timer.tscdeadline;\n}\n\nvoid kvm_set_lapic_tscdeadline_msr(struct kvm_vcpu *vcpu, u64 data)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (!kvm_apic_present(vcpu) || !apic_lvtt_tscdeadline(apic))\n\t\treturn;\n\n\thrtimer_cancel(&apic->lapic_timer.timer);\n\tapic->lapic_timer.tscdeadline = data;\n\tstart_apic_timer(apic);\n}\n\nvoid kvm_lapic_set_tpr(struct kvm_vcpu *vcpu, unsigned long cr8)\n{\n\tapic_set_tpr(vcpu->arch.apic, (cr8 & 0x0f) << 4);\n}\n\nu64 kvm_lapic_get_cr8(struct kvm_vcpu *vcpu)\n{\n\tu64 tpr;\n\n\ttpr = (u64) kvm_lapic_get_reg(vcpu->arch.apic, APIC_TASKPRI);\n\n\treturn (tpr & 0xf0) >> 4;\n}\n\nvoid kvm_lapic_set_base(struct kvm_vcpu *vcpu, u64 value)\n{\n\tu64 old_value = vcpu->arch.apic_base;\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tvcpu->arch.apic_base = value;\n\n\tif ((old_value ^ value) & MSR_IA32_APICBASE_ENABLE)\n\t\tkvm_update_cpuid_runtime(vcpu);\n\n\tif (!apic)\n\t\treturn;\n\n\t \n\tif ((old_value ^ value) & MSR_IA32_APICBASE_ENABLE) {\n\t\tif (value & MSR_IA32_APICBASE_ENABLE) {\n\t\t\tkvm_apic_set_xapic_id(apic, vcpu->vcpu_id);\n\t\t\tstatic_branch_slow_dec_deferred(&apic_hw_disabled);\n\t\t\t \n\t\t\tkvm_make_request(KVM_REQ_APF_READY, vcpu);\n\t\t} else {\n\t\t\tstatic_branch_inc(&apic_hw_disabled.key);\n\t\t\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n\t\t}\n\t}\n\n\tif ((old_value ^ value) & X2APIC_ENABLE) {\n\t\tif (value & X2APIC_ENABLE)\n\t\t\tkvm_apic_set_x2apic_id(apic, vcpu->vcpu_id);\n\t\telse if (value & MSR_IA32_APICBASE_ENABLE)\n\t\t\tkvm_apic_set_xapic_id(apic, vcpu->vcpu_id);\n\t}\n\n\tif ((old_value ^ value) & (MSR_IA32_APICBASE_ENABLE | X2APIC_ENABLE)) {\n\t\tkvm_make_request(KVM_REQ_APICV_UPDATE, vcpu);\n\t\tstatic_call_cond(kvm_x86_set_virtual_apic_mode)(vcpu);\n\t}\n\n\tapic->base_address = apic->vcpu->arch.apic_base &\n\t\t\t     MSR_IA32_APICBASE_BASE;\n\n\tif ((value & MSR_IA32_APICBASE_ENABLE) &&\n\t     apic->base_address != APIC_DEFAULT_PHYS_BASE) {\n\t\tkvm_set_apicv_inhibit(apic->vcpu->kvm,\n\t\t\t\t      APICV_INHIBIT_REASON_APIC_BASE_MODIFIED);\n\t}\n}\n\nvoid kvm_apic_update_apicv(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (apic->apicv_active) {\n\t\t \n\t\tapic->irr_pending = true;\n\t\tapic->isr_count = 1;\n\t} else {\n\t\t \n\t\tapic->isr_count = count_vectors(apic->regs + APIC_ISR);\n\t}\n\tapic->highest_isr_cache = -1;\n}\n\nint kvm_alloc_apic_access_page(struct kvm *kvm)\n{\n\tstruct page *page;\n\tvoid __user *hva;\n\tint ret = 0;\n\n\tmutex_lock(&kvm->slots_lock);\n\tif (kvm->arch.apic_access_memslot_enabled ||\n\t    kvm->arch.apic_access_memslot_inhibited)\n\t\tgoto out;\n\n\thva = __x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT,\n\t\t\t\t      APIC_DEFAULT_PHYS_BASE, PAGE_SIZE);\n\tif (IS_ERR(hva)) {\n\t\tret = PTR_ERR(hva);\n\t\tgoto out;\n\t}\n\n\tpage = gfn_to_page(kvm, APIC_DEFAULT_PHYS_BASE >> PAGE_SHIFT);\n\tif (is_error_page(page)) {\n\t\tret = -EFAULT;\n\t\tgoto out;\n\t}\n\n\t \n\tput_page(page);\n\tkvm->arch.apic_access_memslot_enabled = true;\nout:\n\tmutex_unlock(&kvm->slots_lock);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(kvm_alloc_apic_access_page);\n\nvoid kvm_inhibit_apic_access_page(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm *kvm = vcpu->kvm;\n\n\tif (!kvm->arch.apic_access_memslot_enabled)\n\t\treturn;\n\n\tkvm_vcpu_srcu_read_unlock(vcpu);\n\n\tmutex_lock(&kvm->slots_lock);\n\n\tif (kvm->arch.apic_access_memslot_enabled) {\n\t\t__x86_set_memory_region(kvm, APIC_ACCESS_PAGE_PRIVATE_MEMSLOT, 0, 0);\n\t\t \n\t\tkvm->arch.apic_access_memslot_enabled = false;\n\n\t\t \n\t\tkvm->arch.apic_access_memslot_inhibited = true;\n\t}\n\n\tmutex_unlock(&kvm->slots_lock);\n\n\tkvm_vcpu_srcu_read_lock(vcpu);\n}\n\nvoid kvm_lapic_reset(struct kvm_vcpu *vcpu, bool init_event)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu64 msr_val;\n\tint i;\n\n\tstatic_call_cond(kvm_x86_apicv_pre_state_restore)(vcpu);\n\n\tif (!init_event) {\n\t\tmsr_val = APIC_DEFAULT_PHYS_BASE | MSR_IA32_APICBASE_ENABLE;\n\t\tif (kvm_vcpu_is_reset_bsp(vcpu))\n\t\t\tmsr_val |= MSR_IA32_APICBASE_BSP;\n\t\tkvm_lapic_set_base(vcpu, msr_val);\n\t}\n\n\tif (!apic)\n\t\treturn;\n\n\t \n\thrtimer_cancel(&apic->lapic_timer.timer);\n\n\t \n\tif (!init_event)\n\t\tkvm_apic_set_xapic_id(apic, vcpu->vcpu_id);\n\tkvm_apic_set_version(apic->vcpu);\n\n\tfor (i = 0; i < apic->nr_lvt_entries; i++)\n\t\tkvm_lapic_set_reg(apic, APIC_LVTx(i), APIC_LVT_MASKED);\n\tapic_update_lvtt(apic);\n\tif (kvm_vcpu_is_reset_bsp(vcpu) &&\n\t    kvm_check_has_quirk(vcpu->kvm, KVM_X86_QUIRK_LINT0_REENABLED))\n\t\tkvm_lapic_set_reg(apic, APIC_LVT0,\n\t\t\t     SET_APIC_DELIVERY_MODE(0, APIC_MODE_EXTINT));\n\tapic_manage_nmi_watchdog(apic, kvm_lapic_get_reg(apic, APIC_LVT0));\n\n\tkvm_apic_set_dfr(apic, 0xffffffffU);\n\tapic_set_spiv(apic, 0xff);\n\tkvm_lapic_set_reg(apic, APIC_TASKPRI, 0);\n\tif (!apic_x2apic_mode(apic))\n\t\tkvm_apic_set_ldr(apic, 0);\n\tkvm_lapic_set_reg(apic, APIC_ESR, 0);\n\tif (!apic_x2apic_mode(apic)) {\n\t\tkvm_lapic_set_reg(apic, APIC_ICR, 0);\n\t\tkvm_lapic_set_reg(apic, APIC_ICR2, 0);\n\t} else {\n\t\tkvm_lapic_set_reg64(apic, APIC_ICR, 0);\n\t}\n\tkvm_lapic_set_reg(apic, APIC_TDCR, 0);\n\tkvm_lapic_set_reg(apic, APIC_TMICT, 0);\n\tfor (i = 0; i < 8; i++) {\n\t\tkvm_lapic_set_reg(apic, APIC_IRR + 0x10 * i, 0);\n\t\tkvm_lapic_set_reg(apic, APIC_ISR + 0x10 * i, 0);\n\t\tkvm_lapic_set_reg(apic, APIC_TMR + 0x10 * i, 0);\n\t}\n\tkvm_apic_update_apicv(vcpu);\n\tupdate_divide_count(apic);\n\tatomic_set(&apic->lapic_timer.pending, 0);\n\n\tvcpu->arch.pv_eoi.msr_val = 0;\n\tapic_update_ppr(apic);\n\tif (apic->apicv_active) {\n\t\tstatic_call_cond(kvm_x86_apicv_post_state_restore)(vcpu);\n\t\tstatic_call_cond(kvm_x86_hwapic_irr_update)(vcpu, -1);\n\t\tstatic_call_cond(kvm_x86_hwapic_isr_update)(-1);\n\t}\n\n\tvcpu->arch.apic_arb_prio = 0;\n\tvcpu->arch.apic_attention = 0;\n\n\tkvm_recalculate_apic_map(vcpu->kvm);\n}\n\n \n\nstatic bool lapic_is_periodic(struct kvm_lapic *apic)\n{\n\treturn apic_lvtt_period(apic);\n}\n\nint apic_has_pending_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (apic_enabled(apic) && apic_lvt_enabled(apic, APIC_LVTT))\n\t\treturn atomic_read(&apic->lapic_timer.pending);\n\n\treturn 0;\n}\n\nint kvm_apic_local_deliver(struct kvm_lapic *apic, int lvt_type)\n{\n\tu32 reg = kvm_lapic_get_reg(apic, lvt_type);\n\tint vector, mode, trig_mode;\n\tint r;\n\n\tif (kvm_apic_hw_enabled(apic) && !(reg & APIC_LVT_MASKED)) {\n\t\tvector = reg & APIC_VECTOR_MASK;\n\t\tmode = reg & APIC_MODE_MASK;\n\t\ttrig_mode = reg & APIC_LVT_LEVEL_TRIGGER;\n\n\t\tr = __apic_accept_irq(apic, mode, vector, 1, trig_mode, NULL);\n\t\tif (r && lvt_type == APIC_LVTPC)\n\t\t\tkvm_lapic_set_reg(apic, APIC_LVTPC, reg | APIC_LVT_MASKED);\n\t\treturn r;\n\t}\n\treturn 0;\n}\n\nvoid kvm_apic_nmi_wd_deliver(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (apic)\n\t\tkvm_apic_local_deliver(apic, APIC_LVT0);\n}\n\nstatic const struct kvm_io_device_ops apic_mmio_ops = {\n\t.read     = apic_mmio_read,\n\t.write    = apic_mmio_write,\n};\n\nstatic enum hrtimer_restart apic_timer_fn(struct hrtimer *data)\n{\n\tstruct kvm_timer *ktimer = container_of(data, struct kvm_timer, timer);\n\tstruct kvm_lapic *apic = container_of(ktimer, struct kvm_lapic, lapic_timer);\n\n\tapic_timer_expired(apic, true);\n\n\tif (lapic_is_periodic(apic)) {\n\t\tadvance_periodic_target_expiration(apic);\n\t\thrtimer_add_expires_ns(&ktimer->timer, ktimer->period);\n\t\treturn HRTIMER_RESTART;\n\t} else\n\t\treturn HRTIMER_NORESTART;\n}\n\nint kvm_create_lapic(struct kvm_vcpu *vcpu, int timer_advance_ns)\n{\n\tstruct kvm_lapic *apic;\n\n\tASSERT(vcpu != NULL);\n\n\tapic = kzalloc(sizeof(*apic), GFP_KERNEL_ACCOUNT);\n\tif (!apic)\n\t\tgoto nomem;\n\n\tvcpu->arch.apic = apic;\n\n\tapic->regs = (void *)get_zeroed_page(GFP_KERNEL_ACCOUNT);\n\tif (!apic->regs) {\n\t\tprintk(KERN_ERR \"malloc apic regs error for vcpu %x\\n\",\n\t\t       vcpu->vcpu_id);\n\t\tgoto nomem_free_apic;\n\t}\n\tapic->vcpu = vcpu;\n\n\tapic->nr_lvt_entries = kvm_apic_calc_nr_lvt_entries(vcpu);\n\n\thrtimer_init(&apic->lapic_timer.timer, CLOCK_MONOTONIC,\n\t\t     HRTIMER_MODE_ABS_HARD);\n\tapic->lapic_timer.timer.function = apic_timer_fn;\n\tif (timer_advance_ns == -1) {\n\t\tapic->lapic_timer.timer_advance_ns = LAPIC_TIMER_ADVANCE_NS_INIT;\n\t\tlapic_timer_advance_dynamic = true;\n\t} else {\n\t\tapic->lapic_timer.timer_advance_ns = timer_advance_ns;\n\t\tlapic_timer_advance_dynamic = false;\n\t}\n\n\t \n\tvcpu->arch.apic_base = MSR_IA32_APICBASE_ENABLE;\n\tstatic_branch_inc(&apic_sw_disabled.key);  \n\tkvm_iodevice_init(&apic->dev, &apic_mmio_ops);\n\n\treturn 0;\nnomem_free_apic:\n\tkfree(apic);\n\tvcpu->arch.apic = NULL;\nnomem:\n\treturn -ENOMEM;\n}\n\nint kvm_apic_has_interrupt(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 ppr;\n\n\tif (!kvm_apic_present(vcpu))\n\t\treturn -1;\n\n\t__apic_update_ppr(apic, &ppr);\n\treturn apic_has_interrupt_for_ppr(apic, ppr);\n}\nEXPORT_SYMBOL_GPL(kvm_apic_has_interrupt);\n\nint kvm_apic_accept_pic_intr(struct kvm_vcpu *vcpu)\n{\n\tu32 lvt0 = kvm_lapic_get_reg(vcpu->arch.apic, APIC_LVT0);\n\n\tif (!kvm_apic_hw_enabled(vcpu->arch.apic))\n\t\treturn 1;\n\tif ((lvt0 & APIC_LVT_MASKED) == 0 &&\n\t    GET_APIC_DELIVERY_MODE(lvt0) == APIC_MODE_EXTINT)\n\t\treturn 1;\n\treturn 0;\n}\n\nvoid kvm_inject_apic_timer_irqs(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tif (atomic_read(&apic->lapic_timer.pending) > 0) {\n\t\tkvm_apic_inject_pending_timer_irqs(apic);\n\t\tatomic_set(&apic->lapic_timer.pending, 0);\n\t}\n}\n\nint kvm_get_apic_interrupt(struct kvm_vcpu *vcpu)\n{\n\tint vector = kvm_apic_has_interrupt(vcpu);\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 ppr;\n\n\tif (vector == -1)\n\t\treturn -1;\n\n\t \n\n\tapic_clear_irr(vector, apic);\n\tif (to_hv_vcpu(vcpu) && test_bit(vector, to_hv_synic(vcpu)->auto_eoi_bitmap)) {\n\t\t \n\t\tapic_update_ppr(apic);\n\t} else {\n\t\t \n\t\tapic_set_isr(vector, apic);\n\t\t__apic_update_ppr(apic, &ppr);\n\t}\n\n\treturn vector;\n}\n\nstatic int kvm_apic_state_fixup(struct kvm_vcpu *vcpu,\n\t\tstruct kvm_lapic_state *s, bool set)\n{\n\tif (apic_x2apic_mode(vcpu->arch.apic)) {\n\t\tu32 *id = (u32 *)(s->regs + APIC_ID);\n\t\tu32 *ldr = (u32 *)(s->regs + APIC_LDR);\n\t\tu64 icr;\n\n\t\tif (vcpu->kvm->arch.x2apic_format) {\n\t\t\tif (*id != vcpu->vcpu_id)\n\t\t\t\treturn -EINVAL;\n\t\t} else {\n\t\t\tif (set)\n\t\t\t\t*id >>= 24;\n\t\t\telse\n\t\t\t\t*id <<= 24;\n\t\t}\n\n\t\t \n\t\tif (set) {\n\t\t\t*ldr = kvm_apic_calc_x2apic_ldr(*id);\n\n\t\t\ticr = __kvm_lapic_get_reg(s->regs, APIC_ICR) |\n\t\t\t      (u64)__kvm_lapic_get_reg(s->regs, APIC_ICR2) << 32;\n\t\t\t__kvm_lapic_set_reg64(s->regs, APIC_ICR, icr);\n\t\t} else {\n\t\t\ticr = __kvm_lapic_get_reg64(s->regs, APIC_ICR);\n\t\t\t__kvm_lapic_set_reg(s->regs, APIC_ICR2, icr >> 32);\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint kvm_apic_get_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)\n{\n\tmemcpy(s->regs, vcpu->arch.apic->regs, sizeof(*s));\n\n\t \n\t__kvm_lapic_set_reg(s->regs, APIC_TMCCT,\n\t\t\t    __apic_read(vcpu->arch.apic, APIC_TMCCT));\n\n\treturn kvm_apic_state_fixup(vcpu, s, false);\n}\n\nint kvm_apic_set_state(struct kvm_vcpu *vcpu, struct kvm_lapic_state *s)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tint r;\n\n\tstatic_call_cond(kvm_x86_apicv_pre_state_restore)(vcpu);\n\n\tkvm_lapic_set_base(vcpu, vcpu->arch.apic_base);\n\t \n\tapic_set_spiv(apic, *((u32 *)(s->regs + APIC_SPIV)));\n\n\tr = kvm_apic_state_fixup(vcpu, s, true);\n\tif (r) {\n\t\tkvm_recalculate_apic_map(vcpu->kvm);\n\t\treturn r;\n\t}\n\tmemcpy(vcpu->arch.apic->regs, s->regs, sizeof(*s));\n\n\tatomic_set_release(&apic->vcpu->kvm->arch.apic_map_dirty, DIRTY);\n\tkvm_recalculate_apic_map(vcpu->kvm);\n\tkvm_apic_set_version(vcpu);\n\n\tapic_update_ppr(apic);\n\tcancel_apic_timer(apic);\n\tapic->lapic_timer.expired_tscdeadline = 0;\n\tapic_update_lvtt(apic);\n\tapic_manage_nmi_watchdog(apic, kvm_lapic_get_reg(apic, APIC_LVT0));\n\tupdate_divide_count(apic);\n\t__start_apic_timer(apic, APIC_TMCCT);\n\tkvm_lapic_set_reg(apic, APIC_TMCCT, 0);\n\tkvm_apic_update_apicv(vcpu);\n\tif (apic->apicv_active) {\n\t\tstatic_call_cond(kvm_x86_apicv_post_state_restore)(vcpu);\n\t\tstatic_call_cond(kvm_x86_hwapic_irr_update)(vcpu, apic_find_highest_irr(apic));\n\t\tstatic_call_cond(kvm_x86_hwapic_isr_update)(apic_find_highest_isr(apic));\n\t}\n\tkvm_make_request(KVM_REQ_EVENT, vcpu);\n\tif (ioapic_in_kernel(vcpu->kvm))\n\t\tkvm_rtc_eoi_tracking_restore_one(vcpu);\n\n\tvcpu->arch.apic_arb_prio = 0;\n\n\treturn 0;\n}\n\nvoid __kvm_migrate_apic_timer(struct kvm_vcpu *vcpu)\n{\n\tstruct hrtimer *timer;\n\n\tif (!lapic_in_kernel(vcpu) ||\n\t\tkvm_can_post_timer_interrupt(vcpu))\n\t\treturn;\n\n\ttimer = &vcpu->arch.apic->lapic_timer.timer;\n\tif (hrtimer_cancel(timer))\n\t\thrtimer_start_expires(timer, HRTIMER_MODE_ABS_HARD);\n}\n\n \nstatic void apic_sync_pv_eoi_from_guest(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_lapic *apic)\n{\n\tint vector;\n\t \n\tBUG_ON(!pv_eoi_enabled(vcpu));\n\n\tif (pv_eoi_test_and_clr_pending(vcpu))\n\t\treturn;\n\tvector = apic_set_eoi(apic);\n\ttrace_kvm_pv_eoi(apic, vector);\n}\n\nvoid kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)\n{\n\tu32 data;\n\n\tif (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))\n\t\tapic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);\n\n\tif (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))\n\t\treturn;\n\n\tif (kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,\n\t\t\t\t  sizeof(u32)))\n\t\treturn;\n\n\tapic_set_tpr(vcpu->arch.apic, data & 0xff);\n}\n\n \nstatic void apic_sync_pv_eoi_to_guest(struct kvm_vcpu *vcpu,\n\t\t\t\t\tstruct kvm_lapic *apic)\n{\n\tif (!pv_eoi_enabled(vcpu) ||\n\t     \n\t    apic->irr_pending ||\n\t     \n\t    apic->highest_isr_cache == -1 ||\n\t     \n\t    kvm_ioapic_handles_vector(apic, apic->highest_isr_cache)) {\n\t\t \n\t\treturn;\n\t}\n\n\tpv_eoi_set_pending(apic->vcpu);\n}\n\nvoid kvm_lapic_sync_to_vapic(struct kvm_vcpu *vcpu)\n{\n\tu32 data, tpr;\n\tint max_irr, max_isr;\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\n\tapic_sync_pv_eoi_to_guest(vcpu, apic);\n\n\tif (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))\n\t\treturn;\n\n\ttpr = kvm_lapic_get_reg(apic, APIC_TASKPRI) & 0xff;\n\tmax_irr = apic_find_highest_irr(apic);\n\tif (max_irr < 0)\n\t\tmax_irr = 0;\n\tmax_isr = apic_find_highest_isr(apic);\n\tif (max_isr < 0)\n\t\tmax_isr = 0;\n\tdata = (tpr & 0xff) | ((max_isr & 0xf0) << 8) | (max_irr << 24);\n\n\tkvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,\n\t\t\t\tsizeof(u32));\n}\n\nint kvm_lapic_set_vapic_addr(struct kvm_vcpu *vcpu, gpa_t vapic_addr)\n{\n\tif (vapic_addr) {\n\t\tif (kvm_gfn_to_hva_cache_init(vcpu->kvm,\n\t\t\t\t\t&vcpu->arch.apic->vapic_cache,\n\t\t\t\t\tvapic_addr, sizeof(u32)))\n\t\t\treturn -EINVAL;\n\t\t__set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);\n\t} else {\n\t\t__clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);\n\t}\n\n\tvcpu->arch.apic->vapic_addr = vapic_addr;\n\treturn 0;\n}\n\nint kvm_x2apic_icr_write(struct kvm_lapic *apic, u64 data)\n{\n\tdata &= ~APIC_ICR_BUSY;\n\n\tkvm_apic_send_ipi(apic, (u32)data, (u32)(data >> 32));\n\tkvm_lapic_set_reg64(apic, APIC_ICR, data);\n\ttrace_kvm_apic_write(APIC_ICR, data);\n\treturn 0;\n}\n\nstatic int kvm_lapic_msr_read(struct kvm_lapic *apic, u32 reg, u64 *data)\n{\n\tu32 low;\n\n\tif (reg == APIC_ICR) {\n\t\t*data = kvm_lapic_get_reg64(apic, APIC_ICR);\n\t\treturn 0;\n\t}\n\n\tif (kvm_lapic_reg_read(apic, reg, 4, &low))\n\t\treturn 1;\n\n\t*data = low;\n\n\treturn 0;\n}\n\nstatic int kvm_lapic_msr_write(struct kvm_lapic *apic, u32 reg, u64 data)\n{\n\t \n\tif (reg == APIC_ICR)\n\t\treturn kvm_x2apic_icr_write(apic, data);\n\n\t \n\tif (data >> 32)\n\t\treturn 1;\n\n\treturn kvm_lapic_reg_write(apic, reg, (u32)data);\n}\n\nint kvm_x2apic_msr_write(struct kvm_vcpu *vcpu, u32 msr, u64 data)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 reg = (msr - APIC_BASE_MSR) << 4;\n\n\tif (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(apic))\n\t\treturn 1;\n\n\treturn kvm_lapic_msr_write(apic, reg, data);\n}\n\nint kvm_x2apic_msr_read(struct kvm_vcpu *vcpu, u32 msr, u64 *data)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu32 reg = (msr - APIC_BASE_MSR) << 4;\n\n\tif (!lapic_in_kernel(vcpu) || !apic_x2apic_mode(apic))\n\t\treturn 1;\n\n\treturn kvm_lapic_msr_read(apic, reg, data);\n}\n\nint kvm_hv_vapic_msr_write(struct kvm_vcpu *vcpu, u32 reg, u64 data)\n{\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn 1;\n\n\treturn kvm_lapic_msr_write(vcpu->arch.apic, reg, data);\n}\n\nint kvm_hv_vapic_msr_read(struct kvm_vcpu *vcpu, u32 reg, u64 *data)\n{\n\tif (!lapic_in_kernel(vcpu))\n\t\treturn 1;\n\n\treturn kvm_lapic_msr_read(vcpu->arch.apic, reg, data);\n}\n\nint kvm_lapic_set_pv_eoi(struct kvm_vcpu *vcpu, u64 data, unsigned long len)\n{\n\tu64 addr = data & ~KVM_MSR_ENABLED;\n\tstruct gfn_to_hva_cache *ghc = &vcpu->arch.pv_eoi.data;\n\tunsigned long new_len;\n\tint ret;\n\n\tif (!IS_ALIGNED(addr, 4))\n\t\treturn 1;\n\n\tif (data & KVM_MSR_ENABLED) {\n\t\tif (addr == ghc->gpa && len <= ghc->len)\n\t\t\tnew_len = ghc->len;\n\t\telse\n\t\t\tnew_len = len;\n\n\t\tret = kvm_gfn_to_hva_cache_init(vcpu->kvm, ghc, addr, new_len);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tvcpu->arch.pv_eoi.msr_val = data;\n\n\treturn 0;\n}\n\nint kvm_apic_accept_events(struct kvm_vcpu *vcpu)\n{\n\tstruct kvm_lapic *apic = vcpu->arch.apic;\n\tu8 sipi_vector;\n\tint r;\n\n\tif (!kvm_apic_has_pending_init_or_sipi(vcpu))\n\t\treturn 0;\n\n\tif (is_guest_mode(vcpu)) {\n\t\tr = kvm_check_nested_events(vcpu);\n\t\tif (r < 0)\n\t\t\treturn r == -EBUSY ? 0 : r;\n\t\t \n\t}\n\n\t \n\tif (!kvm_apic_init_sipi_allowed(vcpu)) {\n\t\tWARN_ON_ONCE(vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED);\n\t\tclear_bit(KVM_APIC_SIPI, &apic->pending_events);\n\t\treturn 0;\n\t}\n\n\tif (test_and_clear_bit(KVM_APIC_INIT, &apic->pending_events)) {\n\t\tkvm_vcpu_reset(vcpu, true);\n\t\tif (kvm_vcpu_is_bsp(apic->vcpu))\n\t\t\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\t\telse\n\t\t\tvcpu->arch.mp_state = KVM_MP_STATE_INIT_RECEIVED;\n\t}\n\tif (test_and_clear_bit(KVM_APIC_SIPI, &apic->pending_events)) {\n\t\tif (vcpu->arch.mp_state == KVM_MP_STATE_INIT_RECEIVED) {\n\t\t\t \n\t\t\tsmp_rmb();\n\t\t\tsipi_vector = apic->sipi_vector;\n\t\t\tstatic_call(kvm_x86_vcpu_deliver_sipi_vector)(vcpu, sipi_vector);\n\t\t\tvcpu->arch.mp_state = KVM_MP_STATE_RUNNABLE;\n\t\t}\n\t}\n\treturn 0;\n}\n\nvoid kvm_lapic_exit(void)\n{\n\tstatic_key_deferred_flush(&apic_hw_disabled);\n\tWARN_ON(static_branch_unlikely(&apic_hw_disabled.key));\n\tstatic_key_deferred_flush(&apic_sw_disabled);\n\tWARN_ON(static_branch_unlikely(&apic_sw_disabled.key));\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}