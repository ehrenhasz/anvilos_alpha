{
  "module_name": "entry_32.S",
  "hash_id": "8f43ba153bfd4127e5536eda92caef8aff90951f214da60b4b113e933749df33",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/entry/entry_32.S",
  "human_readable_source": " \n \n\n#include <linux/linkage.h>\n#include <linux/err.h>\n#include <asm/thread_info.h>\n#include <asm/irqflags.h>\n#include <asm/errno.h>\n#include <asm/segment.h>\n#include <asm/smp.h>\n#include <asm/percpu.h>\n#include <asm/processor-flags.h>\n#include <asm/irq_vectors.h>\n#include <asm/cpufeatures.h>\n#include <asm/alternative.h>\n#include <asm/asm.h>\n#include <asm/smap.h>\n#include <asm/frame.h>\n#include <asm/trapnr.h>\n#include <asm/nospec-branch.h>\n\n#include \"calling.h\"\n\n\t.section .entry.text, \"ax\"\n\n#define PTI_SWITCH_MASK         (1 << PAGE_SHIFT)\n\n \n.macro SWITCH_TO_USER_CR3 scratch_reg:req\n\tALTERNATIVE \"jmp .Lend_\\@\", \"\", X86_FEATURE_PTI\n\n\tmovl\t%cr3, \\scratch_reg\n\torl\t$PTI_SWITCH_MASK, \\scratch_reg\n\tmovl\t\\scratch_reg, %cr3\n.Lend_\\@:\n.endm\n\n.macro BUG_IF_WRONG_CR3 no_user_check=0\n#ifdef CONFIG_DEBUG_ENTRY\n\tALTERNATIVE \"jmp .Lend_\\@\", \"\", X86_FEATURE_PTI\n\t.if \\no_user_check == 0\n\t \n\ttestl\t$USER_SEGMENT_RPL_MASK, PT_CS(%esp)\n\tjz\t.Lend_\\@\n\t.endif\n\t \n\tmovl\t%cr3, %eax\n\ttestl\t$PTI_SWITCH_MASK, %eax\n\tjnz\t.Lend_\\@\n\t \n\tud2\n.Lend_\\@:\n#endif\n.endm\n\n \n.macro SWITCH_TO_KERNEL_CR3 scratch_reg:req\n\tALTERNATIVE \"jmp .Lend_\\@\", \"\", X86_FEATURE_PTI\n\tmovl\t%cr3, \\scratch_reg\n\t \n\ttestl\t$PTI_SWITCH_MASK, \\scratch_reg\n\tjz\t.Lend_\\@\n\tandl\t$(~PTI_SWITCH_MASK), \\scratch_reg\n\tmovl\t\\scratch_reg, %cr3\n\t \n\torl\t$PTI_SWITCH_MASK, \\scratch_reg\n.Lend_\\@:\n.endm\n\n#define CS_FROM_ENTRY_STACK\t(1 << 31)\n#define CS_FROM_USER_CR3\t(1 << 30)\n#define CS_FROM_KERNEL\t\t(1 << 29)\n#define CS_FROM_ESPFIX\t\t(1 << 28)\n\n.macro FIXUP_FRAME\n\t \n\tandl\t$0x0000ffff, 4*4(%esp)\n\n#ifdef CONFIG_VM86\n\ttestl\t$X86_EFLAGS_VM, 5*4(%esp)\n\tjnz\t.Lfrom_usermode_no_fixup_\\@\n#endif\n\ttestl\t$USER_SEGMENT_RPL_MASK, 4*4(%esp)\n\tjnz\t.Lfrom_usermode_no_fixup_\\@\n\n\torl\t$CS_FROM_KERNEL, 4*4(%esp)\n\n\t \n\n\tpushl\t%ss\t\t# ss\n\tpushl\t%esp\t\t# sp (points at ss)\n\taddl\t$7*4, (%esp)\t# point sp back at the previous context\n\tpushl\t7*4(%esp)\t# flags\n\tpushl\t7*4(%esp)\t# cs\n\tpushl\t7*4(%esp)\t# ip\n\tpushl\t7*4(%esp)\t# orig_eax\n\tpushl\t7*4(%esp)\t# gs / function\n\tpushl\t7*4(%esp)\t# fs\n.Lfrom_usermode_no_fixup_\\@:\n.endm\n\n.macro IRET_FRAME\n\t \n\ttestl $CS_FROM_KERNEL, 1*4(%esp)\n\tjz .Lfinished_frame_\\@\n\n\t \n\tpushl\t%eax\n\tpushl\t%ecx\n\tmovl\t5*4(%esp), %eax\t\t# (modified) regs->sp\n\n\tmovl\t4*4(%esp), %ecx\t\t# flags\n\tmovl\t%ecx, %ss:-1*4(%eax)\n\n\tmovl\t3*4(%esp), %ecx\t\t# cs\n\tandl\t$0x0000ffff, %ecx\n\tmovl\t%ecx, %ss:-2*4(%eax)\n\n\tmovl\t2*4(%esp), %ecx\t\t# ip\n\tmovl\t%ecx, %ss:-3*4(%eax)\n\n\tmovl\t1*4(%esp), %ecx\t\t# eax\n\tmovl\t%ecx, %ss:-4*4(%eax)\n\n\tpopl\t%ecx\n\tlea\t-4*4(%eax), %esp\n\tpopl\t%eax\n.Lfinished_frame_\\@:\n.endm\n\n.macro SAVE_ALL pt_regs_ax=%eax switch_stacks=0 skip_gs=0 unwind_espfix=0\n\tcld\n.if \\skip_gs == 0\n\tpushl\t$0\n.endif\n\tpushl\t%fs\n\n\tpushl\t%eax\n\tmovl\t$(__KERNEL_PERCPU), %eax\n\tmovl\t%eax, %fs\n.if \\unwind_espfix > 0\n\tUNWIND_ESPFIX_STACK\n.endif\n\tpopl\t%eax\n\n\tFIXUP_FRAME\n\tpushl\t%es\n\tpushl\t%ds\n\tpushl\t\\pt_regs_ax\n\tpushl\t%ebp\n\tpushl\t%edi\n\tpushl\t%esi\n\tpushl\t%edx\n\tpushl\t%ecx\n\tpushl\t%ebx\n\tmovl\t$(__USER_DS), %edx\n\tmovl\t%edx, %ds\n\tmovl\t%edx, %es\n\t \n.if \\switch_stacks > 0\n\tSWITCH_TO_KERNEL_STACK\n.endif\n.endm\n\n.macro SAVE_ALL_NMI cr3_reg:req unwind_espfix=0\n\tSAVE_ALL unwind_espfix=\\unwind_espfix\n\n\tBUG_IF_WRONG_CR3\n\n\t \n\tSWITCH_TO_KERNEL_CR3 scratch_reg=\\cr3_reg\n\n.Lend_\\@:\n.endm\n\n.macro RESTORE_INT_REGS\n\tpopl\t%ebx\n\tpopl\t%ecx\n\tpopl\t%edx\n\tpopl\t%esi\n\tpopl\t%edi\n\tpopl\t%ebp\n\tpopl\t%eax\n.endm\n\n.macro RESTORE_REGS pop=0\n\tRESTORE_INT_REGS\n1:\tpopl\t%ds\n2:\tpopl\t%es\n3:\tpopl\t%fs\n4:\taddl\t$(4 + \\pop), %esp\t \n\tIRET_FRAME\n\n\t \n\t_ASM_EXTABLE_TYPE(1b, 2b, EX_TYPE_POP_ZERO|EX_REG_DS)\n\t_ASM_EXTABLE_TYPE(2b, 3b, EX_TYPE_POP_ZERO|EX_REG_ES)\n\t_ASM_EXTABLE_TYPE(3b, 4b, EX_TYPE_POP_ZERO|EX_REG_FS)\n.endm\n\n.macro RESTORE_ALL_NMI cr3_reg:req pop=0\n\t \n\tALTERNATIVE \"jmp .Lswitched_\\@\", \"\", X86_FEATURE_PTI\n\n\ttestl\t$PTI_SWITCH_MASK, \\cr3_reg\n\tjz\t.Lswitched_\\@\n\n\t \n\tmovl\t\\cr3_reg, %cr3\n\n.Lswitched_\\@:\n\n\tBUG_IF_WRONG_CR3\n\n\tRESTORE_REGS pop=\\pop\n.endm\n\n.macro CHECK_AND_APPLY_ESPFIX\n#ifdef CONFIG_X86_ESPFIX32\n#define GDT_ESPFIX_OFFSET (GDT_ENTRY_ESPFIX_SS * 8)\n#define GDT_ESPFIX_SS PER_CPU_VAR(gdt_page) + GDT_ESPFIX_OFFSET\n\n\tALTERNATIVE\t\"jmp .Lend_\\@\", \"\", X86_BUG_ESPFIX\n\n\tmovl\tPT_EFLAGS(%esp), %eax\t\t# mix EFLAGS, SS and CS\n\t \n\tmovb\tPT_OLDSS(%esp), %ah\n\tmovb\tPT_CS(%esp), %al\n\tandl\t$(X86_EFLAGS_VM | (SEGMENT_TI_MASK << 8) | SEGMENT_RPL_MASK), %eax\n\tcmpl\t$((SEGMENT_LDT << 8) | USER_RPL), %eax\n\tjne\t.Lend_\\@\t# returning to user-space with LDT SS\n\n\t \n\tmov\t%esp, %edx\t\t\t \n\tmov\tPT_OLDESP(%esp), %eax\t\t \n\tmov\t%dx, %ax\t\t\t \n\tsub\t%eax, %edx\t\t\t \n\tshr\t$16, %edx\n\tmov\t%dl, GDT_ESPFIX_SS + 4\t\t \n\tmov\t%dh, GDT_ESPFIX_SS + 7\t\t \n\tpushl\t$__ESPFIX_SS\n\tpushl\t%eax\t\t\t\t \n\t \n\tcli\n\tlss\t(%esp), %esp\t\t\t \n.Lend_\\@:\n#endif  \n.endm\n\n \n\n.macro SWITCH_TO_KERNEL_STACK\n\n\tBUG_IF_WRONG_CR3\n\n\tSWITCH_TO_KERNEL_CR3 scratch_reg=%eax\n\n\t \n\n\t \n\tmovl\tPER_CPU_VAR(cpu_entry_area), %ecx\n\taddl\t$CPU_ENTRY_AREA_entry_stack + SIZEOF_entry_stack, %ecx\n\tsubl\t%esp, %ecx\t \n\tcmpl\t$SIZEOF_entry_stack, %ecx\n\tjae\t.Lend_\\@\n\n\t \n\tmovl\t%esp, %esi\n\tmovl\t%esi, %edi\n\n\t \n\tandl\t$(MASK_entry_stack), %edi\n\taddl\t$(SIZEOF_entry_stack), %edi\n\n\t \n\tmovl\tTSS_entry2task_stack(%edi), %edi\n\n\t \n#ifdef CONFIG_VM86\n\tmovl\tPT_EFLAGS(%esp), %ecx\t\t# mix EFLAGS and CS\n\tmovb\tPT_CS(%esp), %cl\n\tandl\t$(X86_EFLAGS_VM | SEGMENT_RPL_MASK), %ecx\n#else\n\tmovl\tPT_CS(%esp), %ecx\n\tandl\t$SEGMENT_RPL_MASK, %ecx\n#endif\n\tcmpl\t$USER_RPL, %ecx\n\tjb\t.Lentry_from_kernel_\\@\n\n\t \n\tmovl\t$PTREGS_SIZE, %ecx\n\n#ifdef CONFIG_VM86\n\ttestl\t$X86_EFLAGS_VM, PT_EFLAGS(%esi)\n\tjz\t.Lcopy_pt_regs_\\@\n\n\t \n\taddl\t$(4 * 4), %ecx\n\n#endif\n.Lcopy_pt_regs_\\@:\n\n\t \n\tsubl\t%ecx, %edi\n\n\t \n\tmovl\t%edi, %esp\n\n\t \n\tshrl\t$2, %ecx\n\tcld\n\trep movsl\n\n\tjmp .Lend_\\@\n\n.Lentry_from_kernel_\\@:\n\n\t \n\n\t \n\tmovl\t%esi, %ecx\n\n\t \n\tandl\t$(MASK_entry_stack), %ecx\n\taddl\t$(SIZEOF_entry_stack), %ecx\n\n\t \n\tsub\t%esi, %ecx\n\n\t \n\torl\t$CS_FROM_ENTRY_STACK, PT_CS(%esp)\n\n\t \n\ttestl\t$PTI_SWITCH_MASK, %eax\n\tjz\t.Lcopy_pt_regs_\\@\n\torl\t$CS_FROM_USER_CR3, PT_CS(%esp)\n\n\t \n\tjmp .Lcopy_pt_regs_\\@\n\n.Lend_\\@:\n.endm\n\n \n.macro SWITCH_TO_ENTRY_STACK\n\n\t \n\tmovl\t$PTREGS_SIZE, %ecx\n\n#ifdef CONFIG_VM86\n\ttestl\t$(X86_EFLAGS_VM), PT_EFLAGS(%esp)\n\tjz\t.Lcopy_pt_regs_\\@\n\n\t \n\taddl    $(4 * 4), %ecx\n\n.Lcopy_pt_regs_\\@:\n#endif\n\n\t \n\tmovl\tPER_CPU_VAR(cpu_tss_rw + TSS_sp0), %edi\n\tsubl\t%ecx, %edi\n\tmovl\t%esp, %esi\n\n\t \n\tmovl\t%edi, %ebx\n\n\t \n\tshrl\t$2, %ecx\n\tcld\n\trep movsl\n\n\t \n\tmovl\t%ebx, %esp\n\n.Lend_\\@:\n.endm\n\n \n.macro PARANOID_EXIT_TO_KERNEL_MODE\n\n\t \n\ttestl\t$CS_FROM_ENTRY_STACK, PT_CS(%esp)\n\tjz\t.Lend_\\@\n\n\t \n\n\t \n\tandl\t$(~CS_FROM_ENTRY_STACK), PT_CS(%esp)\n\n\t \n\tmovl\t%esp, %esi\n\tmovl\tPER_CPU_VAR(cpu_tss_rw + TSS_sp0), %edi\n\n\t \n\tmovl\tPER_CPU_VAR(cpu_tss_rw + TSS_sp1), %ecx\n\tsubl\t%esi, %ecx\n\n\t \n\tsubl\t%ecx, %edi\n\n\t \n\tmovl\t%edi, %ebx\n\n\t \n\tshrl\t$2, %ecx\n\tcld\n\trep movsl\n\n\t \n\tmovl\t%ebx, %esp\n\n\t \n\ttestl\t$CS_FROM_USER_CR3, PT_CS(%esp)\n\tjz\t.Lend_\\@\n\n\t \n\tandl\t$(~CS_FROM_USER_CR3), PT_CS(%esp)\n\n\tSWITCH_TO_USER_CR3 scratch_reg=%eax\n\n.Lend_\\@:\n.endm\n\n \n.macro idtentry vector asmsym cfunc has_error_code:req\nSYM_CODE_START(\\asmsym)\n\tASM_CLAC\n\tcld\n\n\t.if \\has_error_code == 0\n\t\tpushl\t$0\t\t \n\t.endif\n\n\t \n\tpushl\t$\\cfunc\n\t \n\tjmp\thandle_exception\nSYM_CODE_END(\\asmsym)\n.endm\n\n.macro idtentry_irq vector cfunc\n\t.p2align CONFIG_X86_L1_CACHE_SHIFT\nSYM_CODE_START_LOCAL(asm_\\cfunc)\n\tASM_CLAC\n\tSAVE_ALL switch_stacks=1\n\tENCODE_FRAME_POINTER\n\tmovl\t%esp, %eax\n\tmovl\tPT_ORIG_EAX(%esp), %edx\t\t \n\tmovl\t$-1, PT_ORIG_EAX(%esp)\t\t \n\tcall\t\\cfunc\n\tjmp\thandle_exception_return\nSYM_CODE_END(asm_\\cfunc)\n.endm\n\n.macro idtentry_sysvec vector cfunc\n\tidtentry \\vector asm_\\cfunc \\cfunc has_error_code=0\n.endm\n\n \n\t.align 16\n\t.globl __irqentry_text_start\n__irqentry_text_start:\n\n#include <asm/idtentry.h>\n\n\t.align 16\n\t.globl __irqentry_text_end\n__irqentry_text_end:\n\n \n.pushsection .text, \"ax\"\nSYM_CODE_START(__switch_to_asm)\n\t \n\tpushl\t%ebp\n\tpushl\t%ebx\n\tpushl\t%edi\n\tpushl\t%esi\n\t \n\tpushfl\n\n\t \n\tmovl\t%esp, TASK_threadsp(%eax)\n\tmovl\tTASK_threadsp(%edx), %esp\n\n#ifdef CONFIG_STACKPROTECTOR\n\tmovl\tTASK_stack_canary(%edx), %ebx\n\tmovl\t%ebx, PER_CPU_VAR(__stack_chk_guard)\n#endif\n\n\t \n\tFILL_RETURN_BUFFER %ebx, RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW\n\n\t \n\tpopfl\n\t \n\tpopl\t%esi\n\tpopl\t%edi\n\tpopl\t%ebx\n\tpopl\t%ebp\n\n\tjmp\t__switch_to\nSYM_CODE_END(__switch_to_asm)\n.popsection\n\n \n.pushsection .text, \"ax\"\nSYM_CODE_START(ret_from_fork_asm)\n\tmovl\t%esp, %edx\t \n\n\t \n\tpushl\t$.Lsyscall_32_done\n\n\tFRAME_BEGIN\n\t \n\tmovl\t%ebx, %ecx\t \n\tpushl\t%edi\t\t \n\tcall\tret_from_fork\n\taddl\t$4, %esp\n\tFRAME_END\n\n\tRET\nSYM_CODE_END(ret_from_fork_asm)\n.popsection\n\nSYM_ENTRY(__begin_SYSENTER_singlestep_region, SYM_L_GLOBAL, SYM_A_NONE)\n \n\n \nSYM_FUNC_START(entry_SYSENTER_32)\n\t \n\tpushfl\n\tpushl\t%eax\n\tBUG_IF_WRONG_CR3 no_user_check=1\n\tSWITCH_TO_KERNEL_CR3 scratch_reg=%eax\n\tpopl\t%eax\n\tpopfl\n\n\t \n\tmovl\tTSS_entry2task_stack(%esp), %esp\n\n.Lsysenter_past_esp:\n\tpushl\t$__USER_DS\t\t \n\tpushl\t$0\t\t\t \n\tpushfl\t\t\t\t \n\tpushl\t$__USER_CS\t\t \n\tpushl\t$0\t\t\t \n\tpushl\t%eax\t\t\t \n\tSAVE_ALL pt_regs_ax=$-ENOSYS\t \n\n\t \n\ttestl\t$X86_EFLAGS_NT|X86_EFLAGS_AC|X86_EFLAGS_TF, PT_EFLAGS(%esp)\n\tjnz\t.Lsysenter_fix_flags\n.Lsysenter_flags_fixed:\n\n\tmovl\t%esp, %eax\n\tcall\tdo_SYSENTER_32\n\ttestl\t%eax, %eax\n\tjz\t.Lsyscall_32_done\n\n\tSTACKLEAK_ERASE\n\n\t \n\n\t \n\n\t \n\tmovl\tPER_CPU_VAR(cpu_tss_rw + TSS_sp0), %eax\n\tsubl\t$(2*4), %eax\n\n\t \n\tmovl\tPT_EFLAGS(%esp), %edi\n\tmovl\tPT_EAX(%esp), %esi\n\tmovl\t%edi, (%eax)\n\tmovl\t%esi, 4(%eax)\n\n\t \n\tmovl\tPT_EIP(%esp), %edx\t \n\tmovl\tPT_OLDESP(%esp), %ecx\t \n1:\tmov\tPT_FS(%esp), %fs\n\n\tpopl\t%ebx\t\t\t \n\taddl\t$2*4, %esp\t\t \n\tpopl\t%esi\t\t\t \n\tpopl\t%edi\t\t\t \n\tpopl\t%ebp\t\t\t \n\n\t \n\tmovl\t%eax, %esp\n\n\t \n\tSWITCH_TO_USER_CR3 scratch_reg=%eax\n\n\t \n\tbtrl\t$X86_EFLAGS_IF_BIT, (%esp)\n\tBUG_IF_WRONG_CR3 no_user_check=1\n\tpopfl\n\tpopl\t%eax\n\n\t \n\tsti\n\tsysexit\n\n2:\tmovl    $0, PT_FS(%esp)\n\tjmp     1b\n\t_ASM_EXTABLE(1b, 2b)\n\n.Lsysenter_fix_flags:\n\tpushl\t$X86_EFLAGS_FIXED\n\tpopfl\n\tjmp\t.Lsysenter_flags_fixed\nSYM_ENTRY(__end_SYSENTER_singlestep_region, SYM_L_GLOBAL, SYM_A_NONE)\nSYM_FUNC_END(entry_SYSENTER_32)\n\n \nSYM_FUNC_START(entry_INT80_32)\n\tASM_CLAC\n\tpushl\t%eax\t\t\t \n\n\tSAVE_ALL pt_regs_ax=$-ENOSYS switch_stacks=1\t \n\n\tmovl\t%esp, %eax\n\tcall\tdo_int80_syscall_32\n.Lsyscall_32_done:\n\tSTACKLEAK_ERASE\n\nrestore_all_switch_stack:\n\tSWITCH_TO_ENTRY_STACK\n\tCHECK_AND_APPLY_ESPFIX\n\n\t \n\tSWITCH_TO_USER_CR3 scratch_reg=%eax\n\n\tBUG_IF_WRONG_CR3\n\n\t \n\tRESTORE_REGS pop=4\t\t\t# skip orig_eax/error_code\n.Lirq_return:\n\t \n\tiret\n\n.Lasm_iret_error:\n\tpushl\t$0\t\t\t\t# no error code\n\tpushl\t$iret_error\n\n#ifdef CONFIG_DEBUG_ENTRY\n\t \n\tpushl\t%eax\n\tSWITCH_TO_USER_CR3 scratch_reg=%eax\n\tpopl\t%eax\n#endif\n\n\tjmp\thandle_exception\n\n\t_ASM_EXTABLE(.Lirq_return, .Lasm_iret_error)\nSYM_FUNC_END(entry_INT80_32)\n\n.macro FIXUP_ESPFIX_STACK\n \n#ifdef CONFIG_X86_ESPFIX32\n\t \n\tpushl\t%ecx\n\tsubl\t$2*4, %esp\n\tsgdt\t(%esp)\n\tmovl\t2(%esp), %ecx\t\t\t\t \n\t \n\tmov\t%cs:GDT_ESPFIX_OFFSET + 4(%ecx), %al\t \n\tmov\t%cs:GDT_ESPFIX_OFFSET + 7(%ecx), %ah\t \n\tshl\t$16, %eax\n\taddl\t$2*4, %esp\n\tpopl\t%ecx\n\taddl\t%esp, %eax\t\t\t \n\tpushl\t$__KERNEL_DS\n\tpushl\t%eax\n\tlss\t(%esp), %esp\t\t\t \n#endif\n.endm\n\n.macro UNWIND_ESPFIX_STACK\n\t \n#ifdef CONFIG_X86_ESPFIX32\n\tmovl\t%ss, %eax\n\t \n\tcmpw\t$__ESPFIX_SS, %ax\n\tjne\t.Lno_fixup_\\@\n\t \n\tFIXUP_ESPFIX_STACK\n.Lno_fixup_\\@:\n#endif\n.endm\n\nSYM_CODE_START_LOCAL_NOALIGN(handle_exception)\n\t \n\tSAVE_ALL switch_stacks=1 skip_gs=1 unwind_espfix=1\n\tENCODE_FRAME_POINTER\n\n\tmovl\tPT_GS(%esp), %edi\t\t# get the function address\n\n\t \n\tmovl\tPT_ORIG_EAX(%esp), %edx\t\t# get the error code\n\tmovl\t$-1, PT_ORIG_EAX(%esp)\t\t# no syscall to restart\n\n\tmovl\t%esp, %eax\t\t\t# pt_regs pointer\n\tCALL_NOSPEC edi\n\nhandle_exception_return:\n#ifdef CONFIG_VM86\n\tmovl\tPT_EFLAGS(%esp), %eax\t\t# mix EFLAGS and CS\n\tmovb\tPT_CS(%esp), %al\n\tandl\t$(X86_EFLAGS_VM | SEGMENT_RPL_MASK), %eax\n#else\n\t \n\tmovl\tPT_CS(%esp), %eax\n\tandl\t$SEGMENT_RPL_MASK, %eax\n#endif\n\tcmpl\t$USER_RPL, %eax\t\t\t# returning to v8086 or userspace ?\n\tjnb\tret_to_user\n\n\tPARANOID_EXIT_TO_KERNEL_MODE\n\tBUG_IF_WRONG_CR3\n\tRESTORE_REGS 4\n\tjmp\t.Lirq_return\n\nret_to_user:\n\tmovl\t%esp, %eax\n\tjmp\trestore_all_switch_stack\nSYM_CODE_END(handle_exception)\n\nSYM_CODE_START(asm_exc_double_fault)\n1:\n\t \n\n\tclts\t\t\t\t \n\tpushl\t$X86_EFLAGS_FIXED\n\tpopfl\t\t\t\t \n\n\tcall\tdoublefault_shim\n\n\t \n1:\n\thlt\n\tjmp 1b\nSYM_CODE_END(asm_exc_double_fault)\n\n \nSYM_CODE_START(asm_exc_nmi)\n\tASM_CLAC\n\n#ifdef CONFIG_X86_ESPFIX32\n\t \n\tpushl\t%eax\n\tmovl\t%ss, %eax\n\tcmpw\t$__ESPFIX_SS, %ax\n\tpopl\t%eax\n\tje\t.Lnmi_espfix_stack\n#endif\n\n\tpushl\t%eax\t\t\t\t# pt_regs->orig_ax\n\tSAVE_ALL_NMI cr3_reg=%edi\n\tENCODE_FRAME_POINTER\n\txorl\t%edx, %edx\t\t\t# zero error code\n\tmovl\t%esp, %eax\t\t\t# pt_regs pointer\n\n\t \n\tmovl\tPER_CPU_VAR(cpu_entry_area), %ecx\n\taddl\t$CPU_ENTRY_AREA_entry_stack + SIZEOF_entry_stack, %ecx\n\tsubl\t%eax, %ecx\t \n\tcmpl\t$SIZEOF_entry_stack, %ecx\n\tjb\t.Lnmi_from_sysenter_stack\n\n\t \n\tcall\texc_nmi\n\tjmp\t.Lnmi_return\n\n.Lnmi_from_sysenter_stack:\n\t \n\tmovl\t%esp, %ebx\n\tmovl\tPER_CPU_VAR(pcpu_hot + X86_top_of_stack), %esp\n\tcall\texc_nmi\n\tmovl\t%ebx, %esp\n\n.Lnmi_return:\n#ifdef CONFIG_X86_ESPFIX32\n\ttestl\t$CS_FROM_ESPFIX, PT_CS(%esp)\n\tjnz\t.Lnmi_from_espfix\n#endif\n\n\tCHECK_AND_APPLY_ESPFIX\n\tRESTORE_ALL_NMI cr3_reg=%edi pop=4\n\tjmp\t.Lirq_return\n\n#ifdef CONFIG_X86_ESPFIX32\n.Lnmi_espfix_stack:\n\t \n\tpushl\t%ss\n\tpushl\t%esp\n\taddl\t$4, (%esp)\n\n\t \n\tpushl\t4*4(%esp)\t# flags\n\tpushl\t4*4(%esp)\t# cs\n\tpushl\t4*4(%esp)\t# ip\n\n\tpushl\t%eax\t\t# orig_ax\n\n\tSAVE_ALL_NMI cr3_reg=%edi unwind_espfix=1\n\tENCODE_FRAME_POINTER\n\n\t \n\txorl\t$(CS_FROM_ESPFIX | CS_FROM_KERNEL), PT_CS(%esp)\n\n\txorl\t%edx, %edx\t\t\t# zero error code\n\tmovl\t%esp, %eax\t\t\t# pt_regs pointer\n\tjmp\t.Lnmi_from_sysenter_stack\n\n.Lnmi_from_espfix:\n\tRESTORE_ALL_NMI cr3_reg=%edi\n\t \n\tlss\t(1+5+6)*4(%esp), %esp\t\t\t# back to espfix stack\n\tjmp\t.Lirq_return\n#endif\nSYM_CODE_END(asm_exc_nmi)\n\n.pushsection .text, \"ax\"\nSYM_CODE_START(rewind_stack_and_make_dead)\n\t \n\txorl\t%ebp, %ebp\n\n\tmovl\tPER_CPU_VAR(pcpu_hot + X86_top_of_stack), %esi\n\tleal\t-TOP_OF_KERNEL_STACK_PADDING-PTREGS_SIZE(%esi), %esp\n\n\tcall\tmake_task_dead\n1:\tjmp 1b\nSYM_CODE_END(rewind_stack_and_make_dead)\n.popsection\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}