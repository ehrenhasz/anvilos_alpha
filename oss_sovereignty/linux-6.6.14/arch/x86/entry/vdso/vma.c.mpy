{
  "module_name": "vma.c",
  "hash_id": "66970e30f4f188db50783d4e8168dd406bcfe481c9239cf9b0647db150970ffe",
  "original_prompt": "Ingested from linux-6.6.14/arch/x86/entry/vdso/vma.c",
  "human_readable_source": "\n \n#include <linux/mm.h>\n#include <linux/err.h>\n#include <linux/sched.h>\n#include <linux/sched/task_stack.h>\n#include <linux/slab.h>\n#include <linux/init.h>\n#include <linux/random.h>\n#include <linux/elf.h>\n#include <linux/cpu.h>\n#include <linux/ptrace.h>\n#include <linux/time_namespace.h>\n\n#include <asm/pvclock.h>\n#include <asm/vgtod.h>\n#include <asm/proto.h>\n#include <asm/vdso.h>\n#include <asm/vvar.h>\n#include <asm/tlb.h>\n#include <asm/page.h>\n#include <asm/desc.h>\n#include <asm/cpufeature.h>\n#include <clocksource/hyperv_timer.h>\n\n#undef _ASM_X86_VVAR_H\n#define EMIT_VVAR(name, offset)\t\\\n\tconst size_t name ## _offset = offset;\n#include <asm/vvar.h>\n\nstruct vdso_data *arch_get_vdso_data(void *vvar_page)\n{\n\treturn (struct vdso_data *)(vvar_page + _vdso_data_offset);\n}\n#undef EMIT_VVAR\n\nunsigned int vclocks_used __read_mostly;\n\n#if defined(CONFIG_X86_64)\nunsigned int __read_mostly vdso64_enabled = 1;\n#endif\n\nint __init init_vdso_image(const struct vdso_image *image)\n{\n\tBUILD_BUG_ON(VDSO_CLOCKMODE_MAX >= 32);\n\tBUG_ON(image->size % PAGE_SIZE != 0);\n\n\tapply_alternatives((struct alt_instr *)(image->data + image->alt),\n\t\t\t   (struct alt_instr *)(image->data + image->alt +\n\t\t\t\t\t\timage->alt_len));\n\n\treturn 0;\n}\n\nstatic const struct vm_special_mapping vvar_mapping;\nstruct linux_binprm;\n\nstatic vm_fault_t vdso_fault(const struct vm_special_mapping *sm,\n\t\t      struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tconst struct vdso_image *image = vma->vm_mm->context.vdso_image;\n\n\tif (!image || (vmf->pgoff << PAGE_SHIFT) >= image->size)\n\t\treturn VM_FAULT_SIGBUS;\n\n\tvmf->page = virt_to_page(image->data + (vmf->pgoff << PAGE_SHIFT));\n\tget_page(vmf->page);\n\treturn 0;\n}\n\nstatic void vdso_fix_landing(const struct vdso_image *image,\n\t\tstruct vm_area_struct *new_vma)\n{\n#if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION\n\tif (in_ia32_syscall() && image == &vdso_image_32) {\n\t\tstruct pt_regs *regs = current_pt_regs();\n\t\tunsigned long vdso_land = image->sym_int80_landing_pad;\n\t\tunsigned long old_land_addr = vdso_land +\n\t\t\t(unsigned long)current->mm->context.vdso;\n\n\t\t \n\t\tif (regs->ip == old_land_addr)\n\t\t\tregs->ip = new_vma->vm_start + vdso_land;\n\t}\n#endif\n}\n\nstatic int vdso_mremap(const struct vm_special_mapping *sm,\n\t\tstruct vm_area_struct *new_vma)\n{\n\tconst struct vdso_image *image = current->mm->context.vdso_image;\n\n\tvdso_fix_landing(image, new_vma);\n\tcurrent->mm->context.vdso = (void __user *)new_vma->vm_start;\n\n\treturn 0;\n}\n\n#ifdef CONFIG_TIME_NS\n \nint vdso_join_timens(struct task_struct *task, struct time_namespace *ns)\n{\n\tstruct mm_struct *mm = task->mm;\n\tstruct vm_area_struct *vma;\n\tVMA_ITERATOR(vmi, mm, 0);\n\n\tmmap_read_lock(mm);\n\tfor_each_vma(vmi, vma) {\n\t\tif (vma_is_special_mapping(vma, &vvar_mapping))\n\t\t\tzap_vma_pages(vma);\n\t}\n\tmmap_read_unlock(mm);\n\n\treturn 0;\n}\n#endif\n\nstatic vm_fault_t vvar_fault(const struct vm_special_mapping *sm,\n\t\t      struct vm_area_struct *vma, struct vm_fault *vmf)\n{\n\tconst struct vdso_image *image = vma->vm_mm->context.vdso_image;\n\tunsigned long pfn;\n\tlong sym_offset;\n\n\tif (!image)\n\t\treturn VM_FAULT_SIGBUS;\n\n\tsym_offset = (long)(vmf->pgoff << PAGE_SHIFT) +\n\t\timage->sym_vvar_start;\n\n\t \n\tif (sym_offset == 0)\n\t\treturn VM_FAULT_SIGBUS;\n\n\tif (sym_offset == image->sym_vvar_page) {\n\t\tstruct page *timens_page = find_timens_vvar_page(vma);\n\n\t\tpfn = __pa_symbol(&__vvar_page) >> PAGE_SHIFT;\n\n\t\t \n\t\tif (timens_page) {\n\t\t\tunsigned long addr;\n\t\t\tvm_fault_t err;\n\n\t\t\t \n\t\t\taddr = vmf->address + (image->sym_timens_page - sym_offset);\n\t\t\terr = vmf_insert_pfn(vma, addr, pfn);\n\t\t\tif (unlikely(err & VM_FAULT_ERROR))\n\t\t\t\treturn err;\n\n\t\t\tpfn = page_to_pfn(timens_page);\n\t\t}\n\n\t\treturn vmf_insert_pfn(vma, vmf->address, pfn);\n\t} else if (sym_offset == image->sym_pvclock_page) {\n\t\tstruct pvclock_vsyscall_time_info *pvti =\n\t\t\tpvclock_get_pvti_cpu0_va();\n\t\tif (pvti && vclock_was_used(VDSO_CLOCKMODE_PVCLOCK)) {\n\t\t\treturn vmf_insert_pfn_prot(vma, vmf->address,\n\t\t\t\t\t__pa(pvti) >> PAGE_SHIFT,\n\t\t\t\t\tpgprot_decrypted(vma->vm_page_prot));\n\t\t}\n\t} else if (sym_offset == image->sym_hvclock_page) {\n\t\tpfn = hv_get_tsc_pfn();\n\n\t\tif (pfn && vclock_was_used(VDSO_CLOCKMODE_HVCLOCK))\n\t\t\treturn vmf_insert_pfn(vma, vmf->address, pfn);\n\t} else if (sym_offset == image->sym_timens_page) {\n\t\tstruct page *timens_page = find_timens_vvar_page(vma);\n\n\t\tif (!timens_page)\n\t\t\treturn VM_FAULT_SIGBUS;\n\n\t\tpfn = __pa_symbol(&__vvar_page) >> PAGE_SHIFT;\n\t\treturn vmf_insert_pfn(vma, vmf->address, pfn);\n\t}\n\n\treturn VM_FAULT_SIGBUS;\n}\n\nstatic const struct vm_special_mapping vdso_mapping = {\n\t.name = \"[vdso]\",\n\t.fault = vdso_fault,\n\t.mremap = vdso_mremap,\n};\nstatic const struct vm_special_mapping vvar_mapping = {\n\t.name = \"[vvar]\",\n\t.fault = vvar_fault,\n};\n\n \nstatic int map_vdso(const struct vdso_image *image, unsigned long addr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long text_start;\n\tint ret = 0;\n\n\tif (mmap_write_lock_killable(mm))\n\t\treturn -EINTR;\n\n\taddr = get_unmapped_area(NULL, addr,\n\t\t\t\t image->size - image->sym_vvar_start, 0, 0);\n\tif (IS_ERR_VALUE(addr)) {\n\t\tret = addr;\n\t\tgoto up_fail;\n\t}\n\n\ttext_start = addr - image->sym_vvar_start;\n\n\t \n\tvma = _install_special_mapping(mm,\n\t\t\t\t       text_start,\n\t\t\t\t       image->size,\n\t\t\t\t       VM_READ|VM_EXEC|\n\t\t\t\t       VM_MAYREAD|VM_MAYWRITE|VM_MAYEXEC,\n\t\t\t\t       &vdso_mapping);\n\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto up_fail;\n\t}\n\n\tvma = _install_special_mapping(mm,\n\t\t\t\t       addr,\n\t\t\t\t       -image->sym_vvar_start,\n\t\t\t\t       VM_READ|VM_MAYREAD|VM_IO|VM_DONTDUMP|\n\t\t\t\t       VM_PFNMAP,\n\t\t\t\t       &vvar_mapping);\n\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tdo_munmap(mm, text_start, image->size, NULL);\n\t} else {\n\t\tcurrent->mm->context.vdso = (void __user *)text_start;\n\t\tcurrent->mm->context.vdso_image = image;\n\t}\n\nup_fail:\n\tmmap_write_unlock(mm);\n\treturn ret;\n}\n\n#ifdef CONFIG_X86_64\n \nstatic unsigned long vdso_addr(unsigned long start, unsigned len)\n{\n\tunsigned long addr, end;\n\tunsigned offset;\n\n\t \n\tstart = PAGE_ALIGN(start);\n\n\t \n\tend = (start + len + PMD_SIZE - 1) & PMD_MASK;\n\tif (end >= DEFAULT_MAP_WINDOW)\n\t\tend = DEFAULT_MAP_WINDOW;\n\tend -= len;\n\n\tif (end > start) {\n\t\toffset = get_random_u32_below(((end - start) >> PAGE_SHIFT) + 1);\n\t\taddr = start + (offset << PAGE_SHIFT);\n\t} else {\n\t\taddr = start;\n\t}\n\n\t \n\taddr = align_vdso_addr(addr);\n\n\treturn addr;\n}\n\nstatic int map_vdso_randomized(const struct vdso_image *image)\n{\n\tunsigned long addr = vdso_addr(current->mm->start_stack, image->size-image->sym_vvar_start);\n\n\treturn map_vdso(image, addr);\n}\n#endif\n\nint map_vdso_once(const struct vdso_image *image, unsigned long addr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tVMA_ITERATOR(vmi, mm, 0);\n\n\tmmap_write_lock(mm);\n\t \n\tfor_each_vma(vmi, vma) {\n\t\tif (vma_is_special_mapping(vma, &vdso_mapping) ||\n\t\t\t\tvma_is_special_mapping(vma, &vvar_mapping)) {\n\t\t\tmmap_write_unlock(mm);\n\t\t\treturn -EEXIST;\n\t\t}\n\t}\n\tmmap_write_unlock(mm);\n\n\treturn map_vdso(image, addr);\n}\n\n#if defined(CONFIG_X86_32) || defined(CONFIG_IA32_EMULATION)\nstatic int load_vdso32(void)\n{\n\tif (vdso32_enabled != 1)   \n\t\treturn 0;\n\n\treturn map_vdso(&vdso_image_32, 0);\n}\n#endif\n\n#ifdef CONFIG_X86_64\nint arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)\n{\n\tif (!vdso64_enabled)\n\t\treturn 0;\n\n\treturn map_vdso_randomized(&vdso_image_64);\n}\n\n#ifdef CONFIG_COMPAT\nint compat_arch_setup_additional_pages(struct linux_binprm *bprm,\n\t\t\t\t       int uses_interp, bool x32)\n{\n#ifdef CONFIG_X86_X32_ABI\n\tif (x32) {\n\t\tif (!vdso64_enabled)\n\t\t\treturn 0;\n\t\treturn map_vdso_randomized(&vdso_image_x32);\n\t}\n#endif\n#ifdef CONFIG_IA32_EMULATION\n\treturn load_vdso32();\n#else\n\treturn 0;\n#endif\n}\n#endif\n#else\nint arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)\n{\n\treturn load_vdso32();\n}\n#endif\n\nbool arch_syscall_is_vdso_sigreturn(struct pt_regs *regs)\n{\n#if defined(CONFIG_X86_32) || defined(CONFIG_IA32_EMULATION)\n\tconst struct vdso_image *image = current->mm->context.vdso_image;\n\tunsigned long vdso = (unsigned long) current->mm->context.vdso;\n\n\tif (in_ia32_syscall() && image == &vdso_image_32) {\n\t\tif (regs->ip == vdso + image->sym_vdso32_sigreturn_landing_pad ||\n\t\t    regs->ip == vdso + image->sym_vdso32_rt_sigreturn_landing_pad)\n\t\t\treturn true;\n\t}\n#endif\n\treturn false;\n}\n\n#ifdef CONFIG_X86_64\nstatic __init int vdso_setup(char *s)\n{\n\tvdso64_enabled = simple_strtoul(s, NULL, 0);\n\treturn 1;\n}\n__setup(\"vdso=\", vdso_setup);\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}