{
  "module_name": "fs-writeback.c",
  "hash_id": "ba04c8671522c724dec02b8389af34048f0f63d5c11b875ea60ac54b9803aae4",
  "original_prompt": "Ingested from linux-6.6.14/fs/fs-writeback.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/export.h>\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/sched.h>\n#include <linux/fs.h>\n#include <linux/mm.h>\n#include <linux/pagemap.h>\n#include <linux/kthread.h>\n#include <linux/writeback.h>\n#include <linux/blkdev.h>\n#include <linux/backing-dev.h>\n#include <linux/tracepoint.h>\n#include <linux/device.h>\n#include <linux/memcontrol.h>\n#include \"internal.h\"\n\n \n#define MIN_WRITEBACK_PAGES\t(4096UL >> (PAGE_SHIFT - 10))\n\n \nstruct wb_writeback_work {\n\tlong nr_pages;\n\tstruct super_block *sb;\n\tenum writeback_sync_modes sync_mode;\n\tunsigned int tagged_writepages:1;\n\tunsigned int for_kupdate:1;\n\tunsigned int range_cyclic:1;\n\tunsigned int for_background:1;\n\tunsigned int for_sync:1;\t \n\tunsigned int auto_free:1;\t \n\tenum wb_reason reason;\t\t \n\n\tstruct list_head list;\t\t \n\tstruct wb_completion *done;\t \n};\n\n \nunsigned int dirtytime_expire_interval = 12 * 60 * 60;\n\nstatic inline struct inode *wb_inode(struct list_head *head)\n{\n\treturn list_entry(head, struct inode, i_io_list);\n}\n\n \n#define CREATE_TRACE_POINTS\n#include <trace/events/writeback.h>\n\nEXPORT_TRACEPOINT_SYMBOL_GPL(wbc_writepage);\n\nstatic bool wb_io_lists_populated(struct bdi_writeback *wb)\n{\n\tif (wb_has_dirty_io(wb)) {\n\t\treturn false;\n\t} else {\n\t\tset_bit(WB_has_dirty_io, &wb->state);\n\t\tWARN_ON_ONCE(!wb->avg_write_bandwidth);\n\t\tatomic_long_add(wb->avg_write_bandwidth,\n\t\t\t\t&wb->bdi->tot_write_bandwidth);\n\t\treturn true;\n\t}\n}\n\nstatic void wb_io_lists_depopulated(struct bdi_writeback *wb)\n{\n\tif (wb_has_dirty_io(wb) && list_empty(&wb->b_dirty) &&\n\t    list_empty(&wb->b_io) && list_empty(&wb->b_more_io)) {\n\t\tclear_bit(WB_has_dirty_io, &wb->state);\n\t\tWARN_ON_ONCE(atomic_long_sub_return(wb->avg_write_bandwidth,\n\t\t\t\t\t&wb->bdi->tot_write_bandwidth) < 0);\n\t}\n}\n\n \nstatic bool inode_io_list_move_locked(struct inode *inode,\n\t\t\t\t      struct bdi_writeback *wb,\n\t\t\t\t      struct list_head *head)\n{\n\tassert_spin_locked(&wb->list_lock);\n\tassert_spin_locked(&inode->i_lock);\n\tWARN_ON_ONCE(inode->i_state & I_FREEING);\n\n\tlist_move(&inode->i_io_list, head);\n\n\t \n\tif (head != &wb->b_dirty_time)\n\t\treturn wb_io_lists_populated(wb);\n\n\twb_io_lists_depopulated(wb);\n\treturn false;\n}\n\nstatic void wb_wakeup(struct bdi_writeback *wb)\n{\n\tspin_lock_irq(&wb->work_lock);\n\tif (test_bit(WB_registered, &wb->state))\n\t\tmod_delayed_work(bdi_wq, &wb->dwork, 0);\n\tspin_unlock_irq(&wb->work_lock);\n}\n\nstatic void finish_writeback_work(struct bdi_writeback *wb,\n\t\t\t\t  struct wb_writeback_work *work)\n{\n\tstruct wb_completion *done = work->done;\n\n\tif (work->auto_free)\n\t\tkfree(work);\n\tif (done) {\n\t\twait_queue_head_t *waitq = done->waitq;\n\n\t\t \n\t\tif (atomic_dec_and_test(&done->cnt))\n\t\t\twake_up_all(waitq);\n\t}\n}\n\nstatic void wb_queue_work(struct bdi_writeback *wb,\n\t\t\t  struct wb_writeback_work *work)\n{\n\ttrace_writeback_queue(wb, work);\n\n\tif (work->done)\n\t\tatomic_inc(&work->done->cnt);\n\n\tspin_lock_irq(&wb->work_lock);\n\n\tif (test_bit(WB_registered, &wb->state)) {\n\t\tlist_add_tail(&work->list, &wb->work_list);\n\t\tmod_delayed_work(bdi_wq, &wb->dwork, 0);\n\t} else\n\t\tfinish_writeback_work(wb, work);\n\n\tspin_unlock_irq(&wb->work_lock);\n}\n\n \nvoid wb_wait_for_completion(struct wb_completion *done)\n{\n\tatomic_dec(&done->cnt);\t\t \n\twait_event(*done->waitq, !atomic_read(&done->cnt));\n}\n\n#ifdef CONFIG_CGROUP_WRITEBACK\n\n \n#define WB_FRN_TIME_SHIFT\t13\t \n#define WB_FRN_TIME_AVG_SHIFT\t3\t \n#define WB_FRN_TIME_CUT_DIV\t8\t \n#define WB_FRN_TIME_PERIOD\t(2 * (1 << WB_FRN_TIME_SHIFT))\t \n\n#define WB_FRN_HIST_SLOTS\t16\t \n#define WB_FRN_HIST_UNIT\t(WB_FRN_TIME_PERIOD / WB_FRN_HIST_SLOTS)\n\t\t\t\t\t \n#define WB_FRN_HIST_THR_SLOTS\t(WB_FRN_HIST_SLOTS / 2)\n\t\t\t\t\t \n#define WB_FRN_HIST_MAX_SLOTS\t(WB_FRN_HIST_THR_SLOTS / 2 + 1)\n\t\t\t\t\t \n#define WB_FRN_MAX_IN_FLIGHT\t1024\t \n\n \n#define WB_MAX_INODES_PER_ISW  ((1024UL - sizeof(struct inode_switch_wbs_context)) \\\n                                / sizeof(struct inode *))\n\nstatic atomic_t isw_nr_in_flight = ATOMIC_INIT(0);\nstatic struct workqueue_struct *isw_wq;\n\nvoid __inode_attach_wb(struct inode *inode, struct folio *folio)\n{\n\tstruct backing_dev_info *bdi = inode_to_bdi(inode);\n\tstruct bdi_writeback *wb = NULL;\n\n\tif (inode_cgwb_enabled(inode)) {\n\t\tstruct cgroup_subsys_state *memcg_css;\n\n\t\tif (folio) {\n\t\t\tmemcg_css = mem_cgroup_css_from_folio(folio);\n\t\t\twb = wb_get_create(bdi, memcg_css, GFP_ATOMIC);\n\t\t} else {\n\t\t\t \n\t\t\tmemcg_css = task_get_css(current, memory_cgrp_id);\n\t\t\twb = wb_get_create(bdi, memcg_css, GFP_ATOMIC);\n\t\t\tcss_put(memcg_css);\n\t\t}\n\t}\n\n\tif (!wb)\n\t\twb = &bdi->wb;\n\n\t \n\tif (unlikely(cmpxchg(&inode->i_wb, NULL, wb)))\n\t\twb_put(wb);\n}\nEXPORT_SYMBOL_GPL(__inode_attach_wb);\n\n \nstatic void inode_cgwb_move_to_attached(struct inode *inode,\n\t\t\t\t\tstruct bdi_writeback *wb)\n{\n\tassert_spin_locked(&wb->list_lock);\n\tassert_spin_locked(&inode->i_lock);\n\tWARN_ON_ONCE(inode->i_state & I_FREEING);\n\n\tinode->i_state &= ~I_SYNC_QUEUED;\n\tif (wb != &wb->bdi->wb)\n\t\tlist_move(&inode->i_io_list, &wb->b_attached);\n\telse\n\t\tlist_del_init(&inode->i_io_list);\n\twb_io_lists_depopulated(wb);\n}\n\n \nstatic struct bdi_writeback *\nlocked_inode_to_wb_and_lock_list(struct inode *inode)\n\t__releases(&inode->i_lock)\n\t__acquires(&wb->list_lock)\n{\n\twhile (true) {\n\t\tstruct bdi_writeback *wb = inode_to_wb(inode);\n\n\t\t \n\t\twb_get(wb);\n\t\tspin_unlock(&inode->i_lock);\n\t\tspin_lock(&wb->list_lock);\n\n\t\t \n\t\tif (likely(wb == inode->i_wb)) {\n\t\t\twb_put(wb);\t \n\t\t\treturn wb;\n\t\t}\n\n\t\tspin_unlock(&wb->list_lock);\n\t\twb_put(wb);\n\t\tcpu_relax();\n\t\tspin_lock(&inode->i_lock);\n\t}\n}\n\n \nstatic struct bdi_writeback *inode_to_wb_and_lock_list(struct inode *inode)\n\t__acquires(&wb->list_lock)\n{\n\tspin_lock(&inode->i_lock);\n\treturn locked_inode_to_wb_and_lock_list(inode);\n}\n\nstruct inode_switch_wbs_context {\n\tstruct rcu_work\t\twork;\n\n\t \n\tstruct bdi_writeback\t*new_wb;\n\tstruct inode\t\t*inodes[];\n};\n\nstatic void bdi_down_write_wb_switch_rwsem(struct backing_dev_info *bdi)\n{\n\tdown_write(&bdi->wb_switch_rwsem);\n}\n\nstatic void bdi_up_write_wb_switch_rwsem(struct backing_dev_info *bdi)\n{\n\tup_write(&bdi->wb_switch_rwsem);\n}\n\nstatic bool inode_do_switch_wbs(struct inode *inode,\n\t\t\t\tstruct bdi_writeback *old_wb,\n\t\t\t\tstruct bdi_writeback *new_wb)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tXA_STATE(xas, &mapping->i_pages, 0);\n\tstruct folio *folio;\n\tbool switched = false;\n\n\tspin_lock(&inode->i_lock);\n\txa_lock_irq(&mapping->i_pages);\n\n\t \n\tif (unlikely(inode->i_state & (I_FREEING | I_WILL_FREE)))\n\t\tgoto skip_switch;\n\n\ttrace_inode_switch_wbs(inode, old_wb, new_wb);\n\n\t \n\txas_for_each_marked(&xas, folio, ULONG_MAX, PAGECACHE_TAG_DIRTY) {\n\t\tif (folio_test_dirty(folio)) {\n\t\t\tlong nr = folio_nr_pages(folio);\n\t\t\twb_stat_mod(old_wb, WB_RECLAIMABLE, -nr);\n\t\t\twb_stat_mod(new_wb, WB_RECLAIMABLE, nr);\n\t\t}\n\t}\n\n\txas_set(&xas, 0);\n\txas_for_each_marked(&xas, folio, ULONG_MAX, PAGECACHE_TAG_WRITEBACK) {\n\t\tlong nr = folio_nr_pages(folio);\n\t\tWARN_ON_ONCE(!folio_test_writeback(folio));\n\t\twb_stat_mod(old_wb, WB_WRITEBACK, -nr);\n\t\twb_stat_mod(new_wb, WB_WRITEBACK, nr);\n\t}\n\n\tif (mapping_tagged(mapping, PAGECACHE_TAG_WRITEBACK)) {\n\t\tatomic_dec(&old_wb->writeback_inodes);\n\t\tatomic_inc(&new_wb->writeback_inodes);\n\t}\n\n\twb_get(new_wb);\n\n\t \n\tif (!list_empty(&inode->i_io_list)) {\n\t\tinode->i_wb = new_wb;\n\n\t\tif (inode->i_state & I_DIRTY_ALL) {\n\t\t\tstruct inode *pos;\n\n\t\t\tlist_for_each_entry(pos, &new_wb->b_dirty, i_io_list)\n\t\t\t\tif (time_after_eq(inode->dirtied_when,\n\t\t\t\t\t\t  pos->dirtied_when))\n\t\t\t\t\tbreak;\n\t\t\tinode_io_list_move_locked(inode, new_wb,\n\t\t\t\t\t\t  pos->i_io_list.prev);\n\t\t} else {\n\t\t\tinode_cgwb_move_to_attached(inode, new_wb);\n\t\t}\n\t} else {\n\t\tinode->i_wb = new_wb;\n\t}\n\n\t \n\tinode->i_wb_frn_winner = 0;\n\tinode->i_wb_frn_avg_time = 0;\n\tinode->i_wb_frn_history = 0;\n\tswitched = true;\nskip_switch:\n\t \n\tsmp_store_release(&inode->i_state, inode->i_state & ~I_WB_SWITCH);\n\n\txa_unlock_irq(&mapping->i_pages);\n\tspin_unlock(&inode->i_lock);\n\n\treturn switched;\n}\n\nstatic void inode_switch_wbs_work_fn(struct work_struct *work)\n{\n\tstruct inode_switch_wbs_context *isw =\n\t\tcontainer_of(to_rcu_work(work), struct inode_switch_wbs_context, work);\n\tstruct backing_dev_info *bdi = inode_to_bdi(isw->inodes[0]);\n\tstruct bdi_writeback *old_wb = isw->inodes[0]->i_wb;\n\tstruct bdi_writeback *new_wb = isw->new_wb;\n\tunsigned long nr_switched = 0;\n\tstruct inode **inodep;\n\n\t \n\tdown_read(&bdi->wb_switch_rwsem);\n\n\t \n\tif (old_wb < new_wb) {\n\t\tspin_lock(&old_wb->list_lock);\n\t\tspin_lock_nested(&new_wb->list_lock, SINGLE_DEPTH_NESTING);\n\t} else {\n\t\tspin_lock(&new_wb->list_lock);\n\t\tspin_lock_nested(&old_wb->list_lock, SINGLE_DEPTH_NESTING);\n\t}\n\n\tfor (inodep = isw->inodes; *inodep; inodep++) {\n\t\tWARN_ON_ONCE((*inodep)->i_wb != old_wb);\n\t\tif (inode_do_switch_wbs(*inodep, old_wb, new_wb))\n\t\t\tnr_switched++;\n\t}\n\n\tspin_unlock(&new_wb->list_lock);\n\tspin_unlock(&old_wb->list_lock);\n\n\tup_read(&bdi->wb_switch_rwsem);\n\n\tif (nr_switched) {\n\t\twb_wakeup(new_wb);\n\t\twb_put_many(old_wb, nr_switched);\n\t}\n\n\tfor (inodep = isw->inodes; *inodep; inodep++)\n\t\tiput(*inodep);\n\twb_put(new_wb);\n\tkfree(isw);\n\tatomic_dec(&isw_nr_in_flight);\n}\n\nstatic bool inode_prepare_wbs_switch(struct inode *inode,\n\t\t\t\t     struct bdi_writeback *new_wb)\n{\n\t \n\tsmp_mb();\n\n\tif (IS_DAX(inode))\n\t\treturn false;\n\n\t \n\tspin_lock(&inode->i_lock);\n\tif (!(inode->i_sb->s_flags & SB_ACTIVE) ||\n\t    inode->i_state & (I_WB_SWITCH | I_FREEING | I_WILL_FREE) ||\n\t    inode_to_wb(inode) == new_wb) {\n\t\tspin_unlock(&inode->i_lock);\n\t\treturn false;\n\t}\n\tinode->i_state |= I_WB_SWITCH;\n\t__iget(inode);\n\tspin_unlock(&inode->i_lock);\n\n\treturn true;\n}\n\n \nstatic void inode_switch_wbs(struct inode *inode, int new_wb_id)\n{\n\tstruct backing_dev_info *bdi = inode_to_bdi(inode);\n\tstruct cgroup_subsys_state *memcg_css;\n\tstruct inode_switch_wbs_context *isw;\n\n\t \n\tif (inode->i_state & I_WB_SWITCH)\n\t\treturn;\n\n\t \n\tif (atomic_read(&isw_nr_in_flight) > WB_FRN_MAX_IN_FLIGHT)\n\t\treturn;\n\n\tisw = kzalloc(struct_size(isw, inodes, 2), GFP_ATOMIC);\n\tif (!isw)\n\t\treturn;\n\n\tatomic_inc(&isw_nr_in_flight);\n\n\t \n\trcu_read_lock();\n\tmemcg_css = css_from_id(new_wb_id, &memory_cgrp_subsys);\n\tif (memcg_css && !css_tryget(memcg_css))\n\t\tmemcg_css = NULL;\n\trcu_read_unlock();\n\tif (!memcg_css)\n\t\tgoto out_free;\n\n\tisw->new_wb = wb_get_create(bdi, memcg_css, GFP_ATOMIC);\n\tcss_put(memcg_css);\n\tif (!isw->new_wb)\n\t\tgoto out_free;\n\n\tif (!inode_prepare_wbs_switch(inode, isw->new_wb))\n\t\tgoto out_free;\n\n\tisw->inodes[0] = inode;\n\n\t \n\tINIT_RCU_WORK(&isw->work, inode_switch_wbs_work_fn);\n\tqueue_rcu_work(isw_wq, &isw->work);\n\treturn;\n\nout_free:\n\tatomic_dec(&isw_nr_in_flight);\n\tif (isw->new_wb)\n\t\twb_put(isw->new_wb);\n\tkfree(isw);\n}\n\nstatic bool isw_prepare_wbs_switch(struct inode_switch_wbs_context *isw,\n\t\t\t\t   struct list_head *list, int *nr)\n{\n\tstruct inode *inode;\n\n\tlist_for_each_entry(inode, list, i_io_list) {\n\t\tif (!inode_prepare_wbs_switch(inode, isw->new_wb))\n\t\t\tcontinue;\n\n\t\tisw->inodes[*nr] = inode;\n\t\t(*nr)++;\n\n\t\tif (*nr >= WB_MAX_INODES_PER_ISW - 1)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\n \nbool cleanup_offline_cgwb(struct bdi_writeback *wb)\n{\n\tstruct cgroup_subsys_state *memcg_css;\n\tstruct inode_switch_wbs_context *isw;\n\tint nr;\n\tbool restart = false;\n\n\tisw = kzalloc(struct_size(isw, inodes, WB_MAX_INODES_PER_ISW),\n\t\t      GFP_KERNEL);\n\tif (!isw)\n\t\treturn restart;\n\n\tatomic_inc(&isw_nr_in_flight);\n\n\tfor (memcg_css = wb->memcg_css->parent; memcg_css;\n\t     memcg_css = memcg_css->parent) {\n\t\tisw->new_wb = wb_get_create(wb->bdi, memcg_css, GFP_KERNEL);\n\t\tif (isw->new_wb)\n\t\t\tbreak;\n\t}\n\tif (unlikely(!isw->new_wb))\n\t\tisw->new_wb = &wb->bdi->wb;  \n\n\tnr = 0;\n\tspin_lock(&wb->list_lock);\n\t \n\trestart = isw_prepare_wbs_switch(isw, &wb->b_attached, &nr);\n\tif (!restart)\n\t\trestart = isw_prepare_wbs_switch(isw, &wb->b_dirty_time, &nr);\n\tspin_unlock(&wb->list_lock);\n\n\t \n\tif (nr == 0) {\n\t\tatomic_dec(&isw_nr_in_flight);\n\t\twb_put(isw->new_wb);\n\t\tkfree(isw);\n\t\treturn restart;\n\t}\n\n\t \n\tINIT_RCU_WORK(&isw->work, inode_switch_wbs_work_fn);\n\tqueue_rcu_work(isw_wq, &isw->work);\n\n\treturn restart;\n}\n\n \nvoid wbc_attach_and_unlock_inode(struct writeback_control *wbc,\n\t\t\t\t struct inode *inode)\n{\n\tif (!inode_cgwb_enabled(inode)) {\n\t\tspin_unlock(&inode->i_lock);\n\t\treturn;\n\t}\n\n\twbc->wb = inode_to_wb(inode);\n\twbc->inode = inode;\n\n\twbc->wb_id = wbc->wb->memcg_css->id;\n\twbc->wb_lcand_id = inode->i_wb_frn_winner;\n\twbc->wb_tcand_id = 0;\n\twbc->wb_bytes = 0;\n\twbc->wb_lcand_bytes = 0;\n\twbc->wb_tcand_bytes = 0;\n\n\twb_get(wbc->wb);\n\tspin_unlock(&inode->i_lock);\n\n\t \n\tif (unlikely(wb_dying(wbc->wb) && !css_is_dying(wbc->wb->memcg_css)))\n\t\tinode_switch_wbs(inode, wbc->wb_id);\n}\nEXPORT_SYMBOL_GPL(wbc_attach_and_unlock_inode);\n\n \nvoid wbc_detach_inode(struct writeback_control *wbc)\n{\n\tstruct bdi_writeback *wb = wbc->wb;\n\tstruct inode *inode = wbc->inode;\n\tunsigned long avg_time, max_bytes, max_time;\n\tu16 history;\n\tint max_id;\n\n\tif (!wb)\n\t\treturn;\n\n\thistory = inode->i_wb_frn_history;\n\tavg_time = inode->i_wb_frn_avg_time;\n\n\t \n\tif (wbc->wb_bytes >= wbc->wb_lcand_bytes &&\n\t    wbc->wb_bytes >= wbc->wb_tcand_bytes) {\n\t\tmax_id = wbc->wb_id;\n\t\tmax_bytes = wbc->wb_bytes;\n\t} else if (wbc->wb_lcand_bytes >= wbc->wb_tcand_bytes) {\n\t\tmax_id = wbc->wb_lcand_id;\n\t\tmax_bytes = wbc->wb_lcand_bytes;\n\t} else {\n\t\tmax_id = wbc->wb_tcand_id;\n\t\tmax_bytes = wbc->wb_tcand_bytes;\n\t}\n\n\t \n\tmax_time = DIV_ROUND_UP((max_bytes >> PAGE_SHIFT) << WB_FRN_TIME_SHIFT,\n\t\t\t\twb->avg_write_bandwidth);\n\tif (avg_time)\n\t\tavg_time += (max_time >> WB_FRN_TIME_AVG_SHIFT) -\n\t\t\t    (avg_time >> WB_FRN_TIME_AVG_SHIFT);\n\telse\n\t\tavg_time = max_time;\t \n\n\tif (max_time >= avg_time / WB_FRN_TIME_CUT_DIV) {\n\t\tint slots;\n\n\t\t \n\t\tslots = min(DIV_ROUND_UP(max_time, WB_FRN_HIST_UNIT),\n\t\t\t    (unsigned long)WB_FRN_HIST_MAX_SLOTS);\n\t\thistory <<= slots;\n\t\tif (wbc->wb_id != max_id)\n\t\t\thistory |= (1U << slots) - 1;\n\n\t\tif (history)\n\t\t\ttrace_inode_foreign_history(inode, wbc, history);\n\n\t\t \n\t\tif (hweight16(history) > WB_FRN_HIST_THR_SLOTS)\n\t\t\tinode_switch_wbs(inode, max_id);\n\t}\n\n\t \n\tinode->i_wb_frn_winner = max_id;\n\tinode->i_wb_frn_avg_time = min(avg_time, (unsigned long)U16_MAX);\n\tinode->i_wb_frn_history = history;\n\n\twb_put(wbc->wb);\n\twbc->wb = NULL;\n}\nEXPORT_SYMBOL_GPL(wbc_detach_inode);\n\n \nvoid wbc_account_cgroup_owner(struct writeback_control *wbc, struct page *page,\n\t\t\t      size_t bytes)\n{\n\tstruct folio *folio;\n\tstruct cgroup_subsys_state *css;\n\tint id;\n\n\t \n\tif (!wbc->wb || wbc->no_cgroup_owner)\n\t\treturn;\n\n\tfolio = page_folio(page);\n\tcss = mem_cgroup_css_from_folio(folio);\n\t \n\tif (!(css->flags & CSS_ONLINE))\n\t\treturn;\n\n\tid = css->id;\n\n\tif (id == wbc->wb_id) {\n\t\twbc->wb_bytes += bytes;\n\t\treturn;\n\t}\n\n\tif (id == wbc->wb_lcand_id)\n\t\twbc->wb_lcand_bytes += bytes;\n\n\t \n\tif (!wbc->wb_tcand_bytes)\n\t\twbc->wb_tcand_id = id;\n\tif (id == wbc->wb_tcand_id)\n\t\twbc->wb_tcand_bytes += bytes;\n\telse\n\t\twbc->wb_tcand_bytes -= min(bytes, wbc->wb_tcand_bytes);\n}\nEXPORT_SYMBOL_GPL(wbc_account_cgroup_owner);\n\n \nstatic long wb_split_bdi_pages(struct bdi_writeback *wb, long nr_pages)\n{\n\tunsigned long this_bw = wb->avg_write_bandwidth;\n\tunsigned long tot_bw = atomic_long_read(&wb->bdi->tot_write_bandwidth);\n\n\tif (nr_pages == LONG_MAX)\n\t\treturn LONG_MAX;\n\n\t \n\tif (!tot_bw || this_bw >= tot_bw)\n\t\treturn nr_pages;\n\telse\n\t\treturn DIV_ROUND_UP_ULL((u64)nr_pages * this_bw, tot_bw);\n}\n\n \nstatic void bdi_split_work_to_wbs(struct backing_dev_info *bdi,\n\t\t\t\t  struct wb_writeback_work *base_work,\n\t\t\t\t  bool skip_if_busy)\n{\n\tstruct bdi_writeback *last_wb = NULL;\n\tstruct bdi_writeback *wb = list_entry(&bdi->wb_list,\n\t\t\t\t\t      struct bdi_writeback, bdi_node);\n\n\tmight_sleep();\nrestart:\n\trcu_read_lock();\n\tlist_for_each_entry_continue_rcu(wb, &bdi->wb_list, bdi_node) {\n\t\tDEFINE_WB_COMPLETION(fallback_work_done, bdi);\n\t\tstruct wb_writeback_work fallback_work;\n\t\tstruct wb_writeback_work *work;\n\t\tlong nr_pages;\n\n\t\tif (last_wb) {\n\t\t\twb_put(last_wb);\n\t\t\tlast_wb = NULL;\n\t\t}\n\n\t\t \n\t\tif (!wb_has_dirty_io(wb) &&\n\t\t    (base_work->sync_mode == WB_SYNC_NONE ||\n\t\t     list_empty(&wb->b_dirty_time)))\n\t\t\tcontinue;\n\t\tif (skip_if_busy && writeback_in_progress(wb))\n\t\t\tcontinue;\n\n\t\tnr_pages = wb_split_bdi_pages(wb, base_work->nr_pages);\n\n\t\twork = kmalloc(sizeof(*work), GFP_ATOMIC);\n\t\tif (work) {\n\t\t\t*work = *base_work;\n\t\t\twork->nr_pages = nr_pages;\n\t\t\twork->auto_free = 1;\n\t\t\twb_queue_work(wb, work);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!wb_tryget(wb))\n\t\t\tcontinue;\n\n\t\t \n\t\twork = &fallback_work;\n\t\t*work = *base_work;\n\t\twork->nr_pages = nr_pages;\n\t\twork->auto_free = 0;\n\t\twork->done = &fallback_work_done;\n\n\t\twb_queue_work(wb, work);\n\t\tlast_wb = wb;\n\n\t\trcu_read_unlock();\n\t\twb_wait_for_completion(&fallback_work_done);\n\t\tgoto restart;\n\t}\n\trcu_read_unlock();\n\n\tif (last_wb)\n\t\twb_put(last_wb);\n}\n\n \nint cgroup_writeback_by_id(u64 bdi_id, int memcg_id,\n\t\t\t   enum wb_reason reason, struct wb_completion *done)\n{\n\tstruct backing_dev_info *bdi;\n\tstruct cgroup_subsys_state *memcg_css;\n\tstruct bdi_writeback *wb;\n\tstruct wb_writeback_work *work;\n\tunsigned long dirty;\n\tint ret;\n\n\t \n\tbdi = bdi_get_by_id(bdi_id);\n\tif (!bdi)\n\t\treturn -ENOENT;\n\n\trcu_read_lock();\n\tmemcg_css = css_from_id(memcg_id, &memory_cgrp_subsys);\n\tif (memcg_css && !css_tryget(memcg_css))\n\t\tmemcg_css = NULL;\n\trcu_read_unlock();\n\tif (!memcg_css) {\n\t\tret = -ENOENT;\n\t\tgoto out_bdi_put;\n\t}\n\n\t \n\twb = wb_get_lookup(bdi, memcg_css);\n\tif (!wb) {\n\t\tret = -ENOENT;\n\t\tgoto out_css_put;\n\t}\n\n\t \n\tdirty = memcg_page_state(mem_cgroup_from_css(memcg_css), NR_FILE_DIRTY);\n\tdirty = dirty * 10 / 8;\n\n\t \n\twork = kzalloc(sizeof(*work), GFP_NOWAIT | __GFP_NOWARN);\n\tif (work) {\n\t\twork->nr_pages = dirty;\n\t\twork->sync_mode = WB_SYNC_NONE;\n\t\twork->range_cyclic = 1;\n\t\twork->reason = reason;\n\t\twork->done = done;\n\t\twork->auto_free = 1;\n\t\twb_queue_work(wb, work);\n\t\tret = 0;\n\t} else {\n\t\tret = -ENOMEM;\n\t}\n\n\twb_put(wb);\nout_css_put:\n\tcss_put(memcg_css);\nout_bdi_put:\n\tbdi_put(bdi);\n\treturn ret;\n}\n\n \nvoid cgroup_writeback_umount(void)\n{\n\t \n\tsmp_mb();\n\n\tif (atomic_read(&isw_nr_in_flight)) {\n\t\t \n\t\trcu_barrier();\n\t\tflush_workqueue(isw_wq);\n\t}\n}\n\nstatic int __init cgroup_writeback_init(void)\n{\n\tisw_wq = alloc_workqueue(\"inode_switch_wbs\", 0, 0);\n\tif (!isw_wq)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\nfs_initcall(cgroup_writeback_init);\n\n#else\t \n\nstatic void bdi_down_write_wb_switch_rwsem(struct backing_dev_info *bdi) { }\nstatic void bdi_up_write_wb_switch_rwsem(struct backing_dev_info *bdi) { }\n\nstatic void inode_cgwb_move_to_attached(struct inode *inode,\n\t\t\t\t\tstruct bdi_writeback *wb)\n{\n\tassert_spin_locked(&wb->list_lock);\n\tassert_spin_locked(&inode->i_lock);\n\tWARN_ON_ONCE(inode->i_state & I_FREEING);\n\n\tinode->i_state &= ~I_SYNC_QUEUED;\n\tlist_del_init(&inode->i_io_list);\n\twb_io_lists_depopulated(wb);\n}\n\nstatic struct bdi_writeback *\nlocked_inode_to_wb_and_lock_list(struct inode *inode)\n\t__releases(&inode->i_lock)\n\t__acquires(&wb->list_lock)\n{\n\tstruct bdi_writeback *wb = inode_to_wb(inode);\n\n\tspin_unlock(&inode->i_lock);\n\tspin_lock(&wb->list_lock);\n\treturn wb;\n}\n\nstatic struct bdi_writeback *inode_to_wb_and_lock_list(struct inode *inode)\n\t__acquires(&wb->list_lock)\n{\n\tstruct bdi_writeback *wb = inode_to_wb(inode);\n\n\tspin_lock(&wb->list_lock);\n\treturn wb;\n}\n\nstatic long wb_split_bdi_pages(struct bdi_writeback *wb, long nr_pages)\n{\n\treturn nr_pages;\n}\n\nstatic void bdi_split_work_to_wbs(struct backing_dev_info *bdi,\n\t\t\t\t  struct wb_writeback_work *base_work,\n\t\t\t\t  bool skip_if_busy)\n{\n\tmight_sleep();\n\n\tif (!skip_if_busy || !writeback_in_progress(&bdi->wb)) {\n\t\tbase_work->auto_free = 0;\n\t\twb_queue_work(&bdi->wb, base_work);\n\t}\n}\n\n#endif\t \n\n \nstatic unsigned long get_nr_dirty_pages(void)\n{\n\treturn global_node_page_state(NR_FILE_DIRTY) +\n\t\tget_nr_dirty_inodes();\n}\n\nstatic void wb_start_writeback(struct bdi_writeback *wb, enum wb_reason reason)\n{\n\tif (!wb_has_dirty_io(wb))\n\t\treturn;\n\n\t \n\tif (test_bit(WB_start_all, &wb->state) ||\n\t    test_and_set_bit(WB_start_all, &wb->state))\n\t\treturn;\n\n\twb->start_all_reason = reason;\n\twb_wakeup(wb);\n}\n\n \nvoid wb_start_background_writeback(struct bdi_writeback *wb)\n{\n\t \n\ttrace_writeback_wake_background(wb);\n\twb_wakeup(wb);\n}\n\n \nvoid inode_io_list_del(struct inode *inode)\n{\n\tstruct bdi_writeback *wb;\n\n\twb = inode_to_wb_and_lock_list(inode);\n\tspin_lock(&inode->i_lock);\n\n\tinode->i_state &= ~I_SYNC_QUEUED;\n\tlist_del_init(&inode->i_io_list);\n\twb_io_lists_depopulated(wb);\n\n\tspin_unlock(&inode->i_lock);\n\tspin_unlock(&wb->list_lock);\n}\nEXPORT_SYMBOL(inode_io_list_del);\n\n \nvoid sb_mark_inode_writeback(struct inode *inode)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tunsigned long flags;\n\n\tif (list_empty(&inode->i_wb_list)) {\n\t\tspin_lock_irqsave(&sb->s_inode_wblist_lock, flags);\n\t\tif (list_empty(&inode->i_wb_list)) {\n\t\t\tlist_add_tail(&inode->i_wb_list, &sb->s_inodes_wb);\n\t\t\ttrace_sb_mark_inode_writeback(inode);\n\t\t}\n\t\tspin_unlock_irqrestore(&sb->s_inode_wblist_lock, flags);\n\t}\n}\n\n \nvoid sb_clear_inode_writeback(struct inode *inode)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tunsigned long flags;\n\n\tif (!list_empty(&inode->i_wb_list)) {\n\t\tspin_lock_irqsave(&sb->s_inode_wblist_lock, flags);\n\t\tif (!list_empty(&inode->i_wb_list)) {\n\t\t\tlist_del_init(&inode->i_wb_list);\n\t\t\ttrace_sb_clear_inode_writeback(inode);\n\t\t}\n\t\tspin_unlock_irqrestore(&sb->s_inode_wblist_lock, flags);\n\t}\n}\n\n \nstatic void redirty_tail_locked(struct inode *inode, struct bdi_writeback *wb)\n{\n\tassert_spin_locked(&inode->i_lock);\n\n\tinode->i_state &= ~I_SYNC_QUEUED;\n\t \n\tif (inode->i_state & I_FREEING) {\n\t\tlist_del_init(&inode->i_io_list);\n\t\twb_io_lists_depopulated(wb);\n\t\treturn;\n\t}\n\tif (!list_empty(&wb->b_dirty)) {\n\t\tstruct inode *tail;\n\n\t\ttail = wb_inode(wb->b_dirty.next);\n\t\tif (time_before(inode->dirtied_when, tail->dirtied_when))\n\t\t\tinode->dirtied_when = jiffies;\n\t}\n\tinode_io_list_move_locked(inode, wb, &wb->b_dirty);\n}\n\nstatic void redirty_tail(struct inode *inode, struct bdi_writeback *wb)\n{\n\tspin_lock(&inode->i_lock);\n\tredirty_tail_locked(inode, wb);\n\tspin_unlock(&inode->i_lock);\n}\n\n \nstatic void requeue_io(struct inode *inode, struct bdi_writeback *wb)\n{\n\tinode_io_list_move_locked(inode, wb, &wb->b_more_io);\n}\n\nstatic void inode_sync_complete(struct inode *inode)\n{\n\tinode->i_state &= ~I_SYNC;\n\t \n\tinode_add_lru(inode);\n\t \n\tsmp_mb();\n\twake_up_bit(&inode->i_state, __I_SYNC);\n}\n\nstatic bool inode_dirtied_after(struct inode *inode, unsigned long t)\n{\n\tbool ret = time_after(inode->dirtied_when, t);\n#ifndef CONFIG_64BIT\n\t \n\tret = ret && time_before_eq(inode->dirtied_when, jiffies);\n#endif\n\treturn ret;\n}\n\n \nstatic int move_expired_inodes(struct list_head *delaying_queue,\n\t\t\t       struct list_head *dispatch_queue,\n\t\t\t       unsigned long dirtied_before)\n{\n\tLIST_HEAD(tmp);\n\tstruct list_head *pos, *node;\n\tstruct super_block *sb = NULL;\n\tstruct inode *inode;\n\tint do_sb_sort = 0;\n\tint moved = 0;\n\n\twhile (!list_empty(delaying_queue)) {\n\t\tinode = wb_inode(delaying_queue->prev);\n\t\tif (inode_dirtied_after(inode, dirtied_before))\n\t\t\tbreak;\n\t\tspin_lock(&inode->i_lock);\n\t\tlist_move(&inode->i_io_list, &tmp);\n\t\tmoved++;\n\t\tinode->i_state |= I_SYNC_QUEUED;\n\t\tspin_unlock(&inode->i_lock);\n\t\tif (sb_is_blkdev_sb(inode->i_sb))\n\t\t\tcontinue;\n\t\tif (sb && sb != inode->i_sb)\n\t\t\tdo_sb_sort = 1;\n\t\tsb = inode->i_sb;\n\t}\n\n\t \n\tif (!do_sb_sort) {\n\t\tlist_splice(&tmp, dispatch_queue);\n\t\tgoto out;\n\t}\n\n\t \n\twhile (!list_empty(&tmp)) {\n\t\tsb = wb_inode(tmp.prev)->i_sb;\n\t\tlist_for_each_prev_safe(pos, node, &tmp) {\n\t\t\tinode = wb_inode(pos);\n\t\t\tif (inode->i_sb == sb)\n\t\t\t\tlist_move(&inode->i_io_list, dispatch_queue);\n\t\t}\n\t}\nout:\n\treturn moved;\n}\n\n \nstatic void queue_io(struct bdi_writeback *wb, struct wb_writeback_work *work,\n\t\t     unsigned long dirtied_before)\n{\n\tint moved;\n\tunsigned long time_expire_jif = dirtied_before;\n\n\tassert_spin_locked(&wb->list_lock);\n\tlist_splice_init(&wb->b_more_io, &wb->b_io);\n\tmoved = move_expired_inodes(&wb->b_dirty, &wb->b_io, dirtied_before);\n\tif (!work->for_sync)\n\t\ttime_expire_jif = jiffies - dirtytime_expire_interval * HZ;\n\tmoved += move_expired_inodes(&wb->b_dirty_time, &wb->b_io,\n\t\t\t\t     time_expire_jif);\n\tif (moved)\n\t\twb_io_lists_populated(wb);\n\ttrace_writeback_queue_io(wb, work, dirtied_before, moved);\n}\n\nstatic int write_inode(struct inode *inode, struct writeback_control *wbc)\n{\n\tint ret;\n\n\tif (inode->i_sb->s_op->write_inode && !is_bad_inode(inode)) {\n\t\ttrace_writeback_write_inode_start(inode, wbc);\n\t\tret = inode->i_sb->s_op->write_inode(inode, wbc);\n\t\ttrace_writeback_write_inode(inode, wbc);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\n \nstatic void __inode_wait_for_writeback(struct inode *inode)\n\t__releases(inode->i_lock)\n\t__acquires(inode->i_lock)\n{\n\tDEFINE_WAIT_BIT(wq, &inode->i_state, __I_SYNC);\n\twait_queue_head_t *wqh;\n\n\twqh = bit_waitqueue(&inode->i_state, __I_SYNC);\n\twhile (inode->i_state & I_SYNC) {\n\t\tspin_unlock(&inode->i_lock);\n\t\t__wait_on_bit(wqh, &wq, bit_wait,\n\t\t\t      TASK_UNINTERRUPTIBLE);\n\t\tspin_lock(&inode->i_lock);\n\t}\n}\n\n \nvoid inode_wait_for_writeback(struct inode *inode)\n{\n\tspin_lock(&inode->i_lock);\n\t__inode_wait_for_writeback(inode);\n\tspin_unlock(&inode->i_lock);\n}\n\n \nstatic void inode_sleep_on_writeback(struct inode *inode)\n\t__releases(inode->i_lock)\n{\n\tDEFINE_WAIT(wait);\n\twait_queue_head_t *wqh = bit_waitqueue(&inode->i_state, __I_SYNC);\n\tint sleep;\n\n\tprepare_to_wait(wqh, &wait, TASK_UNINTERRUPTIBLE);\n\tsleep = inode->i_state & I_SYNC;\n\tspin_unlock(&inode->i_lock);\n\tif (sleep)\n\t\tschedule();\n\tfinish_wait(wqh, &wait);\n}\n\n \nstatic void requeue_inode(struct inode *inode, struct bdi_writeback *wb,\n\t\t\t  struct writeback_control *wbc)\n{\n\tif (inode->i_state & I_FREEING)\n\t\treturn;\n\n\t \n\tif ((inode->i_state & I_DIRTY) &&\n\t    (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages))\n\t\tinode->dirtied_when = jiffies;\n\n\tif (wbc->pages_skipped) {\n\t\t \n\t\tif (inode->i_state & I_DIRTY_ALL)\n\t\t\tredirty_tail_locked(inode, wb);\n\t\telse\n\t\t\tinode_cgwb_move_to_attached(inode, wb);\n\t\treturn;\n\t}\n\n\tif (mapping_tagged(inode->i_mapping, PAGECACHE_TAG_DIRTY)) {\n\t\t \n\t\tif (wbc->nr_to_write <= 0) {\n\t\t\t \n\t\t\trequeue_io(inode, wb);\n\t\t} else {\n\t\t\t \n\t\t\tredirty_tail_locked(inode, wb);\n\t\t}\n\t} else if (inode->i_state & I_DIRTY) {\n\t\t \n\t\tredirty_tail_locked(inode, wb);\n\t} else if (inode->i_state & I_DIRTY_TIME) {\n\t\tinode->dirtied_when = jiffies;\n\t\tinode_io_list_move_locked(inode, wb, &wb->b_dirty_time);\n\t\tinode->i_state &= ~I_SYNC_QUEUED;\n\t} else {\n\t\t \n\t\tinode_cgwb_move_to_attached(inode, wb);\n\t}\n}\n\n \nstatic int\n__writeback_single_inode(struct inode *inode, struct writeback_control *wbc)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tlong nr_to_write = wbc->nr_to_write;\n\tunsigned dirty;\n\tint ret;\n\n\tWARN_ON(!(inode->i_state & I_SYNC));\n\n\ttrace_writeback_single_inode_start(inode, wbc, nr_to_write);\n\n\tret = do_writepages(mapping, wbc);\n\n\t \n\tif (wbc->sync_mode == WB_SYNC_ALL && !wbc->for_sync) {\n\t\tint err = filemap_fdatawait(mapping);\n\t\tif (ret == 0)\n\t\t\tret = err;\n\t}\n\n\t \n\tif ((inode->i_state & I_DIRTY_TIME) &&\n\t    (wbc->sync_mode == WB_SYNC_ALL ||\n\t     time_after(jiffies, inode->dirtied_time_when +\n\t\t\tdirtytime_expire_interval * HZ))) {\n\t\ttrace_writeback_lazytime(inode);\n\t\tmark_inode_dirty_sync(inode);\n\t}\n\n\t \n\tspin_lock(&inode->i_lock);\n\tdirty = inode->i_state & I_DIRTY;\n\tinode->i_state &= ~dirty;\n\n\t \n\tsmp_mb();\n\n\tif (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))\n\t\tinode->i_state |= I_DIRTY_PAGES;\n\telse if (unlikely(inode->i_state & I_PINNING_FSCACHE_WB)) {\n\t\tif (!(inode->i_state & I_DIRTY_PAGES)) {\n\t\t\tinode->i_state &= ~I_PINNING_FSCACHE_WB;\n\t\t\twbc->unpinned_fscache_wb = true;\n\t\t\tdirty |= I_PINNING_FSCACHE_WB;  \n\t\t}\n\t}\n\n\tspin_unlock(&inode->i_lock);\n\n\t \n\tif (dirty & ~I_DIRTY_PAGES) {\n\t\tint err = write_inode(inode, wbc);\n\t\tif (ret == 0)\n\t\t\tret = err;\n\t}\n\twbc->unpinned_fscache_wb = false;\n\ttrace_writeback_single_inode(inode, wbc, nr_to_write);\n\treturn ret;\n}\n\n \nstatic int writeback_single_inode(struct inode *inode,\n\t\t\t\t  struct writeback_control *wbc)\n{\n\tstruct bdi_writeback *wb;\n\tint ret = 0;\n\n\tspin_lock(&inode->i_lock);\n\tif (!atomic_read(&inode->i_count))\n\t\tWARN_ON(!(inode->i_state & (I_WILL_FREE|I_FREEING)));\n\telse\n\t\tWARN_ON(inode->i_state & I_WILL_FREE);\n\n\tif (inode->i_state & I_SYNC) {\n\t\t \n\t\tif (wbc->sync_mode != WB_SYNC_ALL)\n\t\t\tgoto out;\n\t\t__inode_wait_for_writeback(inode);\n\t}\n\tWARN_ON(inode->i_state & I_SYNC);\n\t \n\tif (!(inode->i_state & I_DIRTY_ALL) &&\n\t    (wbc->sync_mode != WB_SYNC_ALL ||\n\t     !mapping_tagged(inode->i_mapping, PAGECACHE_TAG_WRITEBACK)))\n\t\tgoto out;\n\tinode->i_state |= I_SYNC;\n\twbc_attach_and_unlock_inode(wbc, inode);\n\n\tret = __writeback_single_inode(inode, wbc);\n\n\twbc_detach_inode(wbc);\n\n\twb = inode_to_wb_and_lock_list(inode);\n\tspin_lock(&inode->i_lock);\n\t \n\tif (!(inode->i_state & I_FREEING)) {\n\t\t \n\t\tif (!(inode->i_state & I_DIRTY_ALL))\n\t\t\tinode_cgwb_move_to_attached(inode, wb);\n\t\telse if (!(inode->i_state & I_SYNC_QUEUED)) {\n\t\t\tif ((inode->i_state & I_DIRTY))\n\t\t\t\tredirty_tail_locked(inode, wb);\n\t\t\telse if (inode->i_state & I_DIRTY_TIME) {\n\t\t\t\tinode->dirtied_when = jiffies;\n\t\t\t\tinode_io_list_move_locked(inode,\n\t\t\t\t\t\t\t  wb,\n\t\t\t\t\t\t\t  &wb->b_dirty_time);\n\t\t\t}\n\t\t}\n\t}\n\n\tspin_unlock(&wb->list_lock);\n\tinode_sync_complete(inode);\nout:\n\tspin_unlock(&inode->i_lock);\n\treturn ret;\n}\n\nstatic long writeback_chunk_size(struct bdi_writeback *wb,\n\t\t\t\t struct wb_writeback_work *work)\n{\n\tlong pages;\n\n\t \n\tif (work->sync_mode == WB_SYNC_ALL || work->tagged_writepages)\n\t\tpages = LONG_MAX;\n\telse {\n\t\tpages = min(wb->avg_write_bandwidth / 2,\n\t\t\t    global_wb_domain.dirty_limit / DIRTY_SCOPE);\n\t\tpages = min(pages, work->nr_pages);\n\t\tpages = round_down(pages + MIN_WRITEBACK_PAGES,\n\t\t\t\t   MIN_WRITEBACK_PAGES);\n\t}\n\n\treturn pages;\n}\n\n \nstatic long writeback_sb_inodes(struct super_block *sb,\n\t\t\t\tstruct bdi_writeback *wb,\n\t\t\t\tstruct wb_writeback_work *work)\n{\n\tstruct writeback_control wbc = {\n\t\t.sync_mode\t\t= work->sync_mode,\n\t\t.tagged_writepages\t= work->tagged_writepages,\n\t\t.for_kupdate\t\t= work->for_kupdate,\n\t\t.for_background\t\t= work->for_background,\n\t\t.for_sync\t\t= work->for_sync,\n\t\t.range_cyclic\t\t= work->range_cyclic,\n\t\t.range_start\t\t= 0,\n\t\t.range_end\t\t= LLONG_MAX,\n\t};\n\tunsigned long start_time = jiffies;\n\tlong write_chunk;\n\tlong total_wrote = 0;   \n\n\twhile (!list_empty(&wb->b_io)) {\n\t\tstruct inode *inode = wb_inode(wb->b_io.prev);\n\t\tstruct bdi_writeback *tmp_wb;\n\t\tlong wrote;\n\n\t\tif (inode->i_sb != sb) {\n\t\t\tif (work->sb) {\n\t\t\t\t \n\t\t\t\tredirty_tail(inode, wb);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tspin_lock(&inode->i_lock);\n\t\tif (inode->i_state & (I_NEW | I_FREEING | I_WILL_FREE)) {\n\t\t\tredirty_tail_locked(inode, wb);\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tif ((inode->i_state & I_SYNC) && wbc.sync_mode != WB_SYNC_ALL) {\n\t\t\t \n\t\t\trequeue_io(inode, wb);\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\ttrace_writeback_sb_inodes_requeue(inode);\n\t\t\tcontinue;\n\t\t}\n\t\tspin_unlock(&wb->list_lock);\n\n\t\t \n\t\tif (inode->i_state & I_SYNC) {\n\t\t\t \n\t\t\tinode_sleep_on_writeback(inode);\n\t\t\t \n\t\t\tspin_lock(&wb->list_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tinode->i_state |= I_SYNC;\n\t\twbc_attach_and_unlock_inode(&wbc, inode);\n\n\t\twrite_chunk = writeback_chunk_size(wb, work);\n\t\twbc.nr_to_write = write_chunk;\n\t\twbc.pages_skipped = 0;\n\n\t\t \n\t\t__writeback_single_inode(inode, &wbc);\n\n\t\twbc_detach_inode(&wbc);\n\t\twork->nr_pages -= write_chunk - wbc.nr_to_write;\n\t\twrote = write_chunk - wbc.nr_to_write - wbc.pages_skipped;\n\t\twrote = wrote < 0 ? 0 : wrote;\n\t\ttotal_wrote += wrote;\n\n\t\tif (need_resched()) {\n\t\t\t \n\t\t\tblk_flush_plug(current->plug, false);\n\t\t\tcond_resched();\n\t\t}\n\n\t\t \n\t\ttmp_wb = inode_to_wb_and_lock_list(inode);\n\t\tspin_lock(&inode->i_lock);\n\t\tif (!(inode->i_state & I_DIRTY_ALL))\n\t\t\ttotal_wrote++;\n\t\trequeue_inode(inode, tmp_wb, &wbc);\n\t\tinode_sync_complete(inode);\n\t\tspin_unlock(&inode->i_lock);\n\n\t\tif (unlikely(tmp_wb != wb)) {\n\t\t\tspin_unlock(&tmp_wb->list_lock);\n\t\t\tspin_lock(&wb->list_lock);\n\t\t}\n\n\t\t \n\t\tif (total_wrote) {\n\t\t\tif (time_is_before_jiffies(start_time + HZ / 10UL))\n\t\t\t\tbreak;\n\t\t\tif (work->nr_pages <= 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\treturn total_wrote;\n}\n\nstatic long __writeback_inodes_wb(struct bdi_writeback *wb,\n\t\t\t\t  struct wb_writeback_work *work)\n{\n\tunsigned long start_time = jiffies;\n\tlong wrote = 0;\n\n\twhile (!list_empty(&wb->b_io)) {\n\t\tstruct inode *inode = wb_inode(wb->b_io.prev);\n\t\tstruct super_block *sb = inode->i_sb;\n\n\t\tif (!super_trylock_shared(sb)) {\n\t\t\t \n\t\t\tredirty_tail(inode, wb);\n\t\t\tcontinue;\n\t\t}\n\t\twrote += writeback_sb_inodes(sb, wb, work);\n\t\tup_read(&sb->s_umount);\n\n\t\t \n\t\tif (wrote) {\n\t\t\tif (time_is_before_jiffies(start_time + HZ / 10UL))\n\t\t\t\tbreak;\n\t\t\tif (work->nr_pages <= 0)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\t \n\treturn wrote;\n}\n\nstatic long writeback_inodes_wb(struct bdi_writeback *wb, long nr_pages,\n\t\t\t\tenum wb_reason reason)\n{\n\tstruct wb_writeback_work work = {\n\t\t.nr_pages\t= nr_pages,\n\t\t.sync_mode\t= WB_SYNC_NONE,\n\t\t.range_cyclic\t= 1,\n\t\t.reason\t\t= reason,\n\t};\n\tstruct blk_plug plug;\n\n\tblk_start_plug(&plug);\n\tspin_lock(&wb->list_lock);\n\tif (list_empty(&wb->b_io))\n\t\tqueue_io(wb, &work, jiffies);\n\t__writeback_inodes_wb(wb, &work);\n\tspin_unlock(&wb->list_lock);\n\tblk_finish_plug(&plug);\n\n\treturn nr_pages - work.nr_pages;\n}\n\n \nstatic long wb_writeback(struct bdi_writeback *wb,\n\t\t\t struct wb_writeback_work *work)\n{\n\tlong nr_pages = work->nr_pages;\n\tunsigned long dirtied_before = jiffies;\n\tstruct inode *inode;\n\tlong progress;\n\tstruct blk_plug plug;\n\n\tblk_start_plug(&plug);\n\tfor (;;) {\n\t\t \n\t\tif (work->nr_pages <= 0)\n\t\t\tbreak;\n\n\t\t \n\t\tif ((work->for_background || work->for_kupdate) &&\n\t\t    !list_empty(&wb->work_list))\n\t\t\tbreak;\n\n\t\t \n\t\tif (work->for_background && !wb_over_bg_thresh(wb))\n\t\t\tbreak;\n\n\n\t\tspin_lock(&wb->list_lock);\n\n\t\t \n\t\tif (work->for_kupdate) {\n\t\t\tdirtied_before = jiffies -\n\t\t\t\tmsecs_to_jiffies(dirty_expire_interval * 10);\n\t\t} else if (work->for_background)\n\t\t\tdirtied_before = jiffies;\n\n\t\ttrace_writeback_start(wb, work);\n\t\tif (list_empty(&wb->b_io))\n\t\t\tqueue_io(wb, work, dirtied_before);\n\t\tif (work->sb)\n\t\t\tprogress = writeback_sb_inodes(work->sb, wb, work);\n\t\telse\n\t\t\tprogress = __writeback_inodes_wb(wb, work);\n\t\ttrace_writeback_written(wb, work);\n\n\t\t \n\t\tif (progress) {\n\t\t\tspin_unlock(&wb->list_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (list_empty(&wb->b_more_io)) {\n\t\t\tspin_unlock(&wb->list_lock);\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\ttrace_writeback_wait(wb, work);\n\t\tinode = wb_inode(wb->b_more_io.prev);\n\t\tspin_lock(&inode->i_lock);\n\t\tspin_unlock(&wb->list_lock);\n\t\t \n\t\tinode_sleep_on_writeback(inode);\n\t}\n\tblk_finish_plug(&plug);\n\n\treturn nr_pages - work->nr_pages;\n}\n\n \nstatic struct wb_writeback_work *get_next_work_item(struct bdi_writeback *wb)\n{\n\tstruct wb_writeback_work *work = NULL;\n\n\tspin_lock_irq(&wb->work_lock);\n\tif (!list_empty(&wb->work_list)) {\n\t\twork = list_entry(wb->work_list.next,\n\t\t\t\t  struct wb_writeback_work, list);\n\t\tlist_del_init(&work->list);\n\t}\n\tspin_unlock_irq(&wb->work_lock);\n\treturn work;\n}\n\nstatic long wb_check_background_flush(struct bdi_writeback *wb)\n{\n\tif (wb_over_bg_thresh(wb)) {\n\n\t\tstruct wb_writeback_work work = {\n\t\t\t.nr_pages\t= LONG_MAX,\n\t\t\t.sync_mode\t= WB_SYNC_NONE,\n\t\t\t.for_background\t= 1,\n\t\t\t.range_cyclic\t= 1,\n\t\t\t.reason\t\t= WB_REASON_BACKGROUND,\n\t\t};\n\n\t\treturn wb_writeback(wb, &work);\n\t}\n\n\treturn 0;\n}\n\nstatic long wb_check_old_data_flush(struct bdi_writeback *wb)\n{\n\tunsigned long expired;\n\tlong nr_pages;\n\n\t \n\tif (!dirty_writeback_interval)\n\t\treturn 0;\n\n\texpired = wb->last_old_flush +\n\t\t\tmsecs_to_jiffies(dirty_writeback_interval * 10);\n\tif (time_before(jiffies, expired))\n\t\treturn 0;\n\n\twb->last_old_flush = jiffies;\n\tnr_pages = get_nr_dirty_pages();\n\n\tif (nr_pages) {\n\t\tstruct wb_writeback_work work = {\n\t\t\t.nr_pages\t= nr_pages,\n\t\t\t.sync_mode\t= WB_SYNC_NONE,\n\t\t\t.for_kupdate\t= 1,\n\t\t\t.range_cyclic\t= 1,\n\t\t\t.reason\t\t= WB_REASON_PERIODIC,\n\t\t};\n\n\t\treturn wb_writeback(wb, &work);\n\t}\n\n\treturn 0;\n}\n\nstatic long wb_check_start_all(struct bdi_writeback *wb)\n{\n\tlong nr_pages;\n\n\tif (!test_bit(WB_start_all, &wb->state))\n\t\treturn 0;\n\n\tnr_pages = get_nr_dirty_pages();\n\tif (nr_pages) {\n\t\tstruct wb_writeback_work work = {\n\t\t\t.nr_pages\t= wb_split_bdi_pages(wb, nr_pages),\n\t\t\t.sync_mode\t= WB_SYNC_NONE,\n\t\t\t.range_cyclic\t= 1,\n\t\t\t.reason\t\t= wb->start_all_reason,\n\t\t};\n\n\t\tnr_pages = wb_writeback(wb, &work);\n\t}\n\n\tclear_bit(WB_start_all, &wb->state);\n\treturn nr_pages;\n}\n\n\n \nstatic long wb_do_writeback(struct bdi_writeback *wb)\n{\n\tstruct wb_writeback_work *work;\n\tlong wrote = 0;\n\n\tset_bit(WB_writeback_running, &wb->state);\n\twhile ((work = get_next_work_item(wb)) != NULL) {\n\t\ttrace_writeback_exec(wb, work);\n\t\twrote += wb_writeback(wb, work);\n\t\tfinish_writeback_work(wb, work);\n\t}\n\n\t \n\twrote += wb_check_start_all(wb);\n\n\t \n\twrote += wb_check_old_data_flush(wb);\n\twrote += wb_check_background_flush(wb);\n\tclear_bit(WB_writeback_running, &wb->state);\n\n\treturn wrote;\n}\n\n \nvoid wb_workfn(struct work_struct *work)\n{\n\tstruct bdi_writeback *wb = container_of(to_delayed_work(work),\n\t\t\t\t\t\tstruct bdi_writeback, dwork);\n\tlong pages_written;\n\n\tset_worker_desc(\"flush-%s\", bdi_dev_name(wb->bdi));\n\n\tif (likely(!current_is_workqueue_rescuer() ||\n\t\t   !test_bit(WB_registered, &wb->state))) {\n\t\t \n\t\tdo {\n\t\t\tpages_written = wb_do_writeback(wb);\n\t\t\ttrace_writeback_pages_written(pages_written);\n\t\t} while (!list_empty(&wb->work_list));\n\t} else {\n\t\t \n\t\tpages_written = writeback_inodes_wb(wb, 1024,\n\t\t\t\t\t\t    WB_REASON_FORKER_THREAD);\n\t\ttrace_writeback_pages_written(pages_written);\n\t}\n\n\tif (!list_empty(&wb->work_list))\n\t\twb_wakeup(wb);\n\telse if (wb_has_dirty_io(wb) && dirty_writeback_interval)\n\t\twb_wakeup_delayed(wb);\n}\n\n \nstatic void __wakeup_flusher_threads_bdi(struct backing_dev_info *bdi,\n\t\t\t\t\t enum wb_reason reason)\n{\n\tstruct bdi_writeback *wb;\n\n\tif (!bdi_has_dirty_io(bdi))\n\t\treturn;\n\n\tlist_for_each_entry_rcu(wb, &bdi->wb_list, bdi_node)\n\t\twb_start_writeback(wb, reason);\n}\n\nvoid wakeup_flusher_threads_bdi(struct backing_dev_info *bdi,\n\t\t\t\tenum wb_reason reason)\n{\n\trcu_read_lock();\n\t__wakeup_flusher_threads_bdi(bdi, reason);\n\trcu_read_unlock();\n}\n\n \nvoid wakeup_flusher_threads(enum wb_reason reason)\n{\n\tstruct backing_dev_info *bdi;\n\n\t \n\tblk_flush_plug(current->plug, true);\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(bdi, &bdi_list, bdi_list)\n\t\t__wakeup_flusher_threads_bdi(bdi, reason);\n\trcu_read_unlock();\n}\n\n \nstatic void wakeup_dirtytime_writeback(struct work_struct *w);\nstatic DECLARE_DELAYED_WORK(dirtytime_work, wakeup_dirtytime_writeback);\n\nstatic void wakeup_dirtytime_writeback(struct work_struct *w)\n{\n\tstruct backing_dev_info *bdi;\n\n\trcu_read_lock();\n\tlist_for_each_entry_rcu(bdi, &bdi_list, bdi_list) {\n\t\tstruct bdi_writeback *wb;\n\n\t\tlist_for_each_entry_rcu(wb, &bdi->wb_list, bdi_node)\n\t\t\tif (!list_empty(&wb->b_dirty_time))\n\t\t\t\twb_wakeup(wb);\n\t}\n\trcu_read_unlock();\n\tschedule_delayed_work(&dirtytime_work, dirtytime_expire_interval * HZ);\n}\n\nstatic int __init start_dirtytime_writeback(void)\n{\n\tschedule_delayed_work(&dirtytime_work, dirtytime_expire_interval * HZ);\n\treturn 0;\n}\n__initcall(start_dirtytime_writeback);\n\nint dirtytime_interval_handler(struct ctl_table *table, int write,\n\t\t\t       void *buffer, size_t *lenp, loff_t *ppos)\n{\n\tint ret;\n\n\tret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);\n\tif (ret == 0 && write)\n\t\tmod_delayed_work(system_wq, &dirtytime_work, 0);\n\treturn ret;\n}\n\n \nvoid __mark_inode_dirty(struct inode *inode, int flags)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tint dirtytime = 0;\n\tstruct bdi_writeback *wb = NULL;\n\n\ttrace_writeback_mark_inode_dirty(inode, flags);\n\n\tif (flags & I_DIRTY_INODE) {\n\t\t \n\t\tif (inode->i_state & I_DIRTY_TIME) {\n\t\t\tspin_lock(&inode->i_lock);\n\t\t\tif (inode->i_state & I_DIRTY_TIME) {\n\t\t\t\tinode->i_state &= ~I_DIRTY_TIME;\n\t\t\t\tflags |= I_DIRTY_TIME;\n\t\t\t}\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t}\n\n\t\t \n\t\ttrace_writeback_dirty_inode_start(inode, flags);\n\t\tif (sb->s_op->dirty_inode)\n\t\t\tsb->s_op->dirty_inode(inode,\n\t\t\t\tflags & (I_DIRTY_INODE | I_DIRTY_TIME));\n\t\ttrace_writeback_dirty_inode(inode, flags);\n\n\t\t \n\t\tflags &= ~I_DIRTY_TIME;\n\t} else {\n\t\t \n\t\tdirtytime = flags & I_DIRTY_TIME;\n\t\tWARN_ON_ONCE(dirtytime && flags != I_DIRTY_TIME);\n\t}\n\n\t \n\tsmp_mb();\n\n\tif ((inode->i_state & flags) == flags)\n\t\treturn;\n\n\tspin_lock(&inode->i_lock);\n\tif ((inode->i_state & flags) != flags) {\n\t\tconst int was_dirty = inode->i_state & I_DIRTY;\n\n\t\tinode_attach_wb(inode, NULL);\n\n\t\tinode->i_state |= flags;\n\n\t\t \n\t\tif (!was_dirty) {\n\t\t\twb = locked_inode_to_wb_and_lock_list(inode);\n\t\t\tspin_lock(&inode->i_lock);\n\t\t}\n\n\t\t \n\t\tif (inode->i_state & I_SYNC_QUEUED)\n\t\t\tgoto out_unlock;\n\n\t\t \n\t\tif (!S_ISBLK(inode->i_mode)) {\n\t\t\tif (inode_unhashed(inode))\n\t\t\t\tgoto out_unlock;\n\t\t}\n\t\tif (inode->i_state & I_FREEING)\n\t\t\tgoto out_unlock;\n\n\t\t \n\t\tif (!was_dirty) {\n\t\t\tstruct list_head *dirty_list;\n\t\t\tbool wakeup_bdi = false;\n\n\t\t\tinode->dirtied_when = jiffies;\n\t\t\tif (dirtytime)\n\t\t\t\tinode->dirtied_time_when = jiffies;\n\n\t\t\tif (inode->i_state & I_DIRTY)\n\t\t\t\tdirty_list = &wb->b_dirty;\n\t\t\telse\n\t\t\t\tdirty_list = &wb->b_dirty_time;\n\n\t\t\twakeup_bdi = inode_io_list_move_locked(inode, wb,\n\t\t\t\t\t\t\t       dirty_list);\n\n\t\t\tspin_unlock(&wb->list_lock);\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\ttrace_writeback_dirty_inode_enqueue(inode);\n\n\t\t\t \n\t\t\tif (wakeup_bdi &&\n\t\t\t    (wb->bdi->capabilities & BDI_CAP_WRITEBACK))\n\t\t\t\twb_wakeup_delayed(wb);\n\t\t\treturn;\n\t\t}\n\t}\nout_unlock:\n\tif (wb)\n\t\tspin_unlock(&wb->list_lock);\n\tspin_unlock(&inode->i_lock);\n}\nEXPORT_SYMBOL(__mark_inode_dirty);\n\n \nstatic void wait_sb_inodes(struct super_block *sb)\n{\n\tLIST_HEAD(sync_list);\n\n\t \n\tWARN_ON(!rwsem_is_locked(&sb->s_umount));\n\n\tmutex_lock(&sb->s_sync_lock);\n\n\t \n\trcu_read_lock();\n\tspin_lock_irq(&sb->s_inode_wblist_lock);\n\tlist_splice_init(&sb->s_inodes_wb, &sync_list);\n\n\t \n\twhile (!list_empty(&sync_list)) {\n\t\tstruct inode *inode = list_first_entry(&sync_list, struct inode,\n\t\t\t\t\t\t       i_wb_list);\n\t\tstruct address_space *mapping = inode->i_mapping;\n\n\t\t \n\t\tlist_move_tail(&inode->i_wb_list, &sb->s_inodes_wb);\n\n\t\t \n\t\tif (!mapping_tagged(mapping, PAGECACHE_TAG_WRITEBACK))\n\t\t\tcontinue;\n\n\t\tspin_unlock_irq(&sb->s_inode_wblist_lock);\n\n\t\tspin_lock(&inode->i_lock);\n\t\tif (inode->i_state & (I_FREEING|I_WILL_FREE|I_NEW)) {\n\t\t\tspin_unlock(&inode->i_lock);\n\n\t\t\tspin_lock_irq(&sb->s_inode_wblist_lock);\n\t\t\tcontinue;\n\t\t}\n\t\t__iget(inode);\n\t\tspin_unlock(&inode->i_lock);\n\t\trcu_read_unlock();\n\n\t\t \n\t\tfilemap_fdatawait_keep_errors(mapping);\n\n\t\tcond_resched();\n\n\t\tiput(inode);\n\n\t\trcu_read_lock();\n\t\tspin_lock_irq(&sb->s_inode_wblist_lock);\n\t}\n\tspin_unlock_irq(&sb->s_inode_wblist_lock);\n\trcu_read_unlock();\n\tmutex_unlock(&sb->s_sync_lock);\n}\n\nstatic void __writeback_inodes_sb_nr(struct super_block *sb, unsigned long nr,\n\t\t\t\t     enum wb_reason reason, bool skip_if_busy)\n{\n\tstruct backing_dev_info *bdi = sb->s_bdi;\n\tDEFINE_WB_COMPLETION(done, bdi);\n\tstruct wb_writeback_work work = {\n\t\t.sb\t\t\t= sb,\n\t\t.sync_mode\t\t= WB_SYNC_NONE,\n\t\t.tagged_writepages\t= 1,\n\t\t.done\t\t\t= &done,\n\t\t.nr_pages\t\t= nr,\n\t\t.reason\t\t\t= reason,\n\t};\n\n\tif (!bdi_has_dirty_io(bdi) || bdi == &noop_backing_dev_info)\n\t\treturn;\n\tWARN_ON(!rwsem_is_locked(&sb->s_umount));\n\n\tbdi_split_work_to_wbs(sb->s_bdi, &work, skip_if_busy);\n\twb_wait_for_completion(&done);\n}\n\n \nvoid writeback_inodes_sb_nr(struct super_block *sb,\n\t\t\t    unsigned long nr,\n\t\t\t    enum wb_reason reason)\n{\n\t__writeback_inodes_sb_nr(sb, nr, reason, false);\n}\nEXPORT_SYMBOL(writeback_inodes_sb_nr);\n\n \nvoid writeback_inodes_sb(struct super_block *sb, enum wb_reason reason)\n{\n\treturn writeback_inodes_sb_nr(sb, get_nr_dirty_pages(), reason);\n}\nEXPORT_SYMBOL(writeback_inodes_sb);\n\n \nvoid try_to_writeback_inodes_sb(struct super_block *sb, enum wb_reason reason)\n{\n\tif (!down_read_trylock(&sb->s_umount))\n\t\treturn;\n\n\t__writeback_inodes_sb_nr(sb, get_nr_dirty_pages(), reason, true);\n\tup_read(&sb->s_umount);\n}\nEXPORT_SYMBOL(try_to_writeback_inodes_sb);\n\n \nvoid sync_inodes_sb(struct super_block *sb)\n{\n\tstruct backing_dev_info *bdi = sb->s_bdi;\n\tDEFINE_WB_COMPLETION(done, bdi);\n\tstruct wb_writeback_work work = {\n\t\t.sb\t\t= sb,\n\t\t.sync_mode\t= WB_SYNC_ALL,\n\t\t.nr_pages\t= LONG_MAX,\n\t\t.range_cyclic\t= 0,\n\t\t.done\t\t= &done,\n\t\t.reason\t\t= WB_REASON_SYNC,\n\t\t.for_sync\t= 1,\n\t};\n\n\t \n\tif (bdi == &noop_backing_dev_info)\n\t\treturn;\n\tWARN_ON(!rwsem_is_locked(&sb->s_umount));\n\n\t \n\tbdi_down_write_wb_switch_rwsem(bdi);\n\tbdi_split_work_to_wbs(bdi, &work, false);\n\twb_wait_for_completion(&done);\n\tbdi_up_write_wb_switch_rwsem(bdi);\n\n\twait_sb_inodes(sb);\n}\nEXPORT_SYMBOL(sync_inodes_sb);\n\n \nint write_inode_now(struct inode *inode, int sync)\n{\n\tstruct writeback_control wbc = {\n\t\t.nr_to_write = LONG_MAX,\n\t\t.sync_mode = sync ? WB_SYNC_ALL : WB_SYNC_NONE,\n\t\t.range_start = 0,\n\t\t.range_end = LLONG_MAX,\n\t};\n\n\tif (!mapping_can_writeback(inode->i_mapping))\n\t\twbc.nr_to_write = 0;\n\n\tmight_sleep();\n\treturn writeback_single_inode(inode, &wbc);\n}\nEXPORT_SYMBOL(write_inode_now);\n\n \nint sync_inode_metadata(struct inode *inode, int wait)\n{\n\tstruct writeback_control wbc = {\n\t\t.sync_mode = wait ? WB_SYNC_ALL : WB_SYNC_NONE,\n\t\t.nr_to_write = 0,  \n\t};\n\n\treturn writeback_single_inode(inode, &wbc);\n}\nEXPORT_SYMBOL(sync_inode_metadata);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}