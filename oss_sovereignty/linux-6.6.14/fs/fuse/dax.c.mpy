{
  "module_name": "dax.c",
  "hash_id": "c31c7eac6d7b8031873d14976feca79c65386e8f3f40006741bf0626e06d59bb",
  "original_prompt": "Ingested from linux-6.6.14/fs/fuse/dax.c",
  "human_readable_source": "\n \n\n#include \"fuse_i.h\"\n\n#include <linux/delay.h>\n#include <linux/dax.h>\n#include <linux/uio.h>\n#include <linux/pagemap.h>\n#include <linux/pfn_t.h>\n#include <linux/iomap.h>\n#include <linux/interval_tree.h>\n\n \n#define FUSE_DAX_SHIFT\t21\n#define FUSE_DAX_SZ\t(1 << FUSE_DAX_SHIFT)\n#define FUSE_DAX_PAGES\t(FUSE_DAX_SZ / PAGE_SIZE)\n\n \n#define FUSE_DAX_RECLAIM_CHUNK\t\t(10)\n\n \n#define FUSE_DAX_RECLAIM_THRESHOLD\t(20)\n\n \nstruct fuse_dax_mapping {\n\t \n\tstruct inode *inode;\n\n\t \n\tstruct list_head list;\n\n\t \n\tstruct interval_tree_node itn;\n\n\t \n\tstruct list_head busy_list;\n\n\t \n\tu64 window_offset;\n\n\t \n\tloff_t length;\n\n\t \n\tbool writable;\n\n\t \n\trefcount_t refcnt;\n};\n\n \nstruct fuse_inode_dax {\n\t \n\tstruct rw_semaphore sem;\n\n\t \n\tstruct rb_root_cached tree;\n\tunsigned long nr;\n};\n\nstruct fuse_conn_dax {\n\t \n\tstruct dax_device *dev;\n\n\t \n\tspinlock_t lock;\n\n\t \n\tunsigned long nr_busy_ranges;\n\tstruct list_head busy_ranges;\n\n\t \n\tstruct delayed_work free_work;\n\n\t \n\twait_queue_head_t range_waitq;\n\n\t \n\tlong nr_free_ranges;\n\tstruct list_head free_ranges;\n\n\tunsigned long nr_ranges;\n};\n\nstatic inline struct fuse_dax_mapping *\nnode_to_dmap(struct interval_tree_node *node)\n{\n\tif (!node)\n\t\treturn NULL;\n\n\treturn container_of(node, struct fuse_dax_mapping, itn);\n}\n\nstatic struct fuse_dax_mapping *\nalloc_dax_mapping_reclaim(struct fuse_conn_dax *fcd, struct inode *inode);\n\nstatic void\n__kick_dmap_free_worker(struct fuse_conn_dax *fcd, unsigned long delay_ms)\n{\n\tunsigned long free_threshold;\n\n\t \n\tfree_threshold = max_t(unsigned long, fcd->nr_ranges * FUSE_DAX_RECLAIM_THRESHOLD / 100,\n\t\t\t     1);\n\tif (fcd->nr_free_ranges < free_threshold)\n\t\tqueue_delayed_work(system_long_wq, &fcd->free_work,\n\t\t\t\t   msecs_to_jiffies(delay_ms));\n}\n\nstatic void kick_dmap_free_worker(struct fuse_conn_dax *fcd,\n\t\t\t\t  unsigned long delay_ms)\n{\n\tspin_lock(&fcd->lock);\n\t__kick_dmap_free_worker(fcd, delay_ms);\n\tspin_unlock(&fcd->lock);\n}\n\nstatic struct fuse_dax_mapping *alloc_dax_mapping(struct fuse_conn_dax *fcd)\n{\n\tstruct fuse_dax_mapping *dmap;\n\n\tspin_lock(&fcd->lock);\n\tdmap = list_first_entry_or_null(&fcd->free_ranges,\n\t\t\t\t\tstruct fuse_dax_mapping, list);\n\tif (dmap) {\n\t\tlist_del_init(&dmap->list);\n\t\tWARN_ON(fcd->nr_free_ranges <= 0);\n\t\tfcd->nr_free_ranges--;\n\t}\n\t__kick_dmap_free_worker(fcd, 0);\n\tspin_unlock(&fcd->lock);\n\n\treturn dmap;\n}\n\n \nstatic void __dmap_remove_busy_list(struct fuse_conn_dax *fcd,\n\t\t\t\t    struct fuse_dax_mapping *dmap)\n{\n\tlist_del_init(&dmap->busy_list);\n\tWARN_ON(fcd->nr_busy_ranges == 0);\n\tfcd->nr_busy_ranges--;\n}\n\nstatic void dmap_remove_busy_list(struct fuse_conn_dax *fcd,\n\t\t\t\t  struct fuse_dax_mapping *dmap)\n{\n\tspin_lock(&fcd->lock);\n\t__dmap_remove_busy_list(fcd, dmap);\n\tspin_unlock(&fcd->lock);\n}\n\n \nstatic void __dmap_add_to_free_pool(struct fuse_conn_dax *fcd,\n\t\t\t\tstruct fuse_dax_mapping *dmap)\n{\n\tlist_add_tail(&dmap->list, &fcd->free_ranges);\n\tfcd->nr_free_ranges++;\n\twake_up(&fcd->range_waitq);\n}\n\nstatic void dmap_add_to_free_pool(struct fuse_conn_dax *fcd,\n\t\t\t\tstruct fuse_dax_mapping *dmap)\n{\n\t \n\tspin_lock(&fcd->lock);\n\t__dmap_add_to_free_pool(fcd, dmap);\n\tspin_unlock(&fcd->lock);\n}\n\nstatic int fuse_setup_one_mapping(struct inode *inode, unsigned long start_idx,\n\t\t\t\t  struct fuse_dax_mapping *dmap, bool writable,\n\t\t\t\t  bool upgrade)\n{\n\tstruct fuse_mount *fm = get_fuse_mount(inode);\n\tstruct fuse_conn_dax *fcd = fm->fc->dax;\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\tstruct fuse_setupmapping_in inarg;\n\tloff_t offset = start_idx << FUSE_DAX_SHIFT;\n\tFUSE_ARGS(args);\n\tssize_t err;\n\n\tWARN_ON(fcd->nr_free_ranges < 0);\n\n\t \n\tmemset(&inarg, 0, sizeof(inarg));\n\tinarg.foffset = offset;\n\tinarg.fh = -1;\n\tinarg.moffset = dmap->window_offset;\n\tinarg.len = FUSE_DAX_SZ;\n\tinarg.flags |= FUSE_SETUPMAPPING_FLAG_READ;\n\tif (writable)\n\t\tinarg.flags |= FUSE_SETUPMAPPING_FLAG_WRITE;\n\targs.opcode = FUSE_SETUPMAPPING;\n\targs.nodeid = fi->nodeid;\n\targs.in_numargs = 1;\n\targs.in_args[0].size = sizeof(inarg);\n\targs.in_args[0].value = &inarg;\n\terr = fuse_simple_request(fm, &args);\n\tif (err < 0)\n\t\treturn err;\n\tdmap->writable = writable;\n\tif (!upgrade) {\n\t\t \n\t\tdmap->inode = inode;\n\t\tdmap->itn.start = dmap->itn.last = start_idx;\n\t\t \n\t\tinterval_tree_insert(&dmap->itn, &fi->dax->tree);\n\t\tfi->dax->nr++;\n\t\tspin_lock(&fcd->lock);\n\t\tlist_add_tail(&dmap->busy_list, &fcd->busy_ranges);\n\t\tfcd->nr_busy_ranges++;\n\t\tspin_unlock(&fcd->lock);\n\t}\n\treturn 0;\n}\n\nstatic int fuse_send_removemapping(struct inode *inode,\n\t\t\t\t   struct fuse_removemapping_in *inargp,\n\t\t\t\t   struct fuse_removemapping_one *remove_one)\n{\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\tstruct fuse_mount *fm = get_fuse_mount(inode);\n\tFUSE_ARGS(args);\n\n\targs.opcode = FUSE_REMOVEMAPPING;\n\targs.nodeid = fi->nodeid;\n\targs.in_numargs = 2;\n\targs.in_args[0].size = sizeof(*inargp);\n\targs.in_args[0].value = inargp;\n\targs.in_args[1].size = inargp->count * sizeof(*remove_one);\n\targs.in_args[1].value = remove_one;\n\treturn fuse_simple_request(fm, &args);\n}\n\nstatic int dmap_removemapping_list(struct inode *inode, unsigned int num,\n\t\t\t\t   struct list_head *to_remove)\n{\n\tstruct fuse_removemapping_one *remove_one, *ptr;\n\tstruct fuse_removemapping_in inarg;\n\tstruct fuse_dax_mapping *dmap;\n\tint ret, i = 0, nr_alloc;\n\n\tnr_alloc = min_t(unsigned int, num, FUSE_REMOVEMAPPING_MAX_ENTRY);\n\tremove_one = kmalloc_array(nr_alloc, sizeof(*remove_one), GFP_NOFS);\n\tif (!remove_one)\n\t\treturn -ENOMEM;\n\n\tptr = remove_one;\n\tlist_for_each_entry(dmap, to_remove, list) {\n\t\tptr->moffset = dmap->window_offset;\n\t\tptr->len = dmap->length;\n\t\tptr++;\n\t\ti++;\n\t\tnum--;\n\t\tif (i >= nr_alloc || num == 0) {\n\t\t\tmemset(&inarg, 0, sizeof(inarg));\n\t\t\tinarg.count = i;\n\t\t\tret = fuse_send_removemapping(inode, &inarg,\n\t\t\t\t\t\t      remove_one);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tptr = remove_one;\n\t\t\ti = 0;\n\t\t}\n\t}\nout:\n\tkfree(remove_one);\n\treturn ret;\n}\n\n \nstatic void dmap_reinit_add_to_free_pool(struct fuse_conn_dax *fcd,\n\t\t\t\t\t    struct fuse_dax_mapping *dmap)\n{\n\tpr_debug(\"fuse: freeing memory range start_idx=0x%lx end_idx=0x%lx window_offset=0x%llx length=0x%llx\\n\",\n\t\t dmap->itn.start, dmap->itn.last, dmap->window_offset,\n\t\t dmap->length);\n\t__dmap_remove_busy_list(fcd, dmap);\n\tdmap->inode = NULL;\n\tdmap->itn.start = dmap->itn.last = 0;\n\t__dmap_add_to_free_pool(fcd, dmap);\n}\n\n \nstatic void inode_reclaim_dmap_range(struct fuse_conn_dax *fcd,\n\t\t\t\t     struct inode *inode,\n\t\t\t\t     loff_t start, loff_t end)\n{\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\tstruct fuse_dax_mapping *dmap, *n;\n\tint err, num = 0;\n\tLIST_HEAD(to_remove);\n\tunsigned long start_idx = start >> FUSE_DAX_SHIFT;\n\tunsigned long end_idx = end >> FUSE_DAX_SHIFT;\n\tstruct interval_tree_node *node;\n\n\twhile (1) {\n\t\tnode = interval_tree_iter_first(&fi->dax->tree, start_idx,\n\t\t\t\t\t\tend_idx);\n\t\tif (!node)\n\t\t\tbreak;\n\t\tdmap = node_to_dmap(node);\n\t\t \n\t\tWARN_ON(refcount_read(&dmap->refcnt) > 1);\n\t\tinterval_tree_remove(&dmap->itn, &fi->dax->tree);\n\t\tnum++;\n\t\tlist_add(&dmap->list, &to_remove);\n\t}\n\n\t \n\tif (list_empty(&to_remove))\n\t\treturn;\n\n\tWARN_ON(fi->dax->nr < num);\n\tfi->dax->nr -= num;\n\terr = dmap_removemapping_list(inode, num, &to_remove);\n\tif (err && err != -ENOTCONN) {\n\t\tpr_warn(\"Failed to removemappings. start=0x%llx end=0x%llx\\n\",\n\t\t\tstart, end);\n\t}\n\tspin_lock(&fcd->lock);\n\tlist_for_each_entry_safe(dmap, n, &to_remove, list) {\n\t\tlist_del_init(&dmap->list);\n\t\tdmap_reinit_add_to_free_pool(fcd, dmap);\n\t}\n\tspin_unlock(&fcd->lock);\n}\n\nstatic int dmap_removemapping_one(struct inode *inode,\n\t\t\t\t  struct fuse_dax_mapping *dmap)\n{\n\tstruct fuse_removemapping_one forget_one;\n\tstruct fuse_removemapping_in inarg;\n\n\tmemset(&inarg, 0, sizeof(inarg));\n\tinarg.count = 1;\n\tmemset(&forget_one, 0, sizeof(forget_one));\n\tforget_one.moffset = dmap->window_offset;\n\tforget_one.len = dmap->length;\n\n\treturn fuse_send_removemapping(inode, &inarg, &forget_one);\n}\n\n \nvoid fuse_dax_inode_cleanup(struct inode *inode)\n{\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\n\t \n\tinode_reclaim_dmap_range(fc->dax, inode, 0, -1);\n\tWARN_ON(fi->dax->nr);\n}\n\nstatic void fuse_fill_iomap_hole(struct iomap *iomap, loff_t length)\n{\n\tiomap->addr = IOMAP_NULL_ADDR;\n\tiomap->length = length;\n\tiomap->type = IOMAP_HOLE;\n}\n\nstatic void fuse_fill_iomap(struct inode *inode, loff_t pos, loff_t length,\n\t\t\t    struct iomap *iomap, struct fuse_dax_mapping *dmap,\n\t\t\t    unsigned int flags)\n{\n\tloff_t offset, len;\n\tloff_t i_size = i_size_read(inode);\n\n\toffset = pos - (dmap->itn.start << FUSE_DAX_SHIFT);\n\tlen = min(length, dmap->length - offset);\n\n\t \n\tif (pos + len > i_size)\n\t\tlen = i_size - pos;\n\n\tif (len > 0) {\n\t\tiomap->addr = dmap->window_offset + offset;\n\t\tiomap->length = len;\n\t\tif (flags & IOMAP_FAULT)\n\t\t\tiomap->length = ALIGN(len, PAGE_SIZE);\n\t\tiomap->type = IOMAP_MAPPED;\n\t\t \n\t\trefcount_inc(&dmap->refcnt);\n\n\t\t \n\t\tWARN_ON_ONCE(iomap->private);\n\t\tiomap->private = dmap;\n\t} else {\n\t\t \n\t\tfuse_fill_iomap_hole(iomap, length);\n\t}\n}\n\nstatic int fuse_setup_new_dax_mapping(struct inode *inode, loff_t pos,\n\t\t\t\t      loff_t length, unsigned int flags,\n\t\t\t\t      struct iomap *iomap)\n{\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tstruct fuse_conn_dax *fcd = fc->dax;\n\tstruct fuse_dax_mapping *dmap, *alloc_dmap = NULL;\n\tint ret;\n\tbool writable = flags & IOMAP_WRITE;\n\tunsigned long start_idx = pos >> FUSE_DAX_SHIFT;\n\tstruct interval_tree_node *node;\n\n\t \n\tif (flags & IOMAP_FAULT) {\n\t\talloc_dmap = alloc_dax_mapping(fcd);\n\t\tif (!alloc_dmap)\n\t\t\treturn -EAGAIN;\n\t} else {\n\t\talloc_dmap = alloc_dax_mapping_reclaim(fcd, inode);\n\t\tif (IS_ERR(alloc_dmap))\n\t\t\treturn PTR_ERR(alloc_dmap);\n\t}\n\n\t \n\tif (WARN_ON(!alloc_dmap))\n\t\treturn -EIO;\n\n\t \n\tdown_write(&fi->dax->sem);\n\t \n\tnode = interval_tree_iter_first(&fi->dax->tree, start_idx, start_idx);\n\tif (node) {\n\t\tdmap = node_to_dmap(node);\n\t\tfuse_fill_iomap(inode, pos, length, iomap, dmap, flags);\n\t\tdmap_add_to_free_pool(fcd, alloc_dmap);\n\t\tup_write(&fi->dax->sem);\n\t\treturn 0;\n\t}\n\n\t \n\tret = fuse_setup_one_mapping(inode, pos >> FUSE_DAX_SHIFT, alloc_dmap,\n\t\t\t\t     writable, false);\n\tif (ret < 0) {\n\t\tdmap_add_to_free_pool(fcd, alloc_dmap);\n\t\tup_write(&fi->dax->sem);\n\t\treturn ret;\n\t}\n\tfuse_fill_iomap(inode, pos, length, iomap, alloc_dmap, flags);\n\tup_write(&fi->dax->sem);\n\treturn 0;\n}\n\nstatic int fuse_upgrade_dax_mapping(struct inode *inode, loff_t pos,\n\t\t\t\t    loff_t length, unsigned int flags,\n\t\t\t\t    struct iomap *iomap)\n{\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\tstruct fuse_dax_mapping *dmap;\n\tint ret;\n\tunsigned long idx = pos >> FUSE_DAX_SHIFT;\n\tstruct interval_tree_node *node;\n\n\t \n\tdown_write(&fi->dax->sem);\n\tnode = interval_tree_iter_first(&fi->dax->tree, idx, idx);\n\n\t \n\tret = -EIO;\n\tif (WARN_ON(!node))\n\t\tgoto out_err;\n\n\tdmap = node_to_dmap(node);\n\n\t \n\tif (refcount_dec_and_test(&dmap->refcnt)) {\n\t\t \n\t\tWARN_ON_ONCE(1);\n\t}\n\n\t \n\tif (dmap->writable) {\n\t\tret = 0;\n\t\tgoto out_fill_iomap;\n\t}\n\n\tret = fuse_setup_one_mapping(inode, pos >> FUSE_DAX_SHIFT, dmap, true,\n\t\t\t\t     true);\n\tif (ret < 0)\n\t\tgoto out_err;\nout_fill_iomap:\n\tfuse_fill_iomap(inode, pos, length, iomap, dmap, flags);\nout_err:\n\tup_write(&fi->dax->sem);\n\treturn ret;\n}\n\n \nstatic int fuse_iomap_begin(struct inode *inode, loff_t pos, loff_t length,\n\t\t\t    unsigned int flags, struct iomap *iomap,\n\t\t\t    struct iomap *srcmap)\n{\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tstruct fuse_dax_mapping *dmap;\n\tbool writable = flags & IOMAP_WRITE;\n\tunsigned long start_idx = pos >> FUSE_DAX_SHIFT;\n\tstruct interval_tree_node *node;\n\n\t \n\tif (WARN_ON(flags & IOMAP_REPORT))\n\t\treturn -EIO;\n\n\tiomap->offset = pos;\n\tiomap->flags = 0;\n\tiomap->bdev = NULL;\n\tiomap->dax_dev = fc->dax->dev;\n\n\t \n\tdown_read(&fi->dax->sem);\n\tnode = interval_tree_iter_first(&fi->dax->tree, start_idx, start_idx);\n\tif (node) {\n\t\tdmap = node_to_dmap(node);\n\t\tif (writable && !dmap->writable) {\n\t\t\t \n\t\t\trefcount_inc(&dmap->refcnt);\n\t\t\tup_read(&fi->dax->sem);\n\t\t\tpr_debug(\"%s: Upgrading mapping at offset 0x%llx length 0x%llx\\n\",\n\t\t\t\t __func__, pos, length);\n\t\t\treturn fuse_upgrade_dax_mapping(inode, pos, length,\n\t\t\t\t\t\t\tflags, iomap);\n\t\t} else {\n\t\t\tfuse_fill_iomap(inode, pos, length, iomap, dmap, flags);\n\t\t\tup_read(&fi->dax->sem);\n\t\t\treturn 0;\n\t\t}\n\t} else {\n\t\tup_read(&fi->dax->sem);\n\t\tpr_debug(\"%s: no mapping at offset 0x%llx length 0x%llx\\n\",\n\t\t\t\t__func__, pos, length);\n\t\tif (pos >= i_size_read(inode))\n\t\t\tgoto iomap_hole;\n\n\t\treturn fuse_setup_new_dax_mapping(inode, pos, length, flags,\n\t\t\t\t\t\t  iomap);\n\t}\n\n\t \niomap_hole:\n\tfuse_fill_iomap_hole(iomap, length);\n\tpr_debug(\"%s returning hole mapping. pos=0x%llx length_asked=0x%llx length_returned=0x%llx\\n\",\n\t\t __func__, pos, length, iomap->length);\n\treturn 0;\n}\n\nstatic int fuse_iomap_end(struct inode *inode, loff_t pos, loff_t length,\n\t\t\t  ssize_t written, unsigned int flags,\n\t\t\t  struct iomap *iomap)\n{\n\tstruct fuse_dax_mapping *dmap = iomap->private;\n\n\tif (dmap) {\n\t\tif (refcount_dec_and_test(&dmap->refcnt)) {\n\t\t\t \n\t\t\tWARN_ON_ONCE(1);\n\t\t}\n\t}\n\n\t \n\treturn 0;\n}\n\nstatic const struct iomap_ops fuse_iomap_ops = {\n\t.iomap_begin = fuse_iomap_begin,\n\t.iomap_end = fuse_iomap_end,\n};\n\nstatic void fuse_wait_dax_page(struct inode *inode)\n{\n\tfilemap_invalidate_unlock(inode->i_mapping);\n\tschedule();\n\tfilemap_invalidate_lock(inode->i_mapping);\n}\n\n \nstatic int __fuse_dax_break_layouts(struct inode *inode, bool *retry,\n\t\t\t\t    loff_t start, loff_t end)\n{\n\tstruct page *page;\n\n\tpage = dax_layout_busy_page_range(inode->i_mapping, start, end);\n\tif (!page)\n\t\treturn 0;\n\n\t*retry = true;\n\treturn ___wait_var_event(&page->_refcount,\n\t\t\tatomic_read(&page->_refcount) == 1, TASK_INTERRUPTIBLE,\n\t\t\t0, 0, fuse_wait_dax_page(inode));\n}\n\n \nint fuse_dax_break_layouts(struct inode *inode, u64 dmap_start,\n\t\t\t\t  u64 dmap_end)\n{\n\tbool\tretry;\n\tint\tret;\n\n\tdo {\n\t\tretry = false;\n\t\tret = __fuse_dax_break_layouts(inode, &retry, dmap_start,\n\t\t\t\t\t       dmap_end);\n\t} while (ret == 0 && retry);\n\n\treturn ret;\n}\n\nssize_t fuse_dax_read_iter(struct kiocb *iocb, struct iov_iter *to)\n{\n\tstruct inode *inode = file_inode(iocb->ki_filp);\n\tssize_t ret;\n\n\tif (iocb->ki_flags & IOCB_NOWAIT) {\n\t\tif (!inode_trylock_shared(inode))\n\t\t\treturn -EAGAIN;\n\t} else {\n\t\tinode_lock_shared(inode);\n\t}\n\n\tret = dax_iomap_rw(iocb, to, &fuse_iomap_ops);\n\tinode_unlock_shared(inode);\n\n\t \n\treturn ret;\n}\n\nstatic bool file_extending_write(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct inode *inode = file_inode(iocb->ki_filp);\n\n\treturn (iov_iter_rw(from) == WRITE &&\n\t\t((iocb->ki_pos) >= i_size_read(inode) ||\n\t\t  (iocb->ki_pos + iov_iter_count(from) > i_size_read(inode))));\n}\n\nstatic ssize_t fuse_dax_direct_write(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct inode *inode = file_inode(iocb->ki_filp);\n\tstruct fuse_io_priv io = FUSE_IO_PRIV_SYNC(iocb);\n\tssize_t ret;\n\n\tret = fuse_direct_io(&io, from, &iocb->ki_pos, FUSE_DIO_WRITE);\n\n\tfuse_write_update_attr(inode, iocb->ki_pos, ret);\n\treturn ret;\n}\n\nssize_t fuse_dax_write_iter(struct kiocb *iocb, struct iov_iter *from)\n{\n\tstruct inode *inode = file_inode(iocb->ki_filp);\n\tssize_t ret;\n\n\tif (iocb->ki_flags & IOCB_NOWAIT) {\n\t\tif (!inode_trylock(inode))\n\t\t\treturn -EAGAIN;\n\t} else {\n\t\tinode_lock(inode);\n\t}\n\n\tret = generic_write_checks(iocb, from);\n\tif (ret <= 0)\n\t\tgoto out;\n\n\tret = file_remove_privs(iocb->ki_filp);\n\tif (ret)\n\t\tgoto out;\n\t \n\n\t \n\tif (file_extending_write(iocb, from))\n\t\tret = fuse_dax_direct_write(iocb, from);\n\telse\n\t\tret = dax_iomap_rw(iocb, from, &fuse_iomap_ops);\n\nout:\n\tinode_unlock(inode);\n\n\tif (ret > 0)\n\t\tret = generic_write_sync(iocb, ret);\n\treturn ret;\n}\n\nstatic int fuse_dax_writepages(struct address_space *mapping,\n\t\t\t       struct writeback_control *wbc)\n{\n\n\tstruct inode *inode = mapping->host;\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\n\treturn dax_writeback_mapping_range(mapping, fc->dax->dev, wbc);\n}\n\nstatic vm_fault_t __fuse_dax_fault(struct vm_fault *vmf, unsigned int order,\n\t\tbool write)\n{\n\tvm_fault_t ret;\n\tstruct inode *inode = file_inode(vmf->vma->vm_file);\n\tstruct super_block *sb = inode->i_sb;\n\tpfn_t pfn;\n\tint error = 0;\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tstruct fuse_conn_dax *fcd = fc->dax;\n\tbool retry = false;\n\n\tif (write)\n\t\tsb_start_pagefault(sb);\nretry:\n\tif (retry && !(fcd->nr_free_ranges > 0))\n\t\twait_event(fcd->range_waitq, (fcd->nr_free_ranges > 0));\n\n\t \n\tfilemap_invalidate_lock_shared(inode->i_mapping);\n\tret = dax_iomap_fault(vmf, order, &pfn, &error, &fuse_iomap_ops);\n\tif ((ret & VM_FAULT_ERROR) && error == -EAGAIN) {\n\t\terror = 0;\n\t\tretry = true;\n\t\tfilemap_invalidate_unlock_shared(inode->i_mapping);\n\t\tgoto retry;\n\t}\n\n\tif (ret & VM_FAULT_NEEDDSYNC)\n\t\tret = dax_finish_sync_fault(vmf, order, pfn);\n\tfilemap_invalidate_unlock_shared(inode->i_mapping);\n\n\tif (write)\n\t\tsb_end_pagefault(sb);\n\n\treturn ret;\n}\n\nstatic vm_fault_t fuse_dax_fault(struct vm_fault *vmf)\n{\n\treturn __fuse_dax_fault(vmf, 0, vmf->flags & FAULT_FLAG_WRITE);\n}\n\nstatic vm_fault_t fuse_dax_huge_fault(struct vm_fault *vmf, unsigned int order)\n{\n\treturn __fuse_dax_fault(vmf, order, vmf->flags & FAULT_FLAG_WRITE);\n}\n\nstatic vm_fault_t fuse_dax_page_mkwrite(struct vm_fault *vmf)\n{\n\treturn __fuse_dax_fault(vmf, 0, true);\n}\n\nstatic vm_fault_t fuse_dax_pfn_mkwrite(struct vm_fault *vmf)\n{\n\treturn __fuse_dax_fault(vmf, 0, true);\n}\n\nstatic const struct vm_operations_struct fuse_dax_vm_ops = {\n\t.fault\t\t= fuse_dax_fault,\n\t.huge_fault\t= fuse_dax_huge_fault,\n\t.page_mkwrite\t= fuse_dax_page_mkwrite,\n\t.pfn_mkwrite\t= fuse_dax_pfn_mkwrite,\n};\n\nint fuse_dax_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tfile_accessed(file);\n\tvma->vm_ops = &fuse_dax_vm_ops;\n\tvm_flags_set(vma, VM_MIXEDMAP | VM_HUGEPAGE);\n\treturn 0;\n}\n\nstatic int dmap_writeback_invalidate(struct inode *inode,\n\t\t\t\t     struct fuse_dax_mapping *dmap)\n{\n\tint ret;\n\tloff_t start_pos = dmap->itn.start << FUSE_DAX_SHIFT;\n\tloff_t end_pos = (start_pos + FUSE_DAX_SZ - 1);\n\n\tret = filemap_fdatawrite_range(inode->i_mapping, start_pos, end_pos);\n\tif (ret) {\n\t\tpr_debug(\"fuse: filemap_fdatawrite_range() failed. err=%d start_pos=0x%llx, end_pos=0x%llx\\n\",\n\t\t\t ret, start_pos, end_pos);\n\t\treturn ret;\n\t}\n\n\tret = invalidate_inode_pages2_range(inode->i_mapping,\n\t\t\t\t\t    start_pos >> PAGE_SHIFT,\n\t\t\t\t\t    end_pos >> PAGE_SHIFT);\n\tif (ret)\n\t\tpr_debug(\"fuse: invalidate_inode_pages2_range() failed err=%d\\n\",\n\t\t\t ret);\n\n\treturn ret;\n}\n\nstatic int reclaim_one_dmap_locked(struct inode *inode,\n\t\t\t\t   struct fuse_dax_mapping *dmap)\n{\n\tint ret;\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\n\t \n\tret = dmap_writeback_invalidate(inode, dmap);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tinterval_tree_remove(&dmap->itn, &fi->dax->tree);\n\tfi->dax->nr--;\n\n\t \n\tret = dmap_removemapping_one(inode, dmap);\n\tif (ret && ret != -ENOTCONN) {\n\t\tpr_warn(\"Failed to remove mapping. offset=0x%llx len=0x%llx ret=%d\\n\",\n\t\t\tdmap->window_offset, dmap->length, ret);\n\t}\n\treturn 0;\n}\n\n \nstatic struct fuse_dax_mapping *inode_lookup_first_dmap(struct inode *inode)\n{\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\tstruct fuse_dax_mapping *dmap;\n\tstruct interval_tree_node *node;\n\n\tfor (node = interval_tree_iter_first(&fi->dax->tree, 0, -1); node;\n\t     node = interval_tree_iter_next(node, 0, -1)) {\n\t\tdmap = node_to_dmap(node);\n\t\t \n\t\tif (refcount_read(&dmap->refcnt) > 1)\n\t\t\tcontinue;\n\n\t\treturn dmap;\n\t}\n\n\treturn NULL;\n}\n\n \nstatic struct fuse_dax_mapping *\ninode_inline_reclaim_one_dmap(struct fuse_conn_dax *fcd, struct inode *inode,\n\t\t\t      bool *retry)\n{\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\tstruct fuse_dax_mapping *dmap;\n\tu64 dmap_start, dmap_end;\n\tunsigned long start_idx;\n\tint ret;\n\tstruct interval_tree_node *node;\n\n\tfilemap_invalidate_lock(inode->i_mapping);\n\n\t \n\tdown_read(&fi->dax->sem);\n\tdmap = inode_lookup_first_dmap(inode);\n\tif (dmap) {\n\t\tstart_idx = dmap->itn.start;\n\t\tdmap_start = start_idx << FUSE_DAX_SHIFT;\n\t\tdmap_end = dmap_start + FUSE_DAX_SZ - 1;\n\t}\n\tup_read(&fi->dax->sem);\n\n\tif (!dmap)\n\t\tgoto out_mmap_sem;\n\t \n\tret = fuse_dax_break_layouts(inode, dmap_start, dmap_end);\n\tif (ret) {\n\t\tpr_debug(\"fuse: fuse_dax_break_layouts() failed. err=%d\\n\",\n\t\t\t ret);\n\t\tdmap = ERR_PTR(ret);\n\t\tgoto out_mmap_sem;\n\t}\n\n\tdown_write(&fi->dax->sem);\n\tnode = interval_tree_iter_first(&fi->dax->tree, start_idx, start_idx);\n\t \n\tif (!node) {\n\t\tif (retry)\n\t\t\t*retry = true;\n\t\tgoto out_write_dmap_sem;\n\t}\n\n\tdmap = node_to_dmap(node);\n\t \n\tif (refcount_read(&dmap->refcnt) > 1) {\n\t\tdmap = NULL;\n\t\tif (retry)\n\t\t\t*retry = true;\n\t\tgoto out_write_dmap_sem;\n\t}\n\n\tret = reclaim_one_dmap_locked(inode, dmap);\n\tif (ret < 0) {\n\t\tdmap = ERR_PTR(ret);\n\t\tgoto out_write_dmap_sem;\n\t}\n\n\t \n\tdmap_remove_busy_list(fcd, dmap);\n\tdmap->inode = NULL;\n\tdmap->itn.start = dmap->itn.last = 0;\n\n\tpr_debug(\"fuse: %s: inline reclaimed memory range. inode=%p, window_offset=0x%llx, length=0x%llx\\n\",\n\t\t __func__, inode, dmap->window_offset, dmap->length);\n\nout_write_dmap_sem:\n\tup_write(&fi->dax->sem);\nout_mmap_sem:\n\tfilemap_invalidate_unlock(inode->i_mapping);\n\treturn dmap;\n}\n\nstatic struct fuse_dax_mapping *\nalloc_dax_mapping_reclaim(struct fuse_conn_dax *fcd, struct inode *inode)\n{\n\tstruct fuse_dax_mapping *dmap;\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\n\twhile (1) {\n\t\tbool retry = false;\n\n\t\tdmap = alloc_dax_mapping(fcd);\n\t\tif (dmap)\n\t\t\treturn dmap;\n\n\t\tdmap = inode_inline_reclaim_one_dmap(fcd, inode, &retry);\n\t\t \n\t\tif (dmap)\n\t\t\treturn dmap;\n\n\t\t \n\t\tif (retry)\n\t\t\tcontinue;\n\t\t \n\t\tif (!fi->dax->nr && !(fcd->nr_free_ranges > 0)) {\n\t\t\tif (wait_event_killable_exclusive(fcd->range_waitq,\n\t\t\t\t\t(fcd->nr_free_ranges > 0))) {\n\t\t\t\treturn ERR_PTR(-EINTR);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic int lookup_and_reclaim_dmap_locked(struct fuse_conn_dax *fcd,\n\t\t\t\t\t  struct inode *inode,\n\t\t\t\t\t  unsigned long start_idx)\n{\n\tint ret;\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\tstruct fuse_dax_mapping *dmap;\n\tstruct interval_tree_node *node;\n\n\t \n\tnode = interval_tree_iter_first(&fi->dax->tree, start_idx, start_idx);\n\n\t \n\tif (!node)\n\t\treturn 0;\n\tdmap = node_to_dmap(node);\n\n\t \n\tif (refcount_read(&dmap->refcnt) > 1)\n\t\treturn 0;\n\n\tret = reclaim_one_dmap_locked(inode, dmap);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tspin_lock(&fcd->lock);\n\tdmap_reinit_add_to_free_pool(fcd, dmap);\n\tspin_unlock(&fcd->lock);\n\treturn ret;\n}\n\n \nstatic int lookup_and_reclaim_dmap(struct fuse_conn_dax *fcd,\n\t\t\t\t   struct inode *inode,\n\t\t\t\t   unsigned long start_idx,\n\t\t\t\t   unsigned long end_idx)\n{\n\tint ret;\n\tstruct fuse_inode *fi = get_fuse_inode(inode);\n\tloff_t dmap_start = start_idx << FUSE_DAX_SHIFT;\n\tloff_t dmap_end = (dmap_start + FUSE_DAX_SZ) - 1;\n\n\tfilemap_invalidate_lock(inode->i_mapping);\n\tret = fuse_dax_break_layouts(inode, dmap_start, dmap_end);\n\tif (ret) {\n\t\tpr_debug(\"virtio_fs: fuse_dax_break_layouts() failed. err=%d\\n\",\n\t\t\t ret);\n\t\tgoto out_mmap_sem;\n\t}\n\n\tdown_write(&fi->dax->sem);\n\tret = lookup_and_reclaim_dmap_locked(fcd, inode, start_idx);\n\tup_write(&fi->dax->sem);\nout_mmap_sem:\n\tfilemap_invalidate_unlock(inode->i_mapping);\n\treturn ret;\n}\n\nstatic int try_to_free_dmap_chunks(struct fuse_conn_dax *fcd,\n\t\t\t\t   unsigned long nr_to_free)\n{\n\tstruct fuse_dax_mapping *dmap, *pos, *temp;\n\tint ret, nr_freed = 0;\n\tunsigned long start_idx = 0, end_idx = 0;\n\tstruct inode *inode = NULL;\n\n\t \n\twhile (1) {\n\t\tif (nr_freed >= nr_to_free)\n\t\t\tbreak;\n\n\t\tdmap = NULL;\n\t\tspin_lock(&fcd->lock);\n\n\t\tif (!fcd->nr_busy_ranges) {\n\t\t\tspin_unlock(&fcd->lock);\n\t\t\treturn 0;\n\t\t}\n\n\t\tlist_for_each_entry_safe(pos, temp, &fcd->busy_ranges,\n\t\t\t\t\t\tbusy_list) {\n\t\t\t \n\t\t\tif (refcount_read(&pos->refcnt) > 1)\n\t\t\t\tcontinue;\n\n\t\t\tinode = igrab(pos->inode);\n\t\t\t \n\t\t\tif (!inode)\n\t\t\t\tcontinue;\n\t\t\t \n\t\t\tdmap = pos;\n\t\t\tlist_move_tail(&dmap->busy_list, &fcd->busy_ranges);\n\t\t\tstart_idx = end_idx = dmap->itn.start;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&fcd->lock);\n\t\tif (!dmap)\n\t\t\treturn 0;\n\n\t\tret = lookup_and_reclaim_dmap(fcd, inode, start_idx, end_idx);\n\t\tiput(inode);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tnr_freed++;\n\t}\n\treturn 0;\n}\n\nstatic void fuse_dax_free_mem_worker(struct work_struct *work)\n{\n\tint ret;\n\tstruct fuse_conn_dax *fcd = container_of(work, struct fuse_conn_dax,\n\t\t\t\t\t\t free_work.work);\n\tret = try_to_free_dmap_chunks(fcd, FUSE_DAX_RECLAIM_CHUNK);\n\tif (ret) {\n\t\tpr_debug(\"fuse: try_to_free_dmap_chunks() failed with err=%d\\n\",\n\t\t\t ret);\n\t}\n\n\t \n\tkick_dmap_free_worker(fcd, 1);\n}\n\nstatic void fuse_free_dax_mem_ranges(struct list_head *mem_list)\n{\n\tstruct fuse_dax_mapping *range, *temp;\n\n\t \n\tlist_for_each_entry_safe(range, temp, mem_list, list) {\n\t\tlist_del(&range->list);\n\t\tif (!list_empty(&range->busy_list))\n\t\t\tlist_del(&range->busy_list);\n\t\tkfree(range);\n\t}\n}\n\nvoid fuse_dax_conn_free(struct fuse_conn *fc)\n{\n\tif (fc->dax) {\n\t\tfuse_free_dax_mem_ranges(&fc->dax->free_ranges);\n\t\tkfree(fc->dax);\n\t\tfc->dax = NULL;\n\t}\n}\n\nstatic int fuse_dax_mem_range_init(struct fuse_conn_dax *fcd)\n{\n\tlong nr_pages, nr_ranges;\n\tstruct fuse_dax_mapping *range;\n\tint ret, id;\n\tsize_t dax_size = -1;\n\tunsigned long i;\n\n\tinit_waitqueue_head(&fcd->range_waitq);\n\tINIT_LIST_HEAD(&fcd->free_ranges);\n\tINIT_LIST_HEAD(&fcd->busy_ranges);\n\tINIT_DELAYED_WORK(&fcd->free_work, fuse_dax_free_mem_worker);\n\n\tid = dax_read_lock();\n\tnr_pages = dax_direct_access(fcd->dev, 0, PHYS_PFN(dax_size),\n\t\t\tDAX_ACCESS, NULL, NULL);\n\tdax_read_unlock(id);\n\tif (nr_pages < 0) {\n\t\tpr_debug(\"dax_direct_access() returned %ld\\n\", nr_pages);\n\t\treturn nr_pages;\n\t}\n\n\tnr_ranges = nr_pages/FUSE_DAX_PAGES;\n\tpr_debug(\"%s: dax mapped %ld pages. nr_ranges=%ld\\n\",\n\t\t__func__, nr_pages, nr_ranges);\n\n\tfor (i = 0; i < nr_ranges; i++) {\n\t\trange = kzalloc(sizeof(struct fuse_dax_mapping), GFP_KERNEL);\n\t\tret = -ENOMEM;\n\t\tif (!range)\n\t\t\tgoto out_err;\n\n\t\t \n\t\trange->window_offset = i * FUSE_DAX_SZ;\n\t\trange->length = FUSE_DAX_SZ;\n\t\tINIT_LIST_HEAD(&range->busy_list);\n\t\trefcount_set(&range->refcnt, 1);\n\t\tlist_add_tail(&range->list, &fcd->free_ranges);\n\t}\n\n\tfcd->nr_free_ranges = nr_ranges;\n\tfcd->nr_ranges = nr_ranges;\n\treturn 0;\nout_err:\n\t \n\tfuse_free_dax_mem_ranges(&fcd->free_ranges);\n\treturn ret;\n}\n\nint fuse_dax_conn_alloc(struct fuse_conn *fc, enum fuse_dax_mode dax_mode,\n\t\t\tstruct dax_device *dax_dev)\n{\n\tstruct fuse_conn_dax *fcd;\n\tint err;\n\n\tfc->dax_mode = dax_mode;\n\n\tif (!dax_dev)\n\t\treturn 0;\n\n\tfcd = kzalloc(sizeof(*fcd), GFP_KERNEL);\n\tif (!fcd)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&fcd->lock);\n\tfcd->dev = dax_dev;\n\terr = fuse_dax_mem_range_init(fcd);\n\tif (err) {\n\t\tkfree(fcd);\n\t\treturn err;\n\t}\n\n\tfc->dax = fcd;\n\treturn 0;\n}\n\nbool fuse_dax_inode_alloc(struct super_block *sb, struct fuse_inode *fi)\n{\n\tstruct fuse_conn *fc = get_fuse_conn_super(sb);\n\n\tfi->dax = NULL;\n\tif (fc->dax) {\n\t\tfi->dax = kzalloc(sizeof(*fi->dax), GFP_KERNEL_ACCOUNT);\n\t\tif (!fi->dax)\n\t\t\treturn false;\n\n\t\tinit_rwsem(&fi->dax->sem);\n\t\tfi->dax->tree = RB_ROOT_CACHED;\n\t}\n\n\treturn true;\n}\n\nstatic const struct address_space_operations fuse_dax_file_aops  = {\n\t.writepages\t= fuse_dax_writepages,\n\t.direct_IO\t= noop_direct_IO,\n\t.dirty_folio\t= noop_dirty_folio,\n};\n\nstatic bool fuse_should_enable_dax(struct inode *inode, unsigned int flags)\n{\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\tenum fuse_dax_mode dax_mode = fc->dax_mode;\n\n\tif (dax_mode == FUSE_DAX_NEVER)\n\t\treturn false;\n\n\t \n\tif (!fc->dax)\n\t\treturn false;\n\n\tif (dax_mode == FUSE_DAX_ALWAYS)\n\t\treturn true;\n\n\t \n\treturn fc->inode_dax && (flags & FUSE_ATTR_DAX);\n}\n\nvoid fuse_dax_inode_init(struct inode *inode, unsigned int flags)\n{\n\tif (!fuse_should_enable_dax(inode, flags))\n\t\treturn;\n\n\tinode->i_flags |= S_DAX;\n\tinode->i_data.a_ops = &fuse_dax_file_aops;\n}\n\nvoid fuse_dax_dontcache(struct inode *inode, unsigned int flags)\n{\n\tstruct fuse_conn *fc = get_fuse_conn(inode);\n\n\tif (fuse_is_inode_dax_mode(fc->dax_mode) &&\n\t    ((bool) IS_DAX(inode) != (bool) (flags & FUSE_ATTR_DAX)))\n\t\td_mark_dontcache(inode);\n}\n\nbool fuse_dax_check_alignment(struct fuse_conn *fc, unsigned int map_alignment)\n{\n\tif (fc->dax && (map_alignment > FUSE_DAX_SHIFT)) {\n\t\tpr_warn(\"FUSE: map_alignment %u incompatible with dax mem range size %u\\n\",\n\t\t\tmap_alignment, FUSE_DAX_SZ);\n\t\treturn false;\n\t}\n\treturn true;\n}\n\nvoid fuse_dax_cancel_work(struct fuse_conn *fc)\n{\n\tstruct fuse_conn_dax *fcd = fc->dax;\n\n\tif (fcd)\n\t\tcancel_delayed_work_sync(&fcd->free_work);\n\n}\nEXPORT_SYMBOL_GPL(fuse_dax_cancel_work);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}