{
  "module_name": "aio.c",
  "hash_id": "fb50ee014085e5c0e096fe12644533d5c268c867f7d8a92006eea99ae3380942",
  "original_prompt": "Ingested from linux-6.6.14/fs/aio.c",
  "human_readable_source": " \n#define pr_fmt(fmt) \"%s: \" fmt, __func__\n\n#include <linux/kernel.h>\n#include <linux/init.h>\n#include <linux/errno.h>\n#include <linux/time.h>\n#include <linux/aio_abi.h>\n#include <linux/export.h>\n#include <linux/syscalls.h>\n#include <linux/backing-dev.h>\n#include <linux/refcount.h>\n#include <linux/uio.h>\n\n#include <linux/sched/signal.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/mm.h>\n#include <linux/mman.h>\n#include <linux/percpu.h>\n#include <linux/slab.h>\n#include <linux/timer.h>\n#include <linux/aio.h>\n#include <linux/highmem.h>\n#include <linux/workqueue.h>\n#include <linux/security.h>\n#include <linux/eventfd.h>\n#include <linux/blkdev.h>\n#include <linux/compat.h>\n#include <linux/migrate.h>\n#include <linux/ramfs.h>\n#include <linux/percpu-refcount.h>\n#include <linux/mount.h>\n#include <linux/pseudo_fs.h>\n\n#include <linux/uaccess.h>\n#include <linux/nospec.h>\n\n#include \"internal.h\"\n\n#define KIOCB_KEY\t\t0\n\n#define AIO_RING_MAGIC\t\t\t0xa10a10a1\n#define AIO_RING_COMPAT_FEATURES\t1\n#define AIO_RING_INCOMPAT_FEATURES\t0\nstruct aio_ring {\n\tunsigned\tid;\t \n\tunsigned\tnr;\t \n\tunsigned\thead;\t \n\tunsigned\ttail;\n\n\tunsigned\tmagic;\n\tunsigned\tcompat_features;\n\tunsigned\tincompat_features;\n\tunsigned\theader_length;\t \n\n\n\tstruct io_event\t\tio_events[];\n};  \n\n \n#define AIO_PLUG_THRESHOLD\t2\n\n#define AIO_RING_PAGES\t8\n\nstruct kioctx_table {\n\tstruct rcu_head\t\trcu;\n\tunsigned\t\tnr;\n\tstruct kioctx __rcu\t*table[] __counted_by(nr);\n};\n\nstruct kioctx_cpu {\n\tunsigned\t\treqs_available;\n};\n\nstruct ctx_rq_wait {\n\tstruct completion comp;\n\tatomic_t count;\n};\n\nstruct kioctx {\n\tstruct percpu_ref\tusers;\n\tatomic_t\t\tdead;\n\n\tstruct percpu_ref\treqs;\n\n\tunsigned long\t\tuser_id;\n\n\tstruct __percpu kioctx_cpu *cpu;\n\n\t \n\tunsigned\t\treq_batch;\n\t \n\tunsigned\t\tmax_reqs;\n\n\t \n\tunsigned\t\tnr_events;\n\n\tunsigned long\t\tmmap_base;\n\tunsigned long\t\tmmap_size;\n\n\tstruct page\t\t**ring_pages;\n\tlong\t\t\tnr_pages;\n\n\tstruct rcu_work\t\tfree_rwork;\t \n\n\t \n\tstruct ctx_rq_wait\t*rq_wait;\n\n\tstruct {\n\t\t \n\t\tatomic_t\treqs_available;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tspinlock_t\tctx_lock;\n\t\tstruct list_head active_reqs;\t \n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tstruct mutex\tring_lock;\n\t\twait_queue_head_t wait;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct {\n\t\tunsigned\ttail;\n\t\tunsigned\tcompleted_events;\n\t\tspinlock_t\tcompletion_lock;\n\t} ____cacheline_aligned_in_smp;\n\n\tstruct page\t\t*internal_pages[AIO_RING_PAGES];\n\tstruct file\t\t*aio_ring_file;\n\n\tunsigned\t\tid;\n};\n\n \nstruct fsync_iocb {\n\tstruct file\t\t*file;\n\tstruct work_struct\twork;\n\tbool\t\t\tdatasync;\n\tstruct cred\t\t*creds;\n};\n\nstruct poll_iocb {\n\tstruct file\t\t*file;\n\tstruct wait_queue_head\t*head;\n\t__poll_t\t\tevents;\n\tbool\t\t\tcancelled;\n\tbool\t\t\twork_scheduled;\n\tbool\t\t\twork_need_resched;\n\tstruct wait_queue_entry\twait;\n\tstruct work_struct\twork;\n};\n\n \nstruct aio_kiocb {\n\tunion {\n\t\tstruct file\t\t*ki_filp;\n\t\tstruct kiocb\t\trw;\n\t\tstruct fsync_iocb\tfsync;\n\t\tstruct poll_iocb\tpoll;\n\t};\n\n\tstruct kioctx\t\t*ki_ctx;\n\tkiocb_cancel_fn\t\t*ki_cancel;\n\n\tstruct io_event\t\tki_res;\n\n\tstruct list_head\tki_list;\t \n\trefcount_t\t\tki_refcnt;\n\n\t \n\tstruct eventfd_ctx\t*ki_eventfd;\n};\n\n \nstatic DEFINE_SPINLOCK(aio_nr_lock);\nstatic unsigned long aio_nr;\t\t \nstatic unsigned long aio_max_nr = 0x10000;  \n \n#ifdef CONFIG_SYSCTL\nstatic struct ctl_table aio_sysctls[] = {\n\t{\n\t\t.procname\t= \"aio-nr\",\n\t\t.data\t\t= &aio_nr,\n\t\t.maxlen\t\t= sizeof(aio_nr),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{\n\t\t.procname\t= \"aio-max-nr\",\n\t\t.data\t\t= &aio_max_nr,\n\t\t.maxlen\t\t= sizeof(aio_max_nr),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t},\n\t{}\n};\n\nstatic void __init aio_sysctl_init(void)\n{\n\tregister_sysctl_init(\"fs\", aio_sysctls);\n}\n#else\n#define aio_sysctl_init() do { } while (0)\n#endif\n\nstatic struct kmem_cache\t*kiocb_cachep;\nstatic struct kmem_cache\t*kioctx_cachep;\n\nstatic struct vfsmount *aio_mnt;\n\nstatic const struct file_operations aio_ring_fops;\nstatic const struct address_space_operations aio_ctx_aops;\n\nstatic struct file *aio_private_file(struct kioctx *ctx, loff_t nr_pages)\n{\n\tstruct file *file;\n\tstruct inode *inode = alloc_anon_inode(aio_mnt->mnt_sb);\n\tif (IS_ERR(inode))\n\t\treturn ERR_CAST(inode);\n\n\tinode->i_mapping->a_ops = &aio_ctx_aops;\n\tinode->i_mapping->private_data = ctx;\n\tinode->i_size = PAGE_SIZE * nr_pages;\n\n\tfile = alloc_file_pseudo(inode, aio_mnt, \"[aio]\",\n\t\t\t\tO_RDWR, &aio_ring_fops);\n\tif (IS_ERR(file))\n\t\tiput(inode);\n\treturn file;\n}\n\nstatic int aio_init_fs_context(struct fs_context *fc)\n{\n\tif (!init_pseudo(fc, AIO_RING_MAGIC))\n\t\treturn -ENOMEM;\n\tfc->s_iflags |= SB_I_NOEXEC;\n\treturn 0;\n}\n\n \nstatic int __init aio_setup(void)\n{\n\tstatic struct file_system_type aio_fs = {\n\t\t.name\t\t= \"aio\",\n\t\t.init_fs_context = aio_init_fs_context,\n\t\t.kill_sb\t= kill_anon_super,\n\t};\n\taio_mnt = kern_mount(&aio_fs);\n\tif (IS_ERR(aio_mnt))\n\t\tpanic(\"Failed to create aio fs mount.\");\n\n\tkiocb_cachep = KMEM_CACHE(aio_kiocb, SLAB_HWCACHE_ALIGN|SLAB_PANIC);\n\tkioctx_cachep = KMEM_CACHE(kioctx,SLAB_HWCACHE_ALIGN|SLAB_PANIC);\n\taio_sysctl_init();\n\treturn 0;\n}\n__initcall(aio_setup);\n\nstatic void put_aio_ring_file(struct kioctx *ctx)\n{\n\tstruct file *aio_ring_file = ctx->aio_ring_file;\n\tstruct address_space *i_mapping;\n\n\tif (aio_ring_file) {\n\t\ttruncate_setsize(file_inode(aio_ring_file), 0);\n\n\t\t \n\t\ti_mapping = aio_ring_file->f_mapping;\n\t\tspin_lock(&i_mapping->private_lock);\n\t\ti_mapping->private_data = NULL;\n\t\tctx->aio_ring_file = NULL;\n\t\tspin_unlock(&i_mapping->private_lock);\n\n\t\tfput(aio_ring_file);\n\t}\n}\n\nstatic void aio_free_ring(struct kioctx *ctx)\n{\n\tint i;\n\n\t \n\tput_aio_ring_file(ctx);\n\n\tfor (i = 0; i < ctx->nr_pages; i++) {\n\t\tstruct page *page;\n\t\tpr_debug(\"pid(%d) [%d] page->count=%d\\n\", current->pid, i,\n\t\t\t\tpage_count(ctx->ring_pages[i]));\n\t\tpage = ctx->ring_pages[i];\n\t\tif (!page)\n\t\t\tcontinue;\n\t\tctx->ring_pages[i] = NULL;\n\t\tput_page(page);\n\t}\n\n\tif (ctx->ring_pages && ctx->ring_pages != ctx->internal_pages) {\n\t\tkfree(ctx->ring_pages);\n\t\tctx->ring_pages = NULL;\n\t}\n}\n\nstatic int aio_ring_mremap(struct vm_area_struct *vma)\n{\n\tstruct file *file = vma->vm_file;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct kioctx_table *table;\n\tint i, res = -EINVAL;\n\n\tspin_lock(&mm->ioctx_lock);\n\trcu_read_lock();\n\ttable = rcu_dereference(mm->ioctx_table);\n\tif (!table)\n\t\tgoto out_unlock;\n\n\tfor (i = 0; i < table->nr; i++) {\n\t\tstruct kioctx *ctx;\n\n\t\tctx = rcu_dereference(table->table[i]);\n\t\tif (ctx && ctx->aio_ring_file == file) {\n\t\t\tif (!atomic_read(&ctx->dead)) {\n\t\t\t\tctx->user_id = ctx->mmap_base = vma->vm_start;\n\t\t\t\tres = 0;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\nout_unlock:\n\trcu_read_unlock();\n\tspin_unlock(&mm->ioctx_lock);\n\treturn res;\n}\n\nstatic const struct vm_operations_struct aio_ring_vm_ops = {\n\t.mremap\t\t= aio_ring_mremap,\n#if IS_ENABLED(CONFIG_MMU)\n\t.fault\t\t= filemap_fault,\n\t.map_pages\t= filemap_map_pages,\n\t.page_mkwrite\t= filemap_page_mkwrite,\n#endif\n};\n\nstatic int aio_ring_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tvm_flags_set(vma, VM_DONTEXPAND);\n\tvma->vm_ops = &aio_ring_vm_ops;\n\treturn 0;\n}\n\nstatic const struct file_operations aio_ring_fops = {\n\t.mmap = aio_ring_mmap,\n};\n\n#if IS_ENABLED(CONFIG_MIGRATION)\nstatic int aio_migrate_folio(struct address_space *mapping, struct folio *dst,\n\t\t\tstruct folio *src, enum migrate_mode mode)\n{\n\tstruct kioctx *ctx;\n\tunsigned long flags;\n\tpgoff_t idx;\n\tint rc;\n\n\t \n\tif (mode == MIGRATE_SYNC_NO_COPY)\n\t\treturn -EINVAL;\n\n\trc = 0;\n\n\t \n\tspin_lock(&mapping->private_lock);\n\tctx = mapping->private_data;\n\tif (!ctx) {\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\t \n\tif (!mutex_trylock(&ctx->ring_lock)) {\n\t\trc = -EAGAIN;\n\t\tgoto out;\n\t}\n\n\tidx = src->index;\n\tif (idx < (pgoff_t)ctx->nr_pages) {\n\t\t \n\t\tif (ctx->ring_pages[idx] != &src->page)\n\t\t\trc = -EAGAIN;\n\t} else\n\t\trc = -EINVAL;\n\n\tif (rc != 0)\n\t\tgoto out_unlock;\n\n\t \n\tBUG_ON(folio_test_writeback(src));\n\tfolio_get(dst);\n\n\trc = folio_migrate_mapping(mapping, dst, src, 1);\n\tif (rc != MIGRATEPAGE_SUCCESS) {\n\t\tfolio_put(dst);\n\t\tgoto out_unlock;\n\t}\n\n\t \n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\tfolio_migrate_copy(dst, src);\n\tBUG_ON(ctx->ring_pages[idx] != &src->page);\n\tctx->ring_pages[idx] = &dst->page;\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\t \n\tfolio_put(src);\n\nout_unlock:\n\tmutex_unlock(&ctx->ring_lock);\nout:\n\tspin_unlock(&mapping->private_lock);\n\treturn rc;\n}\n#else\n#define aio_migrate_folio NULL\n#endif\n\nstatic const struct address_space_operations aio_ctx_aops = {\n\t.dirty_folio\t= noop_dirty_folio,\n\t.migrate_folio\t= aio_migrate_folio,\n};\n\nstatic int aio_setup_ring(struct kioctx *ctx, unsigned int nr_events)\n{\n\tstruct aio_ring *ring;\n\tstruct mm_struct *mm = current->mm;\n\tunsigned long size, unused;\n\tint nr_pages;\n\tint i;\n\tstruct file *file;\n\n\t \n\tnr_events += 2;\t \n\n\tsize = sizeof(struct aio_ring);\n\tsize += sizeof(struct io_event) * nr_events;\n\n\tnr_pages = PFN_UP(size);\n\tif (nr_pages < 0)\n\t\treturn -EINVAL;\n\n\tfile = aio_private_file(ctx, nr_pages);\n\tif (IS_ERR(file)) {\n\t\tctx->aio_ring_file = NULL;\n\t\treturn -ENOMEM;\n\t}\n\n\tctx->aio_ring_file = file;\n\tnr_events = (PAGE_SIZE * nr_pages - sizeof(struct aio_ring))\n\t\t\t/ sizeof(struct io_event);\n\n\tctx->ring_pages = ctx->internal_pages;\n\tif (nr_pages > AIO_RING_PAGES) {\n\t\tctx->ring_pages = kcalloc(nr_pages, sizeof(struct page *),\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!ctx->ring_pages) {\n\t\t\tput_aio_ring_file(ctx);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tstruct page *page;\n\t\tpage = find_or_create_page(file->f_mapping,\n\t\t\t\t\t   i, GFP_USER | __GFP_ZERO);\n\t\tif (!page)\n\t\t\tbreak;\n\t\tpr_debug(\"pid(%d) page[%d]->count=%d\\n\",\n\t\t\t current->pid, i, page_count(page));\n\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\n\t\tctx->ring_pages[i] = page;\n\t}\n\tctx->nr_pages = i;\n\n\tif (unlikely(i != nr_pages)) {\n\t\taio_free_ring(ctx);\n\t\treturn -ENOMEM;\n\t}\n\n\tctx->mmap_size = nr_pages * PAGE_SIZE;\n\tpr_debug(\"attempting mmap of %lu bytes\\n\", ctx->mmap_size);\n\n\tif (mmap_write_lock_killable(mm)) {\n\t\tctx->mmap_size = 0;\n\t\taio_free_ring(ctx);\n\t\treturn -EINTR;\n\t}\n\n\tctx->mmap_base = do_mmap(ctx->aio_ring_file, 0, ctx->mmap_size,\n\t\t\t\t PROT_READ | PROT_WRITE,\n\t\t\t\t MAP_SHARED, 0, 0, &unused, NULL);\n\tmmap_write_unlock(mm);\n\tif (IS_ERR((void *)ctx->mmap_base)) {\n\t\tctx->mmap_size = 0;\n\t\taio_free_ring(ctx);\n\t\treturn -ENOMEM;\n\t}\n\n\tpr_debug(\"mmap address: 0x%08lx\\n\", ctx->mmap_base);\n\n\tctx->user_id = ctx->mmap_base;\n\tctx->nr_events = nr_events;  \n\n\tring = page_address(ctx->ring_pages[0]);\n\tring->nr = nr_events;\t \n\tring->id = ~0U;\n\tring->head = ring->tail = 0;\n\tring->magic = AIO_RING_MAGIC;\n\tring->compat_features = AIO_RING_COMPAT_FEATURES;\n\tring->incompat_features = AIO_RING_INCOMPAT_FEATURES;\n\tring->header_length = sizeof(struct aio_ring);\n\tflush_dcache_page(ctx->ring_pages[0]);\n\n\treturn 0;\n}\n\n#define AIO_EVENTS_PER_PAGE\t(PAGE_SIZE / sizeof(struct io_event))\n#define AIO_EVENTS_FIRST_PAGE\t((PAGE_SIZE - sizeof(struct aio_ring)) / sizeof(struct io_event))\n#define AIO_EVENTS_OFFSET\t(AIO_EVENTS_PER_PAGE - AIO_EVENTS_FIRST_PAGE)\n\nvoid kiocb_set_cancel_fn(struct kiocb *iocb, kiocb_cancel_fn *cancel)\n{\n\tstruct aio_kiocb *req = container_of(iocb, struct aio_kiocb, rw);\n\tstruct kioctx *ctx = req->ki_ctx;\n\tunsigned long flags;\n\n\tif (WARN_ON_ONCE(!list_empty(&req->ki_list)))\n\t\treturn;\n\n\tspin_lock_irqsave(&ctx->ctx_lock, flags);\n\tlist_add_tail(&req->ki_list, &ctx->active_reqs);\n\treq->ki_cancel = cancel;\n\tspin_unlock_irqrestore(&ctx->ctx_lock, flags);\n}\nEXPORT_SYMBOL(kiocb_set_cancel_fn);\n\n \nstatic void free_ioctx(struct work_struct *work)\n{\n\tstruct kioctx *ctx = container_of(to_rcu_work(work), struct kioctx,\n\t\t\t\t\t  free_rwork);\n\tpr_debug(\"freeing %p\\n\", ctx);\n\n\taio_free_ring(ctx);\n\tfree_percpu(ctx->cpu);\n\tpercpu_ref_exit(&ctx->reqs);\n\tpercpu_ref_exit(&ctx->users);\n\tkmem_cache_free(kioctx_cachep, ctx);\n}\n\nstatic void free_ioctx_reqs(struct percpu_ref *ref)\n{\n\tstruct kioctx *ctx = container_of(ref, struct kioctx, reqs);\n\n\t \n\tif (ctx->rq_wait && atomic_dec_and_test(&ctx->rq_wait->count))\n\t\tcomplete(&ctx->rq_wait->comp);\n\n\t \n\tINIT_RCU_WORK(&ctx->free_rwork, free_ioctx);\n\tqueue_rcu_work(system_wq, &ctx->free_rwork);\n}\n\n \nstatic void free_ioctx_users(struct percpu_ref *ref)\n{\n\tstruct kioctx *ctx = container_of(ref, struct kioctx, users);\n\tstruct aio_kiocb *req;\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\n\twhile (!list_empty(&ctx->active_reqs)) {\n\t\treq = list_first_entry(&ctx->active_reqs,\n\t\t\t\t       struct aio_kiocb, ki_list);\n\t\treq->ki_cancel(&req->rw);\n\t\tlist_del_init(&req->ki_list);\n\t}\n\n\tspin_unlock_irq(&ctx->ctx_lock);\n\n\tpercpu_ref_kill(&ctx->reqs);\n\tpercpu_ref_put(&ctx->reqs);\n}\n\nstatic int ioctx_add_table(struct kioctx *ctx, struct mm_struct *mm)\n{\n\tunsigned i, new_nr;\n\tstruct kioctx_table *table, *old;\n\tstruct aio_ring *ring;\n\n\tspin_lock(&mm->ioctx_lock);\n\ttable = rcu_dereference_raw(mm->ioctx_table);\n\n\twhile (1) {\n\t\tif (table)\n\t\t\tfor (i = 0; i < table->nr; i++)\n\t\t\t\tif (!rcu_access_pointer(table->table[i])) {\n\t\t\t\t\tctx->id = i;\n\t\t\t\t\trcu_assign_pointer(table->table[i], ctx);\n\t\t\t\t\tspin_unlock(&mm->ioctx_lock);\n\n\t\t\t\t\t \n\t\t\t\t\tring = page_address(ctx->ring_pages[0]);\n\t\t\t\t\tring->id = ctx->id;\n\t\t\t\t\treturn 0;\n\t\t\t\t}\n\n\t\tnew_nr = (table ? table->nr : 1) * 4;\n\t\tspin_unlock(&mm->ioctx_lock);\n\n\t\ttable = kzalloc(struct_size(table, table, new_nr), GFP_KERNEL);\n\t\tif (!table)\n\t\t\treturn -ENOMEM;\n\n\t\ttable->nr = new_nr;\n\n\t\tspin_lock(&mm->ioctx_lock);\n\t\told = rcu_dereference_raw(mm->ioctx_table);\n\n\t\tif (!old) {\n\t\t\trcu_assign_pointer(mm->ioctx_table, table);\n\t\t} else if (table->nr > old->nr) {\n\t\t\tmemcpy(table->table, old->table,\n\t\t\t       old->nr * sizeof(struct kioctx *));\n\n\t\t\trcu_assign_pointer(mm->ioctx_table, table);\n\t\t\tkfree_rcu(old, rcu);\n\t\t} else {\n\t\t\tkfree(table);\n\t\t\ttable = old;\n\t\t}\n\t}\n}\n\nstatic void aio_nr_sub(unsigned nr)\n{\n\tspin_lock(&aio_nr_lock);\n\tif (WARN_ON(aio_nr - nr > aio_nr))\n\t\taio_nr = 0;\n\telse\n\t\taio_nr -= nr;\n\tspin_unlock(&aio_nr_lock);\n}\n\n \nstatic struct kioctx *ioctx_alloc(unsigned nr_events)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct kioctx *ctx;\n\tint err = -ENOMEM;\n\n\t \n\tunsigned int max_reqs = nr_events;\n\n\t \n\tnr_events = max(nr_events, num_possible_cpus() * 4);\n\tnr_events *= 2;\n\n\t \n\tif (nr_events > (0x10000000U / sizeof(struct io_event))) {\n\t\tpr_debug(\"ENOMEM: nr_events too high\\n\");\n\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (!nr_events || (unsigned long)max_reqs > aio_max_nr)\n\t\treturn ERR_PTR(-EAGAIN);\n\n\tctx = kmem_cache_zalloc(kioctx_cachep, GFP_KERNEL);\n\tif (!ctx)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tctx->max_reqs = max_reqs;\n\n\tspin_lock_init(&ctx->ctx_lock);\n\tspin_lock_init(&ctx->completion_lock);\n\tmutex_init(&ctx->ring_lock);\n\t \n\tmutex_lock(&ctx->ring_lock);\n\tinit_waitqueue_head(&ctx->wait);\n\n\tINIT_LIST_HEAD(&ctx->active_reqs);\n\n\tif (percpu_ref_init(&ctx->users, free_ioctx_users, 0, GFP_KERNEL))\n\t\tgoto err;\n\n\tif (percpu_ref_init(&ctx->reqs, free_ioctx_reqs, 0, GFP_KERNEL))\n\t\tgoto err;\n\n\tctx->cpu = alloc_percpu(struct kioctx_cpu);\n\tif (!ctx->cpu)\n\t\tgoto err;\n\n\terr = aio_setup_ring(ctx, nr_events);\n\tif (err < 0)\n\t\tgoto err;\n\n\tatomic_set(&ctx->reqs_available, ctx->nr_events - 1);\n\tctx->req_batch = (ctx->nr_events - 1) / (num_possible_cpus() * 4);\n\tif (ctx->req_batch < 1)\n\t\tctx->req_batch = 1;\n\n\t \n\tspin_lock(&aio_nr_lock);\n\tif (aio_nr + ctx->max_reqs > aio_max_nr ||\n\t    aio_nr + ctx->max_reqs < aio_nr) {\n\t\tspin_unlock(&aio_nr_lock);\n\t\terr = -EAGAIN;\n\t\tgoto err_ctx;\n\t}\n\taio_nr += ctx->max_reqs;\n\tspin_unlock(&aio_nr_lock);\n\n\tpercpu_ref_get(&ctx->users);\t \n\tpercpu_ref_get(&ctx->reqs);\t \n\n\terr = ioctx_add_table(ctx, mm);\n\tif (err)\n\t\tgoto err_cleanup;\n\n\t \n\tmutex_unlock(&ctx->ring_lock);\n\n\tpr_debug(\"allocated ioctx %p[%ld]: mm=%p mask=0x%x\\n\",\n\t\t ctx, ctx->user_id, mm, ctx->nr_events);\n\treturn ctx;\n\nerr_cleanup:\n\taio_nr_sub(ctx->max_reqs);\nerr_ctx:\n\tatomic_set(&ctx->dead, 1);\n\tif (ctx->mmap_size)\n\t\tvm_munmap(ctx->mmap_base, ctx->mmap_size);\n\taio_free_ring(ctx);\nerr:\n\tmutex_unlock(&ctx->ring_lock);\n\tfree_percpu(ctx->cpu);\n\tpercpu_ref_exit(&ctx->reqs);\n\tpercpu_ref_exit(&ctx->users);\n\tkmem_cache_free(kioctx_cachep, ctx);\n\tpr_debug(\"error allocating ioctx %d\\n\", err);\n\treturn ERR_PTR(err);\n}\n\n \nstatic int kill_ioctx(struct mm_struct *mm, struct kioctx *ctx,\n\t\t      struct ctx_rq_wait *wait)\n{\n\tstruct kioctx_table *table;\n\n\tspin_lock(&mm->ioctx_lock);\n\tif (atomic_xchg(&ctx->dead, 1)) {\n\t\tspin_unlock(&mm->ioctx_lock);\n\t\treturn -EINVAL;\n\t}\n\n\ttable = rcu_dereference_raw(mm->ioctx_table);\n\tWARN_ON(ctx != rcu_access_pointer(table->table[ctx->id]));\n\tRCU_INIT_POINTER(table->table[ctx->id], NULL);\n\tspin_unlock(&mm->ioctx_lock);\n\n\t \n\twake_up_all(&ctx->wait);\n\n\t \n\taio_nr_sub(ctx->max_reqs);\n\n\tif (ctx->mmap_size)\n\t\tvm_munmap(ctx->mmap_base, ctx->mmap_size);\n\n\tctx->rq_wait = wait;\n\tpercpu_ref_kill(&ctx->users);\n\treturn 0;\n}\n\n \nvoid exit_aio(struct mm_struct *mm)\n{\n\tstruct kioctx_table *table = rcu_dereference_raw(mm->ioctx_table);\n\tstruct ctx_rq_wait wait;\n\tint i, skipped;\n\n\tif (!table)\n\t\treturn;\n\n\tatomic_set(&wait.count, table->nr);\n\tinit_completion(&wait.comp);\n\n\tskipped = 0;\n\tfor (i = 0; i < table->nr; ++i) {\n\t\tstruct kioctx *ctx =\n\t\t\trcu_dereference_protected(table->table[i], true);\n\n\t\tif (!ctx) {\n\t\t\tskipped++;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tctx->mmap_size = 0;\n\t\tkill_ioctx(mm, ctx, &wait);\n\t}\n\n\tif (!atomic_sub_and_test(skipped, &wait.count)) {\n\t\t \n\t\twait_for_completion(&wait.comp);\n\t}\n\n\tRCU_INIT_POINTER(mm->ioctx_table, NULL);\n\tkfree(table);\n}\n\nstatic void put_reqs_available(struct kioctx *ctx, unsigned nr)\n{\n\tstruct kioctx_cpu *kcpu;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tkcpu = this_cpu_ptr(ctx->cpu);\n\tkcpu->reqs_available += nr;\n\n\twhile (kcpu->reqs_available >= ctx->req_batch * 2) {\n\t\tkcpu->reqs_available -= ctx->req_batch;\n\t\tatomic_add(ctx->req_batch, &ctx->reqs_available);\n\t}\n\n\tlocal_irq_restore(flags);\n}\n\nstatic bool __get_reqs_available(struct kioctx *ctx)\n{\n\tstruct kioctx_cpu *kcpu;\n\tbool ret = false;\n\tunsigned long flags;\n\n\tlocal_irq_save(flags);\n\tkcpu = this_cpu_ptr(ctx->cpu);\n\tif (!kcpu->reqs_available) {\n\t\tint avail = atomic_read(&ctx->reqs_available);\n\n\t\tdo {\n\t\t\tif (avail < ctx->req_batch)\n\t\t\t\tgoto out;\n\t\t} while (!atomic_try_cmpxchg(&ctx->reqs_available,\n\t\t\t\t\t     &avail, avail - ctx->req_batch));\n\n\t\tkcpu->reqs_available += ctx->req_batch;\n\t}\n\n\tret = true;\n\tkcpu->reqs_available--;\nout:\n\tlocal_irq_restore(flags);\n\treturn ret;\n}\n\n \nstatic void refill_reqs_available(struct kioctx *ctx, unsigned head,\n                                  unsigned tail)\n{\n\tunsigned events_in_ring, completed;\n\n\t \n\thead %= ctx->nr_events;\n\tif (head <= tail)\n\t\tevents_in_ring = tail - head;\n\telse\n\t\tevents_in_ring = ctx->nr_events - (head - tail);\n\n\tcompleted = ctx->completed_events;\n\tif (events_in_ring < completed)\n\t\tcompleted -= events_in_ring;\n\telse\n\t\tcompleted = 0;\n\n\tif (!completed)\n\t\treturn;\n\n\tctx->completed_events -= completed;\n\tput_reqs_available(ctx, completed);\n}\n\n \nstatic void user_refill_reqs_available(struct kioctx *ctx)\n{\n\tspin_lock_irq(&ctx->completion_lock);\n\tif (ctx->completed_events) {\n\t\tstruct aio_ring *ring;\n\t\tunsigned head;\n\n\t\t \n\t\tring = page_address(ctx->ring_pages[0]);\n\t\thead = ring->head;\n\n\t\trefill_reqs_available(ctx, head, ctx->tail);\n\t}\n\n\tspin_unlock_irq(&ctx->completion_lock);\n}\n\nstatic bool get_reqs_available(struct kioctx *ctx)\n{\n\tif (__get_reqs_available(ctx))\n\t\treturn true;\n\tuser_refill_reqs_available(ctx);\n\treturn __get_reqs_available(ctx);\n}\n\n \nstatic inline struct aio_kiocb *aio_get_req(struct kioctx *ctx)\n{\n\tstruct aio_kiocb *req;\n\n\treq = kmem_cache_alloc(kiocb_cachep, GFP_KERNEL);\n\tif (unlikely(!req))\n\t\treturn NULL;\n\n\tif (unlikely(!get_reqs_available(ctx))) {\n\t\tkmem_cache_free(kiocb_cachep, req);\n\t\treturn NULL;\n\t}\n\n\tpercpu_ref_get(&ctx->reqs);\n\treq->ki_ctx = ctx;\n\tINIT_LIST_HEAD(&req->ki_list);\n\trefcount_set(&req->ki_refcnt, 2);\n\treq->ki_eventfd = NULL;\n\treturn req;\n}\n\nstatic struct kioctx *lookup_ioctx(unsigned long ctx_id)\n{\n\tstruct aio_ring __user *ring  = (void __user *)ctx_id;\n\tstruct mm_struct *mm = current->mm;\n\tstruct kioctx *ctx, *ret = NULL;\n\tstruct kioctx_table *table;\n\tunsigned id;\n\n\tif (get_user(id, &ring->id))\n\t\treturn NULL;\n\n\trcu_read_lock();\n\ttable = rcu_dereference(mm->ioctx_table);\n\n\tif (!table || id >= table->nr)\n\t\tgoto out;\n\n\tid = array_index_nospec(id, table->nr);\n\tctx = rcu_dereference(table->table[id]);\n\tif (ctx && ctx->user_id == ctx_id) {\n\t\tif (percpu_ref_tryget_live(&ctx->users))\n\t\t\tret = ctx;\n\t}\nout:\n\trcu_read_unlock();\n\treturn ret;\n}\n\nstatic inline void iocb_destroy(struct aio_kiocb *iocb)\n{\n\tif (iocb->ki_eventfd)\n\t\teventfd_ctx_put(iocb->ki_eventfd);\n\tif (iocb->ki_filp)\n\t\tfput(iocb->ki_filp);\n\tpercpu_ref_put(&iocb->ki_ctx->reqs);\n\tkmem_cache_free(kiocb_cachep, iocb);\n}\n\n \nstatic void aio_complete(struct aio_kiocb *iocb)\n{\n\tstruct kioctx\t*ctx = iocb->ki_ctx;\n\tstruct aio_ring\t*ring;\n\tstruct io_event\t*ev_page, *event;\n\tunsigned tail, pos, head;\n\tunsigned long\tflags;\n\n\t \n\tspin_lock_irqsave(&ctx->completion_lock, flags);\n\n\ttail = ctx->tail;\n\tpos = tail + AIO_EVENTS_OFFSET;\n\n\tif (++tail >= ctx->nr_events)\n\t\ttail = 0;\n\n\tev_page = page_address(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);\n\tevent = ev_page + pos % AIO_EVENTS_PER_PAGE;\n\n\t*event = iocb->ki_res;\n\n\tflush_dcache_page(ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE]);\n\n\tpr_debug(\"%p[%u]: %p: %p %Lx %Lx %Lx\\n\", ctx, tail, iocb,\n\t\t (void __user *)(unsigned long)iocb->ki_res.obj,\n\t\t iocb->ki_res.data, iocb->ki_res.res, iocb->ki_res.res2);\n\n\t \n\tsmp_wmb();\t \n\n\tctx->tail = tail;\n\n\tring = page_address(ctx->ring_pages[0]);\n\thead = ring->head;\n\tring->tail = tail;\n\tflush_dcache_page(ctx->ring_pages[0]);\n\n\tctx->completed_events++;\n\tif (ctx->completed_events > 1)\n\t\trefill_reqs_available(ctx, head, tail);\n\tspin_unlock_irqrestore(&ctx->completion_lock, flags);\n\n\tpr_debug(\"added to ring %p at [%u]\\n\", iocb, tail);\n\n\t \n\tif (iocb->ki_eventfd)\n\t\teventfd_signal(iocb->ki_eventfd, 1);\n\n\t \n\tsmp_mb();\n\n\tif (waitqueue_active(&ctx->wait))\n\t\twake_up(&ctx->wait);\n}\n\nstatic inline void iocb_put(struct aio_kiocb *iocb)\n{\n\tif (refcount_dec_and_test(&iocb->ki_refcnt)) {\n\t\taio_complete(iocb);\n\t\tiocb_destroy(iocb);\n\t}\n}\n\n \nstatic long aio_read_events_ring(struct kioctx *ctx,\n\t\t\t\t struct io_event __user *event, long nr)\n{\n\tstruct aio_ring *ring;\n\tunsigned head, tail, pos;\n\tlong ret = 0;\n\tint copy_ret;\n\n\t \n\tsched_annotate_sleep();\n\tmutex_lock(&ctx->ring_lock);\n\n\t \n\tring = page_address(ctx->ring_pages[0]);\n\thead = ring->head;\n\ttail = ring->tail;\n\n\t \n\tsmp_rmb();\n\n\tpr_debug(\"h%u t%u m%u\\n\", head, tail, ctx->nr_events);\n\n\tif (head == tail)\n\t\tgoto out;\n\n\thead %= ctx->nr_events;\n\ttail %= ctx->nr_events;\n\n\twhile (ret < nr) {\n\t\tlong avail;\n\t\tstruct io_event *ev;\n\t\tstruct page *page;\n\n\t\tavail = (head <= tail ?  tail : ctx->nr_events) - head;\n\t\tif (head == tail)\n\t\t\tbreak;\n\n\t\tpos = head + AIO_EVENTS_OFFSET;\n\t\tpage = ctx->ring_pages[pos / AIO_EVENTS_PER_PAGE];\n\t\tpos %= AIO_EVENTS_PER_PAGE;\n\n\t\tavail = min(avail, nr - ret);\n\t\tavail = min_t(long, avail, AIO_EVENTS_PER_PAGE - pos);\n\n\t\tev = page_address(page);\n\t\tcopy_ret = copy_to_user(event + ret, ev + pos,\n\t\t\t\t\tsizeof(*ev) * avail);\n\n\t\tif (unlikely(copy_ret)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret += avail;\n\t\thead += avail;\n\t\thead %= ctx->nr_events;\n\t}\n\n\tring = page_address(ctx->ring_pages[0]);\n\tring->head = head;\n\tflush_dcache_page(ctx->ring_pages[0]);\n\n\tpr_debug(\"%li  h%u t%u\\n\", ret, head, tail);\nout:\n\tmutex_unlock(&ctx->ring_lock);\n\n\treturn ret;\n}\n\nstatic bool aio_read_events(struct kioctx *ctx, long min_nr, long nr,\n\t\t\t    struct io_event __user *event, long *i)\n{\n\tlong ret = aio_read_events_ring(ctx, event + *i, nr - *i);\n\n\tif (ret > 0)\n\t\t*i += ret;\n\n\tif (unlikely(atomic_read(&ctx->dead)))\n\t\tret = -EINVAL;\n\n\tif (!*i)\n\t\t*i = ret;\n\n\treturn ret < 0 || *i >= min_nr;\n}\n\nstatic long read_events(struct kioctx *ctx, long min_nr, long nr,\n\t\t\tstruct io_event __user *event,\n\t\t\tktime_t until)\n{\n\tlong ret = 0;\n\n\t \n\tif (until == 0)\n\t\taio_read_events(ctx, min_nr, nr, event, &ret);\n\telse\n\t\twait_event_interruptible_hrtimeout(ctx->wait,\n\t\t\t\taio_read_events(ctx, min_nr, nr, event, &ret),\n\t\t\t\tuntil);\n\treturn ret;\n}\n\n \nSYSCALL_DEFINE2(io_setup, unsigned, nr_events, aio_context_t __user *, ctxp)\n{\n\tstruct kioctx *ioctx = NULL;\n\tunsigned long ctx;\n\tlong ret;\n\n\tret = get_user(ctx, ctxp);\n\tif (unlikely(ret))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (unlikely(ctx || nr_events == 0)) {\n\t\tpr_debug(\"EINVAL: ctx %lu nr_events %u\\n\",\n\t\t         ctx, nr_events);\n\t\tgoto out;\n\t}\n\n\tioctx = ioctx_alloc(nr_events);\n\tret = PTR_ERR(ioctx);\n\tif (!IS_ERR(ioctx)) {\n\t\tret = put_user(ioctx->user_id, ctxp);\n\t\tif (ret)\n\t\t\tkill_ioctx(current->mm, ioctx, NULL);\n\t\tpercpu_ref_put(&ioctx->users);\n\t}\n\nout:\n\treturn ret;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE2(io_setup, unsigned, nr_events, u32 __user *, ctx32p)\n{\n\tstruct kioctx *ioctx = NULL;\n\tunsigned long ctx;\n\tlong ret;\n\n\tret = get_user(ctx, ctx32p);\n\tif (unlikely(ret))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (unlikely(ctx || nr_events == 0)) {\n\t\tpr_debug(\"EINVAL: ctx %lu nr_events %u\\n\",\n\t\t         ctx, nr_events);\n\t\tgoto out;\n\t}\n\n\tioctx = ioctx_alloc(nr_events);\n\tret = PTR_ERR(ioctx);\n\tif (!IS_ERR(ioctx)) {\n\t\t \n\t\tret = put_user((u32)ioctx->user_id, ctx32p);\n\t\tif (ret)\n\t\t\tkill_ioctx(current->mm, ioctx, NULL);\n\t\tpercpu_ref_put(&ioctx->users);\n\t}\n\nout:\n\treturn ret;\n}\n#endif\n\n \nSYSCALL_DEFINE1(io_destroy, aio_context_t, ctx)\n{\n\tstruct kioctx *ioctx = lookup_ioctx(ctx);\n\tif (likely(NULL != ioctx)) {\n\t\tstruct ctx_rq_wait wait;\n\t\tint ret;\n\n\t\tinit_completion(&wait.comp);\n\t\tatomic_set(&wait.count, 1);\n\n\t\t \n\t\tret = kill_ioctx(current->mm, ioctx, &wait);\n\t\tpercpu_ref_put(&ioctx->users);\n\n\t\t \n\t\tif (!ret)\n\t\t\twait_for_completion(&wait.comp);\n\n\t\treturn ret;\n\t}\n\tpr_debug(\"EINVAL: invalid context id\\n\");\n\treturn -EINVAL;\n}\n\nstatic void aio_remove_iocb(struct aio_kiocb *iocb)\n{\n\tstruct kioctx *ctx = iocb->ki_ctx;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&ctx->ctx_lock, flags);\n\tlist_del(&iocb->ki_list);\n\tspin_unlock_irqrestore(&ctx->ctx_lock, flags);\n}\n\nstatic void aio_complete_rw(struct kiocb *kiocb, long res)\n{\n\tstruct aio_kiocb *iocb = container_of(kiocb, struct aio_kiocb, rw);\n\n\tif (!list_empty_careful(&iocb->ki_list))\n\t\taio_remove_iocb(iocb);\n\n\tif (kiocb->ki_flags & IOCB_WRITE) {\n\t\tstruct inode *inode = file_inode(kiocb->ki_filp);\n\n\t\tif (S_ISREG(inode->i_mode))\n\t\t\tkiocb_end_write(kiocb);\n\t}\n\n\tiocb->ki_res.res = res;\n\tiocb->ki_res.res2 = 0;\n\tiocb_put(iocb);\n}\n\nstatic int aio_prep_rw(struct kiocb *req, const struct iocb *iocb)\n{\n\tint ret;\n\n\treq->ki_complete = aio_complete_rw;\n\treq->private = NULL;\n\treq->ki_pos = iocb->aio_offset;\n\treq->ki_flags = req->ki_filp->f_iocb_flags;\n\tif (iocb->aio_flags & IOCB_FLAG_RESFD)\n\t\treq->ki_flags |= IOCB_EVENTFD;\n\tif (iocb->aio_flags & IOCB_FLAG_IOPRIO) {\n\t\t \n\t\tret = ioprio_check_cap(iocb->aio_reqprio);\n\t\tif (ret) {\n\t\t\tpr_debug(\"aio ioprio check cap error: %d\\n\", ret);\n\t\t\treturn ret;\n\t\t}\n\n\t\treq->ki_ioprio = iocb->aio_reqprio;\n\t} else\n\t\treq->ki_ioprio = get_current_ioprio();\n\n\tret = kiocb_set_rw_flags(req, iocb->aio_rw_flags);\n\tif (unlikely(ret))\n\t\treturn ret;\n\n\treq->ki_flags &= ~IOCB_HIPRI;  \n\treturn 0;\n}\n\nstatic ssize_t aio_setup_rw(int rw, const struct iocb *iocb,\n\t\tstruct iovec **iovec, bool vectored, bool compat,\n\t\tstruct iov_iter *iter)\n{\n\tvoid __user *buf = (void __user *)(uintptr_t)iocb->aio_buf;\n\tsize_t len = iocb->aio_nbytes;\n\n\tif (!vectored) {\n\t\tssize_t ret = import_single_range(rw, buf, len, *iovec, iter);\n\t\t*iovec = NULL;\n\t\treturn ret;\n\t}\n\n\treturn __import_iovec(rw, buf, len, UIO_FASTIOV, iovec, iter, compat);\n}\n\nstatic inline void aio_rw_done(struct kiocb *req, ssize_t ret)\n{\n\tswitch (ret) {\n\tcase -EIOCBQUEUED:\n\t\tbreak;\n\tcase -ERESTARTSYS:\n\tcase -ERESTARTNOINTR:\n\tcase -ERESTARTNOHAND:\n\tcase -ERESTART_RESTARTBLOCK:\n\t\t \n\t\tret = -EINTR;\n\t\tfallthrough;\n\tdefault:\n\t\treq->ki_complete(req, ret);\n\t}\n}\n\nstatic int aio_read(struct kiocb *req, const struct iocb *iocb,\n\t\t\tbool vectored, bool compat)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct iov_iter iter;\n\tstruct file *file;\n\tint ret;\n\n\tret = aio_prep_rw(req, iocb);\n\tif (ret)\n\t\treturn ret;\n\tfile = req->ki_filp;\n\tif (unlikely(!(file->f_mode & FMODE_READ)))\n\t\treturn -EBADF;\n\tif (unlikely(!file->f_op->read_iter))\n\t\treturn -EINVAL;\n\n\tret = aio_setup_rw(ITER_DEST, iocb, &iovec, vectored, compat, &iter);\n\tif (ret < 0)\n\t\treturn ret;\n\tret = rw_verify_area(READ, file, &req->ki_pos, iov_iter_count(&iter));\n\tif (!ret)\n\t\taio_rw_done(req, call_read_iter(file, req, &iter));\n\tkfree(iovec);\n\treturn ret;\n}\n\nstatic int aio_write(struct kiocb *req, const struct iocb *iocb,\n\t\t\t bool vectored, bool compat)\n{\n\tstruct iovec inline_vecs[UIO_FASTIOV], *iovec = inline_vecs;\n\tstruct iov_iter iter;\n\tstruct file *file;\n\tint ret;\n\n\tret = aio_prep_rw(req, iocb);\n\tif (ret)\n\t\treturn ret;\n\tfile = req->ki_filp;\n\n\tif (unlikely(!(file->f_mode & FMODE_WRITE)))\n\t\treturn -EBADF;\n\tif (unlikely(!file->f_op->write_iter))\n\t\treturn -EINVAL;\n\n\tret = aio_setup_rw(ITER_SOURCE, iocb, &iovec, vectored, compat, &iter);\n\tif (ret < 0)\n\t\treturn ret;\n\tret = rw_verify_area(WRITE, file, &req->ki_pos, iov_iter_count(&iter));\n\tif (!ret) {\n\t\tif (S_ISREG(file_inode(file)->i_mode))\n\t\t\tkiocb_start_write(req);\n\t\treq->ki_flags |= IOCB_WRITE;\n\t\taio_rw_done(req, call_write_iter(file, req, &iter));\n\t}\n\tkfree(iovec);\n\treturn ret;\n}\n\nstatic void aio_fsync_work(struct work_struct *work)\n{\n\tstruct aio_kiocb *iocb = container_of(work, struct aio_kiocb, fsync.work);\n\tconst struct cred *old_cred = override_creds(iocb->fsync.creds);\n\n\tiocb->ki_res.res = vfs_fsync(iocb->fsync.file, iocb->fsync.datasync);\n\trevert_creds(old_cred);\n\tput_cred(iocb->fsync.creds);\n\tiocb_put(iocb);\n}\n\nstatic int aio_fsync(struct fsync_iocb *req, const struct iocb *iocb,\n\t\t     bool datasync)\n{\n\tif (unlikely(iocb->aio_buf || iocb->aio_offset || iocb->aio_nbytes ||\n\t\t\tiocb->aio_rw_flags))\n\t\treturn -EINVAL;\n\n\tif (unlikely(!req->file->f_op->fsync))\n\t\treturn -EINVAL;\n\n\treq->creds = prepare_creds();\n\tif (!req->creds)\n\t\treturn -ENOMEM;\n\n\treq->datasync = datasync;\n\tINIT_WORK(&req->work, aio_fsync_work);\n\tschedule_work(&req->work);\n\treturn 0;\n}\n\nstatic void aio_poll_put_work(struct work_struct *work)\n{\n\tstruct poll_iocb *req = container_of(work, struct poll_iocb, work);\n\tstruct aio_kiocb *iocb = container_of(req, struct aio_kiocb, poll);\n\n\tiocb_put(iocb);\n}\n\n \nstatic bool poll_iocb_lock_wq(struct poll_iocb *req)\n{\n\twait_queue_head_t *head;\n\n\t \n\trcu_read_lock();\n\thead = smp_load_acquire(&req->head);\n\tif (head) {\n\t\tspin_lock(&head->lock);\n\t\tif (!list_empty(&req->wait.entry))\n\t\t\treturn true;\n\t\tspin_unlock(&head->lock);\n\t}\n\trcu_read_unlock();\n\treturn false;\n}\n\nstatic void poll_iocb_unlock_wq(struct poll_iocb *req)\n{\n\tspin_unlock(&req->head->lock);\n\trcu_read_unlock();\n}\n\nstatic void aio_poll_complete_work(struct work_struct *work)\n{\n\tstruct poll_iocb *req = container_of(work, struct poll_iocb, work);\n\tstruct aio_kiocb *iocb = container_of(req, struct aio_kiocb, poll);\n\tstruct poll_table_struct pt = { ._key = req->events };\n\tstruct kioctx *ctx = iocb->ki_ctx;\n\t__poll_t mask = 0;\n\n\tif (!READ_ONCE(req->cancelled))\n\t\tmask = vfs_poll(req->file, &pt) & req->events;\n\n\t \n\tspin_lock_irq(&ctx->ctx_lock);\n\tif (poll_iocb_lock_wq(req)) {\n\t\tif (!mask && !READ_ONCE(req->cancelled)) {\n\t\t\t \n\t\t\tif (req->work_need_resched) {\n\t\t\t\tschedule_work(&req->work);\n\t\t\t\treq->work_need_resched = false;\n\t\t\t} else {\n\t\t\t\treq->work_scheduled = false;\n\t\t\t}\n\t\t\tpoll_iocb_unlock_wq(req);\n\t\t\tspin_unlock_irq(&ctx->ctx_lock);\n\t\t\treturn;\n\t\t}\n\t\tlist_del_init(&req->wait.entry);\n\t\tpoll_iocb_unlock_wq(req);\n\t}  \n\tlist_del_init(&iocb->ki_list);\n\tiocb->ki_res.res = mangle_poll(mask);\n\tspin_unlock_irq(&ctx->ctx_lock);\n\n\tiocb_put(iocb);\n}\n\n \nstatic int aio_poll_cancel(struct kiocb *iocb)\n{\n\tstruct aio_kiocb *aiocb = container_of(iocb, struct aio_kiocb, rw);\n\tstruct poll_iocb *req = &aiocb->poll;\n\n\tif (poll_iocb_lock_wq(req)) {\n\t\tWRITE_ONCE(req->cancelled, true);\n\t\tif (!req->work_scheduled) {\n\t\t\tschedule_work(&aiocb->poll.work);\n\t\t\treq->work_scheduled = true;\n\t\t}\n\t\tpoll_iocb_unlock_wq(req);\n\t}  \n\n\treturn 0;\n}\n\nstatic int aio_poll_wake(struct wait_queue_entry *wait, unsigned mode, int sync,\n\t\tvoid *key)\n{\n\tstruct poll_iocb *req = container_of(wait, struct poll_iocb, wait);\n\tstruct aio_kiocb *iocb = container_of(req, struct aio_kiocb, poll);\n\t__poll_t mask = key_to_poll(key);\n\tunsigned long flags;\n\n\t \n\tif (mask && !(mask & req->events))\n\t\treturn 0;\n\n\t \n\tif (mask && !req->work_scheduled &&\n\t    spin_trylock_irqsave(&iocb->ki_ctx->ctx_lock, flags)) {\n\t\tstruct kioctx *ctx = iocb->ki_ctx;\n\n\t\tlist_del_init(&req->wait.entry);\n\t\tlist_del(&iocb->ki_list);\n\t\tiocb->ki_res.res = mangle_poll(mask);\n\t\tif (iocb->ki_eventfd && !eventfd_signal_allowed()) {\n\t\t\tiocb = NULL;\n\t\t\tINIT_WORK(&req->work, aio_poll_put_work);\n\t\t\tschedule_work(&req->work);\n\t\t}\n\t\tspin_unlock_irqrestore(&ctx->ctx_lock, flags);\n\t\tif (iocb)\n\t\t\tiocb_put(iocb);\n\t} else {\n\t\t \n\t\tif (req->work_scheduled) {\n\t\t\treq->work_need_resched = true;\n\t\t} else {\n\t\t\tschedule_work(&req->work);\n\t\t\treq->work_scheduled = true;\n\t\t}\n\n\t\t \n\t\tif (mask & POLLFREE) {\n\t\t\tWRITE_ONCE(req->cancelled, true);\n\t\t\tlist_del_init(&req->wait.entry);\n\n\t\t\t \n\t\t\tsmp_store_release(&req->head, NULL);\n\t\t}\n\t}\n\treturn 1;\n}\n\nstruct aio_poll_table {\n\tstruct poll_table_struct\tpt;\n\tstruct aio_kiocb\t\t*iocb;\n\tbool\t\t\t\tqueued;\n\tint\t\t\t\terror;\n};\n\nstatic void\naio_poll_queue_proc(struct file *file, struct wait_queue_head *head,\n\t\tstruct poll_table_struct *p)\n{\n\tstruct aio_poll_table *pt = container_of(p, struct aio_poll_table, pt);\n\n\t \n\tif (unlikely(pt->queued)) {\n\t\tpt->error = -EINVAL;\n\t\treturn;\n\t}\n\n\tpt->queued = true;\n\tpt->error = 0;\n\tpt->iocb->poll.head = head;\n\tadd_wait_queue(head, &pt->iocb->poll.wait);\n}\n\nstatic int aio_poll(struct aio_kiocb *aiocb, const struct iocb *iocb)\n{\n\tstruct kioctx *ctx = aiocb->ki_ctx;\n\tstruct poll_iocb *req = &aiocb->poll;\n\tstruct aio_poll_table apt;\n\tbool cancel = false;\n\t__poll_t mask;\n\n\t \n\tif ((u16)iocb->aio_buf != iocb->aio_buf)\n\t\treturn -EINVAL;\n\t \n\tif (iocb->aio_offset || iocb->aio_nbytes || iocb->aio_rw_flags)\n\t\treturn -EINVAL;\n\n\tINIT_WORK(&req->work, aio_poll_complete_work);\n\treq->events = demangle_poll(iocb->aio_buf) | EPOLLERR | EPOLLHUP;\n\n\treq->head = NULL;\n\treq->cancelled = false;\n\treq->work_scheduled = false;\n\treq->work_need_resched = false;\n\n\tapt.pt._qproc = aio_poll_queue_proc;\n\tapt.pt._key = req->events;\n\tapt.iocb = aiocb;\n\tapt.queued = false;\n\tapt.error = -EINVAL;  \n\n\t \n\tINIT_LIST_HEAD(&req->wait.entry);\n\tinit_waitqueue_func_entry(&req->wait, aio_poll_wake);\n\n\tmask = vfs_poll(req->file, &apt.pt) & req->events;\n\tspin_lock_irq(&ctx->ctx_lock);\n\tif (likely(apt.queued)) {\n\t\tbool on_queue = poll_iocb_lock_wq(req);\n\n\t\tif (!on_queue || req->work_scheduled) {\n\t\t\t \n\t\t\tif (apt.error)  \n\t\t\t\tcancel = true;\n\t\t\tapt.error = 0;\n\t\t\tmask = 0;\n\t\t}\n\t\tif (mask || apt.error) {\n\t\t\t \n\t\t\tlist_del_init(&req->wait.entry);\n\t\t} else if (cancel) {\n\t\t\t \n\t\t\tWRITE_ONCE(req->cancelled, true);\n\t\t} else if (on_queue) {\n\t\t\t \n\t\t\tlist_add_tail(&aiocb->ki_list, &ctx->active_reqs);\n\t\t\taiocb->ki_cancel = aio_poll_cancel;\n\t\t}\n\t\tif (on_queue)\n\t\t\tpoll_iocb_unlock_wq(req);\n\t}\n\tif (mask) {  \n\t\taiocb->ki_res.res = mangle_poll(mask);\n\t\tapt.error = 0;\n\t}\n\tspin_unlock_irq(&ctx->ctx_lock);\n\tif (mask)\n\t\tiocb_put(aiocb);\n\treturn apt.error;\n}\n\nstatic int __io_submit_one(struct kioctx *ctx, const struct iocb *iocb,\n\t\t\t   struct iocb __user *user_iocb, struct aio_kiocb *req,\n\t\t\t   bool compat)\n{\n\treq->ki_filp = fget(iocb->aio_fildes);\n\tif (unlikely(!req->ki_filp))\n\t\treturn -EBADF;\n\n\tif (iocb->aio_flags & IOCB_FLAG_RESFD) {\n\t\tstruct eventfd_ctx *eventfd;\n\t\t \n\t\teventfd = eventfd_ctx_fdget(iocb->aio_resfd);\n\t\tif (IS_ERR(eventfd))\n\t\t\treturn PTR_ERR(eventfd);\n\n\t\treq->ki_eventfd = eventfd;\n\t}\n\n\tif (unlikely(put_user(KIOCB_KEY, &user_iocb->aio_key))) {\n\t\tpr_debug(\"EFAULT: aio_key\\n\");\n\t\treturn -EFAULT;\n\t}\n\n\treq->ki_res.obj = (u64)(unsigned long)user_iocb;\n\treq->ki_res.data = iocb->aio_data;\n\treq->ki_res.res = 0;\n\treq->ki_res.res2 = 0;\n\n\tswitch (iocb->aio_lio_opcode) {\n\tcase IOCB_CMD_PREAD:\n\t\treturn aio_read(&req->rw, iocb, false, compat);\n\tcase IOCB_CMD_PWRITE:\n\t\treturn aio_write(&req->rw, iocb, false, compat);\n\tcase IOCB_CMD_PREADV:\n\t\treturn aio_read(&req->rw, iocb, true, compat);\n\tcase IOCB_CMD_PWRITEV:\n\t\treturn aio_write(&req->rw, iocb, true, compat);\n\tcase IOCB_CMD_FSYNC:\n\t\treturn aio_fsync(&req->fsync, iocb, false);\n\tcase IOCB_CMD_FDSYNC:\n\t\treturn aio_fsync(&req->fsync, iocb, true);\n\tcase IOCB_CMD_POLL:\n\t\treturn aio_poll(req, iocb);\n\tdefault:\n\t\tpr_debug(\"invalid aio operation %d\\n\", iocb->aio_lio_opcode);\n\t\treturn -EINVAL;\n\t}\n}\n\nstatic int io_submit_one(struct kioctx *ctx, struct iocb __user *user_iocb,\n\t\t\t bool compat)\n{\n\tstruct aio_kiocb *req;\n\tstruct iocb iocb;\n\tint err;\n\n\tif (unlikely(copy_from_user(&iocb, user_iocb, sizeof(iocb))))\n\t\treturn -EFAULT;\n\n\t \n\tif (unlikely(iocb.aio_reserved2)) {\n\t\tpr_debug(\"EINVAL: reserve field set\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (unlikely(\n\t    (iocb.aio_buf != (unsigned long)iocb.aio_buf) ||\n\t    (iocb.aio_nbytes != (size_t)iocb.aio_nbytes) ||\n\t    ((ssize_t)iocb.aio_nbytes < 0)\n\t   )) {\n\t\tpr_debug(\"EINVAL: overflow check\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\treq = aio_get_req(ctx);\n\tif (unlikely(!req))\n\t\treturn -EAGAIN;\n\n\terr = __io_submit_one(ctx, &iocb, user_iocb, req, compat);\n\n\t \n\tiocb_put(req);\n\n\t \n\tif (unlikely(err)) {\n\t\tiocb_destroy(req);\n\t\tput_reqs_available(ctx, 1);\n\t}\n\treturn err;\n}\n\n \nSYSCALL_DEFINE3(io_submit, aio_context_t, ctx_id, long, nr,\n\t\tstruct iocb __user * __user *, iocbpp)\n{\n\tstruct kioctx *ctx;\n\tlong ret = 0;\n\tint i = 0;\n\tstruct blk_plug plug;\n\n\tif (unlikely(nr < 0))\n\t\treturn -EINVAL;\n\n\tctx = lookup_ioctx(ctx_id);\n\tif (unlikely(!ctx)) {\n\t\tpr_debug(\"EINVAL: invalid context id\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (nr > ctx->nr_events)\n\t\tnr = ctx->nr_events;\n\n\tif (nr > AIO_PLUG_THRESHOLD)\n\t\tblk_start_plug(&plug);\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct iocb __user *user_iocb;\n\n\t\tif (unlikely(get_user(user_iocb, iocbpp + i))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = io_submit_one(ctx, user_iocb, false);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tif (nr > AIO_PLUG_THRESHOLD)\n\t\tblk_finish_plug(&plug);\n\n\tpercpu_ref_put(&ctx->users);\n\treturn i ? i : ret;\n}\n\n#ifdef CONFIG_COMPAT\nCOMPAT_SYSCALL_DEFINE3(io_submit, compat_aio_context_t, ctx_id,\n\t\t       int, nr, compat_uptr_t __user *, iocbpp)\n{\n\tstruct kioctx *ctx;\n\tlong ret = 0;\n\tint i = 0;\n\tstruct blk_plug plug;\n\n\tif (unlikely(nr < 0))\n\t\treturn -EINVAL;\n\n\tctx = lookup_ioctx(ctx_id);\n\tif (unlikely(!ctx)) {\n\t\tpr_debug(\"EINVAL: invalid context id\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (nr > ctx->nr_events)\n\t\tnr = ctx->nr_events;\n\n\tif (nr > AIO_PLUG_THRESHOLD)\n\t\tblk_start_plug(&plug);\n\tfor (i = 0; i < nr; i++) {\n\t\tcompat_uptr_t user_iocb;\n\n\t\tif (unlikely(get_user(user_iocb, iocbpp + i))) {\n\t\t\tret = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\n\t\tret = io_submit_one(ctx, compat_ptr(user_iocb), true);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\tif (nr > AIO_PLUG_THRESHOLD)\n\t\tblk_finish_plug(&plug);\n\n\tpercpu_ref_put(&ctx->users);\n\treturn i ? i : ret;\n}\n#endif\n\n \nSYSCALL_DEFINE3(io_cancel, aio_context_t, ctx_id, struct iocb __user *, iocb,\n\t\tstruct io_event __user *, result)\n{\n\tstruct kioctx *ctx;\n\tstruct aio_kiocb *kiocb;\n\tint ret = -EINVAL;\n\tu32 key;\n\tu64 obj = (u64)(unsigned long)iocb;\n\n\tif (unlikely(get_user(key, &iocb->aio_key)))\n\t\treturn -EFAULT;\n\tif (unlikely(key != KIOCB_KEY))\n\t\treturn -EINVAL;\n\n\tctx = lookup_ioctx(ctx_id);\n\tif (unlikely(!ctx))\n\t\treturn -EINVAL;\n\n\tspin_lock_irq(&ctx->ctx_lock);\n\t \n\tlist_for_each_entry(kiocb, &ctx->active_reqs, ki_list) {\n\t\tif (kiocb->ki_res.obj == obj) {\n\t\t\tret = kiocb->ki_cancel(&kiocb->rw);\n\t\t\tlist_del_init(&kiocb->ki_list);\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock_irq(&ctx->ctx_lock);\n\n\tif (!ret) {\n\t\t \n\t\tret = -EINPROGRESS;\n\t}\n\n\tpercpu_ref_put(&ctx->users);\n\n\treturn ret;\n}\n\nstatic long do_io_getevents(aio_context_t ctx_id,\n\t\tlong min_nr,\n\t\tlong nr,\n\t\tstruct io_event __user *events,\n\t\tstruct timespec64 *ts)\n{\n\tktime_t until = ts ? timespec64_to_ktime(*ts) : KTIME_MAX;\n\tstruct kioctx *ioctx = lookup_ioctx(ctx_id);\n\tlong ret = -EINVAL;\n\n\tif (likely(ioctx)) {\n\t\tif (likely(min_nr <= nr && min_nr >= 0))\n\t\t\tret = read_events(ioctx, min_nr, nr, events, until);\n\t\tpercpu_ref_put(&ioctx->users);\n\t}\n\n\treturn ret;\n}\n\n \n#ifdef CONFIG_64BIT\n\nSYSCALL_DEFINE5(io_getevents, aio_context_t, ctx_id,\n\t\tlong, min_nr,\n\t\tlong, nr,\n\t\tstruct io_event __user *, events,\n\t\tstruct __kernel_timespec __user *, timeout)\n{\n\tstruct timespec64\tts;\n\tint\t\t\tret;\n\n\tif (timeout && unlikely(get_timespec64(&ts, timeout)))\n\t\treturn -EFAULT;\n\n\tret = do_io_getevents(ctx_id, min_nr, nr, events, timeout ? &ts : NULL);\n\tif (!ret && signal_pending(current))\n\t\tret = -EINTR;\n\treturn ret;\n}\n\n#endif\n\nstruct __aio_sigset {\n\tconst sigset_t __user\t*sigmask;\n\tsize_t\t\tsigsetsize;\n};\n\nSYSCALL_DEFINE6(io_pgetevents,\n\t\taio_context_t, ctx_id,\n\t\tlong, min_nr,\n\t\tlong, nr,\n\t\tstruct io_event __user *, events,\n\t\tstruct __kernel_timespec __user *, timeout,\n\t\tconst struct __aio_sigset __user *, usig)\n{\n\tstruct __aio_sigset\tksig = { NULL, };\n\tstruct timespec64\tts;\n\tbool interrupted;\n\tint ret;\n\n\tif (timeout && unlikely(get_timespec64(&ts, timeout)))\n\t\treturn -EFAULT;\n\n\tif (usig && copy_from_user(&ksig, usig, sizeof(ksig)))\n\t\treturn -EFAULT;\n\n\tret = set_user_sigmask(ksig.sigmask, ksig.sigsetsize);\n\tif (ret)\n\t\treturn ret;\n\n\tret = do_io_getevents(ctx_id, min_nr, nr, events, timeout ? &ts : NULL);\n\n\tinterrupted = signal_pending(current);\n\trestore_saved_sigmask_unless(interrupted);\n\tif (interrupted && !ret)\n\t\tret = -ERESTARTNOHAND;\n\n\treturn ret;\n}\n\n#if defined(CONFIG_COMPAT_32BIT_TIME) && !defined(CONFIG_64BIT)\n\nSYSCALL_DEFINE6(io_pgetevents_time32,\n\t\taio_context_t, ctx_id,\n\t\tlong, min_nr,\n\t\tlong, nr,\n\t\tstruct io_event __user *, events,\n\t\tstruct old_timespec32 __user *, timeout,\n\t\tconst struct __aio_sigset __user *, usig)\n{\n\tstruct __aio_sigset\tksig = { NULL, };\n\tstruct timespec64\tts;\n\tbool interrupted;\n\tint ret;\n\n\tif (timeout && unlikely(get_old_timespec32(&ts, timeout)))\n\t\treturn -EFAULT;\n\n\tif (usig && copy_from_user(&ksig, usig, sizeof(ksig)))\n\t\treturn -EFAULT;\n\n\n\tret = set_user_sigmask(ksig.sigmask, ksig.sigsetsize);\n\tif (ret)\n\t\treturn ret;\n\n\tret = do_io_getevents(ctx_id, min_nr, nr, events, timeout ? &ts : NULL);\n\n\tinterrupted = signal_pending(current);\n\trestore_saved_sigmask_unless(interrupted);\n\tif (interrupted && !ret)\n\t\tret = -ERESTARTNOHAND;\n\n\treturn ret;\n}\n\n#endif\n\n#if defined(CONFIG_COMPAT_32BIT_TIME)\n\nSYSCALL_DEFINE5(io_getevents_time32, __u32, ctx_id,\n\t\t__s32, min_nr,\n\t\t__s32, nr,\n\t\tstruct io_event __user *, events,\n\t\tstruct old_timespec32 __user *, timeout)\n{\n\tstruct timespec64 t;\n\tint ret;\n\n\tif (timeout && get_old_timespec32(&t, timeout))\n\t\treturn -EFAULT;\n\n\tret = do_io_getevents(ctx_id, min_nr, nr, events, timeout ? &t : NULL);\n\tif (!ret && signal_pending(current))\n\t\tret = -EINTR;\n\treturn ret;\n}\n\n#endif\n\n#ifdef CONFIG_COMPAT\n\nstruct __compat_aio_sigset {\n\tcompat_uptr_t\t\tsigmask;\n\tcompat_size_t\t\tsigsetsize;\n};\n\n#if defined(CONFIG_COMPAT_32BIT_TIME)\n\nCOMPAT_SYSCALL_DEFINE6(io_pgetevents,\n\t\tcompat_aio_context_t, ctx_id,\n\t\tcompat_long_t, min_nr,\n\t\tcompat_long_t, nr,\n\t\tstruct io_event __user *, events,\n\t\tstruct old_timespec32 __user *, timeout,\n\t\tconst struct __compat_aio_sigset __user *, usig)\n{\n\tstruct __compat_aio_sigset ksig = { 0, };\n\tstruct timespec64 t;\n\tbool interrupted;\n\tint ret;\n\n\tif (timeout && get_old_timespec32(&t, timeout))\n\t\treturn -EFAULT;\n\n\tif (usig && copy_from_user(&ksig, usig, sizeof(ksig)))\n\t\treturn -EFAULT;\n\n\tret = set_compat_user_sigmask(compat_ptr(ksig.sigmask), ksig.sigsetsize);\n\tif (ret)\n\t\treturn ret;\n\n\tret = do_io_getevents(ctx_id, min_nr, nr, events, timeout ? &t : NULL);\n\n\tinterrupted = signal_pending(current);\n\trestore_saved_sigmask_unless(interrupted);\n\tif (interrupted && !ret)\n\t\tret = -ERESTARTNOHAND;\n\n\treturn ret;\n}\n\n#endif\n\nCOMPAT_SYSCALL_DEFINE6(io_pgetevents_time64,\n\t\tcompat_aio_context_t, ctx_id,\n\t\tcompat_long_t, min_nr,\n\t\tcompat_long_t, nr,\n\t\tstruct io_event __user *, events,\n\t\tstruct __kernel_timespec __user *, timeout,\n\t\tconst struct __compat_aio_sigset __user *, usig)\n{\n\tstruct __compat_aio_sigset ksig = { 0, };\n\tstruct timespec64 t;\n\tbool interrupted;\n\tint ret;\n\n\tif (timeout && get_timespec64(&t, timeout))\n\t\treturn -EFAULT;\n\n\tif (usig && copy_from_user(&ksig, usig, sizeof(ksig)))\n\t\treturn -EFAULT;\n\n\tret = set_compat_user_sigmask(compat_ptr(ksig.sigmask), ksig.sigsetsize);\n\tif (ret)\n\t\treturn ret;\n\n\tret = do_io_getevents(ctx_id, min_nr, nr, events, timeout ? &t : NULL);\n\n\tinterrupted = signal_pending(current);\n\trestore_saved_sigmask_unless(interrupted);\n\tif (interrupted && !ret)\n\t\tret = -ERESTARTNOHAND;\n\n\treturn ret;\n}\n#endif\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}