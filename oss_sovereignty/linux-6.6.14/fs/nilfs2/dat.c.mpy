{
  "module_name": "dat.c",
  "hash_id": "eec83da9c48666775a1452c68cbc589b7ee4285f77cbdeabcb9512cbdd71f3f7",
  "original_prompt": "Ingested from linux-6.6.14/fs/nilfs2/dat.c",
  "human_readable_source": "\n \n\n#include <linux/types.h>\n#include <linux/buffer_head.h>\n#include <linux/string.h>\n#include <linux/errno.h>\n#include \"nilfs.h\"\n#include \"mdt.h\"\n#include \"alloc.h\"\n#include \"dat.h\"\n\n\n#define NILFS_CNO_MIN\t((__u64)1)\n#define NILFS_CNO_MAX\t(~(__u64)0)\n\n \nstruct nilfs_dat_info {\n\tstruct nilfs_mdt_info mi;\n\tstruct nilfs_palloc_cache palloc_cache;\n\tstruct nilfs_shadow_map shadow;\n};\n\nstatic inline struct nilfs_dat_info *NILFS_DAT_I(struct inode *dat)\n{\n\treturn (struct nilfs_dat_info *)NILFS_MDT(dat);\n}\n\nstatic int nilfs_dat_prepare_entry(struct inode *dat,\n\t\t\t\t   struct nilfs_palloc_req *req, int create)\n{\n\tint ret;\n\n\tret = nilfs_palloc_get_entry_block(dat, req->pr_entry_nr,\n\t\t\t\t\t   create, &req->pr_entry_bh);\n\tif (unlikely(ret == -ENOENT)) {\n\t\tnilfs_err(dat->i_sb,\n\t\t\t  \"DAT doesn't have a block to manage vblocknr = %llu\",\n\t\t\t  (unsigned long long)req->pr_entry_nr);\n\t\t \n\t\tret = -EINVAL;\n\t}\n\treturn ret;\n}\n\nstatic void nilfs_dat_commit_entry(struct inode *dat,\n\t\t\t\t   struct nilfs_palloc_req *req)\n{\n\tmark_buffer_dirty(req->pr_entry_bh);\n\tnilfs_mdt_mark_dirty(dat);\n\tbrelse(req->pr_entry_bh);\n}\n\nstatic void nilfs_dat_abort_entry(struct inode *dat,\n\t\t\t\t  struct nilfs_palloc_req *req)\n{\n\tbrelse(req->pr_entry_bh);\n}\n\nint nilfs_dat_prepare_alloc(struct inode *dat, struct nilfs_palloc_req *req)\n{\n\tint ret;\n\n\tret = nilfs_palloc_prepare_alloc_entry(dat, req);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tret = nilfs_dat_prepare_entry(dat, req, 1);\n\tif (ret < 0)\n\t\tnilfs_palloc_abort_alloc_entry(dat, req);\n\n\treturn ret;\n}\n\nvoid nilfs_dat_commit_alloc(struct inode *dat, struct nilfs_palloc_req *req)\n{\n\tstruct nilfs_dat_entry *entry;\n\tvoid *kaddr;\n\n\tkaddr = kmap_atomic(req->pr_entry_bh->b_page);\n\tentry = nilfs_palloc_block_get_entry(dat, req->pr_entry_nr,\n\t\t\t\t\t     req->pr_entry_bh, kaddr);\n\tentry->de_start = cpu_to_le64(NILFS_CNO_MIN);\n\tentry->de_end = cpu_to_le64(NILFS_CNO_MAX);\n\tentry->de_blocknr = cpu_to_le64(0);\n\tkunmap_atomic(kaddr);\n\n\tnilfs_palloc_commit_alloc_entry(dat, req);\n\tnilfs_dat_commit_entry(dat, req);\n}\n\nvoid nilfs_dat_abort_alloc(struct inode *dat, struct nilfs_palloc_req *req)\n{\n\tnilfs_dat_abort_entry(dat, req);\n\tnilfs_palloc_abort_alloc_entry(dat, req);\n}\n\nstatic void nilfs_dat_commit_free(struct inode *dat,\n\t\t\t\t  struct nilfs_palloc_req *req)\n{\n\tstruct nilfs_dat_entry *entry;\n\tvoid *kaddr;\n\n\tkaddr = kmap_atomic(req->pr_entry_bh->b_page);\n\tentry = nilfs_palloc_block_get_entry(dat, req->pr_entry_nr,\n\t\t\t\t\t     req->pr_entry_bh, kaddr);\n\tentry->de_start = cpu_to_le64(NILFS_CNO_MIN);\n\tentry->de_end = cpu_to_le64(NILFS_CNO_MIN);\n\tentry->de_blocknr = cpu_to_le64(0);\n\tkunmap_atomic(kaddr);\n\n\tnilfs_dat_commit_entry(dat, req);\n\n\tif (unlikely(req->pr_desc_bh == NULL || req->pr_bitmap_bh == NULL)) {\n\t\tnilfs_error(dat->i_sb,\n\t\t\t    \"state inconsistency probably due to duplicate use of vblocknr = %llu\",\n\t\t\t    (unsigned long long)req->pr_entry_nr);\n\t\treturn;\n\t}\n\tnilfs_palloc_commit_free_entry(dat, req);\n}\n\nint nilfs_dat_prepare_start(struct inode *dat, struct nilfs_palloc_req *req)\n{\n\treturn nilfs_dat_prepare_entry(dat, req, 0);\n}\n\nvoid nilfs_dat_commit_start(struct inode *dat, struct nilfs_palloc_req *req,\n\t\t\t    sector_t blocknr)\n{\n\tstruct nilfs_dat_entry *entry;\n\tvoid *kaddr;\n\n\tkaddr = kmap_atomic(req->pr_entry_bh->b_page);\n\tentry = nilfs_palloc_block_get_entry(dat, req->pr_entry_nr,\n\t\t\t\t\t     req->pr_entry_bh, kaddr);\n\tentry->de_start = cpu_to_le64(nilfs_mdt_cno(dat));\n\tentry->de_blocknr = cpu_to_le64(blocknr);\n\tkunmap_atomic(kaddr);\n\n\tnilfs_dat_commit_entry(dat, req);\n}\n\nint nilfs_dat_prepare_end(struct inode *dat, struct nilfs_palloc_req *req)\n{\n\tstruct nilfs_dat_entry *entry;\n\t__u64 start;\n\tsector_t blocknr;\n\tvoid *kaddr;\n\tint ret;\n\n\tret = nilfs_dat_prepare_entry(dat, req, 0);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tkaddr = kmap_atomic(req->pr_entry_bh->b_page);\n\tentry = nilfs_palloc_block_get_entry(dat, req->pr_entry_nr,\n\t\t\t\t\t     req->pr_entry_bh, kaddr);\n\tstart = le64_to_cpu(entry->de_start);\n\tblocknr = le64_to_cpu(entry->de_blocknr);\n\tkunmap_atomic(kaddr);\n\n\tif (blocknr == 0) {\n\t\tret = nilfs_palloc_prepare_free_entry(dat, req);\n\t\tif (ret < 0) {\n\t\t\tnilfs_dat_abort_entry(dat, req);\n\t\t\treturn ret;\n\t\t}\n\t}\n\tif (unlikely(start > nilfs_mdt_cno(dat))) {\n\t\tnilfs_err(dat->i_sb,\n\t\t\t  \"vblocknr = %llu has abnormal lifetime: start cno (= %llu) > current cno (= %llu)\",\n\t\t\t  (unsigned long long)req->pr_entry_nr,\n\t\t\t  (unsigned long long)start,\n\t\t\t  (unsigned long long)nilfs_mdt_cno(dat));\n\t\tnilfs_dat_abort_entry(dat, req);\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nvoid nilfs_dat_commit_end(struct inode *dat, struct nilfs_palloc_req *req,\n\t\t\t  int dead)\n{\n\tstruct nilfs_dat_entry *entry;\n\t__u64 start, end;\n\tsector_t blocknr;\n\tvoid *kaddr;\n\n\tkaddr = kmap_atomic(req->pr_entry_bh->b_page);\n\tentry = nilfs_palloc_block_get_entry(dat, req->pr_entry_nr,\n\t\t\t\t\t     req->pr_entry_bh, kaddr);\n\tend = start = le64_to_cpu(entry->de_start);\n\tif (!dead) {\n\t\tend = nilfs_mdt_cno(dat);\n\t\tWARN_ON(start > end);\n\t}\n\tentry->de_end = cpu_to_le64(end);\n\tblocknr = le64_to_cpu(entry->de_blocknr);\n\tkunmap_atomic(kaddr);\n\n\tif (blocknr == 0)\n\t\tnilfs_dat_commit_free(dat, req);\n\telse\n\t\tnilfs_dat_commit_entry(dat, req);\n}\n\nvoid nilfs_dat_abort_end(struct inode *dat, struct nilfs_palloc_req *req)\n{\n\tstruct nilfs_dat_entry *entry;\n\t__u64 start;\n\tsector_t blocknr;\n\tvoid *kaddr;\n\n\tkaddr = kmap_atomic(req->pr_entry_bh->b_page);\n\tentry = nilfs_palloc_block_get_entry(dat, req->pr_entry_nr,\n\t\t\t\t\t     req->pr_entry_bh, kaddr);\n\tstart = le64_to_cpu(entry->de_start);\n\tblocknr = le64_to_cpu(entry->de_blocknr);\n\tkunmap_atomic(kaddr);\n\n\tif (start == nilfs_mdt_cno(dat) && blocknr == 0)\n\t\tnilfs_palloc_abort_free_entry(dat, req);\n\tnilfs_dat_abort_entry(dat, req);\n}\n\nint nilfs_dat_prepare_update(struct inode *dat,\n\t\t\t     struct nilfs_palloc_req *oldreq,\n\t\t\t     struct nilfs_palloc_req *newreq)\n{\n\tint ret;\n\n\tret = nilfs_dat_prepare_end(dat, oldreq);\n\tif (!ret) {\n\t\tret = nilfs_dat_prepare_alloc(dat, newreq);\n\t\tif (ret < 0)\n\t\t\tnilfs_dat_abort_end(dat, oldreq);\n\t}\n\treturn ret;\n}\n\nvoid nilfs_dat_commit_update(struct inode *dat,\n\t\t\t     struct nilfs_palloc_req *oldreq,\n\t\t\t     struct nilfs_palloc_req *newreq, int dead)\n{\n\tnilfs_dat_commit_end(dat, oldreq, dead);\n\tnilfs_dat_commit_alloc(dat, newreq);\n}\n\nvoid nilfs_dat_abort_update(struct inode *dat,\n\t\t\t    struct nilfs_palloc_req *oldreq,\n\t\t\t    struct nilfs_palloc_req *newreq)\n{\n\tnilfs_dat_abort_end(dat, oldreq);\n\tnilfs_dat_abort_alloc(dat, newreq);\n}\n\n \nint nilfs_dat_mark_dirty(struct inode *dat, __u64 vblocknr)\n{\n\tstruct nilfs_palloc_req req;\n\tint ret;\n\n\treq.pr_entry_nr = vblocknr;\n\tret = nilfs_dat_prepare_entry(dat, &req, 0);\n\tif (ret == 0)\n\t\tnilfs_dat_commit_entry(dat, &req);\n\treturn ret;\n}\n\n \nint nilfs_dat_freev(struct inode *dat, __u64 *vblocknrs, size_t nitems)\n{\n\treturn nilfs_palloc_freev(dat, vblocknrs, nitems);\n}\n\n \nint nilfs_dat_move(struct inode *dat, __u64 vblocknr, sector_t blocknr)\n{\n\tstruct buffer_head *entry_bh;\n\tstruct nilfs_dat_entry *entry;\n\tvoid *kaddr;\n\tint ret;\n\n\tret = nilfs_palloc_get_entry_block(dat, vblocknr, 0, &entry_bh);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tif (!buffer_nilfs_redirected(entry_bh)) {\n\t\tret = nilfs_mdt_freeze_buffer(dat, entry_bh);\n\t\tif (ret) {\n\t\t\tbrelse(entry_bh);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tkaddr = kmap_atomic(entry_bh->b_page);\n\tentry = nilfs_palloc_block_get_entry(dat, vblocknr, entry_bh, kaddr);\n\tif (unlikely(entry->de_blocknr == cpu_to_le64(0))) {\n\t\tnilfs_crit(dat->i_sb,\n\t\t\t   \"%s: invalid vblocknr = %llu, [%llu, %llu)\",\n\t\t\t   __func__, (unsigned long long)vblocknr,\n\t\t\t   (unsigned long long)le64_to_cpu(entry->de_start),\n\t\t\t   (unsigned long long)le64_to_cpu(entry->de_end));\n\t\tkunmap_atomic(kaddr);\n\t\tbrelse(entry_bh);\n\t\treturn -EINVAL;\n\t}\n\tWARN_ON(blocknr == 0);\n\tentry->de_blocknr = cpu_to_le64(blocknr);\n\tkunmap_atomic(kaddr);\n\n\tmark_buffer_dirty(entry_bh);\n\tnilfs_mdt_mark_dirty(dat);\n\n\tbrelse(entry_bh);\n\n\treturn 0;\n}\n\n \nint nilfs_dat_translate(struct inode *dat, __u64 vblocknr, sector_t *blocknrp)\n{\n\tstruct buffer_head *entry_bh, *bh;\n\tstruct nilfs_dat_entry *entry;\n\tsector_t blocknr;\n\tvoid *kaddr;\n\tint ret;\n\n\tret = nilfs_palloc_get_entry_block(dat, vblocknr, 0, &entry_bh);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tif (!nilfs_doing_gc() && buffer_nilfs_redirected(entry_bh)) {\n\t\tbh = nilfs_mdt_get_frozen_buffer(dat, entry_bh);\n\t\tif (bh) {\n\t\t\tWARN_ON(!buffer_uptodate(bh));\n\t\t\tbrelse(entry_bh);\n\t\t\tentry_bh = bh;\n\t\t}\n\t}\n\n\tkaddr = kmap_atomic(entry_bh->b_page);\n\tentry = nilfs_palloc_block_get_entry(dat, vblocknr, entry_bh, kaddr);\n\tblocknr = le64_to_cpu(entry->de_blocknr);\n\tif (blocknr == 0) {\n\t\tret = -ENOENT;\n\t\tgoto out;\n\t}\n\t*blocknrp = blocknr;\n\n out:\n\tkunmap_atomic(kaddr);\n\tbrelse(entry_bh);\n\treturn ret;\n}\n\nssize_t nilfs_dat_get_vinfo(struct inode *dat, void *buf, unsigned int visz,\n\t\t\t    size_t nvi)\n{\n\tstruct buffer_head *entry_bh;\n\tstruct nilfs_dat_entry *entry;\n\tstruct nilfs_vinfo *vinfo = buf;\n\t__u64 first, last;\n\tvoid *kaddr;\n\tunsigned long entries_per_block = NILFS_MDT(dat)->mi_entries_per_block;\n\tint i, j, n, ret;\n\n\tfor (i = 0; i < nvi; i += n) {\n\t\tret = nilfs_palloc_get_entry_block(dat, vinfo->vi_vblocknr,\n\t\t\t\t\t\t   0, &entry_bh);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tkaddr = kmap_atomic(entry_bh->b_page);\n\t\t \n\t\tfirst = vinfo->vi_vblocknr;\n\t\tdo_div(first, entries_per_block);\n\t\tfirst *= entries_per_block;\n\t\tlast = first + entries_per_block - 1;\n\t\tfor (j = i, n = 0;\n\t\t     j < nvi && vinfo->vi_vblocknr >= first &&\n\t\t\t     vinfo->vi_vblocknr <= last;\n\t\t     j++, n++, vinfo = (void *)vinfo + visz) {\n\t\t\tentry = nilfs_palloc_block_get_entry(\n\t\t\t\tdat, vinfo->vi_vblocknr, entry_bh, kaddr);\n\t\t\tvinfo->vi_start = le64_to_cpu(entry->de_start);\n\t\t\tvinfo->vi_end = le64_to_cpu(entry->de_end);\n\t\t\tvinfo->vi_blocknr = le64_to_cpu(entry->de_blocknr);\n\t\t}\n\t\tkunmap_atomic(kaddr);\n\t\tbrelse(entry_bh);\n\t}\n\n\treturn nvi;\n}\n\n \nint nilfs_dat_read(struct super_block *sb, size_t entry_size,\n\t\t   struct nilfs_inode *raw_inode, struct inode **inodep)\n{\n\tstatic struct lock_class_key dat_lock_key;\n\tstruct inode *dat;\n\tstruct nilfs_dat_info *di;\n\tint err;\n\n\tif (entry_size > sb->s_blocksize) {\n\t\tnilfs_err(sb, \"too large DAT entry size: %zu bytes\",\n\t\t\t  entry_size);\n\t\treturn -EINVAL;\n\t} else if (entry_size < NILFS_MIN_DAT_ENTRY_SIZE) {\n\t\tnilfs_err(sb, \"too small DAT entry size: %zu bytes\",\n\t\t\t  entry_size);\n\t\treturn -EINVAL;\n\t}\n\n\tdat = nilfs_iget_locked(sb, NULL, NILFS_DAT_INO);\n\tif (unlikely(!dat))\n\t\treturn -ENOMEM;\n\tif (!(dat->i_state & I_NEW))\n\t\tgoto out;\n\n\terr = nilfs_mdt_init(dat, NILFS_MDT_GFP, sizeof(*di));\n\tif (err)\n\t\tgoto failed;\n\n\terr = nilfs_palloc_init_blockgroup(dat, entry_size);\n\tif (err)\n\t\tgoto failed;\n\n\tdi = NILFS_DAT_I(dat);\n\tlockdep_set_class(&di->mi.mi_sem, &dat_lock_key);\n\tnilfs_palloc_setup_cache(dat, &di->palloc_cache);\n\terr = nilfs_mdt_setup_shadow_map(dat, &di->shadow);\n\tif (err)\n\t\tgoto failed;\n\n\terr = nilfs_read_inode_common(dat, raw_inode);\n\tif (err)\n\t\tgoto failed;\n\n\tunlock_new_inode(dat);\n out:\n\t*inodep = dat;\n\treturn 0;\n failed:\n\tiget_failed(dat);\n\treturn err;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}