{
  "module_name": "transport_rdma.c",
  "hash_id": "d6d16e8369a3b54af374a9a4c55ff1961c0d7e1792eeed7e14c4b69dee7da704",
  "original_prompt": "Ingested from linux-6.6.14/fs/smb/server/transport_rdma.c",
  "human_readable_source": "\n \n\n#define SUBMOD_NAME\t\"smb_direct\"\n\n#include <linux/kthread.h>\n#include <linux/list.h>\n#include <linux/mempool.h>\n#include <linux/highmem.h>\n#include <linux/scatterlist.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/rdma_cm.h>\n#include <rdma/rw.h>\n\n#include \"glob.h\"\n#include \"connection.h\"\n#include \"smb_common.h\"\n#include \"smbstatus.h\"\n#include \"transport_rdma.h\"\n\n#define SMB_DIRECT_PORT_IWARP\t\t5445\n#define SMB_DIRECT_PORT_INFINIBAND\t445\n\n#define SMB_DIRECT_VERSION_LE\t\tcpu_to_le16(0x0100)\n\n \n#define SMB_DIRECT_NEGOTIATE_TIMEOUT\t\t120\n\n#define SMB_DIRECT_MAX_SEND_SGES\t\t6\n#define SMB_DIRECT_MAX_RECV_SGES\t\t1\n\n \n#define SMB_DIRECT_CM_INITIATOR_DEPTH\t\t8\n\n \n#define SMB_DIRECT_CM_RETRY\t\t\t6\n \n#define SMB_DIRECT_CM_RNR_RETRY\t\t0\n\n \n\n \nstatic int smb_direct_port = SMB_DIRECT_PORT_INFINIBAND;\n\n \nstatic int smb_direct_receive_credit_max = 255;\n\n \nstatic int smb_direct_send_credit_target = 255;\n\n \nstatic int smb_direct_max_send_size = 1364;\n\n \nstatic int smb_direct_max_fragmented_recv_size = 1024 * 1024;\n\n \nstatic int smb_direct_max_receive_size = 1364;\n\nstatic int smb_direct_max_read_write_size = SMBD_DEFAULT_IOSIZE;\n\nstatic LIST_HEAD(smb_direct_device_list);\nstatic DEFINE_RWLOCK(smb_direct_device_lock);\n\nstruct smb_direct_device {\n\tstruct ib_device\t*ib_dev;\n\tstruct list_head\tlist;\n};\n\nstatic struct smb_direct_listener {\n\tstruct rdma_cm_id\t*cm_id;\n} smb_direct_listener;\n\nstatic struct workqueue_struct *smb_direct_wq;\n\nenum smb_direct_status {\n\tSMB_DIRECT_CS_NEW = 0,\n\tSMB_DIRECT_CS_CONNECTED,\n\tSMB_DIRECT_CS_DISCONNECTING,\n\tSMB_DIRECT_CS_DISCONNECTED,\n};\n\nstruct smb_direct_transport {\n\tstruct ksmbd_transport\ttransport;\n\n\tenum smb_direct_status\tstatus;\n\tbool\t\t\tfull_packet_received;\n\twait_queue_head_t\twait_status;\n\n\tstruct rdma_cm_id\t*cm_id;\n\tstruct ib_cq\t\t*send_cq;\n\tstruct ib_cq\t\t*recv_cq;\n\tstruct ib_pd\t\t*pd;\n\tstruct ib_qp\t\t*qp;\n\n\tint\t\t\tmax_send_size;\n\tint\t\t\tmax_recv_size;\n\tint\t\t\tmax_fragmented_send_size;\n\tint\t\t\tmax_fragmented_recv_size;\n\tint\t\t\tmax_rdma_rw_size;\n\n\tspinlock_t\t\treassembly_queue_lock;\n\tstruct list_head\treassembly_queue;\n\tint\t\t\treassembly_data_length;\n\tint\t\t\treassembly_queue_length;\n\tint\t\t\tfirst_entry_offset;\n\twait_queue_head_t\twait_reassembly_queue;\n\n\tspinlock_t\t\treceive_credit_lock;\n\tint\t\t\trecv_credits;\n\tint\t\t\tcount_avail_recvmsg;\n\tint\t\t\trecv_credit_max;\n\tint\t\t\trecv_credit_target;\n\n\tspinlock_t\t\trecvmsg_queue_lock;\n\tstruct list_head\trecvmsg_queue;\n\n\tspinlock_t\t\tempty_recvmsg_queue_lock;\n\tstruct list_head\tempty_recvmsg_queue;\n\n\tint\t\t\tsend_credit_target;\n\tatomic_t\t\tsend_credits;\n\tspinlock_t\t\tlock_new_recv_credits;\n\tint\t\t\tnew_recv_credits;\n\tint\t\t\tmax_rw_credits;\n\tint\t\t\tpages_per_rw_credit;\n\tatomic_t\t\trw_credits;\n\n\twait_queue_head_t\twait_send_credits;\n\twait_queue_head_t\twait_rw_credits;\n\n\tmempool_t\t\t*sendmsg_mempool;\n\tstruct kmem_cache\t*sendmsg_cache;\n\tmempool_t\t\t*recvmsg_mempool;\n\tstruct kmem_cache\t*recvmsg_cache;\n\n\twait_queue_head_t\twait_send_pending;\n\tatomic_t\t\tsend_pending;\n\n\tstruct delayed_work\tpost_recv_credits_work;\n\tstruct work_struct\tsend_immediate_work;\n\tstruct work_struct\tdisconnect_work;\n\n\tbool\t\t\tnegotiation_requested;\n};\n\n#define KSMBD_TRANS(t) ((struct ksmbd_transport *)&((t)->transport))\n\nenum {\n\tSMB_DIRECT_MSG_NEGOTIATE_REQ = 0,\n\tSMB_DIRECT_MSG_DATA_TRANSFER\n};\n\nstatic struct ksmbd_transport_ops ksmbd_smb_direct_transport_ops;\n\nstruct smb_direct_send_ctx {\n\tstruct list_head\tmsg_list;\n\tint\t\t\twr_cnt;\n\tbool\t\t\tneed_invalidate_rkey;\n\tunsigned int\t\tremote_key;\n};\n\nstruct smb_direct_sendmsg {\n\tstruct smb_direct_transport\t*transport;\n\tstruct ib_send_wr\twr;\n\tstruct list_head\tlist;\n\tint\t\t\tnum_sge;\n\tstruct ib_sge\t\tsge[SMB_DIRECT_MAX_SEND_SGES];\n\tstruct ib_cqe\t\tcqe;\n\tu8\t\t\tpacket[];\n};\n\nstruct smb_direct_recvmsg {\n\tstruct smb_direct_transport\t*transport;\n\tstruct list_head\tlist;\n\tint\t\t\ttype;\n\tstruct ib_sge\t\tsge;\n\tstruct ib_cqe\t\tcqe;\n\tbool\t\t\tfirst_segment;\n\tu8\t\t\tpacket[];\n};\n\nstruct smb_direct_rdma_rw_msg {\n\tstruct smb_direct_transport\t*t;\n\tstruct ib_cqe\t\tcqe;\n\tint\t\t\tstatus;\n\tstruct completion\t*completion;\n\tstruct list_head\tlist;\n\tstruct rdma_rw_ctx\trw_ctx;\n\tstruct sg_table\t\tsgt;\n\tstruct scatterlist\tsg_list[];\n};\n\nvoid init_smbd_max_io_size(unsigned int sz)\n{\n\tsz = clamp_val(sz, SMBD_MIN_IOSIZE, SMBD_MAX_IOSIZE);\n\tsmb_direct_max_read_write_size = sz;\n}\n\nunsigned int get_smbd_max_read_write_size(void)\n{\n\treturn smb_direct_max_read_write_size;\n}\n\nstatic inline int get_buf_page_count(void *buf, int size)\n{\n\treturn DIV_ROUND_UP((uintptr_t)buf + size, PAGE_SIZE) -\n\t\t(uintptr_t)buf / PAGE_SIZE;\n}\n\nstatic void smb_direct_destroy_pools(struct smb_direct_transport *transport);\nstatic void smb_direct_post_recv_credits(struct work_struct *work);\nstatic int smb_direct_post_send_data(struct smb_direct_transport *t,\n\t\t\t\t     struct smb_direct_send_ctx *send_ctx,\n\t\t\t\t     struct kvec *iov, int niov,\n\t\t\t\t     int remaining_data_length);\n\nstatic inline struct smb_direct_transport *\nsmb_trans_direct_transfort(struct ksmbd_transport *t)\n{\n\treturn container_of(t, struct smb_direct_transport, transport);\n}\n\nstatic inline void\n*smb_direct_recvmsg_payload(struct smb_direct_recvmsg *recvmsg)\n{\n\treturn (void *)recvmsg->packet;\n}\n\nstatic inline bool is_receive_credit_post_required(int receive_credits,\n\t\t\t\t\t\t   int avail_recvmsg_count)\n{\n\treturn receive_credits <= (smb_direct_receive_credit_max >> 3) &&\n\t\tavail_recvmsg_count >= (receive_credits >> 2);\n}\n\nstatic struct\nsmb_direct_recvmsg *get_free_recvmsg(struct smb_direct_transport *t)\n{\n\tstruct smb_direct_recvmsg *recvmsg = NULL;\n\n\tspin_lock(&t->recvmsg_queue_lock);\n\tif (!list_empty(&t->recvmsg_queue)) {\n\t\trecvmsg = list_first_entry(&t->recvmsg_queue,\n\t\t\t\t\t   struct smb_direct_recvmsg,\n\t\t\t\t\t   list);\n\t\tlist_del(&recvmsg->list);\n\t}\n\tspin_unlock(&t->recvmsg_queue_lock);\n\treturn recvmsg;\n}\n\nstatic void put_recvmsg(struct smb_direct_transport *t,\n\t\t\tstruct smb_direct_recvmsg *recvmsg)\n{\n\tib_dma_unmap_single(t->cm_id->device, recvmsg->sge.addr,\n\t\t\t    recvmsg->sge.length, DMA_FROM_DEVICE);\n\n\tspin_lock(&t->recvmsg_queue_lock);\n\tlist_add(&recvmsg->list, &t->recvmsg_queue);\n\tspin_unlock(&t->recvmsg_queue_lock);\n}\n\nstatic struct\nsmb_direct_recvmsg *get_empty_recvmsg(struct smb_direct_transport *t)\n{\n\tstruct smb_direct_recvmsg *recvmsg = NULL;\n\n\tspin_lock(&t->empty_recvmsg_queue_lock);\n\tif (!list_empty(&t->empty_recvmsg_queue)) {\n\t\trecvmsg = list_first_entry(&t->empty_recvmsg_queue,\n\t\t\t\t\t   struct smb_direct_recvmsg, list);\n\t\tlist_del(&recvmsg->list);\n\t}\n\tspin_unlock(&t->empty_recvmsg_queue_lock);\n\treturn recvmsg;\n}\n\nstatic void put_empty_recvmsg(struct smb_direct_transport *t,\n\t\t\t      struct smb_direct_recvmsg *recvmsg)\n{\n\tib_dma_unmap_single(t->cm_id->device, recvmsg->sge.addr,\n\t\t\t    recvmsg->sge.length, DMA_FROM_DEVICE);\n\n\tspin_lock(&t->empty_recvmsg_queue_lock);\n\tlist_add_tail(&recvmsg->list, &t->empty_recvmsg_queue);\n\tspin_unlock(&t->empty_recvmsg_queue_lock);\n}\n\nstatic void enqueue_reassembly(struct smb_direct_transport *t,\n\t\t\t       struct smb_direct_recvmsg *recvmsg,\n\t\t\t       int data_length)\n{\n\tspin_lock(&t->reassembly_queue_lock);\n\tlist_add_tail(&recvmsg->list, &t->reassembly_queue);\n\tt->reassembly_queue_length++;\n\t \n\tvirt_wmb();\n\tt->reassembly_data_length += data_length;\n\tspin_unlock(&t->reassembly_queue_lock);\n}\n\nstatic struct smb_direct_recvmsg *get_first_reassembly(struct smb_direct_transport *t)\n{\n\tif (!list_empty(&t->reassembly_queue))\n\t\treturn list_first_entry(&t->reassembly_queue,\n\t\t\t\tstruct smb_direct_recvmsg, list);\n\telse\n\t\treturn NULL;\n}\n\nstatic void smb_direct_disconnect_rdma_work(struct work_struct *work)\n{\n\tstruct smb_direct_transport *t =\n\t\tcontainer_of(work, struct smb_direct_transport,\n\t\t\t     disconnect_work);\n\n\tif (t->status == SMB_DIRECT_CS_CONNECTED) {\n\t\tt->status = SMB_DIRECT_CS_DISCONNECTING;\n\t\trdma_disconnect(t->cm_id);\n\t}\n}\n\nstatic void\nsmb_direct_disconnect_rdma_connection(struct smb_direct_transport *t)\n{\n\tif (t->status == SMB_DIRECT_CS_CONNECTED)\n\t\tqueue_work(smb_direct_wq, &t->disconnect_work);\n}\n\nstatic void smb_direct_send_immediate_work(struct work_struct *work)\n{\n\tstruct smb_direct_transport *t = container_of(work,\n\t\t\tstruct smb_direct_transport, send_immediate_work);\n\n\tif (t->status != SMB_DIRECT_CS_CONNECTED)\n\t\treturn;\n\n\tsmb_direct_post_send_data(t, NULL, NULL, 0, 0);\n}\n\nstatic struct smb_direct_transport *alloc_transport(struct rdma_cm_id *cm_id)\n{\n\tstruct smb_direct_transport *t;\n\tstruct ksmbd_conn *conn;\n\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (!t)\n\t\treturn NULL;\n\n\tt->cm_id = cm_id;\n\tcm_id->context = t;\n\n\tt->status = SMB_DIRECT_CS_NEW;\n\tinit_waitqueue_head(&t->wait_status);\n\n\tspin_lock_init(&t->reassembly_queue_lock);\n\tINIT_LIST_HEAD(&t->reassembly_queue);\n\tt->reassembly_data_length = 0;\n\tt->reassembly_queue_length = 0;\n\tinit_waitqueue_head(&t->wait_reassembly_queue);\n\tinit_waitqueue_head(&t->wait_send_credits);\n\tinit_waitqueue_head(&t->wait_rw_credits);\n\n\tspin_lock_init(&t->receive_credit_lock);\n\tspin_lock_init(&t->recvmsg_queue_lock);\n\tINIT_LIST_HEAD(&t->recvmsg_queue);\n\n\tspin_lock_init(&t->empty_recvmsg_queue_lock);\n\tINIT_LIST_HEAD(&t->empty_recvmsg_queue);\n\n\tinit_waitqueue_head(&t->wait_send_pending);\n\tatomic_set(&t->send_pending, 0);\n\n\tspin_lock_init(&t->lock_new_recv_credits);\n\n\tINIT_DELAYED_WORK(&t->post_recv_credits_work,\n\t\t\t  smb_direct_post_recv_credits);\n\tINIT_WORK(&t->send_immediate_work, smb_direct_send_immediate_work);\n\tINIT_WORK(&t->disconnect_work, smb_direct_disconnect_rdma_work);\n\n\tconn = ksmbd_conn_alloc();\n\tif (!conn)\n\t\tgoto err;\n\tconn->transport = KSMBD_TRANS(t);\n\tKSMBD_TRANS(t)->conn = conn;\n\tKSMBD_TRANS(t)->ops = &ksmbd_smb_direct_transport_ops;\n\treturn t;\nerr:\n\tkfree(t);\n\treturn NULL;\n}\n\nstatic void free_transport(struct smb_direct_transport *t)\n{\n\tstruct smb_direct_recvmsg *recvmsg;\n\n\twake_up_interruptible(&t->wait_send_credits);\n\n\tksmbd_debug(RDMA, \"wait for all send posted to IB to finish\\n\");\n\twait_event(t->wait_send_pending,\n\t\t   atomic_read(&t->send_pending) == 0);\n\n\tcancel_work_sync(&t->disconnect_work);\n\tcancel_delayed_work_sync(&t->post_recv_credits_work);\n\tcancel_work_sync(&t->send_immediate_work);\n\n\tif (t->qp) {\n\t\tib_drain_qp(t->qp);\n\t\tib_mr_pool_destroy(t->qp, &t->qp->rdma_mrs);\n\t\tib_destroy_qp(t->qp);\n\t}\n\n\tksmbd_debug(RDMA, \"drain the reassembly queue\\n\");\n\tdo {\n\t\tspin_lock(&t->reassembly_queue_lock);\n\t\trecvmsg = get_first_reassembly(t);\n\t\tif (recvmsg) {\n\t\t\tlist_del(&recvmsg->list);\n\t\t\tspin_unlock(&t->reassembly_queue_lock);\n\t\t\tput_recvmsg(t, recvmsg);\n\t\t} else {\n\t\t\tspin_unlock(&t->reassembly_queue_lock);\n\t\t}\n\t} while (recvmsg);\n\tt->reassembly_data_length = 0;\n\n\tif (t->send_cq)\n\t\tib_free_cq(t->send_cq);\n\tif (t->recv_cq)\n\t\tib_free_cq(t->recv_cq);\n\tif (t->pd)\n\t\tib_dealloc_pd(t->pd);\n\tif (t->cm_id)\n\t\trdma_destroy_id(t->cm_id);\n\n\tsmb_direct_destroy_pools(t);\n\tksmbd_conn_free(KSMBD_TRANS(t)->conn);\n\tkfree(t);\n}\n\nstatic struct smb_direct_sendmsg\n*smb_direct_alloc_sendmsg(struct smb_direct_transport *t)\n{\n\tstruct smb_direct_sendmsg *msg;\n\n\tmsg = mempool_alloc(t->sendmsg_mempool, GFP_KERNEL);\n\tif (!msg)\n\t\treturn ERR_PTR(-ENOMEM);\n\tmsg->transport = t;\n\tINIT_LIST_HEAD(&msg->list);\n\tmsg->num_sge = 0;\n\treturn msg;\n}\n\nstatic void smb_direct_free_sendmsg(struct smb_direct_transport *t,\n\t\t\t\t    struct smb_direct_sendmsg *msg)\n{\n\tint i;\n\n\tif (msg->num_sge > 0) {\n\t\tib_dma_unmap_single(t->cm_id->device,\n\t\t\t\t    msg->sge[0].addr, msg->sge[0].length,\n\t\t\t\t    DMA_TO_DEVICE);\n\t\tfor (i = 1; i < msg->num_sge; i++)\n\t\t\tib_dma_unmap_page(t->cm_id->device,\n\t\t\t\t\t  msg->sge[i].addr, msg->sge[i].length,\n\t\t\t\t\t  DMA_TO_DEVICE);\n\t}\n\tmempool_free(msg, t->sendmsg_mempool);\n}\n\nstatic int smb_direct_check_recvmsg(struct smb_direct_recvmsg *recvmsg)\n{\n\tswitch (recvmsg->type) {\n\tcase SMB_DIRECT_MSG_DATA_TRANSFER: {\n\t\tstruct smb_direct_data_transfer *req =\n\t\t\t(struct smb_direct_data_transfer *)recvmsg->packet;\n\t\tstruct smb2_hdr *hdr = (struct smb2_hdr *)(recvmsg->packet\n\t\t\t\t+ le32_to_cpu(req->data_offset));\n\t\tksmbd_debug(RDMA,\n\t\t\t    \"CreditGranted: %u, CreditRequested: %u, DataLength: %u, RemainingDataLength: %u, SMB: %x, Command: %u\\n\",\n\t\t\t    le16_to_cpu(req->credits_granted),\n\t\t\t    le16_to_cpu(req->credits_requested),\n\t\t\t    req->data_length, req->remaining_data_length,\n\t\t\t    hdr->ProtocolId, hdr->Command);\n\t\tbreak;\n\t}\n\tcase SMB_DIRECT_MSG_NEGOTIATE_REQ: {\n\t\tstruct smb_direct_negotiate_req *req =\n\t\t\t(struct smb_direct_negotiate_req *)recvmsg->packet;\n\t\tksmbd_debug(RDMA,\n\t\t\t    \"MinVersion: %u, MaxVersion: %u, CreditRequested: %u, MaxSendSize: %u, MaxRecvSize: %u, MaxFragmentedSize: %u\\n\",\n\t\t\t    le16_to_cpu(req->min_version),\n\t\t\t    le16_to_cpu(req->max_version),\n\t\t\t    le16_to_cpu(req->credits_requested),\n\t\t\t    le32_to_cpu(req->preferred_send_size),\n\t\t\t    le32_to_cpu(req->max_receive_size),\n\t\t\t    le32_to_cpu(req->max_fragmented_size));\n\t\tif (le16_to_cpu(req->min_version) > 0x0100 ||\n\t\t    le16_to_cpu(req->max_version) < 0x0100)\n\t\t\treturn -EOPNOTSUPP;\n\t\tif (le16_to_cpu(req->credits_requested) <= 0 ||\n\t\t    le32_to_cpu(req->max_receive_size) <= 128 ||\n\t\t    le32_to_cpu(req->max_fragmented_size) <=\n\t\t\t\t\t128 * 1024)\n\t\t\treturn -ECONNABORTED;\n\n\t\tbreak;\n\t}\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}\n\nstatic void recv_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct smb_direct_recvmsg *recvmsg;\n\tstruct smb_direct_transport *t;\n\n\trecvmsg = container_of(wc->wr_cqe, struct smb_direct_recvmsg, cqe);\n\tt = recvmsg->transport;\n\n\tif (wc->status != IB_WC_SUCCESS || wc->opcode != IB_WC_RECV) {\n\t\tif (wc->status != IB_WC_WR_FLUSH_ERR) {\n\t\t\tpr_err(\"Recv error. status='%s (%d)' opcode=%d\\n\",\n\t\t\t       ib_wc_status_msg(wc->status), wc->status,\n\t\t\t       wc->opcode);\n\t\t\tsmb_direct_disconnect_rdma_connection(t);\n\t\t}\n\t\tput_empty_recvmsg(t, recvmsg);\n\t\treturn;\n\t}\n\n\tksmbd_debug(RDMA, \"Recv completed. status='%s (%d)', opcode=%d\\n\",\n\t\t    ib_wc_status_msg(wc->status), wc->status,\n\t\t    wc->opcode);\n\n\tib_dma_sync_single_for_cpu(wc->qp->device, recvmsg->sge.addr,\n\t\t\t\t   recvmsg->sge.length, DMA_FROM_DEVICE);\n\n\tswitch (recvmsg->type) {\n\tcase SMB_DIRECT_MSG_NEGOTIATE_REQ:\n\t\tif (wc->byte_len < sizeof(struct smb_direct_negotiate_req)) {\n\t\t\tput_empty_recvmsg(t, recvmsg);\n\t\t\treturn;\n\t\t}\n\t\tt->negotiation_requested = true;\n\t\tt->full_packet_received = true;\n\t\tt->status = SMB_DIRECT_CS_CONNECTED;\n\t\tenqueue_reassembly(t, recvmsg, 0);\n\t\twake_up_interruptible(&t->wait_status);\n\t\tbreak;\n\tcase SMB_DIRECT_MSG_DATA_TRANSFER: {\n\t\tstruct smb_direct_data_transfer *data_transfer =\n\t\t\t(struct smb_direct_data_transfer *)recvmsg->packet;\n\t\tunsigned int data_length;\n\t\tint avail_recvmsg_count, receive_credits;\n\n\t\tif (wc->byte_len <\n\t\t    offsetof(struct smb_direct_data_transfer, padding)) {\n\t\t\tput_empty_recvmsg(t, recvmsg);\n\t\t\treturn;\n\t\t}\n\n\t\tdata_length = le32_to_cpu(data_transfer->data_length);\n\t\tif (data_length) {\n\t\t\tif (wc->byte_len < sizeof(struct smb_direct_data_transfer) +\n\t\t\t    (u64)data_length) {\n\t\t\t\tput_empty_recvmsg(t, recvmsg);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tif (t->full_packet_received)\n\t\t\t\trecvmsg->first_segment = true;\n\n\t\t\tif (le32_to_cpu(data_transfer->remaining_data_length))\n\t\t\t\tt->full_packet_received = false;\n\t\t\telse\n\t\t\t\tt->full_packet_received = true;\n\n\t\t\tenqueue_reassembly(t, recvmsg, (int)data_length);\n\t\t\twake_up_interruptible(&t->wait_reassembly_queue);\n\n\t\t\tspin_lock(&t->receive_credit_lock);\n\t\t\treceive_credits = --(t->recv_credits);\n\t\t\tavail_recvmsg_count = t->count_avail_recvmsg;\n\t\t\tspin_unlock(&t->receive_credit_lock);\n\t\t} else {\n\t\t\tput_empty_recvmsg(t, recvmsg);\n\n\t\t\tspin_lock(&t->receive_credit_lock);\n\t\t\treceive_credits = --(t->recv_credits);\n\t\t\tavail_recvmsg_count = ++(t->count_avail_recvmsg);\n\t\t\tspin_unlock(&t->receive_credit_lock);\n\t\t}\n\n\t\tt->recv_credit_target =\n\t\t\t\tle16_to_cpu(data_transfer->credits_requested);\n\t\tatomic_add(le16_to_cpu(data_transfer->credits_granted),\n\t\t\t   &t->send_credits);\n\n\t\tif (le16_to_cpu(data_transfer->flags) &\n\t\t    SMB_DIRECT_RESPONSE_REQUESTED)\n\t\t\tqueue_work(smb_direct_wq, &t->send_immediate_work);\n\n\t\tif (atomic_read(&t->send_credits) > 0)\n\t\t\twake_up_interruptible(&t->wait_send_credits);\n\n\t\tif (is_receive_credit_post_required(receive_credits, avail_recvmsg_count))\n\t\t\tmod_delayed_work(smb_direct_wq,\n\t\t\t\t\t &t->post_recv_credits_work, 0);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic int smb_direct_post_recv(struct smb_direct_transport *t,\n\t\t\t\tstruct smb_direct_recvmsg *recvmsg)\n{\n\tstruct ib_recv_wr wr;\n\tint ret;\n\n\trecvmsg->sge.addr = ib_dma_map_single(t->cm_id->device,\n\t\t\t\t\t      recvmsg->packet, t->max_recv_size,\n\t\t\t\t\t      DMA_FROM_DEVICE);\n\tret = ib_dma_mapping_error(t->cm_id->device, recvmsg->sge.addr);\n\tif (ret)\n\t\treturn ret;\n\trecvmsg->sge.length = t->max_recv_size;\n\trecvmsg->sge.lkey = t->pd->local_dma_lkey;\n\trecvmsg->cqe.done = recv_done;\n\n\twr.wr_cqe = &recvmsg->cqe;\n\twr.next = NULL;\n\twr.sg_list = &recvmsg->sge;\n\twr.num_sge = 1;\n\n\tret = ib_post_recv(t->qp, &wr, NULL);\n\tif (ret) {\n\t\tpr_err(\"Can't post recv: %d\\n\", ret);\n\t\tib_dma_unmap_single(t->cm_id->device,\n\t\t\t\t    recvmsg->sge.addr, recvmsg->sge.length,\n\t\t\t\t    DMA_FROM_DEVICE);\n\t\tsmb_direct_disconnect_rdma_connection(t);\n\t\treturn ret;\n\t}\n\treturn ret;\n}\n\nstatic int smb_direct_read(struct ksmbd_transport *t, char *buf,\n\t\t\t   unsigned int size, int unused)\n{\n\tstruct smb_direct_recvmsg *recvmsg;\n\tstruct smb_direct_data_transfer *data_transfer;\n\tint to_copy, to_read, data_read, offset;\n\tu32 data_length, remaining_data_length, data_offset;\n\tint rc;\n\tstruct smb_direct_transport *st = smb_trans_direct_transfort(t);\n\nagain:\n\tif (st->status != SMB_DIRECT_CS_CONNECTED) {\n\t\tpr_err(\"disconnected\\n\");\n\t\treturn -ENOTCONN;\n\t}\n\n\t \n\tif (st->reassembly_data_length >= size) {\n\t\tint queue_length;\n\t\tint queue_removed = 0;\n\n\t\t \n\t\tvirt_rmb();\n\t\tqueue_length = st->reassembly_queue_length;\n\t\tdata_read = 0;\n\t\tto_read = size;\n\t\toffset = st->first_entry_offset;\n\t\twhile (data_read < size) {\n\t\t\trecvmsg = get_first_reassembly(st);\n\t\t\tdata_transfer = smb_direct_recvmsg_payload(recvmsg);\n\t\t\tdata_length = le32_to_cpu(data_transfer->data_length);\n\t\t\tremaining_data_length =\n\t\t\t\tle32_to_cpu(data_transfer->remaining_data_length);\n\t\t\tdata_offset = le32_to_cpu(data_transfer->data_offset);\n\n\t\t\t \n\t\t\tif (recvmsg->first_segment && size == 4) {\n\t\t\t\tunsigned int rfc1002_len =\n\t\t\t\t\tdata_length + remaining_data_length;\n\t\t\t\t*((__be32 *)buf) = cpu_to_be32(rfc1002_len);\n\t\t\t\tdata_read = 4;\n\t\t\t\trecvmsg->first_segment = false;\n\t\t\t\tksmbd_debug(RDMA,\n\t\t\t\t\t    \"returning rfc1002 length %d\\n\",\n\t\t\t\t\t    rfc1002_len);\n\t\t\t\tgoto read_rfc1002_done;\n\t\t\t}\n\n\t\t\tto_copy = min_t(int, data_length - offset, to_read);\n\t\t\tmemcpy(buf + data_read, (char *)data_transfer + data_offset + offset,\n\t\t\t       to_copy);\n\n\t\t\t \n\t\t\tif (to_copy == data_length - offset) {\n\t\t\t\tqueue_length--;\n\t\t\t\t \n\t\t\t\tif (queue_length) {\n\t\t\t\t\tlist_del(&recvmsg->list);\n\t\t\t\t} else {\n\t\t\t\t\tspin_lock_irq(&st->reassembly_queue_lock);\n\t\t\t\t\tlist_del(&recvmsg->list);\n\t\t\t\t\tspin_unlock_irq(&st->reassembly_queue_lock);\n\t\t\t\t}\n\t\t\t\tqueue_removed++;\n\t\t\t\tput_recvmsg(st, recvmsg);\n\t\t\t\toffset = 0;\n\t\t\t} else {\n\t\t\t\toffset += to_copy;\n\t\t\t}\n\n\t\t\tto_read -= to_copy;\n\t\t\tdata_read += to_copy;\n\t\t}\n\n\t\tspin_lock_irq(&st->reassembly_queue_lock);\n\t\tst->reassembly_data_length -= data_read;\n\t\tst->reassembly_queue_length -= queue_removed;\n\t\tspin_unlock_irq(&st->reassembly_queue_lock);\n\n\t\tspin_lock(&st->receive_credit_lock);\n\t\tst->count_avail_recvmsg += queue_removed;\n\t\tif (is_receive_credit_post_required(st->recv_credits, st->count_avail_recvmsg)) {\n\t\t\tspin_unlock(&st->receive_credit_lock);\n\t\t\tmod_delayed_work(smb_direct_wq,\n\t\t\t\t\t &st->post_recv_credits_work, 0);\n\t\t} else {\n\t\t\tspin_unlock(&st->receive_credit_lock);\n\t\t}\n\n\t\tst->first_entry_offset = offset;\n\t\tksmbd_debug(RDMA,\n\t\t\t    \"returning to thread data_read=%d reassembly_data_length=%d first_entry_offset=%d\\n\",\n\t\t\t    data_read, st->reassembly_data_length,\n\t\t\t    st->first_entry_offset);\nread_rfc1002_done:\n\t\treturn data_read;\n\t}\n\n\tksmbd_debug(RDMA, \"wait_event on more data\\n\");\n\trc = wait_event_interruptible(st->wait_reassembly_queue,\n\t\t\t\t      st->reassembly_data_length >= size ||\n\t\t\t\t       st->status != SMB_DIRECT_CS_CONNECTED);\n\tif (rc)\n\t\treturn -EINTR;\n\n\tgoto again;\n}\n\nstatic void smb_direct_post_recv_credits(struct work_struct *work)\n{\n\tstruct smb_direct_transport *t = container_of(work,\n\t\tstruct smb_direct_transport, post_recv_credits_work.work);\n\tstruct smb_direct_recvmsg *recvmsg;\n\tint receive_credits, credits = 0;\n\tint ret;\n\tint use_free = 1;\n\n\tspin_lock(&t->receive_credit_lock);\n\treceive_credits = t->recv_credits;\n\tspin_unlock(&t->receive_credit_lock);\n\n\tif (receive_credits < t->recv_credit_target) {\n\t\twhile (true) {\n\t\t\tif (use_free)\n\t\t\t\trecvmsg = get_free_recvmsg(t);\n\t\t\telse\n\t\t\t\trecvmsg = get_empty_recvmsg(t);\n\t\t\tif (!recvmsg) {\n\t\t\t\tif (use_free) {\n\t\t\t\t\tuse_free = 0;\n\t\t\t\t\tcontinue;\n\t\t\t\t} else {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\trecvmsg->type = SMB_DIRECT_MSG_DATA_TRANSFER;\n\t\t\trecvmsg->first_segment = false;\n\n\t\t\tret = smb_direct_post_recv(t, recvmsg);\n\t\t\tif (ret) {\n\t\t\t\tpr_err(\"Can't post recv: %d\\n\", ret);\n\t\t\t\tput_recvmsg(t, recvmsg);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcredits++;\n\t\t}\n\t}\n\n\tspin_lock(&t->receive_credit_lock);\n\tt->recv_credits += credits;\n\tt->count_avail_recvmsg -= credits;\n\tspin_unlock(&t->receive_credit_lock);\n\n\tspin_lock(&t->lock_new_recv_credits);\n\tt->new_recv_credits += credits;\n\tspin_unlock(&t->lock_new_recv_credits);\n\n\tif (credits)\n\t\tqueue_work(smb_direct_wq, &t->send_immediate_work);\n}\n\nstatic void send_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct smb_direct_sendmsg *sendmsg, *sibling;\n\tstruct smb_direct_transport *t;\n\tstruct list_head *pos, *prev, *end;\n\n\tsendmsg = container_of(wc->wr_cqe, struct smb_direct_sendmsg, cqe);\n\tt = sendmsg->transport;\n\n\tksmbd_debug(RDMA, \"Send completed. status='%s (%d)', opcode=%d\\n\",\n\t\t    ib_wc_status_msg(wc->status), wc->status,\n\t\t    wc->opcode);\n\n\tif (wc->status != IB_WC_SUCCESS || wc->opcode != IB_WC_SEND) {\n\t\tpr_err(\"Send error. status='%s (%d)', opcode=%d\\n\",\n\t\t       ib_wc_status_msg(wc->status), wc->status,\n\t\t       wc->opcode);\n\t\tsmb_direct_disconnect_rdma_connection(t);\n\t}\n\n\tif (atomic_dec_and_test(&t->send_pending))\n\t\twake_up(&t->wait_send_pending);\n\n\t \n\tfor (pos = &sendmsg->list, prev = pos->prev, end = sendmsg->list.next;\n\t     prev != end; pos = prev, prev = prev->prev) {\n\t\tsibling = container_of(pos, struct smb_direct_sendmsg, list);\n\t\tsmb_direct_free_sendmsg(t, sibling);\n\t}\n\n\tsibling = container_of(pos, struct smb_direct_sendmsg, list);\n\tsmb_direct_free_sendmsg(t, sibling);\n}\n\nstatic int manage_credits_prior_sending(struct smb_direct_transport *t)\n{\n\tint new_credits;\n\n\tspin_lock(&t->lock_new_recv_credits);\n\tnew_credits = t->new_recv_credits;\n\tt->new_recv_credits = 0;\n\tspin_unlock(&t->lock_new_recv_credits);\n\n\treturn new_credits;\n}\n\nstatic int smb_direct_post_send(struct smb_direct_transport *t,\n\t\t\t\tstruct ib_send_wr *wr)\n{\n\tint ret;\n\n\tatomic_inc(&t->send_pending);\n\tret = ib_post_send(t->qp, wr, NULL);\n\tif (ret) {\n\t\tpr_err(\"failed to post send: %d\\n\", ret);\n\t\tif (atomic_dec_and_test(&t->send_pending))\n\t\t\twake_up(&t->wait_send_pending);\n\t\tsmb_direct_disconnect_rdma_connection(t);\n\t}\n\treturn ret;\n}\n\nstatic void smb_direct_send_ctx_init(struct smb_direct_transport *t,\n\t\t\t\t     struct smb_direct_send_ctx *send_ctx,\n\t\t\t\t     bool need_invalidate_rkey,\n\t\t\t\t     unsigned int remote_key)\n{\n\tINIT_LIST_HEAD(&send_ctx->msg_list);\n\tsend_ctx->wr_cnt = 0;\n\tsend_ctx->need_invalidate_rkey = need_invalidate_rkey;\n\tsend_ctx->remote_key = remote_key;\n}\n\nstatic int smb_direct_flush_send_list(struct smb_direct_transport *t,\n\t\t\t\t      struct smb_direct_send_ctx *send_ctx,\n\t\t\t\t      bool is_last)\n{\n\tstruct smb_direct_sendmsg *first, *last;\n\tint ret;\n\n\tif (list_empty(&send_ctx->msg_list))\n\t\treturn 0;\n\n\tfirst = list_first_entry(&send_ctx->msg_list,\n\t\t\t\t struct smb_direct_sendmsg,\n\t\t\t\t list);\n\tlast = list_last_entry(&send_ctx->msg_list,\n\t\t\t       struct smb_direct_sendmsg,\n\t\t\t       list);\n\n\tlast->wr.send_flags = IB_SEND_SIGNALED;\n\tlast->wr.wr_cqe = &last->cqe;\n\tif (is_last && send_ctx->need_invalidate_rkey) {\n\t\tlast->wr.opcode = IB_WR_SEND_WITH_INV;\n\t\tlast->wr.ex.invalidate_rkey = send_ctx->remote_key;\n\t}\n\n\tret = smb_direct_post_send(t, &first->wr);\n\tif (!ret) {\n\t\tsmb_direct_send_ctx_init(t, send_ctx,\n\t\t\t\t\t send_ctx->need_invalidate_rkey,\n\t\t\t\t\t send_ctx->remote_key);\n\t} else {\n\t\tatomic_add(send_ctx->wr_cnt, &t->send_credits);\n\t\twake_up(&t->wait_send_credits);\n\t\tlist_for_each_entry_safe(first, last, &send_ctx->msg_list,\n\t\t\t\t\t list) {\n\t\t\tsmb_direct_free_sendmsg(t, first);\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic int wait_for_credits(struct smb_direct_transport *t,\n\t\t\t    wait_queue_head_t *waitq, atomic_t *total_credits,\n\t\t\t    int needed)\n{\n\tint ret;\n\n\tdo {\n\t\tif (atomic_sub_return(needed, total_credits) >= 0)\n\t\t\treturn 0;\n\n\t\tatomic_add(needed, total_credits);\n\t\tret = wait_event_interruptible(*waitq,\n\t\t\t\t\t       atomic_read(total_credits) >= needed ||\n\t\t\t\t\t       t->status != SMB_DIRECT_CS_CONNECTED);\n\n\t\tif (t->status != SMB_DIRECT_CS_CONNECTED)\n\t\t\treturn -ENOTCONN;\n\t\telse if (ret < 0)\n\t\t\treturn ret;\n\t} while (true);\n}\n\nstatic int wait_for_send_credits(struct smb_direct_transport *t,\n\t\t\t\t struct smb_direct_send_ctx *send_ctx)\n{\n\tint ret;\n\n\tif (send_ctx &&\n\t    (send_ctx->wr_cnt >= 16 || atomic_read(&t->send_credits) <= 1)) {\n\t\tret = smb_direct_flush_send_list(t, send_ctx, false);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn wait_for_credits(t, &t->wait_send_credits, &t->send_credits, 1);\n}\n\nstatic int wait_for_rw_credits(struct smb_direct_transport *t, int credits)\n{\n\treturn wait_for_credits(t, &t->wait_rw_credits, &t->rw_credits, credits);\n}\n\nstatic int calc_rw_credits(struct smb_direct_transport *t,\n\t\t\t   char *buf, unsigned int len)\n{\n\treturn DIV_ROUND_UP(get_buf_page_count(buf, len),\n\t\t\t    t->pages_per_rw_credit);\n}\n\nstatic int smb_direct_create_header(struct smb_direct_transport *t,\n\t\t\t\t    int size, int remaining_data_length,\n\t\t\t\t    struct smb_direct_sendmsg **sendmsg_out)\n{\n\tstruct smb_direct_sendmsg *sendmsg;\n\tstruct smb_direct_data_transfer *packet;\n\tint header_length;\n\tint ret;\n\n\tsendmsg = smb_direct_alloc_sendmsg(t);\n\tif (IS_ERR(sendmsg))\n\t\treturn PTR_ERR(sendmsg);\n\n\t \n\tpacket = (struct smb_direct_data_transfer *)sendmsg->packet;\n\tpacket->credits_requested = cpu_to_le16(t->send_credit_target);\n\tpacket->credits_granted = cpu_to_le16(manage_credits_prior_sending(t));\n\n\tpacket->flags = 0;\n\tpacket->reserved = 0;\n\tif (!size)\n\t\tpacket->data_offset = 0;\n\telse\n\t\tpacket->data_offset = cpu_to_le32(24);\n\tpacket->data_length = cpu_to_le32(size);\n\tpacket->remaining_data_length = cpu_to_le32(remaining_data_length);\n\tpacket->padding = 0;\n\n\tksmbd_debug(RDMA,\n\t\t    \"credits_requested=%d credits_granted=%d data_offset=%d data_length=%d remaining_data_length=%d\\n\",\n\t\t    le16_to_cpu(packet->credits_requested),\n\t\t    le16_to_cpu(packet->credits_granted),\n\t\t    le32_to_cpu(packet->data_offset),\n\t\t    le32_to_cpu(packet->data_length),\n\t\t    le32_to_cpu(packet->remaining_data_length));\n\n\t \n\theader_length = sizeof(struct smb_direct_data_transfer);\n\t \n\tif (!size)\n\t\theader_length =\n\t\t\toffsetof(struct smb_direct_data_transfer, padding);\n\n\tsendmsg->sge[0].addr = ib_dma_map_single(t->cm_id->device,\n\t\t\t\t\t\t (void *)packet,\n\t\t\t\t\t\t header_length,\n\t\t\t\t\t\t DMA_TO_DEVICE);\n\tret = ib_dma_mapping_error(t->cm_id->device, sendmsg->sge[0].addr);\n\tif (ret) {\n\t\tsmb_direct_free_sendmsg(t, sendmsg);\n\t\treturn ret;\n\t}\n\n\tsendmsg->num_sge = 1;\n\tsendmsg->sge[0].length = header_length;\n\tsendmsg->sge[0].lkey = t->pd->local_dma_lkey;\n\n\t*sendmsg_out = sendmsg;\n\treturn 0;\n}\n\nstatic int get_sg_list(void *buf, int size, struct scatterlist *sg_list, int nentries)\n{\n\tbool high = is_vmalloc_addr(buf);\n\tstruct page *page;\n\tint offset, len;\n\tint i = 0;\n\n\tif (size <= 0 || nentries < get_buf_page_count(buf, size))\n\t\treturn -EINVAL;\n\n\toffset = offset_in_page(buf);\n\tbuf -= offset;\n\twhile (size > 0) {\n\t\tlen = min_t(int, PAGE_SIZE - offset, size);\n\t\tif (high)\n\t\t\tpage = vmalloc_to_page(buf);\n\t\telse\n\t\t\tpage = kmap_to_page(buf);\n\n\t\tif (!sg_list)\n\t\t\treturn -EINVAL;\n\t\tsg_set_page(sg_list, page, len, offset);\n\t\tsg_list = sg_next(sg_list);\n\n\t\tbuf += PAGE_SIZE;\n\t\tsize -= len;\n\t\toffset = 0;\n\t\ti++;\n\t}\n\treturn i;\n}\n\nstatic int get_mapped_sg_list(struct ib_device *device, void *buf, int size,\n\t\t\t      struct scatterlist *sg_list, int nentries,\n\t\t\t      enum dma_data_direction dir)\n{\n\tint npages;\n\n\tnpages = get_sg_list(buf, size, sg_list, nentries);\n\tif (npages < 0)\n\t\treturn -EINVAL;\n\treturn ib_dma_map_sg(device, sg_list, npages, dir);\n}\n\nstatic int post_sendmsg(struct smb_direct_transport *t,\n\t\t\tstruct smb_direct_send_ctx *send_ctx,\n\t\t\tstruct smb_direct_sendmsg *msg)\n{\n\tint i;\n\n\tfor (i = 0; i < msg->num_sge; i++)\n\t\tib_dma_sync_single_for_device(t->cm_id->device,\n\t\t\t\t\t      msg->sge[i].addr, msg->sge[i].length,\n\t\t\t\t\t      DMA_TO_DEVICE);\n\n\tmsg->cqe.done = send_done;\n\tmsg->wr.opcode = IB_WR_SEND;\n\tmsg->wr.sg_list = &msg->sge[0];\n\tmsg->wr.num_sge = msg->num_sge;\n\tmsg->wr.next = NULL;\n\n\tif (send_ctx) {\n\t\tmsg->wr.wr_cqe = NULL;\n\t\tmsg->wr.send_flags = 0;\n\t\tif (!list_empty(&send_ctx->msg_list)) {\n\t\t\tstruct smb_direct_sendmsg *last;\n\n\t\t\tlast = list_last_entry(&send_ctx->msg_list,\n\t\t\t\t\t       struct smb_direct_sendmsg,\n\t\t\t\t\t       list);\n\t\t\tlast->wr.next = &msg->wr;\n\t\t}\n\t\tlist_add_tail(&msg->list, &send_ctx->msg_list);\n\t\tsend_ctx->wr_cnt++;\n\t\treturn 0;\n\t}\n\n\tmsg->wr.wr_cqe = &msg->cqe;\n\tmsg->wr.send_flags = IB_SEND_SIGNALED;\n\treturn smb_direct_post_send(t, &msg->wr);\n}\n\nstatic int smb_direct_post_send_data(struct smb_direct_transport *t,\n\t\t\t\t     struct smb_direct_send_ctx *send_ctx,\n\t\t\t\t     struct kvec *iov, int niov,\n\t\t\t\t     int remaining_data_length)\n{\n\tint i, j, ret;\n\tstruct smb_direct_sendmsg *msg;\n\tint data_length;\n\tstruct scatterlist sg[SMB_DIRECT_MAX_SEND_SGES - 1];\n\n\tret = wait_for_send_credits(t, send_ctx);\n\tif (ret)\n\t\treturn ret;\n\n\tdata_length = 0;\n\tfor (i = 0; i < niov; i++)\n\t\tdata_length += iov[i].iov_len;\n\n\tret = smb_direct_create_header(t, data_length, remaining_data_length,\n\t\t\t\t       &msg);\n\tif (ret) {\n\t\tatomic_inc(&t->send_credits);\n\t\treturn ret;\n\t}\n\n\tfor (i = 0; i < niov; i++) {\n\t\tstruct ib_sge *sge;\n\t\tint sg_cnt;\n\n\t\tsg_init_table(sg, SMB_DIRECT_MAX_SEND_SGES - 1);\n\t\tsg_cnt = get_mapped_sg_list(t->cm_id->device,\n\t\t\t\t\t    iov[i].iov_base, iov[i].iov_len,\n\t\t\t\t\t    sg, SMB_DIRECT_MAX_SEND_SGES - 1,\n\t\t\t\t\t    DMA_TO_DEVICE);\n\t\tif (sg_cnt <= 0) {\n\t\t\tpr_err(\"failed to map buffer\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto err;\n\t\t} else if (sg_cnt + msg->num_sge > SMB_DIRECT_MAX_SEND_SGES) {\n\t\t\tpr_err(\"buffer not fitted into sges\\n\");\n\t\t\tret = -E2BIG;\n\t\t\tib_dma_unmap_sg(t->cm_id->device, sg, sg_cnt,\n\t\t\t\t\tDMA_TO_DEVICE);\n\t\t\tgoto err;\n\t\t}\n\n\t\tfor (j = 0; j < sg_cnt; j++) {\n\t\t\tsge = &msg->sge[msg->num_sge];\n\t\t\tsge->addr = sg_dma_address(&sg[j]);\n\t\t\tsge->length = sg_dma_len(&sg[j]);\n\t\t\tsge->lkey  = t->pd->local_dma_lkey;\n\t\t\tmsg->num_sge++;\n\t\t}\n\t}\n\n\tret = post_sendmsg(t, send_ctx, msg);\n\tif (ret)\n\t\tgoto err;\n\treturn 0;\nerr:\n\tsmb_direct_free_sendmsg(t, msg);\n\tatomic_inc(&t->send_credits);\n\treturn ret;\n}\n\nstatic int smb_direct_writev(struct ksmbd_transport *t,\n\t\t\t     struct kvec *iov, int niovs, int buflen,\n\t\t\t     bool need_invalidate, unsigned int remote_key)\n{\n\tstruct smb_direct_transport *st = smb_trans_direct_transfort(t);\n\tint remaining_data_length;\n\tint start, i, j;\n\tint max_iov_size = st->max_send_size -\n\t\t\tsizeof(struct smb_direct_data_transfer);\n\tint ret;\n\tstruct kvec vec;\n\tstruct smb_direct_send_ctx send_ctx;\n\n\tif (st->status != SMB_DIRECT_CS_CONNECTED)\n\t\treturn -ENOTCONN;\n\n\t\n\tbuflen -= 4;\n\n\tremaining_data_length = buflen;\n\tksmbd_debug(RDMA, \"Sending smb (RDMA): smb_len=%u\\n\", buflen);\n\n\tsmb_direct_send_ctx_init(st, &send_ctx, need_invalidate, remote_key);\n\tstart = i = 1;\n\tbuflen = 0;\n\twhile (true) {\n\t\tbuflen += iov[i].iov_len;\n\t\tif (buflen > max_iov_size) {\n\t\t\tif (i > start) {\n\t\t\t\tremaining_data_length -=\n\t\t\t\t\t(buflen - iov[i].iov_len);\n\t\t\t\tret = smb_direct_post_send_data(st, &send_ctx,\n\t\t\t\t\t\t\t\t&iov[start], i - start,\n\t\t\t\t\t\t\t\tremaining_data_length);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto done;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t\tint nvec  = (buflen + max_iov_size - 1) /\n\t\t\t\t\t\tmax_iov_size;\n\n\t\t\t\tfor (j = 0; j < nvec; j++) {\n\t\t\t\t\tvec.iov_base =\n\t\t\t\t\t\t(char *)iov[start].iov_base +\n\t\t\t\t\t\tj * max_iov_size;\n\t\t\t\t\tvec.iov_len =\n\t\t\t\t\t\tmin_t(int, max_iov_size,\n\t\t\t\t\t\t      buflen - max_iov_size * j);\n\t\t\t\t\tremaining_data_length -= vec.iov_len;\n\t\t\t\t\tret = smb_direct_post_send_data(st, &send_ctx, &vec, 1,\n\t\t\t\t\t\t\t\t\tremaining_data_length);\n\t\t\t\t\tif (ret)\n\t\t\t\t\t\tgoto done;\n\t\t\t\t}\n\t\t\t\ti++;\n\t\t\t\tif (i == niovs)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tstart = i;\n\t\t\tbuflen = 0;\n\t\t} else {\n\t\t\ti++;\n\t\t\tif (i == niovs) {\n\t\t\t\t \n\t\t\t\tremaining_data_length -= buflen;\n\t\t\t\tret = smb_direct_post_send_data(st, &send_ctx,\n\t\t\t\t\t\t\t\t&iov[start], i - start,\n\t\t\t\t\t\t\t\tremaining_data_length);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto done;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\ndone:\n\tret = smb_direct_flush_send_list(st, &send_ctx, true);\n\n\t \n\n\twait_event(st->wait_send_pending,\n\t\t   atomic_read(&st->send_pending) == 0);\n\treturn ret;\n}\n\nstatic void smb_direct_free_rdma_rw_msg(struct smb_direct_transport *t,\n\t\t\t\t\tstruct smb_direct_rdma_rw_msg *msg,\n\t\t\t\t\tenum dma_data_direction dir)\n{\n\trdma_rw_ctx_destroy(&msg->rw_ctx, t->qp, t->qp->port,\n\t\t\t    msg->sgt.sgl, msg->sgt.nents, dir);\n\tsg_free_table_chained(&msg->sgt, SG_CHUNK_SIZE);\n\tkfree(msg);\n}\n\nstatic void read_write_done(struct ib_cq *cq, struct ib_wc *wc,\n\t\t\t    enum dma_data_direction dir)\n{\n\tstruct smb_direct_rdma_rw_msg *msg = container_of(wc->wr_cqe,\n\t\t\t\t\t\t\t  struct smb_direct_rdma_rw_msg, cqe);\n\tstruct smb_direct_transport *t = msg->t;\n\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\tmsg->status = -EIO;\n\t\tpr_err(\"read/write error. opcode = %d, status = %s(%d)\\n\",\n\t\t       wc->opcode, ib_wc_status_msg(wc->status), wc->status);\n\t\tif (wc->status != IB_WC_WR_FLUSH_ERR)\n\t\t\tsmb_direct_disconnect_rdma_connection(t);\n\t}\n\n\tcomplete(msg->completion);\n}\n\nstatic void read_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tread_write_done(cq, wc, DMA_FROM_DEVICE);\n}\n\nstatic void write_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tread_write_done(cq, wc, DMA_TO_DEVICE);\n}\n\nstatic int smb_direct_rdma_xmit(struct smb_direct_transport *t,\n\t\t\t\tvoid *buf, int buf_len,\n\t\t\t\tstruct smb2_buffer_desc_v1 *desc,\n\t\t\t\tunsigned int desc_len,\n\t\t\t\tbool is_read)\n{\n\tstruct smb_direct_rdma_rw_msg *msg, *next_msg;\n\tint i, ret;\n\tDECLARE_COMPLETION_ONSTACK(completion);\n\tstruct ib_send_wr *first_wr;\n\tLIST_HEAD(msg_list);\n\tchar *desc_buf;\n\tint credits_needed;\n\tunsigned int desc_buf_len, desc_num = 0;\n\n\tif (t->status != SMB_DIRECT_CS_CONNECTED)\n\t\treturn -ENOTCONN;\n\n\tif (buf_len > t->max_rdma_rw_size)\n\t\treturn -EINVAL;\n\n\t \n\tcredits_needed = 0;\n\tdesc_buf = buf;\n\tfor (i = 0; i < desc_len / sizeof(*desc); i++) {\n\t\tif (!buf_len)\n\t\t\tbreak;\n\n\t\tdesc_buf_len = le32_to_cpu(desc[i].length);\n\t\tif (!desc_buf_len)\n\t\t\treturn -EINVAL;\n\n\t\tif (desc_buf_len > buf_len) {\n\t\t\tdesc_buf_len = buf_len;\n\t\t\tdesc[i].length = cpu_to_le32(desc_buf_len);\n\t\t\tbuf_len = 0;\n\t\t}\n\n\t\tcredits_needed += calc_rw_credits(t, desc_buf, desc_buf_len);\n\t\tdesc_buf += desc_buf_len;\n\t\tbuf_len -= desc_buf_len;\n\t\tdesc_num++;\n\t}\n\n\tksmbd_debug(RDMA, \"RDMA %s, len %#x, needed credits %#x\\n\",\n\t\t    is_read ? \"read\" : \"write\", buf_len, credits_needed);\n\n\tret = wait_for_rw_credits(t, credits_needed);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t \n\tdesc_buf = buf;\n\tfor (i = 0; i < desc_num; i++) {\n\t\tmsg = kzalloc(offsetof(struct smb_direct_rdma_rw_msg, sg_list) +\n\t\t\t      sizeof(struct scatterlist) * SG_CHUNK_SIZE, GFP_KERNEL);\n\t\tif (!msg) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tdesc_buf_len = le32_to_cpu(desc[i].length);\n\n\t\tmsg->t = t;\n\t\tmsg->cqe.done = is_read ? read_done : write_done;\n\t\tmsg->completion = &completion;\n\n\t\tmsg->sgt.sgl = &msg->sg_list[0];\n\t\tret = sg_alloc_table_chained(&msg->sgt,\n\t\t\t\t\t     get_buf_page_count(desc_buf, desc_buf_len),\n\t\t\t\t\t     msg->sg_list, SG_CHUNK_SIZE);\n\t\tif (ret) {\n\t\t\tkfree(msg);\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = get_sg_list(desc_buf, desc_buf_len,\n\t\t\t\t  msg->sgt.sgl, msg->sgt.orig_nents);\n\t\tif (ret < 0) {\n\t\t\tsg_free_table_chained(&msg->sgt, SG_CHUNK_SIZE);\n\t\t\tkfree(msg);\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = rdma_rw_ctx_init(&msg->rw_ctx, t->qp, t->qp->port,\n\t\t\t\t       msg->sgt.sgl,\n\t\t\t\t       get_buf_page_count(desc_buf, desc_buf_len),\n\t\t\t\t       0,\n\t\t\t\t       le64_to_cpu(desc[i].offset),\n\t\t\t\t       le32_to_cpu(desc[i].token),\n\t\t\t\t       is_read ? DMA_FROM_DEVICE : DMA_TO_DEVICE);\n\t\tif (ret < 0) {\n\t\t\tpr_err(\"failed to init rdma_rw_ctx: %d\\n\", ret);\n\t\t\tsg_free_table_chained(&msg->sgt, SG_CHUNK_SIZE);\n\t\t\tkfree(msg);\n\t\t\tgoto out;\n\t\t}\n\n\t\tlist_add_tail(&msg->list, &msg_list);\n\t\tdesc_buf += desc_buf_len;\n\t}\n\n\t \n\tfirst_wr = NULL;\n\tlist_for_each_entry_reverse(msg, &msg_list, list) {\n\t\tfirst_wr = rdma_rw_ctx_wrs(&msg->rw_ctx, t->qp, t->qp->port,\n\t\t\t\t\t   &msg->cqe, first_wr);\n\t}\n\n\tret = ib_post_send(t->qp, first_wr, NULL);\n\tif (ret) {\n\t\tpr_err(\"failed to post send wr for RDMA R/W: %d\\n\", ret);\n\t\tgoto out;\n\t}\n\n\tmsg = list_last_entry(&msg_list, struct smb_direct_rdma_rw_msg, list);\n\twait_for_completion(&completion);\n\tret = msg->status;\nout:\n\tlist_for_each_entry_safe(msg, next_msg, &msg_list, list) {\n\t\tlist_del(&msg->list);\n\t\tsmb_direct_free_rdma_rw_msg(t, msg,\n\t\t\t\t\t    is_read ? DMA_FROM_DEVICE : DMA_TO_DEVICE);\n\t}\n\tatomic_add(credits_needed, &t->rw_credits);\n\twake_up(&t->wait_rw_credits);\n\treturn ret;\n}\n\nstatic int smb_direct_rdma_write(struct ksmbd_transport *t,\n\t\t\t\t void *buf, unsigned int buflen,\n\t\t\t\t struct smb2_buffer_desc_v1 *desc,\n\t\t\t\t unsigned int desc_len)\n{\n\treturn smb_direct_rdma_xmit(smb_trans_direct_transfort(t), buf, buflen,\n\t\t\t\t    desc, desc_len, false);\n}\n\nstatic int smb_direct_rdma_read(struct ksmbd_transport *t,\n\t\t\t\tvoid *buf, unsigned int buflen,\n\t\t\t\tstruct smb2_buffer_desc_v1 *desc,\n\t\t\t\tunsigned int desc_len)\n{\n\treturn smb_direct_rdma_xmit(smb_trans_direct_transfort(t), buf, buflen,\n\t\t\t\t    desc, desc_len, true);\n}\n\nstatic void smb_direct_disconnect(struct ksmbd_transport *t)\n{\n\tstruct smb_direct_transport *st = smb_trans_direct_transfort(t);\n\n\tksmbd_debug(RDMA, \"Disconnecting cm_id=%p\\n\", st->cm_id);\n\n\tsmb_direct_disconnect_rdma_work(&st->disconnect_work);\n\twait_event_interruptible(st->wait_status,\n\t\t\t\t st->status == SMB_DIRECT_CS_DISCONNECTED);\n\tfree_transport(st);\n}\n\nstatic void smb_direct_shutdown(struct ksmbd_transport *t)\n{\n\tstruct smb_direct_transport *st = smb_trans_direct_transfort(t);\n\n\tksmbd_debug(RDMA, \"smb-direct shutdown cm_id=%p\\n\", st->cm_id);\n\n\tsmb_direct_disconnect_rdma_work(&st->disconnect_work);\n}\n\nstatic int smb_direct_cm_handler(struct rdma_cm_id *cm_id,\n\t\t\t\t struct rdma_cm_event *event)\n{\n\tstruct smb_direct_transport *t = cm_id->context;\n\n\tksmbd_debug(RDMA, \"RDMA CM event. cm_id=%p event=%s (%d)\\n\",\n\t\t    cm_id, rdma_event_msg(event->event), event->event);\n\n\tswitch (event->event) {\n\tcase RDMA_CM_EVENT_ESTABLISHED: {\n\t\tt->status = SMB_DIRECT_CS_CONNECTED;\n\t\twake_up_interruptible(&t->wait_status);\n\t\tbreak;\n\t}\n\tcase RDMA_CM_EVENT_DEVICE_REMOVAL:\n\tcase RDMA_CM_EVENT_DISCONNECTED: {\n\t\tib_drain_qp(t->qp);\n\n\t\tt->status = SMB_DIRECT_CS_DISCONNECTED;\n\t\twake_up_interruptible(&t->wait_status);\n\t\twake_up_interruptible(&t->wait_reassembly_queue);\n\t\twake_up(&t->wait_send_credits);\n\t\tbreak;\n\t}\n\tcase RDMA_CM_EVENT_CONNECT_ERROR: {\n\t\tt->status = SMB_DIRECT_CS_DISCONNECTED;\n\t\twake_up_interruptible(&t->wait_status);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tpr_err(\"Unexpected RDMA CM event. cm_id=%p, event=%s (%d)\\n\",\n\t\t       cm_id, rdma_event_msg(event->event),\n\t\t       event->event);\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic void smb_direct_qpair_handler(struct ib_event *event, void *context)\n{\n\tstruct smb_direct_transport *t = context;\n\n\tksmbd_debug(RDMA, \"Received QP event. cm_id=%p, event=%s (%d)\\n\",\n\t\t    t->cm_id, ib_event_msg(event->event), event->event);\n\n\tswitch (event->event) {\n\tcase IB_EVENT_CQ_ERR:\n\tcase IB_EVENT_QP_FATAL:\n\t\tsmb_direct_disconnect_rdma_connection(t);\n\t\tbreak;\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic int smb_direct_send_negotiate_response(struct smb_direct_transport *t,\n\t\t\t\t\t      int failed)\n{\n\tstruct smb_direct_sendmsg *sendmsg;\n\tstruct smb_direct_negotiate_resp *resp;\n\tint ret;\n\n\tsendmsg = smb_direct_alloc_sendmsg(t);\n\tif (IS_ERR(sendmsg))\n\t\treturn -ENOMEM;\n\n\tresp = (struct smb_direct_negotiate_resp *)sendmsg->packet;\n\tif (failed) {\n\t\tmemset(resp, 0, sizeof(*resp));\n\t\tresp->min_version = cpu_to_le16(0x0100);\n\t\tresp->max_version = cpu_to_le16(0x0100);\n\t\tresp->status = STATUS_NOT_SUPPORTED;\n\t} else {\n\t\tresp->status = STATUS_SUCCESS;\n\t\tresp->min_version = SMB_DIRECT_VERSION_LE;\n\t\tresp->max_version = SMB_DIRECT_VERSION_LE;\n\t\tresp->negotiated_version = SMB_DIRECT_VERSION_LE;\n\t\tresp->reserved = 0;\n\t\tresp->credits_requested =\n\t\t\t\tcpu_to_le16(t->send_credit_target);\n\t\tresp->credits_granted = cpu_to_le16(manage_credits_prior_sending(t));\n\t\tresp->max_readwrite_size = cpu_to_le32(t->max_rdma_rw_size);\n\t\tresp->preferred_send_size = cpu_to_le32(t->max_send_size);\n\t\tresp->max_receive_size = cpu_to_le32(t->max_recv_size);\n\t\tresp->max_fragmented_size =\n\t\t\t\tcpu_to_le32(t->max_fragmented_recv_size);\n\t}\n\n\tsendmsg->sge[0].addr = ib_dma_map_single(t->cm_id->device,\n\t\t\t\t\t\t (void *)resp, sizeof(*resp),\n\t\t\t\t\t\t DMA_TO_DEVICE);\n\tret = ib_dma_mapping_error(t->cm_id->device, sendmsg->sge[0].addr);\n\tif (ret) {\n\t\tsmb_direct_free_sendmsg(t, sendmsg);\n\t\treturn ret;\n\t}\n\n\tsendmsg->num_sge = 1;\n\tsendmsg->sge[0].length = sizeof(*resp);\n\tsendmsg->sge[0].lkey = t->pd->local_dma_lkey;\n\n\tret = post_sendmsg(t, NULL, sendmsg);\n\tif (ret) {\n\t\tsmb_direct_free_sendmsg(t, sendmsg);\n\t\treturn ret;\n\t}\n\n\twait_event(t->wait_send_pending,\n\t\t   atomic_read(&t->send_pending) == 0);\n\treturn 0;\n}\n\nstatic int smb_direct_accept_client(struct smb_direct_transport *t)\n{\n\tstruct rdma_conn_param conn_param;\n\tstruct ib_port_immutable port_immutable;\n\tu32 ird_ord_hdr[2];\n\tint ret;\n\n\tmemset(&conn_param, 0, sizeof(conn_param));\n\tconn_param.initiator_depth = min_t(u8, t->cm_id->device->attrs.max_qp_rd_atom,\n\t\t\t\t\t   SMB_DIRECT_CM_INITIATOR_DEPTH);\n\tconn_param.responder_resources = 0;\n\n\tt->cm_id->device->ops.get_port_immutable(t->cm_id->device,\n\t\t\t\t\t\t t->cm_id->port_num,\n\t\t\t\t\t\t &port_immutable);\n\tif (port_immutable.core_cap_flags & RDMA_CORE_PORT_IWARP) {\n\t\tird_ord_hdr[0] = conn_param.responder_resources;\n\t\tird_ord_hdr[1] = 1;\n\t\tconn_param.private_data = ird_ord_hdr;\n\t\tconn_param.private_data_len = sizeof(ird_ord_hdr);\n\t} else {\n\t\tconn_param.private_data = NULL;\n\t\tconn_param.private_data_len = 0;\n\t}\n\tconn_param.retry_count = SMB_DIRECT_CM_RETRY;\n\tconn_param.rnr_retry_count = SMB_DIRECT_CM_RNR_RETRY;\n\tconn_param.flow_control = 0;\n\n\tret = rdma_accept(t->cm_id, &conn_param);\n\tif (ret) {\n\t\tpr_err(\"error at rdma_accept: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic int smb_direct_prepare_negotiation(struct smb_direct_transport *t)\n{\n\tint ret;\n\tstruct smb_direct_recvmsg *recvmsg;\n\n\trecvmsg = get_free_recvmsg(t);\n\tif (!recvmsg)\n\t\treturn -ENOMEM;\n\trecvmsg->type = SMB_DIRECT_MSG_NEGOTIATE_REQ;\n\n\tret = smb_direct_post_recv(t, recvmsg);\n\tif (ret) {\n\t\tpr_err(\"Can't post recv: %d\\n\", ret);\n\t\tgoto out_err;\n\t}\n\n\tt->negotiation_requested = false;\n\tret = smb_direct_accept_client(t);\n\tif (ret) {\n\t\tpr_err(\"Can't accept client\\n\");\n\t\tgoto out_err;\n\t}\n\n\tsmb_direct_post_recv_credits(&t->post_recv_credits_work.work);\n\treturn 0;\nout_err:\n\tput_recvmsg(t, recvmsg);\n\treturn ret;\n}\n\nstatic unsigned int smb_direct_get_max_fr_pages(struct smb_direct_transport *t)\n{\n\treturn min_t(unsigned int,\n\t\t     t->cm_id->device->attrs.max_fast_reg_page_list_len,\n\t\t     256);\n}\n\nstatic int smb_direct_init_params(struct smb_direct_transport *t,\n\t\t\t\t  struct ib_qp_cap *cap)\n{\n\tstruct ib_device *device = t->cm_id->device;\n\tint max_send_sges, max_rw_wrs, max_send_wrs;\n\tunsigned int max_sge_per_wr, wrs_per_credit;\n\n\t \n\tt->max_send_size = smb_direct_max_send_size;\n\tmax_send_sges = DIV_ROUND_UP(t->max_send_size, PAGE_SIZE) + 3;\n\tif (max_send_sges > SMB_DIRECT_MAX_SEND_SGES) {\n\t\tpr_err(\"max_send_size %d is too large\\n\", t->max_send_size);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tt->max_rdma_rw_size = smb_direct_max_read_write_size;\n\tt->pages_per_rw_credit = smb_direct_get_max_fr_pages(t);\n\tt->max_rw_credits = DIV_ROUND_UP(t->max_rdma_rw_size,\n\t\t\t\t\t (t->pages_per_rw_credit - 1) *\n\t\t\t\t\t PAGE_SIZE);\n\n\tmax_sge_per_wr = min_t(unsigned int, device->attrs.max_send_sge,\n\t\t\t       device->attrs.max_sge_rd);\n\tmax_sge_per_wr = max_t(unsigned int, max_sge_per_wr,\n\t\t\t       max_send_sges);\n\twrs_per_credit = max_t(unsigned int, 4,\n\t\t\t       DIV_ROUND_UP(t->pages_per_rw_credit,\n\t\t\t\t\t    max_sge_per_wr) + 1);\n\tmax_rw_wrs = t->max_rw_credits * wrs_per_credit;\n\n\tmax_send_wrs = smb_direct_send_credit_target + max_rw_wrs;\n\tif (max_send_wrs > device->attrs.max_cqe ||\n\t    max_send_wrs > device->attrs.max_qp_wr) {\n\t\tpr_err(\"consider lowering send_credit_target = %d\\n\",\n\t\t       smb_direct_send_credit_target);\n\t\tpr_err(\"Possible CQE overrun, device reporting max_cqe %d max_qp_wr %d\\n\",\n\t\t       device->attrs.max_cqe, device->attrs.max_qp_wr);\n\t\treturn -EINVAL;\n\t}\n\n\tif (smb_direct_receive_credit_max > device->attrs.max_cqe ||\n\t    smb_direct_receive_credit_max > device->attrs.max_qp_wr) {\n\t\tpr_err(\"consider lowering receive_credit_max = %d\\n\",\n\t\t       smb_direct_receive_credit_max);\n\t\tpr_err(\"Possible CQE overrun, device reporting max_cpe %d max_qp_wr %d\\n\",\n\t\t       device->attrs.max_cqe, device->attrs.max_qp_wr);\n\t\treturn -EINVAL;\n\t}\n\n\tif (device->attrs.max_recv_sge < SMB_DIRECT_MAX_RECV_SGES) {\n\t\tpr_err(\"warning: device max_recv_sge = %d too small\\n\",\n\t\t       device->attrs.max_recv_sge);\n\t\treturn -EINVAL;\n\t}\n\n\tt->recv_credits = 0;\n\tt->count_avail_recvmsg = 0;\n\n\tt->recv_credit_max = smb_direct_receive_credit_max;\n\tt->recv_credit_target = 10;\n\tt->new_recv_credits = 0;\n\n\tt->send_credit_target = smb_direct_send_credit_target;\n\tatomic_set(&t->send_credits, 0);\n\tatomic_set(&t->rw_credits, t->max_rw_credits);\n\n\tt->max_send_size = smb_direct_max_send_size;\n\tt->max_recv_size = smb_direct_max_receive_size;\n\tt->max_fragmented_recv_size = smb_direct_max_fragmented_recv_size;\n\n\tcap->max_send_wr = max_send_wrs;\n\tcap->max_recv_wr = t->recv_credit_max;\n\tcap->max_send_sge = max_sge_per_wr;\n\tcap->max_recv_sge = SMB_DIRECT_MAX_RECV_SGES;\n\tcap->max_inline_data = 0;\n\tcap->max_rdma_ctxs = t->max_rw_credits;\n\treturn 0;\n}\n\nstatic void smb_direct_destroy_pools(struct smb_direct_transport *t)\n{\n\tstruct smb_direct_recvmsg *recvmsg;\n\n\twhile ((recvmsg = get_free_recvmsg(t)))\n\t\tmempool_free(recvmsg, t->recvmsg_mempool);\n\twhile ((recvmsg = get_empty_recvmsg(t)))\n\t\tmempool_free(recvmsg, t->recvmsg_mempool);\n\n\tmempool_destroy(t->recvmsg_mempool);\n\tt->recvmsg_mempool = NULL;\n\n\tkmem_cache_destroy(t->recvmsg_cache);\n\tt->recvmsg_cache = NULL;\n\n\tmempool_destroy(t->sendmsg_mempool);\n\tt->sendmsg_mempool = NULL;\n\n\tkmem_cache_destroy(t->sendmsg_cache);\n\tt->sendmsg_cache = NULL;\n}\n\nstatic int smb_direct_create_pools(struct smb_direct_transport *t)\n{\n\tchar name[80];\n\tint i;\n\tstruct smb_direct_recvmsg *recvmsg;\n\n\tsnprintf(name, sizeof(name), \"smb_direct_rqst_pool_%p\", t);\n\tt->sendmsg_cache = kmem_cache_create(name,\n\t\t\t\t\t     sizeof(struct smb_direct_sendmsg) +\n\t\t\t\t\t      sizeof(struct smb_direct_negotiate_resp),\n\t\t\t\t\t     0, SLAB_HWCACHE_ALIGN, NULL);\n\tif (!t->sendmsg_cache)\n\t\treturn -ENOMEM;\n\n\tt->sendmsg_mempool = mempool_create(t->send_credit_target,\n\t\t\t\t\t    mempool_alloc_slab, mempool_free_slab,\n\t\t\t\t\t    t->sendmsg_cache);\n\tif (!t->sendmsg_mempool)\n\t\tgoto err;\n\n\tsnprintf(name, sizeof(name), \"smb_direct_resp_%p\", t);\n\tt->recvmsg_cache = kmem_cache_create(name,\n\t\t\t\t\t     sizeof(struct smb_direct_recvmsg) +\n\t\t\t\t\t      t->max_recv_size,\n\t\t\t\t\t     0, SLAB_HWCACHE_ALIGN, NULL);\n\tif (!t->recvmsg_cache)\n\t\tgoto err;\n\n\tt->recvmsg_mempool =\n\t\tmempool_create(t->recv_credit_max, mempool_alloc_slab,\n\t\t\t       mempool_free_slab, t->recvmsg_cache);\n\tif (!t->recvmsg_mempool)\n\t\tgoto err;\n\n\tINIT_LIST_HEAD(&t->recvmsg_queue);\n\n\tfor (i = 0; i < t->recv_credit_max; i++) {\n\t\trecvmsg = mempool_alloc(t->recvmsg_mempool, GFP_KERNEL);\n\t\tif (!recvmsg)\n\t\t\tgoto err;\n\t\trecvmsg->transport = t;\n\t\tlist_add(&recvmsg->list, &t->recvmsg_queue);\n\t}\n\tt->count_avail_recvmsg = t->recv_credit_max;\n\n\treturn 0;\nerr:\n\tsmb_direct_destroy_pools(t);\n\treturn -ENOMEM;\n}\n\nstatic int smb_direct_create_qpair(struct smb_direct_transport *t,\n\t\t\t\t   struct ib_qp_cap *cap)\n{\n\tint ret;\n\tstruct ib_qp_init_attr qp_attr;\n\tint pages_per_rw;\n\n\tt->pd = ib_alloc_pd(t->cm_id->device, 0);\n\tif (IS_ERR(t->pd)) {\n\t\tpr_err(\"Can't create RDMA PD\\n\");\n\t\tret = PTR_ERR(t->pd);\n\t\tt->pd = NULL;\n\t\treturn ret;\n\t}\n\n\tt->send_cq = ib_alloc_cq(t->cm_id->device, t,\n\t\t\t\t smb_direct_send_credit_target + cap->max_rdma_ctxs,\n\t\t\t\t 0, IB_POLL_WORKQUEUE);\n\tif (IS_ERR(t->send_cq)) {\n\t\tpr_err(\"Can't create RDMA send CQ\\n\");\n\t\tret = PTR_ERR(t->send_cq);\n\t\tt->send_cq = NULL;\n\t\tgoto err;\n\t}\n\n\tt->recv_cq = ib_alloc_cq(t->cm_id->device, t,\n\t\t\t\t t->recv_credit_max, 0, IB_POLL_WORKQUEUE);\n\tif (IS_ERR(t->recv_cq)) {\n\t\tpr_err(\"Can't create RDMA recv CQ\\n\");\n\t\tret = PTR_ERR(t->recv_cq);\n\t\tt->recv_cq = NULL;\n\t\tgoto err;\n\t}\n\n\tmemset(&qp_attr, 0, sizeof(qp_attr));\n\tqp_attr.event_handler = smb_direct_qpair_handler;\n\tqp_attr.qp_context = t;\n\tqp_attr.cap = *cap;\n\tqp_attr.sq_sig_type = IB_SIGNAL_REQ_WR;\n\tqp_attr.qp_type = IB_QPT_RC;\n\tqp_attr.send_cq = t->send_cq;\n\tqp_attr.recv_cq = t->recv_cq;\n\tqp_attr.port_num = ~0;\n\n\tret = rdma_create_qp(t->cm_id, t->pd, &qp_attr);\n\tif (ret) {\n\t\tpr_err(\"Can't create RDMA QP: %d\\n\", ret);\n\t\tgoto err;\n\t}\n\n\tt->qp = t->cm_id->qp;\n\tt->cm_id->event_handler = smb_direct_cm_handler;\n\n\tpages_per_rw = DIV_ROUND_UP(t->max_rdma_rw_size, PAGE_SIZE) + 1;\n\tif (pages_per_rw > t->cm_id->device->attrs.max_sgl_rd) {\n\t\tret = ib_mr_pool_init(t->qp, &t->qp->rdma_mrs,\n\t\t\t\t      t->max_rw_credits, IB_MR_TYPE_MEM_REG,\n\t\t\t\t      t->pages_per_rw_credit, 0);\n\t\tif (ret) {\n\t\t\tpr_err(\"failed to init mr pool count %d pages %d\\n\",\n\t\t\t       t->max_rw_credits, t->pages_per_rw_credit);\n\t\t\tgoto err;\n\t\t}\n\t}\n\n\treturn 0;\nerr:\n\tif (t->qp) {\n\t\tib_destroy_qp(t->qp);\n\t\tt->qp = NULL;\n\t}\n\tif (t->recv_cq) {\n\t\tib_destroy_cq(t->recv_cq);\n\t\tt->recv_cq = NULL;\n\t}\n\tif (t->send_cq) {\n\t\tib_destroy_cq(t->send_cq);\n\t\tt->send_cq = NULL;\n\t}\n\tif (t->pd) {\n\t\tib_dealloc_pd(t->pd);\n\t\tt->pd = NULL;\n\t}\n\treturn ret;\n}\n\nstatic int smb_direct_prepare(struct ksmbd_transport *t)\n{\n\tstruct smb_direct_transport *st = smb_trans_direct_transfort(t);\n\tstruct smb_direct_recvmsg *recvmsg;\n\tstruct smb_direct_negotiate_req *req;\n\tint ret;\n\n\tksmbd_debug(RDMA, \"Waiting for SMB_DIRECT negotiate request\\n\");\n\tret = wait_event_interruptible_timeout(st->wait_status,\n\t\t\t\t\t       st->negotiation_requested ||\n\t\t\t\t\t       st->status == SMB_DIRECT_CS_DISCONNECTED,\n\t\t\t\t\t       SMB_DIRECT_NEGOTIATE_TIMEOUT * HZ);\n\tif (ret <= 0 || st->status == SMB_DIRECT_CS_DISCONNECTED)\n\t\treturn ret < 0 ? ret : -ETIMEDOUT;\n\n\trecvmsg = get_first_reassembly(st);\n\tif (!recvmsg)\n\t\treturn -ECONNABORTED;\n\n\tret = smb_direct_check_recvmsg(recvmsg);\n\tif (ret == -ECONNABORTED)\n\t\tgoto out;\n\n\treq = (struct smb_direct_negotiate_req *)recvmsg->packet;\n\tst->max_recv_size = min_t(int, st->max_recv_size,\n\t\t\t\t  le32_to_cpu(req->preferred_send_size));\n\tst->max_send_size = min_t(int, st->max_send_size,\n\t\t\t\t  le32_to_cpu(req->max_receive_size));\n\tst->max_fragmented_send_size =\n\t\tle32_to_cpu(req->max_fragmented_size);\n\tst->max_fragmented_recv_size =\n\t\t(st->recv_credit_max * st->max_recv_size) / 2;\n\n\tret = smb_direct_send_negotiate_response(st, ret);\nout:\n\tspin_lock_irq(&st->reassembly_queue_lock);\n\tst->reassembly_queue_length--;\n\tlist_del(&recvmsg->list);\n\tspin_unlock_irq(&st->reassembly_queue_lock);\n\tput_recvmsg(st, recvmsg);\n\n\treturn ret;\n}\n\nstatic int smb_direct_connect(struct smb_direct_transport *st)\n{\n\tint ret;\n\tstruct ib_qp_cap qp_cap;\n\n\tret = smb_direct_init_params(st, &qp_cap);\n\tif (ret) {\n\t\tpr_err(\"Can't configure RDMA parameters\\n\");\n\t\treturn ret;\n\t}\n\n\tret = smb_direct_create_pools(st);\n\tif (ret) {\n\t\tpr_err(\"Can't init RDMA pool: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = smb_direct_create_qpair(st, &qp_cap);\n\tif (ret) {\n\t\tpr_err(\"Can't accept RDMA client: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tret = smb_direct_prepare_negotiation(st);\n\tif (ret) {\n\t\tpr_err(\"Can't negotiate: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\treturn 0;\n}\n\nstatic bool rdma_frwr_is_supported(struct ib_device_attr *attrs)\n{\n\tif (!(attrs->device_cap_flags & IB_DEVICE_MEM_MGT_EXTENSIONS))\n\t\treturn false;\n\tif (attrs->max_fast_reg_page_list_len == 0)\n\t\treturn false;\n\treturn true;\n}\n\nstatic int smb_direct_handle_connect_request(struct rdma_cm_id *new_cm_id)\n{\n\tstruct smb_direct_transport *t;\n\tstruct task_struct *handler;\n\tint ret;\n\n\tif (!rdma_frwr_is_supported(&new_cm_id->device->attrs)) {\n\t\tksmbd_debug(RDMA,\n\t\t\t    \"Fast Registration Work Requests is not supported. device capabilities=%llx\\n\",\n\t\t\t    new_cm_id->device->attrs.device_cap_flags);\n\t\treturn -EPROTONOSUPPORT;\n\t}\n\n\tt = alloc_transport(new_cm_id);\n\tif (!t)\n\t\treturn -ENOMEM;\n\n\tret = smb_direct_connect(t);\n\tif (ret)\n\t\tgoto out_err;\n\n\thandler = kthread_run(ksmbd_conn_handler_loop,\n\t\t\t      KSMBD_TRANS(t)->conn, \"ksmbd:r%u\",\n\t\t\t      smb_direct_port);\n\tif (IS_ERR(handler)) {\n\t\tret = PTR_ERR(handler);\n\t\tpr_err(\"Can't start thread\\n\");\n\t\tgoto out_err;\n\t}\n\n\treturn 0;\nout_err:\n\tfree_transport(t);\n\treturn ret;\n}\n\nstatic int smb_direct_listen_handler(struct rdma_cm_id *cm_id,\n\t\t\t\t     struct rdma_cm_event *event)\n{\n\tswitch (event->event) {\n\tcase RDMA_CM_EVENT_CONNECT_REQUEST: {\n\t\tint ret = smb_direct_handle_connect_request(cm_id);\n\n\t\tif (ret) {\n\t\t\tpr_err(\"Can't create transport: %d\\n\", ret);\n\t\t\treturn ret;\n\t\t}\n\n\t\tksmbd_debug(RDMA, \"Received connection request. cm_id=%p\\n\",\n\t\t\t    cm_id);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tpr_err(\"Unexpected listen event. cm_id=%p, event=%s (%d)\\n\",\n\t\t       cm_id, rdma_event_msg(event->event), event->event);\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\nstatic int smb_direct_listen(int port)\n{\n\tint ret;\n\tstruct rdma_cm_id *cm_id;\n\tstruct sockaddr_in sin = {\n\t\t.sin_family\t\t= AF_INET,\n\t\t.sin_addr.s_addr\t= htonl(INADDR_ANY),\n\t\t.sin_port\t\t= htons(port),\n\t};\n\n\tcm_id = rdma_create_id(&init_net, smb_direct_listen_handler,\n\t\t\t       &smb_direct_listener, RDMA_PS_TCP, IB_QPT_RC);\n\tif (IS_ERR(cm_id)) {\n\t\tpr_err(\"Can't create cm id: %ld\\n\", PTR_ERR(cm_id));\n\t\treturn PTR_ERR(cm_id);\n\t}\n\n\tret = rdma_bind_addr(cm_id, (struct sockaddr *)&sin);\n\tif (ret) {\n\t\tpr_err(\"Can't bind: %d\\n\", ret);\n\t\tgoto err;\n\t}\n\n\tsmb_direct_listener.cm_id = cm_id;\n\n\tret = rdma_listen(cm_id, 10);\n\tif (ret) {\n\t\tpr_err(\"Can't listen: %d\\n\", ret);\n\t\tgoto err;\n\t}\n\treturn 0;\nerr:\n\tsmb_direct_listener.cm_id = NULL;\n\trdma_destroy_id(cm_id);\n\treturn ret;\n}\n\nstatic int smb_direct_ib_client_add(struct ib_device *ib_dev)\n{\n\tstruct smb_direct_device *smb_dev;\n\n\t \n\tif (ib_dev->node_type != RDMA_NODE_IB_CA)\n\t\tsmb_direct_port = SMB_DIRECT_PORT_IWARP;\n\n\tif (!rdma_frwr_is_supported(&ib_dev->attrs))\n\t\treturn 0;\n\n\tsmb_dev = kzalloc(sizeof(*smb_dev), GFP_KERNEL);\n\tif (!smb_dev)\n\t\treturn -ENOMEM;\n\tsmb_dev->ib_dev = ib_dev;\n\n\twrite_lock(&smb_direct_device_lock);\n\tlist_add(&smb_dev->list, &smb_direct_device_list);\n\twrite_unlock(&smb_direct_device_lock);\n\n\tksmbd_debug(RDMA, \"ib device added: name %s\\n\", ib_dev->name);\n\treturn 0;\n}\n\nstatic void smb_direct_ib_client_remove(struct ib_device *ib_dev,\n\t\t\t\t\tvoid *client_data)\n{\n\tstruct smb_direct_device *smb_dev, *tmp;\n\n\twrite_lock(&smb_direct_device_lock);\n\tlist_for_each_entry_safe(smb_dev, tmp, &smb_direct_device_list, list) {\n\t\tif (smb_dev->ib_dev == ib_dev) {\n\t\t\tlist_del(&smb_dev->list);\n\t\t\tkfree(smb_dev);\n\t\t\tbreak;\n\t\t}\n\t}\n\twrite_unlock(&smb_direct_device_lock);\n}\n\nstatic struct ib_client smb_direct_ib_client = {\n\t.name\t= \"ksmbd_smb_direct_ib\",\n\t.add\t= smb_direct_ib_client_add,\n\t.remove\t= smb_direct_ib_client_remove,\n};\n\nint ksmbd_rdma_init(void)\n{\n\tint ret;\n\n\tsmb_direct_listener.cm_id = NULL;\n\n\tret = ib_register_client(&smb_direct_ib_client);\n\tif (ret) {\n\t\tpr_err(\"failed to ib_register_client\\n\");\n\t\treturn ret;\n\t}\n\n\t \n\tsmb_direct_wq = alloc_workqueue(\"ksmbd-smb_direct-wq\",\n\t\t\t\t\tWQ_HIGHPRI | WQ_MEM_RECLAIM, 0);\n\tif (!smb_direct_wq)\n\t\treturn -ENOMEM;\n\n\tret = smb_direct_listen(smb_direct_port);\n\tif (ret) {\n\t\tdestroy_workqueue(smb_direct_wq);\n\t\tsmb_direct_wq = NULL;\n\t\tpr_err(\"Can't listen: %d\\n\", ret);\n\t\treturn ret;\n\t}\n\n\tksmbd_debug(RDMA, \"init RDMA listener. cm_id=%p\\n\",\n\t\t    smb_direct_listener.cm_id);\n\treturn 0;\n}\n\nvoid ksmbd_rdma_destroy(void)\n{\n\tif (!smb_direct_listener.cm_id)\n\t\treturn;\n\n\tib_unregister_client(&smb_direct_ib_client);\n\trdma_destroy_id(smb_direct_listener.cm_id);\n\n\tsmb_direct_listener.cm_id = NULL;\n\n\tif (smb_direct_wq) {\n\t\tdestroy_workqueue(smb_direct_wq);\n\t\tsmb_direct_wq = NULL;\n\t}\n}\n\nbool ksmbd_rdma_capable_netdev(struct net_device *netdev)\n{\n\tstruct smb_direct_device *smb_dev;\n\tint i;\n\tbool rdma_capable = false;\n\n\tread_lock(&smb_direct_device_lock);\n\tlist_for_each_entry(smb_dev, &smb_direct_device_list, list) {\n\t\tfor (i = 0; i < smb_dev->ib_dev->phys_port_cnt; i++) {\n\t\t\tstruct net_device *ndev;\n\n\t\t\tif (smb_dev->ib_dev->ops.get_netdev) {\n\t\t\t\tndev = smb_dev->ib_dev->ops.get_netdev(\n\t\t\t\t\tsmb_dev->ib_dev, i + 1);\n\t\t\t\tif (!ndev)\n\t\t\t\t\tcontinue;\n\n\t\t\t\tif (ndev == netdev) {\n\t\t\t\t\tdev_put(ndev);\n\t\t\t\t\trdma_capable = true;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tdev_put(ndev);\n\t\t\t \n\t\t\t} else if (netdev->type == ARPHRD_INFINIBAND) {\n\t\t\t\tstruct netdev_hw_addr *ha;\n\t\t\t\tunion ib_gid gid;\n\t\t\t\tu32 port_num;\n\t\t\t\tint ret;\n\n\t\t\t\tnetdev_hw_addr_list_for_each(\n\t\t\t\t\tha, &netdev->dev_addrs) {\n\t\t\t\t\tmemcpy(&gid, ha->addr + 4, sizeof(gid));\n\t\t\t\t\tret = ib_find_gid(smb_dev->ib_dev, &gid,\n\t\t\t\t\t\t\t  &port_num, NULL);\n\t\t\t\t\tif (!ret) {\n\t\t\t\t\t\trdma_capable = true;\n\t\t\t\t\t\tgoto out;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\nout:\n\tread_unlock(&smb_direct_device_lock);\n\n\tif (rdma_capable == false) {\n\t\tstruct ib_device *ibdev;\n\n\t\tibdev = ib_device_get_by_netdev(netdev, RDMA_DRIVER_UNKNOWN);\n\t\tif (ibdev) {\n\t\t\tif (rdma_frwr_is_supported(&ibdev->attrs))\n\t\t\t\trdma_capable = true;\n\t\t\tib_device_put(ibdev);\n\t\t}\n\t}\n\n\treturn rdma_capable;\n}\n\nstatic struct ksmbd_transport_ops ksmbd_smb_direct_transport_ops = {\n\t.prepare\t= smb_direct_prepare,\n\t.disconnect\t= smb_direct_disconnect,\n\t.shutdown\t= smb_direct_shutdown,\n\t.writev\t\t= smb_direct_writev,\n\t.read\t\t= smb_direct_read,\n\t.rdma_read\t= smb_direct_rdma_read,\n\t.rdma_write\t= smb_direct_rdma_write,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}