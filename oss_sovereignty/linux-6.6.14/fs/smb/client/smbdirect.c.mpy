{
  "module_name": "smbdirect.c",
  "hash_id": "0f7a6e621e9af47291b0fc7a1e3614ecf19daecb3dd7d96af4aead2cc361cdbf",
  "original_prompt": "Ingested from linux-6.6.14/fs/smb/client/smbdirect.c",
  "human_readable_source": "\n \n#include <linux/module.h>\n#include <linux/highmem.h>\n#include \"smbdirect.h\"\n#include \"cifs_debug.h\"\n#include \"cifsproto.h\"\n#include \"smb2proto.h\"\n\nstatic struct smbd_response *get_empty_queue_buffer(\n\t\tstruct smbd_connection *info);\nstatic struct smbd_response *get_receive_buffer(\n\t\tstruct smbd_connection *info);\nstatic void put_receive_buffer(\n\t\tstruct smbd_connection *info,\n\t\tstruct smbd_response *response);\nstatic int allocate_receive_buffers(struct smbd_connection *info, int num_buf);\nstatic void destroy_receive_buffers(struct smbd_connection *info);\n\nstatic void put_empty_packet(\n\t\tstruct smbd_connection *info, struct smbd_response *response);\nstatic void enqueue_reassembly(\n\t\tstruct smbd_connection *info,\n\t\tstruct smbd_response *response, int data_length);\nstatic struct smbd_response *_get_first_reassembly(\n\t\tstruct smbd_connection *info);\n\nstatic int smbd_post_recv(\n\t\tstruct smbd_connection *info,\n\t\tstruct smbd_response *response);\n\nstatic int smbd_post_send_empty(struct smbd_connection *info);\n\nstatic void destroy_mr_list(struct smbd_connection *info);\nstatic int allocate_mr_list(struct smbd_connection *info);\n\nstruct smb_extract_to_rdma {\n\tstruct ib_sge\t\t*sge;\n\tunsigned int\t\tnr_sge;\n\tunsigned int\t\tmax_sge;\n\tstruct ib_device\t*device;\n\tu32\t\t\tlocal_dma_lkey;\n\tenum dma_data_direction\tdirection;\n};\nstatic ssize_t smb_extract_iter_to_rdma(struct iov_iter *iter, size_t len,\n\t\t\t\t\tstruct smb_extract_to_rdma *rdma);\n\n \n#define SMBD_V1\t0x0100\n\n \n#define SMB_PORT\t445\n#define SMBD_PORT\t5445\n\n \n#define RDMA_RESOLVE_TIMEOUT\t5000\n\n \n#define SMBD_NEGOTIATE_TIMEOUT\t120\n\n \n#define SMBD_MIN_RECEIVE_SIZE\t\t128\n#define SMBD_MIN_FRAGMENTED_SIZE\t131072\n\n \n#define SMBD_CM_RESPONDER_RESOURCES\t32\n\n \n#define SMBD_CM_RETRY\t\t\t6\n \n#define SMBD_CM_RNR_RETRY\t\t0\n\n \n \nint smbd_receive_credit_max = 255;\n\n \nint smbd_send_credit_target = 255;\n\n \nint smbd_max_send_size = 1364;\n\n \nint smbd_max_fragmented_recv_size = 1024 * 1024;\n\n \nint smbd_max_receive_size = 1364;\n\n \nint smbd_keep_alive_interval = 120;\n\n \n \nint smbd_max_frmr_depth = 2048;\n\n \nint rdma_readwrite_threshold = 4096;\n\n \n#define LOG_OUTGOING\t\t\t0x1\n#define LOG_INCOMING\t\t\t0x2\n#define LOG_READ\t\t\t0x4\n#define LOG_WRITE\t\t\t0x8\n#define LOG_RDMA_SEND\t\t\t0x10\n#define LOG_RDMA_RECV\t\t\t0x20\n#define LOG_KEEP_ALIVE\t\t\t0x40\n#define LOG_RDMA_EVENT\t\t\t0x80\n#define LOG_RDMA_MR\t\t\t0x100\nstatic unsigned int smbd_logging_class;\nmodule_param(smbd_logging_class, uint, 0644);\nMODULE_PARM_DESC(smbd_logging_class,\n\t\"Logging class for SMBD transport 0x0 to 0x100\");\n\n#define ERR\t\t0x0\n#define INFO\t\t0x1\nstatic unsigned int smbd_logging_level = ERR;\nmodule_param(smbd_logging_level, uint, 0644);\nMODULE_PARM_DESC(smbd_logging_level,\n\t\"Logging level for SMBD transport, 0 (default): error, 1: info\");\n\n#define log_rdma(level, class, fmt, args...)\t\t\t\t\\\ndo {\t\t\t\t\t\t\t\t\t\\\n\tif (level <= smbd_logging_level || class & smbd_logging_class)\t\\\n\t\tcifs_dbg(VFS, \"%s:%d \" fmt, __func__, __LINE__, ##args);\\\n} while (0)\n\n#define log_outgoing(level, fmt, args...) \\\n\t\tlog_rdma(level, LOG_OUTGOING, fmt, ##args)\n#define log_incoming(level, fmt, args...) \\\n\t\tlog_rdma(level, LOG_INCOMING, fmt, ##args)\n#define log_read(level, fmt, args...)\tlog_rdma(level, LOG_READ, fmt, ##args)\n#define log_write(level, fmt, args...)\tlog_rdma(level, LOG_WRITE, fmt, ##args)\n#define log_rdma_send(level, fmt, args...) \\\n\t\tlog_rdma(level, LOG_RDMA_SEND, fmt, ##args)\n#define log_rdma_recv(level, fmt, args...) \\\n\t\tlog_rdma(level, LOG_RDMA_RECV, fmt, ##args)\n#define log_keep_alive(level, fmt, args...) \\\n\t\tlog_rdma(level, LOG_KEEP_ALIVE, fmt, ##args)\n#define log_rdma_event(level, fmt, args...) \\\n\t\tlog_rdma(level, LOG_RDMA_EVENT, fmt, ##args)\n#define log_rdma_mr(level, fmt, args...) \\\n\t\tlog_rdma(level, LOG_RDMA_MR, fmt, ##args)\n\nstatic void smbd_disconnect_rdma_work(struct work_struct *work)\n{\n\tstruct smbd_connection *info =\n\t\tcontainer_of(work, struct smbd_connection, disconnect_work);\n\n\tif (info->transport_status == SMBD_CONNECTED) {\n\t\tinfo->transport_status = SMBD_DISCONNECTING;\n\t\trdma_disconnect(info->id);\n\t}\n}\n\nstatic void smbd_disconnect_rdma_connection(struct smbd_connection *info)\n{\n\tqueue_work(info->workqueue, &info->disconnect_work);\n}\n\n \nstatic int smbd_conn_upcall(\n\t\tstruct rdma_cm_id *id, struct rdma_cm_event *event)\n{\n\tstruct smbd_connection *info = id->context;\n\n\tlog_rdma_event(INFO, \"event=%d status=%d\\n\",\n\t\tevent->event, event->status);\n\n\tswitch (event->event) {\n\tcase RDMA_CM_EVENT_ADDR_RESOLVED:\n\tcase RDMA_CM_EVENT_ROUTE_RESOLVED:\n\t\tinfo->ri_rc = 0;\n\t\tcomplete(&info->ri_done);\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_ADDR_ERROR:\n\t\tinfo->ri_rc = -EHOSTUNREACH;\n\t\tcomplete(&info->ri_done);\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_ROUTE_ERROR:\n\t\tinfo->ri_rc = -ENETUNREACH;\n\t\tcomplete(&info->ri_done);\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_ESTABLISHED:\n\t\tlog_rdma_event(INFO, \"connected event=%d\\n\", event->event);\n\t\tinfo->transport_status = SMBD_CONNECTED;\n\t\twake_up_interruptible(&info->conn_wait);\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_CONNECT_ERROR:\n\tcase RDMA_CM_EVENT_UNREACHABLE:\n\tcase RDMA_CM_EVENT_REJECTED:\n\t\tlog_rdma_event(INFO, \"connecting failed event=%d\\n\", event->event);\n\t\tinfo->transport_status = SMBD_DISCONNECTED;\n\t\twake_up_interruptible(&info->conn_wait);\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_DEVICE_REMOVAL:\n\tcase RDMA_CM_EVENT_DISCONNECTED:\n\t\t \n\t\tif (info->transport_status == SMBD_NEGOTIATE_FAILED) {\n\t\t\tinfo->transport_status = SMBD_DISCONNECTED;\n\t\t\twake_up(&info->conn_wait);\n\t\t\tbreak;\n\t\t}\n\n\t\tinfo->transport_status = SMBD_DISCONNECTED;\n\t\twake_up_interruptible(&info->disconn_wait);\n\t\twake_up_interruptible(&info->wait_reassembly_queue);\n\t\twake_up_interruptible_all(&info->wait_send_queue);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n\n\treturn 0;\n}\n\n \nstatic void\nsmbd_qp_async_error_upcall(struct ib_event *event, void *context)\n{\n\tstruct smbd_connection *info = context;\n\n\tlog_rdma_event(ERR, \"%s on device %s info %p\\n\",\n\t\tib_event_msg(event->event), event->device->name, info);\n\n\tswitch (event->event) {\n\tcase IB_EVENT_CQ_ERR:\n\tcase IB_EVENT_QP_FATAL:\n\t\tsmbd_disconnect_rdma_connection(info);\n\t\tbreak;\n\n\tdefault:\n\t\tbreak;\n\t}\n}\n\nstatic inline void *smbd_request_payload(struct smbd_request *request)\n{\n\treturn (void *)request->packet;\n}\n\nstatic inline void *smbd_response_payload(struct smbd_response *response)\n{\n\treturn (void *)response->packet;\n}\n\n \nstatic void send_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tint i;\n\tstruct smbd_request *request =\n\t\tcontainer_of(wc->wr_cqe, struct smbd_request, cqe);\n\n\tlog_rdma_send(INFO, \"smbd_request 0x%p completed wc->status=%d\\n\",\n\t\trequest, wc->status);\n\n\tif (wc->status != IB_WC_SUCCESS || wc->opcode != IB_WC_SEND) {\n\t\tlog_rdma_send(ERR, \"wc->status=%d wc->opcode=%d\\n\",\n\t\t\twc->status, wc->opcode);\n\t\tsmbd_disconnect_rdma_connection(request->info);\n\t}\n\n\tfor (i = 0; i < request->num_sge; i++)\n\t\tib_dma_unmap_single(request->info->id->device,\n\t\t\trequest->sge[i].addr,\n\t\t\trequest->sge[i].length,\n\t\t\tDMA_TO_DEVICE);\n\n\tif (atomic_dec_and_test(&request->info->send_pending))\n\t\twake_up(&request->info->wait_send_pending);\n\n\twake_up(&request->info->wait_post_send);\n\n\tmempool_free(request, request->info->request_mempool);\n}\n\nstatic void dump_smbd_negotiate_resp(struct smbd_negotiate_resp *resp)\n{\n\tlog_rdma_event(INFO, \"resp message min_version %u max_version %u negotiated_version %u credits_requested %u credits_granted %u status %u max_readwrite_size %u preferred_send_size %u max_receive_size %u max_fragmented_size %u\\n\",\n\t\t       resp->min_version, resp->max_version,\n\t\t       resp->negotiated_version, resp->credits_requested,\n\t\t       resp->credits_granted, resp->status,\n\t\t       resp->max_readwrite_size, resp->preferred_send_size,\n\t\t       resp->max_receive_size, resp->max_fragmented_size);\n}\n\n \nstatic bool process_negotiation_response(\n\t\tstruct smbd_response *response, int packet_length)\n{\n\tstruct smbd_connection *info = response->info;\n\tstruct smbd_negotiate_resp *packet = smbd_response_payload(response);\n\n\tif (packet_length < sizeof(struct smbd_negotiate_resp)) {\n\t\tlog_rdma_event(ERR,\n\t\t\t\"error: packet_length=%d\\n\", packet_length);\n\t\treturn false;\n\t}\n\n\tif (le16_to_cpu(packet->negotiated_version) != SMBD_V1) {\n\t\tlog_rdma_event(ERR, \"error: negotiated_version=%x\\n\",\n\t\t\tle16_to_cpu(packet->negotiated_version));\n\t\treturn false;\n\t}\n\tinfo->protocol = le16_to_cpu(packet->negotiated_version);\n\n\tif (packet->credits_requested == 0) {\n\t\tlog_rdma_event(ERR, \"error: credits_requested==0\\n\");\n\t\treturn false;\n\t}\n\tinfo->receive_credit_target = le16_to_cpu(packet->credits_requested);\n\n\tif (packet->credits_granted == 0) {\n\t\tlog_rdma_event(ERR, \"error: credits_granted==0\\n\");\n\t\treturn false;\n\t}\n\tatomic_set(&info->send_credits, le16_to_cpu(packet->credits_granted));\n\n\tatomic_set(&info->receive_credits, 0);\n\n\tif (le32_to_cpu(packet->preferred_send_size) > info->max_receive_size) {\n\t\tlog_rdma_event(ERR, \"error: preferred_send_size=%d\\n\",\n\t\t\tle32_to_cpu(packet->preferred_send_size));\n\t\treturn false;\n\t}\n\tinfo->max_receive_size = le32_to_cpu(packet->preferred_send_size);\n\n\tif (le32_to_cpu(packet->max_receive_size) < SMBD_MIN_RECEIVE_SIZE) {\n\t\tlog_rdma_event(ERR, \"error: max_receive_size=%d\\n\",\n\t\t\tle32_to_cpu(packet->max_receive_size));\n\t\treturn false;\n\t}\n\tinfo->max_send_size = min_t(int, info->max_send_size,\n\t\t\t\t\tle32_to_cpu(packet->max_receive_size));\n\n\tif (le32_to_cpu(packet->max_fragmented_size) <\n\t\t\tSMBD_MIN_FRAGMENTED_SIZE) {\n\t\tlog_rdma_event(ERR, \"error: max_fragmented_size=%d\\n\",\n\t\t\tle32_to_cpu(packet->max_fragmented_size));\n\t\treturn false;\n\t}\n\tinfo->max_fragmented_send_size =\n\t\tle32_to_cpu(packet->max_fragmented_size);\n\tinfo->rdma_readwrite_threshold =\n\t\trdma_readwrite_threshold > info->max_fragmented_send_size ?\n\t\tinfo->max_fragmented_send_size :\n\t\trdma_readwrite_threshold;\n\n\n\tinfo->max_readwrite_size = min_t(u32,\n\t\t\tle32_to_cpu(packet->max_readwrite_size),\n\t\t\tinfo->max_frmr_depth * PAGE_SIZE);\n\tinfo->max_frmr_depth = info->max_readwrite_size / PAGE_SIZE;\n\n\treturn true;\n}\n\nstatic void smbd_post_send_credits(struct work_struct *work)\n{\n\tint ret = 0;\n\tint use_receive_queue = 1;\n\tint rc;\n\tstruct smbd_response *response;\n\tstruct smbd_connection *info =\n\t\tcontainer_of(work, struct smbd_connection,\n\t\t\tpost_send_credits_work);\n\n\tif (info->transport_status != SMBD_CONNECTED) {\n\t\twake_up(&info->wait_receive_queues);\n\t\treturn;\n\t}\n\n\tif (info->receive_credit_target >\n\t\tatomic_read(&info->receive_credits)) {\n\t\twhile (true) {\n\t\t\tif (use_receive_queue)\n\t\t\t\tresponse = get_receive_buffer(info);\n\t\t\telse\n\t\t\t\tresponse = get_empty_queue_buffer(info);\n\t\t\tif (!response) {\n\t\t\t\t \n\t\t\t\tif (use_receive_queue) {\n\t\t\t\t\tuse_receive_queue = 0;\n\t\t\t\t\tcontinue;\n\t\t\t\t} else\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tresponse->type = SMBD_TRANSFER_DATA;\n\t\t\tresponse->first_segment = false;\n\t\t\trc = smbd_post_recv(info, response);\n\t\t\tif (rc) {\n\t\t\t\tlog_rdma_recv(ERR,\n\t\t\t\t\t\"post_recv failed rc=%d\\n\", rc);\n\t\t\t\tput_receive_buffer(info, response);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tret++;\n\t\t}\n\t}\n\n\tspin_lock(&info->lock_new_credits_offered);\n\tinfo->new_credits_offered += ret;\n\tspin_unlock(&info->lock_new_credits_offered);\n\n\t \n\tinfo->send_immediate = true;\n\tif (atomic_read(&info->receive_credits) <\n\t\tinfo->receive_credit_target - 1) {\n\t\tif (info->keep_alive_requested == KEEP_ALIVE_PENDING ||\n\t\t    info->send_immediate) {\n\t\t\tlog_keep_alive(INFO, \"send an empty message\\n\");\n\t\t\tsmbd_post_send_empty(info);\n\t\t}\n\t}\n}\n\n \nstatic void recv_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct smbd_data_transfer *data_transfer;\n\tstruct smbd_response *response =\n\t\tcontainer_of(wc->wr_cqe, struct smbd_response, cqe);\n\tstruct smbd_connection *info = response->info;\n\tint data_length = 0;\n\n\tlog_rdma_recv(INFO, \"response=0x%p type=%d wc status=%d wc opcode %d byte_len=%d pkey_index=%u\\n\",\n\t\t      response, response->type, wc->status, wc->opcode,\n\t\t      wc->byte_len, wc->pkey_index);\n\n\tif (wc->status != IB_WC_SUCCESS || wc->opcode != IB_WC_RECV) {\n\t\tlog_rdma_recv(INFO, \"wc->status=%d opcode=%d\\n\",\n\t\t\twc->status, wc->opcode);\n\t\tsmbd_disconnect_rdma_connection(info);\n\t\tgoto error;\n\t}\n\n\tib_dma_sync_single_for_cpu(\n\t\twc->qp->device,\n\t\tresponse->sge.addr,\n\t\tresponse->sge.length,\n\t\tDMA_FROM_DEVICE);\n\n\tswitch (response->type) {\n\t \n\tcase SMBD_NEGOTIATE_RESP:\n\t\tdump_smbd_negotiate_resp(smbd_response_payload(response));\n\t\tinfo->full_packet_received = true;\n\t\tinfo->negotiate_done =\n\t\t\tprocess_negotiation_response(response, wc->byte_len);\n\t\tcomplete(&info->negotiate_completion);\n\t\tbreak;\n\n\t \n\tcase SMBD_TRANSFER_DATA:\n\t\tdata_transfer = smbd_response_payload(response);\n\t\tdata_length = le32_to_cpu(data_transfer->data_length);\n\n\t\t \n\t\tif (data_length) {\n\t\t\tif (info->full_packet_received)\n\t\t\t\tresponse->first_segment = true;\n\n\t\t\tif (le32_to_cpu(data_transfer->remaining_data_length))\n\t\t\t\tinfo->full_packet_received = false;\n\t\t\telse\n\t\t\t\tinfo->full_packet_received = true;\n\n\t\t\tenqueue_reassembly(\n\t\t\t\tinfo,\n\t\t\t\tresponse,\n\t\t\t\tdata_length);\n\t\t} else\n\t\t\tput_empty_packet(info, response);\n\n\t\tif (data_length)\n\t\t\twake_up_interruptible(&info->wait_reassembly_queue);\n\n\t\tatomic_dec(&info->receive_credits);\n\t\tinfo->receive_credit_target =\n\t\t\tle16_to_cpu(data_transfer->credits_requested);\n\t\tif (le16_to_cpu(data_transfer->credits_granted)) {\n\t\t\tatomic_add(le16_to_cpu(data_transfer->credits_granted),\n\t\t\t\t&info->send_credits);\n\t\t\t \n\t\t\twake_up_interruptible(&info->wait_send_queue);\n\t\t}\n\n\t\tlog_incoming(INFO, \"data flags %d data_offset %d data_length %d remaining_data_length %d\\n\",\n\t\t\t     le16_to_cpu(data_transfer->flags),\n\t\t\t     le32_to_cpu(data_transfer->data_offset),\n\t\t\t     le32_to_cpu(data_transfer->data_length),\n\t\t\t     le32_to_cpu(data_transfer->remaining_data_length));\n\n\t\t \n\t\tinfo->keep_alive_requested = KEEP_ALIVE_NONE;\n\t\tif (le16_to_cpu(data_transfer->flags) &\n\t\t\t\tSMB_DIRECT_RESPONSE_REQUESTED) {\n\t\t\tinfo->keep_alive_requested = KEEP_ALIVE_PENDING;\n\t\t}\n\n\t\treturn;\n\n\tdefault:\n\t\tlog_rdma_recv(ERR,\n\t\t\t\"unexpected response type=%d\\n\", response->type);\n\t}\n\nerror:\n\tput_receive_buffer(info, response);\n}\n\nstatic struct rdma_cm_id *smbd_create_id(\n\t\tstruct smbd_connection *info,\n\t\tstruct sockaddr *dstaddr, int port)\n{\n\tstruct rdma_cm_id *id;\n\tint rc;\n\t__be16 *sport;\n\n\tid = rdma_create_id(&init_net, smbd_conn_upcall, info,\n\t\tRDMA_PS_TCP, IB_QPT_RC);\n\tif (IS_ERR(id)) {\n\t\trc = PTR_ERR(id);\n\t\tlog_rdma_event(ERR, \"rdma_create_id() failed %i\\n\", rc);\n\t\treturn id;\n\t}\n\n\tif (dstaddr->sa_family == AF_INET6)\n\t\tsport = &((struct sockaddr_in6 *)dstaddr)->sin6_port;\n\telse\n\t\tsport = &((struct sockaddr_in *)dstaddr)->sin_port;\n\n\t*sport = htons(port);\n\n\tinit_completion(&info->ri_done);\n\tinfo->ri_rc = -ETIMEDOUT;\n\n\trc = rdma_resolve_addr(id, NULL, (struct sockaddr *)dstaddr,\n\t\tRDMA_RESOLVE_TIMEOUT);\n\tif (rc) {\n\t\tlog_rdma_event(ERR, \"rdma_resolve_addr() failed %i\\n\", rc);\n\t\tgoto out;\n\t}\n\trc = wait_for_completion_interruptible_timeout(\n\t\t&info->ri_done, msecs_to_jiffies(RDMA_RESOLVE_TIMEOUT));\n\t \n\tif (rc < 0) {\n\t\tlog_rdma_event(ERR, \"rdma_resolve_addr timeout rc: %i\\n\", rc);\n\t\tgoto out;\n\t}\n\trc = info->ri_rc;\n\tif (rc) {\n\t\tlog_rdma_event(ERR, \"rdma_resolve_addr() completed %i\\n\", rc);\n\t\tgoto out;\n\t}\n\n\tinfo->ri_rc = -ETIMEDOUT;\n\trc = rdma_resolve_route(id, RDMA_RESOLVE_TIMEOUT);\n\tif (rc) {\n\t\tlog_rdma_event(ERR, \"rdma_resolve_route() failed %i\\n\", rc);\n\t\tgoto out;\n\t}\n\trc = wait_for_completion_interruptible_timeout(\n\t\t&info->ri_done, msecs_to_jiffies(RDMA_RESOLVE_TIMEOUT));\n\t \n\tif (rc < 0)  {\n\t\tlog_rdma_event(ERR, \"rdma_resolve_addr timeout rc: %i\\n\", rc);\n\t\tgoto out;\n\t}\n\trc = info->ri_rc;\n\tif (rc) {\n\t\tlog_rdma_event(ERR, \"rdma_resolve_route() completed %i\\n\", rc);\n\t\tgoto out;\n\t}\n\n\treturn id;\n\nout:\n\trdma_destroy_id(id);\n\treturn ERR_PTR(rc);\n}\n\n \nstatic bool frwr_is_supported(struct ib_device_attr *attrs)\n{\n\tif (!(attrs->device_cap_flags & IB_DEVICE_MEM_MGT_EXTENSIONS))\n\t\treturn false;\n\tif (attrs->max_fast_reg_page_list_len == 0)\n\t\treturn false;\n\treturn true;\n}\n\nstatic int smbd_ia_open(\n\t\tstruct smbd_connection *info,\n\t\tstruct sockaddr *dstaddr, int port)\n{\n\tint rc;\n\n\tinfo->id = smbd_create_id(info, dstaddr, port);\n\tif (IS_ERR(info->id)) {\n\t\trc = PTR_ERR(info->id);\n\t\tgoto out1;\n\t}\n\n\tif (!frwr_is_supported(&info->id->device->attrs)) {\n\t\tlog_rdma_event(ERR, \"Fast Registration Work Requests (FRWR) is not supported\\n\");\n\t\tlog_rdma_event(ERR, \"Device capability flags = %llx max_fast_reg_page_list_len = %u\\n\",\n\t\t\t       info->id->device->attrs.device_cap_flags,\n\t\t\t       info->id->device->attrs.max_fast_reg_page_list_len);\n\t\trc = -EPROTONOSUPPORT;\n\t\tgoto out2;\n\t}\n\tinfo->max_frmr_depth = min_t(int,\n\t\tsmbd_max_frmr_depth,\n\t\tinfo->id->device->attrs.max_fast_reg_page_list_len);\n\tinfo->mr_type = IB_MR_TYPE_MEM_REG;\n\tif (info->id->device->attrs.kernel_cap_flags & IBK_SG_GAPS_REG)\n\t\tinfo->mr_type = IB_MR_TYPE_SG_GAPS;\n\n\tinfo->pd = ib_alloc_pd(info->id->device, 0);\n\tif (IS_ERR(info->pd)) {\n\t\trc = PTR_ERR(info->pd);\n\t\tlog_rdma_event(ERR, \"ib_alloc_pd() returned %d\\n\", rc);\n\t\tgoto out2;\n\t}\n\n\treturn 0;\n\nout2:\n\trdma_destroy_id(info->id);\n\tinfo->id = NULL;\n\nout1:\n\treturn rc;\n}\n\n \nstatic int smbd_post_send_negotiate_req(struct smbd_connection *info)\n{\n\tstruct ib_send_wr send_wr;\n\tint rc = -ENOMEM;\n\tstruct smbd_request *request;\n\tstruct smbd_negotiate_req *packet;\n\n\trequest = mempool_alloc(info->request_mempool, GFP_KERNEL);\n\tif (!request)\n\t\treturn rc;\n\n\trequest->info = info;\n\n\tpacket = smbd_request_payload(request);\n\tpacket->min_version = cpu_to_le16(SMBD_V1);\n\tpacket->max_version = cpu_to_le16(SMBD_V1);\n\tpacket->reserved = 0;\n\tpacket->credits_requested = cpu_to_le16(info->send_credit_target);\n\tpacket->preferred_send_size = cpu_to_le32(info->max_send_size);\n\tpacket->max_receive_size = cpu_to_le32(info->max_receive_size);\n\tpacket->max_fragmented_size =\n\t\tcpu_to_le32(info->max_fragmented_recv_size);\n\n\trequest->num_sge = 1;\n\trequest->sge[0].addr = ib_dma_map_single(\n\t\t\t\tinfo->id->device, (void *)packet,\n\t\t\t\tsizeof(*packet), DMA_TO_DEVICE);\n\tif (ib_dma_mapping_error(info->id->device, request->sge[0].addr)) {\n\t\trc = -EIO;\n\t\tgoto dma_mapping_failed;\n\t}\n\n\trequest->sge[0].length = sizeof(*packet);\n\trequest->sge[0].lkey = info->pd->local_dma_lkey;\n\n\tib_dma_sync_single_for_device(\n\t\tinfo->id->device, request->sge[0].addr,\n\t\trequest->sge[0].length, DMA_TO_DEVICE);\n\n\trequest->cqe.done = send_done;\n\n\tsend_wr.next = NULL;\n\tsend_wr.wr_cqe = &request->cqe;\n\tsend_wr.sg_list = request->sge;\n\tsend_wr.num_sge = request->num_sge;\n\tsend_wr.opcode = IB_WR_SEND;\n\tsend_wr.send_flags = IB_SEND_SIGNALED;\n\n\tlog_rdma_send(INFO, \"sge addr=0x%llx length=%u lkey=0x%x\\n\",\n\t\trequest->sge[0].addr,\n\t\trequest->sge[0].length, request->sge[0].lkey);\n\n\tatomic_inc(&info->send_pending);\n\trc = ib_post_send(info->id->qp, &send_wr, NULL);\n\tif (!rc)\n\t\treturn 0;\n\n\t \n\tlog_rdma_send(ERR, \"ib_post_send failed rc=%d\\n\", rc);\n\tatomic_dec(&info->send_pending);\n\tib_dma_unmap_single(info->id->device, request->sge[0].addr,\n\t\trequest->sge[0].length, DMA_TO_DEVICE);\n\n\tsmbd_disconnect_rdma_connection(info);\n\ndma_mapping_failed:\n\tmempool_free(request, info->request_mempool);\n\treturn rc;\n}\n\n \nstatic int manage_credits_prior_sending(struct smbd_connection *info)\n{\n\tint new_credits;\n\n\tspin_lock(&info->lock_new_credits_offered);\n\tnew_credits = info->new_credits_offered;\n\tinfo->new_credits_offered = 0;\n\tspin_unlock(&info->lock_new_credits_offered);\n\n\treturn new_credits;\n}\n\n \nstatic int manage_keep_alive_before_sending(struct smbd_connection *info)\n{\n\tif (info->keep_alive_requested == KEEP_ALIVE_PENDING) {\n\t\tinfo->keep_alive_requested = KEEP_ALIVE_SENT;\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\n \nstatic int smbd_post_send(struct smbd_connection *info,\n\t\tstruct smbd_request *request)\n{\n\tstruct ib_send_wr send_wr;\n\tint rc, i;\n\n\tfor (i = 0; i < request->num_sge; i++) {\n\t\tlog_rdma_send(INFO,\n\t\t\t\"rdma_request sge[%d] addr=0x%llx length=%u\\n\",\n\t\t\ti, request->sge[i].addr, request->sge[i].length);\n\t\tib_dma_sync_single_for_device(\n\t\t\tinfo->id->device,\n\t\t\trequest->sge[i].addr,\n\t\t\trequest->sge[i].length,\n\t\t\tDMA_TO_DEVICE);\n\t}\n\n\trequest->cqe.done = send_done;\n\n\tsend_wr.next = NULL;\n\tsend_wr.wr_cqe = &request->cqe;\n\tsend_wr.sg_list = request->sge;\n\tsend_wr.num_sge = request->num_sge;\n\tsend_wr.opcode = IB_WR_SEND;\n\tsend_wr.send_flags = IB_SEND_SIGNALED;\n\n\trc = ib_post_send(info->id->qp, &send_wr, NULL);\n\tif (rc) {\n\t\tlog_rdma_send(ERR, \"ib_post_send failed rc=%d\\n\", rc);\n\t\tsmbd_disconnect_rdma_connection(info);\n\t\trc = -EAGAIN;\n\t} else\n\t\t \n\t\tmod_delayed_work(info->workqueue, &info->idle_timer_work,\n\t\t\tinfo->keep_alive_interval*HZ);\n\n\treturn rc;\n}\n\nstatic int smbd_post_send_iter(struct smbd_connection *info,\n\t\t\t       struct iov_iter *iter,\n\t\t\t       int *_remaining_data_length)\n{\n\tint i, rc;\n\tint header_length;\n\tint data_length;\n\tstruct smbd_request *request;\n\tstruct smbd_data_transfer *packet;\n\tint new_credits = 0;\n\nwait_credit:\n\t \n\trc = wait_event_interruptible(info->wait_send_queue,\n\t\tatomic_read(&info->send_credits) > 0 ||\n\t\tinfo->transport_status != SMBD_CONNECTED);\n\tif (rc)\n\t\tgoto err_wait_credit;\n\n\tif (info->transport_status != SMBD_CONNECTED) {\n\t\tlog_outgoing(ERR, \"disconnected not sending on wait_credit\\n\");\n\t\trc = -EAGAIN;\n\t\tgoto err_wait_credit;\n\t}\n\tif (unlikely(atomic_dec_return(&info->send_credits) < 0)) {\n\t\tatomic_inc(&info->send_credits);\n\t\tgoto wait_credit;\n\t}\n\nwait_send_queue:\n\twait_event(info->wait_post_send,\n\t\tatomic_read(&info->send_pending) < info->send_credit_target ||\n\t\tinfo->transport_status != SMBD_CONNECTED);\n\n\tif (info->transport_status != SMBD_CONNECTED) {\n\t\tlog_outgoing(ERR, \"disconnected not sending on wait_send_queue\\n\");\n\t\trc = -EAGAIN;\n\t\tgoto err_wait_send_queue;\n\t}\n\n\tif (unlikely(atomic_inc_return(&info->send_pending) >\n\t\t\t\tinfo->send_credit_target)) {\n\t\tatomic_dec(&info->send_pending);\n\t\tgoto wait_send_queue;\n\t}\n\n\trequest = mempool_alloc(info->request_mempool, GFP_KERNEL);\n\tif (!request) {\n\t\trc = -ENOMEM;\n\t\tgoto err_alloc;\n\t}\n\n\trequest->info = info;\n\tmemset(request->sge, 0, sizeof(request->sge));\n\n\t \n\tif (iter) {\n\t\tstruct smb_extract_to_rdma extract = {\n\t\t\t.nr_sge\t\t= 1,\n\t\t\t.max_sge\t= SMBDIRECT_MAX_SEND_SGE,\n\t\t\t.sge\t\t= request->sge,\n\t\t\t.device\t\t= info->id->device,\n\t\t\t.local_dma_lkey\t= info->pd->local_dma_lkey,\n\t\t\t.direction\t= DMA_TO_DEVICE,\n\t\t};\n\n\t\trc = smb_extract_iter_to_rdma(iter, *_remaining_data_length,\n\t\t\t\t\t      &extract);\n\t\tif (rc < 0)\n\t\t\tgoto err_dma;\n\t\tdata_length = rc;\n\t\trequest->num_sge = extract.nr_sge;\n\t\t*_remaining_data_length -= data_length;\n\t} else {\n\t\tdata_length = 0;\n\t\trequest->num_sge = 1;\n\t}\n\n\t \n\tpacket = smbd_request_payload(request);\n\tpacket->credits_requested = cpu_to_le16(info->send_credit_target);\n\n\tnew_credits = manage_credits_prior_sending(info);\n\tatomic_add(new_credits, &info->receive_credits);\n\tpacket->credits_granted = cpu_to_le16(new_credits);\n\n\tinfo->send_immediate = false;\n\n\tpacket->flags = 0;\n\tif (manage_keep_alive_before_sending(info))\n\t\tpacket->flags |= cpu_to_le16(SMB_DIRECT_RESPONSE_REQUESTED);\n\n\tpacket->reserved = 0;\n\tif (!data_length)\n\t\tpacket->data_offset = 0;\n\telse\n\t\tpacket->data_offset = cpu_to_le32(24);\n\tpacket->data_length = cpu_to_le32(data_length);\n\tpacket->remaining_data_length = cpu_to_le32(*_remaining_data_length);\n\tpacket->padding = 0;\n\n\tlog_outgoing(INFO, \"credits_requested=%d credits_granted=%d data_offset=%d data_length=%d remaining_data_length=%d\\n\",\n\t\t     le16_to_cpu(packet->credits_requested),\n\t\t     le16_to_cpu(packet->credits_granted),\n\t\t     le32_to_cpu(packet->data_offset),\n\t\t     le32_to_cpu(packet->data_length),\n\t\t     le32_to_cpu(packet->remaining_data_length));\n\n\t \n\theader_length = sizeof(struct smbd_data_transfer);\n\t \n\tif (!data_length)\n\t\theader_length = offsetof(struct smbd_data_transfer, padding);\n\n\trequest->sge[0].addr = ib_dma_map_single(info->id->device,\n\t\t\t\t\t\t (void *)packet,\n\t\t\t\t\t\t header_length,\n\t\t\t\t\t\t DMA_TO_DEVICE);\n\tif (ib_dma_mapping_error(info->id->device, request->sge[0].addr)) {\n\t\trc = -EIO;\n\t\trequest->sge[0].addr = 0;\n\t\tgoto err_dma;\n\t}\n\n\trequest->sge[0].length = header_length;\n\trequest->sge[0].lkey = info->pd->local_dma_lkey;\n\n\trc = smbd_post_send(info, request);\n\tif (!rc)\n\t\treturn 0;\n\nerr_dma:\n\tfor (i = 0; i < request->num_sge; i++)\n\t\tif (request->sge[i].addr)\n\t\t\tib_dma_unmap_single(info->id->device,\n\t\t\t\t\t    request->sge[i].addr,\n\t\t\t\t\t    request->sge[i].length,\n\t\t\t\t\t    DMA_TO_DEVICE);\n\tmempool_free(request, info->request_mempool);\n\n\t \n\tspin_lock(&info->lock_new_credits_offered);\n\tinfo->new_credits_offered += new_credits;\n\tspin_unlock(&info->lock_new_credits_offered);\n\tatomic_sub(new_credits, &info->receive_credits);\n\nerr_alloc:\n\tif (atomic_dec_and_test(&info->send_pending))\n\t\twake_up(&info->wait_send_pending);\n\nerr_wait_send_queue:\n\t \n\tatomic_inc(&info->send_credits);\n\nerr_wait_credit:\n\treturn rc;\n}\n\n \nstatic int smbd_post_send_empty(struct smbd_connection *info)\n{\n\tint remaining_data_length = 0;\n\n\tinfo->count_send_empty++;\n\treturn smbd_post_send_iter(info, NULL, &remaining_data_length);\n}\n\n \nstatic int smbd_post_recv(\n\t\tstruct smbd_connection *info, struct smbd_response *response)\n{\n\tstruct ib_recv_wr recv_wr;\n\tint rc = -EIO;\n\n\tresponse->sge.addr = ib_dma_map_single(\n\t\t\t\tinfo->id->device, response->packet,\n\t\t\t\tinfo->max_receive_size, DMA_FROM_DEVICE);\n\tif (ib_dma_mapping_error(info->id->device, response->sge.addr))\n\t\treturn rc;\n\n\tresponse->sge.length = info->max_receive_size;\n\tresponse->sge.lkey = info->pd->local_dma_lkey;\n\n\tresponse->cqe.done = recv_done;\n\n\trecv_wr.wr_cqe = &response->cqe;\n\trecv_wr.next = NULL;\n\trecv_wr.sg_list = &response->sge;\n\trecv_wr.num_sge = 1;\n\n\trc = ib_post_recv(info->id->qp, &recv_wr, NULL);\n\tif (rc) {\n\t\tib_dma_unmap_single(info->id->device, response->sge.addr,\n\t\t\t\t    response->sge.length, DMA_FROM_DEVICE);\n\t\tsmbd_disconnect_rdma_connection(info);\n\t\tlog_rdma_recv(ERR, \"ib_post_recv failed rc=%d\\n\", rc);\n\t}\n\n\treturn rc;\n}\n\n \nstatic int smbd_negotiate(struct smbd_connection *info)\n{\n\tint rc;\n\tstruct smbd_response *response = get_receive_buffer(info);\n\n\tresponse->type = SMBD_NEGOTIATE_RESP;\n\trc = smbd_post_recv(info, response);\n\tlog_rdma_event(INFO, \"smbd_post_recv rc=%d iov.addr=0x%llx iov.length=%u iov.lkey=0x%x\\n\",\n\t\t       rc, response->sge.addr,\n\t\t       response->sge.length, response->sge.lkey);\n\tif (rc)\n\t\treturn rc;\n\n\tinit_completion(&info->negotiate_completion);\n\tinfo->negotiate_done = false;\n\trc = smbd_post_send_negotiate_req(info);\n\tif (rc)\n\t\treturn rc;\n\n\trc = wait_for_completion_interruptible_timeout(\n\t\t&info->negotiate_completion, SMBD_NEGOTIATE_TIMEOUT * HZ);\n\tlog_rdma_event(INFO, \"wait_for_completion_timeout rc=%d\\n\", rc);\n\n\tif (info->negotiate_done)\n\t\treturn 0;\n\n\tif (rc == 0)\n\t\trc = -ETIMEDOUT;\n\telse if (rc == -ERESTARTSYS)\n\t\trc = -EINTR;\n\telse\n\t\trc = -ENOTCONN;\n\n\treturn rc;\n}\n\nstatic void put_empty_packet(\n\t\tstruct smbd_connection *info, struct smbd_response *response)\n{\n\tspin_lock(&info->empty_packet_queue_lock);\n\tlist_add_tail(&response->list, &info->empty_packet_queue);\n\tinfo->count_empty_packet_queue++;\n\tspin_unlock(&info->empty_packet_queue_lock);\n\n\tqueue_work(info->workqueue, &info->post_send_credits_work);\n}\n\n \nstatic void enqueue_reassembly(\n\tstruct smbd_connection *info,\n\tstruct smbd_response *response,\n\tint data_length)\n{\n\tspin_lock(&info->reassembly_queue_lock);\n\tlist_add_tail(&response->list, &info->reassembly_queue);\n\tinfo->reassembly_queue_length++;\n\t \n\tvirt_wmb();\n\tinfo->reassembly_data_length += data_length;\n\tspin_unlock(&info->reassembly_queue_lock);\n\tinfo->count_reassembly_queue++;\n\tinfo->count_enqueue_reassembly_queue++;\n}\n\n \nstatic struct smbd_response *_get_first_reassembly(struct smbd_connection *info)\n{\n\tstruct smbd_response *ret = NULL;\n\n\tif (!list_empty(&info->reassembly_queue)) {\n\t\tret = list_first_entry(\n\t\t\t&info->reassembly_queue,\n\t\t\tstruct smbd_response, list);\n\t}\n\treturn ret;\n}\n\nstatic struct smbd_response *get_empty_queue_buffer(\n\t\tstruct smbd_connection *info)\n{\n\tstruct smbd_response *ret = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&info->empty_packet_queue_lock, flags);\n\tif (!list_empty(&info->empty_packet_queue)) {\n\t\tret = list_first_entry(\n\t\t\t&info->empty_packet_queue,\n\t\t\tstruct smbd_response, list);\n\t\tlist_del(&ret->list);\n\t\tinfo->count_empty_packet_queue--;\n\t}\n\tspin_unlock_irqrestore(&info->empty_packet_queue_lock, flags);\n\n\treturn ret;\n}\n\n \nstatic struct smbd_response *get_receive_buffer(struct smbd_connection *info)\n{\n\tstruct smbd_response *ret = NULL;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&info->receive_queue_lock, flags);\n\tif (!list_empty(&info->receive_queue)) {\n\t\tret = list_first_entry(\n\t\t\t&info->receive_queue,\n\t\t\tstruct smbd_response, list);\n\t\tlist_del(&ret->list);\n\t\tinfo->count_receive_queue--;\n\t\tinfo->count_get_receive_buffer++;\n\t}\n\tspin_unlock_irqrestore(&info->receive_queue_lock, flags);\n\n\treturn ret;\n}\n\n \nstatic void put_receive_buffer(\n\tstruct smbd_connection *info, struct smbd_response *response)\n{\n\tunsigned long flags;\n\n\tib_dma_unmap_single(info->id->device, response->sge.addr,\n\t\tresponse->sge.length, DMA_FROM_DEVICE);\n\n\tspin_lock_irqsave(&info->receive_queue_lock, flags);\n\tlist_add_tail(&response->list, &info->receive_queue);\n\tinfo->count_receive_queue++;\n\tinfo->count_put_receive_buffer++;\n\tspin_unlock_irqrestore(&info->receive_queue_lock, flags);\n\n\tqueue_work(info->workqueue, &info->post_send_credits_work);\n}\n\n \nstatic int allocate_receive_buffers(struct smbd_connection *info, int num_buf)\n{\n\tint i;\n\tstruct smbd_response *response;\n\n\tINIT_LIST_HEAD(&info->reassembly_queue);\n\tspin_lock_init(&info->reassembly_queue_lock);\n\tinfo->reassembly_data_length = 0;\n\tinfo->reassembly_queue_length = 0;\n\n\tINIT_LIST_HEAD(&info->receive_queue);\n\tspin_lock_init(&info->receive_queue_lock);\n\tinfo->count_receive_queue = 0;\n\n\tINIT_LIST_HEAD(&info->empty_packet_queue);\n\tspin_lock_init(&info->empty_packet_queue_lock);\n\tinfo->count_empty_packet_queue = 0;\n\n\tinit_waitqueue_head(&info->wait_receive_queues);\n\n\tfor (i = 0; i < num_buf; i++) {\n\t\tresponse = mempool_alloc(info->response_mempool, GFP_KERNEL);\n\t\tif (!response)\n\t\t\tgoto allocate_failed;\n\n\t\tresponse->info = info;\n\t\tlist_add_tail(&response->list, &info->receive_queue);\n\t\tinfo->count_receive_queue++;\n\t}\n\n\treturn 0;\n\nallocate_failed:\n\twhile (!list_empty(&info->receive_queue)) {\n\t\tresponse = list_first_entry(\n\t\t\t\t&info->receive_queue,\n\t\t\t\tstruct smbd_response, list);\n\t\tlist_del(&response->list);\n\t\tinfo->count_receive_queue--;\n\n\t\tmempool_free(response, info->response_mempool);\n\t}\n\treturn -ENOMEM;\n}\n\nstatic void destroy_receive_buffers(struct smbd_connection *info)\n{\n\tstruct smbd_response *response;\n\n\twhile ((response = get_receive_buffer(info)))\n\t\tmempool_free(response, info->response_mempool);\n\n\twhile ((response = get_empty_queue_buffer(info)))\n\t\tmempool_free(response, info->response_mempool);\n}\n\n \nstatic void idle_connection_timer(struct work_struct *work)\n{\n\tstruct smbd_connection *info = container_of(\n\t\t\t\t\twork, struct smbd_connection,\n\t\t\t\t\tidle_timer_work.work);\n\n\tif (info->keep_alive_requested != KEEP_ALIVE_NONE) {\n\t\tlog_keep_alive(ERR,\n\t\t\t\"error status info->keep_alive_requested=%d\\n\",\n\t\t\tinfo->keep_alive_requested);\n\t\tsmbd_disconnect_rdma_connection(info);\n\t\treturn;\n\t}\n\n\tlog_keep_alive(INFO, \"about to send an empty idle message\\n\");\n\tsmbd_post_send_empty(info);\n\n\t \n\tqueue_delayed_work(info->workqueue, &info->idle_timer_work,\n\t\t\tinfo->keep_alive_interval*HZ);\n}\n\n \nvoid smbd_destroy(struct TCP_Server_Info *server)\n{\n\tstruct smbd_connection *info = server->smbd_conn;\n\tstruct smbd_response *response;\n\tunsigned long flags;\n\n\tif (!info) {\n\t\tlog_rdma_event(INFO, \"rdma session already destroyed\\n\");\n\t\treturn;\n\t}\n\n\tlog_rdma_event(INFO, \"destroying rdma session\\n\");\n\tif (info->transport_status != SMBD_DISCONNECTED) {\n\t\trdma_disconnect(server->smbd_conn->id);\n\t\tlog_rdma_event(INFO, \"wait for transport being disconnected\\n\");\n\t\twait_event_interruptible(\n\t\t\tinfo->disconn_wait,\n\t\t\tinfo->transport_status == SMBD_DISCONNECTED);\n\t}\n\n\tlog_rdma_event(INFO, \"destroying qp\\n\");\n\tib_drain_qp(info->id->qp);\n\trdma_destroy_qp(info->id);\n\n\tlog_rdma_event(INFO, \"cancelling idle timer\\n\");\n\tcancel_delayed_work_sync(&info->idle_timer_work);\n\n\tlog_rdma_event(INFO, \"wait for all send posted to IB to finish\\n\");\n\twait_event(info->wait_send_pending,\n\t\tatomic_read(&info->send_pending) == 0);\n\n\t \n\tlog_rdma_event(INFO, \"drain the reassembly queue\\n\");\n\tdo {\n\t\tspin_lock_irqsave(&info->reassembly_queue_lock, flags);\n\t\tresponse = _get_first_reassembly(info);\n\t\tif (response) {\n\t\t\tlist_del(&response->list);\n\t\t\tspin_unlock_irqrestore(\n\t\t\t\t&info->reassembly_queue_lock, flags);\n\t\t\tput_receive_buffer(info, response);\n\t\t} else\n\t\t\tspin_unlock_irqrestore(\n\t\t\t\t&info->reassembly_queue_lock, flags);\n\t} while (response);\n\tinfo->reassembly_data_length = 0;\n\n\tlog_rdma_event(INFO, \"free receive buffers\\n\");\n\twait_event(info->wait_receive_queues,\n\t\tinfo->count_receive_queue + info->count_empty_packet_queue\n\t\t\t== info->receive_credit_max);\n\tdestroy_receive_buffers(info);\n\n\t \n\tlog_rdma_event(INFO, \"freeing mr list\\n\");\n\twake_up_interruptible_all(&info->wait_mr);\n\twhile (atomic_read(&info->mr_used_count)) {\n\t\tcifs_server_unlock(server);\n\t\tmsleep(1000);\n\t\tcifs_server_lock(server);\n\t}\n\tdestroy_mr_list(info);\n\n\tib_free_cq(info->send_cq);\n\tib_free_cq(info->recv_cq);\n\tib_dealloc_pd(info->pd);\n\trdma_destroy_id(info->id);\n\n\t \n\tmempool_destroy(info->request_mempool);\n\tkmem_cache_destroy(info->request_cache);\n\n\tmempool_destroy(info->response_mempool);\n\tkmem_cache_destroy(info->response_cache);\n\n\tinfo->transport_status = SMBD_DESTROYED;\n\n\tdestroy_workqueue(info->workqueue);\n\tlog_rdma_event(INFO,  \"rdma session destroyed\\n\");\n\tkfree(info);\n\tserver->smbd_conn = NULL;\n}\n\n \nint smbd_reconnect(struct TCP_Server_Info *server)\n{\n\tlog_rdma_event(INFO, \"reconnecting rdma session\\n\");\n\n\tif (!server->smbd_conn) {\n\t\tlog_rdma_event(INFO, \"rdma session already destroyed\\n\");\n\t\tgoto create_conn;\n\t}\n\n\t \n\tif (server->smbd_conn->transport_status == SMBD_CONNECTED) {\n\t\tlog_rdma_event(INFO, \"disconnecting transport\\n\");\n\t\tsmbd_destroy(server);\n\t}\n\ncreate_conn:\n\tlog_rdma_event(INFO, \"creating rdma session\\n\");\n\tserver->smbd_conn = smbd_get_connection(\n\t\tserver, (struct sockaddr *) &server->dstaddr);\n\n\tif (server->smbd_conn) {\n\t\tcifs_dbg(VFS, \"RDMA transport re-established\\n\");\n\t\ttrace_smb3_smbd_connect_done(server->hostname, server->conn_id, &server->dstaddr);\n\t\treturn 0;\n\t}\n\ttrace_smb3_smbd_connect_err(server->hostname, server->conn_id, &server->dstaddr);\n\treturn -ENOENT;\n}\n\nstatic void destroy_caches_and_workqueue(struct smbd_connection *info)\n{\n\tdestroy_receive_buffers(info);\n\tdestroy_workqueue(info->workqueue);\n\tmempool_destroy(info->response_mempool);\n\tkmem_cache_destroy(info->response_cache);\n\tmempool_destroy(info->request_mempool);\n\tkmem_cache_destroy(info->request_cache);\n}\n\n#define MAX_NAME_LEN\t80\nstatic int allocate_caches_and_workqueue(struct smbd_connection *info)\n{\n\tchar name[MAX_NAME_LEN];\n\tint rc;\n\n\tscnprintf(name, MAX_NAME_LEN, \"smbd_request_%p\", info);\n\tinfo->request_cache =\n\t\tkmem_cache_create(\n\t\t\tname,\n\t\t\tsizeof(struct smbd_request) +\n\t\t\t\tsizeof(struct smbd_data_transfer),\n\t\t\t0, SLAB_HWCACHE_ALIGN, NULL);\n\tif (!info->request_cache)\n\t\treturn -ENOMEM;\n\n\tinfo->request_mempool =\n\t\tmempool_create(info->send_credit_target, mempool_alloc_slab,\n\t\t\tmempool_free_slab, info->request_cache);\n\tif (!info->request_mempool)\n\t\tgoto out1;\n\n\tscnprintf(name, MAX_NAME_LEN, \"smbd_response_%p\", info);\n\tinfo->response_cache =\n\t\tkmem_cache_create(\n\t\t\tname,\n\t\t\tsizeof(struct smbd_response) +\n\t\t\t\tinfo->max_receive_size,\n\t\t\t0, SLAB_HWCACHE_ALIGN, NULL);\n\tif (!info->response_cache)\n\t\tgoto out2;\n\n\tinfo->response_mempool =\n\t\tmempool_create(info->receive_credit_max, mempool_alloc_slab,\n\t\t       mempool_free_slab, info->response_cache);\n\tif (!info->response_mempool)\n\t\tgoto out3;\n\n\tscnprintf(name, MAX_NAME_LEN, \"smbd_%p\", info);\n\tinfo->workqueue = create_workqueue(name);\n\tif (!info->workqueue)\n\t\tgoto out4;\n\n\trc = allocate_receive_buffers(info, info->receive_credit_max);\n\tif (rc) {\n\t\tlog_rdma_event(ERR, \"failed to allocate receive buffers\\n\");\n\t\tgoto out5;\n\t}\n\n\treturn 0;\n\nout5:\n\tdestroy_workqueue(info->workqueue);\nout4:\n\tmempool_destroy(info->response_mempool);\nout3:\n\tkmem_cache_destroy(info->response_cache);\nout2:\n\tmempool_destroy(info->request_mempool);\nout1:\n\tkmem_cache_destroy(info->request_cache);\n\treturn -ENOMEM;\n}\n\n \nstatic struct smbd_connection *_smbd_get_connection(\n\tstruct TCP_Server_Info *server, struct sockaddr *dstaddr, int port)\n{\n\tint rc;\n\tstruct smbd_connection *info;\n\tstruct rdma_conn_param conn_param;\n\tstruct ib_qp_init_attr qp_attr;\n\tstruct sockaddr_in *addr_in = (struct sockaddr_in *) dstaddr;\n\tstruct ib_port_immutable port_immutable;\n\tu32 ird_ord_hdr[2];\n\n\tinfo = kzalloc(sizeof(struct smbd_connection), GFP_KERNEL);\n\tif (!info)\n\t\treturn NULL;\n\n\tinfo->transport_status = SMBD_CONNECTING;\n\trc = smbd_ia_open(info, dstaddr, port);\n\tif (rc) {\n\t\tlog_rdma_event(INFO, \"smbd_ia_open rc=%d\\n\", rc);\n\t\tgoto create_id_failed;\n\t}\n\n\tif (smbd_send_credit_target > info->id->device->attrs.max_cqe ||\n\t    smbd_send_credit_target > info->id->device->attrs.max_qp_wr) {\n\t\tlog_rdma_event(ERR, \"consider lowering send_credit_target = %d. Possible CQE overrun, device reporting max_cqe %d max_qp_wr %d\\n\",\n\t\t\t       smbd_send_credit_target,\n\t\t\t       info->id->device->attrs.max_cqe,\n\t\t\t       info->id->device->attrs.max_qp_wr);\n\t\tgoto config_failed;\n\t}\n\n\tif (smbd_receive_credit_max > info->id->device->attrs.max_cqe ||\n\t    smbd_receive_credit_max > info->id->device->attrs.max_qp_wr) {\n\t\tlog_rdma_event(ERR, \"consider lowering receive_credit_max = %d. Possible CQE overrun, device reporting max_cqe %d max_qp_wr %d\\n\",\n\t\t\t       smbd_receive_credit_max,\n\t\t\t       info->id->device->attrs.max_cqe,\n\t\t\t       info->id->device->attrs.max_qp_wr);\n\t\tgoto config_failed;\n\t}\n\n\tinfo->receive_credit_max = smbd_receive_credit_max;\n\tinfo->send_credit_target = smbd_send_credit_target;\n\tinfo->max_send_size = smbd_max_send_size;\n\tinfo->max_fragmented_recv_size = smbd_max_fragmented_recv_size;\n\tinfo->max_receive_size = smbd_max_receive_size;\n\tinfo->keep_alive_interval = smbd_keep_alive_interval;\n\n\tif (info->id->device->attrs.max_send_sge < SMBDIRECT_MAX_SEND_SGE ||\n\t    info->id->device->attrs.max_recv_sge < SMBDIRECT_MAX_RECV_SGE) {\n\t\tlog_rdma_event(ERR,\n\t\t\t\"device %.*s max_send_sge/max_recv_sge = %d/%d too small\\n\",\n\t\t\tIB_DEVICE_NAME_MAX,\n\t\t\tinfo->id->device->name,\n\t\t\tinfo->id->device->attrs.max_send_sge,\n\t\t\tinfo->id->device->attrs.max_recv_sge);\n\t\tgoto config_failed;\n\t}\n\n\tinfo->send_cq = NULL;\n\tinfo->recv_cq = NULL;\n\tinfo->send_cq =\n\t\tib_alloc_cq_any(info->id->device, info,\n\t\t\t\tinfo->send_credit_target, IB_POLL_SOFTIRQ);\n\tif (IS_ERR(info->send_cq)) {\n\t\tinfo->send_cq = NULL;\n\t\tgoto alloc_cq_failed;\n\t}\n\n\tinfo->recv_cq =\n\t\tib_alloc_cq_any(info->id->device, info,\n\t\t\t\tinfo->receive_credit_max, IB_POLL_SOFTIRQ);\n\tif (IS_ERR(info->recv_cq)) {\n\t\tinfo->recv_cq = NULL;\n\t\tgoto alloc_cq_failed;\n\t}\n\n\tmemset(&qp_attr, 0, sizeof(qp_attr));\n\tqp_attr.event_handler = smbd_qp_async_error_upcall;\n\tqp_attr.qp_context = info;\n\tqp_attr.cap.max_send_wr = info->send_credit_target;\n\tqp_attr.cap.max_recv_wr = info->receive_credit_max;\n\tqp_attr.cap.max_send_sge = SMBDIRECT_MAX_SEND_SGE;\n\tqp_attr.cap.max_recv_sge = SMBDIRECT_MAX_RECV_SGE;\n\tqp_attr.cap.max_inline_data = 0;\n\tqp_attr.sq_sig_type = IB_SIGNAL_REQ_WR;\n\tqp_attr.qp_type = IB_QPT_RC;\n\tqp_attr.send_cq = info->send_cq;\n\tqp_attr.recv_cq = info->recv_cq;\n\tqp_attr.port_num = ~0;\n\n\trc = rdma_create_qp(info->id, info->pd, &qp_attr);\n\tif (rc) {\n\t\tlog_rdma_event(ERR, \"rdma_create_qp failed %i\\n\", rc);\n\t\tgoto create_qp_failed;\n\t}\n\n\tmemset(&conn_param, 0, sizeof(conn_param));\n\tconn_param.initiator_depth = 0;\n\n\tconn_param.responder_resources =\n\t\tinfo->id->device->attrs.max_qp_rd_atom\n\t\t\t< SMBD_CM_RESPONDER_RESOURCES ?\n\t\tinfo->id->device->attrs.max_qp_rd_atom :\n\t\tSMBD_CM_RESPONDER_RESOURCES;\n\tinfo->responder_resources = conn_param.responder_resources;\n\tlog_rdma_mr(INFO, \"responder_resources=%d\\n\",\n\t\tinfo->responder_resources);\n\n\t \n\tinfo->id->device->ops.get_port_immutable(\n\t\tinfo->id->device, info->id->port_num, &port_immutable);\n\tif (port_immutable.core_cap_flags & RDMA_CORE_PORT_IWARP) {\n\t\tird_ord_hdr[0] = info->responder_resources;\n\t\tird_ord_hdr[1] = 1;\n\t\tconn_param.private_data = ird_ord_hdr;\n\t\tconn_param.private_data_len = sizeof(ird_ord_hdr);\n\t} else {\n\t\tconn_param.private_data = NULL;\n\t\tconn_param.private_data_len = 0;\n\t}\n\n\tconn_param.retry_count = SMBD_CM_RETRY;\n\tconn_param.rnr_retry_count = SMBD_CM_RNR_RETRY;\n\tconn_param.flow_control = 0;\n\n\tlog_rdma_event(INFO, \"connecting to IP %pI4 port %d\\n\",\n\t\t&addr_in->sin_addr, port);\n\n\tinit_waitqueue_head(&info->conn_wait);\n\tinit_waitqueue_head(&info->disconn_wait);\n\tinit_waitqueue_head(&info->wait_reassembly_queue);\n\trc = rdma_connect(info->id, &conn_param);\n\tif (rc) {\n\t\tlog_rdma_event(ERR, \"rdma_connect() failed with %i\\n\", rc);\n\t\tgoto rdma_connect_failed;\n\t}\n\n\twait_event_interruptible(\n\t\tinfo->conn_wait, info->transport_status != SMBD_CONNECTING);\n\n\tif (info->transport_status != SMBD_CONNECTED) {\n\t\tlog_rdma_event(ERR, \"rdma_connect failed port=%d\\n\", port);\n\t\tgoto rdma_connect_failed;\n\t}\n\n\tlog_rdma_event(INFO, \"rdma_connect connected\\n\");\n\n\trc = allocate_caches_and_workqueue(info);\n\tif (rc) {\n\t\tlog_rdma_event(ERR, \"cache allocation failed\\n\");\n\t\tgoto allocate_cache_failed;\n\t}\n\n\tinit_waitqueue_head(&info->wait_send_queue);\n\tINIT_DELAYED_WORK(&info->idle_timer_work, idle_connection_timer);\n\tqueue_delayed_work(info->workqueue, &info->idle_timer_work,\n\t\tinfo->keep_alive_interval*HZ);\n\n\tinit_waitqueue_head(&info->wait_send_pending);\n\tatomic_set(&info->send_pending, 0);\n\n\tinit_waitqueue_head(&info->wait_post_send);\n\n\tINIT_WORK(&info->disconnect_work, smbd_disconnect_rdma_work);\n\tINIT_WORK(&info->post_send_credits_work, smbd_post_send_credits);\n\tinfo->new_credits_offered = 0;\n\tspin_lock_init(&info->lock_new_credits_offered);\n\n\trc = smbd_negotiate(info);\n\tif (rc) {\n\t\tlog_rdma_event(ERR, \"smbd_negotiate rc=%d\\n\", rc);\n\t\tgoto negotiation_failed;\n\t}\n\n\trc = allocate_mr_list(info);\n\tif (rc) {\n\t\tlog_rdma_mr(ERR, \"memory registration allocation failed\\n\");\n\t\tgoto allocate_mr_failed;\n\t}\n\n\treturn info;\n\nallocate_mr_failed:\n\t \n\tserver->smbd_conn = info;\n\tsmbd_destroy(server);\n\treturn NULL;\n\nnegotiation_failed:\n\tcancel_delayed_work_sync(&info->idle_timer_work);\n\tdestroy_caches_and_workqueue(info);\n\tinfo->transport_status = SMBD_NEGOTIATE_FAILED;\n\tinit_waitqueue_head(&info->conn_wait);\n\trdma_disconnect(info->id);\n\twait_event(info->conn_wait,\n\t\tinfo->transport_status == SMBD_DISCONNECTED);\n\nallocate_cache_failed:\nrdma_connect_failed:\n\trdma_destroy_qp(info->id);\n\ncreate_qp_failed:\nalloc_cq_failed:\n\tif (info->send_cq)\n\t\tib_free_cq(info->send_cq);\n\tif (info->recv_cq)\n\t\tib_free_cq(info->recv_cq);\n\nconfig_failed:\n\tib_dealloc_pd(info->pd);\n\trdma_destroy_id(info->id);\n\ncreate_id_failed:\n\tkfree(info);\n\treturn NULL;\n}\n\nstruct smbd_connection *smbd_get_connection(\n\tstruct TCP_Server_Info *server, struct sockaddr *dstaddr)\n{\n\tstruct smbd_connection *ret;\n\tint port = SMBD_PORT;\n\ntry_again:\n\tret = _smbd_get_connection(server, dstaddr, port);\n\n\t \n\tif (!ret && port == SMBD_PORT) {\n\t\tport = SMB_PORT;\n\t\tgoto try_again;\n\t}\n\treturn ret;\n}\n\n \nstatic int smbd_recv_buf(struct smbd_connection *info, char *buf,\n\t\tunsigned int size)\n{\n\tstruct smbd_response *response;\n\tstruct smbd_data_transfer *data_transfer;\n\tint to_copy, to_read, data_read, offset;\n\tu32 data_length, remaining_data_length, data_offset;\n\tint rc;\n\nagain:\n\t \n\tlog_read(INFO, \"size=%d info->reassembly_data_length=%d\\n\", size,\n\t\tinfo->reassembly_data_length);\n\tif (info->reassembly_data_length >= size) {\n\t\tint queue_length;\n\t\tint queue_removed = 0;\n\n\t\t \n\t\tvirt_rmb();\n\t\tqueue_length = info->reassembly_queue_length;\n\t\tdata_read = 0;\n\t\tto_read = size;\n\t\toffset = info->first_entry_offset;\n\t\twhile (data_read < size) {\n\t\t\tresponse = _get_first_reassembly(info);\n\t\t\tdata_transfer = smbd_response_payload(response);\n\t\t\tdata_length = le32_to_cpu(data_transfer->data_length);\n\t\t\tremaining_data_length =\n\t\t\t\tle32_to_cpu(\n\t\t\t\t\tdata_transfer->remaining_data_length);\n\t\t\tdata_offset = le32_to_cpu(data_transfer->data_offset);\n\n\t\t\t \n\t\t\tif (response->first_segment && size == 4) {\n\t\t\t\tunsigned int rfc1002_len =\n\t\t\t\t\tdata_length + remaining_data_length;\n\t\t\t\t*((__be32 *)buf) = cpu_to_be32(rfc1002_len);\n\t\t\t\tdata_read = 4;\n\t\t\t\tresponse->first_segment = false;\n\t\t\t\tlog_read(INFO, \"returning rfc1002 length %d\\n\",\n\t\t\t\t\trfc1002_len);\n\t\t\t\tgoto read_rfc1002_done;\n\t\t\t}\n\n\t\t\tto_copy = min_t(int, data_length - offset, to_read);\n\t\t\tmemcpy(\n\t\t\t\tbuf + data_read,\n\t\t\t\t(char *)data_transfer + data_offset + offset,\n\t\t\t\tto_copy);\n\n\t\t\t \n\t\t\tif (to_copy == data_length - offset) {\n\t\t\t\tqueue_length--;\n\t\t\t\t \n\t\t\t\tif (queue_length)\n\t\t\t\t\tlist_del(&response->list);\n\t\t\t\telse {\n\t\t\t\t\tspin_lock_irq(\n\t\t\t\t\t\t&info->reassembly_queue_lock);\n\t\t\t\t\tlist_del(&response->list);\n\t\t\t\t\tspin_unlock_irq(\n\t\t\t\t\t\t&info->reassembly_queue_lock);\n\t\t\t\t}\n\t\t\t\tqueue_removed++;\n\t\t\t\tinfo->count_reassembly_queue--;\n\t\t\t\tinfo->count_dequeue_reassembly_queue++;\n\t\t\t\tput_receive_buffer(info, response);\n\t\t\t\toffset = 0;\n\t\t\t\tlog_read(INFO, \"put_receive_buffer offset=0\\n\");\n\t\t\t} else\n\t\t\t\toffset += to_copy;\n\n\t\t\tto_read -= to_copy;\n\t\t\tdata_read += to_copy;\n\n\t\t\tlog_read(INFO, \"_get_first_reassembly memcpy %d bytes data_transfer_length-offset=%d after that to_read=%d data_read=%d offset=%d\\n\",\n\t\t\t\t to_copy, data_length - offset,\n\t\t\t\t to_read, data_read, offset);\n\t\t}\n\n\t\tspin_lock_irq(&info->reassembly_queue_lock);\n\t\tinfo->reassembly_data_length -= data_read;\n\t\tinfo->reassembly_queue_length -= queue_removed;\n\t\tspin_unlock_irq(&info->reassembly_queue_lock);\n\n\t\tinfo->first_entry_offset = offset;\n\t\tlog_read(INFO, \"returning to thread data_read=%d reassembly_data_length=%d first_entry_offset=%d\\n\",\n\t\t\t data_read, info->reassembly_data_length,\n\t\t\t info->first_entry_offset);\nread_rfc1002_done:\n\t\treturn data_read;\n\t}\n\n\tlog_read(INFO, \"wait_event on more data\\n\");\n\trc = wait_event_interruptible(\n\t\tinfo->wait_reassembly_queue,\n\t\tinfo->reassembly_data_length >= size ||\n\t\t\tinfo->transport_status != SMBD_CONNECTED);\n\t \n\tif (rc)\n\t\treturn rc;\n\n\tif (info->transport_status != SMBD_CONNECTED) {\n\t\tlog_read(ERR, \"disconnected\\n\");\n\t\treturn -ECONNABORTED;\n\t}\n\n\tgoto again;\n}\n\n \nstatic int smbd_recv_page(struct smbd_connection *info,\n\t\tstruct page *page, unsigned int page_offset,\n\t\tunsigned int to_read)\n{\n\tint ret;\n\tchar *to_address;\n\tvoid *page_address;\n\n\t \n\tret = wait_event_interruptible(\n\t\tinfo->wait_reassembly_queue,\n\t\tinfo->reassembly_data_length >= to_read ||\n\t\t\tinfo->transport_status != SMBD_CONNECTED);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tpage_address = kmap_atomic(page);\n\tto_address = (char *) page_address + page_offset;\n\n\tlog_read(INFO, \"reading from page=%p address=%p to_read=%d\\n\",\n\t\tpage, to_address, to_read);\n\n\tret = smbd_recv_buf(info, to_address, to_read);\n\tkunmap_atomic(page_address);\n\n\treturn ret;\n}\n\n \nint smbd_recv(struct smbd_connection *info, struct msghdr *msg)\n{\n\tchar *buf;\n\tstruct page *page;\n\tunsigned int to_read, page_offset;\n\tint rc;\n\n\tif (iov_iter_rw(&msg->msg_iter) == WRITE) {\n\t\t \n\t\tcifs_dbg(VFS, \"Invalid msg iter dir %u\\n\",\n\t\t\t iov_iter_rw(&msg->msg_iter));\n\t\trc = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tswitch (iov_iter_type(&msg->msg_iter)) {\n\tcase ITER_KVEC:\n\t\tbuf = msg->msg_iter.kvec->iov_base;\n\t\tto_read = msg->msg_iter.kvec->iov_len;\n\t\trc = smbd_recv_buf(info, buf, to_read);\n\t\tbreak;\n\n\tcase ITER_BVEC:\n\t\tpage = msg->msg_iter.bvec->bv_page;\n\t\tpage_offset = msg->msg_iter.bvec->bv_offset;\n\t\tto_read = msg->msg_iter.bvec->bv_len;\n\t\trc = smbd_recv_page(info, page, page_offset, to_read);\n\t\tbreak;\n\n\tdefault:\n\t\t \n\t\tcifs_dbg(VFS, \"Invalid msg type %d\\n\",\n\t\t\t iov_iter_type(&msg->msg_iter));\n\t\trc = -EINVAL;\n\t}\n\nout:\n\t \n\tif (rc > 0)\n\t\tmsg->msg_iter.count = 0;\n\treturn rc;\n}\n\n \nint smbd_send(struct TCP_Server_Info *server,\n\tint num_rqst, struct smb_rqst *rqst_array)\n{\n\tstruct smbd_connection *info = server->smbd_conn;\n\tstruct smb_rqst *rqst;\n\tstruct iov_iter iter;\n\tunsigned int remaining_data_length, klen;\n\tint rc, i, rqst_idx;\n\n\tif (info->transport_status != SMBD_CONNECTED)\n\t\treturn -EAGAIN;\n\n\t \n\tremaining_data_length = 0;\n\tfor (i = 0; i < num_rqst; i++)\n\t\tremaining_data_length += smb_rqst_len(server, &rqst_array[i]);\n\n\tif (unlikely(remaining_data_length > info->max_fragmented_send_size)) {\n\t\t \n\t\tlog_write(ERR, \"payload size %d > max size %d\\n\",\n\t\t\tremaining_data_length, info->max_fragmented_send_size);\n\t\treturn -EINVAL;\n\t}\n\n\tlog_write(INFO, \"num_rqst=%d total length=%u\\n\",\n\t\t\tnum_rqst, remaining_data_length);\n\n\trqst_idx = 0;\n\tdo {\n\t\trqst = &rqst_array[rqst_idx];\n\n\t\tcifs_dbg(FYI, \"Sending smb (RDMA): idx=%d smb_len=%lu\\n\",\n\t\t\t rqst_idx, smb_rqst_len(server, rqst));\n\t\tfor (i = 0; i < rqst->rq_nvec; i++)\n\t\t\tdump_smb(rqst->rq_iov[i].iov_base, rqst->rq_iov[i].iov_len);\n\n\t\tlog_write(INFO, \"RDMA-WR[%u] nvec=%d len=%u iter=%zu rqlen=%lu\\n\",\n\t\t\t  rqst_idx, rqst->rq_nvec, remaining_data_length,\n\t\t\t  iov_iter_count(&rqst->rq_iter), smb_rqst_len(server, rqst));\n\n\t\t \n\t\tklen = 0;\n\t\tfor (i = 0; i < rqst->rq_nvec; i++)\n\t\t\tklen += rqst->rq_iov[i].iov_len;\n\t\tiov_iter_kvec(&iter, ITER_SOURCE, rqst->rq_iov, rqst->rq_nvec, klen);\n\n\t\trc = smbd_post_send_iter(info, &iter, &remaining_data_length);\n\t\tif (rc < 0)\n\t\t\tbreak;\n\n\t\tif (iov_iter_count(&rqst->rq_iter) > 0) {\n\t\t\t \n\t\t\trc = smbd_post_send_iter(info, &rqst->rq_iter,\n\t\t\t\t\t\t &remaining_data_length);\n\t\t\tif (rc < 0)\n\t\t\t\tbreak;\n\t\t}\n\n\t} while (++rqst_idx < num_rqst);\n\n\t \n\n\twait_event(info->wait_send_pending,\n\t\tatomic_read(&info->send_pending) == 0);\n\n\treturn rc;\n}\n\nstatic void register_mr_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct smbd_mr *mr;\n\tstruct ib_cqe *cqe;\n\n\tif (wc->status) {\n\t\tlog_rdma_mr(ERR, \"status=%d\\n\", wc->status);\n\t\tcqe = wc->wr_cqe;\n\t\tmr = container_of(cqe, struct smbd_mr, cqe);\n\t\tsmbd_disconnect_rdma_connection(mr->conn);\n\t}\n}\n\n \nstatic void smbd_mr_recovery_work(struct work_struct *work)\n{\n\tstruct smbd_connection *info =\n\t\tcontainer_of(work, struct smbd_connection, mr_recovery_work);\n\tstruct smbd_mr *smbdirect_mr;\n\tint rc;\n\n\tlist_for_each_entry(smbdirect_mr, &info->mr_list, list) {\n\t\tif (smbdirect_mr->state == MR_ERROR) {\n\n\t\t\t \n\t\t\trc = ib_dereg_mr(smbdirect_mr->mr);\n\t\t\tif (rc) {\n\t\t\t\tlog_rdma_mr(ERR,\n\t\t\t\t\t\"ib_dereg_mr failed rc=%x\\n\",\n\t\t\t\t\trc);\n\t\t\t\tsmbd_disconnect_rdma_connection(info);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tsmbdirect_mr->mr = ib_alloc_mr(\n\t\t\t\tinfo->pd, info->mr_type,\n\t\t\t\tinfo->max_frmr_depth);\n\t\t\tif (IS_ERR(smbdirect_mr->mr)) {\n\t\t\t\tlog_rdma_mr(ERR, \"ib_alloc_mr failed mr_type=%x max_frmr_depth=%x\\n\",\n\t\t\t\t\t    info->mr_type,\n\t\t\t\t\t    info->max_frmr_depth);\n\t\t\t\tsmbd_disconnect_rdma_connection(info);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t} else\n\t\t\t \n\t\t\tcontinue;\n\n\t\tsmbdirect_mr->state = MR_READY;\n\n\t\t \n\t\tif (atomic_inc_return(&info->mr_ready_count) == 1)\n\t\t\twake_up_interruptible(&info->wait_mr);\n\t}\n}\n\nstatic void destroy_mr_list(struct smbd_connection *info)\n{\n\tstruct smbd_mr *mr, *tmp;\n\n\tcancel_work_sync(&info->mr_recovery_work);\n\tlist_for_each_entry_safe(mr, tmp, &info->mr_list, list) {\n\t\tif (mr->state == MR_INVALIDATED)\n\t\t\tib_dma_unmap_sg(info->id->device, mr->sgt.sgl,\n\t\t\t\tmr->sgt.nents, mr->dir);\n\t\tib_dereg_mr(mr->mr);\n\t\tkfree(mr->sgt.sgl);\n\t\tkfree(mr);\n\t}\n}\n\n \nstatic int allocate_mr_list(struct smbd_connection *info)\n{\n\tint i;\n\tstruct smbd_mr *smbdirect_mr, *tmp;\n\n\tINIT_LIST_HEAD(&info->mr_list);\n\tinit_waitqueue_head(&info->wait_mr);\n\tspin_lock_init(&info->mr_list_lock);\n\tatomic_set(&info->mr_ready_count, 0);\n\tatomic_set(&info->mr_used_count, 0);\n\tinit_waitqueue_head(&info->wait_for_mr_cleanup);\n\tINIT_WORK(&info->mr_recovery_work, smbd_mr_recovery_work);\n\t \n\tfor (i = 0; i < info->responder_resources * 2; i++) {\n\t\tsmbdirect_mr = kzalloc(sizeof(*smbdirect_mr), GFP_KERNEL);\n\t\tif (!smbdirect_mr)\n\t\t\tgoto out;\n\t\tsmbdirect_mr->mr = ib_alloc_mr(info->pd, info->mr_type,\n\t\t\t\t\tinfo->max_frmr_depth);\n\t\tif (IS_ERR(smbdirect_mr->mr)) {\n\t\t\tlog_rdma_mr(ERR, \"ib_alloc_mr failed mr_type=%x max_frmr_depth=%x\\n\",\n\t\t\t\t    info->mr_type, info->max_frmr_depth);\n\t\t\tgoto out;\n\t\t}\n\t\tsmbdirect_mr->sgt.sgl = kcalloc(info->max_frmr_depth,\n\t\t\t\t\t\tsizeof(struct scatterlist),\n\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!smbdirect_mr->sgt.sgl) {\n\t\t\tlog_rdma_mr(ERR, \"failed to allocate sgl\\n\");\n\t\t\tib_dereg_mr(smbdirect_mr->mr);\n\t\t\tgoto out;\n\t\t}\n\t\tsmbdirect_mr->state = MR_READY;\n\t\tsmbdirect_mr->conn = info;\n\n\t\tlist_add_tail(&smbdirect_mr->list, &info->mr_list);\n\t\tatomic_inc(&info->mr_ready_count);\n\t}\n\treturn 0;\n\nout:\n\tkfree(smbdirect_mr);\n\n\tlist_for_each_entry_safe(smbdirect_mr, tmp, &info->mr_list, list) {\n\t\tlist_del(&smbdirect_mr->list);\n\t\tib_dereg_mr(smbdirect_mr->mr);\n\t\tkfree(smbdirect_mr->sgt.sgl);\n\t\tkfree(smbdirect_mr);\n\t}\n\treturn -ENOMEM;\n}\n\n \nstatic struct smbd_mr *get_mr(struct smbd_connection *info)\n{\n\tstruct smbd_mr *ret;\n\tint rc;\nagain:\n\trc = wait_event_interruptible(info->wait_mr,\n\t\tatomic_read(&info->mr_ready_count) ||\n\t\tinfo->transport_status != SMBD_CONNECTED);\n\tif (rc) {\n\t\tlog_rdma_mr(ERR, \"wait_event_interruptible rc=%x\\n\", rc);\n\t\treturn NULL;\n\t}\n\n\tif (info->transport_status != SMBD_CONNECTED) {\n\t\tlog_rdma_mr(ERR, \"info->transport_status=%x\\n\",\n\t\t\tinfo->transport_status);\n\t\treturn NULL;\n\t}\n\n\tspin_lock(&info->mr_list_lock);\n\tlist_for_each_entry(ret, &info->mr_list, list) {\n\t\tif (ret->state == MR_READY) {\n\t\t\tret->state = MR_REGISTERED;\n\t\t\tspin_unlock(&info->mr_list_lock);\n\t\t\tatomic_dec(&info->mr_ready_count);\n\t\t\tatomic_inc(&info->mr_used_count);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tspin_unlock(&info->mr_list_lock);\n\t \n\tgoto again;\n}\n\n \nstatic int smbd_iter_to_mr(struct smbd_connection *info,\n\t\t\t   struct iov_iter *iter,\n\t\t\t   struct sg_table *sgt,\n\t\t\t   unsigned int max_sg)\n{\n\tint ret;\n\n\tmemset(sgt->sgl, 0, max_sg * sizeof(struct scatterlist));\n\n\tret = extract_iter_to_sg(iter, iov_iter_count(iter), sgt, max_sg, 0);\n\tWARN_ON(ret < 0);\n\tif (sgt->nents > 0)\n\t\tsg_mark_end(&sgt->sgl[sgt->nents - 1]);\n\treturn ret;\n}\n\n \nstruct smbd_mr *smbd_register_mr(struct smbd_connection *info,\n\t\t\t\t struct iov_iter *iter,\n\t\t\t\t bool writing, bool need_invalidate)\n{\n\tstruct smbd_mr *smbdirect_mr;\n\tint rc, num_pages;\n\tenum dma_data_direction dir;\n\tstruct ib_reg_wr *reg_wr;\n\n\tnum_pages = iov_iter_npages(iter, info->max_frmr_depth + 1);\n\tif (num_pages > info->max_frmr_depth) {\n\t\tlog_rdma_mr(ERR, \"num_pages=%d max_frmr_depth=%d\\n\",\n\t\t\tnum_pages, info->max_frmr_depth);\n\t\tWARN_ON_ONCE(1);\n\t\treturn NULL;\n\t}\n\n\tsmbdirect_mr = get_mr(info);\n\tif (!smbdirect_mr) {\n\t\tlog_rdma_mr(ERR, \"get_mr returning NULL\\n\");\n\t\treturn NULL;\n\t}\n\n\tdir = writing ? DMA_FROM_DEVICE : DMA_TO_DEVICE;\n\tsmbdirect_mr->dir = dir;\n\tsmbdirect_mr->need_invalidate = need_invalidate;\n\tsmbdirect_mr->sgt.nents = 0;\n\tsmbdirect_mr->sgt.orig_nents = 0;\n\n\tlog_rdma_mr(INFO, \"num_pages=0x%x count=0x%zx depth=%u\\n\",\n\t\t    num_pages, iov_iter_count(iter), info->max_frmr_depth);\n\tsmbd_iter_to_mr(info, iter, &smbdirect_mr->sgt, info->max_frmr_depth);\n\n\trc = ib_dma_map_sg(info->id->device, smbdirect_mr->sgt.sgl,\n\t\t\t   smbdirect_mr->sgt.nents, dir);\n\tif (!rc) {\n\t\tlog_rdma_mr(ERR, \"ib_dma_map_sg num_pages=%x dir=%x rc=%x\\n\",\n\t\t\tnum_pages, dir, rc);\n\t\tgoto dma_map_error;\n\t}\n\n\trc = ib_map_mr_sg(smbdirect_mr->mr, smbdirect_mr->sgt.sgl,\n\t\t\t  smbdirect_mr->sgt.nents, NULL, PAGE_SIZE);\n\tif (rc != smbdirect_mr->sgt.nents) {\n\t\tlog_rdma_mr(ERR,\n\t\t\t\"ib_map_mr_sg failed rc = %d nents = %x\\n\",\n\t\t\trc, smbdirect_mr->sgt.nents);\n\t\tgoto map_mr_error;\n\t}\n\n\tib_update_fast_reg_key(smbdirect_mr->mr,\n\t\tib_inc_rkey(smbdirect_mr->mr->rkey));\n\treg_wr = &smbdirect_mr->wr;\n\treg_wr->wr.opcode = IB_WR_REG_MR;\n\tsmbdirect_mr->cqe.done = register_mr_done;\n\treg_wr->wr.wr_cqe = &smbdirect_mr->cqe;\n\treg_wr->wr.num_sge = 0;\n\treg_wr->wr.send_flags = IB_SEND_SIGNALED;\n\treg_wr->mr = smbdirect_mr->mr;\n\treg_wr->key = smbdirect_mr->mr->rkey;\n\treg_wr->access = writing ?\n\t\t\tIB_ACCESS_REMOTE_WRITE | IB_ACCESS_LOCAL_WRITE :\n\t\t\tIB_ACCESS_REMOTE_READ;\n\n\t \n\trc = ib_post_send(info->id->qp, &reg_wr->wr, NULL);\n\tif (!rc)\n\t\treturn smbdirect_mr;\n\n\tlog_rdma_mr(ERR, \"ib_post_send failed rc=%x reg_wr->key=%x\\n\",\n\t\trc, reg_wr->key);\n\n\t \nmap_mr_error:\n\tib_dma_unmap_sg(info->id->device, smbdirect_mr->sgt.sgl,\n\t\t\tsmbdirect_mr->sgt.nents, smbdirect_mr->dir);\n\ndma_map_error:\n\tsmbdirect_mr->state = MR_ERROR;\n\tif (atomic_dec_and_test(&info->mr_used_count))\n\t\twake_up(&info->wait_for_mr_cleanup);\n\n\tsmbd_disconnect_rdma_connection(info);\n\n\treturn NULL;\n}\n\nstatic void local_inv_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct smbd_mr *smbdirect_mr;\n\tstruct ib_cqe *cqe;\n\n\tcqe = wc->wr_cqe;\n\tsmbdirect_mr = container_of(cqe, struct smbd_mr, cqe);\n\tsmbdirect_mr->state = MR_INVALIDATED;\n\tif (wc->status != IB_WC_SUCCESS) {\n\t\tlog_rdma_mr(ERR, \"invalidate failed status=%x\\n\", wc->status);\n\t\tsmbdirect_mr->state = MR_ERROR;\n\t}\n\tcomplete(&smbdirect_mr->invalidate_done);\n}\n\n \nint smbd_deregister_mr(struct smbd_mr *smbdirect_mr)\n{\n\tstruct ib_send_wr *wr;\n\tstruct smbd_connection *info = smbdirect_mr->conn;\n\tint rc = 0;\n\n\tif (smbdirect_mr->need_invalidate) {\n\t\t \n\t\twr = &smbdirect_mr->inv_wr;\n\t\twr->opcode = IB_WR_LOCAL_INV;\n\t\tsmbdirect_mr->cqe.done = local_inv_done;\n\t\twr->wr_cqe = &smbdirect_mr->cqe;\n\t\twr->num_sge = 0;\n\t\twr->ex.invalidate_rkey = smbdirect_mr->mr->rkey;\n\t\twr->send_flags = IB_SEND_SIGNALED;\n\n\t\tinit_completion(&smbdirect_mr->invalidate_done);\n\t\trc = ib_post_send(info->id->qp, wr, NULL);\n\t\tif (rc) {\n\t\t\tlog_rdma_mr(ERR, \"ib_post_send failed rc=%x\\n\", rc);\n\t\t\tsmbd_disconnect_rdma_connection(info);\n\t\t\tgoto done;\n\t\t}\n\t\twait_for_completion(&smbdirect_mr->invalidate_done);\n\t\tsmbdirect_mr->need_invalidate = false;\n\t} else\n\t\t \n\t\tsmbdirect_mr->state = MR_INVALIDATED;\n\n\tif (smbdirect_mr->state == MR_INVALIDATED) {\n\t\tib_dma_unmap_sg(\n\t\t\tinfo->id->device, smbdirect_mr->sgt.sgl,\n\t\t\tsmbdirect_mr->sgt.nents,\n\t\t\tsmbdirect_mr->dir);\n\t\tsmbdirect_mr->state = MR_READY;\n\t\tif (atomic_inc_return(&info->mr_ready_count) == 1)\n\t\t\twake_up_interruptible(&info->wait_mr);\n\t} else\n\t\t \n\t\tqueue_work(info->workqueue, &info->mr_recovery_work);\n\ndone:\n\tif (atomic_dec_and_test(&info->mr_used_count))\n\t\twake_up(&info->wait_for_mr_cleanup);\n\n\treturn rc;\n}\n\nstatic bool smb_set_sge(struct smb_extract_to_rdma *rdma,\n\t\t\tstruct page *lowest_page, size_t off, size_t len)\n{\n\tstruct ib_sge *sge = &rdma->sge[rdma->nr_sge];\n\tu64 addr;\n\n\taddr = ib_dma_map_page(rdma->device, lowest_page,\n\t\t\t       off, len, rdma->direction);\n\tif (ib_dma_mapping_error(rdma->device, addr))\n\t\treturn false;\n\n\tsge->addr   = addr;\n\tsge->length = len;\n\tsge->lkey   = rdma->local_dma_lkey;\n\trdma->nr_sge++;\n\treturn true;\n}\n\n \nstatic ssize_t smb_extract_bvec_to_rdma(struct iov_iter *iter,\n\t\t\t\t\tstruct smb_extract_to_rdma *rdma,\n\t\t\t\t\tssize_t maxsize)\n{\n\tconst struct bio_vec *bv = iter->bvec;\n\tunsigned long start = iter->iov_offset;\n\tunsigned int i;\n\tssize_t ret = 0;\n\n\tfor (i = 0; i < iter->nr_segs; i++) {\n\t\tsize_t off, len;\n\n\t\tlen = bv[i].bv_len;\n\t\tif (start >= len) {\n\t\t\tstart -= len;\n\t\t\tcontinue;\n\t\t}\n\n\t\tlen = min_t(size_t, maxsize, len - start);\n\t\toff = bv[i].bv_offset + start;\n\n\t\tif (!smb_set_sge(rdma, bv[i].bv_page, off, len))\n\t\t\treturn -EIO;\n\n\t\tret += len;\n\t\tmaxsize -= len;\n\t\tif (rdma->nr_sge >= rdma->max_sge || maxsize <= 0)\n\t\t\tbreak;\n\t\tstart = 0;\n\t}\n\n\treturn ret;\n}\n\n \nstatic ssize_t smb_extract_kvec_to_rdma(struct iov_iter *iter,\n\t\t\t\t\tstruct smb_extract_to_rdma *rdma,\n\t\t\t\t\tssize_t maxsize)\n{\n\tconst struct kvec *kv = iter->kvec;\n\tunsigned long start = iter->iov_offset;\n\tunsigned int i;\n\tssize_t ret = 0;\n\n\tfor (i = 0; i < iter->nr_segs; i++) {\n\t\tstruct page *page;\n\t\tunsigned long kaddr;\n\t\tsize_t off, len, seg;\n\n\t\tlen = kv[i].iov_len;\n\t\tif (start >= len) {\n\t\t\tstart -= len;\n\t\t\tcontinue;\n\t\t}\n\n\t\tkaddr = (unsigned long)kv[i].iov_base + start;\n\t\toff = kaddr & ~PAGE_MASK;\n\t\tlen = min_t(size_t, maxsize, len - start);\n\t\tkaddr &= PAGE_MASK;\n\n\t\tmaxsize -= len;\n\t\tdo {\n\t\t\tseg = min_t(size_t, len, PAGE_SIZE - off);\n\n\t\t\tif (is_vmalloc_or_module_addr((void *)kaddr))\n\t\t\t\tpage = vmalloc_to_page((void *)kaddr);\n\t\t\telse\n\t\t\t\tpage = virt_to_page((void *)kaddr);\n\n\t\t\tif (!smb_set_sge(rdma, page, off, seg))\n\t\t\t\treturn -EIO;\n\n\t\t\tret += seg;\n\t\t\tlen -= seg;\n\t\t\tkaddr += PAGE_SIZE;\n\t\t\toff = 0;\n\t\t} while (len > 0 && rdma->nr_sge < rdma->max_sge);\n\n\t\tif (rdma->nr_sge >= rdma->max_sge || maxsize <= 0)\n\t\t\tbreak;\n\t\tstart = 0;\n\t}\n\n\treturn ret;\n}\n\n \nstatic ssize_t smb_extract_xarray_to_rdma(struct iov_iter *iter,\n\t\t\t\t\t  struct smb_extract_to_rdma *rdma,\n\t\t\t\t\t  ssize_t maxsize)\n{\n\tstruct xarray *xa = iter->xarray;\n\tstruct folio *folio;\n\tloff_t start = iter->xarray_start + iter->iov_offset;\n\tpgoff_t index = start / PAGE_SIZE;\n\tssize_t ret = 0;\n\tsize_t off, len;\n\tXA_STATE(xas, xa, index);\n\n\trcu_read_lock();\n\n\txas_for_each(&xas, folio, ULONG_MAX) {\n\t\tif (xas_retry(&xas, folio))\n\t\t\tcontinue;\n\t\tif (WARN_ON(xa_is_value(folio)))\n\t\t\tbreak;\n\t\tif (WARN_ON(folio_test_hugetlb(folio)))\n\t\t\tbreak;\n\n\t\toff = offset_in_folio(folio, start);\n\t\tlen = min_t(size_t, maxsize, folio_size(folio) - off);\n\n\t\tif (!smb_set_sge(rdma, folio_page(folio, 0), off, len)) {\n\t\t\trcu_read_unlock();\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tmaxsize -= len;\n\t\tret += len;\n\t\tif (rdma->nr_sge >= rdma->max_sge || maxsize <= 0)\n\t\t\tbreak;\n\t}\n\n\trcu_read_unlock();\n\treturn ret;\n}\n\n \nstatic ssize_t smb_extract_iter_to_rdma(struct iov_iter *iter, size_t len,\n\t\t\t\t\tstruct smb_extract_to_rdma *rdma)\n{\n\tssize_t ret;\n\tint before = rdma->nr_sge;\n\n\tswitch (iov_iter_type(iter)) {\n\tcase ITER_BVEC:\n\t\tret = smb_extract_bvec_to_rdma(iter, rdma, len);\n\t\tbreak;\n\tcase ITER_KVEC:\n\t\tret = smb_extract_kvec_to_rdma(iter, rdma, len);\n\t\tbreak;\n\tcase ITER_XARRAY:\n\t\tret = smb_extract_xarray_to_rdma(iter, rdma, len);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn -EIO;\n\t}\n\n\tif (ret > 0) {\n\t\tiov_iter_advance(iter, ret);\n\t} else if (ret < 0) {\n\t\twhile (rdma->nr_sge > before) {\n\t\t\tstruct ib_sge *sge = &rdma->sge[rdma->nr_sge--];\n\n\t\t\tib_dma_unmap_single(rdma->device, sge->addr, sge->length,\n\t\t\t\t\t    rdma->direction);\n\t\t\tsge->addr = 0;\n\t\t}\n\t}\n\n\treturn ret;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}