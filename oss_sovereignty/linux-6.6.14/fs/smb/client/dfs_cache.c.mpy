{
  "module_name": "dfs_cache.c",
  "hash_id": "3d75abf7f1d2acd351bd1f54b7d35ca242f46358430f2d93eea14dc942f2c2df",
  "original_prompt": "Ingested from linux-6.6.14/fs/smb/client/dfs_cache.c",
  "human_readable_source": "\n \n\n#include <linux/jhash.h>\n#include <linux/ktime.h>\n#include <linux/slab.h>\n#include <linux/proc_fs.h>\n#include <linux/nls.h>\n#include <linux/workqueue.h>\n#include <linux/uuid.h>\n#include \"cifsglob.h\"\n#include \"smb2pdu.h\"\n#include \"smb2proto.h\"\n#include \"cifsproto.h\"\n#include \"cifs_debug.h\"\n#include \"cifs_unicode.h\"\n#include \"smb2glob.h\"\n#include \"dns_resolve.h\"\n#include \"dfs.h\"\n\n#include \"dfs_cache.h\"\n\n#define CACHE_HTABLE_SIZE\t32\n#define CACHE_MAX_ENTRIES\t64\n#define CACHE_MIN_TTL\t\t120  \n#define CACHE_DEFAULT_TTL\t300  \n\nstruct cache_dfs_tgt {\n\tchar *name;\n\tint path_consumed;\n\tstruct list_head list;\n};\n\nstruct cache_entry {\n\tstruct hlist_node hlist;\n\tconst char *path;\n\tint hdr_flags;  \n\tint ttl;  \n\tint srvtype;  \n\tint ref_flags;  \n\tstruct timespec64 etime;\n\tint path_consumed;  \n\tint numtgts;\n\tstruct list_head tlist;\n\tstruct cache_dfs_tgt *tgthint;\n};\n\nstatic struct kmem_cache *cache_slab __read_mostly;\nstruct workqueue_struct *dfscache_wq;\n\natomic_t dfs_cache_ttl;\n\nstatic struct nls_table *cache_cp;\n\n \nstatic atomic_t cache_count;\n\nstatic struct hlist_head cache_htable[CACHE_HTABLE_SIZE];\nstatic DECLARE_RWSEM(htable_rw_lock);\n\n \nchar *dfs_cache_canonical_path(const char *path, const struct nls_table *cp, int remap)\n{\n\tchar *tmp;\n\tint plen = 0;\n\tchar *npath;\n\n\tif (!path || strlen(path) < 3 || (*path != '\\\\' && *path != '/'))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif (unlikely(strcmp(cp->charset, cache_cp->charset))) {\n\t\ttmp = (char *)cifs_strndup_to_utf16(path, strlen(path), &plen, cp, remap);\n\t\tif (!tmp) {\n\t\t\tcifs_dbg(VFS, \"%s: failed to convert path to utf16\\n\", __func__);\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\n\t\tnpath = cifs_strndup_from_utf16(tmp, plen, true, cache_cp);\n\t\tkfree(tmp);\n\n\t\tif (!npath) {\n\t\t\tcifs_dbg(VFS, \"%s: failed to convert path from utf16\\n\", __func__);\n\t\t\treturn ERR_PTR(-EINVAL);\n\t\t}\n\t} else {\n\t\tnpath = kstrdup(path, GFP_KERNEL);\n\t\tif (!npath)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tconvert_delimiter(npath, '\\\\');\n\treturn npath;\n}\n\nstatic inline bool cache_entry_expired(const struct cache_entry *ce)\n{\n\tstruct timespec64 ts;\n\n\tktime_get_coarse_real_ts64(&ts);\n\treturn timespec64_compare(&ts, &ce->etime) >= 0;\n}\n\nstatic inline void free_tgts(struct cache_entry *ce)\n{\n\tstruct cache_dfs_tgt *t, *n;\n\n\tlist_for_each_entry_safe(t, n, &ce->tlist, list) {\n\t\tlist_del(&t->list);\n\t\tkfree(t->name);\n\t\tkfree(t);\n\t}\n}\n\nstatic inline void flush_cache_ent(struct cache_entry *ce)\n{\n\thlist_del_init(&ce->hlist);\n\tkfree(ce->path);\n\tfree_tgts(ce);\n\tatomic_dec(&cache_count);\n\tkmem_cache_free(cache_slab, ce);\n}\n\nstatic void flush_cache_ents(void)\n{\n\tint i;\n\n\tfor (i = 0; i < CACHE_HTABLE_SIZE; i++) {\n\t\tstruct hlist_head *l = &cache_htable[i];\n\t\tstruct hlist_node *n;\n\t\tstruct cache_entry *ce;\n\n\t\thlist_for_each_entry_safe(ce, n, l, hlist) {\n\t\t\tif (!hlist_unhashed(&ce->hlist))\n\t\t\t\tflush_cache_ent(ce);\n\t\t}\n\t}\n}\n\n \nstatic int dfscache_proc_show(struct seq_file *m, void *v)\n{\n\tint i;\n\tstruct cache_entry *ce;\n\tstruct cache_dfs_tgt *t;\n\n\tseq_puts(m, \"DFS cache\\n---------\\n\");\n\n\tdown_read(&htable_rw_lock);\n\tfor (i = 0; i < CACHE_HTABLE_SIZE; i++) {\n\t\tstruct hlist_head *l = &cache_htable[i];\n\n\t\thlist_for_each_entry(ce, l, hlist) {\n\t\t\tif (hlist_unhashed(&ce->hlist))\n\t\t\t\tcontinue;\n\n\t\t\tseq_printf(m,\n\t\t\t\t   \"cache entry: path=%s,type=%s,ttl=%d,etime=%ld,hdr_flags=0x%x,ref_flags=0x%x,interlink=%s,path_consumed=%d,expired=%s\\n\",\n\t\t\t\t   ce->path, ce->srvtype == DFS_TYPE_ROOT ? \"root\" : \"link\",\n\t\t\t\t   ce->ttl, ce->etime.tv_nsec, ce->hdr_flags, ce->ref_flags,\n\t\t\t\t   DFS_INTERLINK(ce->hdr_flags) ? \"yes\" : \"no\",\n\t\t\t\t   ce->path_consumed, cache_entry_expired(ce) ? \"yes\" : \"no\");\n\n\t\t\tlist_for_each_entry(t, &ce->tlist, list) {\n\t\t\t\tseq_printf(m, \"  %s%s\\n\",\n\t\t\t\t\t   t->name,\n\t\t\t\t\t   READ_ONCE(ce->tgthint) == t ? \" (target hint)\" : \"\");\n\t\t\t}\n\t\t}\n\t}\n\tup_read(&htable_rw_lock);\n\n\treturn 0;\n}\n\nstatic ssize_t dfscache_proc_write(struct file *file, const char __user *buffer,\n\t\t\t\t   size_t count, loff_t *ppos)\n{\n\tchar c;\n\tint rc;\n\n\trc = get_user(c, buffer);\n\tif (rc)\n\t\treturn rc;\n\n\tif (c != '0')\n\t\treturn -EINVAL;\n\n\tcifs_dbg(FYI, \"clearing dfs cache\\n\");\n\n\tdown_write(&htable_rw_lock);\n\tflush_cache_ents();\n\tup_write(&htable_rw_lock);\n\n\treturn count;\n}\n\nstatic int dfscache_proc_open(struct inode *inode, struct file *file)\n{\n\treturn single_open(file, dfscache_proc_show, NULL);\n}\n\nconst struct proc_ops dfscache_proc_ops = {\n\t.proc_open\t= dfscache_proc_open,\n\t.proc_read\t= seq_read,\n\t.proc_lseek\t= seq_lseek,\n\t.proc_release\t= single_release,\n\t.proc_write\t= dfscache_proc_write,\n};\n\n#ifdef CONFIG_CIFS_DEBUG2\nstatic inline void dump_tgts(const struct cache_entry *ce)\n{\n\tstruct cache_dfs_tgt *t;\n\n\tcifs_dbg(FYI, \"target list:\\n\");\n\tlist_for_each_entry(t, &ce->tlist, list) {\n\t\tcifs_dbg(FYI, \"  %s%s\\n\", t->name,\n\t\t\t READ_ONCE(ce->tgthint) == t ? \" (target hint)\" : \"\");\n\t}\n}\n\nstatic inline void dump_ce(const struct cache_entry *ce)\n{\n\tcifs_dbg(FYI, \"cache entry: path=%s,type=%s,ttl=%d,etime=%ld,hdr_flags=0x%x,ref_flags=0x%x,interlink=%s,path_consumed=%d,expired=%s\\n\",\n\t\t ce->path,\n\t\t ce->srvtype == DFS_TYPE_ROOT ? \"root\" : \"link\", ce->ttl,\n\t\t ce->etime.tv_nsec,\n\t\t ce->hdr_flags, ce->ref_flags,\n\t\t DFS_INTERLINK(ce->hdr_flags) ? \"yes\" : \"no\",\n\t\t ce->path_consumed,\n\t\t cache_entry_expired(ce) ? \"yes\" : \"no\");\n\tdump_tgts(ce);\n}\n\nstatic inline void dump_refs(const struct dfs_info3_param *refs, int numrefs)\n{\n\tint i;\n\n\tcifs_dbg(FYI, \"DFS referrals returned by the server:\\n\");\n\tfor (i = 0; i < numrefs; i++) {\n\t\tconst struct dfs_info3_param *ref = &refs[i];\n\n\t\tcifs_dbg(FYI,\n\t\t\t \"\\n\"\n\t\t\t \"flags:         0x%x\\n\"\n\t\t\t \"path_consumed: %d\\n\"\n\t\t\t \"server_type:   0x%x\\n\"\n\t\t\t \"ref_flag:      0x%x\\n\"\n\t\t\t \"path_name:     %s\\n\"\n\t\t\t \"node_name:     %s\\n\"\n\t\t\t \"ttl:           %d (%dm)\\n\",\n\t\t\t ref->flags, ref->path_consumed, ref->server_type,\n\t\t\t ref->ref_flag, ref->path_name, ref->node_name,\n\t\t\t ref->ttl, ref->ttl / 60);\n\t}\n}\n#else\n#define dump_tgts(e)\n#define dump_ce(e)\n#define dump_refs(r, n)\n#endif\n\n \nint dfs_cache_init(void)\n{\n\tint rc;\n\tint i;\n\n\tdfscache_wq = alloc_workqueue(\"cifs-dfscache\",\n\t\t\t\t      WQ_UNBOUND|WQ_FREEZABLE|WQ_MEM_RECLAIM,\n\t\t\t\t      0);\n\tif (!dfscache_wq)\n\t\treturn -ENOMEM;\n\n\tcache_slab = kmem_cache_create(\"cifs_dfs_cache\",\n\t\t\t\t       sizeof(struct cache_entry), 0,\n\t\t\t\t       SLAB_HWCACHE_ALIGN, NULL);\n\tif (!cache_slab) {\n\t\trc = -ENOMEM;\n\t\tgoto out_destroy_wq;\n\t}\n\n\tfor (i = 0; i < CACHE_HTABLE_SIZE; i++)\n\t\tINIT_HLIST_HEAD(&cache_htable[i]);\n\n\tatomic_set(&cache_count, 0);\n\tatomic_set(&dfs_cache_ttl, CACHE_DEFAULT_TTL);\n\tcache_cp = load_nls(\"utf8\");\n\tif (!cache_cp)\n\t\tcache_cp = load_nls_default();\n\n\tcifs_dbg(FYI, \"%s: initialized DFS referral cache\\n\", __func__);\n\treturn 0;\n\nout_destroy_wq:\n\tdestroy_workqueue(dfscache_wq);\n\treturn rc;\n}\n\nstatic int cache_entry_hash(const void *data, int size, unsigned int *hash)\n{\n\tint i, clen;\n\tconst unsigned char *s = data;\n\twchar_t c;\n\tunsigned int h = 0;\n\n\tfor (i = 0; i < size; i += clen) {\n\t\tclen = cache_cp->char2uni(&s[i], size - i, &c);\n\t\tif (unlikely(clen < 0)) {\n\t\t\tcifs_dbg(VFS, \"%s: can't convert char\\n\", __func__);\n\t\t\treturn clen;\n\t\t}\n\t\tc = cifs_toupper(c);\n\t\th = jhash(&c, sizeof(c), h);\n\t}\n\t*hash = h % CACHE_HTABLE_SIZE;\n\treturn 0;\n}\n\n \nstatic inline char *get_tgt_name(const struct cache_entry *ce)\n{\n\tstruct cache_dfs_tgt *t = READ_ONCE(ce->tgthint);\n\n\treturn t ? t->name : ERR_PTR(-ENOENT);\n}\n\n \nstatic inline struct timespec64 get_expire_time(int ttl)\n{\n\tstruct timespec64 ts = {\n\t\t.tv_sec = ttl,\n\t\t.tv_nsec = 0,\n\t};\n\tstruct timespec64 now;\n\n\tktime_get_coarse_real_ts64(&now);\n\treturn timespec64_add(now, ts);\n}\n\n \nstatic struct cache_dfs_tgt *alloc_target(const char *name, int path_consumed)\n{\n\tstruct cache_dfs_tgt *t;\n\n\tt = kmalloc(sizeof(*t), GFP_ATOMIC);\n\tif (!t)\n\t\treturn ERR_PTR(-ENOMEM);\n\tt->name = kstrdup(name, GFP_ATOMIC);\n\tif (!t->name) {\n\t\tkfree(t);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tt->path_consumed = path_consumed;\n\tINIT_LIST_HEAD(&t->list);\n\treturn t;\n}\n\n \nstatic int copy_ref_data(const struct dfs_info3_param *refs, int numrefs,\n\t\t\t struct cache_entry *ce, const char *tgthint)\n{\n\tstruct cache_dfs_tgt *target;\n\tint i;\n\n\tce->ttl = max_t(int, refs[0].ttl, CACHE_MIN_TTL);\n\tce->etime = get_expire_time(ce->ttl);\n\tce->srvtype = refs[0].server_type;\n\tce->hdr_flags = refs[0].flags;\n\tce->ref_flags = refs[0].ref_flag;\n\tce->path_consumed = refs[0].path_consumed;\n\n\tfor (i = 0; i < numrefs; i++) {\n\t\tstruct cache_dfs_tgt *t;\n\n\t\tt = alloc_target(refs[i].node_name, refs[i].path_consumed);\n\t\tif (IS_ERR(t)) {\n\t\t\tfree_tgts(ce);\n\t\t\treturn PTR_ERR(t);\n\t\t}\n\t\tif (tgthint && !strcasecmp(t->name, tgthint)) {\n\t\t\tlist_add(&t->list, &ce->tlist);\n\t\t\ttgthint = NULL;\n\t\t} else {\n\t\t\tlist_add_tail(&t->list, &ce->tlist);\n\t\t}\n\t\tce->numtgts++;\n\t}\n\n\ttarget = list_first_entry_or_null(&ce->tlist, struct cache_dfs_tgt,\n\t\t\t\t\t  list);\n\tWRITE_ONCE(ce->tgthint, target);\n\n\treturn 0;\n}\n\n \nstatic struct cache_entry *alloc_cache_entry(struct dfs_info3_param *refs, int numrefs)\n{\n\tstruct cache_entry *ce;\n\tint rc;\n\n\tce = kmem_cache_zalloc(cache_slab, GFP_KERNEL);\n\tif (!ce)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tce->path = refs[0].path_name;\n\trefs[0].path_name = NULL;\n\n\tINIT_HLIST_NODE(&ce->hlist);\n\tINIT_LIST_HEAD(&ce->tlist);\n\n\trc = copy_ref_data(refs, numrefs, ce, NULL);\n\tif (rc) {\n\t\tkfree(ce->path);\n\t\tkmem_cache_free(cache_slab, ce);\n\t\tce = ERR_PTR(rc);\n\t}\n\treturn ce;\n}\n\nstatic void remove_oldest_entry_locked(void)\n{\n\tint i;\n\tstruct cache_entry *ce;\n\tstruct cache_entry *to_del = NULL;\n\n\tWARN_ON(!rwsem_is_locked(&htable_rw_lock));\n\n\tfor (i = 0; i < CACHE_HTABLE_SIZE; i++) {\n\t\tstruct hlist_head *l = &cache_htable[i];\n\n\t\thlist_for_each_entry(ce, l, hlist) {\n\t\t\tif (hlist_unhashed(&ce->hlist))\n\t\t\t\tcontinue;\n\t\t\tif (!to_del || timespec64_compare(&ce->etime,\n\t\t\t\t\t\t\t  &to_del->etime) < 0)\n\t\t\t\tto_del = ce;\n\t\t}\n\t}\n\n\tif (!to_del) {\n\t\tcifs_dbg(FYI, \"%s: no entry to remove\\n\", __func__);\n\t\treturn;\n\t}\n\n\tcifs_dbg(FYI, \"%s: removing entry\\n\", __func__);\n\tdump_ce(to_del);\n\tflush_cache_ent(to_del);\n}\n\n \nstatic struct cache_entry *add_cache_entry_locked(struct dfs_info3_param *refs,\n\t\t\t\t\t\t  int numrefs)\n{\n\tint rc;\n\tstruct cache_entry *ce;\n\tunsigned int hash;\n\tint ttl;\n\n\tWARN_ON(!rwsem_is_locked(&htable_rw_lock));\n\n\tif (atomic_read(&cache_count) >= CACHE_MAX_ENTRIES) {\n\t\tcifs_dbg(FYI, \"%s: reached max cache size (%d)\\n\", __func__, CACHE_MAX_ENTRIES);\n\t\tremove_oldest_entry_locked();\n\t}\n\n\trc = cache_entry_hash(refs[0].path_name, strlen(refs[0].path_name), &hash);\n\tif (rc)\n\t\treturn ERR_PTR(rc);\n\n\tce = alloc_cache_entry(refs, numrefs);\n\tif (IS_ERR(ce))\n\t\treturn ce;\n\n\tttl = min_t(int, atomic_read(&dfs_cache_ttl), ce->ttl);\n\tatomic_set(&dfs_cache_ttl, ttl);\n\n\thlist_add_head(&ce->hlist, &cache_htable[hash]);\n\tdump_ce(ce);\n\n\tatomic_inc(&cache_count);\n\n\treturn ce;\n}\n\n \nstatic bool dfs_path_equal(const char *s1, int len1, const char *s2, int len2)\n{\n\tint i, l1, l2;\n\twchar_t c1, c2;\n\n\tif (len1 != len2)\n\t\treturn false;\n\n\tfor (i = 0; i < len1; i += l1) {\n\t\tl1 = cache_cp->char2uni(&s1[i], len1 - i, &c1);\n\t\tl2 = cache_cp->char2uni(&s2[i], len2 - i, &c2);\n\t\tif (unlikely(l1 < 0 && l2 < 0)) {\n\t\t\tif (s1[i] != s2[i])\n\t\t\t\treturn false;\n\t\t\tl1 = 1;\n\t\t\tcontinue;\n\t\t}\n\t\tif (l1 != l2)\n\t\t\treturn false;\n\t\tif (cifs_toupper(c1) != cifs_toupper(c2))\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\nstatic struct cache_entry *__lookup_cache_entry(const char *path, unsigned int hash, int len)\n{\n\tstruct cache_entry *ce;\n\n\thlist_for_each_entry(ce, &cache_htable[hash], hlist) {\n\t\tif (dfs_path_equal(ce->path, strlen(ce->path), path, len)) {\n\t\t\tdump_ce(ce);\n\t\t\treturn ce;\n\t\t}\n\t}\n\treturn ERR_PTR(-ENOENT);\n}\n\n \nstatic struct cache_entry *lookup_cache_entry(const char *path)\n{\n\tstruct cache_entry *ce;\n\tint cnt = 0;\n\tconst char *s = path, *e;\n\tchar sep = *s;\n\tunsigned int hash;\n\tint rc;\n\n\twhile ((s = strchr(s, sep)) && ++cnt < 3)\n\t\ts++;\n\n\tif (cnt < 3) {\n\t\trc = cache_entry_hash(path, strlen(path), &hash);\n\t\tif (rc)\n\t\t\treturn ERR_PTR(rc);\n\t\treturn __lookup_cache_entry(path, hash, strlen(path));\n\t}\n\t \n\te = path + strlen(path) - 1;\n\twhile (e > s) {\n\t\tint len;\n\n\t\t \n\t\twhile (e > s && *e == sep)\n\t\t\te--;\n\t\tif (e == s)\n\t\t\tbreak;\n\n\t\tlen = e + 1 - path;\n\t\trc = cache_entry_hash(path, len, &hash);\n\t\tif (rc)\n\t\t\treturn ERR_PTR(rc);\n\t\tce = __lookup_cache_entry(path, hash, len);\n\t\tif (!IS_ERR(ce))\n\t\t\treturn ce;\n\n\t\t \n\t\twhile (e > s && *e != sep)\n\t\t\te--;\n\t}\n\treturn ERR_PTR(-ENOENT);\n}\n\n \nvoid dfs_cache_destroy(void)\n{\n\tunload_nls(cache_cp);\n\tflush_cache_ents();\n\tkmem_cache_destroy(cache_slab);\n\tdestroy_workqueue(dfscache_wq);\n\n\tcifs_dbg(FYI, \"%s: destroyed DFS referral cache\\n\", __func__);\n}\n\n \nstatic int update_cache_entry_locked(struct cache_entry *ce, const struct dfs_info3_param *refs,\n\t\t\t\t     int numrefs)\n{\n\tstruct cache_dfs_tgt *target;\n\tchar *th = NULL;\n\tint rc;\n\n\tWARN_ON(!rwsem_is_locked(&htable_rw_lock));\n\n\ttarget = READ_ONCE(ce->tgthint);\n\tif (target) {\n\t\tth = kstrdup(target->name, GFP_ATOMIC);\n\t\tif (!th)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tfree_tgts(ce);\n\tce->numtgts = 0;\n\n\trc = copy_ref_data(refs, numrefs, ce, th);\n\n\tkfree(th);\n\n\treturn rc;\n}\n\nstatic int get_dfs_referral(const unsigned int xid, struct cifs_ses *ses, const char *path,\n\t\t\t    struct dfs_info3_param **refs, int *numrefs)\n{\n\tint rc;\n\tint i;\n\n\t*refs = NULL;\n\t*numrefs = 0;\n\n\tif (!ses || !ses->server || !ses->server->ops->get_dfs_refer)\n\t\treturn -EOPNOTSUPP;\n\tif (unlikely(!cache_cp))\n\t\treturn -EINVAL;\n\n\tcifs_dbg(FYI, \"%s: ipc=%s referral=%s\\n\", __func__, ses->tcon_ipc->tree_name, path);\n\trc =  ses->server->ops->get_dfs_refer(xid, ses, path, refs, numrefs, cache_cp,\n\t\t\t\t\t      NO_MAP_UNI_RSVD);\n\tif (!rc) {\n\t\tstruct dfs_info3_param *ref = *refs;\n\n\t\tfor (i = 0; i < *numrefs; i++)\n\t\t\tconvert_delimiter(ref[i].path_name, '\\\\');\n\t}\n\treturn rc;\n}\n\n \nstatic struct cache_entry *cache_refresh_path(const unsigned int xid,\n\t\t\t\t\t      struct cifs_ses *ses,\n\t\t\t\t\t      const char *path,\n\t\t\t\t\t      bool force_refresh)\n{\n\tstruct dfs_info3_param *refs = NULL;\n\tstruct cache_entry *ce;\n\tint numrefs = 0;\n\tint rc;\n\n\tcifs_dbg(FYI, \"%s: search path: %s\\n\", __func__, path);\n\n\tdown_read(&htable_rw_lock);\n\n\tce = lookup_cache_entry(path);\n\tif (!IS_ERR(ce)) {\n\t\tif (!force_refresh && !cache_entry_expired(ce))\n\t\t\treturn ce;\n\t} else if (PTR_ERR(ce) != -ENOENT) {\n\t\tup_read(&htable_rw_lock);\n\t\treturn ce;\n\t}\n\n\t \n\tup_read(&htable_rw_lock);\n\n\t \n\trc = get_dfs_referral(xid, ses, path, &refs, &numrefs);\n\tif (rc) {\n\t\tce = ERR_PTR(rc);\n\t\tgoto out;\n\t}\n\n\tdump_refs(refs, numrefs);\n\n\tdown_write(&htable_rw_lock);\n\t \n\tce = lookup_cache_entry(path);\n\tif (!IS_ERR(ce)) {\n\t\tif (force_refresh || cache_entry_expired(ce)) {\n\t\t\trc = update_cache_entry_locked(ce, refs, numrefs);\n\t\t\tif (rc)\n\t\t\t\tce = ERR_PTR(rc);\n\t\t}\n\t} else if (PTR_ERR(ce) == -ENOENT) {\n\t\tce = add_cache_entry_locked(refs, numrefs);\n\t}\n\n\tif (IS_ERR(ce)) {\n\t\tup_write(&htable_rw_lock);\n\t\tgoto out;\n\t}\n\n\tdowngrade_write(&htable_rw_lock);\nout:\n\tfree_dfs_info_array(refs, numrefs);\n\treturn ce;\n}\n\n \nstatic int setup_referral(const char *path, struct cache_entry *ce,\n\t\t\t  struct dfs_info3_param *ref, const char *target)\n{\n\tint rc;\n\n\tcifs_dbg(FYI, \"%s: set up new ref\\n\", __func__);\n\n\tmemset(ref, 0, sizeof(*ref));\n\n\tref->path_name = kstrdup(path, GFP_ATOMIC);\n\tif (!ref->path_name)\n\t\treturn -ENOMEM;\n\n\tref->node_name = kstrdup(target, GFP_ATOMIC);\n\tif (!ref->node_name) {\n\t\trc = -ENOMEM;\n\t\tgoto err_free_path;\n\t}\n\n\tref->path_consumed = ce->path_consumed;\n\tref->ttl = ce->ttl;\n\tref->server_type = ce->srvtype;\n\tref->ref_flag = ce->ref_flags;\n\tref->flags = ce->hdr_flags;\n\n\treturn 0;\n\nerr_free_path:\n\tkfree(ref->path_name);\n\tref->path_name = NULL;\n\treturn rc;\n}\n\n \nstatic int get_targets(struct cache_entry *ce, struct dfs_cache_tgt_list *tl)\n{\n\tint rc;\n\tstruct list_head *head = &tl->tl_list;\n\tstruct cache_dfs_tgt *t;\n\tstruct dfs_cache_tgt_iterator *it, *nit;\n\n\tmemset(tl, 0, sizeof(*tl));\n\tINIT_LIST_HEAD(head);\n\n\tlist_for_each_entry(t, &ce->tlist, list) {\n\t\tit = kzalloc(sizeof(*it), GFP_ATOMIC);\n\t\tif (!it) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err_free_it;\n\t\t}\n\n\t\tit->it_name = kstrdup(t->name, GFP_ATOMIC);\n\t\tif (!it->it_name) {\n\t\t\tkfree(it);\n\t\t\trc = -ENOMEM;\n\t\t\tgoto err_free_it;\n\t\t}\n\t\tit->it_path_consumed = t->path_consumed;\n\n\t\tif (READ_ONCE(ce->tgthint) == t)\n\t\t\tlist_add(&it->it_list, head);\n\t\telse\n\t\t\tlist_add_tail(&it->it_list, head);\n\t}\n\n\ttl->tl_numtgts = ce->numtgts;\n\n\treturn 0;\n\nerr_free_it:\n\tlist_for_each_entry_safe(it, nit, head, it_list) {\n\t\tlist_del(&it->it_list);\n\t\tkfree(it->it_name);\n\t\tkfree(it);\n\t}\n\treturn rc;\n}\n\n \nint dfs_cache_find(const unsigned int xid, struct cifs_ses *ses, const struct nls_table *cp,\n\t\t   int remap, const char *path, struct dfs_info3_param *ref,\n\t\t   struct dfs_cache_tgt_list *tgt_list)\n{\n\tint rc;\n\tconst char *npath;\n\tstruct cache_entry *ce;\n\n\tnpath = dfs_cache_canonical_path(path, cp, remap);\n\tif (IS_ERR(npath))\n\t\treturn PTR_ERR(npath);\n\n\tce = cache_refresh_path(xid, ses, npath, false);\n\tif (IS_ERR(ce)) {\n\t\trc = PTR_ERR(ce);\n\t\tgoto out_free_path;\n\t}\n\n\tif (ref)\n\t\trc = setup_referral(path, ce, ref, get_tgt_name(ce));\n\telse\n\t\trc = 0;\n\tif (!rc && tgt_list)\n\t\trc = get_targets(ce, tgt_list);\n\n\tup_read(&htable_rw_lock);\n\nout_free_path:\n\tkfree(npath);\n\treturn rc;\n}\n\n \nint dfs_cache_noreq_find(const char *path, struct dfs_info3_param *ref,\n\t\t\t struct dfs_cache_tgt_list *tgt_list)\n{\n\tint rc;\n\tstruct cache_entry *ce;\n\n\tcifs_dbg(FYI, \"%s: path: %s\\n\", __func__, path);\n\n\tdown_read(&htable_rw_lock);\n\n\tce = lookup_cache_entry(path);\n\tif (IS_ERR(ce)) {\n\t\trc = PTR_ERR(ce);\n\t\tgoto out_unlock;\n\t}\n\n\tif (ref)\n\t\trc = setup_referral(path, ce, ref, get_tgt_name(ce));\n\telse\n\t\trc = 0;\n\tif (!rc && tgt_list)\n\t\trc = get_targets(ce, tgt_list);\n\nout_unlock:\n\tup_read(&htable_rw_lock);\n\treturn rc;\n}\n\n \nvoid dfs_cache_noreq_update_tgthint(const char *path, const struct dfs_cache_tgt_iterator *it)\n{\n\tstruct cache_dfs_tgt *t;\n\tstruct cache_entry *ce;\n\n\tif (!path || !it)\n\t\treturn;\n\n\tcifs_dbg(FYI, \"%s: path: %s\\n\", __func__, path);\n\n\tdown_read(&htable_rw_lock);\n\n\tce = lookup_cache_entry(path);\n\tif (IS_ERR(ce))\n\t\tgoto out_unlock;\n\n\tt = READ_ONCE(ce->tgthint);\n\n\tif (unlikely(!strcasecmp(it->it_name, t->name)))\n\t\tgoto out_unlock;\n\n\tlist_for_each_entry(t, &ce->tlist, list) {\n\t\tif (!strcasecmp(t->name, it->it_name)) {\n\t\t\tWRITE_ONCE(ce->tgthint, t);\n\t\t\tcifs_dbg(FYI, \"%s: new target hint: %s\\n\", __func__,\n\t\t\t\t it->it_name);\n\t\t\tbreak;\n\t\t}\n\t}\n\nout_unlock:\n\tup_read(&htable_rw_lock);\n}\n\n \nint dfs_cache_get_tgt_referral(const char *path, const struct dfs_cache_tgt_iterator *it,\n\t\t\t       struct dfs_info3_param *ref)\n{\n\tint rc;\n\tstruct cache_entry *ce;\n\n\tif (!it || !ref)\n\t\treturn -EINVAL;\n\n\tcifs_dbg(FYI, \"%s: path: %s\\n\", __func__, path);\n\n\tdown_read(&htable_rw_lock);\n\n\tce = lookup_cache_entry(path);\n\tif (IS_ERR(ce)) {\n\t\trc = PTR_ERR(ce);\n\t\tgoto out_unlock;\n\t}\n\n\tcifs_dbg(FYI, \"%s: target name: %s\\n\", __func__, it->it_name);\n\n\trc = setup_referral(path, ce, ref, it->it_name);\n\nout_unlock:\n\tup_read(&htable_rw_lock);\n\treturn rc;\n}\n\n \nstatic const char *parse_target_share(const char *target, char **share)\n{\n\tconst char *s, *seps = \"/\\\\\";\n\tsize_t len;\n\n\ts = strpbrk(target + 1, seps);\n\tif (!s)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tlen = strcspn(s + 1, seps);\n\tif (!len)\n\t\treturn ERR_PTR(-EINVAL);\n\ts += len;\n\n\tlen = s - target + 1;\n\t*share = kstrndup(target, len, GFP_KERNEL);\n\tif (!*share)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\ts = target + len;\n\treturn s + strspn(s, seps);\n}\n\n \nint dfs_cache_get_tgt_share(char *path, const struct dfs_cache_tgt_iterator *it, char **share,\n\t\t\t    char **prefix)\n{\n\tchar sep;\n\tchar *target_share;\n\tchar *ppath = NULL;\n\tconst char *target_ppath, *dfsref_ppath;\n\tsize_t target_pplen, dfsref_pplen;\n\tsize_t len, c;\n\n\tif (!it || !path || !share || !prefix || strlen(path) < it->it_path_consumed)\n\t\treturn -EINVAL;\n\n\tsep = it->it_name[0];\n\tif (sep != '\\\\' && sep != '/')\n\t\treturn -EINVAL;\n\n\ttarget_ppath = parse_target_share(it->it_name, &target_share);\n\tif (IS_ERR(target_ppath))\n\t\treturn PTR_ERR(target_ppath);\n\n\t \n\tdfsref_ppath = path + it->it_path_consumed;\n\tdfsref_ppath += strspn(dfsref_ppath, \"/\\\\\");\n\n\ttarget_pplen = strlen(target_ppath);\n\tdfsref_pplen = strlen(dfsref_ppath);\n\n\t \n\tif (target_pplen || dfsref_pplen) {\n\t\tlen = target_pplen + dfsref_pplen + 2;\n\t\tppath = kzalloc(len, GFP_KERNEL);\n\t\tif (!ppath) {\n\t\t\tkfree(target_share);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tc = strscpy(ppath, target_ppath, len);\n\t\tif (c && dfsref_pplen)\n\t\t\tppath[c] = sep;\n\t\tstrlcat(ppath, dfsref_ppath, len);\n\t}\n\t*share = target_share;\n\t*prefix = ppath;\n\treturn 0;\n}\n\nstatic bool target_share_equal(struct TCP_Server_Info *server, const char *s1, const char *s2)\n{\n\tchar unc[sizeof(\"\\\\\\\\\") + SERVER_NAME_LENGTH] = {0};\n\tconst char *host;\n\tsize_t hostlen;\n\tstruct sockaddr_storage ss;\n\tbool match;\n\tint rc;\n\n\tif (strcasecmp(s1, s2))\n\t\treturn false;\n\n\t \n\textract_unc_hostname(s1, &host, &hostlen);\n\tscnprintf(unc, sizeof(unc), \"\\\\\\\\%.*s\", (int)hostlen, host);\n\n\trc = dns_resolve_server_name_to_ip(unc, (struct sockaddr *)&ss, NULL);\n\tif (rc < 0) {\n\t\tcifs_dbg(FYI, \"%s: could not resolve %.*s. assuming server address matches.\\n\",\n\t\t\t __func__, (int)hostlen, host);\n\t\treturn true;\n\t}\n\n\tcifs_server_lock(server);\n\tmatch = cifs_match_ipaddr((struct sockaddr *)&server->dstaddr, (struct sockaddr *)&ss);\n\tcifs_server_unlock(server);\n\n\treturn match;\n}\n\n \nstatic void mark_for_reconnect_if_needed(struct TCP_Server_Info *server,\n\t\t\t\t\t const char *path,\n\t\t\t\t\t struct dfs_cache_tgt_list *old_tl,\n\t\t\t\t\t struct dfs_cache_tgt_list *new_tl)\n{\n\tstruct dfs_cache_tgt_iterator *oit, *nit;\n\n\tfor (oit = dfs_cache_get_tgt_iterator(old_tl); oit;\n\t     oit = dfs_cache_get_next_tgt(old_tl, oit)) {\n\t\tfor (nit = dfs_cache_get_tgt_iterator(new_tl); nit;\n\t\t     nit = dfs_cache_get_next_tgt(new_tl, nit)) {\n\t\t\tif (target_share_equal(server,\n\t\t\t\t\t       dfs_cache_get_tgt_name(oit),\n\t\t\t\t\t       dfs_cache_get_tgt_name(nit))) {\n\t\t\t\tdfs_cache_noreq_update_tgthint(path, nit);\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}\n\n\tcifs_dbg(FYI, \"%s: no cached or matched targets. mark dfs share for reconnect.\\n\", __func__);\n\tcifs_signal_cifsd_for_reconnect(server, true);\n}\n\nstatic bool is_ses_good(struct cifs_ses *ses)\n{\n\tstruct TCP_Server_Info *server = ses->server;\n\tstruct cifs_tcon *tcon = ses->tcon_ipc;\n\tbool ret;\n\n\tspin_lock(&ses->ses_lock);\n\tspin_lock(&ses->chan_lock);\n\tret = !cifs_chan_needs_reconnect(ses, server) &&\n\t\tses->ses_status == SES_GOOD &&\n\t\t!tcon->need_reconnect;\n\tspin_unlock(&ses->chan_lock);\n\tspin_unlock(&ses->ses_lock);\n\treturn ret;\n}\n\n \nstatic int __refresh_tcon(const char *path, struct cifs_ses *ses, bool force_refresh)\n{\n\tstruct TCP_Server_Info *server = ses->server;\n\tDFS_CACHE_TGT_LIST(old_tl);\n\tDFS_CACHE_TGT_LIST(new_tl);\n\tbool needs_refresh = false;\n\tstruct cache_entry *ce;\n\tunsigned int xid;\n\tint rc = 0;\n\n\txid = get_xid();\n\n\tdown_read(&htable_rw_lock);\n\tce = lookup_cache_entry(path);\n\tneeds_refresh = force_refresh || IS_ERR(ce) || cache_entry_expired(ce);\n\tif (!IS_ERR(ce)) {\n\t\trc = get_targets(ce, &old_tl);\n\t\tcifs_dbg(FYI, \"%s: get_targets: %d\\n\", __func__, rc);\n\t}\n\tup_read(&htable_rw_lock);\n\n\tif (!needs_refresh) {\n\t\trc = 0;\n\t\tgoto out;\n\t}\n\n\tses = CIFS_DFS_ROOT_SES(ses);\n\tif (!is_ses_good(ses)) {\n\t\tcifs_dbg(FYI, \"%s: skip cache refresh due to disconnected ipc\\n\",\n\t\t\t __func__);\n\t\tgoto out;\n\t}\n\n\tce = cache_refresh_path(xid, ses, path, true);\n\tif (!IS_ERR(ce)) {\n\t\trc = get_targets(ce, &new_tl);\n\t\tup_read(&htable_rw_lock);\n\t\tcifs_dbg(FYI, \"%s: get_targets: %d\\n\", __func__, rc);\n\t\tmark_for_reconnect_if_needed(server, path, &old_tl, &new_tl);\n\t}\n\nout:\n\tfree_xid(xid);\n\tdfs_cache_free_tgts(&old_tl);\n\tdfs_cache_free_tgts(&new_tl);\n\treturn rc;\n}\n\nstatic int refresh_tcon(struct cifs_tcon *tcon, bool force_refresh)\n{\n\tstruct TCP_Server_Info *server = tcon->ses->server;\n\tstruct cifs_ses *ses = tcon->ses;\n\n\tmutex_lock(&server->refpath_lock);\n\tif (server->leaf_fullpath)\n\t\t__refresh_tcon(server->leaf_fullpath + 1, ses, force_refresh);\n\tmutex_unlock(&server->refpath_lock);\n\treturn 0;\n}\n\n \nint dfs_cache_remount_fs(struct cifs_sb_info *cifs_sb)\n{\n\tstruct cifs_tcon *tcon;\n\n\tif (!cifs_sb || !cifs_sb->master_tlink)\n\t\treturn -EINVAL;\n\n\ttcon = cifs_sb_master_tcon(cifs_sb);\n\n\tspin_lock(&tcon->tc_lock);\n\tif (!tcon->origin_fullpath) {\n\t\tspin_unlock(&tcon->tc_lock);\n\t\tcifs_dbg(FYI, \"%s: not a dfs mount\\n\", __func__);\n\t\treturn 0;\n\t}\n\tspin_unlock(&tcon->tc_lock);\n\n\t \n\tcifs_autodisable_serverino(cifs_sb);\n\t \n\tcifs_sb->mnt_cifs_flags |= CIFS_MOUNT_USE_PREFIX_PATH;\n\n\treturn refresh_tcon(tcon, true);\n}\n\n \nvoid dfs_cache_refresh(struct work_struct *work)\n{\n\tstruct TCP_Server_Info *server;\n\tstruct dfs_root_ses *rses;\n\tstruct cifs_tcon *tcon;\n\tstruct cifs_ses *ses;\n\n\ttcon = container_of(work, struct cifs_tcon, dfs_cache_work.work);\n\tses = tcon->ses;\n\tserver = ses->server;\n\n\tmutex_lock(&server->refpath_lock);\n\tif (server->leaf_fullpath)\n\t\t__refresh_tcon(server->leaf_fullpath + 1, ses, false);\n\tmutex_unlock(&server->refpath_lock);\n\n\tlist_for_each_entry(rses, &tcon->dfs_ses_list, list) {\n\t\tses = rses->ses;\n\t\tserver = ses->server;\n\t\tmutex_lock(&server->refpath_lock);\n\t\tif (server->leaf_fullpath)\n\t\t\t__refresh_tcon(server->leaf_fullpath + 1, ses, false);\n\t\tmutex_unlock(&server->refpath_lock);\n\t}\n\n\tqueue_delayed_work(dfscache_wq, &tcon->dfs_cache_work,\n\t\t\t   atomic_read(&dfs_cache_ttl) * HZ);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}