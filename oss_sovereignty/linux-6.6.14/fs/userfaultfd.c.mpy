{
  "module_name": "userfaultfd.c",
  "hash_id": "db33b827e154f206e2edf5d698c73107fe43d74606b308397b2302d4447183dd",
  "original_prompt": "Ingested from linux-6.6.14/fs/userfaultfd.c",
  "human_readable_source": "\n \n\n#include <linux/list.h>\n#include <linux/hashtable.h>\n#include <linux/sched/signal.h>\n#include <linux/sched/mm.h>\n#include <linux/mm.h>\n#include <linux/mm_inline.h>\n#include <linux/mmu_notifier.h>\n#include <linux/poll.h>\n#include <linux/slab.h>\n#include <linux/seq_file.h>\n#include <linux/file.h>\n#include <linux/bug.h>\n#include <linux/anon_inodes.h>\n#include <linux/syscalls.h>\n#include <linux/userfaultfd_k.h>\n#include <linux/mempolicy.h>\n#include <linux/ioctl.h>\n#include <linux/security.h>\n#include <linux/hugetlb.h>\n#include <linux/swapops.h>\n#include <linux/miscdevice.h>\n\nstatic int sysctl_unprivileged_userfaultfd __read_mostly;\n\n#ifdef CONFIG_SYSCTL\nstatic struct ctl_table vm_userfaultfd_table[] = {\n\t{\n\t\t.procname\t= \"unprivileged_userfaultfd\",\n\t\t.data\t\t= &sysctl_unprivileged_userfaultfd,\n\t\t.maxlen\t\t= sizeof(sysctl_unprivileged_userfaultfd),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= SYSCTL_ZERO,\n\t\t.extra2\t\t= SYSCTL_ONE,\n\t},\n\t{ }\n};\n#endif\n\nstatic struct kmem_cache *userfaultfd_ctx_cachep __read_mostly;\n\n \nstruct userfaultfd_ctx {\n\t \n\twait_queue_head_t fault_pending_wqh;\n\t \n\twait_queue_head_t fault_wqh;\n\t \n\twait_queue_head_t fd_wqh;\n\t \n\twait_queue_head_t event_wqh;\n\t \n\tseqcount_spinlock_t refile_seq;\n\t \n\trefcount_t refcount;\n\t \n\tunsigned int flags;\n\t \n\tunsigned int features;\n\t \n\tbool released;\n\t \n\tatomic_t mmap_changing;\n\t \n\tstruct mm_struct *mm;\n};\n\nstruct userfaultfd_fork_ctx {\n\tstruct userfaultfd_ctx *orig;\n\tstruct userfaultfd_ctx *new;\n\tstruct list_head list;\n};\n\nstruct userfaultfd_unmap_ctx {\n\tstruct userfaultfd_ctx *ctx;\n\tunsigned long start;\n\tunsigned long end;\n\tstruct list_head list;\n};\n\nstruct userfaultfd_wait_queue {\n\tstruct uffd_msg msg;\n\twait_queue_entry_t wq;\n\tstruct userfaultfd_ctx *ctx;\n\tbool waken;\n};\n\nstruct userfaultfd_wake_range {\n\tunsigned long start;\n\tunsigned long len;\n};\n\n \n#define UFFD_FEATURE_INITIALIZED\t\t(1u << 31)\n\nstatic bool userfaultfd_is_initialized(struct userfaultfd_ctx *ctx)\n{\n\treturn ctx->features & UFFD_FEATURE_INITIALIZED;\n}\n\n \nbool userfaultfd_wp_unpopulated(struct vm_area_struct *vma)\n{\n\tstruct userfaultfd_ctx *ctx = vma->vm_userfaultfd_ctx.ctx;\n\n\tif (!ctx)\n\t\treturn false;\n\n\treturn ctx->features & UFFD_FEATURE_WP_UNPOPULATED;\n}\n\nstatic void userfaultfd_set_vm_flags(struct vm_area_struct *vma,\n\t\t\t\t     vm_flags_t flags)\n{\n\tconst bool uffd_wp_changed = (vma->vm_flags ^ flags) & VM_UFFD_WP;\n\n\tvm_flags_reset(vma, flags);\n\t \n\tif ((vma->vm_flags & VM_SHARED) && uffd_wp_changed)\n\t\tvma_set_page_prot(vma);\n}\n\nstatic int userfaultfd_wake_function(wait_queue_entry_t *wq, unsigned mode,\n\t\t\t\t     int wake_flags, void *key)\n{\n\tstruct userfaultfd_wake_range *range = key;\n\tint ret;\n\tstruct userfaultfd_wait_queue *uwq;\n\tunsigned long start, len;\n\n\tuwq = container_of(wq, struct userfaultfd_wait_queue, wq);\n\tret = 0;\n\t \n\tstart = range->start;\n\tlen = range->len;\n\tif (len && (start > uwq->msg.arg.pagefault.address ||\n\t\t    start + len <= uwq->msg.arg.pagefault.address))\n\t\tgoto out;\n\tWRITE_ONCE(uwq->waken, true);\n\t \n\tret = wake_up_state(wq->private, mode);\n\tif (ret) {\n\t\t \n\t\tlist_del_init(&wq->entry);\n\t}\nout:\n\treturn ret;\n}\n\n \nstatic void userfaultfd_ctx_get(struct userfaultfd_ctx *ctx)\n{\n\trefcount_inc(&ctx->refcount);\n}\n\n \nstatic void userfaultfd_ctx_put(struct userfaultfd_ctx *ctx)\n{\n\tif (refcount_dec_and_test(&ctx->refcount)) {\n\t\tVM_BUG_ON(spin_is_locked(&ctx->fault_pending_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->fault_pending_wqh));\n\t\tVM_BUG_ON(spin_is_locked(&ctx->fault_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->fault_wqh));\n\t\tVM_BUG_ON(spin_is_locked(&ctx->event_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->event_wqh));\n\t\tVM_BUG_ON(spin_is_locked(&ctx->fd_wqh.lock));\n\t\tVM_BUG_ON(waitqueue_active(&ctx->fd_wqh));\n\t\tmmdrop(ctx->mm);\n\t\tkmem_cache_free(userfaultfd_ctx_cachep, ctx);\n\t}\n}\n\nstatic inline void msg_init(struct uffd_msg *msg)\n{\n\tBUILD_BUG_ON(sizeof(struct uffd_msg) != 32);\n\t \n\tmemset(msg, 0, sizeof(struct uffd_msg));\n}\n\nstatic inline struct uffd_msg userfault_msg(unsigned long address,\n\t\t\t\t\t    unsigned long real_address,\n\t\t\t\t\t    unsigned int flags,\n\t\t\t\t\t    unsigned long reason,\n\t\t\t\t\t    unsigned int features)\n{\n\tstruct uffd_msg msg;\n\n\tmsg_init(&msg);\n\tmsg.event = UFFD_EVENT_PAGEFAULT;\n\n\tmsg.arg.pagefault.address = (features & UFFD_FEATURE_EXACT_ADDRESS) ?\n\t\t\t\t    real_address : address;\n\n\t \n\tif (flags & FAULT_FLAG_WRITE)\n\t\tmsg.arg.pagefault.flags |= UFFD_PAGEFAULT_FLAG_WRITE;\n\tif (reason & VM_UFFD_WP)\n\t\tmsg.arg.pagefault.flags |= UFFD_PAGEFAULT_FLAG_WP;\n\tif (reason & VM_UFFD_MINOR)\n\t\tmsg.arg.pagefault.flags |= UFFD_PAGEFAULT_FLAG_MINOR;\n\tif (features & UFFD_FEATURE_THREAD_ID)\n\t\tmsg.arg.pagefault.feat.ptid = task_pid_vnr(current);\n\treturn msg;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\n \nstatic inline bool userfaultfd_huge_must_wait(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t      struct vm_fault *vmf,\n\t\t\t\t\t      unsigned long reason)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tpte_t *ptep, pte;\n\tbool ret = true;\n\n\tassert_fault_locked(vmf);\n\n\tptep = hugetlb_walk(vma, vmf->address, vma_mmu_pagesize(vma));\n\tif (!ptep)\n\t\tgoto out;\n\n\tret = false;\n\tpte = huge_ptep_get(ptep);\n\n\t \n\tif (huge_pte_none_mostly(pte))\n\t\tret = true;\n\tif (!huge_pte_write(pte) && (reason & VM_UFFD_WP))\n\t\tret = true;\nout:\n\treturn ret;\n}\n#else\nstatic inline bool userfaultfd_huge_must_wait(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t      struct vm_fault *vmf,\n\t\t\t\t\t      unsigned long reason)\n{\n\treturn false;\t \n}\n#endif  \n\n \nstatic inline bool userfaultfd_must_wait(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t struct vm_fault *vmf,\n\t\t\t\t\t unsigned long reason)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tunsigned long address = vmf->address;\n\tpgd_t *pgd;\n\tp4d_t *p4d;\n\tpud_t *pud;\n\tpmd_t *pmd, _pmd;\n\tpte_t *pte;\n\tpte_t ptent;\n\tbool ret = true;\n\n\tassert_fault_locked(vmf);\n\n\tpgd = pgd_offset(mm, address);\n\tif (!pgd_present(*pgd))\n\t\tgoto out;\n\tp4d = p4d_offset(pgd, address);\n\tif (!p4d_present(*p4d))\n\t\tgoto out;\n\tpud = pud_offset(p4d, address);\n\tif (!pud_present(*pud))\n\t\tgoto out;\n\tpmd = pmd_offset(pud, address);\nagain:\n\t_pmd = pmdp_get_lockless(pmd);\n\tif (pmd_none(_pmd))\n\t\tgoto out;\n\n\tret = false;\n\tif (!pmd_present(_pmd) || pmd_devmap(_pmd))\n\t\tgoto out;\n\n\tif (pmd_trans_huge(_pmd)) {\n\t\tif (!pmd_write(_pmd) && (reason & VM_UFFD_WP))\n\t\t\tret = true;\n\t\tgoto out;\n\t}\n\n\tpte = pte_offset_map(pmd, address);\n\tif (!pte) {\n\t\tret = true;\n\t\tgoto again;\n\t}\n\t \n\tptent = ptep_get(pte);\n\tif (pte_none_mostly(ptent))\n\t\tret = true;\n\tif (!pte_write(ptent) && (reason & VM_UFFD_WP))\n\t\tret = true;\n\tpte_unmap(pte);\n\nout:\n\treturn ret;\n}\n\nstatic inline unsigned int userfaultfd_get_blocking_state(unsigned int flags)\n{\n\tif (flags & FAULT_FLAG_INTERRUPTIBLE)\n\t\treturn TASK_INTERRUPTIBLE;\n\n\tif (flags & FAULT_FLAG_KILLABLE)\n\t\treturn TASK_KILLABLE;\n\n\treturn TASK_UNINTERRUPTIBLE;\n}\n\n \nvm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct userfaultfd_ctx *ctx;\n\tstruct userfaultfd_wait_queue uwq;\n\tvm_fault_t ret = VM_FAULT_SIGBUS;\n\tbool must_wait;\n\tunsigned int blocking_state;\n\n\t \n\tif (current->flags & (PF_EXITING|PF_DUMPCORE))\n\t\tgoto out;\n\n\tassert_fault_locked(vmf);\n\n\tctx = vma->vm_userfaultfd_ctx.ctx;\n\tif (!ctx)\n\t\tgoto out;\n\n\tBUG_ON(ctx->mm != mm);\n\n\t \n\tVM_BUG_ON(reason & ~__VM_UFFD_FLAGS);\n\t \n\tVM_BUG_ON(!reason || (reason & (reason - 1)));\n\n\tif (ctx->features & UFFD_FEATURE_SIGBUS)\n\t\tgoto out;\n\tif (!(vmf->flags & FAULT_FLAG_USER) && (ctx->flags & UFFD_USER_MODE_ONLY))\n\t\tgoto out;\n\n\t \n\tif (unlikely(READ_ONCE(ctx->released))) {\n\t\t \n\t\tret = VM_FAULT_NOPAGE;\n\t\tgoto out;\n\t}\n\n\t \n\tif (unlikely(!(vmf->flags & FAULT_FLAG_ALLOW_RETRY))) {\n\t\t \n\t\tBUG_ON(vmf->flags & FAULT_FLAG_RETRY_NOWAIT);\n#ifdef CONFIG_DEBUG_VM\n\t\tif (printk_ratelimit()) {\n\t\t\tprintk(KERN_WARNING\n\t\t\t       \"FAULT_FLAG_ALLOW_RETRY missing %x\\n\",\n\t\t\t       vmf->flags);\n\t\t\tdump_stack();\n\t\t}\n#endif\n\t\tgoto out;\n\t}\n\n\t \n\tret = VM_FAULT_RETRY;\n\tif (vmf->flags & FAULT_FLAG_RETRY_NOWAIT)\n\t\tgoto out;\n\n\t \n\tuserfaultfd_ctx_get(ctx);\n\n\tinit_waitqueue_func_entry(&uwq.wq, userfaultfd_wake_function);\n\tuwq.wq.private = current;\n\tuwq.msg = userfault_msg(vmf->address, vmf->real_address, vmf->flags,\n\t\t\t\treason, ctx->features);\n\tuwq.ctx = ctx;\n\tuwq.waken = false;\n\n\tblocking_state = userfaultfd_get_blocking_state(vmf->flags);\n\n         \n\tif (is_vm_hugetlb_page(vma))\n\t\thugetlb_vma_lock_read(vma);\n\n\tspin_lock_irq(&ctx->fault_pending_wqh.lock);\n\t \n\t__add_wait_queue(&ctx->fault_pending_wqh, &uwq.wq);\n\t \n\tset_current_state(blocking_state);\n\tspin_unlock_irq(&ctx->fault_pending_wqh.lock);\n\n\tif (!is_vm_hugetlb_page(vma))\n\t\tmust_wait = userfaultfd_must_wait(ctx, vmf, reason);\n\telse\n\t\tmust_wait = userfaultfd_huge_must_wait(ctx, vmf, reason);\n\tif (is_vm_hugetlb_page(vma))\n\t\thugetlb_vma_unlock_read(vma);\n\trelease_fault_lock(vmf);\n\n\tif (likely(must_wait && !READ_ONCE(ctx->released))) {\n\t\twake_up_poll(&ctx->fd_wqh, EPOLLIN);\n\t\tschedule();\n\t}\n\n\t__set_current_state(TASK_RUNNING);\n\n\t \n\tif (!list_empty_careful(&uwq.wq.entry)) {\n\t\tspin_lock_irq(&ctx->fault_pending_wqh.lock);\n\t\t \n\t\tlist_del(&uwq.wq.entry);\n\t\tspin_unlock_irq(&ctx->fault_pending_wqh.lock);\n\t}\n\n\t \n\tuserfaultfd_ctx_put(ctx);\n\nout:\n\treturn ret;\n}\n\nstatic void userfaultfd_event_wait_completion(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t      struct userfaultfd_wait_queue *ewq)\n{\n\tstruct userfaultfd_ctx *release_new_ctx;\n\n\tif (WARN_ON_ONCE(current->flags & PF_EXITING))\n\t\tgoto out;\n\n\tewq->ctx = ctx;\n\tinit_waitqueue_entry(&ewq->wq, current);\n\trelease_new_ctx = NULL;\n\n\tspin_lock_irq(&ctx->event_wqh.lock);\n\t \n\t__add_wait_queue(&ctx->event_wqh, &ewq->wq);\n\tfor (;;) {\n\t\tset_current_state(TASK_KILLABLE);\n\t\tif (ewq->msg.event == 0)\n\t\t\tbreak;\n\t\tif (READ_ONCE(ctx->released) ||\n\t\t    fatal_signal_pending(current)) {\n\t\t\t \n\t\t\t__remove_wait_queue(&ctx->event_wqh, &ewq->wq);\n\t\t\tif (ewq->msg.event == UFFD_EVENT_FORK) {\n\t\t\t\tstruct userfaultfd_ctx *new;\n\n\t\t\t\tnew = (struct userfaultfd_ctx *)\n\t\t\t\t\t(unsigned long)\n\t\t\t\t\tewq->msg.arg.reserved.reserved1;\n\t\t\t\trelease_new_ctx = new;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_unlock_irq(&ctx->event_wqh.lock);\n\n\t\twake_up_poll(&ctx->fd_wqh, EPOLLIN);\n\t\tschedule();\n\n\t\tspin_lock_irq(&ctx->event_wqh.lock);\n\t}\n\t__set_current_state(TASK_RUNNING);\n\tspin_unlock_irq(&ctx->event_wqh.lock);\n\n\tif (release_new_ctx) {\n\t\tstruct vm_area_struct *vma;\n\t\tstruct mm_struct *mm = release_new_ctx->mm;\n\t\tVMA_ITERATOR(vmi, mm, 0);\n\n\t\t \n\t\tmmap_write_lock(mm);\n\t\tfor_each_vma(vmi, vma) {\n\t\t\tif (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {\n\t\t\t\tvma_start_write(vma);\n\t\t\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t\t\t\tuserfaultfd_set_vm_flags(vma,\n\t\t\t\t\t\t\t vma->vm_flags & ~__VM_UFFD_FLAGS);\n\t\t\t}\n\t\t}\n\t\tmmap_write_unlock(mm);\n\n\t\tuserfaultfd_ctx_put(release_new_ctx);\n\t}\n\n\t \nout:\n\tatomic_dec(&ctx->mmap_changing);\n\tVM_BUG_ON(atomic_read(&ctx->mmap_changing) < 0);\n\tuserfaultfd_ctx_put(ctx);\n}\n\nstatic void userfaultfd_event_complete(struct userfaultfd_ctx *ctx,\n\t\t\t\t       struct userfaultfd_wait_queue *ewq)\n{\n\tewq->msg.event = 0;\n\twake_up_locked(&ctx->event_wqh);\n\t__remove_wait_queue(&ctx->event_wqh, &ewq->wq);\n}\n\nint dup_userfaultfd(struct vm_area_struct *vma, struct list_head *fcs)\n{\n\tstruct userfaultfd_ctx *ctx = NULL, *octx;\n\tstruct userfaultfd_fork_ctx *fctx;\n\n\toctx = vma->vm_userfaultfd_ctx.ctx;\n\tif (!octx || !(octx->features & UFFD_FEATURE_EVENT_FORK)) {\n\t\tvma_start_write(vma);\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t\tuserfaultfd_set_vm_flags(vma, vma->vm_flags & ~__VM_UFFD_FLAGS);\n\t\treturn 0;\n\t}\n\n\tlist_for_each_entry(fctx, fcs, list)\n\t\tif (fctx->orig == octx) {\n\t\t\tctx = fctx->new;\n\t\t\tbreak;\n\t\t}\n\n\tif (!ctx) {\n\t\tfctx = kmalloc(sizeof(*fctx), GFP_KERNEL);\n\t\tif (!fctx)\n\t\t\treturn -ENOMEM;\n\n\t\tctx = kmem_cache_alloc(userfaultfd_ctx_cachep, GFP_KERNEL);\n\t\tif (!ctx) {\n\t\t\tkfree(fctx);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\trefcount_set(&ctx->refcount, 1);\n\t\tctx->flags = octx->flags;\n\t\tctx->features = octx->features;\n\t\tctx->released = false;\n\t\tatomic_set(&ctx->mmap_changing, 0);\n\t\tctx->mm = vma->vm_mm;\n\t\tmmgrab(ctx->mm);\n\n\t\tuserfaultfd_ctx_get(octx);\n\t\tatomic_inc(&octx->mmap_changing);\n\t\tfctx->orig = octx;\n\t\tfctx->new = ctx;\n\t\tlist_add_tail(&fctx->list, fcs);\n\t}\n\n\tvma->vm_userfaultfd_ctx.ctx = ctx;\n\treturn 0;\n}\n\nstatic void dup_fctx(struct userfaultfd_fork_ctx *fctx)\n{\n\tstruct userfaultfd_ctx *ctx = fctx->orig;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tmsg_init(&ewq.msg);\n\n\tewq.msg.event = UFFD_EVENT_FORK;\n\tewq.msg.arg.reserved.reserved1 = (unsigned long)fctx->new;\n\n\tuserfaultfd_event_wait_completion(ctx, &ewq);\n}\n\nvoid dup_userfaultfd_complete(struct list_head *fcs)\n{\n\tstruct userfaultfd_fork_ctx *fctx, *n;\n\n\tlist_for_each_entry_safe(fctx, n, fcs, list) {\n\t\tdup_fctx(fctx);\n\t\tlist_del(&fctx->list);\n\t\tkfree(fctx);\n\t}\n}\n\nvoid mremap_userfaultfd_prep(struct vm_area_struct *vma,\n\t\t\t     struct vm_userfaultfd_ctx *vm_ctx)\n{\n\tstruct userfaultfd_ctx *ctx;\n\n\tctx = vma->vm_userfaultfd_ctx.ctx;\n\n\tif (!ctx)\n\t\treturn;\n\n\tif (ctx->features & UFFD_FEATURE_EVENT_REMAP) {\n\t\tvm_ctx->ctx = ctx;\n\t\tuserfaultfd_ctx_get(ctx);\n\t\tatomic_inc(&ctx->mmap_changing);\n\t} else {\n\t\t \n\t\tvma_start_write(vma);\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t\tuserfaultfd_set_vm_flags(vma, vma->vm_flags & ~__VM_UFFD_FLAGS);\n\t}\n}\n\nvoid mremap_userfaultfd_complete(struct vm_userfaultfd_ctx *vm_ctx,\n\t\t\t\t unsigned long from, unsigned long to,\n\t\t\t\t unsigned long len)\n{\n\tstruct userfaultfd_ctx *ctx = vm_ctx->ctx;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tif (!ctx)\n\t\treturn;\n\n\tif (to & ~PAGE_MASK) {\n\t\tuserfaultfd_ctx_put(ctx);\n\t\treturn;\n\t}\n\n\tmsg_init(&ewq.msg);\n\n\tewq.msg.event = UFFD_EVENT_REMAP;\n\tewq.msg.arg.remap.from = from;\n\tewq.msg.arg.remap.to = to;\n\tewq.msg.arg.remap.len = len;\n\n\tuserfaultfd_event_wait_completion(ctx, &ewq);\n}\n\nbool userfaultfd_remove(struct vm_area_struct *vma,\n\t\t\tunsigned long start, unsigned long end)\n{\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct userfaultfd_ctx *ctx;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tctx = vma->vm_userfaultfd_ctx.ctx;\n\tif (!ctx || !(ctx->features & UFFD_FEATURE_EVENT_REMOVE))\n\t\treturn true;\n\n\tuserfaultfd_ctx_get(ctx);\n\tatomic_inc(&ctx->mmap_changing);\n\tmmap_read_unlock(mm);\n\n\tmsg_init(&ewq.msg);\n\n\tewq.msg.event = UFFD_EVENT_REMOVE;\n\tewq.msg.arg.remove.start = start;\n\tewq.msg.arg.remove.end = end;\n\n\tuserfaultfd_event_wait_completion(ctx, &ewq);\n\n\treturn false;\n}\n\nstatic bool has_unmap_ctx(struct userfaultfd_ctx *ctx, struct list_head *unmaps,\n\t\t\t  unsigned long start, unsigned long end)\n{\n\tstruct userfaultfd_unmap_ctx *unmap_ctx;\n\n\tlist_for_each_entry(unmap_ctx, unmaps, list)\n\t\tif (unmap_ctx->ctx == ctx && unmap_ctx->start == start &&\n\t\t    unmap_ctx->end == end)\n\t\t\treturn true;\n\n\treturn false;\n}\n\nint userfaultfd_unmap_prep(struct vm_area_struct *vma, unsigned long start,\n\t\t\t   unsigned long end, struct list_head *unmaps)\n{\n\tstruct userfaultfd_unmap_ctx *unmap_ctx;\n\tstruct userfaultfd_ctx *ctx = vma->vm_userfaultfd_ctx.ctx;\n\n\tif (!ctx || !(ctx->features & UFFD_FEATURE_EVENT_UNMAP) ||\n\t    has_unmap_ctx(ctx, unmaps, start, end))\n\t\treturn 0;\n\n\tunmap_ctx = kzalloc(sizeof(*unmap_ctx), GFP_KERNEL);\n\tif (!unmap_ctx)\n\t\treturn -ENOMEM;\n\n\tuserfaultfd_ctx_get(ctx);\n\tatomic_inc(&ctx->mmap_changing);\n\tunmap_ctx->ctx = ctx;\n\tunmap_ctx->start = start;\n\tunmap_ctx->end = end;\n\tlist_add_tail(&unmap_ctx->list, unmaps);\n\n\treturn 0;\n}\n\nvoid userfaultfd_unmap_complete(struct mm_struct *mm, struct list_head *uf)\n{\n\tstruct userfaultfd_unmap_ctx *ctx, *n;\n\tstruct userfaultfd_wait_queue ewq;\n\n\tlist_for_each_entry_safe(ctx, n, uf, list) {\n\t\tmsg_init(&ewq.msg);\n\n\t\tewq.msg.event = UFFD_EVENT_UNMAP;\n\t\tewq.msg.arg.remove.start = ctx->start;\n\t\tewq.msg.arg.remove.end = ctx->end;\n\n\t\tuserfaultfd_event_wait_completion(ctx->ctx, &ewq);\n\n\t\tlist_del(&ctx->list);\n\t\tkfree(ctx);\n\t}\n}\n\nstatic int userfaultfd_release(struct inode *inode, struct file *file)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev;\n\t \n\tstruct userfaultfd_wake_range range = { .len = 0, };\n\tunsigned long new_flags;\n\tVMA_ITERATOR(vmi, mm, 0);\n\n\tWRITE_ONCE(ctx->released, true);\n\n\tif (!mmget_not_zero(mm))\n\t\tgoto wakeup;\n\n\t \n\tmmap_write_lock(mm);\n\tprev = NULL;\n\tfor_each_vma(vmi, vma) {\n\t\tcond_resched();\n\t\tBUG_ON(!!vma->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(vma->vm_flags & __VM_UFFD_FLAGS));\n\t\tif (vma->vm_userfaultfd_ctx.ctx != ctx) {\n\t\t\tprev = vma;\n\t\t\tcontinue;\n\t\t}\n\t\tnew_flags = vma->vm_flags & ~__VM_UFFD_FLAGS;\n\t\tprev = vma_merge(&vmi, mm, prev, vma->vm_start, vma->vm_end,\n\t\t\t\t new_flags, vma->anon_vma,\n\t\t\t\t vma->vm_file, vma->vm_pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX, anon_vma_name(vma));\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t} else {\n\t\t\tprev = vma;\n\t\t}\n\n\t\tvma_start_write(vma);\n\t\tuserfaultfd_set_vm_flags(vma, new_flags);\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\t}\n\tmmap_write_unlock(mm);\n\tmmput(mm);\nwakeup:\n\t \n\tspin_lock_irq(&ctx->fault_pending_wqh.lock);\n\t__wake_up_locked_key(&ctx->fault_pending_wqh, TASK_NORMAL, &range);\n\t__wake_up(&ctx->fault_wqh, TASK_NORMAL, 1, &range);\n\tspin_unlock_irq(&ctx->fault_pending_wqh.lock);\n\n\t \n\twake_up_all(&ctx->event_wqh);\n\n\twake_up_poll(&ctx->fd_wqh, EPOLLHUP);\n\tuserfaultfd_ctx_put(ctx);\n\treturn 0;\n}\n\n \nstatic inline struct userfaultfd_wait_queue *find_userfault_in(\n\t\twait_queue_head_t *wqh)\n{\n\twait_queue_entry_t *wq;\n\tstruct userfaultfd_wait_queue *uwq;\n\n\tlockdep_assert_held(&wqh->lock);\n\n\tuwq = NULL;\n\tif (!waitqueue_active(wqh))\n\t\tgoto out;\n\t \n\twq = list_last_entry(&wqh->head, typeof(*wq), entry);\n\tuwq = container_of(wq, struct userfaultfd_wait_queue, wq);\nout:\n\treturn uwq;\n}\n\nstatic inline struct userfaultfd_wait_queue *find_userfault(\n\t\tstruct userfaultfd_ctx *ctx)\n{\n\treturn find_userfault_in(&ctx->fault_pending_wqh);\n}\n\nstatic inline struct userfaultfd_wait_queue *find_userfault_evt(\n\t\tstruct userfaultfd_ctx *ctx)\n{\n\treturn find_userfault_in(&ctx->event_wqh);\n}\n\nstatic __poll_t userfaultfd_poll(struct file *file, poll_table *wait)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\t__poll_t ret;\n\n\tpoll_wait(file, &ctx->fd_wqh, wait);\n\n\tif (!userfaultfd_is_initialized(ctx))\n\t\treturn EPOLLERR;\n\n\t \n\tif (unlikely(!(file->f_flags & O_NONBLOCK)))\n\t\treturn EPOLLERR;\n\t \n\tret = 0;\n\tsmp_mb();\n\tif (waitqueue_active(&ctx->fault_pending_wqh))\n\t\tret = EPOLLIN;\n\telse if (waitqueue_active(&ctx->event_wqh))\n\t\tret = EPOLLIN;\n\n\treturn ret;\n}\n\nstatic const struct file_operations userfaultfd_fops;\n\nstatic int resolve_userfault_fork(struct userfaultfd_ctx *new,\n\t\t\t\t  struct inode *inode,\n\t\t\t\t  struct uffd_msg *msg)\n{\n\tint fd;\n\n\tfd = anon_inode_getfd_secure(\"[userfaultfd]\", &userfaultfd_fops, new,\n\t\t\tO_RDONLY | (new->flags & UFFD_SHARED_FCNTL_FLAGS), inode);\n\tif (fd < 0)\n\t\treturn fd;\n\n\tmsg->arg.reserved.reserved1 = 0;\n\tmsg->arg.fork.ufd = fd;\n\treturn 0;\n}\n\nstatic ssize_t userfaultfd_ctx_read(struct userfaultfd_ctx *ctx, int no_wait,\n\t\t\t\t    struct uffd_msg *msg, struct inode *inode)\n{\n\tssize_t ret;\n\tDECLARE_WAITQUEUE(wait, current);\n\tstruct userfaultfd_wait_queue *uwq;\n\t \n\tLIST_HEAD(fork_event);\n\tstruct userfaultfd_ctx *fork_nctx = NULL;\n\n\t \n\tspin_lock_irq(&ctx->fd_wqh.lock);\n\t__add_wait_queue(&ctx->fd_wqh, &wait);\n\tfor (;;) {\n\t\tset_current_state(TASK_INTERRUPTIBLE);\n\t\tspin_lock(&ctx->fault_pending_wqh.lock);\n\t\tuwq = find_userfault(ctx);\n\t\tif (uwq) {\n\t\t\t \n\t\t\twrite_seqcount_begin(&ctx->refile_seq);\n\n\t\t\t \n\t\t\tlist_del(&uwq->wq.entry);\n\t\t\tadd_wait_queue(&ctx->fault_wqh, &uwq->wq);\n\n\t\t\twrite_seqcount_end(&ctx->refile_seq);\n\n\t\t\t \n\t\t\t*msg = uwq->msg;\n\t\t\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&ctx->fault_pending_wqh.lock);\n\n\t\tspin_lock(&ctx->event_wqh.lock);\n\t\tuwq = find_userfault_evt(ctx);\n\t\tif (uwq) {\n\t\t\t*msg = uwq->msg;\n\n\t\t\tif (uwq->msg.event == UFFD_EVENT_FORK) {\n\t\t\t\tfork_nctx = (struct userfaultfd_ctx *)\n\t\t\t\t\t(unsigned long)\n\t\t\t\t\tuwq->msg.arg.reserved.reserved1;\n\t\t\t\tlist_move(&uwq->wq.entry, &fork_event);\n\t\t\t\t \n\t\t\t\tuserfaultfd_ctx_get(fork_nctx);\n\t\t\t\tspin_unlock(&ctx->event_wqh.lock);\n\t\t\t\tret = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tuserfaultfd_event_complete(ctx, uwq);\n\t\t\tspin_unlock(&ctx->event_wqh.lock);\n\t\t\tret = 0;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&ctx->event_wqh.lock);\n\n\t\tif (signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\t\tif (no_wait) {\n\t\t\tret = -EAGAIN;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock_irq(&ctx->fd_wqh.lock);\n\t\tschedule();\n\t\tspin_lock_irq(&ctx->fd_wqh.lock);\n\t}\n\t__remove_wait_queue(&ctx->fd_wqh, &wait);\n\t__set_current_state(TASK_RUNNING);\n\tspin_unlock_irq(&ctx->fd_wqh.lock);\n\n\tif (!ret && msg->event == UFFD_EVENT_FORK) {\n\t\tret = resolve_userfault_fork(fork_nctx, inode, msg);\n\t\tspin_lock_irq(&ctx->event_wqh.lock);\n\t\tif (!list_empty(&fork_event)) {\n\t\t\t \n\t\t\tuserfaultfd_ctx_put(fork_nctx);\n\n\t\t\tuwq = list_first_entry(&fork_event,\n\t\t\t\t\t       typeof(*uwq),\n\t\t\t\t\t       wq.entry);\n\t\t\t \n\t\t\tlist_del(&uwq->wq.entry);\n\t\t\t__add_wait_queue(&ctx->event_wqh, &uwq->wq);\n\n\t\t\t \n\t\t\tif (likely(!ret))\n\t\t\t\tuserfaultfd_event_complete(ctx, uwq);\n\t\t} else {\n\t\t\t \n\t\t\tif (ret)\n\t\t\t\tuserfaultfd_ctx_put(fork_nctx);\n\t\t}\n\t\tspin_unlock_irq(&ctx->event_wqh.lock);\n\t}\n\n\treturn ret;\n}\n\nstatic ssize_t userfaultfd_read(struct file *file, char __user *buf,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\tssize_t _ret, ret = 0;\n\tstruct uffd_msg msg;\n\tint no_wait = file->f_flags & O_NONBLOCK;\n\tstruct inode *inode = file_inode(file);\n\n\tif (!userfaultfd_is_initialized(ctx))\n\t\treturn -EINVAL;\n\n\tfor (;;) {\n\t\tif (count < sizeof(msg))\n\t\t\treturn ret ? ret : -EINVAL;\n\t\t_ret = userfaultfd_ctx_read(ctx, no_wait, &msg, inode);\n\t\tif (_ret < 0)\n\t\t\treturn ret ? ret : _ret;\n\t\tif (copy_to_user((__u64 __user *) buf, &msg, sizeof(msg)))\n\t\t\treturn ret ? ret : -EFAULT;\n\t\tret += sizeof(msg);\n\t\tbuf += sizeof(msg);\n\t\tcount -= sizeof(msg);\n\t\t \n\t\tno_wait = O_NONBLOCK;\n\t}\n}\n\nstatic void __wake_userfault(struct userfaultfd_ctx *ctx,\n\t\t\t     struct userfaultfd_wake_range *range)\n{\n\tspin_lock_irq(&ctx->fault_pending_wqh.lock);\n\t \n\tif (waitqueue_active(&ctx->fault_pending_wqh))\n\t\t__wake_up_locked_key(&ctx->fault_pending_wqh, TASK_NORMAL,\n\t\t\t\t     range);\n\tif (waitqueue_active(&ctx->fault_wqh))\n\t\t__wake_up(&ctx->fault_wqh, TASK_NORMAL, 1, range);\n\tspin_unlock_irq(&ctx->fault_pending_wqh.lock);\n}\n\nstatic __always_inline void wake_userfault(struct userfaultfd_ctx *ctx,\n\t\t\t\t\t   struct userfaultfd_wake_range *range)\n{\n\tunsigned seq;\n\tbool need_wakeup;\n\n\t \n\tsmp_mb();\n\n\t \n\tdo {\n\t\tseq = read_seqcount_begin(&ctx->refile_seq);\n\t\tneed_wakeup = waitqueue_active(&ctx->fault_pending_wqh) ||\n\t\t\twaitqueue_active(&ctx->fault_wqh);\n\t\tcond_resched();\n\t} while (read_seqcount_retry(&ctx->refile_seq, seq));\n\tif (need_wakeup)\n\t\t__wake_userfault(ctx, range);\n}\n\nstatic __always_inline int validate_unaligned_range(\n\tstruct mm_struct *mm, __u64 start, __u64 len)\n{\n\t__u64 task_size = mm->task_size;\n\n\tif (len & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\tif (!len)\n\t\treturn -EINVAL;\n\tif (start < mmap_min_addr)\n\t\treturn -EINVAL;\n\tif (start >= task_size)\n\t\treturn -EINVAL;\n\tif (len > task_size - start)\n\t\treturn -EINVAL;\n\tif (start + len <= start)\n\t\treturn -EINVAL;\n\treturn 0;\n}\n\nstatic __always_inline int validate_range(struct mm_struct *mm,\n\t\t\t\t\t  __u64 start, __u64 len)\n{\n\tif (start & ~PAGE_MASK)\n\t\treturn -EINVAL;\n\n\treturn validate_unaligned_range(mm, start, len);\n}\n\nstatic int userfaultfd_register(struct userfaultfd_ctx *ctx,\n\t\t\t\tunsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_register uffdio_register;\n\tstruct uffdio_register __user *user_uffdio_register;\n\tunsigned long vm_flags, new_flags;\n\tbool found;\n\tbool basic_ioctls;\n\tunsigned long start, end, vma_end;\n\tstruct vma_iterator vmi;\n\tpgoff_t pgoff;\n\n\tuser_uffdio_register = (struct uffdio_register __user *) arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_register, user_uffdio_register,\n\t\t\t   sizeof(uffdio_register)-sizeof(__u64)))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (!uffdio_register.mode)\n\t\tgoto out;\n\tif (uffdio_register.mode & ~UFFD_API_REGISTER_MODES)\n\t\tgoto out;\n\tvm_flags = 0;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_MISSING)\n\t\tvm_flags |= VM_UFFD_MISSING;\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_WP) {\n#ifndef CONFIG_HAVE_ARCH_USERFAULTFD_WP\n\t\tgoto out;\n#endif\n\t\tvm_flags |= VM_UFFD_WP;\n\t}\n\tif (uffdio_register.mode & UFFDIO_REGISTER_MODE_MINOR) {\n#ifndef CONFIG_HAVE_ARCH_USERFAULTFD_MINOR\n\t\tgoto out;\n#endif\n\t\tvm_flags |= VM_UFFD_MINOR;\n\t}\n\n\tret = validate_range(mm, uffdio_register.range.start,\n\t\t\t     uffdio_register.range.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_register.range.start;\n\tend = start + uffdio_register.range.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tmmap_write_lock(mm);\n\tvma_iter_init(&vmi, mm, start);\n\tvma = vma_find(&vmi, end);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t \n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t \n\tfound = false;\n\tbasic_ioctls = false;\n\tcur = vma;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & __VM_UFFD_FLAGS));\n\n\t\t \n\t\tret = -EINVAL;\n\t\tif (!vma_can_userfault(cur, vm_flags))\n\t\t\tgoto out_unlock;\n\n\t\t \n\t\tret = -EPERM;\n\t\tif (unlikely(!(cur->vm_flags & VM_MAYWRITE)))\n\t\t\tgoto out_unlock;\n\n\t\t \n\t\tif (is_vm_hugetlb_page(cur) && end <= cur->vm_end &&\n\t\t    end > cur->vm_start) {\n\t\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(cur);\n\n\t\t\tret = -EINVAL;\n\n\t\t\tif (end & (vma_hpagesize - 1))\n\t\t\t\tgoto out_unlock;\n\t\t}\n\t\tif ((vm_flags & VM_UFFD_WP) && !(cur->vm_flags & VM_MAYWRITE))\n\t\t\tgoto out_unlock;\n\n\t\t \n\t\tret = -EBUSY;\n\t\tif (cur->vm_userfaultfd_ctx.ctx &&\n\t\t    cur->vm_userfaultfd_ctx.ctx != ctx)\n\t\t\tgoto out_unlock;\n\n\t\t \n\t\tif (is_vm_hugetlb_page(cur))\n\t\t\tbasic_ioctls = true;\n\n\t\tfound = true;\n\t} for_each_vma_range(vmi, cur, end);\n\tBUG_ON(!found);\n\n\tvma_iter_set(&vmi, start);\n\tprev = vma_prev(&vmi);\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tfor_each_vma_range(vmi, vma, end) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma, vm_flags));\n\t\tBUG_ON(vma->vm_userfaultfd_ctx.ctx &&\n\t\t       vma->vm_userfaultfd_ctx.ctx != ctx);\n\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n\n\t\t \n\t\tif (vma->vm_userfaultfd_ctx.ctx == ctx &&\n\t\t    (vma->vm_flags & vm_flags) == vm_flags)\n\t\t\tgoto skip;\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tnew_flags = (vma->vm_flags & ~__VM_UFFD_FLAGS) | vm_flags;\n\t\tpgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);\n\t\tprev = vma_merge(&vmi, mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t ((struct vm_userfaultfd_ctx){ ctx }),\n\t\t\t\t anon_vma_name(vma));\n\t\tif (prev) {\n\t\t\t \n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(&vmi, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(&vmi, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t \n\t\tvma_start_write(vma);\n\t\tuserfaultfd_set_vm_flags(vma, new_flags);\n\t\tvma->vm_userfaultfd_ctx.ctx = ctx;\n\n\t\tif (is_vm_hugetlb_page(vma) && uffd_disable_huge_pmd_share(vma))\n\t\t\thugetlb_unshare_all_pmds(vma);\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t}\n\nout_unlock:\n\tmmap_write_unlock(mm);\n\tmmput(mm);\n\tif (!ret) {\n\t\t__u64 ioctls_out;\n\n\t\tioctls_out = basic_ioctls ? UFFD_API_RANGE_IOCTLS_BASIC :\n\t\t    UFFD_API_RANGE_IOCTLS;\n\n\t\t \n\t\tif (!(uffdio_register.mode & UFFDIO_REGISTER_MODE_WP))\n\t\t\tioctls_out &= ~((__u64)1 << _UFFDIO_WRITEPROTECT);\n\n\t\t \n\t\tif (!(uffdio_register.mode & UFFDIO_REGISTER_MODE_MINOR))\n\t\t\tioctls_out &= ~((__u64)1 << _UFFDIO_CONTINUE);\n\n\t\t \n\t\tif (put_user(ioctls_out, &user_uffdio_register->ioctls))\n\t\t\tret = -EFAULT;\n\t}\nout:\n\treturn ret;\n}\n\nstatic int userfaultfd_unregister(struct userfaultfd_ctx *ctx,\n\t\t\t\t  unsigned long arg)\n{\n\tstruct mm_struct *mm = ctx->mm;\n\tstruct vm_area_struct *vma, *prev, *cur;\n\tint ret;\n\tstruct uffdio_range uffdio_unregister;\n\tunsigned long new_flags;\n\tbool found;\n\tunsigned long start, end, vma_end;\n\tconst void __user *buf = (void __user *)arg;\n\tstruct vma_iterator vmi;\n\tpgoff_t pgoff;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_unregister, buf, sizeof(uffdio_unregister)))\n\t\tgoto out;\n\n\tret = validate_range(mm, uffdio_unregister.start,\n\t\t\t     uffdio_unregister.len);\n\tif (ret)\n\t\tgoto out;\n\n\tstart = uffdio_unregister.start;\n\tend = start + uffdio_unregister.len;\n\n\tret = -ENOMEM;\n\tif (!mmget_not_zero(mm))\n\t\tgoto out;\n\n\tmmap_write_lock(mm);\n\tret = -EINVAL;\n\tvma_iter_init(&vmi, mm, start);\n\tvma = vma_find(&vmi, end);\n\tif (!vma)\n\t\tgoto out_unlock;\n\n\t \n\tif (is_vm_hugetlb_page(vma)) {\n\t\tunsigned long vma_hpagesize = vma_kernel_pagesize(vma);\n\n\t\tif (start & (vma_hpagesize - 1))\n\t\t\tgoto out_unlock;\n\t}\n\n\t \n\tfound = false;\n\tcur = vma;\n\tdo {\n\t\tcond_resched();\n\n\t\tBUG_ON(!!cur->vm_userfaultfd_ctx.ctx ^\n\t\t       !!(cur->vm_flags & __VM_UFFD_FLAGS));\n\n\t\t \n\t\tif (!vma_can_userfault(cur, cur->vm_flags))\n\t\t\tgoto out_unlock;\n\n\t\tfound = true;\n\t} for_each_vma_range(vmi, cur, end);\n\tBUG_ON(!found);\n\n\tvma_iter_set(&vmi, start);\n\tprev = vma_prev(&vmi);\n\tif (vma->vm_start < start)\n\t\tprev = vma;\n\n\tret = 0;\n\tfor_each_vma_range(vmi, vma, end) {\n\t\tcond_resched();\n\n\t\tBUG_ON(!vma_can_userfault(vma, vma->vm_flags));\n\n\t\t \n\t\tif (!vma->vm_userfaultfd_ctx.ctx)\n\t\t\tgoto skip;\n\n\t\tWARN_ON(!(vma->vm_flags & VM_MAYWRITE));\n\n\t\tif (vma->vm_start > start)\n\t\t\tstart = vma->vm_start;\n\t\tvma_end = min(end, vma->vm_end);\n\n\t\tif (userfaultfd_missing(vma)) {\n\t\t\t \n\t\t\tstruct userfaultfd_wake_range range;\n\t\t\trange.start = start;\n\t\t\trange.len = vma_end - start;\n\t\t\twake_userfault(vma->vm_userfaultfd_ctx.ctx, &range);\n\t\t}\n\n\t\t \n\t\tif (userfaultfd_wp(vma))\n\t\t\tuffd_wp_range(vma, start, vma_end - start, false);\n\n\t\tnew_flags = vma->vm_flags & ~__VM_UFFD_FLAGS;\n\t\tpgoff = vma->vm_pgoff + ((start - vma->vm_start) >> PAGE_SHIFT);\n\t\tprev = vma_merge(&vmi, mm, prev, start, vma_end, new_flags,\n\t\t\t\t vma->anon_vma, vma->vm_file, pgoff,\n\t\t\t\t vma_policy(vma),\n\t\t\t\t NULL_VM_UFFD_CTX, anon_vma_name(vma));\n\t\tif (prev) {\n\t\t\tvma = prev;\n\t\t\tgoto next;\n\t\t}\n\t\tif (vma->vm_start < start) {\n\t\t\tret = split_vma(&vmi, vma, start, 1);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (vma->vm_end > end) {\n\t\t\tret = split_vma(&vmi, vma, end, 0);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\tnext:\n\t\t \n\t\tvma_start_write(vma);\n\t\tuserfaultfd_set_vm_flags(vma, new_flags);\n\t\tvma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;\n\n\tskip:\n\t\tprev = vma;\n\t\tstart = vma->vm_end;\n\t}\n\nout_unlock:\n\tmmap_write_unlock(mm);\n\tmmput(mm);\nout:\n\treturn ret;\n}\n\n \nstatic int userfaultfd_wake(struct userfaultfd_ctx *ctx,\n\t\t\t    unsigned long arg)\n{\n\tint ret;\n\tstruct uffdio_range uffdio_wake;\n\tstruct userfaultfd_wake_range range;\n\tconst void __user *buf = (void __user *)arg;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_wake, buf, sizeof(uffdio_wake)))\n\t\tgoto out;\n\n\tret = validate_range(ctx->mm, uffdio_wake.start, uffdio_wake.len);\n\tif (ret)\n\t\tgoto out;\n\n\trange.start = uffdio_wake.start;\n\trange.len = uffdio_wake.len;\n\n\t \n\tVM_BUG_ON(!range.len);\n\n\twake_userfault(ctx, &range);\n\tret = 0;\n\nout:\n\treturn ret;\n}\n\nstatic int userfaultfd_copy(struct userfaultfd_ctx *ctx,\n\t\t\t    unsigned long arg)\n{\n\t__s64 ret;\n\tstruct uffdio_copy uffdio_copy;\n\tstruct uffdio_copy __user *user_uffdio_copy;\n\tstruct userfaultfd_wake_range range;\n\tuffd_flags_t flags = 0;\n\n\tuser_uffdio_copy = (struct uffdio_copy __user *) arg;\n\n\tret = -EAGAIN;\n\tif (atomic_read(&ctx->mmap_changing))\n\t\tgoto out;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_copy, user_uffdio_copy,\n\t\t\t    \n\t\t\t   sizeof(uffdio_copy)-sizeof(__s64)))\n\t\tgoto out;\n\n\tret = validate_unaligned_range(ctx->mm, uffdio_copy.src,\n\t\t\t\t       uffdio_copy.len);\n\tif (ret)\n\t\tgoto out;\n\tret = validate_range(ctx->mm, uffdio_copy.dst, uffdio_copy.len);\n\tif (ret)\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (uffdio_copy.mode & ~(UFFDIO_COPY_MODE_DONTWAKE|UFFDIO_COPY_MODE_WP))\n\t\tgoto out;\n\tif (uffdio_copy.mode & UFFDIO_COPY_MODE_WP)\n\t\tflags |= MFILL_ATOMIC_WP;\n\tif (mmget_not_zero(ctx->mm)) {\n\t\tret = mfill_atomic_copy(ctx->mm, uffdio_copy.dst, uffdio_copy.src,\n\t\t\t\t\tuffdio_copy.len, &ctx->mmap_changing,\n\t\t\t\t\tflags);\n\t\tmmput(ctx->mm);\n\t} else {\n\t\treturn -ESRCH;\n\t}\n\tif (unlikely(put_user(ret, &user_uffdio_copy->copy)))\n\t\treturn -EFAULT;\n\tif (ret < 0)\n\t\tgoto out;\n\tBUG_ON(!ret);\n\t \n\trange.len = ret;\n\tif (!(uffdio_copy.mode & UFFDIO_COPY_MODE_DONTWAKE)) {\n\t\trange.start = uffdio_copy.dst;\n\t\twake_userfault(ctx, &range);\n\t}\n\tret = range.len == uffdio_copy.len ? 0 : -EAGAIN;\nout:\n\treturn ret;\n}\n\nstatic int userfaultfd_zeropage(struct userfaultfd_ctx *ctx,\n\t\t\t\tunsigned long arg)\n{\n\t__s64 ret;\n\tstruct uffdio_zeropage uffdio_zeropage;\n\tstruct uffdio_zeropage __user *user_uffdio_zeropage;\n\tstruct userfaultfd_wake_range range;\n\n\tuser_uffdio_zeropage = (struct uffdio_zeropage __user *) arg;\n\n\tret = -EAGAIN;\n\tif (atomic_read(&ctx->mmap_changing))\n\t\tgoto out;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_zeropage, user_uffdio_zeropage,\n\t\t\t    \n\t\t\t   sizeof(uffdio_zeropage)-sizeof(__s64)))\n\t\tgoto out;\n\n\tret = validate_range(ctx->mm, uffdio_zeropage.range.start,\n\t\t\t     uffdio_zeropage.range.len);\n\tif (ret)\n\t\tgoto out;\n\tret = -EINVAL;\n\tif (uffdio_zeropage.mode & ~UFFDIO_ZEROPAGE_MODE_DONTWAKE)\n\t\tgoto out;\n\n\tif (mmget_not_zero(ctx->mm)) {\n\t\tret = mfill_atomic_zeropage(ctx->mm, uffdio_zeropage.range.start,\n\t\t\t\t\t   uffdio_zeropage.range.len,\n\t\t\t\t\t   &ctx->mmap_changing);\n\t\tmmput(ctx->mm);\n\t} else {\n\t\treturn -ESRCH;\n\t}\n\tif (unlikely(put_user(ret, &user_uffdio_zeropage->zeropage)))\n\t\treturn -EFAULT;\n\tif (ret < 0)\n\t\tgoto out;\n\t \n\tBUG_ON(!ret);\n\trange.len = ret;\n\tif (!(uffdio_zeropage.mode & UFFDIO_ZEROPAGE_MODE_DONTWAKE)) {\n\t\trange.start = uffdio_zeropage.range.start;\n\t\twake_userfault(ctx, &range);\n\t}\n\tret = range.len == uffdio_zeropage.range.len ? 0 : -EAGAIN;\nout:\n\treturn ret;\n}\n\nstatic int userfaultfd_writeprotect(struct userfaultfd_ctx *ctx,\n\t\t\t\t    unsigned long arg)\n{\n\tint ret;\n\tstruct uffdio_writeprotect uffdio_wp;\n\tstruct uffdio_writeprotect __user *user_uffdio_wp;\n\tstruct userfaultfd_wake_range range;\n\tbool mode_wp, mode_dontwake;\n\n\tif (atomic_read(&ctx->mmap_changing))\n\t\treturn -EAGAIN;\n\n\tuser_uffdio_wp = (struct uffdio_writeprotect __user *) arg;\n\n\tif (copy_from_user(&uffdio_wp, user_uffdio_wp,\n\t\t\t   sizeof(struct uffdio_writeprotect)))\n\t\treturn -EFAULT;\n\n\tret = validate_range(ctx->mm, uffdio_wp.range.start,\n\t\t\t     uffdio_wp.range.len);\n\tif (ret)\n\t\treturn ret;\n\n\tif (uffdio_wp.mode & ~(UFFDIO_WRITEPROTECT_MODE_DONTWAKE |\n\t\t\t       UFFDIO_WRITEPROTECT_MODE_WP))\n\t\treturn -EINVAL;\n\n\tmode_wp = uffdio_wp.mode & UFFDIO_WRITEPROTECT_MODE_WP;\n\tmode_dontwake = uffdio_wp.mode & UFFDIO_WRITEPROTECT_MODE_DONTWAKE;\n\n\tif (mode_wp && mode_dontwake)\n\t\treturn -EINVAL;\n\n\tif (mmget_not_zero(ctx->mm)) {\n\t\tret = mwriteprotect_range(ctx->mm, uffdio_wp.range.start,\n\t\t\t\t\t  uffdio_wp.range.len, mode_wp,\n\t\t\t\t\t  &ctx->mmap_changing);\n\t\tmmput(ctx->mm);\n\t} else {\n\t\treturn -ESRCH;\n\t}\n\n\tif (ret)\n\t\treturn ret;\n\n\tif (!mode_wp && !mode_dontwake) {\n\t\trange.start = uffdio_wp.range.start;\n\t\trange.len = uffdio_wp.range.len;\n\t\twake_userfault(ctx, &range);\n\t}\n\treturn ret;\n}\n\nstatic int userfaultfd_continue(struct userfaultfd_ctx *ctx, unsigned long arg)\n{\n\t__s64 ret;\n\tstruct uffdio_continue uffdio_continue;\n\tstruct uffdio_continue __user *user_uffdio_continue;\n\tstruct userfaultfd_wake_range range;\n\tuffd_flags_t flags = 0;\n\n\tuser_uffdio_continue = (struct uffdio_continue __user *)arg;\n\n\tret = -EAGAIN;\n\tif (atomic_read(&ctx->mmap_changing))\n\t\tgoto out;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_continue, user_uffdio_continue,\n\t\t\t    \n\t\t\t   sizeof(uffdio_continue) - (sizeof(__s64))))\n\t\tgoto out;\n\n\tret = validate_range(ctx->mm, uffdio_continue.range.start,\n\t\t\t     uffdio_continue.range.len);\n\tif (ret)\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (uffdio_continue.mode & ~(UFFDIO_CONTINUE_MODE_DONTWAKE |\n\t\t\t\t     UFFDIO_CONTINUE_MODE_WP))\n\t\tgoto out;\n\tif (uffdio_continue.mode & UFFDIO_CONTINUE_MODE_WP)\n\t\tflags |= MFILL_ATOMIC_WP;\n\n\tif (mmget_not_zero(ctx->mm)) {\n\t\tret = mfill_atomic_continue(ctx->mm, uffdio_continue.range.start,\n\t\t\t\t\t    uffdio_continue.range.len,\n\t\t\t\t\t    &ctx->mmap_changing, flags);\n\t\tmmput(ctx->mm);\n\t} else {\n\t\treturn -ESRCH;\n\t}\n\n\tif (unlikely(put_user(ret, &user_uffdio_continue->mapped)))\n\t\treturn -EFAULT;\n\tif (ret < 0)\n\t\tgoto out;\n\n\t \n\tBUG_ON(!ret);\n\trange.len = ret;\n\tif (!(uffdio_continue.mode & UFFDIO_CONTINUE_MODE_DONTWAKE)) {\n\t\trange.start = uffdio_continue.range.start;\n\t\twake_userfault(ctx, &range);\n\t}\n\tret = range.len == uffdio_continue.range.len ? 0 : -EAGAIN;\n\nout:\n\treturn ret;\n}\n\nstatic inline int userfaultfd_poison(struct userfaultfd_ctx *ctx, unsigned long arg)\n{\n\t__s64 ret;\n\tstruct uffdio_poison uffdio_poison;\n\tstruct uffdio_poison __user *user_uffdio_poison;\n\tstruct userfaultfd_wake_range range;\n\n\tuser_uffdio_poison = (struct uffdio_poison __user *)arg;\n\n\tret = -EAGAIN;\n\tif (atomic_read(&ctx->mmap_changing))\n\t\tgoto out;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_poison, user_uffdio_poison,\n\t\t\t    \n\t\t\t   sizeof(uffdio_poison) - (sizeof(__s64))))\n\t\tgoto out;\n\n\tret = validate_range(ctx->mm, uffdio_poison.range.start,\n\t\t\t     uffdio_poison.range.len);\n\tif (ret)\n\t\tgoto out;\n\n\tret = -EINVAL;\n\tif (uffdio_poison.mode & ~UFFDIO_POISON_MODE_DONTWAKE)\n\t\tgoto out;\n\n\tif (mmget_not_zero(ctx->mm)) {\n\t\tret = mfill_atomic_poison(ctx->mm, uffdio_poison.range.start,\n\t\t\t\t\t  uffdio_poison.range.len,\n\t\t\t\t\t  &ctx->mmap_changing, 0);\n\t\tmmput(ctx->mm);\n\t} else {\n\t\treturn -ESRCH;\n\t}\n\n\tif (unlikely(put_user(ret, &user_uffdio_poison->updated)))\n\t\treturn -EFAULT;\n\tif (ret < 0)\n\t\tgoto out;\n\n\t \n\tBUG_ON(!ret);\n\trange.len = ret;\n\tif (!(uffdio_poison.mode & UFFDIO_POISON_MODE_DONTWAKE)) {\n\t\trange.start = uffdio_poison.range.start;\n\t\twake_userfault(ctx, &range);\n\t}\n\tret = range.len == uffdio_poison.range.len ? 0 : -EAGAIN;\n\nout:\n\treturn ret;\n}\n\nstatic inline unsigned int uffd_ctx_features(__u64 user_features)\n{\n\t \n\treturn (unsigned int)user_features | UFFD_FEATURE_INITIALIZED;\n}\n\n \nstatic int userfaultfd_api(struct userfaultfd_ctx *ctx,\n\t\t\t   unsigned long arg)\n{\n\tstruct uffdio_api uffdio_api;\n\tvoid __user *buf = (void __user *)arg;\n\tunsigned int ctx_features;\n\tint ret;\n\t__u64 features;\n\n\tret = -EFAULT;\n\tif (copy_from_user(&uffdio_api, buf, sizeof(uffdio_api)))\n\t\tgoto out;\n\tfeatures = uffdio_api.features;\n\tret = -EINVAL;\n\tif (uffdio_api.api != UFFD_API || (features & ~UFFD_API_FEATURES))\n\t\tgoto err_out;\n\tret = -EPERM;\n\tif ((features & UFFD_FEATURE_EVENT_FORK) && !capable(CAP_SYS_PTRACE))\n\t\tgoto err_out;\n\t \n\tuffdio_api.features = UFFD_API_FEATURES;\n#ifndef CONFIG_HAVE_ARCH_USERFAULTFD_MINOR\n\tuffdio_api.features &=\n\t\t~(UFFD_FEATURE_MINOR_HUGETLBFS | UFFD_FEATURE_MINOR_SHMEM);\n#endif\n#ifndef CONFIG_HAVE_ARCH_USERFAULTFD_WP\n\tuffdio_api.features &= ~UFFD_FEATURE_PAGEFAULT_FLAG_WP;\n#endif\n#ifndef CONFIG_PTE_MARKER_UFFD_WP\n\tuffdio_api.features &= ~UFFD_FEATURE_WP_HUGETLBFS_SHMEM;\n\tuffdio_api.features &= ~UFFD_FEATURE_WP_UNPOPULATED;\n#endif\n\tuffdio_api.ioctls = UFFD_API_IOCTLS;\n\tret = -EFAULT;\n\tif (copy_to_user(buf, &uffdio_api, sizeof(uffdio_api)))\n\t\tgoto out;\n\n\t \n\tctx_features = uffd_ctx_features(features);\n\tret = -EINVAL;\n\tif (cmpxchg(&ctx->features, 0, ctx_features) != 0)\n\t\tgoto err_out;\n\n\tret = 0;\nout:\n\treturn ret;\nerr_out:\n\tmemset(&uffdio_api, 0, sizeof(uffdio_api));\n\tif (copy_to_user(buf, &uffdio_api, sizeof(uffdio_api)))\n\t\tret = -EFAULT;\n\tgoto out;\n}\n\nstatic long userfaultfd_ioctl(struct file *file, unsigned cmd,\n\t\t\t      unsigned long arg)\n{\n\tint ret = -EINVAL;\n\tstruct userfaultfd_ctx *ctx = file->private_data;\n\n\tif (cmd != UFFDIO_API && !userfaultfd_is_initialized(ctx))\n\t\treturn -EINVAL;\n\n\tswitch(cmd) {\n\tcase UFFDIO_API:\n\t\tret = userfaultfd_api(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_REGISTER:\n\t\tret = userfaultfd_register(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_UNREGISTER:\n\t\tret = userfaultfd_unregister(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_WAKE:\n\t\tret = userfaultfd_wake(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_COPY:\n\t\tret = userfaultfd_copy(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_ZEROPAGE:\n\t\tret = userfaultfd_zeropage(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_WRITEPROTECT:\n\t\tret = userfaultfd_writeprotect(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_CONTINUE:\n\t\tret = userfaultfd_continue(ctx, arg);\n\t\tbreak;\n\tcase UFFDIO_POISON:\n\t\tret = userfaultfd_poison(ctx, arg);\n\t\tbreak;\n\t}\n\treturn ret;\n}\n\n#ifdef CONFIG_PROC_FS\nstatic void userfaultfd_show_fdinfo(struct seq_file *m, struct file *f)\n{\n\tstruct userfaultfd_ctx *ctx = f->private_data;\n\twait_queue_entry_t *wq;\n\tunsigned long pending = 0, total = 0;\n\n\tspin_lock_irq(&ctx->fault_pending_wqh.lock);\n\tlist_for_each_entry(wq, &ctx->fault_pending_wqh.head, entry) {\n\t\tpending++;\n\t\ttotal++;\n\t}\n\tlist_for_each_entry(wq, &ctx->fault_wqh.head, entry) {\n\t\ttotal++;\n\t}\n\tspin_unlock_irq(&ctx->fault_pending_wqh.lock);\n\n\t \n\tseq_printf(m, \"pending:\\t%lu\\ntotal:\\t%lu\\nAPI:\\t%Lx:%x:%Lx\\n\",\n\t\t   pending, total, UFFD_API, ctx->features,\n\t\t   UFFD_API_IOCTLS|UFFD_API_RANGE_IOCTLS);\n}\n#endif\n\nstatic const struct file_operations userfaultfd_fops = {\n#ifdef CONFIG_PROC_FS\n\t.show_fdinfo\t= userfaultfd_show_fdinfo,\n#endif\n\t.release\t= userfaultfd_release,\n\t.poll\t\t= userfaultfd_poll,\n\t.read\t\t= userfaultfd_read,\n\t.unlocked_ioctl = userfaultfd_ioctl,\n\t.compat_ioctl\t= compat_ptr_ioctl,\n\t.llseek\t\t= noop_llseek,\n};\n\nstatic void init_once_userfaultfd_ctx(void *mem)\n{\n\tstruct userfaultfd_ctx *ctx = (struct userfaultfd_ctx *) mem;\n\n\tinit_waitqueue_head(&ctx->fault_pending_wqh);\n\tinit_waitqueue_head(&ctx->fault_wqh);\n\tinit_waitqueue_head(&ctx->event_wqh);\n\tinit_waitqueue_head(&ctx->fd_wqh);\n\tseqcount_spinlock_init(&ctx->refile_seq, &ctx->fault_pending_wqh.lock);\n}\n\nstatic int new_userfaultfd(int flags)\n{\n\tstruct userfaultfd_ctx *ctx;\n\tint fd;\n\n\tBUG_ON(!current->mm);\n\n\t \n\tBUILD_BUG_ON(UFFD_USER_MODE_ONLY & UFFD_SHARED_FCNTL_FLAGS);\n\tBUILD_BUG_ON(UFFD_CLOEXEC != O_CLOEXEC);\n\tBUILD_BUG_ON(UFFD_NONBLOCK != O_NONBLOCK);\n\n\tif (flags & ~(UFFD_SHARED_FCNTL_FLAGS | UFFD_USER_MODE_ONLY))\n\t\treturn -EINVAL;\n\n\tctx = kmem_cache_alloc(userfaultfd_ctx_cachep, GFP_KERNEL);\n\tif (!ctx)\n\t\treturn -ENOMEM;\n\n\trefcount_set(&ctx->refcount, 1);\n\tctx->flags = flags;\n\tctx->features = 0;\n\tctx->released = false;\n\tatomic_set(&ctx->mmap_changing, 0);\n\tctx->mm = current->mm;\n\t \n\tmmgrab(ctx->mm);\n\n\tfd = anon_inode_getfd_secure(\"[userfaultfd]\", &userfaultfd_fops, ctx,\n\t\t\tO_RDONLY | (flags & UFFD_SHARED_FCNTL_FLAGS), NULL);\n\tif (fd < 0) {\n\t\tmmdrop(ctx->mm);\n\t\tkmem_cache_free(userfaultfd_ctx_cachep, ctx);\n\t}\n\treturn fd;\n}\n\nstatic inline bool userfaultfd_syscall_allowed(int flags)\n{\n\t \n\tif (flags & UFFD_USER_MODE_ONLY)\n\t\treturn true;\n\n\t \n\tif (capable(CAP_SYS_PTRACE))\n\t\treturn true;\n\n\t \n\treturn sysctl_unprivileged_userfaultfd;\n}\n\nSYSCALL_DEFINE1(userfaultfd, int, flags)\n{\n\tif (!userfaultfd_syscall_allowed(flags))\n\t\treturn -EPERM;\n\n\treturn new_userfaultfd(flags);\n}\n\nstatic long userfaultfd_dev_ioctl(struct file *file, unsigned int cmd, unsigned long flags)\n{\n\tif (cmd != USERFAULTFD_IOC_NEW)\n\t\treturn -EINVAL;\n\n\treturn new_userfaultfd(flags);\n}\n\nstatic const struct file_operations userfaultfd_dev_fops = {\n\t.unlocked_ioctl = userfaultfd_dev_ioctl,\n\t.compat_ioctl = userfaultfd_dev_ioctl,\n\t.owner = THIS_MODULE,\n\t.llseek = noop_llseek,\n};\n\nstatic struct miscdevice userfaultfd_misc = {\n\t.minor = MISC_DYNAMIC_MINOR,\n\t.name = \"userfaultfd\",\n\t.fops = &userfaultfd_dev_fops\n};\n\nstatic int __init userfaultfd_init(void)\n{\n\tint ret;\n\n\tret = misc_register(&userfaultfd_misc);\n\tif (ret)\n\t\treturn ret;\n\n\tuserfaultfd_ctx_cachep = kmem_cache_create(\"userfaultfd_ctx_cache\",\n\t\t\t\t\t\tsizeof(struct userfaultfd_ctx),\n\t\t\t\t\t\t0,\n\t\t\t\t\t\tSLAB_HWCACHE_ALIGN|SLAB_PANIC,\n\t\t\t\t\t\tinit_once_userfaultfd_ctx);\n#ifdef CONFIG_SYSCTL\n\tregister_sysctl_init(\"vm\", vm_userfaultfd_table);\n#endif\n\treturn 0;\n}\n__initcall(userfaultfd_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}