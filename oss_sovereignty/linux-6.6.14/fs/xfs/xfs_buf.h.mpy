{
  "module_name": "xfs_buf.h",
  "hash_id": "d0187211ea331e70c0278737c044e0339f2101157d315fa25eace16e11772f06",
  "original_prompt": "Ingested from linux-6.6.14/fs/xfs/xfs_buf.h",
  "human_readable_source": "\n \n#ifndef __XFS_BUF_H__\n#define __XFS_BUF_H__\n\n#include <linux/list.h>\n#include <linux/types.h>\n#include <linux/spinlock.h>\n#include <linux/mm.h>\n#include <linux/fs.h>\n#include <linux/dax.h>\n#include <linux/uio.h>\n#include <linux/list_lru.h>\n\nextern struct kmem_cache *xfs_buf_cache;\n\n \nstruct xfs_buf;\n\n#define XFS_BUF_DADDR_NULL\t((xfs_daddr_t) (-1LL))\n\n#define XBF_READ\t (1u << 0)  \n#define XBF_WRITE\t (1u << 1)  \n#define XBF_READ_AHEAD\t (1u << 2)  \n#define XBF_NO_IOACCT\t (1u << 3)  \n#define XBF_ASYNC\t (1u << 4)  \n#define XBF_DONE\t (1u << 5)  \n#define XBF_STALE\t (1u << 6)  \n#define XBF_WRITE_FAIL\t (1u << 7)  \n\n \n#define _XBF_INODES\t (1u << 16) \n#define _XBF_DQUOTS\t (1u << 17) \n#define _XBF_LOGRECOVERY (1u << 18) \n\n \n#define _XBF_PAGES\t (1u << 20) \n#define _XBF_KMEM\t (1u << 21) \n#define _XBF_DELWRI_Q\t (1u << 22) \n\n \n \n#define XBF_LIVESCAN\t (1u << 28)\n#define XBF_INCORE\t (1u << 29) \n#define XBF_TRYLOCK\t (1u << 30) \n#define XBF_UNMAPPED\t (1u << 31) \n\n\ntypedef unsigned int xfs_buf_flags_t;\n\n#define XFS_BUF_FLAGS \\\n\t{ XBF_READ,\t\t\"READ\" }, \\\n\t{ XBF_WRITE,\t\t\"WRITE\" }, \\\n\t{ XBF_READ_AHEAD,\t\"READ_AHEAD\" }, \\\n\t{ XBF_NO_IOACCT,\t\"NO_IOACCT\" }, \\\n\t{ XBF_ASYNC,\t\t\"ASYNC\" }, \\\n\t{ XBF_DONE,\t\t\"DONE\" }, \\\n\t{ XBF_STALE,\t\t\"STALE\" }, \\\n\t{ XBF_WRITE_FAIL,\t\"WRITE_FAIL\" }, \\\n\t{ _XBF_INODES,\t\t\"INODES\" }, \\\n\t{ _XBF_DQUOTS,\t\t\"DQUOTS\" }, \\\n\t{ _XBF_LOGRECOVERY,\t\"LOG_RECOVERY\" }, \\\n\t{ _XBF_PAGES,\t\t\"PAGES\" }, \\\n\t{ _XBF_KMEM,\t\t\"KMEM\" }, \\\n\t{ _XBF_DELWRI_Q,\t\"DELWRI_Q\" }, \\\n\t  \\\n\t{ XBF_LIVESCAN,\t\t\"LIVESCAN\" }, \\\n\t{ XBF_INCORE,\t\t\"INCORE\" }, \\\n\t{ XBF_TRYLOCK,\t\t\"TRYLOCK\" }, \\\n\t{ XBF_UNMAPPED,\t\t\"UNMAPPED\" }\n\n \n#define XFS_BSTATE_DISPOSE\t (1 << 0)\t \n#define XFS_BSTATE_IN_FLIGHT\t (1 << 1)\t \n\n \ntypedef struct xfs_buftarg {\n\tdev_t\t\t\tbt_dev;\n\tstruct block_device\t*bt_bdev;\n\tstruct dax_device\t*bt_daxdev;\n\tu64\t\t\tbt_dax_part_off;\n\tstruct xfs_mount\t*bt_mount;\n\tunsigned int\t\tbt_meta_sectorsize;\n\tsize_t\t\t\tbt_meta_sectormask;\n\tsize_t\t\t\tbt_logical_sectorsize;\n\tsize_t\t\t\tbt_logical_sectormask;\n\n\t \n\tstruct shrinker\t\tbt_shrinker;\n\tstruct list_lru\t\tbt_lru;\n\n\tstruct percpu_counter\tbt_io_count;\n\tstruct ratelimit_state\tbt_ioerror_rl;\n} xfs_buftarg_t;\n\n#define XB_PAGES\t2\n\nstruct xfs_buf_map {\n\txfs_daddr_t\t\tbm_bn;\t \n\tint\t\t\tbm_len;\t \n\tunsigned int\t\tbm_flags;\n};\n\n \n#define XBM_LIVESCAN\t\t(1U << 0)\n\n#define DEFINE_SINGLE_BUF_MAP(map, blkno, numblk) \\\n\tstruct xfs_buf_map (map) = { .bm_bn = (blkno), .bm_len = (numblk) };\n\nstruct xfs_buf_ops {\n\tchar *name;\n\tunion {\n\t\t__be32 magic[2];\t \n\t\t__be16 magic16[2];\t \n\t};\n\tvoid (*verify_read)(struct xfs_buf *);\n\tvoid (*verify_write)(struct xfs_buf *);\n\txfs_failaddr_t (*verify_struct)(struct xfs_buf *bp);\n};\n\nstruct xfs_buf {\n\t \n\tstruct rhash_head\tb_rhash_head;\t \n\n\txfs_daddr_t\t\tb_rhash_key;\t \n\tint\t\t\tb_length;\t \n\tatomic_t\t\tb_hold;\t\t \n\tatomic_t\t\tb_lru_ref;\t \n\txfs_buf_flags_t\t\tb_flags;\t \n\tstruct semaphore\tb_sema;\t\t \n\n\t \n\tstruct list_head\tb_lru;\t\t \n\tspinlock_t\t\tb_lock;\t\t \n\tunsigned int\t\tb_state;\t \n\tint\t\t\tb_io_error;\t \n\twait_queue_head_t\tb_waiters;\t \n\tstruct list_head\tb_list;\n\tstruct xfs_perag\t*b_pag;\t\t \n\tstruct xfs_mount\t*b_mount;\n\tstruct xfs_buftarg\t*b_target;\t \n\tvoid\t\t\t*b_addr;\t \n\tstruct work_struct\tb_ioend_work;\n\tstruct completion\tb_iowait;\t \n\tstruct xfs_buf_log_item\t*b_log_item;\n\tstruct list_head\tb_li_list;\t \n\tstruct xfs_trans\t*b_transp;\n\tstruct page\t\t**b_pages;\t \n\tstruct page\t\t*b_page_array[XB_PAGES];  \n\tstruct xfs_buf_map\t*b_maps;\t \n\tstruct xfs_buf_map\t__b_map;\t \n\tint\t\t\tb_map_count;\n\tatomic_t\t\tb_pin_count;\t \n\tatomic_t\t\tb_io_remaining;\t \n\tunsigned int\t\tb_page_count;\t \n\tunsigned int\t\tb_offset;\t \n\tint\t\t\tb_error;\t \n\n\t \n\tint\t\t\tb_retries;\n\tunsigned long\t\tb_first_retry_time;  \n\tint\t\t\tb_last_error;\n\n\tconst struct xfs_buf_ops\t*b_ops;\n\tstruct rcu_head\t\tb_rcu;\n};\n\n \nint xfs_buf_get_map(struct xfs_buftarg *target, struct xfs_buf_map *map,\n\t\tint nmaps, xfs_buf_flags_t flags, struct xfs_buf **bpp);\nint xfs_buf_read_map(struct xfs_buftarg *target, struct xfs_buf_map *map,\n\t\tint nmaps, xfs_buf_flags_t flags, struct xfs_buf **bpp,\n\t\tconst struct xfs_buf_ops *ops, xfs_failaddr_t fa);\nvoid xfs_buf_readahead_map(struct xfs_buftarg *target,\n\t\t\t       struct xfs_buf_map *map, int nmaps,\n\t\t\t       const struct xfs_buf_ops *ops);\n\nstatic inline int\nxfs_buf_incore(\n\tstruct xfs_buftarg\t*target,\n\txfs_daddr_t\t\tblkno,\n\tsize_t\t\t\tnumblks,\n\txfs_buf_flags_t\t\tflags,\n\tstruct xfs_buf\t\t**bpp)\n{\n\tDEFINE_SINGLE_BUF_MAP(map, blkno, numblks);\n\n\treturn xfs_buf_get_map(target, &map, 1, XBF_INCORE | flags, bpp);\n}\n\nstatic inline int\nxfs_buf_get(\n\tstruct xfs_buftarg\t*target,\n\txfs_daddr_t\t\tblkno,\n\tsize_t\t\t\tnumblks,\n\tstruct xfs_buf\t\t**bpp)\n{\n\tDEFINE_SINGLE_BUF_MAP(map, blkno, numblks);\n\n\treturn xfs_buf_get_map(target, &map, 1, 0, bpp);\n}\n\nstatic inline int\nxfs_buf_read(\n\tstruct xfs_buftarg\t*target,\n\txfs_daddr_t\t\tblkno,\n\tsize_t\t\t\tnumblks,\n\txfs_buf_flags_t\t\tflags,\n\tstruct xfs_buf\t\t**bpp,\n\tconst struct xfs_buf_ops *ops)\n{\n\tDEFINE_SINGLE_BUF_MAP(map, blkno, numblks);\n\n\treturn xfs_buf_read_map(target, &map, 1, flags, bpp, ops,\n\t\t\t__builtin_return_address(0));\n}\n\nstatic inline void\nxfs_buf_readahead(\n\tstruct xfs_buftarg\t*target,\n\txfs_daddr_t\t\tblkno,\n\tsize_t\t\t\tnumblks,\n\tconst struct xfs_buf_ops *ops)\n{\n\tDEFINE_SINGLE_BUF_MAP(map, blkno, numblks);\n\treturn xfs_buf_readahead_map(target, &map, 1, ops);\n}\n\nint xfs_buf_get_uncached(struct xfs_buftarg *target, size_t numblks,\n\t\txfs_buf_flags_t flags, struct xfs_buf **bpp);\nint xfs_buf_read_uncached(struct xfs_buftarg *target, xfs_daddr_t daddr,\n\t\tsize_t numblks, xfs_buf_flags_t flags, struct xfs_buf **bpp,\n\t\tconst struct xfs_buf_ops *ops);\nint _xfs_buf_read(struct xfs_buf *bp, xfs_buf_flags_t flags);\nvoid xfs_buf_hold(struct xfs_buf *bp);\n\n \nextern void xfs_buf_rele(struct xfs_buf *);\n\n \nextern int xfs_buf_trylock(struct xfs_buf *);\nextern void xfs_buf_lock(struct xfs_buf *);\nextern void xfs_buf_unlock(struct xfs_buf *);\n#define xfs_buf_islocked(bp) \\\n\t((bp)->b_sema.count <= 0)\n\nstatic inline void xfs_buf_relse(struct xfs_buf *bp)\n{\n\txfs_buf_unlock(bp);\n\txfs_buf_rele(bp);\n}\n\n \nextern int xfs_bwrite(struct xfs_buf *bp);\n\nextern void __xfs_buf_ioerror(struct xfs_buf *bp, int error,\n\t\txfs_failaddr_t failaddr);\n#define xfs_buf_ioerror(bp, err) __xfs_buf_ioerror((bp), (err), __this_address)\nextern void xfs_buf_ioerror_alert(struct xfs_buf *bp, xfs_failaddr_t fa);\nvoid xfs_buf_ioend_fail(struct xfs_buf *);\nvoid xfs_buf_zero(struct xfs_buf *bp, size_t boff, size_t bsize);\nvoid __xfs_buf_mark_corrupt(struct xfs_buf *bp, xfs_failaddr_t fa);\n#define xfs_buf_mark_corrupt(bp) __xfs_buf_mark_corrupt((bp), __this_address)\n\n \nextern void *xfs_buf_offset(struct xfs_buf *, size_t);\nextern void xfs_buf_stale(struct xfs_buf *bp);\n\n \nextern void xfs_buf_delwri_cancel(struct list_head *);\nextern bool xfs_buf_delwri_queue(struct xfs_buf *, struct list_head *);\nextern int xfs_buf_delwri_submit(struct list_head *);\nextern int xfs_buf_delwri_submit_nowait(struct list_head *);\nextern int xfs_buf_delwri_pushbuf(struct xfs_buf *, struct list_head *);\n\nstatic inline xfs_daddr_t xfs_buf_daddr(struct xfs_buf *bp)\n{\n\treturn bp->b_maps[0].bm_bn;\n}\n\nvoid xfs_buf_set_ref(struct xfs_buf *bp, int lru_ref);\n\n \nstatic inline void xfs_buf_oneshot(struct xfs_buf *bp)\n{\n\tif (!list_empty(&bp->b_lru) || atomic_read(&bp->b_lru_ref) > 1)\n\t\treturn;\n\tatomic_set(&bp->b_lru_ref, 0);\n}\n\nstatic inline int xfs_buf_ispinned(struct xfs_buf *bp)\n{\n\treturn atomic_read(&bp->b_pin_count);\n}\n\nstatic inline int\nxfs_buf_verify_cksum(struct xfs_buf *bp, unsigned long cksum_offset)\n{\n\treturn xfs_verify_cksum(bp->b_addr, BBTOB(bp->b_length),\n\t\t\t\tcksum_offset);\n}\n\nstatic inline void\nxfs_buf_update_cksum(struct xfs_buf *bp, unsigned long cksum_offset)\n{\n\txfs_update_cksum(bp->b_addr, BBTOB(bp->b_length),\n\t\t\t cksum_offset);\n}\n\n \nstruct xfs_buftarg *xfs_alloc_buftarg(struct xfs_mount *mp,\n\t\tstruct block_device *bdev);\nextern void xfs_free_buftarg(struct xfs_buftarg *);\nextern void xfs_buftarg_wait(struct xfs_buftarg *);\nextern void xfs_buftarg_drain(struct xfs_buftarg *);\nextern int xfs_setsize_buftarg(struct xfs_buftarg *, unsigned int);\n\n#define xfs_getsize_buftarg(buftarg)\tblock_size((buftarg)->bt_bdev)\n#define xfs_readonly_buftarg(buftarg)\tbdev_read_only((buftarg)->bt_bdev)\n\nint xfs_buf_reverify(struct xfs_buf *bp, const struct xfs_buf_ops *ops);\nbool xfs_verify_magic(struct xfs_buf *bp, __be32 dmagic);\nbool xfs_verify_magic16(struct xfs_buf *bp, __be16 dmagic);\n\n#endif\t \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}