{
  "module_name": "ialloc.c",
  "hash_id": "a460c28efacabda4339404fc8e64c55cf551222fbdb5a75473298bb77385d414",
  "original_prompt": "Ingested from linux-6.6.14/fs/xfs/scrub/ialloc.c",
  "human_readable_source": "\n \n#include \"xfs.h\"\n#include \"xfs_fs.h\"\n#include \"xfs_shared.h\"\n#include \"xfs_format.h\"\n#include \"xfs_trans_resv.h\"\n#include \"xfs_mount.h\"\n#include \"xfs_btree.h\"\n#include \"xfs_log_format.h\"\n#include \"xfs_trans.h\"\n#include \"xfs_inode.h\"\n#include \"xfs_ialloc.h\"\n#include \"xfs_ialloc_btree.h\"\n#include \"xfs_icache.h\"\n#include \"xfs_rmap.h\"\n#include \"scrub/scrub.h\"\n#include \"scrub/common.h\"\n#include \"scrub/btree.h\"\n#include \"scrub/trace.h\"\n#include \"xfs_ag.h\"\n\n \nint\nxchk_setup_ag_iallocbt(\n\tstruct xfs_scrub\t*sc)\n{\n\tif (xchk_need_intent_drain(sc))\n\t\txchk_fsgates_enable(sc, XCHK_FSGATES_DRAIN);\n\treturn xchk_setup_ag_btree(sc, sc->flags & XCHK_TRY_HARDER);\n}\n\n \n\nstruct xchk_iallocbt {\n\t \n\tunsigned long long\tinodes;\n\n\t \n\txfs_agino_t\t\tnext_startino;\n\n\t \n\txfs_agino_t\t\tnext_cluster_ino;\n};\n\n \nSTATIC int\nxchk_inobt_xref_finobt(\n\tstruct xfs_scrub\t*sc,\n\tstruct xfs_inobt_rec_incore *irec,\n\txfs_agino_t\t\tagino,\n\tbool\t\t\tfree,\n\tbool\t\t\thole)\n{\n\tstruct xfs_inobt_rec_incore frec;\n\tstruct xfs_btree_cur\t*cur = sc->sa.fino_cur;\n\tbool\t\t\tffree, fhole;\n\tunsigned int\t\tfrec_idx, fhole_idx;\n\tint\t\t\thas_record;\n\tint\t\t\terror;\n\n\tASSERT(cur->bc_btnum == XFS_BTNUM_FINO);\n\n\terror = xfs_inobt_lookup(cur, agino, XFS_LOOKUP_LE, &has_record);\n\tif (error)\n\t\treturn error;\n\tif (!has_record)\n\t\tgoto no_record;\n\n\terror = xfs_inobt_get_rec(cur, &frec, &has_record);\n\tif (!has_record)\n\t\treturn -EFSCORRUPTED;\n\n\tif (frec.ir_startino + XFS_INODES_PER_CHUNK <= agino)\n\t\tgoto no_record;\n\n\t \n\tfrec_idx = agino - frec.ir_startino;\n\tffree = frec.ir_free & (1ULL << frec_idx);\n\tfhole_idx = frec_idx / XFS_INODES_PER_HOLEMASK_BIT;\n\tfhole = frec.ir_holemask & (1U << fhole_idx);\n\n\tif (ffree != free)\n\t\txchk_btree_xref_set_corrupt(sc, cur, 0);\n\tif (fhole != hole)\n\t\txchk_btree_xref_set_corrupt(sc, cur, 0);\n\treturn 0;\n\nno_record:\n\t \n\tif (irec->ir_free == 0)\n\t\treturn 0;\n\n\t \n\tif (irec->ir_free == XFS_INOBT_ALL_FREE)\n\t\treturn 0;\n\n\t \n\tif (hole)\n\t\treturn 0;\n\n\t \n\tif (!free)\n\t\treturn 0;\n\n\txchk_btree_xref_set_corrupt(sc, cur, 0);\n\treturn 0;\n}\n\n \nSTATIC void\nxchk_inobt_chunk_xref_finobt(\n\tstruct xfs_scrub\t\t*sc,\n\tstruct xfs_inobt_rec_incore\t*irec,\n\txfs_agino_t\t\t\tagino,\n\tunsigned int\t\t\tnr_inodes)\n{\n\txfs_agino_t\t\t\ti;\n\tunsigned int\t\t\trec_idx;\n\tint\t\t\t\terror;\n\n\tASSERT(sc->sm->sm_type == XFS_SCRUB_TYPE_INOBT);\n\n\tif (!sc->sa.fino_cur || xchk_skip_xref(sc->sm))\n\t\treturn;\n\n\tfor (i = agino, rec_idx = agino - irec->ir_startino;\n\t     i < agino + nr_inodes;\n\t     i++, rec_idx++) {\n\t\tbool\t\t\tfree, hole;\n\t\tunsigned int\t\thole_idx;\n\n\t\tfree = irec->ir_free & (1ULL << rec_idx);\n\t\thole_idx = rec_idx / XFS_INODES_PER_HOLEMASK_BIT;\n\t\thole = irec->ir_holemask & (1U << hole_idx);\n\n\t\terror = xchk_inobt_xref_finobt(sc, irec, i, free, hole);\n\t\tif (!xchk_should_check_xref(sc, &error, &sc->sa.fino_cur))\n\t\t\treturn;\n\t}\n}\n\n \nSTATIC int\nxchk_finobt_xref_inobt(\n\tstruct xfs_scrub\t*sc,\n\tstruct xfs_inobt_rec_incore *frec,\n\txfs_agino_t\t\tagino,\n\tbool\t\t\tffree,\n\tbool\t\t\tfhole)\n{\n\tstruct xfs_inobt_rec_incore irec;\n\tstruct xfs_btree_cur\t*cur = sc->sa.ino_cur;\n\tbool\t\t\tfree, hole;\n\tunsigned int\t\trec_idx, hole_idx;\n\tint\t\t\thas_record;\n\tint\t\t\terror;\n\n\tASSERT(cur->bc_btnum == XFS_BTNUM_INO);\n\n\terror = xfs_inobt_lookup(cur, agino, XFS_LOOKUP_LE, &has_record);\n\tif (error)\n\t\treturn error;\n\tif (!has_record)\n\t\tgoto no_record;\n\n\terror = xfs_inobt_get_rec(cur, &irec, &has_record);\n\tif (!has_record)\n\t\treturn -EFSCORRUPTED;\n\n\tif (irec.ir_startino + XFS_INODES_PER_CHUNK <= agino)\n\t\tgoto no_record;\n\n\t \n\trec_idx = agino - irec.ir_startino;\n\tfree = irec.ir_free & (1ULL << rec_idx);\n\thole_idx = rec_idx / XFS_INODES_PER_HOLEMASK_BIT;\n\thole = irec.ir_holemask & (1U << hole_idx);\n\n\tif (ffree != free)\n\t\txchk_btree_xref_set_corrupt(sc, cur, 0);\n\tif (fhole != hole)\n\t\txchk_btree_xref_set_corrupt(sc, cur, 0);\n\treturn 0;\n\nno_record:\n\t \n\txchk_btree_xref_set_corrupt(sc, cur, 0);\n\treturn 0;\n}\n\n \nSTATIC void\nxchk_finobt_chunk_xref_inobt(\n\tstruct xfs_scrub\t\t*sc,\n\tstruct xfs_inobt_rec_incore\t*frec,\n\txfs_agino_t\t\t\tagino,\n\tunsigned int\t\t\tnr_inodes)\n{\n\txfs_agino_t\t\t\ti;\n\tunsigned int\t\t\trec_idx;\n\tint\t\t\t\terror;\n\n\tASSERT(sc->sm->sm_type == XFS_SCRUB_TYPE_FINOBT);\n\n\tif (!sc->sa.ino_cur || xchk_skip_xref(sc->sm))\n\t\treturn;\n\n\tfor (i = agino, rec_idx = agino - frec->ir_startino;\n\t     i < agino + nr_inodes;\n\t     i++, rec_idx++) {\n\t\tbool\t\t\tffree, fhole;\n\t\tunsigned int\t\thole_idx;\n\n\t\tffree = frec->ir_free & (1ULL << rec_idx);\n\t\thole_idx = rec_idx / XFS_INODES_PER_HOLEMASK_BIT;\n\t\tfhole = frec->ir_holemask & (1U << hole_idx);\n\n\t\terror = xchk_finobt_xref_inobt(sc, frec, i, ffree, fhole);\n\t\tif (!xchk_should_check_xref(sc, &error, &sc->sa.ino_cur))\n\t\t\treturn;\n\t}\n}\n\n \nSTATIC bool\nxchk_iallocbt_chunk(\n\tstruct xchk_btree\t\t*bs,\n\tstruct xfs_inobt_rec_incore\t*irec,\n\txfs_agino_t\t\t\tagino,\n\tunsigned int\t\t\tnr_inodes)\n{\n\tstruct xfs_scrub\t\t*sc = bs->sc;\n\tstruct xfs_mount\t\t*mp = bs->cur->bc_mp;\n\tstruct xfs_perag\t\t*pag = bs->cur->bc_ag.pag;\n\txfs_agblock_t\t\t\tagbno;\n\txfs_extlen_t\t\t\tlen;\n\n\tagbno = XFS_AGINO_TO_AGBNO(mp, agino);\n\tlen = XFS_B_TO_FSB(mp, nr_inodes * mp->m_sb.sb_inodesize);\n\n\tif (!xfs_verify_agbext(pag, agbno, len))\n\t\txchk_btree_set_corrupt(bs->sc, bs->cur, 0);\n\n\tif (bs->sc->sm->sm_flags & XFS_SCRUB_OFLAG_CORRUPT)\n\t\treturn false;\n\n\txchk_xref_is_used_space(sc, agbno, len);\n\tif (sc->sm->sm_type == XFS_SCRUB_TYPE_INOBT)\n\t\txchk_inobt_chunk_xref_finobt(sc, irec, agino, nr_inodes);\n\telse\n\t\txchk_finobt_chunk_xref_inobt(sc, irec, agino, nr_inodes);\n\txchk_xref_is_only_owned_by(sc, agbno, len, &XFS_RMAP_OINFO_INODES);\n\txchk_xref_is_not_shared(sc, agbno, len);\n\txchk_xref_is_not_cow_staging(sc, agbno, len);\n\treturn true;\n}\n\n \nSTATIC int\nxchk_iallocbt_check_cluster_ifree(\n\tstruct xchk_btree\t\t*bs,\n\tstruct xfs_inobt_rec_incore\t*irec,\n\tunsigned int\t\t\tirec_ino,\n\tstruct xfs_dinode\t\t*dip)\n{\n\tstruct xfs_mount\t\t*mp = bs->cur->bc_mp;\n\txfs_ino_t\t\t\tfsino;\n\txfs_agino_t\t\t\tagino;\n\tbool\t\t\t\tirec_free;\n\tbool\t\t\t\tino_inuse;\n\tbool\t\t\t\tfreemask_ok;\n\tint\t\t\t\terror = 0;\n\n\tif (xchk_should_terminate(bs->sc, &error))\n\t\treturn error;\n\n\t \n\tagino = irec->ir_startino + irec_ino;\n\tfsino = XFS_AGINO_TO_INO(mp, bs->cur->bc_ag.pag->pag_agno, agino);\n\tirec_free = (irec->ir_free & XFS_INOBT_MASK(irec_ino));\n\n\tif (be16_to_cpu(dip->di_magic) != XFS_DINODE_MAGIC ||\n\t    (dip->di_version >= 3 && be64_to_cpu(dip->di_ino) != fsino)) {\n\t\txchk_btree_set_corrupt(bs->sc, bs->cur, 0);\n\t\tgoto out;\n\t}\n\n\terror = xchk_inode_is_allocated(bs->sc, agino, &ino_inuse);\n\tif (error == -ENODATA) {\n\t\t \n\t\tfreemask_ok = irec_free ^ !!(dip->di_mode);\n\t\tif (!(bs->sc->flags & XCHK_TRY_HARDER) && !freemask_ok)\n\t\t\treturn -EDEADLOCK;\n\t} else if (error < 0) {\n\t\t \n\t\tgoto out;\n\t} else {\n\t\t \n\t\tfreemask_ok = irec_free ^ ino_inuse;\n\t}\n\tif (!freemask_ok)\n\t\txchk_btree_set_corrupt(bs->sc, bs->cur, 0);\nout:\n\treturn 0;\n}\n\n \nSTATIC int\nxchk_iallocbt_check_cluster(\n\tstruct xchk_btree\t\t*bs,\n\tstruct xfs_inobt_rec_incore\t*irec,\n\tunsigned int\t\t\tcluster_base)\n{\n\tstruct xfs_imap\t\t\timap;\n\tstruct xfs_mount\t\t*mp = bs->cur->bc_mp;\n\tstruct xfs_buf\t\t\t*cluster_bp;\n\tunsigned int\t\t\tnr_inodes;\n\txfs_agnumber_t\t\t\tagno = bs->cur->bc_ag.pag->pag_agno;\n\txfs_agblock_t\t\t\tagbno;\n\tunsigned int\t\t\tcluster_index;\n\tuint16_t\t\t\tcluster_mask = 0;\n\tuint16_t\t\t\tir_holemask;\n\tint\t\t\t\terror = 0;\n\n\tnr_inodes = min_t(unsigned int, XFS_INODES_PER_CHUNK,\n\t\t\tM_IGEO(mp)->inodes_per_cluster);\n\n\t \n\tagbno = XFS_AGINO_TO_AGBNO(mp, irec->ir_startino + cluster_base);\n\n\t \n\tfor (cluster_index = 0;\n\t     cluster_index < nr_inodes;\n\t     cluster_index += XFS_INODES_PER_HOLEMASK_BIT)\n\t\tcluster_mask |= XFS_INOBT_MASK((cluster_base + cluster_index) /\n\t\t\t\tXFS_INODES_PER_HOLEMASK_BIT);\n\n\t \n\tir_holemask = (irec->ir_holemask & cluster_mask);\n\timap.im_blkno = XFS_AGB_TO_DADDR(mp, agno, agbno);\n\timap.im_len = XFS_FSB_TO_BB(mp, M_IGEO(mp)->blocks_per_cluster);\n\timap.im_boffset = XFS_INO_TO_OFFSET(mp, irec->ir_startino) <<\n\t\t\tmp->m_sb.sb_inodelog;\n\n\tif (imap.im_boffset != 0 && cluster_base != 0) {\n\t\tASSERT(imap.im_boffset == 0 || cluster_base == 0);\n\t\txchk_btree_set_corrupt(bs->sc, bs->cur, 0);\n\t\treturn 0;\n\t}\n\n\ttrace_xchk_iallocbt_check_cluster(mp, agno, irec->ir_startino,\n\t\t\timap.im_blkno, imap.im_len, cluster_base, nr_inodes,\n\t\t\tcluster_mask, ir_holemask,\n\t\t\tXFS_INO_TO_OFFSET(mp, irec->ir_startino +\n\t\t\t\t\t  cluster_base));\n\n\t \n\tif (ir_holemask != cluster_mask && ir_holemask != 0) {\n\t\txchk_btree_set_corrupt(bs->sc, bs->cur, 0);\n\t\treturn 0;\n\t}\n\n\t \n\tif (ir_holemask) {\n\t\txchk_xref_is_not_owned_by(bs->sc, agbno,\n\t\t\t\tM_IGEO(mp)->blocks_per_cluster,\n\t\t\t\t&XFS_RMAP_OINFO_INODES);\n\t\treturn 0;\n\t}\n\n\txchk_xref_is_only_owned_by(bs->sc, agbno, M_IGEO(mp)->blocks_per_cluster,\n\t\t\t&XFS_RMAP_OINFO_INODES);\n\n\t \n\terror = xfs_imap_to_bp(mp, bs->cur->bc_tp, &imap, &cluster_bp);\n\tif (!xchk_btree_xref_process_error(bs->sc, bs->cur, 0, &error))\n\t\treturn error;\n\n\t \n\tfor (cluster_index = 0; cluster_index < nr_inodes; cluster_index++) {\n\t\tstruct xfs_dinode\t*dip;\n\n\t\tif (imap.im_boffset >= BBTOB(cluster_bp->b_length)) {\n\t\t\txchk_btree_set_corrupt(bs->sc, bs->cur, 0);\n\t\t\tbreak;\n\t\t}\n\n\t\tdip = xfs_buf_offset(cluster_bp, imap.im_boffset);\n\t\terror = xchk_iallocbt_check_cluster_ifree(bs, irec,\n\t\t\t\tcluster_base + cluster_index, dip);\n\t\tif (error)\n\t\t\tbreak;\n\t\timap.im_boffset += mp->m_sb.sb_inodesize;\n\t}\n\n\txfs_trans_brelse(bs->cur->bc_tp, cluster_bp);\n\treturn error;\n}\n\n \nSTATIC int\nxchk_iallocbt_check_clusters(\n\tstruct xchk_btree\t\t*bs,\n\tstruct xfs_inobt_rec_incore\t*irec)\n{\n\tunsigned int\t\t\tcluster_base;\n\tint\t\t\t\terror = 0;\n\n\t \n\tfor (cluster_base = 0;\n\t     cluster_base < XFS_INODES_PER_CHUNK;\n\t     cluster_base += M_IGEO(bs->sc->mp)->inodes_per_cluster) {\n\t\terror = xchk_iallocbt_check_cluster(bs, irec, cluster_base);\n\t\tif (error)\n\t\t\tbreak;\n\t}\n\n\treturn error;\n}\n\n \nSTATIC void\nxchk_iallocbt_rec_alignment(\n\tstruct xchk_btree\t\t*bs,\n\tstruct xfs_inobt_rec_incore\t*irec)\n{\n\tstruct xfs_mount\t\t*mp = bs->sc->mp;\n\tstruct xchk_iallocbt\t\t*iabt = bs->private;\n\tstruct xfs_ino_geometry\t\t*igeo = M_IGEO(mp);\n\n\t \n\tif (bs->cur->bc_btnum == XFS_BTNUM_FINO) {\n\t\tunsigned int\timask;\n\n\t\timask = min_t(unsigned int, XFS_INODES_PER_CHUNK,\n\t\t\t\tigeo->cluster_align_inodes) - 1;\n\t\tif (irec->ir_startino & imask)\n\t\t\txchk_btree_set_corrupt(bs->sc, bs->cur, 0);\n\t\treturn;\n\t}\n\n\tif (iabt->next_startino != NULLAGINO) {\n\t\t \n\t\tif (irec->ir_startino != iabt->next_startino) {\n\t\t\txchk_btree_set_corrupt(bs->sc, bs->cur, 0);\n\t\t\treturn;\n\t\t}\n\n\t\tiabt->next_startino += XFS_INODES_PER_CHUNK;\n\n\t\t \n\t\tif (iabt->next_startino >= iabt->next_cluster_ino) {\n\t\t\tiabt->next_startino = NULLAGINO;\n\t\t\tiabt->next_cluster_ino = NULLAGINO;\n\t\t}\n\t\treturn;\n\t}\n\n\t \n\tif (irec->ir_startino & (igeo->cluster_align_inodes - 1)) {\n\t\txchk_btree_set_corrupt(bs->sc, bs->cur, 0);\n\t\treturn;\n\t}\n\n\tif (irec->ir_startino & (igeo->inodes_per_cluster - 1)) {\n\t\txchk_btree_set_corrupt(bs->sc, bs->cur, 0);\n\t\treturn;\n\t}\n\n\tif (igeo->inodes_per_cluster <= XFS_INODES_PER_CHUNK)\n\t\treturn;\n\n\t \n\tiabt->next_startino = irec->ir_startino + XFS_INODES_PER_CHUNK;\n\tiabt->next_cluster_ino = irec->ir_startino + igeo->inodes_per_cluster;\n}\n\n \nSTATIC int\nxchk_iallocbt_rec(\n\tstruct xchk_btree\t\t*bs,\n\tconst union xfs_btree_rec\t*rec)\n{\n\tstruct xfs_mount\t\t*mp = bs->cur->bc_mp;\n\tstruct xchk_iallocbt\t\t*iabt = bs->private;\n\tstruct xfs_inobt_rec_incore\tirec;\n\tuint64_t\t\t\tholes;\n\txfs_agino_t\t\t\tagino;\n\tint\t\t\t\tholecount;\n\tint\t\t\t\ti;\n\tint\t\t\t\terror = 0;\n\tuint16_t\t\t\tholemask;\n\n\txfs_inobt_btrec_to_irec(mp, rec, &irec);\n\tif (xfs_inobt_check_irec(bs->cur, &irec) != NULL) {\n\t\txchk_btree_set_corrupt(bs->sc, bs->cur, 0);\n\t\treturn 0;\n\t}\n\n\tagino = irec.ir_startino;\n\n\txchk_iallocbt_rec_alignment(bs, &irec);\n\tif (bs->sc->sm->sm_flags & XFS_SCRUB_OFLAG_CORRUPT)\n\t\tgoto out;\n\n\tiabt->inodes += irec.ir_count;\n\n\t \n\tif (!xfs_inobt_issparse(irec.ir_holemask)) {\n\t\tif (irec.ir_count != XFS_INODES_PER_CHUNK)\n\t\t\txchk_btree_set_corrupt(bs->sc, bs->cur, 0);\n\n\t\tif (!xchk_iallocbt_chunk(bs, &irec, agino,\n\t\t\t\t\tXFS_INODES_PER_CHUNK))\n\t\t\tgoto out;\n\t\tgoto check_clusters;\n\t}\n\n\t \n\tholemask = irec.ir_holemask;\n\tholecount = 0;\n\tholes = ~xfs_inobt_irec_to_allocmask(&irec);\n\tif ((holes & irec.ir_free) != holes ||\n\t    irec.ir_freecount > irec.ir_count)\n\t\txchk_btree_set_corrupt(bs->sc, bs->cur, 0);\n\n\tfor (i = 0; i < XFS_INOBT_HOLEMASK_BITS; i++) {\n\t\tif (holemask & 1)\n\t\t\tholecount += XFS_INODES_PER_HOLEMASK_BIT;\n\t\telse if (!xchk_iallocbt_chunk(bs, &irec, agino,\n\t\t\t\t\tXFS_INODES_PER_HOLEMASK_BIT))\n\t\t\tgoto out;\n\t\tholemask >>= 1;\n\t\tagino += XFS_INODES_PER_HOLEMASK_BIT;\n\t}\n\n\tif (holecount > XFS_INODES_PER_CHUNK ||\n\t    holecount + irec.ir_count != XFS_INODES_PER_CHUNK)\n\t\txchk_btree_set_corrupt(bs->sc, bs->cur, 0);\n\ncheck_clusters:\n\tif (bs->sc->sm->sm_flags & XFS_SCRUB_OFLAG_CORRUPT)\n\t\tgoto out;\n\n\terror = xchk_iallocbt_check_clusters(bs, &irec);\n\tif (error)\n\t\tgoto out;\n\nout:\n\treturn error;\n}\n\n \nSTATIC void\nxchk_iallocbt_xref_rmap_btreeblks(\n\tstruct xfs_scrub\t*sc,\n\tint\t\t\twhich)\n{\n\txfs_filblks_t\t\tblocks;\n\txfs_extlen_t\t\tinobt_blocks = 0;\n\txfs_extlen_t\t\tfinobt_blocks = 0;\n\tint\t\t\terror;\n\n\tif (!sc->sa.ino_cur || !sc->sa.rmap_cur ||\n\t    (xfs_has_finobt(sc->mp) && !sc->sa.fino_cur) ||\n\t    xchk_skip_xref(sc->sm))\n\t\treturn;\n\n\t \n\terror = xfs_btree_count_blocks(sc->sa.ino_cur, &inobt_blocks);\n\tif (!xchk_process_error(sc, 0, 0, &error))\n\t\treturn;\n\n\tif (sc->sa.fino_cur) {\n\t\terror = xfs_btree_count_blocks(sc->sa.fino_cur, &finobt_blocks);\n\t\tif (!xchk_process_error(sc, 0, 0, &error))\n\t\t\treturn;\n\t}\n\n\terror = xchk_count_rmap_ownedby_ag(sc, sc->sa.rmap_cur,\n\t\t\t&XFS_RMAP_OINFO_INOBT, &blocks);\n\tif (!xchk_should_check_xref(sc, &error, &sc->sa.rmap_cur))\n\t\treturn;\n\tif (blocks != inobt_blocks + finobt_blocks)\n\t\txchk_btree_set_corrupt(sc, sc->sa.ino_cur, 0);\n}\n\n \nSTATIC void\nxchk_iallocbt_xref_rmap_inodes(\n\tstruct xfs_scrub\t*sc,\n\tint\t\t\twhich,\n\tunsigned long long\tinodes)\n{\n\txfs_filblks_t\t\tblocks;\n\txfs_filblks_t\t\tinode_blocks;\n\tint\t\t\terror;\n\n\tif (!sc->sa.rmap_cur || xchk_skip_xref(sc->sm))\n\t\treturn;\n\n\t \n\terror = xchk_count_rmap_ownedby_ag(sc, sc->sa.rmap_cur,\n\t\t\t&XFS_RMAP_OINFO_INODES, &blocks);\n\tif (!xchk_should_check_xref(sc, &error, &sc->sa.rmap_cur))\n\t\treturn;\n\tinode_blocks = XFS_B_TO_FSB(sc->mp, inodes * sc->mp->m_sb.sb_inodesize);\n\tif (blocks != inode_blocks)\n\t\txchk_btree_xref_set_corrupt(sc, sc->sa.rmap_cur, 0);\n}\n\n \nSTATIC int\nxchk_iallocbt(\n\tstruct xfs_scrub\t*sc,\n\txfs_btnum_t\t\twhich)\n{\n\tstruct xfs_btree_cur\t*cur;\n\tstruct xchk_iallocbt\tiabt = {\n\t\t.inodes\t\t= 0,\n\t\t.next_startino\t= NULLAGINO,\n\t\t.next_cluster_ino = NULLAGINO,\n\t};\n\tint\t\t\terror;\n\n\tcur = which == XFS_BTNUM_INO ? sc->sa.ino_cur : sc->sa.fino_cur;\n\terror = xchk_btree(sc, cur, xchk_iallocbt_rec, &XFS_RMAP_OINFO_INOBT,\n\t\t\t&iabt);\n\tif (error)\n\t\treturn error;\n\n\txchk_iallocbt_xref_rmap_btreeblks(sc, which);\n\n\t \n\tif (which == XFS_BTNUM_INO)\n\t\txchk_iallocbt_xref_rmap_inodes(sc, which, iabt.inodes);\n\n\treturn error;\n}\n\nint\nxchk_inobt(\n\tstruct xfs_scrub\t*sc)\n{\n\treturn xchk_iallocbt(sc, XFS_BTNUM_INO);\n}\n\nint\nxchk_finobt(\n\tstruct xfs_scrub\t*sc)\n{\n\treturn xchk_iallocbt(sc, XFS_BTNUM_FINO);\n}\n\n \nstatic inline void\nxchk_xref_inode_check(\n\tstruct xfs_scrub\t*sc,\n\txfs_agblock_t\t\tagbno,\n\txfs_extlen_t\t\tlen,\n\tstruct xfs_btree_cur\t**icur,\n\tenum xbtree_recpacking\texpected)\n{\n\tenum xbtree_recpacking\toutcome;\n\tint\t\t\terror;\n\n\tif (!(*icur) || xchk_skip_xref(sc->sm))\n\t\treturn;\n\n\terror = xfs_ialloc_has_inodes_at_extent(*icur, agbno, len, &outcome);\n\tif (!xchk_should_check_xref(sc, &error, icur))\n\t\treturn;\n\tif (outcome != expected)\n\t\txchk_btree_xref_set_corrupt(sc, *icur, 0);\n}\n\n \nvoid\nxchk_xref_is_not_inode_chunk(\n\tstruct xfs_scrub\t*sc,\n\txfs_agblock_t\t\tagbno,\n\txfs_extlen_t\t\tlen)\n{\n\txchk_xref_inode_check(sc, agbno, len, &sc->sa.ino_cur,\n\t\t\tXBTREE_RECPACKING_EMPTY);\n\txchk_xref_inode_check(sc, agbno, len, &sc->sa.fino_cur,\n\t\t\tXBTREE_RECPACKING_EMPTY);\n}\n\n \nvoid\nxchk_xref_is_inode_chunk(\n\tstruct xfs_scrub\t*sc,\n\txfs_agblock_t\t\tagbno,\n\txfs_extlen_t\t\tlen)\n{\n\txchk_xref_inode_check(sc, agbno, len, &sc->sa.ino_cur,\n\t\t\tXBTREE_RECPACKING_FULL);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}