{
  "module_name": "xfs_icache.c",
  "hash_id": "53ac910471500f21ce8021c1e60377932444381eb67c41197aa247d97b653c07",
  "original_prompt": "Ingested from linux-6.6.14/fs/xfs/xfs_icache.c",
  "human_readable_source": "\n \n#include \"xfs.h\"\n#include \"xfs_fs.h\"\n#include \"xfs_shared.h\"\n#include \"xfs_format.h\"\n#include \"xfs_log_format.h\"\n#include \"xfs_trans_resv.h\"\n#include \"xfs_mount.h\"\n#include \"xfs_inode.h\"\n#include \"xfs_trans.h\"\n#include \"xfs_trans_priv.h\"\n#include \"xfs_inode_item.h\"\n#include \"xfs_quota.h\"\n#include \"xfs_trace.h\"\n#include \"xfs_icache.h\"\n#include \"xfs_bmap_util.h\"\n#include \"xfs_dquot_item.h\"\n#include \"xfs_dquot.h\"\n#include \"xfs_reflink.h\"\n#include \"xfs_ialloc.h\"\n#include \"xfs_ag.h\"\n#include \"xfs_log_priv.h\"\n\n#include <linux/iversion.h>\n\n \n\n \n#define XFS_ICI_RECLAIM_TAG\t0\n \n#define XFS_ICI_BLOCKGC_TAG\t1\n\n \nenum xfs_icwalk_goal {\n\t \n\tXFS_ICWALK_BLOCKGC\t= XFS_ICI_BLOCKGC_TAG,\n\tXFS_ICWALK_RECLAIM\t= XFS_ICI_RECLAIM_TAG,\n};\n\nstatic int xfs_icwalk(struct xfs_mount *mp,\n\t\tenum xfs_icwalk_goal goal, struct xfs_icwalk *icw);\nstatic int xfs_icwalk_ag(struct xfs_perag *pag,\n\t\tenum xfs_icwalk_goal goal, struct xfs_icwalk *icw);\n\n \n\n \n#define XFS_ICWALK_FLAG_SCAN_LIMIT\t(1U << 28)\n\n#define XFS_ICWALK_FLAG_RECLAIM_SICK\t(1U << 27)\n#define XFS_ICWALK_FLAG_UNION\t\t(1U << 26)  \n\n#define XFS_ICWALK_PRIVATE_FLAGS\t(XFS_ICWALK_FLAG_SCAN_LIMIT | \\\n\t\t\t\t\t XFS_ICWALK_FLAG_RECLAIM_SICK | \\\n\t\t\t\t\t XFS_ICWALK_FLAG_UNION)\n\n \nstruct xfs_inode *\nxfs_inode_alloc(\n\tstruct xfs_mount\t*mp,\n\txfs_ino_t\t\tino)\n{\n\tstruct xfs_inode\t*ip;\n\n\t \n\tip = alloc_inode_sb(mp->m_super, xfs_inode_cache, GFP_KERNEL | __GFP_NOFAIL);\n\n\tif (inode_init_always(mp->m_super, VFS_I(ip))) {\n\t\tkmem_cache_free(xfs_inode_cache, ip);\n\t\treturn NULL;\n\t}\n\n\t \n\tVFS_I(ip)->i_mode = 0;\n\tVFS_I(ip)->i_state = 0;\n\tmapping_set_large_folios(VFS_I(ip)->i_mapping);\n\n\tXFS_STATS_INC(mp, vn_active);\n\tASSERT(atomic_read(&ip->i_pincount) == 0);\n\tASSERT(ip->i_ino == 0);\n\n\t \n\tip->i_ino = ino;\n\tip->i_mount = mp;\n\tmemset(&ip->i_imap, 0, sizeof(struct xfs_imap));\n\tip->i_cowfp = NULL;\n\tmemset(&ip->i_af, 0, sizeof(ip->i_af));\n\tip->i_af.if_format = XFS_DINODE_FMT_EXTENTS;\n\tmemset(&ip->i_df, 0, sizeof(ip->i_df));\n\tip->i_flags = 0;\n\tip->i_delayed_blks = 0;\n\tip->i_diflags2 = mp->m_ino_geo.new_diflags2;\n\tip->i_nblocks = 0;\n\tip->i_forkoff = 0;\n\tip->i_sick = 0;\n\tip->i_checked = 0;\n\tINIT_WORK(&ip->i_ioend_work, xfs_end_io);\n\tINIT_LIST_HEAD(&ip->i_ioend_list);\n\tspin_lock_init(&ip->i_ioend_lock);\n\tip->i_next_unlinked = NULLAGINO;\n\tip->i_prev_unlinked = 0;\n\n\treturn ip;\n}\n\nSTATIC void\nxfs_inode_free_callback(\n\tstruct rcu_head\t\t*head)\n{\n\tstruct inode\t\t*inode = container_of(head, struct inode, i_rcu);\n\tstruct xfs_inode\t*ip = XFS_I(inode);\n\n\tswitch (VFS_I(ip)->i_mode & S_IFMT) {\n\tcase S_IFREG:\n\tcase S_IFDIR:\n\tcase S_IFLNK:\n\t\txfs_idestroy_fork(&ip->i_df);\n\t\tbreak;\n\t}\n\n\txfs_ifork_zap_attr(ip);\n\n\tif (ip->i_cowfp) {\n\t\txfs_idestroy_fork(ip->i_cowfp);\n\t\tkmem_cache_free(xfs_ifork_cache, ip->i_cowfp);\n\t}\n\tif (ip->i_itemp) {\n\t\tASSERT(!test_bit(XFS_LI_IN_AIL,\n\t\t\t\t &ip->i_itemp->ili_item.li_flags));\n\t\txfs_inode_item_destroy(ip);\n\t\tip->i_itemp = NULL;\n\t}\n\n\tkmem_cache_free(xfs_inode_cache, ip);\n}\n\nstatic void\n__xfs_inode_free(\n\tstruct xfs_inode\t*ip)\n{\n\t \n\tASSERT(atomic_read(&ip->i_pincount) == 0);\n\tASSERT(!ip->i_itemp || list_empty(&ip->i_itemp->ili_item.li_bio_list));\n\tXFS_STATS_DEC(ip->i_mount, vn_active);\n\n\tcall_rcu(&VFS_I(ip)->i_rcu, xfs_inode_free_callback);\n}\n\nvoid\nxfs_inode_free(\n\tstruct xfs_inode\t*ip)\n{\n\tASSERT(!xfs_iflags_test(ip, XFS_IFLUSHING));\n\n\t \n\tspin_lock(&ip->i_flags_lock);\n\tip->i_flags = XFS_IRECLAIM;\n\tip->i_ino = 0;\n\tspin_unlock(&ip->i_flags_lock);\n\n\t__xfs_inode_free(ip);\n}\n\n \nstatic void\nxfs_reclaim_work_queue(\n\tstruct xfs_mount        *mp)\n{\n\n\trcu_read_lock();\n\tif (radix_tree_tagged(&mp->m_perag_tree, XFS_ICI_RECLAIM_TAG)) {\n\t\tqueue_delayed_work(mp->m_reclaim_workqueue, &mp->m_reclaim_work,\n\t\t\tmsecs_to_jiffies(xfs_syncd_centisecs / 6 * 10));\n\t}\n\trcu_read_unlock();\n}\n\n \nstatic inline void\nxfs_blockgc_queue(\n\tstruct xfs_perag\t*pag)\n{\n\tstruct xfs_mount\t*mp = pag->pag_mount;\n\n\tif (!xfs_is_blockgc_enabled(mp))\n\t\treturn;\n\n\trcu_read_lock();\n\tif (radix_tree_tagged(&pag->pag_ici_root, XFS_ICI_BLOCKGC_TAG))\n\t\tqueue_delayed_work(pag->pag_mount->m_blockgc_wq,\n\t\t\t\t   &pag->pag_blockgc_work,\n\t\t\t\t   msecs_to_jiffies(xfs_blockgc_secs * 1000));\n\trcu_read_unlock();\n}\n\n \nstatic void\nxfs_perag_set_inode_tag(\n\tstruct xfs_perag\t*pag,\n\txfs_agino_t\t\tagino,\n\tunsigned int\t\ttag)\n{\n\tstruct xfs_mount\t*mp = pag->pag_mount;\n\tbool\t\t\twas_tagged;\n\n\tlockdep_assert_held(&pag->pag_ici_lock);\n\n\twas_tagged = radix_tree_tagged(&pag->pag_ici_root, tag);\n\tradix_tree_tag_set(&pag->pag_ici_root, agino, tag);\n\n\tif (tag == XFS_ICI_RECLAIM_TAG)\n\t\tpag->pag_ici_reclaimable++;\n\n\tif (was_tagged)\n\t\treturn;\n\n\t \n\tspin_lock(&mp->m_perag_lock);\n\tradix_tree_tag_set(&mp->m_perag_tree, pag->pag_agno, tag);\n\tspin_unlock(&mp->m_perag_lock);\n\n\t \n\tswitch (tag) {\n\tcase XFS_ICI_RECLAIM_TAG:\n\t\txfs_reclaim_work_queue(mp);\n\t\tbreak;\n\tcase XFS_ICI_BLOCKGC_TAG:\n\t\txfs_blockgc_queue(pag);\n\t\tbreak;\n\t}\n\n\ttrace_xfs_perag_set_inode_tag(pag, _RET_IP_);\n}\n\n \nstatic void\nxfs_perag_clear_inode_tag(\n\tstruct xfs_perag\t*pag,\n\txfs_agino_t\t\tagino,\n\tunsigned int\t\ttag)\n{\n\tstruct xfs_mount\t*mp = pag->pag_mount;\n\n\tlockdep_assert_held(&pag->pag_ici_lock);\n\n\t \n\tif (agino != NULLAGINO)\n\t\tradix_tree_tag_clear(&pag->pag_ici_root, agino, tag);\n\telse\n\t\tASSERT(tag == XFS_ICI_RECLAIM_TAG);\n\n\tif (tag == XFS_ICI_RECLAIM_TAG)\n\t\tpag->pag_ici_reclaimable--;\n\n\tif (radix_tree_tagged(&pag->pag_ici_root, tag))\n\t\treturn;\n\n\t \n\tspin_lock(&mp->m_perag_lock);\n\tradix_tree_tag_clear(&mp->m_perag_tree, pag->pag_agno, tag);\n\tspin_unlock(&mp->m_perag_lock);\n\n\ttrace_xfs_perag_clear_inode_tag(pag, _RET_IP_);\n}\n\n \nstatic int\nxfs_reinit_inode(\n\tstruct xfs_mount\t*mp,\n\tstruct inode\t\t*inode)\n{\n\tint\t\t\terror;\n\tuint32_t\t\tnlink = inode->i_nlink;\n\tuint32_t\t\tgeneration = inode->i_generation;\n\tuint64_t\t\tversion = inode_peek_iversion(inode);\n\tumode_t\t\t\tmode = inode->i_mode;\n\tdev_t\t\t\tdev = inode->i_rdev;\n\tkuid_t\t\t\tuid = inode->i_uid;\n\tkgid_t\t\t\tgid = inode->i_gid;\n\n\terror = inode_init_always(mp->m_super, inode);\n\n\tset_nlink(inode, nlink);\n\tinode->i_generation = generation;\n\tinode_set_iversion_queried(inode, version);\n\tinode->i_mode = mode;\n\tinode->i_rdev = dev;\n\tinode->i_uid = uid;\n\tinode->i_gid = gid;\n\tmapping_set_large_folios(inode->i_mapping);\n\treturn error;\n}\n\n \nstatic int\nxfs_iget_recycle(\n\tstruct xfs_perag\t*pag,\n\tstruct xfs_inode\t*ip) __releases(&ip->i_flags_lock)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct inode\t\t*inode = VFS_I(ip);\n\tint\t\t\terror;\n\n\ttrace_xfs_iget_recycle(ip);\n\n\tif (!xfs_ilock_nowait(ip, XFS_ILOCK_EXCL))\n\t\treturn -EAGAIN;\n\n\t \n\tip->i_flags |= XFS_IRECLAIM;\n\n\tspin_unlock(&ip->i_flags_lock);\n\trcu_read_unlock();\n\n\tASSERT(!rwsem_is_locked(&inode->i_rwsem));\n\terror = xfs_reinit_inode(mp, inode);\n\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n\tif (error) {\n\t\t \n\t\trcu_read_lock();\n\t\tspin_lock(&ip->i_flags_lock);\n\t\tip->i_flags &= ~(XFS_INEW | XFS_IRECLAIM);\n\t\tASSERT(ip->i_flags & XFS_IRECLAIMABLE);\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\trcu_read_unlock();\n\n\t\ttrace_xfs_iget_recycle_fail(ip);\n\t\treturn error;\n\t}\n\n\tspin_lock(&pag->pag_ici_lock);\n\tspin_lock(&ip->i_flags_lock);\n\n\t \n\tip->i_flags &= ~XFS_IRECLAIM_RESET_FLAGS;\n\tip->i_flags |= XFS_INEW;\n\txfs_perag_clear_inode_tag(pag, XFS_INO_TO_AGINO(mp, ip->i_ino),\n\t\t\tXFS_ICI_RECLAIM_TAG);\n\tinode->i_state = I_NEW;\n\tspin_unlock(&ip->i_flags_lock);\n\tspin_unlock(&pag->pag_ici_lock);\n\n\treturn 0;\n}\n\n \nstatic int\nxfs_iget_check_free_state(\n\tstruct xfs_inode\t*ip,\n\tint\t\t\tflags)\n{\n\tif (flags & XFS_IGET_CREATE) {\n\t\t \n\t\tif (VFS_I(ip)->i_mode != 0) {\n\t\t\txfs_warn(ip->i_mount,\n\"Corruption detected! Free inode 0x%llx not marked free! (mode 0x%x)\",\n\t\t\t\tip->i_ino, VFS_I(ip)->i_mode);\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\n\t\tif (ip->i_nblocks != 0) {\n\t\t\txfs_warn(ip->i_mount,\n\"Corruption detected! Free inode 0x%llx has blocks allocated!\",\n\t\t\t\tip->i_ino);\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t\treturn 0;\n\t}\n\n\t \n\tif (VFS_I(ip)->i_mode == 0)\n\t\treturn -ENOENT;\n\n\treturn 0;\n}\n\n \nstatic bool\nxfs_inodegc_queue_all(\n\tstruct xfs_mount\t*mp)\n{\n\tstruct xfs_inodegc\t*gc;\n\tint\t\t\tcpu;\n\tbool\t\t\tret = false;\n\n\tfor_each_cpu(cpu, &mp->m_inodegc_cpumask) {\n\t\tgc = per_cpu_ptr(mp->m_inodegc, cpu);\n\t\tif (!llist_empty(&gc->list)) {\n\t\t\tmod_delayed_work_on(cpu, mp->m_inodegc_wq, &gc->work, 0);\n\t\t\tret = true;\n\t\t}\n\t}\n\n\treturn ret;\n}\n\n \nstatic int\nxfs_inodegc_wait_all(\n\tstruct xfs_mount\t*mp)\n{\n\tint\t\t\tcpu;\n\tint\t\t\terror = 0;\n\n\tflush_workqueue(mp->m_inodegc_wq);\n\tfor_each_cpu(cpu, &mp->m_inodegc_cpumask) {\n\t\tstruct xfs_inodegc\t*gc;\n\n\t\tgc = per_cpu_ptr(mp->m_inodegc, cpu);\n\t\tif (gc->error && !error)\n\t\t\terror = gc->error;\n\t\tgc->error = 0;\n\t}\n\n\treturn error;\n}\n\n \nstatic int\nxfs_iget_cache_hit(\n\tstruct xfs_perag\t*pag,\n\tstruct xfs_inode\t*ip,\n\txfs_ino_t\t\tino,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags) __releases(RCU)\n{\n\tstruct inode\t\t*inode = VFS_I(ip);\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tint\t\t\terror;\n\n\t \n\tspin_lock(&ip->i_flags_lock);\n\tif (ip->i_ino != ino)\n\t\tgoto out_skip;\n\n\t \n\tif (ip->i_flags & (XFS_INEW | XFS_IRECLAIM | XFS_INACTIVATING))\n\t\tgoto out_skip;\n\n\tif (ip->i_flags & XFS_NEED_INACTIVE) {\n\t\t \n\t\tif (VFS_I(ip)->i_nlink == 0) {\n\t\t\terror = -ENOENT;\n\t\t\tgoto out_error;\n\t\t}\n\t\tgoto out_inodegc_flush;\n\t}\n\n\t \n\terror = xfs_iget_check_free_state(ip, flags);\n\tif (error)\n\t\tgoto out_error;\n\n\t \n\tif ((flags & XFS_IGET_INCORE) &&\n\t    (ip->i_flags & XFS_IRECLAIMABLE))\n\t\tgoto out_skip;\n\n\t \n\tif (ip->i_flags & XFS_IRECLAIMABLE) {\n\t\t \n\t\terror = xfs_iget_recycle(pag, ip);\n\t\tif (error == -EAGAIN)\n\t\t\tgoto out_skip;\n\t\tif (error)\n\t\t\treturn error;\n\t} else {\n\t\t \n\t\tif (!igrab(inode))\n\t\t\tgoto out_skip;\n\n\t\t \n\t\tspin_unlock(&ip->i_flags_lock);\n\t\trcu_read_unlock();\n\t\ttrace_xfs_iget_hit(ip);\n\t}\n\n\tif (lock_flags != 0)\n\t\txfs_ilock(ip, lock_flags);\n\n\tif (!(flags & XFS_IGET_INCORE))\n\t\txfs_iflags_clear(ip, XFS_ISTALE);\n\tXFS_STATS_INC(mp, xs_ig_found);\n\n\treturn 0;\n\nout_skip:\n\ttrace_xfs_iget_skip(ip);\n\tXFS_STATS_INC(mp, xs_ig_frecycle);\n\terror = -EAGAIN;\nout_error:\n\tspin_unlock(&ip->i_flags_lock);\n\trcu_read_unlock();\n\treturn error;\n\nout_inodegc_flush:\n\tspin_unlock(&ip->i_flags_lock);\n\trcu_read_unlock();\n\t \n\tif (xfs_is_inodegc_enabled(mp))\n\t\txfs_inodegc_queue_all(mp);\n\treturn -EAGAIN;\n}\n\nstatic int\nxfs_iget_cache_miss(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_perag\t*pag,\n\txfs_trans_t\t\t*tp,\n\txfs_ino_t\t\tino,\n\tstruct xfs_inode\t**ipp,\n\tint\t\t\tflags,\n\tint\t\t\tlock_flags)\n{\n\tstruct xfs_inode\t*ip;\n\tint\t\t\terror;\n\txfs_agino_t\t\tagino = XFS_INO_TO_AGINO(mp, ino);\n\tint\t\t\tiflags;\n\n\tip = xfs_inode_alloc(mp, ino);\n\tif (!ip)\n\t\treturn -ENOMEM;\n\n\terror = xfs_imap(pag, tp, ip->i_ino, &ip->i_imap, flags);\n\tif (error)\n\t\tgoto out_destroy;\n\n\t \n\tif (xfs_has_v3inodes(mp) &&\n\t    (flags & XFS_IGET_CREATE) && !xfs_has_ikeep(mp)) {\n\t\tVFS_I(ip)->i_generation = get_random_u32();\n\t} else {\n\t\tstruct xfs_buf\t\t*bp;\n\n\t\terror = xfs_imap_to_bp(mp, tp, &ip->i_imap, &bp);\n\t\tif (error)\n\t\t\tgoto out_destroy;\n\n\t\terror = xfs_inode_from_disk(ip,\n\t\t\t\txfs_buf_offset(bp, ip->i_imap.im_boffset));\n\t\tif (!error)\n\t\t\txfs_buf_set_ref(bp, XFS_INO_REF);\n\t\txfs_trans_brelse(tp, bp);\n\n\t\tif (error)\n\t\t\tgoto out_destroy;\n\t}\n\n\ttrace_xfs_iget_miss(ip);\n\n\t \n\terror = xfs_iget_check_free_state(ip, flags);\n\tif (error)\n\t\tgoto out_destroy;\n\n\t \n\tif (radix_tree_preload(GFP_NOFS)) {\n\t\terror = -EAGAIN;\n\t\tgoto out_destroy;\n\t}\n\n\t \n\tif (lock_flags) {\n\t\tif (!xfs_ilock_nowait(ip, lock_flags))\n\t\t\tBUG();\n\t}\n\n\t \n\tiflags = XFS_INEW;\n\tif (flags & XFS_IGET_DONTCACHE)\n\t\td_mark_dontcache(VFS_I(ip));\n\tip->i_udquot = NULL;\n\tip->i_gdquot = NULL;\n\tip->i_pdquot = NULL;\n\txfs_iflags_set(ip, iflags);\n\n\t \n\tspin_lock(&pag->pag_ici_lock);\n\terror = radix_tree_insert(&pag->pag_ici_root, agino, ip);\n\tif (unlikely(error)) {\n\t\tWARN_ON(error != -EEXIST);\n\t\tXFS_STATS_INC(mp, xs_ig_dup);\n\t\terror = -EAGAIN;\n\t\tgoto out_preload_end;\n\t}\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\n\t*ipp = ip;\n\treturn 0;\n\nout_preload_end:\n\tspin_unlock(&pag->pag_ici_lock);\n\tradix_tree_preload_end();\n\tif (lock_flags)\n\t\txfs_iunlock(ip, lock_flags);\nout_destroy:\n\t__destroy_inode(VFS_I(ip));\n\txfs_inode_free(ip);\n\treturn error;\n}\n\n \nint\nxfs_iget(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_trans\t*tp,\n\txfs_ino_t\t\tino,\n\tuint\t\t\tflags,\n\tuint\t\t\tlock_flags,\n\tstruct xfs_inode\t**ipp)\n{\n\tstruct xfs_inode\t*ip;\n\tstruct xfs_perag\t*pag;\n\txfs_agino_t\t\tagino;\n\tint\t\t\terror;\n\n\tASSERT((lock_flags & (XFS_IOLOCK_EXCL | XFS_IOLOCK_SHARED)) == 0);\n\n\t \n\tif (!ino || XFS_INO_TO_AGNO(mp, ino) >= mp->m_sb.sb_agcount)\n\t\treturn -EINVAL;\n\n\tXFS_STATS_INC(mp, xs_ig_attempts);\n\n\t \n\tpag = xfs_perag_get(mp, XFS_INO_TO_AGNO(mp, ino));\n\tagino = XFS_INO_TO_AGINO(mp, ino);\n\nagain:\n\terror = 0;\n\trcu_read_lock();\n\tip = radix_tree_lookup(&pag->pag_ici_root, agino);\n\n\tif (ip) {\n\t\terror = xfs_iget_cache_hit(pag, ip, ino, flags, lock_flags);\n\t\tif (error)\n\t\t\tgoto out_error_or_again;\n\t} else {\n\t\trcu_read_unlock();\n\t\tif (flags & XFS_IGET_INCORE) {\n\t\t\terror = -ENODATA;\n\t\t\tgoto out_error_or_again;\n\t\t}\n\t\tXFS_STATS_INC(mp, xs_ig_missed);\n\n\t\terror = xfs_iget_cache_miss(mp, pag, tp, ino, &ip,\n\t\t\t\t\t\t\tflags, lock_flags);\n\t\tif (error)\n\t\t\tgoto out_error_or_again;\n\t}\n\txfs_perag_put(pag);\n\n\t*ipp = ip;\n\n\t \n\tif (xfs_iflags_test(ip, XFS_INEW) && VFS_I(ip)->i_mode != 0)\n\t\txfs_setup_existing_inode(ip);\n\treturn 0;\n\nout_error_or_again:\n\tif (!(flags & (XFS_IGET_INCORE | XFS_IGET_NORETRY)) &&\n\t    error == -EAGAIN) {\n\t\tdelay(1);\n\t\tgoto again;\n\t}\n\txfs_perag_put(pag);\n\treturn error;\n}\n\n \nstatic bool\nxfs_reclaim_igrab(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_icwalk\t*icw)\n{\n\tASSERT(rcu_read_lock_held());\n\n\tspin_lock(&ip->i_flags_lock);\n\tif (!__xfs_iflags_test(ip, XFS_IRECLAIMABLE) ||\n\t    __xfs_iflags_test(ip, XFS_IRECLAIM)) {\n\t\t \n\t\tspin_unlock(&ip->i_flags_lock);\n\t\treturn false;\n\t}\n\n\t \n\tif (ip->i_sick &&\n\t    (!icw || !(icw->icw_flags & XFS_ICWALK_FLAG_RECLAIM_SICK))) {\n\t\tspin_unlock(&ip->i_flags_lock);\n\t\treturn false;\n\t}\n\n\t__xfs_iflags_set(ip, XFS_IRECLAIM);\n\tspin_unlock(&ip->i_flags_lock);\n\treturn true;\n}\n\n \nstatic void\nxfs_reclaim_inode(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_perag\t*pag)\n{\n\txfs_ino_t\t\tino = ip->i_ino;  \n\n\tif (!xfs_ilock_nowait(ip, XFS_ILOCK_EXCL))\n\t\tgoto out;\n\tif (xfs_iflags_test_and_set(ip, XFS_IFLUSHING))\n\t\tgoto out_iunlock;\n\n\t \n\tif (xlog_is_shutdown(ip->i_mount->m_log)) {\n\t\txfs_iunpin_wait(ip);\n\t\txfs_iflush_shutdown_abort(ip);\n\t\tgoto reclaim;\n\t}\n\tif (xfs_ipincount(ip))\n\t\tgoto out_clear_flush;\n\tif (!xfs_inode_clean(ip))\n\t\tgoto out_clear_flush;\n\n\txfs_iflags_clear(ip, XFS_IFLUSHING);\nreclaim:\n\ttrace_xfs_inode_reclaiming(ip);\n\n\t \n\tspin_lock(&ip->i_flags_lock);\n\tip->i_flags = XFS_IRECLAIM;\n\tip->i_ino = 0;\n\tip->i_sick = 0;\n\tip->i_checked = 0;\n\tspin_unlock(&ip->i_flags_lock);\n\n\tASSERT(!ip->i_itemp || ip->i_itemp->ili_item.li_buf == NULL);\n\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n\n\tXFS_STATS_INC(ip->i_mount, xs_ig_reclaims);\n\t \n\tspin_lock(&pag->pag_ici_lock);\n\tif (!radix_tree_delete(&pag->pag_ici_root,\n\t\t\t\tXFS_INO_TO_AGINO(ip->i_mount, ino)))\n\t\tASSERT(0);\n\txfs_perag_clear_inode_tag(pag, NULLAGINO, XFS_ICI_RECLAIM_TAG);\n\tspin_unlock(&pag->pag_ici_lock);\n\n\t \n\txfs_ilock(ip, XFS_ILOCK_EXCL);\n\tASSERT(!ip->i_udquot && !ip->i_gdquot && !ip->i_pdquot);\n\txfs_iunlock(ip, XFS_ILOCK_EXCL);\n\tASSERT(xfs_inode_clean(ip));\n\n\t__xfs_inode_free(ip);\n\treturn;\n\nout_clear_flush:\n\txfs_iflags_clear(ip, XFS_IFLUSHING);\nout_iunlock:\n\txfs_iunlock(ip, XFS_ILOCK_EXCL);\nout:\n\txfs_iflags_clear(ip, XFS_IRECLAIM);\n}\n\n \nstatic inline bool\nxfs_want_reclaim_sick(\n\tstruct xfs_mount\t*mp)\n{\n\treturn xfs_is_unmounting(mp) || xfs_has_norecovery(mp) ||\n\t       xfs_is_shutdown(mp);\n}\n\nvoid\nxfs_reclaim_inodes(\n\tstruct xfs_mount\t*mp)\n{\n\tstruct xfs_icwalk\ticw = {\n\t\t.icw_flags\t= 0,\n\t};\n\n\tif (xfs_want_reclaim_sick(mp))\n\t\ticw.icw_flags |= XFS_ICWALK_FLAG_RECLAIM_SICK;\n\n\twhile (radix_tree_tagged(&mp->m_perag_tree, XFS_ICI_RECLAIM_TAG)) {\n\t\txfs_ail_push_all_sync(mp->m_ail);\n\t\txfs_icwalk(mp, XFS_ICWALK_RECLAIM, &icw);\n\t}\n}\n\n \nlong\nxfs_reclaim_inodes_nr(\n\tstruct xfs_mount\t*mp,\n\tunsigned long\t\tnr_to_scan)\n{\n\tstruct xfs_icwalk\ticw = {\n\t\t.icw_flags\t= XFS_ICWALK_FLAG_SCAN_LIMIT,\n\t\t.icw_scan_limit\t= min_t(unsigned long, LONG_MAX, nr_to_scan),\n\t};\n\n\tif (xfs_want_reclaim_sick(mp))\n\t\ticw.icw_flags |= XFS_ICWALK_FLAG_RECLAIM_SICK;\n\n\t \n\txfs_reclaim_work_queue(mp);\n\txfs_ail_push_all(mp->m_ail);\n\n\txfs_icwalk(mp, XFS_ICWALK_RECLAIM, &icw);\n\treturn 0;\n}\n\n \nlong\nxfs_reclaim_inodes_count(\n\tstruct xfs_mount\t*mp)\n{\n\tstruct xfs_perag\t*pag;\n\txfs_agnumber_t\t\tag = 0;\n\tlong\t\t\treclaimable = 0;\n\n\twhile ((pag = xfs_perag_get_tag(mp, ag, XFS_ICI_RECLAIM_TAG))) {\n\t\tag = pag->pag_agno + 1;\n\t\treclaimable += pag->pag_ici_reclaimable;\n\t\txfs_perag_put(pag);\n\t}\n\treturn reclaimable;\n}\n\nSTATIC bool\nxfs_icwalk_match_id(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_icwalk\t*icw)\n{\n\tif ((icw->icw_flags & XFS_ICWALK_FLAG_UID) &&\n\t    !uid_eq(VFS_I(ip)->i_uid, icw->icw_uid))\n\t\treturn false;\n\n\tif ((icw->icw_flags & XFS_ICWALK_FLAG_GID) &&\n\t    !gid_eq(VFS_I(ip)->i_gid, icw->icw_gid))\n\t\treturn false;\n\n\tif ((icw->icw_flags & XFS_ICWALK_FLAG_PRID) &&\n\t    ip->i_projid != icw->icw_prid)\n\t\treturn false;\n\n\treturn true;\n}\n\n \nSTATIC bool\nxfs_icwalk_match_id_union(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_icwalk\t*icw)\n{\n\tif ((icw->icw_flags & XFS_ICWALK_FLAG_UID) &&\n\t    uid_eq(VFS_I(ip)->i_uid, icw->icw_uid))\n\t\treturn true;\n\n\tif ((icw->icw_flags & XFS_ICWALK_FLAG_GID) &&\n\t    gid_eq(VFS_I(ip)->i_gid, icw->icw_gid))\n\t\treturn true;\n\n\tif ((icw->icw_flags & XFS_ICWALK_FLAG_PRID) &&\n\t    ip->i_projid == icw->icw_prid)\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic bool\nxfs_icwalk_match(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_icwalk\t*icw)\n{\n\tbool\t\t\tmatch;\n\n\tif (!icw)\n\t\treturn true;\n\n\tif (icw->icw_flags & XFS_ICWALK_FLAG_UNION)\n\t\tmatch = xfs_icwalk_match_id_union(ip, icw);\n\telse\n\t\tmatch = xfs_icwalk_match_id(ip, icw);\n\tif (!match)\n\t\treturn false;\n\n\t \n\tif ((icw->icw_flags & XFS_ICWALK_FLAG_MINFILESIZE) &&\n\t    XFS_ISIZE(ip) < icw->icw_min_file_size)\n\t\treturn false;\n\n\treturn true;\n}\n\n \nvoid\nxfs_reclaim_worker(\n\tstruct work_struct *work)\n{\n\tstruct xfs_mount *mp = container_of(to_delayed_work(work),\n\t\t\t\t\tstruct xfs_mount, m_reclaim_work);\n\n\txfs_icwalk(mp, XFS_ICWALK_RECLAIM, NULL);\n\txfs_reclaim_work_queue(mp);\n}\n\nSTATIC int\nxfs_inode_free_eofblocks(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_icwalk\t*icw,\n\tunsigned int\t\t*lockflags)\n{\n\tbool\t\t\twait;\n\n\twait = icw && (icw->icw_flags & XFS_ICWALK_FLAG_SYNC);\n\n\tif (!xfs_iflags_test(ip, XFS_IEOFBLOCKS))\n\t\treturn 0;\n\n\t \n\tif (!wait && mapping_tagged(VFS_I(ip)->i_mapping, PAGECACHE_TAG_DIRTY))\n\t\treturn 0;\n\n\tif (!xfs_icwalk_match(ip, icw))\n\t\treturn 0;\n\n\t \n\tif (!xfs_ilock_nowait(ip, XFS_IOLOCK_EXCL)) {\n\t\tif (wait)\n\t\t\treturn -EAGAIN;\n\t\treturn 0;\n\t}\n\t*lockflags |= XFS_IOLOCK_EXCL;\n\n\tif (xfs_can_free_eofblocks(ip, false))\n\t\treturn xfs_free_eofblocks(ip);\n\n\t \n\ttrace_xfs_inode_free_eofblocks_invalid(ip);\n\txfs_inode_clear_eofblocks_tag(ip);\n\treturn 0;\n}\n\nstatic void\nxfs_blockgc_set_iflag(\n\tstruct xfs_inode\t*ip,\n\tunsigned long\t\tiflag)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_perag\t*pag;\n\n\tASSERT((iflag & ~(XFS_IEOFBLOCKS | XFS_ICOWBLOCKS)) == 0);\n\n\t \n\tif (ip->i_flags & iflag)\n\t\treturn;\n\tspin_lock(&ip->i_flags_lock);\n\tip->i_flags |= iflag;\n\tspin_unlock(&ip->i_flags_lock);\n\n\tpag = xfs_perag_get(mp, XFS_INO_TO_AGNO(mp, ip->i_ino));\n\tspin_lock(&pag->pag_ici_lock);\n\n\txfs_perag_set_inode_tag(pag, XFS_INO_TO_AGINO(mp, ip->i_ino),\n\t\t\tXFS_ICI_BLOCKGC_TAG);\n\n\tspin_unlock(&pag->pag_ici_lock);\n\txfs_perag_put(pag);\n}\n\nvoid\nxfs_inode_set_eofblocks_tag(\n\txfs_inode_t\t*ip)\n{\n\ttrace_xfs_inode_set_eofblocks_tag(ip);\n\treturn xfs_blockgc_set_iflag(ip, XFS_IEOFBLOCKS);\n}\n\nstatic void\nxfs_blockgc_clear_iflag(\n\tstruct xfs_inode\t*ip,\n\tunsigned long\t\tiflag)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_perag\t*pag;\n\tbool\t\t\tclear_tag;\n\n\tASSERT((iflag & ~(XFS_IEOFBLOCKS | XFS_ICOWBLOCKS)) == 0);\n\n\tspin_lock(&ip->i_flags_lock);\n\tip->i_flags &= ~iflag;\n\tclear_tag = (ip->i_flags & (XFS_IEOFBLOCKS | XFS_ICOWBLOCKS)) == 0;\n\tspin_unlock(&ip->i_flags_lock);\n\n\tif (!clear_tag)\n\t\treturn;\n\n\tpag = xfs_perag_get(mp, XFS_INO_TO_AGNO(mp, ip->i_ino));\n\tspin_lock(&pag->pag_ici_lock);\n\n\txfs_perag_clear_inode_tag(pag, XFS_INO_TO_AGINO(mp, ip->i_ino),\n\t\t\tXFS_ICI_BLOCKGC_TAG);\n\n\tspin_unlock(&pag->pag_ici_lock);\n\txfs_perag_put(pag);\n}\n\nvoid\nxfs_inode_clear_eofblocks_tag(\n\txfs_inode_t\t*ip)\n{\n\ttrace_xfs_inode_clear_eofblocks_tag(ip);\n\treturn xfs_blockgc_clear_iflag(ip, XFS_IEOFBLOCKS);\n}\n\n \nstatic bool\nxfs_prep_free_cowblocks(\n\tstruct xfs_inode\t*ip)\n{\n\t \n\tif (!xfs_inode_has_cow_data(ip)) {\n\t\ttrace_xfs_inode_free_cowblocks_invalid(ip);\n\t\txfs_inode_clear_cowblocks_tag(ip);\n\t\treturn false;\n\t}\n\n\t \n\tif ((VFS_I(ip)->i_state & I_DIRTY_PAGES) ||\n\t    mapping_tagged(VFS_I(ip)->i_mapping, PAGECACHE_TAG_DIRTY) ||\n\t    mapping_tagged(VFS_I(ip)->i_mapping, PAGECACHE_TAG_WRITEBACK) ||\n\t    atomic_read(&VFS_I(ip)->i_dio_count))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nSTATIC int\nxfs_inode_free_cowblocks(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_icwalk\t*icw,\n\tunsigned int\t\t*lockflags)\n{\n\tbool\t\t\twait;\n\tint\t\t\tret = 0;\n\n\twait = icw && (icw->icw_flags & XFS_ICWALK_FLAG_SYNC);\n\n\tif (!xfs_iflags_test(ip, XFS_ICOWBLOCKS))\n\t\treturn 0;\n\n\tif (!xfs_prep_free_cowblocks(ip))\n\t\treturn 0;\n\n\tif (!xfs_icwalk_match(ip, icw))\n\t\treturn 0;\n\n\t \n\tif (!(*lockflags & XFS_IOLOCK_EXCL) &&\n\t    !xfs_ilock_nowait(ip, XFS_IOLOCK_EXCL)) {\n\t\tif (wait)\n\t\t\treturn -EAGAIN;\n\t\treturn 0;\n\t}\n\t*lockflags |= XFS_IOLOCK_EXCL;\n\n\tif (!xfs_ilock_nowait(ip, XFS_MMAPLOCK_EXCL)) {\n\t\tif (wait)\n\t\t\treturn -EAGAIN;\n\t\treturn 0;\n\t}\n\t*lockflags |= XFS_MMAPLOCK_EXCL;\n\n\t \n\tif (xfs_prep_free_cowblocks(ip))\n\t\tret = xfs_reflink_cancel_cow_range(ip, 0, NULLFILEOFF, false);\n\treturn ret;\n}\n\nvoid\nxfs_inode_set_cowblocks_tag(\n\txfs_inode_t\t*ip)\n{\n\ttrace_xfs_inode_set_cowblocks_tag(ip);\n\treturn xfs_blockgc_set_iflag(ip, XFS_ICOWBLOCKS);\n}\n\nvoid\nxfs_inode_clear_cowblocks_tag(\n\txfs_inode_t\t*ip)\n{\n\ttrace_xfs_inode_clear_cowblocks_tag(ip);\n\treturn xfs_blockgc_clear_iflag(ip, XFS_ICOWBLOCKS);\n}\n\n \nvoid\nxfs_blockgc_stop(\n\tstruct xfs_mount\t*mp)\n{\n\tstruct xfs_perag\t*pag;\n\txfs_agnumber_t\t\tagno;\n\n\tif (!xfs_clear_blockgc_enabled(mp))\n\t\treturn;\n\n\tfor_each_perag(mp, agno, pag)\n\t\tcancel_delayed_work_sync(&pag->pag_blockgc_work);\n\ttrace_xfs_blockgc_stop(mp, __return_address);\n}\n\n \nvoid\nxfs_blockgc_start(\n\tstruct xfs_mount\t*mp)\n{\n\tstruct xfs_perag\t*pag;\n\txfs_agnumber_t\t\tagno;\n\n\tif (xfs_set_blockgc_enabled(mp))\n\t\treturn;\n\n\ttrace_xfs_blockgc_start(mp, __return_address);\n\tfor_each_perag_tag(mp, agno, pag, XFS_ICI_BLOCKGC_TAG)\n\t\txfs_blockgc_queue(pag);\n}\n\n \n#define XFS_BLOCKGC_NOGRAB_IFLAGS\t(XFS_INEW | \\\n\t\t\t\t\t XFS_NEED_INACTIVE | \\\n\t\t\t\t\t XFS_INACTIVATING | \\\n\t\t\t\t\t XFS_IRECLAIMABLE | \\\n\t\t\t\t\t XFS_IRECLAIM)\n \nstatic bool\nxfs_blockgc_igrab(\n\tstruct xfs_inode\t*ip)\n{\n\tstruct inode\t\t*inode = VFS_I(ip);\n\n\tASSERT(rcu_read_lock_held());\n\n\t \n\tspin_lock(&ip->i_flags_lock);\n\tif (!ip->i_ino)\n\t\tgoto out_unlock_noent;\n\n\tif (ip->i_flags & XFS_BLOCKGC_NOGRAB_IFLAGS)\n\t\tgoto out_unlock_noent;\n\tspin_unlock(&ip->i_flags_lock);\n\n\t \n\tif (xfs_is_shutdown(ip->i_mount))\n\t\treturn false;\n\n\t \n\tif (!igrab(inode))\n\t\treturn false;\n\n\t \n\treturn true;\n\nout_unlock_noent:\n\tspin_unlock(&ip->i_flags_lock);\n\treturn false;\n}\n\n \nstatic int\nxfs_blockgc_scan_inode(\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_icwalk\t*icw)\n{\n\tunsigned int\t\tlockflags = 0;\n\tint\t\t\terror;\n\n\terror = xfs_inode_free_eofblocks(ip, icw, &lockflags);\n\tif (error)\n\t\tgoto unlock;\n\n\terror = xfs_inode_free_cowblocks(ip, icw, &lockflags);\nunlock:\n\tif (lockflags)\n\t\txfs_iunlock(ip, lockflags);\n\txfs_irele(ip);\n\treturn error;\n}\n\n \nvoid\nxfs_blockgc_worker(\n\tstruct work_struct\t*work)\n{\n\tstruct xfs_perag\t*pag = container_of(to_delayed_work(work),\n\t\t\t\t\tstruct xfs_perag, pag_blockgc_work);\n\tstruct xfs_mount\t*mp = pag->pag_mount;\n\tint\t\t\terror;\n\n\ttrace_xfs_blockgc_worker(mp, __return_address);\n\n\terror = xfs_icwalk_ag(pag, XFS_ICWALK_BLOCKGC, NULL);\n\tif (error)\n\t\txfs_info(mp, \"AG %u preallocation gc worker failed, err=%d\",\n\t\t\t\tpag->pag_agno, error);\n\txfs_blockgc_queue(pag);\n}\n\n \nint\nxfs_blockgc_free_space(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_icwalk\t*icw)\n{\n\tint\t\t\terror;\n\n\ttrace_xfs_blockgc_free_space(mp, icw, _RET_IP_);\n\n\terror = xfs_icwalk(mp, XFS_ICWALK_BLOCKGC, icw);\n\tif (error)\n\t\treturn error;\n\n\treturn xfs_inodegc_flush(mp);\n}\n\n \nint\nxfs_blockgc_flush_all(\n\tstruct xfs_mount\t*mp)\n{\n\tstruct xfs_perag\t*pag;\n\txfs_agnumber_t\t\tagno;\n\n\ttrace_xfs_blockgc_flush_all(mp, __return_address);\n\n\t \n\tfor_each_perag_tag(mp, agno, pag, XFS_ICI_BLOCKGC_TAG)\n\t\tmod_delayed_work(pag->pag_mount->m_blockgc_wq,\n\t\t\t\t&pag->pag_blockgc_work, 0);\n\n\tfor_each_perag_tag(mp, agno, pag, XFS_ICI_BLOCKGC_TAG)\n\t\tflush_delayed_work(&pag->pag_blockgc_work);\n\n\treturn xfs_inodegc_flush(mp);\n}\n\n \nint\nxfs_blockgc_free_dquots(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_dquot\t*udqp,\n\tstruct xfs_dquot\t*gdqp,\n\tstruct xfs_dquot\t*pdqp,\n\tunsigned int\t\tiwalk_flags)\n{\n\tstruct xfs_icwalk\ticw = {0};\n\tbool\t\t\tdo_work = false;\n\n\tif (!udqp && !gdqp && !pdqp)\n\t\treturn 0;\n\n\t \n\ticw.icw_flags = XFS_ICWALK_FLAG_UNION | iwalk_flags;\n\n\tif (XFS_IS_UQUOTA_ENFORCED(mp) && udqp && xfs_dquot_lowsp(udqp)) {\n\t\ticw.icw_uid = make_kuid(mp->m_super->s_user_ns, udqp->q_id);\n\t\ticw.icw_flags |= XFS_ICWALK_FLAG_UID;\n\t\tdo_work = true;\n\t}\n\n\tif (XFS_IS_UQUOTA_ENFORCED(mp) && gdqp && xfs_dquot_lowsp(gdqp)) {\n\t\ticw.icw_gid = make_kgid(mp->m_super->s_user_ns, gdqp->q_id);\n\t\ticw.icw_flags |= XFS_ICWALK_FLAG_GID;\n\t\tdo_work = true;\n\t}\n\n\tif (XFS_IS_PQUOTA_ENFORCED(mp) && pdqp && xfs_dquot_lowsp(pdqp)) {\n\t\ticw.icw_prid = pdqp->q_id;\n\t\ticw.icw_flags |= XFS_ICWALK_FLAG_PRID;\n\t\tdo_work = true;\n\t}\n\n\tif (!do_work)\n\t\treturn 0;\n\n\treturn xfs_blockgc_free_space(mp, &icw);\n}\n\n \nint\nxfs_blockgc_free_quota(\n\tstruct xfs_inode\t*ip,\n\tunsigned int\t\tiwalk_flags)\n{\n\treturn xfs_blockgc_free_dquots(ip->i_mount,\n\t\t\txfs_inode_dquot(ip, XFS_DQTYPE_USER),\n\t\t\txfs_inode_dquot(ip, XFS_DQTYPE_GROUP),\n\t\t\txfs_inode_dquot(ip, XFS_DQTYPE_PROJ), iwalk_flags);\n}\n\n \n\n \n#define XFS_LOOKUP_BATCH\t32\n\n\n \nstatic inline bool\nxfs_icwalk_igrab(\n\tenum xfs_icwalk_goal\tgoal,\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_icwalk\t*icw)\n{\n\tswitch (goal) {\n\tcase XFS_ICWALK_BLOCKGC:\n\t\treturn xfs_blockgc_igrab(ip);\n\tcase XFS_ICWALK_RECLAIM:\n\t\treturn xfs_reclaim_igrab(ip, icw);\n\tdefault:\n\t\treturn false;\n\t}\n}\n\n \nstatic inline int\nxfs_icwalk_process_inode(\n\tenum xfs_icwalk_goal\tgoal,\n\tstruct xfs_inode\t*ip,\n\tstruct xfs_perag\t*pag,\n\tstruct xfs_icwalk\t*icw)\n{\n\tint\t\t\terror = 0;\n\n\tswitch (goal) {\n\tcase XFS_ICWALK_BLOCKGC:\n\t\terror = xfs_blockgc_scan_inode(ip, icw);\n\t\tbreak;\n\tcase XFS_ICWALK_RECLAIM:\n\t\txfs_reclaim_inode(ip, pag);\n\t\tbreak;\n\t}\n\treturn error;\n}\n\n \nstatic int\nxfs_icwalk_ag(\n\tstruct xfs_perag\t*pag,\n\tenum xfs_icwalk_goal\tgoal,\n\tstruct xfs_icwalk\t*icw)\n{\n\tstruct xfs_mount\t*mp = pag->pag_mount;\n\tuint32_t\t\tfirst_index;\n\tint\t\t\tlast_error = 0;\n\tint\t\t\tskipped;\n\tbool\t\t\tdone;\n\tint\t\t\tnr_found;\n\nrestart:\n\tdone = false;\n\tskipped = 0;\n\tif (goal == XFS_ICWALK_RECLAIM)\n\t\tfirst_index = READ_ONCE(pag->pag_ici_reclaim_cursor);\n\telse\n\t\tfirst_index = 0;\n\tnr_found = 0;\n\tdo {\n\t\tstruct xfs_inode *batch[XFS_LOOKUP_BATCH];\n\t\tint\t\terror = 0;\n\t\tint\t\ti;\n\n\t\trcu_read_lock();\n\n\t\tnr_found = radix_tree_gang_lookup_tag(&pag->pag_ici_root,\n\t\t\t\t(void **) batch, first_index,\n\t\t\t\tXFS_LOOKUP_BATCH, goal);\n\t\tif (!nr_found) {\n\t\t\tdone = true;\n\t\t\trcu_read_unlock();\n\t\t\tbreak;\n\t\t}\n\n\t\t \n\t\tfor (i = 0; i < nr_found; i++) {\n\t\t\tstruct xfs_inode *ip = batch[i];\n\n\t\t\tif (done || !xfs_icwalk_igrab(goal, ip, icw))\n\t\t\t\tbatch[i] = NULL;\n\n\t\t\t \n\t\t\tif (XFS_INO_TO_AGNO(mp, ip->i_ino) != pag->pag_agno)\n\t\t\t\tcontinue;\n\t\t\tfirst_index = XFS_INO_TO_AGINO(mp, ip->i_ino + 1);\n\t\t\tif (first_index < XFS_INO_TO_AGINO(mp, ip->i_ino))\n\t\t\t\tdone = true;\n\t\t}\n\n\t\t \n\t\trcu_read_unlock();\n\n\t\tfor (i = 0; i < nr_found; i++) {\n\t\t\tif (!batch[i])\n\t\t\t\tcontinue;\n\t\t\terror = xfs_icwalk_process_inode(goal, batch[i], pag,\n\t\t\t\t\ticw);\n\t\t\tif (error == -EAGAIN) {\n\t\t\t\tskipped++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (error && last_error != -EFSCORRUPTED)\n\t\t\t\tlast_error = error;\n\t\t}\n\n\t\t \n\t\tif (error == -EFSCORRUPTED)\n\t\t\tbreak;\n\n\t\tcond_resched();\n\n\t\tif (icw && (icw->icw_flags & XFS_ICWALK_FLAG_SCAN_LIMIT)) {\n\t\t\ticw->icw_scan_limit -= XFS_LOOKUP_BATCH;\n\t\t\tif (icw->icw_scan_limit <= 0)\n\t\t\t\tbreak;\n\t\t}\n\t} while (nr_found && !done);\n\n\tif (goal == XFS_ICWALK_RECLAIM) {\n\t\tif (done)\n\t\t\tfirst_index = 0;\n\t\tWRITE_ONCE(pag->pag_ici_reclaim_cursor, first_index);\n\t}\n\n\tif (skipped) {\n\t\tdelay(1);\n\t\tgoto restart;\n\t}\n\treturn last_error;\n}\n\n \nstatic int\nxfs_icwalk(\n\tstruct xfs_mount\t*mp,\n\tenum xfs_icwalk_goal\tgoal,\n\tstruct xfs_icwalk\t*icw)\n{\n\tstruct xfs_perag\t*pag;\n\tint\t\t\terror = 0;\n\tint\t\t\tlast_error = 0;\n\txfs_agnumber_t\t\tagno;\n\n\tfor_each_perag_tag(mp, agno, pag, goal) {\n\t\terror = xfs_icwalk_ag(pag, goal, icw);\n\t\tif (error) {\n\t\t\tlast_error = error;\n\t\t\tif (error == -EFSCORRUPTED) {\n\t\t\t\txfs_perag_rele(pag);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\treturn last_error;\n\tBUILD_BUG_ON(XFS_ICWALK_PRIVATE_FLAGS & XFS_ICWALK_FLAGS_VALID);\n}\n\n#ifdef DEBUG\nstatic void\nxfs_check_delalloc(\n\tstruct xfs_inode\t*ip,\n\tint\t\t\twhichfork)\n{\n\tstruct xfs_ifork\t*ifp = xfs_ifork_ptr(ip, whichfork);\n\tstruct xfs_bmbt_irec\tgot;\n\tstruct xfs_iext_cursor\ticur;\n\n\tif (!ifp || !xfs_iext_lookup_extent(ip, ifp, 0, &icur, &got))\n\t\treturn;\n\tdo {\n\t\tif (isnullstartblock(got.br_startblock)) {\n\t\t\txfs_warn(ip->i_mount,\n\t\"ino %llx %s fork has delalloc extent at [0x%llx:0x%llx]\",\n\t\t\t\tip->i_ino,\n\t\t\t\twhichfork == XFS_DATA_FORK ? \"data\" : \"cow\",\n\t\t\t\tgot.br_startoff, got.br_blockcount);\n\t\t}\n\t} while (xfs_iext_next_extent(ifp, &icur, &got));\n}\n#else\n#define xfs_check_delalloc(ip, whichfork)\tdo { } while (0)\n#endif\n\n \nstatic void\nxfs_inodegc_set_reclaimable(\n\tstruct xfs_inode\t*ip)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_perag\t*pag;\n\n\tif (!xfs_is_shutdown(mp) && ip->i_delayed_blks) {\n\t\txfs_check_delalloc(ip, XFS_DATA_FORK);\n\t\txfs_check_delalloc(ip, XFS_COW_FORK);\n\t\tASSERT(0);\n\t}\n\n\tpag = xfs_perag_get(mp, XFS_INO_TO_AGNO(mp, ip->i_ino));\n\tspin_lock(&pag->pag_ici_lock);\n\tspin_lock(&ip->i_flags_lock);\n\n\ttrace_xfs_inode_set_reclaimable(ip);\n\tip->i_flags &= ~(XFS_NEED_INACTIVE | XFS_INACTIVATING);\n\tip->i_flags |= XFS_IRECLAIMABLE;\n\txfs_perag_set_inode_tag(pag, XFS_INO_TO_AGINO(mp, ip->i_ino),\n\t\t\tXFS_ICI_RECLAIM_TAG);\n\n\tspin_unlock(&ip->i_flags_lock);\n\tspin_unlock(&pag->pag_ici_lock);\n\txfs_perag_put(pag);\n}\n\n \nstatic int\nxfs_inodegc_inactivate(\n\tstruct xfs_inode\t*ip)\n{\n\tint\t\t\terror;\n\n\ttrace_xfs_inode_inactivating(ip);\n\terror = xfs_inactive(ip);\n\txfs_inodegc_set_reclaimable(ip);\n\treturn error;\n\n}\n\nvoid\nxfs_inodegc_worker(\n\tstruct work_struct\t*work)\n{\n\tstruct xfs_inodegc\t*gc = container_of(to_delayed_work(work),\n\t\t\t\t\t\tstruct xfs_inodegc, work);\n\tstruct llist_node\t*node = llist_del_all(&gc->list);\n\tstruct xfs_inode\t*ip, *n;\n\tstruct xfs_mount\t*mp = gc->mp;\n\tunsigned int\t\tnofs_flag;\n\n\t \n\tcpumask_clear_cpu(gc->cpu, &mp->m_inodegc_cpumask);\n\tsmp_mb__after_atomic();\n\n\tWRITE_ONCE(gc->items, 0);\n\n\tif (!node)\n\t\treturn;\n\n\t \n\tnofs_flag = memalloc_nofs_save();\n\n\tip = llist_entry(node, struct xfs_inode, i_gclist);\n\ttrace_xfs_inodegc_worker(mp, READ_ONCE(gc->shrinker_hits));\n\n\tWRITE_ONCE(gc->shrinker_hits, 0);\n\tllist_for_each_entry_safe(ip, n, node, i_gclist) {\n\t\tint\terror;\n\n\t\txfs_iflags_set(ip, XFS_INACTIVATING);\n\t\terror = xfs_inodegc_inactivate(ip);\n\t\tif (error && !gc->error)\n\t\t\tgc->error = error;\n\t}\n\n\tmemalloc_nofs_restore(nofs_flag);\n}\n\n \nvoid\nxfs_inodegc_push(\n\tstruct xfs_mount\t*mp)\n{\n\tif (!xfs_is_inodegc_enabled(mp))\n\t\treturn;\n\ttrace_xfs_inodegc_push(mp, __return_address);\n\txfs_inodegc_queue_all(mp);\n}\n\n \nint\nxfs_inodegc_flush(\n\tstruct xfs_mount\t*mp)\n{\n\txfs_inodegc_push(mp);\n\ttrace_xfs_inodegc_flush(mp, __return_address);\n\treturn xfs_inodegc_wait_all(mp);\n}\n\n \nvoid\nxfs_inodegc_stop(\n\tstruct xfs_mount\t*mp)\n{\n\tbool\t\t\trerun;\n\n\tif (!xfs_clear_inodegc_enabled(mp))\n\t\treturn;\n\n\t \n\txfs_inodegc_queue_all(mp);\n\tdo {\n\t\tflush_workqueue(mp->m_inodegc_wq);\n\t\trerun = xfs_inodegc_queue_all(mp);\n\t} while (rerun);\n\n\ttrace_xfs_inodegc_stop(mp, __return_address);\n}\n\n \nvoid\nxfs_inodegc_start(\n\tstruct xfs_mount\t*mp)\n{\n\tif (xfs_set_inodegc_enabled(mp))\n\t\treturn;\n\n\ttrace_xfs_inodegc_start(mp, __return_address);\n\txfs_inodegc_queue_all(mp);\n}\n\n#ifdef CONFIG_XFS_RT\nstatic inline bool\nxfs_inodegc_want_queue_rt_file(\n\tstruct xfs_inode\t*ip)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\n\tif (!XFS_IS_REALTIME_INODE(ip))\n\t\treturn false;\n\n\tif (__percpu_counter_compare(&mp->m_frextents,\n\t\t\t\tmp->m_low_rtexts[XFS_LOWSP_5_PCNT],\n\t\t\t\tXFS_FDBLOCKS_BATCH) < 0)\n\t\treturn true;\n\n\treturn false;\n}\n#else\n# define xfs_inodegc_want_queue_rt_file(ip)\t(false)\n#endif  \n\n \nstatic inline bool\nxfs_inodegc_want_queue_work(\n\tstruct xfs_inode\t*ip,\n\tunsigned int\t\titems)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\n\tif (items > mp->m_ino_geo.inodes_per_cluster)\n\t\treturn true;\n\n\tif (__percpu_counter_compare(&mp->m_fdblocks,\n\t\t\t\tmp->m_low_space[XFS_LOWSP_5_PCNT],\n\t\t\t\tXFS_FDBLOCKS_BATCH) < 0)\n\t\treturn true;\n\n\tif (xfs_inodegc_want_queue_rt_file(ip))\n\t\treturn true;\n\n\tif (xfs_inode_near_dquot_enforcement(ip, XFS_DQTYPE_USER))\n\t\treturn true;\n\n\tif (xfs_inode_near_dquot_enforcement(ip, XFS_DQTYPE_GROUP))\n\t\treturn true;\n\n\tif (xfs_inode_near_dquot_enforcement(ip, XFS_DQTYPE_PROJ))\n\t\treturn true;\n\n\treturn false;\n}\n\n \n#define XFS_INODEGC_MAX_BACKLOG\t\t(4 * XFS_INODES_PER_CHUNK)\n\n \nstatic inline bool\nxfs_inodegc_want_flush_work(\n\tstruct xfs_inode\t*ip,\n\tunsigned int\t\titems,\n\tunsigned int\t\tshrinker_hits)\n{\n\tif (current->journal_info)\n\t\treturn false;\n\n\tif (shrinker_hits > 0)\n\t\treturn true;\n\n\tif (items > XFS_INODEGC_MAX_BACKLOG)\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic void\nxfs_inodegc_queue(\n\tstruct xfs_inode\t*ip)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tstruct xfs_inodegc\t*gc;\n\tint\t\t\titems;\n\tunsigned int\t\tshrinker_hits;\n\tunsigned int\t\tcpu_nr;\n\tunsigned long\t\tqueue_delay = 1;\n\n\ttrace_xfs_inode_set_need_inactive(ip);\n\tspin_lock(&ip->i_flags_lock);\n\tip->i_flags |= XFS_NEED_INACTIVE;\n\tspin_unlock(&ip->i_flags_lock);\n\n\tcpu_nr = get_cpu();\n\tgc = this_cpu_ptr(mp->m_inodegc);\n\tllist_add(&ip->i_gclist, &gc->list);\n\titems = READ_ONCE(gc->items);\n\tWRITE_ONCE(gc->items, items + 1);\n\tshrinker_hits = READ_ONCE(gc->shrinker_hits);\n\n\t \n\tsmp_mb__before_atomic();\n\tif (!cpumask_test_cpu(cpu_nr, &mp->m_inodegc_cpumask))\n\t\tcpumask_test_and_set_cpu(cpu_nr, &mp->m_inodegc_cpumask);\n\n\t \n\tif (!xfs_is_inodegc_enabled(mp)) {\n\t\tput_cpu();\n\t\treturn;\n\t}\n\n\tif (xfs_inodegc_want_queue_work(ip, items))\n\t\tqueue_delay = 0;\n\n\ttrace_xfs_inodegc_queue(mp, __return_address);\n\tmod_delayed_work_on(current_cpu(), mp->m_inodegc_wq, &gc->work,\n\t\t\tqueue_delay);\n\tput_cpu();\n\n\tif (xfs_inodegc_want_flush_work(ip, items, shrinker_hits)) {\n\t\ttrace_xfs_inodegc_throttle(mp, __return_address);\n\t\tflush_delayed_work(&gc->work);\n\t}\n}\n\n \nvoid\nxfs_inode_mark_reclaimable(\n\tstruct xfs_inode\t*ip)\n{\n\tstruct xfs_mount\t*mp = ip->i_mount;\n\tbool\t\t\tneed_inactive;\n\n\tXFS_STATS_INC(mp, vn_reclaim);\n\n\t \n\tASSERT_ALWAYS(!xfs_iflags_test(ip, XFS_ALL_IRECLAIM_FLAGS));\n\n\tneed_inactive = xfs_inode_needs_inactive(ip);\n\tif (need_inactive) {\n\t\txfs_inodegc_queue(ip);\n\t\treturn;\n\t}\n\n\t \n\txfs_qm_dqdetach(ip);\n\txfs_inodegc_set_reclaimable(ip);\n}\n\n \n#define XFS_INODEGC_SHRINKER_COUNT\t(1UL << DEF_PRIORITY)\n#define XFS_INODEGC_SHRINKER_BATCH\t((XFS_INODEGC_SHRINKER_COUNT / 2) + 1)\n\nstatic unsigned long\nxfs_inodegc_shrinker_count(\n\tstruct shrinker\t\t*shrink,\n\tstruct shrink_control\t*sc)\n{\n\tstruct xfs_mount\t*mp = container_of(shrink, struct xfs_mount,\n\t\t\t\t\t\t   m_inodegc_shrinker);\n\tstruct xfs_inodegc\t*gc;\n\tint\t\t\tcpu;\n\n\tif (!xfs_is_inodegc_enabled(mp))\n\t\treturn 0;\n\n\tfor_each_cpu(cpu, &mp->m_inodegc_cpumask) {\n\t\tgc = per_cpu_ptr(mp->m_inodegc, cpu);\n\t\tif (!llist_empty(&gc->list))\n\t\t\treturn XFS_INODEGC_SHRINKER_COUNT;\n\t}\n\n\treturn 0;\n}\n\nstatic unsigned long\nxfs_inodegc_shrinker_scan(\n\tstruct shrinker\t\t*shrink,\n\tstruct shrink_control\t*sc)\n{\n\tstruct xfs_mount\t*mp = container_of(shrink, struct xfs_mount,\n\t\t\t\t\t\t   m_inodegc_shrinker);\n\tstruct xfs_inodegc\t*gc;\n\tint\t\t\tcpu;\n\tbool\t\t\tno_items = true;\n\n\tif (!xfs_is_inodegc_enabled(mp))\n\t\treturn SHRINK_STOP;\n\n\ttrace_xfs_inodegc_shrinker_scan(mp, sc, __return_address);\n\n\tfor_each_cpu(cpu, &mp->m_inodegc_cpumask) {\n\t\tgc = per_cpu_ptr(mp->m_inodegc, cpu);\n\t\tif (!llist_empty(&gc->list)) {\n\t\t\tunsigned int\th = READ_ONCE(gc->shrinker_hits);\n\n\t\t\tWRITE_ONCE(gc->shrinker_hits, h + 1);\n\t\t\tmod_delayed_work_on(cpu, mp->m_inodegc_wq, &gc->work, 0);\n\t\t\tno_items = false;\n\t\t}\n\t}\n\n\t \n\tif (no_items)\n\t\treturn LONG_MAX;\n\n\treturn SHRINK_STOP;\n}\n\n \nint\nxfs_inodegc_register_shrinker(\n\tstruct xfs_mount\t*mp)\n{\n\tstruct shrinker\t\t*shrink = &mp->m_inodegc_shrinker;\n\n\tshrink->count_objects = xfs_inodegc_shrinker_count;\n\tshrink->scan_objects = xfs_inodegc_shrinker_scan;\n\tshrink->seeks = 0;\n\tshrink->flags = SHRINKER_NONSLAB;\n\tshrink->batch = XFS_INODEGC_SHRINKER_BATCH;\n\n\treturn register_shrinker(shrink, \"xfs-inodegc:%s\", mp->m_super->s_id);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}