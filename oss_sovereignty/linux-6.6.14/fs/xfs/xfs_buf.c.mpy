{
  "module_name": "xfs_buf.c",
  "hash_id": "d0532248f0e89a08eeecc2eb9bb80f991aa5e39705ab83fa46c9923cb346650f",
  "original_prompt": "Ingested from linux-6.6.14/fs/xfs/xfs_buf.c",
  "human_readable_source": "\n \n#include \"xfs.h\"\n#include <linux/backing-dev.h>\n#include <linux/dax.h>\n\n#include \"xfs_shared.h\"\n#include \"xfs_format.h\"\n#include \"xfs_log_format.h\"\n#include \"xfs_trans_resv.h\"\n#include \"xfs_mount.h\"\n#include \"xfs_trace.h\"\n#include \"xfs_log.h\"\n#include \"xfs_log_recover.h\"\n#include \"xfs_log_priv.h\"\n#include \"xfs_trans.h\"\n#include \"xfs_buf_item.h\"\n#include \"xfs_errortag.h\"\n#include \"xfs_error.h\"\n#include \"xfs_ag.h\"\n\nstruct kmem_cache *xfs_buf_cache;\n\n \n\nstatic int __xfs_buf_submit(struct xfs_buf *bp, bool wait);\n\nstatic inline int\nxfs_buf_submit(\n\tstruct xfs_buf\t\t*bp)\n{\n\treturn __xfs_buf_submit(bp, !(bp->b_flags & XBF_ASYNC));\n}\n\nstatic inline int\nxfs_buf_is_vmapped(\n\tstruct xfs_buf\t*bp)\n{\n\t \n\treturn bp->b_addr && bp->b_page_count > 1;\n}\n\nstatic inline int\nxfs_buf_vmap_len(\n\tstruct xfs_buf\t*bp)\n{\n\treturn (bp->b_page_count * PAGE_SIZE);\n}\n\n \nstatic inline void\nxfs_buf_ioacct_inc(\n\tstruct xfs_buf\t*bp)\n{\n\tif (bp->b_flags & XBF_NO_IOACCT)\n\t\treturn;\n\n\tASSERT(bp->b_flags & XBF_ASYNC);\n\tspin_lock(&bp->b_lock);\n\tif (!(bp->b_state & XFS_BSTATE_IN_FLIGHT)) {\n\t\tbp->b_state |= XFS_BSTATE_IN_FLIGHT;\n\t\tpercpu_counter_inc(&bp->b_target->bt_io_count);\n\t}\n\tspin_unlock(&bp->b_lock);\n}\n\n \nstatic inline void\n__xfs_buf_ioacct_dec(\n\tstruct xfs_buf\t*bp)\n{\n\tlockdep_assert_held(&bp->b_lock);\n\n\tif (bp->b_state & XFS_BSTATE_IN_FLIGHT) {\n\t\tbp->b_state &= ~XFS_BSTATE_IN_FLIGHT;\n\t\tpercpu_counter_dec(&bp->b_target->bt_io_count);\n\t}\n}\n\nstatic inline void\nxfs_buf_ioacct_dec(\n\tstruct xfs_buf\t*bp)\n{\n\tspin_lock(&bp->b_lock);\n\t__xfs_buf_ioacct_dec(bp);\n\tspin_unlock(&bp->b_lock);\n}\n\n \nvoid\nxfs_buf_stale(\n\tstruct xfs_buf\t*bp)\n{\n\tASSERT(xfs_buf_islocked(bp));\n\n\tbp->b_flags |= XBF_STALE;\n\n\t \n\tbp->b_flags &= ~_XBF_DELWRI_Q;\n\n\t \n\tspin_lock(&bp->b_lock);\n\t__xfs_buf_ioacct_dec(bp);\n\n\tatomic_set(&bp->b_lru_ref, 0);\n\tif (!(bp->b_state & XFS_BSTATE_DISPOSE) &&\n\t    (list_lru_del(&bp->b_target->bt_lru, &bp->b_lru)))\n\t\tatomic_dec(&bp->b_hold);\n\n\tASSERT(atomic_read(&bp->b_hold) >= 1);\n\tspin_unlock(&bp->b_lock);\n}\n\nstatic int\nxfs_buf_get_maps(\n\tstruct xfs_buf\t\t*bp,\n\tint\t\t\tmap_count)\n{\n\tASSERT(bp->b_maps == NULL);\n\tbp->b_map_count = map_count;\n\n\tif (map_count == 1) {\n\t\tbp->b_maps = &bp->__b_map;\n\t\treturn 0;\n\t}\n\n\tbp->b_maps = kmem_zalloc(map_count * sizeof(struct xfs_buf_map),\n\t\t\t\tKM_NOFS);\n\tif (!bp->b_maps)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\n \nstatic void\nxfs_buf_free_maps(\n\tstruct xfs_buf\t*bp)\n{\n\tif (bp->b_maps != &bp->__b_map) {\n\t\tkmem_free(bp->b_maps);\n\t\tbp->b_maps = NULL;\n\t}\n}\n\nstatic int\n_xfs_buf_alloc(\n\tstruct xfs_buftarg\t*target,\n\tstruct xfs_buf_map\t*map,\n\tint\t\t\tnmaps,\n\txfs_buf_flags_t\t\tflags,\n\tstruct xfs_buf\t\t**bpp)\n{\n\tstruct xfs_buf\t\t*bp;\n\tint\t\t\terror;\n\tint\t\t\ti;\n\n\t*bpp = NULL;\n\tbp = kmem_cache_zalloc(xfs_buf_cache, GFP_NOFS | __GFP_NOFAIL);\n\n\t \n\tflags &= ~(XBF_UNMAPPED | XBF_TRYLOCK | XBF_ASYNC | XBF_READ_AHEAD);\n\n\tatomic_set(&bp->b_hold, 1);\n\tatomic_set(&bp->b_lru_ref, 1);\n\tinit_completion(&bp->b_iowait);\n\tINIT_LIST_HEAD(&bp->b_lru);\n\tINIT_LIST_HEAD(&bp->b_list);\n\tINIT_LIST_HEAD(&bp->b_li_list);\n\tsema_init(&bp->b_sema, 0);  \n\tspin_lock_init(&bp->b_lock);\n\tbp->b_target = target;\n\tbp->b_mount = target->bt_mount;\n\tbp->b_flags = flags;\n\n\t \n\terror = xfs_buf_get_maps(bp, nmaps);\n\tif (error)  {\n\t\tkmem_cache_free(xfs_buf_cache, bp);\n\t\treturn error;\n\t}\n\n\tbp->b_rhash_key = map[0].bm_bn;\n\tbp->b_length = 0;\n\tfor (i = 0; i < nmaps; i++) {\n\t\tbp->b_maps[i].bm_bn = map[i].bm_bn;\n\t\tbp->b_maps[i].bm_len = map[i].bm_len;\n\t\tbp->b_length += map[i].bm_len;\n\t}\n\n\tatomic_set(&bp->b_pin_count, 0);\n\tinit_waitqueue_head(&bp->b_waiters);\n\n\tXFS_STATS_INC(bp->b_mount, xb_create);\n\ttrace_xfs_buf_init(bp, _RET_IP_);\n\n\t*bpp = bp;\n\treturn 0;\n}\n\nstatic void\nxfs_buf_free_pages(\n\tstruct xfs_buf\t*bp)\n{\n\tuint\t\ti;\n\n\tASSERT(bp->b_flags & _XBF_PAGES);\n\n\tif (xfs_buf_is_vmapped(bp))\n\t\tvm_unmap_ram(bp->b_addr, bp->b_page_count);\n\n\tfor (i = 0; i < bp->b_page_count; i++) {\n\t\tif (bp->b_pages[i])\n\t\t\t__free_page(bp->b_pages[i]);\n\t}\n\tmm_account_reclaimed_pages(bp->b_page_count);\n\n\tif (bp->b_pages != bp->b_page_array)\n\t\tkmem_free(bp->b_pages);\n\tbp->b_pages = NULL;\n\tbp->b_flags &= ~_XBF_PAGES;\n}\n\nstatic void\nxfs_buf_free_callback(\n\tstruct callback_head\t*cb)\n{\n\tstruct xfs_buf\t\t*bp = container_of(cb, struct xfs_buf, b_rcu);\n\n\txfs_buf_free_maps(bp);\n\tkmem_cache_free(xfs_buf_cache, bp);\n}\n\nstatic void\nxfs_buf_free(\n\tstruct xfs_buf\t\t*bp)\n{\n\ttrace_xfs_buf_free(bp, _RET_IP_);\n\n\tASSERT(list_empty(&bp->b_lru));\n\n\tif (bp->b_flags & _XBF_PAGES)\n\t\txfs_buf_free_pages(bp);\n\telse if (bp->b_flags & _XBF_KMEM)\n\t\tkmem_free(bp->b_addr);\n\n\tcall_rcu(&bp->b_rcu, xfs_buf_free_callback);\n}\n\nstatic int\nxfs_buf_alloc_kmem(\n\tstruct xfs_buf\t*bp,\n\txfs_buf_flags_t\tflags)\n{\n\txfs_km_flags_t\tkmflag_mask = KM_NOFS;\n\tsize_t\t\tsize = BBTOB(bp->b_length);\n\n\t \n\tif (!(flags & XBF_READ))\n\t\tkmflag_mask |= KM_ZERO;\n\n\tbp->b_addr = kmem_alloc(size, kmflag_mask);\n\tif (!bp->b_addr)\n\t\treturn -ENOMEM;\n\n\tif (((unsigned long)(bp->b_addr + size - 1) & PAGE_MASK) !=\n\t    ((unsigned long)bp->b_addr & PAGE_MASK)) {\n\t\t \n\t\tkmem_free(bp->b_addr);\n\t\tbp->b_addr = NULL;\n\t\treturn -ENOMEM;\n\t}\n\tbp->b_offset = offset_in_page(bp->b_addr);\n\tbp->b_pages = bp->b_page_array;\n\tbp->b_pages[0] = kmem_to_page(bp->b_addr);\n\tbp->b_page_count = 1;\n\tbp->b_flags |= _XBF_KMEM;\n\treturn 0;\n}\n\nstatic int\nxfs_buf_alloc_pages(\n\tstruct xfs_buf\t*bp,\n\txfs_buf_flags_t\tflags)\n{\n\tgfp_t\t\tgfp_mask = __GFP_NOWARN;\n\tlong\t\tfilled = 0;\n\n\tif (flags & XBF_READ_AHEAD)\n\t\tgfp_mask |= __GFP_NORETRY;\n\telse\n\t\tgfp_mask |= GFP_NOFS;\n\n\t \n\tbp->b_page_count = DIV_ROUND_UP(BBTOB(bp->b_length), PAGE_SIZE);\n\tif (bp->b_page_count <= XB_PAGES) {\n\t\tbp->b_pages = bp->b_page_array;\n\t} else {\n\t\tbp->b_pages = kzalloc(sizeof(struct page *) * bp->b_page_count,\n\t\t\t\t\tgfp_mask);\n\t\tif (!bp->b_pages)\n\t\t\treturn -ENOMEM;\n\t}\n\tbp->b_flags |= _XBF_PAGES;\n\n\t \n\tif (!(flags & XBF_READ))\n\t\tgfp_mask |= __GFP_ZERO;\n\n\t \n\tfor (;;) {\n\t\tlong\tlast = filled;\n\n\t\tfilled = alloc_pages_bulk_array(gfp_mask, bp->b_page_count,\n\t\t\t\t\t\tbp->b_pages);\n\t\tif (filled == bp->b_page_count) {\n\t\t\tXFS_STATS_INC(bp->b_mount, xb_page_found);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (filled != last)\n\t\t\tcontinue;\n\n\t\tif (flags & XBF_READ_AHEAD) {\n\t\t\txfs_buf_free_pages(bp);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tXFS_STATS_INC(bp->b_mount, xb_page_retries);\n\t\tmemalloc_retry_wait(gfp_mask);\n\t}\n\treturn 0;\n}\n\n \nSTATIC int\n_xfs_buf_map_pages(\n\tstruct xfs_buf\t\t*bp,\n\txfs_buf_flags_t\t\tflags)\n{\n\tASSERT(bp->b_flags & _XBF_PAGES);\n\tif (bp->b_page_count == 1) {\n\t\t \n\t\tbp->b_addr = page_address(bp->b_pages[0]);\n\t} else if (flags & XBF_UNMAPPED) {\n\t\tbp->b_addr = NULL;\n\t} else {\n\t\tint retried = 0;\n\t\tunsigned nofs_flag;\n\n\t\t \n\t\tnofs_flag = memalloc_nofs_save();\n\t\tdo {\n\t\t\tbp->b_addr = vm_map_ram(bp->b_pages, bp->b_page_count,\n\t\t\t\t\t\t-1);\n\t\t\tif (bp->b_addr)\n\t\t\t\tbreak;\n\t\t\tvm_unmap_aliases();\n\t\t} while (retried++ <= 1);\n\t\tmemalloc_nofs_restore(nofs_flag);\n\n\t\tif (!bp->b_addr)\n\t\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int\n_xfs_buf_obj_cmp(\n\tstruct rhashtable_compare_arg\t*arg,\n\tconst void\t\t\t*obj)\n{\n\tconst struct xfs_buf_map\t*map = arg->key;\n\tconst struct xfs_buf\t\t*bp = obj;\n\n\t \n\tBUILD_BUG_ON(offsetof(struct xfs_buf_map, bm_bn) != 0);\n\n\tif (bp->b_rhash_key != map->bm_bn)\n\t\treturn 1;\n\n\tif (unlikely(bp->b_length != map->bm_len)) {\n\t\t \n\t\tif (!(map->bm_flags & XBM_LIVESCAN))\n\t\t\tASSERT(bp->b_flags & XBF_STALE);\n\t\treturn 1;\n\t}\n\treturn 0;\n}\n\nstatic const struct rhashtable_params xfs_buf_hash_params = {\n\t.min_size\t\t= 32,\t \n\t.nelem_hint\t\t= 16,\n\t.key_len\t\t= sizeof(xfs_daddr_t),\n\t.key_offset\t\t= offsetof(struct xfs_buf, b_rhash_key),\n\t.head_offset\t\t= offsetof(struct xfs_buf, b_rhash_head),\n\t.automatic_shrinking\t= true,\n\t.obj_cmpfn\t\t= _xfs_buf_obj_cmp,\n};\n\nint\nxfs_buf_hash_init(\n\tstruct xfs_perag\t*pag)\n{\n\tspin_lock_init(&pag->pag_buf_lock);\n\treturn rhashtable_init(&pag->pag_buf_hash, &xfs_buf_hash_params);\n}\n\nvoid\nxfs_buf_hash_destroy(\n\tstruct xfs_perag\t*pag)\n{\n\trhashtable_destroy(&pag->pag_buf_hash);\n}\n\nstatic int\nxfs_buf_map_verify(\n\tstruct xfs_buftarg\t*btp,\n\tstruct xfs_buf_map\t*map)\n{\n\txfs_daddr_t\t\teofs;\n\n\t \n\tASSERT(!(BBTOB(map->bm_len) < btp->bt_meta_sectorsize));\n\tASSERT(!(BBTOB(map->bm_bn) & (xfs_off_t)btp->bt_meta_sectormask));\n\n\t \n\teofs = XFS_FSB_TO_BB(btp->bt_mount, btp->bt_mount->m_sb.sb_dblocks);\n\tif (map->bm_bn < 0 || map->bm_bn >= eofs) {\n\t\txfs_alert(btp->bt_mount,\n\t\t\t  \"%s: daddr 0x%llx out of range, EOFS 0x%llx\",\n\t\t\t  __func__, map->bm_bn, eofs);\n\t\tWARN_ON(1);\n\t\treturn -EFSCORRUPTED;\n\t}\n\treturn 0;\n}\n\nstatic int\nxfs_buf_find_lock(\n\tstruct xfs_buf          *bp,\n\txfs_buf_flags_t\t\tflags)\n{\n\tif (flags & XBF_TRYLOCK) {\n\t\tif (!xfs_buf_trylock(bp)) {\n\t\t\tXFS_STATS_INC(bp->b_mount, xb_busy_locked);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t} else {\n\t\txfs_buf_lock(bp);\n\t\tXFS_STATS_INC(bp->b_mount, xb_get_locked_waited);\n\t}\n\n\t \n\tif (bp->b_flags & XBF_STALE) {\n\t\tif (flags & XBF_LIVESCAN) {\n\t\t\txfs_buf_unlock(bp);\n\t\t\treturn -ENOENT;\n\t\t}\n\t\tASSERT((bp->b_flags & _XBF_DELWRI_Q) == 0);\n\t\tbp->b_flags &= _XBF_KMEM | _XBF_PAGES;\n\t\tbp->b_ops = NULL;\n\t}\n\treturn 0;\n}\n\nstatic inline int\nxfs_buf_lookup(\n\tstruct xfs_perag\t*pag,\n\tstruct xfs_buf_map\t*map,\n\txfs_buf_flags_t\t\tflags,\n\tstruct xfs_buf\t\t**bpp)\n{\n\tstruct xfs_buf          *bp;\n\tint\t\t\terror;\n\n\trcu_read_lock();\n\tbp = rhashtable_lookup(&pag->pag_buf_hash, map, xfs_buf_hash_params);\n\tif (!bp || !atomic_inc_not_zero(&bp->b_hold)) {\n\t\trcu_read_unlock();\n\t\treturn -ENOENT;\n\t}\n\trcu_read_unlock();\n\n\terror = xfs_buf_find_lock(bp, flags);\n\tif (error) {\n\t\txfs_buf_rele(bp);\n\t\treturn error;\n\t}\n\n\ttrace_xfs_buf_find(bp, flags, _RET_IP_);\n\t*bpp = bp;\n\treturn 0;\n}\n\n \nstatic int\nxfs_buf_find_insert(\n\tstruct xfs_buftarg\t*btp,\n\tstruct xfs_perag\t*pag,\n\tstruct xfs_buf_map\t*cmap,\n\tstruct xfs_buf_map\t*map,\n\tint\t\t\tnmaps,\n\txfs_buf_flags_t\t\tflags,\n\tstruct xfs_buf\t\t**bpp)\n{\n\tstruct xfs_buf\t\t*new_bp;\n\tstruct xfs_buf\t\t*bp;\n\tint\t\t\terror;\n\n\terror = _xfs_buf_alloc(btp, map, nmaps, flags, &new_bp);\n\tif (error)\n\t\tgoto out_drop_pag;\n\n\t \n\tif (BBTOB(new_bp->b_length) >= PAGE_SIZE ||\n\t    xfs_buf_alloc_kmem(new_bp, flags) < 0) {\n\t\terror = xfs_buf_alloc_pages(new_bp, flags);\n\t\tif (error)\n\t\t\tgoto out_free_buf;\n\t}\n\n\tspin_lock(&pag->pag_buf_lock);\n\tbp = rhashtable_lookup_get_insert_fast(&pag->pag_buf_hash,\n\t\t\t&new_bp->b_rhash_head, xfs_buf_hash_params);\n\tif (IS_ERR(bp)) {\n\t\terror = PTR_ERR(bp);\n\t\tspin_unlock(&pag->pag_buf_lock);\n\t\tgoto out_free_buf;\n\t}\n\tif (bp) {\n\t\t \n\t\tatomic_inc(&bp->b_hold);\n\t\tspin_unlock(&pag->pag_buf_lock);\n\t\terror = xfs_buf_find_lock(bp, flags);\n\t\tif (error)\n\t\t\txfs_buf_rele(bp);\n\t\telse\n\t\t\t*bpp = bp;\n\t\tgoto out_free_buf;\n\t}\n\n\t \n\tnew_bp->b_pag = pag;\n\tspin_unlock(&pag->pag_buf_lock);\n\t*bpp = new_bp;\n\treturn 0;\n\nout_free_buf:\n\txfs_buf_free(new_bp);\nout_drop_pag:\n\txfs_perag_put(pag);\n\treturn error;\n}\n\n \nint\nxfs_buf_get_map(\n\tstruct xfs_buftarg\t*btp,\n\tstruct xfs_buf_map\t*map,\n\tint\t\t\tnmaps,\n\txfs_buf_flags_t\t\tflags,\n\tstruct xfs_buf\t\t**bpp)\n{\n\tstruct xfs_perag\t*pag;\n\tstruct xfs_buf\t\t*bp = NULL;\n\tstruct xfs_buf_map\tcmap = { .bm_bn = map[0].bm_bn };\n\tint\t\t\terror;\n\tint\t\t\ti;\n\n\tif (flags & XBF_LIVESCAN)\n\t\tcmap.bm_flags |= XBM_LIVESCAN;\n\tfor (i = 0; i < nmaps; i++)\n\t\tcmap.bm_len += map[i].bm_len;\n\n\terror = xfs_buf_map_verify(btp, &cmap);\n\tif (error)\n\t\treturn error;\n\n\tpag = xfs_perag_get(btp->bt_mount,\n\t\t\t    xfs_daddr_to_agno(btp->bt_mount, cmap.bm_bn));\n\n\terror = xfs_buf_lookup(pag, &cmap, flags, &bp);\n\tif (error && error != -ENOENT)\n\t\tgoto out_put_perag;\n\n\t \n\tif (unlikely(!bp)) {\n\t\tXFS_STATS_INC(btp->bt_mount, xb_miss_locked);\n\n\t\tif (flags & XBF_INCORE)\n\t\t\tgoto out_put_perag;\n\n\t\t \n\t\terror = xfs_buf_find_insert(btp, pag, &cmap, map, nmaps,\n\t\t\t\tflags, &bp);\n\t\tif (error)\n\t\t\treturn error;\n\t} else {\n\t\tXFS_STATS_INC(btp->bt_mount, xb_get_locked);\n\t\txfs_perag_put(pag);\n\t}\n\n\t \n\tif (!bp->b_addr) {\n\t\terror = _xfs_buf_map_pages(bp, flags);\n\t\tif (unlikely(error)) {\n\t\t\txfs_warn_ratelimited(btp->bt_mount,\n\t\t\t\t\"%s: failed to map %u pages\", __func__,\n\t\t\t\tbp->b_page_count);\n\t\t\txfs_buf_relse(bp);\n\t\t\treturn error;\n\t\t}\n\t}\n\n\t \n\tif (!(flags & XBF_READ))\n\t\txfs_buf_ioerror(bp, 0);\n\n\tXFS_STATS_INC(btp->bt_mount, xb_get);\n\ttrace_xfs_buf_get(bp, flags, _RET_IP_);\n\t*bpp = bp;\n\treturn 0;\n\nout_put_perag:\n\txfs_perag_put(pag);\n\treturn error;\n}\n\nint\n_xfs_buf_read(\n\tstruct xfs_buf\t\t*bp,\n\txfs_buf_flags_t\t\tflags)\n{\n\tASSERT(!(flags & XBF_WRITE));\n\tASSERT(bp->b_maps[0].bm_bn != XFS_BUF_DADDR_NULL);\n\n\tbp->b_flags &= ~(XBF_WRITE | XBF_ASYNC | XBF_READ_AHEAD | XBF_DONE);\n\tbp->b_flags |= flags & (XBF_READ | XBF_ASYNC | XBF_READ_AHEAD);\n\n\treturn xfs_buf_submit(bp);\n}\n\n \nint\nxfs_buf_reverify(\n\tstruct xfs_buf\t\t*bp,\n\tconst struct xfs_buf_ops *ops)\n{\n\tASSERT(bp->b_flags & XBF_DONE);\n\tASSERT(bp->b_error == 0);\n\n\tif (!ops || bp->b_ops)\n\t\treturn 0;\n\n\tbp->b_ops = ops;\n\tbp->b_ops->verify_read(bp);\n\tif (bp->b_error)\n\t\tbp->b_flags &= ~XBF_DONE;\n\treturn bp->b_error;\n}\n\nint\nxfs_buf_read_map(\n\tstruct xfs_buftarg\t*target,\n\tstruct xfs_buf_map\t*map,\n\tint\t\t\tnmaps,\n\txfs_buf_flags_t\t\tflags,\n\tstruct xfs_buf\t\t**bpp,\n\tconst struct xfs_buf_ops *ops,\n\txfs_failaddr_t\t\tfa)\n{\n\tstruct xfs_buf\t\t*bp;\n\tint\t\t\terror;\n\n\tflags |= XBF_READ;\n\t*bpp = NULL;\n\n\terror = xfs_buf_get_map(target, map, nmaps, flags, &bp);\n\tif (error)\n\t\treturn error;\n\n\ttrace_xfs_buf_read(bp, flags, _RET_IP_);\n\n\tif (!(bp->b_flags & XBF_DONE)) {\n\t\t \n\t\tXFS_STATS_INC(target->bt_mount, xb_get_read);\n\t\tbp->b_ops = ops;\n\t\terror = _xfs_buf_read(bp, flags);\n\n\t\t \n\t\tif (flags & XBF_ASYNC)\n\t\t\treturn 0;\n\t} else {\n\t\t \n\t\terror = xfs_buf_reverify(bp, ops);\n\n\t\t \n\t\tif (flags & XBF_ASYNC) {\n\t\t\txfs_buf_relse(bp);\n\t\t\treturn 0;\n\t\t}\n\n\t\t \n\t\tbp->b_flags &= ~XBF_READ;\n\t\tASSERT(bp->b_ops != NULL || ops == NULL);\n\t}\n\n\t \n\tif (error) {\n\t\t \n\t\tif (!xlog_is_shutdown(target->bt_mount->m_log))\n\t\t\txfs_buf_ioerror_alert(bp, fa);\n\n\t\tbp->b_flags &= ~XBF_DONE;\n\t\txfs_buf_stale(bp);\n\t\txfs_buf_relse(bp);\n\n\t\t \n\t\tif (error == -EFSBADCRC)\n\t\t\terror = -EFSCORRUPTED;\n\t\treturn error;\n\t}\n\n\t*bpp = bp;\n\treturn 0;\n}\n\n \nvoid\nxfs_buf_readahead_map(\n\tstruct xfs_buftarg\t*target,\n\tstruct xfs_buf_map\t*map,\n\tint\t\t\tnmaps,\n\tconst struct xfs_buf_ops *ops)\n{\n\tstruct xfs_buf\t\t*bp;\n\n\txfs_buf_read_map(target, map, nmaps,\n\t\t     XBF_TRYLOCK | XBF_ASYNC | XBF_READ_AHEAD, &bp, ops,\n\t\t     __this_address);\n}\n\n \nint\nxfs_buf_read_uncached(\n\tstruct xfs_buftarg\t*target,\n\txfs_daddr_t\t\tdaddr,\n\tsize_t\t\t\tnumblks,\n\txfs_buf_flags_t\t\tflags,\n\tstruct xfs_buf\t\t**bpp,\n\tconst struct xfs_buf_ops *ops)\n{\n\tstruct xfs_buf\t\t*bp;\n\tint\t\t\terror;\n\n\t*bpp = NULL;\n\n\terror = xfs_buf_get_uncached(target, numblks, flags, &bp);\n\tif (error)\n\t\treturn error;\n\n\t \n\tASSERT(bp->b_map_count == 1);\n\tbp->b_rhash_key = XFS_BUF_DADDR_NULL;\n\tbp->b_maps[0].bm_bn = daddr;\n\tbp->b_flags |= XBF_READ;\n\tbp->b_ops = ops;\n\n\txfs_buf_submit(bp);\n\tif (bp->b_error) {\n\t\terror = bp->b_error;\n\t\txfs_buf_relse(bp);\n\t\treturn error;\n\t}\n\n\t*bpp = bp;\n\treturn 0;\n}\n\nint\nxfs_buf_get_uncached(\n\tstruct xfs_buftarg\t*target,\n\tsize_t\t\t\tnumblks,\n\txfs_buf_flags_t\t\tflags,\n\tstruct xfs_buf\t\t**bpp)\n{\n\tint\t\t\terror;\n\tstruct xfs_buf\t\t*bp;\n\tDEFINE_SINGLE_BUF_MAP(map, XFS_BUF_DADDR_NULL, numblks);\n\n\t*bpp = NULL;\n\n\t \n\terror = _xfs_buf_alloc(target, &map, 1, flags & XBF_NO_IOACCT, &bp);\n\tif (error)\n\t\treturn error;\n\n\terror = xfs_buf_alloc_pages(bp, flags);\n\tif (error)\n\t\tgoto fail_free_buf;\n\n\terror = _xfs_buf_map_pages(bp, 0);\n\tif (unlikely(error)) {\n\t\txfs_warn(target->bt_mount,\n\t\t\t\"%s: failed to map pages\", __func__);\n\t\tgoto fail_free_buf;\n\t}\n\n\ttrace_xfs_buf_get_uncached(bp, _RET_IP_);\n\t*bpp = bp;\n\treturn 0;\n\nfail_free_buf:\n\txfs_buf_free(bp);\n\treturn error;\n}\n\n \nvoid\nxfs_buf_hold(\n\tstruct xfs_buf\t\t*bp)\n{\n\ttrace_xfs_buf_hold(bp, _RET_IP_);\n\tatomic_inc(&bp->b_hold);\n}\n\n \nvoid\nxfs_buf_rele(\n\tstruct xfs_buf\t\t*bp)\n{\n\tstruct xfs_perag\t*pag = bp->b_pag;\n\tbool\t\t\trelease;\n\tbool\t\t\tfreebuf = false;\n\n\ttrace_xfs_buf_rele(bp, _RET_IP_);\n\n\tif (!pag) {\n\t\tASSERT(list_empty(&bp->b_lru));\n\t\tif (atomic_dec_and_test(&bp->b_hold)) {\n\t\t\txfs_buf_ioacct_dec(bp);\n\t\t\txfs_buf_free(bp);\n\t\t}\n\t\treturn;\n\t}\n\n\tASSERT(atomic_read(&bp->b_hold) > 0);\n\n\t \n\tspin_lock(&bp->b_lock);\n\trelease = atomic_dec_and_lock(&bp->b_hold, &pag->pag_buf_lock);\n\tif (!release) {\n\t\t \n\t\tif ((atomic_read(&bp->b_hold) == 1) && !list_empty(&bp->b_lru))\n\t\t\t__xfs_buf_ioacct_dec(bp);\n\t\tgoto out_unlock;\n\t}\n\n\t \n\t__xfs_buf_ioacct_dec(bp);\n\tif (!(bp->b_flags & XBF_STALE) && atomic_read(&bp->b_lru_ref)) {\n\t\t \n\t\tif (list_lru_add(&bp->b_target->bt_lru, &bp->b_lru)) {\n\t\t\tbp->b_state &= ~XFS_BSTATE_DISPOSE;\n\t\t\tatomic_inc(&bp->b_hold);\n\t\t}\n\t\tspin_unlock(&pag->pag_buf_lock);\n\t} else {\n\t\t \n\t\tif (!(bp->b_state & XFS_BSTATE_DISPOSE)) {\n\t\t\tlist_lru_del(&bp->b_target->bt_lru, &bp->b_lru);\n\t\t} else {\n\t\t\tASSERT(list_empty(&bp->b_lru));\n\t\t}\n\n\t\tASSERT(!(bp->b_flags & _XBF_DELWRI_Q));\n\t\trhashtable_remove_fast(&pag->pag_buf_hash, &bp->b_rhash_head,\n\t\t\t\t       xfs_buf_hash_params);\n\t\tspin_unlock(&pag->pag_buf_lock);\n\t\txfs_perag_put(pag);\n\t\tfreebuf = true;\n\t}\n\nout_unlock:\n\tspin_unlock(&bp->b_lock);\n\n\tif (freebuf)\n\t\txfs_buf_free(bp);\n}\n\n\n \nint\nxfs_buf_trylock(\n\tstruct xfs_buf\t\t*bp)\n{\n\tint\t\t\tlocked;\n\n\tlocked = down_trylock(&bp->b_sema) == 0;\n\tif (locked)\n\t\ttrace_xfs_buf_trylock(bp, _RET_IP_);\n\telse\n\t\ttrace_xfs_buf_trylock_fail(bp, _RET_IP_);\n\treturn locked;\n}\n\n \nvoid\nxfs_buf_lock(\n\tstruct xfs_buf\t\t*bp)\n{\n\ttrace_xfs_buf_lock(bp, _RET_IP_);\n\n\tif (atomic_read(&bp->b_pin_count) && (bp->b_flags & XBF_STALE))\n\t\txfs_log_force(bp->b_mount, 0);\n\tdown(&bp->b_sema);\n\n\ttrace_xfs_buf_lock_done(bp, _RET_IP_);\n}\n\nvoid\nxfs_buf_unlock(\n\tstruct xfs_buf\t\t*bp)\n{\n\tASSERT(xfs_buf_islocked(bp));\n\n\tup(&bp->b_sema);\n\ttrace_xfs_buf_unlock(bp, _RET_IP_);\n}\n\nSTATIC void\nxfs_buf_wait_unpin(\n\tstruct xfs_buf\t\t*bp)\n{\n\tDECLARE_WAITQUEUE\t(wait, current);\n\n\tif (atomic_read(&bp->b_pin_count) == 0)\n\t\treturn;\n\n\tadd_wait_queue(&bp->b_waiters, &wait);\n\tfor (;;) {\n\t\tset_current_state(TASK_UNINTERRUPTIBLE);\n\t\tif (atomic_read(&bp->b_pin_count) == 0)\n\t\t\tbreak;\n\t\tio_schedule();\n\t}\n\tremove_wait_queue(&bp->b_waiters, &wait);\n\tset_current_state(TASK_RUNNING);\n}\n\nstatic void\nxfs_buf_ioerror_alert_ratelimited(\n\tstruct xfs_buf\t\t*bp)\n{\n\tstatic unsigned long\tlasttime;\n\tstatic struct xfs_buftarg *lasttarg;\n\n\tif (bp->b_target != lasttarg ||\n\t    time_after(jiffies, (lasttime + 5*HZ))) {\n\t\tlasttime = jiffies;\n\t\txfs_buf_ioerror_alert(bp, __this_address);\n\t}\n\tlasttarg = bp->b_target;\n}\n\n \nstatic bool\nxfs_buf_ioerror_permanent(\n\tstruct xfs_buf\t\t*bp,\n\tstruct xfs_error_cfg\t*cfg)\n{\n\tstruct xfs_mount\t*mp = bp->b_mount;\n\n\tif (cfg->max_retries != XFS_ERR_RETRY_FOREVER &&\n\t    ++bp->b_retries > cfg->max_retries)\n\t\treturn true;\n\tif (cfg->retry_timeout != XFS_ERR_RETRY_FOREVER &&\n\t    time_after(jiffies, cfg->retry_timeout + bp->b_first_retry_time))\n\t\treturn true;\n\n\t \n\tif (xfs_is_unmounting(mp) && mp->m_fail_unmount)\n\t\treturn true;\n\n\treturn false;\n}\n\n \nstatic bool\nxfs_buf_ioend_handle_error(\n\tstruct xfs_buf\t\t*bp)\n{\n\tstruct xfs_mount\t*mp = bp->b_mount;\n\tstruct xfs_error_cfg\t*cfg;\n\n\t \n\tif (xlog_is_shutdown(mp->m_log))\n\t\tgoto out_stale;\n\n\txfs_buf_ioerror_alert_ratelimited(bp);\n\n\t \n\tif (bp->b_flags & _XBF_LOGRECOVERY) {\n\t\txfs_force_shutdown(mp, SHUTDOWN_META_IO_ERROR);\n\t\treturn false;\n\t}\n\n\t \n\tif (!(bp->b_flags & XBF_ASYNC))\n\t\tgoto out_stale;\n\n\ttrace_xfs_buf_iodone_async(bp, _RET_IP_);\n\n\tcfg = xfs_error_get_cfg(mp, XFS_ERR_METADATA, bp->b_error);\n\tif (bp->b_last_error != bp->b_error ||\n\t    !(bp->b_flags & (XBF_STALE | XBF_WRITE_FAIL))) {\n\t\tbp->b_last_error = bp->b_error;\n\t\tif (cfg->retry_timeout != XFS_ERR_RETRY_FOREVER &&\n\t\t    !bp->b_first_retry_time)\n\t\t\tbp->b_first_retry_time = jiffies;\n\t\tgoto resubmit;\n\t}\n\n\t \n\tif (xfs_buf_ioerror_permanent(bp, cfg)) {\n\t\txfs_force_shutdown(mp, SHUTDOWN_META_IO_ERROR);\n\t\tgoto out_stale;\n\t}\n\n\t \n\tif (bp->b_flags & _XBF_INODES)\n\t\txfs_buf_inode_io_fail(bp);\n\telse if (bp->b_flags & _XBF_DQUOTS)\n\t\txfs_buf_dquot_io_fail(bp);\n\telse\n\t\tASSERT(list_empty(&bp->b_li_list));\n\txfs_buf_ioerror(bp, 0);\n\txfs_buf_relse(bp);\n\treturn true;\n\nresubmit:\n\txfs_buf_ioerror(bp, 0);\n\tbp->b_flags |= (XBF_DONE | XBF_WRITE_FAIL);\n\txfs_buf_submit(bp);\n\treturn true;\nout_stale:\n\txfs_buf_stale(bp);\n\tbp->b_flags |= XBF_DONE;\n\tbp->b_flags &= ~XBF_WRITE;\n\ttrace_xfs_buf_error_relse(bp, _RET_IP_);\n\treturn false;\n}\n\nstatic void\nxfs_buf_ioend(\n\tstruct xfs_buf\t*bp)\n{\n\ttrace_xfs_buf_iodone(bp, _RET_IP_);\n\n\t \n\tif (!bp->b_error && bp->b_io_error)\n\t\txfs_buf_ioerror(bp, bp->b_io_error);\n\n\tif (bp->b_flags & XBF_READ) {\n\t\tif (!bp->b_error && bp->b_ops)\n\t\t\tbp->b_ops->verify_read(bp);\n\t\tif (!bp->b_error)\n\t\t\tbp->b_flags |= XBF_DONE;\n\t} else {\n\t\tif (!bp->b_error) {\n\t\t\tbp->b_flags &= ~XBF_WRITE_FAIL;\n\t\t\tbp->b_flags |= XBF_DONE;\n\t\t}\n\n\t\tif (unlikely(bp->b_error) && xfs_buf_ioend_handle_error(bp))\n\t\t\treturn;\n\n\t\t \n\t\tbp->b_last_error = 0;\n\t\tbp->b_retries = 0;\n\t\tbp->b_first_retry_time = 0;\n\n\t\t \n\t\tif (bp->b_log_item)\n\t\t\txfs_buf_item_done(bp);\n\n\t\tif (bp->b_flags & _XBF_INODES)\n\t\t\txfs_buf_inode_iodone(bp);\n\t\telse if (bp->b_flags & _XBF_DQUOTS)\n\t\t\txfs_buf_dquot_iodone(bp);\n\n\t}\n\n\tbp->b_flags &= ~(XBF_READ | XBF_WRITE | XBF_READ_AHEAD |\n\t\t\t _XBF_LOGRECOVERY);\n\n\tif (bp->b_flags & XBF_ASYNC)\n\t\txfs_buf_relse(bp);\n\telse\n\t\tcomplete(&bp->b_iowait);\n}\n\nstatic void\nxfs_buf_ioend_work(\n\tstruct work_struct\t*work)\n{\n\tstruct xfs_buf\t\t*bp =\n\t\tcontainer_of(work, struct xfs_buf, b_ioend_work);\n\n\txfs_buf_ioend(bp);\n}\n\nstatic void\nxfs_buf_ioend_async(\n\tstruct xfs_buf\t*bp)\n{\n\tINIT_WORK(&bp->b_ioend_work, xfs_buf_ioend_work);\n\tqueue_work(bp->b_mount->m_buf_workqueue, &bp->b_ioend_work);\n}\n\nvoid\n__xfs_buf_ioerror(\n\tstruct xfs_buf\t\t*bp,\n\tint\t\t\terror,\n\txfs_failaddr_t\t\tfailaddr)\n{\n\tASSERT(error <= 0 && error >= -1000);\n\tbp->b_error = error;\n\ttrace_xfs_buf_ioerror(bp, error, failaddr);\n}\n\nvoid\nxfs_buf_ioerror_alert(\n\tstruct xfs_buf\t\t*bp,\n\txfs_failaddr_t\t\tfunc)\n{\n\txfs_buf_alert_ratelimited(bp, \"XFS: metadata IO error\",\n\t\t\"metadata I/O error in \\\"%pS\\\" at daddr 0x%llx len %d error %d\",\n\t\t\t\t  func, (uint64_t)xfs_buf_daddr(bp),\n\t\t\t\t  bp->b_length, -bp->b_error);\n}\n\n \nvoid\nxfs_buf_ioend_fail(\n\tstruct xfs_buf\t*bp)\n{\n\tbp->b_flags &= ~XBF_DONE;\n\txfs_buf_stale(bp);\n\txfs_buf_ioerror(bp, -EIO);\n\txfs_buf_ioend(bp);\n}\n\nint\nxfs_bwrite(\n\tstruct xfs_buf\t\t*bp)\n{\n\tint\t\t\terror;\n\n\tASSERT(xfs_buf_islocked(bp));\n\n\tbp->b_flags |= XBF_WRITE;\n\tbp->b_flags &= ~(XBF_ASYNC | XBF_READ | _XBF_DELWRI_Q |\n\t\t\t XBF_DONE);\n\n\terror = xfs_buf_submit(bp);\n\tif (error)\n\t\txfs_force_shutdown(bp->b_mount, SHUTDOWN_META_IO_ERROR);\n\treturn error;\n}\n\nstatic void\nxfs_buf_bio_end_io(\n\tstruct bio\t\t*bio)\n{\n\tstruct xfs_buf\t\t*bp = (struct xfs_buf *)bio->bi_private;\n\n\tif (!bio->bi_status &&\n\t    (bp->b_flags & XBF_WRITE) && (bp->b_flags & XBF_ASYNC) &&\n\t    XFS_TEST_ERROR(false, bp->b_mount, XFS_ERRTAG_BUF_IOERROR))\n\t\tbio->bi_status = BLK_STS_IOERR;\n\n\t \n\tif (bio->bi_status) {\n\t\tint error = blk_status_to_errno(bio->bi_status);\n\n\t\tcmpxchg(&bp->b_io_error, 0, error);\n\t}\n\n\tif (!bp->b_error && xfs_buf_is_vmapped(bp) && (bp->b_flags & XBF_READ))\n\t\tinvalidate_kernel_vmap_range(bp->b_addr, xfs_buf_vmap_len(bp));\n\n\tif (atomic_dec_and_test(&bp->b_io_remaining) == 1)\n\t\txfs_buf_ioend_async(bp);\n\tbio_put(bio);\n}\n\nstatic void\nxfs_buf_ioapply_map(\n\tstruct xfs_buf\t*bp,\n\tint\t\tmap,\n\tint\t\t*buf_offset,\n\tint\t\t*count,\n\tblk_opf_t\top)\n{\n\tint\t\tpage_index;\n\tunsigned int\ttotal_nr_pages = bp->b_page_count;\n\tint\t\tnr_pages;\n\tstruct bio\t*bio;\n\tsector_t\tsector =  bp->b_maps[map].bm_bn;\n\tint\t\tsize;\n\tint\t\toffset;\n\n\t \n\tpage_index = 0;\n\toffset = *buf_offset;\n\twhile (offset >= PAGE_SIZE) {\n\t\tpage_index++;\n\t\toffset -= PAGE_SIZE;\n\t}\n\n\t \n\tsize = min_t(int, BBTOB(bp->b_maps[map].bm_len), *count);\n\t*count -= size;\n\t*buf_offset += size;\n\nnext_chunk:\n\tatomic_inc(&bp->b_io_remaining);\n\tnr_pages = bio_max_segs(total_nr_pages);\n\n\tbio = bio_alloc(bp->b_target->bt_bdev, nr_pages, op, GFP_NOIO);\n\tbio->bi_iter.bi_sector = sector;\n\tbio->bi_end_io = xfs_buf_bio_end_io;\n\tbio->bi_private = bp;\n\n\tfor (; size && nr_pages; nr_pages--, page_index++) {\n\t\tint\trbytes, nbytes = PAGE_SIZE - offset;\n\n\t\tif (nbytes > size)\n\t\t\tnbytes = size;\n\n\t\trbytes = bio_add_page(bio, bp->b_pages[page_index], nbytes,\n\t\t\t\t      offset);\n\t\tif (rbytes < nbytes)\n\t\t\tbreak;\n\n\t\toffset = 0;\n\t\tsector += BTOBB(nbytes);\n\t\tsize -= nbytes;\n\t\ttotal_nr_pages--;\n\t}\n\n\tif (likely(bio->bi_iter.bi_size)) {\n\t\tif (xfs_buf_is_vmapped(bp)) {\n\t\t\tflush_kernel_vmap_range(bp->b_addr,\n\t\t\t\t\t\txfs_buf_vmap_len(bp));\n\t\t}\n\t\tsubmit_bio(bio);\n\t\tif (size)\n\t\t\tgoto next_chunk;\n\t} else {\n\t\t \n\t\tatomic_dec(&bp->b_io_remaining);\n\t\txfs_buf_ioerror(bp, -EIO);\n\t\tbio_put(bio);\n\t}\n\n}\n\nSTATIC void\n_xfs_buf_ioapply(\n\tstruct xfs_buf\t*bp)\n{\n\tstruct blk_plug\tplug;\n\tblk_opf_t\top;\n\tint\t\toffset;\n\tint\t\tsize;\n\tint\t\ti;\n\n\t \n\tbp->b_error = 0;\n\n\tif (bp->b_flags & XBF_WRITE) {\n\t\top = REQ_OP_WRITE;\n\n\t\t \n\t\tif (bp->b_ops) {\n\t\t\tbp->b_ops->verify_write(bp);\n\t\t\tif (bp->b_error) {\n\t\t\t\txfs_force_shutdown(bp->b_mount,\n\t\t\t\t\t\t   SHUTDOWN_CORRUPT_INCORE);\n\t\t\t\treturn;\n\t\t\t}\n\t\t} else if (bp->b_rhash_key != XFS_BUF_DADDR_NULL) {\n\t\t\tstruct xfs_mount *mp = bp->b_mount;\n\n\t\t\t \n\t\t\tif (xfs_has_crc(mp)) {\n\t\t\t\txfs_warn(mp,\n\t\t\t\t\t\"%s: no buf ops on daddr 0x%llx len %d\",\n\t\t\t\t\t__func__, xfs_buf_daddr(bp),\n\t\t\t\t\tbp->b_length);\n\t\t\t\txfs_hex_dump(bp->b_addr,\n\t\t\t\t\t\tXFS_CORRUPTION_DUMP_LEN);\n\t\t\t\tdump_stack();\n\t\t\t}\n\t\t}\n\t} else {\n\t\top = REQ_OP_READ;\n\t\tif (bp->b_flags & XBF_READ_AHEAD)\n\t\t\top |= REQ_RAHEAD;\n\t}\n\n\t \n\top |= REQ_META;\n\n\t \n\toffset = bp->b_offset;\n\tsize = BBTOB(bp->b_length);\n\tblk_start_plug(&plug);\n\tfor (i = 0; i < bp->b_map_count; i++) {\n\t\txfs_buf_ioapply_map(bp, i, &offset, &size, op);\n\t\tif (bp->b_error)\n\t\t\tbreak;\n\t\tif (size <= 0)\n\t\t\tbreak;\t \n\t}\n\tblk_finish_plug(&plug);\n}\n\n \nstatic int\nxfs_buf_iowait(\n\tstruct xfs_buf\t*bp)\n{\n\tASSERT(!(bp->b_flags & XBF_ASYNC));\n\n\ttrace_xfs_buf_iowait(bp, _RET_IP_);\n\twait_for_completion(&bp->b_iowait);\n\ttrace_xfs_buf_iowait_done(bp, _RET_IP_);\n\n\treturn bp->b_error;\n}\n\n \nstatic int\n__xfs_buf_submit(\n\tstruct xfs_buf\t*bp,\n\tbool\t\twait)\n{\n\tint\t\terror = 0;\n\n\ttrace_xfs_buf_submit(bp, _RET_IP_);\n\n\tASSERT(!(bp->b_flags & _XBF_DELWRI_Q));\n\n\t \n\tif (bp->b_mount->m_log &&\n\t    xlog_is_shutdown(bp->b_mount->m_log)) {\n\t\txfs_buf_ioend_fail(bp);\n\t\treturn -EIO;\n\t}\n\n\t \n\txfs_buf_hold(bp);\n\n\tif (bp->b_flags & XBF_WRITE)\n\t\txfs_buf_wait_unpin(bp);\n\n\t \n\tbp->b_io_error = 0;\n\n\t \n\tatomic_set(&bp->b_io_remaining, 1);\n\tif (bp->b_flags & XBF_ASYNC)\n\t\txfs_buf_ioacct_inc(bp);\n\t_xfs_buf_ioapply(bp);\n\n\t \n\tif (atomic_dec_and_test(&bp->b_io_remaining) == 1) {\n\t\tif (bp->b_error || !(bp->b_flags & XBF_ASYNC))\n\t\t\txfs_buf_ioend(bp);\n\t\telse\n\t\t\txfs_buf_ioend_async(bp);\n\t}\n\n\tif (wait)\n\t\terror = xfs_buf_iowait(bp);\n\n\t \n\txfs_buf_rele(bp);\n\treturn error;\n}\n\nvoid *\nxfs_buf_offset(\n\tstruct xfs_buf\t\t*bp,\n\tsize_t\t\t\toffset)\n{\n\tstruct page\t\t*page;\n\n\tif (bp->b_addr)\n\t\treturn bp->b_addr + offset;\n\n\tpage = bp->b_pages[offset >> PAGE_SHIFT];\n\treturn page_address(page) + (offset & (PAGE_SIZE-1));\n}\n\nvoid\nxfs_buf_zero(\n\tstruct xfs_buf\t\t*bp,\n\tsize_t\t\t\tboff,\n\tsize_t\t\t\tbsize)\n{\n\tsize_t\t\t\tbend;\n\n\tbend = boff + bsize;\n\twhile (boff < bend) {\n\t\tstruct page\t*page;\n\t\tint\t\tpage_index, page_offset, csize;\n\n\t\tpage_index = (boff + bp->b_offset) >> PAGE_SHIFT;\n\t\tpage_offset = (boff + bp->b_offset) & ~PAGE_MASK;\n\t\tpage = bp->b_pages[page_index];\n\t\tcsize = min_t(size_t, PAGE_SIZE - page_offset,\n\t\t\t\t      BBTOB(bp->b_length) - boff);\n\n\t\tASSERT((csize + page_offset) <= PAGE_SIZE);\n\n\t\tmemset(page_address(page) + page_offset, 0, csize);\n\n\t\tboff += csize;\n\t}\n}\n\n \nvoid\n__xfs_buf_mark_corrupt(\n\tstruct xfs_buf\t\t*bp,\n\txfs_failaddr_t\t\tfa)\n{\n\tASSERT(bp->b_flags & XBF_DONE);\n\n\txfs_buf_corruption_error(bp, fa);\n\txfs_buf_stale(bp);\n}\n\n \n\n \nstatic enum lru_status\nxfs_buftarg_drain_rele(\n\tstruct list_head\t*item,\n\tstruct list_lru_one\t*lru,\n\tspinlock_t\t\t*lru_lock,\n\tvoid\t\t\t*arg)\n\n{\n\tstruct xfs_buf\t\t*bp = container_of(item, struct xfs_buf, b_lru);\n\tstruct list_head\t*dispose = arg;\n\n\tif (atomic_read(&bp->b_hold) > 1) {\n\t\t \n\t\ttrace_xfs_buf_drain_buftarg(bp, _RET_IP_);\n\t\treturn LRU_SKIP;\n\t}\n\tif (!spin_trylock(&bp->b_lock))\n\t\treturn LRU_SKIP;\n\n\t \n\tatomic_set(&bp->b_lru_ref, 0);\n\tbp->b_state |= XFS_BSTATE_DISPOSE;\n\tlist_lru_isolate_move(lru, item, dispose);\n\tspin_unlock(&bp->b_lock);\n\treturn LRU_REMOVED;\n}\n\n \nvoid\nxfs_buftarg_wait(\n\tstruct xfs_buftarg\t*btp)\n{\n\t \n\twhile (percpu_counter_sum(&btp->bt_io_count))\n\t\tdelay(100);\n\tflush_workqueue(btp->bt_mount->m_buf_workqueue);\n}\n\nvoid\nxfs_buftarg_drain(\n\tstruct xfs_buftarg\t*btp)\n{\n\tLIST_HEAD(dispose);\n\tint\t\t\tloop = 0;\n\tbool\t\t\twrite_fail = false;\n\n\txfs_buftarg_wait(btp);\n\n\t \n\twhile (list_lru_count(&btp->bt_lru)) {\n\t\tlist_lru_walk(&btp->bt_lru, xfs_buftarg_drain_rele,\n\t\t\t      &dispose, LONG_MAX);\n\n\t\twhile (!list_empty(&dispose)) {\n\t\t\tstruct xfs_buf *bp;\n\t\t\tbp = list_first_entry(&dispose, struct xfs_buf, b_lru);\n\t\t\tlist_del_init(&bp->b_lru);\n\t\t\tif (bp->b_flags & XBF_WRITE_FAIL) {\n\t\t\t\twrite_fail = true;\n\t\t\t\txfs_buf_alert_ratelimited(bp,\n\t\t\t\t\t\"XFS: Corruption Alert\",\n\"Corruption Alert: Buffer at daddr 0x%llx had permanent write failures!\",\n\t\t\t\t\t(long long)xfs_buf_daddr(bp));\n\t\t\t}\n\t\t\txfs_buf_rele(bp);\n\t\t}\n\t\tif (loop++ != 0)\n\t\t\tdelay(100);\n\t}\n\n\t \n\tif (write_fail) {\n\t\tASSERT(xlog_is_shutdown(btp->bt_mount->m_log));\n\t\txfs_alert(btp->bt_mount,\n\t      \"Please run xfs_repair to determine the extent of the problem.\");\n\t}\n}\n\nstatic enum lru_status\nxfs_buftarg_isolate(\n\tstruct list_head\t*item,\n\tstruct list_lru_one\t*lru,\n\tspinlock_t\t\t*lru_lock,\n\tvoid\t\t\t*arg)\n{\n\tstruct xfs_buf\t\t*bp = container_of(item, struct xfs_buf, b_lru);\n\tstruct list_head\t*dispose = arg;\n\n\t \n\tif (!spin_trylock(&bp->b_lock))\n\t\treturn LRU_SKIP;\n\t \n\tif (atomic_add_unless(&bp->b_lru_ref, -1, 0)) {\n\t\tspin_unlock(&bp->b_lock);\n\t\treturn LRU_ROTATE;\n\t}\n\n\tbp->b_state |= XFS_BSTATE_DISPOSE;\n\tlist_lru_isolate_move(lru, item, dispose);\n\tspin_unlock(&bp->b_lock);\n\treturn LRU_REMOVED;\n}\n\nstatic unsigned long\nxfs_buftarg_shrink_scan(\n\tstruct shrinker\t\t*shrink,\n\tstruct shrink_control\t*sc)\n{\n\tstruct xfs_buftarg\t*btp = container_of(shrink,\n\t\t\t\t\tstruct xfs_buftarg, bt_shrinker);\n\tLIST_HEAD(dispose);\n\tunsigned long\t\tfreed;\n\n\tfreed = list_lru_shrink_walk(&btp->bt_lru, sc,\n\t\t\t\t     xfs_buftarg_isolate, &dispose);\n\n\twhile (!list_empty(&dispose)) {\n\t\tstruct xfs_buf *bp;\n\t\tbp = list_first_entry(&dispose, struct xfs_buf, b_lru);\n\t\tlist_del_init(&bp->b_lru);\n\t\txfs_buf_rele(bp);\n\t}\n\n\treturn freed;\n}\n\nstatic unsigned long\nxfs_buftarg_shrink_count(\n\tstruct shrinker\t\t*shrink,\n\tstruct shrink_control\t*sc)\n{\n\tstruct xfs_buftarg\t*btp = container_of(shrink,\n\t\t\t\t\tstruct xfs_buftarg, bt_shrinker);\n\treturn list_lru_shrink_count(&btp->bt_lru, sc);\n}\n\nvoid\nxfs_free_buftarg(\n\tstruct xfs_buftarg\t*btp)\n{\n\tstruct block_device\t*bdev = btp->bt_bdev;\n\n\tunregister_shrinker(&btp->bt_shrinker);\n\tASSERT(percpu_counter_sum(&btp->bt_io_count) == 0);\n\tpercpu_counter_destroy(&btp->bt_io_count);\n\tlist_lru_destroy(&btp->bt_lru);\n\n\tfs_put_dax(btp->bt_daxdev, btp->bt_mount);\n\t \n\tif (bdev != btp->bt_mount->m_super->s_bdev)\n\t\tblkdev_put(bdev, btp->bt_mount->m_super);\n\n\tkmem_free(btp);\n}\n\nint\nxfs_setsize_buftarg(\n\txfs_buftarg_t\t\t*btp,\n\tunsigned int\t\tsectorsize)\n{\n\t \n\tbtp->bt_meta_sectorsize = sectorsize;\n\tbtp->bt_meta_sectormask = sectorsize - 1;\n\n\tif (set_blocksize(btp->bt_bdev, sectorsize)) {\n\t\txfs_warn(btp->bt_mount,\n\t\t\t\"Cannot set_blocksize to %u on device %pg\",\n\t\t\tsectorsize, btp->bt_bdev);\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tbtp->bt_logical_sectorsize = bdev_logical_block_size(btp->bt_bdev);\n\tbtp->bt_logical_sectormask = bdev_logical_block_size(btp->bt_bdev) - 1;\n\n\treturn 0;\n}\n\n \nSTATIC int\nxfs_setsize_buftarg_early(\n\txfs_buftarg_t\t\t*btp,\n\tstruct block_device\t*bdev)\n{\n\treturn xfs_setsize_buftarg(btp, bdev_logical_block_size(bdev));\n}\n\nstruct xfs_buftarg *\nxfs_alloc_buftarg(\n\tstruct xfs_mount\t*mp,\n\tstruct block_device\t*bdev)\n{\n\txfs_buftarg_t\t\t*btp;\n\tconst struct dax_holder_operations *ops = NULL;\n\n#if defined(CONFIG_FS_DAX) && defined(CONFIG_MEMORY_FAILURE)\n\tops = &xfs_dax_holder_operations;\n#endif\n\tbtp = kmem_zalloc(sizeof(*btp), KM_NOFS);\n\n\tbtp->bt_mount = mp;\n\tbtp->bt_dev =  bdev->bd_dev;\n\tbtp->bt_bdev = bdev;\n\tbtp->bt_daxdev = fs_dax_get_by_bdev(bdev, &btp->bt_dax_part_off,\n\t\t\t\t\t    mp, ops);\n\n\t \n\tratelimit_state_init(&btp->bt_ioerror_rl, 30 * HZ,\n\t\t\t     DEFAULT_RATELIMIT_BURST);\n\n\tif (xfs_setsize_buftarg_early(btp, bdev))\n\t\tgoto error_free;\n\n\tif (list_lru_init(&btp->bt_lru))\n\t\tgoto error_free;\n\n\tif (percpu_counter_init(&btp->bt_io_count, 0, GFP_KERNEL))\n\t\tgoto error_lru;\n\n\tbtp->bt_shrinker.count_objects = xfs_buftarg_shrink_count;\n\tbtp->bt_shrinker.scan_objects = xfs_buftarg_shrink_scan;\n\tbtp->bt_shrinker.seeks = DEFAULT_SEEKS;\n\tbtp->bt_shrinker.flags = SHRINKER_NUMA_AWARE;\n\tif (register_shrinker(&btp->bt_shrinker, \"xfs-buf:%s\",\n\t\t\t      mp->m_super->s_id))\n\t\tgoto error_pcpu;\n\treturn btp;\n\nerror_pcpu:\n\tpercpu_counter_destroy(&btp->bt_io_count);\nerror_lru:\n\tlist_lru_destroy(&btp->bt_lru);\nerror_free:\n\tkmem_free(btp);\n\treturn NULL;\n}\n\n \nvoid\nxfs_buf_delwri_cancel(\n\tstruct list_head\t*list)\n{\n\tstruct xfs_buf\t\t*bp;\n\n\twhile (!list_empty(list)) {\n\t\tbp = list_first_entry(list, struct xfs_buf, b_list);\n\n\t\txfs_buf_lock(bp);\n\t\tbp->b_flags &= ~_XBF_DELWRI_Q;\n\t\tlist_del_init(&bp->b_list);\n\t\txfs_buf_relse(bp);\n\t}\n}\n\n \nbool\nxfs_buf_delwri_queue(\n\tstruct xfs_buf\t\t*bp,\n\tstruct list_head\t*list)\n{\n\tASSERT(xfs_buf_islocked(bp));\n\tASSERT(!(bp->b_flags & XBF_READ));\n\n\t \n\tif (bp->b_flags & _XBF_DELWRI_Q) {\n\t\ttrace_xfs_buf_delwri_queued(bp, _RET_IP_);\n\t\treturn false;\n\t}\n\n\ttrace_xfs_buf_delwri_queue(bp, _RET_IP_);\n\n\t \n\tbp->b_flags |= _XBF_DELWRI_Q;\n\tif (list_empty(&bp->b_list)) {\n\t\tatomic_inc(&bp->b_hold);\n\t\tlist_add_tail(&bp->b_list, list);\n\t}\n\n\treturn true;\n}\n\n \nstatic int\nxfs_buf_cmp(\n\tvoid\t\t\t*priv,\n\tconst struct list_head\t*a,\n\tconst struct list_head\t*b)\n{\n\tstruct xfs_buf\t*ap = container_of(a, struct xfs_buf, b_list);\n\tstruct xfs_buf\t*bp = container_of(b, struct xfs_buf, b_list);\n\txfs_daddr_t\t\tdiff;\n\n\tdiff = ap->b_maps[0].bm_bn - bp->b_maps[0].bm_bn;\n\tif (diff < 0)\n\t\treturn -1;\n\tif (diff > 0)\n\t\treturn 1;\n\treturn 0;\n}\n\n \nstatic int\nxfs_buf_delwri_submit_buffers(\n\tstruct list_head\t*buffer_list,\n\tstruct list_head\t*wait_list)\n{\n\tstruct xfs_buf\t\t*bp, *n;\n\tint\t\t\tpinned = 0;\n\tstruct blk_plug\t\tplug;\n\n\tlist_sort(NULL, buffer_list, xfs_buf_cmp);\n\n\tblk_start_plug(&plug);\n\tlist_for_each_entry_safe(bp, n, buffer_list, b_list) {\n\t\tif (!wait_list) {\n\t\t\tif (!xfs_buf_trylock(bp))\n\t\t\t\tcontinue;\n\t\t\tif (xfs_buf_ispinned(bp)) {\n\t\t\t\txfs_buf_unlock(bp);\n\t\t\t\tpinned++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t} else {\n\t\t\txfs_buf_lock(bp);\n\t\t}\n\n\t\t \n\t\tif (!(bp->b_flags & _XBF_DELWRI_Q)) {\n\t\t\tlist_del_init(&bp->b_list);\n\t\t\txfs_buf_relse(bp);\n\t\t\tcontinue;\n\t\t}\n\n\t\ttrace_xfs_buf_delwri_split(bp, _RET_IP_);\n\n\t\t \n\t\tbp->b_flags &= ~_XBF_DELWRI_Q;\n\t\tbp->b_flags |= XBF_WRITE;\n\t\tif (wait_list) {\n\t\t\tbp->b_flags &= ~XBF_ASYNC;\n\t\t\tlist_move_tail(&bp->b_list, wait_list);\n\t\t} else {\n\t\t\tbp->b_flags |= XBF_ASYNC;\n\t\t\tlist_del_init(&bp->b_list);\n\t\t}\n\t\t__xfs_buf_submit(bp, false);\n\t}\n\tblk_finish_plug(&plug);\n\n\treturn pinned;\n}\n\n \nint\nxfs_buf_delwri_submit_nowait(\n\tstruct list_head\t*buffer_list)\n{\n\treturn xfs_buf_delwri_submit_buffers(buffer_list, NULL);\n}\n\n \nint\nxfs_buf_delwri_submit(\n\tstruct list_head\t*buffer_list)\n{\n\tLIST_HEAD\t\t(wait_list);\n\tint\t\t\terror = 0, error2;\n\tstruct xfs_buf\t\t*bp;\n\n\txfs_buf_delwri_submit_buffers(buffer_list, &wait_list);\n\n\t \n\twhile (!list_empty(&wait_list)) {\n\t\tbp = list_first_entry(&wait_list, struct xfs_buf, b_list);\n\n\t\tlist_del_init(&bp->b_list);\n\n\t\t \n\t\terror2 = xfs_buf_iowait(bp);\n\t\txfs_buf_relse(bp);\n\t\tif (!error)\n\t\t\terror = error2;\n\t}\n\n\treturn error;\n}\n\n \nint\nxfs_buf_delwri_pushbuf(\n\tstruct xfs_buf\t\t*bp,\n\tstruct list_head\t*buffer_list)\n{\n\tLIST_HEAD\t\t(submit_list);\n\tint\t\t\terror;\n\n\tASSERT(bp->b_flags & _XBF_DELWRI_Q);\n\n\ttrace_xfs_buf_delwri_pushbuf(bp, _RET_IP_);\n\n\t \n\txfs_buf_lock(bp);\n\tlist_move(&bp->b_list, &submit_list);\n\txfs_buf_unlock(bp);\n\n\t \n\txfs_buf_delwri_submit_buffers(&submit_list, buffer_list);\n\n\t \n\terror = xfs_buf_iowait(bp);\n\tbp->b_flags |= _XBF_DELWRI_Q;\n\txfs_buf_unlock(bp);\n\n\treturn error;\n}\n\nvoid xfs_buf_set_ref(struct xfs_buf *bp, int lru_ref)\n{\n\t \n\tif (XFS_TEST_ERROR(false, bp->b_mount, XFS_ERRTAG_BUF_LRU_REF))\n\t\tlru_ref = 0;\n\n\tatomic_set(&bp->b_lru_ref, lru_ref);\n}\n\n \nbool\nxfs_verify_magic(\n\tstruct xfs_buf\t\t*bp,\n\t__be32\t\t\tdmagic)\n{\n\tstruct xfs_mount\t*mp = bp->b_mount;\n\tint\t\t\tidx;\n\n\tidx = xfs_has_crc(mp);\n\tif (WARN_ON(!bp->b_ops || !bp->b_ops->magic[idx]))\n\t\treturn false;\n\treturn dmagic == bp->b_ops->magic[idx];\n}\n \nbool\nxfs_verify_magic16(\n\tstruct xfs_buf\t\t*bp,\n\t__be16\t\t\tdmagic)\n{\n\tstruct xfs_mount\t*mp = bp->b_mount;\n\tint\t\t\tidx;\n\n\tidx = xfs_has_crc(mp);\n\tif (WARN_ON(!bp->b_ops || !bp->b_ops->magic16[idx]))\n\t\treturn false;\n\treturn dmagic == bp->b_ops->magic16[idx];\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}