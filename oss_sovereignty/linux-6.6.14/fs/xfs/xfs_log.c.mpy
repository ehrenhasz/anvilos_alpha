{
  "module_name": "xfs_log.c",
  "hash_id": "0acf47239913ab35db9f9d45d645e1cdf1745f771f6a3c5dc5644dadd977574e",
  "original_prompt": "Ingested from linux-6.6.14/fs/xfs/xfs_log.c",
  "human_readable_source": "\n \n#include \"xfs.h\"\n#include \"xfs_fs.h\"\n#include \"xfs_shared.h\"\n#include \"xfs_format.h\"\n#include \"xfs_log_format.h\"\n#include \"xfs_trans_resv.h\"\n#include \"xfs_mount.h\"\n#include \"xfs_errortag.h\"\n#include \"xfs_error.h\"\n#include \"xfs_trans.h\"\n#include \"xfs_trans_priv.h\"\n#include \"xfs_log.h\"\n#include \"xfs_log_priv.h\"\n#include \"xfs_trace.h\"\n#include \"xfs_sysfs.h\"\n#include \"xfs_sb.h\"\n#include \"xfs_health.h\"\n\nstruct kmem_cache\t*xfs_log_ticket_cache;\n\n \nSTATIC struct xlog *\nxlog_alloc_log(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_buftarg\t*log_target,\n\txfs_daddr_t\t\tblk_offset,\n\tint\t\t\tnum_bblks);\nSTATIC int\nxlog_space_left(\n\tstruct xlog\t\t*log,\n\tatomic64_t\t\t*head);\nSTATIC void\nxlog_dealloc_log(\n\tstruct xlog\t\t*log);\n\n \nSTATIC void xlog_state_done_syncing(\n\tstruct xlog_in_core\t*iclog);\nSTATIC void xlog_state_do_callback(\n\tstruct xlog\t\t*log);\nSTATIC int\nxlog_state_get_iclog_space(\n\tstruct xlog\t\t*log,\n\tint\t\t\tlen,\n\tstruct xlog_in_core\t**iclog,\n\tstruct xlog_ticket\t*ticket,\n\tint\t\t\t*logoffsetp);\nSTATIC void\nxlog_grant_push_ail(\n\tstruct xlog\t\t*log,\n\tint\t\t\tneed_bytes);\nSTATIC void\nxlog_sync(\n\tstruct xlog\t\t*log,\n\tstruct xlog_in_core\t*iclog,\n\tstruct xlog_ticket\t*ticket);\n#if defined(DEBUG)\nSTATIC void\nxlog_verify_grant_tail(\n\tstruct xlog *log);\nSTATIC void\nxlog_verify_iclog(\n\tstruct xlog\t\t*log,\n\tstruct xlog_in_core\t*iclog,\n\tint\t\t\tcount);\nSTATIC void\nxlog_verify_tail_lsn(\n\tstruct xlog\t\t*log,\n\tstruct xlog_in_core\t*iclog);\n#else\n#define xlog_verify_grant_tail(a)\n#define xlog_verify_iclog(a,b,c)\n#define xlog_verify_tail_lsn(a,b)\n#endif\n\nSTATIC int\nxlog_iclogs_empty(\n\tstruct xlog\t\t*log);\n\nstatic int\nxfs_log_cover(struct xfs_mount *);\n\n \nvoid *\nxlog_prepare_iovec(\n\tstruct xfs_log_vec\t*lv,\n\tstruct xfs_log_iovec\t**vecp,\n\tuint\t\t\ttype)\n{\n\tstruct xfs_log_iovec\t*vec = *vecp;\n\tstruct xlog_op_header\t*oph;\n\tuint32_t\t\tlen;\n\tvoid\t\t\t*buf;\n\n\tif (vec) {\n\t\tASSERT(vec - lv->lv_iovecp < lv->lv_niovecs);\n\t\tvec++;\n\t} else {\n\t\tvec = &lv->lv_iovecp[0];\n\t}\n\n\tlen = lv->lv_buf_len + sizeof(struct xlog_op_header);\n\tif (!IS_ALIGNED(len, sizeof(uint64_t))) {\n\t\tlv->lv_buf_len = round_up(len, sizeof(uint64_t)) -\n\t\t\t\t\tsizeof(struct xlog_op_header);\n\t}\n\n\tvec->i_type = type;\n\tvec->i_addr = lv->lv_buf + lv->lv_buf_len;\n\n\toph = vec->i_addr;\n\toph->oh_clientid = XFS_TRANSACTION;\n\toph->oh_res2 = 0;\n\toph->oh_flags = 0;\n\n\tbuf = vec->i_addr + sizeof(struct xlog_op_header);\n\tASSERT(IS_ALIGNED((unsigned long)buf, sizeof(uint64_t)));\n\n\t*vecp = vec;\n\treturn buf;\n}\n\nstatic void\nxlog_grant_sub_space(\n\tstruct xlog\t\t*log,\n\tatomic64_t\t\t*head,\n\tint\t\t\tbytes)\n{\n\tint64_t\thead_val = atomic64_read(head);\n\tint64_t new, old;\n\n\tdo {\n\t\tint\tcycle, space;\n\n\t\txlog_crack_grant_head_val(head_val, &cycle, &space);\n\n\t\tspace -= bytes;\n\t\tif (space < 0) {\n\t\t\tspace += log->l_logsize;\n\t\t\tcycle--;\n\t\t}\n\n\t\told = head_val;\n\t\tnew = xlog_assign_grant_head_val(cycle, space);\n\t\thead_val = atomic64_cmpxchg(head, old, new);\n\t} while (head_val != old);\n}\n\nstatic void\nxlog_grant_add_space(\n\tstruct xlog\t\t*log,\n\tatomic64_t\t\t*head,\n\tint\t\t\tbytes)\n{\n\tint64_t\thead_val = atomic64_read(head);\n\tint64_t new, old;\n\n\tdo {\n\t\tint\t\ttmp;\n\t\tint\t\tcycle, space;\n\n\t\txlog_crack_grant_head_val(head_val, &cycle, &space);\n\n\t\ttmp = log->l_logsize - space;\n\t\tif (tmp > bytes)\n\t\t\tspace += bytes;\n\t\telse {\n\t\t\tspace = bytes - tmp;\n\t\t\tcycle++;\n\t\t}\n\n\t\told = head_val;\n\t\tnew = xlog_assign_grant_head_val(cycle, space);\n\t\thead_val = atomic64_cmpxchg(head, old, new);\n\t} while (head_val != old);\n}\n\nSTATIC void\nxlog_grant_head_init(\n\tstruct xlog_grant_head\t*head)\n{\n\txlog_assign_grant_head(&head->grant, 1, 0);\n\tINIT_LIST_HEAD(&head->waiters);\n\tspin_lock_init(&head->lock);\n}\n\nSTATIC void\nxlog_grant_head_wake_all(\n\tstruct xlog_grant_head\t*head)\n{\n\tstruct xlog_ticket\t*tic;\n\n\tspin_lock(&head->lock);\n\tlist_for_each_entry(tic, &head->waiters, t_queue)\n\t\twake_up_process(tic->t_task);\n\tspin_unlock(&head->lock);\n}\n\nstatic inline int\nxlog_ticket_reservation(\n\tstruct xlog\t\t*log,\n\tstruct xlog_grant_head\t*head,\n\tstruct xlog_ticket\t*tic)\n{\n\tif (head == &log->l_write_head) {\n\t\tASSERT(tic->t_flags & XLOG_TIC_PERM_RESERV);\n\t\treturn tic->t_unit_res;\n\t}\n\n\tif (tic->t_flags & XLOG_TIC_PERM_RESERV)\n\t\treturn tic->t_unit_res * tic->t_cnt;\n\n\treturn tic->t_unit_res;\n}\n\nSTATIC bool\nxlog_grant_head_wake(\n\tstruct xlog\t\t*log,\n\tstruct xlog_grant_head\t*head,\n\tint\t\t\t*free_bytes)\n{\n\tstruct xlog_ticket\t*tic;\n\tint\t\t\tneed_bytes;\n\tbool\t\t\twoken_task = false;\n\n\tlist_for_each_entry(tic, &head->waiters, t_queue) {\n\n\t\t \n\n\t\tneed_bytes = xlog_ticket_reservation(log, head, tic);\n\t\tif (*free_bytes < need_bytes) {\n\t\t\tif (!woken_task)\n\t\t\t\txlog_grant_push_ail(log, need_bytes);\n\t\t\treturn false;\n\t\t}\n\n\t\t*free_bytes -= need_bytes;\n\t\ttrace_xfs_log_grant_wake_up(log, tic);\n\t\twake_up_process(tic->t_task);\n\t\twoken_task = true;\n\t}\n\n\treturn true;\n}\n\nSTATIC int\nxlog_grant_head_wait(\n\tstruct xlog\t\t*log,\n\tstruct xlog_grant_head\t*head,\n\tstruct xlog_ticket\t*tic,\n\tint\t\t\tneed_bytes) __releases(&head->lock)\n\t\t\t\t\t    __acquires(&head->lock)\n{\n\tlist_add_tail(&tic->t_queue, &head->waiters);\n\n\tdo {\n\t\tif (xlog_is_shutdown(log))\n\t\t\tgoto shutdown;\n\t\txlog_grant_push_ail(log, need_bytes);\n\n\t\t__set_current_state(TASK_UNINTERRUPTIBLE);\n\t\tspin_unlock(&head->lock);\n\n\t\tXFS_STATS_INC(log->l_mp, xs_sleep_logspace);\n\n\t\ttrace_xfs_log_grant_sleep(log, tic);\n\t\tschedule();\n\t\ttrace_xfs_log_grant_wake(log, tic);\n\n\t\tspin_lock(&head->lock);\n\t\tif (xlog_is_shutdown(log))\n\t\t\tgoto shutdown;\n\t} while (xlog_space_left(log, &head->grant) < need_bytes);\n\n\tlist_del_init(&tic->t_queue);\n\treturn 0;\nshutdown:\n\tlist_del_init(&tic->t_queue);\n\treturn -EIO;\n}\n\n \nSTATIC int\nxlog_grant_head_check(\n\tstruct xlog\t\t*log,\n\tstruct xlog_grant_head\t*head,\n\tstruct xlog_ticket\t*tic,\n\tint\t\t\t*need_bytes)\n{\n\tint\t\t\tfree_bytes;\n\tint\t\t\terror = 0;\n\n\tASSERT(!xlog_in_recovery(log));\n\n\t \n\t*need_bytes = xlog_ticket_reservation(log, head, tic);\n\tfree_bytes = xlog_space_left(log, &head->grant);\n\tif (!list_empty_careful(&head->waiters)) {\n\t\tspin_lock(&head->lock);\n\t\tif (!xlog_grant_head_wake(log, head, &free_bytes) ||\n\t\t    free_bytes < *need_bytes) {\n\t\t\terror = xlog_grant_head_wait(log, head, tic,\n\t\t\t\t\t\t     *need_bytes);\n\t\t}\n\t\tspin_unlock(&head->lock);\n\t} else if (free_bytes < *need_bytes) {\n\t\tspin_lock(&head->lock);\n\t\terror = xlog_grant_head_wait(log, head, tic, *need_bytes);\n\t\tspin_unlock(&head->lock);\n\t}\n\n\treturn error;\n}\n\nbool\nxfs_log_writable(\n\tstruct xfs_mount\t*mp)\n{\n\t \n\tif (xfs_has_norecovery(mp))\n\t\treturn false;\n\tif (xfs_readonly_buftarg(mp->m_ddev_targp))\n\t\treturn false;\n\tif (xfs_readonly_buftarg(mp->m_log->l_targ))\n\t\treturn false;\n\tif (xlog_is_shutdown(mp->m_log))\n\t\treturn false;\n\treturn true;\n}\n\n \nint\nxfs_log_regrant(\n\tstruct xfs_mount\t*mp,\n\tstruct xlog_ticket\t*tic)\n{\n\tstruct xlog\t\t*log = mp->m_log;\n\tint\t\t\tneed_bytes;\n\tint\t\t\terror = 0;\n\n\tif (xlog_is_shutdown(log))\n\t\treturn -EIO;\n\n\tXFS_STATS_INC(mp, xs_try_logspace);\n\n\t \n\ttic->t_tid++;\n\n\txlog_grant_push_ail(log, tic->t_unit_res);\n\n\ttic->t_curr_res = tic->t_unit_res;\n\tif (tic->t_cnt > 0)\n\t\treturn 0;\n\n\ttrace_xfs_log_regrant(log, tic);\n\n\terror = xlog_grant_head_check(log, &log->l_write_head, tic,\n\t\t\t\t      &need_bytes);\n\tif (error)\n\t\tgoto out_error;\n\n\txlog_grant_add_space(log, &log->l_write_head.grant, need_bytes);\n\ttrace_xfs_log_regrant_exit(log, tic);\n\txlog_verify_grant_tail(log);\n\treturn 0;\n\nout_error:\n\t \n\ttic->t_curr_res = 0;\n\ttic->t_cnt = 0;\t \n\treturn error;\n}\n\n \nint\nxfs_log_reserve(\n\tstruct xfs_mount\t*mp,\n\tint\t\t\tunit_bytes,\n\tint\t\t\tcnt,\n\tstruct xlog_ticket\t**ticp,\n\tbool\t\t\tpermanent)\n{\n\tstruct xlog\t\t*log = mp->m_log;\n\tstruct xlog_ticket\t*tic;\n\tint\t\t\tneed_bytes;\n\tint\t\t\terror = 0;\n\n\tif (xlog_is_shutdown(log))\n\t\treturn -EIO;\n\n\tXFS_STATS_INC(mp, xs_try_logspace);\n\n\tASSERT(*ticp == NULL);\n\ttic = xlog_ticket_alloc(log, unit_bytes, cnt, permanent);\n\t*ticp = tic;\n\n\txlog_grant_push_ail(log, tic->t_cnt ? tic->t_unit_res * tic->t_cnt\n\t\t\t\t\t    : tic->t_unit_res);\n\n\ttrace_xfs_log_reserve(log, tic);\n\n\terror = xlog_grant_head_check(log, &log->l_reserve_head, tic,\n\t\t\t\t      &need_bytes);\n\tif (error)\n\t\tgoto out_error;\n\n\txlog_grant_add_space(log, &log->l_reserve_head.grant, need_bytes);\n\txlog_grant_add_space(log, &log->l_write_head.grant, need_bytes);\n\ttrace_xfs_log_reserve_exit(log, tic);\n\txlog_verify_grant_tail(log);\n\treturn 0;\n\nout_error:\n\t \n\ttic->t_curr_res = 0;\n\ttic->t_cnt = 0;\t \n\treturn error;\n}\n\n \nstatic void\nxlog_state_shutdown_callbacks(\n\tstruct xlog\t\t*log)\n{\n\tstruct xlog_in_core\t*iclog;\n\tLIST_HEAD(cb_list);\n\n\ticlog = log->l_iclog;\n\tdo {\n\t\tif (atomic_read(&iclog->ic_refcnt)) {\n\t\t\t \n\t\t\tcontinue;\n\t\t}\n\t\tlist_splice_init(&iclog->ic_callbacks, &cb_list);\n\t\tspin_unlock(&log->l_icloglock);\n\n\t\txlog_cil_process_committed(&cb_list);\n\n\t\tspin_lock(&log->l_icloglock);\n\t\twake_up_all(&iclog->ic_write_wait);\n\t\twake_up_all(&iclog->ic_force_wait);\n\t} while ((iclog = iclog->ic_next) != log->l_iclog);\n\n\twake_up_all(&log->l_flush_wait);\n}\n\n \nint\nxlog_state_release_iclog(\n\tstruct xlog\t\t*log,\n\tstruct xlog_in_core\t*iclog,\n\tstruct xlog_ticket\t*ticket)\n{\n\txfs_lsn_t\t\ttail_lsn;\n\tbool\t\t\tlast_ref;\n\n\tlockdep_assert_held(&log->l_icloglock);\n\n\ttrace_xlog_iclog_release(iclog, _RET_IP_);\n\t \n\tif ((iclog->ic_state == XLOG_STATE_WANT_SYNC ||\n\t     (iclog->ic_flags & XLOG_ICL_NEED_FUA)) &&\n\t    !iclog->ic_header.h_tail_lsn) {\n\t\ttail_lsn = xlog_assign_tail_lsn(log->l_mp);\n\t\ticlog->ic_header.h_tail_lsn = cpu_to_be64(tail_lsn);\n\t}\n\n\tlast_ref = atomic_dec_and_test(&iclog->ic_refcnt);\n\n\tif (xlog_is_shutdown(log)) {\n\t\t \n\t\tif (last_ref)\n\t\t\txlog_state_shutdown_callbacks(log);\n\t\treturn -EIO;\n\t}\n\n\tif (!last_ref)\n\t\treturn 0;\n\n\tif (iclog->ic_state != XLOG_STATE_WANT_SYNC) {\n\t\tASSERT(iclog->ic_state == XLOG_STATE_ACTIVE);\n\t\treturn 0;\n\t}\n\n\ticlog->ic_state = XLOG_STATE_SYNCING;\n\txlog_verify_tail_lsn(log, iclog);\n\ttrace_xlog_iclog_syncing(iclog, _RET_IP_);\n\n\tspin_unlock(&log->l_icloglock);\n\txlog_sync(log, iclog, ticket);\n\tspin_lock(&log->l_icloglock);\n\treturn 0;\n}\n\n \nint\nxfs_log_mount(\n\txfs_mount_t\t*mp,\n\txfs_buftarg_t\t*log_target,\n\txfs_daddr_t\tblk_offset,\n\tint\t\tnum_bblks)\n{\n\tstruct xlog\t*log;\n\tint\t\terror = 0;\n\tint\t\tmin_logfsbs;\n\n\tif (!xfs_has_norecovery(mp)) {\n\t\txfs_notice(mp, \"Mounting V%d Filesystem %pU\",\n\t\t\t   XFS_SB_VERSION_NUM(&mp->m_sb),\n\t\t\t   &mp->m_sb.sb_uuid);\n\t} else {\n\t\txfs_notice(mp,\n\"Mounting V%d filesystem %pU in no-recovery mode. Filesystem will be inconsistent.\",\n\t\t\t   XFS_SB_VERSION_NUM(&mp->m_sb),\n\t\t\t   &mp->m_sb.sb_uuid);\n\t\tASSERT(xfs_is_readonly(mp));\n\t}\n\n\tlog = xlog_alloc_log(mp, log_target, blk_offset, num_bblks);\n\tif (IS_ERR(log)) {\n\t\terror = PTR_ERR(log);\n\t\tgoto out;\n\t}\n\tmp->m_log = log;\n\n\t \n\tmin_logfsbs = xfs_log_calc_minimum_size(mp);\n\tif (mp->m_sb.sb_logblocks < min_logfsbs) {\n\t\txfs_warn(mp,\n\t\t\"Log size %d blocks too small, minimum size is %d blocks\",\n\t\t\t mp->m_sb.sb_logblocks, min_logfsbs);\n\n\t\t \n\t\tif (xfs_has_crc(mp)) {\n\t\t\txfs_crit(mp, \"AAIEEE! Log failed size checks. Abort!\");\n\t\t\tASSERT(0);\n\t\t\terror = -EINVAL;\n\t\t\tgoto out_free_log;\n\t\t}\n\t\txfs_crit(mp, \"Log size out of supported range.\");\n\t\txfs_crit(mp,\n\"Continuing onwards, but if log hangs are experienced then please report this message in the bug report.\");\n\t}\n\n\t \n\terror = xfs_trans_ail_init(mp);\n\tif (error) {\n\t\txfs_warn(mp, \"AIL initialisation failed: error %d\", error);\n\t\tgoto out_free_log;\n\t}\n\tlog->l_ailp = mp->m_ail;\n\n\t \n\tif (!xfs_has_norecovery(mp)) {\n\t\terror = xlog_recover(log);\n\t\tif (error) {\n\t\t\txfs_warn(mp, \"log mount/recovery failed: error %d\",\n\t\t\t\terror);\n\t\t\txlog_recover_cancel(log);\n\t\t\tgoto out_destroy_ail;\n\t\t}\n\t}\n\n\terror = xfs_sysfs_init(&log->l_kobj, &xfs_log_ktype, &mp->m_kobj,\n\t\t\t       \"log\");\n\tif (error)\n\t\tgoto out_destroy_ail;\n\n\t \n\tclear_bit(XLOG_ACTIVE_RECOVERY, &log->l_opstate);\n\n\t \n\txlog_cil_init_post_recovery(log);\n\n\treturn 0;\n\nout_destroy_ail:\n\txfs_trans_ail_destroy(mp);\nout_free_log:\n\txlog_dealloc_log(log);\nout:\n\treturn error;\n}\n\n \nint\nxfs_log_mount_finish(\n\tstruct xfs_mount\t*mp)\n{\n\tstruct xlog\t\t*log = mp->m_log;\n\tint\t\t\terror = 0;\n\n\tif (xfs_has_norecovery(mp)) {\n\t\tASSERT(xfs_is_readonly(mp));\n\t\treturn 0;\n\t}\n\n\t \n\tmp->m_super->s_flags |= SB_ACTIVE;\n\txfs_log_work_queue(mp);\n\tif (xlog_recovery_needed(log))\n\t\terror = xlog_recover_finish(log);\n\tmp->m_super->s_flags &= ~SB_ACTIVE;\n\tevict_inodes(mp->m_super);\n\n\t \n\tif (xlog_recovery_needed(log)) {\n\t\tif (!error) {\n\t\t\txfs_log_force(mp, XFS_LOG_SYNC);\n\t\t\txfs_ail_push_all_sync(mp->m_ail);\n\t\t}\n\t\txfs_notice(mp, \"Ending recovery (logdev: %s)\",\n\t\t\t\tmp->m_logname ? mp->m_logname : \"internal\");\n\t} else {\n\t\txfs_info(mp, \"Ending clean mount\");\n\t}\n\txfs_buftarg_drain(mp->m_ddev_targp);\n\n\tclear_bit(XLOG_RECOVERY_NEEDED, &log->l_opstate);\n\n\t \n\tASSERT(!error || xlog_is_shutdown(log));\n\n\treturn error;\n}\n\n \nvoid\nxfs_log_mount_cancel(\n\tstruct xfs_mount\t*mp)\n{\n\txlog_recover_cancel(mp->m_log);\n\txfs_log_unmount(mp);\n}\n\n \nstatic inline int\nxlog_force_iclog(\n\tstruct xlog_in_core\t*iclog)\n{\n\tatomic_inc(&iclog->ic_refcnt);\n\ticlog->ic_flags |= XLOG_ICL_NEED_FLUSH | XLOG_ICL_NEED_FUA;\n\tif (iclog->ic_state == XLOG_STATE_ACTIVE)\n\t\txlog_state_switch_iclogs(iclog->ic_log, iclog, 0);\n\treturn xlog_state_release_iclog(iclog->ic_log, iclog, NULL);\n}\n\n \nstatic void\nxlog_wait_iclog_completion(struct xlog *log)\n{\n\tint\t\ti;\n\tstruct xlog_in_core\t*iclog = log->l_iclog;\n\n\tfor (i = 0; i < log->l_iclog_bufs; i++) {\n\t\tdown(&iclog->ic_sema);\n\t\tup(&iclog->ic_sema);\n\t\ticlog = iclog->ic_next;\n\t}\n}\n\n \nint\nxlog_wait_on_iclog(\n\tstruct xlog_in_core\t*iclog)\n\t\t__releases(iclog->ic_log->l_icloglock)\n{\n\tstruct xlog\t\t*log = iclog->ic_log;\n\n\ttrace_xlog_iclog_wait_on(iclog, _RET_IP_);\n\tif (!xlog_is_shutdown(log) &&\n\t    iclog->ic_state != XLOG_STATE_ACTIVE &&\n\t    iclog->ic_state != XLOG_STATE_DIRTY) {\n\t\tXFS_STATS_INC(log->l_mp, xs_log_force_sleep);\n\t\txlog_wait(&iclog->ic_force_wait, &log->l_icloglock);\n\t} else {\n\t\tspin_unlock(&log->l_icloglock);\n\t}\n\n\tif (xlog_is_shutdown(log))\n\t\treturn -EIO;\n\treturn 0;\n}\n\n \nstatic int\nxlog_write_unmount_record(\n\tstruct xlog\t\t*log,\n\tstruct xlog_ticket\t*ticket)\n{\n\tstruct  {\n\t\tstruct xlog_op_header ophdr;\n\t\tstruct xfs_unmount_log_format ulf;\n\t} unmount_rec = {\n\t\t.ophdr = {\n\t\t\t.oh_clientid = XFS_LOG,\n\t\t\t.oh_tid = cpu_to_be32(ticket->t_tid),\n\t\t\t.oh_flags = XLOG_UNMOUNT_TRANS,\n\t\t},\n\t\t.ulf = {\n\t\t\t.magic = XLOG_UNMOUNT_TYPE,\n\t\t},\n\t};\n\tstruct xfs_log_iovec reg = {\n\t\t.i_addr = &unmount_rec,\n\t\t.i_len = sizeof(unmount_rec),\n\t\t.i_type = XLOG_REG_TYPE_UNMOUNT,\n\t};\n\tstruct xfs_log_vec vec = {\n\t\t.lv_niovecs = 1,\n\t\t.lv_iovecp = &reg,\n\t};\n\tLIST_HEAD(lv_chain);\n\tlist_add(&vec.lv_list, &lv_chain);\n\n\tBUILD_BUG_ON((sizeof(struct xlog_op_header) +\n\t\t      sizeof(struct xfs_unmount_log_format)) !=\n\t\t\t\t\t\t\tsizeof(unmount_rec));\n\n\t \n\tticket->t_curr_res -= sizeof(unmount_rec);\n\n\treturn xlog_write(log, NULL, &lv_chain, ticket, reg.i_len);\n}\n\n \nstatic void\nxlog_unmount_write(\n\tstruct xlog\t\t*log)\n{\n\tstruct xfs_mount\t*mp = log->l_mp;\n\tstruct xlog_in_core\t*iclog;\n\tstruct xlog_ticket\t*tic = NULL;\n\tint\t\t\terror;\n\n\terror = xfs_log_reserve(mp, 600, 1, &tic, 0);\n\tif (error)\n\t\tgoto out_err;\n\n\terror = xlog_write_unmount_record(log, tic);\n\t \nout_err:\n\tif (error)\n\t\txfs_alert(mp, \"%s: unmount record failed\", __func__);\n\n\tspin_lock(&log->l_icloglock);\n\ticlog = log->l_iclog;\n\terror = xlog_force_iclog(iclog);\n\txlog_wait_on_iclog(iclog);\n\n\tif (tic) {\n\t\ttrace_xfs_log_umount_write(log, tic);\n\t\txfs_log_ticket_ungrant(log, tic);\n\t}\n}\n\nstatic void\nxfs_log_unmount_verify_iclog(\n\tstruct xlog\t\t*log)\n{\n\tstruct xlog_in_core\t*iclog = log->l_iclog;\n\n\tdo {\n\t\tASSERT(iclog->ic_state == XLOG_STATE_ACTIVE);\n\t\tASSERT(iclog->ic_offset == 0);\n\t} while ((iclog = iclog->ic_next) != log->l_iclog);\n}\n\n \nstatic void\nxfs_log_unmount_write(\n\tstruct xfs_mount\t*mp)\n{\n\tstruct xlog\t\t*log = mp->m_log;\n\n\tif (!xfs_log_writable(mp))\n\t\treturn;\n\n\txfs_log_force(mp, XFS_LOG_SYNC);\n\n\tif (xlog_is_shutdown(log))\n\t\treturn;\n\n\t \n\tif (XFS_TEST_ERROR(xfs_fs_has_sickness(mp, XFS_SICK_FS_COUNTERS), mp,\n\t\t\tXFS_ERRTAG_FORCE_SUMMARY_RECALC)) {\n\t\txfs_alert(mp, \"%s: will fix summary counters at next mount\",\n\t\t\t\t__func__);\n\t\treturn;\n\t}\n\n\txfs_log_unmount_verify_iclog(log);\n\txlog_unmount_write(log);\n}\n\n \nint\nxfs_log_quiesce(\n\tstruct xfs_mount\t*mp)\n{\n\t \n\tif (xfs_clear_incompat_log_features(mp)) {\n\t\tint error;\n\n\t\terror = xfs_sync_sb(mp, false);\n\t\tif (error)\n\t\t\txfs_warn(mp,\n\t\"Failed to clear log incompat features on quiesce\");\n\t}\n\n\tcancel_delayed_work_sync(&mp->m_log->l_work);\n\txfs_log_force(mp, XFS_LOG_SYNC);\n\n\t \n\txfs_ail_push_all_sync(mp->m_ail);\n\txfs_buftarg_wait(mp->m_ddev_targp);\n\txfs_buf_lock(mp->m_sb_bp);\n\txfs_buf_unlock(mp->m_sb_bp);\n\n\treturn xfs_log_cover(mp);\n}\n\nvoid\nxfs_log_clean(\n\tstruct xfs_mount\t*mp)\n{\n\txfs_log_quiesce(mp);\n\txfs_log_unmount_write(mp);\n}\n\n \nvoid\nxfs_log_unmount(\n\tstruct xfs_mount\t*mp)\n{\n\txfs_log_clean(mp);\n\n\t \n\txlog_wait_iclog_completion(mp->m_log);\n\n\txfs_buftarg_drain(mp->m_ddev_targp);\n\n\txfs_trans_ail_destroy(mp);\n\n\txfs_sysfs_del(&mp->m_log->l_kobj);\n\n\txlog_dealloc_log(mp->m_log);\n}\n\nvoid\nxfs_log_item_init(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_log_item\t*item,\n\tint\t\t\ttype,\n\tconst struct xfs_item_ops *ops)\n{\n\titem->li_log = mp->m_log;\n\titem->li_ailp = mp->m_ail;\n\titem->li_type = type;\n\titem->li_ops = ops;\n\titem->li_lv = NULL;\n\n\tINIT_LIST_HEAD(&item->li_ail);\n\tINIT_LIST_HEAD(&item->li_cil);\n\tINIT_LIST_HEAD(&item->li_bio_list);\n\tINIT_LIST_HEAD(&item->li_trans);\n}\n\n \nvoid\nxfs_log_space_wake(\n\tstruct xfs_mount\t*mp)\n{\n\tstruct xlog\t\t*log = mp->m_log;\n\tint\t\t\tfree_bytes;\n\n\tif (xlog_is_shutdown(log))\n\t\treturn;\n\n\tif (!list_empty_careful(&log->l_write_head.waiters)) {\n\t\tASSERT(!xlog_in_recovery(log));\n\n\t\tspin_lock(&log->l_write_head.lock);\n\t\tfree_bytes = xlog_space_left(log, &log->l_write_head.grant);\n\t\txlog_grant_head_wake(log, &log->l_write_head, &free_bytes);\n\t\tspin_unlock(&log->l_write_head.lock);\n\t}\n\n\tif (!list_empty_careful(&log->l_reserve_head.waiters)) {\n\t\tASSERT(!xlog_in_recovery(log));\n\n\t\tspin_lock(&log->l_reserve_head.lock);\n\t\tfree_bytes = xlog_space_left(log, &log->l_reserve_head.grant);\n\t\txlog_grant_head_wake(log, &log->l_reserve_head, &free_bytes);\n\t\tspin_unlock(&log->l_reserve_head.lock);\n\t}\n}\n\n \nstatic bool\nxfs_log_need_covered(\n\tstruct xfs_mount\t*mp)\n{\n\tstruct xlog\t\t*log = mp->m_log;\n\tbool\t\t\tneeded = false;\n\n\tif (!xlog_cil_empty(log))\n\t\treturn false;\n\n\tspin_lock(&log->l_icloglock);\n\tswitch (log->l_covered_state) {\n\tcase XLOG_STATE_COVER_DONE:\n\tcase XLOG_STATE_COVER_DONE2:\n\tcase XLOG_STATE_COVER_IDLE:\n\t\tbreak;\n\tcase XLOG_STATE_COVER_NEED:\n\tcase XLOG_STATE_COVER_NEED2:\n\t\tif (xfs_ail_min_lsn(log->l_ailp))\n\t\t\tbreak;\n\t\tif (!xlog_iclogs_empty(log))\n\t\t\tbreak;\n\n\t\tneeded = true;\n\t\tif (log->l_covered_state == XLOG_STATE_COVER_NEED)\n\t\t\tlog->l_covered_state = XLOG_STATE_COVER_DONE;\n\t\telse\n\t\t\tlog->l_covered_state = XLOG_STATE_COVER_DONE2;\n\t\tbreak;\n\tdefault:\n\t\tneeded = true;\n\t\tbreak;\n\t}\n\tspin_unlock(&log->l_icloglock);\n\treturn needed;\n}\n\n \nstatic int\nxfs_log_cover(\n\tstruct xfs_mount\t*mp)\n{\n\tint\t\t\terror = 0;\n\tbool\t\t\tneed_covered;\n\n\tASSERT((xlog_cil_empty(mp->m_log) && xlog_iclogs_empty(mp->m_log) &&\n\t        !xfs_ail_min_lsn(mp->m_log->l_ailp)) ||\n\t\txlog_is_shutdown(mp->m_log));\n\n\tif (!xfs_log_writable(mp))\n\t\treturn 0;\n\n\t \n\tneed_covered = xfs_log_need_covered(mp);\n\tif (!need_covered && !xfs_has_lazysbcount(mp))\n\t\treturn 0;\n\n\t \n\tdo {\n\t\terror = xfs_sync_sb(mp, true);\n\t\tif (error)\n\t\t\tbreak;\n\t\txfs_ail_push_all_sync(mp->m_ail);\n\t} while (xfs_log_need_covered(mp));\n\n\treturn error;\n}\n\n \nxfs_lsn_t\nxlog_assign_tail_lsn_locked(\n\tstruct xfs_mount\t*mp)\n{\n\tstruct xlog\t\t*log = mp->m_log;\n\tstruct xfs_log_item\t*lip;\n\txfs_lsn_t\t\ttail_lsn;\n\n\tassert_spin_locked(&mp->m_ail->ail_lock);\n\n\t \n\tlip = xfs_ail_min(mp->m_ail);\n\tif (lip)\n\t\ttail_lsn = lip->li_lsn;\n\telse\n\t\ttail_lsn = atomic64_read(&log->l_last_sync_lsn);\n\ttrace_xfs_log_assign_tail_lsn(log, tail_lsn);\n\tatomic64_set(&log->l_tail_lsn, tail_lsn);\n\treturn tail_lsn;\n}\n\nxfs_lsn_t\nxlog_assign_tail_lsn(\n\tstruct xfs_mount\t*mp)\n{\n\txfs_lsn_t\t\ttail_lsn;\n\n\tspin_lock(&mp->m_ail->ail_lock);\n\ttail_lsn = xlog_assign_tail_lsn_locked(mp);\n\tspin_unlock(&mp->m_ail->ail_lock);\n\n\treturn tail_lsn;\n}\n\n \nSTATIC int\nxlog_space_left(\n\tstruct xlog\t*log,\n\tatomic64_t\t*head)\n{\n\tint\t\ttail_bytes;\n\tint\t\ttail_cycle;\n\tint\t\thead_cycle;\n\tint\t\thead_bytes;\n\n\txlog_crack_grant_head(head, &head_cycle, &head_bytes);\n\txlog_crack_atomic_lsn(&log->l_tail_lsn, &tail_cycle, &tail_bytes);\n\ttail_bytes = BBTOB(tail_bytes);\n\tif (tail_cycle == head_cycle && head_bytes >= tail_bytes)\n\t\treturn log->l_logsize - (head_bytes - tail_bytes);\n\tif (tail_cycle + 1 < head_cycle)\n\t\treturn 0;\n\n\t \n\tif (xlog_is_shutdown(log))\n\t\treturn log->l_logsize;\n\n\tif (tail_cycle < head_cycle) {\n\t\tASSERT(tail_cycle == (head_cycle - 1));\n\t\treturn tail_bytes - head_bytes;\n\t}\n\n\t \n\txfs_alert(log->l_mp, \"xlog_space_left: head behind tail\");\n\txfs_alert(log->l_mp, \"  tail_cycle = %d, tail_bytes = %d\",\n\t\t  tail_cycle, tail_bytes);\n\txfs_alert(log->l_mp, \"  GH   cycle = %d, GH   bytes = %d\",\n\t\t  head_cycle, head_bytes);\n\tASSERT(0);\n\treturn log->l_logsize;\n}\n\n\nstatic void\nxlog_ioend_work(\n\tstruct work_struct\t*work)\n{\n\tstruct xlog_in_core     *iclog =\n\t\tcontainer_of(work, struct xlog_in_core, ic_end_io_work);\n\tstruct xlog\t\t*log = iclog->ic_log;\n\tint\t\t\terror;\n\n\terror = blk_status_to_errno(iclog->ic_bio.bi_status);\n#ifdef DEBUG\n\t \n\tif (iclog->ic_fail_crc)\n\t\terror = -EIO;\n#endif\n\n\t \n\tif (XFS_TEST_ERROR(error, log->l_mp, XFS_ERRTAG_IODONE_IOERR)) {\n\t\txfs_alert(log->l_mp, \"log I/O error %d\", error);\n\t\txlog_force_shutdown(log, SHUTDOWN_LOG_IO_ERROR);\n\t}\n\n\txlog_state_done_syncing(iclog);\n\tbio_uninit(&iclog->ic_bio);\n\n\t \n\tup(&iclog->ic_sema);\n}\n\n \nSTATIC void\nxlog_get_iclog_buffer_size(\n\tstruct xfs_mount\t*mp,\n\tstruct xlog\t\t*log)\n{\n\tif (mp->m_logbufs <= 0)\n\t\tmp->m_logbufs = XLOG_MAX_ICLOGS;\n\tif (mp->m_logbsize <= 0)\n\t\tmp->m_logbsize = XLOG_BIG_RECORD_BSIZE;\n\n\tlog->l_iclog_bufs = mp->m_logbufs;\n\tlog->l_iclog_size = mp->m_logbsize;\n\n\t \n\tlog->l_iclog_heads =\n\t\tDIV_ROUND_UP(mp->m_logbsize, XLOG_HEADER_CYCLE_SIZE);\n\tlog->l_iclog_hsize = log->l_iclog_heads << BBSHIFT;\n}\n\nvoid\nxfs_log_work_queue(\n\tstruct xfs_mount        *mp)\n{\n\tqueue_delayed_work(mp->m_sync_workqueue, &mp->m_log->l_work,\n\t\t\t\tmsecs_to_jiffies(xfs_syncd_centisecs * 10));\n}\n\n \nstatic inline void\nxlog_clear_incompat(\n\tstruct xlog\t\t*log)\n{\n\tstruct xfs_mount\t*mp = log->l_mp;\n\n\tif (!xfs_sb_has_incompat_log_feature(&mp->m_sb,\n\t\t\t\tXFS_SB_FEAT_INCOMPAT_LOG_ALL))\n\t\treturn;\n\n\tif (log->l_covered_state != XLOG_STATE_COVER_DONE2)\n\t\treturn;\n\n\tif (!down_write_trylock(&log->l_incompat_users))\n\t\treturn;\n\n\txfs_clear_incompat_log_features(mp);\n\tup_write(&log->l_incompat_users);\n}\n\n \nstatic void\nxfs_log_worker(\n\tstruct work_struct\t*work)\n{\n\tstruct xlog\t\t*log = container_of(to_delayed_work(work),\n\t\t\t\t\t\tstruct xlog, l_work);\n\tstruct xfs_mount\t*mp = log->l_mp;\n\n\t \n\tif (xfs_fs_writable(mp, SB_FREEZE_WRITE) && xfs_log_need_covered(mp)) {\n\t\t \n\t\txlog_clear_incompat(log);\n\t\txfs_sync_sb(mp, true);\n\t} else\n\t\txfs_log_force(mp, 0);\n\n\t \n\txfs_ail_push_all(mp->m_ail);\n\n\t \n\txfs_log_work_queue(mp);\n}\n\n \nSTATIC struct xlog *\nxlog_alloc_log(\n\tstruct xfs_mount\t*mp,\n\tstruct xfs_buftarg\t*log_target,\n\txfs_daddr_t\t\tblk_offset,\n\tint\t\t\tnum_bblks)\n{\n\tstruct xlog\t\t*log;\n\txlog_rec_header_t\t*head;\n\txlog_in_core_t\t\t**iclogp;\n\txlog_in_core_t\t\t*iclog, *prev_iclog=NULL;\n\tint\t\t\ti;\n\tint\t\t\terror = -ENOMEM;\n\tuint\t\t\tlog2_size = 0;\n\n\tlog = kmem_zalloc(sizeof(struct xlog), KM_MAYFAIL);\n\tif (!log) {\n\t\txfs_warn(mp, \"Log allocation failed: No memory!\");\n\t\tgoto out;\n\t}\n\n\tlog->l_mp\t   = mp;\n\tlog->l_targ\t   = log_target;\n\tlog->l_logsize     = BBTOB(num_bblks);\n\tlog->l_logBBstart  = blk_offset;\n\tlog->l_logBBsize   = num_bblks;\n\tlog->l_covered_state = XLOG_STATE_COVER_IDLE;\n\tset_bit(XLOG_ACTIVE_RECOVERY, &log->l_opstate);\n\tINIT_DELAYED_WORK(&log->l_work, xfs_log_worker);\n\n\tlog->l_prev_block  = -1;\n\t \n\txlog_assign_atomic_lsn(&log->l_tail_lsn, 1, 0);\n\txlog_assign_atomic_lsn(&log->l_last_sync_lsn, 1, 0);\n\tlog->l_curr_cycle  = 1;\t     \n\n\tif (xfs_has_logv2(mp) && mp->m_sb.sb_logsunit > 1)\n\t\tlog->l_iclog_roundoff = mp->m_sb.sb_logsunit;\n\telse\n\t\tlog->l_iclog_roundoff = BBSIZE;\n\n\txlog_grant_head_init(&log->l_reserve_head);\n\txlog_grant_head_init(&log->l_write_head);\n\n\terror = -EFSCORRUPTED;\n\tif (xfs_has_sector(mp)) {\n\t        log2_size = mp->m_sb.sb_logsectlog;\n\t\tif (log2_size < BBSHIFT) {\n\t\t\txfs_warn(mp, \"Log sector size too small (0x%x < 0x%x)\",\n\t\t\t\tlog2_size, BBSHIFT);\n\t\t\tgoto out_free_log;\n\t\t}\n\n\t        log2_size -= BBSHIFT;\n\t\tif (log2_size > mp->m_sectbb_log) {\n\t\t\txfs_warn(mp, \"Log sector size too large (0x%x > 0x%x)\",\n\t\t\t\tlog2_size, mp->m_sectbb_log);\n\t\t\tgoto out_free_log;\n\t\t}\n\n\t\t \n\t\tif (log2_size && log->l_logBBstart > 0 &&\n\t\t\t    !xfs_has_logv2(mp)) {\n\t\t\txfs_warn(mp,\n\t\t\"log sector size (0x%x) invalid for configuration.\",\n\t\t\t\tlog2_size);\n\t\t\tgoto out_free_log;\n\t\t}\n\t}\n\tlog->l_sectBBsize = 1 << log2_size;\n\n\tinit_rwsem(&log->l_incompat_users);\n\n\txlog_get_iclog_buffer_size(mp, log);\n\n\tspin_lock_init(&log->l_icloglock);\n\tinit_waitqueue_head(&log->l_flush_wait);\n\n\ticlogp = &log->l_iclog;\n\t \n\tASSERT(log->l_iclog_size >= 4096);\n\tfor (i = 0; i < log->l_iclog_bufs; i++) {\n\t\tsize_t bvec_size = howmany(log->l_iclog_size, PAGE_SIZE) *\n\t\t\t\tsizeof(struct bio_vec);\n\n\t\ticlog = kmem_zalloc(sizeof(*iclog) + bvec_size, KM_MAYFAIL);\n\t\tif (!iclog)\n\t\t\tgoto out_free_iclog;\n\n\t\t*iclogp = iclog;\n\t\ticlog->ic_prev = prev_iclog;\n\t\tprev_iclog = iclog;\n\n\t\ticlog->ic_data = kvzalloc(log->l_iclog_size,\n\t\t\t\tGFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\t\tif (!iclog->ic_data)\n\t\t\tgoto out_free_iclog;\n\t\thead = &iclog->ic_header;\n\t\tmemset(head, 0, sizeof(xlog_rec_header_t));\n\t\thead->h_magicno = cpu_to_be32(XLOG_HEADER_MAGIC_NUM);\n\t\thead->h_version = cpu_to_be32(\n\t\t\txfs_has_logv2(log->l_mp) ? 2 : 1);\n\t\thead->h_size = cpu_to_be32(log->l_iclog_size);\n\t\t \n\t\thead->h_fmt = cpu_to_be32(XLOG_FMT);\n\t\tmemcpy(&head->h_fs_uuid, &mp->m_sb.sb_uuid, sizeof(uuid_t));\n\n\t\ticlog->ic_size = log->l_iclog_size - log->l_iclog_hsize;\n\t\ticlog->ic_state = XLOG_STATE_ACTIVE;\n\t\ticlog->ic_log = log;\n\t\tatomic_set(&iclog->ic_refcnt, 0);\n\t\tINIT_LIST_HEAD(&iclog->ic_callbacks);\n\t\ticlog->ic_datap = (void *)iclog->ic_data + log->l_iclog_hsize;\n\n\t\tinit_waitqueue_head(&iclog->ic_force_wait);\n\t\tinit_waitqueue_head(&iclog->ic_write_wait);\n\t\tINIT_WORK(&iclog->ic_end_io_work, xlog_ioend_work);\n\t\tsema_init(&iclog->ic_sema, 1);\n\n\t\ticlogp = &iclog->ic_next;\n\t}\n\t*iclogp = log->l_iclog;\t\t\t \n\tlog->l_iclog->ic_prev = prev_iclog;\t \n\n\tlog->l_ioend_workqueue = alloc_workqueue(\"xfs-log/%s\",\n\t\t\tXFS_WQFLAGS(WQ_FREEZABLE | WQ_MEM_RECLAIM |\n\t\t\t\t    WQ_HIGHPRI),\n\t\t\t0, mp->m_super->s_id);\n\tif (!log->l_ioend_workqueue)\n\t\tgoto out_free_iclog;\n\n\terror = xlog_cil_init(log);\n\tif (error)\n\t\tgoto out_destroy_workqueue;\n\treturn log;\n\nout_destroy_workqueue:\n\tdestroy_workqueue(log->l_ioend_workqueue);\nout_free_iclog:\n\tfor (iclog = log->l_iclog; iclog; iclog = prev_iclog) {\n\t\tprev_iclog = iclog->ic_next;\n\t\tkmem_free(iclog->ic_data);\n\t\tkmem_free(iclog);\n\t\tif (prev_iclog == log->l_iclog)\n\t\t\tbreak;\n\t}\nout_free_log:\n\tkmem_free(log);\nout:\n\treturn ERR_PTR(error);\n}\t \n\n \nxfs_lsn_t\nxlog_grant_push_threshold(\n\tstruct xlog\t*log,\n\tint\t\tneed_bytes)\n{\n\txfs_lsn_t\tthreshold_lsn = 0;\n\txfs_lsn_t\tlast_sync_lsn;\n\tint\t\tfree_blocks;\n\tint\t\tfree_bytes;\n\tint\t\tthreshold_block;\n\tint\t\tthreshold_cycle;\n\tint\t\tfree_threshold;\n\n\tASSERT(BTOBB(need_bytes) < log->l_logBBsize);\n\n\tfree_bytes = xlog_space_left(log, &log->l_reserve_head.grant);\n\tfree_blocks = BTOBBT(free_bytes);\n\n\t \n\tfree_threshold = BTOBB(need_bytes);\n\tfree_threshold = max(free_threshold, (log->l_logBBsize >> 2));\n\tfree_threshold = max(free_threshold, 256);\n\tif (free_blocks >= free_threshold)\n\t\treturn NULLCOMMITLSN;\n\n\txlog_crack_atomic_lsn(&log->l_tail_lsn, &threshold_cycle,\n\t\t\t\t\t\t&threshold_block);\n\tthreshold_block += free_threshold;\n\tif (threshold_block >= log->l_logBBsize) {\n\t\tthreshold_block -= log->l_logBBsize;\n\t\tthreshold_cycle += 1;\n\t}\n\tthreshold_lsn = xlog_assign_lsn(threshold_cycle,\n\t\t\t\t\tthreshold_block);\n\t \n\tlast_sync_lsn = atomic64_read(&log->l_last_sync_lsn);\n\tif (XFS_LSN_CMP(threshold_lsn, last_sync_lsn) > 0)\n\t\tthreshold_lsn = last_sync_lsn;\n\n\treturn threshold_lsn;\n}\n\n \nSTATIC void\nxlog_grant_push_ail(\n\tstruct xlog\t*log,\n\tint\t\tneed_bytes)\n{\n\txfs_lsn_t\tthreshold_lsn;\n\n\tthreshold_lsn = xlog_grant_push_threshold(log, need_bytes);\n\tif (threshold_lsn == NULLCOMMITLSN || xlog_is_shutdown(log))\n\t\treturn;\n\n\t \n\txfs_ail_push(log->l_ailp, threshold_lsn);\n}\n\n \nSTATIC void\nxlog_pack_data(\n\tstruct xlog\t\t*log,\n\tstruct xlog_in_core\t*iclog,\n\tint\t\t\troundoff)\n{\n\tint\t\t\ti, j, k;\n\tint\t\t\tsize = iclog->ic_offset + roundoff;\n\t__be32\t\t\tcycle_lsn;\n\tchar\t\t\t*dp;\n\n\tcycle_lsn = CYCLE_LSN_DISK(iclog->ic_header.h_lsn);\n\n\tdp = iclog->ic_datap;\n\tfor (i = 0; i < BTOBB(size); i++) {\n\t\tif (i >= (XLOG_HEADER_CYCLE_SIZE / BBSIZE))\n\t\t\tbreak;\n\t\ticlog->ic_header.h_cycle_data[i] = *(__be32 *)dp;\n\t\t*(__be32 *)dp = cycle_lsn;\n\t\tdp += BBSIZE;\n\t}\n\n\tif (xfs_has_logv2(log->l_mp)) {\n\t\txlog_in_core_2_t *xhdr = iclog->ic_data;\n\n\t\tfor ( ; i < BTOBB(size); i++) {\n\t\t\tj = i / (XLOG_HEADER_CYCLE_SIZE / BBSIZE);\n\t\t\tk = i % (XLOG_HEADER_CYCLE_SIZE / BBSIZE);\n\t\t\txhdr[j].hic_xheader.xh_cycle_data[k] = *(__be32 *)dp;\n\t\t\t*(__be32 *)dp = cycle_lsn;\n\t\t\tdp += BBSIZE;\n\t\t}\n\n\t\tfor (i = 1; i < log->l_iclog_heads; i++)\n\t\t\txhdr[i].hic_xheader.xh_cycle = cycle_lsn;\n\t}\n}\n\n \n__le32\nxlog_cksum(\n\tstruct xlog\t\t*log,\n\tstruct xlog_rec_header\t*rhead,\n\tchar\t\t\t*dp,\n\tint\t\t\tsize)\n{\n\tuint32_t\t\tcrc;\n\n\t \n\tcrc = xfs_start_cksum_update((char *)rhead,\n\t\t\t      sizeof(struct xlog_rec_header),\n\t\t\t      offsetof(struct xlog_rec_header, h_crc));\n\n\t \n\tif (xfs_has_logv2(log->l_mp)) {\n\t\tunion xlog_in_core2 *xhdr = (union xlog_in_core2 *)rhead;\n\t\tint\t\ti;\n\t\tint\t\txheads;\n\n\t\txheads = DIV_ROUND_UP(size, XLOG_HEADER_CYCLE_SIZE);\n\n\t\tfor (i = 1; i < xheads; i++) {\n\t\t\tcrc = crc32c(crc, &xhdr[i].hic_xheader,\n\t\t\t\t     sizeof(struct xlog_rec_ext_header));\n\t\t}\n\t}\n\n\t \n\tcrc = crc32c(crc, dp, size);\n\n\treturn xfs_end_cksum(crc);\n}\n\nstatic void\nxlog_bio_end_io(\n\tstruct bio\t\t*bio)\n{\n\tstruct xlog_in_core\t*iclog = bio->bi_private;\n\n\tqueue_work(iclog->ic_log->l_ioend_workqueue,\n\t\t   &iclog->ic_end_io_work);\n}\n\nstatic int\nxlog_map_iclog_data(\n\tstruct bio\t\t*bio,\n\tvoid\t\t\t*data,\n\tsize_t\t\t\tcount)\n{\n\tdo {\n\t\tstruct page\t*page = kmem_to_page(data);\n\t\tunsigned int\toff = offset_in_page(data);\n\t\tsize_t\t\tlen = min_t(size_t, count, PAGE_SIZE - off);\n\n\t\tif (bio_add_page(bio, page, len, off) != len)\n\t\t\treturn -EIO;\n\n\t\tdata += len;\n\t\tcount -= len;\n\t} while (count);\n\n\treturn 0;\n}\n\nSTATIC void\nxlog_write_iclog(\n\tstruct xlog\t\t*log,\n\tstruct xlog_in_core\t*iclog,\n\tuint64_t\t\tbno,\n\tunsigned int\t\tcount)\n{\n\tASSERT(bno < log->l_logBBsize);\n\ttrace_xlog_iclog_write(iclog, _RET_IP_);\n\n\t \n\tdown(&iclog->ic_sema);\n\tif (xlog_is_shutdown(log)) {\n\t\t \n\t\txlog_state_done_syncing(iclog);\n\t\tup(&iclog->ic_sema);\n\t\treturn;\n\t}\n\n\t \n\tbio_init(&iclog->ic_bio, log->l_targ->bt_bdev, iclog->ic_bvec,\n\t\t howmany(count, PAGE_SIZE),\n\t\t REQ_OP_WRITE | REQ_META | REQ_SYNC | REQ_IDLE);\n\ticlog->ic_bio.bi_iter.bi_sector = log->l_logBBstart + bno;\n\ticlog->ic_bio.bi_end_io = xlog_bio_end_io;\n\ticlog->ic_bio.bi_private = iclog;\n\n\tif (iclog->ic_flags & XLOG_ICL_NEED_FLUSH) {\n\t\ticlog->ic_bio.bi_opf |= REQ_PREFLUSH;\n\t\t \n\t\tif (log->l_targ != log->l_mp->m_ddev_targp &&\n\t\t    blkdev_issue_flush(log->l_mp->m_ddev_targp->bt_bdev)) {\n\t\t\txlog_force_shutdown(log, SHUTDOWN_LOG_IO_ERROR);\n\t\t\treturn;\n\t\t}\n\t}\n\tif (iclog->ic_flags & XLOG_ICL_NEED_FUA)\n\t\ticlog->ic_bio.bi_opf |= REQ_FUA;\n\n\ticlog->ic_flags &= ~(XLOG_ICL_NEED_FLUSH | XLOG_ICL_NEED_FUA);\n\n\tif (xlog_map_iclog_data(&iclog->ic_bio, iclog->ic_data, count)) {\n\t\txlog_force_shutdown(log, SHUTDOWN_LOG_IO_ERROR);\n\t\treturn;\n\t}\n\tif (is_vmalloc_addr(iclog->ic_data))\n\t\tflush_kernel_vmap_range(iclog->ic_data, count);\n\n\t \n\tif (bno + BTOBB(count) > log->l_logBBsize) {\n\t\tstruct bio *split;\n\n\t\tsplit = bio_split(&iclog->ic_bio, log->l_logBBsize - bno,\n\t\t\t\t  GFP_NOIO, &fs_bio_set);\n\t\tbio_chain(split, &iclog->ic_bio);\n\t\tsubmit_bio(split);\n\n\t\t \n\t\ticlog->ic_bio.bi_iter.bi_sector = log->l_logBBstart;\n\t}\n\n\tsubmit_bio(&iclog->ic_bio);\n}\n\n \nstatic void\nxlog_split_iclog(\n\tstruct xlog\t\t*log,\n\tvoid\t\t\t*data,\n\tuint64_t\t\tbno,\n\tunsigned int\t\tcount)\n{\n\tunsigned int\t\tsplit_offset = BBTOB(log->l_logBBsize - bno);\n\tunsigned int\t\ti;\n\n\tfor (i = split_offset; i < count; i += BBSIZE) {\n\t\tuint32_t cycle = get_unaligned_be32(data + i);\n\n\t\tif (++cycle == XLOG_HEADER_MAGIC_NUM)\n\t\t\tcycle++;\n\t\tput_unaligned_be32(cycle, data + i);\n\t}\n}\n\nstatic int\nxlog_calc_iclog_size(\n\tstruct xlog\t\t*log,\n\tstruct xlog_in_core\t*iclog,\n\tuint32_t\t\t*roundoff)\n{\n\tuint32_t\t\tcount_init, count;\n\n\t \n\tcount_init = log->l_iclog_hsize + iclog->ic_offset;\n\tcount = roundup(count_init, log->l_iclog_roundoff);\n\n\t*roundoff = count - count_init;\n\n\tASSERT(count >= count_init);\n\tASSERT(*roundoff < log->l_iclog_roundoff);\n\treturn count;\n}\n\n \nSTATIC void\nxlog_sync(\n\tstruct xlog\t\t*log,\n\tstruct xlog_in_core\t*iclog,\n\tstruct xlog_ticket\t*ticket)\n{\n\tunsigned int\t\tcount;\t\t \n\tunsigned int\t\troundoff;        \n\tuint64_t\t\tbno;\n\tunsigned int\t\tsize;\n\n\tASSERT(atomic_read(&iclog->ic_refcnt) == 0);\n\ttrace_xlog_iclog_sync(iclog, _RET_IP_);\n\n\tcount = xlog_calc_iclog_size(log, iclog, &roundoff);\n\n\t \n\tif (ticket) {\n\t\tticket->t_curr_res -= roundoff;\n\t} else {\n\t\txlog_grant_add_space(log, &log->l_reserve_head.grant, roundoff);\n\t\txlog_grant_add_space(log, &log->l_write_head.grant, roundoff);\n\t}\n\n\t \n\txlog_pack_data(log, iclog, roundoff);\n\n\t \n\tsize = iclog->ic_offset;\n\tif (xfs_has_logv2(log->l_mp))\n\t\tsize += roundoff;\n\ticlog->ic_header.h_len = cpu_to_be32(size);\n\n\tXFS_STATS_INC(log->l_mp, xs_log_writes);\n\tXFS_STATS_ADD(log->l_mp, xs_log_blocks, BTOBB(count));\n\n\tbno = BLOCK_LSN(be64_to_cpu(iclog->ic_header.h_lsn));\n\n\t \n\tif (bno + BTOBB(count) > log->l_logBBsize)\n\t\txlog_split_iclog(log, &iclog->ic_header, bno, count);\n\n\t \n\ticlog->ic_header.h_crc = xlog_cksum(log, &iclog->ic_header,\n\t\t\t\t\t    iclog->ic_datap, size);\n\t \n#ifdef DEBUG\n\tif (XFS_TEST_ERROR(false, log->l_mp, XFS_ERRTAG_LOG_BAD_CRC)) {\n\t\ticlog->ic_header.h_crc &= cpu_to_le32(0xAAAAAAAA);\n\t\ticlog->ic_fail_crc = true;\n\t\txfs_warn(log->l_mp,\n\t\"Intentionally corrupted log record at LSN 0x%llx. Shutdown imminent.\",\n\t\t\t be64_to_cpu(iclog->ic_header.h_lsn));\n\t}\n#endif\n\txlog_verify_iclog(log, iclog, count);\n\txlog_write_iclog(log, iclog, bno, count);\n}\n\n \nSTATIC void\nxlog_dealloc_log(\n\tstruct xlog\t*log)\n{\n\txlog_in_core_t\t*iclog, *next_iclog;\n\tint\t\ti;\n\n\t \n\txlog_cil_destroy(log);\n\n\ticlog = log->l_iclog;\n\tfor (i = 0; i < log->l_iclog_bufs; i++) {\n\t\tnext_iclog = iclog->ic_next;\n\t\tkmem_free(iclog->ic_data);\n\t\tkmem_free(iclog);\n\t\ticlog = next_iclog;\n\t}\n\n\tlog->l_mp->m_log = NULL;\n\tdestroy_workqueue(log->l_ioend_workqueue);\n\tkmem_free(log);\n}\n\n \nstatic inline void\nxlog_state_finish_copy(\n\tstruct xlog\t\t*log,\n\tstruct xlog_in_core\t*iclog,\n\tint\t\t\trecord_cnt,\n\tint\t\t\tcopy_bytes)\n{\n\tlockdep_assert_held(&log->l_icloglock);\n\n\tbe32_add_cpu(&iclog->ic_header.h_num_logops, record_cnt);\n\ticlog->ic_offset += copy_bytes;\n}\n\n \nvoid\nxlog_print_tic_res(\n\tstruct xfs_mount\t*mp,\n\tstruct xlog_ticket\t*ticket)\n{\n\txfs_warn(mp, \"ticket reservation summary:\");\n\txfs_warn(mp, \"  unit res    = %d bytes\", ticket->t_unit_res);\n\txfs_warn(mp, \"  current res = %d bytes\", ticket->t_curr_res);\n\txfs_warn(mp, \"  original count  = %d\", ticket->t_ocnt);\n\txfs_warn(mp, \"  remaining count = %d\", ticket->t_cnt);\n}\n\n \nvoid\nxlog_print_trans(\n\tstruct xfs_trans\t*tp)\n{\n\tstruct xfs_mount\t*mp = tp->t_mountp;\n\tstruct xfs_log_item\t*lip;\n\n\t \n\txfs_warn(mp, \"transaction summary:\");\n\txfs_warn(mp, \"  log res   = %d\", tp->t_log_res);\n\txfs_warn(mp, \"  log count = %d\", tp->t_log_count);\n\txfs_warn(mp, \"  flags     = 0x%x\", tp->t_flags);\n\n\txlog_print_tic_res(mp, tp->t_ticket);\n\n\t \n\tlist_for_each_entry(lip, &tp->t_items, li_trans) {\n\t\tstruct xfs_log_vec\t*lv = lip->li_lv;\n\t\tstruct xfs_log_iovec\t*vec;\n\t\tint\t\t\ti;\n\n\t\txfs_warn(mp, \"log item: \");\n\t\txfs_warn(mp, \"  type\t= 0x%x\", lip->li_type);\n\t\txfs_warn(mp, \"  flags\t= 0x%lx\", lip->li_flags);\n\t\tif (!lv)\n\t\t\tcontinue;\n\t\txfs_warn(mp, \"  niovecs\t= %d\", lv->lv_niovecs);\n\t\txfs_warn(mp, \"  size\t= %d\", lv->lv_size);\n\t\txfs_warn(mp, \"  bytes\t= %d\", lv->lv_bytes);\n\t\txfs_warn(mp, \"  buf len\t= %d\", lv->lv_buf_len);\n\n\t\t \n\t\tvec = lv->lv_iovecp;\n\t\tfor (i = 0; i < lv->lv_niovecs; i++) {\n\t\t\tint dumplen = min(vec->i_len, 32);\n\n\t\t\txfs_warn(mp, \"  iovec[%d]\", i);\n\t\t\txfs_warn(mp, \"    type\t= 0x%x\", vec->i_type);\n\t\t\txfs_warn(mp, \"    len\t= %d\", vec->i_len);\n\t\t\txfs_warn(mp, \"    first %d bytes of iovec[%d]:\", dumplen, i);\n\t\t\txfs_hex_dump(vec->i_addr, dumplen);\n\n\t\t\tvec++;\n\t\t}\n\t}\n}\n\nstatic inline void\nxlog_write_iovec(\n\tstruct xlog_in_core\t*iclog,\n\tuint32_t\t\t*log_offset,\n\tvoid\t\t\t*data,\n\tuint32_t\t\twrite_len,\n\tint\t\t\t*bytes_left,\n\tuint32_t\t\t*record_cnt,\n\tuint32_t\t\t*data_cnt)\n{\n\tASSERT(*log_offset < iclog->ic_log->l_iclog_size);\n\tASSERT(*log_offset % sizeof(int32_t) == 0);\n\tASSERT(write_len % sizeof(int32_t) == 0);\n\n\tmemcpy(iclog->ic_datap + *log_offset, data, write_len);\n\t*log_offset += write_len;\n\t*bytes_left -= write_len;\n\t(*record_cnt)++;\n\t*data_cnt += write_len;\n}\n\n \nstatic void\nxlog_write_full(\n\tstruct xfs_log_vec\t*lv,\n\tstruct xlog_ticket\t*ticket,\n\tstruct xlog_in_core\t*iclog,\n\tuint32_t\t\t*log_offset,\n\tuint32_t\t\t*len,\n\tuint32_t\t\t*record_cnt,\n\tuint32_t\t\t*data_cnt)\n{\n\tint\t\t\tindex;\n\n\tASSERT(*log_offset + *len <= iclog->ic_size ||\n\t\ticlog->ic_state == XLOG_STATE_WANT_SYNC);\n\n\t \n\tfor (index = 0; index < lv->lv_niovecs; index++) {\n\t\tstruct xfs_log_iovec\t*reg = &lv->lv_iovecp[index];\n\t\tstruct xlog_op_header\t*ophdr = reg->i_addr;\n\n\t\tophdr->oh_tid = cpu_to_be32(ticket->t_tid);\n\t\txlog_write_iovec(iclog, log_offset, reg->i_addr,\n\t\t\t\treg->i_len, len, record_cnt, data_cnt);\n\t}\n}\n\nstatic int\nxlog_write_get_more_iclog_space(\n\tstruct xlog_ticket\t*ticket,\n\tstruct xlog_in_core\t**iclogp,\n\tuint32_t\t\t*log_offset,\n\tuint32_t\t\tlen,\n\tuint32_t\t\t*record_cnt,\n\tuint32_t\t\t*data_cnt)\n{\n\tstruct xlog_in_core\t*iclog = *iclogp;\n\tstruct xlog\t\t*log = iclog->ic_log;\n\tint\t\t\terror;\n\n\tspin_lock(&log->l_icloglock);\n\tASSERT(iclog->ic_state == XLOG_STATE_WANT_SYNC);\n\txlog_state_finish_copy(log, iclog, *record_cnt, *data_cnt);\n\terror = xlog_state_release_iclog(log, iclog, ticket);\n\tspin_unlock(&log->l_icloglock);\n\tif (error)\n\t\treturn error;\n\n\terror = xlog_state_get_iclog_space(log, len, &iclog, ticket,\n\t\t\t\t\tlog_offset);\n\tif (error)\n\t\treturn error;\n\t*record_cnt = 0;\n\t*data_cnt = 0;\n\t*iclogp = iclog;\n\treturn 0;\n}\n\n \nstatic int\nxlog_write_partial(\n\tstruct xfs_log_vec\t*lv,\n\tstruct xlog_ticket\t*ticket,\n\tstruct xlog_in_core\t**iclogp,\n\tuint32_t\t\t*log_offset,\n\tuint32_t\t\t*len,\n\tuint32_t\t\t*record_cnt,\n\tuint32_t\t\t*data_cnt)\n{\n\tstruct xlog_in_core\t*iclog = *iclogp;\n\tstruct xlog_op_header\t*ophdr;\n\tint\t\t\tindex = 0;\n\tuint32_t\t\trlen;\n\tint\t\t\terror;\n\n\t \n\tfor (index = 0; index < lv->lv_niovecs; index++) {\n\t\tstruct xfs_log_iovec\t*reg = &lv->lv_iovecp[index];\n\t\tuint32_t\t\treg_offset = 0;\n\n\t\t \n\t\tif (iclog->ic_size - *log_offset <=\n\t\t\t\t\tsizeof(struct xlog_op_header)) {\n\t\t\terror = xlog_write_get_more_iclog_space(ticket,\n\t\t\t\t\t&iclog, log_offset, *len, record_cnt,\n\t\t\t\t\tdata_cnt);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\t\t}\n\n\t\tophdr = reg->i_addr;\n\t\trlen = min_t(uint32_t, reg->i_len, iclog->ic_size - *log_offset);\n\n\t\tophdr->oh_tid = cpu_to_be32(ticket->t_tid);\n\t\tophdr->oh_len = cpu_to_be32(rlen - sizeof(struct xlog_op_header));\n\t\tif (rlen != reg->i_len)\n\t\t\tophdr->oh_flags |= XLOG_CONTINUE_TRANS;\n\n\t\txlog_write_iovec(iclog, log_offset, reg->i_addr,\n\t\t\t\trlen, len, record_cnt, data_cnt);\n\n\t\t \n\t\tif (rlen == reg->i_len)\n\t\t\tcontinue;\n\n\t\t \n\t\tdo {\n\t\t\t \n\t\t\terror = xlog_write_get_more_iclog_space(ticket,\n\t\t\t\t\t&iclog, log_offset,\n\t\t\t\t\t*len + sizeof(struct xlog_op_header),\n\t\t\t\t\trecord_cnt, data_cnt);\n\t\t\tif (error)\n\t\t\t\treturn error;\n\n\t\t\tophdr = iclog->ic_datap + *log_offset;\n\t\t\tophdr->oh_tid = cpu_to_be32(ticket->t_tid);\n\t\t\tophdr->oh_clientid = XFS_TRANSACTION;\n\t\t\tophdr->oh_res2 = 0;\n\t\t\tophdr->oh_flags = XLOG_WAS_CONT_TRANS;\n\n\t\t\tticket->t_curr_res -= sizeof(struct xlog_op_header);\n\t\t\t*log_offset += sizeof(struct xlog_op_header);\n\t\t\t*data_cnt += sizeof(struct xlog_op_header);\n\n\t\t\t \n\t\t\treg_offset += rlen;\n\t\t\trlen = reg->i_len - reg_offset;\n\t\t\tif (rlen <= iclog->ic_size - *log_offset)\n\t\t\t\tophdr->oh_flags |= XLOG_END_TRANS;\n\t\t\telse\n\t\t\t\tophdr->oh_flags |= XLOG_CONTINUE_TRANS;\n\n\t\t\trlen = min_t(uint32_t, rlen, iclog->ic_size - *log_offset);\n\t\t\tophdr->oh_len = cpu_to_be32(rlen);\n\n\t\t\txlog_write_iovec(iclog, log_offset,\n\t\t\t\t\treg->i_addr + reg_offset,\n\t\t\t\t\trlen, len, record_cnt, data_cnt);\n\n\t\t} while (ophdr->oh_flags & XLOG_CONTINUE_TRANS);\n\t}\n\n\t \n\t*iclogp = iclog;\n\treturn 0;\n}\n\n \nint\nxlog_write(\n\tstruct xlog\t\t*log,\n\tstruct xfs_cil_ctx\t*ctx,\n\tstruct list_head\t*lv_chain,\n\tstruct xlog_ticket\t*ticket,\n\tuint32_t\t\tlen)\n\n{\n\tstruct xlog_in_core\t*iclog = NULL;\n\tstruct xfs_log_vec\t*lv;\n\tuint32_t\t\trecord_cnt = 0;\n\tuint32_t\t\tdata_cnt = 0;\n\tint\t\t\terror = 0;\n\tint\t\t\tlog_offset;\n\n\tif (ticket->t_curr_res < 0) {\n\t\txfs_alert_tag(log->l_mp, XFS_PTAG_LOGRES,\n\t\t     \"ctx ticket reservation ran out. Need to up reservation\");\n\t\txlog_print_tic_res(log->l_mp, ticket);\n\t\txlog_force_shutdown(log, SHUTDOWN_LOG_IO_ERROR);\n\t}\n\n\terror = xlog_state_get_iclog_space(log, len, &iclog, ticket,\n\t\t\t\t\t   &log_offset);\n\tif (error)\n\t\treturn error;\n\n\tASSERT(log_offset <= iclog->ic_size - 1);\n\n\t \n\tif (ctx)\n\t\txlog_cil_set_ctx_write_state(ctx, iclog);\n\n\tlist_for_each_entry(lv, lv_chain, lv_list) {\n\t\t \n\t\tif (lv->lv_niovecs &&\n\t\t    lv->lv_bytes > iclog->ic_size - log_offset) {\n\t\t\terror = xlog_write_partial(lv, ticket, &iclog,\n\t\t\t\t\t&log_offset, &len, &record_cnt,\n\t\t\t\t\t&data_cnt);\n\t\t\tif (error) {\n\t\t\t\t \n\t\t\t\treturn error;\n\t\t\t}\n\t\t} else {\n\t\t\txlog_write_full(lv, ticket, iclog, &log_offset,\n\t\t\t\t\t &len, &record_cnt, &data_cnt);\n\t\t}\n\t}\n\tASSERT(len == 0);\n\n\t \n\tspin_lock(&log->l_icloglock);\n\txlog_state_finish_copy(log, iclog, record_cnt, 0);\n\terror = xlog_state_release_iclog(log, iclog, ticket);\n\tspin_unlock(&log->l_icloglock);\n\n\treturn error;\n}\n\nstatic void\nxlog_state_activate_iclog(\n\tstruct xlog_in_core\t*iclog,\n\tint\t\t\t*iclogs_changed)\n{\n\tASSERT(list_empty_careful(&iclog->ic_callbacks));\n\ttrace_xlog_iclog_activate(iclog, _RET_IP_);\n\n\t \n\tif (*iclogs_changed == 0 &&\n\t    iclog->ic_header.h_num_logops == cpu_to_be32(XLOG_COVER_OPS)) {\n\t\t*iclogs_changed = 1;\n\t} else {\n\t\t \n\t\t*iclogs_changed = 2;\n\t}\n\n\ticlog->ic_state\t= XLOG_STATE_ACTIVE;\n\ticlog->ic_offset = 0;\n\ticlog->ic_header.h_num_logops = 0;\n\tmemset(iclog->ic_header.h_cycle_data, 0,\n\t\tsizeof(iclog->ic_header.h_cycle_data));\n\ticlog->ic_header.h_lsn = 0;\n\ticlog->ic_header.h_tail_lsn = 0;\n}\n\n \nstatic void\nxlog_state_activate_iclogs(\n\tstruct xlog\t\t*log,\n\tint\t\t\t*iclogs_changed)\n{\n\tstruct xlog_in_core\t*iclog = log->l_iclog;\n\n\tdo {\n\t\tif (iclog->ic_state == XLOG_STATE_DIRTY)\n\t\t\txlog_state_activate_iclog(iclog, iclogs_changed);\n\t\t \n\t\telse if (iclog->ic_state != XLOG_STATE_ACTIVE)\n\t\t\tbreak;\n\t} while ((iclog = iclog->ic_next) != log->l_iclog);\n}\n\nstatic int\nxlog_covered_state(\n\tint\t\t\tprev_state,\n\tint\t\t\ticlogs_changed)\n{\n\t \n\tswitch (prev_state) {\n\tcase XLOG_STATE_COVER_IDLE:\n\t\tif (iclogs_changed == 1)\n\t\t\treturn XLOG_STATE_COVER_IDLE;\n\t\tfallthrough;\n\tcase XLOG_STATE_COVER_NEED:\n\tcase XLOG_STATE_COVER_NEED2:\n\t\tbreak;\n\tcase XLOG_STATE_COVER_DONE:\n\t\tif (iclogs_changed == 1)\n\t\t\treturn XLOG_STATE_COVER_NEED2;\n\t\tbreak;\n\tcase XLOG_STATE_COVER_DONE2:\n\t\tif (iclogs_changed == 1)\n\t\t\treturn XLOG_STATE_COVER_IDLE;\n\t\tbreak;\n\tdefault:\n\t\tASSERT(0);\n\t}\n\n\treturn XLOG_STATE_COVER_NEED;\n}\n\nSTATIC void\nxlog_state_clean_iclog(\n\tstruct xlog\t\t*log,\n\tstruct xlog_in_core\t*dirty_iclog)\n{\n\tint\t\t\ticlogs_changed = 0;\n\n\ttrace_xlog_iclog_clean(dirty_iclog, _RET_IP_);\n\n\tdirty_iclog->ic_state = XLOG_STATE_DIRTY;\n\n\txlog_state_activate_iclogs(log, &iclogs_changed);\n\twake_up_all(&dirty_iclog->ic_force_wait);\n\n\tif (iclogs_changed) {\n\t\tlog->l_covered_state = xlog_covered_state(log->l_covered_state,\n\t\t\t\ticlogs_changed);\n\t}\n}\n\nSTATIC xfs_lsn_t\nxlog_get_lowest_lsn(\n\tstruct xlog\t\t*log)\n{\n\tstruct xlog_in_core\t*iclog = log->l_iclog;\n\txfs_lsn_t\t\tlowest_lsn = 0, lsn;\n\n\tdo {\n\t\tif (iclog->ic_state == XLOG_STATE_ACTIVE ||\n\t\t    iclog->ic_state == XLOG_STATE_DIRTY)\n\t\t\tcontinue;\n\n\t\tlsn = be64_to_cpu(iclog->ic_header.h_lsn);\n\t\tif ((lsn && !lowest_lsn) || XFS_LSN_CMP(lsn, lowest_lsn) < 0)\n\t\t\tlowest_lsn = lsn;\n\t} while ((iclog = iclog->ic_next) != log->l_iclog);\n\n\treturn lowest_lsn;\n}\n\n \nstatic void\nxlog_state_set_callback(\n\tstruct xlog\t\t*log,\n\tstruct xlog_in_core\t*iclog,\n\txfs_lsn_t\t\theader_lsn)\n{\n\ttrace_xlog_iclog_callback(iclog, _RET_IP_);\n\ticlog->ic_state = XLOG_STATE_CALLBACK;\n\n\tASSERT(XFS_LSN_CMP(atomic64_read(&log->l_last_sync_lsn),\n\t\t\t   header_lsn) <= 0);\n\n\tif (list_empty_careful(&iclog->ic_callbacks))\n\t\treturn;\n\n\tatomic64_set(&log->l_last_sync_lsn, header_lsn);\n\txlog_grant_push_ail(log, 0);\n}\n\n \nstatic bool\nxlog_state_iodone_process_iclog(\n\tstruct xlog\t\t*log,\n\tstruct xlog_in_core\t*iclog)\n{\n\txfs_lsn_t\t\tlowest_lsn;\n\txfs_lsn_t\t\theader_lsn;\n\n\tswitch (iclog->ic_state) {\n\tcase XLOG_STATE_ACTIVE:\n\tcase XLOG_STATE_DIRTY:\n\t\t \n\t\treturn false;\n\tcase XLOG_STATE_DONE_SYNC:\n\t\t \n\t\theader_lsn = be64_to_cpu(iclog->ic_header.h_lsn);\n\t\tlowest_lsn = xlog_get_lowest_lsn(log);\n\t\tif (lowest_lsn && XFS_LSN_CMP(lowest_lsn, header_lsn) < 0)\n\t\t\treturn false;\n\t\txlog_state_set_callback(log, iclog, header_lsn);\n\t\treturn false;\n\tdefault:\n\t\t \n\t\treturn true;\n\t}\n}\n\n \nstatic bool\nxlog_state_do_iclog_callbacks(\n\tstruct xlog\t\t*log)\n\t\t__releases(&log->l_icloglock)\n\t\t__acquires(&log->l_icloglock)\n{\n\tstruct xlog_in_core\t*first_iclog = log->l_iclog;\n\tstruct xlog_in_core\t*iclog = first_iclog;\n\tbool\t\t\tran_callback = false;\n\n\tdo {\n\t\tLIST_HEAD(cb_list);\n\n\t\tif (xlog_state_iodone_process_iclog(log, iclog))\n\t\t\tbreak;\n\t\tif (iclog->ic_state != XLOG_STATE_CALLBACK) {\n\t\t\ticlog = iclog->ic_next;\n\t\t\tcontinue;\n\t\t}\n\t\tlist_splice_init(&iclog->ic_callbacks, &cb_list);\n\t\tspin_unlock(&log->l_icloglock);\n\n\t\ttrace_xlog_iclog_callbacks_start(iclog, _RET_IP_);\n\t\txlog_cil_process_committed(&cb_list);\n\t\ttrace_xlog_iclog_callbacks_done(iclog, _RET_IP_);\n\t\tran_callback = true;\n\n\t\tspin_lock(&log->l_icloglock);\n\t\txlog_state_clean_iclog(log, iclog);\n\t\ticlog = iclog->ic_next;\n\t} while (iclog != first_iclog);\n\n\treturn ran_callback;\n}\n\n\n \nSTATIC void\nxlog_state_do_callback(\n\tstruct xlog\t\t*log)\n{\n\tint\t\t\tflushcnt = 0;\n\tint\t\t\trepeats = 0;\n\n\tspin_lock(&log->l_icloglock);\n\twhile (xlog_state_do_iclog_callbacks(log)) {\n\t\tif (xlog_is_shutdown(log))\n\t\t\tbreak;\n\n\t\tif (++repeats > 5000) {\n\t\t\tflushcnt += repeats;\n\t\t\trepeats = 0;\n\t\t\txfs_warn(log->l_mp,\n\t\t\t\t\"%s: possible infinite loop (%d iterations)\",\n\t\t\t\t__func__, flushcnt);\n\t\t}\n\t}\n\n\tif (log->l_iclog->ic_state == XLOG_STATE_ACTIVE)\n\t\twake_up_all(&log->l_flush_wait);\n\n\tspin_unlock(&log->l_icloglock);\n}\n\n\n \nSTATIC void\nxlog_state_done_syncing(\n\tstruct xlog_in_core\t*iclog)\n{\n\tstruct xlog\t\t*log = iclog->ic_log;\n\n\tspin_lock(&log->l_icloglock);\n\tASSERT(atomic_read(&iclog->ic_refcnt) == 0);\n\ttrace_xlog_iclog_sync_done(iclog, _RET_IP_);\n\n\t \n\tif (!xlog_is_shutdown(log)) {\n\t\tASSERT(iclog->ic_state == XLOG_STATE_SYNCING);\n\t\ticlog->ic_state = XLOG_STATE_DONE_SYNC;\n\t}\n\n\t \n\twake_up_all(&iclog->ic_write_wait);\n\tspin_unlock(&log->l_icloglock);\n\txlog_state_do_callback(log);\n}\n\n \nSTATIC int\nxlog_state_get_iclog_space(\n\tstruct xlog\t\t*log,\n\tint\t\t\tlen,\n\tstruct xlog_in_core\t**iclogp,\n\tstruct xlog_ticket\t*ticket,\n\tint\t\t\t*logoffsetp)\n{\n\tint\t\t  log_offset;\n\txlog_rec_header_t *head;\n\txlog_in_core_t\t  *iclog;\n\nrestart:\n\tspin_lock(&log->l_icloglock);\n\tif (xlog_is_shutdown(log)) {\n\t\tspin_unlock(&log->l_icloglock);\n\t\treturn -EIO;\n\t}\n\n\ticlog = log->l_iclog;\n\tif (iclog->ic_state != XLOG_STATE_ACTIVE) {\n\t\tXFS_STATS_INC(log->l_mp, xs_log_noiclogs);\n\n\t\t \n\t\txlog_wait(&log->l_flush_wait, &log->l_icloglock);\n\t\tgoto restart;\n\t}\n\n\thead = &iclog->ic_header;\n\n\tatomic_inc(&iclog->ic_refcnt);\t \n\tlog_offset = iclog->ic_offset;\n\n\ttrace_xlog_iclog_get_space(iclog, _RET_IP_);\n\n\t \n\tif (log_offset == 0) {\n\t\tticket->t_curr_res -= log->l_iclog_hsize;\n\t\thead->h_cycle = cpu_to_be32(log->l_curr_cycle);\n\t\thead->h_lsn = cpu_to_be64(\n\t\t\txlog_assign_lsn(log->l_curr_cycle, log->l_curr_block));\n\t\tASSERT(log->l_curr_block >= 0);\n\t}\n\n\t \n\tif (iclog->ic_size - iclog->ic_offset < 2*sizeof(xlog_op_header_t)) {\n\t\tint\t\terror = 0;\n\n\t\txlog_state_switch_iclogs(log, iclog, iclog->ic_size);\n\n\t\t \n\t\tif (!atomic_add_unless(&iclog->ic_refcnt, -1, 1))\n\t\t\terror = xlog_state_release_iclog(log, iclog, ticket);\n\t\tspin_unlock(&log->l_icloglock);\n\t\tif (error)\n\t\t\treturn error;\n\t\tgoto restart;\n\t}\n\n\t \n\tif (len <= iclog->ic_size - iclog->ic_offset)\n\t\ticlog->ic_offset += len;\n\telse\n\t\txlog_state_switch_iclogs(log, iclog, iclog->ic_size);\n\t*iclogp = iclog;\n\n\tASSERT(iclog->ic_offset <= iclog->ic_size);\n\tspin_unlock(&log->l_icloglock);\n\n\t*logoffsetp = log_offset;\n\treturn 0;\n}\n\n \nvoid\nxfs_log_ticket_regrant(\n\tstruct xlog\t\t*log,\n\tstruct xlog_ticket\t*ticket)\n{\n\ttrace_xfs_log_ticket_regrant(log, ticket);\n\n\tif (ticket->t_cnt > 0)\n\t\tticket->t_cnt--;\n\n\txlog_grant_sub_space(log, &log->l_reserve_head.grant,\n\t\t\t\t\tticket->t_curr_res);\n\txlog_grant_sub_space(log, &log->l_write_head.grant,\n\t\t\t\t\tticket->t_curr_res);\n\tticket->t_curr_res = ticket->t_unit_res;\n\n\ttrace_xfs_log_ticket_regrant_sub(log, ticket);\n\n\t \n\tif (!ticket->t_cnt) {\n\t\txlog_grant_add_space(log, &log->l_reserve_head.grant,\n\t\t\t\t     ticket->t_unit_res);\n\t\ttrace_xfs_log_ticket_regrant_exit(log, ticket);\n\n\t\tticket->t_curr_res = ticket->t_unit_res;\n\t}\n\n\txfs_log_ticket_put(ticket);\n}\n\n \nvoid\nxfs_log_ticket_ungrant(\n\tstruct xlog\t\t*log,\n\tstruct xlog_ticket\t*ticket)\n{\n\tint\t\t\tbytes;\n\n\ttrace_xfs_log_ticket_ungrant(log, ticket);\n\n\tif (ticket->t_cnt > 0)\n\t\tticket->t_cnt--;\n\n\ttrace_xfs_log_ticket_ungrant_sub(log, ticket);\n\n\t \n\tbytes = ticket->t_curr_res;\n\tif (ticket->t_cnt > 0) {\n\t\tASSERT(ticket->t_flags & XLOG_TIC_PERM_RESERV);\n\t\tbytes += ticket->t_unit_res*ticket->t_cnt;\n\t}\n\n\txlog_grant_sub_space(log, &log->l_reserve_head.grant, bytes);\n\txlog_grant_sub_space(log, &log->l_write_head.grant, bytes);\n\n\ttrace_xfs_log_ticket_ungrant_exit(log, ticket);\n\n\txfs_log_space_wake(log->l_mp);\n\txfs_log_ticket_put(ticket);\n}\n\n \nvoid\nxlog_state_switch_iclogs(\n\tstruct xlog\t\t*log,\n\tstruct xlog_in_core\t*iclog,\n\tint\t\t\teventual_size)\n{\n\tASSERT(iclog->ic_state == XLOG_STATE_ACTIVE);\n\tassert_spin_locked(&log->l_icloglock);\n\ttrace_xlog_iclog_switch(iclog, _RET_IP_);\n\n\tif (!eventual_size)\n\t\teventual_size = iclog->ic_offset;\n\ticlog->ic_state = XLOG_STATE_WANT_SYNC;\n\ticlog->ic_header.h_prev_block = cpu_to_be32(log->l_prev_block);\n\tlog->l_prev_block = log->l_curr_block;\n\tlog->l_prev_cycle = log->l_curr_cycle;\n\n\t \n\tlog->l_curr_block += BTOBB(eventual_size)+BTOBB(log->l_iclog_hsize);\n\n\t \n\tif (log->l_iclog_roundoff > BBSIZE) {\n\t\tuint32_t sunit_bb = BTOBB(log->l_iclog_roundoff);\n\t\tlog->l_curr_block = roundup(log->l_curr_block, sunit_bb);\n\t}\n\n\tif (log->l_curr_block >= log->l_logBBsize) {\n\t\t \n\t\tlog->l_curr_block -= log->l_logBBsize;\n\t\tASSERT(log->l_curr_block >= 0);\n\t\tsmp_wmb();\n\t\tlog->l_curr_cycle++;\n\t\tif (log->l_curr_cycle == XLOG_HEADER_MAGIC_NUM)\n\t\t\tlog->l_curr_cycle++;\n\t}\n\tASSERT(iclog == log->l_iclog);\n\tlog->l_iclog = iclog->ic_next;\n}\n\n \nstatic int\nxlog_force_and_check_iclog(\n\tstruct xlog_in_core\t*iclog,\n\tbool\t\t\t*completed)\n{\n\txfs_lsn_t\t\tlsn = be64_to_cpu(iclog->ic_header.h_lsn);\n\tint\t\t\terror;\n\n\t*completed = false;\n\terror = xlog_force_iclog(iclog);\n\tif (error)\n\t\treturn error;\n\n\t \n\tif (be64_to_cpu(iclog->ic_header.h_lsn) != lsn)\n\t\t*completed = true;\n\treturn 0;\n}\n\n \nint\nxfs_log_force(\n\tstruct xfs_mount\t*mp,\n\tuint\t\t\tflags)\n{\n\tstruct xlog\t\t*log = mp->m_log;\n\tstruct xlog_in_core\t*iclog;\n\n\tXFS_STATS_INC(mp, xs_log_force);\n\ttrace_xfs_log_force(mp, 0, _RET_IP_);\n\n\txlog_cil_force(log);\n\n\tspin_lock(&log->l_icloglock);\n\tif (xlog_is_shutdown(log))\n\t\tgoto out_error;\n\n\ticlog = log->l_iclog;\n\ttrace_xlog_iclog_force(iclog, _RET_IP_);\n\n\tif (iclog->ic_state == XLOG_STATE_DIRTY ||\n\t    (iclog->ic_state == XLOG_STATE_ACTIVE &&\n\t     atomic_read(&iclog->ic_refcnt) == 0 && iclog->ic_offset == 0)) {\n\t\t \n\t\ticlog = iclog->ic_prev;\n\t} else if (iclog->ic_state == XLOG_STATE_ACTIVE) {\n\t\tif (atomic_read(&iclog->ic_refcnt) == 0) {\n\t\t\t \n\t\t\tbool\tcompleted;\n\n\t\t\tif (xlog_force_and_check_iclog(iclog, &completed))\n\t\t\t\tgoto out_error;\n\n\t\t\tif (completed)\n\t\t\t\tgoto out_unlock;\n\t\t} else {\n\t\t\t \n\t\t\txlog_state_switch_iclogs(log, iclog, 0);\n\t\t}\n\t}\n\n\t \n\tif (iclog->ic_state == XLOG_STATE_WANT_SYNC)\n\t\ticlog->ic_flags |= XLOG_ICL_NEED_FLUSH | XLOG_ICL_NEED_FUA;\n\n\tif (flags & XFS_LOG_SYNC)\n\t\treturn xlog_wait_on_iclog(iclog);\nout_unlock:\n\tspin_unlock(&log->l_icloglock);\n\treturn 0;\nout_error:\n\tspin_unlock(&log->l_icloglock);\n\treturn -EIO;\n}\n\n \nstatic int\nxlog_force_lsn(\n\tstruct xlog\t\t*log,\n\txfs_lsn_t\t\tlsn,\n\tuint\t\t\tflags,\n\tint\t\t\t*log_flushed,\n\tbool\t\t\talready_slept)\n{\n\tstruct xlog_in_core\t*iclog;\n\tbool\t\t\tcompleted;\n\n\tspin_lock(&log->l_icloglock);\n\tif (xlog_is_shutdown(log))\n\t\tgoto out_error;\n\n\ticlog = log->l_iclog;\n\twhile (be64_to_cpu(iclog->ic_header.h_lsn) != lsn) {\n\t\ttrace_xlog_iclog_force_lsn(iclog, _RET_IP_);\n\t\ticlog = iclog->ic_next;\n\t\tif (iclog == log->l_iclog)\n\t\t\tgoto out_unlock;\n\t}\n\n\tswitch (iclog->ic_state) {\n\tcase XLOG_STATE_ACTIVE:\n\t\t \n\t\tif (!already_slept &&\n\t\t    (iclog->ic_prev->ic_state == XLOG_STATE_WANT_SYNC ||\n\t\t     iclog->ic_prev->ic_state == XLOG_STATE_SYNCING)) {\n\t\t\txlog_wait(&iclog->ic_prev->ic_write_wait,\n\t\t\t\t\t&log->l_icloglock);\n\t\t\treturn -EAGAIN;\n\t\t}\n\t\tif (xlog_force_and_check_iclog(iclog, &completed))\n\t\t\tgoto out_error;\n\t\tif (log_flushed)\n\t\t\t*log_flushed = 1;\n\t\tif (completed)\n\t\t\tgoto out_unlock;\n\t\tbreak;\n\tcase XLOG_STATE_WANT_SYNC:\n\t\t \n\t\ticlog->ic_flags |= XLOG_ICL_NEED_FLUSH | XLOG_ICL_NEED_FUA;\n\t\tbreak;\n\tdefault:\n\t\t \n\t\tbreak;\n\t}\n\n\tif (flags & XFS_LOG_SYNC)\n\t\treturn xlog_wait_on_iclog(iclog);\nout_unlock:\n\tspin_unlock(&log->l_icloglock);\n\treturn 0;\nout_error:\n\tspin_unlock(&log->l_icloglock);\n\treturn -EIO;\n}\n\n \nint\nxfs_log_force_seq(\n\tstruct xfs_mount\t*mp,\n\txfs_csn_t\t\tseq,\n\tuint\t\t\tflags,\n\tint\t\t\t*log_flushed)\n{\n\tstruct xlog\t\t*log = mp->m_log;\n\txfs_lsn_t\t\tlsn;\n\tint\t\t\tret;\n\tASSERT(seq != 0);\n\n\tXFS_STATS_INC(mp, xs_log_force);\n\ttrace_xfs_log_force(mp, seq, _RET_IP_);\n\n\tlsn = xlog_cil_force_seq(log, seq);\n\tif (lsn == NULLCOMMITLSN)\n\t\treturn 0;\n\n\tret = xlog_force_lsn(log, lsn, flags, log_flushed, false);\n\tif (ret == -EAGAIN) {\n\t\tXFS_STATS_INC(mp, xs_log_force_sleep);\n\t\tret = xlog_force_lsn(log, lsn, flags, log_flushed, true);\n\t}\n\treturn ret;\n}\n\n \nvoid\nxfs_log_ticket_put(\n\txlog_ticket_t\t*ticket)\n{\n\tASSERT(atomic_read(&ticket->t_ref) > 0);\n\tif (atomic_dec_and_test(&ticket->t_ref))\n\t\tkmem_cache_free(xfs_log_ticket_cache, ticket);\n}\n\nxlog_ticket_t *\nxfs_log_ticket_get(\n\txlog_ticket_t\t*ticket)\n{\n\tASSERT(atomic_read(&ticket->t_ref) > 0);\n\tatomic_inc(&ticket->t_ref);\n\treturn ticket;\n}\n\n \nstatic int\nxlog_calc_unit_res(\n\tstruct xlog\t\t*log,\n\tint\t\t\tunit_bytes,\n\tint\t\t\t*niclogs)\n{\n\tint\t\t\ticlog_space;\n\tuint\t\t\tnum_headers;\n\n\t \n\n\t \n\tunit_bytes += sizeof(xlog_op_header_t);\n\tunit_bytes += sizeof(xfs_trans_header_t);\n\n\t \n\tunit_bytes += sizeof(xlog_op_header_t);\n\n\t \n\ticlog_space = log->l_iclog_size - log->l_iclog_hsize;\n\tnum_headers = howmany(unit_bytes, iclog_space);\n\n\t \n\tunit_bytes += sizeof(xlog_op_header_t) * num_headers;\n\n\t \n\twhile (!num_headers ||\n\t       howmany(unit_bytes, iclog_space) > num_headers) {\n\t\tunit_bytes += sizeof(xlog_op_header_t);\n\t\tnum_headers++;\n\t}\n\tunit_bytes += log->l_iclog_hsize * num_headers;\n\n\t \n\tunit_bytes += log->l_iclog_hsize;\n\n\t \n\tunit_bytes += 2 * log->l_iclog_roundoff;\n\n\tif (niclogs)\n\t\t*niclogs = num_headers;\n\treturn unit_bytes;\n}\n\nint\nxfs_log_calc_unit_res(\n\tstruct xfs_mount\t*mp,\n\tint\t\t\tunit_bytes)\n{\n\treturn xlog_calc_unit_res(mp->m_log, unit_bytes, NULL);\n}\n\n \nstruct xlog_ticket *\nxlog_ticket_alloc(\n\tstruct xlog\t\t*log,\n\tint\t\t\tunit_bytes,\n\tint\t\t\tcnt,\n\tbool\t\t\tpermanent)\n{\n\tstruct xlog_ticket\t*tic;\n\tint\t\t\tunit_res;\n\n\ttic = kmem_cache_zalloc(xfs_log_ticket_cache, GFP_NOFS | __GFP_NOFAIL);\n\n\tunit_res = xlog_calc_unit_res(log, unit_bytes, &tic->t_iclog_hdrs);\n\n\tatomic_set(&tic->t_ref, 1);\n\ttic->t_task\t\t= current;\n\tINIT_LIST_HEAD(&tic->t_queue);\n\ttic->t_unit_res\t\t= unit_res;\n\ttic->t_curr_res\t\t= unit_res;\n\ttic->t_cnt\t\t= cnt;\n\ttic->t_ocnt\t\t= cnt;\n\ttic->t_tid\t\t= get_random_u32();\n\tif (permanent)\n\t\ttic->t_flags |= XLOG_TIC_PERM_RESERV;\n\n\treturn tic;\n}\n\n#if defined(DEBUG)\n \nSTATIC void\nxlog_verify_grant_tail(\n\tstruct xlog\t*log)\n{\n\tint\t\ttail_cycle, tail_blocks;\n\tint\t\tcycle, space;\n\n\txlog_crack_grant_head(&log->l_write_head.grant, &cycle, &space);\n\txlog_crack_atomic_lsn(&log->l_tail_lsn, &tail_cycle, &tail_blocks);\n\tif (tail_cycle != cycle) {\n\t\tif (cycle - 1 != tail_cycle &&\n\t\t    !test_and_set_bit(XLOG_TAIL_WARN, &log->l_opstate)) {\n\t\t\txfs_alert_tag(log->l_mp, XFS_PTAG_LOGRES,\n\t\t\t\t\"%s: cycle - 1 != tail_cycle\", __func__);\n\t\t}\n\n\t\tif (space > BBTOB(tail_blocks) &&\n\t\t    !test_and_set_bit(XLOG_TAIL_WARN, &log->l_opstate)) {\n\t\t\txfs_alert_tag(log->l_mp, XFS_PTAG_LOGRES,\n\t\t\t\t\"%s: space > BBTOB(tail_blocks)\", __func__);\n\t\t}\n\t}\n}\n\n \nSTATIC void\nxlog_verify_tail_lsn(\n\tstruct xlog\t\t*log,\n\tstruct xlog_in_core\t*iclog)\n{\n\txfs_lsn_t\ttail_lsn = be64_to_cpu(iclog->ic_header.h_tail_lsn);\n\tint\t\tblocks;\n\n    if (CYCLE_LSN(tail_lsn) == log->l_prev_cycle) {\n\tblocks =\n\t    log->l_logBBsize - (log->l_prev_block - BLOCK_LSN(tail_lsn));\n\tif (blocks < BTOBB(iclog->ic_offset)+BTOBB(log->l_iclog_hsize))\n\t\txfs_emerg(log->l_mp, \"%s: ran out of log space\", __func__);\n    } else {\n\tASSERT(CYCLE_LSN(tail_lsn)+1 == log->l_prev_cycle);\n\n\tif (BLOCK_LSN(tail_lsn) == log->l_prev_block)\n\t\txfs_emerg(log->l_mp, \"%s: tail wrapped\", __func__);\n\n\tblocks = BLOCK_LSN(tail_lsn) - log->l_prev_block;\n\tif (blocks < BTOBB(iclog->ic_offset) + 1)\n\t\txfs_emerg(log->l_mp, \"%s: ran out of log space\", __func__);\n    }\n}\n\n \nSTATIC void\nxlog_verify_iclog(\n\tstruct xlog\t\t*log,\n\tstruct xlog_in_core\t*iclog,\n\tint\t\t\tcount)\n{\n\txlog_op_header_t\t*ophead;\n\txlog_in_core_t\t\t*icptr;\n\txlog_in_core_2_t\t*xhdr;\n\tvoid\t\t\t*base_ptr, *ptr, *p;\n\tptrdiff_t\t\tfield_offset;\n\tuint8_t\t\t\tclientid;\n\tint\t\t\tlen, i, j, k, op_len;\n\tint\t\t\tidx;\n\n\t \n\tspin_lock(&log->l_icloglock);\n\ticptr = log->l_iclog;\n\tfor (i = 0; i < log->l_iclog_bufs; i++, icptr = icptr->ic_next)\n\t\tASSERT(icptr);\n\n\tif (icptr != log->l_iclog)\n\t\txfs_emerg(log->l_mp, \"%s: corrupt iclog ring\", __func__);\n\tspin_unlock(&log->l_icloglock);\n\n\t \n\tif (iclog->ic_header.h_magicno != cpu_to_be32(XLOG_HEADER_MAGIC_NUM))\n\t\txfs_emerg(log->l_mp, \"%s: invalid magic num\", __func__);\n\n\tbase_ptr = ptr = &iclog->ic_header;\n\tp = &iclog->ic_header;\n\tfor (ptr += BBSIZE; ptr < base_ptr + count; ptr += BBSIZE) {\n\t\tif (*(__be32 *)ptr == cpu_to_be32(XLOG_HEADER_MAGIC_NUM))\n\t\t\txfs_emerg(log->l_mp, \"%s: unexpected magic num\",\n\t\t\t\t__func__);\n\t}\n\n\t \n\tlen = be32_to_cpu(iclog->ic_header.h_num_logops);\n\tbase_ptr = ptr = iclog->ic_datap;\n\tophead = ptr;\n\txhdr = iclog->ic_data;\n\tfor (i = 0; i < len; i++) {\n\t\tophead = ptr;\n\n\t\t \n\t\tp = &ophead->oh_clientid;\n\t\tfield_offset = p - base_ptr;\n\t\tif (field_offset & 0x1ff) {\n\t\t\tclientid = ophead->oh_clientid;\n\t\t} else {\n\t\t\tidx = BTOBBT((void *)&ophead->oh_clientid - iclog->ic_datap);\n\t\t\tif (idx >= (XLOG_HEADER_CYCLE_SIZE / BBSIZE)) {\n\t\t\t\tj = idx / (XLOG_HEADER_CYCLE_SIZE / BBSIZE);\n\t\t\t\tk = idx % (XLOG_HEADER_CYCLE_SIZE / BBSIZE);\n\t\t\t\tclientid = xlog_get_client_id(\n\t\t\t\t\txhdr[j].hic_xheader.xh_cycle_data[k]);\n\t\t\t} else {\n\t\t\t\tclientid = xlog_get_client_id(\n\t\t\t\t\ticlog->ic_header.h_cycle_data[idx]);\n\t\t\t}\n\t\t}\n\t\tif (clientid != XFS_TRANSACTION && clientid != XFS_LOG) {\n\t\t\txfs_warn(log->l_mp,\n\t\t\t\t\"%s: op %d invalid clientid %d op \"PTR_FMT\" offset 0x%lx\",\n\t\t\t\t__func__, i, clientid, ophead,\n\t\t\t\t(unsigned long)field_offset);\n\t\t}\n\n\t\t \n\t\tp = &ophead->oh_len;\n\t\tfield_offset = p - base_ptr;\n\t\tif (field_offset & 0x1ff) {\n\t\t\top_len = be32_to_cpu(ophead->oh_len);\n\t\t} else {\n\t\t\tidx = BTOBBT((void *)&ophead->oh_len - iclog->ic_datap);\n\t\t\tif (idx >= (XLOG_HEADER_CYCLE_SIZE / BBSIZE)) {\n\t\t\t\tj = idx / (XLOG_HEADER_CYCLE_SIZE / BBSIZE);\n\t\t\t\tk = idx % (XLOG_HEADER_CYCLE_SIZE / BBSIZE);\n\t\t\t\top_len = be32_to_cpu(xhdr[j].hic_xheader.xh_cycle_data[k]);\n\t\t\t} else {\n\t\t\t\top_len = be32_to_cpu(iclog->ic_header.h_cycle_data[idx]);\n\t\t\t}\n\t\t}\n\t\tptr += sizeof(xlog_op_header_t) + op_len;\n\t}\n}\n#endif\n\n \nbool\nxlog_force_shutdown(\n\tstruct xlog\t*log,\n\tuint32_t\tshutdown_flags)\n{\n\tbool\t\tlog_error = (shutdown_flags & SHUTDOWN_LOG_IO_ERROR);\n\n\tif (!log)\n\t\treturn false;\n\n\t \n\tif (!log_error && !xlog_in_recovery(log))\n\t\txfs_log_force(log->l_mp, XFS_LOG_SYNC);\n\n\t \n\tspin_lock(&log->l_icloglock);\n\tif (test_and_set_bit(XLOG_IO_ERROR, &log->l_opstate)) {\n\t\tspin_unlock(&log->l_icloglock);\n\t\treturn false;\n\t}\n\tspin_unlock(&log->l_icloglock);\n\n\t \n\tif (!test_and_set_bit(XFS_OPSTATE_SHUTDOWN, &log->l_mp->m_opstate)) {\n\t\txfs_alert_tag(log->l_mp, XFS_PTAG_SHUTDOWN_LOGERROR,\n\"Filesystem has been shut down due to log error (0x%x).\",\n\t\t\t\tshutdown_flags);\n\t\txfs_alert(log->l_mp,\n\"Please unmount the filesystem and rectify the problem(s).\");\n\t\tif (xfs_error_level >= XFS_ERRLEVEL_HIGH)\n\t\t\txfs_stack_trace();\n\t}\n\n\t \n\txlog_grant_head_wake_all(&log->l_reserve_head);\n\txlog_grant_head_wake_all(&log->l_write_head);\n\n\t \n\tspin_lock(&log->l_cilp->xc_push_lock);\n\twake_up_all(&log->l_cilp->xc_start_wait);\n\twake_up_all(&log->l_cilp->xc_commit_wait);\n\tspin_unlock(&log->l_cilp->xc_push_lock);\n\n\tspin_lock(&log->l_icloglock);\n\txlog_state_shutdown_callbacks(log);\n\tspin_unlock(&log->l_icloglock);\n\n\twake_up_var(&log->l_opstate);\n\treturn log_error;\n}\n\nSTATIC int\nxlog_iclogs_empty(\n\tstruct xlog\t*log)\n{\n\txlog_in_core_t\t*iclog;\n\n\ticlog = log->l_iclog;\n\tdo {\n\t\t \n\t\tif (iclog->ic_header.h_num_logops)\n\t\t\treturn 0;\n\t\ticlog = iclog->ic_next;\n\t} while (iclog != log->l_iclog);\n\treturn 1;\n}\n\n \nbool\nxfs_log_check_lsn(\n\tstruct xfs_mount\t*mp,\n\txfs_lsn_t\t\tlsn)\n{\n\tstruct xlog\t\t*log = mp->m_log;\n\tbool\t\t\tvalid;\n\n\t \n\tif (xfs_has_norecovery(mp))\n\t\treturn true;\n\n\t \n\tif (lsn == NULLCOMMITLSN)\n\t\treturn true;\n\n\tvalid = xlog_valid_lsn(mp->m_log, lsn);\n\n\t \n\tif (!valid) {\n\t\tspin_lock(&log->l_icloglock);\n\t\txfs_warn(mp,\n\"Corruption warning: Metadata has LSN (%d:%d) ahead of current LSN (%d:%d). \"\n\"Please unmount and run xfs_repair (>= v4.3) to resolve.\",\n\t\t\t CYCLE_LSN(lsn), BLOCK_LSN(lsn),\n\t\t\t log->l_curr_cycle, log->l_curr_block);\n\t\tspin_unlock(&log->l_icloglock);\n\t}\n\n\treturn valid;\n}\n\n \nvoid\nxlog_use_incompat_feat(\n\tstruct xlog\t\t*log)\n{\n\tdown_read(&log->l_incompat_users);\n}\n\n \nvoid\nxlog_drop_incompat_feat(\n\tstruct xlog\t\t*log)\n{\n\tup_read(&log->l_incompat_users);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}