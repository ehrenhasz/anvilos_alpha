{
  "module_name": "inode.c",
  "hash_id": "d1428a895dd400b9b89dea260b4027912a53d07e65a75cede181adc3d44f28e6",
  "original_prompt": "Ingested from linux-6.6.14/fs/inode.c",
  "human_readable_source": "\n \n#include <linux/export.h>\n#include <linux/fs.h>\n#include <linux/filelock.h>\n#include <linux/mm.h>\n#include <linux/backing-dev.h>\n#include <linux/hash.h>\n#include <linux/swap.h>\n#include <linux/security.h>\n#include <linux/cdev.h>\n#include <linux/memblock.h>\n#include <linux/fsnotify.h>\n#include <linux/mount.h>\n#include <linux/posix_acl.h>\n#include <linux/buffer_head.h>  \n#include <linux/ratelimit.h>\n#include <linux/list_lru.h>\n#include <linux/iversion.h>\n#include <trace/events/writeback.h>\n#include \"internal.h\"\n\n \n\nstatic unsigned int i_hash_mask __read_mostly;\nstatic unsigned int i_hash_shift __read_mostly;\nstatic struct hlist_head *inode_hashtable __read_mostly;\nstatic __cacheline_aligned_in_smp DEFINE_SPINLOCK(inode_hash_lock);\n\n \nconst struct address_space_operations empty_aops = {\n};\nEXPORT_SYMBOL(empty_aops);\n\nstatic DEFINE_PER_CPU(unsigned long, nr_inodes);\nstatic DEFINE_PER_CPU(unsigned long, nr_unused);\n\nstatic struct kmem_cache *inode_cachep __read_mostly;\n\nstatic long get_nr_inodes(void)\n{\n\tint i;\n\tlong sum = 0;\n\tfor_each_possible_cpu(i)\n\t\tsum += per_cpu(nr_inodes, i);\n\treturn sum < 0 ? 0 : sum;\n}\n\nstatic inline long get_nr_inodes_unused(void)\n{\n\tint i;\n\tlong sum = 0;\n\tfor_each_possible_cpu(i)\n\t\tsum += per_cpu(nr_unused, i);\n\treturn sum < 0 ? 0 : sum;\n}\n\nlong get_nr_dirty_inodes(void)\n{\n\t \n\tlong nr_dirty = get_nr_inodes() - get_nr_inodes_unused();\n\treturn nr_dirty > 0 ? nr_dirty : 0;\n}\n\n \n#ifdef CONFIG_SYSCTL\n \nstatic struct inodes_stat_t inodes_stat;\n\nstatic int proc_nr_inodes(struct ctl_table *table, int write, void *buffer,\n\t\t\t  size_t *lenp, loff_t *ppos)\n{\n\tinodes_stat.nr_inodes = get_nr_inodes();\n\tinodes_stat.nr_unused = get_nr_inodes_unused();\n\treturn proc_doulongvec_minmax(table, write, buffer, lenp, ppos);\n}\n\nstatic struct ctl_table inodes_sysctls[] = {\n\t{\n\t\t.procname\t= \"inode-nr\",\n\t\t.data\t\t= &inodes_stat,\n\t\t.maxlen\t\t= 2*sizeof(long),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_nr_inodes,\n\t},\n\t{\n\t\t.procname\t= \"inode-state\",\n\t\t.data\t\t= &inodes_stat,\n\t\t.maxlen\t\t= 7*sizeof(long),\n\t\t.mode\t\t= 0444,\n\t\t.proc_handler\t= proc_nr_inodes,\n\t},\n\t{ }\n};\n\nstatic int __init init_fs_inode_sysctls(void)\n{\n\tregister_sysctl_init(\"fs\", inodes_sysctls);\n\treturn 0;\n}\nearly_initcall(init_fs_inode_sysctls);\n#endif\n\nstatic int no_open(struct inode *inode, struct file *file)\n{\n\treturn -ENXIO;\n}\n\n \nint inode_init_always(struct super_block *sb, struct inode *inode)\n{\n\tstatic const struct inode_operations empty_iops;\n\tstatic const struct file_operations no_open_fops = {.open = no_open};\n\tstruct address_space *const mapping = &inode->i_data;\n\n\tinode->i_sb = sb;\n\tinode->i_blkbits = sb->s_blocksize_bits;\n\tinode->i_flags = 0;\n\tatomic64_set(&inode->i_sequence, 0);\n\tatomic_set(&inode->i_count, 1);\n\tinode->i_op = &empty_iops;\n\tinode->i_fop = &no_open_fops;\n\tinode->i_ino = 0;\n\tinode->__i_nlink = 1;\n\tinode->i_opflags = 0;\n\tif (sb->s_xattr)\n\t\tinode->i_opflags |= IOP_XATTR;\n\ti_uid_write(inode, 0);\n\ti_gid_write(inode, 0);\n\tatomic_set(&inode->i_writecount, 0);\n\tinode->i_size = 0;\n\tinode->i_write_hint = WRITE_LIFE_NOT_SET;\n\tinode->i_blocks = 0;\n\tinode->i_bytes = 0;\n\tinode->i_generation = 0;\n\tinode->i_pipe = NULL;\n\tinode->i_cdev = NULL;\n\tinode->i_link = NULL;\n\tinode->i_dir_seq = 0;\n\tinode->i_rdev = 0;\n\tinode->dirtied_when = 0;\n\n#ifdef CONFIG_CGROUP_WRITEBACK\n\tinode->i_wb_frn_winner = 0;\n\tinode->i_wb_frn_avg_time = 0;\n\tinode->i_wb_frn_history = 0;\n#endif\n\n\tspin_lock_init(&inode->i_lock);\n\tlockdep_set_class(&inode->i_lock, &sb->s_type->i_lock_key);\n\n\tinit_rwsem(&inode->i_rwsem);\n\tlockdep_set_class(&inode->i_rwsem, &sb->s_type->i_mutex_key);\n\n\tatomic_set(&inode->i_dio_count, 0);\n\n\tmapping->a_ops = &empty_aops;\n\tmapping->host = inode;\n\tmapping->flags = 0;\n\tmapping->wb_err = 0;\n\tatomic_set(&mapping->i_mmap_writable, 0);\n#ifdef CONFIG_READ_ONLY_THP_FOR_FS\n\tatomic_set(&mapping->nr_thps, 0);\n#endif\n\tmapping_set_gfp_mask(mapping, GFP_HIGHUSER_MOVABLE);\n\tmapping->private_data = NULL;\n\tmapping->writeback_index = 0;\n\tinit_rwsem(&mapping->invalidate_lock);\n\tlockdep_set_class_and_name(&mapping->invalidate_lock,\n\t\t\t\t   &sb->s_type->invalidate_lock_key,\n\t\t\t\t   \"mapping.invalidate_lock\");\n\tif (sb->s_iflags & SB_I_STABLE_WRITES)\n\t\tmapping_set_stable_writes(mapping);\n\tinode->i_private = NULL;\n\tinode->i_mapping = mapping;\n\tINIT_HLIST_HEAD(&inode->i_dentry);\t \n#ifdef CONFIG_FS_POSIX_ACL\n\tinode->i_acl = inode->i_default_acl = ACL_NOT_CACHED;\n#endif\n\n#ifdef CONFIG_FSNOTIFY\n\tinode->i_fsnotify_mask = 0;\n#endif\n\tinode->i_flctx = NULL;\n\n\tif (unlikely(security_inode_alloc(inode)))\n\t\treturn -ENOMEM;\n\tthis_cpu_inc(nr_inodes);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(inode_init_always);\n\nvoid free_inode_nonrcu(struct inode *inode)\n{\n\tkmem_cache_free(inode_cachep, inode);\n}\nEXPORT_SYMBOL(free_inode_nonrcu);\n\nstatic void i_callback(struct rcu_head *head)\n{\n\tstruct inode *inode = container_of(head, struct inode, i_rcu);\n\tif (inode->free_inode)\n\t\tinode->free_inode(inode);\n\telse\n\t\tfree_inode_nonrcu(inode);\n}\n\nstatic struct inode *alloc_inode(struct super_block *sb)\n{\n\tconst struct super_operations *ops = sb->s_op;\n\tstruct inode *inode;\n\n\tif (ops->alloc_inode)\n\t\tinode = ops->alloc_inode(sb);\n\telse\n\t\tinode = alloc_inode_sb(sb, inode_cachep, GFP_KERNEL);\n\n\tif (!inode)\n\t\treturn NULL;\n\n\tif (unlikely(inode_init_always(sb, inode))) {\n\t\tif (ops->destroy_inode) {\n\t\t\tops->destroy_inode(inode);\n\t\t\tif (!ops->free_inode)\n\t\t\t\treturn NULL;\n\t\t}\n\t\tinode->free_inode = ops->free_inode;\n\t\ti_callback(&inode->i_rcu);\n\t\treturn NULL;\n\t}\n\n\treturn inode;\n}\n\nvoid __destroy_inode(struct inode *inode)\n{\n\tBUG_ON(inode_has_buffers(inode));\n\tinode_detach_wb(inode);\n\tsecurity_inode_free(inode);\n\tfsnotify_inode_delete(inode);\n\tlocks_free_lock_context(inode);\n\tif (!inode->i_nlink) {\n\t\tWARN_ON(atomic_long_read(&inode->i_sb->s_remove_count) == 0);\n\t\tatomic_long_dec(&inode->i_sb->s_remove_count);\n\t}\n\n#ifdef CONFIG_FS_POSIX_ACL\n\tif (inode->i_acl && !is_uncached_acl(inode->i_acl))\n\t\tposix_acl_release(inode->i_acl);\n\tif (inode->i_default_acl && !is_uncached_acl(inode->i_default_acl))\n\t\tposix_acl_release(inode->i_default_acl);\n#endif\n\tthis_cpu_dec(nr_inodes);\n}\nEXPORT_SYMBOL(__destroy_inode);\n\nstatic void destroy_inode(struct inode *inode)\n{\n\tconst struct super_operations *ops = inode->i_sb->s_op;\n\n\tBUG_ON(!list_empty(&inode->i_lru));\n\t__destroy_inode(inode);\n\tif (ops->destroy_inode) {\n\t\tops->destroy_inode(inode);\n\t\tif (!ops->free_inode)\n\t\t\treturn;\n\t}\n\tinode->free_inode = ops->free_inode;\n\tcall_rcu(&inode->i_rcu, i_callback);\n}\n\n \nvoid drop_nlink(struct inode *inode)\n{\n\tWARN_ON(inode->i_nlink == 0);\n\tinode->__i_nlink--;\n\tif (!inode->i_nlink)\n\t\tatomic_long_inc(&inode->i_sb->s_remove_count);\n}\nEXPORT_SYMBOL(drop_nlink);\n\n \nvoid clear_nlink(struct inode *inode)\n{\n\tif (inode->i_nlink) {\n\t\tinode->__i_nlink = 0;\n\t\tatomic_long_inc(&inode->i_sb->s_remove_count);\n\t}\n}\nEXPORT_SYMBOL(clear_nlink);\n\n \nvoid set_nlink(struct inode *inode, unsigned int nlink)\n{\n\tif (!nlink) {\n\t\tclear_nlink(inode);\n\t} else {\n\t\t \n\t\tif (inode->i_nlink == 0)\n\t\t\tatomic_long_dec(&inode->i_sb->s_remove_count);\n\n\t\tinode->__i_nlink = nlink;\n\t}\n}\nEXPORT_SYMBOL(set_nlink);\n\n \nvoid inc_nlink(struct inode *inode)\n{\n\tif (unlikely(inode->i_nlink == 0)) {\n\t\tWARN_ON(!(inode->i_state & I_LINKABLE));\n\t\tatomic_long_dec(&inode->i_sb->s_remove_count);\n\t}\n\n\tinode->__i_nlink++;\n}\nEXPORT_SYMBOL(inc_nlink);\n\nstatic void __address_space_init_once(struct address_space *mapping)\n{\n\txa_init_flags(&mapping->i_pages, XA_FLAGS_LOCK_IRQ | XA_FLAGS_ACCOUNT);\n\tinit_rwsem(&mapping->i_mmap_rwsem);\n\tINIT_LIST_HEAD(&mapping->private_list);\n\tspin_lock_init(&mapping->private_lock);\n\tmapping->i_mmap = RB_ROOT_CACHED;\n}\n\nvoid address_space_init_once(struct address_space *mapping)\n{\n\tmemset(mapping, 0, sizeof(*mapping));\n\t__address_space_init_once(mapping);\n}\nEXPORT_SYMBOL(address_space_init_once);\n\n \nvoid inode_init_once(struct inode *inode)\n{\n\tmemset(inode, 0, sizeof(*inode));\n\tINIT_HLIST_NODE(&inode->i_hash);\n\tINIT_LIST_HEAD(&inode->i_devices);\n\tINIT_LIST_HEAD(&inode->i_io_list);\n\tINIT_LIST_HEAD(&inode->i_wb_list);\n\tINIT_LIST_HEAD(&inode->i_lru);\n\tINIT_LIST_HEAD(&inode->i_sb_list);\n\t__address_space_init_once(&inode->i_data);\n\ti_size_ordered_init(inode);\n}\nEXPORT_SYMBOL(inode_init_once);\n\nstatic void init_once(void *foo)\n{\n\tstruct inode *inode = (struct inode *) foo;\n\n\tinode_init_once(inode);\n}\n\n \nvoid __iget(struct inode *inode)\n{\n\tatomic_inc(&inode->i_count);\n}\n\n \nvoid ihold(struct inode *inode)\n{\n\tWARN_ON(atomic_inc_return(&inode->i_count) < 2);\n}\nEXPORT_SYMBOL(ihold);\n\nstatic void __inode_add_lru(struct inode *inode, bool rotate)\n{\n\tif (inode->i_state & (I_DIRTY_ALL | I_SYNC | I_FREEING | I_WILL_FREE))\n\t\treturn;\n\tif (atomic_read(&inode->i_count))\n\t\treturn;\n\tif (!(inode->i_sb->s_flags & SB_ACTIVE))\n\t\treturn;\n\tif (!mapping_shrinkable(&inode->i_data))\n\t\treturn;\n\n\tif (list_lru_add(&inode->i_sb->s_inode_lru, &inode->i_lru))\n\t\tthis_cpu_inc(nr_unused);\n\telse if (rotate)\n\t\tinode->i_state |= I_REFERENCED;\n}\n\n \nvoid inode_add_lru(struct inode *inode)\n{\n\t__inode_add_lru(inode, false);\n}\n\nstatic void inode_lru_list_del(struct inode *inode)\n{\n\tif (list_lru_del(&inode->i_sb->s_inode_lru, &inode->i_lru))\n\t\tthis_cpu_dec(nr_unused);\n}\n\n \nvoid inode_sb_list_add(struct inode *inode)\n{\n\tspin_lock(&inode->i_sb->s_inode_list_lock);\n\tlist_add(&inode->i_sb_list, &inode->i_sb->s_inodes);\n\tspin_unlock(&inode->i_sb->s_inode_list_lock);\n}\nEXPORT_SYMBOL_GPL(inode_sb_list_add);\n\nstatic inline void inode_sb_list_del(struct inode *inode)\n{\n\tif (!list_empty(&inode->i_sb_list)) {\n\t\tspin_lock(&inode->i_sb->s_inode_list_lock);\n\t\tlist_del_init(&inode->i_sb_list);\n\t\tspin_unlock(&inode->i_sb->s_inode_list_lock);\n\t}\n}\n\nstatic unsigned long hash(struct super_block *sb, unsigned long hashval)\n{\n\tunsigned long tmp;\n\n\ttmp = (hashval * (unsigned long)sb) ^ (GOLDEN_RATIO_PRIME + hashval) /\n\t\t\tL1_CACHE_BYTES;\n\ttmp = tmp ^ ((tmp ^ GOLDEN_RATIO_PRIME) >> i_hash_shift);\n\treturn tmp & i_hash_mask;\n}\n\n \nvoid __insert_inode_hash(struct inode *inode, unsigned long hashval)\n{\n\tstruct hlist_head *b = inode_hashtable + hash(inode->i_sb, hashval);\n\n\tspin_lock(&inode_hash_lock);\n\tspin_lock(&inode->i_lock);\n\thlist_add_head_rcu(&inode->i_hash, b);\n\tspin_unlock(&inode->i_lock);\n\tspin_unlock(&inode_hash_lock);\n}\nEXPORT_SYMBOL(__insert_inode_hash);\n\n \nvoid __remove_inode_hash(struct inode *inode)\n{\n\tspin_lock(&inode_hash_lock);\n\tspin_lock(&inode->i_lock);\n\thlist_del_init_rcu(&inode->i_hash);\n\tspin_unlock(&inode->i_lock);\n\tspin_unlock(&inode_hash_lock);\n}\nEXPORT_SYMBOL(__remove_inode_hash);\n\nvoid dump_mapping(const struct address_space *mapping)\n{\n\tstruct inode *host;\n\tconst struct address_space_operations *a_ops;\n\tstruct hlist_node *dentry_first;\n\tstruct dentry *dentry_ptr;\n\tstruct dentry dentry;\n\tunsigned long ino;\n\n\t \n\tif (get_kernel_nofault(host, &mapping->host) ||\n\t    get_kernel_nofault(a_ops, &mapping->a_ops)) {\n\t\tpr_warn(\"invalid mapping:%px\\n\", mapping);\n\t\treturn;\n\t}\n\n\tif (!host) {\n\t\tpr_warn(\"aops:%ps\\n\", a_ops);\n\t\treturn;\n\t}\n\n\tif (get_kernel_nofault(dentry_first, &host->i_dentry.first) ||\n\t    get_kernel_nofault(ino, &host->i_ino)) {\n\t\tpr_warn(\"aops:%ps invalid inode:%px\\n\", a_ops, host);\n\t\treturn;\n\t}\n\n\tif (!dentry_first) {\n\t\tpr_warn(\"aops:%ps ino:%lx\\n\", a_ops, ino);\n\t\treturn;\n\t}\n\n\tdentry_ptr = container_of(dentry_first, struct dentry, d_u.d_alias);\n\tif (get_kernel_nofault(dentry, dentry_ptr)) {\n\t\tpr_warn(\"aops:%ps ino:%lx invalid dentry:%px\\n\",\n\t\t\t\ta_ops, ino, dentry_ptr);\n\t\treturn;\n\t}\n\n\t \n\tpr_warn(\"aops:%ps ino:%lx dentry name:\\\"%pd\\\"\\n\", a_ops, ino, &dentry);\n}\n\nvoid clear_inode(struct inode *inode)\n{\n\t \n\txa_lock_irq(&inode->i_data.i_pages);\n\tBUG_ON(inode->i_data.nrpages);\n\t \n\txa_unlock_irq(&inode->i_data.i_pages);\n\tBUG_ON(!list_empty(&inode->i_data.private_list));\n\tBUG_ON(!(inode->i_state & I_FREEING));\n\tBUG_ON(inode->i_state & I_CLEAR);\n\tBUG_ON(!list_empty(&inode->i_wb_list));\n\t \n\tinode->i_state = I_FREEING | I_CLEAR;\n}\nEXPORT_SYMBOL(clear_inode);\n\n \nstatic void evict(struct inode *inode)\n{\n\tconst struct super_operations *op = inode->i_sb->s_op;\n\n\tBUG_ON(!(inode->i_state & I_FREEING));\n\tBUG_ON(!list_empty(&inode->i_lru));\n\n\tif (!list_empty(&inode->i_io_list))\n\t\tinode_io_list_del(inode);\n\n\tinode_sb_list_del(inode);\n\n\t \n\tinode_wait_for_writeback(inode);\n\n\tif (op->evict_inode) {\n\t\top->evict_inode(inode);\n\t} else {\n\t\ttruncate_inode_pages_final(&inode->i_data);\n\t\tclear_inode(inode);\n\t}\n\tif (S_ISCHR(inode->i_mode) && inode->i_cdev)\n\t\tcd_forget(inode);\n\n\tremove_inode_hash(inode);\n\n\tspin_lock(&inode->i_lock);\n\twake_up_bit(&inode->i_state, __I_NEW);\n\tBUG_ON(inode->i_state != (I_FREEING | I_CLEAR));\n\tspin_unlock(&inode->i_lock);\n\n\tdestroy_inode(inode);\n}\n\n \nstatic void dispose_list(struct list_head *head)\n{\n\twhile (!list_empty(head)) {\n\t\tstruct inode *inode;\n\n\t\tinode = list_first_entry(head, struct inode, i_lru);\n\t\tlist_del_init(&inode->i_lru);\n\n\t\tevict(inode);\n\t\tcond_resched();\n\t}\n}\n\n \nvoid evict_inodes(struct super_block *sb)\n{\n\tstruct inode *inode, *next;\n\tLIST_HEAD(dispose);\n\nagain:\n\tspin_lock(&sb->s_inode_list_lock);\n\tlist_for_each_entry_safe(inode, next, &sb->s_inodes, i_sb_list) {\n\t\tif (atomic_read(&inode->i_count))\n\t\t\tcontinue;\n\n\t\tspin_lock(&inode->i_lock);\n\t\tif (inode->i_state & (I_NEW | I_FREEING | I_WILL_FREE)) {\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tinode->i_state |= I_FREEING;\n\t\tinode_lru_list_del(inode);\n\t\tspin_unlock(&inode->i_lock);\n\t\tlist_add(&inode->i_lru, &dispose);\n\n\t\t \n\t\tif (need_resched()) {\n\t\t\tspin_unlock(&sb->s_inode_list_lock);\n\t\t\tcond_resched();\n\t\t\tdispose_list(&dispose);\n\t\t\tgoto again;\n\t\t}\n\t}\n\tspin_unlock(&sb->s_inode_list_lock);\n\n\tdispose_list(&dispose);\n}\nEXPORT_SYMBOL_GPL(evict_inodes);\n\n \nvoid invalidate_inodes(struct super_block *sb)\n{\n\tstruct inode *inode, *next;\n\tLIST_HEAD(dispose);\n\nagain:\n\tspin_lock(&sb->s_inode_list_lock);\n\tlist_for_each_entry_safe(inode, next, &sb->s_inodes, i_sb_list) {\n\t\tspin_lock(&inode->i_lock);\n\t\tif (inode->i_state & (I_NEW | I_FREEING | I_WILL_FREE)) {\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\tcontinue;\n\t\t}\n\t\tif (atomic_read(&inode->i_count)) {\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\tcontinue;\n\t\t}\n\n\t\tinode->i_state |= I_FREEING;\n\t\tinode_lru_list_del(inode);\n\t\tspin_unlock(&inode->i_lock);\n\t\tlist_add(&inode->i_lru, &dispose);\n\t\tif (need_resched()) {\n\t\t\tspin_unlock(&sb->s_inode_list_lock);\n\t\t\tcond_resched();\n\t\t\tdispose_list(&dispose);\n\t\t\tgoto again;\n\t\t}\n\t}\n\tspin_unlock(&sb->s_inode_list_lock);\n\n\tdispose_list(&dispose);\n}\n\n \nstatic enum lru_status inode_lru_isolate(struct list_head *item,\n\t\tstruct list_lru_one *lru, spinlock_t *lru_lock, void *arg)\n{\n\tstruct list_head *freeable = arg;\n\tstruct inode\t*inode = container_of(item, struct inode, i_lru);\n\n\t \n\tif (!spin_trylock(&inode->i_lock))\n\t\treturn LRU_SKIP;\n\n\t \n\tif (atomic_read(&inode->i_count) ||\n\t    (inode->i_state & ~I_REFERENCED) ||\n\t    !mapping_shrinkable(&inode->i_data)) {\n\t\tlist_lru_isolate(lru, &inode->i_lru);\n\t\tspin_unlock(&inode->i_lock);\n\t\tthis_cpu_dec(nr_unused);\n\t\treturn LRU_REMOVED;\n\t}\n\n\t \n\tif (inode->i_state & I_REFERENCED) {\n\t\tinode->i_state &= ~I_REFERENCED;\n\t\tspin_unlock(&inode->i_lock);\n\t\treturn LRU_ROTATE;\n\t}\n\n\t \n\tif (inode_has_buffers(inode) || !mapping_empty(&inode->i_data)) {\n\t\t__iget(inode);\n\t\tspin_unlock(&inode->i_lock);\n\t\tspin_unlock(lru_lock);\n\t\tif (remove_inode_buffers(inode)) {\n\t\t\tunsigned long reap;\n\t\t\treap = invalidate_mapping_pages(&inode->i_data, 0, -1);\n\t\t\tif (current_is_kswapd())\n\t\t\t\t__count_vm_events(KSWAPD_INODESTEAL, reap);\n\t\t\telse\n\t\t\t\t__count_vm_events(PGINODESTEAL, reap);\n\t\t\tmm_account_reclaimed_pages(reap);\n\t\t}\n\t\tiput(inode);\n\t\tspin_lock(lru_lock);\n\t\treturn LRU_RETRY;\n\t}\n\n\tWARN_ON(inode->i_state & I_NEW);\n\tinode->i_state |= I_FREEING;\n\tlist_lru_isolate_move(lru, &inode->i_lru, freeable);\n\tspin_unlock(&inode->i_lock);\n\n\tthis_cpu_dec(nr_unused);\n\treturn LRU_REMOVED;\n}\n\n \nlong prune_icache_sb(struct super_block *sb, struct shrink_control *sc)\n{\n\tLIST_HEAD(freeable);\n\tlong freed;\n\n\tfreed = list_lru_shrink_walk(&sb->s_inode_lru, sc,\n\t\t\t\t     inode_lru_isolate, &freeable);\n\tdispose_list(&freeable);\n\treturn freed;\n}\n\nstatic void __wait_on_freeing_inode(struct inode *inode);\n \nstatic struct inode *find_inode(struct super_block *sb,\n\t\t\t\tstruct hlist_head *head,\n\t\t\t\tint (*test)(struct inode *, void *),\n\t\t\t\tvoid *data)\n{\n\tstruct inode *inode = NULL;\n\nrepeat:\n\thlist_for_each_entry(inode, head, i_hash) {\n\t\tif (inode->i_sb != sb)\n\t\t\tcontinue;\n\t\tif (!test(inode, data))\n\t\t\tcontinue;\n\t\tspin_lock(&inode->i_lock);\n\t\tif (inode->i_state & (I_FREEING|I_WILL_FREE)) {\n\t\t\t__wait_on_freeing_inode(inode);\n\t\t\tgoto repeat;\n\t\t}\n\t\tif (unlikely(inode->i_state & I_CREATING)) {\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\treturn ERR_PTR(-ESTALE);\n\t\t}\n\t\t__iget(inode);\n\t\tspin_unlock(&inode->i_lock);\n\t\treturn inode;\n\t}\n\treturn NULL;\n}\n\n \nstatic struct inode *find_inode_fast(struct super_block *sb,\n\t\t\t\tstruct hlist_head *head, unsigned long ino)\n{\n\tstruct inode *inode = NULL;\n\nrepeat:\n\thlist_for_each_entry(inode, head, i_hash) {\n\t\tif (inode->i_ino != ino)\n\t\t\tcontinue;\n\t\tif (inode->i_sb != sb)\n\t\t\tcontinue;\n\t\tspin_lock(&inode->i_lock);\n\t\tif (inode->i_state & (I_FREEING|I_WILL_FREE)) {\n\t\t\t__wait_on_freeing_inode(inode);\n\t\t\tgoto repeat;\n\t\t}\n\t\tif (unlikely(inode->i_state & I_CREATING)) {\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\treturn ERR_PTR(-ESTALE);\n\t\t}\n\t\t__iget(inode);\n\t\tspin_unlock(&inode->i_lock);\n\t\treturn inode;\n\t}\n\treturn NULL;\n}\n\n \n#define LAST_INO_BATCH 1024\nstatic DEFINE_PER_CPU(unsigned int, last_ino);\n\nunsigned int get_next_ino(void)\n{\n\tunsigned int *p = &get_cpu_var(last_ino);\n\tunsigned int res = *p;\n\n#ifdef CONFIG_SMP\n\tif (unlikely((res & (LAST_INO_BATCH-1)) == 0)) {\n\t\tstatic atomic_t shared_last_ino;\n\t\tint next = atomic_add_return(LAST_INO_BATCH, &shared_last_ino);\n\n\t\tres = next - LAST_INO_BATCH;\n\t}\n#endif\n\n\tres++;\n\t \n\tif (unlikely(!res))\n\t\tres++;\n\t*p = res;\n\tput_cpu_var(last_ino);\n\treturn res;\n}\nEXPORT_SYMBOL(get_next_ino);\n\n \nstruct inode *new_inode_pseudo(struct super_block *sb)\n{\n\tstruct inode *inode = alloc_inode(sb);\n\n\tif (inode) {\n\t\tspin_lock(&inode->i_lock);\n\t\tinode->i_state = 0;\n\t\tspin_unlock(&inode->i_lock);\n\t}\n\treturn inode;\n}\n\n \nstruct inode *new_inode(struct super_block *sb)\n{\n\tstruct inode *inode;\n\n\tinode = new_inode_pseudo(sb);\n\tif (inode)\n\t\tinode_sb_list_add(inode);\n\treturn inode;\n}\nEXPORT_SYMBOL(new_inode);\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\nvoid lockdep_annotate_inode_mutex_key(struct inode *inode)\n{\n\tif (S_ISDIR(inode->i_mode)) {\n\t\tstruct file_system_type *type = inode->i_sb->s_type;\n\n\t\t \n\t\tif (lockdep_match_class(&inode->i_rwsem, &type->i_mutex_key)) {\n\t\t\t \n\t\t\t \n\t\t\tinit_rwsem(&inode->i_rwsem);\n\t\t\tlockdep_set_class(&inode->i_rwsem,\n\t\t\t\t\t  &type->i_mutex_dir_key);\n\t\t}\n\t}\n}\nEXPORT_SYMBOL(lockdep_annotate_inode_mutex_key);\n#endif\n\n \nvoid unlock_new_inode(struct inode *inode)\n{\n\tlockdep_annotate_inode_mutex_key(inode);\n\tspin_lock(&inode->i_lock);\n\tWARN_ON(!(inode->i_state & I_NEW));\n\tinode->i_state &= ~I_NEW & ~I_CREATING;\n\tsmp_mb();\n\twake_up_bit(&inode->i_state, __I_NEW);\n\tspin_unlock(&inode->i_lock);\n}\nEXPORT_SYMBOL(unlock_new_inode);\n\nvoid discard_new_inode(struct inode *inode)\n{\n\tlockdep_annotate_inode_mutex_key(inode);\n\tspin_lock(&inode->i_lock);\n\tWARN_ON(!(inode->i_state & I_NEW));\n\tinode->i_state &= ~I_NEW;\n\tsmp_mb();\n\twake_up_bit(&inode->i_state, __I_NEW);\n\tspin_unlock(&inode->i_lock);\n\tiput(inode);\n}\nEXPORT_SYMBOL(discard_new_inode);\n\n \nvoid lock_two_inodes(struct inode *inode1, struct inode *inode2,\n\t\t     unsigned subclass1, unsigned subclass2)\n{\n\tif (!inode1 || !inode2) {\n\t\t \n\t\tif (!inode1)\n\t\t\tswap(inode1, inode2);\n\t\tgoto lock;\n\t}\n\n\t \n\tif (S_ISDIR(inode2->i_mode) == S_ISDIR(inode1->i_mode)) {\n\t\tif (inode1 > inode2)\n\t\t\tswap(inode1, inode2);\n\t} else if (!S_ISDIR(inode1->i_mode))\n\t\tswap(inode1, inode2);\nlock:\n\tif (inode1)\n\t\tinode_lock_nested(inode1, subclass1);\n\tif (inode2 && inode2 != inode1)\n\t\tinode_lock_nested(inode2, subclass2);\n}\n\n \nvoid lock_two_nondirectories(struct inode *inode1, struct inode *inode2)\n{\n\tif (inode1)\n\t\tWARN_ON_ONCE(S_ISDIR(inode1->i_mode));\n\tif (inode2)\n\t\tWARN_ON_ONCE(S_ISDIR(inode2->i_mode));\n\tlock_two_inodes(inode1, inode2, I_MUTEX_NORMAL, I_MUTEX_NONDIR2);\n}\nEXPORT_SYMBOL(lock_two_nondirectories);\n\n \nvoid unlock_two_nondirectories(struct inode *inode1, struct inode *inode2)\n{\n\tif (inode1) {\n\t\tWARN_ON_ONCE(S_ISDIR(inode1->i_mode));\n\t\tinode_unlock(inode1);\n\t}\n\tif (inode2 && inode2 != inode1) {\n\t\tWARN_ON_ONCE(S_ISDIR(inode2->i_mode));\n\t\tinode_unlock(inode2);\n\t}\n}\nEXPORT_SYMBOL(unlock_two_nondirectories);\n\n \nstruct inode *inode_insert5(struct inode *inode, unsigned long hashval,\n\t\t\t    int (*test)(struct inode *, void *),\n\t\t\t    int (*set)(struct inode *, void *), void *data)\n{\n\tstruct hlist_head *head = inode_hashtable + hash(inode->i_sb, hashval);\n\tstruct inode *old;\n\nagain:\n\tspin_lock(&inode_hash_lock);\n\told = find_inode(inode->i_sb, head, test, data);\n\tif (unlikely(old)) {\n\t\t \n\t\tspin_unlock(&inode_hash_lock);\n\t\tif (IS_ERR(old))\n\t\t\treturn NULL;\n\t\twait_on_inode(old);\n\t\tif (unlikely(inode_unhashed(old))) {\n\t\t\tiput(old);\n\t\t\tgoto again;\n\t\t}\n\t\treturn old;\n\t}\n\n\tif (set && unlikely(set(inode, data))) {\n\t\tinode = NULL;\n\t\tgoto unlock;\n\t}\n\n\t \n\tspin_lock(&inode->i_lock);\n\tinode->i_state |= I_NEW;\n\thlist_add_head_rcu(&inode->i_hash, head);\n\tspin_unlock(&inode->i_lock);\n\n\t \n\tif (list_empty(&inode->i_sb_list))\n\t\tinode_sb_list_add(inode);\nunlock:\n\tspin_unlock(&inode_hash_lock);\n\n\treturn inode;\n}\nEXPORT_SYMBOL(inode_insert5);\n\n \nstruct inode *iget5_locked(struct super_block *sb, unsigned long hashval,\n\t\tint (*test)(struct inode *, void *),\n\t\tint (*set)(struct inode *, void *), void *data)\n{\n\tstruct inode *inode = ilookup5(sb, hashval, test, data);\n\n\tif (!inode) {\n\t\tstruct inode *new = alloc_inode(sb);\n\n\t\tif (new) {\n\t\t\tnew->i_state = 0;\n\t\t\tinode = inode_insert5(new, hashval, test, set, data);\n\t\t\tif (unlikely(inode != new))\n\t\t\t\tdestroy_inode(new);\n\t\t}\n\t}\n\treturn inode;\n}\nEXPORT_SYMBOL(iget5_locked);\n\n \nstruct inode *iget_locked(struct super_block *sb, unsigned long ino)\n{\n\tstruct hlist_head *head = inode_hashtable + hash(sb, ino);\n\tstruct inode *inode;\nagain:\n\tspin_lock(&inode_hash_lock);\n\tinode = find_inode_fast(sb, head, ino);\n\tspin_unlock(&inode_hash_lock);\n\tif (inode) {\n\t\tif (IS_ERR(inode))\n\t\t\treturn NULL;\n\t\twait_on_inode(inode);\n\t\tif (unlikely(inode_unhashed(inode))) {\n\t\t\tiput(inode);\n\t\t\tgoto again;\n\t\t}\n\t\treturn inode;\n\t}\n\n\tinode = alloc_inode(sb);\n\tif (inode) {\n\t\tstruct inode *old;\n\n\t\tspin_lock(&inode_hash_lock);\n\t\t \n\t\told = find_inode_fast(sb, head, ino);\n\t\tif (!old) {\n\t\t\tinode->i_ino = ino;\n\t\t\tspin_lock(&inode->i_lock);\n\t\t\tinode->i_state = I_NEW;\n\t\t\thlist_add_head_rcu(&inode->i_hash, head);\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\tinode_sb_list_add(inode);\n\t\t\tspin_unlock(&inode_hash_lock);\n\n\t\t\t \n\t\t\treturn inode;\n\t\t}\n\n\t\t \n\t\tspin_unlock(&inode_hash_lock);\n\t\tdestroy_inode(inode);\n\t\tif (IS_ERR(old))\n\t\t\treturn NULL;\n\t\tinode = old;\n\t\twait_on_inode(inode);\n\t\tif (unlikely(inode_unhashed(inode))) {\n\t\t\tiput(inode);\n\t\t\tgoto again;\n\t\t}\n\t}\n\treturn inode;\n}\nEXPORT_SYMBOL(iget_locked);\n\n \nstatic int test_inode_iunique(struct super_block *sb, unsigned long ino)\n{\n\tstruct hlist_head *b = inode_hashtable + hash(sb, ino);\n\tstruct inode *inode;\n\n\thlist_for_each_entry_rcu(inode, b, i_hash) {\n\t\tif (inode->i_ino == ino && inode->i_sb == sb)\n\t\t\treturn 0;\n\t}\n\treturn 1;\n}\n\n \nino_t iunique(struct super_block *sb, ino_t max_reserved)\n{\n\t \n\tstatic DEFINE_SPINLOCK(iunique_lock);\n\tstatic unsigned int counter;\n\tino_t res;\n\n\trcu_read_lock();\n\tspin_lock(&iunique_lock);\n\tdo {\n\t\tif (counter <= max_reserved)\n\t\t\tcounter = max_reserved + 1;\n\t\tres = counter++;\n\t} while (!test_inode_iunique(sb, res));\n\tspin_unlock(&iunique_lock);\n\trcu_read_unlock();\n\n\treturn res;\n}\nEXPORT_SYMBOL(iunique);\n\nstruct inode *igrab(struct inode *inode)\n{\n\tspin_lock(&inode->i_lock);\n\tif (!(inode->i_state & (I_FREEING|I_WILL_FREE))) {\n\t\t__iget(inode);\n\t\tspin_unlock(&inode->i_lock);\n\t} else {\n\t\tspin_unlock(&inode->i_lock);\n\t\t \n\t\tinode = NULL;\n\t}\n\treturn inode;\n}\nEXPORT_SYMBOL(igrab);\n\n \nstruct inode *ilookup5_nowait(struct super_block *sb, unsigned long hashval,\n\t\tint (*test)(struct inode *, void *), void *data)\n{\n\tstruct hlist_head *head = inode_hashtable + hash(sb, hashval);\n\tstruct inode *inode;\n\n\tspin_lock(&inode_hash_lock);\n\tinode = find_inode(sb, head, test, data);\n\tspin_unlock(&inode_hash_lock);\n\n\treturn IS_ERR(inode) ? NULL : inode;\n}\nEXPORT_SYMBOL(ilookup5_nowait);\n\n \nstruct inode *ilookup5(struct super_block *sb, unsigned long hashval,\n\t\tint (*test)(struct inode *, void *), void *data)\n{\n\tstruct inode *inode;\nagain:\n\tinode = ilookup5_nowait(sb, hashval, test, data);\n\tif (inode) {\n\t\twait_on_inode(inode);\n\t\tif (unlikely(inode_unhashed(inode))) {\n\t\t\tiput(inode);\n\t\t\tgoto again;\n\t\t}\n\t}\n\treturn inode;\n}\nEXPORT_SYMBOL(ilookup5);\n\n \nstruct inode *ilookup(struct super_block *sb, unsigned long ino)\n{\n\tstruct hlist_head *head = inode_hashtable + hash(sb, ino);\n\tstruct inode *inode;\nagain:\n\tspin_lock(&inode_hash_lock);\n\tinode = find_inode_fast(sb, head, ino);\n\tspin_unlock(&inode_hash_lock);\n\n\tif (inode) {\n\t\tif (IS_ERR(inode))\n\t\t\treturn NULL;\n\t\twait_on_inode(inode);\n\t\tif (unlikely(inode_unhashed(inode))) {\n\t\t\tiput(inode);\n\t\t\tgoto again;\n\t\t}\n\t}\n\treturn inode;\n}\nEXPORT_SYMBOL(ilookup);\n\n \nstruct inode *find_inode_nowait(struct super_block *sb,\n\t\t\t\tunsigned long hashval,\n\t\t\t\tint (*match)(struct inode *, unsigned long,\n\t\t\t\t\t     void *),\n\t\t\t\tvoid *data)\n{\n\tstruct hlist_head *head = inode_hashtable + hash(sb, hashval);\n\tstruct inode *inode, *ret_inode = NULL;\n\tint mval;\n\n\tspin_lock(&inode_hash_lock);\n\thlist_for_each_entry(inode, head, i_hash) {\n\t\tif (inode->i_sb != sb)\n\t\t\tcontinue;\n\t\tmval = match(inode, hashval, data);\n\t\tif (mval == 0)\n\t\t\tcontinue;\n\t\tif (mval == 1)\n\t\t\tret_inode = inode;\n\t\tgoto out;\n\t}\nout:\n\tspin_unlock(&inode_hash_lock);\n\treturn ret_inode;\n}\nEXPORT_SYMBOL(find_inode_nowait);\n\n \nstruct inode *find_inode_rcu(struct super_block *sb, unsigned long hashval,\n\t\t\t     int (*test)(struct inode *, void *), void *data)\n{\n\tstruct hlist_head *head = inode_hashtable + hash(sb, hashval);\n\tstruct inode *inode;\n\n\tRCU_LOCKDEP_WARN(!rcu_read_lock_held(),\n\t\t\t \"suspicious find_inode_rcu() usage\");\n\n\thlist_for_each_entry_rcu(inode, head, i_hash) {\n\t\tif (inode->i_sb == sb &&\n\t\t    !(READ_ONCE(inode->i_state) & (I_FREEING | I_WILL_FREE)) &&\n\t\t    test(inode, data))\n\t\t\treturn inode;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(find_inode_rcu);\n\n \nstruct inode *find_inode_by_ino_rcu(struct super_block *sb,\n\t\t\t\t    unsigned long ino)\n{\n\tstruct hlist_head *head = inode_hashtable + hash(sb, ino);\n\tstruct inode *inode;\n\n\tRCU_LOCKDEP_WARN(!rcu_read_lock_held(),\n\t\t\t \"suspicious find_inode_by_ino_rcu() usage\");\n\n\thlist_for_each_entry_rcu(inode, head, i_hash) {\n\t\tif (inode->i_ino == ino &&\n\t\t    inode->i_sb == sb &&\n\t\t    !(READ_ONCE(inode->i_state) & (I_FREEING | I_WILL_FREE)))\n\t\t    return inode;\n\t}\n\treturn NULL;\n}\nEXPORT_SYMBOL(find_inode_by_ino_rcu);\n\nint insert_inode_locked(struct inode *inode)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tino_t ino = inode->i_ino;\n\tstruct hlist_head *head = inode_hashtable + hash(sb, ino);\n\n\twhile (1) {\n\t\tstruct inode *old = NULL;\n\t\tspin_lock(&inode_hash_lock);\n\t\thlist_for_each_entry(old, head, i_hash) {\n\t\t\tif (old->i_ino != ino)\n\t\t\t\tcontinue;\n\t\t\tif (old->i_sb != sb)\n\t\t\t\tcontinue;\n\t\t\tspin_lock(&old->i_lock);\n\t\t\tif (old->i_state & (I_FREEING|I_WILL_FREE)) {\n\t\t\t\tspin_unlock(&old->i_lock);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tif (likely(!old)) {\n\t\t\tspin_lock(&inode->i_lock);\n\t\t\tinode->i_state |= I_NEW | I_CREATING;\n\t\t\thlist_add_head_rcu(&inode->i_hash, head);\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\tspin_unlock(&inode_hash_lock);\n\t\t\treturn 0;\n\t\t}\n\t\tif (unlikely(old->i_state & I_CREATING)) {\n\t\t\tspin_unlock(&old->i_lock);\n\t\t\tspin_unlock(&inode_hash_lock);\n\t\t\treturn -EBUSY;\n\t\t}\n\t\t__iget(old);\n\t\tspin_unlock(&old->i_lock);\n\t\tspin_unlock(&inode_hash_lock);\n\t\twait_on_inode(old);\n\t\tif (unlikely(!inode_unhashed(old))) {\n\t\t\tiput(old);\n\t\t\treturn -EBUSY;\n\t\t}\n\t\tiput(old);\n\t}\n}\nEXPORT_SYMBOL(insert_inode_locked);\n\nint insert_inode_locked4(struct inode *inode, unsigned long hashval,\n\t\tint (*test)(struct inode *, void *), void *data)\n{\n\tstruct inode *old;\n\n\tinode->i_state |= I_CREATING;\n\told = inode_insert5(inode, hashval, test, NULL, data);\n\n\tif (old != inode) {\n\t\tiput(old);\n\t\treturn -EBUSY;\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(insert_inode_locked4);\n\n\nint generic_delete_inode(struct inode *inode)\n{\n\treturn 1;\n}\nEXPORT_SYMBOL(generic_delete_inode);\n\n \nstatic void iput_final(struct inode *inode)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tconst struct super_operations *op = inode->i_sb->s_op;\n\tunsigned long state;\n\tint drop;\n\n\tWARN_ON(inode->i_state & I_NEW);\n\n\tif (op->drop_inode)\n\t\tdrop = op->drop_inode(inode);\n\telse\n\t\tdrop = generic_drop_inode(inode);\n\n\tif (!drop &&\n\t    !(inode->i_state & I_DONTCACHE) &&\n\t    (sb->s_flags & SB_ACTIVE)) {\n\t\t__inode_add_lru(inode, true);\n\t\tspin_unlock(&inode->i_lock);\n\t\treturn;\n\t}\n\n\tstate = inode->i_state;\n\tif (!drop) {\n\t\tWRITE_ONCE(inode->i_state, state | I_WILL_FREE);\n\t\tspin_unlock(&inode->i_lock);\n\n\t\twrite_inode_now(inode, 1);\n\n\t\tspin_lock(&inode->i_lock);\n\t\tstate = inode->i_state;\n\t\tWARN_ON(state & I_NEW);\n\t\tstate &= ~I_WILL_FREE;\n\t}\n\n\tWRITE_ONCE(inode->i_state, state | I_FREEING);\n\tif (!list_empty(&inode->i_lru))\n\t\tinode_lru_list_del(inode);\n\tspin_unlock(&inode->i_lock);\n\n\tevict(inode);\n}\n\n \nvoid iput(struct inode *inode)\n{\n\tif (!inode)\n\t\treturn;\n\tBUG_ON(inode->i_state & I_CLEAR);\nretry:\n\tif (atomic_dec_and_lock(&inode->i_count, &inode->i_lock)) {\n\t\tif (inode->i_nlink && (inode->i_state & I_DIRTY_TIME)) {\n\t\t\tatomic_inc(&inode->i_count);\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t\ttrace_writeback_lazytime_iput(inode);\n\t\t\tmark_inode_dirty_sync(inode);\n\t\t\tgoto retry;\n\t\t}\n\t\tiput_final(inode);\n\t}\n}\nEXPORT_SYMBOL(iput);\n\n#ifdef CONFIG_BLOCK\n \nint bmap(struct inode *inode, sector_t *block)\n{\n\tif (!inode->i_mapping->a_ops->bmap)\n\t\treturn -EINVAL;\n\n\t*block = inode->i_mapping->a_ops->bmap(inode->i_mapping, *block);\n\treturn 0;\n}\nEXPORT_SYMBOL(bmap);\n#endif\n\n \nstatic int relatime_need_update(struct vfsmount *mnt, struct inode *inode,\n\t\t\t     struct timespec64 now)\n{\n\tstruct timespec64 ctime;\n\n\tif (!(mnt->mnt_flags & MNT_RELATIME))\n\t\treturn 1;\n\t \n\tif (timespec64_compare(&inode->i_mtime, &inode->i_atime) >= 0)\n\t\treturn 1;\n\t \n\tctime = inode_get_ctime(inode);\n\tif (timespec64_compare(&ctime, &inode->i_atime) >= 0)\n\t\treturn 1;\n\n\t \n\tif ((long)(now.tv_sec - inode->i_atime.tv_sec) >= 24*60*60)\n\t\treturn 1;\n\t \n\treturn 0;\n}\n\n \nint inode_update_timestamps(struct inode *inode, int flags)\n{\n\tint updated = 0;\n\tstruct timespec64 now;\n\n\tif (flags & (S_MTIME|S_CTIME|S_VERSION)) {\n\t\tstruct timespec64 ctime = inode_get_ctime(inode);\n\n\t\tnow = inode_set_ctime_current(inode);\n\t\tif (!timespec64_equal(&now, &ctime))\n\t\t\tupdated |= S_CTIME;\n\t\tif (!timespec64_equal(&now, &inode->i_mtime)) {\n\t\t\tinode->i_mtime = now;\n\t\t\tupdated |= S_MTIME;\n\t\t}\n\t\tif (IS_I_VERSION(inode) && inode_maybe_inc_iversion(inode, updated))\n\t\t\tupdated |= S_VERSION;\n\t} else {\n\t\tnow = current_time(inode);\n\t}\n\n\tif (flags & S_ATIME) {\n\t\tif (!timespec64_equal(&now, &inode->i_atime)) {\n\t\t\tinode->i_atime = now;\n\t\t\tupdated |= S_ATIME;\n\t\t}\n\t}\n\treturn updated;\n}\nEXPORT_SYMBOL(inode_update_timestamps);\n\n \nint generic_update_time(struct inode *inode, int flags)\n{\n\tint updated = inode_update_timestamps(inode, flags);\n\tint dirty_flags = 0;\n\n\tif (updated & (S_ATIME|S_MTIME|S_CTIME))\n\t\tdirty_flags = inode->i_sb->s_flags & SB_LAZYTIME ? I_DIRTY_TIME : I_DIRTY_SYNC;\n\tif (updated & S_VERSION)\n\t\tdirty_flags |= I_DIRTY_SYNC;\n\t__mark_inode_dirty(inode, dirty_flags);\n\treturn updated;\n}\nEXPORT_SYMBOL(generic_update_time);\n\n \nint inode_update_time(struct inode *inode, int flags)\n{\n\tif (inode->i_op->update_time)\n\t\treturn inode->i_op->update_time(inode, flags);\n\tgeneric_update_time(inode, flags);\n\treturn 0;\n}\nEXPORT_SYMBOL(inode_update_time);\n\n \nbool atime_needs_update(const struct path *path, struct inode *inode)\n{\n\tstruct vfsmount *mnt = path->mnt;\n\tstruct timespec64 now;\n\n\tif (inode->i_flags & S_NOATIME)\n\t\treturn false;\n\n\t \n\tif (HAS_UNMAPPED_ID(mnt_idmap(mnt), inode))\n\t\treturn false;\n\n\tif (IS_NOATIME(inode))\n\t\treturn false;\n\tif ((inode->i_sb->s_flags & SB_NODIRATIME) && S_ISDIR(inode->i_mode))\n\t\treturn false;\n\n\tif (mnt->mnt_flags & MNT_NOATIME)\n\t\treturn false;\n\tif ((mnt->mnt_flags & MNT_NODIRATIME) && S_ISDIR(inode->i_mode))\n\t\treturn false;\n\n\tnow = current_time(inode);\n\n\tif (!relatime_need_update(mnt, inode, now))\n\t\treturn false;\n\n\tif (timespec64_equal(&inode->i_atime, &now))\n\t\treturn false;\n\n\treturn true;\n}\n\nvoid touch_atime(const struct path *path)\n{\n\tstruct vfsmount *mnt = path->mnt;\n\tstruct inode *inode = d_inode(path->dentry);\n\n\tif (!atime_needs_update(path, inode))\n\t\treturn;\n\n\tif (!sb_start_write_trylock(inode->i_sb))\n\t\treturn;\n\n\tif (__mnt_want_write(mnt) != 0)\n\t\tgoto skip_update;\n\t \n\tinode_update_time(inode, S_ATIME);\n\t__mnt_drop_write(mnt);\nskip_update:\n\tsb_end_write(inode->i_sb);\n}\nEXPORT_SYMBOL(touch_atime);\n\n \nint dentry_needs_remove_privs(struct mnt_idmap *idmap,\n\t\t\t      struct dentry *dentry)\n{\n\tstruct inode *inode = d_inode(dentry);\n\tint mask = 0;\n\tint ret;\n\n\tif (IS_NOSEC(inode))\n\t\treturn 0;\n\n\tmask = setattr_should_drop_suidgid(idmap, inode);\n\tret = security_inode_need_killpriv(dentry);\n\tif (ret < 0)\n\t\treturn ret;\n\tif (ret)\n\t\tmask |= ATTR_KILL_PRIV;\n\treturn mask;\n}\n\nstatic int __remove_privs(struct mnt_idmap *idmap,\n\t\t\t  struct dentry *dentry, int kill)\n{\n\tstruct iattr newattrs;\n\n\tnewattrs.ia_valid = ATTR_FORCE | kill;\n\t \n\treturn notify_change(idmap, dentry, &newattrs, NULL);\n}\n\nstatic int __file_remove_privs(struct file *file, unsigned int flags)\n{\n\tstruct dentry *dentry = file_dentry(file);\n\tstruct inode *inode = file_inode(file);\n\tint error = 0;\n\tint kill;\n\n\tif (IS_NOSEC(inode) || !S_ISREG(inode->i_mode))\n\t\treturn 0;\n\n\tkill = dentry_needs_remove_privs(file_mnt_idmap(file), dentry);\n\tif (kill < 0)\n\t\treturn kill;\n\n\tif (kill) {\n\t\tif (flags & IOCB_NOWAIT)\n\t\t\treturn -EAGAIN;\n\n\t\terror = __remove_privs(file_mnt_idmap(file), dentry, kill);\n\t}\n\n\tif (!error)\n\t\tinode_has_no_xattr(inode);\n\treturn error;\n}\n\n \nint file_remove_privs(struct file *file)\n{\n\treturn __file_remove_privs(file, 0);\n}\nEXPORT_SYMBOL(file_remove_privs);\n\nstatic int inode_needs_update_time(struct inode *inode)\n{\n\tint sync_it = 0;\n\tstruct timespec64 now = current_time(inode);\n\tstruct timespec64 ctime;\n\n\t \n\tif (IS_NOCMTIME(inode))\n\t\treturn 0;\n\n\tif (!timespec64_equal(&inode->i_mtime, &now))\n\t\tsync_it = S_MTIME;\n\n\tctime = inode_get_ctime(inode);\n\tif (!timespec64_equal(&ctime, &now))\n\t\tsync_it |= S_CTIME;\n\n\tif (IS_I_VERSION(inode) && inode_iversion_need_inc(inode))\n\t\tsync_it |= S_VERSION;\n\n\treturn sync_it;\n}\n\nstatic int __file_update_time(struct file *file, int sync_mode)\n{\n\tint ret = 0;\n\tstruct inode *inode = file_inode(file);\n\n\t \n\tif (!__mnt_want_write_file(file)) {\n\t\tret = inode_update_time(inode, sync_mode);\n\t\t__mnt_drop_write_file(file);\n\t}\n\n\treturn ret;\n}\n\n \nint file_update_time(struct file *file)\n{\n\tint ret;\n\tstruct inode *inode = file_inode(file);\n\n\tret = inode_needs_update_time(inode);\n\tif (ret <= 0)\n\t\treturn ret;\n\n\treturn __file_update_time(file, ret);\n}\nEXPORT_SYMBOL(file_update_time);\n\n \nstatic int file_modified_flags(struct file *file, int flags)\n{\n\tint ret;\n\tstruct inode *inode = file_inode(file);\n\n\t \n\tret = __file_remove_privs(file, flags);\n\tif (ret)\n\t\treturn ret;\n\n\tif (unlikely(file->f_mode & FMODE_NOCMTIME))\n\t\treturn 0;\n\n\tret = inode_needs_update_time(inode);\n\tif (ret <= 0)\n\t\treturn ret;\n\tif (flags & IOCB_NOWAIT)\n\t\treturn -EAGAIN;\n\n\treturn __file_update_time(file, ret);\n}\n\n \nint file_modified(struct file *file)\n{\n\treturn file_modified_flags(file, 0);\n}\nEXPORT_SYMBOL(file_modified);\n\n \nint kiocb_modified(struct kiocb *iocb)\n{\n\treturn file_modified_flags(iocb->ki_filp, iocb->ki_flags);\n}\nEXPORT_SYMBOL_GPL(kiocb_modified);\n\nint inode_needs_sync(struct inode *inode)\n{\n\tif (IS_SYNC(inode))\n\t\treturn 1;\n\tif (S_ISDIR(inode->i_mode) && IS_DIRSYNC(inode))\n\t\treturn 1;\n\treturn 0;\n}\nEXPORT_SYMBOL(inode_needs_sync);\n\n \nstatic void __wait_on_freeing_inode(struct inode *inode)\n{\n\twait_queue_head_t *wq;\n\tDEFINE_WAIT_BIT(wait, &inode->i_state, __I_NEW);\n\twq = bit_waitqueue(&inode->i_state, __I_NEW);\n\tprepare_to_wait(wq, &wait.wq_entry, TASK_UNINTERRUPTIBLE);\n\tspin_unlock(&inode->i_lock);\n\tspin_unlock(&inode_hash_lock);\n\tschedule();\n\tfinish_wait(wq, &wait.wq_entry);\n\tspin_lock(&inode_hash_lock);\n}\n\nstatic __initdata unsigned long ihash_entries;\nstatic int __init set_ihash_entries(char *str)\n{\n\tif (!str)\n\t\treturn 0;\n\tihash_entries = simple_strtoul(str, &str, 0);\n\treturn 1;\n}\n__setup(\"ihash_entries=\", set_ihash_entries);\n\n \nvoid __init inode_init_early(void)\n{\n\t \n\tif (hashdist)\n\t\treturn;\n\n\tinode_hashtable =\n\t\talloc_large_system_hash(\"Inode-cache\",\n\t\t\t\t\tsizeof(struct hlist_head),\n\t\t\t\t\tihash_entries,\n\t\t\t\t\t14,\n\t\t\t\t\tHASH_EARLY | HASH_ZERO,\n\t\t\t\t\t&i_hash_shift,\n\t\t\t\t\t&i_hash_mask,\n\t\t\t\t\t0,\n\t\t\t\t\t0);\n}\n\nvoid __init inode_init(void)\n{\n\t \n\tinode_cachep = kmem_cache_create(\"inode_cache\",\n\t\t\t\t\t sizeof(struct inode),\n\t\t\t\t\t 0,\n\t\t\t\t\t (SLAB_RECLAIM_ACCOUNT|SLAB_PANIC|\n\t\t\t\t\t SLAB_MEM_SPREAD|SLAB_ACCOUNT),\n\t\t\t\t\t init_once);\n\n\t \n\tif (!hashdist)\n\t\treturn;\n\n\tinode_hashtable =\n\t\talloc_large_system_hash(\"Inode-cache\",\n\t\t\t\t\tsizeof(struct hlist_head),\n\t\t\t\t\tihash_entries,\n\t\t\t\t\t14,\n\t\t\t\t\tHASH_ZERO,\n\t\t\t\t\t&i_hash_shift,\n\t\t\t\t\t&i_hash_mask,\n\t\t\t\t\t0,\n\t\t\t\t\t0);\n}\n\nvoid init_special_inode(struct inode *inode, umode_t mode, dev_t rdev)\n{\n\tinode->i_mode = mode;\n\tif (S_ISCHR(mode)) {\n\t\tinode->i_fop = &def_chr_fops;\n\t\tinode->i_rdev = rdev;\n\t} else if (S_ISBLK(mode)) {\n\t\tif (IS_ENABLED(CONFIG_BLOCK))\n\t\t\tinode->i_fop = &def_blk_fops;\n\t\tinode->i_rdev = rdev;\n\t} else if (S_ISFIFO(mode))\n\t\tinode->i_fop = &pipefifo_fops;\n\telse if (S_ISSOCK(mode))\n\t\t;\t \n\telse\n\t\tprintk(KERN_DEBUG \"init_special_inode: bogus i_mode (%o) for\"\n\t\t\t\t  \" inode %s:%lu\\n\", mode, inode->i_sb->s_id,\n\t\t\t\t  inode->i_ino);\n}\nEXPORT_SYMBOL(init_special_inode);\n\n \nvoid inode_init_owner(struct mnt_idmap *idmap, struct inode *inode,\n\t\t      const struct inode *dir, umode_t mode)\n{\n\tinode_fsuid_set(inode, idmap);\n\tif (dir && dir->i_mode & S_ISGID) {\n\t\tinode->i_gid = dir->i_gid;\n\n\t\t \n\t\tif (S_ISDIR(mode))\n\t\t\tmode |= S_ISGID;\n\t} else\n\t\tinode_fsgid_set(inode, idmap);\n\tinode->i_mode = mode;\n}\nEXPORT_SYMBOL(inode_init_owner);\n\n \nbool inode_owner_or_capable(struct mnt_idmap *idmap,\n\t\t\t    const struct inode *inode)\n{\n\tvfsuid_t vfsuid;\n\tstruct user_namespace *ns;\n\n\tvfsuid = i_uid_into_vfsuid(idmap, inode);\n\tif (vfsuid_eq_kuid(vfsuid, current_fsuid()))\n\t\treturn true;\n\n\tns = current_user_ns();\n\tif (vfsuid_has_mapping(ns, vfsuid) && ns_capable(ns, CAP_FOWNER))\n\t\treturn true;\n\treturn false;\n}\nEXPORT_SYMBOL(inode_owner_or_capable);\n\n \nstatic void __inode_dio_wait(struct inode *inode)\n{\n\twait_queue_head_t *wq = bit_waitqueue(&inode->i_state, __I_DIO_WAKEUP);\n\tDEFINE_WAIT_BIT(q, &inode->i_state, __I_DIO_WAKEUP);\n\n\tdo {\n\t\tprepare_to_wait(wq, &q.wq_entry, TASK_UNINTERRUPTIBLE);\n\t\tif (atomic_read(&inode->i_dio_count))\n\t\t\tschedule();\n\t} while (atomic_read(&inode->i_dio_count));\n\tfinish_wait(wq, &q.wq_entry);\n}\n\n \nvoid inode_dio_wait(struct inode *inode)\n{\n\tif (atomic_read(&inode->i_dio_count))\n\t\t__inode_dio_wait(inode);\n}\nEXPORT_SYMBOL(inode_dio_wait);\n\n \nvoid inode_set_flags(struct inode *inode, unsigned int flags,\n\t\t     unsigned int mask)\n{\n\tWARN_ON_ONCE(flags & ~mask);\n\tset_mask_bits(&inode->i_flags, mask, flags);\n}\nEXPORT_SYMBOL(inode_set_flags);\n\nvoid inode_nohighmem(struct inode *inode)\n{\n\tmapping_set_gfp_mask(inode->i_mapping, GFP_USER);\n}\nEXPORT_SYMBOL(inode_nohighmem);\n\n \nstruct timespec64 timestamp_truncate(struct timespec64 t, struct inode *inode)\n{\n\tstruct super_block *sb = inode->i_sb;\n\tunsigned int gran = sb->s_time_gran;\n\n\tt.tv_sec = clamp(t.tv_sec, sb->s_time_min, sb->s_time_max);\n\tif (unlikely(t.tv_sec == sb->s_time_max || t.tv_sec == sb->s_time_min))\n\t\tt.tv_nsec = 0;\n\n\t \n\tif (gran == 1)\n\t\t;  \n\telse if (gran == NSEC_PER_SEC)\n\t\tt.tv_nsec = 0;\n\telse if (gran > 1 && gran < NSEC_PER_SEC)\n\t\tt.tv_nsec -= t.tv_nsec % gran;\n\telse\n\t\tWARN(1, \"invalid file time granularity: %u\", gran);\n\treturn t;\n}\nEXPORT_SYMBOL(timestamp_truncate);\n\n \nstruct timespec64 current_time(struct inode *inode)\n{\n\tstruct timespec64 now;\n\n\tktime_get_coarse_real_ts64(&now);\n\treturn timestamp_truncate(now, inode);\n}\nEXPORT_SYMBOL(current_time);\n\n \nstruct timespec64 inode_set_ctime_current(struct inode *inode)\n{\n\tstruct timespec64 now = current_time(inode);\n\n\tinode_set_ctime(inode, now.tv_sec, now.tv_nsec);\n\treturn now;\n}\nEXPORT_SYMBOL(inode_set_ctime_current);\n\n \nbool in_group_or_capable(struct mnt_idmap *idmap,\n\t\t\t const struct inode *inode, vfsgid_t vfsgid)\n{\n\tif (vfsgid_in_group_p(vfsgid))\n\t\treturn true;\n\tif (capable_wrt_inode_uidgid(idmap, inode, CAP_FSETID))\n\t\treturn true;\n\treturn false;\n}\n\n \numode_t mode_strip_sgid(struct mnt_idmap *idmap,\n\t\t\tconst struct inode *dir, umode_t mode)\n{\n\tif ((mode & (S_ISGID | S_IXGRP)) != (S_ISGID | S_IXGRP))\n\t\treturn mode;\n\tif (S_ISDIR(mode) || !dir || !(dir->i_mode & S_ISGID))\n\t\treturn mode;\n\tif (in_group_or_capable(idmap, dir, i_gid_into_vfsgid(idmap, dir)))\n\t\treturn mode;\n\treturn mode & ~S_ISGID;\n}\nEXPORT_SYMBOL(mode_strip_sgid);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}