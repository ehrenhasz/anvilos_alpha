{
  "module_name": "write.c",
  "hash_id": "3ee7dba39c9f0a693167051bbebdfaa5e134a81c1967ad9f84e43e0d2c33dff6",
  "original_prompt": "Ingested from linux-6.6.14/fs/nfs/write.c",
  "human_readable_source": "\n \n\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n#include <linux/pagemap.h>\n#include <linux/file.h>\n#include <linux/writeback.h>\n#include <linux/swap.h>\n#include <linux/migrate.h>\n\n#include <linux/sunrpc/clnt.h>\n#include <linux/nfs_fs.h>\n#include <linux/nfs_mount.h>\n#include <linux/nfs_page.h>\n#include <linux/backing-dev.h>\n#include <linux/export.h>\n#include <linux/freezer.h>\n#include <linux/wait.h>\n#include <linux/iversion.h>\n#include <linux/filelock.h>\n\n#include <linux/uaccess.h>\n#include <linux/sched/mm.h>\n\n#include \"delegation.h\"\n#include \"internal.h\"\n#include \"iostat.h\"\n#include \"nfs4_fs.h\"\n#include \"fscache.h\"\n#include \"pnfs.h\"\n\n#include \"nfstrace.h\"\n\n#define NFSDBG_FACILITY\t\tNFSDBG_PAGECACHE\n\n#define MIN_POOL_WRITE\t\t(32)\n#define MIN_POOL_COMMIT\t\t(4)\n\nstruct nfs_io_completion {\n\tvoid (*complete)(void *data);\n\tvoid *data;\n\tstruct kref refcount;\n};\n\n \nstatic void nfs_redirty_request(struct nfs_page *req);\nstatic const struct rpc_call_ops nfs_commit_ops;\nstatic const struct nfs_pgio_completion_ops nfs_async_write_completion_ops;\nstatic const struct nfs_commit_completion_ops nfs_commit_completion_ops;\nstatic const struct nfs_rw_ops nfs_rw_write_ops;\nstatic void nfs_inode_remove_request(struct nfs_page *req);\nstatic void nfs_clear_request_commit(struct nfs_commit_info *cinfo,\n\t\t\t\t     struct nfs_page *req);\nstatic void nfs_init_cinfo_from_inode(struct nfs_commit_info *cinfo,\n\t\t\t\t      struct inode *inode);\nstatic struct nfs_page *\nnfs_page_search_commits_for_head_request_locked(struct nfs_inode *nfsi,\n\t\t\t\t\t\tstruct folio *folio);\n\nstatic struct kmem_cache *nfs_wdata_cachep;\nstatic mempool_t *nfs_wdata_mempool;\nstatic struct kmem_cache *nfs_cdata_cachep;\nstatic mempool_t *nfs_commit_mempool;\n\nstruct nfs_commit_data *nfs_commitdata_alloc(void)\n{\n\tstruct nfs_commit_data *p;\n\n\tp = kmem_cache_zalloc(nfs_cdata_cachep, nfs_io_gfp_mask());\n\tif (!p) {\n\t\tp = mempool_alloc(nfs_commit_mempool, GFP_NOWAIT);\n\t\tif (!p)\n\t\t\treturn NULL;\n\t\tmemset(p, 0, sizeof(*p));\n\t}\n\tINIT_LIST_HEAD(&p->pages);\n\treturn p;\n}\nEXPORT_SYMBOL_GPL(nfs_commitdata_alloc);\n\nvoid nfs_commit_free(struct nfs_commit_data *p)\n{\n\tmempool_free(p, nfs_commit_mempool);\n}\nEXPORT_SYMBOL_GPL(nfs_commit_free);\n\nstatic struct nfs_pgio_header *nfs_writehdr_alloc(void)\n{\n\tstruct nfs_pgio_header *p;\n\n\tp = kmem_cache_zalloc(nfs_wdata_cachep, nfs_io_gfp_mask());\n\tif (!p) {\n\t\tp = mempool_alloc(nfs_wdata_mempool, GFP_NOWAIT);\n\t\tif (!p)\n\t\t\treturn NULL;\n\t\tmemset(p, 0, sizeof(*p));\n\t}\n\tp->rw_mode = FMODE_WRITE;\n\treturn p;\n}\n\nstatic void nfs_writehdr_free(struct nfs_pgio_header *hdr)\n{\n\tmempool_free(hdr, nfs_wdata_mempool);\n}\n\nstatic struct nfs_io_completion *nfs_io_completion_alloc(gfp_t gfp_flags)\n{\n\treturn kmalloc(sizeof(struct nfs_io_completion), gfp_flags);\n}\n\nstatic void nfs_io_completion_init(struct nfs_io_completion *ioc,\n\t\tvoid (*complete)(void *), void *data)\n{\n\tioc->complete = complete;\n\tioc->data = data;\n\tkref_init(&ioc->refcount);\n}\n\nstatic void nfs_io_completion_release(struct kref *kref)\n{\n\tstruct nfs_io_completion *ioc = container_of(kref,\n\t\t\tstruct nfs_io_completion, refcount);\n\tioc->complete(ioc->data);\n\tkfree(ioc);\n}\n\nstatic void nfs_io_completion_get(struct nfs_io_completion *ioc)\n{\n\tif (ioc != NULL)\n\t\tkref_get(&ioc->refcount);\n}\n\nstatic void nfs_io_completion_put(struct nfs_io_completion *ioc)\n{\n\tif (ioc != NULL)\n\t\tkref_put(&ioc->refcount, nfs_io_completion_release);\n}\n\nstatic void\nnfs_page_set_inode_ref(struct nfs_page *req, struct inode *inode)\n{\n\tif (!test_and_set_bit(PG_INODE_REF, &req->wb_flags)) {\n\t\tkref_get(&req->wb_kref);\n\t\tatomic_long_inc(&NFS_I(inode)->nrequests);\n\t}\n}\n\nstatic int\nnfs_cancel_remove_inode(struct nfs_page *req, struct inode *inode)\n{\n\tint ret;\n\n\tif (!test_bit(PG_REMOVE, &req->wb_flags))\n\t\treturn 0;\n\tret = nfs_page_group_lock(req);\n\tif (ret)\n\t\treturn ret;\n\tif (test_and_clear_bit(PG_REMOVE, &req->wb_flags))\n\t\tnfs_page_set_inode_ref(req, inode);\n\tnfs_page_group_unlock(req);\n\treturn 0;\n}\n\nstatic struct nfs_page *nfs_folio_private_request(struct folio *folio)\n{\n\treturn folio_get_private(folio);\n}\n\n \nstatic struct nfs_page *nfs_folio_find_private_request(struct folio *folio)\n{\n\tstruct address_space *mapping = folio_file_mapping(folio);\n\tstruct nfs_page *req;\n\n\tif (!folio_test_private(folio))\n\t\treturn NULL;\n\tspin_lock(&mapping->private_lock);\n\treq = nfs_folio_private_request(folio);\n\tif (req) {\n\t\tWARN_ON_ONCE(req->wb_head != req);\n\t\tkref_get(&req->wb_kref);\n\t}\n\tspin_unlock(&mapping->private_lock);\n\treturn req;\n}\n\nstatic struct nfs_page *nfs_folio_find_swap_request(struct folio *folio)\n{\n\tstruct inode *inode = folio_file_mapping(folio)->host;\n\tstruct nfs_inode *nfsi = NFS_I(inode);\n\tstruct nfs_page *req = NULL;\n\tif (!folio_test_swapcache(folio))\n\t\treturn NULL;\n\tmutex_lock(&nfsi->commit_mutex);\n\tif (folio_test_swapcache(folio)) {\n\t\treq = nfs_page_search_commits_for_head_request_locked(nfsi,\n\t\t\t\t\t\t\t\t      folio);\n\t\tif (req) {\n\t\t\tWARN_ON_ONCE(req->wb_head != req);\n\t\t\tkref_get(&req->wb_kref);\n\t\t}\n\t}\n\tmutex_unlock(&nfsi->commit_mutex);\n\treturn req;\n}\n\n \nstatic struct nfs_page *nfs_folio_find_head_request(struct folio *folio)\n{\n\tstruct nfs_page *req;\n\n\treq = nfs_folio_find_private_request(folio);\n\tif (!req)\n\t\treq = nfs_folio_find_swap_request(folio);\n\treturn req;\n}\n\nstatic struct nfs_page *nfs_folio_find_and_lock_request(struct folio *folio)\n{\n\tstruct inode *inode = folio_file_mapping(folio)->host;\n\tstruct nfs_page *req, *head;\n\tint ret;\n\n\tfor (;;) {\n\t\treq = nfs_folio_find_head_request(folio);\n\t\tif (!req)\n\t\t\treturn req;\n\t\thead = nfs_page_group_lock_head(req);\n\t\tif (head != req)\n\t\t\tnfs_release_request(req);\n\t\tif (IS_ERR(head))\n\t\t\treturn head;\n\t\tret = nfs_cancel_remove_inode(head, inode);\n\t\tif (ret < 0) {\n\t\t\tnfs_unlock_and_release_request(head);\n\t\t\treturn ERR_PTR(ret);\n\t\t}\n\t\t \n\t\tif (head == nfs_folio_private_request(folio))\n\t\t\tbreak;\n\t\tif (folio_test_swapcache(folio))\n\t\t\tbreak;\n\t\tnfs_unlock_and_release_request(head);\n\t}\n\treturn head;\n}\n\n \nstatic void nfs_grow_file(struct folio *folio, unsigned int offset,\n\t\t\t  unsigned int count)\n{\n\tstruct inode *inode = folio_file_mapping(folio)->host;\n\tloff_t end, i_size;\n\tpgoff_t end_index;\n\n\tspin_lock(&inode->i_lock);\n\ti_size = i_size_read(inode);\n\tend_index = ((i_size - 1) >> folio_shift(folio)) << folio_order(folio);\n\tif (i_size > 0 && folio_index(folio) < end_index)\n\t\tgoto out;\n\tend = folio_file_pos(folio) + (loff_t)offset + (loff_t)count;\n\tif (i_size >= end)\n\t\tgoto out;\n\ttrace_nfs_size_grow(inode, end);\n\ti_size_write(inode, end);\n\tNFS_I(inode)->cache_validity &= ~NFS_INO_INVALID_SIZE;\n\tnfs_inc_stats(inode, NFSIOS_EXTENDWRITE);\nout:\n\tspin_unlock(&inode->i_lock);\n\tnfs_fscache_invalidate(inode, 0);\n}\n\n \nstatic void nfs_set_pageerror(struct address_space *mapping)\n{\n\tstruct inode *inode = mapping->host;\n\n\tnfs_zap_mapping(mapping->host, mapping);\n\t \n\tspin_lock(&inode->i_lock);\n\tnfs_set_cache_invalid(inode, NFS_INO_REVAL_FORCED |\n\t\t\t\t\t     NFS_INO_INVALID_CHANGE |\n\t\t\t\t\t     NFS_INO_INVALID_SIZE);\n\tspin_unlock(&inode->i_lock);\n}\n\nstatic void nfs_mapping_set_error(struct folio *folio, int error)\n{\n\tstruct address_space *mapping = folio_file_mapping(folio);\n\n\tfolio_set_error(folio);\n\tfilemap_set_wb_err(mapping, error);\n\tif (mapping->host)\n\t\terrseq_set(&mapping->host->i_sb->s_wb_err,\n\t\t\t   error == -ENOSPC ? -ENOSPC : -EIO);\n\tnfs_set_pageerror(mapping);\n}\n\n \nstatic struct nfs_page *\nnfs_page_group_search_locked(struct nfs_page *head, unsigned int page_offset)\n{\n\tstruct nfs_page *req;\n\n\treq = head;\n\tdo {\n\t\tif (page_offset >= req->wb_pgbase &&\n\t\t    page_offset < (req->wb_pgbase + req->wb_bytes))\n\t\t\treturn req;\n\n\t\treq = req->wb_this_page;\n\t} while (req != head);\n\n\treturn NULL;\n}\n\n \nstatic bool nfs_page_group_covers_page(struct nfs_page *req)\n{\n\tunsigned int len = nfs_folio_length(nfs_page_to_folio(req));\n\tstruct nfs_page *tmp;\n\tunsigned int pos = 0;\n\n\tnfs_page_group_lock(req);\n\n\tfor (;;) {\n\t\ttmp = nfs_page_group_search_locked(req->wb_head, pos);\n\t\tif (!tmp)\n\t\t\tbreak;\n\t\tpos = tmp->wb_pgbase + tmp->wb_bytes;\n\t}\n\n\tnfs_page_group_unlock(req);\n\treturn pos >= len;\n}\n\n \nstatic void nfs_mark_uptodate(struct nfs_page *req)\n{\n\tstruct folio *folio = nfs_page_to_folio(req);\n\n\tif (folio_test_uptodate(folio))\n\t\treturn;\n\tif (!nfs_page_group_covers_page(req))\n\t\treturn;\n\tfolio_mark_uptodate(folio);\n}\n\nstatic int wb_priority(struct writeback_control *wbc)\n{\n\tint ret = 0;\n\n\tif (wbc->sync_mode == WB_SYNC_ALL)\n\t\tret = FLUSH_COND_STABLE;\n\treturn ret;\n}\n\n \n\nint nfs_congestion_kb;\n\n#define NFS_CONGESTION_ON_THRESH \t(nfs_congestion_kb >> (PAGE_SHIFT-10))\n#define NFS_CONGESTION_OFF_THRESH\t\\\n\t(NFS_CONGESTION_ON_THRESH - (NFS_CONGESTION_ON_THRESH >> 2))\n\nstatic void nfs_folio_set_writeback(struct folio *folio)\n{\n\tstruct nfs_server *nfss = NFS_SERVER(folio_file_mapping(folio)->host);\n\n\tfolio_start_writeback(folio);\n\tif (atomic_long_inc_return(&nfss->writeback) > NFS_CONGESTION_ON_THRESH)\n\t\tnfss->write_congested = 1;\n}\n\nstatic void nfs_folio_end_writeback(struct folio *folio)\n{\n\tstruct nfs_server *nfss = NFS_SERVER(folio_file_mapping(folio)->host);\n\n\tfolio_end_writeback(folio);\n\tif (atomic_long_dec_return(&nfss->writeback) <\n\t    NFS_CONGESTION_OFF_THRESH)\n\t\tnfss->write_congested = 0;\n}\n\nstatic void nfs_page_end_writeback(struct nfs_page *req)\n{\n\tif (nfs_page_group_sync_on_bit(req, PG_WB_END)) {\n\t\tnfs_unlock_request(req);\n\t\tnfs_folio_end_writeback(nfs_page_to_folio(req));\n\t} else\n\t\tnfs_unlock_request(req);\n}\n\n \nstatic void\nnfs_destroy_unlinked_subrequests(struct nfs_page *destroy_list,\n\t\t\t\t struct nfs_page *old_head,\n\t\t\t\t struct inode *inode)\n{\n\twhile (destroy_list) {\n\t\tstruct nfs_page *subreq = destroy_list;\n\n\t\tdestroy_list = (subreq->wb_this_page == old_head) ?\n\t\t\t\t   NULL : subreq->wb_this_page;\n\n\t\t \n\t\tnfs_page_set_headlock(subreq);\n\t\tWARN_ON_ONCE(old_head != subreq->wb_head);\n\n\t\t \n\t\tsubreq->wb_this_page = subreq;\n\t\tsubreq->wb_head = subreq;\n\n\t\tclear_bit(PG_REMOVE, &subreq->wb_flags);\n\n\t\t \n\t\tif (!kref_read(&subreq->wb_kref)) {\n\t\t\t \n\t\t\tif (test_and_clear_bit(PG_TEARDOWN, &subreq->wb_flags)) {\n\t\t\t\tnfs_page_clear_headlock(subreq);\n\t\t\t\tnfs_free_request(subreq);\n\t\t\t} else\n\t\t\t\tnfs_page_clear_headlock(subreq);\n\t\t\tcontinue;\n\t\t}\n\t\tnfs_page_clear_headlock(subreq);\n\n\t\tnfs_release_request(old_head);\n\n\t\tif (test_and_clear_bit(PG_INODE_REF, &subreq->wb_flags)) {\n\t\t\tnfs_release_request(subreq);\n\t\t\tatomic_long_dec(&NFS_I(inode)->nrequests);\n\t\t}\n\n\t\t \n\t\tnfs_unlock_and_release_request(subreq);\n\t}\n}\n\n \nvoid nfs_join_page_group(struct nfs_page *head, struct nfs_commit_info *cinfo,\n\t\t\t struct inode *inode)\n{\n\tstruct nfs_page *subreq;\n\tstruct nfs_page *destroy_list = NULL;\n\tunsigned int pgbase, off, bytes;\n\n\tpgbase = head->wb_pgbase;\n\tbytes = head->wb_bytes;\n\toff = head->wb_offset;\n\tfor (subreq = head->wb_this_page; subreq != head;\n\t\t\tsubreq = subreq->wb_this_page) {\n\t\t \n\t\tif (pgbase > subreq->wb_pgbase) {\n\t\t\toff -= pgbase - subreq->wb_pgbase;\n\t\t\tbytes += pgbase - subreq->wb_pgbase;\n\t\t\tpgbase = subreq->wb_pgbase;\n\t\t}\n\t\tbytes = max(subreq->wb_pgbase + subreq->wb_bytes\n\t\t\t\t- pgbase, bytes);\n\t}\n\n\t \n\thead->wb_pgbase = pgbase;\n\thead->wb_bytes = bytes;\n\thead->wb_offset = off;\n\n\t \n\tsubreq = head;\n\tdo {\n\t\tnfs_clear_request_commit(cinfo, subreq);\n\t\tsubreq = subreq->wb_this_page;\n\t} while (subreq != head);\n\n\t \n\tif (head->wb_this_page != head) {\n\t\t \n\t\tdestroy_list = head->wb_this_page;\n\t\thead->wb_this_page = head;\n\t}\n\n\tnfs_destroy_unlinked_subrequests(destroy_list, head, inode);\n}\n\n \nstatic struct nfs_page *nfs_lock_and_join_requests(struct folio *folio)\n{\n\tstruct inode *inode = folio_file_mapping(folio)->host;\n\tstruct nfs_page *head;\n\tstruct nfs_commit_info cinfo;\n\tint ret;\n\n\tnfs_init_cinfo_from_inode(&cinfo, inode);\n\t \n\thead = nfs_folio_find_and_lock_request(folio);\n\tif (IS_ERR_OR_NULL(head))\n\t\treturn head;\n\n\t \n\tret = nfs_page_group_lock_subrequests(head);\n\tif (ret < 0) {\n\t\tnfs_unlock_and_release_request(head);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tnfs_join_page_group(head, &cinfo, inode);\n\n\treturn head;\n}\n\nstatic void nfs_write_error(struct nfs_page *req, int error)\n{\n\ttrace_nfs_write_error(nfs_page_to_inode(req), req, error);\n\tnfs_mapping_set_error(nfs_page_to_folio(req), error);\n\tnfs_inode_remove_request(req);\n\tnfs_page_end_writeback(req);\n\tnfs_release_request(req);\n}\n\n \nstatic int nfs_page_async_flush(struct folio *folio,\n\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\tstruct nfs_pageio_descriptor *pgio)\n{\n\tstruct nfs_page *req;\n\tint ret = 0;\n\n\treq = nfs_lock_and_join_requests(folio);\n\tif (!req)\n\t\tgoto out;\n\tret = PTR_ERR(req);\n\tif (IS_ERR(req))\n\t\tgoto out;\n\n\tnfs_folio_set_writeback(folio);\n\tWARN_ON_ONCE(test_bit(PG_CLEAN, &req->wb_flags));\n\n\t \n\tret = pgio->pg_error;\n\tif (nfs_error_is_fatal_on_server(ret))\n\t\tgoto out_launder;\n\n\tret = 0;\n\tif (!nfs_pageio_add_request(pgio, req)) {\n\t\tret = pgio->pg_error;\n\t\t \n\t\tif (nfs_error_is_fatal_on_server(ret))\n\t\t\tgoto out_launder;\n\t\tif (wbc->sync_mode == WB_SYNC_NONE)\n\t\t\tret = AOP_WRITEPAGE_ACTIVATE;\n\t\tfolio_redirty_for_writepage(wbc, folio);\n\t\tnfs_redirty_request(req);\n\t\tpgio->pg_error = 0;\n\t} else\n\t\tnfs_add_stats(folio_file_mapping(folio)->host,\n\t\t\t      NFSIOS_WRITEPAGES, 1);\nout:\n\treturn ret;\nout_launder:\n\tnfs_write_error(req, ret);\n\treturn 0;\n}\n\nstatic int nfs_do_writepage(struct folio *folio, struct writeback_control *wbc,\n\t\t\t    struct nfs_pageio_descriptor *pgio)\n{\n\tnfs_pageio_cond_complete(pgio, folio_index(folio));\n\treturn nfs_page_async_flush(folio, wbc, pgio);\n}\n\n \nstatic int nfs_writepage_locked(struct folio *folio,\n\t\t\t\tstruct writeback_control *wbc)\n{\n\tstruct nfs_pageio_descriptor pgio;\n\tstruct inode *inode = folio_file_mapping(folio)->host;\n\tint err;\n\n\tif (wbc->sync_mode == WB_SYNC_NONE &&\n\t    NFS_SERVER(inode)->write_congested)\n\t\treturn AOP_WRITEPAGE_ACTIVATE;\n\n\tnfs_inc_stats(inode, NFSIOS_VFSWRITEPAGE);\n\tnfs_pageio_init_write(&pgio, inode, 0, false,\n\t\t\t      &nfs_async_write_completion_ops);\n\terr = nfs_do_writepage(folio, wbc, &pgio);\n\tpgio.pg_error = 0;\n\tnfs_pageio_complete(&pgio);\n\treturn err;\n}\n\nint nfs_writepage(struct page *page, struct writeback_control *wbc)\n{\n\tstruct folio *folio = page_folio(page);\n\tint ret;\n\n\tret = nfs_writepage_locked(folio, wbc);\n\tif (ret != AOP_WRITEPAGE_ACTIVATE)\n\t\tunlock_page(page);\n\treturn ret;\n}\n\nstatic int nfs_writepages_callback(struct folio *folio,\n\t\t\t\t   struct writeback_control *wbc, void *data)\n{\n\tint ret;\n\n\tret = nfs_do_writepage(folio, wbc, data);\n\tif (ret != AOP_WRITEPAGE_ACTIVATE)\n\t\tfolio_unlock(folio);\n\treturn ret;\n}\n\nstatic void nfs_io_completion_commit(void *inode)\n{\n\tnfs_commit_inode(inode, 0);\n}\n\nint nfs_writepages(struct address_space *mapping, struct writeback_control *wbc)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct nfs_pageio_descriptor pgio;\n\tstruct nfs_io_completion *ioc = NULL;\n\tunsigned int mntflags = NFS_SERVER(inode)->flags;\n\tint priority = 0;\n\tint err;\n\n\tif (wbc->sync_mode == WB_SYNC_NONE &&\n\t    NFS_SERVER(inode)->write_congested)\n\t\treturn 0;\n\n\tnfs_inc_stats(inode, NFSIOS_VFSWRITEPAGES);\n\n\tif (!(mntflags & NFS_MOUNT_WRITE_EAGER) || wbc->for_kupdate ||\n\t    wbc->for_background || wbc->for_sync || wbc->for_reclaim) {\n\t\tioc = nfs_io_completion_alloc(GFP_KERNEL);\n\t\tif (ioc)\n\t\t\tnfs_io_completion_init(ioc, nfs_io_completion_commit,\n\t\t\t\t\t       inode);\n\t\tpriority = wb_priority(wbc);\n\t}\n\n\tdo {\n\t\tnfs_pageio_init_write(&pgio, inode, priority, false,\n\t\t\t\t      &nfs_async_write_completion_ops);\n\t\tpgio.pg_io_completion = ioc;\n\t\terr = write_cache_pages(mapping, wbc, nfs_writepages_callback,\n\t\t\t\t\t&pgio);\n\t\tpgio.pg_error = 0;\n\t\tnfs_pageio_complete(&pgio);\n\t} while (err < 0 && !nfs_error_is_fatal(err));\n\tnfs_io_completion_put(ioc);\n\n\tif (err < 0)\n\t\tgoto out_err;\n\treturn 0;\nout_err:\n\treturn err;\n}\n\n \nstatic void nfs_inode_add_request(struct nfs_page *req)\n{\n\tstruct folio *folio = nfs_page_to_folio(req);\n\tstruct address_space *mapping = folio_file_mapping(folio);\n\tstruct nfs_inode *nfsi = NFS_I(mapping->host);\n\n\tWARN_ON_ONCE(req->wb_this_page != req);\n\n\t \n\tnfs_lock_request(req);\n\n\t \n\tspin_lock(&mapping->private_lock);\n\tif (likely(!folio_test_swapcache(folio))) {\n\t\tset_bit(PG_MAPPED, &req->wb_flags);\n\t\tfolio_set_private(folio);\n\t\tfolio->private = req;\n\t}\n\tspin_unlock(&mapping->private_lock);\n\tatomic_long_inc(&nfsi->nrequests);\n\t \n\tWARN_ON(test_and_set_bit(PG_INODE_REF, &req->wb_flags));\n\tkref_get(&req->wb_kref);\n}\n\n \nstatic void nfs_inode_remove_request(struct nfs_page *req)\n{\n\tstruct nfs_inode *nfsi = NFS_I(nfs_page_to_inode(req));\n\n\tif (nfs_page_group_sync_on_bit(req, PG_REMOVE)) {\n\t\tstruct folio *folio = nfs_page_to_folio(req->wb_head);\n\t\tstruct address_space *mapping = folio_file_mapping(folio);\n\n\t\tspin_lock(&mapping->private_lock);\n\t\tif (likely(folio && !folio_test_swapcache(folio))) {\n\t\t\tfolio->private = NULL;\n\t\t\tfolio_clear_private(folio);\n\t\t\tclear_bit(PG_MAPPED, &req->wb_head->wb_flags);\n\t\t}\n\t\tspin_unlock(&mapping->private_lock);\n\t}\n\n\tif (test_and_clear_bit(PG_INODE_REF, &req->wb_flags)) {\n\t\tatomic_long_dec(&nfsi->nrequests);\n\t\tnfs_release_request(req);\n\t}\n}\n\nstatic void nfs_mark_request_dirty(struct nfs_page *req)\n{\n\tstruct folio *folio = nfs_page_to_folio(req);\n\tif (folio)\n\t\tfilemap_dirty_folio(folio_mapping(folio), folio);\n}\n\n \nstatic struct nfs_page *\nnfs_page_search_commits_for_head_request_locked(struct nfs_inode *nfsi,\n\t\t\t\t\t\tstruct folio *folio)\n{\n\tstruct nfs_page *freq, *t;\n\tstruct nfs_commit_info cinfo;\n\tstruct inode *inode = &nfsi->vfs_inode;\n\n\tnfs_init_cinfo_from_inode(&cinfo, inode);\n\n\t \n\tfreq = pnfs_search_commit_reqs(inode, &cinfo, folio);\n\tif (freq)\n\t\treturn freq->wb_head;\n\n\t \n\tlist_for_each_entry_safe(freq, t, &cinfo.mds->list, wb_list) {\n\t\tif (nfs_page_to_folio(freq) == folio)\n\t\t\treturn freq->wb_head;\n\t}\n\n\treturn NULL;\n}\n\n \nvoid\nnfs_request_add_commit_list_locked(struct nfs_page *req, struct list_head *dst,\n\t\t\t    struct nfs_commit_info *cinfo)\n{\n\tset_bit(PG_CLEAN, &req->wb_flags);\n\tnfs_list_add_request(req, dst);\n\tatomic_long_inc(&cinfo->mds->ncommit);\n}\nEXPORT_SYMBOL_GPL(nfs_request_add_commit_list_locked);\n\n \nvoid\nnfs_request_add_commit_list(struct nfs_page *req, struct nfs_commit_info *cinfo)\n{\n\tmutex_lock(&NFS_I(cinfo->inode)->commit_mutex);\n\tnfs_request_add_commit_list_locked(req, &cinfo->mds->list, cinfo);\n\tmutex_unlock(&NFS_I(cinfo->inode)->commit_mutex);\n\tnfs_folio_mark_unstable(nfs_page_to_folio(req), cinfo);\n}\nEXPORT_SYMBOL_GPL(nfs_request_add_commit_list);\n\n \nvoid\nnfs_request_remove_commit_list(struct nfs_page *req,\n\t\t\t       struct nfs_commit_info *cinfo)\n{\n\tif (!test_and_clear_bit(PG_CLEAN, &(req)->wb_flags))\n\t\treturn;\n\tnfs_list_remove_request(req);\n\tatomic_long_dec(&cinfo->mds->ncommit);\n}\nEXPORT_SYMBOL_GPL(nfs_request_remove_commit_list);\n\nstatic void nfs_init_cinfo_from_inode(struct nfs_commit_info *cinfo,\n\t\t\t\t      struct inode *inode)\n{\n\tcinfo->inode = inode;\n\tcinfo->mds = &NFS_I(inode)->commit_info;\n\tcinfo->ds = pnfs_get_ds_info(inode);\n\tcinfo->dreq = NULL;\n\tcinfo->completion_ops = &nfs_commit_completion_ops;\n}\n\nvoid nfs_init_cinfo(struct nfs_commit_info *cinfo,\n\t\t    struct inode *inode,\n\t\t    struct nfs_direct_req *dreq)\n{\n\tif (dreq)\n\t\tnfs_init_cinfo_from_dreq(cinfo, dreq);\n\telse\n\t\tnfs_init_cinfo_from_inode(cinfo, inode);\n}\nEXPORT_SYMBOL_GPL(nfs_init_cinfo);\n\n \nvoid\nnfs_mark_request_commit(struct nfs_page *req, struct pnfs_layout_segment *lseg,\n\t\t\tstruct nfs_commit_info *cinfo, u32 ds_commit_idx)\n{\n\tif (pnfs_mark_request_commit(req, lseg, cinfo, ds_commit_idx))\n\t\treturn;\n\tnfs_request_add_commit_list(req, cinfo);\n}\n\nstatic void nfs_folio_clear_commit(struct folio *folio)\n{\n\tif (folio) {\n\t\tlong nr = folio_nr_pages(folio);\n\n\t\tnode_stat_mod_folio(folio, NR_WRITEBACK, -nr);\n\t\twb_stat_mod(&inode_to_bdi(folio_file_mapping(folio)->host)->wb,\n\t\t\t    WB_WRITEBACK, -nr);\n\t}\n}\n\n \nstatic void nfs_clear_request_commit(struct nfs_commit_info *cinfo,\n\t\t\t\t     struct nfs_page *req)\n{\n\tif (test_bit(PG_CLEAN, &req->wb_flags)) {\n\t\tstruct nfs_open_context *ctx = nfs_req_openctx(req);\n\t\tstruct inode *inode = d_inode(ctx->dentry);\n\n\t\tmutex_lock(&NFS_I(inode)->commit_mutex);\n\t\tif (!pnfs_clear_request_commit(req, cinfo)) {\n\t\t\tnfs_request_remove_commit_list(req, cinfo);\n\t\t}\n\t\tmutex_unlock(&NFS_I(inode)->commit_mutex);\n\t\tnfs_folio_clear_commit(nfs_page_to_folio(req));\n\t}\n}\n\nint nfs_write_need_commit(struct nfs_pgio_header *hdr)\n{\n\tif (hdr->verf.committed == NFS_DATA_SYNC)\n\t\treturn hdr->lseg == NULL;\n\treturn hdr->verf.committed != NFS_FILE_SYNC;\n}\n\nstatic void nfs_async_write_init(struct nfs_pgio_header *hdr)\n{\n\tnfs_io_completion_get(hdr->io_completion);\n}\n\nstatic void nfs_write_completion(struct nfs_pgio_header *hdr)\n{\n\tstruct nfs_commit_info cinfo;\n\tunsigned long bytes = 0;\n\n\tif (test_bit(NFS_IOHDR_REDO, &hdr->flags))\n\t\tgoto out;\n\tnfs_init_cinfo_from_inode(&cinfo, hdr->inode);\n\twhile (!list_empty(&hdr->pages)) {\n\t\tstruct nfs_page *req = nfs_list_entry(hdr->pages.next);\n\n\t\tbytes += req->wb_bytes;\n\t\tnfs_list_remove_request(req);\n\t\tif (test_bit(NFS_IOHDR_ERROR, &hdr->flags) &&\n\t\t    (hdr->good_bytes < bytes)) {\n\t\t\ttrace_nfs_comp_error(hdr->inode, req, hdr->error);\n\t\t\tnfs_mapping_set_error(nfs_page_to_folio(req),\n\t\t\t\t\t      hdr->error);\n\t\t\tgoto remove_req;\n\t\t}\n\t\tif (nfs_write_need_commit(hdr)) {\n\t\t\t \n\t\t\treq->wb_nio = 0;\n\t\t\tmemcpy(&req->wb_verf, &hdr->verf.verifier, sizeof(req->wb_verf));\n\t\t\tnfs_mark_request_commit(req, hdr->lseg, &cinfo,\n\t\t\t\thdr->pgio_mirror_idx);\n\t\t\tgoto next;\n\t\t}\nremove_req:\n\t\tnfs_inode_remove_request(req);\nnext:\n\t\tnfs_page_end_writeback(req);\n\t\tnfs_release_request(req);\n\t}\nout:\n\tnfs_io_completion_put(hdr->io_completion);\n\thdr->release(hdr);\n}\n\nunsigned long\nnfs_reqs_to_commit(struct nfs_commit_info *cinfo)\n{\n\treturn atomic_long_read(&cinfo->mds->ncommit);\n}\n\n \nint\nnfs_scan_commit_list(struct list_head *src, struct list_head *dst,\n\t\t     struct nfs_commit_info *cinfo, int max)\n{\n\tstruct nfs_page *req, *tmp;\n\tint ret = 0;\n\n\tlist_for_each_entry_safe(req, tmp, src, wb_list) {\n\t\tkref_get(&req->wb_kref);\n\t\tif (!nfs_lock_request(req)) {\n\t\t\tnfs_release_request(req);\n\t\t\tcontinue;\n\t\t}\n\t\tnfs_request_remove_commit_list(req, cinfo);\n\t\tclear_bit(PG_COMMIT_TO_DS, &req->wb_flags);\n\t\tnfs_list_add_request(req, dst);\n\t\tret++;\n\t\tif ((ret == max) && !cinfo->dreq)\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nfs_scan_commit_list);\n\n \nint\nnfs_scan_commit(struct inode *inode, struct list_head *dst,\n\t\tstruct nfs_commit_info *cinfo)\n{\n\tint ret = 0;\n\n\tif (!atomic_long_read(&cinfo->mds->ncommit))\n\t\treturn 0;\n\tmutex_lock(&NFS_I(cinfo->inode)->commit_mutex);\n\tif (atomic_long_read(&cinfo->mds->ncommit) > 0) {\n\t\tconst int max = INT_MAX;\n\n\t\tret = nfs_scan_commit_list(&cinfo->mds->list, dst,\n\t\t\t\t\t   cinfo, max);\n\t\tret += pnfs_scan_commit_lists(inode, cinfo, max - ret);\n\t}\n\tmutex_unlock(&NFS_I(cinfo->inode)->commit_mutex);\n\treturn ret;\n}\n\n \nstatic struct nfs_page *nfs_try_to_update_request(struct folio *folio,\n\t\t\t\t\t\t  unsigned int offset,\n\t\t\t\t\t\t  unsigned int bytes)\n{\n\tstruct nfs_page *req;\n\tunsigned int rqend;\n\tunsigned int end;\n\tint error;\n\n\tend = offset + bytes;\n\n\treq = nfs_lock_and_join_requests(folio);\n\tif (IS_ERR_OR_NULL(req))\n\t\treturn req;\n\n\trqend = req->wb_offset + req->wb_bytes;\n\t \n\tif (offset > rqend || end < req->wb_offset)\n\t\tgoto out_flushme;\n\n\t \n\tif (offset < req->wb_offset) {\n\t\treq->wb_offset = offset;\n\t\treq->wb_pgbase = offset;\n\t}\n\tif (end > rqend)\n\t\treq->wb_bytes = end - req->wb_offset;\n\telse\n\t\treq->wb_bytes = rqend - req->wb_offset;\n\treq->wb_nio = 0;\n\treturn req;\nout_flushme:\n\t \n\tnfs_mark_request_dirty(req);\n\tnfs_unlock_and_release_request(req);\n\terror = nfs_wb_folio(folio_file_mapping(folio)->host, folio);\n\treturn (error < 0) ? ERR_PTR(error) : NULL;\n}\n\n \nstatic struct nfs_page *nfs_setup_write_request(struct nfs_open_context *ctx,\n\t\t\t\t\t\tstruct folio *folio,\n\t\t\t\t\t\tunsigned int offset,\n\t\t\t\t\t\tunsigned int bytes)\n{\n\tstruct nfs_page *req;\n\n\treq = nfs_try_to_update_request(folio, offset, bytes);\n\tif (req != NULL)\n\t\tgoto out;\n\treq = nfs_page_create_from_folio(ctx, folio, offset, bytes);\n\tif (IS_ERR(req))\n\t\tgoto out;\n\tnfs_inode_add_request(req);\nout:\n\treturn req;\n}\n\nstatic int nfs_writepage_setup(struct nfs_open_context *ctx,\n\t\t\t       struct folio *folio, unsigned int offset,\n\t\t\t       unsigned int count)\n{\n\tstruct nfs_page *req;\n\n\treq = nfs_setup_write_request(ctx, folio, offset, count);\n\tif (IS_ERR(req))\n\t\treturn PTR_ERR(req);\n\t \n\tnfs_grow_file(folio, offset, count);\n\tnfs_mark_uptodate(req);\n\tnfs_mark_request_dirty(req);\n\tnfs_unlock_and_release_request(req);\n\treturn 0;\n}\n\nint nfs_flush_incompatible(struct file *file, struct folio *folio)\n{\n\tstruct nfs_open_context *ctx = nfs_file_open_context(file);\n\tstruct nfs_lock_context *l_ctx;\n\tstruct file_lock_context *flctx = locks_inode_context(file_inode(file));\n\tstruct nfs_page\t*req;\n\tint do_flush, status;\n\t \n\tdo {\n\t\treq = nfs_folio_find_head_request(folio);\n\t\tif (req == NULL)\n\t\t\treturn 0;\n\t\tl_ctx = req->wb_lock_context;\n\t\tdo_flush = nfs_page_to_folio(req) != folio ||\n\t\t\t   !nfs_match_open_context(nfs_req_openctx(req), ctx);\n\t\tif (l_ctx && flctx &&\n\t\t    !(list_empty_careful(&flctx->flc_posix) &&\n\t\t      list_empty_careful(&flctx->flc_flock))) {\n\t\t\tdo_flush |= l_ctx->lockowner != current->files;\n\t\t}\n\t\tnfs_release_request(req);\n\t\tif (!do_flush)\n\t\t\treturn 0;\n\t\tstatus = nfs_wb_folio(folio_file_mapping(folio)->host, folio);\n\t} while (status == 0);\n\treturn status;\n}\n\n \nint\nnfs_key_timeout_notify(struct file *filp, struct inode *inode)\n{\n\tstruct nfs_open_context *ctx = nfs_file_open_context(filp);\n\n\tif (nfs_ctx_key_to_expire(ctx, inode) &&\n\t    !rcu_access_pointer(ctx->ll_cred))\n\t\t \n\t\treturn -EACCES;\n\treturn 0;\n}\n\n \nbool nfs_ctx_key_to_expire(struct nfs_open_context *ctx, struct inode *inode)\n{\n\tstruct rpc_auth *auth = NFS_SERVER(inode)->client->cl_auth;\n\tstruct rpc_cred *cred, *new, *old = NULL;\n\tstruct auth_cred acred = {\n\t\t.cred = ctx->cred,\n\t};\n\tbool ret = false;\n\n\trcu_read_lock();\n\tcred = rcu_dereference(ctx->ll_cred);\n\tif (cred && !(cred->cr_ops->crkey_timeout &&\n\t\t      cred->cr_ops->crkey_timeout(cred)))\n\t\tgoto out;\n\trcu_read_unlock();\n\n\tnew = auth->au_ops->lookup_cred(auth, &acred, 0);\n\tif (new == cred) {\n\t\tput_rpccred(new);\n\t\treturn true;\n\t}\n\tif (IS_ERR_OR_NULL(new)) {\n\t\tnew = NULL;\n\t\tret = true;\n\t} else if (new->cr_ops->crkey_timeout &&\n\t\t   new->cr_ops->crkey_timeout(new))\n\t\tret = true;\n\n\trcu_read_lock();\n\told = rcu_dereference_protected(xchg(&ctx->ll_cred,\n\t\t\t\t\t     RCU_INITIALIZER(new)), 1);\nout:\n\trcu_read_unlock();\n\tput_rpccred(old);\n\treturn ret;\n}\n\n \nstatic bool nfs_folio_write_uptodate(struct folio *folio, unsigned int pagelen)\n{\n\tstruct inode *inode = folio_file_mapping(folio)->host;\n\tstruct nfs_inode *nfsi = NFS_I(inode);\n\n\tif (nfs_have_delegated_attributes(inode))\n\t\tgoto out;\n\tif (nfsi->cache_validity &\n\t    (NFS_INO_INVALID_CHANGE | NFS_INO_INVALID_SIZE))\n\t\treturn false;\n\tsmp_rmb();\n\tif (test_bit(NFS_INO_INVALIDATING, &nfsi->flags) && pagelen != 0)\n\t\treturn false;\nout:\n\tif (nfsi->cache_validity & NFS_INO_INVALID_DATA && pagelen != 0)\n\t\treturn false;\n\treturn folio_test_uptodate(folio) != 0;\n}\n\nstatic bool\nis_whole_file_wrlock(struct file_lock *fl)\n{\n\treturn fl->fl_start == 0 && fl->fl_end == OFFSET_MAX &&\n\t\t\tfl->fl_type == F_WRLCK;\n}\n\n \nstatic int nfs_can_extend_write(struct file *file, struct folio *folio,\n\t\t\t\tunsigned int pagelen)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct file_lock_context *flctx = locks_inode_context(inode);\n\tstruct file_lock *fl;\n\tint ret;\n\n\tif (file->f_flags & O_DSYNC)\n\t\treturn 0;\n\tif (!nfs_folio_write_uptodate(folio, pagelen))\n\t\treturn 0;\n\tif (NFS_PROTO(inode)->have_delegation(inode, FMODE_WRITE))\n\t\treturn 1;\n\tif (!flctx || (list_empty_careful(&flctx->flc_flock) &&\n\t\t       list_empty_careful(&flctx->flc_posix)))\n\t\treturn 1;\n\n\t \n\tret = 0;\n\tspin_lock(&flctx->flc_lock);\n\tif (!list_empty(&flctx->flc_posix)) {\n\t\tfl = list_first_entry(&flctx->flc_posix, struct file_lock,\n\t\t\t\t\tfl_list);\n\t\tif (is_whole_file_wrlock(fl))\n\t\t\tret = 1;\n\t} else if (!list_empty(&flctx->flc_flock)) {\n\t\tfl = list_first_entry(&flctx->flc_flock, struct file_lock,\n\t\t\t\t\tfl_list);\n\t\tif (fl->fl_type == F_WRLCK)\n\t\t\tret = 1;\n\t}\n\tspin_unlock(&flctx->flc_lock);\n\treturn ret;\n}\n\n \nint nfs_update_folio(struct file *file, struct folio *folio,\n\t\t     unsigned int offset, unsigned int count)\n{\n\tstruct nfs_open_context *ctx = nfs_file_open_context(file);\n\tstruct address_space *mapping = folio_file_mapping(folio);\n\tstruct inode *inode = mapping->host;\n\tunsigned int pagelen = nfs_folio_length(folio);\n\tint\t\tstatus = 0;\n\n\tnfs_inc_stats(inode, NFSIOS_VFSUPDATEPAGE);\n\n\tdprintk(\"NFS:       nfs_update_folio(%pD2 %d@%lld)\\n\", file, count,\n\t\t(long long)(folio_file_pos(folio) + offset));\n\n\tif (!count)\n\t\tgoto out;\n\n\tif (nfs_can_extend_write(file, folio, pagelen)) {\n\t\tcount = max(count + offset, pagelen);\n\t\toffset = 0;\n\t}\n\n\tstatus = nfs_writepage_setup(ctx, folio, offset, count);\n\tif (status < 0)\n\t\tnfs_set_pageerror(mapping);\nout:\n\tdprintk(\"NFS:       nfs_update_folio returns %d (isize %lld)\\n\",\n\t\t\tstatus, (long long)i_size_read(inode));\n\treturn status;\n}\n\nstatic int flush_task_priority(int how)\n{\n\tswitch (how & (FLUSH_HIGHPRI|FLUSH_LOWPRI)) {\n\t\tcase FLUSH_HIGHPRI:\n\t\t\treturn RPC_PRIORITY_HIGH;\n\t\tcase FLUSH_LOWPRI:\n\t\t\treturn RPC_PRIORITY_LOW;\n\t}\n\treturn RPC_PRIORITY_NORMAL;\n}\n\nstatic void nfs_initiate_write(struct nfs_pgio_header *hdr,\n\t\t\t       struct rpc_message *msg,\n\t\t\t       const struct nfs_rpc_ops *rpc_ops,\n\t\t\t       struct rpc_task_setup *task_setup_data, int how)\n{\n\tint priority = flush_task_priority(how);\n\n\tif (IS_SWAPFILE(hdr->inode))\n\t\ttask_setup_data->flags |= RPC_TASK_SWAPPER;\n\ttask_setup_data->priority = priority;\n\trpc_ops->write_setup(hdr, msg, &task_setup_data->rpc_client);\n\ttrace_nfs_initiate_write(hdr);\n}\n\n \nstatic void nfs_redirty_request(struct nfs_page *req)\n{\n\tstruct nfs_inode *nfsi = NFS_I(nfs_page_to_inode(req));\n\n\t \n\treq->wb_nio++;\n\tnfs_mark_request_dirty(req);\n\tatomic_long_inc(&nfsi->redirtied_pages);\n\tnfs_page_end_writeback(req);\n\tnfs_release_request(req);\n}\n\nstatic void nfs_async_write_error(struct list_head *head, int error)\n{\n\tstruct nfs_page\t*req;\n\n\twhile (!list_empty(head)) {\n\t\treq = nfs_list_entry(head->next);\n\t\tnfs_list_remove_request(req);\n\t\tif (nfs_error_is_fatal_on_server(error))\n\t\t\tnfs_write_error(req, error);\n\t\telse\n\t\t\tnfs_redirty_request(req);\n\t}\n}\n\nstatic void nfs_async_write_reschedule_io(struct nfs_pgio_header *hdr)\n{\n\tnfs_async_write_error(&hdr->pages, 0);\n}\n\nstatic const struct nfs_pgio_completion_ops nfs_async_write_completion_ops = {\n\t.init_hdr = nfs_async_write_init,\n\t.error_cleanup = nfs_async_write_error,\n\t.completion = nfs_write_completion,\n\t.reschedule_io = nfs_async_write_reschedule_io,\n};\n\nvoid nfs_pageio_init_write(struct nfs_pageio_descriptor *pgio,\n\t\t\t       struct inode *inode, int ioflags, bool force_mds,\n\t\t\t       const struct nfs_pgio_completion_ops *compl_ops)\n{\n\tstruct nfs_server *server = NFS_SERVER(inode);\n\tconst struct nfs_pageio_ops *pg_ops = &nfs_pgio_rw_ops;\n\n#ifdef CONFIG_NFS_V4_1\n\tif (server->pnfs_curr_ld && !force_mds)\n\t\tpg_ops = server->pnfs_curr_ld->pg_write_ops;\n#endif\n\tnfs_pageio_init(pgio, inode, pg_ops, compl_ops, &nfs_rw_write_ops,\n\t\t\tserver->wsize, ioflags);\n}\nEXPORT_SYMBOL_GPL(nfs_pageio_init_write);\n\nvoid nfs_pageio_reset_write_mds(struct nfs_pageio_descriptor *pgio)\n{\n\tstruct nfs_pgio_mirror *mirror;\n\n\tif (pgio->pg_ops && pgio->pg_ops->pg_cleanup)\n\t\tpgio->pg_ops->pg_cleanup(pgio);\n\n\tpgio->pg_ops = &nfs_pgio_rw_ops;\n\n\tnfs_pageio_stop_mirroring(pgio);\n\n\tmirror = &pgio->pg_mirrors[0];\n\tmirror->pg_bsize = NFS_SERVER(pgio->pg_inode)->wsize;\n}\nEXPORT_SYMBOL_GPL(nfs_pageio_reset_write_mds);\n\n\nvoid nfs_commit_prepare(struct rpc_task *task, void *calldata)\n{\n\tstruct nfs_commit_data *data = calldata;\n\n\tNFS_PROTO(data->inode)->commit_rpc_prepare(task, data);\n}\n\nstatic void nfs_writeback_check_extend(struct nfs_pgio_header *hdr,\n\t\tstruct nfs_fattr *fattr)\n{\n\tstruct nfs_pgio_args *argp = &hdr->args;\n\tstruct nfs_pgio_res *resp = &hdr->res;\n\tu64 size = argp->offset + resp->count;\n\n\tif (!(fattr->valid & NFS_ATTR_FATTR_SIZE))\n\t\tfattr->size = size;\n\tif (nfs_size_to_loff_t(fattr->size) < i_size_read(hdr->inode)) {\n\t\tfattr->valid &= ~NFS_ATTR_FATTR_SIZE;\n\t\treturn;\n\t}\n\tif (size != fattr->size)\n\t\treturn;\n\t \n\tnfs_fattr_set_barrier(fattr);\n\t \n\tfattr->valid |= NFS_ATTR_FATTR_SIZE;\n}\n\nvoid nfs_writeback_update_inode(struct nfs_pgio_header *hdr)\n{\n\tstruct nfs_fattr *fattr = &hdr->fattr;\n\tstruct inode *inode = hdr->inode;\n\n\tspin_lock(&inode->i_lock);\n\tnfs_writeback_check_extend(hdr, fattr);\n\tnfs_post_op_update_inode_force_wcc_locked(inode, fattr);\n\tspin_unlock(&inode->i_lock);\n}\nEXPORT_SYMBOL_GPL(nfs_writeback_update_inode);\n\n \nstatic int nfs_writeback_done(struct rpc_task *task,\n\t\t\t      struct nfs_pgio_header *hdr,\n\t\t\t      struct inode *inode)\n{\n\tint status;\n\n\t \n\tstatus = NFS_PROTO(inode)->write_done(task, hdr);\n\tif (status != 0)\n\t\treturn status;\n\n\tnfs_add_stats(inode, NFSIOS_SERVERWRITTENBYTES, hdr->res.count);\n\ttrace_nfs_writeback_done(task, hdr);\n\n\tif (task->tk_status >= 0) {\n\t\tenum nfs3_stable_how committed = hdr->res.verf->committed;\n\n\t\tif (committed == NFS_UNSTABLE) {\n\t\t\t \n\t\t\tset_bit(NFS_IOHDR_UNSTABLE_WRITES, &hdr->flags);\n\t\t}\n\n\t\tif (committed < hdr->args.stable) {\n\t\t\t \n\t\t\tstatic unsigned long    complain;\n\n\t\t\t \n\t\t\tif (time_before(complain, jiffies)) {\n\t\t\t\tdprintk(\"NFS:       faulty NFS server %s:\"\n\t\t\t\t\t\" (committed = %d) != (stable = %d)\\n\",\n\t\t\t\t\tNFS_SERVER(inode)->nfs_client->cl_hostname,\n\t\t\t\t\tcommitted, hdr->args.stable);\n\t\t\t\tcomplain = jiffies + 300 * HZ;\n\t\t\t}\n\t\t}\n\t}\n\n\t \n\tif (nfs_should_remove_suid(inode)) {\n\t\tspin_lock(&inode->i_lock);\n\t\tnfs_set_cache_invalid(inode, NFS_INO_INVALID_MODE);\n\t\tspin_unlock(&inode->i_lock);\n\t}\n\treturn 0;\n}\n\n \nstatic void nfs_writeback_result(struct rpc_task *task,\n\t\t\t\t struct nfs_pgio_header *hdr)\n{\n\tstruct nfs_pgio_args\t*argp = &hdr->args;\n\tstruct nfs_pgio_res\t*resp = &hdr->res;\n\n\tif (resp->count < argp->count) {\n\t\tstatic unsigned long    complain;\n\n\t\t \n\t\tnfs_inc_stats(hdr->inode, NFSIOS_SHORTWRITE);\n\n\t\t \n\t\tif (resp->count == 0) {\n\t\t\tif (time_before(complain, jiffies)) {\n\t\t\t\tprintk(KERN_WARNING\n\t\t\t\t       \"NFS: Server wrote zero bytes, expected %u.\\n\",\n\t\t\t\t       argp->count);\n\t\t\t\tcomplain = jiffies + 300 * HZ;\n\t\t\t}\n\t\t\tnfs_set_pgio_error(hdr, -EIO, argp->offset);\n\t\t\ttask->tk_status = -EIO;\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tif (!task->tk_ops) {\n\t\t\thdr->pnfs_error = -EAGAIN;\n\t\t\treturn;\n\t\t}\n\n\t\t \n\t\tif (resp->verf->committed != NFS_UNSTABLE) {\n\t\t\t \n\t\t\thdr->mds_offset += resp->count;\n\t\t\targp->offset += resp->count;\n\t\t\targp->pgbase += resp->count;\n\t\t\targp->count -= resp->count;\n\t\t} else {\n\t\t\t \n\t\t\targp->stable = NFS_FILE_SYNC;\n\t\t}\n\t\tresp->count = 0;\n\t\tresp->verf->committed = 0;\n\t\trpc_restart_call_prepare(task);\n\t}\n}\n\nstatic int wait_on_commit(struct nfs_mds_commit_info *cinfo)\n{\n\treturn wait_var_event_killable(&cinfo->rpcs_out,\n\t\t\t\t       !atomic_read(&cinfo->rpcs_out));\n}\n\nstatic void nfs_commit_begin(struct nfs_mds_commit_info *cinfo)\n{\n\tatomic_inc(&cinfo->rpcs_out);\n}\n\nbool nfs_commit_end(struct nfs_mds_commit_info *cinfo)\n{\n\tif (atomic_dec_and_test(&cinfo->rpcs_out)) {\n\t\twake_up_var(&cinfo->rpcs_out);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\nvoid nfs_commitdata_release(struct nfs_commit_data *data)\n{\n\tput_nfs_open_context(data->context);\n\tnfs_commit_free(data);\n}\nEXPORT_SYMBOL_GPL(nfs_commitdata_release);\n\nint nfs_initiate_commit(struct rpc_clnt *clnt, struct nfs_commit_data *data,\n\t\t\tconst struct nfs_rpc_ops *nfs_ops,\n\t\t\tconst struct rpc_call_ops *call_ops,\n\t\t\tint how, int flags)\n{\n\tstruct rpc_task *task;\n\tint priority = flush_task_priority(how);\n\tstruct rpc_message msg = {\n\t\t.rpc_argp = &data->args,\n\t\t.rpc_resp = &data->res,\n\t\t.rpc_cred = data->cred,\n\t};\n\tstruct rpc_task_setup task_setup_data = {\n\t\t.task = &data->task,\n\t\t.rpc_client = clnt,\n\t\t.rpc_message = &msg,\n\t\t.callback_ops = call_ops,\n\t\t.callback_data = data,\n\t\t.workqueue = nfsiod_workqueue,\n\t\t.flags = RPC_TASK_ASYNC | flags,\n\t\t.priority = priority,\n\t};\n\n\tif (nfs_server_capable(data->inode, NFS_CAP_MOVEABLE))\n\t\ttask_setup_data.flags |= RPC_TASK_MOVEABLE;\n\n\t \n\tnfs_ops->commit_setup(data, &msg, &task_setup_data.rpc_client);\n\ttrace_nfs_initiate_commit(data);\n\n\tdprintk(\"NFS: initiated commit call\\n\");\n\n\ttask = rpc_run_task(&task_setup_data);\n\tif (IS_ERR(task))\n\t\treturn PTR_ERR(task);\n\tif (how & FLUSH_SYNC)\n\t\trpc_wait_for_completion_task(task);\n\trpc_put_task(task);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(nfs_initiate_commit);\n\nstatic loff_t nfs_get_lwb(struct list_head *head)\n{\n\tloff_t lwb = 0;\n\tstruct nfs_page *req;\n\n\tlist_for_each_entry(req, head, wb_list)\n\t\tif (lwb < (req_offset(req) + req->wb_bytes))\n\t\t\tlwb = req_offset(req) + req->wb_bytes;\n\n\treturn lwb;\n}\n\n \nvoid nfs_init_commit(struct nfs_commit_data *data,\n\t\t     struct list_head *head,\n\t\t     struct pnfs_layout_segment *lseg,\n\t\t     struct nfs_commit_info *cinfo)\n{\n\tstruct nfs_page *first;\n\tstruct nfs_open_context *ctx;\n\tstruct inode *inode;\n\n\t \n\n\tif (head)\n\t\tlist_splice_init(head, &data->pages);\n\n\tfirst = nfs_list_entry(data->pages.next);\n\tctx = nfs_req_openctx(first);\n\tinode = d_inode(ctx->dentry);\n\n\tdata->inode\t  = inode;\n\tdata->cred\t  = ctx->cred;\n\tdata->lseg\t  = lseg;  \n\t \n\tif (lseg)\n\t\tdata->lwb = nfs_get_lwb(&data->pages);\n\tdata->mds_ops     = &nfs_commit_ops;\n\tdata->completion_ops = cinfo->completion_ops;\n\tdata->dreq\t  = cinfo->dreq;\n\n\tdata->args.fh     = NFS_FH(data->inode);\n\t \n\tdata->args.offset = 0;\n\tdata->args.count  = 0;\n\tdata->context     = get_nfs_open_context(ctx);\n\tdata->res.fattr   = &data->fattr;\n\tdata->res.verf    = &data->verf;\n\tnfs_fattr_init(&data->fattr);\n\tnfs_commit_begin(cinfo->mds);\n}\nEXPORT_SYMBOL_GPL(nfs_init_commit);\n\nvoid nfs_retry_commit(struct list_head *page_list,\n\t\t      struct pnfs_layout_segment *lseg,\n\t\t      struct nfs_commit_info *cinfo,\n\t\t      u32 ds_commit_idx)\n{\n\tstruct nfs_page *req;\n\n\twhile (!list_empty(page_list)) {\n\t\treq = nfs_list_entry(page_list->next);\n\t\tnfs_list_remove_request(req);\n\t\tnfs_mark_request_commit(req, lseg, cinfo, ds_commit_idx);\n\t\tnfs_folio_clear_commit(nfs_page_to_folio(req));\n\t\tnfs_unlock_and_release_request(req);\n\t}\n}\nEXPORT_SYMBOL_GPL(nfs_retry_commit);\n\nstatic void nfs_commit_resched_write(struct nfs_commit_info *cinfo,\n\t\t\t\t     struct nfs_page *req)\n{\n\tstruct folio *folio = nfs_page_to_folio(req);\n\n\tfilemap_dirty_folio(folio_mapping(folio), folio);\n}\n\n \nstatic int\nnfs_commit_list(struct inode *inode, struct list_head *head, int how,\n\t\tstruct nfs_commit_info *cinfo)\n{\n\tstruct nfs_commit_data\t*data;\n\tunsigned short task_flags = 0;\n\n\t \n\tif (list_empty(head))\n\t\treturn 0;\n\n\tdata = nfs_commitdata_alloc();\n\tif (!data) {\n\t\tnfs_retry_commit(head, NULL, cinfo, -1);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tnfs_init_commit(data, head, NULL, cinfo);\n\tif (NFS_SERVER(inode)->nfs_client->cl_minorversion)\n\t\ttask_flags = RPC_TASK_MOVEABLE;\n\treturn nfs_initiate_commit(NFS_CLIENT(inode), data, NFS_PROTO(inode),\n\t\t\t\t   data->mds_ops, how,\n\t\t\t\t   RPC_TASK_CRED_NOREF | task_flags);\n}\n\n \nstatic void nfs_commit_done(struct rpc_task *task, void *calldata)\n{\n\tstruct nfs_commit_data\t*data = calldata;\n\n\t \n\tNFS_PROTO(data->inode)->commit_done(task, data);\n\ttrace_nfs_commit_done(task, data);\n}\n\nstatic void nfs_commit_release_pages(struct nfs_commit_data *data)\n{\n\tconst struct nfs_writeverf *verf = data->res.verf;\n\tstruct nfs_page\t*req;\n\tint status = data->task.tk_status;\n\tstruct nfs_commit_info cinfo;\n\tstruct nfs_server *nfss;\n\tstruct folio *folio;\n\n\twhile (!list_empty(&data->pages)) {\n\t\treq = nfs_list_entry(data->pages.next);\n\t\tnfs_list_remove_request(req);\n\t\tfolio = nfs_page_to_folio(req);\n\t\tnfs_folio_clear_commit(folio);\n\n\t\tdprintk(\"NFS:       commit (%s/%llu %d@%lld)\",\n\t\t\tnfs_req_openctx(req)->dentry->d_sb->s_id,\n\t\t\t(unsigned long long)NFS_FILEID(d_inode(nfs_req_openctx(req)->dentry)),\n\t\t\treq->wb_bytes,\n\t\t\t(long long)req_offset(req));\n\t\tif (status < 0) {\n\t\t\tif (folio) {\n\t\t\t\ttrace_nfs_commit_error(data->inode, req,\n\t\t\t\t\t\t       status);\n\t\t\t\tnfs_mapping_set_error(folio, status);\n\t\t\t\tnfs_inode_remove_request(req);\n\t\t\t}\n\t\t\tdprintk_cont(\", error = %d\\n\", status);\n\t\t\tgoto next;\n\t\t}\n\n\t\t \n\t\tif (nfs_write_match_verf(verf, req)) {\n\t\t\t \n\t\t\tif (folio)\n\t\t\t\tnfs_inode_remove_request(req);\n\t\t\tdprintk_cont(\" OK\\n\");\n\t\t\tgoto next;\n\t\t}\n\t\t \n\t\tdprintk_cont(\" mismatch\\n\");\n\t\tnfs_mark_request_dirty(req);\n\t\tatomic_long_inc(&NFS_I(data->inode)->redirtied_pages);\n\tnext:\n\t\tnfs_unlock_and_release_request(req);\n\t\t \n\t\tcond_resched();\n\t}\n\tnfss = NFS_SERVER(data->inode);\n\tif (atomic_long_read(&nfss->writeback) < NFS_CONGESTION_OFF_THRESH)\n\t\tnfss->write_congested = 0;\n\n\tnfs_init_cinfo(&cinfo, data->inode, data->dreq);\n\tnfs_commit_end(cinfo.mds);\n}\n\nstatic void nfs_commit_release(void *calldata)\n{\n\tstruct nfs_commit_data *data = calldata;\n\n\tdata->completion_ops->completion(data);\n\tnfs_commitdata_release(calldata);\n}\n\nstatic const struct rpc_call_ops nfs_commit_ops = {\n\t.rpc_call_prepare = nfs_commit_prepare,\n\t.rpc_call_done = nfs_commit_done,\n\t.rpc_release = nfs_commit_release,\n};\n\nstatic const struct nfs_commit_completion_ops nfs_commit_completion_ops = {\n\t.completion = nfs_commit_release_pages,\n\t.resched_write = nfs_commit_resched_write,\n};\n\nint nfs_generic_commit_list(struct inode *inode, struct list_head *head,\n\t\t\t    int how, struct nfs_commit_info *cinfo)\n{\n\tint status;\n\n\tstatus = pnfs_commit_list(inode, head, how, cinfo);\n\tif (status == PNFS_NOT_ATTEMPTED)\n\t\tstatus = nfs_commit_list(inode, head, how, cinfo);\n\treturn status;\n}\n\nstatic int __nfs_commit_inode(struct inode *inode, int how,\n\t\tstruct writeback_control *wbc)\n{\n\tLIST_HEAD(head);\n\tstruct nfs_commit_info cinfo;\n\tint may_wait = how & FLUSH_SYNC;\n\tint ret, nscan;\n\n\thow &= ~FLUSH_SYNC;\n\tnfs_init_cinfo_from_inode(&cinfo, inode);\n\tnfs_commit_begin(cinfo.mds);\n\tfor (;;) {\n\t\tret = nscan = nfs_scan_commit(inode, &head, &cinfo);\n\t\tif (ret <= 0)\n\t\t\tbreak;\n\t\tret = nfs_generic_commit_list(inode, &head, how, &cinfo);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t\tret = 0;\n\t\tif (wbc && wbc->sync_mode == WB_SYNC_NONE) {\n\t\t\tif (nscan < wbc->nr_to_write)\n\t\t\t\twbc->nr_to_write -= nscan;\n\t\t\telse\n\t\t\t\twbc->nr_to_write = 0;\n\t\t}\n\t\tif (nscan < INT_MAX)\n\t\t\tbreak;\n\t\tcond_resched();\n\t}\n\tnfs_commit_end(cinfo.mds);\n\tif (ret || !may_wait)\n\t\treturn ret;\n\treturn wait_on_commit(cinfo.mds);\n}\n\nint nfs_commit_inode(struct inode *inode, int how)\n{\n\treturn __nfs_commit_inode(inode, how, NULL);\n}\nEXPORT_SYMBOL_GPL(nfs_commit_inode);\n\nint nfs_write_inode(struct inode *inode, struct writeback_control *wbc)\n{\n\tstruct nfs_inode *nfsi = NFS_I(inode);\n\tint flags = FLUSH_SYNC;\n\tint ret = 0;\n\n\tif (wbc->sync_mode == WB_SYNC_NONE) {\n\t\t \n\t\tif (!atomic_long_read(&nfsi->commit_info.ncommit))\n\t\t\tgoto check_requests_outstanding;\n\n\t\t \n\t\tif (mapping_tagged(inode->i_mapping, PAGECACHE_TAG_WRITEBACK))\n\t\t\tgoto out_mark_dirty;\n\n\t\t \n\t\tflags = 0;\n\t}\n\n\tret = __nfs_commit_inode(inode, flags, wbc);\n\tif (!ret) {\n\t\tif (flags & FLUSH_SYNC)\n\t\t\treturn 0;\n\t} else if (atomic_long_read(&nfsi->commit_info.ncommit))\n\t\tgoto out_mark_dirty;\n\ncheck_requests_outstanding:\n\tif (!atomic_read(&nfsi->commit_info.rpcs_out))\n\t\treturn ret;\nout_mark_dirty:\n\t__mark_inode_dirty(inode, I_DIRTY_DATASYNC);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nfs_write_inode);\n\n \nint nfs_filemap_write_and_wait_range(struct address_space *mapping,\n\t\tloff_t lstart, loff_t lend)\n{\n\tint ret;\n\n\tret = filemap_write_and_wait_range(mapping, lstart, lend);\n\tif (ret == 0)\n\t\tret = pnfs_sync_inode(mapping->host, true);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nfs_filemap_write_and_wait_range);\n\n \nint nfs_wb_all(struct inode *inode)\n{\n\tint ret;\n\n\ttrace_nfs_writeback_inode_enter(inode);\n\n\tret = filemap_write_and_wait(inode->i_mapping);\n\tif (ret)\n\t\tgoto out;\n\tret = nfs_commit_inode(inode, FLUSH_SYNC);\n\tif (ret < 0)\n\t\tgoto out;\n\tpnfs_sync_inode(inode, true);\n\tret = 0;\n\nout:\n\ttrace_nfs_writeback_inode_exit(inode, ret);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(nfs_wb_all);\n\nint nfs_wb_folio_cancel(struct inode *inode, struct folio *folio)\n{\n\tstruct nfs_page *req;\n\tint ret = 0;\n\n\tfolio_wait_writeback(folio);\n\n\t \n\treq = nfs_lock_and_join_requests(folio);\n\n\tif (IS_ERR(req)) {\n\t\tret = PTR_ERR(req);\n\t} else if (req) {\n\t\t \n\t\tnfs_inode_remove_request(req);\n\t\tnfs_unlock_and_release_request(req);\n\t}\n\n\treturn ret;\n}\n\n \nint nfs_wb_folio(struct inode *inode, struct folio *folio)\n{\n\tloff_t range_start = folio_file_pos(folio);\n\tloff_t range_end = range_start + (loff_t)folio_size(folio) - 1;\n\tstruct writeback_control wbc = {\n\t\t.sync_mode = WB_SYNC_ALL,\n\t\t.nr_to_write = 0,\n\t\t.range_start = range_start,\n\t\t.range_end = range_end,\n\t};\n\tint ret;\n\n\ttrace_nfs_writeback_folio(inode, folio);\n\n\tfor (;;) {\n\t\tfolio_wait_writeback(folio);\n\t\tif (folio_clear_dirty_for_io(folio)) {\n\t\t\tret = nfs_writepage_locked(folio, &wbc);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out_error;\n\t\t\tcontinue;\n\t\t}\n\t\tret = 0;\n\t\tif (!folio_test_private(folio))\n\t\t\tbreak;\n\t\tret = nfs_commit_inode(inode, FLUSH_SYNC);\n\t\tif (ret < 0)\n\t\t\tgoto out_error;\n\t}\nout_error:\n\ttrace_nfs_writeback_folio_done(inode, folio, ret);\n\treturn ret;\n}\n\n#ifdef CONFIG_MIGRATION\nint nfs_migrate_folio(struct address_space *mapping, struct folio *dst,\n\t\tstruct folio *src, enum migrate_mode mode)\n{\n\t \n\tif (folio_test_private(src))\n\t\treturn -EBUSY;\n\n\tif (folio_test_fscache(src)) {\n\t\tif (mode == MIGRATE_ASYNC)\n\t\t\treturn -EBUSY;\n\t\tfolio_wait_fscache(src);\n\t}\n\n\treturn migrate_folio(mapping, dst, src, mode);\n}\n#endif\n\nint __init nfs_init_writepagecache(void)\n{\n\tnfs_wdata_cachep = kmem_cache_create(\"nfs_write_data\",\n\t\t\t\t\t     sizeof(struct nfs_pgio_header),\n\t\t\t\t\t     0, SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t     NULL);\n\tif (nfs_wdata_cachep == NULL)\n\t\treturn -ENOMEM;\n\n\tnfs_wdata_mempool = mempool_create_slab_pool(MIN_POOL_WRITE,\n\t\t\t\t\t\t     nfs_wdata_cachep);\n\tif (nfs_wdata_mempool == NULL)\n\t\tgoto out_destroy_write_cache;\n\n\tnfs_cdata_cachep = kmem_cache_create(\"nfs_commit_data\",\n\t\t\t\t\t     sizeof(struct nfs_commit_data),\n\t\t\t\t\t     0, SLAB_HWCACHE_ALIGN,\n\t\t\t\t\t     NULL);\n\tif (nfs_cdata_cachep == NULL)\n\t\tgoto out_destroy_write_mempool;\n\n\tnfs_commit_mempool = mempool_create_slab_pool(MIN_POOL_COMMIT,\n\t\t\t\t\t\t      nfs_cdata_cachep);\n\tif (nfs_commit_mempool == NULL)\n\t\tgoto out_destroy_commit_cache;\n\n\t \n\tnfs_congestion_kb = (16*int_sqrt(totalram_pages())) << (PAGE_SHIFT-10);\n\tif (nfs_congestion_kb > 256*1024)\n\t\tnfs_congestion_kb = 256*1024;\n\n\treturn 0;\n\nout_destroy_commit_cache:\n\tkmem_cache_destroy(nfs_cdata_cachep);\nout_destroy_write_mempool:\n\tmempool_destroy(nfs_wdata_mempool);\nout_destroy_write_cache:\n\tkmem_cache_destroy(nfs_wdata_cachep);\n\treturn -ENOMEM;\n}\n\nvoid nfs_destroy_writepagecache(void)\n{\n\tmempool_destroy(nfs_commit_mempool);\n\tkmem_cache_destroy(nfs_cdata_cachep);\n\tmempool_destroy(nfs_wdata_mempool);\n\tkmem_cache_destroy(nfs_wdata_cachep);\n}\n\nstatic const struct nfs_rw_ops nfs_rw_write_ops = {\n\t.rw_alloc_header\t= nfs_writehdr_alloc,\n\t.rw_free_header\t\t= nfs_writehdr_free,\n\t.rw_done\t\t= nfs_writeback_done,\n\t.rw_result\t\t= nfs_writeback_result,\n\t.rw_initiate\t\t= nfs_initiate_write,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}