{
  "module_name": "eventpoll.c",
  "hash_id": "12281910b345a297b9d4ee6a6bdb93b1bbca2e43579cd8af10812d3f6a785dc0",
  "original_prompt": "Ingested from linux-6.6.14/fs/eventpoll.c",
  "human_readable_source": "\n \n\n#include <linux/init.h>\n#include <linux/kernel.h>\n#include <linux/sched/signal.h>\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/signal.h>\n#include <linux/errno.h>\n#include <linux/mm.h>\n#include <linux/slab.h>\n#include <linux/poll.h>\n#include <linux/string.h>\n#include <linux/list.h>\n#include <linux/hash.h>\n#include <linux/spinlock.h>\n#include <linux/syscalls.h>\n#include <linux/rbtree.h>\n#include <linux/wait.h>\n#include <linux/eventpoll.h>\n#include <linux/mount.h>\n#include <linux/bitops.h>\n#include <linux/mutex.h>\n#include <linux/anon_inodes.h>\n#include <linux/device.h>\n#include <linux/uaccess.h>\n#include <asm/io.h>\n#include <asm/mman.h>\n#include <linux/atomic.h>\n#include <linux/proc_fs.h>\n#include <linux/seq_file.h>\n#include <linux/compat.h>\n#include <linux/rculist.h>\n#include <net/busy_poll.h>\n\n \n\n \n#define EP_PRIVATE_BITS (EPOLLWAKEUP | EPOLLONESHOT | EPOLLET | EPOLLEXCLUSIVE)\n\n#define EPOLLINOUT_BITS (EPOLLIN | EPOLLOUT)\n\n#define EPOLLEXCLUSIVE_OK_BITS (EPOLLINOUT_BITS | EPOLLERR | EPOLLHUP | \\\n\t\t\t\tEPOLLWAKEUP | EPOLLET | EPOLLEXCLUSIVE)\n\n \n#define EP_MAX_NESTS 4\n\n#define EP_MAX_EVENTS (INT_MAX / sizeof(struct epoll_event))\n\n#define EP_UNACTIVE_PTR ((void *) -1L)\n\n#define EP_ITEM_COST (sizeof(struct epitem) + sizeof(struct eppoll_entry))\n\nstruct epoll_filefd {\n\tstruct file *file;\n\tint fd;\n} __packed;\n\n \nstruct eppoll_entry {\n\t \n\tstruct eppoll_entry *next;\n\n\t \n\tstruct epitem *base;\n\n\t \n\twait_queue_entry_t wait;\n\n\t \n\twait_queue_head_t *whead;\n};\n\n \nstruct epitem {\n\tunion {\n\t\t \n\t\tstruct rb_node rbn;\n\t\t \n\t\tstruct rcu_head rcu;\n\t};\n\n\t \n\tstruct list_head rdllink;\n\n\t \n\tstruct epitem *next;\n\n\t \n\tstruct epoll_filefd ffd;\n\n\t \n\tbool dying;\n\n\t \n\tstruct eppoll_entry *pwqlist;\n\n\t \n\tstruct eventpoll *ep;\n\n\t \n\tstruct hlist_node fllink;\n\n\t \n\tstruct wakeup_source __rcu *ws;\n\n\t \n\tstruct epoll_event event;\n};\n\n \nstruct eventpoll {\n\t \n\tstruct mutex mtx;\n\n\t \n\twait_queue_head_t wq;\n\n\t \n\twait_queue_head_t poll_wait;\n\n\t \n\tstruct list_head rdllist;\n\n\t \n\trwlock_t lock;\n\n\t \n\tstruct rb_root_cached rbr;\n\n\t \n\tstruct epitem *ovflist;\n\n\t \n\tstruct wakeup_source *ws;\n\n\t \n\tstruct user_struct *user;\n\n\tstruct file *file;\n\n\t \n\tu64 gen;\n\tstruct hlist_head refs;\n\n\t \n\trefcount_t refcount;\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\n\t \n\tunsigned int napi_id;\n#endif\n\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\t \n\tu8 nests;\n#endif\n};\n\n \nstruct ep_pqueue {\n\tpoll_table pt;\n\tstruct epitem *epi;\n};\n\n \n \nstatic long max_user_watches __read_mostly;\n\n \nstatic DEFINE_MUTEX(epnested_mutex);\n\nstatic u64 loop_check_gen = 0;\n\n \nstatic struct eventpoll *inserting_into;\n\n \nstatic struct kmem_cache *epi_cache __read_mostly;\n\n \nstatic struct kmem_cache *pwq_cache __read_mostly;\n\n \nstruct epitems_head {\n\tstruct hlist_head epitems;\n\tstruct epitems_head *next;\n};\nstatic struct epitems_head *tfile_check_list = EP_UNACTIVE_PTR;\n\nstatic struct kmem_cache *ephead_cache __read_mostly;\n\nstatic inline void free_ephead(struct epitems_head *head)\n{\n\tif (head)\n\t\tkmem_cache_free(ephead_cache, head);\n}\n\nstatic void list_file(struct file *file)\n{\n\tstruct epitems_head *head;\n\n\thead = container_of(file->f_ep, struct epitems_head, epitems);\n\tif (!head->next) {\n\t\thead->next = tfile_check_list;\n\t\ttfile_check_list = head;\n\t}\n}\n\nstatic void unlist_file(struct epitems_head *head)\n{\n\tstruct epitems_head *to_free = head;\n\tstruct hlist_node *p = rcu_dereference(hlist_first_rcu(&head->epitems));\n\tif (p) {\n\t\tstruct epitem *epi= container_of(p, struct epitem, fllink);\n\t\tspin_lock(&epi->ffd.file->f_lock);\n\t\tif (!hlist_empty(&head->epitems))\n\t\t\tto_free = NULL;\n\t\thead->next = NULL;\n\t\tspin_unlock(&epi->ffd.file->f_lock);\n\t}\n\tfree_ephead(to_free);\n}\n\n#ifdef CONFIG_SYSCTL\n\n#include <linux/sysctl.h>\n\nstatic long long_zero;\nstatic long long_max = LONG_MAX;\n\nstatic struct ctl_table epoll_table[] = {\n\t{\n\t\t.procname\t= \"max_user_watches\",\n\t\t.data\t\t= &max_user_watches,\n\t\t.maxlen\t\t= sizeof(max_user_watches),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t\t.extra1\t\t= &long_zero,\n\t\t.extra2\t\t= &long_max,\n\t},\n\t{ }\n};\n\nstatic void __init epoll_sysctls_init(void)\n{\n\tregister_sysctl(\"fs/epoll\", epoll_table);\n}\n#else\n#define epoll_sysctls_init() do { } while (0)\n#endif  \n\nstatic const struct file_operations eventpoll_fops;\n\nstatic inline int is_file_epoll(struct file *f)\n{\n\treturn f->f_op == &eventpoll_fops;\n}\n\n \nstatic inline void ep_set_ffd(struct epoll_filefd *ffd,\n\t\t\t      struct file *file, int fd)\n{\n\tffd->file = file;\n\tffd->fd = fd;\n}\n\n \nstatic inline int ep_cmp_ffd(struct epoll_filefd *p1,\n\t\t\t     struct epoll_filefd *p2)\n{\n\treturn (p1->file > p2->file ? +1:\n\t        (p1->file < p2->file ? -1 : p1->fd - p2->fd));\n}\n\n \nstatic inline int ep_is_linked(struct epitem *epi)\n{\n\treturn !list_empty(&epi->rdllink);\n}\n\nstatic inline struct eppoll_entry *ep_pwq_from_wait(wait_queue_entry_t *p)\n{\n\treturn container_of(p, struct eppoll_entry, wait);\n}\n\n \nstatic inline struct epitem *ep_item_from_wait(wait_queue_entry_t *p)\n{\n\treturn container_of(p, struct eppoll_entry, wait)->base;\n}\n\n \nstatic inline int ep_events_available(struct eventpoll *ep)\n{\n\treturn !list_empty_careful(&ep->rdllist) ||\n\t\tREAD_ONCE(ep->ovflist) != EP_UNACTIVE_PTR;\n}\n\n#ifdef CONFIG_NET_RX_BUSY_POLL\nstatic bool ep_busy_loop_end(void *p, unsigned long start_time)\n{\n\tstruct eventpoll *ep = p;\n\n\treturn ep_events_available(ep) || busy_loop_timeout(start_time);\n}\n\n \nstatic bool ep_busy_loop(struct eventpoll *ep, int nonblock)\n{\n\tunsigned int napi_id = READ_ONCE(ep->napi_id);\n\n\tif ((napi_id >= MIN_NAPI_ID) && net_busy_loop_on()) {\n\t\tnapi_busy_loop(napi_id, nonblock ? NULL : ep_busy_loop_end, ep, false,\n\t\t\t       BUSY_POLL_BUDGET);\n\t\tif (ep_events_available(ep))\n\t\t\treturn true;\n\t\t \n\t\tep->napi_id = 0;\n\t\treturn false;\n\t}\n\treturn false;\n}\n\n \nstatic inline void ep_set_busy_poll_napi_id(struct epitem *epi)\n{\n\tstruct eventpoll *ep;\n\tunsigned int napi_id;\n\tstruct socket *sock;\n\tstruct sock *sk;\n\n\tif (!net_busy_loop_on())\n\t\treturn;\n\n\tsock = sock_from_file(epi->ffd.file);\n\tif (!sock)\n\t\treturn;\n\n\tsk = sock->sk;\n\tif (!sk)\n\t\treturn;\n\n\tnapi_id = READ_ONCE(sk->sk_napi_id);\n\tep = epi->ep;\n\n\t \n\tif (napi_id < MIN_NAPI_ID || napi_id == ep->napi_id)\n\t\treturn;\n\n\t \n\tep->napi_id = napi_id;\n}\n\n#else\n\nstatic inline bool ep_busy_loop(struct eventpoll *ep, int nonblock)\n{\n\treturn false;\n}\n\nstatic inline void ep_set_busy_poll_napi_id(struct epitem *epi)\n{\n}\n\n#endif  \n\n \n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n\nstatic void ep_poll_safewake(struct eventpoll *ep, struct epitem *epi,\n\t\t\t     unsigned pollflags)\n{\n\tstruct eventpoll *ep_src;\n\tunsigned long flags;\n\tu8 nests = 0;\n\n\t \n\tif (epi) {\n\t\tif ((is_file_epoll(epi->ffd.file))) {\n\t\t\tep_src = epi->ffd.file->private_data;\n\t\t\tnests = ep_src->nests;\n\t\t} else {\n\t\t\tnests = 1;\n\t\t}\n\t}\n\tspin_lock_irqsave_nested(&ep->poll_wait.lock, flags, nests);\n\tep->nests = nests + 1;\n\twake_up_locked_poll(&ep->poll_wait, EPOLLIN | pollflags);\n\tep->nests = 0;\n\tspin_unlock_irqrestore(&ep->poll_wait.lock, flags);\n}\n\n#else\n\nstatic void ep_poll_safewake(struct eventpoll *ep, struct epitem *epi,\n\t\t\t     __poll_t pollflags)\n{\n\twake_up_poll(&ep->poll_wait, EPOLLIN | pollflags);\n}\n\n#endif\n\nstatic void ep_remove_wait_queue(struct eppoll_entry *pwq)\n{\n\twait_queue_head_t *whead;\n\n\trcu_read_lock();\n\t \n\twhead = smp_load_acquire(&pwq->whead);\n\tif (whead)\n\t\tremove_wait_queue(whead, &pwq->wait);\n\trcu_read_unlock();\n}\n\n \nstatic void ep_unregister_pollwait(struct eventpoll *ep, struct epitem *epi)\n{\n\tstruct eppoll_entry **p = &epi->pwqlist;\n\tstruct eppoll_entry *pwq;\n\n\twhile ((pwq = *p) != NULL) {\n\t\t*p = pwq->next;\n\t\tep_remove_wait_queue(pwq);\n\t\tkmem_cache_free(pwq_cache, pwq);\n\t}\n}\n\n \nstatic inline struct wakeup_source *ep_wakeup_source(struct epitem *epi)\n{\n\treturn rcu_dereference_check(epi->ws, lockdep_is_held(&epi->ep->mtx));\n}\n\n \nstatic inline void ep_pm_stay_awake(struct epitem *epi)\n{\n\tstruct wakeup_source *ws = ep_wakeup_source(epi);\n\n\tif (ws)\n\t\t__pm_stay_awake(ws);\n}\n\nstatic inline bool ep_has_wakeup_source(struct epitem *epi)\n{\n\treturn rcu_access_pointer(epi->ws) ? true : false;\n}\n\n \nstatic inline void ep_pm_stay_awake_rcu(struct epitem *epi)\n{\n\tstruct wakeup_source *ws;\n\n\trcu_read_lock();\n\tws = rcu_dereference(epi->ws);\n\tif (ws)\n\t\t__pm_stay_awake(ws);\n\trcu_read_unlock();\n}\n\n\n \nstatic void ep_start_scan(struct eventpoll *ep, struct list_head *txlist)\n{\n\t \n\tlockdep_assert_irqs_enabled();\n\twrite_lock_irq(&ep->lock);\n\tlist_splice_init(&ep->rdllist, txlist);\n\tWRITE_ONCE(ep->ovflist, NULL);\n\twrite_unlock_irq(&ep->lock);\n}\n\nstatic void ep_done_scan(struct eventpoll *ep,\n\t\t\t struct list_head *txlist)\n{\n\tstruct epitem *epi, *nepi;\n\n\twrite_lock_irq(&ep->lock);\n\t \n\tfor (nepi = READ_ONCE(ep->ovflist); (epi = nepi) != NULL;\n\t     nepi = epi->next, epi->next = EP_UNACTIVE_PTR) {\n\t\t \n\t\tif (!ep_is_linked(epi)) {\n\t\t\t \n\t\t\tlist_add(&epi->rdllink, &ep->rdllist);\n\t\t\tep_pm_stay_awake(epi);\n\t\t}\n\t}\n\t \n\tWRITE_ONCE(ep->ovflist, EP_UNACTIVE_PTR);\n\n\t \n\tlist_splice(txlist, &ep->rdllist);\n\t__pm_relax(ep->ws);\n\n\tif (!list_empty(&ep->rdllist)) {\n\t\tif (waitqueue_active(&ep->wq))\n\t\t\twake_up(&ep->wq);\n\t}\n\n\twrite_unlock_irq(&ep->lock);\n}\n\nstatic void epi_rcu_free(struct rcu_head *head)\n{\n\tstruct epitem *epi = container_of(head, struct epitem, rcu);\n\tkmem_cache_free(epi_cache, epi);\n}\n\nstatic void ep_get(struct eventpoll *ep)\n{\n\trefcount_inc(&ep->refcount);\n}\n\n \nstatic bool ep_refcount_dec_and_test(struct eventpoll *ep)\n{\n\tif (!refcount_dec_and_test(&ep->refcount))\n\t\treturn false;\n\n\tWARN_ON_ONCE(!RB_EMPTY_ROOT(&ep->rbr.rb_root));\n\treturn true;\n}\n\nstatic void ep_free(struct eventpoll *ep)\n{\n\tmutex_destroy(&ep->mtx);\n\tfree_uid(ep->user);\n\twakeup_source_unregister(ep->ws);\n\tkfree(ep);\n}\n\n \nstatic bool __ep_remove(struct eventpoll *ep, struct epitem *epi, bool force)\n{\n\tstruct file *file = epi->ffd.file;\n\tstruct epitems_head *to_free;\n\tstruct hlist_head *head;\n\n\tlockdep_assert_irqs_enabled();\n\n\t \n\tep_unregister_pollwait(ep, epi);\n\n\t \n\tspin_lock(&file->f_lock);\n\tif (epi->dying && !force) {\n\t\tspin_unlock(&file->f_lock);\n\t\treturn false;\n\t}\n\n\tto_free = NULL;\n\thead = file->f_ep;\n\tif (head->first == &epi->fllink && !epi->fllink.next) {\n\t\tfile->f_ep = NULL;\n\t\tif (!is_file_epoll(file)) {\n\t\t\tstruct epitems_head *v;\n\t\t\tv = container_of(head, struct epitems_head, epitems);\n\t\t\tif (!smp_load_acquire(&v->next))\n\t\t\t\tto_free = v;\n\t\t}\n\t}\n\thlist_del_rcu(&epi->fllink);\n\tspin_unlock(&file->f_lock);\n\tfree_ephead(to_free);\n\n\trb_erase_cached(&epi->rbn, &ep->rbr);\n\n\twrite_lock_irq(&ep->lock);\n\tif (ep_is_linked(epi))\n\t\tlist_del_init(&epi->rdllink);\n\twrite_unlock_irq(&ep->lock);\n\n\twakeup_source_unregister(ep_wakeup_source(epi));\n\t \n\tcall_rcu(&epi->rcu, epi_rcu_free);\n\n\tpercpu_counter_dec(&ep->user->epoll_watches);\n\treturn ep_refcount_dec_and_test(ep);\n}\n\n \nstatic void ep_remove_safe(struct eventpoll *ep, struct epitem *epi)\n{\n\tWARN_ON_ONCE(__ep_remove(ep, epi, false));\n}\n\nstatic void ep_clear_and_put(struct eventpoll *ep)\n{\n\tstruct rb_node *rbp, *next;\n\tstruct epitem *epi;\n\tbool dispose;\n\n\t \n\tif (waitqueue_active(&ep->poll_wait))\n\t\tep_poll_safewake(ep, NULL, 0);\n\n\tmutex_lock(&ep->mtx);\n\n\t \n\tfor (rbp = rb_first_cached(&ep->rbr); rbp; rbp = rb_next(rbp)) {\n\t\tepi = rb_entry(rbp, struct epitem, rbn);\n\n\t\tep_unregister_pollwait(ep, epi);\n\t\tcond_resched();\n\t}\n\n\t \n\tfor (rbp = rb_first_cached(&ep->rbr); rbp; rbp = next) {\n\t\tnext = rb_next(rbp);\n\t\tepi = rb_entry(rbp, struct epitem, rbn);\n\t\tep_remove_safe(ep, epi);\n\t\tcond_resched();\n\t}\n\n\tdispose = ep_refcount_dec_and_test(ep);\n\tmutex_unlock(&ep->mtx);\n\n\tif (dispose)\n\t\tep_free(ep);\n}\n\nstatic int ep_eventpoll_release(struct inode *inode, struct file *file)\n{\n\tstruct eventpoll *ep = file->private_data;\n\n\tif (ep)\n\t\tep_clear_and_put(ep);\n\n\treturn 0;\n}\n\nstatic __poll_t ep_item_poll(const struct epitem *epi, poll_table *pt, int depth);\n\nstatic __poll_t __ep_eventpoll_poll(struct file *file, poll_table *wait, int depth)\n{\n\tstruct eventpoll *ep = file->private_data;\n\tLIST_HEAD(txlist);\n\tstruct epitem *epi, *tmp;\n\tpoll_table pt;\n\t__poll_t res = 0;\n\n\tinit_poll_funcptr(&pt, NULL);\n\n\t \n\tpoll_wait(file, &ep->poll_wait, wait);\n\n\t \n\tmutex_lock_nested(&ep->mtx, depth);\n\tep_start_scan(ep, &txlist);\n\tlist_for_each_entry_safe(epi, tmp, &txlist, rdllink) {\n\t\tif (ep_item_poll(epi, &pt, depth + 1)) {\n\t\t\tres = EPOLLIN | EPOLLRDNORM;\n\t\t\tbreak;\n\t\t} else {\n\t\t\t \n\t\t\t__pm_relax(ep_wakeup_source(epi));\n\t\t\tlist_del_init(&epi->rdllink);\n\t\t}\n\t}\n\tep_done_scan(ep, &txlist);\n\tmutex_unlock(&ep->mtx);\n\treturn res;\n}\n\n \nstatic __poll_t ep_item_poll(const struct epitem *epi, poll_table *pt,\n\t\t\t\t int depth)\n{\n\tstruct file *file = epi->ffd.file;\n\t__poll_t res;\n\n\tpt->_key = epi->event.events;\n\tif (!is_file_epoll(file))\n\t\tres = vfs_poll(file, pt);\n\telse\n\t\tres = __ep_eventpoll_poll(file, pt, depth);\n\treturn res & epi->event.events;\n}\n\nstatic __poll_t ep_eventpoll_poll(struct file *file, poll_table *wait)\n{\n\treturn __ep_eventpoll_poll(file, wait, 0);\n}\n\n#ifdef CONFIG_PROC_FS\nstatic void ep_show_fdinfo(struct seq_file *m, struct file *f)\n{\n\tstruct eventpoll *ep = f->private_data;\n\tstruct rb_node *rbp;\n\n\tmutex_lock(&ep->mtx);\n\tfor (rbp = rb_first_cached(&ep->rbr); rbp; rbp = rb_next(rbp)) {\n\t\tstruct epitem *epi = rb_entry(rbp, struct epitem, rbn);\n\t\tstruct inode *inode = file_inode(epi->ffd.file);\n\n\t\tseq_printf(m, \"tfd: %8d events: %8x data: %16llx \"\n\t\t\t   \" pos:%lli ino:%lx sdev:%x\\n\",\n\t\t\t   epi->ffd.fd, epi->event.events,\n\t\t\t   (long long)epi->event.data,\n\t\t\t   (long long)epi->ffd.file->f_pos,\n\t\t\t   inode->i_ino, inode->i_sb->s_dev);\n\t\tif (seq_has_overflowed(m))\n\t\t\tbreak;\n\t}\n\tmutex_unlock(&ep->mtx);\n}\n#endif\n\n \nstatic const struct file_operations eventpoll_fops = {\n#ifdef CONFIG_PROC_FS\n\t.show_fdinfo\t= ep_show_fdinfo,\n#endif\n\t.release\t= ep_eventpoll_release,\n\t.poll\t\t= ep_eventpoll_poll,\n\t.llseek\t\t= noop_llseek,\n};\n\n \nvoid eventpoll_release_file(struct file *file)\n{\n\tstruct eventpoll *ep;\n\tstruct epitem *epi;\n\tbool dispose;\n\n\t \nagain:\n\tspin_lock(&file->f_lock);\n\tif (file->f_ep && file->f_ep->first) {\n\t\tepi = hlist_entry(file->f_ep->first, struct epitem, fllink);\n\t\tepi->dying = true;\n\t\tspin_unlock(&file->f_lock);\n\n\t\t \n\t\tep = epi->ep;\n\t\tmutex_lock(&ep->mtx);\n\t\tdispose = __ep_remove(ep, epi, true);\n\t\tmutex_unlock(&ep->mtx);\n\n\t\tif (dispose)\n\t\t\tep_free(ep);\n\t\tgoto again;\n\t}\n\tspin_unlock(&file->f_lock);\n}\n\nstatic int ep_alloc(struct eventpoll **pep)\n{\n\tstruct eventpoll *ep;\n\n\tep = kzalloc(sizeof(*ep), GFP_KERNEL);\n\tif (unlikely(!ep))\n\t\treturn -ENOMEM;\n\n\tmutex_init(&ep->mtx);\n\trwlock_init(&ep->lock);\n\tinit_waitqueue_head(&ep->wq);\n\tinit_waitqueue_head(&ep->poll_wait);\n\tINIT_LIST_HEAD(&ep->rdllist);\n\tep->rbr = RB_ROOT_CACHED;\n\tep->ovflist = EP_UNACTIVE_PTR;\n\tep->user = get_current_user();\n\trefcount_set(&ep->refcount, 1);\n\n\t*pep = ep;\n\n\treturn 0;\n}\n\n \nstatic struct epitem *ep_find(struct eventpoll *ep, struct file *file, int fd)\n{\n\tint kcmp;\n\tstruct rb_node *rbp;\n\tstruct epitem *epi, *epir = NULL;\n\tstruct epoll_filefd ffd;\n\n\tep_set_ffd(&ffd, file, fd);\n\tfor (rbp = ep->rbr.rb_root.rb_node; rbp; ) {\n\t\tepi = rb_entry(rbp, struct epitem, rbn);\n\t\tkcmp = ep_cmp_ffd(&ffd, &epi->ffd);\n\t\tif (kcmp > 0)\n\t\t\trbp = rbp->rb_right;\n\t\telse if (kcmp < 0)\n\t\t\trbp = rbp->rb_left;\n\t\telse {\n\t\t\tepir = epi;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn epir;\n}\n\n#ifdef CONFIG_KCMP\nstatic struct epitem *ep_find_tfd(struct eventpoll *ep, int tfd, unsigned long toff)\n{\n\tstruct rb_node *rbp;\n\tstruct epitem *epi;\n\n\tfor (rbp = rb_first_cached(&ep->rbr); rbp; rbp = rb_next(rbp)) {\n\t\tepi = rb_entry(rbp, struct epitem, rbn);\n\t\tif (epi->ffd.fd == tfd) {\n\t\t\tif (toff == 0)\n\t\t\t\treturn epi;\n\t\t\telse\n\t\t\t\ttoff--;\n\t\t}\n\t\tcond_resched();\n\t}\n\n\treturn NULL;\n}\n\nstruct file *get_epoll_tfile_raw_ptr(struct file *file, int tfd,\n\t\t\t\t     unsigned long toff)\n{\n\tstruct file *file_raw;\n\tstruct eventpoll *ep;\n\tstruct epitem *epi;\n\n\tif (!is_file_epoll(file))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tep = file->private_data;\n\n\tmutex_lock(&ep->mtx);\n\tepi = ep_find_tfd(ep, tfd, toff);\n\tif (epi)\n\t\tfile_raw = epi->ffd.file;\n\telse\n\t\tfile_raw = ERR_PTR(-ENOENT);\n\tmutex_unlock(&ep->mtx);\n\n\treturn file_raw;\n}\n#endif  \n\n \nstatic inline bool list_add_tail_lockless(struct list_head *new,\n\t\t\t\t\t  struct list_head *head)\n{\n\tstruct list_head *prev;\n\n\t \n\tif (!try_cmpxchg(&new->next, &new, head))\n\t\treturn false;\n\n\t \n\n\tprev = xchg(&head->prev, new);\n\n\t \n\n\tprev->next = new;\n\tnew->prev = prev;\n\n\treturn true;\n}\n\n \nstatic inline bool chain_epi_lockless(struct epitem *epi)\n{\n\tstruct eventpoll *ep = epi->ep;\n\n\t \n\tif (epi->next != EP_UNACTIVE_PTR)\n\t\treturn false;\n\n\t \n\tif (cmpxchg(&epi->next, EP_UNACTIVE_PTR, NULL) != EP_UNACTIVE_PTR)\n\t\treturn false;\n\n\t \n\tepi->next = xchg(&ep->ovflist, epi);\n\n\treturn true;\n}\n\n \nstatic int ep_poll_callback(wait_queue_entry_t *wait, unsigned mode, int sync, void *key)\n{\n\tint pwake = 0;\n\tstruct epitem *epi = ep_item_from_wait(wait);\n\tstruct eventpoll *ep = epi->ep;\n\t__poll_t pollflags = key_to_poll(key);\n\tunsigned long flags;\n\tint ewake = 0;\n\n\tread_lock_irqsave(&ep->lock, flags);\n\n\tep_set_busy_poll_napi_id(epi);\n\n\t \n\tif (!(epi->event.events & ~EP_PRIVATE_BITS))\n\t\tgoto out_unlock;\n\n\t \n\tif (pollflags && !(pollflags & epi->event.events))\n\t\tgoto out_unlock;\n\n\t \n\tif (READ_ONCE(ep->ovflist) != EP_UNACTIVE_PTR) {\n\t\tif (chain_epi_lockless(epi))\n\t\t\tep_pm_stay_awake_rcu(epi);\n\t} else if (!ep_is_linked(epi)) {\n\t\t \n\t\tif (list_add_tail_lockless(&epi->rdllink, &ep->rdllist))\n\t\t\tep_pm_stay_awake_rcu(epi);\n\t}\n\n\t \n\tif (waitqueue_active(&ep->wq)) {\n\t\tif ((epi->event.events & EPOLLEXCLUSIVE) &&\n\t\t\t\t\t!(pollflags & POLLFREE)) {\n\t\t\tswitch (pollflags & EPOLLINOUT_BITS) {\n\t\t\tcase EPOLLIN:\n\t\t\t\tif (epi->event.events & EPOLLIN)\n\t\t\t\t\tewake = 1;\n\t\t\t\tbreak;\n\t\t\tcase EPOLLOUT:\n\t\t\t\tif (epi->event.events & EPOLLOUT)\n\t\t\t\t\tewake = 1;\n\t\t\t\tbreak;\n\t\t\tcase 0:\n\t\t\t\tewake = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\twake_up(&ep->wq);\n\t}\n\tif (waitqueue_active(&ep->poll_wait))\n\t\tpwake++;\n\nout_unlock:\n\tread_unlock_irqrestore(&ep->lock, flags);\n\n\t \n\tif (pwake)\n\t\tep_poll_safewake(ep, epi, pollflags & EPOLL_URING_WAKE);\n\n\tif (!(epi->event.events & EPOLLEXCLUSIVE))\n\t\tewake = 1;\n\n\tif (pollflags & POLLFREE) {\n\t\t \n\t\tlist_del_init(&wait->entry);\n\t\t \n\t\tsmp_store_release(&ep_pwq_from_wait(wait)->whead, NULL);\n\t}\n\n\treturn ewake;\n}\n\n \nstatic void ep_ptable_queue_proc(struct file *file, wait_queue_head_t *whead,\n\t\t\t\t poll_table *pt)\n{\n\tstruct ep_pqueue *epq = container_of(pt, struct ep_pqueue, pt);\n\tstruct epitem *epi = epq->epi;\n\tstruct eppoll_entry *pwq;\n\n\tif (unlikely(!epi))\t\n\t\treturn;\n\n\tpwq = kmem_cache_alloc(pwq_cache, GFP_KERNEL);\n\tif (unlikely(!pwq)) {\n\t\tepq->epi = NULL;\n\t\treturn;\n\t}\n\n\tinit_waitqueue_func_entry(&pwq->wait, ep_poll_callback);\n\tpwq->whead = whead;\n\tpwq->base = epi;\n\tif (epi->event.events & EPOLLEXCLUSIVE)\n\t\tadd_wait_queue_exclusive(whead, &pwq->wait);\n\telse\n\t\tadd_wait_queue(whead, &pwq->wait);\n\tpwq->next = epi->pwqlist;\n\tepi->pwqlist = pwq;\n}\n\nstatic void ep_rbtree_insert(struct eventpoll *ep, struct epitem *epi)\n{\n\tint kcmp;\n\tstruct rb_node **p = &ep->rbr.rb_root.rb_node, *parent = NULL;\n\tstruct epitem *epic;\n\tbool leftmost = true;\n\n\twhile (*p) {\n\t\tparent = *p;\n\t\tepic = rb_entry(parent, struct epitem, rbn);\n\t\tkcmp = ep_cmp_ffd(&epi->ffd, &epic->ffd);\n\t\tif (kcmp > 0) {\n\t\t\tp = &parent->rb_right;\n\t\t\tleftmost = false;\n\t\t} else\n\t\t\tp = &parent->rb_left;\n\t}\n\trb_link_node(&epi->rbn, parent, p);\n\trb_insert_color_cached(&epi->rbn, &ep->rbr, leftmost);\n}\n\n\n\n#define PATH_ARR_SIZE 5\n \nstatic const int path_limits[PATH_ARR_SIZE] = { 1000, 500, 100, 50, 10 };\nstatic int path_count[PATH_ARR_SIZE];\n\nstatic int path_count_inc(int nests)\n{\n\t \n\tif (nests == 0)\n\t\treturn 0;\n\n\tif (++path_count[nests] > path_limits[nests])\n\t\treturn -1;\n\treturn 0;\n}\n\nstatic void path_count_init(void)\n{\n\tint i;\n\n\tfor (i = 0; i < PATH_ARR_SIZE; i++)\n\t\tpath_count[i] = 0;\n}\n\nstatic int reverse_path_check_proc(struct hlist_head *refs, int depth)\n{\n\tint error = 0;\n\tstruct epitem *epi;\n\n\tif (depth > EP_MAX_NESTS)  \n\t\treturn -1;\n\n\t \n\thlist_for_each_entry_rcu(epi, refs, fllink) {\n\t\tstruct hlist_head *refs = &epi->ep->refs;\n\t\tif (hlist_empty(refs))\n\t\t\terror = path_count_inc(depth);\n\t\telse\n\t\t\terror = reverse_path_check_proc(refs, depth + 1);\n\t\tif (error != 0)\n\t\t\tbreak;\n\t}\n\treturn error;\n}\n\n \nstatic int reverse_path_check(void)\n{\n\tstruct epitems_head *p;\n\n\tfor (p = tfile_check_list; p != EP_UNACTIVE_PTR; p = p->next) {\n\t\tint error;\n\t\tpath_count_init();\n\t\trcu_read_lock();\n\t\terror = reverse_path_check_proc(&p->epitems, 0);\n\t\trcu_read_unlock();\n\t\tif (error)\n\t\t\treturn error;\n\t}\n\treturn 0;\n}\n\nstatic int ep_create_wakeup_source(struct epitem *epi)\n{\n\tstruct name_snapshot n;\n\tstruct wakeup_source *ws;\n\n\tif (!epi->ep->ws) {\n\t\tepi->ep->ws = wakeup_source_register(NULL, \"eventpoll\");\n\t\tif (!epi->ep->ws)\n\t\t\treturn -ENOMEM;\n\t}\n\n\ttake_dentry_name_snapshot(&n, epi->ffd.file->f_path.dentry);\n\tws = wakeup_source_register(NULL, n.name.name);\n\trelease_dentry_name_snapshot(&n);\n\n\tif (!ws)\n\t\treturn -ENOMEM;\n\trcu_assign_pointer(epi->ws, ws);\n\n\treturn 0;\n}\n\n \nstatic noinline void ep_destroy_wakeup_source(struct epitem *epi)\n{\n\tstruct wakeup_source *ws = ep_wakeup_source(epi);\n\n\tRCU_INIT_POINTER(epi->ws, NULL);\n\n\t \n\tsynchronize_rcu();\n\twakeup_source_unregister(ws);\n}\n\nstatic int attach_epitem(struct file *file, struct epitem *epi)\n{\n\tstruct epitems_head *to_free = NULL;\n\tstruct hlist_head *head = NULL;\n\tstruct eventpoll *ep = NULL;\n\n\tif (is_file_epoll(file))\n\t\tep = file->private_data;\n\n\tif (ep) {\n\t\thead = &ep->refs;\n\t} else if (!READ_ONCE(file->f_ep)) {\nallocate:\n\t\tto_free = kmem_cache_zalloc(ephead_cache, GFP_KERNEL);\n\t\tif (!to_free)\n\t\t\treturn -ENOMEM;\n\t\thead = &to_free->epitems;\n\t}\n\tspin_lock(&file->f_lock);\n\tif (!file->f_ep) {\n\t\tif (unlikely(!head)) {\n\t\t\tspin_unlock(&file->f_lock);\n\t\t\tgoto allocate;\n\t\t}\n\t\tfile->f_ep = head;\n\t\tto_free = NULL;\n\t}\n\thlist_add_head_rcu(&epi->fllink, file->f_ep);\n\tspin_unlock(&file->f_lock);\n\tfree_ephead(to_free);\n\treturn 0;\n}\n\n \nstatic int ep_insert(struct eventpoll *ep, const struct epoll_event *event,\n\t\t     struct file *tfile, int fd, int full_check)\n{\n\tint error, pwake = 0;\n\t__poll_t revents;\n\tstruct epitem *epi;\n\tstruct ep_pqueue epq;\n\tstruct eventpoll *tep = NULL;\n\n\tif (is_file_epoll(tfile))\n\t\ttep = tfile->private_data;\n\n\tlockdep_assert_irqs_enabled();\n\n\tif (unlikely(percpu_counter_compare(&ep->user->epoll_watches,\n\t\t\t\t\t    max_user_watches) >= 0))\n\t\treturn -ENOSPC;\n\tpercpu_counter_inc(&ep->user->epoll_watches);\n\n\tif (!(epi = kmem_cache_zalloc(epi_cache, GFP_KERNEL))) {\n\t\tpercpu_counter_dec(&ep->user->epoll_watches);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\tINIT_LIST_HEAD(&epi->rdllink);\n\tepi->ep = ep;\n\tep_set_ffd(&epi->ffd, tfile, fd);\n\tepi->event = *event;\n\tepi->next = EP_UNACTIVE_PTR;\n\n\tif (tep)\n\t\tmutex_lock_nested(&tep->mtx, 1);\n\t \n\tif (unlikely(attach_epitem(tfile, epi) < 0)) {\n\t\tif (tep)\n\t\t\tmutex_unlock(&tep->mtx);\n\t\tkmem_cache_free(epi_cache, epi);\n\t\tpercpu_counter_dec(&ep->user->epoll_watches);\n\t\treturn -ENOMEM;\n\t}\n\n\tif (full_check && !tep)\n\t\tlist_file(tfile);\n\n\t \n\tep_rbtree_insert(ep, epi);\n\tif (tep)\n\t\tmutex_unlock(&tep->mtx);\n\n\t \n\tep_get(ep);\n\n\t \n\tif (unlikely(full_check && reverse_path_check())) {\n\t\tep_remove_safe(ep, epi);\n\t\treturn -EINVAL;\n\t}\n\n\tif (epi->event.events & EPOLLWAKEUP) {\n\t\terror = ep_create_wakeup_source(epi);\n\t\tif (error) {\n\t\t\tep_remove_safe(ep, epi);\n\t\t\treturn error;\n\t\t}\n\t}\n\n\t \n\tepq.epi = epi;\n\tinit_poll_funcptr(&epq.pt, ep_ptable_queue_proc);\n\n\t \n\trevents = ep_item_poll(epi, &epq.pt, 1);\n\n\t \n\tif (unlikely(!epq.epi)) {\n\t\tep_remove_safe(ep, epi);\n\t\treturn -ENOMEM;\n\t}\n\n\t \n\twrite_lock_irq(&ep->lock);\n\n\t \n\tep_set_busy_poll_napi_id(epi);\n\n\t \n\tif (revents && !ep_is_linked(epi)) {\n\t\tlist_add_tail(&epi->rdllink, &ep->rdllist);\n\t\tep_pm_stay_awake(epi);\n\n\t\t \n\t\tif (waitqueue_active(&ep->wq))\n\t\t\twake_up(&ep->wq);\n\t\tif (waitqueue_active(&ep->poll_wait))\n\t\t\tpwake++;\n\t}\n\n\twrite_unlock_irq(&ep->lock);\n\n\t \n\tif (pwake)\n\t\tep_poll_safewake(ep, NULL, 0);\n\n\treturn 0;\n}\n\n \nstatic int ep_modify(struct eventpoll *ep, struct epitem *epi,\n\t\t     const struct epoll_event *event)\n{\n\tint pwake = 0;\n\tpoll_table pt;\n\n\tlockdep_assert_irqs_enabled();\n\n\tinit_poll_funcptr(&pt, NULL);\n\n\t \n\tepi->event.events = event->events;  \n\tepi->event.data = event->data;  \n\tif (epi->event.events & EPOLLWAKEUP) {\n\t\tif (!ep_has_wakeup_source(epi))\n\t\t\tep_create_wakeup_source(epi);\n\t} else if (ep_has_wakeup_source(epi)) {\n\t\tep_destroy_wakeup_source(epi);\n\t}\n\n\t \n\tsmp_mb();\n\n\t \n\tif (ep_item_poll(epi, &pt, 1)) {\n\t\twrite_lock_irq(&ep->lock);\n\t\tif (!ep_is_linked(epi)) {\n\t\t\tlist_add_tail(&epi->rdllink, &ep->rdllist);\n\t\t\tep_pm_stay_awake(epi);\n\n\t\t\t \n\t\t\tif (waitqueue_active(&ep->wq))\n\t\t\t\twake_up(&ep->wq);\n\t\t\tif (waitqueue_active(&ep->poll_wait))\n\t\t\t\tpwake++;\n\t\t}\n\t\twrite_unlock_irq(&ep->lock);\n\t}\n\n\t \n\tif (pwake)\n\t\tep_poll_safewake(ep, NULL, 0);\n\n\treturn 0;\n}\n\nstatic int ep_send_events(struct eventpoll *ep,\n\t\t\t  struct epoll_event __user *events, int maxevents)\n{\n\tstruct epitem *epi, *tmp;\n\tLIST_HEAD(txlist);\n\tpoll_table pt;\n\tint res = 0;\n\n\t \n\tif (fatal_signal_pending(current))\n\t\treturn -EINTR;\n\n\tinit_poll_funcptr(&pt, NULL);\n\n\tmutex_lock(&ep->mtx);\n\tep_start_scan(ep, &txlist);\n\n\t \n\tlist_for_each_entry_safe(epi, tmp, &txlist, rdllink) {\n\t\tstruct wakeup_source *ws;\n\t\t__poll_t revents;\n\n\t\tif (res >= maxevents)\n\t\t\tbreak;\n\n\t\t \n\t\tws = ep_wakeup_source(epi);\n\t\tif (ws) {\n\t\t\tif (ws->active)\n\t\t\t\t__pm_stay_awake(ep->ws);\n\t\t\t__pm_relax(ws);\n\t\t}\n\n\t\tlist_del_init(&epi->rdllink);\n\n\t\t \n\t\trevents = ep_item_poll(epi, &pt, 1);\n\t\tif (!revents)\n\t\t\tcontinue;\n\n\t\tevents = epoll_put_uevent(revents, epi->event.data, events);\n\t\tif (!events) {\n\t\t\tlist_add(&epi->rdllink, &txlist);\n\t\t\tep_pm_stay_awake(epi);\n\t\t\tif (!res)\n\t\t\t\tres = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\tres++;\n\t\tif (epi->event.events & EPOLLONESHOT)\n\t\t\tepi->event.events &= EP_PRIVATE_BITS;\n\t\telse if (!(epi->event.events & EPOLLET)) {\n\t\t\t \n\t\t\tlist_add_tail(&epi->rdllink, &ep->rdllist);\n\t\t\tep_pm_stay_awake(epi);\n\t\t}\n\t}\n\tep_done_scan(ep, &txlist);\n\tmutex_unlock(&ep->mtx);\n\n\treturn res;\n}\n\nstatic struct timespec64 *ep_timeout_to_timespec(struct timespec64 *to, long ms)\n{\n\tstruct timespec64 now;\n\n\tif (ms < 0)\n\t\treturn NULL;\n\n\tif (!ms) {\n\t\tto->tv_sec = 0;\n\t\tto->tv_nsec = 0;\n\t\treturn to;\n\t}\n\n\tto->tv_sec = ms / MSEC_PER_SEC;\n\tto->tv_nsec = NSEC_PER_MSEC * (ms % MSEC_PER_SEC);\n\n\tktime_get_ts64(&now);\n\t*to = timespec64_add_safe(now, *to);\n\treturn to;\n}\n\n \nstatic int ep_autoremove_wake_function(struct wait_queue_entry *wq_entry,\n\t\t\t\t       unsigned int mode, int sync, void *key)\n{\n\tint ret = default_wake_function(wq_entry, mode, sync, key);\n\n\t \n\tlist_del_init_careful(&wq_entry->entry);\n\treturn ret;\n}\n\n \nstatic int ep_poll(struct eventpoll *ep, struct epoll_event __user *events,\n\t\t   int maxevents, struct timespec64 *timeout)\n{\n\tint res, eavail, timed_out = 0;\n\tu64 slack = 0;\n\twait_queue_entry_t wait;\n\tktime_t expires, *to = NULL;\n\n\tlockdep_assert_irqs_enabled();\n\n\tif (timeout && (timeout->tv_sec | timeout->tv_nsec)) {\n\t\tslack = select_estimate_accuracy(timeout);\n\t\tto = &expires;\n\t\t*to = timespec64_to_ktime(*timeout);\n\t} else if (timeout) {\n\t\t \n\t\ttimed_out = 1;\n\t}\n\n\t \n\teavail = ep_events_available(ep);\n\n\twhile (1) {\n\t\tif (eavail) {\n\t\t\t \n\t\t\tres = ep_send_events(ep, events, maxevents);\n\t\t\tif (res)\n\t\t\t\treturn res;\n\t\t}\n\n\t\tif (timed_out)\n\t\t\treturn 0;\n\n\t\teavail = ep_busy_loop(ep, timed_out);\n\t\tif (eavail)\n\t\t\tcontinue;\n\n\t\tif (signal_pending(current))\n\t\t\treturn -EINTR;\n\n\t\t \n\t\tinit_wait(&wait);\n\t\twait.func = ep_autoremove_wake_function;\n\n\t\twrite_lock_irq(&ep->lock);\n\t\t \n\t\t__set_current_state(TASK_INTERRUPTIBLE);\n\n\t\t \n\t\teavail = ep_events_available(ep);\n\t\tif (!eavail)\n\t\t\t__add_wait_queue_exclusive(&ep->wq, &wait);\n\n\t\twrite_unlock_irq(&ep->lock);\n\n\t\tif (!eavail)\n\t\t\ttimed_out = !schedule_hrtimeout_range(to, slack,\n\t\t\t\t\t\t\t      HRTIMER_MODE_ABS);\n\t\t__set_current_state(TASK_RUNNING);\n\n\t\t \n\t\teavail = 1;\n\n\t\tif (!list_empty_careful(&wait.entry)) {\n\t\t\twrite_lock_irq(&ep->lock);\n\t\t\t \n\t\t\tif (timed_out)\n\t\t\t\teavail = list_empty(&wait.entry);\n\t\t\t__remove_wait_queue(&ep->wq, &wait);\n\t\t\twrite_unlock_irq(&ep->lock);\n\t\t}\n\t}\n}\n\n \nstatic int ep_loop_check_proc(struct eventpoll *ep, int depth)\n{\n\tint error = 0;\n\tstruct rb_node *rbp;\n\tstruct epitem *epi;\n\n\tmutex_lock_nested(&ep->mtx, depth + 1);\n\tep->gen = loop_check_gen;\n\tfor (rbp = rb_first_cached(&ep->rbr); rbp; rbp = rb_next(rbp)) {\n\t\tepi = rb_entry(rbp, struct epitem, rbn);\n\t\tif (unlikely(is_file_epoll(epi->ffd.file))) {\n\t\t\tstruct eventpoll *ep_tovisit;\n\t\t\tep_tovisit = epi->ffd.file->private_data;\n\t\t\tif (ep_tovisit->gen == loop_check_gen)\n\t\t\t\tcontinue;\n\t\t\tif (ep_tovisit == inserting_into || depth > EP_MAX_NESTS)\n\t\t\t\terror = -1;\n\t\t\telse\n\t\t\t\terror = ep_loop_check_proc(ep_tovisit, depth + 1);\n\t\t\tif (error != 0)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\t \n\t\t\tlist_file(epi->ffd.file);\n\t\t}\n\t}\n\tmutex_unlock(&ep->mtx);\n\n\treturn error;\n}\n\n \nstatic int ep_loop_check(struct eventpoll *ep, struct eventpoll *to)\n{\n\tinserting_into = ep;\n\treturn ep_loop_check_proc(to, 0);\n}\n\nstatic void clear_tfile_check_list(void)\n{\n\trcu_read_lock();\n\twhile (tfile_check_list != EP_UNACTIVE_PTR) {\n\t\tstruct epitems_head *head = tfile_check_list;\n\t\ttfile_check_list = head->next;\n\t\tunlist_file(head);\n\t}\n\trcu_read_unlock();\n}\n\n \nstatic int do_epoll_create(int flags)\n{\n\tint error, fd;\n\tstruct eventpoll *ep = NULL;\n\tstruct file *file;\n\n\t \n\tBUILD_BUG_ON(EPOLL_CLOEXEC != O_CLOEXEC);\n\n\tif (flags & ~EPOLL_CLOEXEC)\n\t\treturn -EINVAL;\n\t \n\terror = ep_alloc(&ep);\n\tif (error < 0)\n\t\treturn error;\n\t \n\tfd = get_unused_fd_flags(O_RDWR | (flags & O_CLOEXEC));\n\tif (fd < 0) {\n\t\terror = fd;\n\t\tgoto out_free_ep;\n\t}\n\tfile = anon_inode_getfile(\"[eventpoll]\", &eventpoll_fops, ep,\n\t\t\t\t O_RDWR | (flags & O_CLOEXEC));\n\tif (IS_ERR(file)) {\n\t\terror = PTR_ERR(file);\n\t\tgoto out_free_fd;\n\t}\n\tep->file = file;\n\tfd_install(fd, file);\n\treturn fd;\n\nout_free_fd:\n\tput_unused_fd(fd);\nout_free_ep:\n\tep_clear_and_put(ep);\n\treturn error;\n}\n\nSYSCALL_DEFINE1(epoll_create1, int, flags)\n{\n\treturn do_epoll_create(flags);\n}\n\nSYSCALL_DEFINE1(epoll_create, int, size)\n{\n\tif (size <= 0)\n\t\treturn -EINVAL;\n\n\treturn do_epoll_create(0);\n}\n\n#ifdef CONFIG_PM_SLEEP\nstatic inline void ep_take_care_of_epollwakeup(struct epoll_event *epev)\n{\n\tif ((epev->events & EPOLLWAKEUP) && !capable(CAP_BLOCK_SUSPEND))\n\t\tepev->events &= ~EPOLLWAKEUP;\n}\n#else\nstatic inline void ep_take_care_of_epollwakeup(struct epoll_event *epev)\n{\n\tepev->events &= ~EPOLLWAKEUP;\n}\n#endif\n\nstatic inline int epoll_mutex_lock(struct mutex *mutex, int depth,\n\t\t\t\t   bool nonblock)\n{\n\tif (!nonblock) {\n\t\tmutex_lock_nested(mutex, depth);\n\t\treturn 0;\n\t}\n\tif (mutex_trylock(mutex))\n\t\treturn 0;\n\treturn -EAGAIN;\n}\n\nint do_epoll_ctl(int epfd, int op, int fd, struct epoll_event *epds,\n\t\t bool nonblock)\n{\n\tint error;\n\tint full_check = 0;\n\tstruct fd f, tf;\n\tstruct eventpoll *ep;\n\tstruct epitem *epi;\n\tstruct eventpoll *tep = NULL;\n\n\terror = -EBADF;\n\tf = fdget(epfd);\n\tif (!f.file)\n\t\tgoto error_return;\n\n\t \n\ttf = fdget(fd);\n\tif (!tf.file)\n\t\tgoto error_fput;\n\n\t \n\terror = -EPERM;\n\tif (!file_can_poll(tf.file))\n\t\tgoto error_tgt_fput;\n\n\t \n\tif (ep_op_has_event(op))\n\t\tep_take_care_of_epollwakeup(epds);\n\n\t \n\terror = -EINVAL;\n\tif (f.file == tf.file || !is_file_epoll(f.file))\n\t\tgoto error_tgt_fput;\n\n\t \n\tif (ep_op_has_event(op) && (epds->events & EPOLLEXCLUSIVE)) {\n\t\tif (op == EPOLL_CTL_MOD)\n\t\t\tgoto error_tgt_fput;\n\t\tif (op == EPOLL_CTL_ADD && (is_file_epoll(tf.file) ||\n\t\t\t\t(epds->events & ~EPOLLEXCLUSIVE_OK_BITS)))\n\t\t\tgoto error_tgt_fput;\n\t}\n\n\t \n\tep = f.file->private_data;\n\n\t \n\terror = epoll_mutex_lock(&ep->mtx, 0, nonblock);\n\tif (error)\n\t\tgoto error_tgt_fput;\n\tif (op == EPOLL_CTL_ADD) {\n\t\tif (READ_ONCE(f.file->f_ep) || ep->gen == loop_check_gen ||\n\t\t    is_file_epoll(tf.file)) {\n\t\t\tmutex_unlock(&ep->mtx);\n\t\t\terror = epoll_mutex_lock(&epnested_mutex, 0, nonblock);\n\t\t\tif (error)\n\t\t\t\tgoto error_tgt_fput;\n\t\t\tloop_check_gen++;\n\t\t\tfull_check = 1;\n\t\t\tif (is_file_epoll(tf.file)) {\n\t\t\t\ttep = tf.file->private_data;\n\t\t\t\terror = -ELOOP;\n\t\t\t\tif (ep_loop_check(ep, tep) != 0)\n\t\t\t\t\tgoto error_tgt_fput;\n\t\t\t}\n\t\t\terror = epoll_mutex_lock(&ep->mtx, 0, nonblock);\n\t\t\tif (error)\n\t\t\t\tgoto error_tgt_fput;\n\t\t}\n\t}\n\n\t \n\tepi = ep_find(ep, tf.file, fd);\n\n\terror = -EINVAL;\n\tswitch (op) {\n\tcase EPOLL_CTL_ADD:\n\t\tif (!epi) {\n\t\t\tepds->events |= EPOLLERR | EPOLLHUP;\n\t\t\terror = ep_insert(ep, epds, tf.file, fd, full_check);\n\t\t} else\n\t\t\terror = -EEXIST;\n\t\tbreak;\n\tcase EPOLL_CTL_DEL:\n\t\tif (epi) {\n\t\t\t \n\t\t\tep_remove_safe(ep, epi);\n\t\t\terror = 0;\n\t\t} else {\n\t\t\terror = -ENOENT;\n\t\t}\n\t\tbreak;\n\tcase EPOLL_CTL_MOD:\n\t\tif (epi) {\n\t\t\tif (!(epi->event.events & EPOLLEXCLUSIVE)) {\n\t\t\t\tepds->events |= EPOLLERR | EPOLLHUP;\n\t\t\t\terror = ep_modify(ep, epi, epds);\n\t\t\t}\n\t\t} else\n\t\t\terror = -ENOENT;\n\t\tbreak;\n\t}\n\tmutex_unlock(&ep->mtx);\n\nerror_tgt_fput:\n\tif (full_check) {\n\t\tclear_tfile_check_list();\n\t\tloop_check_gen++;\n\t\tmutex_unlock(&epnested_mutex);\n\t}\n\n\tfdput(tf);\nerror_fput:\n\tfdput(f);\nerror_return:\n\n\treturn error;\n}\n\n \nSYSCALL_DEFINE4(epoll_ctl, int, epfd, int, op, int, fd,\n\t\tstruct epoll_event __user *, event)\n{\n\tstruct epoll_event epds;\n\n\tif (ep_op_has_event(op) &&\n\t    copy_from_user(&epds, event, sizeof(struct epoll_event)))\n\t\treturn -EFAULT;\n\n\treturn do_epoll_ctl(epfd, op, fd, &epds, false);\n}\n\n \nstatic int do_epoll_wait(int epfd, struct epoll_event __user *events,\n\t\t\t int maxevents, struct timespec64 *to)\n{\n\tint error;\n\tstruct fd f;\n\tstruct eventpoll *ep;\n\n\t \n\tif (maxevents <= 0 || maxevents > EP_MAX_EVENTS)\n\t\treturn -EINVAL;\n\n\t \n\tif (!access_ok(events, maxevents * sizeof(struct epoll_event)))\n\t\treturn -EFAULT;\n\n\t \n\tf = fdget(epfd);\n\tif (!f.file)\n\t\treturn -EBADF;\n\n\t \n\terror = -EINVAL;\n\tif (!is_file_epoll(f.file))\n\t\tgoto error_fput;\n\n\t \n\tep = f.file->private_data;\n\n\t \n\terror = ep_poll(ep, events, maxevents, to);\n\nerror_fput:\n\tfdput(f);\n\treturn error;\n}\n\nSYSCALL_DEFINE4(epoll_wait, int, epfd, struct epoll_event __user *, events,\n\t\tint, maxevents, int, timeout)\n{\n\tstruct timespec64 to;\n\n\treturn do_epoll_wait(epfd, events, maxevents,\n\t\t\t     ep_timeout_to_timespec(&to, timeout));\n}\n\n \nstatic int do_epoll_pwait(int epfd, struct epoll_event __user *events,\n\t\t\t  int maxevents, struct timespec64 *to,\n\t\t\t  const sigset_t __user *sigmask, size_t sigsetsize)\n{\n\tint error;\n\n\t \n\terror = set_user_sigmask(sigmask, sigsetsize);\n\tif (error)\n\t\treturn error;\n\n\terror = do_epoll_wait(epfd, events, maxevents, to);\n\n\trestore_saved_sigmask_unless(error == -EINTR);\n\n\treturn error;\n}\n\nSYSCALL_DEFINE6(epoll_pwait, int, epfd, struct epoll_event __user *, events,\n\t\tint, maxevents, int, timeout, const sigset_t __user *, sigmask,\n\t\tsize_t, sigsetsize)\n{\n\tstruct timespec64 to;\n\n\treturn do_epoll_pwait(epfd, events, maxevents,\n\t\t\t      ep_timeout_to_timespec(&to, timeout),\n\t\t\t      sigmask, sigsetsize);\n}\n\nSYSCALL_DEFINE6(epoll_pwait2, int, epfd, struct epoll_event __user *, events,\n\t\tint, maxevents, const struct __kernel_timespec __user *, timeout,\n\t\tconst sigset_t __user *, sigmask, size_t, sigsetsize)\n{\n\tstruct timespec64 ts, *to = NULL;\n\n\tif (timeout) {\n\t\tif (get_timespec64(&ts, timeout))\n\t\t\treturn -EFAULT;\n\t\tto = &ts;\n\t\tif (poll_select_set_timeout(to, ts.tv_sec, ts.tv_nsec))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn do_epoll_pwait(epfd, events, maxevents, to,\n\t\t\t      sigmask, sigsetsize);\n}\n\n#ifdef CONFIG_COMPAT\nstatic int do_compat_epoll_pwait(int epfd, struct epoll_event __user *events,\n\t\t\t\t int maxevents, struct timespec64 *timeout,\n\t\t\t\t const compat_sigset_t __user *sigmask,\n\t\t\t\t compat_size_t sigsetsize)\n{\n\tlong err;\n\n\t \n\terr = set_compat_user_sigmask(sigmask, sigsetsize);\n\tif (err)\n\t\treturn err;\n\n\terr = do_epoll_wait(epfd, events, maxevents, timeout);\n\n\trestore_saved_sigmask_unless(err == -EINTR);\n\n\treturn err;\n}\n\nCOMPAT_SYSCALL_DEFINE6(epoll_pwait, int, epfd,\n\t\t       struct epoll_event __user *, events,\n\t\t       int, maxevents, int, timeout,\n\t\t       const compat_sigset_t __user *, sigmask,\n\t\t       compat_size_t, sigsetsize)\n{\n\tstruct timespec64 to;\n\n\treturn do_compat_epoll_pwait(epfd, events, maxevents,\n\t\t\t\t     ep_timeout_to_timespec(&to, timeout),\n\t\t\t\t     sigmask, sigsetsize);\n}\n\nCOMPAT_SYSCALL_DEFINE6(epoll_pwait2, int, epfd,\n\t\t       struct epoll_event __user *, events,\n\t\t       int, maxevents,\n\t\t       const struct __kernel_timespec __user *, timeout,\n\t\t       const compat_sigset_t __user *, sigmask,\n\t\t       compat_size_t, sigsetsize)\n{\n\tstruct timespec64 ts, *to = NULL;\n\n\tif (timeout) {\n\t\tif (get_timespec64(&ts, timeout))\n\t\t\treturn -EFAULT;\n\t\tto = &ts;\n\t\tif (poll_select_set_timeout(to, ts.tv_sec, ts.tv_nsec))\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn do_compat_epoll_pwait(epfd, events, maxevents, to,\n\t\t\t\t     sigmask, sigsetsize);\n}\n\n#endif\n\nstatic int __init eventpoll_init(void)\n{\n\tstruct sysinfo si;\n\n\tsi_meminfo(&si);\n\t \n\tmax_user_watches = (((si.totalram - si.totalhigh) / 25) << PAGE_SHIFT) /\n\t\tEP_ITEM_COST;\n\tBUG_ON(max_user_watches < 0);\n\n\t \n\tBUILD_BUG_ON(sizeof(void *) <= 8 && sizeof(struct epitem) > 128);\n\n\t \n\tepi_cache = kmem_cache_create(\"eventpoll_epi\", sizeof(struct epitem),\n\t\t\t0, SLAB_HWCACHE_ALIGN|SLAB_PANIC|SLAB_ACCOUNT, NULL);\n\n\t \n\tpwq_cache = kmem_cache_create(\"eventpoll_pwq\",\n\t\tsizeof(struct eppoll_entry), 0, SLAB_PANIC|SLAB_ACCOUNT, NULL);\n\tepoll_sysctls_init();\n\n\tephead_cache = kmem_cache_create(\"ep_head\",\n\t\tsizeof(struct epitems_head), 0, SLAB_PANIC|SLAB_ACCOUNT, NULL);\n\n\treturn 0;\n}\nfs_initcall(eventpoll_init);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}