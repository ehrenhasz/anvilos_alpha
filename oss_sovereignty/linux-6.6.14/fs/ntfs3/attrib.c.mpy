{
  "module_name": "attrib.c",
  "hash_id": "bf6f85b10c55f7a1ed87df647a32b0a5f6d25a102fb24f1f0ae083902af30fb2",
  "original_prompt": "Ingested from linux-6.6.14/fs/ntfs3/attrib.c",
  "human_readable_source": "\n \n\n#include <linux/fs.h>\n#include <linux/slab.h>\n#include <linux/kernel.h>\n\n#include \"debug.h\"\n#include \"ntfs.h\"\n#include \"ntfs_fs.h\"\n\n \n#ifndef NTFS_MIN_LOG2_OF_CLUMP\n#define NTFS_MIN_LOG2_OF_CLUMP 16\n#endif\n\n#ifndef NTFS_MAX_LOG2_OF_CLUMP\n#define NTFS_MAX_LOG2_OF_CLUMP 26\n#endif\n\n\n#define NTFS_CLUMP_MIN (1 << (NTFS_MIN_LOG2_OF_CLUMP + 8))\n\n#define NTFS_CLUMP_MAX (1ull << (NTFS_MAX_LOG2_OF_CLUMP + 8))\n\nstatic inline u64 get_pre_allocated(u64 size)\n{\n\tu32 clump;\n\tu8 align_shift;\n\tu64 ret;\n\n\tif (size <= NTFS_CLUMP_MIN) {\n\t\tclump = 1 << NTFS_MIN_LOG2_OF_CLUMP;\n\t\talign_shift = NTFS_MIN_LOG2_OF_CLUMP;\n\t} else if (size >= NTFS_CLUMP_MAX) {\n\t\tclump = 1 << NTFS_MAX_LOG2_OF_CLUMP;\n\t\talign_shift = NTFS_MAX_LOG2_OF_CLUMP;\n\t} else {\n\t\talign_shift = NTFS_MIN_LOG2_OF_CLUMP - 1 +\n\t\t\t      __ffs(size >> (8 + NTFS_MIN_LOG2_OF_CLUMP));\n\t\tclump = 1u << align_shift;\n\t}\n\n\tret = (((size + clump - 1) >> align_shift)) << align_shift;\n\n\treturn ret;\n}\n\n \nstatic int attr_load_runs(struct ATTRIB *attr, struct ntfs_inode *ni,\n\t\t\t  struct runs_tree *run, const CLST *vcn)\n{\n\tint err;\n\tCLST svcn = le64_to_cpu(attr->nres.svcn);\n\tCLST evcn = le64_to_cpu(attr->nres.evcn);\n\tu32 asize;\n\tu16 run_off;\n\n\tif (svcn >= evcn + 1 || run_is_mapped_full(run, svcn, evcn))\n\t\treturn 0;\n\n\tif (vcn && (evcn < *vcn || *vcn < svcn))\n\t\treturn -EINVAL;\n\n\tasize = le32_to_cpu(attr->size);\n\trun_off = le16_to_cpu(attr->nres.run_off);\n\n\tif (run_off > asize)\n\t\treturn -EINVAL;\n\n\terr = run_unpack_ex(run, ni->mi.sbi, ni->mi.rno, svcn, evcn,\n\t\t\t    vcn ? *vcn : svcn, Add2Ptr(attr, run_off),\n\t\t\t    asize - run_off);\n\tif (err < 0)\n\t\treturn err;\n\n\treturn 0;\n}\n\n \nstatic int run_deallocate_ex(struct ntfs_sb_info *sbi, struct runs_tree *run,\n\t\t\t     CLST vcn, CLST len, CLST *done, bool trim)\n{\n\tint err = 0;\n\tCLST vcn_next, vcn0 = vcn, lcn, clen, dn = 0;\n\tsize_t idx;\n\n\tif (!len)\n\t\tgoto out;\n\n\tif (!run_lookup_entry(run, vcn, &lcn, &clen, &idx)) {\nfailed:\n\t\trun_truncate(run, vcn0);\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tfor (;;) {\n\t\tif (clen > len)\n\t\t\tclen = len;\n\n\t\tif (!clen) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (lcn != SPARSE_LCN) {\n\t\t\tif (sbi) {\n\t\t\t\t \n\t\t\t\tmark_as_free_ex(sbi, lcn, clen, trim);\n\t\t\t}\n\t\t\tdn += clen;\n\t\t}\n\n\t\tlen -= clen;\n\t\tif (!len)\n\t\t\tbreak;\n\n\t\tvcn_next = vcn + clen;\n\t\tif (!run_get_entry(run, ++idx, &vcn, &lcn, &clen) ||\n\t\t    vcn != vcn_next) {\n\t\t\t \n\t\t\tgoto failed;\n\t\t}\n\t}\n\nout:\n\tif (done)\n\t\t*done += dn;\n\n\treturn err;\n}\n\n \nint attr_allocate_clusters(struct ntfs_sb_info *sbi, struct runs_tree *run,\n\t\t\t   CLST vcn, CLST lcn, CLST len, CLST *pre_alloc,\n\t\t\t   enum ALLOCATE_OPT opt, CLST *alen, const size_t fr,\n\t\t\t   CLST *new_lcn, CLST *new_len)\n{\n\tint err;\n\tCLST flen, vcn0 = vcn, pre = pre_alloc ? *pre_alloc : 0;\n\tsize_t cnt = run->count;\n\n\tfor (;;) {\n\t\terr = ntfs_look_for_free_space(sbi, lcn, len + pre, &lcn, &flen,\n\t\t\t\t\t       opt);\n\n\t\tif (err == -ENOSPC && pre) {\n\t\t\tpre = 0;\n\t\t\tif (*pre_alloc)\n\t\t\t\t*pre_alloc = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (vcn == vcn0) {\n\t\t\t \n\t\t\tif (new_lcn)\n\t\t\t\t*new_lcn = lcn;\n\t\t\tif (new_len)\n\t\t\t\t*new_len = flen;\n\t\t}\n\n\t\t \n\t\tif (!run_add_entry(run, vcn, lcn, flen, opt & ALLOCATE_MFT)) {\n\t\t\t \n\t\t\tmark_as_free_ex(sbi, lcn, len, false);\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (opt & ALLOCATE_ZERO) {\n\t\t\tu8 shift = sbi->cluster_bits - SECTOR_SHIFT;\n\n\t\t\terr = blkdev_issue_zeroout(sbi->sb->s_bdev,\n\t\t\t\t\t\t   (sector_t)lcn << shift,\n\t\t\t\t\t\t   (sector_t)flen << shift,\n\t\t\t\t\t\t   GFP_NOFS, 0);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tvcn += flen;\n\n\t\tif (flen >= len || (opt & ALLOCATE_MFT) ||\n\t\t    (fr && run->count - cnt >= fr)) {\n\t\t\t*alen = vcn - vcn0;\n\t\t\treturn 0;\n\t\t}\n\n\t\tlen -= flen;\n\t}\n\nout:\n\t \n\tif (vcn - vcn0) {\n\t\trun_deallocate_ex(sbi, run, vcn0, vcn - vcn0, NULL, false);\n\t\trun_truncate(run, vcn0);\n\t}\n\n\treturn err;\n}\n\n \nint attr_make_nonresident(struct ntfs_inode *ni, struct ATTRIB *attr,\n\t\t\t  struct ATTR_LIST_ENTRY *le, struct mft_inode *mi,\n\t\t\t  u64 new_size, struct runs_tree *run,\n\t\t\t  struct ATTRIB **ins_attr, struct page *page)\n{\n\tstruct ntfs_sb_info *sbi;\n\tstruct ATTRIB *attr_s;\n\tstruct MFT_REC *rec;\n\tu32 used, asize, rsize, aoff, align;\n\tbool is_data;\n\tCLST len, alen;\n\tchar *next;\n\tint err;\n\n\tif (attr->non_res) {\n\t\t*ins_attr = attr;\n\t\treturn 0;\n\t}\n\n\tsbi = mi->sbi;\n\trec = mi->mrec;\n\tattr_s = NULL;\n\tused = le32_to_cpu(rec->used);\n\tasize = le32_to_cpu(attr->size);\n\tnext = Add2Ptr(attr, asize);\n\taoff = PtrOffset(rec, attr);\n\trsize = le32_to_cpu(attr->res.data_size);\n\tis_data = attr->type == ATTR_DATA && !attr->name_len;\n\n\talign = sbi->cluster_size;\n\tif (is_attr_compressed(attr))\n\t\talign <<= COMPRESSION_UNIT;\n\tlen = (rsize + align - 1) >> sbi->cluster_bits;\n\n\trun_init(run);\n\n\t \n\tattr_s = kmemdup(attr, asize, GFP_NOFS);\n\tif (!attr_s) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tif (!len) {\n\t\t \n\t\talen = 0;\n\t} else {\n\t\tconst char *data = resident_data(attr);\n\n\t\terr = attr_allocate_clusters(sbi, run, 0, 0, len, NULL,\n\t\t\t\t\t     ALLOCATE_DEF, &alen, 0, NULL,\n\t\t\t\t\t     NULL);\n\t\tif (err)\n\t\t\tgoto out1;\n\n\t\tif (!rsize) {\n\t\t\t \n\t\t} else if (!is_data) {\n\t\t\terr = ntfs_sb_write_run(sbi, run, 0, data, rsize, 0);\n\t\t\tif (err)\n\t\t\t\tgoto out2;\n\t\t} else if (!page) {\n\t\t\tchar *kaddr;\n\n\t\t\tpage = grab_cache_page(ni->vfs_inode.i_mapping, 0);\n\t\t\tif (!page) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out2;\n\t\t\t}\n\t\t\tkaddr = kmap_atomic(page);\n\t\t\tmemcpy(kaddr, data, rsize);\n\t\t\tmemset(kaddr + rsize, 0, PAGE_SIZE - rsize);\n\t\t\tkunmap_atomic(kaddr);\n\t\t\tflush_dcache_page(page);\n\t\t\tSetPageUptodate(page);\n\t\t\tset_page_dirty(page);\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t}\n\t}\n\n\t \n\tused -= asize;\n\tmemmove(attr, Add2Ptr(attr, asize), used - aoff);\n\trec->used = cpu_to_le32(used);\n\tmi->dirty = true;\n\tif (le)\n\t\tal_remove_le(ni, le);\n\n\terr = ni_insert_nonresident(ni, attr_s->type, attr_name(attr_s),\n\t\t\t\t    attr_s->name_len, run, 0, alen,\n\t\t\t\t    attr_s->flags, &attr, NULL, NULL);\n\tif (err)\n\t\tgoto out3;\n\n\tkfree(attr_s);\n\tattr->nres.data_size = cpu_to_le64(rsize);\n\tattr->nres.valid_size = attr->nres.data_size;\n\n\t*ins_attr = attr;\n\n\tif (is_data)\n\t\tni->ni_flags &= ~NI_FLAG_RESIDENT;\n\n\t \n\treturn 0;\n\nout3:\n\tattr = Add2Ptr(rec, aoff);\n\tmemmove(next, attr, used - aoff);\n\tmemcpy(attr, attr_s, asize);\n\trec->used = cpu_to_le32(used + asize);\n\tmi->dirty = true;\nout2:\n\t \n\trun_deallocate(sbi, run, false);\n\trun_close(run);\nout1:\n\tkfree(attr_s);\nout:\n\treturn err;\n}\n\n \nstatic int attr_set_size_res(struct ntfs_inode *ni, struct ATTRIB *attr,\n\t\t\t     struct ATTR_LIST_ENTRY *le, struct mft_inode *mi,\n\t\t\t     u64 new_size, struct runs_tree *run,\n\t\t\t     struct ATTRIB **ins_attr)\n{\n\tstruct ntfs_sb_info *sbi = mi->sbi;\n\tstruct MFT_REC *rec = mi->mrec;\n\tu32 used = le32_to_cpu(rec->used);\n\tu32 asize = le32_to_cpu(attr->size);\n\tu32 aoff = PtrOffset(rec, attr);\n\tu32 rsize = le32_to_cpu(attr->res.data_size);\n\tu32 tail = used - aoff - asize;\n\tchar *next = Add2Ptr(attr, asize);\n\ts64 dsize = ALIGN(new_size, 8) - ALIGN(rsize, 8);\n\n\tif (dsize < 0) {\n\t\tmemmove(next + dsize, next, tail);\n\t} else if (dsize > 0) {\n\t\tif (used + dsize > sbi->max_bytes_per_attr)\n\t\t\treturn attr_make_nonresident(ni, attr, le, mi, new_size,\n\t\t\t\t\t\t     run, ins_attr, NULL);\n\n\t\tmemmove(next + dsize, next, tail);\n\t\tmemset(next, 0, dsize);\n\t}\n\n\tif (new_size > rsize)\n\t\tmemset(Add2Ptr(resident_data(attr), rsize), 0,\n\t\t       new_size - rsize);\n\n\trec->used = cpu_to_le32(used + dsize);\n\tattr->size = cpu_to_le32(asize + dsize);\n\tattr->res.data_size = cpu_to_le32(new_size);\n\tmi->dirty = true;\n\t*ins_attr = attr;\n\n\treturn 0;\n}\n\n \nint attr_set_size(struct ntfs_inode *ni, enum ATTR_TYPE type,\n\t\t  const __le16 *name, u8 name_len, struct runs_tree *run,\n\t\t  u64 new_size, const u64 *new_valid, bool keep_prealloc,\n\t\t  struct ATTRIB **ret)\n{\n\tint err = 0;\n\tstruct ntfs_sb_info *sbi = ni->mi.sbi;\n\tu8 cluster_bits = sbi->cluster_bits;\n\tbool is_mft = ni->mi.rno == MFT_REC_MFT && type == ATTR_DATA &&\n\t\t      !name_len;\n\tu64 old_valid, old_size, old_alloc, new_alloc, new_alloc_tmp;\n\tstruct ATTRIB *attr = NULL, *attr_b;\n\tstruct ATTR_LIST_ENTRY *le, *le_b;\n\tstruct mft_inode *mi, *mi_b;\n\tCLST alen, vcn, lcn, new_alen, old_alen, svcn, evcn;\n\tCLST next_svcn, pre_alloc = -1, done = 0;\n\tbool is_ext, is_bad = false;\n\tbool dirty = false;\n\tu32 align;\n\tstruct MFT_REC *rec;\n\nagain:\n\talen = 0;\n\tle_b = NULL;\n\tattr_b = ni_find_attr(ni, NULL, &le_b, type, name, name_len, NULL,\n\t\t\t      &mi_b);\n\tif (!attr_b) {\n\t\terr = -ENOENT;\n\t\tgoto bad_inode;\n\t}\n\n\tif (!attr_b->non_res) {\n\t\terr = attr_set_size_res(ni, attr_b, le_b, mi_b, new_size, run,\n\t\t\t\t\t&attr_b);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\t \n\t\tif (!attr_b->non_res) {\n\t\t\tdirty = true;\n\t\t\tgoto ok1;\n\t\t}\n\n\t\t \n\t\tgoto again;\n\t}\n\n\tis_ext = is_attr_ext(attr_b);\n\talign = sbi->cluster_size;\n\tif (is_ext)\n\t\talign <<= attr_b->nres.c_unit;\n\n\told_valid = le64_to_cpu(attr_b->nres.valid_size);\n\told_size = le64_to_cpu(attr_b->nres.data_size);\n\told_alloc = le64_to_cpu(attr_b->nres.alloc_size);\n\nagain_1:\n\told_alen = old_alloc >> cluster_bits;\n\n\tnew_alloc = (new_size + align - 1) & ~(u64)(align - 1);\n\tnew_alen = new_alloc >> cluster_bits;\n\n\tif (keep_prealloc && new_size < old_size) {\n\t\tattr_b->nres.data_size = cpu_to_le64(new_size);\n\t\tmi_b->dirty = dirty = true;\n\t\tgoto ok;\n\t}\n\n\tvcn = old_alen - 1;\n\n\tsvcn = le64_to_cpu(attr_b->nres.svcn);\n\tevcn = le64_to_cpu(attr_b->nres.evcn);\n\n\tif (svcn <= vcn && vcn <= evcn) {\n\t\tattr = attr_b;\n\t\tle = le_b;\n\t\tmi = mi_b;\n\t} else if (!le_b) {\n\t\terr = -EINVAL;\n\t\tgoto bad_inode;\n\t} else {\n\t\tle = le_b;\n\t\tattr = ni_find_attr(ni, attr_b, &le, type, name, name_len, &vcn,\n\t\t\t\t    &mi);\n\t\tif (!attr) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto bad_inode;\n\t\t}\n\nnext_le_1:\n\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\tevcn = le64_to_cpu(attr->nres.evcn);\n\t}\n\t \nnext_le:\n\trec = mi->mrec;\n\terr = attr_load_runs(attr, ni, run, NULL);\n\tif (err)\n\t\tgoto out;\n\n\tif (new_size > old_size) {\n\t\tCLST to_allocate;\n\t\tsize_t free;\n\n\t\tif (new_alloc <= old_alloc) {\n\t\t\tattr_b->nres.data_size = cpu_to_le64(new_size);\n\t\t\tmi_b->dirty = dirty = true;\n\t\t\tgoto ok;\n\t\t}\n\n\t\t \n\t\tto_allocate = new_alen - old_alen;\nadd_alloc_in_same_attr_seg:\n\t\tlcn = 0;\n\t\tif (is_mft) {\n\t\t\t \n\t\t\tpre_alloc = 0;\n\t\t} else if (is_ext) {\n\t\t\t \n\t\t\tpre_alloc = 0;\n\t\t} else if (pre_alloc == -1) {\n\t\t\tpre_alloc = 0;\n\t\t\tif (type == ATTR_DATA && !name_len &&\n\t\t\t    sbi->options->prealloc) {\n\t\t\t\tpre_alloc = bytes_to_cluster(\n\t\t\t\t\t\t    sbi, get_pre_allocated(\n\t\t\t\t\t\t\t\t new_size)) -\n\t\t\t\t\t    new_alen;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (old_alen &&\n\t\t\t    !run_lookup_entry(run, vcn, &lcn, NULL, NULL)) {\n\t\t\t\tlcn = SPARSE_LCN;\n\t\t\t}\n\n\t\t\tif (lcn == SPARSE_LCN)\n\t\t\t\tlcn = 0;\n\t\t\telse if (lcn)\n\t\t\t\tlcn += 1;\n\n\t\t\tfree = wnd_zeroes(&sbi->used.bitmap);\n\t\t\tif (to_allocate > free) {\n\t\t\t\terr = -ENOSPC;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (pre_alloc && to_allocate + pre_alloc > free)\n\t\t\t\tpre_alloc = 0;\n\t\t}\n\n\t\tvcn = old_alen;\n\n\t\tif (is_ext) {\n\t\t\tif (!run_add_entry(run, vcn, SPARSE_LCN, to_allocate,\n\t\t\t\t\t   false)) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\talen = to_allocate;\n\t\t} else {\n\t\t\t \n\t\t\terr = attr_allocate_clusters(\n\t\t\t\tsbi, run, vcn, lcn, to_allocate, &pre_alloc,\n\t\t\t\tis_mft ? ALLOCATE_MFT : ALLOCATE_DEF, &alen,\n\t\t\t\tis_mft ? 0 :\n\t\t\t\t\t (sbi->record_size -\n\t\t\t\t\t  le32_to_cpu(rec->used) + 8) /\n\t\t\t\t\t\t\t 3 +\n\t\t\t\t\t\t 1,\n\t\t\t\tNULL, NULL);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tdone += alen;\n\t\tvcn += alen;\n\t\tif (to_allocate > alen)\n\t\t\tto_allocate -= alen;\n\t\telse\n\t\t\tto_allocate = 0;\n\npack_runs:\n\t\terr = mi_pack_runs(mi, attr, run, vcn - svcn);\n\t\tif (err)\n\t\t\tgoto undo_1;\n\n\t\tnext_svcn = le64_to_cpu(attr->nres.evcn) + 1;\n\t\tnew_alloc_tmp = (u64)next_svcn << cluster_bits;\n\t\tattr_b->nres.alloc_size = cpu_to_le64(new_alloc_tmp);\n\t\tmi_b->dirty = dirty = true;\n\n\t\tif (next_svcn >= vcn && !to_allocate) {\n\t\t\t \n\t\t\tattr_b->nres.data_size = cpu_to_le64(new_size);\n\t\t\tgoto ok;\n\t\t}\n\n\t\t \n\t\tif (is_mft && next_svcn == vcn &&\n\t\t    ((u64)done << sbi->cluster_bits) >= 2 * sbi->record_size) {\n\t\t\tnew_size = new_alloc_tmp;\n\t\t\tattr_b->nres.data_size = attr_b->nres.alloc_size;\n\t\t\tgoto ok;\n\t\t}\n\n\t\tif (le32_to_cpu(rec->used) < sbi->record_size) {\n\t\t\told_alen = next_svcn;\n\t\t\tevcn = old_alen - 1;\n\t\t\tgoto add_alloc_in_same_attr_seg;\n\t\t}\n\n\t\tattr_b->nres.data_size = attr_b->nres.alloc_size;\n\t\tif (new_alloc_tmp < old_valid)\n\t\t\tattr_b->nres.valid_size = attr_b->nres.data_size;\n\n\t\tif (type == ATTR_LIST) {\n\t\t\terr = ni_expand_list(ni);\n\t\t\tif (err)\n\t\t\t\tgoto undo_2;\n\t\t\tif (next_svcn < vcn)\n\t\t\t\tgoto pack_runs;\n\n\t\t\t \n\t\t\tgoto again;\n\t\t}\n\n\t\tif (!ni->attr_list.size) {\n\t\t\terr = ni_create_attr_list(ni);\n\t\t\t \n\t\t\tif (err)\n\t\t\t\tgoto undo_2;\n\t\t\t \n\t\t}\n\n\t\tif (next_svcn >= vcn) {\n\t\t\t \n\t\t\tgoto again;\n\t\t}\n\n\t\t \n\t\terr = ni_insert_nonresident(ni, type, name, name_len, run,\n\t\t\t\t\t    next_svcn, vcn - next_svcn,\n\t\t\t\t\t    attr_b->flags, &attr, &mi, NULL);\n\n\t\t \n\t\tle_b = NULL;\n\t\tattr_b = ni_find_attr(ni, NULL, &le_b, type, name, name_len,\n\t\t\t\t      NULL, &mi_b);\n\t\tif (!attr_b) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto bad_inode;\n\t\t}\n\n\t\tif (err) {\n\t\t\t \n\t\t\tattr = NULL;\n\t\t\tgoto undo_2;\n\t\t}\n\n\t\tif (!is_mft)\n\t\t\trun_truncate_head(run, evcn + 1);\n\n\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\tevcn = le64_to_cpu(attr->nres.evcn);\n\n\t\t \n\t\told_valid = old_size = old_alloc = (u64)vcn << cluster_bits;\n\t\tattr_b->nres.valid_size = attr_b->nres.data_size =\n\t\t\tattr_b->nres.alloc_size = cpu_to_le64(old_size);\n\t\tmi_b->dirty = dirty = true;\n\t\tgoto again_1;\n\t}\n\n\tif (new_size != old_size ||\n\t    (new_alloc != old_alloc && !keep_prealloc)) {\n\t\t \n\t\tCLST dlen = 0;\n\n\t\tvcn = max(svcn, new_alen);\n\t\tnew_alloc_tmp = (u64)vcn << cluster_bits;\n\n\t\tif (vcn > svcn) {\n\t\t\terr = mi_pack_runs(mi, attr, run, vcn - svcn);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t} else if (le && le->vcn) {\n\t\t\tu16 le_sz = le16_to_cpu(le->size);\n\n\t\t\t \n\t\t\tmi_remove_attr(NULL, mi, attr);\n\n\t\t\tif (!al_remove_le(ni, le)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto bad_inode;\n\t\t\t}\n\n\t\t\tle = (struct ATTR_LIST_ENTRY *)((u8 *)le - le_sz);\n\t\t} else {\n\t\t\tattr->nres.evcn = cpu_to_le64((u64)vcn - 1);\n\t\t\tmi->dirty = true;\n\t\t}\n\n\t\tattr_b->nres.alloc_size = cpu_to_le64(new_alloc_tmp);\n\n\t\tif (vcn == new_alen) {\n\t\t\tattr_b->nres.data_size = cpu_to_le64(new_size);\n\t\t\tif (new_size < old_valid)\n\t\t\t\tattr_b->nres.valid_size =\n\t\t\t\t\tattr_b->nres.data_size;\n\t\t} else {\n\t\t\tif (new_alloc_tmp <=\n\t\t\t    le64_to_cpu(attr_b->nres.data_size))\n\t\t\t\tattr_b->nres.data_size =\n\t\t\t\t\tattr_b->nres.alloc_size;\n\t\t\tif (new_alloc_tmp <\n\t\t\t    le64_to_cpu(attr_b->nres.valid_size))\n\t\t\t\tattr_b->nres.valid_size =\n\t\t\t\t\tattr_b->nres.alloc_size;\n\t\t}\n\t\tmi_b->dirty = dirty = true;\n\n\t\terr = run_deallocate_ex(sbi, run, vcn, evcn - vcn + 1, &dlen,\n\t\t\t\t\ttrue);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (is_ext) {\n\t\t\t \n\t\t\tle64_sub_cpu(&attr_b->nres.total_size,\n\t\t\t\t     ((u64)dlen << cluster_bits));\n\t\t}\n\n\t\trun_truncate(run, vcn);\n\n\t\tif (new_alloc_tmp <= new_alloc)\n\t\t\tgoto ok;\n\n\t\told_size = new_alloc_tmp;\n\t\tvcn = svcn - 1;\n\n\t\tif (le == le_b) {\n\t\t\tattr = attr_b;\n\t\t\tmi = mi_b;\n\t\t\tevcn = svcn - 1;\n\t\t\tsvcn = 0;\n\t\t\tgoto next_le;\n\t\t}\n\n\t\tif (le->type != type || le->name_len != name_len ||\n\t\t    memcmp(le_name(le), name, name_len * sizeof(short))) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto bad_inode;\n\t\t}\n\n\t\terr = ni_load_mi(ni, le, &mi);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tattr = mi_find_attr(mi, NULL, type, name, name_len, &le->id);\n\t\tif (!attr) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto bad_inode;\n\t\t}\n\t\tgoto next_le_1;\n\t}\n\nok:\n\tif (new_valid) {\n\t\t__le64 valid = cpu_to_le64(min(*new_valid, new_size));\n\n\t\tif (attr_b->nres.valid_size != valid) {\n\t\t\tattr_b->nres.valid_size = valid;\n\t\t\tmi_b->dirty = true;\n\t\t}\n\t}\n\nok1:\n\tif (ret)\n\t\t*ret = attr_b;\n\n\tif (((type == ATTR_DATA && !name_len) ||\n\t     (type == ATTR_ALLOC && name == I30_NAME))) {\n\t\t \n\t\tif (attr_b->non_res) {\n\t\t\tnew_alloc = le64_to_cpu(attr_b->nres.alloc_size);\n\t\t\tif (inode_get_bytes(&ni->vfs_inode) != new_alloc) {\n\t\t\t\tinode_set_bytes(&ni->vfs_inode, new_alloc);\n\t\t\t\tdirty = true;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (dirty) {\n\t\t\tni->ni_flags |= NI_FLAG_UPDATE_PARENT;\n\t\t\tmark_inode_dirty(&ni->vfs_inode);\n\t\t}\n\t}\n\n\treturn 0;\n\nundo_2:\n\tvcn -= alen;\n\tattr_b->nres.data_size = cpu_to_le64(old_size);\n\tattr_b->nres.valid_size = cpu_to_le64(old_valid);\n\tattr_b->nres.alloc_size = cpu_to_le64(old_alloc);\n\n\t \n\tif (attr)\n\t\tgoto restore_run;\n\n\tif (le64_to_cpu(attr_b->nres.svcn) <= svcn &&\n\t    svcn <= le64_to_cpu(attr_b->nres.evcn)) {\n\t\tattr = attr_b;\n\t\tle = le_b;\n\t\tmi = mi_b;\n\t} else if (!le_b) {\n\t\terr = -EINVAL;\n\t\tgoto bad_inode;\n\t} else {\n\t\tle = le_b;\n\t\tattr = ni_find_attr(ni, attr_b, &le, type, name, name_len,\n\t\t\t\t    &svcn, &mi);\n\t\tif (!attr)\n\t\t\tgoto bad_inode;\n\t}\n\nrestore_run:\n\tif (mi_pack_runs(mi, attr, run, evcn - svcn + 1))\n\t\tis_bad = true;\n\nundo_1:\n\trun_deallocate_ex(sbi, run, vcn, alen, NULL, false);\n\n\trun_truncate(run, vcn);\nout:\n\tif (is_bad) {\nbad_inode:\n\t\t_ntfs_bad_inode(&ni->vfs_inode);\n\t}\n\treturn err;\n}\n\n \nint attr_data_get_block(struct ntfs_inode *ni, CLST vcn, CLST clen, CLST *lcn,\n\t\t\tCLST *len, bool *new, bool zero)\n{\n\tint err = 0;\n\tstruct runs_tree *run = &ni->file.run;\n\tstruct ntfs_sb_info *sbi;\n\tu8 cluster_bits;\n\tstruct ATTRIB *attr = NULL, *attr_b;\n\tstruct ATTR_LIST_ENTRY *le, *le_b;\n\tstruct mft_inode *mi, *mi_b;\n\tCLST hint, svcn, to_alloc, evcn1, next_svcn, asize, end, vcn0, alen;\n\tCLST alloc, evcn;\n\tunsigned fr;\n\tu64 total_size, total_size0;\n\tint step = 0;\n\n\tif (new)\n\t\t*new = false;\n\n\t \n\tdown_read(&ni->file.run_lock);\n\tif (!run_lookup_entry(run, vcn, lcn, len, NULL))\n\t\t*len = 0;\n\tup_read(&ni->file.run_lock);\n\n\tif (*len) {\n\t\tif (*lcn != SPARSE_LCN || !new)\n\t\t\treturn 0;  \n\t\telse if (clen > *len)\n\t\t\tclen = *len;\n\t}\n\n\t \n\tsbi = ni->mi.sbi;\n\tcluster_bits = sbi->cluster_bits;\n\n\tni_lock(ni);\n\tdown_write(&ni->file.run_lock);\n\n\tle_b = NULL;\n\tattr_b = ni_find_attr(ni, NULL, &le_b, ATTR_DATA, NULL, 0, NULL, &mi_b);\n\tif (!attr_b) {\n\t\terr = -ENOENT;\n\t\tgoto out;\n\t}\n\n\tif (!attr_b->non_res) {\n\t\t*lcn = RESIDENT_LCN;\n\t\t*len = 1;\n\t\tgoto out;\n\t}\n\n\tasize = le64_to_cpu(attr_b->nres.alloc_size) >> cluster_bits;\n\tif (vcn >= asize) {\n\t\tif (new) {\n\t\t\terr = -EINVAL;\n\t\t} else {\n\t\t\t*len = 1;\n\t\t\t*lcn = SPARSE_LCN;\n\t\t}\n\t\tgoto out;\n\t}\n\n\tsvcn = le64_to_cpu(attr_b->nres.svcn);\n\tevcn1 = le64_to_cpu(attr_b->nres.evcn) + 1;\n\n\tattr = attr_b;\n\tle = le_b;\n\tmi = mi_b;\n\n\tif (le_b && (vcn < svcn || evcn1 <= vcn)) {\n\t\tattr = ni_find_attr(ni, attr_b, &le, ATTR_DATA, NULL, 0, &vcn,\n\t\t\t\t    &mi);\n\t\tif (!attr) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\tevcn1 = le64_to_cpu(attr->nres.evcn) + 1;\n\t}\n\n\t \n\terr = attr_load_runs(attr, ni, run, NULL);\n\tif (err)\n\t\tgoto out;\n\n\tif (!*len) {\n\t\tif (run_lookup_entry(run, vcn, lcn, len, NULL)) {\n\t\t\tif (*lcn != SPARSE_LCN || !new)\n\t\t\t\tgoto ok;  \n\n\t\t\tif (clen > *len)\n\t\t\t\tclen = *len;\n\t\t} else if (!new) {\n\t\t\t \n\t\t\tgoto ok;\n\t\t}\n\t}\n\n\tif (!is_attr_ext(attr_b)) {\n\t\t \n\t\terr = -EINVAL;\n\t\tgoto out;\n\t}\n\n\tvcn0 = vcn;\n\tto_alloc = clen;\n\tfr = (sbi->record_size - le32_to_cpu(mi->mrec->used) + 8) / 3 + 1;\n\t \n\tif (attr_b->nres.c_unit) {\n\t\tCLST clst_per_frame = 1u << attr_b->nres.c_unit;\n\t\tCLST cmask = ~(clst_per_frame - 1);\n\n\t\t \n\t\tvcn = vcn0 & cmask;\n\t\tto_alloc = ((vcn0 + clen + clst_per_frame - 1) & cmask) - vcn;\n\t\tif (fr < clst_per_frame)\n\t\t\tfr = clst_per_frame;\n\t\tzero = true;\n\n\t\t \n\t\tif (vcn < svcn || evcn1 <= vcn) {\n\t\t\t \n\t\t\tattr = ni_find_attr(ni, attr_b, &le, ATTR_DATA, NULL, 0,\n\t\t\t\t\t    &vcn, &mi);\n\t\t\tif (!attr) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\t\tevcn1 = le64_to_cpu(attr->nres.evcn) + 1;\n\t\t\terr = attr_load_runs(attr, ni, run, NULL);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\tif (vcn + to_alloc > asize)\n\t\tto_alloc = asize - vcn;\n\n\t \n\thint = 0;\n\n\tif (vcn > evcn1) {\n\t\tif (!run_add_entry(run, evcn1, SPARSE_LCN, vcn - evcn1,\n\t\t\t\t   false)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t} else if (vcn && !run_lookup_entry(run, vcn - 1, &hint, NULL, NULL)) {\n\t\thint = -1;\n\t}\n\n\t \n\terr = attr_allocate_clusters(sbi, run, vcn, hint + 1, to_alloc, NULL,\n\t\t\t\t     zero ? ALLOCATE_ZERO : ALLOCATE_DEF, &alen,\n\t\t\t\t     fr, lcn, len);\n\tif (err)\n\t\tgoto out;\n\t*new = true;\n\tstep = 1;\n\n\tend = vcn + alen;\n\t \n\ttotal_size0 = le64_to_cpu(attr_b->nres.total_size);\n\ttotal_size = total_size0 + ((u64)alen << cluster_bits);\n\n\tif (vcn != vcn0) {\n\t\tif (!run_lookup_entry(run, vcn0, lcn, len, NULL)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tif (*lcn == SPARSE_LCN) {\n\t\t\t \n\t\t\tWARN_ON(1);\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tif (vcn0 + *len > end)\n\t\t\t*len = end - vcn0;\n\t}\n\nrepack:\n\terr = mi_pack_runs(mi, attr, run, max(end, evcn1) - svcn);\n\tif (err)\n\t\tgoto out;\n\n\tattr_b->nres.total_size = cpu_to_le64(total_size);\n\tinode_set_bytes(&ni->vfs_inode, total_size);\n\tni->ni_flags |= NI_FLAG_UPDATE_PARENT;\n\n\tmi_b->dirty = true;\n\tmark_inode_dirty(&ni->vfs_inode);\n\n\t \n\tnext_svcn = le64_to_cpu(attr->nres.evcn) + 1;\n\n\tif (end <= evcn1) {\n\t\tif (next_svcn == evcn1) {\n\t\t\t \n\t\t\tgoto ok;\n\t\t}\n\t\t \n\t\tif (!ni->attr_list.size) {\n\t\t\terr = ni_create_attr_list(ni);\n\t\t\tif (err)\n\t\t\t\tgoto undo1;\n\t\t\t \n\t\t\tle_b = NULL;\n\t\t\tattr_b = ni_find_attr(ni, NULL, &le_b, ATTR_DATA, NULL,\n\t\t\t\t\t      0, NULL, &mi_b);\n\t\t\tif (!attr_b) {\n\t\t\t\terr = -ENOENT;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tattr = attr_b;\n\t\t\tle = le_b;\n\t\t\tmi = mi_b;\n\t\t\tgoto repack;\n\t\t}\n\t}\n\n\t \n\tif (!ntfs_check_for_free_space(sbi, 1, 1)) {\n\t\t \n\t\terr = -ENOSPC;\n\t\tgoto undo1;\n\t}\n\n\tstep = 2;\n\tsvcn = evcn1;\n\n\t \n\tattr = ni_find_attr(ni, attr, &le, ATTR_DATA, NULL, 0, &svcn, &mi);\n\n\tif (!attr) {\n\t\t \n\t\tgoto ins_ext;\n\t}\n\n\t \n\talloc = bytes_to_cluster(sbi, le64_to_cpu(attr_b->nres.alloc_size));\n\tevcn = le64_to_cpu(attr->nres.evcn);\n\n\tif (end < next_svcn)\n\t\tend = next_svcn;\n\twhile (end > evcn) {\n\t\t \n\t\tmi_remove_attr(NULL, mi, attr);\n\n\t\tif (!al_remove_le(ni, le)) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (evcn + 1 >= alloc) {\n\t\t\t \n\t\t\tevcn1 = evcn + 1;\n\t\t\tgoto ins_ext;\n\t\t}\n\n\t\tif (ni_load_mi(ni, le, &mi)) {\n\t\t\tattr = NULL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tattr = mi_find_attr(mi, NULL, ATTR_DATA, NULL, 0, &le->id);\n\t\tif (!attr) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\tevcn = le64_to_cpu(attr->nres.evcn);\n\t}\n\n\tif (end < svcn)\n\t\tend = svcn;\n\n\terr = attr_load_runs(attr, ni, run, &end);\n\tif (err)\n\t\tgoto out;\n\n\tevcn1 = evcn + 1;\n\tattr->nres.svcn = cpu_to_le64(next_svcn);\n\terr = mi_pack_runs(mi, attr, run, evcn1 - next_svcn);\n\tif (err)\n\t\tgoto out;\n\n\tle->vcn = cpu_to_le64(next_svcn);\n\tni->attr_list.dirty = true;\n\tmi->dirty = true;\n\tnext_svcn = le64_to_cpu(attr->nres.evcn) + 1;\n\nins_ext:\n\tif (evcn1 > next_svcn) {\n\t\terr = ni_insert_nonresident(ni, ATTR_DATA, NULL, 0, run,\n\t\t\t\t\t    next_svcn, evcn1 - next_svcn,\n\t\t\t\t\t    attr_b->flags, &attr, &mi, NULL);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\nok:\n\trun_truncate_around(run, vcn);\nout:\n\tif (err && step > 1) {\n\t\t \n\t\t_ntfs_bad_inode(&ni->vfs_inode);\n\t}\n\tup_write(&ni->file.run_lock);\n\tni_unlock(ni);\n\n\treturn err;\n\nundo1:\n\t \n\tattr_b->nres.total_size = cpu_to_le64(total_size0);\n\tinode_set_bytes(&ni->vfs_inode, total_size0);\n\n\tif (run_deallocate_ex(sbi, run, vcn, alen, NULL, false) ||\n\t    !run_add_entry(run, vcn, SPARSE_LCN, alen, false) ||\n\t    mi_pack_runs(mi, attr, run, max(end, evcn1) - svcn)) {\n\t\t_ntfs_bad_inode(&ni->vfs_inode);\n\t}\n\tgoto out;\n}\n\nint attr_data_read_resident(struct ntfs_inode *ni, struct page *page)\n{\n\tu64 vbo;\n\tstruct ATTRIB *attr;\n\tu32 data_size;\n\n\tattr = ni_find_attr(ni, NULL, NULL, ATTR_DATA, NULL, 0, NULL, NULL);\n\tif (!attr)\n\t\treturn -EINVAL;\n\n\tif (attr->non_res)\n\t\treturn E_NTFS_NONRESIDENT;\n\n\tvbo = page->index << PAGE_SHIFT;\n\tdata_size = le32_to_cpu(attr->res.data_size);\n\tif (vbo < data_size) {\n\t\tconst char *data = resident_data(attr);\n\t\tchar *kaddr = kmap_atomic(page);\n\t\tu32 use = data_size - vbo;\n\n\t\tif (use > PAGE_SIZE)\n\t\t\tuse = PAGE_SIZE;\n\n\t\tmemcpy(kaddr, data + vbo, use);\n\t\tmemset(kaddr + use, 0, PAGE_SIZE - use);\n\t\tkunmap_atomic(kaddr);\n\t\tflush_dcache_page(page);\n\t\tSetPageUptodate(page);\n\t} else if (!PageUptodate(page)) {\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tSetPageUptodate(page);\n\t}\n\n\treturn 0;\n}\n\nint attr_data_write_resident(struct ntfs_inode *ni, struct page *page)\n{\n\tu64 vbo;\n\tstruct mft_inode *mi;\n\tstruct ATTRIB *attr;\n\tu32 data_size;\n\n\tattr = ni_find_attr(ni, NULL, NULL, ATTR_DATA, NULL, 0, NULL, &mi);\n\tif (!attr)\n\t\treturn -EINVAL;\n\n\tif (attr->non_res) {\n\t\t \n\t\treturn E_NTFS_NONRESIDENT;\n\t}\n\n\tvbo = page->index << PAGE_SHIFT;\n\tdata_size = le32_to_cpu(attr->res.data_size);\n\tif (vbo < data_size) {\n\t\tchar *data = resident_data(attr);\n\t\tchar *kaddr = kmap_atomic(page);\n\t\tu32 use = data_size - vbo;\n\n\t\tif (use > PAGE_SIZE)\n\t\t\tuse = PAGE_SIZE;\n\t\tmemcpy(data + vbo, kaddr, use);\n\t\tkunmap_atomic(kaddr);\n\t\tmi->dirty = true;\n\t}\n\tni->i_valid = data_size;\n\n\treturn 0;\n}\n\n \nint attr_load_runs_vcn(struct ntfs_inode *ni, enum ATTR_TYPE type,\n\t\t       const __le16 *name, u8 name_len, struct runs_tree *run,\n\t\t       CLST vcn)\n{\n\tstruct ATTRIB *attr;\n\tint err;\n\tCLST svcn, evcn;\n\tu16 ro;\n\n\tif (!ni) {\n\t\t \n\t\treturn -ENOENT;\n\t}\n\n\tattr = ni_find_attr(ni, NULL, NULL, type, name, name_len, &vcn, NULL);\n\tif (!attr) {\n\t\t \n\t\treturn -ENOENT;\n\t}\n\n\tsvcn = le64_to_cpu(attr->nres.svcn);\n\tevcn = le64_to_cpu(attr->nres.evcn);\n\n\tif (evcn < vcn || vcn < svcn) {\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\tro = le16_to_cpu(attr->nres.run_off);\n\n\tif (ro > le32_to_cpu(attr->size))\n\t\treturn -EINVAL;\n\n\terr = run_unpack_ex(run, ni->mi.sbi, ni->mi.rno, svcn, evcn, svcn,\n\t\t\t    Add2Ptr(attr, ro), le32_to_cpu(attr->size) - ro);\n\tif (err < 0)\n\t\treturn err;\n\treturn 0;\n}\n\n \nint attr_load_runs_range(struct ntfs_inode *ni, enum ATTR_TYPE type,\n\t\t\t const __le16 *name, u8 name_len, struct runs_tree *run,\n\t\t\t u64 from, u64 to)\n{\n\tstruct ntfs_sb_info *sbi = ni->mi.sbi;\n\tu8 cluster_bits = sbi->cluster_bits;\n\tCLST vcn;\n\tCLST vcn_last = (to - 1) >> cluster_bits;\n\tCLST lcn, clen;\n\tint err;\n\n\tfor (vcn = from >> cluster_bits; vcn <= vcn_last; vcn += clen) {\n\t\tif (!run_lookup_entry(run, vcn, &lcn, &clen, NULL)) {\n\t\t\terr = attr_load_runs_vcn(ni, type, name, name_len, run,\n\t\t\t\t\t\t vcn);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tclen = 0;  \n\t\t}\n\t}\n\n\treturn 0;\n}\n\n#ifdef CONFIG_NTFS3_LZX_XPRESS\n \nint attr_wof_frame_info(struct ntfs_inode *ni, struct ATTRIB *attr,\n\t\t\tstruct runs_tree *run, u64 frame, u64 frames,\n\t\t\tu8 frame_bits, u32 *ondisk_size, u64 *vbo_data)\n{\n\tstruct ntfs_sb_info *sbi = ni->mi.sbi;\n\tu64 vbo[2], off[2], wof_size;\n\tu32 voff;\n\tu8 bytes_per_off;\n\tchar *addr;\n\tstruct page *page;\n\tint i, err;\n\t__le32 *off32;\n\t__le64 *off64;\n\n\tif (ni->vfs_inode.i_size < 0x100000000ull) {\n\t\t \n\t\tbytes_per_off = sizeof(__le32);\n\t\tvbo[1] = frame << 2;\n\t\t*vbo_data = frames << 2;\n\t} else {\n\t\t \n\t\tbytes_per_off = sizeof(__le64);\n\t\tvbo[1] = frame << 3;\n\t\t*vbo_data = frames << 3;\n\t}\n\n\t \n\tif (!attr->non_res) {\n\t\tif (vbo[1] + bytes_per_off > le32_to_cpu(attr->res.data_size)) {\n\t\t\tntfs_inode_err(&ni->vfs_inode, \"is corrupted\");\n\t\t\treturn -EINVAL;\n\t\t}\n\t\taddr = resident_data(attr);\n\n\t\tif (bytes_per_off == sizeof(__le32)) {\n\t\t\toff32 = Add2Ptr(addr, vbo[1]);\n\t\t\toff[0] = vbo[1] ? le32_to_cpu(off32[-1]) : 0;\n\t\t\toff[1] = le32_to_cpu(off32[0]);\n\t\t} else {\n\t\t\toff64 = Add2Ptr(addr, vbo[1]);\n\t\t\toff[0] = vbo[1] ? le64_to_cpu(off64[-1]) : 0;\n\t\t\toff[1] = le64_to_cpu(off64[0]);\n\t\t}\n\n\t\t*vbo_data += off[0];\n\t\t*ondisk_size = off[1] - off[0];\n\t\treturn 0;\n\t}\n\n\twof_size = le64_to_cpu(attr->nres.data_size);\n\tdown_write(&ni->file.run_lock);\n\tpage = ni->file.offs_page;\n\tif (!page) {\n\t\tpage = alloc_page(GFP_KERNEL);\n\t\tif (!page) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tpage->index = -1;\n\t\tni->file.offs_page = page;\n\t}\n\tlock_page(page);\n\taddr = page_address(page);\n\n\tif (vbo[1]) {\n\t\tvoff = vbo[1] & (PAGE_SIZE - 1);\n\t\tvbo[0] = vbo[1] - bytes_per_off;\n\t\ti = 0;\n\t} else {\n\t\tvoff = 0;\n\t\tvbo[0] = 0;\n\t\toff[0] = 0;\n\t\ti = 1;\n\t}\n\n\tdo {\n\t\tpgoff_t index = vbo[i] >> PAGE_SHIFT;\n\n\t\tif (index != page->index) {\n\t\t\tu64 from = vbo[i] & ~(u64)(PAGE_SIZE - 1);\n\t\t\tu64 to = min(from + PAGE_SIZE, wof_size);\n\n\t\t\terr = attr_load_runs_range(ni, ATTR_DATA, WOF_NAME,\n\t\t\t\t\t\t   ARRAY_SIZE(WOF_NAME), run,\n\t\t\t\t\t\t   from, to);\n\t\t\tif (err)\n\t\t\t\tgoto out1;\n\n\t\t\terr = ntfs_bio_pages(sbi, run, &page, 1, from,\n\t\t\t\t\t     to - from, REQ_OP_READ);\n\t\t\tif (err) {\n\t\t\t\tpage->index = -1;\n\t\t\t\tgoto out1;\n\t\t\t}\n\t\t\tpage->index = index;\n\t\t}\n\n\t\tif (i) {\n\t\t\tif (bytes_per_off == sizeof(__le32)) {\n\t\t\t\toff32 = Add2Ptr(addr, voff);\n\t\t\t\toff[1] = le32_to_cpu(*off32);\n\t\t\t} else {\n\t\t\t\toff64 = Add2Ptr(addr, voff);\n\t\t\t\toff[1] = le64_to_cpu(*off64);\n\t\t\t}\n\t\t} else if (!voff) {\n\t\t\tif (bytes_per_off == sizeof(__le32)) {\n\t\t\t\toff32 = Add2Ptr(addr, PAGE_SIZE - sizeof(u32));\n\t\t\t\toff[0] = le32_to_cpu(*off32);\n\t\t\t} else {\n\t\t\t\toff64 = Add2Ptr(addr, PAGE_SIZE - sizeof(u64));\n\t\t\t\toff[0] = le64_to_cpu(*off64);\n\t\t\t}\n\t\t} else {\n\t\t\t \n\t\t\tif (bytes_per_off == sizeof(__le32)) {\n\t\t\t\toff32 = Add2Ptr(addr, voff);\n\t\t\t\toff[0] = le32_to_cpu(off32[-1]);\n\t\t\t\toff[1] = le32_to_cpu(off32[0]);\n\t\t\t} else {\n\t\t\t\toff64 = Add2Ptr(addr, voff);\n\t\t\t\toff[0] = le64_to_cpu(off64[-1]);\n\t\t\t\toff[1] = le64_to_cpu(off64[0]);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t} while (++i < 2);\n\n\t*vbo_data += off[0];\n\t*ondisk_size = off[1] - off[0];\n\nout1:\n\tunlock_page(page);\nout:\n\tup_write(&ni->file.run_lock);\n\treturn err;\n}\n#endif\n\n \nint attr_is_frame_compressed(struct ntfs_inode *ni, struct ATTRIB *attr,\n\t\t\t     CLST frame, CLST *clst_data)\n{\n\tint err;\n\tu32 clst_frame;\n\tCLST clen, lcn, vcn, alen, slen, vcn_next;\n\tsize_t idx;\n\tstruct runs_tree *run;\n\n\t*clst_data = 0;\n\n\tif (!is_attr_compressed(attr))\n\t\treturn 0;\n\n\tif (!attr->non_res)\n\t\treturn 0;\n\n\tclst_frame = 1u << attr->nres.c_unit;\n\tvcn = frame * clst_frame;\n\trun = &ni->file.run;\n\n\tif (!run_lookup_entry(run, vcn, &lcn, &clen, &idx)) {\n\t\terr = attr_load_runs_vcn(ni, attr->type, attr_name(attr),\n\t\t\t\t\t attr->name_len, run, vcn);\n\t\tif (err)\n\t\t\treturn err;\n\n\t\tif (!run_lookup_entry(run, vcn, &lcn, &clen, &idx))\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (lcn == SPARSE_LCN) {\n\t\t \n\t\treturn 0;\n\t}\n\n\tif (clen >= clst_frame) {\n\t\t \n\t\t*clst_data = clst_frame;\n\t\treturn 0;\n\t}\n\n\talen = bytes_to_cluster(ni->mi.sbi, le64_to_cpu(attr->nres.alloc_size));\n\tslen = 0;\n\t*clst_data = clen;\n\n\t \n\twhile ((vcn += clen) < alen) {\n\t\tvcn_next = vcn;\n\n\t\tif (!run_get_entry(run, ++idx, &vcn, &lcn, &clen) ||\n\t\t    vcn_next != vcn) {\n\t\t\terr = attr_load_runs_vcn(ni, attr->type,\n\t\t\t\t\t\t attr_name(attr),\n\t\t\t\t\t\t attr->name_len, run, vcn_next);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tvcn = vcn_next;\n\n\t\t\tif (!run_lookup_entry(run, vcn, &lcn, &clen, &idx))\n\t\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (lcn == SPARSE_LCN) {\n\t\t\tslen += clen;\n\t\t} else {\n\t\t\tif (slen) {\n\t\t\t\t \n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\t*clst_data += clen;\n\t\t}\n\n\t\tif (*clst_data + slen >= clst_frame) {\n\t\t\tif (!slen) {\n\t\t\t\t \n\t\t\t\t*clst_data = clst_frame;\n\t\t\t} else {\n\t\t\t\t \n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nint attr_allocate_frame(struct ntfs_inode *ni, CLST frame, size_t compr_size,\n\t\t\tu64 new_valid)\n{\n\tint err = 0;\n\tstruct runs_tree *run = &ni->file.run;\n\tstruct ntfs_sb_info *sbi = ni->mi.sbi;\n\tstruct ATTRIB *attr = NULL, *attr_b;\n\tstruct ATTR_LIST_ENTRY *le, *le_b;\n\tstruct mft_inode *mi, *mi_b;\n\tCLST svcn, evcn1, next_svcn, len;\n\tCLST vcn, end, clst_data;\n\tu64 total_size, valid_size, data_size;\n\n\tle_b = NULL;\n\tattr_b = ni_find_attr(ni, NULL, &le_b, ATTR_DATA, NULL, 0, NULL, &mi_b);\n\tif (!attr_b)\n\t\treturn -ENOENT;\n\n\tif (!is_attr_ext(attr_b))\n\t\treturn -EINVAL;\n\n\tvcn = frame << NTFS_LZNT_CUNIT;\n\ttotal_size = le64_to_cpu(attr_b->nres.total_size);\n\n\tsvcn = le64_to_cpu(attr_b->nres.svcn);\n\tevcn1 = le64_to_cpu(attr_b->nres.evcn) + 1;\n\tdata_size = le64_to_cpu(attr_b->nres.data_size);\n\n\tif (svcn <= vcn && vcn < evcn1) {\n\t\tattr = attr_b;\n\t\tle = le_b;\n\t\tmi = mi_b;\n\t} else if (!le_b) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t} else {\n\t\tle = le_b;\n\t\tattr = ni_find_attr(ni, attr_b, &le, ATTR_DATA, NULL, 0, &vcn,\n\t\t\t\t    &mi);\n\t\tif (!attr) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\tevcn1 = le64_to_cpu(attr->nres.evcn) + 1;\n\t}\n\n\terr = attr_load_runs(attr, ni, run, NULL);\n\tif (err)\n\t\tgoto out;\n\n\terr = attr_is_frame_compressed(ni, attr_b, frame, &clst_data);\n\tif (err)\n\t\tgoto out;\n\n\ttotal_size -= (u64)clst_data << sbi->cluster_bits;\n\n\tlen = bytes_to_cluster(sbi, compr_size);\n\n\tif (len == clst_data)\n\t\tgoto out;\n\n\tif (len < clst_data) {\n\t\terr = run_deallocate_ex(sbi, run, vcn + len, clst_data - len,\n\t\t\t\t\tNULL, true);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (!run_add_entry(run, vcn + len, SPARSE_LCN, clst_data - len,\n\t\t\t\t   false)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tend = vcn + clst_data;\n\t\t \n\t} else {\n\t\tCLST alen, hint = 0;\n\t\t \n\t\tif (vcn + clst_data &&\n\t\t    !run_lookup_entry(run, vcn + clst_data - 1, &hint, NULL,\n\t\t\t\t      NULL)) {\n\t\t\thint = -1;\n\t\t}\n\n\t\terr = attr_allocate_clusters(sbi, run, vcn + clst_data,\n\t\t\t\t\t     hint + 1, len - clst_data, NULL,\n\t\t\t\t\t     ALLOCATE_DEF, &alen, 0, NULL,\n\t\t\t\t\t     NULL);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tend = vcn + len;\n\t\t \n\t}\n\n\ttotal_size += (u64)len << sbi->cluster_bits;\n\nrepack:\n\terr = mi_pack_runs(mi, attr, run, max(end, evcn1) - svcn);\n\tif (err)\n\t\tgoto out;\n\n\tattr_b->nres.total_size = cpu_to_le64(total_size);\n\tinode_set_bytes(&ni->vfs_inode, total_size);\n\n\tmi_b->dirty = true;\n\tmark_inode_dirty(&ni->vfs_inode);\n\n\t \n\tnext_svcn = le64_to_cpu(attr->nres.evcn) + 1;\n\n\tif (end <= evcn1) {\n\t\tif (next_svcn == evcn1) {\n\t\t\t \n\t\t\tgoto ok;\n\t\t}\n\t\t \n\t\tif (!ni->attr_list.size) {\n\t\t\terr = ni_create_attr_list(ni);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\t \n\t\t\tle_b = NULL;\n\t\t\tattr_b = ni_find_attr(ni, NULL, &le_b, ATTR_DATA, NULL,\n\t\t\t\t\t      0, NULL, &mi_b);\n\t\t\tif (!attr_b)\n\t\t\t\treturn -ENOENT;\n\n\t\t\tattr = attr_b;\n\t\t\tle = le_b;\n\t\t\tmi = mi_b;\n\t\t\tgoto repack;\n\t\t}\n\t}\n\n\tsvcn = evcn1;\n\n\t \n\tattr = ni_find_attr(ni, attr, &le, ATTR_DATA, NULL, 0, &svcn, &mi);\n\n\tif (attr) {\n\t\tCLST alloc = bytes_to_cluster(\n\t\t\tsbi, le64_to_cpu(attr_b->nres.alloc_size));\n\t\tCLST evcn = le64_to_cpu(attr->nres.evcn);\n\n\t\tif (end < next_svcn)\n\t\t\tend = next_svcn;\n\t\twhile (end > evcn) {\n\t\t\t \n\t\t\tmi_remove_attr(NULL, mi, attr);\n\n\t\t\tif (!al_remove_le(ni, le)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (evcn + 1 >= alloc) {\n\t\t\t\t \n\t\t\t\tevcn1 = evcn + 1;\n\t\t\t\tgoto ins_ext;\n\t\t\t}\n\n\t\t\tif (ni_load_mi(ni, le, &mi)) {\n\t\t\t\tattr = NULL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tattr = mi_find_attr(mi, NULL, ATTR_DATA, NULL, 0,\n\t\t\t\t\t    &le->id);\n\t\t\tif (!attr) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\t\tevcn = le64_to_cpu(attr->nres.evcn);\n\t\t}\n\n\t\tif (end < svcn)\n\t\t\tend = svcn;\n\n\t\terr = attr_load_runs(attr, ni, run, &end);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tevcn1 = evcn + 1;\n\t\tattr->nres.svcn = cpu_to_le64(next_svcn);\n\t\terr = mi_pack_runs(mi, attr, run, evcn1 - next_svcn);\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tle->vcn = cpu_to_le64(next_svcn);\n\t\tni->attr_list.dirty = true;\n\t\tmi->dirty = true;\n\n\t\tnext_svcn = le64_to_cpu(attr->nres.evcn) + 1;\n\t}\nins_ext:\n\tif (evcn1 > next_svcn) {\n\t\terr = ni_insert_nonresident(ni, ATTR_DATA, NULL, 0, run,\n\t\t\t\t\t    next_svcn, evcn1 - next_svcn,\n\t\t\t\t\t    attr_b->flags, &attr, &mi, NULL);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\nok:\n\trun_truncate_around(run, vcn);\nout:\n\tif (new_valid > data_size)\n\t\tnew_valid = data_size;\n\n\tvalid_size = le64_to_cpu(attr_b->nres.valid_size);\n\tif (new_valid != valid_size) {\n\t\tattr_b->nres.valid_size = cpu_to_le64(valid_size);\n\t\tmi_b->dirty = true;\n\t}\n\n\treturn err;\n}\n\n \nint attr_collapse_range(struct ntfs_inode *ni, u64 vbo, u64 bytes)\n{\n\tint err = 0;\n\tstruct runs_tree *run = &ni->file.run;\n\tstruct ntfs_sb_info *sbi = ni->mi.sbi;\n\tstruct ATTRIB *attr = NULL, *attr_b;\n\tstruct ATTR_LIST_ENTRY *le, *le_b;\n\tstruct mft_inode *mi, *mi_b;\n\tCLST svcn, evcn1, len, dealloc, alen;\n\tCLST vcn, end;\n\tu64 valid_size, data_size, alloc_size, total_size;\n\tu32 mask;\n\t__le16 a_flags;\n\n\tif (!bytes)\n\t\treturn 0;\n\n\tle_b = NULL;\n\tattr_b = ni_find_attr(ni, NULL, &le_b, ATTR_DATA, NULL, 0, NULL, &mi_b);\n\tif (!attr_b)\n\t\treturn -ENOENT;\n\n\tif (!attr_b->non_res) {\n\t\t \n\t\treturn 0;\n\t}\n\n\tdata_size = le64_to_cpu(attr_b->nres.data_size);\n\talloc_size = le64_to_cpu(attr_b->nres.alloc_size);\n\ta_flags = attr_b->flags;\n\n\tif (is_attr_ext(attr_b)) {\n\t\ttotal_size = le64_to_cpu(attr_b->nres.total_size);\n\t\tmask = (sbi->cluster_size << attr_b->nres.c_unit) - 1;\n\t} else {\n\t\ttotal_size = alloc_size;\n\t\tmask = sbi->cluster_mask;\n\t}\n\n\tif ((vbo & mask) || (bytes & mask)) {\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\tif (vbo > data_size)\n\t\treturn -EINVAL;\n\n\tdown_write(&ni->file.run_lock);\n\n\tif (vbo + bytes >= data_size) {\n\t\tu64 new_valid = min(ni->i_valid, vbo);\n\n\t\t \n\t\ttruncate_setsize(&ni->vfs_inode, vbo);\n\t\terr = attr_set_size(ni, ATTR_DATA, NULL, 0, &ni->file.run, vbo,\n\t\t\t\t    &new_valid, true, NULL);\n\n\t\tif (!err && new_valid < ni->i_valid)\n\t\t\tni->i_valid = new_valid;\n\n\t\tgoto out;\n\t}\n\n\t \n\talen = alloc_size >> sbi->cluster_bits;\n\tvcn = vbo >> sbi->cluster_bits;\n\tlen = bytes >> sbi->cluster_bits;\n\tend = vcn + len;\n\tdealloc = 0;\n\n\tsvcn = le64_to_cpu(attr_b->nres.svcn);\n\tevcn1 = le64_to_cpu(attr_b->nres.evcn) + 1;\n\n\tif (svcn <= vcn && vcn < evcn1) {\n\t\tattr = attr_b;\n\t\tle = le_b;\n\t\tmi = mi_b;\n\t} else if (!le_b) {\n\t\terr = -EINVAL;\n\t\tgoto out;\n\t} else {\n\t\tle = le_b;\n\t\tattr = ni_find_attr(ni, attr_b, &le, ATTR_DATA, NULL, 0, &vcn,\n\t\t\t\t    &mi);\n\t\tif (!attr) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\tevcn1 = le64_to_cpu(attr->nres.evcn) + 1;\n\t}\n\n\tfor (;;) {\n\t\tif (svcn >= end) {\n\t\t\t \n\t\t\tattr->nres.svcn = cpu_to_le64(svcn - len);\n\t\t\tattr->nres.evcn = cpu_to_le64(evcn1 - 1 - len);\n\t\t\tif (le) {\n\t\t\t\tle->vcn = attr->nres.svcn;\n\t\t\t\tni->attr_list.dirty = true;\n\t\t\t}\n\t\t\tmi->dirty = true;\n\t\t} else if (svcn < vcn || end < evcn1) {\n\t\t\tCLST vcn1, eat, next_svcn;\n\n\t\t\t \n\t\t\terr = attr_load_runs(attr, ni, run, &svcn);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t\tvcn1 = max(vcn, svcn);\n\t\t\teat = min(end, evcn1) - vcn1;\n\n\t\t\terr = run_deallocate_ex(sbi, run, vcn1, eat, &dealloc,\n\t\t\t\t\t\ttrue);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\tif (!run_collapse_range(run, vcn1, eat)) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (svcn >= vcn) {\n\t\t\t\t \n\t\t\t\tattr->nres.svcn = cpu_to_le64(vcn);\n\t\t\t\tif (le) {\n\t\t\t\t\tle->vcn = attr->nres.svcn;\n\t\t\t\t\tni->attr_list.dirty = true;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\terr = mi_pack_runs(mi, attr, run, evcn1 - svcn - eat);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\n\t\t\tnext_svcn = le64_to_cpu(attr->nres.evcn) + 1;\n\t\t\tif (next_svcn + eat < evcn1) {\n\t\t\t\terr = ni_insert_nonresident(\n\t\t\t\t\tni, ATTR_DATA, NULL, 0, run, next_svcn,\n\t\t\t\t\tevcn1 - eat - next_svcn, a_flags, &attr,\n\t\t\t\t\t&mi, &le);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto out;\n\n\t\t\t\t \n\t\t\t\tattr_b = NULL;\n\t\t\t}\n\n\t\t\t \n\t\t\trun_truncate(run, 0);\n\t\t} else {\n\t\t\tu16 le_sz;\n\t\t\tu16 roff = le16_to_cpu(attr->nres.run_off);\n\n\t\t\tif (roff > le32_to_cpu(attr->size)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\trun_unpack_ex(RUN_DEALLOCATE, sbi, ni->mi.rno, svcn,\n\t\t\t\t      evcn1 - 1, svcn, Add2Ptr(attr, roff),\n\t\t\t\t      le32_to_cpu(attr->size) - roff);\n\n\t\t\t \n\t\t\tmi_remove_attr(NULL, mi, attr);\n\t\t\tif (!le)\n\t\t\t\tbreak;\n\n\t\t\tle_sz = le16_to_cpu(le->size);\n\t\t\tif (!al_remove_le(ni, le)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tif (evcn1 >= alen)\n\t\t\t\tbreak;\n\n\t\t\tif (!svcn) {\n\t\t\t\t \n\t\t\t\tif (ni_load_mi(ni, le, &mi)) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\n\t\t\t\t \n\t\t\t\tattr = mi_find_attr(mi, NULL, ATTR_DATA, NULL,\n\t\t\t\t\t\t    0, &le->id);\n\t\t\t\tif (!attr) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t\tgoto next_attr;\n\t\t\t}\n\t\t\tle = (struct ATTR_LIST_ENTRY *)((u8 *)le - le_sz);\n\t\t}\n\n\t\tif (evcn1 >= alen)\n\t\t\tbreak;\n\n\t\tattr = ni_enum_attr_ex(ni, attr, &le, &mi);\n\t\tif (!attr) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\nnext_attr:\n\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\tevcn1 = le64_to_cpu(attr->nres.evcn) + 1;\n\t}\n\n\tif (!attr_b) {\n\t\tle_b = NULL;\n\t\tattr_b = ni_find_attr(ni, NULL, &le_b, ATTR_DATA, NULL, 0, NULL,\n\t\t\t\t      &mi_b);\n\t\tif (!attr_b) {\n\t\t\terr = -ENOENT;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tdata_size -= bytes;\n\tvalid_size = ni->i_valid;\n\tif (vbo + bytes <= valid_size)\n\t\tvalid_size -= bytes;\n\telse if (vbo < valid_size)\n\t\tvalid_size = vbo;\n\n\tattr_b->nres.alloc_size = cpu_to_le64(alloc_size - bytes);\n\tattr_b->nres.data_size = cpu_to_le64(data_size);\n\tattr_b->nres.valid_size = cpu_to_le64(min(valid_size, data_size));\n\ttotal_size -= (u64)dealloc << sbi->cluster_bits;\n\tif (is_attr_ext(attr_b))\n\t\tattr_b->nres.total_size = cpu_to_le64(total_size);\n\tmi_b->dirty = true;\n\n\t \n\tni->i_valid = valid_size;\n\tni->vfs_inode.i_size = data_size;\n\tinode_set_bytes(&ni->vfs_inode, total_size);\n\tni->ni_flags |= NI_FLAG_UPDATE_PARENT;\n\tmark_inode_dirty(&ni->vfs_inode);\n\nout:\n\tup_write(&ni->file.run_lock);\n\tif (err)\n\t\t_ntfs_bad_inode(&ni->vfs_inode);\n\n\treturn err;\n}\n\n \nint attr_punch_hole(struct ntfs_inode *ni, u64 vbo, u64 bytes, u32 *frame_size)\n{\n\tint err = 0;\n\tstruct runs_tree *run = &ni->file.run;\n\tstruct ntfs_sb_info *sbi = ni->mi.sbi;\n\tstruct ATTRIB *attr = NULL, *attr_b;\n\tstruct ATTR_LIST_ENTRY *le, *le_b;\n\tstruct mft_inode *mi, *mi_b;\n\tCLST svcn, evcn1, vcn, len, end, alen, hole, next_svcn;\n\tu64 total_size, alloc_size;\n\tu32 mask;\n\t__le16 a_flags;\n\tstruct runs_tree run2;\n\n\tif (!bytes)\n\t\treturn 0;\n\n\tle_b = NULL;\n\tattr_b = ni_find_attr(ni, NULL, &le_b, ATTR_DATA, NULL, 0, NULL, &mi_b);\n\tif (!attr_b)\n\t\treturn -ENOENT;\n\n\tif (!attr_b->non_res) {\n\t\tu32 data_size = le32_to_cpu(attr_b->res.data_size);\n\t\tu32 from, to;\n\n\t\tif (vbo > data_size)\n\t\t\treturn 0;\n\n\t\tfrom = vbo;\n\t\tto = min_t(u64, vbo + bytes, data_size);\n\t\tmemset(Add2Ptr(resident_data(attr_b), from), 0, to - from);\n\t\treturn 0;\n\t}\n\n\tif (!is_attr_ext(attr_b))\n\t\treturn -EOPNOTSUPP;\n\n\talloc_size = le64_to_cpu(attr_b->nres.alloc_size);\n\ttotal_size = le64_to_cpu(attr_b->nres.total_size);\n\n\tif (vbo >= alloc_size) {\n\t\t \n\t\treturn 0;\n\t}\n\n\tmask = (sbi->cluster_size << attr_b->nres.c_unit) - 1;\n\n\tbytes += vbo;\n\tif (bytes > alloc_size)\n\t\tbytes = alloc_size;\n\tbytes -= vbo;\n\n\tif ((vbo & mask) || (bytes & mask)) {\n\t\t \n\t\tif (frame_size == NULL) {\n\t\t\t \n\t\t\treturn -EINVAL;\n\t\t}\n\t\t*frame_size = mask + 1;\n\t\treturn E_NTFS_NOTALIGNED;\n\t}\n\n\tdown_write(&ni->file.run_lock);\n\trun_init(&run2);\n\trun_truncate(run, 0);\n\n\t \n\talen = alloc_size >> sbi->cluster_bits;\n\tvcn = vbo >> sbi->cluster_bits;\n\tlen = bytes >> sbi->cluster_bits;\n\tend = vcn + len;\n\thole = 0;\n\n\tsvcn = le64_to_cpu(attr_b->nres.svcn);\n\tevcn1 = le64_to_cpu(attr_b->nres.evcn) + 1;\n\ta_flags = attr_b->flags;\n\n\tif (svcn <= vcn && vcn < evcn1) {\n\t\tattr = attr_b;\n\t\tle = le_b;\n\t\tmi = mi_b;\n\t} else if (!le_b) {\n\t\terr = -EINVAL;\n\t\tgoto bad_inode;\n\t} else {\n\t\tle = le_b;\n\t\tattr = ni_find_attr(ni, attr_b, &le, ATTR_DATA, NULL, 0, &vcn,\n\t\t\t\t    &mi);\n\t\tif (!attr) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto bad_inode;\n\t\t}\n\n\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\tevcn1 = le64_to_cpu(attr->nres.evcn) + 1;\n\t}\n\n\twhile (svcn < end) {\n\t\tCLST vcn1, zero, hole2 = hole;\n\n\t\terr = attr_load_runs(attr, ni, run, &svcn);\n\t\tif (err)\n\t\t\tgoto done;\n\t\tvcn1 = max(vcn, svcn);\n\t\tzero = min(end, evcn1) - vcn1;\n\n\t\t \n\t\terr = run_deallocate_ex(NULL, run, vcn1, zero, &hole2, false);\n\t\tif (err)\n\t\t\tgoto done;\n\n\t\t \n\t\tif (hole2 == hole)\n\t\t\tgoto next_attr;\n\n\t\t \n\t\terr = run_clone(run, &run2);\n\t\tif (err)\n\t\t\tgoto done;\n\n\t\t \n\t\tif (!run_add_entry(run, vcn1, SPARSE_LCN, zero, false)) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto done;\n\t\t}\n\n\t\t \n\t\terr = mi_pack_runs(mi, attr, run, evcn1 - svcn);\n\t\tif (err)\n\t\t\tgoto done;\n\t\tnext_svcn = le64_to_cpu(attr->nres.evcn) + 1;\n\t\tif (next_svcn < evcn1) {\n\t\t\t \n\t\t\terr = ni_insert_nonresident(ni, ATTR_DATA, NULL, 0, run,\n\t\t\t\t\t\t    next_svcn,\n\t\t\t\t\t\t    evcn1 - next_svcn, a_flags,\n\t\t\t\t\t\t    &attr, &mi, &le);\n\t\t\tif (err)\n\t\t\t\tgoto undo_punch;\n\n\t\t\t \n\t\t\tattr_b = NULL;\n\t\t}\n\n\t\t \n\t\trun_deallocate_ex(sbi, &run2, vcn1, zero, &hole, true);\n\nnext_attr:\n\t\t \n\t\trun_truncate(run, 0);\n\n\t\tif (evcn1 >= alen)\n\t\t\tbreak;\n\n\t\t \n\t\tattr = ni_enum_attr_ex(ni, attr, &le, &mi);\n\t\tif (!attr) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto bad_inode;\n\t\t}\n\n\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\tevcn1 = le64_to_cpu(attr->nres.evcn) + 1;\n\t}\n\ndone:\n\tif (!hole)\n\t\tgoto out;\n\n\tif (!attr_b) {\n\t\tattr_b = ni_find_attr(ni, NULL, NULL, ATTR_DATA, NULL, 0, NULL,\n\t\t\t\t      &mi_b);\n\t\tif (!attr_b) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto bad_inode;\n\t\t}\n\t}\n\n\ttotal_size -= (u64)hole << sbi->cluster_bits;\n\tattr_b->nres.total_size = cpu_to_le64(total_size);\n\tmi_b->dirty = true;\n\n\t \n\tinode_set_bytes(&ni->vfs_inode, total_size);\n\tni->ni_flags |= NI_FLAG_UPDATE_PARENT;\n\tmark_inode_dirty(&ni->vfs_inode);\n\nout:\n\trun_close(&run2);\n\tup_write(&ni->file.run_lock);\n\treturn err;\n\nbad_inode:\n\t_ntfs_bad_inode(&ni->vfs_inode);\n\tgoto out;\n\nundo_punch:\n\t \n\tif (mi_pack_runs(mi, attr, &run2, evcn1 - svcn))\n\t\tgoto bad_inode;\n\n\tgoto done;\n}\n\n \nint attr_insert_range(struct ntfs_inode *ni, u64 vbo, u64 bytes)\n{\n\tint err = 0;\n\tstruct runs_tree *run = &ni->file.run;\n\tstruct ntfs_sb_info *sbi = ni->mi.sbi;\n\tstruct ATTRIB *attr = NULL, *attr_b;\n\tstruct ATTR_LIST_ENTRY *le, *le_b;\n\tstruct mft_inode *mi, *mi_b;\n\tCLST vcn, svcn, evcn1, len, next_svcn;\n\tu64 data_size, alloc_size;\n\tu32 mask;\n\t__le16 a_flags;\n\n\tif (!bytes)\n\t\treturn 0;\n\n\tle_b = NULL;\n\tattr_b = ni_find_attr(ni, NULL, &le_b, ATTR_DATA, NULL, 0, NULL, &mi_b);\n\tif (!attr_b)\n\t\treturn -ENOENT;\n\n\tif (!is_attr_ext(attr_b)) {\n\t\t \n\t\treturn -EOPNOTSUPP;\n\t}\n\n\tif (!attr_b->non_res) {\n\t\tdata_size = le32_to_cpu(attr_b->res.data_size);\n\t\talloc_size = data_size;\n\t\tmask = sbi->cluster_mask;  \n\t} else {\n\t\tdata_size = le64_to_cpu(attr_b->nres.data_size);\n\t\talloc_size = le64_to_cpu(attr_b->nres.alloc_size);\n\t\tmask = (sbi->cluster_size << attr_b->nres.c_unit) - 1;\n\t}\n\n\tif (vbo > data_size) {\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\tif ((vbo & mask) || (bytes & mask)) {\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\t \n\tif (bytes > sbi->maxbytes_sparse - alloc_size)\n\t\treturn -EFBIG;\n\n\tvcn = vbo >> sbi->cluster_bits;\n\tlen = bytes >> sbi->cluster_bits;\n\n\tdown_write(&ni->file.run_lock);\n\n\tif (!attr_b->non_res) {\n\t\terr = attr_set_size(ni, ATTR_DATA, NULL, 0, run,\n\t\t\t\t    data_size + bytes, NULL, false, NULL);\n\n\t\tle_b = NULL;\n\t\tattr_b = ni_find_attr(ni, NULL, &le_b, ATTR_DATA, NULL, 0, NULL,\n\t\t\t\t      &mi_b);\n\t\tif (!attr_b) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto bad_inode;\n\t\t}\n\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tif (!attr_b->non_res) {\n\t\t\t \n\t\t\tchar *data = Add2Ptr(attr_b,\n\t\t\t\t\t     le16_to_cpu(attr_b->res.data_off));\n\n\t\t\tmemmove(data + bytes, data, bytes);\n\t\t\tmemset(data, 0, bytes);\n\t\t\tgoto done;\n\t\t}\n\n\t\t \n\t\tdata_size = le64_to_cpu(attr_b->nres.data_size);\n\t\talloc_size = le64_to_cpu(attr_b->nres.alloc_size);\n\t}\n\n\t \n\ta_flags = attr_b->flags;\n\tsvcn = le64_to_cpu(attr_b->nres.svcn);\n\tevcn1 = le64_to_cpu(attr_b->nres.evcn) + 1;\n\n\tif (svcn <= vcn && vcn < evcn1) {\n\t\tattr = attr_b;\n\t\tle = le_b;\n\t\tmi = mi_b;\n\t} else if (!le_b) {\n\t\terr = -EINVAL;\n\t\tgoto bad_inode;\n\t} else {\n\t\tle = le_b;\n\t\tattr = ni_find_attr(ni, attr_b, &le, ATTR_DATA, NULL, 0, &vcn,\n\t\t\t\t    &mi);\n\t\tif (!attr) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto bad_inode;\n\t\t}\n\n\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\tevcn1 = le64_to_cpu(attr->nres.evcn) + 1;\n\t}\n\n\trun_truncate(run, 0);  \n\terr = attr_load_runs(attr, ni, run, NULL);\n\tif (err)\n\t\tgoto out;\n\n\tif (!run_insert_range(run, vcn, len)) {\n\t\terr = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\t \n\terr = mi_pack_runs(mi, attr, run, evcn1 + len - svcn);\n\tif (err)\n\t\tgoto out;\n\n\tnext_svcn = le64_to_cpu(attr->nres.evcn) + 1;\n\n\twhile ((attr = ni_enum_attr_ex(ni, attr, &le, &mi)) &&\n\t       attr->type == ATTR_DATA && !attr->name_len) {\n\t\tle64_add_cpu(&attr->nres.svcn, len);\n\t\tle64_add_cpu(&attr->nres.evcn, len);\n\t\tif (le) {\n\t\t\tle->vcn = attr->nres.svcn;\n\t\t\tni->attr_list.dirty = true;\n\t\t}\n\t\tmi->dirty = true;\n\t}\n\n\tif (next_svcn < evcn1 + len) {\n\t\terr = ni_insert_nonresident(ni, ATTR_DATA, NULL, 0, run,\n\t\t\t\t\t    next_svcn, evcn1 + len - next_svcn,\n\t\t\t\t\t    a_flags, NULL, NULL, NULL);\n\n\t\tle_b = NULL;\n\t\tattr_b = ni_find_attr(ni, NULL, &le_b, ATTR_DATA, NULL, 0, NULL,\n\t\t\t\t      &mi_b);\n\t\tif (!attr_b) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto bad_inode;\n\t\t}\n\n\t\tif (err) {\n\t\t\t \n\t\t\tgoto undo_insert_range;\n\t\t}\n\t}\n\n\t \n\tif (vbo <= ni->i_valid)\n\t\tni->i_valid += bytes;\n\n\tattr_b->nres.data_size = cpu_to_le64(data_size + bytes);\n\tattr_b->nres.alloc_size = cpu_to_le64(alloc_size + bytes);\n\n\t \n\tif (ni->i_valid > data_size + bytes)\n\t\tattr_b->nres.valid_size = attr_b->nres.data_size;\n\telse\n\t\tattr_b->nres.valid_size = cpu_to_le64(ni->i_valid);\n\tmi_b->dirty = true;\n\ndone:\n\tni->vfs_inode.i_size += bytes;\n\tni->ni_flags |= NI_FLAG_UPDATE_PARENT;\n\tmark_inode_dirty(&ni->vfs_inode);\n\nout:\n\trun_truncate(run, 0);  \n\n\tup_write(&ni->file.run_lock);\n\n\treturn err;\n\nbad_inode:\n\t_ntfs_bad_inode(&ni->vfs_inode);\n\tgoto out;\n\nundo_insert_range:\n\tsvcn = le64_to_cpu(attr_b->nres.svcn);\n\tevcn1 = le64_to_cpu(attr_b->nres.evcn) + 1;\n\n\tif (svcn <= vcn && vcn < evcn1) {\n\t\tattr = attr_b;\n\t\tle = le_b;\n\t\tmi = mi_b;\n\t} else if (!le_b) {\n\t\tgoto bad_inode;\n\t} else {\n\t\tle = le_b;\n\t\tattr = ni_find_attr(ni, attr_b, &le, ATTR_DATA, NULL, 0, &vcn,\n\t\t\t\t    &mi);\n\t\tif (!attr) {\n\t\t\tgoto bad_inode;\n\t\t}\n\n\t\tsvcn = le64_to_cpu(attr->nres.svcn);\n\t\tevcn1 = le64_to_cpu(attr->nres.evcn) + 1;\n\t}\n\n\tif (attr_load_runs(attr, ni, run, NULL))\n\t\tgoto bad_inode;\n\n\tif (!run_collapse_range(run, vcn, len))\n\t\tgoto bad_inode;\n\n\tif (mi_pack_runs(mi, attr, run, evcn1 + len - svcn))\n\t\tgoto bad_inode;\n\n\twhile ((attr = ni_enum_attr_ex(ni, attr, &le, &mi)) &&\n\t       attr->type == ATTR_DATA && !attr->name_len) {\n\t\tle64_sub_cpu(&attr->nres.svcn, len);\n\t\tle64_sub_cpu(&attr->nres.evcn, len);\n\t\tif (le) {\n\t\t\tle->vcn = attr->nres.svcn;\n\t\t\tni->attr_list.dirty = true;\n\t\t}\n\t\tmi->dirty = true;\n\t}\n\n\tgoto out;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}