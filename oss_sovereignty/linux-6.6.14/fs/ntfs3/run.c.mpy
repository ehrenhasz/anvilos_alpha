{
  "module_name": "run.c",
  "hash_id": "a030f7f028288fee8fa0c8bd2ded79de73c34d6074bbc81f01ff1c7e0de8e4d0",
  "original_prompt": "Ingested from linux-6.6.14/fs/ntfs3/run.c",
  "human_readable_source": "\n \n\n#include <linux/blkdev.h>\n#include <linux/fs.h>\n#include <linux/log2.h>\n\n#include \"debug.h\"\n#include \"ntfs.h\"\n#include \"ntfs_fs.h\"\n\n \n#define NTFS3_RUN_MAX_BYTES 0x10000\n\nstruct ntfs_run {\n\tCLST vcn;  \n\tCLST len;  \n\tCLST lcn;  \n};\n\n \nstatic bool run_lookup(const struct runs_tree *run, CLST vcn, size_t *index)\n{\n\tsize_t min_idx, max_idx, mid_idx;\n\tstruct ntfs_run *r;\n\n\tif (!run->count) {\n\t\t*index = 0;\n\t\treturn false;\n\t}\n\n\tmin_idx = 0;\n\tmax_idx = run->count - 1;\n\n\t \n\tr = run->runs;\n\tif (vcn < r->vcn) {\n\t\t*index = 0;\n\t\treturn false;\n\t}\n\n\tif (vcn < r->vcn + r->len) {\n\t\t*index = 0;\n\t\treturn true;\n\t}\n\n\tr += max_idx;\n\tif (vcn >= r->vcn + r->len) {\n\t\t*index = run->count;\n\t\treturn false;\n\t}\n\n\tif (vcn >= r->vcn) {\n\t\t*index = max_idx;\n\t\treturn true;\n\t}\n\n\tdo {\n\t\tmid_idx = min_idx + ((max_idx - min_idx) >> 1);\n\t\tr = run->runs + mid_idx;\n\n\t\tif (vcn < r->vcn) {\n\t\t\tmax_idx = mid_idx - 1;\n\t\t\tif (!mid_idx)\n\t\t\t\tbreak;\n\t\t} else if (vcn >= r->vcn + r->len) {\n\t\t\tmin_idx = mid_idx + 1;\n\t\t} else {\n\t\t\t*index = mid_idx;\n\t\t\treturn true;\n\t\t}\n\t} while (min_idx <= max_idx);\n\n\t*index = max_idx + 1;\n\treturn false;\n}\n\n \nstatic void run_consolidate(struct runs_tree *run, size_t index)\n{\n\tsize_t i;\n\tstruct ntfs_run *r = run->runs + index;\n\n\twhile (index + 1 < run->count) {\n\t\t \n\t\tstruct ntfs_run *n = r + 1;\n\t\tCLST end = r->vcn + r->len;\n\t\tCLST dl;\n\n\t\t \n\t\tif (n->vcn > end)\n\t\t\tbreak;\n\n\t\tdl = end - n->vcn;\n\n\t\t \n\t\tif (dl > 0) {\n\t\t\tif (n->len <= dl)\n\t\t\t\tgoto remove_next_range;\n\n\t\t\tn->len -= dl;\n\t\t\tn->vcn += dl;\n\t\t\tif (n->lcn != SPARSE_LCN)\n\t\t\t\tn->lcn += dl;\n\t\t\tdl = 0;\n\t\t}\n\n\t\t \n\t\tif ((n->lcn == SPARSE_LCN) != (r->lcn == SPARSE_LCN)) {\n\t\t\tindex += 1;\n\t\t\tr = n;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (n->lcn != SPARSE_LCN && n->lcn != r->lcn + r->len)\n\t\t\tbreak;\n\n\t\t \n\t\tr->len += n->len - dl;\n\nremove_next_range:\n\t\ti = run->count - (index + 1);\n\t\tif (i > 1)\n\t\t\tmemmove(n, n + 1, sizeof(*n) * (i - 1));\n\n\t\trun->count -= 1;\n\t}\n}\n\n \nbool run_is_mapped_full(const struct runs_tree *run, CLST svcn, CLST evcn)\n{\n\tsize_t i;\n\tconst struct ntfs_run *r, *end;\n\tCLST next_vcn;\n\n\tif (!run_lookup(run, svcn, &i))\n\t\treturn false;\n\n\tend = run->runs + run->count;\n\tr = run->runs + i;\n\n\tfor (;;) {\n\t\tnext_vcn = r->vcn + r->len;\n\t\tif (next_vcn > evcn)\n\t\t\treturn true;\n\n\t\tif (++r >= end)\n\t\t\treturn false;\n\n\t\tif (r->vcn != next_vcn)\n\t\t\treturn false;\n\t}\n}\n\nbool run_lookup_entry(const struct runs_tree *run, CLST vcn, CLST *lcn,\n\t\t      CLST *len, size_t *index)\n{\n\tsize_t idx;\n\tCLST gap;\n\tstruct ntfs_run *r;\n\n\t \n\tif (!run->runs)\n\t\treturn false;\n\n\tif (!run_lookup(run, vcn, &idx))\n\t\treturn false;\n\n\tr = run->runs + idx;\n\n\tif (vcn >= r->vcn + r->len)\n\t\treturn false;\n\n\tgap = vcn - r->vcn;\n\tif (r->len <= gap)\n\t\treturn false;\n\n\t*lcn = r->lcn == SPARSE_LCN ? SPARSE_LCN : (r->lcn + gap);\n\n\tif (len)\n\t\t*len = r->len - gap;\n\tif (index)\n\t\t*index = idx;\n\n\treturn true;\n}\n\n \nvoid run_truncate_head(struct runs_tree *run, CLST vcn)\n{\n\tsize_t index;\n\tstruct ntfs_run *r;\n\n\tif (run_lookup(run, vcn, &index)) {\n\t\tr = run->runs + index;\n\n\t\tif (vcn > r->vcn) {\n\t\t\tCLST dlen = vcn - r->vcn;\n\n\t\t\tr->vcn = vcn;\n\t\t\tr->len -= dlen;\n\t\t\tif (r->lcn != SPARSE_LCN)\n\t\t\t\tr->lcn += dlen;\n\t\t}\n\n\t\tif (!index)\n\t\t\treturn;\n\t}\n\tr = run->runs;\n\tmemmove(r, r + index, sizeof(*r) * (run->count - index));\n\n\trun->count -= index;\n\n\tif (!run->count) {\n\t\tkvfree(run->runs);\n\t\trun->runs = NULL;\n\t\trun->allocated = 0;\n\t}\n}\n\n \nvoid run_truncate(struct runs_tree *run, CLST vcn)\n{\n\tsize_t index;\n\n\t \n\tif (run_lookup(run, vcn, &index)) {\n\t\tstruct ntfs_run *r = run->runs + index;\n\n\t\tr->len = vcn - r->vcn;\n\n\t\tif (r->len > 0)\n\t\t\tindex += 1;\n\t}\n\n\t \n\trun->count = index;\n\n\t \n\tif (!index) {\n\t\tkvfree(run->runs);\n\t\trun->runs = NULL;\n\t\trun->allocated = 0;\n\t}\n}\n\n \nvoid run_truncate_around(struct runs_tree *run, CLST vcn)\n{\n\trun_truncate_head(run, vcn);\n\n\tif (run->count >= NTFS3_RUN_MAX_BYTES / sizeof(struct ntfs_run) / 2)\n\t\trun_truncate(run, (run->runs + (run->count >> 1))->vcn);\n}\n\n \nbool run_add_entry(struct runs_tree *run, CLST vcn, CLST lcn, CLST len,\n\t\t   bool is_mft)\n{\n\tsize_t used, index;\n\tstruct ntfs_run *r;\n\tbool inrange;\n\tCLST tail_vcn = 0, tail_len = 0, tail_lcn = 0;\n\tbool should_add_tail = false;\n\n\t \n\tinrange = run_lookup(run, vcn, &index);\n\n\t \n\tif (!inrange && index > 0) {\n\t\tstruct ntfs_run *t = run->runs + index - 1;\n\n\t\tif (t->vcn + t->len == vcn &&\n\t\t    (t->lcn == SPARSE_LCN) == (lcn == SPARSE_LCN) &&\n\t\t    (lcn == SPARSE_LCN || lcn == t->lcn + t->len)) {\n\t\t\tinrange = true;\n\t\t\tindex -= 1;\n\t\t}\n\t}\n\n\t \n\tif (!inrange) {\nrequires_new_range:\n\t\t \n\t\tused = run->count * sizeof(struct ntfs_run);\n\n\t\t \n\t\tif (run->allocated < used + sizeof(struct ntfs_run)) {\n\t\t\tsize_t bytes;\n\t\t\tstruct ntfs_run *new_ptr;\n\n\t\t\t \n\t\t\tif (!used) {\n\t\t\t\tbytes = 64;\n\t\t\t} else if (used <= 16 * PAGE_SIZE) {\n\t\t\t\tif (is_power_of_2(run->allocated))\n\t\t\t\t\tbytes = run->allocated << 1;\n\t\t\t\telse\n\t\t\t\t\tbytes = (size_t)1\n\t\t\t\t\t\t<< (2 + blksize_bits(used));\n\t\t\t} else {\n\t\t\t\tbytes = run->allocated + (16 * PAGE_SIZE);\n\t\t\t}\n\n\t\t\tWARN_ON(!is_mft && bytes > NTFS3_RUN_MAX_BYTES);\n\n\t\t\tnew_ptr = kvmalloc(bytes, GFP_KERNEL);\n\n\t\t\tif (!new_ptr)\n\t\t\t\treturn false;\n\n\t\t\tr = new_ptr + index;\n\t\t\tmemcpy(new_ptr, run->runs,\n\t\t\t       index * sizeof(struct ntfs_run));\n\t\t\tmemcpy(r + 1, run->runs + index,\n\t\t\t       sizeof(struct ntfs_run) * (run->count - index));\n\n\t\t\tkvfree(run->runs);\n\t\t\trun->runs = new_ptr;\n\t\t\trun->allocated = bytes;\n\n\t\t} else {\n\t\t\tsize_t i = run->count - index;\n\n\t\t\tr = run->runs + index;\n\n\t\t\t \n\t\t\tif (i > 0)\n\t\t\t\tmemmove(r + 1, r, sizeof(struct ntfs_run) * i);\n\t\t}\n\n\t\tr->vcn = vcn;\n\t\tr->lcn = lcn;\n\t\tr->len = len;\n\t\trun->count += 1;\n\t} else {\n\t\tr = run->runs + index;\n\n\t\t \n\t\tif (((lcn == SPARSE_LCN) != (r->lcn == SPARSE_LCN)) ||\n\t\t    (lcn != SPARSE_LCN && lcn != r->lcn + (vcn - r->vcn))) {\n\t\t\tCLST to_eat = vcn - r->vcn;\n\t\t\tCLST Tovcn = to_eat + len;\n\n\t\t\tshould_add_tail = Tovcn < r->len;\n\n\t\t\tif (should_add_tail) {\n\t\t\t\ttail_lcn = r->lcn == SPARSE_LCN ?\n\t\t\t\t\t\t   SPARSE_LCN :\n\t\t\t\t\t\t   (r->lcn + Tovcn);\n\t\t\t\ttail_vcn = r->vcn + Tovcn;\n\t\t\t\ttail_len = r->len - Tovcn;\n\t\t\t}\n\n\t\t\tif (to_eat > 0) {\n\t\t\t\tr->len = to_eat;\n\t\t\t\tinrange = false;\n\t\t\t\tindex += 1;\n\t\t\t\tgoto requires_new_range;\n\t\t\t}\n\n\t\t\t \n\t\t\tr->lcn = lcn;\n\t\t}\n\n\t\t \n\t\tif (r->vcn + r->len < vcn + len)\n\t\t\tr->len += len - ((r->vcn + r->len) - vcn);\n\t}\n\n\t \n\tif (inrange && index > 0)\n\t\tindex -= 1;\n\trun_consolidate(run, index);\n\trun_consolidate(run, index + 1);\n\n\t \n\tif (should_add_tail &&\n\t    !run_add_entry(run, tail_vcn, tail_lcn, tail_len, is_mft))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nbool run_collapse_range(struct runs_tree *run, CLST vcn, CLST len)\n{\n\tsize_t index, eat;\n\tstruct ntfs_run *r, *e, *eat_start, *eat_end;\n\tCLST end;\n\n\tif (WARN_ON(!run_lookup(run, vcn, &index)))\n\t\treturn true;  \n\n\te = run->runs + run->count;\n\tr = run->runs + index;\n\tend = vcn + len;\n\n\tif (vcn > r->vcn) {\n\t\tif (r->vcn + r->len <= end) {\n\t\t\t \n\t\t\tr->len = vcn - r->vcn;\n\t\t} else if (r->lcn == SPARSE_LCN) {\n\t\t\t \n\t\t\tr->len -= len;\n\t\t} else {\n\t\t\t \n\t\t\tif (!run_add_entry(run, vcn, SPARSE_LCN, len, false))\n\t\t\t\treturn false;\n\t\t\treturn run_collapse_range(run, vcn, len);\n\t\t}\n\n\t\tr += 1;\n\t}\n\n\teat_start = r;\n\teat_end = r;\n\n\tfor (; r < e; r++) {\n\t\tCLST d;\n\n\t\tif (r->vcn >= end) {\n\t\t\tr->vcn -= len;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (r->vcn + r->len <= end) {\n\t\t\t \n\t\t\teat_end = r + 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\td = end - r->vcn;\n\t\tif (r->lcn != SPARSE_LCN)\n\t\t\tr->lcn += d;\n\t\tr->len -= d;\n\t\tr->vcn -= len - d;\n\t}\n\n\teat = eat_end - eat_start;\n\tmemmove(eat_start, eat_end, (e - eat_end) * sizeof(*r));\n\trun->count -= eat;\n\n\treturn true;\n}\n\n \nbool run_insert_range(struct runs_tree *run, CLST vcn, CLST len)\n{\n\tsize_t index;\n\tstruct ntfs_run *r, *e;\n\n\tif (WARN_ON(!run_lookup(run, vcn, &index)))\n\t\treturn false;  \n\n\te = run->runs + run->count;\n\tr = run->runs + index;\n\n\tif (vcn > r->vcn)\n\t\tr += 1;\n\n\tfor (; r < e; r++)\n\t\tr->vcn += len;\n\n\tr = run->runs + index;\n\n\tif (vcn > r->vcn) {\n\t\t \n\t\tCLST len1 = vcn - r->vcn;\n\t\tCLST len2 = r->len - len1;\n\t\tCLST lcn2 = r->lcn == SPARSE_LCN ? SPARSE_LCN : (r->lcn + len1);\n\n\t\tr->len = len1;\n\n\t\tif (!run_add_entry(run, vcn + len, lcn2, len2, false))\n\t\t\treturn false;\n\t}\n\n\tif (!run_add_entry(run, vcn, SPARSE_LCN, len, false))\n\t\treturn false;\n\n\treturn true;\n}\n\n \nbool run_get_entry(const struct runs_tree *run, size_t index, CLST *vcn,\n\t\t   CLST *lcn, CLST *len)\n{\n\tconst struct ntfs_run *r;\n\n\tif (index >= run->count)\n\t\treturn false;\n\n\tr = run->runs + index;\n\n\tif (!r->len)\n\t\treturn false;\n\n\tif (vcn)\n\t\t*vcn = r->vcn;\n\tif (lcn)\n\t\t*lcn = r->lcn;\n\tif (len)\n\t\t*len = r->len;\n\treturn true;\n}\n\n \n#ifdef __BIG_ENDIAN\nstatic inline int run_packed_size(const s64 n)\n{\n\tconst u8 *p = (const u8 *)&n + sizeof(n) - 1;\n\n\tif (n >= 0) {\n\t\tif (p[-7] || p[-6] || p[-5] || p[-4])\n\t\t\tp -= 4;\n\t\tif (p[-3] || p[-2])\n\t\t\tp -= 2;\n\t\tif (p[-1])\n\t\t\tp -= 1;\n\t\tif (p[0] & 0x80)\n\t\t\tp -= 1;\n\t} else {\n\t\tif (p[-7] != 0xff || p[-6] != 0xff || p[-5] != 0xff ||\n\t\t    p[-4] != 0xff)\n\t\t\tp -= 4;\n\t\tif (p[-3] != 0xff || p[-2] != 0xff)\n\t\t\tp -= 2;\n\t\tif (p[-1] != 0xff)\n\t\t\tp -= 1;\n\t\tif (!(p[0] & 0x80))\n\t\t\tp -= 1;\n\t}\n\treturn (const u8 *)&n + sizeof(n) - p;\n}\n\n \nstatic inline void run_pack_s64(u8 *run_buf, u8 size, s64 v)\n{\n\tconst u8 *p = (u8 *)&v;\n\n\tswitch (size) {\n\tcase 8:\n\t\trun_buf[7] = p[0];\n\t\tfallthrough;\n\tcase 7:\n\t\trun_buf[6] = p[1];\n\t\tfallthrough;\n\tcase 6:\n\t\trun_buf[5] = p[2];\n\t\tfallthrough;\n\tcase 5:\n\t\trun_buf[4] = p[3];\n\t\tfallthrough;\n\tcase 4:\n\t\trun_buf[3] = p[4];\n\t\tfallthrough;\n\tcase 3:\n\t\trun_buf[2] = p[5];\n\t\tfallthrough;\n\tcase 2:\n\t\trun_buf[1] = p[6];\n\t\tfallthrough;\n\tcase 1:\n\t\trun_buf[0] = p[7];\n\t}\n}\n\n \nstatic inline s64 run_unpack_s64(const u8 *run_buf, u8 size, s64 v)\n{\n\tu8 *p = (u8 *)&v;\n\n\tswitch (size) {\n\tcase 8:\n\t\tp[0] = run_buf[7];\n\t\tfallthrough;\n\tcase 7:\n\t\tp[1] = run_buf[6];\n\t\tfallthrough;\n\tcase 6:\n\t\tp[2] = run_buf[5];\n\t\tfallthrough;\n\tcase 5:\n\t\tp[3] = run_buf[4];\n\t\tfallthrough;\n\tcase 4:\n\t\tp[4] = run_buf[3];\n\t\tfallthrough;\n\tcase 3:\n\t\tp[5] = run_buf[2];\n\t\tfallthrough;\n\tcase 2:\n\t\tp[6] = run_buf[1];\n\t\tfallthrough;\n\tcase 1:\n\t\tp[7] = run_buf[0];\n\t}\n\treturn v;\n}\n\n#else\n\nstatic inline int run_packed_size(const s64 n)\n{\n\tconst u8 *p = (const u8 *)&n;\n\n\tif (n >= 0) {\n\t\tif (p[7] || p[6] || p[5] || p[4])\n\t\t\tp += 4;\n\t\tif (p[3] || p[2])\n\t\t\tp += 2;\n\t\tif (p[1])\n\t\t\tp += 1;\n\t\tif (p[0] & 0x80)\n\t\t\tp += 1;\n\t} else {\n\t\tif (p[7] != 0xff || p[6] != 0xff || p[5] != 0xff ||\n\t\t    p[4] != 0xff)\n\t\t\tp += 4;\n\t\tif (p[3] != 0xff || p[2] != 0xff)\n\t\t\tp += 2;\n\t\tif (p[1] != 0xff)\n\t\t\tp += 1;\n\t\tif (!(p[0] & 0x80))\n\t\t\tp += 1;\n\t}\n\n\treturn 1 + p - (const u8 *)&n;\n}\n\n \nstatic inline void run_pack_s64(u8 *run_buf, u8 size, s64 v)\n{\n\tconst u8 *p = (u8 *)&v;\n\n\t \n\tswitch (size) {\n\tcase 8:\n\t\trun_buf[7] = p[7];\n\t\tfallthrough;\n\tcase 7:\n\t\trun_buf[6] = p[6];\n\t\tfallthrough;\n\tcase 6:\n\t\trun_buf[5] = p[5];\n\t\tfallthrough;\n\tcase 5:\n\t\trun_buf[4] = p[4];\n\t\tfallthrough;\n\tcase 4:\n\t\trun_buf[3] = p[3];\n\t\tfallthrough;\n\tcase 3:\n\t\trun_buf[2] = p[2];\n\t\tfallthrough;\n\tcase 2:\n\t\trun_buf[1] = p[1];\n\t\tfallthrough;\n\tcase 1:\n\t\trun_buf[0] = p[0];\n\t}\n}\n\n \nstatic inline s64 run_unpack_s64(const u8 *run_buf, u8 size, s64 v)\n{\n\tu8 *p = (u8 *)&v;\n\n\t \n\tswitch (size) {\n\tcase 8:\n\t\tp[7] = run_buf[7];\n\t\tfallthrough;\n\tcase 7:\n\t\tp[6] = run_buf[6];\n\t\tfallthrough;\n\tcase 6:\n\t\tp[5] = run_buf[5];\n\t\tfallthrough;\n\tcase 5:\n\t\tp[4] = run_buf[4];\n\t\tfallthrough;\n\tcase 4:\n\t\tp[3] = run_buf[3];\n\t\tfallthrough;\n\tcase 3:\n\t\tp[2] = run_buf[2];\n\t\tfallthrough;\n\tcase 2:\n\t\tp[1] = run_buf[1];\n\t\tfallthrough;\n\tcase 1:\n\t\tp[0] = run_buf[0];\n\t}\n\treturn v;\n}\n#endif\n\n \nint run_pack(const struct runs_tree *run, CLST svcn, CLST len, u8 *run_buf,\n\t     u32 run_buf_size, CLST *packed_vcns)\n{\n\tCLST next_vcn, vcn, lcn;\n\tCLST prev_lcn = 0;\n\tCLST evcn1 = svcn + len;\n\tconst struct ntfs_run *r, *r_end;\n\tint packed_size = 0;\n\tsize_t i;\n\ts64 dlcn;\n\tint offset_size, size_size, tmp;\n\n\t*packed_vcns = 0;\n\n\tif (!len)\n\t\tgoto out;\n\n\t \n\tif (!run_lookup(run, svcn, &i))\n\t\treturn -ENOENT;\n\n\tr_end = run->runs + run->count;\n\tr = run->runs + i;\n\n\tfor (next_vcn = r->vcn + r->len; next_vcn < evcn1;\n\t     next_vcn = r->vcn + r->len) {\n\t\tif (++r >= r_end || r->vcn != next_vcn)\n\t\t\treturn -ENOENT;\n\t}\n\n\t \n\tr = run->runs + i;\n\tlen = svcn - r->vcn;\n\tvcn = svcn;\n\tlcn = r->lcn == SPARSE_LCN ? SPARSE_LCN : (r->lcn + len);\n\tlen = r->len - len;\n\n\tfor (;;) {\n\t\tnext_vcn = vcn + len;\n\t\tif (next_vcn > evcn1)\n\t\t\tlen = evcn1 - vcn;\n\n\t\t \n\t\tsize_size = run_packed_size(len);\n\n\t\t \n\t\tif (lcn == SPARSE_LCN) {\n\t\t\toffset_size = 0;\n\t\t\tdlcn = 0;\n\t\t} else {\n\t\t\t \n\t\t\tdlcn = (s64)lcn - prev_lcn;\n\t\t\toffset_size = run_packed_size(dlcn);\n\t\t\tprev_lcn = lcn;\n\t\t}\n\n\t\ttmp = run_buf_size - packed_size - 2 - offset_size;\n\t\tif (tmp <= 0)\n\t\t\tgoto out;\n\n\t\t \n\t\tif (tmp < size_size)\n\t\t\tgoto out;\n\n\t\tif (run_buf) {\n\t\t\t \n\t\t\trun_buf[0] = ((u8)(size_size | (offset_size << 4)));\n\t\t\trun_buf += 1;\n\n\t\t\t \n\t\t\trun_pack_s64(run_buf, size_size, len);\n\n\t\t\trun_buf += size_size;\n\t\t\t \n\t\t\trun_pack_s64(run_buf, offset_size, dlcn);\n\t\t\trun_buf += offset_size;\n\t\t}\n\n\t\tpacked_size += 1 + offset_size + size_size;\n\t\t*packed_vcns += len;\n\n\t\tif (packed_size + 1 >= run_buf_size || next_vcn >= evcn1)\n\t\t\tgoto out;\n\n\t\tr += 1;\n\t\tvcn = r->vcn;\n\t\tlcn = r->lcn;\n\t\tlen = r->len;\n\t}\n\nout:\n\t \n\tif (run_buf)\n\t\trun_buf[0] = 0;\n\n\treturn packed_size + 1;\n}\n\n \nint run_unpack(struct runs_tree *run, struct ntfs_sb_info *sbi, CLST ino,\n\t       CLST svcn, CLST evcn, CLST vcn, const u8 *run_buf,\n\t       int run_buf_size)\n{\n\tu64 prev_lcn, vcn64, lcn, next_vcn;\n\tconst u8 *run_last, *run_0;\n\tbool is_mft = ino == MFT_REC_MFT;\n\n\tif (run_buf_size < 0)\n\t\treturn -EINVAL;\n\n\t \n\tif (evcn + 1 == svcn)\n\t\treturn 0;\n\n\tif (evcn < svcn)\n\t\treturn -EINVAL;\n\n\trun_0 = run_buf;\n\trun_last = run_buf + run_buf_size;\n\tprev_lcn = 0;\n\tvcn64 = svcn;\n\n\t \n\t \n\twhile (run_buf < run_last) {\n\t\t \n\t\tu8 size_size = *run_buf & 0xF;\n\t\t \n\t\tu8 offset_size = *run_buf++ >> 4;\n\t\tu64 len;\n\n\t\tif (!size_size)\n\t\t\tbreak;\n\n\t\t \n\t\tif (size_size > 8)\n\t\t\treturn -EINVAL;\n\n\t\tlen = run_unpack_s64(run_buf, size_size, 0);\n\t\t \n\t\trun_buf += size_size;\n\n\t\tif (!len)\n\t\t\treturn -EINVAL;\n\n\t\tif (!offset_size)\n\t\t\tlcn = SPARSE_LCN64;\n\t\telse if (offset_size <= 8) {\n\t\t\ts64 dlcn;\n\n\t\t\t \n\t\t\tdlcn = (run_buf[offset_size - 1] & 0x80) ? (s64)-1 : 0;\n\t\t\tdlcn = run_unpack_s64(run_buf, offset_size, dlcn);\n\t\t\t \n\t\t\trun_buf += offset_size;\n\n\t\t\tif (!dlcn)\n\t\t\t\treturn -EINVAL;\n\t\t\tlcn = prev_lcn + dlcn;\n\t\t\tprev_lcn = lcn;\n\t\t} else\n\t\t\treturn -EINVAL;\n\n\t\tnext_vcn = vcn64 + len;\n\t\t \n\t\tif (next_vcn > evcn + 1)\n\t\t\treturn -EINVAL;\n\n#ifndef CONFIG_NTFS3_64BIT_CLUSTER\n\t\tif (next_vcn > 0x100000000ull || (lcn + len) > 0x100000000ull) {\n\t\t\tntfs_err(\n\t\t\t\tsbi->sb,\n\t\t\t\t\"This driver is compiled without CONFIG_NTFS3_64BIT_CLUSTER (like windows driver).\\n\"\n\t\t\t\t\"Volume contains 64 bits run: vcn %llx, lcn %llx, len %llx.\\n\"\n\t\t\t\t\"Activate CONFIG_NTFS3_64BIT_CLUSTER to process this case\",\n\t\t\t\tvcn64, lcn, len);\n\t\t\treturn -EOPNOTSUPP;\n\t\t}\n#endif\n\t\tif (lcn != SPARSE_LCN64 && lcn + len > sbi->used.bitmap.nbits) {\n\t\t\t \n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (!run)\n\t\t\t;  \n\t\telse if (run == RUN_DEALLOCATE) {\n\t\t\t \n\t\t\tif (lcn != SPARSE_LCN64)\n\t\t\t\tmark_as_free_ex(sbi, lcn, len, true);\n\t\t} else if (vcn64 >= vcn) {\n\t\t\tif (!run_add_entry(run, vcn64, lcn, len, is_mft))\n\t\t\t\treturn -ENOMEM;\n\t\t} else if (next_vcn > vcn) {\n\t\t\tu64 dlen = vcn - vcn64;\n\n\t\t\tif (!run_add_entry(run, vcn, lcn + dlen, len - dlen,\n\t\t\t\t\t   is_mft))\n\t\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tvcn64 = next_vcn;\n\t}\n\n\tif (vcn64 != evcn + 1) {\n\t\t \n\t\treturn -EINVAL;\n\t}\n\n\treturn run_buf - run_0;\n}\n\n#ifdef NTFS3_CHECK_FREE_CLST\n \nint run_unpack_ex(struct runs_tree *run, struct ntfs_sb_info *sbi, CLST ino,\n\t\t  CLST svcn, CLST evcn, CLST vcn, const u8 *run_buf,\n\t\t  int run_buf_size)\n{\n\tint ret, err;\n\tCLST next_vcn, lcn, len;\n\tsize_t index;\n\tbool ok;\n\tstruct wnd_bitmap *wnd;\n\n\tret = run_unpack(run, sbi, ino, svcn, evcn, vcn, run_buf, run_buf_size);\n\tif (ret <= 0)\n\t\treturn ret;\n\n\tif (!sbi->used.bitmap.sb || !run || run == RUN_DEALLOCATE)\n\t\treturn ret;\n\n\tif (ino == MFT_REC_BADCLUST)\n\t\treturn ret;\n\n\tnext_vcn = vcn = svcn;\n\twnd = &sbi->used.bitmap;\n\n\tfor (ok = run_lookup_entry(run, vcn, &lcn, &len, &index);\n\t     next_vcn <= evcn;\n\t     ok = run_get_entry(run, ++index, &vcn, &lcn, &len)) {\n\t\tif (!ok || next_vcn != vcn)\n\t\t\treturn -EINVAL;\n\n\t\tnext_vcn = vcn + len;\n\n\t\tif (lcn == SPARSE_LCN)\n\t\t\tcontinue;\n\n\t\tif (sbi->flags & NTFS_FLAGS_NEED_REPLAY)\n\t\t\tcontinue;\n\n\t\tdown_read_nested(&wnd->rw_lock, BITMAP_MUTEX_CLUSTERS);\n\t\t \n\t\tok = wnd_is_used(wnd, lcn, len);\n\t\tup_read(&wnd->rw_lock);\n\t\tif (ok)\n\t\t\tcontinue;\n\n\t\t \n\t\tntfs_set_state(sbi, NTFS_DIRTY_ERROR);\n\n\t\tif (down_write_trylock(&wnd->rw_lock)) {\n\t\t\t \n\t\t\tsize_t done;\n\t\t\terr = wnd_set_used_safe(wnd, lcn, len, &done);\n\t\t\tup_write(&wnd->rw_lock);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn ret;\n}\n#endif\n\n \nint run_get_highest_vcn(CLST vcn, const u8 *run_buf, u64 *highest_vcn)\n{\n\tu64 vcn64 = vcn;\n\tu8 size_size;\n\n\twhile ((size_size = *run_buf & 0xF)) {\n\t\tu8 offset_size = *run_buf++ >> 4;\n\t\tu64 len;\n\n\t\tif (size_size > 8 || offset_size > 8)\n\t\t\treturn -EINVAL;\n\n\t\tlen = run_unpack_s64(run_buf, size_size, 0);\n\t\tif (!len)\n\t\t\treturn -EINVAL;\n\n\t\trun_buf += size_size + offset_size;\n\t\tvcn64 += len;\n\n#ifndef CONFIG_NTFS3_64BIT_CLUSTER\n\t\tif (vcn64 > 0x100000000ull)\n\t\t\treturn -EINVAL;\n#endif\n\t}\n\n\t*highest_vcn = vcn64 - 1;\n\treturn 0;\n}\n\n \nint run_clone(const struct runs_tree *run, struct runs_tree *new_run)\n{\n\tsize_t bytes = run->count * sizeof(struct ntfs_run);\n\n\tif (bytes > new_run->allocated) {\n\t\tstruct ntfs_run *new_ptr = kvmalloc(bytes, GFP_KERNEL);\n\n\t\tif (!new_ptr)\n\t\t\treturn -ENOMEM;\n\n\t\tkvfree(new_run->runs);\n\t\tnew_run->runs = new_ptr;\n\t\tnew_run->allocated = bytes;\n\t}\n\n\tmemcpy(new_run->runs, run->runs, bytes);\n\tnew_run->count = run->count;\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}