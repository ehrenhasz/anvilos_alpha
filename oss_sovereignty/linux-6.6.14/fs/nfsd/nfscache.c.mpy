{
  "module_name": "nfscache.c",
  "hash_id": "4c62e659d4f9df8488ffdb6af21a1fa3325b0d78989117ac18f0d15ea226e5c0",
  "original_prompt": "Ingested from linux-6.6.14/fs/nfsd/nfscache.c",
  "human_readable_source": "\n \n\n#include <linux/sunrpc/svc_xprt.h>\n#include <linux/slab.h>\n#include <linux/vmalloc.h>\n#include <linux/sunrpc/addr.h>\n#include <linux/highmem.h>\n#include <linux/log2.h>\n#include <linux/hash.h>\n#include <net/checksum.h>\n\n#include \"nfsd.h\"\n#include \"cache.h\"\n#include \"trace.h\"\n\n \n#define TARGET_BUCKET_SIZE\t64\n\nstruct nfsd_drc_bucket {\n\tstruct rb_root rb_head;\n\tstruct list_head lru_head;\n\tspinlock_t cache_lock;\n};\n\nstatic struct kmem_cache\t*drc_slab;\n\nstatic int\tnfsd_cache_append(struct svc_rqst *rqstp, struct kvec *vec);\nstatic unsigned long nfsd_reply_cache_count(struct shrinker *shrink,\n\t\t\t\t\t    struct shrink_control *sc);\nstatic unsigned long nfsd_reply_cache_scan(struct shrinker *shrink,\n\t\t\t\t\t   struct shrink_control *sc);\n\n \nstatic unsigned int\nnfsd_cache_size_limit(void)\n{\n\tunsigned int limit;\n\tunsigned long low_pages = totalram_pages() - totalhigh_pages();\n\n\tlimit = (16 * int_sqrt(low_pages)) << (PAGE_SHIFT-10);\n\treturn min_t(unsigned int, limit, 256*1024);\n}\n\n \nstatic unsigned int\nnfsd_hashsize(unsigned int limit)\n{\n\treturn roundup_pow_of_two(limit / TARGET_BUCKET_SIZE);\n}\n\nstatic struct nfsd_cacherep *\nnfsd_cacherep_alloc(struct svc_rqst *rqstp, __wsum csum,\n\t\t    struct nfsd_net *nn)\n{\n\tstruct nfsd_cacherep *rp;\n\n\trp = kmem_cache_alloc(drc_slab, GFP_KERNEL);\n\tif (rp) {\n\t\trp->c_state = RC_UNUSED;\n\t\trp->c_type = RC_NOCACHE;\n\t\tRB_CLEAR_NODE(&rp->c_node);\n\t\tINIT_LIST_HEAD(&rp->c_lru);\n\n\t\tmemset(&rp->c_key, 0, sizeof(rp->c_key));\n\t\trp->c_key.k_xid = rqstp->rq_xid;\n\t\trp->c_key.k_proc = rqstp->rq_proc;\n\t\trpc_copy_addr((struct sockaddr *)&rp->c_key.k_addr, svc_addr(rqstp));\n\t\trpc_set_port((struct sockaddr *)&rp->c_key.k_addr, rpc_get_port(svc_addr(rqstp)));\n\t\trp->c_key.k_prot = rqstp->rq_prot;\n\t\trp->c_key.k_vers = rqstp->rq_vers;\n\t\trp->c_key.k_len = rqstp->rq_arg.len;\n\t\trp->c_key.k_csum = csum;\n\t}\n\treturn rp;\n}\n\nstatic void nfsd_cacherep_free(struct nfsd_cacherep *rp)\n{\n\tif (rp->c_type == RC_REPLBUFF)\n\t\tkfree(rp->c_replvec.iov_base);\n\tkmem_cache_free(drc_slab, rp);\n}\n\nstatic unsigned long\nnfsd_cacherep_dispose(struct list_head *dispose)\n{\n\tstruct nfsd_cacherep *rp;\n\tunsigned long freed = 0;\n\n\twhile (!list_empty(dispose)) {\n\t\trp = list_first_entry(dispose, struct nfsd_cacherep, c_lru);\n\t\tlist_del(&rp->c_lru);\n\t\tnfsd_cacherep_free(rp);\n\t\tfreed++;\n\t}\n\treturn freed;\n}\n\nstatic void\nnfsd_cacherep_unlink_locked(struct nfsd_net *nn, struct nfsd_drc_bucket *b,\n\t\t\t    struct nfsd_cacherep *rp)\n{\n\tif (rp->c_type == RC_REPLBUFF && rp->c_replvec.iov_base)\n\t\tnfsd_stats_drc_mem_usage_sub(nn, rp->c_replvec.iov_len);\n\tif (rp->c_state != RC_UNUSED) {\n\t\trb_erase(&rp->c_node, &b->rb_head);\n\t\tlist_del(&rp->c_lru);\n\t\tatomic_dec(&nn->num_drc_entries);\n\t\tnfsd_stats_drc_mem_usage_sub(nn, sizeof(*rp));\n\t}\n}\n\nstatic void\nnfsd_reply_cache_free_locked(struct nfsd_drc_bucket *b, struct nfsd_cacherep *rp,\n\t\t\t\tstruct nfsd_net *nn)\n{\n\tnfsd_cacherep_unlink_locked(nn, b, rp);\n\tnfsd_cacherep_free(rp);\n}\n\nstatic void\nnfsd_reply_cache_free(struct nfsd_drc_bucket *b, struct nfsd_cacherep *rp,\n\t\t\tstruct nfsd_net *nn)\n{\n\tspin_lock(&b->cache_lock);\n\tnfsd_cacherep_unlink_locked(nn, b, rp);\n\tspin_unlock(&b->cache_lock);\n\tnfsd_cacherep_free(rp);\n}\n\nint nfsd_drc_slab_create(void)\n{\n\tdrc_slab = kmem_cache_create(\"nfsd_drc\",\n\t\t\t\tsizeof(struct nfsd_cacherep), 0, 0, NULL);\n\treturn drc_slab ? 0: -ENOMEM;\n}\n\nvoid nfsd_drc_slab_free(void)\n{\n\tkmem_cache_destroy(drc_slab);\n}\n\n \nint nfsd_net_reply_cache_init(struct nfsd_net *nn)\n{\n\treturn nfsd_percpu_counters_init(nn->counter, NFSD_NET_COUNTERS_NUM);\n}\n\n \nvoid nfsd_net_reply_cache_destroy(struct nfsd_net *nn)\n{\n\tnfsd_percpu_counters_destroy(nn->counter, NFSD_NET_COUNTERS_NUM);\n}\n\nint nfsd_reply_cache_init(struct nfsd_net *nn)\n{\n\tunsigned int hashsize;\n\tunsigned int i;\n\tint status = 0;\n\n\tnn->max_drc_entries = nfsd_cache_size_limit();\n\tatomic_set(&nn->num_drc_entries, 0);\n\thashsize = nfsd_hashsize(nn->max_drc_entries);\n\tnn->maskbits = ilog2(hashsize);\n\n\tnn->nfsd_reply_cache_shrinker.scan_objects = nfsd_reply_cache_scan;\n\tnn->nfsd_reply_cache_shrinker.count_objects = nfsd_reply_cache_count;\n\tnn->nfsd_reply_cache_shrinker.seeks = 1;\n\tstatus = register_shrinker(&nn->nfsd_reply_cache_shrinker,\n\t\t\t\t   \"nfsd-reply:%s\", nn->nfsd_name);\n\tif (status)\n\t\treturn status;\n\n\tnn->drc_hashtbl = kvzalloc(array_size(hashsize,\n\t\t\t\tsizeof(*nn->drc_hashtbl)), GFP_KERNEL);\n\tif (!nn->drc_hashtbl)\n\t\tgoto out_shrinker;\n\n\tfor (i = 0; i < hashsize; i++) {\n\t\tINIT_LIST_HEAD(&nn->drc_hashtbl[i].lru_head);\n\t\tspin_lock_init(&nn->drc_hashtbl[i].cache_lock);\n\t}\n\tnn->drc_hashsize = hashsize;\n\n\treturn 0;\nout_shrinker:\n\tunregister_shrinker(&nn->nfsd_reply_cache_shrinker);\n\tprintk(KERN_ERR \"nfsd: failed to allocate reply cache\\n\");\n\treturn -ENOMEM;\n}\n\nvoid nfsd_reply_cache_shutdown(struct nfsd_net *nn)\n{\n\tstruct nfsd_cacherep *rp;\n\tunsigned int i;\n\n\tunregister_shrinker(&nn->nfsd_reply_cache_shrinker);\n\n\tfor (i = 0; i < nn->drc_hashsize; i++) {\n\t\tstruct list_head *head = &nn->drc_hashtbl[i].lru_head;\n\t\twhile (!list_empty(head)) {\n\t\t\trp = list_first_entry(head, struct nfsd_cacherep, c_lru);\n\t\t\tnfsd_reply_cache_free_locked(&nn->drc_hashtbl[i],\n\t\t\t\t\t\t\t\t\trp, nn);\n\t\t}\n\t}\n\n\tkvfree(nn->drc_hashtbl);\n\tnn->drc_hashtbl = NULL;\n\tnn->drc_hashsize = 0;\n\n}\n\n \nstatic void\nlru_put_end(struct nfsd_drc_bucket *b, struct nfsd_cacherep *rp)\n{\n\trp->c_timestamp = jiffies;\n\tlist_move_tail(&rp->c_lru, &b->lru_head);\n}\n\nstatic noinline struct nfsd_drc_bucket *\nnfsd_cache_bucket_find(__be32 xid, struct nfsd_net *nn)\n{\n\tunsigned int hash = hash_32((__force u32)xid, nn->maskbits);\n\n\treturn &nn->drc_hashtbl[hash];\n}\n\n \nstatic void\nnfsd_prune_bucket_locked(struct nfsd_net *nn, struct nfsd_drc_bucket *b,\n\t\t\t unsigned int max, struct list_head *dispose)\n{\n\tunsigned long expiry = jiffies - RC_EXPIRE;\n\tstruct nfsd_cacherep *rp, *tmp;\n\tunsigned int freed = 0;\n\n\tlockdep_assert_held(&b->cache_lock);\n\n\t \n\tlist_for_each_entry_safe(rp, tmp, &b->lru_head, c_lru) {\n\t\t \n\t\tif (rp->c_state == RC_INPROG)\n\t\t\tcontinue;\n\n\t\tif (atomic_read(&nn->num_drc_entries) <= nn->max_drc_entries &&\n\t\t    time_before(expiry, rp->c_timestamp))\n\t\t\tbreak;\n\n\t\tnfsd_cacherep_unlink_locked(nn, b, rp);\n\t\tlist_add(&rp->c_lru, dispose);\n\n\t\tif (max && ++freed > max)\n\t\t\tbreak;\n\t}\n}\n\n \nstatic unsigned long\nnfsd_reply_cache_count(struct shrinker *shrink, struct shrink_control *sc)\n{\n\tstruct nfsd_net *nn = container_of(shrink,\n\t\t\t\tstruct nfsd_net, nfsd_reply_cache_shrinker);\n\n\treturn atomic_read(&nn->num_drc_entries);\n}\n\n \nstatic unsigned long\nnfsd_reply_cache_scan(struct shrinker *shrink, struct shrink_control *sc)\n{\n\tstruct nfsd_net *nn = container_of(shrink,\n\t\t\t\tstruct nfsd_net, nfsd_reply_cache_shrinker);\n\tunsigned long freed = 0;\n\tLIST_HEAD(dispose);\n\tunsigned int i;\n\n\tfor (i = 0; i < nn->drc_hashsize; i++) {\n\t\tstruct nfsd_drc_bucket *b = &nn->drc_hashtbl[i];\n\n\t\tif (list_empty(&b->lru_head))\n\t\t\tcontinue;\n\n\t\tspin_lock(&b->cache_lock);\n\t\tnfsd_prune_bucket_locked(nn, b, 0, &dispose);\n\t\tspin_unlock(&b->cache_lock);\n\n\t\tfreed += nfsd_cacherep_dispose(&dispose);\n\t\tif (freed > sc->nr_to_scan)\n\t\t\tbreak;\n\t}\n\n\ttrace_nfsd_drc_gc(nn, freed);\n\treturn freed;\n}\n\n \nstatic __wsum nfsd_cache_csum(struct xdr_buf *buf, unsigned int start,\n\t\t\t      unsigned int remaining)\n{\n\tunsigned int base, len;\n\tstruct xdr_buf subbuf;\n\t__wsum csum = 0;\n\tvoid *p;\n\tint idx;\n\n\tif (remaining > RC_CSUMLEN)\n\t\tremaining = RC_CSUMLEN;\n\tif (xdr_buf_subsegment(buf, &subbuf, start, remaining))\n\t\treturn csum;\n\n\t \n\tif (subbuf.head[0].iov_len) {\n\t\tlen = min_t(unsigned int, subbuf.head[0].iov_len, remaining);\n\t\tcsum = csum_partial(subbuf.head[0].iov_base, len, csum);\n\t\tremaining -= len;\n\t}\n\n\t \n\tidx = subbuf.page_base / PAGE_SIZE;\n\tbase = subbuf.page_base & ~PAGE_MASK;\n\twhile (remaining) {\n\t\tp = page_address(subbuf.pages[idx]) + base;\n\t\tlen = min_t(unsigned int, PAGE_SIZE - base, remaining);\n\t\tcsum = csum_partial(p, len, csum);\n\t\tremaining -= len;\n\t\tbase = 0;\n\t\t++idx;\n\t}\n\treturn csum;\n}\n\nstatic int\nnfsd_cache_key_cmp(const struct nfsd_cacherep *key,\n\t\t   const struct nfsd_cacherep *rp, struct nfsd_net *nn)\n{\n\tif (key->c_key.k_xid == rp->c_key.k_xid &&\n\t    key->c_key.k_csum != rp->c_key.k_csum) {\n\t\tnfsd_stats_payload_misses_inc(nn);\n\t\ttrace_nfsd_drc_mismatch(nn, key, rp);\n\t}\n\n\treturn memcmp(&key->c_key, &rp->c_key, sizeof(key->c_key));\n}\n\n \nstatic struct nfsd_cacherep *\nnfsd_cache_insert(struct nfsd_drc_bucket *b, struct nfsd_cacherep *key,\n\t\t\tstruct nfsd_net *nn)\n{\n\tstruct nfsd_cacherep\t*rp, *ret = key;\n\tstruct rb_node\t\t**p = &b->rb_head.rb_node,\n\t\t\t\t*parent = NULL;\n\tunsigned int\t\tentries = 0;\n\tint cmp;\n\n\twhile (*p != NULL) {\n\t\t++entries;\n\t\tparent = *p;\n\t\trp = rb_entry(parent, struct nfsd_cacherep, c_node);\n\n\t\tcmp = nfsd_cache_key_cmp(key, rp, nn);\n\t\tif (cmp < 0)\n\t\t\tp = &parent->rb_left;\n\t\telse if (cmp > 0)\n\t\t\tp = &parent->rb_right;\n\t\telse {\n\t\t\tret = rp;\n\t\t\tgoto out;\n\t\t}\n\t}\n\trb_link_node(&key->c_node, parent, p);\n\trb_insert_color(&key->c_node, &b->rb_head);\nout:\n\t \n\tif (entries > nn->longest_chain) {\n\t\tnn->longest_chain = entries;\n\t\tnn->longest_chain_cachesize = atomic_read(&nn->num_drc_entries);\n\t} else if (entries == nn->longest_chain) {\n\t\t \n\t\tnn->longest_chain_cachesize = min_t(unsigned int,\n\t\t\t\tnn->longest_chain_cachesize,\n\t\t\t\tatomic_read(&nn->num_drc_entries));\n\t}\n\n\tlru_put_end(b, ret);\n\treturn ret;\n}\n\n \nint nfsd_cache_lookup(struct svc_rqst *rqstp, unsigned int start,\n\t\t      unsigned int len, struct nfsd_cacherep **cacherep)\n{\n\tstruct nfsd_net\t\t*nn;\n\tstruct nfsd_cacherep\t*rp, *found;\n\t__wsum\t\t\tcsum;\n\tstruct nfsd_drc_bucket\t*b;\n\tint type = rqstp->rq_cachetype;\n\tunsigned long freed;\n\tLIST_HEAD(dispose);\n\tint rtn = RC_DOIT;\n\n\tif (type == RC_NOCACHE) {\n\t\tnfsd_stats_rc_nocache_inc();\n\t\tgoto out;\n\t}\n\n\tcsum = nfsd_cache_csum(&rqstp->rq_arg, start, len);\n\n\t \n\tnn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\trp = nfsd_cacherep_alloc(rqstp, csum, nn);\n\tif (!rp)\n\t\tgoto out;\n\n\tb = nfsd_cache_bucket_find(rqstp->rq_xid, nn);\n\tspin_lock(&b->cache_lock);\n\tfound = nfsd_cache_insert(b, rp, nn);\n\tif (found != rp)\n\t\tgoto found_entry;\n\t*cacherep = rp;\n\trp->c_state = RC_INPROG;\n\tnfsd_prune_bucket_locked(nn, b, 3, &dispose);\n\tspin_unlock(&b->cache_lock);\n\n\tfreed = nfsd_cacherep_dispose(&dispose);\n\ttrace_nfsd_drc_gc(nn, freed);\n\n\tnfsd_stats_rc_misses_inc();\n\tatomic_inc(&nn->num_drc_entries);\n\tnfsd_stats_drc_mem_usage_add(nn, sizeof(*rp));\n\tgoto out;\n\nfound_entry:\n\t \n\tnfsd_reply_cache_free_locked(NULL, rp, nn);\n\tnfsd_stats_rc_hits_inc();\n\trtn = RC_DROPIT;\n\trp = found;\n\n\t \n\tif (rp->c_state == RC_INPROG)\n\t\tgoto out_trace;\n\n\t \n\trtn = RC_DOIT;\n\tif (!test_bit(RQ_SECURE, &rqstp->rq_flags) && rp->c_secure)\n\t\tgoto out_trace;\n\n\t \n\tswitch (rp->c_type) {\n\tcase RC_NOCACHE:\n\t\tbreak;\n\tcase RC_REPLSTAT:\n\t\txdr_stream_encode_be32(&rqstp->rq_res_stream, rp->c_replstat);\n\t\trtn = RC_REPLY;\n\t\tbreak;\n\tcase RC_REPLBUFF:\n\t\tif (!nfsd_cache_append(rqstp, &rp->c_replvec))\n\t\t\tgoto out_unlock;  \n\t\trtn = RC_REPLY;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ONCE(1, \"nfsd: bad repcache type %d\\n\", rp->c_type);\n\t}\n\nout_trace:\n\ttrace_nfsd_drc_found(nn, rqstp, rtn);\nout_unlock:\n\tspin_unlock(&b->cache_lock);\nout:\n\treturn rtn;\n}\n\n \nvoid nfsd_cache_update(struct svc_rqst *rqstp, struct nfsd_cacherep *rp,\n\t\t       int cachetype, __be32 *statp)\n{\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\tstruct kvec\t*resv = &rqstp->rq_res.head[0], *cachv;\n\tstruct nfsd_drc_bucket *b;\n\tint\t\tlen;\n\tsize_t\t\tbufsize = 0;\n\n\tif (!rp)\n\t\treturn;\n\n\tb = nfsd_cache_bucket_find(rp->c_key.k_xid, nn);\n\n\tlen = resv->iov_len - ((char*)statp - (char*)resv->iov_base);\n\tlen >>= 2;\n\n\t \n\tif (!statp || len > (256 >> 2)) {\n\t\tnfsd_reply_cache_free(b, rp, nn);\n\t\treturn;\n\t}\n\n\tswitch (cachetype) {\n\tcase RC_REPLSTAT:\n\t\tif (len != 1)\n\t\t\tprintk(\"nfsd: RC_REPLSTAT/reply len %d!\\n\",len);\n\t\trp->c_replstat = *statp;\n\t\tbreak;\n\tcase RC_REPLBUFF:\n\t\tcachv = &rp->c_replvec;\n\t\tbufsize = len << 2;\n\t\tcachv->iov_base = kmalloc(bufsize, GFP_KERNEL);\n\t\tif (!cachv->iov_base) {\n\t\t\tnfsd_reply_cache_free(b, rp, nn);\n\t\t\treturn;\n\t\t}\n\t\tcachv->iov_len = bufsize;\n\t\tmemcpy(cachv->iov_base, statp, bufsize);\n\t\tbreak;\n\tcase RC_NOCACHE:\n\t\tnfsd_reply_cache_free(b, rp, nn);\n\t\treturn;\n\t}\n\tspin_lock(&b->cache_lock);\n\tnfsd_stats_drc_mem_usage_add(nn, bufsize);\n\tlru_put_end(b, rp);\n\trp->c_secure = test_bit(RQ_SECURE, &rqstp->rq_flags);\n\trp->c_type = cachetype;\n\trp->c_state = RC_DONE;\n\tspin_unlock(&b->cache_lock);\n\treturn;\n}\n\nstatic int\nnfsd_cache_append(struct svc_rqst *rqstp, struct kvec *data)\n{\n\t__be32 *p;\n\n\tp = xdr_reserve_space(&rqstp->rq_res_stream, data->iov_len);\n\tif (unlikely(!p))\n\t\treturn false;\n\tmemcpy(p, data->iov_base, data->iov_len);\n\txdr_commit_encode(&rqstp->rq_res_stream);\n\treturn true;\n}\n\n \nint nfsd_reply_cache_stats_show(struct seq_file *m, void *v)\n{\n\tstruct nfsd_net *nn = net_generic(file_inode(m->file)->i_sb->s_fs_info,\n\t\t\t\t\t  nfsd_net_id);\n\n\tseq_printf(m, \"max entries:           %u\\n\", nn->max_drc_entries);\n\tseq_printf(m, \"num entries:           %u\\n\",\n\t\t   atomic_read(&nn->num_drc_entries));\n\tseq_printf(m, \"hash buckets:          %u\\n\", 1 << nn->maskbits);\n\tseq_printf(m, \"mem usage:             %lld\\n\",\n\t\t   percpu_counter_sum_positive(&nn->counter[NFSD_NET_DRC_MEM_USAGE]));\n\tseq_printf(m, \"cache hits:            %lld\\n\",\n\t\t   percpu_counter_sum_positive(&nfsdstats.counter[NFSD_STATS_RC_HITS]));\n\tseq_printf(m, \"cache misses:          %lld\\n\",\n\t\t   percpu_counter_sum_positive(&nfsdstats.counter[NFSD_STATS_RC_MISSES]));\n\tseq_printf(m, \"not cached:            %lld\\n\",\n\t\t   percpu_counter_sum_positive(&nfsdstats.counter[NFSD_STATS_RC_NOCACHE]));\n\tseq_printf(m, \"payload misses:        %lld\\n\",\n\t\t   percpu_counter_sum_positive(&nn->counter[NFSD_NET_PAYLOAD_MISSES]));\n\tseq_printf(m, \"longest chain len:     %u\\n\", nn->longest_chain);\n\tseq_printf(m, \"cachesize at longest:  %u\\n\", nn->longest_chain_cachesize);\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}