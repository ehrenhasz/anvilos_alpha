{
  "module_name": "aops.c",
  "hash_id": "407e35b7aa732fd9d4d7ccf5a4b6abde7cb32463b465dd83c364a477d765e535",
  "original_prompt": "Ingested from linux-6.6.14/fs/gfs2/aops.c",
  "human_readable_source": "\n \n\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/completion.h>\n#include <linux/buffer_head.h>\n#include <linux/pagemap.h>\n#include <linux/pagevec.h>\n#include <linux/mpage.h>\n#include <linux/fs.h>\n#include <linux/writeback.h>\n#include <linux/swap.h>\n#include <linux/gfs2_ondisk.h>\n#include <linux/backing-dev.h>\n#include <linux/uio.h>\n#include <trace/events/writeback.h>\n#include <linux/sched/signal.h>\n\n#include \"gfs2.h\"\n#include \"incore.h\"\n#include \"bmap.h\"\n#include \"glock.h\"\n#include \"inode.h\"\n#include \"log.h\"\n#include \"meta_io.h\"\n#include \"quota.h\"\n#include \"trans.h\"\n#include \"rgrp.h\"\n#include \"super.h\"\n#include \"util.h\"\n#include \"glops.h\"\n#include \"aops.h\"\n\n\nvoid gfs2_trans_add_databufs(struct gfs2_inode *ip, struct folio *folio,\n\t\t\t     size_t from, size_t len)\n{\n\tstruct buffer_head *head = folio_buffers(folio);\n\tunsigned int bsize = head->b_size;\n\tstruct buffer_head *bh;\n\tsize_t to = from + len;\n\tsize_t start, end;\n\n\tfor (bh = head, start = 0; bh != head || !start;\n\t     bh = bh->b_this_page, start = end) {\n\t\tend = start + bsize;\n\t\tif (end <= from)\n\t\t\tcontinue;\n\t\tif (start >= to)\n\t\t\tbreak;\n\t\tset_buffer_uptodate(bh);\n\t\tgfs2_trans_add_data(ip->i_gl, bh);\n\t}\n}\n\n \n\nstatic int gfs2_get_block_noalloc(struct inode *inode, sector_t lblock,\n\t\t\t\t  struct buffer_head *bh_result, int create)\n{\n\tint error;\n\n\terror = gfs2_block_map(inode, lblock, bh_result, 0);\n\tif (error)\n\t\treturn error;\n\tif (!buffer_mapped(bh_result))\n\t\treturn -ENODATA;\n\treturn 0;\n}\n\n \nstatic int gfs2_write_jdata_folio(struct folio *folio,\n\t\t\t\t struct writeback_control *wbc)\n{\n\tstruct inode * const inode = folio->mapping->host;\n\tloff_t i_size = i_size_read(inode);\n\n\t \n\tif (folio_pos(folio) < i_size &&\n\t    i_size < folio_pos(folio) + folio_size(folio))\n\t\tfolio_zero_segment(folio, offset_in_folio(folio, i_size),\n\t\t\t\tfolio_size(folio));\n\n\treturn __block_write_full_folio(inode, folio, gfs2_get_block_noalloc,\n\t\t\twbc, end_buffer_async_write);\n}\n\n \nstatic int __gfs2_jdata_write_folio(struct folio *folio,\n\t\tstruct writeback_control *wbc)\n{\n\tstruct inode *inode = folio->mapping->host;\n\tstruct gfs2_inode *ip = GFS2_I(inode);\n\n\tif (folio_test_checked(folio)) {\n\t\tfolio_clear_checked(folio);\n\t\tif (!folio_buffers(folio)) {\n\t\t\tfolio_create_empty_buffers(folio,\n\t\t\t\t\tinode->i_sb->s_blocksize,\n\t\t\t\t\tBIT(BH_Dirty)|BIT(BH_Uptodate));\n\t\t}\n\t\tgfs2_trans_add_databufs(ip, folio, 0, folio_size(folio));\n\t}\n\treturn gfs2_write_jdata_folio(folio, wbc);\n}\n\n \n\nstatic int gfs2_jdata_writepage(struct page *page, struct writeback_control *wbc)\n{\n\tstruct folio *folio = page_folio(page);\n\tstruct inode *inode = page->mapping->host;\n\tstruct gfs2_inode *ip = GFS2_I(inode);\n\tstruct gfs2_sbd *sdp = GFS2_SB(inode);\n\n\tif (gfs2_assert_withdraw(sdp, gfs2_glock_is_held_excl(ip->i_gl)))\n\t\tgoto out;\n\tif (folio_test_checked(folio) || current->journal_info)\n\t\tgoto out_ignore;\n\treturn __gfs2_jdata_write_folio(folio, wbc);\n\nout_ignore:\n\tfolio_redirty_for_writepage(wbc, folio);\nout:\n\tfolio_unlock(folio);\n\treturn 0;\n}\n\n \nstatic int gfs2_writepages(struct address_space *mapping,\n\t\t\t   struct writeback_control *wbc)\n{\n\tstruct gfs2_sbd *sdp = gfs2_mapping2sbd(mapping);\n\tstruct iomap_writepage_ctx wpc = { };\n\tint ret;\n\n\t \n\tret = iomap_writepages(mapping, wbc, &wpc, &gfs2_writeback_ops);\n\tif (ret == 0 && wbc->nr_to_write > 0)\n\t\tset_bit(SDF_FORCE_AIL_FLUSH, &sdp->sd_flags);\n\treturn ret;\n}\n\n \n\nstatic int gfs2_write_jdata_batch(struct address_space *mapping,\n\t\t\t\t    struct writeback_control *wbc,\n\t\t\t\t    struct folio_batch *fbatch,\n\t\t\t\t    pgoff_t *done_index)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct gfs2_sbd *sdp = GFS2_SB(inode);\n\tunsigned nrblocks;\n\tint i;\n\tint ret;\n\tint nr_pages = 0;\n\tint nr_folios = folio_batch_count(fbatch);\n\n\tfor (i = 0; i < nr_folios; i++)\n\t\tnr_pages += folio_nr_pages(fbatch->folios[i]);\n\tnrblocks = nr_pages * (PAGE_SIZE >> inode->i_blkbits);\n\n\tret = gfs2_trans_begin(sdp, nrblocks, nrblocks);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tfor (i = 0; i < nr_folios; i++) {\n\t\tstruct folio *folio = fbatch->folios[i];\n\n\t\t*done_index = folio->index;\n\n\t\tfolio_lock(folio);\n\n\t\tif (unlikely(folio->mapping != mapping)) {\ncontinue_unlock:\n\t\t\tfolio_unlock(folio);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!folio_test_dirty(folio)) {\n\t\t\t \n\t\t\tgoto continue_unlock;\n\t\t}\n\n\t\tif (folio_test_writeback(folio)) {\n\t\t\tif (wbc->sync_mode != WB_SYNC_NONE)\n\t\t\t\tfolio_wait_writeback(folio);\n\t\t\telse\n\t\t\t\tgoto continue_unlock;\n\t\t}\n\n\t\tBUG_ON(folio_test_writeback(folio));\n\t\tif (!folio_clear_dirty_for_io(folio))\n\t\t\tgoto continue_unlock;\n\n\t\ttrace_wbc_writepage(wbc, inode_to_bdi(inode));\n\n\t\tret = __gfs2_jdata_write_folio(folio, wbc);\n\t\tif (unlikely(ret)) {\n\t\t\tif (ret == AOP_WRITEPAGE_ACTIVATE) {\n\t\t\t\tfolio_unlock(folio);\n\t\t\t\tret = 0;\n\t\t\t} else {\n\n\t\t\t\t \n\t\t\t\t*done_index = folio_next_index(folio);\n\t\t\t\tret = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (--wbc->nr_to_write <= 0 && wbc->sync_mode == WB_SYNC_NONE) {\n\t\t\tret = 1;\n\t\t\tbreak;\n\t\t}\n\n\t}\n\tgfs2_trans_end(sdp);\n\treturn ret;\n}\n\n \n\nstatic int gfs2_write_cache_jdata(struct address_space *mapping,\n\t\t\t\t  struct writeback_control *wbc)\n{\n\tint ret = 0;\n\tint done = 0;\n\tstruct folio_batch fbatch;\n\tint nr_folios;\n\tpgoff_t writeback_index;\n\tpgoff_t index;\n\tpgoff_t end;\n\tpgoff_t done_index;\n\tint cycled;\n\tint range_whole = 0;\n\txa_mark_t tag;\n\n\tfolio_batch_init(&fbatch);\n\tif (wbc->range_cyclic) {\n\t\twriteback_index = mapping->writeback_index;  \n\t\tindex = writeback_index;\n\t\tif (index == 0)\n\t\t\tcycled = 1;\n\t\telse\n\t\t\tcycled = 0;\n\t\tend = -1;\n\t} else {\n\t\tindex = wbc->range_start >> PAGE_SHIFT;\n\t\tend = wbc->range_end >> PAGE_SHIFT;\n\t\tif (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)\n\t\t\trange_whole = 1;\n\t\tcycled = 1;  \n\t}\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag = PAGECACHE_TAG_TOWRITE;\n\telse\n\t\ttag = PAGECACHE_TAG_DIRTY;\n\nretry:\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag_pages_for_writeback(mapping, index, end);\n\tdone_index = index;\n\twhile (!done && (index <= end)) {\n\t\tnr_folios = filemap_get_folios_tag(mapping, &index, end,\n\t\t\t\ttag, &fbatch);\n\t\tif (nr_folios == 0)\n\t\t\tbreak;\n\n\t\tret = gfs2_write_jdata_batch(mapping, wbc, &fbatch,\n\t\t\t\t&done_index);\n\t\tif (ret)\n\t\t\tdone = 1;\n\t\tif (ret > 0)\n\t\t\tret = 0;\n\t\tfolio_batch_release(&fbatch);\n\t\tcond_resched();\n\t}\n\n\tif (!cycled && !done) {\n\t\t \n\t\tcycled = 1;\n\t\tindex = 0;\n\t\tend = writeback_index - 1;\n\t\tgoto retry;\n\t}\n\n\tif (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))\n\t\tmapping->writeback_index = done_index;\n\n\treturn ret;\n}\n\n\n \n\nstatic int gfs2_jdata_writepages(struct address_space *mapping,\n\t\t\t\t struct writeback_control *wbc)\n{\n\tstruct gfs2_inode *ip = GFS2_I(mapping->host);\n\tstruct gfs2_sbd *sdp = GFS2_SB(mapping->host);\n\tint ret;\n\n\tret = gfs2_write_cache_jdata(mapping, wbc);\n\tif (ret == 0 && wbc->sync_mode == WB_SYNC_ALL) {\n\t\tgfs2_log_flush(sdp, ip->i_gl, GFS2_LOG_HEAD_FLUSH_NORMAL |\n\t\t\t       GFS2_LFC_JDATA_WPAGES);\n\t\tret = gfs2_write_cache_jdata(mapping, wbc);\n\t}\n\treturn ret;\n}\n\n \nstatic int stuffed_readpage(struct gfs2_inode *ip, struct page *page)\n{\n\tstruct buffer_head *dibh;\n\tu64 dsize = i_size_read(&ip->i_inode);\n\tvoid *kaddr;\n\tint error;\n\n\t \n\tif (unlikely(page->index)) {\n\t\tzero_user(page, 0, PAGE_SIZE);\n\t\tSetPageUptodate(page);\n\t\treturn 0;\n\t}\n\n\terror = gfs2_meta_inode_buffer(ip, &dibh);\n\tif (error)\n\t\treturn error;\n\n\tkaddr = kmap_local_page(page);\n\tmemcpy(kaddr, dibh->b_data + sizeof(struct gfs2_dinode), dsize);\n\tmemset(kaddr + dsize, 0, PAGE_SIZE - dsize);\n\tkunmap_local(kaddr);\n\tflush_dcache_page(page);\n\tbrelse(dibh);\n\tSetPageUptodate(page);\n\n\treturn 0;\n}\n\n \nstatic int gfs2_read_folio(struct file *file, struct folio *folio)\n{\n\tstruct inode *inode = folio->mapping->host;\n\tstruct gfs2_inode *ip = GFS2_I(inode);\n\tstruct gfs2_sbd *sdp = GFS2_SB(inode);\n\tint error;\n\n\tif (!gfs2_is_jdata(ip) ||\n\t    (i_blocksize(inode) == PAGE_SIZE && !folio_buffers(folio))) {\n\t\terror = iomap_read_folio(folio, &gfs2_iomap_ops);\n\t} else if (gfs2_is_stuffed(ip)) {\n\t\terror = stuffed_readpage(ip, &folio->page);\n\t\tfolio_unlock(folio);\n\t} else {\n\t\terror = mpage_read_folio(folio, gfs2_block_map);\n\t}\n\n\tif (unlikely(gfs2_withdrawn(sdp)))\n\t\treturn -EIO;\n\n\treturn error;\n}\n\n \n\nint gfs2_internal_read(struct gfs2_inode *ip, char *buf, loff_t *pos,\n                       unsigned size)\n{\n\tstruct address_space *mapping = ip->i_inode.i_mapping;\n\tunsigned long index = *pos >> PAGE_SHIFT;\n\tunsigned offset = *pos & (PAGE_SIZE - 1);\n\tunsigned copied = 0;\n\tunsigned amt;\n\tstruct page *page;\n\n\tdo {\n\t\tpage = read_cache_page(mapping, index, gfs2_read_folio, NULL);\n\t\tif (IS_ERR(page)) {\n\t\t\tif (PTR_ERR(page) == -EINTR)\n\t\t\t\tcontinue;\n\t\t\treturn PTR_ERR(page);\n\t\t}\n\t\tamt = size - copied;\n\t\tif (offset + size > PAGE_SIZE)\n\t\t\tamt = PAGE_SIZE - offset;\n\t\tmemcpy_from_page(buf + copied, page, offset, amt);\n\t\tput_page(page);\n\t\tcopied += amt;\n\t\tindex++;\n\t\toffset = 0;\n\t} while(copied < size);\n\t(*pos) += size;\n\treturn size;\n}\n\n \n\nstatic void gfs2_readahead(struct readahead_control *rac)\n{\n\tstruct inode *inode = rac->mapping->host;\n\tstruct gfs2_inode *ip = GFS2_I(inode);\n\n\tif (gfs2_is_stuffed(ip))\n\t\t;\n\telse if (gfs2_is_jdata(ip))\n\t\tmpage_readahead(rac, gfs2_block_map);\n\telse\n\t\tiomap_readahead(rac, &gfs2_iomap_ops);\n}\n\n \nvoid adjust_fs_space(struct inode *inode)\n{\n\tstruct gfs2_sbd *sdp = GFS2_SB(inode);\n\tstruct gfs2_inode *m_ip = GFS2_I(sdp->sd_statfs_inode);\n\tstruct gfs2_statfs_change_host *m_sc = &sdp->sd_statfs_master;\n\tstruct gfs2_statfs_change_host *l_sc = &sdp->sd_statfs_local;\n\tstruct buffer_head *m_bh;\n\tu64 fs_total, new_free;\n\n\tif (gfs2_trans_begin(sdp, 2 * RES_STATFS, 0) != 0)\n\t\treturn;\n\n\t \n\tfs_total = gfs2_ri_total(sdp);\n\tif (gfs2_meta_inode_buffer(m_ip, &m_bh) != 0)\n\t\tgoto out;\n\n\tspin_lock(&sdp->sd_statfs_spin);\n\tgfs2_statfs_change_in(m_sc, m_bh->b_data +\n\t\t\t      sizeof(struct gfs2_dinode));\n\tif (fs_total > (m_sc->sc_total + l_sc->sc_total))\n\t\tnew_free = fs_total - (m_sc->sc_total + l_sc->sc_total);\n\telse\n\t\tnew_free = 0;\n\tspin_unlock(&sdp->sd_statfs_spin);\n\tfs_warn(sdp, \"File system extended by %llu blocks.\\n\",\n\t\t(unsigned long long)new_free);\n\tgfs2_statfs_change(sdp, new_free, new_free, 0);\n\n\tupdate_statfs(sdp, m_bh);\n\tbrelse(m_bh);\nout:\n\tsdp->sd_rindex_uptodate = 0;\n\tgfs2_trans_end(sdp);\n}\n\nstatic bool jdata_dirty_folio(struct address_space *mapping,\n\t\tstruct folio *folio)\n{\n\tif (current->journal_info)\n\t\tfolio_set_checked(folio);\n\treturn block_dirty_folio(mapping, folio);\n}\n\n \n\nstatic sector_t gfs2_bmap(struct address_space *mapping, sector_t lblock)\n{\n\tstruct gfs2_inode *ip = GFS2_I(mapping->host);\n\tstruct gfs2_holder i_gh;\n\tsector_t dblock = 0;\n\tint error;\n\n\terror = gfs2_glock_nq_init(ip->i_gl, LM_ST_SHARED, LM_FLAG_ANY, &i_gh);\n\tif (error)\n\t\treturn 0;\n\n\tif (!gfs2_is_stuffed(ip))\n\t\tdblock = iomap_bmap(mapping, lblock, &gfs2_iomap_ops);\n\n\tgfs2_glock_dq_uninit(&i_gh);\n\n\treturn dblock;\n}\n\nstatic void gfs2_discard(struct gfs2_sbd *sdp, struct buffer_head *bh)\n{\n\tstruct gfs2_bufdata *bd;\n\n\tlock_buffer(bh);\n\tgfs2_log_lock(sdp);\n\tclear_buffer_dirty(bh);\n\tbd = bh->b_private;\n\tif (bd) {\n\t\tif (!list_empty(&bd->bd_list) && !buffer_pinned(bh))\n\t\t\tlist_del_init(&bd->bd_list);\n\t\telse {\n\t\t\tspin_lock(&sdp->sd_ail_lock);\n\t\t\tgfs2_remove_from_journal(bh, REMOVE_JDATA);\n\t\t\tspin_unlock(&sdp->sd_ail_lock);\n\t\t}\n\t}\n\tbh->b_bdev = NULL;\n\tclear_buffer_mapped(bh);\n\tclear_buffer_req(bh);\n\tclear_buffer_new(bh);\n\tgfs2_log_unlock(sdp);\n\tunlock_buffer(bh);\n}\n\nstatic void gfs2_invalidate_folio(struct folio *folio, size_t offset,\n\t\t\t\tsize_t length)\n{\n\tstruct gfs2_sbd *sdp = GFS2_SB(folio->mapping->host);\n\tsize_t stop = offset + length;\n\tint partial_page = (offset || length < folio_size(folio));\n\tstruct buffer_head *bh, *head;\n\tunsigned long pos = 0;\n\n\tBUG_ON(!folio_test_locked(folio));\n\tif (!partial_page)\n\t\tfolio_clear_checked(folio);\n\thead = folio_buffers(folio);\n\tif (!head)\n\t\tgoto out;\n\n\tbh = head;\n\tdo {\n\t\tif (pos + bh->b_size > stop)\n\t\t\treturn;\n\n\t\tif (offset <= pos)\n\t\t\tgfs2_discard(sdp, bh);\n\t\tpos += bh->b_size;\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\nout:\n\tif (!partial_page)\n\t\tfilemap_release_folio(folio, 0);\n}\n\n \n\nbool gfs2_release_folio(struct folio *folio, gfp_t gfp_mask)\n{\n\tstruct address_space *mapping = folio->mapping;\n\tstruct gfs2_sbd *sdp = gfs2_mapping2sbd(mapping);\n\tstruct buffer_head *bh, *head;\n\tstruct gfs2_bufdata *bd;\n\n\thead = folio_buffers(folio);\n\tif (!head)\n\t\treturn false;\n\n\t \n\n\tgfs2_log_lock(sdp);\n\tbh = head;\n\tdo {\n\t\tif (atomic_read(&bh->b_count))\n\t\t\tgoto cannot_release;\n\t\tbd = bh->b_private;\n\t\tif (bd && bd->bd_tr)\n\t\t\tgoto cannot_release;\n\t\tif (buffer_dirty(bh) || WARN_ON(buffer_pinned(bh)))\n\t\t\tgoto cannot_release;\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\n\tbh = head;\n\tdo {\n\t\tbd = bh->b_private;\n\t\tif (bd) {\n\t\t\tgfs2_assert_warn(sdp, bd->bd_bh == bh);\n\t\t\tbd->bd_bh = NULL;\n\t\t\tbh->b_private = NULL;\n\t\t\t \n\t\t\tif (!bd->bd_blkno && !list_empty(&bd->bd_list))\n\t\t\t\tlist_del_init(&bd->bd_list);\n\t\t\tif (list_empty(&bd->bd_list))\n\t\t\t\tkmem_cache_free(gfs2_bufdata_cachep, bd);\n\t\t}\n\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\tgfs2_log_unlock(sdp);\n\n\treturn try_to_free_buffers(folio);\n\ncannot_release:\n\tgfs2_log_unlock(sdp);\n\treturn false;\n}\n\nstatic const struct address_space_operations gfs2_aops = {\n\t.writepages = gfs2_writepages,\n\t.read_folio = gfs2_read_folio,\n\t.readahead = gfs2_readahead,\n\t.dirty_folio = iomap_dirty_folio,\n\t.release_folio = iomap_release_folio,\n\t.invalidate_folio = iomap_invalidate_folio,\n\t.bmap = gfs2_bmap,\n\t.migrate_folio = filemap_migrate_folio,\n\t.is_partially_uptodate = iomap_is_partially_uptodate,\n\t.error_remove_page = generic_error_remove_page,\n};\n\nstatic const struct address_space_operations gfs2_jdata_aops = {\n\t.writepage = gfs2_jdata_writepage,\n\t.writepages = gfs2_jdata_writepages,\n\t.read_folio = gfs2_read_folio,\n\t.readahead = gfs2_readahead,\n\t.dirty_folio = jdata_dirty_folio,\n\t.bmap = gfs2_bmap,\n\t.invalidate_folio = gfs2_invalidate_folio,\n\t.release_folio = gfs2_release_folio,\n\t.is_partially_uptodate = block_is_partially_uptodate,\n\t.error_remove_page = generic_error_remove_page,\n};\n\nvoid gfs2_set_aops(struct inode *inode)\n{\n\tif (gfs2_is_jdata(GFS2_I(inode)))\n\t\tinode->i_mapping->a_ops = &gfs2_jdata_aops;\n\telse\n\t\tinode->i_mapping->a_ops = &gfs2_aops;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}