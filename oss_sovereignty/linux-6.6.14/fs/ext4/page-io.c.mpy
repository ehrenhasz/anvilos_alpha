{
  "module_name": "page-io.c",
  "hash_id": "032caf5f72a7822dfa8bc26a84aec6aaa36012d4bd18fbb78a51db7b2a3d7809",
  "original_prompt": "Ingested from linux-6.6.14/fs/ext4/page-io.c",
  "human_readable_source": "\n \n\n#include <linux/fs.h>\n#include <linux/time.h>\n#include <linux/highuid.h>\n#include <linux/pagemap.h>\n#include <linux/quotaops.h>\n#include <linux/string.h>\n#include <linux/buffer_head.h>\n#include <linux/writeback.h>\n#include <linux/pagevec.h>\n#include <linux/mpage.h>\n#include <linux/namei.h>\n#include <linux/uio.h>\n#include <linux/bio.h>\n#include <linux/workqueue.h>\n#include <linux/kernel.h>\n#include <linux/slab.h>\n#include <linux/mm.h>\n#include <linux/sched/mm.h>\n\n#include \"ext4_jbd2.h\"\n#include \"xattr.h\"\n#include \"acl.h\"\n\nstatic struct kmem_cache *io_end_cachep;\nstatic struct kmem_cache *io_end_vec_cachep;\n\nint __init ext4_init_pageio(void)\n{\n\tio_end_cachep = KMEM_CACHE(ext4_io_end, SLAB_RECLAIM_ACCOUNT);\n\tif (io_end_cachep == NULL)\n\t\treturn -ENOMEM;\n\n\tio_end_vec_cachep = KMEM_CACHE(ext4_io_end_vec, 0);\n\tif (io_end_vec_cachep == NULL) {\n\t\tkmem_cache_destroy(io_end_cachep);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nvoid ext4_exit_pageio(void)\n{\n\tkmem_cache_destroy(io_end_cachep);\n\tkmem_cache_destroy(io_end_vec_cachep);\n}\n\nstruct ext4_io_end_vec *ext4_alloc_io_end_vec(ext4_io_end_t *io_end)\n{\n\tstruct ext4_io_end_vec *io_end_vec;\n\n\tio_end_vec = kmem_cache_zalloc(io_end_vec_cachep, GFP_NOFS);\n\tif (!io_end_vec)\n\t\treturn ERR_PTR(-ENOMEM);\n\tINIT_LIST_HEAD(&io_end_vec->list);\n\tlist_add_tail(&io_end_vec->list, &io_end->list_vec);\n\treturn io_end_vec;\n}\n\nstatic void ext4_free_io_end_vec(ext4_io_end_t *io_end)\n{\n\tstruct ext4_io_end_vec *io_end_vec, *tmp;\n\n\tif (list_empty(&io_end->list_vec))\n\t\treturn;\n\tlist_for_each_entry_safe(io_end_vec, tmp, &io_end->list_vec, list) {\n\t\tlist_del(&io_end_vec->list);\n\t\tkmem_cache_free(io_end_vec_cachep, io_end_vec);\n\t}\n}\n\nstruct ext4_io_end_vec *ext4_last_io_end_vec(ext4_io_end_t *io_end)\n{\n\tBUG_ON(list_empty(&io_end->list_vec));\n\treturn list_last_entry(&io_end->list_vec, struct ext4_io_end_vec, list);\n}\n\n \nstatic void buffer_io_error(struct buffer_head *bh)\n{\n\tprintk_ratelimited(KERN_ERR \"Buffer I/O error on device %pg, logical block %llu\\n\",\n\t\t       bh->b_bdev,\n\t\t\t(unsigned long long)bh->b_blocknr);\n}\n\nstatic void ext4_finish_bio(struct bio *bio)\n{\n\tstruct folio_iter fi;\n\n\tbio_for_each_folio_all(fi, bio) {\n\t\tstruct folio *folio = fi.folio;\n\t\tstruct folio *io_folio = NULL;\n\t\tstruct buffer_head *bh, *head;\n\t\tsize_t bio_start = fi.offset;\n\t\tsize_t bio_end = bio_start + fi.length;\n\t\tunsigned under_io = 0;\n\t\tunsigned long flags;\n\n\t\tif (fscrypt_is_bounce_folio(folio)) {\n\t\t\tio_folio = folio;\n\t\t\tfolio = fscrypt_pagecache_folio(folio);\n\t\t}\n\n\t\tif (bio->bi_status) {\n\t\t\tint err = blk_status_to_errno(bio->bi_status);\n\t\t\tfolio_set_error(folio);\n\t\t\tmapping_set_error(folio->mapping, err);\n\t\t}\n\t\tbh = head = folio_buffers(folio);\n\t\t \n\t\tspin_lock_irqsave(&head->b_uptodate_lock, flags);\n\t\tdo {\n\t\t\tif (bh_offset(bh) < bio_start ||\n\t\t\t    bh_offset(bh) + bh->b_size > bio_end) {\n\t\t\t\tif (buffer_async_write(bh))\n\t\t\t\t\tunder_io++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tclear_buffer_async_write(bh);\n\t\t\tif (bio->bi_status) {\n\t\t\t\tset_buffer_write_io_error(bh);\n\t\t\t\tbuffer_io_error(bh);\n\t\t\t}\n\t\t} while ((bh = bh->b_this_page) != head);\n\t\tspin_unlock_irqrestore(&head->b_uptodate_lock, flags);\n\t\tif (!under_io) {\n\t\t\tfscrypt_free_bounce_page(&io_folio->page);\n\t\t\tfolio_end_writeback(folio);\n\t\t}\n\t}\n}\n\nstatic void ext4_release_io_end(ext4_io_end_t *io_end)\n{\n\tstruct bio *bio, *next_bio;\n\n\tBUG_ON(!list_empty(&io_end->list));\n\tBUG_ON(io_end->flag & EXT4_IO_END_UNWRITTEN);\n\tWARN_ON(io_end->handle);\n\n\tfor (bio = io_end->bio; bio; bio = next_bio) {\n\t\tnext_bio = bio->bi_private;\n\t\text4_finish_bio(bio);\n\t\tbio_put(bio);\n\t}\n\text4_free_io_end_vec(io_end);\n\tkmem_cache_free(io_end_cachep, io_end);\n}\n\n \nstatic int ext4_end_io_end(ext4_io_end_t *io_end)\n{\n\tstruct inode *inode = io_end->inode;\n\thandle_t *handle = io_end->handle;\n\tint ret = 0;\n\n\text4_debug(\"ext4_end_io_nolock: io_end 0x%p from inode %lu,list->next 0x%p,\"\n\t\t   \"list->prev 0x%p\\n\",\n\t\t   io_end, inode->i_ino, io_end->list.next, io_end->list.prev);\n\n\tio_end->handle = NULL;\t \n\tret = ext4_convert_unwritten_io_end_vec(handle, io_end);\n\tif (ret < 0 && !ext4_forced_shutdown(inode->i_sb)) {\n\t\text4_msg(inode->i_sb, KERN_EMERG,\n\t\t\t \"failed to convert unwritten extents to written \"\n\t\t\t \"extents -- potential data loss!  \"\n\t\t\t \"(inode %lu, error %d)\", inode->i_ino, ret);\n\t}\n\text4_clear_io_unwritten_flag(io_end);\n\text4_release_io_end(io_end);\n\treturn ret;\n}\n\nstatic void dump_completed_IO(struct inode *inode, struct list_head *head)\n{\n#ifdef\tEXT4FS_DEBUG\n\tstruct list_head *cur, *before, *after;\n\text4_io_end_t *io_end, *io_end0, *io_end1;\n\n\tif (list_empty(head))\n\t\treturn;\n\n\text4_debug(\"Dump inode %lu completed io list\\n\", inode->i_ino);\n\tlist_for_each_entry(io_end, head, list) {\n\t\tcur = &io_end->list;\n\t\tbefore = cur->prev;\n\t\tio_end0 = container_of(before, ext4_io_end_t, list);\n\t\tafter = cur->next;\n\t\tio_end1 = container_of(after, ext4_io_end_t, list);\n\n\t\text4_debug(\"io 0x%p from inode %lu,prev 0x%p,next 0x%p\\n\",\n\t\t\t    io_end, inode->i_ino, io_end0, io_end1);\n\t}\n#endif\n}\n\n \nstatic void ext4_add_complete_io(ext4_io_end_t *io_end)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(io_end->inode);\n\tstruct ext4_sb_info *sbi = EXT4_SB(io_end->inode->i_sb);\n\tstruct workqueue_struct *wq;\n\tunsigned long flags;\n\n\t \n\tWARN_ON(!(io_end->flag & EXT4_IO_END_UNWRITTEN));\n\tWARN_ON(!io_end->handle && sbi->s_journal);\n\tspin_lock_irqsave(&ei->i_completed_io_lock, flags);\n\twq = sbi->rsv_conversion_wq;\n\tif (list_empty(&ei->i_rsv_conversion_list))\n\t\tqueue_work(wq, &ei->i_rsv_conversion_work);\n\tlist_add_tail(&io_end->list, &ei->i_rsv_conversion_list);\n\tspin_unlock_irqrestore(&ei->i_completed_io_lock, flags);\n}\n\nstatic int ext4_do_flush_completed_IO(struct inode *inode,\n\t\t\t\t      struct list_head *head)\n{\n\text4_io_end_t *io_end;\n\tstruct list_head unwritten;\n\tunsigned long flags;\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tint err, ret = 0;\n\n\tspin_lock_irqsave(&ei->i_completed_io_lock, flags);\n\tdump_completed_IO(inode, head);\n\tlist_replace_init(head, &unwritten);\n\tspin_unlock_irqrestore(&ei->i_completed_io_lock, flags);\n\n\twhile (!list_empty(&unwritten)) {\n\t\tio_end = list_entry(unwritten.next, ext4_io_end_t, list);\n\t\tBUG_ON(!(io_end->flag & EXT4_IO_END_UNWRITTEN));\n\t\tlist_del_init(&io_end->list);\n\n\t\terr = ext4_end_io_end(io_end);\n\t\tif (unlikely(!ret && err))\n\t\t\tret = err;\n\t}\n\treturn ret;\n}\n\n \nvoid ext4_end_io_rsv_work(struct work_struct *work)\n{\n\tstruct ext4_inode_info *ei = container_of(work, struct ext4_inode_info,\n\t\t\t\t\t\t  i_rsv_conversion_work);\n\text4_do_flush_completed_IO(&ei->vfs_inode, &ei->i_rsv_conversion_list);\n}\n\next4_io_end_t *ext4_init_io_end(struct inode *inode, gfp_t flags)\n{\n\text4_io_end_t *io_end = kmem_cache_zalloc(io_end_cachep, flags);\n\n\tif (io_end) {\n\t\tio_end->inode = inode;\n\t\tINIT_LIST_HEAD(&io_end->list);\n\t\tINIT_LIST_HEAD(&io_end->list_vec);\n\t\trefcount_set(&io_end->count, 1);\n\t}\n\treturn io_end;\n}\n\nvoid ext4_put_io_end_defer(ext4_io_end_t *io_end)\n{\n\tif (refcount_dec_and_test(&io_end->count)) {\n\t\tif (!(io_end->flag & EXT4_IO_END_UNWRITTEN) ||\n\t\t\t\tlist_empty(&io_end->list_vec)) {\n\t\t\text4_release_io_end(io_end);\n\t\t\treturn;\n\t\t}\n\t\text4_add_complete_io(io_end);\n\t}\n}\n\nint ext4_put_io_end(ext4_io_end_t *io_end)\n{\n\tint err = 0;\n\n\tif (refcount_dec_and_test(&io_end->count)) {\n\t\tif (io_end->flag & EXT4_IO_END_UNWRITTEN) {\n\t\t\terr = ext4_convert_unwritten_io_end_vec(io_end->handle,\n\t\t\t\t\t\t\t\tio_end);\n\t\t\tio_end->handle = NULL;\n\t\t\text4_clear_io_unwritten_flag(io_end);\n\t\t}\n\t\text4_release_io_end(io_end);\n\t}\n\treturn err;\n}\n\next4_io_end_t *ext4_get_io_end(ext4_io_end_t *io_end)\n{\n\trefcount_inc(&io_end->count);\n\treturn io_end;\n}\n\n \nstatic void ext4_end_bio(struct bio *bio)\n{\n\text4_io_end_t *io_end = bio->bi_private;\n\tsector_t bi_sector = bio->bi_iter.bi_sector;\n\n\tif (WARN_ONCE(!io_end, \"io_end is NULL: %pg: sector %Lu len %u err %d\\n\",\n\t\t      bio->bi_bdev,\n\t\t      (long long) bio->bi_iter.bi_sector,\n\t\t      (unsigned) bio_sectors(bio),\n\t\t      bio->bi_status)) {\n\t\text4_finish_bio(bio);\n\t\tbio_put(bio);\n\t\treturn;\n\t}\n\tbio->bi_end_io = NULL;\n\n\tif (bio->bi_status) {\n\t\tstruct inode *inode = io_end->inode;\n\n\t\text4_warning(inode->i_sb, \"I/O error %d writing to inode %lu \"\n\t\t\t     \"starting block %llu)\",\n\t\t\t     bio->bi_status, inode->i_ino,\n\t\t\t     (unsigned long long)\n\t\t\t     bi_sector >> (inode->i_blkbits - 9));\n\t\tmapping_set_error(inode->i_mapping,\n\t\t\t\tblk_status_to_errno(bio->bi_status));\n\t}\n\n\tif (io_end->flag & EXT4_IO_END_UNWRITTEN) {\n\t\t \n\t\tbio->bi_private = xchg(&io_end->bio, bio);\n\t\text4_put_io_end_defer(io_end);\n\t} else {\n\t\t \n\t\text4_put_io_end_defer(io_end);\n\t\text4_finish_bio(bio);\n\t\tbio_put(bio);\n\t}\n}\n\nvoid ext4_io_submit(struct ext4_io_submit *io)\n{\n\tstruct bio *bio = io->io_bio;\n\n\tif (bio) {\n\t\tif (io->io_wbc->sync_mode == WB_SYNC_ALL)\n\t\t\tio->io_bio->bi_opf |= REQ_SYNC;\n\t\tsubmit_bio(io->io_bio);\n\t}\n\tio->io_bio = NULL;\n}\n\nvoid ext4_io_submit_init(struct ext4_io_submit *io,\n\t\t\t struct writeback_control *wbc)\n{\n\tio->io_wbc = wbc;\n\tio->io_bio = NULL;\n\tio->io_end = NULL;\n}\n\nstatic void io_submit_init_bio(struct ext4_io_submit *io,\n\t\t\t       struct buffer_head *bh)\n{\n\tstruct bio *bio;\n\n\t \n\tbio = bio_alloc(bh->b_bdev, BIO_MAX_VECS, REQ_OP_WRITE, GFP_NOIO);\n\tfscrypt_set_bio_crypt_ctx_bh(bio, bh, GFP_NOIO);\n\tbio->bi_iter.bi_sector = bh->b_blocknr * (bh->b_size >> 9);\n\tbio->bi_end_io = ext4_end_bio;\n\tbio->bi_private = ext4_get_io_end(io->io_end);\n\tio->io_bio = bio;\n\tio->io_next_block = bh->b_blocknr;\n\twbc_init_bio(io->io_wbc, bio);\n}\n\nstatic void io_submit_add_bh(struct ext4_io_submit *io,\n\t\t\t     struct inode *inode,\n\t\t\t     struct folio *folio,\n\t\t\t     struct folio *io_folio,\n\t\t\t     struct buffer_head *bh)\n{\n\tif (io->io_bio && (bh->b_blocknr != io->io_next_block ||\n\t\t\t   !fscrypt_mergeable_bio_bh(io->io_bio, bh))) {\nsubmit_and_retry:\n\t\text4_io_submit(io);\n\t}\n\tif (io->io_bio == NULL)\n\t\tio_submit_init_bio(io, bh);\n\tif (!bio_add_folio(io->io_bio, io_folio, bh->b_size, bh_offset(bh)))\n\t\tgoto submit_and_retry;\n\twbc_account_cgroup_owner(io->io_wbc, &folio->page, bh->b_size);\n\tio->io_next_block++;\n}\n\nint ext4_bio_write_folio(struct ext4_io_submit *io, struct folio *folio,\n\t\tsize_t len)\n{\n\tstruct folio *io_folio = folio;\n\tstruct inode *inode = folio->mapping->host;\n\tunsigned block_start;\n\tstruct buffer_head *bh, *head;\n\tint ret = 0;\n\tint nr_to_submit = 0;\n\tstruct writeback_control *wbc = io->io_wbc;\n\tbool keep_towrite = false;\n\n\tBUG_ON(!folio_test_locked(folio));\n\tBUG_ON(folio_test_writeback(folio));\n\n\tfolio_clear_error(folio);\n\n\t \n\tif (len < folio_size(folio))\n\t\tfolio_zero_segment(folio, len, folio_size(folio));\n\t \n\tbh = head = folio_buffers(folio);\n\tdo {\n\t\tblock_start = bh_offset(bh);\n\t\tif (block_start >= len) {\n\t\t\tclear_buffer_dirty(bh);\n\t\t\tset_buffer_uptodate(bh);\n\t\t\tcontinue;\n\t\t}\n\t\tif (!buffer_dirty(bh) || buffer_delay(bh) ||\n\t\t    !buffer_mapped(bh) || buffer_unwritten(bh)) {\n\t\t\t \n\t\t\tif (!buffer_mapped(bh))\n\t\t\t\tclear_buffer_dirty(bh);\n\t\t\t \n\t\t\tif (buffer_dirty(bh) ||\n\t\t\t    (buffer_jbd(bh) && buffer_jbddirty(bh))) {\n\t\t\t\tif (!folio_test_dirty(folio))\n\t\t\t\t\tfolio_redirty_for_writepage(wbc, folio);\n\t\t\t\tkeep_towrite = true;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\tif (buffer_new(bh))\n\t\t\tclear_buffer_new(bh);\n\t\tset_buffer_async_write(bh);\n\t\tclear_buffer_dirty(bh);\n\t\tnr_to_submit++;\n\t} while ((bh = bh->b_this_page) != head);\n\n\t \n\tif (!nr_to_submit)\n\t\treturn 0;\n\n\tbh = head = folio_buffers(folio);\n\n\t \n\tif (fscrypt_inode_uses_fs_layer_crypto(inode)) {\n\t\tgfp_t gfp_flags = GFP_NOFS;\n\t\tunsigned int enc_bytes = round_up(len, i_blocksize(inode));\n\t\tstruct page *bounce_page;\n\n\t\t \n\t\tif (io->io_bio)\n\t\t\tgfp_flags = GFP_NOWAIT | __GFP_NOWARN;\n\tretry_encrypt:\n\t\tbounce_page = fscrypt_encrypt_pagecache_blocks(&folio->page,\n\t\t\t\t\tenc_bytes, 0, gfp_flags);\n\t\tif (IS_ERR(bounce_page)) {\n\t\t\tret = PTR_ERR(bounce_page);\n\t\t\tif (ret == -ENOMEM &&\n\t\t\t    (io->io_bio || wbc->sync_mode == WB_SYNC_ALL)) {\n\t\t\t\tgfp_t new_gfp_flags = GFP_NOFS;\n\t\t\t\tif (io->io_bio)\n\t\t\t\t\text4_io_submit(io);\n\t\t\t\telse\n\t\t\t\t\tnew_gfp_flags |= __GFP_NOFAIL;\n\t\t\t\tmemalloc_retry_wait(gfp_flags);\n\t\t\t\tgfp_flags = new_gfp_flags;\n\t\t\t\tgoto retry_encrypt;\n\t\t\t}\n\n\t\t\tprintk_ratelimited(KERN_ERR \"%s: ret = %d\\n\", __func__, ret);\n\t\t\tfolio_redirty_for_writepage(wbc, folio);\n\t\t\tdo {\n\t\t\t\tif (buffer_async_write(bh)) {\n\t\t\t\t\tclear_buffer_async_write(bh);\n\t\t\t\t\tset_buffer_dirty(bh);\n\t\t\t\t}\n\t\t\t\tbh = bh->b_this_page;\n\t\t\t} while (bh != head);\n\n\t\t\treturn ret;\n\t\t}\n\t\tio_folio = page_folio(bounce_page);\n\t}\n\n\t__folio_start_writeback(folio, keep_towrite);\n\n\t \n\tdo {\n\t\tif (!buffer_async_write(bh))\n\t\t\tcontinue;\n\t\tio_submit_add_bh(io, inode, folio, io_folio, bh);\n\t} while ((bh = bh->b_this_page) != head);\n\n\treturn 0;\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}