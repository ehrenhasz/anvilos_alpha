{
  "module_name": "readpage.c",
  "hash_id": "b59547a1bd90bcfbf989b9517a9749969c49c3150573dc42a6b566d23791b694",
  "original_prompt": "Ingested from linux-6.6.14/fs/ext4/readpage.c",
  "human_readable_source": "\n \n\n#include <linux/kernel.h>\n#include <linux/export.h>\n#include <linux/mm.h>\n#include <linux/kdev_t.h>\n#include <linux/gfp.h>\n#include <linux/bio.h>\n#include <linux/fs.h>\n#include <linux/buffer_head.h>\n#include <linux/blkdev.h>\n#include <linux/highmem.h>\n#include <linux/prefetch.h>\n#include <linux/mpage.h>\n#include <linux/writeback.h>\n#include <linux/backing-dev.h>\n#include <linux/pagevec.h>\n\n#include \"ext4.h\"\n\n#define NUM_PREALLOC_POST_READ_CTXS\t128\n\nstatic struct kmem_cache *bio_post_read_ctx_cache;\nstatic mempool_t *bio_post_read_ctx_pool;\n\n \nenum bio_post_read_step {\n\tSTEP_INITIAL = 0,\n\tSTEP_DECRYPT,\n\tSTEP_VERITY,\n\tSTEP_MAX,\n};\n\nstruct bio_post_read_ctx {\n\tstruct bio *bio;\n\tstruct work_struct work;\n\tunsigned int cur_step;\n\tunsigned int enabled_steps;\n};\n\nstatic void __read_end_io(struct bio *bio)\n{\n\tstruct folio_iter fi;\n\n\tbio_for_each_folio_all(fi, bio) {\n\t\tstruct folio *folio = fi.folio;\n\n\t\tif (bio->bi_status)\n\t\t\tfolio_clear_uptodate(folio);\n\t\telse\n\t\t\tfolio_mark_uptodate(folio);\n\t\tfolio_unlock(folio);\n\t}\n\tif (bio->bi_private)\n\t\tmempool_free(bio->bi_private, bio_post_read_ctx_pool);\n\tbio_put(bio);\n}\n\nstatic void bio_post_read_processing(struct bio_post_read_ctx *ctx);\n\nstatic void decrypt_work(struct work_struct *work)\n{\n\tstruct bio_post_read_ctx *ctx =\n\t\tcontainer_of(work, struct bio_post_read_ctx, work);\n\tstruct bio *bio = ctx->bio;\n\n\tif (fscrypt_decrypt_bio(bio))\n\t\tbio_post_read_processing(ctx);\n\telse\n\t\t__read_end_io(bio);\n}\n\nstatic void verity_work(struct work_struct *work)\n{\n\tstruct bio_post_read_ctx *ctx =\n\t\tcontainer_of(work, struct bio_post_read_ctx, work);\n\tstruct bio *bio = ctx->bio;\n\n\t \n\tBUILD_BUG_ON(STEP_VERITY + 1 != STEP_MAX);\n\tmempool_free(ctx, bio_post_read_ctx_pool);\n\tbio->bi_private = NULL;\n\n\tfsverity_verify_bio(bio);\n\n\t__read_end_io(bio);\n}\n\nstatic void bio_post_read_processing(struct bio_post_read_ctx *ctx)\n{\n\t \n\tswitch (++ctx->cur_step) {\n\tcase STEP_DECRYPT:\n\t\tif (ctx->enabled_steps & (1 << STEP_DECRYPT)) {\n\t\t\tINIT_WORK(&ctx->work, decrypt_work);\n\t\t\tfscrypt_enqueue_decrypt_work(&ctx->work);\n\t\t\treturn;\n\t\t}\n\t\tctx->cur_step++;\n\t\tfallthrough;\n\tcase STEP_VERITY:\n\t\tif (ctx->enabled_steps & (1 << STEP_VERITY)) {\n\t\t\tINIT_WORK(&ctx->work, verity_work);\n\t\t\tfsverity_enqueue_verify_work(&ctx->work);\n\t\t\treturn;\n\t\t}\n\t\tctx->cur_step++;\n\t\tfallthrough;\n\tdefault:\n\t\t__read_end_io(ctx->bio);\n\t}\n}\n\nstatic bool bio_post_read_required(struct bio *bio)\n{\n\treturn bio->bi_private && !bio->bi_status;\n}\n\n \nstatic void mpage_end_io(struct bio *bio)\n{\n\tif (bio_post_read_required(bio)) {\n\t\tstruct bio_post_read_ctx *ctx = bio->bi_private;\n\n\t\tctx->cur_step = STEP_INITIAL;\n\t\tbio_post_read_processing(ctx);\n\t\treturn;\n\t}\n\t__read_end_io(bio);\n}\n\nstatic inline bool ext4_need_verity(const struct inode *inode, pgoff_t idx)\n{\n\treturn fsverity_active(inode) &&\n\t       idx < DIV_ROUND_UP(inode->i_size, PAGE_SIZE);\n}\n\nstatic void ext4_set_bio_post_read_ctx(struct bio *bio,\n\t\t\t\t       const struct inode *inode,\n\t\t\t\t       pgoff_t first_idx)\n{\n\tunsigned int post_read_steps = 0;\n\n\tif (fscrypt_inode_uses_fs_layer_crypto(inode))\n\t\tpost_read_steps |= 1 << STEP_DECRYPT;\n\n\tif (ext4_need_verity(inode, first_idx))\n\t\tpost_read_steps |= 1 << STEP_VERITY;\n\n\tif (post_read_steps) {\n\t\t \n\t\tstruct bio_post_read_ctx *ctx =\n\t\t\tmempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);\n\n\t\tctx->bio = bio;\n\t\tctx->enabled_steps = post_read_steps;\n\t\tbio->bi_private = ctx;\n\t}\n}\n\nstatic inline loff_t ext4_readpage_limit(struct inode *inode)\n{\n\tif (IS_ENABLED(CONFIG_FS_VERITY) && IS_VERITY(inode))\n\t\treturn inode->i_sb->s_maxbytes;\n\n\treturn i_size_read(inode);\n}\n\nint ext4_mpage_readpages(struct inode *inode,\n\t\tstruct readahead_control *rac, struct folio *folio)\n{\n\tstruct bio *bio = NULL;\n\tsector_t last_block_in_bio = 0;\n\n\tconst unsigned blkbits = inode->i_blkbits;\n\tconst unsigned blocks_per_page = PAGE_SIZE >> blkbits;\n\tconst unsigned blocksize = 1 << blkbits;\n\tsector_t next_block;\n\tsector_t block_in_file;\n\tsector_t last_block;\n\tsector_t last_block_in_file;\n\tsector_t blocks[MAX_BUF_PER_PAGE];\n\tunsigned page_block;\n\tstruct block_device *bdev = inode->i_sb->s_bdev;\n\tint length;\n\tunsigned relative_block = 0;\n\tstruct ext4_map_blocks map;\n\tunsigned int nr_pages = rac ? readahead_count(rac) : 1;\n\n\tmap.m_pblk = 0;\n\tmap.m_lblk = 0;\n\tmap.m_len = 0;\n\tmap.m_flags = 0;\n\n\tfor (; nr_pages; nr_pages--) {\n\t\tint fully_mapped = 1;\n\t\tunsigned first_hole = blocks_per_page;\n\n\t\tif (rac)\n\t\t\tfolio = readahead_folio(rac);\n\t\tprefetchw(&folio->flags);\n\n\t\tif (folio_buffers(folio))\n\t\t\tgoto confused;\n\n\t\tblock_in_file = next_block =\n\t\t\t(sector_t)folio->index << (PAGE_SHIFT - blkbits);\n\t\tlast_block = block_in_file + nr_pages * blocks_per_page;\n\t\tlast_block_in_file = (ext4_readpage_limit(inode) +\n\t\t\t\t      blocksize - 1) >> blkbits;\n\t\tif (last_block > last_block_in_file)\n\t\t\tlast_block = last_block_in_file;\n\t\tpage_block = 0;\n\n\t\t \n\t\tif ((map.m_flags & EXT4_MAP_MAPPED) &&\n\t\t    block_in_file > map.m_lblk &&\n\t\t    block_in_file < (map.m_lblk + map.m_len)) {\n\t\t\tunsigned map_offset = block_in_file - map.m_lblk;\n\t\t\tunsigned last = map.m_len - map_offset;\n\n\t\t\tfor (relative_block = 0; ; relative_block++) {\n\t\t\t\tif (relative_block == last) {\n\t\t\t\t\t \n\t\t\t\t\tmap.m_flags &= ~EXT4_MAP_MAPPED;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tif (page_block == blocks_per_page)\n\t\t\t\t\tbreak;\n\t\t\t\tblocks[page_block] = map.m_pblk + map_offset +\n\t\t\t\t\trelative_block;\n\t\t\t\tpage_block++;\n\t\t\t\tblock_in_file++;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\twhile (page_block < blocks_per_page) {\n\t\t\tif (block_in_file < last_block) {\n\t\t\t\tmap.m_lblk = block_in_file;\n\t\t\t\tmap.m_len = last_block - block_in_file;\n\n\t\t\t\tif (ext4_map_blocks(NULL, inode, &map, 0) < 0) {\n\t\t\t\tset_error_page:\n\t\t\t\t\tfolio_set_error(folio);\n\t\t\t\t\tfolio_zero_segment(folio, 0,\n\t\t\t\t\t\t\t  folio_size(folio));\n\t\t\t\t\tfolio_unlock(folio);\n\t\t\t\t\tgoto next_page;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif ((map.m_flags & EXT4_MAP_MAPPED) == 0) {\n\t\t\t\tfully_mapped = 0;\n\t\t\t\tif (first_hole == blocks_per_page)\n\t\t\t\t\tfirst_hole = page_block;\n\t\t\t\tpage_block++;\n\t\t\t\tblock_in_file++;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (first_hole != blocks_per_page)\n\t\t\t\tgoto confused;\t\t \n\n\t\t\t \n\t\t\tif (page_block && blocks[page_block-1] != map.m_pblk-1)\n\t\t\t\tgoto confused;\n\t\t\tfor (relative_block = 0; ; relative_block++) {\n\t\t\t\tif (relative_block == map.m_len) {\n\t\t\t\t\t \n\t\t\t\t\tmap.m_flags &= ~EXT4_MAP_MAPPED;\n\t\t\t\t\tbreak;\n\t\t\t\t} else if (page_block == blocks_per_page)\n\t\t\t\t\tbreak;\n\t\t\t\tblocks[page_block] = map.m_pblk+relative_block;\n\t\t\t\tpage_block++;\n\t\t\t\tblock_in_file++;\n\t\t\t}\n\t\t}\n\t\tif (first_hole != blocks_per_page) {\n\t\t\tfolio_zero_segment(folio, first_hole << blkbits,\n\t\t\t\t\t  folio_size(folio));\n\t\t\tif (first_hole == 0) {\n\t\t\t\tif (ext4_need_verity(inode, folio->index) &&\n\t\t\t\t    !fsverity_verify_folio(folio))\n\t\t\t\t\tgoto set_error_page;\n\t\t\t\tfolio_mark_uptodate(folio);\n\t\t\t\tfolio_unlock(folio);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t} else if (fully_mapped) {\n\t\t\tfolio_set_mappedtodisk(folio);\n\t\t}\n\n\t\t \n\t\tif (bio && (last_block_in_bio != blocks[0] - 1 ||\n\t\t\t    !fscrypt_mergeable_bio(bio, inode, next_block))) {\n\t\tsubmit_and_realloc:\n\t\t\tsubmit_bio(bio);\n\t\t\tbio = NULL;\n\t\t}\n\t\tif (bio == NULL) {\n\t\t\t \n\t\t\tbio = bio_alloc(bdev, bio_max_segs(nr_pages),\n\t\t\t\t\tREQ_OP_READ, GFP_KERNEL);\n\t\t\tfscrypt_set_bio_crypt_ctx(bio, inode, next_block,\n\t\t\t\t\t\t  GFP_KERNEL);\n\t\t\text4_set_bio_post_read_ctx(bio, inode, folio->index);\n\t\t\tbio->bi_iter.bi_sector = blocks[0] << (blkbits - 9);\n\t\t\tbio->bi_end_io = mpage_end_io;\n\t\t\tif (rac)\n\t\t\t\tbio->bi_opf |= REQ_RAHEAD;\n\t\t}\n\n\t\tlength = first_hole << blkbits;\n\t\tif (!bio_add_folio(bio, folio, length, 0))\n\t\t\tgoto submit_and_realloc;\n\n\t\tif (((map.m_flags & EXT4_MAP_BOUNDARY) &&\n\t\t     (relative_block == map.m_len)) ||\n\t\t    (first_hole != blocks_per_page)) {\n\t\t\tsubmit_bio(bio);\n\t\t\tbio = NULL;\n\t\t} else\n\t\t\tlast_block_in_bio = blocks[blocks_per_page - 1];\n\t\tcontinue;\n\tconfused:\n\t\tif (bio) {\n\t\t\tsubmit_bio(bio);\n\t\t\tbio = NULL;\n\t\t}\n\t\tif (!folio_test_uptodate(folio))\n\t\t\tblock_read_full_folio(folio, ext4_get_block);\n\t\telse\n\t\t\tfolio_unlock(folio);\nnext_page:\n\t\t;  \n\t}\n\tif (bio)\n\t\tsubmit_bio(bio);\n\treturn 0;\n}\n\nint __init ext4_init_post_read_processing(void)\n{\n\tbio_post_read_ctx_cache = KMEM_CACHE(bio_post_read_ctx, SLAB_RECLAIM_ACCOUNT);\n\n\tif (!bio_post_read_ctx_cache)\n\t\tgoto fail;\n\tbio_post_read_ctx_pool =\n\t\tmempool_create_slab_pool(NUM_PREALLOC_POST_READ_CTXS,\n\t\t\t\t\t bio_post_read_ctx_cache);\n\tif (!bio_post_read_ctx_pool)\n\t\tgoto fail_free_cache;\n\treturn 0;\n\nfail_free_cache:\n\tkmem_cache_destroy(bio_post_read_ctx_cache);\nfail:\n\treturn -ENOMEM;\n}\n\nvoid ext4_exit_post_read_processing(void)\n{\n\tmempool_destroy(bio_post_read_ctx_pool);\n\tkmem_cache_destroy(bio_post_read_ctx_cache);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}