{
  "module_name": "mds_client.c",
  "hash_id": "2b43b70674bcf6d2461483bcdfb44490b80d9ea39f88e6c24743b504c0d5ec14",
  "original_prompt": "Ingested from linux-6.6.14/fs/ceph/mds_client.c",
  "human_readable_source": "\n#include <linux/ceph/ceph_debug.h>\n\n#include <linux/fs.h>\n#include <linux/wait.h>\n#include <linux/slab.h>\n#include <linux/gfp.h>\n#include <linux/sched.h>\n#include <linux/debugfs.h>\n#include <linux/seq_file.h>\n#include <linux/ratelimit.h>\n#include <linux/bits.h>\n#include <linux/ktime.h>\n#include <linux/bitmap.h>\n\n#include \"super.h\"\n#include \"mds_client.h\"\n#include \"crypto.h\"\n\n#include <linux/ceph/ceph_features.h>\n#include <linux/ceph/messenger.h>\n#include <linux/ceph/decode.h>\n#include <linux/ceph/pagelist.h>\n#include <linux/ceph/auth.h>\n#include <linux/ceph/debugfs.h>\n\n#define RECONNECT_MAX_SIZE (INT_MAX - PAGE_SIZE)\n\n \n\nstruct ceph_reconnect_state {\n\tstruct ceph_mds_session *session;\n\tint nr_caps, nr_realms;\n\tstruct ceph_pagelist *pagelist;\n\tunsigned msg_version;\n\tbool allow_multi;\n};\n\nstatic void __wake_requests(struct ceph_mds_client *mdsc,\n\t\t\t    struct list_head *head);\nstatic void ceph_cap_release_work(struct work_struct *work);\nstatic void ceph_cap_reclaim_work(struct work_struct *work);\n\nstatic const struct ceph_connection_operations mds_con_ops;\n\n\n \n\nstatic int parse_reply_info_quota(void **p, void *end,\n\t\t\t\t  struct ceph_mds_reply_info_in *info)\n{\n\tu8 struct_v, struct_compat;\n\tu32 struct_len;\n\n\tceph_decode_8_safe(p, end, struct_v, bad);\n\tceph_decode_8_safe(p, end, struct_compat, bad);\n\t \n\tif (!struct_v || struct_compat != 1)\n\t\tgoto bad;\n\tceph_decode_32_safe(p, end, struct_len, bad);\n\tceph_decode_need(p, end, struct_len, bad);\n\tend = *p + struct_len;\n\tceph_decode_64_safe(p, end, info->max_bytes, bad);\n\tceph_decode_64_safe(p, end, info->max_files, bad);\n\t*p = end;\n\treturn 0;\nbad:\n\treturn -EIO;\n}\n\n \nstatic int parse_reply_info_in(void **p, void *end,\n\t\t\t       struct ceph_mds_reply_info_in *info,\n\t\t\t       u64 features)\n{\n\tint err = 0;\n\tu8 struct_v = 0;\n\n\tif (features == (u64)-1) {\n\t\tu32 struct_len;\n\t\tu8 struct_compat;\n\t\tceph_decode_8_safe(p, end, struct_v, bad);\n\t\tceph_decode_8_safe(p, end, struct_compat, bad);\n\t\t \n\t\tif (!struct_v || struct_compat != 1)\n\t\t\tgoto bad;\n\t\tceph_decode_32_safe(p, end, struct_len, bad);\n\t\tceph_decode_need(p, end, struct_len, bad);\n\t\tend = *p + struct_len;\n\t}\n\n\tceph_decode_need(p, end, sizeof(struct ceph_mds_reply_inode), bad);\n\tinfo->in = *p;\n\t*p += sizeof(struct ceph_mds_reply_inode) +\n\t\tsizeof(*info->in->fragtree.splits) *\n\t\tle32_to_cpu(info->in->fragtree.nsplits);\n\n\tceph_decode_32_safe(p, end, info->symlink_len, bad);\n\tceph_decode_need(p, end, info->symlink_len, bad);\n\tinfo->symlink = *p;\n\t*p += info->symlink_len;\n\n\tceph_decode_copy_safe(p, end, &info->dir_layout,\n\t\t\t      sizeof(info->dir_layout), bad);\n\tceph_decode_32_safe(p, end, info->xattr_len, bad);\n\tceph_decode_need(p, end, info->xattr_len, bad);\n\tinfo->xattr_data = *p;\n\t*p += info->xattr_len;\n\n\tif (features == (u64)-1) {\n\t\t \n\t\tceph_decode_64_safe(p, end, info->inline_version, bad);\n\t\tceph_decode_32_safe(p, end, info->inline_len, bad);\n\t\tceph_decode_need(p, end, info->inline_len, bad);\n\t\tinfo->inline_data = *p;\n\t\t*p += info->inline_len;\n\t\t \n\t\terr = parse_reply_info_quota(p, end, info);\n\t\tif (err < 0)\n\t\t\tgoto out_bad;\n\t\t \n\t\tceph_decode_32_safe(p, end, info->pool_ns_len, bad);\n\t\tif (info->pool_ns_len > 0) {\n\t\t\tceph_decode_need(p, end, info->pool_ns_len, bad);\n\t\t\tinfo->pool_ns_data = *p;\n\t\t\t*p += info->pool_ns_len;\n\t\t}\n\n\t\t \n\t\tceph_decode_need(p, end, sizeof(info->btime), bad);\n\t\tceph_decode_copy(p, &info->btime, sizeof(info->btime));\n\n\t\t \n\t\tceph_decode_64_safe(p, end, info->change_attr, bad);\n\n\t\t \n\t\tif (struct_v >= 2) {\n\t\t\tceph_decode_32_safe(p, end, info->dir_pin, bad);\n\t\t} else {\n\t\t\tinfo->dir_pin = -ENODATA;\n\t\t}\n\n\t\t \n\t\tif (struct_v >= 3) {\n\t\t\tceph_decode_need(p, end, sizeof(info->snap_btime), bad);\n\t\t\tceph_decode_copy(p, &info->snap_btime,\n\t\t\t\t\t sizeof(info->snap_btime));\n\t\t} else {\n\t\t\tmemset(&info->snap_btime, 0, sizeof(info->snap_btime));\n\t\t}\n\n\t\t \n\t\tif (struct_v >= 4) {\n\t\t\tceph_decode_64_safe(p, end, info->rsnaps, bad);\n\t\t} else {\n\t\t\tinfo->rsnaps = 0;\n\t\t}\n\n\t\tif (struct_v >= 5) {\n\t\t\tu32 alen;\n\n\t\t\tceph_decode_32_safe(p, end, alen, bad);\n\n\t\t\twhile (alen--) {\n\t\t\t\tu32 len;\n\n\t\t\t\t \n\t\t\t\tceph_decode_32_safe(p, end, len, bad);\n\t\t\t\tceph_decode_skip_n(p, end, len, bad);\n\t\t\t\t \n\t\t\t\tceph_decode_32_safe(p, end, len, bad);\n\t\t\t\tceph_decode_skip_n(p, end, len, bad);\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tif (struct_v >= 6)\n\t\t\tceph_decode_skip_8(p, end, bad);\n\n\t\tinfo->fscrypt_auth = NULL;\n\t\tinfo->fscrypt_auth_len = 0;\n\t\tinfo->fscrypt_file = NULL;\n\t\tinfo->fscrypt_file_len = 0;\n\t\tif (struct_v >= 7) {\n\t\t\tceph_decode_32_safe(p, end, info->fscrypt_auth_len, bad);\n\t\t\tif (info->fscrypt_auth_len) {\n\t\t\t\tinfo->fscrypt_auth = kmalloc(info->fscrypt_auth_len,\n\t\t\t\t\t\t\t     GFP_KERNEL);\n\t\t\t\tif (!info->fscrypt_auth)\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\tceph_decode_copy_safe(p, end, info->fscrypt_auth,\n\t\t\t\t\t\t      info->fscrypt_auth_len, bad);\n\t\t\t}\n\t\t\tceph_decode_32_safe(p, end, info->fscrypt_file_len, bad);\n\t\t\tif (info->fscrypt_file_len) {\n\t\t\t\tinfo->fscrypt_file = kmalloc(info->fscrypt_file_len,\n\t\t\t\t\t\t\t     GFP_KERNEL);\n\t\t\t\tif (!info->fscrypt_file)\n\t\t\t\t\treturn -ENOMEM;\n\t\t\t\tceph_decode_copy_safe(p, end, info->fscrypt_file,\n\t\t\t\t\t\t      info->fscrypt_file_len, bad);\n\t\t\t}\n\t\t}\n\t\t*p = end;\n\t} else {\n\t\t \n\t\tif (features & CEPH_FEATURE_MDS_INLINE_DATA) {\n\t\t\tceph_decode_64_safe(p, end, info->inline_version, bad);\n\t\t\tceph_decode_32_safe(p, end, info->inline_len, bad);\n\t\t\tceph_decode_need(p, end, info->inline_len, bad);\n\t\t\tinfo->inline_data = *p;\n\t\t\t*p += info->inline_len;\n\t\t} else\n\t\t\tinfo->inline_version = CEPH_INLINE_NONE;\n\n\t\tif (features & CEPH_FEATURE_MDS_QUOTA) {\n\t\t\terr = parse_reply_info_quota(p, end, info);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out_bad;\n\t\t} else {\n\t\t\tinfo->max_bytes = 0;\n\t\t\tinfo->max_files = 0;\n\t\t}\n\n\t\tinfo->pool_ns_len = 0;\n\t\tinfo->pool_ns_data = NULL;\n\t\tif (features & CEPH_FEATURE_FS_FILE_LAYOUT_V2) {\n\t\t\tceph_decode_32_safe(p, end, info->pool_ns_len, bad);\n\t\t\tif (info->pool_ns_len > 0) {\n\t\t\t\tceph_decode_need(p, end, info->pool_ns_len, bad);\n\t\t\t\tinfo->pool_ns_data = *p;\n\t\t\t\t*p += info->pool_ns_len;\n\t\t\t}\n\t\t}\n\n\t\tif (features & CEPH_FEATURE_FS_BTIME) {\n\t\t\tceph_decode_need(p, end, sizeof(info->btime), bad);\n\t\t\tceph_decode_copy(p, &info->btime, sizeof(info->btime));\n\t\t\tceph_decode_64_safe(p, end, info->change_attr, bad);\n\t\t}\n\n\t\tinfo->dir_pin = -ENODATA;\n\t\t \n\t}\n\treturn 0;\nbad:\n\terr = -EIO;\nout_bad:\n\treturn err;\n}\n\nstatic int parse_reply_info_dir(void **p, void *end,\n\t\t\t\tstruct ceph_mds_reply_dirfrag **dirfrag,\n\t\t\t\tu64 features)\n{\n\tif (features == (u64)-1) {\n\t\tu8 struct_v, struct_compat;\n\t\tu32 struct_len;\n\t\tceph_decode_8_safe(p, end, struct_v, bad);\n\t\tceph_decode_8_safe(p, end, struct_compat, bad);\n\t\t \n\t\tif (!struct_v || struct_compat != 1)\n\t\t\tgoto bad;\n\t\tceph_decode_32_safe(p, end, struct_len, bad);\n\t\tceph_decode_need(p, end, struct_len, bad);\n\t\tend = *p + struct_len;\n\t}\n\n\tceph_decode_need(p, end, sizeof(**dirfrag), bad);\n\t*dirfrag = *p;\n\t*p += sizeof(**dirfrag) + sizeof(u32) * le32_to_cpu((*dirfrag)->ndist);\n\tif (unlikely(*p > end))\n\t\tgoto bad;\n\tif (features == (u64)-1)\n\t\t*p = end;\n\treturn 0;\nbad:\n\treturn -EIO;\n}\n\nstatic int parse_reply_info_lease(void **p, void *end,\n\t\t\t\t  struct ceph_mds_reply_lease **lease,\n\t\t\t\t  u64 features, u32 *altname_len, u8 **altname)\n{\n\tu8 struct_v;\n\tu32 struct_len;\n\tvoid *lend;\n\n\tif (features == (u64)-1) {\n\t\tu8 struct_compat;\n\n\t\tceph_decode_8_safe(p, end, struct_v, bad);\n\t\tceph_decode_8_safe(p, end, struct_compat, bad);\n\n\t\t \n\t\tif (!struct_v || struct_compat != 1)\n\t\t\tgoto bad;\n\n\t\tceph_decode_32_safe(p, end, struct_len, bad);\n\t} else {\n\t\tstruct_len = sizeof(**lease);\n\t\t*altname_len = 0;\n\t\t*altname = NULL;\n\t}\n\n\tlend = *p + struct_len;\n\tceph_decode_need(p, end, struct_len, bad);\n\t*lease = *p;\n\t*p += sizeof(**lease);\n\n\tif (features == (u64)-1) {\n\t\tif (struct_v >= 2) {\n\t\t\tceph_decode_32_safe(p, end, *altname_len, bad);\n\t\t\tceph_decode_need(p, end, *altname_len, bad);\n\t\t\t*altname = *p;\n\t\t\t*p += *altname_len;\n\t\t} else {\n\t\t\t*altname = NULL;\n\t\t\t*altname_len = 0;\n\t\t}\n\t}\n\t*p = lend;\n\treturn 0;\nbad:\n\treturn -EIO;\n}\n\n \nstatic int parse_reply_info_trace(void **p, void *end,\n\t\t\t\t  struct ceph_mds_reply_info_parsed *info,\n\t\t\t\t  u64 features)\n{\n\tint err;\n\n\tif (info->head->is_dentry) {\n\t\terr = parse_reply_info_in(p, end, &info->diri, features);\n\t\tif (err < 0)\n\t\t\tgoto out_bad;\n\n\t\terr = parse_reply_info_dir(p, end, &info->dirfrag, features);\n\t\tif (err < 0)\n\t\t\tgoto out_bad;\n\n\t\tceph_decode_32_safe(p, end, info->dname_len, bad);\n\t\tceph_decode_need(p, end, info->dname_len, bad);\n\t\tinfo->dname = *p;\n\t\t*p += info->dname_len;\n\n\t\terr = parse_reply_info_lease(p, end, &info->dlease, features,\n\t\t\t\t\t     &info->altname_len, &info->altname);\n\t\tif (err < 0)\n\t\t\tgoto out_bad;\n\t}\n\n\tif (info->head->is_target) {\n\t\terr = parse_reply_info_in(p, end, &info->targeti, features);\n\t\tif (err < 0)\n\t\t\tgoto out_bad;\n\t}\n\n\tif (unlikely(*p != end))\n\t\tgoto bad;\n\treturn 0;\n\nbad:\n\terr = -EIO;\nout_bad:\n\tpr_err(\"problem parsing mds trace %d\\n\", err);\n\treturn err;\n}\n\n \nstatic int parse_reply_info_readdir(void **p, void *end,\n\t\t\t\t    struct ceph_mds_request *req,\n\t\t\t\t    u64 features)\n{\n\tstruct ceph_mds_reply_info_parsed *info = &req->r_reply_info;\n\tu32 num, i = 0;\n\tint err;\n\n\terr = parse_reply_info_dir(p, end, &info->dir_dir, features);\n\tif (err < 0)\n\t\tgoto out_bad;\n\n\tceph_decode_need(p, end, sizeof(num) + 2, bad);\n\tnum = ceph_decode_32(p);\n\t{\n\t\tu16 flags = ceph_decode_16(p);\n\t\tinfo->dir_end = !!(flags & CEPH_READDIR_FRAG_END);\n\t\tinfo->dir_complete = !!(flags & CEPH_READDIR_FRAG_COMPLETE);\n\t\tinfo->hash_order = !!(flags & CEPH_READDIR_HASH_ORDER);\n\t\tinfo->offset_hash = !!(flags & CEPH_READDIR_OFFSET_HASH);\n\t}\n\tif (num == 0)\n\t\tgoto done;\n\n\tBUG_ON(!info->dir_entries);\n\tif ((unsigned long)(info->dir_entries + num) >\n\t    (unsigned long)info->dir_entries + info->dir_buf_size) {\n\t\tpr_err(\"dir contents are larger than expected\\n\");\n\t\tWARN_ON(1);\n\t\tgoto bad;\n\t}\n\n\tinfo->dir_nr = num;\n\twhile (num) {\n\t\tstruct inode *inode = d_inode(req->r_dentry);\n\t\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\t\tstruct ceph_mds_reply_dir_entry *rde = info->dir_entries + i;\n\t\tstruct fscrypt_str tname = FSTR_INIT(NULL, 0);\n\t\tstruct fscrypt_str oname = FSTR_INIT(NULL, 0);\n\t\tstruct ceph_fname fname;\n\t\tu32 altname_len, _name_len;\n\t\tu8 *altname, *_name;\n\n\t\t \n\t\tceph_decode_32_safe(p, end, _name_len, bad);\n\t\tceph_decode_need(p, end, _name_len, bad);\n\t\t_name = *p;\n\t\t*p += _name_len;\n\t\tdout(\"parsed dir dname '%.*s'\\n\", _name_len, _name);\n\n\t\tif (info->hash_order)\n\t\t\trde->raw_hash = ceph_str_hash(ci->i_dir_layout.dl_dir_hash,\n\t\t\t\t\t\t      _name, _name_len);\n\n\t\t \n\t\terr = parse_reply_info_lease(p, end, &rde->lease, features,\n\t\t\t\t\t     &altname_len, &altname);\n\t\tif (err)\n\t\t\tgoto out_bad;\n\n\t\t \n\t\tfname.dir = inode;\n\t\tfname.name = _name;\n\t\tfname.name_len = _name_len;\n\t\tfname.ctext = altname;\n\t\tfname.ctext_len = altname_len;\n\t\t \n\t\tfname.no_copy = true;\n\t\tif (altname_len == 0) {\n\t\t\t \n\t\t\ttname.name = _name;\n\n\t\t\t \n\t\t\toname.name = _name;\n\t\t\toname.len = _name_len;\n\t\t} else {\n\t\t\t \n\t\t\toname.name = altname;\n\t\t\toname.len = altname_len;\n\t\t}\n\t\trde->is_nokey = false;\n\t\terr = ceph_fname_to_usr(&fname, &tname, &oname, &rde->is_nokey);\n\t\tif (err) {\n\t\t\tpr_err(\"%s unable to decode %.*s, got %d\\n\", __func__,\n\t\t\t       _name_len, _name, err);\n\t\t\tgoto out_bad;\n\t\t}\n\t\trde->name = oname.name;\n\t\trde->name_len = oname.len;\n\n\t\t \n\t\terr = parse_reply_info_in(p, end, &rde->inode, features);\n\t\tif (err < 0)\n\t\t\tgoto out_bad;\n\t\t \n\t\trde->offset = 0;\n\t\ti++;\n\t\tnum--;\n\t}\n\ndone:\n\t \n\t*p = end;\n\treturn 0;\n\nbad:\n\terr = -EIO;\nout_bad:\n\tpr_err(\"problem parsing dir contents %d\\n\", err);\n\treturn err;\n}\n\n \nstatic int parse_reply_info_filelock(void **p, void *end,\n\t\t\t\t     struct ceph_mds_reply_info_parsed *info,\n\t\t\t\t     u64 features)\n{\n\tif (*p + sizeof(*info->filelock_reply) > end)\n\t\tgoto bad;\n\n\tinfo->filelock_reply = *p;\n\n\t \n\t*p = end;\n\treturn 0;\nbad:\n\treturn -EIO;\n}\n\n\n#if BITS_PER_LONG == 64\n\n#define DELEGATED_INO_AVAILABLE\t\txa_mk_value(1)\n\nstatic int ceph_parse_deleg_inos(void **p, void *end,\n\t\t\t\t struct ceph_mds_session *s)\n{\n\tu32 sets;\n\n\tceph_decode_32_safe(p, end, sets, bad);\n\tdout(\"got %u sets of delegated inodes\\n\", sets);\n\twhile (sets--) {\n\t\tu64 start, len;\n\n\t\tceph_decode_64_safe(p, end, start, bad);\n\t\tceph_decode_64_safe(p, end, len, bad);\n\n\t\t \n\t\tif (start < CEPH_INO_SYSTEM_BASE) {\n\t\t\tpr_warn_ratelimited(\"ceph: ignoring reserved inode range delegation (start=0x%llx len=0x%llx)\\n\",\n\t\t\t\t\tstart, len);\n\t\t\tcontinue;\n\t\t}\n\t\twhile (len--) {\n\t\t\tint err = xa_insert(&s->s_delegated_inos, start++,\n\t\t\t\t\t    DELEGATED_INO_AVAILABLE,\n\t\t\t\t\t    GFP_KERNEL);\n\t\t\tif (!err) {\n\t\t\t\tdout(\"added delegated inode 0x%llx\\n\",\n\t\t\t\t     start - 1);\n\t\t\t} else if (err == -EBUSY) {\n\t\t\t\tpr_warn(\"MDS delegated inode 0x%llx more than once.\\n\",\n\t\t\t\t\tstart - 1);\n\t\t\t} else {\n\t\t\t\treturn err;\n\t\t\t}\n\t\t}\n\t}\n\treturn 0;\nbad:\n\treturn -EIO;\n}\n\nu64 ceph_get_deleg_ino(struct ceph_mds_session *s)\n{\n\tunsigned long ino;\n\tvoid *val;\n\n\txa_for_each(&s->s_delegated_inos, ino, val) {\n\t\tval = xa_erase(&s->s_delegated_inos, ino);\n\t\tif (val == DELEGATED_INO_AVAILABLE)\n\t\t\treturn ino;\n\t}\n\treturn 0;\n}\n\nint ceph_restore_deleg_ino(struct ceph_mds_session *s, u64 ino)\n{\n\treturn xa_insert(&s->s_delegated_inos, ino, DELEGATED_INO_AVAILABLE,\n\t\t\t GFP_KERNEL);\n}\n#else  \n \nstatic int ceph_parse_deleg_inos(void **p, void *end,\n\t\t\t\t struct ceph_mds_session *s)\n{\n\tu32 sets;\n\n\tceph_decode_32_safe(p, end, sets, bad);\n\tif (sets)\n\t\tceph_decode_skip_n(p, end, sets * 2 * sizeof(__le64), bad);\n\treturn 0;\nbad:\n\treturn -EIO;\n}\n\nu64 ceph_get_deleg_ino(struct ceph_mds_session *s)\n{\n\treturn 0;\n}\n\nint ceph_restore_deleg_ino(struct ceph_mds_session *s, u64 ino)\n{\n\treturn 0;\n}\n#endif  \n\n \nstatic int parse_reply_info_create(void **p, void *end,\n\t\t\t\t  struct ceph_mds_reply_info_parsed *info,\n\t\t\t\t  u64 features, struct ceph_mds_session *s)\n{\n\tint ret;\n\n\tif (features == (u64)-1 ||\n\t    (features & CEPH_FEATURE_REPLY_CREATE_INODE)) {\n\t\tif (*p == end) {\n\t\t\t \n\t\t\tinfo->has_create_ino = false;\n\t\t} else if (test_bit(CEPHFS_FEATURE_DELEG_INO, &s->s_features)) {\n\t\t\tinfo->has_create_ino = true;\n\t\t\t \n\t\t\tceph_decode_skip_n(p, end, 2 + sizeof(u32), bad);\n\t\t\tceph_decode_64_safe(p, end, info->ino, bad);\n\t\t\tret = ceph_parse_deleg_inos(p, end, s);\n\t\t\tif (ret)\n\t\t\t\treturn ret;\n\t\t} else {\n\t\t\t \n\t\t\tceph_decode_64_safe(p, end, info->ino, bad);\n\t\t\tinfo->has_create_ino = true;\n\t\t}\n\t} else {\n\t\tif (*p != end)\n\t\t\tgoto bad;\n\t}\n\n\t \n\t*p = end;\n\treturn 0;\nbad:\n\treturn -EIO;\n}\n\nstatic int parse_reply_info_getvxattr(void **p, void *end,\n\t\t\t\t      struct ceph_mds_reply_info_parsed *info,\n\t\t\t\t      u64 features)\n{\n\tu32 value_len;\n\n\tceph_decode_skip_8(p, end, bad);  \n\tceph_decode_skip_8(p, end, bad);  \n\tceph_decode_skip_32(p, end, bad);  \n\n\tceph_decode_32_safe(p, end, value_len, bad);\n\n\tif (value_len == end - *p) {\n\t  info->xattr_info.xattr_value = *p;\n\t  info->xattr_info.xattr_value_len = value_len;\n\t  *p = end;\n\t  return value_len;\n\t}\nbad:\n\treturn -EIO;\n}\n\n \nstatic int parse_reply_info_extra(void **p, void *end,\n\t\t\t\t  struct ceph_mds_request *req,\n\t\t\t\t  u64 features, struct ceph_mds_session *s)\n{\n\tstruct ceph_mds_reply_info_parsed *info = &req->r_reply_info;\n\tu32 op = le32_to_cpu(info->head->op);\n\n\tif (op == CEPH_MDS_OP_GETFILELOCK)\n\t\treturn parse_reply_info_filelock(p, end, info, features);\n\telse if (op == CEPH_MDS_OP_READDIR || op == CEPH_MDS_OP_LSSNAP)\n\t\treturn parse_reply_info_readdir(p, end, req, features);\n\telse if (op == CEPH_MDS_OP_CREATE)\n\t\treturn parse_reply_info_create(p, end, info, features, s);\n\telse if (op == CEPH_MDS_OP_GETVXATTR)\n\t\treturn parse_reply_info_getvxattr(p, end, info, features);\n\telse\n\t\treturn -EIO;\n}\n\n \nstatic int parse_reply_info(struct ceph_mds_session *s, struct ceph_msg *msg,\n\t\t\t    struct ceph_mds_request *req, u64 features)\n{\n\tstruct ceph_mds_reply_info_parsed *info = &req->r_reply_info;\n\tvoid *p, *end;\n\tu32 len;\n\tint err;\n\n\tinfo->head = msg->front.iov_base;\n\tp = msg->front.iov_base + sizeof(struct ceph_mds_reply_head);\n\tend = p + msg->front.iov_len - sizeof(struct ceph_mds_reply_head);\n\n\t \n\tceph_decode_32_safe(&p, end, len, bad);\n\tif (len > 0) {\n\t\tceph_decode_need(&p, end, len, bad);\n\t\terr = parse_reply_info_trace(&p, p+len, info, features);\n\t\tif (err < 0)\n\t\t\tgoto out_bad;\n\t}\n\n\t \n\tceph_decode_32_safe(&p, end, len, bad);\n\tif (len > 0) {\n\t\tceph_decode_need(&p, end, len, bad);\n\t\terr = parse_reply_info_extra(&p, p+len, req, features, s);\n\t\tif (err < 0)\n\t\t\tgoto out_bad;\n\t}\n\n\t \n\tceph_decode_32_safe(&p, end, len, bad);\n\tinfo->snapblob_len = len;\n\tinfo->snapblob = p;\n\tp += len;\n\n\tif (p != end)\n\t\tgoto bad;\n\treturn 0;\n\nbad:\n\terr = -EIO;\nout_bad:\n\tpr_err(\"mds parse_reply err %d\\n\", err);\n\tceph_msg_dump(msg);\n\treturn err;\n}\n\nstatic void destroy_reply_info(struct ceph_mds_reply_info_parsed *info)\n{\n\tint i;\n\n\tkfree(info->diri.fscrypt_auth);\n\tkfree(info->diri.fscrypt_file);\n\tkfree(info->targeti.fscrypt_auth);\n\tkfree(info->targeti.fscrypt_file);\n\tif (!info->dir_entries)\n\t\treturn;\n\n\tfor (i = 0; i < info->dir_nr; i++) {\n\t\tstruct ceph_mds_reply_dir_entry *rde = info->dir_entries + i;\n\n\t\tkfree(rde->inode.fscrypt_auth);\n\t\tkfree(rde->inode.fscrypt_file);\n\t}\n\tfree_pages((unsigned long)info->dir_entries, get_order(info->dir_buf_size));\n}\n\n \nint ceph_wait_on_conflict_unlink(struct dentry *dentry)\n{\n\tstruct ceph_fs_client *fsc = ceph_sb_to_client(dentry->d_sb);\n\tstruct dentry *pdentry = dentry->d_parent;\n\tstruct dentry *udentry, *found = NULL;\n\tstruct ceph_dentry_info *di;\n\tstruct qstr dname;\n\tu32 hash = dentry->d_name.hash;\n\tint err;\n\n\tdname.name = dentry->d_name.name;\n\tdname.len = dentry->d_name.len;\n\n\trcu_read_lock();\n\thash_for_each_possible_rcu(fsc->async_unlink_conflict, di,\n\t\t\t\t   hnode, hash) {\n\t\tudentry = di->dentry;\n\n\t\tspin_lock(&udentry->d_lock);\n\t\tif (udentry->d_name.hash != hash)\n\t\t\tgoto next;\n\t\tif (unlikely(udentry->d_parent != pdentry))\n\t\t\tgoto next;\n\t\tif (!hash_hashed(&di->hnode))\n\t\t\tgoto next;\n\n\t\tif (!test_bit(CEPH_DENTRY_ASYNC_UNLINK_BIT, &di->flags))\n\t\t\tpr_warn(\"%s dentry %p:%pd async unlink bit is not set\\n\",\n\t\t\t\t__func__, dentry, dentry);\n\n\t\tif (!d_same_name(udentry, pdentry, &dname))\n\t\t\tgoto next;\n\n\t\tfound = dget_dlock(udentry);\n\t\tspin_unlock(&udentry->d_lock);\n\t\tbreak;\nnext:\n\t\tspin_unlock(&udentry->d_lock);\n\t}\n\trcu_read_unlock();\n\n\tif (likely(!found))\n\t\treturn 0;\n\n\tdout(\"%s dentry %p:%pd conflict with old %p:%pd\\n\", __func__,\n\t     dentry, dentry, found, found);\n\n\terr = wait_on_bit(&di->flags, CEPH_DENTRY_ASYNC_UNLINK_BIT,\n\t\t\t  TASK_KILLABLE);\n\tdput(found);\n\treturn err;\n}\n\n\n \nconst char *ceph_session_state_name(int s)\n{\n\tswitch (s) {\n\tcase CEPH_MDS_SESSION_NEW: return \"new\";\n\tcase CEPH_MDS_SESSION_OPENING: return \"opening\";\n\tcase CEPH_MDS_SESSION_OPEN: return \"open\";\n\tcase CEPH_MDS_SESSION_HUNG: return \"hung\";\n\tcase CEPH_MDS_SESSION_CLOSING: return \"closing\";\n\tcase CEPH_MDS_SESSION_CLOSED: return \"closed\";\n\tcase CEPH_MDS_SESSION_RESTARTING: return \"restarting\";\n\tcase CEPH_MDS_SESSION_RECONNECTING: return \"reconnecting\";\n\tcase CEPH_MDS_SESSION_REJECTED: return \"rejected\";\n\tdefault: return \"???\";\n\t}\n}\n\nstruct ceph_mds_session *ceph_get_mds_session(struct ceph_mds_session *s)\n{\n\tif (refcount_inc_not_zero(&s->s_ref))\n\t\treturn s;\n\treturn NULL;\n}\n\nvoid ceph_put_mds_session(struct ceph_mds_session *s)\n{\n\tif (IS_ERR_OR_NULL(s))\n\t\treturn;\n\n\tif (refcount_dec_and_test(&s->s_ref)) {\n\t\tif (s->s_auth.authorizer)\n\t\t\tceph_auth_destroy_authorizer(s->s_auth.authorizer);\n\t\tWARN_ON(mutex_is_locked(&s->s_mutex));\n\t\txa_destroy(&s->s_delegated_inos);\n\t\tkfree(s);\n\t}\n}\n\n \nstruct ceph_mds_session *__ceph_lookup_mds_session(struct ceph_mds_client *mdsc,\n\t\t\t\t\t\t   int mds)\n{\n\tif (mds >= mdsc->max_sessions || !mdsc->sessions[mds])\n\t\treturn NULL;\n\treturn ceph_get_mds_session(mdsc->sessions[mds]);\n}\n\nstatic bool __have_session(struct ceph_mds_client *mdsc, int mds)\n{\n\tif (mds >= mdsc->max_sessions || !mdsc->sessions[mds])\n\t\treturn false;\n\telse\n\t\treturn true;\n}\n\nstatic int __verify_registered_session(struct ceph_mds_client *mdsc,\n\t\t\t\t       struct ceph_mds_session *s)\n{\n\tif (s->s_mds >= mdsc->max_sessions ||\n\t    mdsc->sessions[s->s_mds] != s)\n\t\treturn -ENOENT;\n\treturn 0;\n}\n\n \nstatic struct ceph_mds_session *register_session(struct ceph_mds_client *mdsc,\n\t\t\t\t\t\t int mds)\n{\n\tstruct ceph_mds_session *s;\n\n\tif (READ_ONCE(mdsc->fsc->mount_state) == CEPH_MOUNT_FENCE_IO)\n\t\treturn ERR_PTR(-EIO);\n\n\tif (mds >= mdsc->mdsmap->possible_max_rank)\n\t\treturn ERR_PTR(-EINVAL);\n\n\ts = kzalloc(sizeof(*s), GFP_NOFS);\n\tif (!s)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (mds >= mdsc->max_sessions) {\n\t\tint newmax = 1 << get_count_order(mds + 1);\n\t\tstruct ceph_mds_session **sa;\n\n\t\tdout(\"%s: realloc to %d\\n\", __func__, newmax);\n\t\tsa = kcalloc(newmax, sizeof(void *), GFP_NOFS);\n\t\tif (!sa)\n\t\t\tgoto fail_realloc;\n\t\tif (mdsc->sessions) {\n\t\t\tmemcpy(sa, mdsc->sessions,\n\t\t\t       mdsc->max_sessions * sizeof(void *));\n\t\t\tkfree(mdsc->sessions);\n\t\t}\n\t\tmdsc->sessions = sa;\n\t\tmdsc->max_sessions = newmax;\n\t}\n\n\tdout(\"%s: mds%d\\n\", __func__, mds);\n\ts->s_mdsc = mdsc;\n\ts->s_mds = mds;\n\ts->s_state = CEPH_MDS_SESSION_NEW;\n\tmutex_init(&s->s_mutex);\n\n\tceph_con_init(&s->s_con, s, &mds_con_ops, &mdsc->fsc->client->msgr);\n\n\tatomic_set(&s->s_cap_gen, 1);\n\ts->s_cap_ttl = jiffies - 1;\n\n\tspin_lock_init(&s->s_cap_lock);\n\tINIT_LIST_HEAD(&s->s_caps);\n\trefcount_set(&s->s_ref, 1);\n\tINIT_LIST_HEAD(&s->s_waiting);\n\tINIT_LIST_HEAD(&s->s_unsafe);\n\txa_init(&s->s_delegated_inos);\n\tINIT_LIST_HEAD(&s->s_cap_releases);\n\tINIT_WORK(&s->s_cap_release_work, ceph_cap_release_work);\n\n\tINIT_LIST_HEAD(&s->s_cap_dirty);\n\tINIT_LIST_HEAD(&s->s_cap_flushing);\n\n\tmdsc->sessions[mds] = s;\n\tatomic_inc(&mdsc->num_sessions);\n\trefcount_inc(&s->s_ref);   \n\n\tceph_con_open(&s->s_con, CEPH_ENTITY_TYPE_MDS, mds,\n\t\t      ceph_mdsmap_get_addr(mdsc->mdsmap, mds));\n\n\treturn s;\n\nfail_realloc:\n\tkfree(s);\n\treturn ERR_PTR(-ENOMEM);\n}\n\n \nstatic void __unregister_session(struct ceph_mds_client *mdsc,\n\t\t\t       struct ceph_mds_session *s)\n{\n\tdout(\"__unregister_session mds%d %p\\n\", s->s_mds, s);\n\tBUG_ON(mdsc->sessions[s->s_mds] != s);\n\tmdsc->sessions[s->s_mds] = NULL;\n\tceph_con_close(&s->s_con);\n\tceph_put_mds_session(s);\n\tatomic_dec(&mdsc->num_sessions);\n}\n\n \nstatic void put_request_session(struct ceph_mds_request *req)\n{\n\tif (req->r_session) {\n\t\tceph_put_mds_session(req->r_session);\n\t\treq->r_session = NULL;\n\t}\n}\n\nvoid ceph_mdsc_iterate_sessions(struct ceph_mds_client *mdsc,\n\t\t\t\tvoid (*cb)(struct ceph_mds_session *),\n\t\t\t\tbool check_state)\n{\n\tint mds;\n\n\tmutex_lock(&mdsc->mutex);\n\tfor (mds = 0; mds < mdsc->max_sessions; ++mds) {\n\t\tstruct ceph_mds_session *s;\n\n\t\ts = __ceph_lookup_mds_session(mdsc, mds);\n\t\tif (!s)\n\t\t\tcontinue;\n\n\t\tif (check_state && !check_session_state(s)) {\n\t\t\tceph_put_mds_session(s);\n\t\t\tcontinue;\n\t\t}\n\n\t\tmutex_unlock(&mdsc->mutex);\n\t\tcb(s);\n\t\tceph_put_mds_session(s);\n\t\tmutex_lock(&mdsc->mutex);\n\t}\n\tmutex_unlock(&mdsc->mutex);\n}\n\nvoid ceph_mdsc_release_request(struct kref *kref)\n{\n\tstruct ceph_mds_request *req = container_of(kref,\n\t\t\t\t\t\t    struct ceph_mds_request,\n\t\t\t\t\t\t    r_kref);\n\tceph_mdsc_release_dir_caps_no_check(req);\n\tdestroy_reply_info(&req->r_reply_info);\n\tif (req->r_request)\n\t\tceph_msg_put(req->r_request);\n\tif (req->r_reply)\n\t\tceph_msg_put(req->r_reply);\n\tif (req->r_inode) {\n\t\tceph_put_cap_refs(ceph_inode(req->r_inode), CEPH_CAP_PIN);\n\t\tiput(req->r_inode);\n\t}\n\tif (req->r_parent) {\n\t\tceph_put_cap_refs(ceph_inode(req->r_parent), CEPH_CAP_PIN);\n\t\tiput(req->r_parent);\n\t}\n\tiput(req->r_target_inode);\n\tiput(req->r_new_inode);\n\tif (req->r_dentry)\n\t\tdput(req->r_dentry);\n\tif (req->r_old_dentry)\n\t\tdput(req->r_old_dentry);\n\tif (req->r_old_dentry_dir) {\n\t\t \n\t\tceph_put_cap_refs(ceph_inode(req->r_old_dentry_dir),\n\t\t\t\t  CEPH_CAP_PIN);\n\t\tiput(req->r_old_dentry_dir);\n\t}\n\tkfree(req->r_path1);\n\tkfree(req->r_path2);\n\tput_cred(req->r_cred);\n\tif (req->r_pagelist)\n\t\tceph_pagelist_release(req->r_pagelist);\n\tkfree(req->r_fscrypt_auth);\n\tkfree(req->r_altname);\n\tput_request_session(req);\n\tceph_unreserve_caps(req->r_mdsc, &req->r_caps_reservation);\n\tWARN_ON_ONCE(!list_empty(&req->r_wait));\n\tkmem_cache_free(ceph_mds_request_cachep, req);\n}\n\nDEFINE_RB_FUNCS(request, struct ceph_mds_request, r_tid, r_node)\n\n \nstatic struct ceph_mds_request *\nlookup_get_request(struct ceph_mds_client *mdsc, u64 tid)\n{\n\tstruct ceph_mds_request *req;\n\n\treq = lookup_request(&mdsc->request_tree, tid);\n\tif (req)\n\t\tceph_mdsc_get_request(req);\n\n\treturn req;\n}\n\n \nstatic void __register_request(struct ceph_mds_client *mdsc,\n\t\t\t       struct ceph_mds_request *req,\n\t\t\t       struct inode *dir)\n{\n\tint ret = 0;\n\n\treq->r_tid = ++mdsc->last_tid;\n\tif (req->r_num_caps) {\n\t\tret = ceph_reserve_caps(mdsc, &req->r_caps_reservation,\n\t\t\t\t\treq->r_num_caps);\n\t\tif (ret < 0) {\n\t\t\tpr_err(\"__register_request %p \"\n\t\t\t       \"failed to reserve caps: %d\\n\", req, ret);\n\t\t\t \n\t\t\treq->r_err = ret;\n\t\t\treturn;\n\t\t}\n\t}\n\tdout(\"__register_request %p tid %lld\\n\", req, req->r_tid);\n\tceph_mdsc_get_request(req);\n\tinsert_request(&mdsc->request_tree, req);\n\n\treq->r_cred = get_current_cred();\n\n\tif (mdsc->oldest_tid == 0 && req->r_op != CEPH_MDS_OP_SETFILELOCK)\n\t\tmdsc->oldest_tid = req->r_tid;\n\n\tif (dir) {\n\t\tstruct ceph_inode_info *ci = ceph_inode(dir);\n\n\t\tihold(dir);\n\t\treq->r_unsafe_dir = dir;\n\t\tspin_lock(&ci->i_unsafe_lock);\n\t\tlist_add_tail(&req->r_unsafe_dir_item, &ci->i_unsafe_dirops);\n\t\tspin_unlock(&ci->i_unsafe_lock);\n\t}\n}\n\nstatic void __unregister_request(struct ceph_mds_client *mdsc,\n\t\t\t\t struct ceph_mds_request *req)\n{\n\tdout(\"__unregister_request %p tid %lld\\n\", req, req->r_tid);\n\n\t \n\tlist_del_init(&req->r_unsafe_item);\n\n\tif (req->r_tid == mdsc->oldest_tid) {\n\t\tstruct rb_node *p = rb_next(&req->r_node);\n\t\tmdsc->oldest_tid = 0;\n\t\twhile (p) {\n\t\t\tstruct ceph_mds_request *next_req =\n\t\t\t\trb_entry(p, struct ceph_mds_request, r_node);\n\t\t\tif (next_req->r_op != CEPH_MDS_OP_SETFILELOCK) {\n\t\t\t\tmdsc->oldest_tid = next_req->r_tid;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tp = rb_next(p);\n\t\t}\n\t}\n\n\terase_request(&mdsc->request_tree, req);\n\n\tif (req->r_unsafe_dir) {\n\t\tstruct ceph_inode_info *ci = ceph_inode(req->r_unsafe_dir);\n\t\tspin_lock(&ci->i_unsafe_lock);\n\t\tlist_del_init(&req->r_unsafe_dir_item);\n\t\tspin_unlock(&ci->i_unsafe_lock);\n\t}\n\tif (req->r_target_inode &&\n\t    test_bit(CEPH_MDS_R_GOT_UNSAFE, &req->r_req_flags)) {\n\t\tstruct ceph_inode_info *ci = ceph_inode(req->r_target_inode);\n\t\tspin_lock(&ci->i_unsafe_lock);\n\t\tlist_del_init(&req->r_unsafe_target_item);\n\t\tspin_unlock(&ci->i_unsafe_lock);\n\t}\n\n\tif (req->r_unsafe_dir) {\n\t\tiput(req->r_unsafe_dir);\n\t\treq->r_unsafe_dir = NULL;\n\t}\n\n\tcomplete_all(&req->r_safe_completion);\n\n\tceph_mdsc_put_request(req);\n}\n\n \nstatic struct inode *get_nonsnap_parent(struct dentry *dentry)\n{\n\tstruct inode *inode = NULL;\n\n\twhile (dentry && !IS_ROOT(dentry)) {\n\t\tinode = d_inode_rcu(dentry);\n\t\tif (!inode || ceph_snap(inode) == CEPH_NOSNAP)\n\t\t\tbreak;\n\t\tdentry = dentry->d_parent;\n\t}\n\tif (inode)\n\t\tinode = igrab(inode);\n\treturn inode;\n}\n\n \nstatic int __choose_mds(struct ceph_mds_client *mdsc,\n\t\t\tstruct ceph_mds_request *req,\n\t\t\tbool *random)\n{\n\tstruct inode *inode;\n\tstruct ceph_inode_info *ci;\n\tstruct ceph_cap *cap;\n\tint mode = req->r_direct_mode;\n\tint mds = -1;\n\tu32 hash = req->r_direct_hash;\n\tbool is_hash = test_bit(CEPH_MDS_R_DIRECT_IS_HASH, &req->r_req_flags);\n\n\tif (random)\n\t\t*random = false;\n\n\t \n\tif (req->r_resend_mds >= 0 &&\n\t    (__have_session(mdsc, req->r_resend_mds) ||\n\t     ceph_mdsmap_get_state(mdsc->mdsmap, req->r_resend_mds) > 0)) {\n\t\tdout(\"%s using resend_mds mds%d\\n\", __func__,\n\t\t     req->r_resend_mds);\n\t\treturn req->r_resend_mds;\n\t}\n\n\tif (mode == USE_RANDOM_MDS)\n\t\tgoto random;\n\n\tinode = NULL;\n\tif (req->r_inode) {\n\t\tif (ceph_snap(req->r_inode) != CEPH_SNAPDIR) {\n\t\t\tinode = req->r_inode;\n\t\t\tihold(inode);\n\t\t} else {\n\t\t\t \n\t\t\trcu_read_lock();\n\t\t\tinode = get_nonsnap_parent(req->r_dentry);\n\t\t\trcu_read_unlock();\n\t\t\tdout(\"%s using snapdir's parent %p\\n\", __func__, inode);\n\t\t}\n\t} else if (req->r_dentry) {\n\t\t \n\t\tstruct dentry *parent;\n\t\tstruct inode *dir;\n\n\t\trcu_read_lock();\n\t\tparent = READ_ONCE(req->r_dentry->d_parent);\n\t\tdir = req->r_parent ? : d_inode_rcu(parent);\n\n\t\tif (!dir || dir->i_sb != mdsc->fsc->sb) {\n\t\t\t \n\t\t\tinode = d_inode(req->r_dentry);\n\t\t\tif (inode)\n\t\t\t\tihold(inode);\n\t\t} else if (ceph_snap(dir) != CEPH_NOSNAP) {\n\t\t\t \n\t\t\tinode = get_nonsnap_parent(parent);\n\t\t\tdout(\"%s using nonsnap parent %p\\n\", __func__, inode);\n\t\t} else {\n\t\t\t \n\t\t\tinode = d_inode(req->r_dentry);\n\t\t\tif (!inode || mode == USE_AUTH_MDS) {\n\t\t\t\t \n\t\t\t\tinode = igrab(dir);\n\t\t\t\thash = ceph_dentry_hash(dir, req->r_dentry);\n\t\t\t\tis_hash = true;\n\t\t\t} else {\n\t\t\t\tihold(inode);\n\t\t\t}\n\t\t}\n\t\trcu_read_unlock();\n\t}\n\n\tdout(\"%s %p is_hash=%d (0x%x) mode %d\\n\", __func__, inode, (int)is_hash,\n\t     hash, mode);\n\tif (!inode)\n\t\tgoto random;\n\tci = ceph_inode(inode);\n\n\tif (is_hash && S_ISDIR(inode->i_mode)) {\n\t\tstruct ceph_inode_frag frag;\n\t\tint found;\n\n\t\tceph_choose_frag(ci, hash, &frag, &found);\n\t\tif (found) {\n\t\t\tif (mode == USE_ANY_MDS && frag.ndist > 0) {\n\t\t\t\tu8 r;\n\n\t\t\t\t \n\t\t\t\tget_random_bytes(&r, 1);\n\t\t\t\tr %= frag.ndist;\n\t\t\t\tmds = frag.dist[r];\n\t\t\t\tdout(\"%s %p %llx.%llx frag %u mds%d (%d/%d)\\n\",\n\t\t\t\t     __func__, inode, ceph_vinop(inode),\n\t\t\t\t     frag.frag, mds, (int)r, frag.ndist);\n\t\t\t\tif (ceph_mdsmap_get_state(mdsc->mdsmap, mds) >=\n\t\t\t\t    CEPH_MDS_STATE_ACTIVE &&\n\t\t\t\t    !ceph_mdsmap_is_laggy(mdsc->mdsmap, mds))\n\t\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (frag.mds >= 0) {\n\t\t\t\t \n\t\t\t\tmds = frag.mds;\n\t\t\t\tdout(\"%s %p %llx.%llx frag %u mds%d (auth)\\n\",\n\t\t\t\t     __func__, inode, ceph_vinop(inode),\n\t\t\t\t     frag.frag, mds);\n\t\t\t\tif (ceph_mdsmap_get_state(mdsc->mdsmap, mds) >=\n\t\t\t\t    CEPH_MDS_STATE_ACTIVE) {\n\t\t\t\t\tif (!ceph_mdsmap_is_laggy(mdsc->mdsmap,\n\t\t\t\t\t\t\t\t  mds))\n\t\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tmode = USE_AUTH_MDS;\n\t\t}\n\t}\n\n\tspin_lock(&ci->i_ceph_lock);\n\tcap = NULL;\n\tif (mode == USE_AUTH_MDS)\n\t\tcap = ci->i_auth_cap;\n\tif (!cap && !RB_EMPTY_ROOT(&ci->i_caps))\n\t\tcap = rb_entry(rb_first(&ci->i_caps), struct ceph_cap, ci_node);\n\tif (!cap) {\n\t\tspin_unlock(&ci->i_ceph_lock);\n\t\tiput(inode);\n\t\tgoto random;\n\t}\n\tmds = cap->session->s_mds;\n\tdout(\"%s %p %llx.%llx mds%d (%scap %p)\\n\", __func__,\n\t     inode, ceph_vinop(inode), mds,\n\t     cap == ci->i_auth_cap ? \"auth \" : \"\", cap);\n\tspin_unlock(&ci->i_ceph_lock);\nout:\n\tiput(inode);\n\treturn mds;\n\nrandom:\n\tif (random)\n\t\t*random = true;\n\n\tmds = ceph_mdsmap_get_random_mds(mdsc->mdsmap);\n\tdout(\"%s chose random mds%d\\n\", __func__, mds);\n\treturn mds;\n}\n\n\n \nstruct ceph_msg *ceph_create_session_msg(u32 op, u64 seq)\n{\n\tstruct ceph_msg *msg;\n\tstruct ceph_mds_session_head *h;\n\n\tmsg = ceph_msg_new(CEPH_MSG_CLIENT_SESSION, sizeof(*h), GFP_NOFS,\n\t\t\t   false);\n\tif (!msg) {\n\t\tpr_err(\"ENOMEM creating session %s msg\\n\",\n\t\t       ceph_session_op_name(op));\n\t\treturn NULL;\n\t}\n\th = msg->front.iov_base;\n\th->op = cpu_to_le32(op);\n\th->seq = cpu_to_le64(seq);\n\n\treturn msg;\n}\n\nstatic const unsigned char feature_bits[] = CEPHFS_FEATURES_CLIENT_SUPPORTED;\n#define FEATURE_BYTES(c) (DIV_ROUND_UP((size_t)feature_bits[c - 1] + 1, 64) * 8)\nstatic int encode_supported_features(void **p, void *end)\n{\n\tstatic const size_t count = ARRAY_SIZE(feature_bits);\n\n\tif (count > 0) {\n\t\tsize_t i;\n\t\tsize_t size = FEATURE_BYTES(count);\n\t\tunsigned long bit;\n\n\t\tif (WARN_ON_ONCE(*p + 4 + size > end))\n\t\t\treturn -ERANGE;\n\n\t\tceph_encode_32(p, size);\n\t\tmemset(*p, 0, size);\n\t\tfor (i = 0; i < count; i++) {\n\t\t\tbit = feature_bits[i];\n\t\t\t((unsigned char *)(*p))[bit / 8] |= BIT(bit % 8);\n\t\t}\n\t\t*p += size;\n\t} else {\n\t\tif (WARN_ON_ONCE(*p + 4 > end))\n\t\t\treturn -ERANGE;\n\n\t\tceph_encode_32(p, 0);\n\t}\n\n\treturn 0;\n}\n\nstatic const unsigned char metric_bits[] = CEPHFS_METRIC_SPEC_CLIENT_SUPPORTED;\n#define METRIC_BYTES(cnt) (DIV_ROUND_UP((size_t)metric_bits[cnt - 1] + 1, 64) * 8)\nstatic int encode_metric_spec(void **p, void *end)\n{\n\tstatic const size_t count = ARRAY_SIZE(metric_bits);\n\n\t \n\tif (WARN_ON_ONCE(*p + 2 > end))\n\t\treturn -ERANGE;\n\n\tceph_encode_8(p, 1);  \n\tceph_encode_8(p, 1);  \n\n\tif (count > 0) {\n\t\tsize_t i;\n\t\tsize_t size = METRIC_BYTES(count);\n\n\t\tif (WARN_ON_ONCE(*p + 4 + 4 + size > end))\n\t\t\treturn -ERANGE;\n\n\t\t \n\t\tceph_encode_32(p, 4 + size);\n\n\t\t \n\t\tceph_encode_32(p, size);\n\t\tmemset(*p, 0, size);\n\t\tfor (i = 0; i < count; i++)\n\t\t\t((unsigned char *)(*p))[i / 8] |= BIT(metric_bits[i] % 8);\n\t\t*p += size;\n\t} else {\n\t\tif (WARN_ON_ONCE(*p + 4 + 4 > end))\n\t\t\treturn -ERANGE;\n\n\t\t \n\t\tceph_encode_32(p, 4);\n\t\t \n\t\tceph_encode_32(p, 0);\n\t}\n\n\treturn 0;\n}\n\n \nstatic struct ceph_msg *create_session_open_msg(struct ceph_mds_client *mdsc, u64 seq)\n{\n\tstruct ceph_msg *msg;\n\tstruct ceph_mds_session_head *h;\n\tint i;\n\tint extra_bytes = 0;\n\tint metadata_key_count = 0;\n\tstruct ceph_options *opt = mdsc->fsc->client->options;\n\tstruct ceph_mount_options *fsopt = mdsc->fsc->mount_options;\n\tsize_t size, count;\n\tvoid *p, *end;\n\tint ret;\n\n\tconst char* metadata[][2] = {\n\t\t{\"hostname\", mdsc->nodename},\n\t\t{\"kernel_version\", init_utsname()->release},\n\t\t{\"entity_id\", opt->name ? : \"\"},\n\t\t{\"root\", fsopt->server_path ? : \"/\"},\n\t\t{NULL, NULL}\n\t};\n\n\t \n\textra_bytes = 4;   \n\tfor (i = 0; metadata[i][0]; ++i) {\n\t\textra_bytes += 8 + strlen(metadata[i][0]) +\n\t\t\tstrlen(metadata[i][1]);\n\t\tmetadata_key_count++;\n\t}\n\n\t \n\tsize = 0;\n\tcount = ARRAY_SIZE(feature_bits);\n\tif (count > 0)\n\t\tsize = FEATURE_BYTES(count);\n\textra_bytes += 4 + size;\n\n\t \n\tsize = 0;\n\tcount = ARRAY_SIZE(metric_bits);\n\tif (count > 0)\n\t\tsize = METRIC_BYTES(count);\n\textra_bytes += 2 + 4 + 4 + size;\n\n\t \n\tmsg = ceph_msg_new(CEPH_MSG_CLIENT_SESSION, sizeof(*h) + extra_bytes,\n\t\t\t   GFP_NOFS, false);\n\tif (!msg) {\n\t\tpr_err(\"ENOMEM creating session open msg\\n\");\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tp = msg->front.iov_base;\n\tend = p + msg->front.iov_len;\n\n\th = p;\n\th->op = cpu_to_le32(CEPH_SESSION_REQUEST_OPEN);\n\th->seq = cpu_to_le64(seq);\n\n\t \n\tmsg->hdr.version = cpu_to_le16(4);\n\tmsg->hdr.compat_version = cpu_to_le16(1);\n\n\t \n\tp += sizeof(*h);\n\n\t \n\tceph_encode_32(&p, metadata_key_count);\n\n\t \n\tfor (i = 0; metadata[i][0]; ++i) {\n\t\tsize_t const key_len = strlen(metadata[i][0]);\n\t\tsize_t const val_len = strlen(metadata[i][1]);\n\n\t\tceph_encode_32(&p, key_len);\n\t\tmemcpy(p, metadata[i][0], key_len);\n\t\tp += key_len;\n\t\tceph_encode_32(&p, val_len);\n\t\tmemcpy(p, metadata[i][1], val_len);\n\t\tp += val_len;\n\t}\n\n\tret = encode_supported_features(&p, end);\n\tif (ret) {\n\t\tpr_err(\"encode_supported_features failed!\\n\");\n\t\tceph_msg_put(msg);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tret = encode_metric_spec(&p, end);\n\tif (ret) {\n\t\tpr_err(\"encode_metric_spec failed!\\n\");\n\t\tceph_msg_put(msg);\n\t\treturn ERR_PTR(ret);\n\t}\n\n\tmsg->front.iov_len = p - msg->front.iov_base;\n\tmsg->hdr.front_len = cpu_to_le32(msg->front.iov_len);\n\n\treturn msg;\n}\n\n \nstatic int __open_session(struct ceph_mds_client *mdsc,\n\t\t\t  struct ceph_mds_session *session)\n{\n\tstruct ceph_msg *msg;\n\tint mstate;\n\tint mds = session->s_mds;\n\n\tif (READ_ONCE(mdsc->fsc->mount_state) == CEPH_MOUNT_FENCE_IO)\n\t\treturn -EIO;\n\n\t \n\tmstate = ceph_mdsmap_get_state(mdsc->mdsmap, mds);\n\tdout(\"open_session to mds%d (%s)\\n\", mds,\n\t     ceph_mds_state_name(mstate));\n\tsession->s_state = CEPH_MDS_SESSION_OPENING;\n\tsession->s_renew_requested = jiffies;\n\n\t \n\tmsg = create_session_open_msg(mdsc, session->s_seq);\n\tif (IS_ERR(msg))\n\t\treturn PTR_ERR(msg);\n\tceph_con_send(&session->s_con, msg);\n\treturn 0;\n}\n\n \nstatic struct ceph_mds_session *\n__open_export_target_session(struct ceph_mds_client *mdsc, int target)\n{\n\tstruct ceph_mds_session *session;\n\tint ret;\n\n\tsession = __ceph_lookup_mds_session(mdsc, target);\n\tif (!session) {\n\t\tsession = register_session(mdsc, target);\n\t\tif (IS_ERR(session))\n\t\t\treturn session;\n\t}\n\tif (session->s_state == CEPH_MDS_SESSION_NEW ||\n\t    session->s_state == CEPH_MDS_SESSION_CLOSING) {\n\t\tret = __open_session(mdsc, session);\n\t\tif (ret)\n\t\t\treturn ERR_PTR(ret);\n\t}\n\n\treturn session;\n}\n\nstruct ceph_mds_session *\nceph_mdsc_open_export_target_session(struct ceph_mds_client *mdsc, int target)\n{\n\tstruct ceph_mds_session *session;\n\n\tdout(\"open_export_target_session to mds%d\\n\", target);\n\n\tmutex_lock(&mdsc->mutex);\n\tsession = __open_export_target_session(mdsc, target);\n\tmutex_unlock(&mdsc->mutex);\n\n\treturn session;\n}\n\nstatic void __open_export_target_sessions(struct ceph_mds_client *mdsc,\n\t\t\t\t\t  struct ceph_mds_session *session)\n{\n\tstruct ceph_mds_info *mi;\n\tstruct ceph_mds_session *ts;\n\tint i, mds = session->s_mds;\n\n\tif (mds >= mdsc->mdsmap->possible_max_rank)\n\t\treturn;\n\n\tmi = &mdsc->mdsmap->m_info[mds];\n\tdout(\"open_export_target_sessions for mds%d (%d targets)\\n\",\n\t     session->s_mds, mi->num_export_targets);\n\n\tfor (i = 0; i < mi->num_export_targets; i++) {\n\t\tts = __open_export_target_session(mdsc, mi->export_targets[i]);\n\t\tceph_put_mds_session(ts);\n\t}\n}\n\nvoid ceph_mdsc_open_export_target_sessions(struct ceph_mds_client *mdsc,\n\t\t\t\t\t   struct ceph_mds_session *session)\n{\n\tmutex_lock(&mdsc->mutex);\n\t__open_export_target_sessions(mdsc, session);\n\tmutex_unlock(&mdsc->mutex);\n}\n\n \n\nstatic void detach_cap_releases(struct ceph_mds_session *session,\n\t\t\t\tstruct list_head *target)\n{\n\tlockdep_assert_held(&session->s_cap_lock);\n\n\tlist_splice_init(&session->s_cap_releases, target);\n\tsession->s_num_cap_releases = 0;\n\tdout(\"dispose_cap_releases mds%d\\n\", session->s_mds);\n}\n\nstatic void dispose_cap_releases(struct ceph_mds_client *mdsc,\n\t\t\t\t struct list_head *dispose)\n{\n\twhile (!list_empty(dispose)) {\n\t\tstruct ceph_cap *cap;\n\t\t \n\t\tcap = list_first_entry(dispose, struct ceph_cap, session_caps);\n\t\tlist_del(&cap->session_caps);\n\t\tceph_put_cap(mdsc, cap);\n\t}\n}\n\nstatic void cleanup_session_requests(struct ceph_mds_client *mdsc,\n\t\t\t\t     struct ceph_mds_session *session)\n{\n\tstruct ceph_mds_request *req;\n\tstruct rb_node *p;\n\n\tdout(\"cleanup_session_requests mds%d\\n\", session->s_mds);\n\tmutex_lock(&mdsc->mutex);\n\twhile (!list_empty(&session->s_unsafe)) {\n\t\treq = list_first_entry(&session->s_unsafe,\n\t\t\t\t       struct ceph_mds_request, r_unsafe_item);\n\t\tpr_warn_ratelimited(\" dropping unsafe request %llu\\n\",\n\t\t\t\t    req->r_tid);\n\t\tif (req->r_target_inode)\n\t\t\tmapping_set_error(req->r_target_inode->i_mapping, -EIO);\n\t\tif (req->r_unsafe_dir)\n\t\t\tmapping_set_error(req->r_unsafe_dir->i_mapping, -EIO);\n\t\t__unregister_request(mdsc, req);\n\t}\n\t \n\tp = rb_first(&mdsc->request_tree);\n\twhile (p) {\n\t\treq = rb_entry(p, struct ceph_mds_request, r_node);\n\t\tp = rb_next(p);\n\t\tif (req->r_session &&\n\t\t    req->r_session->s_mds == session->s_mds)\n\t\t\treq->r_attempts = 0;\n\t}\n\tmutex_unlock(&mdsc->mutex);\n}\n\n \nint ceph_iterate_session_caps(struct ceph_mds_session *session,\n\t\t\t      int (*cb)(struct inode *, int mds, void *),\n\t\t\t      void *arg)\n{\n\tstruct list_head *p;\n\tstruct ceph_cap *cap;\n\tstruct inode *inode, *last_inode = NULL;\n\tstruct ceph_cap *old_cap = NULL;\n\tint ret;\n\n\tdout(\"iterate_session_caps %p mds%d\\n\", session, session->s_mds);\n\tspin_lock(&session->s_cap_lock);\n\tp = session->s_caps.next;\n\twhile (p != &session->s_caps) {\n\t\tint mds;\n\n\t\tcap = list_entry(p, struct ceph_cap, session_caps);\n\t\tinode = igrab(&cap->ci->netfs.inode);\n\t\tif (!inode) {\n\t\t\tp = p->next;\n\t\t\tcontinue;\n\t\t}\n\t\tsession->s_cap_iterator = cap;\n\t\tmds = cap->mds;\n\t\tspin_unlock(&session->s_cap_lock);\n\n\t\tif (last_inode) {\n\t\t\tiput(last_inode);\n\t\t\tlast_inode = NULL;\n\t\t}\n\t\tif (old_cap) {\n\t\t\tceph_put_cap(session->s_mdsc, old_cap);\n\t\t\told_cap = NULL;\n\t\t}\n\n\t\tret = cb(inode, mds, arg);\n\t\tlast_inode = inode;\n\n\t\tspin_lock(&session->s_cap_lock);\n\t\tp = p->next;\n\t\tif (!cap->ci) {\n\t\t\tdout(\"iterate_session_caps  finishing cap %p removal\\n\",\n\t\t\t     cap);\n\t\t\tBUG_ON(cap->session != session);\n\t\t\tcap->session = NULL;\n\t\t\tlist_del_init(&cap->session_caps);\n\t\t\tsession->s_nr_caps--;\n\t\t\tatomic64_dec(&session->s_mdsc->metric.total_caps);\n\t\t\tif (cap->queue_release)\n\t\t\t\t__ceph_queue_cap_release(session, cap);\n\t\t\telse\n\t\t\t\told_cap = cap;   \n\t\t}\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n\tret = 0;\nout:\n\tsession->s_cap_iterator = NULL;\n\tspin_unlock(&session->s_cap_lock);\n\n\tiput(last_inode);\n\tif (old_cap)\n\t\tceph_put_cap(session->s_mdsc, old_cap);\n\n\treturn ret;\n}\n\nstatic int remove_session_caps_cb(struct inode *inode, int mds, void *arg)\n{\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tbool invalidate = false;\n\tstruct ceph_cap *cap;\n\tint iputs = 0;\n\n\tspin_lock(&ci->i_ceph_lock);\n\tcap = __get_cap_for_mds(ci, mds);\n\tif (cap) {\n\t\tdout(\" removing cap %p, ci is %p, inode is %p\\n\",\n\t\t     cap, ci, &ci->netfs.inode);\n\n\t\tiputs = ceph_purge_inode_cap(inode, cap, &invalidate);\n\t}\n\tspin_unlock(&ci->i_ceph_lock);\n\n\tif (cap)\n\t\twake_up_all(&ci->i_cap_wq);\n\tif (invalidate)\n\t\tceph_queue_invalidate(inode);\n\twhile (iputs--)\n\t\tiput(inode);\n\treturn 0;\n}\n\n \nstatic void remove_session_caps(struct ceph_mds_session *session)\n{\n\tstruct ceph_fs_client *fsc = session->s_mdsc->fsc;\n\tstruct super_block *sb = fsc->sb;\n\tLIST_HEAD(dispose);\n\n\tdout(\"remove_session_caps on %p\\n\", session);\n\tceph_iterate_session_caps(session, remove_session_caps_cb, fsc);\n\n\twake_up_all(&fsc->mdsc->cap_flushing_wq);\n\n\tspin_lock(&session->s_cap_lock);\n\tif (session->s_nr_caps > 0) {\n\t\tstruct inode *inode;\n\t\tstruct ceph_cap *cap, *prev = NULL;\n\t\tstruct ceph_vino vino;\n\t\t \n\t\twhile (!list_empty(&session->s_caps)) {\n\t\t\tcap = list_entry(session->s_caps.next,\n\t\t\t\t\t struct ceph_cap, session_caps);\n\t\t\tif (cap == prev)\n\t\t\t\tbreak;\n\t\t\tprev = cap;\n\t\t\tvino = cap->ci->i_vino;\n\t\t\tspin_unlock(&session->s_cap_lock);\n\n\t\t\tinode = ceph_find_inode(sb, vino);\n\t\t\tiput(inode);\n\n\t\t\tspin_lock(&session->s_cap_lock);\n\t\t}\n\t}\n\n\t \n\tdetach_cap_releases(session, &dispose);\n\n\tBUG_ON(session->s_nr_caps > 0);\n\tBUG_ON(!list_empty(&session->s_cap_flushing));\n\tspin_unlock(&session->s_cap_lock);\n\tdispose_cap_releases(session->s_mdsc, &dispose);\n}\n\nenum {\n\tRECONNECT,\n\tRENEWCAPS,\n\tFORCE_RO,\n};\n\n \nstatic int wake_up_session_cb(struct inode *inode, int mds, void *arg)\n{\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tunsigned long ev = (unsigned long)arg;\n\n\tif (ev == RECONNECT) {\n\t\tspin_lock(&ci->i_ceph_lock);\n\t\tci->i_wanted_max_size = 0;\n\t\tci->i_requested_max_size = 0;\n\t\tspin_unlock(&ci->i_ceph_lock);\n\t} else if (ev == RENEWCAPS) {\n\t\tstruct ceph_cap *cap;\n\n\t\tspin_lock(&ci->i_ceph_lock);\n\t\tcap = __get_cap_for_mds(ci, mds);\n\t\t \n\t\tif (cap && cap->cap_gen < atomic_read(&cap->session->s_cap_gen))\n\t\t\tcap->issued = cap->implemented = CEPH_CAP_PIN;\n\t\tspin_unlock(&ci->i_ceph_lock);\n\t} else if (ev == FORCE_RO) {\n\t}\n\twake_up_all(&ci->i_cap_wq);\n\treturn 0;\n}\n\nstatic void wake_up_session_caps(struct ceph_mds_session *session, int ev)\n{\n\tdout(\"wake_up_session_caps %p mds%d\\n\", session, session->s_mds);\n\tceph_iterate_session_caps(session, wake_up_session_cb,\n\t\t\t\t  (void *)(unsigned long)ev);\n}\n\n \nstatic int send_renew_caps(struct ceph_mds_client *mdsc,\n\t\t\t   struct ceph_mds_session *session)\n{\n\tstruct ceph_msg *msg;\n\tint state;\n\n\tif (time_after_eq(jiffies, session->s_cap_ttl) &&\n\t    time_after_eq(session->s_cap_ttl, session->s_renew_requested))\n\t\tpr_info(\"mds%d caps stale\\n\", session->s_mds);\n\tsession->s_renew_requested = jiffies;\n\n\t \n\tstate = ceph_mdsmap_get_state(mdsc->mdsmap, session->s_mds);\n\tif (state < CEPH_MDS_STATE_RECONNECT) {\n\t\tdout(\"send_renew_caps ignoring mds%d (%s)\\n\",\n\t\t     session->s_mds, ceph_mds_state_name(state));\n\t\treturn 0;\n\t}\n\n\tdout(\"send_renew_caps to mds%d (%s)\\n\", session->s_mds,\n\t\tceph_mds_state_name(state));\n\tmsg = ceph_create_session_msg(CEPH_SESSION_REQUEST_RENEWCAPS,\n\t\t\t\t      ++session->s_renew_seq);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\tceph_con_send(&session->s_con, msg);\n\treturn 0;\n}\n\nstatic int send_flushmsg_ack(struct ceph_mds_client *mdsc,\n\t\t\t     struct ceph_mds_session *session, u64 seq)\n{\n\tstruct ceph_msg *msg;\n\n\tdout(\"send_flushmsg_ack to mds%d (%s)s seq %lld\\n\",\n\t     session->s_mds, ceph_session_state_name(session->s_state), seq);\n\tmsg = ceph_create_session_msg(CEPH_SESSION_FLUSHMSG_ACK, seq);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\tceph_con_send(&session->s_con, msg);\n\treturn 0;\n}\n\n\n \nstatic void renewed_caps(struct ceph_mds_client *mdsc,\n\t\t\t struct ceph_mds_session *session, int is_renew)\n{\n\tint was_stale;\n\tint wake = 0;\n\n\tspin_lock(&session->s_cap_lock);\n\twas_stale = is_renew && time_after_eq(jiffies, session->s_cap_ttl);\n\n\tsession->s_cap_ttl = session->s_renew_requested +\n\t\tmdsc->mdsmap->m_session_timeout*HZ;\n\n\tif (was_stale) {\n\t\tif (time_before(jiffies, session->s_cap_ttl)) {\n\t\t\tpr_info(\"mds%d caps renewed\\n\", session->s_mds);\n\t\t\twake = 1;\n\t\t} else {\n\t\t\tpr_info(\"mds%d caps still stale\\n\", session->s_mds);\n\t\t}\n\t}\n\tdout(\"renewed_caps mds%d ttl now %lu, was %s, now %s\\n\",\n\t     session->s_mds, session->s_cap_ttl, was_stale ? \"stale\" : \"fresh\",\n\t     time_before(jiffies, session->s_cap_ttl) ? \"stale\" : \"fresh\");\n\tspin_unlock(&session->s_cap_lock);\n\n\tif (wake)\n\t\twake_up_session_caps(session, RENEWCAPS);\n}\n\n \nstatic int request_close_session(struct ceph_mds_session *session)\n{\n\tstruct ceph_msg *msg;\n\n\tdout(\"request_close_session mds%d state %s seq %lld\\n\",\n\t     session->s_mds, ceph_session_state_name(session->s_state),\n\t     session->s_seq);\n\tmsg = ceph_create_session_msg(CEPH_SESSION_REQUEST_CLOSE,\n\t\t\t\t      session->s_seq);\n\tif (!msg)\n\t\treturn -ENOMEM;\n\tceph_con_send(&session->s_con, msg);\n\treturn 1;\n}\n\n \nstatic int __close_session(struct ceph_mds_client *mdsc,\n\t\t\t struct ceph_mds_session *session)\n{\n\tif (session->s_state >= CEPH_MDS_SESSION_CLOSING)\n\t\treturn 0;\n\tsession->s_state = CEPH_MDS_SESSION_CLOSING;\n\treturn request_close_session(session);\n}\n\nstatic bool drop_negative_children(struct dentry *dentry)\n{\n\tstruct dentry *child;\n\tbool all_negative = true;\n\n\tif (!d_is_dir(dentry))\n\t\tgoto out;\n\n\tspin_lock(&dentry->d_lock);\n\tlist_for_each_entry(child, &dentry->d_subdirs, d_child) {\n\t\tif (d_really_is_positive(child)) {\n\t\t\tall_negative = false;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&dentry->d_lock);\n\n\tif (all_negative)\n\t\tshrink_dcache_parent(dentry);\nout:\n\treturn all_negative;\n}\n\n \nstatic int trim_caps_cb(struct inode *inode, int mds, void *arg)\n{\n\tint *remaining = arg;\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tint used, wanted, oissued, mine;\n\tstruct ceph_cap *cap;\n\n\tif (*remaining <= 0)\n\t\treturn -1;\n\n\tspin_lock(&ci->i_ceph_lock);\n\tcap = __get_cap_for_mds(ci, mds);\n\tif (!cap) {\n\t\tspin_unlock(&ci->i_ceph_lock);\n\t\treturn 0;\n\t}\n\tmine = cap->issued | cap->implemented;\n\tused = __ceph_caps_used(ci);\n\twanted = __ceph_caps_file_wanted(ci);\n\toissued = __ceph_caps_issued_other(ci, cap);\n\n\tdout(\"trim_caps_cb %p cap %p mine %s oissued %s used %s wanted %s\\n\",\n\t     inode, cap, ceph_cap_string(mine), ceph_cap_string(oissued),\n\t     ceph_cap_string(used), ceph_cap_string(wanted));\n\tif (cap == ci->i_auth_cap) {\n\t\tif (ci->i_dirty_caps || ci->i_flushing_caps ||\n\t\t    !list_empty(&ci->i_cap_snaps))\n\t\t\tgoto out;\n\t\tif ((used | wanted) & CEPH_CAP_ANY_WR)\n\t\t\tgoto out;\n\t\t \n\t\tif (atomic_read(&ci->i_filelock_ref) > 0)\n\t\t\tgoto out;\n\t}\n\t \n\tif (S_ISREG(inode->i_mode) &&\n\t    wanted == 0 && used == CEPH_CAP_FILE_CACHE &&\n\t    !(oissued & CEPH_CAP_FILE_CACHE)) {\n\t  used = 0;\n\t  oissued = 0;\n\t}\n\tif ((used | wanted) & ~oissued & mine)\n\t\tgoto out;    \n\n\tif (oissued) {\n\t\t \n\t\tceph_remove_cap(cap, true);\n\t\t(*remaining)--;\n\t} else {\n\t\tstruct dentry *dentry;\n\t\t \n\t\tspin_unlock(&ci->i_ceph_lock);\n\t\tdentry = d_find_any_alias(inode);\n\t\tif (dentry && drop_negative_children(dentry)) {\n\t\t\tint count;\n\t\t\tdput(dentry);\n\t\t\td_prune_aliases(inode);\n\t\t\tcount = atomic_read(&inode->i_count);\n\t\t\tif (count == 1)\n\t\t\t\t(*remaining)--;\n\t\t\tdout(\"trim_caps_cb %p cap %p pruned, count now %d\\n\",\n\t\t\t     inode, cap, count);\n\t\t} else {\n\t\t\tdput(dentry);\n\t\t}\n\t\treturn 0;\n\t}\n\nout:\n\tspin_unlock(&ci->i_ceph_lock);\n\treturn 0;\n}\n\n \nint ceph_trim_caps(struct ceph_mds_client *mdsc,\n\t\t   struct ceph_mds_session *session,\n\t\t   int max_caps)\n{\n\tint trim_caps = session->s_nr_caps - max_caps;\n\n\tdout(\"trim_caps mds%d start: %d / %d, trim %d\\n\",\n\t     session->s_mds, session->s_nr_caps, max_caps, trim_caps);\n\tif (trim_caps > 0) {\n\t\tint remaining = trim_caps;\n\n\t\tceph_iterate_session_caps(session, trim_caps_cb, &remaining);\n\t\tdout(\"trim_caps mds%d done: %d / %d, trimmed %d\\n\",\n\t\t     session->s_mds, session->s_nr_caps, max_caps,\n\t\t\ttrim_caps - remaining);\n\t}\n\n\tceph_flush_cap_releases(mdsc, session);\n\treturn 0;\n}\n\nstatic int check_caps_flush(struct ceph_mds_client *mdsc,\n\t\t\t    u64 want_flush_tid)\n{\n\tint ret = 1;\n\n\tspin_lock(&mdsc->cap_dirty_lock);\n\tif (!list_empty(&mdsc->cap_flush_list)) {\n\t\tstruct ceph_cap_flush *cf =\n\t\t\tlist_first_entry(&mdsc->cap_flush_list,\n\t\t\t\t\t struct ceph_cap_flush, g_list);\n\t\tif (cf->tid <= want_flush_tid) {\n\t\t\tdout(\"check_caps_flush still flushing tid \"\n\t\t\t     \"%llu <= %llu\\n\", cf->tid, want_flush_tid);\n\t\t\tret = 0;\n\t\t}\n\t}\n\tspin_unlock(&mdsc->cap_dirty_lock);\n\treturn ret;\n}\n\n \nstatic void wait_caps_flush(struct ceph_mds_client *mdsc,\n\t\t\t    u64 want_flush_tid)\n{\n\tdout(\"check_caps_flush want %llu\\n\", want_flush_tid);\n\n\twait_event(mdsc->cap_flushing_wq,\n\t\t   check_caps_flush(mdsc, want_flush_tid));\n\n\tdout(\"check_caps_flush ok, flushed thru %llu\\n\", want_flush_tid);\n}\n\n \nstatic void ceph_send_cap_releases(struct ceph_mds_client *mdsc,\n\t\t\t\t   struct ceph_mds_session *session)\n{\n\tstruct ceph_msg *msg = NULL;\n\tstruct ceph_mds_cap_release *head;\n\tstruct ceph_mds_cap_item *item;\n\tstruct ceph_osd_client *osdc = &mdsc->fsc->client->osdc;\n\tstruct ceph_cap *cap;\n\tLIST_HEAD(tmp_list);\n\tint num_cap_releases;\n\t__le32\tbarrier, *cap_barrier;\n\n\tdown_read(&osdc->lock);\n\tbarrier = cpu_to_le32(osdc->epoch_barrier);\n\tup_read(&osdc->lock);\n\n\tspin_lock(&session->s_cap_lock);\nagain:\n\tlist_splice_init(&session->s_cap_releases, &tmp_list);\n\tnum_cap_releases = session->s_num_cap_releases;\n\tsession->s_num_cap_releases = 0;\n\tspin_unlock(&session->s_cap_lock);\n\n\twhile (!list_empty(&tmp_list)) {\n\t\tif (!msg) {\n\t\t\tmsg = ceph_msg_new(CEPH_MSG_CLIENT_CAPRELEASE,\n\t\t\t\t\tPAGE_SIZE, GFP_NOFS, false);\n\t\t\tif (!msg)\n\t\t\t\tgoto out_err;\n\t\t\thead = msg->front.iov_base;\n\t\t\thead->num = cpu_to_le32(0);\n\t\t\tmsg->front.iov_len = sizeof(*head);\n\n\t\t\tmsg->hdr.version = cpu_to_le16(2);\n\t\t\tmsg->hdr.compat_version = cpu_to_le16(1);\n\t\t}\n\n\t\tcap = list_first_entry(&tmp_list, struct ceph_cap,\n\t\t\t\t\tsession_caps);\n\t\tlist_del(&cap->session_caps);\n\t\tnum_cap_releases--;\n\n\t\thead = msg->front.iov_base;\n\t\tput_unaligned_le32(get_unaligned_le32(&head->num) + 1,\n\t\t\t\t   &head->num);\n\t\titem = msg->front.iov_base + msg->front.iov_len;\n\t\titem->ino = cpu_to_le64(cap->cap_ino);\n\t\titem->cap_id = cpu_to_le64(cap->cap_id);\n\t\titem->migrate_seq = cpu_to_le32(cap->mseq);\n\t\titem->seq = cpu_to_le32(cap->issue_seq);\n\t\tmsg->front.iov_len += sizeof(*item);\n\n\t\tceph_put_cap(mdsc, cap);\n\n\t\tif (le32_to_cpu(head->num) == CEPH_CAPS_PER_RELEASE) {\n\t\t\t\n\t\t\tcap_barrier = msg->front.iov_base + msg->front.iov_len;\n\t\t\t*cap_barrier = barrier;\n\t\t\tmsg->front.iov_len += sizeof(*cap_barrier);\n\n\t\t\tmsg->hdr.front_len = cpu_to_le32(msg->front.iov_len);\n\t\t\tdout(\"send_cap_releases mds%d %p\\n\", session->s_mds, msg);\n\t\t\tceph_con_send(&session->s_con, msg);\n\t\t\tmsg = NULL;\n\t\t}\n\t}\n\n\tBUG_ON(num_cap_releases != 0);\n\n\tspin_lock(&session->s_cap_lock);\n\tif (!list_empty(&session->s_cap_releases))\n\t\tgoto again;\n\tspin_unlock(&session->s_cap_lock);\n\n\tif (msg) {\n\t\t\n\t\tcap_barrier = msg->front.iov_base + msg->front.iov_len;\n\t\t*cap_barrier = barrier;\n\t\tmsg->front.iov_len += sizeof(*cap_barrier);\n\n\t\tmsg->hdr.front_len = cpu_to_le32(msg->front.iov_len);\n\t\tdout(\"send_cap_releases mds%d %p\\n\", session->s_mds, msg);\n\t\tceph_con_send(&session->s_con, msg);\n\t}\n\treturn;\nout_err:\n\tpr_err(\"send_cap_releases mds%d, failed to allocate message\\n\",\n\t\tsession->s_mds);\n\tspin_lock(&session->s_cap_lock);\n\tlist_splice(&tmp_list, &session->s_cap_releases);\n\tsession->s_num_cap_releases += num_cap_releases;\n\tspin_unlock(&session->s_cap_lock);\n}\n\nstatic void ceph_cap_release_work(struct work_struct *work)\n{\n\tstruct ceph_mds_session *session =\n\t\tcontainer_of(work, struct ceph_mds_session, s_cap_release_work);\n\n\tmutex_lock(&session->s_mutex);\n\tif (session->s_state == CEPH_MDS_SESSION_OPEN ||\n\t    session->s_state == CEPH_MDS_SESSION_HUNG)\n\t\tceph_send_cap_releases(session->s_mdsc, session);\n\tmutex_unlock(&session->s_mutex);\n\tceph_put_mds_session(session);\n}\n\nvoid ceph_flush_cap_releases(struct ceph_mds_client *mdsc,\n\t\t             struct ceph_mds_session *session)\n{\n\tif (mdsc->stopping)\n\t\treturn;\n\n\tceph_get_mds_session(session);\n\tif (queue_work(mdsc->fsc->cap_wq,\n\t\t       &session->s_cap_release_work)) {\n\t\tdout(\"cap release work queued\\n\");\n\t} else {\n\t\tceph_put_mds_session(session);\n\t\tdout(\"failed to queue cap release work\\n\");\n\t}\n}\n\n \nvoid __ceph_queue_cap_release(struct ceph_mds_session *session,\n\t\t\t      struct ceph_cap *cap)\n{\n\tlist_add_tail(&cap->session_caps, &session->s_cap_releases);\n\tsession->s_num_cap_releases++;\n\n\tif (!(session->s_num_cap_releases % CEPH_CAPS_PER_RELEASE))\n\t\tceph_flush_cap_releases(session->s_mdsc, session);\n}\n\nstatic void ceph_cap_reclaim_work(struct work_struct *work)\n{\n\tstruct ceph_mds_client *mdsc =\n\t\tcontainer_of(work, struct ceph_mds_client, cap_reclaim_work);\n\tint ret = ceph_trim_dentries(mdsc);\n\tif (ret == -EAGAIN)\n\t\tceph_queue_cap_reclaim_work(mdsc);\n}\n\nvoid ceph_queue_cap_reclaim_work(struct ceph_mds_client *mdsc)\n{\n\tif (mdsc->stopping)\n\t\treturn;\n\n        if (queue_work(mdsc->fsc->cap_wq, &mdsc->cap_reclaim_work)) {\n                dout(\"caps reclaim work queued\\n\");\n        } else {\n                dout(\"failed to queue caps release work\\n\");\n        }\n}\n\nvoid ceph_reclaim_caps_nr(struct ceph_mds_client *mdsc, int nr)\n{\n\tint val;\n\tif (!nr)\n\t\treturn;\n\tval = atomic_add_return(nr, &mdsc->cap_reclaim_pending);\n\tif ((val % CEPH_CAPS_PER_RELEASE) < nr) {\n\t\tatomic_set(&mdsc->cap_reclaim_pending, 0);\n\t\tceph_queue_cap_reclaim_work(mdsc);\n\t}\n}\n\n \n\nint ceph_alloc_readdir_reply_buffer(struct ceph_mds_request *req,\n\t\t\t\t    struct inode *dir)\n{\n\tstruct ceph_inode_info *ci = ceph_inode(dir);\n\tstruct ceph_mds_reply_info_parsed *rinfo = &req->r_reply_info;\n\tstruct ceph_mount_options *opt = req->r_mdsc->fsc->mount_options;\n\tsize_t size = sizeof(struct ceph_mds_reply_dir_entry);\n\tunsigned int num_entries;\n\tint order;\n\n\tspin_lock(&ci->i_ceph_lock);\n\tnum_entries = ci->i_files + ci->i_subdirs;\n\tspin_unlock(&ci->i_ceph_lock);\n\tnum_entries = max(num_entries, 1U);\n\tnum_entries = min(num_entries, opt->max_readdir);\n\n\torder = get_order(size * num_entries);\n\twhile (order >= 0) {\n\t\trinfo->dir_entries = (void*)__get_free_pages(GFP_KERNEL |\n\t\t\t\t\t\t\t     __GFP_NOWARN |\n\t\t\t\t\t\t\t     __GFP_ZERO,\n\t\t\t\t\t\t\t     order);\n\t\tif (rinfo->dir_entries)\n\t\t\tbreak;\n\t\torder--;\n\t}\n\tif (!rinfo->dir_entries)\n\t\treturn -ENOMEM;\n\n\tnum_entries = (PAGE_SIZE << order) / size;\n\tnum_entries = min(num_entries, opt->max_readdir);\n\n\trinfo->dir_buf_size = PAGE_SIZE << order;\n\treq->r_num_caps = num_entries + 1;\n\treq->r_args.readdir.max_entries = cpu_to_le32(num_entries);\n\treq->r_args.readdir.max_bytes = cpu_to_le32(opt->max_readdir_bytes);\n\treturn 0;\n}\n\n \nstruct ceph_mds_request *\nceph_mdsc_create_request(struct ceph_mds_client *mdsc, int op, int mode)\n{\n\tstruct ceph_mds_request *req;\n\n\treq = kmem_cache_zalloc(ceph_mds_request_cachep, GFP_NOFS);\n\tif (!req)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tmutex_init(&req->r_fill_mutex);\n\treq->r_mdsc = mdsc;\n\treq->r_started = jiffies;\n\treq->r_start_latency = ktime_get();\n\treq->r_resend_mds = -1;\n\tINIT_LIST_HEAD(&req->r_unsafe_dir_item);\n\tINIT_LIST_HEAD(&req->r_unsafe_target_item);\n\treq->r_fmode = -1;\n\treq->r_feature_needed = -1;\n\tkref_init(&req->r_kref);\n\tRB_CLEAR_NODE(&req->r_node);\n\tINIT_LIST_HEAD(&req->r_wait);\n\tinit_completion(&req->r_completion);\n\tinit_completion(&req->r_safe_completion);\n\tINIT_LIST_HEAD(&req->r_unsafe_item);\n\n\tktime_get_coarse_real_ts64(&req->r_stamp);\n\n\treq->r_op = op;\n\treq->r_direct_mode = mode;\n\treturn req;\n}\n\n \nstatic struct ceph_mds_request *__get_oldest_req(struct ceph_mds_client *mdsc)\n{\n\tif (RB_EMPTY_ROOT(&mdsc->request_tree))\n\t\treturn NULL;\n\treturn rb_entry(rb_first(&mdsc->request_tree),\n\t\t\tstruct ceph_mds_request, r_node);\n}\n\nstatic inline  u64 __get_oldest_tid(struct ceph_mds_client *mdsc)\n{\n\treturn mdsc->oldest_tid;\n}\n\n#if IS_ENABLED(CONFIG_FS_ENCRYPTION)\nstatic u8 *get_fscrypt_altname(const struct ceph_mds_request *req, u32 *plen)\n{\n\tstruct inode *dir = req->r_parent;\n\tstruct dentry *dentry = req->r_dentry;\n\tu8 *cryptbuf = NULL;\n\tu32 len = 0;\n\tint ret = 0;\n\n\t \n\tif (!dir || !dentry)\n\t\tgoto success;\n\n\t \n\tif (!IS_ENCRYPTED(dir))\n\t\tgoto success;\n\n\tret = ceph_fscrypt_prepare_readdir(dir);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\n\t \n\tif (!fscrypt_has_encryption_key(dir))\n\t\tgoto success;\n\n\tif (!fscrypt_fname_encrypted_size(dir, dentry->d_name.len, NAME_MAX,\n\t\t\t\t\t  &len)) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\t}\n\n\t \n\tif (len <= CEPH_NOHASH_NAME_MAX) {\n\t\tlen = 0;\n\t\tgoto success;\n\t}\n\n\tcryptbuf = kmalloc(len, GFP_KERNEL);\n\tif (!cryptbuf)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tret = fscrypt_fname_encrypt(dir, &dentry->d_name, cryptbuf, len);\n\tif (ret) {\n\t\tkfree(cryptbuf);\n\t\treturn ERR_PTR(ret);\n\t}\nsuccess:\n\t*plen = len;\n\treturn cryptbuf;\n}\n#else\nstatic u8 *get_fscrypt_altname(const struct ceph_mds_request *req, u32 *plen)\n{\n\t*plen = 0;\n\treturn NULL;\n}\n#endif\n\n \nchar *ceph_mdsc_build_path(struct dentry *dentry, int *plen, u64 *pbase,\n\t\t\t   int for_wire)\n{\n\tstruct dentry *cur;\n\tstruct inode *inode;\n\tchar *path;\n\tint pos;\n\tunsigned seq;\n\tu64 base;\n\n\tif (!dentry)\n\t\treturn ERR_PTR(-EINVAL);\n\n\tpath = __getname();\n\tif (!path)\n\t\treturn ERR_PTR(-ENOMEM);\nretry:\n\tpos = PATH_MAX - 1;\n\tpath[pos] = '\\0';\n\n\tseq = read_seqbegin(&rename_lock);\n\tcur = dget(dentry);\n\tfor (;;) {\n\t\tstruct dentry *parent;\n\n\t\tspin_lock(&cur->d_lock);\n\t\tinode = d_inode(cur);\n\t\tif (inode && ceph_snap(inode) == CEPH_SNAPDIR) {\n\t\t\tdout(\"build_path path+%d: %p SNAPDIR\\n\",\n\t\t\t     pos, cur);\n\t\t\tspin_unlock(&cur->d_lock);\n\t\t\tparent = dget_parent(cur);\n\t\t} else if (for_wire && inode && dentry != cur &&\n\t\t\t   ceph_snap(inode) == CEPH_NOSNAP) {\n\t\t\tspin_unlock(&cur->d_lock);\n\t\t\tpos++;  \n\t\t\tbreak;\n\t\t} else if (!for_wire || !IS_ENCRYPTED(d_inode(cur->d_parent))) {\n\t\t\tpos -= cur->d_name.len;\n\t\t\tif (pos < 0) {\n\t\t\t\tspin_unlock(&cur->d_lock);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmemcpy(path + pos, cur->d_name.name, cur->d_name.len);\n\t\t\tspin_unlock(&cur->d_lock);\n\t\t\tparent = dget_parent(cur);\n\t\t} else {\n\t\t\tint len, ret;\n\t\t\tchar buf[NAME_MAX];\n\n\t\t\t \n\t\t\tmemcpy(buf, cur->d_name.name, cur->d_name.len);\n\t\t\tlen = cur->d_name.len;\n\t\t\tspin_unlock(&cur->d_lock);\n\t\t\tparent = dget_parent(cur);\n\n\t\t\tret = ceph_fscrypt_prepare_readdir(d_inode(parent));\n\t\t\tif (ret < 0) {\n\t\t\t\tdput(parent);\n\t\t\t\tdput(cur);\n\t\t\t\treturn ERR_PTR(ret);\n\t\t\t}\n\n\t\t\tif (fscrypt_has_encryption_key(d_inode(parent))) {\n\t\t\t\tlen = ceph_encode_encrypted_fname(d_inode(parent),\n\t\t\t\t\t\t\t\t  cur, buf);\n\t\t\t\tif (len < 0) {\n\t\t\t\t\tdput(parent);\n\t\t\t\t\tdput(cur);\n\t\t\t\t\treturn ERR_PTR(len);\n\t\t\t\t}\n\t\t\t}\n\t\t\tpos -= len;\n\t\t\tif (pos < 0) {\n\t\t\t\tdput(parent);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tmemcpy(path + pos, buf, len);\n\t\t}\n\t\tdput(cur);\n\t\tcur = parent;\n\n\t\t \n\t\tif (IS_ROOT(cur))\n\t\t\tbreak;\n\n\t\t \n\t\tif (--pos < 0)\n\t\t\tbreak;\n\n\t\tpath[pos] = '/';\n\t}\n\tinode = d_inode(cur);\n\tbase = inode ? ceph_ino(inode) : 0;\n\tdput(cur);\n\n\tif (read_seqretry(&rename_lock, seq))\n\t\tgoto retry;\n\n\tif (pos < 0) {\n\t\t \n\t\tpr_warn(\"build_path did not end path lookup where expected (pos = %d)\\n\",\n\t\t\tpos);\n\t\tgoto retry;\n\t}\n\n\t*pbase = base;\n\t*plen = PATH_MAX - 1 - pos;\n\tdout(\"build_path on %p %d built %llx '%.*s'\\n\",\n\t     dentry, d_count(dentry), base, *plen, path + pos);\n\treturn path + pos;\n}\n\nstatic int build_dentry_path(struct dentry *dentry, struct inode *dir,\n\t\t\t     const char **ppath, int *ppathlen, u64 *pino,\n\t\t\t     bool *pfreepath, bool parent_locked)\n{\n\tchar *path;\n\n\trcu_read_lock();\n\tif (!dir)\n\t\tdir = d_inode_rcu(dentry->d_parent);\n\tif (dir && parent_locked && ceph_snap(dir) == CEPH_NOSNAP &&\n\t    !IS_ENCRYPTED(dir)) {\n\t\t*pino = ceph_ino(dir);\n\t\trcu_read_unlock();\n\t\t*ppath = dentry->d_name.name;\n\t\t*ppathlen = dentry->d_name.len;\n\t\treturn 0;\n\t}\n\trcu_read_unlock();\n\tpath = ceph_mdsc_build_path(dentry, ppathlen, pino, 1);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\t*ppath = path;\n\t*pfreepath = true;\n\treturn 0;\n}\n\nstatic int build_inode_path(struct inode *inode,\n\t\t\t    const char **ppath, int *ppathlen, u64 *pino,\n\t\t\t    bool *pfreepath)\n{\n\tstruct dentry *dentry;\n\tchar *path;\n\n\tif (ceph_snap(inode) == CEPH_NOSNAP) {\n\t\t*pino = ceph_ino(inode);\n\t\t*ppathlen = 0;\n\t\treturn 0;\n\t}\n\tdentry = d_find_alias(inode);\n\tpath = ceph_mdsc_build_path(dentry, ppathlen, pino, 1);\n\tdput(dentry);\n\tif (IS_ERR(path))\n\t\treturn PTR_ERR(path);\n\t*ppath = path;\n\t*pfreepath = true;\n\treturn 0;\n}\n\n \nstatic int set_request_path_attr(struct inode *rinode, struct dentry *rdentry,\n\t\t\t\t  struct inode *rdiri, const char *rpath,\n\t\t\t\t  u64 rino, const char **ppath, int *pathlen,\n\t\t\t\t  u64 *ino, bool *freepath, bool parent_locked)\n{\n\tint r = 0;\n\n\tif (rinode) {\n\t\tr = build_inode_path(rinode, ppath, pathlen, ino, freepath);\n\t\tdout(\" inode %p %llx.%llx\\n\", rinode, ceph_ino(rinode),\n\t\t     ceph_snap(rinode));\n\t} else if (rdentry) {\n\t\tr = build_dentry_path(rdentry, rdiri, ppath, pathlen, ino,\n\t\t\t\t\tfreepath, parent_locked);\n\t\tdout(\" dentry %p %llx/%.*s\\n\", rdentry, *ino, *pathlen,\n\t\t     *ppath);\n\t} else if (rpath || rino) {\n\t\t*ino = rino;\n\t\t*ppath = rpath;\n\t\t*pathlen = rpath ? strlen(rpath) : 0;\n\t\tdout(\" path %.*s\\n\", *pathlen, rpath);\n\t}\n\n\treturn r;\n}\n\nstatic void encode_mclientrequest_tail(void **p,\n\t\t\t\t       const struct ceph_mds_request *req)\n{\n\tstruct ceph_timespec ts;\n\tint i;\n\n\tceph_encode_timespec64(&ts, &req->r_stamp);\n\tceph_encode_copy(p, &ts, sizeof(ts));\n\n\t \n\tceph_encode_32(p, req->r_cred->group_info->ngroups);\n\tfor (i = 0; i < req->r_cred->group_info->ngroups; i++)\n\t\tceph_encode_64(p, from_kgid(&init_user_ns,\n\t\t\t\t\t    req->r_cred->group_info->gid[i]));\n\n\t \n\tceph_encode_32(p, req->r_altname_len);\n\tceph_encode_copy(p, req->r_altname, req->r_altname_len);\n\n\t \n\tif (req->r_fscrypt_auth) {\n\t\tu32 authlen = ceph_fscrypt_auth_len(req->r_fscrypt_auth);\n\n\t\tceph_encode_32(p, authlen);\n\t\tceph_encode_copy(p, req->r_fscrypt_auth, authlen);\n\t} else {\n\t\tceph_encode_32(p, 0);\n\t}\n\tif (test_bit(CEPH_MDS_R_FSCRYPT_FILE, &req->r_req_flags)) {\n\t\tceph_encode_32(p, sizeof(__le64));\n\t\tceph_encode_64(p, req->r_fscrypt_file);\n\t} else {\n\t\tceph_encode_32(p, 0);\n\t}\n}\n\nstatic struct ceph_mds_request_head_legacy *\nfind_legacy_request_head(void *p, u64 features)\n{\n\tbool legacy = !(features & CEPH_FEATURE_FS_BTIME);\n\tstruct ceph_mds_request_head_old *ohead;\n\n\tif (legacy)\n\t\treturn (struct ceph_mds_request_head_legacy *)p;\n\tohead = (struct ceph_mds_request_head_old *)p;\n\treturn (struct ceph_mds_request_head_legacy *)&ohead->oldest_client_tid;\n}\n\n \nstatic struct ceph_msg *create_request_message(struct ceph_mds_session *session,\n\t\t\t\t\t       struct ceph_mds_request *req,\n\t\t\t\t\t       bool drop_cap_releases)\n{\n\tint mds = session->s_mds;\n\tstruct ceph_mds_client *mdsc = session->s_mdsc;\n\tstruct ceph_msg *msg;\n\tstruct ceph_mds_request_head_legacy *lhead;\n\tconst char *path1 = NULL;\n\tconst char *path2 = NULL;\n\tu64 ino1 = 0, ino2 = 0;\n\tint pathlen1 = 0, pathlen2 = 0;\n\tbool freepath1 = false, freepath2 = false;\n\tstruct dentry *old_dentry = NULL;\n\tint len;\n\tu16 releases;\n\tvoid *p, *end;\n\tint ret;\n\tbool legacy = !(session->s_con.peer_features & CEPH_FEATURE_FS_BTIME);\n\tbool old_version = !test_bit(CEPHFS_FEATURE_32BITS_RETRY_FWD,\n\t\t\t\t     &session->s_features);\n\n\tret = set_request_path_attr(req->r_inode, req->r_dentry,\n\t\t\t      req->r_parent, req->r_path1, req->r_ino1.ino,\n\t\t\t      &path1, &pathlen1, &ino1, &freepath1,\n\t\t\t      test_bit(CEPH_MDS_R_PARENT_LOCKED,\n\t\t\t\t\t&req->r_req_flags));\n\tif (ret < 0) {\n\t\tmsg = ERR_PTR(ret);\n\t\tgoto out;\n\t}\n\n\t \n\tif (req->r_old_dentry &&\n\t    !(req->r_old_dentry->d_flags & DCACHE_DISCONNECTED))\n\t\told_dentry = req->r_old_dentry;\n\tret = set_request_path_attr(NULL, old_dentry,\n\t\t\t      req->r_old_dentry_dir,\n\t\t\t      req->r_path2, req->r_ino2.ino,\n\t\t\t      &path2, &pathlen2, &ino2, &freepath2, true);\n\tif (ret < 0) {\n\t\tmsg = ERR_PTR(ret);\n\t\tgoto out_free1;\n\t}\n\n\treq->r_altname = get_fscrypt_altname(req, &req->r_altname_len);\n\tif (IS_ERR(req->r_altname)) {\n\t\tmsg = ERR_CAST(req->r_altname);\n\t\treq->r_altname = NULL;\n\t\tgoto out_free2;\n\t}\n\n\t \n\tif (legacy)\n\t\tlen = sizeof(struct ceph_mds_request_head_legacy);\n\telse if (old_version)\n\t\tlen = sizeof(struct ceph_mds_request_head_old);\n\telse\n\t\tlen = sizeof(struct ceph_mds_request_head);\n\n\t \n\tlen += 2 * (1 + sizeof(u32) + sizeof(u64));\n\tlen += pathlen1 + pathlen2;\n\n\t \n\tlen += sizeof(struct ceph_mds_request_release) *\n\t\t(!!req->r_inode_drop + !!req->r_dentry_drop +\n\t\t !!req->r_old_inode_drop + !!req->r_old_dentry_drop);\n\n\tif (req->r_dentry_drop)\n\t\tlen += pathlen1;\n\tif (req->r_old_dentry_drop)\n\t\tlen += pathlen2;\n\n\t \n\n\t \n\tlen += sizeof(struct ceph_timespec);\n\n\t \n\tlen += sizeof(u32) + (sizeof(u64) * req->r_cred->group_info->ngroups);\n\n\t \n\tlen += sizeof(u32) + req->r_altname_len;\n\n\t \n\tlen += sizeof(u32); \n\tif (req->r_fscrypt_auth)\n\t\tlen += ceph_fscrypt_auth_len(req->r_fscrypt_auth);\n\n\t \n\tlen += sizeof(u32);\n\tif (test_bit(CEPH_MDS_R_FSCRYPT_FILE, &req->r_req_flags))\n\t\tlen += sizeof(__le64);\n\n\tmsg = ceph_msg_new2(CEPH_MSG_CLIENT_REQUEST, len, 1, GFP_NOFS, false);\n\tif (!msg) {\n\t\tmsg = ERR_PTR(-ENOMEM);\n\t\tgoto out_free2;\n\t}\n\n\tmsg->hdr.tid = cpu_to_le64(req->r_tid);\n\n\tlhead = find_legacy_request_head(msg->front.iov_base,\n\t\t\t\t\t session->s_con.peer_features);\n\n\t \n\tif (legacy) {\n\t\tmsg->hdr.version = cpu_to_le16(3);\n\t\tp = msg->front.iov_base + sizeof(*lhead);\n\t} else if (old_version) {\n\t\tstruct ceph_mds_request_head_old *ohead = msg->front.iov_base;\n\n\t\tmsg->hdr.version = cpu_to_le16(4);\n\t\tohead->version = cpu_to_le16(1);\n\t\tp = msg->front.iov_base + sizeof(*ohead);\n\t} else {\n\t\tstruct ceph_mds_request_head *nhead = msg->front.iov_base;\n\n\t\tmsg->hdr.version = cpu_to_le16(6);\n\t\tnhead->version = cpu_to_le16(CEPH_MDS_REQUEST_HEAD_VERSION);\n\t\tp = msg->front.iov_base + sizeof(*nhead);\n\t}\n\n\tend = msg->front.iov_base + msg->front.iov_len;\n\n\tlhead->mdsmap_epoch = cpu_to_le32(mdsc->mdsmap->m_epoch);\n\tlhead->op = cpu_to_le32(req->r_op);\n\tlhead->caller_uid = cpu_to_le32(from_kuid(&init_user_ns,\n\t\t\t\t\t\t  req->r_cred->fsuid));\n\tlhead->caller_gid = cpu_to_le32(from_kgid(&init_user_ns,\n\t\t\t\t\t\t  req->r_cred->fsgid));\n\tlhead->ino = cpu_to_le64(req->r_deleg_ino);\n\tlhead->args = req->r_args;\n\n\tceph_encode_filepath(&p, end, ino1, path1);\n\tceph_encode_filepath(&p, end, ino2, path2);\n\n\t \n\treq->r_request_release_offset = p - msg->front.iov_base;\n\n\t \n\treleases = 0;\n\tif (req->r_inode_drop)\n\t\treleases += ceph_encode_inode_release(&p,\n\t\t      req->r_inode ? req->r_inode : d_inode(req->r_dentry),\n\t\t      mds, req->r_inode_drop, req->r_inode_unless,\n\t\t      req->r_op == CEPH_MDS_OP_READDIR);\n\tif (req->r_dentry_drop) {\n\t\tret = ceph_encode_dentry_release(&p, req->r_dentry,\n\t\t\t\treq->r_parent, mds, req->r_dentry_drop,\n\t\t\t\treq->r_dentry_unless);\n\t\tif (ret < 0)\n\t\t\tgoto out_err;\n\t\treleases += ret;\n\t}\n\tif (req->r_old_dentry_drop) {\n\t\tret = ceph_encode_dentry_release(&p, req->r_old_dentry,\n\t\t\t\treq->r_old_dentry_dir, mds,\n\t\t\t\treq->r_old_dentry_drop,\n\t\t\t\treq->r_old_dentry_unless);\n\t\tif (ret < 0)\n\t\t\tgoto out_err;\n\t\treleases += ret;\n\t}\n\tif (req->r_old_inode_drop)\n\t\treleases += ceph_encode_inode_release(&p,\n\t\t      d_inode(req->r_old_dentry),\n\t\t      mds, req->r_old_inode_drop, req->r_old_inode_unless, 0);\n\n\tif (drop_cap_releases) {\n\t\treleases = 0;\n\t\tp = msg->front.iov_base + req->r_request_release_offset;\n\t}\n\n\tlhead->num_releases = cpu_to_le16(releases);\n\n\tencode_mclientrequest_tail(&p, req);\n\n\tif (WARN_ON_ONCE(p > end)) {\n\t\tceph_msg_put(msg);\n\t\tmsg = ERR_PTR(-ERANGE);\n\t\tgoto out_free2;\n\t}\n\n\tmsg->front.iov_len = p - msg->front.iov_base;\n\tmsg->hdr.front_len = cpu_to_le32(msg->front.iov_len);\n\n\tif (req->r_pagelist) {\n\t\tstruct ceph_pagelist *pagelist = req->r_pagelist;\n\t\tceph_msg_data_add_pagelist(msg, pagelist);\n\t\tmsg->hdr.data_len = cpu_to_le32(pagelist->length);\n\t} else {\n\t\tmsg->hdr.data_len = 0;\n\t}\n\n\tmsg->hdr.data_off = cpu_to_le16(0);\n\nout_free2:\n\tif (freepath2)\n\t\tceph_mdsc_free_path((char *)path2, pathlen2);\nout_free1:\n\tif (freepath1)\n\t\tceph_mdsc_free_path((char *)path1, pathlen1);\nout:\n\treturn msg;\nout_err:\n\tceph_msg_put(msg);\n\tmsg = ERR_PTR(ret);\n\tgoto out_free2;\n}\n\n \nstatic void complete_request(struct ceph_mds_client *mdsc,\n\t\t\t     struct ceph_mds_request *req)\n{\n\treq->r_end_latency = ktime_get();\n\n\tif (req->r_callback)\n\t\treq->r_callback(mdsc, req);\n\tcomplete_all(&req->r_completion);\n}\n\n \nstatic int __prepare_send_request(struct ceph_mds_session *session,\n\t\t\t\t  struct ceph_mds_request *req,\n\t\t\t\t  bool drop_cap_releases)\n{\n\tint mds = session->s_mds;\n\tstruct ceph_mds_client *mdsc = session->s_mdsc;\n\tstruct ceph_mds_request_head_legacy *lhead;\n\tstruct ceph_mds_request_head *nhead;\n\tstruct ceph_msg *msg;\n\tint flags = 0, old_max_retry;\n\tbool old_version = !test_bit(CEPHFS_FEATURE_32BITS_RETRY_FWD,\n\t\t\t\t     &session->s_features);\n\n\t \n\tif (req->r_attempts) {\n\t       old_max_retry = sizeof_field(struct ceph_mds_request_head_old,\n\t\t\t\t\t    num_retry);\n\t       old_max_retry = 1 << (old_max_retry * BITS_PER_BYTE);\n\t       if ((old_version && req->r_attempts >= old_max_retry) ||\n\t\t   ((uint32_t)req->r_attempts >= U32_MAX)) {\n\t\t\tpr_warn_ratelimited(\"%s request tid %llu seq overflow\\n\",\n\t\t\t\t\t    __func__, req->r_tid);\n\t\t\treturn -EMULTIHOP;\n\t       }\n\t}\n\n\treq->r_attempts++;\n\tif (req->r_inode) {\n\t\tstruct ceph_cap *cap =\n\t\t\tceph_get_cap_for_mds(ceph_inode(req->r_inode), mds);\n\n\t\tif (cap)\n\t\t\treq->r_sent_on_mseq = cap->mseq;\n\t\telse\n\t\t\treq->r_sent_on_mseq = -1;\n\t}\n\tdout(\"%s %p tid %lld %s (attempt %d)\\n\", __func__, req,\n\t     req->r_tid, ceph_mds_op_name(req->r_op), req->r_attempts);\n\n\tif (test_bit(CEPH_MDS_R_GOT_UNSAFE, &req->r_req_flags)) {\n\t\tvoid *p;\n\n\t\t \n\t\tmsg = req->r_request;\n\t\tlhead = find_legacy_request_head(msg->front.iov_base,\n\t\t\t\t\t\t session->s_con.peer_features);\n\n\t\tflags = le32_to_cpu(lhead->flags);\n\t\tflags |= CEPH_MDS_FLAG_REPLAY;\n\t\tlhead->flags = cpu_to_le32(flags);\n\n\t\tif (req->r_target_inode)\n\t\t\tlhead->ino = cpu_to_le64(ceph_ino(req->r_target_inode));\n\n\t\tlhead->num_retry = req->r_attempts - 1;\n\t\tif (!old_version) {\n\t\t\tnhead = (struct ceph_mds_request_head*)msg->front.iov_base;\n\t\t\tnhead->ext_num_retry = cpu_to_le32(req->r_attempts - 1);\n\t\t}\n\n\t\t \n\t\tlhead->num_releases = 0;\n\n\t\tp = msg->front.iov_base + req->r_request_release_offset;\n\t\tencode_mclientrequest_tail(&p, req);\n\n\t\tmsg->front.iov_len = p - msg->front.iov_base;\n\t\tmsg->hdr.front_len = cpu_to_le32(msg->front.iov_len);\n\t\treturn 0;\n\t}\n\n\tif (req->r_request) {\n\t\tceph_msg_put(req->r_request);\n\t\treq->r_request = NULL;\n\t}\n\tmsg = create_request_message(session, req, drop_cap_releases);\n\tif (IS_ERR(msg)) {\n\t\treq->r_err = PTR_ERR(msg);\n\t\treturn PTR_ERR(msg);\n\t}\n\treq->r_request = msg;\n\n\tlhead = find_legacy_request_head(msg->front.iov_base,\n\t\t\t\t\t session->s_con.peer_features);\n\tlhead->oldest_client_tid = cpu_to_le64(__get_oldest_tid(mdsc));\n\tif (test_bit(CEPH_MDS_R_GOT_UNSAFE, &req->r_req_flags))\n\t\tflags |= CEPH_MDS_FLAG_REPLAY;\n\tif (test_bit(CEPH_MDS_R_ASYNC, &req->r_req_flags))\n\t\tflags |= CEPH_MDS_FLAG_ASYNC;\n\tif (req->r_parent)\n\t\tflags |= CEPH_MDS_FLAG_WANT_DENTRY;\n\tlhead->flags = cpu_to_le32(flags);\n\tlhead->num_fwd = req->r_num_fwd;\n\tlhead->num_retry = req->r_attempts - 1;\n\tif (!old_version) {\n\t\tnhead = (struct ceph_mds_request_head*)msg->front.iov_base;\n\t\tnhead->ext_num_fwd = cpu_to_le32(req->r_num_fwd);\n\t\tnhead->ext_num_retry = cpu_to_le32(req->r_attempts - 1);\n\t}\n\n\tdout(\" r_parent = %p\\n\", req->r_parent);\n\treturn 0;\n}\n\n \nstatic int __send_request(struct ceph_mds_session *session,\n\t\t\t  struct ceph_mds_request *req,\n\t\t\t  bool drop_cap_releases)\n{\n\tint err;\n\n\terr = __prepare_send_request(session, req, drop_cap_releases);\n\tif (!err) {\n\t\tceph_msg_get(req->r_request);\n\t\tceph_con_send(&session->s_con, req->r_request);\n\t}\n\n\treturn err;\n}\n\n \nstatic void __do_request(struct ceph_mds_client *mdsc,\n\t\t\tstruct ceph_mds_request *req)\n{\n\tstruct ceph_mds_session *session = NULL;\n\tint mds = -1;\n\tint err = 0;\n\tbool random;\n\n\tif (req->r_err || test_bit(CEPH_MDS_R_GOT_RESULT, &req->r_req_flags)) {\n\t\tif (test_bit(CEPH_MDS_R_ABORTED, &req->r_req_flags))\n\t\t\t__unregister_request(mdsc, req);\n\t\treturn;\n\t}\n\n\tif (READ_ONCE(mdsc->fsc->mount_state) == CEPH_MOUNT_FENCE_IO) {\n\t\tdout(\"do_request metadata corrupted\\n\");\n\t\terr = -EIO;\n\t\tgoto finish;\n\t}\n\tif (req->r_timeout &&\n\t    time_after_eq(jiffies, req->r_started + req->r_timeout)) {\n\t\tdout(\"do_request timed out\\n\");\n\t\terr = -ETIMEDOUT;\n\t\tgoto finish;\n\t}\n\tif (READ_ONCE(mdsc->fsc->mount_state) == CEPH_MOUNT_SHUTDOWN) {\n\t\tdout(\"do_request forced umount\\n\");\n\t\terr = -EIO;\n\t\tgoto finish;\n\t}\n\tif (READ_ONCE(mdsc->fsc->mount_state) == CEPH_MOUNT_MOUNTING) {\n\t\tif (mdsc->mdsmap_err) {\n\t\t\terr = mdsc->mdsmap_err;\n\t\t\tdout(\"do_request mdsmap err %d\\n\", err);\n\t\t\tgoto finish;\n\t\t}\n\t\tif (mdsc->mdsmap->m_epoch == 0) {\n\t\t\tdout(\"do_request no mdsmap, waiting for map\\n\");\n\t\t\tlist_add(&req->r_wait, &mdsc->waiting_for_map);\n\t\t\treturn;\n\t\t}\n\t\tif (!(mdsc->fsc->mount_options->flags &\n\t\t      CEPH_MOUNT_OPT_MOUNTWAIT) &&\n\t\t    !ceph_mdsmap_is_cluster_available(mdsc->mdsmap)) {\n\t\t\terr = -EHOSTUNREACH;\n\t\t\tgoto finish;\n\t\t}\n\t}\n\n\tput_request_session(req);\n\n\tmds = __choose_mds(mdsc, req, &random);\n\tif (mds < 0 ||\n\t    ceph_mdsmap_get_state(mdsc->mdsmap, mds) < CEPH_MDS_STATE_ACTIVE) {\n\t\tif (test_bit(CEPH_MDS_R_ASYNC, &req->r_req_flags)) {\n\t\t\terr = -EJUKEBOX;\n\t\t\tgoto finish;\n\t\t}\n\t\tdout(\"do_request no mds or not active, waiting for map\\n\");\n\t\tlist_add(&req->r_wait, &mdsc->waiting_for_map);\n\t\treturn;\n\t}\n\n\t \n\tsession = __ceph_lookup_mds_session(mdsc, mds);\n\tif (!session) {\n\t\tsession = register_session(mdsc, mds);\n\t\tif (IS_ERR(session)) {\n\t\t\terr = PTR_ERR(session);\n\t\t\tgoto finish;\n\t\t}\n\t}\n\treq->r_session = ceph_get_mds_session(session);\n\n\tdout(\"do_request mds%d session %p state %s\\n\", mds, session,\n\t     ceph_session_state_name(session->s_state));\n\n\t \n\tif (req->r_feature_needed > 0 &&\n\t    !test_bit(req->r_feature_needed, &session->s_features)) {\n\t\terr = -EOPNOTSUPP;\n\t\tgoto out_session;\n\t}\n\n\tif (session->s_state != CEPH_MDS_SESSION_OPEN &&\n\t    session->s_state != CEPH_MDS_SESSION_HUNG) {\n\t\t \n\t\tif (test_bit(CEPH_MDS_R_ASYNC, &req->r_req_flags)) {\n\t\t\terr = -EJUKEBOX;\n\t\t\tgoto out_session;\n\t\t}\n\n\t\t \n\t\tif (session->s_state == CEPH_MDS_SESSION_REJECTED) {\n\t\t\tif (ceph_test_mount_opt(mdsc->fsc, CLEANRECOVER))\n\t\t\t\tlist_add(&req->r_wait, &mdsc->waiting_for_map);\n\t\t\telse\n\t\t\t\terr = -EACCES;\n\t\t\tgoto out_session;\n\t\t}\n\n\t\tif (session->s_state == CEPH_MDS_SESSION_NEW ||\n\t\t    session->s_state == CEPH_MDS_SESSION_CLOSING) {\n\t\t\terr = __open_session(mdsc, session);\n\t\t\tif (err)\n\t\t\t\tgoto out_session;\n\t\t\t \n\t\t\tif (random)\n\t\t\t\treq->r_resend_mds = mds;\n\t\t}\n\t\tlist_add(&req->r_wait, &session->s_waiting);\n\t\tgoto out_session;\n\t}\n\n\t \n\treq->r_resend_mds = -1;    \n\n\tif (req->r_request_started == 0)    \n\t\treq->r_request_started = jiffies;\n\n\t \n\tif (test_bit(CEPH_MDS_R_ASYNC, &req->r_req_flags) && req->r_num_fwd) {\n\t\tstruct ceph_dentry_info *di = ceph_dentry(req->r_dentry);\n\t\tstruct ceph_inode_info *ci;\n\t\tstruct ceph_cap *cap;\n\n\t\t \n\t\tif (!d_inode(req->r_dentry)) {\n\t\t\terr = wait_on_bit(&di->flags, CEPH_DENTRY_ASYNC_CREATE_BIT,\n\t\t\t\t\t  TASK_KILLABLE);\n\t\t\tif (err) {\n\t\t\t\tmutex_lock(&req->r_fill_mutex);\n\t\t\t\tset_bit(CEPH_MDS_R_ABORTED, &req->r_req_flags);\n\t\t\t\tmutex_unlock(&req->r_fill_mutex);\n\t\t\t\tgoto out_session;\n\t\t\t}\n\t\t}\n\n\t\tci = ceph_inode(d_inode(req->r_dentry));\n\n\t\tspin_lock(&ci->i_ceph_lock);\n\t\tcap = ci->i_auth_cap;\n\t\tif (ci->i_ceph_flags & CEPH_I_ASYNC_CREATE && mds != cap->mds) {\n\t\t\tdout(\"do_request session changed for auth cap %d -> %d\\n\",\n\t\t\t     cap->session->s_mds, session->s_mds);\n\n\t\t\t \n\t\t\tspin_lock(&cap->session->s_cap_lock);\n\t\t\tcap->session->s_nr_caps--;\n\t\t\tlist_del_init(&cap->session_caps);\n\t\t\tspin_unlock(&cap->session->s_cap_lock);\n\n\t\t\t \n\t\t\tcap->mds = mds;\n\t\t\tcap->session = session;\n\t\t\tspin_lock(&session->s_cap_lock);\n\t\t\tsession->s_nr_caps++;\n\t\t\tlist_add_tail(&cap->session_caps, &session->s_caps);\n\t\t\tspin_unlock(&session->s_cap_lock);\n\n\t\t\tchange_auth_cap_ses(ci, session);\n\t\t}\n\t\tspin_unlock(&ci->i_ceph_lock);\n\t}\n\n\terr = __send_request(session, req, false);\n\nout_session:\n\tceph_put_mds_session(session);\nfinish:\n\tif (err) {\n\t\tdout(\"__do_request early error %d\\n\", err);\n\t\treq->r_err = err;\n\t\tcomplete_request(mdsc, req);\n\t\t__unregister_request(mdsc, req);\n\t}\n\treturn;\n}\n\n \nstatic void __wake_requests(struct ceph_mds_client *mdsc,\n\t\t\t    struct list_head *head)\n{\n\tstruct ceph_mds_request *req;\n\tLIST_HEAD(tmp_list);\n\n\tlist_splice_init(head, &tmp_list);\n\n\twhile (!list_empty(&tmp_list)) {\n\t\treq = list_entry(tmp_list.next,\n\t\t\t\t struct ceph_mds_request, r_wait);\n\t\tlist_del_init(&req->r_wait);\n\t\tdout(\" wake request %p tid %llu\\n\", req, req->r_tid);\n\t\t__do_request(mdsc, req);\n\t}\n}\n\n \nstatic void kick_requests(struct ceph_mds_client *mdsc, int mds)\n{\n\tstruct ceph_mds_request *req;\n\tstruct rb_node *p = rb_first(&mdsc->request_tree);\n\n\tdout(\"kick_requests mds%d\\n\", mds);\n\twhile (p) {\n\t\treq = rb_entry(p, struct ceph_mds_request, r_node);\n\t\tp = rb_next(p);\n\t\tif (test_bit(CEPH_MDS_R_GOT_UNSAFE, &req->r_req_flags))\n\t\t\tcontinue;\n\t\tif (req->r_attempts > 0)\n\t\t\tcontinue;  \n\t\tif (req->r_session &&\n\t\t    req->r_session->s_mds == mds) {\n\t\t\tdout(\" kicking tid %llu\\n\", req->r_tid);\n\t\t\tlist_del_init(&req->r_wait);\n\t\t\t__do_request(mdsc, req);\n\t\t}\n\t}\n}\n\nint ceph_mdsc_submit_request(struct ceph_mds_client *mdsc, struct inode *dir,\n\t\t\t      struct ceph_mds_request *req)\n{\n\tint err = 0;\n\n\t \n\tif (req->r_inode)\n\t\tceph_get_cap_refs(ceph_inode(req->r_inode), CEPH_CAP_PIN);\n\tif (req->r_parent) {\n\t\tstruct ceph_inode_info *ci = ceph_inode(req->r_parent);\n\t\tint fmode = (req->r_op & CEPH_MDS_OP_WRITE) ?\n\t\t\t    CEPH_FILE_MODE_WR : CEPH_FILE_MODE_RD;\n\t\tspin_lock(&ci->i_ceph_lock);\n\t\tceph_take_cap_refs(ci, CEPH_CAP_PIN, false);\n\t\t__ceph_touch_fmode(ci, mdsc, fmode);\n\t\tspin_unlock(&ci->i_ceph_lock);\n\t}\n\tif (req->r_old_dentry_dir)\n\t\tceph_get_cap_refs(ceph_inode(req->r_old_dentry_dir),\n\t\t\t\t  CEPH_CAP_PIN);\n\n\tif (req->r_inode) {\n\t\terr = ceph_wait_on_async_create(req->r_inode);\n\t\tif (err) {\n\t\t\tdout(\"%s: wait for async create returned: %d\\n\",\n\t\t\t     __func__, err);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tif (!err && req->r_old_inode) {\n\t\terr = ceph_wait_on_async_create(req->r_old_inode);\n\t\tif (err) {\n\t\t\tdout(\"%s: wait for async create returned: %d\\n\",\n\t\t\t     __func__, err);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\tdout(\"submit_request on %p for inode %p\\n\", req, dir);\n\tmutex_lock(&mdsc->mutex);\n\t__register_request(mdsc, req, dir);\n\t__do_request(mdsc, req);\n\terr = req->r_err;\n\tmutex_unlock(&mdsc->mutex);\n\treturn err;\n}\n\nint ceph_mdsc_wait_request(struct ceph_mds_client *mdsc,\n\t\t\t   struct ceph_mds_request *req,\n\t\t\t   ceph_mds_request_wait_callback_t wait_func)\n{\n\tint err;\n\n\t \n\tdout(\"do_request waiting\\n\");\n\tif (wait_func) {\n\t\terr = wait_func(mdsc, req);\n\t} else {\n\t\tlong timeleft = wait_for_completion_killable_timeout(\n\t\t\t\t\t&req->r_completion,\n\t\t\t\t\tceph_timeout_jiffies(req->r_timeout));\n\t\tif (timeleft > 0)\n\t\t\terr = 0;\n\t\telse if (!timeleft)\n\t\t\terr = -ETIMEDOUT;   \n\t\telse\n\t\t\terr = timeleft;   \n\t}\n\tdout(\"do_request waited, got %d\\n\", err);\n\tmutex_lock(&mdsc->mutex);\n\n\t \n\tif (test_bit(CEPH_MDS_R_GOT_RESULT, &req->r_req_flags)) {\n\t\terr = le32_to_cpu(req->r_reply_info.head->result);\n\t} else if (err < 0) {\n\t\tdout(\"aborted request %lld with %d\\n\", req->r_tid, err);\n\n\t\t \n\t\tmutex_lock(&req->r_fill_mutex);\n\t\treq->r_err = err;\n\t\tset_bit(CEPH_MDS_R_ABORTED, &req->r_req_flags);\n\t\tmutex_unlock(&req->r_fill_mutex);\n\n\t\tif (req->r_parent &&\n\t\t    (req->r_op & CEPH_MDS_OP_WRITE))\n\t\t\tceph_invalidate_dir_request(req);\n\t} else {\n\t\terr = req->r_err;\n\t}\n\n\tmutex_unlock(&mdsc->mutex);\n\treturn err;\n}\n\n \nint ceph_mdsc_do_request(struct ceph_mds_client *mdsc,\n\t\t\t struct inode *dir,\n\t\t\t struct ceph_mds_request *req)\n{\n\tint err;\n\n\tdout(\"do_request on %p\\n\", req);\n\n\t \n\terr = ceph_mdsc_submit_request(mdsc, dir, req);\n\tif (!err)\n\t\terr = ceph_mdsc_wait_request(mdsc, req, NULL);\n\tdout(\"do_request %p done, result %d\\n\", req, err);\n\treturn err;\n}\n\n \nvoid ceph_invalidate_dir_request(struct ceph_mds_request *req)\n{\n\tstruct inode *dir = req->r_parent;\n\tstruct inode *old_dir = req->r_old_dentry_dir;\n\n\tdout(\"invalidate_dir_request %p %p (complete, lease(s))\\n\", dir, old_dir);\n\n\tceph_dir_clear_complete(dir);\n\tif (old_dir)\n\t\tceph_dir_clear_complete(old_dir);\n\tif (req->r_dentry)\n\t\tceph_invalidate_dentry_lease(req->r_dentry);\n\tif (req->r_old_dentry)\n\t\tceph_invalidate_dentry_lease(req->r_old_dentry);\n}\n\n \nstatic void handle_reply(struct ceph_mds_session *session, struct ceph_msg *msg)\n{\n\tstruct ceph_mds_client *mdsc = session->s_mdsc;\n\tstruct ceph_mds_request *req;\n\tstruct ceph_mds_reply_head *head = msg->front.iov_base;\n\tstruct ceph_mds_reply_info_parsed *rinfo;   \n\tstruct ceph_snap_realm *realm;\n\tu64 tid;\n\tint err, result;\n\tint mds = session->s_mds;\n\tbool close_sessions = false;\n\n\tif (msg->front.iov_len < sizeof(*head)) {\n\t\tpr_err(\"mdsc_handle_reply got corrupt (short) reply\\n\");\n\t\tceph_msg_dump(msg);\n\t\treturn;\n\t}\n\n\t \n\ttid = le64_to_cpu(msg->hdr.tid);\n\tmutex_lock(&mdsc->mutex);\n\treq = lookup_get_request(mdsc, tid);\n\tif (!req) {\n\t\tdout(\"handle_reply on unknown tid %llu\\n\", tid);\n\t\tmutex_unlock(&mdsc->mutex);\n\t\treturn;\n\t}\n\tdout(\"handle_reply %p\\n\", req);\n\n\t \n\tif (req->r_session != session) {\n\t\tpr_err(\"mdsc_handle_reply got %llu on session mds%d\"\n\t\t       \" not mds%d\\n\", tid, session->s_mds,\n\t\t       req->r_session ? req->r_session->s_mds : -1);\n\t\tmutex_unlock(&mdsc->mutex);\n\t\tgoto out;\n\t}\n\n\t \n\tif ((test_bit(CEPH_MDS_R_GOT_UNSAFE, &req->r_req_flags) && !head->safe) ||\n\t    (test_bit(CEPH_MDS_R_GOT_SAFE, &req->r_req_flags) && head->safe)) {\n\t\tpr_warn(\"got a dup %s reply on %llu from mds%d\\n\",\n\t\t\t   head->safe ? \"safe\" : \"unsafe\", tid, mds);\n\t\tmutex_unlock(&mdsc->mutex);\n\t\tgoto out;\n\t}\n\tif (test_bit(CEPH_MDS_R_GOT_SAFE, &req->r_req_flags)) {\n\t\tpr_warn(\"got unsafe after safe on %llu from mds%d\\n\",\n\t\t\t   tid, mds);\n\t\tmutex_unlock(&mdsc->mutex);\n\t\tgoto out;\n\t}\n\n\tresult = le32_to_cpu(head->result);\n\n\tif (head->safe) {\n\t\tset_bit(CEPH_MDS_R_GOT_SAFE, &req->r_req_flags);\n\t\t__unregister_request(mdsc, req);\n\n\t\t \n\t\tif (mdsc->stopping && !__get_oldest_req(mdsc))\n\t\t\tcomplete_all(&mdsc->safe_umount_waiters);\n\n\t\tif (test_bit(CEPH_MDS_R_GOT_UNSAFE, &req->r_req_flags)) {\n\t\t\t \n\t\t\tdout(\"got safe reply %llu, mds%d\\n\", tid, mds);\n\n\t\t\tmutex_unlock(&mdsc->mutex);\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\tset_bit(CEPH_MDS_R_GOT_UNSAFE, &req->r_req_flags);\n\t\tlist_add_tail(&req->r_unsafe_item, &req->r_session->s_unsafe);\n\t}\n\n\tdout(\"handle_reply tid %lld result %d\\n\", tid, result);\n\tif (test_bit(CEPHFS_FEATURE_REPLY_ENCODING, &session->s_features))\n\t\terr = parse_reply_info(session, msg, req, (u64)-1);\n\telse\n\t\terr = parse_reply_info(session, msg, req,\n\t\t\t\t       session->s_con.peer_features);\n\tmutex_unlock(&mdsc->mutex);\n\n\t \n\trinfo = &req->r_reply_info;\n\tif ((err >= 0) && rinfo->head->is_target) {\n\t\tstruct inode *in = xchg(&req->r_new_inode, NULL);\n\t\tstruct ceph_vino tvino = {\n\t\t\t.ino  = le64_to_cpu(rinfo->targeti.in->ino),\n\t\t\t.snap = le64_to_cpu(rinfo->targeti.in->snapid)\n\t\t};\n\n\t\t \n\t\tif (req->r_op == CEPH_MDS_OP_CREATE &&\n\t\t    !req->r_reply_info.has_create_ino) {\n\t\t\t \n\t\t\tWARN_ON_ONCE(req->r_deleg_ino);\n\t\t\tiput(in);\n\t\t\tin = NULL;\n\t\t}\n\n\t\tin = ceph_get_inode(mdsc->fsc->sb, tvino, in);\n\t\tif (IS_ERR(in)) {\n\t\t\terr = PTR_ERR(in);\n\t\t\tmutex_lock(&session->s_mutex);\n\t\t\tgoto out_err;\n\t\t}\n\t\treq->r_target_inode = in;\n\t}\n\n\tmutex_lock(&session->s_mutex);\n\tif (err < 0) {\n\t\tpr_err(\"mdsc_handle_reply got corrupt reply mds%d(tid:%lld)\\n\", mds, tid);\n\t\tceph_msg_dump(msg);\n\t\tgoto out_err;\n\t}\n\n\t \n\trealm = NULL;\n\tif (rinfo->snapblob_len) {\n\t\tdown_write(&mdsc->snap_rwsem);\n\t\terr = ceph_update_snap_trace(mdsc, rinfo->snapblob,\n\t\t\t\trinfo->snapblob + rinfo->snapblob_len,\n\t\t\t\tle32_to_cpu(head->op) == CEPH_MDS_OP_RMSNAP,\n\t\t\t\t&realm);\n\t\tif (err) {\n\t\t\tup_write(&mdsc->snap_rwsem);\n\t\t\tclose_sessions = true;\n\t\t\tif (err == -EIO)\n\t\t\t\tceph_msg_dump(msg);\n\t\t\tgoto out_err;\n\t\t}\n\t\tdowngrade_write(&mdsc->snap_rwsem);\n\t} else {\n\t\tdown_read(&mdsc->snap_rwsem);\n\t}\n\n\t \n\tmutex_lock(&req->r_fill_mutex);\n\tcurrent->journal_info = req;\n\terr = ceph_fill_trace(mdsc->fsc->sb, req);\n\tif (err == 0) {\n\t\tif (result == 0 && (req->r_op == CEPH_MDS_OP_READDIR ||\n\t\t\t\t    req->r_op == CEPH_MDS_OP_LSSNAP))\n\t\t\terr = ceph_readdir_prepopulate(req, req->r_session);\n\t}\n\tcurrent->journal_info = NULL;\n\tmutex_unlock(&req->r_fill_mutex);\n\n\tup_read(&mdsc->snap_rwsem);\n\tif (realm)\n\t\tceph_put_snap_realm(mdsc, realm);\n\n\tif (err == 0) {\n\t\tif (req->r_target_inode &&\n\t\t    test_bit(CEPH_MDS_R_GOT_UNSAFE, &req->r_req_flags)) {\n\t\t\tstruct ceph_inode_info *ci =\n\t\t\t\tceph_inode(req->r_target_inode);\n\t\t\tspin_lock(&ci->i_unsafe_lock);\n\t\t\tlist_add_tail(&req->r_unsafe_target_item,\n\t\t\t\t      &ci->i_unsafe_iops);\n\t\t\tspin_unlock(&ci->i_unsafe_lock);\n\t\t}\n\n\t\tceph_unreserve_caps(mdsc, &req->r_caps_reservation);\n\t}\nout_err:\n\tmutex_lock(&mdsc->mutex);\n\tif (!test_bit(CEPH_MDS_R_ABORTED, &req->r_req_flags)) {\n\t\tif (err) {\n\t\t\treq->r_err = err;\n\t\t} else {\n\t\t\treq->r_reply =  ceph_msg_get(msg);\n\t\t\tset_bit(CEPH_MDS_R_GOT_RESULT, &req->r_req_flags);\n\t\t}\n\t} else {\n\t\tdout(\"reply arrived after request %lld was aborted\\n\", tid);\n\t}\n\tmutex_unlock(&mdsc->mutex);\n\n\tmutex_unlock(&session->s_mutex);\n\n\t \n\tcomplete_request(mdsc, req);\n\n\tceph_update_metadata_metrics(&mdsc->metric, req->r_start_latency,\n\t\t\t\t     req->r_end_latency, err);\nout:\n\tceph_mdsc_put_request(req);\n\n\t \n\tif (close_sessions)\n\t\tceph_mdsc_close_sessions(mdsc);\n\treturn;\n}\n\n\n\n \nstatic void handle_forward(struct ceph_mds_client *mdsc,\n\t\t\t   struct ceph_mds_session *session,\n\t\t\t   struct ceph_msg *msg)\n{\n\tstruct ceph_mds_request *req;\n\tu64 tid = le64_to_cpu(msg->hdr.tid);\n\tu32 next_mds;\n\tu32 fwd_seq;\n\tint err = -EINVAL;\n\tvoid *p = msg->front.iov_base;\n\tvoid *end = p + msg->front.iov_len;\n\tbool aborted = false;\n\n\tceph_decode_need(&p, end, 2*sizeof(u32), bad);\n\tnext_mds = ceph_decode_32(&p);\n\tfwd_seq = ceph_decode_32(&p);\n\n\tmutex_lock(&mdsc->mutex);\n\treq = lookup_get_request(mdsc, tid);\n\tif (!req) {\n\t\tmutex_unlock(&mdsc->mutex);\n\t\tdout(\"forward tid %llu to mds%d - req dne\\n\", tid, next_mds);\n\t\treturn;   \n\t}\n\n\tif (test_bit(CEPH_MDS_R_ABORTED, &req->r_req_flags)) {\n\t\tdout(\"forward tid %llu aborted, unregistering\\n\", tid);\n\t\t__unregister_request(mdsc, req);\n\t} else if (fwd_seq <= req->r_num_fwd || (uint32_t)fwd_seq >= U32_MAX) {\n\t\t \n\t\tmutex_lock(&req->r_fill_mutex);\n\t\treq->r_err = -EMULTIHOP;\n\t\tset_bit(CEPH_MDS_R_ABORTED, &req->r_req_flags);\n\t\tmutex_unlock(&req->r_fill_mutex);\n\t\taborted = true;\n\t\tpr_warn_ratelimited(\"forward tid %llu seq overflow\\n\", tid);\n\t} else {\n\t\t \n\t\tdout(\"forward tid %llu to mds%d (we resend)\\n\", tid, next_mds);\n\t\tBUG_ON(req->r_err);\n\t\tBUG_ON(test_bit(CEPH_MDS_R_GOT_RESULT, &req->r_req_flags));\n\t\treq->r_attempts = 0;\n\t\treq->r_num_fwd = fwd_seq;\n\t\treq->r_resend_mds = next_mds;\n\t\tput_request_session(req);\n\t\t__do_request(mdsc, req);\n\t}\n\tmutex_unlock(&mdsc->mutex);\n\n\t \n\tif (aborted)\n\t\tcomplete_request(mdsc, req);\n\tceph_mdsc_put_request(req);\n\treturn;\n\nbad:\n\tpr_err(\"mdsc_handle_forward decode error err=%d\\n\", err);\n\tceph_msg_dump(msg);\n}\n\nstatic int __decode_session_metadata(void **p, void *end,\n\t\t\t\t     bool *blocklisted)\n{\n\t \n\tu32 n;\n\tbool err_str;\n\tceph_decode_32_safe(p, end, n, bad);\n\twhile (n-- > 0) {\n\t\tu32 len;\n\t\tceph_decode_32_safe(p, end, len, bad);\n\t\tceph_decode_need(p, end, len, bad);\n\t\terr_str = !strncmp(*p, \"error_string\", len);\n\t\t*p += len;\n\t\tceph_decode_32_safe(p, end, len, bad);\n\t\tceph_decode_need(p, end, len, bad);\n\t\t \n\t\tif (err_str && strnstr(*p, \"blacklisted\", len))\n\t\t\t*blocklisted = true;\n\t\t*p += len;\n\t}\n\treturn 0;\nbad:\n\treturn -1;\n}\n\n \nstatic void handle_session(struct ceph_mds_session *session,\n\t\t\t   struct ceph_msg *msg)\n{\n\tstruct ceph_mds_client *mdsc = session->s_mdsc;\n\tint mds = session->s_mds;\n\tint msg_version = le16_to_cpu(msg->hdr.version);\n\tvoid *p = msg->front.iov_base;\n\tvoid *end = p + msg->front.iov_len;\n\tstruct ceph_mds_session_head *h;\n\tu32 op;\n\tu64 seq, features = 0;\n\tint wake = 0;\n\tbool blocklisted = false;\n\n\t \n\tceph_decode_need(&p, end, sizeof(*h), bad);\n\th = p;\n\tp += sizeof(*h);\n\n\top = le32_to_cpu(h->op);\n\tseq = le64_to_cpu(h->seq);\n\n\tif (msg_version >= 3) {\n\t\tu32 len;\n\t\t \n\t\tif (msg_version >= 5)\n\t\t\tceph_decode_skip_map(&p, end, string, string, bad);\n\t\telse if (__decode_session_metadata(&p, end, &blocklisted) < 0)\n\t\t\tgoto bad;\n\n\t\t \n\t\tceph_decode_32_safe(&p, end, len, bad);\n\t\tif (len) {\n\t\t\tceph_decode_64_safe(&p, end, features, bad);\n\t\t\tp += len - sizeof(features);\n\t\t}\n\t}\n\n\tif (msg_version >= 5) {\n\t\tu32 flags, len;\n\n\t\t \n\t\tceph_decode_skip_16(&p, end, bad);  \n\t\tceph_decode_32_safe(&p, end, len, bad);  \n\t\tceph_decode_skip_n(&p, end, len, bad);  \n\n\t\t \n\t\tceph_decode_32_safe(&p, end, flags, bad);\n\t\tif (flags & CEPH_SESSION_BLOCKLISTED) {\n\t\t\tpr_warn(\"mds%d session blocklisted\\n\", session->s_mds);\n\t\t\tblocklisted = true;\n\t\t}\n\t}\n\n\tmutex_lock(&mdsc->mutex);\n\tif (op == CEPH_SESSION_CLOSE) {\n\t\tceph_get_mds_session(session);\n\t\t__unregister_session(mdsc, session);\n\t}\n\t \n\tsession->s_ttl = jiffies + HZ*mdsc->mdsmap->m_session_autoclose;\n\tmutex_unlock(&mdsc->mutex);\n\n\tmutex_lock(&session->s_mutex);\n\n\tdout(\"handle_session mds%d %s %p state %s seq %llu\\n\",\n\t     mds, ceph_session_op_name(op), session,\n\t     ceph_session_state_name(session->s_state), seq);\n\n\tif (session->s_state == CEPH_MDS_SESSION_HUNG) {\n\t\tsession->s_state = CEPH_MDS_SESSION_OPEN;\n\t\tpr_info(\"mds%d came back\\n\", session->s_mds);\n\t}\n\n\tswitch (op) {\n\tcase CEPH_SESSION_OPEN:\n\t\tif (session->s_state == CEPH_MDS_SESSION_RECONNECTING)\n\t\t\tpr_info(\"mds%d reconnect success\\n\", session->s_mds);\n\n\t\tif (session->s_state == CEPH_MDS_SESSION_OPEN) {\n\t\t\tpr_notice(\"mds%d is already opened\\n\", session->s_mds);\n\t\t} else {\n\t\t\tsession->s_state = CEPH_MDS_SESSION_OPEN;\n\t\t\tsession->s_features = features;\n\t\t\trenewed_caps(mdsc, session, 0);\n\t\t\tif (test_bit(CEPHFS_FEATURE_METRIC_COLLECT,\n\t\t\t\t     &session->s_features))\n\t\t\t\tmetric_schedule_delayed(&mdsc->metric);\n\t\t}\n\n\t\t \n\t\tif (!session->s_seq && seq)\n\t\t\tsession->s_seq = seq;\n\n\t\twake = 1;\n\t\tif (mdsc->stopping)\n\t\t\t__close_session(mdsc, session);\n\t\tbreak;\n\n\tcase CEPH_SESSION_RENEWCAPS:\n\t\tif (session->s_renew_seq == seq)\n\t\t\trenewed_caps(mdsc, session, 1);\n\t\tbreak;\n\n\tcase CEPH_SESSION_CLOSE:\n\t\tif (session->s_state == CEPH_MDS_SESSION_RECONNECTING)\n\t\t\tpr_info(\"mds%d reconnect denied\\n\", session->s_mds);\n\t\tsession->s_state = CEPH_MDS_SESSION_CLOSED;\n\t\tcleanup_session_requests(mdsc, session);\n\t\tremove_session_caps(session);\n\t\twake = 2;  \n\t\twake_up_all(&mdsc->session_close_wq);\n\t\tbreak;\n\n\tcase CEPH_SESSION_STALE:\n\t\tpr_info(\"mds%d caps went stale, renewing\\n\",\n\t\t\tsession->s_mds);\n\t\tatomic_inc(&session->s_cap_gen);\n\t\tsession->s_cap_ttl = jiffies - 1;\n\t\tsend_renew_caps(mdsc, session);\n\t\tbreak;\n\n\tcase CEPH_SESSION_RECALL_STATE:\n\t\tceph_trim_caps(mdsc, session, le32_to_cpu(h->max_caps));\n\t\tbreak;\n\n\tcase CEPH_SESSION_FLUSHMSG:\n\t\t \n\t\tspin_lock(&session->s_cap_lock);\n\t\tif (session->s_num_cap_releases)\n\t\t\tceph_flush_cap_releases(mdsc, session);\n\t\tspin_unlock(&session->s_cap_lock);\n\n\t\tsend_flushmsg_ack(mdsc, session, seq);\n\t\tbreak;\n\n\tcase CEPH_SESSION_FORCE_RO:\n\t\tdout(\"force_session_readonly %p\\n\", session);\n\t\tspin_lock(&session->s_cap_lock);\n\t\tsession->s_readonly = true;\n\t\tspin_unlock(&session->s_cap_lock);\n\t\twake_up_session_caps(session, FORCE_RO);\n\t\tbreak;\n\n\tcase CEPH_SESSION_REJECT:\n\t\tWARN_ON(session->s_state != CEPH_MDS_SESSION_OPENING);\n\t\tpr_info(\"mds%d rejected session\\n\", session->s_mds);\n\t\tsession->s_state = CEPH_MDS_SESSION_REJECTED;\n\t\tcleanup_session_requests(mdsc, session);\n\t\tremove_session_caps(session);\n\t\tif (blocklisted)\n\t\t\tmdsc->fsc->blocklisted = true;\n\t\twake = 2;  \n\t\tbreak;\n\n\tdefault:\n\t\tpr_err(\"mdsc_handle_session bad op %d mds%d\\n\", op, mds);\n\t\tWARN_ON(1);\n\t}\n\n\tmutex_unlock(&session->s_mutex);\n\tif (wake) {\n\t\tmutex_lock(&mdsc->mutex);\n\t\t__wake_requests(mdsc, &session->s_waiting);\n\t\tif (wake == 2)\n\t\t\tkick_requests(mdsc, mds);\n\t\tmutex_unlock(&mdsc->mutex);\n\t}\n\tif (op == CEPH_SESSION_CLOSE)\n\t\tceph_put_mds_session(session);\n\treturn;\n\nbad:\n\tpr_err(\"mdsc_handle_session corrupt message mds%d len %d\\n\", mds,\n\t       (int)msg->front.iov_len);\n\tceph_msg_dump(msg);\n\treturn;\n}\n\nvoid ceph_mdsc_release_dir_caps(struct ceph_mds_request *req)\n{\n\tint dcaps;\n\n\tdcaps = xchg(&req->r_dir_caps, 0);\n\tif (dcaps) {\n\t\tdout(\"releasing r_dir_caps=%s\\n\", ceph_cap_string(dcaps));\n\t\tceph_put_cap_refs(ceph_inode(req->r_parent), dcaps);\n\t}\n}\n\nvoid ceph_mdsc_release_dir_caps_no_check(struct ceph_mds_request *req)\n{\n\tint dcaps;\n\n\tdcaps = xchg(&req->r_dir_caps, 0);\n\tif (dcaps) {\n\t\tdout(\"releasing r_dir_caps=%s\\n\", ceph_cap_string(dcaps));\n\t\tceph_put_cap_refs_no_check_caps(ceph_inode(req->r_parent),\n\t\t\t\t\t\tdcaps);\n\t}\n}\n\n \nstatic void replay_unsafe_requests(struct ceph_mds_client *mdsc,\n\t\t\t\t   struct ceph_mds_session *session)\n{\n\tstruct ceph_mds_request *req, *nreq;\n\tstruct rb_node *p;\n\n\tdout(\"replay_unsafe_requests mds%d\\n\", session->s_mds);\n\n\tmutex_lock(&mdsc->mutex);\n\tlist_for_each_entry_safe(req, nreq, &session->s_unsafe, r_unsafe_item)\n\t\t__send_request(session, req, true);\n\n\t \n\tp = rb_first(&mdsc->request_tree);\n\twhile (p) {\n\t\treq = rb_entry(p, struct ceph_mds_request, r_node);\n\t\tp = rb_next(p);\n\t\tif (test_bit(CEPH_MDS_R_GOT_UNSAFE, &req->r_req_flags))\n\t\t\tcontinue;\n\t\tif (req->r_attempts == 0)\n\t\t\tcontinue;  \n\t\tif (!req->r_session)\n\t\t\tcontinue;\n\t\tif (req->r_session->s_mds != session->s_mds)\n\t\t\tcontinue;\n\n\t\tceph_mdsc_release_dir_caps_no_check(req);\n\n\t\t__send_request(session, req, true);\n\t}\n\tmutex_unlock(&mdsc->mutex);\n}\n\nstatic int send_reconnect_partial(struct ceph_reconnect_state *recon_state)\n{\n\tstruct ceph_msg *reply;\n\tstruct ceph_pagelist *_pagelist;\n\tstruct page *page;\n\t__le32 *addr;\n\tint err = -ENOMEM;\n\n\tif (!recon_state->allow_multi)\n\t\treturn -ENOSPC;\n\n\t \n\tBUG_ON(!recon_state->nr_caps == !recon_state->nr_realms);\n\n\t \n\t_pagelist = ceph_pagelist_alloc(GFP_NOFS);\n\tif (!_pagelist)\n\t\treturn -ENOMEM;\n\n\treply = ceph_msg_new2(CEPH_MSG_CLIENT_RECONNECT, 0, 1, GFP_NOFS, false);\n\tif (!reply)\n\t\tgoto fail_msg;\n\n\t \n\terr = ceph_pagelist_encode_32(_pagelist, 0);\n\tif (err < 0)\n\t\tgoto fail;\n\n\tif (recon_state->nr_caps) {\n\t\t \n\t\terr = ceph_pagelist_encode_32(recon_state->pagelist, 0);\n\t\tif (err)\n\t\t\tgoto fail;\n\t} else {\n\t\t \n\t\terr = ceph_pagelist_encode_32(_pagelist, 0);\n\t\tif (err < 0)\n\t\t\tgoto fail;\n\t}\n\n\terr = ceph_pagelist_encode_8(recon_state->pagelist, 1);\n\tif (err)\n\t\tgoto fail;\n\n\tpage = list_first_entry(&recon_state->pagelist->head, struct page, lru);\n\taddr = kmap_atomic(page);\n\tif (recon_state->nr_caps) {\n\t\t \n\t\t*addr = cpu_to_le32(recon_state->nr_caps);\n\t} else {\n\t\t \n\t\t*(addr + 1) = cpu_to_le32(recon_state->nr_realms);\n\t}\n\tkunmap_atomic(addr);\n\n\treply->hdr.version = cpu_to_le16(5);\n\treply->hdr.compat_version = cpu_to_le16(4);\n\n\treply->hdr.data_len = cpu_to_le32(recon_state->pagelist->length);\n\tceph_msg_data_add_pagelist(reply, recon_state->pagelist);\n\n\tceph_con_send(&recon_state->session->s_con, reply);\n\tceph_pagelist_release(recon_state->pagelist);\n\n\trecon_state->pagelist = _pagelist;\n\trecon_state->nr_caps = 0;\n\trecon_state->nr_realms = 0;\n\trecon_state->msg_version = 5;\n\treturn 0;\nfail:\n\tceph_msg_put(reply);\nfail_msg:\n\tceph_pagelist_release(_pagelist);\n\treturn err;\n}\n\nstatic struct dentry* d_find_primary(struct inode *inode)\n{\n\tstruct dentry *alias, *dn = NULL;\n\n\tif (hlist_empty(&inode->i_dentry))\n\t\treturn NULL;\n\n\tspin_lock(&inode->i_lock);\n\tif (hlist_empty(&inode->i_dentry))\n\t\tgoto out_unlock;\n\n\tif (S_ISDIR(inode->i_mode)) {\n\t\talias = hlist_entry(inode->i_dentry.first, struct dentry, d_u.d_alias);\n\t\tif (!IS_ROOT(alias))\n\t\t\tdn = dget(alias);\n\t\tgoto out_unlock;\n\t}\n\n\thlist_for_each_entry(alias, &inode->i_dentry, d_u.d_alias) {\n\t\tspin_lock(&alias->d_lock);\n\t\tif (!d_unhashed(alias) &&\n\t\t    (ceph_dentry(alias)->flags & CEPH_DENTRY_PRIMARY_LINK)) {\n\t\t\tdn = dget_dlock(alias);\n\t\t}\n\t\tspin_unlock(&alias->d_lock);\n\t\tif (dn)\n\t\t\tbreak;\n\t}\nout_unlock:\n\tspin_unlock(&inode->i_lock);\n\treturn dn;\n}\n\n \nstatic int reconnect_caps_cb(struct inode *inode, int mds, void *arg)\n{\n\tunion {\n\t\tstruct ceph_mds_cap_reconnect v2;\n\t\tstruct ceph_mds_cap_reconnect_v1 v1;\n\t} rec;\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tstruct ceph_reconnect_state *recon_state = arg;\n\tstruct ceph_pagelist *pagelist = recon_state->pagelist;\n\tstruct dentry *dentry;\n\tstruct ceph_cap *cap;\n\tchar *path;\n\tint pathlen = 0, err;\n\tu64 pathbase;\n\tu64 snap_follows;\n\n\tdentry = d_find_primary(inode);\n\tif (dentry) {\n\t\t \n\t\tpath = ceph_mdsc_build_path(dentry, &pathlen, &pathbase,\n\t\t\t\t\t    recon_state->msg_version >= 2);\n\t\tdput(dentry);\n\t\tif (IS_ERR(path)) {\n\t\t\terr = PTR_ERR(path);\n\t\t\tgoto out_err;\n\t\t}\n\t} else {\n\t\tpath = NULL;\n\t\tpathbase = 0;\n\t}\n\n\tspin_lock(&ci->i_ceph_lock);\n\tcap = __get_cap_for_mds(ci, mds);\n\tif (!cap) {\n\t\tspin_unlock(&ci->i_ceph_lock);\n\t\terr = 0;\n\t\tgoto out_err;\n\t}\n\tdout(\" adding %p ino %llx.%llx cap %p %lld %s\\n\",\n\t     inode, ceph_vinop(inode), cap, cap->cap_id,\n\t     ceph_cap_string(cap->issued));\n\n\tcap->seq = 0;         \n\tcap->issue_seq = 0;   \n\tcap->mseq = 0;        \n\tcap->cap_gen = atomic_read(&cap->session->s_cap_gen);\n\n\t \n\tif (S_ISDIR(inode->i_mode)) {\n\t\tif (cap->issued & CEPH_CAP_DIR_CREATE) {\n\t\t\tceph_put_string(rcu_dereference_raw(ci->i_cached_layout.pool_ns));\n\t\t\tmemset(&ci->i_cached_layout, 0, sizeof(ci->i_cached_layout));\n\t\t}\n\t\tcap->issued &= ~CEPH_CAP_ANY_DIR_OPS;\n\t}\n\n\tif (recon_state->msg_version >= 2) {\n\t\trec.v2.cap_id = cpu_to_le64(cap->cap_id);\n\t\trec.v2.wanted = cpu_to_le32(__ceph_caps_wanted(ci));\n\t\trec.v2.issued = cpu_to_le32(cap->issued);\n\t\trec.v2.snaprealm = cpu_to_le64(ci->i_snap_realm->ino);\n\t\trec.v2.pathbase = cpu_to_le64(pathbase);\n\t\trec.v2.flock_len = (__force __le32)\n\t\t\t((ci->i_ceph_flags & CEPH_I_ERROR_FILELOCK) ? 0 : 1);\n\t} else {\n\t\trec.v1.cap_id = cpu_to_le64(cap->cap_id);\n\t\trec.v1.wanted = cpu_to_le32(__ceph_caps_wanted(ci));\n\t\trec.v1.issued = cpu_to_le32(cap->issued);\n\t\trec.v1.size = cpu_to_le64(i_size_read(inode));\n\t\tceph_encode_timespec64(&rec.v1.mtime, &inode->i_mtime);\n\t\tceph_encode_timespec64(&rec.v1.atime, &inode->i_atime);\n\t\trec.v1.snaprealm = cpu_to_le64(ci->i_snap_realm->ino);\n\t\trec.v1.pathbase = cpu_to_le64(pathbase);\n\t}\n\n\tif (list_empty(&ci->i_cap_snaps)) {\n\t\tsnap_follows = ci->i_head_snapc ? ci->i_head_snapc->seq : 0;\n\t} else {\n\t\tstruct ceph_cap_snap *capsnap =\n\t\t\tlist_first_entry(&ci->i_cap_snaps,\n\t\t\t\t\t struct ceph_cap_snap, ci_item);\n\t\tsnap_follows = capsnap->follows;\n\t}\n\tspin_unlock(&ci->i_ceph_lock);\n\n\tif (recon_state->msg_version >= 2) {\n\t\tint num_fcntl_locks, num_flock_locks;\n\t\tstruct ceph_filelock *flocks = NULL;\n\t\tsize_t struct_len, total_len = sizeof(u64);\n\t\tu8 struct_v = 0;\n\nencode_again:\n\t\tif (rec.v2.flock_len) {\n\t\t\tceph_count_locks(inode, &num_fcntl_locks, &num_flock_locks);\n\t\t} else {\n\t\t\tnum_fcntl_locks = 0;\n\t\t\tnum_flock_locks = 0;\n\t\t}\n\t\tif (num_fcntl_locks + num_flock_locks > 0) {\n\t\t\tflocks = kmalloc_array(num_fcntl_locks + num_flock_locks,\n\t\t\t\t\t       sizeof(struct ceph_filelock),\n\t\t\t\t\t       GFP_NOFS);\n\t\t\tif (!flocks) {\n\t\t\t\terr = -ENOMEM;\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t\terr = ceph_encode_locks_to_buffer(inode, flocks,\n\t\t\t\t\t\t\t  num_fcntl_locks,\n\t\t\t\t\t\t\t  num_flock_locks);\n\t\t\tif (err) {\n\t\t\t\tkfree(flocks);\n\t\t\t\tflocks = NULL;\n\t\t\t\tif (err == -ENOSPC)\n\t\t\t\t\tgoto encode_again;\n\t\t\t\tgoto out_err;\n\t\t\t}\n\t\t} else {\n\t\t\tkfree(flocks);\n\t\t\tflocks = NULL;\n\t\t}\n\n\t\tif (recon_state->msg_version >= 3) {\n\t\t\t \n\t\t\ttotal_len += 2 * sizeof(u8) + sizeof(u32);\n\t\t\tstruct_v = 2;\n\t\t}\n\t\t \n\t\tstruct_len = 2 * sizeof(u32) +\n\t\t\t    (num_fcntl_locks + num_flock_locks) *\n\t\t\t    sizeof(struct ceph_filelock);\n\t\trec.v2.flock_len = cpu_to_le32(struct_len);\n\n\t\tstruct_len += sizeof(u32) + pathlen + sizeof(rec.v2);\n\n\t\tif (struct_v >= 2)\n\t\t\tstruct_len += sizeof(u64);  \n\n\t\ttotal_len += struct_len;\n\n\t\tif (pagelist->length + total_len > RECONNECT_MAX_SIZE) {\n\t\t\terr = send_reconnect_partial(recon_state);\n\t\t\tif (err)\n\t\t\t\tgoto out_freeflocks;\n\t\t\tpagelist = recon_state->pagelist;\n\t\t}\n\n\t\terr = ceph_pagelist_reserve(pagelist, total_len);\n\t\tif (err)\n\t\t\tgoto out_freeflocks;\n\n\t\tceph_pagelist_encode_64(pagelist, ceph_ino(inode));\n\t\tif (recon_state->msg_version >= 3) {\n\t\t\tceph_pagelist_encode_8(pagelist, struct_v);\n\t\t\tceph_pagelist_encode_8(pagelist, 1);\n\t\t\tceph_pagelist_encode_32(pagelist, struct_len);\n\t\t}\n\t\tceph_pagelist_encode_string(pagelist, path, pathlen);\n\t\tceph_pagelist_append(pagelist, &rec, sizeof(rec.v2));\n\t\tceph_locks_to_pagelist(flocks, pagelist,\n\t\t\t\t       num_fcntl_locks, num_flock_locks);\n\t\tif (struct_v >= 2)\n\t\t\tceph_pagelist_encode_64(pagelist, snap_follows);\nout_freeflocks:\n\t\tkfree(flocks);\n\t} else {\n\t\terr = ceph_pagelist_reserve(pagelist,\n\t\t\t\t\t    sizeof(u64) + sizeof(u32) +\n\t\t\t\t\t    pathlen + sizeof(rec.v1));\n\t\tif (err)\n\t\t\tgoto out_err;\n\n\t\tceph_pagelist_encode_64(pagelist, ceph_ino(inode));\n\t\tceph_pagelist_encode_string(pagelist, path, pathlen);\n\t\tceph_pagelist_append(pagelist, &rec, sizeof(rec.v1));\n\t}\n\nout_err:\n\tceph_mdsc_free_path(path, pathlen);\n\tif (!err)\n\t\trecon_state->nr_caps++;\n\treturn err;\n}\n\nstatic int encode_snap_realms(struct ceph_mds_client *mdsc,\n\t\t\t      struct ceph_reconnect_state *recon_state)\n{\n\tstruct rb_node *p;\n\tstruct ceph_pagelist *pagelist = recon_state->pagelist;\n\tint err = 0;\n\n\tif (recon_state->msg_version >= 4) {\n\t\terr = ceph_pagelist_encode_32(pagelist, mdsc->num_snap_realms);\n\t\tif (err < 0)\n\t\t\tgoto fail;\n\t}\n\n\t \n\tfor (p = rb_first(&mdsc->snap_realms); p; p = rb_next(p)) {\n\t\tstruct ceph_snap_realm *realm =\n\t\t       rb_entry(p, struct ceph_snap_realm, node);\n\t\tstruct ceph_mds_snaprealm_reconnect sr_rec;\n\n\t\tif (recon_state->msg_version >= 4) {\n\t\t\tsize_t need = sizeof(u8) * 2 + sizeof(u32) +\n\t\t\t\t      sizeof(sr_rec);\n\n\t\t\tif (pagelist->length + need > RECONNECT_MAX_SIZE) {\n\t\t\t\terr = send_reconnect_partial(recon_state);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto fail;\n\t\t\t\tpagelist = recon_state->pagelist;\n\t\t\t}\n\n\t\t\terr = ceph_pagelist_reserve(pagelist, need);\n\t\t\tif (err)\n\t\t\t\tgoto fail;\n\n\t\t\tceph_pagelist_encode_8(pagelist, 1);\n\t\t\tceph_pagelist_encode_8(pagelist, 1);\n\t\t\tceph_pagelist_encode_32(pagelist, sizeof(sr_rec));\n\t\t}\n\n\t\tdout(\" adding snap realm %llx seq %lld parent %llx\\n\",\n\t\t     realm->ino, realm->seq, realm->parent_ino);\n\t\tsr_rec.ino = cpu_to_le64(realm->ino);\n\t\tsr_rec.seq = cpu_to_le64(realm->seq);\n\t\tsr_rec.parent = cpu_to_le64(realm->parent_ino);\n\n\t\terr = ceph_pagelist_append(pagelist, &sr_rec, sizeof(sr_rec));\n\t\tif (err)\n\t\t\tgoto fail;\n\n\t\trecon_state->nr_realms++;\n\t}\nfail:\n\treturn err;\n}\n\n\n \nstatic void send_mds_reconnect(struct ceph_mds_client *mdsc,\n\t\t\t       struct ceph_mds_session *session)\n{\n\tstruct ceph_msg *reply;\n\tint mds = session->s_mds;\n\tint err = -ENOMEM;\n\tstruct ceph_reconnect_state recon_state = {\n\t\t.session = session,\n\t};\n\tLIST_HEAD(dispose);\n\n\tpr_info(\"mds%d reconnect start\\n\", mds);\n\n\trecon_state.pagelist = ceph_pagelist_alloc(GFP_NOFS);\n\tif (!recon_state.pagelist)\n\t\tgoto fail_nopagelist;\n\n\treply = ceph_msg_new2(CEPH_MSG_CLIENT_RECONNECT, 0, 1, GFP_NOFS, false);\n\tif (!reply)\n\t\tgoto fail_nomsg;\n\n\txa_destroy(&session->s_delegated_inos);\n\n\tmutex_lock(&session->s_mutex);\n\tsession->s_state = CEPH_MDS_SESSION_RECONNECTING;\n\tsession->s_seq = 0;\n\n\tdout(\"session %p state %s\\n\", session,\n\t     ceph_session_state_name(session->s_state));\n\n\tatomic_inc(&session->s_cap_gen);\n\n\tspin_lock(&session->s_cap_lock);\n\t \n\tsession->s_readonly = 0;\n\t \n\tsession->s_cap_reconnect = 1;\n\t \n\tdetach_cap_releases(session, &dispose);\n\tspin_unlock(&session->s_cap_lock);\n\tdispose_cap_releases(mdsc, &dispose);\n\n\t \n\tif (mdsc->fsc->sb->s_root)\n\t\tshrink_dcache_parent(mdsc->fsc->sb->s_root);\n\n\tceph_con_close(&session->s_con);\n\tceph_con_open(&session->s_con,\n\t\t      CEPH_ENTITY_TYPE_MDS, mds,\n\t\t      ceph_mdsmap_get_addr(mdsc->mdsmap, mds));\n\n\t \n\treplay_unsafe_requests(mdsc, session);\n\n\tceph_early_kick_flushing_caps(mdsc, session);\n\n\tdown_read(&mdsc->snap_rwsem);\n\n\t \n\terr = ceph_pagelist_encode_32(recon_state.pagelist, 0);\n\tif (err)\n\t\tgoto fail;\n\n\tif (test_bit(CEPHFS_FEATURE_MULTI_RECONNECT, &session->s_features)) {\n\t\trecon_state.msg_version = 3;\n\t\trecon_state.allow_multi = true;\n\t} else if (session->s_con.peer_features & CEPH_FEATURE_MDSENC) {\n\t\trecon_state.msg_version = 3;\n\t} else {\n\t\trecon_state.msg_version = 2;\n\t}\n\t \n\terr = ceph_iterate_session_caps(session, reconnect_caps_cb, &recon_state);\n\n\tspin_lock(&session->s_cap_lock);\n\tsession->s_cap_reconnect = 0;\n\tspin_unlock(&session->s_cap_lock);\n\n\tif (err < 0)\n\t\tgoto fail;\n\n\t \n\tif (mdsc->num_snap_realms) {\n\t\tsize_t total_len =\n\t\t\trecon_state.pagelist->length +\n\t\t\tmdsc->num_snap_realms *\n\t\t\tsizeof(struct ceph_mds_snaprealm_reconnect);\n\t\tif (recon_state.msg_version >= 4) {\n\t\t\t \n\t\t\ttotal_len += sizeof(u32);\n\t\t\t \n\t\t\ttotal_len += mdsc->num_snap_realms *\n\t\t\t\t     (2 * sizeof(u8) + sizeof(u32));\n\t\t}\n\t\tif (total_len > RECONNECT_MAX_SIZE) {\n\t\t\tif (!recon_state.allow_multi) {\n\t\t\t\terr = -ENOSPC;\n\t\t\t\tgoto fail;\n\t\t\t}\n\t\t\tif (recon_state.nr_caps) {\n\t\t\t\terr = send_reconnect_partial(&recon_state);\n\t\t\t\tif (err)\n\t\t\t\t\tgoto fail;\n\t\t\t}\n\t\t\trecon_state.msg_version = 5;\n\t\t}\n\t}\n\n\terr = encode_snap_realms(mdsc, &recon_state);\n\tif (err < 0)\n\t\tgoto fail;\n\n\tif (recon_state.msg_version >= 5) {\n\t\terr = ceph_pagelist_encode_8(recon_state.pagelist, 0);\n\t\tif (err < 0)\n\t\t\tgoto fail;\n\t}\n\n\tif (recon_state.nr_caps || recon_state.nr_realms) {\n\t\tstruct page *page =\n\t\t\tlist_first_entry(&recon_state.pagelist->head,\n\t\t\t\t\tstruct page, lru);\n\t\t__le32 *addr = kmap_atomic(page);\n\t\tif (recon_state.nr_caps) {\n\t\t\tWARN_ON(recon_state.nr_realms != mdsc->num_snap_realms);\n\t\t\t*addr = cpu_to_le32(recon_state.nr_caps);\n\t\t} else if (recon_state.msg_version >= 4) {\n\t\t\t*(addr + 1) = cpu_to_le32(recon_state.nr_realms);\n\t\t}\n\t\tkunmap_atomic(addr);\n\t}\n\n\treply->hdr.version = cpu_to_le16(recon_state.msg_version);\n\tif (recon_state.msg_version >= 4)\n\t\treply->hdr.compat_version = cpu_to_le16(4);\n\n\treply->hdr.data_len = cpu_to_le32(recon_state.pagelist->length);\n\tceph_msg_data_add_pagelist(reply, recon_state.pagelist);\n\n\tceph_con_send(&session->s_con, reply);\n\n\tmutex_unlock(&session->s_mutex);\n\n\tmutex_lock(&mdsc->mutex);\n\t__wake_requests(mdsc, &session->s_waiting);\n\tmutex_unlock(&mdsc->mutex);\n\n\tup_read(&mdsc->snap_rwsem);\n\tceph_pagelist_release(recon_state.pagelist);\n\treturn;\n\nfail:\n\tceph_msg_put(reply);\n\tup_read(&mdsc->snap_rwsem);\n\tmutex_unlock(&session->s_mutex);\nfail_nomsg:\n\tceph_pagelist_release(recon_state.pagelist);\nfail_nopagelist:\n\tpr_err(\"error %d preparing reconnect for mds%d\\n\", err, mds);\n\treturn;\n}\n\n\n \nstatic void check_new_map(struct ceph_mds_client *mdsc,\n\t\t\t  struct ceph_mdsmap *newmap,\n\t\t\t  struct ceph_mdsmap *oldmap)\n{\n\tint i, j, err;\n\tint oldstate, newstate;\n\tstruct ceph_mds_session *s;\n\tunsigned long targets[DIV_ROUND_UP(CEPH_MAX_MDS, sizeof(unsigned long))] = {0};\n\n\tdout(\"check_new_map new %u old %u\\n\",\n\t     newmap->m_epoch, oldmap->m_epoch);\n\n\tif (newmap->m_info) {\n\t\tfor (i = 0; i < newmap->possible_max_rank; i++) {\n\t\t\tfor (j = 0; j < newmap->m_info[i].num_export_targets; j++)\n\t\t\t\tset_bit(newmap->m_info[i].export_targets[j], targets);\n\t\t}\n\t}\n\n\tfor (i = 0; i < oldmap->possible_max_rank && i < mdsc->max_sessions; i++) {\n\t\tif (!mdsc->sessions[i])\n\t\t\tcontinue;\n\t\ts = mdsc->sessions[i];\n\t\toldstate = ceph_mdsmap_get_state(oldmap, i);\n\t\tnewstate = ceph_mdsmap_get_state(newmap, i);\n\n\t\tdout(\"check_new_map mds%d state %s%s -> %s%s (session %s)\\n\",\n\t\t     i, ceph_mds_state_name(oldstate),\n\t\t     ceph_mdsmap_is_laggy(oldmap, i) ? \" (laggy)\" : \"\",\n\t\t     ceph_mds_state_name(newstate),\n\t\t     ceph_mdsmap_is_laggy(newmap, i) ? \" (laggy)\" : \"\",\n\t\t     ceph_session_state_name(s->s_state));\n\n\t\tif (i >= newmap->possible_max_rank) {\n\t\t\t \n\t\t\tceph_get_mds_session(s);\n\t\t\t__unregister_session(mdsc, s);\n\t\t\t__wake_requests(mdsc, &s->s_waiting);\n\t\t\tmutex_unlock(&mdsc->mutex);\n\n\t\t\tmutex_lock(&s->s_mutex);\n\t\t\tcleanup_session_requests(mdsc, s);\n\t\t\tremove_session_caps(s);\n\t\t\tmutex_unlock(&s->s_mutex);\n\n\t\t\tceph_put_mds_session(s);\n\n\t\t\tmutex_lock(&mdsc->mutex);\n\t\t\tkick_requests(mdsc, i);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (memcmp(ceph_mdsmap_get_addr(oldmap, i),\n\t\t\t   ceph_mdsmap_get_addr(newmap, i),\n\t\t\t   sizeof(struct ceph_entity_addr))) {\n\t\t\t \n\t\t\tmutex_unlock(&mdsc->mutex);\n\t\t\tmutex_lock(&s->s_mutex);\n\t\t\tmutex_lock(&mdsc->mutex);\n\t\t\tceph_con_close(&s->s_con);\n\t\t\tmutex_unlock(&s->s_mutex);\n\t\t\ts->s_state = CEPH_MDS_SESSION_RESTARTING;\n\t\t} else if (oldstate == newstate) {\n\t\t\tcontinue;   \n\t\t}\n\n\t\t \n\t\tif (s->s_state == CEPH_MDS_SESSION_RESTARTING &&\n\t\t    newstate >= CEPH_MDS_STATE_RECONNECT) {\n\t\t\tmutex_unlock(&mdsc->mutex);\n\t\t\tclear_bit(i, targets);\n\t\t\tsend_mds_reconnect(mdsc, s);\n\t\t\tmutex_lock(&mdsc->mutex);\n\t\t}\n\n\t\t \n\t\tif (oldstate < CEPH_MDS_STATE_ACTIVE &&\n\t\t    newstate >= CEPH_MDS_STATE_ACTIVE) {\n\t\t\tif (oldstate != CEPH_MDS_STATE_CREATING &&\n\t\t\t    oldstate != CEPH_MDS_STATE_STARTING)\n\t\t\t\tpr_info(\"mds%d recovery completed\\n\", s->s_mds);\n\t\t\tkick_requests(mdsc, i);\n\t\t\tmutex_unlock(&mdsc->mutex);\n\t\t\tmutex_lock(&s->s_mutex);\n\t\t\tmutex_lock(&mdsc->mutex);\n\t\t\tceph_kick_flushing_caps(mdsc, s);\n\t\t\tmutex_unlock(&s->s_mutex);\n\t\t\twake_up_session_caps(s, RECONNECT);\n\t\t}\n\t}\n\n\t \n\tfor (i = 0; i < newmap->possible_max_rank; i++) {\n\t\t \n\t\tnewstate = ceph_mdsmap_get_state(newmap, i);\n\t\tif (!test_bit(i, targets) || newstate != CEPH_MDS_STATE_RECONNECT)\n\t\t\tcontinue;\n\n\t\t \n\t\ts = __ceph_lookup_mds_session(mdsc, i);\n\t\tif (likely(!s)) {\n\t\t\ts = __open_export_target_session(mdsc, i);\n\t\t\tif (IS_ERR(s)) {\n\t\t\t\terr = PTR_ERR(s);\n\t\t\t\tpr_err(\"failed to open export target session, err %d\\n\",\n\t\t\t\t       err);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tdout(\"send reconnect to export target mds.%d\\n\", i);\n\t\tmutex_unlock(&mdsc->mutex);\n\t\tsend_mds_reconnect(mdsc, s);\n\t\tceph_put_mds_session(s);\n\t\tmutex_lock(&mdsc->mutex);\n\t}\n\n\tfor (i = 0; i < newmap->possible_max_rank && i < mdsc->max_sessions; i++) {\n\t\ts = mdsc->sessions[i];\n\t\tif (!s)\n\t\t\tcontinue;\n\t\tif (!ceph_mdsmap_is_laggy(newmap, i))\n\t\t\tcontinue;\n\t\tif (s->s_state == CEPH_MDS_SESSION_OPEN ||\n\t\t    s->s_state == CEPH_MDS_SESSION_HUNG ||\n\t\t    s->s_state == CEPH_MDS_SESSION_CLOSING) {\n\t\t\tdout(\" connecting to export targets of laggy mds%d\\n\",\n\t\t\t     i);\n\t\t\t__open_export_target_sessions(mdsc, s);\n\t\t}\n\t}\n}\n\n\n\n \n\n \nvoid __ceph_mdsc_drop_dentry_lease(struct dentry *dentry)\n{\n\tstruct ceph_dentry_info *di = ceph_dentry(dentry);\n\n\tceph_put_mds_session(di->lease_session);\n\tdi->lease_session = NULL;\n}\n\nstatic void handle_lease(struct ceph_mds_client *mdsc,\n\t\t\t struct ceph_mds_session *session,\n\t\t\t struct ceph_msg *msg)\n{\n\tstruct super_block *sb = mdsc->fsc->sb;\n\tstruct inode *inode;\n\tstruct dentry *parent, *dentry;\n\tstruct ceph_dentry_info *di;\n\tint mds = session->s_mds;\n\tstruct ceph_mds_lease *h = msg->front.iov_base;\n\tu32 seq;\n\tstruct ceph_vino vino;\n\tstruct qstr dname;\n\tint release = 0;\n\n\tdout(\"handle_lease from mds%d\\n\", mds);\n\n\tif (!ceph_inc_mds_stopping_blocker(mdsc, session))\n\t\treturn;\n\n\t \n\tif (msg->front.iov_len < sizeof(*h) + sizeof(u32))\n\t\tgoto bad;\n\tvino.ino = le64_to_cpu(h->ino);\n\tvino.snap = CEPH_NOSNAP;\n\tseq = le32_to_cpu(h->seq);\n\tdname.len = get_unaligned_le32(h + 1);\n\tif (msg->front.iov_len < sizeof(*h) + sizeof(u32) + dname.len)\n\t\tgoto bad;\n\tdname.name = (void *)(h + 1) + sizeof(u32);\n\n\t \n\tinode = ceph_find_inode(sb, vino);\n\tdout(\"handle_lease %s, ino %llx %p %.*s\\n\",\n\t     ceph_lease_op_name(h->action), vino.ino, inode,\n\t     dname.len, dname.name);\n\n\tmutex_lock(&session->s_mutex);\n\tif (!inode) {\n\t\tdout(\"handle_lease no inode %llx\\n\", vino.ino);\n\t\tgoto release;\n\t}\n\n\t \n\tparent = d_find_alias(inode);\n\tif (!parent) {\n\t\tdout(\"no parent dentry on inode %p\\n\", inode);\n\t\tWARN_ON(1);\n\t\tgoto release;   \n\t}\n\tdname.hash = full_name_hash(parent, dname.name, dname.len);\n\tdentry = d_lookup(parent, &dname);\n\tdput(parent);\n\tif (!dentry)\n\t\tgoto release;\n\n\tspin_lock(&dentry->d_lock);\n\tdi = ceph_dentry(dentry);\n\tswitch (h->action) {\n\tcase CEPH_MDS_LEASE_REVOKE:\n\t\tif (di->lease_session == session) {\n\t\t\tif (ceph_seq_cmp(di->lease_seq, seq) > 0)\n\t\t\t\th->seq = cpu_to_le32(di->lease_seq);\n\t\t\t__ceph_mdsc_drop_dentry_lease(dentry);\n\t\t}\n\t\trelease = 1;\n\t\tbreak;\n\n\tcase CEPH_MDS_LEASE_RENEW:\n\t\tif (di->lease_session == session &&\n\t\t    di->lease_gen == atomic_read(&session->s_cap_gen) &&\n\t\t    di->lease_renew_from &&\n\t\t    di->lease_renew_after == 0) {\n\t\t\tunsigned long duration =\n\t\t\t\tmsecs_to_jiffies(le32_to_cpu(h->duration_ms));\n\n\t\t\tdi->lease_seq = seq;\n\t\t\tdi->time = di->lease_renew_from + duration;\n\t\t\tdi->lease_renew_after = di->lease_renew_from +\n\t\t\t\t(duration >> 1);\n\t\t\tdi->lease_renew_from = 0;\n\t\t}\n\t\tbreak;\n\t}\n\tspin_unlock(&dentry->d_lock);\n\tdput(dentry);\n\n\tif (!release)\n\t\tgoto out;\n\nrelease:\n\t \n\th->action = CEPH_MDS_LEASE_REVOKE_ACK;\n\tceph_msg_get(msg);\n\tceph_con_send(&session->s_con, msg);\n\nout:\n\tmutex_unlock(&session->s_mutex);\n\tiput(inode);\n\n\tceph_dec_mds_stopping_blocker(mdsc);\n\treturn;\n\nbad:\n\tceph_dec_mds_stopping_blocker(mdsc);\n\n\tpr_err(\"corrupt lease message\\n\");\n\tceph_msg_dump(msg);\n}\n\nvoid ceph_mdsc_lease_send_msg(struct ceph_mds_session *session,\n\t\t\t      struct dentry *dentry, char action,\n\t\t\t      u32 seq)\n{\n\tstruct ceph_msg *msg;\n\tstruct ceph_mds_lease *lease;\n\tstruct inode *dir;\n\tint len = sizeof(*lease) + sizeof(u32) + NAME_MAX;\n\n\tdout(\"lease_send_msg identry %p %s to mds%d\\n\",\n\t     dentry, ceph_lease_op_name(action), session->s_mds);\n\n\tmsg = ceph_msg_new(CEPH_MSG_CLIENT_LEASE, len, GFP_NOFS, false);\n\tif (!msg)\n\t\treturn;\n\tlease = msg->front.iov_base;\n\tlease->action = action;\n\tlease->seq = cpu_to_le32(seq);\n\n\tspin_lock(&dentry->d_lock);\n\tdir = d_inode(dentry->d_parent);\n\tlease->ino = cpu_to_le64(ceph_ino(dir));\n\tlease->first = lease->last = cpu_to_le64(ceph_snap(dir));\n\n\tput_unaligned_le32(dentry->d_name.len, lease + 1);\n\tmemcpy((void *)(lease + 1) + 4,\n\t       dentry->d_name.name, dentry->d_name.len);\n\tspin_unlock(&dentry->d_lock);\n\n\tceph_con_send(&session->s_con, msg);\n}\n\n \nstatic void lock_unlock_session(struct ceph_mds_session *s)\n{\n\tmutex_lock(&s->s_mutex);\n\tmutex_unlock(&s->s_mutex);\n}\n\nstatic void maybe_recover_session(struct ceph_mds_client *mdsc)\n{\n\tstruct ceph_fs_client *fsc = mdsc->fsc;\n\n\tif (!ceph_test_mount_opt(fsc, CLEANRECOVER))\n\t\treturn;\n\n\tif (READ_ONCE(fsc->mount_state) != CEPH_MOUNT_MOUNTED)\n\t\treturn;\n\n\tif (!READ_ONCE(fsc->blocklisted))\n\t\treturn;\n\n\tpr_info(\"auto reconnect after blocklisted\\n\");\n\tceph_force_reconnect(fsc->sb);\n}\n\nbool check_session_state(struct ceph_mds_session *s)\n{\n\tswitch (s->s_state) {\n\tcase CEPH_MDS_SESSION_OPEN:\n\t\tif (s->s_ttl && time_after(jiffies, s->s_ttl)) {\n\t\t\ts->s_state = CEPH_MDS_SESSION_HUNG;\n\t\t\tpr_info(\"mds%d hung\\n\", s->s_mds);\n\t\t}\n\t\tbreak;\n\tcase CEPH_MDS_SESSION_CLOSING:\n\tcase CEPH_MDS_SESSION_NEW:\n\tcase CEPH_MDS_SESSION_RESTARTING:\n\tcase CEPH_MDS_SESSION_CLOSED:\n\tcase CEPH_MDS_SESSION_REJECTED:\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\n \nvoid inc_session_sequence(struct ceph_mds_session *s)\n{\n\tlockdep_assert_held(&s->s_mutex);\n\n\ts->s_seq++;\n\n\tif (s->s_state == CEPH_MDS_SESSION_CLOSING) {\n\t\tint ret;\n\n\t\tdout(\"resending session close request for mds%d\\n\", s->s_mds);\n\t\tret = request_close_session(s);\n\t\tif (ret < 0)\n\t\t\tpr_err(\"unable to close session to mds%d: %d\\n\",\n\t\t\t       s->s_mds, ret);\n\t}\n}\n\n \nstatic void schedule_delayed(struct ceph_mds_client *mdsc, unsigned long delay)\n{\n\tunsigned long max_delay = HZ * 5;\n\n\t \n\tif (!delay || (delay > max_delay))\n\t\tdelay = max_delay;\n\tschedule_delayed_work(&mdsc->delayed_work,\n\t\t\t      round_jiffies_relative(delay));\n}\n\nstatic void delayed_work(struct work_struct *work)\n{\n\tstruct ceph_mds_client *mdsc =\n\t\tcontainer_of(work, struct ceph_mds_client, delayed_work.work);\n\tunsigned long delay;\n\tint renew_interval;\n\tint renew_caps;\n\tint i;\n\n\tdout(\"mdsc delayed_work\\n\");\n\n\tif (mdsc->stopping >= CEPH_MDSC_STOPPING_FLUSHED)\n\t\treturn;\n\n\tmutex_lock(&mdsc->mutex);\n\trenew_interval = mdsc->mdsmap->m_session_timeout >> 2;\n\trenew_caps = time_after_eq(jiffies, HZ*renew_interval +\n\t\t\t\t   mdsc->last_renew_caps);\n\tif (renew_caps)\n\t\tmdsc->last_renew_caps = jiffies;\n\n\tfor (i = 0; i < mdsc->max_sessions; i++) {\n\t\tstruct ceph_mds_session *s = __ceph_lookup_mds_session(mdsc, i);\n\t\tif (!s)\n\t\t\tcontinue;\n\n\t\tif (!check_session_state(s)) {\n\t\t\tceph_put_mds_session(s);\n\t\t\tcontinue;\n\t\t}\n\t\tmutex_unlock(&mdsc->mutex);\n\n\t\tmutex_lock(&s->s_mutex);\n\t\tif (renew_caps)\n\t\t\tsend_renew_caps(mdsc, s);\n\t\telse\n\t\t\tceph_con_keepalive(&s->s_con);\n\t\tif (s->s_state == CEPH_MDS_SESSION_OPEN ||\n\t\t    s->s_state == CEPH_MDS_SESSION_HUNG)\n\t\t\tceph_send_cap_releases(mdsc, s);\n\t\tmutex_unlock(&s->s_mutex);\n\t\tceph_put_mds_session(s);\n\n\t\tmutex_lock(&mdsc->mutex);\n\t}\n\tmutex_unlock(&mdsc->mutex);\n\n\tdelay = ceph_check_delayed_caps(mdsc);\n\n\tceph_queue_cap_reclaim_work(mdsc);\n\n\tceph_trim_snapid_map(mdsc);\n\n\tmaybe_recover_session(mdsc);\n\n\tschedule_delayed(mdsc, delay);\n}\n\nint ceph_mdsc_init(struct ceph_fs_client *fsc)\n\n{\n\tstruct ceph_mds_client *mdsc;\n\tint err;\n\n\tmdsc = kzalloc(sizeof(struct ceph_mds_client), GFP_NOFS);\n\tif (!mdsc)\n\t\treturn -ENOMEM;\n\tmdsc->fsc = fsc;\n\tmutex_init(&mdsc->mutex);\n\tmdsc->mdsmap = kzalloc(sizeof(*mdsc->mdsmap), GFP_NOFS);\n\tif (!mdsc->mdsmap) {\n\t\terr = -ENOMEM;\n\t\tgoto err_mdsc;\n\t}\n\n\tinit_completion(&mdsc->safe_umount_waiters);\n\tspin_lock_init(&mdsc->stopping_lock);\n\tatomic_set(&mdsc->stopping_blockers, 0);\n\tinit_completion(&mdsc->stopping_waiter);\n\tinit_waitqueue_head(&mdsc->session_close_wq);\n\tINIT_LIST_HEAD(&mdsc->waiting_for_map);\n\tmdsc->quotarealms_inodes = RB_ROOT;\n\tmutex_init(&mdsc->quotarealms_inodes_mutex);\n\tinit_rwsem(&mdsc->snap_rwsem);\n\tmdsc->snap_realms = RB_ROOT;\n\tINIT_LIST_HEAD(&mdsc->snap_empty);\n\tspin_lock_init(&mdsc->snap_empty_lock);\n\tmdsc->request_tree = RB_ROOT;\n\tINIT_DELAYED_WORK(&mdsc->delayed_work, delayed_work);\n\tmdsc->last_renew_caps = jiffies;\n\tINIT_LIST_HEAD(&mdsc->cap_delay_list);\n\tINIT_LIST_HEAD(&mdsc->cap_wait_list);\n\tspin_lock_init(&mdsc->cap_delay_lock);\n\tINIT_LIST_HEAD(&mdsc->snap_flush_list);\n\tspin_lock_init(&mdsc->snap_flush_lock);\n\tmdsc->last_cap_flush_tid = 1;\n\tINIT_LIST_HEAD(&mdsc->cap_flush_list);\n\tINIT_LIST_HEAD(&mdsc->cap_dirty_migrating);\n\tspin_lock_init(&mdsc->cap_dirty_lock);\n\tinit_waitqueue_head(&mdsc->cap_flushing_wq);\n\tINIT_WORK(&mdsc->cap_reclaim_work, ceph_cap_reclaim_work);\n\terr = ceph_metric_init(&mdsc->metric);\n\tif (err)\n\t\tgoto err_mdsmap;\n\n\tspin_lock_init(&mdsc->dentry_list_lock);\n\tINIT_LIST_HEAD(&mdsc->dentry_leases);\n\tINIT_LIST_HEAD(&mdsc->dentry_dir_leases);\n\n\tceph_caps_init(mdsc);\n\tceph_adjust_caps_max_min(mdsc, fsc->mount_options);\n\n\tspin_lock_init(&mdsc->snapid_map_lock);\n\tmdsc->snapid_map_tree = RB_ROOT;\n\tINIT_LIST_HEAD(&mdsc->snapid_map_lru);\n\n\tinit_rwsem(&mdsc->pool_perm_rwsem);\n\tmdsc->pool_perm_tree = RB_ROOT;\n\n\tstrscpy(mdsc->nodename, utsname()->nodename,\n\t\tsizeof(mdsc->nodename));\n\n\tfsc->mdsc = mdsc;\n\treturn 0;\n\nerr_mdsmap:\n\tkfree(mdsc->mdsmap);\nerr_mdsc:\n\tkfree(mdsc);\n\treturn err;\n}\n\n \nstatic void wait_requests(struct ceph_mds_client *mdsc)\n{\n\tstruct ceph_options *opts = mdsc->fsc->client->options;\n\tstruct ceph_mds_request *req;\n\n\tmutex_lock(&mdsc->mutex);\n\tif (__get_oldest_req(mdsc)) {\n\t\tmutex_unlock(&mdsc->mutex);\n\n\t\tdout(\"wait_requests waiting for requests\\n\");\n\t\twait_for_completion_timeout(&mdsc->safe_umount_waiters,\n\t\t\t\t    ceph_timeout_jiffies(opts->mount_timeout));\n\n\t\t \n\t\tmutex_lock(&mdsc->mutex);\n\t\twhile ((req = __get_oldest_req(mdsc))) {\n\t\t\tdout(\"wait_requests timed out on tid %llu\\n\",\n\t\t\t     req->r_tid);\n\t\t\tlist_del_init(&req->r_wait);\n\t\t\t__unregister_request(mdsc, req);\n\t\t}\n\t}\n\tmutex_unlock(&mdsc->mutex);\n\tdout(\"wait_requests done\\n\");\n}\n\nvoid send_flush_mdlog(struct ceph_mds_session *s)\n{\n\tstruct ceph_msg *msg;\n\n\t \n\tif (!CEPH_HAVE_FEATURE(s->s_con.peer_features, SERVER_LUMINOUS))\n\t\treturn;\n\n\tmutex_lock(&s->s_mutex);\n\tdout(\"request mdlog flush to mds%d (%s)s seq %lld\\n\", s->s_mds,\n\t     ceph_session_state_name(s->s_state), s->s_seq);\n\tmsg = ceph_create_session_msg(CEPH_SESSION_REQUEST_FLUSH_MDLOG,\n\t\t\t\t      s->s_seq);\n\tif (!msg) {\n\t\tpr_err(\"failed to request mdlog flush to mds%d (%s) seq %lld\\n\",\n\t\t       s->s_mds, ceph_session_state_name(s->s_state), s->s_seq);\n\t} else {\n\t\tceph_con_send(&s->s_con, msg);\n\t}\n\tmutex_unlock(&s->s_mutex);\n}\n\n \nvoid ceph_mdsc_pre_umount(struct ceph_mds_client *mdsc)\n{\n\tdout(\"pre_umount\\n\");\n\tmdsc->stopping = CEPH_MDSC_STOPPING_BEGIN;\n\n\tceph_mdsc_iterate_sessions(mdsc, send_flush_mdlog, true);\n\tceph_mdsc_iterate_sessions(mdsc, lock_unlock_session, false);\n\tceph_flush_dirty_caps(mdsc);\n\twait_requests(mdsc);\n\n\t \n\tceph_msgr_flush();\n\n\tceph_cleanup_quotarealms_inodes(mdsc);\n}\n\n \nstatic void flush_mdlog_and_wait_mdsc_unsafe_requests(struct ceph_mds_client *mdsc,\n\t\t\t\t\t\t u64 want_tid)\n{\n\tstruct ceph_mds_request *req = NULL, *nextreq;\n\tstruct ceph_mds_session *last_session = NULL;\n\tstruct rb_node *n;\n\n\tmutex_lock(&mdsc->mutex);\n\tdout(\"%s want %lld\\n\", __func__, want_tid);\nrestart:\n\treq = __get_oldest_req(mdsc);\n\twhile (req && req->r_tid <= want_tid) {\n\t\t \n\t\tn = rb_next(&req->r_node);\n\t\tif (n)\n\t\t\tnextreq = rb_entry(n, struct ceph_mds_request, r_node);\n\t\telse\n\t\t\tnextreq = NULL;\n\t\tif (req->r_op != CEPH_MDS_OP_SETFILELOCK &&\n\t\t    (req->r_op & CEPH_MDS_OP_WRITE)) {\n\t\t\tstruct ceph_mds_session *s = req->r_session;\n\n\t\t\tif (!s) {\n\t\t\t\treq = nextreq;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tceph_mdsc_get_request(req);\n\t\t\tif (nextreq)\n\t\t\t\tceph_mdsc_get_request(nextreq);\n\t\t\ts = ceph_get_mds_session(s);\n\t\t\tmutex_unlock(&mdsc->mutex);\n\n\t\t\t \n\t\t\tif (last_session != s) {\n\t\t\t\tsend_flush_mdlog(s);\n\t\t\t\tceph_put_mds_session(last_session);\n\t\t\t\tlast_session = s;\n\t\t\t} else {\n\t\t\t\tceph_put_mds_session(s);\n\t\t\t}\n\t\t\tdout(\"%s wait on %llu (want %llu)\\n\", __func__,\n\t\t\t     req->r_tid, want_tid);\n\t\t\twait_for_completion(&req->r_safe_completion);\n\n\t\t\tmutex_lock(&mdsc->mutex);\n\t\t\tceph_mdsc_put_request(req);\n\t\t\tif (!nextreq)\n\t\t\t\tbreak;   \n\t\t\tif (RB_EMPTY_NODE(&nextreq->r_node)) {\n\t\t\t\t \n\t\t\t\tceph_mdsc_put_request(nextreq);\n\t\t\t\tgoto restart;\n\t\t\t}\n\t\t\tceph_mdsc_put_request(nextreq);   \n\t\t}\n\t\treq = nextreq;\n\t}\n\tmutex_unlock(&mdsc->mutex);\n\tceph_put_mds_session(last_session);\n\tdout(\"%s done\\n\", __func__);\n}\n\nvoid ceph_mdsc_sync(struct ceph_mds_client *mdsc)\n{\n\tu64 want_tid, want_flush;\n\n\tif (READ_ONCE(mdsc->fsc->mount_state) >= CEPH_MOUNT_SHUTDOWN)\n\t\treturn;\n\n\tdout(\"sync\\n\");\n\tmutex_lock(&mdsc->mutex);\n\twant_tid = mdsc->last_tid;\n\tmutex_unlock(&mdsc->mutex);\n\n\tceph_flush_dirty_caps(mdsc);\n\tspin_lock(&mdsc->cap_dirty_lock);\n\twant_flush = mdsc->last_cap_flush_tid;\n\tif (!list_empty(&mdsc->cap_flush_list)) {\n\t\tstruct ceph_cap_flush *cf =\n\t\t\tlist_last_entry(&mdsc->cap_flush_list,\n\t\t\t\t\tstruct ceph_cap_flush, g_list);\n\t\tcf->wake = true;\n\t}\n\tspin_unlock(&mdsc->cap_dirty_lock);\n\n\tdout(\"sync want tid %lld flush_seq %lld\\n\",\n\t     want_tid, want_flush);\n\n\tflush_mdlog_and_wait_mdsc_unsafe_requests(mdsc, want_tid);\n\twait_caps_flush(mdsc, want_flush);\n}\n\n \nstatic bool done_closing_sessions(struct ceph_mds_client *mdsc, int skipped)\n{\n\tif (READ_ONCE(mdsc->fsc->mount_state) == CEPH_MOUNT_SHUTDOWN)\n\t\treturn true;\n\treturn atomic_read(&mdsc->num_sessions) <= skipped;\n}\n\n \nvoid ceph_mdsc_close_sessions(struct ceph_mds_client *mdsc)\n{\n\tstruct ceph_options *opts = mdsc->fsc->client->options;\n\tstruct ceph_mds_session *session;\n\tint i;\n\tint skipped = 0;\n\n\tdout(\"close_sessions\\n\");\n\n\t \n\tmutex_lock(&mdsc->mutex);\n\tfor (i = 0; i < mdsc->max_sessions; i++) {\n\t\tsession = __ceph_lookup_mds_session(mdsc, i);\n\t\tif (!session)\n\t\t\tcontinue;\n\t\tmutex_unlock(&mdsc->mutex);\n\t\tmutex_lock(&session->s_mutex);\n\t\tif (__close_session(mdsc, session) <= 0)\n\t\t\tskipped++;\n\t\tmutex_unlock(&session->s_mutex);\n\t\tceph_put_mds_session(session);\n\t\tmutex_lock(&mdsc->mutex);\n\t}\n\tmutex_unlock(&mdsc->mutex);\n\n\tdout(\"waiting for sessions to close\\n\");\n\twait_event_timeout(mdsc->session_close_wq,\n\t\t\t   done_closing_sessions(mdsc, skipped),\n\t\t\t   ceph_timeout_jiffies(opts->mount_timeout));\n\n\t \n\tmutex_lock(&mdsc->mutex);\n\tfor (i = 0; i < mdsc->max_sessions; i++) {\n\t\tif (mdsc->sessions[i]) {\n\t\t\tsession = ceph_get_mds_session(mdsc->sessions[i]);\n\t\t\t__unregister_session(mdsc, session);\n\t\t\tmutex_unlock(&mdsc->mutex);\n\t\t\tmutex_lock(&session->s_mutex);\n\t\t\tremove_session_caps(session);\n\t\t\tmutex_unlock(&session->s_mutex);\n\t\t\tceph_put_mds_session(session);\n\t\t\tmutex_lock(&mdsc->mutex);\n\t\t}\n\t}\n\tWARN_ON(!list_empty(&mdsc->cap_delay_list));\n\tmutex_unlock(&mdsc->mutex);\n\n\tceph_cleanup_snapid_map(mdsc);\n\tceph_cleanup_global_and_empty_realms(mdsc);\n\n\tcancel_work_sync(&mdsc->cap_reclaim_work);\n\tcancel_delayed_work_sync(&mdsc->delayed_work);  \n\n\tdout(\"stopped\\n\");\n}\n\nvoid ceph_mdsc_force_umount(struct ceph_mds_client *mdsc)\n{\n\tstruct ceph_mds_session *session;\n\tint mds;\n\n\tdout(\"force umount\\n\");\n\n\tmutex_lock(&mdsc->mutex);\n\tfor (mds = 0; mds < mdsc->max_sessions; mds++) {\n\t\tsession = __ceph_lookup_mds_session(mdsc, mds);\n\t\tif (!session)\n\t\t\tcontinue;\n\n\t\tif (session->s_state == CEPH_MDS_SESSION_REJECTED)\n\t\t\t__unregister_session(mdsc, session);\n\t\t__wake_requests(mdsc, &session->s_waiting);\n\t\tmutex_unlock(&mdsc->mutex);\n\n\t\tmutex_lock(&session->s_mutex);\n\t\t__close_session(mdsc, session);\n\t\tif (session->s_state == CEPH_MDS_SESSION_CLOSING) {\n\t\t\tcleanup_session_requests(mdsc, session);\n\t\t\tremove_session_caps(session);\n\t\t}\n\t\tmutex_unlock(&session->s_mutex);\n\t\tceph_put_mds_session(session);\n\n\t\tmutex_lock(&mdsc->mutex);\n\t\tkick_requests(mdsc, mds);\n\t}\n\t__wake_requests(mdsc, &mdsc->waiting_for_map);\n\tmutex_unlock(&mdsc->mutex);\n}\n\nstatic void ceph_mdsc_stop(struct ceph_mds_client *mdsc)\n{\n\tdout(\"stop\\n\");\n\t \n\tflush_delayed_work(&mdsc->delayed_work);\n\n\tif (mdsc->mdsmap)\n\t\tceph_mdsmap_destroy(mdsc->mdsmap);\n\tkfree(mdsc->sessions);\n\tceph_caps_finalize(mdsc);\n\tceph_pool_perm_destroy(mdsc);\n}\n\nvoid ceph_mdsc_destroy(struct ceph_fs_client *fsc)\n{\n\tstruct ceph_mds_client *mdsc = fsc->mdsc;\n\tdout(\"mdsc_destroy %p\\n\", mdsc);\n\n\tif (!mdsc)\n\t\treturn;\n\n\t \n\tceph_msgr_flush();\n\n\tceph_mdsc_stop(mdsc);\n\n\tceph_metric_destroy(&mdsc->metric);\n\n\tfsc->mdsc = NULL;\n\tkfree(mdsc);\n\tdout(\"mdsc_destroy %p done\\n\", mdsc);\n}\n\nvoid ceph_mdsc_handle_fsmap(struct ceph_mds_client *mdsc, struct ceph_msg *msg)\n{\n\tstruct ceph_fs_client *fsc = mdsc->fsc;\n\tconst char *mds_namespace = fsc->mount_options->mds_namespace;\n\tvoid *p = msg->front.iov_base;\n\tvoid *end = p + msg->front.iov_len;\n\tu32 epoch;\n\tu32 num_fs;\n\tu32 mount_fscid = (u32)-1;\n\tint err = -EINVAL;\n\n\tceph_decode_need(&p, end, sizeof(u32), bad);\n\tepoch = ceph_decode_32(&p);\n\n\tdout(\"handle_fsmap epoch %u\\n\", epoch);\n\n\t \n\tceph_decode_skip_n(&p, end, 2 + sizeof(u32) * 3, bad);\n\n\tceph_decode_32_safe(&p, end, num_fs, bad);\n\twhile (num_fs-- > 0) {\n\t\tvoid *info_p, *info_end;\n\t\tu32 info_len;\n\t\tu32 fscid, namelen;\n\n\t\tceph_decode_need(&p, end, 2 + sizeof(u32), bad);\n\t\tp += 2;\t\t\n\t\tinfo_len = ceph_decode_32(&p);\n\t\tceph_decode_need(&p, end, info_len, bad);\n\t\tinfo_p = p;\n\t\tinfo_end = p + info_len;\n\t\tp = info_end;\n\n\t\tceph_decode_need(&info_p, info_end, sizeof(u32) * 2, bad);\n\t\tfscid = ceph_decode_32(&info_p);\n\t\tnamelen = ceph_decode_32(&info_p);\n\t\tceph_decode_need(&info_p, info_end, namelen, bad);\n\n\t\tif (mds_namespace &&\n\t\t    strlen(mds_namespace) == namelen &&\n\t\t    !strncmp(mds_namespace, (char *)info_p, namelen)) {\n\t\t\tmount_fscid = fscid;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tceph_monc_got_map(&fsc->client->monc, CEPH_SUB_FSMAP, epoch);\n\tif (mount_fscid != (u32)-1) {\n\t\tfsc->client->monc.fs_cluster_id = mount_fscid;\n\t\tceph_monc_want_map(&fsc->client->monc, CEPH_SUB_MDSMAP,\n\t\t\t\t   0, true);\n\t\tceph_monc_renew_subs(&fsc->client->monc);\n\t} else {\n\t\terr = -ENOENT;\n\t\tgoto err_out;\n\t}\n\treturn;\n\nbad:\n\tpr_err(\"error decoding fsmap %d. Shutting down mount.\\n\", err);\n\tceph_umount_begin(mdsc->fsc->sb);\n\tceph_msg_dump(msg);\nerr_out:\n\tmutex_lock(&mdsc->mutex);\n\tmdsc->mdsmap_err = err;\n\t__wake_requests(mdsc, &mdsc->waiting_for_map);\n\tmutex_unlock(&mdsc->mutex);\n}\n\n \nvoid ceph_mdsc_handle_mdsmap(struct ceph_mds_client *mdsc, struct ceph_msg *msg)\n{\n\tu32 epoch;\n\tu32 maplen;\n\tvoid *p = msg->front.iov_base;\n\tvoid *end = p + msg->front.iov_len;\n\tstruct ceph_mdsmap *newmap, *oldmap;\n\tstruct ceph_fsid fsid;\n\tint err = -EINVAL;\n\n\tceph_decode_need(&p, end, sizeof(fsid)+2*sizeof(u32), bad);\n\tceph_decode_copy(&p, &fsid, sizeof(fsid));\n\tif (ceph_check_fsid(mdsc->fsc->client, &fsid) < 0)\n\t\treturn;\n\tepoch = ceph_decode_32(&p);\n\tmaplen = ceph_decode_32(&p);\n\tdout(\"handle_map epoch %u len %d\\n\", epoch, (int)maplen);\n\n\t \n\tmutex_lock(&mdsc->mutex);\n\tif (mdsc->mdsmap && epoch <= mdsc->mdsmap->m_epoch) {\n\t\tdout(\"handle_map epoch %u <= our %u\\n\",\n\t\t     epoch, mdsc->mdsmap->m_epoch);\n\t\tmutex_unlock(&mdsc->mutex);\n\t\treturn;\n\t}\n\n\tnewmap = ceph_mdsmap_decode(&p, end, ceph_msgr2(mdsc->fsc->client));\n\tif (IS_ERR(newmap)) {\n\t\terr = PTR_ERR(newmap);\n\t\tgoto bad_unlock;\n\t}\n\n\t \n\tif (mdsc->mdsmap) {\n\t\toldmap = mdsc->mdsmap;\n\t\tmdsc->mdsmap = newmap;\n\t\tcheck_new_map(mdsc, newmap, oldmap);\n\t\tceph_mdsmap_destroy(oldmap);\n\t} else {\n\t\tmdsc->mdsmap = newmap;   \n\t}\n\tmdsc->fsc->max_file_size = min((loff_t)mdsc->mdsmap->m_max_file_size,\n\t\t\t\t\tMAX_LFS_FILESIZE);\n\n\t__wake_requests(mdsc, &mdsc->waiting_for_map);\n\tceph_monc_got_map(&mdsc->fsc->client->monc, CEPH_SUB_MDSMAP,\n\t\t\t  mdsc->mdsmap->m_epoch);\n\n\tmutex_unlock(&mdsc->mutex);\n\tschedule_delayed(mdsc, 0);\n\treturn;\n\nbad_unlock:\n\tmutex_unlock(&mdsc->mutex);\nbad:\n\tpr_err(\"error decoding mdsmap %d. Shutting down mount.\\n\", err);\n\tceph_umount_begin(mdsc->fsc->sb);\n\tceph_msg_dump(msg);\n\treturn;\n}\n\nstatic struct ceph_connection *mds_get_con(struct ceph_connection *con)\n{\n\tstruct ceph_mds_session *s = con->private;\n\n\tif (ceph_get_mds_session(s))\n\t\treturn con;\n\treturn NULL;\n}\n\nstatic void mds_put_con(struct ceph_connection *con)\n{\n\tstruct ceph_mds_session *s = con->private;\n\n\tceph_put_mds_session(s);\n}\n\n \nstatic void mds_peer_reset(struct ceph_connection *con)\n{\n\tstruct ceph_mds_session *s = con->private;\n\tstruct ceph_mds_client *mdsc = s->s_mdsc;\n\n\tpr_warn(\"mds%d closed our session\\n\", s->s_mds);\n\tif (READ_ONCE(mdsc->fsc->mount_state) != CEPH_MOUNT_FENCE_IO)\n\t\tsend_mds_reconnect(mdsc, s);\n}\n\nstatic void mds_dispatch(struct ceph_connection *con, struct ceph_msg *msg)\n{\n\tstruct ceph_mds_session *s = con->private;\n\tstruct ceph_mds_client *mdsc = s->s_mdsc;\n\tint type = le16_to_cpu(msg->hdr.type);\n\n\tmutex_lock(&mdsc->mutex);\n\tif (__verify_registered_session(mdsc, s) < 0) {\n\t\tmutex_unlock(&mdsc->mutex);\n\t\tgoto out;\n\t}\n\tmutex_unlock(&mdsc->mutex);\n\n\tswitch (type) {\n\tcase CEPH_MSG_MDS_MAP:\n\t\tceph_mdsc_handle_mdsmap(mdsc, msg);\n\t\tbreak;\n\tcase CEPH_MSG_FS_MAP_USER:\n\t\tceph_mdsc_handle_fsmap(mdsc, msg);\n\t\tbreak;\n\tcase CEPH_MSG_CLIENT_SESSION:\n\t\thandle_session(s, msg);\n\t\tbreak;\n\tcase CEPH_MSG_CLIENT_REPLY:\n\t\thandle_reply(s, msg);\n\t\tbreak;\n\tcase CEPH_MSG_CLIENT_REQUEST_FORWARD:\n\t\thandle_forward(mdsc, s, msg);\n\t\tbreak;\n\tcase CEPH_MSG_CLIENT_CAPS:\n\t\tceph_handle_caps(s, msg);\n\t\tbreak;\n\tcase CEPH_MSG_CLIENT_SNAP:\n\t\tceph_handle_snap(mdsc, s, msg);\n\t\tbreak;\n\tcase CEPH_MSG_CLIENT_LEASE:\n\t\thandle_lease(mdsc, s, msg);\n\t\tbreak;\n\tcase CEPH_MSG_CLIENT_QUOTA:\n\t\tceph_handle_quota(mdsc, s, msg);\n\t\tbreak;\n\n\tdefault:\n\t\tpr_err(\"received unknown message type %d %s\\n\", type,\n\t\t       ceph_msg_type_name(type));\n\t}\nout:\n\tceph_msg_put(msg);\n}\n\n \n\n \nstatic struct ceph_auth_handshake *\nmds_get_authorizer(struct ceph_connection *con, int *proto, int force_new)\n{\n\tstruct ceph_mds_session *s = con->private;\n\tstruct ceph_mds_client *mdsc = s->s_mdsc;\n\tstruct ceph_auth_client *ac = mdsc->fsc->client->monc.auth;\n\tstruct ceph_auth_handshake *auth = &s->s_auth;\n\tint ret;\n\n\tret = __ceph_auth_get_authorizer(ac, auth, CEPH_ENTITY_TYPE_MDS,\n\t\t\t\t\t force_new, proto, NULL, NULL);\n\tif (ret)\n\t\treturn ERR_PTR(ret);\n\n\treturn auth;\n}\n\nstatic int mds_add_authorizer_challenge(struct ceph_connection *con,\n\t\t\t\t    void *challenge_buf, int challenge_buf_len)\n{\n\tstruct ceph_mds_session *s = con->private;\n\tstruct ceph_mds_client *mdsc = s->s_mdsc;\n\tstruct ceph_auth_client *ac = mdsc->fsc->client->monc.auth;\n\n\treturn ceph_auth_add_authorizer_challenge(ac, s->s_auth.authorizer,\n\t\t\t\t\t    challenge_buf, challenge_buf_len);\n}\n\nstatic int mds_verify_authorizer_reply(struct ceph_connection *con)\n{\n\tstruct ceph_mds_session *s = con->private;\n\tstruct ceph_mds_client *mdsc = s->s_mdsc;\n\tstruct ceph_auth_client *ac = mdsc->fsc->client->monc.auth;\n\tstruct ceph_auth_handshake *auth = &s->s_auth;\n\n\treturn ceph_auth_verify_authorizer_reply(ac, auth->authorizer,\n\t\tauth->authorizer_reply_buf, auth->authorizer_reply_buf_len,\n\t\tNULL, NULL, NULL, NULL);\n}\n\nstatic int mds_invalidate_authorizer(struct ceph_connection *con)\n{\n\tstruct ceph_mds_session *s = con->private;\n\tstruct ceph_mds_client *mdsc = s->s_mdsc;\n\tstruct ceph_auth_client *ac = mdsc->fsc->client->monc.auth;\n\n\tceph_auth_invalidate_authorizer(ac, CEPH_ENTITY_TYPE_MDS);\n\n\treturn ceph_monc_validate_auth(&mdsc->fsc->client->monc);\n}\n\nstatic int mds_get_auth_request(struct ceph_connection *con,\n\t\t\t\tvoid *buf, int *buf_len,\n\t\t\t\tvoid **authorizer, int *authorizer_len)\n{\n\tstruct ceph_mds_session *s = con->private;\n\tstruct ceph_auth_client *ac = s->s_mdsc->fsc->client->monc.auth;\n\tstruct ceph_auth_handshake *auth = &s->s_auth;\n\tint ret;\n\n\tret = ceph_auth_get_authorizer(ac, auth, CEPH_ENTITY_TYPE_MDS,\n\t\t\t\t       buf, buf_len);\n\tif (ret)\n\t\treturn ret;\n\n\t*authorizer = auth->authorizer_buf;\n\t*authorizer_len = auth->authorizer_buf_len;\n\treturn 0;\n}\n\nstatic int mds_handle_auth_reply_more(struct ceph_connection *con,\n\t\t\t\t      void *reply, int reply_len,\n\t\t\t\t      void *buf, int *buf_len,\n\t\t\t\t      void **authorizer, int *authorizer_len)\n{\n\tstruct ceph_mds_session *s = con->private;\n\tstruct ceph_auth_client *ac = s->s_mdsc->fsc->client->monc.auth;\n\tstruct ceph_auth_handshake *auth = &s->s_auth;\n\tint ret;\n\n\tret = ceph_auth_handle_svc_reply_more(ac, auth, reply, reply_len,\n\t\t\t\t\t      buf, buf_len);\n\tif (ret)\n\t\treturn ret;\n\n\t*authorizer = auth->authorizer_buf;\n\t*authorizer_len = auth->authorizer_buf_len;\n\treturn 0;\n}\n\nstatic int mds_handle_auth_done(struct ceph_connection *con,\n\t\t\t\tu64 global_id, void *reply, int reply_len,\n\t\t\t\tu8 *session_key, int *session_key_len,\n\t\t\t\tu8 *con_secret, int *con_secret_len)\n{\n\tstruct ceph_mds_session *s = con->private;\n\tstruct ceph_auth_client *ac = s->s_mdsc->fsc->client->monc.auth;\n\tstruct ceph_auth_handshake *auth = &s->s_auth;\n\n\treturn ceph_auth_handle_svc_reply_done(ac, auth, reply, reply_len,\n\t\t\t\t\t       session_key, session_key_len,\n\t\t\t\t\t       con_secret, con_secret_len);\n}\n\nstatic int mds_handle_auth_bad_method(struct ceph_connection *con,\n\t\t\t\t      int used_proto, int result,\n\t\t\t\t      const int *allowed_protos, int proto_cnt,\n\t\t\t\t      const int *allowed_modes, int mode_cnt)\n{\n\tstruct ceph_mds_session *s = con->private;\n\tstruct ceph_mon_client *monc = &s->s_mdsc->fsc->client->monc;\n\tint ret;\n\n\tif (ceph_auth_handle_bad_authorizer(monc->auth, CEPH_ENTITY_TYPE_MDS,\n\t\t\t\t\t    used_proto, result,\n\t\t\t\t\t    allowed_protos, proto_cnt,\n\t\t\t\t\t    allowed_modes, mode_cnt)) {\n\t\tret = ceph_monc_validate_auth(monc);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn -EACCES;\n}\n\nstatic struct ceph_msg *mds_alloc_msg(struct ceph_connection *con,\n\t\t\t\tstruct ceph_msg_header *hdr, int *skip)\n{\n\tstruct ceph_msg *msg;\n\tint type = (int) le16_to_cpu(hdr->type);\n\tint front_len = (int) le32_to_cpu(hdr->front_len);\n\n\tif (con->in_msg)\n\t\treturn con->in_msg;\n\n\t*skip = 0;\n\tmsg = ceph_msg_new(type, front_len, GFP_NOFS, false);\n\tif (!msg) {\n\t\tpr_err(\"unable to allocate msg type %d len %d\\n\",\n\t\t       type, front_len);\n\t\treturn NULL;\n\t}\n\n\treturn msg;\n}\n\nstatic int mds_sign_message(struct ceph_msg *msg)\n{\n       struct ceph_mds_session *s = msg->con->private;\n       struct ceph_auth_handshake *auth = &s->s_auth;\n\n       return ceph_auth_sign_message(auth, msg);\n}\n\nstatic int mds_check_message_signature(struct ceph_msg *msg)\n{\n       struct ceph_mds_session *s = msg->con->private;\n       struct ceph_auth_handshake *auth = &s->s_auth;\n\n       return ceph_auth_check_message_signature(auth, msg);\n}\n\nstatic const struct ceph_connection_operations mds_con_ops = {\n\t.get = mds_get_con,\n\t.put = mds_put_con,\n\t.alloc_msg = mds_alloc_msg,\n\t.dispatch = mds_dispatch,\n\t.peer_reset = mds_peer_reset,\n\t.get_authorizer = mds_get_authorizer,\n\t.add_authorizer_challenge = mds_add_authorizer_challenge,\n\t.verify_authorizer_reply = mds_verify_authorizer_reply,\n\t.invalidate_authorizer = mds_invalidate_authorizer,\n\t.sign_message = mds_sign_message,\n\t.check_message_signature = mds_check_message_signature,\n\t.get_auth_request = mds_get_auth_request,\n\t.handle_auth_reply_more = mds_handle_auth_reply_more,\n\t.handle_auth_done = mds_handle_auth_done,\n\t.handle_auth_bad_method = mds_handle_auth_bad_method,\n};\n\n \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}