{
  "module_name": "addr.c",
  "hash_id": "cf7782e8f92cd1b68c561e2dbea6e4f49c6ac8115bd0fe4fadab979be2b7eda2",
  "original_prompt": "Ingested from linux-6.6.14/fs/ceph/addr.c",
  "human_readable_source": "\n#include <linux/ceph/ceph_debug.h>\n\n#include <linux/backing-dev.h>\n#include <linux/fs.h>\n#include <linux/mm.h>\n#include <linux/swap.h>\n#include <linux/pagemap.h>\n#include <linux/slab.h>\n#include <linux/pagevec.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/signal.h>\n#include <linux/iversion.h>\n#include <linux/ktime.h>\n#include <linux/netfs.h>\n\n#include \"super.h\"\n#include \"mds_client.h\"\n#include \"cache.h\"\n#include \"metric.h\"\n#include \"crypto.h\"\n#include <linux/ceph/osd_client.h>\n#include <linux/ceph/striper.h>\n\n \n\n#define CONGESTION_ON_THRESH(congestion_kb) (congestion_kb >> (PAGE_SHIFT-10))\n#define CONGESTION_OFF_THRESH(congestion_kb)\t\t\t\t\\\n\t(CONGESTION_ON_THRESH(congestion_kb) -\t\t\t\t\\\n\t (CONGESTION_ON_THRESH(congestion_kb) >> 2))\n\nstatic int ceph_netfs_check_write_begin(struct file *file, loff_t pos, unsigned int len,\n\t\t\t\t\tstruct folio **foliop, void **_fsdata);\n\nstatic inline struct ceph_snap_context *page_snap_context(struct page *page)\n{\n\tif (PagePrivate(page))\n\t\treturn (void *)page->private;\n\treturn NULL;\n}\n\n \nstatic bool ceph_dirty_folio(struct address_space *mapping, struct folio *folio)\n{\n\tstruct inode *inode;\n\tstruct ceph_inode_info *ci;\n\tstruct ceph_snap_context *snapc;\n\n\tif (folio_test_dirty(folio)) {\n\t\tdout(\"%p dirty_folio %p idx %lu -- already dirty\\n\",\n\t\t     mapping->host, folio, folio->index);\n\t\tVM_BUG_ON_FOLIO(!folio_test_private(folio), folio);\n\t\treturn false;\n\t}\n\n\tinode = mapping->host;\n\tci = ceph_inode(inode);\n\n\t \n\tspin_lock(&ci->i_ceph_lock);\n\tBUG_ON(ci->i_wr_ref == 0);  \n\tif (__ceph_have_pending_cap_snap(ci)) {\n\t\tstruct ceph_cap_snap *capsnap =\n\t\t\t\tlist_last_entry(&ci->i_cap_snaps,\n\t\t\t\t\t\tstruct ceph_cap_snap,\n\t\t\t\t\t\tci_item);\n\t\tsnapc = ceph_get_snap_context(capsnap->context);\n\t\tcapsnap->dirty_pages++;\n\t} else {\n\t\tBUG_ON(!ci->i_head_snapc);\n\t\tsnapc = ceph_get_snap_context(ci->i_head_snapc);\n\t\t++ci->i_wrbuffer_ref_head;\n\t}\n\tif (ci->i_wrbuffer_ref == 0)\n\t\tihold(inode);\n\t++ci->i_wrbuffer_ref;\n\tdout(\"%p dirty_folio %p idx %lu head %d/%d -> %d/%d \"\n\t     \"snapc %p seq %lld (%d snaps)\\n\",\n\t     mapping->host, folio, folio->index,\n\t     ci->i_wrbuffer_ref-1, ci->i_wrbuffer_ref_head-1,\n\t     ci->i_wrbuffer_ref, ci->i_wrbuffer_ref_head,\n\t     snapc, snapc->seq, snapc->num_snaps);\n\tspin_unlock(&ci->i_ceph_lock);\n\n\t \n\tVM_WARN_ON_FOLIO(folio->private, folio);\n\tfolio_attach_private(folio, snapc);\n\n\treturn ceph_fscache_dirty_folio(mapping, folio);\n}\n\n \nstatic void ceph_invalidate_folio(struct folio *folio, size_t offset,\n\t\t\t\tsize_t length)\n{\n\tstruct inode *inode;\n\tstruct ceph_inode_info *ci;\n\tstruct ceph_snap_context *snapc;\n\n\tinode = folio->mapping->host;\n\tci = ceph_inode(inode);\n\n\tif (offset != 0 || length != folio_size(folio)) {\n\t\tdout(\"%p invalidate_folio idx %lu partial dirty page %zu~%zu\\n\",\n\t\t     inode, folio->index, offset, length);\n\t\treturn;\n\t}\n\n\tWARN_ON(!folio_test_locked(folio));\n\tif (folio_test_private(folio)) {\n\t\tdout(\"%p invalidate_folio idx %lu full dirty page\\n\",\n\t\t     inode, folio->index);\n\n\t\tsnapc = folio_detach_private(folio);\n\t\tceph_put_wrbuffer_cap_refs(ci, 1, snapc);\n\t\tceph_put_snap_context(snapc);\n\t}\n\n\tfolio_wait_fscache(folio);\n}\n\nstatic bool ceph_release_folio(struct folio *folio, gfp_t gfp)\n{\n\tstruct inode *inode = folio->mapping->host;\n\n\tdout(\"%llx:%llx release_folio idx %lu (%sdirty)\\n\",\n\t     ceph_vinop(inode),\n\t     folio->index, folio_test_dirty(folio) ? \"\" : \"not \");\n\n\tif (folio_test_private(folio))\n\t\treturn false;\n\n\tif (folio_test_fscache(folio)) {\n\t\tif (current_is_kswapd() || !(gfp & __GFP_FS))\n\t\t\treturn false;\n\t\tfolio_wait_fscache(folio);\n\t}\n\tceph_fscache_note_page_release(inode);\n\treturn true;\n}\n\nstatic void ceph_netfs_expand_readahead(struct netfs_io_request *rreq)\n{\n\tstruct inode *inode = rreq->inode;\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tstruct ceph_file_layout *lo = &ci->i_layout;\n\tunsigned long max_pages = inode->i_sb->s_bdi->ra_pages;\n\tloff_t end = rreq->start + rreq->len, new_end;\n\tstruct ceph_netfs_request_data *priv = rreq->netfs_priv;\n\tunsigned long max_len;\n\tu32 blockoff;\n\n\tif (priv) {\n\t\t \n\t\tif (priv->file_ra_disabled)\n\t\t\tmax_pages = 0;\n\t\telse\n\t\t\tmax_pages = priv->file_ra_pages;\n\n\t}\n\n\t \n\tif (!max_pages)\n\t\treturn;\n\n\tmax_len = max_pages << PAGE_SHIFT;\n\n\t \n\tnew_end = min(round_up(end, lo->stripe_unit), rreq->i_size);\n\tif (new_end > end && new_end <= rreq->start + max_len)\n\t\trreq->len = new_end - rreq->start;\n\n\t \n\tdiv_u64_rem(rreq->start, lo->stripe_unit, &blockoff);\n\tif (rreq->len + blockoff <= max_len) {\n\t\trreq->start -= blockoff;\n\t\trreq->len += blockoff;\n\t}\n}\n\nstatic bool ceph_netfs_clamp_length(struct netfs_io_subrequest *subreq)\n{\n\tstruct inode *inode = subreq->rreq->inode;\n\tstruct ceph_fs_client *fsc = ceph_inode_to_client(inode);\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tu64 objno, objoff;\n\tu32 xlen;\n\n\t \n\tceph_calc_file_object_mapping(&ci->i_layout, subreq->start, subreq->len,\n\t\t\t\t      &objno, &objoff, &xlen);\n\tsubreq->len = min(xlen, fsc->mount_options->rsize);\n\treturn true;\n}\n\nstatic void finish_netfs_read(struct ceph_osd_request *req)\n{\n\tstruct inode *inode = req->r_inode;\n\tstruct ceph_fs_client *fsc = ceph_inode_to_client(inode);\n\tstruct ceph_osd_data *osd_data = osd_req_op_extent_osd_data(req, 0);\n\tstruct netfs_io_subrequest *subreq = req->r_priv;\n\tstruct ceph_osd_req_op *op = &req->r_ops[0];\n\tint err = req->r_result;\n\tbool sparse = (op->op == CEPH_OSD_OP_SPARSE_READ);\n\n\tceph_update_read_metrics(&fsc->mdsc->metric, req->r_start_latency,\n\t\t\t\t req->r_end_latency, osd_data->length, err);\n\n\tdout(\"%s: result %d subreq->len=%zu i_size=%lld\\n\", __func__, req->r_result,\n\t     subreq->len, i_size_read(req->r_inode));\n\n\t \n\tif (err == -ENOENT)\n\t\terr = 0;\n\telse if (err == -EBLOCKLISTED)\n\t\tfsc->blocklisted = true;\n\n\tif (err >= 0) {\n\t\tif (sparse && err > 0)\n\t\t\terr = ceph_sparse_ext_map_end(op);\n\t\tif (err < subreq->len)\n\t\t\t__set_bit(NETFS_SREQ_CLEAR_TAIL, &subreq->flags);\n\t\tif (IS_ENCRYPTED(inode) && err > 0) {\n\t\t\terr = ceph_fscrypt_decrypt_extents(inode,\n\t\t\t\t\tosd_data->pages, subreq->start,\n\t\t\t\t\top->extent.sparse_ext,\n\t\t\t\t\top->extent.sparse_ext_cnt);\n\t\t\tif (err > subreq->len)\n\t\t\t\terr = subreq->len;\n\t\t}\n\t}\n\n\tif (osd_data->type == CEPH_OSD_DATA_TYPE_PAGES) {\n\t\tceph_put_page_vector(osd_data->pages,\n\t\t\t\t     calc_pages_for(osd_data->alignment,\n\t\t\t\t\tosd_data->length), false);\n\t}\n\tnetfs_subreq_terminated(subreq, err, false);\n\tiput(req->r_inode);\n\tceph_dec_osd_stopping_blocker(fsc->mdsc);\n}\n\nstatic bool ceph_netfs_issue_op_inline(struct netfs_io_subrequest *subreq)\n{\n\tstruct netfs_io_request *rreq = subreq->rreq;\n\tstruct inode *inode = rreq->inode;\n\tstruct ceph_mds_reply_info_parsed *rinfo;\n\tstruct ceph_mds_reply_info_in *iinfo;\n\tstruct ceph_mds_request *req;\n\tstruct ceph_mds_client *mdsc = ceph_sb_to_mdsc(inode->i_sb);\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tstruct iov_iter iter;\n\tssize_t err = 0;\n\tsize_t len;\n\tint mode;\n\n\t__set_bit(NETFS_SREQ_CLEAR_TAIL, &subreq->flags);\n\t__clear_bit(NETFS_SREQ_COPY_TO_CACHE, &subreq->flags);\n\n\tif (subreq->start >= inode->i_size)\n\t\tgoto out;\n\n\t \n\tmode = ceph_try_to_choose_auth_mds(inode, CEPH_STAT_CAP_INLINE_DATA);\n\treq = ceph_mdsc_create_request(mdsc, CEPH_MDS_OP_GETATTR, mode);\n\tif (IS_ERR(req)) {\n\t\terr = PTR_ERR(req);\n\t\tgoto out;\n\t}\n\treq->r_ino1 = ci->i_vino;\n\treq->r_args.getattr.mask = cpu_to_le32(CEPH_STAT_CAP_INLINE_DATA);\n\treq->r_num_caps = 2;\n\n\terr = ceph_mdsc_do_request(mdsc, NULL, req);\n\tif (err < 0)\n\t\tgoto out;\n\n\trinfo = &req->r_reply_info;\n\tiinfo = &rinfo->targeti;\n\tif (iinfo->inline_version == CEPH_INLINE_NONE) {\n\t\t \n\t\tceph_mdsc_put_request(req);\n\t\treturn false;\n\t}\n\n\tlen = min_t(size_t, iinfo->inline_len - subreq->start, subreq->len);\n\tiov_iter_xarray(&iter, ITER_DEST, &rreq->mapping->i_pages, subreq->start, len);\n\terr = copy_to_iter(iinfo->inline_data + subreq->start, len, &iter);\n\tif (err == 0)\n\t\terr = -EFAULT;\n\n\tceph_mdsc_put_request(req);\nout:\n\tnetfs_subreq_terminated(subreq, err, false);\n\treturn true;\n}\n\nstatic void ceph_netfs_issue_read(struct netfs_io_subrequest *subreq)\n{\n\tstruct netfs_io_request *rreq = subreq->rreq;\n\tstruct inode *inode = rreq->inode;\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tstruct ceph_fs_client *fsc = ceph_inode_to_client(inode);\n\tstruct ceph_osd_request *req = NULL;\n\tstruct ceph_vino vino = ceph_vino(inode);\n\tstruct iov_iter iter;\n\tint err = 0;\n\tu64 len = subreq->len;\n\tbool sparse = IS_ENCRYPTED(inode) || ceph_test_mount_opt(fsc, SPARSEREAD);\n\tu64 off = subreq->start;\n\n\tif (ceph_inode_is_shutdown(inode)) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tif (ceph_has_inline_data(ci) && ceph_netfs_issue_op_inline(subreq))\n\t\treturn;\n\n\tceph_fscrypt_adjust_off_and_len(inode, &off, &len);\n\n\treq = ceph_osdc_new_request(&fsc->client->osdc, &ci->i_layout, vino,\n\t\t\toff, &len, 0, 1, sparse ? CEPH_OSD_OP_SPARSE_READ : CEPH_OSD_OP_READ,\n\t\t\tCEPH_OSD_FLAG_READ | fsc->client->osdc.client->options->read_from_replica,\n\t\t\tNULL, ci->i_truncate_seq, ci->i_truncate_size, false);\n\tif (IS_ERR(req)) {\n\t\terr = PTR_ERR(req);\n\t\treq = NULL;\n\t\tgoto out;\n\t}\n\n\tif (sparse) {\n\t\terr = ceph_alloc_sparse_ext_map(&req->r_ops[0]);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tdout(\"%s: pos=%llu orig_len=%zu len=%llu\\n\", __func__, subreq->start, subreq->len, len);\n\n\tiov_iter_xarray(&iter, ITER_DEST, &rreq->mapping->i_pages, subreq->start, len);\n\n\t \n\tif (IS_ENCRYPTED(inode)) {\n\t\tstruct page **pages;\n\t\tsize_t page_off;\n\n\t\terr = iov_iter_get_pages_alloc2(&iter, &pages, len, &page_off);\n\t\tif (err < 0) {\n\t\t\tdout(\"%s: iov_ter_get_pages_alloc returned %d\\n\",\n\t\t\t     __func__, err);\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\tWARN_ON_ONCE(page_off);\n\t\tlen = err;\n\t\terr = 0;\n\n\t\tosd_req_op_extent_osd_data_pages(req, 0, pages, len, 0, false,\n\t\t\t\t\t\t false);\n\t} else {\n\t\tosd_req_op_extent_osd_iter(req, 0, &iter);\n\t}\n\tif (!ceph_inc_osd_stopping_blocker(fsc->mdsc)) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\treq->r_callback = finish_netfs_read;\n\treq->r_priv = subreq;\n\treq->r_inode = inode;\n\tihold(inode);\n\n\tceph_osdc_start_request(req->r_osdc, req);\nout:\n\tceph_osdc_put_request(req);\n\tif (err)\n\t\tnetfs_subreq_terminated(subreq, err, false);\n\tdout(\"%s: result %d\\n\", __func__, err);\n}\n\nstatic int ceph_init_request(struct netfs_io_request *rreq, struct file *file)\n{\n\tstruct inode *inode = rreq->inode;\n\tint got = 0, want = CEPH_CAP_FILE_CACHE;\n\tstruct ceph_netfs_request_data *priv;\n\tint ret = 0;\n\n\tif (rreq->origin != NETFS_READAHEAD)\n\t\treturn 0;\n\n\tpriv = kzalloc(sizeof(*priv), GFP_NOFS);\n\tif (!priv)\n\t\treturn -ENOMEM;\n\n\tif (file) {\n\t\tstruct ceph_rw_context *rw_ctx;\n\t\tstruct ceph_file_info *fi = file->private_data;\n\n\t\tpriv->file_ra_pages = file->f_ra.ra_pages;\n\t\tpriv->file_ra_disabled = file->f_mode & FMODE_RANDOM;\n\n\t\trw_ctx = ceph_find_rw_context(fi);\n\t\tif (rw_ctx) {\n\t\t\trreq->netfs_priv = priv;\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\tret = ceph_try_get_caps(inode, CEPH_CAP_FILE_RD, want, true, &got);\n\tif (ret < 0) {\n\t\tdout(\"start_read %p, error getting cap\\n\", inode);\n\t\tgoto out;\n\t}\n\n\tif (!(got & want)) {\n\t\tdout(\"start_read %p, no cache cap\\n\", inode);\n\t\tret = -EACCES;\n\t\tgoto out;\n\t}\n\tif (ret == 0) {\n\t\tret = -EACCES;\n\t\tgoto out;\n\t}\n\n\tpriv->caps = got;\n\trreq->netfs_priv = priv;\n\nout:\n\tif (ret < 0)\n\t\tkfree(priv);\n\n\treturn ret;\n}\n\nstatic void ceph_netfs_free_request(struct netfs_io_request *rreq)\n{\n\tstruct ceph_netfs_request_data *priv = rreq->netfs_priv;\n\n\tif (!priv)\n\t\treturn;\n\n\tif (priv->caps)\n\t\tceph_put_cap_refs(ceph_inode(rreq->inode), priv->caps);\n\tkfree(priv);\n\trreq->netfs_priv = NULL;\n}\n\nconst struct netfs_request_ops ceph_netfs_ops = {\n\t.init_request\t\t= ceph_init_request,\n\t.free_request\t\t= ceph_netfs_free_request,\n\t.begin_cache_operation\t= ceph_begin_cache_operation,\n\t.issue_read\t\t= ceph_netfs_issue_read,\n\t.expand_readahead\t= ceph_netfs_expand_readahead,\n\t.clamp_length\t\t= ceph_netfs_clamp_length,\n\t.check_write_begin\t= ceph_netfs_check_write_begin,\n};\n\n#ifdef CONFIG_CEPH_FSCACHE\nstatic void ceph_set_page_fscache(struct page *page)\n{\n\tset_page_fscache(page);\n}\n\nstatic void ceph_fscache_write_terminated(void *priv, ssize_t error, bool was_async)\n{\n\tstruct inode *inode = priv;\n\n\tif (IS_ERR_VALUE(error) && error != -ENOBUFS)\n\t\tceph_fscache_invalidate(inode, false);\n}\n\nstatic void ceph_fscache_write_to_cache(struct inode *inode, u64 off, u64 len, bool caching)\n{\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tstruct fscache_cookie *cookie = ceph_fscache_cookie(ci);\n\n\tfscache_write_to_cache(cookie, inode->i_mapping, off, len, i_size_read(inode),\n\t\t\t       ceph_fscache_write_terminated, inode, caching);\n}\n#else\nstatic inline void ceph_set_page_fscache(struct page *page)\n{\n}\n\nstatic inline void ceph_fscache_write_to_cache(struct inode *inode, u64 off, u64 len, bool caching)\n{\n}\n#endif  \n\nstruct ceph_writeback_ctl\n{\n\tloff_t i_size;\n\tu64 truncate_size;\n\tu32 truncate_seq;\n\tbool size_stable;\n\tbool head_snapc;\n};\n\n \nstatic struct ceph_snap_context *\nget_oldest_context(struct inode *inode, struct ceph_writeback_ctl *ctl,\n\t\t   struct ceph_snap_context *page_snapc)\n{\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tstruct ceph_snap_context *snapc = NULL;\n\tstruct ceph_cap_snap *capsnap = NULL;\n\n\tspin_lock(&ci->i_ceph_lock);\n\tlist_for_each_entry(capsnap, &ci->i_cap_snaps, ci_item) {\n\t\tdout(\" cap_snap %p snapc %p has %d dirty pages\\n\", capsnap,\n\t\t     capsnap->context, capsnap->dirty_pages);\n\t\tif (!capsnap->dirty_pages)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (snapc && capsnap->context != page_snapc)\n\t\t\tcontinue;\n\n\t\tif (ctl) {\n\t\t\tif (capsnap->writing) {\n\t\t\t\tctl->i_size = i_size_read(inode);\n\t\t\t\tctl->size_stable = false;\n\t\t\t} else {\n\t\t\t\tctl->i_size = capsnap->size;\n\t\t\t\tctl->size_stable = true;\n\t\t\t}\n\t\t\tctl->truncate_size = capsnap->truncate_size;\n\t\t\tctl->truncate_seq = capsnap->truncate_seq;\n\t\t\tctl->head_snapc = false;\n\t\t}\n\n\t\tif (snapc)\n\t\t\tbreak;\n\n\t\tsnapc = ceph_get_snap_context(capsnap->context);\n\t\tif (!page_snapc ||\n\t\t    page_snapc == snapc ||\n\t\t    page_snapc->seq > snapc->seq)\n\t\t\tbreak;\n\t}\n\tif (!snapc && ci->i_wrbuffer_ref_head) {\n\t\tsnapc = ceph_get_snap_context(ci->i_head_snapc);\n\t\tdout(\" head snapc %p has %d dirty pages\\n\",\n\t\t     snapc, ci->i_wrbuffer_ref_head);\n\t\tif (ctl) {\n\t\t\tctl->i_size = i_size_read(inode);\n\t\t\tctl->truncate_size = ci->i_truncate_size;\n\t\t\tctl->truncate_seq = ci->i_truncate_seq;\n\t\t\tctl->size_stable = false;\n\t\t\tctl->head_snapc = true;\n\t\t}\n\t}\n\tspin_unlock(&ci->i_ceph_lock);\n\treturn snapc;\n}\n\nstatic u64 get_writepages_data_length(struct inode *inode,\n\t\t\t\t      struct page *page, u64 start)\n{\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tstruct ceph_snap_context *snapc;\n\tstruct ceph_cap_snap *capsnap = NULL;\n\tu64 end = i_size_read(inode);\n\tu64 ret;\n\n\tsnapc = page_snap_context(ceph_fscrypt_pagecache_page(page));\n\tif (snapc != ci->i_head_snapc) {\n\t\tbool found = false;\n\t\tspin_lock(&ci->i_ceph_lock);\n\t\tlist_for_each_entry(capsnap, &ci->i_cap_snaps, ci_item) {\n\t\t\tif (capsnap->context == snapc) {\n\t\t\t\tif (!capsnap->writing)\n\t\t\t\t\tend = capsnap->size;\n\t\t\t\tfound = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&ci->i_ceph_lock);\n\t\tWARN_ON(!found);\n\t}\n\tif (end > ceph_fscrypt_page_offset(page) + thp_size(page))\n\t\tend = ceph_fscrypt_page_offset(page) + thp_size(page);\n\tret = end > start ? end - start : 0;\n\tif (ret && fscrypt_is_bounce_page(page))\n\t\tret = round_up(ret, CEPH_FSCRYPT_BLOCK_SIZE);\n\treturn ret;\n}\n\n \nstatic int writepage_nounlock(struct page *page, struct writeback_control *wbc)\n{\n\tstruct folio *folio = page_folio(page);\n\tstruct inode *inode = page->mapping->host;\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tstruct ceph_fs_client *fsc = ceph_inode_to_client(inode);\n\tstruct ceph_snap_context *snapc, *oldest;\n\tloff_t page_off = page_offset(page);\n\tint err;\n\tloff_t len = thp_size(page);\n\tloff_t wlen;\n\tstruct ceph_writeback_ctl ceph_wbc;\n\tstruct ceph_osd_client *osdc = &fsc->client->osdc;\n\tstruct ceph_osd_request *req;\n\tbool caching = ceph_is_cache_enabled(inode);\n\tstruct page *bounce_page = NULL;\n\n\tdout(\"writepage %p idx %lu\\n\", page, page->index);\n\n\tif (ceph_inode_is_shutdown(inode))\n\t\treturn -EIO;\n\n\t \n\tsnapc = page_snap_context(page);\n\tif (!snapc) {\n\t\tdout(\"writepage %p page %p not dirty?\\n\", inode, page);\n\t\treturn 0;\n\t}\n\toldest = get_oldest_context(inode, &ceph_wbc, snapc);\n\tif (snapc->seq > oldest->seq) {\n\t\tdout(\"writepage %p page %p snapc %p not writeable - noop\\n\",\n\t\t     inode, page, snapc);\n\t\t \n\t\tWARN_ON(!(current->flags & PF_MEMALLOC));\n\t\tceph_put_snap_context(oldest);\n\t\tredirty_page_for_writepage(wbc, page);\n\t\treturn 0;\n\t}\n\tceph_put_snap_context(oldest);\n\n\t \n\tif (page_off >= ceph_wbc.i_size) {\n\t\tdout(\"folio at %lu beyond eof %llu\\n\", folio->index,\n\t\t\t\tceph_wbc.i_size);\n\t\tfolio_invalidate(folio, 0, folio_size(folio));\n\t\treturn 0;\n\t}\n\n\tif (ceph_wbc.i_size < page_off + len)\n\t\tlen = ceph_wbc.i_size - page_off;\n\n\twlen = IS_ENCRYPTED(inode) ? round_up(len, CEPH_FSCRYPT_BLOCK_SIZE) : len;\n\tdout(\"writepage %p page %p index %lu on %llu~%llu snapc %p seq %lld\\n\",\n\t     inode, page, page->index, page_off, wlen, snapc, snapc->seq);\n\n\tif (atomic_long_inc_return(&fsc->writeback_count) >\n\t    CONGESTION_ON_THRESH(fsc->mount_options->congestion_kb))\n\t\tfsc->write_congested = true;\n\n\treq = ceph_osdc_new_request(osdc, &ci->i_layout, ceph_vino(inode),\n\t\t\t\t    page_off, &wlen, 0, 1, CEPH_OSD_OP_WRITE,\n\t\t\t\t    CEPH_OSD_FLAG_WRITE, snapc,\n\t\t\t\t    ceph_wbc.truncate_seq,\n\t\t\t\t    ceph_wbc.truncate_size, true);\n\tif (IS_ERR(req)) {\n\t\tredirty_page_for_writepage(wbc, page);\n\t\treturn PTR_ERR(req);\n\t}\n\n\tif (wlen < len)\n\t\tlen = wlen;\n\n\tset_page_writeback(page);\n\tif (caching)\n\t\tceph_set_page_fscache(page);\n\tceph_fscache_write_to_cache(inode, page_off, len, caching);\n\n\tif (IS_ENCRYPTED(inode)) {\n\t\tbounce_page = fscrypt_encrypt_pagecache_blocks(page,\n\t\t\t\t\t\t    CEPH_FSCRYPT_BLOCK_SIZE, 0,\n\t\t\t\t\t\t    GFP_NOFS);\n\t\tif (IS_ERR(bounce_page)) {\n\t\t\tredirty_page_for_writepage(wbc, page);\n\t\t\tend_page_writeback(page);\n\t\t\tceph_osdc_put_request(req);\n\t\t\treturn PTR_ERR(bounce_page);\n\t\t}\n\t}\n\n\t \n\tWARN_ON_ONCE(len > thp_size(page));\n\tosd_req_op_extent_osd_data_pages(req, 0,\n\t\t\tbounce_page ? &bounce_page : &page, wlen, 0,\n\t\t\tfalse, false);\n\tdout(\"writepage %llu~%llu (%llu bytes, %sencrypted)\\n\",\n\t     page_off, len, wlen, IS_ENCRYPTED(inode) ? \"\" : \"not \");\n\n\treq->r_mtime = inode->i_mtime;\n\tceph_osdc_start_request(osdc, req);\n\terr = ceph_osdc_wait_request(osdc, req);\n\n\tceph_update_write_metrics(&fsc->mdsc->metric, req->r_start_latency,\n\t\t\t\t  req->r_end_latency, len, err);\n\tfscrypt_free_bounce_page(bounce_page);\n\tceph_osdc_put_request(req);\n\tif (err == 0)\n\t\terr = len;\n\n\tif (err < 0) {\n\t\tstruct writeback_control tmp_wbc;\n\t\tif (!wbc)\n\t\t\twbc = &tmp_wbc;\n\t\tif (err == -ERESTARTSYS) {\n\t\t\t \n\t\t\tdout(\"writepage interrupted page %p\\n\", page);\n\t\t\tredirty_page_for_writepage(wbc, page);\n\t\t\tend_page_writeback(page);\n\t\t\treturn err;\n\t\t}\n\t\tif (err == -EBLOCKLISTED)\n\t\t\tfsc->blocklisted = true;\n\t\tdout(\"writepage setting page/mapping error %d %p\\n\",\n\t\t     err, page);\n\t\tmapping_set_error(&inode->i_data, err);\n\t\twbc->pages_skipped++;\n\t} else {\n\t\tdout(\"writepage cleaned page %p\\n\", page);\n\t\terr = 0;   \n\t}\n\toldest = detach_page_private(page);\n\tWARN_ON_ONCE(oldest != snapc);\n\tend_page_writeback(page);\n\tceph_put_wrbuffer_cap_refs(ci, 1, snapc);\n\tceph_put_snap_context(snapc);   \n\n\tif (atomic_long_dec_return(&fsc->writeback_count) <\n\t    CONGESTION_OFF_THRESH(fsc->mount_options->congestion_kb))\n\t\tfsc->write_congested = false;\n\n\treturn err;\n}\n\nstatic int ceph_writepage(struct page *page, struct writeback_control *wbc)\n{\n\tint err;\n\tstruct inode *inode = page->mapping->host;\n\tBUG_ON(!inode);\n\tihold(inode);\n\n\tif (wbc->sync_mode == WB_SYNC_NONE &&\n\t    ceph_inode_to_client(inode)->write_congested)\n\t\treturn AOP_WRITEPAGE_ACTIVATE;\n\n\twait_on_page_fscache(page);\n\n\terr = writepage_nounlock(page, wbc);\n\tif (err == -ERESTARTSYS) {\n\t\t \n\t\terr = 0;\n\t}\n\tunlock_page(page);\n\tiput(inode);\n\treturn err;\n}\n\n \nstatic void writepages_finish(struct ceph_osd_request *req)\n{\n\tstruct inode *inode = req->r_inode;\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tstruct ceph_osd_data *osd_data;\n\tstruct page *page;\n\tint num_pages, total_pages = 0;\n\tint i, j;\n\tint rc = req->r_result;\n\tstruct ceph_snap_context *snapc = req->r_snapc;\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct ceph_fs_client *fsc = ceph_inode_to_client(inode);\n\tunsigned int len = 0;\n\tbool remove_page;\n\n\tdout(\"writepages_finish %p rc %d\\n\", inode, rc);\n\tif (rc < 0) {\n\t\tmapping_set_error(mapping, rc);\n\t\tceph_set_error_write(ci);\n\t\tif (rc == -EBLOCKLISTED)\n\t\t\tfsc->blocklisted = true;\n\t} else {\n\t\tceph_clear_error_write(ci);\n\t}\n\n\t \n\tremove_page = !(ceph_caps_issued(ci) &\n\t\t\t(CEPH_CAP_FILE_CACHE|CEPH_CAP_FILE_LAZYIO));\n\n\t \n\tfor (i = 0; i < req->r_num_ops; i++) {\n\t\tif (req->r_ops[i].op != CEPH_OSD_OP_WRITE) {\n\t\t\tpr_warn(\"%s incorrect op %d req %p index %d tid %llu\\n\",\n\t\t\t\t__func__, req->r_ops[i].op, req, i, req->r_tid);\n\t\t\tbreak;\n\t\t}\n\n\t\tosd_data = osd_req_op_extent_osd_data(req, i);\n\t\tBUG_ON(osd_data->type != CEPH_OSD_DATA_TYPE_PAGES);\n\t\tlen += osd_data->length;\n\t\tnum_pages = calc_pages_for((u64)osd_data->alignment,\n\t\t\t\t\t   (u64)osd_data->length);\n\t\ttotal_pages += num_pages;\n\t\tfor (j = 0; j < num_pages; j++) {\n\t\t\tpage = osd_data->pages[j];\n\t\t\tif (fscrypt_is_bounce_page(page)) {\n\t\t\t\tpage = fscrypt_pagecache_page(page);\n\t\t\t\tfscrypt_free_bounce_page(osd_data->pages[j]);\n\t\t\t\tosd_data->pages[j] = page;\n\t\t\t}\n\t\t\tBUG_ON(!page);\n\t\t\tWARN_ON(!PageUptodate(page));\n\n\t\t\tif (atomic_long_dec_return(&fsc->writeback_count) <\n\t\t\t     CONGESTION_OFF_THRESH(\n\t\t\t\t\tfsc->mount_options->congestion_kb))\n\t\t\t\tfsc->write_congested = false;\n\n\t\t\tceph_put_snap_context(detach_page_private(page));\n\t\t\tend_page_writeback(page);\n\t\t\tdout(\"unlocking %p\\n\", page);\n\n\t\t\tif (remove_page)\n\t\t\t\tgeneric_error_remove_page(inode->i_mapping,\n\t\t\t\t\t\t\t  page);\n\n\t\t\tunlock_page(page);\n\t\t}\n\t\tdout(\"writepages_finish %p wrote %llu bytes cleaned %d pages\\n\",\n\t\t     inode, osd_data->length, rc >= 0 ? num_pages : 0);\n\n\t\trelease_pages(osd_data->pages, num_pages);\n\t}\n\n\tceph_update_write_metrics(&fsc->mdsc->metric, req->r_start_latency,\n\t\t\t\t  req->r_end_latency, len, rc);\n\n\tceph_put_wrbuffer_cap_refs(ci, total_pages, snapc);\n\n\tosd_data = osd_req_op_extent_osd_data(req, 0);\n\tif (osd_data->pages_from_pool)\n\t\tmempool_free(osd_data->pages, ceph_wb_pagevec_pool);\n\telse\n\t\tkfree(osd_data->pages);\n\tceph_osdc_put_request(req);\n\tceph_dec_osd_stopping_blocker(fsc->mdsc);\n}\n\n \nstatic int ceph_writepages_start(struct address_space *mapping,\n\t\t\t\t struct writeback_control *wbc)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tstruct ceph_fs_client *fsc = ceph_inode_to_client(inode);\n\tstruct ceph_vino vino = ceph_vino(inode);\n\tpgoff_t index, start_index, end = -1;\n\tstruct ceph_snap_context *snapc = NULL, *last_snapc = NULL, *pgsnapc;\n\tstruct folio_batch fbatch;\n\tint rc = 0;\n\tunsigned int wsize = i_blocksize(inode);\n\tstruct ceph_osd_request *req = NULL;\n\tstruct ceph_writeback_ctl ceph_wbc;\n\tbool should_loop, range_whole = false;\n\tbool done = false;\n\tbool caching = ceph_is_cache_enabled(inode);\n\txa_mark_t tag;\n\n\tif (wbc->sync_mode == WB_SYNC_NONE &&\n\t    fsc->write_congested)\n\t\treturn 0;\n\n\tdout(\"writepages_start %p (mode=%s)\\n\", inode,\n\t     wbc->sync_mode == WB_SYNC_NONE ? \"NONE\" :\n\t     (wbc->sync_mode == WB_SYNC_ALL ? \"ALL\" : \"HOLD\"));\n\n\tif (ceph_inode_is_shutdown(inode)) {\n\t\tif (ci->i_wrbuffer_ref > 0) {\n\t\t\tpr_warn_ratelimited(\n\t\t\t\t\"writepage_start %p %lld forced umount\\n\",\n\t\t\t\tinode, ceph_ino(inode));\n\t\t}\n\t\tmapping_set_error(mapping, -EIO);\n\t\treturn -EIO;  \n\t}\n\tif (fsc->mount_options->wsize < wsize)\n\t\twsize = fsc->mount_options->wsize;\n\n\tfolio_batch_init(&fbatch);\n\n\tstart_index = wbc->range_cyclic ? mapping->writeback_index : 0;\n\tindex = start_index;\n\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages) {\n\t\ttag = PAGECACHE_TAG_TOWRITE;\n\t} else {\n\t\ttag = PAGECACHE_TAG_DIRTY;\n\t}\nretry:\n\t \n\tsnapc = get_oldest_context(inode, &ceph_wbc, NULL);\n\tif (!snapc) {\n\t\t \n\t\tdout(\" no snap context with dirty data?\\n\");\n\t\tgoto out;\n\t}\n\tdout(\" oldest snapc is %p seq %lld (%d snaps)\\n\",\n\t     snapc, snapc->seq, snapc->num_snaps);\n\n\tshould_loop = false;\n\tif (ceph_wbc.head_snapc && snapc != last_snapc) {\n\t\t \n\t\tif (wbc->range_cyclic) {\n\t\t\tindex = start_index;\n\t\t\tend = -1;\n\t\t\tif (index > 0)\n\t\t\t\tshould_loop = true;\n\t\t\tdout(\" cyclic, start at %lu\\n\", index);\n\t\t} else {\n\t\t\tindex = wbc->range_start >> PAGE_SHIFT;\n\t\t\tend = wbc->range_end >> PAGE_SHIFT;\n\t\t\tif (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)\n\t\t\t\trange_whole = true;\n\t\t\tdout(\" not cyclic, %lu to %lu\\n\", index, end);\n\t\t}\n\t} else if (!ceph_wbc.head_snapc) {\n\t\t \n\t\tif (index > 0)\n\t\t\tshould_loop = true;\n\t\tdout(\" non-head snapc, range whole\\n\");\n\t}\n\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag_pages_for_writeback(mapping, index, end);\n\n\tceph_put_snap_context(last_snapc);\n\tlast_snapc = snapc;\n\n\twhile (!done && index <= end) {\n\t\tint num_ops = 0, op_idx;\n\t\tunsigned i, nr_folios, max_pages, locked_pages = 0;\n\t\tstruct page **pages = NULL, **data_pages;\n\t\tstruct page *page;\n\t\tpgoff_t strip_unit_end = 0;\n\t\tu64 offset = 0, len = 0;\n\t\tbool from_pool = false;\n\n\t\tmax_pages = wsize >> PAGE_SHIFT;\n\nget_more_pages:\n\t\tnr_folios = filemap_get_folios_tag(mapping, &index,\n\t\t\t\t\t\t   end, tag, &fbatch);\n\t\tdout(\"pagevec_lookup_range_tag got %d\\n\", nr_folios);\n\t\tif (!nr_folios && !locked_pages)\n\t\t\tbreak;\n\t\tfor (i = 0; i < nr_folios && locked_pages < max_pages; i++) {\n\t\t\tpage = &fbatch.folios[i]->page;\n\t\t\tdout(\"? %p idx %lu\\n\", page, page->index);\n\t\t\tif (locked_pages == 0)\n\t\t\t\tlock_page(page);   \n\t\t\telse if (!trylock_page(page))\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tif (unlikely(!PageDirty(page)) ||\n\t\t\t    unlikely(page->mapping != mapping)) {\n\t\t\t\tdout(\"!dirty or !mapping %p\\n\", page);\n\t\t\t\tunlock_page(page);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t \n\t\t\tpgsnapc = page_snap_context(page);\n\t\t\tif (pgsnapc != snapc) {\n\t\t\t\tdout(\"page snapc %p %lld != oldest %p %lld\\n\",\n\t\t\t\t     pgsnapc, pgsnapc->seq, snapc, snapc->seq);\n\t\t\t\tif (!should_loop &&\n\t\t\t\t    !ceph_wbc.head_snapc &&\n\t\t\t\t    wbc->sync_mode != WB_SYNC_NONE)\n\t\t\t\t\tshould_loop = true;\n\t\t\t\tunlock_page(page);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (page_offset(page) >= ceph_wbc.i_size) {\n\t\t\t\tstruct folio *folio = page_folio(page);\n\n\t\t\t\tdout(\"folio at %lu beyond eof %llu\\n\",\n\t\t\t\t     folio->index, ceph_wbc.i_size);\n\t\t\t\tif ((ceph_wbc.size_stable ||\n\t\t\t\t    folio_pos(folio) >= i_size_read(inode)) &&\n\t\t\t\t    folio_clear_dirty_for_io(folio))\n\t\t\t\t\tfolio_invalidate(folio, 0,\n\t\t\t\t\t\t\tfolio_size(folio));\n\t\t\t\tfolio_unlock(folio);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (strip_unit_end && (page->index > strip_unit_end)) {\n\t\t\t\tdout(\"end of strip unit %p\\n\", page);\n\t\t\t\tunlock_page(page);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (PageWriteback(page) || PageFsCache(page)) {\n\t\t\t\tif (wbc->sync_mode == WB_SYNC_NONE) {\n\t\t\t\t\tdout(\"%p under writeback\\n\", page);\n\t\t\t\t\tunlock_page(page);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tdout(\"waiting on writeback %p\\n\", page);\n\t\t\t\twait_on_page_writeback(page);\n\t\t\t\twait_on_page_fscache(page);\n\t\t\t}\n\n\t\t\tif (!clear_page_dirty_for_io(page)) {\n\t\t\t\tdout(\"%p !clear_page_dirty_for_io\\n\", page);\n\t\t\t\tunlock_page(page);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t \n\t\t\tif (locked_pages == 0) {\n\t\t\t\tu64 objnum;\n\t\t\t\tu64 objoff;\n\t\t\t\tu32 xlen;\n\n\t\t\t\t \n\t\t\t\toffset = (u64)page_offset(page);\n\t\t\t\tceph_calc_file_object_mapping(&ci->i_layout,\n\t\t\t\t\t\t\t      offset, wsize,\n\t\t\t\t\t\t\t      &objnum, &objoff,\n\t\t\t\t\t\t\t      &xlen);\n\t\t\t\tlen = xlen;\n\n\t\t\t\tnum_ops = 1;\n\t\t\t\tstrip_unit_end = page->index +\n\t\t\t\t\t((len - 1) >> PAGE_SHIFT);\n\n\t\t\t\tBUG_ON(pages);\n\t\t\t\tmax_pages = calc_pages_for(0, (u64)len);\n\t\t\t\tpages = kmalloc_array(max_pages,\n\t\t\t\t\t\t      sizeof(*pages),\n\t\t\t\t\t\t      GFP_NOFS);\n\t\t\t\tif (!pages) {\n\t\t\t\t\tfrom_pool = true;\n\t\t\t\t\tpages = mempool_alloc(ceph_wb_pagevec_pool, GFP_NOFS);\n\t\t\t\t\tBUG_ON(!pages);\n\t\t\t\t}\n\n\t\t\t\tlen = 0;\n\t\t\t} else if (page->index !=\n\t\t\t\t   (offset + len) >> PAGE_SHIFT) {\n\t\t\t\tif (num_ops >= (from_pool ?  CEPH_OSD_SLAB_OPS :\n\t\t\t\t\t\t\t     CEPH_OSD_MAX_OPS)) {\n\t\t\t\t\tredirty_page_for_writepage(wbc, page);\n\t\t\t\t\tunlock_page(page);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tnum_ops++;\n\t\t\t\toffset = (u64)page_offset(page);\n\t\t\t\tlen = 0;\n\t\t\t}\n\n\t\t\t \n\t\t\tdout(\"%p will write page %p idx %lu\\n\",\n\t\t\t     inode, page, page->index);\n\n\t\t\tif (atomic_long_inc_return(&fsc->writeback_count) >\n\t\t\t    CONGESTION_ON_THRESH(\n\t\t\t\t    fsc->mount_options->congestion_kb))\n\t\t\t\tfsc->write_congested = true;\n\n\t\t\tif (IS_ENCRYPTED(inode)) {\n\t\t\t\tpages[locked_pages] =\n\t\t\t\t\tfscrypt_encrypt_pagecache_blocks(page,\n\t\t\t\t\t\tPAGE_SIZE, 0,\n\t\t\t\t\t\tlocked_pages ? GFP_NOWAIT : GFP_NOFS);\n\t\t\t\tif (IS_ERR(pages[locked_pages])) {\n\t\t\t\t\tif (PTR_ERR(pages[locked_pages]) == -EINVAL)\n\t\t\t\t\t\tpr_err(\"%s: inode->i_blkbits=%hhu\\n\",\n\t\t\t\t\t\t\t__func__, inode->i_blkbits);\n\t\t\t\t\t \n\t\t\t\t\tBUG_ON(locked_pages == 0);\n\t\t\t\t\tpages[locked_pages] = NULL;\n\t\t\t\t\tredirty_page_for_writepage(wbc, page);\n\t\t\t\t\tunlock_page(page);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\t++locked_pages;\n\t\t\t} else {\n\t\t\t\tpages[locked_pages++] = page;\n\t\t\t}\n\n\t\t\tfbatch.folios[i] = NULL;\n\t\t\tlen += thp_size(page);\n\t\t}\n\n\t\t \n\t\tif (!locked_pages)\n\t\t\tgoto release_folios;\n\t\tif (i) {\n\t\t\tunsigned j, n = 0;\n\t\t\t \n\t\t\tfor (j = 0; j < nr_folios; j++) {\n\t\t\t\tif (!fbatch.folios[j])\n\t\t\t\t\tcontinue;\n\t\t\t\tif (n < j)\n\t\t\t\t\tfbatch.folios[n] = fbatch.folios[j];\n\t\t\t\tn++;\n\t\t\t}\n\t\t\tfbatch.nr = n;\n\n\t\t\tif (nr_folios && i == nr_folios &&\n\t\t\t    locked_pages < max_pages) {\n\t\t\t\tdout(\"reached end fbatch, trying for more\\n\");\n\t\t\t\tfolio_batch_release(&fbatch);\n\t\t\t\tgoto get_more_pages;\n\t\t\t}\n\t\t}\n\nnew_request:\n\t\toffset = ceph_fscrypt_page_offset(pages[0]);\n\t\tlen = wsize;\n\n\t\treq = ceph_osdc_new_request(&fsc->client->osdc,\n\t\t\t\t\t&ci->i_layout, vino,\n\t\t\t\t\toffset, &len, 0, num_ops,\n\t\t\t\t\tCEPH_OSD_OP_WRITE, CEPH_OSD_FLAG_WRITE,\n\t\t\t\t\tsnapc, ceph_wbc.truncate_seq,\n\t\t\t\t\tceph_wbc.truncate_size, false);\n\t\tif (IS_ERR(req)) {\n\t\t\treq = ceph_osdc_new_request(&fsc->client->osdc,\n\t\t\t\t\t\t&ci->i_layout, vino,\n\t\t\t\t\t\toffset, &len, 0,\n\t\t\t\t\t\tmin(num_ops,\n\t\t\t\t\t\t    CEPH_OSD_SLAB_OPS),\n\t\t\t\t\t\tCEPH_OSD_OP_WRITE,\n\t\t\t\t\t\tCEPH_OSD_FLAG_WRITE,\n\t\t\t\t\t\tsnapc, ceph_wbc.truncate_seq,\n\t\t\t\t\t\tceph_wbc.truncate_size, true);\n\t\t\tBUG_ON(IS_ERR(req));\n\t\t}\n\t\tBUG_ON(len < ceph_fscrypt_page_offset(pages[locked_pages - 1]) +\n\t\t\t     thp_size(pages[locked_pages - 1]) - offset);\n\n\t\tif (!ceph_inc_osd_stopping_blocker(fsc->mdsc)) {\n\t\t\trc = -EIO;\n\t\t\tgoto release_folios;\n\t\t}\n\t\treq->r_callback = writepages_finish;\n\t\treq->r_inode = inode;\n\n\t\t \n\t\tlen = 0;\n\t\tdata_pages = pages;\n\t\top_idx = 0;\n\t\tfor (i = 0; i < locked_pages; i++) {\n\t\t\tstruct page *page = ceph_fscrypt_pagecache_page(pages[i]);\n\n\t\t\tu64 cur_offset = page_offset(page);\n\t\t\t \n\t\t\tif (offset + len != cur_offset) {\n\t\t\t\t \n\t\t\t\tif (op_idx + 1 == req->r_num_ops)\n\t\t\t\t\tbreak;\n\n\t\t\t\t \n\t\t\t\tceph_fscache_write_to_cache(inode, offset, len, caching);\n\n\t\t\t\t \n\t\t\t\tosd_req_op_extent_dup_last(req, op_idx,\n\t\t\t\t\t\t\t   cur_offset - offset);\n\t\t\t\tdout(\"writepages got pages at %llu~%llu\\n\",\n\t\t\t\t     offset, len);\n\t\t\t\tosd_req_op_extent_osd_data_pages(req, op_idx,\n\t\t\t\t\t\t\tdata_pages, len, 0,\n\t\t\t\t\t\t\tfrom_pool, false);\n\t\t\t\tosd_req_op_extent_update(req, op_idx, len);\n\n\t\t\t\tlen = 0;\n\t\t\t\toffset = cur_offset;\n\t\t\t\tdata_pages = pages + i;\n\t\t\t\top_idx++;\n\t\t\t}\n\n\t\t\tset_page_writeback(page);\n\t\t\tif (caching)\n\t\t\t\tceph_set_page_fscache(page);\n\t\t\tlen += thp_size(page);\n\t\t}\n\t\tceph_fscache_write_to_cache(inode, offset, len, caching);\n\n\t\tif (ceph_wbc.size_stable) {\n\t\t\tlen = min(len, ceph_wbc.i_size - offset);\n\t\t} else if (i == locked_pages) {\n\t\t\t \n\t\t\tu64 min_len = len + 1 - thp_size(page);\n\t\t\tlen = get_writepages_data_length(inode, pages[i - 1],\n\t\t\t\t\t\t\t offset);\n\t\t\tlen = max(len, min_len);\n\t\t}\n\t\tif (IS_ENCRYPTED(inode))\n\t\t\tlen = round_up(len, CEPH_FSCRYPT_BLOCK_SIZE);\n\n\t\tdout(\"writepages got pages at %llu~%llu\\n\", offset, len);\n\n\t\tif (IS_ENCRYPTED(inode) &&\n\t\t    ((offset | len) & ~CEPH_FSCRYPT_BLOCK_MASK))\n\t\t\tpr_warn(\"%s: bad encrypted write offset=%lld len=%llu\\n\",\n\t\t\t\t__func__, offset, len);\n\n\t\tosd_req_op_extent_osd_data_pages(req, op_idx, data_pages, len,\n\t\t\t\t\t\t 0, from_pool, false);\n\t\tosd_req_op_extent_update(req, op_idx, len);\n\n\t\tBUG_ON(op_idx + 1 != req->r_num_ops);\n\n\t\tfrom_pool = false;\n\t\tif (i < locked_pages) {\n\t\t\tBUG_ON(num_ops <= req->r_num_ops);\n\t\t\tnum_ops -= req->r_num_ops;\n\t\t\tlocked_pages -= i;\n\n\t\t\t \n\t\t\tdata_pages = pages;\n\t\t\tpages = kmalloc_array(locked_pages, sizeof(*pages),\n\t\t\t\t\t      GFP_NOFS);\n\t\t\tif (!pages) {\n\t\t\t\tfrom_pool = true;\n\t\t\t\tpages = mempool_alloc(ceph_wb_pagevec_pool, GFP_NOFS);\n\t\t\t\tBUG_ON(!pages);\n\t\t\t}\n\t\t\tmemcpy(pages, data_pages + i,\n\t\t\t       locked_pages * sizeof(*pages));\n\t\t\tmemset(data_pages + i, 0,\n\t\t\t       locked_pages * sizeof(*pages));\n\t\t} else {\n\t\t\tBUG_ON(num_ops != req->r_num_ops);\n\t\t\tindex = pages[i - 1]->index + 1;\n\t\t\t \n\t\t\tpages = NULL;\n\t\t}\n\n\t\treq->r_mtime = inode->i_mtime;\n\t\tceph_osdc_start_request(&fsc->client->osdc, req);\n\t\treq = NULL;\n\n\t\twbc->nr_to_write -= i;\n\t\tif (pages)\n\t\t\tgoto new_request;\n\n\t\t \n\t\tif (wbc->nr_to_write <= 0 && wbc->sync_mode == WB_SYNC_NONE)\n\t\t\tdone = true;\n\nrelease_folios:\n\t\tdout(\"folio_batch release on %d folios (%p)\\n\", (int)fbatch.nr,\n\t\t     fbatch.nr ? fbatch.folios[0] : NULL);\n\t\tfolio_batch_release(&fbatch);\n\t}\n\n\tif (should_loop && !done) {\n\t\t \n\t\tdout(\"writepages looping back to beginning of file\\n\");\n\t\tend = start_index - 1;  \n\n\t\t \n\t\tif (wbc->sync_mode != WB_SYNC_NONE &&\n\t\t    start_index == 0 &&  \n\t\t    !ceph_wbc.head_snapc) {\n\t\t\tstruct page *page;\n\t\t\tunsigned i, nr;\n\t\t\tindex = 0;\n\t\t\twhile ((index <= end) &&\n\t\t\t       (nr = filemap_get_folios_tag(mapping, &index,\n\t\t\t\t\t\t(pgoff_t)-1,\n\t\t\t\t\t\tPAGECACHE_TAG_WRITEBACK,\n\t\t\t\t\t\t&fbatch))) {\n\t\t\t\tfor (i = 0; i < nr; i++) {\n\t\t\t\t\tpage = &fbatch.folios[i]->page;\n\t\t\t\t\tif (page_snap_context(page) != snapc)\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\twait_on_page_writeback(page);\n\t\t\t\t}\n\t\t\t\tfolio_batch_release(&fbatch);\n\t\t\t\tcond_resched();\n\t\t\t}\n\t\t}\n\n\t\tstart_index = 0;\n\t\tindex = 0;\n\t\tgoto retry;\n\t}\n\n\tif (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))\n\t\tmapping->writeback_index = index;\n\nout:\n\tceph_osdc_put_request(req);\n\tceph_put_snap_context(last_snapc);\n\tdout(\"writepages dend - startone, rc = %d\\n\", rc);\n\treturn rc;\n}\n\n\n\n \nstatic int context_is_writeable_or_written(struct inode *inode,\n\t\t\t\t\t   struct ceph_snap_context *snapc)\n{\n\tstruct ceph_snap_context *oldest = get_oldest_context(inode, NULL, NULL);\n\tint ret = !oldest || snapc->seq <= oldest->seq;\n\n\tceph_put_snap_context(oldest);\n\treturn ret;\n}\n\n \nstatic struct ceph_snap_context *\nceph_find_incompatible(struct page *page)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\n\tif (ceph_inode_is_shutdown(inode)) {\n\t\tdout(\" page %p %llx:%llx is shutdown\\n\", page,\n\t\t     ceph_vinop(inode));\n\t\treturn ERR_PTR(-ESTALE);\n\t}\n\n\tfor (;;) {\n\t\tstruct ceph_snap_context *snapc, *oldest;\n\n\t\twait_on_page_writeback(page);\n\n\t\tsnapc = page_snap_context(page);\n\t\tif (!snapc || snapc == ci->i_head_snapc)\n\t\t\tbreak;\n\n\t\t \n\t\toldest = get_oldest_context(inode, NULL, NULL);\n\t\tif (snapc->seq > oldest->seq) {\n\t\t\t \n\t\t\tceph_put_snap_context(oldest);\n\t\t\tdout(\" page %p snapc %p not current or oldest\\n\", page, snapc);\n\t\t\treturn ceph_get_snap_context(snapc);\n\t\t}\n\t\tceph_put_snap_context(oldest);\n\n\t\t \n\t\tdout(\" page %p snapc %p not current, but oldest\\n\", page, snapc);\n\t\tif (clear_page_dirty_for_io(page)) {\n\t\t\tint r = writepage_nounlock(page, NULL);\n\t\t\tif (r < 0)\n\t\t\t\treturn ERR_PTR(r);\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic int ceph_netfs_check_write_begin(struct file *file, loff_t pos, unsigned int len,\n\t\t\t\t\tstruct folio **foliop, void **_fsdata)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tstruct ceph_snap_context *snapc;\n\n\tsnapc = ceph_find_incompatible(folio_page(*foliop, 0));\n\tif (snapc) {\n\t\tint r;\n\n\t\tfolio_unlock(*foliop);\n\t\tfolio_put(*foliop);\n\t\t*foliop = NULL;\n\t\tif (IS_ERR(snapc))\n\t\t\treturn PTR_ERR(snapc);\n\n\t\tceph_queue_writeback(inode);\n\t\tr = wait_event_killable(ci->i_cap_wq,\n\t\t\t\t\tcontext_is_writeable_or_written(inode, snapc));\n\t\tceph_put_snap_context(snapc);\n\t\treturn r == 0 ? -EAGAIN : r;\n\t}\n\treturn 0;\n}\n\n \nstatic int ceph_write_begin(struct file *file, struct address_space *mapping,\n\t\t\t    loff_t pos, unsigned len,\n\t\t\t    struct page **pagep, void **fsdata)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tstruct folio *folio = NULL;\n\tint r;\n\n\tr = netfs_write_begin(&ci->netfs, file, inode->i_mapping, pos, len, &folio, NULL);\n\tif (r < 0)\n\t\treturn r;\n\n\tfolio_wait_fscache(folio);\n\tWARN_ON_ONCE(!folio_test_locked(folio));\n\t*pagep = &folio->page;\n\treturn 0;\n}\n\n \nstatic int ceph_write_end(struct file *file, struct address_space *mapping,\n\t\t\t  loff_t pos, unsigned len, unsigned copied,\n\t\t\t  struct page *subpage, void *fsdata)\n{\n\tstruct folio *folio = page_folio(subpage);\n\tstruct inode *inode = file_inode(file);\n\tbool check_cap = false;\n\n\tdout(\"write_end file %p inode %p folio %p %d~%d (%d)\\n\", file,\n\t     inode, folio, (int)pos, (int)copied, (int)len);\n\n\tif (!folio_test_uptodate(folio)) {\n\t\t \n\t\tif (copied < len) {\n\t\t\tcopied = 0;\n\t\t\tgoto out;\n\t\t}\n\t\tfolio_mark_uptodate(folio);\n\t}\n\n\t \n\tif (pos+copied > i_size_read(inode))\n\t\tcheck_cap = ceph_inode_set_size(inode, pos+copied);\n\n\tfolio_mark_dirty(folio);\n\nout:\n\tfolio_unlock(folio);\n\tfolio_put(folio);\n\n\tif (check_cap)\n\t\tceph_check_caps(ceph_inode(inode), CHECK_CAPS_AUTHONLY);\n\n\treturn copied;\n}\n\nconst struct address_space_operations ceph_aops = {\n\t.read_folio = netfs_read_folio,\n\t.readahead = netfs_readahead,\n\t.writepage = ceph_writepage,\n\t.writepages = ceph_writepages_start,\n\t.write_begin = ceph_write_begin,\n\t.write_end = ceph_write_end,\n\t.dirty_folio = ceph_dirty_folio,\n\t.invalidate_folio = ceph_invalidate_folio,\n\t.release_folio = ceph_release_folio,\n\t.direct_IO = noop_direct_IO,\n};\n\nstatic void ceph_block_sigs(sigset_t *oldset)\n{\n\tsigset_t mask;\n\tsiginitsetinv(&mask, sigmask(SIGKILL));\n\tsigprocmask(SIG_BLOCK, &mask, oldset);\n}\n\nstatic void ceph_restore_sigs(sigset_t *oldset)\n{\n\tsigprocmask(SIG_SETMASK, oldset, NULL);\n}\n\n \nstatic vm_fault_t ceph_filemap_fault(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct inode *inode = file_inode(vma->vm_file);\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tstruct ceph_file_info *fi = vma->vm_file->private_data;\n\tloff_t off = (loff_t)vmf->pgoff << PAGE_SHIFT;\n\tint want, got, err;\n\tsigset_t oldset;\n\tvm_fault_t ret = VM_FAULT_SIGBUS;\n\n\tif (ceph_inode_is_shutdown(inode))\n\t\treturn ret;\n\n\tceph_block_sigs(&oldset);\n\n\tdout(\"filemap_fault %p %llx.%llx %llu trying to get caps\\n\",\n\t     inode, ceph_vinop(inode), off);\n\tif (fi->fmode & CEPH_FILE_MODE_LAZY)\n\t\twant = CEPH_CAP_FILE_CACHE | CEPH_CAP_FILE_LAZYIO;\n\telse\n\t\twant = CEPH_CAP_FILE_CACHE;\n\n\tgot = 0;\n\terr = ceph_get_caps(vma->vm_file, CEPH_CAP_FILE_RD, want, -1, &got);\n\tif (err < 0)\n\t\tgoto out_restore;\n\n\tdout(\"filemap_fault %p %llu got cap refs on %s\\n\",\n\t     inode, off, ceph_cap_string(got));\n\n\tif ((got & (CEPH_CAP_FILE_CACHE | CEPH_CAP_FILE_LAZYIO)) ||\n\t    !ceph_has_inline_data(ci)) {\n\t\tCEPH_DEFINE_RW_CONTEXT(rw_ctx, got);\n\t\tceph_add_rw_context(fi, &rw_ctx);\n\t\tret = filemap_fault(vmf);\n\t\tceph_del_rw_context(fi, &rw_ctx);\n\t\tdout(\"filemap_fault %p %llu drop cap refs %s ret %x\\n\",\n\t\t     inode, off, ceph_cap_string(got), ret);\n\t} else\n\t\terr = -EAGAIN;\n\n\tceph_put_cap_refs(ci, got);\n\n\tif (err != -EAGAIN)\n\t\tgoto out_restore;\n\n\t \n\tif (off >= PAGE_SIZE) {\n\t\t \n\t\tret = VM_FAULT_SIGBUS;\n\t} else {\n\t\tstruct address_space *mapping = inode->i_mapping;\n\t\tstruct page *page;\n\n\t\tfilemap_invalidate_lock_shared(mapping);\n\t\tpage = find_or_create_page(mapping, 0,\n\t\t\t\tmapping_gfp_constraint(mapping, ~__GFP_FS));\n\t\tif (!page) {\n\t\t\tret = VM_FAULT_OOM;\n\t\t\tgoto out_inline;\n\t\t}\n\t\terr = __ceph_do_getattr(inode, page,\n\t\t\t\t\t CEPH_STAT_CAP_INLINE_DATA, true);\n\t\tif (err < 0 || off >= i_size_read(inode)) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tret = vmf_error(err);\n\t\t\tgoto out_inline;\n\t\t}\n\t\tif (err < PAGE_SIZE)\n\t\t\tzero_user_segment(page, err, PAGE_SIZE);\n\t\telse\n\t\t\tflush_dcache_page(page);\n\t\tSetPageUptodate(page);\n\t\tvmf->page = page;\n\t\tret = VM_FAULT_MAJOR | VM_FAULT_LOCKED;\nout_inline:\n\t\tfilemap_invalidate_unlock_shared(mapping);\n\t\tdout(\"filemap_fault %p %llu read inline data ret %x\\n\",\n\t\t     inode, off, ret);\n\t}\nout_restore:\n\tceph_restore_sigs(&oldset);\n\tif (err < 0)\n\t\tret = vmf_error(err);\n\n\treturn ret;\n}\n\nstatic vm_fault_t ceph_page_mkwrite(struct vm_fault *vmf)\n{\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct inode *inode = file_inode(vma->vm_file);\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tstruct ceph_file_info *fi = vma->vm_file->private_data;\n\tstruct ceph_cap_flush *prealloc_cf;\n\tstruct page *page = vmf->page;\n\tloff_t off = page_offset(page);\n\tloff_t size = i_size_read(inode);\n\tsize_t len;\n\tint want, got, err;\n\tsigset_t oldset;\n\tvm_fault_t ret = VM_FAULT_SIGBUS;\n\n\tif (ceph_inode_is_shutdown(inode))\n\t\treturn ret;\n\n\tprealloc_cf = ceph_alloc_cap_flush();\n\tif (!prealloc_cf)\n\t\treturn VM_FAULT_OOM;\n\n\tsb_start_pagefault(inode->i_sb);\n\tceph_block_sigs(&oldset);\n\n\tif (off + thp_size(page) <= size)\n\t\tlen = thp_size(page);\n\telse\n\t\tlen = offset_in_thp(page, size);\n\n\tdout(\"page_mkwrite %p %llx.%llx %llu~%zd getting caps i_size %llu\\n\",\n\t     inode, ceph_vinop(inode), off, len, size);\n\tif (fi->fmode & CEPH_FILE_MODE_LAZY)\n\t\twant = CEPH_CAP_FILE_BUFFER | CEPH_CAP_FILE_LAZYIO;\n\telse\n\t\twant = CEPH_CAP_FILE_BUFFER;\n\n\tgot = 0;\n\terr = ceph_get_caps(vma->vm_file, CEPH_CAP_FILE_WR, want, off + len, &got);\n\tif (err < 0)\n\t\tgoto out_free;\n\n\tdout(\"page_mkwrite %p %llu~%zd got cap refs on %s\\n\",\n\t     inode, off, len, ceph_cap_string(got));\n\n\t \n\tfile_update_time(vma->vm_file);\n\tinode_inc_iversion_raw(inode);\n\n\tdo {\n\t\tstruct ceph_snap_context *snapc;\n\n\t\tlock_page(page);\n\n\t\tif (page_mkwrite_check_truncate(page, inode) < 0) {\n\t\t\tunlock_page(page);\n\t\t\tret = VM_FAULT_NOPAGE;\n\t\t\tbreak;\n\t\t}\n\n\t\tsnapc = ceph_find_incompatible(page);\n\t\tif (!snapc) {\n\t\t\t \n\t\t\tset_page_dirty(page);\n\t\t\tret = VM_FAULT_LOCKED;\n\t\t\tbreak;\n\t\t}\n\n\t\tunlock_page(page);\n\n\t\tif (IS_ERR(snapc)) {\n\t\t\tret = VM_FAULT_SIGBUS;\n\t\t\tbreak;\n\t\t}\n\n\t\tceph_queue_writeback(inode);\n\t\terr = wait_event_killable(ci->i_cap_wq,\n\t\t\t\tcontext_is_writeable_or_written(inode, snapc));\n\t\tceph_put_snap_context(snapc);\n\t} while (err == 0);\n\n\tif (ret == VM_FAULT_LOCKED) {\n\t\tint dirty;\n\t\tspin_lock(&ci->i_ceph_lock);\n\t\tdirty = __ceph_mark_dirty_caps(ci, CEPH_CAP_FILE_WR,\n\t\t\t\t\t       &prealloc_cf);\n\t\tspin_unlock(&ci->i_ceph_lock);\n\t\tif (dirty)\n\t\t\t__mark_inode_dirty(inode, dirty);\n\t}\n\n\tdout(\"page_mkwrite %p %llu~%zd dropping cap refs on %s ret %x\\n\",\n\t     inode, off, len, ceph_cap_string(got), ret);\n\tceph_put_cap_refs_async(ci, got);\nout_free:\n\tceph_restore_sigs(&oldset);\n\tsb_end_pagefault(inode->i_sb);\n\tceph_free_cap_flush(prealloc_cf);\n\tif (err < 0)\n\t\tret = vmf_error(err);\n\treturn ret;\n}\n\nvoid ceph_fill_inline_data(struct inode *inode, struct page *locked_page,\n\t\t\t   char\t*data, size_t len)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\n\n\tif (locked_page) {\n\t\tpage = locked_page;\n\t} else {\n\t\tif (i_size_read(inode) == 0)\n\t\t\treturn;\n\t\tpage = find_or_create_page(mapping, 0,\n\t\t\t\t\t   mapping_gfp_constraint(mapping,\n\t\t\t\t\t   ~__GFP_FS));\n\t\tif (!page)\n\t\t\treturn;\n\t\tif (PageUptodate(page)) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tdout(\"fill_inline_data %p %llx.%llx len %zu locked_page %p\\n\",\n\t     inode, ceph_vinop(inode), len, locked_page);\n\n\tif (len > 0) {\n\t\tvoid *kaddr = kmap_atomic(page);\n\t\tmemcpy(kaddr, data, len);\n\t\tkunmap_atomic(kaddr);\n\t}\n\n\tif (page != locked_page) {\n\t\tif (len < PAGE_SIZE)\n\t\t\tzero_user_segment(page, len, PAGE_SIZE);\n\t\telse\n\t\t\tflush_dcache_page(page);\n\n\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t}\n}\n\nint ceph_uninline_data(struct file *file)\n{\n\tstruct inode *inode = file_inode(file);\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tstruct ceph_fs_client *fsc = ceph_inode_to_client(inode);\n\tstruct ceph_osd_request *req = NULL;\n\tstruct ceph_cap_flush *prealloc_cf = NULL;\n\tstruct folio *folio = NULL;\n\tu64 inline_version = CEPH_INLINE_NONE;\n\tstruct page *pages[1];\n\tint err = 0;\n\tu64 len;\n\n\tspin_lock(&ci->i_ceph_lock);\n\tinline_version = ci->i_inline_version;\n\tspin_unlock(&ci->i_ceph_lock);\n\n\tdout(\"uninline_data %p %llx.%llx inline_version %llu\\n\",\n\t     inode, ceph_vinop(inode), inline_version);\n\n\tif (ceph_inode_is_shutdown(inode)) {\n\t\terr = -EIO;\n\t\tgoto out;\n\t}\n\n\tif (inline_version == CEPH_INLINE_NONE)\n\t\treturn 0;\n\n\tprealloc_cf = ceph_alloc_cap_flush();\n\tif (!prealloc_cf)\n\t\treturn -ENOMEM;\n\n\tif (inline_version == 1)  \n\t\tgoto out_uninline;\n\n\tfolio = read_mapping_folio(inode->i_mapping, 0, file);\n\tif (IS_ERR(folio)) {\n\t\terr = PTR_ERR(folio);\n\t\tgoto out;\n\t}\n\n\tfolio_lock(folio);\n\n\tlen = i_size_read(inode);\n\tif (len > folio_size(folio))\n\t\tlen = folio_size(folio);\n\n\treq = ceph_osdc_new_request(&fsc->client->osdc, &ci->i_layout,\n\t\t\t\t    ceph_vino(inode), 0, &len, 0, 1,\n\t\t\t\t    CEPH_OSD_OP_CREATE, CEPH_OSD_FLAG_WRITE,\n\t\t\t\t    NULL, 0, 0, false);\n\tif (IS_ERR(req)) {\n\t\terr = PTR_ERR(req);\n\t\tgoto out_unlock;\n\t}\n\n\treq->r_mtime = inode->i_mtime;\n\tceph_osdc_start_request(&fsc->client->osdc, req);\n\terr = ceph_osdc_wait_request(&fsc->client->osdc, req);\n\tceph_osdc_put_request(req);\n\tif (err < 0)\n\t\tgoto out_unlock;\n\n\treq = ceph_osdc_new_request(&fsc->client->osdc, &ci->i_layout,\n\t\t\t\t    ceph_vino(inode), 0, &len, 1, 3,\n\t\t\t\t    CEPH_OSD_OP_WRITE, CEPH_OSD_FLAG_WRITE,\n\t\t\t\t    NULL, ci->i_truncate_seq,\n\t\t\t\t    ci->i_truncate_size, false);\n\tif (IS_ERR(req)) {\n\t\terr = PTR_ERR(req);\n\t\tgoto out_unlock;\n\t}\n\n\tpages[0] = folio_page(folio, 0);\n\tosd_req_op_extent_osd_data_pages(req, 1, pages, len, 0, false, false);\n\n\t{\n\t\t__le64 xattr_buf = cpu_to_le64(inline_version);\n\t\terr = osd_req_op_xattr_init(req, 0, CEPH_OSD_OP_CMPXATTR,\n\t\t\t\t\t    \"inline_version\", &xattr_buf,\n\t\t\t\t\t    sizeof(xattr_buf),\n\t\t\t\t\t    CEPH_OSD_CMPXATTR_OP_GT,\n\t\t\t\t\t    CEPH_OSD_CMPXATTR_MODE_U64);\n\t\tif (err)\n\t\t\tgoto out_put_req;\n\t}\n\n\t{\n\t\tchar xattr_buf[32];\n\t\tint xattr_len = snprintf(xattr_buf, sizeof(xattr_buf),\n\t\t\t\t\t \"%llu\", inline_version);\n\t\terr = osd_req_op_xattr_init(req, 2, CEPH_OSD_OP_SETXATTR,\n\t\t\t\t\t    \"inline_version\",\n\t\t\t\t\t    xattr_buf, xattr_len, 0, 0);\n\t\tif (err)\n\t\t\tgoto out_put_req;\n\t}\n\n\treq->r_mtime = inode->i_mtime;\n\tceph_osdc_start_request(&fsc->client->osdc, req);\n\terr = ceph_osdc_wait_request(&fsc->client->osdc, req);\n\n\tceph_update_write_metrics(&fsc->mdsc->metric, req->r_start_latency,\n\t\t\t\t  req->r_end_latency, len, err);\n\nout_uninline:\n\tif (!err) {\n\t\tint dirty;\n\n\t\t \n\t\tdown_read(&fsc->mdsc->snap_rwsem);\n\t\tspin_lock(&ci->i_ceph_lock);\n\t\tci->i_inline_version = CEPH_INLINE_NONE;\n\t\tdirty = __ceph_mark_dirty_caps(ci, CEPH_CAP_FILE_WR, &prealloc_cf);\n\t\tspin_unlock(&ci->i_ceph_lock);\n\t\tup_read(&fsc->mdsc->snap_rwsem);\n\t\tif (dirty)\n\t\t\t__mark_inode_dirty(inode, dirty);\n\t}\nout_put_req:\n\tceph_osdc_put_request(req);\n\tif (err == -ECANCELED)\n\t\terr = 0;\nout_unlock:\n\tif (folio) {\n\t\tfolio_unlock(folio);\n\t\tfolio_put(folio);\n\t}\nout:\n\tceph_free_cap_flush(prealloc_cf);\n\tdout(\"uninline_data %p %llx.%llx inline_version %llu = %d\\n\",\n\t     inode, ceph_vinop(inode), inline_version, err);\n\treturn err;\n}\n\nstatic const struct vm_operations_struct ceph_vmops = {\n\t.fault\t\t= ceph_filemap_fault,\n\t.page_mkwrite\t= ceph_page_mkwrite,\n};\n\nint ceph_mmap(struct file *file, struct vm_area_struct *vma)\n{\n\tstruct address_space *mapping = file->f_mapping;\n\n\tif (!mapping->a_ops->read_folio)\n\t\treturn -ENOEXEC;\n\tvma->vm_ops = &ceph_vmops;\n\treturn 0;\n}\n\nenum {\n\tPOOL_READ\t= 1,\n\tPOOL_WRITE\t= 2,\n};\n\nstatic int __ceph_pool_perm_get(struct ceph_inode_info *ci,\n\t\t\t\ts64 pool, struct ceph_string *pool_ns)\n{\n\tstruct ceph_fs_client *fsc = ceph_inode_to_client(&ci->netfs.inode);\n\tstruct ceph_mds_client *mdsc = fsc->mdsc;\n\tstruct ceph_osd_request *rd_req = NULL, *wr_req = NULL;\n\tstruct rb_node **p, *parent;\n\tstruct ceph_pool_perm *perm;\n\tstruct page **pages;\n\tsize_t pool_ns_len;\n\tint err = 0, err2 = 0, have = 0;\n\n\tdown_read(&mdsc->pool_perm_rwsem);\n\tp = &mdsc->pool_perm_tree.rb_node;\n\twhile (*p) {\n\t\tperm = rb_entry(*p, struct ceph_pool_perm, node);\n\t\tif (pool < perm->pool)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (pool > perm->pool)\n\t\t\tp = &(*p)->rb_right;\n\t\telse {\n\t\t\tint ret = ceph_compare_string(pool_ns,\n\t\t\t\t\t\tperm->pool_ns,\n\t\t\t\t\t\tperm->pool_ns_len);\n\t\t\tif (ret < 0)\n\t\t\t\tp = &(*p)->rb_left;\n\t\t\telse if (ret > 0)\n\t\t\t\tp = &(*p)->rb_right;\n\t\t\telse {\n\t\t\t\thave = perm->perm;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tup_read(&mdsc->pool_perm_rwsem);\n\tif (*p)\n\t\tgoto out;\n\n\tif (pool_ns)\n\t\tdout(\"__ceph_pool_perm_get pool %lld ns %.*s no perm cached\\n\",\n\t\t     pool, (int)pool_ns->len, pool_ns->str);\n\telse\n\t\tdout(\"__ceph_pool_perm_get pool %lld no perm cached\\n\", pool);\n\n\tdown_write(&mdsc->pool_perm_rwsem);\n\tp = &mdsc->pool_perm_tree.rb_node;\n\tparent = NULL;\n\twhile (*p) {\n\t\tparent = *p;\n\t\tperm = rb_entry(parent, struct ceph_pool_perm, node);\n\t\tif (pool < perm->pool)\n\t\t\tp = &(*p)->rb_left;\n\t\telse if (pool > perm->pool)\n\t\t\tp = &(*p)->rb_right;\n\t\telse {\n\t\t\tint ret = ceph_compare_string(pool_ns,\n\t\t\t\t\t\tperm->pool_ns,\n\t\t\t\t\t\tperm->pool_ns_len);\n\t\t\tif (ret < 0)\n\t\t\t\tp = &(*p)->rb_left;\n\t\t\telse if (ret > 0)\n\t\t\t\tp = &(*p)->rb_right;\n\t\t\telse {\n\t\t\t\thave = perm->perm;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\tif (*p) {\n\t\tup_write(&mdsc->pool_perm_rwsem);\n\t\tgoto out;\n\t}\n\n\trd_req = ceph_osdc_alloc_request(&fsc->client->osdc, NULL,\n\t\t\t\t\t 1, false, GFP_NOFS);\n\tif (!rd_req) {\n\t\terr = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\trd_req->r_flags = CEPH_OSD_FLAG_READ;\n\tosd_req_op_init(rd_req, 0, CEPH_OSD_OP_STAT, 0);\n\trd_req->r_base_oloc.pool = pool;\n\tif (pool_ns)\n\t\trd_req->r_base_oloc.pool_ns = ceph_get_string(pool_ns);\n\tceph_oid_printf(&rd_req->r_base_oid, \"%llx.00000000\", ci->i_vino.ino);\n\n\terr = ceph_osdc_alloc_messages(rd_req, GFP_NOFS);\n\tif (err)\n\t\tgoto out_unlock;\n\n\twr_req = ceph_osdc_alloc_request(&fsc->client->osdc, NULL,\n\t\t\t\t\t 1, false, GFP_NOFS);\n\tif (!wr_req) {\n\t\terr = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\twr_req->r_flags = CEPH_OSD_FLAG_WRITE;\n\tosd_req_op_init(wr_req, 0, CEPH_OSD_OP_CREATE, CEPH_OSD_OP_FLAG_EXCL);\n\tceph_oloc_copy(&wr_req->r_base_oloc, &rd_req->r_base_oloc);\n\tceph_oid_copy(&wr_req->r_base_oid, &rd_req->r_base_oid);\n\n\terr = ceph_osdc_alloc_messages(wr_req, GFP_NOFS);\n\tif (err)\n\t\tgoto out_unlock;\n\n\t \n\tpages = ceph_alloc_page_vector(1, GFP_KERNEL);\n\tif (IS_ERR(pages)) {\n\t\terr = PTR_ERR(pages);\n\t\tgoto out_unlock;\n\t}\n\n\tosd_req_op_raw_data_in_pages(rd_req, 0, pages, PAGE_SIZE,\n\t\t\t\t     0, false, true);\n\tceph_osdc_start_request(&fsc->client->osdc, rd_req);\n\n\twr_req->r_mtime = ci->netfs.inode.i_mtime;\n\tceph_osdc_start_request(&fsc->client->osdc, wr_req);\n\n\terr = ceph_osdc_wait_request(&fsc->client->osdc, rd_req);\n\terr2 = ceph_osdc_wait_request(&fsc->client->osdc, wr_req);\n\n\tif (err >= 0 || err == -ENOENT)\n\t\thave |= POOL_READ;\n\telse if (err != -EPERM) {\n\t\tif (err == -EBLOCKLISTED)\n\t\t\tfsc->blocklisted = true;\n\t\tgoto out_unlock;\n\t}\n\n\tif (err2 == 0 || err2 == -EEXIST)\n\t\thave |= POOL_WRITE;\n\telse if (err2 != -EPERM) {\n\t\tif (err2 == -EBLOCKLISTED)\n\t\t\tfsc->blocklisted = true;\n\t\terr = err2;\n\t\tgoto out_unlock;\n\t}\n\n\tpool_ns_len = pool_ns ? pool_ns->len : 0;\n\tperm = kmalloc(sizeof(*perm) + pool_ns_len + 1, GFP_NOFS);\n\tif (!perm) {\n\t\terr = -ENOMEM;\n\t\tgoto out_unlock;\n\t}\n\n\tperm->pool = pool;\n\tperm->perm = have;\n\tperm->pool_ns_len = pool_ns_len;\n\tif (pool_ns_len > 0)\n\t\tmemcpy(perm->pool_ns, pool_ns->str, pool_ns_len);\n\tperm->pool_ns[pool_ns_len] = 0;\n\n\trb_link_node(&perm->node, parent, p);\n\trb_insert_color(&perm->node, &mdsc->pool_perm_tree);\n\terr = 0;\nout_unlock:\n\tup_write(&mdsc->pool_perm_rwsem);\n\n\tceph_osdc_put_request(rd_req);\n\tceph_osdc_put_request(wr_req);\nout:\n\tif (!err)\n\t\terr = have;\n\tif (pool_ns)\n\t\tdout(\"__ceph_pool_perm_get pool %lld ns %.*s result = %d\\n\",\n\t\t     pool, (int)pool_ns->len, pool_ns->str, err);\n\telse\n\t\tdout(\"__ceph_pool_perm_get pool %lld result = %d\\n\", pool, err);\n\treturn err;\n}\n\nint ceph_pool_perm_check(struct inode *inode, int need)\n{\n\tstruct ceph_inode_info *ci = ceph_inode(inode);\n\tstruct ceph_string *pool_ns;\n\ts64 pool;\n\tint ret, flags;\n\n\t \n\tif (!S_ISREG(inode->i_mode))\n\t\treturn 0;\n\n\tif (ci->i_vino.snap != CEPH_NOSNAP) {\n\t\t \n\t\treturn 0;\n\t}\n\n\tif (ceph_test_mount_opt(ceph_inode_to_client(inode),\n\t\t\t\tNOPOOLPERM))\n\t\treturn 0;\n\n\tspin_lock(&ci->i_ceph_lock);\n\tflags = ci->i_ceph_flags;\n\tpool = ci->i_layout.pool_id;\n\tspin_unlock(&ci->i_ceph_lock);\ncheck:\n\tif (flags & CEPH_I_POOL_PERM) {\n\t\tif ((need & CEPH_CAP_FILE_RD) && !(flags & CEPH_I_POOL_RD)) {\n\t\t\tdout(\"ceph_pool_perm_check pool %lld no read perm\\n\",\n\t\t\t     pool);\n\t\t\treturn -EPERM;\n\t\t}\n\t\tif ((need & CEPH_CAP_FILE_WR) && !(flags & CEPH_I_POOL_WR)) {\n\t\t\tdout(\"ceph_pool_perm_check pool %lld no write perm\\n\",\n\t\t\t     pool);\n\t\t\treturn -EPERM;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tpool_ns = ceph_try_get_string(ci->i_layout.pool_ns);\n\tret = __ceph_pool_perm_get(ci, pool, pool_ns);\n\tceph_put_string(pool_ns);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tflags = CEPH_I_POOL_PERM;\n\tif (ret & POOL_READ)\n\t\tflags |= CEPH_I_POOL_RD;\n\tif (ret & POOL_WRITE)\n\t\tflags |= CEPH_I_POOL_WR;\n\n\tspin_lock(&ci->i_ceph_lock);\n\tif (pool == ci->i_layout.pool_id &&\n\t    pool_ns == rcu_dereference_raw(ci->i_layout.pool_ns)) {\n\t\tci->i_ceph_flags |= flags;\n        } else {\n\t\tpool = ci->i_layout.pool_id;\n\t\tflags = ci->i_ceph_flags;\n\t}\n\tspin_unlock(&ci->i_ceph_lock);\n\tgoto check;\n}\n\nvoid ceph_pool_perm_destroy(struct ceph_mds_client *mdsc)\n{\n\tstruct ceph_pool_perm *perm;\n\tstruct rb_node *n;\n\n\twhile (!RB_EMPTY_ROOT(&mdsc->pool_perm_tree)) {\n\t\tn = rb_first(&mdsc->pool_perm_tree);\n\t\tperm = rb_entry(n, struct ceph_pool_perm, node);\n\t\trb_erase(n, &mdsc->pool_perm_tree);\n\t\tkfree(perm);\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}