{
  "module_name": "vmcore.c",
  "hash_id": "58a39810e39a14d595761a06d7239606c39c3c880e65d56ff8b237399a56d110",
  "original_prompt": "Ingested from linux-6.6.14/fs/proc/vmcore.c",
  "human_readable_source": "\n \n\n#include <linux/mm.h>\n#include <linux/kcore.h>\n#include <linux/user.h>\n#include <linux/elf.h>\n#include <linux/elfcore.h>\n#include <linux/export.h>\n#include <linux/slab.h>\n#include <linux/highmem.h>\n#include <linux/printk.h>\n#include <linux/memblock.h>\n#include <linux/init.h>\n#include <linux/crash_dump.h>\n#include <linux/list.h>\n#include <linux/moduleparam.h>\n#include <linux/mutex.h>\n#include <linux/vmalloc.h>\n#include <linux/pagemap.h>\n#include <linux/uio.h>\n#include <linux/cc_platform.h>\n#include <asm/io.h>\n#include \"internal.h\"\n\n \nstatic LIST_HEAD(vmcore_list);\n\n \nstatic char *elfcorebuf;\nstatic size_t elfcorebuf_sz;\nstatic size_t elfcorebuf_sz_orig;\n\nstatic char *elfnotes_buf;\nstatic size_t elfnotes_sz;\n \nstatic size_t elfnotes_orig_sz;\n\n \nstatic u64 vmcore_size;\n\nstatic struct proc_dir_entry *proc_vmcore;\n\n#ifdef CONFIG_PROC_VMCORE_DEVICE_DUMP\n \nstatic LIST_HEAD(vmcoredd_list);\nstatic DEFINE_MUTEX(vmcoredd_mutex);\n\nstatic bool vmcoredd_disabled;\ncore_param(novmcoredd, vmcoredd_disabled, bool, 0);\n#endif  \n\n \nstatic size_t vmcoredd_orig_sz;\n\nstatic DEFINE_SPINLOCK(vmcore_cb_lock);\nDEFINE_STATIC_SRCU(vmcore_cb_srcu);\n \nstatic LIST_HEAD(vmcore_cb_list);\n \nstatic bool vmcore_opened;\n\nvoid register_vmcore_cb(struct vmcore_cb *cb)\n{\n\tINIT_LIST_HEAD(&cb->next);\n\tspin_lock(&vmcore_cb_lock);\n\tlist_add_tail(&cb->next, &vmcore_cb_list);\n\t \n\tif (vmcore_opened)\n\t\tpr_warn_once(\"Unexpected vmcore callback registration\\n\");\n\tspin_unlock(&vmcore_cb_lock);\n}\nEXPORT_SYMBOL_GPL(register_vmcore_cb);\n\nvoid unregister_vmcore_cb(struct vmcore_cb *cb)\n{\n\tspin_lock(&vmcore_cb_lock);\n\tlist_del_rcu(&cb->next);\n\t \n\tif (vmcore_opened)\n\t\tpr_warn_once(\"Unexpected vmcore callback unregistration\\n\");\n\tspin_unlock(&vmcore_cb_lock);\n\n\tsynchronize_srcu(&vmcore_cb_srcu);\n}\nEXPORT_SYMBOL_GPL(unregister_vmcore_cb);\n\nstatic bool pfn_is_ram(unsigned long pfn)\n{\n\tstruct vmcore_cb *cb;\n\tbool ret = true;\n\n\tlist_for_each_entry_srcu(cb, &vmcore_cb_list, next,\n\t\t\t\t srcu_read_lock_held(&vmcore_cb_srcu)) {\n\t\tif (unlikely(!cb->pfn_is_ram))\n\t\t\tcontinue;\n\t\tret = cb->pfn_is_ram(cb, pfn);\n\t\tif (!ret)\n\t\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int open_vmcore(struct inode *inode, struct file *file)\n{\n\tspin_lock(&vmcore_cb_lock);\n\tvmcore_opened = true;\n\tspin_unlock(&vmcore_cb_lock);\n\n\treturn 0;\n}\n\n \nssize_t read_from_oldmem(struct iov_iter *iter, size_t count,\n\t\t\t u64 *ppos, bool encrypted)\n{\n\tunsigned long pfn, offset;\n\tssize_t nr_bytes;\n\tssize_t read = 0, tmp;\n\tint idx;\n\n\tif (!count)\n\t\treturn 0;\n\n\toffset = (unsigned long)(*ppos % PAGE_SIZE);\n\tpfn = (unsigned long)(*ppos / PAGE_SIZE);\n\n\tidx = srcu_read_lock(&vmcore_cb_srcu);\n\tdo {\n\t\tif (count > (PAGE_SIZE - offset))\n\t\t\tnr_bytes = PAGE_SIZE - offset;\n\t\telse\n\t\t\tnr_bytes = count;\n\n\t\t \n\t\tif (!pfn_is_ram(pfn)) {\n\t\t\ttmp = iov_iter_zero(nr_bytes, iter);\n\t\t} else {\n\t\t\tif (encrypted)\n\t\t\t\ttmp = copy_oldmem_page_encrypted(iter, pfn,\n\t\t\t\t\t\t\t\t nr_bytes,\n\t\t\t\t\t\t\t\t offset);\n\t\t\telse\n\t\t\t\ttmp = copy_oldmem_page(iter, pfn, nr_bytes,\n\t\t\t\t\t\t       offset);\n\t\t}\n\t\tif (tmp < nr_bytes) {\n\t\t\tsrcu_read_unlock(&vmcore_cb_srcu, idx);\n\t\t\treturn -EFAULT;\n\t\t}\n\n\t\t*ppos += nr_bytes;\n\t\tcount -= nr_bytes;\n\t\tread += nr_bytes;\n\t\t++pfn;\n\t\toffset = 0;\n\t} while (count);\n\tsrcu_read_unlock(&vmcore_cb_srcu, idx);\n\n\treturn read;\n}\n\n \nint __weak elfcorehdr_alloc(unsigned long long *addr, unsigned long long *size)\n{\n\treturn 0;\n}\n\n \nvoid __weak elfcorehdr_free(unsigned long long addr)\n{}\n\n \nssize_t __weak elfcorehdr_read(char *buf, size_t count, u64 *ppos)\n{\n\tstruct kvec kvec = { .iov_base = buf, .iov_len = count };\n\tstruct iov_iter iter;\n\n\tiov_iter_kvec(&iter, ITER_DEST, &kvec, 1, count);\n\n\treturn read_from_oldmem(&iter, count, ppos, false);\n}\n\n \nssize_t __weak elfcorehdr_read_notes(char *buf, size_t count, u64 *ppos)\n{\n\tstruct kvec kvec = { .iov_base = buf, .iov_len = count };\n\tstruct iov_iter iter;\n\n\tiov_iter_kvec(&iter, ITER_DEST, &kvec, 1, count);\n\n\treturn read_from_oldmem(&iter, count, ppos,\n\t\t\tcc_platform_has(CC_ATTR_MEM_ENCRYPT));\n}\n\n \nint __weak remap_oldmem_pfn_range(struct vm_area_struct *vma,\n\t\t\t\t  unsigned long from, unsigned long pfn,\n\t\t\t\t  unsigned long size, pgprot_t prot)\n{\n\tprot = pgprot_encrypted(prot);\n\treturn remap_pfn_range(vma, from, pfn, size, prot);\n}\n\n \nssize_t __weak copy_oldmem_page_encrypted(struct iov_iter *iter,\n\t\tunsigned long pfn, size_t csize, unsigned long offset)\n{\n\treturn copy_oldmem_page(iter, pfn, csize, offset);\n}\n\n#ifdef CONFIG_PROC_VMCORE_DEVICE_DUMP\nstatic int vmcoredd_copy_dumps(struct iov_iter *iter, u64 start, size_t size)\n{\n\tstruct vmcoredd_node *dump;\n\tu64 offset = 0;\n\tint ret = 0;\n\tsize_t tsz;\n\tchar *buf;\n\n\tmutex_lock(&vmcoredd_mutex);\n\tlist_for_each_entry(dump, &vmcoredd_list, list) {\n\t\tif (start < offset + dump->size) {\n\t\t\ttsz = min(offset + (u64)dump->size - start, (u64)size);\n\t\t\tbuf = dump->buf + start - offset;\n\t\t\tif (copy_to_iter(buf, tsz, iter) < tsz) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\n\t\t\tsize -= tsz;\n\t\t\tstart += tsz;\n\n\t\t\t \n\t\t\tif (!size)\n\t\t\t\tgoto out_unlock;\n\t\t}\n\t\toffset += dump->size;\n\t}\n\nout_unlock:\n\tmutex_unlock(&vmcoredd_mutex);\n\treturn ret;\n}\n\n#ifdef CONFIG_MMU\nstatic int vmcoredd_mmap_dumps(struct vm_area_struct *vma, unsigned long dst,\n\t\t\t       u64 start, size_t size)\n{\n\tstruct vmcoredd_node *dump;\n\tu64 offset = 0;\n\tint ret = 0;\n\tsize_t tsz;\n\tchar *buf;\n\n\tmutex_lock(&vmcoredd_mutex);\n\tlist_for_each_entry(dump, &vmcoredd_list, list) {\n\t\tif (start < offset + dump->size) {\n\t\t\ttsz = min(offset + (u64)dump->size - start, (u64)size);\n\t\t\tbuf = dump->buf + start - offset;\n\t\t\tif (remap_vmalloc_range_partial(vma, dst, buf, 0,\n\t\t\t\t\t\t\ttsz)) {\n\t\t\t\tret = -EFAULT;\n\t\t\t\tgoto out_unlock;\n\t\t\t}\n\n\t\t\tsize -= tsz;\n\t\t\tstart += tsz;\n\t\t\tdst += tsz;\n\n\t\t\t \n\t\t\tif (!size)\n\t\t\t\tgoto out_unlock;\n\t\t}\n\t\toffset += dump->size;\n\t}\n\nout_unlock:\n\tmutex_unlock(&vmcoredd_mutex);\n\treturn ret;\n}\n#endif  \n#endif  \n\n \nstatic ssize_t __read_vmcore(struct iov_iter *iter, loff_t *fpos)\n{\n\tssize_t acc = 0, tmp;\n\tsize_t tsz;\n\tu64 start;\n\tstruct vmcore *m = NULL;\n\n\tif (!iov_iter_count(iter) || *fpos >= vmcore_size)\n\t\treturn 0;\n\n\tiov_iter_truncate(iter, vmcore_size - *fpos);\n\n\t \n\tif (*fpos < elfcorebuf_sz) {\n\t\ttsz = min(elfcorebuf_sz - (size_t)*fpos, iov_iter_count(iter));\n\t\tif (copy_to_iter(elfcorebuf + *fpos, tsz, iter) < tsz)\n\t\t\treturn -EFAULT;\n\t\t*fpos += tsz;\n\t\tacc += tsz;\n\n\t\t \n\t\tif (!iov_iter_count(iter))\n\t\t\treturn acc;\n\t}\n\n\t \n\tif (*fpos < elfcorebuf_sz + elfnotes_sz) {\n\t\tvoid *kaddr;\n\n\t\t \n#ifdef CONFIG_PROC_VMCORE_DEVICE_DUMP\n\t\t \n\t\tif (*fpos < elfcorebuf_sz + vmcoredd_orig_sz) {\n\t\t\ttsz = min(elfcorebuf_sz + vmcoredd_orig_sz -\n\t\t\t\t  (size_t)*fpos, iov_iter_count(iter));\n\t\t\tstart = *fpos - elfcorebuf_sz;\n\t\t\tif (vmcoredd_copy_dumps(iter, start, tsz))\n\t\t\t\treturn -EFAULT;\n\n\t\t\t*fpos += tsz;\n\t\t\tacc += tsz;\n\n\t\t\t \n\t\t\tif (!iov_iter_count(iter))\n\t\t\t\treturn acc;\n\t\t}\n#endif  \n\n\t\t \n\t\ttsz = min(elfcorebuf_sz + elfnotes_sz - (size_t)*fpos,\n\t\t\t  iov_iter_count(iter));\n\t\tkaddr = elfnotes_buf + *fpos - elfcorebuf_sz - vmcoredd_orig_sz;\n\t\tif (copy_to_iter(kaddr, tsz, iter) < tsz)\n\t\t\treturn -EFAULT;\n\n\t\t*fpos += tsz;\n\t\tacc += tsz;\n\n\t\t \n\t\tif (!iov_iter_count(iter))\n\t\t\treturn acc;\n\t}\n\n\tlist_for_each_entry(m, &vmcore_list, list) {\n\t\tif (*fpos < m->offset + m->size) {\n\t\t\ttsz = (size_t)min_t(unsigned long long,\n\t\t\t\t\t    m->offset + m->size - *fpos,\n\t\t\t\t\t    iov_iter_count(iter));\n\t\t\tstart = m->paddr + *fpos - m->offset;\n\t\t\ttmp = read_from_oldmem(iter, tsz, &start,\n\t\t\t\t\tcc_platform_has(CC_ATTR_MEM_ENCRYPT));\n\t\t\tif (tmp < 0)\n\t\t\t\treturn tmp;\n\t\t\t*fpos += tsz;\n\t\t\tacc += tsz;\n\n\t\t\t \n\t\t\tif (!iov_iter_count(iter))\n\t\t\t\treturn acc;\n\t\t}\n\t}\n\n\treturn acc;\n}\n\nstatic ssize_t read_vmcore(struct kiocb *iocb, struct iov_iter *iter)\n{\n\treturn __read_vmcore(iter, &iocb->ki_pos);\n}\n\n \nstatic vm_fault_t mmap_vmcore_fault(struct vm_fault *vmf)\n{\n#ifdef CONFIG_S390\n\tstruct address_space *mapping = vmf->vma->vm_file->f_mapping;\n\tpgoff_t index = vmf->pgoff;\n\tstruct iov_iter iter;\n\tstruct kvec kvec;\n\tstruct page *page;\n\tloff_t offset;\n\tint rc;\n\n\tpage = find_or_create_page(mapping, index, GFP_KERNEL);\n\tif (!page)\n\t\treturn VM_FAULT_OOM;\n\tif (!PageUptodate(page)) {\n\t\toffset = (loff_t) index << PAGE_SHIFT;\n\t\tkvec.iov_base = page_address(page);\n\t\tkvec.iov_len = PAGE_SIZE;\n\t\tiov_iter_kvec(&iter, ITER_DEST, &kvec, 1, PAGE_SIZE);\n\n\t\trc = __read_vmcore(&iter, &offset);\n\t\tif (rc < 0) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\treturn vmf_error(rc);\n\t\t}\n\t\tSetPageUptodate(page);\n\t}\n\tunlock_page(page);\n\tvmf->page = page;\n\treturn 0;\n#else\n\treturn VM_FAULT_SIGBUS;\n#endif\n}\n\nstatic const struct vm_operations_struct vmcore_mmap_ops = {\n\t.fault = mmap_vmcore_fault,\n};\n\n \nstatic inline char *vmcore_alloc_buf(size_t size)\n{\n#ifdef CONFIG_MMU\n\treturn vmalloc_user(size);\n#else\n\treturn vzalloc(size);\n#endif\n}\n\n \n#ifdef CONFIG_MMU\n \nstatic int remap_oldmem_pfn_checked(struct vm_area_struct *vma,\n\t\t\t\t    unsigned long from, unsigned long pfn,\n\t\t\t\t    unsigned long size, pgprot_t prot)\n{\n\tunsigned long map_size;\n\tunsigned long pos_start, pos_end, pos;\n\tunsigned long zeropage_pfn = my_zero_pfn(0);\n\tsize_t len = 0;\n\n\tpos_start = pfn;\n\tpos_end = pfn + (size >> PAGE_SHIFT);\n\n\tfor (pos = pos_start; pos < pos_end; ++pos) {\n\t\tif (!pfn_is_ram(pos)) {\n\t\t\t \n\t\t\tif (pos > pos_start) {\n\t\t\t\t \n\t\t\t\tmap_size = (pos - pos_start) << PAGE_SHIFT;\n\t\t\t\tif (remap_oldmem_pfn_range(vma, from + len,\n\t\t\t\t\t\t\t   pos_start, map_size,\n\t\t\t\t\t\t\t   prot))\n\t\t\t\t\tgoto fail;\n\t\t\t\tlen += map_size;\n\t\t\t}\n\t\t\t \n\t\t\tif (remap_oldmem_pfn_range(vma, from + len,\n\t\t\t\t\t\t   zeropage_pfn,\n\t\t\t\t\t\t   PAGE_SIZE, prot))\n\t\t\t\tgoto fail;\n\t\t\tlen += PAGE_SIZE;\n\t\t\tpos_start = pos + 1;\n\t\t}\n\t}\n\tif (pos > pos_start) {\n\t\t \n\t\tmap_size = (pos - pos_start) << PAGE_SHIFT;\n\t\tif (remap_oldmem_pfn_range(vma, from + len, pos_start,\n\t\t\t\t\t   map_size, prot))\n\t\t\tgoto fail;\n\t}\n\treturn 0;\nfail:\n\tdo_munmap(vma->vm_mm, from, len, NULL);\n\treturn -EAGAIN;\n}\n\nstatic int vmcore_remap_oldmem_pfn(struct vm_area_struct *vma,\n\t\t\t    unsigned long from, unsigned long pfn,\n\t\t\t    unsigned long size, pgprot_t prot)\n{\n\tint ret, idx;\n\n\t \n\tidx = srcu_read_lock(&vmcore_cb_srcu);\n\tif (!list_empty(&vmcore_cb_list))\n\t\tret = remap_oldmem_pfn_checked(vma, from, pfn, size, prot);\n\telse\n\t\tret = remap_oldmem_pfn_range(vma, from, pfn, size, prot);\n\tsrcu_read_unlock(&vmcore_cb_srcu, idx);\n\treturn ret;\n}\n\nstatic int mmap_vmcore(struct file *file, struct vm_area_struct *vma)\n{\n\tsize_t size = vma->vm_end - vma->vm_start;\n\tu64 start, end, len, tsz;\n\tstruct vmcore *m;\n\n\tstart = (u64)vma->vm_pgoff << PAGE_SHIFT;\n\tend = start + size;\n\n\tif (size > vmcore_size || end > vmcore_size)\n\t\treturn -EINVAL;\n\n\tif (vma->vm_flags & (VM_WRITE | VM_EXEC))\n\t\treturn -EPERM;\n\n\tvm_flags_mod(vma, VM_MIXEDMAP, VM_MAYWRITE | VM_MAYEXEC);\n\tvma->vm_ops = &vmcore_mmap_ops;\n\n\tlen = 0;\n\n\tif (start < elfcorebuf_sz) {\n\t\tu64 pfn;\n\n\t\ttsz = min(elfcorebuf_sz - (size_t)start, size);\n\t\tpfn = __pa(elfcorebuf + start) >> PAGE_SHIFT;\n\t\tif (remap_pfn_range(vma, vma->vm_start, pfn, tsz,\n\t\t\t\t    vma->vm_page_prot))\n\t\t\treturn -EAGAIN;\n\t\tsize -= tsz;\n\t\tstart += tsz;\n\t\tlen += tsz;\n\n\t\tif (size == 0)\n\t\t\treturn 0;\n\t}\n\n\tif (start < elfcorebuf_sz + elfnotes_sz) {\n\t\tvoid *kaddr;\n\n\t\t \n#ifdef CONFIG_PROC_VMCORE_DEVICE_DUMP\n\t\t \n\t\tif (start < elfcorebuf_sz + vmcoredd_orig_sz) {\n\t\t\tu64 start_off;\n\n\t\t\ttsz = min(elfcorebuf_sz + vmcoredd_orig_sz -\n\t\t\t\t  (size_t)start, size);\n\t\t\tstart_off = start - elfcorebuf_sz;\n\t\t\tif (vmcoredd_mmap_dumps(vma, vma->vm_start + len,\n\t\t\t\t\t\tstart_off, tsz))\n\t\t\t\tgoto fail;\n\n\t\t\tsize -= tsz;\n\t\t\tstart += tsz;\n\t\t\tlen += tsz;\n\n\t\t\t \n\t\t\tif (!size)\n\t\t\t\treturn 0;\n\t\t}\n#endif  \n\n\t\t \n\t\ttsz = min(elfcorebuf_sz + elfnotes_sz - (size_t)start, size);\n\t\tkaddr = elfnotes_buf + start - elfcorebuf_sz - vmcoredd_orig_sz;\n\t\tif (remap_vmalloc_range_partial(vma, vma->vm_start + len,\n\t\t\t\t\t\tkaddr, 0, tsz))\n\t\t\tgoto fail;\n\n\t\tsize -= tsz;\n\t\tstart += tsz;\n\t\tlen += tsz;\n\n\t\tif (size == 0)\n\t\t\treturn 0;\n\t}\n\n\tlist_for_each_entry(m, &vmcore_list, list) {\n\t\tif (start < m->offset + m->size) {\n\t\t\tu64 paddr = 0;\n\n\t\t\ttsz = (size_t)min_t(unsigned long long,\n\t\t\t\t\t    m->offset + m->size - start, size);\n\t\t\tpaddr = m->paddr + start - m->offset;\n\t\t\tif (vmcore_remap_oldmem_pfn(vma, vma->vm_start + len,\n\t\t\t\t\t\t    paddr >> PAGE_SHIFT, tsz,\n\t\t\t\t\t\t    vma->vm_page_prot))\n\t\t\t\tgoto fail;\n\t\t\tsize -= tsz;\n\t\t\tstart += tsz;\n\t\t\tlen += tsz;\n\n\t\t\tif (size == 0)\n\t\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 0;\nfail:\n\tdo_munmap(vma->vm_mm, vma->vm_start, len, NULL);\n\treturn -EAGAIN;\n}\n#else\nstatic int mmap_vmcore(struct file *file, struct vm_area_struct *vma)\n{\n\treturn -ENOSYS;\n}\n#endif\n\nstatic const struct proc_ops vmcore_proc_ops = {\n\t.proc_open\t= open_vmcore,\n\t.proc_read_iter\t= read_vmcore,\n\t.proc_lseek\t= default_llseek,\n\t.proc_mmap\t= mmap_vmcore,\n};\n\nstatic struct vmcore* __init get_new_element(void)\n{\n\treturn kzalloc(sizeof(struct vmcore), GFP_KERNEL);\n}\n\nstatic u64 get_vmcore_size(size_t elfsz, size_t elfnotesegsz,\n\t\t\t   struct list_head *vc_list)\n{\n\tu64 size;\n\tstruct vmcore *m;\n\n\tsize = elfsz + elfnotesegsz;\n\tlist_for_each_entry(m, vc_list, list) {\n\t\tsize += m->size;\n\t}\n\treturn size;\n}\n\n \nstatic int __init update_note_header_size_elf64(const Elf64_Ehdr *ehdr_ptr)\n{\n\tint i, rc=0;\n\tElf64_Phdr *phdr_ptr;\n\tElf64_Nhdr *nhdr_ptr;\n\n\tphdr_ptr = (Elf64_Phdr *)(ehdr_ptr + 1);\n\tfor (i = 0; i < ehdr_ptr->e_phnum; i++, phdr_ptr++) {\n\t\tvoid *notes_section;\n\t\tu64 offset, max_sz, sz, real_sz = 0;\n\t\tif (phdr_ptr->p_type != PT_NOTE)\n\t\t\tcontinue;\n\t\tmax_sz = phdr_ptr->p_memsz;\n\t\toffset = phdr_ptr->p_offset;\n\t\tnotes_section = kmalloc(max_sz, GFP_KERNEL);\n\t\tif (!notes_section)\n\t\t\treturn -ENOMEM;\n\t\trc = elfcorehdr_read_notes(notes_section, max_sz, &offset);\n\t\tif (rc < 0) {\n\t\t\tkfree(notes_section);\n\t\t\treturn rc;\n\t\t}\n\t\tnhdr_ptr = notes_section;\n\t\twhile (nhdr_ptr->n_namesz != 0) {\n\t\t\tsz = sizeof(Elf64_Nhdr) +\n\t\t\t\t(((u64)nhdr_ptr->n_namesz + 3) & ~3) +\n\t\t\t\t(((u64)nhdr_ptr->n_descsz + 3) & ~3);\n\t\t\tif ((real_sz + sz) > max_sz) {\n\t\t\t\tpr_warn(\"Warning: Exceeded p_memsz, dropping PT_NOTE entry n_namesz=0x%x, n_descsz=0x%x\\n\",\n\t\t\t\t\tnhdr_ptr->n_namesz, nhdr_ptr->n_descsz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\treal_sz += sz;\n\t\t\tnhdr_ptr = (Elf64_Nhdr*)((char*)nhdr_ptr + sz);\n\t\t}\n\t\tkfree(notes_section);\n\t\tphdr_ptr->p_memsz = real_sz;\n\t\tif (real_sz == 0) {\n\t\t\tpr_warn(\"Warning: Zero PT_NOTE entries found\\n\");\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int __init get_note_number_and_size_elf64(const Elf64_Ehdr *ehdr_ptr,\n\t\t\t\t\t\t int *nr_ptnote, u64 *sz_ptnote)\n{\n\tint i;\n\tElf64_Phdr *phdr_ptr;\n\n\t*nr_ptnote = *sz_ptnote = 0;\n\n\tphdr_ptr = (Elf64_Phdr *)(ehdr_ptr + 1);\n\tfor (i = 0; i < ehdr_ptr->e_phnum; i++, phdr_ptr++) {\n\t\tif (phdr_ptr->p_type != PT_NOTE)\n\t\t\tcontinue;\n\t\t*nr_ptnote += 1;\n\t\t*sz_ptnote += phdr_ptr->p_memsz;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int __init copy_notes_elf64(const Elf64_Ehdr *ehdr_ptr, char *notes_buf)\n{\n\tint i, rc=0;\n\tElf64_Phdr *phdr_ptr;\n\n\tphdr_ptr = (Elf64_Phdr*)(ehdr_ptr + 1);\n\n\tfor (i = 0; i < ehdr_ptr->e_phnum; i++, phdr_ptr++) {\n\t\tu64 offset;\n\t\tif (phdr_ptr->p_type != PT_NOTE)\n\t\t\tcontinue;\n\t\toffset = phdr_ptr->p_offset;\n\t\trc = elfcorehdr_read_notes(notes_buf, phdr_ptr->p_memsz,\n\t\t\t\t\t   &offset);\n\t\tif (rc < 0)\n\t\t\treturn rc;\n\t\tnotes_buf += phdr_ptr->p_memsz;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int __init merge_note_headers_elf64(char *elfptr, size_t *elfsz,\n\t\t\t\t\t   char **notes_buf, size_t *notes_sz)\n{\n\tint i, nr_ptnote=0, rc=0;\n\tchar *tmp;\n\tElf64_Ehdr *ehdr_ptr;\n\tElf64_Phdr phdr;\n\tu64 phdr_sz = 0, note_off;\n\n\tehdr_ptr = (Elf64_Ehdr *)elfptr;\n\n\trc = update_note_header_size_elf64(ehdr_ptr);\n\tif (rc < 0)\n\t\treturn rc;\n\n\trc = get_note_number_and_size_elf64(ehdr_ptr, &nr_ptnote, &phdr_sz);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t*notes_sz = roundup(phdr_sz, PAGE_SIZE);\n\t*notes_buf = vmcore_alloc_buf(*notes_sz);\n\tif (!*notes_buf)\n\t\treturn -ENOMEM;\n\n\trc = copy_notes_elf64(ehdr_ptr, *notes_buf);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t \n\tphdr.p_type    = PT_NOTE;\n\tphdr.p_flags   = 0;\n\tnote_off = sizeof(Elf64_Ehdr) +\n\t\t\t(ehdr_ptr->e_phnum - nr_ptnote +1) * sizeof(Elf64_Phdr);\n\tphdr.p_offset  = roundup(note_off, PAGE_SIZE);\n\tphdr.p_vaddr   = phdr.p_paddr = 0;\n\tphdr.p_filesz  = phdr.p_memsz = phdr_sz;\n\tphdr.p_align   = 4;\n\n\t \n\ttmp = elfptr + sizeof(Elf64_Ehdr);\n\tmemcpy(tmp, &phdr, sizeof(phdr));\n\ttmp += sizeof(phdr);\n\n\t \n\ti = (nr_ptnote - 1) * sizeof(Elf64_Phdr);\n\t*elfsz = *elfsz - i;\n\tmemmove(tmp, tmp+i, ((*elfsz)-sizeof(Elf64_Ehdr)-sizeof(Elf64_Phdr)));\n\tmemset(elfptr + *elfsz, 0, i);\n\t*elfsz = roundup(*elfsz, PAGE_SIZE);\n\n\t \n\tehdr_ptr->e_phnum = ehdr_ptr->e_phnum - nr_ptnote + 1;\n\n\t \n\telfnotes_orig_sz = phdr.p_memsz;\n\n\treturn 0;\n}\n\n \nstatic int __init update_note_header_size_elf32(const Elf32_Ehdr *ehdr_ptr)\n{\n\tint i, rc=0;\n\tElf32_Phdr *phdr_ptr;\n\tElf32_Nhdr *nhdr_ptr;\n\n\tphdr_ptr = (Elf32_Phdr *)(ehdr_ptr + 1);\n\tfor (i = 0; i < ehdr_ptr->e_phnum; i++, phdr_ptr++) {\n\t\tvoid *notes_section;\n\t\tu64 offset, max_sz, sz, real_sz = 0;\n\t\tif (phdr_ptr->p_type != PT_NOTE)\n\t\t\tcontinue;\n\t\tmax_sz = phdr_ptr->p_memsz;\n\t\toffset = phdr_ptr->p_offset;\n\t\tnotes_section = kmalloc(max_sz, GFP_KERNEL);\n\t\tif (!notes_section)\n\t\t\treturn -ENOMEM;\n\t\trc = elfcorehdr_read_notes(notes_section, max_sz, &offset);\n\t\tif (rc < 0) {\n\t\t\tkfree(notes_section);\n\t\t\treturn rc;\n\t\t}\n\t\tnhdr_ptr = notes_section;\n\t\twhile (nhdr_ptr->n_namesz != 0) {\n\t\t\tsz = sizeof(Elf32_Nhdr) +\n\t\t\t\t(((u64)nhdr_ptr->n_namesz + 3) & ~3) +\n\t\t\t\t(((u64)nhdr_ptr->n_descsz + 3) & ~3);\n\t\t\tif ((real_sz + sz) > max_sz) {\n\t\t\t\tpr_warn(\"Warning: Exceeded p_memsz, dropping PT_NOTE entry n_namesz=0x%x, n_descsz=0x%x\\n\",\n\t\t\t\t\tnhdr_ptr->n_namesz, nhdr_ptr->n_descsz);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\treal_sz += sz;\n\t\t\tnhdr_ptr = (Elf32_Nhdr*)((char*)nhdr_ptr + sz);\n\t\t}\n\t\tkfree(notes_section);\n\t\tphdr_ptr->p_memsz = real_sz;\n\t\tif (real_sz == 0) {\n\t\t\tpr_warn(\"Warning: Zero PT_NOTE entries found\\n\");\n\t\t}\n\t}\n\n\treturn 0;\n}\n\n \nstatic int __init get_note_number_and_size_elf32(const Elf32_Ehdr *ehdr_ptr,\n\t\t\t\t\t\t int *nr_ptnote, u64 *sz_ptnote)\n{\n\tint i;\n\tElf32_Phdr *phdr_ptr;\n\n\t*nr_ptnote = *sz_ptnote = 0;\n\n\tphdr_ptr = (Elf32_Phdr *)(ehdr_ptr + 1);\n\tfor (i = 0; i < ehdr_ptr->e_phnum; i++, phdr_ptr++) {\n\t\tif (phdr_ptr->p_type != PT_NOTE)\n\t\t\tcontinue;\n\t\t*nr_ptnote += 1;\n\t\t*sz_ptnote += phdr_ptr->p_memsz;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int __init copy_notes_elf32(const Elf32_Ehdr *ehdr_ptr, char *notes_buf)\n{\n\tint i, rc=0;\n\tElf32_Phdr *phdr_ptr;\n\n\tphdr_ptr = (Elf32_Phdr*)(ehdr_ptr + 1);\n\n\tfor (i = 0; i < ehdr_ptr->e_phnum; i++, phdr_ptr++) {\n\t\tu64 offset;\n\t\tif (phdr_ptr->p_type != PT_NOTE)\n\t\t\tcontinue;\n\t\toffset = phdr_ptr->p_offset;\n\t\trc = elfcorehdr_read_notes(notes_buf, phdr_ptr->p_memsz,\n\t\t\t\t\t   &offset);\n\t\tif (rc < 0)\n\t\t\treturn rc;\n\t\tnotes_buf += phdr_ptr->p_memsz;\n\t}\n\n\treturn 0;\n}\n\n \nstatic int __init merge_note_headers_elf32(char *elfptr, size_t *elfsz,\n\t\t\t\t\t   char **notes_buf, size_t *notes_sz)\n{\n\tint i, nr_ptnote=0, rc=0;\n\tchar *tmp;\n\tElf32_Ehdr *ehdr_ptr;\n\tElf32_Phdr phdr;\n\tu64 phdr_sz = 0, note_off;\n\n\tehdr_ptr = (Elf32_Ehdr *)elfptr;\n\n\trc = update_note_header_size_elf32(ehdr_ptr);\n\tif (rc < 0)\n\t\treturn rc;\n\n\trc = get_note_number_and_size_elf32(ehdr_ptr, &nr_ptnote, &phdr_sz);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t*notes_sz = roundup(phdr_sz, PAGE_SIZE);\n\t*notes_buf = vmcore_alloc_buf(*notes_sz);\n\tif (!*notes_buf)\n\t\treturn -ENOMEM;\n\n\trc = copy_notes_elf32(ehdr_ptr, *notes_buf);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t \n\tphdr.p_type    = PT_NOTE;\n\tphdr.p_flags   = 0;\n\tnote_off = sizeof(Elf32_Ehdr) +\n\t\t\t(ehdr_ptr->e_phnum - nr_ptnote +1) * sizeof(Elf32_Phdr);\n\tphdr.p_offset  = roundup(note_off, PAGE_SIZE);\n\tphdr.p_vaddr   = phdr.p_paddr = 0;\n\tphdr.p_filesz  = phdr.p_memsz = phdr_sz;\n\tphdr.p_align   = 4;\n\n\t \n\ttmp = elfptr + sizeof(Elf32_Ehdr);\n\tmemcpy(tmp, &phdr, sizeof(phdr));\n\ttmp += sizeof(phdr);\n\n\t \n\ti = (nr_ptnote - 1) * sizeof(Elf32_Phdr);\n\t*elfsz = *elfsz - i;\n\tmemmove(tmp, tmp+i, ((*elfsz)-sizeof(Elf32_Ehdr)-sizeof(Elf32_Phdr)));\n\tmemset(elfptr + *elfsz, 0, i);\n\t*elfsz = roundup(*elfsz, PAGE_SIZE);\n\n\t \n\tehdr_ptr->e_phnum = ehdr_ptr->e_phnum - nr_ptnote + 1;\n\n\t \n\telfnotes_orig_sz = phdr.p_memsz;\n\n\treturn 0;\n}\n\n \nstatic int __init process_ptload_program_headers_elf64(char *elfptr,\n\t\t\t\t\t\tsize_t elfsz,\n\t\t\t\t\t\tsize_t elfnotes_sz,\n\t\t\t\t\t\tstruct list_head *vc_list)\n{\n\tint i;\n\tElf64_Ehdr *ehdr_ptr;\n\tElf64_Phdr *phdr_ptr;\n\tloff_t vmcore_off;\n\tstruct vmcore *new;\n\n\tehdr_ptr = (Elf64_Ehdr *)elfptr;\n\tphdr_ptr = (Elf64_Phdr*)(elfptr + sizeof(Elf64_Ehdr));  \n\n\t \n\tvmcore_off = elfsz + elfnotes_sz;\n\n\tfor (i = 0; i < ehdr_ptr->e_phnum; i++, phdr_ptr++) {\n\t\tu64 paddr, start, end, size;\n\n\t\tif (phdr_ptr->p_type != PT_LOAD)\n\t\t\tcontinue;\n\n\t\tpaddr = phdr_ptr->p_offset;\n\t\tstart = rounddown(paddr, PAGE_SIZE);\n\t\tend = roundup(paddr + phdr_ptr->p_memsz, PAGE_SIZE);\n\t\tsize = end - start;\n\n\t\t \n\t\tnew = get_new_element();\n\t\tif (!new)\n\t\t\treturn -ENOMEM;\n\t\tnew->paddr = start;\n\t\tnew->size = size;\n\t\tlist_add_tail(&new->list, vc_list);\n\n\t\t \n\t\tphdr_ptr->p_offset = vmcore_off + (paddr - start);\n\t\tvmcore_off = vmcore_off + size;\n\t}\n\treturn 0;\n}\n\nstatic int __init process_ptload_program_headers_elf32(char *elfptr,\n\t\t\t\t\t\tsize_t elfsz,\n\t\t\t\t\t\tsize_t elfnotes_sz,\n\t\t\t\t\t\tstruct list_head *vc_list)\n{\n\tint i;\n\tElf32_Ehdr *ehdr_ptr;\n\tElf32_Phdr *phdr_ptr;\n\tloff_t vmcore_off;\n\tstruct vmcore *new;\n\n\tehdr_ptr = (Elf32_Ehdr *)elfptr;\n\tphdr_ptr = (Elf32_Phdr*)(elfptr + sizeof(Elf32_Ehdr));  \n\n\t \n\tvmcore_off = elfsz + elfnotes_sz;\n\n\tfor (i = 0; i < ehdr_ptr->e_phnum; i++, phdr_ptr++) {\n\t\tu64 paddr, start, end, size;\n\n\t\tif (phdr_ptr->p_type != PT_LOAD)\n\t\t\tcontinue;\n\n\t\tpaddr = phdr_ptr->p_offset;\n\t\tstart = rounddown(paddr, PAGE_SIZE);\n\t\tend = roundup(paddr + phdr_ptr->p_memsz, PAGE_SIZE);\n\t\tsize = end - start;\n\n\t\t \n\t\tnew = get_new_element();\n\t\tif (!new)\n\t\t\treturn -ENOMEM;\n\t\tnew->paddr = start;\n\t\tnew->size = size;\n\t\tlist_add_tail(&new->list, vc_list);\n\n\t\t \n\t\tphdr_ptr->p_offset = vmcore_off + (paddr - start);\n\t\tvmcore_off = vmcore_off + size;\n\t}\n\treturn 0;\n}\n\n \nstatic void set_vmcore_list_offsets(size_t elfsz, size_t elfnotes_sz,\n\t\t\t\t    struct list_head *vc_list)\n{\n\tloff_t vmcore_off;\n\tstruct vmcore *m;\n\n\t \n\tvmcore_off = elfsz + elfnotes_sz;\n\n\tlist_for_each_entry(m, vc_list, list) {\n\t\tm->offset = vmcore_off;\n\t\tvmcore_off += m->size;\n\t}\n}\n\nstatic void free_elfcorebuf(void)\n{\n\tfree_pages((unsigned long)elfcorebuf, get_order(elfcorebuf_sz_orig));\n\telfcorebuf = NULL;\n\tvfree(elfnotes_buf);\n\telfnotes_buf = NULL;\n}\n\nstatic int __init parse_crash_elf64_headers(void)\n{\n\tint rc=0;\n\tElf64_Ehdr ehdr;\n\tu64 addr;\n\n\taddr = elfcorehdr_addr;\n\n\t \n\trc = elfcorehdr_read((char *)&ehdr, sizeof(Elf64_Ehdr), &addr);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t \n\tif (memcmp(ehdr.e_ident, ELFMAG, SELFMAG) != 0 ||\n\t\t(ehdr.e_type != ET_CORE) ||\n\t\t!vmcore_elf64_check_arch(&ehdr) ||\n\t\tehdr.e_ident[EI_CLASS] != ELFCLASS64 ||\n\t\tehdr.e_ident[EI_VERSION] != EV_CURRENT ||\n\t\tehdr.e_version != EV_CURRENT ||\n\t\tehdr.e_ehsize != sizeof(Elf64_Ehdr) ||\n\t\tehdr.e_phentsize != sizeof(Elf64_Phdr) ||\n\t\tehdr.e_phnum == 0) {\n\t\tpr_warn(\"Warning: Core image elf header is not sane\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\telfcorebuf_sz_orig = sizeof(Elf64_Ehdr) +\n\t\t\t\tehdr.e_phnum * sizeof(Elf64_Phdr);\n\telfcorebuf_sz = elfcorebuf_sz_orig;\n\telfcorebuf = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\t      get_order(elfcorebuf_sz_orig));\n\tif (!elfcorebuf)\n\t\treturn -ENOMEM;\n\taddr = elfcorehdr_addr;\n\trc = elfcorehdr_read(elfcorebuf, elfcorebuf_sz_orig, &addr);\n\tif (rc < 0)\n\t\tgoto fail;\n\n\t \n\trc = merge_note_headers_elf64(elfcorebuf, &elfcorebuf_sz,\n\t\t\t\t      &elfnotes_buf, &elfnotes_sz);\n\tif (rc)\n\t\tgoto fail;\n\trc = process_ptload_program_headers_elf64(elfcorebuf, elfcorebuf_sz,\n\t\t\t\t\t\t  elfnotes_sz, &vmcore_list);\n\tif (rc)\n\t\tgoto fail;\n\tset_vmcore_list_offsets(elfcorebuf_sz, elfnotes_sz, &vmcore_list);\n\treturn 0;\nfail:\n\tfree_elfcorebuf();\n\treturn rc;\n}\n\nstatic int __init parse_crash_elf32_headers(void)\n{\n\tint rc=0;\n\tElf32_Ehdr ehdr;\n\tu64 addr;\n\n\taddr = elfcorehdr_addr;\n\n\t \n\trc = elfcorehdr_read((char *)&ehdr, sizeof(Elf32_Ehdr), &addr);\n\tif (rc < 0)\n\t\treturn rc;\n\n\t \n\tif (memcmp(ehdr.e_ident, ELFMAG, SELFMAG) != 0 ||\n\t\t(ehdr.e_type != ET_CORE) ||\n\t\t!vmcore_elf32_check_arch(&ehdr) ||\n\t\tehdr.e_ident[EI_CLASS] != ELFCLASS32||\n\t\tehdr.e_ident[EI_VERSION] != EV_CURRENT ||\n\t\tehdr.e_version != EV_CURRENT ||\n\t\tehdr.e_ehsize != sizeof(Elf32_Ehdr) ||\n\t\tehdr.e_phentsize != sizeof(Elf32_Phdr) ||\n\t\tehdr.e_phnum == 0) {\n\t\tpr_warn(\"Warning: Core image elf header is not sane\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\telfcorebuf_sz_orig = sizeof(Elf32_Ehdr) + ehdr.e_phnum * sizeof(Elf32_Phdr);\n\telfcorebuf_sz = elfcorebuf_sz_orig;\n\telfcorebuf = (void *)__get_free_pages(GFP_KERNEL | __GFP_ZERO,\n\t\t\t\t\t      get_order(elfcorebuf_sz_orig));\n\tif (!elfcorebuf)\n\t\treturn -ENOMEM;\n\taddr = elfcorehdr_addr;\n\trc = elfcorehdr_read(elfcorebuf, elfcorebuf_sz_orig, &addr);\n\tif (rc < 0)\n\t\tgoto fail;\n\n\t \n\trc = merge_note_headers_elf32(elfcorebuf, &elfcorebuf_sz,\n\t\t\t\t      &elfnotes_buf, &elfnotes_sz);\n\tif (rc)\n\t\tgoto fail;\n\trc = process_ptload_program_headers_elf32(elfcorebuf, elfcorebuf_sz,\n\t\t\t\t\t\t  elfnotes_sz, &vmcore_list);\n\tif (rc)\n\t\tgoto fail;\n\tset_vmcore_list_offsets(elfcorebuf_sz, elfnotes_sz, &vmcore_list);\n\treturn 0;\nfail:\n\tfree_elfcorebuf();\n\treturn rc;\n}\n\nstatic int __init parse_crash_elf_headers(void)\n{\n\tunsigned char e_ident[EI_NIDENT];\n\tu64 addr;\n\tint rc=0;\n\n\taddr = elfcorehdr_addr;\n\trc = elfcorehdr_read(e_ident, EI_NIDENT, &addr);\n\tif (rc < 0)\n\t\treturn rc;\n\tif (memcmp(e_ident, ELFMAG, SELFMAG) != 0) {\n\t\tpr_warn(\"Warning: Core image elf header not found\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (e_ident[EI_CLASS] == ELFCLASS64) {\n\t\trc = parse_crash_elf64_headers();\n\t\tif (rc)\n\t\t\treturn rc;\n\t} else if (e_ident[EI_CLASS] == ELFCLASS32) {\n\t\trc = parse_crash_elf32_headers();\n\t\tif (rc)\n\t\t\treturn rc;\n\t} else {\n\t\tpr_warn(\"Warning: Core image elf header is not sane\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t \n\tvmcore_size = get_vmcore_size(elfcorebuf_sz, elfnotes_sz,\n\t\t\t\t      &vmcore_list);\n\n\treturn 0;\n}\n\n#ifdef CONFIG_PROC_VMCORE_DEVICE_DUMP\n \nstatic void vmcoredd_write_header(void *buf, struct vmcoredd_data *data,\n\t\t\t\t  u32 size)\n{\n\tstruct vmcoredd_header *vdd_hdr = (struct vmcoredd_header *)buf;\n\n\tvdd_hdr->n_namesz = sizeof(vdd_hdr->name);\n\tvdd_hdr->n_descsz = size + sizeof(vdd_hdr->dump_name);\n\tvdd_hdr->n_type = NT_VMCOREDD;\n\n\tstrncpy((char *)vdd_hdr->name, VMCOREDD_NOTE_NAME,\n\t\tsizeof(vdd_hdr->name));\n\tmemcpy(vdd_hdr->dump_name, data->dump_name, sizeof(vdd_hdr->dump_name));\n}\n\n \nstatic void vmcoredd_update_program_headers(char *elfptr, size_t elfnotesz,\n\t\t\t\t\t    size_t vmcoreddsz)\n{\n\tunsigned char *e_ident = (unsigned char *)elfptr;\n\tu64 start, end, size;\n\tloff_t vmcore_off;\n\tu32 i;\n\n\tvmcore_off = elfcorebuf_sz + elfnotesz;\n\n\tif (e_ident[EI_CLASS] == ELFCLASS64) {\n\t\tElf64_Ehdr *ehdr = (Elf64_Ehdr *)elfptr;\n\t\tElf64_Phdr *phdr = (Elf64_Phdr *)(elfptr + sizeof(Elf64_Ehdr));\n\n\t\t \n\t\tfor (i = 0; i < ehdr->e_phnum; i++, phdr++) {\n\t\t\tif (phdr->p_type == PT_NOTE) {\n\t\t\t\t \n\t\t\t\tphdr->p_memsz = elfnotes_orig_sz + vmcoreddsz;\n\t\t\t\tphdr->p_filesz = phdr->p_memsz;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tstart = rounddown(phdr->p_offset, PAGE_SIZE);\n\t\t\tend = roundup(phdr->p_offset + phdr->p_memsz,\n\t\t\t\t      PAGE_SIZE);\n\t\t\tsize = end - start;\n\t\t\tphdr->p_offset = vmcore_off + (phdr->p_offset - start);\n\t\t\tvmcore_off += size;\n\t\t}\n\t} else {\n\t\tElf32_Ehdr *ehdr = (Elf32_Ehdr *)elfptr;\n\t\tElf32_Phdr *phdr = (Elf32_Phdr *)(elfptr + sizeof(Elf32_Ehdr));\n\n\t\t \n\t\tfor (i = 0; i < ehdr->e_phnum; i++, phdr++) {\n\t\t\tif (phdr->p_type == PT_NOTE) {\n\t\t\t\t \n\t\t\t\tphdr->p_memsz = elfnotes_orig_sz + vmcoreddsz;\n\t\t\t\tphdr->p_filesz = phdr->p_memsz;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tstart = rounddown(phdr->p_offset, PAGE_SIZE);\n\t\t\tend = roundup(phdr->p_offset + phdr->p_memsz,\n\t\t\t\t      PAGE_SIZE);\n\t\t\tsize = end - start;\n\t\t\tphdr->p_offset = vmcore_off + (phdr->p_offset - start);\n\t\t\tvmcore_off += size;\n\t\t}\n\t}\n}\n\n \nstatic void vmcoredd_update_size(size_t dump_size)\n{\n\tvmcoredd_orig_sz += dump_size;\n\telfnotes_sz = roundup(elfnotes_orig_sz, PAGE_SIZE) + vmcoredd_orig_sz;\n\tvmcoredd_update_program_headers(elfcorebuf, elfnotes_sz,\n\t\t\t\t\tvmcoredd_orig_sz);\n\n\t \n\tset_vmcore_list_offsets(elfcorebuf_sz, elfnotes_sz, &vmcore_list);\n\n\tvmcore_size = get_vmcore_size(elfcorebuf_sz, elfnotes_sz,\n\t\t\t\t      &vmcore_list);\n\tproc_vmcore->size = vmcore_size;\n}\n\n \nint vmcore_add_device_dump(struct vmcoredd_data *data)\n{\n\tstruct vmcoredd_node *dump;\n\tvoid *buf = NULL;\n\tsize_t data_size;\n\tint ret;\n\n\tif (vmcoredd_disabled) {\n\t\tpr_err_once(\"Device dump is disabled\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (!data || !strlen(data->dump_name) ||\n\t    !data->vmcoredd_callback || !data->size)\n\t\treturn -EINVAL;\n\n\tdump = vzalloc(sizeof(*dump));\n\tif (!dump) {\n\t\tret = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\n\t \n\tdata_size = roundup(sizeof(struct vmcoredd_header) + data->size,\n\t\t\t    PAGE_SIZE);\n\n\t \n\tbuf = vmcore_alloc_buf(data_size);\n\tif (!buf) {\n\t\tret = -ENOMEM;\n\t\tgoto out_err;\n\t}\n\n\tvmcoredd_write_header(buf, data, data_size -\n\t\t\t      sizeof(struct vmcoredd_header));\n\n\t \n\tret = data->vmcoredd_callback(data, buf +\n\t\t\t\t      sizeof(struct vmcoredd_header));\n\tif (ret)\n\t\tgoto out_err;\n\n\tdump->buf = buf;\n\tdump->size = data_size;\n\n\t \n\tmutex_lock(&vmcoredd_mutex);\n\tlist_add_tail(&dump->list, &vmcoredd_list);\n\tmutex_unlock(&vmcoredd_mutex);\n\n\tvmcoredd_update_size(data_size);\n\treturn 0;\n\nout_err:\n\tvfree(buf);\n\tvfree(dump);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(vmcore_add_device_dump);\n#endif  \n\n \nstatic void vmcore_free_device_dumps(void)\n{\n#ifdef CONFIG_PROC_VMCORE_DEVICE_DUMP\n\tmutex_lock(&vmcoredd_mutex);\n\twhile (!list_empty(&vmcoredd_list)) {\n\t\tstruct vmcoredd_node *dump;\n\n\t\tdump = list_first_entry(&vmcoredd_list, struct vmcoredd_node,\n\t\t\t\t\tlist);\n\t\tlist_del(&dump->list);\n\t\tvfree(dump->buf);\n\t\tvfree(dump);\n\t}\n\tmutex_unlock(&vmcoredd_mutex);\n#endif  \n}\n\n \nstatic int __init vmcore_init(void)\n{\n\tint rc = 0;\n\n\t \n\trc = elfcorehdr_alloc(&elfcorehdr_addr, &elfcorehdr_size);\n\tif (rc)\n\t\treturn rc;\n\t \n\tif (!(is_vmcore_usable()))\n\t\treturn rc;\n\trc = parse_crash_elf_headers();\n\tif (rc) {\n\t\telfcorehdr_free(elfcorehdr_addr);\n\t\tpr_warn(\"Kdump: vmcore not initialized\\n\");\n\t\treturn rc;\n\t}\n\telfcorehdr_free(elfcorehdr_addr);\n\telfcorehdr_addr = ELFCORE_ADDR_ERR;\n\n\tproc_vmcore = proc_create(\"vmcore\", S_IRUSR, NULL, &vmcore_proc_ops);\n\tif (proc_vmcore)\n\t\tproc_vmcore->size = vmcore_size;\n\treturn 0;\n}\nfs_initcall(vmcore_init);\n\n \nvoid vmcore_cleanup(void)\n{\n\tif (proc_vmcore) {\n\t\tproc_remove(proc_vmcore);\n\t\tproc_vmcore = NULL;\n\t}\n\n\t \n\twhile (!list_empty(&vmcore_list)) {\n\t\tstruct vmcore *m;\n\n\t\tm = list_first_entry(&vmcore_list, struct vmcore, list);\n\t\tlist_del(&m->list);\n\t\tkfree(m);\n\t}\n\tfree_elfcorebuf();\n\n\t \n\tvmcore_free_device_dumps();\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}