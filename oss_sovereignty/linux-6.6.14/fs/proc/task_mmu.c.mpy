{
  "module_name": "task_mmu.c",
  "hash_id": "a15c7f2269ff361b24a904926170200f725e13bc711d68c653db639c11500daf",
  "original_prompt": "Ingested from linux-6.6.14/fs/proc/task_mmu.c",
  "human_readable_source": "\n#include <linux/pagewalk.h>\n#include <linux/mm_inline.h>\n#include <linux/hugetlb.h>\n#include <linux/huge_mm.h>\n#include <linux/mount.h>\n#include <linux/ksm.h>\n#include <linux/seq_file.h>\n#include <linux/highmem.h>\n#include <linux/ptrace.h>\n#include <linux/slab.h>\n#include <linux/pagemap.h>\n#include <linux/mempolicy.h>\n#include <linux/rmap.h>\n#include <linux/swap.h>\n#include <linux/sched/mm.h>\n#include <linux/swapops.h>\n#include <linux/mmu_notifier.h>\n#include <linux/page_idle.h>\n#include <linux/shmem_fs.h>\n#include <linux/uaccess.h>\n#include <linux/pkeys.h>\n\n#include <asm/elf.h>\n#include <asm/tlb.h>\n#include <asm/tlbflush.h>\n#include \"internal.h\"\n\n#define SEQ_PUT_DEC(str, val) \\\n\t\tseq_put_decimal_ull_width(m, str, (val) << (PAGE_SHIFT-10), 8)\nvoid task_mem(struct seq_file *m, struct mm_struct *mm)\n{\n\tunsigned long text, lib, swap, anon, file, shmem;\n\tunsigned long hiwater_vm, total_vm, hiwater_rss, total_rss;\n\n\tanon = get_mm_counter(mm, MM_ANONPAGES);\n\tfile = get_mm_counter(mm, MM_FILEPAGES);\n\tshmem = get_mm_counter(mm, MM_SHMEMPAGES);\n\n\t \n\thiwater_vm = total_vm = mm->total_vm;\n\tif (hiwater_vm < mm->hiwater_vm)\n\t\thiwater_vm = mm->hiwater_vm;\n\thiwater_rss = total_rss = anon + file + shmem;\n\tif (hiwater_rss < mm->hiwater_rss)\n\t\thiwater_rss = mm->hiwater_rss;\n\n\t \n\ttext = PAGE_ALIGN(mm->end_code) - (mm->start_code & PAGE_MASK);\n\ttext = min(text, mm->exec_vm << PAGE_SHIFT);\n\tlib = (mm->exec_vm << PAGE_SHIFT) - text;\n\n\tswap = get_mm_counter(mm, MM_SWAPENTS);\n\tSEQ_PUT_DEC(\"VmPeak:\\t\", hiwater_vm);\n\tSEQ_PUT_DEC(\" kB\\nVmSize:\\t\", total_vm);\n\tSEQ_PUT_DEC(\" kB\\nVmLck:\\t\", mm->locked_vm);\n\tSEQ_PUT_DEC(\" kB\\nVmPin:\\t\", atomic64_read(&mm->pinned_vm));\n\tSEQ_PUT_DEC(\" kB\\nVmHWM:\\t\", hiwater_rss);\n\tSEQ_PUT_DEC(\" kB\\nVmRSS:\\t\", total_rss);\n\tSEQ_PUT_DEC(\" kB\\nRssAnon:\\t\", anon);\n\tSEQ_PUT_DEC(\" kB\\nRssFile:\\t\", file);\n\tSEQ_PUT_DEC(\" kB\\nRssShmem:\\t\", shmem);\n\tSEQ_PUT_DEC(\" kB\\nVmData:\\t\", mm->data_vm);\n\tSEQ_PUT_DEC(\" kB\\nVmStk:\\t\", mm->stack_vm);\n\tseq_put_decimal_ull_width(m,\n\t\t    \" kB\\nVmExe:\\t\", text >> 10, 8);\n\tseq_put_decimal_ull_width(m,\n\t\t    \" kB\\nVmLib:\\t\", lib >> 10, 8);\n\tseq_put_decimal_ull_width(m,\n\t\t    \" kB\\nVmPTE:\\t\", mm_pgtables_bytes(mm) >> 10, 8);\n\tSEQ_PUT_DEC(\" kB\\nVmSwap:\\t\", swap);\n\tseq_puts(m, \" kB\\n\");\n\thugetlb_report_usage(m, mm);\n}\n#undef SEQ_PUT_DEC\n\nunsigned long task_vsize(struct mm_struct *mm)\n{\n\treturn PAGE_SIZE * mm->total_vm;\n}\n\nunsigned long task_statm(struct mm_struct *mm,\n\t\t\t unsigned long *shared, unsigned long *text,\n\t\t\t unsigned long *data, unsigned long *resident)\n{\n\t*shared = get_mm_counter(mm, MM_FILEPAGES) +\n\t\t\tget_mm_counter(mm, MM_SHMEMPAGES);\n\t*text = (PAGE_ALIGN(mm->end_code) - (mm->start_code & PAGE_MASK))\n\t\t\t\t\t\t\t\t>> PAGE_SHIFT;\n\t*data = mm->data_vm + mm->stack_vm;\n\t*resident = *shared + get_mm_counter(mm, MM_ANONPAGES);\n\treturn mm->total_vm;\n}\n\n#ifdef CONFIG_NUMA\n \nstatic void hold_task_mempolicy(struct proc_maps_private *priv)\n{\n\tstruct task_struct *task = priv->task;\n\n\ttask_lock(task);\n\tpriv->task_mempolicy = get_task_policy(task);\n\tmpol_get(priv->task_mempolicy);\n\ttask_unlock(task);\n}\nstatic void release_task_mempolicy(struct proc_maps_private *priv)\n{\n\tmpol_put(priv->task_mempolicy);\n}\n#else\nstatic void hold_task_mempolicy(struct proc_maps_private *priv)\n{\n}\nstatic void release_task_mempolicy(struct proc_maps_private *priv)\n{\n}\n#endif\n\nstatic struct vm_area_struct *proc_get_vma(struct proc_maps_private *priv,\n\t\t\t\t\t\tloff_t *ppos)\n{\n\tstruct vm_area_struct *vma = vma_next(&priv->iter);\n\n\tif (vma) {\n\t\t*ppos = vma->vm_start;\n\t} else {\n\t\t*ppos = -2UL;\n\t\tvma = get_gate_vma(priv->mm);\n\t}\n\n\treturn vma;\n}\n\nstatic void *m_start(struct seq_file *m, loff_t *ppos)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tunsigned long last_addr = *ppos;\n\tstruct mm_struct *mm;\n\n\t \n\tif (last_addr == -1UL)\n\t\treturn NULL;\n\n\tpriv->task = get_proc_task(priv->inode);\n\tif (!priv->task)\n\t\treturn ERR_PTR(-ESRCH);\n\n\tmm = priv->mm;\n\tif (!mm || !mmget_not_zero(mm)) {\n\t\tput_task_struct(priv->task);\n\t\tpriv->task = NULL;\n\t\treturn NULL;\n\t}\n\n\tif (mmap_read_lock_killable(mm)) {\n\t\tmmput(mm);\n\t\tput_task_struct(priv->task);\n\t\tpriv->task = NULL;\n\t\treturn ERR_PTR(-EINTR);\n\t}\n\n\tvma_iter_init(&priv->iter, mm, last_addr);\n\thold_task_mempolicy(priv);\n\tif (last_addr == -2UL)\n\t\treturn get_gate_vma(mm);\n\n\treturn proc_get_vma(priv, ppos);\n}\n\nstatic void *m_next(struct seq_file *m, void *v, loff_t *ppos)\n{\n\tif (*ppos == -2UL) {\n\t\t*ppos = -1UL;\n\t\treturn NULL;\n\t}\n\treturn proc_get_vma(m->private, ppos);\n}\n\nstatic void m_stop(struct seq_file *m, void *v)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tstruct mm_struct *mm = priv->mm;\n\n\tif (!priv->task)\n\t\treturn;\n\n\trelease_task_mempolicy(priv);\n\tmmap_read_unlock(mm);\n\tmmput(mm);\n\tput_task_struct(priv->task);\n\tpriv->task = NULL;\n}\n\nstatic int proc_maps_open(struct inode *inode, struct file *file,\n\t\t\tconst struct seq_operations *ops, int psize)\n{\n\tstruct proc_maps_private *priv = __seq_open_private(file, ops, psize);\n\n\tif (!priv)\n\t\treturn -ENOMEM;\n\n\tpriv->inode = inode;\n\tpriv->mm = proc_mem_open(inode, PTRACE_MODE_READ);\n\tif (IS_ERR(priv->mm)) {\n\t\tint err = PTR_ERR(priv->mm);\n\n\t\tseq_release_private(inode, file);\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nstatic int proc_map_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *seq = file->private_data;\n\tstruct proc_maps_private *priv = seq->private;\n\n\tif (priv->mm)\n\t\tmmdrop(priv->mm);\n\n\treturn seq_release_private(inode, file);\n}\n\nstatic int do_maps_open(struct inode *inode, struct file *file,\n\t\t\tconst struct seq_operations *ops)\n{\n\treturn proc_maps_open(inode, file, ops,\n\t\t\t\tsizeof(struct proc_maps_private));\n}\n\nstatic void show_vma_header_prefix(struct seq_file *m,\n\t\t\t\t   unsigned long start, unsigned long end,\n\t\t\t\t   vm_flags_t flags, unsigned long long pgoff,\n\t\t\t\t   dev_t dev, unsigned long ino)\n{\n\tseq_setwidth(m, 25 + sizeof(void *) * 6 - 1);\n\tseq_put_hex_ll(m, NULL, start, 8);\n\tseq_put_hex_ll(m, \"-\", end, 8);\n\tseq_putc(m, ' ');\n\tseq_putc(m, flags & VM_READ ? 'r' : '-');\n\tseq_putc(m, flags & VM_WRITE ? 'w' : '-');\n\tseq_putc(m, flags & VM_EXEC ? 'x' : '-');\n\tseq_putc(m, flags & VM_MAYSHARE ? 's' : 'p');\n\tseq_put_hex_ll(m, \" \", pgoff, 8);\n\tseq_put_hex_ll(m, \" \", MAJOR(dev), 2);\n\tseq_put_hex_ll(m, \":\", MINOR(dev), 2);\n\tseq_put_decimal_ull(m, \" \", ino);\n\tseq_putc(m, ' ');\n}\n\nstatic void\nshow_map_vma(struct seq_file *m, struct vm_area_struct *vma)\n{\n\tstruct anon_vma_name *anon_name = NULL;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct file *file = vma->vm_file;\n\tvm_flags_t flags = vma->vm_flags;\n\tunsigned long ino = 0;\n\tunsigned long long pgoff = 0;\n\tunsigned long start, end;\n\tdev_t dev = 0;\n\tconst char *name = NULL;\n\n\tif (file) {\n\t\tstruct inode *inode = file_inode(vma->vm_file);\n\t\tdev = inode->i_sb->s_dev;\n\t\tino = inode->i_ino;\n\t\tpgoff = ((loff_t)vma->vm_pgoff) << PAGE_SHIFT;\n\t}\n\n\tstart = vma->vm_start;\n\tend = vma->vm_end;\n\tshow_vma_header_prefix(m, start, end, flags, pgoff, dev, ino);\n\tif (mm)\n\t\tanon_name = anon_vma_name(vma);\n\n\t \n\tif (file) {\n\t\tseq_pad(m, ' ');\n\t\t \n\t\tif (anon_name)\n\t\t\tseq_printf(m, \"[anon_shmem:%s]\", anon_name->name);\n\t\telse\n\t\t\tseq_file_path(m, file, \"\\n\");\n\t\tgoto done;\n\t}\n\n\tif (vma->vm_ops && vma->vm_ops->name) {\n\t\tname = vma->vm_ops->name(vma);\n\t\tif (name)\n\t\t\tgoto done;\n\t}\n\n\tname = arch_vma_name(vma);\n\tif (!name) {\n\t\tif (!mm) {\n\t\t\tname = \"[vdso]\";\n\t\t\tgoto done;\n\t\t}\n\n\t\tif (vma_is_initial_heap(vma)) {\n\t\t\tname = \"[heap]\";\n\t\t\tgoto done;\n\t\t}\n\n\t\tif (vma_is_initial_stack(vma)) {\n\t\t\tname = \"[stack]\";\n\t\t\tgoto done;\n\t\t}\n\n\t\tif (anon_name) {\n\t\t\tseq_pad(m, ' ');\n\t\t\tseq_printf(m, \"[anon:%s]\", anon_name->name);\n\t\t}\n\t}\n\ndone:\n\tif (name) {\n\t\tseq_pad(m, ' ');\n\t\tseq_puts(m, name);\n\t}\n\tseq_putc(m, '\\n');\n}\n\nstatic int show_map(struct seq_file *m, void *v)\n{\n\tshow_map_vma(m, v);\n\treturn 0;\n}\n\nstatic const struct seq_operations proc_pid_maps_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= show_map\n};\n\nstatic int pid_maps_open(struct inode *inode, struct file *file)\n{\n\treturn do_maps_open(inode, file, &proc_pid_maps_op);\n}\n\nconst struct file_operations proc_pid_maps_operations = {\n\t.open\t\t= pid_maps_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= proc_map_release,\n};\n\n \n#define PSS_SHIFT 12\n\n#ifdef CONFIG_PROC_PAGE_MONITOR\nstruct mem_size_stats {\n\tunsigned long resident;\n\tunsigned long shared_clean;\n\tunsigned long shared_dirty;\n\tunsigned long private_clean;\n\tunsigned long private_dirty;\n\tunsigned long referenced;\n\tunsigned long anonymous;\n\tunsigned long lazyfree;\n\tunsigned long anonymous_thp;\n\tunsigned long shmem_thp;\n\tunsigned long file_thp;\n\tunsigned long swap;\n\tunsigned long shared_hugetlb;\n\tunsigned long private_hugetlb;\n\tunsigned long ksm;\n\tu64 pss;\n\tu64 pss_anon;\n\tu64 pss_file;\n\tu64 pss_shmem;\n\tu64 pss_dirty;\n\tu64 pss_locked;\n\tu64 swap_pss;\n};\n\nstatic void smaps_page_accumulate(struct mem_size_stats *mss,\n\t\tstruct page *page, unsigned long size, unsigned long pss,\n\t\tbool dirty, bool locked, bool private)\n{\n\tmss->pss += pss;\n\n\tif (PageAnon(page))\n\t\tmss->pss_anon += pss;\n\telse if (PageSwapBacked(page))\n\t\tmss->pss_shmem += pss;\n\telse\n\t\tmss->pss_file += pss;\n\n\tif (locked)\n\t\tmss->pss_locked += pss;\n\n\tif (dirty || PageDirty(page)) {\n\t\tmss->pss_dirty += pss;\n\t\tif (private)\n\t\t\tmss->private_dirty += size;\n\t\telse\n\t\t\tmss->shared_dirty += size;\n\t} else {\n\t\tif (private)\n\t\t\tmss->private_clean += size;\n\t\telse\n\t\t\tmss->shared_clean += size;\n\t}\n}\n\nstatic void smaps_account(struct mem_size_stats *mss, struct page *page,\n\t\tbool compound, bool young, bool dirty, bool locked,\n\t\tbool migration)\n{\n\tint i, nr = compound ? compound_nr(page) : 1;\n\tunsigned long size = nr * PAGE_SIZE;\n\n\t \n\tif (PageAnon(page)) {\n\t\tmss->anonymous += size;\n\t\tif (!PageSwapBacked(page) && !dirty && !PageDirty(page))\n\t\t\tmss->lazyfree += size;\n\t}\n\n\tif (PageKsm(page))\n\t\tmss->ksm += size;\n\n\tmss->resident += size;\n\t \n\tif (young || page_is_young(page) || PageReferenced(page))\n\t\tmss->referenced += size;\n\n\t \n\tif ((page_count(page) == 1) || migration) {\n\t\tsmaps_page_accumulate(mss, page, size, size << PSS_SHIFT, dirty,\n\t\t\tlocked, true);\n\t\treturn;\n\t}\n\tfor (i = 0; i < nr; i++, page++) {\n\t\tint mapcount = page_mapcount(page);\n\t\tunsigned long pss = PAGE_SIZE << PSS_SHIFT;\n\t\tif (mapcount >= 2)\n\t\t\tpss /= mapcount;\n\t\tsmaps_page_accumulate(mss, page, PAGE_SIZE, pss, dirty, locked,\n\t\t\t\t      mapcount < 2);\n\t}\n}\n\n#ifdef CONFIG_SHMEM\nstatic int smaps_pte_hole(unsigned long addr, unsigned long end,\n\t\t\t  __always_unused int depth, struct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\n\tmss->swap += shmem_partial_swap_usage(walk->vma->vm_file->f_mapping,\n\t\t\t\t\t      linear_page_index(vma, addr),\n\t\t\t\t\t      linear_page_index(vma, end));\n\n\treturn 0;\n}\n#else\n#define smaps_pte_hole\t\tNULL\n#endif  \n\nstatic void smaps_pte_hole_lookup(unsigned long addr, struct mm_walk *walk)\n{\n#ifdef CONFIG_SHMEM\n\tif (walk->ops->pte_hole) {\n\t\t \n\t\tsmaps_pte_hole(addr, addr + PAGE_SIZE, 0, walk);\n\t}\n#endif\n}\n\nstatic void smaps_pte_entry(pte_t *pte, unsigned long addr,\n\t\tstruct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tbool locked = !!(vma->vm_flags & VM_LOCKED);\n\tstruct page *page = NULL;\n\tbool migration = false, young = false, dirty = false;\n\tpte_t ptent = ptep_get(pte);\n\n\tif (pte_present(ptent)) {\n\t\tpage = vm_normal_page(vma, addr, ptent);\n\t\tyoung = pte_young(ptent);\n\t\tdirty = pte_dirty(ptent);\n\t} else if (is_swap_pte(ptent)) {\n\t\tswp_entry_t swpent = pte_to_swp_entry(ptent);\n\n\t\tif (!non_swap_entry(swpent)) {\n\t\t\tint mapcount;\n\n\t\t\tmss->swap += PAGE_SIZE;\n\t\t\tmapcount = swp_swapcount(swpent);\n\t\t\tif (mapcount >= 2) {\n\t\t\t\tu64 pss_delta = (u64)PAGE_SIZE << PSS_SHIFT;\n\n\t\t\t\tdo_div(pss_delta, mapcount);\n\t\t\t\tmss->swap_pss += pss_delta;\n\t\t\t} else {\n\t\t\t\tmss->swap_pss += (u64)PAGE_SIZE << PSS_SHIFT;\n\t\t\t}\n\t\t} else if (is_pfn_swap_entry(swpent)) {\n\t\t\tif (is_migration_entry(swpent))\n\t\t\t\tmigration = true;\n\t\t\tpage = pfn_swap_entry_to_page(swpent);\n\t\t}\n\t} else {\n\t\tsmaps_pte_hole_lookup(addr, walk);\n\t\treturn;\n\t}\n\n\tif (!page)\n\t\treturn;\n\n\tsmaps_account(mss, page, false, young, dirty, locked, migration);\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic void smaps_pmd_entry(pmd_t *pmd, unsigned long addr,\n\t\tstruct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tbool locked = !!(vma->vm_flags & VM_LOCKED);\n\tstruct page *page = NULL;\n\tbool migration = false;\n\n\tif (pmd_present(*pmd)) {\n\t\tpage = vm_normal_page_pmd(vma, addr, *pmd);\n\t} else if (unlikely(thp_migration_supported() && is_swap_pmd(*pmd))) {\n\t\tswp_entry_t entry = pmd_to_swp_entry(*pmd);\n\n\t\tif (is_migration_entry(entry)) {\n\t\t\tmigration = true;\n\t\t\tpage = pfn_swap_entry_to_page(entry);\n\t\t}\n\t}\n\tif (IS_ERR_OR_NULL(page))\n\t\treturn;\n\tif (PageAnon(page))\n\t\tmss->anonymous_thp += HPAGE_PMD_SIZE;\n\telse if (PageSwapBacked(page))\n\t\tmss->shmem_thp += HPAGE_PMD_SIZE;\n\telse if (is_zone_device_page(page))\n\t\t ;\n\telse\n\t\tmss->file_thp += HPAGE_PMD_SIZE;\n\n\tsmaps_account(mss, page, true, pmd_young(*pmd), pmd_dirty(*pmd),\n\t\t      locked, migration);\n}\n#else\nstatic void smaps_pmd_entry(pmd_t *pmd, unsigned long addr,\n\t\tstruct mm_walk *walk)\n{\n}\n#endif\n\nstatic int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,\n\t\t\t   struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->vma;\n\tpte_t *pte;\n\tspinlock_t *ptl;\n\n\tptl = pmd_trans_huge_lock(pmd, vma);\n\tif (ptl) {\n\t\tsmaps_pmd_entry(pmd, addr, walk);\n\t\tspin_unlock(ptl);\n\t\tgoto out;\n\t}\n\n\tpte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n\tif (!pte) {\n\t\twalk->action = ACTION_AGAIN;\n\t\treturn 0;\n\t}\n\tfor (; addr != end; pte++, addr += PAGE_SIZE)\n\t\tsmaps_pte_entry(pte, addr, walk);\n\tpte_unmap_unlock(pte - 1, ptl);\nout:\n\tcond_resched();\n\treturn 0;\n}\n\nstatic void show_smap_vma_flags(struct seq_file *m, struct vm_area_struct *vma)\n{\n\t \n\tstatic const char mnemonics[BITS_PER_LONG][2] = {\n\t\t \n\t\t[0 ... (BITS_PER_LONG-1)] = \"??\",\n\n\t\t[ilog2(VM_READ)]\t= \"rd\",\n\t\t[ilog2(VM_WRITE)]\t= \"wr\",\n\t\t[ilog2(VM_EXEC)]\t= \"ex\",\n\t\t[ilog2(VM_SHARED)]\t= \"sh\",\n\t\t[ilog2(VM_MAYREAD)]\t= \"mr\",\n\t\t[ilog2(VM_MAYWRITE)]\t= \"mw\",\n\t\t[ilog2(VM_MAYEXEC)]\t= \"me\",\n\t\t[ilog2(VM_MAYSHARE)]\t= \"ms\",\n\t\t[ilog2(VM_GROWSDOWN)]\t= \"gd\",\n\t\t[ilog2(VM_PFNMAP)]\t= \"pf\",\n\t\t[ilog2(VM_LOCKED)]\t= \"lo\",\n\t\t[ilog2(VM_IO)]\t\t= \"io\",\n\t\t[ilog2(VM_SEQ_READ)]\t= \"sr\",\n\t\t[ilog2(VM_RAND_READ)]\t= \"rr\",\n\t\t[ilog2(VM_DONTCOPY)]\t= \"dc\",\n\t\t[ilog2(VM_DONTEXPAND)]\t= \"de\",\n\t\t[ilog2(VM_LOCKONFAULT)]\t= \"lf\",\n\t\t[ilog2(VM_ACCOUNT)]\t= \"ac\",\n\t\t[ilog2(VM_NORESERVE)]\t= \"nr\",\n\t\t[ilog2(VM_HUGETLB)]\t= \"ht\",\n\t\t[ilog2(VM_SYNC)]\t= \"sf\",\n\t\t[ilog2(VM_ARCH_1)]\t= \"ar\",\n\t\t[ilog2(VM_WIPEONFORK)]\t= \"wf\",\n\t\t[ilog2(VM_DONTDUMP)]\t= \"dd\",\n#ifdef CONFIG_ARM64_BTI\n\t\t[ilog2(VM_ARM64_BTI)]\t= \"bt\",\n#endif\n#ifdef CONFIG_MEM_SOFT_DIRTY\n\t\t[ilog2(VM_SOFTDIRTY)]\t= \"sd\",\n#endif\n\t\t[ilog2(VM_MIXEDMAP)]\t= \"mm\",\n\t\t[ilog2(VM_HUGEPAGE)]\t= \"hg\",\n\t\t[ilog2(VM_NOHUGEPAGE)]\t= \"nh\",\n\t\t[ilog2(VM_MERGEABLE)]\t= \"mg\",\n\t\t[ilog2(VM_UFFD_MISSING)]= \"um\",\n\t\t[ilog2(VM_UFFD_WP)]\t= \"uw\",\n#ifdef CONFIG_ARM64_MTE\n\t\t[ilog2(VM_MTE)]\t\t= \"mt\",\n\t\t[ilog2(VM_MTE_ALLOWED)]\t= \"\",\n#endif\n#ifdef CONFIG_ARCH_HAS_PKEYS\n\t\t \n\t\t[ilog2(VM_PKEY_BIT0)]\t= \"\",\n\t\t[ilog2(VM_PKEY_BIT1)]\t= \"\",\n\t\t[ilog2(VM_PKEY_BIT2)]\t= \"\",\n\t\t[ilog2(VM_PKEY_BIT3)]\t= \"\",\n#if VM_PKEY_BIT4\n\t\t[ilog2(VM_PKEY_BIT4)]\t= \"\",\n#endif\n#endif  \n#ifdef CONFIG_HAVE_ARCH_USERFAULTFD_MINOR\n\t\t[ilog2(VM_UFFD_MINOR)]\t= \"ui\",\n#endif  \n#ifdef CONFIG_X86_USER_SHADOW_STACK\n\t\t[ilog2(VM_SHADOW_STACK)] = \"ss\",\n#endif\n\t};\n\tsize_t i;\n\n\tseq_puts(m, \"VmFlags: \");\n\tfor (i = 0; i < BITS_PER_LONG; i++) {\n\t\tif (!mnemonics[i][0])\n\t\t\tcontinue;\n\t\tif (vma->vm_flags & (1UL << i)) {\n\t\t\tseq_putc(m, mnemonics[i][0]);\n\t\t\tseq_putc(m, mnemonics[i][1]);\n\t\t\tseq_putc(m, ' ');\n\t\t}\n\t}\n\tseq_putc(m, '\\n');\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\nstatic int smaps_hugetlb_range(pte_t *pte, unsigned long hmask,\n\t\t\t\t unsigned long addr, unsigned long end,\n\t\t\t\t struct mm_walk *walk)\n{\n\tstruct mem_size_stats *mss = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tstruct page *page = NULL;\n\tpte_t ptent = ptep_get(pte);\n\n\tif (pte_present(ptent)) {\n\t\tpage = vm_normal_page(vma, addr, ptent);\n\t} else if (is_swap_pte(ptent)) {\n\t\tswp_entry_t swpent = pte_to_swp_entry(ptent);\n\n\t\tif (is_pfn_swap_entry(swpent))\n\t\t\tpage = pfn_swap_entry_to_page(swpent);\n\t}\n\tif (page) {\n\t\tif (page_mapcount(page) >= 2 || hugetlb_pmd_shared(pte))\n\t\t\tmss->shared_hugetlb += huge_page_size(hstate_vma(vma));\n\t\telse\n\t\t\tmss->private_hugetlb += huge_page_size(hstate_vma(vma));\n\t}\n\treturn 0;\n}\n#else\n#define smaps_hugetlb_range\tNULL\n#endif  \n\nstatic const struct mm_walk_ops smaps_walk_ops = {\n\t.pmd_entry\t\t= smaps_pte_range,\n\t.hugetlb_entry\t\t= smaps_hugetlb_range,\n\t.walk_lock\t\t= PGWALK_RDLOCK,\n};\n\nstatic const struct mm_walk_ops smaps_shmem_walk_ops = {\n\t.pmd_entry\t\t= smaps_pte_range,\n\t.hugetlb_entry\t\t= smaps_hugetlb_range,\n\t.pte_hole\t\t= smaps_pte_hole,\n\t.walk_lock\t\t= PGWALK_RDLOCK,\n};\n\n \nstatic void smap_gather_stats(struct vm_area_struct *vma,\n\t\tstruct mem_size_stats *mss, unsigned long start)\n{\n\tconst struct mm_walk_ops *ops = &smaps_walk_ops;\n\n\t \n\tif (start >= vma->vm_end)\n\t\treturn;\n\n\tif (vma->vm_file && shmem_mapping(vma->vm_file->f_mapping)) {\n\t\t \n\t\tunsigned long shmem_swapped = shmem_swap_usage(vma);\n\n\t\tif (!start && (!shmem_swapped || (vma->vm_flags & VM_SHARED) ||\n\t\t\t\t\t!(vma->vm_flags & VM_WRITE))) {\n\t\t\tmss->swap += shmem_swapped;\n\t\t} else {\n\t\t\tops = &smaps_shmem_walk_ops;\n\t\t}\n\t}\n\n\t \n\tif (!start)\n\t\twalk_page_vma(vma, ops, mss);\n\telse\n\t\twalk_page_range(vma->vm_mm, start, vma->vm_end, ops, mss);\n}\n\n#define SEQ_PUT_DEC(str, val) \\\n\t\tseq_put_decimal_ull_width(m, str, (val) >> 10, 8)\n\n \nstatic void __show_smap(struct seq_file *m, const struct mem_size_stats *mss,\n\tbool rollup_mode)\n{\n\tSEQ_PUT_DEC(\"Rss:            \", mss->resident);\n\tSEQ_PUT_DEC(\" kB\\nPss:            \", mss->pss >> PSS_SHIFT);\n\tSEQ_PUT_DEC(\" kB\\nPss_Dirty:      \", mss->pss_dirty >> PSS_SHIFT);\n\tif (rollup_mode) {\n\t\t \n\t\tSEQ_PUT_DEC(\" kB\\nPss_Anon:       \",\n\t\t\tmss->pss_anon >> PSS_SHIFT);\n\t\tSEQ_PUT_DEC(\" kB\\nPss_File:       \",\n\t\t\tmss->pss_file >> PSS_SHIFT);\n\t\tSEQ_PUT_DEC(\" kB\\nPss_Shmem:      \",\n\t\t\tmss->pss_shmem >> PSS_SHIFT);\n\t}\n\tSEQ_PUT_DEC(\" kB\\nShared_Clean:   \", mss->shared_clean);\n\tSEQ_PUT_DEC(\" kB\\nShared_Dirty:   \", mss->shared_dirty);\n\tSEQ_PUT_DEC(\" kB\\nPrivate_Clean:  \", mss->private_clean);\n\tSEQ_PUT_DEC(\" kB\\nPrivate_Dirty:  \", mss->private_dirty);\n\tSEQ_PUT_DEC(\" kB\\nReferenced:     \", mss->referenced);\n\tSEQ_PUT_DEC(\" kB\\nAnonymous:      \", mss->anonymous);\n\tSEQ_PUT_DEC(\" kB\\nKSM:            \", mss->ksm);\n\tSEQ_PUT_DEC(\" kB\\nLazyFree:       \", mss->lazyfree);\n\tSEQ_PUT_DEC(\" kB\\nAnonHugePages:  \", mss->anonymous_thp);\n\tSEQ_PUT_DEC(\" kB\\nShmemPmdMapped: \", mss->shmem_thp);\n\tSEQ_PUT_DEC(\" kB\\nFilePmdMapped:  \", mss->file_thp);\n\tSEQ_PUT_DEC(\" kB\\nShared_Hugetlb: \", mss->shared_hugetlb);\n\tseq_put_decimal_ull_width(m, \" kB\\nPrivate_Hugetlb: \",\n\t\t\t\t  mss->private_hugetlb >> 10, 7);\n\tSEQ_PUT_DEC(\" kB\\nSwap:           \", mss->swap);\n\tSEQ_PUT_DEC(\" kB\\nSwapPss:        \",\n\t\t\t\t\tmss->swap_pss >> PSS_SHIFT);\n\tSEQ_PUT_DEC(\" kB\\nLocked:         \",\n\t\t\t\t\tmss->pss_locked >> PSS_SHIFT);\n\tseq_puts(m, \" kB\\n\");\n}\n\nstatic int show_smap(struct seq_file *m, void *v)\n{\n\tstruct vm_area_struct *vma = v;\n\tstruct mem_size_stats mss;\n\n\tmemset(&mss, 0, sizeof(mss));\n\n\tsmap_gather_stats(vma, &mss, 0);\n\n\tshow_map_vma(m, vma);\n\n\tSEQ_PUT_DEC(\"Size:           \", vma->vm_end - vma->vm_start);\n\tSEQ_PUT_DEC(\" kB\\nKernelPageSize: \", vma_kernel_pagesize(vma));\n\tSEQ_PUT_DEC(\" kB\\nMMUPageSize:    \", vma_mmu_pagesize(vma));\n\tseq_puts(m, \" kB\\n\");\n\n\t__show_smap(m, &mss, false);\n\n\tseq_printf(m, \"THPeligible:    %8u\\n\",\n\t\t   hugepage_vma_check(vma, vma->vm_flags, true, false, true));\n\n\tif (arch_pkeys_enabled())\n\t\tseq_printf(m, \"ProtectionKey:  %8u\\n\", vma_pkey(vma));\n\tshow_smap_vma_flags(m, vma);\n\n\treturn 0;\n}\n\nstatic int show_smaps_rollup(struct seq_file *m, void *v)\n{\n\tstruct proc_maps_private *priv = m->private;\n\tstruct mem_size_stats mss;\n\tstruct mm_struct *mm = priv->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long vma_start = 0, last_vma_end = 0;\n\tint ret = 0;\n\tVMA_ITERATOR(vmi, mm, 0);\n\n\tpriv->task = get_proc_task(priv->inode);\n\tif (!priv->task)\n\t\treturn -ESRCH;\n\n\tif (!mm || !mmget_not_zero(mm)) {\n\t\tret = -ESRCH;\n\t\tgoto out_put_task;\n\t}\n\n\tmemset(&mss, 0, sizeof(mss));\n\n\tret = mmap_read_lock_killable(mm);\n\tif (ret)\n\t\tgoto out_put_mm;\n\n\thold_task_mempolicy(priv);\n\tvma = vma_next(&vmi);\n\n\tif (unlikely(!vma))\n\t\tgoto empty_set;\n\n\tvma_start = vma->vm_start;\n\tdo {\n\t\tsmap_gather_stats(vma, &mss, 0);\n\t\tlast_vma_end = vma->vm_end;\n\n\t\t \n\t\tif (mmap_lock_is_contended(mm)) {\n\t\t\tvma_iter_invalidate(&vmi);\n\t\t\tmmap_read_unlock(mm);\n\t\t\tret = mmap_read_lock_killable(mm);\n\t\t\tif (ret) {\n\t\t\t\trelease_task_mempolicy(priv);\n\t\t\t\tgoto out_put_mm;\n\t\t\t}\n\n\t\t\t \n\t\t\tvma = vma_next(&vmi);\n\t\t\t \n\t\t\tif (!vma)\n\t\t\t\tbreak;\n\n\t\t\t \n\t\t\tif (vma->vm_start >= last_vma_end)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tif (vma->vm_end > last_vma_end)\n\t\t\t\tsmap_gather_stats(vma, &mss, last_vma_end);\n\t\t}\n\t} for_each_vma(vmi, vma);\n\nempty_set:\n\tshow_vma_header_prefix(m, vma_start, last_vma_end, 0, 0, 0, 0);\n\tseq_pad(m, ' ');\n\tseq_puts(m, \"[rollup]\\n\");\n\n\t__show_smap(m, &mss, true);\n\n\trelease_task_mempolicy(priv);\n\tmmap_read_unlock(mm);\n\nout_put_mm:\n\tmmput(mm);\nout_put_task:\n\tput_task_struct(priv->task);\n\tpriv->task = NULL;\n\n\treturn ret;\n}\n#undef SEQ_PUT_DEC\n\nstatic const struct seq_operations proc_pid_smaps_op = {\n\t.start\t= m_start,\n\t.next\t= m_next,\n\t.stop\t= m_stop,\n\t.show\t= show_smap\n};\n\nstatic int pid_smaps_open(struct inode *inode, struct file *file)\n{\n\treturn do_maps_open(inode, file, &proc_pid_smaps_op);\n}\n\nstatic int smaps_rollup_open(struct inode *inode, struct file *file)\n{\n\tint ret;\n\tstruct proc_maps_private *priv;\n\n\tpriv = kzalloc(sizeof(*priv), GFP_KERNEL_ACCOUNT);\n\tif (!priv)\n\t\treturn -ENOMEM;\n\n\tret = single_open(file, show_smaps_rollup, priv);\n\tif (ret)\n\t\tgoto out_free;\n\n\tpriv->inode = inode;\n\tpriv->mm = proc_mem_open(inode, PTRACE_MODE_READ);\n\tif (IS_ERR(priv->mm)) {\n\t\tret = PTR_ERR(priv->mm);\n\n\t\tsingle_release(inode, file);\n\t\tgoto out_free;\n\t}\n\n\treturn 0;\n\nout_free:\n\tkfree(priv);\n\treturn ret;\n}\n\nstatic int smaps_rollup_release(struct inode *inode, struct file *file)\n{\n\tstruct seq_file *seq = file->private_data;\n\tstruct proc_maps_private *priv = seq->private;\n\n\tif (priv->mm)\n\t\tmmdrop(priv->mm);\n\n\tkfree(priv);\n\treturn single_release(inode, file);\n}\n\nconst struct file_operations proc_pid_smaps_operations = {\n\t.open\t\t= pid_smaps_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= proc_map_release,\n};\n\nconst struct file_operations proc_pid_smaps_rollup_operations = {\n\t.open\t\t= smaps_rollup_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= smaps_rollup_release,\n};\n\nenum clear_refs_types {\n\tCLEAR_REFS_ALL = 1,\n\tCLEAR_REFS_ANON,\n\tCLEAR_REFS_MAPPED,\n\tCLEAR_REFS_SOFT_DIRTY,\n\tCLEAR_REFS_MM_HIWATER_RSS,\n\tCLEAR_REFS_LAST,\n};\n\nstruct clear_refs_private {\n\tenum clear_refs_types type;\n};\n\n#ifdef CONFIG_MEM_SOFT_DIRTY\n\nstatic inline bool pte_is_pinned(struct vm_area_struct *vma, unsigned long addr, pte_t pte)\n{\n\tstruct page *page;\n\n\tif (!pte_write(pte))\n\t\treturn false;\n\tif (!is_cow_mapping(vma->vm_flags))\n\t\treturn false;\n\tif (likely(!test_bit(MMF_HAS_PINNED, &vma->vm_mm->flags)))\n\t\treturn false;\n\tpage = vm_normal_page(vma, addr, pte);\n\tif (!page)\n\t\treturn false;\n\treturn page_maybe_dma_pinned(page);\n}\n\nstatic inline void clear_soft_dirty(struct vm_area_struct *vma,\n\t\tunsigned long addr, pte_t *pte)\n{\n\t \n\tpte_t ptent = ptep_get(pte);\n\n\tif (pte_present(ptent)) {\n\t\tpte_t old_pte;\n\n\t\tif (pte_is_pinned(vma, addr, ptent))\n\t\t\treturn;\n\t\told_pte = ptep_modify_prot_start(vma, addr, pte);\n\t\tptent = pte_wrprotect(old_pte);\n\t\tptent = pte_clear_soft_dirty(ptent);\n\t\tptep_modify_prot_commit(vma, addr, pte, old_pte, ptent);\n\t} else if (is_swap_pte(ptent)) {\n\t\tptent = pte_swp_clear_soft_dirty(ptent);\n\t\tset_pte_at(vma->vm_mm, addr, pte, ptent);\n\t}\n}\n#else\nstatic inline void clear_soft_dirty(struct vm_area_struct *vma,\n\t\tunsigned long addr, pte_t *pte)\n{\n}\n#endif\n\n#if defined(CONFIG_MEM_SOFT_DIRTY) && defined(CONFIG_TRANSPARENT_HUGEPAGE)\nstatic inline void clear_soft_dirty_pmd(struct vm_area_struct *vma,\n\t\tunsigned long addr, pmd_t *pmdp)\n{\n\tpmd_t old, pmd = *pmdp;\n\n\tif (pmd_present(pmd)) {\n\t\t \n\t\told = pmdp_invalidate(vma, addr, pmdp);\n\t\tif (pmd_dirty(old))\n\t\t\tpmd = pmd_mkdirty(pmd);\n\t\tif (pmd_young(old))\n\t\t\tpmd = pmd_mkyoung(pmd);\n\n\t\tpmd = pmd_wrprotect(pmd);\n\t\tpmd = pmd_clear_soft_dirty(pmd);\n\n\t\tset_pmd_at(vma->vm_mm, addr, pmdp, pmd);\n\t} else if (is_migration_entry(pmd_to_swp_entry(pmd))) {\n\t\tpmd = pmd_swp_clear_soft_dirty(pmd);\n\t\tset_pmd_at(vma->vm_mm, addr, pmdp, pmd);\n\t}\n}\n#else\nstatic inline void clear_soft_dirty_pmd(struct vm_area_struct *vma,\n\t\tunsigned long addr, pmd_t *pmdp)\n{\n}\n#endif\n\nstatic int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,\n\t\t\t\tunsigned long end, struct mm_walk *walk)\n{\n\tstruct clear_refs_private *cp = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tpte_t *pte, ptent;\n\tspinlock_t *ptl;\n\tstruct page *page;\n\n\tptl = pmd_trans_huge_lock(pmd, vma);\n\tif (ptl) {\n\t\tif (cp->type == CLEAR_REFS_SOFT_DIRTY) {\n\t\t\tclear_soft_dirty_pmd(vma, addr, pmd);\n\t\t\tgoto out;\n\t\t}\n\n\t\tif (!pmd_present(*pmd))\n\t\t\tgoto out;\n\n\t\tpage = pmd_page(*pmd);\n\n\t\t \n\t\tpmdp_test_and_clear_young(vma, addr, pmd);\n\t\ttest_and_clear_page_young(page);\n\t\tClearPageReferenced(page);\nout:\n\t\tspin_unlock(ptl);\n\t\treturn 0;\n\t}\n\n\tpte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);\n\tif (!pte) {\n\t\twalk->action = ACTION_AGAIN;\n\t\treturn 0;\n\t}\n\tfor (; addr != end; pte++, addr += PAGE_SIZE) {\n\t\tptent = ptep_get(pte);\n\n\t\tif (cp->type == CLEAR_REFS_SOFT_DIRTY) {\n\t\t\tclear_soft_dirty(vma, addr, pte);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!pte_present(ptent))\n\t\t\tcontinue;\n\n\t\tpage = vm_normal_page(vma, addr, ptent);\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\t \n\t\tptep_test_and_clear_young(vma, addr, pte);\n\t\ttest_and_clear_page_young(page);\n\t\tClearPageReferenced(page);\n\t}\n\tpte_unmap_unlock(pte - 1, ptl);\n\tcond_resched();\n\treturn 0;\n}\n\nstatic int clear_refs_test_walk(unsigned long start, unsigned long end,\n\t\t\t\tstruct mm_walk *walk)\n{\n\tstruct clear_refs_private *cp = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\n\tif (vma->vm_flags & VM_PFNMAP)\n\t\treturn 1;\n\n\t \n\tif (cp->type == CLEAR_REFS_ANON && vma->vm_file)\n\t\treturn 1;\n\tif (cp->type == CLEAR_REFS_MAPPED && !vma->vm_file)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic const struct mm_walk_ops clear_refs_walk_ops = {\n\t.pmd_entry\t\t= clear_refs_pte_range,\n\t.test_walk\t\t= clear_refs_test_walk,\n\t.walk_lock\t\t= PGWALK_WRLOCK,\n};\n\nstatic ssize_t clear_refs_write(struct file *file, const char __user *buf,\n\t\t\t\tsize_t count, loff_t *ppos)\n{\n\tstruct task_struct *task;\n\tchar buffer[PROC_NUMBUF];\n\tstruct mm_struct *mm;\n\tstruct vm_area_struct *vma;\n\tenum clear_refs_types type;\n\tint itype;\n\tint rv;\n\n\tmemset(buffer, 0, sizeof(buffer));\n\tif (count > sizeof(buffer) - 1)\n\t\tcount = sizeof(buffer) - 1;\n\tif (copy_from_user(buffer, buf, count))\n\t\treturn -EFAULT;\n\trv = kstrtoint(strstrip(buffer), 10, &itype);\n\tif (rv < 0)\n\t\treturn rv;\n\ttype = (enum clear_refs_types)itype;\n\tif (type < CLEAR_REFS_ALL || type >= CLEAR_REFS_LAST)\n\t\treturn -EINVAL;\n\n\ttask = get_proc_task(file_inode(file));\n\tif (!task)\n\t\treturn -ESRCH;\n\tmm = get_task_mm(task);\n\tif (mm) {\n\t\tVMA_ITERATOR(vmi, mm, 0);\n\t\tstruct mmu_notifier_range range;\n\t\tstruct clear_refs_private cp = {\n\t\t\t.type = type,\n\t\t};\n\n\t\tif (mmap_write_lock_killable(mm)) {\n\t\t\tcount = -EINTR;\n\t\t\tgoto out_mm;\n\t\t}\n\t\tif (type == CLEAR_REFS_MM_HIWATER_RSS) {\n\t\t\t \n\t\t\treset_mm_hiwater_rss(mm);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (type == CLEAR_REFS_SOFT_DIRTY) {\n\t\t\tfor_each_vma(vmi, vma) {\n\t\t\t\tif (!(vma->vm_flags & VM_SOFTDIRTY))\n\t\t\t\t\tcontinue;\n\t\t\t\tvm_flags_clear(vma, VM_SOFTDIRTY);\n\t\t\t\tvma_set_page_prot(vma);\n\t\t\t}\n\n\t\t\tinc_tlb_flush_pending(mm);\n\t\t\tmmu_notifier_range_init(&range, MMU_NOTIFY_SOFT_DIRTY,\n\t\t\t\t\t\t0, mm, 0, -1UL);\n\t\t\tmmu_notifier_invalidate_range_start(&range);\n\t\t}\n\t\twalk_page_range(mm, 0, -1, &clear_refs_walk_ops, &cp);\n\t\tif (type == CLEAR_REFS_SOFT_DIRTY) {\n\t\t\tmmu_notifier_invalidate_range_end(&range);\n\t\t\tflush_tlb_mm(mm);\n\t\t\tdec_tlb_flush_pending(mm);\n\t\t}\nout_unlock:\n\t\tmmap_write_unlock(mm);\nout_mm:\n\t\tmmput(mm);\n\t}\n\tput_task_struct(task);\n\n\treturn count;\n}\n\nconst struct file_operations proc_clear_refs_operations = {\n\t.write\t\t= clear_refs_write,\n\t.llseek\t\t= noop_llseek,\n};\n\ntypedef struct {\n\tu64 pme;\n} pagemap_entry_t;\n\nstruct pagemapread {\n\tint pos, len;\t\t \n\tpagemap_entry_t *buffer;\n\tbool show_pfn;\n};\n\n#define PAGEMAP_WALK_SIZE\t(PMD_SIZE)\n#define PAGEMAP_WALK_MASK\t(PMD_MASK)\n\n#define PM_ENTRY_BYTES\t\tsizeof(pagemap_entry_t)\n#define PM_PFRAME_BITS\t\t55\n#define PM_PFRAME_MASK\t\tGENMASK_ULL(PM_PFRAME_BITS - 1, 0)\n#define PM_SOFT_DIRTY\t\tBIT_ULL(55)\n#define PM_MMAP_EXCLUSIVE\tBIT_ULL(56)\n#define PM_UFFD_WP\t\tBIT_ULL(57)\n#define PM_FILE\t\t\tBIT_ULL(61)\n#define PM_SWAP\t\t\tBIT_ULL(62)\n#define PM_PRESENT\t\tBIT_ULL(63)\n\n#define PM_END_OF_BUFFER    1\n\nstatic inline pagemap_entry_t make_pme(u64 frame, u64 flags)\n{\n\treturn (pagemap_entry_t) { .pme = (frame & PM_PFRAME_MASK) | flags };\n}\n\nstatic int add_to_pagemap(unsigned long addr, pagemap_entry_t *pme,\n\t\t\t  struct pagemapread *pm)\n{\n\tpm->buffer[pm->pos++] = *pme;\n\tif (pm->pos >= pm->len)\n\t\treturn PM_END_OF_BUFFER;\n\treturn 0;\n}\n\nstatic int pagemap_pte_hole(unsigned long start, unsigned long end,\n\t\t\t    __always_unused int depth, struct mm_walk *walk)\n{\n\tstruct pagemapread *pm = walk->private;\n\tunsigned long addr = start;\n\tint err = 0;\n\n\twhile (addr < end) {\n\t\tstruct vm_area_struct *vma = find_vma(walk->mm, addr);\n\t\tpagemap_entry_t pme = make_pme(0, 0);\n\t\t \n\t\tunsigned long hole_end;\n\n\t\tif (vma)\n\t\t\thole_end = min(end, vma->vm_start);\n\t\telse\n\t\t\thole_end = end;\n\n\t\tfor (; addr < hole_end; addr += PAGE_SIZE) {\n\t\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\n\t\tif (!vma)\n\t\t\tbreak;\n\n\t\t \n\t\tif (vma->vm_flags & VM_SOFTDIRTY)\n\t\t\tpme = make_pme(0, PM_SOFT_DIRTY);\n\t\tfor (; addr < min(end, vma->vm_end); addr += PAGE_SIZE) {\n\t\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\t}\nout:\n\treturn err;\n}\n\nstatic pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,\n\t\tstruct vm_area_struct *vma, unsigned long addr, pte_t pte)\n{\n\tu64 frame = 0, flags = 0;\n\tstruct page *page = NULL;\n\tbool migration = false;\n\n\tif (pte_present(pte)) {\n\t\tif (pm->show_pfn)\n\t\t\tframe = pte_pfn(pte);\n\t\tflags |= PM_PRESENT;\n\t\tpage = vm_normal_page(vma, addr, pte);\n\t\tif (pte_soft_dirty(pte))\n\t\t\tflags |= PM_SOFT_DIRTY;\n\t\tif (pte_uffd_wp(pte))\n\t\t\tflags |= PM_UFFD_WP;\n\t} else if (is_swap_pte(pte)) {\n\t\tswp_entry_t entry;\n\t\tif (pte_swp_soft_dirty(pte))\n\t\t\tflags |= PM_SOFT_DIRTY;\n\t\tif (pte_swp_uffd_wp(pte))\n\t\t\tflags |= PM_UFFD_WP;\n\t\tentry = pte_to_swp_entry(pte);\n\t\tif (pm->show_pfn) {\n\t\t\tpgoff_t offset;\n\t\t\t \n\t\t\tif (is_pfn_swap_entry(entry))\n\t\t\t\toffset = swp_offset_pfn(entry);\n\t\t\telse\n\t\t\t\toffset = swp_offset(entry);\n\t\t\tframe = swp_type(entry) |\n\t\t\t    (offset << MAX_SWAPFILES_SHIFT);\n\t\t}\n\t\tflags |= PM_SWAP;\n\t\tmigration = is_migration_entry(entry);\n\t\tif (is_pfn_swap_entry(entry))\n\t\t\tpage = pfn_swap_entry_to_page(entry);\n\t\tif (pte_marker_entry_uffd_wp(entry))\n\t\t\tflags |= PM_UFFD_WP;\n\t}\n\n\tif (page && !PageAnon(page))\n\t\tflags |= PM_FILE;\n\tif (page && !migration && page_mapcount(page) == 1)\n\t\tflags |= PM_MMAP_EXCLUSIVE;\n\tif (vma->vm_flags & VM_SOFTDIRTY)\n\t\tflags |= PM_SOFT_DIRTY;\n\n\treturn make_pme(frame, flags);\n}\n\nstatic int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,\n\t\t\t     struct mm_walk *walk)\n{\n\tstruct vm_area_struct *vma = walk->vma;\n\tstruct pagemapread *pm = walk->private;\n\tspinlock_t *ptl;\n\tpte_t *pte, *orig_pte;\n\tint err = 0;\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tbool migration = false;\n\n\tptl = pmd_trans_huge_lock(pmdp, vma);\n\tif (ptl) {\n\t\tu64 flags = 0, frame = 0;\n\t\tpmd_t pmd = *pmdp;\n\t\tstruct page *page = NULL;\n\n\t\tif (vma->vm_flags & VM_SOFTDIRTY)\n\t\t\tflags |= PM_SOFT_DIRTY;\n\n\t\tif (pmd_present(pmd)) {\n\t\t\tpage = pmd_page(pmd);\n\n\t\t\tflags |= PM_PRESENT;\n\t\t\tif (pmd_soft_dirty(pmd))\n\t\t\t\tflags |= PM_SOFT_DIRTY;\n\t\t\tif (pmd_uffd_wp(pmd))\n\t\t\t\tflags |= PM_UFFD_WP;\n\t\t\tif (pm->show_pfn)\n\t\t\t\tframe = pmd_pfn(pmd) +\n\t\t\t\t\t((addr & ~PMD_MASK) >> PAGE_SHIFT);\n\t\t}\n#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION\n\t\telse if (is_swap_pmd(pmd)) {\n\t\t\tswp_entry_t entry = pmd_to_swp_entry(pmd);\n\t\t\tunsigned long offset;\n\n\t\t\tif (pm->show_pfn) {\n\t\t\t\tif (is_pfn_swap_entry(entry))\n\t\t\t\t\toffset = swp_offset_pfn(entry);\n\t\t\t\telse\n\t\t\t\t\toffset = swp_offset(entry);\n\t\t\t\toffset = offset +\n\t\t\t\t\t((addr & ~PMD_MASK) >> PAGE_SHIFT);\n\t\t\t\tframe = swp_type(entry) |\n\t\t\t\t\t(offset << MAX_SWAPFILES_SHIFT);\n\t\t\t}\n\t\t\tflags |= PM_SWAP;\n\t\t\tif (pmd_swp_soft_dirty(pmd))\n\t\t\t\tflags |= PM_SOFT_DIRTY;\n\t\t\tif (pmd_swp_uffd_wp(pmd))\n\t\t\t\tflags |= PM_UFFD_WP;\n\t\t\tVM_BUG_ON(!is_pmd_migration_entry(pmd));\n\t\t\tmigration = is_migration_entry(entry);\n\t\t\tpage = pfn_swap_entry_to_page(entry);\n\t\t}\n#endif\n\n\t\tif (page && !migration && page_mapcount(page) == 1)\n\t\t\tflags |= PM_MMAP_EXCLUSIVE;\n\n\t\tfor (; addr != end; addr += PAGE_SIZE) {\n\t\t\tpagemap_entry_t pme = make_pme(frame, flags);\n\n\t\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\t\t\tif (pm->show_pfn) {\n\t\t\t\tif (flags & PM_PRESENT)\n\t\t\t\t\tframe++;\n\t\t\t\telse if (flags & PM_SWAP)\n\t\t\t\t\tframe += (1 << MAX_SWAPFILES_SHIFT);\n\t\t\t}\n\t\t}\n\t\tspin_unlock(ptl);\n\t\treturn err;\n\t}\n#endif  \n\n\t \n\torig_pte = pte = pte_offset_map_lock(walk->mm, pmdp, addr, &ptl);\n\tif (!pte) {\n\t\twalk->action = ACTION_AGAIN;\n\t\treturn err;\n\t}\n\tfor (; addr < end; pte++, addr += PAGE_SIZE) {\n\t\tpagemap_entry_t pme;\n\n\t\tpme = pte_to_pagemap_entry(pm, vma, addr, ptep_get(pte));\n\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\tif (err)\n\t\t\tbreak;\n\t}\n\tpte_unmap_unlock(orig_pte, ptl);\n\n\tcond_resched();\n\n\treturn err;\n}\n\n#ifdef CONFIG_HUGETLB_PAGE\n \nstatic int pagemap_hugetlb_range(pte_t *ptep, unsigned long hmask,\n\t\t\t\t unsigned long addr, unsigned long end,\n\t\t\t\t struct mm_walk *walk)\n{\n\tstruct pagemapread *pm = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tu64 flags = 0, frame = 0;\n\tint err = 0;\n\tpte_t pte;\n\n\tif (vma->vm_flags & VM_SOFTDIRTY)\n\t\tflags |= PM_SOFT_DIRTY;\n\n\tpte = huge_ptep_get(ptep);\n\tif (pte_present(pte)) {\n\t\tstruct page *page = pte_page(pte);\n\n\t\tif (!PageAnon(page))\n\t\t\tflags |= PM_FILE;\n\n\t\tif (page_mapcount(page) == 1)\n\t\t\tflags |= PM_MMAP_EXCLUSIVE;\n\n\t\tif (huge_pte_uffd_wp(pte))\n\t\t\tflags |= PM_UFFD_WP;\n\n\t\tflags |= PM_PRESENT;\n\t\tif (pm->show_pfn)\n\t\t\tframe = pte_pfn(pte) +\n\t\t\t\t((addr & ~hmask) >> PAGE_SHIFT);\n\t} else if (pte_swp_uffd_wp_any(pte)) {\n\t\tflags |= PM_UFFD_WP;\n\t}\n\n\tfor (; addr != end; addr += PAGE_SIZE) {\n\t\tpagemap_entry_t pme = make_pme(frame, flags);\n\n\t\terr = add_to_pagemap(addr, &pme, pm);\n\t\tif (err)\n\t\t\treturn err;\n\t\tif (pm->show_pfn && (flags & PM_PRESENT))\n\t\t\tframe++;\n\t}\n\n\tcond_resched();\n\n\treturn err;\n}\n#else\n#define pagemap_hugetlb_range\tNULL\n#endif  \n\nstatic const struct mm_walk_ops pagemap_ops = {\n\t.pmd_entry\t= pagemap_pmd_range,\n\t.pte_hole\t= pagemap_pte_hole,\n\t.hugetlb_entry\t= pagemap_hugetlb_range,\n\t.walk_lock\t= PGWALK_RDLOCK,\n};\n\n \nstatic ssize_t pagemap_read(struct file *file, char __user *buf,\n\t\t\t    size_t count, loff_t *ppos)\n{\n\tstruct mm_struct *mm = file->private_data;\n\tstruct pagemapread pm;\n\tunsigned long src;\n\tunsigned long svpfn;\n\tunsigned long start_vaddr;\n\tunsigned long end_vaddr;\n\tint ret = 0, copied = 0;\n\n\tif (!mm || !mmget_not_zero(mm))\n\t\tgoto out;\n\n\tret = -EINVAL;\n\t \n\tif ((*ppos % PM_ENTRY_BYTES) || (count % PM_ENTRY_BYTES))\n\t\tgoto out_mm;\n\n\tret = 0;\n\tif (!count)\n\t\tgoto out_mm;\n\n\t \n\tpm.show_pfn = file_ns_capable(file, &init_user_ns, CAP_SYS_ADMIN);\n\n\tpm.len = (PAGEMAP_WALK_SIZE >> PAGE_SHIFT);\n\tpm.buffer = kmalloc_array(pm.len, PM_ENTRY_BYTES, GFP_KERNEL);\n\tret = -ENOMEM;\n\tif (!pm.buffer)\n\t\tgoto out_mm;\n\n\tsrc = *ppos;\n\tsvpfn = src / PM_ENTRY_BYTES;\n\tend_vaddr = mm->task_size;\n\n\t \n\tstart_vaddr = end_vaddr;\n\tif (svpfn <= (ULONG_MAX >> PAGE_SHIFT)) {\n\t\tunsigned long end;\n\n\t\tret = mmap_read_lock_killable(mm);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t\tstart_vaddr = untagged_addr_remote(mm, svpfn << PAGE_SHIFT);\n\t\tmmap_read_unlock(mm);\n\n\t\tend = start_vaddr + ((count / PM_ENTRY_BYTES) << PAGE_SHIFT);\n\t\tif (end >= start_vaddr && end < mm->task_size)\n\t\t\tend_vaddr = end;\n\t}\n\n\t \n\tif (start_vaddr > mm->task_size)\n\t\tstart_vaddr = end_vaddr;\n\n\tret = 0;\n\twhile (count && (start_vaddr < end_vaddr)) {\n\t\tint len;\n\t\tunsigned long end;\n\n\t\tpm.pos = 0;\n\t\tend = (start_vaddr + PAGEMAP_WALK_SIZE) & PAGEMAP_WALK_MASK;\n\t\t \n\t\tif (end < start_vaddr || end > end_vaddr)\n\t\t\tend = end_vaddr;\n\t\tret = mmap_read_lock_killable(mm);\n\t\tif (ret)\n\t\t\tgoto out_free;\n\t\tret = walk_page_range(mm, start_vaddr, end, &pagemap_ops, &pm);\n\t\tmmap_read_unlock(mm);\n\t\tstart_vaddr = end;\n\n\t\tlen = min(count, PM_ENTRY_BYTES * pm.pos);\n\t\tif (copy_to_user(buf, pm.buffer, len)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_free;\n\t\t}\n\t\tcopied += len;\n\t\tbuf += len;\n\t\tcount -= len;\n\t}\n\t*ppos += copied;\n\tif (!ret || ret == PM_END_OF_BUFFER)\n\t\tret = copied;\n\nout_free:\n\tkfree(pm.buffer);\nout_mm:\n\tmmput(mm);\nout:\n\treturn ret;\n}\n\nstatic int pagemap_open(struct inode *inode, struct file *file)\n{\n\tstruct mm_struct *mm;\n\n\tmm = proc_mem_open(inode, PTRACE_MODE_READ);\n\tif (IS_ERR(mm))\n\t\treturn PTR_ERR(mm);\n\tfile->private_data = mm;\n\treturn 0;\n}\n\nstatic int pagemap_release(struct inode *inode, struct file *file)\n{\n\tstruct mm_struct *mm = file->private_data;\n\n\tif (mm)\n\t\tmmdrop(mm);\n\treturn 0;\n}\n\nconst struct file_operations proc_pagemap_operations = {\n\t.llseek\t\t= mem_lseek,  \n\t.read\t\t= pagemap_read,\n\t.open\t\t= pagemap_open,\n\t.release\t= pagemap_release,\n};\n#endif  \n\n#ifdef CONFIG_NUMA\n\nstruct numa_maps {\n\tunsigned long pages;\n\tunsigned long anon;\n\tunsigned long active;\n\tunsigned long writeback;\n\tunsigned long mapcount_max;\n\tunsigned long dirty;\n\tunsigned long swapcache;\n\tunsigned long node[MAX_NUMNODES];\n};\n\nstruct numa_maps_private {\n\tstruct proc_maps_private proc_maps;\n\tstruct numa_maps md;\n};\n\nstatic void gather_stats(struct page *page, struct numa_maps *md, int pte_dirty,\n\t\t\tunsigned long nr_pages)\n{\n\tint count = page_mapcount(page);\n\n\tmd->pages += nr_pages;\n\tif (pte_dirty || PageDirty(page))\n\t\tmd->dirty += nr_pages;\n\n\tif (PageSwapCache(page))\n\t\tmd->swapcache += nr_pages;\n\n\tif (PageActive(page) || PageUnevictable(page))\n\t\tmd->active += nr_pages;\n\n\tif (PageWriteback(page))\n\t\tmd->writeback += nr_pages;\n\n\tif (PageAnon(page))\n\t\tmd->anon += nr_pages;\n\n\tif (count > md->mapcount_max)\n\t\tmd->mapcount_max = count;\n\n\tmd->node[page_to_nid(page)] += nr_pages;\n}\n\nstatic struct page *can_gather_numa_stats(pte_t pte, struct vm_area_struct *vma,\n\t\tunsigned long addr)\n{\n\tstruct page *page;\n\tint nid;\n\n\tif (!pte_present(pte))\n\t\treturn NULL;\n\n\tpage = vm_normal_page(vma, addr, pte);\n\tif (!page || is_zone_device_page(page))\n\t\treturn NULL;\n\n\tif (PageReserved(page))\n\t\treturn NULL;\n\n\tnid = page_to_nid(page);\n\tif (!node_isset(nid, node_states[N_MEMORY]))\n\t\treturn NULL;\n\n\treturn page;\n}\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\nstatic struct page *can_gather_numa_stats_pmd(pmd_t pmd,\n\t\t\t\t\t      struct vm_area_struct *vma,\n\t\t\t\t\t      unsigned long addr)\n{\n\tstruct page *page;\n\tint nid;\n\n\tif (!pmd_present(pmd))\n\t\treturn NULL;\n\n\tpage = vm_normal_page_pmd(vma, addr, pmd);\n\tif (!page)\n\t\treturn NULL;\n\n\tif (PageReserved(page))\n\t\treturn NULL;\n\n\tnid = page_to_nid(page);\n\tif (!node_isset(nid, node_states[N_MEMORY]))\n\t\treturn NULL;\n\n\treturn page;\n}\n#endif\n\nstatic int gather_pte_stats(pmd_t *pmd, unsigned long addr,\n\t\tunsigned long end, struct mm_walk *walk)\n{\n\tstruct numa_maps *md = walk->private;\n\tstruct vm_area_struct *vma = walk->vma;\n\tspinlock_t *ptl;\n\tpte_t *orig_pte;\n\tpte_t *pte;\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n\tptl = pmd_trans_huge_lock(pmd, vma);\n\tif (ptl) {\n\t\tstruct page *page;\n\n\t\tpage = can_gather_numa_stats_pmd(*pmd, vma, addr);\n\t\tif (page)\n\t\t\tgather_stats(page, md, pmd_dirty(*pmd),\n\t\t\t\t     HPAGE_PMD_SIZE/PAGE_SIZE);\n\t\tspin_unlock(ptl);\n\t\treturn 0;\n\t}\n#endif\n\torig_pte = pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);\n\tif (!pte) {\n\t\twalk->action = ACTION_AGAIN;\n\t\treturn 0;\n\t}\n\tdo {\n\t\tpte_t ptent = ptep_get(pte);\n\t\tstruct page *page = can_gather_numa_stats(ptent, vma, addr);\n\t\tif (!page)\n\t\t\tcontinue;\n\t\tgather_stats(page, md, pte_dirty(ptent), 1);\n\n\t} while (pte++, addr += PAGE_SIZE, addr != end);\n\tpte_unmap_unlock(orig_pte, ptl);\n\tcond_resched();\n\treturn 0;\n}\n#ifdef CONFIG_HUGETLB_PAGE\nstatic int gather_hugetlb_stats(pte_t *pte, unsigned long hmask,\n\t\tunsigned long addr, unsigned long end, struct mm_walk *walk)\n{\n\tpte_t huge_pte = huge_ptep_get(pte);\n\tstruct numa_maps *md;\n\tstruct page *page;\n\n\tif (!pte_present(huge_pte))\n\t\treturn 0;\n\n\tpage = pte_page(huge_pte);\n\n\tmd = walk->private;\n\tgather_stats(page, md, pte_dirty(huge_pte), 1);\n\treturn 0;\n}\n\n#else\nstatic int gather_hugetlb_stats(pte_t *pte, unsigned long hmask,\n\t\tunsigned long addr, unsigned long end, struct mm_walk *walk)\n{\n\treturn 0;\n}\n#endif\n\nstatic const struct mm_walk_ops show_numa_ops = {\n\t.hugetlb_entry = gather_hugetlb_stats,\n\t.pmd_entry = gather_pte_stats,\n\t.walk_lock = PGWALK_RDLOCK,\n};\n\n \nstatic int show_numa_map(struct seq_file *m, void *v)\n{\n\tstruct numa_maps_private *numa_priv = m->private;\n\tstruct proc_maps_private *proc_priv = &numa_priv->proc_maps;\n\tstruct vm_area_struct *vma = v;\n\tstruct numa_maps *md = &numa_priv->md;\n\tstruct file *file = vma->vm_file;\n\tstruct mm_struct *mm = vma->vm_mm;\n\tstruct mempolicy *pol;\n\tchar buffer[64];\n\tint nid;\n\n\tif (!mm)\n\t\treturn 0;\n\n\t \n\tmemset(md, 0, sizeof(*md));\n\n\tpol = __get_vma_policy(vma, vma->vm_start);\n\tif (pol) {\n\t\tmpol_to_str(buffer, sizeof(buffer), pol);\n\t\tmpol_cond_put(pol);\n\t} else {\n\t\tmpol_to_str(buffer, sizeof(buffer), proc_priv->task_mempolicy);\n\t}\n\n\tseq_printf(m, \"%08lx %s\", vma->vm_start, buffer);\n\n\tif (file) {\n\t\tseq_puts(m, \" file=\");\n\t\tseq_file_path(m, file, \"\\n\\t= \");\n\t} else if (vma_is_initial_heap(vma)) {\n\t\tseq_puts(m, \" heap\");\n\t} else if (vma_is_initial_stack(vma)) {\n\t\tseq_puts(m, \" stack\");\n\t}\n\n\tif (is_vm_hugetlb_page(vma))\n\t\tseq_puts(m, \" huge\");\n\n\t \n\twalk_page_vma(vma, &show_numa_ops, md);\n\n\tif (!md->pages)\n\t\tgoto out;\n\n\tif (md->anon)\n\t\tseq_printf(m, \" anon=%lu\", md->anon);\n\n\tif (md->dirty)\n\t\tseq_printf(m, \" dirty=%lu\", md->dirty);\n\n\tif (md->pages != md->anon && md->pages != md->dirty)\n\t\tseq_printf(m, \" mapped=%lu\", md->pages);\n\n\tif (md->mapcount_max > 1)\n\t\tseq_printf(m, \" mapmax=%lu\", md->mapcount_max);\n\n\tif (md->swapcache)\n\t\tseq_printf(m, \" swapcache=%lu\", md->swapcache);\n\n\tif (md->active < md->pages && !is_vm_hugetlb_page(vma))\n\t\tseq_printf(m, \" active=%lu\", md->active);\n\n\tif (md->writeback)\n\t\tseq_printf(m, \" writeback=%lu\", md->writeback);\n\n\tfor_each_node_state(nid, N_MEMORY)\n\t\tif (md->node[nid])\n\t\t\tseq_printf(m, \" N%d=%lu\", nid, md->node[nid]);\n\n\tseq_printf(m, \" kernelpagesize_kB=%lu\", vma_kernel_pagesize(vma) >> 10);\nout:\n\tseq_putc(m, '\\n');\n\treturn 0;\n}\n\nstatic const struct seq_operations proc_pid_numa_maps_op = {\n\t.start  = m_start,\n\t.next   = m_next,\n\t.stop   = m_stop,\n\t.show   = show_numa_map,\n};\n\nstatic int pid_numa_maps_open(struct inode *inode, struct file *file)\n{\n\treturn proc_maps_open(inode, file, &proc_pid_numa_maps_op,\n\t\t\t\tsizeof(struct numa_maps_private));\n}\n\nconst struct file_operations proc_pid_numa_maps_operations = {\n\t.open\t\t= pid_numa_maps_open,\n\t.read\t\t= seq_read,\n\t.llseek\t\t= seq_lseek,\n\t.release\t= proc_map_release,\n};\n\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}