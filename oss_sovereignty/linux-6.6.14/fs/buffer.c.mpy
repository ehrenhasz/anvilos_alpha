{
  "module_name": "buffer.c",
  "hash_id": "f6426ae4d013dfebfcb17bec65a3eeda3280fe23f035a4cc62a6056a718f9f37",
  "original_prompt": "Ingested from linux-6.6.14/fs/buffer.c",
  "human_readable_source": "\n \n\n \n\n#include <linux/kernel.h>\n#include <linux/sched/signal.h>\n#include <linux/syscalls.h>\n#include <linux/fs.h>\n#include <linux/iomap.h>\n#include <linux/mm.h>\n#include <linux/percpu.h>\n#include <linux/slab.h>\n#include <linux/capability.h>\n#include <linux/blkdev.h>\n#include <linux/file.h>\n#include <linux/quotaops.h>\n#include <linux/highmem.h>\n#include <linux/export.h>\n#include <linux/backing-dev.h>\n#include <linux/writeback.h>\n#include <linux/hash.h>\n#include <linux/suspend.h>\n#include <linux/buffer_head.h>\n#include <linux/task_io_accounting_ops.h>\n#include <linux/bio.h>\n#include <linux/cpu.h>\n#include <linux/bitops.h>\n#include <linux/mpage.h>\n#include <linux/bit_spinlock.h>\n#include <linux/pagevec.h>\n#include <linux/sched/mm.h>\n#include <trace/events/block.h>\n#include <linux/fscrypt.h>\n#include <linux/fsverity.h>\n#include <linux/sched/isolation.h>\n\n#include \"internal.h\"\n\nstatic int fsync_buffers_list(spinlock_t *lock, struct list_head *list);\nstatic void submit_bh_wbc(blk_opf_t opf, struct buffer_head *bh,\n\t\t\t  struct writeback_control *wbc);\n\n#define BH_ENTRY(list) list_entry((list), struct buffer_head, b_assoc_buffers)\n\ninline void touch_buffer(struct buffer_head *bh)\n{\n\ttrace_block_touch_buffer(bh);\n\tfolio_mark_accessed(bh->b_folio);\n}\nEXPORT_SYMBOL(touch_buffer);\n\nvoid __lock_buffer(struct buffer_head *bh)\n{\n\twait_on_bit_lock_io(&bh->b_state, BH_Lock, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL(__lock_buffer);\n\nvoid unlock_buffer(struct buffer_head *bh)\n{\n\tclear_bit_unlock(BH_Lock, &bh->b_state);\n\tsmp_mb__after_atomic();\n\twake_up_bit(&bh->b_state, BH_Lock);\n}\nEXPORT_SYMBOL(unlock_buffer);\n\n \nvoid buffer_check_dirty_writeback(struct folio *folio,\n\t\t\t\t     bool *dirty, bool *writeback)\n{\n\tstruct buffer_head *head, *bh;\n\t*dirty = false;\n\t*writeback = false;\n\n\tBUG_ON(!folio_test_locked(folio));\n\n\thead = folio_buffers(folio);\n\tif (!head)\n\t\treturn;\n\n\tif (folio_test_writeback(folio))\n\t\t*writeback = true;\n\n\tbh = head;\n\tdo {\n\t\tif (buffer_locked(bh))\n\t\t\t*writeback = true;\n\n\t\tif (buffer_dirty(bh))\n\t\t\t*dirty = true;\n\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n}\n\n \nvoid __wait_on_buffer(struct buffer_head * bh)\n{\n\twait_on_bit_io(&bh->b_state, BH_Lock, TASK_UNINTERRUPTIBLE);\n}\nEXPORT_SYMBOL(__wait_on_buffer);\n\nstatic void buffer_io_error(struct buffer_head *bh, char *msg)\n{\n\tif (!test_bit(BH_Quiet, &bh->b_state))\n\t\tprintk_ratelimited(KERN_ERR\n\t\t\t\"Buffer I/O error on dev %pg, logical block %llu%s\\n\",\n\t\t\tbh->b_bdev, (unsigned long long)bh->b_blocknr, msg);\n}\n\n \nstatic void __end_buffer_read_notouch(struct buffer_head *bh, int uptodate)\n{\n\tif (uptodate) {\n\t\tset_buffer_uptodate(bh);\n\t} else {\n\t\t \n\t\tclear_buffer_uptodate(bh);\n\t}\n\tunlock_buffer(bh);\n}\n\n \nvoid end_buffer_read_sync(struct buffer_head *bh, int uptodate)\n{\n\t__end_buffer_read_notouch(bh, uptodate);\n\tput_bh(bh);\n}\nEXPORT_SYMBOL(end_buffer_read_sync);\n\nvoid end_buffer_write_sync(struct buffer_head *bh, int uptodate)\n{\n\tif (uptodate) {\n\t\tset_buffer_uptodate(bh);\n\t} else {\n\t\tbuffer_io_error(bh, \", lost sync page write\");\n\t\tmark_buffer_write_io_error(bh);\n\t\tclear_buffer_uptodate(bh);\n\t}\n\tunlock_buffer(bh);\n\tput_bh(bh);\n}\nEXPORT_SYMBOL(end_buffer_write_sync);\n\n \nstatic struct buffer_head *\n__find_get_block_slow(struct block_device *bdev, sector_t block)\n{\n\tstruct inode *bd_inode = bdev->bd_inode;\n\tstruct address_space *bd_mapping = bd_inode->i_mapping;\n\tstruct buffer_head *ret = NULL;\n\tpgoff_t index;\n\tstruct buffer_head *bh;\n\tstruct buffer_head *head;\n\tstruct folio *folio;\n\tint all_mapped = 1;\n\tstatic DEFINE_RATELIMIT_STATE(last_warned, HZ, 1);\n\n\tindex = block >> (PAGE_SHIFT - bd_inode->i_blkbits);\n\tfolio = __filemap_get_folio(bd_mapping, index, FGP_ACCESSED, 0);\n\tif (IS_ERR(folio))\n\t\tgoto out;\n\n\tspin_lock(&bd_mapping->private_lock);\n\thead = folio_buffers(folio);\n\tif (!head)\n\t\tgoto out_unlock;\n\tbh = head;\n\tdo {\n\t\tif (!buffer_mapped(bh))\n\t\t\tall_mapped = 0;\n\t\telse if (bh->b_blocknr == block) {\n\t\t\tret = bh;\n\t\t\tget_bh(bh);\n\t\t\tgoto out_unlock;\n\t\t}\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\n\t \n\tratelimit_set_flags(&last_warned, RATELIMIT_MSG_ON_RELEASE);\n\tif (all_mapped && __ratelimit(&last_warned)) {\n\t\tprintk(\"__find_get_block_slow() failed. block=%llu, \"\n\t\t       \"b_blocknr=%llu, b_state=0x%08lx, b_size=%zu, \"\n\t\t       \"device %pg blocksize: %d\\n\",\n\t\t       (unsigned long long)block,\n\t\t       (unsigned long long)bh->b_blocknr,\n\t\t       bh->b_state, bh->b_size, bdev,\n\t\t       1 << bd_inode->i_blkbits);\n\t}\nout_unlock:\n\tspin_unlock(&bd_mapping->private_lock);\n\tfolio_put(folio);\nout:\n\treturn ret;\n}\n\nstatic void end_buffer_async_read(struct buffer_head *bh, int uptodate)\n{\n\tunsigned long flags;\n\tstruct buffer_head *first;\n\tstruct buffer_head *tmp;\n\tstruct folio *folio;\n\tint folio_uptodate = 1;\n\n\tBUG_ON(!buffer_async_read(bh));\n\n\tfolio = bh->b_folio;\n\tif (uptodate) {\n\t\tset_buffer_uptodate(bh);\n\t} else {\n\t\tclear_buffer_uptodate(bh);\n\t\tbuffer_io_error(bh, \", async page read\");\n\t\tfolio_set_error(folio);\n\t}\n\n\t \n\tfirst = folio_buffers(folio);\n\tspin_lock_irqsave(&first->b_uptodate_lock, flags);\n\tclear_buffer_async_read(bh);\n\tunlock_buffer(bh);\n\ttmp = bh;\n\tdo {\n\t\tif (!buffer_uptodate(tmp))\n\t\t\tfolio_uptodate = 0;\n\t\tif (buffer_async_read(tmp)) {\n\t\t\tBUG_ON(!buffer_locked(tmp));\n\t\t\tgoto still_busy;\n\t\t}\n\t\ttmp = tmp->b_this_page;\n\t} while (tmp != bh);\n\tspin_unlock_irqrestore(&first->b_uptodate_lock, flags);\n\n\t \n\tif (folio_uptodate)\n\t\tfolio_mark_uptodate(folio);\n\tfolio_unlock(folio);\n\treturn;\n\nstill_busy:\n\tspin_unlock_irqrestore(&first->b_uptodate_lock, flags);\n\treturn;\n}\n\nstruct postprocess_bh_ctx {\n\tstruct work_struct work;\n\tstruct buffer_head *bh;\n};\n\nstatic void verify_bh(struct work_struct *work)\n{\n\tstruct postprocess_bh_ctx *ctx =\n\t\tcontainer_of(work, struct postprocess_bh_ctx, work);\n\tstruct buffer_head *bh = ctx->bh;\n\tbool valid;\n\n\tvalid = fsverity_verify_blocks(bh->b_folio, bh->b_size, bh_offset(bh));\n\tend_buffer_async_read(bh, valid);\n\tkfree(ctx);\n}\n\nstatic bool need_fsverity(struct buffer_head *bh)\n{\n\tstruct folio *folio = bh->b_folio;\n\tstruct inode *inode = folio->mapping->host;\n\n\treturn fsverity_active(inode) &&\n\t\t \n\t\tfolio->index < DIV_ROUND_UP(inode->i_size, PAGE_SIZE);\n}\n\nstatic void decrypt_bh(struct work_struct *work)\n{\n\tstruct postprocess_bh_ctx *ctx =\n\t\tcontainer_of(work, struct postprocess_bh_ctx, work);\n\tstruct buffer_head *bh = ctx->bh;\n\tint err;\n\n\terr = fscrypt_decrypt_pagecache_blocks(bh->b_folio, bh->b_size,\n\t\t\t\t\t       bh_offset(bh));\n\tif (err == 0 && need_fsverity(bh)) {\n\t\t \n\t\tINIT_WORK(&ctx->work, verify_bh);\n\t\tfsverity_enqueue_verify_work(&ctx->work);\n\t\treturn;\n\t}\n\tend_buffer_async_read(bh, err == 0);\n\tkfree(ctx);\n}\n\n \nstatic void end_buffer_async_read_io(struct buffer_head *bh, int uptodate)\n{\n\tstruct inode *inode = bh->b_folio->mapping->host;\n\tbool decrypt = fscrypt_inode_uses_fs_layer_crypto(inode);\n\tbool verify = need_fsverity(bh);\n\n\t \n\tif (uptodate && (decrypt || verify)) {\n\t\tstruct postprocess_bh_ctx *ctx =\n\t\t\tkmalloc(sizeof(*ctx), GFP_ATOMIC);\n\n\t\tif (ctx) {\n\t\t\tctx->bh = bh;\n\t\t\tif (decrypt) {\n\t\t\t\tINIT_WORK(&ctx->work, decrypt_bh);\n\t\t\t\tfscrypt_enqueue_decrypt_work(&ctx->work);\n\t\t\t} else {\n\t\t\t\tINIT_WORK(&ctx->work, verify_bh);\n\t\t\t\tfsverity_enqueue_verify_work(&ctx->work);\n\t\t\t}\n\t\t\treturn;\n\t\t}\n\t\tuptodate = 0;\n\t}\n\tend_buffer_async_read(bh, uptodate);\n}\n\n \nvoid end_buffer_async_write(struct buffer_head *bh, int uptodate)\n{\n\tunsigned long flags;\n\tstruct buffer_head *first;\n\tstruct buffer_head *tmp;\n\tstruct folio *folio;\n\n\tBUG_ON(!buffer_async_write(bh));\n\n\tfolio = bh->b_folio;\n\tif (uptodate) {\n\t\tset_buffer_uptodate(bh);\n\t} else {\n\t\tbuffer_io_error(bh, \", lost async page write\");\n\t\tmark_buffer_write_io_error(bh);\n\t\tclear_buffer_uptodate(bh);\n\t\tfolio_set_error(folio);\n\t}\n\n\tfirst = folio_buffers(folio);\n\tspin_lock_irqsave(&first->b_uptodate_lock, flags);\n\n\tclear_buffer_async_write(bh);\n\tunlock_buffer(bh);\n\ttmp = bh->b_this_page;\n\twhile (tmp != bh) {\n\t\tif (buffer_async_write(tmp)) {\n\t\t\tBUG_ON(!buffer_locked(tmp));\n\t\t\tgoto still_busy;\n\t\t}\n\t\ttmp = tmp->b_this_page;\n\t}\n\tspin_unlock_irqrestore(&first->b_uptodate_lock, flags);\n\tfolio_end_writeback(folio);\n\treturn;\n\nstill_busy:\n\tspin_unlock_irqrestore(&first->b_uptodate_lock, flags);\n\treturn;\n}\nEXPORT_SYMBOL(end_buffer_async_write);\n\n \nstatic void mark_buffer_async_read(struct buffer_head *bh)\n{\n\tbh->b_end_io = end_buffer_async_read_io;\n\tset_buffer_async_read(bh);\n}\n\nstatic void mark_buffer_async_write_endio(struct buffer_head *bh,\n\t\t\t\t\t  bh_end_io_t *handler)\n{\n\tbh->b_end_io = handler;\n\tset_buffer_async_write(bh);\n}\n\nvoid mark_buffer_async_write(struct buffer_head *bh)\n{\n\tmark_buffer_async_write_endio(bh, end_buffer_async_write);\n}\nEXPORT_SYMBOL(mark_buffer_async_write);\n\n\n \n\n \nstatic void __remove_assoc_queue(struct buffer_head *bh)\n{\n\tlist_del_init(&bh->b_assoc_buffers);\n\tWARN_ON(!bh->b_assoc_map);\n\tbh->b_assoc_map = NULL;\n}\n\nint inode_has_buffers(struct inode *inode)\n{\n\treturn !list_empty(&inode->i_data.private_list);\n}\n\n \nstatic int osync_buffers_list(spinlock_t *lock, struct list_head *list)\n{\n\tstruct buffer_head *bh;\n\tstruct list_head *p;\n\tint err = 0;\n\n\tspin_lock(lock);\nrepeat:\n\tlist_for_each_prev(p, list) {\n\t\tbh = BH_ENTRY(p);\n\t\tif (buffer_locked(bh)) {\n\t\t\tget_bh(bh);\n\t\t\tspin_unlock(lock);\n\t\t\twait_on_buffer(bh);\n\t\t\tif (!buffer_uptodate(bh))\n\t\t\t\terr = -EIO;\n\t\t\tbrelse(bh);\n\t\t\tspin_lock(lock);\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\tspin_unlock(lock);\n\treturn err;\n}\n\n \nint sync_mapping_buffers(struct address_space *mapping)\n{\n\tstruct address_space *buffer_mapping = mapping->private_data;\n\n\tif (buffer_mapping == NULL || list_empty(&mapping->private_list))\n\t\treturn 0;\n\n\treturn fsync_buffers_list(&buffer_mapping->private_lock,\n\t\t\t\t\t&mapping->private_list);\n}\nEXPORT_SYMBOL(sync_mapping_buffers);\n\n \nint generic_buffers_fsync_noflush(struct file *file, loff_t start, loff_t end,\n\t\t\t\t  bool datasync)\n{\n\tstruct inode *inode = file->f_mapping->host;\n\tint err;\n\tint ret;\n\n\terr = file_write_and_wait_range(file, start, end);\n\tif (err)\n\t\treturn err;\n\n\tret = sync_mapping_buffers(inode->i_mapping);\n\tif (!(inode->i_state & I_DIRTY_ALL))\n\t\tgoto out;\n\tif (datasync && !(inode->i_state & I_DIRTY_DATASYNC))\n\t\tgoto out;\n\n\terr = sync_inode_metadata(inode, 1);\n\tif (ret == 0)\n\t\tret = err;\n\nout:\n\t \n\terr = file_check_and_advance_wb_err(file);\n\tif (ret == 0)\n\t\tret = err;\n\treturn ret;\n}\nEXPORT_SYMBOL(generic_buffers_fsync_noflush);\n\n \nint generic_buffers_fsync(struct file *file, loff_t start, loff_t end,\n\t\t\t  bool datasync)\n{\n\tstruct inode *inode = file->f_mapping->host;\n\tint ret;\n\n\tret = generic_buffers_fsync_noflush(file, start, end, datasync);\n\tif (!ret)\n\t\tret = blkdev_issue_flush(inode->i_sb->s_bdev);\n\treturn ret;\n}\nEXPORT_SYMBOL(generic_buffers_fsync);\n\n \nvoid write_boundary_block(struct block_device *bdev,\n\t\t\tsector_t bblock, unsigned blocksize)\n{\n\tstruct buffer_head *bh = __find_get_block(bdev, bblock + 1, blocksize);\n\tif (bh) {\n\t\tif (buffer_dirty(bh))\n\t\t\twrite_dirty_buffer(bh, 0);\n\t\tput_bh(bh);\n\t}\n}\n\nvoid mark_buffer_dirty_inode(struct buffer_head *bh, struct inode *inode)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct address_space *buffer_mapping = bh->b_folio->mapping;\n\n\tmark_buffer_dirty(bh);\n\tif (!mapping->private_data) {\n\t\tmapping->private_data = buffer_mapping;\n\t} else {\n\t\tBUG_ON(mapping->private_data != buffer_mapping);\n\t}\n\tif (!bh->b_assoc_map) {\n\t\tspin_lock(&buffer_mapping->private_lock);\n\t\tlist_move_tail(&bh->b_assoc_buffers,\n\t\t\t\t&mapping->private_list);\n\t\tbh->b_assoc_map = mapping;\n\t\tspin_unlock(&buffer_mapping->private_lock);\n\t}\n}\nEXPORT_SYMBOL(mark_buffer_dirty_inode);\n\n \nbool block_dirty_folio(struct address_space *mapping, struct folio *folio)\n{\n\tstruct buffer_head *head;\n\tbool newly_dirty;\n\n\tspin_lock(&mapping->private_lock);\n\thead = folio_buffers(folio);\n\tif (head) {\n\t\tstruct buffer_head *bh = head;\n\n\t\tdo {\n\t\t\tset_buffer_dirty(bh);\n\t\t\tbh = bh->b_this_page;\n\t\t} while (bh != head);\n\t}\n\t \n\tfolio_memcg_lock(folio);\n\tnewly_dirty = !folio_test_set_dirty(folio);\n\tspin_unlock(&mapping->private_lock);\n\n\tif (newly_dirty)\n\t\t__folio_mark_dirty(folio, mapping, 1);\n\n\tfolio_memcg_unlock(folio);\n\n\tif (newly_dirty)\n\t\t__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);\n\n\treturn newly_dirty;\n}\nEXPORT_SYMBOL(block_dirty_folio);\n\n \nstatic int fsync_buffers_list(spinlock_t *lock, struct list_head *list)\n{\n\tstruct buffer_head *bh;\n\tstruct list_head tmp;\n\tstruct address_space *mapping;\n\tint err = 0, err2;\n\tstruct blk_plug plug;\n\n\tINIT_LIST_HEAD(&tmp);\n\tblk_start_plug(&plug);\n\n\tspin_lock(lock);\n\twhile (!list_empty(list)) {\n\t\tbh = BH_ENTRY(list->next);\n\t\tmapping = bh->b_assoc_map;\n\t\t__remove_assoc_queue(bh);\n\t\t \n\t\tsmp_mb();\n\t\tif (buffer_dirty(bh) || buffer_locked(bh)) {\n\t\t\tlist_add(&bh->b_assoc_buffers, &tmp);\n\t\t\tbh->b_assoc_map = mapping;\n\t\t\tif (buffer_dirty(bh)) {\n\t\t\t\tget_bh(bh);\n\t\t\t\tspin_unlock(lock);\n\t\t\t\t \n\t\t\t\twrite_dirty_buffer(bh, REQ_SYNC);\n\n\t\t\t\t \n\t\t\t\tbrelse(bh);\n\t\t\t\tspin_lock(lock);\n\t\t\t}\n\t\t}\n\t}\n\n\tspin_unlock(lock);\n\tblk_finish_plug(&plug);\n\tspin_lock(lock);\n\n\twhile (!list_empty(&tmp)) {\n\t\tbh = BH_ENTRY(tmp.prev);\n\t\tget_bh(bh);\n\t\tmapping = bh->b_assoc_map;\n\t\t__remove_assoc_queue(bh);\n\t\t \n\t\tsmp_mb();\n\t\tif (buffer_dirty(bh)) {\n\t\t\tlist_add(&bh->b_assoc_buffers,\n\t\t\t\t &mapping->private_list);\n\t\t\tbh->b_assoc_map = mapping;\n\t\t}\n\t\tspin_unlock(lock);\n\t\twait_on_buffer(bh);\n\t\tif (!buffer_uptodate(bh))\n\t\t\terr = -EIO;\n\t\tbrelse(bh);\n\t\tspin_lock(lock);\n\t}\n\t\n\tspin_unlock(lock);\n\terr2 = osync_buffers_list(lock, list);\n\tif (err)\n\t\treturn err;\n\telse\n\t\treturn err2;\n}\n\n \nvoid invalidate_inode_buffers(struct inode *inode)\n{\n\tif (inode_has_buffers(inode)) {\n\t\tstruct address_space *mapping = &inode->i_data;\n\t\tstruct list_head *list = &mapping->private_list;\n\t\tstruct address_space *buffer_mapping = mapping->private_data;\n\n\t\tspin_lock(&buffer_mapping->private_lock);\n\t\twhile (!list_empty(list))\n\t\t\t__remove_assoc_queue(BH_ENTRY(list->next));\n\t\tspin_unlock(&buffer_mapping->private_lock);\n\t}\n}\nEXPORT_SYMBOL(invalidate_inode_buffers);\n\n \nint remove_inode_buffers(struct inode *inode)\n{\n\tint ret = 1;\n\n\tif (inode_has_buffers(inode)) {\n\t\tstruct address_space *mapping = &inode->i_data;\n\t\tstruct list_head *list = &mapping->private_list;\n\t\tstruct address_space *buffer_mapping = mapping->private_data;\n\n\t\tspin_lock(&buffer_mapping->private_lock);\n\t\twhile (!list_empty(list)) {\n\t\t\tstruct buffer_head *bh = BH_ENTRY(list->next);\n\t\t\tif (buffer_dirty(bh)) {\n\t\t\t\tret = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\t__remove_assoc_queue(bh);\n\t\t}\n\t\tspin_unlock(&buffer_mapping->private_lock);\n\t}\n\treturn ret;\n}\n\n \nstruct buffer_head *folio_alloc_buffers(struct folio *folio, unsigned long size,\n\t\t\t\t\tbool retry)\n{\n\tstruct buffer_head *bh, *head;\n\tgfp_t gfp = GFP_NOFS | __GFP_ACCOUNT;\n\tlong offset;\n\tstruct mem_cgroup *memcg, *old_memcg;\n\n\tif (retry)\n\t\tgfp |= __GFP_NOFAIL;\n\n\t \n\tmemcg = folio_memcg(folio);\n\told_memcg = set_active_memcg(memcg);\n\n\thead = NULL;\n\toffset = folio_size(folio);\n\twhile ((offset -= size) >= 0) {\n\t\tbh = alloc_buffer_head(gfp);\n\t\tif (!bh)\n\t\t\tgoto no_grow;\n\n\t\tbh->b_this_page = head;\n\t\tbh->b_blocknr = -1;\n\t\thead = bh;\n\n\t\tbh->b_size = size;\n\n\t\t \n\t\tfolio_set_bh(bh, folio, offset);\n\t}\nout:\n\tset_active_memcg(old_memcg);\n\treturn head;\n \nno_grow:\n\tif (head) {\n\t\tdo {\n\t\t\tbh = head;\n\t\t\thead = head->b_this_page;\n\t\t\tfree_buffer_head(bh);\n\t\t} while (head);\n\t}\n\n\tgoto out;\n}\nEXPORT_SYMBOL_GPL(folio_alloc_buffers);\n\nstruct buffer_head *alloc_page_buffers(struct page *page, unsigned long size,\n\t\t\t\t       bool retry)\n{\n\treturn folio_alloc_buffers(page_folio(page), size, retry);\n}\nEXPORT_SYMBOL_GPL(alloc_page_buffers);\n\nstatic inline void link_dev_buffers(struct folio *folio,\n\t\tstruct buffer_head *head)\n{\n\tstruct buffer_head *bh, *tail;\n\n\tbh = head;\n\tdo {\n\t\ttail = bh;\n\t\tbh = bh->b_this_page;\n\t} while (bh);\n\ttail->b_this_page = head;\n\tfolio_attach_private(folio, head);\n}\n\nstatic sector_t blkdev_max_block(struct block_device *bdev, unsigned int size)\n{\n\tsector_t retval = ~((sector_t)0);\n\tloff_t sz = bdev_nr_bytes(bdev);\n\n\tif (sz) {\n\t\tunsigned int sizebits = blksize_bits(size);\n\t\tretval = (sz >> sizebits);\n\t}\n\treturn retval;\n}\n\n  \nstatic sector_t folio_init_buffers(struct folio *folio,\n\t\tstruct block_device *bdev, sector_t block, int size)\n{\n\tstruct buffer_head *head = folio_buffers(folio);\n\tstruct buffer_head *bh = head;\n\tbool uptodate = folio_test_uptodate(folio);\n\tsector_t end_block = blkdev_max_block(bdev, size);\n\n\tdo {\n\t\tif (!buffer_mapped(bh)) {\n\t\t\tbh->b_end_io = NULL;\n\t\t\tbh->b_private = NULL;\n\t\t\tbh->b_bdev = bdev;\n\t\t\tbh->b_blocknr = block;\n\t\t\tif (uptodate)\n\t\t\t\tset_buffer_uptodate(bh);\n\t\t\tif (block < end_block)\n\t\t\t\tset_buffer_mapped(bh);\n\t\t}\n\t\tblock++;\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\n\t \n\treturn end_block;\n}\n\n \nstatic int\ngrow_dev_page(struct block_device *bdev, sector_t block,\n\t      pgoff_t index, int size, int sizebits, gfp_t gfp)\n{\n\tstruct inode *inode = bdev->bd_inode;\n\tstruct folio *folio;\n\tstruct buffer_head *bh;\n\tsector_t end_block;\n\tint ret = 0;\n\tgfp_t gfp_mask;\n\n\tgfp_mask = mapping_gfp_constraint(inode->i_mapping, ~__GFP_FS) | gfp;\n\n\t \n\tgfp_mask |= __GFP_NOFAIL;\n\n\tfolio = __filemap_get_folio(inode->i_mapping, index,\n\t\t\tFGP_LOCK | FGP_ACCESSED | FGP_CREAT, gfp_mask);\n\n\tbh = folio_buffers(folio);\n\tif (bh) {\n\t\tif (bh->b_size == size) {\n\t\t\tend_block = folio_init_buffers(folio, bdev,\n\t\t\t\t\t(sector_t)index << sizebits, size);\n\t\t\tgoto done;\n\t\t}\n\t\tif (!try_to_free_buffers(folio))\n\t\t\tgoto failed;\n\t}\n\n\tbh = folio_alloc_buffers(folio, size, true);\n\n\t \n\tspin_lock(&inode->i_mapping->private_lock);\n\tlink_dev_buffers(folio, bh);\n\tend_block = folio_init_buffers(folio, bdev,\n\t\t\t(sector_t)index << sizebits, size);\n\tspin_unlock(&inode->i_mapping->private_lock);\ndone:\n\tret = (block < end_block) ? 1 : -ENXIO;\nfailed:\n\tfolio_unlock(folio);\n\tfolio_put(folio);\n\treturn ret;\n}\n\n \nstatic int\ngrow_buffers(struct block_device *bdev, sector_t block, int size, gfp_t gfp)\n{\n\tpgoff_t index;\n\tint sizebits;\n\n\tsizebits = PAGE_SHIFT - __ffs(size);\n\tindex = block >> sizebits;\n\n\t \n\tif (unlikely(index != block >> sizebits)) {\n\t\tprintk(KERN_ERR \"%s: requested out-of-range block %llu for \"\n\t\t\t\"device %pg\\n\",\n\t\t\t__func__, (unsigned long long)block,\n\t\t\tbdev);\n\t\treturn -EIO;\n\t}\n\n\t \n\treturn grow_dev_page(bdev, block, index, size, sizebits, gfp);\n}\n\nstatic struct buffer_head *\n__getblk_slow(struct block_device *bdev, sector_t block,\n\t     unsigned size, gfp_t gfp)\n{\n\t \n\tif (unlikely(size & (bdev_logical_block_size(bdev)-1) ||\n\t\t\t(size < 512 || size > PAGE_SIZE))) {\n\t\tprintk(KERN_ERR \"getblk(): invalid block size %d requested\\n\",\n\t\t\t\t\tsize);\n\t\tprintk(KERN_ERR \"logical block size: %d\\n\",\n\t\t\t\t\tbdev_logical_block_size(bdev));\n\n\t\tdump_stack();\n\t\treturn NULL;\n\t}\n\n\tfor (;;) {\n\t\tstruct buffer_head *bh;\n\t\tint ret;\n\n\t\tbh = __find_get_block(bdev, block, size);\n\t\tif (bh)\n\t\t\treturn bh;\n\n\t\tret = grow_buffers(bdev, block, size, gfp);\n\t\tif (ret < 0)\n\t\t\treturn NULL;\n\t}\n}\n\n \n\n \nvoid mark_buffer_dirty(struct buffer_head *bh)\n{\n\tWARN_ON_ONCE(!buffer_uptodate(bh));\n\n\ttrace_block_dirty_buffer(bh);\n\n\t \n\tif (buffer_dirty(bh)) {\n\t\tsmp_mb();\n\t\tif (buffer_dirty(bh))\n\t\t\treturn;\n\t}\n\n\tif (!test_set_buffer_dirty(bh)) {\n\t\tstruct folio *folio = bh->b_folio;\n\t\tstruct address_space *mapping = NULL;\n\n\t\tfolio_memcg_lock(folio);\n\t\tif (!folio_test_set_dirty(folio)) {\n\t\t\tmapping = folio->mapping;\n\t\t\tif (mapping)\n\t\t\t\t__folio_mark_dirty(folio, mapping, 0);\n\t\t}\n\t\tfolio_memcg_unlock(folio);\n\t\tif (mapping)\n\t\t\t__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);\n\t}\n}\nEXPORT_SYMBOL(mark_buffer_dirty);\n\nvoid mark_buffer_write_io_error(struct buffer_head *bh)\n{\n\tset_buffer_write_io_error(bh);\n\t \n\tif (bh->b_folio && bh->b_folio->mapping)\n\t\tmapping_set_error(bh->b_folio->mapping, -EIO);\n\tif (bh->b_assoc_map) {\n\t\tmapping_set_error(bh->b_assoc_map, -EIO);\n\t\terrseq_set(&bh->b_assoc_map->host->i_sb->s_wb_err, -EIO);\n\t}\n}\nEXPORT_SYMBOL(mark_buffer_write_io_error);\n\n \nvoid __brelse(struct buffer_head * buf)\n{\n\tif (atomic_read(&buf->b_count)) {\n\t\tput_bh(buf);\n\t\treturn;\n\t}\n\tWARN(1, KERN_ERR \"VFS: brelse: Trying to free free buffer\\n\");\n}\nEXPORT_SYMBOL(__brelse);\n\n \nvoid __bforget(struct buffer_head *bh)\n{\n\tclear_buffer_dirty(bh);\n\tif (bh->b_assoc_map) {\n\t\tstruct address_space *buffer_mapping = bh->b_folio->mapping;\n\n\t\tspin_lock(&buffer_mapping->private_lock);\n\t\tlist_del_init(&bh->b_assoc_buffers);\n\t\tbh->b_assoc_map = NULL;\n\t\tspin_unlock(&buffer_mapping->private_lock);\n\t}\n\t__brelse(bh);\n}\nEXPORT_SYMBOL(__bforget);\n\nstatic struct buffer_head *__bread_slow(struct buffer_head *bh)\n{\n\tlock_buffer(bh);\n\tif (buffer_uptodate(bh)) {\n\t\tunlock_buffer(bh);\n\t\treturn bh;\n\t} else {\n\t\tget_bh(bh);\n\t\tbh->b_end_io = end_buffer_read_sync;\n\t\tsubmit_bh(REQ_OP_READ, bh);\n\t\twait_on_buffer(bh);\n\t\tif (buffer_uptodate(bh))\n\t\t\treturn bh;\n\t}\n\tbrelse(bh);\n\treturn NULL;\n}\n\n \n\n#define BH_LRU_SIZE\t16\n\nstruct bh_lru {\n\tstruct buffer_head *bhs[BH_LRU_SIZE];\n};\n\nstatic DEFINE_PER_CPU(struct bh_lru, bh_lrus) = {{ NULL }};\n\n#ifdef CONFIG_SMP\n#define bh_lru_lock()\tlocal_irq_disable()\n#define bh_lru_unlock()\tlocal_irq_enable()\n#else\n#define bh_lru_lock()\tpreempt_disable()\n#define bh_lru_unlock()\tpreempt_enable()\n#endif\n\nstatic inline void check_irqs_on(void)\n{\n#ifdef irqs_disabled\n\tBUG_ON(irqs_disabled());\n#endif\n}\n\n \nstatic void bh_lru_install(struct buffer_head *bh)\n{\n\tstruct buffer_head *evictee = bh;\n\tstruct bh_lru *b;\n\tint i;\n\n\tcheck_irqs_on();\n\tbh_lru_lock();\n\n\t \n\tif (lru_cache_disabled() || cpu_is_isolated(smp_processor_id())) {\n\t\tbh_lru_unlock();\n\t\treturn;\n\t}\n\n\tb = this_cpu_ptr(&bh_lrus);\n\tfor (i = 0; i < BH_LRU_SIZE; i++) {\n\t\tswap(evictee, b->bhs[i]);\n\t\tif (evictee == bh) {\n\t\t\tbh_lru_unlock();\n\t\t\treturn;\n\t\t}\n\t}\n\n\tget_bh(bh);\n\tbh_lru_unlock();\n\tbrelse(evictee);\n}\n\n \nstatic struct buffer_head *\nlookup_bh_lru(struct block_device *bdev, sector_t block, unsigned size)\n{\n\tstruct buffer_head *ret = NULL;\n\tunsigned int i;\n\n\tcheck_irqs_on();\n\tbh_lru_lock();\n\tif (cpu_is_isolated(smp_processor_id())) {\n\t\tbh_lru_unlock();\n\t\treturn NULL;\n\t}\n\tfor (i = 0; i < BH_LRU_SIZE; i++) {\n\t\tstruct buffer_head *bh = __this_cpu_read(bh_lrus.bhs[i]);\n\n\t\tif (bh && bh->b_blocknr == block && bh->b_bdev == bdev &&\n\t\t    bh->b_size == size) {\n\t\t\tif (i) {\n\t\t\t\twhile (i) {\n\t\t\t\t\t__this_cpu_write(bh_lrus.bhs[i],\n\t\t\t\t\t\t__this_cpu_read(bh_lrus.bhs[i - 1]));\n\t\t\t\t\ti--;\n\t\t\t\t}\n\t\t\t\t__this_cpu_write(bh_lrus.bhs[0], bh);\n\t\t\t}\n\t\t\tget_bh(bh);\n\t\t\tret = bh;\n\t\t\tbreak;\n\t\t}\n\t}\n\tbh_lru_unlock();\n\treturn ret;\n}\n\n \nstruct buffer_head *\n__find_get_block(struct block_device *bdev, sector_t block, unsigned size)\n{\n\tstruct buffer_head *bh = lookup_bh_lru(bdev, block, size);\n\n\tif (bh == NULL) {\n\t\t \n\t\tbh = __find_get_block_slow(bdev, block);\n\t\tif (bh)\n\t\t\tbh_lru_install(bh);\n\t} else\n\t\ttouch_buffer(bh);\n\n\treturn bh;\n}\nEXPORT_SYMBOL(__find_get_block);\n\n \nstruct buffer_head *\n__getblk_gfp(struct block_device *bdev, sector_t block,\n\t     unsigned size, gfp_t gfp)\n{\n\tstruct buffer_head *bh = __find_get_block(bdev, block, size);\n\n\tmight_sleep();\n\tif (bh == NULL)\n\t\tbh = __getblk_slow(bdev, block, size, gfp);\n\treturn bh;\n}\nEXPORT_SYMBOL(__getblk_gfp);\n\n \nvoid __breadahead(struct block_device *bdev, sector_t block, unsigned size)\n{\n\tstruct buffer_head *bh = __getblk(bdev, block, size);\n\tif (likely(bh)) {\n\t\tbh_readahead(bh, REQ_RAHEAD);\n\t\tbrelse(bh);\n\t}\n}\nEXPORT_SYMBOL(__breadahead);\n\n \nstruct buffer_head *\n__bread_gfp(struct block_device *bdev, sector_t block,\n\t\t   unsigned size, gfp_t gfp)\n{\n\tstruct buffer_head *bh = __getblk_gfp(bdev, block, size, gfp);\n\n\tif (likely(bh) && !buffer_uptodate(bh))\n\t\tbh = __bread_slow(bh);\n\treturn bh;\n}\nEXPORT_SYMBOL(__bread_gfp);\n\nstatic void __invalidate_bh_lrus(struct bh_lru *b)\n{\n\tint i;\n\n\tfor (i = 0; i < BH_LRU_SIZE; i++) {\n\t\tbrelse(b->bhs[i]);\n\t\tb->bhs[i] = NULL;\n\t}\n}\n \nstatic void invalidate_bh_lru(void *arg)\n{\n\tstruct bh_lru *b = &get_cpu_var(bh_lrus);\n\n\t__invalidate_bh_lrus(b);\n\tput_cpu_var(bh_lrus);\n}\n\nbool has_bh_in_lru(int cpu, void *dummy)\n{\n\tstruct bh_lru *b = per_cpu_ptr(&bh_lrus, cpu);\n\tint i;\n\t\n\tfor (i = 0; i < BH_LRU_SIZE; i++) {\n\t\tif (b->bhs[i])\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nvoid invalidate_bh_lrus(void)\n{\n\ton_each_cpu_cond(has_bh_in_lru, invalidate_bh_lru, NULL, 1);\n}\nEXPORT_SYMBOL_GPL(invalidate_bh_lrus);\n\n \nvoid invalidate_bh_lrus_cpu(void)\n{\n\tstruct bh_lru *b;\n\n\tbh_lru_lock();\n\tb = this_cpu_ptr(&bh_lrus);\n\t__invalidate_bh_lrus(b);\n\tbh_lru_unlock();\n}\n\nvoid folio_set_bh(struct buffer_head *bh, struct folio *folio,\n\t\t  unsigned long offset)\n{\n\tbh->b_folio = folio;\n\tBUG_ON(offset >= folio_size(folio));\n\tif (folio_test_highmem(folio))\n\t\t \n\t\tbh->b_data = (char *)(0 + offset);\n\telse\n\t\tbh->b_data = folio_address(folio) + offset;\n}\nEXPORT_SYMBOL(folio_set_bh);\n\n \n\n \n#define BUFFER_FLAGS_DISCARD \\\n\t(1 << BH_Mapped | 1 << BH_New | 1 << BH_Req | \\\n\t 1 << BH_Delay | 1 << BH_Unwritten)\n\nstatic void discard_buffer(struct buffer_head * bh)\n{\n\tunsigned long b_state;\n\n\tlock_buffer(bh);\n\tclear_buffer_dirty(bh);\n\tbh->b_bdev = NULL;\n\tb_state = READ_ONCE(bh->b_state);\n\tdo {\n\t} while (!try_cmpxchg(&bh->b_state, &b_state,\n\t\t\t      b_state & ~BUFFER_FLAGS_DISCARD));\n\tunlock_buffer(bh);\n}\n\n \nvoid block_invalidate_folio(struct folio *folio, size_t offset, size_t length)\n{\n\tstruct buffer_head *head, *bh, *next;\n\tsize_t curr_off = 0;\n\tsize_t stop = length + offset;\n\n\tBUG_ON(!folio_test_locked(folio));\n\n\t \n\tBUG_ON(stop > folio_size(folio) || stop < length);\n\n\thead = folio_buffers(folio);\n\tif (!head)\n\t\treturn;\n\n\tbh = head;\n\tdo {\n\t\tsize_t next_off = curr_off + bh->b_size;\n\t\tnext = bh->b_this_page;\n\n\t\t \n\t\tif (next_off > stop)\n\t\t\tgoto out;\n\n\t\t \n\t\tif (offset <= curr_off)\n\t\t\tdiscard_buffer(bh);\n\t\tcurr_off = next_off;\n\t\tbh = next;\n\t} while (bh != head);\n\n\t \n\tif (length == folio_size(folio))\n\t\tfilemap_release_folio(folio, 0);\nout:\n\treturn;\n}\nEXPORT_SYMBOL(block_invalidate_folio);\n\n \nvoid folio_create_empty_buffers(struct folio *folio, unsigned long blocksize,\n\t\t\t\tunsigned long b_state)\n{\n\tstruct buffer_head *bh, *head, *tail;\n\n\thead = folio_alloc_buffers(folio, blocksize, true);\n\tbh = head;\n\tdo {\n\t\tbh->b_state |= b_state;\n\t\ttail = bh;\n\t\tbh = bh->b_this_page;\n\t} while (bh);\n\ttail->b_this_page = head;\n\n\tspin_lock(&folio->mapping->private_lock);\n\tif (folio_test_uptodate(folio) || folio_test_dirty(folio)) {\n\t\tbh = head;\n\t\tdo {\n\t\t\tif (folio_test_dirty(folio))\n\t\t\t\tset_buffer_dirty(bh);\n\t\t\tif (folio_test_uptodate(folio))\n\t\t\t\tset_buffer_uptodate(bh);\n\t\t\tbh = bh->b_this_page;\n\t\t} while (bh != head);\n\t}\n\tfolio_attach_private(folio, head);\n\tspin_unlock(&folio->mapping->private_lock);\n}\nEXPORT_SYMBOL(folio_create_empty_buffers);\n\nvoid create_empty_buffers(struct page *page,\n\t\t\tunsigned long blocksize, unsigned long b_state)\n{\n\tfolio_create_empty_buffers(page_folio(page), blocksize, b_state);\n}\nEXPORT_SYMBOL(create_empty_buffers);\n\n \nvoid clean_bdev_aliases(struct block_device *bdev, sector_t block, sector_t len)\n{\n\tstruct inode *bd_inode = bdev->bd_inode;\n\tstruct address_space *bd_mapping = bd_inode->i_mapping;\n\tstruct folio_batch fbatch;\n\tpgoff_t index = block >> (PAGE_SHIFT - bd_inode->i_blkbits);\n\tpgoff_t end;\n\tint i, count;\n\tstruct buffer_head *bh;\n\tstruct buffer_head *head;\n\n\tend = (block + len - 1) >> (PAGE_SHIFT - bd_inode->i_blkbits);\n\tfolio_batch_init(&fbatch);\n\twhile (filemap_get_folios(bd_mapping, &index, end, &fbatch)) {\n\t\tcount = folio_batch_count(&fbatch);\n\t\tfor (i = 0; i < count; i++) {\n\t\t\tstruct folio *folio = fbatch.folios[i];\n\n\t\t\tif (!folio_buffers(folio))\n\t\t\t\tcontinue;\n\t\t\t \n\t\t\tfolio_lock(folio);\n\t\t\t \n\t\t\thead = folio_buffers(folio);\n\t\t\tif (!head)\n\t\t\t\tgoto unlock_page;\n\t\t\tbh = head;\n\t\t\tdo {\n\t\t\t\tif (!buffer_mapped(bh) || (bh->b_blocknr < block))\n\t\t\t\t\tgoto next;\n\t\t\t\tif (bh->b_blocknr >= block + len)\n\t\t\t\t\tbreak;\n\t\t\t\tclear_buffer_dirty(bh);\n\t\t\t\twait_on_buffer(bh);\n\t\t\t\tclear_buffer_req(bh);\nnext:\n\t\t\t\tbh = bh->b_this_page;\n\t\t\t} while (bh != head);\nunlock_page:\n\t\t\tfolio_unlock(folio);\n\t\t}\n\t\tfolio_batch_release(&fbatch);\n\t\tcond_resched();\n\t\t \n\t\tif (index > end || !index)\n\t\t\tbreak;\n\t}\n}\nEXPORT_SYMBOL(clean_bdev_aliases);\n\n \nstatic inline int block_size_bits(unsigned int blocksize)\n{\n\treturn ilog2(blocksize);\n}\n\nstatic struct buffer_head *folio_create_buffers(struct folio *folio,\n\t\t\t\t\t\tstruct inode *inode,\n\t\t\t\t\t\tunsigned int b_state)\n{\n\tBUG_ON(!folio_test_locked(folio));\n\n\tif (!folio_buffers(folio))\n\t\tfolio_create_empty_buffers(folio,\n\t\t\t\t\t   1 << READ_ONCE(inode->i_blkbits),\n\t\t\t\t\t   b_state);\n\treturn folio_buffers(folio);\n}\n\n \n\n \nint __block_write_full_folio(struct inode *inode, struct folio *folio,\n\t\t\tget_block_t *get_block, struct writeback_control *wbc,\n\t\t\tbh_end_io_t *handler)\n{\n\tint err;\n\tsector_t block;\n\tsector_t last_block;\n\tstruct buffer_head *bh, *head;\n\tunsigned int blocksize, bbits;\n\tint nr_underway = 0;\n\tblk_opf_t write_flags = wbc_to_write_flags(wbc);\n\n\thead = folio_create_buffers(folio, inode,\n\t\t\t\t    (1 << BH_Dirty) | (1 << BH_Uptodate));\n\n\t \n\n\tbh = head;\n\tblocksize = bh->b_size;\n\tbbits = block_size_bits(blocksize);\n\n\tblock = (sector_t)folio->index << (PAGE_SHIFT - bbits);\n\tlast_block = (i_size_read(inode) - 1) >> bbits;\n\n\t \n\tdo {\n\t\tif (block > last_block) {\n\t\t\t \n\t\t\t \n\t\t\tclear_buffer_dirty(bh);\n\t\t\tset_buffer_uptodate(bh);\n\t\t} else if ((!buffer_mapped(bh) || buffer_delay(bh)) &&\n\t\t\t   buffer_dirty(bh)) {\n\t\t\tWARN_ON(bh->b_size != blocksize);\n\t\t\terr = get_block(inode, block, bh, 1);\n\t\t\tif (err)\n\t\t\t\tgoto recover;\n\t\t\tclear_buffer_delay(bh);\n\t\t\tif (buffer_new(bh)) {\n\t\t\t\t \n\t\t\t\tclear_buffer_new(bh);\n\t\t\t\tclean_bdev_bh_alias(bh);\n\t\t\t}\n\t\t}\n\t\tbh = bh->b_this_page;\n\t\tblock++;\n\t} while (bh != head);\n\n\tdo {\n\t\tif (!buffer_mapped(bh))\n\t\t\tcontinue;\n\t\t \n\t\tif (wbc->sync_mode != WB_SYNC_NONE) {\n\t\t\tlock_buffer(bh);\n\t\t} else if (!trylock_buffer(bh)) {\n\t\t\tfolio_redirty_for_writepage(wbc, folio);\n\t\t\tcontinue;\n\t\t}\n\t\tif (test_clear_buffer_dirty(bh)) {\n\t\t\tmark_buffer_async_write_endio(bh, handler);\n\t\t} else {\n\t\t\tunlock_buffer(bh);\n\t\t}\n\t} while ((bh = bh->b_this_page) != head);\n\n\t \n\tBUG_ON(folio_test_writeback(folio));\n\tfolio_start_writeback(folio);\n\n\tdo {\n\t\tstruct buffer_head *next = bh->b_this_page;\n\t\tif (buffer_async_write(bh)) {\n\t\t\tsubmit_bh_wbc(REQ_OP_WRITE | write_flags, bh, wbc);\n\t\t\tnr_underway++;\n\t\t}\n\t\tbh = next;\n\t} while (bh != head);\n\tfolio_unlock(folio);\n\n\terr = 0;\ndone:\n\tif (nr_underway == 0) {\n\t\t \n\t\tfolio_end_writeback(folio);\n\n\t\t \n\t}\n\treturn err;\n\nrecover:\n\t \n\tbh = head;\n\t \n\tdo {\n\t\tif (buffer_mapped(bh) && buffer_dirty(bh) &&\n\t\t    !buffer_delay(bh)) {\n\t\t\tlock_buffer(bh);\n\t\t\tmark_buffer_async_write_endio(bh, handler);\n\t\t} else {\n\t\t\t \n\t\t\tclear_buffer_dirty(bh);\n\t\t}\n\t} while ((bh = bh->b_this_page) != head);\n\tfolio_set_error(folio);\n\tBUG_ON(folio_test_writeback(folio));\n\tmapping_set_error(folio->mapping, err);\n\tfolio_start_writeback(folio);\n\tdo {\n\t\tstruct buffer_head *next = bh->b_this_page;\n\t\tif (buffer_async_write(bh)) {\n\t\t\tclear_buffer_dirty(bh);\n\t\t\tsubmit_bh_wbc(REQ_OP_WRITE | write_flags, bh, wbc);\n\t\t\tnr_underway++;\n\t\t}\n\t\tbh = next;\n\t} while (bh != head);\n\tfolio_unlock(folio);\n\tgoto done;\n}\nEXPORT_SYMBOL(__block_write_full_folio);\n\n \nvoid folio_zero_new_buffers(struct folio *folio, size_t from, size_t to)\n{\n\tsize_t block_start, block_end;\n\tstruct buffer_head *head, *bh;\n\n\tBUG_ON(!folio_test_locked(folio));\n\thead = folio_buffers(folio);\n\tif (!head)\n\t\treturn;\n\n\tbh = head;\n\tblock_start = 0;\n\tdo {\n\t\tblock_end = block_start + bh->b_size;\n\n\t\tif (buffer_new(bh)) {\n\t\t\tif (block_end > from && block_start < to) {\n\t\t\t\tif (!folio_test_uptodate(folio)) {\n\t\t\t\t\tsize_t start, xend;\n\n\t\t\t\t\tstart = max(from, block_start);\n\t\t\t\t\txend = min(to, block_end);\n\n\t\t\t\t\tfolio_zero_segment(folio, start, xend);\n\t\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t\t}\n\n\t\t\t\tclear_buffer_new(bh);\n\t\t\t\tmark_buffer_dirty(bh);\n\t\t\t}\n\t\t}\n\n\t\tblock_start = block_end;\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n}\nEXPORT_SYMBOL(folio_zero_new_buffers);\n\nstatic int\niomap_to_bh(struct inode *inode, sector_t block, struct buffer_head *bh,\n\t\tconst struct iomap *iomap)\n{\n\tloff_t offset = block << inode->i_blkbits;\n\n\tbh->b_bdev = iomap->bdev;\n\n\t \n\tif (offset >= iomap->offset + iomap->length)\n\t\treturn -EIO;\n\n\tswitch (iomap->type) {\n\tcase IOMAP_HOLE:\n\t\t \n\t\tif (!buffer_uptodate(bh) ||\n\t\t    (offset >= i_size_read(inode)))\n\t\t\tset_buffer_new(bh);\n\t\treturn 0;\n\tcase IOMAP_DELALLOC:\n\t\tif (!buffer_uptodate(bh) ||\n\t\t    (offset >= i_size_read(inode)))\n\t\t\tset_buffer_new(bh);\n\t\tset_buffer_uptodate(bh);\n\t\tset_buffer_mapped(bh);\n\t\tset_buffer_delay(bh);\n\t\treturn 0;\n\tcase IOMAP_UNWRITTEN:\n\t\t \n\t\tset_buffer_new(bh);\n\t\tset_buffer_unwritten(bh);\n\t\tfallthrough;\n\tcase IOMAP_MAPPED:\n\t\tif ((iomap->flags & IOMAP_F_NEW) ||\n\t\t    offset >= i_size_read(inode)) {\n\t\t\t \n\t\t\tif (S_ISBLK(inode->i_mode))\n\t\t\t\treturn -EIO;\n\t\t\tset_buffer_new(bh);\n\t\t}\n\t\tbh->b_blocknr = (iomap->addr + offset - iomap->offset) >>\n\t\t\t\tinode->i_blkbits;\n\t\tset_buffer_mapped(bh);\n\t\treturn 0;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\treturn -EIO;\n\t}\n}\n\nint __block_write_begin_int(struct folio *folio, loff_t pos, unsigned len,\n\t\tget_block_t *get_block, const struct iomap *iomap)\n{\n\tunsigned from = pos & (PAGE_SIZE - 1);\n\tunsigned to = from + len;\n\tstruct inode *inode = folio->mapping->host;\n\tunsigned block_start, block_end;\n\tsector_t block;\n\tint err = 0;\n\tunsigned blocksize, bbits;\n\tstruct buffer_head *bh, *head, *wait[2], **wait_bh=wait;\n\n\tBUG_ON(!folio_test_locked(folio));\n\tBUG_ON(from > PAGE_SIZE);\n\tBUG_ON(to > PAGE_SIZE);\n\tBUG_ON(from > to);\n\n\thead = folio_create_buffers(folio, inode, 0);\n\tblocksize = head->b_size;\n\tbbits = block_size_bits(blocksize);\n\n\tblock = (sector_t)folio->index << (PAGE_SHIFT - bbits);\n\n\tfor(bh = head, block_start = 0; bh != head || !block_start;\n\t    block++, block_start=block_end, bh = bh->b_this_page) {\n\t\tblock_end = block_start + blocksize;\n\t\tif (block_end <= from || block_start >= to) {\n\t\t\tif (folio_test_uptodate(folio)) {\n\t\t\t\tif (!buffer_uptodate(bh))\n\t\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\tif (buffer_new(bh))\n\t\t\tclear_buffer_new(bh);\n\t\tif (!buffer_mapped(bh)) {\n\t\t\tWARN_ON(bh->b_size != blocksize);\n\t\t\tif (get_block)\n\t\t\t\terr = get_block(inode, block, bh, 1);\n\t\t\telse\n\t\t\t\terr = iomap_to_bh(inode, block, bh, iomap);\n\t\t\tif (err)\n\t\t\t\tbreak;\n\n\t\t\tif (buffer_new(bh)) {\n\t\t\t\tclean_bdev_bh_alias(bh);\n\t\t\t\tif (folio_test_uptodate(folio)) {\n\t\t\t\t\tclear_buffer_new(bh);\n\t\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t\t\tmark_buffer_dirty(bh);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (block_end > to || block_start < from)\n\t\t\t\t\tfolio_zero_segments(folio,\n\t\t\t\t\t\tto, block_end,\n\t\t\t\t\t\tblock_start, from);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t}\n\t\tif (folio_test_uptodate(folio)) {\n\t\t\tif (!buffer_uptodate(bh))\n\t\t\t\tset_buffer_uptodate(bh);\n\t\t\tcontinue; \n\t\t}\n\t\tif (!buffer_uptodate(bh) && !buffer_delay(bh) &&\n\t\t    !buffer_unwritten(bh) &&\n\t\t     (block_start < from || block_end > to)) {\n\t\t\tbh_read_nowait(bh, 0);\n\t\t\t*wait_bh++=bh;\n\t\t}\n\t}\n\t \n\twhile(wait_bh > wait) {\n\t\twait_on_buffer(*--wait_bh);\n\t\tif (!buffer_uptodate(*wait_bh))\n\t\t\terr = -EIO;\n\t}\n\tif (unlikely(err))\n\t\tfolio_zero_new_buffers(folio, from, to);\n\treturn err;\n}\n\nint __block_write_begin(struct page *page, loff_t pos, unsigned len,\n\t\tget_block_t *get_block)\n{\n\treturn __block_write_begin_int(page_folio(page), pos, len, get_block,\n\t\t\t\t       NULL);\n}\nEXPORT_SYMBOL(__block_write_begin);\n\nstatic void __block_commit_write(struct folio *folio, size_t from, size_t to)\n{\n\tsize_t block_start, block_end;\n\tbool partial = false;\n\tunsigned blocksize;\n\tstruct buffer_head *bh, *head;\n\n\tbh = head = folio_buffers(folio);\n\tblocksize = bh->b_size;\n\n\tblock_start = 0;\n\tdo {\n\t\tblock_end = block_start + blocksize;\n\t\tif (block_end <= from || block_start >= to) {\n\t\t\tif (!buffer_uptodate(bh))\n\t\t\t\tpartial = true;\n\t\t} else {\n\t\t\tset_buffer_uptodate(bh);\n\t\t\tmark_buffer_dirty(bh);\n\t\t}\n\t\tif (buffer_new(bh))\n\t\t\tclear_buffer_new(bh);\n\n\t\tblock_start = block_end;\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\n\t \n\tif (!partial)\n\t\tfolio_mark_uptodate(folio);\n}\n\n \nint block_write_begin(struct address_space *mapping, loff_t pos, unsigned len,\n\t\tstruct page **pagep, get_block_t *get_block)\n{\n\tpgoff_t index = pos >> PAGE_SHIFT;\n\tstruct page *page;\n\tint status;\n\n\tpage = grab_cache_page_write_begin(mapping, index);\n\tif (!page)\n\t\treturn -ENOMEM;\n\n\tstatus = __block_write_begin(page, pos, len, get_block);\n\tif (unlikely(status)) {\n\t\tunlock_page(page);\n\t\tput_page(page);\n\t\tpage = NULL;\n\t}\n\n\t*pagep = page;\n\treturn status;\n}\nEXPORT_SYMBOL(block_write_begin);\n\nint block_write_end(struct file *file, struct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned copied,\n\t\t\tstruct page *page, void *fsdata)\n{\n\tstruct folio *folio = page_folio(page);\n\tsize_t start = pos - folio_pos(folio);\n\n\tif (unlikely(copied < len)) {\n\t\t \n\t\tif (!folio_test_uptodate(folio))\n\t\t\tcopied = 0;\n\n\t\tfolio_zero_new_buffers(folio, start+copied, start+len);\n\t}\n\tflush_dcache_folio(folio);\n\n\t \n\t__block_commit_write(folio, start, start + copied);\n\n\treturn copied;\n}\nEXPORT_SYMBOL(block_write_end);\n\nint generic_write_end(struct file *file, struct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned copied,\n\t\t\tstruct page *page, void *fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tloff_t old_size = inode->i_size;\n\tbool i_size_changed = false;\n\n\tcopied = block_write_end(file, mapping, pos, len, copied, page, fsdata);\n\n\t \n\tif (pos + copied > inode->i_size) {\n\t\ti_size_write(inode, pos + copied);\n\t\ti_size_changed = true;\n\t}\n\n\tunlock_page(page);\n\tput_page(page);\n\n\tif (old_size < pos)\n\t\tpagecache_isize_extended(inode, old_size, pos);\n\t \n\tif (i_size_changed)\n\t\tmark_inode_dirty(inode);\n\treturn copied;\n}\nEXPORT_SYMBOL(generic_write_end);\n\n \nbool block_is_partially_uptodate(struct folio *folio, size_t from, size_t count)\n{\n\tunsigned block_start, block_end, blocksize;\n\tunsigned to;\n\tstruct buffer_head *bh, *head;\n\tbool ret = true;\n\n\thead = folio_buffers(folio);\n\tif (!head)\n\t\treturn false;\n\tblocksize = head->b_size;\n\tto = min_t(unsigned, folio_size(folio) - from, count);\n\tto = from + to;\n\tif (from < blocksize && to > folio_size(folio) - blocksize)\n\t\treturn false;\n\n\tbh = head;\n\tblock_start = 0;\n\tdo {\n\t\tblock_end = block_start + blocksize;\n\t\tif (block_end > from && block_start < to) {\n\t\t\tif (!buffer_uptodate(bh)) {\n\t\t\t\tret = false;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (block_end >= to)\n\t\t\t\tbreak;\n\t\t}\n\t\tblock_start = block_end;\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\n\treturn ret;\n}\nEXPORT_SYMBOL(block_is_partially_uptodate);\n\n \nint block_read_full_folio(struct folio *folio, get_block_t *get_block)\n{\n\tstruct inode *inode = folio->mapping->host;\n\tsector_t iblock, lblock;\n\tstruct buffer_head *bh, *head, *arr[MAX_BUF_PER_PAGE];\n\tunsigned int blocksize, bbits;\n\tint nr, i;\n\tint fully_mapped = 1;\n\tbool page_error = false;\n\tloff_t limit = i_size_read(inode);\n\n\t \n\tif (IS_ENABLED(CONFIG_FS_VERITY) && IS_VERITY(inode))\n\t\tlimit = inode->i_sb->s_maxbytes;\n\n\tVM_BUG_ON_FOLIO(folio_test_large(folio), folio);\n\n\thead = folio_create_buffers(folio, inode, 0);\n\tblocksize = head->b_size;\n\tbbits = block_size_bits(blocksize);\n\n\tiblock = (sector_t)folio->index << (PAGE_SHIFT - bbits);\n\tlblock = (limit+blocksize-1) >> bbits;\n\tbh = head;\n\tnr = 0;\n\ti = 0;\n\n\tdo {\n\t\tif (buffer_uptodate(bh))\n\t\t\tcontinue;\n\n\t\tif (!buffer_mapped(bh)) {\n\t\t\tint err = 0;\n\n\t\t\tfully_mapped = 0;\n\t\t\tif (iblock < lblock) {\n\t\t\t\tWARN_ON(bh->b_size != blocksize);\n\t\t\t\terr = get_block(inode, iblock, bh, 0);\n\t\t\t\tif (err) {\n\t\t\t\t\tfolio_set_error(folio);\n\t\t\t\t\tpage_error = true;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!buffer_mapped(bh)) {\n\t\t\t\tfolio_zero_range(folio, i * blocksize,\n\t\t\t\t\t\tblocksize);\n\t\t\t\tif (!err)\n\t\t\t\t\tset_buffer_uptodate(bh);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t \n\t\t\tif (buffer_uptodate(bh))\n\t\t\t\tcontinue;\n\t\t}\n\t\tarr[nr++] = bh;\n\t} while (i++, iblock++, (bh = bh->b_this_page) != head);\n\n\tif (fully_mapped)\n\t\tfolio_set_mappedtodisk(folio);\n\n\tif (!nr) {\n\t\t \n\t\tif (!page_error)\n\t\t\tfolio_mark_uptodate(folio);\n\t\tfolio_unlock(folio);\n\t\treturn 0;\n\t}\n\n\t \n\tfor (i = 0; i < nr; i++) {\n\t\tbh = arr[i];\n\t\tlock_buffer(bh);\n\t\tmark_buffer_async_read(bh);\n\t}\n\n\t \n\tfor (i = 0; i < nr; i++) {\n\t\tbh = arr[i];\n\t\tif (buffer_uptodate(bh))\n\t\t\tend_buffer_async_read(bh, 1);\n\t\telse\n\t\t\tsubmit_bh(REQ_OP_READ, bh);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(block_read_full_folio);\n\n \nint generic_cont_expand_simple(struct inode *inode, loff_t size)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tconst struct address_space_operations *aops = mapping->a_ops;\n\tstruct page *page;\n\tvoid *fsdata = NULL;\n\tint err;\n\n\terr = inode_newsize_ok(inode, size);\n\tif (err)\n\t\tgoto out;\n\n\terr = aops->write_begin(NULL, mapping, size, 0, &page, &fsdata);\n\tif (err)\n\t\tgoto out;\n\n\terr = aops->write_end(NULL, mapping, size, 0, 0, page, fsdata);\n\tBUG_ON(err > 0);\n\nout:\n\treturn err;\n}\nEXPORT_SYMBOL(generic_cont_expand_simple);\n\nstatic int cont_expand_zero(struct file *file, struct address_space *mapping,\n\t\t\t    loff_t pos, loff_t *bytes)\n{\n\tstruct inode *inode = mapping->host;\n\tconst struct address_space_operations *aops = mapping->a_ops;\n\tunsigned int blocksize = i_blocksize(inode);\n\tstruct page *page;\n\tvoid *fsdata = NULL;\n\tpgoff_t index, curidx;\n\tloff_t curpos;\n\tunsigned zerofrom, offset, len;\n\tint err = 0;\n\n\tindex = pos >> PAGE_SHIFT;\n\toffset = pos & ~PAGE_MASK;\n\n\twhile (index > (curidx = (curpos = *bytes)>>PAGE_SHIFT)) {\n\t\tzerofrom = curpos & ~PAGE_MASK;\n\t\tif (zerofrom & (blocksize-1)) {\n\t\t\t*bytes |= (blocksize-1);\n\t\t\t(*bytes)++;\n\t\t}\n\t\tlen = PAGE_SIZE - zerofrom;\n\n\t\terr = aops->write_begin(file, mapping, curpos, len,\n\t\t\t\t\t    &page, &fsdata);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tzero_user(page, zerofrom, len);\n\t\terr = aops->write_end(file, mapping, curpos, len, len,\n\t\t\t\t\t\tpage, fsdata);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t\tBUG_ON(err != len);\n\t\terr = 0;\n\n\t\tbalance_dirty_pages_ratelimited(mapping);\n\n\t\tif (fatal_signal_pending(current)) {\n\t\t\terr = -EINTR;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tif (index == curidx) {\n\t\tzerofrom = curpos & ~PAGE_MASK;\n\t\t \n\t\tif (offset <= zerofrom) {\n\t\t\tgoto out;\n\t\t}\n\t\tif (zerofrom & (blocksize-1)) {\n\t\t\t*bytes |= (blocksize-1);\n\t\t\t(*bytes)++;\n\t\t}\n\t\tlen = offset - zerofrom;\n\n\t\terr = aops->write_begin(file, mapping, curpos, len,\n\t\t\t\t\t    &page, &fsdata);\n\t\tif (err)\n\t\t\tgoto out;\n\t\tzero_user(page, zerofrom, len);\n\t\terr = aops->write_end(file, mapping, curpos, len, len,\n\t\t\t\t\t\tpage, fsdata);\n\t\tif (err < 0)\n\t\t\tgoto out;\n\t\tBUG_ON(err != len);\n\t\terr = 0;\n\t}\nout:\n\treturn err;\n}\n\n \nint cont_write_begin(struct file *file, struct address_space *mapping,\n\t\t\tloff_t pos, unsigned len,\n\t\t\tstruct page **pagep, void **fsdata,\n\t\t\tget_block_t *get_block, loff_t *bytes)\n{\n\tstruct inode *inode = mapping->host;\n\tunsigned int blocksize = i_blocksize(inode);\n\tunsigned int zerofrom;\n\tint err;\n\n\terr = cont_expand_zero(file, mapping, pos, bytes);\n\tif (err)\n\t\treturn err;\n\n\tzerofrom = *bytes & ~PAGE_MASK;\n\tif (pos+len > *bytes && zerofrom & (blocksize-1)) {\n\t\t*bytes |= (blocksize-1);\n\t\t(*bytes)++;\n\t}\n\n\treturn block_write_begin(mapping, pos, len, pagep, get_block);\n}\nEXPORT_SYMBOL(cont_write_begin);\n\nvoid block_commit_write(struct page *page, unsigned from, unsigned to)\n{\n\tstruct folio *folio = page_folio(page);\n\t__block_commit_write(folio, from, to);\n}\nEXPORT_SYMBOL(block_commit_write);\n\n \nint block_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf,\n\t\t\t get_block_t get_block)\n{\n\tstruct folio *folio = page_folio(vmf->page);\n\tstruct inode *inode = file_inode(vma->vm_file);\n\tunsigned long end;\n\tloff_t size;\n\tint ret;\n\n\tfolio_lock(folio);\n\tsize = i_size_read(inode);\n\tif ((folio->mapping != inode->i_mapping) ||\n\t    (folio_pos(folio) >= size)) {\n\t\t \n\t\tret = -EFAULT;\n\t\tgoto out_unlock;\n\t}\n\n\tend = folio_size(folio);\n\t \n\tif (folio_pos(folio) + end > size)\n\t\tend = size - folio_pos(folio);\n\n\tret = __block_write_begin_int(folio, 0, end, get_block, NULL);\n\tif (unlikely(ret))\n\t\tgoto out_unlock;\n\n\t__block_commit_write(folio, 0, end);\n\n\tfolio_mark_dirty(folio);\n\tfolio_wait_stable(folio);\n\treturn 0;\nout_unlock:\n\tfolio_unlock(folio);\n\treturn ret;\n}\nEXPORT_SYMBOL(block_page_mkwrite);\n\nint block_truncate_page(struct address_space *mapping,\n\t\t\tloff_t from, get_block_t *get_block)\n{\n\tpgoff_t index = from >> PAGE_SHIFT;\n\tunsigned blocksize;\n\tsector_t iblock;\n\tsize_t offset, length, pos;\n\tstruct inode *inode = mapping->host;\n\tstruct folio *folio;\n\tstruct buffer_head *bh;\n\tint err = 0;\n\n\tblocksize = i_blocksize(inode);\n\tlength = from & (blocksize - 1);\n\n\t \n\tif (!length)\n\t\treturn 0;\n\n\tlength = blocksize - length;\n\tiblock = (sector_t)index << (PAGE_SHIFT - inode->i_blkbits);\n\t\n\tfolio = filemap_grab_folio(mapping, index);\n\tif (IS_ERR(folio))\n\t\treturn PTR_ERR(folio);\n\n\tbh = folio_buffers(folio);\n\tif (!bh) {\n\t\tfolio_create_empty_buffers(folio, blocksize, 0);\n\t\tbh = folio_buffers(folio);\n\t}\n\n\t \n\toffset = offset_in_folio(folio, from);\n\tpos = blocksize;\n\twhile (offset >= pos) {\n\t\tbh = bh->b_this_page;\n\t\tiblock++;\n\t\tpos += blocksize;\n\t}\n\n\tif (!buffer_mapped(bh)) {\n\t\tWARN_ON(bh->b_size != blocksize);\n\t\terr = get_block(inode, iblock, bh, 0);\n\t\tif (err)\n\t\t\tgoto unlock;\n\t\t \n\t\tif (!buffer_mapped(bh))\n\t\t\tgoto unlock;\n\t}\n\n\t \n\tif (folio_test_uptodate(folio))\n\t\tset_buffer_uptodate(bh);\n\n\tif (!buffer_uptodate(bh) && !buffer_delay(bh) && !buffer_unwritten(bh)) {\n\t\terr = bh_read(bh, 0);\n\t\t \n\t\tif (err < 0)\n\t\t\tgoto unlock;\n\t}\n\n\tfolio_zero_range(folio, offset, length);\n\tmark_buffer_dirty(bh);\n\nunlock:\n\tfolio_unlock(folio);\n\tfolio_put(folio);\n\n\treturn err;\n}\nEXPORT_SYMBOL(block_truncate_page);\n\n \nint block_write_full_page(struct page *page, get_block_t *get_block,\n\t\t\tstruct writeback_control *wbc)\n{\n\tstruct folio *folio = page_folio(page);\n\tstruct inode * const inode = folio->mapping->host;\n\tloff_t i_size = i_size_read(inode);\n\n\t \n\tif (folio_pos(folio) + folio_size(folio) <= i_size)\n\t\treturn __block_write_full_folio(inode, folio, get_block, wbc,\n\t\t\t\t\t       end_buffer_async_write);\n\n\t \n\tif (folio_pos(folio) >= i_size) {\n\t\tfolio_unlock(folio);\n\t\treturn 0;  \n\t}\n\n\t \n\tfolio_zero_segment(folio, offset_in_folio(folio, i_size),\n\t\t\tfolio_size(folio));\n\treturn __block_write_full_folio(inode, folio, get_block, wbc,\n\t\t\tend_buffer_async_write);\n}\nEXPORT_SYMBOL(block_write_full_page);\n\nsector_t generic_block_bmap(struct address_space *mapping, sector_t block,\n\t\t\t    get_block_t *get_block)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct buffer_head tmp = {\n\t\t.b_size = i_blocksize(inode),\n\t};\n\n\tget_block(inode, block, &tmp, 0);\n\treturn tmp.b_blocknr;\n}\nEXPORT_SYMBOL(generic_block_bmap);\n\nstatic void end_bio_bh_io_sync(struct bio *bio)\n{\n\tstruct buffer_head *bh = bio->bi_private;\n\n\tif (unlikely(bio_flagged(bio, BIO_QUIET)))\n\t\tset_bit(BH_Quiet, &bh->b_state);\n\n\tbh->b_end_io(bh, !bio->bi_status);\n\tbio_put(bio);\n}\n\nstatic void submit_bh_wbc(blk_opf_t opf, struct buffer_head *bh,\n\t\t\t  struct writeback_control *wbc)\n{\n\tconst enum req_op op = opf & REQ_OP_MASK;\n\tstruct bio *bio;\n\n\tBUG_ON(!buffer_locked(bh));\n\tBUG_ON(!buffer_mapped(bh));\n\tBUG_ON(!bh->b_end_io);\n\tBUG_ON(buffer_delay(bh));\n\tBUG_ON(buffer_unwritten(bh));\n\n\t \n\tif (test_set_buffer_req(bh) && (op == REQ_OP_WRITE))\n\t\tclear_buffer_write_io_error(bh);\n\n\tif (buffer_meta(bh))\n\t\topf |= REQ_META;\n\tif (buffer_prio(bh))\n\t\topf |= REQ_PRIO;\n\n\tbio = bio_alloc(bh->b_bdev, 1, opf, GFP_NOIO);\n\n\tfscrypt_set_bio_crypt_ctx_bh(bio, bh, GFP_NOIO);\n\n\tbio->bi_iter.bi_sector = bh->b_blocknr * (bh->b_size >> 9);\n\n\t__bio_add_page(bio, bh->b_page, bh->b_size, bh_offset(bh));\n\n\tbio->bi_end_io = end_bio_bh_io_sync;\n\tbio->bi_private = bh;\n\n\t \n\tguard_bio_eod(bio);\n\n\tif (wbc) {\n\t\twbc_init_bio(wbc, bio);\n\t\twbc_account_cgroup_owner(wbc, bh->b_page, bh->b_size);\n\t}\n\n\tsubmit_bio(bio);\n}\n\nvoid submit_bh(blk_opf_t opf, struct buffer_head *bh)\n{\n\tsubmit_bh_wbc(opf, bh, NULL);\n}\nEXPORT_SYMBOL(submit_bh);\n\nvoid write_dirty_buffer(struct buffer_head *bh, blk_opf_t op_flags)\n{\n\tlock_buffer(bh);\n\tif (!test_clear_buffer_dirty(bh)) {\n\t\tunlock_buffer(bh);\n\t\treturn;\n\t}\n\tbh->b_end_io = end_buffer_write_sync;\n\tget_bh(bh);\n\tsubmit_bh(REQ_OP_WRITE | op_flags, bh);\n}\nEXPORT_SYMBOL(write_dirty_buffer);\n\n \nint __sync_dirty_buffer(struct buffer_head *bh, blk_opf_t op_flags)\n{\n\tWARN_ON(atomic_read(&bh->b_count) < 1);\n\tlock_buffer(bh);\n\tif (test_clear_buffer_dirty(bh)) {\n\t\t \n\t\tif (!buffer_mapped(bh)) {\n\t\t\tunlock_buffer(bh);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tget_bh(bh);\n\t\tbh->b_end_io = end_buffer_write_sync;\n\t\tsubmit_bh(REQ_OP_WRITE | op_flags, bh);\n\t\twait_on_buffer(bh);\n\t\tif (!buffer_uptodate(bh))\n\t\t\treturn -EIO;\n\t} else {\n\t\tunlock_buffer(bh);\n\t}\n\treturn 0;\n}\nEXPORT_SYMBOL(__sync_dirty_buffer);\n\nint sync_dirty_buffer(struct buffer_head *bh)\n{\n\treturn __sync_dirty_buffer(bh, REQ_SYNC);\n}\nEXPORT_SYMBOL(sync_dirty_buffer);\n\n \nstatic inline int buffer_busy(struct buffer_head *bh)\n{\n\treturn atomic_read(&bh->b_count) |\n\t\t(bh->b_state & ((1 << BH_Dirty) | (1 << BH_Lock)));\n}\n\nstatic bool\ndrop_buffers(struct folio *folio, struct buffer_head **buffers_to_free)\n{\n\tstruct buffer_head *head = folio_buffers(folio);\n\tstruct buffer_head *bh;\n\n\tbh = head;\n\tdo {\n\t\tif (buffer_busy(bh))\n\t\t\tgoto failed;\n\t\tbh = bh->b_this_page;\n\t} while (bh != head);\n\n\tdo {\n\t\tstruct buffer_head *next = bh->b_this_page;\n\n\t\tif (bh->b_assoc_map)\n\t\t\t__remove_assoc_queue(bh);\n\t\tbh = next;\n\t} while (bh != head);\n\t*buffers_to_free = head;\n\tfolio_detach_private(folio);\n\treturn true;\nfailed:\n\treturn false;\n}\n\nbool try_to_free_buffers(struct folio *folio)\n{\n\tstruct address_space * const mapping = folio->mapping;\n\tstruct buffer_head *buffers_to_free = NULL;\n\tbool ret = 0;\n\n\tBUG_ON(!folio_test_locked(folio));\n\tif (folio_test_writeback(folio))\n\t\treturn false;\n\n\tif (mapping == NULL) {\t\t \n\t\tret = drop_buffers(folio, &buffers_to_free);\n\t\tgoto out;\n\t}\n\n\tspin_lock(&mapping->private_lock);\n\tret = drop_buffers(folio, &buffers_to_free);\n\n\t \n\tif (ret)\n\t\tfolio_cancel_dirty(folio);\n\tspin_unlock(&mapping->private_lock);\nout:\n\tif (buffers_to_free) {\n\t\tstruct buffer_head *bh = buffers_to_free;\n\n\t\tdo {\n\t\t\tstruct buffer_head *next = bh->b_this_page;\n\t\t\tfree_buffer_head(bh);\n\t\t\tbh = next;\n\t\t} while (bh != buffers_to_free);\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(try_to_free_buffers);\n\n \nstatic struct kmem_cache *bh_cachep __read_mostly;\n\n \nstatic unsigned long max_buffer_heads;\n\nint buffer_heads_over_limit;\n\nstruct bh_accounting {\n\tint nr;\t\t\t \n\tint ratelimit;\t\t \n};\n\nstatic DEFINE_PER_CPU(struct bh_accounting, bh_accounting) = {0, 0};\n\nstatic void recalc_bh_state(void)\n{\n\tint i;\n\tint tot = 0;\n\n\tif (__this_cpu_inc_return(bh_accounting.ratelimit) - 1 < 4096)\n\t\treturn;\n\t__this_cpu_write(bh_accounting.ratelimit, 0);\n\tfor_each_online_cpu(i)\n\t\ttot += per_cpu(bh_accounting, i).nr;\n\tbuffer_heads_over_limit = (tot > max_buffer_heads);\n}\n\nstruct buffer_head *alloc_buffer_head(gfp_t gfp_flags)\n{\n\tstruct buffer_head *ret = kmem_cache_zalloc(bh_cachep, gfp_flags);\n\tif (ret) {\n\t\tINIT_LIST_HEAD(&ret->b_assoc_buffers);\n\t\tspin_lock_init(&ret->b_uptodate_lock);\n\t\tpreempt_disable();\n\t\t__this_cpu_inc(bh_accounting.nr);\n\t\trecalc_bh_state();\n\t\tpreempt_enable();\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(alloc_buffer_head);\n\nvoid free_buffer_head(struct buffer_head *bh)\n{\n\tBUG_ON(!list_empty(&bh->b_assoc_buffers));\n\tkmem_cache_free(bh_cachep, bh);\n\tpreempt_disable();\n\t__this_cpu_dec(bh_accounting.nr);\n\trecalc_bh_state();\n\tpreempt_enable();\n}\nEXPORT_SYMBOL(free_buffer_head);\n\nstatic int buffer_exit_cpu_dead(unsigned int cpu)\n{\n\tint i;\n\tstruct bh_lru *b = &per_cpu(bh_lrus, cpu);\n\n\tfor (i = 0; i < BH_LRU_SIZE; i++) {\n\t\tbrelse(b->bhs[i]);\n\t\tb->bhs[i] = NULL;\n\t}\n\tthis_cpu_add(bh_accounting.nr, per_cpu(bh_accounting, cpu).nr);\n\tper_cpu(bh_accounting, cpu).nr = 0;\n\treturn 0;\n}\n\n \nint bh_uptodate_or_lock(struct buffer_head *bh)\n{\n\tif (!buffer_uptodate(bh)) {\n\t\tlock_buffer(bh);\n\t\tif (!buffer_uptodate(bh))\n\t\t\treturn 0;\n\t\tunlock_buffer(bh);\n\t}\n\treturn 1;\n}\nEXPORT_SYMBOL(bh_uptodate_or_lock);\n\n \nint __bh_read(struct buffer_head *bh, blk_opf_t op_flags, bool wait)\n{\n\tint ret = 0;\n\n\tBUG_ON(!buffer_locked(bh));\n\n\tget_bh(bh);\n\tbh->b_end_io = end_buffer_read_sync;\n\tsubmit_bh(REQ_OP_READ | op_flags, bh);\n\tif (wait) {\n\t\twait_on_buffer(bh);\n\t\tif (!buffer_uptodate(bh))\n\t\t\tret = -EIO;\n\t}\n\treturn ret;\n}\nEXPORT_SYMBOL(__bh_read);\n\n \nvoid __bh_read_batch(int nr, struct buffer_head *bhs[],\n\t\t     blk_opf_t op_flags, bool force_lock)\n{\n\tint i;\n\n\tfor (i = 0; i < nr; i++) {\n\t\tstruct buffer_head *bh = bhs[i];\n\n\t\tif (buffer_uptodate(bh))\n\t\t\tcontinue;\n\n\t\tif (force_lock)\n\t\t\tlock_buffer(bh);\n\t\telse\n\t\t\tif (!trylock_buffer(bh))\n\t\t\t\tcontinue;\n\n\t\tif (buffer_uptodate(bh)) {\n\t\t\tunlock_buffer(bh);\n\t\t\tcontinue;\n\t\t}\n\n\t\tbh->b_end_io = end_buffer_read_sync;\n\t\tget_bh(bh);\n\t\tsubmit_bh(REQ_OP_READ | op_flags, bh);\n\t}\n}\nEXPORT_SYMBOL(__bh_read_batch);\n\nvoid __init buffer_init(void)\n{\n\tunsigned long nrpages;\n\tint ret;\n\n\tbh_cachep = kmem_cache_create(\"buffer_head\",\n\t\t\tsizeof(struct buffer_head), 0,\n\t\t\t\t(SLAB_RECLAIM_ACCOUNT|SLAB_PANIC|\n\t\t\t\tSLAB_MEM_SPREAD),\n\t\t\t\tNULL);\n\n\t \n\tnrpages = (nr_free_buffer_pages() * 10) / 100;\n\tmax_buffer_heads = nrpages * (PAGE_SIZE / sizeof(struct buffer_head));\n\tret = cpuhp_setup_state_nocalls(CPUHP_FS_BUFF_DEAD, \"fs/buffer:dead\",\n\t\t\t\t\tNULL, buffer_exit_cpu_dead);\n\tWARN_ON(ret < 0);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}