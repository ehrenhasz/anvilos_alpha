{
  "module_name": "zdata.c",
  "hash_id": "7f4da3e3ac1efb5262b8dc9b2181a844bdf086c7dc672752443da09e9c261237",
  "original_prompt": "Ingested from linux-6.6.14/fs/erofs/zdata.c",
  "human_readable_source": "\n \n#include \"compress.h\"\n#include <linux/psi.h>\n#include <linux/cpuhotplug.h>\n#include <trace/events/erofs.h>\n\n#define Z_EROFS_PCLUSTER_MAX_PAGES\t(Z_EROFS_PCLUSTER_MAX_SIZE / PAGE_SIZE)\n#define Z_EROFS_INLINE_BVECS\t\t2\n\n \ntypedef void *z_erofs_next_pcluster_t;\n\nstruct z_erofs_bvec {\n\tstruct page *page;\n\tint offset;\n\tunsigned int end;\n};\n\n#define __Z_EROFS_BVSET(name, total) \\\nstruct name { \\\n\t  \\\n\tstruct page *nextpage; \\\n\tstruct z_erofs_bvec bvec[total]; \\\n}\n__Z_EROFS_BVSET(z_erofs_bvset,);\n__Z_EROFS_BVSET(z_erofs_bvset_inline, Z_EROFS_INLINE_BVECS);\n\n \nstruct z_erofs_pcluster {\n\tstruct erofs_workgroup obj;\n\tstruct mutex lock;\n\n\t \n\tz_erofs_next_pcluster_t next;\n\n\t \n\tunsigned int length;\n\n\t \n\tunsigned int vcnt;\n\n\t \n\tunsigned short pageofs_out;\n\n\t \n\tunsigned short pageofs_in;\n\n\tunion {\n\t\t \n\t\tstruct z_erofs_bvset_inline bvset;\n\n\t\t \n\t\tstruct rcu_head rcu;\n\t};\n\n\tunion {\n\t\t \n\t\tunsigned short pclusterpages;\n\n\t\t \n\t\tunsigned short tailpacking_size;\n\t};\n\n\t \n\tunsigned char algorithmformat;\n\n\t \n\tbool partial;\n\n\t \n\tbool multibases;\n\n\t \n\tstruct z_erofs_bvec compressed_bvecs[];\n};\n\n \n#define Z_EROFS_PCLUSTER_TAIL           ((void *) 0x700 + POISON_POINTER_DELTA)\n#define Z_EROFS_PCLUSTER_NIL            (NULL)\n\nstruct z_erofs_decompressqueue {\n\tstruct super_block *sb;\n\tatomic_t pending_bios;\n\tz_erofs_next_pcluster_t head;\n\n\tunion {\n\t\tstruct completion done;\n\t\tstruct work_struct work;\n\t\tstruct kthread_work kthread_work;\n\t} u;\n\tbool eio, sync;\n};\n\nstatic inline bool z_erofs_is_inline_pcluster(struct z_erofs_pcluster *pcl)\n{\n\treturn !pcl->obj.index;\n}\n\nstatic inline unsigned int z_erofs_pclusterpages(struct z_erofs_pcluster *pcl)\n{\n\tif (z_erofs_is_inline_pcluster(pcl))\n\t\treturn 1;\n\treturn pcl->pclusterpages;\n}\n\n \n#define Z_EROFS_PAGE_EIO\t\t\t(1 << 30)\n\nstatic inline void z_erofs_onlinepage_init(struct page *page)\n{\n\tunion {\n\t\tatomic_t o;\n\t\tunsigned long v;\n\t} u = { .o = ATOMIC_INIT(1) };\n\n\tset_page_private(page, u.v);\n\tsmp_wmb();\n\tSetPagePrivate(page);\n}\n\nstatic inline void z_erofs_onlinepage_split(struct page *page)\n{\n\tatomic_inc((atomic_t *)&page->private);\n}\n\nstatic void z_erofs_onlinepage_endio(struct page *page, int err)\n{\n\tint orig, v;\n\n\tDBG_BUGON(!PagePrivate(page));\n\n\tdo {\n\t\torig = atomic_read((atomic_t *)&page->private);\n\t\tv = (orig - 1) | (err ? Z_EROFS_PAGE_EIO : 0);\n\t} while (atomic_cmpxchg((atomic_t *)&page->private, orig, v) != orig);\n\n\tif (!(v & ~Z_EROFS_PAGE_EIO)) {\n\t\tset_page_private(page, 0);\n\t\tClearPagePrivate(page);\n\t\tif (!(v & Z_EROFS_PAGE_EIO))\n\t\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t}\n}\n\n#define Z_EROFS_ONSTACK_PAGES\t\t32\n\n \nstruct z_erofs_pcluster_slab {\n\tstruct kmem_cache *slab;\n\tunsigned int maxpages;\n\tchar name[48];\n};\n\n#define _PCLP(n) { .maxpages = n }\n\nstatic struct z_erofs_pcluster_slab pcluster_pool[] __read_mostly = {\n\t_PCLP(1), _PCLP(4), _PCLP(16), _PCLP(64), _PCLP(128),\n\t_PCLP(Z_EROFS_PCLUSTER_MAX_PAGES)\n};\n\nstruct z_erofs_bvec_iter {\n\tstruct page *bvpage;\n\tstruct z_erofs_bvset *bvset;\n\tunsigned int nr, cur;\n};\n\nstatic struct page *z_erofs_bvec_iter_end(struct z_erofs_bvec_iter *iter)\n{\n\tif (iter->bvpage)\n\t\tkunmap_local(iter->bvset);\n\treturn iter->bvpage;\n}\n\nstatic struct page *z_erofs_bvset_flip(struct z_erofs_bvec_iter *iter)\n{\n\tunsigned long base = (unsigned long)((struct z_erofs_bvset *)0)->bvec;\n\t \n\tstruct page *nextpage = iter->bvset->nextpage;\n\tstruct page *oldpage;\n\n\tDBG_BUGON(!nextpage);\n\toldpage = z_erofs_bvec_iter_end(iter);\n\titer->bvpage = nextpage;\n\titer->bvset = kmap_local_page(nextpage);\n\titer->nr = (PAGE_SIZE - base) / sizeof(struct z_erofs_bvec);\n\titer->cur = 0;\n\treturn oldpage;\n}\n\nstatic void z_erofs_bvec_iter_begin(struct z_erofs_bvec_iter *iter,\n\t\t\t\t    struct z_erofs_bvset_inline *bvset,\n\t\t\t\t    unsigned int bootstrap_nr,\n\t\t\t\t    unsigned int cur)\n{\n\t*iter = (struct z_erofs_bvec_iter) {\n\t\t.nr = bootstrap_nr,\n\t\t.bvset = (struct z_erofs_bvset *)bvset,\n\t};\n\n\twhile (cur > iter->nr) {\n\t\tcur -= iter->nr;\n\t\tz_erofs_bvset_flip(iter);\n\t}\n\titer->cur = cur;\n}\n\nstatic int z_erofs_bvec_enqueue(struct z_erofs_bvec_iter *iter,\n\t\t\t\tstruct z_erofs_bvec *bvec,\n\t\t\t\tstruct page **candidate_bvpage,\n\t\t\t\tstruct page **pagepool)\n{\n\tif (iter->cur >= iter->nr) {\n\t\tstruct page *nextpage = *candidate_bvpage;\n\n\t\tif (!nextpage) {\n\t\t\tnextpage = erofs_allocpage(pagepool, GFP_NOFS);\n\t\t\tif (!nextpage)\n\t\t\t\treturn -ENOMEM;\n\t\t\tset_page_private(nextpage, Z_EROFS_SHORTLIVED_PAGE);\n\t\t}\n\t\tDBG_BUGON(iter->bvset->nextpage);\n\t\titer->bvset->nextpage = nextpage;\n\t\tz_erofs_bvset_flip(iter);\n\n\t\titer->bvset->nextpage = NULL;\n\t\t*candidate_bvpage = NULL;\n\t}\n\titer->bvset->bvec[iter->cur++] = *bvec;\n\treturn 0;\n}\n\nstatic void z_erofs_bvec_dequeue(struct z_erofs_bvec_iter *iter,\n\t\t\t\t struct z_erofs_bvec *bvec,\n\t\t\t\t struct page **old_bvpage)\n{\n\tif (iter->cur == iter->nr)\n\t\t*old_bvpage = z_erofs_bvset_flip(iter);\n\telse\n\t\t*old_bvpage = NULL;\n\t*bvec = iter->bvset->bvec[iter->cur++];\n}\n\nstatic void z_erofs_destroy_pcluster_pool(void)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(pcluster_pool); ++i) {\n\t\tif (!pcluster_pool[i].slab)\n\t\t\tcontinue;\n\t\tkmem_cache_destroy(pcluster_pool[i].slab);\n\t\tpcluster_pool[i].slab = NULL;\n\t}\n}\n\nstatic int z_erofs_create_pcluster_pool(void)\n{\n\tstruct z_erofs_pcluster_slab *pcs;\n\tstruct z_erofs_pcluster *a;\n\tunsigned int size;\n\n\tfor (pcs = pcluster_pool;\n\t     pcs < pcluster_pool + ARRAY_SIZE(pcluster_pool); ++pcs) {\n\t\tsize = struct_size(a, compressed_bvecs, pcs->maxpages);\n\n\t\tsprintf(pcs->name, \"erofs_pcluster-%u\", pcs->maxpages);\n\t\tpcs->slab = kmem_cache_create(pcs->name, size, 0,\n\t\t\t\t\t      SLAB_RECLAIM_ACCOUNT, NULL);\n\t\tif (pcs->slab)\n\t\t\tcontinue;\n\n\t\tz_erofs_destroy_pcluster_pool();\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic struct z_erofs_pcluster *z_erofs_alloc_pcluster(unsigned int nrpages)\n{\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(pcluster_pool); ++i) {\n\t\tstruct z_erofs_pcluster_slab *pcs = pcluster_pool + i;\n\t\tstruct z_erofs_pcluster *pcl;\n\n\t\tif (nrpages > pcs->maxpages)\n\t\t\tcontinue;\n\n\t\tpcl = kmem_cache_zalloc(pcs->slab, GFP_NOFS);\n\t\tif (!pcl)\n\t\t\treturn ERR_PTR(-ENOMEM);\n\t\tpcl->pclusterpages = nrpages;\n\t\treturn pcl;\n\t}\n\treturn ERR_PTR(-EINVAL);\n}\n\nstatic void z_erofs_free_pcluster(struct z_erofs_pcluster *pcl)\n{\n\tunsigned int pclusterpages = z_erofs_pclusterpages(pcl);\n\tint i;\n\n\tfor (i = 0; i < ARRAY_SIZE(pcluster_pool); ++i) {\n\t\tstruct z_erofs_pcluster_slab *pcs = pcluster_pool + i;\n\n\t\tif (pclusterpages > pcs->maxpages)\n\t\t\tcontinue;\n\n\t\tkmem_cache_free(pcs->slab, pcl);\n\t\treturn;\n\t}\n\tDBG_BUGON(1);\n}\n\nstatic struct workqueue_struct *z_erofs_workqueue __read_mostly;\n\n#ifdef CONFIG_EROFS_FS_PCPU_KTHREAD\nstatic struct kthread_worker __rcu **z_erofs_pcpu_workers;\n\nstatic void erofs_destroy_percpu_workers(void)\n{\n\tstruct kthread_worker *worker;\n\tunsigned int cpu;\n\n\tfor_each_possible_cpu(cpu) {\n\t\tworker = rcu_dereference_protected(\n\t\t\t\t\tz_erofs_pcpu_workers[cpu], 1);\n\t\trcu_assign_pointer(z_erofs_pcpu_workers[cpu], NULL);\n\t\tif (worker)\n\t\t\tkthread_destroy_worker(worker);\n\t}\n\tkfree(z_erofs_pcpu_workers);\n}\n\nstatic struct kthread_worker *erofs_init_percpu_worker(int cpu)\n{\n\tstruct kthread_worker *worker =\n\t\tkthread_create_worker_on_cpu(cpu, 0, \"erofs_worker/%u\", cpu);\n\n\tif (IS_ERR(worker))\n\t\treturn worker;\n\tif (IS_ENABLED(CONFIG_EROFS_FS_PCPU_KTHREAD_HIPRI))\n\t\tsched_set_fifo_low(worker->task);\n\treturn worker;\n}\n\nstatic int erofs_init_percpu_workers(void)\n{\n\tstruct kthread_worker *worker;\n\tunsigned int cpu;\n\n\tz_erofs_pcpu_workers = kcalloc(num_possible_cpus(),\n\t\t\tsizeof(struct kthread_worker *), GFP_ATOMIC);\n\tif (!z_erofs_pcpu_workers)\n\t\treturn -ENOMEM;\n\n\tfor_each_online_cpu(cpu) {\t \n\t\tworker = erofs_init_percpu_worker(cpu);\n\t\tif (!IS_ERR(worker))\n\t\t\trcu_assign_pointer(z_erofs_pcpu_workers[cpu], worker);\n\t}\n\treturn 0;\n}\n#else\nstatic inline void erofs_destroy_percpu_workers(void) {}\nstatic inline int erofs_init_percpu_workers(void) { return 0; }\n#endif\n\n#if defined(CONFIG_HOTPLUG_CPU) && defined(CONFIG_EROFS_FS_PCPU_KTHREAD)\nstatic DEFINE_SPINLOCK(z_erofs_pcpu_worker_lock);\nstatic enum cpuhp_state erofs_cpuhp_state;\n\nstatic int erofs_cpu_online(unsigned int cpu)\n{\n\tstruct kthread_worker *worker, *old;\n\n\tworker = erofs_init_percpu_worker(cpu);\n\tif (IS_ERR(worker))\n\t\treturn PTR_ERR(worker);\n\n\tspin_lock(&z_erofs_pcpu_worker_lock);\n\told = rcu_dereference_protected(z_erofs_pcpu_workers[cpu],\n\t\t\tlockdep_is_held(&z_erofs_pcpu_worker_lock));\n\tif (!old)\n\t\trcu_assign_pointer(z_erofs_pcpu_workers[cpu], worker);\n\tspin_unlock(&z_erofs_pcpu_worker_lock);\n\tif (old)\n\t\tkthread_destroy_worker(worker);\n\treturn 0;\n}\n\nstatic int erofs_cpu_offline(unsigned int cpu)\n{\n\tstruct kthread_worker *worker;\n\n\tspin_lock(&z_erofs_pcpu_worker_lock);\n\tworker = rcu_dereference_protected(z_erofs_pcpu_workers[cpu],\n\t\t\tlockdep_is_held(&z_erofs_pcpu_worker_lock));\n\trcu_assign_pointer(z_erofs_pcpu_workers[cpu], NULL);\n\tspin_unlock(&z_erofs_pcpu_worker_lock);\n\n\tsynchronize_rcu();\n\tif (worker)\n\t\tkthread_destroy_worker(worker);\n\treturn 0;\n}\n\nstatic int erofs_cpu_hotplug_init(void)\n{\n\tint state;\n\n\tstate = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,\n\t\t\t\"fs/erofs:online\", erofs_cpu_online, erofs_cpu_offline);\n\tif (state < 0)\n\t\treturn state;\n\n\terofs_cpuhp_state = state;\n\treturn 0;\n}\n\nstatic void erofs_cpu_hotplug_destroy(void)\n{\n\tif (erofs_cpuhp_state)\n\t\tcpuhp_remove_state_nocalls(erofs_cpuhp_state);\n}\n#else  \nstatic inline int erofs_cpu_hotplug_init(void) { return 0; }\nstatic inline void erofs_cpu_hotplug_destroy(void) {}\n#endif\n\nvoid z_erofs_exit_zip_subsystem(void)\n{\n\terofs_cpu_hotplug_destroy();\n\terofs_destroy_percpu_workers();\n\tdestroy_workqueue(z_erofs_workqueue);\n\tz_erofs_destroy_pcluster_pool();\n}\n\nint __init z_erofs_init_zip_subsystem(void)\n{\n\tint err = z_erofs_create_pcluster_pool();\n\n\tif (err)\n\t\tgoto out_error_pcluster_pool;\n\n\tz_erofs_workqueue = alloc_workqueue(\"erofs_worker\",\n\t\t\tWQ_UNBOUND | WQ_HIGHPRI, num_possible_cpus());\n\tif (!z_erofs_workqueue) {\n\t\terr = -ENOMEM;\n\t\tgoto out_error_workqueue_init;\n\t}\n\n\terr = erofs_init_percpu_workers();\n\tif (err)\n\t\tgoto out_error_pcpu_worker;\n\n\terr = erofs_cpu_hotplug_init();\n\tif (err < 0)\n\t\tgoto out_error_cpuhp_init;\n\treturn err;\n\nout_error_cpuhp_init:\n\terofs_destroy_percpu_workers();\nout_error_pcpu_worker:\n\tdestroy_workqueue(z_erofs_workqueue);\nout_error_workqueue_init:\n\tz_erofs_destroy_pcluster_pool();\nout_error_pcluster_pool:\n\treturn err;\n}\n\nenum z_erofs_pclustermode {\n\tZ_EROFS_PCLUSTER_INFLIGHT,\n\t \n\tZ_EROFS_PCLUSTER_FOLLOWED_NOINPLACE,\n\t \n\tZ_EROFS_PCLUSTER_FOLLOWED,\n};\n\nstruct z_erofs_decompress_frontend {\n\tstruct inode *const inode;\n\tstruct erofs_map_blocks map;\n\tstruct z_erofs_bvec_iter biter;\n\n\tstruct page *pagepool;\n\tstruct page *candidate_bvpage;\n\tstruct z_erofs_pcluster *pcl;\n\tz_erofs_next_pcluster_t owned_head;\n\tenum z_erofs_pclustermode mode;\n\n\terofs_off_t headoffset;\n\n\t \n\tunsigned int icur;\n};\n\n#define DECOMPRESS_FRONTEND_INIT(__i) { \\\n\t.inode = __i, .owned_head = Z_EROFS_PCLUSTER_TAIL, \\\n\t.mode = Z_EROFS_PCLUSTER_FOLLOWED }\n\nstatic bool z_erofs_should_alloc_cache(struct z_erofs_decompress_frontend *fe)\n{\n\tunsigned int cachestrategy = EROFS_I_SB(fe->inode)->opt.cache_strategy;\n\n\tif (cachestrategy <= EROFS_ZIP_CACHE_DISABLED)\n\t\treturn false;\n\n\tif (!(fe->map.m_flags & EROFS_MAP_FULL_MAPPED))\n\t\treturn true;\n\n\tif (cachestrategy >= EROFS_ZIP_CACHE_READAROUND &&\n\t    fe->map.m_la < fe->headoffset)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic void z_erofs_bind_cache(struct z_erofs_decompress_frontend *fe)\n{\n\tstruct address_space *mc = MNGD_MAPPING(EROFS_I_SB(fe->inode));\n\tstruct z_erofs_pcluster *pcl = fe->pcl;\n\tbool shouldalloc = z_erofs_should_alloc_cache(fe);\n\tbool standalone = true;\n\t \n\tgfp_t gfp = (mapping_gfp_mask(mc) & ~__GFP_DIRECT_RECLAIM) |\n\t\t\t__GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN;\n\tunsigned int i;\n\n\tif (fe->mode < Z_EROFS_PCLUSTER_FOLLOWED)\n\t\treturn;\n\n\tfor (i = 0; i < pcl->pclusterpages; ++i) {\n\t\tstruct page *page;\n\t\tvoid *t;\t \n\t\tstruct page *newpage = NULL;\n\n\t\t \n\t\tif (READ_ONCE(pcl->compressed_bvecs[i].page))\n\t\t\tcontinue;\n\n\t\tpage = find_get_page(mc, pcl->obj.index + i);\n\n\t\tif (page) {\n\t\t\tt = (void *)((unsigned long)page | 1);\n\t\t} else {\n\t\t\t \n\t\t\tstandalone = false;\n\t\t\tif (!shouldalloc)\n\t\t\t\tcontinue;\n\n\t\t\t \n\t\t\tnewpage = erofs_allocpage(&fe->pagepool, gfp);\n\t\t\tif (!newpage)\n\t\t\t\tcontinue;\n\t\t\tset_page_private(newpage, Z_EROFS_PREALLOCATED_PAGE);\n\t\t\tt = (void *)((unsigned long)newpage | 1);\n\t\t}\n\n\t\tif (!cmpxchg_relaxed(&pcl->compressed_bvecs[i].page, NULL, t))\n\t\t\tcontinue;\n\n\t\tif (page)\n\t\t\tput_page(page);\n\t\telse if (newpage)\n\t\t\terofs_pagepool_add(&fe->pagepool, newpage);\n\t}\n\n\t \n\tif (standalone)\n\t\tfe->mode = Z_EROFS_PCLUSTER_FOLLOWED_NOINPLACE;\n}\n\n \nint erofs_try_to_free_all_cached_pages(struct erofs_sb_info *sbi,\n\t\t\t\t       struct erofs_workgroup *grp)\n{\n\tstruct z_erofs_pcluster *const pcl =\n\t\tcontainer_of(grp, struct z_erofs_pcluster, obj);\n\tint i;\n\n\tDBG_BUGON(z_erofs_is_inline_pcluster(pcl));\n\t \n\tfor (i = 0; i < pcl->pclusterpages; ++i) {\n\t\tstruct page *page = pcl->compressed_bvecs[i].page;\n\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!trylock_page(page))\n\t\t\treturn -EBUSY;\n\n\t\tif (!erofs_page_is_managed(sbi, page))\n\t\t\tcontinue;\n\n\t\t \n\t\tWRITE_ONCE(pcl->compressed_bvecs[i].page, NULL);\n\t\tdetach_page_private(page);\n\t\tunlock_page(page);\n\t}\n\treturn 0;\n}\n\nstatic bool z_erofs_cache_release_folio(struct folio *folio, gfp_t gfp)\n{\n\tstruct z_erofs_pcluster *pcl = folio_get_private(folio);\n\tbool ret;\n\tint i;\n\n\tif (!folio_test_private(folio))\n\t\treturn true;\n\n\tret = false;\n\tspin_lock(&pcl->obj.lockref.lock);\n\tif (pcl->obj.lockref.count > 0)\n\t\tgoto out;\n\n\tDBG_BUGON(z_erofs_is_inline_pcluster(pcl));\n\tfor (i = 0; i < pcl->pclusterpages; ++i) {\n\t\tif (pcl->compressed_bvecs[i].page == &folio->page) {\n\t\t\tWRITE_ONCE(pcl->compressed_bvecs[i].page, NULL);\n\t\t\tret = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (ret)\n\t\tfolio_detach_private(folio);\nout:\n\tspin_unlock(&pcl->obj.lockref.lock);\n\treturn ret;\n}\n\n \nstatic void z_erofs_cache_invalidate_folio(struct folio *folio,\n\t\t\t\t\t   size_t offset, size_t length)\n{\n\tconst size_t stop = length + offset;\n\n\t \n\tDBG_BUGON(stop > folio_size(folio) || stop < length);\n\n\tif (offset == 0 && stop == folio_size(folio))\n\t\twhile (!z_erofs_cache_release_folio(folio, GFP_NOFS))\n\t\t\tcond_resched();\n}\n\nstatic const struct address_space_operations z_erofs_cache_aops = {\n\t.release_folio = z_erofs_cache_release_folio,\n\t.invalidate_folio = z_erofs_cache_invalidate_folio,\n};\n\nint erofs_init_managed_cache(struct super_block *sb)\n{\n\tstruct inode *const inode = new_inode(sb);\n\n\tif (!inode)\n\t\treturn -ENOMEM;\n\n\tset_nlink(inode, 1);\n\tinode->i_size = OFFSET_MAX;\n\tinode->i_mapping->a_ops = &z_erofs_cache_aops;\n\tmapping_set_gfp_mask(inode->i_mapping, GFP_NOFS);\n\tEROFS_SB(sb)->managed_cache = inode;\n\treturn 0;\n}\n\nstatic bool z_erofs_try_inplace_io(struct z_erofs_decompress_frontend *fe,\n\t\t\t\t   struct z_erofs_bvec *bvec)\n{\n\tstruct z_erofs_pcluster *const pcl = fe->pcl;\n\n\twhile (fe->icur > 0) {\n\t\tif (!cmpxchg(&pcl->compressed_bvecs[--fe->icur].page,\n\t\t\t     NULL, bvec->page)) {\n\t\t\tpcl->compressed_bvecs[fe->icur] = *bvec;\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}\n\n \nstatic int z_erofs_attach_page(struct z_erofs_decompress_frontend *fe,\n\t\t\t       struct z_erofs_bvec *bvec, bool exclusive)\n{\n\tint ret;\n\n\tif (exclusive) {\n\t\t \n\t\tif (z_erofs_try_inplace_io(fe, bvec))\n\t\t\treturn 0;\n\t\t \n\t\tif (fe->mode >= Z_EROFS_PCLUSTER_FOLLOWED &&\n\t\t    !fe->candidate_bvpage)\n\t\t\tfe->candidate_bvpage = bvec->page;\n\t}\n\tret = z_erofs_bvec_enqueue(&fe->biter, bvec, &fe->candidate_bvpage,\n\t\t\t\t   &fe->pagepool);\n\tfe->pcl->vcnt += (ret >= 0);\n\treturn ret;\n}\n\nstatic void z_erofs_try_to_claim_pcluster(struct z_erofs_decompress_frontend *f)\n{\n\tstruct z_erofs_pcluster *pcl = f->pcl;\n\tz_erofs_next_pcluster_t *owned_head = &f->owned_head;\n\n\t \n\tif (cmpxchg(&pcl->next, Z_EROFS_PCLUSTER_NIL,\n\t\t    *owned_head) == Z_EROFS_PCLUSTER_NIL) {\n\t\t*owned_head = &pcl->next;\n\t\t \n\t\tf->mode = Z_EROFS_PCLUSTER_FOLLOWED;\n\t\treturn;\n\t}\n\n\t \n\tf->mode = Z_EROFS_PCLUSTER_INFLIGHT;\n}\n\nstatic int z_erofs_register_pcluster(struct z_erofs_decompress_frontend *fe)\n{\n\tstruct erofs_map_blocks *map = &fe->map;\n\tbool ztailpacking = map->m_flags & EROFS_MAP_META;\n\tstruct z_erofs_pcluster *pcl;\n\tstruct erofs_workgroup *grp;\n\tint err;\n\n\tif (!(map->m_flags & EROFS_MAP_ENCODED) ||\n\t    (!ztailpacking && !(map->m_pa >> PAGE_SHIFT))) {\n\t\tDBG_BUGON(1);\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\t \n\tpcl = z_erofs_alloc_pcluster(ztailpacking ? 1 :\n\t\t\t\t     map->m_plen >> PAGE_SHIFT);\n\tif (IS_ERR(pcl))\n\t\treturn PTR_ERR(pcl);\n\n\tspin_lock_init(&pcl->obj.lockref.lock);\n\tpcl->obj.lockref.count = 1;\t \n\tpcl->algorithmformat = map->m_algorithmformat;\n\tpcl->length = 0;\n\tpcl->partial = true;\n\n\t \n\tpcl->next = fe->owned_head;\n\tpcl->pageofs_out = map->m_la & ~PAGE_MASK;\n\tfe->mode = Z_EROFS_PCLUSTER_FOLLOWED;\n\n\t \n\tmutex_init(&pcl->lock);\n\tDBG_BUGON(!mutex_trylock(&pcl->lock));\n\n\tif (ztailpacking) {\n\t\tpcl->obj.index = 0;\t \n\t\tpcl->pageofs_in = erofs_blkoff(fe->inode->i_sb, map->m_pa);\n\t\tpcl->tailpacking_size = map->m_plen;\n\t} else {\n\t\tpcl->obj.index = map->m_pa >> PAGE_SHIFT;\n\n\t\tgrp = erofs_insert_workgroup(fe->inode->i_sb, &pcl->obj);\n\t\tif (IS_ERR(grp)) {\n\t\t\terr = PTR_ERR(grp);\n\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (grp != &pcl->obj) {\n\t\t\tfe->pcl = container_of(grp,\n\t\t\t\t\tstruct z_erofs_pcluster, obj);\n\t\t\terr = -EEXIST;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\tfe->owned_head = &pcl->next;\n\tfe->pcl = pcl;\n\treturn 0;\n\nerr_out:\n\tmutex_unlock(&pcl->lock);\n\tz_erofs_free_pcluster(pcl);\n\treturn err;\n}\n\nstatic int z_erofs_pcluster_begin(struct z_erofs_decompress_frontend *fe)\n{\n\tstruct erofs_map_blocks *map = &fe->map;\n\tstruct super_block *sb = fe->inode->i_sb;\n\terofs_blk_t blknr = erofs_blknr(sb, map->m_pa);\n\tstruct erofs_workgroup *grp = NULL;\n\tint ret;\n\n\tDBG_BUGON(fe->pcl);\n\n\t \n\tDBG_BUGON(fe->owned_head == Z_EROFS_PCLUSTER_NIL);\n\n\tif (!(map->m_flags & EROFS_MAP_META)) {\n\t\tgrp = erofs_find_workgroup(sb, blknr);\n\t} else if ((map->m_pa & ~PAGE_MASK) + map->m_plen > PAGE_SIZE) {\n\t\tDBG_BUGON(1);\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\tif (grp) {\n\t\tfe->pcl = container_of(grp, struct z_erofs_pcluster, obj);\n\t\tret = -EEXIST;\n\t} else {\n\t\tret = z_erofs_register_pcluster(fe);\n\t}\n\n\tif (ret == -EEXIST) {\n\t\tmutex_lock(&fe->pcl->lock);\n\t\tz_erofs_try_to_claim_pcluster(fe);\n\t} else if (ret) {\n\t\treturn ret;\n\t}\n\n\tz_erofs_bvec_iter_begin(&fe->biter, &fe->pcl->bvset,\n\t\t\t\tZ_EROFS_INLINE_BVECS, fe->pcl->vcnt);\n\tif (!z_erofs_is_inline_pcluster(fe->pcl)) {\n\t\t \n\t\tz_erofs_bind_cache(fe);\n\t} else {\n\t\tvoid *mptr;\n\n\t\tmptr = erofs_read_metabuf(&map->buf, sb, blknr, EROFS_NO_KMAP);\n\t\tif (IS_ERR(mptr)) {\n\t\t\tret = PTR_ERR(mptr);\n\t\t\terofs_err(sb, \"failed to get inline data %d\", ret);\n\t\t\treturn ret;\n\t\t}\n\t\tget_page(map->buf.page);\n\t\tWRITE_ONCE(fe->pcl->compressed_bvecs[0].page, map->buf.page);\n\t\tfe->mode = Z_EROFS_PCLUSTER_FOLLOWED_NOINPLACE;\n\t}\n\t \n\tfe->icur = z_erofs_pclusterpages(fe->pcl);\n\treturn 0;\n}\n\n \nstatic void z_erofs_rcu_callback(struct rcu_head *head)\n{\n\tz_erofs_free_pcluster(container_of(head,\n\t\t\tstruct z_erofs_pcluster, rcu));\n}\n\nvoid erofs_workgroup_free_rcu(struct erofs_workgroup *grp)\n{\n\tstruct z_erofs_pcluster *const pcl =\n\t\tcontainer_of(grp, struct z_erofs_pcluster, obj);\n\n\tcall_rcu(&pcl->rcu, z_erofs_rcu_callback);\n}\n\nstatic void z_erofs_pcluster_end(struct z_erofs_decompress_frontend *fe)\n{\n\tstruct z_erofs_pcluster *pcl = fe->pcl;\n\n\tif (!pcl)\n\t\treturn;\n\n\tz_erofs_bvec_iter_end(&fe->biter);\n\tmutex_unlock(&pcl->lock);\n\n\tif (fe->candidate_bvpage)\n\t\tfe->candidate_bvpage = NULL;\n\n\t \n\tif (fe->mode < Z_EROFS_PCLUSTER_FOLLOWED_NOINPLACE)\n\t\terofs_workgroup_put(&pcl->obj);\n\n\tfe->pcl = NULL;\n}\n\nstatic int z_erofs_read_fragment(struct super_block *sb, struct page *page,\n\t\t\tunsigned int cur, unsigned int end, erofs_off_t pos)\n{\n\tstruct inode *packed_inode = EROFS_SB(sb)->packed_inode;\n\tstruct erofs_buf buf = __EROFS_BUF_INITIALIZER;\n\tunsigned int cnt;\n\tu8 *src;\n\n\tif (!packed_inode)\n\t\treturn -EFSCORRUPTED;\n\n\tbuf.inode = packed_inode;\n\tfor (; cur < end; cur += cnt, pos += cnt) {\n\t\tcnt = min_t(unsigned int, end - cur,\n\t\t\t    sb->s_blocksize - erofs_blkoff(sb, pos));\n\t\tsrc = erofs_bread(&buf, erofs_blknr(sb, pos), EROFS_KMAP);\n\t\tif (IS_ERR(src)) {\n\t\t\terofs_put_metabuf(&buf);\n\t\t\treturn PTR_ERR(src);\n\t\t}\n\t\tmemcpy_to_page(page, cur, src + erofs_blkoff(sb, pos), cnt);\n\t}\n\terofs_put_metabuf(&buf);\n\treturn 0;\n}\n\nstatic int z_erofs_do_read_page(struct z_erofs_decompress_frontend *fe,\n\t\t\t\tstruct page *page)\n{\n\tstruct inode *const inode = fe->inode;\n\tstruct erofs_map_blocks *const map = &fe->map;\n\tconst loff_t offset = page_offset(page);\n\tbool tight = true, exclusive;\n\tunsigned int cur, end, len, split;\n\tint err = 0;\n\n\tz_erofs_onlinepage_init(page);\n\n\tsplit = 0;\n\tend = PAGE_SIZE;\nrepeat:\n\tif (offset + end - 1 < map->m_la ||\n\t    offset + end - 1 >= map->m_la + map->m_llen) {\n\t\tz_erofs_pcluster_end(fe);\n\t\tmap->m_la = offset + end - 1;\n\t\tmap->m_llen = 0;\n\t\terr = z_erofs_map_blocks_iter(inode, map, 0);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\tcur = offset > map->m_la ? 0 : map->m_la - offset;\n\t \n\t++split;\n\n\tif (!(map->m_flags & EROFS_MAP_MAPPED)) {\n\t\tzero_user_segment(page, cur, end);\n\t\ttight = false;\n\t\tgoto next_part;\n\t}\n\n\tif (map->m_flags & EROFS_MAP_FRAGMENT) {\n\t\terofs_off_t fpos = offset + cur - map->m_la;\n\n\t\tlen = min_t(unsigned int, map->m_llen - fpos, end - cur);\n\t\terr = z_erofs_read_fragment(inode->i_sb, page, cur, cur + len,\n\t\t\t\tEROFS_I(inode)->z_fragmentoff + fpos);\n\t\tif (err)\n\t\t\tgoto out;\n\t\ttight = false;\n\t\tgoto next_part;\n\t}\n\n\tif (!fe->pcl) {\n\t\terr = z_erofs_pcluster_begin(fe);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\t \n\ttight &= (fe->mode > Z_EROFS_PCLUSTER_FOLLOWED_NOINPLACE);\n\texclusive = (!cur && ((split <= 1) || tight));\n\tif (cur)\n\t\ttight &= (fe->mode >= Z_EROFS_PCLUSTER_FOLLOWED);\n\n\terr = z_erofs_attach_page(fe, &((struct z_erofs_bvec) {\n\t\t\t\t\t.page = page,\n\t\t\t\t\t.offset = offset - map->m_la,\n\t\t\t\t\t.end = end,\n\t\t\t\t  }), exclusive);\n\tif (err)\n\t\tgoto out;\n\n\tz_erofs_onlinepage_split(page);\n\tif (fe->pcl->pageofs_out != (map->m_la & ~PAGE_MASK))\n\t\tfe->pcl->multibases = true;\n\tif (fe->pcl->length < offset + end - map->m_la) {\n\t\tfe->pcl->length = offset + end - map->m_la;\n\t\tfe->pcl->pageofs_out = map->m_la & ~PAGE_MASK;\n\t}\n\tif ((map->m_flags & EROFS_MAP_FULL_MAPPED) &&\n\t    !(map->m_flags & EROFS_MAP_PARTIAL_REF) &&\n\t    fe->pcl->length == map->m_llen)\n\t\tfe->pcl->partial = false;\nnext_part:\n\t \n\tmap->m_llen = offset + cur - map->m_la;\n\tmap->m_flags &= ~EROFS_MAP_FULL_MAPPED;\n\n\tend = cur;\n\tif (end > 0)\n\t\tgoto repeat;\n\nout:\n\tz_erofs_onlinepage_endio(page, err);\n\treturn err;\n}\n\nstatic bool z_erofs_is_sync_decompress(struct erofs_sb_info *sbi,\n\t\t\t\t       unsigned int readahead_pages)\n{\n\t \n\tif ((sbi->opt.sync_decompress == EROFS_SYNC_DECOMPRESS_AUTO) &&\n\t    !readahead_pages)\n\t\treturn true;\n\n\tif ((sbi->opt.sync_decompress == EROFS_SYNC_DECOMPRESS_FORCE_ON) &&\n\t    (readahead_pages <= sbi->opt.max_sync_decompress_pages))\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic bool z_erofs_page_is_invalidated(struct page *page)\n{\n\treturn !page->mapping && !z_erofs_is_shortlived_page(page);\n}\n\nstruct z_erofs_decompress_backend {\n\tstruct page *onstack_pages[Z_EROFS_ONSTACK_PAGES];\n\tstruct super_block *sb;\n\tstruct z_erofs_pcluster *pcl;\n\n\t \n\tstruct page **decompressed_pages;\n\t \n\tstruct page **compressed_pages;\n\n\tstruct list_head decompressed_secondary_bvecs;\n\tstruct page **pagepool;\n\tunsigned int onstack_used, nr_pages;\n};\n\nstruct z_erofs_bvec_item {\n\tstruct z_erofs_bvec bvec;\n\tstruct list_head list;\n};\n\nstatic void z_erofs_do_decompressed_bvec(struct z_erofs_decompress_backend *be,\n\t\t\t\t\t struct z_erofs_bvec *bvec)\n{\n\tstruct z_erofs_bvec_item *item;\n\tunsigned int pgnr;\n\n\tif (!((bvec->offset + be->pcl->pageofs_out) & ~PAGE_MASK) &&\n\t    (bvec->end == PAGE_SIZE ||\n\t     bvec->offset + bvec->end == be->pcl->length)) {\n\t\tpgnr = (bvec->offset + be->pcl->pageofs_out) >> PAGE_SHIFT;\n\t\tDBG_BUGON(pgnr >= be->nr_pages);\n\t\tif (!be->decompressed_pages[pgnr]) {\n\t\t\tbe->decompressed_pages[pgnr] = bvec->page;\n\t\t\treturn;\n\t\t}\n\t}\n\n\t \n\titem = kmalloc(sizeof(*item), GFP_KERNEL | __GFP_NOFAIL);\n\titem->bvec = *bvec;\n\tlist_add(&item->list, &be->decompressed_secondary_bvecs);\n}\n\nstatic void z_erofs_fill_other_copies(struct z_erofs_decompress_backend *be,\n\t\t\t\t      int err)\n{\n\tunsigned int off0 = be->pcl->pageofs_out;\n\tstruct list_head *p, *n;\n\n\tlist_for_each_safe(p, n, &be->decompressed_secondary_bvecs) {\n\t\tstruct z_erofs_bvec_item *bvi;\n\t\tunsigned int end, cur;\n\t\tvoid *dst, *src;\n\n\t\tbvi = container_of(p, struct z_erofs_bvec_item, list);\n\t\tcur = bvi->bvec.offset < 0 ? -bvi->bvec.offset : 0;\n\t\tend = min_t(unsigned int, be->pcl->length - bvi->bvec.offset,\n\t\t\t    bvi->bvec.end);\n\t\tdst = kmap_local_page(bvi->bvec.page);\n\t\twhile (cur < end) {\n\t\t\tunsigned int pgnr, scur, len;\n\n\t\t\tpgnr = (bvi->bvec.offset + cur + off0) >> PAGE_SHIFT;\n\t\t\tDBG_BUGON(pgnr >= be->nr_pages);\n\n\t\t\tscur = bvi->bvec.offset + cur -\n\t\t\t\t\t((pgnr << PAGE_SHIFT) - off0);\n\t\t\tlen = min_t(unsigned int, end - cur, PAGE_SIZE - scur);\n\t\t\tif (!be->decompressed_pages[pgnr]) {\n\t\t\t\terr = -EFSCORRUPTED;\n\t\t\t\tcur += len;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tsrc = kmap_local_page(be->decompressed_pages[pgnr]);\n\t\t\tmemcpy(dst + cur, src + scur, len);\n\t\t\tkunmap_local(src);\n\t\t\tcur += len;\n\t\t}\n\t\tkunmap_local(dst);\n\t\tz_erofs_onlinepage_endio(bvi->bvec.page, err);\n\t\tlist_del(p);\n\t\tkfree(bvi);\n\t}\n}\n\nstatic void z_erofs_parse_out_bvecs(struct z_erofs_decompress_backend *be)\n{\n\tstruct z_erofs_pcluster *pcl = be->pcl;\n\tstruct z_erofs_bvec_iter biter;\n\tstruct page *old_bvpage;\n\tint i;\n\n\tz_erofs_bvec_iter_begin(&biter, &pcl->bvset, Z_EROFS_INLINE_BVECS, 0);\n\tfor (i = 0; i < pcl->vcnt; ++i) {\n\t\tstruct z_erofs_bvec bvec;\n\n\t\tz_erofs_bvec_dequeue(&biter, &bvec, &old_bvpage);\n\n\t\tif (old_bvpage)\n\t\t\tz_erofs_put_shortlivedpage(be->pagepool, old_bvpage);\n\n\t\tDBG_BUGON(z_erofs_page_is_invalidated(bvec.page));\n\t\tz_erofs_do_decompressed_bvec(be, &bvec);\n\t}\n\n\told_bvpage = z_erofs_bvec_iter_end(&biter);\n\tif (old_bvpage)\n\t\tz_erofs_put_shortlivedpage(be->pagepool, old_bvpage);\n}\n\nstatic int z_erofs_parse_in_bvecs(struct z_erofs_decompress_backend *be,\n\t\t\t\t  bool *overlapped)\n{\n\tstruct z_erofs_pcluster *pcl = be->pcl;\n\tunsigned int pclusterpages = z_erofs_pclusterpages(pcl);\n\tint i, err = 0;\n\n\t*overlapped = false;\n\tfor (i = 0; i < pclusterpages; ++i) {\n\t\tstruct z_erofs_bvec *bvec = &pcl->compressed_bvecs[i];\n\t\tstruct page *page = bvec->page;\n\n\t\t \n\t\tif (!page) {\n\t\t\tDBG_BUGON(1);\n\t\t\tcontinue;\n\t\t}\n\t\tbe->compressed_pages[i] = page;\n\n\t\tif (z_erofs_is_inline_pcluster(pcl)) {\n\t\t\tif (!PageUptodate(page))\n\t\t\t\terr = -EIO;\n\t\t\tcontinue;\n\t\t}\n\n\t\tDBG_BUGON(z_erofs_page_is_invalidated(page));\n\t\tif (!z_erofs_is_shortlived_page(page)) {\n\t\t\tif (erofs_page_is_managed(EROFS_SB(be->sb), page)) {\n\t\t\t\tif (!PageUptodate(page))\n\t\t\t\t\terr = -EIO;\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tz_erofs_do_decompressed_bvec(be, bvec);\n\t\t\t*overlapped = true;\n\t\t}\n\t}\n\n\tif (err)\n\t\treturn err;\n\treturn 0;\n}\n\nstatic int z_erofs_decompress_pcluster(struct z_erofs_decompress_backend *be,\n\t\t\t\t       int err)\n{\n\tstruct erofs_sb_info *const sbi = EROFS_SB(be->sb);\n\tstruct z_erofs_pcluster *pcl = be->pcl;\n\tunsigned int pclusterpages = z_erofs_pclusterpages(pcl);\n\tconst struct z_erofs_decompressor *decompressor =\n\t\t\t\t&erofs_decompressors[pcl->algorithmformat];\n\tunsigned int i, inputsize;\n\tint err2;\n\tstruct page *page;\n\tbool overlapped;\n\n\tmutex_lock(&pcl->lock);\n\tbe->nr_pages = PAGE_ALIGN(pcl->length + pcl->pageofs_out) >> PAGE_SHIFT;\n\n\t \n\tbe->decompressed_pages = NULL;\n\tbe->compressed_pages = NULL;\n\tbe->onstack_used = 0;\n\tif (be->nr_pages <= Z_EROFS_ONSTACK_PAGES) {\n\t\tbe->decompressed_pages = be->onstack_pages;\n\t\tbe->onstack_used = be->nr_pages;\n\t\tmemset(be->decompressed_pages, 0,\n\t\t       sizeof(struct page *) * be->nr_pages);\n\t}\n\n\tif (pclusterpages + be->onstack_used <= Z_EROFS_ONSTACK_PAGES)\n\t\tbe->compressed_pages = be->onstack_pages + be->onstack_used;\n\n\tif (!be->decompressed_pages)\n\t\tbe->decompressed_pages =\n\t\t\tkvcalloc(be->nr_pages, sizeof(struct page *),\n\t\t\t\t GFP_KERNEL | __GFP_NOFAIL);\n\tif (!be->compressed_pages)\n\t\tbe->compressed_pages =\n\t\t\tkvcalloc(pclusterpages, sizeof(struct page *),\n\t\t\t\t GFP_KERNEL | __GFP_NOFAIL);\n\n\tz_erofs_parse_out_bvecs(be);\n\terr2 = z_erofs_parse_in_bvecs(be, &overlapped);\n\tif (err2)\n\t\terr = err2;\n\tif (err)\n\t\tgoto out;\n\n\tif (z_erofs_is_inline_pcluster(pcl))\n\t\tinputsize = pcl->tailpacking_size;\n\telse\n\t\tinputsize = pclusterpages * PAGE_SIZE;\n\n\terr = decompressor->decompress(&(struct z_erofs_decompress_req) {\n\t\t\t\t\t.sb = be->sb,\n\t\t\t\t\t.in = be->compressed_pages,\n\t\t\t\t\t.out = be->decompressed_pages,\n\t\t\t\t\t.pageofs_in = pcl->pageofs_in,\n\t\t\t\t\t.pageofs_out = pcl->pageofs_out,\n\t\t\t\t\t.inputsize = inputsize,\n\t\t\t\t\t.outputsize = pcl->length,\n\t\t\t\t\t.alg = pcl->algorithmformat,\n\t\t\t\t\t.inplace_io = overlapped,\n\t\t\t\t\t.partial_decoding = pcl->partial,\n\t\t\t\t\t.fillgaps = pcl->multibases,\n\t\t\t\t }, be->pagepool);\n\nout:\n\t \n\tif (z_erofs_is_inline_pcluster(pcl)) {\n\t\tpage = pcl->compressed_bvecs[0].page;\n\t\tWRITE_ONCE(pcl->compressed_bvecs[0].page, NULL);\n\t\tput_page(page);\n\t} else {\n\t\tfor (i = 0; i < pclusterpages; ++i) {\n\t\t\t \n\t\t\tpage = be->compressed_pages[i];\n\n\t\t\tif (erofs_page_is_managed(sbi, page))\n\t\t\t\tcontinue;\n\t\t\t(void)z_erofs_put_shortlivedpage(be->pagepool, page);\n\t\t\tWRITE_ONCE(pcl->compressed_bvecs[i].page, NULL);\n\t\t}\n\t}\n\tif (be->compressed_pages < be->onstack_pages ||\n\t    be->compressed_pages >= be->onstack_pages + Z_EROFS_ONSTACK_PAGES)\n\t\tkvfree(be->compressed_pages);\n\tz_erofs_fill_other_copies(be, err);\n\n\tfor (i = 0; i < be->nr_pages; ++i) {\n\t\tpage = be->decompressed_pages[i];\n\t\tif (!page)\n\t\t\tcontinue;\n\n\t\tDBG_BUGON(z_erofs_page_is_invalidated(page));\n\n\t\t \n\t\tif (z_erofs_put_shortlivedpage(be->pagepool, page))\n\t\t\tcontinue;\n\t\tz_erofs_onlinepage_endio(page, err);\n\t}\n\n\tif (be->decompressed_pages != be->onstack_pages)\n\t\tkvfree(be->decompressed_pages);\n\n\tpcl->length = 0;\n\tpcl->partial = true;\n\tpcl->multibases = false;\n\tpcl->bvset.nextpage = NULL;\n\tpcl->vcnt = 0;\n\n\t \n\tWRITE_ONCE(pcl->next, Z_EROFS_PCLUSTER_NIL);\n\tmutex_unlock(&pcl->lock);\n\treturn err;\n}\n\nstatic void z_erofs_decompress_queue(const struct z_erofs_decompressqueue *io,\n\t\t\t\t     struct page **pagepool)\n{\n\tstruct z_erofs_decompress_backend be = {\n\t\t.sb = io->sb,\n\t\t.pagepool = pagepool,\n\t\t.decompressed_secondary_bvecs =\n\t\t\tLIST_HEAD_INIT(be.decompressed_secondary_bvecs),\n\t};\n\tz_erofs_next_pcluster_t owned = io->head;\n\n\twhile (owned != Z_EROFS_PCLUSTER_TAIL) {\n\t\tDBG_BUGON(owned == Z_EROFS_PCLUSTER_NIL);\n\n\t\tbe.pcl = container_of(owned, struct z_erofs_pcluster, next);\n\t\towned = READ_ONCE(be.pcl->next);\n\n\t\tz_erofs_decompress_pcluster(&be, io->eio ? -EIO : 0);\n\t\tif (z_erofs_is_inline_pcluster(be.pcl))\n\t\t\tz_erofs_free_pcluster(be.pcl);\n\t\telse\n\t\t\terofs_workgroup_put(&be.pcl->obj);\n\t}\n}\n\nstatic void z_erofs_decompressqueue_work(struct work_struct *work)\n{\n\tstruct z_erofs_decompressqueue *bgq =\n\t\tcontainer_of(work, struct z_erofs_decompressqueue, u.work);\n\tstruct page *pagepool = NULL;\n\n\tDBG_BUGON(bgq->head == Z_EROFS_PCLUSTER_TAIL);\n\tz_erofs_decompress_queue(bgq, &pagepool);\n\terofs_release_pages(&pagepool);\n\tkvfree(bgq);\n}\n\n#ifdef CONFIG_EROFS_FS_PCPU_KTHREAD\nstatic void z_erofs_decompressqueue_kthread_work(struct kthread_work *work)\n{\n\tz_erofs_decompressqueue_work((struct work_struct *)work);\n}\n#endif\n\nstatic void z_erofs_decompress_kickoff(struct z_erofs_decompressqueue *io,\n\t\t\t\t       int bios)\n{\n\tstruct erofs_sb_info *const sbi = EROFS_SB(io->sb);\n\n\t \n\tif (io->sync) {\n\t\tif (!atomic_add_return(bios, &io->pending_bios))\n\t\t\tcomplete(&io->u.done);\n\t\treturn;\n\t}\n\n\tif (atomic_add_return(bios, &io->pending_bios))\n\t\treturn;\n\t \n\tif (!in_task() || irqs_disabled() || rcu_read_lock_any_held()) {\n#ifdef CONFIG_EROFS_FS_PCPU_KTHREAD\n\t\tstruct kthread_worker *worker;\n\n\t\trcu_read_lock();\n\t\tworker = rcu_dereference(\n\t\t\t\tz_erofs_pcpu_workers[raw_smp_processor_id()]);\n\t\tif (!worker) {\n\t\t\tINIT_WORK(&io->u.work, z_erofs_decompressqueue_work);\n\t\t\tqueue_work(z_erofs_workqueue, &io->u.work);\n\t\t} else {\n\t\t\tkthread_queue_work(worker, &io->u.kthread_work);\n\t\t}\n\t\trcu_read_unlock();\n#else\n\t\tqueue_work(z_erofs_workqueue, &io->u.work);\n#endif\n\t\t \n\t\tif (sbi->opt.sync_decompress == EROFS_SYNC_DECOMPRESS_AUTO)\n\t\t\tsbi->opt.sync_decompress = EROFS_SYNC_DECOMPRESS_FORCE_ON;\n\t\treturn;\n\t}\n\tz_erofs_decompressqueue_work(&io->u.work);\n}\n\nstatic struct page *pickup_page_for_submission(struct z_erofs_pcluster *pcl,\n\t\t\t\t\t       unsigned int nr,\n\t\t\t\t\t       struct page **pagepool,\n\t\t\t\t\t       struct address_space *mc)\n{\n\tconst pgoff_t index = pcl->obj.index;\n\tgfp_t gfp = mapping_gfp_mask(mc);\n\tbool tocache = false;\n\n\tstruct address_space *mapping;\n\tstruct page *oldpage, *page;\n\tint justfound;\n\nrepeat:\n\tpage = READ_ONCE(pcl->compressed_bvecs[nr].page);\n\toldpage = page;\n\n\tif (!page)\n\t\tgoto out_allocpage;\n\n\tjustfound = (unsigned long)page & 1UL;\n\tpage = (struct page *)((unsigned long)page & ~1UL);\n\n\t \n\tif (page->private == Z_EROFS_PREALLOCATED_PAGE) {\n\t\tWRITE_ONCE(pcl->compressed_bvecs[nr].page, page);\n\t\tset_page_private(page, 0);\n\t\ttocache = true;\n\t\tgoto out_tocache;\n\t}\n\tmapping = READ_ONCE(page->mapping);\n\n\t \n\tif (mapping && mapping != mc)\n\t\t \n\t\tgoto out;\n\n\t \n\tif (z_erofs_is_shortlived_page(page))\n\t\tgoto out;\n\n\tlock_page(page);\n\n\t \n\tDBG_BUGON(justfound && PagePrivate(page));\n\n\t \n\tif (page->mapping == mc) {\n\t\tWRITE_ONCE(pcl->compressed_bvecs[nr].page, page);\n\n\t\tif (!PagePrivate(page)) {\n\t\t\t \n\t\t\tDBG_BUGON(!justfound);\n\n\t\t\tjustfound = 0;\n\t\t\tset_page_private(page, (unsigned long)pcl);\n\t\t\tSetPagePrivate(page);\n\t\t}\n\n\t\t \n\t\tif (PageUptodate(page)) {\n\t\t\tunlock_page(page);\n\t\t\tpage = NULL;\n\t\t}\n\t\tgoto out;\n\t}\n\n\t \n\tDBG_BUGON(page->mapping);\n\tDBG_BUGON(!justfound);\n\n\ttocache = true;\n\tunlock_page(page);\n\tput_page(page);\nout_allocpage:\n\tpage = erofs_allocpage(pagepool, gfp | __GFP_NOFAIL);\n\tif (oldpage != cmpxchg(&pcl->compressed_bvecs[nr].page,\n\t\t\t       oldpage, page)) {\n\t\terofs_pagepool_add(pagepool, page);\n\t\tcond_resched();\n\t\tgoto repeat;\n\t}\nout_tocache:\n\tif (!tocache || add_to_page_cache_lru(page, mc, index + nr, gfp)) {\n\t\t \n\t\tset_page_private(page, Z_EROFS_SHORTLIVED_PAGE);\n\t\tgoto out;\n\t}\n\tattach_page_private(page, pcl);\n\t \n\tput_page(page);\n\nout:\t \n\treturn page;\n}\n\nstatic struct z_erofs_decompressqueue *jobqueue_init(struct super_block *sb,\n\t\t\t      struct z_erofs_decompressqueue *fgq, bool *fg)\n{\n\tstruct z_erofs_decompressqueue *q;\n\n\tif (fg && !*fg) {\n\t\tq = kvzalloc(sizeof(*q), GFP_KERNEL | __GFP_NOWARN);\n\t\tif (!q) {\n\t\t\t*fg = true;\n\t\t\tgoto fg_out;\n\t\t}\n#ifdef CONFIG_EROFS_FS_PCPU_KTHREAD\n\t\tkthread_init_work(&q->u.kthread_work,\n\t\t\t\t  z_erofs_decompressqueue_kthread_work);\n#else\n\t\tINIT_WORK(&q->u.work, z_erofs_decompressqueue_work);\n#endif\n\t} else {\nfg_out:\n\t\tq = fgq;\n\t\tinit_completion(&fgq->u.done);\n\t\tatomic_set(&fgq->pending_bios, 0);\n\t\tq->eio = false;\n\t\tq->sync = true;\n\t}\n\tq->sb = sb;\n\tq->head = Z_EROFS_PCLUSTER_TAIL;\n\treturn q;\n}\n\n \nenum {\n\tJQ_BYPASS,\n\tJQ_SUBMIT,\n\tNR_JOBQUEUES,\n};\n\nstatic void move_to_bypass_jobqueue(struct z_erofs_pcluster *pcl,\n\t\t\t\t    z_erofs_next_pcluster_t qtail[],\n\t\t\t\t    z_erofs_next_pcluster_t owned_head)\n{\n\tz_erofs_next_pcluster_t *const submit_qtail = qtail[JQ_SUBMIT];\n\tz_erofs_next_pcluster_t *const bypass_qtail = qtail[JQ_BYPASS];\n\n\tWRITE_ONCE(pcl->next, Z_EROFS_PCLUSTER_TAIL);\n\n\tWRITE_ONCE(*submit_qtail, owned_head);\n\tWRITE_ONCE(*bypass_qtail, &pcl->next);\n\n\tqtail[JQ_BYPASS] = &pcl->next;\n}\n\nstatic void z_erofs_decompressqueue_endio(struct bio *bio)\n{\n\tstruct z_erofs_decompressqueue *q = bio->bi_private;\n\tblk_status_t err = bio->bi_status;\n\tstruct bio_vec *bvec;\n\tstruct bvec_iter_all iter_all;\n\n\tbio_for_each_segment_all(bvec, bio, iter_all) {\n\t\tstruct page *page = bvec->bv_page;\n\n\t\tDBG_BUGON(PageUptodate(page));\n\t\tDBG_BUGON(z_erofs_page_is_invalidated(page));\n\n\t\tif (erofs_page_is_managed(EROFS_SB(q->sb), page)) {\n\t\t\tif (!err)\n\t\t\t\tSetPageUptodate(page);\n\t\t\tunlock_page(page);\n\t\t}\n\t}\n\tif (err)\n\t\tq->eio = true;\n\tz_erofs_decompress_kickoff(q, -1);\n\tbio_put(bio);\n}\n\nstatic void z_erofs_submit_queue(struct z_erofs_decompress_frontend *f,\n\t\t\t\t struct z_erofs_decompressqueue *fgq,\n\t\t\t\t bool *force_fg, bool readahead)\n{\n\tstruct super_block *sb = f->inode->i_sb;\n\tstruct address_space *mc = MNGD_MAPPING(EROFS_SB(sb));\n\tz_erofs_next_pcluster_t qtail[NR_JOBQUEUES];\n\tstruct z_erofs_decompressqueue *q[NR_JOBQUEUES];\n\tz_erofs_next_pcluster_t owned_head = f->owned_head;\n\t \n\tpgoff_t last_index;\n\tstruct block_device *last_bdev;\n\tunsigned int nr_bios = 0;\n\tstruct bio *bio = NULL;\n\tunsigned long pflags;\n\tint memstall = 0;\n\n\t \n\tq[JQ_BYPASS] = jobqueue_init(sb, fgq + JQ_BYPASS, NULL);\n\tq[JQ_SUBMIT] = jobqueue_init(sb, fgq + JQ_SUBMIT, force_fg);\n\n\tqtail[JQ_BYPASS] = &q[JQ_BYPASS]->head;\n\tqtail[JQ_SUBMIT] = &q[JQ_SUBMIT]->head;\n\n\t \n\tq[JQ_SUBMIT]->head = owned_head;\n\n\tdo {\n\t\tstruct erofs_map_dev mdev;\n\t\tstruct z_erofs_pcluster *pcl;\n\t\tpgoff_t cur, end;\n\t\tunsigned int i = 0;\n\t\tbool bypass = true;\n\n\t\tDBG_BUGON(owned_head == Z_EROFS_PCLUSTER_NIL);\n\t\tpcl = container_of(owned_head, struct z_erofs_pcluster, next);\n\t\towned_head = READ_ONCE(pcl->next);\n\n\t\tif (z_erofs_is_inline_pcluster(pcl)) {\n\t\t\tmove_to_bypass_jobqueue(pcl, qtail, owned_head);\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tmdev = (struct erofs_map_dev) {\n\t\t\t.m_pa = erofs_pos(sb, pcl->obj.index),\n\t\t};\n\t\t(void)erofs_map_dev(sb, &mdev);\n\n\t\tcur = erofs_blknr(sb, mdev.m_pa);\n\t\tend = cur + pcl->pclusterpages;\n\n\t\tdo {\n\t\t\tstruct page *page;\n\n\t\t\tpage = pickup_page_for_submission(pcl, i++,\n\t\t\t\t\t&f->pagepool, mc);\n\t\t\tif (!page)\n\t\t\t\tcontinue;\n\n\t\t\tif (bio && (cur != last_index + 1 ||\n\t\t\t\t    last_bdev != mdev.m_bdev)) {\nsubmit_bio_retry:\n\t\t\t\tsubmit_bio(bio);\n\t\t\t\tif (memstall) {\n\t\t\t\t\tpsi_memstall_leave(&pflags);\n\t\t\t\t\tmemstall = 0;\n\t\t\t\t}\n\t\t\t\tbio = NULL;\n\t\t\t}\n\n\t\t\tif (unlikely(PageWorkingset(page)) && !memstall) {\n\t\t\t\tpsi_memstall_enter(&pflags);\n\t\t\t\tmemstall = 1;\n\t\t\t}\n\n\t\t\tif (!bio) {\n\t\t\t\tbio = bio_alloc(mdev.m_bdev, BIO_MAX_VECS,\n\t\t\t\t\t\tREQ_OP_READ, GFP_NOIO);\n\t\t\t\tbio->bi_end_io = z_erofs_decompressqueue_endio;\n\n\t\t\t\tlast_bdev = mdev.m_bdev;\n\t\t\t\tbio->bi_iter.bi_sector = (sector_t)cur <<\n\t\t\t\t\t(sb->s_blocksize_bits - 9);\n\t\t\t\tbio->bi_private = q[JQ_SUBMIT];\n\t\t\t\tif (readahead)\n\t\t\t\t\tbio->bi_opf |= REQ_RAHEAD;\n\t\t\t\t++nr_bios;\n\t\t\t}\n\n\t\t\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE)\n\t\t\t\tgoto submit_bio_retry;\n\n\t\t\tlast_index = cur;\n\t\t\tbypass = false;\n\t\t} while (++cur < end);\n\n\t\tif (!bypass)\n\t\t\tqtail[JQ_SUBMIT] = &pcl->next;\n\t\telse\n\t\t\tmove_to_bypass_jobqueue(pcl, qtail, owned_head);\n\t} while (owned_head != Z_EROFS_PCLUSTER_TAIL);\n\n\tif (bio) {\n\t\tsubmit_bio(bio);\n\t\tif (memstall)\n\t\t\tpsi_memstall_leave(&pflags);\n\t}\n\n\t \n\tif (!*force_fg && !nr_bios) {\n\t\tkvfree(q[JQ_SUBMIT]);\n\t\treturn;\n\t}\n\tz_erofs_decompress_kickoff(q[JQ_SUBMIT], nr_bios);\n}\n\nstatic void z_erofs_runqueue(struct z_erofs_decompress_frontend *f,\n\t\t\t     bool force_fg, bool ra)\n{\n\tstruct z_erofs_decompressqueue io[NR_JOBQUEUES];\n\n\tif (f->owned_head == Z_EROFS_PCLUSTER_TAIL)\n\t\treturn;\n\tz_erofs_submit_queue(f, io, &force_fg, ra);\n\n\t \n\tz_erofs_decompress_queue(&io[JQ_BYPASS], &f->pagepool);\n\n\tif (!force_fg)\n\t\treturn;\n\n\t \n\twait_for_completion_io(&io[JQ_SUBMIT].u.done);\n\n\t \n\tz_erofs_decompress_queue(&io[JQ_SUBMIT], &f->pagepool);\n}\n\n \nstatic void z_erofs_pcluster_readmore(struct z_erofs_decompress_frontend *f,\n\t\tstruct readahead_control *rac, bool backmost)\n{\n\tstruct inode *inode = f->inode;\n\tstruct erofs_map_blocks *map = &f->map;\n\terofs_off_t cur, end, headoffset = f->headoffset;\n\tint err;\n\n\tif (backmost) {\n\t\tif (rac)\n\t\t\tend = headoffset + readahead_length(rac) - 1;\n\t\telse\n\t\t\tend = headoffset + PAGE_SIZE - 1;\n\t\tmap->m_la = end;\n\t\terr = z_erofs_map_blocks_iter(inode, map,\n\t\t\t\t\t      EROFS_GET_BLOCKS_READMORE);\n\t\tif (err)\n\t\t\treturn;\n\n\t\t \n\t\tif (rac) {\n\t\t\tcur = round_up(map->m_la + map->m_llen, PAGE_SIZE);\n\t\t\treadahead_expand(rac, headoffset, cur - headoffset);\n\t\t\treturn;\n\t\t}\n\t\tend = round_up(end, PAGE_SIZE);\n\t} else {\n\t\tend = round_up(map->m_la, PAGE_SIZE);\n\n\t\tif (!map->m_llen)\n\t\t\treturn;\n\t}\n\n\tcur = map->m_la + map->m_llen - 1;\n\twhile ((cur >= end) && (cur < i_size_read(inode))) {\n\t\tpgoff_t index = cur >> PAGE_SHIFT;\n\t\tstruct page *page;\n\n\t\tpage = erofs_grab_cache_page_nowait(inode->i_mapping, index);\n\t\tif (page) {\n\t\t\tif (PageUptodate(page))\n\t\t\t\tunlock_page(page);\n\t\t\telse\n\t\t\t\t(void)z_erofs_do_read_page(f, page);\n\t\t\tput_page(page);\n\t\t}\n\n\t\tif (cur < PAGE_SIZE)\n\t\t\tbreak;\n\t\tcur = (index << PAGE_SHIFT) - 1;\n\t}\n}\n\nstatic int z_erofs_read_folio(struct file *file, struct folio *folio)\n{\n\tstruct inode *const inode = folio->mapping->host;\n\tstruct erofs_sb_info *const sbi = EROFS_I_SB(inode);\n\tstruct z_erofs_decompress_frontend f = DECOMPRESS_FRONTEND_INIT(inode);\n\tint err;\n\n\ttrace_erofs_read_folio(folio, false);\n\tf.headoffset = (erofs_off_t)folio->index << PAGE_SHIFT;\n\n\tz_erofs_pcluster_readmore(&f, NULL, true);\n\terr = z_erofs_do_read_page(&f, &folio->page);\n\tz_erofs_pcluster_readmore(&f, NULL, false);\n\tz_erofs_pcluster_end(&f);\n\n\t \n\tz_erofs_runqueue(&f, z_erofs_is_sync_decompress(sbi, 0), false);\n\n\tif (err && err != -EINTR)\n\t\terofs_err(inode->i_sb, \"read error %d @ %lu of nid %llu\",\n\t\t\t  err, folio->index, EROFS_I(inode)->nid);\n\n\terofs_put_metabuf(&f.map.buf);\n\terofs_release_pages(&f.pagepool);\n\treturn err;\n}\n\nstatic void z_erofs_readahead(struct readahead_control *rac)\n{\n\tstruct inode *const inode = rac->mapping->host;\n\tstruct erofs_sb_info *const sbi = EROFS_I_SB(inode);\n\tstruct z_erofs_decompress_frontend f = DECOMPRESS_FRONTEND_INIT(inode);\n\tstruct folio *head = NULL, *folio;\n\tunsigned int nr_folios;\n\tint err;\n\n\tf.headoffset = readahead_pos(rac);\n\n\tz_erofs_pcluster_readmore(&f, rac, true);\n\tnr_folios = readahead_count(rac);\n\ttrace_erofs_readpages(inode, readahead_index(rac), nr_folios, false);\n\n\twhile ((folio = readahead_folio(rac))) {\n\t\tfolio->private = head;\n\t\thead = folio;\n\t}\n\n\t \n\twhile (head) {\n\t\tfolio = head;\n\t\thead = folio_get_private(folio);\n\n\t\terr = z_erofs_do_read_page(&f, &folio->page);\n\t\tif (err && err != -EINTR)\n\t\t\terofs_err(inode->i_sb, \"readahead error at folio %lu @ nid %llu\",\n\t\t\t\t  folio->index, EROFS_I(inode)->nid);\n\t}\n\tz_erofs_pcluster_readmore(&f, rac, false);\n\tz_erofs_pcluster_end(&f);\n\n\tz_erofs_runqueue(&f, z_erofs_is_sync_decompress(sbi, nr_folios), true);\n\terofs_put_metabuf(&f.map.buf);\n\terofs_release_pages(&f.pagepool);\n}\n\nconst struct address_space_operations z_erofs_aops = {\n\t.read_folio = z_erofs_read_folio,\n\t.readahead = z_erofs_readahead,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}