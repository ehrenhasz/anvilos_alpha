{
  "module_name": "segment.c",
  "hash_id": "c39ea5b3b5ceb5011ca9eea4274f88a2892ddce4ed07bab5f41b7f5a9a7d200f",
  "original_prompt": "Ingested from linux-6.6.14/fs/f2fs/segment.c",
  "human_readable_source": "\n \n#include <linux/fs.h>\n#include <linux/f2fs_fs.h>\n#include <linux/bio.h>\n#include <linux/blkdev.h>\n#include <linux/sched/mm.h>\n#include <linux/prefetch.h>\n#include <linux/kthread.h>\n#include <linux/swap.h>\n#include <linux/timer.h>\n#include <linux/freezer.h>\n#include <linux/sched/signal.h>\n#include <linux/random.h>\n\n#include \"f2fs.h\"\n#include \"segment.h\"\n#include \"node.h\"\n#include \"gc.h\"\n#include \"iostat.h\"\n#include <trace/events/f2fs.h>\n\n#define __reverse_ffz(x) __reverse_ffs(~(x))\n\nstatic struct kmem_cache *discard_entry_slab;\nstatic struct kmem_cache *discard_cmd_slab;\nstatic struct kmem_cache *sit_entry_set_slab;\nstatic struct kmem_cache *revoke_entry_slab;\n\nstatic unsigned long __reverse_ulong(unsigned char *str)\n{\n\tunsigned long tmp = 0;\n\tint shift = 24, idx = 0;\n\n#if BITS_PER_LONG == 64\n\tshift = 56;\n#endif\n\twhile (shift >= 0) {\n\t\ttmp |= (unsigned long)str[idx++] << shift;\n\t\tshift -= BITS_PER_BYTE;\n\t}\n\treturn tmp;\n}\n\n \nstatic inline unsigned long __reverse_ffs(unsigned long word)\n{\n\tint num = 0;\n\n#if BITS_PER_LONG == 64\n\tif ((word & 0xffffffff00000000UL) == 0)\n\t\tnum += 32;\n\telse\n\t\tword >>= 32;\n#endif\n\tif ((word & 0xffff0000) == 0)\n\t\tnum += 16;\n\telse\n\t\tword >>= 16;\n\n\tif ((word & 0xff00) == 0)\n\t\tnum += 8;\n\telse\n\t\tword >>= 8;\n\n\tif ((word & 0xf0) == 0)\n\t\tnum += 4;\n\telse\n\t\tword >>= 4;\n\n\tif ((word & 0xc) == 0)\n\t\tnum += 2;\n\telse\n\t\tword >>= 2;\n\n\tif ((word & 0x2) == 0)\n\t\tnum += 1;\n\treturn num;\n}\n\n \nstatic unsigned long __find_rev_next_bit(const unsigned long *addr,\n\t\t\tunsigned long size, unsigned long offset)\n{\n\tconst unsigned long *p = addr + BIT_WORD(offset);\n\tunsigned long result = size;\n\tunsigned long tmp;\n\n\tif (offset >= size)\n\t\treturn size;\n\n\tsize -= (offset & ~(BITS_PER_LONG - 1));\n\toffset %= BITS_PER_LONG;\n\n\twhile (1) {\n\t\tif (*p == 0)\n\t\t\tgoto pass;\n\n\t\ttmp = __reverse_ulong((unsigned char *)p);\n\n\t\ttmp &= ~0UL >> offset;\n\t\tif (size < BITS_PER_LONG)\n\t\t\ttmp &= (~0UL << (BITS_PER_LONG - size));\n\t\tif (tmp)\n\t\t\tgoto found;\npass:\n\t\tif (size <= BITS_PER_LONG)\n\t\t\tbreak;\n\t\tsize -= BITS_PER_LONG;\n\t\toffset = 0;\n\t\tp++;\n\t}\n\treturn result;\nfound:\n\treturn result - size + __reverse_ffs(tmp);\n}\n\nstatic unsigned long __find_rev_next_zero_bit(const unsigned long *addr,\n\t\t\tunsigned long size, unsigned long offset)\n{\n\tconst unsigned long *p = addr + BIT_WORD(offset);\n\tunsigned long result = size;\n\tunsigned long tmp;\n\n\tif (offset >= size)\n\t\treturn size;\n\n\tsize -= (offset & ~(BITS_PER_LONG - 1));\n\toffset %= BITS_PER_LONG;\n\n\twhile (1) {\n\t\tif (*p == ~0UL)\n\t\t\tgoto pass;\n\n\t\ttmp = __reverse_ulong((unsigned char *)p);\n\n\t\tif (offset)\n\t\t\ttmp |= ~0UL << (BITS_PER_LONG - offset);\n\t\tif (size < BITS_PER_LONG)\n\t\t\ttmp |= ~0UL >> size;\n\t\tif (tmp != ~0UL)\n\t\t\tgoto found;\npass:\n\t\tif (size <= BITS_PER_LONG)\n\t\t\tbreak;\n\t\tsize -= BITS_PER_LONG;\n\t\toffset = 0;\n\t\tp++;\n\t}\n\treturn result;\nfound:\n\treturn result - size + __reverse_ffz(tmp);\n}\n\nbool f2fs_need_SSR(struct f2fs_sb_info *sbi)\n{\n\tint node_secs = get_blocktype_secs(sbi, F2FS_DIRTY_NODES);\n\tint dent_secs = get_blocktype_secs(sbi, F2FS_DIRTY_DENTS);\n\tint imeta_secs = get_blocktype_secs(sbi, F2FS_DIRTY_IMETA);\n\n\tif (f2fs_lfs_mode(sbi))\n\t\treturn false;\n\tif (sbi->gc_mode == GC_URGENT_HIGH)\n\t\treturn true;\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED)))\n\t\treturn true;\n\n\treturn free_sections(sbi) <= (node_secs + 2 * dent_secs + imeta_secs +\n\t\t\tSM_I(sbi)->min_ssr_sections + reserved_sections(sbi));\n}\n\nvoid f2fs_abort_atomic_write(struct inode *inode, bool clean)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\n\tif (!f2fs_is_atomic_file(inode))\n\t\treturn;\n\n\trelease_atomic_write_cnt(inode);\n\tclear_inode_flag(inode, FI_ATOMIC_COMMITTED);\n\tclear_inode_flag(inode, FI_ATOMIC_REPLACE);\n\tclear_inode_flag(inode, FI_ATOMIC_FILE);\n\tstat_dec_atomic_inode(inode);\n\n\tF2FS_I(inode)->atomic_write_task = NULL;\n\n\tif (clean) {\n\t\ttruncate_inode_pages_final(inode->i_mapping);\n\t\tf2fs_i_size_write(inode, fi->original_i_size);\n\t\tfi->original_i_size = 0;\n\t}\n\t \n\tsync_inode_metadata(inode, 0);\n}\n\nstatic int __replace_atomic_write_block(struct inode *inode, pgoff_t index,\n\t\t\tblock_t new_addr, block_t *old_addr, bool recover)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct dnode_of_data dn;\n\tstruct node_info ni;\n\tint err;\n\nretry:\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\terr = f2fs_get_dnode_of_data(&dn, index, ALLOC_NODE);\n\tif (err) {\n\t\tif (err == -ENOMEM) {\n\t\t\tf2fs_io_schedule_timeout(DEFAULT_IO_TIMEOUT);\n\t\t\tgoto retry;\n\t\t}\n\t\treturn err;\n\t}\n\n\terr = f2fs_get_node_info(sbi, dn.nid, &ni, false);\n\tif (err) {\n\t\tf2fs_put_dnode(&dn);\n\t\treturn err;\n\t}\n\n\tif (recover) {\n\t\t \n\t\tif (!__is_valid_data_blkaddr(new_addr)) {\n\t\t\tif (new_addr == NULL_ADDR)\n\t\t\t\tdec_valid_block_count(sbi, inode, 1);\n\t\t\tf2fs_invalidate_blocks(sbi, dn.data_blkaddr);\n\t\t\tf2fs_update_data_blkaddr(&dn, new_addr);\n\t\t} else {\n\t\t\tf2fs_replace_block(sbi, &dn, dn.data_blkaddr,\n\t\t\t\tnew_addr, ni.version, true, true);\n\t\t}\n\t} else {\n\t\tblkcnt_t count = 1;\n\n\t\terr = inc_valid_block_count(sbi, inode, &count);\n\t\tif (err) {\n\t\t\tf2fs_put_dnode(&dn);\n\t\t\treturn err;\n\t\t}\n\n\t\t*old_addr = dn.data_blkaddr;\n\t\tf2fs_truncate_data_blocks_range(&dn, 1);\n\t\tdec_valid_block_count(sbi, F2FS_I(inode)->cow_inode, count);\n\n\t\tf2fs_replace_block(sbi, &dn, dn.data_blkaddr, new_addr,\n\t\t\t\t\tni.version, true, false);\n\t}\n\n\tf2fs_put_dnode(&dn);\n\n\ttrace_f2fs_replace_atomic_write_block(inode, F2FS_I(inode)->cow_inode,\n\t\t\tindex, old_addr ? *old_addr : 0, new_addr, recover);\n\treturn 0;\n}\n\nstatic void __complete_revoke_list(struct inode *inode, struct list_head *head,\n\t\t\t\t\tbool revoke)\n{\n\tstruct revoke_entry *cur, *tmp;\n\tpgoff_t start_index = 0;\n\tbool truncate = is_inode_flag_set(inode, FI_ATOMIC_REPLACE);\n\n\tlist_for_each_entry_safe(cur, tmp, head, list) {\n\t\tif (revoke) {\n\t\t\t__replace_atomic_write_block(inode, cur->index,\n\t\t\t\t\t\tcur->old_addr, NULL, true);\n\t\t} else if (truncate) {\n\t\t\tf2fs_truncate_hole(inode, start_index, cur->index);\n\t\t\tstart_index = cur->index + 1;\n\t\t}\n\n\t\tlist_del(&cur->list);\n\t\tkmem_cache_free(revoke_entry_slab, cur);\n\t}\n\n\tif (!revoke && truncate)\n\t\tf2fs_do_truncate_blocks(inode, start_index * PAGE_SIZE, false);\n}\n\nstatic int __f2fs_commit_atomic_write(struct inode *inode)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct inode *cow_inode = fi->cow_inode;\n\tstruct revoke_entry *new;\n\tstruct list_head revoke_list;\n\tblock_t blkaddr;\n\tstruct dnode_of_data dn;\n\tpgoff_t len = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);\n\tpgoff_t off = 0, blen, index;\n\tint ret = 0, i;\n\n\tINIT_LIST_HEAD(&revoke_list);\n\n\twhile (len) {\n\t\tblen = min_t(pgoff_t, ADDRS_PER_BLOCK(cow_inode), len);\n\n\t\tset_new_dnode(&dn, cow_inode, NULL, NULL, 0);\n\t\tret = f2fs_get_dnode_of_data(&dn, off, LOOKUP_NODE_RA);\n\t\tif (ret && ret != -ENOENT) {\n\t\t\tgoto out;\n\t\t} else if (ret == -ENOENT) {\n\t\t\tret = 0;\n\t\t\tif (dn.max_level == 0)\n\t\t\t\tgoto out;\n\t\t\tgoto next;\n\t\t}\n\n\t\tblen = min((pgoff_t)ADDRS_PER_PAGE(dn.node_page, cow_inode),\n\t\t\t\tlen);\n\t\tindex = off;\n\t\tfor (i = 0; i < blen; i++, dn.ofs_in_node++, index++) {\n\t\t\tblkaddr = f2fs_data_blkaddr(&dn);\n\n\t\t\tif (!__is_valid_data_blkaddr(blkaddr)) {\n\t\t\t\tcontinue;\n\t\t\t} else if (!f2fs_is_valid_blkaddr(sbi, blkaddr,\n\t\t\t\t\tDATA_GENERIC_ENHANCE)) {\n\t\t\t\tf2fs_put_dnode(&dn);\n\t\t\t\tret = -EFSCORRUPTED;\n\t\t\t\tf2fs_handle_error(sbi,\n\t\t\t\t\t\tERROR_INVALID_BLKADDR);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tnew = f2fs_kmem_cache_alloc(revoke_entry_slab, GFP_NOFS,\n\t\t\t\t\t\t\ttrue, NULL);\n\n\t\t\tret = __replace_atomic_write_block(inode, index, blkaddr,\n\t\t\t\t\t\t\t&new->old_addr, false);\n\t\t\tif (ret) {\n\t\t\t\tf2fs_put_dnode(&dn);\n\t\t\t\tkmem_cache_free(revoke_entry_slab, new);\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tf2fs_update_data_blkaddr(&dn, NULL_ADDR);\n\t\t\tnew->index = index;\n\t\t\tlist_add_tail(&new->list, &revoke_list);\n\t\t}\n\t\tf2fs_put_dnode(&dn);\nnext:\n\t\toff += blen;\n\t\tlen -= blen;\n\t}\n\nout:\n\tif (ret) {\n\t\tsbi->revoked_atomic_block += fi->atomic_write_cnt;\n\t} else {\n\t\tsbi->committed_atomic_block += fi->atomic_write_cnt;\n\t\tset_inode_flag(inode, FI_ATOMIC_COMMITTED);\n\t}\n\n\t__complete_revoke_list(inode, &revoke_list, ret ? true : false);\n\n\treturn ret;\n}\n\nint f2fs_commit_atomic_write(struct inode *inode)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tint err;\n\n\terr = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);\n\tif (err)\n\t\treturn err;\n\n\tf2fs_down_write(&fi->i_gc_rwsem[WRITE]);\n\tf2fs_lock_op(sbi);\n\n\terr = __f2fs_commit_atomic_write(inode);\n\n\tf2fs_unlock_op(sbi);\n\tf2fs_up_write(&fi->i_gc_rwsem[WRITE]);\n\n\treturn err;\n}\n\n \nvoid f2fs_balance_fs(struct f2fs_sb_info *sbi, bool need)\n{\n\tif (time_to_inject(sbi, FAULT_CHECKPOINT))\n\t\tf2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_FAULT_INJECT);\n\n\t \n\tif (need && excess_cached_nats(sbi))\n\t\tf2fs_balance_fs_bg(sbi, false);\n\n\tif (!f2fs_is_checkpoint_ready(sbi))\n\t\treturn;\n\n\t \n\tif (has_enough_free_secs(sbi, 0, 0))\n\t\treturn;\n\n\tif (test_opt(sbi, GC_MERGE) && sbi->gc_thread &&\n\t\t\t\tsbi->gc_thread->f2fs_gc_task) {\n\t\tDEFINE_WAIT(wait);\n\n\t\tprepare_to_wait(&sbi->gc_thread->fggc_wq, &wait,\n\t\t\t\t\tTASK_UNINTERRUPTIBLE);\n\t\twake_up(&sbi->gc_thread->gc_wait_queue_head);\n\t\tio_schedule();\n\t\tfinish_wait(&sbi->gc_thread->fggc_wq, &wait);\n\t} else {\n\t\tstruct f2fs_gc_control gc_control = {\n\t\t\t.victim_segno = NULL_SEGNO,\n\t\t\t.init_gc_type = BG_GC,\n\t\t\t.no_bg_gc = true,\n\t\t\t.should_migrate_blocks = false,\n\t\t\t.err_gc_skipped = false,\n\t\t\t.nr_free_secs = 1 };\n\t\tf2fs_down_write(&sbi->gc_lock);\n\t\tstat_inc_gc_call_count(sbi, FOREGROUND);\n\t\tf2fs_gc(sbi, &gc_control);\n\t}\n}\n\nstatic inline bool excess_dirty_threshold(struct f2fs_sb_info *sbi)\n{\n\tint factor = f2fs_rwsem_is_locked(&sbi->cp_rwsem) ? 3 : 2;\n\tunsigned int dents = get_pages(sbi, F2FS_DIRTY_DENTS);\n\tunsigned int qdata = get_pages(sbi, F2FS_DIRTY_QDATA);\n\tunsigned int nodes = get_pages(sbi, F2FS_DIRTY_NODES);\n\tunsigned int meta = get_pages(sbi, F2FS_DIRTY_META);\n\tunsigned int imeta = get_pages(sbi, F2FS_DIRTY_IMETA);\n\tunsigned int threshold = sbi->blocks_per_seg * factor *\n\t\t\t\t\tDEFAULT_DIRTY_THRESHOLD;\n\tunsigned int global_threshold = threshold * 3 / 2;\n\n\tif (dents >= threshold || qdata >= threshold ||\n\t\tnodes >= threshold || meta >= threshold ||\n\t\timeta >= threshold)\n\t\treturn true;\n\treturn dents + qdata + nodes + meta + imeta >  global_threshold;\n}\n\nvoid f2fs_balance_fs_bg(struct f2fs_sb_info *sbi, bool from_bg)\n{\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))\n\t\treturn;\n\n\t \n\tif (!f2fs_available_free_memory(sbi, READ_EXTENT_CACHE))\n\t\tf2fs_shrink_read_extent_tree(sbi,\n\t\t\t\tREAD_EXTENT_CACHE_SHRINK_NUMBER);\n\n\t \n\tif (!f2fs_available_free_memory(sbi, AGE_EXTENT_CACHE))\n\t\tf2fs_shrink_age_extent_tree(sbi,\n\t\t\t\tAGE_EXTENT_CACHE_SHRINK_NUMBER);\n\n\t \n\tif (!f2fs_available_free_memory(sbi, NAT_ENTRIES))\n\t\tf2fs_try_to_free_nats(sbi, NAT_ENTRY_PER_BLOCK);\n\n\tif (!f2fs_available_free_memory(sbi, FREE_NIDS))\n\t\tf2fs_try_to_free_nids(sbi, MAX_FREE_NIDS);\n\telse\n\t\tf2fs_build_free_nids(sbi, false, false);\n\n\tif (excess_dirty_nats(sbi) || excess_dirty_threshold(sbi) ||\n\t\texcess_prefree_segs(sbi) || !f2fs_space_for_roll_forward(sbi))\n\t\tgoto do_sync;\n\n\t \n\tif (is_inflight_io(sbi, REQ_TIME) ||\n\t\t(!f2fs_time_over(sbi, REQ_TIME) && f2fs_rwsem_is_locked(&sbi->cp_rwsem)))\n\t\treturn;\n\n\t \n\tif (f2fs_time_over(sbi, CP_TIME))\n\t\tgoto do_sync;\n\n\t \n\tif (f2fs_available_free_memory(sbi, NAT_ENTRIES) &&\n\t\tf2fs_available_free_memory(sbi, INO_ENTRIES))\n\t\treturn;\n\ndo_sync:\n\tif (test_opt(sbi, DATA_FLUSH) && from_bg) {\n\t\tstruct blk_plug plug;\n\n\t\tmutex_lock(&sbi->flush_lock);\n\n\t\tblk_start_plug(&plug);\n\t\tf2fs_sync_dirty_inodes(sbi, FILE_INODE, false);\n\t\tblk_finish_plug(&plug);\n\n\t\tmutex_unlock(&sbi->flush_lock);\n\t}\n\tstat_inc_cp_call_count(sbi, BACKGROUND);\n\tf2fs_sync_fs(sbi->sb, 1);\n}\n\nstatic int __submit_flush_wait(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev)\n{\n\tint ret = blkdev_issue_flush(bdev);\n\n\ttrace_f2fs_issue_flush(bdev, test_opt(sbi, NOBARRIER),\n\t\t\t\ttest_opt(sbi, FLUSH_MERGE), ret);\n\tif (!ret)\n\t\tf2fs_update_iostat(sbi, NULL, FS_FLUSH_IO, 0);\n\treturn ret;\n}\n\nstatic int submit_flush_wait(struct f2fs_sb_info *sbi, nid_t ino)\n{\n\tint ret = 0;\n\tint i;\n\n\tif (!f2fs_is_multi_device(sbi))\n\t\treturn __submit_flush_wait(sbi, sbi->sb->s_bdev);\n\n\tfor (i = 0; i < sbi->s_ndevs; i++) {\n\t\tif (!f2fs_is_dirty_device(sbi, ino, i, FLUSH_INO))\n\t\t\tcontinue;\n\t\tret = __submit_flush_wait(sbi, FDEV(i).bdev);\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\treturn ret;\n}\n\nstatic int issue_flush_thread(void *data)\n{\n\tstruct f2fs_sb_info *sbi = data;\n\tstruct flush_cmd_control *fcc = SM_I(sbi)->fcc_info;\n\twait_queue_head_t *q = &fcc->flush_wait_queue;\nrepeat:\n\tif (kthread_should_stop())\n\t\treturn 0;\n\n\tif (!llist_empty(&fcc->issue_list)) {\n\t\tstruct flush_cmd *cmd, *next;\n\t\tint ret;\n\n\t\tfcc->dispatch_list = llist_del_all(&fcc->issue_list);\n\t\tfcc->dispatch_list = llist_reverse_order(fcc->dispatch_list);\n\n\t\tcmd = llist_entry(fcc->dispatch_list, struct flush_cmd, llnode);\n\n\t\tret = submit_flush_wait(sbi, cmd->ino);\n\t\tatomic_inc(&fcc->issued_flush);\n\n\t\tllist_for_each_entry_safe(cmd, next,\n\t\t\t\t\t  fcc->dispatch_list, llnode) {\n\t\t\tcmd->ret = ret;\n\t\t\tcomplete(&cmd->wait);\n\t\t}\n\t\tfcc->dispatch_list = NULL;\n\t}\n\n\twait_event_interruptible(*q,\n\t\tkthread_should_stop() || !llist_empty(&fcc->issue_list));\n\tgoto repeat;\n}\n\nint f2fs_issue_flush(struct f2fs_sb_info *sbi, nid_t ino)\n{\n\tstruct flush_cmd_control *fcc = SM_I(sbi)->fcc_info;\n\tstruct flush_cmd cmd;\n\tint ret;\n\n\tif (test_opt(sbi, NOBARRIER))\n\t\treturn 0;\n\n\tif (!test_opt(sbi, FLUSH_MERGE)) {\n\t\tatomic_inc(&fcc->queued_flush);\n\t\tret = submit_flush_wait(sbi, ino);\n\t\tatomic_dec(&fcc->queued_flush);\n\t\tatomic_inc(&fcc->issued_flush);\n\t\treturn ret;\n\t}\n\n\tif (atomic_inc_return(&fcc->queued_flush) == 1 ||\n\t    f2fs_is_multi_device(sbi)) {\n\t\tret = submit_flush_wait(sbi, ino);\n\t\tatomic_dec(&fcc->queued_flush);\n\n\t\tatomic_inc(&fcc->issued_flush);\n\t\treturn ret;\n\t}\n\n\tcmd.ino = ino;\n\tinit_completion(&cmd.wait);\n\n\tllist_add(&cmd.llnode, &fcc->issue_list);\n\n\t \n\tsmp_mb();\n\n\tif (waitqueue_active(&fcc->flush_wait_queue))\n\t\twake_up(&fcc->flush_wait_queue);\n\n\tif (fcc->f2fs_issue_flush) {\n\t\twait_for_completion(&cmd.wait);\n\t\tatomic_dec(&fcc->queued_flush);\n\t} else {\n\t\tstruct llist_node *list;\n\n\t\tlist = llist_del_all(&fcc->issue_list);\n\t\tif (!list) {\n\t\t\twait_for_completion(&cmd.wait);\n\t\t\tatomic_dec(&fcc->queued_flush);\n\t\t} else {\n\t\t\tstruct flush_cmd *tmp, *next;\n\n\t\t\tret = submit_flush_wait(sbi, ino);\n\n\t\t\tllist_for_each_entry_safe(tmp, next, list, llnode) {\n\t\t\t\tif (tmp == &cmd) {\n\t\t\t\t\tcmd.ret = ret;\n\t\t\t\t\tatomic_dec(&fcc->queued_flush);\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\ttmp->ret = ret;\n\t\t\t\tcomplete(&tmp->wait);\n\t\t\t}\n\t\t}\n\t}\n\n\treturn cmd.ret;\n}\n\nint f2fs_create_flush_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tdev_t dev = sbi->sb->s_bdev->bd_dev;\n\tstruct flush_cmd_control *fcc;\n\n\tif (SM_I(sbi)->fcc_info) {\n\t\tfcc = SM_I(sbi)->fcc_info;\n\t\tif (fcc->f2fs_issue_flush)\n\t\t\treturn 0;\n\t\tgoto init_thread;\n\t}\n\n\tfcc = f2fs_kzalloc(sbi, sizeof(struct flush_cmd_control), GFP_KERNEL);\n\tif (!fcc)\n\t\treturn -ENOMEM;\n\tatomic_set(&fcc->issued_flush, 0);\n\tatomic_set(&fcc->queued_flush, 0);\n\tinit_waitqueue_head(&fcc->flush_wait_queue);\n\tinit_llist_head(&fcc->issue_list);\n\tSM_I(sbi)->fcc_info = fcc;\n\tif (!test_opt(sbi, FLUSH_MERGE))\n\t\treturn 0;\n\ninit_thread:\n\tfcc->f2fs_issue_flush = kthread_run(issue_flush_thread, sbi,\n\t\t\t\t\"f2fs_flush-%u:%u\", MAJOR(dev), MINOR(dev));\n\tif (IS_ERR(fcc->f2fs_issue_flush)) {\n\t\tint err = PTR_ERR(fcc->f2fs_issue_flush);\n\n\t\tfcc->f2fs_issue_flush = NULL;\n\t\treturn err;\n\t}\n\n\treturn 0;\n}\n\nvoid f2fs_destroy_flush_cmd_control(struct f2fs_sb_info *sbi, bool free)\n{\n\tstruct flush_cmd_control *fcc = SM_I(sbi)->fcc_info;\n\n\tif (fcc && fcc->f2fs_issue_flush) {\n\t\tstruct task_struct *flush_thread = fcc->f2fs_issue_flush;\n\n\t\tfcc->f2fs_issue_flush = NULL;\n\t\tkthread_stop(flush_thread);\n\t}\n\tif (free) {\n\t\tkfree(fcc);\n\t\tSM_I(sbi)->fcc_info = NULL;\n\t}\n}\n\nint f2fs_flush_device_cache(struct f2fs_sb_info *sbi)\n{\n\tint ret = 0, i;\n\n\tif (!f2fs_is_multi_device(sbi))\n\t\treturn 0;\n\n\tif (test_opt(sbi, NOBARRIER))\n\t\treturn 0;\n\n\tfor (i = 1; i < sbi->s_ndevs; i++) {\n\t\tint count = DEFAULT_RETRY_IO_COUNT;\n\n\t\tif (!f2fs_test_bit(i, (char *)&sbi->dirty_device))\n\t\t\tcontinue;\n\n\t\tdo {\n\t\t\tret = __submit_flush_wait(sbi, FDEV(i).bdev);\n\t\t\tif (ret)\n\t\t\t\tf2fs_io_schedule_timeout(DEFAULT_IO_TIMEOUT);\n\t\t} while (ret && --count);\n\n\t\tif (ret) {\n\t\t\tf2fs_stop_checkpoint(sbi, false,\n\t\t\t\t\tSTOP_CP_REASON_FLUSH_FAIL);\n\t\t\tbreak;\n\t\t}\n\n\t\tspin_lock(&sbi->dev_lock);\n\t\tf2fs_clear_bit(i, (char *)&sbi->dirty_device);\n\t\tspin_unlock(&sbi->dev_lock);\n\t}\n\n\treturn ret;\n}\n\nstatic void __locate_dirty_segment(struct f2fs_sb_info *sbi, unsigned int segno,\n\t\tenum dirty_type dirty_type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\n\t \n\tif (IS_CURSEG(sbi, segno))\n\t\treturn;\n\n\tif (!test_and_set_bit(segno, dirty_i->dirty_segmap[dirty_type]))\n\t\tdirty_i->nr_dirty[dirty_type]++;\n\n\tif (dirty_type == DIRTY) {\n\t\tstruct seg_entry *sentry = get_seg_entry(sbi, segno);\n\t\tenum dirty_type t = sentry->type;\n\n\t\tif (unlikely(t >= DIRTY)) {\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\treturn;\n\t\t}\n\t\tif (!test_and_set_bit(segno, dirty_i->dirty_segmap[t]))\n\t\t\tdirty_i->nr_dirty[t]++;\n\n\t\tif (__is_large_section(sbi)) {\n\t\t\tunsigned int secno = GET_SEC_FROM_SEG(sbi, segno);\n\t\t\tblock_t valid_blocks =\n\t\t\t\tget_valid_blocks(sbi, segno, true);\n\n\t\t\tf2fs_bug_on(sbi, unlikely(!valid_blocks ||\n\t\t\t\t\tvalid_blocks == CAP_BLKS_PER_SEC(sbi)));\n\n\t\t\tif (!IS_CURSEC(sbi, secno))\n\t\t\t\tset_bit(secno, dirty_i->dirty_secmap);\n\t\t}\n\t}\n}\n\nstatic void __remove_dirty_segment(struct f2fs_sb_info *sbi, unsigned int segno,\n\t\tenum dirty_type dirty_type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tblock_t valid_blocks;\n\n\tif (test_and_clear_bit(segno, dirty_i->dirty_segmap[dirty_type]))\n\t\tdirty_i->nr_dirty[dirty_type]--;\n\n\tif (dirty_type == DIRTY) {\n\t\tstruct seg_entry *sentry = get_seg_entry(sbi, segno);\n\t\tenum dirty_type t = sentry->type;\n\n\t\tif (test_and_clear_bit(segno, dirty_i->dirty_segmap[t]))\n\t\t\tdirty_i->nr_dirty[t]--;\n\n\t\tvalid_blocks = get_valid_blocks(sbi, segno, true);\n\t\tif (valid_blocks == 0) {\n\t\t\tclear_bit(GET_SEC_FROM_SEG(sbi, segno),\n\t\t\t\t\t\tdirty_i->victim_secmap);\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\t\tclear_bit(segno, SIT_I(sbi)->invalid_segmap);\n#endif\n\t\t}\n\t\tif (__is_large_section(sbi)) {\n\t\t\tunsigned int secno = GET_SEC_FROM_SEG(sbi, segno);\n\n\t\t\tif (!valid_blocks ||\n\t\t\t\t\tvalid_blocks == CAP_BLKS_PER_SEC(sbi)) {\n\t\t\t\tclear_bit(secno, dirty_i->dirty_secmap);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tif (!IS_CURSEC(sbi, secno))\n\t\t\t\tset_bit(secno, dirty_i->dirty_secmap);\n\t\t}\n\t}\n}\n\n \nstatic void locate_dirty_segment(struct f2fs_sb_info *sbi, unsigned int segno)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned short valid_blocks, ckpt_valid_blocks;\n\tunsigned int usable_blocks;\n\n\tif (segno == NULL_SEGNO || IS_CURSEG(sbi, segno))\n\t\treturn;\n\n\tusable_blocks = f2fs_usable_blks_in_seg(sbi, segno);\n\tmutex_lock(&dirty_i->seglist_lock);\n\n\tvalid_blocks = get_valid_blocks(sbi, segno, false);\n\tckpt_valid_blocks = get_ckpt_valid_blocks(sbi, segno, false);\n\n\tif (valid_blocks == 0 && (!is_sbi_flag_set(sbi, SBI_CP_DISABLED) ||\n\t\tckpt_valid_blocks == usable_blocks)) {\n\t\t__locate_dirty_segment(sbi, segno, PRE);\n\t\t__remove_dirty_segment(sbi, segno, DIRTY);\n\t} else if (valid_blocks < usable_blocks) {\n\t\t__locate_dirty_segment(sbi, segno, DIRTY);\n\t} else {\n\t\t \n\t\t__remove_dirty_segment(sbi, segno, DIRTY);\n\t}\n\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\n \nvoid f2fs_dirty_to_prefree(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned int segno;\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\tfor_each_set_bit(segno, dirty_i->dirty_segmap[DIRTY], MAIN_SEGS(sbi)) {\n\t\tif (get_valid_blocks(sbi, segno, false))\n\t\t\tcontinue;\n\t\tif (IS_CURSEG(sbi, segno))\n\t\t\tcontinue;\n\t\t__locate_dirty_segment(sbi, segno, PRE);\n\t\t__remove_dirty_segment(sbi, segno, DIRTY);\n\t}\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\nblock_t f2fs_get_unusable_blocks(struct f2fs_sb_info *sbi)\n{\n\tint ovp_hole_segs =\n\t\t(overprovision_segments(sbi) - reserved_segments(sbi));\n\tblock_t ovp_holes = ovp_hole_segs << sbi->log_blocks_per_seg;\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tblock_t holes[2] = {0, 0};\t \n\tblock_t unusable;\n\tstruct seg_entry *se;\n\tunsigned int segno;\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\tfor_each_set_bit(segno, dirty_i->dirty_segmap[DIRTY], MAIN_SEGS(sbi)) {\n\t\tse = get_seg_entry(sbi, segno);\n\t\tif (IS_NODESEG(se->type))\n\t\t\tholes[NODE] += f2fs_usable_blks_in_seg(sbi, segno) -\n\t\t\t\t\t\t\tse->valid_blocks;\n\t\telse\n\t\t\tholes[DATA] += f2fs_usable_blks_in_seg(sbi, segno) -\n\t\t\t\t\t\t\tse->valid_blocks;\n\t}\n\tmutex_unlock(&dirty_i->seglist_lock);\n\n\tunusable = max(holes[DATA], holes[NODE]);\n\tif (unusable > ovp_holes)\n\t\treturn unusable - ovp_holes;\n\treturn 0;\n}\n\nint f2fs_disable_cp_again(struct f2fs_sb_info *sbi, block_t unusable)\n{\n\tint ovp_hole_segs =\n\t\t(overprovision_segments(sbi) - reserved_segments(sbi));\n\tif (unusable > F2FS_OPTION(sbi).unusable_cap)\n\t\treturn -EAGAIN;\n\tif (is_sbi_flag_set(sbi, SBI_CP_DISABLED_QUICK) &&\n\t\tdirty_segments(sbi) > ovp_hole_segs)\n\t\treturn -EAGAIN;\n\treturn 0;\n}\n\n \nstatic unsigned int get_free_segment(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned int segno = 0;\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\tfor_each_set_bit(segno, dirty_i->dirty_segmap[DIRTY], MAIN_SEGS(sbi)) {\n\t\tif (get_valid_blocks(sbi, segno, false))\n\t\t\tcontinue;\n\t\tif (get_ckpt_valid_blocks(sbi, segno, false))\n\t\t\tcontinue;\n\t\tmutex_unlock(&dirty_i->seglist_lock);\n\t\treturn segno;\n\t}\n\tmutex_unlock(&dirty_i->seglist_lock);\n\treturn NULL_SEGNO;\n}\n\nstatic struct discard_cmd *__create_discard_cmd(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t lstart,\n\t\tblock_t start, block_t len)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *pend_list;\n\tstruct discard_cmd *dc;\n\n\tf2fs_bug_on(sbi, !len);\n\n\tpend_list = &dcc->pend_list[plist_idx(len)];\n\n\tdc = f2fs_kmem_cache_alloc(discard_cmd_slab, GFP_NOFS, true, NULL);\n\tINIT_LIST_HEAD(&dc->list);\n\tdc->bdev = bdev;\n\tdc->di.lstart = lstart;\n\tdc->di.start = start;\n\tdc->di.len = len;\n\tdc->ref = 0;\n\tdc->state = D_PREP;\n\tdc->queued = 0;\n\tdc->error = 0;\n\tinit_completion(&dc->wait);\n\tlist_add_tail(&dc->list, pend_list);\n\tspin_lock_init(&dc->lock);\n\tdc->bio_ref = 0;\n\tatomic_inc(&dcc->discard_cmd_cnt);\n\tdcc->undiscard_blks += len;\n\n\treturn dc;\n}\n\nstatic bool f2fs_check_discard_tree(struct f2fs_sb_info *sbi)\n{\n#ifdef CONFIG_F2FS_CHECK_FS\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct rb_node *cur = rb_first_cached(&dcc->root), *next;\n\tstruct discard_cmd *cur_dc, *next_dc;\n\n\twhile (cur) {\n\t\tnext = rb_next(cur);\n\t\tif (!next)\n\t\t\treturn true;\n\n\t\tcur_dc = rb_entry(cur, struct discard_cmd, rb_node);\n\t\tnext_dc = rb_entry(next, struct discard_cmd, rb_node);\n\n\t\tif (cur_dc->di.lstart + cur_dc->di.len > next_dc->di.lstart) {\n\t\t\tf2fs_info(sbi, \"broken discard_rbtree, \"\n\t\t\t\t\"cur(%u, %u) next(%u, %u)\",\n\t\t\t\tcur_dc->di.lstart, cur_dc->di.len,\n\t\t\t\tnext_dc->di.lstart, next_dc->di.len);\n\t\t\treturn false;\n\t\t}\n\t\tcur = next;\n\t}\n#endif\n\treturn true;\n}\n\nstatic struct discard_cmd *__lookup_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tblock_t blkaddr)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct rb_node *node = dcc->root.rb_root.rb_node;\n\tstruct discard_cmd *dc;\n\n\twhile (node) {\n\t\tdc = rb_entry(node, struct discard_cmd, rb_node);\n\n\t\tif (blkaddr < dc->di.lstart)\n\t\t\tnode = node->rb_left;\n\t\telse if (blkaddr >= dc->di.lstart + dc->di.len)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn dc;\n\t}\n\treturn NULL;\n}\n\nstatic struct discard_cmd *__lookup_discard_cmd_ret(struct rb_root_cached *root,\n\t\t\t\tblock_t blkaddr,\n\t\t\t\tstruct discard_cmd **prev_entry,\n\t\t\t\tstruct discard_cmd **next_entry,\n\t\t\t\tstruct rb_node ***insert_p,\n\t\t\t\tstruct rb_node **insert_parent)\n{\n\tstruct rb_node **pnode = &root->rb_root.rb_node;\n\tstruct rb_node *parent = NULL, *tmp_node;\n\tstruct discard_cmd *dc;\n\n\t*insert_p = NULL;\n\t*insert_parent = NULL;\n\t*prev_entry = NULL;\n\t*next_entry = NULL;\n\n\tif (RB_EMPTY_ROOT(&root->rb_root))\n\t\treturn NULL;\n\n\twhile (*pnode) {\n\t\tparent = *pnode;\n\t\tdc = rb_entry(*pnode, struct discard_cmd, rb_node);\n\n\t\tif (blkaddr < dc->di.lstart)\n\t\t\tpnode = &(*pnode)->rb_left;\n\t\telse if (blkaddr >= dc->di.lstart + dc->di.len)\n\t\t\tpnode = &(*pnode)->rb_right;\n\t\telse\n\t\t\tgoto lookup_neighbors;\n\t}\n\n\t*insert_p = pnode;\n\t*insert_parent = parent;\n\n\tdc = rb_entry(parent, struct discard_cmd, rb_node);\n\ttmp_node = parent;\n\tif (parent && blkaddr > dc->di.lstart)\n\t\ttmp_node = rb_next(parent);\n\t*next_entry = rb_entry_safe(tmp_node, struct discard_cmd, rb_node);\n\n\ttmp_node = parent;\n\tif (parent && blkaddr < dc->di.lstart)\n\t\ttmp_node = rb_prev(parent);\n\t*prev_entry = rb_entry_safe(tmp_node, struct discard_cmd, rb_node);\n\treturn NULL;\n\nlookup_neighbors:\n\t \n\ttmp_node = rb_prev(&dc->rb_node);\n\t*prev_entry = rb_entry_safe(tmp_node, struct discard_cmd, rb_node);\n\n\t \n\ttmp_node = rb_next(&dc->rb_node);\n\t*next_entry = rb_entry_safe(tmp_node, struct discard_cmd, rb_node);\n\treturn dc;\n}\n\nstatic void __detach_discard_cmd(struct discard_cmd_control *dcc,\n\t\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tif (dc->state == D_DONE)\n\t\tatomic_sub(dc->queued, &dcc->queued_discard);\n\n\tlist_del(&dc->list);\n\trb_erase_cached(&dc->rb_node, &dcc->root);\n\tdcc->undiscard_blks -= dc->di.len;\n\n\tkmem_cache_free(discard_cmd_slab, dc);\n\n\tatomic_dec(&dcc->discard_cmd_cnt);\n}\n\nstatic void __remove_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tunsigned long flags;\n\n\ttrace_f2fs_remove_discard(dc->bdev, dc->di.start, dc->di.len);\n\n\tspin_lock_irqsave(&dc->lock, flags);\n\tif (dc->bio_ref) {\n\t\tspin_unlock_irqrestore(&dc->lock, flags);\n\t\treturn;\n\t}\n\tspin_unlock_irqrestore(&dc->lock, flags);\n\n\tf2fs_bug_on(sbi, dc->ref);\n\n\tif (dc->error == -EOPNOTSUPP)\n\t\tdc->error = 0;\n\n\tif (dc->error)\n\t\tprintk_ratelimited(\n\t\t\t\"%sF2FS-fs (%s): Issue discard(%u, %u, %u) failed, ret: %d\",\n\t\t\tKERN_INFO, sbi->sb->s_id,\n\t\t\tdc->di.lstart, dc->di.start, dc->di.len, dc->error);\n\t__detach_discard_cmd(dcc, dc);\n}\n\nstatic void f2fs_submit_discard_endio(struct bio *bio)\n{\n\tstruct discard_cmd *dc = (struct discard_cmd *)bio->bi_private;\n\tunsigned long flags;\n\n\tspin_lock_irqsave(&dc->lock, flags);\n\tif (!dc->error)\n\t\tdc->error = blk_status_to_errno(bio->bi_status);\n\tdc->bio_ref--;\n\tif (!dc->bio_ref && dc->state == D_SUBMIT) {\n\t\tdc->state = D_DONE;\n\t\tcomplete_all(&dc->wait);\n\t}\n\tspin_unlock_irqrestore(&dc->lock, flags);\n\tbio_put(bio);\n}\n\nstatic void __check_sit_bitmap(struct f2fs_sb_info *sbi,\n\t\t\t\tblock_t start, block_t end)\n{\n#ifdef CONFIG_F2FS_CHECK_FS\n\tstruct seg_entry *sentry;\n\tunsigned int segno;\n\tblock_t blk = start;\n\tunsigned long offset, size, max_blocks = sbi->blocks_per_seg;\n\tunsigned long *map;\n\n\twhile (blk < end) {\n\t\tsegno = GET_SEGNO(sbi, blk);\n\t\tsentry = get_seg_entry(sbi, segno);\n\t\toffset = GET_BLKOFF_FROM_SEG0(sbi, blk);\n\n\t\tif (end < START_BLOCK(sbi, segno + 1))\n\t\t\tsize = GET_BLKOFF_FROM_SEG0(sbi, end);\n\t\telse\n\t\t\tsize = max_blocks;\n\t\tmap = (unsigned long *)(sentry->cur_valid_map);\n\t\toffset = __find_rev_next_bit(map, size, offset);\n\t\tf2fs_bug_on(sbi, offset != size);\n\t\tblk = START_BLOCK(sbi, segno + 1);\n\t}\n#endif\n}\n\nstatic void __init_discard_policy(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct discard_policy *dpolicy,\n\t\t\t\tint discard_type, unsigned int granularity)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\n\t \n\tdpolicy->type = discard_type;\n\tdpolicy->sync = true;\n\tdpolicy->ordered = false;\n\tdpolicy->granularity = granularity;\n\n\tdpolicy->max_requests = dcc->max_discard_request;\n\tdpolicy->io_aware_gran = dcc->discard_io_aware_gran;\n\tdpolicy->timeout = false;\n\n\tif (discard_type == DPOLICY_BG) {\n\t\tdpolicy->min_interval = dcc->min_discard_issue_time;\n\t\tdpolicy->mid_interval = dcc->mid_discard_issue_time;\n\t\tdpolicy->max_interval = dcc->max_discard_issue_time;\n\t\tdpolicy->io_aware = true;\n\t\tdpolicy->sync = false;\n\t\tdpolicy->ordered = true;\n\t\tif (utilization(sbi) > dcc->discard_urgent_util) {\n\t\t\tdpolicy->granularity = MIN_DISCARD_GRANULARITY;\n\t\t\tif (atomic_read(&dcc->discard_cmd_cnt))\n\t\t\t\tdpolicy->max_interval =\n\t\t\t\t\tdcc->min_discard_issue_time;\n\t\t}\n\t} else if (discard_type == DPOLICY_FORCE) {\n\t\tdpolicy->min_interval = dcc->min_discard_issue_time;\n\t\tdpolicy->mid_interval = dcc->mid_discard_issue_time;\n\t\tdpolicy->max_interval = dcc->max_discard_issue_time;\n\t\tdpolicy->io_aware = false;\n\t} else if (discard_type == DPOLICY_FSTRIM) {\n\t\tdpolicy->io_aware = false;\n\t} else if (discard_type == DPOLICY_UMOUNT) {\n\t\tdpolicy->io_aware = false;\n\t\t \n\t\tdpolicy->granularity = MIN_DISCARD_GRANULARITY;\n\t\tdpolicy->timeout = true;\n\t}\n}\n\nstatic void __update_discard_tree_range(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev, block_t lstart,\n\t\t\t\tblock_t start, block_t len);\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic void __submit_zone_reset_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\t   struct discard_cmd *dc, blk_opf_t flag,\n\t\t\t\t   struct list_head *wait_list,\n\t\t\t\t   unsigned int *issued)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct block_device *bdev = dc->bdev;\n\tstruct bio *bio = bio_alloc(bdev, 0, REQ_OP_ZONE_RESET | flag, GFP_NOFS);\n\tunsigned long flags;\n\n\ttrace_f2fs_issue_reset_zone(bdev, dc->di.start);\n\n\tspin_lock_irqsave(&dc->lock, flags);\n\tdc->state = D_SUBMIT;\n\tdc->bio_ref++;\n\tspin_unlock_irqrestore(&dc->lock, flags);\n\n\tif (issued)\n\t\t(*issued)++;\n\n\tatomic_inc(&dcc->queued_discard);\n\tdc->queued++;\n\tlist_move_tail(&dc->list, wait_list);\n\n\t \n\t__check_sit_bitmap(sbi, dc->di.lstart, dc->di.lstart + dc->di.len);\n\n\tbio->bi_iter.bi_sector = SECTOR_FROM_BLOCK(dc->di.start);\n\tbio->bi_private = dc;\n\tbio->bi_end_io = f2fs_submit_discard_endio;\n\tsubmit_bio(bio);\n\n\tatomic_inc(&dcc->issued_discard);\n\tf2fs_update_iostat(sbi, NULL, FS_ZONE_RESET_IO, dc->di.len * F2FS_BLKSIZE);\n}\n#endif\n\n \nstatic int __submit_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct discard_policy *dpolicy,\n\t\t\t\tstruct discard_cmd *dc, int *issued)\n{\n\tstruct block_device *bdev = dc->bdev;\n\tunsigned int max_discard_blocks =\n\t\t\tSECTOR_TO_BLOCK(bdev_max_discard_sectors(bdev));\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *wait_list = (dpolicy->type == DPOLICY_FSTRIM) ?\n\t\t\t\t\t&(dcc->fstrim_list) : &(dcc->wait_list);\n\tblk_opf_t flag = dpolicy->sync ? REQ_SYNC : 0;\n\tblock_t lstart, start, len, total_len;\n\tint err = 0;\n\n\tif (dc->state != D_PREP)\n\t\treturn 0;\n\n\tif (is_sbi_flag_set(sbi, SBI_NEED_FSCK))\n\t\treturn 0;\n\n#ifdef CONFIG_BLK_DEV_ZONED\n\tif (f2fs_sb_has_blkzoned(sbi) && bdev_is_zoned(bdev)) {\n\t\tint devi = f2fs_bdev_index(sbi, bdev);\n\n\t\tif (devi < 0)\n\t\t\treturn -EINVAL;\n\n\t\tif (f2fs_blkz_is_seq(sbi, devi, dc->di.start)) {\n\t\t\t__submit_zone_reset_cmd(sbi, dc, flag,\n\t\t\t\t\t\twait_list, issued);\n\t\t\treturn 0;\n\t\t}\n\t}\n#endif\n\n\ttrace_f2fs_issue_discard(bdev, dc->di.start, dc->di.len);\n\n\tlstart = dc->di.lstart;\n\tstart = dc->di.start;\n\tlen = dc->di.len;\n\ttotal_len = len;\n\n\tdc->di.len = 0;\n\n\twhile (total_len && *issued < dpolicy->max_requests && !err) {\n\t\tstruct bio *bio = NULL;\n\t\tunsigned long flags;\n\t\tbool last = true;\n\n\t\tif (len > max_discard_blocks) {\n\t\t\tlen = max_discard_blocks;\n\t\t\tlast = false;\n\t\t}\n\n\t\t(*issued)++;\n\t\tif (*issued == dpolicy->max_requests)\n\t\t\tlast = true;\n\n\t\tdc->di.len += len;\n\n\t\tif (time_to_inject(sbi, FAULT_DISCARD)) {\n\t\t\terr = -EIO;\n\t\t} else {\n\t\t\terr = __blkdev_issue_discard(bdev,\n\t\t\t\t\tSECTOR_FROM_BLOCK(start),\n\t\t\t\t\tSECTOR_FROM_BLOCK(len),\n\t\t\t\t\tGFP_NOFS, &bio);\n\t\t}\n\t\tif (err) {\n\t\t\tspin_lock_irqsave(&dc->lock, flags);\n\t\t\tif (dc->state == D_PARTIAL)\n\t\t\t\tdc->state = D_SUBMIT;\n\t\t\tspin_unlock_irqrestore(&dc->lock, flags);\n\n\t\t\tbreak;\n\t\t}\n\n\t\tf2fs_bug_on(sbi, !bio);\n\n\t\t \n\t\tspin_lock_irqsave(&dc->lock, flags);\n\t\tif (last)\n\t\t\tdc->state = D_SUBMIT;\n\t\telse\n\t\t\tdc->state = D_PARTIAL;\n\t\tdc->bio_ref++;\n\t\tspin_unlock_irqrestore(&dc->lock, flags);\n\n\t\tatomic_inc(&dcc->queued_discard);\n\t\tdc->queued++;\n\t\tlist_move_tail(&dc->list, wait_list);\n\n\t\t \n\t\t__check_sit_bitmap(sbi, lstart, lstart + len);\n\n\t\tbio->bi_private = dc;\n\t\tbio->bi_end_io = f2fs_submit_discard_endio;\n\t\tbio->bi_opf |= flag;\n\t\tsubmit_bio(bio);\n\n\t\tatomic_inc(&dcc->issued_discard);\n\n\t\tf2fs_update_iostat(sbi, NULL, FS_DISCARD_IO, len * F2FS_BLKSIZE);\n\n\t\tlstart += len;\n\t\tstart += len;\n\t\ttotal_len -= len;\n\t\tlen = total_len;\n\t}\n\n\tif (!err && len) {\n\t\tdcc->undiscard_blks -= len;\n\t\t__update_discard_tree_range(sbi, bdev, lstart, start, len);\n\t}\n\treturn err;\n}\n\nstatic void __insert_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev, block_t lstart,\n\t\t\t\tblock_t start, block_t len)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct rb_node **p = &dcc->root.rb_root.rb_node;\n\tstruct rb_node *parent = NULL;\n\tstruct discard_cmd *dc;\n\tbool leftmost = true;\n\n\t \n\twhile (*p) {\n\t\tparent = *p;\n\t\tdc = rb_entry(parent, struct discard_cmd, rb_node);\n\n\t\tif (lstart < dc->di.lstart) {\n\t\t\tp = &(*p)->rb_left;\n\t\t} else if (lstart >= dc->di.lstart + dc->di.len) {\n\t\t\tp = &(*p)->rb_right;\n\t\t\tleftmost = false;\n\t\t} else {\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t}\n\t}\n\n\tdc = __create_discard_cmd(sbi, bdev, lstart, start, len);\n\n\trb_link_node(&dc->rb_node, parent, p);\n\trb_insert_color_cached(&dc->rb_node, &dcc->root, leftmost);\n}\n\nstatic void __relocate_discard_cmd(struct discard_cmd_control *dcc,\n\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tlist_move_tail(&dc->list, &dcc->pend_list[plist_idx(dc->di.len)]);\n}\n\nstatic void __punch_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct discard_cmd *dc, block_t blkaddr)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_info di = dc->di;\n\tbool modified = false;\n\n\tif (dc->state == D_DONE || dc->di.len == 1) {\n\t\t__remove_discard_cmd(sbi, dc);\n\t\treturn;\n\t}\n\n\tdcc->undiscard_blks -= di.len;\n\n\tif (blkaddr > di.lstart) {\n\t\tdc->di.len = blkaddr - dc->di.lstart;\n\t\tdcc->undiscard_blks += dc->di.len;\n\t\t__relocate_discard_cmd(dcc, dc);\n\t\tmodified = true;\n\t}\n\n\tif (blkaddr < di.lstart + di.len - 1) {\n\t\tif (modified) {\n\t\t\t__insert_discard_cmd(sbi, dc->bdev, blkaddr + 1,\n\t\t\t\t\tdi.start + blkaddr + 1 - di.lstart,\n\t\t\t\t\tdi.lstart + di.len - 1 - blkaddr);\n\t\t} else {\n\t\t\tdc->di.lstart++;\n\t\t\tdc->di.len--;\n\t\t\tdc->di.start++;\n\t\t\tdcc->undiscard_blks += dc->di.len;\n\t\t\t__relocate_discard_cmd(dcc, dc);\n\t\t}\n\t}\n}\n\nstatic void __update_discard_tree_range(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct block_device *bdev, block_t lstart,\n\t\t\t\tblock_t start, block_t len)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_cmd *prev_dc = NULL, *next_dc = NULL;\n\tstruct discard_cmd *dc;\n\tstruct discard_info di = {0};\n\tstruct rb_node **insert_p = NULL, *insert_parent = NULL;\n\tunsigned int max_discard_blocks =\n\t\t\tSECTOR_TO_BLOCK(bdev_max_discard_sectors(bdev));\n\tblock_t end = lstart + len;\n\n\tdc = __lookup_discard_cmd_ret(&dcc->root, lstart,\n\t\t\t\t&prev_dc, &next_dc, &insert_p, &insert_parent);\n\tif (dc)\n\t\tprev_dc = dc;\n\n\tif (!prev_dc) {\n\t\tdi.lstart = lstart;\n\t\tdi.len = next_dc ? next_dc->di.lstart - lstart : len;\n\t\tdi.len = min(di.len, len);\n\t\tdi.start = start;\n\t}\n\n\twhile (1) {\n\t\tstruct rb_node *node;\n\t\tbool merged = false;\n\t\tstruct discard_cmd *tdc = NULL;\n\n\t\tif (prev_dc) {\n\t\t\tdi.lstart = prev_dc->di.lstart + prev_dc->di.len;\n\t\t\tif (di.lstart < lstart)\n\t\t\t\tdi.lstart = lstart;\n\t\t\tif (di.lstart >= end)\n\t\t\t\tbreak;\n\n\t\t\tif (!next_dc || next_dc->di.lstart > end)\n\t\t\t\tdi.len = end - di.lstart;\n\t\t\telse\n\t\t\t\tdi.len = next_dc->di.lstart - di.lstart;\n\t\t\tdi.start = start + di.lstart - lstart;\n\t\t}\n\n\t\tif (!di.len)\n\t\t\tgoto next;\n\n\t\tif (prev_dc && prev_dc->state == D_PREP &&\n\t\t\tprev_dc->bdev == bdev &&\n\t\t\t__is_discard_back_mergeable(&di, &prev_dc->di,\n\t\t\t\t\t\t\tmax_discard_blocks)) {\n\t\t\tprev_dc->di.len += di.len;\n\t\t\tdcc->undiscard_blks += di.len;\n\t\t\t__relocate_discard_cmd(dcc, prev_dc);\n\t\t\tdi = prev_dc->di;\n\t\t\ttdc = prev_dc;\n\t\t\tmerged = true;\n\t\t}\n\n\t\tif (next_dc && next_dc->state == D_PREP &&\n\t\t\tnext_dc->bdev == bdev &&\n\t\t\t__is_discard_front_mergeable(&di, &next_dc->di,\n\t\t\t\t\t\t\tmax_discard_blocks)) {\n\t\t\tnext_dc->di.lstart = di.lstart;\n\t\t\tnext_dc->di.len += di.len;\n\t\t\tnext_dc->di.start = di.start;\n\t\t\tdcc->undiscard_blks += di.len;\n\t\t\t__relocate_discard_cmd(dcc, next_dc);\n\t\t\tif (tdc)\n\t\t\t\t__remove_discard_cmd(sbi, tdc);\n\t\t\tmerged = true;\n\t\t}\n\n\t\tif (!merged)\n\t\t\t__insert_discard_cmd(sbi, bdev,\n\t\t\t\t\t\tdi.lstart, di.start, di.len);\n next:\n\t\tprev_dc = next_dc;\n\t\tif (!prev_dc)\n\t\t\tbreak;\n\n\t\tnode = rb_next(&prev_dc->rb_node);\n\t\tnext_dc = rb_entry_safe(node, struct discard_cmd, rb_node);\n\t}\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic void __queue_zone_reset_cmd(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t blkstart, block_t lblkstart,\n\t\tblock_t blklen)\n{\n\ttrace_f2fs_queue_reset_zone(bdev, blkstart);\n\n\tmutex_lock(&SM_I(sbi)->dcc_info->cmd_lock);\n\t__insert_discard_cmd(sbi, bdev, lblkstart, blkstart, blklen);\n\tmutex_unlock(&SM_I(sbi)->dcc_info->cmd_lock);\n}\n#endif\n\nstatic void __queue_discard_cmd(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t blkstart, block_t blklen)\n{\n\tblock_t lblkstart = blkstart;\n\n\tif (!f2fs_bdev_support_discard(bdev))\n\t\treturn;\n\n\ttrace_f2fs_queue_discard(bdev, blkstart, blklen);\n\n\tif (f2fs_is_multi_device(sbi)) {\n\t\tint devi = f2fs_target_device_index(sbi, blkstart);\n\n\t\tblkstart -= FDEV(devi).start_blk;\n\t}\n\tmutex_lock(&SM_I(sbi)->dcc_info->cmd_lock);\n\t__update_discard_tree_range(sbi, bdev, lblkstart, blkstart, blklen);\n\tmutex_unlock(&SM_I(sbi)->dcc_info->cmd_lock);\n}\n\nstatic void __issue_discard_cmd_orderly(struct f2fs_sb_info *sbi,\n\t\tstruct discard_policy *dpolicy, int *issued)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_cmd *prev_dc = NULL, *next_dc = NULL;\n\tstruct rb_node **insert_p = NULL, *insert_parent = NULL;\n\tstruct discard_cmd *dc;\n\tstruct blk_plug plug;\n\tbool io_interrupted = false;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tdc = __lookup_discard_cmd_ret(&dcc->root, dcc->next_pos,\n\t\t\t\t&prev_dc, &next_dc, &insert_p, &insert_parent);\n\tif (!dc)\n\t\tdc = next_dc;\n\n\tblk_start_plug(&plug);\n\n\twhile (dc) {\n\t\tstruct rb_node *node;\n\t\tint err = 0;\n\n\t\tif (dc->state != D_PREP)\n\t\t\tgoto next;\n\n\t\tif (dpolicy->io_aware && !is_idle(sbi, DISCARD_TIME)) {\n\t\t\tio_interrupted = true;\n\t\t\tbreak;\n\t\t}\n\n\t\tdcc->next_pos = dc->di.lstart + dc->di.len;\n\t\terr = __submit_discard_cmd(sbi, dpolicy, dc, issued);\n\n\t\tif (*issued >= dpolicy->max_requests)\n\t\t\tbreak;\nnext:\n\t\tnode = rb_next(&dc->rb_node);\n\t\tif (err)\n\t\t\t__remove_discard_cmd(sbi, dc);\n\t\tdc = rb_entry_safe(node, struct discard_cmd, rb_node);\n\t}\n\n\tblk_finish_plug(&plug);\n\n\tif (!dc)\n\t\tdcc->next_pos = 0;\n\n\tmutex_unlock(&dcc->cmd_lock);\n\n\tif (!(*issued) && io_interrupted)\n\t\t*issued = -1;\n}\nstatic unsigned int __wait_all_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct discard_policy *dpolicy);\n\nstatic int __issue_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct discard_policy *dpolicy)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *pend_list;\n\tstruct discard_cmd *dc, *tmp;\n\tstruct blk_plug plug;\n\tint i, issued;\n\tbool io_interrupted = false;\n\n\tif (dpolicy->timeout)\n\t\tf2fs_update_time(sbi, UMOUNT_DISCARD_TIMEOUT);\n\nretry:\n\tissued = 0;\n\tfor (i = MAX_PLIST_NUM - 1; i >= 0; i--) {\n\t\tif (dpolicy->timeout &&\n\t\t\t\tf2fs_time_over(sbi, UMOUNT_DISCARD_TIMEOUT))\n\t\t\tbreak;\n\n\t\tif (i + 1 < dpolicy->granularity)\n\t\t\tbreak;\n\n\t\tif (i + 1 < dcc->max_ordered_discard && dpolicy->ordered) {\n\t\t\t__issue_discard_cmd_orderly(sbi, dpolicy, &issued);\n\t\t\treturn issued;\n\t\t}\n\n\t\tpend_list = &dcc->pend_list[i];\n\n\t\tmutex_lock(&dcc->cmd_lock);\n\t\tif (list_empty(pend_list))\n\t\t\tgoto next;\n\t\tif (unlikely(dcc->rbtree_check))\n\t\t\tf2fs_bug_on(sbi, !f2fs_check_discard_tree(sbi));\n\t\tblk_start_plug(&plug);\n\t\tlist_for_each_entry_safe(dc, tmp, pend_list, list) {\n\t\t\tf2fs_bug_on(sbi, dc->state != D_PREP);\n\n\t\t\tif (dpolicy->timeout &&\n\t\t\t\tf2fs_time_over(sbi, UMOUNT_DISCARD_TIMEOUT))\n\t\t\t\tbreak;\n\n\t\t\tif (dpolicy->io_aware && i < dpolicy->io_aware_gran &&\n\t\t\t\t\t\t!is_idle(sbi, DISCARD_TIME)) {\n\t\t\t\tio_interrupted = true;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t__submit_discard_cmd(sbi, dpolicy, dc, &issued);\n\n\t\t\tif (issued >= dpolicy->max_requests)\n\t\t\t\tbreak;\n\t\t}\n\t\tblk_finish_plug(&plug);\nnext:\n\t\tmutex_unlock(&dcc->cmd_lock);\n\n\t\tif (issued >= dpolicy->max_requests || io_interrupted)\n\t\t\tbreak;\n\t}\n\n\tif (dpolicy->type == DPOLICY_UMOUNT && issued) {\n\t\t__wait_all_discard_cmd(sbi, dpolicy);\n\t\tgoto retry;\n\t}\n\n\tif (!issued && io_interrupted)\n\t\tissued = -1;\n\n\treturn issued;\n}\n\nstatic bool __drop_discard_cmd(struct f2fs_sb_info *sbi)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *pend_list;\n\tstruct discard_cmd *dc, *tmp;\n\tint i;\n\tbool dropped = false;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tfor (i = MAX_PLIST_NUM - 1; i >= 0; i--) {\n\t\tpend_list = &dcc->pend_list[i];\n\t\tlist_for_each_entry_safe(dc, tmp, pend_list, list) {\n\t\t\tf2fs_bug_on(sbi, dc->state != D_PREP);\n\t\t\t__remove_discard_cmd(sbi, dc);\n\t\t\tdropped = true;\n\t\t}\n\t}\n\tmutex_unlock(&dcc->cmd_lock);\n\n\treturn dropped;\n}\n\nvoid f2fs_drop_discard_cmd(struct f2fs_sb_info *sbi)\n{\n\t__drop_discard_cmd(sbi);\n}\n\nstatic unsigned int __wait_one_discard_bio(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\t\tstruct discard_cmd *dc)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tunsigned int len = 0;\n\n\twait_for_completion_io(&dc->wait);\n\tmutex_lock(&dcc->cmd_lock);\n\tf2fs_bug_on(sbi, dc->state != D_DONE);\n\tdc->ref--;\n\tif (!dc->ref) {\n\t\tif (!dc->error)\n\t\t\tlen = dc->di.len;\n\t\t__remove_discard_cmd(sbi, dc);\n\t}\n\tmutex_unlock(&dcc->cmd_lock);\n\n\treturn len;\n}\n\nstatic unsigned int __wait_discard_cmd_range(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tstruct discard_policy *dpolicy,\n\t\t\t\t\t\tblock_t start, block_t end)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *wait_list = (dpolicy->type == DPOLICY_FSTRIM) ?\n\t\t\t\t\t&(dcc->fstrim_list) : &(dcc->wait_list);\n\tstruct discard_cmd *dc = NULL, *iter, *tmp;\n\tunsigned int trimmed = 0;\n\nnext:\n\tdc = NULL;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tlist_for_each_entry_safe(iter, tmp, wait_list, list) {\n\t\tif (iter->di.lstart + iter->di.len <= start ||\n\t\t\t\t\tend <= iter->di.lstart)\n\t\t\tcontinue;\n\t\tif (iter->di.len < dpolicy->granularity)\n\t\t\tcontinue;\n\t\tif (iter->state == D_DONE && !iter->ref) {\n\t\t\twait_for_completion_io(&iter->wait);\n\t\t\tif (!iter->error)\n\t\t\t\ttrimmed += iter->di.len;\n\t\t\t__remove_discard_cmd(sbi, iter);\n\t\t} else {\n\t\t\titer->ref++;\n\t\t\tdc = iter;\n\t\t\tbreak;\n\t\t}\n\t}\n\tmutex_unlock(&dcc->cmd_lock);\n\n\tif (dc) {\n\t\ttrimmed += __wait_one_discard_bio(sbi, dc);\n\t\tgoto next;\n\t}\n\n\treturn trimmed;\n}\n\nstatic unsigned int __wait_all_discard_cmd(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tstruct discard_policy *dpolicy)\n{\n\tstruct discard_policy dp;\n\tunsigned int discard_blks;\n\n\tif (dpolicy)\n\t\treturn __wait_discard_cmd_range(sbi, dpolicy, 0, UINT_MAX);\n\n\t \n\t__init_discard_policy(sbi, &dp, DPOLICY_FSTRIM, MIN_DISCARD_GRANULARITY);\n\tdiscard_blks = __wait_discard_cmd_range(sbi, &dp, 0, UINT_MAX);\n\t__init_discard_policy(sbi, &dp, DPOLICY_UMOUNT, MIN_DISCARD_GRANULARITY);\n\tdiscard_blks += __wait_discard_cmd_range(sbi, &dp, 0, UINT_MAX);\n\n\treturn discard_blks;\n}\n\n \nstatic void f2fs_wait_discard_bio(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_cmd *dc;\n\tbool need_wait = false;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tdc = __lookup_discard_cmd(sbi, blkaddr);\n#ifdef CONFIG_BLK_DEV_ZONED\n\tif (dc && f2fs_sb_has_blkzoned(sbi) && bdev_is_zoned(dc->bdev)) {\n\t\tint devi = f2fs_bdev_index(sbi, dc->bdev);\n\n\t\tif (devi < 0) {\n\t\t\tmutex_unlock(&dcc->cmd_lock);\n\t\t\treturn;\n\t\t}\n\n\t\tif (f2fs_blkz_is_seq(sbi, devi, dc->di.start)) {\n\t\t\t \n\t\t\tif (dc->state == D_PREP)\n\t\t\t\t__submit_zone_reset_cmd(sbi, dc, REQ_SYNC,\n\t\t\t\t\t\t\t&dcc->wait_list, NULL);\n\t\t\tdc->ref++;\n\t\t\tmutex_unlock(&dcc->cmd_lock);\n\t\t\t \n\t\t\t__wait_one_discard_bio(sbi, dc);\n\t\t\treturn;\n\t\t}\n\t}\n#endif\n\tif (dc) {\n\t\tif (dc->state == D_PREP) {\n\t\t\t__punch_discard_cmd(sbi, dc, blkaddr);\n\t\t} else {\n\t\t\tdc->ref++;\n\t\t\tneed_wait = true;\n\t\t}\n\t}\n\tmutex_unlock(&dcc->cmd_lock);\n\n\tif (need_wait)\n\t\t__wait_one_discard_bio(sbi, dc);\n}\n\nvoid f2fs_stop_discard_thread(struct f2fs_sb_info *sbi)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\n\tif (dcc && dcc->f2fs_issue_discard) {\n\t\tstruct task_struct *discard_thread = dcc->f2fs_issue_discard;\n\n\t\tdcc->f2fs_issue_discard = NULL;\n\t\tkthread_stop(discard_thread);\n\t}\n}\n\n \nbool f2fs_issue_discard_timeout(struct f2fs_sb_info *sbi)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_policy dpolicy;\n\tbool dropped;\n\n\tif (!atomic_read(&dcc->discard_cmd_cnt))\n\t\treturn true;\n\n\t__init_discard_policy(sbi, &dpolicy, DPOLICY_UMOUNT,\n\t\t\t\t\tdcc->discard_granularity);\n\t__issue_discard_cmd(sbi, &dpolicy);\n\tdropped = __drop_discard_cmd(sbi);\n\n\t \n\t__wait_all_discard_cmd(sbi, NULL);\n\n\tf2fs_bug_on(sbi, atomic_read(&dcc->discard_cmd_cnt));\n\treturn !dropped;\n}\n\nstatic int issue_discard_thread(void *data)\n{\n\tstruct f2fs_sb_info *sbi = data;\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\twait_queue_head_t *q = &dcc->discard_wait_queue;\n\tstruct discard_policy dpolicy;\n\tunsigned int wait_ms = dcc->min_discard_issue_time;\n\tint issued;\n\n\tset_freezable();\n\n\tdo {\n\t\twait_event_interruptible_timeout(*q,\n\t\t\t\tkthread_should_stop() || freezing(current) ||\n\t\t\t\tdcc->discard_wake,\n\t\t\t\tmsecs_to_jiffies(wait_ms));\n\n\t\tif (sbi->gc_mode == GC_URGENT_HIGH ||\n\t\t\t!f2fs_available_free_memory(sbi, DISCARD_CACHE))\n\t\t\t__init_discard_policy(sbi, &dpolicy, DPOLICY_FORCE,\n\t\t\t\t\t\tMIN_DISCARD_GRANULARITY);\n\t\telse\n\t\t\t__init_discard_policy(sbi, &dpolicy, DPOLICY_BG,\n\t\t\t\t\t\tdcc->discard_granularity);\n\n\t\tif (dcc->discard_wake)\n\t\t\tdcc->discard_wake = false;\n\n\t\t \n\t\tif (atomic_read(&dcc->queued_discard))\n\t\t\t__wait_all_discard_cmd(sbi, NULL);\n\n\t\tif (try_to_freeze())\n\t\t\tcontinue;\n\t\tif (f2fs_readonly(sbi->sb))\n\t\t\tcontinue;\n\t\tif (kthread_should_stop())\n\t\t\treturn 0;\n\t\tif (is_sbi_flag_set(sbi, SBI_NEED_FSCK) ||\n\t\t\t!atomic_read(&dcc->discard_cmd_cnt)) {\n\t\t\twait_ms = dpolicy.max_interval;\n\t\t\tcontinue;\n\t\t}\n\n\t\tsb_start_intwrite(sbi->sb);\n\n\t\tissued = __issue_discard_cmd(sbi, &dpolicy);\n\t\tif (issued > 0) {\n\t\t\t__wait_all_discard_cmd(sbi, &dpolicy);\n\t\t\twait_ms = dpolicy.min_interval;\n\t\t} else if (issued == -1) {\n\t\t\twait_ms = f2fs_time_to_wait(sbi, DISCARD_TIME);\n\t\t\tif (!wait_ms)\n\t\t\t\twait_ms = dpolicy.mid_interval;\n\t\t} else {\n\t\t\twait_ms = dpolicy.max_interval;\n\t\t}\n\t\tif (!atomic_read(&dcc->discard_cmd_cnt))\n\t\t\twait_ms = dpolicy.max_interval;\n\n\t\tsb_end_intwrite(sbi->sb);\n\n\t} while (!kthread_should_stop());\n\treturn 0;\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic int __f2fs_issue_discard_zone(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t blkstart, block_t blklen)\n{\n\tsector_t sector, nr_sects;\n\tblock_t lblkstart = blkstart;\n\tint devi = 0;\n\tu64 remainder = 0;\n\n\tif (f2fs_is_multi_device(sbi)) {\n\t\tdevi = f2fs_target_device_index(sbi, blkstart);\n\t\tif (blkstart < FDEV(devi).start_blk ||\n\t\t    blkstart > FDEV(devi).end_blk) {\n\t\t\tf2fs_err(sbi, \"Invalid block %x\", blkstart);\n\t\t\treturn -EIO;\n\t\t}\n\t\tblkstart -= FDEV(devi).start_blk;\n\t}\n\n\t \n\tif (f2fs_blkz_is_seq(sbi, devi, blkstart)) {\n\t\tsector = SECTOR_FROM_BLOCK(blkstart);\n\t\tnr_sects = SECTOR_FROM_BLOCK(blklen);\n\t\tdiv64_u64_rem(sector, bdev_zone_sectors(bdev), &remainder);\n\n\t\tif (remainder || nr_sects != bdev_zone_sectors(bdev)) {\n\t\t\tf2fs_err(sbi, \"(%d) %s: Unaligned zone reset attempted (block %x + %x)\",\n\t\t\t\t devi, sbi->s_ndevs ? FDEV(devi).path : \"\",\n\t\t\t\t blkstart, blklen);\n\t\t\treturn -EIO;\n\t\t}\n\n\t\tif (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING))) {\n\t\t\ttrace_f2fs_issue_reset_zone(bdev, blkstart);\n\t\t\treturn blkdev_zone_mgmt(bdev, REQ_OP_ZONE_RESET,\n\t\t\t\t\t\tsector, nr_sects, GFP_NOFS);\n\t\t}\n\n\t\t__queue_zone_reset_cmd(sbi, bdev, blkstart, lblkstart, blklen);\n\t\treturn 0;\n\t}\n\n\t \n\t__queue_discard_cmd(sbi, bdev, lblkstart, blklen);\n\treturn 0;\n}\n#endif\n\nstatic int __issue_discard_async(struct f2fs_sb_info *sbi,\n\t\tstruct block_device *bdev, block_t blkstart, block_t blklen)\n{\n#ifdef CONFIG_BLK_DEV_ZONED\n\tif (f2fs_sb_has_blkzoned(sbi) && bdev_is_zoned(bdev))\n\t\treturn __f2fs_issue_discard_zone(sbi, bdev, blkstart, blklen);\n#endif\n\t__queue_discard_cmd(sbi, bdev, blkstart, blklen);\n\treturn 0;\n}\n\nstatic int f2fs_issue_discard(struct f2fs_sb_info *sbi,\n\t\t\t\tblock_t blkstart, block_t blklen)\n{\n\tsector_t start = blkstart, len = 0;\n\tstruct block_device *bdev;\n\tstruct seg_entry *se;\n\tunsigned int offset;\n\tblock_t i;\n\tint err = 0;\n\n\tbdev = f2fs_target_device(sbi, blkstart, NULL);\n\n\tfor (i = blkstart; i < blkstart + blklen; i++, len++) {\n\t\tif (i != start) {\n\t\t\tstruct block_device *bdev2 =\n\t\t\t\tf2fs_target_device(sbi, i, NULL);\n\n\t\t\tif (bdev2 != bdev) {\n\t\t\t\terr = __issue_discard_async(sbi, bdev,\n\t\t\t\t\t\tstart, len);\n\t\t\t\tif (err)\n\t\t\t\t\treturn err;\n\t\t\t\tbdev = bdev2;\n\t\t\t\tstart = i;\n\t\t\t\tlen = 0;\n\t\t\t}\n\t\t}\n\n\t\tse = get_seg_entry(sbi, GET_SEGNO(sbi, i));\n\t\toffset = GET_BLKOFF_FROM_SEG0(sbi, i);\n\n\t\tif (f2fs_block_unit_discard(sbi) &&\n\t\t\t\t!f2fs_test_and_set_bit(offset, se->discard_map))\n\t\t\tsbi->discard_blks--;\n\t}\n\n\tif (len)\n\t\terr = __issue_discard_async(sbi, bdev, start, len);\n\treturn err;\n}\n\nstatic bool add_discard_addrs(struct f2fs_sb_info *sbi, struct cp_control *cpc,\n\t\t\t\t\t\t\tbool check_only)\n{\n\tint entries = SIT_VBLOCK_MAP_SIZE / sizeof(unsigned long);\n\tint max_blocks = sbi->blocks_per_seg;\n\tstruct seg_entry *se = get_seg_entry(sbi, cpc->trim_start);\n\tunsigned long *cur_map = (unsigned long *)se->cur_valid_map;\n\tunsigned long *ckpt_map = (unsigned long *)se->ckpt_valid_map;\n\tunsigned long *discard_map = (unsigned long *)se->discard_map;\n\tunsigned long *dmap = SIT_I(sbi)->tmp_map;\n\tunsigned int start = 0, end = -1;\n\tbool force = (cpc->reason & CP_DISCARD);\n\tstruct discard_entry *de = NULL;\n\tstruct list_head *head = &SM_I(sbi)->dcc_info->entry_list;\n\tint i;\n\n\tif (se->valid_blocks == max_blocks || !f2fs_hw_support_discard(sbi) ||\n\t\t\t!f2fs_block_unit_discard(sbi))\n\t\treturn false;\n\n\tif (!force) {\n\t\tif (!f2fs_realtime_discard_enable(sbi) || !se->valid_blocks ||\n\t\t\tSM_I(sbi)->dcc_info->nr_discards >=\n\t\t\t\tSM_I(sbi)->dcc_info->max_discards)\n\t\t\treturn false;\n\t}\n\n\t \n\tfor (i = 0; i < entries; i++)\n\t\tdmap[i] = force ? ~ckpt_map[i] & ~discard_map[i] :\n\t\t\t\t(cur_map[i] ^ ckpt_map[i]) & ckpt_map[i];\n\n\twhile (force || SM_I(sbi)->dcc_info->nr_discards <=\n\t\t\t\tSM_I(sbi)->dcc_info->max_discards) {\n\t\tstart = __find_rev_next_bit(dmap, max_blocks, end + 1);\n\t\tif (start >= max_blocks)\n\t\t\tbreak;\n\n\t\tend = __find_rev_next_zero_bit(dmap, max_blocks, start + 1);\n\t\tif (force && start && end != max_blocks\n\t\t\t\t\t&& (end - start) < cpc->trim_minlen)\n\t\t\tcontinue;\n\n\t\tif (check_only)\n\t\t\treturn true;\n\n\t\tif (!de) {\n\t\t\tde = f2fs_kmem_cache_alloc(discard_entry_slab,\n\t\t\t\t\t\tGFP_F2FS_ZERO, true, NULL);\n\t\t\tde->start_blkaddr = START_BLOCK(sbi, cpc->trim_start);\n\t\t\tlist_add_tail(&de->list, head);\n\t\t}\n\n\t\tfor (i = start; i < end; i++)\n\t\t\t__set_bit_le(i, (void *)de->discard_map);\n\n\t\tSM_I(sbi)->dcc_info->nr_discards += end - start;\n\t}\n\treturn false;\n}\n\nstatic void release_discard_addr(struct discard_entry *entry)\n{\n\tlist_del(&entry->list);\n\tkmem_cache_free(discard_entry_slab, entry);\n}\n\nvoid f2fs_release_discard_addrs(struct f2fs_sb_info *sbi)\n{\n\tstruct list_head *head = &(SM_I(sbi)->dcc_info->entry_list);\n\tstruct discard_entry *entry, *this;\n\n\t \n\tlist_for_each_entry_safe(entry, this, head, list)\n\t\trelease_discard_addr(entry);\n}\n\n \nstatic void set_prefree_as_free_segments(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned int segno;\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\tfor_each_set_bit(segno, dirty_i->dirty_segmap[PRE], MAIN_SEGS(sbi))\n\t\t__set_test_and_free(sbi, segno, false);\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\nvoid f2fs_clear_prefree_segments(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tstruct cp_control *cpc)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct list_head *head = &dcc->entry_list;\n\tstruct discard_entry *entry, *this;\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned long *prefree_map = dirty_i->dirty_segmap[PRE];\n\tunsigned int start = 0, end = -1;\n\tunsigned int secno, start_segno;\n\tbool force = (cpc->reason & CP_DISCARD);\n\tbool section_alignment = F2FS_OPTION(sbi).discard_unit ==\n\t\t\t\t\t\tDISCARD_UNIT_SECTION;\n\n\tif (f2fs_lfs_mode(sbi) && __is_large_section(sbi))\n\t\tsection_alignment = true;\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\n\twhile (1) {\n\t\tint i;\n\n\t\tif (section_alignment && end != -1)\n\t\t\tend--;\n\t\tstart = find_next_bit(prefree_map, MAIN_SEGS(sbi), end + 1);\n\t\tif (start >= MAIN_SEGS(sbi))\n\t\t\tbreak;\n\t\tend = find_next_zero_bit(prefree_map, MAIN_SEGS(sbi),\n\t\t\t\t\t\t\t\tstart + 1);\n\n\t\tif (section_alignment) {\n\t\t\tstart = rounddown(start, sbi->segs_per_sec);\n\t\t\tend = roundup(end, sbi->segs_per_sec);\n\t\t}\n\n\t\tfor (i = start; i < end; i++) {\n\t\t\tif (test_and_clear_bit(i, prefree_map))\n\t\t\t\tdirty_i->nr_dirty[PRE]--;\n\t\t}\n\n\t\tif (!f2fs_realtime_discard_enable(sbi))\n\t\t\tcontinue;\n\n\t\tif (force && start >= cpc->trim_start &&\n\t\t\t\t\t(end - 1) <= cpc->trim_end)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (!f2fs_sb_has_blkzoned(sbi) &&\n\t\t    (!f2fs_lfs_mode(sbi) || !__is_large_section(sbi))) {\n\t\t\tf2fs_issue_discard(sbi, START_BLOCK(sbi, start),\n\t\t\t\t(end - start) << sbi->log_blocks_per_seg);\n\t\t\tcontinue;\n\t\t}\nnext:\n\t\tsecno = GET_SEC_FROM_SEG(sbi, start);\n\t\tstart_segno = GET_SEG_FROM_SEC(sbi, secno);\n\t\tif (!IS_CURSEC(sbi, secno) &&\n\t\t\t!get_valid_blocks(sbi, start, true))\n\t\t\tf2fs_issue_discard(sbi, START_BLOCK(sbi, start_segno),\n\t\t\t\tsbi->segs_per_sec << sbi->log_blocks_per_seg);\n\n\t\tstart = start_segno + sbi->segs_per_sec;\n\t\tif (start < end)\n\t\t\tgoto next;\n\t\telse\n\t\t\tend = start - 1;\n\t}\n\tmutex_unlock(&dirty_i->seglist_lock);\n\n\tif (!f2fs_block_unit_discard(sbi))\n\t\tgoto wakeup;\n\n\t \n\tlist_for_each_entry_safe(entry, this, head, list) {\n\t\tunsigned int cur_pos = 0, next_pos, len, total_len = 0;\n\t\tbool is_valid = test_bit_le(0, entry->discard_map);\n\nfind_next:\n\t\tif (is_valid) {\n\t\t\tnext_pos = find_next_zero_bit_le(entry->discard_map,\n\t\t\t\t\tsbi->blocks_per_seg, cur_pos);\n\t\t\tlen = next_pos - cur_pos;\n\n\t\t\tif (f2fs_sb_has_blkzoned(sbi) ||\n\t\t\t    (force && len < cpc->trim_minlen))\n\t\t\t\tgoto skip;\n\n\t\t\tf2fs_issue_discard(sbi, entry->start_blkaddr + cur_pos,\n\t\t\t\t\t\t\t\t\tlen);\n\t\t\ttotal_len += len;\n\t\t} else {\n\t\t\tnext_pos = find_next_bit_le(entry->discard_map,\n\t\t\t\t\tsbi->blocks_per_seg, cur_pos);\n\t\t}\nskip:\n\t\tcur_pos = next_pos;\n\t\tis_valid = !is_valid;\n\n\t\tif (cur_pos < sbi->blocks_per_seg)\n\t\t\tgoto find_next;\n\n\t\trelease_discard_addr(entry);\n\t\tdcc->nr_discards -= total_len;\n\t}\n\nwakeup:\n\twake_up_discard_thread(sbi, false);\n}\n\nint f2fs_start_discard_thread(struct f2fs_sb_info *sbi)\n{\n\tdev_t dev = sbi->sb->s_bdev->bd_dev;\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tint err = 0;\n\n\tif (!f2fs_realtime_discard_enable(sbi))\n\t\treturn 0;\n\n\tdcc->f2fs_issue_discard = kthread_run(issue_discard_thread, sbi,\n\t\t\t\t\"f2fs_discard-%u:%u\", MAJOR(dev), MINOR(dev));\n\tif (IS_ERR(dcc->f2fs_issue_discard)) {\n\t\terr = PTR_ERR(dcc->f2fs_issue_discard);\n\t\tdcc->f2fs_issue_discard = NULL;\n\t}\n\n\treturn err;\n}\n\nstatic int create_discard_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tstruct discard_cmd_control *dcc;\n\tint err = 0, i;\n\n\tif (SM_I(sbi)->dcc_info) {\n\t\tdcc = SM_I(sbi)->dcc_info;\n\t\tgoto init_thread;\n\t}\n\n\tdcc = f2fs_kzalloc(sbi, sizeof(struct discard_cmd_control), GFP_KERNEL);\n\tif (!dcc)\n\t\treturn -ENOMEM;\n\n\tdcc->discard_io_aware_gran = MAX_PLIST_NUM;\n\tdcc->discard_granularity = DEFAULT_DISCARD_GRANULARITY;\n\tdcc->max_ordered_discard = DEFAULT_MAX_ORDERED_DISCARD_GRANULARITY;\n\tif (F2FS_OPTION(sbi).discard_unit == DISCARD_UNIT_SEGMENT)\n\t\tdcc->discard_granularity = sbi->blocks_per_seg;\n\telse if (F2FS_OPTION(sbi).discard_unit == DISCARD_UNIT_SECTION)\n\t\tdcc->discard_granularity = BLKS_PER_SEC(sbi);\n\n\tINIT_LIST_HEAD(&dcc->entry_list);\n\tfor (i = 0; i < MAX_PLIST_NUM; i++)\n\t\tINIT_LIST_HEAD(&dcc->pend_list[i]);\n\tINIT_LIST_HEAD(&dcc->wait_list);\n\tINIT_LIST_HEAD(&dcc->fstrim_list);\n\tmutex_init(&dcc->cmd_lock);\n\tatomic_set(&dcc->issued_discard, 0);\n\tatomic_set(&dcc->queued_discard, 0);\n\tatomic_set(&dcc->discard_cmd_cnt, 0);\n\tdcc->nr_discards = 0;\n\tdcc->max_discards = MAIN_SEGS(sbi) << sbi->log_blocks_per_seg;\n\tdcc->max_discard_request = DEF_MAX_DISCARD_REQUEST;\n\tdcc->min_discard_issue_time = DEF_MIN_DISCARD_ISSUE_TIME;\n\tdcc->mid_discard_issue_time = DEF_MID_DISCARD_ISSUE_TIME;\n\tdcc->max_discard_issue_time = DEF_MAX_DISCARD_ISSUE_TIME;\n\tdcc->discard_urgent_util = DEF_DISCARD_URGENT_UTIL;\n\tdcc->undiscard_blks = 0;\n\tdcc->next_pos = 0;\n\tdcc->root = RB_ROOT_CACHED;\n\tdcc->rbtree_check = false;\n\n\tinit_waitqueue_head(&dcc->discard_wait_queue);\n\tSM_I(sbi)->dcc_info = dcc;\ninit_thread:\n\terr = f2fs_start_discard_thread(sbi);\n\tif (err) {\n\t\tkfree(dcc);\n\t\tSM_I(sbi)->dcc_info = NULL;\n\t}\n\n\treturn err;\n}\n\nstatic void destroy_discard_cmd_control(struct f2fs_sb_info *sbi)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\n\tif (!dcc)\n\t\treturn;\n\n\tf2fs_stop_discard_thread(sbi);\n\n\t \n\tf2fs_issue_discard_timeout(sbi);\n\n\tkfree(dcc);\n\tSM_I(sbi)->dcc_info = NULL;\n}\n\nstatic bool __mark_sit_entry_dirty(struct f2fs_sb_info *sbi, unsigned int segno)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\n\tif (!__test_and_set_bit(segno, sit_i->dirty_sentries_bitmap)) {\n\t\tsit_i->dirty_sentries++;\n\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic void __set_sit_entry_type(struct f2fs_sb_info *sbi, int type,\n\t\t\t\t\tunsigned int segno, int modified)\n{\n\tstruct seg_entry *se = get_seg_entry(sbi, segno);\n\n\tse->type = type;\n\tif (modified)\n\t\t__mark_sit_entry_dirty(sbi, segno);\n}\n\nstatic inline unsigned long long get_segment_mtime(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\t\t\tblock_t blkaddr)\n{\n\tunsigned int segno = GET_SEGNO(sbi, blkaddr);\n\n\tif (segno == NULL_SEGNO)\n\t\treturn 0;\n\treturn get_seg_entry(sbi, segno)->mtime;\n}\n\nstatic void update_segment_mtime(struct f2fs_sb_info *sbi, block_t blkaddr,\n\t\t\t\t\t\tunsigned long long old_mtime)\n{\n\tstruct seg_entry *se;\n\tunsigned int segno = GET_SEGNO(sbi, blkaddr);\n\tunsigned long long ctime = get_mtime(sbi, false);\n\tunsigned long long mtime = old_mtime ? old_mtime : ctime;\n\n\tif (segno == NULL_SEGNO)\n\t\treturn;\n\n\tse = get_seg_entry(sbi, segno);\n\n\tif (!se->mtime)\n\t\tse->mtime = mtime;\n\telse\n\t\tse->mtime = div_u64(se->mtime * se->valid_blocks + mtime,\n\t\t\t\t\t\tse->valid_blocks + 1);\n\n\tif (ctime > SIT_I(sbi)->max_mtime)\n\t\tSIT_I(sbi)->max_mtime = ctime;\n}\n\nstatic void update_sit_entry(struct f2fs_sb_info *sbi, block_t blkaddr, int del)\n{\n\tstruct seg_entry *se;\n\tunsigned int segno, offset;\n\tlong int new_vblocks;\n\tbool exist;\n#ifdef CONFIG_F2FS_CHECK_FS\n\tbool mir_exist;\n#endif\n\n\tsegno = GET_SEGNO(sbi, blkaddr);\n\n\tse = get_seg_entry(sbi, segno);\n\tnew_vblocks = se->valid_blocks + del;\n\toffset = GET_BLKOFF_FROM_SEG0(sbi, blkaddr);\n\n\tf2fs_bug_on(sbi, (new_vblocks < 0 ||\n\t\t\t(new_vblocks > f2fs_usable_blks_in_seg(sbi, segno))));\n\n\tse->valid_blocks = new_vblocks;\n\n\t \n\tif (del > 0) {\n\t\texist = f2fs_test_and_set_bit(offset, se->cur_valid_map);\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\tmir_exist = f2fs_test_and_set_bit(offset,\n\t\t\t\t\t\tse->cur_valid_map_mir);\n\t\tif (unlikely(exist != mir_exist)) {\n\t\t\tf2fs_err(sbi, \"Inconsistent error when setting bitmap, blk:%u, old bit:%d\",\n\t\t\t\t blkaddr, exist);\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t}\n#endif\n\t\tif (unlikely(exist)) {\n\t\t\tf2fs_err(sbi, \"Bitmap was wrongly set, blk:%u\",\n\t\t\t\t blkaddr);\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\tse->valid_blocks--;\n\t\t\tdel = 0;\n\t\t}\n\n\t\tif (f2fs_block_unit_discard(sbi) &&\n\t\t\t\t!f2fs_test_and_set_bit(offset, se->discard_map))\n\t\t\tsbi->discard_blks--;\n\n\t\t \n\t\tif (!is_sbi_flag_set(sbi, SBI_CP_DISABLED)) {\n\t\t\tif (!f2fs_test_and_set_bit(offset, se->ckpt_valid_map))\n\t\t\t\tse->ckpt_valid_blocks++;\n\t\t}\n\t} else {\n\t\texist = f2fs_test_and_clear_bit(offset, se->cur_valid_map);\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\tmir_exist = f2fs_test_and_clear_bit(offset,\n\t\t\t\t\t\tse->cur_valid_map_mir);\n\t\tif (unlikely(exist != mir_exist)) {\n\t\t\tf2fs_err(sbi, \"Inconsistent error when clearing bitmap, blk:%u, old bit:%d\",\n\t\t\t\t blkaddr, exist);\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t}\n#endif\n\t\tif (unlikely(!exist)) {\n\t\t\tf2fs_err(sbi, \"Bitmap was wrongly cleared, blk:%u\",\n\t\t\t\t blkaddr);\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\tse->valid_blocks++;\n\t\t\tdel = 0;\n\t\t} else if (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED))) {\n\t\t\t \n\t\t\tif (f2fs_test_bit(offset, se->ckpt_valid_map)) {\n\t\t\t\tspin_lock(&sbi->stat_lock);\n\t\t\t\tsbi->unusable_block_count++;\n\t\t\t\tspin_unlock(&sbi->stat_lock);\n\t\t\t}\n\t\t}\n\n\t\tif (f2fs_block_unit_discard(sbi) &&\n\t\t\tf2fs_test_and_clear_bit(offset, se->discard_map))\n\t\t\tsbi->discard_blks++;\n\t}\n\tif (!f2fs_test_bit(offset, se->ckpt_valid_map))\n\t\tse->ckpt_valid_blocks += del;\n\n\t__mark_sit_entry_dirty(sbi, segno);\n\n\t \n\tSIT_I(sbi)->written_valid_blocks += del;\n\n\tif (__is_large_section(sbi))\n\t\tget_sec_entry(sbi, segno)->valid_blocks += del;\n}\n\nvoid f2fs_invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr)\n{\n\tunsigned int segno = GET_SEGNO(sbi, addr);\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\n\tf2fs_bug_on(sbi, addr == NULL_ADDR);\n\tif (addr == NEW_ADDR || addr == COMPRESS_ADDR)\n\t\treturn;\n\n\tinvalidate_mapping_pages(META_MAPPING(sbi), addr, addr);\n\tf2fs_invalidate_compress_page(sbi, addr);\n\n\t \n\tdown_write(&sit_i->sentry_lock);\n\n\tupdate_segment_mtime(sbi, addr, 0);\n\tupdate_sit_entry(sbi, addr, -1);\n\n\t \n\tlocate_dirty_segment(sbi, segno);\n\n\tup_write(&sit_i->sentry_lock);\n}\n\nbool f2fs_is_checkpointed_data(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned int segno, offset;\n\tstruct seg_entry *se;\n\tbool is_cp = false;\n\n\tif (!__is_valid_data_blkaddr(blkaddr))\n\t\treturn true;\n\n\tdown_read(&sit_i->sentry_lock);\n\n\tsegno = GET_SEGNO(sbi, blkaddr);\n\tse = get_seg_entry(sbi, segno);\n\toffset = GET_BLKOFF_FROM_SEG0(sbi, blkaddr);\n\n\tif (f2fs_test_bit(offset, se->ckpt_valid_map))\n\t\tis_cp = true;\n\n\tup_read(&sit_i->sentry_lock);\n\n\treturn is_cp;\n}\n\nstatic unsigned short f2fs_curseg_valid_blocks(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\n\tif (sbi->ckpt->alloc_type[type] == SSR)\n\t\treturn sbi->blocks_per_seg;\n\treturn curseg->next_blkoff;\n}\n\n \nint f2fs_npages_for_summary_flush(struct f2fs_sb_info *sbi, bool for_ra)\n{\n\tint valid_sum_count = 0;\n\tint i, sum_in_page;\n\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tif (sbi->ckpt->alloc_type[i] != SSR && for_ra)\n\t\t\tvalid_sum_count +=\n\t\t\t\tle16_to_cpu(F2FS_CKPT(sbi)->cur_data_blkoff[i]);\n\t\telse\n\t\t\tvalid_sum_count += f2fs_curseg_valid_blocks(sbi, i);\n\t}\n\n\tsum_in_page = (PAGE_SIZE - 2 * SUM_JOURNAL_SIZE -\n\t\t\tSUM_FOOTER_SIZE) / SUMMARY_SIZE;\n\tif (valid_sum_count <= sum_in_page)\n\t\treturn 1;\n\telse if ((valid_sum_count - sum_in_page) <=\n\t\t(PAGE_SIZE - SUM_FOOTER_SIZE) / SUMMARY_SIZE)\n\t\treturn 2;\n\treturn 3;\n}\n\n \nstruct page *f2fs_get_sum_page(struct f2fs_sb_info *sbi, unsigned int segno)\n{\n\tif (unlikely(f2fs_cp_error(sbi)))\n\t\treturn ERR_PTR(-EIO);\n\treturn f2fs_get_meta_page_retry(sbi, GET_SUM_BLOCK(sbi, segno));\n}\n\nvoid f2fs_update_meta_page(struct f2fs_sb_info *sbi,\n\t\t\t\t\tvoid *src, block_t blk_addr)\n{\n\tstruct page *page = f2fs_grab_meta_page(sbi, blk_addr);\n\n\tmemcpy(page_address(page), src, PAGE_SIZE);\n\tset_page_dirty(page);\n\tf2fs_put_page(page, 1);\n}\n\nstatic void write_sum_page(struct f2fs_sb_info *sbi,\n\t\t\tstruct f2fs_summary_block *sum_blk, block_t blk_addr)\n{\n\tf2fs_update_meta_page(sbi, (void *)sum_blk, blk_addr);\n}\n\nstatic void write_current_sum_page(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tint type, block_t blk_addr)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tstruct page *page = f2fs_grab_meta_page(sbi, blk_addr);\n\tstruct f2fs_summary_block *src = curseg->sum_blk;\n\tstruct f2fs_summary_block *dst;\n\n\tdst = (struct f2fs_summary_block *)page_address(page);\n\tmemset(dst, 0, PAGE_SIZE);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\n\tdown_read(&curseg->journal_rwsem);\n\tmemcpy(&dst->journal, curseg->journal, SUM_JOURNAL_SIZE);\n\tup_read(&curseg->journal_rwsem);\n\n\tmemcpy(dst->entries, src->entries, SUM_ENTRY_SIZE);\n\tmemcpy(&dst->footer, &src->footer, SUM_FOOTER_SIZE);\n\n\tmutex_unlock(&curseg->curseg_mutex);\n\n\tset_page_dirty(page);\n\tf2fs_put_page(page, 1);\n}\n\nstatic int is_next_segment_free(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct curseg_info *curseg, int type)\n{\n\tunsigned int segno = curseg->segno + 1;\n\tstruct free_segmap_info *free_i = FREE_I(sbi);\n\n\tif (segno < MAIN_SEGS(sbi) && segno % sbi->segs_per_sec)\n\t\treturn !test_bit(segno, free_i->free_segmap);\n\treturn 0;\n}\n\n \nstatic void get_new_segment(struct f2fs_sb_info *sbi,\n\t\t\tunsigned int *newseg, bool new_sec, int dir)\n{\n\tstruct free_segmap_info *free_i = FREE_I(sbi);\n\tunsigned int segno, secno, zoneno;\n\tunsigned int total_zones = MAIN_SECS(sbi) / sbi->secs_per_zone;\n\tunsigned int hint = GET_SEC_FROM_SEG(sbi, *newseg);\n\tunsigned int old_zoneno = GET_ZONE_FROM_SEG(sbi, *newseg);\n\tunsigned int left_start = hint;\n\tbool init = true;\n\tint go_left = 0;\n\tint i;\n\n\tspin_lock(&free_i->segmap_lock);\n\n\tif (!new_sec && ((*newseg + 1) % sbi->segs_per_sec)) {\n\t\tsegno = find_next_zero_bit(free_i->free_segmap,\n\t\t\tGET_SEG_FROM_SEC(sbi, hint + 1), *newseg + 1);\n\t\tif (segno < GET_SEG_FROM_SEC(sbi, hint + 1))\n\t\t\tgoto got_it;\n\t}\nfind_other_zone:\n\tsecno = find_next_zero_bit(free_i->free_secmap, MAIN_SECS(sbi), hint);\n\tif (secno >= MAIN_SECS(sbi)) {\n\t\tif (dir == ALLOC_RIGHT) {\n\t\t\tsecno = find_first_zero_bit(free_i->free_secmap,\n\t\t\t\t\t\t\tMAIN_SECS(sbi));\n\t\t\tf2fs_bug_on(sbi, secno >= MAIN_SECS(sbi));\n\t\t} else {\n\t\t\tgo_left = 1;\n\t\t\tleft_start = hint - 1;\n\t\t}\n\t}\n\tif (go_left == 0)\n\t\tgoto skip_left;\n\n\twhile (test_bit(left_start, free_i->free_secmap)) {\n\t\tif (left_start > 0) {\n\t\t\tleft_start--;\n\t\t\tcontinue;\n\t\t}\n\t\tleft_start = find_first_zero_bit(free_i->free_secmap,\n\t\t\t\t\t\t\tMAIN_SECS(sbi));\n\t\tf2fs_bug_on(sbi, left_start >= MAIN_SECS(sbi));\n\t\tbreak;\n\t}\n\tsecno = left_start;\nskip_left:\n\tsegno = GET_SEG_FROM_SEC(sbi, secno);\n\tzoneno = GET_ZONE_FROM_SEC(sbi, secno);\n\n\t \n\tif (!init)\n\t\tgoto got_it;\n\tif (sbi->secs_per_zone == 1)\n\t\tgoto got_it;\n\tif (zoneno == old_zoneno)\n\t\tgoto got_it;\n\tif (dir == ALLOC_LEFT) {\n\t\tif (!go_left && zoneno + 1 >= total_zones)\n\t\t\tgoto got_it;\n\t\tif (go_left && zoneno == 0)\n\t\t\tgoto got_it;\n\t}\n\tfor (i = 0; i < NR_CURSEG_TYPE; i++)\n\t\tif (CURSEG_I(sbi, i)->zone == zoneno)\n\t\t\tbreak;\n\n\tif (i < NR_CURSEG_TYPE) {\n\t\t \n\t\tif (go_left)\n\t\t\thint = zoneno * sbi->secs_per_zone - 1;\n\t\telse if (zoneno + 1 >= total_zones)\n\t\t\thint = 0;\n\t\telse\n\t\t\thint = (zoneno + 1) * sbi->secs_per_zone;\n\t\tinit = false;\n\t\tgoto find_other_zone;\n\t}\ngot_it:\n\t \n\tf2fs_bug_on(sbi, test_bit(segno, free_i->free_segmap));\n\t__set_inuse(sbi, segno);\n\t*newseg = segno;\n\tspin_unlock(&free_i->segmap_lock);\n}\n\nstatic void reset_curseg(struct f2fs_sb_info *sbi, int type, int modified)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tstruct summary_footer *sum_footer;\n\tunsigned short seg_type = curseg->seg_type;\n\n\tcurseg->inited = true;\n\tcurseg->segno = curseg->next_segno;\n\tcurseg->zone = GET_ZONE_FROM_SEG(sbi, curseg->segno);\n\tcurseg->next_blkoff = 0;\n\tcurseg->next_segno = NULL_SEGNO;\n\n\tsum_footer = &(curseg->sum_blk->footer);\n\tmemset(sum_footer, 0, sizeof(struct summary_footer));\n\n\tsanity_check_seg_type(sbi, seg_type);\n\n\tif (IS_DATASEG(seg_type))\n\t\tSET_SUM_TYPE(sum_footer, SUM_TYPE_DATA);\n\tif (IS_NODESEG(seg_type))\n\t\tSET_SUM_TYPE(sum_footer, SUM_TYPE_NODE);\n\t__set_sit_entry_type(sbi, seg_type, curseg->segno, modified);\n}\n\nstatic unsigned int __get_next_segno(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned short seg_type = curseg->seg_type;\n\n\tsanity_check_seg_type(sbi, seg_type);\n\tif (f2fs_need_rand_seg(sbi))\n\t\treturn get_random_u32_below(MAIN_SECS(sbi) * sbi->segs_per_sec);\n\n\t \n\tif (__is_large_section(sbi))\n\t\treturn curseg->segno;\n\n\t \n\tif (!curseg->inited)\n\t\treturn 0;\n\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED)))\n\t\treturn 0;\n\n\tif (test_opt(sbi, NOHEAP) &&\n\t\t(seg_type == CURSEG_HOT_DATA || IS_NODESEG(seg_type)))\n\t\treturn 0;\n\n\tif (SIT_I(sbi)->last_victim[ALLOC_NEXT])\n\t\treturn SIT_I(sbi)->last_victim[ALLOC_NEXT];\n\n\t \n\tif (F2FS_OPTION(sbi).alloc_mode == ALLOC_MODE_REUSE)\n\t\treturn 0;\n\n\treturn curseg->segno;\n}\n\n \nstatic void new_curseg(struct f2fs_sb_info *sbi, int type, bool new_sec)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned short seg_type = curseg->seg_type;\n\tunsigned int segno = curseg->segno;\n\tint dir = ALLOC_LEFT;\n\n\tif (curseg->inited)\n\t\twrite_sum_page(sbi, curseg->sum_blk,\n\t\t\t\tGET_SUM_BLOCK(sbi, segno));\n\tif (seg_type == CURSEG_WARM_DATA || seg_type == CURSEG_COLD_DATA)\n\t\tdir = ALLOC_RIGHT;\n\n\tif (test_opt(sbi, NOHEAP))\n\t\tdir = ALLOC_RIGHT;\n\n\tsegno = __get_next_segno(sbi, type);\n\tget_new_segment(sbi, &segno, new_sec, dir);\n\tcurseg->next_segno = segno;\n\treset_curseg(sbi, type, 1);\n\tcurseg->alloc_type = LFS;\n\tif (F2FS_OPTION(sbi).fs_mode == FS_MODE_FRAGMENT_BLK)\n\t\tcurseg->fragment_remained_chunk =\n\t\t\t\tget_random_u32_inclusive(1, sbi->max_fragment_chunk);\n}\n\nstatic int __next_free_blkoff(struct f2fs_sb_info *sbi,\n\t\t\t\t\tint segno, block_t start)\n{\n\tstruct seg_entry *se = get_seg_entry(sbi, segno);\n\tint entries = SIT_VBLOCK_MAP_SIZE / sizeof(unsigned long);\n\tunsigned long *target_map = SIT_I(sbi)->tmp_map;\n\tunsigned long *ckpt_map = (unsigned long *)se->ckpt_valid_map;\n\tunsigned long *cur_map = (unsigned long *)se->cur_valid_map;\n\tint i;\n\n\tfor (i = 0; i < entries; i++)\n\t\ttarget_map[i] = ckpt_map[i] | cur_map[i];\n\n\treturn __find_rev_next_zero_bit(target_map, sbi->blocks_per_seg, start);\n}\n\nstatic int f2fs_find_next_ssr_block(struct f2fs_sb_info *sbi,\n\t\tstruct curseg_info *seg)\n{\n\treturn __next_free_blkoff(sbi, seg->segno, seg->next_blkoff + 1);\n}\n\nbool f2fs_segment_has_free_slot(struct f2fs_sb_info *sbi, int segno)\n{\n\treturn __next_free_blkoff(sbi, segno, 0) < sbi->blocks_per_seg;\n}\n\n \nstatic void change_curseg(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned int new_segno = curseg->next_segno;\n\tstruct f2fs_summary_block *sum_node;\n\tstruct page *sum_page;\n\n\twrite_sum_page(sbi, curseg->sum_blk, GET_SUM_BLOCK(sbi, curseg->segno));\n\n\t__set_test_and_inuse(sbi, new_segno);\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\t__remove_dirty_segment(sbi, new_segno, PRE);\n\t__remove_dirty_segment(sbi, new_segno, DIRTY);\n\tmutex_unlock(&dirty_i->seglist_lock);\n\n\treset_curseg(sbi, type, 1);\n\tcurseg->alloc_type = SSR;\n\tcurseg->next_blkoff = __next_free_blkoff(sbi, curseg->segno, 0);\n\n\tsum_page = f2fs_get_sum_page(sbi, new_segno);\n\tif (IS_ERR(sum_page)) {\n\t\t \n\t\tmemset(curseg->sum_blk, 0, SUM_ENTRY_SIZE);\n\t\treturn;\n\t}\n\tsum_node = (struct f2fs_summary_block *)page_address(sum_page);\n\tmemcpy(curseg->sum_blk, sum_node, SUM_ENTRY_SIZE);\n\tf2fs_put_page(sum_page, 1);\n}\n\nstatic int get_ssr_segment(struct f2fs_sb_info *sbi, int type,\n\t\t\t\tint alloc_mode, unsigned long long age);\n\nstatic void get_atssr_segment(struct f2fs_sb_info *sbi, int type,\n\t\t\t\t\tint target_type, int alloc_mode,\n\t\t\t\t\tunsigned long long age)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\n\tcurseg->seg_type = target_type;\n\n\tif (get_ssr_segment(sbi, type, alloc_mode, age)) {\n\t\tstruct seg_entry *se = get_seg_entry(sbi, curseg->next_segno);\n\n\t\tcurseg->seg_type = se->type;\n\t\tchange_curseg(sbi, type);\n\t} else {\n\t\t \n\t\tcurseg->seg_type = CURSEG_COLD_DATA;\n\t\tnew_curseg(sbi, type, true);\n\t}\n\tstat_inc_seg_type(sbi, curseg);\n}\n\nstatic void __f2fs_init_atgc_curseg(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, CURSEG_ALL_DATA_ATGC);\n\n\tif (!sbi->am.atgc_enabled)\n\t\treturn;\n\n\tf2fs_down_read(&SM_I(sbi)->curseg_lock);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\tdown_write(&SIT_I(sbi)->sentry_lock);\n\n\tget_atssr_segment(sbi, CURSEG_ALL_DATA_ATGC, CURSEG_COLD_DATA, SSR, 0);\n\n\tup_write(&SIT_I(sbi)->sentry_lock);\n\tmutex_unlock(&curseg->curseg_mutex);\n\n\tf2fs_up_read(&SM_I(sbi)->curseg_lock);\n\n}\nvoid f2fs_init_inmem_curseg(struct f2fs_sb_info *sbi)\n{\n\t__f2fs_init_atgc_curseg(sbi);\n}\n\nstatic void __f2fs_save_inmem_curseg(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\tif (!curseg->inited)\n\t\tgoto out;\n\n\tif (get_valid_blocks(sbi, curseg->segno, false)) {\n\t\twrite_sum_page(sbi, curseg->sum_blk,\n\t\t\t\tGET_SUM_BLOCK(sbi, curseg->segno));\n\t} else {\n\t\tmutex_lock(&DIRTY_I(sbi)->seglist_lock);\n\t\t__set_test_and_free(sbi, curseg->segno, true);\n\t\tmutex_unlock(&DIRTY_I(sbi)->seglist_lock);\n\t}\nout:\n\tmutex_unlock(&curseg->curseg_mutex);\n}\n\nvoid f2fs_save_inmem_curseg(struct f2fs_sb_info *sbi)\n{\n\t__f2fs_save_inmem_curseg(sbi, CURSEG_COLD_DATA_PINNED);\n\n\tif (sbi->am.atgc_enabled)\n\t\t__f2fs_save_inmem_curseg(sbi, CURSEG_ALL_DATA_ATGC);\n}\n\nstatic void __f2fs_restore_inmem_curseg(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\tif (!curseg->inited)\n\t\tgoto out;\n\tif (get_valid_blocks(sbi, curseg->segno, false))\n\t\tgoto out;\n\n\tmutex_lock(&DIRTY_I(sbi)->seglist_lock);\n\t__set_test_and_inuse(sbi, curseg->segno);\n\tmutex_unlock(&DIRTY_I(sbi)->seglist_lock);\nout:\n\tmutex_unlock(&curseg->curseg_mutex);\n}\n\nvoid f2fs_restore_inmem_curseg(struct f2fs_sb_info *sbi)\n{\n\t__f2fs_restore_inmem_curseg(sbi, CURSEG_COLD_DATA_PINNED);\n\n\tif (sbi->am.atgc_enabled)\n\t\t__f2fs_restore_inmem_curseg(sbi, CURSEG_ALL_DATA_ATGC);\n}\n\nstatic int get_ssr_segment(struct f2fs_sb_info *sbi, int type,\n\t\t\t\tint alloc_mode, unsigned long long age)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned segno = NULL_SEGNO;\n\tunsigned short seg_type = curseg->seg_type;\n\tint i, cnt;\n\tbool reversed = false;\n\n\tsanity_check_seg_type(sbi, seg_type);\n\n\t \n\tif (!f2fs_get_victim(sbi, &segno, BG_GC, seg_type, alloc_mode, age)) {\n\t\tcurseg->next_segno = segno;\n\t\treturn 1;\n\t}\n\n\t \n\tif (IS_NODESEG(seg_type)) {\n\t\tif (seg_type >= CURSEG_WARM_NODE) {\n\t\t\treversed = true;\n\t\t\ti = CURSEG_COLD_NODE;\n\t\t} else {\n\t\t\ti = CURSEG_HOT_NODE;\n\t\t}\n\t\tcnt = NR_CURSEG_NODE_TYPE;\n\t} else {\n\t\tif (seg_type >= CURSEG_WARM_DATA) {\n\t\t\treversed = true;\n\t\t\ti = CURSEG_COLD_DATA;\n\t\t} else {\n\t\t\ti = CURSEG_HOT_DATA;\n\t\t}\n\t\tcnt = NR_CURSEG_DATA_TYPE;\n\t}\n\n\tfor (; cnt-- > 0; reversed ? i-- : i++) {\n\t\tif (i == seg_type)\n\t\t\tcontinue;\n\t\tif (!f2fs_get_victim(sbi, &segno, BG_GC, i, alloc_mode, age)) {\n\t\t\tcurseg->next_segno = segno;\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\t \n\tif (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED))) {\n\t\tsegno = get_free_segment(sbi);\n\t\tif (segno != NULL_SEGNO) {\n\t\t\tcurseg->next_segno = segno;\n\t\t\treturn 1;\n\t\t}\n\t}\n\treturn 0;\n}\n\nstatic bool need_new_seg(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\n\tif (!is_set_ckpt_flags(sbi, CP_CRC_RECOVERY_FLAG) &&\n\t    curseg->seg_type == CURSEG_WARM_NODE)\n\t\treturn true;\n\tif (curseg->alloc_type == LFS &&\n\t    is_next_segment_free(sbi, curseg, type) &&\n\t    likely(!is_sbi_flag_set(sbi, SBI_CP_DISABLED)))\n\t\treturn true;\n\tif (!f2fs_need_SSR(sbi) || !get_ssr_segment(sbi, type, SSR, 0))\n\t\treturn true;\n\treturn false;\n}\n\nvoid f2fs_allocate_segment_for_resize(struct f2fs_sb_info *sbi, int type,\n\t\t\t\t\tunsigned int start, unsigned int end)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned int segno;\n\n\tf2fs_down_read(&SM_I(sbi)->curseg_lock);\n\tmutex_lock(&curseg->curseg_mutex);\n\tdown_write(&SIT_I(sbi)->sentry_lock);\n\n\tsegno = CURSEG_I(sbi, type)->segno;\n\tif (segno < start || segno > end)\n\t\tgoto unlock;\n\n\tif (f2fs_need_SSR(sbi) && get_ssr_segment(sbi, type, SSR, 0))\n\t\tchange_curseg(sbi, type);\n\telse\n\t\tnew_curseg(sbi, type, true);\n\n\tstat_inc_seg_type(sbi, curseg);\n\n\tlocate_dirty_segment(sbi, segno);\nunlock:\n\tup_write(&SIT_I(sbi)->sentry_lock);\n\n\tif (segno != curseg->segno)\n\t\tf2fs_notice(sbi, \"For resize: curseg of type %d: %u ==> %u\",\n\t\t\t    type, segno, curseg->segno);\n\n\tmutex_unlock(&curseg->curseg_mutex);\n\tf2fs_up_read(&SM_I(sbi)->curseg_lock);\n}\n\nstatic void __allocate_new_segment(struct f2fs_sb_info *sbi, int type,\n\t\t\t\t\t\tbool new_sec, bool force)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned int old_segno;\n\n\tif (!force && curseg->inited &&\n\t    !curseg->next_blkoff &&\n\t    !get_valid_blocks(sbi, curseg->segno, new_sec) &&\n\t    !get_ckpt_valid_blocks(sbi, curseg->segno, new_sec))\n\t\treturn;\n\n\told_segno = curseg->segno;\n\tnew_curseg(sbi, type, true);\n\tstat_inc_seg_type(sbi, curseg);\n\tlocate_dirty_segment(sbi, old_segno);\n}\n\nvoid f2fs_allocate_new_section(struct f2fs_sb_info *sbi, int type, bool force)\n{\n\tf2fs_down_read(&SM_I(sbi)->curseg_lock);\n\tdown_write(&SIT_I(sbi)->sentry_lock);\n\t__allocate_new_segment(sbi, type, true, force);\n\tup_write(&SIT_I(sbi)->sentry_lock);\n\tf2fs_up_read(&SM_I(sbi)->curseg_lock);\n}\n\nvoid f2fs_allocate_new_segments(struct f2fs_sb_info *sbi)\n{\n\tint i;\n\n\tf2fs_down_read(&SM_I(sbi)->curseg_lock);\n\tdown_write(&SIT_I(sbi)->sentry_lock);\n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++)\n\t\t__allocate_new_segment(sbi, i, false, false);\n\tup_write(&SIT_I(sbi)->sentry_lock);\n\tf2fs_up_read(&SM_I(sbi)->curseg_lock);\n}\n\nbool f2fs_exist_trim_candidates(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tstruct cp_control *cpc)\n{\n\t__u64 trim_start = cpc->trim_start;\n\tbool has_candidate = false;\n\n\tdown_write(&SIT_I(sbi)->sentry_lock);\n\tfor (; cpc->trim_start <= cpc->trim_end; cpc->trim_start++) {\n\t\tif (add_discard_addrs(sbi, cpc, true)) {\n\t\t\thas_candidate = true;\n\t\t\tbreak;\n\t\t}\n\t}\n\tup_write(&SIT_I(sbi)->sentry_lock);\n\n\tcpc->trim_start = trim_start;\n\treturn has_candidate;\n}\n\nstatic unsigned int __issue_discard_cmd_range(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct discard_policy *dpolicy,\n\t\t\t\t\tunsigned int start, unsigned int end)\n{\n\tstruct discard_cmd_control *dcc = SM_I(sbi)->dcc_info;\n\tstruct discard_cmd *prev_dc = NULL, *next_dc = NULL;\n\tstruct rb_node **insert_p = NULL, *insert_parent = NULL;\n\tstruct discard_cmd *dc;\n\tstruct blk_plug plug;\n\tint issued;\n\tunsigned int trimmed = 0;\n\nnext:\n\tissued = 0;\n\n\tmutex_lock(&dcc->cmd_lock);\n\tif (unlikely(dcc->rbtree_check))\n\t\tf2fs_bug_on(sbi, !f2fs_check_discard_tree(sbi));\n\n\tdc = __lookup_discard_cmd_ret(&dcc->root, start,\n\t\t\t\t&prev_dc, &next_dc, &insert_p, &insert_parent);\n\tif (!dc)\n\t\tdc = next_dc;\n\n\tblk_start_plug(&plug);\n\n\twhile (dc && dc->di.lstart <= end) {\n\t\tstruct rb_node *node;\n\t\tint err = 0;\n\n\t\tif (dc->di.len < dpolicy->granularity)\n\t\t\tgoto skip;\n\n\t\tif (dc->state != D_PREP) {\n\t\t\tlist_move_tail(&dc->list, &dcc->fstrim_list);\n\t\t\tgoto skip;\n\t\t}\n\n\t\terr = __submit_discard_cmd(sbi, dpolicy, dc, &issued);\n\n\t\tif (issued >= dpolicy->max_requests) {\n\t\t\tstart = dc->di.lstart + dc->di.len;\n\n\t\t\tif (err)\n\t\t\t\t__remove_discard_cmd(sbi, dc);\n\n\t\t\tblk_finish_plug(&plug);\n\t\t\tmutex_unlock(&dcc->cmd_lock);\n\t\t\ttrimmed += __wait_all_discard_cmd(sbi, NULL);\n\t\t\tf2fs_io_schedule_timeout(DEFAULT_IO_TIMEOUT);\n\t\t\tgoto next;\n\t\t}\nskip:\n\t\tnode = rb_next(&dc->rb_node);\n\t\tif (err)\n\t\t\t__remove_discard_cmd(sbi, dc);\n\t\tdc = rb_entry_safe(node, struct discard_cmd, rb_node);\n\n\t\tif (fatal_signal_pending(current))\n\t\t\tbreak;\n\t}\n\n\tblk_finish_plug(&plug);\n\tmutex_unlock(&dcc->cmd_lock);\n\n\treturn trimmed;\n}\n\nint f2fs_trim_fs(struct f2fs_sb_info *sbi, struct fstrim_range *range)\n{\n\t__u64 start = F2FS_BYTES_TO_BLK(range->start);\n\t__u64 end = start + F2FS_BYTES_TO_BLK(range->len) - 1;\n\tunsigned int start_segno, end_segno;\n\tblock_t start_block, end_block;\n\tstruct cp_control cpc;\n\tstruct discard_policy dpolicy;\n\tunsigned long long trimmed = 0;\n\tint err = 0;\n\tbool need_align = f2fs_lfs_mode(sbi) && __is_large_section(sbi);\n\n\tif (start >= MAX_BLKADDR(sbi) || range->len < sbi->blocksize)\n\t\treturn -EINVAL;\n\n\tif (end < MAIN_BLKADDR(sbi))\n\t\tgoto out;\n\n\tif (is_sbi_flag_set(sbi, SBI_NEED_FSCK)) {\n\t\tf2fs_warn(sbi, \"Found FS corruption, run fsck to fix.\");\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\t \n\tstart_segno = (start <= MAIN_BLKADDR(sbi)) ? 0 : GET_SEGNO(sbi, start);\n\tend_segno = (end >= MAX_BLKADDR(sbi)) ? MAIN_SEGS(sbi) - 1 :\n\t\t\t\t\t\tGET_SEGNO(sbi, end);\n\tif (need_align) {\n\t\tstart_segno = rounddown(start_segno, sbi->segs_per_sec);\n\t\tend_segno = roundup(end_segno + 1, sbi->segs_per_sec) - 1;\n\t}\n\n\tcpc.reason = CP_DISCARD;\n\tcpc.trim_minlen = max_t(__u64, 1, F2FS_BYTES_TO_BLK(range->minlen));\n\tcpc.trim_start = start_segno;\n\tcpc.trim_end = end_segno;\n\n\tif (sbi->discard_blks == 0)\n\t\tgoto out;\n\n\tf2fs_down_write(&sbi->gc_lock);\n\tstat_inc_cp_call_count(sbi, TOTAL_CALL);\n\terr = f2fs_write_checkpoint(sbi, &cpc);\n\tf2fs_up_write(&sbi->gc_lock);\n\tif (err)\n\t\tgoto out;\n\n\t \n\tif (f2fs_realtime_discard_enable(sbi))\n\t\tgoto out;\n\n\tstart_block = START_BLOCK(sbi, start_segno);\n\tend_block = START_BLOCK(sbi, end_segno + 1);\n\n\t__init_discard_policy(sbi, &dpolicy, DPOLICY_FSTRIM, cpc.trim_minlen);\n\ttrimmed = __issue_discard_cmd_range(sbi, &dpolicy,\n\t\t\t\t\tstart_block, end_block);\n\n\ttrimmed += __wait_discard_cmd_range(sbi, &dpolicy,\n\t\t\t\t\tstart_block, end_block);\nout:\n\tif (!err)\n\t\trange->len = F2FS_BLK_TO_BYTES(trimmed);\n\treturn err;\n}\n\nint f2fs_rw_hint_to_seg_type(enum rw_hint hint)\n{\n\tswitch (hint) {\n\tcase WRITE_LIFE_SHORT:\n\t\treturn CURSEG_HOT_DATA;\n\tcase WRITE_LIFE_EXTREME:\n\t\treturn CURSEG_COLD_DATA;\n\tdefault:\n\t\treturn CURSEG_WARM_DATA;\n\t}\n}\n\nstatic int __get_segment_type_2(struct f2fs_io_info *fio)\n{\n\tif (fio->type == DATA)\n\t\treturn CURSEG_HOT_DATA;\n\telse\n\t\treturn CURSEG_HOT_NODE;\n}\n\nstatic int __get_segment_type_4(struct f2fs_io_info *fio)\n{\n\tif (fio->type == DATA) {\n\t\tstruct inode *inode = fio->page->mapping->host;\n\n\t\tif (S_ISDIR(inode->i_mode))\n\t\t\treturn CURSEG_HOT_DATA;\n\t\telse\n\t\t\treturn CURSEG_COLD_DATA;\n\t} else {\n\t\tif (IS_DNODE(fio->page) && is_cold_node(fio->page))\n\t\t\treturn CURSEG_WARM_NODE;\n\t\telse\n\t\t\treturn CURSEG_COLD_NODE;\n\t}\n}\n\nstatic int __get_age_segment_type(struct inode *inode, pgoff_t pgofs)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct extent_info ei = {};\n\n\tif (f2fs_lookup_age_extent_cache(inode, pgofs, &ei)) {\n\t\tif (!ei.age)\n\t\t\treturn NO_CHECK_TYPE;\n\t\tif (ei.age <= sbi->hot_data_age_threshold)\n\t\t\treturn CURSEG_HOT_DATA;\n\t\tif (ei.age <= sbi->warm_data_age_threshold)\n\t\t\treturn CURSEG_WARM_DATA;\n\t\treturn CURSEG_COLD_DATA;\n\t}\n\treturn NO_CHECK_TYPE;\n}\n\nstatic int __get_segment_type_6(struct f2fs_io_info *fio)\n{\n\tif (fio->type == DATA) {\n\t\tstruct inode *inode = fio->page->mapping->host;\n\t\tint type;\n\n\t\tif (is_inode_flag_set(inode, FI_ALIGNED_WRITE))\n\t\t\treturn CURSEG_COLD_DATA_PINNED;\n\n\t\tif (page_private_gcing(fio->page)) {\n\t\t\tif (fio->sbi->am.atgc_enabled &&\n\t\t\t\t(fio->io_type == FS_DATA_IO) &&\n\t\t\t\t(fio->sbi->gc_mode != GC_URGENT_HIGH))\n\t\t\t\treturn CURSEG_ALL_DATA_ATGC;\n\t\t\telse\n\t\t\t\treturn CURSEG_COLD_DATA;\n\t\t}\n\t\tif (file_is_cold(inode) || f2fs_need_compress_data(inode))\n\t\t\treturn CURSEG_COLD_DATA;\n\n\t\ttype = __get_age_segment_type(inode, fio->page->index);\n\t\tif (type != NO_CHECK_TYPE)\n\t\t\treturn type;\n\n\t\tif (file_is_hot(inode) ||\n\t\t\t\tis_inode_flag_set(inode, FI_HOT_DATA) ||\n\t\t\t\tf2fs_is_cow_file(inode))\n\t\t\treturn CURSEG_HOT_DATA;\n\t\treturn f2fs_rw_hint_to_seg_type(inode->i_write_hint);\n\t} else {\n\t\tif (IS_DNODE(fio->page))\n\t\t\treturn is_cold_node(fio->page) ? CURSEG_WARM_NODE :\n\t\t\t\t\t\tCURSEG_HOT_NODE;\n\t\treturn CURSEG_COLD_NODE;\n\t}\n}\n\nstatic int __get_segment_type(struct f2fs_io_info *fio)\n{\n\tint type = 0;\n\n\tswitch (F2FS_OPTION(fio->sbi).active_logs) {\n\tcase 2:\n\t\ttype = __get_segment_type_2(fio);\n\t\tbreak;\n\tcase 4:\n\t\ttype = __get_segment_type_4(fio);\n\t\tbreak;\n\tcase 6:\n\t\ttype = __get_segment_type_6(fio);\n\t\tbreak;\n\tdefault:\n\t\tf2fs_bug_on(fio->sbi, true);\n\t}\n\n\tif (IS_HOT(type))\n\t\tfio->temp = HOT;\n\telse if (IS_WARM(type))\n\t\tfio->temp = WARM;\n\telse\n\t\tfio->temp = COLD;\n\treturn type;\n}\n\nstatic void f2fs_randomize_chunk(struct f2fs_sb_info *sbi,\n\t\tstruct curseg_info *seg)\n{\n\t \n\tif (--seg->fragment_remained_chunk > 0)\n\t\treturn;\n\n\tseg->fragment_remained_chunk =\n\t\tget_random_u32_inclusive(1, sbi->max_fragment_chunk);\n\tseg->next_blkoff +=\n\t\tget_random_u32_inclusive(1, sbi->max_fragment_hole);\n}\n\nvoid f2fs_allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,\n\t\tblock_t old_blkaddr, block_t *new_blkaddr,\n\t\tstruct f2fs_summary *sum, int type,\n\t\tstruct f2fs_io_info *fio)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct curseg_info *curseg = CURSEG_I(sbi, type);\n\tunsigned long long old_mtime;\n\tbool from_gc = (type == CURSEG_ALL_DATA_ATGC);\n\tstruct seg_entry *se = NULL;\n\tbool segment_full = false;\n\n\tf2fs_down_read(&SM_I(sbi)->curseg_lock);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\tdown_write(&sit_i->sentry_lock);\n\n\tif (from_gc) {\n\t\tf2fs_bug_on(sbi, GET_SEGNO(sbi, old_blkaddr) == NULL_SEGNO);\n\t\tse = get_seg_entry(sbi, GET_SEGNO(sbi, old_blkaddr));\n\t\tsanity_check_seg_type(sbi, se->type);\n\t\tf2fs_bug_on(sbi, IS_NODESEG(se->type));\n\t}\n\t*new_blkaddr = NEXT_FREE_BLKADDR(sbi, curseg);\n\n\tf2fs_bug_on(sbi, curseg->next_blkoff >= sbi->blocks_per_seg);\n\n\tf2fs_wait_discard_bio(sbi, *new_blkaddr);\n\n\tcurseg->sum_blk->entries[curseg->next_blkoff] = *sum;\n\tif (curseg->alloc_type == SSR) {\n\t\tcurseg->next_blkoff = f2fs_find_next_ssr_block(sbi, curseg);\n\t} else {\n\t\tcurseg->next_blkoff++;\n\t\tif (F2FS_OPTION(sbi).fs_mode == FS_MODE_FRAGMENT_BLK)\n\t\t\tf2fs_randomize_chunk(sbi, curseg);\n\t}\n\tif (curseg->next_blkoff >= f2fs_usable_blks_in_seg(sbi, curseg->segno))\n\t\tsegment_full = true;\n\tstat_inc_block_count(sbi, curseg);\n\n\tif (from_gc) {\n\t\told_mtime = get_segment_mtime(sbi, old_blkaddr);\n\t} else {\n\t\tupdate_segment_mtime(sbi, old_blkaddr, 0);\n\t\told_mtime = 0;\n\t}\n\tupdate_segment_mtime(sbi, *new_blkaddr, old_mtime);\n\n\t \n\tupdate_sit_entry(sbi, *new_blkaddr, 1);\n\tif (GET_SEGNO(sbi, old_blkaddr) != NULL_SEGNO)\n\t\tupdate_sit_entry(sbi, old_blkaddr, -1);\n\n\t \n\tif (segment_full) {\n\t\tif (from_gc) {\n\t\t\tget_atssr_segment(sbi, type, se->type,\n\t\t\t\t\t\tAT_SSR, se->mtime);\n\t\t} else {\n\t\t\tif (need_new_seg(sbi, type))\n\t\t\t\tnew_curseg(sbi, type, false);\n\t\t\telse\n\t\t\t\tchange_curseg(sbi, type);\n\t\t\tstat_inc_seg_type(sbi, curseg);\n\t\t}\n\t}\n\t \n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, old_blkaddr));\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, *new_blkaddr));\n\n\tif (IS_DATASEG(type))\n\t\tatomic64_inc(&sbi->allocated_data_blocks);\n\n\tup_write(&sit_i->sentry_lock);\n\n\tif (page && IS_NODESEG(type)) {\n\t\tfill_node_footer_blkaddr(page, NEXT_FREE_BLKADDR(sbi, curseg));\n\n\t\tf2fs_inode_chksum_set(sbi, page);\n\t}\n\n\tif (fio) {\n\t\tstruct f2fs_bio_info *io;\n\n\t\tif (F2FS_IO_ALIGNED(sbi))\n\t\t\tfio->retry = 0;\n\n\t\tINIT_LIST_HEAD(&fio->list);\n\t\tfio->in_list = 1;\n\t\tio = sbi->write_io[fio->type] + fio->temp;\n\t\tspin_lock(&io->io_lock);\n\t\tlist_add_tail(&fio->list, &io->io_list);\n\t\tspin_unlock(&io->io_lock);\n\t}\n\n\tmutex_unlock(&curseg->curseg_mutex);\n\n\tf2fs_up_read(&SM_I(sbi)->curseg_lock);\n}\n\nvoid f2fs_update_device_state(struct f2fs_sb_info *sbi, nid_t ino,\n\t\t\t\t\tblock_t blkaddr, unsigned int blkcnt)\n{\n\tif (!f2fs_is_multi_device(sbi))\n\t\treturn;\n\n\twhile (1) {\n\t\tunsigned int devidx = f2fs_target_device_index(sbi, blkaddr);\n\t\tunsigned int blks = FDEV(devidx).end_blk - blkaddr + 1;\n\n\t\t \n\t\tf2fs_set_dirty_device(sbi, ino, devidx, FLUSH_INO);\n\n\t\t \n\t\tif (!f2fs_test_bit(devidx, (char *)&sbi->dirty_device)) {\n\t\t\tspin_lock(&sbi->dev_lock);\n\t\t\tf2fs_set_bit(devidx, (char *)&sbi->dirty_device);\n\t\t\tspin_unlock(&sbi->dev_lock);\n\t\t}\n\n\t\tif (blkcnt <= blks)\n\t\t\tbreak;\n\t\tblkcnt -= blks;\n\t\tblkaddr += blks;\n\t}\n}\n\nstatic void do_write_page(struct f2fs_summary *sum, struct f2fs_io_info *fio)\n{\n\tint type = __get_segment_type(fio);\n\tbool keep_order = (f2fs_lfs_mode(fio->sbi) && type == CURSEG_COLD_DATA);\n\n\tif (keep_order)\n\t\tf2fs_down_read(&fio->sbi->io_order_lock);\nreallocate:\n\tf2fs_allocate_data_block(fio->sbi, fio->page, fio->old_blkaddr,\n\t\t\t&fio->new_blkaddr, sum, type, fio);\n\tif (GET_SEGNO(fio->sbi, fio->old_blkaddr) != NULL_SEGNO) {\n\t\tinvalidate_mapping_pages(META_MAPPING(fio->sbi),\n\t\t\t\t\tfio->old_blkaddr, fio->old_blkaddr);\n\t\tf2fs_invalidate_compress_page(fio->sbi, fio->old_blkaddr);\n\t}\n\n\t \n\tf2fs_submit_page_write(fio);\n\tif (fio->retry) {\n\t\tfio->old_blkaddr = fio->new_blkaddr;\n\t\tgoto reallocate;\n\t}\n\n\tf2fs_update_device_state(fio->sbi, fio->ino, fio->new_blkaddr, 1);\n\n\tif (keep_order)\n\t\tf2fs_up_read(&fio->sbi->io_order_lock);\n}\n\nvoid f2fs_do_write_meta_page(struct f2fs_sb_info *sbi, struct page *page,\n\t\t\t\t\tenum iostat_type io_type)\n{\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = sbi,\n\t\t.type = META,\n\t\t.temp = HOT,\n\t\t.op = REQ_OP_WRITE,\n\t\t.op_flags = REQ_SYNC | REQ_META | REQ_PRIO,\n\t\t.old_blkaddr = page->index,\n\t\t.new_blkaddr = page->index,\n\t\t.page = page,\n\t\t.encrypted_page = NULL,\n\t\t.in_list = 0,\n\t};\n\n\tif (unlikely(page->index >= MAIN_BLKADDR(sbi)))\n\t\tfio.op_flags &= ~REQ_META;\n\n\tset_page_writeback(page);\n\tf2fs_submit_page_write(&fio);\n\n\tstat_inc_meta_count(sbi, page->index);\n\tf2fs_update_iostat(sbi, NULL, io_type, F2FS_BLKSIZE);\n}\n\nvoid f2fs_do_write_node_page(unsigned int nid, struct f2fs_io_info *fio)\n{\n\tstruct f2fs_summary sum;\n\n\tset_summary(&sum, nid, 0, 0);\n\tdo_write_page(&sum, fio);\n\n\tf2fs_update_iostat(fio->sbi, NULL, fio->io_type, F2FS_BLKSIZE);\n}\n\nvoid f2fs_outplace_write_data(struct dnode_of_data *dn,\n\t\t\t\t\tstruct f2fs_io_info *fio)\n{\n\tstruct f2fs_sb_info *sbi = fio->sbi;\n\tstruct f2fs_summary sum;\n\n\tf2fs_bug_on(sbi, dn->data_blkaddr == NULL_ADDR);\n\tif (fio->io_type == FS_DATA_IO || fio->io_type == FS_CP_DATA_IO)\n\t\tf2fs_update_age_extent_cache(dn);\n\tset_summary(&sum, dn->nid, dn->ofs_in_node, fio->version);\n\tdo_write_page(&sum, fio);\n\tf2fs_update_data_blkaddr(dn, fio->new_blkaddr);\n\n\tf2fs_update_iostat(sbi, dn->inode, fio->io_type, F2FS_BLKSIZE);\n}\n\nint f2fs_inplace_write_data(struct f2fs_io_info *fio)\n{\n\tint err;\n\tstruct f2fs_sb_info *sbi = fio->sbi;\n\tunsigned int segno;\n\n\tfio->new_blkaddr = fio->old_blkaddr;\n\t \n\t__get_segment_type(fio);\n\n\tsegno = GET_SEGNO(sbi, fio->new_blkaddr);\n\n\tif (!IS_DATASEG(get_seg_entry(sbi, segno)->type)) {\n\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\t\tf2fs_warn(sbi, \"%s: incorrect segment(%u) type, run fsck to fix.\",\n\t\t\t  __func__, segno);\n\t\terr = -EFSCORRUPTED;\n\t\tf2fs_handle_error(sbi, ERROR_INCONSISTENT_SUM_TYPE);\n\t\tgoto drop_bio;\n\t}\n\n\tif (f2fs_cp_error(sbi)) {\n\t\terr = -EIO;\n\t\tgoto drop_bio;\n\t}\n\n\tif (fio->post_read)\n\t\tinvalidate_mapping_pages(META_MAPPING(sbi),\n\t\t\t\tfio->new_blkaddr, fio->new_blkaddr);\n\n\tstat_inc_inplace_blocks(fio->sbi);\n\n\tif (fio->bio && !IS_F2FS_IPU_NOCACHE(sbi))\n\t\terr = f2fs_merge_page_bio(fio);\n\telse\n\t\terr = f2fs_submit_page_bio(fio);\n\tif (!err) {\n\t\tf2fs_update_device_state(fio->sbi, fio->ino,\n\t\t\t\t\t\tfio->new_blkaddr, 1);\n\t\tf2fs_update_iostat(fio->sbi, fio->page->mapping->host,\n\t\t\t\t\t\tfio->io_type, F2FS_BLKSIZE);\n\t}\n\n\treturn err;\ndrop_bio:\n\tif (fio->bio && *(fio->bio)) {\n\t\tstruct bio *bio = *(fio->bio);\n\n\t\tbio->bi_status = BLK_STS_IOERR;\n\t\tbio_endio(bio);\n\t\t*(fio->bio) = NULL;\n\t}\n\treturn err;\n}\n\nstatic inline int __f2fs_get_curseg(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\tunsigned int segno)\n{\n\tint i;\n\n\tfor (i = CURSEG_HOT_DATA; i < NO_CHECK_TYPE; i++) {\n\t\tif (CURSEG_I(sbi, i)->segno == segno)\n\t\t\tbreak;\n\t}\n\treturn i;\n}\n\nvoid f2fs_do_replace_block(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,\n\t\t\t\tblock_t old_blkaddr, block_t new_blkaddr,\n\t\t\t\tbool recover_curseg, bool recover_newaddr,\n\t\t\t\tbool from_gc)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct curseg_info *curseg;\n\tunsigned int segno, old_cursegno;\n\tstruct seg_entry *se;\n\tint type;\n\tunsigned short old_blkoff;\n\tunsigned char old_alloc_type;\n\n\tsegno = GET_SEGNO(sbi, new_blkaddr);\n\tse = get_seg_entry(sbi, segno);\n\ttype = se->type;\n\n\tf2fs_down_write(&SM_I(sbi)->curseg_lock);\n\n\tif (!recover_curseg) {\n\t\t \n\t\tif (se->valid_blocks == 0 && !IS_CURSEG(sbi, segno)) {\n\t\t\tif (old_blkaddr == NULL_ADDR)\n\t\t\t\ttype = CURSEG_COLD_DATA;\n\t\t\telse\n\t\t\t\ttype = CURSEG_WARM_DATA;\n\t\t}\n\t} else {\n\t\tif (IS_CURSEG(sbi, segno)) {\n\t\t\t \n\t\t\ttype = __f2fs_get_curseg(sbi, segno);\n\t\t\tf2fs_bug_on(sbi, type == NO_CHECK_TYPE);\n\t\t} else {\n\t\t\ttype = CURSEG_WARM_DATA;\n\t\t}\n\t}\n\n\tf2fs_bug_on(sbi, !IS_DATASEG(type));\n\tcurseg = CURSEG_I(sbi, type);\n\n\tmutex_lock(&curseg->curseg_mutex);\n\tdown_write(&sit_i->sentry_lock);\n\n\told_cursegno = curseg->segno;\n\told_blkoff = curseg->next_blkoff;\n\told_alloc_type = curseg->alloc_type;\n\n\t \n\tif (segno != curseg->segno) {\n\t\tcurseg->next_segno = segno;\n\t\tchange_curseg(sbi, type);\n\t}\n\n\tcurseg->next_blkoff = GET_BLKOFF_FROM_SEG0(sbi, new_blkaddr);\n\tcurseg->sum_blk->entries[curseg->next_blkoff] = *sum;\n\n\tif (!recover_curseg || recover_newaddr) {\n\t\tif (!from_gc)\n\t\t\tupdate_segment_mtime(sbi, new_blkaddr, 0);\n\t\tupdate_sit_entry(sbi, new_blkaddr, 1);\n\t}\n\tif (GET_SEGNO(sbi, old_blkaddr) != NULL_SEGNO) {\n\t\tinvalidate_mapping_pages(META_MAPPING(sbi),\n\t\t\t\t\told_blkaddr, old_blkaddr);\n\t\tf2fs_invalidate_compress_page(sbi, old_blkaddr);\n\t\tif (!from_gc)\n\t\t\tupdate_segment_mtime(sbi, old_blkaddr, 0);\n\t\tupdate_sit_entry(sbi, old_blkaddr, -1);\n\t}\n\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, old_blkaddr));\n\tlocate_dirty_segment(sbi, GET_SEGNO(sbi, new_blkaddr));\n\n\tlocate_dirty_segment(sbi, old_cursegno);\n\n\tif (recover_curseg) {\n\t\tif (old_cursegno != curseg->segno) {\n\t\t\tcurseg->next_segno = old_cursegno;\n\t\t\tchange_curseg(sbi, type);\n\t\t}\n\t\tcurseg->next_blkoff = old_blkoff;\n\t\tcurseg->alloc_type = old_alloc_type;\n\t}\n\n\tup_write(&sit_i->sentry_lock);\n\tmutex_unlock(&curseg->curseg_mutex);\n\tf2fs_up_write(&SM_I(sbi)->curseg_lock);\n}\n\nvoid f2fs_replace_block(struct f2fs_sb_info *sbi, struct dnode_of_data *dn,\n\t\t\t\tblock_t old_addr, block_t new_addr,\n\t\t\t\tunsigned char version, bool recover_curseg,\n\t\t\t\tbool recover_newaddr)\n{\n\tstruct f2fs_summary sum;\n\n\tset_summary(&sum, dn->nid, dn->ofs_in_node, version);\n\n\tf2fs_do_replace_block(sbi, &sum, old_addr, new_addr,\n\t\t\t\t\trecover_curseg, recover_newaddr, false);\n\n\tf2fs_update_data_blkaddr(dn, new_addr);\n}\n\nvoid f2fs_wait_on_page_writeback(struct page *page,\n\t\t\t\tenum page_type type, bool ordered, bool locked)\n{\n\tif (PageWriteback(page)) {\n\t\tstruct f2fs_sb_info *sbi = F2FS_P_SB(page);\n\n\t\t \n\t\tf2fs_submit_merged_write_cond(sbi, NULL, page, 0, type);\n\t\t \n\t\tf2fs_submit_merged_ipu_write(sbi, NULL, page);\n\t\tif (ordered) {\n\t\t\twait_on_page_writeback(page);\n\t\t\tf2fs_bug_on(sbi, locked && PageWriteback(page));\n\t\t} else {\n\t\t\twait_for_stable_page(page);\n\t\t}\n\t}\n}\n\nvoid f2fs_wait_on_block_writeback(struct inode *inode, block_t blkaddr)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct page *cpage;\n\n\tif (!f2fs_post_read_required(inode))\n\t\treturn;\n\n\tif (!__is_valid_data_blkaddr(blkaddr))\n\t\treturn;\n\n\tcpage = find_lock_page(META_MAPPING(sbi), blkaddr);\n\tif (cpage) {\n\t\tf2fs_wait_on_page_writeback(cpage, DATA, true, true);\n\t\tf2fs_put_page(cpage, 1);\n\t}\n}\n\nvoid f2fs_wait_on_block_writeback_range(struct inode *inode, block_t blkaddr,\n\t\t\t\t\t\t\t\tblock_t len)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tblock_t i;\n\n\tif (!f2fs_post_read_required(inode))\n\t\treturn;\n\n\tfor (i = 0; i < len; i++)\n\t\tf2fs_wait_on_block_writeback(inode, blkaddr + i);\n\n\tinvalidate_mapping_pages(META_MAPPING(sbi), blkaddr, blkaddr + len - 1);\n}\n\nstatic int read_compacted_summaries(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct curseg_info *seg_i;\n\tunsigned char *kaddr;\n\tstruct page *page;\n\tblock_t start;\n\tint i, j, offset;\n\n\tstart = start_sum_block(sbi);\n\n\tpage = f2fs_get_meta_page(sbi, start++);\n\tif (IS_ERR(page))\n\t\treturn PTR_ERR(page);\n\tkaddr = (unsigned char *)page_address(page);\n\n\t \n\tseg_i = CURSEG_I(sbi, CURSEG_HOT_DATA);\n\tmemcpy(seg_i->journal, kaddr, SUM_JOURNAL_SIZE);\n\n\t \n\tseg_i = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tmemcpy(seg_i->journal, kaddr + SUM_JOURNAL_SIZE, SUM_JOURNAL_SIZE);\n\toffset = 2 * SUM_JOURNAL_SIZE;\n\n\t \n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tunsigned short blk_off;\n\t\tunsigned int segno;\n\n\t\tseg_i = CURSEG_I(sbi, i);\n\t\tsegno = le32_to_cpu(ckpt->cur_data_segno[i]);\n\t\tblk_off = le16_to_cpu(ckpt->cur_data_blkoff[i]);\n\t\tseg_i->next_segno = segno;\n\t\treset_curseg(sbi, i, 0);\n\t\tseg_i->alloc_type = ckpt->alloc_type[i];\n\t\tseg_i->next_blkoff = blk_off;\n\n\t\tif (seg_i->alloc_type == SSR)\n\t\t\tblk_off = sbi->blocks_per_seg;\n\n\t\tfor (j = 0; j < blk_off; j++) {\n\t\t\tstruct f2fs_summary *s;\n\n\t\t\ts = (struct f2fs_summary *)(kaddr + offset);\n\t\t\tseg_i->sum_blk->entries[j] = *s;\n\t\t\toffset += SUMMARY_SIZE;\n\t\t\tif (offset + SUMMARY_SIZE <= PAGE_SIZE -\n\t\t\t\t\t\tSUM_FOOTER_SIZE)\n\t\t\t\tcontinue;\n\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tpage = NULL;\n\n\t\t\tpage = f2fs_get_meta_page(sbi, start++);\n\t\t\tif (IS_ERR(page))\n\t\t\t\treturn PTR_ERR(page);\n\t\t\tkaddr = (unsigned char *)page_address(page);\n\t\t\toffset = 0;\n\t\t}\n\t}\n\tf2fs_put_page(page, 1);\n\treturn 0;\n}\n\nstatic int read_normal_summaries(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct f2fs_summary_block *sum;\n\tstruct curseg_info *curseg;\n\tstruct page *new;\n\tunsigned short blk_off;\n\tunsigned int segno = 0;\n\tblock_t blk_addr = 0;\n\tint err = 0;\n\n\t \n\tif (IS_DATASEG(type)) {\n\t\tsegno = le32_to_cpu(ckpt->cur_data_segno[type]);\n\t\tblk_off = le16_to_cpu(ckpt->cur_data_blkoff[type -\n\t\t\t\t\t\t\tCURSEG_HOT_DATA]);\n\t\tif (__exist_node_summaries(sbi))\n\t\t\tblk_addr = sum_blk_addr(sbi, NR_CURSEG_PERSIST_TYPE, type);\n\t\telse\n\t\t\tblk_addr = sum_blk_addr(sbi, NR_CURSEG_DATA_TYPE, type);\n\t} else {\n\t\tsegno = le32_to_cpu(ckpt->cur_node_segno[type -\n\t\t\t\t\t\t\tCURSEG_HOT_NODE]);\n\t\tblk_off = le16_to_cpu(ckpt->cur_node_blkoff[type -\n\t\t\t\t\t\t\tCURSEG_HOT_NODE]);\n\t\tif (__exist_node_summaries(sbi))\n\t\t\tblk_addr = sum_blk_addr(sbi, NR_CURSEG_NODE_TYPE,\n\t\t\t\t\t\t\ttype - CURSEG_HOT_NODE);\n\t\telse\n\t\t\tblk_addr = GET_SUM_BLOCK(sbi, segno);\n\t}\n\n\tnew = f2fs_get_meta_page(sbi, blk_addr);\n\tif (IS_ERR(new))\n\t\treturn PTR_ERR(new);\n\tsum = (struct f2fs_summary_block *)page_address(new);\n\n\tif (IS_NODESEG(type)) {\n\t\tif (__exist_node_summaries(sbi)) {\n\t\t\tstruct f2fs_summary *ns = &sum->entries[0];\n\t\t\tint i;\n\n\t\t\tfor (i = 0; i < sbi->blocks_per_seg; i++, ns++) {\n\t\t\t\tns->version = 0;\n\t\t\t\tns->ofs_in_node = 0;\n\t\t\t}\n\t\t} else {\n\t\t\terr = f2fs_restore_node_summary(sbi, segno, sum);\n\t\t\tif (err)\n\t\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tcurseg = CURSEG_I(sbi, type);\n\tmutex_lock(&curseg->curseg_mutex);\n\n\t \n\tdown_write(&curseg->journal_rwsem);\n\tmemcpy(curseg->journal, &sum->journal, SUM_JOURNAL_SIZE);\n\tup_write(&curseg->journal_rwsem);\n\n\tmemcpy(curseg->sum_blk->entries, sum->entries, SUM_ENTRY_SIZE);\n\tmemcpy(&curseg->sum_blk->footer, &sum->footer, SUM_FOOTER_SIZE);\n\tcurseg->next_segno = segno;\n\treset_curseg(sbi, type, 0);\n\tcurseg->alloc_type = ckpt->alloc_type[type];\n\tcurseg->next_blkoff = blk_off;\n\tmutex_unlock(&curseg->curseg_mutex);\nout:\n\tf2fs_put_page(new, 1);\n\treturn err;\n}\n\nstatic int restore_curseg_summaries(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_journal *sit_j = CURSEG_I(sbi, CURSEG_COLD_DATA)->journal;\n\tstruct f2fs_journal *nat_j = CURSEG_I(sbi, CURSEG_HOT_DATA)->journal;\n\tint type = CURSEG_HOT_DATA;\n\tint err;\n\n\tif (is_set_ckpt_flags(sbi, CP_COMPACT_SUM_FLAG)) {\n\t\tint npages = f2fs_npages_for_summary_flush(sbi, true);\n\n\t\tif (npages >= 2)\n\t\t\tf2fs_ra_meta_pages(sbi, start_sum_block(sbi), npages,\n\t\t\t\t\t\t\tMETA_CP, true);\n\n\t\t \n\t\terr = read_compacted_summaries(sbi);\n\t\tif (err)\n\t\t\treturn err;\n\t\ttype = CURSEG_HOT_NODE;\n\t}\n\n\tif (__exist_node_summaries(sbi))\n\t\tf2fs_ra_meta_pages(sbi,\n\t\t\t\tsum_blk_addr(sbi, NR_CURSEG_PERSIST_TYPE, type),\n\t\t\t\tNR_CURSEG_PERSIST_TYPE - type, META_CP, true);\n\n\tfor (; type <= CURSEG_COLD_NODE; type++) {\n\t\terr = read_normal_summaries(sbi, type);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\t \n\tif (nats_in_cursum(nat_j) > NAT_JOURNAL_ENTRIES ||\n\t\t\tsits_in_cursum(sit_j) > SIT_JOURNAL_ENTRIES) {\n\t\tf2fs_err(sbi, \"invalid journal entries nats %u sits %u\",\n\t\t\t nats_in_cursum(nat_j), sits_in_cursum(sit_j));\n\t\treturn -EINVAL;\n\t}\n\n\treturn 0;\n}\n\nstatic void write_compacted_summaries(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tstruct page *page;\n\tunsigned char *kaddr;\n\tstruct f2fs_summary *summary;\n\tstruct curseg_info *seg_i;\n\tint written_size = 0;\n\tint i, j;\n\n\tpage = f2fs_grab_meta_page(sbi, blkaddr++);\n\tkaddr = (unsigned char *)page_address(page);\n\tmemset(kaddr, 0, PAGE_SIZE);\n\n\t \n\tseg_i = CURSEG_I(sbi, CURSEG_HOT_DATA);\n\tmemcpy(kaddr, seg_i->journal, SUM_JOURNAL_SIZE);\n\twritten_size += SUM_JOURNAL_SIZE;\n\n\t \n\tseg_i = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tmemcpy(kaddr + written_size, seg_i->journal, SUM_JOURNAL_SIZE);\n\twritten_size += SUM_JOURNAL_SIZE;\n\n\t \n\tfor (i = CURSEG_HOT_DATA; i <= CURSEG_COLD_DATA; i++) {\n\t\tseg_i = CURSEG_I(sbi, i);\n\t\tfor (j = 0; j < f2fs_curseg_valid_blocks(sbi, i); j++) {\n\t\t\tif (!page) {\n\t\t\t\tpage = f2fs_grab_meta_page(sbi, blkaddr++);\n\t\t\t\tkaddr = (unsigned char *)page_address(page);\n\t\t\t\tmemset(kaddr, 0, PAGE_SIZE);\n\t\t\t\twritten_size = 0;\n\t\t\t}\n\t\t\tsummary = (struct f2fs_summary *)(kaddr + written_size);\n\t\t\t*summary = seg_i->sum_blk->entries[j];\n\t\t\twritten_size += SUMMARY_SIZE;\n\n\t\t\tif (written_size + SUMMARY_SIZE <= PAGE_SIZE -\n\t\t\t\t\t\t\tSUM_FOOTER_SIZE)\n\t\t\t\tcontinue;\n\n\t\t\tset_page_dirty(page);\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tpage = NULL;\n\t\t}\n\t}\n\tif (page) {\n\t\tset_page_dirty(page);\n\t\tf2fs_put_page(page, 1);\n\t}\n}\n\nstatic void write_normal_summaries(struct f2fs_sb_info *sbi,\n\t\t\t\t\tblock_t blkaddr, int type)\n{\n\tint i, end;\n\n\tif (IS_DATASEG(type))\n\t\tend = type + NR_CURSEG_DATA_TYPE;\n\telse\n\t\tend = type + NR_CURSEG_NODE_TYPE;\n\n\tfor (i = type; i < end; i++)\n\t\twrite_current_sum_page(sbi, i, blkaddr + (i - type));\n}\n\nvoid f2fs_write_data_summaries(struct f2fs_sb_info *sbi, block_t start_blk)\n{\n\tif (is_set_ckpt_flags(sbi, CP_COMPACT_SUM_FLAG))\n\t\twrite_compacted_summaries(sbi, start_blk);\n\telse\n\t\twrite_normal_summaries(sbi, start_blk, CURSEG_HOT_DATA);\n}\n\nvoid f2fs_write_node_summaries(struct f2fs_sb_info *sbi, block_t start_blk)\n{\n\twrite_normal_summaries(sbi, start_blk, CURSEG_HOT_NODE);\n}\n\nint f2fs_lookup_journal_in_cursum(struct f2fs_journal *journal, int type,\n\t\t\t\t\tunsigned int val, int alloc)\n{\n\tint i;\n\n\tif (type == NAT_JOURNAL) {\n\t\tfor (i = 0; i < nats_in_cursum(journal); i++) {\n\t\t\tif (le32_to_cpu(nid_in_journal(journal, i)) == val)\n\t\t\t\treturn i;\n\t\t}\n\t\tif (alloc && __has_cursum_space(journal, 1, NAT_JOURNAL))\n\t\t\treturn update_nats_in_cursum(journal, 1);\n\t} else if (type == SIT_JOURNAL) {\n\t\tfor (i = 0; i < sits_in_cursum(journal); i++)\n\t\t\tif (le32_to_cpu(segno_in_journal(journal, i)) == val)\n\t\t\t\treturn i;\n\t\tif (alloc && __has_cursum_space(journal, 1, SIT_JOURNAL))\n\t\t\treturn update_sits_in_cursum(journal, 1);\n\t}\n\treturn -1;\n}\n\nstatic struct page *get_current_sit_page(struct f2fs_sb_info *sbi,\n\t\t\t\t\tunsigned int segno)\n{\n\treturn f2fs_get_meta_page(sbi, current_sit_addr(sbi, segno));\n}\n\nstatic struct page *get_next_sit_page(struct f2fs_sb_info *sbi,\n\t\t\t\t\tunsigned int start)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct page *page;\n\tpgoff_t src_off, dst_off;\n\n\tsrc_off = current_sit_addr(sbi, start);\n\tdst_off = next_sit_addr(sbi, src_off);\n\n\tpage = f2fs_grab_meta_page(sbi, dst_off);\n\tseg_info_to_sit_page(sbi, page, start);\n\n\tset_page_dirty(page);\n\tset_to_next_sit(sit_i, start);\n\n\treturn page;\n}\n\nstatic struct sit_entry_set *grab_sit_entry_set(void)\n{\n\tstruct sit_entry_set *ses =\n\t\t\tf2fs_kmem_cache_alloc(sit_entry_set_slab,\n\t\t\t\t\t\tGFP_NOFS, true, NULL);\n\n\tses->entry_cnt = 0;\n\tINIT_LIST_HEAD(&ses->set_list);\n\treturn ses;\n}\n\nstatic void release_sit_entry_set(struct sit_entry_set *ses)\n{\n\tlist_del(&ses->set_list);\n\tkmem_cache_free(sit_entry_set_slab, ses);\n}\n\nstatic void adjust_sit_entry_set(struct sit_entry_set *ses,\n\t\t\t\t\t\tstruct list_head *head)\n{\n\tstruct sit_entry_set *next = ses;\n\n\tif (list_is_last(&ses->set_list, head))\n\t\treturn;\n\n\tlist_for_each_entry_continue(next, head, set_list)\n\t\tif (ses->entry_cnt <= next->entry_cnt) {\n\t\t\tlist_move_tail(&ses->set_list, &next->set_list);\n\t\t\treturn;\n\t\t}\n\n\tlist_move_tail(&ses->set_list, head);\n}\n\nstatic void add_sit_entry(unsigned int segno, struct list_head *head)\n{\n\tstruct sit_entry_set *ses;\n\tunsigned int start_segno = START_SEGNO(segno);\n\n\tlist_for_each_entry(ses, head, set_list) {\n\t\tif (ses->start_segno == start_segno) {\n\t\t\tses->entry_cnt++;\n\t\t\tadjust_sit_entry_set(ses, head);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tses = grab_sit_entry_set();\n\n\tses->start_segno = start_segno;\n\tses->entry_cnt++;\n\tlist_add(&ses->set_list, head);\n}\n\nstatic void add_sits_in_set(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_sm_info *sm_info = SM_I(sbi);\n\tstruct list_head *set_list = &sm_info->sit_entry_set;\n\tunsigned long *bitmap = SIT_I(sbi)->dirty_sentries_bitmap;\n\tunsigned int segno;\n\n\tfor_each_set_bit(segno, bitmap, MAIN_SEGS(sbi))\n\t\tadd_sit_entry(segno, set_list);\n}\n\nstatic void remove_sits_in_journal(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *curseg = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tstruct f2fs_journal *journal = curseg->journal;\n\tint i;\n\n\tdown_write(&curseg->journal_rwsem);\n\tfor (i = 0; i < sits_in_cursum(journal); i++) {\n\t\tunsigned int segno;\n\t\tbool dirtied;\n\n\t\tsegno = le32_to_cpu(segno_in_journal(journal, i));\n\t\tdirtied = __mark_sit_entry_dirty(sbi, segno);\n\n\t\tif (!dirtied)\n\t\t\tadd_sit_entry(segno, &SM_I(sbi)->sit_entry_set);\n\t}\n\tupdate_sits_in_cursum(journal, -i);\n\tup_write(&curseg->journal_rwsem);\n}\n\n \nvoid f2fs_flush_sit_entries(struct f2fs_sb_info *sbi, struct cp_control *cpc)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned long *bitmap = sit_i->dirty_sentries_bitmap;\n\tstruct curseg_info *curseg = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tstruct f2fs_journal *journal = curseg->journal;\n\tstruct sit_entry_set *ses, *tmp;\n\tstruct list_head *head = &SM_I(sbi)->sit_entry_set;\n\tbool to_journal = !is_sbi_flag_set(sbi, SBI_IS_RESIZEFS);\n\tstruct seg_entry *se;\n\n\tdown_write(&sit_i->sentry_lock);\n\n\tif (!sit_i->dirty_sentries)\n\t\tgoto out;\n\n\t \n\tadd_sits_in_set(sbi);\n\n\t \n\tif (!__has_cursum_space(journal, sit_i->dirty_sentries, SIT_JOURNAL) ||\n\t\t\t\t\t\t\t\t!to_journal)\n\t\tremove_sits_in_journal(sbi);\n\n\t \n\tlist_for_each_entry_safe(ses, tmp, head, set_list) {\n\t\tstruct page *page = NULL;\n\t\tstruct f2fs_sit_block *raw_sit = NULL;\n\t\tunsigned int start_segno = ses->start_segno;\n\t\tunsigned int end = min(start_segno + SIT_ENTRY_PER_BLOCK,\n\t\t\t\t\t\t(unsigned long)MAIN_SEGS(sbi));\n\t\tunsigned int segno = start_segno;\n\n\t\tif (to_journal &&\n\t\t\t!__has_cursum_space(journal, ses->entry_cnt, SIT_JOURNAL))\n\t\t\tto_journal = false;\n\n\t\tif (to_journal) {\n\t\t\tdown_write(&curseg->journal_rwsem);\n\t\t} else {\n\t\t\tpage = get_next_sit_page(sbi, start_segno);\n\t\t\traw_sit = page_address(page);\n\t\t}\n\n\t\t \n\t\tfor_each_set_bit_from(segno, bitmap, end) {\n\t\t\tint offset, sit_offset;\n\n\t\t\tse = get_seg_entry(sbi, segno);\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\t\tif (memcmp(se->cur_valid_map, se->cur_valid_map_mir,\n\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE))\n\t\t\t\tf2fs_bug_on(sbi, 1);\n#endif\n\n\t\t\t \n\t\t\tif (!(cpc->reason & CP_DISCARD)) {\n\t\t\t\tcpc->trim_start = segno;\n\t\t\t\tadd_discard_addrs(sbi, cpc, false);\n\t\t\t}\n\n\t\t\tif (to_journal) {\n\t\t\t\toffset = f2fs_lookup_journal_in_cursum(journal,\n\t\t\t\t\t\t\tSIT_JOURNAL, segno, 1);\n\t\t\t\tf2fs_bug_on(sbi, offset < 0);\n\t\t\t\tsegno_in_journal(journal, offset) =\n\t\t\t\t\t\t\tcpu_to_le32(segno);\n\t\t\t\tseg_info_to_raw_sit(se,\n\t\t\t\t\t&sit_in_journal(journal, offset));\n\t\t\t\tcheck_block_count(sbi, segno,\n\t\t\t\t\t&sit_in_journal(journal, offset));\n\t\t\t} else {\n\t\t\t\tsit_offset = SIT_ENTRY_OFFSET(sit_i, segno);\n\t\t\t\tseg_info_to_raw_sit(se,\n\t\t\t\t\t\t&raw_sit->entries[sit_offset]);\n\t\t\t\tcheck_block_count(sbi, segno,\n\t\t\t\t\t\t&raw_sit->entries[sit_offset]);\n\t\t\t}\n\n\t\t\t__clear_bit(segno, bitmap);\n\t\t\tsit_i->dirty_sentries--;\n\t\t\tses->entry_cnt--;\n\t\t}\n\n\t\tif (to_journal)\n\t\t\tup_write(&curseg->journal_rwsem);\n\t\telse\n\t\t\tf2fs_put_page(page, 1);\n\n\t\tf2fs_bug_on(sbi, ses->entry_cnt);\n\t\trelease_sit_entry_set(ses);\n\t}\n\n\tf2fs_bug_on(sbi, !list_empty(head));\n\tf2fs_bug_on(sbi, sit_i->dirty_sentries);\nout:\n\tif (cpc->reason & CP_DISCARD) {\n\t\t__u64 trim_start = cpc->trim_start;\n\n\t\tfor (; cpc->trim_start <= cpc->trim_end; cpc->trim_start++)\n\t\t\tadd_discard_addrs(sbi, cpc, false);\n\n\t\tcpc->trim_start = trim_start;\n\t}\n\tup_write(&sit_i->sentry_lock);\n\n\tset_prefree_as_free_segments(sbi);\n}\n\nstatic int build_sit_info(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct sit_info *sit_i;\n\tunsigned int sit_segs, start;\n\tchar *src_bitmap, *bitmap;\n\tunsigned int bitmap_size, main_bitmap_size, sit_bitmap_size;\n\tunsigned int discard_map = f2fs_block_unit_discard(sbi) ? 1 : 0;\n\n\t \n\tsit_i = f2fs_kzalloc(sbi, sizeof(struct sit_info), GFP_KERNEL);\n\tif (!sit_i)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->sit_info = sit_i;\n\n\tsit_i->sentries =\n\t\tf2fs_kvzalloc(sbi, array_size(sizeof(struct seg_entry),\n\t\t\t\t\t      MAIN_SEGS(sbi)),\n\t\t\t      GFP_KERNEL);\n\tif (!sit_i->sentries)\n\t\treturn -ENOMEM;\n\n\tmain_bitmap_size = f2fs_bitmap_size(MAIN_SEGS(sbi));\n\tsit_i->dirty_sentries_bitmap = f2fs_kvzalloc(sbi, main_bitmap_size,\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n\tif (!sit_i->dirty_sentries_bitmap)\n\t\treturn -ENOMEM;\n\n#ifdef CONFIG_F2FS_CHECK_FS\n\tbitmap_size = MAIN_SEGS(sbi) * SIT_VBLOCK_MAP_SIZE * (3 + discard_map);\n#else\n\tbitmap_size = MAIN_SEGS(sbi) * SIT_VBLOCK_MAP_SIZE * (2 + discard_map);\n#endif\n\tsit_i->bitmap = f2fs_kvzalloc(sbi, bitmap_size, GFP_KERNEL);\n\tif (!sit_i->bitmap)\n\t\treturn -ENOMEM;\n\n\tbitmap = sit_i->bitmap;\n\n\tfor (start = 0; start < MAIN_SEGS(sbi); start++) {\n\t\tsit_i->sentries[start].cur_valid_map = bitmap;\n\t\tbitmap += SIT_VBLOCK_MAP_SIZE;\n\n\t\tsit_i->sentries[start].ckpt_valid_map = bitmap;\n\t\tbitmap += SIT_VBLOCK_MAP_SIZE;\n\n#ifdef CONFIG_F2FS_CHECK_FS\n\t\tsit_i->sentries[start].cur_valid_map_mir = bitmap;\n\t\tbitmap += SIT_VBLOCK_MAP_SIZE;\n#endif\n\n\t\tif (discard_map) {\n\t\t\tsit_i->sentries[start].discard_map = bitmap;\n\t\t\tbitmap += SIT_VBLOCK_MAP_SIZE;\n\t\t}\n\t}\n\n\tsit_i->tmp_map = f2fs_kzalloc(sbi, SIT_VBLOCK_MAP_SIZE, GFP_KERNEL);\n\tif (!sit_i->tmp_map)\n\t\treturn -ENOMEM;\n\n\tif (__is_large_section(sbi)) {\n\t\tsit_i->sec_entries =\n\t\t\tf2fs_kvzalloc(sbi, array_size(sizeof(struct sec_entry),\n\t\t\t\t\t\t      MAIN_SECS(sbi)),\n\t\t\t\t      GFP_KERNEL);\n\t\tif (!sit_i->sec_entries)\n\t\t\treturn -ENOMEM;\n\t}\n\n\t \n\tsit_segs = le32_to_cpu(raw_super->segment_count_sit) >> 1;\n\n\t \n\tsit_bitmap_size = __bitmap_size(sbi, SIT_BITMAP);\n\tsrc_bitmap = __bitmap_ptr(sbi, SIT_BITMAP);\n\n\tsit_i->sit_bitmap = kmemdup(src_bitmap, sit_bitmap_size, GFP_KERNEL);\n\tif (!sit_i->sit_bitmap)\n\t\treturn -ENOMEM;\n\n#ifdef CONFIG_F2FS_CHECK_FS\n\tsit_i->sit_bitmap_mir = kmemdup(src_bitmap,\n\t\t\t\t\tsit_bitmap_size, GFP_KERNEL);\n\tif (!sit_i->sit_bitmap_mir)\n\t\treturn -ENOMEM;\n\n\tsit_i->invalid_segmap = f2fs_kvzalloc(sbi,\n\t\t\t\t\tmain_bitmap_size, GFP_KERNEL);\n\tif (!sit_i->invalid_segmap)\n\t\treturn -ENOMEM;\n#endif\n\n\tsit_i->sit_base_addr = le32_to_cpu(raw_super->sit_blkaddr);\n\tsit_i->sit_blocks = sit_segs << sbi->log_blocks_per_seg;\n\tsit_i->written_valid_blocks = 0;\n\tsit_i->bitmap_size = sit_bitmap_size;\n\tsit_i->dirty_sentries = 0;\n\tsit_i->sents_per_block = SIT_ENTRY_PER_BLOCK;\n\tsit_i->elapsed_time = le64_to_cpu(sbi->ckpt->elapsed_time);\n\tsit_i->mounted_time = ktime_get_boottime_seconds();\n\tinit_rwsem(&sit_i->sentry_lock);\n\treturn 0;\n}\n\nstatic int build_free_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct free_segmap_info *free_i;\n\tunsigned int bitmap_size, sec_bitmap_size;\n\n\t \n\tfree_i = f2fs_kzalloc(sbi, sizeof(struct free_segmap_info), GFP_KERNEL);\n\tif (!free_i)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->free_info = free_i;\n\n\tbitmap_size = f2fs_bitmap_size(MAIN_SEGS(sbi));\n\tfree_i->free_segmap = f2fs_kvmalloc(sbi, bitmap_size, GFP_KERNEL);\n\tif (!free_i->free_segmap)\n\t\treturn -ENOMEM;\n\n\tsec_bitmap_size = f2fs_bitmap_size(MAIN_SECS(sbi));\n\tfree_i->free_secmap = f2fs_kvmalloc(sbi, sec_bitmap_size, GFP_KERNEL);\n\tif (!free_i->free_secmap)\n\t\treturn -ENOMEM;\n\n\t \n\tmemset(free_i->free_segmap, 0xff, bitmap_size);\n\tmemset(free_i->free_secmap, 0xff, sec_bitmap_size);\n\n\t \n\tfree_i->start_segno = GET_SEGNO_FROM_SEG0(sbi, MAIN_BLKADDR(sbi));\n\tfree_i->free_segments = 0;\n\tfree_i->free_sections = 0;\n\tspin_lock_init(&free_i->segmap_lock);\n\treturn 0;\n}\n\nstatic int build_curseg(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *array;\n\tint i;\n\n\tarray = f2fs_kzalloc(sbi, array_size(NR_CURSEG_TYPE,\n\t\t\t\t\tsizeof(*array)), GFP_KERNEL);\n\tif (!array)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->curseg_array = array;\n\n\tfor (i = 0; i < NO_CHECK_TYPE; i++) {\n\t\tmutex_init(&array[i].curseg_mutex);\n\t\tarray[i].sum_blk = f2fs_kzalloc(sbi, PAGE_SIZE, GFP_KERNEL);\n\t\tif (!array[i].sum_blk)\n\t\t\treturn -ENOMEM;\n\t\tinit_rwsem(&array[i].journal_rwsem);\n\t\tarray[i].journal = f2fs_kzalloc(sbi,\n\t\t\t\tsizeof(struct f2fs_journal), GFP_KERNEL);\n\t\tif (!array[i].journal)\n\t\t\treturn -ENOMEM;\n\t\tif (i < NR_PERSISTENT_LOG)\n\t\t\tarray[i].seg_type = CURSEG_HOT_DATA + i;\n\t\telse if (i == CURSEG_COLD_DATA_PINNED)\n\t\t\tarray[i].seg_type = CURSEG_COLD_DATA;\n\t\telse if (i == CURSEG_ALL_DATA_ATGC)\n\t\t\tarray[i].seg_type = CURSEG_COLD_DATA;\n\t\tarray[i].segno = NULL_SEGNO;\n\t\tarray[i].next_blkoff = 0;\n\t\tarray[i].inited = false;\n\t}\n\treturn restore_curseg_summaries(sbi);\n}\n\nstatic int build_sit_entries(struct f2fs_sb_info *sbi)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tstruct curseg_info *curseg = CURSEG_I(sbi, CURSEG_COLD_DATA);\n\tstruct f2fs_journal *journal = curseg->journal;\n\tstruct seg_entry *se;\n\tstruct f2fs_sit_entry sit;\n\tint sit_blk_cnt = SIT_BLK_CNT(sbi);\n\tunsigned int i, start, end;\n\tunsigned int readed, start_blk = 0;\n\tint err = 0;\n\tblock_t sit_valid_blocks[2] = {0, 0};\n\n\tdo {\n\t\treaded = f2fs_ra_meta_pages(sbi, start_blk, BIO_MAX_VECS,\n\t\t\t\t\t\t\tMETA_SIT, true);\n\n\t\tstart = start_blk * sit_i->sents_per_block;\n\t\tend = (start_blk + readed) * sit_i->sents_per_block;\n\n\t\tfor (; start < end && start < MAIN_SEGS(sbi); start++) {\n\t\t\tstruct f2fs_sit_block *sit_blk;\n\t\t\tstruct page *page;\n\n\t\t\tse = &sit_i->sentries[start];\n\t\t\tpage = get_current_sit_page(sbi, start);\n\t\t\tif (IS_ERR(page))\n\t\t\t\treturn PTR_ERR(page);\n\t\t\tsit_blk = (struct f2fs_sit_block *)page_address(page);\n\t\t\tsit = sit_blk->entries[SIT_ENTRY_OFFSET(sit_i, start)];\n\t\t\tf2fs_put_page(page, 1);\n\n\t\t\terr = check_block_count(sbi, start, &sit);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t\tseg_info_from_raw_sit(se, &sit);\n\n\t\t\tif (se->type >= NR_PERSISTENT_LOG) {\n\t\t\t\tf2fs_err(sbi, \"Invalid segment type: %u, segno: %u\",\n\t\t\t\t\t\t\tse->type, start);\n\t\t\t\tf2fs_handle_error(sbi,\n\t\t\t\t\t\tERROR_INCONSISTENT_SUM_TYPE);\n\t\t\t\treturn -EFSCORRUPTED;\n\t\t\t}\n\n\t\t\tsit_valid_blocks[SE_PAGETYPE(se)] += se->valid_blocks;\n\n\t\t\tif (f2fs_block_unit_discard(sbi)) {\n\t\t\t\t \n\t\t\t\tif (is_set_ckpt_flags(sbi, CP_TRIMMED_FLAG)) {\n\t\t\t\t\tmemset(se->discard_map, 0xff,\n\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t\t} else {\n\t\t\t\t\tmemcpy(se->discard_map,\n\t\t\t\t\t\tse->cur_valid_map,\n\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t\t\tsbi->discard_blks +=\n\t\t\t\t\t\tsbi->blocks_per_seg -\n\t\t\t\t\t\tse->valid_blocks;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (__is_large_section(sbi))\n\t\t\t\tget_sec_entry(sbi, start)->valid_blocks +=\n\t\t\t\t\t\t\tse->valid_blocks;\n\t\t}\n\t\tstart_blk += readed;\n\t} while (start_blk < sit_blk_cnt);\n\n\tdown_read(&curseg->journal_rwsem);\n\tfor (i = 0; i < sits_in_cursum(journal); i++) {\n\t\tunsigned int old_valid_blocks;\n\n\t\tstart = le32_to_cpu(segno_in_journal(journal, i));\n\t\tif (start >= MAIN_SEGS(sbi)) {\n\t\t\tf2fs_err(sbi, \"Wrong journal entry on segno %u\",\n\t\t\t\t start);\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tf2fs_handle_error(sbi, ERROR_CORRUPTED_JOURNAL);\n\t\t\tbreak;\n\t\t}\n\n\t\tse = &sit_i->sentries[start];\n\t\tsit = sit_in_journal(journal, i);\n\n\t\told_valid_blocks = se->valid_blocks;\n\n\t\tsit_valid_blocks[SE_PAGETYPE(se)] -= old_valid_blocks;\n\n\t\terr = check_block_count(sbi, start, &sit);\n\t\tif (err)\n\t\t\tbreak;\n\t\tseg_info_from_raw_sit(se, &sit);\n\n\t\tif (se->type >= NR_PERSISTENT_LOG) {\n\t\t\tf2fs_err(sbi, \"Invalid segment type: %u, segno: %u\",\n\t\t\t\t\t\t\tse->type, start);\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tf2fs_handle_error(sbi, ERROR_INCONSISTENT_SUM_TYPE);\n\t\t\tbreak;\n\t\t}\n\n\t\tsit_valid_blocks[SE_PAGETYPE(se)] += se->valid_blocks;\n\n\t\tif (f2fs_block_unit_discard(sbi)) {\n\t\t\tif (is_set_ckpt_flags(sbi, CP_TRIMMED_FLAG)) {\n\t\t\t\tmemset(se->discard_map, 0xff, SIT_VBLOCK_MAP_SIZE);\n\t\t\t} else {\n\t\t\t\tmemcpy(se->discard_map, se->cur_valid_map,\n\t\t\t\t\t\t\tSIT_VBLOCK_MAP_SIZE);\n\t\t\t\tsbi->discard_blks += old_valid_blocks;\n\t\t\t\tsbi->discard_blks -= se->valid_blocks;\n\t\t\t}\n\t\t}\n\n\t\tif (__is_large_section(sbi)) {\n\t\t\tget_sec_entry(sbi, start)->valid_blocks +=\n\t\t\t\t\t\t\tse->valid_blocks;\n\t\t\tget_sec_entry(sbi, start)->valid_blocks -=\n\t\t\t\t\t\t\told_valid_blocks;\n\t\t}\n\t}\n\tup_read(&curseg->journal_rwsem);\n\n\tif (err)\n\t\treturn err;\n\n\tif (sit_valid_blocks[NODE] != valid_node_count(sbi)) {\n\t\tf2fs_err(sbi, \"SIT is corrupted node# %u vs %u\",\n\t\t\t sit_valid_blocks[NODE], valid_node_count(sbi));\n\t\tf2fs_handle_error(sbi, ERROR_INCONSISTENT_NODE_COUNT);\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\tif (sit_valid_blocks[DATA] + sit_valid_blocks[NODE] >\n\t\t\t\tvalid_user_blocks(sbi)) {\n\t\tf2fs_err(sbi, \"SIT is corrupted data# %u %u vs %u\",\n\t\t\t sit_valid_blocks[DATA], sit_valid_blocks[NODE],\n\t\t\t valid_user_blocks(sbi));\n\t\tf2fs_handle_error(sbi, ERROR_INCONSISTENT_BLOCK_COUNT);\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\treturn 0;\n}\n\nstatic void init_free_segmap(struct f2fs_sb_info *sbi)\n{\n\tunsigned int start;\n\tint type;\n\tstruct seg_entry *sentry;\n\n\tfor (start = 0; start < MAIN_SEGS(sbi); start++) {\n\t\tif (f2fs_usable_blks_in_seg(sbi, start) == 0)\n\t\t\tcontinue;\n\t\tsentry = get_seg_entry(sbi, start);\n\t\tif (!sentry->valid_blocks)\n\t\t\t__set_free(sbi, start);\n\t\telse\n\t\t\tSIT_I(sbi)->written_valid_blocks +=\n\t\t\t\t\t\tsentry->valid_blocks;\n\t}\n\n\t \n\tfor (type = CURSEG_HOT_DATA; type <= CURSEG_COLD_NODE; type++) {\n\t\tstruct curseg_info *curseg_t = CURSEG_I(sbi, type);\n\n\t\t__set_test_and_inuse(sbi, curseg_t->segno);\n\t}\n}\n\nstatic void init_dirty_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tstruct free_segmap_info *free_i = FREE_I(sbi);\n\tunsigned int segno = 0, offset = 0, secno;\n\tblock_t valid_blocks, usable_blks_in_seg;\n\n\twhile (1) {\n\t\t \n\t\tsegno = find_next_inuse(free_i, MAIN_SEGS(sbi), offset);\n\t\tif (segno >= MAIN_SEGS(sbi))\n\t\t\tbreak;\n\t\toffset = segno + 1;\n\t\tvalid_blocks = get_valid_blocks(sbi, segno, false);\n\t\tusable_blks_in_seg = f2fs_usable_blks_in_seg(sbi, segno);\n\t\tif (valid_blocks == usable_blks_in_seg || !valid_blocks)\n\t\t\tcontinue;\n\t\tif (valid_blocks > usable_blks_in_seg) {\n\t\t\tf2fs_bug_on(sbi, 1);\n\t\t\tcontinue;\n\t\t}\n\t\tmutex_lock(&dirty_i->seglist_lock);\n\t\t__locate_dirty_segment(sbi, segno, DIRTY);\n\t\tmutex_unlock(&dirty_i->seglist_lock);\n\t}\n\n\tif (!__is_large_section(sbi))\n\t\treturn;\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\tfor (segno = 0; segno < MAIN_SEGS(sbi); segno += sbi->segs_per_sec) {\n\t\tvalid_blocks = get_valid_blocks(sbi, segno, true);\n\t\tsecno = GET_SEC_FROM_SEG(sbi, segno);\n\n\t\tif (!valid_blocks || valid_blocks == CAP_BLKS_PER_SEC(sbi))\n\t\t\tcontinue;\n\t\tif (IS_CURSEC(sbi, secno))\n\t\t\tcontinue;\n\t\tset_bit(secno, dirty_i->dirty_secmap);\n\t}\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\nstatic int init_victim_secmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tunsigned int bitmap_size = f2fs_bitmap_size(MAIN_SECS(sbi));\n\n\tdirty_i->victim_secmap = f2fs_kvzalloc(sbi, bitmap_size, GFP_KERNEL);\n\tif (!dirty_i->victim_secmap)\n\t\treturn -ENOMEM;\n\n\tdirty_i->pinned_secmap = f2fs_kvzalloc(sbi, bitmap_size, GFP_KERNEL);\n\tif (!dirty_i->pinned_secmap)\n\t\treturn -ENOMEM;\n\n\tdirty_i->pinned_secmap_cnt = 0;\n\tdirty_i->enable_pin_section = true;\n\treturn 0;\n}\n\nstatic int build_dirty_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i;\n\tunsigned int bitmap_size, i;\n\n\t \n\tdirty_i = f2fs_kzalloc(sbi, sizeof(struct dirty_seglist_info),\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n\tif (!dirty_i)\n\t\treturn -ENOMEM;\n\n\tSM_I(sbi)->dirty_info = dirty_i;\n\tmutex_init(&dirty_i->seglist_lock);\n\n\tbitmap_size = f2fs_bitmap_size(MAIN_SEGS(sbi));\n\n\tfor (i = 0; i < NR_DIRTY_TYPE; i++) {\n\t\tdirty_i->dirty_segmap[i] = f2fs_kvzalloc(sbi, bitmap_size,\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n\t\tif (!dirty_i->dirty_segmap[i])\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (__is_large_section(sbi)) {\n\t\tbitmap_size = f2fs_bitmap_size(MAIN_SECS(sbi));\n\t\tdirty_i->dirty_secmap = f2fs_kvzalloc(sbi,\n\t\t\t\t\t\tbitmap_size, GFP_KERNEL);\n\t\tif (!dirty_i->dirty_secmap)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tinit_dirty_segmap(sbi);\n\treturn init_victim_secmap(sbi);\n}\n\nstatic int sanity_check_curseg(struct f2fs_sb_info *sbi)\n{\n\tint i;\n\n\t \n\tfor (i = 0; i < NR_PERSISTENT_LOG; i++) {\n\t\tstruct curseg_info *curseg = CURSEG_I(sbi, i);\n\t\tstruct seg_entry *se = get_seg_entry(sbi, curseg->segno);\n\t\tunsigned int blkofs = curseg->next_blkoff;\n\n\t\tif (f2fs_sb_has_readonly(sbi) &&\n\t\t\ti != CURSEG_HOT_DATA && i != CURSEG_HOT_NODE)\n\t\t\tcontinue;\n\n\t\tsanity_check_seg_type(sbi, curseg->seg_type);\n\n\t\tif (curseg->alloc_type != LFS && curseg->alloc_type != SSR) {\n\t\t\tf2fs_err(sbi,\n\t\t\t\t \"Current segment has invalid alloc_type:%d\",\n\t\t\t\t curseg->alloc_type);\n\t\t\tf2fs_handle_error(sbi, ERROR_INVALID_CURSEG);\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\n\t\tif (f2fs_test_bit(blkofs, se->cur_valid_map))\n\t\t\tgoto out;\n\n\t\tif (curseg->alloc_type == SSR)\n\t\t\tcontinue;\n\n\t\tfor (blkofs += 1; blkofs < sbi->blocks_per_seg; blkofs++) {\n\t\t\tif (!f2fs_test_bit(blkofs, se->cur_valid_map))\n\t\t\t\tcontinue;\nout:\n\t\t\tf2fs_err(sbi,\n\t\t\t\t \"Current segment's next free block offset is inconsistent with bitmap, logtype:%u, segno:%u, type:%u, next_blkoff:%u, blkofs:%u\",\n\t\t\t\t i, curseg->segno, curseg->alloc_type,\n\t\t\t\t curseg->next_blkoff, blkofs);\n\t\t\tf2fs_handle_error(sbi, ERROR_INVALID_CURSEG);\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\t}\n\treturn 0;\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\n\nstatic int check_zone_write_pointer(struct f2fs_sb_info *sbi,\n\t\t\t\t    struct f2fs_dev_info *fdev,\n\t\t\t\t    struct blk_zone *zone)\n{\n\tunsigned int wp_segno, wp_blkoff, zone_secno, zone_segno, segno;\n\tblock_t zone_block, wp_block, last_valid_block;\n\tunsigned int log_sectors_per_block = sbi->log_blocksize - SECTOR_SHIFT;\n\tint i, s, b, ret;\n\tstruct seg_entry *se;\n\n\tif (zone->type != BLK_ZONE_TYPE_SEQWRITE_REQ)\n\t\treturn 0;\n\n\twp_block = fdev->start_blk + (zone->wp >> log_sectors_per_block);\n\twp_segno = GET_SEGNO(sbi, wp_block);\n\twp_blkoff = wp_block - START_BLOCK(sbi, wp_segno);\n\tzone_block = fdev->start_blk + (zone->start >> log_sectors_per_block);\n\tzone_segno = GET_SEGNO(sbi, zone_block);\n\tzone_secno = GET_SEC_FROM_SEG(sbi, zone_segno);\n\n\tif (zone_segno >= MAIN_SEGS(sbi))\n\t\treturn 0;\n\n\t \n\tfor (i = 0; i < NO_CHECK_TYPE; i++)\n\t\tif (zone_secno == GET_SEC_FROM_SEG(sbi,\n\t\t\t\t\t\t   CURSEG_I(sbi, i)->segno))\n\t\t\treturn 0;\n\n\t \n\tlast_valid_block = zone_block - 1;\n\tfor (s = sbi->segs_per_sec - 1; s >= 0; s--) {\n\t\tsegno = zone_segno + s;\n\t\tse = get_seg_entry(sbi, segno);\n\t\tfor (b = sbi->blocks_per_seg - 1; b >= 0; b--)\n\t\t\tif (f2fs_test_bit(b, se->cur_valid_map)) {\n\t\t\t\tlast_valid_block = START_BLOCK(sbi, segno) + b;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tif (last_valid_block >= zone_block)\n\t\t\tbreak;\n\t}\n\n\t \n\tif ((last_valid_block + 1 == wp_block) ||\n\t\t\t(zone->wp == zone->start + zone->len))\n\t\treturn 0;\n\n\tif (last_valid_block + 1 == zone_block) {\n\t\t \n\t\tf2fs_notice(sbi,\n\t\t\t    \"Zone without valid block has non-zero write \"\n\t\t\t    \"pointer. Reset the write pointer: wp[0x%x,0x%x]\",\n\t\t\t    wp_segno, wp_blkoff);\n\t\tret = __f2fs_issue_discard_zone(sbi, fdev->bdev, zone_block,\n\t\t\t\t\tzone->len >> log_sectors_per_block);\n\t\tif (ret)\n\t\t\tf2fs_err(sbi, \"Discard zone failed: %s (errno=%d)\",\n\t\t\t\t fdev->path, ret);\n\n\t\treturn ret;\n\t}\n\n\t \n\tf2fs_notice(sbi, \"Valid blocks are not aligned with write pointer: \"\n\t\t    \"valid block[0x%x,0x%x] wp[0x%x,0x%x]\",\n\t\t    GET_SEGNO(sbi, last_valid_block),\n\t\t    GET_BLKOFF_FROM_SEG0(sbi, last_valid_block),\n\t\t    wp_segno, wp_blkoff);\n\n\tret = blkdev_zone_mgmt(fdev->bdev, REQ_OP_ZONE_FINISH,\n\t\t\t\tzone->start, zone->len, GFP_NOFS);\n\tif (ret == -EOPNOTSUPP) {\n\t\tret = blkdev_issue_zeroout(fdev->bdev, zone->wp,\n\t\t\t\t\tzone->len - (zone->wp - zone->start),\n\t\t\t\t\tGFP_NOFS, 0);\n\t\tif (ret)\n\t\t\tf2fs_err(sbi, \"Fill up zone failed: %s (errno=%d)\",\n\t\t\t\t\tfdev->path, ret);\n\t} else if (ret) {\n\t\tf2fs_err(sbi, \"Finishing zone failed: %s (errno=%d)\",\n\t\t\t\tfdev->path, ret);\n\t}\n\n\treturn ret;\n}\n\nstatic struct f2fs_dev_info *get_target_zoned_dev(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\t  block_t zone_blkaddr)\n{\n\tint i;\n\n\tfor (i = 0; i < sbi->s_ndevs; i++) {\n\t\tif (!bdev_is_zoned(FDEV(i).bdev))\n\t\t\tcontinue;\n\t\tif (sbi->s_ndevs == 1 || (FDEV(i).start_blk <= zone_blkaddr &&\n\t\t\t\tzone_blkaddr <= FDEV(i).end_blk))\n\t\t\treturn &FDEV(i);\n\t}\n\n\treturn NULL;\n}\n\nstatic int report_one_zone_cb(struct blk_zone *zone, unsigned int idx,\n\t\t\t      void *data)\n{\n\tmemcpy(data, zone, sizeof(struct blk_zone));\n\treturn 0;\n}\n\nstatic int fix_curseg_write_pointer(struct f2fs_sb_info *sbi, int type)\n{\n\tstruct curseg_info *cs = CURSEG_I(sbi, type);\n\tstruct f2fs_dev_info *zbd;\n\tstruct blk_zone zone;\n\tunsigned int cs_section, wp_segno, wp_blkoff, wp_sector_off;\n\tblock_t cs_zone_block, wp_block;\n\tunsigned int log_sectors_per_block = sbi->log_blocksize - SECTOR_SHIFT;\n\tsector_t zone_sector;\n\tint err;\n\n\tcs_section = GET_SEC_FROM_SEG(sbi, cs->segno);\n\tcs_zone_block = START_BLOCK(sbi, GET_SEG_FROM_SEC(sbi, cs_section));\n\n\tzbd = get_target_zoned_dev(sbi, cs_zone_block);\n\tif (!zbd)\n\t\treturn 0;\n\n\t \n\tzone_sector = (sector_t)(cs_zone_block - zbd->start_blk)\n\t\t<< log_sectors_per_block;\n\terr = blkdev_report_zones(zbd->bdev, zone_sector, 1,\n\t\t\t\t  report_one_zone_cb, &zone);\n\tif (err != 1) {\n\t\tf2fs_err(sbi, \"Report zone failed: %s errno=(%d)\",\n\t\t\t zbd->path, err);\n\t\treturn err;\n\t}\n\n\tif (zone.type != BLK_ZONE_TYPE_SEQWRITE_REQ)\n\t\treturn 0;\n\n\twp_block = zbd->start_blk + (zone.wp >> log_sectors_per_block);\n\twp_segno = GET_SEGNO(sbi, wp_block);\n\twp_blkoff = wp_block - START_BLOCK(sbi, wp_segno);\n\twp_sector_off = zone.wp & GENMASK(log_sectors_per_block - 1, 0);\n\n\tif (cs->segno == wp_segno && cs->next_blkoff == wp_blkoff &&\n\t\twp_sector_off == 0)\n\t\treturn 0;\n\n\tf2fs_notice(sbi, \"Unaligned curseg[%d] with write pointer: \"\n\t\t    \"curseg[0x%x,0x%x] wp[0x%x,0x%x]\",\n\t\t    type, cs->segno, cs->next_blkoff, wp_segno, wp_blkoff);\n\n\tf2fs_notice(sbi, \"Assign new section to curseg[%d]: \"\n\t\t    \"curseg[0x%x,0x%x]\", type, cs->segno, cs->next_blkoff);\n\n\tf2fs_allocate_new_section(sbi, type, true);\n\n\t \n\tif (check_zone_write_pointer(sbi, zbd, &zone))\n\t\treturn -EIO;\n\n\t \n\tcs_section = GET_SEC_FROM_SEG(sbi, cs->segno);\n\tcs_zone_block = START_BLOCK(sbi, GET_SEG_FROM_SEC(sbi, cs_section));\n\n\tzbd = get_target_zoned_dev(sbi, cs_zone_block);\n\tif (!zbd)\n\t\treturn 0;\n\n\tzone_sector = (sector_t)(cs_zone_block - zbd->start_blk)\n\t\t<< log_sectors_per_block;\n\terr = blkdev_report_zones(zbd->bdev, zone_sector, 1,\n\t\t\t\t  report_one_zone_cb, &zone);\n\tif (err != 1) {\n\t\tf2fs_err(sbi, \"Report zone failed: %s errno=(%d)\",\n\t\t\t zbd->path, err);\n\t\treturn err;\n\t}\n\n\tif (zone.type != BLK_ZONE_TYPE_SEQWRITE_REQ)\n\t\treturn 0;\n\n\tif (zone.wp != zone.start) {\n\t\tf2fs_notice(sbi,\n\t\t\t    \"New zone for curseg[%d] is not yet discarded. \"\n\t\t\t    \"Reset the zone: curseg[0x%x,0x%x]\",\n\t\t\t    type, cs->segno, cs->next_blkoff);\n\t\terr = __f2fs_issue_discard_zone(sbi, zbd->bdev,\tcs_zone_block,\n\t\t\t\t\tzone.len >> log_sectors_per_block);\n\t\tif (err) {\n\t\t\tf2fs_err(sbi, \"Discard zone failed: %s (errno=%d)\",\n\t\t\t\t zbd->path, err);\n\t\t\treturn err;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint f2fs_fix_curseg_write_pointer(struct f2fs_sb_info *sbi)\n{\n\tint i, ret;\n\n\tfor (i = 0; i < NR_PERSISTENT_LOG; i++) {\n\t\tret = fix_curseg_write_pointer(sbi, i);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\nstruct check_zone_write_pointer_args {\n\tstruct f2fs_sb_info *sbi;\n\tstruct f2fs_dev_info *fdev;\n};\n\nstatic int check_zone_write_pointer_cb(struct blk_zone *zone, unsigned int idx,\n\t\t\t\t      void *data)\n{\n\tstruct check_zone_write_pointer_args *args;\n\n\targs = (struct check_zone_write_pointer_args *)data;\n\n\treturn check_zone_write_pointer(args->sbi, args->fdev, zone);\n}\n\nint f2fs_check_write_pointer(struct f2fs_sb_info *sbi)\n{\n\tint i, ret;\n\tstruct check_zone_write_pointer_args args;\n\n\tfor (i = 0; i < sbi->s_ndevs; i++) {\n\t\tif (!bdev_is_zoned(FDEV(i).bdev))\n\t\t\tcontinue;\n\n\t\targs.sbi = sbi;\n\t\targs.fdev = &FDEV(i);\n\t\tret = blkdev_report_zones(FDEV(i).bdev, 0, BLK_ALL_ZONES,\n\t\t\t\t\t  check_zone_write_pointer_cb, &args);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\treturn 0;\n}\n\n \nstatic inline unsigned int f2fs_usable_zone_blks_in_seg(\n\t\t\tstruct f2fs_sb_info *sbi, unsigned int segno)\n{\n\tblock_t seg_start, sec_start_blkaddr, sec_cap_blkaddr;\n\tunsigned int secno;\n\n\tif (!sbi->unusable_blocks_per_sec)\n\t\treturn sbi->blocks_per_seg;\n\n\tsecno = GET_SEC_FROM_SEG(sbi, segno);\n\tseg_start = START_BLOCK(sbi, segno);\n\tsec_start_blkaddr = START_BLOCK(sbi, GET_SEG_FROM_SEC(sbi, secno));\n\tsec_cap_blkaddr = sec_start_blkaddr + CAP_BLKS_PER_SEC(sbi);\n\n\t \n\tif (seg_start >= sec_cap_blkaddr)\n\t\treturn 0;\n\tif (seg_start + sbi->blocks_per_seg > sec_cap_blkaddr)\n\t\treturn sec_cap_blkaddr - seg_start;\n\n\treturn sbi->blocks_per_seg;\n}\n#else\nint f2fs_fix_curseg_write_pointer(struct f2fs_sb_info *sbi)\n{\n\treturn 0;\n}\n\nint f2fs_check_write_pointer(struct f2fs_sb_info *sbi)\n{\n\treturn 0;\n}\n\nstatic inline unsigned int f2fs_usable_zone_blks_in_seg(struct f2fs_sb_info *sbi,\n\t\t\t\t\t\t\tunsigned int segno)\n{\n\treturn 0;\n}\n\n#endif\nunsigned int f2fs_usable_blks_in_seg(struct f2fs_sb_info *sbi,\n\t\t\t\t\tunsigned int segno)\n{\n\tif (f2fs_sb_has_blkzoned(sbi))\n\t\treturn f2fs_usable_zone_blks_in_seg(sbi, segno);\n\n\treturn sbi->blocks_per_seg;\n}\n\nunsigned int f2fs_usable_segs_in_sec(struct f2fs_sb_info *sbi,\n\t\t\t\t\tunsigned int segno)\n{\n\tif (f2fs_sb_has_blkzoned(sbi))\n\t\treturn CAP_SEGS_PER_SEC(sbi);\n\n\treturn sbi->segs_per_sec;\n}\n\n \nstatic void init_min_max_mtime(struct f2fs_sb_info *sbi)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\tunsigned int segno;\n\n\tdown_write(&sit_i->sentry_lock);\n\n\tsit_i->min_mtime = ULLONG_MAX;\n\n\tfor (segno = 0; segno < MAIN_SEGS(sbi); segno += sbi->segs_per_sec) {\n\t\tunsigned int i;\n\t\tunsigned long long mtime = 0;\n\n\t\tfor (i = 0; i < sbi->segs_per_sec; i++)\n\t\t\tmtime += get_seg_entry(sbi, segno + i)->mtime;\n\n\t\tmtime = div_u64(mtime, sbi->segs_per_sec);\n\n\t\tif (sit_i->min_mtime > mtime)\n\t\t\tsit_i->min_mtime = mtime;\n\t}\n\tsit_i->max_mtime = get_mtime(sbi, false);\n\tsit_i->dirty_max_mtime = 0;\n\tup_write(&sit_i->sentry_lock);\n}\n\nint f2fs_build_segment_manager(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);\n\tstruct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);\n\tstruct f2fs_sm_info *sm_info;\n\tint err;\n\n\tsm_info = f2fs_kzalloc(sbi, sizeof(struct f2fs_sm_info), GFP_KERNEL);\n\tif (!sm_info)\n\t\treturn -ENOMEM;\n\n\t \n\tsbi->sm_info = sm_info;\n\tsm_info->seg0_blkaddr = le32_to_cpu(raw_super->segment0_blkaddr);\n\tsm_info->main_blkaddr = le32_to_cpu(raw_super->main_blkaddr);\n\tsm_info->segment_count = le32_to_cpu(raw_super->segment_count);\n\tsm_info->reserved_segments = le32_to_cpu(ckpt->rsvd_segment_count);\n\tsm_info->ovp_segments = le32_to_cpu(ckpt->overprov_segment_count);\n\tsm_info->main_segments = le32_to_cpu(raw_super->segment_count_main);\n\tsm_info->ssa_blkaddr = le32_to_cpu(raw_super->ssa_blkaddr);\n\tsm_info->rec_prefree_segments = sm_info->main_segments *\n\t\t\t\t\tDEF_RECLAIM_PREFREE_SEGMENTS / 100;\n\tif (sm_info->rec_prefree_segments > DEF_MAX_RECLAIM_PREFREE_SEGMENTS)\n\t\tsm_info->rec_prefree_segments = DEF_MAX_RECLAIM_PREFREE_SEGMENTS;\n\n\tif (!f2fs_lfs_mode(sbi))\n\t\tsm_info->ipu_policy = BIT(F2FS_IPU_FSYNC);\n\tsm_info->min_ipu_util = DEF_MIN_IPU_UTIL;\n\tsm_info->min_fsync_blocks = DEF_MIN_FSYNC_BLOCKS;\n\tsm_info->min_seq_blocks = sbi->blocks_per_seg;\n\tsm_info->min_hot_blocks = DEF_MIN_HOT_BLOCKS;\n\tsm_info->min_ssr_sections = reserved_sections(sbi);\n\n\tINIT_LIST_HEAD(&sm_info->sit_entry_set);\n\n\tinit_f2fs_rwsem(&sm_info->curseg_lock);\n\n\terr = f2fs_create_flush_cmd_control(sbi);\n\tif (err)\n\t\treturn err;\n\n\terr = create_discard_cmd_control(sbi);\n\tif (err)\n\t\treturn err;\n\n\terr = build_sit_info(sbi);\n\tif (err)\n\t\treturn err;\n\terr = build_free_segmap(sbi);\n\tif (err)\n\t\treturn err;\n\terr = build_curseg(sbi);\n\tif (err)\n\t\treturn err;\n\n\t \n\terr = build_sit_entries(sbi);\n\tif (err)\n\t\treturn err;\n\n\tinit_free_segmap(sbi);\n\terr = build_dirty_segmap(sbi);\n\tif (err)\n\t\treturn err;\n\n\terr = sanity_check_curseg(sbi);\n\tif (err)\n\t\treturn err;\n\n\tinit_min_max_mtime(sbi);\n\treturn 0;\n}\n\nstatic void discard_dirty_segmap(struct f2fs_sb_info *sbi,\n\t\tenum dirty_type dirty_type)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\n\tmutex_lock(&dirty_i->seglist_lock);\n\tkvfree(dirty_i->dirty_segmap[dirty_type]);\n\tdirty_i->nr_dirty[dirty_type] = 0;\n\tmutex_unlock(&dirty_i->seglist_lock);\n}\n\nstatic void destroy_victim_secmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\n\tkvfree(dirty_i->pinned_secmap);\n\tkvfree(dirty_i->victim_secmap);\n}\n\nstatic void destroy_dirty_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct dirty_seglist_info *dirty_i = DIRTY_I(sbi);\n\tint i;\n\n\tif (!dirty_i)\n\t\treturn;\n\n\t \n\tfor (i = 0; i < NR_DIRTY_TYPE; i++)\n\t\tdiscard_dirty_segmap(sbi, i);\n\n\tif (__is_large_section(sbi)) {\n\t\tmutex_lock(&dirty_i->seglist_lock);\n\t\tkvfree(dirty_i->dirty_secmap);\n\t\tmutex_unlock(&dirty_i->seglist_lock);\n\t}\n\n\tdestroy_victim_secmap(sbi);\n\tSM_I(sbi)->dirty_info = NULL;\n\tkfree(dirty_i);\n}\n\nstatic void destroy_curseg(struct f2fs_sb_info *sbi)\n{\n\tstruct curseg_info *array = SM_I(sbi)->curseg_array;\n\tint i;\n\n\tif (!array)\n\t\treturn;\n\tSM_I(sbi)->curseg_array = NULL;\n\tfor (i = 0; i < NR_CURSEG_TYPE; i++) {\n\t\tkfree(array[i].sum_blk);\n\t\tkfree(array[i].journal);\n\t}\n\tkfree(array);\n}\n\nstatic void destroy_free_segmap(struct f2fs_sb_info *sbi)\n{\n\tstruct free_segmap_info *free_i = SM_I(sbi)->free_info;\n\n\tif (!free_i)\n\t\treturn;\n\tSM_I(sbi)->free_info = NULL;\n\tkvfree(free_i->free_segmap);\n\tkvfree(free_i->free_secmap);\n\tkfree(free_i);\n}\n\nstatic void destroy_sit_info(struct f2fs_sb_info *sbi)\n{\n\tstruct sit_info *sit_i = SIT_I(sbi);\n\n\tif (!sit_i)\n\t\treturn;\n\n\tif (sit_i->sentries)\n\t\tkvfree(sit_i->bitmap);\n\tkfree(sit_i->tmp_map);\n\n\tkvfree(sit_i->sentries);\n\tkvfree(sit_i->sec_entries);\n\tkvfree(sit_i->dirty_sentries_bitmap);\n\n\tSM_I(sbi)->sit_info = NULL;\n\tkvfree(sit_i->sit_bitmap);\n#ifdef CONFIG_F2FS_CHECK_FS\n\tkvfree(sit_i->sit_bitmap_mir);\n\tkvfree(sit_i->invalid_segmap);\n#endif\n\tkfree(sit_i);\n}\n\nvoid f2fs_destroy_segment_manager(struct f2fs_sb_info *sbi)\n{\n\tstruct f2fs_sm_info *sm_info = SM_I(sbi);\n\n\tif (!sm_info)\n\t\treturn;\n\tf2fs_destroy_flush_cmd_control(sbi, true);\n\tdestroy_discard_cmd_control(sbi);\n\tdestroy_dirty_segmap(sbi);\n\tdestroy_curseg(sbi);\n\tdestroy_free_segmap(sbi);\n\tdestroy_sit_info(sbi);\n\tsbi->sm_info = NULL;\n\tkfree(sm_info);\n}\n\nint __init f2fs_create_segment_manager_caches(void)\n{\n\tdiscard_entry_slab = f2fs_kmem_cache_create(\"f2fs_discard_entry\",\n\t\t\tsizeof(struct discard_entry));\n\tif (!discard_entry_slab)\n\t\tgoto fail;\n\n\tdiscard_cmd_slab = f2fs_kmem_cache_create(\"f2fs_discard_cmd\",\n\t\t\tsizeof(struct discard_cmd));\n\tif (!discard_cmd_slab)\n\t\tgoto destroy_discard_entry;\n\n\tsit_entry_set_slab = f2fs_kmem_cache_create(\"f2fs_sit_entry_set\",\n\t\t\tsizeof(struct sit_entry_set));\n\tif (!sit_entry_set_slab)\n\t\tgoto destroy_discard_cmd;\n\n\trevoke_entry_slab = f2fs_kmem_cache_create(\"f2fs_revoke_entry\",\n\t\t\tsizeof(struct revoke_entry));\n\tif (!revoke_entry_slab)\n\t\tgoto destroy_sit_entry_set;\n\treturn 0;\n\ndestroy_sit_entry_set:\n\tkmem_cache_destroy(sit_entry_set_slab);\ndestroy_discard_cmd:\n\tkmem_cache_destroy(discard_cmd_slab);\ndestroy_discard_entry:\n\tkmem_cache_destroy(discard_entry_slab);\nfail:\n\treturn -ENOMEM;\n}\n\nvoid f2fs_destroy_segment_manager_caches(void)\n{\n\tkmem_cache_destroy(sit_entry_set_slab);\n\tkmem_cache_destroy(discard_cmd_slab);\n\tkmem_cache_destroy(discard_entry_slab);\n\tkmem_cache_destroy(revoke_entry_slab);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}