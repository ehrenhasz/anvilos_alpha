{
  "module_name": "data.c",
  "hash_id": "f517f878cfe9ae971e657d722b42db3587211d84960d136874db5161dd749da9",
  "original_prompt": "Ingested from linux-6.6.14/fs/f2fs/data.c",
  "human_readable_source": "\n \n#include <linux/fs.h>\n#include <linux/f2fs_fs.h>\n#include <linux/buffer_head.h>\n#include <linux/sched/mm.h>\n#include <linux/mpage.h>\n#include <linux/writeback.h>\n#include <linux/pagevec.h>\n#include <linux/blkdev.h>\n#include <linux/bio.h>\n#include <linux/blk-crypto.h>\n#include <linux/swap.h>\n#include <linux/prefetch.h>\n#include <linux/uio.h>\n#include <linux/sched/signal.h>\n#include <linux/fiemap.h>\n#include <linux/iomap.h>\n\n#include \"f2fs.h\"\n#include \"node.h\"\n#include \"segment.h\"\n#include \"iostat.h\"\n#include <trace/events/f2fs.h>\n\n#define NUM_PREALLOC_POST_READ_CTXS\t128\n\nstatic struct kmem_cache *bio_post_read_ctx_cache;\nstatic struct kmem_cache *bio_entry_slab;\nstatic mempool_t *bio_post_read_ctx_pool;\nstatic struct bio_set f2fs_bioset;\n\n#define\tF2FS_BIO_POOL_SIZE\tNR_CURSEG_TYPE\n\nint __init f2fs_init_bioset(void)\n{\n\treturn bioset_init(&f2fs_bioset, F2FS_BIO_POOL_SIZE,\n\t\t\t\t\t0, BIOSET_NEED_BVECS);\n}\n\nvoid f2fs_destroy_bioset(void)\n{\n\tbioset_exit(&f2fs_bioset);\n}\n\nstatic bool __is_cp_guaranteed(struct page *page)\n{\n\tstruct address_space *mapping = page->mapping;\n\tstruct inode *inode;\n\tstruct f2fs_sb_info *sbi;\n\n\tif (!mapping)\n\t\treturn false;\n\n\tinode = mapping->host;\n\tsbi = F2FS_I_SB(inode);\n\n\tif (inode->i_ino == F2FS_META_INO(sbi) ||\n\t\t\tinode->i_ino == F2FS_NODE_INO(sbi) ||\n\t\t\tS_ISDIR(inode->i_mode))\n\t\treturn true;\n\n\tif (f2fs_is_compressed_page(page))\n\t\treturn false;\n\tif ((S_ISREG(inode->i_mode) && IS_NOQUOTA(inode)) ||\n\t\t\tpage_private_gcing(page))\n\t\treturn true;\n\treturn false;\n}\n\nstatic enum count_type __read_io_type(struct page *page)\n{\n\tstruct address_space *mapping = page_file_mapping(page);\n\n\tif (mapping) {\n\t\tstruct inode *inode = mapping->host;\n\t\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\t\tif (inode->i_ino == F2FS_META_INO(sbi))\n\t\t\treturn F2FS_RD_META;\n\n\t\tif (inode->i_ino == F2FS_NODE_INO(sbi))\n\t\t\treturn F2FS_RD_NODE;\n\t}\n\treturn F2FS_RD_DATA;\n}\n\n \nenum bio_post_read_step {\n#ifdef CONFIG_FS_ENCRYPTION\n\tSTEP_DECRYPT\t= BIT(0),\n#else\n\tSTEP_DECRYPT\t= 0,\t \n#endif\n#ifdef CONFIG_F2FS_FS_COMPRESSION\n\tSTEP_DECOMPRESS\t= BIT(1),\n#else\n\tSTEP_DECOMPRESS\t= 0,\t \n#endif\n#ifdef CONFIG_FS_VERITY\n\tSTEP_VERITY\t= BIT(2),\n#else\n\tSTEP_VERITY\t= 0,\t \n#endif\n};\n\nstruct bio_post_read_ctx {\n\tstruct bio *bio;\n\tstruct f2fs_sb_info *sbi;\n\tstruct work_struct work;\n\tunsigned int enabled_steps;\n\t \n\tbool decompression_attempted;\n\tblock_t fs_blkaddr;\n};\n\n \nstatic void f2fs_finish_read_bio(struct bio *bio, bool in_task)\n{\n\tstruct bio_vec *bv;\n\tstruct bvec_iter_all iter_all;\n\tstruct bio_post_read_ctx *ctx = bio->bi_private;\n\n\tbio_for_each_segment_all(bv, bio, iter_all) {\n\t\tstruct page *page = bv->bv_page;\n\n\t\tif (f2fs_is_compressed_page(page)) {\n\t\t\tif (ctx && !ctx->decompression_attempted)\n\t\t\t\tf2fs_end_read_compressed_page(page, true, 0,\n\t\t\t\t\t\t\tin_task);\n\t\t\tf2fs_put_page_dic(page, in_task);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (bio->bi_status)\n\t\t\tClearPageUptodate(page);\n\t\telse\n\t\t\tSetPageUptodate(page);\n\t\tdec_page_count(F2FS_P_SB(page), __read_io_type(page));\n\t\tunlock_page(page);\n\t}\n\n\tif (ctx)\n\t\tmempool_free(ctx, bio_post_read_ctx_pool);\n\tbio_put(bio);\n}\n\nstatic void f2fs_verify_bio(struct work_struct *work)\n{\n\tstruct bio_post_read_ctx *ctx =\n\t\tcontainer_of(work, struct bio_post_read_ctx, work);\n\tstruct bio *bio = ctx->bio;\n\tbool may_have_compressed_pages = (ctx->enabled_steps & STEP_DECOMPRESS);\n\n\t \n\tmempool_free(ctx, bio_post_read_ctx_pool);\n\tbio->bi_private = NULL;\n\n\t \n\tif (may_have_compressed_pages) {\n\t\tstruct bio_vec *bv;\n\t\tstruct bvec_iter_all iter_all;\n\n\t\tbio_for_each_segment_all(bv, bio, iter_all) {\n\t\t\tstruct page *page = bv->bv_page;\n\n\t\t\tif (!f2fs_is_compressed_page(page) &&\n\t\t\t    !fsverity_verify_page(page)) {\n\t\t\t\tbio->bi_status = BLK_STS_IOERR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfsverity_verify_bio(bio);\n\t}\n\n\tf2fs_finish_read_bio(bio, true);\n}\n\n \nstatic void f2fs_verify_and_finish_bio(struct bio *bio, bool in_task)\n{\n\tstruct bio_post_read_ctx *ctx = bio->bi_private;\n\n\tif (ctx && (ctx->enabled_steps & STEP_VERITY)) {\n\t\tINIT_WORK(&ctx->work, f2fs_verify_bio);\n\t\tfsverity_enqueue_verify_work(&ctx->work);\n\t} else {\n\t\tf2fs_finish_read_bio(bio, in_task);\n\t}\n}\n\n \nstatic void f2fs_handle_step_decompress(struct bio_post_read_ctx *ctx,\n\t\tbool in_task)\n{\n\tstruct bio_vec *bv;\n\tstruct bvec_iter_all iter_all;\n\tbool all_compressed = true;\n\tblock_t blkaddr = ctx->fs_blkaddr;\n\n\tbio_for_each_segment_all(bv, ctx->bio, iter_all) {\n\t\tstruct page *page = bv->bv_page;\n\n\t\tif (f2fs_is_compressed_page(page))\n\t\t\tf2fs_end_read_compressed_page(page, false, blkaddr,\n\t\t\t\t\t\t      in_task);\n\t\telse\n\t\t\tall_compressed = false;\n\n\t\tblkaddr++;\n\t}\n\n\tctx->decompression_attempted = true;\n\n\t \n\tif (all_compressed)\n\t\tctx->enabled_steps &= ~STEP_VERITY;\n}\n\nstatic void f2fs_post_read_work(struct work_struct *work)\n{\n\tstruct bio_post_read_ctx *ctx =\n\t\tcontainer_of(work, struct bio_post_read_ctx, work);\n\tstruct bio *bio = ctx->bio;\n\n\tif ((ctx->enabled_steps & STEP_DECRYPT) && !fscrypt_decrypt_bio(bio)) {\n\t\tf2fs_finish_read_bio(bio, true);\n\t\treturn;\n\t}\n\n\tif (ctx->enabled_steps & STEP_DECOMPRESS)\n\t\tf2fs_handle_step_decompress(ctx, true);\n\n\tf2fs_verify_and_finish_bio(bio, true);\n}\n\nstatic void f2fs_read_end_io(struct bio *bio)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_P_SB(bio_first_page_all(bio));\n\tstruct bio_post_read_ctx *ctx;\n\tbool intask = in_task();\n\n\tiostat_update_and_unbind_ctx(bio);\n\tctx = bio->bi_private;\n\n\tif (time_to_inject(sbi, FAULT_READ_IO))\n\t\tbio->bi_status = BLK_STS_IOERR;\n\n\tif (bio->bi_status) {\n\t\tf2fs_finish_read_bio(bio, intask);\n\t\treturn;\n\t}\n\n\tif (ctx) {\n\t\tunsigned int enabled_steps = ctx->enabled_steps &\n\t\t\t\t\t(STEP_DECRYPT | STEP_DECOMPRESS);\n\n\t\t \n\t\tif (enabled_steps == STEP_DECOMPRESS &&\n\t\t\t\t!f2fs_low_mem_mode(sbi)) {\n\t\t\tf2fs_handle_step_decompress(ctx, intask);\n\t\t} else if (enabled_steps) {\n\t\t\tINIT_WORK(&ctx->work, f2fs_post_read_work);\n\t\t\tqueue_work(ctx->sbi->post_read_wq, &ctx->work);\n\t\t\treturn;\n\t\t}\n\t}\n\n\tf2fs_verify_and_finish_bio(bio, intask);\n}\n\nstatic void f2fs_write_end_io(struct bio *bio)\n{\n\tstruct f2fs_sb_info *sbi;\n\tstruct bio_vec *bvec;\n\tstruct bvec_iter_all iter_all;\n\n\tiostat_update_and_unbind_ctx(bio);\n\tsbi = bio->bi_private;\n\n\tif (time_to_inject(sbi, FAULT_WRITE_IO))\n\t\tbio->bi_status = BLK_STS_IOERR;\n\n\tbio_for_each_segment_all(bvec, bio, iter_all) {\n\t\tstruct page *page = bvec->bv_page;\n\t\tenum count_type type = WB_DATA_TYPE(page);\n\n\t\tif (page_private_dummy(page)) {\n\t\t\tclear_page_private_dummy(page);\n\t\t\tunlock_page(page);\n\t\t\tmempool_free(page, sbi->write_io_dummy);\n\n\t\t\tif (unlikely(bio->bi_status))\n\t\t\t\tf2fs_stop_checkpoint(sbi, true,\n\t\t\t\t\t\tSTOP_CP_REASON_WRITE_FAIL);\n\t\t\tcontinue;\n\t\t}\n\n\t\tfscrypt_finalize_bounce_page(&page);\n\n#ifdef CONFIG_F2FS_FS_COMPRESSION\n\t\tif (f2fs_is_compressed_page(page)) {\n\t\t\tf2fs_compress_write_end_io(bio, page);\n\t\t\tcontinue;\n\t\t}\n#endif\n\n\t\tif (unlikely(bio->bi_status)) {\n\t\t\tmapping_set_error(page->mapping, -EIO);\n\t\t\tif (type == F2FS_WB_CP_DATA)\n\t\t\t\tf2fs_stop_checkpoint(sbi, true,\n\t\t\t\t\t\tSTOP_CP_REASON_WRITE_FAIL);\n\t\t}\n\n\t\tf2fs_bug_on(sbi, page->mapping == NODE_MAPPING(sbi) &&\n\t\t\t\t\tpage->index != nid_of_node(page));\n\n\t\tdec_page_count(sbi, type);\n\t\tif (f2fs_in_warm_node_list(sbi, page))\n\t\t\tf2fs_del_fsync_node_entry(sbi, page);\n\t\tclear_page_private_gcing(page);\n\t\tend_page_writeback(page);\n\t}\n\tif (!get_pages(sbi, F2FS_WB_CP_DATA) &&\n\t\t\t\twq_has_sleeper(&sbi->cp_wait))\n\t\twake_up(&sbi->cp_wait);\n\n\tbio_put(bio);\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic void f2fs_zone_write_end_io(struct bio *bio)\n{\n\tstruct f2fs_bio_info *io = (struct f2fs_bio_info *)bio->bi_private;\n\n\tbio->bi_private = io->bi_private;\n\tcomplete(&io->zone_wait);\n\tf2fs_write_end_io(bio);\n}\n#endif\n\nstruct block_device *f2fs_target_device(struct f2fs_sb_info *sbi,\n\t\tblock_t blk_addr, sector_t *sector)\n{\n\tstruct block_device *bdev = sbi->sb->s_bdev;\n\tint i;\n\n\tif (f2fs_is_multi_device(sbi)) {\n\t\tfor (i = 0; i < sbi->s_ndevs; i++) {\n\t\t\tif (FDEV(i).start_blk <= blk_addr &&\n\t\t\t    FDEV(i).end_blk >= blk_addr) {\n\t\t\t\tblk_addr -= FDEV(i).start_blk;\n\t\t\t\tbdev = FDEV(i).bdev;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (sector)\n\t\t*sector = SECTOR_FROM_BLOCK(blk_addr);\n\treturn bdev;\n}\n\nint f2fs_target_device_index(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tint i;\n\n\tif (!f2fs_is_multi_device(sbi))\n\t\treturn 0;\n\n\tfor (i = 0; i < sbi->s_ndevs; i++)\n\t\tif (FDEV(i).start_blk <= blkaddr && FDEV(i).end_blk >= blkaddr)\n\t\t\treturn i;\n\treturn 0;\n}\n\nstatic blk_opf_t f2fs_io_flags(struct f2fs_io_info *fio)\n{\n\tunsigned int temp_mask = GENMASK(NR_TEMP_TYPE - 1, 0);\n\tunsigned int fua_flag, meta_flag, io_flag;\n\tblk_opf_t op_flags = 0;\n\n\tif (fio->op != REQ_OP_WRITE)\n\t\treturn 0;\n\tif (fio->type == DATA)\n\t\tio_flag = fio->sbi->data_io_flag;\n\telse if (fio->type == NODE)\n\t\tio_flag = fio->sbi->node_io_flag;\n\telse\n\t\treturn 0;\n\n\tfua_flag = io_flag & temp_mask;\n\tmeta_flag = (io_flag >> NR_TEMP_TYPE) & temp_mask;\n\n\t \n\tif (BIT(fio->temp) & meta_flag)\n\t\top_flags |= REQ_META;\n\tif (BIT(fio->temp) & fua_flag)\n\t\top_flags |= REQ_FUA;\n\treturn op_flags;\n}\n\nstatic struct bio *__bio_alloc(struct f2fs_io_info *fio, int npages)\n{\n\tstruct f2fs_sb_info *sbi = fio->sbi;\n\tstruct block_device *bdev;\n\tsector_t sector;\n\tstruct bio *bio;\n\n\tbdev = f2fs_target_device(sbi, fio->new_blkaddr, &sector);\n\tbio = bio_alloc_bioset(bdev, npages,\n\t\t\t\tfio->op | fio->op_flags | f2fs_io_flags(fio),\n\t\t\t\tGFP_NOIO, &f2fs_bioset);\n\tbio->bi_iter.bi_sector = sector;\n\tif (is_read_io(fio->op)) {\n\t\tbio->bi_end_io = f2fs_read_end_io;\n\t\tbio->bi_private = NULL;\n\t} else {\n\t\tbio->bi_end_io = f2fs_write_end_io;\n\t\tbio->bi_private = sbi;\n\t}\n\tiostat_alloc_and_bind_ctx(sbi, bio, NULL);\n\n\tif (fio->io_wbc)\n\t\twbc_init_bio(fio->io_wbc, bio);\n\n\treturn bio;\n}\n\nstatic void f2fs_set_bio_crypt_ctx(struct bio *bio, const struct inode *inode,\n\t\t\t\t  pgoff_t first_idx,\n\t\t\t\t  const struct f2fs_io_info *fio,\n\t\t\t\t  gfp_t gfp_mask)\n{\n\t \n\tif (!fio || !fio->encrypted_page)\n\t\tfscrypt_set_bio_crypt_ctx(bio, inode, first_idx, gfp_mask);\n}\n\nstatic bool f2fs_crypt_mergeable_bio(struct bio *bio, const struct inode *inode,\n\t\t\t\t     pgoff_t next_idx,\n\t\t\t\t     const struct f2fs_io_info *fio)\n{\n\t \n\tif (fio && fio->encrypted_page)\n\t\treturn !bio_has_crypt_ctx(bio);\n\n\treturn fscrypt_mergeable_bio(bio, inode, next_idx);\n}\n\nvoid f2fs_submit_read_bio(struct f2fs_sb_info *sbi, struct bio *bio,\n\t\t\t\t enum page_type type)\n{\n\tWARN_ON_ONCE(!is_read_io(bio_op(bio)));\n\ttrace_f2fs_submit_read_bio(sbi->sb, type, bio);\n\n\tiostat_update_submit_ctx(bio, type);\n\tsubmit_bio(bio);\n}\n\nstatic void f2fs_align_write_bio(struct f2fs_sb_info *sbi, struct bio *bio)\n{\n\tunsigned int start =\n\t\t(bio->bi_iter.bi_size >> F2FS_BLKSIZE_BITS) % F2FS_IO_SIZE(sbi);\n\n\tif (start == 0)\n\t\treturn;\n\n\t \n\tfor (; start < F2FS_IO_SIZE(sbi); start++) {\n\t\tstruct page *page =\n\t\t\tmempool_alloc(sbi->write_io_dummy,\n\t\t\t\t      GFP_NOIO | __GFP_NOFAIL);\n\t\tf2fs_bug_on(sbi, !page);\n\n\t\tlock_page(page);\n\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tset_page_private_dummy(page);\n\n\t\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE)\n\t\t\tf2fs_bug_on(sbi, 1);\n\t}\n}\n\nstatic void f2fs_submit_write_bio(struct f2fs_sb_info *sbi, struct bio *bio,\n\t\t\t\t  enum page_type type)\n{\n\tWARN_ON_ONCE(is_read_io(bio_op(bio)));\n\n\tif (type == DATA || type == NODE) {\n\t\tif (f2fs_lfs_mode(sbi) && current->plug)\n\t\t\tblk_finish_plug(current->plug);\n\n\t\tif (F2FS_IO_ALIGNED(sbi)) {\n\t\t\tf2fs_align_write_bio(sbi, bio);\n\t\t\t \n\t\t\tif (type == NODE)\n\t\t\t\tset_sbi_flag(sbi, SBI_NEED_CP);\n\t\t}\n\t}\n\n\ttrace_f2fs_submit_write_bio(sbi->sb, type, bio);\n\tiostat_update_submit_ctx(bio, type);\n\tsubmit_bio(bio);\n}\n\nstatic void __submit_merged_bio(struct f2fs_bio_info *io)\n{\n\tstruct f2fs_io_info *fio = &io->fio;\n\n\tif (!io->bio)\n\t\treturn;\n\n\tif (is_read_io(fio->op)) {\n\t\ttrace_f2fs_prepare_read_bio(io->sbi->sb, fio->type, io->bio);\n\t\tf2fs_submit_read_bio(io->sbi, io->bio, fio->type);\n\t} else {\n\t\ttrace_f2fs_prepare_write_bio(io->sbi->sb, fio->type, io->bio);\n\t\tf2fs_submit_write_bio(io->sbi, io->bio, fio->type);\n\t}\n\tio->bio = NULL;\n}\n\nstatic bool __has_merged_page(struct bio *bio, struct inode *inode,\n\t\t\t\t\t\tstruct page *page, nid_t ino)\n{\n\tstruct bio_vec *bvec;\n\tstruct bvec_iter_all iter_all;\n\n\tif (!bio)\n\t\treturn false;\n\n\tif (!inode && !page && !ino)\n\t\treturn true;\n\n\tbio_for_each_segment_all(bvec, bio, iter_all) {\n\t\tstruct page *target = bvec->bv_page;\n\n\t\tif (fscrypt_is_bounce_page(target)) {\n\t\t\ttarget = fscrypt_pagecache_page(target);\n\t\t\tif (IS_ERR(target))\n\t\t\t\tcontinue;\n\t\t}\n\t\tif (f2fs_is_compressed_page(target)) {\n\t\t\ttarget = f2fs_compress_control_page(target);\n\t\t\tif (IS_ERR(target))\n\t\t\t\tcontinue;\n\t\t}\n\n\t\tif (inode && inode == target->mapping->host)\n\t\t\treturn true;\n\t\tif (page && page == target)\n\t\t\treturn true;\n\t\tif (ino && ino == ino_of_node(target))\n\t\t\treturn true;\n\t}\n\n\treturn false;\n}\n\nint f2fs_init_write_merge_io(struct f2fs_sb_info *sbi)\n{\n\tint i;\n\n\tfor (i = 0; i < NR_PAGE_TYPE; i++) {\n\t\tint n = (i == META) ? 1 : NR_TEMP_TYPE;\n\t\tint j;\n\n\t\tsbi->write_io[i] = f2fs_kmalloc(sbi,\n\t\t\t\tarray_size(n, sizeof(struct f2fs_bio_info)),\n\t\t\t\tGFP_KERNEL);\n\t\tif (!sbi->write_io[i])\n\t\t\treturn -ENOMEM;\n\n\t\tfor (j = HOT; j < n; j++) {\n\t\t\tinit_f2fs_rwsem(&sbi->write_io[i][j].io_rwsem);\n\t\t\tsbi->write_io[i][j].sbi = sbi;\n\t\t\tsbi->write_io[i][j].bio = NULL;\n\t\t\tspin_lock_init(&sbi->write_io[i][j].io_lock);\n\t\t\tINIT_LIST_HEAD(&sbi->write_io[i][j].io_list);\n\t\t\tINIT_LIST_HEAD(&sbi->write_io[i][j].bio_list);\n\t\t\tinit_f2fs_rwsem(&sbi->write_io[i][j].bio_list_lock);\n#ifdef CONFIG_BLK_DEV_ZONED\n\t\t\tinit_completion(&sbi->write_io[i][j].zone_wait);\n\t\t\tsbi->write_io[i][j].zone_pending_bio = NULL;\n\t\t\tsbi->write_io[i][j].bi_private = NULL;\n#endif\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nstatic void __f2fs_submit_merged_write(struct f2fs_sb_info *sbi,\n\t\t\t\tenum page_type type, enum temp_type temp)\n{\n\tenum page_type btype = PAGE_TYPE_OF_BIO(type);\n\tstruct f2fs_bio_info *io = sbi->write_io[btype] + temp;\n\n\tf2fs_down_write(&io->io_rwsem);\n\n\tif (!io->bio)\n\t\tgoto unlock_out;\n\n\t \n\tif (type >= META_FLUSH) {\n\t\tio->fio.type = META_FLUSH;\n\t\tio->bio->bi_opf |= REQ_META | REQ_PRIO | REQ_SYNC;\n\t\tif (!test_opt(sbi, NOBARRIER))\n\t\t\tio->bio->bi_opf |= REQ_PREFLUSH | REQ_FUA;\n\t}\n\t__submit_merged_bio(io);\nunlock_out:\n\tf2fs_up_write(&io->io_rwsem);\n}\n\nstatic void __submit_merged_write_cond(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct inode *inode, struct page *page,\n\t\t\t\tnid_t ino, enum page_type type, bool force)\n{\n\tenum temp_type temp;\n\tbool ret = true;\n\n\tfor (temp = HOT; temp < NR_TEMP_TYPE; temp++) {\n\t\tif (!force)\t{\n\t\t\tenum page_type btype = PAGE_TYPE_OF_BIO(type);\n\t\t\tstruct f2fs_bio_info *io = sbi->write_io[btype] + temp;\n\n\t\t\tf2fs_down_read(&io->io_rwsem);\n\t\t\tret = __has_merged_page(io->bio, inode, page, ino);\n\t\t\tf2fs_up_read(&io->io_rwsem);\n\t\t}\n\t\tif (ret)\n\t\t\t__f2fs_submit_merged_write(sbi, type, temp);\n\n\t\t \n\t\tif (type >= META)\n\t\t\tbreak;\n\t}\n}\n\nvoid f2fs_submit_merged_write(struct f2fs_sb_info *sbi, enum page_type type)\n{\n\t__submit_merged_write_cond(sbi, NULL, NULL, 0, type, true);\n}\n\nvoid f2fs_submit_merged_write_cond(struct f2fs_sb_info *sbi,\n\t\t\t\tstruct inode *inode, struct page *page,\n\t\t\t\tnid_t ino, enum page_type type)\n{\n\t__submit_merged_write_cond(sbi, inode, page, ino, type, false);\n}\n\nvoid f2fs_flush_merged_writes(struct f2fs_sb_info *sbi)\n{\n\tf2fs_submit_merged_write(sbi, DATA);\n\tf2fs_submit_merged_write(sbi, NODE);\n\tf2fs_submit_merged_write(sbi, META);\n}\n\n \nint f2fs_submit_page_bio(struct f2fs_io_info *fio)\n{\n\tstruct bio *bio;\n\tstruct page *page = fio->encrypted_page ?\n\t\t\tfio->encrypted_page : fio->page;\n\n\tif (!f2fs_is_valid_blkaddr(fio->sbi, fio->new_blkaddr,\n\t\t\tfio->is_por ? META_POR : (__is_meta_io(fio) ?\n\t\t\tMETA_GENERIC : DATA_GENERIC_ENHANCE))) {\n\t\tf2fs_handle_error(fio->sbi, ERROR_INVALID_BLKADDR);\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\ttrace_f2fs_submit_page_bio(page, fio);\n\n\t \n\tbio = __bio_alloc(fio, 1);\n\n\tf2fs_set_bio_crypt_ctx(bio, fio->page->mapping->host,\n\t\t\t       fio->page->index, fio, GFP_NOIO);\n\n\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {\n\t\tbio_put(bio);\n\t\treturn -EFAULT;\n\t}\n\n\tif (fio->io_wbc && !is_read_io(fio->op))\n\t\twbc_account_cgroup_owner(fio->io_wbc, fio->page, PAGE_SIZE);\n\n\tinc_page_count(fio->sbi, is_read_io(fio->op) ?\n\t\t\t__read_io_type(page) : WB_DATA_TYPE(fio->page));\n\n\tif (is_read_io(bio_op(bio)))\n\t\tf2fs_submit_read_bio(fio->sbi, bio, fio->type);\n\telse\n\t\tf2fs_submit_write_bio(fio->sbi, bio, fio->type);\n\treturn 0;\n}\n\nstatic bool page_is_mergeable(struct f2fs_sb_info *sbi, struct bio *bio,\n\t\t\t\tblock_t last_blkaddr, block_t cur_blkaddr)\n{\n\tif (unlikely(sbi->max_io_bytes &&\n\t\t\tbio->bi_iter.bi_size >= sbi->max_io_bytes))\n\t\treturn false;\n\tif (last_blkaddr + 1 != cur_blkaddr)\n\t\treturn false;\n\treturn bio->bi_bdev == f2fs_target_device(sbi, cur_blkaddr, NULL);\n}\n\nstatic bool io_type_is_mergeable(struct f2fs_bio_info *io,\n\t\t\t\t\t\tstruct f2fs_io_info *fio)\n{\n\tif (io->fio.op != fio->op)\n\t\treturn false;\n\treturn io->fio.op_flags == fio->op_flags;\n}\n\nstatic bool io_is_mergeable(struct f2fs_sb_info *sbi, struct bio *bio,\n\t\t\t\t\tstruct f2fs_bio_info *io,\n\t\t\t\t\tstruct f2fs_io_info *fio,\n\t\t\t\t\tblock_t last_blkaddr,\n\t\t\t\t\tblock_t cur_blkaddr)\n{\n\tif (F2FS_IO_ALIGNED(sbi) && (fio->type == DATA || fio->type == NODE)) {\n\t\tunsigned int filled_blocks =\n\t\t\t\tF2FS_BYTES_TO_BLK(bio->bi_iter.bi_size);\n\t\tunsigned int io_size = F2FS_IO_SIZE(sbi);\n\t\tunsigned int left_vecs = bio->bi_max_vecs - bio->bi_vcnt;\n\n\t\t \n\t\tif (!(filled_blocks % io_size) && left_vecs < io_size)\n\t\t\treturn false;\n\t}\n\tif (!page_is_mergeable(sbi, bio, last_blkaddr, cur_blkaddr))\n\t\treturn false;\n\treturn io_type_is_mergeable(io, fio);\n}\n\nstatic void add_bio_entry(struct f2fs_sb_info *sbi, struct bio *bio,\n\t\t\t\tstruct page *page, enum temp_type temp)\n{\n\tstruct f2fs_bio_info *io = sbi->write_io[DATA] + temp;\n\tstruct bio_entry *be;\n\n\tbe = f2fs_kmem_cache_alloc(bio_entry_slab, GFP_NOFS, true, NULL);\n\tbe->bio = bio;\n\tbio_get(bio);\n\n\tif (bio_add_page(bio, page, PAGE_SIZE, 0) != PAGE_SIZE)\n\t\tf2fs_bug_on(sbi, 1);\n\n\tf2fs_down_write(&io->bio_list_lock);\n\tlist_add_tail(&be->list, &io->bio_list);\n\tf2fs_up_write(&io->bio_list_lock);\n}\n\nstatic void del_bio_entry(struct bio_entry *be)\n{\n\tlist_del(&be->list);\n\tkmem_cache_free(bio_entry_slab, be);\n}\n\nstatic int add_ipu_page(struct f2fs_io_info *fio, struct bio **bio,\n\t\t\t\t\t\t\tstruct page *page)\n{\n\tstruct f2fs_sb_info *sbi = fio->sbi;\n\tenum temp_type temp;\n\tbool found = false;\n\tint ret = -EAGAIN;\n\n\tfor (temp = HOT; temp < NR_TEMP_TYPE && !found; temp++) {\n\t\tstruct f2fs_bio_info *io = sbi->write_io[DATA] + temp;\n\t\tstruct list_head *head = &io->bio_list;\n\t\tstruct bio_entry *be;\n\n\t\tf2fs_down_write(&io->bio_list_lock);\n\t\tlist_for_each_entry(be, head, list) {\n\t\t\tif (be->bio != *bio)\n\t\t\t\tcontinue;\n\n\t\t\tfound = true;\n\n\t\t\tf2fs_bug_on(sbi, !page_is_mergeable(sbi, *bio,\n\t\t\t\t\t\t\t    *fio->last_block,\n\t\t\t\t\t\t\t    fio->new_blkaddr));\n\t\t\tif (f2fs_crypt_mergeable_bio(*bio,\n\t\t\t\t\tfio->page->mapping->host,\n\t\t\t\t\tfio->page->index, fio) &&\n\t\t\t    bio_add_page(*bio, page, PAGE_SIZE, 0) ==\n\t\t\t\t\tPAGE_SIZE) {\n\t\t\t\tret = 0;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t \n\t\t\tdel_bio_entry(be);\n\t\t\tf2fs_submit_write_bio(sbi, *bio, DATA);\n\t\t\tbreak;\n\t\t}\n\t\tf2fs_up_write(&io->bio_list_lock);\n\t}\n\n\tif (ret) {\n\t\tbio_put(*bio);\n\t\t*bio = NULL;\n\t}\n\n\treturn ret;\n}\n\nvoid f2fs_submit_merged_ipu_write(struct f2fs_sb_info *sbi,\n\t\t\t\t\tstruct bio **bio, struct page *page)\n{\n\tenum temp_type temp;\n\tbool found = false;\n\tstruct bio *target = bio ? *bio : NULL;\n\n\tf2fs_bug_on(sbi, !target && !page);\n\n\tfor (temp = HOT; temp < NR_TEMP_TYPE && !found; temp++) {\n\t\tstruct f2fs_bio_info *io = sbi->write_io[DATA] + temp;\n\t\tstruct list_head *head = &io->bio_list;\n\t\tstruct bio_entry *be;\n\n\t\tif (list_empty(head))\n\t\t\tcontinue;\n\n\t\tf2fs_down_read(&io->bio_list_lock);\n\t\tlist_for_each_entry(be, head, list) {\n\t\t\tif (target)\n\t\t\t\tfound = (target == be->bio);\n\t\t\telse\n\t\t\t\tfound = __has_merged_page(be->bio, NULL,\n\t\t\t\t\t\t\t\tpage, 0);\n\t\t\tif (found)\n\t\t\t\tbreak;\n\t\t}\n\t\tf2fs_up_read(&io->bio_list_lock);\n\n\t\tif (!found)\n\t\t\tcontinue;\n\n\t\tfound = false;\n\n\t\tf2fs_down_write(&io->bio_list_lock);\n\t\tlist_for_each_entry(be, head, list) {\n\t\t\tif (target)\n\t\t\t\tfound = (target == be->bio);\n\t\t\telse\n\t\t\t\tfound = __has_merged_page(be->bio, NULL,\n\t\t\t\t\t\t\t\tpage, 0);\n\t\t\tif (found) {\n\t\t\t\ttarget = be->bio;\n\t\t\t\tdel_bio_entry(be);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tf2fs_up_write(&io->bio_list_lock);\n\t}\n\n\tif (found)\n\t\tf2fs_submit_write_bio(sbi, target, DATA);\n\tif (bio && *bio) {\n\t\tbio_put(*bio);\n\t\t*bio = NULL;\n\t}\n}\n\nint f2fs_merge_page_bio(struct f2fs_io_info *fio)\n{\n\tstruct bio *bio = *fio->bio;\n\tstruct page *page = fio->encrypted_page ?\n\t\t\tfio->encrypted_page : fio->page;\n\n\tif (!f2fs_is_valid_blkaddr(fio->sbi, fio->new_blkaddr,\n\t\t\t__is_meta_io(fio) ? META_GENERIC : DATA_GENERIC)) {\n\t\tf2fs_handle_error(fio->sbi, ERROR_INVALID_BLKADDR);\n\t\treturn -EFSCORRUPTED;\n\t}\n\n\ttrace_f2fs_submit_page_bio(page, fio);\n\n\tif (bio && !page_is_mergeable(fio->sbi, bio, *fio->last_block,\n\t\t\t\t\t\tfio->new_blkaddr))\n\t\tf2fs_submit_merged_ipu_write(fio->sbi, &bio, NULL);\nalloc_new:\n\tif (!bio) {\n\t\tbio = __bio_alloc(fio, BIO_MAX_VECS);\n\t\tf2fs_set_bio_crypt_ctx(bio, fio->page->mapping->host,\n\t\t\t\t       fio->page->index, fio, GFP_NOIO);\n\n\t\tadd_bio_entry(fio->sbi, bio, page, fio->temp);\n\t} else {\n\t\tif (add_ipu_page(fio, &bio, page))\n\t\t\tgoto alloc_new;\n\t}\n\n\tif (fio->io_wbc)\n\t\twbc_account_cgroup_owner(fio->io_wbc, fio->page, PAGE_SIZE);\n\n\tinc_page_count(fio->sbi, WB_DATA_TYPE(page));\n\n\t*fio->last_block = fio->new_blkaddr;\n\t*fio->bio = bio;\n\n\treturn 0;\n}\n\n#ifdef CONFIG_BLK_DEV_ZONED\nstatic bool is_end_zone_blkaddr(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tint devi = 0;\n\n\tif (f2fs_is_multi_device(sbi)) {\n\t\tdevi = f2fs_target_device_index(sbi, blkaddr);\n\t\tif (blkaddr < FDEV(devi).start_blk ||\n\t\t    blkaddr > FDEV(devi).end_blk) {\n\t\t\tf2fs_err(sbi, \"Invalid block %x\", blkaddr);\n\t\t\treturn false;\n\t\t}\n\t\tblkaddr -= FDEV(devi).start_blk;\n\t}\n\treturn bdev_zoned_model(FDEV(devi).bdev) == BLK_ZONED_HM &&\n\t\tf2fs_blkz_is_seq(sbi, devi, blkaddr) &&\n\t\t(blkaddr % sbi->blocks_per_blkz == sbi->blocks_per_blkz - 1);\n}\n#endif\n\nvoid f2fs_submit_page_write(struct f2fs_io_info *fio)\n{\n\tstruct f2fs_sb_info *sbi = fio->sbi;\n\tenum page_type btype = PAGE_TYPE_OF_BIO(fio->type);\n\tstruct f2fs_bio_info *io = sbi->write_io[btype] + fio->temp;\n\tstruct page *bio_page;\n\n\tf2fs_bug_on(sbi, is_read_io(fio->op));\n\n\tf2fs_down_write(&io->io_rwsem);\n\n#ifdef CONFIG_BLK_DEV_ZONED\n\tif (f2fs_sb_has_blkzoned(sbi) && btype < META && io->zone_pending_bio) {\n\t\twait_for_completion_io(&io->zone_wait);\n\t\tbio_put(io->zone_pending_bio);\n\t\tio->zone_pending_bio = NULL;\n\t\tio->bi_private = NULL;\n\t}\n#endif\n\nnext:\n\tif (fio->in_list) {\n\t\tspin_lock(&io->io_lock);\n\t\tif (list_empty(&io->io_list)) {\n\t\t\tspin_unlock(&io->io_lock);\n\t\t\tgoto out;\n\t\t}\n\t\tfio = list_first_entry(&io->io_list,\n\t\t\t\t\t\tstruct f2fs_io_info, list);\n\t\tlist_del(&fio->list);\n\t\tspin_unlock(&io->io_lock);\n\t}\n\n\tverify_fio_blkaddr(fio);\n\n\tif (fio->encrypted_page)\n\t\tbio_page = fio->encrypted_page;\n\telse if (fio->compressed_page)\n\t\tbio_page = fio->compressed_page;\n\telse\n\t\tbio_page = fio->page;\n\n\t \n\tfio->submitted = 1;\n\n\tinc_page_count(sbi, WB_DATA_TYPE(bio_page));\n\n\tif (io->bio &&\n\t    (!io_is_mergeable(sbi, io->bio, io, fio, io->last_block_in_bio,\n\t\t\t      fio->new_blkaddr) ||\n\t     !f2fs_crypt_mergeable_bio(io->bio, fio->page->mapping->host,\n\t\t\t\t       bio_page->index, fio)))\n\t\t__submit_merged_bio(io);\nalloc_new:\n\tif (io->bio == NULL) {\n\t\tif (F2FS_IO_ALIGNED(sbi) &&\n\t\t\t\t(fio->type == DATA || fio->type == NODE) &&\n\t\t\t\tfio->new_blkaddr & F2FS_IO_SIZE_MASK(sbi)) {\n\t\t\tdec_page_count(sbi, WB_DATA_TYPE(bio_page));\n\t\t\tfio->retry = 1;\n\t\t\tgoto skip;\n\t\t}\n\t\tio->bio = __bio_alloc(fio, BIO_MAX_VECS);\n\t\tf2fs_set_bio_crypt_ctx(io->bio, fio->page->mapping->host,\n\t\t\t\t       bio_page->index, fio, GFP_NOIO);\n\t\tio->fio = *fio;\n\t}\n\n\tif (bio_add_page(io->bio, bio_page, PAGE_SIZE, 0) < PAGE_SIZE) {\n\t\t__submit_merged_bio(io);\n\t\tgoto alloc_new;\n\t}\n\n\tif (fio->io_wbc)\n\t\twbc_account_cgroup_owner(fio->io_wbc, fio->page, PAGE_SIZE);\n\n\tio->last_block_in_bio = fio->new_blkaddr;\n\n\ttrace_f2fs_submit_page_write(fio->page, fio);\nskip:\n\tif (fio->in_list)\n\t\tgoto next;\nout:\n#ifdef CONFIG_BLK_DEV_ZONED\n\tif (f2fs_sb_has_blkzoned(sbi) && btype < META &&\n\t\t\tis_end_zone_blkaddr(sbi, fio->new_blkaddr)) {\n\t\tbio_get(io->bio);\n\t\treinit_completion(&io->zone_wait);\n\t\tio->bi_private = io->bio->bi_private;\n\t\tio->bio->bi_private = io;\n\t\tio->bio->bi_end_io = f2fs_zone_write_end_io;\n\t\tio->zone_pending_bio = io->bio;\n\t\t__submit_merged_bio(io);\n\t}\n#endif\n\tif (is_sbi_flag_set(sbi, SBI_IS_SHUTDOWN) ||\n\t\t\t\t!f2fs_is_checkpoint_ready(sbi))\n\t\t__submit_merged_bio(io);\n\tf2fs_up_write(&io->io_rwsem);\n}\n\nstatic struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,\n\t\t\t\t      unsigned nr_pages, blk_opf_t op_flag,\n\t\t\t\t      pgoff_t first_idx, bool for_write)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\tstruct bio_post_read_ctx *ctx = NULL;\n\tunsigned int post_read_steps = 0;\n\tsector_t sector;\n\tstruct block_device *bdev = f2fs_target_device(sbi, blkaddr, &sector);\n\n\tbio = bio_alloc_bioset(bdev, bio_max_segs(nr_pages),\n\t\t\t       REQ_OP_READ | op_flag,\n\t\t\t       for_write ? GFP_NOIO : GFP_KERNEL, &f2fs_bioset);\n\tif (!bio)\n\t\treturn ERR_PTR(-ENOMEM);\n\tbio->bi_iter.bi_sector = sector;\n\tf2fs_set_bio_crypt_ctx(bio, inode, first_idx, NULL, GFP_NOFS);\n\tbio->bi_end_io = f2fs_read_end_io;\n\n\tif (fscrypt_inode_uses_fs_layer_crypto(inode))\n\t\tpost_read_steps |= STEP_DECRYPT;\n\n\tif (f2fs_need_verity(inode, first_idx))\n\t\tpost_read_steps |= STEP_VERITY;\n\n\t \n\n\tif (post_read_steps || f2fs_compressed_file(inode)) {\n\t\t \n\t\tctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);\n\t\tctx->bio = bio;\n\t\tctx->sbi = sbi;\n\t\tctx->enabled_steps = post_read_steps;\n\t\tctx->fs_blkaddr = blkaddr;\n\t\tctx->decompression_attempted = false;\n\t\tbio->bi_private = ctx;\n\t}\n\tiostat_alloc_and_bind_ctx(sbi, bio, ctx);\n\n\treturn bio;\n}\n\n \nstatic int f2fs_submit_page_read(struct inode *inode, struct page *page,\n\t\t\t\t block_t blkaddr, blk_opf_t op_flags,\n\t\t\t\t bool for_write)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio;\n\n\tbio = f2fs_grab_read_bio(inode, blkaddr, 1, op_flags,\n\t\t\t\t\tpage->index, for_write);\n\tif (IS_ERR(bio))\n\t\treturn PTR_ERR(bio);\n\n\t \n\tf2fs_wait_on_block_writeback(inode, blkaddr);\n\n\tif (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {\n\t\tiostat_update_and_unbind_ctx(bio);\n\t\tif (bio->bi_private)\n\t\t\tmempool_free(bio->bi_private, bio_post_read_ctx_pool);\n\t\tbio_put(bio);\n\t\treturn -EFAULT;\n\t}\n\tinc_page_count(sbi, F2FS_RD_DATA);\n\tf2fs_update_iostat(sbi, NULL, FS_DATA_READ_IO, F2FS_BLKSIZE);\n\tf2fs_submit_read_bio(sbi, bio, DATA);\n\treturn 0;\n}\n\nstatic void __set_data_blkaddr(struct dnode_of_data *dn)\n{\n\tstruct f2fs_node *rn = F2FS_NODE(dn->node_page);\n\t__le32 *addr_array;\n\tint base = 0;\n\n\tif (IS_INODE(dn->node_page) && f2fs_has_extra_attr(dn->inode))\n\t\tbase = get_extra_isize(dn->inode);\n\n\t \n\taddr_array = blkaddr_in_node(rn);\n\taddr_array[base + dn->ofs_in_node] = cpu_to_le32(dn->data_blkaddr);\n}\n\n \nvoid f2fs_set_data_blkaddr(struct dnode_of_data *dn)\n{\n\tf2fs_wait_on_page_writeback(dn->node_page, NODE, true, true);\n\t__set_data_blkaddr(dn);\n\tif (set_page_dirty(dn->node_page))\n\t\tdn->node_changed = true;\n}\n\nvoid f2fs_update_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr)\n{\n\tdn->data_blkaddr = blkaddr;\n\tf2fs_set_data_blkaddr(dn);\n\tf2fs_update_read_extent_cache(dn);\n}\n\n \nint f2fs_reserve_new_blocks(struct dnode_of_data *dn, blkcnt_t count)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);\n\tint err;\n\n\tif (!count)\n\t\treturn 0;\n\n\tif (unlikely(is_inode_flag_set(dn->inode, FI_NO_ALLOC)))\n\t\treturn -EPERM;\n\tif (unlikely((err = inc_valid_block_count(sbi, dn->inode, &count))))\n\t\treturn err;\n\n\ttrace_f2fs_reserve_new_blocks(dn->inode, dn->nid,\n\t\t\t\t\t\tdn->ofs_in_node, count);\n\n\tf2fs_wait_on_page_writeback(dn->node_page, NODE, true, true);\n\n\tfor (; count > 0; dn->ofs_in_node++) {\n\t\tblock_t blkaddr = f2fs_data_blkaddr(dn);\n\n\t\tif (blkaddr == NULL_ADDR) {\n\t\t\tdn->data_blkaddr = NEW_ADDR;\n\t\t\t__set_data_blkaddr(dn);\n\t\t\tcount--;\n\t\t}\n\t}\n\n\tif (set_page_dirty(dn->node_page))\n\t\tdn->node_changed = true;\n\treturn 0;\n}\n\n \nint f2fs_reserve_new_block(struct dnode_of_data *dn)\n{\n\tunsigned int ofs_in_node = dn->ofs_in_node;\n\tint ret;\n\n\tret = f2fs_reserve_new_blocks(dn, 1);\n\tdn->ofs_in_node = ofs_in_node;\n\treturn ret;\n}\n\nint f2fs_reserve_block(struct dnode_of_data *dn, pgoff_t index)\n{\n\tbool need_put = dn->inode_page ? false : true;\n\tint err;\n\n\terr = f2fs_get_dnode_of_data(dn, index, ALLOC_NODE);\n\tif (err)\n\t\treturn err;\n\n\tif (dn->data_blkaddr == NULL_ADDR)\n\t\terr = f2fs_reserve_new_block(dn);\n\tif (err || need_put)\n\t\tf2fs_put_dnode(dn);\n\treturn err;\n}\n\nstruct page *f2fs_get_read_data_page(struct inode *inode, pgoff_t index,\n\t\t\t\t     blk_opf_t op_flags, bool for_write,\n\t\t\t\t     pgoff_t *next_pgofs)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct dnode_of_data dn;\n\tstruct page *page;\n\tint err;\n\n\tpage = f2fs_grab_cache_page(mapping, index, for_write);\n\tif (!page)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tif (f2fs_lookup_read_extent_cache_block(inode, index,\n\t\t\t\t\t\t&dn.data_blkaddr)) {\n\t\tif (!f2fs_is_valid_blkaddr(F2FS_I_SB(inode), dn.data_blkaddr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE_READ)) {\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tf2fs_handle_error(F2FS_I_SB(inode),\n\t\t\t\t\t\tERROR_INVALID_BLKADDR);\n\t\t\tgoto put_err;\n\t\t}\n\t\tgoto got_it;\n\t}\n\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\terr = f2fs_get_dnode_of_data(&dn, index, LOOKUP_NODE);\n\tif (err) {\n\t\tif (err == -ENOENT && next_pgofs)\n\t\t\t*next_pgofs = f2fs_get_next_page_offset(&dn, index);\n\t\tgoto put_err;\n\t}\n\tf2fs_put_dnode(&dn);\n\n\tif (unlikely(dn.data_blkaddr == NULL_ADDR)) {\n\t\terr = -ENOENT;\n\t\tif (next_pgofs)\n\t\t\t*next_pgofs = index + 1;\n\t\tgoto put_err;\n\t}\n\tif (dn.data_blkaddr != NEW_ADDR &&\n\t\t\t!f2fs_is_valid_blkaddr(F2FS_I_SB(inode),\n\t\t\t\t\t\tdn.data_blkaddr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE)) {\n\t\terr = -EFSCORRUPTED;\n\t\tf2fs_handle_error(F2FS_I_SB(inode),\n\t\t\t\t\tERROR_INVALID_BLKADDR);\n\t\tgoto put_err;\n\t}\ngot_it:\n\tif (PageUptodate(page)) {\n\t\tunlock_page(page);\n\t\treturn page;\n\t}\n\n\t \n\tif (dn.data_blkaddr == NEW_ADDR) {\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tif (!PageUptodate(page))\n\t\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t\treturn page;\n\t}\n\n\terr = f2fs_submit_page_read(inode, page, dn.data_blkaddr,\n\t\t\t\t\t\top_flags, for_write);\n\tif (err)\n\t\tgoto put_err;\n\treturn page;\n\nput_err:\n\tf2fs_put_page(page, 1);\n\treturn ERR_PTR(err);\n}\n\nstruct page *f2fs_find_data_page(struct inode *inode, pgoff_t index,\n\t\t\t\t\tpgoff_t *next_pgofs)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\n\n\tpage = find_get_page(mapping, index);\n\tif (page && PageUptodate(page))\n\t\treturn page;\n\tf2fs_put_page(page, 0);\n\n\tpage = f2fs_get_read_data_page(inode, index, 0, false, next_pgofs);\n\tif (IS_ERR(page))\n\t\treturn page;\n\n\tif (PageUptodate(page))\n\t\treturn page;\n\n\twait_on_page_locked(page);\n\tif (unlikely(!PageUptodate(page))) {\n\t\tf2fs_put_page(page, 0);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\treturn page;\n}\n\n \nstruct page *f2fs_get_lock_data_page(struct inode *inode, pgoff_t index,\n\t\t\t\t\t\t\tbool for_write)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\n\n\tpage = f2fs_get_read_data_page(inode, index, 0, for_write, NULL);\n\tif (IS_ERR(page))\n\t\treturn page;\n\n\t \n\tlock_page(page);\n\tif (unlikely(page->mapping != mapping || !PageUptodate(page))) {\n\t\tf2fs_put_page(page, 1);\n\t\treturn ERR_PTR(-EIO);\n\t}\n\treturn page;\n}\n\n \nstruct page *f2fs_get_new_data_page(struct inode *inode,\n\t\tstruct page *ipage, pgoff_t index, bool new_i_size)\n{\n\tstruct address_space *mapping = inode->i_mapping;\n\tstruct page *page;\n\tstruct dnode_of_data dn;\n\tint err;\n\n\tpage = f2fs_grab_cache_page(mapping, index, true);\n\tif (!page) {\n\t\t \n\t\tf2fs_put_page(ipage, 1);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tset_new_dnode(&dn, inode, ipage, NULL, 0);\n\terr = f2fs_reserve_block(&dn, index);\n\tif (err) {\n\t\tf2fs_put_page(page, 1);\n\t\treturn ERR_PTR(err);\n\t}\n\tif (!ipage)\n\t\tf2fs_put_dnode(&dn);\n\n\tif (PageUptodate(page))\n\t\tgoto got_it;\n\n\tif (dn.data_blkaddr == NEW_ADDR) {\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tif (!PageUptodate(page))\n\t\t\tSetPageUptodate(page);\n\t} else {\n\t\tf2fs_put_page(page, 1);\n\n\t\t \n\t\tf2fs_bug_on(F2FS_I_SB(inode), ipage);\n\t\tpage = f2fs_get_lock_data_page(inode, index, true);\n\t\tif (IS_ERR(page))\n\t\t\treturn page;\n\t}\ngot_it:\n\tif (new_i_size && i_size_read(inode) <\n\t\t\t\t((loff_t)(index + 1) << PAGE_SHIFT))\n\t\tf2fs_i_size_write(inode, ((loff_t)(index + 1) << PAGE_SHIFT));\n\treturn page;\n}\n\nstatic int __allocate_data_block(struct dnode_of_data *dn, int seg_type)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);\n\tstruct f2fs_summary sum;\n\tstruct node_info ni;\n\tblock_t old_blkaddr;\n\tblkcnt_t count = 1;\n\tint err;\n\n\tif (unlikely(is_inode_flag_set(dn->inode, FI_NO_ALLOC)))\n\t\treturn -EPERM;\n\n\terr = f2fs_get_node_info(sbi, dn->nid, &ni, false);\n\tif (err)\n\t\treturn err;\n\n\tdn->data_blkaddr = f2fs_data_blkaddr(dn);\n\tif (dn->data_blkaddr == NULL_ADDR) {\n\t\terr = inc_valid_block_count(sbi, dn->inode, &count);\n\t\tif (unlikely(err))\n\t\t\treturn err;\n\t}\n\n\tset_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);\n\told_blkaddr = dn->data_blkaddr;\n\tf2fs_allocate_data_block(sbi, NULL, old_blkaddr, &dn->data_blkaddr,\n\t\t\t\t&sum, seg_type, NULL);\n\tif (GET_SEGNO(sbi, old_blkaddr) != NULL_SEGNO) {\n\t\tinvalidate_mapping_pages(META_MAPPING(sbi),\n\t\t\t\t\told_blkaddr, old_blkaddr);\n\t\tf2fs_invalidate_compress_page(sbi, old_blkaddr);\n\t}\n\tf2fs_update_data_blkaddr(dn, dn->data_blkaddr);\n\treturn 0;\n}\n\nstatic void f2fs_map_lock(struct f2fs_sb_info *sbi, int flag)\n{\n\tif (flag == F2FS_GET_BLOCK_PRE_AIO)\n\t\tf2fs_down_read(&sbi->node_change);\n\telse\n\t\tf2fs_lock_op(sbi);\n}\n\nstatic void f2fs_map_unlock(struct f2fs_sb_info *sbi, int flag)\n{\n\tif (flag == F2FS_GET_BLOCK_PRE_AIO)\n\t\tf2fs_up_read(&sbi->node_change);\n\telse\n\t\tf2fs_unlock_op(sbi);\n}\n\nint f2fs_get_block_locked(struct dnode_of_data *dn, pgoff_t index)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);\n\tint err = 0;\n\n\tf2fs_map_lock(sbi, F2FS_GET_BLOCK_PRE_AIO);\n\tif (!f2fs_lookup_read_extent_cache_block(dn->inode, index,\n\t\t\t\t\t\t&dn->data_blkaddr))\n\t\terr = f2fs_reserve_block(dn, index);\n\tf2fs_map_unlock(sbi, F2FS_GET_BLOCK_PRE_AIO);\n\n\treturn err;\n}\n\nstatic int f2fs_map_no_dnode(struct inode *inode,\n\t\tstruct f2fs_map_blocks *map, struct dnode_of_data *dn,\n\t\tpgoff_t pgoff)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\t \n\tif (map->m_may_create &&\n\t    (is_sbi_flag_set(sbi, SBI_IS_SHUTDOWN) || f2fs_cp_error(sbi)))\n\t\treturn -EIO;\n\n\tif (map->m_next_pgofs)\n\t\t*map->m_next_pgofs = f2fs_get_next_page_offset(dn, pgoff);\n\tif (map->m_next_extent)\n\t\t*map->m_next_extent = f2fs_get_next_page_offset(dn, pgoff);\n\treturn 0;\n}\n\nstatic bool f2fs_map_blocks_cached(struct inode *inode,\n\t\tstruct f2fs_map_blocks *map, int flag)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tunsigned int maxblocks = map->m_len;\n\tpgoff_t pgoff = (pgoff_t)map->m_lblk;\n\tstruct extent_info ei = {};\n\n\tif (!f2fs_lookup_read_extent_cache(inode, pgoff, &ei))\n\t\treturn false;\n\n\tmap->m_pblk = ei.blk + pgoff - ei.fofs;\n\tmap->m_len = min((pgoff_t)maxblocks, ei.fofs + ei.len - pgoff);\n\tmap->m_flags = F2FS_MAP_MAPPED;\n\tif (map->m_next_extent)\n\t\t*map->m_next_extent = pgoff + map->m_len;\n\n\t \n\tif (flag == F2FS_GET_BLOCK_DIO)\n\t\tf2fs_wait_on_block_writeback_range(inode,\n\t\t\t\t\tmap->m_pblk, map->m_len);\n\n\tif (f2fs_allow_multi_device_dio(sbi, flag)) {\n\t\tint bidx = f2fs_target_device_index(sbi, map->m_pblk);\n\t\tstruct f2fs_dev_info *dev = &sbi->devs[bidx];\n\n\t\tmap->m_bdev = dev->bdev;\n\t\tmap->m_pblk -= dev->start_blk;\n\t\tmap->m_len = min(map->m_len, dev->end_blk + 1 - map->m_pblk);\n\t} else {\n\t\tmap->m_bdev = inode->i_sb->s_bdev;\n\t}\n\treturn true;\n}\n\n \nint f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map, int flag)\n{\n\tunsigned int maxblocks = map->m_len;\n\tstruct dnode_of_data dn;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tint mode = map->m_may_create ? ALLOC_NODE : LOOKUP_NODE;\n\tpgoff_t pgofs, end_offset, end;\n\tint err = 0, ofs = 1;\n\tunsigned int ofs_in_node, last_ofs_in_node;\n\tblkcnt_t prealloc;\n\tblock_t blkaddr;\n\tunsigned int start_pgofs;\n\tint bidx = 0;\n\tbool is_hole;\n\n\tif (!maxblocks)\n\t\treturn 0;\n\n\tif (!map->m_may_create && f2fs_map_blocks_cached(inode, map, flag))\n\t\tgoto out;\n\n\tmap->m_bdev = inode->i_sb->s_bdev;\n\tmap->m_multidev_dio =\n\t\tf2fs_allow_multi_device_dio(F2FS_I_SB(inode), flag);\n\n\tmap->m_len = 0;\n\tmap->m_flags = 0;\n\n\t \n\tpgofs =\t(pgoff_t)map->m_lblk;\n\tend = pgofs + maxblocks;\n\nnext_dnode:\n\tif (map->m_may_create)\n\t\tf2fs_map_lock(sbi, flag);\n\n\t \n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\terr = f2fs_get_dnode_of_data(&dn, pgofs, mode);\n\tif (err) {\n\t\tif (flag == F2FS_GET_BLOCK_BMAP)\n\t\t\tmap->m_pblk = 0;\n\t\tif (err == -ENOENT)\n\t\t\terr = f2fs_map_no_dnode(inode, map, &dn, pgofs);\n\t\tgoto unlock_out;\n\t}\n\n\tstart_pgofs = pgofs;\n\tprealloc = 0;\n\tlast_ofs_in_node = ofs_in_node = dn.ofs_in_node;\n\tend_offset = ADDRS_PER_PAGE(dn.node_page, inode);\n\nnext_block:\n\tblkaddr = f2fs_data_blkaddr(&dn);\n\tis_hole = !__is_valid_data_blkaddr(blkaddr);\n\tif (!is_hole &&\n\t    !f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC_ENHANCE)) {\n\t\terr = -EFSCORRUPTED;\n\t\tf2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);\n\t\tgoto sync_out;\n\t}\n\n\t \n\tif (map->m_may_create &&\n\t    (is_hole || (f2fs_lfs_mode(sbi) && flag == F2FS_GET_BLOCK_DIO))) {\n\t\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\t\terr = -EIO;\n\t\t\tgoto sync_out;\n\t\t}\n\n\t\tswitch (flag) {\n\t\tcase F2FS_GET_BLOCK_PRE_AIO:\n\t\t\tif (blkaddr == NULL_ADDR) {\n\t\t\t\tprealloc++;\n\t\t\t\tlast_ofs_in_node = dn.ofs_in_node;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase F2FS_GET_BLOCK_PRE_DIO:\n\t\tcase F2FS_GET_BLOCK_DIO:\n\t\t\terr = __allocate_data_block(&dn, map->m_seg_type);\n\t\t\tif (err)\n\t\t\t\tgoto sync_out;\n\t\t\tif (flag == F2FS_GET_BLOCK_PRE_DIO)\n\t\t\t\tfile_need_truncate(inode);\n\t\t\tset_inode_flag(inode, FI_APPEND_WRITE);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tWARN_ON_ONCE(1);\n\t\t\terr = -EIO;\n\t\t\tgoto sync_out;\n\t\t}\n\n\t\tblkaddr = dn.data_blkaddr;\n\t\tif (is_hole)\n\t\t\tmap->m_flags |= F2FS_MAP_NEW;\n\t} else if (is_hole) {\n\t\tif (f2fs_compressed_file(inode) &&\n\t\t    f2fs_sanity_check_cluster(&dn) &&\n\t\t    (flag != F2FS_GET_BLOCK_FIEMAP ||\n\t\t     IS_ENABLED(CONFIG_F2FS_CHECK_FS))) {\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tf2fs_handle_error(sbi,\n\t\t\t\t\tERROR_CORRUPTED_CLUSTER);\n\t\t\tgoto sync_out;\n\t\t}\n\n\t\tswitch (flag) {\n\t\tcase F2FS_GET_BLOCK_PRECACHE:\n\t\t\tgoto sync_out;\n\t\tcase F2FS_GET_BLOCK_BMAP:\n\t\t\tmap->m_pblk = 0;\n\t\t\tgoto sync_out;\n\t\tcase F2FS_GET_BLOCK_FIEMAP:\n\t\t\tif (blkaddr == NULL_ADDR) {\n\t\t\t\tif (map->m_next_pgofs)\n\t\t\t\t\t*map->m_next_pgofs = pgofs + 1;\n\t\t\t\tgoto sync_out;\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\t \n\t\t\tif (map->m_next_pgofs)\n\t\t\t\t*map->m_next_pgofs = pgofs + 1;\n\t\t\tgoto sync_out;\n\t\t}\n\t}\n\n\tif (flag == F2FS_GET_BLOCK_PRE_AIO)\n\t\tgoto skip;\n\n\tif (map->m_multidev_dio)\n\t\tbidx = f2fs_target_device_index(sbi, blkaddr);\n\n\tif (map->m_len == 0) {\n\t\t \n\t\tif (blkaddr == NEW_ADDR)\n\t\t\tmap->m_flags |= F2FS_MAP_DELALLOC;\n\t\tmap->m_flags |= F2FS_MAP_MAPPED;\n\n\t\tmap->m_pblk = blkaddr;\n\t\tmap->m_len = 1;\n\n\t\tif (map->m_multidev_dio)\n\t\t\tmap->m_bdev = FDEV(bidx).bdev;\n\t} else if ((map->m_pblk != NEW_ADDR &&\n\t\t\tblkaddr == (map->m_pblk + ofs)) ||\n\t\t\t(map->m_pblk == NEW_ADDR && blkaddr == NEW_ADDR) ||\n\t\t\tflag == F2FS_GET_BLOCK_PRE_DIO) {\n\t\tif (map->m_multidev_dio && map->m_bdev != FDEV(bidx).bdev)\n\t\t\tgoto sync_out;\n\t\tofs++;\n\t\tmap->m_len++;\n\t} else {\n\t\tgoto sync_out;\n\t}\n\nskip:\n\tdn.ofs_in_node++;\n\tpgofs++;\n\n\t \n\tif (flag == F2FS_GET_BLOCK_PRE_AIO &&\n\t\t\t(pgofs == end || dn.ofs_in_node == end_offset)) {\n\n\t\tdn.ofs_in_node = ofs_in_node;\n\t\terr = f2fs_reserve_new_blocks(&dn, prealloc);\n\t\tif (err)\n\t\t\tgoto sync_out;\n\n\t\tmap->m_len += dn.ofs_in_node - ofs_in_node;\n\t\tif (prealloc && dn.ofs_in_node != last_ofs_in_node + 1) {\n\t\t\terr = -ENOSPC;\n\t\t\tgoto sync_out;\n\t\t}\n\t\tdn.ofs_in_node = end_offset;\n\t}\n\n\tif (pgofs >= end)\n\t\tgoto sync_out;\n\telse if (dn.ofs_in_node < end_offset)\n\t\tgoto next_block;\n\n\tif (flag == F2FS_GET_BLOCK_PRECACHE) {\n\t\tif (map->m_flags & F2FS_MAP_MAPPED) {\n\t\t\tunsigned int ofs = start_pgofs - map->m_lblk;\n\n\t\t\tf2fs_update_read_extent_cache_range(&dn,\n\t\t\t\tstart_pgofs, map->m_pblk + ofs,\n\t\t\t\tmap->m_len - ofs);\n\t\t}\n\t}\n\n\tf2fs_put_dnode(&dn);\n\n\tif (map->m_may_create) {\n\t\tf2fs_map_unlock(sbi, flag);\n\t\tf2fs_balance_fs(sbi, dn.node_changed);\n\t}\n\tgoto next_dnode;\n\nsync_out:\n\n\tif (flag == F2FS_GET_BLOCK_DIO && map->m_flags & F2FS_MAP_MAPPED) {\n\t\t \n\t\tf2fs_wait_on_block_writeback_range(inode,\n\t\t\t\t\t\tmap->m_pblk, map->m_len);\n\n\t\tif (map->m_multidev_dio) {\n\t\t\tblock_t blk_addr = map->m_pblk;\n\n\t\t\tbidx = f2fs_target_device_index(sbi, map->m_pblk);\n\n\t\t\tmap->m_bdev = FDEV(bidx).bdev;\n\t\t\tmap->m_pblk -= FDEV(bidx).start_blk;\n\n\t\t\tif (map->m_may_create)\n\t\t\t\tf2fs_update_device_state(sbi, inode->i_ino,\n\t\t\t\t\t\t\tblk_addr, map->m_len);\n\n\t\t\tf2fs_bug_on(sbi, blk_addr + map->m_len >\n\t\t\t\t\t\tFDEV(bidx).end_blk + 1);\n\t\t}\n\t}\n\n\tif (flag == F2FS_GET_BLOCK_PRECACHE) {\n\t\tif (map->m_flags & F2FS_MAP_MAPPED) {\n\t\t\tunsigned int ofs = start_pgofs - map->m_lblk;\n\n\t\t\tf2fs_update_read_extent_cache_range(&dn,\n\t\t\t\tstart_pgofs, map->m_pblk + ofs,\n\t\t\t\tmap->m_len - ofs);\n\t\t}\n\t\tif (map->m_next_extent)\n\t\t\t*map->m_next_extent = pgofs + 1;\n\t}\n\tf2fs_put_dnode(&dn);\nunlock_out:\n\tif (map->m_may_create) {\n\t\tf2fs_map_unlock(sbi, flag);\n\t\tf2fs_balance_fs(sbi, dn.node_changed);\n\t}\nout:\n\ttrace_f2fs_map_blocks(inode, map, flag, err);\n\treturn err;\n}\n\nbool f2fs_overwrite_io(struct inode *inode, loff_t pos, size_t len)\n{\n\tstruct f2fs_map_blocks map;\n\tblock_t last_lblk;\n\tint err;\n\n\tif (pos + len > i_size_read(inode))\n\t\treturn false;\n\n\tmap.m_lblk = F2FS_BYTES_TO_BLK(pos);\n\tmap.m_next_pgofs = NULL;\n\tmap.m_next_extent = NULL;\n\tmap.m_seg_type = NO_CHECK_TYPE;\n\tmap.m_may_create = false;\n\tlast_lblk = F2FS_BLK_ALIGN(pos + len);\n\n\twhile (map.m_lblk < last_lblk) {\n\t\tmap.m_len = last_lblk - map.m_lblk;\n\t\terr = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_DEFAULT);\n\t\tif (err || map.m_len == 0)\n\t\t\treturn false;\n\t\tmap.m_lblk += map.m_len;\n\t}\n\treturn true;\n}\n\nstatic inline u64 bytes_to_blks(struct inode *inode, u64 bytes)\n{\n\treturn (bytes >> inode->i_blkbits);\n}\n\nstatic inline u64 blks_to_bytes(struct inode *inode, u64 blks)\n{\n\treturn (blks << inode->i_blkbits);\n}\n\nstatic int f2fs_xattr_fiemap(struct inode *inode,\n\t\t\t\tstruct fiemap_extent_info *fieinfo)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct page *page;\n\tstruct node_info ni;\n\t__u64 phys = 0, len;\n\t__u32 flags;\n\tnid_t xnid = F2FS_I(inode)->i_xattr_nid;\n\tint err = 0;\n\n\tif (f2fs_has_inline_xattr(inode)) {\n\t\tint offset;\n\n\t\tpage = f2fs_grab_cache_page(NODE_MAPPING(sbi),\n\t\t\t\t\t\tinode->i_ino, false);\n\t\tif (!page)\n\t\t\treturn -ENOMEM;\n\n\t\terr = f2fs_get_node_info(sbi, inode->i_ino, &ni, false);\n\t\tif (err) {\n\t\t\tf2fs_put_page(page, 1);\n\t\t\treturn err;\n\t\t}\n\n\t\tphys = blks_to_bytes(inode, ni.blk_addr);\n\t\toffset = offsetof(struct f2fs_inode, i_addr) +\n\t\t\t\t\tsizeof(__le32) * (DEF_ADDRS_PER_INODE -\n\t\t\t\t\tget_inline_xattr_addrs(inode));\n\n\t\tphys += offset;\n\t\tlen = inline_xattr_size(inode);\n\n\t\tf2fs_put_page(page, 1);\n\n\t\tflags = FIEMAP_EXTENT_DATA_INLINE | FIEMAP_EXTENT_NOT_ALIGNED;\n\n\t\tif (!xnid)\n\t\t\tflags |= FIEMAP_EXTENT_LAST;\n\n\t\terr = fiemap_fill_next_extent(fieinfo, 0, phys, len, flags);\n\t\ttrace_f2fs_fiemap(inode, 0, phys, len, flags, err);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tif (xnid) {\n\t\tpage = f2fs_grab_cache_page(NODE_MAPPING(sbi), xnid, false);\n\t\tif (!page)\n\t\t\treturn -ENOMEM;\n\n\t\terr = f2fs_get_node_info(sbi, xnid, &ni, false);\n\t\tif (err) {\n\t\t\tf2fs_put_page(page, 1);\n\t\t\treturn err;\n\t\t}\n\n\t\tphys = blks_to_bytes(inode, ni.blk_addr);\n\t\tlen = inode->i_sb->s_blocksize;\n\n\t\tf2fs_put_page(page, 1);\n\n\t\tflags = FIEMAP_EXTENT_LAST;\n\t}\n\n\tif (phys) {\n\t\terr = fiemap_fill_next_extent(fieinfo, 0, phys, len, flags);\n\t\ttrace_f2fs_fiemap(inode, 0, phys, len, flags, err);\n\t}\n\n\treturn (err < 0 ? err : 0);\n}\n\nstatic loff_t max_inode_blocks(struct inode *inode)\n{\n\tloff_t result = ADDRS_PER_INODE(inode);\n\tloff_t leaf_count = ADDRS_PER_BLOCK(inode);\n\n\t \n\tresult += (leaf_count * 2);\n\n\t \n\tleaf_count *= NIDS_PER_BLOCK;\n\tresult += (leaf_count * 2);\n\n\t \n\tleaf_count *= NIDS_PER_BLOCK;\n\tresult += leaf_count;\n\n\treturn result;\n}\n\nint f2fs_fiemap(struct inode *inode, struct fiemap_extent_info *fieinfo,\n\t\tu64 start, u64 len)\n{\n\tstruct f2fs_map_blocks map;\n\tsector_t start_blk, last_blk;\n\tpgoff_t next_pgofs;\n\tu64 logical = 0, phys = 0, size = 0;\n\tu32 flags = 0;\n\tint ret = 0;\n\tbool compr_cluster = false, compr_appended;\n\tunsigned int cluster_size = F2FS_I(inode)->i_cluster_size;\n\tunsigned int count_in_cluster = 0;\n\tloff_t maxbytes;\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_CACHE) {\n\t\tret = f2fs_precache_extents(inode);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tret = fiemap_prep(inode, fieinfo, start, &len, FIEMAP_FLAG_XATTR);\n\tif (ret)\n\t\treturn ret;\n\n\tinode_lock(inode);\n\n\tmaxbytes = max_file_blocks(inode) << F2FS_BLKSIZE_BITS;\n\tif (start > maxbytes) {\n\t\tret = -EFBIG;\n\t\tgoto out;\n\t}\n\n\tif (len > maxbytes || (maxbytes - len) < start)\n\t\tlen = maxbytes - start;\n\n\tif (fieinfo->fi_flags & FIEMAP_FLAG_XATTR) {\n\t\tret = f2fs_xattr_fiemap(inode, fieinfo);\n\t\tgoto out;\n\t}\n\n\tif (f2fs_has_inline_data(inode) || f2fs_has_inline_dentry(inode)) {\n\t\tret = f2fs_inline_data_fiemap(inode, fieinfo, start, len);\n\t\tif (ret != -EAGAIN)\n\t\t\tgoto out;\n\t}\n\n\tif (bytes_to_blks(inode, len) == 0)\n\t\tlen = blks_to_bytes(inode, 1);\n\n\tstart_blk = bytes_to_blks(inode, start);\n\tlast_blk = bytes_to_blks(inode, start + len - 1);\n\nnext:\n\tmemset(&map, 0, sizeof(map));\n\tmap.m_lblk = start_blk;\n\tmap.m_len = bytes_to_blks(inode, len);\n\tmap.m_next_pgofs = &next_pgofs;\n\tmap.m_seg_type = NO_CHECK_TYPE;\n\n\tif (compr_cluster) {\n\t\tmap.m_lblk += 1;\n\t\tmap.m_len = cluster_size - count_in_cluster;\n\t}\n\n\tret = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_FIEMAP);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tif (!compr_cluster && !(map.m_flags & F2FS_MAP_FLAGS)) {\n\t\tstart_blk = next_pgofs;\n\n\t\tif (blks_to_bytes(inode, start_blk) < blks_to_bytes(inode,\n\t\t\t\t\t\tmax_inode_blocks(inode)))\n\t\t\tgoto prep_next;\n\n\t\tflags |= FIEMAP_EXTENT_LAST;\n\t}\n\n\tcompr_appended = false;\n\t \n\tif (compr_cluster && ((map.m_flags & F2FS_MAP_DELALLOC) ||\n\t\t\t!(map.m_flags & F2FS_MAP_FLAGS))) {\n\t\tcompr_appended = true;\n\t\tgoto skip_fill;\n\t}\n\n\tif (size) {\n\t\tflags |= FIEMAP_EXTENT_MERGED;\n\t\tif (IS_ENCRYPTED(inode))\n\t\t\tflags |= FIEMAP_EXTENT_DATA_ENCRYPTED;\n\n\t\tret = fiemap_fill_next_extent(fieinfo, logical,\n\t\t\t\tphys, size, flags);\n\t\ttrace_f2fs_fiemap(inode, logical, phys, size, flags, ret);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tsize = 0;\n\t}\n\n\tif (start_blk > last_blk)\n\t\tgoto out;\n\nskip_fill:\n\tif (map.m_pblk == COMPRESS_ADDR) {\n\t\tcompr_cluster = true;\n\t\tcount_in_cluster = 1;\n\t} else if (compr_appended) {\n\t\tunsigned int appended_blks = cluster_size -\n\t\t\t\t\t\tcount_in_cluster + 1;\n\t\tsize += blks_to_bytes(inode, appended_blks);\n\t\tstart_blk += appended_blks;\n\t\tcompr_cluster = false;\n\t} else {\n\t\tlogical = blks_to_bytes(inode, start_blk);\n\t\tphys = __is_valid_data_blkaddr(map.m_pblk) ?\n\t\t\tblks_to_bytes(inode, map.m_pblk) : 0;\n\t\tsize = blks_to_bytes(inode, map.m_len);\n\t\tflags = 0;\n\n\t\tif (compr_cluster) {\n\t\t\tflags = FIEMAP_EXTENT_ENCODED;\n\t\t\tcount_in_cluster += map.m_len;\n\t\t\tif (count_in_cluster == cluster_size) {\n\t\t\t\tcompr_cluster = false;\n\t\t\t\tsize += blks_to_bytes(inode, 1);\n\t\t\t}\n\t\t} else if (map.m_flags & F2FS_MAP_DELALLOC) {\n\t\t\tflags = FIEMAP_EXTENT_UNWRITTEN;\n\t\t}\n\n\t\tstart_blk += bytes_to_blks(inode, size);\n\t}\n\nprep_next:\n\tcond_resched();\n\tif (fatal_signal_pending(current))\n\t\tret = -EINTR;\n\telse\n\t\tgoto next;\nout:\n\tif (ret == 1)\n\t\tret = 0;\n\n\tinode_unlock(inode);\n\treturn ret;\n}\n\nstatic inline loff_t f2fs_readpage_limit(struct inode *inode)\n{\n\tif (IS_ENABLED(CONFIG_FS_VERITY) && IS_VERITY(inode))\n\t\treturn inode->i_sb->s_maxbytes;\n\n\treturn i_size_read(inode);\n}\n\nstatic int f2fs_read_single_page(struct inode *inode, struct page *page,\n\t\t\t\t\tunsigned nr_pages,\n\t\t\t\t\tstruct f2fs_map_blocks *map,\n\t\t\t\t\tstruct bio **bio_ret,\n\t\t\t\t\tsector_t *last_block_in_bio,\n\t\t\t\t\tbool is_readahead)\n{\n\tstruct bio *bio = *bio_ret;\n\tconst unsigned blocksize = blks_to_bytes(inode, 1);\n\tsector_t block_in_file;\n\tsector_t last_block;\n\tsector_t last_block_in_file;\n\tsector_t block_nr;\n\tint ret = 0;\n\n\tblock_in_file = (sector_t)page_index(page);\n\tlast_block = block_in_file + nr_pages;\n\tlast_block_in_file = bytes_to_blks(inode,\n\t\t\tf2fs_readpage_limit(inode) + blocksize - 1);\n\tif (last_block > last_block_in_file)\n\t\tlast_block = last_block_in_file;\n\n\t \n\tif (block_in_file >= last_block)\n\t\tgoto zero_out;\n\t \n\tif ((map->m_flags & F2FS_MAP_MAPPED) &&\n\t\t\tblock_in_file > map->m_lblk &&\n\t\t\tblock_in_file < (map->m_lblk + map->m_len))\n\t\tgoto got_it;\n\n\t \n\tmap->m_lblk = block_in_file;\n\tmap->m_len = last_block - block_in_file;\n\n\tret = f2fs_map_blocks(inode, map, F2FS_GET_BLOCK_DEFAULT);\n\tif (ret)\n\t\tgoto out;\ngot_it:\n\tif ((map->m_flags & F2FS_MAP_MAPPED)) {\n\t\tblock_nr = map->m_pblk + block_in_file - map->m_lblk;\n\t\tSetPageMappedToDisk(page);\n\n\t\tif (!f2fs_is_valid_blkaddr(F2FS_I_SB(inode), block_nr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE_READ)) {\n\t\t\tret = -EFSCORRUPTED;\n\t\t\tf2fs_handle_error(F2FS_I_SB(inode),\n\t\t\t\t\t\tERROR_INVALID_BLKADDR);\n\t\t\tgoto out;\n\t\t}\n\t} else {\nzero_out:\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tif (f2fs_need_verity(inode, page->index) &&\n\t\t    !fsverity_verify_page(page)) {\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (!PageUptodate(page))\n\t\t\tSetPageUptodate(page);\n\t\tunlock_page(page);\n\t\tgoto out;\n\t}\n\n\t \n\tif (bio && (!page_is_mergeable(F2FS_I_SB(inode), bio,\n\t\t\t\t       *last_block_in_bio, block_nr) ||\n\t\t    !f2fs_crypt_mergeable_bio(bio, inode, page->index, NULL))) {\nsubmit_and_realloc:\n\t\tf2fs_submit_read_bio(F2FS_I_SB(inode), bio, DATA);\n\t\tbio = NULL;\n\t}\n\tif (bio == NULL) {\n\t\tbio = f2fs_grab_read_bio(inode, block_nr, nr_pages,\n\t\t\t\tis_readahead ? REQ_RAHEAD : 0, page->index,\n\t\t\t\tfalse);\n\t\tif (IS_ERR(bio)) {\n\t\t\tret = PTR_ERR(bio);\n\t\t\tbio = NULL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t \n\tf2fs_wait_on_block_writeback(inode, block_nr);\n\n\tif (bio_add_page(bio, page, blocksize, 0) < blocksize)\n\t\tgoto submit_and_realloc;\n\n\tinc_page_count(F2FS_I_SB(inode), F2FS_RD_DATA);\n\tf2fs_update_iostat(F2FS_I_SB(inode), NULL, FS_DATA_READ_IO,\n\t\t\t\t\t\t\tF2FS_BLKSIZE);\n\t*last_block_in_bio = block_nr;\nout:\n\t*bio_ret = bio;\n\treturn ret;\n}\n\n#ifdef CONFIG_F2FS_FS_COMPRESSION\nint f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,\n\t\t\t\tunsigned nr_pages, sector_t *last_block_in_bio,\n\t\t\t\tbool is_readahead, bool for_write)\n{\n\tstruct dnode_of_data dn;\n\tstruct inode *inode = cc->inode;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct bio *bio = *bio_ret;\n\tunsigned int start_idx = cc->cluster_idx << cc->log_cluster_size;\n\tsector_t last_block_in_file;\n\tconst unsigned blocksize = blks_to_bytes(inode, 1);\n\tstruct decompress_io_ctx *dic = NULL;\n\tstruct extent_info ei = {};\n\tbool from_dnode = true;\n\tint i;\n\tint ret = 0;\n\n\tf2fs_bug_on(sbi, f2fs_cluster_is_empty(cc));\n\n\tlast_block_in_file = bytes_to_blks(inode,\n\t\t\tf2fs_readpage_limit(inode) + blocksize - 1);\n\n\t \n\tfor (i = 0; i < cc->cluster_size; i++) {\n\t\tstruct page *page = cc->rpages[i];\n\n\t\tif (!page)\n\t\t\tcontinue;\n\t\tif ((sector_t)page->index >= last_block_in_file) {\n\t\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\t\tif (!PageUptodate(page))\n\t\t\t\tSetPageUptodate(page);\n\t\t} else if (!PageUptodate(page)) {\n\t\t\tcontinue;\n\t\t}\n\t\tunlock_page(page);\n\t\tif (for_write)\n\t\t\tput_page(page);\n\t\tcc->rpages[i] = NULL;\n\t\tcc->nr_rpages--;\n\t}\n\n\t \n\tif (f2fs_cluster_is_empty(cc))\n\t\tgoto out;\n\n\tif (f2fs_lookup_read_extent_cache(inode, start_idx, &ei))\n\t\tfrom_dnode = false;\n\n\tif (!from_dnode)\n\t\tgoto skip_reading_dnode;\n\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\tret = f2fs_get_dnode_of_data(&dn, start_idx, LOOKUP_NODE);\n\tif (ret)\n\t\tgoto out;\n\n\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\tret = -EIO;\n\t\tgoto out_put_dnode;\n\t}\n\tf2fs_bug_on(sbi, dn.data_blkaddr != COMPRESS_ADDR);\n\nskip_reading_dnode:\n\tfor (i = 1; i < cc->cluster_size; i++) {\n\t\tblock_t blkaddr;\n\n\t\tblkaddr = from_dnode ? data_blkaddr(dn.inode, dn.node_page,\n\t\t\t\t\tdn.ofs_in_node + i) :\n\t\t\t\t\tei.blk + i - 1;\n\n\t\tif (!__is_valid_data_blkaddr(blkaddr))\n\t\t\tbreak;\n\n\t\tif (!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC)) {\n\t\t\tret = -EFAULT;\n\t\t\tgoto out_put_dnode;\n\t\t}\n\t\tcc->nr_cpages++;\n\n\t\tif (!from_dnode && i >= ei.c_len)\n\t\t\tbreak;\n\t}\n\n\t \n\tif (cc->nr_cpages == 0) {\n\t\tret = 0;\n\t\tgoto out_put_dnode;\n\t}\n\n\tdic = f2fs_alloc_dic(cc);\n\tif (IS_ERR(dic)) {\n\t\tret = PTR_ERR(dic);\n\t\tgoto out_put_dnode;\n\t}\n\n\tfor (i = 0; i < cc->nr_cpages; i++) {\n\t\tstruct page *page = dic->cpages[i];\n\t\tblock_t blkaddr;\n\t\tstruct bio_post_read_ctx *ctx;\n\n\t\tblkaddr = from_dnode ? data_blkaddr(dn.inode, dn.node_page,\n\t\t\t\t\tdn.ofs_in_node + i + 1) :\n\t\t\t\t\tei.blk + i;\n\n\t\tf2fs_wait_on_block_writeback(inode, blkaddr);\n\n\t\tif (f2fs_load_compressed_page(sbi, page, blkaddr)) {\n\t\t\tif (atomic_dec_and_test(&dic->remaining_pages)) {\n\t\t\t\tf2fs_decompress_cluster(dic, true);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (bio && (!page_is_mergeable(sbi, bio,\n\t\t\t\t\t*last_block_in_bio, blkaddr) ||\n\t\t    !f2fs_crypt_mergeable_bio(bio, inode, page->index, NULL))) {\nsubmit_and_realloc:\n\t\t\tf2fs_submit_read_bio(sbi, bio, DATA);\n\t\t\tbio = NULL;\n\t\t}\n\n\t\tif (!bio) {\n\t\t\tbio = f2fs_grab_read_bio(inode, blkaddr, nr_pages,\n\t\t\t\t\tis_readahead ? REQ_RAHEAD : 0,\n\t\t\t\t\tpage->index, for_write);\n\t\t\tif (IS_ERR(bio)) {\n\t\t\t\tret = PTR_ERR(bio);\n\t\t\t\tf2fs_decompress_end_io(dic, ret, true);\n\t\t\t\tf2fs_put_dnode(&dn);\n\t\t\t\t*bio_ret = NULL;\n\t\t\t\treturn ret;\n\t\t\t}\n\t\t}\n\n\t\tif (bio_add_page(bio, page, blocksize, 0) < blocksize)\n\t\t\tgoto submit_and_realloc;\n\n\t\tctx = get_post_read_ctx(bio);\n\t\tctx->enabled_steps |= STEP_DECOMPRESS;\n\t\trefcount_inc(&dic->refcnt);\n\n\t\tinc_page_count(sbi, F2FS_RD_DATA);\n\t\tf2fs_update_iostat(sbi, inode, FS_DATA_READ_IO, F2FS_BLKSIZE);\n\t\t*last_block_in_bio = blkaddr;\n\t}\n\n\tif (from_dnode)\n\t\tf2fs_put_dnode(&dn);\n\n\t*bio_ret = bio;\n\treturn 0;\n\nout_put_dnode:\n\tif (from_dnode)\n\t\tf2fs_put_dnode(&dn);\nout:\n\tfor (i = 0; i < cc->cluster_size; i++) {\n\t\tif (cc->rpages[i]) {\n\t\t\tClearPageUptodate(cc->rpages[i]);\n\t\t\tunlock_page(cc->rpages[i]);\n\t\t}\n\t}\n\t*bio_ret = bio;\n\treturn ret;\n}\n#endif\n\n \nstatic int f2fs_mpage_readpages(struct inode *inode,\n\t\tstruct readahead_control *rac, struct page *page)\n{\n\tstruct bio *bio = NULL;\n\tsector_t last_block_in_bio = 0;\n\tstruct f2fs_map_blocks map;\n#ifdef CONFIG_F2FS_FS_COMPRESSION\n\tstruct compress_ctx cc = {\n\t\t.inode = inode,\n\t\t.log_cluster_size = F2FS_I(inode)->i_log_cluster_size,\n\t\t.cluster_size = F2FS_I(inode)->i_cluster_size,\n\t\t.cluster_idx = NULL_CLUSTER,\n\t\t.rpages = NULL,\n\t\t.cpages = NULL,\n\t\t.nr_rpages = 0,\n\t\t.nr_cpages = 0,\n\t};\n\tpgoff_t nc_cluster_idx = NULL_CLUSTER;\n#endif\n\tunsigned nr_pages = rac ? readahead_count(rac) : 1;\n\tunsigned max_nr_pages = nr_pages;\n\tint ret = 0;\n\n\tmap.m_pblk = 0;\n\tmap.m_lblk = 0;\n\tmap.m_len = 0;\n\tmap.m_flags = 0;\n\tmap.m_next_pgofs = NULL;\n\tmap.m_next_extent = NULL;\n\tmap.m_seg_type = NO_CHECK_TYPE;\n\tmap.m_may_create = false;\n\n\tfor (; nr_pages; nr_pages--) {\n\t\tif (rac) {\n\t\t\tpage = readahead_page(rac);\n\t\t\tprefetchw(&page->flags);\n\t\t}\n\n#ifdef CONFIG_F2FS_FS_COMPRESSION\n\t\tif (f2fs_compressed_file(inode)) {\n\t\t\t \n\t\t\tif (!f2fs_cluster_can_merge_page(&cc, page->index)) {\n\t\t\t\tret = f2fs_read_multi_pages(&cc, &bio,\n\t\t\t\t\t\t\tmax_nr_pages,\n\t\t\t\t\t\t\t&last_block_in_bio,\n\t\t\t\t\t\t\trac != NULL, false);\n\t\t\t\tf2fs_destroy_compress_ctx(&cc, false);\n\t\t\t\tif (ret)\n\t\t\t\t\tgoto set_error_page;\n\t\t\t}\n\t\t\tif (cc.cluster_idx == NULL_CLUSTER) {\n\t\t\t\tif (nc_cluster_idx ==\n\t\t\t\t\tpage->index >> cc.log_cluster_size) {\n\t\t\t\t\tgoto read_single_page;\n\t\t\t\t}\n\n\t\t\t\tret = f2fs_is_compressed_cluster(inode, page->index);\n\t\t\t\tif (ret < 0)\n\t\t\t\t\tgoto set_error_page;\n\t\t\t\telse if (!ret) {\n\t\t\t\t\tnc_cluster_idx =\n\t\t\t\t\t\tpage->index >> cc.log_cluster_size;\n\t\t\t\t\tgoto read_single_page;\n\t\t\t\t}\n\n\t\t\t\tnc_cluster_idx = NULL_CLUSTER;\n\t\t\t}\n\t\t\tret = f2fs_init_compress_ctx(&cc);\n\t\t\tif (ret)\n\t\t\t\tgoto set_error_page;\n\n\t\t\tf2fs_compress_ctx_add_page(&cc, page);\n\n\t\t\tgoto next_page;\n\t\t}\nread_single_page:\n#endif\n\n\t\tret = f2fs_read_single_page(inode, page, max_nr_pages, &map,\n\t\t\t\t\t&bio, &last_block_in_bio, rac);\n\t\tif (ret) {\n#ifdef CONFIG_F2FS_FS_COMPRESSION\nset_error_page:\n#endif\n\t\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\t\tunlock_page(page);\n\t\t}\n#ifdef CONFIG_F2FS_FS_COMPRESSION\nnext_page:\n#endif\n\t\tif (rac)\n\t\t\tput_page(page);\n\n#ifdef CONFIG_F2FS_FS_COMPRESSION\n\t\tif (f2fs_compressed_file(inode)) {\n\t\t\t \n\t\t\tif (nr_pages == 1 && !f2fs_cluster_is_empty(&cc)) {\n\t\t\t\tret = f2fs_read_multi_pages(&cc, &bio,\n\t\t\t\t\t\t\tmax_nr_pages,\n\t\t\t\t\t\t\t&last_block_in_bio,\n\t\t\t\t\t\t\trac != NULL, false);\n\t\t\t\tf2fs_destroy_compress_ctx(&cc, false);\n\t\t\t}\n\t\t}\n#endif\n\t}\n\tif (bio)\n\t\tf2fs_submit_read_bio(F2FS_I_SB(inode), bio, DATA);\n\treturn ret;\n}\n\nstatic int f2fs_read_data_folio(struct file *file, struct folio *folio)\n{\n\tstruct page *page = &folio->page;\n\tstruct inode *inode = page_file_mapping(page)->host;\n\tint ret = -EAGAIN;\n\n\ttrace_f2fs_readpage(page, DATA);\n\n\tif (!f2fs_is_compress_backend_ready(inode)) {\n\t\tunlock_page(page);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t \n\tif (f2fs_has_inline_data(inode))\n\t\tret = f2fs_read_inline_data(inode, page);\n\tif (ret == -EAGAIN)\n\t\tret = f2fs_mpage_readpages(inode, NULL, page);\n\treturn ret;\n}\n\nstatic void f2fs_readahead(struct readahead_control *rac)\n{\n\tstruct inode *inode = rac->mapping->host;\n\n\ttrace_f2fs_readpages(inode, readahead_index(rac), readahead_count(rac));\n\n\tif (!f2fs_is_compress_backend_ready(inode))\n\t\treturn;\n\n\t \n\tif (f2fs_has_inline_data(inode))\n\t\treturn;\n\n\tf2fs_mpage_readpages(inode, rac, NULL);\n}\n\nint f2fs_encrypt_one_page(struct f2fs_io_info *fio)\n{\n\tstruct inode *inode = fio->page->mapping->host;\n\tstruct page *mpage, *page;\n\tgfp_t gfp_flags = GFP_NOFS;\n\n\tif (!f2fs_encrypted_file(inode))\n\t\treturn 0;\n\n\tpage = fio->compressed_page ? fio->compressed_page : fio->page;\n\n\tif (fscrypt_inode_uses_inline_crypto(inode))\n\t\treturn 0;\n\nretry_encrypt:\n\tfio->encrypted_page = fscrypt_encrypt_pagecache_blocks(page,\n\t\t\t\t\tPAGE_SIZE, 0, gfp_flags);\n\tif (IS_ERR(fio->encrypted_page)) {\n\t\t \n\t\tif (PTR_ERR(fio->encrypted_page) == -ENOMEM) {\n\t\t\tf2fs_flush_merged_writes(fio->sbi);\n\t\t\tmemalloc_retry_wait(GFP_NOFS);\n\t\t\tgfp_flags |= __GFP_NOFAIL;\n\t\t\tgoto retry_encrypt;\n\t\t}\n\t\treturn PTR_ERR(fio->encrypted_page);\n\t}\n\n\tmpage = find_lock_page(META_MAPPING(fio->sbi), fio->old_blkaddr);\n\tif (mpage) {\n\t\tif (PageUptodate(mpage))\n\t\t\tmemcpy(page_address(mpage),\n\t\t\t\tpage_address(fio->encrypted_page), PAGE_SIZE);\n\t\tf2fs_put_page(mpage, 1);\n\t}\n\treturn 0;\n}\n\nstatic inline bool check_inplace_update_policy(struct inode *inode,\n\t\t\t\tstruct f2fs_io_info *fio)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tif (IS_F2FS_IPU_HONOR_OPU_WRITE(sbi) &&\n\t    is_inode_flag_set(inode, FI_OPU_WRITE))\n\t\treturn false;\n\tif (IS_F2FS_IPU_FORCE(sbi))\n\t\treturn true;\n\tif (IS_F2FS_IPU_SSR(sbi) && f2fs_need_SSR(sbi))\n\t\treturn true;\n\tif (IS_F2FS_IPU_UTIL(sbi) && utilization(sbi) > SM_I(sbi)->min_ipu_util)\n\t\treturn true;\n\tif (IS_F2FS_IPU_SSR_UTIL(sbi) && f2fs_need_SSR(sbi) &&\n\t    utilization(sbi) > SM_I(sbi)->min_ipu_util)\n\t\treturn true;\n\n\t \n\tif (IS_F2FS_IPU_ASYNC(sbi) && fio && fio->op == REQ_OP_WRITE &&\n\t    !(fio->op_flags & REQ_SYNC) && !IS_ENCRYPTED(inode))\n\t\treturn true;\n\n\t \n\tif (IS_F2FS_IPU_FSYNC(sbi) && is_inode_flag_set(inode, FI_NEED_IPU))\n\t\treturn true;\n\n\tif (unlikely(fio && is_sbi_flag_set(sbi, SBI_CP_DISABLED) &&\n\t\t\t!f2fs_is_checkpointed_data(sbi, fio->old_blkaddr)))\n\t\treturn true;\n\n\treturn false;\n}\n\nbool f2fs_should_update_inplace(struct inode *inode, struct f2fs_io_info *fio)\n{\n\t \n\tif (is_inode_flag_set(inode, FI_ALIGNED_WRITE))\n\t\treturn false;\n\n\tif (f2fs_is_pinned_file(inode))\n\t\treturn true;\n\n\t \n\tif (file_is_cold(inode) && !is_inode_flag_set(inode, FI_OPU_WRITE))\n\t\treturn true;\n\n\treturn check_inplace_update_policy(inode, fio);\n}\n\nbool f2fs_should_update_outplace(struct inode *inode, struct f2fs_io_info *fio)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\t \n\tif (f2fs_is_pinned_file(inode))\n\t\treturn false;\n\tif (fio && is_sbi_flag_set(sbi, SBI_NEED_FSCK))\n\t\treturn true;\n\tif (f2fs_lfs_mode(sbi))\n\t\treturn true;\n\tif (S_ISDIR(inode->i_mode))\n\t\treturn true;\n\tif (IS_NOQUOTA(inode))\n\t\treturn true;\n\tif (f2fs_is_atomic_file(inode))\n\t\treturn true;\n\n\t \n\tif (is_inode_flag_set(inode, FI_ALIGNED_WRITE))\n\t\treturn true;\n\n\tif (is_inode_flag_set(inode, FI_OPU_WRITE))\n\t\treturn true;\n\n\tif (fio) {\n\t\tif (page_private_gcing(fio->page))\n\t\t\treturn true;\n\t\tif (page_private_dummy(fio->page))\n\t\t\treturn true;\n\t\tif (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED) &&\n\t\t\tf2fs_is_checkpointed_data(sbi, fio->old_blkaddr)))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic inline bool need_inplace_update(struct f2fs_io_info *fio)\n{\n\tstruct inode *inode = fio->page->mapping->host;\n\n\tif (f2fs_should_update_outplace(inode, fio))\n\t\treturn false;\n\n\treturn f2fs_should_update_inplace(inode, fio);\n}\n\nint f2fs_do_write_data_page(struct f2fs_io_info *fio)\n{\n\tstruct page *page = fio->page;\n\tstruct inode *inode = page->mapping->host;\n\tstruct dnode_of_data dn;\n\tstruct node_info ni;\n\tbool ipu_force = false;\n\tint err = 0;\n\n\t \n\tif (f2fs_is_atomic_file(inode))\n\t\tset_new_dnode(&dn, F2FS_I(inode)->cow_inode, NULL, NULL, 0);\n\telse\n\t\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\n\tif (need_inplace_update(fio) &&\n\t    f2fs_lookup_read_extent_cache_block(inode, page->index,\n\t\t\t\t\t\t&fio->old_blkaddr)) {\n\t\tif (!f2fs_is_valid_blkaddr(fio->sbi, fio->old_blkaddr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE)) {\n\t\t\tf2fs_handle_error(fio->sbi,\n\t\t\t\t\t\tERROR_INVALID_BLKADDR);\n\t\t\treturn -EFSCORRUPTED;\n\t\t}\n\n\t\tipu_force = true;\n\t\tfio->need_lock = LOCK_DONE;\n\t\tgoto got_it;\n\t}\n\n\t \n\tif (fio->need_lock == LOCK_REQ && !f2fs_trylock_op(fio->sbi))\n\t\treturn -EAGAIN;\n\n\terr = f2fs_get_dnode_of_data(&dn, page->index, LOOKUP_NODE);\n\tif (err)\n\t\tgoto out;\n\n\tfio->old_blkaddr = dn.data_blkaddr;\n\n\t \n\tif (fio->old_blkaddr == NULL_ADDR) {\n\t\tClearPageUptodate(page);\n\t\tclear_page_private_gcing(page);\n\t\tgoto out_writepage;\n\t}\ngot_it:\n\tif (__is_valid_data_blkaddr(fio->old_blkaddr) &&\n\t\t!f2fs_is_valid_blkaddr(fio->sbi, fio->old_blkaddr,\n\t\t\t\t\t\tDATA_GENERIC_ENHANCE)) {\n\t\terr = -EFSCORRUPTED;\n\t\tf2fs_handle_error(fio->sbi, ERROR_INVALID_BLKADDR);\n\t\tgoto out_writepage;\n\t}\n\n\t \n\tif (fio->post_read)\n\t\tf2fs_wait_on_block_writeback(inode, fio->old_blkaddr);\n\n\t \n\tif (ipu_force ||\n\t\t(__is_valid_data_blkaddr(fio->old_blkaddr) &&\n\t\t\t\t\tneed_inplace_update(fio))) {\n\t\terr = f2fs_encrypt_one_page(fio);\n\t\tif (err)\n\t\t\tgoto out_writepage;\n\n\t\tset_page_writeback(page);\n\t\tf2fs_put_dnode(&dn);\n\t\tif (fio->need_lock == LOCK_REQ)\n\t\t\tf2fs_unlock_op(fio->sbi);\n\t\terr = f2fs_inplace_write_data(fio);\n\t\tif (err) {\n\t\t\tif (fscrypt_inode_uses_fs_layer_crypto(inode))\n\t\t\t\tfscrypt_finalize_bounce_page(&fio->encrypted_page);\n\t\t\tif (PageWriteback(page))\n\t\t\t\tend_page_writeback(page);\n\t\t} else {\n\t\t\tset_inode_flag(inode, FI_UPDATE_WRITE);\n\t\t}\n\t\ttrace_f2fs_do_write_data_page(fio->page, IPU);\n\t\treturn err;\n\t}\n\n\tif (fio->need_lock == LOCK_RETRY) {\n\t\tif (!f2fs_trylock_op(fio->sbi)) {\n\t\t\terr = -EAGAIN;\n\t\t\tgoto out_writepage;\n\t\t}\n\t\tfio->need_lock = LOCK_REQ;\n\t}\n\n\terr = f2fs_get_node_info(fio->sbi, dn.nid, &ni, false);\n\tif (err)\n\t\tgoto out_writepage;\n\n\tfio->version = ni.version;\n\n\terr = f2fs_encrypt_one_page(fio);\n\tif (err)\n\t\tgoto out_writepage;\n\n\tset_page_writeback(page);\n\n\tif (fio->compr_blocks && fio->old_blkaddr == COMPRESS_ADDR)\n\t\tf2fs_i_compr_blocks_update(inode, fio->compr_blocks - 1, false);\n\n\t \n\tf2fs_outplace_write_data(&dn, fio);\n\ttrace_f2fs_do_write_data_page(page, OPU);\n\tset_inode_flag(inode, FI_APPEND_WRITE);\n\tif (page->index == 0)\n\t\tset_inode_flag(inode, FI_FIRST_BLOCK_WRITTEN);\nout_writepage:\n\tf2fs_put_dnode(&dn);\nout:\n\tif (fio->need_lock == LOCK_REQ)\n\t\tf2fs_unlock_op(fio->sbi);\n\treturn err;\n}\n\nint f2fs_write_single_data_page(struct page *page, int *submitted,\n\t\t\t\tstruct bio **bio,\n\t\t\t\tsector_t *last_block,\n\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\tenum iostat_type io_type,\n\t\t\t\tint compr_blocks,\n\t\t\t\tbool allow_balance)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tloff_t i_size = i_size_read(inode);\n\tconst pgoff_t end_index = ((unsigned long long)i_size)\n\t\t\t\t\t\t\t>> PAGE_SHIFT;\n\tloff_t psize = (loff_t)(page->index + 1) << PAGE_SHIFT;\n\tunsigned offset = 0;\n\tbool need_balance_fs = false;\n\tbool quota_inode = IS_NOQUOTA(inode);\n\tint err = 0;\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = sbi,\n\t\t.ino = inode->i_ino,\n\t\t.type = DATA,\n\t\t.op = REQ_OP_WRITE,\n\t\t.op_flags = wbc_to_write_flags(wbc),\n\t\t.old_blkaddr = NULL_ADDR,\n\t\t.page = page,\n\t\t.encrypted_page = NULL,\n\t\t.submitted = 0,\n\t\t.compr_blocks = compr_blocks,\n\t\t.need_lock = LOCK_RETRY,\n\t\t.post_read = f2fs_post_read_required(inode) ? 1 : 0,\n\t\t.io_type = io_type,\n\t\t.io_wbc = wbc,\n\t\t.bio = bio,\n\t\t.last_block = last_block,\n\t};\n\n\ttrace_f2fs_writepage(page, DATA);\n\n\t \n\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\tmapping_set_error(page->mapping, -EIO);\n\t\t \n\t\tif (S_ISDIR(inode->i_mode) &&\n\t\t\t\t!is_sbi_flag_set(sbi, SBI_IS_CLOSE))\n\t\t\tgoto redirty_out;\n\n\t\t \n\t\tif (F2FS_OPTION(sbi).errors == MOUNT_ERRORS_READONLY)\n\t\t\tgoto redirty_out;\n\t\tgoto out;\n\t}\n\n\tif (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))\n\t\tgoto redirty_out;\n\n\tif (page->index < end_index ||\n\t\t\tf2fs_verity_in_progress(inode) ||\n\t\t\tcompr_blocks)\n\t\tgoto write;\n\n\t \n\toffset = i_size & (PAGE_SIZE - 1);\n\tif ((page->index >= end_index + 1) || !offset)\n\t\tgoto out;\n\n\tzero_user_segment(page, offset, PAGE_SIZE);\nwrite:\n\tif (f2fs_is_drop_cache(inode))\n\t\tgoto out;\n\n\t \n\tif (S_ISDIR(inode->i_mode) || quota_inode) {\n\t\t \n\t\tif (quota_inode)\n\t\t\tf2fs_down_read(&sbi->node_write);\n\n\t\tfio.need_lock = LOCK_DONE;\n\t\terr = f2fs_do_write_data_page(&fio);\n\n\t\tif (quota_inode)\n\t\t\tf2fs_up_read(&sbi->node_write);\n\n\t\tgoto done;\n\t}\n\n\tif (!wbc->for_reclaim)\n\t\tneed_balance_fs = true;\n\telse if (has_not_enough_free_secs(sbi, 0, 0))\n\t\tgoto redirty_out;\n\telse\n\t\tset_inode_flag(inode, FI_HOT_DATA);\n\n\terr = -EAGAIN;\n\tif (f2fs_has_inline_data(inode)) {\n\t\terr = f2fs_write_inline_data(inode, page);\n\t\tif (!err)\n\t\t\tgoto out;\n\t}\n\n\tif (err == -EAGAIN) {\n\t\terr = f2fs_do_write_data_page(&fio);\n\t\tif (err == -EAGAIN) {\n\t\t\tfio.need_lock = LOCK_REQ;\n\t\t\terr = f2fs_do_write_data_page(&fio);\n\t\t}\n\t}\n\n\tif (err) {\n\t\tfile_set_keep_isize(inode);\n\t} else {\n\t\tspin_lock(&F2FS_I(inode)->i_size_lock);\n\t\tif (F2FS_I(inode)->last_disk_size < psize)\n\t\t\tF2FS_I(inode)->last_disk_size = psize;\n\t\tspin_unlock(&F2FS_I(inode)->i_size_lock);\n\t}\n\ndone:\n\tif (err && err != -ENOENT)\n\t\tgoto redirty_out;\n\nout:\n\tinode_dec_dirty_pages(inode);\n\tif (err) {\n\t\tClearPageUptodate(page);\n\t\tclear_page_private_gcing(page);\n\t}\n\n\tif (wbc->for_reclaim) {\n\t\tf2fs_submit_merged_write_cond(sbi, NULL, page, 0, DATA);\n\t\tclear_inode_flag(inode, FI_HOT_DATA);\n\t\tf2fs_remove_dirty_inode(inode);\n\t\tsubmitted = NULL;\n\t}\n\tunlock_page(page);\n\tif (!S_ISDIR(inode->i_mode) && !IS_NOQUOTA(inode) &&\n\t\t\t!F2FS_I(inode)->wb_task && allow_balance)\n\t\tf2fs_balance_fs(sbi, need_balance_fs);\n\n\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\tf2fs_submit_merged_write(sbi, DATA);\n\t\tif (bio && *bio)\n\t\t\tf2fs_submit_merged_ipu_write(sbi, bio, NULL);\n\t\tsubmitted = NULL;\n\t}\n\n\tif (submitted)\n\t\t*submitted = fio.submitted;\n\n\treturn 0;\n\nredirty_out:\n\tredirty_page_for_writepage(wbc, page);\n\t \n\tif (!err || wbc->for_reclaim)\n\t\treturn AOP_WRITEPAGE_ACTIVATE;\n\tunlock_page(page);\n\treturn err;\n}\n\nstatic int f2fs_write_data_page(struct page *page,\n\t\t\t\t\tstruct writeback_control *wbc)\n{\n#ifdef CONFIG_F2FS_FS_COMPRESSION\n\tstruct inode *inode = page->mapping->host;\n\n\tif (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))\n\t\tgoto out;\n\n\tif (f2fs_compressed_file(inode)) {\n\t\tif (f2fs_is_compressed_cluster(inode, page->index)) {\n\t\t\tredirty_page_for_writepage(wbc, page);\n\t\t\treturn AOP_WRITEPAGE_ACTIVATE;\n\t\t}\n\t}\nout:\n#endif\n\n\treturn f2fs_write_single_data_page(page, NULL, NULL, NULL,\n\t\t\t\t\t\twbc, FS_DATA_IO, 0, true);\n}\n\n \nstatic int f2fs_write_cache_pages(struct address_space *mapping,\n\t\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\t\tenum iostat_type io_type)\n{\n\tint ret = 0;\n\tint done = 0, retry = 0;\n\tstruct page *pages_local[F2FS_ONSTACK_PAGES];\n\tstruct page **pages = pages_local;\n\tstruct folio_batch fbatch;\n\tstruct f2fs_sb_info *sbi = F2FS_M_SB(mapping);\n\tstruct bio *bio = NULL;\n\tsector_t last_block;\n#ifdef CONFIG_F2FS_FS_COMPRESSION\n\tstruct inode *inode = mapping->host;\n\tstruct compress_ctx cc = {\n\t\t.inode = inode,\n\t\t.log_cluster_size = F2FS_I(inode)->i_log_cluster_size,\n\t\t.cluster_size = F2FS_I(inode)->i_cluster_size,\n\t\t.cluster_idx = NULL_CLUSTER,\n\t\t.rpages = NULL,\n\t\t.nr_rpages = 0,\n\t\t.cpages = NULL,\n\t\t.valid_nr_cpages = 0,\n\t\t.rbuf = NULL,\n\t\t.cbuf = NULL,\n\t\t.rlen = PAGE_SIZE * F2FS_I(inode)->i_cluster_size,\n\t\t.private = NULL,\n\t};\n#endif\n\tint nr_folios, p, idx;\n\tint nr_pages;\n\tunsigned int max_pages = F2FS_ONSTACK_PAGES;\n\tpgoff_t index;\n\tpgoff_t end;\t\t \n\tpgoff_t done_index;\n\tint range_whole = 0;\n\txa_mark_t tag;\n\tint nwritten = 0;\n\tint submitted = 0;\n\tint i;\n\n#ifdef CONFIG_F2FS_FS_COMPRESSION\n\tif (f2fs_compressed_file(inode) &&\n\t\t1 << cc.log_cluster_size > F2FS_ONSTACK_PAGES) {\n\t\tpages = f2fs_kzalloc(sbi, sizeof(struct page *) <<\n\t\t\t\tcc.log_cluster_size, GFP_NOFS | __GFP_NOFAIL);\n\t\tmax_pages = 1 << cc.log_cluster_size;\n\t}\n#endif\n\n\tfolio_batch_init(&fbatch);\n\n\tif (get_dirty_pages(mapping->host) <=\n\t\t\t\tSM_I(F2FS_M_SB(mapping))->min_hot_blocks)\n\t\tset_inode_flag(mapping->host, FI_HOT_DATA);\n\telse\n\t\tclear_inode_flag(mapping->host, FI_HOT_DATA);\n\n\tif (wbc->range_cyclic) {\n\t\tindex = mapping->writeback_index;  \n\t\tend = -1;\n\t} else {\n\t\tindex = wbc->range_start >> PAGE_SHIFT;\n\t\tend = wbc->range_end >> PAGE_SHIFT;\n\t\tif (wbc->range_start == 0 && wbc->range_end == LLONG_MAX)\n\t\t\trange_whole = 1;\n\t}\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag = PAGECACHE_TAG_TOWRITE;\n\telse\n\t\ttag = PAGECACHE_TAG_DIRTY;\nretry:\n\tretry = 0;\n\tif (wbc->sync_mode == WB_SYNC_ALL || wbc->tagged_writepages)\n\t\ttag_pages_for_writeback(mapping, index, end);\n\tdone_index = index;\n\twhile (!done && !retry && (index <= end)) {\n\t\tnr_pages = 0;\nagain:\n\t\tnr_folios = filemap_get_folios_tag(mapping, &index, end,\n\t\t\t\ttag, &fbatch);\n\t\tif (nr_folios == 0) {\n\t\t\tif (nr_pages)\n\t\t\t\tgoto write;\n\t\t\tbreak;\n\t\t}\n\n\t\tfor (i = 0; i < nr_folios; i++) {\n\t\t\tstruct folio *folio = fbatch.folios[i];\n\n\t\t\tidx = 0;\n\t\t\tp = folio_nr_pages(folio);\nadd_more:\n\t\t\tpages[nr_pages] = folio_page(folio, idx);\n\t\t\tfolio_get(folio);\n\t\t\tif (++nr_pages == max_pages) {\n\t\t\t\tindex = folio->index + idx + 1;\n\t\t\t\tfolio_batch_release(&fbatch);\n\t\t\t\tgoto write;\n\t\t\t}\n\t\t\tif (++idx < p)\n\t\t\t\tgoto add_more;\n\t\t}\n\t\tfolio_batch_release(&fbatch);\n\t\tgoto again;\nwrite:\n\t\tfor (i = 0; i < nr_pages; i++) {\n\t\t\tstruct page *page = pages[i];\n\t\t\tstruct folio *folio = page_folio(page);\n\t\t\tbool need_readd;\nreadd:\n\t\t\tneed_readd = false;\n#ifdef CONFIG_F2FS_FS_COMPRESSION\n\t\t\tif (f2fs_compressed_file(inode)) {\n\t\t\t\tvoid *fsdata = NULL;\n\t\t\t\tstruct page *pagep;\n\t\t\t\tint ret2;\n\n\t\t\t\tret = f2fs_init_compress_ctx(&cc);\n\t\t\t\tif (ret) {\n\t\t\t\t\tdone = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tif (!f2fs_cluster_can_merge_page(&cc,\n\t\t\t\t\t\t\t\tfolio->index)) {\n\t\t\t\t\tret = f2fs_write_multi_pages(&cc,\n\t\t\t\t\t\t&submitted, wbc, io_type);\n\t\t\t\t\tif (!ret)\n\t\t\t\t\t\tneed_readd = true;\n\t\t\t\t\tgoto result;\n\t\t\t\t}\n\n\t\t\t\tif (unlikely(f2fs_cp_error(sbi)))\n\t\t\t\t\tgoto lock_folio;\n\n\t\t\t\tif (!f2fs_cluster_is_empty(&cc))\n\t\t\t\t\tgoto lock_folio;\n\n\t\t\t\tif (f2fs_all_cluster_page_ready(&cc,\n\t\t\t\t\tpages, i, nr_pages, true))\n\t\t\t\t\tgoto lock_folio;\n\n\t\t\t\tret2 = f2fs_prepare_compress_overwrite(\n\t\t\t\t\t\t\tinode, &pagep,\n\t\t\t\t\t\t\tfolio->index, &fsdata);\n\t\t\t\tif (ret2 < 0) {\n\t\t\t\t\tret = ret2;\n\t\t\t\t\tdone = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t} else if (ret2 &&\n\t\t\t\t\t(!f2fs_compress_write_end(inode,\n\t\t\t\t\t\tfsdata, folio->index, 1) ||\n\t\t\t\t\t !f2fs_all_cluster_page_ready(&cc,\n\t\t\t\t\t\tpages, i, nr_pages,\n\t\t\t\t\t\tfalse))) {\n\t\t\t\t\tretry = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n#endif\n\t\t\t \n\t\t\tif (atomic_read(&sbi->wb_sync_req[DATA]) &&\n\t\t\t\t\twbc->sync_mode == WB_SYNC_NONE) {\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t}\n#ifdef CONFIG_F2FS_FS_COMPRESSION\nlock_folio:\n#endif\n\t\t\tdone_index = folio->index;\nretry_write:\n\t\t\tfolio_lock(folio);\n\n\t\t\tif (unlikely(folio->mapping != mapping)) {\ncontinue_unlock:\n\t\t\t\tfolio_unlock(folio);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!folio_test_dirty(folio)) {\n\t\t\t\t \n\t\t\t\tgoto continue_unlock;\n\t\t\t}\n\n\t\t\tif (folio_test_writeback(folio)) {\n\t\t\t\tif (wbc->sync_mode == WB_SYNC_NONE)\n\t\t\t\t\tgoto continue_unlock;\n\t\t\t\tf2fs_wait_on_page_writeback(&folio->page, DATA, true, true);\n\t\t\t}\n\n\t\t\tif (!folio_clear_dirty_for_io(folio))\n\t\t\t\tgoto continue_unlock;\n\n#ifdef CONFIG_F2FS_FS_COMPRESSION\n\t\t\tif (f2fs_compressed_file(inode)) {\n\t\t\t\tfolio_get(folio);\n\t\t\t\tf2fs_compress_ctx_add_page(&cc, &folio->page);\n\t\t\t\tcontinue;\n\t\t\t}\n#endif\n\t\t\tret = f2fs_write_single_data_page(&folio->page,\n\t\t\t\t\t&submitted, &bio, &last_block,\n\t\t\t\t\twbc, io_type, 0, true);\n\t\t\tif (ret == AOP_WRITEPAGE_ACTIVATE)\n\t\t\t\tfolio_unlock(folio);\n#ifdef CONFIG_F2FS_FS_COMPRESSION\nresult:\n#endif\n\t\t\tnwritten += submitted;\n\t\t\twbc->nr_to_write -= submitted;\n\n\t\t\tif (unlikely(ret)) {\n\t\t\t\t \n\t\t\t\tif (ret == AOP_WRITEPAGE_ACTIVATE) {\n\t\t\t\t\tret = 0;\n\t\t\t\t\tgoto next;\n\t\t\t\t} else if (ret == -EAGAIN) {\n\t\t\t\t\tret = 0;\n\t\t\t\t\tif (wbc->sync_mode == WB_SYNC_ALL) {\n\t\t\t\t\t\tf2fs_io_schedule_timeout(\n\t\t\t\t\t\t\tDEFAULT_IO_TIMEOUT);\n\t\t\t\t\t\tgoto retry_write;\n\t\t\t\t\t}\n\t\t\t\t\tgoto next;\n\t\t\t\t}\n\t\t\t\tdone_index = folio_next_index(folio);\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (wbc->nr_to_write <= 0 &&\n\t\t\t\t\twbc->sync_mode == WB_SYNC_NONE) {\n\t\t\t\tdone = 1;\n\t\t\t\tbreak;\n\t\t\t}\nnext:\n\t\t\tif (need_readd)\n\t\t\t\tgoto readd;\n\t\t}\n\t\trelease_pages(pages, nr_pages);\n\t\tcond_resched();\n\t}\n#ifdef CONFIG_F2FS_FS_COMPRESSION\n\t \n\tif (f2fs_compressed_file(inode) && !f2fs_cluster_is_empty(&cc)) {\n\t\tret = f2fs_write_multi_pages(&cc, &submitted, wbc, io_type);\n\t\tnwritten += submitted;\n\t\twbc->nr_to_write -= submitted;\n\t\tif (ret) {\n\t\t\tdone = 1;\n\t\t\tretry = 0;\n\t\t}\n\t}\n\tif (f2fs_compressed_file(inode))\n\t\tf2fs_destroy_compress_ctx(&cc, false);\n#endif\n\tif (retry) {\n\t\tindex = 0;\n\t\tend = -1;\n\t\tgoto retry;\n\t}\n\tif (wbc->range_cyclic && !done)\n\t\tdone_index = 0;\n\tif (wbc->range_cyclic || (range_whole && wbc->nr_to_write > 0))\n\t\tmapping->writeback_index = done_index;\n\n\tif (nwritten)\n\t\tf2fs_submit_merged_write_cond(F2FS_M_SB(mapping), mapping->host,\n\t\t\t\t\t\t\t\tNULL, 0, DATA);\n\t \n\tif (bio)\n\t\tf2fs_submit_merged_ipu_write(sbi, &bio, NULL);\n\n#ifdef CONFIG_F2FS_FS_COMPRESSION\n\tif (pages != pages_local)\n\t\tkfree(pages);\n#endif\n\n\treturn ret;\n}\n\nstatic inline bool __should_serialize_io(struct inode *inode,\n\t\t\t\t\tstruct writeback_control *wbc)\n{\n\t \n\tif (F2FS_I(inode)->wb_task)\n\t\treturn false;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn false;\n\tif (IS_NOQUOTA(inode))\n\t\treturn false;\n\n\tif (f2fs_need_compress_data(inode))\n\t\treturn true;\n\tif (wbc->sync_mode != WB_SYNC_ALL)\n\t\treturn true;\n\tif (get_dirty_pages(inode) >= SM_I(F2FS_I_SB(inode))->min_seq_blocks)\n\t\treturn true;\n\treturn false;\n}\n\nstatic int __f2fs_write_data_pages(struct address_space *mapping,\n\t\t\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\t\t\tenum iostat_type io_type)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct blk_plug plug;\n\tint ret;\n\tbool locked = false;\n\n\t \n\tif (!mapping->a_ops->writepage)\n\t\treturn 0;\n\n\t \n\tif (!get_dirty_pages(inode) && wbc->sync_mode == WB_SYNC_NONE)\n\t\treturn 0;\n\n\t \n\tif (unlikely(is_sbi_flag_set(sbi, SBI_POR_DOING)))\n\t\tgoto skip_write;\n\n\tif ((S_ISDIR(inode->i_mode) || IS_NOQUOTA(inode)) &&\n\t\t\twbc->sync_mode == WB_SYNC_NONE &&\n\t\t\tget_dirty_pages(inode) < nr_pages_to_skip(sbi, DATA) &&\n\t\t\tf2fs_available_free_memory(sbi, DIRTY_DENTS))\n\t\tgoto skip_write;\n\n\t \n\tif (is_inode_flag_set(inode, FI_SKIP_WRITES))\n\t\tgoto skip_write;\n\n\ttrace_f2fs_writepages(mapping->host, wbc, DATA);\n\n\t \n\tif (wbc->sync_mode == WB_SYNC_ALL)\n\t\tatomic_inc(&sbi->wb_sync_req[DATA]);\n\telse if (atomic_read(&sbi->wb_sync_req[DATA])) {\n\t\t \n\t\tif (current->plug)\n\t\t\tblk_finish_plug(current->plug);\n\t\tgoto skip_write;\n\t}\n\n\tif (__should_serialize_io(inode, wbc)) {\n\t\tmutex_lock(&sbi->writepages);\n\t\tlocked = true;\n\t}\n\n\tblk_start_plug(&plug);\n\tret = f2fs_write_cache_pages(mapping, wbc, io_type);\n\tblk_finish_plug(&plug);\n\n\tif (locked)\n\t\tmutex_unlock(&sbi->writepages);\n\n\tif (wbc->sync_mode == WB_SYNC_ALL)\n\t\tatomic_dec(&sbi->wb_sync_req[DATA]);\n\t \n\n\tf2fs_remove_dirty_inode(inode);\n\treturn ret;\n\nskip_write:\n\twbc->pages_skipped += get_dirty_pages(inode);\n\ttrace_f2fs_writepages(mapping->host, wbc, DATA);\n\treturn 0;\n}\n\nstatic int f2fs_write_data_pages(struct address_space *mapping,\n\t\t\t    struct writeback_control *wbc)\n{\n\tstruct inode *inode = mapping->host;\n\n\treturn __f2fs_write_data_pages(mapping, wbc,\n\t\t\tF2FS_I(inode)->cp_task == current ?\n\t\t\tFS_CP_DATA_IO : FS_DATA_IO);\n}\n\nvoid f2fs_write_failed(struct inode *inode, loff_t to)\n{\n\tloff_t i_size = i_size_read(inode);\n\n\tif (IS_NOQUOTA(inode))\n\t\treturn;\n\n\t \n\tif (to > i_size && !f2fs_verity_in_progress(inode)) {\n\t\tf2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t\tfilemap_invalidate_lock(inode->i_mapping);\n\n\t\ttruncate_pagecache(inode, i_size);\n\t\tf2fs_truncate_blocks(inode, i_size, true);\n\n\t\tfilemap_invalidate_unlock(inode->i_mapping);\n\t\tf2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\t}\n}\n\nstatic int prepare_write_begin(struct f2fs_sb_info *sbi,\n\t\t\tstruct page *page, loff_t pos, unsigned len,\n\t\t\tblock_t *blk_addr, bool *node_changed)\n{\n\tstruct inode *inode = page->mapping->host;\n\tpgoff_t index = page->index;\n\tstruct dnode_of_data dn;\n\tstruct page *ipage;\n\tbool locked = false;\n\tint flag = F2FS_GET_BLOCK_PRE_AIO;\n\tint err = 0;\n\n\t \n\tif (len == PAGE_SIZE && is_inode_flag_set(inode, FI_PREALLOCATED_ALL))\n\t\treturn 0;\n\n\t \n\tif (f2fs_has_inline_data(inode)) {\n\t\tif (pos + len > MAX_INLINE_DATA(inode))\n\t\t\tflag = F2FS_GET_BLOCK_DEFAULT;\n\t\tf2fs_map_lock(sbi, flag);\n\t\tlocked = true;\n\t} else if ((pos & PAGE_MASK) >= i_size_read(inode)) {\n\t\tf2fs_map_lock(sbi, flag);\n\t\tlocked = true;\n\t}\n\nrestart:\n\t \n\tipage = f2fs_get_node_page(sbi, inode->i_ino);\n\tif (IS_ERR(ipage)) {\n\t\terr = PTR_ERR(ipage);\n\t\tgoto unlock_out;\n\t}\n\n\tset_new_dnode(&dn, inode, ipage, ipage, 0);\n\n\tif (f2fs_has_inline_data(inode)) {\n\t\tif (pos + len <= MAX_INLINE_DATA(inode)) {\n\t\t\tf2fs_do_read_inline_data(page, ipage);\n\t\t\tset_inode_flag(inode, FI_DATA_EXIST);\n\t\t\tif (inode->i_nlink)\n\t\t\t\tset_page_private_inline(ipage);\n\t\t\tgoto out;\n\t\t}\n\t\terr = f2fs_convert_inline_page(&dn, page);\n\t\tif (err || dn.data_blkaddr != NULL_ADDR)\n\t\t\tgoto out;\n\t}\n\n\tif (!f2fs_lookup_read_extent_cache_block(inode, index,\n\t\t\t\t\t\t &dn.data_blkaddr)) {\n\t\tif (locked) {\n\t\t\terr = f2fs_reserve_block(&dn, index);\n\t\t\tgoto out;\n\t\t}\n\n\t\t \n\t\terr = f2fs_get_dnode_of_data(&dn, index, LOOKUP_NODE);\n\t\tif (!err && dn.data_blkaddr != NULL_ADDR)\n\t\t\tgoto out;\n\t\tf2fs_put_dnode(&dn);\n\t\tf2fs_map_lock(sbi, F2FS_GET_BLOCK_PRE_AIO);\n\t\tWARN_ON(flag != F2FS_GET_BLOCK_PRE_AIO);\n\t\tlocked = true;\n\t\tgoto restart;\n\t}\nout:\n\tif (!err) {\n\t\t \n\t\t*blk_addr = dn.data_blkaddr;\n\t\t*node_changed = dn.node_changed;\n\t}\n\tf2fs_put_dnode(&dn);\nunlock_out:\n\tif (locked)\n\t\tf2fs_map_unlock(sbi, flag);\n\treturn err;\n}\n\nstatic int __find_data_block(struct inode *inode, pgoff_t index,\n\t\t\t\tblock_t *blk_addr)\n{\n\tstruct dnode_of_data dn;\n\tstruct page *ipage;\n\tint err = 0;\n\n\tipage = f2fs_get_node_page(F2FS_I_SB(inode), inode->i_ino);\n\tif (IS_ERR(ipage))\n\t\treturn PTR_ERR(ipage);\n\n\tset_new_dnode(&dn, inode, ipage, ipage, 0);\n\n\tif (!f2fs_lookup_read_extent_cache_block(inode, index,\n\t\t\t\t\t\t &dn.data_blkaddr)) {\n\t\t \n\t\terr = f2fs_get_dnode_of_data(&dn, index, LOOKUP_NODE);\n\t\tif (err) {\n\t\t\tdn.data_blkaddr = NULL_ADDR;\n\t\t\terr = 0;\n\t\t}\n\t}\n\t*blk_addr = dn.data_blkaddr;\n\tf2fs_put_dnode(&dn);\n\treturn err;\n}\n\nstatic int __reserve_data_block(struct inode *inode, pgoff_t index,\n\t\t\t\tblock_t *blk_addr, bool *node_changed)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct dnode_of_data dn;\n\tstruct page *ipage;\n\tint err = 0;\n\n\tf2fs_map_lock(sbi, F2FS_GET_BLOCK_PRE_AIO);\n\n\tipage = f2fs_get_node_page(sbi, inode->i_ino);\n\tif (IS_ERR(ipage)) {\n\t\terr = PTR_ERR(ipage);\n\t\tgoto unlock_out;\n\t}\n\tset_new_dnode(&dn, inode, ipage, ipage, 0);\n\n\tif (!f2fs_lookup_read_extent_cache_block(dn.inode, index,\n\t\t\t\t\t\t&dn.data_blkaddr))\n\t\terr = f2fs_reserve_block(&dn, index);\n\n\t*blk_addr = dn.data_blkaddr;\n\t*node_changed = dn.node_changed;\n\tf2fs_put_dnode(&dn);\n\nunlock_out:\n\tf2fs_map_unlock(sbi, F2FS_GET_BLOCK_PRE_AIO);\n\treturn err;\n}\n\nstatic int prepare_atomic_write_begin(struct f2fs_sb_info *sbi,\n\t\t\tstruct page *page, loff_t pos, unsigned int len,\n\t\t\tblock_t *blk_addr, bool *node_changed, bool *use_cow)\n{\n\tstruct inode *inode = page->mapping->host;\n\tstruct inode *cow_inode = F2FS_I(inode)->cow_inode;\n\tpgoff_t index = page->index;\n\tint err = 0;\n\tblock_t ori_blk_addr = NULL_ADDR;\n\n\t \n\tif ((pos & PAGE_MASK) >= i_size_read(inode))\n\t\tgoto reserve_block;\n\n\t \n\terr = __find_data_block(cow_inode, index, blk_addr);\n\tif (err) {\n\t\treturn err;\n\t} else if (*blk_addr != NULL_ADDR) {\n\t\t*use_cow = true;\n\t\treturn 0;\n\t}\n\n\tif (is_inode_flag_set(inode, FI_ATOMIC_REPLACE))\n\t\tgoto reserve_block;\n\n\t \n\terr = __find_data_block(inode, index, &ori_blk_addr);\n\tif (err)\n\t\treturn err;\n\nreserve_block:\n\t \n\terr = __reserve_data_block(cow_inode, index, blk_addr, node_changed);\n\tif (err)\n\t\treturn err;\n\tinc_atomic_write_cnt(inode);\n\n\tif (ori_blk_addr != NULL_ADDR)\n\t\t*blk_addr = ori_blk_addr;\n\treturn 0;\n}\n\nstatic int f2fs_write_begin(struct file *file, struct address_space *mapping,\n\t\tloff_t pos, unsigned len, struct page **pagep, void **fsdata)\n{\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct page *page = NULL;\n\tpgoff_t index = ((unsigned long long) pos) >> PAGE_SHIFT;\n\tbool need_balance = false;\n\tbool use_cow = false;\n\tblock_t blkaddr = NULL_ADDR;\n\tint err = 0;\n\n\ttrace_f2fs_write_begin(inode, pos, len);\n\n\tif (!f2fs_is_checkpoint_ready(sbi)) {\n\t\terr = -ENOSPC;\n\t\tgoto fail;\n\t}\n\n\t \n\tif (index != 0) {\n\t\terr = f2fs_convert_inline_inode(inode);\n\t\tif (err)\n\t\t\tgoto fail;\n\t}\n\n#ifdef CONFIG_F2FS_FS_COMPRESSION\n\tif (f2fs_compressed_file(inode)) {\n\t\tint ret;\n\n\t\t*fsdata = NULL;\n\n\t\tif (len == PAGE_SIZE && !(f2fs_is_atomic_file(inode)))\n\t\t\tgoto repeat;\n\n\t\tret = f2fs_prepare_compress_overwrite(inode, pagep,\n\t\t\t\t\t\t\tindex, fsdata);\n\t\tif (ret < 0) {\n\t\t\terr = ret;\n\t\t\tgoto fail;\n\t\t} else if (ret) {\n\t\t\treturn 0;\n\t\t}\n\t}\n#endif\n\nrepeat:\n\t \n\tpage = f2fs_pagecache_get_page(mapping, index,\n\t\t\t\tFGP_LOCK | FGP_WRITE | FGP_CREAT, GFP_NOFS);\n\tif (!page) {\n\t\terr = -ENOMEM;\n\t\tgoto fail;\n\t}\n\n\t \n\n\t*pagep = page;\n\n\tif (f2fs_is_atomic_file(inode))\n\t\terr = prepare_atomic_write_begin(sbi, page, pos, len,\n\t\t\t\t\t&blkaddr, &need_balance, &use_cow);\n\telse\n\t\terr = prepare_write_begin(sbi, page, pos, len,\n\t\t\t\t\t&blkaddr, &need_balance);\n\tif (err)\n\t\tgoto fail;\n\n\tif (need_balance && !IS_NOQUOTA(inode) &&\n\t\t\thas_not_enough_free_secs(sbi, 0, 0)) {\n\t\tunlock_page(page);\n\t\tf2fs_balance_fs(sbi, true);\n\t\tlock_page(page);\n\t\tif (page->mapping != mapping) {\n\t\t\t \n\t\t\tf2fs_put_page(page, 1);\n\t\t\tgoto repeat;\n\t\t}\n\t}\n\n\tf2fs_wait_on_page_writeback(page, DATA, false, true);\n\n\tif (len == PAGE_SIZE || PageUptodate(page))\n\t\treturn 0;\n\n\tif (!(pos & (PAGE_SIZE - 1)) && (pos + len) >= i_size_read(inode) &&\n\t    !f2fs_verity_in_progress(inode)) {\n\t\tzero_user_segment(page, len, PAGE_SIZE);\n\t\treturn 0;\n\t}\n\n\tif (blkaddr == NEW_ADDR) {\n\t\tzero_user_segment(page, 0, PAGE_SIZE);\n\t\tSetPageUptodate(page);\n\t} else {\n\t\tif (!f2fs_is_valid_blkaddr(sbi, blkaddr,\n\t\t\t\tDATA_GENERIC_ENHANCE_READ)) {\n\t\t\terr = -EFSCORRUPTED;\n\t\t\tf2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);\n\t\t\tgoto fail;\n\t\t}\n\t\terr = f2fs_submit_page_read(use_cow ?\n\t\t\t\tF2FS_I(inode)->cow_inode : inode, page,\n\t\t\t\tblkaddr, 0, true);\n\t\tif (err)\n\t\t\tgoto fail;\n\n\t\tlock_page(page);\n\t\tif (unlikely(page->mapping != mapping)) {\n\t\t\tf2fs_put_page(page, 1);\n\t\t\tgoto repeat;\n\t\t}\n\t\tif (unlikely(!PageUptodate(page))) {\n\t\t\terr = -EIO;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\treturn 0;\n\nfail:\n\tf2fs_put_page(page, 1);\n\tf2fs_write_failed(inode, pos + len);\n\treturn err;\n}\n\nstatic int f2fs_write_end(struct file *file,\n\t\t\tstruct address_space *mapping,\n\t\t\tloff_t pos, unsigned len, unsigned copied,\n\t\t\tstruct page *page, void *fsdata)\n{\n\tstruct inode *inode = page->mapping->host;\n\n\ttrace_f2fs_write_end(inode, pos, len, copied);\n\n\t \n\tif (!PageUptodate(page)) {\n\t\tif (unlikely(copied != len))\n\t\t\tcopied = 0;\n\t\telse\n\t\t\tSetPageUptodate(page);\n\t}\n\n#ifdef CONFIG_F2FS_FS_COMPRESSION\n\t \n\tif (f2fs_compressed_file(inode) && fsdata) {\n\t\tf2fs_compress_write_end(inode, fsdata, page->index, copied);\n\t\tf2fs_update_time(F2FS_I_SB(inode), REQ_TIME);\n\n\t\tif (pos + copied > i_size_read(inode) &&\n\t\t\t\t!f2fs_verity_in_progress(inode))\n\t\t\tf2fs_i_size_write(inode, pos + copied);\n\t\treturn copied;\n\t}\n#endif\n\n\tif (!copied)\n\t\tgoto unlock_out;\n\n\tset_page_dirty(page);\n\n\tif (pos + copied > i_size_read(inode) &&\n\t    !f2fs_verity_in_progress(inode)) {\n\t\tf2fs_i_size_write(inode, pos + copied);\n\t\tif (f2fs_is_atomic_file(inode))\n\t\t\tf2fs_i_size_write(F2FS_I(inode)->cow_inode,\n\t\t\t\t\tpos + copied);\n\t}\nunlock_out:\n\tf2fs_put_page(page, 1);\n\tf2fs_update_time(F2FS_I_SB(inode), REQ_TIME);\n\treturn copied;\n}\n\nvoid f2fs_invalidate_folio(struct folio *folio, size_t offset, size_t length)\n{\n\tstruct inode *inode = folio->mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\n\tif (inode->i_ino >= F2FS_ROOT_INO(sbi) &&\n\t\t\t\t(offset || length != folio_size(folio)))\n\t\treturn;\n\n\tif (folio_test_dirty(folio)) {\n\t\tif (inode->i_ino == F2FS_META_INO(sbi)) {\n\t\t\tdec_page_count(sbi, F2FS_DIRTY_META);\n\t\t} else if (inode->i_ino == F2FS_NODE_INO(sbi)) {\n\t\t\tdec_page_count(sbi, F2FS_DIRTY_NODES);\n\t\t} else {\n\t\t\tinode_dec_dirty_pages(inode);\n\t\t\tf2fs_remove_dirty_inode(inode);\n\t\t}\n\t}\n\tclear_page_private_all(&folio->page);\n}\n\nbool f2fs_release_folio(struct folio *folio, gfp_t wait)\n{\n\t \n\tif (folio_test_dirty(folio))\n\t\treturn false;\n\n\tclear_page_private_all(&folio->page);\n\treturn true;\n}\n\nstatic bool f2fs_dirty_data_folio(struct address_space *mapping,\n\t\tstruct folio *folio)\n{\n\tstruct inode *inode = mapping->host;\n\n\ttrace_f2fs_set_page_dirty(&folio->page, DATA);\n\n\tif (!folio_test_uptodate(folio))\n\t\tfolio_mark_uptodate(folio);\n\tBUG_ON(folio_test_swapcache(folio));\n\n\tif (filemap_dirty_folio(mapping, folio)) {\n\t\tf2fs_update_dirty_folio(inode, folio);\n\t\treturn true;\n\t}\n\treturn false;\n}\n\n\nstatic sector_t f2fs_bmap_compress(struct inode *inode, sector_t block)\n{\n#ifdef CONFIG_F2FS_FS_COMPRESSION\n\tstruct dnode_of_data dn;\n\tsector_t start_idx, blknr = 0;\n\tint ret;\n\n\tstart_idx = round_down(block, F2FS_I(inode)->i_cluster_size);\n\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\tret = f2fs_get_dnode_of_data(&dn, start_idx, LOOKUP_NODE);\n\tif (ret)\n\t\treturn 0;\n\n\tif (dn.data_blkaddr != COMPRESS_ADDR) {\n\t\tdn.ofs_in_node += block - start_idx;\n\t\tblknr = f2fs_data_blkaddr(&dn);\n\t\tif (!__is_valid_data_blkaddr(blknr))\n\t\t\tblknr = 0;\n\t}\n\n\tf2fs_put_dnode(&dn);\n\treturn blknr;\n#else\n\treturn 0;\n#endif\n}\n\n\nstatic sector_t f2fs_bmap(struct address_space *mapping, sector_t block)\n{\n\tstruct inode *inode = mapping->host;\n\tsector_t blknr = 0;\n\n\tif (f2fs_has_inline_data(inode))\n\t\tgoto out;\n\n\t \n\tif (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY))\n\t\tfilemap_write_and_wait(mapping);\n\n\t \n\tif (unlikely(block >= max_file_blocks(inode)))\n\t\tgoto out;\n\n\tif (f2fs_compressed_file(inode)) {\n\t\tblknr = f2fs_bmap_compress(inode, block);\n\t} else {\n\t\tstruct f2fs_map_blocks map;\n\n\t\tmemset(&map, 0, sizeof(map));\n\t\tmap.m_lblk = block;\n\t\tmap.m_len = 1;\n\t\tmap.m_next_pgofs = NULL;\n\t\tmap.m_seg_type = NO_CHECK_TYPE;\n\n\t\tif (!f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_BMAP))\n\t\t\tblknr = map.m_pblk;\n\t}\nout:\n\ttrace_f2fs_bmap(inode, block, blknr);\n\treturn blknr;\n}\n\n#ifdef CONFIG_SWAP\nstatic int f2fs_migrate_blocks(struct inode *inode, block_t start_blk,\n\t\t\t\t\t\t\tunsigned int blkcnt)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tunsigned int blkofs;\n\tunsigned int blk_per_sec = BLKS_PER_SEC(sbi);\n\tunsigned int secidx = start_blk / blk_per_sec;\n\tunsigned int end_sec = secidx + blkcnt / blk_per_sec;\n\tint ret = 0;\n\n\tf2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\tfilemap_invalidate_lock(inode->i_mapping);\n\n\tset_inode_flag(inode, FI_ALIGNED_WRITE);\n\tset_inode_flag(inode, FI_OPU_WRITE);\n\n\tfor (; secidx < end_sec; secidx++) {\n\t\tf2fs_down_write(&sbi->pin_sem);\n\n\t\tf2fs_lock_op(sbi);\n\t\tf2fs_allocate_new_section(sbi, CURSEG_COLD_DATA_PINNED, false);\n\t\tf2fs_unlock_op(sbi);\n\n\t\tset_inode_flag(inode, FI_SKIP_WRITES);\n\n\t\tfor (blkofs = 0; blkofs < blk_per_sec; blkofs++) {\n\t\t\tstruct page *page;\n\t\t\tunsigned int blkidx = secidx * blk_per_sec + blkofs;\n\n\t\t\tpage = f2fs_get_lock_data_page(inode, blkidx, true);\n\t\t\tif (IS_ERR(page)) {\n\t\t\t\tf2fs_up_write(&sbi->pin_sem);\n\t\t\t\tret = PTR_ERR(page);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tset_page_dirty(page);\n\t\t\tf2fs_put_page(page, 1);\n\t\t}\n\n\t\tclear_inode_flag(inode, FI_SKIP_WRITES);\n\n\t\tret = filemap_fdatawrite(inode->i_mapping);\n\n\t\tf2fs_up_write(&sbi->pin_sem);\n\n\t\tif (ret)\n\t\t\tbreak;\n\t}\n\ndone:\n\tclear_inode_flag(inode, FI_SKIP_WRITES);\n\tclear_inode_flag(inode, FI_OPU_WRITE);\n\tclear_inode_flag(inode, FI_ALIGNED_WRITE);\n\n\tfilemap_invalidate_unlock(inode->i_mapping);\n\tf2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);\n\n\treturn ret;\n}\n\nstatic int check_swap_activate(struct swap_info_struct *sis,\n\t\t\t\tstruct file *swap_file, sector_t *span)\n{\n\tstruct address_space *mapping = swap_file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tsector_t cur_lblock;\n\tsector_t last_lblock;\n\tsector_t pblock;\n\tsector_t lowest_pblock = -1;\n\tsector_t highest_pblock = 0;\n\tint nr_extents = 0;\n\tunsigned long nr_pblocks;\n\tunsigned int blks_per_sec = BLKS_PER_SEC(sbi);\n\tunsigned int sec_blks_mask = BLKS_PER_SEC(sbi) - 1;\n\tunsigned int not_aligned = 0;\n\tint ret = 0;\n\n\t \n\tcur_lblock = 0;\n\tlast_lblock = bytes_to_blks(inode, i_size_read(inode));\n\n\twhile (cur_lblock < last_lblock && cur_lblock < sis->max) {\n\t\tstruct f2fs_map_blocks map;\nretry:\n\t\tcond_resched();\n\n\t\tmemset(&map, 0, sizeof(map));\n\t\tmap.m_lblk = cur_lblock;\n\t\tmap.m_len = last_lblock - cur_lblock;\n\t\tmap.m_next_pgofs = NULL;\n\t\tmap.m_next_extent = NULL;\n\t\tmap.m_seg_type = NO_CHECK_TYPE;\n\t\tmap.m_may_create = false;\n\n\t\tret = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_FIEMAP);\n\t\tif (ret)\n\t\t\tgoto out;\n\n\t\t \n\t\tif (!(map.m_flags & F2FS_MAP_FLAGS)) {\n\t\t\tf2fs_err(sbi, \"Swapfile has holes\");\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tpblock = map.m_pblk;\n\t\tnr_pblocks = map.m_len;\n\n\t\tif ((pblock - SM_I(sbi)->main_blkaddr) & sec_blks_mask ||\n\t\t\t\tnr_pblocks & sec_blks_mask) {\n\t\t\tnot_aligned++;\n\n\t\t\tnr_pblocks = roundup(nr_pblocks, blks_per_sec);\n\t\t\tif (cur_lblock + nr_pblocks > sis->max)\n\t\t\t\tnr_pblocks -= blks_per_sec;\n\n\t\t\tif (!nr_pblocks) {\n\t\t\t\t \n\t\t\t\tnr_pblocks = map.m_len;\n\t\t\t\tf2fs_warn(sbi, \"Swapfile: last extent is not aligned to section\");\n\t\t\t\tgoto next;\n\t\t\t}\n\n\t\t\tret = f2fs_migrate_blocks(inode, cur_lblock,\n\t\t\t\t\t\t\tnr_pblocks);\n\t\t\tif (ret)\n\t\t\t\tgoto out;\n\t\t\tgoto retry;\n\t\t}\nnext:\n\t\tif (cur_lblock + nr_pblocks >= sis->max)\n\t\t\tnr_pblocks = sis->max - cur_lblock;\n\n\t\tif (cur_lblock) {\t \n\t\t\tif (pblock < lowest_pblock)\n\t\t\t\tlowest_pblock = pblock;\n\t\t\tif (pblock + nr_pblocks - 1 > highest_pblock)\n\t\t\t\thighest_pblock = pblock + nr_pblocks - 1;\n\t\t}\n\n\t\t \n\t\tret = add_swap_extent(sis, cur_lblock, nr_pblocks, pblock);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t\tnr_extents += ret;\n\t\tcur_lblock += nr_pblocks;\n\t}\n\tret = nr_extents;\n\t*span = 1 + highest_pblock - lowest_pblock;\n\tif (cur_lblock == 0)\n\t\tcur_lblock = 1;\t \n\tsis->max = cur_lblock;\n\tsis->pages = cur_lblock - 1;\n\tsis->highest_bit = cur_lblock - 1;\nout:\n\tif (not_aligned)\n\t\tf2fs_warn(sbi, \"Swapfile (%u) is not align to section: 1) creat(), 2) ioctl(F2FS_IOC_SET_PIN_FILE), 3) fallocate(%u * N)\",\n\t\t\t  not_aligned, blks_per_sec * F2FS_BLKSIZE);\n\treturn ret;\n}\n\nstatic int f2fs_swap_activate(struct swap_info_struct *sis, struct file *file,\n\t\t\t\tsector_t *span)\n{\n\tstruct inode *inode = file_inode(file);\n\tint ret;\n\n\tif (!S_ISREG(inode->i_mode))\n\t\treturn -EINVAL;\n\n\tif (f2fs_readonly(F2FS_I_SB(inode)->sb))\n\t\treturn -EROFS;\n\n\tif (f2fs_lfs_mode(F2FS_I_SB(inode))) {\n\t\tf2fs_err(F2FS_I_SB(inode),\n\t\t\t\"Swapfile not supported in LFS mode\");\n\t\treturn -EINVAL;\n\t}\n\n\tret = f2fs_convert_inline_inode(inode);\n\tif (ret)\n\t\treturn ret;\n\n\tif (!f2fs_disable_compressed_file(inode))\n\t\treturn -EINVAL;\n\n\tf2fs_precache_extents(inode);\n\n\tret = check_swap_activate(sis, file, span);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tstat_inc_swapfile_inode(inode);\n\tset_inode_flag(inode, FI_PIN_FILE);\n\tf2fs_update_time(F2FS_I_SB(inode), REQ_TIME);\n\treturn ret;\n}\n\nstatic void f2fs_swap_deactivate(struct file *file)\n{\n\tstruct inode *inode = file_inode(file);\n\n\tstat_dec_swapfile_inode(inode);\n\tclear_inode_flag(inode, FI_PIN_FILE);\n}\n#else\nstatic int f2fs_swap_activate(struct swap_info_struct *sis, struct file *file,\n\t\t\t\tsector_t *span)\n{\n\treturn -EOPNOTSUPP;\n}\n\nstatic void f2fs_swap_deactivate(struct file *file)\n{\n}\n#endif\n\nconst struct address_space_operations f2fs_dblock_aops = {\n\t.read_folio\t= f2fs_read_data_folio,\n\t.readahead\t= f2fs_readahead,\n\t.writepage\t= f2fs_write_data_page,\n\t.writepages\t= f2fs_write_data_pages,\n\t.write_begin\t= f2fs_write_begin,\n\t.write_end\t= f2fs_write_end,\n\t.dirty_folio\t= f2fs_dirty_data_folio,\n\t.migrate_folio\t= filemap_migrate_folio,\n\t.invalidate_folio = f2fs_invalidate_folio,\n\t.release_folio\t= f2fs_release_folio,\n\t.bmap\t\t= f2fs_bmap,\n\t.swap_activate  = f2fs_swap_activate,\n\t.swap_deactivate = f2fs_swap_deactivate,\n};\n\nvoid f2fs_clear_page_cache_dirty_tag(struct page *page)\n{\n\tstruct address_space *mapping = page_mapping(page);\n\tunsigned long flags;\n\n\txa_lock_irqsave(&mapping->i_pages, flags);\n\t__xa_clear_mark(&mapping->i_pages, page_index(page),\n\t\t\t\t\t\tPAGECACHE_TAG_DIRTY);\n\txa_unlock_irqrestore(&mapping->i_pages, flags);\n}\n\nint __init f2fs_init_post_read_processing(void)\n{\n\tbio_post_read_ctx_cache =\n\t\tkmem_cache_create(\"f2fs_bio_post_read_ctx\",\n\t\t\t\t  sizeof(struct bio_post_read_ctx), 0, 0, NULL);\n\tif (!bio_post_read_ctx_cache)\n\t\tgoto fail;\n\tbio_post_read_ctx_pool =\n\t\tmempool_create_slab_pool(NUM_PREALLOC_POST_READ_CTXS,\n\t\t\t\t\t bio_post_read_ctx_cache);\n\tif (!bio_post_read_ctx_pool)\n\t\tgoto fail_free_cache;\n\treturn 0;\n\nfail_free_cache:\n\tkmem_cache_destroy(bio_post_read_ctx_cache);\nfail:\n\treturn -ENOMEM;\n}\n\nvoid f2fs_destroy_post_read_processing(void)\n{\n\tmempool_destroy(bio_post_read_ctx_pool);\n\tkmem_cache_destroy(bio_post_read_ctx_cache);\n}\n\nint f2fs_init_post_read_wq(struct f2fs_sb_info *sbi)\n{\n\tif (!f2fs_sb_has_encrypt(sbi) &&\n\t\t!f2fs_sb_has_verity(sbi) &&\n\t\t!f2fs_sb_has_compression(sbi))\n\t\treturn 0;\n\n\tsbi->post_read_wq = alloc_workqueue(\"f2fs_post_read_wq\",\n\t\t\t\t\t\t WQ_UNBOUND | WQ_HIGHPRI,\n\t\t\t\t\t\t num_online_cpus());\n\treturn sbi->post_read_wq ? 0 : -ENOMEM;\n}\n\nvoid f2fs_destroy_post_read_wq(struct f2fs_sb_info *sbi)\n{\n\tif (sbi->post_read_wq)\n\t\tdestroy_workqueue(sbi->post_read_wq);\n}\n\nint __init f2fs_init_bio_entry_cache(void)\n{\n\tbio_entry_slab = f2fs_kmem_cache_create(\"f2fs_bio_entry_slab\",\n\t\t\tsizeof(struct bio_entry));\n\treturn bio_entry_slab ? 0 : -ENOMEM;\n}\n\nvoid f2fs_destroy_bio_entry_cache(void)\n{\n\tkmem_cache_destroy(bio_entry_slab);\n}\n\nstatic int f2fs_iomap_begin(struct inode *inode, loff_t offset, loff_t length,\n\t\t\t    unsigned int flags, struct iomap *iomap,\n\t\t\t    struct iomap *srcmap)\n{\n\tstruct f2fs_map_blocks map = {};\n\tpgoff_t next_pgofs = 0;\n\tint err;\n\n\tmap.m_lblk = bytes_to_blks(inode, offset);\n\tmap.m_len = bytes_to_blks(inode, offset + length - 1) - map.m_lblk + 1;\n\tmap.m_next_pgofs = &next_pgofs;\n\tmap.m_seg_type = f2fs_rw_hint_to_seg_type(inode->i_write_hint);\n\tif (flags & IOMAP_WRITE)\n\t\tmap.m_may_create = true;\n\n\terr = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_DIO);\n\tif (err)\n\t\treturn err;\n\n\tiomap->offset = blks_to_bytes(inode, map.m_lblk);\n\n\t \n\tmap.m_len = fscrypt_limit_io_blocks(inode, map.m_lblk, map.m_len);\n\n\t \n\tif (WARN_ON_ONCE(map.m_pblk == NEW_ADDR))\n\t\treturn -EINVAL;\n\tif (WARN_ON_ONCE(map.m_pblk == COMPRESS_ADDR))\n\t\treturn -EINVAL;\n\n\tif (map.m_pblk != NULL_ADDR) {\n\t\tiomap->length = blks_to_bytes(inode, map.m_len);\n\t\tiomap->type = IOMAP_MAPPED;\n\t\tiomap->flags |= IOMAP_F_MERGED;\n\t\tiomap->bdev = map.m_bdev;\n\t\tiomap->addr = blks_to_bytes(inode, map.m_pblk);\n\t} else {\n\t\tif (flags & IOMAP_WRITE)\n\t\t\treturn -ENOTBLK;\n\t\tiomap->length = blks_to_bytes(inode, next_pgofs) -\n\t\t\t\tiomap->offset;\n\t\tiomap->type = IOMAP_HOLE;\n\t\tiomap->addr = IOMAP_NULL_ADDR;\n\t}\n\n\tif (map.m_flags & F2FS_MAP_NEW)\n\t\tiomap->flags |= IOMAP_F_NEW;\n\tif ((inode->i_state & I_DIRTY_DATASYNC) ||\n\t    offset + length > i_size_read(inode))\n\t\tiomap->flags |= IOMAP_F_DIRTY;\n\n\treturn 0;\n}\n\nconst struct iomap_ops f2fs_iomap_ops = {\n\t.iomap_begin\t= f2fs_iomap_begin,\n};\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}