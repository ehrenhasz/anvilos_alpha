{
  "module_name": "compress.c",
  "hash_id": "15eaebef4241063d3de578bae0d503df49f4f52fc67695d0ac0875cba284eb4b",
  "original_prompt": "Ingested from linux-6.6.14/fs/f2fs/compress.c",
  "human_readable_source": "\n \n\n#include <linux/fs.h>\n#include <linux/f2fs_fs.h>\n#include <linux/moduleparam.h>\n#include <linux/writeback.h>\n#include <linux/backing-dev.h>\n#include <linux/lzo.h>\n#include <linux/lz4.h>\n#include <linux/zstd.h>\n#include <linux/pagevec.h>\n\n#include \"f2fs.h\"\n#include \"node.h\"\n#include \"segment.h\"\n#include <trace/events/f2fs.h>\n\nstatic struct kmem_cache *cic_entry_slab;\nstatic struct kmem_cache *dic_entry_slab;\n\nstatic void *page_array_alloc(struct inode *inode, int nr)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tunsigned int size = sizeof(struct page *) * nr;\n\n\tif (likely(size <= sbi->page_array_slab_size))\n\t\treturn f2fs_kmem_cache_alloc(sbi->page_array_slab,\n\t\t\t\t\tGFP_F2FS_ZERO, false, F2FS_I_SB(inode));\n\treturn f2fs_kzalloc(sbi, size, GFP_NOFS);\n}\n\nstatic void page_array_free(struct inode *inode, void *pages, int nr)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tunsigned int size = sizeof(struct page *) * nr;\n\n\tif (!pages)\n\t\treturn;\n\n\tif (likely(size <= sbi->page_array_slab_size))\n\t\tkmem_cache_free(sbi->page_array_slab, pages);\n\telse\n\t\tkfree(pages);\n}\n\nstruct f2fs_compress_ops {\n\tint (*init_compress_ctx)(struct compress_ctx *cc);\n\tvoid (*destroy_compress_ctx)(struct compress_ctx *cc);\n\tint (*compress_pages)(struct compress_ctx *cc);\n\tint (*init_decompress_ctx)(struct decompress_io_ctx *dic);\n\tvoid (*destroy_decompress_ctx)(struct decompress_io_ctx *dic);\n\tint (*decompress_pages)(struct decompress_io_ctx *dic);\n\tbool (*is_level_valid)(int level);\n};\n\nstatic unsigned int offset_in_cluster(struct compress_ctx *cc, pgoff_t index)\n{\n\treturn index & (cc->cluster_size - 1);\n}\n\nstatic pgoff_t cluster_idx(struct compress_ctx *cc, pgoff_t index)\n{\n\treturn index >> cc->log_cluster_size;\n}\n\nstatic pgoff_t start_idx_of_cluster(struct compress_ctx *cc)\n{\n\treturn cc->cluster_idx << cc->log_cluster_size;\n}\n\nbool f2fs_is_compressed_page(struct page *page)\n{\n\tif (!PagePrivate(page))\n\t\treturn false;\n\tif (!page_private(page))\n\t\treturn false;\n\tif (page_private_nonpointer(page))\n\t\treturn false;\n\n\tf2fs_bug_on(F2FS_M_SB(page->mapping),\n\t\t*((u32 *)page_private(page)) != F2FS_COMPRESSED_PAGE_MAGIC);\n\treturn true;\n}\n\nstatic void f2fs_set_compressed_page(struct page *page,\n\t\tstruct inode *inode, pgoff_t index, void *data)\n{\n\tattach_page_private(page, (void *)data);\n\n\t \n\tpage->index = index;\n\tpage->mapping = inode->i_mapping;\n}\n\nstatic void f2fs_drop_rpages(struct compress_ctx *cc, int len, bool unlock)\n{\n\tint i;\n\n\tfor (i = 0; i < len; i++) {\n\t\tif (!cc->rpages[i])\n\t\t\tcontinue;\n\t\tif (unlock)\n\t\t\tunlock_page(cc->rpages[i]);\n\t\telse\n\t\t\tput_page(cc->rpages[i]);\n\t}\n}\n\nstatic void f2fs_put_rpages(struct compress_ctx *cc)\n{\n\tf2fs_drop_rpages(cc, cc->cluster_size, false);\n}\n\nstatic void f2fs_unlock_rpages(struct compress_ctx *cc, int len)\n{\n\tf2fs_drop_rpages(cc, len, true);\n}\n\nstatic void f2fs_put_rpages_wbc(struct compress_ctx *cc,\n\t\tstruct writeback_control *wbc, bool redirty, int unlock)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < cc->cluster_size; i++) {\n\t\tif (!cc->rpages[i])\n\t\t\tcontinue;\n\t\tif (redirty)\n\t\t\tredirty_page_for_writepage(wbc, cc->rpages[i]);\n\t\tf2fs_put_page(cc->rpages[i], unlock);\n\t}\n}\n\nstruct page *f2fs_compress_control_page(struct page *page)\n{\n\treturn ((struct compress_io_ctx *)page_private(page))->rpages[0];\n}\n\nint f2fs_init_compress_ctx(struct compress_ctx *cc)\n{\n\tif (cc->rpages)\n\t\treturn 0;\n\n\tcc->rpages = page_array_alloc(cc->inode, cc->cluster_size);\n\treturn cc->rpages ? 0 : -ENOMEM;\n}\n\nvoid f2fs_destroy_compress_ctx(struct compress_ctx *cc, bool reuse)\n{\n\tpage_array_free(cc->inode, cc->rpages, cc->cluster_size);\n\tcc->rpages = NULL;\n\tcc->nr_rpages = 0;\n\tcc->nr_cpages = 0;\n\tcc->valid_nr_cpages = 0;\n\tif (!reuse)\n\t\tcc->cluster_idx = NULL_CLUSTER;\n}\n\nvoid f2fs_compress_ctx_add_page(struct compress_ctx *cc, struct page *page)\n{\n\tunsigned int cluster_ofs;\n\n\tif (!f2fs_cluster_can_merge_page(cc, page->index))\n\t\tf2fs_bug_on(F2FS_I_SB(cc->inode), 1);\n\n\tcluster_ofs = offset_in_cluster(cc, page->index);\n\tcc->rpages[cluster_ofs] = page;\n\tcc->nr_rpages++;\n\tcc->cluster_idx = cluster_idx(cc, page->index);\n}\n\n#ifdef CONFIG_F2FS_FS_LZO\nstatic int lzo_init_compress_ctx(struct compress_ctx *cc)\n{\n\tcc->private = f2fs_kvmalloc(F2FS_I_SB(cc->inode),\n\t\t\t\tLZO1X_MEM_COMPRESS, GFP_NOFS);\n\tif (!cc->private)\n\t\treturn -ENOMEM;\n\n\tcc->clen = lzo1x_worst_compress(PAGE_SIZE << cc->log_cluster_size);\n\treturn 0;\n}\n\nstatic void lzo_destroy_compress_ctx(struct compress_ctx *cc)\n{\n\tkvfree(cc->private);\n\tcc->private = NULL;\n}\n\nstatic int lzo_compress_pages(struct compress_ctx *cc)\n{\n\tint ret;\n\n\tret = lzo1x_1_compress(cc->rbuf, cc->rlen, cc->cbuf->cdata,\n\t\t\t\t\t&cc->clen, cc->private);\n\tif (ret != LZO_E_OK) {\n\t\tprintk_ratelimited(\"%sF2FS-fs (%s): lzo compress failed, ret:%d\\n\",\n\t\t\t\tKERN_ERR, F2FS_I_SB(cc->inode)->sb->s_id, ret);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}\n\nstatic int lzo_decompress_pages(struct decompress_io_ctx *dic)\n{\n\tint ret;\n\n\tret = lzo1x_decompress_safe(dic->cbuf->cdata, dic->clen,\n\t\t\t\t\t\tdic->rbuf, &dic->rlen);\n\tif (ret != LZO_E_OK) {\n\t\tprintk_ratelimited(\"%sF2FS-fs (%s): lzo decompress failed, ret:%d\\n\",\n\t\t\t\tKERN_ERR, F2FS_I_SB(dic->inode)->sb->s_id, ret);\n\t\treturn -EIO;\n\t}\n\n\tif (dic->rlen != PAGE_SIZE << dic->log_cluster_size) {\n\t\tprintk_ratelimited(\"%sF2FS-fs (%s): lzo invalid rlen:%zu, \"\n\t\t\t\t\t\"expected:%lu\\n\", KERN_ERR,\n\t\t\t\t\tF2FS_I_SB(dic->inode)->sb->s_id,\n\t\t\t\t\tdic->rlen,\n\t\t\t\t\tPAGE_SIZE << dic->log_cluster_size);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}\n\nstatic const struct f2fs_compress_ops f2fs_lzo_ops = {\n\t.init_compress_ctx\t= lzo_init_compress_ctx,\n\t.destroy_compress_ctx\t= lzo_destroy_compress_ctx,\n\t.compress_pages\t\t= lzo_compress_pages,\n\t.decompress_pages\t= lzo_decompress_pages,\n};\n#endif\n\n#ifdef CONFIG_F2FS_FS_LZ4\nstatic int lz4_init_compress_ctx(struct compress_ctx *cc)\n{\n\tunsigned int size = LZ4_MEM_COMPRESS;\n\n#ifdef CONFIG_F2FS_FS_LZ4HC\n\tif (F2FS_I(cc->inode)->i_compress_level)\n\t\tsize = LZ4HC_MEM_COMPRESS;\n#endif\n\n\tcc->private = f2fs_kvmalloc(F2FS_I_SB(cc->inode), size, GFP_NOFS);\n\tif (!cc->private)\n\t\treturn -ENOMEM;\n\n\t \n\tcc->clen = cc->rlen - PAGE_SIZE - COMPRESS_HEADER_SIZE;\n\treturn 0;\n}\n\nstatic void lz4_destroy_compress_ctx(struct compress_ctx *cc)\n{\n\tkvfree(cc->private);\n\tcc->private = NULL;\n}\n\nstatic int lz4_compress_pages(struct compress_ctx *cc)\n{\n\tint len = -EINVAL;\n\tunsigned char level = F2FS_I(cc->inode)->i_compress_level;\n\n\tif (!level)\n\t\tlen = LZ4_compress_default(cc->rbuf, cc->cbuf->cdata, cc->rlen,\n\t\t\t\t\t\tcc->clen, cc->private);\n#ifdef CONFIG_F2FS_FS_LZ4HC\n\telse\n\t\tlen = LZ4_compress_HC(cc->rbuf, cc->cbuf->cdata, cc->rlen,\n\t\t\t\t\tcc->clen, level, cc->private);\n#endif\n\tif (len < 0)\n\t\treturn len;\n\tif (!len)\n\t\treturn -EAGAIN;\n\n\tcc->clen = len;\n\treturn 0;\n}\n\nstatic int lz4_decompress_pages(struct decompress_io_ctx *dic)\n{\n\tint ret;\n\n\tret = LZ4_decompress_safe(dic->cbuf->cdata, dic->rbuf,\n\t\t\t\t\t\tdic->clen, dic->rlen);\n\tif (ret < 0) {\n\t\tprintk_ratelimited(\"%sF2FS-fs (%s): lz4 decompress failed, ret:%d\\n\",\n\t\t\t\tKERN_ERR, F2FS_I_SB(dic->inode)->sb->s_id, ret);\n\t\treturn -EIO;\n\t}\n\n\tif (ret != PAGE_SIZE << dic->log_cluster_size) {\n\t\tprintk_ratelimited(\"%sF2FS-fs (%s): lz4 invalid ret:%d, \"\n\t\t\t\t\t\"expected:%lu\\n\", KERN_ERR,\n\t\t\t\t\tF2FS_I_SB(dic->inode)->sb->s_id, ret,\n\t\t\t\t\tPAGE_SIZE << dic->log_cluster_size);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}\n\nstatic bool lz4_is_level_valid(int lvl)\n{\n#ifdef CONFIG_F2FS_FS_LZ4HC\n\treturn !lvl || (lvl >= LZ4HC_MIN_CLEVEL && lvl <= LZ4HC_MAX_CLEVEL);\n#else\n\treturn lvl == 0;\n#endif\n}\n\nstatic const struct f2fs_compress_ops f2fs_lz4_ops = {\n\t.init_compress_ctx\t= lz4_init_compress_ctx,\n\t.destroy_compress_ctx\t= lz4_destroy_compress_ctx,\n\t.compress_pages\t\t= lz4_compress_pages,\n\t.decompress_pages\t= lz4_decompress_pages,\n\t.is_level_valid\t\t= lz4_is_level_valid,\n};\n#endif\n\n#ifdef CONFIG_F2FS_FS_ZSTD\nstatic int zstd_init_compress_ctx(struct compress_ctx *cc)\n{\n\tzstd_parameters params;\n\tzstd_cstream *stream;\n\tvoid *workspace;\n\tunsigned int workspace_size;\n\tunsigned char level = F2FS_I(cc->inode)->i_compress_level;\n\n\t \n\tif (!level)\n\t\tlevel = F2FS_ZSTD_DEFAULT_CLEVEL;\n\n\tparams = zstd_get_params(level, cc->rlen);\n\tworkspace_size = zstd_cstream_workspace_bound(&params.cParams);\n\n\tworkspace = f2fs_kvmalloc(F2FS_I_SB(cc->inode),\n\t\t\t\t\tworkspace_size, GFP_NOFS);\n\tif (!workspace)\n\t\treturn -ENOMEM;\n\n\tstream = zstd_init_cstream(&params, 0, workspace, workspace_size);\n\tif (!stream) {\n\t\tprintk_ratelimited(\"%sF2FS-fs (%s): %s zstd_init_cstream failed\\n\",\n\t\t\t\tKERN_ERR, F2FS_I_SB(cc->inode)->sb->s_id,\n\t\t\t\t__func__);\n\t\tkvfree(workspace);\n\t\treturn -EIO;\n\t}\n\n\tcc->private = workspace;\n\tcc->private2 = stream;\n\n\tcc->clen = cc->rlen - PAGE_SIZE - COMPRESS_HEADER_SIZE;\n\treturn 0;\n}\n\nstatic void zstd_destroy_compress_ctx(struct compress_ctx *cc)\n{\n\tkvfree(cc->private);\n\tcc->private = NULL;\n\tcc->private2 = NULL;\n}\n\nstatic int zstd_compress_pages(struct compress_ctx *cc)\n{\n\tzstd_cstream *stream = cc->private2;\n\tzstd_in_buffer inbuf;\n\tzstd_out_buffer outbuf;\n\tint src_size = cc->rlen;\n\tint dst_size = src_size - PAGE_SIZE - COMPRESS_HEADER_SIZE;\n\tint ret;\n\n\tinbuf.pos = 0;\n\tinbuf.src = cc->rbuf;\n\tinbuf.size = src_size;\n\n\toutbuf.pos = 0;\n\toutbuf.dst = cc->cbuf->cdata;\n\toutbuf.size = dst_size;\n\n\tret = zstd_compress_stream(stream, &outbuf, &inbuf);\n\tif (zstd_is_error(ret)) {\n\t\tprintk_ratelimited(\"%sF2FS-fs (%s): %s zstd_compress_stream failed, ret: %d\\n\",\n\t\t\t\tKERN_ERR, F2FS_I_SB(cc->inode)->sb->s_id,\n\t\t\t\t__func__, zstd_get_error_code(ret));\n\t\treturn -EIO;\n\t}\n\n\tret = zstd_end_stream(stream, &outbuf);\n\tif (zstd_is_error(ret)) {\n\t\tprintk_ratelimited(\"%sF2FS-fs (%s): %s zstd_end_stream returned %d\\n\",\n\t\t\t\tKERN_ERR, F2FS_I_SB(cc->inode)->sb->s_id,\n\t\t\t\t__func__, zstd_get_error_code(ret));\n\t\treturn -EIO;\n\t}\n\n\t \n\tif (ret)\n\t\treturn -EAGAIN;\n\n\tcc->clen = outbuf.pos;\n\treturn 0;\n}\n\nstatic int zstd_init_decompress_ctx(struct decompress_io_ctx *dic)\n{\n\tzstd_dstream *stream;\n\tvoid *workspace;\n\tunsigned int workspace_size;\n\tunsigned int max_window_size =\n\t\t\tMAX_COMPRESS_WINDOW_SIZE(dic->log_cluster_size);\n\n\tworkspace_size = zstd_dstream_workspace_bound(max_window_size);\n\n\tworkspace = f2fs_kvmalloc(F2FS_I_SB(dic->inode),\n\t\t\t\t\tworkspace_size, GFP_NOFS);\n\tif (!workspace)\n\t\treturn -ENOMEM;\n\n\tstream = zstd_init_dstream(max_window_size, workspace, workspace_size);\n\tif (!stream) {\n\t\tprintk_ratelimited(\"%sF2FS-fs (%s): %s zstd_init_dstream failed\\n\",\n\t\t\t\tKERN_ERR, F2FS_I_SB(dic->inode)->sb->s_id,\n\t\t\t\t__func__);\n\t\tkvfree(workspace);\n\t\treturn -EIO;\n\t}\n\n\tdic->private = workspace;\n\tdic->private2 = stream;\n\n\treturn 0;\n}\n\nstatic void zstd_destroy_decompress_ctx(struct decompress_io_ctx *dic)\n{\n\tkvfree(dic->private);\n\tdic->private = NULL;\n\tdic->private2 = NULL;\n}\n\nstatic int zstd_decompress_pages(struct decompress_io_ctx *dic)\n{\n\tzstd_dstream *stream = dic->private2;\n\tzstd_in_buffer inbuf;\n\tzstd_out_buffer outbuf;\n\tint ret;\n\n\tinbuf.pos = 0;\n\tinbuf.src = dic->cbuf->cdata;\n\tinbuf.size = dic->clen;\n\n\toutbuf.pos = 0;\n\toutbuf.dst = dic->rbuf;\n\toutbuf.size = dic->rlen;\n\n\tret = zstd_decompress_stream(stream, &outbuf, &inbuf);\n\tif (zstd_is_error(ret)) {\n\t\tprintk_ratelimited(\"%sF2FS-fs (%s): %s zstd_decompress_stream failed, ret: %d\\n\",\n\t\t\t\tKERN_ERR, F2FS_I_SB(dic->inode)->sb->s_id,\n\t\t\t\t__func__, zstd_get_error_code(ret));\n\t\treturn -EIO;\n\t}\n\n\tif (dic->rlen != outbuf.pos) {\n\t\tprintk_ratelimited(\"%sF2FS-fs (%s): %s ZSTD invalid rlen:%zu, \"\n\t\t\t\t\"expected:%lu\\n\", KERN_ERR,\n\t\t\t\tF2FS_I_SB(dic->inode)->sb->s_id,\n\t\t\t\t__func__, dic->rlen,\n\t\t\t\tPAGE_SIZE << dic->log_cluster_size);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic bool zstd_is_level_valid(int lvl)\n{\n\treturn lvl >= zstd_min_clevel() && lvl <= zstd_max_clevel();\n}\n\nstatic const struct f2fs_compress_ops f2fs_zstd_ops = {\n\t.init_compress_ctx\t= zstd_init_compress_ctx,\n\t.destroy_compress_ctx\t= zstd_destroy_compress_ctx,\n\t.compress_pages\t\t= zstd_compress_pages,\n\t.init_decompress_ctx\t= zstd_init_decompress_ctx,\n\t.destroy_decompress_ctx\t= zstd_destroy_decompress_ctx,\n\t.decompress_pages\t= zstd_decompress_pages,\n\t.is_level_valid\t\t= zstd_is_level_valid,\n};\n#endif\n\n#ifdef CONFIG_F2FS_FS_LZO\n#ifdef CONFIG_F2FS_FS_LZORLE\nstatic int lzorle_compress_pages(struct compress_ctx *cc)\n{\n\tint ret;\n\n\tret = lzorle1x_1_compress(cc->rbuf, cc->rlen, cc->cbuf->cdata,\n\t\t\t\t\t&cc->clen, cc->private);\n\tif (ret != LZO_E_OK) {\n\t\tprintk_ratelimited(\"%sF2FS-fs (%s): lzo-rle compress failed, ret:%d\\n\",\n\t\t\t\tKERN_ERR, F2FS_I_SB(cc->inode)->sb->s_id, ret);\n\t\treturn -EIO;\n\t}\n\treturn 0;\n}\n\nstatic const struct f2fs_compress_ops f2fs_lzorle_ops = {\n\t.init_compress_ctx\t= lzo_init_compress_ctx,\n\t.destroy_compress_ctx\t= lzo_destroy_compress_ctx,\n\t.compress_pages\t\t= lzorle_compress_pages,\n\t.decompress_pages\t= lzo_decompress_pages,\n};\n#endif\n#endif\n\nstatic const struct f2fs_compress_ops *f2fs_cops[COMPRESS_MAX] = {\n#ifdef CONFIG_F2FS_FS_LZO\n\t&f2fs_lzo_ops,\n#else\n\tNULL,\n#endif\n#ifdef CONFIG_F2FS_FS_LZ4\n\t&f2fs_lz4_ops,\n#else\n\tNULL,\n#endif\n#ifdef CONFIG_F2FS_FS_ZSTD\n\t&f2fs_zstd_ops,\n#else\n\tNULL,\n#endif\n#if defined(CONFIG_F2FS_FS_LZO) && defined(CONFIG_F2FS_FS_LZORLE)\n\t&f2fs_lzorle_ops,\n#else\n\tNULL,\n#endif\n};\n\nbool f2fs_is_compress_backend_ready(struct inode *inode)\n{\n\tif (!f2fs_compressed_file(inode))\n\t\treturn true;\n\treturn f2fs_cops[F2FS_I(inode)->i_compress_algorithm];\n}\n\nbool f2fs_is_compress_level_valid(int alg, int lvl)\n{\n\tconst struct f2fs_compress_ops *cops = f2fs_cops[alg];\n\n\tif (cops->is_level_valid)\n\t\treturn cops->is_level_valid(lvl);\n\n\treturn lvl == 0;\n}\n\nstatic mempool_t *compress_page_pool;\nstatic int num_compress_pages = 512;\nmodule_param(num_compress_pages, uint, 0444);\nMODULE_PARM_DESC(num_compress_pages,\n\t\t\"Number of intermediate compress pages to preallocate\");\n\nint __init f2fs_init_compress_mempool(void)\n{\n\tcompress_page_pool = mempool_create_page_pool(num_compress_pages, 0);\n\treturn compress_page_pool ? 0 : -ENOMEM;\n}\n\nvoid f2fs_destroy_compress_mempool(void)\n{\n\tmempool_destroy(compress_page_pool);\n}\n\nstatic struct page *f2fs_compress_alloc_page(void)\n{\n\tstruct page *page;\n\n\tpage = mempool_alloc(compress_page_pool, GFP_NOFS);\n\tlock_page(page);\n\n\treturn page;\n}\n\nstatic void f2fs_compress_free_page(struct page *page)\n{\n\tif (!page)\n\t\treturn;\n\tdetach_page_private(page);\n\tpage->mapping = NULL;\n\tunlock_page(page);\n\tmempool_free(page, compress_page_pool);\n}\n\n#define MAX_VMAP_RETRIES\t3\n\nstatic void *f2fs_vmap(struct page **pages, unsigned int count)\n{\n\tint i;\n\tvoid *buf = NULL;\n\n\tfor (i = 0; i < MAX_VMAP_RETRIES; i++) {\n\t\tbuf = vm_map_ram(pages, count, -1);\n\t\tif (buf)\n\t\t\tbreak;\n\t\tvm_unmap_aliases();\n\t}\n\treturn buf;\n}\n\nstatic int f2fs_compress_pages(struct compress_ctx *cc)\n{\n\tstruct f2fs_inode_info *fi = F2FS_I(cc->inode);\n\tconst struct f2fs_compress_ops *cops =\n\t\t\t\tf2fs_cops[fi->i_compress_algorithm];\n\tunsigned int max_len, new_nr_cpages;\n\tu32 chksum = 0;\n\tint i, ret;\n\n\ttrace_f2fs_compress_pages_start(cc->inode, cc->cluster_idx,\n\t\t\t\tcc->cluster_size, fi->i_compress_algorithm);\n\n\tif (cops->init_compress_ctx) {\n\t\tret = cops->init_compress_ctx(cc);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tmax_len = COMPRESS_HEADER_SIZE + cc->clen;\n\tcc->nr_cpages = DIV_ROUND_UP(max_len, PAGE_SIZE);\n\tcc->valid_nr_cpages = cc->nr_cpages;\n\n\tcc->cpages = page_array_alloc(cc->inode, cc->nr_cpages);\n\tif (!cc->cpages) {\n\t\tret = -ENOMEM;\n\t\tgoto destroy_compress_ctx;\n\t}\n\n\tfor (i = 0; i < cc->nr_cpages; i++)\n\t\tcc->cpages[i] = f2fs_compress_alloc_page();\n\n\tcc->rbuf = f2fs_vmap(cc->rpages, cc->cluster_size);\n\tif (!cc->rbuf) {\n\t\tret = -ENOMEM;\n\t\tgoto out_free_cpages;\n\t}\n\n\tcc->cbuf = f2fs_vmap(cc->cpages, cc->nr_cpages);\n\tif (!cc->cbuf) {\n\t\tret = -ENOMEM;\n\t\tgoto out_vunmap_rbuf;\n\t}\n\n\tret = cops->compress_pages(cc);\n\tif (ret)\n\t\tgoto out_vunmap_cbuf;\n\n\tmax_len = PAGE_SIZE * (cc->cluster_size - 1) - COMPRESS_HEADER_SIZE;\n\n\tif (cc->clen > max_len) {\n\t\tret = -EAGAIN;\n\t\tgoto out_vunmap_cbuf;\n\t}\n\n\tcc->cbuf->clen = cpu_to_le32(cc->clen);\n\n\tif (fi->i_compress_flag & BIT(COMPRESS_CHKSUM))\n\t\tchksum = f2fs_crc32(F2FS_I_SB(cc->inode),\n\t\t\t\t\tcc->cbuf->cdata, cc->clen);\n\tcc->cbuf->chksum = cpu_to_le32(chksum);\n\n\tfor (i = 0; i < COMPRESS_DATA_RESERVED_SIZE; i++)\n\t\tcc->cbuf->reserved[i] = cpu_to_le32(0);\n\n\tnew_nr_cpages = DIV_ROUND_UP(cc->clen + COMPRESS_HEADER_SIZE, PAGE_SIZE);\n\n\t \n\tmemset(&cc->cbuf->cdata[cc->clen], 0,\n\t\t\t(new_nr_cpages * PAGE_SIZE) -\n\t\t\t(cc->clen + COMPRESS_HEADER_SIZE));\n\n\tvm_unmap_ram(cc->cbuf, cc->nr_cpages);\n\tvm_unmap_ram(cc->rbuf, cc->cluster_size);\n\n\tfor (i = new_nr_cpages; i < cc->nr_cpages; i++) {\n\t\tf2fs_compress_free_page(cc->cpages[i]);\n\t\tcc->cpages[i] = NULL;\n\t}\n\n\tif (cops->destroy_compress_ctx)\n\t\tcops->destroy_compress_ctx(cc);\n\n\tcc->valid_nr_cpages = new_nr_cpages;\n\n\ttrace_f2fs_compress_pages_end(cc->inode, cc->cluster_idx,\n\t\t\t\t\t\t\tcc->clen, ret);\n\treturn 0;\n\nout_vunmap_cbuf:\n\tvm_unmap_ram(cc->cbuf, cc->nr_cpages);\nout_vunmap_rbuf:\n\tvm_unmap_ram(cc->rbuf, cc->cluster_size);\nout_free_cpages:\n\tfor (i = 0; i < cc->nr_cpages; i++) {\n\t\tif (cc->cpages[i])\n\t\t\tf2fs_compress_free_page(cc->cpages[i]);\n\t}\n\tpage_array_free(cc->inode, cc->cpages, cc->nr_cpages);\n\tcc->cpages = NULL;\ndestroy_compress_ctx:\n\tif (cops->destroy_compress_ctx)\n\t\tcops->destroy_compress_ctx(cc);\nout:\n\ttrace_f2fs_compress_pages_end(cc->inode, cc->cluster_idx,\n\t\t\t\t\t\t\tcc->clen, ret);\n\treturn ret;\n}\n\nstatic int f2fs_prepare_decomp_mem(struct decompress_io_ctx *dic,\n\t\tbool pre_alloc);\nstatic void f2fs_release_decomp_mem(struct decompress_io_ctx *dic,\n\t\tbool bypass_destroy_callback, bool pre_alloc);\n\nvoid f2fs_decompress_cluster(struct decompress_io_ctx *dic, bool in_task)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(dic->inode);\n\tstruct f2fs_inode_info *fi = F2FS_I(dic->inode);\n\tconst struct f2fs_compress_ops *cops =\n\t\t\tf2fs_cops[fi->i_compress_algorithm];\n\tbool bypass_callback = false;\n\tint ret;\n\n\ttrace_f2fs_decompress_pages_start(dic->inode, dic->cluster_idx,\n\t\t\t\tdic->cluster_size, fi->i_compress_algorithm);\n\n\tif (dic->failed) {\n\t\tret = -EIO;\n\t\tgoto out_end_io;\n\t}\n\n\tret = f2fs_prepare_decomp_mem(dic, false);\n\tif (ret) {\n\t\tbypass_callback = true;\n\t\tgoto out_release;\n\t}\n\n\tdic->clen = le32_to_cpu(dic->cbuf->clen);\n\tdic->rlen = PAGE_SIZE << dic->log_cluster_size;\n\n\tif (dic->clen > PAGE_SIZE * dic->nr_cpages - COMPRESS_HEADER_SIZE) {\n\t\tret = -EFSCORRUPTED;\n\n\t\t \n\t\tif (!in_task)\n\t\t\tf2fs_handle_error_async(sbi, ERROR_FAIL_DECOMPRESSION);\n\t\telse\n\t\t\tf2fs_handle_error(sbi, ERROR_FAIL_DECOMPRESSION);\n\t\tgoto out_release;\n\t}\n\n\tret = cops->decompress_pages(dic);\n\n\tif (!ret && (fi->i_compress_flag & BIT(COMPRESS_CHKSUM))) {\n\t\tu32 provided = le32_to_cpu(dic->cbuf->chksum);\n\t\tu32 calculated = f2fs_crc32(sbi, dic->cbuf->cdata, dic->clen);\n\n\t\tif (provided != calculated) {\n\t\t\tif (!is_inode_flag_set(dic->inode, FI_COMPRESS_CORRUPT)) {\n\t\t\t\tset_inode_flag(dic->inode, FI_COMPRESS_CORRUPT);\n\t\t\t\tprintk_ratelimited(\n\t\t\t\t\t\"%sF2FS-fs (%s): checksum invalid, nid = %lu, %x vs %x\",\n\t\t\t\t\tKERN_INFO, sbi->sb->s_id, dic->inode->i_ino,\n\t\t\t\t\tprovided, calculated);\n\t\t\t}\n\t\t\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\t\t}\n\t}\n\nout_release:\n\tf2fs_release_decomp_mem(dic, bypass_callback, false);\n\nout_end_io:\n\ttrace_f2fs_decompress_pages_end(dic->inode, dic->cluster_idx,\n\t\t\t\t\t\t\tdic->clen, ret);\n\tf2fs_decompress_end_io(dic, ret, in_task);\n}\n\n \nvoid f2fs_end_read_compressed_page(struct page *page, bool failed,\n\t\tblock_t blkaddr, bool in_task)\n{\n\tstruct decompress_io_ctx *dic =\n\t\t\t(struct decompress_io_ctx *)page_private(page);\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(dic->inode);\n\n\tdec_page_count(sbi, F2FS_RD_DATA);\n\n\tif (failed)\n\t\tWRITE_ONCE(dic->failed, true);\n\telse if (blkaddr && in_task)\n\t\tf2fs_cache_compressed_page(sbi, page,\n\t\t\t\t\tdic->inode->i_ino, blkaddr);\n\n\tif (atomic_dec_and_test(&dic->remaining_pages))\n\t\tf2fs_decompress_cluster(dic, in_task);\n}\n\nstatic bool is_page_in_cluster(struct compress_ctx *cc, pgoff_t index)\n{\n\tif (cc->cluster_idx == NULL_CLUSTER)\n\t\treturn true;\n\treturn cc->cluster_idx == cluster_idx(cc, index);\n}\n\nbool f2fs_cluster_is_empty(struct compress_ctx *cc)\n{\n\treturn cc->nr_rpages == 0;\n}\n\nstatic bool f2fs_cluster_is_full(struct compress_ctx *cc)\n{\n\treturn cc->cluster_size == cc->nr_rpages;\n}\n\nbool f2fs_cluster_can_merge_page(struct compress_ctx *cc, pgoff_t index)\n{\n\tif (f2fs_cluster_is_empty(cc))\n\t\treturn true;\n\treturn is_page_in_cluster(cc, index);\n}\n\nbool f2fs_all_cluster_page_ready(struct compress_ctx *cc, struct page **pages,\n\t\t\t\tint index, int nr_pages, bool uptodate)\n{\n\tunsigned long pgidx = pages[index]->index;\n\tint i = uptodate ? 0 : 1;\n\n\t \n\tif (uptodate && (pgidx % cc->cluster_size))\n\t\treturn false;\n\n\tif (nr_pages - index < cc->cluster_size)\n\t\treturn false;\n\n\tfor (; i < cc->cluster_size; i++) {\n\t\tif (pages[index + i]->index != pgidx + i)\n\t\t\treturn false;\n\t\tif (uptodate && !PageUptodate(pages[index + i]))\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}\n\nstatic bool cluster_has_invalid_data(struct compress_ctx *cc)\n{\n\tloff_t i_size = i_size_read(cc->inode);\n\tunsigned nr_pages = DIV_ROUND_UP(i_size, PAGE_SIZE);\n\tint i;\n\n\tfor (i = 0; i < cc->cluster_size; i++) {\n\t\tstruct page *page = cc->rpages[i];\n\n\t\tf2fs_bug_on(F2FS_I_SB(cc->inode), !page);\n\n\t\t \n\t\tif (page->index >= nr_pages)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nbool f2fs_sanity_check_cluster(struct dnode_of_data *dn)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);\n\tunsigned int cluster_size = F2FS_I(dn->inode)->i_cluster_size;\n\tbool compressed = dn->data_blkaddr == COMPRESS_ADDR;\n\tint cluster_end = 0;\n\tint i;\n\tchar *reason = \"\";\n\n\tif (!compressed)\n\t\treturn false;\n\n\t \n\tif (dn->ofs_in_node % cluster_size) {\n\t\treason = \"[*|C|*|*]\";\n\t\tgoto out;\n\t}\n\n\tfor (i = 1; i < cluster_size; i++) {\n\t\tblock_t blkaddr = data_blkaddr(dn->inode, dn->node_page,\n\t\t\t\t\t\t\tdn->ofs_in_node + i);\n\n\t\t \n\t\tif (blkaddr == COMPRESS_ADDR) {\n\t\t\treason = \"[C|*|C|*]\";\n\t\t\tgoto out;\n\t\t}\n\t\tif (!__is_valid_data_blkaddr(blkaddr)) {\n\t\t\tif (!cluster_end)\n\t\t\t\tcluster_end = i;\n\t\t\tcontinue;\n\t\t}\n\t\t \n\t\tif (cluster_end) {\n\t\t\treason = \"[C|N|N|V]\";\n\t\t\tgoto out;\n\t\t}\n\t}\n\treturn false;\nout:\n\tf2fs_warn(sbi, \"access invalid cluster, ino:%lu, nid:%u, ofs_in_node:%u, reason:%s\",\n\t\t\tdn->inode->i_ino, dn->nid, dn->ofs_in_node, reason);\n\tset_sbi_flag(sbi, SBI_NEED_FSCK);\n\treturn true;\n}\n\nstatic int __f2fs_cluster_blocks(struct inode *inode,\n\t\t\t\tunsigned int cluster_idx, bool compr)\n{\n\tstruct dnode_of_data dn;\n\tunsigned int cluster_size = F2FS_I(inode)->i_cluster_size;\n\tunsigned int start_idx = cluster_idx <<\n\t\t\t\tF2FS_I(inode)->i_log_cluster_size;\n\tint ret;\n\n\tset_new_dnode(&dn, inode, NULL, NULL, 0);\n\tret = f2fs_get_dnode_of_data(&dn, start_idx, LOOKUP_NODE);\n\tif (ret) {\n\t\tif (ret == -ENOENT)\n\t\t\tret = 0;\n\t\tgoto fail;\n\t}\n\n\tif (f2fs_sanity_check_cluster(&dn)) {\n\t\tret = -EFSCORRUPTED;\n\t\tf2fs_handle_error(F2FS_I_SB(inode), ERROR_CORRUPTED_CLUSTER);\n\t\tgoto fail;\n\t}\n\n\tif (dn.data_blkaddr == COMPRESS_ADDR) {\n\t\tint i;\n\n\t\tret = 1;\n\t\tfor (i = 1; i < cluster_size; i++) {\n\t\t\tblock_t blkaddr;\n\n\t\t\tblkaddr = data_blkaddr(dn.inode,\n\t\t\t\t\tdn.node_page, dn.ofs_in_node + i);\n\t\t\tif (compr) {\n\t\t\t\tif (__is_valid_data_blkaddr(blkaddr))\n\t\t\t\t\tret++;\n\t\t\t} else {\n\t\t\t\tif (blkaddr != NULL_ADDR)\n\t\t\t\t\tret++;\n\t\t\t}\n\t\t}\n\n\t\tf2fs_bug_on(F2FS_I_SB(inode),\n\t\t\t!compr && ret != cluster_size &&\n\t\t\t!is_inode_flag_set(inode, FI_COMPRESS_RELEASED));\n\t}\nfail:\n\tf2fs_put_dnode(&dn);\n\treturn ret;\n}\n\n \nstatic int f2fs_compressed_blocks(struct compress_ctx *cc)\n{\n\treturn __f2fs_cluster_blocks(cc->inode, cc->cluster_idx, true);\n}\n\n \nint f2fs_is_compressed_cluster(struct inode *inode, pgoff_t index)\n{\n\treturn __f2fs_cluster_blocks(inode,\n\t\tindex >> F2FS_I(inode)->i_log_cluster_size,\n\t\tfalse);\n}\n\nstatic bool cluster_may_compress(struct compress_ctx *cc)\n{\n\tif (!f2fs_need_compress_data(cc->inode))\n\t\treturn false;\n\tif (f2fs_is_atomic_file(cc->inode))\n\t\treturn false;\n\tif (!f2fs_cluster_is_full(cc))\n\t\treturn false;\n\tif (unlikely(f2fs_cp_error(F2FS_I_SB(cc->inode))))\n\t\treturn false;\n\treturn !cluster_has_invalid_data(cc);\n}\n\nstatic void set_cluster_writeback(struct compress_ctx *cc)\n{\n\tint i;\n\n\tfor (i = 0; i < cc->cluster_size; i++) {\n\t\tif (cc->rpages[i])\n\t\t\tset_page_writeback(cc->rpages[i]);\n\t}\n}\n\nstatic void set_cluster_dirty(struct compress_ctx *cc)\n{\n\tint i;\n\n\tfor (i = 0; i < cc->cluster_size; i++)\n\t\tif (cc->rpages[i])\n\t\t\tset_page_dirty(cc->rpages[i]);\n}\n\nstatic int prepare_compress_overwrite(struct compress_ctx *cc,\n\t\tstruct page **pagep, pgoff_t index, void **fsdata)\n{\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(cc->inode);\n\tstruct address_space *mapping = cc->inode->i_mapping;\n\tstruct page *page;\n\tsector_t last_block_in_bio;\n\tfgf_t fgp_flag = FGP_LOCK | FGP_WRITE | FGP_CREAT;\n\tpgoff_t start_idx = start_idx_of_cluster(cc);\n\tint i, ret;\n\nretry:\n\tret = f2fs_is_compressed_cluster(cc->inode, start_idx);\n\tif (ret <= 0)\n\t\treturn ret;\n\n\tret = f2fs_init_compress_ctx(cc);\n\tif (ret)\n\t\treturn ret;\n\n\t \n\tfor (i = 0; i < cc->cluster_size; i++) {\n\t\tpage = f2fs_pagecache_get_page(mapping, start_idx + i,\n\t\t\t\t\t\t\tfgp_flag, GFP_NOFS);\n\t\tif (!page) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto unlock_pages;\n\t\t}\n\n\t\tif (PageUptodate(page))\n\t\t\tf2fs_put_page(page, 1);\n\t\telse\n\t\t\tf2fs_compress_ctx_add_page(cc, page);\n\t}\n\n\tif (!f2fs_cluster_is_empty(cc)) {\n\t\tstruct bio *bio = NULL;\n\n\t\tret = f2fs_read_multi_pages(cc, &bio, cc->cluster_size,\n\t\t\t\t\t&last_block_in_bio, false, true);\n\t\tf2fs_put_rpages(cc);\n\t\tf2fs_destroy_compress_ctx(cc, true);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tif (bio)\n\t\t\tf2fs_submit_read_bio(sbi, bio, DATA);\n\n\t\tret = f2fs_init_compress_ctx(cc);\n\t\tif (ret)\n\t\t\tgoto out;\n\t}\n\n\tfor (i = 0; i < cc->cluster_size; i++) {\n\t\tf2fs_bug_on(sbi, cc->rpages[i]);\n\n\t\tpage = find_lock_page(mapping, start_idx + i);\n\t\tif (!page) {\n\t\t\t \n\t\t\tgoto release_and_retry;\n\t\t}\n\n\t\tf2fs_wait_on_page_writeback(page, DATA, true, true);\n\t\tf2fs_compress_ctx_add_page(cc, page);\n\n\t\tif (!PageUptodate(page)) {\nrelease_and_retry:\n\t\t\tf2fs_put_rpages(cc);\n\t\t\tf2fs_unlock_rpages(cc, i + 1);\n\t\t\tf2fs_destroy_compress_ctx(cc, true);\n\t\t\tgoto retry;\n\t\t}\n\t}\n\n\tif (likely(!ret)) {\n\t\t*fsdata = cc->rpages;\n\t\t*pagep = cc->rpages[offset_in_cluster(cc, index)];\n\t\treturn cc->cluster_size;\n\t}\n\nunlock_pages:\n\tf2fs_put_rpages(cc);\n\tf2fs_unlock_rpages(cc, i);\n\tf2fs_destroy_compress_ctx(cc, true);\nout:\n\treturn ret;\n}\n\nint f2fs_prepare_compress_overwrite(struct inode *inode,\n\t\tstruct page **pagep, pgoff_t index, void **fsdata)\n{\n\tstruct compress_ctx cc = {\n\t\t.inode = inode,\n\t\t.log_cluster_size = F2FS_I(inode)->i_log_cluster_size,\n\t\t.cluster_size = F2FS_I(inode)->i_cluster_size,\n\t\t.cluster_idx = index >> F2FS_I(inode)->i_log_cluster_size,\n\t\t.rpages = NULL,\n\t\t.nr_rpages = 0,\n\t};\n\n\treturn prepare_compress_overwrite(&cc, pagep, index, fsdata);\n}\n\nbool f2fs_compress_write_end(struct inode *inode, void *fsdata,\n\t\t\t\t\tpgoff_t index, unsigned copied)\n\n{\n\tstruct compress_ctx cc = {\n\t\t.inode = inode,\n\t\t.log_cluster_size = F2FS_I(inode)->i_log_cluster_size,\n\t\t.cluster_size = F2FS_I(inode)->i_cluster_size,\n\t\t.rpages = fsdata,\n\t};\n\tbool first_index = (index == cc.rpages[0]->index);\n\n\tif (copied)\n\t\tset_cluster_dirty(&cc);\n\n\tf2fs_put_rpages_wbc(&cc, NULL, false, 1);\n\tf2fs_destroy_compress_ctx(&cc, false);\n\n\treturn first_index;\n}\n\nint f2fs_truncate_partial_cluster(struct inode *inode, u64 from, bool lock)\n{\n\tvoid *fsdata = NULL;\n\tstruct page *pagep;\n\tint log_cluster_size = F2FS_I(inode)->i_log_cluster_size;\n\tpgoff_t start_idx = from >> (PAGE_SHIFT + log_cluster_size) <<\n\t\t\t\t\t\t\tlog_cluster_size;\n\tint err;\n\n\terr = f2fs_is_compressed_cluster(inode, start_idx);\n\tif (err < 0)\n\t\treturn err;\n\n\t \n\tif (!err)\n\t\treturn f2fs_do_truncate_blocks(inode, from, lock);\n\n\t \n\terr = f2fs_prepare_compress_overwrite(inode, &pagep,\n\t\t\t\t\t\tstart_idx, &fsdata);\n\n\t \n\tf2fs_bug_on(F2FS_I_SB(inode), err == 0);\n\n\tif (err <= 0)\n\t\treturn err;\n\n\tif (err > 0) {\n\t\tstruct page **rpages = fsdata;\n\t\tint cluster_size = F2FS_I(inode)->i_cluster_size;\n\t\tint i;\n\n\t\tfor (i = cluster_size - 1; i >= 0; i--) {\n\t\t\tloff_t start = rpages[i]->index << PAGE_SHIFT;\n\n\t\t\tif (from <= start) {\n\t\t\t\tzero_user_segment(rpages[i], 0, PAGE_SIZE);\n\t\t\t} else {\n\t\t\t\tzero_user_segment(rpages[i], from - start,\n\t\t\t\t\t\t\t\tPAGE_SIZE);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tf2fs_compress_write_end(inode, fsdata, start_idx, true);\n\t}\n\treturn 0;\n}\n\nstatic int f2fs_write_compressed_pages(struct compress_ctx *cc,\n\t\t\t\t\tint *submitted,\n\t\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\t\tenum iostat_type io_type)\n{\n\tstruct inode *inode = cc->inode;\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(inode);\n\tstruct f2fs_inode_info *fi = F2FS_I(inode);\n\tstruct f2fs_io_info fio = {\n\t\t.sbi = sbi,\n\t\t.ino = cc->inode->i_ino,\n\t\t.type = DATA,\n\t\t.op = REQ_OP_WRITE,\n\t\t.op_flags = wbc_to_write_flags(wbc),\n\t\t.old_blkaddr = NEW_ADDR,\n\t\t.page = NULL,\n\t\t.encrypted_page = NULL,\n\t\t.compressed_page = NULL,\n\t\t.submitted = 0,\n\t\t.io_type = io_type,\n\t\t.io_wbc = wbc,\n\t\t.encrypted = fscrypt_inode_uses_fs_layer_crypto(cc->inode) ?\n\t\t\t\t\t\t\t\t\t1 : 0,\n\t};\n\tstruct dnode_of_data dn;\n\tstruct node_info ni;\n\tstruct compress_io_ctx *cic;\n\tpgoff_t start_idx = start_idx_of_cluster(cc);\n\tunsigned int last_index = cc->cluster_size - 1;\n\tloff_t psize;\n\tint i, err;\n\tbool quota_inode = IS_NOQUOTA(inode);\n\n\t \n\tif (unlikely(f2fs_cp_error(sbi))) {\n\t\tmapping_set_error(cc->rpages[0]->mapping, -EIO);\n\t\tgoto out_free;\n\t}\n\n\tif (quota_inode) {\n\t\t \n\t\tf2fs_down_read(&sbi->node_write);\n\t} else if (!f2fs_trylock_op(sbi)) {\n\t\tgoto out_free;\n\t}\n\n\tset_new_dnode(&dn, cc->inode, NULL, NULL, 0);\n\n\terr = f2fs_get_dnode_of_data(&dn, start_idx, LOOKUP_NODE);\n\tif (err)\n\t\tgoto out_unlock_op;\n\n\tfor (i = 0; i < cc->cluster_size; i++) {\n\t\tif (data_blkaddr(dn.inode, dn.node_page,\n\t\t\t\t\tdn.ofs_in_node + i) == NULL_ADDR)\n\t\t\tgoto out_put_dnode;\n\t}\n\n\tpsize = (loff_t)(cc->rpages[last_index]->index + 1) << PAGE_SHIFT;\n\n\terr = f2fs_get_node_info(fio.sbi, dn.nid, &ni, false);\n\tif (err)\n\t\tgoto out_put_dnode;\n\n\tfio.version = ni.version;\n\n\tcic = f2fs_kmem_cache_alloc(cic_entry_slab, GFP_F2FS_ZERO, false, sbi);\n\tif (!cic)\n\t\tgoto out_put_dnode;\n\n\tcic->magic = F2FS_COMPRESSED_PAGE_MAGIC;\n\tcic->inode = inode;\n\tatomic_set(&cic->pending_pages, cc->valid_nr_cpages);\n\tcic->rpages = page_array_alloc(cc->inode, cc->cluster_size);\n\tif (!cic->rpages)\n\t\tgoto out_put_cic;\n\n\tcic->nr_rpages = cc->cluster_size;\n\n\tfor (i = 0; i < cc->valid_nr_cpages; i++) {\n\t\tf2fs_set_compressed_page(cc->cpages[i], inode,\n\t\t\t\t\tcc->rpages[i + 1]->index, cic);\n\t\tfio.compressed_page = cc->cpages[i];\n\n\t\tfio.old_blkaddr = data_blkaddr(dn.inode, dn.node_page,\n\t\t\t\t\t\tdn.ofs_in_node + i + 1);\n\n\t\t \n\t\tf2fs_wait_on_block_writeback(inode, fio.old_blkaddr);\n\n\t\tif (fio.encrypted) {\n\t\t\tfio.page = cc->rpages[i + 1];\n\t\t\terr = f2fs_encrypt_one_page(&fio);\n\t\t\tif (err)\n\t\t\t\tgoto out_destroy_crypt;\n\t\t\tcc->cpages[i] = fio.encrypted_page;\n\t\t}\n\t}\n\n\tset_cluster_writeback(cc);\n\n\tfor (i = 0; i < cc->cluster_size; i++)\n\t\tcic->rpages[i] = cc->rpages[i];\n\n\tfor (i = 0; i < cc->cluster_size; i++, dn.ofs_in_node++) {\n\t\tblock_t blkaddr;\n\n\t\tblkaddr = f2fs_data_blkaddr(&dn);\n\t\tfio.page = cc->rpages[i];\n\t\tfio.old_blkaddr = blkaddr;\n\n\t\t \n\t\tif (i == 0) {\n\t\t\tif (blkaddr == COMPRESS_ADDR)\n\t\t\t\tfio.compr_blocks++;\n\t\t\tif (__is_valid_data_blkaddr(blkaddr))\n\t\t\t\tf2fs_invalidate_blocks(sbi, blkaddr);\n\t\t\tf2fs_update_data_blkaddr(&dn, COMPRESS_ADDR);\n\t\t\tgoto unlock_continue;\n\t\t}\n\n\t\tif (fio.compr_blocks && __is_valid_data_blkaddr(blkaddr))\n\t\t\tfio.compr_blocks++;\n\n\t\tif (i > cc->valid_nr_cpages) {\n\t\t\tif (__is_valid_data_blkaddr(blkaddr)) {\n\t\t\t\tf2fs_invalidate_blocks(sbi, blkaddr);\n\t\t\t\tf2fs_update_data_blkaddr(&dn, NEW_ADDR);\n\t\t\t}\n\t\t\tgoto unlock_continue;\n\t\t}\n\n\t\tf2fs_bug_on(fio.sbi, blkaddr == NULL_ADDR);\n\n\t\tif (fio.encrypted)\n\t\t\tfio.encrypted_page = cc->cpages[i - 1];\n\t\telse\n\t\t\tfio.compressed_page = cc->cpages[i - 1];\n\n\t\tcc->cpages[i - 1] = NULL;\n\t\tf2fs_outplace_write_data(&dn, &fio);\n\t\t(*submitted)++;\nunlock_continue:\n\t\tinode_dec_dirty_pages(cc->inode);\n\t\tunlock_page(fio.page);\n\t}\n\n\tif (fio.compr_blocks)\n\t\tf2fs_i_compr_blocks_update(inode, fio.compr_blocks - 1, false);\n\tf2fs_i_compr_blocks_update(inode, cc->valid_nr_cpages, true);\n\tadd_compr_block_stat(inode, cc->valid_nr_cpages);\n\n\tset_inode_flag(cc->inode, FI_APPEND_WRITE);\n\tif (cc->cluster_idx == 0)\n\t\tset_inode_flag(inode, FI_FIRST_BLOCK_WRITTEN);\n\n\tf2fs_put_dnode(&dn);\n\tif (quota_inode)\n\t\tf2fs_up_read(&sbi->node_write);\n\telse\n\t\tf2fs_unlock_op(sbi);\n\n\tspin_lock(&fi->i_size_lock);\n\tif (fi->last_disk_size < psize)\n\t\tfi->last_disk_size = psize;\n\tspin_unlock(&fi->i_size_lock);\n\n\tf2fs_put_rpages(cc);\n\tpage_array_free(cc->inode, cc->cpages, cc->nr_cpages);\n\tcc->cpages = NULL;\n\tf2fs_destroy_compress_ctx(cc, false);\n\treturn 0;\n\nout_destroy_crypt:\n\tpage_array_free(cc->inode, cic->rpages, cc->cluster_size);\n\n\tfor (--i; i >= 0; i--)\n\t\tfscrypt_finalize_bounce_page(&cc->cpages[i]);\nout_put_cic:\n\tkmem_cache_free(cic_entry_slab, cic);\nout_put_dnode:\n\tf2fs_put_dnode(&dn);\nout_unlock_op:\n\tif (quota_inode)\n\t\tf2fs_up_read(&sbi->node_write);\n\telse\n\t\tf2fs_unlock_op(sbi);\nout_free:\n\tfor (i = 0; i < cc->valid_nr_cpages; i++) {\n\t\tf2fs_compress_free_page(cc->cpages[i]);\n\t\tcc->cpages[i] = NULL;\n\t}\n\tpage_array_free(cc->inode, cc->cpages, cc->nr_cpages);\n\tcc->cpages = NULL;\n\treturn -EAGAIN;\n}\n\nvoid f2fs_compress_write_end_io(struct bio *bio, struct page *page)\n{\n\tstruct f2fs_sb_info *sbi = bio->bi_private;\n\tstruct compress_io_ctx *cic =\n\t\t\t(struct compress_io_ctx *)page_private(page);\n\tint i;\n\n\tif (unlikely(bio->bi_status))\n\t\tmapping_set_error(cic->inode->i_mapping, -EIO);\n\n\tf2fs_compress_free_page(page);\n\n\tdec_page_count(sbi, F2FS_WB_DATA);\n\n\tif (atomic_dec_return(&cic->pending_pages))\n\t\treturn;\n\n\tfor (i = 0; i < cic->nr_rpages; i++) {\n\t\tWARN_ON(!cic->rpages[i]);\n\t\tclear_page_private_gcing(cic->rpages[i]);\n\t\tend_page_writeback(cic->rpages[i]);\n\t}\n\n\tpage_array_free(cic->inode, cic->rpages, cic->nr_rpages);\n\tkmem_cache_free(cic_entry_slab, cic);\n}\n\nstatic int f2fs_write_raw_pages(struct compress_ctx *cc,\n\t\t\t\t\tint *submitted,\n\t\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\t\tenum iostat_type io_type)\n{\n\tstruct address_space *mapping = cc->inode->i_mapping;\n\tint _submitted, compr_blocks, ret, i;\n\n\tcompr_blocks = f2fs_compressed_blocks(cc);\n\n\tfor (i = 0; i < cc->cluster_size; i++) {\n\t\tif (!cc->rpages[i])\n\t\t\tcontinue;\n\n\t\tredirty_page_for_writepage(wbc, cc->rpages[i]);\n\t\tunlock_page(cc->rpages[i]);\n\t}\n\n\tif (compr_blocks < 0)\n\t\treturn compr_blocks;\n\n\tfor (i = 0; i < cc->cluster_size; i++) {\n\t\tif (!cc->rpages[i])\n\t\t\tcontinue;\nretry_write:\n\t\tlock_page(cc->rpages[i]);\n\n\t\tif (cc->rpages[i]->mapping != mapping) {\ncontinue_unlock:\n\t\t\tunlock_page(cc->rpages[i]);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!PageDirty(cc->rpages[i]))\n\t\t\tgoto continue_unlock;\n\n\t\tif (PageWriteback(cc->rpages[i])) {\n\t\t\tif (wbc->sync_mode == WB_SYNC_NONE)\n\t\t\t\tgoto continue_unlock;\n\t\t\tf2fs_wait_on_page_writeback(cc->rpages[i], DATA, true, true);\n\t\t}\n\n\t\tif (!clear_page_dirty_for_io(cc->rpages[i]))\n\t\t\tgoto continue_unlock;\n\n\t\tret = f2fs_write_single_data_page(cc->rpages[i], &_submitted,\n\t\t\t\t\t\tNULL, NULL, wbc, io_type,\n\t\t\t\t\t\tcompr_blocks, false);\n\t\tif (ret) {\n\t\t\tif (ret == AOP_WRITEPAGE_ACTIVATE) {\n\t\t\t\tunlock_page(cc->rpages[i]);\n\t\t\t\tret = 0;\n\t\t\t} else if (ret == -EAGAIN) {\n\t\t\t\t \n\t\t\t\tif (IS_NOQUOTA(cc->inode))\n\t\t\t\t\treturn 0;\n\t\t\t\tret = 0;\n\t\t\t\tf2fs_io_schedule_timeout(DEFAULT_IO_TIMEOUT);\n\t\t\t\tgoto retry_write;\n\t\t\t}\n\t\t\treturn ret;\n\t\t}\n\n\t\t*submitted += _submitted;\n\t}\n\n\tf2fs_balance_fs(F2FS_M_SB(mapping), true);\n\n\treturn 0;\n}\n\nint f2fs_write_multi_pages(struct compress_ctx *cc,\n\t\t\t\t\tint *submitted,\n\t\t\t\t\tstruct writeback_control *wbc,\n\t\t\t\t\tenum iostat_type io_type)\n{\n\tint err;\n\n\t*submitted = 0;\n\tif (cluster_may_compress(cc)) {\n\t\terr = f2fs_compress_pages(cc);\n\t\tif (err == -EAGAIN) {\n\t\t\tadd_compr_block_stat(cc->inode, cc->cluster_size);\n\t\t\tgoto write;\n\t\t} else if (err) {\n\t\t\tf2fs_put_rpages_wbc(cc, wbc, true, 1);\n\t\t\tgoto destroy_out;\n\t\t}\n\n\t\terr = f2fs_write_compressed_pages(cc, submitted,\n\t\t\t\t\t\t\twbc, io_type);\n\t\tif (!err)\n\t\t\treturn 0;\n\t\tf2fs_bug_on(F2FS_I_SB(cc->inode), err != -EAGAIN);\n\t}\nwrite:\n\tf2fs_bug_on(F2FS_I_SB(cc->inode), *submitted);\n\n\terr = f2fs_write_raw_pages(cc, submitted, wbc, io_type);\n\tf2fs_put_rpages_wbc(cc, wbc, false, 0);\ndestroy_out:\n\tf2fs_destroy_compress_ctx(cc, false);\n\treturn err;\n}\n\nstatic inline bool allow_memalloc_for_decomp(struct f2fs_sb_info *sbi,\n\t\tbool pre_alloc)\n{\n\treturn pre_alloc ^ f2fs_low_mem_mode(sbi);\n}\n\nstatic int f2fs_prepare_decomp_mem(struct decompress_io_ctx *dic,\n\t\tbool pre_alloc)\n{\n\tconst struct f2fs_compress_ops *cops =\n\t\tf2fs_cops[F2FS_I(dic->inode)->i_compress_algorithm];\n\tint i;\n\n\tif (!allow_memalloc_for_decomp(F2FS_I_SB(dic->inode), pre_alloc))\n\t\treturn 0;\n\n\tdic->tpages = page_array_alloc(dic->inode, dic->cluster_size);\n\tif (!dic->tpages)\n\t\treturn -ENOMEM;\n\n\tfor (i = 0; i < dic->cluster_size; i++) {\n\t\tif (dic->rpages[i]) {\n\t\t\tdic->tpages[i] = dic->rpages[i];\n\t\t\tcontinue;\n\t\t}\n\n\t\tdic->tpages[i] = f2fs_compress_alloc_page();\n\t}\n\n\tdic->rbuf = f2fs_vmap(dic->tpages, dic->cluster_size);\n\tif (!dic->rbuf)\n\t\treturn -ENOMEM;\n\n\tdic->cbuf = f2fs_vmap(dic->cpages, dic->nr_cpages);\n\tif (!dic->cbuf)\n\t\treturn -ENOMEM;\n\n\tif (cops->init_decompress_ctx)\n\t\treturn cops->init_decompress_ctx(dic);\n\n\treturn 0;\n}\n\nstatic void f2fs_release_decomp_mem(struct decompress_io_ctx *dic,\n\t\tbool bypass_destroy_callback, bool pre_alloc)\n{\n\tconst struct f2fs_compress_ops *cops =\n\t\tf2fs_cops[F2FS_I(dic->inode)->i_compress_algorithm];\n\n\tif (!allow_memalloc_for_decomp(F2FS_I_SB(dic->inode), pre_alloc))\n\t\treturn;\n\n\tif (!bypass_destroy_callback && cops->destroy_decompress_ctx)\n\t\tcops->destroy_decompress_ctx(dic);\n\n\tif (dic->cbuf)\n\t\tvm_unmap_ram(dic->cbuf, dic->nr_cpages);\n\n\tif (dic->rbuf)\n\t\tvm_unmap_ram(dic->rbuf, dic->cluster_size);\n}\n\nstatic void f2fs_free_dic(struct decompress_io_ctx *dic,\n\t\tbool bypass_destroy_callback);\n\nstruct decompress_io_ctx *f2fs_alloc_dic(struct compress_ctx *cc)\n{\n\tstruct decompress_io_ctx *dic;\n\tpgoff_t start_idx = start_idx_of_cluster(cc);\n\tstruct f2fs_sb_info *sbi = F2FS_I_SB(cc->inode);\n\tint i, ret;\n\n\tdic = f2fs_kmem_cache_alloc(dic_entry_slab, GFP_F2FS_ZERO, false, sbi);\n\tif (!dic)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tdic->rpages = page_array_alloc(cc->inode, cc->cluster_size);\n\tif (!dic->rpages) {\n\t\tkmem_cache_free(dic_entry_slab, dic);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tdic->magic = F2FS_COMPRESSED_PAGE_MAGIC;\n\tdic->inode = cc->inode;\n\tatomic_set(&dic->remaining_pages, cc->nr_cpages);\n\tdic->cluster_idx = cc->cluster_idx;\n\tdic->cluster_size = cc->cluster_size;\n\tdic->log_cluster_size = cc->log_cluster_size;\n\tdic->nr_cpages = cc->nr_cpages;\n\trefcount_set(&dic->refcnt, 1);\n\tdic->failed = false;\n\tdic->need_verity = f2fs_need_verity(cc->inode, start_idx);\n\n\tfor (i = 0; i < dic->cluster_size; i++)\n\t\tdic->rpages[i] = cc->rpages[i];\n\tdic->nr_rpages = cc->cluster_size;\n\n\tdic->cpages = page_array_alloc(dic->inode, dic->nr_cpages);\n\tif (!dic->cpages) {\n\t\tret = -ENOMEM;\n\t\tgoto out_free;\n\t}\n\n\tfor (i = 0; i < dic->nr_cpages; i++) {\n\t\tstruct page *page;\n\n\t\tpage = f2fs_compress_alloc_page();\n\t\tf2fs_set_compressed_page(page, cc->inode,\n\t\t\t\t\tstart_idx + i + 1, dic);\n\t\tdic->cpages[i] = page;\n\t}\n\n\tret = f2fs_prepare_decomp_mem(dic, true);\n\tif (ret)\n\t\tgoto out_free;\n\n\treturn dic;\n\nout_free:\n\tf2fs_free_dic(dic, true);\n\treturn ERR_PTR(ret);\n}\n\nstatic void f2fs_free_dic(struct decompress_io_ctx *dic,\n\t\tbool bypass_destroy_callback)\n{\n\tint i;\n\n\tf2fs_release_decomp_mem(dic, bypass_destroy_callback, true);\n\n\tif (dic->tpages) {\n\t\tfor (i = 0; i < dic->cluster_size; i++) {\n\t\t\tif (dic->rpages[i])\n\t\t\t\tcontinue;\n\t\t\tif (!dic->tpages[i])\n\t\t\t\tcontinue;\n\t\t\tf2fs_compress_free_page(dic->tpages[i]);\n\t\t}\n\t\tpage_array_free(dic->inode, dic->tpages, dic->cluster_size);\n\t}\n\n\tif (dic->cpages) {\n\t\tfor (i = 0; i < dic->nr_cpages; i++) {\n\t\t\tif (!dic->cpages[i])\n\t\t\t\tcontinue;\n\t\t\tf2fs_compress_free_page(dic->cpages[i]);\n\t\t}\n\t\tpage_array_free(dic->inode, dic->cpages, dic->nr_cpages);\n\t}\n\n\tpage_array_free(dic->inode, dic->rpages, dic->nr_rpages);\n\tkmem_cache_free(dic_entry_slab, dic);\n}\n\nstatic void f2fs_late_free_dic(struct work_struct *work)\n{\n\tstruct decompress_io_ctx *dic =\n\t\tcontainer_of(work, struct decompress_io_ctx, free_work);\n\n\tf2fs_free_dic(dic, false);\n}\n\nstatic void f2fs_put_dic(struct decompress_io_ctx *dic, bool in_task)\n{\n\tif (refcount_dec_and_test(&dic->refcnt)) {\n\t\tif (in_task) {\n\t\t\tf2fs_free_dic(dic, false);\n\t\t} else {\n\t\t\tINIT_WORK(&dic->free_work, f2fs_late_free_dic);\n\t\t\tqueue_work(F2FS_I_SB(dic->inode)->post_read_wq,\n\t\t\t\t\t&dic->free_work);\n\t\t}\n\t}\n}\n\nstatic void f2fs_verify_cluster(struct work_struct *work)\n{\n\tstruct decompress_io_ctx *dic =\n\t\tcontainer_of(work, struct decompress_io_ctx, verity_work);\n\tint i;\n\n\t \n\tfor (i = 0; i < dic->cluster_size; i++) {\n\t\tstruct page *rpage = dic->rpages[i];\n\n\t\tif (!rpage)\n\t\t\tcontinue;\n\n\t\tif (fsverity_verify_page(rpage))\n\t\t\tSetPageUptodate(rpage);\n\t\telse\n\t\t\tClearPageUptodate(rpage);\n\t\tunlock_page(rpage);\n\t}\n\n\tf2fs_put_dic(dic, true);\n}\n\n \nvoid f2fs_decompress_end_io(struct decompress_io_ctx *dic, bool failed,\n\t\t\t\tbool in_task)\n{\n\tint i;\n\n\tif (!failed && dic->need_verity) {\n\t\t \n\t\tINIT_WORK(&dic->verity_work, f2fs_verify_cluster);\n\t\tfsverity_enqueue_verify_work(&dic->verity_work);\n\t\treturn;\n\t}\n\n\t \n\tfor (i = 0; i < dic->cluster_size; i++) {\n\t\tstruct page *rpage = dic->rpages[i];\n\n\t\tif (!rpage)\n\t\t\tcontinue;\n\n\t\tif (failed)\n\t\t\tClearPageUptodate(rpage);\n\t\telse\n\t\t\tSetPageUptodate(rpage);\n\t\tunlock_page(rpage);\n\t}\n\n\t \n\tf2fs_put_dic(dic, in_task);\n}\n\n \nvoid f2fs_put_page_dic(struct page *page, bool in_task)\n{\n\tstruct decompress_io_ctx *dic =\n\t\t\t(struct decompress_io_ctx *)page_private(page);\n\n\tf2fs_put_dic(dic, in_task);\n}\n\n \nunsigned int f2fs_cluster_blocks_are_contiguous(struct dnode_of_data *dn)\n{\n\tbool compressed = f2fs_data_blkaddr(dn) == COMPRESS_ADDR;\n\tint i = compressed ? 1 : 0;\n\tblock_t first_blkaddr = data_blkaddr(dn->inode, dn->node_page,\n\t\t\t\t\t\tdn->ofs_in_node + i);\n\n\tfor (i += 1; i < F2FS_I(dn->inode)->i_cluster_size; i++) {\n\t\tblock_t blkaddr = data_blkaddr(dn->inode, dn->node_page,\n\t\t\t\t\t\tdn->ofs_in_node + i);\n\n\t\tif (!__is_valid_data_blkaddr(blkaddr))\n\t\t\tbreak;\n\t\tif (first_blkaddr + i - (compressed ? 1 : 0) != blkaddr)\n\t\t\treturn 0;\n\t}\n\n\treturn compressed ? i - 1 : i;\n}\n\nconst struct address_space_operations f2fs_compress_aops = {\n\t.release_folio = f2fs_release_folio,\n\t.invalidate_folio = f2fs_invalidate_folio,\n\t.migrate_folio\t= filemap_migrate_folio,\n};\n\nstruct address_space *COMPRESS_MAPPING(struct f2fs_sb_info *sbi)\n{\n\treturn sbi->compress_inode->i_mapping;\n}\n\nvoid f2fs_invalidate_compress_page(struct f2fs_sb_info *sbi, block_t blkaddr)\n{\n\tif (!sbi->compress_inode)\n\t\treturn;\n\tinvalidate_mapping_pages(COMPRESS_MAPPING(sbi), blkaddr, blkaddr);\n}\n\nvoid f2fs_cache_compressed_page(struct f2fs_sb_info *sbi, struct page *page,\n\t\t\t\t\t\tnid_t ino, block_t blkaddr)\n{\n\tstruct page *cpage;\n\tint ret;\n\n\tif (!test_opt(sbi, COMPRESS_CACHE))\n\t\treturn;\n\n\tif (!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC_ENHANCE_READ))\n\t\treturn;\n\n\tif (!f2fs_available_free_memory(sbi, COMPRESS_PAGE))\n\t\treturn;\n\n\tcpage = find_get_page(COMPRESS_MAPPING(sbi), blkaddr);\n\tif (cpage) {\n\t\tf2fs_put_page(cpage, 0);\n\t\treturn;\n\t}\n\n\tcpage = alloc_page(__GFP_NOWARN | __GFP_IO);\n\tif (!cpage)\n\t\treturn;\n\n\tret = add_to_page_cache_lru(cpage, COMPRESS_MAPPING(sbi),\n\t\t\t\t\t\tblkaddr, GFP_NOFS);\n\tif (ret) {\n\t\tf2fs_put_page(cpage, 0);\n\t\treturn;\n\t}\n\n\tset_page_private_data(cpage, ino);\n\n\tif (!f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC_ENHANCE_READ))\n\t\tgoto out;\n\n\tmemcpy(page_address(cpage), page_address(page), PAGE_SIZE);\n\tSetPageUptodate(cpage);\nout:\n\tf2fs_put_page(cpage, 1);\n}\n\nbool f2fs_load_compressed_page(struct f2fs_sb_info *sbi, struct page *page,\n\t\t\t\t\t\t\t\tblock_t blkaddr)\n{\n\tstruct page *cpage;\n\tbool hitted = false;\n\n\tif (!test_opt(sbi, COMPRESS_CACHE))\n\t\treturn false;\n\n\tcpage = f2fs_pagecache_get_page(COMPRESS_MAPPING(sbi),\n\t\t\t\tblkaddr, FGP_LOCK | FGP_NOWAIT, GFP_NOFS);\n\tif (cpage) {\n\t\tif (PageUptodate(cpage)) {\n\t\t\tatomic_inc(&sbi->compress_page_hit);\n\t\t\tmemcpy(page_address(page),\n\t\t\t\tpage_address(cpage), PAGE_SIZE);\n\t\t\thitted = true;\n\t\t}\n\t\tf2fs_put_page(cpage, 1);\n\t}\n\n\treturn hitted;\n}\n\nvoid f2fs_invalidate_compress_pages(struct f2fs_sb_info *sbi, nid_t ino)\n{\n\tstruct address_space *mapping = COMPRESS_MAPPING(sbi);\n\tstruct folio_batch fbatch;\n\tpgoff_t index = 0;\n\tpgoff_t end = MAX_BLKADDR(sbi);\n\n\tif (!mapping->nrpages)\n\t\treturn;\n\n\tfolio_batch_init(&fbatch);\n\n\tdo {\n\t\tunsigned int nr, i;\n\n\t\tnr = filemap_get_folios(mapping, &index, end - 1, &fbatch);\n\t\tif (!nr)\n\t\t\tbreak;\n\n\t\tfor (i = 0; i < nr; i++) {\n\t\t\tstruct folio *folio = fbatch.folios[i];\n\n\t\t\tfolio_lock(folio);\n\t\t\tif (folio->mapping != mapping) {\n\t\t\t\tfolio_unlock(folio);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (ino != get_page_private_data(&folio->page)) {\n\t\t\t\tfolio_unlock(folio);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tgeneric_error_remove_page(mapping, &folio->page);\n\t\t\tfolio_unlock(folio);\n\t\t}\n\t\tfolio_batch_release(&fbatch);\n\t\tcond_resched();\n\t} while (index < end);\n}\n\nint f2fs_init_compress_inode(struct f2fs_sb_info *sbi)\n{\n\tstruct inode *inode;\n\n\tif (!test_opt(sbi, COMPRESS_CACHE))\n\t\treturn 0;\n\n\tinode = f2fs_iget(sbi->sb, F2FS_COMPRESS_INO(sbi));\n\tif (IS_ERR(inode))\n\t\treturn PTR_ERR(inode);\n\tsbi->compress_inode = inode;\n\n\tsbi->compress_percent = COMPRESS_PERCENT;\n\tsbi->compress_watermark = COMPRESS_WATERMARK;\n\n\tatomic_set(&sbi->compress_page_hit, 0);\n\n\treturn 0;\n}\n\nvoid f2fs_destroy_compress_inode(struct f2fs_sb_info *sbi)\n{\n\tif (!sbi->compress_inode)\n\t\treturn;\n\tiput(sbi->compress_inode);\n\tsbi->compress_inode = NULL;\n}\n\nint f2fs_init_page_array_cache(struct f2fs_sb_info *sbi)\n{\n\tdev_t dev = sbi->sb->s_bdev->bd_dev;\n\tchar slab_name[35];\n\n\tif (!f2fs_sb_has_compression(sbi))\n\t\treturn 0;\n\n\tsprintf(slab_name, \"f2fs_page_array_entry-%u:%u\", MAJOR(dev), MINOR(dev));\n\n\tsbi->page_array_slab_size = sizeof(struct page *) <<\n\t\t\t\t\tF2FS_OPTION(sbi).compress_log_size;\n\n\tsbi->page_array_slab = f2fs_kmem_cache_create(slab_name,\n\t\t\t\t\tsbi->page_array_slab_size);\n\treturn sbi->page_array_slab ? 0 : -ENOMEM;\n}\n\nvoid f2fs_destroy_page_array_cache(struct f2fs_sb_info *sbi)\n{\n\tkmem_cache_destroy(sbi->page_array_slab);\n}\n\nint __init f2fs_init_compress_cache(void)\n{\n\tcic_entry_slab = f2fs_kmem_cache_create(\"f2fs_cic_entry\",\n\t\t\t\t\tsizeof(struct compress_io_ctx));\n\tif (!cic_entry_slab)\n\t\treturn -ENOMEM;\n\tdic_entry_slab = f2fs_kmem_cache_create(\"f2fs_dic_entry\",\n\t\t\t\t\tsizeof(struct decompress_io_ctx));\n\tif (!dic_entry_slab)\n\t\tgoto free_cic;\n\treturn 0;\nfree_cic:\n\tkmem_cache_destroy(cic_entry_slab);\n\treturn -ENOMEM;\n}\n\nvoid f2fs_destroy_compress_cache(void)\n{\n\tkmem_cache_destroy(dic_entry_slab);\n\tkmem_cache_destroy(cic_entry_slab);\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}