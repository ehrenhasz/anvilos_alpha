{
  "module_name": "dax.c",
  "hash_id": "60cc49edcfd1e2f74fc4c6d0aeb3947cdabf6a1b7b4136cd54c14784107f52c3",
  "original_prompt": "Ingested from linux-6.6.14/fs/dax.c",
  "human_readable_source": "\n \n\n#include <linux/atomic.h>\n#include <linux/blkdev.h>\n#include <linux/buffer_head.h>\n#include <linux/dax.h>\n#include <linux/fs.h>\n#include <linux/highmem.h>\n#include <linux/memcontrol.h>\n#include <linux/mm.h>\n#include <linux/mutex.h>\n#include <linux/pagevec.h>\n#include <linux/sched.h>\n#include <linux/sched/signal.h>\n#include <linux/uio.h>\n#include <linux/vmstat.h>\n#include <linux/pfn_t.h>\n#include <linux/sizes.h>\n#include <linux/mmu_notifier.h>\n#include <linux/iomap.h>\n#include <linux/rmap.h>\n#include <asm/pgalloc.h>\n\n#define CREATE_TRACE_POINTS\n#include <trace/events/fs_dax.h>\n\n \n#define DAX_WAIT_TABLE_BITS 12\n#define DAX_WAIT_TABLE_ENTRIES (1 << DAX_WAIT_TABLE_BITS)\n\n \n#define PG_PMD_COLOUR\t((PMD_SIZE >> PAGE_SHIFT) - 1)\n#define PG_PMD_NR\t(PMD_SIZE >> PAGE_SHIFT)\n\nstatic wait_queue_head_t wait_table[DAX_WAIT_TABLE_ENTRIES];\n\nstatic int __init init_dax_wait_table(void)\n{\n\tint i;\n\n\tfor (i = 0; i < DAX_WAIT_TABLE_ENTRIES; i++)\n\t\tinit_waitqueue_head(wait_table + i);\n\treturn 0;\n}\nfs_initcall(init_dax_wait_table);\n\n \n#define DAX_SHIFT\t(4)\n#define DAX_LOCKED\t(1UL << 0)\n#define DAX_PMD\t\t(1UL << 1)\n#define DAX_ZERO_PAGE\t(1UL << 2)\n#define DAX_EMPTY\t(1UL << 3)\n\nstatic unsigned long dax_to_pfn(void *entry)\n{\n\treturn xa_to_value(entry) >> DAX_SHIFT;\n}\n\nstatic void *dax_make_entry(pfn_t pfn, unsigned long flags)\n{\n\treturn xa_mk_value(flags | (pfn_t_to_pfn(pfn) << DAX_SHIFT));\n}\n\nstatic bool dax_is_locked(void *entry)\n{\n\treturn xa_to_value(entry) & DAX_LOCKED;\n}\n\nstatic unsigned int dax_entry_order(void *entry)\n{\n\tif (xa_to_value(entry) & DAX_PMD)\n\t\treturn PMD_ORDER;\n\treturn 0;\n}\n\nstatic unsigned long dax_is_pmd_entry(void *entry)\n{\n\treturn xa_to_value(entry) & DAX_PMD;\n}\n\nstatic bool dax_is_pte_entry(void *entry)\n{\n\treturn !(xa_to_value(entry) & DAX_PMD);\n}\n\nstatic int dax_is_zero_entry(void *entry)\n{\n\treturn xa_to_value(entry) & DAX_ZERO_PAGE;\n}\n\nstatic int dax_is_empty_entry(void *entry)\n{\n\treturn xa_to_value(entry) & DAX_EMPTY;\n}\n\n \nstatic bool dax_is_conflict(void *entry)\n{\n\treturn entry == XA_RETRY_ENTRY;\n}\n\n \nstruct exceptional_entry_key {\n\tstruct xarray *xa;\n\tpgoff_t entry_start;\n};\n\nstruct wait_exceptional_entry_queue {\n\twait_queue_entry_t wait;\n\tstruct exceptional_entry_key key;\n};\n\n \nenum dax_wake_mode {\n\tWAKE_ALL,\n\tWAKE_NEXT,\n};\n\nstatic wait_queue_head_t *dax_entry_waitqueue(struct xa_state *xas,\n\t\tvoid *entry, struct exceptional_entry_key *key)\n{\n\tunsigned long hash;\n\tunsigned long index = xas->xa_index;\n\n\t \n\tif (dax_is_pmd_entry(entry))\n\t\tindex &= ~PG_PMD_COLOUR;\n\tkey->xa = xas->xa;\n\tkey->entry_start = index;\n\n\thash = hash_long((unsigned long)xas->xa ^ index, DAX_WAIT_TABLE_BITS);\n\treturn wait_table + hash;\n}\n\nstatic int wake_exceptional_entry_func(wait_queue_entry_t *wait,\n\t\tunsigned int mode, int sync, void *keyp)\n{\n\tstruct exceptional_entry_key *key = keyp;\n\tstruct wait_exceptional_entry_queue *ewait =\n\t\tcontainer_of(wait, struct wait_exceptional_entry_queue, wait);\n\n\tif (key->xa != ewait->key.xa ||\n\t    key->entry_start != ewait->key.entry_start)\n\t\treturn 0;\n\treturn autoremove_wake_function(wait, mode, sync, NULL);\n}\n\n \nstatic void dax_wake_entry(struct xa_state *xas, void *entry,\n\t\t\t   enum dax_wake_mode mode)\n{\n\tstruct exceptional_entry_key key;\n\twait_queue_head_t *wq;\n\n\twq = dax_entry_waitqueue(xas, entry, &key);\n\n\t \n\tif (waitqueue_active(wq))\n\t\t__wake_up(wq, TASK_NORMAL, mode == WAKE_ALL ? 0 : 1, &key);\n}\n\n \nstatic void *get_unlocked_entry(struct xa_state *xas, unsigned int order)\n{\n\tvoid *entry;\n\tstruct wait_exceptional_entry_queue ewait;\n\twait_queue_head_t *wq;\n\n\tinit_wait(&ewait.wait);\n\tewait.wait.func = wake_exceptional_entry_func;\n\n\tfor (;;) {\n\t\tentry = xas_find_conflict(xas);\n\t\tif (!entry || WARN_ON_ONCE(!xa_is_value(entry)))\n\t\t\treturn entry;\n\t\tif (dax_entry_order(entry) < order)\n\t\t\treturn XA_RETRY_ENTRY;\n\t\tif (!dax_is_locked(entry))\n\t\t\treturn entry;\n\n\t\twq = dax_entry_waitqueue(xas, entry, &ewait.key);\n\t\tprepare_to_wait_exclusive(wq, &ewait.wait,\n\t\t\t\t\t  TASK_UNINTERRUPTIBLE);\n\t\txas_unlock_irq(xas);\n\t\txas_reset(xas);\n\t\tschedule();\n\t\tfinish_wait(wq, &ewait.wait);\n\t\txas_lock_irq(xas);\n\t}\n}\n\n \nstatic void wait_entry_unlocked(struct xa_state *xas, void *entry)\n{\n\tstruct wait_exceptional_entry_queue ewait;\n\twait_queue_head_t *wq;\n\n\tinit_wait(&ewait.wait);\n\tewait.wait.func = wake_exceptional_entry_func;\n\n\twq = dax_entry_waitqueue(xas, entry, &ewait.key);\n\t \n\tprepare_to_wait(wq, &ewait.wait, TASK_UNINTERRUPTIBLE);\n\txas_unlock_irq(xas);\n\tschedule();\n\tfinish_wait(wq, &ewait.wait);\n}\n\nstatic void put_unlocked_entry(struct xa_state *xas, void *entry,\n\t\t\t       enum dax_wake_mode mode)\n{\n\tif (entry && !dax_is_conflict(entry))\n\t\tdax_wake_entry(xas, entry, mode);\n}\n\n \nstatic void dax_unlock_entry(struct xa_state *xas, void *entry)\n{\n\tvoid *old;\n\n\tBUG_ON(dax_is_locked(entry));\n\txas_reset(xas);\n\txas_lock_irq(xas);\n\told = xas_store(xas, entry);\n\txas_unlock_irq(xas);\n\tBUG_ON(!dax_is_locked(old));\n\tdax_wake_entry(xas, entry, WAKE_NEXT);\n}\n\n \nstatic void *dax_lock_entry(struct xa_state *xas, void *entry)\n{\n\tunsigned long v = xa_to_value(entry);\n\treturn xas_store(xas, xa_mk_value(v | DAX_LOCKED));\n}\n\nstatic unsigned long dax_entry_size(void *entry)\n{\n\tif (dax_is_zero_entry(entry))\n\t\treturn 0;\n\telse if (dax_is_empty_entry(entry))\n\t\treturn 0;\n\telse if (dax_is_pmd_entry(entry))\n\t\treturn PMD_SIZE;\n\telse\n\t\treturn PAGE_SIZE;\n}\n\nstatic unsigned long dax_end_pfn(void *entry)\n{\n\treturn dax_to_pfn(entry) + dax_entry_size(entry) / PAGE_SIZE;\n}\n\n \n#define for_each_mapped_pfn(entry, pfn) \\\n\tfor (pfn = dax_to_pfn(entry); \\\n\t\t\tpfn < dax_end_pfn(entry); pfn++)\n\nstatic inline bool dax_page_is_shared(struct page *page)\n{\n\treturn page->mapping == PAGE_MAPPING_DAX_SHARED;\n}\n\n \nstatic inline void dax_page_share_get(struct page *page)\n{\n\tif (page->mapping != PAGE_MAPPING_DAX_SHARED) {\n\t\t \n\t\tif (page->mapping)\n\t\t\tpage->share = 1;\n\t\tpage->mapping = PAGE_MAPPING_DAX_SHARED;\n\t}\n\tpage->share++;\n}\n\nstatic inline unsigned long dax_page_share_put(struct page *page)\n{\n\treturn --page->share;\n}\n\n \nstatic void dax_associate_entry(void *entry, struct address_space *mapping,\n\t\tstruct vm_area_struct *vma, unsigned long address, bool shared)\n{\n\tunsigned long size = dax_entry_size(entry), pfn, index;\n\tint i = 0;\n\n\tif (IS_ENABLED(CONFIG_FS_DAX_LIMITED))\n\t\treturn;\n\n\tindex = linear_page_index(vma, address & ~(size - 1));\n\tfor_each_mapped_pfn(entry, pfn) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\tif (shared) {\n\t\t\tdax_page_share_get(page);\n\t\t} else {\n\t\t\tWARN_ON_ONCE(page->mapping);\n\t\t\tpage->mapping = mapping;\n\t\t\tpage->index = index + i++;\n\t\t}\n\t}\n}\n\nstatic void dax_disassociate_entry(void *entry, struct address_space *mapping,\n\t\tbool trunc)\n{\n\tunsigned long pfn;\n\n\tif (IS_ENABLED(CONFIG_FS_DAX_LIMITED))\n\t\treturn;\n\n\tfor_each_mapped_pfn(entry, pfn) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\tWARN_ON_ONCE(trunc && page_ref_count(page) > 1);\n\t\tif (dax_page_is_shared(page)) {\n\t\t\t \n\t\t\tif (dax_page_share_put(page) > 0)\n\t\t\t\tcontinue;\n\t\t} else\n\t\t\tWARN_ON_ONCE(page->mapping && page->mapping != mapping);\n\t\tpage->mapping = NULL;\n\t\tpage->index = 0;\n\t}\n}\n\nstatic struct page *dax_busy_page(void *entry)\n{\n\tunsigned long pfn;\n\n\tfor_each_mapped_pfn(entry, pfn) {\n\t\tstruct page *page = pfn_to_page(pfn);\n\n\t\tif (page_ref_count(page) > 1)\n\t\t\treturn page;\n\t}\n\treturn NULL;\n}\n\n \ndax_entry_t dax_lock_folio(struct folio *folio)\n{\n\tXA_STATE(xas, NULL, 0);\n\tvoid *entry;\n\n\t \n\trcu_read_lock();\n\tfor (;;) {\n\t\tstruct address_space *mapping = READ_ONCE(folio->mapping);\n\n\t\tentry = NULL;\n\t\tif (!mapping || !dax_mapping(mapping))\n\t\t\tbreak;\n\n\t\t \n\t\tentry = (void *)~0UL;\n\t\tif (S_ISCHR(mapping->host->i_mode))\n\t\t\tbreak;\n\n\t\txas.xa = &mapping->i_pages;\n\t\txas_lock_irq(&xas);\n\t\tif (mapping != folio->mapping) {\n\t\t\txas_unlock_irq(&xas);\n\t\t\tcontinue;\n\t\t}\n\t\txas_set(&xas, folio->index);\n\t\tentry = xas_load(&xas);\n\t\tif (dax_is_locked(entry)) {\n\t\t\trcu_read_unlock();\n\t\t\twait_entry_unlocked(&xas, entry);\n\t\t\trcu_read_lock();\n\t\t\tcontinue;\n\t\t}\n\t\tdax_lock_entry(&xas, entry);\n\t\txas_unlock_irq(&xas);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\treturn (dax_entry_t)entry;\n}\n\nvoid dax_unlock_folio(struct folio *folio, dax_entry_t cookie)\n{\n\tstruct address_space *mapping = folio->mapping;\n\tXA_STATE(xas, &mapping->i_pages, folio->index);\n\n\tif (S_ISCHR(mapping->host->i_mode))\n\t\treturn;\n\n\tdax_unlock_entry(&xas, (void *)cookie);\n}\n\n \ndax_entry_t dax_lock_mapping_entry(struct address_space *mapping, pgoff_t index,\n\t\tstruct page **page)\n{\n\tXA_STATE(xas, NULL, 0);\n\tvoid *entry;\n\n\trcu_read_lock();\n\tfor (;;) {\n\t\tentry = NULL;\n\t\tif (!dax_mapping(mapping))\n\t\t\tbreak;\n\n\t\txas.xa = &mapping->i_pages;\n\t\txas_lock_irq(&xas);\n\t\txas_set(&xas, index);\n\t\tentry = xas_load(&xas);\n\t\tif (dax_is_locked(entry)) {\n\t\t\trcu_read_unlock();\n\t\t\twait_entry_unlocked(&xas, entry);\n\t\t\trcu_read_lock();\n\t\t\tcontinue;\n\t\t}\n\t\tif (!entry ||\n\t\t    dax_is_zero_entry(entry) || dax_is_empty_entry(entry)) {\n\t\t\t \n\t\t\tentry = (void *)~0UL;\n\t\t} else {\n\t\t\t*page = pfn_to_page(dax_to_pfn(entry));\n\t\t\tdax_lock_entry(&xas, entry);\n\t\t}\n\t\txas_unlock_irq(&xas);\n\t\tbreak;\n\t}\n\trcu_read_unlock();\n\treturn (dax_entry_t)entry;\n}\n\nvoid dax_unlock_mapping_entry(struct address_space *mapping, pgoff_t index,\n\t\tdax_entry_t cookie)\n{\n\tXA_STATE(xas, &mapping->i_pages, index);\n\n\tif (cookie == ~0UL)\n\t\treturn;\n\n\tdax_unlock_entry(&xas, (void *)cookie);\n}\n\n \nstatic void *grab_mapping_entry(struct xa_state *xas,\n\t\tstruct address_space *mapping, unsigned int order)\n{\n\tunsigned long index = xas->xa_index;\n\tbool pmd_downgrade;\t \n\tvoid *entry;\n\nretry:\n\tpmd_downgrade = false;\n\txas_lock_irq(xas);\n\tentry = get_unlocked_entry(xas, order);\n\n\tif (entry) {\n\t\tif (dax_is_conflict(entry))\n\t\t\tgoto fallback;\n\t\tif (!xa_is_value(entry)) {\n\t\t\txas_set_err(xas, -EIO);\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\tif (order == 0) {\n\t\t\tif (dax_is_pmd_entry(entry) &&\n\t\t\t    (dax_is_zero_entry(entry) ||\n\t\t\t     dax_is_empty_entry(entry))) {\n\t\t\t\tpmd_downgrade = true;\n\t\t\t}\n\t\t}\n\t}\n\n\tif (pmd_downgrade) {\n\t\t \n\t\tdax_lock_entry(xas, entry);\n\n\t\t \n\t\tif (dax_is_zero_entry(entry)) {\n\t\t\txas_unlock_irq(xas);\n\t\t\tunmap_mapping_pages(mapping,\n\t\t\t\t\txas->xa_index & ~PG_PMD_COLOUR,\n\t\t\t\t\tPG_PMD_NR, false);\n\t\t\txas_reset(xas);\n\t\t\txas_lock_irq(xas);\n\t\t}\n\n\t\tdax_disassociate_entry(entry, mapping, false);\n\t\txas_store(xas, NULL);\t \n\t\tdax_wake_entry(xas, entry, WAKE_ALL);\n\t\tmapping->nrpages -= PG_PMD_NR;\n\t\tentry = NULL;\n\t\txas_set(xas, index);\n\t}\n\n\tif (entry) {\n\t\tdax_lock_entry(xas, entry);\n\t} else {\n\t\tunsigned long flags = DAX_EMPTY;\n\n\t\tif (order > 0)\n\t\t\tflags |= DAX_PMD;\n\t\tentry = dax_make_entry(pfn_to_pfn_t(0), flags);\n\t\tdax_lock_entry(xas, entry);\n\t\tif (xas_error(xas))\n\t\t\tgoto out_unlock;\n\t\tmapping->nrpages += 1UL << order;\n\t}\n\nout_unlock:\n\txas_unlock_irq(xas);\n\tif (xas_nomem(xas, mapping_gfp_mask(mapping) & ~__GFP_HIGHMEM))\n\t\tgoto retry;\n\tif (xas->xa_node == XA_ERROR(-ENOMEM))\n\t\treturn xa_mk_internal(VM_FAULT_OOM);\n\tif (xas_error(xas))\n\t\treturn xa_mk_internal(VM_FAULT_SIGBUS);\n\treturn entry;\nfallback:\n\txas_unlock_irq(xas);\n\treturn xa_mk_internal(VM_FAULT_FALLBACK);\n}\n\n \nstruct page *dax_layout_busy_page_range(struct address_space *mapping,\n\t\t\t\t\tloff_t start, loff_t end)\n{\n\tvoid *entry;\n\tunsigned int scanned = 0;\n\tstruct page *page = NULL;\n\tpgoff_t start_idx = start >> PAGE_SHIFT;\n\tpgoff_t end_idx;\n\tXA_STATE(xas, &mapping->i_pages, start_idx);\n\n\t \n\tif (IS_ENABLED(CONFIG_FS_DAX_LIMITED))\n\t\treturn NULL;\n\n\tif (!dax_mapping(mapping) || !mapping_mapped(mapping))\n\t\treturn NULL;\n\n\t \n\tif (end == LLONG_MAX)\n\t\tend_idx = ULONG_MAX;\n\telse\n\t\tend_idx = end >> PAGE_SHIFT;\n\t \n\tunmap_mapping_pages(mapping, start_idx, end_idx - start_idx + 1, 0);\n\n\txas_lock_irq(&xas);\n\txas_for_each(&xas, entry, end_idx) {\n\t\tif (WARN_ON_ONCE(!xa_is_value(entry)))\n\t\t\tcontinue;\n\t\tif (unlikely(dax_is_locked(entry)))\n\t\t\tentry = get_unlocked_entry(&xas, 0);\n\t\tif (entry)\n\t\t\tpage = dax_busy_page(entry);\n\t\tput_unlocked_entry(&xas, entry, WAKE_NEXT);\n\t\tif (page)\n\t\t\tbreak;\n\t\tif (++scanned % XA_CHECK_SCHED)\n\t\t\tcontinue;\n\n\t\txas_pause(&xas);\n\t\txas_unlock_irq(&xas);\n\t\tcond_resched();\n\t\txas_lock_irq(&xas);\n\t}\n\txas_unlock_irq(&xas);\n\treturn page;\n}\nEXPORT_SYMBOL_GPL(dax_layout_busy_page_range);\n\nstruct page *dax_layout_busy_page(struct address_space *mapping)\n{\n\treturn dax_layout_busy_page_range(mapping, 0, LLONG_MAX);\n}\nEXPORT_SYMBOL_GPL(dax_layout_busy_page);\n\nstatic int __dax_invalidate_entry(struct address_space *mapping,\n\t\t\t\t\t  pgoff_t index, bool trunc)\n{\n\tXA_STATE(xas, &mapping->i_pages, index);\n\tint ret = 0;\n\tvoid *entry;\n\n\txas_lock_irq(&xas);\n\tentry = get_unlocked_entry(&xas, 0);\n\tif (!entry || WARN_ON_ONCE(!xa_is_value(entry)))\n\t\tgoto out;\n\tif (!trunc &&\n\t    (xas_get_mark(&xas, PAGECACHE_TAG_DIRTY) ||\n\t     xas_get_mark(&xas, PAGECACHE_TAG_TOWRITE)))\n\t\tgoto out;\n\tdax_disassociate_entry(entry, mapping, trunc);\n\txas_store(&xas, NULL);\n\tmapping->nrpages -= 1UL << dax_entry_order(entry);\n\tret = 1;\nout:\n\tput_unlocked_entry(&xas, entry, WAKE_ALL);\n\txas_unlock_irq(&xas);\n\treturn ret;\n}\n\nstatic int __dax_clear_dirty_range(struct address_space *mapping,\n\t\tpgoff_t start, pgoff_t end)\n{\n\tXA_STATE(xas, &mapping->i_pages, start);\n\tunsigned int scanned = 0;\n\tvoid *entry;\n\n\txas_lock_irq(&xas);\n\txas_for_each(&xas, entry, end) {\n\t\tentry = get_unlocked_entry(&xas, 0);\n\t\txas_clear_mark(&xas, PAGECACHE_TAG_DIRTY);\n\t\txas_clear_mark(&xas, PAGECACHE_TAG_TOWRITE);\n\t\tput_unlocked_entry(&xas, entry, WAKE_NEXT);\n\n\t\tif (++scanned % XA_CHECK_SCHED)\n\t\t\tcontinue;\n\n\t\txas_pause(&xas);\n\t\txas_unlock_irq(&xas);\n\t\tcond_resched();\n\t\txas_lock_irq(&xas);\n\t}\n\txas_unlock_irq(&xas);\n\n\treturn 0;\n}\n\n \nint dax_delete_mapping_entry(struct address_space *mapping, pgoff_t index)\n{\n\tint ret = __dax_invalidate_entry(mapping, index, true);\n\n\t \n\tWARN_ON_ONCE(!ret);\n\treturn ret;\n}\n\n \nint dax_invalidate_mapping_entry_sync(struct address_space *mapping,\n\t\t\t\t      pgoff_t index)\n{\n\treturn __dax_invalidate_entry(mapping, index, false);\n}\n\nstatic pgoff_t dax_iomap_pgoff(const struct iomap *iomap, loff_t pos)\n{\n\treturn PHYS_PFN(iomap->addr + (pos & PAGE_MASK) - iomap->offset);\n}\n\nstatic int copy_cow_page_dax(struct vm_fault *vmf, const struct iomap_iter *iter)\n{\n\tpgoff_t pgoff = dax_iomap_pgoff(&iter->iomap, iter->pos);\n\tvoid *vto, *kaddr;\n\tlong rc;\n\tint id;\n\n\tid = dax_read_lock();\n\trc = dax_direct_access(iter->iomap.dax_dev, pgoff, 1, DAX_ACCESS,\n\t\t\t\t&kaddr, NULL);\n\tif (rc < 0) {\n\t\tdax_read_unlock(id);\n\t\treturn rc;\n\t}\n\tvto = kmap_atomic(vmf->cow_page);\n\tcopy_user_page(vto, kaddr, vmf->address, vmf->cow_page);\n\tkunmap_atomic(vto);\n\tdax_read_unlock(id);\n\treturn 0;\n}\n\n \nstatic bool dax_fault_is_synchronous(const struct iomap_iter *iter,\n\t\tstruct vm_area_struct *vma)\n{\n\treturn (iter->flags & IOMAP_WRITE) && (vma->vm_flags & VM_SYNC) &&\n\t\t(iter->iomap.flags & IOMAP_F_DIRTY);\n}\n\n \nstatic void *dax_insert_entry(struct xa_state *xas, struct vm_fault *vmf,\n\t\tconst struct iomap_iter *iter, void *entry, pfn_t pfn,\n\t\tunsigned long flags)\n{\n\tstruct address_space *mapping = vmf->vma->vm_file->f_mapping;\n\tvoid *new_entry = dax_make_entry(pfn, flags);\n\tbool write = iter->flags & IOMAP_WRITE;\n\tbool dirty = write && !dax_fault_is_synchronous(iter, vmf->vma);\n\tbool shared = iter->iomap.flags & IOMAP_F_SHARED;\n\n\tif (dirty)\n\t\t__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);\n\n\tif (shared || (dax_is_zero_entry(entry) && !(flags & DAX_ZERO_PAGE))) {\n\t\tunsigned long index = xas->xa_index;\n\t\t \n\t\tif (dax_is_pmd_entry(entry))\n\t\t\tunmap_mapping_pages(mapping, index & ~PG_PMD_COLOUR,\n\t\t\t\t\tPG_PMD_NR, false);\n\t\telse  \n\t\t\tunmap_mapping_pages(mapping, index, 1, false);\n\t}\n\n\txas_reset(xas);\n\txas_lock_irq(xas);\n\tif (shared || dax_is_zero_entry(entry) || dax_is_empty_entry(entry)) {\n\t\tvoid *old;\n\n\t\tdax_disassociate_entry(entry, mapping, false);\n\t\tdax_associate_entry(new_entry, mapping, vmf->vma, vmf->address,\n\t\t\t\tshared);\n\t\t \n\t\told = dax_lock_entry(xas, new_entry);\n\t\tWARN_ON_ONCE(old != xa_mk_value(xa_to_value(entry) |\n\t\t\t\t\tDAX_LOCKED));\n\t\tentry = new_entry;\n\t} else {\n\t\txas_load(xas);\t \n\t}\n\n\tif (dirty)\n\t\txas_set_mark(xas, PAGECACHE_TAG_DIRTY);\n\n\tif (write && shared)\n\t\txas_set_mark(xas, PAGECACHE_TAG_TOWRITE);\n\n\txas_unlock_irq(xas);\n\treturn entry;\n}\n\nstatic int dax_writeback_one(struct xa_state *xas, struct dax_device *dax_dev,\n\t\tstruct address_space *mapping, void *entry)\n{\n\tunsigned long pfn, index, count, end;\n\tlong ret = 0;\n\tstruct vm_area_struct *vma;\n\n\t \n\tif (WARN_ON(!xa_is_value(entry)))\n\t\treturn -EIO;\n\n\tif (unlikely(dax_is_locked(entry))) {\n\t\tvoid *old_entry = entry;\n\n\t\tentry = get_unlocked_entry(xas, 0);\n\n\t\t \n\t\tif (!entry || WARN_ON_ONCE(!xa_is_value(entry)))\n\t\t\tgoto put_unlocked;\n\t\t \n\t\tif (dax_to_pfn(old_entry) != dax_to_pfn(entry))\n\t\t\tgoto put_unlocked;\n\t\tif (WARN_ON_ONCE(dax_is_empty_entry(entry) ||\n\t\t\t\t\tdax_is_zero_entry(entry))) {\n\t\t\tret = -EIO;\n\t\t\tgoto put_unlocked;\n\t\t}\n\n\t\t \n\t\tif (!xas_get_mark(xas, PAGECACHE_TAG_TOWRITE))\n\t\t\tgoto put_unlocked;\n\t}\n\n\t \n\tdax_lock_entry(xas, entry);\n\n\t \n\txas_clear_mark(xas, PAGECACHE_TAG_TOWRITE);\n\txas_unlock_irq(xas);\n\n\t \n\tpfn = dax_to_pfn(entry);\n\tcount = 1UL << dax_entry_order(entry);\n\tindex = xas->xa_index & ~(count - 1);\n\tend = index + count - 1;\n\n\t \n\ti_mmap_lock_read(mapping);\n\tvma_interval_tree_foreach(vma, &mapping->i_mmap, index, end) {\n\t\tpfn_mkclean_range(pfn, count, index, vma);\n\t\tcond_resched();\n\t}\n\ti_mmap_unlock_read(mapping);\n\n\tdax_flush(dax_dev, page_address(pfn_to_page(pfn)), count * PAGE_SIZE);\n\t \n\txas_reset(xas);\n\txas_lock_irq(xas);\n\txas_store(xas, entry);\n\txas_clear_mark(xas, PAGECACHE_TAG_DIRTY);\n\tdax_wake_entry(xas, entry, WAKE_NEXT);\n\n\ttrace_dax_writeback_one(mapping->host, index, count);\n\treturn ret;\n\n put_unlocked:\n\tput_unlocked_entry(xas, entry, WAKE_NEXT);\n\treturn ret;\n}\n\n \nint dax_writeback_mapping_range(struct address_space *mapping,\n\t\tstruct dax_device *dax_dev, struct writeback_control *wbc)\n{\n\tXA_STATE(xas, &mapping->i_pages, wbc->range_start >> PAGE_SHIFT);\n\tstruct inode *inode = mapping->host;\n\tpgoff_t end_index = wbc->range_end >> PAGE_SHIFT;\n\tvoid *entry;\n\tint ret = 0;\n\tunsigned int scanned = 0;\n\n\tif (WARN_ON_ONCE(inode->i_blkbits != PAGE_SHIFT))\n\t\treturn -EIO;\n\n\tif (mapping_empty(mapping) || wbc->sync_mode != WB_SYNC_ALL)\n\t\treturn 0;\n\n\ttrace_dax_writeback_range(inode, xas.xa_index, end_index);\n\n\ttag_pages_for_writeback(mapping, xas.xa_index, end_index);\n\n\txas_lock_irq(&xas);\n\txas_for_each_marked(&xas, entry, end_index, PAGECACHE_TAG_TOWRITE) {\n\t\tret = dax_writeback_one(&xas, dax_dev, mapping, entry);\n\t\tif (ret < 0) {\n\t\t\tmapping_set_error(mapping, ret);\n\t\t\tbreak;\n\t\t}\n\t\tif (++scanned % XA_CHECK_SCHED)\n\t\t\tcontinue;\n\n\t\txas_pause(&xas);\n\t\txas_unlock_irq(&xas);\n\t\tcond_resched();\n\t\txas_lock_irq(&xas);\n\t}\n\txas_unlock_irq(&xas);\n\ttrace_dax_writeback_range_done(inode, xas.xa_index, end_index);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dax_writeback_mapping_range);\n\nstatic int dax_iomap_direct_access(const struct iomap *iomap, loff_t pos,\n\t\tsize_t size, void **kaddr, pfn_t *pfnp)\n{\n\tpgoff_t pgoff = dax_iomap_pgoff(iomap, pos);\n\tint id, rc = 0;\n\tlong length;\n\n\tid = dax_read_lock();\n\tlength = dax_direct_access(iomap->dax_dev, pgoff, PHYS_PFN(size),\n\t\t\t\t   DAX_ACCESS, kaddr, pfnp);\n\tif (length < 0) {\n\t\trc = length;\n\t\tgoto out;\n\t}\n\tif (!pfnp)\n\t\tgoto out_check_addr;\n\trc = -EINVAL;\n\tif (PFN_PHYS(length) < size)\n\t\tgoto out;\n\tif (pfn_t_to_pfn(*pfnp) & (PHYS_PFN(size)-1))\n\t\tgoto out;\n\t \n\tif (length > 1 && !pfn_t_devmap(*pfnp))\n\t\tgoto out;\n\trc = 0;\n\nout_check_addr:\n\tif (!kaddr)\n\t\tgoto out;\n\tif (!*kaddr)\n\t\trc = -EFAULT;\nout:\n\tdax_read_unlock(id);\n\treturn rc;\n}\n\n \nstatic int dax_iomap_copy_around(loff_t pos, uint64_t length, size_t align_size,\n\t\tconst struct iomap *srcmap, void *daddr)\n{\n\tloff_t head_off = pos & (align_size - 1);\n\tsize_t size = ALIGN(head_off + length, align_size);\n\tloff_t end = pos + length;\n\tloff_t pg_end = round_up(end, align_size);\n\t \n\tbool copy_all = head_off == 0 && end == pg_end;\n\t \n\tbool zero_edge = srcmap->flags & IOMAP_F_SHARED ||\n\t\t\t srcmap->type == IOMAP_UNWRITTEN;\n\tvoid *saddr = 0;\n\tint ret = 0;\n\n\tif (!zero_edge) {\n\t\tret = dax_iomap_direct_access(srcmap, pos, size, &saddr, NULL);\n\t\tif (ret)\n\t\t\treturn dax_mem2blk_err(ret);\n\t}\n\n\tif (copy_all) {\n\t\tif (zero_edge)\n\t\t\tmemset(daddr, 0, size);\n\t\telse\n\t\t\tret = copy_mc_to_kernel(daddr, saddr, length);\n\t\tgoto out;\n\t}\n\n\t \n\tif (head_off) {\n\t\tif (zero_edge)\n\t\t\tmemset(daddr, 0, head_off);\n\t\telse {\n\t\t\tret = copy_mc_to_kernel(daddr, saddr, head_off);\n\t\t\tif (ret)\n\t\t\t\treturn -EIO;\n\t\t}\n\t}\n\n\t \n\tif (end < pg_end) {\n\t\tloff_t tail_off = head_off + length;\n\t\tloff_t tail_len = pg_end - end;\n\n\t\tif (zero_edge)\n\t\t\tmemset(daddr + tail_off, 0, tail_len);\n\t\telse {\n\t\t\tret = copy_mc_to_kernel(daddr + tail_off,\n\t\t\t\t\t\tsaddr + tail_off, tail_len);\n\t\t\tif (ret)\n\t\t\t\treturn -EIO;\n\t\t}\n\t}\nout:\n\tif (zero_edge)\n\t\tdax_flush(srcmap->dax_dev, daddr, size);\n\treturn ret ? -EIO : 0;\n}\n\n \nstatic vm_fault_t dax_load_hole(struct xa_state *xas, struct vm_fault *vmf,\n\t\tconst struct iomap_iter *iter, void **entry)\n{\n\tstruct inode *inode = iter->inode;\n\tunsigned long vaddr = vmf->address;\n\tpfn_t pfn = pfn_to_pfn_t(my_zero_pfn(vaddr));\n\tvm_fault_t ret;\n\n\t*entry = dax_insert_entry(xas, vmf, iter, *entry, pfn, DAX_ZERO_PAGE);\n\n\tret = vmf_insert_mixed(vmf->vma, vaddr, pfn);\n\ttrace_dax_load_hole(inode, vmf, ret);\n\treturn ret;\n}\n\n#ifdef CONFIG_FS_DAX_PMD\nstatic vm_fault_t dax_pmd_load_hole(struct xa_state *xas, struct vm_fault *vmf,\n\t\tconst struct iomap_iter *iter, void **entry)\n{\n\tstruct address_space *mapping = vmf->vma->vm_file->f_mapping;\n\tunsigned long pmd_addr = vmf->address & PMD_MASK;\n\tstruct vm_area_struct *vma = vmf->vma;\n\tstruct inode *inode = mapping->host;\n\tpgtable_t pgtable = NULL;\n\tstruct page *zero_page;\n\tspinlock_t *ptl;\n\tpmd_t pmd_entry;\n\tpfn_t pfn;\n\n\tzero_page = mm_get_huge_zero_page(vmf->vma->vm_mm);\n\n\tif (unlikely(!zero_page))\n\t\tgoto fallback;\n\n\tpfn = page_to_pfn_t(zero_page);\n\t*entry = dax_insert_entry(xas, vmf, iter, *entry, pfn,\n\t\t\t\t  DAX_PMD | DAX_ZERO_PAGE);\n\n\tif (arch_needs_pgtable_deposit()) {\n\t\tpgtable = pte_alloc_one(vma->vm_mm);\n\t\tif (!pgtable)\n\t\t\treturn VM_FAULT_OOM;\n\t}\n\n\tptl = pmd_lock(vmf->vma->vm_mm, vmf->pmd);\n\tif (!pmd_none(*(vmf->pmd))) {\n\t\tspin_unlock(ptl);\n\t\tgoto fallback;\n\t}\n\n\tif (pgtable) {\n\t\tpgtable_trans_huge_deposit(vma->vm_mm, vmf->pmd, pgtable);\n\t\tmm_inc_nr_ptes(vma->vm_mm);\n\t}\n\tpmd_entry = mk_pmd(zero_page, vmf->vma->vm_page_prot);\n\tpmd_entry = pmd_mkhuge(pmd_entry);\n\tset_pmd_at(vmf->vma->vm_mm, pmd_addr, vmf->pmd, pmd_entry);\n\tspin_unlock(ptl);\n\ttrace_dax_pmd_load_hole(inode, vmf, zero_page, *entry);\n\treturn VM_FAULT_NOPAGE;\n\nfallback:\n\tif (pgtable)\n\t\tpte_free(vma->vm_mm, pgtable);\n\ttrace_dax_pmd_load_hole_fallback(inode, vmf, zero_page, *entry);\n\treturn VM_FAULT_FALLBACK;\n}\n#else\nstatic vm_fault_t dax_pmd_load_hole(struct xa_state *xas, struct vm_fault *vmf,\n\t\tconst struct iomap_iter *iter, void **entry)\n{\n\treturn VM_FAULT_FALLBACK;\n}\n#endif  \n\nstatic s64 dax_unshare_iter(struct iomap_iter *iter)\n{\n\tstruct iomap *iomap = &iter->iomap;\n\tconst struct iomap *srcmap = iomap_iter_srcmap(iter);\n\tloff_t pos = iter->pos;\n\tloff_t length = iomap_length(iter);\n\tint id = 0;\n\ts64 ret = 0;\n\tvoid *daddr = NULL, *saddr = NULL;\n\n\t \n\tif (!(iomap->flags & IOMAP_F_SHARED))\n\t\treturn length;\n\n\tid = dax_read_lock();\n\tret = dax_iomap_direct_access(iomap, pos, length, &daddr, NULL);\n\tif (ret < 0)\n\t\tgoto out_unlock;\n\n\t \n\tif (srcmap->flags & IOMAP_F_SHARED || srcmap->type == IOMAP_UNWRITTEN) {\n\t\tmemset(daddr, 0, length);\n\t\tdax_flush(iomap->dax_dev, daddr, length);\n\t\tret = length;\n\t\tgoto out_unlock;\n\t}\n\n\tret = dax_iomap_direct_access(srcmap, pos, length, &saddr, NULL);\n\tif (ret < 0)\n\t\tgoto out_unlock;\n\n\tif (copy_mc_to_kernel(daddr, saddr, length) == 0)\n\t\tret = length;\n\telse\n\t\tret = -EIO;\n\nout_unlock:\n\tdax_read_unlock(id);\n\treturn dax_mem2blk_err(ret);\n}\n\nint dax_file_unshare(struct inode *inode, loff_t pos, loff_t len,\n\t\tconst struct iomap_ops *ops)\n{\n\tstruct iomap_iter iter = {\n\t\t.inode\t\t= inode,\n\t\t.pos\t\t= pos,\n\t\t.len\t\t= len,\n\t\t.flags\t\t= IOMAP_WRITE | IOMAP_UNSHARE | IOMAP_DAX,\n\t};\n\tint ret;\n\n\twhile ((ret = iomap_iter(&iter, ops)) > 0)\n\t\titer.processed = dax_unshare_iter(&iter);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dax_file_unshare);\n\nstatic int dax_memzero(struct iomap_iter *iter, loff_t pos, size_t size)\n{\n\tconst struct iomap *iomap = &iter->iomap;\n\tconst struct iomap *srcmap = iomap_iter_srcmap(iter);\n\tunsigned offset = offset_in_page(pos);\n\tpgoff_t pgoff = dax_iomap_pgoff(iomap, pos);\n\tvoid *kaddr;\n\tlong ret;\n\n\tret = dax_direct_access(iomap->dax_dev, pgoff, 1, DAX_ACCESS, &kaddr,\n\t\t\t\tNULL);\n\tif (ret < 0)\n\t\treturn dax_mem2blk_err(ret);\n\n\tmemset(kaddr + offset, 0, size);\n\tif (iomap->flags & IOMAP_F_SHARED)\n\t\tret = dax_iomap_copy_around(pos, size, PAGE_SIZE, srcmap,\n\t\t\t\t\t    kaddr);\n\telse\n\t\tdax_flush(iomap->dax_dev, kaddr + offset, size);\n\treturn ret;\n}\n\nstatic s64 dax_zero_iter(struct iomap_iter *iter, bool *did_zero)\n{\n\tconst struct iomap *iomap = &iter->iomap;\n\tconst struct iomap *srcmap = iomap_iter_srcmap(iter);\n\tloff_t pos = iter->pos;\n\tu64 length = iomap_length(iter);\n\ts64 written = 0;\n\n\t \n\tif (srcmap->type == IOMAP_HOLE || srcmap->type == IOMAP_UNWRITTEN)\n\t\treturn length;\n\n\t \n\tif (iomap->flags & IOMAP_F_SHARED)\n\t\tinvalidate_inode_pages2_range(iter->inode->i_mapping,\n\t\t\t\t\t      pos >> PAGE_SHIFT,\n\t\t\t\t\t      (pos + length - 1) >> PAGE_SHIFT);\n\n\tdo {\n\t\tunsigned offset = offset_in_page(pos);\n\t\tunsigned size = min_t(u64, PAGE_SIZE - offset, length);\n\t\tpgoff_t pgoff = dax_iomap_pgoff(iomap, pos);\n\t\tlong rc;\n\t\tint id;\n\n\t\tid = dax_read_lock();\n\t\tif (IS_ALIGNED(pos, PAGE_SIZE) && size == PAGE_SIZE)\n\t\t\trc = dax_zero_page_range(iomap->dax_dev, pgoff, 1);\n\t\telse\n\t\t\trc = dax_memzero(iter, pos, size);\n\t\tdax_read_unlock(id);\n\n\t\tif (rc < 0)\n\t\t\treturn rc;\n\t\tpos += size;\n\t\tlength -= size;\n\t\twritten += size;\n\t} while (length > 0);\n\n\tif (did_zero)\n\t\t*did_zero = true;\n\treturn written;\n}\n\nint dax_zero_range(struct inode *inode, loff_t pos, loff_t len, bool *did_zero,\n\t\tconst struct iomap_ops *ops)\n{\n\tstruct iomap_iter iter = {\n\t\t.inode\t\t= inode,\n\t\t.pos\t\t= pos,\n\t\t.len\t\t= len,\n\t\t.flags\t\t= IOMAP_DAX | IOMAP_ZERO,\n\t};\n\tint ret;\n\n\twhile ((ret = iomap_iter(&iter, ops)) > 0)\n\t\titer.processed = dax_zero_iter(&iter, did_zero);\n\treturn ret;\n}\nEXPORT_SYMBOL_GPL(dax_zero_range);\n\nint dax_truncate_page(struct inode *inode, loff_t pos, bool *did_zero,\n\t\tconst struct iomap_ops *ops)\n{\n\tunsigned int blocksize = i_blocksize(inode);\n\tunsigned int off = pos & (blocksize - 1);\n\n\t \n\tif (!off)\n\t\treturn 0;\n\treturn dax_zero_range(inode, pos, blocksize - off, did_zero, ops);\n}\nEXPORT_SYMBOL_GPL(dax_truncate_page);\n\nstatic loff_t dax_iomap_iter(const struct iomap_iter *iomi,\n\t\tstruct iov_iter *iter)\n{\n\tconst struct iomap *iomap = &iomi->iomap;\n\tconst struct iomap *srcmap = iomap_iter_srcmap(iomi);\n\tloff_t length = iomap_length(iomi);\n\tloff_t pos = iomi->pos;\n\tstruct dax_device *dax_dev = iomap->dax_dev;\n\tloff_t end = pos + length, done = 0;\n\tbool write = iov_iter_rw(iter) == WRITE;\n\tbool cow = write && iomap->flags & IOMAP_F_SHARED;\n\tssize_t ret = 0;\n\tsize_t xfer;\n\tint id;\n\n\tif (!write) {\n\t\tend = min(end, i_size_read(iomi->inode));\n\t\tif (pos >= end)\n\t\t\treturn 0;\n\n\t\tif (iomap->type == IOMAP_HOLE || iomap->type == IOMAP_UNWRITTEN)\n\t\t\treturn iov_iter_zero(min(length, end - pos), iter);\n\t}\n\n\t \n\tif (WARN_ON_ONCE(iomap->type != IOMAP_MAPPED &&\n\t\t\t!(iomap->flags & IOMAP_F_SHARED)))\n\t\treturn -EIO;\n\n\t \n\tif (iomap->flags & IOMAP_F_NEW || cow) {\n\t\t \n\t\tif (cow)\n\t\t\t__dax_clear_dirty_range(iomi->inode->i_mapping,\n\t\t\t\t\t\tpos >> PAGE_SHIFT,\n\t\t\t\t\t\t(end - 1) >> PAGE_SHIFT);\n\t\tinvalidate_inode_pages2_range(iomi->inode->i_mapping,\n\t\t\t\t\t      pos >> PAGE_SHIFT,\n\t\t\t\t\t      (end - 1) >> PAGE_SHIFT);\n\t}\n\n\tid = dax_read_lock();\n\twhile (pos < end) {\n\t\tunsigned offset = pos & (PAGE_SIZE - 1);\n\t\tconst size_t size = ALIGN(length + offset, PAGE_SIZE);\n\t\tpgoff_t pgoff = dax_iomap_pgoff(iomap, pos);\n\t\tssize_t map_len;\n\t\tbool recovery = false;\n\t\tvoid *kaddr;\n\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -EINTR;\n\t\t\tbreak;\n\t\t}\n\n\t\tmap_len = dax_direct_access(dax_dev, pgoff, PHYS_PFN(size),\n\t\t\t\tDAX_ACCESS, &kaddr, NULL);\n\t\tif (map_len == -EHWPOISON && iov_iter_rw(iter) == WRITE) {\n\t\t\tmap_len = dax_direct_access(dax_dev, pgoff,\n\t\t\t\t\tPHYS_PFN(size), DAX_RECOVERY_WRITE,\n\t\t\t\t\t&kaddr, NULL);\n\t\t\tif (map_len > 0)\n\t\t\t\trecovery = true;\n\t\t}\n\t\tif (map_len < 0) {\n\t\t\tret = dax_mem2blk_err(map_len);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (cow) {\n\t\t\tret = dax_iomap_copy_around(pos, length, PAGE_SIZE,\n\t\t\t\t\t\t    srcmap, kaddr);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tmap_len = PFN_PHYS(map_len);\n\t\tkaddr += offset;\n\t\tmap_len -= offset;\n\t\tif (map_len > end - pos)\n\t\t\tmap_len = end - pos;\n\n\t\tif (recovery)\n\t\t\txfer = dax_recovery_write(dax_dev, pgoff, kaddr,\n\t\t\t\t\tmap_len, iter);\n\t\telse if (write)\n\t\t\txfer = dax_copy_from_iter(dax_dev, pgoff, kaddr,\n\t\t\t\t\tmap_len, iter);\n\t\telse\n\t\t\txfer = dax_copy_to_iter(dax_dev, pgoff, kaddr,\n\t\t\t\t\tmap_len, iter);\n\n\t\tpos += xfer;\n\t\tlength -= xfer;\n\t\tdone += xfer;\n\n\t\tif (xfer == 0)\n\t\t\tret = -EFAULT;\n\t\tif (xfer < map_len)\n\t\t\tbreak;\n\t}\n\tdax_read_unlock(id);\n\n\treturn done ? done : ret;\n}\n\n \nssize_t\ndax_iomap_rw(struct kiocb *iocb, struct iov_iter *iter,\n\t\tconst struct iomap_ops *ops)\n{\n\tstruct iomap_iter iomi = {\n\t\t.inode\t\t= iocb->ki_filp->f_mapping->host,\n\t\t.pos\t\t= iocb->ki_pos,\n\t\t.len\t\t= iov_iter_count(iter),\n\t\t.flags\t\t= IOMAP_DAX,\n\t};\n\tloff_t done = 0;\n\tint ret;\n\n\tif (!iomi.len)\n\t\treturn 0;\n\n\tif (iov_iter_rw(iter) == WRITE) {\n\t\tlockdep_assert_held_write(&iomi.inode->i_rwsem);\n\t\tiomi.flags |= IOMAP_WRITE;\n\t} else {\n\t\tlockdep_assert_held(&iomi.inode->i_rwsem);\n\t}\n\n\tif (iocb->ki_flags & IOCB_NOWAIT)\n\t\tiomi.flags |= IOMAP_NOWAIT;\n\n\twhile ((ret = iomap_iter(&iomi, ops)) > 0)\n\t\tiomi.processed = dax_iomap_iter(&iomi, iter);\n\n\tdone = iomi.pos - iocb->ki_pos;\n\tiocb->ki_pos = iomi.pos;\n\treturn done ? done : ret;\n}\nEXPORT_SYMBOL_GPL(dax_iomap_rw);\n\nstatic vm_fault_t dax_fault_return(int error)\n{\n\tif (error == 0)\n\t\treturn VM_FAULT_NOPAGE;\n\treturn vmf_error(error);\n}\n\n \nstatic vm_fault_t dax_fault_synchronous_pfnp(pfn_t *pfnp, pfn_t pfn)\n{\n\tif (WARN_ON_ONCE(!pfnp))\n\t\treturn VM_FAULT_SIGBUS;\n\t*pfnp = pfn;\n\treturn VM_FAULT_NEEDDSYNC;\n}\n\nstatic vm_fault_t dax_fault_cow_page(struct vm_fault *vmf,\n\t\tconst struct iomap_iter *iter)\n{\n\tvm_fault_t ret;\n\tint error = 0;\n\n\tswitch (iter->iomap.type) {\n\tcase IOMAP_HOLE:\n\tcase IOMAP_UNWRITTEN:\n\t\tclear_user_highpage(vmf->cow_page, vmf->address);\n\t\tbreak;\n\tcase IOMAP_MAPPED:\n\t\terror = copy_cow_page_dax(vmf, iter);\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t\terror = -EIO;\n\t\tbreak;\n\t}\n\n\tif (error)\n\t\treturn dax_fault_return(error);\n\n\t__SetPageUptodate(vmf->cow_page);\n\tret = finish_fault(vmf);\n\tif (!ret)\n\t\treturn VM_FAULT_DONE_COW;\n\treturn ret;\n}\n\n \nstatic vm_fault_t dax_fault_iter(struct vm_fault *vmf,\n\t\tconst struct iomap_iter *iter, pfn_t *pfnp,\n\t\tstruct xa_state *xas, void **entry, bool pmd)\n{\n\tconst struct iomap *iomap = &iter->iomap;\n\tconst struct iomap *srcmap = iomap_iter_srcmap(iter);\n\tsize_t size = pmd ? PMD_SIZE : PAGE_SIZE;\n\tloff_t pos = (loff_t)xas->xa_index << PAGE_SHIFT;\n\tbool write = iter->flags & IOMAP_WRITE;\n\tunsigned long entry_flags = pmd ? DAX_PMD : 0;\n\tint err = 0;\n\tpfn_t pfn;\n\tvoid *kaddr;\n\n\tif (!pmd && vmf->cow_page)\n\t\treturn dax_fault_cow_page(vmf, iter);\n\n\t \n\tif (!write &&\n\t    (iomap->type == IOMAP_UNWRITTEN || iomap->type == IOMAP_HOLE)) {\n\t\tif (!pmd)\n\t\t\treturn dax_load_hole(xas, vmf, iter, entry);\n\t\treturn dax_pmd_load_hole(xas, vmf, iter, entry);\n\t}\n\n\tif (iomap->type != IOMAP_MAPPED && !(iomap->flags & IOMAP_F_SHARED)) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn pmd ? VM_FAULT_FALLBACK : VM_FAULT_SIGBUS;\n\t}\n\n\terr = dax_iomap_direct_access(iomap, pos, size, &kaddr, &pfn);\n\tif (err)\n\t\treturn pmd ? VM_FAULT_FALLBACK : dax_fault_return(err);\n\n\t*entry = dax_insert_entry(xas, vmf, iter, *entry, pfn, entry_flags);\n\n\tif (write && iomap->flags & IOMAP_F_SHARED) {\n\t\terr = dax_iomap_copy_around(pos, size, size, srcmap, kaddr);\n\t\tif (err)\n\t\t\treturn dax_fault_return(err);\n\t}\n\n\tif (dax_fault_is_synchronous(iter, vmf->vma))\n\t\treturn dax_fault_synchronous_pfnp(pfnp, pfn);\n\n\t \n\tif (pmd)\n\t\treturn vmf_insert_pfn_pmd(vmf, pfn, write);\n\n\t \n\tif (write)\n\t\treturn vmf_insert_mixed_mkwrite(vmf->vma, vmf->address, pfn);\n\treturn vmf_insert_mixed(vmf->vma, vmf->address, pfn);\n}\n\nstatic vm_fault_t dax_iomap_pte_fault(struct vm_fault *vmf, pfn_t *pfnp,\n\t\t\t       int *iomap_errp, const struct iomap_ops *ops)\n{\n\tstruct address_space *mapping = vmf->vma->vm_file->f_mapping;\n\tXA_STATE(xas, &mapping->i_pages, vmf->pgoff);\n\tstruct iomap_iter iter = {\n\t\t.inode\t\t= mapping->host,\n\t\t.pos\t\t= (loff_t)vmf->pgoff << PAGE_SHIFT,\n\t\t.len\t\t= PAGE_SIZE,\n\t\t.flags\t\t= IOMAP_DAX | IOMAP_FAULT,\n\t};\n\tvm_fault_t ret = 0;\n\tvoid *entry;\n\tint error;\n\n\ttrace_dax_pte_fault(iter.inode, vmf, ret);\n\t \n\tif (iter.pos >= i_size_read(iter.inode)) {\n\t\tret = VM_FAULT_SIGBUS;\n\t\tgoto out;\n\t}\n\n\tif ((vmf->flags & FAULT_FLAG_WRITE) && !vmf->cow_page)\n\t\titer.flags |= IOMAP_WRITE;\n\n\tentry = grab_mapping_entry(&xas, mapping, 0);\n\tif (xa_is_internal(entry)) {\n\t\tret = xa_to_internal(entry);\n\t\tgoto out;\n\t}\n\n\t \n\tif (pmd_trans_huge(*vmf->pmd) || pmd_devmap(*vmf->pmd)) {\n\t\tret = VM_FAULT_NOPAGE;\n\t\tgoto unlock_entry;\n\t}\n\n\twhile ((error = iomap_iter(&iter, ops)) > 0) {\n\t\tif (WARN_ON_ONCE(iomap_length(&iter) < PAGE_SIZE)) {\n\t\t\titer.processed = -EIO;\t \n\t\t\tcontinue;\n\t\t}\n\n\t\tret = dax_fault_iter(vmf, &iter, pfnp, &xas, &entry, false);\n\t\tif (ret != VM_FAULT_SIGBUS &&\n\t\t    (iter.iomap.flags & IOMAP_F_NEW)) {\n\t\t\tcount_vm_event(PGMAJFAULT);\n\t\t\tcount_memcg_event_mm(vmf->vma->vm_mm, PGMAJFAULT);\n\t\t\tret |= VM_FAULT_MAJOR;\n\t\t}\n\n\t\tif (!(ret & VM_FAULT_ERROR))\n\t\t\titer.processed = PAGE_SIZE;\n\t}\n\n\tif (iomap_errp)\n\t\t*iomap_errp = error;\n\tif (!ret && error)\n\t\tret = dax_fault_return(error);\n\nunlock_entry:\n\tdax_unlock_entry(&xas, entry);\nout:\n\ttrace_dax_pte_fault_done(iter.inode, vmf, ret);\n\treturn ret;\n}\n\n#ifdef CONFIG_FS_DAX_PMD\nstatic bool dax_fault_check_fallback(struct vm_fault *vmf, struct xa_state *xas,\n\t\tpgoff_t max_pgoff)\n{\n\tunsigned long pmd_addr = vmf->address & PMD_MASK;\n\tbool write = vmf->flags & FAULT_FLAG_WRITE;\n\n\t \n\tif ((vmf->pgoff & PG_PMD_COLOUR) !=\n\t    ((vmf->address >> PAGE_SHIFT) & PG_PMD_COLOUR))\n\t\treturn true;\n\n\t \n\tif (write && !(vmf->vma->vm_flags & VM_SHARED))\n\t\treturn true;\n\n\t \n\tif (pmd_addr < vmf->vma->vm_start)\n\t\treturn true;\n\tif ((pmd_addr + PMD_SIZE) > vmf->vma->vm_end)\n\t\treturn true;\n\n\t \n\tif ((xas->xa_index | PG_PMD_COLOUR) >= max_pgoff)\n\t\treturn true;\n\n\treturn false;\n}\n\nstatic vm_fault_t dax_iomap_pmd_fault(struct vm_fault *vmf, pfn_t *pfnp,\n\t\t\t       const struct iomap_ops *ops)\n{\n\tstruct address_space *mapping = vmf->vma->vm_file->f_mapping;\n\tXA_STATE_ORDER(xas, &mapping->i_pages, vmf->pgoff, PMD_ORDER);\n\tstruct iomap_iter iter = {\n\t\t.inode\t\t= mapping->host,\n\t\t.len\t\t= PMD_SIZE,\n\t\t.flags\t\t= IOMAP_DAX | IOMAP_FAULT,\n\t};\n\tvm_fault_t ret = VM_FAULT_FALLBACK;\n\tpgoff_t max_pgoff;\n\tvoid *entry;\n\n\tif (vmf->flags & FAULT_FLAG_WRITE)\n\t\titer.flags |= IOMAP_WRITE;\n\n\t \n\tmax_pgoff = DIV_ROUND_UP(i_size_read(iter.inode), PAGE_SIZE);\n\n\ttrace_dax_pmd_fault(iter.inode, vmf, max_pgoff, 0);\n\n\tif (xas.xa_index >= max_pgoff) {\n\t\tret = VM_FAULT_SIGBUS;\n\t\tgoto out;\n\t}\n\n\tif (dax_fault_check_fallback(vmf, &xas, max_pgoff))\n\t\tgoto fallback;\n\n\t \n\tentry = grab_mapping_entry(&xas, mapping, PMD_ORDER);\n\tif (xa_is_internal(entry)) {\n\t\tret = xa_to_internal(entry);\n\t\tgoto fallback;\n\t}\n\n\t \n\tif (!pmd_none(*vmf->pmd) && !pmd_trans_huge(*vmf->pmd) &&\n\t\t\t!pmd_devmap(*vmf->pmd)) {\n\t\tret = 0;\n\t\tgoto unlock_entry;\n\t}\n\n\titer.pos = (loff_t)xas.xa_index << PAGE_SHIFT;\n\twhile (iomap_iter(&iter, ops) > 0) {\n\t\tif (iomap_length(&iter) < PMD_SIZE)\n\t\t\tcontinue;  \n\n\t\tret = dax_fault_iter(vmf, &iter, pfnp, &xas, &entry, true);\n\t\tif (ret != VM_FAULT_FALLBACK)\n\t\t\titer.processed = PMD_SIZE;\n\t}\n\nunlock_entry:\n\tdax_unlock_entry(&xas, entry);\nfallback:\n\tif (ret == VM_FAULT_FALLBACK) {\n\t\tsplit_huge_pmd(vmf->vma, vmf->pmd, vmf->address);\n\t\tcount_vm_event(THP_FAULT_FALLBACK);\n\t}\nout:\n\ttrace_dax_pmd_fault_done(iter.inode, vmf, max_pgoff, ret);\n\treturn ret;\n}\n#else\nstatic vm_fault_t dax_iomap_pmd_fault(struct vm_fault *vmf, pfn_t *pfnp,\n\t\t\t       const struct iomap_ops *ops)\n{\n\treturn VM_FAULT_FALLBACK;\n}\n#endif  \n\n \nvm_fault_t dax_iomap_fault(struct vm_fault *vmf, unsigned int order,\n\t\t    pfn_t *pfnp, int *iomap_errp, const struct iomap_ops *ops)\n{\n\tif (order == 0)\n\t\treturn dax_iomap_pte_fault(vmf, pfnp, iomap_errp, ops);\n\telse if (order == PMD_ORDER)\n\t\treturn dax_iomap_pmd_fault(vmf, pfnp, ops);\n\telse\n\t\treturn VM_FAULT_FALLBACK;\n}\nEXPORT_SYMBOL_GPL(dax_iomap_fault);\n\n \nstatic vm_fault_t\ndax_insert_pfn_mkwrite(struct vm_fault *vmf, pfn_t pfn, unsigned int order)\n{\n\tstruct address_space *mapping = vmf->vma->vm_file->f_mapping;\n\tXA_STATE_ORDER(xas, &mapping->i_pages, vmf->pgoff, order);\n\tvoid *entry;\n\tvm_fault_t ret;\n\n\txas_lock_irq(&xas);\n\tentry = get_unlocked_entry(&xas, order);\n\t \n\tif (!entry || dax_is_conflict(entry) ||\n\t    (order == 0 && !dax_is_pte_entry(entry))) {\n\t\tput_unlocked_entry(&xas, entry, WAKE_NEXT);\n\t\txas_unlock_irq(&xas);\n\t\ttrace_dax_insert_pfn_mkwrite_no_entry(mapping->host, vmf,\n\t\t\t\t\t\t      VM_FAULT_NOPAGE);\n\t\treturn VM_FAULT_NOPAGE;\n\t}\n\txas_set_mark(&xas, PAGECACHE_TAG_DIRTY);\n\tdax_lock_entry(&xas, entry);\n\txas_unlock_irq(&xas);\n\tif (order == 0)\n\t\tret = vmf_insert_mixed_mkwrite(vmf->vma, vmf->address, pfn);\n#ifdef CONFIG_FS_DAX_PMD\n\telse if (order == PMD_ORDER)\n\t\tret = vmf_insert_pfn_pmd(vmf, pfn, FAULT_FLAG_WRITE);\n#endif\n\telse\n\t\tret = VM_FAULT_FALLBACK;\n\tdax_unlock_entry(&xas, entry);\n\ttrace_dax_insert_pfn_mkwrite(mapping->host, vmf, ret);\n\treturn ret;\n}\n\n \nvm_fault_t dax_finish_sync_fault(struct vm_fault *vmf, unsigned int order,\n\t\tpfn_t pfn)\n{\n\tint err;\n\tloff_t start = ((loff_t)vmf->pgoff) << PAGE_SHIFT;\n\tsize_t len = PAGE_SIZE << order;\n\n\terr = vfs_fsync_range(vmf->vma->vm_file, start, start + len - 1, 1);\n\tif (err)\n\t\treturn VM_FAULT_SIGBUS;\n\treturn dax_insert_pfn_mkwrite(vmf, pfn, order);\n}\nEXPORT_SYMBOL_GPL(dax_finish_sync_fault);\n\nstatic loff_t dax_range_compare_iter(struct iomap_iter *it_src,\n\t\tstruct iomap_iter *it_dest, u64 len, bool *same)\n{\n\tconst struct iomap *smap = &it_src->iomap;\n\tconst struct iomap *dmap = &it_dest->iomap;\n\tloff_t pos1 = it_src->pos, pos2 = it_dest->pos;\n\tvoid *saddr, *daddr;\n\tint id, ret;\n\n\tlen = min(len, min(smap->length, dmap->length));\n\n\tif (smap->type == IOMAP_HOLE && dmap->type == IOMAP_HOLE) {\n\t\t*same = true;\n\t\treturn len;\n\t}\n\n\tif (smap->type == IOMAP_HOLE || dmap->type == IOMAP_HOLE) {\n\t\t*same = false;\n\t\treturn 0;\n\t}\n\n\tid = dax_read_lock();\n\tret = dax_iomap_direct_access(smap, pos1, ALIGN(pos1 + len, PAGE_SIZE),\n\t\t\t\t      &saddr, NULL);\n\tif (ret < 0)\n\t\tgoto out_unlock;\n\n\tret = dax_iomap_direct_access(dmap, pos2, ALIGN(pos2 + len, PAGE_SIZE),\n\t\t\t\t      &daddr, NULL);\n\tif (ret < 0)\n\t\tgoto out_unlock;\n\n\t*same = !memcmp(saddr, daddr, len);\n\tif (!*same)\n\t\tlen = 0;\n\tdax_read_unlock(id);\n\treturn len;\n\nout_unlock:\n\tdax_read_unlock(id);\n\treturn -EIO;\n}\n\nint dax_dedupe_file_range_compare(struct inode *src, loff_t srcoff,\n\t\tstruct inode *dst, loff_t dstoff, loff_t len, bool *same,\n\t\tconst struct iomap_ops *ops)\n{\n\tstruct iomap_iter src_iter = {\n\t\t.inode\t\t= src,\n\t\t.pos\t\t= srcoff,\n\t\t.len\t\t= len,\n\t\t.flags\t\t= IOMAP_DAX,\n\t};\n\tstruct iomap_iter dst_iter = {\n\t\t.inode\t\t= dst,\n\t\t.pos\t\t= dstoff,\n\t\t.len\t\t= len,\n\t\t.flags\t\t= IOMAP_DAX,\n\t};\n\tint ret, compared = 0;\n\n\twhile ((ret = iomap_iter(&src_iter, ops)) > 0 &&\n\t       (ret = iomap_iter(&dst_iter, ops)) > 0) {\n\t\tcompared = dax_range_compare_iter(&src_iter, &dst_iter,\n\t\t\t\tmin(src_iter.len, dst_iter.len), same);\n\t\tif (compared < 0)\n\t\t\treturn ret;\n\t\tsrc_iter.processed = dst_iter.processed = compared;\n\t}\n\treturn ret;\n}\n\nint dax_remap_file_range_prep(struct file *file_in, loff_t pos_in,\n\t\t\t      struct file *file_out, loff_t pos_out,\n\t\t\t      loff_t *len, unsigned int remap_flags,\n\t\t\t      const struct iomap_ops *ops)\n{\n\treturn __generic_remap_file_range_prep(file_in, pos_in, file_out,\n\t\t\t\t\t       pos_out, len, remap_flags, ops);\n}\nEXPORT_SYMBOL_GPL(dax_remap_file_range_prep);\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}