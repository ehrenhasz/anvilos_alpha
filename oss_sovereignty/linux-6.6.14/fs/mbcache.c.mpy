{
  "module_name": "mbcache.c",
  "hash_id": "75f943203d293470e515d185c81fea012c6407372f0f359c041cbed8d6859d56",
  "original_prompt": "Ingested from linux-6.6.14/fs/mbcache.c",
  "human_readable_source": "\n#include <linux/spinlock.h>\n#include <linux/slab.h>\n#include <linux/list.h>\n#include <linux/list_bl.h>\n#include <linux/module.h>\n#include <linux/sched.h>\n#include <linux/workqueue.h>\n#include <linux/mbcache.h>\n\n \n\nstruct mb_cache {\n\t \n\tstruct hlist_bl_head\t*c_hash;\n\t \n\tint\t\t\tc_bucket_bits;\n\t \n\tunsigned long\t\tc_max_entries;\n\t \n\tspinlock_t\t\tc_list_lock;\n\tstruct list_head\tc_list;\n\t \n\tunsigned long\t\tc_entry_count;\n\tstruct shrinker\t\tc_shrink;\n\t \n\tstruct work_struct\tc_shrink_work;\n};\n\nstatic struct kmem_cache *mb_entry_cache;\n\nstatic unsigned long mb_cache_shrink(struct mb_cache *cache,\n\t\t\t\t     unsigned long nr_to_scan);\n\nstatic inline struct hlist_bl_head *mb_cache_entry_head(struct mb_cache *cache,\n\t\t\t\t\t\t\tu32 key)\n{\n\treturn &cache->c_hash[hash_32(key, cache->c_bucket_bits)];\n}\n\n \n#define SYNC_SHRINK_BATCH 64\n\n \nint mb_cache_entry_create(struct mb_cache *cache, gfp_t mask, u32 key,\n\t\t\t  u64 value, bool reusable)\n{\n\tstruct mb_cache_entry *entry, *dup;\n\tstruct hlist_bl_node *dup_node;\n\tstruct hlist_bl_head *head;\n\n\t \n\tif (cache->c_entry_count >= cache->c_max_entries)\n\t\tschedule_work(&cache->c_shrink_work);\n\t \n\tif (cache->c_entry_count >= 2*cache->c_max_entries)\n\t\tmb_cache_shrink(cache, SYNC_SHRINK_BATCH);\n\n\tentry = kmem_cache_alloc(mb_entry_cache, mask);\n\tif (!entry)\n\t\treturn -ENOMEM;\n\n\tINIT_LIST_HEAD(&entry->e_list);\n\t \n\tatomic_set(&entry->e_refcnt, 2);\n\tentry->e_key = key;\n\tentry->e_value = value;\n\tentry->e_flags = 0;\n\tif (reusable)\n\t\tset_bit(MBE_REUSABLE_B, &entry->e_flags);\n\thead = mb_cache_entry_head(cache, key);\n\thlist_bl_lock(head);\n\thlist_bl_for_each_entry(dup, dup_node, head, e_hash_list) {\n\t\tif (dup->e_key == key && dup->e_value == value) {\n\t\t\thlist_bl_unlock(head);\n\t\t\tkmem_cache_free(mb_entry_cache, entry);\n\t\t\treturn -EBUSY;\n\t\t}\n\t}\n\thlist_bl_add_head(&entry->e_hash_list, head);\n\thlist_bl_unlock(head);\n\tspin_lock(&cache->c_list_lock);\n\tlist_add_tail(&entry->e_list, &cache->c_list);\n\tcache->c_entry_count++;\n\tspin_unlock(&cache->c_list_lock);\n\tmb_cache_entry_put(cache, entry);\n\n\treturn 0;\n}\nEXPORT_SYMBOL(mb_cache_entry_create);\n\nvoid __mb_cache_entry_free(struct mb_cache *cache, struct mb_cache_entry *entry)\n{\n\tstruct hlist_bl_head *head;\n\n\thead = mb_cache_entry_head(cache, entry->e_key);\n\thlist_bl_lock(head);\n\thlist_bl_del(&entry->e_hash_list);\n\thlist_bl_unlock(head);\n\tkmem_cache_free(mb_entry_cache, entry);\n}\nEXPORT_SYMBOL(__mb_cache_entry_free);\n\n \nvoid mb_cache_entry_wait_unused(struct mb_cache_entry *entry)\n{\n\twait_var_event(&entry->e_refcnt, atomic_read(&entry->e_refcnt) <= 2);\n}\nEXPORT_SYMBOL(mb_cache_entry_wait_unused);\n\nstatic struct mb_cache_entry *__entry_find(struct mb_cache *cache,\n\t\t\t\t\t   struct mb_cache_entry *entry,\n\t\t\t\t\t   u32 key)\n{\n\tstruct mb_cache_entry *old_entry = entry;\n\tstruct hlist_bl_node *node;\n\tstruct hlist_bl_head *head;\n\n\thead = mb_cache_entry_head(cache, key);\n\thlist_bl_lock(head);\n\tif (entry && !hlist_bl_unhashed(&entry->e_hash_list))\n\t\tnode = entry->e_hash_list.next;\n\telse\n\t\tnode = hlist_bl_first(head);\n\twhile (node) {\n\t\tentry = hlist_bl_entry(node, struct mb_cache_entry,\n\t\t\t\t       e_hash_list);\n\t\tif (entry->e_key == key &&\n\t\t    test_bit(MBE_REUSABLE_B, &entry->e_flags) &&\n\t\t    atomic_inc_not_zero(&entry->e_refcnt))\n\t\t\tgoto out;\n\t\tnode = node->next;\n\t}\n\tentry = NULL;\nout:\n\thlist_bl_unlock(head);\n\tif (old_entry)\n\t\tmb_cache_entry_put(cache, old_entry);\n\n\treturn entry;\n}\n\n \nstruct mb_cache_entry *mb_cache_entry_find_first(struct mb_cache *cache,\n\t\t\t\t\t\t u32 key)\n{\n\treturn __entry_find(cache, NULL, key);\n}\nEXPORT_SYMBOL(mb_cache_entry_find_first);\n\n \nstruct mb_cache_entry *mb_cache_entry_find_next(struct mb_cache *cache,\n\t\t\t\t\t\tstruct mb_cache_entry *entry)\n{\n\treturn __entry_find(cache, entry, entry->e_key);\n}\nEXPORT_SYMBOL(mb_cache_entry_find_next);\n\n \nstruct mb_cache_entry *mb_cache_entry_get(struct mb_cache *cache, u32 key,\n\t\t\t\t\t  u64 value)\n{\n\tstruct hlist_bl_node *node;\n\tstruct hlist_bl_head *head;\n\tstruct mb_cache_entry *entry;\n\n\thead = mb_cache_entry_head(cache, key);\n\thlist_bl_lock(head);\n\thlist_bl_for_each_entry(entry, node, head, e_hash_list) {\n\t\tif (entry->e_key == key && entry->e_value == value &&\n\t\t    atomic_inc_not_zero(&entry->e_refcnt))\n\t\t\tgoto out;\n\t}\n\tentry = NULL;\nout:\n\thlist_bl_unlock(head);\n\treturn entry;\n}\nEXPORT_SYMBOL(mb_cache_entry_get);\n\n \nstruct mb_cache_entry *mb_cache_entry_delete_or_get(struct mb_cache *cache,\n\t\t\t\t\t\t    u32 key, u64 value)\n{\n\tstruct mb_cache_entry *entry;\n\n\tentry = mb_cache_entry_get(cache, key, value);\n\tif (!entry)\n\t\treturn NULL;\n\n\t \n\tif (atomic_cmpxchg(&entry->e_refcnt, 2, 0) != 2)\n\t\treturn entry;\n\n\tspin_lock(&cache->c_list_lock);\n\tif (!list_empty(&entry->e_list))\n\t\tlist_del_init(&entry->e_list);\n\tcache->c_entry_count--;\n\tspin_unlock(&cache->c_list_lock);\n\t__mb_cache_entry_free(cache, entry);\n\treturn NULL;\n}\nEXPORT_SYMBOL(mb_cache_entry_delete_or_get);\n\n \nvoid mb_cache_entry_touch(struct mb_cache *cache,\n\t\t\t  struct mb_cache_entry *entry)\n{\n\tset_bit(MBE_REFERENCED_B, &entry->e_flags);\n}\nEXPORT_SYMBOL(mb_cache_entry_touch);\n\nstatic unsigned long mb_cache_count(struct shrinker *shrink,\n\t\t\t\t    struct shrink_control *sc)\n{\n\tstruct mb_cache *cache = container_of(shrink, struct mb_cache,\n\t\t\t\t\t      c_shrink);\n\n\treturn cache->c_entry_count;\n}\n\n \nstatic unsigned long mb_cache_shrink(struct mb_cache *cache,\n\t\t\t\t     unsigned long nr_to_scan)\n{\n\tstruct mb_cache_entry *entry;\n\tunsigned long shrunk = 0;\n\n\tspin_lock(&cache->c_list_lock);\n\twhile (nr_to_scan-- && !list_empty(&cache->c_list)) {\n\t\tentry = list_first_entry(&cache->c_list,\n\t\t\t\t\t struct mb_cache_entry, e_list);\n\t\t \n\t\tif (test_bit(MBE_REFERENCED_B, &entry->e_flags) ||\n\t\t    atomic_cmpxchg(&entry->e_refcnt, 1, 0) != 1) {\n\t\t\tclear_bit(MBE_REFERENCED_B, &entry->e_flags);\n\t\t\tlist_move_tail(&entry->e_list, &cache->c_list);\n\t\t\tcontinue;\n\t\t}\n\t\tlist_del_init(&entry->e_list);\n\t\tcache->c_entry_count--;\n\t\tspin_unlock(&cache->c_list_lock);\n\t\t__mb_cache_entry_free(cache, entry);\n\t\tshrunk++;\n\t\tcond_resched();\n\t\tspin_lock(&cache->c_list_lock);\n\t}\n\tspin_unlock(&cache->c_list_lock);\n\n\treturn shrunk;\n}\n\nstatic unsigned long mb_cache_scan(struct shrinker *shrink,\n\t\t\t\t   struct shrink_control *sc)\n{\n\tstruct mb_cache *cache = container_of(shrink, struct mb_cache,\n\t\t\t\t\t      c_shrink);\n\treturn mb_cache_shrink(cache, sc->nr_to_scan);\n}\n\n \n#define SHRINK_DIVISOR 16\n\nstatic void mb_cache_shrink_worker(struct work_struct *work)\n{\n\tstruct mb_cache *cache = container_of(work, struct mb_cache,\n\t\t\t\t\t      c_shrink_work);\n\tmb_cache_shrink(cache, cache->c_max_entries / SHRINK_DIVISOR);\n}\n\n \nstruct mb_cache *mb_cache_create(int bucket_bits)\n{\n\tstruct mb_cache *cache;\n\tunsigned long bucket_count = 1UL << bucket_bits;\n\tunsigned long i;\n\n\tcache = kzalloc(sizeof(struct mb_cache), GFP_KERNEL);\n\tif (!cache)\n\t\tgoto err_out;\n\tcache->c_bucket_bits = bucket_bits;\n\tcache->c_max_entries = bucket_count << 4;\n\tINIT_LIST_HEAD(&cache->c_list);\n\tspin_lock_init(&cache->c_list_lock);\n\tcache->c_hash = kmalloc_array(bucket_count,\n\t\t\t\t      sizeof(struct hlist_bl_head),\n\t\t\t\t      GFP_KERNEL);\n\tif (!cache->c_hash) {\n\t\tkfree(cache);\n\t\tgoto err_out;\n\t}\n\tfor (i = 0; i < bucket_count; i++)\n\t\tINIT_HLIST_BL_HEAD(&cache->c_hash[i]);\n\n\tcache->c_shrink.count_objects = mb_cache_count;\n\tcache->c_shrink.scan_objects = mb_cache_scan;\n\tcache->c_shrink.seeks = DEFAULT_SEEKS;\n\tif (register_shrinker(&cache->c_shrink, \"mbcache-shrinker\")) {\n\t\tkfree(cache->c_hash);\n\t\tkfree(cache);\n\t\tgoto err_out;\n\t}\n\n\tINIT_WORK(&cache->c_shrink_work, mb_cache_shrink_worker);\n\n\treturn cache;\n\nerr_out:\n\treturn NULL;\n}\nEXPORT_SYMBOL(mb_cache_create);\n\n \nvoid mb_cache_destroy(struct mb_cache *cache)\n{\n\tstruct mb_cache_entry *entry, *next;\n\n\tunregister_shrinker(&cache->c_shrink);\n\n\t \n\tlist_for_each_entry_safe(entry, next, &cache->c_list, e_list) {\n\t\tlist_del(&entry->e_list);\n\t\tWARN_ON(atomic_read(&entry->e_refcnt) != 1);\n\t\tmb_cache_entry_put(cache, entry);\n\t}\n\tkfree(cache->c_hash);\n\tkfree(cache);\n}\nEXPORT_SYMBOL(mb_cache_destroy);\n\nstatic int __init mbcache_init(void)\n{\n\tmb_entry_cache = kmem_cache_create(\"mbcache\",\n\t\t\t\tsizeof(struct mb_cache_entry), 0,\n\t\t\t\tSLAB_RECLAIM_ACCOUNT|SLAB_MEM_SPREAD, NULL);\n\tif (!mb_entry_cache)\n\t\treturn -ENOMEM;\n\treturn 0;\n}\n\nstatic void __exit mbcache_exit(void)\n{\n\tkmem_cache_destroy(mb_entry_cache);\n}\n\nmodule_init(mbcache_init)\nmodule_exit(mbcache_exit)\n\nMODULE_AUTHOR(\"Jan Kara <jack@suse.cz>\");\nMODULE_DESCRIPTION(\"Meta block cache (for extended attributes)\");\nMODULE_LICENSE(\"GPL\");\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}