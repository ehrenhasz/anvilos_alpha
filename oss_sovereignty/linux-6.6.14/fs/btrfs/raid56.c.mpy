{
  "module_name": "raid56.c",
  "hash_id": "f650740a7a9865e13bfa9424381c939578ec8a75ed439339c863e28e4cea7947",
  "original_prompt": "Ingested from linux-6.6.14/fs/btrfs/raid56.c",
  "human_readable_source": "\n \n\n#include <linux/sched.h>\n#include <linux/bio.h>\n#include <linux/slab.h>\n#include <linux/blkdev.h>\n#include <linux/raid/pq.h>\n#include <linux/hash.h>\n#include <linux/list_sort.h>\n#include <linux/raid/xor.h>\n#include <linux/mm.h>\n#include \"messages.h\"\n#include \"misc.h\"\n#include \"ctree.h\"\n#include \"disk-io.h\"\n#include \"volumes.h\"\n#include \"raid56.h\"\n#include \"async-thread.h\"\n#include \"file-item.h\"\n#include \"btrfs_inode.h\"\n\n \n#define RBIO_RMW_LOCKED_BIT\t1\n\n \n#define RBIO_CACHE_BIT\t\t2\n\n \n#define RBIO_CACHE_READY_BIT\t3\n\n#define RBIO_CACHE_SIZE 1024\n\n#define BTRFS_STRIPE_HASH_TABLE_BITS\t\t\t\t11\n\n \nstruct btrfs_stripe_hash {\n\tstruct list_head hash_list;\n\tspinlock_t lock;\n};\n\n \nstruct btrfs_stripe_hash_table {\n\tstruct list_head stripe_cache;\n\tspinlock_t cache_lock;\n\tint cache_size;\n\tstruct btrfs_stripe_hash table[];\n};\n\n \nstruct sector_ptr {\n\tstruct page *page;\n\tunsigned int pgoff:24;\n\tunsigned int uptodate:8;\n};\n\nstatic void rmw_rbio_work(struct work_struct *work);\nstatic void rmw_rbio_work_locked(struct work_struct *work);\nstatic void index_rbio_pages(struct btrfs_raid_bio *rbio);\nstatic int alloc_rbio_pages(struct btrfs_raid_bio *rbio);\n\nstatic int finish_parity_scrub(struct btrfs_raid_bio *rbio);\nstatic void scrub_rbio_work_locked(struct work_struct *work);\n\nstatic void free_raid_bio_pointers(struct btrfs_raid_bio *rbio)\n{\n\tbitmap_free(rbio->error_bitmap);\n\tkfree(rbio->stripe_pages);\n\tkfree(rbio->bio_sectors);\n\tkfree(rbio->stripe_sectors);\n\tkfree(rbio->finish_pointers);\n}\n\nstatic void free_raid_bio(struct btrfs_raid_bio *rbio)\n{\n\tint i;\n\n\tif (!refcount_dec_and_test(&rbio->refs))\n\t\treturn;\n\n\tWARN_ON(!list_empty(&rbio->stripe_cache));\n\tWARN_ON(!list_empty(&rbio->hash_list));\n\tWARN_ON(!bio_list_empty(&rbio->bio_list));\n\n\tfor (i = 0; i < rbio->nr_pages; i++) {\n\t\tif (rbio->stripe_pages[i]) {\n\t\t\t__free_page(rbio->stripe_pages[i]);\n\t\t\trbio->stripe_pages[i] = NULL;\n\t\t}\n\t}\n\n\tbtrfs_put_bioc(rbio->bioc);\n\tfree_raid_bio_pointers(rbio);\n\tkfree(rbio);\n}\n\nstatic void start_async_work(struct btrfs_raid_bio *rbio, work_func_t work_func)\n{\n\tINIT_WORK(&rbio->work, work_func);\n\tqueue_work(rbio->bioc->fs_info->rmw_workers, &rbio->work);\n}\n\n \nint btrfs_alloc_stripe_hash_table(struct btrfs_fs_info *info)\n{\n\tstruct btrfs_stripe_hash_table *table;\n\tstruct btrfs_stripe_hash_table *x;\n\tstruct btrfs_stripe_hash *cur;\n\tstruct btrfs_stripe_hash *h;\n\tint num_entries = 1 << BTRFS_STRIPE_HASH_TABLE_BITS;\n\tint i;\n\n\tif (info->stripe_hash_table)\n\t\treturn 0;\n\n\t \n\ttable = kvzalloc(struct_size(table, table, num_entries), GFP_KERNEL);\n\tif (!table)\n\t\treturn -ENOMEM;\n\n\tspin_lock_init(&table->cache_lock);\n\tINIT_LIST_HEAD(&table->stripe_cache);\n\n\th = table->table;\n\n\tfor (i = 0; i < num_entries; i++) {\n\t\tcur = h + i;\n\t\tINIT_LIST_HEAD(&cur->hash_list);\n\t\tspin_lock_init(&cur->lock);\n\t}\n\n\tx = cmpxchg(&info->stripe_hash_table, NULL, table);\n\tkvfree(x);\n\treturn 0;\n}\n\n \nstatic void cache_rbio_pages(struct btrfs_raid_bio *rbio)\n{\n\tint i;\n\tint ret;\n\n\tret = alloc_rbio_pages(rbio);\n\tif (ret)\n\t\treturn;\n\n\tfor (i = 0; i < rbio->nr_sectors; i++) {\n\t\t \n\t\tif (!rbio->bio_sectors[i].page) {\n\t\t\t \n\t\t\tif (i < rbio->nr_data * rbio->stripe_nsectors)\n\t\t\t\tASSERT(rbio->stripe_sectors[i].uptodate);\n\t\t\tcontinue;\n\t\t}\n\n\t\tASSERT(rbio->stripe_sectors[i].page);\n\t\tmemcpy_page(rbio->stripe_sectors[i].page,\n\t\t\t    rbio->stripe_sectors[i].pgoff,\n\t\t\t    rbio->bio_sectors[i].page,\n\t\t\t    rbio->bio_sectors[i].pgoff,\n\t\t\t    rbio->bioc->fs_info->sectorsize);\n\t\trbio->stripe_sectors[i].uptodate = 1;\n\t}\n\tset_bit(RBIO_CACHE_READY_BIT, &rbio->flags);\n}\n\n \nstatic int rbio_bucket(struct btrfs_raid_bio *rbio)\n{\n\tu64 num = rbio->bioc->full_stripe_logical;\n\n\t \n\treturn hash_64(num >> 16, BTRFS_STRIPE_HASH_TABLE_BITS);\n}\n\nstatic bool full_page_sectors_uptodate(struct btrfs_raid_bio *rbio,\n\t\t\t\t       unsigned int page_nr)\n{\n\tconst u32 sectorsize = rbio->bioc->fs_info->sectorsize;\n\tconst u32 sectors_per_page = PAGE_SIZE / sectorsize;\n\tint i;\n\n\tASSERT(page_nr < rbio->nr_pages);\n\n\tfor (i = sectors_per_page * page_nr;\n\t     i < sectors_per_page * page_nr + sectors_per_page;\n\t     i++) {\n\t\tif (!rbio->stripe_sectors[i].uptodate)\n\t\t\treturn false;\n\t}\n\treturn true;\n}\n\n \nstatic void index_stripe_sectors(struct btrfs_raid_bio *rbio)\n{\n\tconst u32 sectorsize = rbio->bioc->fs_info->sectorsize;\n\tu32 offset;\n\tint i;\n\n\tfor (i = 0, offset = 0; i < rbio->nr_sectors; i++, offset += sectorsize) {\n\t\tint page_index = offset >> PAGE_SHIFT;\n\n\t\tASSERT(page_index < rbio->nr_pages);\n\t\trbio->stripe_sectors[i].page = rbio->stripe_pages[page_index];\n\t\trbio->stripe_sectors[i].pgoff = offset_in_page(offset);\n\t}\n}\n\nstatic void steal_rbio_page(struct btrfs_raid_bio *src,\n\t\t\t    struct btrfs_raid_bio *dest, int page_nr)\n{\n\tconst u32 sectorsize = src->bioc->fs_info->sectorsize;\n\tconst u32 sectors_per_page = PAGE_SIZE / sectorsize;\n\tint i;\n\n\tif (dest->stripe_pages[page_nr])\n\t\t__free_page(dest->stripe_pages[page_nr]);\n\tdest->stripe_pages[page_nr] = src->stripe_pages[page_nr];\n\tsrc->stripe_pages[page_nr] = NULL;\n\n\t \n\tfor (i = sectors_per_page * page_nr;\n\t     i < sectors_per_page * page_nr + sectors_per_page; i++)\n\t\tdest->stripe_sectors[i].uptodate = true;\n}\n\nstatic bool is_data_stripe_page(struct btrfs_raid_bio *rbio, int page_nr)\n{\n\tconst int sector_nr = (page_nr << PAGE_SHIFT) >>\n\t\t\t      rbio->bioc->fs_info->sectorsize_bits;\n\n\t \n\treturn (sector_nr < rbio->nr_data * rbio->stripe_nsectors);\n}\n\n \nstatic void steal_rbio(struct btrfs_raid_bio *src, struct btrfs_raid_bio *dest)\n{\n\tint i;\n\n\tif (!test_bit(RBIO_CACHE_READY_BIT, &src->flags))\n\t\treturn;\n\n\tfor (i = 0; i < dest->nr_pages; i++) {\n\t\tstruct page *p = src->stripe_pages[i];\n\n\t\t \n\t\tif (!is_data_stripe_page(src, i))\n\t\t\tcontinue;\n\n\t\t \n\t\tASSERT(p);\n\t\tASSERT(full_page_sectors_uptodate(src, i));\n\t\tsteal_rbio_page(src, dest, i);\n\t}\n\tindex_stripe_sectors(dest);\n\tindex_stripe_sectors(src);\n}\n\n \nstatic void merge_rbio(struct btrfs_raid_bio *dest,\n\t\t       struct btrfs_raid_bio *victim)\n{\n\tbio_list_merge(&dest->bio_list, &victim->bio_list);\n\tdest->bio_list_bytes += victim->bio_list_bytes;\n\t \n\tbitmap_or(&dest->dbitmap, &victim->dbitmap, &dest->dbitmap,\n\t\t  dest->stripe_nsectors);\n\tbio_list_init(&victim->bio_list);\n}\n\n \nstatic void __remove_rbio_from_cache(struct btrfs_raid_bio *rbio)\n{\n\tint bucket = rbio_bucket(rbio);\n\tstruct btrfs_stripe_hash_table *table;\n\tstruct btrfs_stripe_hash *h;\n\tint freeit = 0;\n\n\t \n\tif (!test_bit(RBIO_CACHE_BIT, &rbio->flags))\n\t\treturn;\n\n\ttable = rbio->bioc->fs_info->stripe_hash_table;\n\th = table->table + bucket;\n\n\t \n\tspin_lock(&h->lock);\n\n\t \n\tspin_lock(&rbio->bio_list_lock);\n\n\tif (test_and_clear_bit(RBIO_CACHE_BIT, &rbio->flags)) {\n\t\tlist_del_init(&rbio->stripe_cache);\n\t\ttable->cache_size -= 1;\n\t\tfreeit = 1;\n\n\t\t \n\t\tif (bio_list_empty(&rbio->bio_list)) {\n\t\t\tif (!list_empty(&rbio->hash_list)) {\n\t\t\t\tlist_del_init(&rbio->hash_list);\n\t\t\t\trefcount_dec(&rbio->refs);\n\t\t\t\tBUG_ON(!list_empty(&rbio->plug_list));\n\t\t\t}\n\t\t}\n\t}\n\n\tspin_unlock(&rbio->bio_list_lock);\n\tspin_unlock(&h->lock);\n\n\tif (freeit)\n\t\tfree_raid_bio(rbio);\n}\n\n \nstatic void remove_rbio_from_cache(struct btrfs_raid_bio *rbio)\n{\n\tstruct btrfs_stripe_hash_table *table;\n\n\tif (!test_bit(RBIO_CACHE_BIT, &rbio->flags))\n\t\treturn;\n\n\ttable = rbio->bioc->fs_info->stripe_hash_table;\n\n\tspin_lock(&table->cache_lock);\n\t__remove_rbio_from_cache(rbio);\n\tspin_unlock(&table->cache_lock);\n}\n\n \nstatic void btrfs_clear_rbio_cache(struct btrfs_fs_info *info)\n{\n\tstruct btrfs_stripe_hash_table *table;\n\tstruct btrfs_raid_bio *rbio;\n\n\ttable = info->stripe_hash_table;\n\n\tspin_lock(&table->cache_lock);\n\twhile (!list_empty(&table->stripe_cache)) {\n\t\trbio = list_entry(table->stripe_cache.next,\n\t\t\t\t  struct btrfs_raid_bio,\n\t\t\t\t  stripe_cache);\n\t\t__remove_rbio_from_cache(rbio);\n\t}\n\tspin_unlock(&table->cache_lock);\n}\n\n \nvoid btrfs_free_stripe_hash_table(struct btrfs_fs_info *info)\n{\n\tif (!info->stripe_hash_table)\n\t\treturn;\n\tbtrfs_clear_rbio_cache(info);\n\tkvfree(info->stripe_hash_table);\n\tinfo->stripe_hash_table = NULL;\n}\n\n \nstatic void cache_rbio(struct btrfs_raid_bio *rbio)\n{\n\tstruct btrfs_stripe_hash_table *table;\n\n\tif (!test_bit(RBIO_CACHE_READY_BIT, &rbio->flags))\n\t\treturn;\n\n\ttable = rbio->bioc->fs_info->stripe_hash_table;\n\n\tspin_lock(&table->cache_lock);\n\tspin_lock(&rbio->bio_list_lock);\n\n\t \n\tif (!test_and_set_bit(RBIO_CACHE_BIT, &rbio->flags))\n\t\trefcount_inc(&rbio->refs);\n\n\tif (!list_empty(&rbio->stripe_cache)){\n\t\tlist_move(&rbio->stripe_cache, &table->stripe_cache);\n\t} else {\n\t\tlist_add(&rbio->stripe_cache, &table->stripe_cache);\n\t\ttable->cache_size += 1;\n\t}\n\n\tspin_unlock(&rbio->bio_list_lock);\n\n\tif (table->cache_size > RBIO_CACHE_SIZE) {\n\t\tstruct btrfs_raid_bio *found;\n\n\t\tfound = list_entry(table->stripe_cache.prev,\n\t\t\t\t  struct btrfs_raid_bio,\n\t\t\t\t  stripe_cache);\n\n\t\tif (found != rbio)\n\t\t\t__remove_rbio_from_cache(found);\n\t}\n\n\tspin_unlock(&table->cache_lock);\n}\n\n \nstatic void run_xor(void **pages, int src_cnt, ssize_t len)\n{\n\tint src_off = 0;\n\tint xor_src_cnt = 0;\n\tvoid *dest = pages[src_cnt];\n\n\twhile(src_cnt > 0) {\n\t\txor_src_cnt = min(src_cnt, MAX_XOR_BLOCKS);\n\t\txor_blocks(xor_src_cnt, len, dest, pages + src_off);\n\n\t\tsrc_cnt -= xor_src_cnt;\n\t\tsrc_off += xor_src_cnt;\n\t}\n}\n\n \nstatic int rbio_is_full(struct btrfs_raid_bio *rbio)\n{\n\tunsigned long size = rbio->bio_list_bytes;\n\tint ret = 1;\n\n\tspin_lock(&rbio->bio_list_lock);\n\tif (size != rbio->nr_data * BTRFS_STRIPE_LEN)\n\t\tret = 0;\n\tBUG_ON(size > rbio->nr_data * BTRFS_STRIPE_LEN);\n\tspin_unlock(&rbio->bio_list_lock);\n\n\treturn ret;\n}\n\n \nstatic int rbio_can_merge(struct btrfs_raid_bio *last,\n\t\t\t  struct btrfs_raid_bio *cur)\n{\n\tif (test_bit(RBIO_RMW_LOCKED_BIT, &last->flags) ||\n\t    test_bit(RBIO_RMW_LOCKED_BIT, &cur->flags))\n\t\treturn 0;\n\n\t \n\tif (test_bit(RBIO_CACHE_BIT, &last->flags) ||\n\t    test_bit(RBIO_CACHE_BIT, &cur->flags))\n\t\treturn 0;\n\n\tif (last->bioc->full_stripe_logical != cur->bioc->full_stripe_logical)\n\t\treturn 0;\n\n\t \n\tif (last->operation != cur->operation)\n\t\treturn 0;\n\t \n\tif (last->operation == BTRFS_RBIO_PARITY_SCRUB)\n\t\treturn 0;\n\n\tif (last->operation == BTRFS_RBIO_READ_REBUILD)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nstatic unsigned int rbio_stripe_sector_index(const struct btrfs_raid_bio *rbio,\n\t\t\t\t\t     unsigned int stripe_nr,\n\t\t\t\t\t     unsigned int sector_nr)\n{\n\tASSERT(stripe_nr < rbio->real_stripes);\n\tASSERT(sector_nr < rbio->stripe_nsectors);\n\n\treturn stripe_nr * rbio->stripe_nsectors + sector_nr;\n}\n\n \nstatic struct sector_ptr *rbio_stripe_sector(const struct btrfs_raid_bio *rbio,\n\t\t\t\t\t     unsigned int stripe_nr,\n\t\t\t\t\t     unsigned int sector_nr)\n{\n\treturn &rbio->stripe_sectors[rbio_stripe_sector_index(rbio, stripe_nr,\n\t\t\t\t\t\t\t      sector_nr)];\n}\n\n \nstatic struct sector_ptr *rbio_pstripe_sector(const struct btrfs_raid_bio *rbio,\n\t\t\t\t\t      unsigned int sector_nr)\n{\n\treturn rbio_stripe_sector(rbio, rbio->nr_data, sector_nr);\n}\n\n \nstatic struct sector_ptr *rbio_qstripe_sector(const struct btrfs_raid_bio *rbio,\n\t\t\t\t\t      unsigned int sector_nr)\n{\n\tif (rbio->nr_data + 1 == rbio->real_stripes)\n\t\treturn NULL;\n\treturn rbio_stripe_sector(rbio, rbio->nr_data + 1, sector_nr);\n}\n\n \nstatic noinline int lock_stripe_add(struct btrfs_raid_bio *rbio)\n{\n\tstruct btrfs_stripe_hash *h;\n\tstruct btrfs_raid_bio *cur;\n\tstruct btrfs_raid_bio *pending;\n\tstruct btrfs_raid_bio *freeit = NULL;\n\tstruct btrfs_raid_bio *cache_drop = NULL;\n\tint ret = 0;\n\n\th = rbio->bioc->fs_info->stripe_hash_table->table + rbio_bucket(rbio);\n\n\tspin_lock(&h->lock);\n\tlist_for_each_entry(cur, &h->hash_list, hash_list) {\n\t\tif (cur->bioc->full_stripe_logical != rbio->bioc->full_stripe_logical)\n\t\t\tcontinue;\n\n\t\tspin_lock(&cur->bio_list_lock);\n\n\t\t \n\t\tif (bio_list_empty(&cur->bio_list) &&\n\t\t    list_empty(&cur->plug_list) &&\n\t\t    test_bit(RBIO_CACHE_BIT, &cur->flags) &&\n\t\t    !test_bit(RBIO_RMW_LOCKED_BIT, &cur->flags)) {\n\t\t\tlist_del_init(&cur->hash_list);\n\t\t\trefcount_dec(&cur->refs);\n\n\t\t\tsteal_rbio(cur, rbio);\n\t\t\tcache_drop = cur;\n\t\t\tspin_unlock(&cur->bio_list_lock);\n\n\t\t\tgoto lockit;\n\t\t}\n\n\t\t \n\t\tif (rbio_can_merge(cur, rbio)) {\n\t\t\tmerge_rbio(cur, rbio);\n\t\t\tspin_unlock(&cur->bio_list_lock);\n\t\t\tfreeit = rbio;\n\t\t\tret = 1;\n\t\t\tgoto out;\n\t\t}\n\n\n\t\t \n\t\tlist_for_each_entry(pending, &cur->plug_list, plug_list) {\n\t\t\tif (rbio_can_merge(pending, rbio)) {\n\t\t\t\tmerge_rbio(pending, rbio);\n\t\t\t\tspin_unlock(&cur->bio_list_lock);\n\t\t\t\tfreeit = rbio;\n\t\t\t\tret = 1;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tlist_add_tail(&rbio->plug_list, &cur->plug_list);\n\t\tspin_unlock(&cur->bio_list_lock);\n\t\tret = 1;\n\t\tgoto out;\n\t}\nlockit:\n\trefcount_inc(&rbio->refs);\n\tlist_add(&rbio->hash_list, &h->hash_list);\nout:\n\tspin_unlock(&h->lock);\n\tif (cache_drop)\n\t\tremove_rbio_from_cache(cache_drop);\n\tif (freeit)\n\t\tfree_raid_bio(freeit);\n\treturn ret;\n}\n\nstatic void recover_rbio_work_locked(struct work_struct *work);\n\n \nstatic noinline void unlock_stripe(struct btrfs_raid_bio *rbio)\n{\n\tint bucket;\n\tstruct btrfs_stripe_hash *h;\n\tint keep_cache = 0;\n\n\tbucket = rbio_bucket(rbio);\n\th = rbio->bioc->fs_info->stripe_hash_table->table + bucket;\n\n\tif (list_empty(&rbio->plug_list))\n\t\tcache_rbio(rbio);\n\n\tspin_lock(&h->lock);\n\tspin_lock(&rbio->bio_list_lock);\n\n\tif (!list_empty(&rbio->hash_list)) {\n\t\t \n\t\tif (list_empty(&rbio->plug_list) &&\n\t\t    test_bit(RBIO_CACHE_BIT, &rbio->flags)) {\n\t\t\tkeep_cache = 1;\n\t\t\tclear_bit(RBIO_RMW_LOCKED_BIT, &rbio->flags);\n\t\t\tBUG_ON(!bio_list_empty(&rbio->bio_list));\n\t\t\tgoto done;\n\t\t}\n\n\t\tlist_del_init(&rbio->hash_list);\n\t\trefcount_dec(&rbio->refs);\n\n\t\t \n\t\tif (!list_empty(&rbio->plug_list)) {\n\t\t\tstruct btrfs_raid_bio *next;\n\t\t\tstruct list_head *head = rbio->plug_list.next;\n\n\t\t\tnext = list_entry(head, struct btrfs_raid_bio,\n\t\t\t\t\t  plug_list);\n\n\t\t\tlist_del_init(&rbio->plug_list);\n\n\t\t\tlist_add(&next->hash_list, &h->hash_list);\n\t\t\trefcount_inc(&next->refs);\n\t\t\tspin_unlock(&rbio->bio_list_lock);\n\t\t\tspin_unlock(&h->lock);\n\n\t\t\tif (next->operation == BTRFS_RBIO_READ_REBUILD) {\n\t\t\t\tstart_async_work(next, recover_rbio_work_locked);\n\t\t\t} else if (next->operation == BTRFS_RBIO_WRITE) {\n\t\t\t\tsteal_rbio(rbio, next);\n\t\t\t\tstart_async_work(next, rmw_rbio_work_locked);\n\t\t\t} else if (next->operation == BTRFS_RBIO_PARITY_SCRUB) {\n\t\t\t\tsteal_rbio(rbio, next);\n\t\t\t\tstart_async_work(next, scrub_rbio_work_locked);\n\t\t\t}\n\n\t\t\tgoto done_nolock;\n\t\t}\n\t}\ndone:\n\tspin_unlock(&rbio->bio_list_lock);\n\tspin_unlock(&h->lock);\n\ndone_nolock:\n\tif (!keep_cache)\n\t\tremove_rbio_from_cache(rbio);\n}\n\nstatic void rbio_endio_bio_list(struct bio *cur, blk_status_t err)\n{\n\tstruct bio *next;\n\n\twhile (cur) {\n\t\tnext = cur->bi_next;\n\t\tcur->bi_next = NULL;\n\t\tcur->bi_status = err;\n\t\tbio_endio(cur);\n\t\tcur = next;\n\t}\n}\n\n \nstatic void rbio_orig_end_io(struct btrfs_raid_bio *rbio, blk_status_t err)\n{\n\tstruct bio *cur = bio_list_get(&rbio->bio_list);\n\tstruct bio *extra;\n\n\tkfree(rbio->csum_buf);\n\tbitmap_free(rbio->csum_bitmap);\n\trbio->csum_buf = NULL;\n\trbio->csum_bitmap = NULL;\n\n\t \n\tbitmap_clear(&rbio->dbitmap, 0, rbio->stripe_nsectors);\n\n\t \n\tunlock_stripe(rbio);\n\textra = bio_list_get(&rbio->bio_list);\n\tfree_raid_bio(rbio);\n\n\trbio_endio_bio_list(cur, err);\n\tif (extra)\n\t\trbio_endio_bio_list(extra, err);\n}\n\n \nstatic struct sector_ptr *sector_in_rbio(struct btrfs_raid_bio *rbio,\n\t\t\t\t\t int stripe_nr, int sector_nr,\n\t\t\t\t\t bool bio_list_only)\n{\n\tstruct sector_ptr *sector;\n\tint index;\n\n\tASSERT(stripe_nr >= 0 && stripe_nr < rbio->real_stripes);\n\tASSERT(sector_nr >= 0 && sector_nr < rbio->stripe_nsectors);\n\n\tindex = stripe_nr * rbio->stripe_nsectors + sector_nr;\n\tASSERT(index >= 0 && index < rbio->nr_sectors);\n\n\tspin_lock(&rbio->bio_list_lock);\n\tsector = &rbio->bio_sectors[index];\n\tif (sector->page || bio_list_only) {\n\t\t \n\t\tif (!sector->page)\n\t\t\tsector = NULL;\n\t\tspin_unlock(&rbio->bio_list_lock);\n\t\treturn sector;\n\t}\n\tspin_unlock(&rbio->bio_list_lock);\n\n\treturn &rbio->stripe_sectors[index];\n}\n\n \nstatic struct btrfs_raid_bio *alloc_rbio(struct btrfs_fs_info *fs_info,\n\t\t\t\t\t struct btrfs_io_context *bioc)\n{\n\tconst unsigned int real_stripes = bioc->num_stripes - bioc->replace_nr_stripes;\n\tconst unsigned int stripe_npages = BTRFS_STRIPE_LEN >> PAGE_SHIFT;\n\tconst unsigned int num_pages = stripe_npages * real_stripes;\n\tconst unsigned int stripe_nsectors =\n\t\tBTRFS_STRIPE_LEN >> fs_info->sectorsize_bits;\n\tconst unsigned int num_sectors = stripe_nsectors * real_stripes;\n\tstruct btrfs_raid_bio *rbio;\n\n\t \n\tASSERT(IS_ALIGNED(PAGE_SIZE, fs_info->sectorsize));\n\t \n\tASSERT(stripe_nsectors <= BITS_PER_LONG);\n\n\trbio = kzalloc(sizeof(*rbio), GFP_NOFS);\n\tif (!rbio)\n\t\treturn ERR_PTR(-ENOMEM);\n\trbio->stripe_pages = kcalloc(num_pages, sizeof(struct page *),\n\t\t\t\t     GFP_NOFS);\n\trbio->bio_sectors = kcalloc(num_sectors, sizeof(struct sector_ptr),\n\t\t\t\t    GFP_NOFS);\n\trbio->stripe_sectors = kcalloc(num_sectors, sizeof(struct sector_ptr),\n\t\t\t\t       GFP_NOFS);\n\trbio->finish_pointers = kcalloc(real_stripes, sizeof(void *), GFP_NOFS);\n\trbio->error_bitmap = bitmap_zalloc(num_sectors, GFP_NOFS);\n\n\tif (!rbio->stripe_pages || !rbio->bio_sectors || !rbio->stripe_sectors ||\n\t    !rbio->finish_pointers || !rbio->error_bitmap) {\n\t\tfree_raid_bio_pointers(rbio);\n\t\tkfree(rbio);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\tbio_list_init(&rbio->bio_list);\n\tinit_waitqueue_head(&rbio->io_wait);\n\tINIT_LIST_HEAD(&rbio->plug_list);\n\tspin_lock_init(&rbio->bio_list_lock);\n\tINIT_LIST_HEAD(&rbio->stripe_cache);\n\tINIT_LIST_HEAD(&rbio->hash_list);\n\tbtrfs_get_bioc(bioc);\n\trbio->bioc = bioc;\n\trbio->nr_pages = num_pages;\n\trbio->nr_sectors = num_sectors;\n\trbio->real_stripes = real_stripes;\n\trbio->stripe_npages = stripe_npages;\n\trbio->stripe_nsectors = stripe_nsectors;\n\trefcount_set(&rbio->refs, 1);\n\tatomic_set(&rbio->stripes_pending, 0);\n\n\tASSERT(btrfs_nr_parity_stripes(bioc->map_type));\n\trbio->nr_data = real_stripes - btrfs_nr_parity_stripes(bioc->map_type);\n\n\treturn rbio;\n}\n\n \nstatic int alloc_rbio_pages(struct btrfs_raid_bio *rbio)\n{\n\tint ret;\n\n\tret = btrfs_alloc_page_array(rbio->nr_pages, rbio->stripe_pages);\n\tif (ret < 0)\n\t\treturn ret;\n\t \n\tindex_stripe_sectors(rbio);\n\treturn 0;\n}\n\n \nstatic int alloc_rbio_parity_pages(struct btrfs_raid_bio *rbio)\n{\n\tconst int data_pages = rbio->nr_data * rbio->stripe_npages;\n\tint ret;\n\n\tret = btrfs_alloc_page_array(rbio->nr_pages - data_pages,\n\t\t\t\t     rbio->stripe_pages + data_pages);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tindex_stripe_sectors(rbio);\n\treturn 0;\n}\n\n \nstatic int get_rbio_veritical_errors(struct btrfs_raid_bio *rbio, int sector_nr,\n\t\t\t\t     int *faila, int *failb)\n{\n\tint stripe_nr;\n\tint found_errors = 0;\n\n\tif (faila || failb) {\n\t\t \n\t\tASSERT(faila && failb);\n\t\t*faila = -1;\n\t\t*failb = -1;\n\t}\n\n\tfor (stripe_nr = 0; stripe_nr < rbio->real_stripes; stripe_nr++) {\n\t\tint total_sector_nr = stripe_nr * rbio->stripe_nsectors + sector_nr;\n\n\t\tif (test_bit(total_sector_nr, rbio->error_bitmap)) {\n\t\t\tfound_errors++;\n\t\t\tif (faila) {\n\t\t\t\t \n\t\t\t\tif (*faila < 0)\n\t\t\t\t\t*faila = stripe_nr;\n\t\t\t\telse if (*failb < 0)\n\t\t\t\t\t*failb = stripe_nr;\n\t\t\t}\n\t\t}\n\t}\n\treturn found_errors;\n}\n\n \nstatic int rbio_add_io_sector(struct btrfs_raid_bio *rbio,\n\t\t\t      struct bio_list *bio_list,\n\t\t\t      struct sector_ptr *sector,\n\t\t\t      unsigned int stripe_nr,\n\t\t\t      unsigned int sector_nr,\n\t\t\t      enum req_op op)\n{\n\tconst u32 sectorsize = rbio->bioc->fs_info->sectorsize;\n\tstruct bio *last = bio_list->tail;\n\tint ret;\n\tstruct bio *bio;\n\tstruct btrfs_io_stripe *stripe;\n\tu64 disk_start;\n\n\t \n\tASSERT(stripe_nr >= 0 && stripe_nr < rbio->bioc->num_stripes);\n\tASSERT(sector_nr >= 0 && sector_nr < rbio->stripe_nsectors);\n\tASSERT(sector->page);\n\n\tstripe = &rbio->bioc->stripes[stripe_nr];\n\tdisk_start = stripe->physical + sector_nr * sectorsize;\n\n\t \n\tif (!stripe->dev->bdev) {\n\t\tint found_errors;\n\n\t\tset_bit(stripe_nr * rbio->stripe_nsectors + sector_nr,\n\t\t\trbio->error_bitmap);\n\n\t\t \n\t\tfound_errors = get_rbio_veritical_errors(rbio, sector_nr,\n\t\t\t\t\t\t\t NULL, NULL);\n\t\tif (found_errors > rbio->bioc->max_errors)\n\t\t\treturn -EIO;\n\t\treturn 0;\n\t}\n\n\t \n\tif (last) {\n\t\tu64 last_end = last->bi_iter.bi_sector << SECTOR_SHIFT;\n\t\tlast_end += last->bi_iter.bi_size;\n\n\t\t \n\t\tif (last_end == disk_start && !last->bi_status &&\n\t\t    last->bi_bdev == stripe->dev->bdev) {\n\t\t\tret = bio_add_page(last, sector->page, sectorsize,\n\t\t\t\t\t   sector->pgoff);\n\t\t\tif (ret == sectorsize)\n\t\t\t\treturn 0;\n\t\t}\n\t}\n\n\t \n\tbio = bio_alloc(stripe->dev->bdev,\n\t\t\tmax(BTRFS_STRIPE_LEN >> PAGE_SHIFT, 1),\n\t\t\top, GFP_NOFS);\n\tbio->bi_iter.bi_sector = disk_start >> SECTOR_SHIFT;\n\tbio->bi_private = rbio;\n\n\t__bio_add_page(bio, sector->page, sectorsize, sector->pgoff);\n\tbio_list_add(bio_list, bio);\n\treturn 0;\n}\n\nstatic void index_one_bio(struct btrfs_raid_bio *rbio, struct bio *bio)\n{\n\tconst u32 sectorsize = rbio->bioc->fs_info->sectorsize;\n\tstruct bio_vec bvec;\n\tstruct bvec_iter iter;\n\tu32 offset = (bio->bi_iter.bi_sector << SECTOR_SHIFT) -\n\t\t     rbio->bioc->full_stripe_logical;\n\n\tbio_for_each_segment(bvec, bio, iter) {\n\t\tu32 bvec_offset;\n\n\t\tfor (bvec_offset = 0; bvec_offset < bvec.bv_len;\n\t\t     bvec_offset += sectorsize, offset += sectorsize) {\n\t\t\tint index = offset / sectorsize;\n\t\t\tstruct sector_ptr *sector = &rbio->bio_sectors[index];\n\n\t\t\tsector->page = bvec.bv_page;\n\t\t\tsector->pgoff = bvec.bv_offset + bvec_offset;\n\t\t\tASSERT(sector->pgoff < PAGE_SIZE);\n\t\t}\n\t}\n}\n\n \nstatic void index_rbio_pages(struct btrfs_raid_bio *rbio)\n{\n\tstruct bio *bio;\n\n\tspin_lock(&rbio->bio_list_lock);\n\tbio_list_for_each(bio, &rbio->bio_list)\n\t\tindex_one_bio(rbio, bio);\n\n\tspin_unlock(&rbio->bio_list_lock);\n}\n\nstatic void bio_get_trace_info(struct btrfs_raid_bio *rbio, struct bio *bio,\n\t\t\t       struct raid56_bio_trace_info *trace_info)\n{\n\tconst struct btrfs_io_context *bioc = rbio->bioc;\n\tint i;\n\n\tASSERT(bioc);\n\n\t \n\tif (!bio->bi_bdev)\n\t\tgoto not_found;\n\n\tfor (i = 0; i < bioc->num_stripes; i++) {\n\t\tif (bio->bi_bdev != bioc->stripes[i].dev->bdev)\n\t\t\tcontinue;\n\t\ttrace_info->stripe_nr = i;\n\t\ttrace_info->devid = bioc->stripes[i].dev->devid;\n\t\ttrace_info->offset = (bio->bi_iter.bi_sector << SECTOR_SHIFT) -\n\t\t\t\t     bioc->stripes[i].physical;\n\t\treturn;\n\t}\n\nnot_found:\n\ttrace_info->devid = -1;\n\ttrace_info->offset = -1;\n\ttrace_info->stripe_nr = -1;\n}\n\nstatic inline void bio_list_put(struct bio_list *bio_list)\n{\n\tstruct bio *bio;\n\n\twhile ((bio = bio_list_pop(bio_list)))\n\t\tbio_put(bio);\n}\n\n \nstatic void generate_pq_vertical(struct btrfs_raid_bio *rbio, int sectornr)\n{\n\tvoid **pointers = rbio->finish_pointers;\n\tconst u32 sectorsize = rbio->bioc->fs_info->sectorsize;\n\tstruct sector_ptr *sector;\n\tint stripe;\n\tconst bool has_qstripe = rbio->bioc->map_type & BTRFS_BLOCK_GROUP_RAID6;\n\n\t \n\tfor (stripe = 0; stripe < rbio->nr_data; stripe++) {\n\t\tsector = sector_in_rbio(rbio, stripe, sectornr, 0);\n\t\tpointers[stripe] = kmap_local_page(sector->page) +\n\t\t\t\t   sector->pgoff;\n\t}\n\n\t \n\tsector = rbio_pstripe_sector(rbio, sectornr);\n\tsector->uptodate = 1;\n\tpointers[stripe++] = kmap_local_page(sector->page) + sector->pgoff;\n\n\tif (has_qstripe) {\n\t\t \n\t\tsector = rbio_qstripe_sector(rbio, sectornr);\n\t\tsector->uptodate = 1;\n\t\tpointers[stripe++] = kmap_local_page(sector->page) +\n\t\t\t\t     sector->pgoff;\n\n\t\traid6_call.gen_syndrome(rbio->real_stripes, sectorsize,\n\t\t\t\t\tpointers);\n\t} else {\n\t\t \n\t\tmemcpy(pointers[rbio->nr_data], pointers[0], sectorsize);\n\t\trun_xor(pointers + 1, rbio->nr_data - 1, sectorsize);\n\t}\n\tfor (stripe = stripe - 1; stripe >= 0; stripe--)\n\t\tkunmap_local(pointers[stripe]);\n}\n\nstatic int rmw_assemble_write_bios(struct btrfs_raid_bio *rbio,\n\t\t\t\t   struct bio_list *bio_list)\n{\n\t \n\tint total_sector_nr;\n\tint sectornr;\n\tint stripe;\n\tint ret;\n\n\tASSERT(bio_list_size(bio_list) == 0);\n\n\t \n\tASSERT(bitmap_weight(&rbio->dbitmap, rbio->stripe_nsectors));\n\n\t \n\tbitmap_clear(rbio->error_bitmap, 0, rbio->nr_sectors);\n\n\t \n\tfor (total_sector_nr = 0; total_sector_nr < rbio->nr_sectors;\n\t     total_sector_nr++) {\n\t\tstruct sector_ptr *sector;\n\n\t\tstripe = total_sector_nr / rbio->stripe_nsectors;\n\t\tsectornr = total_sector_nr % rbio->stripe_nsectors;\n\n\t\t \n\t\tif (!test_bit(sectornr, &rbio->dbitmap))\n\t\t\tcontinue;\n\n\t\tif (stripe < rbio->nr_data) {\n\t\t\tsector = sector_in_rbio(rbio, stripe, sectornr, 1);\n\t\t\tif (!sector)\n\t\t\t\tcontinue;\n\t\t} else {\n\t\t\tsector = rbio_stripe_sector(rbio, stripe, sectornr);\n\t\t}\n\n\t\tret = rbio_add_io_sector(rbio, bio_list, sector, stripe,\n\t\t\t\t\t sectornr, REQ_OP_WRITE);\n\t\tif (ret)\n\t\t\tgoto error;\n\t}\n\n\tif (likely(!rbio->bioc->replace_nr_stripes))\n\t\treturn 0;\n\n\t \n\tASSERT(rbio->bioc->replace_stripe_src >= 0);\n\n\tfor (total_sector_nr = 0; total_sector_nr < rbio->nr_sectors;\n\t     total_sector_nr++) {\n\t\tstruct sector_ptr *sector;\n\n\t\tstripe = total_sector_nr / rbio->stripe_nsectors;\n\t\tsectornr = total_sector_nr % rbio->stripe_nsectors;\n\n\t\t \n\t\tif (stripe != rbio->bioc->replace_stripe_src) {\n\t\t\t \n\t\t\tASSERT(sectornr == 0);\n\t\t\ttotal_sector_nr += rbio->stripe_nsectors - 1;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (!test_bit(sectornr, &rbio->dbitmap))\n\t\t\tcontinue;\n\n\t\tif (stripe < rbio->nr_data) {\n\t\t\tsector = sector_in_rbio(rbio, stripe, sectornr, 1);\n\t\t\tif (!sector)\n\t\t\t\tcontinue;\n\t\t} else {\n\t\t\tsector = rbio_stripe_sector(rbio, stripe, sectornr);\n\t\t}\n\n\t\tret = rbio_add_io_sector(rbio, bio_list, sector,\n\t\t\t\t\t rbio->real_stripes,\n\t\t\t\t\t sectornr, REQ_OP_WRITE);\n\t\tif (ret)\n\t\t\tgoto error;\n\t}\n\n\treturn 0;\nerror:\n\tbio_list_put(bio_list);\n\treturn -EIO;\n}\n\nstatic void set_rbio_range_error(struct btrfs_raid_bio *rbio, struct bio *bio)\n{\n\tstruct btrfs_fs_info *fs_info = rbio->bioc->fs_info;\n\tu32 offset = (bio->bi_iter.bi_sector << SECTOR_SHIFT) -\n\t\t     rbio->bioc->full_stripe_logical;\n\tint total_nr_sector = offset >> fs_info->sectorsize_bits;\n\n\tASSERT(total_nr_sector < rbio->nr_data * rbio->stripe_nsectors);\n\n\tbitmap_set(rbio->error_bitmap, total_nr_sector,\n\t\t   bio->bi_iter.bi_size >> fs_info->sectorsize_bits);\n\n\t \n\tif (bio->bi_iter.bi_size == 0) {\n\t\tbool found_missing = false;\n\t\tint stripe_nr;\n\n\t\tfor (stripe_nr = 0; stripe_nr < rbio->real_stripes; stripe_nr++) {\n\t\t\tif (!rbio->bioc->stripes[stripe_nr].dev->bdev) {\n\t\t\t\tfound_missing = true;\n\t\t\t\tbitmap_set(rbio->error_bitmap,\n\t\t\t\t\t   stripe_nr * rbio->stripe_nsectors,\n\t\t\t\t\t   rbio->stripe_nsectors);\n\t\t\t}\n\t\t}\n\t\tASSERT(found_missing);\n\t}\n}\n\n \nstatic struct sector_ptr *find_stripe_sector(struct btrfs_raid_bio *rbio,\n\t\t\t\t\t     struct page *page,\n\t\t\t\t\t     unsigned int pgoff)\n{\n\tint i;\n\n\tfor (i = 0; i < rbio->nr_sectors; i++) {\n\t\tstruct sector_ptr *sector = &rbio->stripe_sectors[i];\n\n\t\tif (sector->page == page && sector->pgoff == pgoff)\n\t\t\treturn sector;\n\t}\n\treturn NULL;\n}\n\n \nstatic void set_bio_pages_uptodate(struct btrfs_raid_bio *rbio, struct bio *bio)\n{\n\tconst u32 sectorsize = rbio->bioc->fs_info->sectorsize;\n\tstruct bio_vec *bvec;\n\tstruct bvec_iter_all iter_all;\n\n\tASSERT(!bio_flagged(bio, BIO_CLONED));\n\n\tbio_for_each_segment_all(bvec, bio, iter_all) {\n\t\tstruct sector_ptr *sector;\n\t\tint pgoff;\n\n\t\tfor (pgoff = bvec->bv_offset; pgoff - bvec->bv_offset < bvec->bv_len;\n\t\t     pgoff += sectorsize) {\n\t\t\tsector = find_stripe_sector(rbio, bvec->bv_page, pgoff);\n\t\t\tASSERT(sector);\n\t\t\tif (sector)\n\t\t\t\tsector->uptodate = 1;\n\t\t}\n\t}\n}\n\nstatic int get_bio_sector_nr(struct btrfs_raid_bio *rbio, struct bio *bio)\n{\n\tstruct bio_vec *bv = bio_first_bvec_all(bio);\n\tint i;\n\n\tfor (i = 0; i < rbio->nr_sectors; i++) {\n\t\tstruct sector_ptr *sector;\n\n\t\tsector = &rbio->stripe_sectors[i];\n\t\tif (sector->page == bv->bv_page && sector->pgoff == bv->bv_offset)\n\t\t\tbreak;\n\t\tsector = &rbio->bio_sectors[i];\n\t\tif (sector->page == bv->bv_page && sector->pgoff == bv->bv_offset)\n\t\t\tbreak;\n\t}\n\tASSERT(i < rbio->nr_sectors);\n\treturn i;\n}\n\nstatic void rbio_update_error_bitmap(struct btrfs_raid_bio *rbio, struct bio *bio)\n{\n\tint total_sector_nr = get_bio_sector_nr(rbio, bio);\n\tu32 bio_size = 0;\n\tstruct bio_vec *bvec;\n\tint i;\n\n\tbio_for_each_bvec_all(bvec, bio, i)\n\t\tbio_size += bvec->bv_len;\n\n\t \n\tfor (i = total_sector_nr; i < total_sector_nr +\n\t     (bio_size >> rbio->bioc->fs_info->sectorsize_bits); i++)\n\t\tset_bit(i, rbio->error_bitmap);\n}\n\n \nstatic void verify_bio_data_sectors(struct btrfs_raid_bio *rbio,\n\t\t\t\t    struct bio *bio)\n{\n\tstruct btrfs_fs_info *fs_info = rbio->bioc->fs_info;\n\tint total_sector_nr = get_bio_sector_nr(rbio, bio);\n\tstruct bio_vec *bvec;\n\tstruct bvec_iter_all iter_all;\n\n\t \n\tif (!rbio->csum_bitmap || !rbio->csum_buf)\n\t\treturn;\n\n\t \n\tif (total_sector_nr >= rbio->nr_data * rbio->stripe_nsectors)\n\t\treturn;\n\n\tbio_for_each_segment_all(bvec, bio, iter_all) {\n\t\tint bv_offset;\n\n\t\tfor (bv_offset = bvec->bv_offset;\n\t\t     bv_offset < bvec->bv_offset + bvec->bv_len;\n\t\t     bv_offset += fs_info->sectorsize, total_sector_nr++) {\n\t\t\tu8 csum_buf[BTRFS_CSUM_SIZE];\n\t\t\tu8 *expected_csum = rbio->csum_buf +\n\t\t\t\t\t    total_sector_nr * fs_info->csum_size;\n\t\t\tint ret;\n\n\t\t\t \n\t\t\tif (!test_bit(total_sector_nr, rbio->csum_bitmap))\n\t\t\t\tcontinue;\n\n\t\t\tret = btrfs_check_sector_csum(fs_info, bvec->bv_page,\n\t\t\t\tbv_offset, csum_buf, expected_csum);\n\t\t\tif (ret < 0)\n\t\t\t\tset_bit(total_sector_nr, rbio->error_bitmap);\n\t\t}\n\t}\n}\n\nstatic void raid_wait_read_end_io(struct bio *bio)\n{\n\tstruct btrfs_raid_bio *rbio = bio->bi_private;\n\n\tif (bio->bi_status) {\n\t\trbio_update_error_bitmap(rbio, bio);\n\t} else {\n\t\tset_bio_pages_uptodate(rbio, bio);\n\t\tverify_bio_data_sectors(rbio, bio);\n\t}\n\n\tbio_put(bio);\n\tif (atomic_dec_and_test(&rbio->stripes_pending))\n\t\twake_up(&rbio->io_wait);\n}\n\nstatic void submit_read_wait_bio_list(struct btrfs_raid_bio *rbio,\n\t\t\t     struct bio_list *bio_list)\n{\n\tstruct bio *bio;\n\n\tatomic_set(&rbio->stripes_pending, bio_list_size(bio_list));\n\twhile ((bio = bio_list_pop(bio_list))) {\n\t\tbio->bi_end_io = raid_wait_read_end_io;\n\n\t\tif (trace_raid56_read_enabled()) {\n\t\t\tstruct raid56_bio_trace_info trace_info = { 0 };\n\n\t\t\tbio_get_trace_info(rbio, bio, &trace_info);\n\t\t\ttrace_raid56_read(rbio, bio, &trace_info);\n\t\t}\n\t\tsubmit_bio(bio);\n\t}\n\n\twait_event(rbio->io_wait, atomic_read(&rbio->stripes_pending) == 0);\n}\n\nstatic int alloc_rbio_data_pages(struct btrfs_raid_bio *rbio)\n{\n\tconst int data_pages = rbio->nr_data * rbio->stripe_npages;\n\tint ret;\n\n\tret = btrfs_alloc_page_array(data_pages, rbio->stripe_pages);\n\tif (ret < 0)\n\t\treturn ret;\n\n\tindex_stripe_sectors(rbio);\n\treturn 0;\n}\n\n \nstruct btrfs_plug_cb {\n\tstruct blk_plug_cb cb;\n\tstruct btrfs_fs_info *info;\n\tstruct list_head rbio_list;\n\tstruct work_struct work;\n};\n\n \nstatic int plug_cmp(void *priv, const struct list_head *a,\n\t\t    const struct list_head *b)\n{\n\tconst struct btrfs_raid_bio *ra = container_of(a, struct btrfs_raid_bio,\n\t\t\t\t\t\t       plug_list);\n\tconst struct btrfs_raid_bio *rb = container_of(b, struct btrfs_raid_bio,\n\t\t\t\t\t\t       plug_list);\n\tu64 a_sector = ra->bio_list.head->bi_iter.bi_sector;\n\tu64 b_sector = rb->bio_list.head->bi_iter.bi_sector;\n\n\tif (a_sector < b_sector)\n\t\treturn -1;\n\tif (a_sector > b_sector)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic void raid_unplug(struct blk_plug_cb *cb, bool from_schedule)\n{\n\tstruct btrfs_plug_cb *plug = container_of(cb, struct btrfs_plug_cb, cb);\n\tstruct btrfs_raid_bio *cur;\n\tstruct btrfs_raid_bio *last = NULL;\n\n\tlist_sort(NULL, &plug->rbio_list, plug_cmp);\n\n\twhile (!list_empty(&plug->rbio_list)) {\n\t\tcur = list_entry(plug->rbio_list.next,\n\t\t\t\t struct btrfs_raid_bio, plug_list);\n\t\tlist_del_init(&cur->plug_list);\n\n\t\tif (rbio_is_full(cur)) {\n\t\t\t \n\t\t\tstart_async_work(cur, rmw_rbio_work);\n\t\t\tcontinue;\n\t\t}\n\t\tif (last) {\n\t\t\tif (rbio_can_merge(last, cur)) {\n\t\t\t\tmerge_rbio(last, cur);\n\t\t\t\tfree_raid_bio(cur);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tstart_async_work(last, rmw_rbio_work);\n\t\t}\n\t\tlast = cur;\n\t}\n\tif (last)\n\t\tstart_async_work(last, rmw_rbio_work);\n\tkfree(plug);\n}\n\n \nstatic void rbio_add_bio(struct btrfs_raid_bio *rbio, struct bio *orig_bio)\n{\n\tconst struct btrfs_fs_info *fs_info = rbio->bioc->fs_info;\n\tconst u64 orig_logical = orig_bio->bi_iter.bi_sector << SECTOR_SHIFT;\n\tconst u64 full_stripe_start = rbio->bioc->full_stripe_logical;\n\tconst u32 orig_len = orig_bio->bi_iter.bi_size;\n\tconst u32 sectorsize = fs_info->sectorsize;\n\tu64 cur_logical;\n\n\tASSERT(orig_logical >= full_stripe_start &&\n\t       orig_logical + orig_len <= full_stripe_start +\n\t       rbio->nr_data * BTRFS_STRIPE_LEN);\n\n\tbio_list_add(&rbio->bio_list, orig_bio);\n\trbio->bio_list_bytes += orig_bio->bi_iter.bi_size;\n\n\t \n\tfor (cur_logical = orig_logical; cur_logical < orig_logical + orig_len;\n\t     cur_logical += sectorsize) {\n\t\tint bit = ((u32)(cur_logical - full_stripe_start) >>\n\t\t\t   fs_info->sectorsize_bits) % rbio->stripe_nsectors;\n\n\t\tset_bit(bit, &rbio->dbitmap);\n\t}\n}\n\n \nvoid raid56_parity_write(struct bio *bio, struct btrfs_io_context *bioc)\n{\n\tstruct btrfs_fs_info *fs_info = bioc->fs_info;\n\tstruct btrfs_raid_bio *rbio;\n\tstruct btrfs_plug_cb *plug = NULL;\n\tstruct blk_plug_cb *cb;\n\n\trbio = alloc_rbio(fs_info, bioc);\n\tif (IS_ERR(rbio)) {\n\t\tbio->bi_status = errno_to_blk_status(PTR_ERR(rbio));\n\t\tbio_endio(bio);\n\t\treturn;\n\t}\n\trbio->operation = BTRFS_RBIO_WRITE;\n\trbio_add_bio(rbio, bio);\n\n\t \n\tif (!rbio_is_full(rbio)) {\n\t\tcb = blk_check_plugged(raid_unplug, fs_info, sizeof(*plug));\n\t\tif (cb) {\n\t\t\tplug = container_of(cb, struct btrfs_plug_cb, cb);\n\t\t\tif (!plug->info) {\n\t\t\t\tplug->info = fs_info;\n\t\t\t\tINIT_LIST_HEAD(&plug->rbio_list);\n\t\t\t}\n\t\t\tlist_add_tail(&rbio->plug_list, &plug->rbio_list);\n\t\t\treturn;\n\t\t}\n\t}\n\n\t \n\tstart_async_work(rbio, rmw_rbio_work);\n}\n\nstatic int verify_one_sector(struct btrfs_raid_bio *rbio,\n\t\t\t     int stripe_nr, int sector_nr)\n{\n\tstruct btrfs_fs_info *fs_info = rbio->bioc->fs_info;\n\tstruct sector_ptr *sector;\n\tu8 csum_buf[BTRFS_CSUM_SIZE];\n\tu8 *csum_expected;\n\tint ret;\n\n\tif (!rbio->csum_bitmap || !rbio->csum_buf)\n\t\treturn 0;\n\n\t \n\tif (stripe_nr >= rbio->nr_data)\n\t\treturn 0;\n\t \n\tif (rbio->operation == BTRFS_RBIO_READ_REBUILD) {\n\t\tsector = sector_in_rbio(rbio, stripe_nr, sector_nr, 0);\n\t} else {\n\t\tsector = rbio_stripe_sector(rbio, stripe_nr, sector_nr);\n\t}\n\n\tASSERT(sector->page);\n\n\tcsum_expected = rbio->csum_buf +\n\t\t\t(stripe_nr * rbio->stripe_nsectors + sector_nr) *\n\t\t\tfs_info->csum_size;\n\tret = btrfs_check_sector_csum(fs_info, sector->page, sector->pgoff,\n\t\t\t\t      csum_buf, csum_expected);\n\treturn ret;\n}\n\n \nstatic int recover_vertical(struct btrfs_raid_bio *rbio, int sector_nr,\n\t\t\t    void **pointers, void **unmap_array)\n{\n\tstruct btrfs_fs_info *fs_info = rbio->bioc->fs_info;\n\tstruct sector_ptr *sector;\n\tconst u32 sectorsize = fs_info->sectorsize;\n\tint found_errors;\n\tint faila;\n\tint failb;\n\tint stripe_nr;\n\tint ret = 0;\n\n\t \n\tif (rbio->operation == BTRFS_RBIO_PARITY_SCRUB &&\n\t    !test_bit(sector_nr, &rbio->dbitmap))\n\t\treturn 0;\n\n\tfound_errors = get_rbio_veritical_errors(rbio, sector_nr, &faila,\n\t\t\t\t\t\t &failb);\n\t \n\tif (!found_errors)\n\t\treturn 0;\n\n\tif (found_errors > rbio->bioc->max_errors)\n\t\treturn -EIO;\n\n\t \n\tfor (stripe_nr = 0; stripe_nr < rbio->real_stripes; stripe_nr++) {\n\t\t \n\t\tif (rbio->operation == BTRFS_RBIO_READ_REBUILD) {\n\t\t\tsector = sector_in_rbio(rbio, stripe_nr, sector_nr, 0);\n\t\t} else {\n\t\t\tsector = rbio_stripe_sector(rbio, stripe_nr, sector_nr);\n\t\t}\n\t\tASSERT(sector->page);\n\t\tpointers[stripe_nr] = kmap_local_page(sector->page) +\n\t\t\t\t   sector->pgoff;\n\t\tunmap_array[stripe_nr] = pointers[stripe_nr];\n\t}\n\n\t \n\tif (rbio->bioc->map_type & BTRFS_BLOCK_GROUP_RAID6) {\n\t\t \n\t\tif (failb < 0) {\n\t\t\tif (faila == rbio->nr_data)\n\t\t\t\t \n\t\t\t\tgoto cleanup;\n\t\t\t \n\t\t\tgoto pstripe;\n\t\t}\n\n\t\t \n\t\tif (failb == rbio->real_stripes - 1) {\n\t\t\tif (faila == rbio->real_stripes - 2)\n\t\t\t\t \n\t\t\t\tgoto cleanup;\n\t\t\t \n\t\t\tgoto pstripe;\n\t\t}\n\n\t\tif (failb == rbio->real_stripes - 2) {\n\t\t\traid6_datap_recov(rbio->real_stripes, sectorsize,\n\t\t\t\t\t  faila, pointers);\n\t\t} else {\n\t\t\traid6_2data_recov(rbio->real_stripes, sectorsize,\n\t\t\t\t\t  faila, failb, pointers);\n\t\t}\n\t} else {\n\t\tvoid *p;\n\n\t\t \n\t\tASSERT(failb == -1);\npstripe:\n\t\t \n\t\tmemcpy(pointers[faila], pointers[rbio->nr_data], sectorsize);\n\n\t\t \n\t\tp = pointers[faila];\n\t\tfor (stripe_nr = faila; stripe_nr < rbio->nr_data - 1;\n\t\t     stripe_nr++)\n\t\t\tpointers[stripe_nr] = pointers[stripe_nr + 1];\n\t\tpointers[rbio->nr_data - 1] = p;\n\n\t\t \n\t\trun_xor(pointers, rbio->nr_data - 1, sectorsize);\n\n\t}\n\n\t \n\tif (faila >= 0) {\n\t\tret = verify_one_sector(rbio, faila, sector_nr);\n\t\tif (ret < 0)\n\t\t\tgoto cleanup;\n\n\t\tsector = rbio_stripe_sector(rbio, faila, sector_nr);\n\t\tsector->uptodate = 1;\n\t}\n\tif (failb >= 0) {\n\t\tret = verify_one_sector(rbio, failb, sector_nr);\n\t\tif (ret < 0)\n\t\t\tgoto cleanup;\n\n\t\tsector = rbio_stripe_sector(rbio, failb, sector_nr);\n\t\tsector->uptodate = 1;\n\t}\n\ncleanup:\n\tfor (stripe_nr = rbio->real_stripes - 1; stripe_nr >= 0; stripe_nr--)\n\t\tkunmap_local(unmap_array[stripe_nr]);\n\treturn ret;\n}\n\nstatic int recover_sectors(struct btrfs_raid_bio *rbio)\n{\n\tvoid **pointers = NULL;\n\tvoid **unmap_array = NULL;\n\tint sectornr;\n\tint ret = 0;\n\n\t \n\tpointers = kcalloc(rbio->real_stripes, sizeof(void *), GFP_NOFS);\n\tunmap_array = kcalloc(rbio->real_stripes, sizeof(void *), GFP_NOFS);\n\tif (!pointers || !unmap_array) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tif (rbio->operation == BTRFS_RBIO_READ_REBUILD) {\n\t\tspin_lock(&rbio->bio_list_lock);\n\t\tset_bit(RBIO_RMW_LOCKED_BIT, &rbio->flags);\n\t\tspin_unlock(&rbio->bio_list_lock);\n\t}\n\n\tindex_rbio_pages(rbio);\n\n\tfor (sectornr = 0; sectornr < rbio->stripe_nsectors; sectornr++) {\n\t\tret = recover_vertical(rbio, sectornr, pointers, unmap_array);\n\t\tif (ret < 0)\n\t\t\tbreak;\n\t}\n\nout:\n\tkfree(pointers);\n\tkfree(unmap_array);\n\treturn ret;\n}\n\nstatic void recover_rbio(struct btrfs_raid_bio *rbio)\n{\n\tstruct bio_list bio_list = BIO_EMPTY_LIST;\n\tint total_sector_nr;\n\tint ret = 0;\n\n\t \n\tASSERT(bitmap_weight(rbio->error_bitmap, rbio->nr_sectors));\n\n\t \n\tret = alloc_rbio_pages(rbio);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tindex_rbio_pages(rbio);\n\n\t \n\tfor (total_sector_nr = 0; total_sector_nr < rbio->nr_sectors;\n\t     total_sector_nr++) {\n\t\tint stripe = total_sector_nr / rbio->stripe_nsectors;\n\t\tint sectornr = total_sector_nr % rbio->stripe_nsectors;\n\t\tstruct sector_ptr *sector;\n\n\t\t \n\t\tif (!rbio->bioc->stripes[stripe].dev->bdev ||\n\t\t    test_bit(total_sector_nr, rbio->error_bitmap)) {\n\t\t\t \n\t\t\tset_bit(total_sector_nr, rbio->error_bitmap);\n\t\t\tcontinue;\n\t\t}\n\n\t\tsector = rbio_stripe_sector(rbio, stripe, sectornr);\n\t\tret = rbio_add_io_sector(rbio, &bio_list, sector, stripe,\n\t\t\t\t\t sectornr, REQ_OP_READ);\n\t\tif (ret < 0) {\n\t\t\tbio_list_put(&bio_list);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsubmit_read_wait_bio_list(rbio, &bio_list);\n\tret = recover_sectors(rbio);\nout:\n\trbio_orig_end_io(rbio, errno_to_blk_status(ret));\n}\n\nstatic void recover_rbio_work(struct work_struct *work)\n{\n\tstruct btrfs_raid_bio *rbio;\n\n\trbio = container_of(work, struct btrfs_raid_bio, work);\n\tif (!lock_stripe_add(rbio))\n\t\trecover_rbio(rbio);\n}\n\nstatic void recover_rbio_work_locked(struct work_struct *work)\n{\n\trecover_rbio(container_of(work, struct btrfs_raid_bio, work));\n}\n\nstatic void set_rbio_raid6_extra_error(struct btrfs_raid_bio *rbio, int mirror_num)\n{\n\tbool found = false;\n\tint sector_nr;\n\n\t \n\tASSERT(mirror_num > 2);\n\tfor (sector_nr = 0; sector_nr < rbio->stripe_nsectors; sector_nr++) {\n\t\tint found_errors;\n\t\tint faila;\n\t\tint failb;\n\n\t\tfound_errors = get_rbio_veritical_errors(rbio, sector_nr,\n\t\t\t\t\t\t\t &faila, &failb);\n\t\t \n\t\tif (!found_errors)\n\t\t\tcontinue;\n\n\t\t \n\t\tASSERT(found_errors == 1);\n\t\tfound = true;\n\n\t\t \n\t\tfailb = rbio->real_stripes - (mirror_num - 1);\n\t\tif (failb <= faila)\n\t\t\tfailb--;\n\n\t\t \n\t\tif (failb >= 0)\n\t\t\tset_bit(failb * rbio->stripe_nsectors + sector_nr,\n\t\t\t\trbio->error_bitmap);\n\t}\n\n\t \n\tASSERT(found);\n}\n\n \nvoid raid56_parity_recover(struct bio *bio, struct btrfs_io_context *bioc,\n\t\t\t   int mirror_num)\n{\n\tstruct btrfs_fs_info *fs_info = bioc->fs_info;\n\tstruct btrfs_raid_bio *rbio;\n\n\trbio = alloc_rbio(fs_info, bioc);\n\tif (IS_ERR(rbio)) {\n\t\tbio->bi_status = errno_to_blk_status(PTR_ERR(rbio));\n\t\tbio_endio(bio);\n\t\treturn;\n\t}\n\n\trbio->operation = BTRFS_RBIO_READ_REBUILD;\n\trbio_add_bio(rbio, bio);\n\n\tset_rbio_range_error(rbio, bio);\n\n\t \n\tif (mirror_num > 2)\n\t\tset_rbio_raid6_extra_error(rbio, mirror_num);\n\n\tstart_async_work(rbio, recover_rbio_work);\n}\n\nstatic void fill_data_csums(struct btrfs_raid_bio *rbio)\n{\n\tstruct btrfs_fs_info *fs_info = rbio->bioc->fs_info;\n\tstruct btrfs_root *csum_root = btrfs_csum_root(fs_info,\n\t\t\t\t\t\t       rbio->bioc->full_stripe_logical);\n\tconst u64 start = rbio->bioc->full_stripe_logical;\n\tconst u32 len = (rbio->nr_data * rbio->stripe_nsectors) <<\n\t\t\tfs_info->sectorsize_bits;\n\tint ret;\n\n\t \n\tASSERT(!rbio->csum_buf && !rbio->csum_bitmap);\n\n\t \n\tif (!(rbio->bioc->map_type & BTRFS_BLOCK_GROUP_DATA) ||\n\t    rbio->bioc->map_type & BTRFS_BLOCK_GROUP_METADATA)\n\t\treturn;\n\n\trbio->csum_buf = kzalloc(rbio->nr_data * rbio->stripe_nsectors *\n\t\t\t\t fs_info->csum_size, GFP_NOFS);\n\trbio->csum_bitmap = bitmap_zalloc(rbio->nr_data * rbio->stripe_nsectors,\n\t\t\t\t\t  GFP_NOFS);\n\tif (!rbio->csum_buf || !rbio->csum_bitmap) {\n\t\tret = -ENOMEM;\n\t\tgoto error;\n\t}\n\n\tret = btrfs_lookup_csums_bitmap(csum_root, NULL, start, start + len - 1,\n\t\t\t\t\trbio->csum_buf, rbio->csum_bitmap);\n\tif (ret < 0)\n\t\tgoto error;\n\tif (bitmap_empty(rbio->csum_bitmap, len >> fs_info->sectorsize_bits))\n\t\tgoto no_csum;\n\treturn;\n\nerror:\n\t \n\tbtrfs_warn_rl(fs_info,\n\"sub-stripe write for full stripe %llu is not safe, failed to get csum: %d\",\n\t\t\trbio->bioc->full_stripe_logical, ret);\nno_csum:\n\tkfree(rbio->csum_buf);\n\tbitmap_free(rbio->csum_bitmap);\n\trbio->csum_buf = NULL;\n\trbio->csum_bitmap = NULL;\n}\n\nstatic int rmw_read_wait_recover(struct btrfs_raid_bio *rbio)\n{\n\tstruct bio_list bio_list = BIO_EMPTY_LIST;\n\tint total_sector_nr;\n\tint ret = 0;\n\n\t \n\tfill_data_csums(rbio);\n\n\t \n\tfor (total_sector_nr = 0; total_sector_nr < rbio->nr_sectors;\n\t     total_sector_nr++) {\n\t\tstruct sector_ptr *sector;\n\t\tint stripe = total_sector_nr / rbio->stripe_nsectors;\n\t\tint sectornr = total_sector_nr % rbio->stripe_nsectors;\n\n\t\tsector = rbio_stripe_sector(rbio, stripe, sectornr);\n\t\tret = rbio_add_io_sector(rbio, &bio_list, sector,\n\t\t\t       stripe, sectornr, REQ_OP_READ);\n\t\tif (ret) {\n\t\t\tbio_list_put(&bio_list);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\t \n\tsubmit_read_wait_bio_list(rbio, &bio_list);\n\treturn recover_sectors(rbio);\n}\n\nstatic void raid_wait_write_end_io(struct bio *bio)\n{\n\tstruct btrfs_raid_bio *rbio = bio->bi_private;\n\tblk_status_t err = bio->bi_status;\n\n\tif (err)\n\t\trbio_update_error_bitmap(rbio, bio);\n\tbio_put(bio);\n\tif (atomic_dec_and_test(&rbio->stripes_pending))\n\t\twake_up(&rbio->io_wait);\n}\n\nstatic void submit_write_bios(struct btrfs_raid_bio *rbio,\n\t\t\t      struct bio_list *bio_list)\n{\n\tstruct bio *bio;\n\n\tatomic_set(&rbio->stripes_pending, bio_list_size(bio_list));\n\twhile ((bio = bio_list_pop(bio_list))) {\n\t\tbio->bi_end_io = raid_wait_write_end_io;\n\n\t\tif (trace_raid56_write_enabled()) {\n\t\t\tstruct raid56_bio_trace_info trace_info = { 0 };\n\n\t\t\tbio_get_trace_info(rbio, bio, &trace_info);\n\t\t\ttrace_raid56_write(rbio, bio, &trace_info);\n\t\t}\n\t\tsubmit_bio(bio);\n\t}\n}\n\n \nstatic bool need_read_stripe_sectors(struct btrfs_raid_bio *rbio)\n{\n\tint i;\n\n\tfor (i = 0; i < rbio->nr_data * rbio->stripe_nsectors; i++) {\n\t\tstruct sector_ptr *sector = &rbio->stripe_sectors[i];\n\n\t\t \n\t\tif (!sector->page || !sector->uptodate)\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic void rmw_rbio(struct btrfs_raid_bio *rbio)\n{\n\tstruct bio_list bio_list;\n\tint sectornr;\n\tint ret = 0;\n\n\t \n\tret = alloc_rbio_parity_pages(rbio);\n\tif (ret < 0)\n\t\tgoto out;\n\n\t \n\tif (!rbio_is_full(rbio) && need_read_stripe_sectors(rbio)) {\n\t\t \n\t\tret = alloc_rbio_data_pages(rbio);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\n\t\tindex_rbio_pages(rbio);\n\n\t\tret = rmw_read_wait_recover(rbio);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\n\n\t \n\tspin_lock(&rbio->bio_list_lock);\n\tset_bit(RBIO_RMW_LOCKED_BIT, &rbio->flags);\n\tspin_unlock(&rbio->bio_list_lock);\n\n\tbitmap_clear(rbio->error_bitmap, 0, rbio->nr_sectors);\n\n\tindex_rbio_pages(rbio);\n\n\t \n\tif (!rbio_is_full(rbio))\n\t\tcache_rbio_pages(rbio);\n\telse\n\t\tclear_bit(RBIO_CACHE_READY_BIT, &rbio->flags);\n\n\tfor (sectornr = 0; sectornr < rbio->stripe_nsectors; sectornr++)\n\t\tgenerate_pq_vertical(rbio, sectornr);\n\n\tbio_list_init(&bio_list);\n\tret = rmw_assemble_write_bios(rbio, &bio_list);\n\tif (ret < 0)\n\t\tgoto out;\n\n\t \n\tASSERT(bio_list_size(&bio_list));\n\tsubmit_write_bios(rbio, &bio_list);\n\twait_event(rbio->io_wait, atomic_read(&rbio->stripes_pending) == 0);\n\n\t \n\tfor (sectornr = 0; sectornr < rbio->stripe_nsectors; sectornr++) {\n\t\tint found_errors;\n\n\t\tfound_errors = get_rbio_veritical_errors(rbio, sectornr, NULL, NULL);\n\t\tif (found_errors > rbio->bioc->max_errors) {\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\trbio_orig_end_io(rbio, errno_to_blk_status(ret));\n}\n\nstatic void rmw_rbio_work(struct work_struct *work)\n{\n\tstruct btrfs_raid_bio *rbio;\n\n\trbio = container_of(work, struct btrfs_raid_bio, work);\n\tif (lock_stripe_add(rbio) == 0)\n\t\trmw_rbio(rbio);\n}\n\nstatic void rmw_rbio_work_locked(struct work_struct *work)\n{\n\trmw_rbio(container_of(work, struct btrfs_raid_bio, work));\n}\n\n \n\nstruct btrfs_raid_bio *raid56_parity_alloc_scrub_rbio(struct bio *bio,\n\t\t\t\tstruct btrfs_io_context *bioc,\n\t\t\t\tstruct btrfs_device *scrub_dev,\n\t\t\t\tunsigned long *dbitmap, int stripe_nsectors)\n{\n\tstruct btrfs_fs_info *fs_info = bioc->fs_info;\n\tstruct btrfs_raid_bio *rbio;\n\tint i;\n\n\trbio = alloc_rbio(fs_info, bioc);\n\tif (IS_ERR(rbio))\n\t\treturn NULL;\n\tbio_list_add(&rbio->bio_list, bio);\n\t \n\tASSERT(!bio->bi_iter.bi_size);\n\trbio->operation = BTRFS_RBIO_PARITY_SCRUB;\n\n\t \n\tfor (i = rbio->nr_data; i < rbio->real_stripes; i++) {\n\t\tif (bioc->stripes[i].dev == scrub_dev) {\n\t\t\trbio->scrubp = i;\n\t\t\tbreak;\n\t\t}\n\t}\n\tASSERT(i < rbio->real_stripes);\n\n\tbitmap_copy(&rbio->dbitmap, dbitmap, stripe_nsectors);\n\treturn rbio;\n}\n\n \nstatic int alloc_rbio_essential_pages(struct btrfs_raid_bio *rbio)\n{\n\tconst u32 sectorsize = rbio->bioc->fs_info->sectorsize;\n\tint total_sector_nr;\n\n\tfor (total_sector_nr = 0; total_sector_nr < rbio->nr_sectors;\n\t     total_sector_nr++) {\n\t\tstruct page *page;\n\t\tint sectornr = total_sector_nr % rbio->stripe_nsectors;\n\t\tint index = (total_sector_nr * sectorsize) >> PAGE_SHIFT;\n\n\t\tif (!test_bit(sectornr, &rbio->dbitmap))\n\t\t\tcontinue;\n\t\tif (rbio->stripe_pages[index])\n\t\t\tcontinue;\n\t\tpage = alloc_page(GFP_NOFS);\n\t\tif (!page)\n\t\t\treturn -ENOMEM;\n\t\trbio->stripe_pages[index] = page;\n\t}\n\tindex_stripe_sectors(rbio);\n\treturn 0;\n}\n\nstatic int finish_parity_scrub(struct btrfs_raid_bio *rbio)\n{\n\tstruct btrfs_io_context *bioc = rbio->bioc;\n\tconst u32 sectorsize = bioc->fs_info->sectorsize;\n\tvoid **pointers = rbio->finish_pointers;\n\tunsigned long *pbitmap = &rbio->finish_pbitmap;\n\tint nr_data = rbio->nr_data;\n\tint stripe;\n\tint sectornr;\n\tbool has_qstripe;\n\tstruct sector_ptr p_sector = { 0 };\n\tstruct sector_ptr q_sector = { 0 };\n\tstruct bio_list bio_list;\n\tint is_replace = 0;\n\tint ret;\n\n\tbio_list_init(&bio_list);\n\n\tif (rbio->real_stripes - rbio->nr_data == 1)\n\t\thas_qstripe = false;\n\telse if (rbio->real_stripes - rbio->nr_data == 2)\n\t\thas_qstripe = true;\n\telse\n\t\tBUG();\n\n\t \n\tif (bioc->replace_nr_stripes && bioc->replace_stripe_src == rbio->scrubp) {\n\t\tis_replace = 1;\n\t\tbitmap_copy(pbitmap, &rbio->dbitmap, rbio->stripe_nsectors);\n\t}\n\n\t \n\tclear_bit(RBIO_CACHE_READY_BIT, &rbio->flags);\n\n\tp_sector.page = alloc_page(GFP_NOFS);\n\tif (!p_sector.page)\n\t\treturn -ENOMEM;\n\tp_sector.pgoff = 0;\n\tp_sector.uptodate = 1;\n\n\tif (has_qstripe) {\n\t\t \n\t\tq_sector.page = alloc_page(GFP_NOFS);\n\t\tif (!q_sector.page) {\n\t\t\t__free_page(p_sector.page);\n\t\t\tp_sector.page = NULL;\n\t\t\treturn -ENOMEM;\n\t\t}\n\t\tq_sector.pgoff = 0;\n\t\tq_sector.uptodate = 1;\n\t\tpointers[rbio->real_stripes - 1] = kmap_local_page(q_sector.page);\n\t}\n\n\tbitmap_clear(rbio->error_bitmap, 0, rbio->nr_sectors);\n\n\t \n\tpointers[nr_data] = kmap_local_page(p_sector.page);\n\n\tfor_each_set_bit(sectornr, &rbio->dbitmap, rbio->stripe_nsectors) {\n\t\tstruct sector_ptr *sector;\n\t\tvoid *parity;\n\n\t\t \n\t\tfor (stripe = 0; stripe < nr_data; stripe++) {\n\t\t\tsector = sector_in_rbio(rbio, stripe, sectornr, 0);\n\t\t\tpointers[stripe] = kmap_local_page(sector->page) +\n\t\t\t\t\t   sector->pgoff;\n\t\t}\n\n\t\tif (has_qstripe) {\n\t\t\t \n\t\t\traid6_call.gen_syndrome(rbio->real_stripes, sectorsize,\n\t\t\t\t\t\tpointers);\n\t\t} else {\n\t\t\t \n\t\t\tmemcpy(pointers[nr_data], pointers[0], sectorsize);\n\t\t\trun_xor(pointers + 1, nr_data - 1, sectorsize);\n\t\t}\n\n\t\t \n\t\tsector = rbio_stripe_sector(rbio, rbio->scrubp, sectornr);\n\t\tparity = kmap_local_page(sector->page) + sector->pgoff;\n\t\tif (memcmp(parity, pointers[rbio->scrubp], sectorsize) != 0)\n\t\t\tmemcpy(parity, pointers[rbio->scrubp], sectorsize);\n\t\telse\n\t\t\t \n\t\t\tbitmap_clear(&rbio->dbitmap, sectornr, 1);\n\t\tkunmap_local(parity);\n\n\t\tfor (stripe = nr_data - 1; stripe >= 0; stripe--)\n\t\t\tkunmap_local(pointers[stripe]);\n\t}\n\n\tkunmap_local(pointers[nr_data]);\n\t__free_page(p_sector.page);\n\tp_sector.page = NULL;\n\tif (q_sector.page) {\n\t\tkunmap_local(pointers[rbio->real_stripes - 1]);\n\t\t__free_page(q_sector.page);\n\t\tq_sector.page = NULL;\n\t}\n\n\t \n\tfor_each_set_bit(sectornr, &rbio->dbitmap, rbio->stripe_nsectors) {\n\t\tstruct sector_ptr *sector;\n\n\t\tsector = rbio_stripe_sector(rbio, rbio->scrubp, sectornr);\n\t\tret = rbio_add_io_sector(rbio, &bio_list, sector, rbio->scrubp,\n\t\t\t\t\t sectornr, REQ_OP_WRITE);\n\t\tif (ret)\n\t\t\tgoto cleanup;\n\t}\n\n\tif (!is_replace)\n\t\tgoto submit_write;\n\n\t \n\tASSERT(rbio->bioc->replace_stripe_src >= 0);\n\tfor_each_set_bit(sectornr, pbitmap, rbio->stripe_nsectors) {\n\t\tstruct sector_ptr *sector;\n\n\t\tsector = rbio_stripe_sector(rbio, rbio->scrubp, sectornr);\n\t\tret = rbio_add_io_sector(rbio, &bio_list, sector,\n\t\t\t\t\t rbio->real_stripes,\n\t\t\t\t\t sectornr, REQ_OP_WRITE);\n\t\tif (ret)\n\t\t\tgoto cleanup;\n\t}\n\nsubmit_write:\n\tsubmit_write_bios(rbio, &bio_list);\n\treturn 0;\n\ncleanup:\n\tbio_list_put(&bio_list);\n\treturn ret;\n}\n\nstatic inline int is_data_stripe(struct btrfs_raid_bio *rbio, int stripe)\n{\n\tif (stripe >= 0 && stripe < rbio->nr_data)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic int recover_scrub_rbio(struct btrfs_raid_bio *rbio)\n{\n\tvoid **pointers = NULL;\n\tvoid **unmap_array = NULL;\n\tint sector_nr;\n\tint ret = 0;\n\n\t \n\tpointers = kcalloc(rbio->real_stripes, sizeof(void *), GFP_NOFS);\n\tunmap_array = kcalloc(rbio->real_stripes, sizeof(void *), GFP_NOFS);\n\tif (!pointers || !unmap_array) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tfor (sector_nr = 0; sector_nr < rbio->stripe_nsectors; sector_nr++) {\n\t\tint dfail = 0, failp = -1;\n\t\tint faila;\n\t\tint failb;\n\t\tint found_errors;\n\n\t\tfound_errors = get_rbio_veritical_errors(rbio, sector_nr,\n\t\t\t\t\t\t\t &faila, &failb);\n\t\tif (found_errors > rbio->bioc->max_errors) {\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\tif (found_errors == 0)\n\t\t\tcontinue;\n\n\t\t \n\t\tASSERT(faila >= 0 || failb >= 0);\n\n\t\tif (is_data_stripe(rbio, faila))\n\t\t\tdfail++;\n\t\telse if (is_parity_stripe(faila))\n\t\t\tfailp = faila;\n\n\t\tif (is_data_stripe(rbio, failb))\n\t\t\tdfail++;\n\t\telse if (is_parity_stripe(failb))\n\t\t\tfailp = failb;\n\t\t \n\t\tif (dfail > rbio->bioc->max_errors - 1) {\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\t\t \n\t\tif (dfail == 0)\n\t\t\tcontinue;\n\n\t\t \n\t\tif (failp != rbio->scrubp) {\n\t\t\tret = -EIO;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = recover_vertical(rbio, sector_nr, pointers, unmap_array);\n\t\tif (ret < 0)\n\t\t\tgoto out;\n\t}\nout:\n\tkfree(pointers);\n\tkfree(unmap_array);\n\treturn ret;\n}\n\nstatic int scrub_assemble_read_bios(struct btrfs_raid_bio *rbio)\n{\n\tstruct bio_list bio_list = BIO_EMPTY_LIST;\n\tint total_sector_nr;\n\tint ret = 0;\n\n\t \n\tfor (total_sector_nr = 0; total_sector_nr < rbio->nr_sectors;\n\t     total_sector_nr++) {\n\t\tint sectornr = total_sector_nr % rbio->stripe_nsectors;\n\t\tint stripe = total_sector_nr / rbio->stripe_nsectors;\n\t\tstruct sector_ptr *sector;\n\n\t\t \n\t\tif (!test_bit(sectornr, &rbio->dbitmap))\n\t\t\tcontinue;\n\n\t\t \n\t\tsector = sector_in_rbio(rbio, stripe, sectornr, 1);\n\t\tif (sector)\n\t\t\tcontinue;\n\n\t\tsector = rbio_stripe_sector(rbio, stripe, sectornr);\n\t\t \n\t\tif (sector->uptodate)\n\t\t\tcontinue;\n\n\t\tret = rbio_add_io_sector(rbio, &bio_list, sector, stripe,\n\t\t\t\t\t sectornr, REQ_OP_READ);\n\t\tif (ret) {\n\t\t\tbio_list_put(&bio_list);\n\t\t\treturn ret;\n\t\t}\n\t}\n\n\tsubmit_read_wait_bio_list(rbio, &bio_list);\n\treturn 0;\n}\n\nstatic void scrub_rbio(struct btrfs_raid_bio *rbio)\n{\n\tint sector_nr;\n\tint ret;\n\n\tret = alloc_rbio_essential_pages(rbio);\n\tif (ret)\n\t\tgoto out;\n\n\tbitmap_clear(rbio->error_bitmap, 0, rbio->nr_sectors);\n\n\tret = scrub_assemble_read_bios(rbio);\n\tif (ret < 0)\n\t\tgoto out;\n\n\t \n\tret = recover_scrub_rbio(rbio);\n\tif (ret < 0)\n\t\tgoto out;\n\n\t \n\tret = finish_parity_scrub(rbio);\n\twait_event(rbio->io_wait, atomic_read(&rbio->stripes_pending) == 0);\n\tfor (sector_nr = 0; sector_nr < rbio->stripe_nsectors; sector_nr++) {\n\t\tint found_errors;\n\n\t\tfound_errors = get_rbio_veritical_errors(rbio, sector_nr, NULL, NULL);\n\t\tif (found_errors > rbio->bioc->max_errors) {\n\t\t\tret = -EIO;\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\trbio_orig_end_io(rbio, errno_to_blk_status(ret));\n}\n\nstatic void scrub_rbio_work_locked(struct work_struct *work)\n{\n\tscrub_rbio(container_of(work, struct btrfs_raid_bio, work));\n}\n\nvoid raid56_parity_submit_scrub_rbio(struct btrfs_raid_bio *rbio)\n{\n\tif (!lock_stripe_add(rbio))\n\t\tstart_async_work(rbio, scrub_rbio_work_locked);\n}\n\n \nvoid raid56_parity_cache_data_pages(struct btrfs_raid_bio *rbio,\n\t\t\t\t    struct page **data_pages, u64 data_logical)\n{\n\tconst u64 offset_in_full_stripe = data_logical -\n\t\t\t\t\t  rbio->bioc->full_stripe_logical;\n\tconst int page_index = offset_in_full_stripe >> PAGE_SHIFT;\n\tconst u32 sectorsize = rbio->bioc->fs_info->sectorsize;\n\tconst u32 sectors_per_page = PAGE_SIZE / sectorsize;\n\tint ret;\n\n\t \n\tret = alloc_rbio_data_pages(rbio);\n\tif (ret < 0)\n\t\treturn;\n\n\t \n\tASSERT(IS_ALIGNED(offset_in_full_stripe, BTRFS_STRIPE_LEN));\n\tASSERT(offset_in_full_stripe < (rbio->nr_data << BTRFS_STRIPE_LEN_SHIFT));\n\n\tfor (int page_nr = 0; page_nr < (BTRFS_STRIPE_LEN >> PAGE_SHIFT); page_nr++) {\n\t\tstruct page *dst = rbio->stripe_pages[page_nr + page_index];\n\t\tstruct page *src = data_pages[page_nr];\n\n\t\tmemcpy_page(dst, 0, src, 0, PAGE_SIZE);\n\t\tfor (int sector_nr = sectors_per_page * page_index;\n\t\t     sector_nr < sectors_per_page * (page_index + 1);\n\t\t     sector_nr++)\n\t\t\trbio->stripe_sectors[sector_nr].uptodate = true;\n\t}\n}\n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}