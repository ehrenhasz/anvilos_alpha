{
  "module_name": "free-space-cache.c",
  "hash_id": "a85c884c6e2816e6b5f59c17ceb3bc3ba293951600c691a0e8dc87225db56dcf",
  "original_prompt": "Ingested from linux-6.6.14/fs/btrfs/free-space-cache.c",
  "human_readable_source": "\n \n\n#include <linux/pagemap.h>\n#include <linux/sched.h>\n#include <linux/sched/signal.h>\n#include <linux/slab.h>\n#include <linux/math64.h>\n#include <linux/ratelimit.h>\n#include <linux/error-injection.h>\n#include <linux/sched/mm.h>\n#include \"ctree.h\"\n#include \"fs.h\"\n#include \"messages.h\"\n#include \"misc.h\"\n#include \"free-space-cache.h\"\n#include \"transaction.h\"\n#include \"disk-io.h\"\n#include \"extent_io.h\"\n#include \"volumes.h\"\n#include \"space-info.h\"\n#include \"delalloc-space.h\"\n#include \"block-group.h\"\n#include \"discard.h\"\n#include \"subpage.h\"\n#include \"inode-item.h\"\n#include \"accessors.h\"\n#include \"file-item.h\"\n#include \"file.h\"\n#include \"super.h\"\n\n#define BITS_PER_BITMAP\t\t(PAGE_SIZE * 8UL)\n#define MAX_CACHE_BYTES_PER_GIG\tSZ_64K\n#define FORCE_EXTENT_THRESHOLD\tSZ_1M\n\nstatic struct kmem_cache *btrfs_free_space_cachep;\nstatic struct kmem_cache *btrfs_free_space_bitmap_cachep;\n\nstruct btrfs_trim_range {\n\tu64 start;\n\tu64 bytes;\n\tstruct list_head list;\n};\n\nstatic int link_free_space(struct btrfs_free_space_ctl *ctl,\n\t\t\t   struct btrfs_free_space *info);\nstatic void unlink_free_space(struct btrfs_free_space_ctl *ctl,\n\t\t\t      struct btrfs_free_space *info, bool update_stat);\nstatic int search_bitmap(struct btrfs_free_space_ctl *ctl,\n\t\t\t struct btrfs_free_space *bitmap_info, u64 *offset,\n\t\t\t u64 *bytes, bool for_alloc);\nstatic void free_bitmap(struct btrfs_free_space_ctl *ctl,\n\t\t\tstruct btrfs_free_space *bitmap_info);\nstatic void bitmap_clear_bits(struct btrfs_free_space_ctl *ctl,\n\t\t\t      struct btrfs_free_space *info, u64 offset,\n\t\t\t      u64 bytes, bool update_stats);\n\nstatic void __btrfs_remove_free_space_cache(struct btrfs_free_space_ctl *ctl)\n{\n\tstruct btrfs_free_space *info;\n\tstruct rb_node *node;\n\n\twhile ((node = rb_last(&ctl->free_space_offset)) != NULL) {\n\t\tinfo = rb_entry(node, struct btrfs_free_space, offset_index);\n\t\tif (!info->bitmap) {\n\t\t\tunlink_free_space(ctl, info, true);\n\t\t\tkmem_cache_free(btrfs_free_space_cachep, info);\n\t\t} else {\n\t\t\tfree_bitmap(ctl, info);\n\t\t}\n\n\t\tcond_resched_lock(&ctl->tree_lock);\n\t}\n}\n\nstatic struct inode *__lookup_free_space_inode(struct btrfs_root *root,\n\t\t\t\t\t       struct btrfs_path *path,\n\t\t\t\t\t       u64 offset)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct btrfs_key key;\n\tstruct btrfs_key location;\n\tstruct btrfs_disk_key disk_key;\n\tstruct btrfs_free_space_header *header;\n\tstruct extent_buffer *leaf;\n\tstruct inode *inode = NULL;\n\tunsigned nofs_flag;\n\tint ret;\n\n\tkey.objectid = BTRFS_FREE_SPACE_OBJECTID;\n\tkey.offset = offset;\n\tkey.type = 0;\n\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\tif (ret > 0) {\n\t\tbtrfs_release_path(path);\n\t\treturn ERR_PTR(-ENOENT);\n\t}\n\n\tleaf = path->nodes[0];\n\theader = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\tstruct btrfs_free_space_header);\n\tbtrfs_free_space_key(leaf, header, &disk_key);\n\tbtrfs_disk_key_to_cpu(&location, &disk_key);\n\tbtrfs_release_path(path);\n\n\t \n\tnofs_flag = memalloc_nofs_save();\n\tinode = btrfs_iget_path(fs_info->sb, location.objectid, root, path);\n\tbtrfs_release_path(path);\n\tmemalloc_nofs_restore(nofs_flag);\n\tif (IS_ERR(inode))\n\t\treturn inode;\n\n\tmapping_set_gfp_mask(inode->i_mapping,\n\t\t\tmapping_gfp_constraint(inode->i_mapping,\n\t\t\t~(__GFP_FS | __GFP_HIGHMEM)));\n\n\treturn inode;\n}\n\nstruct inode *lookup_free_space_inode(struct btrfs_block_group *block_group,\n\t\tstruct btrfs_path *path)\n{\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tstruct inode *inode = NULL;\n\tu32 flags = BTRFS_INODE_NODATASUM | BTRFS_INODE_NODATACOW;\n\n\tspin_lock(&block_group->lock);\n\tif (block_group->inode)\n\t\tinode = igrab(block_group->inode);\n\tspin_unlock(&block_group->lock);\n\tif (inode)\n\t\treturn inode;\n\n\tinode = __lookup_free_space_inode(fs_info->tree_root, path,\n\t\t\t\t\t  block_group->start);\n\tif (IS_ERR(inode))\n\t\treturn inode;\n\n\tspin_lock(&block_group->lock);\n\tif (!((BTRFS_I(inode)->flags & flags) == flags)) {\n\t\tbtrfs_info(fs_info, \"Old style space inode found, converting.\");\n\t\tBTRFS_I(inode)->flags |= BTRFS_INODE_NODATASUM |\n\t\t\tBTRFS_INODE_NODATACOW;\n\t\tblock_group->disk_cache_state = BTRFS_DC_CLEAR;\n\t}\n\n\tif (!test_and_set_bit(BLOCK_GROUP_FLAG_IREF, &block_group->runtime_flags))\n\t\tblock_group->inode = igrab(inode);\n\tspin_unlock(&block_group->lock);\n\n\treturn inode;\n}\n\nstatic int __create_free_space_inode(struct btrfs_root *root,\n\t\t\t\t     struct btrfs_trans_handle *trans,\n\t\t\t\t     struct btrfs_path *path,\n\t\t\t\t     u64 ino, u64 offset)\n{\n\tstruct btrfs_key key;\n\tstruct btrfs_disk_key disk_key;\n\tstruct btrfs_free_space_header *header;\n\tstruct btrfs_inode_item *inode_item;\n\tstruct extent_buffer *leaf;\n\t \n\tconst u64 flags = BTRFS_INODE_NOCOMPRESS | BTRFS_INODE_PREALLOC |\n\t\t\t  BTRFS_INODE_NODATASUM | BTRFS_INODE_NODATACOW;\n\tint ret;\n\n\tret = btrfs_insert_empty_inode(trans, root, path, ino);\n\tif (ret)\n\t\treturn ret;\n\n\tleaf = path->nodes[0];\n\tinode_item = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\t    struct btrfs_inode_item);\n\tbtrfs_item_key(leaf, &disk_key, path->slots[0]);\n\tmemzero_extent_buffer(leaf, (unsigned long)inode_item,\n\t\t\t     sizeof(*inode_item));\n\tbtrfs_set_inode_generation(leaf, inode_item, trans->transid);\n\tbtrfs_set_inode_size(leaf, inode_item, 0);\n\tbtrfs_set_inode_nbytes(leaf, inode_item, 0);\n\tbtrfs_set_inode_uid(leaf, inode_item, 0);\n\tbtrfs_set_inode_gid(leaf, inode_item, 0);\n\tbtrfs_set_inode_mode(leaf, inode_item, S_IFREG | 0600);\n\tbtrfs_set_inode_flags(leaf, inode_item, flags);\n\tbtrfs_set_inode_nlink(leaf, inode_item, 1);\n\tbtrfs_set_inode_transid(leaf, inode_item, trans->transid);\n\tbtrfs_set_inode_block_group(leaf, inode_item, offset);\n\tbtrfs_mark_buffer_dirty(trans, leaf);\n\tbtrfs_release_path(path);\n\n\tkey.objectid = BTRFS_FREE_SPACE_OBJECTID;\n\tkey.offset = offset;\n\tkey.type = 0;\n\tret = btrfs_insert_empty_item(trans, root, path, &key,\n\t\t\t\t      sizeof(struct btrfs_free_space_header));\n\tif (ret < 0) {\n\t\tbtrfs_release_path(path);\n\t\treturn ret;\n\t}\n\n\tleaf = path->nodes[0];\n\theader = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\tstruct btrfs_free_space_header);\n\tmemzero_extent_buffer(leaf, (unsigned long)header, sizeof(*header));\n\tbtrfs_set_free_space_key(leaf, header, &disk_key);\n\tbtrfs_mark_buffer_dirty(trans, leaf);\n\tbtrfs_release_path(path);\n\n\treturn 0;\n}\n\nint create_free_space_inode(struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_block_group *block_group,\n\t\t\t    struct btrfs_path *path)\n{\n\tint ret;\n\tu64 ino;\n\n\tret = btrfs_get_free_objectid(trans->fs_info->tree_root, &ino);\n\tif (ret < 0)\n\t\treturn ret;\n\n\treturn __create_free_space_inode(trans->fs_info->tree_root, trans, path,\n\t\t\t\t\t ino, block_group->start);\n}\n\n \nint btrfs_remove_free_space_inode(struct btrfs_trans_handle *trans,\n\t\t\t\t  struct inode *inode,\n\t\t\t\t  struct btrfs_block_group *block_group)\n{\n\tstruct btrfs_path *path;\n\tstruct btrfs_key key;\n\tint ret = 0;\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn -ENOMEM;\n\n\tif (!inode)\n\t\tinode = lookup_free_space_inode(block_group, path);\n\tif (IS_ERR(inode)) {\n\t\tif (PTR_ERR(inode) != -ENOENT)\n\t\t\tret = PTR_ERR(inode);\n\t\tgoto out;\n\t}\n\tret = btrfs_orphan_add(trans, BTRFS_I(inode));\n\tif (ret) {\n\t\tbtrfs_add_delayed_iput(BTRFS_I(inode));\n\t\tgoto out;\n\t}\n\tclear_nlink(inode);\n\t \n\tspin_lock(&block_group->lock);\n\tif (test_and_clear_bit(BLOCK_GROUP_FLAG_IREF, &block_group->runtime_flags)) {\n\t\tblock_group->inode = NULL;\n\t\tspin_unlock(&block_group->lock);\n\t\tiput(inode);\n\t} else {\n\t\tspin_unlock(&block_group->lock);\n\t}\n\t \n\tbtrfs_add_delayed_iput(BTRFS_I(inode));\n\n\tkey.objectid = BTRFS_FREE_SPACE_OBJECTID;\n\tkey.type = 0;\n\tkey.offset = block_group->start;\n\tret = btrfs_search_slot(trans, trans->fs_info->tree_root, &key, path,\n\t\t\t\t-1, 1);\n\tif (ret) {\n\t\tif (ret > 0)\n\t\t\tret = 0;\n\t\tgoto out;\n\t}\n\tret = btrfs_del_item(trans, trans->fs_info->tree_root, path);\nout:\n\tbtrfs_free_path(path);\n\treturn ret;\n}\n\nint btrfs_truncate_free_space_cache(struct btrfs_trans_handle *trans,\n\t\t\t\t    struct btrfs_block_group *block_group,\n\t\t\t\t    struct inode *vfs_inode)\n{\n\tstruct btrfs_truncate_control control = {\n\t\t.inode = BTRFS_I(vfs_inode),\n\t\t.new_size = 0,\n\t\t.ino = btrfs_ino(BTRFS_I(vfs_inode)),\n\t\t.min_type = BTRFS_EXTENT_DATA_KEY,\n\t\t.clear_extent_range = true,\n\t};\n\tstruct btrfs_inode *inode = BTRFS_I(vfs_inode);\n\tstruct btrfs_root *root = inode->root;\n\tstruct extent_state *cached_state = NULL;\n\tint ret = 0;\n\tbool locked = false;\n\n\tif (block_group) {\n\t\tstruct btrfs_path *path = btrfs_alloc_path();\n\n\t\tif (!path) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto fail;\n\t\t}\n\t\tlocked = true;\n\t\tmutex_lock(&trans->transaction->cache_write_mutex);\n\t\tif (!list_empty(&block_group->io_list)) {\n\t\t\tlist_del_init(&block_group->io_list);\n\n\t\t\tbtrfs_wait_cache_io(trans, block_group, path);\n\t\t\tbtrfs_put_block_group(block_group);\n\t\t}\n\n\t\t \n\t\tspin_lock(&block_group->lock);\n\t\tblock_group->disk_cache_state = BTRFS_DC_CLEAR;\n\t\tspin_unlock(&block_group->lock);\n\t\tbtrfs_free_path(path);\n\t}\n\n\tbtrfs_i_size_write(inode, 0);\n\ttruncate_pagecache(vfs_inode, 0);\n\n\tlock_extent(&inode->io_tree, 0, (u64)-1, &cached_state);\n\tbtrfs_drop_extent_map_range(inode, 0, (u64)-1, false);\n\n\t \n\tret = btrfs_truncate_inode_items(trans, root, &control);\n\n\tinode_sub_bytes(&inode->vfs_inode, control.sub_bytes);\n\tbtrfs_inode_safe_disk_i_size_write(inode, control.last_size);\n\n\tunlock_extent(&inode->io_tree, 0, (u64)-1, &cached_state);\n\tif (ret)\n\t\tgoto fail;\n\n\tret = btrfs_update_inode(trans, root, inode);\n\nfail:\n\tif (locked)\n\t\tmutex_unlock(&trans->transaction->cache_write_mutex);\n\tif (ret)\n\t\tbtrfs_abort_transaction(trans, ret);\n\n\treturn ret;\n}\n\nstatic void readahead_cache(struct inode *inode)\n{\n\tstruct file_ra_state ra;\n\tunsigned long last_index;\n\n\tfile_ra_state_init(&ra, inode->i_mapping);\n\tlast_index = (i_size_read(inode) - 1) >> PAGE_SHIFT;\n\n\tpage_cache_sync_readahead(inode->i_mapping, &ra, NULL, 0, last_index);\n}\n\nstatic int io_ctl_init(struct btrfs_io_ctl *io_ctl, struct inode *inode,\n\t\t       int write)\n{\n\tint num_pages;\n\n\tnum_pages = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);\n\n\t \n\tif (write && (num_pages * sizeof(u32) + sizeof(u64)) > PAGE_SIZE)\n\t\treturn -ENOSPC;\n\n\tmemset(io_ctl, 0, sizeof(struct btrfs_io_ctl));\n\n\tio_ctl->pages = kcalloc(num_pages, sizeof(struct page *), GFP_NOFS);\n\tif (!io_ctl->pages)\n\t\treturn -ENOMEM;\n\n\tio_ctl->num_pages = num_pages;\n\tio_ctl->fs_info = btrfs_sb(inode->i_sb);\n\tio_ctl->inode = inode;\n\n\treturn 0;\n}\nALLOW_ERROR_INJECTION(io_ctl_init, ERRNO);\n\nstatic void io_ctl_free(struct btrfs_io_ctl *io_ctl)\n{\n\tkfree(io_ctl->pages);\n\tio_ctl->pages = NULL;\n}\n\nstatic void io_ctl_unmap_page(struct btrfs_io_ctl *io_ctl)\n{\n\tif (io_ctl->cur) {\n\t\tio_ctl->cur = NULL;\n\t\tio_ctl->orig = NULL;\n\t}\n}\n\nstatic void io_ctl_map_page(struct btrfs_io_ctl *io_ctl, int clear)\n{\n\tASSERT(io_ctl->index < io_ctl->num_pages);\n\tio_ctl->page = io_ctl->pages[io_ctl->index++];\n\tio_ctl->cur = page_address(io_ctl->page);\n\tio_ctl->orig = io_ctl->cur;\n\tio_ctl->size = PAGE_SIZE;\n\tif (clear)\n\t\tclear_page(io_ctl->cur);\n}\n\nstatic void io_ctl_drop_pages(struct btrfs_io_ctl *io_ctl)\n{\n\tint i;\n\n\tio_ctl_unmap_page(io_ctl);\n\n\tfor (i = 0; i < io_ctl->num_pages; i++) {\n\t\tif (io_ctl->pages[i]) {\n\t\t\tbtrfs_page_clear_checked(io_ctl->fs_info,\n\t\t\t\t\tio_ctl->pages[i],\n\t\t\t\t\tpage_offset(io_ctl->pages[i]),\n\t\t\t\t\tPAGE_SIZE);\n\t\t\tunlock_page(io_ctl->pages[i]);\n\t\t\tput_page(io_ctl->pages[i]);\n\t\t}\n\t}\n}\n\nstatic int io_ctl_prepare_pages(struct btrfs_io_ctl *io_ctl, bool uptodate)\n{\n\tstruct page *page;\n\tstruct inode *inode = io_ctl->inode;\n\tgfp_t mask = btrfs_alloc_write_mask(inode->i_mapping);\n\tint i;\n\n\tfor (i = 0; i < io_ctl->num_pages; i++) {\n\t\tint ret;\n\n\t\tpage = find_or_create_page(inode->i_mapping, i, mask);\n\t\tif (!page) {\n\t\t\tio_ctl_drop_pages(io_ctl);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\tret = set_page_extent_mapped(page);\n\t\tif (ret < 0) {\n\t\t\tunlock_page(page);\n\t\t\tput_page(page);\n\t\t\tio_ctl_drop_pages(io_ctl);\n\t\t\treturn ret;\n\t\t}\n\n\t\tio_ctl->pages[i] = page;\n\t\tif (uptodate && !PageUptodate(page)) {\n\t\t\tbtrfs_read_folio(NULL, page_folio(page));\n\t\t\tlock_page(page);\n\t\t\tif (page->mapping != inode->i_mapping) {\n\t\t\t\tbtrfs_err(BTRFS_I(inode)->root->fs_info,\n\t\t\t\t\t  \"free space cache page truncated\");\n\t\t\t\tio_ctl_drop_pages(io_ctl);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t\tif (!PageUptodate(page)) {\n\t\t\t\tbtrfs_err(BTRFS_I(inode)->root->fs_info,\n\t\t\t\t\t   \"error reading free space cache\");\n\t\t\t\tio_ctl_drop_pages(io_ctl);\n\t\t\t\treturn -EIO;\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (i = 0; i < io_ctl->num_pages; i++)\n\t\tclear_page_dirty_for_io(io_ctl->pages[i]);\n\n\treturn 0;\n}\n\nstatic void io_ctl_set_generation(struct btrfs_io_ctl *io_ctl, u64 generation)\n{\n\tio_ctl_map_page(io_ctl, 1);\n\n\t \n\tio_ctl->cur += (sizeof(u32) * io_ctl->num_pages);\n\tio_ctl->size -= sizeof(u64) + (sizeof(u32) * io_ctl->num_pages);\n\n\tput_unaligned_le64(generation, io_ctl->cur);\n\tio_ctl->cur += sizeof(u64);\n}\n\nstatic int io_ctl_check_generation(struct btrfs_io_ctl *io_ctl, u64 generation)\n{\n\tu64 cache_gen;\n\n\t \n\tio_ctl->cur += sizeof(u32) * io_ctl->num_pages;\n\tio_ctl->size -= sizeof(u64) + (sizeof(u32) * io_ctl->num_pages);\n\n\tcache_gen = get_unaligned_le64(io_ctl->cur);\n\tif (cache_gen != generation) {\n\t\tbtrfs_err_rl(io_ctl->fs_info,\n\t\t\t\"space cache generation (%llu) does not match inode (%llu)\",\n\t\t\t\tcache_gen, generation);\n\t\tio_ctl_unmap_page(io_ctl);\n\t\treturn -EIO;\n\t}\n\tio_ctl->cur += sizeof(u64);\n\treturn 0;\n}\n\nstatic void io_ctl_set_crc(struct btrfs_io_ctl *io_ctl, int index)\n{\n\tu32 *tmp;\n\tu32 crc = ~(u32)0;\n\tunsigned offset = 0;\n\n\tif (index == 0)\n\t\toffset = sizeof(u32) * io_ctl->num_pages;\n\n\tcrc = btrfs_crc32c(crc, io_ctl->orig + offset, PAGE_SIZE - offset);\n\tbtrfs_crc32c_final(crc, (u8 *)&crc);\n\tio_ctl_unmap_page(io_ctl);\n\ttmp = page_address(io_ctl->pages[0]);\n\ttmp += index;\n\t*tmp = crc;\n}\n\nstatic int io_ctl_check_crc(struct btrfs_io_ctl *io_ctl, int index)\n{\n\tu32 *tmp, val;\n\tu32 crc = ~(u32)0;\n\tunsigned offset = 0;\n\n\tif (index == 0)\n\t\toffset = sizeof(u32) * io_ctl->num_pages;\n\n\ttmp = page_address(io_ctl->pages[0]);\n\ttmp += index;\n\tval = *tmp;\n\n\tio_ctl_map_page(io_ctl, 0);\n\tcrc = btrfs_crc32c(crc, io_ctl->orig + offset, PAGE_SIZE - offset);\n\tbtrfs_crc32c_final(crc, (u8 *)&crc);\n\tif (val != crc) {\n\t\tbtrfs_err_rl(io_ctl->fs_info,\n\t\t\t\"csum mismatch on free space cache\");\n\t\tio_ctl_unmap_page(io_ctl);\n\t\treturn -EIO;\n\t}\n\n\treturn 0;\n}\n\nstatic int io_ctl_add_entry(struct btrfs_io_ctl *io_ctl, u64 offset, u64 bytes,\n\t\t\t    void *bitmap)\n{\n\tstruct btrfs_free_space_entry *entry;\n\n\tif (!io_ctl->cur)\n\t\treturn -ENOSPC;\n\n\tentry = io_ctl->cur;\n\tput_unaligned_le64(offset, &entry->offset);\n\tput_unaligned_le64(bytes, &entry->bytes);\n\tentry->type = (bitmap) ? BTRFS_FREE_SPACE_BITMAP :\n\t\tBTRFS_FREE_SPACE_EXTENT;\n\tio_ctl->cur += sizeof(struct btrfs_free_space_entry);\n\tio_ctl->size -= sizeof(struct btrfs_free_space_entry);\n\n\tif (io_ctl->size >= sizeof(struct btrfs_free_space_entry))\n\t\treturn 0;\n\n\tio_ctl_set_crc(io_ctl, io_ctl->index - 1);\n\n\t \n\tif (io_ctl->index >= io_ctl->num_pages)\n\t\treturn 0;\n\n\t \n\tio_ctl_map_page(io_ctl, 1);\n\treturn 0;\n}\n\nstatic int io_ctl_add_bitmap(struct btrfs_io_ctl *io_ctl, void *bitmap)\n{\n\tif (!io_ctl->cur)\n\t\treturn -ENOSPC;\n\n\t \n\tif (io_ctl->cur != io_ctl->orig) {\n\t\tio_ctl_set_crc(io_ctl, io_ctl->index - 1);\n\t\tif (io_ctl->index >= io_ctl->num_pages)\n\t\t\treturn -ENOSPC;\n\t\tio_ctl_map_page(io_ctl, 0);\n\t}\n\n\tcopy_page(io_ctl->cur, bitmap);\n\tio_ctl_set_crc(io_ctl, io_ctl->index - 1);\n\tif (io_ctl->index < io_ctl->num_pages)\n\t\tio_ctl_map_page(io_ctl, 0);\n\treturn 0;\n}\n\nstatic void io_ctl_zero_remaining_pages(struct btrfs_io_ctl *io_ctl)\n{\n\t \n\tif (io_ctl->cur != io_ctl->orig)\n\t\tio_ctl_set_crc(io_ctl, io_ctl->index - 1);\n\telse\n\t\tio_ctl_unmap_page(io_ctl);\n\n\twhile (io_ctl->index < io_ctl->num_pages) {\n\t\tio_ctl_map_page(io_ctl, 1);\n\t\tio_ctl_set_crc(io_ctl, io_ctl->index - 1);\n\t}\n}\n\nstatic int io_ctl_read_entry(struct btrfs_io_ctl *io_ctl,\n\t\t\t    struct btrfs_free_space *entry, u8 *type)\n{\n\tstruct btrfs_free_space_entry *e;\n\tint ret;\n\n\tif (!io_ctl->cur) {\n\t\tret = io_ctl_check_crc(io_ctl, io_ctl->index);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\te = io_ctl->cur;\n\tentry->offset = get_unaligned_le64(&e->offset);\n\tentry->bytes = get_unaligned_le64(&e->bytes);\n\t*type = e->type;\n\tio_ctl->cur += sizeof(struct btrfs_free_space_entry);\n\tio_ctl->size -= sizeof(struct btrfs_free_space_entry);\n\n\tif (io_ctl->size >= sizeof(struct btrfs_free_space_entry))\n\t\treturn 0;\n\n\tio_ctl_unmap_page(io_ctl);\n\n\treturn 0;\n}\n\nstatic int io_ctl_read_bitmap(struct btrfs_io_ctl *io_ctl,\n\t\t\t      struct btrfs_free_space *entry)\n{\n\tint ret;\n\n\tret = io_ctl_check_crc(io_ctl, io_ctl->index);\n\tif (ret)\n\t\treturn ret;\n\n\tcopy_page(entry->bitmap, io_ctl->cur);\n\tio_ctl_unmap_page(io_ctl);\n\n\treturn 0;\n}\n\nstatic void recalculate_thresholds(struct btrfs_free_space_ctl *ctl)\n{\n\tstruct btrfs_block_group *block_group = ctl->block_group;\n\tu64 max_bytes;\n\tu64 bitmap_bytes;\n\tu64 extent_bytes;\n\tu64 size = block_group->length;\n\tu64 bytes_per_bg = BITS_PER_BITMAP * ctl->unit;\n\tu64 max_bitmaps = div64_u64(size + bytes_per_bg - 1, bytes_per_bg);\n\n\tmax_bitmaps = max_t(u64, max_bitmaps, 1);\n\n\tif (ctl->total_bitmaps > max_bitmaps)\n\t\tbtrfs_err(block_group->fs_info,\n\"invalid free space control: bg start=%llu len=%llu total_bitmaps=%u unit=%u max_bitmaps=%llu bytes_per_bg=%llu\",\n\t\t\t  block_group->start, block_group->length,\n\t\t\t  ctl->total_bitmaps, ctl->unit, max_bitmaps,\n\t\t\t  bytes_per_bg);\n\tASSERT(ctl->total_bitmaps <= max_bitmaps);\n\n\t \n\tif (size < SZ_1G)\n\t\tmax_bytes = MAX_CACHE_BYTES_PER_GIG;\n\telse\n\t\tmax_bytes = MAX_CACHE_BYTES_PER_GIG * div_u64(size, SZ_1G);\n\n\tbitmap_bytes = ctl->total_bitmaps * ctl->unit;\n\n\t \n\textent_bytes = max_bytes - bitmap_bytes;\n\textent_bytes = min_t(u64, extent_bytes, max_bytes >> 1);\n\n\tctl->extents_thresh =\n\t\tdiv_u64(extent_bytes, sizeof(struct btrfs_free_space));\n}\n\nstatic int __load_free_space_cache(struct btrfs_root *root, struct inode *inode,\n\t\t\t\t   struct btrfs_free_space_ctl *ctl,\n\t\t\t\t   struct btrfs_path *path, u64 offset)\n{\n\tstruct btrfs_fs_info *fs_info = root->fs_info;\n\tstruct btrfs_free_space_header *header;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_io_ctl io_ctl;\n\tstruct btrfs_key key;\n\tstruct btrfs_free_space *e, *n;\n\tLIST_HEAD(bitmaps);\n\tu64 num_entries;\n\tu64 num_bitmaps;\n\tu64 generation;\n\tu8 type;\n\tint ret = 0;\n\n\t \n\tif (!i_size_read(inode))\n\t\treturn 0;\n\n\tkey.objectid = BTRFS_FREE_SPACE_OBJECTID;\n\tkey.offset = offset;\n\tkey.type = 0;\n\n\tret = btrfs_search_slot(NULL, root, &key, path, 0, 0);\n\tif (ret < 0)\n\t\treturn 0;\n\telse if (ret > 0) {\n\t\tbtrfs_release_path(path);\n\t\treturn 0;\n\t}\n\n\tret = -1;\n\n\tleaf = path->nodes[0];\n\theader = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\tstruct btrfs_free_space_header);\n\tnum_entries = btrfs_free_space_entries(leaf, header);\n\tnum_bitmaps = btrfs_free_space_bitmaps(leaf, header);\n\tgeneration = btrfs_free_space_generation(leaf, header);\n\tbtrfs_release_path(path);\n\n\tif (!BTRFS_I(inode)->generation) {\n\t\tbtrfs_info(fs_info,\n\t\t\t   \"the free space cache file (%llu) is invalid, skip it\",\n\t\t\t   offset);\n\t\treturn 0;\n\t}\n\n\tif (BTRFS_I(inode)->generation != generation) {\n\t\tbtrfs_err(fs_info,\n\t\t\t  \"free space inode generation (%llu) did not match free space cache generation (%llu)\",\n\t\t\t  BTRFS_I(inode)->generation, generation);\n\t\treturn 0;\n\t}\n\n\tif (!num_entries)\n\t\treturn 0;\n\n\tret = io_ctl_init(&io_ctl, inode, 0);\n\tif (ret)\n\t\treturn ret;\n\n\treadahead_cache(inode);\n\n\tret = io_ctl_prepare_pages(&io_ctl, true);\n\tif (ret)\n\t\tgoto out;\n\n\tret = io_ctl_check_crc(&io_ctl, 0);\n\tif (ret)\n\t\tgoto free_cache;\n\n\tret = io_ctl_check_generation(&io_ctl, generation);\n\tif (ret)\n\t\tgoto free_cache;\n\n\twhile (num_entries) {\n\t\te = kmem_cache_zalloc(btrfs_free_space_cachep,\n\t\t\t\t      GFP_NOFS);\n\t\tif (!e) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto free_cache;\n\t\t}\n\n\t\tret = io_ctl_read_entry(&io_ctl, e, &type);\n\t\tif (ret) {\n\t\t\tkmem_cache_free(btrfs_free_space_cachep, e);\n\t\t\tgoto free_cache;\n\t\t}\n\n\t\tif (!e->bytes) {\n\t\t\tret = -1;\n\t\t\tkmem_cache_free(btrfs_free_space_cachep, e);\n\t\t\tgoto free_cache;\n\t\t}\n\n\t\tif (type == BTRFS_FREE_SPACE_EXTENT) {\n\t\t\tspin_lock(&ctl->tree_lock);\n\t\t\tret = link_free_space(ctl, e);\n\t\t\tspin_unlock(&ctl->tree_lock);\n\t\t\tif (ret) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t\"Duplicate entries in free space cache, dumping\");\n\t\t\t\tkmem_cache_free(btrfs_free_space_cachep, e);\n\t\t\t\tgoto free_cache;\n\t\t\t}\n\t\t} else {\n\t\t\tASSERT(num_bitmaps);\n\t\t\tnum_bitmaps--;\n\t\t\te->bitmap = kmem_cache_zalloc(\n\t\t\t\t\tbtrfs_free_space_bitmap_cachep, GFP_NOFS);\n\t\t\tif (!e->bitmap) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tkmem_cache_free(\n\t\t\t\t\tbtrfs_free_space_cachep, e);\n\t\t\t\tgoto free_cache;\n\t\t\t}\n\t\t\tspin_lock(&ctl->tree_lock);\n\t\t\tret = link_free_space(ctl, e);\n\t\t\tif (ret) {\n\t\t\t\tspin_unlock(&ctl->tree_lock);\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\t\t\"Duplicate entries in free space cache, dumping\");\n\t\t\t\tkmem_cache_free(btrfs_free_space_cachep, e);\n\t\t\t\tgoto free_cache;\n\t\t\t}\n\t\t\tctl->total_bitmaps++;\n\t\t\trecalculate_thresholds(ctl);\n\t\t\tspin_unlock(&ctl->tree_lock);\n\t\t\tlist_add_tail(&e->list, &bitmaps);\n\t\t}\n\n\t\tnum_entries--;\n\t}\n\n\tio_ctl_unmap_page(&io_ctl);\n\n\t \n\tlist_for_each_entry_safe(e, n, &bitmaps, list) {\n\t\tlist_del_init(&e->list);\n\t\tret = io_ctl_read_bitmap(&io_ctl, e);\n\t\tif (ret)\n\t\t\tgoto free_cache;\n\t}\n\n\tio_ctl_drop_pages(&io_ctl);\n\tret = 1;\nout:\n\tio_ctl_free(&io_ctl);\n\treturn ret;\nfree_cache:\n\tio_ctl_drop_pages(&io_ctl);\n\n\tspin_lock(&ctl->tree_lock);\n\t__btrfs_remove_free_space_cache(ctl);\n\tspin_unlock(&ctl->tree_lock);\n\tgoto out;\n}\n\nstatic int copy_free_space_cache(struct btrfs_block_group *block_group,\n\t\t\t\t struct btrfs_free_space_ctl *ctl)\n{\n\tstruct btrfs_free_space *info;\n\tstruct rb_node *n;\n\tint ret = 0;\n\n\twhile (!ret && (n = rb_first(&ctl->free_space_offset)) != NULL) {\n\t\tinfo = rb_entry(n, struct btrfs_free_space, offset_index);\n\t\tif (!info->bitmap) {\n\t\t\tconst u64 offset = info->offset;\n\t\t\tconst u64 bytes = info->bytes;\n\n\t\t\tunlink_free_space(ctl, info, true);\n\t\t\tspin_unlock(&ctl->tree_lock);\n\t\t\tkmem_cache_free(btrfs_free_space_cachep, info);\n\t\t\tret = btrfs_add_free_space(block_group, offset, bytes);\n\t\t\tspin_lock(&ctl->tree_lock);\n\t\t} else {\n\t\t\tu64 offset = info->offset;\n\t\t\tu64 bytes = ctl->unit;\n\n\t\t\tret = search_bitmap(ctl, info, &offset, &bytes, false);\n\t\t\tif (ret == 0) {\n\t\t\t\tbitmap_clear_bits(ctl, info, offset, bytes, true);\n\t\t\t\tspin_unlock(&ctl->tree_lock);\n\t\t\t\tret = btrfs_add_free_space(block_group, offset,\n\t\t\t\t\t\t\t   bytes);\n\t\t\t\tspin_lock(&ctl->tree_lock);\n\t\t\t} else {\n\t\t\t\tfree_bitmap(ctl, info);\n\t\t\t\tret = 0;\n\t\t\t}\n\t\t}\n\t\tcond_resched_lock(&ctl->tree_lock);\n\t}\n\treturn ret;\n}\n\nstatic struct lock_class_key btrfs_free_space_inode_key;\n\nint load_free_space_cache(struct btrfs_block_group *block_group)\n{\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tstruct btrfs_free_space_ctl tmp_ctl = {};\n\tstruct inode *inode;\n\tstruct btrfs_path *path;\n\tint ret = 0;\n\tbool matched;\n\tu64 used = block_group->used;\n\n\t \n\tbtrfs_init_free_space_ctl(block_group, &tmp_ctl);\n\n\t \n\tspin_lock(&block_group->lock);\n\tif (block_group->disk_cache_state != BTRFS_DC_WRITTEN) {\n\t\tspin_unlock(&block_group->lock);\n\t\treturn 0;\n\t}\n\tspin_unlock(&block_group->lock);\n\n\tpath = btrfs_alloc_path();\n\tif (!path)\n\t\treturn 0;\n\tpath->search_commit_root = 1;\n\tpath->skip_locking = 1;\n\n\t \n\tinode = lookup_free_space_inode(block_group, path);\n\tif (IS_ERR(inode)) {\n\t\tbtrfs_free_path(path);\n\t\treturn 0;\n\t}\n\n\t \n\tspin_lock(&block_group->lock);\n\tif (block_group->disk_cache_state != BTRFS_DC_WRITTEN) {\n\t\tspin_unlock(&block_group->lock);\n\t\tbtrfs_free_path(path);\n\t\tgoto out;\n\t}\n\tspin_unlock(&block_group->lock);\n\n\t \n\tlockdep_set_class(&(&inode->i_data)->invalidate_lock,\n\t\t\t  &btrfs_free_space_inode_key);\n\n\tret = __load_free_space_cache(fs_info->tree_root, inode, &tmp_ctl,\n\t\t\t\t      path, block_group->start);\n\tbtrfs_free_path(path);\n\tif (ret <= 0)\n\t\tgoto out;\n\n\tmatched = (tmp_ctl.free_space == (block_group->length - used -\n\t\t\t\t\t  block_group->bytes_super));\n\n\tif (matched) {\n\t\tspin_lock(&tmp_ctl.tree_lock);\n\t\tret = copy_free_space_cache(block_group, &tmp_ctl);\n\t\tspin_unlock(&tmp_ctl.tree_lock);\n\t\t \n\t\tif (ret == 0)\n\t\t\tret = 1;\n\t} else {\n\t\t \n\t\tspin_lock(&tmp_ctl.tree_lock);\n\t\t__btrfs_remove_free_space_cache(&tmp_ctl);\n\t\tspin_unlock(&tmp_ctl.tree_lock);\n\t\tbtrfs_warn(fs_info,\n\t\t\t   \"block group %llu has wrong amount of free space\",\n\t\t\t   block_group->start);\n\t\tret = -1;\n\t}\nout:\n\tif (ret < 0) {\n\t\t \n\t\tspin_lock(&block_group->lock);\n\t\tblock_group->disk_cache_state = BTRFS_DC_CLEAR;\n\t\tspin_unlock(&block_group->lock);\n\t\tret = 0;\n\n\t\tbtrfs_warn(fs_info,\n\t\t\t   \"failed to load free space cache for block group %llu, rebuilding it now\",\n\t\t\t   block_group->start);\n\t}\n\n\tspin_lock(&ctl->tree_lock);\n\tbtrfs_discard_update_discardable(block_group);\n\tspin_unlock(&ctl->tree_lock);\n\tiput(inode);\n\treturn ret;\n}\n\nstatic noinline_for_stack\nint write_cache_extent_entries(struct btrfs_io_ctl *io_ctl,\n\t\t\t      struct btrfs_free_space_ctl *ctl,\n\t\t\t      struct btrfs_block_group *block_group,\n\t\t\t      int *entries, int *bitmaps,\n\t\t\t      struct list_head *bitmap_list)\n{\n\tint ret;\n\tstruct btrfs_free_cluster *cluster = NULL;\n\tstruct btrfs_free_cluster *cluster_locked = NULL;\n\tstruct rb_node *node = rb_first(&ctl->free_space_offset);\n\tstruct btrfs_trim_range *trim_entry;\n\n\t \n\tif (block_group && !list_empty(&block_group->cluster_list)) {\n\t\tcluster = list_entry(block_group->cluster_list.next,\n\t\t\t\t     struct btrfs_free_cluster,\n\t\t\t\t     block_group_list);\n\t}\n\n\tif (!node && cluster) {\n\t\tcluster_locked = cluster;\n\t\tspin_lock(&cluster_locked->lock);\n\t\tnode = rb_first(&cluster->root);\n\t\tcluster = NULL;\n\t}\n\n\t \n\twhile (node) {\n\t\tstruct btrfs_free_space *e;\n\n\t\te = rb_entry(node, struct btrfs_free_space, offset_index);\n\t\t*entries += 1;\n\n\t\tret = io_ctl_add_entry(io_ctl, e->offset, e->bytes,\n\t\t\t\t       e->bitmap);\n\t\tif (ret)\n\t\t\tgoto fail;\n\n\t\tif (e->bitmap) {\n\t\t\tlist_add_tail(&e->list, bitmap_list);\n\t\t\t*bitmaps += 1;\n\t\t}\n\t\tnode = rb_next(node);\n\t\tif (!node && cluster) {\n\t\t\tnode = rb_first(&cluster->root);\n\t\t\tcluster_locked = cluster;\n\t\t\tspin_lock(&cluster_locked->lock);\n\t\t\tcluster = NULL;\n\t\t}\n\t}\n\tif (cluster_locked) {\n\t\tspin_unlock(&cluster_locked->lock);\n\t\tcluster_locked = NULL;\n\t}\n\n\t \n\tlist_for_each_entry(trim_entry, &ctl->trimming_ranges, list) {\n\t\tret = io_ctl_add_entry(io_ctl, trim_entry->start,\n\t\t\t\t       trim_entry->bytes, NULL);\n\t\tif (ret)\n\t\t\tgoto fail;\n\t\t*entries += 1;\n\t}\n\n\treturn 0;\nfail:\n\tif (cluster_locked)\n\t\tspin_unlock(&cluster_locked->lock);\n\treturn -ENOSPC;\n}\n\nstatic noinline_for_stack int\nupdate_cache_item(struct btrfs_trans_handle *trans,\n\t\t  struct btrfs_root *root,\n\t\t  struct inode *inode,\n\t\t  struct btrfs_path *path, u64 offset,\n\t\t  int entries, int bitmaps)\n{\n\tstruct btrfs_key key;\n\tstruct btrfs_free_space_header *header;\n\tstruct extent_buffer *leaf;\n\tint ret;\n\n\tkey.objectid = BTRFS_FREE_SPACE_OBJECTID;\n\tkey.offset = offset;\n\tkey.type = 0;\n\n\tret = btrfs_search_slot(trans, root, &key, path, 0, 1);\n\tif (ret < 0) {\n\t\tclear_extent_bit(&BTRFS_I(inode)->io_tree, 0, inode->i_size - 1,\n\t\t\t\t EXTENT_DELALLOC, NULL);\n\t\tgoto fail;\n\t}\n\tleaf = path->nodes[0];\n\tif (ret > 0) {\n\t\tstruct btrfs_key found_key;\n\t\tASSERT(path->slots[0]);\n\t\tpath->slots[0]--;\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, path->slots[0]);\n\t\tif (found_key.objectid != BTRFS_FREE_SPACE_OBJECTID ||\n\t\t    found_key.offset != offset) {\n\t\t\tclear_extent_bit(&BTRFS_I(inode)->io_tree, 0,\n\t\t\t\t\t inode->i_size - 1, EXTENT_DELALLOC,\n\t\t\t\t\t NULL);\n\t\t\tbtrfs_release_path(path);\n\t\t\tgoto fail;\n\t\t}\n\t}\n\n\tBTRFS_I(inode)->generation = trans->transid;\n\theader = btrfs_item_ptr(leaf, path->slots[0],\n\t\t\t\tstruct btrfs_free_space_header);\n\tbtrfs_set_free_space_entries(leaf, header, entries);\n\tbtrfs_set_free_space_bitmaps(leaf, header, bitmaps);\n\tbtrfs_set_free_space_generation(leaf, header, trans->transid);\n\tbtrfs_mark_buffer_dirty(trans, leaf);\n\tbtrfs_release_path(path);\n\n\treturn 0;\n\nfail:\n\treturn -1;\n}\n\nstatic noinline_for_stack int write_pinned_extent_entries(\n\t\t\t    struct btrfs_trans_handle *trans,\n\t\t\t    struct btrfs_block_group *block_group,\n\t\t\t    struct btrfs_io_ctl *io_ctl,\n\t\t\t    int *entries)\n{\n\tu64 start, extent_start, extent_end, len;\n\tstruct extent_io_tree *unpin = NULL;\n\tint ret;\n\n\tif (!block_group)\n\t\treturn 0;\n\n\t \n\tunpin = &trans->transaction->pinned_extents;\n\n\tstart = block_group->start;\n\n\twhile (start < block_group->start + block_group->length) {\n\t\tif (!find_first_extent_bit(unpin, start,\n\t\t\t\t\t   &extent_start, &extent_end,\n\t\t\t\t\t   EXTENT_DIRTY, NULL))\n\t\t\treturn 0;\n\n\t\t \n\t\tif (extent_start >= block_group->start + block_group->length)\n\t\t\treturn 0;\n\n\t\textent_start = max(extent_start, start);\n\t\textent_end = min(block_group->start + block_group->length,\n\t\t\t\t extent_end + 1);\n\t\tlen = extent_end - extent_start;\n\n\t\t*entries += 1;\n\t\tret = io_ctl_add_entry(io_ctl, extent_start, len, NULL);\n\t\tif (ret)\n\t\t\treturn -ENOSPC;\n\n\t\tstart = extent_end;\n\t}\n\n\treturn 0;\n}\n\nstatic noinline_for_stack int\nwrite_bitmap_entries(struct btrfs_io_ctl *io_ctl, struct list_head *bitmap_list)\n{\n\tstruct btrfs_free_space *entry, *next;\n\tint ret;\n\n\t \n\tlist_for_each_entry_safe(entry, next, bitmap_list, list) {\n\t\tret = io_ctl_add_bitmap(io_ctl, entry->bitmap);\n\t\tif (ret)\n\t\t\treturn -ENOSPC;\n\t\tlist_del_init(&entry->list);\n\t}\n\n\treturn 0;\n}\n\nstatic int flush_dirty_cache(struct inode *inode)\n{\n\tint ret;\n\n\tret = btrfs_wait_ordered_range(inode, 0, (u64)-1);\n\tif (ret)\n\t\tclear_extent_bit(&BTRFS_I(inode)->io_tree, 0, inode->i_size - 1,\n\t\t\t\t EXTENT_DELALLOC, NULL);\n\n\treturn ret;\n}\n\nstatic void noinline_for_stack\ncleanup_bitmap_list(struct list_head *bitmap_list)\n{\n\tstruct btrfs_free_space *entry, *next;\n\n\tlist_for_each_entry_safe(entry, next, bitmap_list, list)\n\t\tlist_del_init(&entry->list);\n}\n\nstatic void noinline_for_stack\ncleanup_write_cache_enospc(struct inode *inode,\n\t\t\t   struct btrfs_io_ctl *io_ctl,\n\t\t\t   struct extent_state **cached_state)\n{\n\tio_ctl_drop_pages(io_ctl);\n\tunlock_extent(&BTRFS_I(inode)->io_tree, 0, i_size_read(inode) - 1,\n\t\t      cached_state);\n}\n\nstatic int __btrfs_wait_cache_io(struct btrfs_root *root,\n\t\t\t\t struct btrfs_trans_handle *trans,\n\t\t\t\t struct btrfs_block_group *block_group,\n\t\t\t\t struct btrfs_io_ctl *io_ctl,\n\t\t\t\t struct btrfs_path *path, u64 offset)\n{\n\tint ret;\n\tstruct inode *inode = io_ctl->inode;\n\n\tif (!inode)\n\t\treturn 0;\n\n\t \n\tret = flush_dirty_cache(inode);\n\tif (ret)\n\t\tgoto out;\n\n\t \n\tret = update_cache_item(trans, root, inode, path, offset,\n\t\t\t\tio_ctl->entries, io_ctl->bitmaps);\nout:\n\tif (ret) {\n\t\tinvalidate_inode_pages2(inode->i_mapping);\n\t\tBTRFS_I(inode)->generation = 0;\n\t\tif (block_group)\n\t\t\tbtrfs_debug(root->fs_info,\n\t  \"failed to write free space cache for block group %llu error %d\",\n\t\t\t\t  block_group->start, ret);\n\t}\n\tbtrfs_update_inode(trans, root, BTRFS_I(inode));\n\n\tif (block_group) {\n\t\t \n\t\tspin_lock(&trans->transaction->dirty_bgs_lock);\n\n\t\t \n\t\tspin_lock(&block_group->lock);\n\n\t\t \n\t\tif (!ret && list_empty(&block_group->dirty_list))\n\t\t\tblock_group->disk_cache_state = BTRFS_DC_WRITTEN;\n\t\telse if (ret)\n\t\t\tblock_group->disk_cache_state = BTRFS_DC_ERROR;\n\n\t\tspin_unlock(&block_group->lock);\n\t\tspin_unlock(&trans->transaction->dirty_bgs_lock);\n\t\tio_ctl->inode = NULL;\n\t\tiput(inode);\n\t}\n\n\treturn ret;\n\n}\n\nint btrfs_wait_cache_io(struct btrfs_trans_handle *trans,\n\t\t\tstruct btrfs_block_group *block_group,\n\t\t\tstruct btrfs_path *path)\n{\n\treturn __btrfs_wait_cache_io(block_group->fs_info->tree_root, trans,\n\t\t\t\t     block_group, &block_group->io_ctl,\n\t\t\t\t     path, block_group->start);\n}\n\n \nstatic int __btrfs_write_out_cache(struct btrfs_root *root, struct inode *inode,\n\t\t\t\t   struct btrfs_free_space_ctl *ctl,\n\t\t\t\t   struct btrfs_block_group *block_group,\n\t\t\t\t   struct btrfs_io_ctl *io_ctl,\n\t\t\t\t   struct btrfs_trans_handle *trans)\n{\n\tstruct extent_state *cached_state = NULL;\n\tLIST_HEAD(bitmap_list);\n\tint entries = 0;\n\tint bitmaps = 0;\n\tint ret;\n\tint must_iput = 0;\n\n\tif (!i_size_read(inode))\n\t\treturn -EIO;\n\n\tWARN_ON(io_ctl->pages);\n\tret = io_ctl_init(io_ctl, inode, 1);\n\tif (ret)\n\t\treturn ret;\n\n\tif (block_group && (block_group->flags & BTRFS_BLOCK_GROUP_DATA)) {\n\t\tdown_write(&block_group->data_rwsem);\n\t\tspin_lock(&block_group->lock);\n\t\tif (block_group->delalloc_bytes) {\n\t\t\tblock_group->disk_cache_state = BTRFS_DC_WRITTEN;\n\t\t\tspin_unlock(&block_group->lock);\n\t\t\tup_write(&block_group->data_rwsem);\n\t\t\tBTRFS_I(inode)->generation = 0;\n\t\t\tret = 0;\n\t\t\tmust_iput = 1;\n\t\t\tgoto out;\n\t\t}\n\t\tspin_unlock(&block_group->lock);\n\t}\n\n\t \n\tret = io_ctl_prepare_pages(io_ctl, false);\n\tif (ret)\n\t\tgoto out_unlock;\n\n\tlock_extent(&BTRFS_I(inode)->io_tree, 0, i_size_read(inode) - 1,\n\t\t    &cached_state);\n\n\tio_ctl_set_generation(io_ctl, trans->transid);\n\n\tmutex_lock(&ctl->cache_writeout_mutex);\n\t \n\tspin_lock(&ctl->tree_lock);\n\tret = write_cache_extent_entries(io_ctl, ctl,\n\t\t\t\t\t block_group, &entries, &bitmaps,\n\t\t\t\t\t &bitmap_list);\n\tif (ret)\n\t\tgoto out_nospc_locked;\n\n\t \n\tret = write_pinned_extent_entries(trans, block_group, io_ctl, &entries);\n\tif (ret)\n\t\tgoto out_nospc_locked;\n\n\t \n\tret = write_bitmap_entries(io_ctl, &bitmap_list);\n\tspin_unlock(&ctl->tree_lock);\n\tmutex_unlock(&ctl->cache_writeout_mutex);\n\tif (ret)\n\t\tgoto out_nospc;\n\n\t \n\tio_ctl_zero_remaining_pages(io_ctl);\n\n\t \n\tret = btrfs_dirty_pages(BTRFS_I(inode), io_ctl->pages,\n\t\t\t\tio_ctl->num_pages, 0, i_size_read(inode),\n\t\t\t\t&cached_state, false);\n\tif (ret)\n\t\tgoto out_nospc;\n\n\tif (block_group && (block_group->flags & BTRFS_BLOCK_GROUP_DATA))\n\t\tup_write(&block_group->data_rwsem);\n\t \n\tio_ctl_drop_pages(io_ctl);\n\tio_ctl_free(io_ctl);\n\n\tunlock_extent(&BTRFS_I(inode)->io_tree, 0, i_size_read(inode) - 1,\n\t\t      &cached_state);\n\n\t \n\tio_ctl->entries = entries;\n\tio_ctl->bitmaps = bitmaps;\n\n\tret = btrfs_fdatawrite_range(inode, 0, (u64)-1);\n\tif (ret)\n\t\tgoto out;\n\n\treturn 0;\n\nout_nospc_locked:\n\tcleanup_bitmap_list(&bitmap_list);\n\tspin_unlock(&ctl->tree_lock);\n\tmutex_unlock(&ctl->cache_writeout_mutex);\n\nout_nospc:\n\tcleanup_write_cache_enospc(inode, io_ctl, &cached_state);\n\nout_unlock:\n\tif (block_group && (block_group->flags & BTRFS_BLOCK_GROUP_DATA))\n\t\tup_write(&block_group->data_rwsem);\n\nout:\n\tio_ctl->inode = NULL;\n\tio_ctl_free(io_ctl);\n\tif (ret) {\n\t\tinvalidate_inode_pages2(inode->i_mapping);\n\t\tBTRFS_I(inode)->generation = 0;\n\t}\n\tbtrfs_update_inode(trans, root, BTRFS_I(inode));\n\tif (must_iput)\n\t\tiput(inode);\n\treturn ret;\n}\n\nint btrfs_write_out_cache(struct btrfs_trans_handle *trans,\n\t\t\t  struct btrfs_block_group *block_group,\n\t\t\t  struct btrfs_path *path)\n{\n\tstruct btrfs_fs_info *fs_info = trans->fs_info;\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tstruct inode *inode;\n\tint ret = 0;\n\n\tspin_lock(&block_group->lock);\n\tif (block_group->disk_cache_state < BTRFS_DC_SETUP) {\n\t\tspin_unlock(&block_group->lock);\n\t\treturn 0;\n\t}\n\tspin_unlock(&block_group->lock);\n\n\tinode = lookup_free_space_inode(block_group, path);\n\tif (IS_ERR(inode))\n\t\treturn 0;\n\n\tret = __btrfs_write_out_cache(fs_info->tree_root, inode, ctl,\n\t\t\t\tblock_group, &block_group->io_ctl, trans);\n\tif (ret) {\n\t\tbtrfs_debug(fs_info,\n\t  \"failed to write free space cache for block group %llu error %d\",\n\t\t\t  block_group->start, ret);\n\t\tspin_lock(&block_group->lock);\n\t\tblock_group->disk_cache_state = BTRFS_DC_ERROR;\n\t\tspin_unlock(&block_group->lock);\n\n\t\tblock_group->io_ctl.inode = NULL;\n\t\tiput(inode);\n\t}\n\n\t \n\n\treturn ret;\n}\n\nstatic inline unsigned long offset_to_bit(u64 bitmap_start, u32 unit,\n\t\t\t\t\t  u64 offset)\n{\n\tASSERT(offset >= bitmap_start);\n\toffset -= bitmap_start;\n\treturn (unsigned long)(div_u64(offset, unit));\n}\n\nstatic inline unsigned long bytes_to_bits(u64 bytes, u32 unit)\n{\n\treturn (unsigned long)(div_u64(bytes, unit));\n}\n\nstatic inline u64 offset_to_bitmap(struct btrfs_free_space_ctl *ctl,\n\t\t\t\t   u64 offset)\n{\n\tu64 bitmap_start;\n\tu64 bytes_per_bitmap;\n\n\tbytes_per_bitmap = BITS_PER_BITMAP * ctl->unit;\n\tbitmap_start = offset - ctl->start;\n\tbitmap_start = div64_u64(bitmap_start, bytes_per_bitmap);\n\tbitmap_start *= bytes_per_bitmap;\n\tbitmap_start += ctl->start;\n\n\treturn bitmap_start;\n}\n\nstatic int tree_insert_offset(struct btrfs_free_space_ctl *ctl,\n\t\t\t      struct btrfs_free_cluster *cluster,\n\t\t\t      struct btrfs_free_space *new_entry)\n{\n\tstruct rb_root *root;\n\tstruct rb_node **p;\n\tstruct rb_node *parent = NULL;\n\n\tlockdep_assert_held(&ctl->tree_lock);\n\n\tif (cluster) {\n\t\tlockdep_assert_held(&cluster->lock);\n\t\troot = &cluster->root;\n\t} else {\n\t\troot = &ctl->free_space_offset;\n\t}\n\n\tp = &root->rb_node;\n\n\twhile (*p) {\n\t\tstruct btrfs_free_space *info;\n\n\t\tparent = *p;\n\t\tinfo = rb_entry(parent, struct btrfs_free_space, offset_index);\n\n\t\tif (new_entry->offset < info->offset) {\n\t\t\tp = &(*p)->rb_left;\n\t\t} else if (new_entry->offset > info->offset) {\n\t\t\tp = &(*p)->rb_right;\n\t\t} else {\n\t\t\t \n\t\t\tif (new_entry->bitmap) {\n\t\t\t\tif (info->bitmap) {\n\t\t\t\t\tWARN_ON_ONCE(1);\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\t}\n\t\t\t\tp = &(*p)->rb_right;\n\t\t\t} else {\n\t\t\t\tif (!info->bitmap) {\n\t\t\t\t\tWARN_ON_ONCE(1);\n\t\t\t\t\treturn -EEXIST;\n\t\t\t\t}\n\t\t\t\tp = &(*p)->rb_left;\n\t\t\t}\n\t\t}\n\t}\n\n\trb_link_node(&new_entry->offset_index, parent, p);\n\trb_insert_color(&new_entry->offset_index, root);\n\n\treturn 0;\n}\n\n \nstatic inline u64 get_max_extent_size(const struct btrfs_free_space *entry)\n{\n\tif (entry->bitmap && entry->max_extent_size)\n\t\treturn entry->max_extent_size;\n\treturn entry->bytes;\n}\n\n \nstatic bool entry_less(struct rb_node *node, const struct rb_node *parent)\n{\n\tconst struct btrfs_free_space *entry, *exist;\n\n\tentry = rb_entry(node, struct btrfs_free_space, bytes_index);\n\texist = rb_entry(parent, struct btrfs_free_space, bytes_index);\n\treturn get_max_extent_size(exist) < get_max_extent_size(entry);\n}\n\n \nstatic struct btrfs_free_space *\ntree_search_offset(struct btrfs_free_space_ctl *ctl,\n\t\t   u64 offset, int bitmap_only, int fuzzy)\n{\n\tstruct rb_node *n = ctl->free_space_offset.rb_node;\n\tstruct btrfs_free_space *entry = NULL, *prev = NULL;\n\n\tlockdep_assert_held(&ctl->tree_lock);\n\n\t \n\twhile (n) {\n\t\tentry = rb_entry(n, struct btrfs_free_space, offset_index);\n\t\tprev = entry;\n\n\t\tif (offset < entry->offset)\n\t\t\tn = n->rb_left;\n\t\telse if (offset > entry->offset)\n\t\t\tn = n->rb_right;\n\t\telse\n\t\t\tbreak;\n\n\t\tentry = NULL;\n\t}\n\n\tif (bitmap_only) {\n\t\tif (!entry)\n\t\t\treturn NULL;\n\t\tif (entry->bitmap)\n\t\t\treturn entry;\n\n\t\t \n\t\tn = rb_next(n);\n\t\tif (!n)\n\t\t\treturn NULL;\n\t\tentry = rb_entry(n, struct btrfs_free_space, offset_index);\n\t\tif (entry->offset != offset)\n\t\t\treturn NULL;\n\n\t\tWARN_ON(!entry->bitmap);\n\t\treturn entry;\n\t} else if (entry) {\n\t\tif (entry->bitmap) {\n\t\t\t \n\t\t\tn = rb_prev(&entry->offset_index);\n\t\t\tif (n) {\n\t\t\t\tprev = rb_entry(n, struct btrfs_free_space,\n\t\t\t\t\t\toffset_index);\n\t\t\t\tif (!prev->bitmap &&\n\t\t\t\t    prev->offset + prev->bytes > offset)\n\t\t\t\t\tentry = prev;\n\t\t\t}\n\t\t}\n\t\treturn entry;\n\t}\n\n\tif (!prev)\n\t\treturn NULL;\n\n\t \n\tentry = prev;\n\tif (entry->offset > offset) {\n\t\tn = rb_prev(&entry->offset_index);\n\t\tif (n) {\n\t\t\tentry = rb_entry(n, struct btrfs_free_space,\n\t\t\t\t\toffset_index);\n\t\t\tASSERT(entry->offset <= offset);\n\t\t} else {\n\t\t\tif (fuzzy)\n\t\t\t\treturn entry;\n\t\t\telse\n\t\t\t\treturn NULL;\n\t\t}\n\t}\n\n\tif (entry->bitmap) {\n\t\tn = rb_prev(&entry->offset_index);\n\t\tif (n) {\n\t\t\tprev = rb_entry(n, struct btrfs_free_space,\n\t\t\t\t\toffset_index);\n\t\t\tif (!prev->bitmap &&\n\t\t\t    prev->offset + prev->bytes > offset)\n\t\t\t\treturn prev;\n\t\t}\n\t\tif (entry->offset + BITS_PER_BITMAP * ctl->unit > offset)\n\t\t\treturn entry;\n\t} else if (entry->offset + entry->bytes > offset)\n\t\treturn entry;\n\n\tif (!fuzzy)\n\t\treturn NULL;\n\n\twhile (1) {\n\t\tn = rb_next(&entry->offset_index);\n\t\tif (!n)\n\t\t\treturn NULL;\n\t\tentry = rb_entry(n, struct btrfs_free_space, offset_index);\n\t\tif (entry->bitmap) {\n\t\t\tif (entry->offset + BITS_PER_BITMAP *\n\t\t\t    ctl->unit > offset)\n\t\t\t\tbreak;\n\t\t} else {\n\t\t\tif (entry->offset + entry->bytes > offset)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\treturn entry;\n}\n\nstatic inline void unlink_free_space(struct btrfs_free_space_ctl *ctl,\n\t\t\t\t     struct btrfs_free_space *info,\n\t\t\t\t     bool update_stat)\n{\n\tlockdep_assert_held(&ctl->tree_lock);\n\n\trb_erase(&info->offset_index, &ctl->free_space_offset);\n\trb_erase_cached(&info->bytes_index, &ctl->free_space_bytes);\n\tctl->free_extents--;\n\n\tif (!info->bitmap && !btrfs_free_space_trimmed(info)) {\n\t\tctl->discardable_extents[BTRFS_STAT_CURR]--;\n\t\tctl->discardable_bytes[BTRFS_STAT_CURR] -= info->bytes;\n\t}\n\n\tif (update_stat)\n\t\tctl->free_space -= info->bytes;\n}\n\nstatic int link_free_space(struct btrfs_free_space_ctl *ctl,\n\t\t\t   struct btrfs_free_space *info)\n{\n\tint ret = 0;\n\n\tlockdep_assert_held(&ctl->tree_lock);\n\n\tASSERT(info->bytes || info->bitmap);\n\tret = tree_insert_offset(ctl, NULL, info);\n\tif (ret)\n\t\treturn ret;\n\n\trb_add_cached(&info->bytes_index, &ctl->free_space_bytes, entry_less);\n\n\tif (!info->bitmap && !btrfs_free_space_trimmed(info)) {\n\t\tctl->discardable_extents[BTRFS_STAT_CURR]++;\n\t\tctl->discardable_bytes[BTRFS_STAT_CURR] += info->bytes;\n\t}\n\n\tctl->free_space += info->bytes;\n\tctl->free_extents++;\n\treturn ret;\n}\n\nstatic void relink_bitmap_entry(struct btrfs_free_space_ctl *ctl,\n\t\t\t\tstruct btrfs_free_space *info)\n{\n\tASSERT(info->bitmap);\n\n\t \n\tif (RB_EMPTY_NODE(&info->bytes_index))\n\t\treturn;\n\n\tlockdep_assert_held(&ctl->tree_lock);\n\n\trb_erase_cached(&info->bytes_index, &ctl->free_space_bytes);\n\trb_add_cached(&info->bytes_index, &ctl->free_space_bytes, entry_less);\n}\n\nstatic inline void bitmap_clear_bits(struct btrfs_free_space_ctl *ctl,\n\t\t\t\t     struct btrfs_free_space *info,\n\t\t\t\t     u64 offset, u64 bytes, bool update_stat)\n{\n\tunsigned long start, count, end;\n\tint extent_delta = -1;\n\n\tstart = offset_to_bit(info->offset, ctl->unit, offset);\n\tcount = bytes_to_bits(bytes, ctl->unit);\n\tend = start + count;\n\tASSERT(end <= BITS_PER_BITMAP);\n\n\tbitmap_clear(info->bitmap, start, count);\n\n\tinfo->bytes -= bytes;\n\tif (info->max_extent_size > ctl->unit)\n\t\tinfo->max_extent_size = 0;\n\n\trelink_bitmap_entry(ctl, info);\n\n\tif (start && test_bit(start - 1, info->bitmap))\n\t\textent_delta++;\n\n\tif (end < BITS_PER_BITMAP && test_bit(end, info->bitmap))\n\t\textent_delta++;\n\n\tinfo->bitmap_extents += extent_delta;\n\tif (!btrfs_free_space_trimmed(info)) {\n\t\tctl->discardable_extents[BTRFS_STAT_CURR] += extent_delta;\n\t\tctl->discardable_bytes[BTRFS_STAT_CURR] -= bytes;\n\t}\n\n\tif (update_stat)\n\t\tctl->free_space -= bytes;\n}\n\nstatic void bitmap_set_bits(struct btrfs_free_space_ctl *ctl,\n\t\t\t    struct btrfs_free_space *info, u64 offset,\n\t\t\t    u64 bytes)\n{\n\tunsigned long start, count, end;\n\tint extent_delta = 1;\n\n\tstart = offset_to_bit(info->offset, ctl->unit, offset);\n\tcount = bytes_to_bits(bytes, ctl->unit);\n\tend = start + count;\n\tASSERT(end <= BITS_PER_BITMAP);\n\n\tbitmap_set(info->bitmap, start, count);\n\n\t \n\tinfo->max_extent_size = 0;\n\tinfo->bytes += bytes;\n\tctl->free_space += bytes;\n\n\trelink_bitmap_entry(ctl, info);\n\n\tif (start && test_bit(start - 1, info->bitmap))\n\t\textent_delta--;\n\n\tif (end < BITS_PER_BITMAP && test_bit(end, info->bitmap))\n\t\textent_delta--;\n\n\tinfo->bitmap_extents += extent_delta;\n\tif (!btrfs_free_space_trimmed(info)) {\n\t\tctl->discardable_extents[BTRFS_STAT_CURR] += extent_delta;\n\t\tctl->discardable_bytes[BTRFS_STAT_CURR] += bytes;\n\t}\n}\n\n \nstatic int search_bitmap(struct btrfs_free_space_ctl *ctl,\n\t\t\t struct btrfs_free_space *bitmap_info, u64 *offset,\n\t\t\t u64 *bytes, bool for_alloc)\n{\n\tunsigned long found_bits = 0;\n\tunsigned long max_bits = 0;\n\tunsigned long bits, i;\n\tunsigned long next_zero;\n\tunsigned long extent_bits;\n\n\t \n\tif (for_alloc &&\n\t    bitmap_info->max_extent_size &&\n\t    bitmap_info->max_extent_size < *bytes) {\n\t\t*bytes = bitmap_info->max_extent_size;\n\t\treturn -1;\n\t}\n\n\ti = offset_to_bit(bitmap_info->offset, ctl->unit,\n\t\t\t  max_t(u64, *offset, bitmap_info->offset));\n\tbits = bytes_to_bits(*bytes, ctl->unit);\n\n\tfor_each_set_bit_from(i, bitmap_info->bitmap, BITS_PER_BITMAP) {\n\t\tif (for_alloc && bits == 1) {\n\t\t\tfound_bits = 1;\n\t\t\tbreak;\n\t\t}\n\t\tnext_zero = find_next_zero_bit(bitmap_info->bitmap,\n\t\t\t\t\t       BITS_PER_BITMAP, i);\n\t\textent_bits = next_zero - i;\n\t\tif (extent_bits >= bits) {\n\t\t\tfound_bits = extent_bits;\n\t\t\tbreak;\n\t\t} else if (extent_bits > max_bits) {\n\t\t\tmax_bits = extent_bits;\n\t\t}\n\t\ti = next_zero;\n\t}\n\n\tif (found_bits) {\n\t\t*offset = (u64)(i * ctl->unit) + bitmap_info->offset;\n\t\t*bytes = (u64)(found_bits) * ctl->unit;\n\t\treturn 0;\n\t}\n\n\t*bytes = (u64)(max_bits) * ctl->unit;\n\tbitmap_info->max_extent_size = *bytes;\n\trelink_bitmap_entry(ctl, bitmap_info);\n\treturn -1;\n}\n\n \nstatic struct btrfs_free_space *\nfind_free_space(struct btrfs_free_space_ctl *ctl, u64 *offset, u64 *bytes,\n\t\tunsigned long align, u64 *max_extent_size, bool use_bytes_index)\n{\n\tstruct btrfs_free_space *entry;\n\tstruct rb_node *node;\n\tu64 tmp;\n\tu64 align_off;\n\tint ret;\n\n\tif (!ctl->free_space_offset.rb_node)\n\t\tgoto out;\nagain:\n\tif (use_bytes_index) {\n\t\tnode = rb_first_cached(&ctl->free_space_bytes);\n\t} else {\n\t\tentry = tree_search_offset(ctl, offset_to_bitmap(ctl, *offset),\n\t\t\t\t\t   0, 1);\n\t\tif (!entry)\n\t\t\tgoto out;\n\t\tnode = &entry->offset_index;\n\t}\n\n\tfor (; node; node = rb_next(node)) {\n\t\tif (use_bytes_index)\n\t\t\tentry = rb_entry(node, struct btrfs_free_space,\n\t\t\t\t\t bytes_index);\n\t\telse\n\t\t\tentry = rb_entry(node, struct btrfs_free_space,\n\t\t\t\t\t offset_index);\n\n\t\t \n\t\tif (entry->bytes < *bytes) {\n\t\t\t*max_extent_size = max(get_max_extent_size(entry),\n\t\t\t\t\t       *max_extent_size);\n\t\t\tif (use_bytes_index)\n\t\t\t\tbreak;\n\t\t\tcontinue;\n\t\t}\n\n\t\t \n\t\tif (*bytes >= align) {\n\t\t\ttmp = entry->offset - ctl->start + align - 1;\n\t\t\ttmp = div64_u64(tmp, align);\n\t\t\ttmp = tmp * align + ctl->start;\n\t\t\talign_off = tmp - entry->offset;\n\t\t} else {\n\t\t\talign_off = 0;\n\t\t\ttmp = entry->offset;\n\t\t}\n\n\t\t \n\t\tif (entry->bytes < *bytes + align_off) {\n\t\t\t*max_extent_size = max(get_max_extent_size(entry),\n\t\t\t\t\t       *max_extent_size);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (entry->bitmap) {\n\t\t\tstruct rb_node *old_next = rb_next(node);\n\t\t\tu64 size = *bytes;\n\n\t\t\tret = search_bitmap(ctl, entry, &tmp, &size, true);\n\t\t\tif (!ret) {\n\t\t\t\t*offset = tmp;\n\t\t\t\t*bytes = size;\n\t\t\t\treturn entry;\n\t\t\t} else {\n\t\t\t\t*max_extent_size =\n\t\t\t\t\tmax(get_max_extent_size(entry),\n\t\t\t\t\t    *max_extent_size);\n\t\t\t}\n\n\t\t\t \n\t\t\tif (use_bytes_index && old_next != rb_next(node))\n\t\t\t\tgoto again;\n\t\t\tcontinue;\n\t\t}\n\n\t\t*offset = tmp;\n\t\t*bytes = entry->bytes - align_off;\n\t\treturn entry;\n\t}\nout:\n\treturn NULL;\n}\n\nstatic void add_new_bitmap(struct btrfs_free_space_ctl *ctl,\n\t\t\t   struct btrfs_free_space *info, u64 offset)\n{\n\tinfo->offset = offset_to_bitmap(ctl, offset);\n\tinfo->bytes = 0;\n\tinfo->bitmap_extents = 0;\n\tINIT_LIST_HEAD(&info->list);\n\tlink_free_space(ctl, info);\n\tctl->total_bitmaps++;\n\trecalculate_thresholds(ctl);\n}\n\nstatic void free_bitmap(struct btrfs_free_space_ctl *ctl,\n\t\t\tstruct btrfs_free_space *bitmap_info)\n{\n\t \n\tif (bitmap_info->bytes && !btrfs_free_space_trimmed(bitmap_info)) {\n\t\tctl->discardable_extents[BTRFS_STAT_CURR] -=\n\t\t\tbitmap_info->bitmap_extents;\n\t\tctl->discardable_bytes[BTRFS_STAT_CURR] -= bitmap_info->bytes;\n\n\t}\n\tunlink_free_space(ctl, bitmap_info, true);\n\tkmem_cache_free(btrfs_free_space_bitmap_cachep, bitmap_info->bitmap);\n\tkmem_cache_free(btrfs_free_space_cachep, bitmap_info);\n\tctl->total_bitmaps--;\n\trecalculate_thresholds(ctl);\n}\n\nstatic noinline int remove_from_bitmap(struct btrfs_free_space_ctl *ctl,\n\t\t\t      struct btrfs_free_space *bitmap_info,\n\t\t\t      u64 *offset, u64 *bytes)\n{\n\tu64 end;\n\tu64 search_start, search_bytes;\n\tint ret;\n\nagain:\n\tend = bitmap_info->offset + (u64)(BITS_PER_BITMAP * ctl->unit) - 1;\n\n\t \n\tsearch_start = *offset;\n\tsearch_bytes = ctl->unit;\n\tsearch_bytes = min(search_bytes, end - search_start + 1);\n\tret = search_bitmap(ctl, bitmap_info, &search_start, &search_bytes,\n\t\t\t    false);\n\tif (ret < 0 || search_start != *offset)\n\t\treturn -EINVAL;\n\n\t \n\tsearch_bytes = min(search_bytes, *bytes);\n\n\t \n\tsearch_bytes = min(search_bytes, end - search_start + 1);\n\n\tbitmap_clear_bits(ctl, bitmap_info, search_start, search_bytes, true);\n\t*offset += search_bytes;\n\t*bytes -= search_bytes;\n\n\tif (*bytes) {\n\t\tstruct rb_node *next = rb_next(&bitmap_info->offset_index);\n\t\tif (!bitmap_info->bytes)\n\t\t\tfree_bitmap(ctl, bitmap_info);\n\n\t\t \n\t\tif (!next)\n\t\t\treturn -EINVAL;\n\n\t\tbitmap_info = rb_entry(next, struct btrfs_free_space,\n\t\t\t\t       offset_index);\n\n\t\t \n\t\tif (!bitmap_info->bitmap)\n\t\t\treturn -EAGAIN;\n\n\t\t \n\t\tsearch_start = *offset;\n\t\tsearch_bytes = ctl->unit;\n\t\tret = search_bitmap(ctl, bitmap_info, &search_start,\n\t\t\t\t    &search_bytes, false);\n\t\tif (ret < 0 || search_start != *offset)\n\t\t\treturn -EAGAIN;\n\n\t\tgoto again;\n\t} else if (!bitmap_info->bytes)\n\t\tfree_bitmap(ctl, bitmap_info);\n\n\treturn 0;\n}\n\nstatic u64 add_bytes_to_bitmap(struct btrfs_free_space_ctl *ctl,\n\t\t\t       struct btrfs_free_space *info, u64 offset,\n\t\t\t       u64 bytes, enum btrfs_trim_state trim_state)\n{\n\tu64 bytes_to_set = 0;\n\tu64 end;\n\n\t \n\tif (trim_state == BTRFS_TRIM_STATE_UNTRIMMED) {\n\t\tif (btrfs_free_space_trimmed(info)) {\n\t\t\tctl->discardable_extents[BTRFS_STAT_CURR] +=\n\t\t\t\tinfo->bitmap_extents;\n\t\t\tctl->discardable_bytes[BTRFS_STAT_CURR] += info->bytes;\n\t\t}\n\t\tinfo->trim_state = BTRFS_TRIM_STATE_UNTRIMMED;\n\t}\n\n\tend = info->offset + (u64)(BITS_PER_BITMAP * ctl->unit);\n\n\tbytes_to_set = min(end - offset, bytes);\n\n\tbitmap_set_bits(ctl, info, offset, bytes_to_set);\n\n\treturn bytes_to_set;\n\n}\n\nstatic bool use_bitmap(struct btrfs_free_space_ctl *ctl,\n\t\t      struct btrfs_free_space *info)\n{\n\tstruct btrfs_block_group *block_group = ctl->block_group;\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tbool forced = false;\n\n#ifdef CONFIG_BTRFS_DEBUG\n\tif (btrfs_should_fragment_free_space(block_group))\n\t\tforced = true;\n#endif\n\n\t \n\tif (!forced && info->bytes >= FORCE_EXTENT_THRESHOLD)\n\t\treturn false;\n\n\t \n\tif (!forced && ctl->free_extents < ctl->extents_thresh) {\n\t\t \n\t\tif (info->bytes <= fs_info->sectorsize * 8) {\n\t\t\tif (ctl->free_extents * 3 <= ctl->extents_thresh)\n\t\t\t\treturn false;\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}\n\n\t \n\tif (((BITS_PER_BITMAP * ctl->unit) >> 1) > block_group->length)\n\t\treturn false;\n\n\treturn true;\n}\n\nstatic const struct btrfs_free_space_op free_space_op = {\n\t.use_bitmap\t\t= use_bitmap,\n};\n\nstatic int insert_into_bitmap(struct btrfs_free_space_ctl *ctl,\n\t\t\t      struct btrfs_free_space *info)\n{\n\tstruct btrfs_free_space *bitmap_info;\n\tstruct btrfs_block_group *block_group = NULL;\n\tint added = 0;\n\tu64 bytes, offset, bytes_added;\n\tenum btrfs_trim_state trim_state;\n\tint ret;\n\n\tbytes = info->bytes;\n\toffset = info->offset;\n\ttrim_state = info->trim_state;\n\n\tif (!ctl->op->use_bitmap(ctl, info))\n\t\treturn 0;\n\n\tif (ctl->op == &free_space_op)\n\t\tblock_group = ctl->block_group;\nagain:\n\t \n\tif (block_group && !list_empty(&block_group->cluster_list)) {\n\t\tstruct btrfs_free_cluster *cluster;\n\t\tstruct rb_node *node;\n\t\tstruct btrfs_free_space *entry;\n\n\t\tcluster = list_entry(block_group->cluster_list.next,\n\t\t\t\t     struct btrfs_free_cluster,\n\t\t\t\t     block_group_list);\n\t\tspin_lock(&cluster->lock);\n\t\tnode = rb_first(&cluster->root);\n\t\tif (!node) {\n\t\t\tspin_unlock(&cluster->lock);\n\t\t\tgoto no_cluster_bitmap;\n\t\t}\n\n\t\tentry = rb_entry(node, struct btrfs_free_space, offset_index);\n\t\tif (!entry->bitmap) {\n\t\t\tspin_unlock(&cluster->lock);\n\t\t\tgoto no_cluster_bitmap;\n\t\t}\n\n\t\tif (entry->offset == offset_to_bitmap(ctl, offset)) {\n\t\t\tbytes_added = add_bytes_to_bitmap(ctl, entry, offset,\n\t\t\t\t\t\t\t  bytes, trim_state);\n\t\t\tbytes -= bytes_added;\n\t\t\toffset += bytes_added;\n\t\t}\n\t\tspin_unlock(&cluster->lock);\n\t\tif (!bytes) {\n\t\t\tret = 1;\n\t\t\tgoto out;\n\t\t}\n\t}\n\nno_cluster_bitmap:\n\tbitmap_info = tree_search_offset(ctl, offset_to_bitmap(ctl, offset),\n\t\t\t\t\t 1, 0);\n\tif (!bitmap_info) {\n\t\tASSERT(added == 0);\n\t\tgoto new_bitmap;\n\t}\n\n\tbytes_added = add_bytes_to_bitmap(ctl, bitmap_info, offset, bytes,\n\t\t\t\t\t  trim_state);\n\tbytes -= bytes_added;\n\toffset += bytes_added;\n\tadded = 0;\n\n\tif (!bytes) {\n\t\tret = 1;\n\t\tgoto out;\n\t} else\n\t\tgoto again;\n\nnew_bitmap:\n\tif (info && info->bitmap) {\n\t\tadd_new_bitmap(ctl, info, offset);\n\t\tadded = 1;\n\t\tinfo = NULL;\n\t\tgoto again;\n\t} else {\n\t\tspin_unlock(&ctl->tree_lock);\n\n\t\t \n\t\tif (!info) {\n\t\t\tinfo = kmem_cache_zalloc(btrfs_free_space_cachep,\n\t\t\t\t\t\t GFP_NOFS);\n\t\t\tif (!info) {\n\t\t\t\tspin_lock(&ctl->tree_lock);\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\t \n\t\tinfo->bitmap = kmem_cache_zalloc(btrfs_free_space_bitmap_cachep,\n\t\t\t\t\t\t GFP_NOFS);\n\t\tinfo->trim_state = BTRFS_TRIM_STATE_TRIMMED;\n\t\tspin_lock(&ctl->tree_lock);\n\t\tif (!info->bitmap) {\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out;\n\t\t}\n\t\tgoto again;\n\t}\n\nout:\n\tif (info) {\n\t\tif (info->bitmap)\n\t\t\tkmem_cache_free(btrfs_free_space_bitmap_cachep,\n\t\t\t\t\tinfo->bitmap);\n\t\tkmem_cache_free(btrfs_free_space_cachep, info);\n\t}\n\n\treturn ret;\n}\n\n \nstatic bool try_merge_free_space(struct btrfs_free_space_ctl *ctl,\n\t\t\t  struct btrfs_free_space *info, bool update_stat)\n{\n\tstruct btrfs_free_space *left_info = NULL;\n\tstruct btrfs_free_space *right_info;\n\tbool merged = false;\n\tu64 offset = info->offset;\n\tu64 bytes = info->bytes;\n\tconst bool is_trimmed = btrfs_free_space_trimmed(info);\n\tstruct rb_node *right_prev = NULL;\n\n\t \n\tright_info = tree_search_offset(ctl, offset + bytes, 0, 0);\n\tif (right_info)\n\t\tright_prev = rb_prev(&right_info->offset_index);\n\n\tif (right_prev)\n\t\tleft_info = rb_entry(right_prev, struct btrfs_free_space, offset_index);\n\telse if (!right_info)\n\t\tleft_info = tree_search_offset(ctl, offset - 1, 0, 0);\n\n\t \n\tif (right_info && !right_info->bitmap &&\n\t    (!is_trimmed || btrfs_free_space_trimmed(right_info))) {\n\t\tunlink_free_space(ctl, right_info, update_stat);\n\t\tinfo->bytes += right_info->bytes;\n\t\tkmem_cache_free(btrfs_free_space_cachep, right_info);\n\t\tmerged = true;\n\t}\n\n\t \n\tif (left_info && !left_info->bitmap &&\n\t    left_info->offset + left_info->bytes == offset &&\n\t    (!is_trimmed || btrfs_free_space_trimmed(left_info))) {\n\t\tunlink_free_space(ctl, left_info, update_stat);\n\t\tinfo->offset = left_info->offset;\n\t\tinfo->bytes += left_info->bytes;\n\t\tkmem_cache_free(btrfs_free_space_cachep, left_info);\n\t\tmerged = true;\n\t}\n\n\treturn merged;\n}\n\nstatic bool steal_from_bitmap_to_end(struct btrfs_free_space_ctl *ctl,\n\t\t\t\t     struct btrfs_free_space *info,\n\t\t\t\t     bool update_stat)\n{\n\tstruct btrfs_free_space *bitmap;\n\tunsigned long i;\n\tunsigned long j;\n\tconst u64 end = info->offset + info->bytes;\n\tconst u64 bitmap_offset = offset_to_bitmap(ctl, end);\n\tu64 bytes;\n\n\tbitmap = tree_search_offset(ctl, bitmap_offset, 1, 0);\n\tif (!bitmap)\n\t\treturn false;\n\n\ti = offset_to_bit(bitmap->offset, ctl->unit, end);\n\tj = find_next_zero_bit(bitmap->bitmap, BITS_PER_BITMAP, i);\n\tif (j == i)\n\t\treturn false;\n\tbytes = (j - i) * ctl->unit;\n\tinfo->bytes += bytes;\n\n\t \n\tif (!btrfs_free_space_trimmed(bitmap))\n\t\tinfo->trim_state = BTRFS_TRIM_STATE_UNTRIMMED;\n\n\tbitmap_clear_bits(ctl, bitmap, end, bytes, update_stat);\n\n\tif (!bitmap->bytes)\n\t\tfree_bitmap(ctl, bitmap);\n\n\treturn true;\n}\n\nstatic bool steal_from_bitmap_to_front(struct btrfs_free_space_ctl *ctl,\n\t\t\t\t       struct btrfs_free_space *info,\n\t\t\t\t       bool update_stat)\n{\n\tstruct btrfs_free_space *bitmap;\n\tu64 bitmap_offset;\n\tunsigned long i;\n\tunsigned long j;\n\tunsigned long prev_j;\n\tu64 bytes;\n\n\tbitmap_offset = offset_to_bitmap(ctl, info->offset);\n\t \n\tif (bitmap_offset == info->offset) {\n\t\tif (info->offset == 0)\n\t\t\treturn false;\n\t\tbitmap_offset = offset_to_bitmap(ctl, info->offset - 1);\n\t}\n\n\tbitmap = tree_search_offset(ctl, bitmap_offset, 1, 0);\n\tif (!bitmap)\n\t\treturn false;\n\n\ti = offset_to_bit(bitmap->offset, ctl->unit, info->offset) - 1;\n\tj = 0;\n\tprev_j = (unsigned long)-1;\n\tfor_each_clear_bit_from(j, bitmap->bitmap, BITS_PER_BITMAP) {\n\t\tif (j > i)\n\t\t\tbreak;\n\t\tprev_j = j;\n\t}\n\tif (prev_j == i)\n\t\treturn false;\n\n\tif (prev_j == (unsigned long)-1)\n\t\tbytes = (i + 1) * ctl->unit;\n\telse\n\t\tbytes = (i - prev_j) * ctl->unit;\n\n\tinfo->offset -= bytes;\n\tinfo->bytes += bytes;\n\n\t \n\tif (!btrfs_free_space_trimmed(bitmap))\n\t\tinfo->trim_state = BTRFS_TRIM_STATE_UNTRIMMED;\n\n\tbitmap_clear_bits(ctl, bitmap, info->offset, bytes, update_stat);\n\n\tif (!bitmap->bytes)\n\t\tfree_bitmap(ctl, bitmap);\n\n\treturn true;\n}\n\n \nstatic void steal_from_bitmap(struct btrfs_free_space_ctl *ctl,\n\t\t\t      struct btrfs_free_space *info,\n\t\t\t      bool update_stat)\n{\n\t \n\tASSERT(!info->bitmap);\n\tASSERT(RB_EMPTY_NODE(&info->offset_index));\n\n\tif (ctl->total_bitmaps > 0) {\n\t\tbool stole_end;\n\t\tbool stole_front = false;\n\n\t\tstole_end = steal_from_bitmap_to_end(ctl, info, update_stat);\n\t\tif (ctl->total_bitmaps > 0)\n\t\t\tstole_front = steal_from_bitmap_to_front(ctl, info,\n\t\t\t\t\t\t\t\t update_stat);\n\n\t\tif (stole_end || stole_front)\n\t\t\ttry_merge_free_space(ctl, info, update_stat);\n\t}\n}\n\nint __btrfs_add_free_space(struct btrfs_block_group *block_group,\n\t\t\t   u64 offset, u64 bytes,\n\t\t\t   enum btrfs_trim_state trim_state)\n{\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tstruct btrfs_free_space *info;\n\tint ret = 0;\n\tu64 filter_bytes = bytes;\n\n\tASSERT(!btrfs_is_zoned(fs_info));\n\n\tinfo = kmem_cache_zalloc(btrfs_free_space_cachep, GFP_NOFS);\n\tif (!info)\n\t\treturn -ENOMEM;\n\n\tinfo->offset = offset;\n\tinfo->bytes = bytes;\n\tinfo->trim_state = trim_state;\n\tRB_CLEAR_NODE(&info->offset_index);\n\tRB_CLEAR_NODE(&info->bytes_index);\n\n\tspin_lock(&ctl->tree_lock);\n\n\tif (try_merge_free_space(ctl, info, true))\n\t\tgoto link;\n\n\t \n\tret = insert_into_bitmap(ctl, info);\n\tif (ret < 0) {\n\t\tgoto out;\n\t} else if (ret) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\nlink:\n\t \n\tsteal_from_bitmap(ctl, info, true);\n\n\tfilter_bytes = max(filter_bytes, info->bytes);\n\n\tret = link_free_space(ctl, info);\n\tif (ret)\n\t\tkmem_cache_free(btrfs_free_space_cachep, info);\nout:\n\tbtrfs_discard_update_discardable(block_group);\n\tspin_unlock(&ctl->tree_lock);\n\n\tif (ret) {\n\t\tbtrfs_crit(fs_info, \"unable to add free space :%d\", ret);\n\t\tASSERT(ret != -EEXIST);\n\t}\n\n\tif (trim_state != BTRFS_TRIM_STATE_TRIMMED) {\n\t\tbtrfs_discard_check_filter(block_group, filter_bytes);\n\t\tbtrfs_discard_queue_work(&fs_info->discard_ctl, block_group);\n\t}\n\n\treturn ret;\n}\n\nstatic int __btrfs_add_free_space_zoned(struct btrfs_block_group *block_group,\n\t\t\t\t\tu64 bytenr, u64 size, bool used)\n{\n\tstruct btrfs_space_info *sinfo = block_group->space_info;\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tu64 offset = bytenr - block_group->start;\n\tu64 to_free, to_unusable;\n\tint bg_reclaim_threshold = 0;\n\tbool initial = (size == block_group->length);\n\tu64 reclaimable_unusable;\n\n\tWARN_ON(!initial && offset + size > block_group->zone_capacity);\n\n\tif (!initial)\n\t\tbg_reclaim_threshold = READ_ONCE(sinfo->bg_reclaim_threshold);\n\n\tspin_lock(&ctl->tree_lock);\n\tif (!used)\n\t\tto_free = size;\n\telse if (initial)\n\t\tto_free = block_group->zone_capacity;\n\telse if (offset >= block_group->alloc_offset)\n\t\tto_free = size;\n\telse if (offset + size <= block_group->alloc_offset)\n\t\tto_free = 0;\n\telse\n\t\tto_free = offset + size - block_group->alloc_offset;\n\tto_unusable = size - to_free;\n\n\tctl->free_space += to_free;\n\t \n\tif (!block_group->ro)\n\t\tblock_group->zone_unusable += to_unusable;\n\tspin_unlock(&ctl->tree_lock);\n\tif (!used) {\n\t\tspin_lock(&block_group->lock);\n\t\tblock_group->alloc_offset -= size;\n\t\tspin_unlock(&block_group->lock);\n\t}\n\n\treclaimable_unusable = block_group->zone_unusable -\n\t\t\t       (block_group->length - block_group->zone_capacity);\n\t \n\tif (block_group->zone_unusable == block_group->length) {\n\t\tbtrfs_mark_bg_unused(block_group);\n\t} else if (bg_reclaim_threshold &&\n\t\t   reclaimable_unusable >=\n\t\t   mult_perc(block_group->zone_capacity, bg_reclaim_threshold)) {\n\t\tbtrfs_mark_bg_to_reclaim(block_group);\n\t}\n\n\treturn 0;\n}\n\nint btrfs_add_free_space(struct btrfs_block_group *block_group,\n\t\t\t u64 bytenr, u64 size)\n{\n\tenum btrfs_trim_state trim_state = BTRFS_TRIM_STATE_UNTRIMMED;\n\n\tif (btrfs_is_zoned(block_group->fs_info))\n\t\treturn __btrfs_add_free_space_zoned(block_group, bytenr, size,\n\t\t\t\t\t\t    true);\n\n\tif (btrfs_test_opt(block_group->fs_info, DISCARD_SYNC))\n\t\ttrim_state = BTRFS_TRIM_STATE_TRIMMED;\n\n\treturn __btrfs_add_free_space(block_group, bytenr, size, trim_state);\n}\n\nint btrfs_add_free_space_unused(struct btrfs_block_group *block_group,\n\t\t\t\tu64 bytenr, u64 size)\n{\n\tif (btrfs_is_zoned(block_group->fs_info))\n\t\treturn __btrfs_add_free_space_zoned(block_group, bytenr, size,\n\t\t\t\t\t\t    false);\n\n\treturn btrfs_add_free_space(block_group, bytenr, size);\n}\n\n \nint btrfs_add_free_space_async_trimmed(struct btrfs_block_group *block_group,\n\t\t\t\t       u64 bytenr, u64 size)\n{\n\tenum btrfs_trim_state trim_state = BTRFS_TRIM_STATE_UNTRIMMED;\n\n\tif (btrfs_is_zoned(block_group->fs_info))\n\t\treturn __btrfs_add_free_space_zoned(block_group, bytenr, size,\n\t\t\t\t\t\t    true);\n\n\tif (btrfs_test_opt(block_group->fs_info, DISCARD_SYNC) ||\n\t    btrfs_test_opt(block_group->fs_info, DISCARD_ASYNC))\n\t\ttrim_state = BTRFS_TRIM_STATE_TRIMMED;\n\n\treturn __btrfs_add_free_space(block_group, bytenr, size, trim_state);\n}\n\nint btrfs_remove_free_space(struct btrfs_block_group *block_group,\n\t\t\t    u64 offset, u64 bytes)\n{\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tstruct btrfs_free_space *info;\n\tint ret;\n\tbool re_search = false;\n\n\tif (btrfs_is_zoned(block_group->fs_info)) {\n\t\t \n\t\tif (block_group->start + block_group->alloc_offset <\n\t\t    offset + bytes) {\n\t\t\tblock_group->alloc_offset =\n\t\t\t\toffset + bytes - block_group->start;\n\t\t}\n\t\treturn 0;\n\t}\n\n\tspin_lock(&ctl->tree_lock);\n\nagain:\n\tret = 0;\n\tif (!bytes)\n\t\tgoto out_lock;\n\n\tinfo = tree_search_offset(ctl, offset, 0, 0);\n\tif (!info) {\n\t\t \n\t\tinfo = tree_search_offset(ctl, offset_to_bitmap(ctl, offset),\n\t\t\t\t\t  1, 0);\n\t\tif (!info) {\n\t\t\t \n\t\t\tWARN_ON(re_search);\n\t\t\tgoto out_lock;\n\t\t}\n\t}\n\n\tre_search = false;\n\tif (!info->bitmap) {\n\t\tunlink_free_space(ctl, info, true);\n\t\tif (offset == info->offset) {\n\t\t\tu64 to_free = min(bytes, info->bytes);\n\n\t\t\tinfo->bytes -= to_free;\n\t\t\tinfo->offset += to_free;\n\t\t\tif (info->bytes) {\n\t\t\t\tret = link_free_space(ctl, info);\n\t\t\t\tWARN_ON(ret);\n\t\t\t} else {\n\t\t\t\tkmem_cache_free(btrfs_free_space_cachep, info);\n\t\t\t}\n\n\t\t\toffset += to_free;\n\t\t\tbytes -= to_free;\n\t\t\tgoto again;\n\t\t} else {\n\t\t\tu64 old_end = info->bytes + info->offset;\n\n\t\t\tinfo->bytes = offset - info->offset;\n\t\t\tret = link_free_space(ctl, info);\n\t\t\tWARN_ON(ret);\n\t\t\tif (ret)\n\t\t\t\tgoto out_lock;\n\n\t\t\t \n\t\t\tif (old_end < offset + bytes) {\n\t\t\t\tbytes -= old_end - offset;\n\t\t\t\toffset = old_end;\n\t\t\t\tgoto again;\n\t\t\t} else if (old_end == offset + bytes) {\n\t\t\t\t \n\t\t\t\tgoto out_lock;\n\t\t\t}\n\t\t\tspin_unlock(&ctl->tree_lock);\n\n\t\t\tret = __btrfs_add_free_space(block_group,\n\t\t\t\t\t\t     offset + bytes,\n\t\t\t\t\t\t     old_end - (offset + bytes),\n\t\t\t\t\t\t     info->trim_state);\n\t\t\tWARN_ON(ret);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = remove_from_bitmap(ctl, info, &offset, &bytes);\n\tif (ret == -EAGAIN) {\n\t\tre_search = true;\n\t\tgoto again;\n\t}\nout_lock:\n\tbtrfs_discard_update_discardable(block_group);\n\tspin_unlock(&ctl->tree_lock);\nout:\n\treturn ret;\n}\n\nvoid btrfs_dump_free_space(struct btrfs_block_group *block_group,\n\t\t\t   u64 bytes)\n{\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tstruct btrfs_free_space *info;\n\tstruct rb_node *n;\n\tint count = 0;\n\n\t \n\tif (btrfs_is_zoned(fs_info)) {\n\t\tbtrfs_info(fs_info, \"free space %llu active %d\",\n\t\t\t   block_group->zone_capacity - block_group->alloc_offset,\n\t\t\t   test_bit(BLOCK_GROUP_FLAG_ZONE_IS_ACTIVE,\n\t\t\t\t    &block_group->runtime_flags));\n\t\treturn;\n\t}\n\n\tspin_lock(&ctl->tree_lock);\n\tfor (n = rb_first(&ctl->free_space_offset); n; n = rb_next(n)) {\n\t\tinfo = rb_entry(n, struct btrfs_free_space, offset_index);\n\t\tif (info->bytes >= bytes && !block_group->ro)\n\t\t\tcount++;\n\t\tbtrfs_crit(fs_info, \"entry offset %llu, bytes %llu, bitmap %s\",\n\t\t\t   info->offset, info->bytes,\n\t\t       (info->bitmap) ? \"yes\" : \"no\");\n\t}\n\tspin_unlock(&ctl->tree_lock);\n\tbtrfs_info(fs_info, \"block group has cluster?: %s\",\n\t       list_empty(&block_group->cluster_list) ? \"no\" : \"yes\");\n\tbtrfs_info(fs_info,\n\t\t   \"%d free space entries at or bigger than %llu bytes\",\n\t\t   count, bytes);\n}\n\nvoid btrfs_init_free_space_ctl(struct btrfs_block_group *block_group,\n\t\t\t       struct btrfs_free_space_ctl *ctl)\n{\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\n\tspin_lock_init(&ctl->tree_lock);\n\tctl->unit = fs_info->sectorsize;\n\tctl->start = block_group->start;\n\tctl->block_group = block_group;\n\tctl->op = &free_space_op;\n\tctl->free_space_bytes = RB_ROOT_CACHED;\n\tINIT_LIST_HEAD(&ctl->trimming_ranges);\n\tmutex_init(&ctl->cache_writeout_mutex);\n\n\t \n\tctl->extents_thresh = (SZ_32K / 2) / sizeof(struct btrfs_free_space);\n}\n\n \nstatic void __btrfs_return_cluster_to_free_space(\n\t\t\t     struct btrfs_block_group *block_group,\n\t\t\t     struct btrfs_free_cluster *cluster)\n{\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tstruct rb_node *node;\n\n\tlockdep_assert_held(&ctl->tree_lock);\n\n\tspin_lock(&cluster->lock);\n\tif (cluster->block_group != block_group) {\n\t\tspin_unlock(&cluster->lock);\n\t\treturn;\n\t}\n\n\tcluster->block_group = NULL;\n\tcluster->window_start = 0;\n\tlist_del_init(&cluster->block_group_list);\n\n\tnode = rb_first(&cluster->root);\n\twhile (node) {\n\t\tstruct btrfs_free_space *entry;\n\n\t\tentry = rb_entry(node, struct btrfs_free_space, offset_index);\n\t\tnode = rb_next(&entry->offset_index);\n\t\trb_erase(&entry->offset_index, &cluster->root);\n\t\tRB_CLEAR_NODE(&entry->offset_index);\n\n\t\tif (!entry->bitmap) {\n\t\t\t \n\t\t\tif (!btrfs_free_space_trimmed(entry)) {\n\t\t\t\tctl->discardable_extents[BTRFS_STAT_CURR]--;\n\t\t\t\tctl->discardable_bytes[BTRFS_STAT_CURR] -=\n\t\t\t\t\tentry->bytes;\n\t\t\t}\n\n\t\t\ttry_merge_free_space(ctl, entry, false);\n\t\t\tsteal_from_bitmap(ctl, entry, false);\n\n\t\t\t \n\t\t\tif (!btrfs_free_space_trimmed(entry)) {\n\t\t\t\tctl->discardable_extents[BTRFS_STAT_CURR]++;\n\t\t\t\tctl->discardable_bytes[BTRFS_STAT_CURR] +=\n\t\t\t\t\tentry->bytes;\n\t\t\t}\n\t\t}\n\t\ttree_insert_offset(ctl, NULL, entry);\n\t\trb_add_cached(&entry->bytes_index, &ctl->free_space_bytes,\n\t\t\t      entry_less);\n\t}\n\tcluster->root = RB_ROOT;\n\tspin_unlock(&cluster->lock);\n\tbtrfs_put_block_group(block_group);\n}\n\nvoid btrfs_remove_free_space_cache(struct btrfs_block_group *block_group)\n{\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tstruct btrfs_free_cluster *cluster;\n\tstruct list_head *head;\n\n\tspin_lock(&ctl->tree_lock);\n\twhile ((head = block_group->cluster_list.next) !=\n\t       &block_group->cluster_list) {\n\t\tcluster = list_entry(head, struct btrfs_free_cluster,\n\t\t\t\t     block_group_list);\n\n\t\tWARN_ON(cluster->block_group != block_group);\n\t\t__btrfs_return_cluster_to_free_space(block_group, cluster);\n\n\t\tcond_resched_lock(&ctl->tree_lock);\n\t}\n\t__btrfs_remove_free_space_cache(ctl);\n\tbtrfs_discard_update_discardable(block_group);\n\tspin_unlock(&ctl->tree_lock);\n\n}\n\n \nbool btrfs_is_free_space_trimmed(struct btrfs_block_group *block_group)\n{\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tstruct btrfs_free_space *info;\n\tstruct rb_node *node;\n\tbool ret = true;\n\n\tspin_lock(&ctl->tree_lock);\n\tnode = rb_first(&ctl->free_space_offset);\n\n\twhile (node) {\n\t\tinfo = rb_entry(node, struct btrfs_free_space, offset_index);\n\n\t\tif (!btrfs_free_space_trimmed(info)) {\n\t\t\tret = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tnode = rb_next(node);\n\t}\n\n\tspin_unlock(&ctl->tree_lock);\n\treturn ret;\n}\n\nu64 btrfs_find_space_for_alloc(struct btrfs_block_group *block_group,\n\t\t\t       u64 offset, u64 bytes, u64 empty_size,\n\t\t\t       u64 *max_extent_size)\n{\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tstruct btrfs_discard_ctl *discard_ctl =\n\t\t\t\t\t&block_group->fs_info->discard_ctl;\n\tstruct btrfs_free_space *entry = NULL;\n\tu64 bytes_search = bytes + empty_size;\n\tu64 ret = 0;\n\tu64 align_gap = 0;\n\tu64 align_gap_len = 0;\n\tenum btrfs_trim_state align_gap_trim_state = BTRFS_TRIM_STATE_UNTRIMMED;\n\tbool use_bytes_index = (offset == block_group->start);\n\n\tASSERT(!btrfs_is_zoned(block_group->fs_info));\n\n\tspin_lock(&ctl->tree_lock);\n\tentry = find_free_space(ctl, &offset, &bytes_search,\n\t\t\t\tblock_group->full_stripe_len, max_extent_size,\n\t\t\t\tuse_bytes_index);\n\tif (!entry)\n\t\tgoto out;\n\n\tret = offset;\n\tif (entry->bitmap) {\n\t\tbitmap_clear_bits(ctl, entry, offset, bytes, true);\n\n\t\tif (!btrfs_free_space_trimmed(entry))\n\t\t\tatomic64_add(bytes, &discard_ctl->discard_bytes_saved);\n\n\t\tif (!entry->bytes)\n\t\t\tfree_bitmap(ctl, entry);\n\t} else {\n\t\tunlink_free_space(ctl, entry, true);\n\t\talign_gap_len = offset - entry->offset;\n\t\talign_gap = entry->offset;\n\t\talign_gap_trim_state = entry->trim_state;\n\n\t\tif (!btrfs_free_space_trimmed(entry))\n\t\t\tatomic64_add(bytes, &discard_ctl->discard_bytes_saved);\n\n\t\tentry->offset = offset + bytes;\n\t\tWARN_ON(entry->bytes < bytes + align_gap_len);\n\n\t\tentry->bytes -= bytes + align_gap_len;\n\t\tif (!entry->bytes)\n\t\t\tkmem_cache_free(btrfs_free_space_cachep, entry);\n\t\telse\n\t\t\tlink_free_space(ctl, entry);\n\t}\nout:\n\tbtrfs_discard_update_discardable(block_group);\n\tspin_unlock(&ctl->tree_lock);\n\n\tif (align_gap_len)\n\t\t__btrfs_add_free_space(block_group, align_gap, align_gap_len,\n\t\t\t\t       align_gap_trim_state);\n\treturn ret;\n}\n\n \nvoid btrfs_return_cluster_to_free_space(\n\t\t\t       struct btrfs_block_group *block_group,\n\t\t\t       struct btrfs_free_cluster *cluster)\n{\n\tstruct btrfs_free_space_ctl *ctl;\n\n\t \n\tspin_lock(&cluster->lock);\n\tif (!block_group) {\n\t\tblock_group = cluster->block_group;\n\t\tif (!block_group) {\n\t\t\tspin_unlock(&cluster->lock);\n\t\t\treturn;\n\t\t}\n\t} else if (cluster->block_group != block_group) {\n\t\t \n\t\tspin_unlock(&cluster->lock);\n\t\treturn;\n\t}\n\tbtrfs_get_block_group(block_group);\n\tspin_unlock(&cluster->lock);\n\n\tctl = block_group->free_space_ctl;\n\n\t \n\tspin_lock(&ctl->tree_lock);\n\t__btrfs_return_cluster_to_free_space(block_group, cluster);\n\tspin_unlock(&ctl->tree_lock);\n\n\tbtrfs_discard_queue_work(&block_group->fs_info->discard_ctl, block_group);\n\n\t \n\tbtrfs_put_block_group(block_group);\n}\n\nstatic u64 btrfs_alloc_from_bitmap(struct btrfs_block_group *block_group,\n\t\t\t\t   struct btrfs_free_cluster *cluster,\n\t\t\t\t   struct btrfs_free_space *entry,\n\t\t\t\t   u64 bytes, u64 min_start,\n\t\t\t\t   u64 *max_extent_size)\n{\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tint err;\n\tu64 search_start = cluster->window_start;\n\tu64 search_bytes = bytes;\n\tu64 ret = 0;\n\n\tsearch_start = min_start;\n\tsearch_bytes = bytes;\n\n\terr = search_bitmap(ctl, entry, &search_start, &search_bytes, true);\n\tif (err) {\n\t\t*max_extent_size = max(get_max_extent_size(entry),\n\t\t\t\t       *max_extent_size);\n\t\treturn 0;\n\t}\n\n\tret = search_start;\n\tbitmap_clear_bits(ctl, entry, ret, bytes, false);\n\n\treturn ret;\n}\n\n \nu64 btrfs_alloc_from_cluster(struct btrfs_block_group *block_group,\n\t\t\t     struct btrfs_free_cluster *cluster, u64 bytes,\n\t\t\t     u64 min_start, u64 *max_extent_size)\n{\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tstruct btrfs_discard_ctl *discard_ctl =\n\t\t\t\t\t&block_group->fs_info->discard_ctl;\n\tstruct btrfs_free_space *entry = NULL;\n\tstruct rb_node *node;\n\tu64 ret = 0;\n\n\tASSERT(!btrfs_is_zoned(block_group->fs_info));\n\n\tspin_lock(&cluster->lock);\n\tif (bytes > cluster->max_size)\n\t\tgoto out;\n\n\tif (cluster->block_group != block_group)\n\t\tgoto out;\n\n\tnode = rb_first(&cluster->root);\n\tif (!node)\n\t\tgoto out;\n\n\tentry = rb_entry(node, struct btrfs_free_space, offset_index);\n\twhile (1) {\n\t\tif (entry->bytes < bytes)\n\t\t\t*max_extent_size = max(get_max_extent_size(entry),\n\t\t\t\t\t       *max_extent_size);\n\n\t\tif (entry->bytes < bytes ||\n\t\t    (!entry->bitmap && entry->offset < min_start)) {\n\t\t\tnode = rb_next(&entry->offset_index);\n\t\t\tif (!node)\n\t\t\t\tbreak;\n\t\t\tentry = rb_entry(node, struct btrfs_free_space,\n\t\t\t\t\t offset_index);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (entry->bitmap) {\n\t\t\tret = btrfs_alloc_from_bitmap(block_group,\n\t\t\t\t\t\t      cluster, entry, bytes,\n\t\t\t\t\t\t      cluster->window_start,\n\t\t\t\t\t\t      max_extent_size);\n\t\t\tif (ret == 0) {\n\t\t\t\tnode = rb_next(&entry->offset_index);\n\t\t\t\tif (!node)\n\t\t\t\t\tbreak;\n\t\t\t\tentry = rb_entry(node, struct btrfs_free_space,\n\t\t\t\t\t\t offset_index);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tcluster->window_start += bytes;\n\t\t} else {\n\t\t\tret = entry->offset;\n\n\t\t\tentry->offset += bytes;\n\t\t\tentry->bytes -= bytes;\n\t\t}\n\n\t\tbreak;\n\t}\nout:\n\tspin_unlock(&cluster->lock);\n\n\tif (!ret)\n\t\treturn 0;\n\n\tspin_lock(&ctl->tree_lock);\n\n\tif (!btrfs_free_space_trimmed(entry))\n\t\tatomic64_add(bytes, &discard_ctl->discard_bytes_saved);\n\n\tctl->free_space -= bytes;\n\tif (!entry->bitmap && !btrfs_free_space_trimmed(entry))\n\t\tctl->discardable_bytes[BTRFS_STAT_CURR] -= bytes;\n\n\tspin_lock(&cluster->lock);\n\tif (entry->bytes == 0) {\n\t\trb_erase(&entry->offset_index, &cluster->root);\n\t\tctl->free_extents--;\n\t\tif (entry->bitmap) {\n\t\t\tkmem_cache_free(btrfs_free_space_bitmap_cachep,\n\t\t\t\t\tentry->bitmap);\n\t\t\tctl->total_bitmaps--;\n\t\t\trecalculate_thresholds(ctl);\n\t\t} else if (!btrfs_free_space_trimmed(entry)) {\n\t\t\tctl->discardable_extents[BTRFS_STAT_CURR]--;\n\t\t}\n\t\tkmem_cache_free(btrfs_free_space_cachep, entry);\n\t}\n\n\tspin_unlock(&cluster->lock);\n\tspin_unlock(&ctl->tree_lock);\n\n\treturn ret;\n}\n\nstatic int btrfs_bitmap_cluster(struct btrfs_block_group *block_group,\n\t\t\t\tstruct btrfs_free_space *entry,\n\t\t\t\tstruct btrfs_free_cluster *cluster,\n\t\t\t\tu64 offset, u64 bytes,\n\t\t\t\tu64 cont1_bytes, u64 min_bytes)\n{\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tunsigned long next_zero;\n\tunsigned long i;\n\tunsigned long want_bits;\n\tunsigned long min_bits;\n\tunsigned long found_bits;\n\tunsigned long max_bits = 0;\n\tunsigned long start = 0;\n\tunsigned long total_found = 0;\n\tint ret;\n\n\tlockdep_assert_held(&ctl->tree_lock);\n\n\ti = offset_to_bit(entry->offset, ctl->unit,\n\t\t\t  max_t(u64, offset, entry->offset));\n\twant_bits = bytes_to_bits(bytes, ctl->unit);\n\tmin_bits = bytes_to_bits(min_bytes, ctl->unit);\n\n\t \n\tif (entry->max_extent_size &&\n\t    entry->max_extent_size < cont1_bytes)\n\t\treturn -ENOSPC;\nagain:\n\tfound_bits = 0;\n\tfor_each_set_bit_from(i, entry->bitmap, BITS_PER_BITMAP) {\n\t\tnext_zero = find_next_zero_bit(entry->bitmap,\n\t\t\t\t\t       BITS_PER_BITMAP, i);\n\t\tif (next_zero - i >= min_bits) {\n\t\t\tfound_bits = next_zero - i;\n\t\t\tif (found_bits > max_bits)\n\t\t\t\tmax_bits = found_bits;\n\t\t\tbreak;\n\t\t}\n\t\tif (next_zero - i > max_bits)\n\t\t\tmax_bits = next_zero - i;\n\t\ti = next_zero;\n\t}\n\n\tif (!found_bits) {\n\t\tentry->max_extent_size = (u64)max_bits * ctl->unit;\n\t\treturn -ENOSPC;\n\t}\n\n\tif (!total_found) {\n\t\tstart = i;\n\t\tcluster->max_size = 0;\n\t}\n\n\ttotal_found += found_bits;\n\n\tif (cluster->max_size < found_bits * ctl->unit)\n\t\tcluster->max_size = found_bits * ctl->unit;\n\n\tif (total_found < want_bits || cluster->max_size < cont1_bytes) {\n\t\ti = next_zero + 1;\n\t\tgoto again;\n\t}\n\n\tcluster->window_start = start * ctl->unit + entry->offset;\n\trb_erase(&entry->offset_index, &ctl->free_space_offset);\n\trb_erase_cached(&entry->bytes_index, &ctl->free_space_bytes);\n\n\t \n\tRB_CLEAR_NODE(&entry->bytes_index);\n\n\tret = tree_insert_offset(ctl, cluster, entry);\n\tASSERT(!ret);  \n\n\ttrace_btrfs_setup_cluster(block_group, cluster,\n\t\t\t\t  total_found * ctl->unit, 1);\n\treturn 0;\n}\n\n \nstatic noinline int\nsetup_cluster_no_bitmap(struct btrfs_block_group *block_group,\n\t\t\tstruct btrfs_free_cluster *cluster,\n\t\t\tstruct list_head *bitmaps, u64 offset, u64 bytes,\n\t\t\tu64 cont1_bytes, u64 min_bytes)\n{\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tstruct btrfs_free_space *first = NULL;\n\tstruct btrfs_free_space *entry = NULL;\n\tstruct btrfs_free_space *last;\n\tstruct rb_node *node;\n\tu64 window_free;\n\tu64 max_extent;\n\tu64 total_size = 0;\n\n\tlockdep_assert_held(&ctl->tree_lock);\n\n\tentry = tree_search_offset(ctl, offset, 0, 1);\n\tif (!entry)\n\t\treturn -ENOSPC;\n\n\t \n\twhile (entry->bitmap || entry->bytes < min_bytes) {\n\t\tif (entry->bitmap && list_empty(&entry->list))\n\t\t\tlist_add_tail(&entry->list, bitmaps);\n\t\tnode = rb_next(&entry->offset_index);\n\t\tif (!node)\n\t\t\treturn -ENOSPC;\n\t\tentry = rb_entry(node, struct btrfs_free_space, offset_index);\n\t}\n\n\twindow_free = entry->bytes;\n\tmax_extent = entry->bytes;\n\tfirst = entry;\n\tlast = entry;\n\n\tfor (node = rb_next(&entry->offset_index); node;\n\t     node = rb_next(&entry->offset_index)) {\n\t\tentry = rb_entry(node, struct btrfs_free_space, offset_index);\n\n\t\tif (entry->bitmap) {\n\t\t\tif (list_empty(&entry->list))\n\t\t\t\tlist_add_tail(&entry->list, bitmaps);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (entry->bytes < min_bytes)\n\t\t\tcontinue;\n\n\t\tlast = entry;\n\t\twindow_free += entry->bytes;\n\t\tif (entry->bytes > max_extent)\n\t\t\tmax_extent = entry->bytes;\n\t}\n\n\tif (window_free < bytes || max_extent < cont1_bytes)\n\t\treturn -ENOSPC;\n\n\tcluster->window_start = first->offset;\n\n\tnode = &first->offset_index;\n\n\t \n\tdo {\n\t\tint ret;\n\n\t\tentry = rb_entry(node, struct btrfs_free_space, offset_index);\n\t\tnode = rb_next(&entry->offset_index);\n\t\tif (entry->bitmap || entry->bytes < min_bytes)\n\t\t\tcontinue;\n\n\t\trb_erase(&entry->offset_index, &ctl->free_space_offset);\n\t\trb_erase_cached(&entry->bytes_index, &ctl->free_space_bytes);\n\t\tret = tree_insert_offset(ctl, cluster, entry);\n\t\ttotal_size += entry->bytes;\n\t\tASSERT(!ret);  \n\t} while (node && entry != last);\n\n\tcluster->max_size = max_extent;\n\ttrace_btrfs_setup_cluster(block_group, cluster, total_size, 0);\n\treturn 0;\n}\n\n \nstatic noinline int\nsetup_cluster_bitmap(struct btrfs_block_group *block_group,\n\t\t     struct btrfs_free_cluster *cluster,\n\t\t     struct list_head *bitmaps, u64 offset, u64 bytes,\n\t\t     u64 cont1_bytes, u64 min_bytes)\n{\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tstruct btrfs_free_space *entry = NULL;\n\tint ret = -ENOSPC;\n\tu64 bitmap_offset = offset_to_bitmap(ctl, offset);\n\n\tif (ctl->total_bitmaps == 0)\n\t\treturn -ENOSPC;\n\n\t \n\tif (!list_empty(bitmaps))\n\t\tentry = list_first_entry(bitmaps, struct btrfs_free_space, list);\n\n\tif (!entry || entry->offset != bitmap_offset) {\n\t\tentry = tree_search_offset(ctl, bitmap_offset, 1, 0);\n\t\tif (entry && list_empty(&entry->list))\n\t\t\tlist_add(&entry->list, bitmaps);\n\t}\n\n\tlist_for_each_entry(entry, bitmaps, list) {\n\t\tif (entry->bytes < bytes)\n\t\t\tcontinue;\n\t\tret = btrfs_bitmap_cluster(block_group, entry, cluster, offset,\n\t\t\t\t\t   bytes, cont1_bytes, min_bytes);\n\t\tif (!ret)\n\t\t\treturn 0;\n\t}\n\n\t \n\treturn -ENOSPC;\n}\n\n \nint btrfs_find_space_cluster(struct btrfs_block_group *block_group,\n\t\t\t     struct btrfs_free_cluster *cluster,\n\t\t\t     u64 offset, u64 bytes, u64 empty_size)\n{\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tstruct btrfs_free_space *entry, *tmp;\n\tLIST_HEAD(bitmaps);\n\tu64 min_bytes;\n\tu64 cont1_bytes;\n\tint ret;\n\n\t \n\tif (btrfs_test_opt(fs_info, SSD_SPREAD)) {\n\t\tcont1_bytes = bytes + empty_size;\n\t\tmin_bytes = cont1_bytes;\n\t} else if (block_group->flags & BTRFS_BLOCK_GROUP_METADATA) {\n\t\tcont1_bytes = bytes;\n\t\tmin_bytes = fs_info->sectorsize;\n\t} else {\n\t\tcont1_bytes = max(bytes, (bytes + empty_size) >> 2);\n\t\tmin_bytes = fs_info->sectorsize;\n\t}\n\n\tspin_lock(&ctl->tree_lock);\n\n\t \n\tif (ctl->free_space < bytes) {\n\t\tspin_unlock(&ctl->tree_lock);\n\t\treturn -ENOSPC;\n\t}\n\n\tspin_lock(&cluster->lock);\n\n\t \n\tif (cluster->block_group) {\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\ttrace_btrfs_find_cluster(block_group, offset, bytes, empty_size,\n\t\t\t\t min_bytes);\n\n\tret = setup_cluster_no_bitmap(block_group, cluster, &bitmaps, offset,\n\t\t\t\t      bytes + empty_size,\n\t\t\t\t      cont1_bytes, min_bytes);\n\tif (ret)\n\t\tret = setup_cluster_bitmap(block_group, cluster, &bitmaps,\n\t\t\t\t\t   offset, bytes + empty_size,\n\t\t\t\t\t   cont1_bytes, min_bytes);\n\n\t \n\tlist_for_each_entry_safe(entry, tmp, &bitmaps, list)\n\t\tlist_del_init(&entry->list);\n\n\tif (!ret) {\n\t\tbtrfs_get_block_group(block_group);\n\t\tlist_add_tail(&cluster->block_group_list,\n\t\t\t      &block_group->cluster_list);\n\t\tcluster->block_group = block_group;\n\t} else {\n\t\ttrace_btrfs_failed_cluster_setup(block_group);\n\t}\nout:\n\tspin_unlock(&cluster->lock);\n\tspin_unlock(&ctl->tree_lock);\n\n\treturn ret;\n}\n\n \nvoid btrfs_init_free_cluster(struct btrfs_free_cluster *cluster)\n{\n\tspin_lock_init(&cluster->lock);\n\tspin_lock_init(&cluster->refill_lock);\n\tcluster->root = RB_ROOT;\n\tcluster->max_size = 0;\n\tcluster->fragmented = false;\n\tINIT_LIST_HEAD(&cluster->block_group_list);\n\tcluster->block_group = NULL;\n}\n\nstatic int do_trimming(struct btrfs_block_group *block_group,\n\t\t       u64 *total_trimmed, u64 start, u64 bytes,\n\t\t       u64 reserved_start, u64 reserved_bytes,\n\t\t       enum btrfs_trim_state reserved_trim_state,\n\t\t       struct btrfs_trim_range *trim_entry)\n{\n\tstruct btrfs_space_info *space_info = block_group->space_info;\n\tstruct btrfs_fs_info *fs_info = block_group->fs_info;\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tint ret;\n\tint update = 0;\n\tconst u64 end = start + bytes;\n\tconst u64 reserved_end = reserved_start + reserved_bytes;\n\tenum btrfs_trim_state trim_state = BTRFS_TRIM_STATE_UNTRIMMED;\n\tu64 trimmed = 0;\n\n\tspin_lock(&space_info->lock);\n\tspin_lock(&block_group->lock);\n\tif (!block_group->ro) {\n\t\tblock_group->reserved += reserved_bytes;\n\t\tspace_info->bytes_reserved += reserved_bytes;\n\t\tupdate = 1;\n\t}\n\tspin_unlock(&block_group->lock);\n\tspin_unlock(&space_info->lock);\n\n\tret = btrfs_discard_extent(fs_info, start, bytes, &trimmed);\n\tif (!ret) {\n\t\t*total_trimmed += trimmed;\n\t\ttrim_state = BTRFS_TRIM_STATE_TRIMMED;\n\t}\n\n\tmutex_lock(&ctl->cache_writeout_mutex);\n\tif (reserved_start < start)\n\t\t__btrfs_add_free_space(block_group, reserved_start,\n\t\t\t\t       start - reserved_start,\n\t\t\t\t       reserved_trim_state);\n\tif (end < reserved_end)\n\t\t__btrfs_add_free_space(block_group, end, reserved_end - end,\n\t\t\t\t       reserved_trim_state);\n\t__btrfs_add_free_space(block_group, start, bytes, trim_state);\n\tlist_del(&trim_entry->list);\n\tmutex_unlock(&ctl->cache_writeout_mutex);\n\n\tif (update) {\n\t\tspin_lock(&space_info->lock);\n\t\tspin_lock(&block_group->lock);\n\t\tif (block_group->ro)\n\t\t\tspace_info->bytes_readonly += reserved_bytes;\n\t\tblock_group->reserved -= reserved_bytes;\n\t\tspace_info->bytes_reserved -= reserved_bytes;\n\t\tspin_unlock(&block_group->lock);\n\t\tspin_unlock(&space_info->lock);\n\t}\n\n\treturn ret;\n}\n\n \nstatic int trim_no_bitmap(struct btrfs_block_group *block_group,\n\t\t\t  u64 *total_trimmed, u64 start, u64 end, u64 minlen,\n\t\t\t  bool async)\n{\n\tstruct btrfs_discard_ctl *discard_ctl =\n\t\t\t\t\t&block_group->fs_info->discard_ctl;\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tstruct btrfs_free_space *entry;\n\tstruct rb_node *node;\n\tint ret = 0;\n\tu64 extent_start;\n\tu64 extent_bytes;\n\tenum btrfs_trim_state extent_trim_state;\n\tu64 bytes;\n\tconst u64 max_discard_size = READ_ONCE(discard_ctl->max_discard_size);\n\n\twhile (start < end) {\n\t\tstruct btrfs_trim_range trim_entry;\n\n\t\tmutex_lock(&ctl->cache_writeout_mutex);\n\t\tspin_lock(&ctl->tree_lock);\n\n\t\tif (ctl->free_space < minlen)\n\t\t\tgoto out_unlock;\n\n\t\tentry = tree_search_offset(ctl, start, 0, 1);\n\t\tif (!entry)\n\t\t\tgoto out_unlock;\n\n\t\t \n\t\twhile (entry->bitmap ||\n\t\t       (async && btrfs_free_space_trimmed(entry))) {\n\t\t\tnode = rb_next(&entry->offset_index);\n\t\t\tif (!node)\n\t\t\t\tgoto out_unlock;\n\t\t\tentry = rb_entry(node, struct btrfs_free_space,\n\t\t\t\t\t offset_index);\n\t\t}\n\n\t\tif (entry->offset >= end)\n\t\t\tgoto out_unlock;\n\n\t\textent_start = entry->offset;\n\t\textent_bytes = entry->bytes;\n\t\textent_trim_state = entry->trim_state;\n\t\tif (async) {\n\t\t\tstart = entry->offset;\n\t\t\tbytes = entry->bytes;\n\t\t\tif (bytes < minlen) {\n\t\t\t\tspin_unlock(&ctl->tree_lock);\n\t\t\t\tmutex_unlock(&ctl->cache_writeout_mutex);\n\t\t\t\tgoto next;\n\t\t\t}\n\t\t\tunlink_free_space(ctl, entry, true);\n\t\t\t \n\t\t\tif (max_discard_size &&\n\t\t\t    bytes >= (max_discard_size +\n\t\t\t\t      BTRFS_ASYNC_DISCARD_MIN_FILTER)) {\n\t\t\t\tbytes = max_discard_size;\n\t\t\t\textent_bytes = max_discard_size;\n\t\t\t\tentry->offset += max_discard_size;\n\t\t\t\tentry->bytes -= max_discard_size;\n\t\t\t\tlink_free_space(ctl, entry);\n\t\t\t} else {\n\t\t\t\tkmem_cache_free(btrfs_free_space_cachep, entry);\n\t\t\t}\n\t\t} else {\n\t\t\tstart = max(start, extent_start);\n\t\t\tbytes = min(extent_start + extent_bytes, end) - start;\n\t\t\tif (bytes < minlen) {\n\t\t\t\tspin_unlock(&ctl->tree_lock);\n\t\t\t\tmutex_unlock(&ctl->cache_writeout_mutex);\n\t\t\t\tgoto next;\n\t\t\t}\n\n\t\t\tunlink_free_space(ctl, entry, true);\n\t\t\tkmem_cache_free(btrfs_free_space_cachep, entry);\n\t\t}\n\n\t\tspin_unlock(&ctl->tree_lock);\n\t\ttrim_entry.start = extent_start;\n\t\ttrim_entry.bytes = extent_bytes;\n\t\tlist_add_tail(&trim_entry.list, &ctl->trimming_ranges);\n\t\tmutex_unlock(&ctl->cache_writeout_mutex);\n\n\t\tret = do_trimming(block_group, total_trimmed, start, bytes,\n\t\t\t\t  extent_start, extent_bytes, extent_trim_state,\n\t\t\t\t  &trim_entry);\n\t\tif (ret) {\n\t\t\tblock_group->discard_cursor = start + bytes;\n\t\t\tbreak;\n\t\t}\nnext:\n\t\tstart += bytes;\n\t\tblock_group->discard_cursor = start;\n\t\tif (async && *total_trimmed)\n\t\t\tbreak;\n\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\n\t\tcond_resched();\n\t}\n\n\treturn ret;\n\nout_unlock:\n\tblock_group->discard_cursor = btrfs_block_group_end(block_group);\n\tspin_unlock(&ctl->tree_lock);\n\tmutex_unlock(&ctl->cache_writeout_mutex);\n\n\treturn ret;\n}\n\n \nstatic void reset_trimming_bitmap(struct btrfs_free_space_ctl *ctl, u64 offset)\n{\n\tstruct btrfs_free_space *entry;\n\n\tspin_lock(&ctl->tree_lock);\n\tentry = tree_search_offset(ctl, offset, 1, 0);\n\tif (entry) {\n\t\tif (btrfs_free_space_trimmed(entry)) {\n\t\t\tctl->discardable_extents[BTRFS_STAT_CURR] +=\n\t\t\t\tentry->bitmap_extents;\n\t\t\tctl->discardable_bytes[BTRFS_STAT_CURR] += entry->bytes;\n\t\t}\n\t\tentry->trim_state = BTRFS_TRIM_STATE_UNTRIMMED;\n\t}\n\n\tspin_unlock(&ctl->tree_lock);\n}\n\nstatic void end_trimming_bitmap(struct btrfs_free_space_ctl *ctl,\n\t\t\t\tstruct btrfs_free_space *entry)\n{\n\tif (btrfs_free_space_trimming_bitmap(entry)) {\n\t\tentry->trim_state = BTRFS_TRIM_STATE_TRIMMED;\n\t\tctl->discardable_extents[BTRFS_STAT_CURR] -=\n\t\t\tentry->bitmap_extents;\n\t\tctl->discardable_bytes[BTRFS_STAT_CURR] -= entry->bytes;\n\t}\n}\n\n \nstatic int trim_bitmaps(struct btrfs_block_group *block_group,\n\t\t\tu64 *total_trimmed, u64 start, u64 end, u64 minlen,\n\t\t\tu64 maxlen, bool async)\n{\n\tstruct btrfs_discard_ctl *discard_ctl =\n\t\t\t\t\t&block_group->fs_info->discard_ctl;\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tstruct btrfs_free_space *entry;\n\tint ret = 0;\n\tint ret2;\n\tu64 bytes;\n\tu64 offset = offset_to_bitmap(ctl, start);\n\tconst u64 max_discard_size = READ_ONCE(discard_ctl->max_discard_size);\n\n\twhile (offset < end) {\n\t\tbool next_bitmap = false;\n\t\tstruct btrfs_trim_range trim_entry;\n\n\t\tmutex_lock(&ctl->cache_writeout_mutex);\n\t\tspin_lock(&ctl->tree_lock);\n\n\t\tif (ctl->free_space < minlen) {\n\t\t\tblock_group->discard_cursor =\n\t\t\t\tbtrfs_block_group_end(block_group);\n\t\t\tspin_unlock(&ctl->tree_lock);\n\t\t\tmutex_unlock(&ctl->cache_writeout_mutex);\n\t\t\tbreak;\n\t\t}\n\n\t\tentry = tree_search_offset(ctl, offset, 1, 0);\n\t\t \n\t\tif (!entry || (async && minlen && start == offset &&\n\t\t\t       btrfs_free_space_trimmed(entry))) {\n\t\t\tspin_unlock(&ctl->tree_lock);\n\t\t\tmutex_unlock(&ctl->cache_writeout_mutex);\n\t\t\tnext_bitmap = true;\n\t\t\tgoto next;\n\t\t}\n\n\t\t \n\t\tif (start == offset)\n\t\t\tentry->trim_state = BTRFS_TRIM_STATE_TRIMMING;\n\n\t\tbytes = minlen;\n\t\tret2 = search_bitmap(ctl, entry, &start, &bytes, false);\n\t\tif (ret2 || start >= end) {\n\t\t\t \n\t\t\tif (ret2 && minlen <= BTRFS_ASYNC_DISCARD_MIN_FILTER)\n\t\t\t\tend_trimming_bitmap(ctl, entry);\n\t\t\telse\n\t\t\t\tentry->trim_state = BTRFS_TRIM_STATE_UNTRIMMED;\n\t\t\tspin_unlock(&ctl->tree_lock);\n\t\t\tmutex_unlock(&ctl->cache_writeout_mutex);\n\t\t\tnext_bitmap = true;\n\t\t\tgoto next;\n\t\t}\n\n\t\t \n\t\tif (async && *total_trimmed) {\n\t\t\tspin_unlock(&ctl->tree_lock);\n\t\t\tmutex_unlock(&ctl->cache_writeout_mutex);\n\t\t\tgoto out;\n\t\t}\n\n\t\tbytes = min(bytes, end - start);\n\t\tif (bytes < minlen || (async && maxlen && bytes > maxlen)) {\n\t\t\tspin_unlock(&ctl->tree_lock);\n\t\t\tmutex_unlock(&ctl->cache_writeout_mutex);\n\t\t\tgoto next;\n\t\t}\n\n\t\t \n\t\tif (async &&\n\t\t    max_discard_size &&\n\t\t    bytes > (max_discard_size + minlen))\n\t\t\tbytes = max_discard_size;\n\n\t\tbitmap_clear_bits(ctl, entry, start, bytes, true);\n\t\tif (entry->bytes == 0)\n\t\t\tfree_bitmap(ctl, entry);\n\n\t\tspin_unlock(&ctl->tree_lock);\n\t\ttrim_entry.start = start;\n\t\ttrim_entry.bytes = bytes;\n\t\tlist_add_tail(&trim_entry.list, &ctl->trimming_ranges);\n\t\tmutex_unlock(&ctl->cache_writeout_mutex);\n\n\t\tret = do_trimming(block_group, total_trimmed, start, bytes,\n\t\t\t\t  start, bytes, 0, &trim_entry);\n\t\tif (ret) {\n\t\t\treset_trimming_bitmap(ctl, offset);\n\t\t\tblock_group->discard_cursor =\n\t\t\t\tbtrfs_block_group_end(block_group);\n\t\t\tbreak;\n\t\t}\nnext:\n\t\tif (next_bitmap) {\n\t\t\toffset += BITS_PER_BITMAP * ctl->unit;\n\t\t\tstart = offset;\n\t\t} else {\n\t\t\tstart += bytes;\n\t\t}\n\t\tblock_group->discard_cursor = start;\n\n\t\tif (fatal_signal_pending(current)) {\n\t\t\tif (start != offset)\n\t\t\t\treset_trimming_bitmap(ctl, offset);\n\t\t\tret = -ERESTARTSYS;\n\t\t\tbreak;\n\t\t}\n\n\t\tcond_resched();\n\t}\n\n\tif (offset >= end)\n\t\tblock_group->discard_cursor = end;\n\nout:\n\treturn ret;\n}\n\nint btrfs_trim_block_group(struct btrfs_block_group *block_group,\n\t\t\t   u64 *trimmed, u64 start, u64 end, u64 minlen)\n{\n\tstruct btrfs_free_space_ctl *ctl = block_group->free_space_ctl;\n\tint ret;\n\tu64 rem = 0;\n\n\tASSERT(!btrfs_is_zoned(block_group->fs_info));\n\n\t*trimmed = 0;\n\n\tspin_lock(&block_group->lock);\n\tif (test_bit(BLOCK_GROUP_FLAG_REMOVED, &block_group->runtime_flags)) {\n\t\tspin_unlock(&block_group->lock);\n\t\treturn 0;\n\t}\n\tbtrfs_freeze_block_group(block_group);\n\tspin_unlock(&block_group->lock);\n\n\tret = trim_no_bitmap(block_group, trimmed, start, end, minlen, false);\n\tif (ret)\n\t\tgoto out;\n\n\tret = trim_bitmaps(block_group, trimmed, start, end, minlen, 0, false);\n\tdiv64_u64_rem(end, BITS_PER_BITMAP * ctl->unit, &rem);\n\t \n\tif (rem)\n\t\treset_trimming_bitmap(ctl, offset_to_bitmap(ctl, end));\nout:\n\tbtrfs_unfreeze_block_group(block_group);\n\treturn ret;\n}\n\nint btrfs_trim_block_group_extents(struct btrfs_block_group *block_group,\n\t\t\t\t   u64 *trimmed, u64 start, u64 end, u64 minlen,\n\t\t\t\t   bool async)\n{\n\tint ret;\n\n\t*trimmed = 0;\n\n\tspin_lock(&block_group->lock);\n\tif (test_bit(BLOCK_GROUP_FLAG_REMOVED, &block_group->runtime_flags)) {\n\t\tspin_unlock(&block_group->lock);\n\t\treturn 0;\n\t}\n\tbtrfs_freeze_block_group(block_group);\n\tspin_unlock(&block_group->lock);\n\n\tret = trim_no_bitmap(block_group, trimmed, start, end, minlen, async);\n\tbtrfs_unfreeze_block_group(block_group);\n\n\treturn ret;\n}\n\nint btrfs_trim_block_group_bitmaps(struct btrfs_block_group *block_group,\n\t\t\t\t   u64 *trimmed, u64 start, u64 end, u64 minlen,\n\t\t\t\t   u64 maxlen, bool async)\n{\n\tint ret;\n\n\t*trimmed = 0;\n\n\tspin_lock(&block_group->lock);\n\tif (test_bit(BLOCK_GROUP_FLAG_REMOVED, &block_group->runtime_flags)) {\n\t\tspin_unlock(&block_group->lock);\n\t\treturn 0;\n\t}\n\tbtrfs_freeze_block_group(block_group);\n\tspin_unlock(&block_group->lock);\n\n\tret = trim_bitmaps(block_group, trimmed, start, end, minlen, maxlen,\n\t\t\t   async);\n\n\tbtrfs_unfreeze_block_group(block_group);\n\n\treturn ret;\n}\n\nbool btrfs_free_space_cache_v1_active(struct btrfs_fs_info *fs_info)\n{\n\treturn btrfs_super_cache_generation(fs_info->super_copy);\n}\n\nstatic int cleanup_free_space_cache_v1(struct btrfs_fs_info *fs_info,\n\t\t\t\t       struct btrfs_trans_handle *trans)\n{\n\tstruct btrfs_block_group *block_group;\n\tstruct rb_node *node;\n\tint ret = 0;\n\n\tbtrfs_info(fs_info, \"cleaning free space cache v1\");\n\n\tnode = rb_first_cached(&fs_info->block_group_cache_tree);\n\twhile (node) {\n\t\tblock_group = rb_entry(node, struct btrfs_block_group, cache_node);\n\t\tret = btrfs_remove_free_space_inode(trans, NULL, block_group);\n\t\tif (ret)\n\t\t\tgoto out;\n\t\tnode = rb_next(node);\n\t}\nout:\n\treturn ret;\n}\n\nint btrfs_set_free_space_cache_v1_active(struct btrfs_fs_info *fs_info, bool active)\n{\n\tstruct btrfs_trans_handle *trans;\n\tint ret;\n\n\t \n\ttrans = btrfs_start_transaction(fs_info->tree_root, 0);\n\tif (IS_ERR(trans))\n\t\treturn PTR_ERR(trans);\n\n\tif (!active) {\n\t\tset_bit(BTRFS_FS_CLEANUP_SPACE_CACHE_V1, &fs_info->flags);\n\t\tret = cleanup_free_space_cache_v1(fs_info, trans);\n\t\tif (ret) {\n\t\t\tbtrfs_abort_transaction(trans, ret);\n\t\t\tbtrfs_end_transaction(trans);\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tret = btrfs_commit_transaction(trans);\nout:\n\tclear_bit(BTRFS_FS_CLEANUP_SPACE_CACHE_V1, &fs_info->flags);\n\n\treturn ret;\n}\n\nint __init btrfs_free_space_init(void)\n{\n\tbtrfs_free_space_cachep = kmem_cache_create(\"btrfs_free_space\",\n\t\t\tsizeof(struct btrfs_free_space), 0,\n\t\t\tSLAB_MEM_SPREAD, NULL);\n\tif (!btrfs_free_space_cachep)\n\t\treturn -ENOMEM;\n\n\tbtrfs_free_space_bitmap_cachep = kmem_cache_create(\"btrfs_free_space_bitmap\",\n\t\t\t\t\t\t\tPAGE_SIZE, PAGE_SIZE,\n\t\t\t\t\t\t\tSLAB_MEM_SPREAD, NULL);\n\tif (!btrfs_free_space_bitmap_cachep) {\n\t\tkmem_cache_destroy(btrfs_free_space_cachep);\n\t\treturn -ENOMEM;\n\t}\n\n\treturn 0;\n}\n\nvoid __cold btrfs_free_space_exit(void)\n{\n\tkmem_cache_destroy(btrfs_free_space_cachep);\n\tkmem_cache_destroy(btrfs_free_space_bitmap_cachep);\n}\n\n#ifdef CONFIG_BTRFS_FS_RUN_SANITY_TESTS\n \nint test_add_free_space_entry(struct btrfs_block_group *cache,\n\t\t\t      u64 offset, u64 bytes, bool bitmap)\n{\n\tstruct btrfs_free_space_ctl *ctl = cache->free_space_ctl;\n\tstruct btrfs_free_space *info = NULL, *bitmap_info;\n\tvoid *map = NULL;\n\tenum btrfs_trim_state trim_state = BTRFS_TRIM_STATE_TRIMMED;\n\tu64 bytes_added;\n\tint ret;\n\nagain:\n\tif (!info) {\n\t\tinfo = kmem_cache_zalloc(btrfs_free_space_cachep, GFP_NOFS);\n\t\tif (!info)\n\t\t\treturn -ENOMEM;\n\t}\n\n\tif (!bitmap) {\n\t\tspin_lock(&ctl->tree_lock);\n\t\tinfo->offset = offset;\n\t\tinfo->bytes = bytes;\n\t\tinfo->max_extent_size = 0;\n\t\tret = link_free_space(ctl, info);\n\t\tspin_unlock(&ctl->tree_lock);\n\t\tif (ret)\n\t\t\tkmem_cache_free(btrfs_free_space_cachep, info);\n\t\treturn ret;\n\t}\n\n\tif (!map) {\n\t\tmap = kmem_cache_zalloc(btrfs_free_space_bitmap_cachep, GFP_NOFS);\n\t\tif (!map) {\n\t\t\tkmem_cache_free(btrfs_free_space_cachep, info);\n\t\t\treturn -ENOMEM;\n\t\t}\n\t}\n\n\tspin_lock(&ctl->tree_lock);\n\tbitmap_info = tree_search_offset(ctl, offset_to_bitmap(ctl, offset),\n\t\t\t\t\t 1, 0);\n\tif (!bitmap_info) {\n\t\tinfo->bitmap = map;\n\t\tmap = NULL;\n\t\tadd_new_bitmap(ctl, info, offset);\n\t\tbitmap_info = info;\n\t\tinfo = NULL;\n\t}\n\n\tbytes_added = add_bytes_to_bitmap(ctl, bitmap_info, offset, bytes,\n\t\t\t\t\t  trim_state);\n\n\tbytes -= bytes_added;\n\toffset += bytes_added;\n\tspin_unlock(&ctl->tree_lock);\n\n\tif (bytes)\n\t\tgoto again;\n\n\tif (info)\n\t\tkmem_cache_free(btrfs_free_space_cachep, info);\n\tif (map)\n\t\tkmem_cache_free(btrfs_free_space_bitmap_cachep, map);\n\treturn 0;\n}\n\n \nint test_check_exists(struct btrfs_block_group *cache,\n\t\t      u64 offset, u64 bytes)\n{\n\tstruct btrfs_free_space_ctl *ctl = cache->free_space_ctl;\n\tstruct btrfs_free_space *info;\n\tint ret = 0;\n\n\tspin_lock(&ctl->tree_lock);\n\tinfo = tree_search_offset(ctl, offset, 0, 0);\n\tif (!info) {\n\t\tinfo = tree_search_offset(ctl, offset_to_bitmap(ctl, offset),\n\t\t\t\t\t  1, 0);\n\t\tif (!info)\n\t\t\tgoto out;\n\t}\n\nhave_info:\n\tif (info->bitmap) {\n\t\tu64 bit_off, bit_bytes;\n\t\tstruct rb_node *n;\n\t\tstruct btrfs_free_space *tmp;\n\n\t\tbit_off = offset;\n\t\tbit_bytes = ctl->unit;\n\t\tret = search_bitmap(ctl, info, &bit_off, &bit_bytes, false);\n\t\tif (!ret) {\n\t\t\tif (bit_off == offset) {\n\t\t\t\tret = 1;\n\t\t\t\tgoto out;\n\t\t\t} else if (bit_off > offset &&\n\t\t\t\t   offset + bytes > bit_off) {\n\t\t\t\tret = 1;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\n\t\tn = rb_prev(&info->offset_index);\n\t\twhile (n) {\n\t\t\ttmp = rb_entry(n, struct btrfs_free_space,\n\t\t\t\t       offset_index);\n\t\t\tif (tmp->offset + tmp->bytes < offset)\n\t\t\t\tbreak;\n\t\t\tif (offset + bytes < tmp->offset) {\n\t\t\t\tn = rb_prev(&tmp->offset_index);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tinfo = tmp;\n\t\t\tgoto have_info;\n\t\t}\n\n\t\tn = rb_next(&info->offset_index);\n\t\twhile (n) {\n\t\t\ttmp = rb_entry(n, struct btrfs_free_space,\n\t\t\t\t       offset_index);\n\t\t\tif (offset + bytes < tmp->offset)\n\t\t\t\tbreak;\n\t\t\tif (tmp->offset + tmp->bytes < offset) {\n\t\t\t\tn = rb_next(&tmp->offset_index);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tinfo = tmp;\n\t\t\tgoto have_info;\n\t\t}\n\n\t\tret = 0;\n\t\tgoto out;\n\t}\n\n\tif (info->offset == offset) {\n\t\tret = 1;\n\t\tgoto out;\n\t}\n\n\tif (offset > info->offset && offset < info->offset + info->bytes)\n\t\tret = 1;\nout:\n\tspin_unlock(&ctl->tree_lock);\n\treturn ret;\n}\n#endif  \n",
  "logic_map": {},
  "failure_modes": [],
  "crash_correlation_map": {}
}